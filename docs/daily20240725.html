<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240724.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "SV4D: Dynamic 3D Content Generation with Multi-Frame and Multi-View\n  Consistency", "author": "Yiming Xie and Chun-Han Yao and Vikram Voleti and Huaizu Jiang and Varun Jampani", "abstract": "  We present Stable Video 4D (SV4D), a latent video diffusion model for\nmulti-frame and multi-view consistent dynamic 3D content generation. Unlike\nprevious methods that rely on separately trained generative models for video\ngeneration and novel view synthesis, we design a unified diffusion model to\ngenerate novel view videos of dynamic 3D objects. Specifically, given a\nmonocular reference video, SV4D generates novel views for each video frame that\nare temporally consistent. We then use the generated novel view videos to\noptimize an implicit 4D representation (dynamic NeRF) efficiently, without the\nneed for cumbersome SDS-based optimization used in most prior works. To train\nour unified novel view video generation model, we curated a dynamic 3D object\ndataset from the existing Objaverse dataset. Extensive experimental results on\nmultiple datasets and user studies demonstrate SV4D's state-of-the-art\nperformance on novel-view video synthesis as well as 4D generation compared to\nprior works.\n", "link": "http://arxiv.org/abs/2407.17470v1", "date": "2024-07-24", "relevancy": 3.2882, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6782}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6782}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SV4D%3A%20Dynamic%203D%20Content%20Generation%20with%20Multi-Frame%20and%20Multi-View%0A%20%20Consistency&body=Title%3A%20SV4D%3A%20Dynamic%203D%20Content%20Generation%20with%20Multi-Frame%20and%20Multi-View%0A%20%20Consistency%0AAuthor%3A%20Yiming%20Xie%20and%20Chun-Han%20Yao%20and%20Vikram%20Voleti%20and%20Huaizu%20Jiang%20and%20Varun%20Jampani%0AAbstract%3A%20%20%20We%20present%20Stable%20Video%204D%20%28SV4D%29%2C%20a%20latent%20video%20diffusion%20model%20for%0Amulti-frame%20and%20multi-view%20consistent%20dynamic%203D%20content%20generation.%20Unlike%0Aprevious%20methods%20that%20rely%20on%20separately%20trained%20generative%20models%20for%20video%0Ageneration%20and%20novel%20view%20synthesis%2C%20we%20design%20a%20unified%20diffusion%20model%20to%0Agenerate%20novel%20view%20videos%20of%20dynamic%203D%20objects.%20Specifically%2C%20given%20a%0Amonocular%20reference%20video%2C%20SV4D%20generates%20novel%20views%20for%20each%20video%20frame%20that%0Aare%20temporally%20consistent.%20We%20then%20use%20the%20generated%20novel%20view%20videos%20to%0Aoptimize%20an%20implicit%204D%20representation%20%28dynamic%20NeRF%29%20efficiently%2C%20without%20the%0Aneed%20for%20cumbersome%20SDS-based%20optimization%20used%20in%20most%20prior%20works.%20To%20train%0Aour%20unified%20novel%20view%20video%20generation%20model%2C%20we%20curated%20a%20dynamic%203D%20object%0Adataset%20from%20the%20existing%20Objaverse%20dataset.%20Extensive%20experimental%20results%20on%0Amultiple%20datasets%20and%20user%20studies%20demonstrate%20SV4D%27s%20state-of-the-art%0Aperformance%20on%20novel-view%20video%20synthesis%20as%20well%20as%204D%20generation%20compared%20to%0Aprior%20works.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17470v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSV4D%253A%2520Dynamic%25203D%2520Content%2520Generation%2520with%2520Multi-Frame%2520and%2520Multi-View%250A%2520%2520Consistency%26entry.906535625%3DYiming%2520Xie%2520and%2520Chun-Han%2520Yao%2520and%2520Vikram%2520Voleti%2520and%2520Huaizu%2520Jiang%2520and%2520Varun%2520Jampani%26entry.1292438233%3D%2520%2520We%2520present%2520Stable%2520Video%25204D%2520%2528SV4D%2529%252C%2520a%2520latent%2520video%2520diffusion%2520model%2520for%250Amulti-frame%2520and%2520multi-view%2520consistent%2520dynamic%25203D%2520content%2520generation.%2520Unlike%250Aprevious%2520methods%2520that%2520rely%2520on%2520separately%2520trained%2520generative%2520models%2520for%2520video%250Ageneration%2520and%2520novel%2520view%2520synthesis%252C%2520we%2520design%2520a%2520unified%2520diffusion%2520model%2520to%250Agenerate%2520novel%2520view%2520videos%2520of%2520dynamic%25203D%2520objects.%2520Specifically%252C%2520given%2520a%250Amonocular%2520reference%2520video%252C%2520SV4D%2520generates%2520novel%2520views%2520for%2520each%2520video%2520frame%2520that%250Aare%2520temporally%2520consistent.%2520We%2520then%2520use%2520the%2520generated%2520novel%2520view%2520videos%2520to%250Aoptimize%2520an%2520implicit%25204D%2520representation%2520%2528dynamic%2520NeRF%2529%2520efficiently%252C%2520without%2520the%250Aneed%2520for%2520cumbersome%2520SDS-based%2520optimization%2520used%2520in%2520most%2520prior%2520works.%2520To%2520train%250Aour%2520unified%2520novel%2520view%2520video%2520generation%2520model%252C%2520we%2520curated%2520a%2520dynamic%25203D%2520object%250Adataset%2520from%2520the%2520existing%2520Objaverse%2520dataset.%2520Extensive%2520experimental%2520results%2520on%250Amultiple%2520datasets%2520and%2520user%2520studies%2520demonstrate%2520SV4D%2527s%2520state-of-the-art%250Aperformance%2520on%2520novel-view%2520video%2520synthesis%2520as%2520well%2520as%25204D%2520generation%2520compared%2520to%250Aprior%2520works.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17470v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SV4D%3A%20Dynamic%203D%20Content%20Generation%20with%20Multi-Frame%20and%20Multi-View%0A%20%20Consistency&entry.906535625=Yiming%20Xie%20and%20Chun-Han%20Yao%20and%20Vikram%20Voleti%20and%20Huaizu%20Jiang%20and%20Varun%20Jampani&entry.1292438233=%20%20We%20present%20Stable%20Video%204D%20%28SV4D%29%2C%20a%20latent%20video%20diffusion%20model%20for%0Amulti-frame%20and%20multi-view%20consistent%20dynamic%203D%20content%20generation.%20Unlike%0Aprevious%20methods%20that%20rely%20on%20separately%20trained%20generative%20models%20for%20video%0Ageneration%20and%20novel%20view%20synthesis%2C%20we%20design%20a%20unified%20diffusion%20model%20to%0Agenerate%20novel%20view%20videos%20of%20dynamic%203D%20objects.%20Specifically%2C%20given%20a%0Amonocular%20reference%20video%2C%20SV4D%20generates%20novel%20views%20for%20each%20video%20frame%20that%0Aare%20temporally%20consistent.%20We%20then%20use%20the%20generated%20novel%20view%20videos%20to%0Aoptimize%20an%20implicit%204D%20representation%20%28dynamic%20NeRF%29%20efficiently%2C%20without%20the%0Aneed%20for%20cumbersome%20SDS-based%20optimization%20used%20in%20most%20prior%20works.%20To%20train%0Aour%20unified%20novel%20view%20video%20generation%20model%2C%20we%20curated%20a%20dynamic%203D%20object%0Adataset%20from%20the%20existing%20Objaverse%20dataset.%20Extensive%20experimental%20results%20on%0Amultiple%20datasets%20and%20user%20studies%20demonstrate%20SV4D%27s%20state-of-the-art%0Aperformance%20on%20novel-view%20video%20synthesis%20as%20well%20as%204D%20generation%20compared%20to%0Aprior%20works.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17470v1&entry.124074799=Read"},
{"title": "3D Gaussian Splatting: Survey, Technologies, Challenges, and\n  Opportunities", "author": "Yanqi Bao and Tianyu Ding and Jing Huo and Yaoli Liu and Yuxin Li and Wenbin Li and Yang Gao and Jiebo Luo", "abstract": "  3D Gaussian Splatting (3DGS) has emerged as a prominent technique with the\npotential to become a mainstream method for 3D representations. It can\neffectively transform multi-view images into explicit 3D Gaussian\nrepresentations through efficient training, and achieve real-time rendering of\nnovel views. This survey aims to analyze existing 3DGS-related works from\nmultiple intersecting perspectives, including related tasks, technologies,\nchallenges, and opportunities. The primary objective is to provide newcomers\nwith a rapid understanding of the field and to assist researchers in\nmethodically organizing existing technologies and challenges. Specifically, we\ndelve into the optimization, application, and extension of 3DGS, categorizing\nthem based on their focuses or motivations. Additionally, we summarize and\nclassify nine types of technical modules and corresponding improvements\nidentified in existing works. Based on these analyses, we further examine the\ncommon challenges and technologies across various tasks, proposing potential\nresearch opportunities.\n", "link": "http://arxiv.org/abs/2407.17418v1", "date": "2024-07-24", "relevancy": 3.1189, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6919}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6103}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Gaussian%20Splatting%3A%20Survey%2C%20Technologies%2C%20Challenges%2C%20and%0A%20%20Opportunities&body=Title%3A%203D%20Gaussian%20Splatting%3A%20Survey%2C%20Technologies%2C%20Challenges%2C%20and%0A%20%20Opportunities%0AAuthor%3A%20Yanqi%20Bao%20and%20Tianyu%20Ding%20and%20Jing%20Huo%20and%20Yaoli%20Liu%20and%20Yuxin%20Li%20and%20Wenbin%20Li%20and%20Yang%20Gao%20and%20Jiebo%20Luo%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20prominent%20technique%20with%20the%0Apotential%20to%20become%20a%20mainstream%20method%20for%203D%20representations.%20It%20can%0Aeffectively%20transform%20multi-view%20images%20into%20explicit%203D%20Gaussian%0Arepresentations%20through%20efficient%20training%2C%20and%20achieve%20real-time%20rendering%20of%0Anovel%20views.%20This%20survey%20aims%20to%20analyze%20existing%203DGS-related%20works%20from%0Amultiple%20intersecting%20perspectives%2C%20including%20related%20tasks%2C%20technologies%2C%0Achallenges%2C%20and%20opportunities.%20The%20primary%20objective%20is%20to%20provide%20newcomers%0Awith%20a%20rapid%20understanding%20of%20the%20field%20and%20to%20assist%20researchers%20in%0Amethodically%20organizing%20existing%20technologies%20and%20challenges.%20Specifically%2C%20we%0Adelve%20into%20the%20optimization%2C%20application%2C%20and%20extension%20of%203DGS%2C%20categorizing%0Athem%20based%20on%20their%20focuses%20or%20motivations.%20Additionally%2C%20we%20summarize%20and%0Aclassify%20nine%20types%20of%20technical%20modules%20and%20corresponding%20improvements%0Aidentified%20in%20existing%20works.%20Based%20on%20these%20analyses%2C%20we%20further%20examine%20the%0Acommon%20challenges%20and%20technologies%20across%20various%20tasks%2C%20proposing%20potential%0Aresearch%20opportunities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17418v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Gaussian%2520Splatting%253A%2520Survey%252C%2520Technologies%252C%2520Challenges%252C%2520and%250A%2520%2520Opportunities%26entry.906535625%3DYanqi%2520Bao%2520and%2520Tianyu%2520Ding%2520and%2520Jing%2520Huo%2520and%2520Yaoli%2520Liu%2520and%2520Yuxin%2520Li%2520and%2520Wenbin%2520Li%2520and%2520Yang%2520Gao%2520and%2520Jiebo%2520Luo%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520emerged%2520as%2520a%2520prominent%2520technique%2520with%2520the%250Apotential%2520to%2520become%2520a%2520mainstream%2520method%2520for%25203D%2520representations.%2520It%2520can%250Aeffectively%2520transform%2520multi-view%2520images%2520into%2520explicit%25203D%2520Gaussian%250Arepresentations%2520through%2520efficient%2520training%252C%2520and%2520achieve%2520real-time%2520rendering%2520of%250Anovel%2520views.%2520This%2520survey%2520aims%2520to%2520analyze%2520existing%25203DGS-related%2520works%2520from%250Amultiple%2520intersecting%2520perspectives%252C%2520including%2520related%2520tasks%252C%2520technologies%252C%250Achallenges%252C%2520and%2520opportunities.%2520The%2520primary%2520objective%2520is%2520to%2520provide%2520newcomers%250Awith%2520a%2520rapid%2520understanding%2520of%2520the%2520field%2520and%2520to%2520assist%2520researchers%2520in%250Amethodically%2520organizing%2520existing%2520technologies%2520and%2520challenges.%2520Specifically%252C%2520we%250Adelve%2520into%2520the%2520optimization%252C%2520application%252C%2520and%2520extension%2520of%25203DGS%252C%2520categorizing%250Athem%2520based%2520on%2520their%2520focuses%2520or%2520motivations.%2520Additionally%252C%2520we%2520summarize%2520and%250Aclassify%2520nine%2520types%2520of%2520technical%2520modules%2520and%2520corresponding%2520improvements%250Aidentified%2520in%2520existing%2520works.%2520Based%2520on%2520these%2520analyses%252C%2520we%2520further%2520examine%2520the%250Acommon%2520challenges%2520and%2520technologies%2520across%2520various%2520tasks%252C%2520proposing%2520potential%250Aresearch%2520opportunities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17418v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Gaussian%20Splatting%3A%20Survey%2C%20Technologies%2C%20Challenges%2C%20and%0A%20%20Opportunities&entry.906535625=Yanqi%20Bao%20and%20Tianyu%20Ding%20and%20Jing%20Huo%20and%20Yaoli%20Liu%20and%20Yuxin%20Li%20and%20Wenbin%20Li%20and%20Yang%20Gao%20and%20Jiebo%20Luo&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20prominent%20technique%20with%20the%0Apotential%20to%20become%20a%20mainstream%20method%20for%203D%20representations.%20It%20can%0Aeffectively%20transform%20multi-view%20images%20into%20explicit%203D%20Gaussian%0Arepresentations%20through%20efficient%20training%2C%20and%20achieve%20real-time%20rendering%20of%0Anovel%20views.%20This%20survey%20aims%20to%20analyze%20existing%203DGS-related%20works%20from%0Amultiple%20intersecting%20perspectives%2C%20including%20related%20tasks%2C%20technologies%2C%0Achallenges%2C%20and%20opportunities.%20The%20primary%20objective%20is%20to%20provide%20newcomers%0Awith%20a%20rapid%20understanding%20of%20the%20field%20and%20to%20assist%20researchers%20in%0Amethodically%20organizing%20existing%20technologies%20and%20challenges.%20Specifically%2C%20we%0Adelve%20into%20the%20optimization%2C%20application%2C%20and%20extension%20of%203DGS%2C%20categorizing%0Athem%20based%20on%20their%20focuses%20or%20motivations.%20Additionally%2C%20we%20summarize%20and%0Aclassify%20nine%20types%20of%20technical%20modules%20and%20corresponding%20improvements%0Aidentified%20in%20existing%20works.%20Based%20on%20these%20analyses%2C%20we%20further%20examine%20the%0Acommon%20challenges%20and%20technologies%20across%20various%20tasks%2C%20proposing%20potential%0Aresearch%20opportunities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17418v1&entry.124074799=Read"},
{"title": "GaussianEditor: Editing 3D Gaussians Delicately with Text Instructions", "author": "Junjie Wang and Jiemin Fang and Xiaopeng Zhang and Lingxi Xie and Qi Tian", "abstract": "  Recently, impressive results have been achieved in 3D scene editing with text\ninstructions based on a 2D diffusion model. However, current diffusion models\nprimarily generate images by predicting noise in the latent space, and the\nediting is usually applied to the whole image, which makes it challenging to\nperform delicate, especially localized, editing for 3D scenes. Inspired by\nrecent 3D Gaussian splatting, we propose a systematic framework, named\nGaussianEditor, to edit 3D scenes delicately via 3D Gaussians with text\ninstructions. Benefiting from the explicit property of 3D Gaussians, we design\na series of techniques to achieve delicate editing. Specifically, we first\nextract the region of interest (RoI) corresponding to the text instruction,\naligning it to 3D Gaussians. The Gaussian RoI is further used to control the\nediting process. Our framework can achieve more delicate and precise editing of\n3D scenes than previous methods while enjoying much faster training speed, i.e.\nwithin 20 minutes on a single V100 GPU, more than twice as fast as\nInstruct-NeRF2NeRF (45 minutes -- 2 hours).\n", "link": "http://arxiv.org/abs/2311.16037v2", "date": "2024-07-24", "relevancy": 3.0103, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6271}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6041}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianEditor%3A%20Editing%203D%20Gaussians%20Delicately%20with%20Text%20Instructions&body=Title%3A%20GaussianEditor%3A%20Editing%203D%20Gaussians%20Delicately%20with%20Text%20Instructions%0AAuthor%3A%20Junjie%20Wang%20and%20Jiemin%20Fang%20and%20Xiaopeng%20Zhang%20and%20Lingxi%20Xie%20and%20Qi%20Tian%0AAbstract%3A%20%20%20Recently%2C%20impressive%20results%20have%20been%20achieved%20in%203D%20scene%20editing%20with%20text%0Ainstructions%20based%20on%20a%202D%20diffusion%20model.%20However%2C%20current%20diffusion%20models%0Aprimarily%20generate%20images%20by%20predicting%20noise%20in%20the%20latent%20space%2C%20and%20the%0Aediting%20is%20usually%20applied%20to%20the%20whole%20image%2C%20which%20makes%20it%20challenging%20to%0Aperform%20delicate%2C%20especially%20localized%2C%20editing%20for%203D%20scenes.%20Inspired%20by%0Arecent%203D%20Gaussian%20splatting%2C%20we%20propose%20a%20systematic%20framework%2C%20named%0AGaussianEditor%2C%20to%20edit%203D%20scenes%20delicately%20via%203D%20Gaussians%20with%20text%0Ainstructions.%20Benefiting%20from%20the%20explicit%20property%20of%203D%20Gaussians%2C%20we%20design%0Aa%20series%20of%20techniques%20to%20achieve%20delicate%20editing.%20Specifically%2C%20we%20first%0Aextract%20the%20region%20of%20interest%20%28RoI%29%20corresponding%20to%20the%20text%20instruction%2C%0Aaligning%20it%20to%203D%20Gaussians.%20The%20Gaussian%20RoI%20is%20further%20used%20to%20control%20the%0Aediting%20process.%20Our%20framework%20can%20achieve%20more%20delicate%20and%20precise%20editing%20of%0A3D%20scenes%20than%20previous%20methods%20while%20enjoying%20much%20faster%20training%20speed%2C%20i.e.%0Awithin%2020%20minutes%20on%20a%20single%20V100%20GPU%2C%20more%20than%20twice%20as%20fast%20as%0AInstruct-NeRF2NeRF%20%2845%20minutes%20--%202%20hours%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.16037v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianEditor%253A%2520Editing%25203D%2520Gaussians%2520Delicately%2520with%2520Text%2520Instructions%26entry.906535625%3DJunjie%2520Wang%2520and%2520Jiemin%2520Fang%2520and%2520Xiaopeng%2520Zhang%2520and%2520Lingxi%2520Xie%2520and%2520Qi%2520Tian%26entry.1292438233%3D%2520%2520Recently%252C%2520impressive%2520results%2520have%2520been%2520achieved%2520in%25203D%2520scene%2520editing%2520with%2520text%250Ainstructions%2520based%2520on%2520a%25202D%2520diffusion%2520model.%2520However%252C%2520current%2520diffusion%2520models%250Aprimarily%2520generate%2520images%2520by%2520predicting%2520noise%2520in%2520the%2520latent%2520space%252C%2520and%2520the%250Aediting%2520is%2520usually%2520applied%2520to%2520the%2520whole%2520image%252C%2520which%2520makes%2520it%2520challenging%2520to%250Aperform%2520delicate%252C%2520especially%2520localized%252C%2520editing%2520for%25203D%2520scenes.%2520Inspired%2520by%250Arecent%25203D%2520Gaussian%2520splatting%252C%2520we%2520propose%2520a%2520systematic%2520framework%252C%2520named%250AGaussianEditor%252C%2520to%2520edit%25203D%2520scenes%2520delicately%2520via%25203D%2520Gaussians%2520with%2520text%250Ainstructions.%2520Benefiting%2520from%2520the%2520explicit%2520property%2520of%25203D%2520Gaussians%252C%2520we%2520design%250Aa%2520series%2520of%2520techniques%2520to%2520achieve%2520delicate%2520editing.%2520Specifically%252C%2520we%2520first%250Aextract%2520the%2520region%2520of%2520interest%2520%2528RoI%2529%2520corresponding%2520to%2520the%2520text%2520instruction%252C%250Aaligning%2520it%2520to%25203D%2520Gaussians.%2520The%2520Gaussian%2520RoI%2520is%2520further%2520used%2520to%2520control%2520the%250Aediting%2520process.%2520Our%2520framework%2520can%2520achieve%2520more%2520delicate%2520and%2520precise%2520editing%2520of%250A3D%2520scenes%2520than%2520previous%2520methods%2520while%2520enjoying%2520much%2520faster%2520training%2520speed%252C%2520i.e.%250Awithin%252020%2520minutes%2520on%2520a%2520single%2520V100%2520GPU%252C%2520more%2520than%2520twice%2520as%2520fast%2520as%250AInstruct-NeRF2NeRF%2520%252845%2520minutes%2520--%25202%2520hours%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.16037v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianEditor%3A%20Editing%203D%20Gaussians%20Delicately%20with%20Text%20Instructions&entry.906535625=Junjie%20Wang%20and%20Jiemin%20Fang%20and%20Xiaopeng%20Zhang%20and%20Lingxi%20Xie%20and%20Qi%20Tian&entry.1292438233=%20%20Recently%2C%20impressive%20results%20have%20been%20achieved%20in%203D%20scene%20editing%20with%20text%0Ainstructions%20based%20on%20a%202D%20diffusion%20model.%20However%2C%20current%20diffusion%20models%0Aprimarily%20generate%20images%20by%20predicting%20noise%20in%20the%20latent%20space%2C%20and%20the%0Aediting%20is%20usually%20applied%20to%20the%20whole%20image%2C%20which%20makes%20it%20challenging%20to%0Aperform%20delicate%2C%20especially%20localized%2C%20editing%20for%203D%20scenes.%20Inspired%20by%0Arecent%203D%20Gaussian%20splatting%2C%20we%20propose%20a%20systematic%20framework%2C%20named%0AGaussianEditor%2C%20to%20edit%203D%20scenes%20delicately%20via%203D%20Gaussians%20with%20text%0Ainstructions.%20Benefiting%20from%20the%20explicit%20property%20of%203D%20Gaussians%2C%20we%20design%0Aa%20series%20of%20techniques%20to%20achieve%20delicate%20editing.%20Specifically%2C%20we%20first%0Aextract%20the%20region%20of%20interest%20%28RoI%29%20corresponding%20to%20the%20text%20instruction%2C%0Aaligning%20it%20to%203D%20Gaussians.%20The%20Gaussian%20RoI%20is%20further%20used%20to%20control%20the%0Aediting%20process.%20Our%20framework%20can%20achieve%20more%20delicate%20and%20precise%20editing%20of%0A3D%20scenes%20than%20previous%20methods%20while%20enjoying%20much%20faster%20training%20speed%2C%20i.e.%0Awithin%2020%20minutes%20on%20a%20single%20V100%20GPU%2C%20more%20than%20twice%20as%20fast%20as%0AInstruct-NeRF2NeRF%20%2845%20minutes%20--%202%20hours%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.16037v2&entry.124074799=Read"},
{"title": "ViPer: Visual Personalization of Generative Models via Individual\n  Preference Learning", "author": "Sogand Salehi and Mahdi Shafiei and Teresa Yeo and Roman Bachmann and Amir Zamir", "abstract": "  Different users find different images generated for the same prompt\ndesirable. This gives rise to personalized image generation which involves\ncreating images aligned with an individual's visual preference. Current\ngenerative models are, however, unpersonalized, as they are tuned to produce\noutputs that appeal to a broad audience. Using them to generate images aligned\nwith individual users relies on iterative manual prompt engineering by the user\nwhich is inefficient and undesirable. We propose to personalize the image\ngeneration process by first capturing the generic preferences of the user in a\none-time process by inviting them to comment on a small selection of images,\nexplaining why they like or dislike each. Based on these comments, we infer a\nuser's structured liked and disliked visual attributes, i.e., their visual\npreference, using a large language model. These attributes are used to guide a\ntext-to-image model toward producing images that are tuned towards the\nindividual user's visual preference. Through a series of user studies and large\nlanguage model guided evaluations, we demonstrate that the proposed method\nresults in generations that are well aligned with individual users' visual\npreferences.\n", "link": "http://arxiv.org/abs/2407.17365v1", "date": "2024-07-24", "relevancy": 2.936, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6051}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.594}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViPer%3A%20Visual%20Personalization%20of%20Generative%20Models%20via%20Individual%0A%20%20Preference%20Learning&body=Title%3A%20ViPer%3A%20Visual%20Personalization%20of%20Generative%20Models%20via%20Individual%0A%20%20Preference%20Learning%0AAuthor%3A%20Sogand%20Salehi%20and%20Mahdi%20Shafiei%20and%20Teresa%20Yeo%20and%20Roman%20Bachmann%20and%20Amir%20Zamir%0AAbstract%3A%20%20%20Different%20users%20find%20different%20images%20generated%20for%20the%20same%20prompt%0Adesirable.%20This%20gives%20rise%20to%20personalized%20image%20generation%20which%20involves%0Acreating%20images%20aligned%20with%20an%20individual%27s%20visual%20preference.%20Current%0Agenerative%20models%20are%2C%20however%2C%20unpersonalized%2C%20as%20they%20are%20tuned%20to%20produce%0Aoutputs%20that%20appeal%20to%20a%20broad%20audience.%20Using%20them%20to%20generate%20images%20aligned%0Awith%20individual%20users%20relies%20on%20iterative%20manual%20prompt%20engineering%20by%20the%20user%0Awhich%20is%20inefficient%20and%20undesirable.%20We%20propose%20to%20personalize%20the%20image%0Ageneration%20process%20by%20first%20capturing%20the%20generic%20preferences%20of%20the%20user%20in%20a%0Aone-time%20process%20by%20inviting%20them%20to%20comment%20on%20a%20small%20selection%20of%20images%2C%0Aexplaining%20why%20they%20like%20or%20dislike%20each.%20Based%20on%20these%20comments%2C%20we%20infer%20a%0Auser%27s%20structured%20liked%20and%20disliked%20visual%20attributes%2C%20i.e.%2C%20their%20visual%0Apreference%2C%20using%20a%20large%20language%20model.%20These%20attributes%20are%20used%20to%20guide%20a%0Atext-to-image%20model%20toward%20producing%20images%20that%20are%20tuned%20towards%20the%0Aindividual%20user%27s%20visual%20preference.%20Through%20a%20series%20of%20user%20studies%20and%20large%0Alanguage%20model%20guided%20evaluations%2C%20we%20demonstrate%20that%20the%20proposed%20method%0Aresults%20in%20generations%20that%20are%20well%20aligned%20with%20individual%20users%27%20visual%0Apreferences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17365v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViPer%253A%2520Visual%2520Personalization%2520of%2520Generative%2520Models%2520via%2520Individual%250A%2520%2520Preference%2520Learning%26entry.906535625%3DSogand%2520Salehi%2520and%2520Mahdi%2520Shafiei%2520and%2520Teresa%2520Yeo%2520and%2520Roman%2520Bachmann%2520and%2520Amir%2520Zamir%26entry.1292438233%3D%2520%2520Different%2520users%2520find%2520different%2520images%2520generated%2520for%2520the%2520same%2520prompt%250Adesirable.%2520This%2520gives%2520rise%2520to%2520personalized%2520image%2520generation%2520which%2520involves%250Acreating%2520images%2520aligned%2520with%2520an%2520individual%2527s%2520visual%2520preference.%2520Current%250Agenerative%2520models%2520are%252C%2520however%252C%2520unpersonalized%252C%2520as%2520they%2520are%2520tuned%2520to%2520produce%250Aoutputs%2520that%2520appeal%2520to%2520a%2520broad%2520audience.%2520Using%2520them%2520to%2520generate%2520images%2520aligned%250Awith%2520individual%2520users%2520relies%2520on%2520iterative%2520manual%2520prompt%2520engineering%2520by%2520the%2520user%250Awhich%2520is%2520inefficient%2520and%2520undesirable.%2520We%2520propose%2520to%2520personalize%2520the%2520image%250Ageneration%2520process%2520by%2520first%2520capturing%2520the%2520generic%2520preferences%2520of%2520the%2520user%2520in%2520a%250Aone-time%2520process%2520by%2520inviting%2520them%2520to%2520comment%2520on%2520a%2520small%2520selection%2520of%2520images%252C%250Aexplaining%2520why%2520they%2520like%2520or%2520dislike%2520each.%2520Based%2520on%2520these%2520comments%252C%2520we%2520infer%2520a%250Auser%2527s%2520structured%2520liked%2520and%2520disliked%2520visual%2520attributes%252C%2520i.e.%252C%2520their%2520visual%250Apreference%252C%2520using%2520a%2520large%2520language%2520model.%2520These%2520attributes%2520are%2520used%2520to%2520guide%2520a%250Atext-to-image%2520model%2520toward%2520producing%2520images%2520that%2520are%2520tuned%2520towards%2520the%250Aindividual%2520user%2527s%2520visual%2520preference.%2520Through%2520a%2520series%2520of%2520user%2520studies%2520and%2520large%250Alanguage%2520model%2520guided%2520evaluations%252C%2520we%2520demonstrate%2520that%2520the%2520proposed%2520method%250Aresults%2520in%2520generations%2520that%2520are%2520well%2520aligned%2520with%2520individual%2520users%2527%2520visual%250Apreferences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17365v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViPer%3A%20Visual%20Personalization%20of%20Generative%20Models%20via%20Individual%0A%20%20Preference%20Learning&entry.906535625=Sogand%20Salehi%20and%20Mahdi%20Shafiei%20and%20Teresa%20Yeo%20and%20Roman%20Bachmann%20and%20Amir%20Zamir&entry.1292438233=%20%20Different%20users%20find%20different%20images%20generated%20for%20the%20same%20prompt%0Adesirable.%20This%20gives%20rise%20to%20personalized%20image%20generation%20which%20involves%0Acreating%20images%20aligned%20with%20an%20individual%27s%20visual%20preference.%20Current%0Agenerative%20models%20are%2C%20however%2C%20unpersonalized%2C%20as%20they%20are%20tuned%20to%20produce%0Aoutputs%20that%20appeal%20to%20a%20broad%20audience.%20Using%20them%20to%20generate%20images%20aligned%0Awith%20individual%20users%20relies%20on%20iterative%20manual%20prompt%20engineering%20by%20the%20user%0Awhich%20is%20inefficient%20and%20undesirable.%20We%20propose%20to%20personalize%20the%20image%0Ageneration%20process%20by%20first%20capturing%20the%20generic%20preferences%20of%20the%20user%20in%20a%0Aone-time%20process%20by%20inviting%20them%20to%20comment%20on%20a%20small%20selection%20of%20images%2C%0Aexplaining%20why%20they%20like%20or%20dislike%20each.%20Based%20on%20these%20comments%2C%20we%20infer%20a%0Auser%27s%20structured%20liked%20and%20disliked%20visual%20attributes%2C%20i.e.%2C%20their%20visual%0Apreference%2C%20using%20a%20large%20language%20model.%20These%20attributes%20are%20used%20to%20guide%20a%0Atext-to-image%20model%20toward%20producing%20images%20that%20are%20tuned%20towards%20the%0Aindividual%20user%27s%20visual%20preference.%20Through%20a%20series%20of%20user%20studies%20and%20large%0Alanguage%20model%20guided%20evaluations%2C%20we%20demonstrate%20that%20the%20proposed%20method%0Aresults%20in%20generations%20that%20are%20well%20aligned%20with%20individual%20users%27%20visual%0Apreferences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17365v1&entry.124074799=Read"},
{"title": "LangOcc: Self-Supervised Open Vocabulary Occupancy Estimation via Volume\n  Rendering", "author": "Simon Boeder and Fabian Gigengack and Benjamin Risse", "abstract": "  Semantic occupancy has recently gained significant traction as a prominent\nmethod for 3D scene representation. However, most existing camera-based methods\nrely on costly datasets with fine-grained 3D voxel labels or LiDAR scans for\ntraining, which limits their practicality and scalability, raising the need for\nself-supervised approaches in this domain. Moreover, most methods are tied to a\npredefined set of classes which they can detect. In this work we present a\nnovel approach for open vocabulary occupancy estimation called\n\\textit{LangOcc}, that is trained only via camera images, and can detect\narbitrary semantics via vision-language alignment. In particular, we distill\nthe knowledge of the strong vision-language aligned encoder CLIP into a 3D\noccupancy model via differentiable volume rendering. Our model estimates\nvision-language aligned features in a 3D voxel grid using only images. It is\ntrained in a self-supervised manner by rendering our estimations back to 2D\nspace, where ground-truth features can be computed. This training mechanism\nautomatically supervises the scene geometry, allowing for a straight-forward\nand powerful training method without any explicit geometry supervision. LangOcc\noutperforms LiDAR-supervised competitors in open vocabulary occupancy by a\nlarge margin, solely relying on vision-based training. We also achieve\nstate-of-the-art results in self-supervised semantic occupancy estimation on\nthe Occ3D-nuScenes dataset, despite not being limited to a specific set of\ncategories, thus demonstrating the effectiveness of our proposed\nvision-language training.\n", "link": "http://arxiv.org/abs/2407.17310v1", "date": "2024-07-24", "relevancy": 2.9197, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.591}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5832}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LangOcc%3A%20Self-Supervised%20Open%20Vocabulary%20Occupancy%20Estimation%20via%20Volume%0A%20%20Rendering&body=Title%3A%20LangOcc%3A%20Self-Supervised%20Open%20Vocabulary%20Occupancy%20Estimation%20via%20Volume%0A%20%20Rendering%0AAuthor%3A%20Simon%20Boeder%20and%20Fabian%20Gigengack%20and%20Benjamin%20Risse%0AAbstract%3A%20%20%20Semantic%20occupancy%20has%20recently%20gained%20significant%20traction%20as%20a%20prominent%0Amethod%20for%203D%20scene%20representation.%20However%2C%20most%20existing%20camera-based%20methods%0Arely%20on%20costly%20datasets%20with%20fine-grained%203D%20voxel%20labels%20or%20LiDAR%20scans%20for%0Atraining%2C%20which%20limits%20their%20practicality%20and%20scalability%2C%20raising%20the%20need%20for%0Aself-supervised%20approaches%20in%20this%20domain.%20Moreover%2C%20most%20methods%20are%20tied%20to%20a%0Apredefined%20set%20of%20classes%20which%20they%20can%20detect.%20In%20this%20work%20we%20present%20a%0Anovel%20approach%20for%20open%20vocabulary%20occupancy%20estimation%20called%0A%5Ctextit%7BLangOcc%7D%2C%20that%20is%20trained%20only%20via%20camera%20images%2C%20and%20can%20detect%0Aarbitrary%20semantics%20via%20vision-language%20alignment.%20In%20particular%2C%20we%20distill%0Athe%20knowledge%20of%20the%20strong%20vision-language%20aligned%20encoder%20CLIP%20into%20a%203D%0Aoccupancy%20model%20via%20differentiable%20volume%20rendering.%20Our%20model%20estimates%0Avision-language%20aligned%20features%20in%20a%203D%20voxel%20grid%20using%20only%20images.%20It%20is%0Atrained%20in%20a%20self-supervised%20manner%20by%20rendering%20our%20estimations%20back%20to%202D%0Aspace%2C%20where%20ground-truth%20features%20can%20be%20computed.%20This%20training%20mechanism%0Aautomatically%20supervises%20the%20scene%20geometry%2C%20allowing%20for%20a%20straight-forward%0Aand%20powerful%20training%20method%20without%20any%20explicit%20geometry%20supervision.%20LangOcc%0Aoutperforms%20LiDAR-supervised%20competitors%20in%20open%20vocabulary%20occupancy%20by%20a%0Alarge%20margin%2C%20solely%20relying%20on%20vision-based%20training.%20We%20also%20achieve%0Astate-of-the-art%20results%20in%20self-supervised%20semantic%20occupancy%20estimation%20on%0Athe%20Occ3D-nuScenes%20dataset%2C%20despite%20not%20being%20limited%20to%20a%20specific%20set%20of%0Acategories%2C%20thus%20demonstrating%20the%20effectiveness%20of%20our%20proposed%0Avision-language%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17310v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLangOcc%253A%2520Self-Supervised%2520Open%2520Vocabulary%2520Occupancy%2520Estimation%2520via%2520Volume%250A%2520%2520Rendering%26entry.906535625%3DSimon%2520Boeder%2520and%2520Fabian%2520Gigengack%2520and%2520Benjamin%2520Risse%26entry.1292438233%3D%2520%2520Semantic%2520occupancy%2520has%2520recently%2520gained%2520significant%2520traction%2520as%2520a%2520prominent%250Amethod%2520for%25203D%2520scene%2520representation.%2520However%252C%2520most%2520existing%2520camera-based%2520methods%250Arely%2520on%2520costly%2520datasets%2520with%2520fine-grained%25203D%2520voxel%2520labels%2520or%2520LiDAR%2520scans%2520for%250Atraining%252C%2520which%2520limits%2520their%2520practicality%2520and%2520scalability%252C%2520raising%2520the%2520need%2520for%250Aself-supervised%2520approaches%2520in%2520this%2520domain.%2520Moreover%252C%2520most%2520methods%2520are%2520tied%2520to%2520a%250Apredefined%2520set%2520of%2520classes%2520which%2520they%2520can%2520detect.%2520In%2520this%2520work%2520we%2520present%2520a%250Anovel%2520approach%2520for%2520open%2520vocabulary%2520occupancy%2520estimation%2520called%250A%255Ctextit%257BLangOcc%257D%252C%2520that%2520is%2520trained%2520only%2520via%2520camera%2520images%252C%2520and%2520can%2520detect%250Aarbitrary%2520semantics%2520via%2520vision-language%2520alignment.%2520In%2520particular%252C%2520we%2520distill%250Athe%2520knowledge%2520of%2520the%2520strong%2520vision-language%2520aligned%2520encoder%2520CLIP%2520into%2520a%25203D%250Aoccupancy%2520model%2520via%2520differentiable%2520volume%2520rendering.%2520Our%2520model%2520estimates%250Avision-language%2520aligned%2520features%2520in%2520a%25203D%2520voxel%2520grid%2520using%2520only%2520images.%2520It%2520is%250Atrained%2520in%2520a%2520self-supervised%2520manner%2520by%2520rendering%2520our%2520estimations%2520back%2520to%25202D%250Aspace%252C%2520where%2520ground-truth%2520features%2520can%2520be%2520computed.%2520This%2520training%2520mechanism%250Aautomatically%2520supervises%2520the%2520scene%2520geometry%252C%2520allowing%2520for%2520a%2520straight-forward%250Aand%2520powerful%2520training%2520method%2520without%2520any%2520explicit%2520geometry%2520supervision.%2520LangOcc%250Aoutperforms%2520LiDAR-supervised%2520competitors%2520in%2520open%2520vocabulary%2520occupancy%2520by%2520a%250Alarge%2520margin%252C%2520solely%2520relying%2520on%2520vision-based%2520training.%2520We%2520also%2520achieve%250Astate-of-the-art%2520results%2520in%2520self-supervised%2520semantic%2520occupancy%2520estimation%2520on%250Athe%2520Occ3D-nuScenes%2520dataset%252C%2520despite%2520not%2520being%2520limited%2520to%2520a%2520specific%2520set%2520of%250Acategories%252C%2520thus%2520demonstrating%2520the%2520effectiveness%2520of%2520our%2520proposed%250Avision-language%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17310v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LangOcc%3A%20Self-Supervised%20Open%20Vocabulary%20Occupancy%20Estimation%20via%20Volume%0A%20%20Rendering&entry.906535625=Simon%20Boeder%20and%20Fabian%20Gigengack%20and%20Benjamin%20Risse&entry.1292438233=%20%20Semantic%20occupancy%20has%20recently%20gained%20significant%20traction%20as%20a%20prominent%0Amethod%20for%203D%20scene%20representation.%20However%2C%20most%20existing%20camera-based%20methods%0Arely%20on%20costly%20datasets%20with%20fine-grained%203D%20voxel%20labels%20or%20LiDAR%20scans%20for%0Atraining%2C%20which%20limits%20their%20practicality%20and%20scalability%2C%20raising%20the%20need%20for%0Aself-supervised%20approaches%20in%20this%20domain.%20Moreover%2C%20most%20methods%20are%20tied%20to%20a%0Apredefined%20set%20of%20classes%20which%20they%20can%20detect.%20In%20this%20work%20we%20present%20a%0Anovel%20approach%20for%20open%20vocabulary%20occupancy%20estimation%20called%0A%5Ctextit%7BLangOcc%7D%2C%20that%20is%20trained%20only%20via%20camera%20images%2C%20and%20can%20detect%0Aarbitrary%20semantics%20via%20vision-language%20alignment.%20In%20particular%2C%20we%20distill%0Athe%20knowledge%20of%20the%20strong%20vision-language%20aligned%20encoder%20CLIP%20into%20a%203D%0Aoccupancy%20model%20via%20differentiable%20volume%20rendering.%20Our%20model%20estimates%0Avision-language%20aligned%20features%20in%20a%203D%20voxel%20grid%20using%20only%20images.%20It%20is%0Atrained%20in%20a%20self-supervised%20manner%20by%20rendering%20our%20estimations%20back%20to%202D%0Aspace%2C%20where%20ground-truth%20features%20can%20be%20computed.%20This%20training%20mechanism%0Aautomatically%20supervises%20the%20scene%20geometry%2C%20allowing%20for%20a%20straight-forward%0Aand%20powerful%20training%20method%20without%20any%20explicit%20geometry%20supervision.%20LangOcc%0Aoutperforms%20LiDAR-supervised%20competitors%20in%20open%20vocabulary%20occupancy%20by%20a%0Alarge%20margin%2C%20solely%20relying%20on%20vision-based%20training.%20We%20also%20achieve%0Astate-of-the-art%20results%20in%20self-supervised%20semantic%20occupancy%20estimation%20on%0Athe%20Occ3D-nuScenes%20dataset%2C%20despite%20not%20being%20limited%20to%20a%20specific%20set%20of%0Acategories%2C%20thus%20demonstrating%20the%20effectiveness%20of%20our%20proposed%0Avision-language%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17310v1&entry.124074799=Read"},
{"title": "MetaCap: Meta-learning Priors from Multi-View Imagery for Sparse-view\n  Human Performance Capture and Rendering", "author": "Guoxing Sun and Rishabh Dabral and Pascal Fua and Christian Theobalt and Marc Habermann", "abstract": "  Faithful human performance capture and free-view rendering from sparse RGB\nobservations is a long-standing problem in Vision and Graphics. The main\nchallenges are the lack of observations and the inherent ambiguities of the\nsetting, e.g. occlusions and depth ambiguity. As a result, radiance fields,\nwhich have shown great promise in capturing high-frequency appearance and\ngeometry details in dense setups, perform poorly when naively supervising them\non sparse camera views, as the field simply overfits to the sparse-view inputs.\nTo address this, we propose MetaCap, a method for efficient and high-quality\ngeometry recovery and novel view synthesis given very sparse or even a single\nview of the human. Our key idea is to meta-learn the radiance field weights\nsolely from potentially sparse multi-view videos, which can serve as a prior\nwhen fine-tuning them on sparse imagery depicting the human. This prior\nprovides a good network weight initialization, thereby effectively addressing\nambiguities in sparse-view capture. Due to the articulated structure of the\nhuman body and motion-induced surface deformations, learning such a prior is\nnon-trivial. Therefore, we propose to meta-learn the field weights in a\npose-canonicalized space, which reduces the spatial feature range and makes\nfeature learning more effective. Consequently, one can fine-tune our field\nparameters to quickly generalize to unseen poses, novel illumination conditions\nas well as novel and sparse (even monocular) camera views. For evaluating our\nmethod under different scenarios, we collect a new dataset, WildDynaCap, which\ncontains subjects captured in, both, a dense camera dome and in-the-wild sparse\ncamera rigs, and demonstrate superior results compared to recent\nstate-of-the-art methods on, both, public and WildDynaCap dataset.\n", "link": "http://arxiv.org/abs/2403.18820v2", "date": "2024-07-24", "relevancy": 2.898, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5818}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5818}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MetaCap%3A%20Meta-learning%20Priors%20from%20Multi-View%20Imagery%20for%20Sparse-view%0A%20%20Human%20Performance%20Capture%20and%20Rendering&body=Title%3A%20MetaCap%3A%20Meta-learning%20Priors%20from%20Multi-View%20Imagery%20for%20Sparse-view%0A%20%20Human%20Performance%20Capture%20and%20Rendering%0AAuthor%3A%20Guoxing%20Sun%20and%20Rishabh%20Dabral%20and%20Pascal%20Fua%20and%20Christian%20Theobalt%20and%20Marc%20Habermann%0AAbstract%3A%20%20%20Faithful%20human%20performance%20capture%20and%20free-view%20rendering%20from%20sparse%20RGB%0Aobservations%20is%20a%20long-standing%20problem%20in%20Vision%20and%20Graphics.%20The%20main%0Achallenges%20are%20the%20lack%20of%20observations%20and%20the%20inherent%20ambiguities%20of%20the%0Asetting%2C%20e.g.%20occlusions%20and%20depth%20ambiguity.%20As%20a%20result%2C%20radiance%20fields%2C%0Awhich%20have%20shown%20great%20promise%20in%20capturing%20high-frequency%20appearance%20and%0Ageometry%20details%20in%20dense%20setups%2C%20perform%20poorly%20when%20naively%20supervising%20them%0Aon%20sparse%20camera%20views%2C%20as%20the%20field%20simply%20overfits%20to%20the%20sparse-view%20inputs.%0ATo%20address%20this%2C%20we%20propose%20MetaCap%2C%20a%20method%20for%20efficient%20and%20high-quality%0Ageometry%20recovery%20and%20novel%20view%20synthesis%20given%20very%20sparse%20or%20even%20a%20single%0Aview%20of%20the%20human.%20Our%20key%20idea%20is%20to%20meta-learn%20the%20radiance%20field%20weights%0Asolely%20from%20potentially%20sparse%20multi-view%20videos%2C%20which%20can%20serve%20as%20a%20prior%0Awhen%20fine-tuning%20them%20on%20sparse%20imagery%20depicting%20the%20human.%20This%20prior%0Aprovides%20a%20good%20network%20weight%20initialization%2C%20thereby%20effectively%20addressing%0Aambiguities%20in%20sparse-view%20capture.%20Due%20to%20the%20articulated%20structure%20of%20the%0Ahuman%20body%20and%20motion-induced%20surface%20deformations%2C%20learning%20such%20a%20prior%20is%0Anon-trivial.%20Therefore%2C%20we%20propose%20to%20meta-learn%20the%20field%20weights%20in%20a%0Apose-canonicalized%20space%2C%20which%20reduces%20the%20spatial%20feature%20range%20and%20makes%0Afeature%20learning%20more%20effective.%20Consequently%2C%20one%20can%20fine-tune%20our%20field%0Aparameters%20to%20quickly%20generalize%20to%20unseen%20poses%2C%20novel%20illumination%20conditions%0Aas%20well%20as%20novel%20and%20sparse%20%28even%20monocular%29%20camera%20views.%20For%20evaluating%20our%0Amethod%20under%20different%20scenarios%2C%20we%20collect%20a%20new%20dataset%2C%20WildDynaCap%2C%20which%0Acontains%20subjects%20captured%20in%2C%20both%2C%20a%20dense%20camera%20dome%20and%20in-the-wild%20sparse%0Acamera%20rigs%2C%20and%20demonstrate%20superior%20results%20compared%20to%20recent%0Astate-of-the-art%20methods%20on%2C%20both%2C%20public%20and%20WildDynaCap%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18820v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetaCap%253A%2520Meta-learning%2520Priors%2520from%2520Multi-View%2520Imagery%2520for%2520Sparse-view%250A%2520%2520Human%2520Performance%2520Capture%2520and%2520Rendering%26entry.906535625%3DGuoxing%2520Sun%2520and%2520Rishabh%2520Dabral%2520and%2520Pascal%2520Fua%2520and%2520Christian%2520Theobalt%2520and%2520Marc%2520Habermann%26entry.1292438233%3D%2520%2520Faithful%2520human%2520performance%2520capture%2520and%2520free-view%2520rendering%2520from%2520sparse%2520RGB%250Aobservations%2520is%2520a%2520long-standing%2520problem%2520in%2520Vision%2520and%2520Graphics.%2520The%2520main%250Achallenges%2520are%2520the%2520lack%2520of%2520observations%2520and%2520the%2520inherent%2520ambiguities%2520of%2520the%250Asetting%252C%2520e.g.%2520occlusions%2520and%2520depth%2520ambiguity.%2520As%2520a%2520result%252C%2520radiance%2520fields%252C%250Awhich%2520have%2520shown%2520great%2520promise%2520in%2520capturing%2520high-frequency%2520appearance%2520and%250Ageometry%2520details%2520in%2520dense%2520setups%252C%2520perform%2520poorly%2520when%2520naively%2520supervising%2520them%250Aon%2520sparse%2520camera%2520views%252C%2520as%2520the%2520field%2520simply%2520overfits%2520to%2520the%2520sparse-view%2520inputs.%250ATo%2520address%2520this%252C%2520we%2520propose%2520MetaCap%252C%2520a%2520method%2520for%2520efficient%2520and%2520high-quality%250Ageometry%2520recovery%2520and%2520novel%2520view%2520synthesis%2520given%2520very%2520sparse%2520or%2520even%2520a%2520single%250Aview%2520of%2520the%2520human.%2520Our%2520key%2520idea%2520is%2520to%2520meta-learn%2520the%2520radiance%2520field%2520weights%250Asolely%2520from%2520potentially%2520sparse%2520multi-view%2520videos%252C%2520which%2520can%2520serve%2520as%2520a%2520prior%250Awhen%2520fine-tuning%2520them%2520on%2520sparse%2520imagery%2520depicting%2520the%2520human.%2520This%2520prior%250Aprovides%2520a%2520good%2520network%2520weight%2520initialization%252C%2520thereby%2520effectively%2520addressing%250Aambiguities%2520in%2520sparse-view%2520capture.%2520Due%2520to%2520the%2520articulated%2520structure%2520of%2520the%250Ahuman%2520body%2520and%2520motion-induced%2520surface%2520deformations%252C%2520learning%2520such%2520a%2520prior%2520is%250Anon-trivial.%2520Therefore%252C%2520we%2520propose%2520to%2520meta-learn%2520the%2520field%2520weights%2520in%2520a%250Apose-canonicalized%2520space%252C%2520which%2520reduces%2520the%2520spatial%2520feature%2520range%2520and%2520makes%250Afeature%2520learning%2520more%2520effective.%2520Consequently%252C%2520one%2520can%2520fine-tune%2520our%2520field%250Aparameters%2520to%2520quickly%2520generalize%2520to%2520unseen%2520poses%252C%2520novel%2520illumination%2520conditions%250Aas%2520well%2520as%2520novel%2520and%2520sparse%2520%2528even%2520monocular%2529%2520camera%2520views.%2520For%2520evaluating%2520our%250Amethod%2520under%2520different%2520scenarios%252C%2520we%2520collect%2520a%2520new%2520dataset%252C%2520WildDynaCap%252C%2520which%250Acontains%2520subjects%2520captured%2520in%252C%2520both%252C%2520a%2520dense%2520camera%2520dome%2520and%2520in-the-wild%2520sparse%250Acamera%2520rigs%252C%2520and%2520demonstrate%2520superior%2520results%2520compared%2520to%2520recent%250Astate-of-the-art%2520methods%2520on%252C%2520both%252C%2520public%2520and%2520WildDynaCap%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.18820v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetaCap%3A%20Meta-learning%20Priors%20from%20Multi-View%20Imagery%20for%20Sparse-view%0A%20%20Human%20Performance%20Capture%20and%20Rendering&entry.906535625=Guoxing%20Sun%20and%20Rishabh%20Dabral%20and%20Pascal%20Fua%20and%20Christian%20Theobalt%20and%20Marc%20Habermann&entry.1292438233=%20%20Faithful%20human%20performance%20capture%20and%20free-view%20rendering%20from%20sparse%20RGB%0Aobservations%20is%20a%20long-standing%20problem%20in%20Vision%20and%20Graphics.%20The%20main%0Achallenges%20are%20the%20lack%20of%20observations%20and%20the%20inherent%20ambiguities%20of%20the%0Asetting%2C%20e.g.%20occlusions%20and%20depth%20ambiguity.%20As%20a%20result%2C%20radiance%20fields%2C%0Awhich%20have%20shown%20great%20promise%20in%20capturing%20high-frequency%20appearance%20and%0Ageometry%20details%20in%20dense%20setups%2C%20perform%20poorly%20when%20naively%20supervising%20them%0Aon%20sparse%20camera%20views%2C%20as%20the%20field%20simply%20overfits%20to%20the%20sparse-view%20inputs.%0ATo%20address%20this%2C%20we%20propose%20MetaCap%2C%20a%20method%20for%20efficient%20and%20high-quality%0Ageometry%20recovery%20and%20novel%20view%20synthesis%20given%20very%20sparse%20or%20even%20a%20single%0Aview%20of%20the%20human.%20Our%20key%20idea%20is%20to%20meta-learn%20the%20radiance%20field%20weights%0Asolely%20from%20potentially%20sparse%20multi-view%20videos%2C%20which%20can%20serve%20as%20a%20prior%0Awhen%20fine-tuning%20them%20on%20sparse%20imagery%20depicting%20the%20human.%20This%20prior%0Aprovides%20a%20good%20network%20weight%20initialization%2C%20thereby%20effectively%20addressing%0Aambiguities%20in%20sparse-view%20capture.%20Due%20to%20the%20articulated%20structure%20of%20the%0Ahuman%20body%20and%20motion-induced%20surface%20deformations%2C%20learning%20such%20a%20prior%20is%0Anon-trivial.%20Therefore%2C%20we%20propose%20to%20meta-learn%20the%20field%20weights%20in%20a%0Apose-canonicalized%20space%2C%20which%20reduces%20the%20spatial%20feature%20range%20and%20makes%0Afeature%20learning%20more%20effective.%20Consequently%2C%20one%20can%20fine-tune%20our%20field%0Aparameters%20to%20quickly%20generalize%20to%20unseen%20poses%2C%20novel%20illumination%20conditions%0Aas%20well%20as%20novel%20and%20sparse%20%28even%20monocular%29%20camera%20views.%20For%20evaluating%20our%0Amethod%20under%20different%20scenarios%2C%20we%20collect%20a%20new%20dataset%2C%20WildDynaCap%2C%20which%0Acontains%20subjects%20captured%20in%2C%20both%2C%20a%20dense%20camera%20dome%20and%20in-the-wild%20sparse%0Acamera%20rigs%2C%20and%20demonstrate%20superior%20results%20compared%20to%20recent%0Astate-of-the-art%20methods%20on%2C%20both%2C%20public%20and%20WildDynaCap%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18820v2&entry.124074799=Read"},
{"title": "MeshVPR: Citywide Visual Place Recognition Using 3D Meshes", "author": "Gabriele Berton and Lorenz Junglas and Riccardo Zaccone and Thomas Pollok and Barbara Caputo and Carlo Masone", "abstract": "  Mesh-based scene representation offers a promising direction for simplifying\nlarge-scale hierarchical visual localization pipelines, combining a visual\nplace recognition step based on global features (retrieval) and a visual\nlocalization step based on local features. While existing work demonstrates the\nviability of meshes for visual localization, the impact of using synthetic\ndatabases rendered from them in visual place recognition remains largely\nunexplored. In this work we investigate using dense 3D textured meshes for\nlarge-scale Visual Place Recognition (VPR). We identify a significant\nperformance drop when using synthetic mesh-based image databases compared to\nreal-world images for retrieval. To address this, we propose MeshVPR, a novel\nVPR pipeline that utilizes a lightweight features alignment framework to bridge\nthe gap between real-world and synthetic domains. MeshVPR leverages pre-trained\nVPR models and is efficient and scalable for city-wide deployments. We\nintroduce novel datasets with freely available 3D meshes and manually collected\nqueries from Berlin, Paris, and Melbourne. Extensive evaluations demonstrate\nthat MeshVPR achieves competitive performance with standard VPR pipelines,\npaving the way for mesh-based localization systems. Data, code, and interactive\nvisualizations are available at https://meshvpr.github.io/\n", "link": "http://arxiv.org/abs/2406.02776v2", "date": "2024-07-24", "relevancy": 2.7751, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5841}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5405}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MeshVPR%3A%20Citywide%20Visual%20Place%20Recognition%20Using%203D%20Meshes&body=Title%3A%20MeshVPR%3A%20Citywide%20Visual%20Place%20Recognition%20Using%203D%20Meshes%0AAuthor%3A%20Gabriele%20Berton%20and%20Lorenz%20Junglas%20and%20Riccardo%20Zaccone%20and%20Thomas%20Pollok%20and%20Barbara%20Caputo%20and%20Carlo%20Masone%0AAbstract%3A%20%20%20Mesh-based%20scene%20representation%20offers%20a%20promising%20direction%20for%20simplifying%0Alarge-scale%20hierarchical%20visual%20localization%20pipelines%2C%20combining%20a%20visual%0Aplace%20recognition%20step%20based%20on%20global%20features%20%28retrieval%29%20and%20a%20visual%0Alocalization%20step%20based%20on%20local%20features.%20While%20existing%20work%20demonstrates%20the%0Aviability%20of%20meshes%20for%20visual%20localization%2C%20the%20impact%20of%20using%20synthetic%0Adatabases%20rendered%20from%20them%20in%20visual%20place%20recognition%20remains%20largely%0Aunexplored.%20In%20this%20work%20we%20investigate%20using%20dense%203D%20textured%20meshes%20for%0Alarge-scale%20Visual%20Place%20Recognition%20%28VPR%29.%20We%20identify%20a%20significant%0Aperformance%20drop%20when%20using%20synthetic%20mesh-based%20image%20databases%20compared%20to%0Areal-world%20images%20for%20retrieval.%20To%20address%20this%2C%20we%20propose%20MeshVPR%2C%20a%20novel%0AVPR%20pipeline%20that%20utilizes%20a%20lightweight%20features%20alignment%20framework%20to%20bridge%0Athe%20gap%20between%20real-world%20and%20synthetic%20domains.%20MeshVPR%20leverages%20pre-trained%0AVPR%20models%20and%20is%20efficient%20and%20scalable%20for%20city-wide%20deployments.%20We%0Aintroduce%20novel%20datasets%20with%20freely%20available%203D%20meshes%20and%20manually%20collected%0Aqueries%20from%20Berlin%2C%20Paris%2C%20and%20Melbourne.%20Extensive%20evaluations%20demonstrate%0Athat%20MeshVPR%20achieves%20competitive%20performance%20with%20standard%20VPR%20pipelines%2C%0Apaving%20the%20way%20for%20mesh-based%20localization%20systems.%20Data%2C%20code%2C%20and%20interactive%0Avisualizations%20are%20available%20at%20https%3A//meshvpr.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02776v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeshVPR%253A%2520Citywide%2520Visual%2520Place%2520Recognition%2520Using%25203D%2520Meshes%26entry.906535625%3DGabriele%2520Berton%2520and%2520Lorenz%2520Junglas%2520and%2520Riccardo%2520Zaccone%2520and%2520Thomas%2520Pollok%2520and%2520Barbara%2520Caputo%2520and%2520Carlo%2520Masone%26entry.1292438233%3D%2520%2520Mesh-based%2520scene%2520representation%2520offers%2520a%2520promising%2520direction%2520for%2520simplifying%250Alarge-scale%2520hierarchical%2520visual%2520localization%2520pipelines%252C%2520combining%2520a%2520visual%250Aplace%2520recognition%2520step%2520based%2520on%2520global%2520features%2520%2528retrieval%2529%2520and%2520a%2520visual%250Alocalization%2520step%2520based%2520on%2520local%2520features.%2520While%2520existing%2520work%2520demonstrates%2520the%250Aviability%2520of%2520meshes%2520for%2520visual%2520localization%252C%2520the%2520impact%2520of%2520using%2520synthetic%250Adatabases%2520rendered%2520from%2520them%2520in%2520visual%2520place%2520recognition%2520remains%2520largely%250Aunexplored.%2520In%2520this%2520work%2520we%2520investigate%2520using%2520dense%25203D%2520textured%2520meshes%2520for%250Alarge-scale%2520Visual%2520Place%2520Recognition%2520%2528VPR%2529.%2520We%2520identify%2520a%2520significant%250Aperformance%2520drop%2520when%2520using%2520synthetic%2520mesh-based%2520image%2520databases%2520compared%2520to%250Areal-world%2520images%2520for%2520retrieval.%2520To%2520address%2520this%252C%2520we%2520propose%2520MeshVPR%252C%2520a%2520novel%250AVPR%2520pipeline%2520that%2520utilizes%2520a%2520lightweight%2520features%2520alignment%2520framework%2520to%2520bridge%250Athe%2520gap%2520between%2520real-world%2520and%2520synthetic%2520domains.%2520MeshVPR%2520leverages%2520pre-trained%250AVPR%2520models%2520and%2520is%2520efficient%2520and%2520scalable%2520for%2520city-wide%2520deployments.%2520We%250Aintroduce%2520novel%2520datasets%2520with%2520freely%2520available%25203D%2520meshes%2520and%2520manually%2520collected%250Aqueries%2520from%2520Berlin%252C%2520Paris%252C%2520and%2520Melbourne.%2520Extensive%2520evaluations%2520demonstrate%250Athat%2520MeshVPR%2520achieves%2520competitive%2520performance%2520with%2520standard%2520VPR%2520pipelines%252C%250Apaving%2520the%2520way%2520for%2520mesh-based%2520localization%2520systems.%2520Data%252C%2520code%252C%2520and%2520interactive%250Avisualizations%2520are%2520available%2520at%2520https%253A//meshvpr.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02776v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MeshVPR%3A%20Citywide%20Visual%20Place%20Recognition%20Using%203D%20Meshes&entry.906535625=Gabriele%20Berton%20and%20Lorenz%20Junglas%20and%20Riccardo%20Zaccone%20and%20Thomas%20Pollok%20and%20Barbara%20Caputo%20and%20Carlo%20Masone&entry.1292438233=%20%20Mesh-based%20scene%20representation%20offers%20a%20promising%20direction%20for%20simplifying%0Alarge-scale%20hierarchical%20visual%20localization%20pipelines%2C%20combining%20a%20visual%0Aplace%20recognition%20step%20based%20on%20global%20features%20%28retrieval%29%20and%20a%20visual%0Alocalization%20step%20based%20on%20local%20features.%20While%20existing%20work%20demonstrates%20the%0Aviability%20of%20meshes%20for%20visual%20localization%2C%20the%20impact%20of%20using%20synthetic%0Adatabases%20rendered%20from%20them%20in%20visual%20place%20recognition%20remains%20largely%0Aunexplored.%20In%20this%20work%20we%20investigate%20using%20dense%203D%20textured%20meshes%20for%0Alarge-scale%20Visual%20Place%20Recognition%20%28VPR%29.%20We%20identify%20a%20significant%0Aperformance%20drop%20when%20using%20synthetic%20mesh-based%20image%20databases%20compared%20to%0Areal-world%20images%20for%20retrieval.%20To%20address%20this%2C%20we%20propose%20MeshVPR%2C%20a%20novel%0AVPR%20pipeline%20that%20utilizes%20a%20lightweight%20features%20alignment%20framework%20to%20bridge%0Athe%20gap%20between%20real-world%20and%20synthetic%20domains.%20MeshVPR%20leverages%20pre-trained%0AVPR%20models%20and%20is%20efficient%20and%20scalable%20for%20city-wide%20deployments.%20We%0Aintroduce%20novel%20datasets%20with%20freely%20available%203D%20meshes%20and%20manually%20collected%0Aqueries%20from%20Berlin%2C%20Paris%2C%20and%20Melbourne.%20Extensive%20evaluations%20demonstrate%0Athat%20MeshVPR%20achieves%20competitive%20performance%20with%20standard%20VPR%20pipelines%2C%0Apaving%20the%20way%20for%20mesh-based%20localization%20systems.%20Data%2C%20code%2C%20and%20interactive%0Avisualizations%20are%20available%20at%20https%3A//meshvpr.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02776v2&entry.124074799=Read"},
{"title": "Multi-HMR: Multi-Person Whole-Body Human Mesh Recovery in a Single Shot", "author": "Fabien Baradel and Matthieu Armando and Salma Galaaoui and Romain Br\u00e9gier and Philippe Weinzaepfel and Gr\u00e9gory Rogez and Thomas Lucas", "abstract": "  We present Multi-HMR, a strong sigle-shot model for multi-person 3D human\nmesh recovery from a single RGB image. Predictions encompass the whole body,\ni.e., including hands and facial expressions, using the SMPL-X parametric model\nand 3D location in the camera coordinate system. Our model detects people by\npredicting coarse 2D heatmaps of person locations, using features produced by a\nstandard Vision Transformer (ViT) backbone. It then predicts their whole-body\npose, shape and 3D location using a new cross-attention module called the Human\nPrediction Head (HPH), with one query attending to the entire set of features\nfor each detected person. As direct prediction of fine-grained hands and facial\nposes in a single shot, i.e., without relying on explicit crops around body\nparts, is hard to learn from existing data, we introduce CUFFS, the Close-Up\nFrames of Full-Body Subjects dataset, containing humans close to the camera\nwith diverse hand poses. We show that incorporating it into the training data\nfurther enhances predictions, particularly for hands. Multi-HMR also optionally\naccounts for camera intrinsics, if available, by encoding camera ray directions\nfor each image token. This simple design achieves strong performance on\nwhole-body and body-only benchmarks simultaneously: a ViT-S backbone on\n$448{\\times}448$ images already yields a fast and competitive model, while\nlarger models and higher resolutions obtain state-of-the-art results.\n", "link": "http://arxiv.org/abs/2402.14654v2", "date": "2024-07-24", "relevancy": 2.7568, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5605}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5482}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-HMR%3A%20Multi-Person%20Whole-Body%20Human%20Mesh%20Recovery%20in%20a%20Single%20Shot&body=Title%3A%20Multi-HMR%3A%20Multi-Person%20Whole-Body%20Human%20Mesh%20Recovery%20in%20a%20Single%20Shot%0AAuthor%3A%20Fabien%20Baradel%20and%20Matthieu%20Armando%20and%20Salma%20Galaaoui%20and%20Romain%20Br%C3%A9gier%20and%20Philippe%20Weinzaepfel%20and%20Gr%C3%A9gory%20Rogez%20and%20Thomas%20Lucas%0AAbstract%3A%20%20%20We%20present%20Multi-HMR%2C%20a%20strong%20sigle-shot%20model%20for%20multi-person%203D%20human%0Amesh%20recovery%20from%20a%20single%20RGB%20image.%20Predictions%20encompass%20the%20whole%20body%2C%0Ai.e.%2C%20including%20hands%20and%20facial%20expressions%2C%20using%20the%20SMPL-X%20parametric%20model%0Aand%203D%20location%20in%20the%20camera%20coordinate%20system.%20Our%20model%20detects%20people%20by%0Apredicting%20coarse%202D%20heatmaps%20of%20person%20locations%2C%20using%20features%20produced%20by%20a%0Astandard%20Vision%20Transformer%20%28ViT%29%20backbone.%20It%20then%20predicts%20their%20whole-body%0Apose%2C%20shape%20and%203D%20location%20using%20a%20new%20cross-attention%20module%20called%20the%20Human%0APrediction%20Head%20%28HPH%29%2C%20with%20one%20query%20attending%20to%20the%20entire%20set%20of%20features%0Afor%20each%20detected%20person.%20As%20direct%20prediction%20of%20fine-grained%20hands%20and%20facial%0Aposes%20in%20a%20single%20shot%2C%20i.e.%2C%20without%20relying%20on%20explicit%20crops%20around%20body%0Aparts%2C%20is%20hard%20to%20learn%20from%20existing%20data%2C%20we%20introduce%20CUFFS%2C%20the%20Close-Up%0AFrames%20of%20Full-Body%20Subjects%20dataset%2C%20containing%20humans%20close%20to%20the%20camera%0Awith%20diverse%20hand%20poses.%20We%20show%20that%20incorporating%20it%20into%20the%20training%20data%0Afurther%20enhances%20predictions%2C%20particularly%20for%20hands.%20Multi-HMR%20also%20optionally%0Aaccounts%20for%20camera%20intrinsics%2C%20if%20available%2C%20by%20encoding%20camera%20ray%20directions%0Afor%20each%20image%20token.%20This%20simple%20design%20achieves%20strong%20performance%20on%0Awhole-body%20and%20body-only%20benchmarks%20simultaneously%3A%20a%20ViT-S%20backbone%20on%0A%24448%7B%5Ctimes%7D448%24%20images%20already%20yields%20a%20fast%20and%20competitive%20model%2C%20while%0Alarger%20models%20and%20higher%20resolutions%20obtain%20state-of-the-art%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14654v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-HMR%253A%2520Multi-Person%2520Whole-Body%2520Human%2520Mesh%2520Recovery%2520in%2520a%2520Single%2520Shot%26entry.906535625%3DFabien%2520Baradel%2520and%2520Matthieu%2520Armando%2520and%2520Salma%2520Galaaoui%2520and%2520Romain%2520Br%25C3%25A9gier%2520and%2520Philippe%2520Weinzaepfel%2520and%2520Gr%25C3%25A9gory%2520Rogez%2520and%2520Thomas%2520Lucas%26entry.1292438233%3D%2520%2520We%2520present%2520Multi-HMR%252C%2520a%2520strong%2520sigle-shot%2520model%2520for%2520multi-person%25203D%2520human%250Amesh%2520recovery%2520from%2520a%2520single%2520RGB%2520image.%2520Predictions%2520encompass%2520the%2520whole%2520body%252C%250Ai.e.%252C%2520including%2520hands%2520and%2520facial%2520expressions%252C%2520using%2520the%2520SMPL-X%2520parametric%2520model%250Aand%25203D%2520location%2520in%2520the%2520camera%2520coordinate%2520system.%2520Our%2520model%2520detects%2520people%2520by%250Apredicting%2520coarse%25202D%2520heatmaps%2520of%2520person%2520locations%252C%2520using%2520features%2520produced%2520by%2520a%250Astandard%2520Vision%2520Transformer%2520%2528ViT%2529%2520backbone.%2520It%2520then%2520predicts%2520their%2520whole-body%250Apose%252C%2520shape%2520and%25203D%2520location%2520using%2520a%2520new%2520cross-attention%2520module%2520called%2520the%2520Human%250APrediction%2520Head%2520%2528HPH%2529%252C%2520with%2520one%2520query%2520attending%2520to%2520the%2520entire%2520set%2520of%2520features%250Afor%2520each%2520detected%2520person.%2520As%2520direct%2520prediction%2520of%2520fine-grained%2520hands%2520and%2520facial%250Aposes%2520in%2520a%2520single%2520shot%252C%2520i.e.%252C%2520without%2520relying%2520on%2520explicit%2520crops%2520around%2520body%250Aparts%252C%2520is%2520hard%2520to%2520learn%2520from%2520existing%2520data%252C%2520we%2520introduce%2520CUFFS%252C%2520the%2520Close-Up%250AFrames%2520of%2520Full-Body%2520Subjects%2520dataset%252C%2520containing%2520humans%2520close%2520to%2520the%2520camera%250Awith%2520diverse%2520hand%2520poses.%2520We%2520show%2520that%2520incorporating%2520it%2520into%2520the%2520training%2520data%250Afurther%2520enhances%2520predictions%252C%2520particularly%2520for%2520hands.%2520Multi-HMR%2520also%2520optionally%250Aaccounts%2520for%2520camera%2520intrinsics%252C%2520if%2520available%252C%2520by%2520encoding%2520camera%2520ray%2520directions%250Afor%2520each%2520image%2520token.%2520This%2520simple%2520design%2520achieves%2520strong%2520performance%2520on%250Awhole-body%2520and%2520body-only%2520benchmarks%2520simultaneously%253A%2520a%2520ViT-S%2520backbone%2520on%250A%2524448%257B%255Ctimes%257D448%2524%2520images%2520already%2520yields%2520a%2520fast%2520and%2520competitive%2520model%252C%2520while%250Alarger%2520models%2520and%2520higher%2520resolutions%2520obtain%2520state-of-the-art%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14654v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-HMR%3A%20Multi-Person%20Whole-Body%20Human%20Mesh%20Recovery%20in%20a%20Single%20Shot&entry.906535625=Fabien%20Baradel%20and%20Matthieu%20Armando%20and%20Salma%20Galaaoui%20and%20Romain%20Br%C3%A9gier%20and%20Philippe%20Weinzaepfel%20and%20Gr%C3%A9gory%20Rogez%20and%20Thomas%20Lucas&entry.1292438233=%20%20We%20present%20Multi-HMR%2C%20a%20strong%20sigle-shot%20model%20for%20multi-person%203D%20human%0Amesh%20recovery%20from%20a%20single%20RGB%20image.%20Predictions%20encompass%20the%20whole%20body%2C%0Ai.e.%2C%20including%20hands%20and%20facial%20expressions%2C%20using%20the%20SMPL-X%20parametric%20model%0Aand%203D%20location%20in%20the%20camera%20coordinate%20system.%20Our%20model%20detects%20people%20by%0Apredicting%20coarse%202D%20heatmaps%20of%20person%20locations%2C%20using%20features%20produced%20by%20a%0Astandard%20Vision%20Transformer%20%28ViT%29%20backbone.%20It%20then%20predicts%20their%20whole-body%0Apose%2C%20shape%20and%203D%20location%20using%20a%20new%20cross-attention%20module%20called%20the%20Human%0APrediction%20Head%20%28HPH%29%2C%20with%20one%20query%20attending%20to%20the%20entire%20set%20of%20features%0Afor%20each%20detected%20person.%20As%20direct%20prediction%20of%20fine-grained%20hands%20and%20facial%0Aposes%20in%20a%20single%20shot%2C%20i.e.%2C%20without%20relying%20on%20explicit%20crops%20around%20body%0Aparts%2C%20is%20hard%20to%20learn%20from%20existing%20data%2C%20we%20introduce%20CUFFS%2C%20the%20Close-Up%0AFrames%20of%20Full-Body%20Subjects%20dataset%2C%20containing%20humans%20close%20to%20the%20camera%0Awith%20diverse%20hand%20poses.%20We%20show%20that%20incorporating%20it%20into%20the%20training%20data%0Afurther%20enhances%20predictions%2C%20particularly%20for%20hands.%20Multi-HMR%20also%20optionally%0Aaccounts%20for%20camera%20intrinsics%2C%20if%20available%2C%20by%20encoding%20camera%20ray%20directions%0Afor%20each%20image%20token.%20This%20simple%20design%20achieves%20strong%20performance%20on%0Awhole-body%20and%20body-only%20benchmarks%20simultaneously%3A%20a%20ViT-S%20backbone%20on%0A%24448%7B%5Ctimes%7D448%24%20images%20already%20yields%20a%20fast%20and%20competitive%20model%2C%20while%0Alarger%20models%20and%20higher%20resolutions%20obtain%20state-of-the-art%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14654v2&entry.124074799=Read"},
{"title": "Deep Spherical Superpixels", "author": "R\u00e9mi Giraud and Micha\u00ebl Cl\u00e9ment", "abstract": "  Over the years, the use of superpixel segmentation has become very popular in\nvarious applications, serving as a preprocessing step to reduce data size by\nadapting to the content of the image, regardless of its semantic content. While\nthe superpixel segmentation of standard planar images, captured with a 90{\\deg}\nfield of view, has been extensively studied, there has been limited focus on\ndedicated methods to omnidirectional or spherical images, captured with a\n360{\\deg} field of view. In this study, we introduce the first deep\nlearning-based superpixel segmentation approach tailored for omnidirectional\nimages called DSS (for Deep Spherical Superpixels). Our methodology leverages\non spherical CNN architectures and the differentiable K-means clustering\nparadigm for superpixels, to generate superpixels that follow the spherical\ngeometry. Additionally, we propose to use data augmentation techniques\nspecifically designed for 360{\\deg} images, enabling our model to efficiently\nlearn from a limited set of annotated omnidirectional data. Our extensive\nvalidation across two datasets demonstrates that taking into account the\ninherent circular geometry of such images into our framework improves the\nsegmentation performance over traditional and deep learning-based superpixel\nmethods. Our code is available online.\n", "link": "http://arxiv.org/abs/2407.17354v1", "date": "2024-07-24", "relevancy": 2.7239, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5524}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5521}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Spherical%20Superpixels&body=Title%3A%20Deep%20Spherical%20Superpixels%0AAuthor%3A%20R%C3%A9mi%20Giraud%20and%20Micha%C3%ABl%20Cl%C3%A9ment%0AAbstract%3A%20%20%20Over%20the%20years%2C%20the%20use%20of%20superpixel%20segmentation%20has%20become%20very%20popular%20in%0Avarious%20applications%2C%20serving%20as%20a%20preprocessing%20step%20to%20reduce%20data%20size%20by%0Aadapting%20to%20the%20content%20of%20the%20image%2C%20regardless%20of%20its%20semantic%20content.%20While%0Athe%20superpixel%20segmentation%20of%20standard%20planar%20images%2C%20captured%20with%20a%2090%7B%5Cdeg%7D%0Afield%20of%20view%2C%20has%20been%20extensively%20studied%2C%20there%20has%20been%20limited%20focus%20on%0Adedicated%20methods%20to%20omnidirectional%20or%20spherical%20images%2C%20captured%20with%20a%0A360%7B%5Cdeg%7D%20field%20of%20view.%20In%20this%20study%2C%20we%20introduce%20the%20first%20deep%0Alearning-based%20superpixel%20segmentation%20approach%20tailored%20for%20omnidirectional%0Aimages%20called%20DSS%20%28for%20Deep%20Spherical%20Superpixels%29.%20Our%20methodology%20leverages%0Aon%20spherical%20CNN%20architectures%20and%20the%20differentiable%20K-means%20clustering%0Aparadigm%20for%20superpixels%2C%20to%20generate%20superpixels%20that%20follow%20the%20spherical%0Ageometry.%20Additionally%2C%20we%20propose%20to%20use%20data%20augmentation%20techniques%0Aspecifically%20designed%20for%20360%7B%5Cdeg%7D%20images%2C%20enabling%20our%20model%20to%20efficiently%0Alearn%20from%20a%20limited%20set%20of%20annotated%20omnidirectional%20data.%20Our%20extensive%0Avalidation%20across%20two%20datasets%20demonstrates%20that%20taking%20into%20account%20the%0Ainherent%20circular%20geometry%20of%20such%20images%20into%20our%20framework%20improves%20the%0Asegmentation%20performance%20over%20traditional%20and%20deep%20learning-based%20superpixel%0Amethods.%20Our%20code%20is%20available%20online.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17354v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Spherical%2520Superpixels%26entry.906535625%3DR%25C3%25A9mi%2520Giraud%2520and%2520Micha%25C3%25ABl%2520Cl%25C3%25A9ment%26entry.1292438233%3D%2520%2520Over%2520the%2520years%252C%2520the%2520use%2520of%2520superpixel%2520segmentation%2520has%2520become%2520very%2520popular%2520in%250Avarious%2520applications%252C%2520serving%2520as%2520a%2520preprocessing%2520step%2520to%2520reduce%2520data%2520size%2520by%250Aadapting%2520to%2520the%2520content%2520of%2520the%2520image%252C%2520regardless%2520of%2520its%2520semantic%2520content.%2520While%250Athe%2520superpixel%2520segmentation%2520of%2520standard%2520planar%2520images%252C%2520captured%2520with%2520a%252090%257B%255Cdeg%257D%250Afield%2520of%2520view%252C%2520has%2520been%2520extensively%2520studied%252C%2520there%2520has%2520been%2520limited%2520focus%2520on%250Adedicated%2520methods%2520to%2520omnidirectional%2520or%2520spherical%2520images%252C%2520captured%2520with%2520a%250A360%257B%255Cdeg%257D%2520field%2520of%2520view.%2520In%2520this%2520study%252C%2520we%2520introduce%2520the%2520first%2520deep%250Alearning-based%2520superpixel%2520segmentation%2520approach%2520tailored%2520for%2520omnidirectional%250Aimages%2520called%2520DSS%2520%2528for%2520Deep%2520Spherical%2520Superpixels%2529.%2520Our%2520methodology%2520leverages%250Aon%2520spherical%2520CNN%2520architectures%2520and%2520the%2520differentiable%2520K-means%2520clustering%250Aparadigm%2520for%2520superpixels%252C%2520to%2520generate%2520superpixels%2520that%2520follow%2520the%2520spherical%250Ageometry.%2520Additionally%252C%2520we%2520propose%2520to%2520use%2520data%2520augmentation%2520techniques%250Aspecifically%2520designed%2520for%2520360%257B%255Cdeg%257D%2520images%252C%2520enabling%2520our%2520model%2520to%2520efficiently%250Alearn%2520from%2520a%2520limited%2520set%2520of%2520annotated%2520omnidirectional%2520data.%2520Our%2520extensive%250Avalidation%2520across%2520two%2520datasets%2520demonstrates%2520that%2520taking%2520into%2520account%2520the%250Ainherent%2520circular%2520geometry%2520of%2520such%2520images%2520into%2520our%2520framework%2520improves%2520the%250Asegmentation%2520performance%2520over%2520traditional%2520and%2520deep%2520learning-based%2520superpixel%250Amethods.%2520Our%2520code%2520is%2520available%2520online.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17354v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Spherical%20Superpixels&entry.906535625=R%C3%A9mi%20Giraud%20and%20Micha%C3%ABl%20Cl%C3%A9ment&entry.1292438233=%20%20Over%20the%20years%2C%20the%20use%20of%20superpixel%20segmentation%20has%20become%20very%20popular%20in%0Avarious%20applications%2C%20serving%20as%20a%20preprocessing%20step%20to%20reduce%20data%20size%20by%0Aadapting%20to%20the%20content%20of%20the%20image%2C%20regardless%20of%20its%20semantic%20content.%20While%0Athe%20superpixel%20segmentation%20of%20standard%20planar%20images%2C%20captured%20with%20a%2090%7B%5Cdeg%7D%0Afield%20of%20view%2C%20has%20been%20extensively%20studied%2C%20there%20has%20been%20limited%20focus%20on%0Adedicated%20methods%20to%20omnidirectional%20or%20spherical%20images%2C%20captured%20with%20a%0A360%7B%5Cdeg%7D%20field%20of%20view.%20In%20this%20study%2C%20we%20introduce%20the%20first%20deep%0Alearning-based%20superpixel%20segmentation%20approach%20tailored%20for%20omnidirectional%0Aimages%20called%20DSS%20%28for%20Deep%20Spherical%20Superpixels%29.%20Our%20methodology%20leverages%0Aon%20spherical%20CNN%20architectures%20and%20the%20differentiable%20K-means%20clustering%0Aparadigm%20for%20superpixels%2C%20to%20generate%20superpixels%20that%20follow%20the%20spherical%0Ageometry.%20Additionally%2C%20we%20propose%20to%20use%20data%20augmentation%20techniques%0Aspecifically%20designed%20for%20360%7B%5Cdeg%7D%20images%2C%20enabling%20our%20model%20to%20efficiently%0Alearn%20from%20a%20limited%20set%20of%20annotated%20omnidirectional%20data.%20Our%20extensive%0Avalidation%20across%20two%20datasets%20demonstrates%20that%20taking%20into%20account%20the%0Ainherent%20circular%20geometry%20of%20such%20images%20into%20our%20framework%20improves%20the%0Asegmentation%20performance%20over%20traditional%20and%20deep%20learning-based%20superpixel%0Amethods.%20Our%20code%20is%20available%20online.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17354v1&entry.124074799=Read"},
{"title": "Robust Point Cloud Registration in Robotic Inspection with Locally\n  Consistent Gaussian Mixture Model", "author": "Lingjie Su and Wei Xu and Wenlong Li", "abstract": "  In robotic inspection of aviation parts, achieving accurate pairwise point\ncloud registration between scanned and model data is essential. However, noise\nand outliers generated in robotic scanned data can compromise registration\naccuracy. To mitigate this challenge, this article proposes a probability-based\nregistration method utilizing Gaussian Mixture Model (GMM) with local\nconsistency constraint. This method converts the registration problem into a\nmodel fitting one, constraining the similarity of posterior distributions\nbetween neighboring points to enhance correspondence robustness. We employ the\nExpectation Maximization algorithm iteratively to find optimal rotation matrix\nand translation vector while obtaining GMM parameters. Both E-step and M-step\nhave closed-form solutions. Simulation and actual experiments confirm the\nmethod's effectiveness, reducing root mean square error by 20% despite the\npresence of noise and outliers. The proposed method excels in robustness and\naccuracy compared to existing methods.\n", "link": "http://arxiv.org/abs/2407.17183v1", "date": "2024-07-24", "relevancy": 2.6994, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5544}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5454}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Point%20Cloud%20Registration%20in%20Robotic%20Inspection%20with%20Locally%0A%20%20Consistent%20Gaussian%20Mixture%20Model&body=Title%3A%20Robust%20Point%20Cloud%20Registration%20in%20Robotic%20Inspection%20with%20Locally%0A%20%20Consistent%20Gaussian%20Mixture%20Model%0AAuthor%3A%20Lingjie%20Su%20and%20Wei%20Xu%20and%20Wenlong%20Li%0AAbstract%3A%20%20%20In%20robotic%20inspection%20of%20aviation%20parts%2C%20achieving%20accurate%20pairwise%20point%0Acloud%20registration%20between%20scanned%20and%20model%20data%20is%20essential.%20However%2C%20noise%0Aand%20outliers%20generated%20in%20robotic%20scanned%20data%20can%20compromise%20registration%0Aaccuracy.%20To%20mitigate%20this%20challenge%2C%20this%20article%20proposes%20a%20probability-based%0Aregistration%20method%20utilizing%20Gaussian%20Mixture%20Model%20%28GMM%29%20with%20local%0Aconsistency%20constraint.%20This%20method%20converts%20the%20registration%20problem%20into%20a%0Amodel%20fitting%20one%2C%20constraining%20the%20similarity%20of%20posterior%20distributions%0Abetween%20neighboring%20points%20to%20enhance%20correspondence%20robustness.%20We%20employ%20the%0AExpectation%20Maximization%20algorithm%20iteratively%20to%20find%20optimal%20rotation%20matrix%0Aand%20translation%20vector%20while%20obtaining%20GMM%20parameters.%20Both%20E-step%20and%20M-step%0Ahave%20closed-form%20solutions.%20Simulation%20and%20actual%20experiments%20confirm%20the%0Amethod%27s%20effectiveness%2C%20reducing%20root%20mean%20square%20error%20by%2020%25%20despite%20the%0Apresence%20of%20noise%20and%20outliers.%20The%20proposed%20method%20excels%20in%20robustness%20and%0Aaccuracy%20compared%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17183v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Point%2520Cloud%2520Registration%2520in%2520Robotic%2520Inspection%2520with%2520Locally%250A%2520%2520Consistent%2520Gaussian%2520Mixture%2520Model%26entry.906535625%3DLingjie%2520Su%2520and%2520Wei%2520Xu%2520and%2520Wenlong%2520Li%26entry.1292438233%3D%2520%2520In%2520robotic%2520inspection%2520of%2520aviation%2520parts%252C%2520achieving%2520accurate%2520pairwise%2520point%250Acloud%2520registration%2520between%2520scanned%2520and%2520model%2520data%2520is%2520essential.%2520However%252C%2520noise%250Aand%2520outliers%2520generated%2520in%2520robotic%2520scanned%2520data%2520can%2520compromise%2520registration%250Aaccuracy.%2520To%2520mitigate%2520this%2520challenge%252C%2520this%2520article%2520proposes%2520a%2520probability-based%250Aregistration%2520method%2520utilizing%2520Gaussian%2520Mixture%2520Model%2520%2528GMM%2529%2520with%2520local%250Aconsistency%2520constraint.%2520This%2520method%2520converts%2520the%2520registration%2520problem%2520into%2520a%250Amodel%2520fitting%2520one%252C%2520constraining%2520the%2520similarity%2520of%2520posterior%2520distributions%250Abetween%2520neighboring%2520points%2520to%2520enhance%2520correspondence%2520robustness.%2520We%2520employ%2520the%250AExpectation%2520Maximization%2520algorithm%2520iteratively%2520to%2520find%2520optimal%2520rotation%2520matrix%250Aand%2520translation%2520vector%2520while%2520obtaining%2520GMM%2520parameters.%2520Both%2520E-step%2520and%2520M-step%250Ahave%2520closed-form%2520solutions.%2520Simulation%2520and%2520actual%2520experiments%2520confirm%2520the%250Amethod%2527s%2520effectiveness%252C%2520reducing%2520root%2520mean%2520square%2520error%2520by%252020%2525%2520despite%2520the%250Apresence%2520of%2520noise%2520and%2520outliers.%2520The%2520proposed%2520method%2520excels%2520in%2520robustness%2520and%250Aaccuracy%2520compared%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17183v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Point%20Cloud%20Registration%20in%20Robotic%20Inspection%20with%20Locally%0A%20%20Consistent%20Gaussian%20Mixture%20Model&entry.906535625=Lingjie%20Su%20and%20Wei%20Xu%20and%20Wenlong%20Li&entry.1292438233=%20%20In%20robotic%20inspection%20of%20aviation%20parts%2C%20achieving%20accurate%20pairwise%20point%0Acloud%20registration%20between%20scanned%20and%20model%20data%20is%20essential.%20However%2C%20noise%0Aand%20outliers%20generated%20in%20robotic%20scanned%20data%20can%20compromise%20registration%0Aaccuracy.%20To%20mitigate%20this%20challenge%2C%20this%20article%20proposes%20a%20probability-based%0Aregistration%20method%20utilizing%20Gaussian%20Mixture%20Model%20%28GMM%29%20with%20local%0Aconsistency%20constraint.%20This%20method%20converts%20the%20registration%20problem%20into%20a%0Amodel%20fitting%20one%2C%20constraining%20the%20similarity%20of%20posterior%20distributions%0Abetween%20neighboring%20points%20to%20enhance%20correspondence%20robustness.%20We%20employ%20the%0AExpectation%20Maximization%20algorithm%20iteratively%20to%20find%20optimal%20rotation%20matrix%0Aand%20translation%20vector%20while%20obtaining%20GMM%20parameters.%20Both%20E-step%20and%20M-step%0Ahave%20closed-form%20solutions.%20Simulation%20and%20actual%20experiments%20confirm%20the%0Amethod%27s%20effectiveness%2C%20reducing%20root%20mean%20square%20error%20by%2020%25%20despite%20the%0Apresence%20of%20noise%20and%20outliers.%20The%20proposed%20method%20excels%20in%20robustness%20and%0Aaccuracy%20compared%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17183v1&entry.124074799=Read"},
{"title": "Graph Neural Networks: A suitable Alternative to MLPs in Latent 3D\n  Medical Image Classification?", "author": "Johannes Kiechle and Daniel M. Lang and Stefan M. Fischer and Lina Felsner and Jan C. Peeken and Julia A. Schnabel", "abstract": "  Recent studies have underscored the capabilities of natural imaging\nfoundation models to serve as powerful feature extractors, even in a zero-shot\nsetting for medical imaging data. Most commonly, a shallow multi-layer\nperceptron (MLP) is appended to the feature extractor to facilitate end-to-end\nlearning and downstream prediction tasks such as classification, thus\nrepresenting the de facto standard. However, as graph neural networks (GNNs)\nhave become a practicable choice for various tasks in medical research in the\nrecent past, we direct attention to the question of how effective GNNs are\ncompared to MLP prediction heads for the task of 3D medical image\nclassification, proposing them as a potential alternative. In our experiments,\nwe devise a subject-level graph for each volumetric dataset instance. Therein\nlatent representations of all slices in the volume, encoded through a DINOv2\npretrained vision transformer (ViT), constitute the nodes and their respective\nnode features. We use public datasets to compare the classification heads\nnumerically and evaluate various graph construction and graph convolution\nmethods in our experiments. Our findings show enhancements of the GNN in\nclassification performance and substantial improvements in runtime compared to\nan MLP prediction head. Additional robustness evaluations further validate the\npromising performance of the GNN, promoting them as a suitable alternative to\ntraditional MLP classification heads. Our code is publicly available at:\nhttps://github.com/compai-lab/2024-miccai-grail-kiechle\n", "link": "http://arxiv.org/abs/2407.17219v1", "date": "2024-07-24", "relevancy": 2.6132, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5313}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.522}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Neural%20Networks%3A%20A%20suitable%20Alternative%20to%20MLPs%20in%20Latent%203D%0A%20%20Medical%20Image%20Classification%3F&body=Title%3A%20Graph%20Neural%20Networks%3A%20A%20suitable%20Alternative%20to%20MLPs%20in%20Latent%203D%0A%20%20Medical%20Image%20Classification%3F%0AAuthor%3A%20Johannes%20Kiechle%20and%20Daniel%20M.%20Lang%20and%20Stefan%20M.%20Fischer%20and%20Lina%20Felsner%20and%20Jan%20C.%20Peeken%20and%20Julia%20A.%20Schnabel%0AAbstract%3A%20%20%20Recent%20studies%20have%20underscored%20the%20capabilities%20of%20natural%20imaging%0Afoundation%20models%20to%20serve%20as%20powerful%20feature%20extractors%2C%20even%20in%20a%20zero-shot%0Asetting%20for%20medical%20imaging%20data.%20Most%20commonly%2C%20a%20shallow%20multi-layer%0Aperceptron%20%28MLP%29%20is%20appended%20to%20the%20feature%20extractor%20to%20facilitate%20end-to-end%0Alearning%20and%20downstream%20prediction%20tasks%20such%20as%20classification%2C%20thus%0Arepresenting%20the%20de%20facto%20standard.%20However%2C%20as%20graph%20neural%20networks%20%28GNNs%29%0Ahave%20become%20a%20practicable%20choice%20for%20various%20tasks%20in%20medical%20research%20in%20the%0Arecent%20past%2C%20we%20direct%20attention%20to%20the%20question%20of%20how%20effective%20GNNs%20are%0Acompared%20to%20MLP%20prediction%20heads%20for%20the%20task%20of%203D%20medical%20image%0Aclassification%2C%20proposing%20them%20as%20a%20potential%20alternative.%20In%20our%20experiments%2C%0Awe%20devise%20a%20subject-level%20graph%20for%20each%20volumetric%20dataset%20instance.%20Therein%0Alatent%20representations%20of%20all%20slices%20in%20the%20volume%2C%20encoded%20through%20a%20DINOv2%0Apretrained%20vision%20transformer%20%28ViT%29%2C%20constitute%20the%20nodes%20and%20their%20respective%0Anode%20features.%20We%20use%20public%20datasets%20to%20compare%20the%20classification%20heads%0Anumerically%20and%20evaluate%20various%20graph%20construction%20and%20graph%20convolution%0Amethods%20in%20our%20experiments.%20Our%20findings%20show%20enhancements%20of%20the%20GNN%20in%0Aclassification%20performance%20and%20substantial%20improvements%20in%20runtime%20compared%20to%0Aan%20MLP%20prediction%20head.%20Additional%20robustness%20evaluations%20further%20validate%20the%0Apromising%20performance%20of%20the%20GNN%2C%20promoting%20them%20as%20a%20suitable%20alternative%20to%0Atraditional%20MLP%20classification%20heads.%20Our%20code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/compai-lab/2024-miccai-grail-kiechle%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17219v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Neural%2520Networks%253A%2520A%2520suitable%2520Alternative%2520to%2520MLPs%2520in%2520Latent%25203D%250A%2520%2520Medical%2520Image%2520Classification%253F%26entry.906535625%3DJohannes%2520Kiechle%2520and%2520Daniel%2520M.%2520Lang%2520and%2520Stefan%2520M.%2520Fischer%2520and%2520Lina%2520Felsner%2520and%2520Jan%2520C.%2520Peeken%2520and%2520Julia%2520A.%2520Schnabel%26entry.1292438233%3D%2520%2520Recent%2520studies%2520have%2520underscored%2520the%2520capabilities%2520of%2520natural%2520imaging%250Afoundation%2520models%2520to%2520serve%2520as%2520powerful%2520feature%2520extractors%252C%2520even%2520in%2520a%2520zero-shot%250Asetting%2520for%2520medical%2520imaging%2520data.%2520Most%2520commonly%252C%2520a%2520shallow%2520multi-layer%250Aperceptron%2520%2528MLP%2529%2520is%2520appended%2520to%2520the%2520feature%2520extractor%2520to%2520facilitate%2520end-to-end%250Alearning%2520and%2520downstream%2520prediction%2520tasks%2520such%2520as%2520classification%252C%2520thus%250Arepresenting%2520the%2520de%2520facto%2520standard.%2520However%252C%2520as%2520graph%2520neural%2520networks%2520%2528GNNs%2529%250Ahave%2520become%2520a%2520practicable%2520choice%2520for%2520various%2520tasks%2520in%2520medical%2520research%2520in%2520the%250Arecent%2520past%252C%2520we%2520direct%2520attention%2520to%2520the%2520question%2520of%2520how%2520effective%2520GNNs%2520are%250Acompared%2520to%2520MLP%2520prediction%2520heads%2520for%2520the%2520task%2520of%25203D%2520medical%2520image%250Aclassification%252C%2520proposing%2520them%2520as%2520a%2520potential%2520alternative.%2520In%2520our%2520experiments%252C%250Awe%2520devise%2520a%2520subject-level%2520graph%2520for%2520each%2520volumetric%2520dataset%2520instance.%2520Therein%250Alatent%2520representations%2520of%2520all%2520slices%2520in%2520the%2520volume%252C%2520encoded%2520through%2520a%2520DINOv2%250Apretrained%2520vision%2520transformer%2520%2528ViT%2529%252C%2520constitute%2520the%2520nodes%2520and%2520their%2520respective%250Anode%2520features.%2520We%2520use%2520public%2520datasets%2520to%2520compare%2520the%2520classification%2520heads%250Anumerically%2520and%2520evaluate%2520various%2520graph%2520construction%2520and%2520graph%2520convolution%250Amethods%2520in%2520our%2520experiments.%2520Our%2520findings%2520show%2520enhancements%2520of%2520the%2520GNN%2520in%250Aclassification%2520performance%2520and%2520substantial%2520improvements%2520in%2520runtime%2520compared%2520to%250Aan%2520MLP%2520prediction%2520head.%2520Additional%2520robustness%2520evaluations%2520further%2520validate%2520the%250Apromising%2520performance%2520of%2520the%2520GNN%252C%2520promoting%2520them%2520as%2520a%2520suitable%2520alternative%2520to%250Atraditional%2520MLP%2520classification%2520heads.%2520Our%2520code%2520is%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/compai-lab/2024-miccai-grail-kiechle%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17219v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Neural%20Networks%3A%20A%20suitable%20Alternative%20to%20MLPs%20in%20Latent%203D%0A%20%20Medical%20Image%20Classification%3F&entry.906535625=Johannes%20Kiechle%20and%20Daniel%20M.%20Lang%20and%20Stefan%20M.%20Fischer%20and%20Lina%20Felsner%20and%20Jan%20C.%20Peeken%20and%20Julia%20A.%20Schnabel&entry.1292438233=%20%20Recent%20studies%20have%20underscored%20the%20capabilities%20of%20natural%20imaging%0Afoundation%20models%20to%20serve%20as%20powerful%20feature%20extractors%2C%20even%20in%20a%20zero-shot%0Asetting%20for%20medical%20imaging%20data.%20Most%20commonly%2C%20a%20shallow%20multi-layer%0Aperceptron%20%28MLP%29%20is%20appended%20to%20the%20feature%20extractor%20to%20facilitate%20end-to-end%0Alearning%20and%20downstream%20prediction%20tasks%20such%20as%20classification%2C%20thus%0Arepresenting%20the%20de%20facto%20standard.%20However%2C%20as%20graph%20neural%20networks%20%28GNNs%29%0Ahave%20become%20a%20practicable%20choice%20for%20various%20tasks%20in%20medical%20research%20in%20the%0Arecent%20past%2C%20we%20direct%20attention%20to%20the%20question%20of%20how%20effective%20GNNs%20are%0Acompared%20to%20MLP%20prediction%20heads%20for%20the%20task%20of%203D%20medical%20image%0Aclassification%2C%20proposing%20them%20as%20a%20potential%20alternative.%20In%20our%20experiments%2C%0Awe%20devise%20a%20subject-level%20graph%20for%20each%20volumetric%20dataset%20instance.%20Therein%0Alatent%20representations%20of%20all%20slices%20in%20the%20volume%2C%20encoded%20through%20a%20DINOv2%0Apretrained%20vision%20transformer%20%28ViT%29%2C%20constitute%20the%20nodes%20and%20their%20respective%0Anode%20features.%20We%20use%20public%20datasets%20to%20compare%20the%20classification%20heads%0Anumerically%20and%20evaluate%20various%20graph%20construction%20and%20graph%20convolution%0Amethods%20in%20our%20experiments.%20Our%20findings%20show%20enhancements%20of%20the%20GNN%20in%0Aclassification%20performance%20and%20substantial%20improvements%20in%20runtime%20compared%20to%0Aan%20MLP%20prediction%20head.%20Additional%20robustness%20evaluations%20further%20validate%20the%0Apromising%20performance%20of%20the%20GNN%2C%20promoting%20them%20as%20a%20suitable%20alternative%20to%0Atraditional%20MLP%20classification%20heads.%20Our%20code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/compai-lab/2024-miccai-grail-kiechle%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17219v1&entry.124074799=Read"},
{"title": "CSCPR: Cross-Source-Context Indoor RGB-D Place Recognition", "author": "Jing Liang and Zhuo Deng and Zheming Zhou and Min Sun and Omid Ghasemalizadeh and Cheng-Hao Kuo and Arnie Sen and Dinesh Manocha", "abstract": "  We present a new algorithm, Cross-Source-Context Place Recognition (CSCPR),\nfor RGB-D indoor place recognition that integrates global retrieval and\nreranking into a single end-to-end model. Unlike prior approaches that\nprimarily focus on the RGB domain, CSCPR is designed to handle the RGB-D data.\nWe extend the Context-of-Clusters (CoCs) for handling noisy colorized point\nclouds and introduce two novel modules for reranking: the Self-Context Cluster\n(SCC) and Cross Source Context Cluster (CSCC), which enhance feature\nrepresentation and match query-database pairs based on local features,\nrespectively. We also present two new datasets, ScanNetIPR and ARKitIPR. Our\nexperiments demonstrate that CSCPR significantly outperforms state-of-the-art\nmodels on these datasets by at least 36.5% in Recall@1 at ScanNet-PR dataset\nand 44% in new datasets. Code and datasets will be released.\n", "link": "http://arxiv.org/abs/2407.17457v1", "date": "2024-07-24", "relevancy": 2.6079, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5495}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5125}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CSCPR%3A%20Cross-Source-Context%20Indoor%20RGB-D%20Place%20Recognition&body=Title%3A%20CSCPR%3A%20Cross-Source-Context%20Indoor%20RGB-D%20Place%20Recognition%0AAuthor%3A%20Jing%20Liang%20and%20Zhuo%20Deng%20and%20Zheming%20Zhou%20and%20Min%20Sun%20and%20Omid%20Ghasemalizadeh%20and%20Cheng-Hao%20Kuo%20and%20Arnie%20Sen%20and%20Dinesh%20Manocha%0AAbstract%3A%20%20%20We%20present%20a%20new%20algorithm%2C%20Cross-Source-Context%20Place%20Recognition%20%28CSCPR%29%2C%0Afor%20RGB-D%20indoor%20place%20recognition%20that%20integrates%20global%20retrieval%20and%0Areranking%20into%20a%20single%20end-to-end%20model.%20Unlike%20prior%20approaches%20that%0Aprimarily%20focus%20on%20the%20RGB%20domain%2C%20CSCPR%20is%20designed%20to%20handle%20the%20RGB-D%20data.%0AWe%20extend%20the%20Context-of-Clusters%20%28CoCs%29%20for%20handling%20noisy%20colorized%20point%0Aclouds%20and%20introduce%20two%20novel%20modules%20for%20reranking%3A%20the%20Self-Context%20Cluster%0A%28SCC%29%20and%20Cross%20Source%20Context%20Cluster%20%28CSCC%29%2C%20which%20enhance%20feature%0Arepresentation%20and%20match%20query-database%20pairs%20based%20on%20local%20features%2C%0Arespectively.%20We%20also%20present%20two%20new%20datasets%2C%20ScanNetIPR%20and%20ARKitIPR.%20Our%0Aexperiments%20demonstrate%20that%20CSCPR%20significantly%20outperforms%20state-of-the-art%0Amodels%20on%20these%20datasets%20by%20at%20least%2036.5%25%20in%20Recall%401%20at%20ScanNet-PR%20dataset%0Aand%2044%25%20in%20new%20datasets.%20Code%20and%20datasets%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17457v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCSCPR%253A%2520Cross-Source-Context%2520Indoor%2520RGB-D%2520Place%2520Recognition%26entry.906535625%3DJing%2520Liang%2520and%2520Zhuo%2520Deng%2520and%2520Zheming%2520Zhou%2520and%2520Min%2520Sun%2520and%2520Omid%2520Ghasemalizadeh%2520and%2520Cheng-Hao%2520Kuo%2520and%2520Arnie%2520Sen%2520and%2520Dinesh%2520Manocha%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520new%2520algorithm%252C%2520Cross-Source-Context%2520Place%2520Recognition%2520%2528CSCPR%2529%252C%250Afor%2520RGB-D%2520indoor%2520place%2520recognition%2520that%2520integrates%2520global%2520retrieval%2520and%250Areranking%2520into%2520a%2520single%2520end-to-end%2520model.%2520Unlike%2520prior%2520approaches%2520that%250Aprimarily%2520focus%2520on%2520the%2520RGB%2520domain%252C%2520CSCPR%2520is%2520designed%2520to%2520handle%2520the%2520RGB-D%2520data.%250AWe%2520extend%2520the%2520Context-of-Clusters%2520%2528CoCs%2529%2520for%2520handling%2520noisy%2520colorized%2520point%250Aclouds%2520and%2520introduce%2520two%2520novel%2520modules%2520for%2520reranking%253A%2520the%2520Self-Context%2520Cluster%250A%2528SCC%2529%2520and%2520Cross%2520Source%2520Context%2520Cluster%2520%2528CSCC%2529%252C%2520which%2520enhance%2520feature%250Arepresentation%2520and%2520match%2520query-database%2520pairs%2520based%2520on%2520local%2520features%252C%250Arespectively.%2520We%2520also%2520present%2520two%2520new%2520datasets%252C%2520ScanNetIPR%2520and%2520ARKitIPR.%2520Our%250Aexperiments%2520demonstrate%2520that%2520CSCPR%2520significantly%2520outperforms%2520state-of-the-art%250Amodels%2520on%2520these%2520datasets%2520by%2520at%2520least%252036.5%2525%2520in%2520Recall%25401%2520at%2520ScanNet-PR%2520dataset%250Aand%252044%2525%2520in%2520new%2520datasets.%2520Code%2520and%2520datasets%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17457v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CSCPR%3A%20Cross-Source-Context%20Indoor%20RGB-D%20Place%20Recognition&entry.906535625=Jing%20Liang%20and%20Zhuo%20Deng%20and%20Zheming%20Zhou%20and%20Min%20Sun%20and%20Omid%20Ghasemalizadeh%20and%20Cheng-Hao%20Kuo%20and%20Arnie%20Sen%20and%20Dinesh%20Manocha&entry.1292438233=%20%20We%20present%20a%20new%20algorithm%2C%20Cross-Source-Context%20Place%20Recognition%20%28CSCPR%29%2C%0Afor%20RGB-D%20indoor%20place%20recognition%20that%20integrates%20global%20retrieval%20and%0Areranking%20into%20a%20single%20end-to-end%20model.%20Unlike%20prior%20approaches%20that%0Aprimarily%20focus%20on%20the%20RGB%20domain%2C%20CSCPR%20is%20designed%20to%20handle%20the%20RGB-D%20data.%0AWe%20extend%20the%20Context-of-Clusters%20%28CoCs%29%20for%20handling%20noisy%20colorized%20point%0Aclouds%20and%20introduce%20two%20novel%20modules%20for%20reranking%3A%20the%20Self-Context%20Cluster%0A%28SCC%29%20and%20Cross%20Source%20Context%20Cluster%20%28CSCC%29%2C%20which%20enhance%20feature%0Arepresentation%20and%20match%20query-database%20pairs%20based%20on%20local%20features%2C%0Arespectively.%20We%20also%20present%20two%20new%20datasets%2C%20ScanNetIPR%20and%20ARKitIPR.%20Our%0Aexperiments%20demonstrate%20that%20CSCPR%20significantly%20outperforms%20state-of-the-art%0Amodels%20on%20these%20datasets%20by%20at%20least%2036.5%25%20in%20Recall%401%20at%20ScanNet-PR%20dataset%0Aand%2044%25%20in%20new%20datasets.%20Code%20and%20datasets%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17457v1&entry.124074799=Read"},
{"title": "Generation of Training Data from HD Maps in the Lanelet2 Framework", "author": "Fabian Immel and Richard Fehler and Frank Bieder and Christoph Stiller", "abstract": "  Using HD maps directly as training data for machine learning tasks has seen a\nmassive surge in popularity and shown promising results, e.g. in the field of\nmap perception. Despite that, a standardized HD map framework supporting all\nparts of map-based automated driving and training label generation from map\ndata does not exist. Furthermore, feeding map perception models with map data\nas part of the input during real-time inference is not addressed by the\nresearch community. In order to fill this gap, we presentlanelet2_ml_converter,\nan integrated extension to the HD map framework Lanelet2, widely used in\nautomated driving systems by academia and industry. With this addition Lanelet2\nunifies map based automated driving, machine learning inference and training,\nall from a single source of map data and format. Requirements for a unified\nframework are analyzed and the implementation of these requirements is\ndescribed. The usability of labels in state of the art machine learning is\ndemonstrated with application examples from the field of map perception. The\nsource code is available embedded in the Lanelet2 framework under\nhttps://github.com/fzi-forschungszentrum-informatik/Lanelet2/tree/feature_ml_converter\n", "link": "http://arxiv.org/abs/2407.17409v1", "date": "2024-07-24", "relevancy": 2.5961, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5509}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5036}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generation%20of%20Training%20Data%20from%20HD%20Maps%20in%20the%20Lanelet2%20Framework&body=Title%3A%20Generation%20of%20Training%20Data%20from%20HD%20Maps%20in%20the%20Lanelet2%20Framework%0AAuthor%3A%20Fabian%20Immel%20and%20Richard%20Fehler%20and%20Frank%20Bieder%20and%20Christoph%20Stiller%0AAbstract%3A%20%20%20Using%20HD%20maps%20directly%20as%20training%20data%20for%20machine%20learning%20tasks%20has%20seen%20a%0Amassive%20surge%20in%20popularity%20and%20shown%20promising%20results%2C%20e.g.%20in%20the%20field%20of%0Amap%20perception.%20Despite%20that%2C%20a%20standardized%20HD%20map%20framework%20supporting%20all%0Aparts%20of%20map-based%20automated%20driving%20and%20training%20label%20generation%20from%20map%0Adata%20does%20not%20exist.%20Furthermore%2C%20feeding%20map%20perception%20models%20with%20map%20data%0Aas%20part%20of%20the%20input%20during%20real-time%20inference%20is%20not%20addressed%20by%20the%0Aresearch%20community.%20In%20order%20to%20fill%20this%20gap%2C%20we%20presentlanelet2_ml_converter%2C%0Aan%20integrated%20extension%20to%20the%20HD%20map%20framework%20Lanelet2%2C%20widely%20used%20in%0Aautomated%20driving%20systems%20by%20academia%20and%20industry.%20With%20this%20addition%20Lanelet2%0Aunifies%20map%20based%20automated%20driving%2C%20machine%20learning%20inference%20and%20training%2C%0Aall%20from%20a%20single%20source%20of%20map%20data%20and%20format.%20Requirements%20for%20a%20unified%0Aframework%20are%20analyzed%20and%20the%20implementation%20of%20these%20requirements%20is%0Adescribed.%20The%20usability%20of%20labels%20in%20state%20of%20the%20art%20machine%20learning%20is%0Ademonstrated%20with%20application%20examples%20from%20the%20field%20of%20map%20perception.%20The%0Asource%20code%20is%20available%20embedded%20in%20the%20Lanelet2%20framework%20under%0Ahttps%3A//github.com/fzi-forschungszentrum-informatik/Lanelet2/tree/feature_ml_converter%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17409v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneration%2520of%2520Training%2520Data%2520from%2520HD%2520Maps%2520in%2520the%2520Lanelet2%2520Framework%26entry.906535625%3DFabian%2520Immel%2520and%2520Richard%2520Fehler%2520and%2520Frank%2520Bieder%2520and%2520Christoph%2520Stiller%26entry.1292438233%3D%2520%2520Using%2520HD%2520maps%2520directly%2520as%2520training%2520data%2520for%2520machine%2520learning%2520tasks%2520has%2520seen%2520a%250Amassive%2520surge%2520in%2520popularity%2520and%2520shown%2520promising%2520results%252C%2520e.g.%2520in%2520the%2520field%2520of%250Amap%2520perception.%2520Despite%2520that%252C%2520a%2520standardized%2520HD%2520map%2520framework%2520supporting%2520all%250Aparts%2520of%2520map-based%2520automated%2520driving%2520and%2520training%2520label%2520generation%2520from%2520map%250Adata%2520does%2520not%2520exist.%2520Furthermore%252C%2520feeding%2520map%2520perception%2520models%2520with%2520map%2520data%250Aas%2520part%2520of%2520the%2520input%2520during%2520real-time%2520inference%2520is%2520not%2520addressed%2520by%2520the%250Aresearch%2520community.%2520In%2520order%2520to%2520fill%2520this%2520gap%252C%2520we%2520presentlanelet2_ml_converter%252C%250Aan%2520integrated%2520extension%2520to%2520the%2520HD%2520map%2520framework%2520Lanelet2%252C%2520widely%2520used%2520in%250Aautomated%2520driving%2520systems%2520by%2520academia%2520and%2520industry.%2520With%2520this%2520addition%2520Lanelet2%250Aunifies%2520map%2520based%2520automated%2520driving%252C%2520machine%2520learning%2520inference%2520and%2520training%252C%250Aall%2520from%2520a%2520single%2520source%2520of%2520map%2520data%2520and%2520format.%2520Requirements%2520for%2520a%2520unified%250Aframework%2520are%2520analyzed%2520and%2520the%2520implementation%2520of%2520these%2520requirements%2520is%250Adescribed.%2520The%2520usability%2520of%2520labels%2520in%2520state%2520of%2520the%2520art%2520machine%2520learning%2520is%250Ademonstrated%2520with%2520application%2520examples%2520from%2520the%2520field%2520of%2520map%2520perception.%2520The%250Asource%2520code%2520is%2520available%2520embedded%2520in%2520the%2520Lanelet2%2520framework%2520under%250Ahttps%253A//github.com/fzi-forschungszentrum-informatik/Lanelet2/tree/feature_ml_converter%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17409v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generation%20of%20Training%20Data%20from%20HD%20Maps%20in%20the%20Lanelet2%20Framework&entry.906535625=Fabian%20Immel%20and%20Richard%20Fehler%20and%20Frank%20Bieder%20and%20Christoph%20Stiller&entry.1292438233=%20%20Using%20HD%20maps%20directly%20as%20training%20data%20for%20machine%20learning%20tasks%20has%20seen%20a%0Amassive%20surge%20in%20popularity%20and%20shown%20promising%20results%2C%20e.g.%20in%20the%20field%20of%0Amap%20perception.%20Despite%20that%2C%20a%20standardized%20HD%20map%20framework%20supporting%20all%0Aparts%20of%20map-based%20automated%20driving%20and%20training%20label%20generation%20from%20map%0Adata%20does%20not%20exist.%20Furthermore%2C%20feeding%20map%20perception%20models%20with%20map%20data%0Aas%20part%20of%20the%20input%20during%20real-time%20inference%20is%20not%20addressed%20by%20the%0Aresearch%20community.%20In%20order%20to%20fill%20this%20gap%2C%20we%20presentlanelet2_ml_converter%2C%0Aan%20integrated%20extension%20to%20the%20HD%20map%20framework%20Lanelet2%2C%20widely%20used%20in%0Aautomated%20driving%20systems%20by%20academia%20and%20industry.%20With%20this%20addition%20Lanelet2%0Aunifies%20map%20based%20automated%20driving%2C%20machine%20learning%20inference%20and%20training%2C%0Aall%20from%20a%20single%20source%20of%20map%20data%20and%20format.%20Requirements%20for%20a%20unified%0Aframework%20are%20analyzed%20and%20the%20implementation%20of%20these%20requirements%20is%0Adescribed.%20The%20usability%20of%20labels%20in%20state%20of%20the%20art%20machine%20learning%20is%0Ademonstrated%20with%20application%20examples%20from%20the%20field%20of%20map%20perception.%20The%0Asource%20code%20is%20available%20embedded%20in%20the%20Lanelet2%20framework%20under%0Ahttps%3A//github.com/fzi-forschungszentrum-informatik/Lanelet2/tree/feature_ml_converter%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17409v1&entry.124074799=Read"},
{"title": "HumanVid: Demystifying Training Data for Camera-controllable Human Image\n  Animation", "author": "Zhenzhi Wang and Yixuan Li and Yanhong Zeng and Youqing Fang and Yuwei Guo and Wenran Liu and Jing Tan and Kai Chen and Tianfan Xue and Bo Dai and Dahua Lin", "abstract": "  Human image animation involves generating videos from a character photo,\nallowing user control and unlocking potential for video and movie production.\nWhile recent approaches yield impressive results using high-quality training\ndata, the inaccessibility of these datasets hampers fair and transparent\nbenchmarking. Moreover, these approaches prioritize 2D human motion and\noverlook the significance of camera motions in videos, leading to limited\ncontrol and unstable video generation.To demystify the training data, we\npresent HumanVid, the first large-scale high-quality dataset tailored for human\nimage animation, which combines crafted real-world and synthetic data. For the\nreal-world data, we compile a vast collection of copyright-free real-world\nvideos from the internet. Through a carefully designed rule-based filtering\nstrategy, we ensure the inclusion of high-quality videos, resulting in a\ncollection of 20K human-centric videos in 1080P resolution. Human and camera\nmotion annotation is accomplished using a 2D pose estimator and a SLAM-based\nmethod. For the synthetic data, we gather 2,300 copyright-free 3D avatar assets\nto augment existing available 3D assets. Notably, we introduce a rule-based\ncamera trajectory generation method, enabling the synthetic pipeline to\nincorporate diverse and precise camera motion annotation, which can rarely be\nfound in real-world data. To verify the effectiveness of HumanVid, we establish\na baseline model named CamAnimate, short for Camera-controllable Human\nAnimation, that considers both human and camera motions as conditions. Through\nextensive experimentation, we demonstrate that such simple baseline training on\nour HumanVid achieves state-of-the-art performance in controlling both human\npose and camera motions, setting a new benchmark. Code and data will be\npublicly available at \\url{https://github.com/zhenzhiwang/HumanVid/}.\n", "link": "http://arxiv.org/abs/2407.17438v1", "date": "2024-07-24", "relevancy": 2.5629, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6773}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6202}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HumanVid%3A%20Demystifying%20Training%20Data%20for%20Camera-controllable%20Human%20Image%0A%20%20Animation&body=Title%3A%20HumanVid%3A%20Demystifying%20Training%20Data%20for%20Camera-controllable%20Human%20Image%0A%20%20Animation%0AAuthor%3A%20Zhenzhi%20Wang%20and%20Yixuan%20Li%20and%20Yanhong%20Zeng%20and%20Youqing%20Fang%20and%20Yuwei%20Guo%20and%20Wenran%20Liu%20and%20Jing%20Tan%20and%20Kai%20Chen%20and%20Tianfan%20Xue%20and%20Bo%20Dai%20and%20Dahua%20Lin%0AAbstract%3A%20%20%20Human%20image%20animation%20involves%20generating%20videos%20from%20a%20character%20photo%2C%0Aallowing%20user%20control%20and%20unlocking%20potential%20for%20video%20and%20movie%20production.%0AWhile%20recent%20approaches%20yield%20impressive%20results%20using%20high-quality%20training%0Adata%2C%20the%20inaccessibility%20of%20these%20datasets%20hampers%20fair%20and%20transparent%0Abenchmarking.%20Moreover%2C%20these%20approaches%20prioritize%202D%20human%20motion%20and%0Aoverlook%20the%20significance%20of%20camera%20motions%20in%20videos%2C%20leading%20to%20limited%0Acontrol%20and%20unstable%20video%20generation.To%20demystify%20the%20training%20data%2C%20we%0Apresent%20HumanVid%2C%20the%20first%20large-scale%20high-quality%20dataset%20tailored%20for%20human%0Aimage%20animation%2C%20which%20combines%20crafted%20real-world%20and%20synthetic%20data.%20For%20the%0Areal-world%20data%2C%20we%20compile%20a%20vast%20collection%20of%20copyright-free%20real-world%0Avideos%20from%20the%20internet.%20Through%20a%20carefully%20designed%20rule-based%20filtering%0Astrategy%2C%20we%20ensure%20the%20inclusion%20of%20high-quality%20videos%2C%20resulting%20in%20a%0Acollection%20of%2020K%20human-centric%20videos%20in%201080P%20resolution.%20Human%20and%20camera%0Amotion%20annotation%20is%20accomplished%20using%20a%202D%20pose%20estimator%20and%20a%20SLAM-based%0Amethod.%20For%20the%20synthetic%20data%2C%20we%20gather%202%2C300%20copyright-free%203D%20avatar%20assets%0Ato%20augment%20existing%20available%203D%20assets.%20Notably%2C%20we%20introduce%20a%20rule-based%0Acamera%20trajectory%20generation%20method%2C%20enabling%20the%20synthetic%20pipeline%20to%0Aincorporate%20diverse%20and%20precise%20camera%20motion%20annotation%2C%20which%20can%20rarely%20be%0Afound%20in%20real-world%20data.%20To%20verify%20the%20effectiveness%20of%20HumanVid%2C%20we%20establish%0Aa%20baseline%20model%20named%20CamAnimate%2C%20short%20for%20Camera-controllable%20Human%0AAnimation%2C%20that%20considers%20both%20human%20and%20camera%20motions%20as%20conditions.%20Through%0Aextensive%20experimentation%2C%20we%20demonstrate%20that%20such%20simple%20baseline%20training%20on%0Aour%20HumanVid%20achieves%20state-of-the-art%20performance%20in%20controlling%20both%20human%0Apose%20and%20camera%20motions%2C%20setting%20a%20new%20benchmark.%20Code%20and%20data%20will%20be%0Apublicly%20available%20at%20%5Curl%7Bhttps%3A//github.com/zhenzhiwang/HumanVid/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17438v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumanVid%253A%2520Demystifying%2520Training%2520Data%2520for%2520Camera-controllable%2520Human%2520Image%250A%2520%2520Animation%26entry.906535625%3DZhenzhi%2520Wang%2520and%2520Yixuan%2520Li%2520and%2520Yanhong%2520Zeng%2520and%2520Youqing%2520Fang%2520and%2520Yuwei%2520Guo%2520and%2520Wenran%2520Liu%2520and%2520Jing%2520Tan%2520and%2520Kai%2520Chen%2520and%2520Tianfan%2520Xue%2520and%2520Bo%2520Dai%2520and%2520Dahua%2520Lin%26entry.1292438233%3D%2520%2520Human%2520image%2520animation%2520involves%2520generating%2520videos%2520from%2520a%2520character%2520photo%252C%250Aallowing%2520user%2520control%2520and%2520unlocking%2520potential%2520for%2520video%2520and%2520movie%2520production.%250AWhile%2520recent%2520approaches%2520yield%2520impressive%2520results%2520using%2520high-quality%2520training%250Adata%252C%2520the%2520inaccessibility%2520of%2520these%2520datasets%2520hampers%2520fair%2520and%2520transparent%250Abenchmarking.%2520Moreover%252C%2520these%2520approaches%2520prioritize%25202D%2520human%2520motion%2520and%250Aoverlook%2520the%2520significance%2520of%2520camera%2520motions%2520in%2520videos%252C%2520leading%2520to%2520limited%250Acontrol%2520and%2520unstable%2520video%2520generation.To%2520demystify%2520the%2520training%2520data%252C%2520we%250Apresent%2520HumanVid%252C%2520the%2520first%2520large-scale%2520high-quality%2520dataset%2520tailored%2520for%2520human%250Aimage%2520animation%252C%2520which%2520combines%2520crafted%2520real-world%2520and%2520synthetic%2520data.%2520For%2520the%250Areal-world%2520data%252C%2520we%2520compile%2520a%2520vast%2520collection%2520of%2520copyright-free%2520real-world%250Avideos%2520from%2520the%2520internet.%2520Through%2520a%2520carefully%2520designed%2520rule-based%2520filtering%250Astrategy%252C%2520we%2520ensure%2520the%2520inclusion%2520of%2520high-quality%2520videos%252C%2520resulting%2520in%2520a%250Acollection%2520of%252020K%2520human-centric%2520videos%2520in%25201080P%2520resolution.%2520Human%2520and%2520camera%250Amotion%2520annotation%2520is%2520accomplished%2520using%2520a%25202D%2520pose%2520estimator%2520and%2520a%2520SLAM-based%250Amethod.%2520For%2520the%2520synthetic%2520data%252C%2520we%2520gather%25202%252C300%2520copyright-free%25203D%2520avatar%2520assets%250Ato%2520augment%2520existing%2520available%25203D%2520assets.%2520Notably%252C%2520we%2520introduce%2520a%2520rule-based%250Acamera%2520trajectory%2520generation%2520method%252C%2520enabling%2520the%2520synthetic%2520pipeline%2520to%250Aincorporate%2520diverse%2520and%2520precise%2520camera%2520motion%2520annotation%252C%2520which%2520can%2520rarely%2520be%250Afound%2520in%2520real-world%2520data.%2520To%2520verify%2520the%2520effectiveness%2520of%2520HumanVid%252C%2520we%2520establish%250Aa%2520baseline%2520model%2520named%2520CamAnimate%252C%2520short%2520for%2520Camera-controllable%2520Human%250AAnimation%252C%2520that%2520considers%2520both%2520human%2520and%2520camera%2520motions%2520as%2520conditions.%2520Through%250Aextensive%2520experimentation%252C%2520we%2520demonstrate%2520that%2520such%2520simple%2520baseline%2520training%2520on%250Aour%2520HumanVid%2520achieves%2520state-of-the-art%2520performance%2520in%2520controlling%2520both%2520human%250Apose%2520and%2520camera%2520motions%252C%2520setting%2520a%2520new%2520benchmark.%2520Code%2520and%2520data%2520will%2520be%250Apublicly%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/zhenzhiwang/HumanVid/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17438v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HumanVid%3A%20Demystifying%20Training%20Data%20for%20Camera-controllable%20Human%20Image%0A%20%20Animation&entry.906535625=Zhenzhi%20Wang%20and%20Yixuan%20Li%20and%20Yanhong%20Zeng%20and%20Youqing%20Fang%20and%20Yuwei%20Guo%20and%20Wenran%20Liu%20and%20Jing%20Tan%20and%20Kai%20Chen%20and%20Tianfan%20Xue%20and%20Bo%20Dai%20and%20Dahua%20Lin&entry.1292438233=%20%20Human%20image%20animation%20involves%20generating%20videos%20from%20a%20character%20photo%2C%0Aallowing%20user%20control%20and%20unlocking%20potential%20for%20video%20and%20movie%20production.%0AWhile%20recent%20approaches%20yield%20impressive%20results%20using%20high-quality%20training%0Adata%2C%20the%20inaccessibility%20of%20these%20datasets%20hampers%20fair%20and%20transparent%0Abenchmarking.%20Moreover%2C%20these%20approaches%20prioritize%202D%20human%20motion%20and%0Aoverlook%20the%20significance%20of%20camera%20motions%20in%20videos%2C%20leading%20to%20limited%0Acontrol%20and%20unstable%20video%20generation.To%20demystify%20the%20training%20data%2C%20we%0Apresent%20HumanVid%2C%20the%20first%20large-scale%20high-quality%20dataset%20tailored%20for%20human%0Aimage%20animation%2C%20which%20combines%20crafted%20real-world%20and%20synthetic%20data.%20For%20the%0Areal-world%20data%2C%20we%20compile%20a%20vast%20collection%20of%20copyright-free%20real-world%0Avideos%20from%20the%20internet.%20Through%20a%20carefully%20designed%20rule-based%20filtering%0Astrategy%2C%20we%20ensure%20the%20inclusion%20of%20high-quality%20videos%2C%20resulting%20in%20a%0Acollection%20of%2020K%20human-centric%20videos%20in%201080P%20resolution.%20Human%20and%20camera%0Amotion%20annotation%20is%20accomplished%20using%20a%202D%20pose%20estimator%20and%20a%20SLAM-based%0Amethod.%20For%20the%20synthetic%20data%2C%20we%20gather%202%2C300%20copyright-free%203D%20avatar%20assets%0Ato%20augment%20existing%20available%203D%20assets.%20Notably%2C%20we%20introduce%20a%20rule-based%0Acamera%20trajectory%20generation%20method%2C%20enabling%20the%20synthetic%20pipeline%20to%0Aincorporate%20diverse%20and%20precise%20camera%20motion%20annotation%2C%20which%20can%20rarely%20be%0Afound%20in%20real-world%20data.%20To%20verify%20the%20effectiveness%20of%20HumanVid%2C%20we%20establish%0Aa%20baseline%20model%20named%20CamAnimate%2C%20short%20for%20Camera-controllable%20Human%0AAnimation%2C%20that%20considers%20both%20human%20and%20camera%20motions%20as%20conditions.%20Through%0Aextensive%20experimentation%2C%20we%20demonstrate%20that%20such%20simple%20baseline%20training%20on%0Aour%20HumanVid%20achieves%20state-of-the-art%20performance%20in%20controlling%20both%20human%0Apose%20and%20camera%20motions%2C%20setting%20a%20new%20benchmark.%20Code%20and%20data%20will%20be%0Apublicly%20available%20at%20%5Curl%7Bhttps%3A//github.com/zhenzhiwang/HumanVid/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17438v1&entry.124074799=Read"},
{"title": "Trackastra: Transformer-based cell tracking for live-cell microscopy", "author": "Benjamin Gallusser and Martin Weigert", "abstract": "  Cell tracking is a ubiquitous image analysis task in live-cell microscopy.\nUnlike multiple object tracking (MOT) for natural images, cell tracking\ntypically involves hundreds of similar-looking objects that can divide in each\nframe, making it a particularly challenging problem. Current state-of-the-art\napproaches follow the tracking-by-detection paradigm, i.e. first all cells are\ndetected per frame and successively linked in a second step to form\nbiologically consistent cell tracks. Linking is commonly solved via discrete\noptimization methods, which require manual tuning of hyperparameters for each\ndataset and are therefore cumbersome to use in practice. Here we propose\nTrackastra, a general purpose cell tracking approach that uses a simple\ntransformer architecture to directly learn pairwise associations of cells\nwithin a temporal window from annotated data. Importantly, unlike existing\ntransformer-based MOT pipelines, our learning architecture also accounts for\ndividing objects such as cells and allows for accurate tracking even with\nsimple greedy linking, thus making strides towards removing the requirement for\na complex linking step. The proposed architecture operates on the full\nspatio-temporal context of detections within a time window by avoiding the\ncomputational burden of processing dense images. We show that our tracking\napproach performs on par with or better than highly tuned state-of-the-art cell\ntracking algorithms for various biological datasets, such as bacteria, cell\ncultures and fluorescent particles. We provide code at\nhttps://github.com/weigertlab/trackastra.\n", "link": "http://arxiv.org/abs/2405.15700v2", "date": "2024-07-24", "relevancy": 2.551, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5151}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5121}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trackastra%3A%20Transformer-based%20cell%20tracking%20for%20live-cell%20microscopy&body=Title%3A%20Trackastra%3A%20Transformer-based%20cell%20tracking%20for%20live-cell%20microscopy%0AAuthor%3A%20Benjamin%20Gallusser%20and%20Martin%20Weigert%0AAbstract%3A%20%20%20Cell%20tracking%20is%20a%20ubiquitous%20image%20analysis%20task%20in%20live-cell%20microscopy.%0AUnlike%20multiple%20object%20tracking%20%28MOT%29%20for%20natural%20images%2C%20cell%20tracking%0Atypically%20involves%20hundreds%20of%20similar-looking%20objects%20that%20can%20divide%20in%20each%0Aframe%2C%20making%20it%20a%20particularly%20challenging%20problem.%20Current%20state-of-the-art%0Aapproaches%20follow%20the%20tracking-by-detection%20paradigm%2C%20i.e.%20first%20all%20cells%20are%0Adetected%20per%20frame%20and%20successively%20linked%20in%20a%20second%20step%20to%20form%0Abiologically%20consistent%20cell%20tracks.%20Linking%20is%20commonly%20solved%20via%20discrete%0Aoptimization%20methods%2C%20which%20require%20manual%20tuning%20of%20hyperparameters%20for%20each%0Adataset%20and%20are%20therefore%20cumbersome%20to%20use%20in%20practice.%20Here%20we%20propose%0ATrackastra%2C%20a%20general%20purpose%20cell%20tracking%20approach%20that%20uses%20a%20simple%0Atransformer%20architecture%20to%20directly%20learn%20pairwise%20associations%20of%20cells%0Awithin%20a%20temporal%20window%20from%20annotated%20data.%20Importantly%2C%20unlike%20existing%0Atransformer-based%20MOT%20pipelines%2C%20our%20learning%20architecture%20also%20accounts%20for%0Adividing%20objects%20such%20as%20cells%20and%20allows%20for%20accurate%20tracking%20even%20with%0Asimple%20greedy%20linking%2C%20thus%20making%20strides%20towards%20removing%20the%20requirement%20for%0Aa%20complex%20linking%20step.%20The%20proposed%20architecture%20operates%20on%20the%20full%0Aspatio-temporal%20context%20of%20detections%20within%20a%20time%20window%20by%20avoiding%20the%0Acomputational%20burden%20of%20processing%20dense%20images.%20We%20show%20that%20our%20tracking%0Aapproach%20performs%20on%20par%20with%20or%20better%20than%20highly%20tuned%20state-of-the-art%20cell%0Atracking%20algorithms%20for%20various%20biological%20datasets%2C%20such%20as%20bacteria%2C%20cell%0Acultures%20and%20fluorescent%20particles.%20We%20provide%20code%20at%0Ahttps%3A//github.com/weigertlab/trackastra.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15700v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrackastra%253A%2520Transformer-based%2520cell%2520tracking%2520for%2520live-cell%2520microscopy%26entry.906535625%3DBenjamin%2520Gallusser%2520and%2520Martin%2520Weigert%26entry.1292438233%3D%2520%2520Cell%2520tracking%2520is%2520a%2520ubiquitous%2520image%2520analysis%2520task%2520in%2520live-cell%2520microscopy.%250AUnlike%2520multiple%2520object%2520tracking%2520%2528MOT%2529%2520for%2520natural%2520images%252C%2520cell%2520tracking%250Atypically%2520involves%2520hundreds%2520of%2520similar-looking%2520objects%2520that%2520can%2520divide%2520in%2520each%250Aframe%252C%2520making%2520it%2520a%2520particularly%2520challenging%2520problem.%2520Current%2520state-of-the-art%250Aapproaches%2520follow%2520the%2520tracking-by-detection%2520paradigm%252C%2520i.e.%2520first%2520all%2520cells%2520are%250Adetected%2520per%2520frame%2520and%2520successively%2520linked%2520in%2520a%2520second%2520step%2520to%2520form%250Abiologically%2520consistent%2520cell%2520tracks.%2520Linking%2520is%2520commonly%2520solved%2520via%2520discrete%250Aoptimization%2520methods%252C%2520which%2520require%2520manual%2520tuning%2520of%2520hyperparameters%2520for%2520each%250Adataset%2520and%2520are%2520therefore%2520cumbersome%2520to%2520use%2520in%2520practice.%2520Here%2520we%2520propose%250ATrackastra%252C%2520a%2520general%2520purpose%2520cell%2520tracking%2520approach%2520that%2520uses%2520a%2520simple%250Atransformer%2520architecture%2520to%2520directly%2520learn%2520pairwise%2520associations%2520of%2520cells%250Awithin%2520a%2520temporal%2520window%2520from%2520annotated%2520data.%2520Importantly%252C%2520unlike%2520existing%250Atransformer-based%2520MOT%2520pipelines%252C%2520our%2520learning%2520architecture%2520also%2520accounts%2520for%250Adividing%2520objects%2520such%2520as%2520cells%2520and%2520allows%2520for%2520accurate%2520tracking%2520even%2520with%250Asimple%2520greedy%2520linking%252C%2520thus%2520making%2520strides%2520towards%2520removing%2520the%2520requirement%2520for%250Aa%2520complex%2520linking%2520step.%2520The%2520proposed%2520architecture%2520operates%2520on%2520the%2520full%250Aspatio-temporal%2520context%2520of%2520detections%2520within%2520a%2520time%2520window%2520by%2520avoiding%2520the%250Acomputational%2520burden%2520of%2520processing%2520dense%2520images.%2520We%2520show%2520that%2520our%2520tracking%250Aapproach%2520performs%2520on%2520par%2520with%2520or%2520better%2520than%2520highly%2520tuned%2520state-of-the-art%2520cell%250Atracking%2520algorithms%2520for%2520various%2520biological%2520datasets%252C%2520such%2520as%2520bacteria%252C%2520cell%250Acultures%2520and%2520fluorescent%2520particles.%2520We%2520provide%2520code%2520at%250Ahttps%253A//github.com/weigertlab/trackastra.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15700v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trackastra%3A%20Transformer-based%20cell%20tracking%20for%20live-cell%20microscopy&entry.906535625=Benjamin%20Gallusser%20and%20Martin%20Weigert&entry.1292438233=%20%20Cell%20tracking%20is%20a%20ubiquitous%20image%20analysis%20task%20in%20live-cell%20microscopy.%0AUnlike%20multiple%20object%20tracking%20%28MOT%29%20for%20natural%20images%2C%20cell%20tracking%0Atypically%20involves%20hundreds%20of%20similar-looking%20objects%20that%20can%20divide%20in%20each%0Aframe%2C%20making%20it%20a%20particularly%20challenging%20problem.%20Current%20state-of-the-art%0Aapproaches%20follow%20the%20tracking-by-detection%20paradigm%2C%20i.e.%20first%20all%20cells%20are%0Adetected%20per%20frame%20and%20successively%20linked%20in%20a%20second%20step%20to%20form%0Abiologically%20consistent%20cell%20tracks.%20Linking%20is%20commonly%20solved%20via%20discrete%0Aoptimization%20methods%2C%20which%20require%20manual%20tuning%20of%20hyperparameters%20for%20each%0Adataset%20and%20are%20therefore%20cumbersome%20to%20use%20in%20practice.%20Here%20we%20propose%0ATrackastra%2C%20a%20general%20purpose%20cell%20tracking%20approach%20that%20uses%20a%20simple%0Atransformer%20architecture%20to%20directly%20learn%20pairwise%20associations%20of%20cells%0Awithin%20a%20temporal%20window%20from%20annotated%20data.%20Importantly%2C%20unlike%20existing%0Atransformer-based%20MOT%20pipelines%2C%20our%20learning%20architecture%20also%20accounts%20for%0Adividing%20objects%20such%20as%20cells%20and%20allows%20for%20accurate%20tracking%20even%20with%0Asimple%20greedy%20linking%2C%20thus%20making%20strides%20towards%20removing%20the%20requirement%20for%0Aa%20complex%20linking%20step.%20The%20proposed%20architecture%20operates%20on%20the%20full%0Aspatio-temporal%20context%20of%20detections%20within%20a%20time%20window%20by%20avoiding%20the%0Acomputational%20burden%20of%20processing%20dense%20images.%20We%20show%20that%20our%20tracking%0Aapproach%20performs%20on%20par%20with%20or%20better%20than%20highly%20tuned%20state-of-the-art%20cell%0Atracking%20algorithms%20for%20various%20biological%20datasets%2C%20such%20as%20bacteria%2C%20cell%0Acultures%20and%20fluorescent%20particles.%20We%20provide%20code%20at%0Ahttps%3A//github.com/weigertlab/trackastra.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15700v2&entry.124074799=Read"},
{"title": "DenseTrack: Drone-based Crowd Tracking via Density-aware\n  Motion-appearance Synergy", "author": "Yi Lei and Huilin Zhu and Jingling Yuan and Guangli Xiang and Xian Zhong and Shengfeng He", "abstract": "  Drone-based crowd tracking faces difficulties in accurately identifying and\nmonitoring objects from an aerial perspective, largely due to their small size\nand close proximity to each other, which complicates both localization and\ntracking. To address these challenges, we present the Density-aware Tracking\n(DenseTrack) framework. DenseTrack capitalizes on crowd counting to precisely\ndetermine object locations, blending visual and motion cues to improve the\ntracking of small-scale objects. It specifically addresses the problem of\ncross-frame motion to enhance tracking accuracy and dependability. DenseTrack\nemploys crowd density estimates as anchors for exact object localization within\nvideo frames. These estimates are merged with motion and position information\nfrom the tracking network, with motion offsets serving as key tracking cues.\nMoreover, DenseTrack enhances the ability to distinguish small-scale objects\nusing insights from the visual-language model, integrating appearance with\nmotion cues. The framework utilizes the Hungarian algorithm to ensure the\naccurate matching of individuals across frames. Demonstrated on DroneCrowd\ndataset, our approach exhibits superior performance, confirming its\neffectiveness in scenarios captured by drones.\n", "link": "http://arxiv.org/abs/2407.17272v1", "date": "2024-07-24", "relevancy": 2.5456, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5207}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5047}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DenseTrack%3A%20Drone-based%20Crowd%20Tracking%20via%20Density-aware%0A%20%20Motion-appearance%20Synergy&body=Title%3A%20DenseTrack%3A%20Drone-based%20Crowd%20Tracking%20via%20Density-aware%0A%20%20Motion-appearance%20Synergy%0AAuthor%3A%20Yi%20Lei%20and%20Huilin%20Zhu%20and%20Jingling%20Yuan%20and%20Guangli%20Xiang%20and%20Xian%20Zhong%20and%20Shengfeng%20He%0AAbstract%3A%20%20%20Drone-based%20crowd%20tracking%20faces%20difficulties%20in%20accurately%20identifying%20and%0Amonitoring%20objects%20from%20an%20aerial%20perspective%2C%20largely%20due%20to%20their%20small%20size%0Aand%20close%20proximity%20to%20each%20other%2C%20which%20complicates%20both%20localization%20and%0Atracking.%20To%20address%20these%20challenges%2C%20we%20present%20the%20Density-aware%20Tracking%0A%28DenseTrack%29%20framework.%20DenseTrack%20capitalizes%20on%20crowd%20counting%20to%20precisely%0Adetermine%20object%20locations%2C%20blending%20visual%20and%20motion%20cues%20to%20improve%20the%0Atracking%20of%20small-scale%20objects.%20It%20specifically%20addresses%20the%20problem%20of%0Across-frame%20motion%20to%20enhance%20tracking%20accuracy%20and%20dependability.%20DenseTrack%0Aemploys%20crowd%20density%20estimates%20as%20anchors%20for%20exact%20object%20localization%20within%0Avideo%20frames.%20These%20estimates%20are%20merged%20with%20motion%20and%20position%20information%0Afrom%20the%20tracking%20network%2C%20with%20motion%20offsets%20serving%20as%20key%20tracking%20cues.%0AMoreover%2C%20DenseTrack%20enhances%20the%20ability%20to%20distinguish%20small-scale%20objects%0Ausing%20insights%20from%20the%20visual-language%20model%2C%20integrating%20appearance%20with%0Amotion%20cues.%20The%20framework%20utilizes%20the%20Hungarian%20algorithm%20to%20ensure%20the%0Aaccurate%20matching%20of%20individuals%20across%20frames.%20Demonstrated%20on%20DroneCrowd%0Adataset%2C%20our%20approach%20exhibits%20superior%20performance%2C%20confirming%20its%0Aeffectiveness%20in%20scenarios%20captured%20by%20drones.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17272v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDenseTrack%253A%2520Drone-based%2520Crowd%2520Tracking%2520via%2520Density-aware%250A%2520%2520Motion-appearance%2520Synergy%26entry.906535625%3DYi%2520Lei%2520and%2520Huilin%2520Zhu%2520and%2520Jingling%2520Yuan%2520and%2520Guangli%2520Xiang%2520and%2520Xian%2520Zhong%2520and%2520Shengfeng%2520He%26entry.1292438233%3D%2520%2520Drone-based%2520crowd%2520tracking%2520faces%2520difficulties%2520in%2520accurately%2520identifying%2520and%250Amonitoring%2520objects%2520from%2520an%2520aerial%2520perspective%252C%2520largely%2520due%2520to%2520their%2520small%2520size%250Aand%2520close%2520proximity%2520to%2520each%2520other%252C%2520which%2520complicates%2520both%2520localization%2520and%250Atracking.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%2520the%2520Density-aware%2520Tracking%250A%2528DenseTrack%2529%2520framework.%2520DenseTrack%2520capitalizes%2520on%2520crowd%2520counting%2520to%2520precisely%250Adetermine%2520object%2520locations%252C%2520blending%2520visual%2520and%2520motion%2520cues%2520to%2520improve%2520the%250Atracking%2520of%2520small-scale%2520objects.%2520It%2520specifically%2520addresses%2520the%2520problem%2520of%250Across-frame%2520motion%2520to%2520enhance%2520tracking%2520accuracy%2520and%2520dependability.%2520DenseTrack%250Aemploys%2520crowd%2520density%2520estimates%2520as%2520anchors%2520for%2520exact%2520object%2520localization%2520within%250Avideo%2520frames.%2520These%2520estimates%2520are%2520merged%2520with%2520motion%2520and%2520position%2520information%250Afrom%2520the%2520tracking%2520network%252C%2520with%2520motion%2520offsets%2520serving%2520as%2520key%2520tracking%2520cues.%250AMoreover%252C%2520DenseTrack%2520enhances%2520the%2520ability%2520to%2520distinguish%2520small-scale%2520objects%250Ausing%2520insights%2520from%2520the%2520visual-language%2520model%252C%2520integrating%2520appearance%2520with%250Amotion%2520cues.%2520The%2520framework%2520utilizes%2520the%2520Hungarian%2520algorithm%2520to%2520ensure%2520the%250Aaccurate%2520matching%2520of%2520individuals%2520across%2520frames.%2520Demonstrated%2520on%2520DroneCrowd%250Adataset%252C%2520our%2520approach%2520exhibits%2520superior%2520performance%252C%2520confirming%2520its%250Aeffectiveness%2520in%2520scenarios%2520captured%2520by%2520drones.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17272v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DenseTrack%3A%20Drone-based%20Crowd%20Tracking%20via%20Density-aware%0A%20%20Motion-appearance%20Synergy&entry.906535625=Yi%20Lei%20and%20Huilin%20Zhu%20and%20Jingling%20Yuan%20and%20Guangli%20Xiang%20and%20Xian%20Zhong%20and%20Shengfeng%20He&entry.1292438233=%20%20Drone-based%20crowd%20tracking%20faces%20difficulties%20in%20accurately%20identifying%20and%0Amonitoring%20objects%20from%20an%20aerial%20perspective%2C%20largely%20due%20to%20their%20small%20size%0Aand%20close%20proximity%20to%20each%20other%2C%20which%20complicates%20both%20localization%20and%0Atracking.%20To%20address%20these%20challenges%2C%20we%20present%20the%20Density-aware%20Tracking%0A%28DenseTrack%29%20framework.%20DenseTrack%20capitalizes%20on%20crowd%20counting%20to%20precisely%0Adetermine%20object%20locations%2C%20blending%20visual%20and%20motion%20cues%20to%20improve%20the%0Atracking%20of%20small-scale%20objects.%20It%20specifically%20addresses%20the%20problem%20of%0Across-frame%20motion%20to%20enhance%20tracking%20accuracy%20and%20dependability.%20DenseTrack%0Aemploys%20crowd%20density%20estimates%20as%20anchors%20for%20exact%20object%20localization%20within%0Avideo%20frames.%20These%20estimates%20are%20merged%20with%20motion%20and%20position%20information%0Afrom%20the%20tracking%20network%2C%20with%20motion%20offsets%20serving%20as%20key%20tracking%20cues.%0AMoreover%2C%20DenseTrack%20enhances%20the%20ability%20to%20distinguish%20small-scale%20objects%0Ausing%20insights%20from%20the%20visual-language%20model%2C%20integrating%20appearance%20with%0Amotion%20cues.%20The%20framework%20utilizes%20the%20Hungarian%20algorithm%20to%20ensure%20the%0Aaccurate%20matching%20of%20individuals%20across%20frames.%20Demonstrated%20on%20DroneCrowd%0Adataset%2C%20our%20approach%20exhibits%20superior%20performance%2C%20confirming%20its%0Aeffectiveness%20in%20scenarios%20captured%20by%20drones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17272v1&entry.124074799=Read"},
{"title": "Copyright Protection in Generative AI: A Technical Perspective", "author": "Jie Ren and Han Xu and Pengfei He and Yingqian Cui and Shenglai Zeng and Jiankun Zhang and Hongzhi Wen and Jiayuan Ding and Pei Huang and Lingjuan Lyu and Hui Liu and Yi Chang and Jiliang Tang", "abstract": "  Generative AI has witnessed rapid advancement in recent years, expanding\ntheir capabilities to create synthesized content such as text, images, audio,\nand code. The high fidelity and authenticity of contents generated by these\nDeep Generative Models (DGMs) have sparked significant copyright concerns.\nThere have been various legal debates on how to effectively safeguard\ncopyrights in DGMs. This work delves into this issue by providing a\ncomprehensive overview of copyright protection from a technical perspective. We\nexamine from two distinct viewpoints: the copyrights pertaining to the source\ndata held by the data owners and those of the generative models maintained by\nthe model builders. For data copyright, we delve into methods data owners can\nprotect their content and DGMs can be utilized without infringing upon these\nrights. For model copyright, our discussion extends to strategies for\npreventing model theft and identifying outputs generated by specific models.\nFinally, we highlight the limitations of existing techniques and identify areas\nthat remain unexplored. Furthermore, we discuss prospective directions for the\nfuture of copyright protection, underscoring its importance for the sustainable\nand ethical development of Generative AI.\n", "link": "http://arxiv.org/abs/2402.02333v2", "date": "2024-07-24", "relevancy": 2.543, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5221}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5094}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Copyright%20Protection%20in%20Generative%20AI%3A%20A%20Technical%20Perspective&body=Title%3A%20Copyright%20Protection%20in%20Generative%20AI%3A%20A%20Technical%20Perspective%0AAuthor%3A%20Jie%20Ren%20and%20Han%20Xu%20and%20Pengfei%20He%20and%20Yingqian%20Cui%20and%20Shenglai%20Zeng%20and%20Jiankun%20Zhang%20and%20Hongzhi%20Wen%20and%20Jiayuan%20Ding%20and%20Pei%20Huang%20and%20Lingjuan%20Lyu%20and%20Hui%20Liu%20and%20Yi%20Chang%20and%20Jiliang%20Tang%0AAbstract%3A%20%20%20Generative%20AI%20has%20witnessed%20rapid%20advancement%20in%20recent%20years%2C%20expanding%0Atheir%20capabilities%20to%20create%20synthesized%20content%20such%20as%20text%2C%20images%2C%20audio%2C%0Aand%20code.%20The%20high%20fidelity%20and%20authenticity%20of%20contents%20generated%20by%20these%0ADeep%20Generative%20Models%20%28DGMs%29%20have%20sparked%20significant%20copyright%20concerns.%0AThere%20have%20been%20various%20legal%20debates%20on%20how%20to%20effectively%20safeguard%0Acopyrights%20in%20DGMs.%20This%20work%20delves%20into%20this%20issue%20by%20providing%20a%0Acomprehensive%20overview%20of%20copyright%20protection%20from%20a%20technical%20perspective.%20We%0Aexamine%20from%20two%20distinct%20viewpoints%3A%20the%20copyrights%20pertaining%20to%20the%20source%0Adata%20held%20by%20the%20data%20owners%20and%20those%20of%20the%20generative%20models%20maintained%20by%0Athe%20model%20builders.%20For%20data%20copyright%2C%20we%20delve%20into%20methods%20data%20owners%20can%0Aprotect%20their%20content%20and%20DGMs%20can%20be%20utilized%20without%20infringing%20upon%20these%0Arights.%20For%20model%20copyright%2C%20our%20discussion%20extends%20to%20strategies%20for%0Apreventing%20model%20theft%20and%20identifying%20outputs%20generated%20by%20specific%20models.%0AFinally%2C%20we%20highlight%20the%20limitations%20of%20existing%20techniques%20and%20identify%20areas%0Athat%20remain%20unexplored.%20Furthermore%2C%20we%20discuss%20prospective%20directions%20for%20the%0Afuture%20of%20copyright%20protection%2C%20underscoring%20its%20importance%20for%20the%20sustainable%0Aand%20ethical%20development%20of%20Generative%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02333v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCopyright%2520Protection%2520in%2520Generative%2520AI%253A%2520A%2520Technical%2520Perspective%26entry.906535625%3DJie%2520Ren%2520and%2520Han%2520Xu%2520and%2520Pengfei%2520He%2520and%2520Yingqian%2520Cui%2520and%2520Shenglai%2520Zeng%2520and%2520Jiankun%2520Zhang%2520and%2520Hongzhi%2520Wen%2520and%2520Jiayuan%2520Ding%2520and%2520Pei%2520Huang%2520and%2520Lingjuan%2520Lyu%2520and%2520Hui%2520Liu%2520and%2520Yi%2520Chang%2520and%2520Jiliang%2520Tang%26entry.1292438233%3D%2520%2520Generative%2520AI%2520has%2520witnessed%2520rapid%2520advancement%2520in%2520recent%2520years%252C%2520expanding%250Atheir%2520capabilities%2520to%2520create%2520synthesized%2520content%2520such%2520as%2520text%252C%2520images%252C%2520audio%252C%250Aand%2520code.%2520The%2520high%2520fidelity%2520and%2520authenticity%2520of%2520contents%2520generated%2520by%2520these%250ADeep%2520Generative%2520Models%2520%2528DGMs%2529%2520have%2520sparked%2520significant%2520copyright%2520concerns.%250AThere%2520have%2520been%2520various%2520legal%2520debates%2520on%2520how%2520to%2520effectively%2520safeguard%250Acopyrights%2520in%2520DGMs.%2520This%2520work%2520delves%2520into%2520this%2520issue%2520by%2520providing%2520a%250Acomprehensive%2520overview%2520of%2520copyright%2520protection%2520from%2520a%2520technical%2520perspective.%2520We%250Aexamine%2520from%2520two%2520distinct%2520viewpoints%253A%2520the%2520copyrights%2520pertaining%2520to%2520the%2520source%250Adata%2520held%2520by%2520the%2520data%2520owners%2520and%2520those%2520of%2520the%2520generative%2520models%2520maintained%2520by%250Athe%2520model%2520builders.%2520For%2520data%2520copyright%252C%2520we%2520delve%2520into%2520methods%2520data%2520owners%2520can%250Aprotect%2520their%2520content%2520and%2520DGMs%2520can%2520be%2520utilized%2520without%2520infringing%2520upon%2520these%250Arights.%2520For%2520model%2520copyright%252C%2520our%2520discussion%2520extends%2520to%2520strategies%2520for%250Apreventing%2520model%2520theft%2520and%2520identifying%2520outputs%2520generated%2520by%2520specific%2520models.%250AFinally%252C%2520we%2520highlight%2520the%2520limitations%2520of%2520existing%2520techniques%2520and%2520identify%2520areas%250Athat%2520remain%2520unexplored.%2520Furthermore%252C%2520we%2520discuss%2520prospective%2520directions%2520for%2520the%250Afuture%2520of%2520copyright%2520protection%252C%2520underscoring%2520its%2520importance%2520for%2520the%2520sustainable%250Aand%2520ethical%2520development%2520of%2520Generative%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02333v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Copyright%20Protection%20in%20Generative%20AI%3A%20A%20Technical%20Perspective&entry.906535625=Jie%20Ren%20and%20Han%20Xu%20and%20Pengfei%20He%20and%20Yingqian%20Cui%20and%20Shenglai%20Zeng%20and%20Jiankun%20Zhang%20and%20Hongzhi%20Wen%20and%20Jiayuan%20Ding%20and%20Pei%20Huang%20and%20Lingjuan%20Lyu%20and%20Hui%20Liu%20and%20Yi%20Chang%20and%20Jiliang%20Tang&entry.1292438233=%20%20Generative%20AI%20has%20witnessed%20rapid%20advancement%20in%20recent%20years%2C%20expanding%0Atheir%20capabilities%20to%20create%20synthesized%20content%20such%20as%20text%2C%20images%2C%20audio%2C%0Aand%20code.%20The%20high%20fidelity%20and%20authenticity%20of%20contents%20generated%20by%20these%0ADeep%20Generative%20Models%20%28DGMs%29%20have%20sparked%20significant%20copyright%20concerns.%0AThere%20have%20been%20various%20legal%20debates%20on%20how%20to%20effectively%20safeguard%0Acopyrights%20in%20DGMs.%20This%20work%20delves%20into%20this%20issue%20by%20providing%20a%0Acomprehensive%20overview%20of%20copyright%20protection%20from%20a%20technical%20perspective.%20We%0Aexamine%20from%20two%20distinct%20viewpoints%3A%20the%20copyrights%20pertaining%20to%20the%20source%0Adata%20held%20by%20the%20data%20owners%20and%20those%20of%20the%20generative%20models%20maintained%20by%0Athe%20model%20builders.%20For%20data%20copyright%2C%20we%20delve%20into%20methods%20data%20owners%20can%0Aprotect%20their%20content%20and%20DGMs%20can%20be%20utilized%20without%20infringing%20upon%20these%0Arights.%20For%20model%20copyright%2C%20our%20discussion%20extends%20to%20strategies%20for%0Apreventing%20model%20theft%20and%20identifying%20outputs%20generated%20by%20specific%20models.%0AFinally%2C%20we%20highlight%20the%20limitations%20of%20existing%20techniques%20and%20identify%20areas%0Athat%20remain%20unexplored.%20Furthermore%2C%20we%20discuss%20prospective%20directions%20for%20the%0Afuture%20of%20copyright%20protection%2C%20underscoring%20its%20importance%20for%20the%20sustainable%0Aand%20ethical%20development%20of%20Generative%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02333v2&entry.124074799=Read"},
{"title": "(PASS) Visual Prompt Locates Good Structure Sparsity through a Recurrent\n  HyperNetwork", "author": "Tianjin Huang and Fang Meng and Li Shen and Fan Liu and Yulong Pei and Mykola Pechenizkiy and Shiwei Liu and Tianlong Chen", "abstract": "  Large-scale neural networks have demonstrated remarkable performance in\ndifferent domains like vision and language processing, although at the cost of\nmassive computation resources. As illustrated by compression literature,\nstructural model pruning is a prominent algorithm to encourage model\nefficiency, thanks to its acceleration-friendly sparsity patterns. One of the\nkey questions of structural pruning is how to estimate the channel\nsignificance. In parallel, work on data-centric AI has shown that\nprompting-based techniques enable impressive generalization of large language\nmodels across diverse downstream tasks. In this paper, we investigate a\ncharming possibility - \\textit{leveraging visual prompts to capture the channel\nimportance and derive high-quality structural sparsity}. To this end, we\npropose a novel algorithmic framework, namely \\texttt{PASS}. It is a tailored\nhyper-network to take both visual prompts and network weight statistics as\ninput, and output layer-wise channel sparsity in a recurrent manner. Such\ndesigns consider the intrinsic channel dependency between layers. Comprehensive\nexperiments across multiple network architectures and six datasets demonstrate\nthe superiority of \\texttt{PASS} in locating good structural sparsity. For\nexample, at the same FLOPs level, \\texttt{PASS} subnetworks achieve $1\\%\\sim\n3\\%$ better accuracy on Food101 dataset; or with a similar performance of\n$80\\%$ accuracy, \\texttt{PASS} subnetworks obtain $0.35\\times$ more speedup\nthan the baselines.\n", "link": "http://arxiv.org/abs/2407.17412v1", "date": "2024-07-24", "relevancy": 2.522, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5123}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5059}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%28PASS%29%20Visual%20Prompt%20Locates%20Good%20Structure%20Sparsity%20through%20a%20Recurrent%0A%20%20HyperNetwork&body=Title%3A%20%28PASS%29%20Visual%20Prompt%20Locates%20Good%20Structure%20Sparsity%20through%20a%20Recurrent%0A%20%20HyperNetwork%0AAuthor%3A%20Tianjin%20Huang%20and%20Fang%20Meng%20and%20Li%20Shen%20and%20Fan%20Liu%20and%20Yulong%20Pei%20and%20Mykola%20Pechenizkiy%20and%20Shiwei%20Liu%20and%20Tianlong%20Chen%0AAbstract%3A%20%20%20Large-scale%20neural%20networks%20have%20demonstrated%20remarkable%20performance%20in%0Adifferent%20domains%20like%20vision%20and%20language%20processing%2C%20although%20at%20the%20cost%20of%0Amassive%20computation%20resources.%20As%20illustrated%20by%20compression%20literature%2C%0Astructural%20model%20pruning%20is%20a%20prominent%20algorithm%20to%20encourage%20model%0Aefficiency%2C%20thanks%20to%20its%20acceleration-friendly%20sparsity%20patterns.%20One%20of%20the%0Akey%20questions%20of%20structural%20pruning%20is%20how%20to%20estimate%20the%20channel%0Asignificance.%20In%20parallel%2C%20work%20on%20data-centric%20AI%20has%20shown%20that%0Aprompting-based%20techniques%20enable%20impressive%20generalization%20of%20large%20language%0Amodels%20across%20diverse%20downstream%20tasks.%20In%20this%20paper%2C%20we%20investigate%20a%0Acharming%20possibility%20-%20%5Ctextit%7Bleveraging%20visual%20prompts%20to%20capture%20the%20channel%0Aimportance%20and%20derive%20high-quality%20structural%20sparsity%7D.%20To%20this%20end%2C%20we%0Apropose%20a%20novel%20algorithmic%20framework%2C%20namely%20%5Ctexttt%7BPASS%7D.%20It%20is%20a%20tailored%0Ahyper-network%20to%20take%20both%20visual%20prompts%20and%20network%20weight%20statistics%20as%0Ainput%2C%20and%20output%20layer-wise%20channel%20sparsity%20in%20a%20recurrent%20manner.%20Such%0Adesigns%20consider%20the%20intrinsic%20channel%20dependency%20between%20layers.%20Comprehensive%0Aexperiments%20across%20multiple%20network%20architectures%20and%20six%20datasets%20demonstrate%0Athe%20superiority%20of%20%5Ctexttt%7BPASS%7D%20in%20locating%20good%20structural%20sparsity.%20For%0Aexample%2C%20at%20the%20same%20FLOPs%20level%2C%20%5Ctexttt%7BPASS%7D%20subnetworks%20achieve%20%241%5C%25%5Csim%0A3%5C%25%24%20better%20accuracy%20on%20Food101%20dataset%3B%20or%20with%20a%20similar%20performance%20of%0A%2480%5C%25%24%20accuracy%2C%20%5Ctexttt%7BPASS%7D%20subnetworks%20obtain%20%240.35%5Ctimes%24%20more%20speedup%0Athan%20the%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17412v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2528PASS%2529%2520Visual%2520Prompt%2520Locates%2520Good%2520Structure%2520Sparsity%2520through%2520a%2520Recurrent%250A%2520%2520HyperNetwork%26entry.906535625%3DTianjin%2520Huang%2520and%2520Fang%2520Meng%2520and%2520Li%2520Shen%2520and%2520Fan%2520Liu%2520and%2520Yulong%2520Pei%2520and%2520Mykola%2520Pechenizkiy%2520and%2520Shiwei%2520Liu%2520and%2520Tianlong%2520Chen%26entry.1292438233%3D%2520%2520Large-scale%2520neural%2520networks%2520have%2520demonstrated%2520remarkable%2520performance%2520in%250Adifferent%2520domains%2520like%2520vision%2520and%2520language%2520processing%252C%2520although%2520at%2520the%2520cost%2520of%250Amassive%2520computation%2520resources.%2520As%2520illustrated%2520by%2520compression%2520literature%252C%250Astructural%2520model%2520pruning%2520is%2520a%2520prominent%2520algorithm%2520to%2520encourage%2520model%250Aefficiency%252C%2520thanks%2520to%2520its%2520acceleration-friendly%2520sparsity%2520patterns.%2520One%2520of%2520the%250Akey%2520questions%2520of%2520structural%2520pruning%2520is%2520how%2520to%2520estimate%2520the%2520channel%250Asignificance.%2520In%2520parallel%252C%2520work%2520on%2520data-centric%2520AI%2520has%2520shown%2520that%250Aprompting-based%2520techniques%2520enable%2520impressive%2520generalization%2520of%2520large%2520language%250Amodels%2520across%2520diverse%2520downstream%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520a%250Acharming%2520possibility%2520-%2520%255Ctextit%257Bleveraging%2520visual%2520prompts%2520to%2520capture%2520the%2520channel%250Aimportance%2520and%2520derive%2520high-quality%2520structural%2520sparsity%257D.%2520To%2520this%2520end%252C%2520we%250Apropose%2520a%2520novel%2520algorithmic%2520framework%252C%2520namely%2520%255Ctexttt%257BPASS%257D.%2520It%2520is%2520a%2520tailored%250Ahyper-network%2520to%2520take%2520both%2520visual%2520prompts%2520and%2520network%2520weight%2520statistics%2520as%250Ainput%252C%2520and%2520output%2520layer-wise%2520channel%2520sparsity%2520in%2520a%2520recurrent%2520manner.%2520Such%250Adesigns%2520consider%2520the%2520intrinsic%2520channel%2520dependency%2520between%2520layers.%2520Comprehensive%250Aexperiments%2520across%2520multiple%2520network%2520architectures%2520and%2520six%2520datasets%2520demonstrate%250Athe%2520superiority%2520of%2520%255Ctexttt%257BPASS%257D%2520in%2520locating%2520good%2520structural%2520sparsity.%2520For%250Aexample%252C%2520at%2520the%2520same%2520FLOPs%2520level%252C%2520%255Ctexttt%257BPASS%257D%2520subnetworks%2520achieve%2520%25241%255C%2525%255Csim%250A3%255C%2525%2524%2520better%2520accuracy%2520on%2520Food101%2520dataset%253B%2520or%2520with%2520a%2520similar%2520performance%2520of%250A%252480%255C%2525%2524%2520accuracy%252C%2520%255Ctexttt%257BPASS%257D%2520subnetworks%2520obtain%2520%25240.35%255Ctimes%2524%2520more%2520speedup%250Athan%2520the%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17412v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%28PASS%29%20Visual%20Prompt%20Locates%20Good%20Structure%20Sparsity%20through%20a%20Recurrent%0A%20%20HyperNetwork&entry.906535625=Tianjin%20Huang%20and%20Fang%20Meng%20and%20Li%20Shen%20and%20Fan%20Liu%20and%20Yulong%20Pei%20and%20Mykola%20Pechenizkiy%20and%20Shiwei%20Liu%20and%20Tianlong%20Chen&entry.1292438233=%20%20Large-scale%20neural%20networks%20have%20demonstrated%20remarkable%20performance%20in%0Adifferent%20domains%20like%20vision%20and%20language%20processing%2C%20although%20at%20the%20cost%20of%0Amassive%20computation%20resources.%20As%20illustrated%20by%20compression%20literature%2C%0Astructural%20model%20pruning%20is%20a%20prominent%20algorithm%20to%20encourage%20model%0Aefficiency%2C%20thanks%20to%20its%20acceleration-friendly%20sparsity%20patterns.%20One%20of%20the%0Akey%20questions%20of%20structural%20pruning%20is%20how%20to%20estimate%20the%20channel%0Asignificance.%20In%20parallel%2C%20work%20on%20data-centric%20AI%20has%20shown%20that%0Aprompting-based%20techniques%20enable%20impressive%20generalization%20of%20large%20language%0Amodels%20across%20diverse%20downstream%20tasks.%20In%20this%20paper%2C%20we%20investigate%20a%0Acharming%20possibility%20-%20%5Ctextit%7Bleveraging%20visual%20prompts%20to%20capture%20the%20channel%0Aimportance%20and%20derive%20high-quality%20structural%20sparsity%7D.%20To%20this%20end%2C%20we%0Apropose%20a%20novel%20algorithmic%20framework%2C%20namely%20%5Ctexttt%7BPASS%7D.%20It%20is%20a%20tailored%0Ahyper-network%20to%20take%20both%20visual%20prompts%20and%20network%20weight%20statistics%20as%0Ainput%2C%20and%20output%20layer-wise%20channel%20sparsity%20in%20a%20recurrent%20manner.%20Such%0Adesigns%20consider%20the%20intrinsic%20channel%20dependency%20between%20layers.%20Comprehensive%0Aexperiments%20across%20multiple%20network%20architectures%20and%20six%20datasets%20demonstrate%0Athe%20superiority%20of%20%5Ctexttt%7BPASS%7D%20in%20locating%20good%20structural%20sparsity.%20For%0Aexample%2C%20at%20the%20same%20FLOPs%20level%2C%20%5Ctexttt%7BPASS%7D%20subnetworks%20achieve%20%241%5C%25%5Csim%0A3%5C%25%24%20better%20accuracy%20on%20Food101%20dataset%3B%20or%20with%20a%20similar%20performance%20of%0A%2480%5C%25%24%20accuracy%2C%20%5Ctexttt%7BPASS%7D%20subnetworks%20obtain%20%240.35%5Ctimes%24%20more%20speedup%0Athan%20the%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17412v1&entry.124074799=Read"},
{"title": "Grammar-based Game Description Generation using Large Language Models", "author": "Tsunehiko Tanaka and Edgar Simo-Serra", "abstract": "  To lower the barriers to game design development, automated game design,\nwhich generates game designs through computational processes, has been\nexplored. In automated game design, machine learning-based techniques such as\nevolutionary algorithms have achieved success. Benefiting from the remarkable\nadvancements in deep learning, applications in computer vision and natural\nlanguage processing have progressed in level generation. However, due to the\nlimited amount of data in game design, the application of deep learning has\nbeen insufficient for tasks such as game description generation. To pioneer a\nnew approach for handling limited data in automated game design, we focus on\nthe in-context learning of large language models (LLMs). LLMs can capture the\nfeatures of a task from a few demonstration examples and apply the capabilities\nacquired during pre-training. We introduce the grammar of game descriptions,\nwhich effectively structures the game design space, into the LLMs' reasoning\nprocess. Grammar helps LLMs capture the characteristics of the complex task of\ngame description generation. Furthermore, we propose a decoding method that\niteratively improves the generated output by leveraging the grammar. Our\nexperiments demonstrate that this approach performs well in generating game\ndescriptions.\n", "link": "http://arxiv.org/abs/2407.17404v1", "date": "2024-07-24", "relevancy": 2.5169, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5194}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4972}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grammar-based%20Game%20Description%20Generation%20using%20Large%20Language%20Models&body=Title%3A%20Grammar-based%20Game%20Description%20Generation%20using%20Large%20Language%20Models%0AAuthor%3A%20Tsunehiko%20Tanaka%20and%20Edgar%20Simo-Serra%0AAbstract%3A%20%20%20To%20lower%20the%20barriers%20to%20game%20design%20development%2C%20automated%20game%20design%2C%0Awhich%20generates%20game%20designs%20through%20computational%20processes%2C%20has%20been%0Aexplored.%20In%20automated%20game%20design%2C%20machine%20learning-based%20techniques%20such%20as%0Aevolutionary%20algorithms%20have%20achieved%20success.%20Benefiting%20from%20the%20remarkable%0Aadvancements%20in%20deep%20learning%2C%20applications%20in%20computer%20vision%20and%20natural%0Alanguage%20processing%20have%20progressed%20in%20level%20generation.%20However%2C%20due%20to%20the%0Alimited%20amount%20of%20data%20in%20game%20design%2C%20the%20application%20of%20deep%20learning%20has%0Abeen%20insufficient%20for%20tasks%20such%20as%20game%20description%20generation.%20To%20pioneer%20a%0Anew%20approach%20for%20handling%20limited%20data%20in%20automated%20game%20design%2C%20we%20focus%20on%0Athe%20in-context%20learning%20of%20large%20language%20models%20%28LLMs%29.%20LLMs%20can%20capture%20the%0Afeatures%20of%20a%20task%20from%20a%20few%20demonstration%20examples%20and%20apply%20the%20capabilities%0Aacquired%20during%20pre-training.%20We%20introduce%20the%20grammar%20of%20game%20descriptions%2C%0Awhich%20effectively%20structures%20the%20game%20design%20space%2C%20into%20the%20LLMs%27%20reasoning%0Aprocess.%20Grammar%20helps%20LLMs%20capture%20the%20characteristics%20of%20the%20complex%20task%20of%0Agame%20description%20generation.%20Furthermore%2C%20we%20propose%20a%20decoding%20method%20that%0Aiteratively%20improves%20the%20generated%20output%20by%20leveraging%20the%20grammar.%20Our%0Aexperiments%20demonstrate%20that%20this%20approach%20performs%20well%20in%20generating%20game%0Adescriptions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17404v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrammar-based%2520Game%2520Description%2520Generation%2520using%2520Large%2520Language%2520Models%26entry.906535625%3DTsunehiko%2520Tanaka%2520and%2520Edgar%2520Simo-Serra%26entry.1292438233%3D%2520%2520To%2520lower%2520the%2520barriers%2520to%2520game%2520design%2520development%252C%2520automated%2520game%2520design%252C%250Awhich%2520generates%2520game%2520designs%2520through%2520computational%2520processes%252C%2520has%2520been%250Aexplored.%2520In%2520automated%2520game%2520design%252C%2520machine%2520learning-based%2520techniques%2520such%2520as%250Aevolutionary%2520algorithms%2520have%2520achieved%2520success.%2520Benefiting%2520from%2520the%2520remarkable%250Aadvancements%2520in%2520deep%2520learning%252C%2520applications%2520in%2520computer%2520vision%2520and%2520natural%250Alanguage%2520processing%2520have%2520progressed%2520in%2520level%2520generation.%2520However%252C%2520due%2520to%2520the%250Alimited%2520amount%2520of%2520data%2520in%2520game%2520design%252C%2520the%2520application%2520of%2520deep%2520learning%2520has%250Abeen%2520insufficient%2520for%2520tasks%2520such%2520as%2520game%2520description%2520generation.%2520To%2520pioneer%2520a%250Anew%2520approach%2520for%2520handling%2520limited%2520data%2520in%2520automated%2520game%2520design%252C%2520we%2520focus%2520on%250Athe%2520in-context%2520learning%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520LLMs%2520can%2520capture%2520the%250Afeatures%2520of%2520a%2520task%2520from%2520a%2520few%2520demonstration%2520examples%2520and%2520apply%2520the%2520capabilities%250Aacquired%2520during%2520pre-training.%2520We%2520introduce%2520the%2520grammar%2520of%2520game%2520descriptions%252C%250Awhich%2520effectively%2520structures%2520the%2520game%2520design%2520space%252C%2520into%2520the%2520LLMs%2527%2520reasoning%250Aprocess.%2520Grammar%2520helps%2520LLMs%2520capture%2520the%2520characteristics%2520of%2520the%2520complex%2520task%2520of%250Agame%2520description%2520generation.%2520Furthermore%252C%2520we%2520propose%2520a%2520decoding%2520method%2520that%250Aiteratively%2520improves%2520the%2520generated%2520output%2520by%2520leveraging%2520the%2520grammar.%2520Our%250Aexperiments%2520demonstrate%2520that%2520this%2520approach%2520performs%2520well%2520in%2520generating%2520game%250Adescriptions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17404v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grammar-based%20Game%20Description%20Generation%20using%20Large%20Language%20Models&entry.906535625=Tsunehiko%20Tanaka%20and%20Edgar%20Simo-Serra&entry.1292438233=%20%20To%20lower%20the%20barriers%20to%20game%20design%20development%2C%20automated%20game%20design%2C%0Awhich%20generates%20game%20designs%20through%20computational%20processes%2C%20has%20been%0Aexplored.%20In%20automated%20game%20design%2C%20machine%20learning-based%20techniques%20such%20as%0Aevolutionary%20algorithms%20have%20achieved%20success.%20Benefiting%20from%20the%20remarkable%0Aadvancements%20in%20deep%20learning%2C%20applications%20in%20computer%20vision%20and%20natural%0Alanguage%20processing%20have%20progressed%20in%20level%20generation.%20However%2C%20due%20to%20the%0Alimited%20amount%20of%20data%20in%20game%20design%2C%20the%20application%20of%20deep%20learning%20has%0Abeen%20insufficient%20for%20tasks%20such%20as%20game%20description%20generation.%20To%20pioneer%20a%0Anew%20approach%20for%20handling%20limited%20data%20in%20automated%20game%20design%2C%20we%20focus%20on%0Athe%20in-context%20learning%20of%20large%20language%20models%20%28LLMs%29.%20LLMs%20can%20capture%20the%0Afeatures%20of%20a%20task%20from%20a%20few%20demonstration%20examples%20and%20apply%20the%20capabilities%0Aacquired%20during%20pre-training.%20We%20introduce%20the%20grammar%20of%20game%20descriptions%2C%0Awhich%20effectively%20structures%20the%20game%20design%20space%2C%20into%20the%20LLMs%27%20reasoning%0Aprocess.%20Grammar%20helps%20LLMs%20capture%20the%20characteristics%20of%20the%20complex%20task%20of%0Agame%20description%20generation.%20Furthermore%2C%20we%20propose%20a%20decoding%20method%20that%0Aiteratively%20improves%20the%20generated%20output%20by%20leveraging%20the%20grammar.%20Our%0Aexperiments%20demonstrate%20that%20this%20approach%20performs%20well%20in%20generating%20game%0Adescriptions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17404v1&entry.124074799=Read"},
{"title": "Do Generative AI Models Output Harm while Representing Non-Western\n  Cultures: Evidence from A Community-Centered Approach", "author": "Sourojit Ghosh and Pranav Narayanan Venkit and Sanjana Gautam and Shomir Wilson and Aylin Caliskan", "abstract": "  Our research investigates the impact of Generative Artificial Intelligence\n(GAI) models, specifically text-to-image generators (T2Is), on the\nrepresentation of non-Western cultures, with a focus on Indian contexts.\nDespite the transformative potential of T2Is in content creation, concerns have\narisen regarding biases that may lead to misrepresentations and\nmarginalizations. Through a community-centered approach and grounded theory\nanalysis of 5 focus groups from diverse Indian subcultures, we explore how T2I\noutputs to English prompts depict Indian culture and its subcultures,\nuncovering novel representational harms such as exoticism and cultural\nmisappropriation. These findings highlight the urgent need for inclusive and\nculturally sensitive T2I systems. We propose design guidelines informed by a\nsociotechnical perspective, aiming to address these issues and contribute to\nthe development of more equitable and representative GAI technologies globally.\nOur work also underscores the necessity of adopting a community-centered\napproach to comprehend the sociotechnical dynamics of these models,\ncomplementing existing work in this space while identifying and addressing the\npotential negative repercussions and harms that may arise when these models are\ndeployed on a global scale.\n", "link": "http://arxiv.org/abs/2407.14779v2", "date": "2024-07-24", "relevancy": 2.5136, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5279}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4913}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Generative%20AI%20Models%20Output%20Harm%20while%20Representing%20Non-Western%0A%20%20Cultures%3A%20Evidence%20from%20A%20Community-Centered%20Approach&body=Title%3A%20Do%20Generative%20AI%20Models%20Output%20Harm%20while%20Representing%20Non-Western%0A%20%20Cultures%3A%20Evidence%20from%20A%20Community-Centered%20Approach%0AAuthor%3A%20Sourojit%20Ghosh%20and%20Pranav%20Narayanan%20Venkit%20and%20Sanjana%20Gautam%20and%20Shomir%20Wilson%20and%20Aylin%20Caliskan%0AAbstract%3A%20%20%20Our%20research%20investigates%20the%20impact%20of%20Generative%20Artificial%20Intelligence%0A%28GAI%29%20models%2C%20specifically%20text-to-image%20generators%20%28T2Is%29%2C%20on%20the%0Arepresentation%20of%20non-Western%20cultures%2C%20with%20a%20focus%20on%20Indian%20contexts.%0ADespite%20the%20transformative%20potential%20of%20T2Is%20in%20content%20creation%2C%20concerns%20have%0Aarisen%20regarding%20biases%20that%20may%20lead%20to%20misrepresentations%20and%0Amarginalizations.%20Through%20a%20community-centered%20approach%20and%20grounded%20theory%0Aanalysis%20of%205%20focus%20groups%20from%20diverse%20Indian%20subcultures%2C%20we%20explore%20how%20T2I%0Aoutputs%20to%20English%20prompts%20depict%20Indian%20culture%20and%20its%20subcultures%2C%0Auncovering%20novel%20representational%20harms%20such%20as%20exoticism%20and%20cultural%0Amisappropriation.%20These%20findings%20highlight%20the%20urgent%20need%20for%20inclusive%20and%0Aculturally%20sensitive%20T2I%20systems.%20We%20propose%20design%20guidelines%20informed%20by%20a%0Asociotechnical%20perspective%2C%20aiming%20to%20address%20these%20issues%20and%20contribute%20to%0Athe%20development%20of%20more%20equitable%20and%20representative%20GAI%20technologies%20globally.%0AOur%20work%20also%20underscores%20the%20necessity%20of%20adopting%20a%20community-centered%0Aapproach%20to%20comprehend%20the%20sociotechnical%20dynamics%20of%20these%20models%2C%0Acomplementing%20existing%20work%20in%20this%20space%20while%20identifying%20and%20addressing%20the%0Apotential%20negative%20repercussions%20and%20harms%20that%20may%20arise%20when%20these%20models%20are%0Adeployed%20on%20a%20global%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14779v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Generative%2520AI%2520Models%2520Output%2520Harm%2520while%2520Representing%2520Non-Western%250A%2520%2520Cultures%253A%2520Evidence%2520from%2520A%2520Community-Centered%2520Approach%26entry.906535625%3DSourojit%2520Ghosh%2520and%2520Pranav%2520Narayanan%2520Venkit%2520and%2520Sanjana%2520Gautam%2520and%2520Shomir%2520Wilson%2520and%2520Aylin%2520Caliskan%26entry.1292438233%3D%2520%2520Our%2520research%2520investigates%2520the%2520impact%2520of%2520Generative%2520Artificial%2520Intelligence%250A%2528GAI%2529%2520models%252C%2520specifically%2520text-to-image%2520generators%2520%2528T2Is%2529%252C%2520on%2520the%250Arepresentation%2520of%2520non-Western%2520cultures%252C%2520with%2520a%2520focus%2520on%2520Indian%2520contexts.%250ADespite%2520the%2520transformative%2520potential%2520of%2520T2Is%2520in%2520content%2520creation%252C%2520concerns%2520have%250Aarisen%2520regarding%2520biases%2520that%2520may%2520lead%2520to%2520misrepresentations%2520and%250Amarginalizations.%2520Through%2520a%2520community-centered%2520approach%2520and%2520grounded%2520theory%250Aanalysis%2520of%25205%2520focus%2520groups%2520from%2520diverse%2520Indian%2520subcultures%252C%2520we%2520explore%2520how%2520T2I%250Aoutputs%2520to%2520English%2520prompts%2520depict%2520Indian%2520culture%2520and%2520its%2520subcultures%252C%250Auncovering%2520novel%2520representational%2520harms%2520such%2520as%2520exoticism%2520and%2520cultural%250Amisappropriation.%2520These%2520findings%2520highlight%2520the%2520urgent%2520need%2520for%2520inclusive%2520and%250Aculturally%2520sensitive%2520T2I%2520systems.%2520We%2520propose%2520design%2520guidelines%2520informed%2520by%2520a%250Asociotechnical%2520perspective%252C%2520aiming%2520to%2520address%2520these%2520issues%2520and%2520contribute%2520to%250Athe%2520development%2520of%2520more%2520equitable%2520and%2520representative%2520GAI%2520technologies%2520globally.%250AOur%2520work%2520also%2520underscores%2520the%2520necessity%2520of%2520adopting%2520a%2520community-centered%250Aapproach%2520to%2520comprehend%2520the%2520sociotechnical%2520dynamics%2520of%2520these%2520models%252C%250Acomplementing%2520existing%2520work%2520in%2520this%2520space%2520while%2520identifying%2520and%2520addressing%2520the%250Apotential%2520negative%2520repercussions%2520and%2520harms%2520that%2520may%2520arise%2520when%2520these%2520models%2520are%250Adeployed%2520on%2520a%2520global%2520scale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14779v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Generative%20AI%20Models%20Output%20Harm%20while%20Representing%20Non-Western%0A%20%20Cultures%3A%20Evidence%20from%20A%20Community-Centered%20Approach&entry.906535625=Sourojit%20Ghosh%20and%20Pranav%20Narayanan%20Venkit%20and%20Sanjana%20Gautam%20and%20Shomir%20Wilson%20and%20Aylin%20Caliskan&entry.1292438233=%20%20Our%20research%20investigates%20the%20impact%20of%20Generative%20Artificial%20Intelligence%0A%28GAI%29%20models%2C%20specifically%20text-to-image%20generators%20%28T2Is%29%2C%20on%20the%0Arepresentation%20of%20non-Western%20cultures%2C%20with%20a%20focus%20on%20Indian%20contexts.%0ADespite%20the%20transformative%20potential%20of%20T2Is%20in%20content%20creation%2C%20concerns%20have%0Aarisen%20regarding%20biases%20that%20may%20lead%20to%20misrepresentations%20and%0Amarginalizations.%20Through%20a%20community-centered%20approach%20and%20grounded%20theory%0Aanalysis%20of%205%20focus%20groups%20from%20diverse%20Indian%20subcultures%2C%20we%20explore%20how%20T2I%0Aoutputs%20to%20English%20prompts%20depict%20Indian%20culture%20and%20its%20subcultures%2C%0Auncovering%20novel%20representational%20harms%20such%20as%20exoticism%20and%20cultural%0Amisappropriation.%20These%20findings%20highlight%20the%20urgent%20need%20for%20inclusive%20and%0Aculturally%20sensitive%20T2I%20systems.%20We%20propose%20design%20guidelines%20informed%20by%20a%0Asociotechnical%20perspective%2C%20aiming%20to%20address%20these%20issues%20and%20contribute%20to%0Athe%20development%20of%20more%20equitable%20and%20representative%20GAI%20technologies%20globally.%0AOur%20work%20also%20underscores%20the%20necessity%20of%20adopting%20a%20community-centered%0Aapproach%20to%20comprehend%20the%20sociotechnical%20dynamics%20of%20these%20models%2C%0Acomplementing%20existing%20work%20in%20this%20space%20while%20identifying%20and%20addressing%20the%0Apotential%20negative%20repercussions%20and%20harms%20that%20may%20arise%20when%20these%20models%20are%0Adeployed%20on%20a%20global%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14779v2&entry.124074799=Read"},
{"title": "DexGANGrasp: Dexterous Generative Adversarial Grasping Synthesis for\n  Task-Oriented Manipulation", "author": "Qian Feng and David S. Martinez Lema and Mohammadhossein Malmir and Hang Li and Jianxiang Feng and Zhaopeng Chen and Alois Knoll", "abstract": "  We introduce DexGanGrasp, a dexterous grasping synthesis method that\ngenerates and evaluates grasps with single view in real time. DexGanGrasp\ncomprises a Conditional Generative Adversarial Networks (cGANs)-based\nDexGenerator to generate dexterous grasps and a discriminator-like DexEvalautor\nto assess the stability of these grasps. Extensive simulation and real-world\nexpriments showcases the effectiveness of our proposed method, outperforming\nthe baseline FFHNet with an 18.57% higher success rate in real-world\nevaluation. We further extend DexGanGrasp to DexAfford-Prompt, an\nopen-vocabulary affordance grounding pipeline for dexterous grasping leveraging\nMultimodal Large Language Models (MLLMs) and Vision Language Models (VLMs), to\nachieve task-oriented grasping with successful real-world deployments.\n", "link": "http://arxiv.org/abs/2407.17348v1", "date": "2024-07-24", "relevancy": 2.4773, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.7378}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5723}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DexGANGrasp%3A%20Dexterous%20Generative%20Adversarial%20Grasping%20Synthesis%20for%0A%20%20Task-Oriented%20Manipulation&body=Title%3A%20DexGANGrasp%3A%20Dexterous%20Generative%20Adversarial%20Grasping%20Synthesis%20for%0A%20%20Task-Oriented%20Manipulation%0AAuthor%3A%20Qian%20Feng%20and%20David%20S.%20Martinez%20Lema%20and%20Mohammadhossein%20Malmir%20and%20Hang%20Li%20and%20Jianxiang%20Feng%20and%20Zhaopeng%20Chen%20and%20Alois%20Knoll%0AAbstract%3A%20%20%20We%20introduce%20DexGanGrasp%2C%20a%20dexterous%20grasping%20synthesis%20method%20that%0Agenerates%20and%20evaluates%20grasps%20with%20single%20view%20in%20real%20time.%20DexGanGrasp%0Acomprises%20a%20Conditional%20Generative%20Adversarial%20Networks%20%28cGANs%29-based%0ADexGenerator%20to%20generate%20dexterous%20grasps%20and%20a%20discriminator-like%20DexEvalautor%0Ato%20assess%20the%20stability%20of%20these%20grasps.%20Extensive%20simulation%20and%20real-world%0Aexpriments%20showcases%20the%20effectiveness%20of%20our%20proposed%20method%2C%20outperforming%0Athe%20baseline%20FFHNet%20with%20an%2018.57%25%20higher%20success%20rate%20in%20real-world%0Aevaluation.%20We%20further%20extend%20DexGanGrasp%20to%20DexAfford-Prompt%2C%20an%0Aopen-vocabulary%20affordance%20grounding%20pipeline%20for%20dexterous%20grasping%20leveraging%0AMultimodal%20Large%20Language%20Models%20%28MLLMs%29%20and%20Vision%20Language%20Models%20%28VLMs%29%2C%20to%0Aachieve%20task-oriented%20grasping%20with%20successful%20real-world%20deployments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17348v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDexGANGrasp%253A%2520Dexterous%2520Generative%2520Adversarial%2520Grasping%2520Synthesis%2520for%250A%2520%2520Task-Oriented%2520Manipulation%26entry.906535625%3DQian%2520Feng%2520and%2520David%2520S.%2520Martinez%2520Lema%2520and%2520Mohammadhossein%2520Malmir%2520and%2520Hang%2520Li%2520and%2520Jianxiang%2520Feng%2520and%2520Zhaopeng%2520Chen%2520and%2520Alois%2520Knoll%26entry.1292438233%3D%2520%2520We%2520introduce%2520DexGanGrasp%252C%2520a%2520dexterous%2520grasping%2520synthesis%2520method%2520that%250Agenerates%2520and%2520evaluates%2520grasps%2520with%2520single%2520view%2520in%2520real%2520time.%2520DexGanGrasp%250Acomprises%2520a%2520Conditional%2520Generative%2520Adversarial%2520Networks%2520%2528cGANs%2529-based%250ADexGenerator%2520to%2520generate%2520dexterous%2520grasps%2520and%2520a%2520discriminator-like%2520DexEvalautor%250Ato%2520assess%2520the%2520stability%2520of%2520these%2520grasps.%2520Extensive%2520simulation%2520and%2520real-world%250Aexpriments%2520showcases%2520the%2520effectiveness%2520of%2520our%2520proposed%2520method%252C%2520outperforming%250Athe%2520baseline%2520FFHNet%2520with%2520an%252018.57%2525%2520higher%2520success%2520rate%2520in%2520real-world%250Aevaluation.%2520We%2520further%2520extend%2520DexGanGrasp%2520to%2520DexAfford-Prompt%252C%2520an%250Aopen-vocabulary%2520affordance%2520grounding%2520pipeline%2520for%2520dexterous%2520grasping%2520leveraging%250AMultimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520and%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%252C%2520to%250Aachieve%2520task-oriented%2520grasping%2520with%2520successful%2520real-world%2520deployments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17348v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DexGANGrasp%3A%20Dexterous%20Generative%20Adversarial%20Grasping%20Synthesis%20for%0A%20%20Task-Oriented%20Manipulation&entry.906535625=Qian%20Feng%20and%20David%20S.%20Martinez%20Lema%20and%20Mohammadhossein%20Malmir%20and%20Hang%20Li%20and%20Jianxiang%20Feng%20and%20Zhaopeng%20Chen%20and%20Alois%20Knoll&entry.1292438233=%20%20We%20introduce%20DexGanGrasp%2C%20a%20dexterous%20grasping%20synthesis%20method%20that%0Agenerates%20and%20evaluates%20grasps%20with%20single%20view%20in%20real%20time.%20DexGanGrasp%0Acomprises%20a%20Conditional%20Generative%20Adversarial%20Networks%20%28cGANs%29-based%0ADexGenerator%20to%20generate%20dexterous%20grasps%20and%20a%20discriminator-like%20DexEvalautor%0Ato%20assess%20the%20stability%20of%20these%20grasps.%20Extensive%20simulation%20and%20real-world%0Aexpriments%20showcases%20the%20effectiveness%20of%20our%20proposed%20method%2C%20outperforming%0Athe%20baseline%20FFHNet%20with%20an%2018.57%25%20higher%20success%20rate%20in%20real-world%0Aevaluation.%20We%20further%20extend%20DexGanGrasp%20to%20DexAfford-Prompt%2C%20an%0Aopen-vocabulary%20affordance%20grounding%20pipeline%20for%20dexterous%20grasping%20leveraging%0AMultimodal%20Large%20Language%20Models%20%28MLLMs%29%20and%20Vision%20Language%20Models%20%28VLMs%29%2C%20to%0Aachieve%20task-oriented%20grasping%20with%20successful%20real-world%20deployments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17348v1&entry.124074799=Read"},
{"title": "ALPI: Auto-Labeller with Proxy Injection for 3D Object Detection using\n  2D Labels Only", "author": "Saad Lahlali and Nicolas Granger and Herv\u00e9 Le Borgne and Quoc-Cuong Pham", "abstract": "  3D object detection plays a crucial role in various applications such as\nautonomous vehicles, robotics and augmented reality. However, training 3D\ndetectors requires a costly precise annotation, which is a hindrance to scaling\nannotation to large datasets. To address this challenge, we propose a weakly\nsupervised 3D annotator that relies solely on 2D bounding box annotations from\nimages, along with size priors. One major problem is that supervising a 3D\ndetection model using only 2D boxes is not reliable due to ambiguities between\ndifferent 3D poses and their identical 2D projection. We introduce a simple yet\neffective and generic solution: we build 3D proxy objects with annotations by\nconstruction and add them to the training dataset. Our method requires only\nsize priors to adapt to new classes. To better align 2D supervision with 3D\ndetection, our method ensures depth invariance with a novel expression of the\n2D losses. Finally, to detect more challenging instances, our annotator follows\nan offline pseudo-labelling scheme which gradually improves its 3D\npseudo-labels. Extensive experiments on the KITTI dataset demonstrate that our\nmethod not only performs on-par or above previous works on the Car category,\nbut also achieves performance close to fully supervised methods on more\nchallenging classes. We further demonstrate the effectiveness and robustness of\nour method by being the first to experiment on the more challenging nuScenes\ndataset. We additionally propose a setting where weak labels are obtained from\na 2D detector pre-trained on MS-COCO instead of human annotations.\n", "link": "http://arxiv.org/abs/2407.17197v1", "date": "2024-07-24", "relevancy": 2.4514, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6747}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6047}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ALPI%3A%20Auto-Labeller%20with%20Proxy%20Injection%20for%203D%20Object%20Detection%20using%0A%20%202D%20Labels%20Only&body=Title%3A%20ALPI%3A%20Auto-Labeller%20with%20Proxy%20Injection%20for%203D%20Object%20Detection%20using%0A%20%202D%20Labels%20Only%0AAuthor%3A%20Saad%20Lahlali%20and%20Nicolas%20Granger%20and%20Herv%C3%A9%20Le%20Borgne%20and%20Quoc-Cuong%20Pham%0AAbstract%3A%20%20%203D%20object%20detection%20plays%20a%20crucial%20role%20in%20various%20applications%20such%20as%0Aautonomous%20vehicles%2C%20robotics%20and%20augmented%20reality.%20However%2C%20training%203D%0Adetectors%20requires%20a%20costly%20precise%20annotation%2C%20which%20is%20a%20hindrance%20to%20scaling%0Aannotation%20to%20large%20datasets.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20weakly%0Asupervised%203D%20annotator%20that%20relies%20solely%20on%202D%20bounding%20box%20annotations%20from%0Aimages%2C%20along%20with%20size%20priors.%20One%20major%20problem%20is%20that%20supervising%20a%203D%0Adetection%20model%20using%20only%202D%20boxes%20is%20not%20reliable%20due%20to%20ambiguities%20between%0Adifferent%203D%20poses%20and%20their%20identical%202D%20projection.%20We%20introduce%20a%20simple%20yet%0Aeffective%20and%20generic%20solution%3A%20we%20build%203D%20proxy%20objects%20with%20annotations%20by%0Aconstruction%20and%20add%20them%20to%20the%20training%20dataset.%20Our%20method%20requires%20only%0Asize%20priors%20to%20adapt%20to%20new%20classes.%20To%20better%20align%202D%20supervision%20with%203D%0Adetection%2C%20our%20method%20ensures%20depth%20invariance%20with%20a%20novel%20expression%20of%20the%0A2D%20losses.%20Finally%2C%20to%20detect%20more%20challenging%20instances%2C%20our%20annotator%20follows%0Aan%20offline%20pseudo-labelling%20scheme%20which%20gradually%20improves%20its%203D%0Apseudo-labels.%20Extensive%20experiments%20on%20the%20KITTI%20dataset%20demonstrate%20that%20our%0Amethod%20not%20only%20performs%20on-par%20or%20above%20previous%20works%20on%20the%20Car%20category%2C%0Abut%20also%20achieves%20performance%20close%20to%20fully%20supervised%20methods%20on%20more%0Achallenging%20classes.%20We%20further%20demonstrate%20the%20effectiveness%20and%20robustness%20of%0Aour%20method%20by%20being%20the%20first%20to%20experiment%20on%20the%20more%20challenging%20nuScenes%0Adataset.%20We%20additionally%20propose%20a%20setting%20where%20weak%20labels%20are%20obtained%20from%0Aa%202D%20detector%20pre-trained%20on%20MS-COCO%20instead%20of%20human%20annotations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17197v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DALPI%253A%2520Auto-Labeller%2520with%2520Proxy%2520Injection%2520for%25203D%2520Object%2520Detection%2520using%250A%2520%25202D%2520Labels%2520Only%26entry.906535625%3DSaad%2520Lahlali%2520and%2520Nicolas%2520Granger%2520and%2520Herv%25C3%25A9%2520Le%2520Borgne%2520and%2520Quoc-Cuong%2520Pham%26entry.1292438233%3D%2520%25203D%2520object%2520detection%2520plays%2520a%2520crucial%2520role%2520in%2520various%2520applications%2520such%2520as%250Aautonomous%2520vehicles%252C%2520robotics%2520and%2520augmented%2520reality.%2520However%252C%2520training%25203D%250Adetectors%2520requires%2520a%2520costly%2520precise%2520annotation%252C%2520which%2520is%2520a%2520hindrance%2520to%2520scaling%250Aannotation%2520to%2520large%2520datasets.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520weakly%250Asupervised%25203D%2520annotator%2520that%2520relies%2520solely%2520on%25202D%2520bounding%2520box%2520annotations%2520from%250Aimages%252C%2520along%2520with%2520size%2520priors.%2520One%2520major%2520problem%2520is%2520that%2520supervising%2520a%25203D%250Adetection%2520model%2520using%2520only%25202D%2520boxes%2520is%2520not%2520reliable%2520due%2520to%2520ambiguities%2520between%250Adifferent%25203D%2520poses%2520and%2520their%2520identical%25202D%2520projection.%2520We%2520introduce%2520a%2520simple%2520yet%250Aeffective%2520and%2520generic%2520solution%253A%2520we%2520build%25203D%2520proxy%2520objects%2520with%2520annotations%2520by%250Aconstruction%2520and%2520add%2520them%2520to%2520the%2520training%2520dataset.%2520Our%2520method%2520requires%2520only%250Asize%2520priors%2520to%2520adapt%2520to%2520new%2520classes.%2520To%2520better%2520align%25202D%2520supervision%2520with%25203D%250Adetection%252C%2520our%2520method%2520ensures%2520depth%2520invariance%2520with%2520a%2520novel%2520expression%2520of%2520the%250A2D%2520losses.%2520Finally%252C%2520to%2520detect%2520more%2520challenging%2520instances%252C%2520our%2520annotator%2520follows%250Aan%2520offline%2520pseudo-labelling%2520scheme%2520which%2520gradually%2520improves%2520its%25203D%250Apseudo-labels.%2520Extensive%2520experiments%2520on%2520the%2520KITTI%2520dataset%2520demonstrate%2520that%2520our%250Amethod%2520not%2520only%2520performs%2520on-par%2520or%2520above%2520previous%2520works%2520on%2520the%2520Car%2520category%252C%250Abut%2520also%2520achieves%2520performance%2520close%2520to%2520fully%2520supervised%2520methods%2520on%2520more%250Achallenging%2520classes.%2520We%2520further%2520demonstrate%2520the%2520effectiveness%2520and%2520robustness%2520of%250Aour%2520method%2520by%2520being%2520the%2520first%2520to%2520experiment%2520on%2520the%2520more%2520challenging%2520nuScenes%250Adataset.%2520We%2520additionally%2520propose%2520a%2520setting%2520where%2520weak%2520labels%2520are%2520obtained%2520from%250Aa%25202D%2520detector%2520pre-trained%2520on%2520MS-COCO%2520instead%2520of%2520human%2520annotations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17197v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ALPI%3A%20Auto-Labeller%20with%20Proxy%20Injection%20for%203D%20Object%20Detection%20using%0A%20%202D%20Labels%20Only&entry.906535625=Saad%20Lahlali%20and%20Nicolas%20Granger%20and%20Herv%C3%A9%20Le%20Borgne%20and%20Quoc-Cuong%20Pham&entry.1292438233=%20%203D%20object%20detection%20plays%20a%20crucial%20role%20in%20various%20applications%20such%20as%0Aautonomous%20vehicles%2C%20robotics%20and%20augmented%20reality.%20However%2C%20training%203D%0Adetectors%20requires%20a%20costly%20precise%20annotation%2C%20which%20is%20a%20hindrance%20to%20scaling%0Aannotation%20to%20large%20datasets.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20weakly%0Asupervised%203D%20annotator%20that%20relies%20solely%20on%202D%20bounding%20box%20annotations%20from%0Aimages%2C%20along%20with%20size%20priors.%20One%20major%20problem%20is%20that%20supervising%20a%203D%0Adetection%20model%20using%20only%202D%20boxes%20is%20not%20reliable%20due%20to%20ambiguities%20between%0Adifferent%203D%20poses%20and%20their%20identical%202D%20projection.%20We%20introduce%20a%20simple%20yet%0Aeffective%20and%20generic%20solution%3A%20we%20build%203D%20proxy%20objects%20with%20annotations%20by%0Aconstruction%20and%20add%20them%20to%20the%20training%20dataset.%20Our%20method%20requires%20only%0Asize%20priors%20to%20adapt%20to%20new%20classes.%20To%20better%20align%202D%20supervision%20with%203D%0Adetection%2C%20our%20method%20ensures%20depth%20invariance%20with%20a%20novel%20expression%20of%20the%0A2D%20losses.%20Finally%2C%20to%20detect%20more%20challenging%20instances%2C%20our%20annotator%20follows%0Aan%20offline%20pseudo-labelling%20scheme%20which%20gradually%20improves%20its%203D%0Apseudo-labels.%20Extensive%20experiments%20on%20the%20KITTI%20dataset%20demonstrate%20that%20our%0Amethod%20not%20only%20performs%20on-par%20or%20above%20previous%20works%20on%20the%20Car%20category%2C%0Abut%20also%20achieves%20performance%20close%20to%20fully%20supervised%20methods%20on%20more%0Achallenging%20classes.%20We%20further%20demonstrate%20the%20effectiveness%20and%20robustness%20of%0Aour%20method%20by%20being%20the%20first%20to%20experiment%20on%20the%20more%20challenging%20nuScenes%0Adataset.%20We%20additionally%20propose%20a%20setting%20where%20weak%20labels%20are%20obtained%20from%0Aa%202D%20detector%20pre-trained%20on%20MS-COCO%20instead%20of%20human%20annotations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17197v1&entry.124074799=Read"},
{"title": "Revolutionizing Text-to-Image Retrieval as Autoregressive Token-to-Voken\n  Generation", "author": "Yongqi Li and Hongru Cai and Wenjie Wang and Leigang Qu and Yinwei Wei and Wenjie Li and Liqiang Nie and Tat-Seng Chua", "abstract": "  Text-to-image retrieval is a fundamental task in multimedia processing,\naiming to retrieve semantically relevant cross-modal content. Traditional\nstudies have typically approached this task as a discriminative problem,\nmatching the text and image via the cross-attention mechanism (one-tower\nframework) or in a common embedding space (two-tower framework). Recently,\ngenerative cross-modal retrieval has emerged as a new research line, which\nassigns images with unique string identifiers and generates the target\nidentifier as the retrieval target. Despite its great potential, existing\ngenerative approaches are limited due to the following issues: insufficient\nvisual information in identifiers, misalignment with high-level semantics, and\nlearning gap towards the retrieval target. To address the above issues, we\npropose an autoregressive voken generation method, named AVG. AVG tokenizes\nimages into vokens, i.e., visual tokens, and innovatively formulates the\ntext-to-image retrieval task as a token-to-voken generation problem. AVG\ndiscretizes an image into a sequence of vokens as the identifier of the image,\nwhile maintaining the alignment with both the visual information and high-level\nsemantics of the image. Additionally, to bridge the learning gap between\ngenerative training and the retrieval target, we incorporate discriminative\ntraining to modify the learning direction during token-to-voken training.\nExtensive experiments demonstrate that AVG achieves superior results in both\neffectiveness and efficiency.\n", "link": "http://arxiv.org/abs/2407.17274v1", "date": "2024-07-24", "relevancy": 2.4315, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6242}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6115}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revolutionizing%20Text-to-Image%20Retrieval%20as%20Autoregressive%20Token-to-Voken%0A%20%20Generation&body=Title%3A%20Revolutionizing%20Text-to-Image%20Retrieval%20as%20Autoregressive%20Token-to-Voken%0A%20%20Generation%0AAuthor%3A%20Yongqi%20Li%20and%20Hongru%20Cai%20and%20Wenjie%20Wang%20and%20Leigang%20Qu%20and%20Yinwei%20Wei%20and%20Wenjie%20Li%20and%20Liqiang%20Nie%20and%20Tat-Seng%20Chua%0AAbstract%3A%20%20%20Text-to-image%20retrieval%20is%20a%20fundamental%20task%20in%20multimedia%20processing%2C%0Aaiming%20to%20retrieve%20semantically%20relevant%20cross-modal%20content.%20Traditional%0Astudies%20have%20typically%20approached%20this%20task%20as%20a%20discriminative%20problem%2C%0Amatching%20the%20text%20and%20image%20via%20the%20cross-attention%20mechanism%20%28one-tower%0Aframework%29%20or%20in%20a%20common%20embedding%20space%20%28two-tower%20framework%29.%20Recently%2C%0Agenerative%20cross-modal%20retrieval%20has%20emerged%20as%20a%20new%20research%20line%2C%20which%0Aassigns%20images%20with%20unique%20string%20identifiers%20and%20generates%20the%20target%0Aidentifier%20as%20the%20retrieval%20target.%20Despite%20its%20great%20potential%2C%20existing%0Agenerative%20approaches%20are%20limited%20due%20to%20the%20following%20issues%3A%20insufficient%0Avisual%20information%20in%20identifiers%2C%20misalignment%20with%20high-level%20semantics%2C%20and%0Alearning%20gap%20towards%20the%20retrieval%20target.%20To%20address%20the%20above%20issues%2C%20we%0Apropose%20an%20autoregressive%20voken%20generation%20method%2C%20named%20AVG.%20AVG%20tokenizes%0Aimages%20into%20vokens%2C%20i.e.%2C%20visual%20tokens%2C%20and%20innovatively%20formulates%20the%0Atext-to-image%20retrieval%20task%20as%20a%20token-to-voken%20generation%20problem.%20AVG%0Adiscretizes%20an%20image%20into%20a%20sequence%20of%20vokens%20as%20the%20identifier%20of%20the%20image%2C%0Awhile%20maintaining%20the%20alignment%20with%20both%20the%20visual%20information%20and%20high-level%0Asemantics%20of%20the%20image.%20Additionally%2C%20to%20bridge%20the%20learning%20gap%20between%0Agenerative%20training%20and%20the%20retrieval%20target%2C%20we%20incorporate%20discriminative%0Atraining%20to%20modify%20the%20learning%20direction%20during%20token-to-voken%20training.%0AExtensive%20experiments%20demonstrate%20that%20AVG%20achieves%20superior%20results%20in%20both%0Aeffectiveness%20and%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17274v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevolutionizing%2520Text-to-Image%2520Retrieval%2520as%2520Autoregressive%2520Token-to-Voken%250A%2520%2520Generation%26entry.906535625%3DYongqi%2520Li%2520and%2520Hongru%2520Cai%2520and%2520Wenjie%2520Wang%2520and%2520Leigang%2520Qu%2520and%2520Yinwei%2520Wei%2520and%2520Wenjie%2520Li%2520and%2520Liqiang%2520Nie%2520and%2520Tat-Seng%2520Chua%26entry.1292438233%3D%2520%2520Text-to-image%2520retrieval%2520is%2520a%2520fundamental%2520task%2520in%2520multimedia%2520processing%252C%250Aaiming%2520to%2520retrieve%2520semantically%2520relevant%2520cross-modal%2520content.%2520Traditional%250Astudies%2520have%2520typically%2520approached%2520this%2520task%2520as%2520a%2520discriminative%2520problem%252C%250Amatching%2520the%2520text%2520and%2520image%2520via%2520the%2520cross-attention%2520mechanism%2520%2528one-tower%250Aframework%2529%2520or%2520in%2520a%2520common%2520embedding%2520space%2520%2528two-tower%2520framework%2529.%2520Recently%252C%250Agenerative%2520cross-modal%2520retrieval%2520has%2520emerged%2520as%2520a%2520new%2520research%2520line%252C%2520which%250Aassigns%2520images%2520with%2520unique%2520string%2520identifiers%2520and%2520generates%2520the%2520target%250Aidentifier%2520as%2520the%2520retrieval%2520target.%2520Despite%2520its%2520great%2520potential%252C%2520existing%250Agenerative%2520approaches%2520are%2520limited%2520due%2520to%2520the%2520following%2520issues%253A%2520insufficient%250Avisual%2520information%2520in%2520identifiers%252C%2520misalignment%2520with%2520high-level%2520semantics%252C%2520and%250Alearning%2520gap%2520towards%2520the%2520retrieval%2520target.%2520To%2520address%2520the%2520above%2520issues%252C%2520we%250Apropose%2520an%2520autoregressive%2520voken%2520generation%2520method%252C%2520named%2520AVG.%2520AVG%2520tokenizes%250Aimages%2520into%2520vokens%252C%2520i.e.%252C%2520visual%2520tokens%252C%2520and%2520innovatively%2520formulates%2520the%250Atext-to-image%2520retrieval%2520task%2520as%2520a%2520token-to-voken%2520generation%2520problem.%2520AVG%250Adiscretizes%2520an%2520image%2520into%2520a%2520sequence%2520of%2520vokens%2520as%2520the%2520identifier%2520of%2520the%2520image%252C%250Awhile%2520maintaining%2520the%2520alignment%2520with%2520both%2520the%2520visual%2520information%2520and%2520high-level%250Asemantics%2520of%2520the%2520image.%2520Additionally%252C%2520to%2520bridge%2520the%2520learning%2520gap%2520between%250Agenerative%2520training%2520and%2520the%2520retrieval%2520target%252C%2520we%2520incorporate%2520discriminative%250Atraining%2520to%2520modify%2520the%2520learning%2520direction%2520during%2520token-to-voken%2520training.%250AExtensive%2520experiments%2520demonstrate%2520that%2520AVG%2520achieves%2520superior%2520results%2520in%2520both%250Aeffectiveness%2520and%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17274v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revolutionizing%20Text-to-Image%20Retrieval%20as%20Autoregressive%20Token-to-Voken%0A%20%20Generation&entry.906535625=Yongqi%20Li%20and%20Hongru%20Cai%20and%20Wenjie%20Wang%20and%20Leigang%20Qu%20and%20Yinwei%20Wei%20and%20Wenjie%20Li%20and%20Liqiang%20Nie%20and%20Tat-Seng%20Chua&entry.1292438233=%20%20Text-to-image%20retrieval%20is%20a%20fundamental%20task%20in%20multimedia%20processing%2C%0Aaiming%20to%20retrieve%20semantically%20relevant%20cross-modal%20content.%20Traditional%0Astudies%20have%20typically%20approached%20this%20task%20as%20a%20discriminative%20problem%2C%0Amatching%20the%20text%20and%20image%20via%20the%20cross-attention%20mechanism%20%28one-tower%0Aframework%29%20or%20in%20a%20common%20embedding%20space%20%28two-tower%20framework%29.%20Recently%2C%0Agenerative%20cross-modal%20retrieval%20has%20emerged%20as%20a%20new%20research%20line%2C%20which%0Aassigns%20images%20with%20unique%20string%20identifiers%20and%20generates%20the%20target%0Aidentifier%20as%20the%20retrieval%20target.%20Despite%20its%20great%20potential%2C%20existing%0Agenerative%20approaches%20are%20limited%20due%20to%20the%20following%20issues%3A%20insufficient%0Avisual%20information%20in%20identifiers%2C%20misalignment%20with%20high-level%20semantics%2C%20and%0Alearning%20gap%20towards%20the%20retrieval%20target.%20To%20address%20the%20above%20issues%2C%20we%0Apropose%20an%20autoregressive%20voken%20generation%20method%2C%20named%20AVG.%20AVG%20tokenizes%0Aimages%20into%20vokens%2C%20i.e.%2C%20visual%20tokens%2C%20and%20innovatively%20formulates%20the%0Atext-to-image%20retrieval%20task%20as%20a%20token-to-voken%20generation%20problem.%20AVG%0Adiscretizes%20an%20image%20into%20a%20sequence%20of%20vokens%20as%20the%20identifier%20of%20the%20image%2C%0Awhile%20maintaining%20the%20alignment%20with%20both%20the%20visual%20information%20and%20high-level%0Asemantics%20of%20the%20image.%20Additionally%2C%20to%20bridge%20the%20learning%20gap%20between%0Agenerative%20training%20and%20the%20retrieval%20target%2C%20we%20incorporate%20discriminative%0Atraining%20to%20modify%20the%20learning%20direction%20during%20token-to-voken%20training.%0AExtensive%20experiments%20demonstrate%20that%20AVG%20achieves%20superior%20results%20in%20both%0Aeffectiveness%20and%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17274v1&entry.124074799=Read"},
{"title": "Global and Local Confidence Based Fraud Detection Graph Neural Network", "author": "Jiaxun Liu and Yue Tian and Guanjun Liu", "abstract": "  This paper presents the Global and Local Confidence Graph Neural Network\n(GLC-GNN), an innovative approach to graph-based anomaly detection that\naddresses the challenges of heterophily and camouflage in fraudulent\nactivities. By introducing a prototype to encapsulate the global features of a\ngraph and calculating a Global Confidence (GC) value for each node, GLC-GNN\neffectively distinguishes between benign and fraudulent nodes. The model\nleverages GC to generate attention values for message aggregation, enhancing\nits ability to capture both homophily and heterophily. Through extensive\nexperiments on four open datasets, GLC-GNN demonstrates superior performance\nover state-of-the-art models in accuracy and convergence speed, while\nmaintaining a compact model size and expedited training process. The\nintegration of global and local confidence measures in GLC-GNN offers a robust\nsolution for detecting anomalies in graphs, with significant implications for\nfraud detection across diverse domains.\n", "link": "http://arxiv.org/abs/2407.17333v1", "date": "2024-07-24", "relevancy": 2.4296, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5039}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4815}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4724}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Global%20and%20Local%20Confidence%20Based%20Fraud%20Detection%20Graph%20Neural%20Network&body=Title%3A%20Global%20and%20Local%20Confidence%20Based%20Fraud%20Detection%20Graph%20Neural%20Network%0AAuthor%3A%20Jiaxun%20Liu%20and%20Yue%20Tian%20and%20Guanjun%20Liu%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20Global%20and%20Local%20Confidence%20Graph%20Neural%20Network%0A%28GLC-GNN%29%2C%20an%20innovative%20approach%20to%20graph-based%20anomaly%20detection%20that%0Aaddresses%20the%20challenges%20of%20heterophily%20and%20camouflage%20in%20fraudulent%0Aactivities.%20By%20introducing%20a%20prototype%20to%20encapsulate%20the%20global%20features%20of%20a%0Agraph%20and%20calculating%20a%20Global%20Confidence%20%28GC%29%20value%20for%20each%20node%2C%20GLC-GNN%0Aeffectively%20distinguishes%20between%20benign%20and%20fraudulent%20nodes.%20The%20model%0Aleverages%20GC%20to%20generate%20attention%20values%20for%20message%20aggregation%2C%20enhancing%0Aits%20ability%20to%20capture%20both%20homophily%20and%20heterophily.%20Through%20extensive%0Aexperiments%20on%20four%20open%20datasets%2C%20GLC-GNN%20demonstrates%20superior%20performance%0Aover%20state-of-the-art%20models%20in%20accuracy%20and%20convergence%20speed%2C%20while%0Amaintaining%20a%20compact%20model%20size%20and%20expedited%20training%20process.%20The%0Aintegration%20of%20global%20and%20local%20confidence%20measures%20in%20GLC-GNN%20offers%20a%20robust%0Asolution%20for%20detecting%20anomalies%20in%20graphs%2C%20with%20significant%20implications%20for%0Afraud%20detection%20across%20diverse%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17333v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobal%2520and%2520Local%2520Confidence%2520Based%2520Fraud%2520Detection%2520Graph%2520Neural%2520Network%26entry.906535625%3DJiaxun%2520Liu%2520and%2520Yue%2520Tian%2520and%2520Guanjun%2520Liu%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520Global%2520and%2520Local%2520Confidence%2520Graph%2520Neural%2520Network%250A%2528GLC-GNN%2529%252C%2520an%2520innovative%2520approach%2520to%2520graph-based%2520anomaly%2520detection%2520that%250Aaddresses%2520the%2520challenges%2520of%2520heterophily%2520and%2520camouflage%2520in%2520fraudulent%250Aactivities.%2520By%2520introducing%2520a%2520prototype%2520to%2520encapsulate%2520the%2520global%2520features%2520of%2520a%250Agraph%2520and%2520calculating%2520a%2520Global%2520Confidence%2520%2528GC%2529%2520value%2520for%2520each%2520node%252C%2520GLC-GNN%250Aeffectively%2520distinguishes%2520between%2520benign%2520and%2520fraudulent%2520nodes.%2520The%2520model%250Aleverages%2520GC%2520to%2520generate%2520attention%2520values%2520for%2520message%2520aggregation%252C%2520enhancing%250Aits%2520ability%2520to%2520capture%2520both%2520homophily%2520and%2520heterophily.%2520Through%2520extensive%250Aexperiments%2520on%2520four%2520open%2520datasets%252C%2520GLC-GNN%2520demonstrates%2520superior%2520performance%250Aover%2520state-of-the-art%2520models%2520in%2520accuracy%2520and%2520convergence%2520speed%252C%2520while%250Amaintaining%2520a%2520compact%2520model%2520size%2520and%2520expedited%2520training%2520process.%2520The%250Aintegration%2520of%2520global%2520and%2520local%2520confidence%2520measures%2520in%2520GLC-GNN%2520offers%2520a%2520robust%250Asolution%2520for%2520detecting%2520anomalies%2520in%2520graphs%252C%2520with%2520significant%2520implications%2520for%250Afraud%2520detection%2520across%2520diverse%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17333v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global%20and%20Local%20Confidence%20Based%20Fraud%20Detection%20Graph%20Neural%20Network&entry.906535625=Jiaxun%20Liu%20and%20Yue%20Tian%20and%20Guanjun%20Liu&entry.1292438233=%20%20This%20paper%20presents%20the%20Global%20and%20Local%20Confidence%20Graph%20Neural%20Network%0A%28GLC-GNN%29%2C%20an%20innovative%20approach%20to%20graph-based%20anomaly%20detection%20that%0Aaddresses%20the%20challenges%20of%20heterophily%20and%20camouflage%20in%20fraudulent%0Aactivities.%20By%20introducing%20a%20prototype%20to%20encapsulate%20the%20global%20features%20of%20a%0Agraph%20and%20calculating%20a%20Global%20Confidence%20%28GC%29%20value%20for%20each%20node%2C%20GLC-GNN%0Aeffectively%20distinguishes%20between%20benign%20and%20fraudulent%20nodes.%20The%20model%0Aleverages%20GC%20to%20generate%20attention%20values%20for%20message%20aggregation%2C%20enhancing%0Aits%20ability%20to%20capture%20both%20homophily%20and%20heterophily.%20Through%20extensive%0Aexperiments%20on%20four%20open%20datasets%2C%20GLC-GNN%20demonstrates%20superior%20performance%0Aover%20state-of-the-art%20models%20in%20accuracy%20and%20convergence%20speed%2C%20while%0Amaintaining%20a%20compact%20model%20size%20and%20expedited%20training%20process.%20The%0Aintegration%20of%20global%20and%20local%20confidence%20measures%20in%20GLC-GNN%20offers%20a%20robust%0Asolution%20for%20detecting%20anomalies%20in%20graphs%2C%20with%20significant%20implications%20for%0Afraud%20detection%20across%20diverse%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17333v1&entry.124074799=Read"},
{"title": "Multi-label Cluster Discrimination for Visual Representation Learning", "author": "Xiang An and Kaicheng Yang and Xiangzi Dai and Ziyong Feng and Jiankang Deng", "abstract": "  Contrastive Language Image Pre-training (CLIP) has recently demonstrated\nsuccess across various tasks due to superior feature representation empowered\nby image-text contrastive learning. However, the instance discrimination method\nused by CLIP can hardly encode the semantic structure of training data. To\nhandle this limitation, cluster discrimination has been proposed through\niterative cluster assignment and classification. Nevertheless, most cluster\ndiscrimination approaches only define a single pseudo-label for each image,\nneglecting multi-label signals in the image. In this paper, we propose a novel\nMulti-Label Cluster Discrimination method named MLCD to enhance representation\nlearning. In the clustering step, we first cluster the large-scale LAION-400M\ndataset into one million centers based on off-the-shelf embedding features.\nConsidering that natural images frequently contain multiple visual objects or\nattributes, we select the multiple closest centers as auxiliary class labels.\nIn the discrimination step, we design a novel multi-label classification loss,\nwhich elegantly separates losses from positive classes and negative classes,\nand alleviates ambiguity on decision boundary. We validate the proposed\nmulti-label cluster discrimination method with experiments on different scales\nof models and pre-training datasets. Experimental results show that our method\nachieves state-of-the-art performance on multiple downstream tasks including\nlinear probe, zero-shot classification, and image-text retrieval.\n", "link": "http://arxiv.org/abs/2407.17331v1", "date": "2024-07-24", "relevancy": 2.3641, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.627}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5811}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5258}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-label%20Cluster%20Discrimination%20for%20Visual%20Representation%20Learning&body=Title%3A%20Multi-label%20Cluster%20Discrimination%20for%20Visual%20Representation%20Learning%0AAuthor%3A%20Xiang%20An%20and%20Kaicheng%20Yang%20and%20Xiangzi%20Dai%20and%20Ziyong%20Feng%20and%20Jiankang%20Deng%0AAbstract%3A%20%20%20Contrastive%20Language%20Image%20Pre-training%20%28CLIP%29%20has%20recently%20demonstrated%0Asuccess%20across%20various%20tasks%20due%20to%20superior%20feature%20representation%20empowered%0Aby%20image-text%20contrastive%20learning.%20However%2C%20the%20instance%20discrimination%20method%0Aused%20by%20CLIP%20can%20hardly%20encode%20the%20semantic%20structure%20of%20training%20data.%20To%0Ahandle%20this%20limitation%2C%20cluster%20discrimination%20has%20been%20proposed%20through%0Aiterative%20cluster%20assignment%20and%20classification.%20Nevertheless%2C%20most%20cluster%0Adiscrimination%20approaches%20only%20define%20a%20single%20pseudo-label%20for%20each%20image%2C%0Aneglecting%20multi-label%20signals%20in%20the%20image.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0AMulti-Label%20Cluster%20Discrimination%20method%20named%20MLCD%20to%20enhance%20representation%0Alearning.%20In%20the%20clustering%20step%2C%20we%20first%20cluster%20the%20large-scale%20LAION-400M%0Adataset%20into%20one%20million%20centers%20based%20on%20off-the-shelf%20embedding%20features.%0AConsidering%20that%20natural%20images%20frequently%20contain%20multiple%20visual%20objects%20or%0Aattributes%2C%20we%20select%20the%20multiple%20closest%20centers%20as%20auxiliary%20class%20labels.%0AIn%20the%20discrimination%20step%2C%20we%20design%20a%20novel%20multi-label%20classification%20loss%2C%0Awhich%20elegantly%20separates%20losses%20from%20positive%20classes%20and%20negative%20classes%2C%0Aand%20alleviates%20ambiguity%20on%20decision%20boundary.%20We%20validate%20the%20proposed%0Amulti-label%20cluster%20discrimination%20method%20with%20experiments%20on%20different%20scales%0Aof%20models%20and%20pre-training%20datasets.%20Experimental%20results%20show%20that%20our%20method%0Aachieves%20state-of-the-art%20performance%20on%20multiple%20downstream%20tasks%20including%0Alinear%20probe%2C%20zero-shot%20classification%2C%20and%20image-text%20retrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17331v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-label%2520Cluster%2520Discrimination%2520for%2520Visual%2520Representation%2520Learning%26entry.906535625%3DXiang%2520An%2520and%2520Kaicheng%2520Yang%2520and%2520Xiangzi%2520Dai%2520and%2520Ziyong%2520Feng%2520and%2520Jiankang%2520Deng%26entry.1292438233%3D%2520%2520Contrastive%2520Language%2520Image%2520Pre-training%2520%2528CLIP%2529%2520has%2520recently%2520demonstrated%250Asuccess%2520across%2520various%2520tasks%2520due%2520to%2520superior%2520feature%2520representation%2520empowered%250Aby%2520image-text%2520contrastive%2520learning.%2520However%252C%2520the%2520instance%2520discrimination%2520method%250Aused%2520by%2520CLIP%2520can%2520hardly%2520encode%2520the%2520semantic%2520structure%2520of%2520training%2520data.%2520To%250Ahandle%2520this%2520limitation%252C%2520cluster%2520discrimination%2520has%2520been%2520proposed%2520through%250Aiterative%2520cluster%2520assignment%2520and%2520classification.%2520Nevertheless%252C%2520most%2520cluster%250Adiscrimination%2520approaches%2520only%2520define%2520a%2520single%2520pseudo-label%2520for%2520each%2520image%252C%250Aneglecting%2520multi-label%2520signals%2520in%2520the%2520image.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250AMulti-Label%2520Cluster%2520Discrimination%2520method%2520named%2520MLCD%2520to%2520enhance%2520representation%250Alearning.%2520In%2520the%2520clustering%2520step%252C%2520we%2520first%2520cluster%2520the%2520large-scale%2520LAION-400M%250Adataset%2520into%2520one%2520million%2520centers%2520based%2520on%2520off-the-shelf%2520embedding%2520features.%250AConsidering%2520that%2520natural%2520images%2520frequently%2520contain%2520multiple%2520visual%2520objects%2520or%250Aattributes%252C%2520we%2520select%2520the%2520multiple%2520closest%2520centers%2520as%2520auxiliary%2520class%2520labels.%250AIn%2520the%2520discrimination%2520step%252C%2520we%2520design%2520a%2520novel%2520multi-label%2520classification%2520loss%252C%250Awhich%2520elegantly%2520separates%2520losses%2520from%2520positive%2520classes%2520and%2520negative%2520classes%252C%250Aand%2520alleviates%2520ambiguity%2520on%2520decision%2520boundary.%2520We%2520validate%2520the%2520proposed%250Amulti-label%2520cluster%2520discrimination%2520method%2520with%2520experiments%2520on%2520different%2520scales%250Aof%2520models%2520and%2520pre-training%2520datasets.%2520Experimental%2520results%2520show%2520that%2520our%2520method%250Aachieves%2520state-of-the-art%2520performance%2520on%2520multiple%2520downstream%2520tasks%2520including%250Alinear%2520probe%252C%2520zero-shot%2520classification%252C%2520and%2520image-text%2520retrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17331v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-label%20Cluster%20Discrimination%20for%20Visual%20Representation%20Learning&entry.906535625=Xiang%20An%20and%20Kaicheng%20Yang%20and%20Xiangzi%20Dai%20and%20Ziyong%20Feng%20and%20Jiankang%20Deng&entry.1292438233=%20%20Contrastive%20Language%20Image%20Pre-training%20%28CLIP%29%20has%20recently%20demonstrated%0Asuccess%20across%20various%20tasks%20due%20to%20superior%20feature%20representation%20empowered%0Aby%20image-text%20contrastive%20learning.%20However%2C%20the%20instance%20discrimination%20method%0Aused%20by%20CLIP%20can%20hardly%20encode%20the%20semantic%20structure%20of%20training%20data.%20To%0Ahandle%20this%20limitation%2C%20cluster%20discrimination%20has%20been%20proposed%20through%0Aiterative%20cluster%20assignment%20and%20classification.%20Nevertheless%2C%20most%20cluster%0Adiscrimination%20approaches%20only%20define%20a%20single%20pseudo-label%20for%20each%20image%2C%0Aneglecting%20multi-label%20signals%20in%20the%20image.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0AMulti-Label%20Cluster%20Discrimination%20method%20named%20MLCD%20to%20enhance%20representation%0Alearning.%20In%20the%20clustering%20step%2C%20we%20first%20cluster%20the%20large-scale%20LAION-400M%0Adataset%20into%20one%20million%20centers%20based%20on%20off-the-shelf%20embedding%20features.%0AConsidering%20that%20natural%20images%20frequently%20contain%20multiple%20visual%20objects%20or%0Aattributes%2C%20we%20select%20the%20multiple%20closest%20centers%20as%20auxiliary%20class%20labels.%0AIn%20the%20discrimination%20step%2C%20we%20design%20a%20novel%20multi-label%20classification%20loss%2C%0Awhich%20elegantly%20separates%20losses%20from%20positive%20classes%20and%20negative%20classes%2C%0Aand%20alleviates%20ambiguity%20on%20decision%20boundary.%20We%20validate%20the%20proposed%0Amulti-label%20cluster%20discrimination%20method%20with%20experiments%20on%20different%20scales%0Aof%20models%20and%20pre-training%20datasets.%20Experimental%20results%20show%20that%20our%20method%0Aachieves%20state-of-the-art%20performance%20on%20multiple%20downstream%20tasks%20including%0Alinear%20probe%2C%20zero-shot%20classification%2C%20and%20image-text%20retrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17331v1&entry.124074799=Read"},
{"title": "Multimodal Query-guided Object Localization", "author": "Aditay Tripathi and Rajath R Dani and Anand Mishra and Anirban Chakraborty", "abstract": "  Consider a scenario in one-shot query-guided object localization where\nneither an image of the object nor the object category name is available as a\nquery. In such a scenario, a hand-drawn sketch of the object could be a choice\nfor a query. However, hand-drawn crude sketches alone, when used as queries,\nmight be ambiguous for object localization, e.g., a sketch of a laptop could be\nconfused for a sofa. On the other hand, a linguistic definition of the\ncategory, e.g., a small portable computer small enough to use in your lap\"\nalong with the sketch query, gives better visual and semantic cues for object\nlocalization. In this work, we present a multimodal query-guided object\nlocalization approach under the challenging open-set setting. In particular, we\nuse queries from two modalities, namely, hand-drawn sketch and description of\nthe object (also known as gloss), to perform object localization. Multimodal\nquery-guided object localization is a challenging task, especially when a large\ndomain gap exists between the queries and the natural images, as well as due to\nthe challenge of combining the complementary and minimal information present\nacross the queries. For example, hand-drawn crude sketches contain abstract\nshape information of an object, while the text descriptions often capture\npartial semantic information about a given object category. To address the\naforementioned challenges, we present a novel cross-modal attention scheme that\nguides the region proposal network to generate object proposals relevant to the\ninput queries and a novel orthogonal projection-based proposal scoring\ntechnique that scores each proposal with respect to the queries, thereby\nyielding the final localization results. ...\n", "link": "http://arxiv.org/abs/2212.00749v2", "date": "2024-07-24", "relevancy": 2.3607, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6153}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5773}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Query-guided%20Object%20Localization&body=Title%3A%20Multimodal%20Query-guided%20Object%20Localization%0AAuthor%3A%20Aditay%20Tripathi%20and%20Rajath%20R%20Dani%20and%20Anand%20Mishra%20and%20Anirban%20Chakraborty%0AAbstract%3A%20%20%20Consider%20a%20scenario%20in%20one-shot%20query-guided%20object%20localization%20where%0Aneither%20an%20image%20of%20the%20object%20nor%20the%20object%20category%20name%20is%20available%20as%20a%0Aquery.%20In%20such%20a%20scenario%2C%20a%20hand-drawn%20sketch%20of%20the%20object%20could%20be%20a%20choice%0Afor%20a%20query.%20However%2C%20hand-drawn%20crude%20sketches%20alone%2C%20when%20used%20as%20queries%2C%0Amight%20be%20ambiguous%20for%20object%20localization%2C%20e.g.%2C%20a%20sketch%20of%20a%20laptop%20could%20be%0Aconfused%20for%20a%20sofa.%20On%20the%20other%20hand%2C%20a%20linguistic%20definition%20of%20the%0Acategory%2C%20e.g.%2C%20a%20small%20portable%20computer%20small%20enough%20to%20use%20in%20your%20lap%22%0Aalong%20with%20the%20sketch%20query%2C%20gives%20better%20visual%20and%20semantic%20cues%20for%20object%0Alocalization.%20In%20this%20work%2C%20we%20present%20a%20multimodal%20query-guided%20object%0Alocalization%20approach%20under%20the%20challenging%20open-set%20setting.%20In%20particular%2C%20we%0Ause%20queries%20from%20two%20modalities%2C%20namely%2C%20hand-drawn%20sketch%20and%20description%20of%0Athe%20object%20%28also%20known%20as%20gloss%29%2C%20to%20perform%20object%20localization.%20Multimodal%0Aquery-guided%20object%20localization%20is%20a%20challenging%20task%2C%20especially%20when%20a%20large%0Adomain%20gap%20exists%20between%20the%20queries%20and%20the%20natural%20images%2C%20as%20well%20as%20due%20to%0Athe%20challenge%20of%20combining%20the%20complementary%20and%20minimal%20information%20present%0Aacross%20the%20queries.%20For%20example%2C%20hand-drawn%20crude%20sketches%20contain%20abstract%0Ashape%20information%20of%20an%20object%2C%20while%20the%20text%20descriptions%20often%20capture%0Apartial%20semantic%20information%20about%20a%20given%20object%20category.%20To%20address%20the%0Aaforementioned%20challenges%2C%20we%20present%20a%20novel%20cross-modal%20attention%20scheme%20that%0Aguides%20the%20region%20proposal%20network%20to%20generate%20object%20proposals%20relevant%20to%20the%0Ainput%20queries%20and%20a%20novel%20orthogonal%20projection-based%20proposal%20scoring%0Atechnique%20that%20scores%20each%20proposal%20with%20respect%20to%20the%20queries%2C%20thereby%0Ayielding%20the%20final%20localization%20results.%20...%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.00749v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Query-guided%2520Object%2520Localization%26entry.906535625%3DAditay%2520Tripathi%2520and%2520Rajath%2520R%2520Dani%2520and%2520Anand%2520Mishra%2520and%2520Anirban%2520Chakraborty%26entry.1292438233%3D%2520%2520Consider%2520a%2520scenario%2520in%2520one-shot%2520query-guided%2520object%2520localization%2520where%250Aneither%2520an%2520image%2520of%2520the%2520object%2520nor%2520the%2520object%2520category%2520name%2520is%2520available%2520as%2520a%250Aquery.%2520In%2520such%2520a%2520scenario%252C%2520a%2520hand-drawn%2520sketch%2520of%2520the%2520object%2520could%2520be%2520a%2520choice%250Afor%2520a%2520query.%2520However%252C%2520hand-drawn%2520crude%2520sketches%2520alone%252C%2520when%2520used%2520as%2520queries%252C%250Amight%2520be%2520ambiguous%2520for%2520object%2520localization%252C%2520e.g.%252C%2520a%2520sketch%2520of%2520a%2520laptop%2520could%2520be%250Aconfused%2520for%2520a%2520sofa.%2520On%2520the%2520other%2520hand%252C%2520a%2520linguistic%2520definition%2520of%2520the%250Acategory%252C%2520e.g.%252C%2520a%2520small%2520portable%2520computer%2520small%2520enough%2520to%2520use%2520in%2520your%2520lap%2522%250Aalong%2520with%2520the%2520sketch%2520query%252C%2520gives%2520better%2520visual%2520and%2520semantic%2520cues%2520for%2520object%250Alocalization.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520multimodal%2520query-guided%2520object%250Alocalization%2520approach%2520under%2520the%2520challenging%2520open-set%2520setting.%2520In%2520particular%252C%2520we%250Ause%2520queries%2520from%2520two%2520modalities%252C%2520namely%252C%2520hand-drawn%2520sketch%2520and%2520description%2520of%250Athe%2520object%2520%2528also%2520known%2520as%2520gloss%2529%252C%2520to%2520perform%2520object%2520localization.%2520Multimodal%250Aquery-guided%2520object%2520localization%2520is%2520a%2520challenging%2520task%252C%2520especially%2520when%2520a%2520large%250Adomain%2520gap%2520exists%2520between%2520the%2520queries%2520and%2520the%2520natural%2520images%252C%2520as%2520well%2520as%2520due%2520to%250Athe%2520challenge%2520of%2520combining%2520the%2520complementary%2520and%2520minimal%2520information%2520present%250Aacross%2520the%2520queries.%2520For%2520example%252C%2520hand-drawn%2520crude%2520sketches%2520contain%2520abstract%250Ashape%2520information%2520of%2520an%2520object%252C%2520while%2520the%2520text%2520descriptions%2520often%2520capture%250Apartial%2520semantic%2520information%2520about%2520a%2520given%2520object%2520category.%2520To%2520address%2520the%250Aaforementioned%2520challenges%252C%2520we%2520present%2520a%2520novel%2520cross-modal%2520attention%2520scheme%2520that%250Aguides%2520the%2520region%2520proposal%2520network%2520to%2520generate%2520object%2520proposals%2520relevant%2520to%2520the%250Ainput%2520queries%2520and%2520a%2520novel%2520orthogonal%2520projection-based%2520proposal%2520scoring%250Atechnique%2520that%2520scores%2520each%2520proposal%2520with%2520respect%2520to%2520the%2520queries%252C%2520thereby%250Ayielding%2520the%2520final%2520localization%2520results.%2520...%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2212.00749v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Query-guided%20Object%20Localization&entry.906535625=Aditay%20Tripathi%20and%20Rajath%20R%20Dani%20and%20Anand%20Mishra%20and%20Anirban%20Chakraborty&entry.1292438233=%20%20Consider%20a%20scenario%20in%20one-shot%20query-guided%20object%20localization%20where%0Aneither%20an%20image%20of%20the%20object%20nor%20the%20object%20category%20name%20is%20available%20as%20a%0Aquery.%20In%20such%20a%20scenario%2C%20a%20hand-drawn%20sketch%20of%20the%20object%20could%20be%20a%20choice%0Afor%20a%20query.%20However%2C%20hand-drawn%20crude%20sketches%20alone%2C%20when%20used%20as%20queries%2C%0Amight%20be%20ambiguous%20for%20object%20localization%2C%20e.g.%2C%20a%20sketch%20of%20a%20laptop%20could%20be%0Aconfused%20for%20a%20sofa.%20On%20the%20other%20hand%2C%20a%20linguistic%20definition%20of%20the%0Acategory%2C%20e.g.%2C%20a%20small%20portable%20computer%20small%20enough%20to%20use%20in%20your%20lap%22%0Aalong%20with%20the%20sketch%20query%2C%20gives%20better%20visual%20and%20semantic%20cues%20for%20object%0Alocalization.%20In%20this%20work%2C%20we%20present%20a%20multimodal%20query-guided%20object%0Alocalization%20approach%20under%20the%20challenging%20open-set%20setting.%20In%20particular%2C%20we%0Ause%20queries%20from%20two%20modalities%2C%20namely%2C%20hand-drawn%20sketch%20and%20description%20of%0Athe%20object%20%28also%20known%20as%20gloss%29%2C%20to%20perform%20object%20localization.%20Multimodal%0Aquery-guided%20object%20localization%20is%20a%20challenging%20task%2C%20especially%20when%20a%20large%0Adomain%20gap%20exists%20between%20the%20queries%20and%20the%20natural%20images%2C%20as%20well%20as%20due%20to%0Athe%20challenge%20of%20combining%20the%20complementary%20and%20minimal%20information%20present%0Aacross%20the%20queries.%20For%20example%2C%20hand-drawn%20crude%20sketches%20contain%20abstract%0Ashape%20information%20of%20an%20object%2C%20while%20the%20text%20descriptions%20often%20capture%0Apartial%20semantic%20information%20about%20a%20given%20object%20category.%20To%20address%20the%0Aaforementioned%20challenges%2C%20we%20present%20a%20novel%20cross-modal%20attention%20scheme%20that%0Aguides%20the%20region%20proposal%20network%20to%20generate%20object%20proposals%20relevant%20to%20the%0Ainput%20queries%20and%20a%20novel%20orthogonal%20projection-based%20proposal%20scoring%0Atechnique%20that%20scores%20each%20proposal%20with%20respect%20to%20the%20queries%2C%20thereby%0Ayielding%20the%20final%20localization%20results.%20...%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.00749v2&entry.124074799=Read"},
{"title": "MutDet: Mutually Optimizing Pre-training for Remote Sensing Object\n  Detection", "author": "Ziyue Huang and Yongchao Feng and Qingjie Liu and Yunhong Wang", "abstract": "  Detection pre-training methods for the DETR series detector have been\nextensively studied in natural scenes, e.g., DETReg. However, the detection\npre-training remains unexplored in remote sensing scenes. In existing\npre-training methods, alignment between object embeddings extracted from a\npre-trained backbone and detector features is significant. However, due to\ndifferences in feature extraction methods, a pronounced feature discrepancy\nstill exists and hinders the pre-training performance. The remote sensing\nimages with complex environments and more densely distributed objects\nexacerbate the discrepancy. In this work, we propose a novel Mutually\noptimizing pre-training framework for remote sensing object Detection, dubbed\nas MutDet. In MutDet, we propose a systemic solution against this challenge.\nFirstly, we propose a mutual enhancement module, which fuses the object\nembeddings and detector features bidirectionally in the last encoder layer,\nenhancing their information interaction.Secondly, contrastive alignment loss is\nemployed to guide this alignment process softly and simultaneously enhances\ndetector features' discriminativity. Finally, we design an auxiliary siamese\nhead to mitigate the task gap arising from the introduction of enhancement\nmodule. Comprehensive experiments on various settings show new state-of-the-art\ntransfer performance. The improvement is particularly pronounced when data\nquantity is limited. When using 10% of the DIOR-R data, MutDet improves DetReg\nby 6.1% in AP50. Codes and models are available at:\nhttps://github.com/floatingstarZ/MutDet.\n", "link": "http://arxiv.org/abs/2407.09920v2", "date": "2024-07-24", "relevancy": 2.3455, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6107}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5714}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MutDet%3A%20Mutually%20Optimizing%20Pre-training%20for%20Remote%20Sensing%20Object%0A%20%20Detection&body=Title%3A%20MutDet%3A%20Mutually%20Optimizing%20Pre-training%20for%20Remote%20Sensing%20Object%0A%20%20Detection%0AAuthor%3A%20Ziyue%20Huang%20and%20Yongchao%20Feng%20and%20Qingjie%20Liu%20and%20Yunhong%20Wang%0AAbstract%3A%20%20%20Detection%20pre-training%20methods%20for%20the%20DETR%20series%20detector%20have%20been%0Aextensively%20studied%20in%20natural%20scenes%2C%20e.g.%2C%20DETReg.%20However%2C%20the%20detection%0Apre-training%20remains%20unexplored%20in%20remote%20sensing%20scenes.%20In%20existing%0Apre-training%20methods%2C%20alignment%20between%20object%20embeddings%20extracted%20from%20a%0Apre-trained%20backbone%20and%20detector%20features%20is%20significant.%20However%2C%20due%20to%0Adifferences%20in%20feature%20extraction%20methods%2C%20a%20pronounced%20feature%20discrepancy%0Astill%20exists%20and%20hinders%20the%20pre-training%20performance.%20The%20remote%20sensing%0Aimages%20with%20complex%20environments%20and%20more%20densely%20distributed%20objects%0Aexacerbate%20the%20discrepancy.%20In%20this%20work%2C%20we%20propose%20a%20novel%20Mutually%0Aoptimizing%20pre-training%20framework%20for%20remote%20sensing%20object%20Detection%2C%20dubbed%0Aas%20MutDet.%20In%20MutDet%2C%20we%20propose%20a%20systemic%20solution%20against%20this%20challenge.%0AFirstly%2C%20we%20propose%20a%20mutual%20enhancement%20module%2C%20which%20fuses%20the%20object%0Aembeddings%20and%20detector%20features%20bidirectionally%20in%20the%20last%20encoder%20layer%2C%0Aenhancing%20their%20information%20interaction.Secondly%2C%20contrastive%20alignment%20loss%20is%0Aemployed%20to%20guide%20this%20alignment%20process%20softly%20and%20simultaneously%20enhances%0Adetector%20features%27%20discriminativity.%20Finally%2C%20we%20design%20an%20auxiliary%20siamese%0Ahead%20to%20mitigate%20the%20task%20gap%20arising%20from%20the%20introduction%20of%20enhancement%0Amodule.%20Comprehensive%20experiments%20on%20various%20settings%20show%20new%20state-of-the-art%0Atransfer%20performance.%20The%20improvement%20is%20particularly%20pronounced%20when%20data%0Aquantity%20is%20limited.%20When%20using%2010%25%20of%20the%20DIOR-R%20data%2C%20MutDet%20improves%20DetReg%0Aby%206.1%25%20in%20AP50.%20Codes%20and%20models%20are%20available%20at%3A%0Ahttps%3A//github.com/floatingstarZ/MutDet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09920v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMutDet%253A%2520Mutually%2520Optimizing%2520Pre-training%2520for%2520Remote%2520Sensing%2520Object%250A%2520%2520Detection%26entry.906535625%3DZiyue%2520Huang%2520and%2520Yongchao%2520Feng%2520and%2520Qingjie%2520Liu%2520and%2520Yunhong%2520Wang%26entry.1292438233%3D%2520%2520Detection%2520pre-training%2520methods%2520for%2520the%2520DETR%2520series%2520detector%2520have%2520been%250Aextensively%2520studied%2520in%2520natural%2520scenes%252C%2520e.g.%252C%2520DETReg.%2520However%252C%2520the%2520detection%250Apre-training%2520remains%2520unexplored%2520in%2520remote%2520sensing%2520scenes.%2520In%2520existing%250Apre-training%2520methods%252C%2520alignment%2520between%2520object%2520embeddings%2520extracted%2520from%2520a%250Apre-trained%2520backbone%2520and%2520detector%2520features%2520is%2520significant.%2520However%252C%2520due%2520to%250Adifferences%2520in%2520feature%2520extraction%2520methods%252C%2520a%2520pronounced%2520feature%2520discrepancy%250Astill%2520exists%2520and%2520hinders%2520the%2520pre-training%2520performance.%2520The%2520remote%2520sensing%250Aimages%2520with%2520complex%2520environments%2520and%2520more%2520densely%2520distributed%2520objects%250Aexacerbate%2520the%2520discrepancy.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520Mutually%250Aoptimizing%2520pre-training%2520framework%2520for%2520remote%2520sensing%2520object%2520Detection%252C%2520dubbed%250Aas%2520MutDet.%2520In%2520MutDet%252C%2520we%2520propose%2520a%2520systemic%2520solution%2520against%2520this%2520challenge.%250AFirstly%252C%2520we%2520propose%2520a%2520mutual%2520enhancement%2520module%252C%2520which%2520fuses%2520the%2520object%250Aembeddings%2520and%2520detector%2520features%2520bidirectionally%2520in%2520the%2520last%2520encoder%2520layer%252C%250Aenhancing%2520their%2520information%2520interaction.Secondly%252C%2520contrastive%2520alignment%2520loss%2520is%250Aemployed%2520to%2520guide%2520this%2520alignment%2520process%2520softly%2520and%2520simultaneously%2520enhances%250Adetector%2520features%2527%2520discriminativity.%2520Finally%252C%2520we%2520design%2520an%2520auxiliary%2520siamese%250Ahead%2520to%2520mitigate%2520the%2520task%2520gap%2520arising%2520from%2520the%2520introduction%2520of%2520enhancement%250Amodule.%2520Comprehensive%2520experiments%2520on%2520various%2520settings%2520show%2520new%2520state-of-the-art%250Atransfer%2520performance.%2520The%2520improvement%2520is%2520particularly%2520pronounced%2520when%2520data%250Aquantity%2520is%2520limited.%2520When%2520using%252010%2525%2520of%2520the%2520DIOR-R%2520data%252C%2520MutDet%2520improves%2520DetReg%250Aby%25206.1%2525%2520in%2520AP50.%2520Codes%2520and%2520models%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/floatingstarZ/MutDet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09920v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MutDet%3A%20Mutually%20Optimizing%20Pre-training%20for%20Remote%20Sensing%20Object%0A%20%20Detection&entry.906535625=Ziyue%20Huang%20and%20Yongchao%20Feng%20and%20Qingjie%20Liu%20and%20Yunhong%20Wang&entry.1292438233=%20%20Detection%20pre-training%20methods%20for%20the%20DETR%20series%20detector%20have%20been%0Aextensively%20studied%20in%20natural%20scenes%2C%20e.g.%2C%20DETReg.%20However%2C%20the%20detection%0Apre-training%20remains%20unexplored%20in%20remote%20sensing%20scenes.%20In%20existing%0Apre-training%20methods%2C%20alignment%20between%20object%20embeddings%20extracted%20from%20a%0Apre-trained%20backbone%20and%20detector%20features%20is%20significant.%20However%2C%20due%20to%0Adifferences%20in%20feature%20extraction%20methods%2C%20a%20pronounced%20feature%20discrepancy%0Astill%20exists%20and%20hinders%20the%20pre-training%20performance.%20The%20remote%20sensing%0Aimages%20with%20complex%20environments%20and%20more%20densely%20distributed%20objects%0Aexacerbate%20the%20discrepancy.%20In%20this%20work%2C%20we%20propose%20a%20novel%20Mutually%0Aoptimizing%20pre-training%20framework%20for%20remote%20sensing%20object%20Detection%2C%20dubbed%0Aas%20MutDet.%20In%20MutDet%2C%20we%20propose%20a%20systemic%20solution%20against%20this%20challenge.%0AFirstly%2C%20we%20propose%20a%20mutual%20enhancement%20module%2C%20which%20fuses%20the%20object%0Aembeddings%20and%20detector%20features%20bidirectionally%20in%20the%20last%20encoder%20layer%2C%0Aenhancing%20their%20information%20interaction.Secondly%2C%20contrastive%20alignment%20loss%20is%0Aemployed%20to%20guide%20this%20alignment%20process%20softly%20and%20simultaneously%20enhances%0Adetector%20features%27%20discriminativity.%20Finally%2C%20we%20design%20an%20auxiliary%20siamese%0Ahead%20to%20mitigate%20the%20task%20gap%20arising%20from%20the%20introduction%20of%20enhancement%0Amodule.%20Comprehensive%20experiments%20on%20various%20settings%20show%20new%20state-of-the-art%0Atransfer%20performance.%20The%20improvement%20is%20particularly%20pronounced%20when%20data%0Aquantity%20is%20limited.%20When%20using%2010%25%20of%20the%20DIOR-R%20data%2C%20MutDet%20improves%20DetReg%0Aby%206.1%25%20in%20AP50.%20Codes%20and%20models%20are%20available%20at%3A%0Ahttps%3A//github.com/floatingstarZ/MutDet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09920v2&entry.124074799=Read"},
{"title": "Velocity Driven Vision: Asynchronous Sensor Fusion Birds Eye View Models\n  for Autonomous Vehicles", "author": "Seamie Hayes and Sushil Sharma and Ciar\u00e1n Eising", "abstract": "  Fusing different sensor modalities can be a difficult task, particularly if\nthey are asynchronous. Asynchronisation may arise due to long processing times\nor improper synchronisation during calibration, and there must exist a way to\nstill utilise this previous information for the purpose of safe driving, and\nobject detection in ego vehicle/ multi-agent trajectory prediction.\nDifficulties arise in the fact that the sensor modalities have captured\ninformation at different times and also at different positions in space.\nTherefore, they are not spatially nor temporally aligned. This paper will\ninvestigate the challenge of radar and LiDAR sensors being asynchronous\nrelative to the camera sensors, for various time latencies. The spatial\nalignment will be resolved before lifting into BEV space via the transformation\nof the radar/LiDAR point clouds into the new ego frame coordinate system. Only\nafter this can we concatenate the radar/LiDAR point cloud and lifted camera\nfeatures. Temporal alignment will be remedied for radar data only, we will\nimplement a novel method of inferring the future radar point positions using\nthe velocity information. Our approach to resolving the issue of sensor\nasynchrony yields promising results. We demonstrate velocity information can\ndrastically improve IoU for asynchronous datasets, as for a time latency of 360\nmilliseconds (ms), IoU improves from 49.54 to 53.63. Additionally, for a time\nlatency of 550ms, the camera+radar (C+R) model outperforms the camera+LiDAR\n(C+L) model by 0.18 IoU. This is an advancement in utilising the\noften-neglected radar sensor modality, which is less favoured than LiDAR for\nautonomous driving purposes.\n", "link": "http://arxiv.org/abs/2407.16636v2", "date": "2024-07-24", "relevancy": 2.3315, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5944}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5862}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.57}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Velocity%20Driven%20Vision%3A%20Asynchronous%20Sensor%20Fusion%20Birds%20Eye%20View%20Models%0A%20%20for%20Autonomous%20Vehicles&body=Title%3A%20Velocity%20Driven%20Vision%3A%20Asynchronous%20Sensor%20Fusion%20Birds%20Eye%20View%20Models%0A%20%20for%20Autonomous%20Vehicles%0AAuthor%3A%20Seamie%20Hayes%20and%20Sushil%20Sharma%20and%20Ciar%C3%A1n%20Eising%0AAbstract%3A%20%20%20Fusing%20different%20sensor%20modalities%20can%20be%20a%20difficult%20task%2C%20particularly%20if%0Athey%20are%20asynchronous.%20Asynchronisation%20may%20arise%20due%20to%20long%20processing%20times%0Aor%20improper%20synchronisation%20during%20calibration%2C%20and%20there%20must%20exist%20a%20way%20to%0Astill%20utilise%20this%20previous%20information%20for%20the%20purpose%20of%20safe%20driving%2C%20and%0Aobject%20detection%20in%20ego%20vehicle/%20multi-agent%20trajectory%20prediction.%0ADifficulties%20arise%20in%20the%20fact%20that%20the%20sensor%20modalities%20have%20captured%0Ainformation%20at%20different%20times%20and%20also%20at%20different%20positions%20in%20space.%0ATherefore%2C%20they%20are%20not%20spatially%20nor%20temporally%20aligned.%20This%20paper%20will%0Ainvestigate%20the%20challenge%20of%20radar%20and%20LiDAR%20sensors%20being%20asynchronous%0Arelative%20to%20the%20camera%20sensors%2C%20for%20various%20time%20latencies.%20The%20spatial%0Aalignment%20will%20be%20resolved%20before%20lifting%20into%20BEV%20space%20via%20the%20transformation%0Aof%20the%20radar/LiDAR%20point%20clouds%20into%20the%20new%20ego%20frame%20coordinate%20system.%20Only%0Aafter%20this%20can%20we%20concatenate%20the%20radar/LiDAR%20point%20cloud%20and%20lifted%20camera%0Afeatures.%20Temporal%20alignment%20will%20be%20remedied%20for%20radar%20data%20only%2C%20we%20will%0Aimplement%20a%20novel%20method%20of%20inferring%20the%20future%20radar%20point%20positions%20using%0Athe%20velocity%20information.%20Our%20approach%20to%20resolving%20the%20issue%20of%20sensor%0Aasynchrony%20yields%20promising%20results.%20We%20demonstrate%20velocity%20information%20can%0Adrastically%20improve%20IoU%20for%20asynchronous%20datasets%2C%20as%20for%20a%20time%20latency%20of%20360%0Amilliseconds%20%28ms%29%2C%20IoU%20improves%20from%2049.54%20to%2053.63.%20Additionally%2C%20for%20a%20time%0Alatency%20of%20550ms%2C%20the%20camera%2Bradar%20%28C%2BR%29%20model%20outperforms%20the%20camera%2BLiDAR%0A%28C%2BL%29%20model%20by%200.18%20IoU.%20This%20is%20an%20advancement%20in%20utilising%20the%0Aoften-neglected%20radar%20sensor%20modality%2C%20which%20is%20less%20favoured%20than%20LiDAR%20for%0Aautonomous%20driving%20purposes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16636v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVelocity%2520Driven%2520Vision%253A%2520Asynchronous%2520Sensor%2520Fusion%2520Birds%2520Eye%2520View%2520Models%250A%2520%2520for%2520Autonomous%2520Vehicles%26entry.906535625%3DSeamie%2520Hayes%2520and%2520Sushil%2520Sharma%2520and%2520Ciar%25C3%25A1n%2520Eising%26entry.1292438233%3D%2520%2520Fusing%2520different%2520sensor%2520modalities%2520can%2520be%2520a%2520difficult%2520task%252C%2520particularly%2520if%250Athey%2520are%2520asynchronous.%2520Asynchronisation%2520may%2520arise%2520due%2520to%2520long%2520processing%2520times%250Aor%2520improper%2520synchronisation%2520during%2520calibration%252C%2520and%2520there%2520must%2520exist%2520a%2520way%2520to%250Astill%2520utilise%2520this%2520previous%2520information%2520for%2520the%2520purpose%2520of%2520safe%2520driving%252C%2520and%250Aobject%2520detection%2520in%2520ego%2520vehicle/%2520multi-agent%2520trajectory%2520prediction.%250ADifficulties%2520arise%2520in%2520the%2520fact%2520that%2520the%2520sensor%2520modalities%2520have%2520captured%250Ainformation%2520at%2520different%2520times%2520and%2520also%2520at%2520different%2520positions%2520in%2520space.%250ATherefore%252C%2520they%2520are%2520not%2520spatially%2520nor%2520temporally%2520aligned.%2520This%2520paper%2520will%250Ainvestigate%2520the%2520challenge%2520of%2520radar%2520and%2520LiDAR%2520sensors%2520being%2520asynchronous%250Arelative%2520to%2520the%2520camera%2520sensors%252C%2520for%2520various%2520time%2520latencies.%2520The%2520spatial%250Aalignment%2520will%2520be%2520resolved%2520before%2520lifting%2520into%2520BEV%2520space%2520via%2520the%2520transformation%250Aof%2520the%2520radar/LiDAR%2520point%2520clouds%2520into%2520the%2520new%2520ego%2520frame%2520coordinate%2520system.%2520Only%250Aafter%2520this%2520can%2520we%2520concatenate%2520the%2520radar/LiDAR%2520point%2520cloud%2520and%2520lifted%2520camera%250Afeatures.%2520Temporal%2520alignment%2520will%2520be%2520remedied%2520for%2520radar%2520data%2520only%252C%2520we%2520will%250Aimplement%2520a%2520novel%2520method%2520of%2520inferring%2520the%2520future%2520radar%2520point%2520positions%2520using%250Athe%2520velocity%2520information.%2520Our%2520approach%2520to%2520resolving%2520the%2520issue%2520of%2520sensor%250Aasynchrony%2520yields%2520promising%2520results.%2520We%2520demonstrate%2520velocity%2520information%2520can%250Adrastically%2520improve%2520IoU%2520for%2520asynchronous%2520datasets%252C%2520as%2520for%2520a%2520time%2520latency%2520of%2520360%250Amilliseconds%2520%2528ms%2529%252C%2520IoU%2520improves%2520from%252049.54%2520to%252053.63.%2520Additionally%252C%2520for%2520a%2520time%250Alatency%2520of%2520550ms%252C%2520the%2520camera%252Bradar%2520%2528C%252BR%2529%2520model%2520outperforms%2520the%2520camera%252BLiDAR%250A%2528C%252BL%2529%2520model%2520by%25200.18%2520IoU.%2520This%2520is%2520an%2520advancement%2520in%2520utilising%2520the%250Aoften-neglected%2520radar%2520sensor%2520modality%252C%2520which%2520is%2520less%2520favoured%2520than%2520LiDAR%2520for%250Aautonomous%2520driving%2520purposes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16636v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Velocity%20Driven%20Vision%3A%20Asynchronous%20Sensor%20Fusion%20Birds%20Eye%20View%20Models%0A%20%20for%20Autonomous%20Vehicles&entry.906535625=Seamie%20Hayes%20and%20Sushil%20Sharma%20and%20Ciar%C3%A1n%20Eising&entry.1292438233=%20%20Fusing%20different%20sensor%20modalities%20can%20be%20a%20difficult%20task%2C%20particularly%20if%0Athey%20are%20asynchronous.%20Asynchronisation%20may%20arise%20due%20to%20long%20processing%20times%0Aor%20improper%20synchronisation%20during%20calibration%2C%20and%20there%20must%20exist%20a%20way%20to%0Astill%20utilise%20this%20previous%20information%20for%20the%20purpose%20of%20safe%20driving%2C%20and%0Aobject%20detection%20in%20ego%20vehicle/%20multi-agent%20trajectory%20prediction.%0ADifficulties%20arise%20in%20the%20fact%20that%20the%20sensor%20modalities%20have%20captured%0Ainformation%20at%20different%20times%20and%20also%20at%20different%20positions%20in%20space.%0ATherefore%2C%20they%20are%20not%20spatially%20nor%20temporally%20aligned.%20This%20paper%20will%0Ainvestigate%20the%20challenge%20of%20radar%20and%20LiDAR%20sensors%20being%20asynchronous%0Arelative%20to%20the%20camera%20sensors%2C%20for%20various%20time%20latencies.%20The%20spatial%0Aalignment%20will%20be%20resolved%20before%20lifting%20into%20BEV%20space%20via%20the%20transformation%0Aof%20the%20radar/LiDAR%20point%20clouds%20into%20the%20new%20ego%20frame%20coordinate%20system.%20Only%0Aafter%20this%20can%20we%20concatenate%20the%20radar/LiDAR%20point%20cloud%20and%20lifted%20camera%0Afeatures.%20Temporal%20alignment%20will%20be%20remedied%20for%20radar%20data%20only%2C%20we%20will%0Aimplement%20a%20novel%20method%20of%20inferring%20the%20future%20radar%20point%20positions%20using%0Athe%20velocity%20information.%20Our%20approach%20to%20resolving%20the%20issue%20of%20sensor%0Aasynchrony%20yields%20promising%20results.%20We%20demonstrate%20velocity%20information%20can%0Adrastically%20improve%20IoU%20for%20asynchronous%20datasets%2C%20as%20for%20a%20time%20latency%20of%20360%0Amilliseconds%20%28ms%29%2C%20IoU%20improves%20from%2049.54%20to%2053.63.%20Additionally%2C%20for%20a%20time%0Alatency%20of%20550ms%2C%20the%20camera%2Bradar%20%28C%2BR%29%20model%20outperforms%20the%20camera%2BLiDAR%0A%28C%2BL%29%20model%20by%200.18%20IoU.%20This%20is%20an%20advancement%20in%20utilising%20the%0Aoften-neglected%20radar%20sensor%20modality%2C%20which%20is%20less%20favoured%20than%20LiDAR%20for%0Aautonomous%20driving%20purposes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16636v2&entry.124074799=Read"},
{"title": "SRFNet: Monocular Depth Estimation with Fine-grained Structure via\n  Spatial Reliability-oriented Fusion of Frames and Events", "author": "Tianbo Pan and Zidong Cao and Lin Wang", "abstract": "  Monocular depth estimation is a crucial task to measure distance relative to\na camera, which is important for applications, such as robot navigation and\nself-driving. Traditional frame-based methods suffer from performance drops due\nto the limited dynamic range and motion blur. Therefore, recent works leverage\nnovel event cameras to complement or guide the frame modality via frame-event\nfeature fusion. However, event streams exhibit spatial sparsity, leaving some\nareas unperceived, especially in regions with marginal light changes.\nTherefore, direct fusion methods, e.g., RAMNet, often ignore the contribution\nof the most confident regions of each modality. This leads to structural\nambiguity in the modality fusion process, thus degrading the depth estimation\nperformance. In this paper, we propose a novel Spatial Reliability-oriented\nFusion Network (SRFNet), that can estimate depth with fine-grained structure at\nboth daytime and nighttime. Our method consists of two key technical\ncomponents. Firstly, we propose an attention-based interactive fusion (AIF)\nmodule that applies spatial priors of events and frames as the initial masks\nand learns the consensus regions to guide the inter-modal feature fusion. The\nfused feature are then fed back to enhance the frame and event feature\nlearning. Meanwhile, it utilizes an output head to generate a fused mask, which\nis iteratively updated for learning consensual spatial priors. Secondly, we\npropose the Reliability-oriented Depth Refinement (RDR) module to estimate\ndense depth with the fine-grained structure based on the fused features and\nmasks. We evaluate the effectiveness of our method on the synthetic and\nreal-world datasets, which shows that, even without pretraining, our method\noutperforms the prior methods, e.g., RAMNet, especially in night scenes. Our\nproject homepage: https://vlislab22.github.io/SRFNet.\n", "link": "http://arxiv.org/abs/2309.12842v2", "date": "2024-07-24", "relevancy": 2.326, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6087}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.563}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SRFNet%3A%20Monocular%20Depth%20Estimation%20with%20Fine-grained%20Structure%20via%0A%20%20Spatial%20Reliability-oriented%20Fusion%20of%20Frames%20and%20Events&body=Title%3A%20SRFNet%3A%20Monocular%20Depth%20Estimation%20with%20Fine-grained%20Structure%20via%0A%20%20Spatial%20Reliability-oriented%20Fusion%20of%20Frames%20and%20Events%0AAuthor%3A%20Tianbo%20Pan%20and%20Zidong%20Cao%20and%20Lin%20Wang%0AAbstract%3A%20%20%20Monocular%20depth%20estimation%20is%20a%20crucial%20task%20to%20measure%20distance%20relative%20to%0Aa%20camera%2C%20which%20is%20important%20for%20applications%2C%20such%20as%20robot%20navigation%20and%0Aself-driving.%20Traditional%20frame-based%20methods%20suffer%20from%20performance%20drops%20due%0Ato%20the%20limited%20dynamic%20range%20and%20motion%20blur.%20Therefore%2C%20recent%20works%20leverage%0Anovel%20event%20cameras%20to%20complement%20or%20guide%20the%20frame%20modality%20via%20frame-event%0Afeature%20fusion.%20However%2C%20event%20streams%20exhibit%20spatial%20sparsity%2C%20leaving%20some%0Aareas%20unperceived%2C%20especially%20in%20regions%20with%20marginal%20light%20changes.%0ATherefore%2C%20direct%20fusion%20methods%2C%20e.g.%2C%20RAMNet%2C%20often%20ignore%20the%20contribution%0Aof%20the%20most%20confident%20regions%20of%20each%20modality.%20This%20leads%20to%20structural%0Aambiguity%20in%20the%20modality%20fusion%20process%2C%20thus%20degrading%20the%20depth%20estimation%0Aperformance.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Spatial%20Reliability-oriented%0AFusion%20Network%20%28SRFNet%29%2C%20that%20can%20estimate%20depth%20with%20fine-grained%20structure%20at%0Aboth%20daytime%20and%20nighttime.%20Our%20method%20consists%20of%20two%20key%20technical%0Acomponents.%20Firstly%2C%20we%20propose%20an%20attention-based%20interactive%20fusion%20%28AIF%29%0Amodule%20that%20applies%20spatial%20priors%20of%20events%20and%20frames%20as%20the%20initial%20masks%0Aand%20learns%20the%20consensus%20regions%20to%20guide%20the%20inter-modal%20feature%20fusion.%20The%0Afused%20feature%20are%20then%20fed%20back%20to%20enhance%20the%20frame%20and%20event%20feature%0Alearning.%20Meanwhile%2C%20it%20utilizes%20an%20output%20head%20to%20generate%20a%20fused%20mask%2C%20which%0Ais%20iteratively%20updated%20for%20learning%20consensual%20spatial%20priors.%20Secondly%2C%20we%0Apropose%20the%20Reliability-oriented%20Depth%20Refinement%20%28RDR%29%20module%20to%20estimate%0Adense%20depth%20with%20the%20fine-grained%20structure%20based%20on%20the%20fused%20features%20and%0Amasks.%20We%20evaluate%20the%20effectiveness%20of%20our%20method%20on%20the%20synthetic%20and%0Areal-world%20datasets%2C%20which%20shows%20that%2C%20even%20without%20pretraining%2C%20our%20method%0Aoutperforms%20the%20prior%20methods%2C%20e.g.%2C%20RAMNet%2C%20especially%20in%20night%20scenes.%20Our%0Aproject%20homepage%3A%20https%3A//vlislab22.github.io/SRFNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.12842v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSRFNet%253A%2520Monocular%2520Depth%2520Estimation%2520with%2520Fine-grained%2520Structure%2520via%250A%2520%2520Spatial%2520Reliability-oriented%2520Fusion%2520of%2520Frames%2520and%2520Events%26entry.906535625%3DTianbo%2520Pan%2520and%2520Zidong%2520Cao%2520and%2520Lin%2520Wang%26entry.1292438233%3D%2520%2520Monocular%2520depth%2520estimation%2520is%2520a%2520crucial%2520task%2520to%2520measure%2520distance%2520relative%2520to%250Aa%2520camera%252C%2520which%2520is%2520important%2520for%2520applications%252C%2520such%2520as%2520robot%2520navigation%2520and%250Aself-driving.%2520Traditional%2520frame-based%2520methods%2520suffer%2520from%2520performance%2520drops%2520due%250Ato%2520the%2520limited%2520dynamic%2520range%2520and%2520motion%2520blur.%2520Therefore%252C%2520recent%2520works%2520leverage%250Anovel%2520event%2520cameras%2520to%2520complement%2520or%2520guide%2520the%2520frame%2520modality%2520via%2520frame-event%250Afeature%2520fusion.%2520However%252C%2520event%2520streams%2520exhibit%2520spatial%2520sparsity%252C%2520leaving%2520some%250Aareas%2520unperceived%252C%2520especially%2520in%2520regions%2520with%2520marginal%2520light%2520changes.%250ATherefore%252C%2520direct%2520fusion%2520methods%252C%2520e.g.%252C%2520RAMNet%252C%2520often%2520ignore%2520the%2520contribution%250Aof%2520the%2520most%2520confident%2520regions%2520of%2520each%2520modality.%2520This%2520leads%2520to%2520structural%250Aambiguity%2520in%2520the%2520modality%2520fusion%2520process%252C%2520thus%2520degrading%2520the%2520depth%2520estimation%250Aperformance.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520Spatial%2520Reliability-oriented%250AFusion%2520Network%2520%2528SRFNet%2529%252C%2520that%2520can%2520estimate%2520depth%2520with%2520fine-grained%2520structure%2520at%250Aboth%2520daytime%2520and%2520nighttime.%2520Our%2520method%2520consists%2520of%2520two%2520key%2520technical%250Acomponents.%2520Firstly%252C%2520we%2520propose%2520an%2520attention-based%2520interactive%2520fusion%2520%2528AIF%2529%250Amodule%2520that%2520applies%2520spatial%2520priors%2520of%2520events%2520and%2520frames%2520as%2520the%2520initial%2520masks%250Aand%2520learns%2520the%2520consensus%2520regions%2520to%2520guide%2520the%2520inter-modal%2520feature%2520fusion.%2520The%250Afused%2520feature%2520are%2520then%2520fed%2520back%2520to%2520enhance%2520the%2520frame%2520and%2520event%2520feature%250Alearning.%2520Meanwhile%252C%2520it%2520utilizes%2520an%2520output%2520head%2520to%2520generate%2520a%2520fused%2520mask%252C%2520which%250Ais%2520iteratively%2520updated%2520for%2520learning%2520consensual%2520spatial%2520priors.%2520Secondly%252C%2520we%250Apropose%2520the%2520Reliability-oriented%2520Depth%2520Refinement%2520%2528RDR%2529%2520module%2520to%2520estimate%250Adense%2520depth%2520with%2520the%2520fine-grained%2520structure%2520based%2520on%2520the%2520fused%2520features%2520and%250Amasks.%2520We%2520evaluate%2520the%2520effectiveness%2520of%2520our%2520method%2520on%2520the%2520synthetic%2520and%250Areal-world%2520datasets%252C%2520which%2520shows%2520that%252C%2520even%2520without%2520pretraining%252C%2520our%2520method%250Aoutperforms%2520the%2520prior%2520methods%252C%2520e.g.%252C%2520RAMNet%252C%2520especially%2520in%2520night%2520scenes.%2520Our%250Aproject%2520homepage%253A%2520https%253A//vlislab22.github.io/SRFNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.12842v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SRFNet%3A%20Monocular%20Depth%20Estimation%20with%20Fine-grained%20Structure%20via%0A%20%20Spatial%20Reliability-oriented%20Fusion%20of%20Frames%20and%20Events&entry.906535625=Tianbo%20Pan%20and%20Zidong%20Cao%20and%20Lin%20Wang&entry.1292438233=%20%20Monocular%20depth%20estimation%20is%20a%20crucial%20task%20to%20measure%20distance%20relative%20to%0Aa%20camera%2C%20which%20is%20important%20for%20applications%2C%20such%20as%20robot%20navigation%20and%0Aself-driving.%20Traditional%20frame-based%20methods%20suffer%20from%20performance%20drops%20due%0Ato%20the%20limited%20dynamic%20range%20and%20motion%20blur.%20Therefore%2C%20recent%20works%20leverage%0Anovel%20event%20cameras%20to%20complement%20or%20guide%20the%20frame%20modality%20via%20frame-event%0Afeature%20fusion.%20However%2C%20event%20streams%20exhibit%20spatial%20sparsity%2C%20leaving%20some%0Aareas%20unperceived%2C%20especially%20in%20regions%20with%20marginal%20light%20changes.%0ATherefore%2C%20direct%20fusion%20methods%2C%20e.g.%2C%20RAMNet%2C%20often%20ignore%20the%20contribution%0Aof%20the%20most%20confident%20regions%20of%20each%20modality.%20This%20leads%20to%20structural%0Aambiguity%20in%20the%20modality%20fusion%20process%2C%20thus%20degrading%20the%20depth%20estimation%0Aperformance.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Spatial%20Reliability-oriented%0AFusion%20Network%20%28SRFNet%29%2C%20that%20can%20estimate%20depth%20with%20fine-grained%20structure%20at%0Aboth%20daytime%20and%20nighttime.%20Our%20method%20consists%20of%20two%20key%20technical%0Acomponents.%20Firstly%2C%20we%20propose%20an%20attention-based%20interactive%20fusion%20%28AIF%29%0Amodule%20that%20applies%20spatial%20priors%20of%20events%20and%20frames%20as%20the%20initial%20masks%0Aand%20learns%20the%20consensus%20regions%20to%20guide%20the%20inter-modal%20feature%20fusion.%20The%0Afused%20feature%20are%20then%20fed%20back%20to%20enhance%20the%20frame%20and%20event%20feature%0Alearning.%20Meanwhile%2C%20it%20utilizes%20an%20output%20head%20to%20generate%20a%20fused%20mask%2C%20which%0Ais%20iteratively%20updated%20for%20learning%20consensual%20spatial%20priors.%20Secondly%2C%20we%0Apropose%20the%20Reliability-oriented%20Depth%20Refinement%20%28RDR%29%20module%20to%20estimate%0Adense%20depth%20with%20the%20fine-grained%20structure%20based%20on%20the%20fused%20features%20and%0Amasks.%20We%20evaluate%20the%20effectiveness%20of%20our%20method%20on%20the%20synthetic%20and%0Areal-world%20datasets%2C%20which%20shows%20that%2C%20even%20without%20pretraining%2C%20our%20method%0Aoutperforms%20the%20prior%20methods%2C%20e.g.%2C%20RAMNet%2C%20especially%20in%20night%20scenes.%20Our%0Aproject%20homepage%3A%20https%3A//vlislab22.github.io/SRFNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.12842v2&entry.124074799=Read"},
{"title": "Efficient 3D-Aware Facial Image Editing via Attribute-Specific Prompt\n  Learning", "author": "Amandeep Kumar and Muhammad Awais and Sanath Narayan and Hisham Cholakkal and Salman Khan and Rao Muhammad Anwer", "abstract": "  Drawing upon StyleGAN's expressivity and disentangled latent space, existing\n2D approaches employ textual prompting to edit facial images with different\nattributes. In contrast, 3D-aware approaches that generate faces at different\ntarget poses require attribute-specific classifiers, learning separate model\nweights for each attribute, and are not scalable for novel attributes. In this\nwork, we propose an efficient, plug-and-play, 3D-aware face editing framework\nbased on attribute-specific prompt learning, enabling the generation of facial\nimages with controllable attributes across various target poses. To this end,\nwe introduce a text-driven learnable style token-based latent attribute editor\n(LAE). The LAE harnesses a pre-trained vision-language model to find\ntext-guided attribute-specific editing direction in the latent space of any\npre-trained 3D-aware GAN. It utilizes learnable style tokens and style mappers\nto learn and transform this editing direction to 3D latent space. To train LAE\nwith multiple attributes, we use directional contrastive loss and style token\nloss. Furthermore, to ensure view consistency and identity preservation across\ndifferent poses and attributes, we employ several 3D-aware identity and pose\npreservation losses. Our experiments show that our proposed framework generates\nhigh-quality images with 3D awareness and view consistency while maintaining\nattribute-specific features. We demonstrate the effectiveness of our method on\ndifferent facial attributes, including hair color and style, expression, and\nothers.\n", "link": "http://arxiv.org/abs/2406.04413v2", "date": "2024-07-24", "relevancy": 2.3048, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6213}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5683}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%203D-Aware%20Facial%20Image%20Editing%20via%20Attribute-Specific%20Prompt%0A%20%20Learning&body=Title%3A%20Efficient%203D-Aware%20Facial%20Image%20Editing%20via%20Attribute-Specific%20Prompt%0A%20%20Learning%0AAuthor%3A%20Amandeep%20Kumar%20and%20Muhammad%20Awais%20and%20Sanath%20Narayan%20and%20Hisham%20Cholakkal%20and%20Salman%20Khan%20and%20Rao%20Muhammad%20Anwer%0AAbstract%3A%20%20%20Drawing%20upon%20StyleGAN%27s%20expressivity%20and%20disentangled%20latent%20space%2C%20existing%0A2D%20approaches%20employ%20textual%20prompting%20to%20edit%20facial%20images%20with%20different%0Aattributes.%20In%20contrast%2C%203D-aware%20approaches%20that%20generate%20faces%20at%20different%0Atarget%20poses%20require%20attribute-specific%20classifiers%2C%20learning%20separate%20model%0Aweights%20for%20each%20attribute%2C%20and%20are%20not%20scalable%20for%20novel%20attributes.%20In%20this%0Awork%2C%20we%20propose%20an%20efficient%2C%20plug-and-play%2C%203D-aware%20face%20editing%20framework%0Abased%20on%20attribute-specific%20prompt%20learning%2C%20enabling%20the%20generation%20of%20facial%0Aimages%20with%20controllable%20attributes%20across%20various%20target%20poses.%20To%20this%20end%2C%0Awe%20introduce%20a%20text-driven%20learnable%20style%20token-based%20latent%20attribute%20editor%0A%28LAE%29.%20The%20LAE%20harnesses%20a%20pre-trained%20vision-language%20model%20to%20find%0Atext-guided%20attribute-specific%20editing%20direction%20in%20the%20latent%20space%20of%20any%0Apre-trained%203D-aware%20GAN.%20It%20utilizes%20learnable%20style%20tokens%20and%20style%20mappers%0Ato%20learn%20and%20transform%20this%20editing%20direction%20to%203D%20latent%20space.%20To%20train%20LAE%0Awith%20multiple%20attributes%2C%20we%20use%20directional%20contrastive%20loss%20and%20style%20token%0Aloss.%20Furthermore%2C%20to%20ensure%20view%20consistency%20and%20identity%20preservation%20across%0Adifferent%20poses%20and%20attributes%2C%20we%20employ%20several%203D-aware%20identity%20and%20pose%0Apreservation%20losses.%20Our%20experiments%20show%20that%20our%20proposed%20framework%20generates%0Ahigh-quality%20images%20with%203D%20awareness%20and%20view%20consistency%20while%20maintaining%0Aattribute-specific%20features.%20We%20demonstrate%20the%20effectiveness%20of%20our%20method%20on%0Adifferent%20facial%20attributes%2C%20including%20hair%20color%20and%20style%2C%20expression%2C%20and%0Aothers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04413v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%25203D-Aware%2520Facial%2520Image%2520Editing%2520via%2520Attribute-Specific%2520Prompt%250A%2520%2520Learning%26entry.906535625%3DAmandeep%2520Kumar%2520and%2520Muhammad%2520Awais%2520and%2520Sanath%2520Narayan%2520and%2520Hisham%2520Cholakkal%2520and%2520Salman%2520Khan%2520and%2520Rao%2520Muhammad%2520Anwer%26entry.1292438233%3D%2520%2520Drawing%2520upon%2520StyleGAN%2527s%2520expressivity%2520and%2520disentangled%2520latent%2520space%252C%2520existing%250A2D%2520approaches%2520employ%2520textual%2520prompting%2520to%2520edit%2520facial%2520images%2520with%2520different%250Aattributes.%2520In%2520contrast%252C%25203D-aware%2520approaches%2520that%2520generate%2520faces%2520at%2520different%250Atarget%2520poses%2520require%2520attribute-specific%2520classifiers%252C%2520learning%2520separate%2520model%250Aweights%2520for%2520each%2520attribute%252C%2520and%2520are%2520not%2520scalable%2520for%2520novel%2520attributes.%2520In%2520this%250Awork%252C%2520we%2520propose%2520an%2520efficient%252C%2520plug-and-play%252C%25203D-aware%2520face%2520editing%2520framework%250Abased%2520on%2520attribute-specific%2520prompt%2520learning%252C%2520enabling%2520the%2520generation%2520of%2520facial%250Aimages%2520with%2520controllable%2520attributes%2520across%2520various%2520target%2520poses.%2520To%2520this%2520end%252C%250Awe%2520introduce%2520a%2520text-driven%2520learnable%2520style%2520token-based%2520latent%2520attribute%2520editor%250A%2528LAE%2529.%2520The%2520LAE%2520harnesses%2520a%2520pre-trained%2520vision-language%2520model%2520to%2520find%250Atext-guided%2520attribute-specific%2520editing%2520direction%2520in%2520the%2520latent%2520space%2520of%2520any%250Apre-trained%25203D-aware%2520GAN.%2520It%2520utilizes%2520learnable%2520style%2520tokens%2520and%2520style%2520mappers%250Ato%2520learn%2520and%2520transform%2520this%2520editing%2520direction%2520to%25203D%2520latent%2520space.%2520To%2520train%2520LAE%250Awith%2520multiple%2520attributes%252C%2520we%2520use%2520directional%2520contrastive%2520loss%2520and%2520style%2520token%250Aloss.%2520Furthermore%252C%2520to%2520ensure%2520view%2520consistency%2520and%2520identity%2520preservation%2520across%250Adifferent%2520poses%2520and%2520attributes%252C%2520we%2520employ%2520several%25203D-aware%2520identity%2520and%2520pose%250Apreservation%2520losses.%2520Our%2520experiments%2520show%2520that%2520our%2520proposed%2520framework%2520generates%250Ahigh-quality%2520images%2520with%25203D%2520awareness%2520and%2520view%2520consistency%2520while%2520maintaining%250Aattribute-specific%2520features.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520on%250Adifferent%2520facial%2520attributes%252C%2520including%2520hair%2520color%2520and%2520style%252C%2520expression%252C%2520and%250Aothers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04413v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%203D-Aware%20Facial%20Image%20Editing%20via%20Attribute-Specific%20Prompt%0A%20%20Learning&entry.906535625=Amandeep%20Kumar%20and%20Muhammad%20Awais%20and%20Sanath%20Narayan%20and%20Hisham%20Cholakkal%20and%20Salman%20Khan%20and%20Rao%20Muhammad%20Anwer&entry.1292438233=%20%20Drawing%20upon%20StyleGAN%27s%20expressivity%20and%20disentangled%20latent%20space%2C%20existing%0A2D%20approaches%20employ%20textual%20prompting%20to%20edit%20facial%20images%20with%20different%0Aattributes.%20In%20contrast%2C%203D-aware%20approaches%20that%20generate%20faces%20at%20different%0Atarget%20poses%20require%20attribute-specific%20classifiers%2C%20learning%20separate%20model%0Aweights%20for%20each%20attribute%2C%20and%20are%20not%20scalable%20for%20novel%20attributes.%20In%20this%0Awork%2C%20we%20propose%20an%20efficient%2C%20plug-and-play%2C%203D-aware%20face%20editing%20framework%0Abased%20on%20attribute-specific%20prompt%20learning%2C%20enabling%20the%20generation%20of%20facial%0Aimages%20with%20controllable%20attributes%20across%20various%20target%20poses.%20To%20this%20end%2C%0Awe%20introduce%20a%20text-driven%20learnable%20style%20token-based%20latent%20attribute%20editor%0A%28LAE%29.%20The%20LAE%20harnesses%20a%20pre-trained%20vision-language%20model%20to%20find%0Atext-guided%20attribute-specific%20editing%20direction%20in%20the%20latent%20space%20of%20any%0Apre-trained%203D-aware%20GAN.%20It%20utilizes%20learnable%20style%20tokens%20and%20style%20mappers%0Ato%20learn%20and%20transform%20this%20editing%20direction%20to%203D%20latent%20space.%20To%20train%20LAE%0Awith%20multiple%20attributes%2C%20we%20use%20directional%20contrastive%20loss%20and%20style%20token%0Aloss.%20Furthermore%2C%20to%20ensure%20view%20consistency%20and%20identity%20preservation%20across%0Adifferent%20poses%20and%20attributes%2C%20we%20employ%20several%203D-aware%20identity%20and%20pose%0Apreservation%20losses.%20Our%20experiments%20show%20that%20our%20proposed%20framework%20generates%0Ahigh-quality%20images%20with%203D%20awareness%20and%20view%20consistency%20while%20maintaining%0Aattribute-specific%20features.%20We%20demonstrate%20the%20effectiveness%20of%20our%20method%20on%0Adifferent%20facial%20attributes%2C%20including%20hair%20color%20and%20style%2C%20expression%2C%20and%0Aothers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04413v2&entry.124074799=Read"},
{"title": "LLMmap: Fingerprinting For Large Language Models", "author": "Dario Pasquini and Evgenios M. Kornaropoulos and Giuseppe Ateniese", "abstract": "  We introduce LLMmap, a first-generation fingerprinting attack targeted at\nLLM-integrated applications. LLMmap employs an active fingerprinting approach,\nsending carefully crafted queries to the application and analyzing the\nresponses to identify the specific LLM model in use. With as few as 8\ninteractions, LLMmap can accurately identify LLMs with over 95% accuracy. More\nimportantly, LLMmap is designed to be robust across different application\nlayers, allowing it to identify LLMs operating under various system prompts,\nstochastic sampling hyperparameters, and even complex generation frameworks\nsuch as RAG or Chain-of-Thought.\n", "link": "http://arxiv.org/abs/2407.15847v2", "date": "2024-07-24", "relevancy": 2.2893, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4742}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4522}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMmap%3A%20Fingerprinting%20For%20Large%20Language%20Models&body=Title%3A%20LLMmap%3A%20Fingerprinting%20For%20Large%20Language%20Models%0AAuthor%3A%20Dario%20Pasquini%20and%20Evgenios%20M.%20Kornaropoulos%20and%20Giuseppe%20Ateniese%0AAbstract%3A%20%20%20We%20introduce%20LLMmap%2C%20a%20first-generation%20fingerprinting%20attack%20targeted%20at%0ALLM-integrated%20applications.%20LLMmap%20employs%20an%20active%20fingerprinting%20approach%2C%0Asending%20carefully%20crafted%20queries%20to%20the%20application%20and%20analyzing%20the%0Aresponses%20to%20identify%20the%20specific%20LLM%20model%20in%20use.%20With%20as%20few%20as%208%0Ainteractions%2C%20LLMmap%20can%20accurately%20identify%20LLMs%20with%20over%2095%25%20accuracy.%20More%0Aimportantly%2C%20LLMmap%20is%20designed%20to%20be%20robust%20across%20different%20application%0Alayers%2C%20allowing%20it%20to%20identify%20LLMs%20operating%20under%20various%20system%20prompts%2C%0Astochastic%20sampling%20hyperparameters%2C%20and%20even%20complex%20generation%20frameworks%0Asuch%20as%20RAG%20or%20Chain-of-Thought.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15847v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMmap%253A%2520Fingerprinting%2520For%2520Large%2520Language%2520Models%26entry.906535625%3DDario%2520Pasquini%2520and%2520Evgenios%2520M.%2520Kornaropoulos%2520and%2520Giuseppe%2520Ateniese%26entry.1292438233%3D%2520%2520We%2520introduce%2520LLMmap%252C%2520a%2520first-generation%2520fingerprinting%2520attack%2520targeted%2520at%250ALLM-integrated%2520applications.%2520LLMmap%2520employs%2520an%2520active%2520fingerprinting%2520approach%252C%250Asending%2520carefully%2520crafted%2520queries%2520to%2520the%2520application%2520and%2520analyzing%2520the%250Aresponses%2520to%2520identify%2520the%2520specific%2520LLM%2520model%2520in%2520use.%2520With%2520as%2520few%2520as%25208%250Ainteractions%252C%2520LLMmap%2520can%2520accurately%2520identify%2520LLMs%2520with%2520over%252095%2525%2520accuracy.%2520More%250Aimportantly%252C%2520LLMmap%2520is%2520designed%2520to%2520be%2520robust%2520across%2520different%2520application%250Alayers%252C%2520allowing%2520it%2520to%2520identify%2520LLMs%2520operating%2520under%2520various%2520system%2520prompts%252C%250Astochastic%2520sampling%2520hyperparameters%252C%2520and%2520even%2520complex%2520generation%2520frameworks%250Asuch%2520as%2520RAG%2520or%2520Chain-of-Thought.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15847v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMmap%3A%20Fingerprinting%20For%20Large%20Language%20Models&entry.906535625=Dario%20Pasquini%20and%20Evgenios%20M.%20Kornaropoulos%20and%20Giuseppe%20Ateniese&entry.1292438233=%20%20We%20introduce%20LLMmap%2C%20a%20first-generation%20fingerprinting%20attack%20targeted%20at%0ALLM-integrated%20applications.%20LLMmap%20employs%20an%20active%20fingerprinting%20approach%2C%0Asending%20carefully%20crafted%20queries%20to%20the%20application%20and%20analyzing%20the%0Aresponses%20to%20identify%20the%20specific%20LLM%20model%20in%20use.%20With%20as%20few%20as%208%0Ainteractions%2C%20LLMmap%20can%20accurately%20identify%20LLMs%20with%20over%2095%25%20accuracy.%20More%0Aimportantly%2C%20LLMmap%20is%20designed%20to%20be%20robust%20across%20different%20application%0Alayers%2C%20allowing%20it%20to%20identify%20LLMs%20operating%20under%20various%20system%20prompts%2C%0Astochastic%20sampling%20hyperparameters%2C%20and%20even%20complex%20generation%20frameworks%0Asuch%20as%20RAG%20or%20Chain-of-Thought.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15847v2&entry.124074799=Read"},
{"title": "Learning from Graphs with Heterophily: Progress and Future", "author": "Chenghua Gong and Yao Cheng and Xiang Li and Caihua Shan and Siqiang Luo", "abstract": "  Graphs are structured data that models complex relations between real-world\nentities. Heterophilous graphs, where linked nodes are prone to be with\ndifferent labels or dissimilar features, have recently attracted significant\nattention and found many applications. Meanwhile, increasing efforts have been\nmade to advance learning from heterophilous graphs. Although there exist\nsurveys on the relevant topic, they focus on heterophilous GNNs, which are only\nsub-topics of heterophilous graph learning. In this survey, we comprehensively\noverview existing works on learning from graphs with heterophily.First, we\ncollect over 180 publications and introduce the development of this field.\nThen, we systematically categorize existing methods based on a hierarchical\ntaxonomy including learning strategies, model architectures and practical\napplications. Finally, we discuss the primary challenges of existing studies\nand highlight promising avenues for future research.More publication details\nand corresponding open-source codes can be accessed and will be continuously\nupdated at our\nrepositories:https://github.com/gongchenghua/Papers-Graphs-with-Heterophily.\n", "link": "http://arxiv.org/abs/2401.09769v3", "date": "2024-07-24", "relevancy": 2.2886, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4975}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4471}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Graphs%20with%20Heterophily%3A%20Progress%20and%20Future&body=Title%3A%20Learning%20from%20Graphs%20with%20Heterophily%3A%20Progress%20and%20Future%0AAuthor%3A%20Chenghua%20Gong%20and%20Yao%20Cheng%20and%20Xiang%20Li%20and%20Caihua%20Shan%20and%20Siqiang%20Luo%0AAbstract%3A%20%20%20Graphs%20are%20structured%20data%20that%20models%20complex%20relations%20between%20real-world%0Aentities.%20Heterophilous%20graphs%2C%20where%20linked%20nodes%20are%20prone%20to%20be%20with%0Adifferent%20labels%20or%20dissimilar%20features%2C%20have%20recently%20attracted%20significant%0Aattention%20and%20found%20many%20applications.%20Meanwhile%2C%20increasing%20efforts%20have%20been%0Amade%20to%20advance%20learning%20from%20heterophilous%20graphs.%20Although%20there%20exist%0Asurveys%20on%20the%20relevant%20topic%2C%20they%20focus%20on%20heterophilous%20GNNs%2C%20which%20are%20only%0Asub-topics%20of%20heterophilous%20graph%20learning.%20In%20this%20survey%2C%20we%20comprehensively%0Aoverview%20existing%20works%20on%20learning%20from%20graphs%20with%20heterophily.First%2C%20we%0Acollect%20over%20180%20publications%20and%20introduce%20the%20development%20of%20this%20field.%0AThen%2C%20we%20systematically%20categorize%20existing%20methods%20based%20on%20a%20hierarchical%0Ataxonomy%20including%20learning%20strategies%2C%20model%20architectures%20and%20practical%0Aapplications.%20Finally%2C%20we%20discuss%20the%20primary%20challenges%20of%20existing%20studies%0Aand%20highlight%20promising%20avenues%20for%20future%20research.More%20publication%20details%0Aand%20corresponding%20open-source%20codes%20can%20be%20accessed%20and%20will%20be%20continuously%0Aupdated%20at%20our%0Arepositories%3Ahttps%3A//github.com/gongchenghua/Papers-Graphs-with-Heterophily.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.09769v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Graphs%2520with%2520Heterophily%253A%2520Progress%2520and%2520Future%26entry.906535625%3DChenghua%2520Gong%2520and%2520Yao%2520Cheng%2520and%2520Xiang%2520Li%2520and%2520Caihua%2520Shan%2520and%2520Siqiang%2520Luo%26entry.1292438233%3D%2520%2520Graphs%2520are%2520structured%2520data%2520that%2520models%2520complex%2520relations%2520between%2520real-world%250Aentities.%2520Heterophilous%2520graphs%252C%2520where%2520linked%2520nodes%2520are%2520prone%2520to%2520be%2520with%250Adifferent%2520labels%2520or%2520dissimilar%2520features%252C%2520have%2520recently%2520attracted%2520significant%250Aattention%2520and%2520found%2520many%2520applications.%2520Meanwhile%252C%2520increasing%2520efforts%2520have%2520been%250Amade%2520to%2520advance%2520learning%2520from%2520heterophilous%2520graphs.%2520Although%2520there%2520exist%250Asurveys%2520on%2520the%2520relevant%2520topic%252C%2520they%2520focus%2520on%2520heterophilous%2520GNNs%252C%2520which%2520are%2520only%250Asub-topics%2520of%2520heterophilous%2520graph%2520learning.%2520In%2520this%2520survey%252C%2520we%2520comprehensively%250Aoverview%2520existing%2520works%2520on%2520learning%2520from%2520graphs%2520with%2520heterophily.First%252C%2520we%250Acollect%2520over%2520180%2520publications%2520and%2520introduce%2520the%2520development%2520of%2520this%2520field.%250AThen%252C%2520we%2520systematically%2520categorize%2520existing%2520methods%2520based%2520on%2520a%2520hierarchical%250Ataxonomy%2520including%2520learning%2520strategies%252C%2520model%2520architectures%2520and%2520practical%250Aapplications.%2520Finally%252C%2520we%2520discuss%2520the%2520primary%2520challenges%2520of%2520existing%2520studies%250Aand%2520highlight%2520promising%2520avenues%2520for%2520future%2520research.More%2520publication%2520details%250Aand%2520corresponding%2520open-source%2520codes%2520can%2520be%2520accessed%2520and%2520will%2520be%2520continuously%250Aupdated%2520at%2520our%250Arepositories%253Ahttps%253A//github.com/gongchenghua/Papers-Graphs-with-Heterophily.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.09769v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Graphs%20with%20Heterophily%3A%20Progress%20and%20Future&entry.906535625=Chenghua%20Gong%20and%20Yao%20Cheng%20and%20Xiang%20Li%20and%20Caihua%20Shan%20and%20Siqiang%20Luo&entry.1292438233=%20%20Graphs%20are%20structured%20data%20that%20models%20complex%20relations%20between%20real-world%0Aentities.%20Heterophilous%20graphs%2C%20where%20linked%20nodes%20are%20prone%20to%20be%20with%0Adifferent%20labels%20or%20dissimilar%20features%2C%20have%20recently%20attracted%20significant%0Aattention%20and%20found%20many%20applications.%20Meanwhile%2C%20increasing%20efforts%20have%20been%0Amade%20to%20advance%20learning%20from%20heterophilous%20graphs.%20Although%20there%20exist%0Asurveys%20on%20the%20relevant%20topic%2C%20they%20focus%20on%20heterophilous%20GNNs%2C%20which%20are%20only%0Asub-topics%20of%20heterophilous%20graph%20learning.%20In%20this%20survey%2C%20we%20comprehensively%0Aoverview%20existing%20works%20on%20learning%20from%20graphs%20with%20heterophily.First%2C%20we%0Acollect%20over%20180%20publications%20and%20introduce%20the%20development%20of%20this%20field.%0AThen%2C%20we%20systematically%20categorize%20existing%20methods%20based%20on%20a%20hierarchical%0Ataxonomy%20including%20learning%20strategies%2C%20model%20architectures%20and%20practical%0Aapplications.%20Finally%2C%20we%20discuss%20the%20primary%20challenges%20of%20existing%20studies%0Aand%20highlight%20promising%20avenues%20for%20future%20research.More%20publication%20details%0Aand%20corresponding%20open-source%20codes%20can%20be%20accessed%20and%20will%20be%20continuously%0Aupdated%20at%20our%0Arepositories%3Ahttps%3A//github.com/gongchenghua/Papers-Graphs-with-Heterophily.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.09769v3&entry.124074799=Read"},
{"title": "Towards Practical Finite Sample Bounds for Motion Planning in TAMP", "author": "Seiji Shaw and Aidan Curtis and Leslie Pack Kaelbling and Tom\u00e1s Lozano-P\u00e9rez and Nicholas Roy", "abstract": "  When using sampling-based motion planners, such as PRMs, in configuration\nspaces, it is difficult to determine how many samples are required for the PRM\nto find a solution consistently. This is relevant in Task and Motion Planning\n(TAMP), where many motion planning problems must be solved in sequence. We\nattempt to solve this problem by proving an upper bound on the number of\nsamples that are sufficient, with high probability, to find a solution by\ndrawing on prior work in deterministic sampling and sample complexity theory.\nWe also introduce a numerical algorithm to compute a tighter number of samples\nbased on the proof of the sample complexity theorem we apply to derive our\nbound. Our experiments show that our numerical bounding algorithm is tight\nwithin two orders of magnitude on planar planning problems and becomes looser\nas the problem's dimensionality increases. When deployed as a heuristic to\nschedule samples in a TAMP planner, we also observe planning time improvements\nin planar problems. While our experiments show much work remains to tighten our\nbounds, the ideas presented in this paper are a step towards a practical sample\nbound.\n", "link": "http://arxiv.org/abs/2407.17394v1", "date": "2024-07-24", "relevancy": 2.2837, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4735}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4506}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Practical%20Finite%20Sample%20Bounds%20for%20Motion%20Planning%20in%20TAMP&body=Title%3A%20Towards%20Practical%20Finite%20Sample%20Bounds%20for%20Motion%20Planning%20in%20TAMP%0AAuthor%3A%20Seiji%20Shaw%20and%20Aidan%20Curtis%20and%20Leslie%20Pack%20Kaelbling%20and%20Tom%C3%A1s%20Lozano-P%C3%A9rez%20and%20Nicholas%20Roy%0AAbstract%3A%20%20%20When%20using%20sampling-based%20motion%20planners%2C%20such%20as%20PRMs%2C%20in%20configuration%0Aspaces%2C%20it%20is%20difficult%20to%20determine%20how%20many%20samples%20are%20required%20for%20the%20PRM%0Ato%20find%20a%20solution%20consistently.%20This%20is%20relevant%20in%20Task%20and%20Motion%20Planning%0A%28TAMP%29%2C%20where%20many%20motion%20planning%20problems%20must%20be%20solved%20in%20sequence.%20We%0Aattempt%20to%20solve%20this%20problem%20by%20proving%20an%20upper%20bound%20on%20the%20number%20of%0Asamples%20that%20are%20sufficient%2C%20with%20high%20probability%2C%20to%20find%20a%20solution%20by%0Adrawing%20on%20prior%20work%20in%20deterministic%20sampling%20and%20sample%20complexity%20theory.%0AWe%20also%20introduce%20a%20numerical%20algorithm%20to%20compute%20a%20tighter%20number%20of%20samples%0Abased%20on%20the%20proof%20of%20the%20sample%20complexity%20theorem%20we%20apply%20to%20derive%20our%0Abound.%20Our%20experiments%20show%20that%20our%20numerical%20bounding%20algorithm%20is%20tight%0Awithin%20two%20orders%20of%20magnitude%20on%20planar%20planning%20problems%20and%20becomes%20looser%0Aas%20the%20problem%27s%20dimensionality%20increases.%20When%20deployed%20as%20a%20heuristic%20to%0Aschedule%20samples%20in%20a%20TAMP%20planner%2C%20we%20also%20observe%20planning%20time%20improvements%0Ain%20planar%20problems.%20While%20our%20experiments%20show%20much%20work%20remains%20to%20tighten%20our%0Abounds%2C%20the%20ideas%20presented%20in%20this%20paper%20are%20a%20step%20towards%20a%20practical%20sample%0Abound.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Practical%2520Finite%2520Sample%2520Bounds%2520for%2520Motion%2520Planning%2520in%2520TAMP%26entry.906535625%3DSeiji%2520Shaw%2520and%2520Aidan%2520Curtis%2520and%2520Leslie%2520Pack%2520Kaelbling%2520and%2520Tom%25C3%25A1s%2520Lozano-P%25C3%25A9rez%2520and%2520Nicholas%2520Roy%26entry.1292438233%3D%2520%2520When%2520using%2520sampling-based%2520motion%2520planners%252C%2520such%2520as%2520PRMs%252C%2520in%2520configuration%250Aspaces%252C%2520it%2520is%2520difficult%2520to%2520determine%2520how%2520many%2520samples%2520are%2520required%2520for%2520the%2520PRM%250Ato%2520find%2520a%2520solution%2520consistently.%2520This%2520is%2520relevant%2520in%2520Task%2520and%2520Motion%2520Planning%250A%2528TAMP%2529%252C%2520where%2520many%2520motion%2520planning%2520problems%2520must%2520be%2520solved%2520in%2520sequence.%2520We%250Aattempt%2520to%2520solve%2520this%2520problem%2520by%2520proving%2520an%2520upper%2520bound%2520on%2520the%2520number%2520of%250Asamples%2520that%2520are%2520sufficient%252C%2520with%2520high%2520probability%252C%2520to%2520find%2520a%2520solution%2520by%250Adrawing%2520on%2520prior%2520work%2520in%2520deterministic%2520sampling%2520and%2520sample%2520complexity%2520theory.%250AWe%2520also%2520introduce%2520a%2520numerical%2520algorithm%2520to%2520compute%2520a%2520tighter%2520number%2520of%2520samples%250Abased%2520on%2520the%2520proof%2520of%2520the%2520sample%2520complexity%2520theorem%2520we%2520apply%2520to%2520derive%2520our%250Abound.%2520Our%2520experiments%2520show%2520that%2520our%2520numerical%2520bounding%2520algorithm%2520is%2520tight%250Awithin%2520two%2520orders%2520of%2520magnitude%2520on%2520planar%2520planning%2520problems%2520and%2520becomes%2520looser%250Aas%2520the%2520problem%2527s%2520dimensionality%2520increases.%2520When%2520deployed%2520as%2520a%2520heuristic%2520to%250Aschedule%2520samples%2520in%2520a%2520TAMP%2520planner%252C%2520we%2520also%2520observe%2520planning%2520time%2520improvements%250Ain%2520planar%2520problems.%2520While%2520our%2520experiments%2520show%2520much%2520work%2520remains%2520to%2520tighten%2520our%250Abounds%252C%2520the%2520ideas%2520presented%2520in%2520this%2520paper%2520are%2520a%2520step%2520towards%2520a%2520practical%2520sample%250Abound.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Practical%20Finite%20Sample%20Bounds%20for%20Motion%20Planning%20in%20TAMP&entry.906535625=Seiji%20Shaw%20and%20Aidan%20Curtis%20and%20Leslie%20Pack%20Kaelbling%20and%20Tom%C3%A1s%20Lozano-P%C3%A9rez%20and%20Nicholas%20Roy&entry.1292438233=%20%20When%20using%20sampling-based%20motion%20planners%2C%20such%20as%20PRMs%2C%20in%20configuration%0Aspaces%2C%20it%20is%20difficult%20to%20determine%20how%20many%20samples%20are%20required%20for%20the%20PRM%0Ato%20find%20a%20solution%20consistently.%20This%20is%20relevant%20in%20Task%20and%20Motion%20Planning%0A%28TAMP%29%2C%20where%20many%20motion%20planning%20problems%20must%20be%20solved%20in%20sequence.%20We%0Aattempt%20to%20solve%20this%20problem%20by%20proving%20an%20upper%20bound%20on%20the%20number%20of%0Asamples%20that%20are%20sufficient%2C%20with%20high%20probability%2C%20to%20find%20a%20solution%20by%0Adrawing%20on%20prior%20work%20in%20deterministic%20sampling%20and%20sample%20complexity%20theory.%0AWe%20also%20introduce%20a%20numerical%20algorithm%20to%20compute%20a%20tighter%20number%20of%20samples%0Abased%20on%20the%20proof%20of%20the%20sample%20complexity%20theorem%20we%20apply%20to%20derive%20our%0Abound.%20Our%20experiments%20show%20that%20our%20numerical%20bounding%20algorithm%20is%20tight%0Awithin%20two%20orders%20of%20magnitude%20on%20planar%20planning%20problems%20and%20becomes%20looser%0Aas%20the%20problem%27s%20dimensionality%20increases.%20When%20deployed%20as%20a%20heuristic%20to%0Aschedule%20samples%20in%20a%20TAMP%20planner%2C%20we%20also%20observe%20planning%20time%20improvements%0Ain%20planar%20problems.%20While%20our%20experiments%20show%20much%20work%20remains%20to%20tighten%20our%0Abounds%2C%20the%20ideas%20presented%20in%20this%20paper%20are%20a%20step%20towards%20a%20practical%20sample%0Abound.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17394v1&entry.124074799=Read"},
{"title": "Cascaded Light Propagation Volumes using Spherical Radial Basis\n  Functions", "author": "Ludovic Silvestre and Jo\u00e3o Pereira", "abstract": "  This paper introduces a contribution made to one of the newest methods for\nsimulating indirect lighting in dynamic scenes , the cascaded light propagation\nvolumes . Our contribution consists on using Spherical Radial Basis Functions\ninstead of Spherical Harmonic, since the first achieves much better results\nwhen many coefficients are used. We explain how to integrate the Spherical\nRadial Basis Functions with the cascaded light propagation volumes, and\nevaluate our technique against the same implementation, but with Spherical\nharmonics.\n", "link": "http://arxiv.org/abs/2407.17336v1", "date": "2024-07-24", "relevancy": 2.2705, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4582}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4582}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cascaded%20Light%20Propagation%20Volumes%20using%20Spherical%20Radial%20Basis%0A%20%20Functions&body=Title%3A%20Cascaded%20Light%20Propagation%20Volumes%20using%20Spherical%20Radial%20Basis%0A%20%20Functions%0AAuthor%3A%20Ludovic%20Silvestre%20and%20Jo%C3%A3o%20Pereira%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20contribution%20made%20to%20one%20of%20the%20newest%20methods%20for%0Asimulating%20indirect%20lighting%20in%20dynamic%20scenes%20%2C%20the%20cascaded%20light%20propagation%0Avolumes%20.%20Our%20contribution%20consists%20on%20using%20Spherical%20Radial%20Basis%20Functions%0Ainstead%20of%20Spherical%20Harmonic%2C%20since%20the%20first%20achieves%20much%20better%20results%0Awhen%20many%20coefficients%20are%20used.%20We%20explain%20how%20to%20integrate%20the%20Spherical%0ARadial%20Basis%20Functions%20with%20the%20cascaded%20light%20propagation%20volumes%2C%20and%0Aevaluate%20our%20technique%20against%20the%20same%20implementation%2C%20but%20with%20Spherical%0Aharmonics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17336v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCascaded%2520Light%2520Propagation%2520Volumes%2520using%2520Spherical%2520Radial%2520Basis%250A%2520%2520Functions%26entry.906535625%3DLudovic%2520Silvestre%2520and%2520Jo%25C3%25A3o%2520Pereira%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520contribution%2520made%2520to%2520one%2520of%2520the%2520newest%2520methods%2520for%250Asimulating%2520indirect%2520lighting%2520in%2520dynamic%2520scenes%2520%252C%2520the%2520cascaded%2520light%2520propagation%250Avolumes%2520.%2520Our%2520contribution%2520consists%2520on%2520using%2520Spherical%2520Radial%2520Basis%2520Functions%250Ainstead%2520of%2520Spherical%2520Harmonic%252C%2520since%2520the%2520first%2520achieves%2520much%2520better%2520results%250Awhen%2520many%2520coefficients%2520are%2520used.%2520We%2520explain%2520how%2520to%2520integrate%2520the%2520Spherical%250ARadial%2520Basis%2520Functions%2520with%2520the%2520cascaded%2520light%2520propagation%2520volumes%252C%2520and%250Aevaluate%2520our%2520technique%2520against%2520the%2520same%2520implementation%252C%2520but%2520with%2520Spherical%250Aharmonics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17336v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cascaded%20Light%20Propagation%20Volumes%20using%20Spherical%20Radial%20Basis%0A%20%20Functions&entry.906535625=Ludovic%20Silvestre%20and%20Jo%C3%A3o%20Pereira&entry.1292438233=%20%20This%20paper%20introduces%20a%20contribution%20made%20to%20one%20of%20the%20newest%20methods%20for%0Asimulating%20indirect%20lighting%20in%20dynamic%20scenes%20%2C%20the%20cascaded%20light%20propagation%0Avolumes%20.%20Our%20contribution%20consists%20on%20using%20Spherical%20Radial%20Basis%20Functions%0Ainstead%20of%20Spherical%20Harmonic%2C%20since%20the%20first%20achieves%20much%20better%20results%0Awhen%20many%20coefficients%20are%20used.%20We%20explain%20how%20to%20integrate%20the%20Spherical%0ARadial%20Basis%20Functions%20with%20the%20cascaded%20light%20propagation%20volumes%2C%20and%0Aevaluate%20our%20technique%20against%20the%20same%20implementation%2C%20but%20with%20Spherical%0Aharmonics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17336v1&entry.124074799=Read"},
{"title": "EXACT: How to Train Your Accuracy", "author": "Ivan Karpukhin and Stanislav Dereka and Sergey Kolesnikov", "abstract": "  Classification tasks are usually evaluated in terms of accuracy. However,\naccuracy is discontinuous and cannot be directly optimized using gradient\nascent. Popular methods minimize cross-entropy, hinge loss, or other surrogate\nlosses, which can lead to suboptimal results. In this paper, we propose a new\noptimization framework by introducing stochasticity to a model's output and\noptimizing expected accuracy, i.e. accuracy of the stochastic model. Extensive\nexperiments on linear models and deep image classification show that the\nproposed optimization method is a powerful alternative to widely used\nclassification losses.\n", "link": "http://arxiv.org/abs/2205.09615v5", "date": "2024-07-24", "relevancy": 2.2386, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4619}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4511}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4301}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EXACT%3A%20How%20to%20Train%20Your%20Accuracy&body=Title%3A%20EXACT%3A%20How%20to%20Train%20Your%20Accuracy%0AAuthor%3A%20Ivan%20Karpukhin%20and%20Stanislav%20Dereka%20and%20Sergey%20Kolesnikov%0AAbstract%3A%20%20%20Classification%20tasks%20are%20usually%20evaluated%20in%20terms%20of%20accuracy.%20However%2C%0Aaccuracy%20is%20discontinuous%20and%20cannot%20be%20directly%20optimized%20using%20gradient%0Aascent.%20Popular%20methods%20minimize%20cross-entropy%2C%20hinge%20loss%2C%20or%20other%20surrogate%0Alosses%2C%20which%20can%20lead%20to%20suboptimal%20results.%20In%20this%20paper%2C%20we%20propose%20a%20new%0Aoptimization%20framework%20by%20introducing%20stochasticity%20to%20a%20model%27s%20output%20and%0Aoptimizing%20expected%20accuracy%2C%20i.e.%20accuracy%20of%20the%20stochastic%20model.%20Extensive%0Aexperiments%20on%20linear%20models%20and%20deep%20image%20classification%20show%20that%20the%0Aproposed%20optimization%20method%20is%20a%20powerful%20alternative%20to%20widely%20used%0Aclassification%20losses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2205.09615v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEXACT%253A%2520How%2520to%2520Train%2520Your%2520Accuracy%26entry.906535625%3DIvan%2520Karpukhin%2520and%2520Stanislav%2520Dereka%2520and%2520Sergey%2520Kolesnikov%26entry.1292438233%3D%2520%2520Classification%2520tasks%2520are%2520usually%2520evaluated%2520in%2520terms%2520of%2520accuracy.%2520However%252C%250Aaccuracy%2520is%2520discontinuous%2520and%2520cannot%2520be%2520directly%2520optimized%2520using%2520gradient%250Aascent.%2520Popular%2520methods%2520minimize%2520cross-entropy%252C%2520hinge%2520loss%252C%2520or%2520other%2520surrogate%250Alosses%252C%2520which%2520can%2520lead%2520to%2520suboptimal%2520results.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%250Aoptimization%2520framework%2520by%2520introducing%2520stochasticity%2520to%2520a%2520model%2527s%2520output%2520and%250Aoptimizing%2520expected%2520accuracy%252C%2520i.e.%2520accuracy%2520of%2520the%2520stochastic%2520model.%2520Extensive%250Aexperiments%2520on%2520linear%2520models%2520and%2520deep%2520image%2520classification%2520show%2520that%2520the%250Aproposed%2520optimization%2520method%2520is%2520a%2520powerful%2520alternative%2520to%2520widely%2520used%250Aclassification%2520losses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2205.09615v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EXACT%3A%20How%20to%20Train%20Your%20Accuracy&entry.906535625=Ivan%20Karpukhin%20and%20Stanislav%20Dereka%20and%20Sergey%20Kolesnikov&entry.1292438233=%20%20Classification%20tasks%20are%20usually%20evaluated%20in%20terms%20of%20accuracy.%20However%2C%0Aaccuracy%20is%20discontinuous%20and%20cannot%20be%20directly%20optimized%20using%20gradient%0Aascent.%20Popular%20methods%20minimize%20cross-entropy%2C%20hinge%20loss%2C%20or%20other%20surrogate%0Alosses%2C%20which%20can%20lead%20to%20suboptimal%20results.%20In%20this%20paper%2C%20we%20propose%20a%20new%0Aoptimization%20framework%20by%20introducing%20stochasticity%20to%20a%20model%27s%20output%20and%0Aoptimizing%20expected%20accuracy%2C%20i.e.%20accuracy%20of%20the%20stochastic%20model.%20Extensive%0Aexperiments%20on%20linear%20models%20and%20deep%20image%20classification%20show%20that%20the%0Aproposed%20optimization%20method%20is%20a%20powerful%20alternative%20to%20widely%20used%0Aclassification%20losses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2205.09615v5&entry.124074799=Read"},
{"title": "A Bounding Box is Worth One Token: Interleaving Layout and Text in a\n  Large Language Model for Document Understanding", "author": "Jinghui Lu and Haiyang Yu and Yanjie Wang and Yongjie Ye and Jingqun Tang and Ziwei Yang and Binghong Wu and Qi Liu and Hao Feng and Han Wang and Hao Liu and Can Huang", "abstract": "  Recently, many studies have demonstrated that exclusively incorporating\nOCR-derived text and spatial layouts with large language models (LLMs) can be\nhighly effective for document understanding tasks. However, existing methods\nthat integrate spatial layouts with text have limitations, such as producing\noverly long text sequences or failing to fully leverage the autoregressive\ntraits of LLMs. In this work, we introduce Interleaving Layout and Text in a\nLarge Language Model (LayTextLLM)} for document understanding. In particular,\nLayTextLLM projects each bounding box to a single embedding and interleaves it\nwith text, efficiently avoiding long sequence issues while leveraging\nautoregressive traits of LLMs. LayTextLLM not only streamlines the interaction\nof layout and textual data but also shows enhanced performance in Key\nInformation Extraction (KIE) and Visual Question Answering (VQA). Comprehensive\nbenchmark evaluations reveal significant improvements, with a 27.2% increase on\nKIE tasks and 12.0% on VQA tasks compared to previous state-of-the-art document\nunderstanding MLLMs, as well as a 15.1% improvement over other SOTA OCR-based\nLLMs on KIE tasks.\n", "link": "http://arxiv.org/abs/2407.01976v2", "date": "2024-07-24", "relevancy": 2.2286, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5726}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5584}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Bounding%20Box%20is%20Worth%20One%20Token%3A%20Interleaving%20Layout%20and%20Text%20in%20a%0A%20%20Large%20Language%20Model%20for%20Document%20Understanding&body=Title%3A%20A%20Bounding%20Box%20is%20Worth%20One%20Token%3A%20Interleaving%20Layout%20and%20Text%20in%20a%0A%20%20Large%20Language%20Model%20for%20Document%20Understanding%0AAuthor%3A%20Jinghui%20Lu%20and%20Haiyang%20Yu%20and%20Yanjie%20Wang%20and%20Yongjie%20Ye%20and%20Jingqun%20Tang%20and%20Ziwei%20Yang%20and%20Binghong%20Wu%20and%20Qi%20Liu%20and%20Hao%20Feng%20and%20Han%20Wang%20and%20Hao%20Liu%20and%20Can%20Huang%0AAbstract%3A%20%20%20Recently%2C%20many%20studies%20have%20demonstrated%20that%20exclusively%20incorporating%0AOCR-derived%20text%20and%20spatial%20layouts%20with%20large%20language%20models%20%28LLMs%29%20can%20be%0Ahighly%20effective%20for%20document%20understanding%20tasks.%20However%2C%20existing%20methods%0Athat%20integrate%20spatial%20layouts%20with%20text%20have%20limitations%2C%20such%20as%20producing%0Aoverly%20long%20text%20sequences%20or%20failing%20to%20fully%20leverage%20the%20autoregressive%0Atraits%20of%20LLMs.%20In%20this%20work%2C%20we%20introduce%20Interleaving%20Layout%20and%20Text%20in%20a%0ALarge%20Language%20Model%20%28LayTextLLM%29%7D%20for%20document%20understanding.%20In%20particular%2C%0ALayTextLLM%20projects%20each%20bounding%20box%20to%20a%20single%20embedding%20and%20interleaves%20it%0Awith%20text%2C%20efficiently%20avoiding%20long%20sequence%20issues%20while%20leveraging%0Aautoregressive%20traits%20of%20LLMs.%20LayTextLLM%20not%20only%20streamlines%20the%20interaction%0Aof%20layout%20and%20textual%20data%20but%20also%20shows%20enhanced%20performance%20in%20Key%0AInformation%20Extraction%20%28KIE%29%20and%20Visual%20Question%20Answering%20%28VQA%29.%20Comprehensive%0Abenchmark%20evaluations%20reveal%20significant%20improvements%2C%20with%20a%2027.2%25%20increase%20on%0AKIE%20tasks%20and%2012.0%25%20on%20VQA%20tasks%20compared%20to%20previous%20state-of-the-art%20document%0Aunderstanding%20MLLMs%2C%20as%20well%20as%20a%2015.1%25%20improvement%20over%20other%20SOTA%20OCR-based%0ALLMs%20on%20KIE%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01976v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Bounding%2520Box%2520is%2520Worth%2520One%2520Token%253A%2520Interleaving%2520Layout%2520and%2520Text%2520in%2520a%250A%2520%2520Large%2520Language%2520Model%2520for%2520Document%2520Understanding%26entry.906535625%3DJinghui%2520Lu%2520and%2520Haiyang%2520Yu%2520and%2520Yanjie%2520Wang%2520and%2520Yongjie%2520Ye%2520and%2520Jingqun%2520Tang%2520and%2520Ziwei%2520Yang%2520and%2520Binghong%2520Wu%2520and%2520Qi%2520Liu%2520and%2520Hao%2520Feng%2520and%2520Han%2520Wang%2520and%2520Hao%2520Liu%2520and%2520Can%2520Huang%26entry.1292438233%3D%2520%2520Recently%252C%2520many%2520studies%2520have%2520demonstrated%2520that%2520exclusively%2520incorporating%250AOCR-derived%2520text%2520and%2520spatial%2520layouts%2520with%2520large%2520language%2520models%2520%2528LLMs%2529%2520can%2520be%250Ahighly%2520effective%2520for%2520document%2520understanding%2520tasks.%2520However%252C%2520existing%2520methods%250Athat%2520integrate%2520spatial%2520layouts%2520with%2520text%2520have%2520limitations%252C%2520such%2520as%2520producing%250Aoverly%2520long%2520text%2520sequences%2520or%2520failing%2520to%2520fully%2520leverage%2520the%2520autoregressive%250Atraits%2520of%2520LLMs.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Interleaving%2520Layout%2520and%2520Text%2520in%2520a%250ALarge%2520Language%2520Model%2520%2528LayTextLLM%2529%257D%2520for%2520document%2520understanding.%2520In%2520particular%252C%250ALayTextLLM%2520projects%2520each%2520bounding%2520box%2520to%2520a%2520single%2520embedding%2520and%2520interleaves%2520it%250Awith%2520text%252C%2520efficiently%2520avoiding%2520long%2520sequence%2520issues%2520while%2520leveraging%250Aautoregressive%2520traits%2520of%2520LLMs.%2520LayTextLLM%2520not%2520only%2520streamlines%2520the%2520interaction%250Aof%2520layout%2520and%2520textual%2520data%2520but%2520also%2520shows%2520enhanced%2520performance%2520in%2520Key%250AInformation%2520Extraction%2520%2528KIE%2529%2520and%2520Visual%2520Question%2520Answering%2520%2528VQA%2529.%2520Comprehensive%250Abenchmark%2520evaluations%2520reveal%2520significant%2520improvements%252C%2520with%2520a%252027.2%2525%2520increase%2520on%250AKIE%2520tasks%2520and%252012.0%2525%2520on%2520VQA%2520tasks%2520compared%2520to%2520previous%2520state-of-the-art%2520document%250Aunderstanding%2520MLLMs%252C%2520as%2520well%2520as%2520a%252015.1%2525%2520improvement%2520over%2520other%2520SOTA%2520OCR-based%250ALLMs%2520on%2520KIE%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01976v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Bounding%20Box%20is%20Worth%20One%20Token%3A%20Interleaving%20Layout%20and%20Text%20in%20a%0A%20%20Large%20Language%20Model%20for%20Document%20Understanding&entry.906535625=Jinghui%20Lu%20and%20Haiyang%20Yu%20and%20Yanjie%20Wang%20and%20Yongjie%20Ye%20and%20Jingqun%20Tang%20and%20Ziwei%20Yang%20and%20Binghong%20Wu%20and%20Qi%20Liu%20and%20Hao%20Feng%20and%20Han%20Wang%20and%20Hao%20Liu%20and%20Can%20Huang&entry.1292438233=%20%20Recently%2C%20many%20studies%20have%20demonstrated%20that%20exclusively%20incorporating%0AOCR-derived%20text%20and%20spatial%20layouts%20with%20large%20language%20models%20%28LLMs%29%20can%20be%0Ahighly%20effective%20for%20document%20understanding%20tasks.%20However%2C%20existing%20methods%0Athat%20integrate%20spatial%20layouts%20with%20text%20have%20limitations%2C%20such%20as%20producing%0Aoverly%20long%20text%20sequences%20or%20failing%20to%20fully%20leverage%20the%20autoregressive%0Atraits%20of%20LLMs.%20In%20this%20work%2C%20we%20introduce%20Interleaving%20Layout%20and%20Text%20in%20a%0ALarge%20Language%20Model%20%28LayTextLLM%29%7D%20for%20document%20understanding.%20In%20particular%2C%0ALayTextLLM%20projects%20each%20bounding%20box%20to%20a%20single%20embedding%20and%20interleaves%20it%0Awith%20text%2C%20efficiently%20avoiding%20long%20sequence%20issues%20while%20leveraging%0Aautoregressive%20traits%20of%20LLMs.%20LayTextLLM%20not%20only%20streamlines%20the%20interaction%0Aof%20layout%20and%20textual%20data%20but%20also%20shows%20enhanced%20performance%20in%20Key%0AInformation%20Extraction%20%28KIE%29%20and%20Visual%20Question%20Answering%20%28VQA%29.%20Comprehensive%0Abenchmark%20evaluations%20reveal%20significant%20improvements%2C%20with%20a%2027.2%25%20increase%20on%0AKIE%20tasks%20and%2012.0%25%20on%20VQA%20tasks%20compared%20to%20previous%20state-of-the-art%20document%0Aunderstanding%20MLLMs%2C%20as%20well%20as%20a%2015.1%25%20improvement%20over%20other%20SOTA%20OCR-based%0ALLMs%20on%20KIE%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01976v2&entry.124074799=Read"},
{"title": "PrevPredMap: Exploring Temporal Modeling with Previous Predictions for\n  Online Vectorized HD Map Construction", "author": "Nan Peng and Xun Zhou and Mingming Wang and Xiaojun Yang and Songming Chen and Guisong Chen", "abstract": "  Temporal information is crucial for detecting occluded instances. Existing\ntemporal representations have progressed from BEV or PV features to more\ncompact query features. Compared to these aforementioned features, predictions\noffer the highest level of abstraction, providing explicit information. In the\ncontext of online vectorized HD map construction, this unique characteristic of\npredictions is potentially advantageous for long-term temporal modeling and the\nintegration of map priors. This paper introduces PrevPredMap, a pioneering\ntemporal modeling framework that leverages previous predictions for\nconstructing online vectorized HD maps. We have meticulously crafted two\nessential modules for PrevPredMap: the previous-predictions-based query\ngenerator and the dynamic-position-query decoder. Specifically, the\nprevious-predictions-based query generator is designed to separately encode\ndifferent types of information from previous predictions, which are then\neffectively utilized by the dynamic-position-query decoder to generate current\npredictions. Furthermore, we have developed a dual-mode strategy to ensure\nPrevPredMap's robust performance across both single-frame and temporal modes.\nExtensive experiments demonstrate that PrevPredMap achieves state-of-the-art\nperformance on the nuScenes and Argoverse2 datasets. Code will be available at\nhttps://github.com/pnnnnnnn/PrevPredMap.\n", "link": "http://arxiv.org/abs/2407.17378v1", "date": "2024-07-24", "relevancy": 2.2279, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5631}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5538}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PrevPredMap%3A%20Exploring%20Temporal%20Modeling%20with%20Previous%20Predictions%20for%0A%20%20Online%20Vectorized%20HD%20Map%20Construction&body=Title%3A%20PrevPredMap%3A%20Exploring%20Temporal%20Modeling%20with%20Previous%20Predictions%20for%0A%20%20Online%20Vectorized%20HD%20Map%20Construction%0AAuthor%3A%20Nan%20Peng%20and%20Xun%20Zhou%20and%20Mingming%20Wang%20and%20Xiaojun%20Yang%20and%20Songming%20Chen%20and%20Guisong%20Chen%0AAbstract%3A%20%20%20Temporal%20information%20is%20crucial%20for%20detecting%20occluded%20instances.%20Existing%0Atemporal%20representations%20have%20progressed%20from%20BEV%20or%20PV%20features%20to%20more%0Acompact%20query%20features.%20Compared%20to%20these%20aforementioned%20features%2C%20predictions%0Aoffer%20the%20highest%20level%20of%20abstraction%2C%20providing%20explicit%20information.%20In%20the%0Acontext%20of%20online%20vectorized%20HD%20map%20construction%2C%20this%20unique%20characteristic%20of%0Apredictions%20is%20potentially%20advantageous%20for%20long-term%20temporal%20modeling%20and%20the%0Aintegration%20of%20map%20priors.%20This%20paper%20introduces%20PrevPredMap%2C%20a%20pioneering%0Atemporal%20modeling%20framework%20that%20leverages%20previous%20predictions%20for%0Aconstructing%20online%20vectorized%20HD%20maps.%20We%20have%20meticulously%20crafted%20two%0Aessential%20modules%20for%20PrevPredMap%3A%20the%20previous-predictions-based%20query%0Agenerator%20and%20the%20dynamic-position-query%20decoder.%20Specifically%2C%20the%0Aprevious-predictions-based%20query%20generator%20is%20designed%20to%20separately%20encode%0Adifferent%20types%20of%20information%20from%20previous%20predictions%2C%20which%20are%20then%0Aeffectively%20utilized%20by%20the%20dynamic-position-query%20decoder%20to%20generate%20current%0Apredictions.%20Furthermore%2C%20we%20have%20developed%20a%20dual-mode%20strategy%20to%20ensure%0APrevPredMap%27s%20robust%20performance%20across%20both%20single-frame%20and%20temporal%20modes.%0AExtensive%20experiments%20demonstrate%20that%20PrevPredMap%20achieves%20state-of-the-art%0Aperformance%20on%20the%20nuScenes%20and%20Argoverse2%20datasets.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/pnnnnnnn/PrevPredMap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17378v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrevPredMap%253A%2520Exploring%2520Temporal%2520Modeling%2520with%2520Previous%2520Predictions%2520for%250A%2520%2520Online%2520Vectorized%2520HD%2520Map%2520Construction%26entry.906535625%3DNan%2520Peng%2520and%2520Xun%2520Zhou%2520and%2520Mingming%2520Wang%2520and%2520Xiaojun%2520Yang%2520and%2520Songming%2520Chen%2520and%2520Guisong%2520Chen%26entry.1292438233%3D%2520%2520Temporal%2520information%2520is%2520crucial%2520for%2520detecting%2520occluded%2520instances.%2520Existing%250Atemporal%2520representations%2520have%2520progressed%2520from%2520BEV%2520or%2520PV%2520features%2520to%2520more%250Acompact%2520query%2520features.%2520Compared%2520to%2520these%2520aforementioned%2520features%252C%2520predictions%250Aoffer%2520the%2520highest%2520level%2520of%2520abstraction%252C%2520providing%2520explicit%2520information.%2520In%2520the%250Acontext%2520of%2520online%2520vectorized%2520HD%2520map%2520construction%252C%2520this%2520unique%2520characteristic%2520of%250Apredictions%2520is%2520potentially%2520advantageous%2520for%2520long-term%2520temporal%2520modeling%2520and%2520the%250Aintegration%2520of%2520map%2520priors.%2520This%2520paper%2520introduces%2520PrevPredMap%252C%2520a%2520pioneering%250Atemporal%2520modeling%2520framework%2520that%2520leverages%2520previous%2520predictions%2520for%250Aconstructing%2520online%2520vectorized%2520HD%2520maps.%2520We%2520have%2520meticulously%2520crafted%2520two%250Aessential%2520modules%2520for%2520PrevPredMap%253A%2520the%2520previous-predictions-based%2520query%250Agenerator%2520and%2520the%2520dynamic-position-query%2520decoder.%2520Specifically%252C%2520the%250Aprevious-predictions-based%2520query%2520generator%2520is%2520designed%2520to%2520separately%2520encode%250Adifferent%2520types%2520of%2520information%2520from%2520previous%2520predictions%252C%2520which%2520are%2520then%250Aeffectively%2520utilized%2520by%2520the%2520dynamic-position-query%2520decoder%2520to%2520generate%2520current%250Apredictions.%2520Furthermore%252C%2520we%2520have%2520developed%2520a%2520dual-mode%2520strategy%2520to%2520ensure%250APrevPredMap%2527s%2520robust%2520performance%2520across%2520both%2520single-frame%2520and%2520temporal%2520modes.%250AExtensive%2520experiments%2520demonstrate%2520that%2520PrevPredMap%2520achieves%2520state-of-the-art%250Aperformance%2520on%2520the%2520nuScenes%2520and%2520Argoverse2%2520datasets.%2520Code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/pnnnnnnn/PrevPredMap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17378v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PrevPredMap%3A%20Exploring%20Temporal%20Modeling%20with%20Previous%20Predictions%20for%0A%20%20Online%20Vectorized%20HD%20Map%20Construction&entry.906535625=Nan%20Peng%20and%20Xun%20Zhou%20and%20Mingming%20Wang%20and%20Xiaojun%20Yang%20and%20Songming%20Chen%20and%20Guisong%20Chen&entry.1292438233=%20%20Temporal%20information%20is%20crucial%20for%20detecting%20occluded%20instances.%20Existing%0Atemporal%20representations%20have%20progressed%20from%20BEV%20or%20PV%20features%20to%20more%0Acompact%20query%20features.%20Compared%20to%20these%20aforementioned%20features%2C%20predictions%0Aoffer%20the%20highest%20level%20of%20abstraction%2C%20providing%20explicit%20information.%20In%20the%0Acontext%20of%20online%20vectorized%20HD%20map%20construction%2C%20this%20unique%20characteristic%20of%0Apredictions%20is%20potentially%20advantageous%20for%20long-term%20temporal%20modeling%20and%20the%0Aintegration%20of%20map%20priors.%20This%20paper%20introduces%20PrevPredMap%2C%20a%20pioneering%0Atemporal%20modeling%20framework%20that%20leverages%20previous%20predictions%20for%0Aconstructing%20online%20vectorized%20HD%20maps.%20We%20have%20meticulously%20crafted%20two%0Aessential%20modules%20for%20PrevPredMap%3A%20the%20previous-predictions-based%20query%0Agenerator%20and%20the%20dynamic-position-query%20decoder.%20Specifically%2C%20the%0Aprevious-predictions-based%20query%20generator%20is%20designed%20to%20separately%20encode%0Adifferent%20types%20of%20information%20from%20previous%20predictions%2C%20which%20are%20then%0Aeffectively%20utilized%20by%20the%20dynamic-position-query%20decoder%20to%20generate%20current%0Apredictions.%20Furthermore%2C%20we%20have%20developed%20a%20dual-mode%20strategy%20to%20ensure%0APrevPredMap%27s%20robust%20performance%20across%20both%20single-frame%20and%20temporal%20modes.%0AExtensive%20experiments%20demonstrate%20that%20PrevPredMap%20achieves%20state-of-the-art%0Aperformance%20on%20the%20nuScenes%20and%20Argoverse2%20datasets.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/pnnnnnnn/PrevPredMap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17378v1&entry.124074799=Read"},
{"title": "QUACK: Quantum Aligned Centroid Kernel", "author": "Kilian Tscharke and Sebastian Issel and Pascal Debus", "abstract": "  Quantum computing (QC) seems to show potential for application in machine\nlearning (ML). In particular quantum kernel methods (QKM) exhibit promising\nproperties for use in supervised ML tasks. However, a major disadvantage of\nkernel methods is their unfavorable quadratic scaling with the number of\ntraining samples. Together with the limits imposed by currently available\nquantum hardware (NISQ devices) with their low qubit coherence times, small\nnumber of qubits, and high error rates, the use of QC in ML at an industrially\nrelevant scale is currently impossible. As a small step in improving the\npotential applications of QKMs, we introduce QUACK, a quantum kernel algorithm\nwhose time complexity scales linear with the number of samples during training,\nand independent of the number of training samples in the inference stage. In\nthe training process, only the kernel entries for the samples and the centers\nof the classes are calculated, i.e. the maximum shape of the kernel for n\nsamples and c classes is (n, c). During training, the parameters of the quantum\nkernel and the positions of the centroids are optimized iteratively. In the\ninference stage, for every new sample the circuit is only evaluated for every\ncentroid, i.e. c times. We show that the QUACK algorithm nevertheless provides\nsatisfactory results and can perform at a similar level as classical kernel\nmethods with quadratic scaling during training. In addition, our (simulated)\nalgorithm is able to handle high-dimensional datasets such as MNIST with 784\nfeatures without any dimensionality reduction.\n", "link": "http://arxiv.org/abs/2405.00304v2", "date": "2024-07-24", "relevancy": 2.2229, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.465}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.436}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QUACK%3A%20Quantum%20Aligned%20Centroid%20Kernel&body=Title%3A%20QUACK%3A%20Quantum%20Aligned%20Centroid%20Kernel%0AAuthor%3A%20Kilian%20Tscharke%20and%20Sebastian%20Issel%20and%20Pascal%20Debus%0AAbstract%3A%20%20%20Quantum%20computing%20%28QC%29%20seems%20to%20show%20potential%20for%20application%20in%20machine%0Alearning%20%28ML%29.%20In%20particular%20quantum%20kernel%20methods%20%28QKM%29%20exhibit%20promising%0Aproperties%20for%20use%20in%20supervised%20ML%20tasks.%20However%2C%20a%20major%20disadvantage%20of%0Akernel%20methods%20is%20their%20unfavorable%20quadratic%20scaling%20with%20the%20number%20of%0Atraining%20samples.%20Together%20with%20the%20limits%20imposed%20by%20currently%20available%0Aquantum%20hardware%20%28NISQ%20devices%29%20with%20their%20low%20qubit%20coherence%20times%2C%20small%0Anumber%20of%20qubits%2C%20and%20high%20error%20rates%2C%20the%20use%20of%20QC%20in%20ML%20at%20an%20industrially%0Arelevant%20scale%20is%20currently%20impossible.%20As%20a%20small%20step%20in%20improving%20the%0Apotential%20applications%20of%20QKMs%2C%20we%20introduce%20QUACK%2C%20a%20quantum%20kernel%20algorithm%0Awhose%20time%20complexity%20scales%20linear%20with%20the%20number%20of%20samples%20during%20training%2C%0Aand%20independent%20of%20the%20number%20of%20training%20samples%20in%20the%20inference%20stage.%20In%0Athe%20training%20process%2C%20only%20the%20kernel%20entries%20for%20the%20samples%20and%20the%20centers%0Aof%20the%20classes%20are%20calculated%2C%20i.e.%20the%20maximum%20shape%20of%20the%20kernel%20for%20n%0Asamples%20and%20c%20classes%20is%20%28n%2C%20c%29.%20During%20training%2C%20the%20parameters%20of%20the%20quantum%0Akernel%20and%20the%20positions%20of%20the%20centroids%20are%20optimized%20iteratively.%20In%20the%0Ainference%20stage%2C%20for%20every%20new%20sample%20the%20circuit%20is%20only%20evaluated%20for%20every%0Acentroid%2C%20i.e.%20c%20times.%20We%20show%20that%20the%20QUACK%20algorithm%20nevertheless%20provides%0Asatisfactory%20results%20and%20can%20perform%20at%20a%20similar%20level%20as%20classical%20kernel%0Amethods%20with%20quadratic%20scaling%20during%20training.%20In%20addition%2C%20our%20%28simulated%29%0Aalgorithm%20is%20able%20to%20handle%20high-dimensional%20datasets%20such%20as%20MNIST%20with%20784%0Afeatures%20without%20any%20dimensionality%20reduction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00304v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQUACK%253A%2520Quantum%2520Aligned%2520Centroid%2520Kernel%26entry.906535625%3DKilian%2520Tscharke%2520and%2520Sebastian%2520Issel%2520and%2520Pascal%2520Debus%26entry.1292438233%3D%2520%2520Quantum%2520computing%2520%2528QC%2529%2520seems%2520to%2520show%2520potential%2520for%2520application%2520in%2520machine%250Alearning%2520%2528ML%2529.%2520In%2520particular%2520quantum%2520kernel%2520methods%2520%2528QKM%2529%2520exhibit%2520promising%250Aproperties%2520for%2520use%2520in%2520supervised%2520ML%2520tasks.%2520However%252C%2520a%2520major%2520disadvantage%2520of%250Akernel%2520methods%2520is%2520their%2520unfavorable%2520quadratic%2520scaling%2520with%2520the%2520number%2520of%250Atraining%2520samples.%2520Together%2520with%2520the%2520limits%2520imposed%2520by%2520currently%2520available%250Aquantum%2520hardware%2520%2528NISQ%2520devices%2529%2520with%2520their%2520low%2520qubit%2520coherence%2520times%252C%2520small%250Anumber%2520of%2520qubits%252C%2520and%2520high%2520error%2520rates%252C%2520the%2520use%2520of%2520QC%2520in%2520ML%2520at%2520an%2520industrially%250Arelevant%2520scale%2520is%2520currently%2520impossible.%2520As%2520a%2520small%2520step%2520in%2520improving%2520the%250Apotential%2520applications%2520of%2520QKMs%252C%2520we%2520introduce%2520QUACK%252C%2520a%2520quantum%2520kernel%2520algorithm%250Awhose%2520time%2520complexity%2520scales%2520linear%2520with%2520the%2520number%2520of%2520samples%2520during%2520training%252C%250Aand%2520independent%2520of%2520the%2520number%2520of%2520training%2520samples%2520in%2520the%2520inference%2520stage.%2520In%250Athe%2520training%2520process%252C%2520only%2520the%2520kernel%2520entries%2520for%2520the%2520samples%2520and%2520the%2520centers%250Aof%2520the%2520classes%2520are%2520calculated%252C%2520i.e.%2520the%2520maximum%2520shape%2520of%2520the%2520kernel%2520for%2520n%250Asamples%2520and%2520c%2520classes%2520is%2520%2528n%252C%2520c%2529.%2520During%2520training%252C%2520the%2520parameters%2520of%2520the%2520quantum%250Akernel%2520and%2520the%2520positions%2520of%2520the%2520centroids%2520are%2520optimized%2520iteratively.%2520In%2520the%250Ainference%2520stage%252C%2520for%2520every%2520new%2520sample%2520the%2520circuit%2520is%2520only%2520evaluated%2520for%2520every%250Acentroid%252C%2520i.e.%2520c%2520times.%2520We%2520show%2520that%2520the%2520QUACK%2520algorithm%2520nevertheless%2520provides%250Asatisfactory%2520results%2520and%2520can%2520perform%2520at%2520a%2520similar%2520level%2520as%2520classical%2520kernel%250Amethods%2520with%2520quadratic%2520scaling%2520during%2520training.%2520In%2520addition%252C%2520our%2520%2528simulated%2529%250Aalgorithm%2520is%2520able%2520to%2520handle%2520high-dimensional%2520datasets%2520such%2520as%2520MNIST%2520with%2520784%250Afeatures%2520without%2520any%2520dimensionality%2520reduction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.00304v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QUACK%3A%20Quantum%20Aligned%20Centroid%20Kernel&entry.906535625=Kilian%20Tscharke%20and%20Sebastian%20Issel%20and%20Pascal%20Debus&entry.1292438233=%20%20Quantum%20computing%20%28QC%29%20seems%20to%20show%20potential%20for%20application%20in%20machine%0Alearning%20%28ML%29.%20In%20particular%20quantum%20kernel%20methods%20%28QKM%29%20exhibit%20promising%0Aproperties%20for%20use%20in%20supervised%20ML%20tasks.%20However%2C%20a%20major%20disadvantage%20of%0Akernel%20methods%20is%20their%20unfavorable%20quadratic%20scaling%20with%20the%20number%20of%0Atraining%20samples.%20Together%20with%20the%20limits%20imposed%20by%20currently%20available%0Aquantum%20hardware%20%28NISQ%20devices%29%20with%20their%20low%20qubit%20coherence%20times%2C%20small%0Anumber%20of%20qubits%2C%20and%20high%20error%20rates%2C%20the%20use%20of%20QC%20in%20ML%20at%20an%20industrially%0Arelevant%20scale%20is%20currently%20impossible.%20As%20a%20small%20step%20in%20improving%20the%0Apotential%20applications%20of%20QKMs%2C%20we%20introduce%20QUACK%2C%20a%20quantum%20kernel%20algorithm%0Awhose%20time%20complexity%20scales%20linear%20with%20the%20number%20of%20samples%20during%20training%2C%0Aand%20independent%20of%20the%20number%20of%20training%20samples%20in%20the%20inference%20stage.%20In%0Athe%20training%20process%2C%20only%20the%20kernel%20entries%20for%20the%20samples%20and%20the%20centers%0Aof%20the%20classes%20are%20calculated%2C%20i.e.%20the%20maximum%20shape%20of%20the%20kernel%20for%20n%0Asamples%20and%20c%20classes%20is%20%28n%2C%20c%29.%20During%20training%2C%20the%20parameters%20of%20the%20quantum%0Akernel%20and%20the%20positions%20of%20the%20centroids%20are%20optimized%20iteratively.%20In%20the%0Ainference%20stage%2C%20for%20every%20new%20sample%20the%20circuit%20is%20only%20evaluated%20for%20every%0Acentroid%2C%20i.e.%20c%20times.%20We%20show%20that%20the%20QUACK%20algorithm%20nevertheless%20provides%0Asatisfactory%20results%20and%20can%20perform%20at%20a%20similar%20level%20as%20classical%20kernel%0Amethods%20with%20quadratic%20scaling%20during%20training.%20In%20addition%2C%20our%20%28simulated%29%0Aalgorithm%20is%20able%20to%20handle%20high-dimensional%20datasets%20such%20as%20MNIST%20with%20784%0Afeatures%20without%20any%20dimensionality%20reduction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00304v2&entry.124074799=Read"},
{"title": "Context-aware Multi-task Learning for Pedestrian Intent and Trajectory\n  Prediction", "author": "Farzeen Munir and Tomasz Piotr Kucner", "abstract": "  The advancement of socially-aware autonomous vehicles hinges on precise\nmodeling of human behavior. Within this broad paradigm, the specific challenge\nlies in accurately predicting pedestrian's trajectory and intention.\nTraditional methodologies have leaned heavily on historical trajectory data,\nfrequently overlooking vital contextual cues such as pedestrian-specific traits\nand environmental factors. Furthermore, there's a notable knowledge gap as\ntrajectory and intention prediction have largely been approached as separate\nproblems, despite their mutual dependence. To bridge this gap, we introduce\nPTINet (Pedestrian Trajectory and Intention Prediction Network), which jointly\nlearns the trajectory and intention prediction by combining past trajectory\nobservations, local contextual features (individual pedestrian behaviors), and\nglobal features (signs, markings etc.). The efficacy of our approach is\nevaluated on widely used public datasets: JAAD and PIE, where it has\ndemonstrated superior performance over existing state-of-the-art models in\ntrajectory and intention prediction. The results from our experiments and\nablation studies robustly validate PTINet's effectiveness in jointly exploring\nintention and trajectory prediction for pedestrian behaviour modelling. The\nexperimental evaluation indicates the advantage of using global and local\ncontextual features for pedestrian trajectory and intention prediction. The\neffectiveness of PTINet in predicting pedestrian behavior paves the way for the\ndevelopment of automated systems capable of seamlessly interacting with\npedestrians in urban settings.\n", "link": "http://arxiv.org/abs/2407.17162v1", "date": "2024-07-24", "relevancy": 2.2131, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6328}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5444}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context-aware%20Multi-task%20Learning%20for%20Pedestrian%20Intent%20and%20Trajectory%0A%20%20Prediction&body=Title%3A%20Context-aware%20Multi-task%20Learning%20for%20Pedestrian%20Intent%20and%20Trajectory%0A%20%20Prediction%0AAuthor%3A%20Farzeen%20Munir%20and%20Tomasz%20Piotr%20Kucner%0AAbstract%3A%20%20%20The%20advancement%20of%20socially-aware%20autonomous%20vehicles%20hinges%20on%20precise%0Amodeling%20of%20human%20behavior.%20Within%20this%20broad%20paradigm%2C%20the%20specific%20challenge%0Alies%20in%20accurately%20predicting%20pedestrian%27s%20trajectory%20and%20intention.%0ATraditional%20methodologies%20have%20leaned%20heavily%20on%20historical%20trajectory%20data%2C%0Afrequently%20overlooking%20vital%20contextual%20cues%20such%20as%20pedestrian-specific%20traits%0Aand%20environmental%20factors.%20Furthermore%2C%20there%27s%20a%20notable%20knowledge%20gap%20as%0Atrajectory%20and%20intention%20prediction%20have%20largely%20been%20approached%20as%20separate%0Aproblems%2C%20despite%20their%20mutual%20dependence.%20To%20bridge%20this%20gap%2C%20we%20introduce%0APTINet%20%28Pedestrian%20Trajectory%20and%20Intention%20Prediction%20Network%29%2C%20which%20jointly%0Alearns%20the%20trajectory%20and%20intention%20prediction%20by%20combining%20past%20trajectory%0Aobservations%2C%20local%20contextual%20features%20%28individual%20pedestrian%20behaviors%29%2C%20and%0Aglobal%20features%20%28signs%2C%20markings%20etc.%29.%20The%20efficacy%20of%20our%20approach%20is%0Aevaluated%20on%20widely%20used%20public%20datasets%3A%20JAAD%20and%20PIE%2C%20where%20it%20has%0Ademonstrated%20superior%20performance%20over%20existing%20state-of-the-art%20models%20in%0Atrajectory%20and%20intention%20prediction.%20The%20results%20from%20our%20experiments%20and%0Aablation%20studies%20robustly%20validate%20PTINet%27s%20effectiveness%20in%20jointly%20exploring%0Aintention%20and%20trajectory%20prediction%20for%20pedestrian%20behaviour%20modelling.%20The%0Aexperimental%20evaluation%20indicates%20the%20advantage%20of%20using%20global%20and%20local%0Acontextual%20features%20for%20pedestrian%20trajectory%20and%20intention%20prediction.%20The%0Aeffectiveness%20of%20PTINet%20in%20predicting%20pedestrian%20behavior%20paves%20the%20way%20for%20the%0Adevelopment%20of%20automated%20systems%20capable%20of%20seamlessly%20interacting%20with%0Apedestrians%20in%20urban%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17162v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext-aware%2520Multi-task%2520Learning%2520for%2520Pedestrian%2520Intent%2520and%2520Trajectory%250A%2520%2520Prediction%26entry.906535625%3DFarzeen%2520Munir%2520and%2520Tomasz%2520Piotr%2520Kucner%26entry.1292438233%3D%2520%2520The%2520advancement%2520of%2520socially-aware%2520autonomous%2520vehicles%2520hinges%2520on%2520precise%250Amodeling%2520of%2520human%2520behavior.%2520Within%2520this%2520broad%2520paradigm%252C%2520the%2520specific%2520challenge%250Alies%2520in%2520accurately%2520predicting%2520pedestrian%2527s%2520trajectory%2520and%2520intention.%250ATraditional%2520methodologies%2520have%2520leaned%2520heavily%2520on%2520historical%2520trajectory%2520data%252C%250Afrequently%2520overlooking%2520vital%2520contextual%2520cues%2520such%2520as%2520pedestrian-specific%2520traits%250Aand%2520environmental%2520factors.%2520Furthermore%252C%2520there%2527s%2520a%2520notable%2520knowledge%2520gap%2520as%250Atrajectory%2520and%2520intention%2520prediction%2520have%2520largely%2520been%2520approached%2520as%2520separate%250Aproblems%252C%2520despite%2520their%2520mutual%2520dependence.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%250APTINet%2520%2528Pedestrian%2520Trajectory%2520and%2520Intention%2520Prediction%2520Network%2529%252C%2520which%2520jointly%250Alearns%2520the%2520trajectory%2520and%2520intention%2520prediction%2520by%2520combining%2520past%2520trajectory%250Aobservations%252C%2520local%2520contextual%2520features%2520%2528individual%2520pedestrian%2520behaviors%2529%252C%2520and%250Aglobal%2520features%2520%2528signs%252C%2520markings%2520etc.%2529.%2520The%2520efficacy%2520of%2520our%2520approach%2520is%250Aevaluated%2520on%2520widely%2520used%2520public%2520datasets%253A%2520JAAD%2520and%2520PIE%252C%2520where%2520it%2520has%250Ademonstrated%2520superior%2520performance%2520over%2520existing%2520state-of-the-art%2520models%2520in%250Atrajectory%2520and%2520intention%2520prediction.%2520The%2520results%2520from%2520our%2520experiments%2520and%250Aablation%2520studies%2520robustly%2520validate%2520PTINet%2527s%2520effectiveness%2520in%2520jointly%2520exploring%250Aintention%2520and%2520trajectory%2520prediction%2520for%2520pedestrian%2520behaviour%2520modelling.%2520The%250Aexperimental%2520evaluation%2520indicates%2520the%2520advantage%2520of%2520using%2520global%2520and%2520local%250Acontextual%2520features%2520for%2520pedestrian%2520trajectory%2520and%2520intention%2520prediction.%2520The%250Aeffectiveness%2520of%2520PTINet%2520in%2520predicting%2520pedestrian%2520behavior%2520paves%2520the%2520way%2520for%2520the%250Adevelopment%2520of%2520automated%2520systems%2520capable%2520of%2520seamlessly%2520interacting%2520with%250Apedestrians%2520in%2520urban%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17162v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context-aware%20Multi-task%20Learning%20for%20Pedestrian%20Intent%20and%20Trajectory%0A%20%20Prediction&entry.906535625=Farzeen%20Munir%20and%20Tomasz%20Piotr%20Kucner&entry.1292438233=%20%20The%20advancement%20of%20socially-aware%20autonomous%20vehicles%20hinges%20on%20precise%0Amodeling%20of%20human%20behavior.%20Within%20this%20broad%20paradigm%2C%20the%20specific%20challenge%0Alies%20in%20accurately%20predicting%20pedestrian%27s%20trajectory%20and%20intention.%0ATraditional%20methodologies%20have%20leaned%20heavily%20on%20historical%20trajectory%20data%2C%0Afrequently%20overlooking%20vital%20contextual%20cues%20such%20as%20pedestrian-specific%20traits%0Aand%20environmental%20factors.%20Furthermore%2C%20there%27s%20a%20notable%20knowledge%20gap%20as%0Atrajectory%20and%20intention%20prediction%20have%20largely%20been%20approached%20as%20separate%0Aproblems%2C%20despite%20their%20mutual%20dependence.%20To%20bridge%20this%20gap%2C%20we%20introduce%0APTINet%20%28Pedestrian%20Trajectory%20and%20Intention%20Prediction%20Network%29%2C%20which%20jointly%0Alearns%20the%20trajectory%20and%20intention%20prediction%20by%20combining%20past%20trajectory%0Aobservations%2C%20local%20contextual%20features%20%28individual%20pedestrian%20behaviors%29%2C%20and%0Aglobal%20features%20%28signs%2C%20markings%20etc.%29.%20The%20efficacy%20of%20our%20approach%20is%0Aevaluated%20on%20widely%20used%20public%20datasets%3A%20JAAD%20and%20PIE%2C%20where%20it%20has%0Ademonstrated%20superior%20performance%20over%20existing%20state-of-the-art%20models%20in%0Atrajectory%20and%20intention%20prediction.%20The%20results%20from%20our%20experiments%20and%0Aablation%20studies%20robustly%20validate%20PTINet%27s%20effectiveness%20in%20jointly%20exploring%0Aintention%20and%20trajectory%20prediction%20for%20pedestrian%20behaviour%20modelling.%20The%0Aexperimental%20evaluation%20indicates%20the%20advantage%20of%20using%20global%20and%20local%0Acontextual%20features%20for%20pedestrian%20trajectory%20and%20intention%20prediction.%20The%0Aeffectiveness%20of%20PTINet%20in%20predicting%20pedestrian%20behavior%20paves%20the%20way%20for%20the%0Adevelopment%20of%20automated%20systems%20capable%20of%20seamlessly%20interacting%20with%0Apedestrians%20in%20urban%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17162v1&entry.124074799=Read"},
{"title": "TLControl: Trajectory and Language Control for Human Motion Synthesis", "author": "Weilin Wan and Zhiyang Dou and Taku Komura and Wenping Wang and Dinesh Jayaraman and Lingjie Liu", "abstract": "  Controllable human motion synthesis is essential for applications in AR/VR,\ngaming and embodied AI. Existing methods often focus solely on either language\nor full trajectory control, lacking precision in synthesizing motions aligned\nwith user-specified trajectories, especially for multi-joint control. To\naddress these issues, we present TLControl, a novel method for realistic human\nmotion synthesis, incorporating both low-level Trajectory and high-level\nLanguage semantics controls, through the integration of neural-based and\noptimization-based techniques. Specifically, we begin with training a VQ-VAE\nfor a compact and well-structured latent motion space organized by body parts.\nWe then propose a Masked Trajectories Transformer (MTT) for predicting a motion\ndistribution conditioned on language and trajectory. Once trained, we use MTT\nto sample initial motion predictions given user-specified partial trajectories\nand text descriptions as conditioning. Finally, we introduce a test-time\noptimization to refine these coarse predictions for precise trajectory control,\nwhich offers flexibility by allowing users to specify various optimization\ngoals and ensures high runtime efficiency. Comprehensive experiments show that\nTLControl significantly outperforms the state-of-the-art in trajectory accuracy\nand time efficiency, making it practical for interactive and high-quality\nanimation generation.\n", "link": "http://arxiv.org/abs/2311.17135v4", "date": "2024-07-24", "relevancy": 2.1847, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5657}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5628}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TLControl%3A%20Trajectory%20and%20Language%20Control%20for%20Human%20Motion%20Synthesis&body=Title%3A%20TLControl%3A%20Trajectory%20and%20Language%20Control%20for%20Human%20Motion%20Synthesis%0AAuthor%3A%20Weilin%20Wan%20and%20Zhiyang%20Dou%20and%20Taku%20Komura%20and%20Wenping%20Wang%20and%20Dinesh%20Jayaraman%20and%20Lingjie%20Liu%0AAbstract%3A%20%20%20Controllable%20human%20motion%20synthesis%20is%20essential%20for%20applications%20in%20AR/VR%2C%0Agaming%20and%20embodied%20AI.%20Existing%20methods%20often%20focus%20solely%20on%20either%20language%0Aor%20full%20trajectory%20control%2C%20lacking%20precision%20in%20synthesizing%20motions%20aligned%0Awith%20user-specified%20trajectories%2C%20especially%20for%20multi-joint%20control.%20To%0Aaddress%20these%20issues%2C%20we%20present%20TLControl%2C%20a%20novel%20method%20for%20realistic%20human%0Amotion%20synthesis%2C%20incorporating%20both%20low-level%20Trajectory%20and%20high-level%0ALanguage%20semantics%20controls%2C%20through%20the%20integration%20of%20neural-based%20and%0Aoptimization-based%20techniques.%20Specifically%2C%20we%20begin%20with%20training%20a%20VQ-VAE%0Afor%20a%20compact%20and%20well-structured%20latent%20motion%20space%20organized%20by%20body%20parts.%0AWe%20then%20propose%20a%20Masked%20Trajectories%20Transformer%20%28MTT%29%20for%20predicting%20a%20motion%0Adistribution%20conditioned%20on%20language%20and%20trajectory.%20Once%20trained%2C%20we%20use%20MTT%0Ato%20sample%20initial%20motion%20predictions%20given%20user-specified%20partial%20trajectories%0Aand%20text%20descriptions%20as%20conditioning.%20Finally%2C%20we%20introduce%20a%20test-time%0Aoptimization%20to%20refine%20these%20coarse%20predictions%20for%20precise%20trajectory%20control%2C%0Awhich%20offers%20flexibility%20by%20allowing%20users%20to%20specify%20various%20optimization%0Agoals%20and%20ensures%20high%20runtime%20efficiency.%20Comprehensive%20experiments%20show%20that%0ATLControl%20significantly%20outperforms%20the%20state-of-the-art%20in%20trajectory%20accuracy%0Aand%20time%20efficiency%2C%20making%20it%20practical%20for%20interactive%20and%20high-quality%0Aanimation%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17135v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTLControl%253A%2520Trajectory%2520and%2520Language%2520Control%2520for%2520Human%2520Motion%2520Synthesis%26entry.906535625%3DWeilin%2520Wan%2520and%2520Zhiyang%2520Dou%2520and%2520Taku%2520Komura%2520and%2520Wenping%2520Wang%2520and%2520Dinesh%2520Jayaraman%2520and%2520Lingjie%2520Liu%26entry.1292438233%3D%2520%2520Controllable%2520human%2520motion%2520synthesis%2520is%2520essential%2520for%2520applications%2520in%2520AR/VR%252C%250Agaming%2520and%2520embodied%2520AI.%2520Existing%2520methods%2520often%2520focus%2520solely%2520on%2520either%2520language%250Aor%2520full%2520trajectory%2520control%252C%2520lacking%2520precision%2520in%2520synthesizing%2520motions%2520aligned%250Awith%2520user-specified%2520trajectories%252C%2520especially%2520for%2520multi-joint%2520control.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520present%2520TLControl%252C%2520a%2520novel%2520method%2520for%2520realistic%2520human%250Amotion%2520synthesis%252C%2520incorporating%2520both%2520low-level%2520Trajectory%2520and%2520high-level%250ALanguage%2520semantics%2520controls%252C%2520through%2520the%2520integration%2520of%2520neural-based%2520and%250Aoptimization-based%2520techniques.%2520Specifically%252C%2520we%2520begin%2520with%2520training%2520a%2520VQ-VAE%250Afor%2520a%2520compact%2520and%2520well-structured%2520latent%2520motion%2520space%2520organized%2520by%2520body%2520parts.%250AWe%2520then%2520propose%2520a%2520Masked%2520Trajectories%2520Transformer%2520%2528MTT%2529%2520for%2520predicting%2520a%2520motion%250Adistribution%2520conditioned%2520on%2520language%2520and%2520trajectory.%2520Once%2520trained%252C%2520we%2520use%2520MTT%250Ato%2520sample%2520initial%2520motion%2520predictions%2520given%2520user-specified%2520partial%2520trajectories%250Aand%2520text%2520descriptions%2520as%2520conditioning.%2520Finally%252C%2520we%2520introduce%2520a%2520test-time%250Aoptimization%2520to%2520refine%2520these%2520coarse%2520predictions%2520for%2520precise%2520trajectory%2520control%252C%250Awhich%2520offers%2520flexibility%2520by%2520allowing%2520users%2520to%2520specify%2520various%2520optimization%250Agoals%2520and%2520ensures%2520high%2520runtime%2520efficiency.%2520Comprehensive%2520experiments%2520show%2520that%250ATLControl%2520significantly%2520outperforms%2520the%2520state-of-the-art%2520in%2520trajectory%2520accuracy%250Aand%2520time%2520efficiency%252C%2520making%2520it%2520practical%2520for%2520interactive%2520and%2520high-quality%250Aanimation%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.17135v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TLControl%3A%20Trajectory%20and%20Language%20Control%20for%20Human%20Motion%20Synthesis&entry.906535625=Weilin%20Wan%20and%20Zhiyang%20Dou%20and%20Taku%20Komura%20and%20Wenping%20Wang%20and%20Dinesh%20Jayaraman%20and%20Lingjie%20Liu&entry.1292438233=%20%20Controllable%20human%20motion%20synthesis%20is%20essential%20for%20applications%20in%20AR/VR%2C%0Agaming%20and%20embodied%20AI.%20Existing%20methods%20often%20focus%20solely%20on%20either%20language%0Aor%20full%20trajectory%20control%2C%20lacking%20precision%20in%20synthesizing%20motions%20aligned%0Awith%20user-specified%20trajectories%2C%20especially%20for%20multi-joint%20control.%20To%0Aaddress%20these%20issues%2C%20we%20present%20TLControl%2C%20a%20novel%20method%20for%20realistic%20human%0Amotion%20synthesis%2C%20incorporating%20both%20low-level%20Trajectory%20and%20high-level%0ALanguage%20semantics%20controls%2C%20through%20the%20integration%20of%20neural-based%20and%0Aoptimization-based%20techniques.%20Specifically%2C%20we%20begin%20with%20training%20a%20VQ-VAE%0Afor%20a%20compact%20and%20well-structured%20latent%20motion%20space%20organized%20by%20body%20parts.%0AWe%20then%20propose%20a%20Masked%20Trajectories%20Transformer%20%28MTT%29%20for%20predicting%20a%20motion%0Adistribution%20conditioned%20on%20language%20and%20trajectory.%20Once%20trained%2C%20we%20use%20MTT%0Ato%20sample%20initial%20motion%20predictions%20given%20user-specified%20partial%20trajectories%0Aand%20text%20descriptions%20as%20conditioning.%20Finally%2C%20we%20introduce%20a%20test-time%0Aoptimization%20to%20refine%20these%20coarse%20predictions%20for%20precise%20trajectory%20control%2C%0Awhich%20offers%20flexibility%20by%20allowing%20users%20to%20specify%20various%20optimization%0Agoals%20and%20ensures%20high%20runtime%20efficiency.%20Comprehensive%20experiments%20show%20that%0ATLControl%20significantly%20outperforms%20the%20state-of-the-art%20in%20trajectory%20accuracy%0Aand%20time%20efficiency%2C%20making%20it%20practical%20for%20interactive%20and%20high-quality%0Aanimation%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17135v4&entry.124074799=Read"},
{"title": "Path Following and Stabilisation of a Bicycle Model using a\n  Reinforcement Learning Approach", "author": "Sebastian Weyrer and Peter Manzl and A. L. Schwab and Johannes Gerstmayr", "abstract": "  Over the years, complex control approaches have been developed to control the\nmotion of a bicycle. Reinforcement Learning (RL), a branch of machine learning,\npromises easy deployment of so-called agents. Deployed agents are increasingly\nconsidered as an alternative to controllers for mechanical systems. The present\nwork introduces an RL approach to do path following with a virtual bicycle\nmodel while simultaneously stabilising it laterally. The bicycle, modelled as\nthe Whipple benchmark model and using multibody system dynamics, has no\nstabilisation aids. The agent succeeds in both path following and stabilisation\nof the bicycle model exclusively by outputting steering angles, which are\nconverted into steering torques via a PD controller. Curriculum learning is\napplied as a state-of-the-art training strategy. Different settings for the\nimplemented RL framework are investigated and compared to each other. The\nperformance of the deployed agents is evaluated using different types of paths\nand measurements. The ability of the deployed agents to do path following and\nstabilisation of the bicycle model travelling between 2m/s and 7m/s along\ncomplex paths including full circles, slalom manoeuvres, and lane changes is\ndemonstrated. Explanatory methods for machine learning are used to analyse the\nfunctionality of a deployed agent and link the introduced RL approach with\nresearch in the field of bicycle dynamics.\n", "link": "http://arxiv.org/abs/2407.17156v1", "date": "2024-07-24", "relevancy": 2.18, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5679}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5618}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Path%20Following%20and%20Stabilisation%20of%20a%20Bicycle%20Model%20using%20a%0A%20%20Reinforcement%20Learning%20Approach&body=Title%3A%20Path%20Following%20and%20Stabilisation%20of%20a%20Bicycle%20Model%20using%20a%0A%20%20Reinforcement%20Learning%20Approach%0AAuthor%3A%20Sebastian%20Weyrer%20and%20Peter%20Manzl%20and%20A.%20L.%20Schwab%20and%20Johannes%20Gerstmayr%0AAbstract%3A%20%20%20Over%20the%20years%2C%20complex%20control%20approaches%20have%20been%20developed%20to%20control%20the%0Amotion%20of%20a%20bicycle.%20Reinforcement%20Learning%20%28RL%29%2C%20a%20branch%20of%20machine%20learning%2C%0Apromises%20easy%20deployment%20of%20so-called%20agents.%20Deployed%20agents%20are%20increasingly%0Aconsidered%20as%20an%20alternative%20to%20controllers%20for%20mechanical%20systems.%20The%20present%0Awork%20introduces%20an%20RL%20approach%20to%20do%20path%20following%20with%20a%20virtual%20bicycle%0Amodel%20while%20simultaneously%20stabilising%20it%20laterally.%20The%20bicycle%2C%20modelled%20as%0Athe%20Whipple%20benchmark%20model%20and%20using%20multibody%20system%20dynamics%2C%20has%20no%0Astabilisation%20aids.%20The%20agent%20succeeds%20in%20both%20path%20following%20and%20stabilisation%0Aof%20the%20bicycle%20model%20exclusively%20by%20outputting%20steering%20angles%2C%20which%20are%0Aconverted%20into%20steering%20torques%20via%20a%20PD%20controller.%20Curriculum%20learning%20is%0Aapplied%20as%20a%20state-of-the-art%20training%20strategy.%20Different%20settings%20for%20the%0Aimplemented%20RL%20framework%20are%20investigated%20and%20compared%20to%20each%20other.%20The%0Aperformance%20of%20the%20deployed%20agents%20is%20evaluated%20using%20different%20types%20of%20paths%0Aand%20measurements.%20The%20ability%20of%20the%20deployed%20agents%20to%20do%20path%20following%20and%0Astabilisation%20of%20the%20bicycle%20model%20travelling%20between%202m/s%20and%207m/s%20along%0Acomplex%20paths%20including%20full%20circles%2C%20slalom%20manoeuvres%2C%20and%20lane%20changes%20is%0Ademonstrated.%20Explanatory%20methods%20for%20machine%20learning%20are%20used%20to%20analyse%20the%0Afunctionality%20of%20a%20deployed%20agent%20and%20link%20the%20introduced%20RL%20approach%20with%0Aresearch%20in%20the%20field%20of%20bicycle%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17156v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPath%2520Following%2520and%2520Stabilisation%2520of%2520a%2520Bicycle%2520Model%2520using%2520a%250A%2520%2520Reinforcement%2520Learning%2520Approach%26entry.906535625%3DSebastian%2520Weyrer%2520and%2520Peter%2520Manzl%2520and%2520A.%2520L.%2520Schwab%2520and%2520Johannes%2520Gerstmayr%26entry.1292438233%3D%2520%2520Over%2520the%2520years%252C%2520complex%2520control%2520approaches%2520have%2520been%2520developed%2520to%2520control%2520the%250Amotion%2520of%2520a%2520bicycle.%2520Reinforcement%2520Learning%2520%2528RL%2529%252C%2520a%2520branch%2520of%2520machine%2520learning%252C%250Apromises%2520easy%2520deployment%2520of%2520so-called%2520agents.%2520Deployed%2520agents%2520are%2520increasingly%250Aconsidered%2520as%2520an%2520alternative%2520to%2520controllers%2520for%2520mechanical%2520systems.%2520The%2520present%250Awork%2520introduces%2520an%2520RL%2520approach%2520to%2520do%2520path%2520following%2520with%2520a%2520virtual%2520bicycle%250Amodel%2520while%2520simultaneously%2520stabilising%2520it%2520laterally.%2520The%2520bicycle%252C%2520modelled%2520as%250Athe%2520Whipple%2520benchmark%2520model%2520and%2520using%2520multibody%2520system%2520dynamics%252C%2520has%2520no%250Astabilisation%2520aids.%2520The%2520agent%2520succeeds%2520in%2520both%2520path%2520following%2520and%2520stabilisation%250Aof%2520the%2520bicycle%2520model%2520exclusively%2520by%2520outputting%2520steering%2520angles%252C%2520which%2520are%250Aconverted%2520into%2520steering%2520torques%2520via%2520a%2520PD%2520controller.%2520Curriculum%2520learning%2520is%250Aapplied%2520as%2520a%2520state-of-the-art%2520training%2520strategy.%2520Different%2520settings%2520for%2520the%250Aimplemented%2520RL%2520framework%2520are%2520investigated%2520and%2520compared%2520to%2520each%2520other.%2520The%250Aperformance%2520of%2520the%2520deployed%2520agents%2520is%2520evaluated%2520using%2520different%2520types%2520of%2520paths%250Aand%2520measurements.%2520The%2520ability%2520of%2520the%2520deployed%2520agents%2520to%2520do%2520path%2520following%2520and%250Astabilisation%2520of%2520the%2520bicycle%2520model%2520travelling%2520between%25202m/s%2520and%25207m/s%2520along%250Acomplex%2520paths%2520including%2520full%2520circles%252C%2520slalom%2520manoeuvres%252C%2520and%2520lane%2520changes%2520is%250Ademonstrated.%2520Explanatory%2520methods%2520for%2520machine%2520learning%2520are%2520used%2520to%2520analyse%2520the%250Afunctionality%2520of%2520a%2520deployed%2520agent%2520and%2520link%2520the%2520introduced%2520RL%2520approach%2520with%250Aresearch%2520in%2520the%2520field%2520of%2520bicycle%2520dynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17156v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Path%20Following%20and%20Stabilisation%20of%20a%20Bicycle%20Model%20using%20a%0A%20%20Reinforcement%20Learning%20Approach&entry.906535625=Sebastian%20Weyrer%20and%20Peter%20Manzl%20and%20A.%20L.%20Schwab%20and%20Johannes%20Gerstmayr&entry.1292438233=%20%20Over%20the%20years%2C%20complex%20control%20approaches%20have%20been%20developed%20to%20control%20the%0Amotion%20of%20a%20bicycle.%20Reinforcement%20Learning%20%28RL%29%2C%20a%20branch%20of%20machine%20learning%2C%0Apromises%20easy%20deployment%20of%20so-called%20agents.%20Deployed%20agents%20are%20increasingly%0Aconsidered%20as%20an%20alternative%20to%20controllers%20for%20mechanical%20systems.%20The%20present%0Awork%20introduces%20an%20RL%20approach%20to%20do%20path%20following%20with%20a%20virtual%20bicycle%0Amodel%20while%20simultaneously%20stabilising%20it%20laterally.%20The%20bicycle%2C%20modelled%20as%0Athe%20Whipple%20benchmark%20model%20and%20using%20multibody%20system%20dynamics%2C%20has%20no%0Astabilisation%20aids.%20The%20agent%20succeeds%20in%20both%20path%20following%20and%20stabilisation%0Aof%20the%20bicycle%20model%20exclusively%20by%20outputting%20steering%20angles%2C%20which%20are%0Aconverted%20into%20steering%20torques%20via%20a%20PD%20controller.%20Curriculum%20learning%20is%0Aapplied%20as%20a%20state-of-the-art%20training%20strategy.%20Different%20settings%20for%20the%0Aimplemented%20RL%20framework%20are%20investigated%20and%20compared%20to%20each%20other.%20The%0Aperformance%20of%20the%20deployed%20agents%20is%20evaluated%20using%20different%20types%20of%20paths%0Aand%20measurements.%20The%20ability%20of%20the%20deployed%20agents%20to%20do%20path%20following%20and%0Astabilisation%20of%20the%20bicycle%20model%20travelling%20between%202m/s%20and%207m/s%20along%0Acomplex%20paths%20including%20full%20circles%2C%20slalom%20manoeuvres%2C%20and%20lane%20changes%20is%0Ademonstrated.%20Explanatory%20methods%20for%20machine%20learning%20are%20used%20to%20analyse%20the%0Afunctionality%20of%20a%20deployed%20agent%20and%20link%20the%20introduced%20RL%20approach%20with%0Aresearch%20in%20the%20field%20of%20bicycle%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17156v1&entry.124074799=Read"},
{"title": "SwinSF: Image Reconstruction from Spatial-Temporal Spike Streams", "author": "Liangyan Jiang and Chuang Zhu and Yanxu Chen", "abstract": "  The spike camera, with its high temporal resolution, low latency, and high\ndynamic range, addresses high-speed imaging challenges like motion blur. It\ncaptures photons at each pixel independently, creating binary spike streams\nrich in temporal information but challenging for image reconstruction. Current\nalgorithms, both traditional and deep learning-based, still need to be improved\nin the utilization of the rich temporal detail and the restoration of the\ndetails of the reconstructed image. To overcome this, we introduce Swin\nSpikeformer (SwinSF), a novel model for dynamic scene reconstruction from spike\nstreams. SwinSF is composed of Spike Feature Extraction, Spatial-Temporal\nFeature Extraction, and Final Reconstruction Module. It combines shifted window\nself-attention and proposed temporal spike attention, ensuring a comprehensive\nfeature extraction that encapsulates both spatial and temporal dynamics,\nleading to a more robust and accurate reconstruction of spike streams.\nFurthermore, we build a new synthesized dataset for spike image reconstruction\nwhich matches the resolution of the latest spike camera, ensuring its relevance\nand applicability to the latest developments in spike camera imaging.\nExperimental results demonstrate that the proposed network SwinSF sets a new\nbenchmark, achieving state-of-the-art performance across a series of datasets,\nincluding both real-world and synthesized data across various resolutions. Our\ncodes and proposed dataset will be available soon.\n", "link": "http://arxiv.org/abs/2407.15708v2", "date": "2024-07-24", "relevancy": 2.1796, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5791}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5214}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5181}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SwinSF%3A%20Image%20Reconstruction%20from%20Spatial-Temporal%20Spike%20Streams&body=Title%3A%20SwinSF%3A%20Image%20Reconstruction%20from%20Spatial-Temporal%20Spike%20Streams%0AAuthor%3A%20Liangyan%20Jiang%20and%20Chuang%20Zhu%20and%20Yanxu%20Chen%0AAbstract%3A%20%20%20The%20spike%20camera%2C%20with%20its%20high%20temporal%20resolution%2C%20low%20latency%2C%20and%20high%0Adynamic%20range%2C%20addresses%20high-speed%20imaging%20challenges%20like%20motion%20blur.%20It%0Acaptures%20photons%20at%20each%20pixel%20independently%2C%20creating%20binary%20spike%20streams%0Arich%20in%20temporal%20information%20but%20challenging%20for%20image%20reconstruction.%20Current%0Aalgorithms%2C%20both%20traditional%20and%20deep%20learning-based%2C%20still%20need%20to%20be%20improved%0Ain%20the%20utilization%20of%20the%20rich%20temporal%20detail%20and%20the%20restoration%20of%20the%0Adetails%20of%20the%20reconstructed%20image.%20To%20overcome%20this%2C%20we%20introduce%20Swin%0ASpikeformer%20%28SwinSF%29%2C%20a%20novel%20model%20for%20dynamic%20scene%20reconstruction%20from%20spike%0Astreams.%20SwinSF%20is%20composed%20of%20Spike%20Feature%20Extraction%2C%20Spatial-Temporal%0AFeature%20Extraction%2C%20and%20Final%20Reconstruction%20Module.%20It%20combines%20shifted%20window%0Aself-attention%20and%20proposed%20temporal%20spike%20attention%2C%20ensuring%20a%20comprehensive%0Afeature%20extraction%20that%20encapsulates%20both%20spatial%20and%20temporal%20dynamics%2C%0Aleading%20to%20a%20more%20robust%20and%20accurate%20reconstruction%20of%20spike%20streams.%0AFurthermore%2C%20we%20build%20a%20new%20synthesized%20dataset%20for%20spike%20image%20reconstruction%0Awhich%20matches%20the%20resolution%20of%20the%20latest%20spike%20camera%2C%20ensuring%20its%20relevance%0Aand%20applicability%20to%20the%20latest%20developments%20in%20spike%20camera%20imaging.%0AExperimental%20results%20demonstrate%20that%20the%20proposed%20network%20SwinSF%20sets%20a%20new%0Abenchmark%2C%20achieving%20state-of-the-art%20performance%20across%20a%20series%20of%20datasets%2C%0Aincluding%20both%20real-world%20and%20synthesized%20data%20across%20various%20resolutions.%20Our%0Acodes%20and%20proposed%20dataset%20will%20be%20available%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15708v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSwinSF%253A%2520Image%2520Reconstruction%2520from%2520Spatial-Temporal%2520Spike%2520Streams%26entry.906535625%3DLiangyan%2520Jiang%2520and%2520Chuang%2520Zhu%2520and%2520Yanxu%2520Chen%26entry.1292438233%3D%2520%2520The%2520spike%2520camera%252C%2520with%2520its%2520high%2520temporal%2520resolution%252C%2520low%2520latency%252C%2520and%2520high%250Adynamic%2520range%252C%2520addresses%2520high-speed%2520imaging%2520challenges%2520like%2520motion%2520blur.%2520It%250Acaptures%2520photons%2520at%2520each%2520pixel%2520independently%252C%2520creating%2520binary%2520spike%2520streams%250Arich%2520in%2520temporal%2520information%2520but%2520challenging%2520for%2520image%2520reconstruction.%2520Current%250Aalgorithms%252C%2520both%2520traditional%2520and%2520deep%2520learning-based%252C%2520still%2520need%2520to%2520be%2520improved%250Ain%2520the%2520utilization%2520of%2520the%2520rich%2520temporal%2520detail%2520and%2520the%2520restoration%2520of%2520the%250Adetails%2520of%2520the%2520reconstructed%2520image.%2520To%2520overcome%2520this%252C%2520we%2520introduce%2520Swin%250ASpikeformer%2520%2528SwinSF%2529%252C%2520a%2520novel%2520model%2520for%2520dynamic%2520scene%2520reconstruction%2520from%2520spike%250Astreams.%2520SwinSF%2520is%2520composed%2520of%2520Spike%2520Feature%2520Extraction%252C%2520Spatial-Temporal%250AFeature%2520Extraction%252C%2520and%2520Final%2520Reconstruction%2520Module.%2520It%2520combines%2520shifted%2520window%250Aself-attention%2520and%2520proposed%2520temporal%2520spike%2520attention%252C%2520ensuring%2520a%2520comprehensive%250Afeature%2520extraction%2520that%2520encapsulates%2520both%2520spatial%2520and%2520temporal%2520dynamics%252C%250Aleading%2520to%2520a%2520more%2520robust%2520and%2520accurate%2520reconstruction%2520of%2520spike%2520streams.%250AFurthermore%252C%2520we%2520build%2520a%2520new%2520synthesized%2520dataset%2520for%2520spike%2520image%2520reconstruction%250Awhich%2520matches%2520the%2520resolution%2520of%2520the%2520latest%2520spike%2520camera%252C%2520ensuring%2520its%2520relevance%250Aand%2520applicability%2520to%2520the%2520latest%2520developments%2520in%2520spike%2520camera%2520imaging.%250AExperimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520network%2520SwinSF%2520sets%2520a%2520new%250Abenchmark%252C%2520achieving%2520state-of-the-art%2520performance%2520across%2520a%2520series%2520of%2520datasets%252C%250Aincluding%2520both%2520real-world%2520and%2520synthesized%2520data%2520across%2520various%2520resolutions.%2520Our%250Acodes%2520and%2520proposed%2520dataset%2520will%2520be%2520available%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15708v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SwinSF%3A%20Image%20Reconstruction%20from%20Spatial-Temporal%20Spike%20Streams&entry.906535625=Liangyan%20Jiang%20and%20Chuang%20Zhu%20and%20Yanxu%20Chen&entry.1292438233=%20%20The%20spike%20camera%2C%20with%20its%20high%20temporal%20resolution%2C%20low%20latency%2C%20and%20high%0Adynamic%20range%2C%20addresses%20high-speed%20imaging%20challenges%20like%20motion%20blur.%20It%0Acaptures%20photons%20at%20each%20pixel%20independently%2C%20creating%20binary%20spike%20streams%0Arich%20in%20temporal%20information%20but%20challenging%20for%20image%20reconstruction.%20Current%0Aalgorithms%2C%20both%20traditional%20and%20deep%20learning-based%2C%20still%20need%20to%20be%20improved%0Ain%20the%20utilization%20of%20the%20rich%20temporal%20detail%20and%20the%20restoration%20of%20the%0Adetails%20of%20the%20reconstructed%20image.%20To%20overcome%20this%2C%20we%20introduce%20Swin%0ASpikeformer%20%28SwinSF%29%2C%20a%20novel%20model%20for%20dynamic%20scene%20reconstruction%20from%20spike%0Astreams.%20SwinSF%20is%20composed%20of%20Spike%20Feature%20Extraction%2C%20Spatial-Temporal%0AFeature%20Extraction%2C%20and%20Final%20Reconstruction%20Module.%20It%20combines%20shifted%20window%0Aself-attention%20and%20proposed%20temporal%20spike%20attention%2C%20ensuring%20a%20comprehensive%0Afeature%20extraction%20that%20encapsulates%20both%20spatial%20and%20temporal%20dynamics%2C%0Aleading%20to%20a%20more%20robust%20and%20accurate%20reconstruction%20of%20spike%20streams.%0AFurthermore%2C%20we%20build%20a%20new%20synthesized%20dataset%20for%20spike%20image%20reconstruction%0Awhich%20matches%20the%20resolution%20of%20the%20latest%20spike%20camera%2C%20ensuring%20its%20relevance%0Aand%20applicability%20to%20the%20latest%20developments%20in%20spike%20camera%20imaging.%0AExperimental%20results%20demonstrate%20that%20the%20proposed%20network%20SwinSF%20sets%20a%20new%0Abenchmark%2C%20achieving%20state-of-the-art%20performance%20across%20a%20series%20of%20datasets%2C%0Aincluding%20both%20real-world%20and%20synthesized%20data%20across%20various%20resolutions.%20Our%0Acodes%20and%20proposed%20dataset%20will%20be%20available%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15708v2&entry.124074799=Read"},
{"title": "Learning Based NMPC Adaptation for Autonomous Driving using Parallelized\n  Digital Twin", "author": "Jean Pierre Allamaa and Panagiotis Patrinos and Herman Van der Auweraer and Tong Duy Son", "abstract": "  In this work, we focus on the challenge of transferring an autonomous driving\ncontroller from simulation to the real world (i.e. Sim2Real). We propose a\ndata-efficient method for online and on-the-fly adaptation of parametrizable\ncontrol architectures such that the target closed-loop performance is optimized\nwhile accounting for uncertainties as model mismatches, changes in the\nenvironment, and task variations. The novelty of the approach resides in\nleveraging black-box optimization enabled by Executable Digital Twins (xDTs)\nfor data-driven parameter calibration through derivative-free methods to\ndirectly adapt the controller in real-time. The xDTs are augmented with Domain\nRandomization for robustness and allow for safe parameter exploration. The\nproposed method requires a minimal amount of interaction with the real-world as\nit pushes the exploration towards the xDTs. We validate our approach through\nreal-world experiments, demonstrating its effectiveness in transferring and\nfine-tuning a NMPC with 9 parameters, in under 10 minutes. This eliminates the\nneed for hours-long manual tuning and lengthy machine learning training and\ndata collection phases. Our results show that the online adapted NMPC directly\ncompensates for the Sim2Real gap and avoids overtuning in simulation.\nImportantly, a 75% improvement in tracking performance is achieved and the\nSim2Real gap over the target performance is reduced from a factor of 876 to\n1.033.\n", "link": "http://arxiv.org/abs/2402.16645v2", "date": "2024-07-24", "relevancy": 2.1613, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5563}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5545}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Based%20NMPC%20Adaptation%20for%20Autonomous%20Driving%20using%20Parallelized%0A%20%20Digital%20Twin&body=Title%3A%20Learning%20Based%20NMPC%20Adaptation%20for%20Autonomous%20Driving%20using%20Parallelized%0A%20%20Digital%20Twin%0AAuthor%3A%20Jean%20Pierre%20Allamaa%20and%20Panagiotis%20Patrinos%20and%20Herman%20Van%20der%20Auweraer%20and%20Tong%20Duy%20Son%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20focus%20on%20the%20challenge%20of%20transferring%20an%20autonomous%20driving%0Acontroller%20from%20simulation%20to%20the%20real%20world%20%28i.e.%20Sim2Real%29.%20We%20propose%20a%0Adata-efficient%20method%20for%20online%20and%20on-the-fly%20adaptation%20of%20parametrizable%0Acontrol%20architectures%20such%20that%20the%20target%20closed-loop%20performance%20is%20optimized%0Awhile%20accounting%20for%20uncertainties%20as%20model%20mismatches%2C%20changes%20in%20the%0Aenvironment%2C%20and%20task%20variations.%20The%20novelty%20of%20the%20approach%20resides%20in%0Aleveraging%20black-box%20optimization%20enabled%20by%20Executable%20Digital%20Twins%20%28xDTs%29%0Afor%20data-driven%20parameter%20calibration%20through%20derivative-free%20methods%20to%0Adirectly%20adapt%20the%20controller%20in%20real-time.%20The%20xDTs%20are%20augmented%20with%20Domain%0ARandomization%20for%20robustness%20and%20allow%20for%20safe%20parameter%20exploration.%20The%0Aproposed%20method%20requires%20a%20minimal%20amount%20of%20interaction%20with%20the%20real-world%20as%0Ait%20pushes%20the%20exploration%20towards%20the%20xDTs.%20We%20validate%20our%20approach%20through%0Areal-world%20experiments%2C%20demonstrating%20its%20effectiveness%20in%20transferring%20and%0Afine-tuning%20a%20NMPC%20with%209%20parameters%2C%20in%20under%2010%20minutes.%20This%20eliminates%20the%0Aneed%20for%20hours-long%20manual%20tuning%20and%20lengthy%20machine%20learning%20training%20and%0Adata%20collection%20phases.%20Our%20results%20show%20that%20the%20online%20adapted%20NMPC%20directly%0Acompensates%20for%20the%20Sim2Real%20gap%20and%20avoids%20overtuning%20in%20simulation.%0AImportantly%2C%20a%2075%25%20improvement%20in%20tracking%20performance%20is%20achieved%20and%20the%0ASim2Real%20gap%20over%20the%20target%20performance%20is%20reduced%20from%20a%20factor%20of%20876%20to%0A1.033.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.16645v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Based%2520NMPC%2520Adaptation%2520for%2520Autonomous%2520Driving%2520using%2520Parallelized%250A%2520%2520Digital%2520Twin%26entry.906535625%3DJean%2520Pierre%2520Allamaa%2520and%2520Panagiotis%2520Patrinos%2520and%2520Herman%2520Van%2520der%2520Auweraer%2520and%2520Tong%2520Duy%2520Son%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520the%2520challenge%2520of%2520transferring%2520an%2520autonomous%2520driving%250Acontroller%2520from%2520simulation%2520to%2520the%2520real%2520world%2520%2528i.e.%2520Sim2Real%2529.%2520We%2520propose%2520a%250Adata-efficient%2520method%2520for%2520online%2520and%2520on-the-fly%2520adaptation%2520of%2520parametrizable%250Acontrol%2520architectures%2520such%2520that%2520the%2520target%2520closed-loop%2520performance%2520is%2520optimized%250Awhile%2520accounting%2520for%2520uncertainties%2520as%2520model%2520mismatches%252C%2520changes%2520in%2520the%250Aenvironment%252C%2520and%2520task%2520variations.%2520The%2520novelty%2520of%2520the%2520approach%2520resides%2520in%250Aleveraging%2520black-box%2520optimization%2520enabled%2520by%2520Executable%2520Digital%2520Twins%2520%2528xDTs%2529%250Afor%2520data-driven%2520parameter%2520calibration%2520through%2520derivative-free%2520methods%2520to%250Adirectly%2520adapt%2520the%2520controller%2520in%2520real-time.%2520The%2520xDTs%2520are%2520augmented%2520with%2520Domain%250ARandomization%2520for%2520robustness%2520and%2520allow%2520for%2520safe%2520parameter%2520exploration.%2520The%250Aproposed%2520method%2520requires%2520a%2520minimal%2520amount%2520of%2520interaction%2520with%2520the%2520real-world%2520as%250Ait%2520pushes%2520the%2520exploration%2520towards%2520the%2520xDTs.%2520We%2520validate%2520our%2520approach%2520through%250Areal-world%2520experiments%252C%2520demonstrating%2520its%2520effectiveness%2520in%2520transferring%2520and%250Afine-tuning%2520a%2520NMPC%2520with%25209%2520parameters%252C%2520in%2520under%252010%2520minutes.%2520This%2520eliminates%2520the%250Aneed%2520for%2520hours-long%2520manual%2520tuning%2520and%2520lengthy%2520machine%2520learning%2520training%2520and%250Adata%2520collection%2520phases.%2520Our%2520results%2520show%2520that%2520the%2520online%2520adapted%2520NMPC%2520directly%250Acompensates%2520for%2520the%2520Sim2Real%2520gap%2520and%2520avoids%2520overtuning%2520in%2520simulation.%250AImportantly%252C%2520a%252075%2525%2520improvement%2520in%2520tracking%2520performance%2520is%2520achieved%2520and%2520the%250ASim2Real%2520gap%2520over%2520the%2520target%2520performance%2520is%2520reduced%2520from%2520a%2520factor%2520of%2520876%2520to%250A1.033.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.16645v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Based%20NMPC%20Adaptation%20for%20Autonomous%20Driving%20using%20Parallelized%0A%20%20Digital%20Twin&entry.906535625=Jean%20Pierre%20Allamaa%20and%20Panagiotis%20Patrinos%20and%20Herman%20Van%20der%20Auweraer%20and%20Tong%20Duy%20Son&entry.1292438233=%20%20In%20this%20work%2C%20we%20focus%20on%20the%20challenge%20of%20transferring%20an%20autonomous%20driving%0Acontroller%20from%20simulation%20to%20the%20real%20world%20%28i.e.%20Sim2Real%29.%20We%20propose%20a%0Adata-efficient%20method%20for%20online%20and%20on-the-fly%20adaptation%20of%20parametrizable%0Acontrol%20architectures%20such%20that%20the%20target%20closed-loop%20performance%20is%20optimized%0Awhile%20accounting%20for%20uncertainties%20as%20model%20mismatches%2C%20changes%20in%20the%0Aenvironment%2C%20and%20task%20variations.%20The%20novelty%20of%20the%20approach%20resides%20in%0Aleveraging%20black-box%20optimization%20enabled%20by%20Executable%20Digital%20Twins%20%28xDTs%29%0Afor%20data-driven%20parameter%20calibration%20through%20derivative-free%20methods%20to%0Adirectly%20adapt%20the%20controller%20in%20real-time.%20The%20xDTs%20are%20augmented%20with%20Domain%0ARandomization%20for%20robustness%20and%20allow%20for%20safe%20parameter%20exploration.%20The%0Aproposed%20method%20requires%20a%20minimal%20amount%20of%20interaction%20with%20the%20real-world%20as%0Ait%20pushes%20the%20exploration%20towards%20the%20xDTs.%20We%20validate%20our%20approach%20through%0Areal-world%20experiments%2C%20demonstrating%20its%20effectiveness%20in%20transferring%20and%0Afine-tuning%20a%20NMPC%20with%209%20parameters%2C%20in%20under%2010%20minutes.%20This%20eliminates%20the%0Aneed%20for%20hours-long%20manual%20tuning%20and%20lengthy%20machine%20learning%20training%20and%0Adata%20collection%20phases.%20Our%20results%20show%20that%20the%20online%20adapted%20NMPC%20directly%0Acompensates%20for%20the%20Sim2Real%20gap%20and%20avoids%20overtuning%20in%20simulation.%0AImportantly%2C%20a%2075%25%20improvement%20in%20tracking%20performance%20is%20achieved%20and%20the%0ASim2Real%20gap%20over%20the%20target%20performance%20is%20reduced%20from%20a%20factor%20of%20876%20to%0A1.033.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16645v2&entry.124074799=Read"},
{"title": "Towards Robust Continual Learning with Bayesian Adaptive Moment\n  Regularization", "author": "Jack Foster and Alexandra Brintrup", "abstract": "  The pursuit of long-term autonomy mandates that machine learning models must\ncontinuously adapt to their changing environments and learn to solve new tasks.\nContinual learning seeks to overcome the challenge of catastrophic forgetting,\nwhere learning to solve new tasks causes a model to forget previously learnt\ninformation. Prior-based continual learning methods are appealing as they are\ncomputationally efficient and do not require auxiliary models or data storage.\nHowever, prior-based approaches typically fail on important benchmarks and are\nthus limited in their potential applications compared to their memory-based\ncounterparts. We introduce Bayesian adaptive moment regularization (BAdam), a\nnovel prior-based method that better constrains parameter growth, reducing\ncatastrophic forgetting. Our method boasts a range of desirable properties such\nas being lightweight and task label-free, converging quickly, and offering\ncalibrated uncertainty that is important for safe real-world deployment.\nResults show that BAdam achieves state-of-the-art performance for prior-based\nmethods on challenging single-headed class-incremental experiments such as\nSplit MNIST and Split FashionMNIST, and does so without relying on task labels\nor discrete task boundaries.\n", "link": "http://arxiv.org/abs/2309.08546v3", "date": "2024-07-24", "relevancy": 2.1608, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5707}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5498}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Robust%20Continual%20Learning%20with%20Bayesian%20Adaptive%20Moment%0A%20%20Regularization&body=Title%3A%20Towards%20Robust%20Continual%20Learning%20with%20Bayesian%20Adaptive%20Moment%0A%20%20Regularization%0AAuthor%3A%20Jack%20Foster%20and%20Alexandra%20Brintrup%0AAbstract%3A%20%20%20The%20pursuit%20of%20long-term%20autonomy%20mandates%20that%20machine%20learning%20models%20must%0Acontinuously%20adapt%20to%20their%20changing%20environments%20and%20learn%20to%20solve%20new%20tasks.%0AContinual%20learning%20seeks%20to%20overcome%20the%20challenge%20of%20catastrophic%20forgetting%2C%0Awhere%20learning%20to%20solve%20new%20tasks%20causes%20a%20model%20to%20forget%20previously%20learnt%0Ainformation.%20Prior-based%20continual%20learning%20methods%20are%20appealing%20as%20they%20are%0Acomputationally%20efficient%20and%20do%20not%20require%20auxiliary%20models%20or%20data%20storage.%0AHowever%2C%20prior-based%20approaches%20typically%20fail%20on%20important%20benchmarks%20and%20are%0Athus%20limited%20in%20their%20potential%20applications%20compared%20to%20their%20memory-based%0Acounterparts.%20We%20introduce%20Bayesian%20adaptive%20moment%20regularization%20%28BAdam%29%2C%20a%0Anovel%20prior-based%20method%20that%20better%20constrains%20parameter%20growth%2C%20reducing%0Acatastrophic%20forgetting.%20Our%20method%20boasts%20a%20range%20of%20desirable%20properties%20such%0Aas%20being%20lightweight%20and%20task%20label-free%2C%20converging%20quickly%2C%20and%20offering%0Acalibrated%20uncertainty%20that%20is%20important%20for%20safe%20real-world%20deployment.%0AResults%20show%20that%20BAdam%20achieves%20state-of-the-art%20performance%20for%20prior-based%0Amethods%20on%20challenging%20single-headed%20class-incremental%20experiments%20such%20as%0ASplit%20MNIST%20and%20Split%20FashionMNIST%2C%20and%20does%20so%20without%20relying%20on%20task%20labels%0Aor%20discrete%20task%20boundaries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.08546v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Robust%2520Continual%2520Learning%2520with%2520Bayesian%2520Adaptive%2520Moment%250A%2520%2520Regularization%26entry.906535625%3DJack%2520Foster%2520and%2520Alexandra%2520Brintrup%26entry.1292438233%3D%2520%2520The%2520pursuit%2520of%2520long-term%2520autonomy%2520mandates%2520that%2520machine%2520learning%2520models%2520must%250Acontinuously%2520adapt%2520to%2520their%2520changing%2520environments%2520and%2520learn%2520to%2520solve%2520new%2520tasks.%250AContinual%2520learning%2520seeks%2520to%2520overcome%2520the%2520challenge%2520of%2520catastrophic%2520forgetting%252C%250Awhere%2520learning%2520to%2520solve%2520new%2520tasks%2520causes%2520a%2520model%2520to%2520forget%2520previously%2520learnt%250Ainformation.%2520Prior-based%2520continual%2520learning%2520methods%2520are%2520appealing%2520as%2520they%2520are%250Acomputationally%2520efficient%2520and%2520do%2520not%2520require%2520auxiliary%2520models%2520or%2520data%2520storage.%250AHowever%252C%2520prior-based%2520approaches%2520typically%2520fail%2520on%2520important%2520benchmarks%2520and%2520are%250Athus%2520limited%2520in%2520their%2520potential%2520applications%2520compared%2520to%2520their%2520memory-based%250Acounterparts.%2520We%2520introduce%2520Bayesian%2520adaptive%2520moment%2520regularization%2520%2528BAdam%2529%252C%2520a%250Anovel%2520prior-based%2520method%2520that%2520better%2520constrains%2520parameter%2520growth%252C%2520reducing%250Acatastrophic%2520forgetting.%2520Our%2520method%2520boasts%2520a%2520range%2520of%2520desirable%2520properties%2520such%250Aas%2520being%2520lightweight%2520and%2520task%2520label-free%252C%2520converging%2520quickly%252C%2520and%2520offering%250Acalibrated%2520uncertainty%2520that%2520is%2520important%2520for%2520safe%2520real-world%2520deployment.%250AResults%2520show%2520that%2520BAdam%2520achieves%2520state-of-the-art%2520performance%2520for%2520prior-based%250Amethods%2520on%2520challenging%2520single-headed%2520class-incremental%2520experiments%2520such%2520as%250ASplit%2520MNIST%2520and%2520Split%2520FashionMNIST%252C%2520and%2520does%2520so%2520without%2520relying%2520on%2520task%2520labels%250Aor%2520discrete%2520task%2520boundaries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.08546v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Robust%20Continual%20Learning%20with%20Bayesian%20Adaptive%20Moment%0A%20%20Regularization&entry.906535625=Jack%20Foster%20and%20Alexandra%20Brintrup&entry.1292438233=%20%20The%20pursuit%20of%20long-term%20autonomy%20mandates%20that%20machine%20learning%20models%20must%0Acontinuously%20adapt%20to%20their%20changing%20environments%20and%20learn%20to%20solve%20new%20tasks.%0AContinual%20learning%20seeks%20to%20overcome%20the%20challenge%20of%20catastrophic%20forgetting%2C%0Awhere%20learning%20to%20solve%20new%20tasks%20causes%20a%20model%20to%20forget%20previously%20learnt%0Ainformation.%20Prior-based%20continual%20learning%20methods%20are%20appealing%20as%20they%20are%0Acomputationally%20efficient%20and%20do%20not%20require%20auxiliary%20models%20or%20data%20storage.%0AHowever%2C%20prior-based%20approaches%20typically%20fail%20on%20important%20benchmarks%20and%20are%0Athus%20limited%20in%20their%20potential%20applications%20compared%20to%20their%20memory-based%0Acounterparts.%20We%20introduce%20Bayesian%20adaptive%20moment%20regularization%20%28BAdam%29%2C%20a%0Anovel%20prior-based%20method%20that%20better%20constrains%20parameter%20growth%2C%20reducing%0Acatastrophic%20forgetting.%20Our%20method%20boasts%20a%20range%20of%20desirable%20properties%20such%0Aas%20being%20lightweight%20and%20task%20label-free%2C%20converging%20quickly%2C%20and%20offering%0Acalibrated%20uncertainty%20that%20is%20important%20for%20safe%20real-world%20deployment.%0AResults%20show%20that%20BAdam%20achieves%20state-of-the-art%20performance%20for%20prior-based%0Amethods%20on%20challenging%20single-headed%20class-incremental%20experiments%20such%20as%0ASplit%20MNIST%20and%20Split%20FashionMNIST%2C%20and%20does%20so%20without%20relying%20on%20task%20labels%0Aor%20discrete%20task%20boundaries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.08546v3&entry.124074799=Read"},
{"title": "$VILA^2$: VILA Augmented VILA", "author": "Yunhao Fang and Ligeng Zhu and Yao Lu and Yan Wang and Pavlo Molchanov and Jang Hyun Cho and Marco Pavone and Song Han and Hongxu Yin", "abstract": "  Visual language models (VLMs) have rapidly progressed, driven by the success\nof large language models (LLMs). While model architectures and training\ninfrastructures advance rapidly, data curation remains under-explored. When\ndata quantity and quality become a bottleneck, existing work either directly\ncrawls more raw data from the Internet that does not have a guarantee of data\nquality or distills from black-box commercial models (e.g., GPT-4V / Gemini)\ncausing the performance upper bounded by that model. In this work, we introduce\na novel approach that includes a self-augment step and a specialist-augment\nstep to iteratively improve data quality and model performance. In the\nself-augment step, a VLM recaptions its own pretraining data to enhance data\nquality, and then retrains from scratch using this refined dataset to improve\nmodel performance. This process can iterate for several rounds. Once\nself-augmentation saturates, we employ several specialist VLMs finetuned from\nthe self-augmented VLM with domain-specific expertise, to further infuse\nspecialist knowledge into the generalist VLM through task-oriented recaptioning\nand retraining. With the combined self-augmented and specialist-augmented\ntraining, we introduce $VILA^2$ (VILA-augmented-VILA), a VLM family that\nconsistently improves the accuracy on a wide range of tasks over prior art, and\nachieves new state-of-the-art results on MMMU leaderboard among open-sourced\nmodels.\n", "link": "http://arxiv.org/abs/2407.17453v1", "date": "2024-07-24", "relevancy": 2.1361, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5509}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5303}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24VILA%5E2%24%3A%20VILA%20Augmented%20VILA&body=Title%3A%20%24VILA%5E2%24%3A%20VILA%20Augmented%20VILA%0AAuthor%3A%20Yunhao%20Fang%20and%20Ligeng%20Zhu%20and%20Yao%20Lu%20and%20Yan%20Wang%20and%20Pavlo%20Molchanov%20and%20Jang%20Hyun%20Cho%20and%20Marco%20Pavone%20and%20Song%20Han%20and%20Hongxu%20Yin%0AAbstract%3A%20%20%20Visual%20language%20models%20%28VLMs%29%20have%20rapidly%20progressed%2C%20driven%20by%20the%20success%0Aof%20large%20language%20models%20%28LLMs%29.%20While%20model%20architectures%20and%20training%0Ainfrastructures%20advance%20rapidly%2C%20data%20curation%20remains%20under-explored.%20When%0Adata%20quantity%20and%20quality%20become%20a%20bottleneck%2C%20existing%20work%20either%20directly%0Acrawls%20more%20raw%20data%20from%20the%20Internet%20that%20does%20not%20have%20a%20guarantee%20of%20data%0Aquality%20or%20distills%20from%20black-box%20commercial%20models%20%28e.g.%2C%20GPT-4V%20/%20Gemini%29%0Acausing%20the%20performance%20upper%20bounded%20by%20that%20model.%20In%20this%20work%2C%20we%20introduce%0Aa%20novel%20approach%20that%20includes%20a%20self-augment%20step%20and%20a%20specialist-augment%0Astep%20to%20iteratively%20improve%20data%20quality%20and%20model%20performance.%20In%20the%0Aself-augment%20step%2C%20a%20VLM%20recaptions%20its%20own%20pretraining%20data%20to%20enhance%20data%0Aquality%2C%20and%20then%20retrains%20from%20scratch%20using%20this%20refined%20dataset%20to%20improve%0Amodel%20performance.%20This%20process%20can%20iterate%20for%20several%20rounds.%20Once%0Aself-augmentation%20saturates%2C%20we%20employ%20several%20specialist%20VLMs%20finetuned%20from%0Athe%20self-augmented%20VLM%20with%20domain-specific%20expertise%2C%20to%20further%20infuse%0Aspecialist%20knowledge%20into%20the%20generalist%20VLM%20through%20task-oriented%20recaptioning%0Aand%20retraining.%20With%20the%20combined%20self-augmented%20and%20specialist-augmented%0Atraining%2C%20we%20introduce%20%24VILA%5E2%24%20%28VILA-augmented-VILA%29%2C%20a%20VLM%20family%20that%0Aconsistently%20improves%20the%20accuracy%20on%20a%20wide%20range%20of%20tasks%20over%20prior%20art%2C%20and%0Aachieves%20new%20state-of-the-art%20results%20on%20MMMU%20leaderboard%20among%20open-sourced%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17453v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524VILA%255E2%2524%253A%2520VILA%2520Augmented%2520VILA%26entry.906535625%3DYunhao%2520Fang%2520and%2520Ligeng%2520Zhu%2520and%2520Yao%2520Lu%2520and%2520Yan%2520Wang%2520and%2520Pavlo%2520Molchanov%2520and%2520Jang%2520Hyun%2520Cho%2520and%2520Marco%2520Pavone%2520and%2520Song%2520Han%2520and%2520Hongxu%2520Yin%26entry.1292438233%3D%2520%2520Visual%2520language%2520models%2520%2528VLMs%2529%2520have%2520rapidly%2520progressed%252C%2520driven%2520by%2520the%2520success%250Aof%2520large%2520language%2520models%2520%2528LLMs%2529.%2520While%2520model%2520architectures%2520and%2520training%250Ainfrastructures%2520advance%2520rapidly%252C%2520data%2520curation%2520remains%2520under-explored.%2520When%250Adata%2520quantity%2520and%2520quality%2520become%2520a%2520bottleneck%252C%2520existing%2520work%2520either%2520directly%250Acrawls%2520more%2520raw%2520data%2520from%2520the%2520Internet%2520that%2520does%2520not%2520have%2520a%2520guarantee%2520of%2520data%250Aquality%2520or%2520distills%2520from%2520black-box%2520commercial%2520models%2520%2528e.g.%252C%2520GPT-4V%2520/%2520Gemini%2529%250Acausing%2520the%2520performance%2520upper%2520bounded%2520by%2520that%2520model.%2520In%2520this%2520work%252C%2520we%2520introduce%250Aa%2520novel%2520approach%2520that%2520includes%2520a%2520self-augment%2520step%2520and%2520a%2520specialist-augment%250Astep%2520to%2520iteratively%2520improve%2520data%2520quality%2520and%2520model%2520performance.%2520In%2520the%250Aself-augment%2520step%252C%2520a%2520VLM%2520recaptions%2520its%2520own%2520pretraining%2520data%2520to%2520enhance%2520data%250Aquality%252C%2520and%2520then%2520retrains%2520from%2520scratch%2520using%2520this%2520refined%2520dataset%2520to%2520improve%250Amodel%2520performance.%2520This%2520process%2520can%2520iterate%2520for%2520several%2520rounds.%2520Once%250Aself-augmentation%2520saturates%252C%2520we%2520employ%2520several%2520specialist%2520VLMs%2520finetuned%2520from%250Athe%2520self-augmented%2520VLM%2520with%2520domain-specific%2520expertise%252C%2520to%2520further%2520infuse%250Aspecialist%2520knowledge%2520into%2520the%2520generalist%2520VLM%2520through%2520task-oriented%2520recaptioning%250Aand%2520retraining.%2520With%2520the%2520combined%2520self-augmented%2520and%2520specialist-augmented%250Atraining%252C%2520we%2520introduce%2520%2524VILA%255E2%2524%2520%2528VILA-augmented-VILA%2529%252C%2520a%2520VLM%2520family%2520that%250Aconsistently%2520improves%2520the%2520accuracy%2520on%2520a%2520wide%2520range%2520of%2520tasks%2520over%2520prior%2520art%252C%2520and%250Aachieves%2520new%2520state-of-the-art%2520results%2520on%2520MMMU%2520leaderboard%2520among%2520open-sourced%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17453v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24VILA%5E2%24%3A%20VILA%20Augmented%20VILA&entry.906535625=Yunhao%20Fang%20and%20Ligeng%20Zhu%20and%20Yao%20Lu%20and%20Yan%20Wang%20and%20Pavlo%20Molchanov%20and%20Jang%20Hyun%20Cho%20and%20Marco%20Pavone%20and%20Song%20Han%20and%20Hongxu%20Yin&entry.1292438233=%20%20Visual%20language%20models%20%28VLMs%29%20have%20rapidly%20progressed%2C%20driven%20by%20the%20success%0Aof%20large%20language%20models%20%28LLMs%29.%20While%20model%20architectures%20and%20training%0Ainfrastructures%20advance%20rapidly%2C%20data%20curation%20remains%20under-explored.%20When%0Adata%20quantity%20and%20quality%20become%20a%20bottleneck%2C%20existing%20work%20either%20directly%0Acrawls%20more%20raw%20data%20from%20the%20Internet%20that%20does%20not%20have%20a%20guarantee%20of%20data%0Aquality%20or%20distills%20from%20black-box%20commercial%20models%20%28e.g.%2C%20GPT-4V%20/%20Gemini%29%0Acausing%20the%20performance%20upper%20bounded%20by%20that%20model.%20In%20this%20work%2C%20we%20introduce%0Aa%20novel%20approach%20that%20includes%20a%20self-augment%20step%20and%20a%20specialist-augment%0Astep%20to%20iteratively%20improve%20data%20quality%20and%20model%20performance.%20In%20the%0Aself-augment%20step%2C%20a%20VLM%20recaptions%20its%20own%20pretraining%20data%20to%20enhance%20data%0Aquality%2C%20and%20then%20retrains%20from%20scratch%20using%20this%20refined%20dataset%20to%20improve%0Amodel%20performance.%20This%20process%20can%20iterate%20for%20several%20rounds.%20Once%0Aself-augmentation%20saturates%2C%20we%20employ%20several%20specialist%20VLMs%20finetuned%20from%0Athe%20self-augmented%20VLM%20with%20domain-specific%20expertise%2C%20to%20further%20infuse%0Aspecialist%20knowledge%20into%20the%20generalist%20VLM%20through%20task-oriented%20recaptioning%0Aand%20retraining.%20With%20the%20combined%20self-augmented%20and%20specialist-augmented%0Atraining%2C%20we%20introduce%20%24VILA%5E2%24%20%28VILA-augmented-VILA%29%2C%20a%20VLM%20family%20that%0Aconsistently%20improves%20the%20accuracy%20on%20a%20wide%20range%20of%20tasks%20over%20prior%20art%2C%20and%0Aachieves%20new%20state-of-the-art%20results%20on%20MMMU%20leaderboard%20among%20open-sourced%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17453v1&entry.124074799=Read"},
{"title": "Hybrid Functional Maps for Crease-Aware Non-Isometric Shape Matching", "author": "Lennart Bastian and Yizheng Xie and Nassir Navab and Zorah L\u00e4hner", "abstract": "  Non-isometric shape correspondence remains a fundamental challenge in\ncomputer vision. Traditional methods using Laplace-Beltrami operator (LBO)\neigenmodes face limitations in characterizing high-frequency extrinsic shape\nchanges like bending and creases. We propose a novel approach of combining the\nnon-orthogonal extrinsic basis of eigenfunctions of the elastic thin-shell\nhessian with the intrinsic ones of the LBO, creating a hybrid spectral space in\nwhich we construct functional maps. To this end, we present a theoretical\nframework to effectively integrate non-orthogonal basis functions into\ndescriptor- and learning-based functional map methods. Our approach can be\nincorporated easily into existing functional map pipelines across varying\napplications and is able to handle complex deformations beyond isometries. We\nshow extensive evaluations across various supervised and unsupervised settings\nand demonstrate significant improvements. Notably, our approach achieves up to\n15% better mean geodesic error for non-isometric correspondence settings and up\nto 45% improvement in scenarios with topological noise.\n", "link": "http://arxiv.org/abs/2312.03678v3", "date": "2024-07-24", "relevancy": 2.1153, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5386}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5231}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Functional%20Maps%20for%20Crease-Aware%20Non-Isometric%20Shape%20Matching&body=Title%3A%20Hybrid%20Functional%20Maps%20for%20Crease-Aware%20Non-Isometric%20Shape%20Matching%0AAuthor%3A%20Lennart%20Bastian%20and%20Yizheng%20Xie%20and%20Nassir%20Navab%20and%20Zorah%20L%C3%A4hner%0AAbstract%3A%20%20%20Non-isometric%20shape%20correspondence%20remains%20a%20fundamental%20challenge%20in%0Acomputer%20vision.%20Traditional%20methods%20using%20Laplace-Beltrami%20operator%20%28LBO%29%0Aeigenmodes%20face%20limitations%20in%20characterizing%20high-frequency%20extrinsic%20shape%0Achanges%20like%20bending%20and%20creases.%20We%20propose%20a%20novel%20approach%20of%20combining%20the%0Anon-orthogonal%20extrinsic%20basis%20of%20eigenfunctions%20of%20the%20elastic%20thin-shell%0Ahessian%20with%20the%20intrinsic%20ones%20of%20the%20LBO%2C%20creating%20a%20hybrid%20spectral%20space%20in%0Awhich%20we%20construct%20functional%20maps.%20To%20this%20end%2C%20we%20present%20a%20theoretical%0Aframework%20to%20effectively%20integrate%20non-orthogonal%20basis%20functions%20into%0Adescriptor-%20and%20learning-based%20functional%20map%20methods.%20Our%20approach%20can%20be%0Aincorporated%20easily%20into%20existing%20functional%20map%20pipelines%20across%20varying%0Aapplications%20and%20is%20able%20to%20handle%20complex%20deformations%20beyond%20isometries.%20We%0Ashow%20extensive%20evaluations%20across%20various%20supervised%20and%20unsupervised%20settings%0Aand%20demonstrate%20significant%20improvements.%20Notably%2C%20our%20approach%20achieves%20up%20to%0A15%25%20better%20mean%20geodesic%20error%20for%20non-isometric%20correspondence%20settings%20and%20up%0Ato%2045%25%20improvement%20in%20scenarios%20with%20topological%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.03678v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Functional%2520Maps%2520for%2520Crease-Aware%2520Non-Isometric%2520Shape%2520Matching%26entry.906535625%3DLennart%2520Bastian%2520and%2520Yizheng%2520Xie%2520and%2520Nassir%2520Navab%2520and%2520Zorah%2520L%25C3%25A4hner%26entry.1292438233%3D%2520%2520Non-isometric%2520shape%2520correspondence%2520remains%2520a%2520fundamental%2520challenge%2520in%250Acomputer%2520vision.%2520Traditional%2520methods%2520using%2520Laplace-Beltrami%2520operator%2520%2528LBO%2529%250Aeigenmodes%2520face%2520limitations%2520in%2520characterizing%2520high-frequency%2520extrinsic%2520shape%250Achanges%2520like%2520bending%2520and%2520creases.%2520We%2520propose%2520a%2520novel%2520approach%2520of%2520combining%2520the%250Anon-orthogonal%2520extrinsic%2520basis%2520of%2520eigenfunctions%2520of%2520the%2520elastic%2520thin-shell%250Ahessian%2520with%2520the%2520intrinsic%2520ones%2520of%2520the%2520LBO%252C%2520creating%2520a%2520hybrid%2520spectral%2520space%2520in%250Awhich%2520we%2520construct%2520functional%2520maps.%2520To%2520this%2520end%252C%2520we%2520present%2520a%2520theoretical%250Aframework%2520to%2520effectively%2520integrate%2520non-orthogonal%2520basis%2520functions%2520into%250Adescriptor-%2520and%2520learning-based%2520functional%2520map%2520methods.%2520Our%2520approach%2520can%2520be%250Aincorporated%2520easily%2520into%2520existing%2520functional%2520map%2520pipelines%2520across%2520varying%250Aapplications%2520and%2520is%2520able%2520to%2520handle%2520complex%2520deformations%2520beyond%2520isometries.%2520We%250Ashow%2520extensive%2520evaluations%2520across%2520various%2520supervised%2520and%2520unsupervised%2520settings%250Aand%2520demonstrate%2520significant%2520improvements.%2520Notably%252C%2520our%2520approach%2520achieves%2520up%2520to%250A15%2525%2520better%2520mean%2520geodesic%2520error%2520for%2520non-isometric%2520correspondence%2520settings%2520and%2520up%250Ato%252045%2525%2520improvement%2520in%2520scenarios%2520with%2520topological%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.03678v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Functional%20Maps%20for%20Crease-Aware%20Non-Isometric%20Shape%20Matching&entry.906535625=Lennart%20Bastian%20and%20Yizheng%20Xie%20and%20Nassir%20Navab%20and%20Zorah%20L%C3%A4hner&entry.1292438233=%20%20Non-isometric%20shape%20correspondence%20remains%20a%20fundamental%20challenge%20in%0Acomputer%20vision.%20Traditional%20methods%20using%20Laplace-Beltrami%20operator%20%28LBO%29%0Aeigenmodes%20face%20limitations%20in%20characterizing%20high-frequency%20extrinsic%20shape%0Achanges%20like%20bending%20and%20creases.%20We%20propose%20a%20novel%20approach%20of%20combining%20the%0Anon-orthogonal%20extrinsic%20basis%20of%20eigenfunctions%20of%20the%20elastic%20thin-shell%0Ahessian%20with%20the%20intrinsic%20ones%20of%20the%20LBO%2C%20creating%20a%20hybrid%20spectral%20space%20in%0Awhich%20we%20construct%20functional%20maps.%20To%20this%20end%2C%20we%20present%20a%20theoretical%0Aframework%20to%20effectively%20integrate%20non-orthogonal%20basis%20functions%20into%0Adescriptor-%20and%20learning-based%20functional%20map%20methods.%20Our%20approach%20can%20be%0Aincorporated%20easily%20into%20existing%20functional%20map%20pipelines%20across%20varying%0Aapplications%20and%20is%20able%20to%20handle%20complex%20deformations%20beyond%20isometries.%20We%0Ashow%20extensive%20evaluations%20across%20various%20supervised%20and%20unsupervised%20settings%0Aand%20demonstrate%20significant%20improvements.%20Notably%2C%20our%20approach%20achieves%20up%20to%0A15%25%20better%20mean%20geodesic%20error%20for%20non-isometric%20correspondence%20settings%20and%20up%0Ato%2045%25%20improvement%20in%20scenarios%20with%20topological%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.03678v3&entry.124074799=Read"},
{"title": "FreeCG: Free the Design Space of Clebsch-Gordan Transform for Machine\n  Learning Force Fields", "author": "Shihao Shao and Haoran Geng and Zun Wang and Qinghua Cui", "abstract": "  The Clebsch-Gordan Transform (CG transform) effectively encodes many-body\ninteractions. Many studies have proven its accuracy in depicting atomic\nenvironments, although this comes with high computational needs. The\ncomputational burden of this challenge is hard to reduce due to the need for\npermutation equivariance, which limits the design space of the CG transform\nlayer. We show that, implementing the CG transform layer on\npermutation-invariant inputs allows complete freedom in the design of this\nlayer without affecting symmetry. Developing further on this premise, our idea\nis to create a CG transform layer that operates on permutation-invariant\nabstract edges generated from real edge information. We bring in group CG\ntransform with sparse path, abstract edges shuffling, and attention enhancer to\nform a powerful and efficient CG transform layer. Our method, known as FreeCG,\nachieves State-of-The-Art (SoTA) results in force prediction for MD17, rMD17,\nMD22, and property prediction in QM9 datasets with notable enhancement. The\nextensibility to other models is also examined. Molecular dynamics simulations\nare carried out on MD17 and other periodic systems, including water and LiPS,\nshowcasing the capacity for real-world applications of FreeCG. It introduces a\nnovel paradigm for carrying out efficient and expressive CG transform in future\ngeometric neural network designs.\n", "link": "http://arxiv.org/abs/2407.02263v3", "date": "2024-07-24", "relevancy": 2.1084, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5352}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5283}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreeCG%3A%20Free%20the%20Design%20Space%20of%20Clebsch-Gordan%20Transform%20for%20Machine%0A%20%20Learning%20Force%20Fields&body=Title%3A%20FreeCG%3A%20Free%20the%20Design%20Space%20of%20Clebsch-Gordan%20Transform%20for%20Machine%0A%20%20Learning%20Force%20Fields%0AAuthor%3A%20Shihao%20Shao%20and%20Haoran%20Geng%20and%20Zun%20Wang%20and%20Qinghua%20Cui%0AAbstract%3A%20%20%20The%20Clebsch-Gordan%20Transform%20%28CG%20transform%29%20effectively%20encodes%20many-body%0Ainteractions.%20Many%20studies%20have%20proven%20its%20accuracy%20in%20depicting%20atomic%0Aenvironments%2C%20although%20this%20comes%20with%20high%20computational%20needs.%20The%0Acomputational%20burden%20of%20this%20challenge%20is%20hard%20to%20reduce%20due%20to%20the%20need%20for%0Apermutation%20equivariance%2C%20which%20limits%20the%20design%20space%20of%20the%20CG%20transform%0Alayer.%20We%20show%20that%2C%20implementing%20the%20CG%20transform%20layer%20on%0Apermutation-invariant%20inputs%20allows%20complete%20freedom%20in%20the%20design%20of%20this%0Alayer%20without%20affecting%20symmetry.%20Developing%20further%20on%20this%20premise%2C%20our%20idea%0Ais%20to%20create%20a%20CG%20transform%20layer%20that%20operates%20on%20permutation-invariant%0Aabstract%20edges%20generated%20from%20real%20edge%20information.%20We%20bring%20in%20group%20CG%0Atransform%20with%20sparse%20path%2C%20abstract%20edges%20shuffling%2C%20and%20attention%20enhancer%20to%0Aform%20a%20powerful%20and%20efficient%20CG%20transform%20layer.%20Our%20method%2C%20known%20as%20FreeCG%2C%0Aachieves%20State-of-The-Art%20%28SoTA%29%20results%20in%20force%20prediction%20for%20MD17%2C%20rMD17%2C%0AMD22%2C%20and%20property%20prediction%20in%20QM9%20datasets%20with%20notable%20enhancement.%20The%0Aextensibility%20to%20other%20models%20is%20also%20examined.%20Molecular%20dynamics%20simulations%0Aare%20carried%20out%20on%20MD17%20and%20other%20periodic%20systems%2C%20including%20water%20and%20LiPS%2C%0Ashowcasing%20the%20capacity%20for%20real-world%20applications%20of%20FreeCG.%20It%20introduces%20a%0Anovel%20paradigm%20for%20carrying%20out%20efficient%20and%20expressive%20CG%20transform%20in%20future%0Ageometric%20neural%20network%20designs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02263v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreeCG%253A%2520Free%2520the%2520Design%2520Space%2520of%2520Clebsch-Gordan%2520Transform%2520for%2520Machine%250A%2520%2520Learning%2520Force%2520Fields%26entry.906535625%3DShihao%2520Shao%2520and%2520Haoran%2520Geng%2520and%2520Zun%2520Wang%2520and%2520Qinghua%2520Cui%26entry.1292438233%3D%2520%2520The%2520Clebsch-Gordan%2520Transform%2520%2528CG%2520transform%2529%2520effectively%2520encodes%2520many-body%250Ainteractions.%2520Many%2520studies%2520have%2520proven%2520its%2520accuracy%2520in%2520depicting%2520atomic%250Aenvironments%252C%2520although%2520this%2520comes%2520with%2520high%2520computational%2520needs.%2520The%250Acomputational%2520burden%2520of%2520this%2520challenge%2520is%2520hard%2520to%2520reduce%2520due%2520to%2520the%2520need%2520for%250Apermutation%2520equivariance%252C%2520which%2520limits%2520the%2520design%2520space%2520of%2520the%2520CG%2520transform%250Alayer.%2520We%2520show%2520that%252C%2520implementing%2520the%2520CG%2520transform%2520layer%2520on%250Apermutation-invariant%2520inputs%2520allows%2520complete%2520freedom%2520in%2520the%2520design%2520of%2520this%250Alayer%2520without%2520affecting%2520symmetry.%2520Developing%2520further%2520on%2520this%2520premise%252C%2520our%2520idea%250Ais%2520to%2520create%2520a%2520CG%2520transform%2520layer%2520that%2520operates%2520on%2520permutation-invariant%250Aabstract%2520edges%2520generated%2520from%2520real%2520edge%2520information.%2520We%2520bring%2520in%2520group%2520CG%250Atransform%2520with%2520sparse%2520path%252C%2520abstract%2520edges%2520shuffling%252C%2520and%2520attention%2520enhancer%2520to%250Aform%2520a%2520powerful%2520and%2520efficient%2520CG%2520transform%2520layer.%2520Our%2520method%252C%2520known%2520as%2520FreeCG%252C%250Aachieves%2520State-of-The-Art%2520%2528SoTA%2529%2520results%2520in%2520force%2520prediction%2520for%2520MD17%252C%2520rMD17%252C%250AMD22%252C%2520and%2520property%2520prediction%2520in%2520QM9%2520datasets%2520with%2520notable%2520enhancement.%2520The%250Aextensibility%2520to%2520other%2520models%2520is%2520also%2520examined.%2520Molecular%2520dynamics%2520simulations%250Aare%2520carried%2520out%2520on%2520MD17%2520and%2520other%2520periodic%2520systems%252C%2520including%2520water%2520and%2520LiPS%252C%250Ashowcasing%2520the%2520capacity%2520for%2520real-world%2520applications%2520of%2520FreeCG.%2520It%2520introduces%2520a%250Anovel%2520paradigm%2520for%2520carrying%2520out%2520efficient%2520and%2520expressive%2520CG%2520transform%2520in%2520future%250Ageometric%2520neural%2520network%2520designs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02263v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeCG%3A%20Free%20the%20Design%20Space%20of%20Clebsch-Gordan%20Transform%20for%20Machine%0A%20%20Learning%20Force%20Fields&entry.906535625=Shihao%20Shao%20and%20Haoran%20Geng%20and%20Zun%20Wang%20and%20Qinghua%20Cui&entry.1292438233=%20%20The%20Clebsch-Gordan%20Transform%20%28CG%20transform%29%20effectively%20encodes%20many-body%0Ainteractions.%20Many%20studies%20have%20proven%20its%20accuracy%20in%20depicting%20atomic%0Aenvironments%2C%20although%20this%20comes%20with%20high%20computational%20needs.%20The%0Acomputational%20burden%20of%20this%20challenge%20is%20hard%20to%20reduce%20due%20to%20the%20need%20for%0Apermutation%20equivariance%2C%20which%20limits%20the%20design%20space%20of%20the%20CG%20transform%0Alayer.%20We%20show%20that%2C%20implementing%20the%20CG%20transform%20layer%20on%0Apermutation-invariant%20inputs%20allows%20complete%20freedom%20in%20the%20design%20of%20this%0Alayer%20without%20affecting%20symmetry.%20Developing%20further%20on%20this%20premise%2C%20our%20idea%0Ais%20to%20create%20a%20CG%20transform%20layer%20that%20operates%20on%20permutation-invariant%0Aabstract%20edges%20generated%20from%20real%20edge%20information.%20We%20bring%20in%20group%20CG%0Atransform%20with%20sparse%20path%2C%20abstract%20edges%20shuffling%2C%20and%20attention%20enhancer%20to%0Aform%20a%20powerful%20and%20efficient%20CG%20transform%20layer.%20Our%20method%2C%20known%20as%20FreeCG%2C%0Aachieves%20State-of-The-Art%20%28SoTA%29%20results%20in%20force%20prediction%20for%20MD17%2C%20rMD17%2C%0AMD22%2C%20and%20property%20prediction%20in%20QM9%20datasets%20with%20notable%20enhancement.%20The%0Aextensibility%20to%20other%20models%20is%20also%20examined.%20Molecular%20dynamics%20simulations%0Aare%20carried%20out%20on%20MD17%20and%20other%20periodic%20systems%2C%20including%20water%20and%20LiPS%2C%0Ashowcasing%20the%20capacity%20for%20real-world%20applications%20of%20FreeCG.%20It%20introduces%20a%0Anovel%20paradigm%20for%20carrying%20out%20efficient%20and%20expressive%20CG%20transform%20in%20future%0Ageometric%20neural%20network%20designs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02263v3&entry.124074799=Read"},
{"title": "RT-DETRv2: Improved Baseline with Bag-of-Freebies for Real-Time\n  Detection Transformer", "author": "Wenyu Lv and Yian Zhao and Qinyao Chang and Kui Huang and Guanzhong Wang and Yi Liu", "abstract": "  In this report, we present RT-DETRv2, an improved Real-Time DEtection\nTRansformer (RT-DETR). RT-DETRv2 builds upon the previous state-of-the-art\nreal-time detector, RT-DETR, and opens up a set of bag-of-freebies for\nflexibility and practicality, as well as optimizing the training strategy to\nachieve enhanced performance. To improve the flexibility, we suggest setting a\ndistinct number of sampling points for features at different scales in the\ndeformable attention to achieve selective multi-scale feature extraction by the\ndecoder. To enhance practicality, we propose an optional discrete sampling\noperator to replace the grid_sample operator that is specific to RT-DETR\ncompared to YOLOs. This removes the deployment constraints typically associated\nwith DETRs. For the training strategy, we propose dynamic data augmentation and\nscale-adaptive hyperparameters customization to improve performance without\nloss of speed. Source code and pre-trained models will be available at\nhttps://github.com/lyuwenyu/RT-DETR.\n", "link": "http://arxiv.org/abs/2407.17140v1", "date": "2024-07-24", "relevancy": 2.1082, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5316}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.524}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5232}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RT-DETRv2%3A%20Improved%20Baseline%20with%20Bag-of-Freebies%20for%20Real-Time%0A%20%20Detection%20Transformer&body=Title%3A%20RT-DETRv2%3A%20Improved%20Baseline%20with%20Bag-of-Freebies%20for%20Real-Time%0A%20%20Detection%20Transformer%0AAuthor%3A%20Wenyu%20Lv%20and%20Yian%20Zhao%20and%20Qinyao%20Chang%20and%20Kui%20Huang%20and%20Guanzhong%20Wang%20and%20Yi%20Liu%0AAbstract%3A%20%20%20In%20this%20report%2C%20we%20present%20RT-DETRv2%2C%20an%20improved%20Real-Time%20DEtection%0ATRansformer%20%28RT-DETR%29.%20RT-DETRv2%20builds%20upon%20the%20previous%20state-of-the-art%0Areal-time%20detector%2C%20RT-DETR%2C%20and%20opens%20up%20a%20set%20of%20bag-of-freebies%20for%0Aflexibility%20and%20practicality%2C%20as%20well%20as%20optimizing%20the%20training%20strategy%20to%0Aachieve%20enhanced%20performance.%20To%20improve%20the%20flexibility%2C%20we%20suggest%20setting%20a%0Adistinct%20number%20of%20sampling%20points%20for%20features%20at%20different%20scales%20in%20the%0Adeformable%20attention%20to%20achieve%20selective%20multi-scale%20feature%20extraction%20by%20the%0Adecoder.%20To%20enhance%20practicality%2C%20we%20propose%20an%20optional%20discrete%20sampling%0Aoperator%20to%20replace%20the%20grid_sample%20operator%20that%20is%20specific%20to%20RT-DETR%0Acompared%20to%20YOLOs.%20This%20removes%20the%20deployment%20constraints%20typically%20associated%0Awith%20DETRs.%20For%20the%20training%20strategy%2C%20we%20propose%20dynamic%20data%20augmentation%20and%0Ascale-adaptive%20hyperparameters%20customization%20to%20improve%20performance%20without%0Aloss%20of%20speed.%20Source%20code%20and%20pre-trained%20models%20will%20be%20available%20at%0Ahttps%3A//github.com/lyuwenyu/RT-DETR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17140v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRT-DETRv2%253A%2520Improved%2520Baseline%2520with%2520Bag-of-Freebies%2520for%2520Real-Time%250A%2520%2520Detection%2520Transformer%26entry.906535625%3DWenyu%2520Lv%2520and%2520Yian%2520Zhao%2520and%2520Qinyao%2520Chang%2520and%2520Kui%2520Huang%2520and%2520Guanzhong%2520Wang%2520and%2520Yi%2520Liu%26entry.1292438233%3D%2520%2520In%2520this%2520report%252C%2520we%2520present%2520RT-DETRv2%252C%2520an%2520improved%2520Real-Time%2520DEtection%250ATRansformer%2520%2528RT-DETR%2529.%2520RT-DETRv2%2520builds%2520upon%2520the%2520previous%2520state-of-the-art%250Areal-time%2520detector%252C%2520RT-DETR%252C%2520and%2520opens%2520up%2520a%2520set%2520of%2520bag-of-freebies%2520for%250Aflexibility%2520and%2520practicality%252C%2520as%2520well%2520as%2520optimizing%2520the%2520training%2520strategy%2520to%250Aachieve%2520enhanced%2520performance.%2520To%2520improve%2520the%2520flexibility%252C%2520we%2520suggest%2520setting%2520a%250Adistinct%2520number%2520of%2520sampling%2520points%2520for%2520features%2520at%2520different%2520scales%2520in%2520the%250Adeformable%2520attention%2520to%2520achieve%2520selective%2520multi-scale%2520feature%2520extraction%2520by%2520the%250Adecoder.%2520To%2520enhance%2520practicality%252C%2520we%2520propose%2520an%2520optional%2520discrete%2520sampling%250Aoperator%2520to%2520replace%2520the%2520grid_sample%2520operator%2520that%2520is%2520specific%2520to%2520RT-DETR%250Acompared%2520to%2520YOLOs.%2520This%2520removes%2520the%2520deployment%2520constraints%2520typically%2520associated%250Awith%2520DETRs.%2520For%2520the%2520training%2520strategy%252C%2520we%2520propose%2520dynamic%2520data%2520augmentation%2520and%250Ascale-adaptive%2520hyperparameters%2520customization%2520to%2520improve%2520performance%2520without%250Aloss%2520of%2520speed.%2520Source%2520code%2520and%2520pre-trained%2520models%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/lyuwenyu/RT-DETR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17140v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RT-DETRv2%3A%20Improved%20Baseline%20with%20Bag-of-Freebies%20for%20Real-Time%0A%20%20Detection%20Transformer&entry.906535625=Wenyu%20Lv%20and%20Yian%20Zhao%20and%20Qinyao%20Chang%20and%20Kui%20Huang%20and%20Guanzhong%20Wang%20and%20Yi%20Liu&entry.1292438233=%20%20In%20this%20report%2C%20we%20present%20RT-DETRv2%2C%20an%20improved%20Real-Time%20DEtection%0ATRansformer%20%28RT-DETR%29.%20RT-DETRv2%20builds%20upon%20the%20previous%20state-of-the-art%0Areal-time%20detector%2C%20RT-DETR%2C%20and%20opens%20up%20a%20set%20of%20bag-of-freebies%20for%0Aflexibility%20and%20practicality%2C%20as%20well%20as%20optimizing%20the%20training%20strategy%20to%0Aachieve%20enhanced%20performance.%20To%20improve%20the%20flexibility%2C%20we%20suggest%20setting%20a%0Adistinct%20number%20of%20sampling%20points%20for%20features%20at%20different%20scales%20in%20the%0Adeformable%20attention%20to%20achieve%20selective%20multi-scale%20feature%20extraction%20by%20the%0Adecoder.%20To%20enhance%20practicality%2C%20we%20propose%20an%20optional%20discrete%20sampling%0Aoperator%20to%20replace%20the%20grid_sample%20operator%20that%20is%20specific%20to%20RT-DETR%0Acompared%20to%20YOLOs.%20This%20removes%20the%20deployment%20constraints%20typically%20associated%0Awith%20DETRs.%20For%20the%20training%20strategy%2C%20we%20propose%20dynamic%20data%20augmentation%20and%0Ascale-adaptive%20hyperparameters%20customization%20to%20improve%20performance%20without%0Aloss%20of%20speed.%20Source%20code%20and%20pre-trained%20models%20will%20be%20available%20at%0Ahttps%3A//github.com/lyuwenyu/RT-DETR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17140v1&entry.124074799=Read"},
{"title": "MMRA: A Benchmark for Multi-granularity Multi-image Relational\n  Association", "author": "Siwei Wu and Kang Zhu and Yu Bai and Yiming Liang and Yizhi Li and Haoning Wu and Jiaheng Liu and Ruibo Liu and Xingwei Qu and Xuxin Cheng and Ge Zhang and Wenhao Huang and Chenghua Lin", "abstract": "  Given the remarkable success that large visual language models (LVLMs) have\nachieved in image perception tasks, the endeavor to make LVMLs perceive the\nworld like humans is drawing increasing attention. Current multi-modal\nbenchmarks mainly focus on the objective fact or certain topic related\npotential knowledge within a image, but overlook the associative relations\nbetween multiple images. Therefore, we define a multi-image relation\nassociation task, and meticulously curate \\textbf{MMRA} benchmark, a\n\\textbf{M}ulti-granularity \\textbf{M}ulti-image \\textbf{R}elational\n\\textbf{A}ssociation benchmark, consisted of \\textbf{1026} samples. In order to\nsystematically and comprehensively evaluate mainstream LVLMs, we establish an\nassociational relation system among images that contain \\textbf{11 subtasks}\n(e.g, UsageSimilarity, SubEvent, etc.) at two granularity levels (i.e.,\n\"\\textbf{image}\" and \"\\textbf{entity}\") according to the relations in\nConceptNet. Our experiments demonstrate that, on our MMRA benchmark, current\nmainstream LVLMs all have their own advantages and disadvantages across\ndifferent subtasks. It is worth noting that, at the entity level, the\nperformance of all models is worse than that of them at the image level,\nindicating that the fine-grained multi-image perception task is still\nchallenging for LVLMs. The tasks related to spatial perception are relatively\ndifficult for LVLMs to handle. Furthermore, we find that LVMLs exhibit a good\nability to perceive image details, and the key to enhancing their multi-image\nassociation capability is to strengthen the reasoning ability of their language\nmodel component. All our codes and data are released at\nhtt\\url{https://github.com/Wusiwei0410/MMRA}.\n", "link": "http://arxiv.org/abs/2407.17379v1", "date": "2024-07-24", "relevancy": 2.0973, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5521}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.507}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMRA%3A%20A%20Benchmark%20for%20Multi-granularity%20Multi-image%20Relational%0A%20%20Association&body=Title%3A%20MMRA%3A%20A%20Benchmark%20for%20Multi-granularity%20Multi-image%20Relational%0A%20%20Association%0AAuthor%3A%20Siwei%20Wu%20and%20Kang%20Zhu%20and%20Yu%20Bai%20and%20Yiming%20Liang%20and%20Yizhi%20Li%20and%20Haoning%20Wu%20and%20Jiaheng%20Liu%20and%20Ruibo%20Liu%20and%20Xingwei%20Qu%20and%20Xuxin%20Cheng%20and%20Ge%20Zhang%20and%20Wenhao%20Huang%20and%20Chenghua%20Lin%0AAbstract%3A%20%20%20Given%20the%20remarkable%20success%20that%20large%20visual%20language%20models%20%28LVLMs%29%20have%0Aachieved%20in%20image%20perception%20tasks%2C%20the%20endeavor%20to%20make%20LVMLs%20perceive%20the%0Aworld%20like%20humans%20is%20drawing%20increasing%20attention.%20Current%20multi-modal%0Abenchmarks%20mainly%20focus%20on%20the%20objective%20fact%20or%20certain%20topic%20related%0Apotential%20knowledge%20within%20a%20image%2C%20but%20overlook%20the%20associative%20relations%0Abetween%20multiple%20images.%20Therefore%2C%20we%20define%20a%20multi-image%20relation%0Aassociation%20task%2C%20and%20meticulously%20curate%20%5Ctextbf%7BMMRA%7D%20benchmark%2C%20a%0A%5Ctextbf%7BM%7Dulti-granularity%20%5Ctextbf%7BM%7Dulti-image%20%5Ctextbf%7BR%7Delational%0A%5Ctextbf%7BA%7Dssociation%20benchmark%2C%20consisted%20of%20%5Ctextbf%7B1026%7D%20samples.%20In%20order%20to%0Asystematically%20and%20comprehensively%20evaluate%20mainstream%20LVLMs%2C%20we%20establish%20an%0Aassociational%20relation%20system%20among%20images%20that%20contain%20%5Ctextbf%7B11%20subtasks%7D%0A%28e.g%2C%20UsageSimilarity%2C%20SubEvent%2C%20etc.%29%20at%20two%20granularity%20levels%20%28i.e.%2C%0A%22%5Ctextbf%7Bimage%7D%22%20and%20%22%5Ctextbf%7Bentity%7D%22%29%20according%20to%20the%20relations%20in%0AConceptNet.%20Our%20experiments%20demonstrate%20that%2C%20on%20our%20MMRA%20benchmark%2C%20current%0Amainstream%20LVLMs%20all%20have%20their%20own%20advantages%20and%20disadvantages%20across%0Adifferent%20subtasks.%20It%20is%20worth%20noting%20that%2C%20at%20the%20entity%20level%2C%20the%0Aperformance%20of%20all%20models%20is%20worse%20than%20that%20of%20them%20at%20the%20image%20level%2C%0Aindicating%20that%20the%20fine-grained%20multi-image%20perception%20task%20is%20still%0Achallenging%20for%20LVLMs.%20The%20tasks%20related%20to%20spatial%20perception%20are%20relatively%0Adifficult%20for%20LVLMs%20to%20handle.%20Furthermore%2C%20we%20find%20that%20LVMLs%20exhibit%20a%20good%0Aability%20to%20perceive%20image%20details%2C%20and%20the%20key%20to%20enhancing%20their%20multi-image%0Aassociation%20capability%20is%20to%20strengthen%20the%20reasoning%20ability%20of%20their%20language%0Amodel%20component.%20All%20our%20codes%20and%20data%20are%20released%20at%0Ahtt%5Curl%7Bhttps%3A//github.com/Wusiwei0410/MMRA%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17379v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMRA%253A%2520A%2520Benchmark%2520for%2520Multi-granularity%2520Multi-image%2520Relational%250A%2520%2520Association%26entry.906535625%3DSiwei%2520Wu%2520and%2520Kang%2520Zhu%2520and%2520Yu%2520Bai%2520and%2520Yiming%2520Liang%2520and%2520Yizhi%2520Li%2520and%2520Haoning%2520Wu%2520and%2520Jiaheng%2520Liu%2520and%2520Ruibo%2520Liu%2520and%2520Xingwei%2520Qu%2520and%2520Xuxin%2520Cheng%2520and%2520Ge%2520Zhang%2520and%2520Wenhao%2520Huang%2520and%2520Chenghua%2520Lin%26entry.1292438233%3D%2520%2520Given%2520the%2520remarkable%2520success%2520that%2520large%2520visual%2520language%2520models%2520%2528LVLMs%2529%2520have%250Aachieved%2520in%2520image%2520perception%2520tasks%252C%2520the%2520endeavor%2520to%2520make%2520LVMLs%2520perceive%2520the%250Aworld%2520like%2520humans%2520is%2520drawing%2520increasing%2520attention.%2520Current%2520multi-modal%250Abenchmarks%2520mainly%2520focus%2520on%2520the%2520objective%2520fact%2520or%2520certain%2520topic%2520related%250Apotential%2520knowledge%2520within%2520a%2520image%252C%2520but%2520overlook%2520the%2520associative%2520relations%250Abetween%2520multiple%2520images.%2520Therefore%252C%2520we%2520define%2520a%2520multi-image%2520relation%250Aassociation%2520task%252C%2520and%2520meticulously%2520curate%2520%255Ctextbf%257BMMRA%257D%2520benchmark%252C%2520a%250A%255Ctextbf%257BM%257Dulti-granularity%2520%255Ctextbf%257BM%257Dulti-image%2520%255Ctextbf%257BR%257Delational%250A%255Ctextbf%257BA%257Dssociation%2520benchmark%252C%2520consisted%2520of%2520%255Ctextbf%257B1026%257D%2520samples.%2520In%2520order%2520to%250Asystematically%2520and%2520comprehensively%2520evaluate%2520mainstream%2520LVLMs%252C%2520we%2520establish%2520an%250Aassociational%2520relation%2520system%2520among%2520images%2520that%2520contain%2520%255Ctextbf%257B11%2520subtasks%257D%250A%2528e.g%252C%2520UsageSimilarity%252C%2520SubEvent%252C%2520etc.%2529%2520at%2520two%2520granularity%2520levels%2520%2528i.e.%252C%250A%2522%255Ctextbf%257Bimage%257D%2522%2520and%2520%2522%255Ctextbf%257Bentity%257D%2522%2529%2520according%2520to%2520the%2520relations%2520in%250AConceptNet.%2520Our%2520experiments%2520demonstrate%2520that%252C%2520on%2520our%2520MMRA%2520benchmark%252C%2520current%250Amainstream%2520LVLMs%2520all%2520have%2520their%2520own%2520advantages%2520and%2520disadvantages%2520across%250Adifferent%2520subtasks.%2520It%2520is%2520worth%2520noting%2520that%252C%2520at%2520the%2520entity%2520level%252C%2520the%250Aperformance%2520of%2520all%2520models%2520is%2520worse%2520than%2520that%2520of%2520them%2520at%2520the%2520image%2520level%252C%250Aindicating%2520that%2520the%2520fine-grained%2520multi-image%2520perception%2520task%2520is%2520still%250Achallenging%2520for%2520LVLMs.%2520The%2520tasks%2520related%2520to%2520spatial%2520perception%2520are%2520relatively%250Adifficult%2520for%2520LVLMs%2520to%2520handle.%2520Furthermore%252C%2520we%2520find%2520that%2520LVMLs%2520exhibit%2520a%2520good%250Aability%2520to%2520perceive%2520image%2520details%252C%2520and%2520the%2520key%2520to%2520enhancing%2520their%2520multi-image%250Aassociation%2520capability%2520is%2520to%2520strengthen%2520the%2520reasoning%2520ability%2520of%2520their%2520language%250Amodel%2520component.%2520All%2520our%2520codes%2520and%2520data%2520are%2520released%2520at%250Ahtt%255Curl%257Bhttps%253A//github.com/Wusiwei0410/MMRA%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17379v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMRA%3A%20A%20Benchmark%20for%20Multi-granularity%20Multi-image%20Relational%0A%20%20Association&entry.906535625=Siwei%20Wu%20and%20Kang%20Zhu%20and%20Yu%20Bai%20and%20Yiming%20Liang%20and%20Yizhi%20Li%20and%20Haoning%20Wu%20and%20Jiaheng%20Liu%20and%20Ruibo%20Liu%20and%20Xingwei%20Qu%20and%20Xuxin%20Cheng%20and%20Ge%20Zhang%20and%20Wenhao%20Huang%20and%20Chenghua%20Lin&entry.1292438233=%20%20Given%20the%20remarkable%20success%20that%20large%20visual%20language%20models%20%28LVLMs%29%20have%0Aachieved%20in%20image%20perception%20tasks%2C%20the%20endeavor%20to%20make%20LVMLs%20perceive%20the%0Aworld%20like%20humans%20is%20drawing%20increasing%20attention.%20Current%20multi-modal%0Abenchmarks%20mainly%20focus%20on%20the%20objective%20fact%20or%20certain%20topic%20related%0Apotential%20knowledge%20within%20a%20image%2C%20but%20overlook%20the%20associative%20relations%0Abetween%20multiple%20images.%20Therefore%2C%20we%20define%20a%20multi-image%20relation%0Aassociation%20task%2C%20and%20meticulously%20curate%20%5Ctextbf%7BMMRA%7D%20benchmark%2C%20a%0A%5Ctextbf%7BM%7Dulti-granularity%20%5Ctextbf%7BM%7Dulti-image%20%5Ctextbf%7BR%7Delational%0A%5Ctextbf%7BA%7Dssociation%20benchmark%2C%20consisted%20of%20%5Ctextbf%7B1026%7D%20samples.%20In%20order%20to%0Asystematically%20and%20comprehensively%20evaluate%20mainstream%20LVLMs%2C%20we%20establish%20an%0Aassociational%20relation%20system%20among%20images%20that%20contain%20%5Ctextbf%7B11%20subtasks%7D%0A%28e.g%2C%20UsageSimilarity%2C%20SubEvent%2C%20etc.%29%20at%20two%20granularity%20levels%20%28i.e.%2C%0A%22%5Ctextbf%7Bimage%7D%22%20and%20%22%5Ctextbf%7Bentity%7D%22%29%20according%20to%20the%20relations%20in%0AConceptNet.%20Our%20experiments%20demonstrate%20that%2C%20on%20our%20MMRA%20benchmark%2C%20current%0Amainstream%20LVLMs%20all%20have%20their%20own%20advantages%20and%20disadvantages%20across%0Adifferent%20subtasks.%20It%20is%20worth%20noting%20that%2C%20at%20the%20entity%20level%2C%20the%0Aperformance%20of%20all%20models%20is%20worse%20than%20that%20of%20them%20at%20the%20image%20level%2C%0Aindicating%20that%20the%20fine-grained%20multi-image%20perception%20task%20is%20still%0Achallenging%20for%20LVLMs.%20The%20tasks%20related%20to%20spatial%20perception%20are%20relatively%0Adifficult%20for%20LVLMs%20to%20handle.%20Furthermore%2C%20we%20find%20that%20LVMLs%20exhibit%20a%20good%0Aability%20to%20perceive%20image%20details%2C%20and%20the%20key%20to%20enhancing%20their%20multi-image%0Aassociation%20capability%20is%20to%20strengthen%20the%20reasoning%20ability%20of%20their%20language%0Amodel%20component.%20All%20our%20codes%20and%20data%20are%20released%20at%0Ahtt%5Curl%7Bhttps%3A//github.com/Wusiwei0410/MMRA%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17379v1&entry.124074799=Read"},
{"title": "Domain Generalized Recaptured Screen Image Identification Using SWIN\n  Transformer", "author": "Preeti Mehta and Aman Sagar and Suchi Kumari", "abstract": "  An increasing number of classification approaches have been developed to\naddress the issue of image rebroadcast and recapturing, a standard attack\nstrategy in insurance frauds, face spoofing, and video piracy. However, most of\nthem neglected scale variations and domain generalization scenarios, performing\npoorly in instances involving domain shifts, typically made worse by\ninter-domain and cross-domain scale variances. To overcome these issues, we\npropose a cascaded data augmentation and SWIN transformer domain generalization\nframework (DAST-DG) in the current research work Initially, we examine the\ndisparity in dataset representation. A feature generator is trained to make\nauthentic images from various domains indistinguishable. This process is then\napplied to recaptured images, creating a dual adversarial learning setup.\nExtensive experiments demonstrate that our approach is practical and surpasses\nstate-of-the-art methods across different databases. Our model achieves an\naccuracy of approximately 82\\% with a precision of 95\\% on high-variance\ndatasets.\n", "link": "http://arxiv.org/abs/2407.17170v1", "date": "2024-07-24", "relevancy": 2.0872, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5471}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5282}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain%20Generalized%20Recaptured%20Screen%20Image%20Identification%20Using%20SWIN%0A%20%20Transformer&body=Title%3A%20Domain%20Generalized%20Recaptured%20Screen%20Image%20Identification%20Using%20SWIN%0A%20%20Transformer%0AAuthor%3A%20Preeti%20Mehta%20and%20Aman%20Sagar%20and%20Suchi%20Kumari%0AAbstract%3A%20%20%20An%20increasing%20number%20of%20classification%20approaches%20have%20been%20developed%20to%0Aaddress%20the%20issue%20of%20image%20rebroadcast%20and%20recapturing%2C%20a%20standard%20attack%0Astrategy%20in%20insurance%20frauds%2C%20face%20spoofing%2C%20and%20video%20piracy.%20However%2C%20most%20of%0Athem%20neglected%20scale%20variations%20and%20domain%20generalization%20scenarios%2C%20performing%0Apoorly%20in%20instances%20involving%20domain%20shifts%2C%20typically%20made%20worse%20by%0Ainter-domain%20and%20cross-domain%20scale%20variances.%20To%20overcome%20these%20issues%2C%20we%0Apropose%20a%20cascaded%20data%20augmentation%20and%20SWIN%20transformer%20domain%20generalization%0Aframework%20%28DAST-DG%29%20in%20the%20current%20research%20work%20Initially%2C%20we%20examine%20the%0Adisparity%20in%20dataset%20representation.%20A%20feature%20generator%20is%20trained%20to%20make%0Aauthentic%20images%20from%20various%20domains%20indistinguishable.%20This%20process%20is%20then%0Aapplied%20to%20recaptured%20images%2C%20creating%20a%20dual%20adversarial%20learning%20setup.%0AExtensive%20experiments%20demonstrate%20that%20our%20approach%20is%20practical%20and%20surpasses%0Astate-of-the-art%20methods%20across%20different%20databases.%20Our%20model%20achieves%20an%0Aaccuracy%20of%20approximately%2082%5C%25%20with%20a%20precision%20of%2095%5C%25%20on%20high-variance%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17170v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain%2520Generalized%2520Recaptured%2520Screen%2520Image%2520Identification%2520Using%2520SWIN%250A%2520%2520Transformer%26entry.906535625%3DPreeti%2520Mehta%2520and%2520Aman%2520Sagar%2520and%2520Suchi%2520Kumari%26entry.1292438233%3D%2520%2520An%2520increasing%2520number%2520of%2520classification%2520approaches%2520have%2520been%2520developed%2520to%250Aaddress%2520the%2520issue%2520of%2520image%2520rebroadcast%2520and%2520recapturing%252C%2520a%2520standard%2520attack%250Astrategy%2520in%2520insurance%2520frauds%252C%2520face%2520spoofing%252C%2520and%2520video%2520piracy.%2520However%252C%2520most%2520of%250Athem%2520neglected%2520scale%2520variations%2520and%2520domain%2520generalization%2520scenarios%252C%2520performing%250Apoorly%2520in%2520instances%2520involving%2520domain%2520shifts%252C%2520typically%2520made%2520worse%2520by%250Ainter-domain%2520and%2520cross-domain%2520scale%2520variances.%2520To%2520overcome%2520these%2520issues%252C%2520we%250Apropose%2520a%2520cascaded%2520data%2520augmentation%2520and%2520SWIN%2520transformer%2520domain%2520generalization%250Aframework%2520%2528DAST-DG%2529%2520in%2520the%2520current%2520research%2520work%2520Initially%252C%2520we%2520examine%2520the%250Adisparity%2520in%2520dataset%2520representation.%2520A%2520feature%2520generator%2520is%2520trained%2520to%2520make%250Aauthentic%2520images%2520from%2520various%2520domains%2520indistinguishable.%2520This%2520process%2520is%2520then%250Aapplied%2520to%2520recaptured%2520images%252C%2520creating%2520a%2520dual%2520adversarial%2520learning%2520setup.%250AExtensive%2520experiments%2520demonstrate%2520that%2520our%2520approach%2520is%2520practical%2520and%2520surpasses%250Astate-of-the-art%2520methods%2520across%2520different%2520databases.%2520Our%2520model%2520achieves%2520an%250Aaccuracy%2520of%2520approximately%252082%255C%2525%2520with%2520a%2520precision%2520of%252095%255C%2525%2520on%2520high-variance%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17170v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain%20Generalized%20Recaptured%20Screen%20Image%20Identification%20Using%20SWIN%0A%20%20Transformer&entry.906535625=Preeti%20Mehta%20and%20Aman%20Sagar%20and%20Suchi%20Kumari&entry.1292438233=%20%20An%20increasing%20number%20of%20classification%20approaches%20have%20been%20developed%20to%0Aaddress%20the%20issue%20of%20image%20rebroadcast%20and%20recapturing%2C%20a%20standard%20attack%0Astrategy%20in%20insurance%20frauds%2C%20face%20spoofing%2C%20and%20video%20piracy.%20However%2C%20most%20of%0Athem%20neglected%20scale%20variations%20and%20domain%20generalization%20scenarios%2C%20performing%0Apoorly%20in%20instances%20involving%20domain%20shifts%2C%20typically%20made%20worse%20by%0Ainter-domain%20and%20cross-domain%20scale%20variances.%20To%20overcome%20these%20issues%2C%20we%0Apropose%20a%20cascaded%20data%20augmentation%20and%20SWIN%20transformer%20domain%20generalization%0Aframework%20%28DAST-DG%29%20in%20the%20current%20research%20work%20Initially%2C%20we%20examine%20the%0Adisparity%20in%20dataset%20representation.%20A%20feature%20generator%20is%20trained%20to%20make%0Aauthentic%20images%20from%20various%20domains%20indistinguishable.%20This%20process%20is%20then%0Aapplied%20to%20recaptured%20images%2C%20creating%20a%20dual%20adversarial%20learning%20setup.%0AExtensive%20experiments%20demonstrate%20that%20our%20approach%20is%20practical%20and%20surpasses%0Astate-of-the-art%20methods%20across%20different%20databases.%20Our%20model%20achieves%20an%0Aaccuracy%20of%20approximately%2082%5C%25%20with%20a%20precision%20of%2095%5C%25%20on%20high-variance%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17170v1&entry.124074799=Read"},
{"title": "Embedding-Free Transformer with Inference Spatial Reduction for\n  Efficient Semantic Segmentation", "author": "Hyunwoo Yu and Yubin Cho and Beoungwoo Kang and Seunghun Moon and Kyeongbo Kong and Suk-Ju Kang", "abstract": "  We present an Encoder-Decoder Attention Transformer, EDAFormer, which\nconsists of the Embedding-Free Transformer (EFT) encoder and the all-attention\ndecoder leveraging our Embedding-Free Attention (EFA) structure. The proposed\nEFA is a novel global context modeling mechanism that focuses on functioning\nthe global non-linearity, not the specific roles of the query, key and value.\nFor the decoder, we explore the optimized structure for considering the\nglobality, which can improve the semantic segmentation performance. In\naddition, we propose a novel Inference Spatial Reduction (ISR) method for the\ncomputational efficiency. Different from the previous spatial reduction\nattention methods, our ISR method further reduces the key-value resolution at\nthe inference phase, which can mitigate the computation-performance trade-off\ngap for the efficient semantic segmentation. Our EDAFormer shows the\nstate-of-the-art performance with the efficient computation compared to the\nexisting transformer-based semantic segmentation models in three public\nbenchmarks, including ADE20K, Cityscapes and COCO-Stuff. Furthermore, our ISR\nmethod reduces the computational cost by up to 61% with minimal mIoU\nperformance degradation on Cityscapes dataset. The code is available at\nhttps://github.com/hyunwoo137/EDAFormer.\n", "link": "http://arxiv.org/abs/2407.17261v1", "date": "2024-07-24", "relevancy": 2.084, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5375}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5164}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embedding-Free%20Transformer%20with%20Inference%20Spatial%20Reduction%20for%0A%20%20Efficient%20Semantic%20Segmentation&body=Title%3A%20Embedding-Free%20Transformer%20with%20Inference%20Spatial%20Reduction%20for%0A%20%20Efficient%20Semantic%20Segmentation%0AAuthor%3A%20Hyunwoo%20Yu%20and%20Yubin%20Cho%20and%20Beoungwoo%20Kang%20and%20Seunghun%20Moon%20and%20Kyeongbo%20Kong%20and%20Suk-Ju%20Kang%0AAbstract%3A%20%20%20We%20present%20an%20Encoder-Decoder%20Attention%20Transformer%2C%20EDAFormer%2C%20which%0Aconsists%20of%20the%20Embedding-Free%20Transformer%20%28EFT%29%20encoder%20and%20the%20all-attention%0Adecoder%20leveraging%20our%20Embedding-Free%20Attention%20%28EFA%29%20structure.%20The%20proposed%0AEFA%20is%20a%20novel%20global%20context%20modeling%20mechanism%20that%20focuses%20on%20functioning%0Athe%20global%20non-linearity%2C%20not%20the%20specific%20roles%20of%20the%20query%2C%20key%20and%20value.%0AFor%20the%20decoder%2C%20we%20explore%20the%20optimized%20structure%20for%20considering%20the%0Aglobality%2C%20which%20can%20improve%20the%20semantic%20segmentation%20performance.%20In%0Aaddition%2C%20we%20propose%20a%20novel%20Inference%20Spatial%20Reduction%20%28ISR%29%20method%20for%20the%0Acomputational%20efficiency.%20Different%20from%20the%20previous%20spatial%20reduction%0Aattention%20methods%2C%20our%20ISR%20method%20further%20reduces%20the%20key-value%20resolution%20at%0Athe%20inference%20phase%2C%20which%20can%20mitigate%20the%20computation-performance%20trade-off%0Agap%20for%20the%20efficient%20semantic%20segmentation.%20Our%20EDAFormer%20shows%20the%0Astate-of-the-art%20performance%20with%20the%20efficient%20computation%20compared%20to%20the%0Aexisting%20transformer-based%20semantic%20segmentation%20models%20in%20three%20public%0Abenchmarks%2C%20including%20ADE20K%2C%20Cityscapes%20and%20COCO-Stuff.%20Furthermore%2C%20our%20ISR%0Amethod%20reduces%20the%20computational%20cost%20by%20up%20to%2061%25%20with%20minimal%20mIoU%0Aperformance%20degradation%20on%20Cityscapes%20dataset.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/hyunwoo137/EDAFormer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17261v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbedding-Free%2520Transformer%2520with%2520Inference%2520Spatial%2520Reduction%2520for%250A%2520%2520Efficient%2520Semantic%2520Segmentation%26entry.906535625%3DHyunwoo%2520Yu%2520and%2520Yubin%2520Cho%2520and%2520Beoungwoo%2520Kang%2520and%2520Seunghun%2520Moon%2520and%2520Kyeongbo%2520Kong%2520and%2520Suk-Ju%2520Kang%26entry.1292438233%3D%2520%2520We%2520present%2520an%2520Encoder-Decoder%2520Attention%2520Transformer%252C%2520EDAFormer%252C%2520which%250Aconsists%2520of%2520the%2520Embedding-Free%2520Transformer%2520%2528EFT%2529%2520encoder%2520and%2520the%2520all-attention%250Adecoder%2520leveraging%2520our%2520Embedding-Free%2520Attention%2520%2528EFA%2529%2520structure.%2520The%2520proposed%250AEFA%2520is%2520a%2520novel%2520global%2520context%2520modeling%2520mechanism%2520that%2520focuses%2520on%2520functioning%250Athe%2520global%2520non-linearity%252C%2520not%2520the%2520specific%2520roles%2520of%2520the%2520query%252C%2520key%2520and%2520value.%250AFor%2520the%2520decoder%252C%2520we%2520explore%2520the%2520optimized%2520structure%2520for%2520considering%2520the%250Aglobality%252C%2520which%2520can%2520improve%2520the%2520semantic%2520segmentation%2520performance.%2520In%250Aaddition%252C%2520we%2520propose%2520a%2520novel%2520Inference%2520Spatial%2520Reduction%2520%2528ISR%2529%2520method%2520for%2520the%250Acomputational%2520efficiency.%2520Different%2520from%2520the%2520previous%2520spatial%2520reduction%250Aattention%2520methods%252C%2520our%2520ISR%2520method%2520further%2520reduces%2520the%2520key-value%2520resolution%2520at%250Athe%2520inference%2520phase%252C%2520which%2520can%2520mitigate%2520the%2520computation-performance%2520trade-off%250Agap%2520for%2520the%2520efficient%2520semantic%2520segmentation.%2520Our%2520EDAFormer%2520shows%2520the%250Astate-of-the-art%2520performance%2520with%2520the%2520efficient%2520computation%2520compared%2520to%2520the%250Aexisting%2520transformer-based%2520semantic%2520segmentation%2520models%2520in%2520three%2520public%250Abenchmarks%252C%2520including%2520ADE20K%252C%2520Cityscapes%2520and%2520COCO-Stuff.%2520Furthermore%252C%2520our%2520ISR%250Amethod%2520reduces%2520the%2520computational%2520cost%2520by%2520up%2520to%252061%2525%2520with%2520minimal%2520mIoU%250Aperformance%2520degradation%2520on%2520Cityscapes%2520dataset.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/hyunwoo137/EDAFormer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17261v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embedding-Free%20Transformer%20with%20Inference%20Spatial%20Reduction%20for%0A%20%20Efficient%20Semantic%20Segmentation&entry.906535625=Hyunwoo%20Yu%20and%20Yubin%20Cho%20and%20Beoungwoo%20Kang%20and%20Seunghun%20Moon%20and%20Kyeongbo%20Kong%20and%20Suk-Ju%20Kang&entry.1292438233=%20%20We%20present%20an%20Encoder-Decoder%20Attention%20Transformer%2C%20EDAFormer%2C%20which%0Aconsists%20of%20the%20Embedding-Free%20Transformer%20%28EFT%29%20encoder%20and%20the%20all-attention%0Adecoder%20leveraging%20our%20Embedding-Free%20Attention%20%28EFA%29%20structure.%20The%20proposed%0AEFA%20is%20a%20novel%20global%20context%20modeling%20mechanism%20that%20focuses%20on%20functioning%0Athe%20global%20non-linearity%2C%20not%20the%20specific%20roles%20of%20the%20query%2C%20key%20and%20value.%0AFor%20the%20decoder%2C%20we%20explore%20the%20optimized%20structure%20for%20considering%20the%0Aglobality%2C%20which%20can%20improve%20the%20semantic%20segmentation%20performance.%20In%0Aaddition%2C%20we%20propose%20a%20novel%20Inference%20Spatial%20Reduction%20%28ISR%29%20method%20for%20the%0Acomputational%20efficiency.%20Different%20from%20the%20previous%20spatial%20reduction%0Aattention%20methods%2C%20our%20ISR%20method%20further%20reduces%20the%20key-value%20resolution%20at%0Athe%20inference%20phase%2C%20which%20can%20mitigate%20the%20computation-performance%20trade-off%0Agap%20for%20the%20efficient%20semantic%20segmentation.%20Our%20EDAFormer%20shows%20the%0Astate-of-the-art%20performance%20with%20the%20efficient%20computation%20compared%20to%20the%0Aexisting%20transformer-based%20semantic%20segmentation%20models%20in%20three%20public%0Abenchmarks%2C%20including%20ADE20K%2C%20Cityscapes%20and%20COCO-Stuff.%20Furthermore%2C%20our%20ISR%0Amethod%20reduces%20the%20computational%20cost%20by%20up%20to%2061%25%20with%20minimal%20mIoU%0Aperformance%20degradation%20on%20Cityscapes%20dataset.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/hyunwoo137/EDAFormer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17261v1&entry.124074799=Read"},
{"title": "Large Language Models as Topological Structure Enhancers for\n  Text-Attributed Graphs", "author": "Shengyin Sun and Yuxiang Ren and Chen Ma and Xuecang Zhang", "abstract": "  The latest advancements in large language models (LLMs) have revolutionized\nthe field of natural language processing (NLP). Inspired by the success of LLMs\nin NLP tasks, some recent work has begun investigating the potential of\napplying LLMs in graph learning tasks. However, most of the existing work\nfocuses on utilizing LLMs as powerful node feature augmenters, leaving\nemploying LLMs to enhance graph topological structures an understudied problem.\nIn this work, we explore how to leverage the information retrieval and text\ngeneration capabilities of LLMs to refine/enhance the topological structure of\ntext-attributed graphs (TAGs) under the node classification setting. First, we\npropose using LLMs to help remove unreliable edges and add reliable ones in the\nTAG. Specifically, we first let the LLM output the semantic similarity between\nnode attributes through delicate prompt designs, and then perform edge deletion\nand edge addition based on the similarity. Second, we propose using\npseudo-labels generated by the LLM to improve graph topology, that is, we\nintroduce the pseudo-label propagation as a regularization to guide the graph\nneural network (GNN) in learning proper edge weights. Finally, we incorporate\nthe two aforementioned LLM-based methods for graph topological refinement into\nthe process of GNN training, and perform extensive experiments on four\nreal-world datasets. The experimental results demonstrate the effectiveness of\nLLM-based graph topology refinement (achieving a 0.15%--2.47% performance gain\non public benchmarks).\n", "link": "http://arxiv.org/abs/2311.14324v2", "date": "2024-07-24", "relevancy": 2.0625, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5226}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5178}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20as%20Topological%20Structure%20Enhancers%20for%0A%20%20Text-Attributed%20Graphs&body=Title%3A%20Large%20Language%20Models%20as%20Topological%20Structure%20Enhancers%20for%0A%20%20Text-Attributed%20Graphs%0AAuthor%3A%20Shengyin%20Sun%20and%20Yuxiang%20Ren%20and%20Chen%20Ma%20and%20Xuecang%20Zhang%0AAbstract%3A%20%20%20The%20latest%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20revolutionized%0Athe%20field%20of%20natural%20language%20processing%20%28NLP%29.%20Inspired%20by%20the%20success%20of%20LLMs%0Ain%20NLP%20tasks%2C%20some%20recent%20work%20has%20begun%20investigating%20the%20potential%20of%0Aapplying%20LLMs%20in%20graph%20learning%20tasks.%20However%2C%20most%20of%20the%20existing%20work%0Afocuses%20on%20utilizing%20LLMs%20as%20powerful%20node%20feature%20augmenters%2C%20leaving%0Aemploying%20LLMs%20to%20enhance%20graph%20topological%20structures%20an%20understudied%20problem.%0AIn%20this%20work%2C%20we%20explore%20how%20to%20leverage%20the%20information%20retrieval%20and%20text%0Ageneration%20capabilities%20of%20LLMs%20to%20refine/enhance%20the%20topological%20structure%20of%0Atext-attributed%20graphs%20%28TAGs%29%20under%20the%20node%20classification%20setting.%20First%2C%20we%0Apropose%20using%20LLMs%20to%20help%20remove%20unreliable%20edges%20and%20add%20reliable%20ones%20in%20the%0ATAG.%20Specifically%2C%20we%20first%20let%20the%20LLM%20output%20the%20semantic%20similarity%20between%0Anode%20attributes%20through%20delicate%20prompt%20designs%2C%20and%20then%20perform%20edge%20deletion%0Aand%20edge%20addition%20based%20on%20the%20similarity.%20Second%2C%20we%20propose%20using%0Apseudo-labels%20generated%20by%20the%20LLM%20to%20improve%20graph%20topology%2C%20that%20is%2C%20we%0Aintroduce%20the%20pseudo-label%20propagation%20as%20a%20regularization%20to%20guide%20the%20graph%0Aneural%20network%20%28GNN%29%20in%20learning%20proper%20edge%20weights.%20Finally%2C%20we%20incorporate%0Athe%20two%20aforementioned%20LLM-based%20methods%20for%20graph%20topological%20refinement%20into%0Athe%20process%20of%20GNN%20training%2C%20and%20perform%20extensive%20experiments%20on%20four%0Areal-world%20datasets.%20The%20experimental%20results%20demonstrate%20the%20effectiveness%20of%0ALLM-based%20graph%20topology%20refinement%20%28achieving%20a%200.15%25--2.47%25%20performance%20gain%0Aon%20public%20benchmarks%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.14324v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520as%2520Topological%2520Structure%2520Enhancers%2520for%250A%2520%2520Text-Attributed%2520Graphs%26entry.906535625%3DShengyin%2520Sun%2520and%2520Yuxiang%2520Ren%2520and%2520Chen%2520Ma%2520and%2520Xuecang%2520Zhang%26entry.1292438233%3D%2520%2520The%2520latest%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520revolutionized%250Athe%2520field%2520of%2520natural%2520language%2520processing%2520%2528NLP%2529.%2520Inspired%2520by%2520the%2520success%2520of%2520LLMs%250Ain%2520NLP%2520tasks%252C%2520some%2520recent%2520work%2520has%2520begun%2520investigating%2520the%2520potential%2520of%250Aapplying%2520LLMs%2520in%2520graph%2520learning%2520tasks.%2520However%252C%2520most%2520of%2520the%2520existing%2520work%250Afocuses%2520on%2520utilizing%2520LLMs%2520as%2520powerful%2520node%2520feature%2520augmenters%252C%2520leaving%250Aemploying%2520LLMs%2520to%2520enhance%2520graph%2520topological%2520structures%2520an%2520understudied%2520problem.%250AIn%2520this%2520work%252C%2520we%2520explore%2520how%2520to%2520leverage%2520the%2520information%2520retrieval%2520and%2520text%250Ageneration%2520capabilities%2520of%2520LLMs%2520to%2520refine/enhance%2520the%2520topological%2520structure%2520of%250Atext-attributed%2520graphs%2520%2528TAGs%2529%2520under%2520the%2520node%2520classification%2520setting.%2520First%252C%2520we%250Apropose%2520using%2520LLMs%2520to%2520help%2520remove%2520unreliable%2520edges%2520and%2520add%2520reliable%2520ones%2520in%2520the%250ATAG.%2520Specifically%252C%2520we%2520first%2520let%2520the%2520LLM%2520output%2520the%2520semantic%2520similarity%2520between%250Anode%2520attributes%2520through%2520delicate%2520prompt%2520designs%252C%2520and%2520then%2520perform%2520edge%2520deletion%250Aand%2520edge%2520addition%2520based%2520on%2520the%2520similarity.%2520Second%252C%2520we%2520propose%2520using%250Apseudo-labels%2520generated%2520by%2520the%2520LLM%2520to%2520improve%2520graph%2520topology%252C%2520that%2520is%252C%2520we%250Aintroduce%2520the%2520pseudo-label%2520propagation%2520as%2520a%2520regularization%2520to%2520guide%2520the%2520graph%250Aneural%2520network%2520%2528GNN%2529%2520in%2520learning%2520proper%2520edge%2520weights.%2520Finally%252C%2520we%2520incorporate%250Athe%2520two%2520aforementioned%2520LLM-based%2520methods%2520for%2520graph%2520topological%2520refinement%2520into%250Athe%2520process%2520of%2520GNN%2520training%252C%2520and%2520perform%2520extensive%2520experiments%2520on%2520four%250Areal-world%2520datasets.%2520The%2520experimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%250ALLM-based%2520graph%2520topology%2520refinement%2520%2528achieving%2520a%25200.15%2525--2.47%2525%2520performance%2520gain%250Aon%2520public%2520benchmarks%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.14324v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20as%20Topological%20Structure%20Enhancers%20for%0A%20%20Text-Attributed%20Graphs&entry.906535625=Shengyin%20Sun%20and%20Yuxiang%20Ren%20and%20Chen%20Ma%20and%20Xuecang%20Zhang&entry.1292438233=%20%20The%20latest%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20revolutionized%0Athe%20field%20of%20natural%20language%20processing%20%28NLP%29.%20Inspired%20by%20the%20success%20of%20LLMs%0Ain%20NLP%20tasks%2C%20some%20recent%20work%20has%20begun%20investigating%20the%20potential%20of%0Aapplying%20LLMs%20in%20graph%20learning%20tasks.%20However%2C%20most%20of%20the%20existing%20work%0Afocuses%20on%20utilizing%20LLMs%20as%20powerful%20node%20feature%20augmenters%2C%20leaving%0Aemploying%20LLMs%20to%20enhance%20graph%20topological%20structures%20an%20understudied%20problem.%0AIn%20this%20work%2C%20we%20explore%20how%20to%20leverage%20the%20information%20retrieval%20and%20text%0Ageneration%20capabilities%20of%20LLMs%20to%20refine/enhance%20the%20topological%20structure%20of%0Atext-attributed%20graphs%20%28TAGs%29%20under%20the%20node%20classification%20setting.%20First%2C%20we%0Apropose%20using%20LLMs%20to%20help%20remove%20unreliable%20edges%20and%20add%20reliable%20ones%20in%20the%0ATAG.%20Specifically%2C%20we%20first%20let%20the%20LLM%20output%20the%20semantic%20similarity%20between%0Anode%20attributes%20through%20delicate%20prompt%20designs%2C%20and%20then%20perform%20edge%20deletion%0Aand%20edge%20addition%20based%20on%20the%20similarity.%20Second%2C%20we%20propose%20using%0Apseudo-labels%20generated%20by%20the%20LLM%20to%20improve%20graph%20topology%2C%20that%20is%2C%20we%0Aintroduce%20the%20pseudo-label%20propagation%20as%20a%20regularization%20to%20guide%20the%20graph%0Aneural%20network%20%28GNN%29%20in%20learning%20proper%20edge%20weights.%20Finally%2C%20we%20incorporate%0Athe%20two%20aforementioned%20LLM-based%20methods%20for%20graph%20topological%20refinement%20into%0Athe%20process%20of%20GNN%20training%2C%20and%20perform%20extensive%20experiments%20on%20four%0Areal-world%20datasets.%20The%20experimental%20results%20demonstrate%20the%20effectiveness%20of%0ALLM-based%20graph%20topology%20refinement%20%28achieving%20a%200.15%25--2.47%25%20performance%20gain%0Aon%20public%20benchmarks%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.14324v2&entry.124074799=Read"},
{"title": "Gradient-based inference of abstract task representations for\n  generalization in neural networks", "author": "Ali Hummos and Felipe del R\u00edo and Brabeeba Mien Wang and Julio Hurtado and Cristian B. Calderon and Guangyu Robert Yang", "abstract": "  Humans and many animals show remarkably adaptive behavior and can respond\ndifferently to the same input depending on their internal goals. The brain not\nonly represents the intermediate abstractions needed to perform a computation\nbut also actively maintains a representation of the computation itself (task\nabstraction). Such separation of the computation and its abstraction is\nassociated with faster learning, flexible decision-making, and broad\ngeneralization capacity. We investigate if such benefits might extend to neural\nnetworks trained with task abstractions. For such benefits to emerge, one needs\na task inference mechanism that possesses two crucial abilities: First, the\nability to infer abstract task representations when no longer explicitly\nprovided (task inference), and second, manipulate task representations to adapt\nto novel problems (task recomposition). To tackle this, we cast task inference\nas an optimization problem from a variational inference perspective and ground\nour approach in an expectation-maximization framework. We show that gradients\nbackpropagated through a neural network to a task representation layer are an\nefficient heuristic to infer current task demands, a process we refer to as\ngradient-based inference (GBI). Further iterative optimization of the task\nrepresentation layer allows for recomposing abstractions to adapt to novel\nsituations. Using a toy example, a novel image classifier, and a language\nmodel, we demonstrate that GBI provides higher learning efficiency and\ngeneralization to novel tasks and limits forgetting. Moreover, we show that GBI\nhas unique advantages such as preserving information for uncertainty estimation\nand detecting out-of-distribution samples.\n", "link": "http://arxiv.org/abs/2407.17356v1", "date": "2024-07-24", "relevancy": 2.0546, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.541}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5108}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient-based%20inference%20of%20abstract%20task%20representations%20for%0A%20%20generalization%20in%20neural%20networks&body=Title%3A%20Gradient-based%20inference%20of%20abstract%20task%20representations%20for%0A%20%20generalization%20in%20neural%20networks%0AAuthor%3A%20Ali%20Hummos%20and%20Felipe%20del%20R%C3%ADo%20and%20Brabeeba%20Mien%20Wang%20and%20Julio%20Hurtado%20and%20Cristian%20B.%20Calderon%20and%20Guangyu%20Robert%20Yang%0AAbstract%3A%20%20%20Humans%20and%20many%20animals%20show%20remarkably%20adaptive%20behavior%20and%20can%20respond%0Adifferently%20to%20the%20same%20input%20depending%20on%20their%20internal%20goals.%20The%20brain%20not%0Aonly%20represents%20the%20intermediate%20abstractions%20needed%20to%20perform%20a%20computation%0Abut%20also%20actively%20maintains%20a%20representation%20of%20the%20computation%20itself%20%28task%0Aabstraction%29.%20Such%20separation%20of%20the%20computation%20and%20its%20abstraction%20is%0Aassociated%20with%20faster%20learning%2C%20flexible%20decision-making%2C%20and%20broad%0Ageneralization%20capacity.%20We%20investigate%20if%20such%20benefits%20might%20extend%20to%20neural%0Anetworks%20trained%20with%20task%20abstractions.%20For%20such%20benefits%20to%20emerge%2C%20one%20needs%0Aa%20task%20inference%20mechanism%20that%20possesses%20two%20crucial%20abilities%3A%20First%2C%20the%0Aability%20to%20infer%20abstract%20task%20representations%20when%20no%20longer%20explicitly%0Aprovided%20%28task%20inference%29%2C%20and%20second%2C%20manipulate%20task%20representations%20to%20adapt%0Ato%20novel%20problems%20%28task%20recomposition%29.%20To%20tackle%20this%2C%20we%20cast%20task%20inference%0Aas%20an%20optimization%20problem%20from%20a%20variational%20inference%20perspective%20and%20ground%0Aour%20approach%20in%20an%20expectation-maximization%20framework.%20We%20show%20that%20gradients%0Abackpropagated%20through%20a%20neural%20network%20to%20a%20task%20representation%20layer%20are%20an%0Aefficient%20heuristic%20to%20infer%20current%20task%20demands%2C%20a%20process%20we%20refer%20to%20as%0Agradient-based%20inference%20%28GBI%29.%20Further%20iterative%20optimization%20of%20the%20task%0Arepresentation%20layer%20allows%20for%20recomposing%20abstractions%20to%20adapt%20to%20novel%0Asituations.%20Using%20a%20toy%20example%2C%20a%20novel%20image%20classifier%2C%20and%20a%20language%0Amodel%2C%20we%20demonstrate%20that%20GBI%20provides%20higher%20learning%20efficiency%20and%0Ageneralization%20to%20novel%20tasks%20and%20limits%20forgetting.%20Moreover%2C%20we%20show%20that%20GBI%0Ahas%20unique%20advantages%20such%20as%20preserving%20information%20for%20uncertainty%20estimation%0Aand%20detecting%20out-of-distribution%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17356v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient-based%2520inference%2520of%2520abstract%2520task%2520representations%2520for%250A%2520%2520generalization%2520in%2520neural%2520networks%26entry.906535625%3DAli%2520Hummos%2520and%2520Felipe%2520del%2520R%25C3%25ADo%2520and%2520Brabeeba%2520Mien%2520Wang%2520and%2520Julio%2520Hurtado%2520and%2520Cristian%2520B.%2520Calderon%2520and%2520Guangyu%2520Robert%2520Yang%26entry.1292438233%3D%2520%2520Humans%2520and%2520many%2520animals%2520show%2520remarkably%2520adaptive%2520behavior%2520and%2520can%2520respond%250Adifferently%2520to%2520the%2520same%2520input%2520depending%2520on%2520their%2520internal%2520goals.%2520The%2520brain%2520not%250Aonly%2520represents%2520the%2520intermediate%2520abstractions%2520needed%2520to%2520perform%2520a%2520computation%250Abut%2520also%2520actively%2520maintains%2520a%2520representation%2520of%2520the%2520computation%2520itself%2520%2528task%250Aabstraction%2529.%2520Such%2520separation%2520of%2520the%2520computation%2520and%2520its%2520abstraction%2520is%250Aassociated%2520with%2520faster%2520learning%252C%2520flexible%2520decision-making%252C%2520and%2520broad%250Ageneralization%2520capacity.%2520We%2520investigate%2520if%2520such%2520benefits%2520might%2520extend%2520to%2520neural%250Anetworks%2520trained%2520with%2520task%2520abstractions.%2520For%2520such%2520benefits%2520to%2520emerge%252C%2520one%2520needs%250Aa%2520task%2520inference%2520mechanism%2520that%2520possesses%2520two%2520crucial%2520abilities%253A%2520First%252C%2520the%250Aability%2520to%2520infer%2520abstract%2520task%2520representations%2520when%2520no%2520longer%2520explicitly%250Aprovided%2520%2528task%2520inference%2529%252C%2520and%2520second%252C%2520manipulate%2520task%2520representations%2520to%2520adapt%250Ato%2520novel%2520problems%2520%2528task%2520recomposition%2529.%2520To%2520tackle%2520this%252C%2520we%2520cast%2520task%2520inference%250Aas%2520an%2520optimization%2520problem%2520from%2520a%2520variational%2520inference%2520perspective%2520and%2520ground%250Aour%2520approach%2520in%2520an%2520expectation-maximization%2520framework.%2520We%2520show%2520that%2520gradients%250Abackpropagated%2520through%2520a%2520neural%2520network%2520to%2520a%2520task%2520representation%2520layer%2520are%2520an%250Aefficient%2520heuristic%2520to%2520infer%2520current%2520task%2520demands%252C%2520a%2520process%2520we%2520refer%2520to%2520as%250Agradient-based%2520inference%2520%2528GBI%2529.%2520Further%2520iterative%2520optimization%2520of%2520the%2520task%250Arepresentation%2520layer%2520allows%2520for%2520recomposing%2520abstractions%2520to%2520adapt%2520to%2520novel%250Asituations.%2520Using%2520a%2520toy%2520example%252C%2520a%2520novel%2520image%2520classifier%252C%2520and%2520a%2520language%250Amodel%252C%2520we%2520demonstrate%2520that%2520GBI%2520provides%2520higher%2520learning%2520efficiency%2520and%250Ageneralization%2520to%2520novel%2520tasks%2520and%2520limits%2520forgetting.%2520Moreover%252C%2520we%2520show%2520that%2520GBI%250Ahas%2520unique%2520advantages%2520such%2520as%2520preserving%2520information%2520for%2520uncertainty%2520estimation%250Aand%2520detecting%2520out-of-distribution%2520samples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17356v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient-based%20inference%20of%20abstract%20task%20representations%20for%0A%20%20generalization%20in%20neural%20networks&entry.906535625=Ali%20Hummos%20and%20Felipe%20del%20R%C3%ADo%20and%20Brabeeba%20Mien%20Wang%20and%20Julio%20Hurtado%20and%20Cristian%20B.%20Calderon%20and%20Guangyu%20Robert%20Yang&entry.1292438233=%20%20Humans%20and%20many%20animals%20show%20remarkably%20adaptive%20behavior%20and%20can%20respond%0Adifferently%20to%20the%20same%20input%20depending%20on%20their%20internal%20goals.%20The%20brain%20not%0Aonly%20represents%20the%20intermediate%20abstractions%20needed%20to%20perform%20a%20computation%0Abut%20also%20actively%20maintains%20a%20representation%20of%20the%20computation%20itself%20%28task%0Aabstraction%29.%20Such%20separation%20of%20the%20computation%20and%20its%20abstraction%20is%0Aassociated%20with%20faster%20learning%2C%20flexible%20decision-making%2C%20and%20broad%0Ageneralization%20capacity.%20We%20investigate%20if%20such%20benefits%20might%20extend%20to%20neural%0Anetworks%20trained%20with%20task%20abstractions.%20For%20such%20benefits%20to%20emerge%2C%20one%20needs%0Aa%20task%20inference%20mechanism%20that%20possesses%20two%20crucial%20abilities%3A%20First%2C%20the%0Aability%20to%20infer%20abstract%20task%20representations%20when%20no%20longer%20explicitly%0Aprovided%20%28task%20inference%29%2C%20and%20second%2C%20manipulate%20task%20representations%20to%20adapt%0Ato%20novel%20problems%20%28task%20recomposition%29.%20To%20tackle%20this%2C%20we%20cast%20task%20inference%0Aas%20an%20optimization%20problem%20from%20a%20variational%20inference%20perspective%20and%20ground%0Aour%20approach%20in%20an%20expectation-maximization%20framework.%20We%20show%20that%20gradients%0Abackpropagated%20through%20a%20neural%20network%20to%20a%20task%20representation%20layer%20are%20an%0Aefficient%20heuristic%20to%20infer%20current%20task%20demands%2C%20a%20process%20we%20refer%20to%20as%0Agradient-based%20inference%20%28GBI%29.%20Further%20iterative%20optimization%20of%20the%20task%0Arepresentation%20layer%20allows%20for%20recomposing%20abstractions%20to%20adapt%20to%20novel%0Asituations.%20Using%20a%20toy%20example%2C%20a%20novel%20image%20classifier%2C%20and%20a%20language%0Amodel%2C%20we%20demonstrate%20that%20GBI%20provides%20higher%20learning%20efficiency%20and%0Ageneralization%20to%20novel%20tasks%20and%20limits%20forgetting.%20Moreover%2C%20we%20show%20that%20GBI%0Ahas%20unique%20advantages%20such%20as%20preserving%20information%20for%20uncertainty%20estimation%0Aand%20detecting%20out-of-distribution%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17356v1&entry.124074799=Read"},
{"title": "$\u03a6$-DVAE: Physics-Informed Dynamical Variational Autoencoders for\n  Unstructured Data Assimilation", "author": "Alex Glyn-Davies and Connor Duffin and \u00d6. Deniz Akyildiz and Mark Girolami", "abstract": "  Incorporating unstructured data into physical models is a challenging problem\nthat is emerging in data assimilation. Traditional approaches focus on\nwell-defined observation operators whose functional forms are typically assumed\nto be known. This prevents these methods from achieving a consistent model-data\nsynthesis in configurations where the mapping from data-space to model-space is\nunknown. To address these shortcomings, in this paper we develop a\nphysics-informed dynamical variational autoencoder ($\\Phi$-DVAE) to embed\ndiverse data streams into time-evolving physical systems described by\ndifferential equations. Our approach combines a standard, possibly nonlinear,\nfilter for the latent state-space model and a VAE, to assimilate the\nunstructured data into the latent dynamical system. Unstructured data, in our\nexample systems, comes in the form of video data and velocity field\nmeasurements, however the methodology is suitably generic to allow for\narbitrary unknown observation operators. A variational Bayesian framework is\nused for the joint estimation of the encoding, latent states, and unknown\nsystem parameters. To demonstrate the method, we provide case studies with the\nLorenz-63 ordinary differential equation, and the advection and Korteweg-de\nVries partial differential equations. Our results, with synthetic data, show\nthat $\\Phi$-DVAE provides a data efficient dynamics encoding methodology which\nis competitive with standard approaches. Unknown parameters are recovered with\nuncertainty quantification, and unseen data are accurately predicted.\n", "link": "http://arxiv.org/abs/2209.15609v3", "date": "2024-07-24", "relevancy": 2.0536, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5391}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5113}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24%CE%A6%24-DVAE%3A%20Physics-Informed%20Dynamical%20Variational%20Autoencoders%20for%0A%20%20Unstructured%20Data%20Assimilation&body=Title%3A%20%24%CE%A6%24-DVAE%3A%20Physics-Informed%20Dynamical%20Variational%20Autoencoders%20for%0A%20%20Unstructured%20Data%20Assimilation%0AAuthor%3A%20Alex%20Glyn-Davies%20and%20Connor%20Duffin%20and%20%C3%96.%20Deniz%20Akyildiz%20and%20Mark%20Girolami%0AAbstract%3A%20%20%20Incorporating%20unstructured%20data%20into%20physical%20models%20is%20a%20challenging%20problem%0Athat%20is%20emerging%20in%20data%20assimilation.%20Traditional%20approaches%20focus%20on%0Awell-defined%20observation%20operators%20whose%20functional%20forms%20are%20typically%20assumed%0Ato%20be%20known.%20This%20prevents%20these%20methods%20from%20achieving%20a%20consistent%20model-data%0Asynthesis%20in%20configurations%20where%20the%20mapping%20from%20data-space%20to%20model-space%20is%0Aunknown.%20To%20address%20these%20shortcomings%2C%20in%20this%20paper%20we%20develop%20a%0Aphysics-informed%20dynamical%20variational%20autoencoder%20%28%24%5CPhi%24-DVAE%29%20to%20embed%0Adiverse%20data%20streams%20into%20time-evolving%20physical%20systems%20described%20by%0Adifferential%20equations.%20Our%20approach%20combines%20a%20standard%2C%20possibly%20nonlinear%2C%0Afilter%20for%20the%20latent%20state-space%20model%20and%20a%20VAE%2C%20to%20assimilate%20the%0Aunstructured%20data%20into%20the%20latent%20dynamical%20system.%20Unstructured%20data%2C%20in%20our%0Aexample%20systems%2C%20comes%20in%20the%20form%20of%20video%20data%20and%20velocity%20field%0Ameasurements%2C%20however%20the%20methodology%20is%20suitably%20generic%20to%20allow%20for%0Aarbitrary%20unknown%20observation%20operators.%20A%20variational%20Bayesian%20framework%20is%0Aused%20for%20the%20joint%20estimation%20of%20the%20encoding%2C%20latent%20states%2C%20and%20unknown%0Asystem%20parameters.%20To%20demonstrate%20the%20method%2C%20we%20provide%20case%20studies%20with%20the%0ALorenz-63%20ordinary%20differential%20equation%2C%20and%20the%20advection%20and%20Korteweg-de%0AVries%20partial%20differential%20equations.%20Our%20results%2C%20with%20synthetic%20data%2C%20show%0Athat%20%24%5CPhi%24-DVAE%20provides%20a%20data%20efficient%20dynamics%20encoding%20methodology%20which%0Ais%20competitive%20with%20standard%20approaches.%20Unknown%20parameters%20are%20recovered%20with%0Auncertainty%20quantification%2C%20and%20unseen%20data%20are%20accurately%20predicted.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.15609v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524%25CE%25A6%2524-DVAE%253A%2520Physics-Informed%2520Dynamical%2520Variational%2520Autoencoders%2520for%250A%2520%2520Unstructured%2520Data%2520Assimilation%26entry.906535625%3DAlex%2520Glyn-Davies%2520and%2520Connor%2520Duffin%2520and%2520%25C3%2596.%2520Deniz%2520Akyildiz%2520and%2520Mark%2520Girolami%26entry.1292438233%3D%2520%2520Incorporating%2520unstructured%2520data%2520into%2520physical%2520models%2520is%2520a%2520challenging%2520problem%250Athat%2520is%2520emerging%2520in%2520data%2520assimilation.%2520Traditional%2520approaches%2520focus%2520on%250Awell-defined%2520observation%2520operators%2520whose%2520functional%2520forms%2520are%2520typically%2520assumed%250Ato%2520be%2520known.%2520This%2520prevents%2520these%2520methods%2520from%2520achieving%2520a%2520consistent%2520model-data%250Asynthesis%2520in%2520configurations%2520where%2520the%2520mapping%2520from%2520data-space%2520to%2520model-space%2520is%250Aunknown.%2520To%2520address%2520these%2520shortcomings%252C%2520in%2520this%2520paper%2520we%2520develop%2520a%250Aphysics-informed%2520dynamical%2520variational%2520autoencoder%2520%2528%2524%255CPhi%2524-DVAE%2529%2520to%2520embed%250Adiverse%2520data%2520streams%2520into%2520time-evolving%2520physical%2520systems%2520described%2520by%250Adifferential%2520equations.%2520Our%2520approach%2520combines%2520a%2520standard%252C%2520possibly%2520nonlinear%252C%250Afilter%2520for%2520the%2520latent%2520state-space%2520model%2520and%2520a%2520VAE%252C%2520to%2520assimilate%2520the%250Aunstructured%2520data%2520into%2520the%2520latent%2520dynamical%2520system.%2520Unstructured%2520data%252C%2520in%2520our%250Aexample%2520systems%252C%2520comes%2520in%2520the%2520form%2520of%2520video%2520data%2520and%2520velocity%2520field%250Ameasurements%252C%2520however%2520the%2520methodology%2520is%2520suitably%2520generic%2520to%2520allow%2520for%250Aarbitrary%2520unknown%2520observation%2520operators.%2520A%2520variational%2520Bayesian%2520framework%2520is%250Aused%2520for%2520the%2520joint%2520estimation%2520of%2520the%2520encoding%252C%2520latent%2520states%252C%2520and%2520unknown%250Asystem%2520parameters.%2520To%2520demonstrate%2520the%2520method%252C%2520we%2520provide%2520case%2520studies%2520with%2520the%250ALorenz-63%2520ordinary%2520differential%2520equation%252C%2520and%2520the%2520advection%2520and%2520Korteweg-de%250AVries%2520partial%2520differential%2520equations.%2520Our%2520results%252C%2520with%2520synthetic%2520data%252C%2520show%250Athat%2520%2524%255CPhi%2524-DVAE%2520provides%2520a%2520data%2520efficient%2520dynamics%2520encoding%2520methodology%2520which%250Ais%2520competitive%2520with%2520standard%2520approaches.%2520Unknown%2520parameters%2520are%2520recovered%2520with%250Auncertainty%2520quantification%252C%2520and%2520unseen%2520data%2520are%2520accurately%2520predicted.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.15609v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24%CE%A6%24-DVAE%3A%20Physics-Informed%20Dynamical%20Variational%20Autoencoders%20for%0A%20%20Unstructured%20Data%20Assimilation&entry.906535625=Alex%20Glyn-Davies%20and%20Connor%20Duffin%20and%20%C3%96.%20Deniz%20Akyildiz%20and%20Mark%20Girolami&entry.1292438233=%20%20Incorporating%20unstructured%20data%20into%20physical%20models%20is%20a%20challenging%20problem%0Athat%20is%20emerging%20in%20data%20assimilation.%20Traditional%20approaches%20focus%20on%0Awell-defined%20observation%20operators%20whose%20functional%20forms%20are%20typically%20assumed%0Ato%20be%20known.%20This%20prevents%20these%20methods%20from%20achieving%20a%20consistent%20model-data%0Asynthesis%20in%20configurations%20where%20the%20mapping%20from%20data-space%20to%20model-space%20is%0Aunknown.%20To%20address%20these%20shortcomings%2C%20in%20this%20paper%20we%20develop%20a%0Aphysics-informed%20dynamical%20variational%20autoencoder%20%28%24%5CPhi%24-DVAE%29%20to%20embed%0Adiverse%20data%20streams%20into%20time-evolving%20physical%20systems%20described%20by%0Adifferential%20equations.%20Our%20approach%20combines%20a%20standard%2C%20possibly%20nonlinear%2C%0Afilter%20for%20the%20latent%20state-space%20model%20and%20a%20VAE%2C%20to%20assimilate%20the%0Aunstructured%20data%20into%20the%20latent%20dynamical%20system.%20Unstructured%20data%2C%20in%20our%0Aexample%20systems%2C%20comes%20in%20the%20form%20of%20video%20data%20and%20velocity%20field%0Ameasurements%2C%20however%20the%20methodology%20is%20suitably%20generic%20to%20allow%20for%0Aarbitrary%20unknown%20observation%20operators.%20A%20variational%20Bayesian%20framework%20is%0Aused%20for%20the%20joint%20estimation%20of%20the%20encoding%2C%20latent%20states%2C%20and%20unknown%0Asystem%20parameters.%20To%20demonstrate%20the%20method%2C%20we%20provide%20case%20studies%20with%20the%0ALorenz-63%20ordinary%20differential%20equation%2C%20and%20the%20advection%20and%20Korteweg-de%0AVries%20partial%20differential%20equations.%20Our%20results%2C%20with%20synthetic%20data%2C%20show%0Athat%20%24%5CPhi%24-DVAE%20provides%20a%20data%20efficient%20dynamics%20encoding%20methodology%20which%0Ais%20competitive%20with%20standard%20approaches.%20Unknown%20parameters%20are%20recovered%20with%0Auncertainty%20quantification%2C%20and%20unseen%20data%20are%20accurately%20predicted.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.15609v3&entry.124074799=Read"},
{"title": "XMeCap: Meme Caption Generation with Sub-Image Adaptability", "author": "Yuyan Chen and Songzhou Yan and Zhihong Zhu and Zhixu Li and Yanghua Xiao", "abstract": "  Humor, deeply rooted in societal meanings and cultural details, poses a\nunique challenge for machines. While advances have been made in natural\nlanguage processing, real-world humor often thrives in a multi-modal context,\nencapsulated distinctively by memes. This paper poses a particular emphasis on\nthe impact of multi-images on meme captioning. After that, we introduce the\n\\textsc{XMeCap} framework, a novel approach that adopts supervised fine-tuning\nand reinforcement learning based on an innovative reward model, which factors\nin both global and local similarities between visuals and text. Our results,\nbenchmarked against contemporary models, manifest a marked improvement in\ncaption generation for both single-image and multi-image memes, as well as\ndifferent meme categories. \\textsc{XMeCap} achieves an average evaluation score\nof 75.85 for single-image memes and 66.32 for multi-image memes, outperforming\nthe best baseline by 3.71\\% and 4.82\\%, respectively. This research not only\nestablishes a new frontier in meme-related studies but also underscores the\npotential of machines in understanding and generating humor in a multi-modal\nsetting.\n", "link": "http://arxiv.org/abs/2407.17152v1", "date": "2024-07-24", "relevancy": 2.0485, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5303}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5093}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XMeCap%3A%20Meme%20Caption%20Generation%20with%20Sub-Image%20Adaptability&body=Title%3A%20XMeCap%3A%20Meme%20Caption%20Generation%20with%20Sub-Image%20Adaptability%0AAuthor%3A%20Yuyan%20Chen%20and%20Songzhou%20Yan%20and%20Zhihong%20Zhu%20and%20Zhixu%20Li%20and%20Yanghua%20Xiao%0AAbstract%3A%20%20%20Humor%2C%20deeply%20rooted%20in%20societal%20meanings%20and%20cultural%20details%2C%20poses%20a%0Aunique%20challenge%20for%20machines.%20While%20advances%20have%20been%20made%20in%20natural%0Alanguage%20processing%2C%20real-world%20humor%20often%20thrives%20in%20a%20multi-modal%20context%2C%0Aencapsulated%20distinctively%20by%20memes.%20This%20paper%20poses%20a%20particular%20emphasis%20on%0Athe%20impact%20of%20multi-images%20on%20meme%20captioning.%20After%20that%2C%20we%20introduce%20the%0A%5Ctextsc%7BXMeCap%7D%20framework%2C%20a%20novel%20approach%20that%20adopts%20supervised%20fine-tuning%0Aand%20reinforcement%20learning%20based%20on%20an%20innovative%20reward%20model%2C%20which%20factors%0Ain%20both%20global%20and%20local%20similarities%20between%20visuals%20and%20text.%20Our%20results%2C%0Abenchmarked%20against%20contemporary%20models%2C%20manifest%20a%20marked%20improvement%20in%0Acaption%20generation%20for%20both%20single-image%20and%20multi-image%20memes%2C%20as%20well%20as%0Adifferent%20meme%20categories.%20%5Ctextsc%7BXMeCap%7D%20achieves%20an%20average%20evaluation%20score%0Aof%2075.85%20for%20single-image%20memes%20and%2066.32%20for%20multi-image%20memes%2C%20outperforming%0Athe%20best%20baseline%20by%203.71%5C%25%20and%204.82%5C%25%2C%20respectively.%20This%20research%20not%20only%0Aestablishes%20a%20new%20frontier%20in%20meme-related%20studies%20but%20also%20underscores%20the%0Apotential%20of%20machines%20in%20understanding%20and%20generating%20humor%20in%20a%20multi-modal%0Asetting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXMeCap%253A%2520Meme%2520Caption%2520Generation%2520with%2520Sub-Image%2520Adaptability%26entry.906535625%3DYuyan%2520Chen%2520and%2520Songzhou%2520Yan%2520and%2520Zhihong%2520Zhu%2520and%2520Zhixu%2520Li%2520and%2520Yanghua%2520Xiao%26entry.1292438233%3D%2520%2520Humor%252C%2520deeply%2520rooted%2520in%2520societal%2520meanings%2520and%2520cultural%2520details%252C%2520poses%2520a%250Aunique%2520challenge%2520for%2520machines.%2520While%2520advances%2520have%2520been%2520made%2520in%2520natural%250Alanguage%2520processing%252C%2520real-world%2520humor%2520often%2520thrives%2520in%2520a%2520multi-modal%2520context%252C%250Aencapsulated%2520distinctively%2520by%2520memes.%2520This%2520paper%2520poses%2520a%2520particular%2520emphasis%2520on%250Athe%2520impact%2520of%2520multi-images%2520on%2520meme%2520captioning.%2520After%2520that%252C%2520we%2520introduce%2520the%250A%255Ctextsc%257BXMeCap%257D%2520framework%252C%2520a%2520novel%2520approach%2520that%2520adopts%2520supervised%2520fine-tuning%250Aand%2520reinforcement%2520learning%2520based%2520on%2520an%2520innovative%2520reward%2520model%252C%2520which%2520factors%250Ain%2520both%2520global%2520and%2520local%2520similarities%2520between%2520visuals%2520and%2520text.%2520Our%2520results%252C%250Abenchmarked%2520against%2520contemporary%2520models%252C%2520manifest%2520a%2520marked%2520improvement%2520in%250Acaption%2520generation%2520for%2520both%2520single-image%2520and%2520multi-image%2520memes%252C%2520as%2520well%2520as%250Adifferent%2520meme%2520categories.%2520%255Ctextsc%257BXMeCap%257D%2520achieves%2520an%2520average%2520evaluation%2520score%250Aof%252075.85%2520for%2520single-image%2520memes%2520and%252066.32%2520for%2520multi-image%2520memes%252C%2520outperforming%250Athe%2520best%2520baseline%2520by%25203.71%255C%2525%2520and%25204.82%255C%2525%252C%2520respectively.%2520This%2520research%2520not%2520only%250Aestablishes%2520a%2520new%2520frontier%2520in%2520meme-related%2520studies%2520but%2520also%2520underscores%2520the%250Apotential%2520of%2520machines%2520in%2520understanding%2520and%2520generating%2520humor%2520in%2520a%2520multi-modal%250Asetting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XMeCap%3A%20Meme%20Caption%20Generation%20with%20Sub-Image%20Adaptability&entry.906535625=Yuyan%20Chen%20and%20Songzhou%20Yan%20and%20Zhihong%20Zhu%20and%20Zhixu%20Li%20and%20Yanghua%20Xiao&entry.1292438233=%20%20Humor%2C%20deeply%20rooted%20in%20societal%20meanings%20and%20cultural%20details%2C%20poses%20a%0Aunique%20challenge%20for%20machines.%20While%20advances%20have%20been%20made%20in%20natural%0Alanguage%20processing%2C%20real-world%20humor%20often%20thrives%20in%20a%20multi-modal%20context%2C%0Aencapsulated%20distinctively%20by%20memes.%20This%20paper%20poses%20a%20particular%20emphasis%20on%0Athe%20impact%20of%20multi-images%20on%20meme%20captioning.%20After%20that%2C%20we%20introduce%20the%0A%5Ctextsc%7BXMeCap%7D%20framework%2C%20a%20novel%20approach%20that%20adopts%20supervised%20fine-tuning%0Aand%20reinforcement%20learning%20based%20on%20an%20innovative%20reward%20model%2C%20which%20factors%0Ain%20both%20global%20and%20local%20similarities%20between%20visuals%20and%20text.%20Our%20results%2C%0Abenchmarked%20against%20contemporary%20models%2C%20manifest%20a%20marked%20improvement%20in%0Acaption%20generation%20for%20both%20single-image%20and%20multi-image%20memes%2C%20as%20well%20as%0Adifferent%20meme%20categories.%20%5Ctextsc%7BXMeCap%7D%20achieves%20an%20average%20evaluation%20score%0Aof%2075.85%20for%20single-image%20memes%20and%2066.32%20for%20multi-image%20memes%2C%20outperforming%0Athe%20best%20baseline%20by%203.71%5C%25%20and%204.82%5C%25%2C%20respectively.%20This%20research%20not%20only%0Aestablishes%20a%20new%20frontier%20in%20meme-related%20studies%20but%20also%20underscores%20the%0Apotential%20of%20machines%20in%20understanding%20and%20generating%20humor%20in%20a%20multi-modal%0Asetting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17152v1&entry.124074799=Read"},
{"title": "Looking at Model Debiasing through the Lens of Anomaly Detection", "author": "Vito Paolo Pastore and Massimiliano Ciranni and Davide Marinelli and Francesca Odone and Vittorio Murino", "abstract": "  It is widely recognized that deep neural networks are sensitive to bias in\nthe data. This means that during training these models are likely to learn\nspurious correlations between data and labels, resulting in limited\ngeneralization abilities and low performance. In this context, model debiasing\napproaches can be devised aiming at reducing the model's dependency on such\nunwanted correlations, either leveraging the knowledge of bias information or\nnot. In this work, we focus on the latter and more realistic scenario, showing\nthe importance of accurately predicting the bias-conflicting and bias-aligned\nsamples to obtain compelling performance in bias mitigation. On this ground, we\npropose to conceive the problem of model bias from an out-of-distribution\nperspective, introducing a new bias identification method based on anomaly\ndetection. We claim that when data is mostly biased, bias-conflicting samples\ncan be regarded as outliers with respect to the bias-aligned distribution in\nthe feature space of a biased model, thus allowing for precisely detecting them\nwith an anomaly detection method. Coupling the proposed bias identification\napproach with bias-conflicting data upsampling and augmentation in a two-step\nstrategy, we reach state-of-the-art performance on synthetic and real benchmark\ndatasets. Ultimately, our proposed approach shows that the data bias issue does\nnot necessarily require complex debiasing methods, given that an accurate bias\nidentification procedure is defined.\n", "link": "http://arxiv.org/abs/2407.17449v1", "date": "2024-07-24", "relevancy": 2.0462, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5162}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5111}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Looking%20at%20Model%20Debiasing%20through%20the%20Lens%20of%20Anomaly%20Detection&body=Title%3A%20Looking%20at%20Model%20Debiasing%20through%20the%20Lens%20of%20Anomaly%20Detection%0AAuthor%3A%20Vito%20Paolo%20Pastore%20and%20Massimiliano%20Ciranni%20and%20Davide%20Marinelli%20and%20Francesca%20Odone%20and%20Vittorio%20Murino%0AAbstract%3A%20%20%20It%20is%20widely%20recognized%20that%20deep%20neural%20networks%20are%20sensitive%20to%20bias%20in%0Athe%20data.%20This%20means%20that%20during%20training%20these%20models%20are%20likely%20to%20learn%0Aspurious%20correlations%20between%20data%20and%20labels%2C%20resulting%20in%20limited%0Ageneralization%20abilities%20and%20low%20performance.%20In%20this%20context%2C%20model%20debiasing%0Aapproaches%20can%20be%20devised%20aiming%20at%20reducing%20the%20model%27s%20dependency%20on%20such%0Aunwanted%20correlations%2C%20either%20leveraging%20the%20knowledge%20of%20bias%20information%20or%0Anot.%20In%20this%20work%2C%20we%20focus%20on%20the%20latter%20and%20more%20realistic%20scenario%2C%20showing%0Athe%20importance%20of%20accurately%20predicting%20the%20bias-conflicting%20and%20bias-aligned%0Asamples%20to%20obtain%20compelling%20performance%20in%20bias%20mitigation.%20On%20this%20ground%2C%20we%0Apropose%20to%20conceive%20the%20problem%20of%20model%20bias%20from%20an%20out-of-distribution%0Aperspective%2C%20introducing%20a%20new%20bias%20identification%20method%20based%20on%20anomaly%0Adetection.%20We%20claim%20that%20when%20data%20is%20mostly%20biased%2C%20bias-conflicting%20samples%0Acan%20be%20regarded%20as%20outliers%20with%20respect%20to%20the%20bias-aligned%20distribution%20in%0Athe%20feature%20space%20of%20a%20biased%20model%2C%20thus%20allowing%20for%20precisely%20detecting%20them%0Awith%20an%20anomaly%20detection%20method.%20Coupling%20the%20proposed%20bias%20identification%0Aapproach%20with%20bias-conflicting%20data%20upsampling%20and%20augmentation%20in%20a%20two-step%0Astrategy%2C%20we%20reach%20state-of-the-art%20performance%20on%20synthetic%20and%20real%20benchmark%0Adatasets.%20Ultimately%2C%20our%20proposed%20approach%20shows%20that%20the%20data%20bias%20issue%20does%0Anot%20necessarily%20require%20complex%20debiasing%20methods%2C%20given%20that%20an%20accurate%20bias%0Aidentification%20procedure%20is%20defined.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17449v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLooking%2520at%2520Model%2520Debiasing%2520through%2520the%2520Lens%2520of%2520Anomaly%2520Detection%26entry.906535625%3DVito%2520Paolo%2520Pastore%2520and%2520Massimiliano%2520Ciranni%2520and%2520Davide%2520Marinelli%2520and%2520Francesca%2520Odone%2520and%2520Vittorio%2520Murino%26entry.1292438233%3D%2520%2520It%2520is%2520widely%2520recognized%2520that%2520deep%2520neural%2520networks%2520are%2520sensitive%2520to%2520bias%2520in%250Athe%2520data.%2520This%2520means%2520that%2520during%2520training%2520these%2520models%2520are%2520likely%2520to%2520learn%250Aspurious%2520correlations%2520between%2520data%2520and%2520labels%252C%2520resulting%2520in%2520limited%250Ageneralization%2520abilities%2520and%2520low%2520performance.%2520In%2520this%2520context%252C%2520model%2520debiasing%250Aapproaches%2520can%2520be%2520devised%2520aiming%2520at%2520reducing%2520the%2520model%2527s%2520dependency%2520on%2520such%250Aunwanted%2520correlations%252C%2520either%2520leveraging%2520the%2520knowledge%2520of%2520bias%2520information%2520or%250Anot.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520the%2520latter%2520and%2520more%2520realistic%2520scenario%252C%2520showing%250Athe%2520importance%2520of%2520accurately%2520predicting%2520the%2520bias-conflicting%2520and%2520bias-aligned%250Asamples%2520to%2520obtain%2520compelling%2520performance%2520in%2520bias%2520mitigation.%2520On%2520this%2520ground%252C%2520we%250Apropose%2520to%2520conceive%2520the%2520problem%2520of%2520model%2520bias%2520from%2520an%2520out-of-distribution%250Aperspective%252C%2520introducing%2520a%2520new%2520bias%2520identification%2520method%2520based%2520on%2520anomaly%250Adetection.%2520We%2520claim%2520that%2520when%2520data%2520is%2520mostly%2520biased%252C%2520bias-conflicting%2520samples%250Acan%2520be%2520regarded%2520as%2520outliers%2520with%2520respect%2520to%2520the%2520bias-aligned%2520distribution%2520in%250Athe%2520feature%2520space%2520of%2520a%2520biased%2520model%252C%2520thus%2520allowing%2520for%2520precisely%2520detecting%2520them%250Awith%2520an%2520anomaly%2520detection%2520method.%2520Coupling%2520the%2520proposed%2520bias%2520identification%250Aapproach%2520with%2520bias-conflicting%2520data%2520upsampling%2520and%2520augmentation%2520in%2520a%2520two-step%250Astrategy%252C%2520we%2520reach%2520state-of-the-art%2520performance%2520on%2520synthetic%2520and%2520real%2520benchmark%250Adatasets.%2520Ultimately%252C%2520our%2520proposed%2520approach%2520shows%2520that%2520the%2520data%2520bias%2520issue%2520does%250Anot%2520necessarily%2520require%2520complex%2520debiasing%2520methods%252C%2520given%2520that%2520an%2520accurate%2520bias%250Aidentification%2520procedure%2520is%2520defined.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17449v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Looking%20at%20Model%20Debiasing%20through%20the%20Lens%20of%20Anomaly%20Detection&entry.906535625=Vito%20Paolo%20Pastore%20and%20Massimiliano%20Ciranni%20and%20Davide%20Marinelli%20and%20Francesca%20Odone%20and%20Vittorio%20Murino&entry.1292438233=%20%20It%20is%20widely%20recognized%20that%20deep%20neural%20networks%20are%20sensitive%20to%20bias%20in%0Athe%20data.%20This%20means%20that%20during%20training%20these%20models%20are%20likely%20to%20learn%0Aspurious%20correlations%20between%20data%20and%20labels%2C%20resulting%20in%20limited%0Ageneralization%20abilities%20and%20low%20performance.%20In%20this%20context%2C%20model%20debiasing%0Aapproaches%20can%20be%20devised%20aiming%20at%20reducing%20the%20model%27s%20dependency%20on%20such%0Aunwanted%20correlations%2C%20either%20leveraging%20the%20knowledge%20of%20bias%20information%20or%0Anot.%20In%20this%20work%2C%20we%20focus%20on%20the%20latter%20and%20more%20realistic%20scenario%2C%20showing%0Athe%20importance%20of%20accurately%20predicting%20the%20bias-conflicting%20and%20bias-aligned%0Asamples%20to%20obtain%20compelling%20performance%20in%20bias%20mitigation.%20On%20this%20ground%2C%20we%0Apropose%20to%20conceive%20the%20problem%20of%20model%20bias%20from%20an%20out-of-distribution%0Aperspective%2C%20introducing%20a%20new%20bias%20identification%20method%20based%20on%20anomaly%0Adetection.%20We%20claim%20that%20when%20data%20is%20mostly%20biased%2C%20bias-conflicting%20samples%0Acan%20be%20regarded%20as%20outliers%20with%20respect%20to%20the%20bias-aligned%20distribution%20in%0Athe%20feature%20space%20of%20a%20biased%20model%2C%20thus%20allowing%20for%20precisely%20detecting%20them%0Awith%20an%20anomaly%20detection%20method.%20Coupling%20the%20proposed%20bias%20identification%0Aapproach%20with%20bias-conflicting%20data%20upsampling%20and%20augmentation%20in%20a%20two-step%0Astrategy%2C%20we%20reach%20state-of-the-art%20performance%20on%20synthetic%20and%20real%20benchmark%0Adatasets.%20Ultimately%2C%20our%20proposed%20approach%20shows%20that%20the%20data%20bias%20issue%20does%0Anot%20necessarily%20require%20complex%20debiasing%20methods%2C%20given%20that%20an%20accurate%20bias%0Aidentification%20procedure%20is%20defined.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17449v1&entry.124074799=Read"},
{"title": "Enhanced Feature Learning via Regularisation: Integrating Neural\n  Networks and Kernel Methods", "author": "Bertille Follain and Francis Bach", "abstract": "  We propose a new method for feature learning and function estimation in\nsupervised learning via regularised empirical risk minimisation. Our approach\nconsiders functions as expectations of Sobolev functions over all possible\none-dimensional projections of the data. This framework is similar to kernel\nridge regression, where the kernel is $\\mathbb{E}_w ( k^{(B)}(w^\\top x,w^\\top\nx^\\prime))$, with $k^{(B)}(a,b) := \\min(|a|, |b|)1_{ab>0}$ the Brownian kernel,\nand the distribution of the projections $w$ is learnt. This can also be viewed\nas an infinite-width one-hidden layer neural network, optimising the first\nlayer's weights through gradient descent and explicitly adjusting the\nnon-linearity and weights of the second layer. We introduce an efficient\ncomputation method for the estimator, called Brownian Kernel Neural Network\n(BKerNN), using particles to approximate the expectation. The optimisation is\nprincipled due to the positive homogeneity of the Brownian kernel. Using\nRademacher complexity, we show that BKerNN's expected risk converges to the\nminimal risk with explicit high-probability rates of $O( \\min((d/n)^{1/2},\nn^{-1/6}))$ (up to logarithmic factors). Numerical experiments confirm our\noptimisation intuitions, and BKerNN outperforms kernel ridge regression, and\nfavourably compares to a one-hidden layer neural network with ReLU activations\nin various settings and real data sets.\n", "link": "http://arxiv.org/abs/2407.17280v1", "date": "2024-07-24", "relevancy": 2.0458, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.516}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5121}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Feature%20Learning%20via%20Regularisation%3A%20Integrating%20Neural%0A%20%20Networks%20and%20Kernel%20Methods&body=Title%3A%20Enhanced%20Feature%20Learning%20via%20Regularisation%3A%20Integrating%20Neural%0A%20%20Networks%20and%20Kernel%20Methods%0AAuthor%3A%20Bertille%20Follain%20and%20Francis%20Bach%0AAbstract%3A%20%20%20We%20propose%20a%20new%20method%20for%20feature%20learning%20and%20function%20estimation%20in%0Asupervised%20learning%20via%20regularised%20empirical%20risk%20minimisation.%20Our%20approach%0Aconsiders%20functions%20as%20expectations%20of%20Sobolev%20functions%20over%20all%20possible%0Aone-dimensional%20projections%20of%20the%20data.%20This%20framework%20is%20similar%20to%20kernel%0Aridge%20regression%2C%20where%20the%20kernel%20is%20%24%5Cmathbb%7BE%7D_w%20%28%20k%5E%7B%28B%29%7D%28w%5E%5Ctop%20x%2Cw%5E%5Ctop%0Ax%5E%5Cprime%29%29%24%2C%20with%20%24k%5E%7B%28B%29%7D%28a%2Cb%29%20%3A%3D%20%5Cmin%28%7Ca%7C%2C%20%7Cb%7C%291_%7Bab%3E0%7D%24%20the%20Brownian%20kernel%2C%0Aand%20the%20distribution%20of%20the%20projections%20%24w%24%20is%20learnt.%20This%20can%20also%20be%20viewed%0Aas%20an%20infinite-width%20one-hidden%20layer%20neural%20network%2C%20optimising%20the%20first%0Alayer%27s%20weights%20through%20gradient%20descent%20and%20explicitly%20adjusting%20the%0Anon-linearity%20and%20weights%20of%20the%20second%20layer.%20We%20introduce%20an%20efficient%0Acomputation%20method%20for%20the%20estimator%2C%20called%20Brownian%20Kernel%20Neural%20Network%0A%28BKerNN%29%2C%20using%20particles%20to%20approximate%20the%20expectation.%20The%20optimisation%20is%0Aprincipled%20due%20to%20the%20positive%20homogeneity%20of%20the%20Brownian%20kernel.%20Using%0ARademacher%20complexity%2C%20we%20show%20that%20BKerNN%27s%20expected%20risk%20converges%20to%20the%0Aminimal%20risk%20with%20explicit%20high-probability%20rates%20of%20%24O%28%20%5Cmin%28%28d/n%29%5E%7B1/2%7D%2C%0An%5E%7B-1/6%7D%29%29%24%20%28up%20to%20logarithmic%20factors%29.%20Numerical%20experiments%20confirm%20our%0Aoptimisation%20intuitions%2C%20and%20BKerNN%20outperforms%20kernel%20ridge%20regression%2C%20and%0Afavourably%20compares%20to%20a%20one-hidden%20layer%20neural%20network%20with%20ReLU%20activations%0Ain%20various%20settings%20and%20real%20data%20sets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17280v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Feature%2520Learning%2520via%2520Regularisation%253A%2520Integrating%2520Neural%250A%2520%2520Networks%2520and%2520Kernel%2520Methods%26entry.906535625%3DBertille%2520Follain%2520and%2520Francis%2520Bach%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520new%2520method%2520for%2520feature%2520learning%2520and%2520function%2520estimation%2520in%250Asupervised%2520learning%2520via%2520regularised%2520empirical%2520risk%2520minimisation.%2520Our%2520approach%250Aconsiders%2520functions%2520as%2520expectations%2520of%2520Sobolev%2520functions%2520over%2520all%2520possible%250Aone-dimensional%2520projections%2520of%2520the%2520data.%2520This%2520framework%2520is%2520similar%2520to%2520kernel%250Aridge%2520regression%252C%2520where%2520the%2520kernel%2520is%2520%2524%255Cmathbb%257BE%257D_w%2520%2528%2520k%255E%257B%2528B%2529%257D%2528w%255E%255Ctop%2520x%252Cw%255E%255Ctop%250Ax%255E%255Cprime%2529%2529%2524%252C%2520with%2520%2524k%255E%257B%2528B%2529%257D%2528a%252Cb%2529%2520%253A%253D%2520%255Cmin%2528%257Ca%257C%252C%2520%257Cb%257C%25291_%257Bab%253E0%257D%2524%2520the%2520Brownian%2520kernel%252C%250Aand%2520the%2520distribution%2520of%2520the%2520projections%2520%2524w%2524%2520is%2520learnt.%2520This%2520can%2520also%2520be%2520viewed%250Aas%2520an%2520infinite-width%2520one-hidden%2520layer%2520neural%2520network%252C%2520optimising%2520the%2520first%250Alayer%2527s%2520weights%2520through%2520gradient%2520descent%2520and%2520explicitly%2520adjusting%2520the%250Anon-linearity%2520and%2520weights%2520of%2520the%2520second%2520layer.%2520We%2520introduce%2520an%2520efficient%250Acomputation%2520method%2520for%2520the%2520estimator%252C%2520called%2520Brownian%2520Kernel%2520Neural%2520Network%250A%2528BKerNN%2529%252C%2520using%2520particles%2520to%2520approximate%2520the%2520expectation.%2520The%2520optimisation%2520is%250Aprincipled%2520due%2520to%2520the%2520positive%2520homogeneity%2520of%2520the%2520Brownian%2520kernel.%2520Using%250ARademacher%2520complexity%252C%2520we%2520show%2520that%2520BKerNN%2527s%2520expected%2520risk%2520converges%2520to%2520the%250Aminimal%2520risk%2520with%2520explicit%2520high-probability%2520rates%2520of%2520%2524O%2528%2520%255Cmin%2528%2528d/n%2529%255E%257B1/2%257D%252C%250An%255E%257B-1/6%257D%2529%2529%2524%2520%2528up%2520to%2520logarithmic%2520factors%2529.%2520Numerical%2520experiments%2520confirm%2520our%250Aoptimisation%2520intuitions%252C%2520and%2520BKerNN%2520outperforms%2520kernel%2520ridge%2520regression%252C%2520and%250Afavourably%2520compares%2520to%2520a%2520one-hidden%2520layer%2520neural%2520network%2520with%2520ReLU%2520activations%250Ain%2520various%2520settings%2520and%2520real%2520data%2520sets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17280v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Feature%20Learning%20via%20Regularisation%3A%20Integrating%20Neural%0A%20%20Networks%20and%20Kernel%20Methods&entry.906535625=Bertille%20Follain%20and%20Francis%20Bach&entry.1292438233=%20%20We%20propose%20a%20new%20method%20for%20feature%20learning%20and%20function%20estimation%20in%0Asupervised%20learning%20via%20regularised%20empirical%20risk%20minimisation.%20Our%20approach%0Aconsiders%20functions%20as%20expectations%20of%20Sobolev%20functions%20over%20all%20possible%0Aone-dimensional%20projections%20of%20the%20data.%20This%20framework%20is%20similar%20to%20kernel%0Aridge%20regression%2C%20where%20the%20kernel%20is%20%24%5Cmathbb%7BE%7D_w%20%28%20k%5E%7B%28B%29%7D%28w%5E%5Ctop%20x%2Cw%5E%5Ctop%0Ax%5E%5Cprime%29%29%24%2C%20with%20%24k%5E%7B%28B%29%7D%28a%2Cb%29%20%3A%3D%20%5Cmin%28%7Ca%7C%2C%20%7Cb%7C%291_%7Bab%3E0%7D%24%20the%20Brownian%20kernel%2C%0Aand%20the%20distribution%20of%20the%20projections%20%24w%24%20is%20learnt.%20This%20can%20also%20be%20viewed%0Aas%20an%20infinite-width%20one-hidden%20layer%20neural%20network%2C%20optimising%20the%20first%0Alayer%27s%20weights%20through%20gradient%20descent%20and%20explicitly%20adjusting%20the%0Anon-linearity%20and%20weights%20of%20the%20second%20layer.%20We%20introduce%20an%20efficient%0Acomputation%20method%20for%20the%20estimator%2C%20called%20Brownian%20Kernel%20Neural%20Network%0A%28BKerNN%29%2C%20using%20particles%20to%20approximate%20the%20expectation.%20The%20optimisation%20is%0Aprincipled%20due%20to%20the%20positive%20homogeneity%20of%20the%20Brownian%20kernel.%20Using%0ARademacher%20complexity%2C%20we%20show%20that%20BKerNN%27s%20expected%20risk%20converges%20to%20the%0Aminimal%20risk%20with%20explicit%20high-probability%20rates%20of%20%24O%28%20%5Cmin%28%28d/n%29%5E%7B1/2%7D%2C%0An%5E%7B-1/6%7D%29%29%24%20%28up%20to%20logarithmic%20factors%29.%20Numerical%20experiments%20confirm%20our%0Aoptimisation%20intuitions%2C%20and%20BKerNN%20outperforms%20kernel%20ridge%20regression%2C%20and%0Afavourably%20compares%20to%20a%20one-hidden%20layer%20neural%20network%20with%20ReLU%20activations%0Ain%20various%20settings%20and%20real%20data%20sets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17280v1&entry.124074799=Read"},
{"title": "Nerva: a Truly Sparse Implementation of Neural Networks", "author": "Wieger Wesselink and Bram Grooten and Qiao Xiao and Cassio de Campos and Mykola Pechenizkiy", "abstract": "  We introduce Nerva, a fast neural network library under development in C++.\nIt supports sparsity by using the sparse matrix operations of Intel's Math\nKernel Library (MKL), which eliminates the need for binary masks. We show that\nNerva significantly decreases training time and memory usage while reaching\nequivalent accuracy to PyTorch. We run static sparse experiments with an MLP on\nCIFAR-10. On high sparsity levels like $99\\%$, the runtime is reduced by a\nfactor of $4\\times$ compared to a PyTorch model using masks. Similar to other\npopular frameworks such as PyTorch and Keras, Nerva offers a Python interface\nfor users to work with.\n", "link": "http://arxiv.org/abs/2407.17437v1", "date": "2024-07-24", "relevancy": 2.0389, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4093}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4087}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4054}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nerva%3A%20a%20Truly%20Sparse%20Implementation%20of%20Neural%20Networks&body=Title%3A%20Nerva%3A%20a%20Truly%20Sparse%20Implementation%20of%20Neural%20Networks%0AAuthor%3A%20Wieger%20Wesselink%20and%20Bram%20Grooten%20and%20Qiao%20Xiao%20and%20Cassio%20de%20Campos%20and%20Mykola%20Pechenizkiy%0AAbstract%3A%20%20%20We%20introduce%20Nerva%2C%20a%20fast%20neural%20network%20library%20under%20development%20in%20C%2B%2B.%0AIt%20supports%20sparsity%20by%20using%20the%20sparse%20matrix%20operations%20of%20Intel%27s%20Math%0AKernel%20Library%20%28MKL%29%2C%20which%20eliminates%20the%20need%20for%20binary%20masks.%20We%20show%20that%0ANerva%20significantly%20decreases%20training%20time%20and%20memory%20usage%20while%20reaching%0Aequivalent%20accuracy%20to%20PyTorch.%20We%20run%20static%20sparse%20experiments%20with%20an%20MLP%20on%0ACIFAR-10.%20On%20high%20sparsity%20levels%20like%20%2499%5C%25%24%2C%20the%20runtime%20is%20reduced%20by%20a%0Afactor%20of%20%244%5Ctimes%24%20compared%20to%20a%20PyTorch%20model%20using%20masks.%20Similar%20to%20other%0Apopular%20frameworks%20such%20as%20PyTorch%20and%20Keras%2C%20Nerva%20offers%20a%20Python%20interface%0Afor%20users%20to%20work%20with.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17437v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNerva%253A%2520a%2520Truly%2520Sparse%2520Implementation%2520of%2520Neural%2520Networks%26entry.906535625%3DWieger%2520Wesselink%2520and%2520Bram%2520Grooten%2520and%2520Qiao%2520Xiao%2520and%2520Cassio%2520de%2520Campos%2520and%2520Mykola%2520Pechenizkiy%26entry.1292438233%3D%2520%2520We%2520introduce%2520Nerva%252C%2520a%2520fast%2520neural%2520network%2520library%2520under%2520development%2520in%2520C%252B%252B.%250AIt%2520supports%2520sparsity%2520by%2520using%2520the%2520sparse%2520matrix%2520operations%2520of%2520Intel%2527s%2520Math%250AKernel%2520Library%2520%2528MKL%2529%252C%2520which%2520eliminates%2520the%2520need%2520for%2520binary%2520masks.%2520We%2520show%2520that%250ANerva%2520significantly%2520decreases%2520training%2520time%2520and%2520memory%2520usage%2520while%2520reaching%250Aequivalent%2520accuracy%2520to%2520PyTorch.%2520We%2520run%2520static%2520sparse%2520experiments%2520with%2520an%2520MLP%2520on%250ACIFAR-10.%2520On%2520high%2520sparsity%2520levels%2520like%2520%252499%255C%2525%2524%252C%2520the%2520runtime%2520is%2520reduced%2520by%2520a%250Afactor%2520of%2520%25244%255Ctimes%2524%2520compared%2520to%2520a%2520PyTorch%2520model%2520using%2520masks.%2520Similar%2520to%2520other%250Apopular%2520frameworks%2520such%2520as%2520PyTorch%2520and%2520Keras%252C%2520Nerva%2520offers%2520a%2520Python%2520interface%250Afor%2520users%2520to%2520work%2520with.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17437v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nerva%3A%20a%20Truly%20Sparse%20Implementation%20of%20Neural%20Networks&entry.906535625=Wieger%20Wesselink%20and%20Bram%20Grooten%20and%20Qiao%20Xiao%20and%20Cassio%20de%20Campos%20and%20Mykola%20Pechenizkiy&entry.1292438233=%20%20We%20introduce%20Nerva%2C%20a%20fast%20neural%20network%20library%20under%20development%20in%20C%2B%2B.%0AIt%20supports%20sparsity%20by%20using%20the%20sparse%20matrix%20operations%20of%20Intel%27s%20Math%0AKernel%20Library%20%28MKL%29%2C%20which%20eliminates%20the%20need%20for%20binary%20masks.%20We%20show%20that%0ANerva%20significantly%20decreases%20training%20time%20and%20memory%20usage%20while%20reaching%0Aequivalent%20accuracy%20to%20PyTorch.%20We%20run%20static%20sparse%20experiments%20with%20an%20MLP%20on%0ACIFAR-10.%20On%20high%20sparsity%20levels%20like%20%2499%5C%25%24%2C%20the%20runtime%20is%20reduced%20by%20a%0Afactor%20of%20%244%5Ctimes%24%20compared%20to%20a%20PyTorch%20model%20using%20masks.%20Similar%20to%20other%0Apopular%20frameworks%20such%20as%20PyTorch%20and%20Keras%2C%20Nerva%20offers%20a%20Python%20interface%0Afor%20users%20to%20work%20with.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17437v1&entry.124074799=Read"},
{"title": "Inter and Intra Prior Learning-based Hyperspectral Image Reconstruction\n  Using Snapshot SWIR Metasurface", "author": "Linqiang Li and Jinglei Hao and Yongqiang Zhao and Pan Liu and Haofang Yan and Ziqin Zhang and Seong G. Kong", "abstract": "  Shortwave-infrared(SWIR) spectral information, ranging from 1 {\\mu}m to\n2.5{\\mu}m, overcomes the limitations of traditional color cameras in acquiring\nscene information. However, conventional SWIR hyperspectral imaging systems\nface challenges due to their bulky setups and low acquisition speeds. This work\nintroduces a snapshot SWIR hyperspectral imaging system based on a metasurface\nfilter and a corresponding filter selection method to achieve the lowest\ncorrelation coefficient among these filters. This system offers the advantages\nof compact size and snapshot imaging. We propose a novel inter and intra prior\nlearning unfolding framework to achieve high-quality SWIR hyperspectral image\nreconstruction, which bridges the gap between prior learning and cross-stage\ninformation interaction. Additionally, We design an adaptive feature transfer\nmechanism to adaptively transfer the contextual correlation of multi-scale\nencoder features to prevent detailed information loss in the decoder.\nExperiment results demonstrate that our method can reconstruct hyperspectral\nimages with high speed and superior performance over existing methods.\n", "link": "http://arxiv.org/abs/2407.07503v3", "date": "2024-07-24", "relevancy": 2.0357, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5182}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5055}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inter%20and%20Intra%20Prior%20Learning-based%20Hyperspectral%20Image%20Reconstruction%0A%20%20Using%20Snapshot%20SWIR%20Metasurface&body=Title%3A%20Inter%20and%20Intra%20Prior%20Learning-based%20Hyperspectral%20Image%20Reconstruction%0A%20%20Using%20Snapshot%20SWIR%20Metasurface%0AAuthor%3A%20Linqiang%20Li%20and%20Jinglei%20Hao%20and%20Yongqiang%20Zhao%20and%20Pan%20Liu%20and%20Haofang%20Yan%20and%20Ziqin%20Zhang%20and%20Seong%20G.%20Kong%0AAbstract%3A%20%20%20Shortwave-infrared%28SWIR%29%20spectral%20information%2C%20ranging%20from%201%20%7B%5Cmu%7Dm%20to%0A2.5%7B%5Cmu%7Dm%2C%20overcomes%20the%20limitations%20of%20traditional%20color%20cameras%20in%20acquiring%0Ascene%20information.%20However%2C%20conventional%20SWIR%20hyperspectral%20imaging%20systems%0Aface%20challenges%20due%20to%20their%20bulky%20setups%20and%20low%20acquisition%20speeds.%20This%20work%0Aintroduces%20a%20snapshot%20SWIR%20hyperspectral%20imaging%20system%20based%20on%20a%20metasurface%0Afilter%20and%20a%20corresponding%20filter%20selection%20method%20to%20achieve%20the%20lowest%0Acorrelation%20coefficient%20among%20these%20filters.%20This%20system%20offers%20the%20advantages%0Aof%20compact%20size%20and%20snapshot%20imaging.%20We%20propose%20a%20novel%20inter%20and%20intra%20prior%0Alearning%20unfolding%20framework%20to%20achieve%20high-quality%20SWIR%20hyperspectral%20image%0Areconstruction%2C%20which%20bridges%20the%20gap%20between%20prior%20learning%20and%20cross-stage%0Ainformation%20interaction.%20Additionally%2C%20We%20design%20an%20adaptive%20feature%20transfer%0Amechanism%20to%20adaptively%20transfer%20the%20contextual%20correlation%20of%20multi-scale%0Aencoder%20features%20to%20prevent%20detailed%20information%20loss%20in%20the%20decoder.%0AExperiment%20results%20demonstrate%20that%20our%20method%20can%20reconstruct%20hyperspectral%0Aimages%20with%20high%20speed%20and%20superior%20performance%20over%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07503v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInter%2520and%2520Intra%2520Prior%2520Learning-based%2520Hyperspectral%2520Image%2520Reconstruction%250A%2520%2520Using%2520Snapshot%2520SWIR%2520Metasurface%26entry.906535625%3DLinqiang%2520Li%2520and%2520Jinglei%2520Hao%2520and%2520Yongqiang%2520Zhao%2520and%2520Pan%2520Liu%2520and%2520Haofang%2520Yan%2520and%2520Ziqin%2520Zhang%2520and%2520Seong%2520G.%2520Kong%26entry.1292438233%3D%2520%2520Shortwave-infrared%2528SWIR%2529%2520spectral%2520information%252C%2520ranging%2520from%25201%2520%257B%255Cmu%257Dm%2520to%250A2.5%257B%255Cmu%257Dm%252C%2520overcomes%2520the%2520limitations%2520of%2520traditional%2520color%2520cameras%2520in%2520acquiring%250Ascene%2520information.%2520However%252C%2520conventional%2520SWIR%2520hyperspectral%2520imaging%2520systems%250Aface%2520challenges%2520due%2520to%2520their%2520bulky%2520setups%2520and%2520low%2520acquisition%2520speeds.%2520This%2520work%250Aintroduces%2520a%2520snapshot%2520SWIR%2520hyperspectral%2520imaging%2520system%2520based%2520on%2520a%2520metasurface%250Afilter%2520and%2520a%2520corresponding%2520filter%2520selection%2520method%2520to%2520achieve%2520the%2520lowest%250Acorrelation%2520coefficient%2520among%2520these%2520filters.%2520This%2520system%2520offers%2520the%2520advantages%250Aof%2520compact%2520size%2520and%2520snapshot%2520imaging.%2520We%2520propose%2520a%2520novel%2520inter%2520and%2520intra%2520prior%250Alearning%2520unfolding%2520framework%2520to%2520achieve%2520high-quality%2520SWIR%2520hyperspectral%2520image%250Areconstruction%252C%2520which%2520bridges%2520the%2520gap%2520between%2520prior%2520learning%2520and%2520cross-stage%250Ainformation%2520interaction.%2520Additionally%252C%2520We%2520design%2520an%2520adaptive%2520feature%2520transfer%250Amechanism%2520to%2520adaptively%2520transfer%2520the%2520contextual%2520correlation%2520of%2520multi-scale%250Aencoder%2520features%2520to%2520prevent%2520detailed%2520information%2520loss%2520in%2520the%2520decoder.%250AExperiment%2520results%2520demonstrate%2520that%2520our%2520method%2520can%2520reconstruct%2520hyperspectral%250Aimages%2520with%2520high%2520speed%2520and%2520superior%2520performance%2520over%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07503v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inter%20and%20Intra%20Prior%20Learning-based%20Hyperspectral%20Image%20Reconstruction%0A%20%20Using%20Snapshot%20SWIR%20Metasurface&entry.906535625=Linqiang%20Li%20and%20Jinglei%20Hao%20and%20Yongqiang%20Zhao%20and%20Pan%20Liu%20and%20Haofang%20Yan%20and%20Ziqin%20Zhang%20and%20Seong%20G.%20Kong&entry.1292438233=%20%20Shortwave-infrared%28SWIR%29%20spectral%20information%2C%20ranging%20from%201%20%7B%5Cmu%7Dm%20to%0A2.5%7B%5Cmu%7Dm%2C%20overcomes%20the%20limitations%20of%20traditional%20color%20cameras%20in%20acquiring%0Ascene%20information.%20However%2C%20conventional%20SWIR%20hyperspectral%20imaging%20systems%0Aface%20challenges%20due%20to%20their%20bulky%20setups%20and%20low%20acquisition%20speeds.%20This%20work%0Aintroduces%20a%20snapshot%20SWIR%20hyperspectral%20imaging%20system%20based%20on%20a%20metasurface%0Afilter%20and%20a%20corresponding%20filter%20selection%20method%20to%20achieve%20the%20lowest%0Acorrelation%20coefficient%20among%20these%20filters.%20This%20system%20offers%20the%20advantages%0Aof%20compact%20size%20and%20snapshot%20imaging.%20We%20propose%20a%20novel%20inter%20and%20intra%20prior%0Alearning%20unfolding%20framework%20to%20achieve%20high-quality%20SWIR%20hyperspectral%20image%0Areconstruction%2C%20which%20bridges%20the%20gap%20between%20prior%20learning%20and%20cross-stage%0Ainformation%20interaction.%20Additionally%2C%20We%20design%20an%20adaptive%20feature%20transfer%0Amechanism%20to%20adaptively%20transfer%20the%20contextual%20correlation%20of%20multi-scale%0Aencoder%20features%20to%20prevent%20detailed%20information%20loss%20in%20the%20decoder.%0AExperiment%20results%20demonstrate%20that%20our%20method%20can%20reconstruct%20hyperspectral%0Aimages%20with%20high%20speed%20and%20superior%20performance%20over%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07503v3&entry.124074799=Read"},
{"title": "Dependency Transformer Grammars: Integrating Dependency Structures into\n  Transformer Language Models", "author": "Yida Zhao and Chao Lou and Kewei Tu", "abstract": "  Syntactic Transformer language models aim to achieve better generalization\nthrough simultaneously modeling syntax trees and sentences. While prior work\nhas been focusing on adding constituency-based structures to Transformers, we\nintroduce Dependency Transformer Grammars (DTGs), a new class of Transformer\nlanguage model with explicit dependency-based inductive bias. DTGs simulate\ndependency transition systems with constrained attention patterns by modifying\nattention masks, incorporate the stack information through relative positional\nencoding, and augment dependency arc representation with a combination of token\nembeddings and operation embeddings. When trained on a dataset of sentences\nannotated with dependency trees, DTGs achieve better generalization while\nmaintaining comparable perplexity with Transformer language model baselines.\nDTGs also outperform recent constituency-based models, showing that dependency\ncan better guide Transformer language models. Our code is released at\nhttps://github.com/zhaoyd1/Dep_Transformer_Grammars.\n", "link": "http://arxiv.org/abs/2407.17406v1", "date": "2024-07-24", "relevancy": 2.0288, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5488}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.505}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dependency%20Transformer%20Grammars%3A%20Integrating%20Dependency%20Structures%20into%0A%20%20Transformer%20Language%20Models&body=Title%3A%20Dependency%20Transformer%20Grammars%3A%20Integrating%20Dependency%20Structures%20into%0A%20%20Transformer%20Language%20Models%0AAuthor%3A%20Yida%20Zhao%20and%20Chao%20Lou%20and%20Kewei%20Tu%0AAbstract%3A%20%20%20Syntactic%20Transformer%20language%20models%20aim%20to%20achieve%20better%20generalization%0Athrough%20simultaneously%20modeling%20syntax%20trees%20and%20sentences.%20While%20prior%20work%0Ahas%20been%20focusing%20on%20adding%20constituency-based%20structures%20to%20Transformers%2C%20we%0Aintroduce%20Dependency%20Transformer%20Grammars%20%28DTGs%29%2C%20a%20new%20class%20of%20Transformer%0Alanguage%20model%20with%20explicit%20dependency-based%20inductive%20bias.%20DTGs%20simulate%0Adependency%20transition%20systems%20with%20constrained%20attention%20patterns%20by%20modifying%0Aattention%20masks%2C%20incorporate%20the%20stack%20information%20through%20relative%20positional%0Aencoding%2C%20and%20augment%20dependency%20arc%20representation%20with%20a%20combination%20of%20token%0Aembeddings%20and%20operation%20embeddings.%20When%20trained%20on%20a%20dataset%20of%20sentences%0Aannotated%20with%20dependency%20trees%2C%20DTGs%20achieve%20better%20generalization%20while%0Amaintaining%20comparable%20perplexity%20with%20Transformer%20language%20model%20baselines.%0ADTGs%20also%20outperform%20recent%20constituency-based%20models%2C%20showing%20that%20dependency%0Acan%20better%20guide%20Transformer%20language%20models.%20Our%20code%20is%20released%20at%0Ahttps%3A//github.com/zhaoyd1/Dep_Transformer_Grammars.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17406v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDependency%2520Transformer%2520Grammars%253A%2520Integrating%2520Dependency%2520Structures%2520into%250A%2520%2520Transformer%2520Language%2520Models%26entry.906535625%3DYida%2520Zhao%2520and%2520Chao%2520Lou%2520and%2520Kewei%2520Tu%26entry.1292438233%3D%2520%2520Syntactic%2520Transformer%2520language%2520models%2520aim%2520to%2520achieve%2520better%2520generalization%250Athrough%2520simultaneously%2520modeling%2520syntax%2520trees%2520and%2520sentences.%2520While%2520prior%2520work%250Ahas%2520been%2520focusing%2520on%2520adding%2520constituency-based%2520structures%2520to%2520Transformers%252C%2520we%250Aintroduce%2520Dependency%2520Transformer%2520Grammars%2520%2528DTGs%2529%252C%2520a%2520new%2520class%2520of%2520Transformer%250Alanguage%2520model%2520with%2520explicit%2520dependency-based%2520inductive%2520bias.%2520DTGs%2520simulate%250Adependency%2520transition%2520systems%2520with%2520constrained%2520attention%2520patterns%2520by%2520modifying%250Aattention%2520masks%252C%2520incorporate%2520the%2520stack%2520information%2520through%2520relative%2520positional%250Aencoding%252C%2520and%2520augment%2520dependency%2520arc%2520representation%2520with%2520a%2520combination%2520of%2520token%250Aembeddings%2520and%2520operation%2520embeddings.%2520When%2520trained%2520on%2520a%2520dataset%2520of%2520sentences%250Aannotated%2520with%2520dependency%2520trees%252C%2520DTGs%2520achieve%2520better%2520generalization%2520while%250Amaintaining%2520comparable%2520perplexity%2520with%2520Transformer%2520language%2520model%2520baselines.%250ADTGs%2520also%2520outperform%2520recent%2520constituency-based%2520models%252C%2520showing%2520that%2520dependency%250Acan%2520better%2520guide%2520Transformer%2520language%2520models.%2520Our%2520code%2520is%2520released%2520at%250Ahttps%253A//github.com/zhaoyd1/Dep_Transformer_Grammars.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17406v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dependency%20Transformer%20Grammars%3A%20Integrating%20Dependency%20Structures%20into%0A%20%20Transformer%20Language%20Models&entry.906535625=Yida%20Zhao%20and%20Chao%20Lou%20and%20Kewei%20Tu&entry.1292438233=%20%20Syntactic%20Transformer%20language%20models%20aim%20to%20achieve%20better%20generalization%0Athrough%20simultaneously%20modeling%20syntax%20trees%20and%20sentences.%20While%20prior%20work%0Ahas%20been%20focusing%20on%20adding%20constituency-based%20structures%20to%20Transformers%2C%20we%0Aintroduce%20Dependency%20Transformer%20Grammars%20%28DTGs%29%2C%20a%20new%20class%20of%20Transformer%0Alanguage%20model%20with%20explicit%20dependency-based%20inductive%20bias.%20DTGs%20simulate%0Adependency%20transition%20systems%20with%20constrained%20attention%20patterns%20by%20modifying%0Aattention%20masks%2C%20incorporate%20the%20stack%20information%20through%20relative%20positional%0Aencoding%2C%20and%20augment%20dependency%20arc%20representation%20with%20a%20combination%20of%20token%0Aembeddings%20and%20operation%20embeddings.%20When%20trained%20on%20a%20dataset%20of%20sentences%0Aannotated%20with%20dependency%20trees%2C%20DTGs%20achieve%20better%20generalization%20while%0Amaintaining%20comparable%20perplexity%20with%20Transformer%20language%20model%20baselines.%0ADTGs%20also%20outperform%20recent%20constituency-based%20models%2C%20showing%20that%20dependency%0Acan%20better%20guide%20Transformer%20language%20models.%20Our%20code%20is%20released%20at%0Ahttps%3A//github.com/zhaoyd1/Dep_Transformer_Grammars.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17406v1&entry.124074799=Read"},
{"title": "Discovering Dynamic Symbolic Policies with Genetic Programming", "author": "Sigur de Vries and Sander Keemink and Marcel van Gerven", "abstract": "  Artificial intelligence techniques are increasingly being applied to solve\ncontrol problems, but often rely on black-box methods without transparent\noutput generation. To improve the interpretability and transparency in control\nsystems, models can be defined as white-box symbolic policies described by\nmathematical expressions. While current approaches to learn symbolic policies\nfocus on static policies that directly map observations to control signals,\nthese may fail in partially observable and volatile environments. We instead\nconsider dynamic symbolic policies with memory, optimised with genetic\nprogramming. The resulting policies are robust, and consist of easy to\ninterpret coupled differential equations. Our results show that dynamic\nsymbolic policies compare with black-box policies on a variety of control\ntasks. Furthermore, the benefit of the memory in dynamic policies is\ndemonstrated on experiments where static policies fall short. Overall, we\npresent a method for evolving high-performing symbolic policies that offer\ninterpretability and transparency, which lacks in black-box models.\n", "link": "http://arxiv.org/abs/2406.02765v3", "date": "2024-07-24", "relevancy": 2.024, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5341}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4984}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discovering%20Dynamic%20Symbolic%20Policies%20with%20Genetic%20Programming&body=Title%3A%20Discovering%20Dynamic%20Symbolic%20Policies%20with%20Genetic%20Programming%0AAuthor%3A%20Sigur%20de%20Vries%20and%20Sander%20Keemink%20and%20Marcel%20van%20Gerven%0AAbstract%3A%20%20%20Artificial%20intelligence%20techniques%20are%20increasingly%20being%20applied%20to%20solve%0Acontrol%20problems%2C%20but%20often%20rely%20on%20black-box%20methods%20without%20transparent%0Aoutput%20generation.%20To%20improve%20the%20interpretability%20and%20transparency%20in%20control%0Asystems%2C%20models%20can%20be%20defined%20as%20white-box%20symbolic%20policies%20described%20by%0Amathematical%20expressions.%20While%20current%20approaches%20to%20learn%20symbolic%20policies%0Afocus%20on%20static%20policies%20that%20directly%20map%20observations%20to%20control%20signals%2C%0Athese%20may%20fail%20in%20partially%20observable%20and%20volatile%20environments.%20We%20instead%0Aconsider%20dynamic%20symbolic%20policies%20with%20memory%2C%20optimised%20with%20genetic%0Aprogramming.%20The%20resulting%20policies%20are%20robust%2C%20and%20consist%20of%20easy%20to%0Ainterpret%20coupled%20differential%20equations.%20Our%20results%20show%20that%20dynamic%0Asymbolic%20policies%20compare%20with%20black-box%20policies%20on%20a%20variety%20of%20control%0Atasks.%20Furthermore%2C%20the%20benefit%20of%20the%20memory%20in%20dynamic%20policies%20is%0Ademonstrated%20on%20experiments%20where%20static%20policies%20fall%20short.%20Overall%2C%20we%0Apresent%20a%20method%20for%20evolving%20high-performing%20symbolic%20policies%20that%20offer%0Ainterpretability%20and%20transparency%2C%20which%20lacks%20in%20black-box%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02765v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscovering%2520Dynamic%2520Symbolic%2520Policies%2520with%2520Genetic%2520Programming%26entry.906535625%3DSigur%2520de%2520Vries%2520and%2520Sander%2520Keemink%2520and%2520Marcel%2520van%2520Gerven%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520techniques%2520are%2520increasingly%2520being%2520applied%2520to%2520solve%250Acontrol%2520problems%252C%2520but%2520often%2520rely%2520on%2520black-box%2520methods%2520without%2520transparent%250Aoutput%2520generation.%2520To%2520improve%2520the%2520interpretability%2520and%2520transparency%2520in%2520control%250Asystems%252C%2520models%2520can%2520be%2520defined%2520as%2520white-box%2520symbolic%2520policies%2520described%2520by%250Amathematical%2520expressions.%2520While%2520current%2520approaches%2520to%2520learn%2520symbolic%2520policies%250Afocus%2520on%2520static%2520policies%2520that%2520directly%2520map%2520observations%2520to%2520control%2520signals%252C%250Athese%2520may%2520fail%2520in%2520partially%2520observable%2520and%2520volatile%2520environments.%2520We%2520instead%250Aconsider%2520dynamic%2520symbolic%2520policies%2520with%2520memory%252C%2520optimised%2520with%2520genetic%250Aprogramming.%2520The%2520resulting%2520policies%2520are%2520robust%252C%2520and%2520consist%2520of%2520easy%2520to%250Ainterpret%2520coupled%2520differential%2520equations.%2520Our%2520results%2520show%2520that%2520dynamic%250Asymbolic%2520policies%2520compare%2520with%2520black-box%2520policies%2520on%2520a%2520variety%2520of%2520control%250Atasks.%2520Furthermore%252C%2520the%2520benefit%2520of%2520the%2520memory%2520in%2520dynamic%2520policies%2520is%250Ademonstrated%2520on%2520experiments%2520where%2520static%2520policies%2520fall%2520short.%2520Overall%252C%2520we%250Apresent%2520a%2520method%2520for%2520evolving%2520high-performing%2520symbolic%2520policies%2520that%2520offer%250Ainterpretability%2520and%2520transparency%252C%2520which%2520lacks%2520in%2520black-box%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02765v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discovering%20Dynamic%20Symbolic%20Policies%20with%20Genetic%20Programming&entry.906535625=Sigur%20de%20Vries%20and%20Sander%20Keemink%20and%20Marcel%20van%20Gerven&entry.1292438233=%20%20Artificial%20intelligence%20techniques%20are%20increasingly%20being%20applied%20to%20solve%0Acontrol%20problems%2C%20but%20often%20rely%20on%20black-box%20methods%20without%20transparent%0Aoutput%20generation.%20To%20improve%20the%20interpretability%20and%20transparency%20in%20control%0Asystems%2C%20models%20can%20be%20defined%20as%20white-box%20symbolic%20policies%20described%20by%0Amathematical%20expressions.%20While%20current%20approaches%20to%20learn%20symbolic%20policies%0Afocus%20on%20static%20policies%20that%20directly%20map%20observations%20to%20control%20signals%2C%0Athese%20may%20fail%20in%20partially%20observable%20and%20volatile%20environments.%20We%20instead%0Aconsider%20dynamic%20symbolic%20policies%20with%20memory%2C%20optimised%20with%20genetic%0Aprogramming.%20The%20resulting%20policies%20are%20robust%2C%20and%20consist%20of%20easy%20to%0Ainterpret%20coupled%20differential%20equations.%20Our%20results%20show%20that%20dynamic%0Asymbolic%20policies%20compare%20with%20black-box%20policies%20on%20a%20variety%20of%20control%0Atasks.%20Furthermore%2C%20the%20benefit%20of%20the%20memory%20in%20dynamic%20policies%20is%0Ademonstrated%20on%20experiments%20where%20static%20policies%20fall%20short.%20Overall%2C%20we%0Apresent%20a%20method%20for%20evolving%20high-performing%20symbolic%20policies%20that%20offer%0Ainterpretability%20and%20transparency%2C%20which%20lacks%20in%20black-box%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02765v3&entry.124074799=Read"},
{"title": "A Novel Two-Step Fine-Tuning Pipeline for Cold-Start Active Learning in\n  Text Classification Tasks", "author": "Fabiano Bel\u00e9m and Washington Cunha and Celso Fran\u00e7a and Claudio Andrade and Leonardo Rocha and Marcos Andr\u00e9 Gon\u00e7alves", "abstract": "  This is the first work to investigate the effectiveness of BERT-based\ncontextual embeddings in active learning (AL) tasks on cold-start scenarios,\nwhere traditional fine-tuning is infeasible due to the absence of labeled data.\nOur primary contribution is the proposal of a more robust fine-tuning pipeline\n- DoTCAL - that diminishes the reliance on labeled data in AL using two steps:\n(1) fully leveraging unlabeled data through domain adaptation of the embeddings\nvia masked language modeling and (2) further adjusting model weights using\nlabeled data selected by AL. Our evaluation contrasts BERT-based embeddings\nwith other prevalent text representation paradigms, including Bag of Words\n(BoW), Latent Semantic Indexing (LSI), and FastText, at two critical stages of\nthe AL process: instance selection and classification. Experiments conducted on\neight ATC benchmarks with varying AL budgets (number of labeled instances) and\nnumber of instances (about 5,000 to 300,000) demonstrate DoTCAL's superior\neffectiveness, achieving up to a 33% improvement in Macro-F1 while reducing\nlabeling efforts by half compared to the traditional one-step method. We also\nfound that in several tasks, BoW and LSI (due to information aggregation)\nproduce results superior (up to 59% ) to BERT, especially in low-budget\nscenarios and hard-to-classify tasks, which is quite surprising.\n", "link": "http://arxiv.org/abs/2407.17284v1", "date": "2024-07-24", "relevancy": 2.0222, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5156}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.507}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Two-Step%20Fine-Tuning%20Pipeline%20for%20Cold-Start%20Active%20Learning%20in%0A%20%20Text%20Classification%20Tasks&body=Title%3A%20A%20Novel%20Two-Step%20Fine-Tuning%20Pipeline%20for%20Cold-Start%20Active%20Learning%20in%0A%20%20Text%20Classification%20Tasks%0AAuthor%3A%20Fabiano%20Bel%C3%A9m%20and%20Washington%20Cunha%20and%20Celso%20Fran%C3%A7a%20and%20Claudio%20Andrade%20and%20Leonardo%20Rocha%20and%20Marcos%20Andr%C3%A9%20Gon%C3%A7alves%0AAbstract%3A%20%20%20This%20is%20the%20first%20work%20to%20investigate%20the%20effectiveness%20of%20BERT-based%0Acontextual%20embeddings%20in%20active%20learning%20%28AL%29%20tasks%20on%20cold-start%20scenarios%2C%0Awhere%20traditional%20fine-tuning%20is%20infeasible%20due%20to%20the%20absence%20of%20labeled%20data.%0AOur%20primary%20contribution%20is%20the%20proposal%20of%20a%20more%20robust%20fine-tuning%20pipeline%0A-%20DoTCAL%20-%20that%20diminishes%20the%20reliance%20on%20labeled%20data%20in%20AL%20using%20two%20steps%3A%0A%281%29%20fully%20leveraging%20unlabeled%20data%20through%20domain%20adaptation%20of%20the%20embeddings%0Avia%20masked%20language%20modeling%20and%20%282%29%20further%20adjusting%20model%20weights%20using%0Alabeled%20data%20selected%20by%20AL.%20Our%20evaluation%20contrasts%20BERT-based%20embeddings%0Awith%20other%20prevalent%20text%20representation%20paradigms%2C%20including%20Bag%20of%20Words%0A%28BoW%29%2C%20Latent%20Semantic%20Indexing%20%28LSI%29%2C%20and%20FastText%2C%20at%20two%20critical%20stages%20of%0Athe%20AL%20process%3A%20instance%20selection%20and%20classification.%20Experiments%20conducted%20on%0Aeight%20ATC%20benchmarks%20with%20varying%20AL%20budgets%20%28number%20of%20labeled%20instances%29%20and%0Anumber%20of%20instances%20%28about%205%2C000%20to%20300%2C000%29%20demonstrate%20DoTCAL%27s%20superior%0Aeffectiveness%2C%20achieving%20up%20to%20a%2033%25%20improvement%20in%20Macro-F1%20while%20reducing%0Alabeling%20efforts%20by%20half%20compared%20to%20the%20traditional%20one-step%20method.%20We%20also%0Afound%20that%20in%20several%20tasks%2C%20BoW%20and%20LSI%20%28due%20to%20information%20aggregation%29%0Aproduce%20results%20superior%20%28up%20to%2059%25%20%29%20to%20BERT%2C%20especially%20in%20low-budget%0Ascenarios%20and%20hard-to-classify%20tasks%2C%20which%20is%20quite%20surprising.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17284v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Two-Step%2520Fine-Tuning%2520Pipeline%2520for%2520Cold-Start%2520Active%2520Learning%2520in%250A%2520%2520Text%2520Classification%2520Tasks%26entry.906535625%3DFabiano%2520Bel%25C3%25A9m%2520and%2520Washington%2520Cunha%2520and%2520Celso%2520Fran%25C3%25A7a%2520and%2520Claudio%2520Andrade%2520and%2520Leonardo%2520Rocha%2520and%2520Marcos%2520Andr%25C3%25A9%2520Gon%25C3%25A7alves%26entry.1292438233%3D%2520%2520This%2520is%2520the%2520first%2520work%2520to%2520investigate%2520the%2520effectiveness%2520of%2520BERT-based%250Acontextual%2520embeddings%2520in%2520active%2520learning%2520%2528AL%2529%2520tasks%2520on%2520cold-start%2520scenarios%252C%250Awhere%2520traditional%2520fine-tuning%2520is%2520infeasible%2520due%2520to%2520the%2520absence%2520of%2520labeled%2520data.%250AOur%2520primary%2520contribution%2520is%2520the%2520proposal%2520of%2520a%2520more%2520robust%2520fine-tuning%2520pipeline%250A-%2520DoTCAL%2520-%2520that%2520diminishes%2520the%2520reliance%2520on%2520labeled%2520data%2520in%2520AL%2520using%2520two%2520steps%253A%250A%25281%2529%2520fully%2520leveraging%2520unlabeled%2520data%2520through%2520domain%2520adaptation%2520of%2520the%2520embeddings%250Avia%2520masked%2520language%2520modeling%2520and%2520%25282%2529%2520further%2520adjusting%2520model%2520weights%2520using%250Alabeled%2520data%2520selected%2520by%2520AL.%2520Our%2520evaluation%2520contrasts%2520BERT-based%2520embeddings%250Awith%2520other%2520prevalent%2520text%2520representation%2520paradigms%252C%2520including%2520Bag%2520of%2520Words%250A%2528BoW%2529%252C%2520Latent%2520Semantic%2520Indexing%2520%2528LSI%2529%252C%2520and%2520FastText%252C%2520at%2520two%2520critical%2520stages%2520of%250Athe%2520AL%2520process%253A%2520instance%2520selection%2520and%2520classification.%2520Experiments%2520conducted%2520on%250Aeight%2520ATC%2520benchmarks%2520with%2520varying%2520AL%2520budgets%2520%2528number%2520of%2520labeled%2520instances%2529%2520and%250Anumber%2520of%2520instances%2520%2528about%25205%252C000%2520to%2520300%252C000%2529%2520demonstrate%2520DoTCAL%2527s%2520superior%250Aeffectiveness%252C%2520achieving%2520up%2520to%2520a%252033%2525%2520improvement%2520in%2520Macro-F1%2520while%2520reducing%250Alabeling%2520efforts%2520by%2520half%2520compared%2520to%2520the%2520traditional%2520one-step%2520method.%2520We%2520also%250Afound%2520that%2520in%2520several%2520tasks%252C%2520BoW%2520and%2520LSI%2520%2528due%2520to%2520information%2520aggregation%2529%250Aproduce%2520results%2520superior%2520%2528up%2520to%252059%2525%2520%2529%2520to%2520BERT%252C%2520especially%2520in%2520low-budget%250Ascenarios%2520and%2520hard-to-classify%2520tasks%252C%2520which%2520is%2520quite%2520surprising.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17284v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Two-Step%20Fine-Tuning%20Pipeline%20for%20Cold-Start%20Active%20Learning%20in%0A%20%20Text%20Classification%20Tasks&entry.906535625=Fabiano%20Bel%C3%A9m%20and%20Washington%20Cunha%20and%20Celso%20Fran%C3%A7a%20and%20Claudio%20Andrade%20and%20Leonardo%20Rocha%20and%20Marcos%20Andr%C3%A9%20Gon%C3%A7alves&entry.1292438233=%20%20This%20is%20the%20first%20work%20to%20investigate%20the%20effectiveness%20of%20BERT-based%0Acontextual%20embeddings%20in%20active%20learning%20%28AL%29%20tasks%20on%20cold-start%20scenarios%2C%0Awhere%20traditional%20fine-tuning%20is%20infeasible%20due%20to%20the%20absence%20of%20labeled%20data.%0AOur%20primary%20contribution%20is%20the%20proposal%20of%20a%20more%20robust%20fine-tuning%20pipeline%0A-%20DoTCAL%20-%20that%20diminishes%20the%20reliance%20on%20labeled%20data%20in%20AL%20using%20two%20steps%3A%0A%281%29%20fully%20leveraging%20unlabeled%20data%20through%20domain%20adaptation%20of%20the%20embeddings%0Avia%20masked%20language%20modeling%20and%20%282%29%20further%20adjusting%20model%20weights%20using%0Alabeled%20data%20selected%20by%20AL.%20Our%20evaluation%20contrasts%20BERT-based%20embeddings%0Awith%20other%20prevalent%20text%20representation%20paradigms%2C%20including%20Bag%20of%20Words%0A%28BoW%29%2C%20Latent%20Semantic%20Indexing%20%28LSI%29%2C%20and%20FastText%2C%20at%20two%20critical%20stages%20of%0Athe%20AL%20process%3A%20instance%20selection%20and%20classification.%20Experiments%20conducted%20on%0Aeight%20ATC%20benchmarks%20with%20varying%20AL%20budgets%20%28number%20of%20labeled%20instances%29%20and%0Anumber%20of%20instances%20%28about%205%2C000%20to%20300%2C000%29%20demonstrate%20DoTCAL%27s%20superior%0Aeffectiveness%2C%20achieving%20up%20to%20a%2033%25%20improvement%20in%20Macro-F1%20while%20reducing%0Alabeling%20efforts%20by%20half%20compared%20to%20the%20traditional%20one-step%20method.%20We%20also%0Afound%20that%20in%20several%20tasks%2C%20BoW%20and%20LSI%20%28due%20to%20information%20aggregation%29%0Aproduce%20results%20superior%20%28up%20to%2059%25%20%29%20to%20BERT%2C%20especially%20in%20low-budget%0Ascenarios%20and%20hard-to-classify%20tasks%2C%20which%20is%20quite%20surprising.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17284v1&entry.124074799=Read"},
{"title": "When Does Bottom-up Beat Top-down in Hierarchical Community Detection?", "author": "Maximilien Dreveton and Daichi Kuroda and Matthias Grossglauser and Patrick Thiran", "abstract": "  Hierarchical clustering of networks consists in finding a tree of\ncommunities, such that lower levels of the hierarchy reveal finer-grained\ncommunity structures. There are two main classes of algorithms tackling this\nproblem. Divisive ($\\textit{top-down}$) algorithms recursively partition the\nnodes into two communities, until a stopping rule indicates that no further\nsplit is needed. In contrast, agglomerative ($\\textit{bottom-up}$) algorithms\nfirst identify the smallest community structure and then repeatedly merge the\ncommunities using a $\\textit{linkage}$ method. In this article, we establish\ntheoretical guarantees for the recovery of the hierarchical tree and community\nstructure of a Hierarchical Stochastic Block Model by a bottom-up algorithm. We\nalso establish that this bottom-up algorithm attains the information-theoretic\nthreshold for exact recovery at intermediate levels of the hierarchy. Notably,\nthese recovery conditions are less restrictive compared to those existing for\ntop-down algorithms. This shows that bottom-up algorithms extend the feasible\nregion for achieving exact recovery at intermediate levels. Numerical\nexperiments on both synthetic and real data sets confirm the superiority of\nbottom-up algorithms over top-down algorithms. We also observe that top-down\nalgorithms can produce dendrograms with inversions. These findings contribute\nto a better understanding of hierarchical clustering techniques and their\napplications in network analysis.\n", "link": "http://arxiv.org/abs/2306.00833v2", "date": "2024-07-24", "relevancy": 2.016, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4229}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4109}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.3759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Does%20Bottom-up%20Beat%20Top-down%20in%20Hierarchical%20Community%20Detection%3F&body=Title%3A%20When%20Does%20Bottom-up%20Beat%20Top-down%20in%20Hierarchical%20Community%20Detection%3F%0AAuthor%3A%20Maximilien%20Dreveton%20and%20Daichi%20Kuroda%20and%20Matthias%20Grossglauser%20and%20Patrick%20Thiran%0AAbstract%3A%20%20%20Hierarchical%20clustering%20of%20networks%20consists%20in%20finding%20a%20tree%20of%0Acommunities%2C%20such%20that%20lower%20levels%20of%20the%20hierarchy%20reveal%20finer-grained%0Acommunity%20structures.%20There%20are%20two%20main%20classes%20of%20algorithms%20tackling%20this%0Aproblem.%20Divisive%20%28%24%5Ctextit%7Btop-down%7D%24%29%20algorithms%20recursively%20partition%20the%0Anodes%20into%20two%20communities%2C%20until%20a%20stopping%20rule%20indicates%20that%20no%20further%0Asplit%20is%20needed.%20In%20contrast%2C%20agglomerative%20%28%24%5Ctextit%7Bbottom-up%7D%24%29%20algorithms%0Afirst%20identify%20the%20smallest%20community%20structure%20and%20then%20repeatedly%20merge%20the%0Acommunities%20using%20a%20%24%5Ctextit%7Blinkage%7D%24%20method.%20In%20this%20article%2C%20we%20establish%0Atheoretical%20guarantees%20for%20the%20recovery%20of%20the%20hierarchical%20tree%20and%20community%0Astructure%20of%20a%20Hierarchical%20Stochastic%20Block%20Model%20by%20a%20bottom-up%20algorithm.%20We%0Aalso%20establish%20that%20this%20bottom-up%20algorithm%20attains%20the%20information-theoretic%0Athreshold%20for%20exact%20recovery%20at%20intermediate%20levels%20of%20the%20hierarchy.%20Notably%2C%0Athese%20recovery%20conditions%20are%20less%20restrictive%20compared%20to%20those%20existing%20for%0Atop-down%20algorithms.%20This%20shows%20that%20bottom-up%20algorithms%20extend%20the%20feasible%0Aregion%20for%20achieving%20exact%20recovery%20at%20intermediate%20levels.%20Numerical%0Aexperiments%20on%20both%20synthetic%20and%20real%20data%20sets%20confirm%20the%20superiority%20of%0Abottom-up%20algorithms%20over%20top-down%20algorithms.%20We%20also%20observe%20that%20top-down%0Aalgorithms%20can%20produce%20dendrograms%20with%20inversions.%20These%20findings%20contribute%0Ato%20a%20better%20understanding%20of%20hierarchical%20clustering%20techniques%20and%20their%0Aapplications%20in%20network%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.00833v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Does%2520Bottom-up%2520Beat%2520Top-down%2520in%2520Hierarchical%2520Community%2520Detection%253F%26entry.906535625%3DMaximilien%2520Dreveton%2520and%2520Daichi%2520Kuroda%2520and%2520Matthias%2520Grossglauser%2520and%2520Patrick%2520Thiran%26entry.1292438233%3D%2520%2520Hierarchical%2520clustering%2520of%2520networks%2520consists%2520in%2520finding%2520a%2520tree%2520of%250Acommunities%252C%2520such%2520that%2520lower%2520levels%2520of%2520the%2520hierarchy%2520reveal%2520finer-grained%250Acommunity%2520structures.%2520There%2520are%2520two%2520main%2520classes%2520of%2520algorithms%2520tackling%2520this%250Aproblem.%2520Divisive%2520%2528%2524%255Ctextit%257Btop-down%257D%2524%2529%2520algorithms%2520recursively%2520partition%2520the%250Anodes%2520into%2520two%2520communities%252C%2520until%2520a%2520stopping%2520rule%2520indicates%2520that%2520no%2520further%250Asplit%2520is%2520needed.%2520In%2520contrast%252C%2520agglomerative%2520%2528%2524%255Ctextit%257Bbottom-up%257D%2524%2529%2520algorithms%250Afirst%2520identify%2520the%2520smallest%2520community%2520structure%2520and%2520then%2520repeatedly%2520merge%2520the%250Acommunities%2520using%2520a%2520%2524%255Ctextit%257Blinkage%257D%2524%2520method.%2520In%2520this%2520article%252C%2520we%2520establish%250Atheoretical%2520guarantees%2520for%2520the%2520recovery%2520of%2520the%2520hierarchical%2520tree%2520and%2520community%250Astructure%2520of%2520a%2520Hierarchical%2520Stochastic%2520Block%2520Model%2520by%2520a%2520bottom-up%2520algorithm.%2520We%250Aalso%2520establish%2520that%2520this%2520bottom-up%2520algorithm%2520attains%2520the%2520information-theoretic%250Athreshold%2520for%2520exact%2520recovery%2520at%2520intermediate%2520levels%2520of%2520the%2520hierarchy.%2520Notably%252C%250Athese%2520recovery%2520conditions%2520are%2520less%2520restrictive%2520compared%2520to%2520those%2520existing%2520for%250Atop-down%2520algorithms.%2520This%2520shows%2520that%2520bottom-up%2520algorithms%2520extend%2520the%2520feasible%250Aregion%2520for%2520achieving%2520exact%2520recovery%2520at%2520intermediate%2520levels.%2520Numerical%250Aexperiments%2520on%2520both%2520synthetic%2520and%2520real%2520data%2520sets%2520confirm%2520the%2520superiority%2520of%250Abottom-up%2520algorithms%2520over%2520top-down%2520algorithms.%2520We%2520also%2520observe%2520that%2520top-down%250Aalgorithms%2520can%2520produce%2520dendrograms%2520with%2520inversions.%2520These%2520findings%2520contribute%250Ato%2520a%2520better%2520understanding%2520of%2520hierarchical%2520clustering%2520techniques%2520and%2520their%250Aapplications%2520in%2520network%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.00833v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Does%20Bottom-up%20Beat%20Top-down%20in%20Hierarchical%20Community%20Detection%3F&entry.906535625=Maximilien%20Dreveton%20and%20Daichi%20Kuroda%20and%20Matthias%20Grossglauser%20and%20Patrick%20Thiran&entry.1292438233=%20%20Hierarchical%20clustering%20of%20networks%20consists%20in%20finding%20a%20tree%20of%0Acommunities%2C%20such%20that%20lower%20levels%20of%20the%20hierarchy%20reveal%20finer-grained%0Acommunity%20structures.%20There%20are%20two%20main%20classes%20of%20algorithms%20tackling%20this%0Aproblem.%20Divisive%20%28%24%5Ctextit%7Btop-down%7D%24%29%20algorithms%20recursively%20partition%20the%0Anodes%20into%20two%20communities%2C%20until%20a%20stopping%20rule%20indicates%20that%20no%20further%0Asplit%20is%20needed.%20In%20contrast%2C%20agglomerative%20%28%24%5Ctextit%7Bbottom-up%7D%24%29%20algorithms%0Afirst%20identify%20the%20smallest%20community%20structure%20and%20then%20repeatedly%20merge%20the%0Acommunities%20using%20a%20%24%5Ctextit%7Blinkage%7D%24%20method.%20In%20this%20article%2C%20we%20establish%0Atheoretical%20guarantees%20for%20the%20recovery%20of%20the%20hierarchical%20tree%20and%20community%0Astructure%20of%20a%20Hierarchical%20Stochastic%20Block%20Model%20by%20a%20bottom-up%20algorithm.%20We%0Aalso%20establish%20that%20this%20bottom-up%20algorithm%20attains%20the%20information-theoretic%0Athreshold%20for%20exact%20recovery%20at%20intermediate%20levels%20of%20the%20hierarchy.%20Notably%2C%0Athese%20recovery%20conditions%20are%20less%20restrictive%20compared%20to%20those%20existing%20for%0Atop-down%20algorithms.%20This%20shows%20that%20bottom-up%20algorithms%20extend%20the%20feasible%0Aregion%20for%20achieving%20exact%20recovery%20at%20intermediate%20levels.%20Numerical%0Aexperiments%20on%20both%20synthetic%20and%20real%20data%20sets%20confirm%20the%20superiority%20of%0Abottom-up%20algorithms%20over%20top-down%20algorithms.%20We%20also%20observe%20that%20top-down%0Aalgorithms%20can%20produce%20dendrograms%20with%20inversions.%20These%20findings%20contribute%0Ato%20a%20better%20understanding%20of%20hierarchical%20clustering%20techniques%20and%20their%0Aapplications%20in%20network%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.00833v2&entry.124074799=Read"},
{"title": "CMR Scaling Law: Predicting Critical Mixture Ratios for Continual\n  Pre-training of Language Models", "author": "Jiawei Gu and Zacc Yang and Chuanghao Ding and Rui Zhao and Fei Tan", "abstract": "  Large Language Models (LLMs) excel in diverse tasks but often underperform in\nspecialized fields due to limited domain-specific or proprietary corpus.\nContinual pre-training (CPT) enhances LLM capabilities by imbuing new\ndomain-specific or proprietary knowledge while replaying general corpus to\nprevent catastrophic forgetting. The data mixture ratio of general corpus and\ndomain-specific corpus, however, has been chosen heuristically, leading to\nsub-optimal training efficiency in practice. In this context, we attempt to\nre-visit the scaling behavior of LLMs under the hood of CPT, and discover a\npower-law relationship between loss, mixture ratio, and training tokens scale.\nWe formalize the trade-off between general and domain-specific capabilities,\nleading to a well-defined Critical Mixture Ratio (CMR) of general and domain\ndata. By striking the balance, CMR maintains the model's general ability and\nachieves the desired domain transfer, ensuring the highest utilization of\navailable resources. Therefore, if we value the balance between efficiency and\neffectiveness, CMR can be consider as the optimal mixture ratio.Through\nextensive experiments, we ascertain the predictability of CMR, and propose CMR\nscaling law and have substantiated its generalization. These findings offer\npractical guidelines for optimizing LLM training in specialized domains,\nensuring both general and domain-specific performance while efficiently\nmanaging training resources.\n", "link": "http://arxiv.org/abs/2407.17467v1", "date": "2024-07-24", "relevancy": 2.0157, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5217}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5152}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CMR%20Scaling%20Law%3A%20Predicting%20Critical%20Mixture%20Ratios%20for%20Continual%0A%20%20Pre-training%20of%20Language%20Models&body=Title%3A%20CMR%20Scaling%20Law%3A%20Predicting%20Critical%20Mixture%20Ratios%20for%20Continual%0A%20%20Pre-training%20of%20Language%20Models%0AAuthor%3A%20Jiawei%20Gu%20and%20Zacc%20Yang%20and%20Chuanghao%20Ding%20and%20Rui%20Zhao%20and%20Fei%20Tan%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20excel%20in%20diverse%20tasks%20but%20often%20underperform%20in%0Aspecialized%20fields%20due%20to%20limited%20domain-specific%20or%20proprietary%20corpus.%0AContinual%20pre-training%20%28CPT%29%20enhances%20LLM%20capabilities%20by%20imbuing%20new%0Adomain-specific%20or%20proprietary%20knowledge%20while%20replaying%20general%20corpus%20to%0Aprevent%20catastrophic%20forgetting.%20The%20data%20mixture%20ratio%20of%20general%20corpus%20and%0Adomain-specific%20corpus%2C%20however%2C%20has%20been%20chosen%20heuristically%2C%20leading%20to%0Asub-optimal%20training%20efficiency%20in%20practice.%20In%20this%20context%2C%20we%20attempt%20to%0Are-visit%20the%20scaling%20behavior%20of%20LLMs%20under%20the%20hood%20of%20CPT%2C%20and%20discover%20a%0Apower-law%20relationship%20between%20loss%2C%20mixture%20ratio%2C%20and%20training%20tokens%20scale.%0AWe%20formalize%20the%20trade-off%20between%20general%20and%20domain-specific%20capabilities%2C%0Aleading%20to%20a%20well-defined%20Critical%20Mixture%20Ratio%20%28CMR%29%20of%20general%20and%20domain%0Adata.%20By%20striking%20the%20balance%2C%20CMR%20maintains%20the%20model%27s%20general%20ability%20and%0Aachieves%20the%20desired%20domain%20transfer%2C%20ensuring%20the%20highest%20utilization%20of%0Aavailable%20resources.%20Therefore%2C%20if%20we%20value%20the%20balance%20between%20efficiency%20and%0Aeffectiveness%2C%20CMR%20can%20be%20consider%20as%20the%20optimal%20mixture%20ratio.Through%0Aextensive%20experiments%2C%20we%20ascertain%20the%20predictability%20of%20CMR%2C%20and%20propose%20CMR%0Ascaling%20law%20and%20have%20substantiated%20its%20generalization.%20These%20findings%20offer%0Apractical%20guidelines%20for%20optimizing%20LLM%20training%20in%20specialized%20domains%2C%0Aensuring%20both%20general%20and%20domain-specific%20performance%20while%20efficiently%0Amanaging%20training%20resources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17467v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCMR%2520Scaling%2520Law%253A%2520Predicting%2520Critical%2520Mixture%2520Ratios%2520for%2520Continual%250A%2520%2520Pre-training%2520of%2520Language%2520Models%26entry.906535625%3DJiawei%2520Gu%2520and%2520Zacc%2520Yang%2520and%2520Chuanghao%2520Ding%2520and%2520Rui%2520Zhao%2520and%2520Fei%2520Tan%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520excel%2520in%2520diverse%2520tasks%2520but%2520often%2520underperform%2520in%250Aspecialized%2520fields%2520due%2520to%2520limited%2520domain-specific%2520or%2520proprietary%2520corpus.%250AContinual%2520pre-training%2520%2528CPT%2529%2520enhances%2520LLM%2520capabilities%2520by%2520imbuing%2520new%250Adomain-specific%2520or%2520proprietary%2520knowledge%2520while%2520replaying%2520general%2520corpus%2520to%250Aprevent%2520catastrophic%2520forgetting.%2520The%2520data%2520mixture%2520ratio%2520of%2520general%2520corpus%2520and%250Adomain-specific%2520corpus%252C%2520however%252C%2520has%2520been%2520chosen%2520heuristically%252C%2520leading%2520to%250Asub-optimal%2520training%2520efficiency%2520in%2520practice.%2520In%2520this%2520context%252C%2520we%2520attempt%2520to%250Are-visit%2520the%2520scaling%2520behavior%2520of%2520LLMs%2520under%2520the%2520hood%2520of%2520CPT%252C%2520and%2520discover%2520a%250Apower-law%2520relationship%2520between%2520loss%252C%2520mixture%2520ratio%252C%2520and%2520training%2520tokens%2520scale.%250AWe%2520formalize%2520the%2520trade-off%2520between%2520general%2520and%2520domain-specific%2520capabilities%252C%250Aleading%2520to%2520a%2520well-defined%2520Critical%2520Mixture%2520Ratio%2520%2528CMR%2529%2520of%2520general%2520and%2520domain%250Adata.%2520By%2520striking%2520the%2520balance%252C%2520CMR%2520maintains%2520the%2520model%2527s%2520general%2520ability%2520and%250Aachieves%2520the%2520desired%2520domain%2520transfer%252C%2520ensuring%2520the%2520highest%2520utilization%2520of%250Aavailable%2520resources.%2520Therefore%252C%2520if%2520we%2520value%2520the%2520balance%2520between%2520efficiency%2520and%250Aeffectiveness%252C%2520CMR%2520can%2520be%2520consider%2520as%2520the%2520optimal%2520mixture%2520ratio.Through%250Aextensive%2520experiments%252C%2520we%2520ascertain%2520the%2520predictability%2520of%2520CMR%252C%2520and%2520propose%2520CMR%250Ascaling%2520law%2520and%2520have%2520substantiated%2520its%2520generalization.%2520These%2520findings%2520offer%250Apractical%2520guidelines%2520for%2520optimizing%2520LLM%2520training%2520in%2520specialized%2520domains%252C%250Aensuring%2520both%2520general%2520and%2520domain-specific%2520performance%2520while%2520efficiently%250Amanaging%2520training%2520resources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17467v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CMR%20Scaling%20Law%3A%20Predicting%20Critical%20Mixture%20Ratios%20for%20Continual%0A%20%20Pre-training%20of%20Language%20Models&entry.906535625=Jiawei%20Gu%20and%20Zacc%20Yang%20and%20Chuanghao%20Ding%20and%20Rui%20Zhao%20and%20Fei%20Tan&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20excel%20in%20diverse%20tasks%20but%20often%20underperform%20in%0Aspecialized%20fields%20due%20to%20limited%20domain-specific%20or%20proprietary%20corpus.%0AContinual%20pre-training%20%28CPT%29%20enhances%20LLM%20capabilities%20by%20imbuing%20new%0Adomain-specific%20or%20proprietary%20knowledge%20while%20replaying%20general%20corpus%20to%0Aprevent%20catastrophic%20forgetting.%20The%20data%20mixture%20ratio%20of%20general%20corpus%20and%0Adomain-specific%20corpus%2C%20however%2C%20has%20been%20chosen%20heuristically%2C%20leading%20to%0Asub-optimal%20training%20efficiency%20in%20practice.%20In%20this%20context%2C%20we%20attempt%20to%0Are-visit%20the%20scaling%20behavior%20of%20LLMs%20under%20the%20hood%20of%20CPT%2C%20and%20discover%20a%0Apower-law%20relationship%20between%20loss%2C%20mixture%20ratio%2C%20and%20training%20tokens%20scale.%0AWe%20formalize%20the%20trade-off%20between%20general%20and%20domain-specific%20capabilities%2C%0Aleading%20to%20a%20well-defined%20Critical%20Mixture%20Ratio%20%28CMR%29%20of%20general%20and%20domain%0Adata.%20By%20striking%20the%20balance%2C%20CMR%20maintains%20the%20model%27s%20general%20ability%20and%0Aachieves%20the%20desired%20domain%20transfer%2C%20ensuring%20the%20highest%20utilization%20of%0Aavailable%20resources.%20Therefore%2C%20if%20we%20value%20the%20balance%20between%20efficiency%20and%0Aeffectiveness%2C%20CMR%20can%20be%20consider%20as%20the%20optimal%20mixture%20ratio.Through%0Aextensive%20experiments%2C%20we%20ascertain%20the%20predictability%20of%20CMR%2C%20and%20propose%20CMR%0Ascaling%20law%20and%20have%20substantiated%20its%20generalization.%20These%20findings%20offer%0Apractical%20guidelines%20for%20optimizing%20LLM%20training%20in%20specialized%20domains%2C%0Aensuring%20both%20general%20and%20domain-specific%20performance%20while%20efficiently%0Amanaging%20training%20resources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17467v1&entry.124074799=Read"},
{"title": "Euler Characteristic Tools For Topological Data Analysis", "author": "Olympio Hacquard and Vadim Lebovici", "abstract": "  In this article, we study Euler characteristic techniques in topological data\nanalysis. Pointwise computing the Euler characteristic of a family of\nsimplicial complexes built from data gives rise to the so-called Euler\ncharacteristic profile. We show that this simple descriptor achieve\nstate-of-the-art performance in supervised tasks at a very low computational\ncost. Inspired by signal analysis, we compute hybrid transforms of Euler\ncharacteristic profiles. These integral transforms mix Euler characteristic\ntechniques with Lebesgue integration to provide highly efficient compressors of\ntopological signals. As a consequence, they show remarkable performances in\nunsupervised settings. On the qualitative side, we provide numerous heuristics\non the topological and geometric information captured by Euler profiles and\ntheir hybrid transforms. Finally, we prove stability results for these\ndescriptors as well as asymptotic guarantees in random settings.\n", "link": "http://arxiv.org/abs/2303.14040v3", "date": "2024-07-24", "relevancy": 2.0105, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4065}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4031}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.3967}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Euler%20Characteristic%20Tools%20For%20Topological%20Data%20Analysis&body=Title%3A%20Euler%20Characteristic%20Tools%20For%20Topological%20Data%20Analysis%0AAuthor%3A%20Olympio%20Hacquard%20and%20Vadim%20Lebovici%0AAbstract%3A%20%20%20In%20this%20article%2C%20we%20study%20Euler%20characteristic%20techniques%20in%20topological%20data%0Aanalysis.%20Pointwise%20computing%20the%20Euler%20characteristic%20of%20a%20family%20of%0Asimplicial%20complexes%20built%20from%20data%20gives%20rise%20to%20the%20so-called%20Euler%0Acharacteristic%20profile.%20We%20show%20that%20this%20simple%20descriptor%20achieve%0Astate-of-the-art%20performance%20in%20supervised%20tasks%20at%20a%20very%20low%20computational%0Acost.%20Inspired%20by%20signal%20analysis%2C%20we%20compute%20hybrid%20transforms%20of%20Euler%0Acharacteristic%20profiles.%20These%20integral%20transforms%20mix%20Euler%20characteristic%0Atechniques%20with%20Lebesgue%20integration%20to%20provide%20highly%20efficient%20compressors%20of%0Atopological%20signals.%20As%20a%20consequence%2C%20they%20show%20remarkable%20performances%20in%0Aunsupervised%20settings.%20On%20the%20qualitative%20side%2C%20we%20provide%20numerous%20heuristics%0Aon%20the%20topological%20and%20geometric%20information%20captured%20by%20Euler%20profiles%20and%0Atheir%20hybrid%20transforms.%20Finally%2C%20we%20prove%20stability%20results%20for%20these%0Adescriptors%20as%20well%20as%20asymptotic%20guarantees%20in%20random%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.14040v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEuler%2520Characteristic%2520Tools%2520For%2520Topological%2520Data%2520Analysis%26entry.906535625%3DOlympio%2520Hacquard%2520and%2520Vadim%2520Lebovici%26entry.1292438233%3D%2520%2520In%2520this%2520article%252C%2520we%2520study%2520Euler%2520characteristic%2520techniques%2520in%2520topological%2520data%250Aanalysis.%2520Pointwise%2520computing%2520the%2520Euler%2520characteristic%2520of%2520a%2520family%2520of%250Asimplicial%2520complexes%2520built%2520from%2520data%2520gives%2520rise%2520to%2520the%2520so-called%2520Euler%250Acharacteristic%2520profile.%2520We%2520show%2520that%2520this%2520simple%2520descriptor%2520achieve%250Astate-of-the-art%2520performance%2520in%2520supervised%2520tasks%2520at%2520a%2520very%2520low%2520computational%250Acost.%2520Inspired%2520by%2520signal%2520analysis%252C%2520we%2520compute%2520hybrid%2520transforms%2520of%2520Euler%250Acharacteristic%2520profiles.%2520These%2520integral%2520transforms%2520mix%2520Euler%2520characteristic%250Atechniques%2520with%2520Lebesgue%2520integration%2520to%2520provide%2520highly%2520efficient%2520compressors%2520of%250Atopological%2520signals.%2520As%2520a%2520consequence%252C%2520they%2520show%2520remarkable%2520performances%2520in%250Aunsupervised%2520settings.%2520On%2520the%2520qualitative%2520side%252C%2520we%2520provide%2520numerous%2520heuristics%250Aon%2520the%2520topological%2520and%2520geometric%2520information%2520captured%2520by%2520Euler%2520profiles%2520and%250Atheir%2520hybrid%2520transforms.%2520Finally%252C%2520we%2520prove%2520stability%2520results%2520for%2520these%250Adescriptors%2520as%2520well%2520as%2520asymptotic%2520guarantees%2520in%2520random%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.14040v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Euler%20Characteristic%20Tools%20For%20Topological%20Data%20Analysis&entry.906535625=Olympio%20Hacquard%20and%20Vadim%20Lebovici&entry.1292438233=%20%20In%20this%20article%2C%20we%20study%20Euler%20characteristic%20techniques%20in%20topological%20data%0Aanalysis.%20Pointwise%20computing%20the%20Euler%20characteristic%20of%20a%20family%20of%0Asimplicial%20complexes%20built%20from%20data%20gives%20rise%20to%20the%20so-called%20Euler%0Acharacteristic%20profile.%20We%20show%20that%20this%20simple%20descriptor%20achieve%0Astate-of-the-art%20performance%20in%20supervised%20tasks%20at%20a%20very%20low%20computational%0Acost.%20Inspired%20by%20signal%20analysis%2C%20we%20compute%20hybrid%20transforms%20of%20Euler%0Acharacteristic%20profiles.%20These%20integral%20transforms%20mix%20Euler%20characteristic%0Atechniques%20with%20Lebesgue%20integration%20to%20provide%20highly%20efficient%20compressors%20of%0Atopological%20signals.%20As%20a%20consequence%2C%20they%20show%20remarkable%20performances%20in%0Aunsupervised%20settings.%20On%20the%20qualitative%20side%2C%20we%20provide%20numerous%20heuristics%0Aon%20the%20topological%20and%20geometric%20information%20captured%20by%20Euler%20profiles%20and%0Atheir%20hybrid%20transforms.%20Finally%2C%20we%20prove%20stability%20results%20for%20these%0Adescriptors%20as%20well%20as%20asymptotic%20guarantees%20in%20random%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.14040v3&entry.124074799=Read"},
{"title": "A Simulation Benchmark for Autonomous Racing with Large-Scale Human Data", "author": "Adrian Remonda and Nicklas Hansen and Ayoub Raji and Nicola Musiu and Marko Bertogna and Eduardo Veas and Xiaolong Wang", "abstract": "  Despite the availability of international prize-money competitions, scaled\nvehicles, and simulation environments, research on autonomous racing and the\ncontrol of sports cars operating close to the limit of handling has been\nlimited by the high costs of vehicle acquisition and management, as well as the\nlimited physics accuracy of open-source simulators. In this paper, we propose a\nracing simulation platform based on the simulator Assetto Corsa to test,\nvalidate, and benchmark autonomous driving algorithms, including reinforcement\nlearning (RL) and classical Model Predictive Control (MPC), in realistic and\nchallenging scenarios. Our contributions include the development of this\nsimulation platform, several state-of-the-art algorithms tailored to the racing\nenvironment, and a comprehensive dataset collected from human drivers.\nAdditionally, we evaluate algorithms in the offline RL setting. All the\nnecessary code (including environment and benchmarks), working examples,\ndatasets, and videos are publicly released and can be found at:\nhttps://assetto-corsa-gym.github.io\n", "link": "http://arxiv.org/abs/2407.16680v2", "date": "2024-07-24", "relevancy": 2.0005, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.54}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4956}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Simulation%20Benchmark%20for%20Autonomous%20Racing%20with%20Large-Scale%20Human%20Data&body=Title%3A%20A%20Simulation%20Benchmark%20for%20Autonomous%20Racing%20with%20Large-Scale%20Human%20Data%0AAuthor%3A%20Adrian%20Remonda%20and%20Nicklas%20Hansen%20and%20Ayoub%20Raji%20and%20Nicola%20Musiu%20and%20Marko%20Bertogna%20and%20Eduardo%20Veas%20and%20Xiaolong%20Wang%0AAbstract%3A%20%20%20Despite%20the%20availability%20of%20international%20prize-money%20competitions%2C%20scaled%0Avehicles%2C%20and%20simulation%20environments%2C%20research%20on%20autonomous%20racing%20and%20the%0Acontrol%20of%20sports%20cars%20operating%20close%20to%20the%20limit%20of%20handling%20has%20been%0Alimited%20by%20the%20high%20costs%20of%20vehicle%20acquisition%20and%20management%2C%20as%20well%20as%20the%0Alimited%20physics%20accuracy%20of%20open-source%20simulators.%20In%20this%20paper%2C%20we%20propose%20a%0Aracing%20simulation%20platform%20based%20on%20the%20simulator%20Assetto%20Corsa%20to%20test%2C%0Avalidate%2C%20and%20benchmark%20autonomous%20driving%20algorithms%2C%20including%20reinforcement%0Alearning%20%28RL%29%20and%20classical%20Model%20Predictive%20Control%20%28MPC%29%2C%20in%20realistic%20and%0Achallenging%20scenarios.%20Our%20contributions%20include%20the%20development%20of%20this%0Asimulation%20platform%2C%20several%20state-of-the-art%20algorithms%20tailored%20to%20the%20racing%0Aenvironment%2C%20and%20a%20comprehensive%20dataset%20collected%20from%20human%20drivers.%0AAdditionally%2C%20we%20evaluate%20algorithms%20in%20the%20offline%20RL%20setting.%20All%20the%0Anecessary%20code%20%28including%20environment%20and%20benchmarks%29%2C%20working%20examples%2C%0Adatasets%2C%20and%20videos%20are%20publicly%20released%20and%20can%20be%20found%20at%3A%0Ahttps%3A//assetto-corsa-gym.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16680v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Simulation%2520Benchmark%2520for%2520Autonomous%2520Racing%2520with%2520Large-Scale%2520Human%2520Data%26entry.906535625%3DAdrian%2520Remonda%2520and%2520Nicklas%2520Hansen%2520and%2520Ayoub%2520Raji%2520and%2520Nicola%2520Musiu%2520and%2520Marko%2520Bertogna%2520and%2520Eduardo%2520Veas%2520and%2520Xiaolong%2520Wang%26entry.1292438233%3D%2520%2520Despite%2520the%2520availability%2520of%2520international%2520prize-money%2520competitions%252C%2520scaled%250Avehicles%252C%2520and%2520simulation%2520environments%252C%2520research%2520on%2520autonomous%2520racing%2520and%2520the%250Acontrol%2520of%2520sports%2520cars%2520operating%2520close%2520to%2520the%2520limit%2520of%2520handling%2520has%2520been%250Alimited%2520by%2520the%2520high%2520costs%2520of%2520vehicle%2520acquisition%2520and%2520management%252C%2520as%2520well%2520as%2520the%250Alimited%2520physics%2520accuracy%2520of%2520open-source%2520simulators.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Aracing%2520simulation%2520platform%2520based%2520on%2520the%2520simulator%2520Assetto%2520Corsa%2520to%2520test%252C%250Avalidate%252C%2520and%2520benchmark%2520autonomous%2520driving%2520algorithms%252C%2520including%2520reinforcement%250Alearning%2520%2528RL%2529%2520and%2520classical%2520Model%2520Predictive%2520Control%2520%2528MPC%2529%252C%2520in%2520realistic%2520and%250Achallenging%2520scenarios.%2520Our%2520contributions%2520include%2520the%2520development%2520of%2520this%250Asimulation%2520platform%252C%2520several%2520state-of-the-art%2520algorithms%2520tailored%2520to%2520the%2520racing%250Aenvironment%252C%2520and%2520a%2520comprehensive%2520dataset%2520collected%2520from%2520human%2520drivers.%250AAdditionally%252C%2520we%2520evaluate%2520algorithms%2520in%2520the%2520offline%2520RL%2520setting.%2520All%2520the%250Anecessary%2520code%2520%2528including%2520environment%2520and%2520benchmarks%2529%252C%2520working%2520examples%252C%250Adatasets%252C%2520and%2520videos%2520are%2520publicly%2520released%2520and%2520can%2520be%2520found%2520at%253A%250Ahttps%253A//assetto-corsa-gym.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16680v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Simulation%20Benchmark%20for%20Autonomous%20Racing%20with%20Large-Scale%20Human%20Data&entry.906535625=Adrian%20Remonda%20and%20Nicklas%20Hansen%20and%20Ayoub%20Raji%20and%20Nicola%20Musiu%20and%20Marko%20Bertogna%20and%20Eduardo%20Veas%20and%20Xiaolong%20Wang&entry.1292438233=%20%20Despite%20the%20availability%20of%20international%20prize-money%20competitions%2C%20scaled%0Avehicles%2C%20and%20simulation%20environments%2C%20research%20on%20autonomous%20racing%20and%20the%0Acontrol%20of%20sports%20cars%20operating%20close%20to%20the%20limit%20of%20handling%20has%20been%0Alimited%20by%20the%20high%20costs%20of%20vehicle%20acquisition%20and%20management%2C%20as%20well%20as%20the%0Alimited%20physics%20accuracy%20of%20open-source%20simulators.%20In%20this%20paper%2C%20we%20propose%20a%0Aracing%20simulation%20platform%20based%20on%20the%20simulator%20Assetto%20Corsa%20to%20test%2C%0Avalidate%2C%20and%20benchmark%20autonomous%20driving%20algorithms%2C%20including%20reinforcement%0Alearning%20%28RL%29%20and%20classical%20Model%20Predictive%20Control%20%28MPC%29%2C%20in%20realistic%20and%0Achallenging%20scenarios.%20Our%20contributions%20include%20the%20development%20of%20this%0Asimulation%20platform%2C%20several%20state-of-the-art%20algorithms%20tailored%20to%20the%20racing%0Aenvironment%2C%20and%20a%20comprehensive%20dataset%20collected%20from%20human%20drivers.%0AAdditionally%2C%20we%20evaluate%20algorithms%20in%20the%20offline%20RL%20setting.%20All%20the%0Anecessary%20code%20%28including%20environment%20and%20benchmarks%29%2C%20working%20examples%2C%0Adatasets%2C%20and%20videos%20are%20publicly%20released%20and%20can%20be%20found%20at%3A%0Ahttps%3A//assetto-corsa-gym.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16680v2&entry.124074799=Read"},
{"title": "AHMF: Adaptive Hybrid-Memory-Fusion Model for Driver Attention\n  Prediction", "author": "Dongyang Xu and Qingfan Wang and Ji Ma and Xiangyun Zeng and Lei Chen", "abstract": "  Accurate driver attention prediction can serve as a critical reference for\nintelligent vehicles in understanding traffic scenes and making informed\ndriving decisions. Though existing studies on driver attention prediction\nimproved performance by incorporating advanced saliency detection techniques,\nthey overlooked the opportunity to achieve human-inspired prediction by\nanalyzing driving tasks from a cognitive science perspective. During driving,\ndrivers' working memory and long-term memory play crucial roles in scene\ncomprehension and experience retrieval, respectively. Together, they form\nsituational awareness, facilitating drivers to quickly understand the current\ntraffic situation and make optimal decisions based on past driving experiences.\nTo explicitly integrate these two types of memory, this paper proposes an\nAdaptive Hybrid-Memory-Fusion (AHMF) driver attention prediction model to\nachieve more human-like predictions. Specifically, the model first encodes\ninformation about specific hazardous stimuli in the current scene to form\nworking memories. Then, it adaptively retrieves similar situational experiences\nfrom the long-term memory for final prediction. Utilizing domain adaptation\ntechniques, the model performs parallel training across multiple datasets,\nthereby enriching the accumulated driving experience within the long-term\nmemory module. Compared to existing models, our model demonstrates significant\nimprovements across various metrics on multiple public datasets, proving the\neffectiveness of integrating hybrid memories in driver attention prediction.\n", "link": "http://arxiv.org/abs/2407.17442v1", "date": "2024-07-24", "relevancy": 1.9936, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5191}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4946}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AHMF%3A%20Adaptive%20Hybrid-Memory-Fusion%20Model%20for%20Driver%20Attention%0A%20%20Prediction&body=Title%3A%20AHMF%3A%20Adaptive%20Hybrid-Memory-Fusion%20Model%20for%20Driver%20Attention%0A%20%20Prediction%0AAuthor%3A%20Dongyang%20Xu%20and%20Qingfan%20Wang%20and%20Ji%20Ma%20and%20Xiangyun%20Zeng%20and%20Lei%20Chen%0AAbstract%3A%20%20%20Accurate%20driver%20attention%20prediction%20can%20serve%20as%20a%20critical%20reference%20for%0Aintelligent%20vehicles%20in%20understanding%20traffic%20scenes%20and%20making%20informed%0Adriving%20decisions.%20Though%20existing%20studies%20on%20driver%20attention%20prediction%0Aimproved%20performance%20by%20incorporating%20advanced%20saliency%20detection%20techniques%2C%0Athey%20overlooked%20the%20opportunity%20to%20achieve%20human-inspired%20prediction%20by%0Aanalyzing%20driving%20tasks%20from%20a%20cognitive%20science%20perspective.%20During%20driving%2C%0Adrivers%27%20working%20memory%20and%20long-term%20memory%20play%20crucial%20roles%20in%20scene%0Acomprehension%20and%20experience%20retrieval%2C%20respectively.%20Together%2C%20they%20form%0Asituational%20awareness%2C%20facilitating%20drivers%20to%20quickly%20understand%20the%20current%0Atraffic%20situation%20and%20make%20optimal%20decisions%20based%20on%20past%20driving%20experiences.%0ATo%20explicitly%20integrate%20these%20two%20types%20of%20memory%2C%20this%20paper%20proposes%20an%0AAdaptive%20Hybrid-Memory-Fusion%20%28AHMF%29%20driver%20attention%20prediction%20model%20to%0Aachieve%20more%20human-like%20predictions.%20Specifically%2C%20the%20model%20first%20encodes%0Ainformation%20about%20specific%20hazardous%20stimuli%20in%20the%20current%20scene%20to%20form%0Aworking%20memories.%20Then%2C%20it%20adaptively%20retrieves%20similar%20situational%20experiences%0Afrom%20the%20long-term%20memory%20for%20final%20prediction.%20Utilizing%20domain%20adaptation%0Atechniques%2C%20the%20model%20performs%20parallel%20training%20across%20multiple%20datasets%2C%0Athereby%20enriching%20the%20accumulated%20driving%20experience%20within%20the%20long-term%0Amemory%20module.%20Compared%20to%20existing%20models%2C%20our%20model%20demonstrates%20significant%0Aimprovements%20across%20various%20metrics%20on%20multiple%20public%20datasets%2C%20proving%20the%0Aeffectiveness%20of%20integrating%20hybrid%20memories%20in%20driver%20attention%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17442v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAHMF%253A%2520Adaptive%2520Hybrid-Memory-Fusion%2520Model%2520for%2520Driver%2520Attention%250A%2520%2520Prediction%26entry.906535625%3DDongyang%2520Xu%2520and%2520Qingfan%2520Wang%2520and%2520Ji%2520Ma%2520and%2520Xiangyun%2520Zeng%2520and%2520Lei%2520Chen%26entry.1292438233%3D%2520%2520Accurate%2520driver%2520attention%2520prediction%2520can%2520serve%2520as%2520a%2520critical%2520reference%2520for%250Aintelligent%2520vehicles%2520in%2520understanding%2520traffic%2520scenes%2520and%2520making%2520informed%250Adriving%2520decisions.%2520Though%2520existing%2520studies%2520on%2520driver%2520attention%2520prediction%250Aimproved%2520performance%2520by%2520incorporating%2520advanced%2520saliency%2520detection%2520techniques%252C%250Athey%2520overlooked%2520the%2520opportunity%2520to%2520achieve%2520human-inspired%2520prediction%2520by%250Aanalyzing%2520driving%2520tasks%2520from%2520a%2520cognitive%2520science%2520perspective.%2520During%2520driving%252C%250Adrivers%2527%2520working%2520memory%2520and%2520long-term%2520memory%2520play%2520crucial%2520roles%2520in%2520scene%250Acomprehension%2520and%2520experience%2520retrieval%252C%2520respectively.%2520Together%252C%2520they%2520form%250Asituational%2520awareness%252C%2520facilitating%2520drivers%2520to%2520quickly%2520understand%2520the%2520current%250Atraffic%2520situation%2520and%2520make%2520optimal%2520decisions%2520based%2520on%2520past%2520driving%2520experiences.%250ATo%2520explicitly%2520integrate%2520these%2520two%2520types%2520of%2520memory%252C%2520this%2520paper%2520proposes%2520an%250AAdaptive%2520Hybrid-Memory-Fusion%2520%2528AHMF%2529%2520driver%2520attention%2520prediction%2520model%2520to%250Aachieve%2520more%2520human-like%2520predictions.%2520Specifically%252C%2520the%2520model%2520first%2520encodes%250Ainformation%2520about%2520specific%2520hazardous%2520stimuli%2520in%2520the%2520current%2520scene%2520to%2520form%250Aworking%2520memories.%2520Then%252C%2520it%2520adaptively%2520retrieves%2520similar%2520situational%2520experiences%250Afrom%2520the%2520long-term%2520memory%2520for%2520final%2520prediction.%2520Utilizing%2520domain%2520adaptation%250Atechniques%252C%2520the%2520model%2520performs%2520parallel%2520training%2520across%2520multiple%2520datasets%252C%250Athereby%2520enriching%2520the%2520accumulated%2520driving%2520experience%2520within%2520the%2520long-term%250Amemory%2520module.%2520Compared%2520to%2520existing%2520models%252C%2520our%2520model%2520demonstrates%2520significant%250Aimprovements%2520across%2520various%2520metrics%2520on%2520multiple%2520public%2520datasets%252C%2520proving%2520the%250Aeffectiveness%2520of%2520integrating%2520hybrid%2520memories%2520in%2520driver%2520attention%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17442v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AHMF%3A%20Adaptive%20Hybrid-Memory-Fusion%20Model%20for%20Driver%20Attention%0A%20%20Prediction&entry.906535625=Dongyang%20Xu%20and%20Qingfan%20Wang%20and%20Ji%20Ma%20and%20Xiangyun%20Zeng%20and%20Lei%20Chen&entry.1292438233=%20%20Accurate%20driver%20attention%20prediction%20can%20serve%20as%20a%20critical%20reference%20for%0Aintelligent%20vehicles%20in%20understanding%20traffic%20scenes%20and%20making%20informed%0Adriving%20decisions.%20Though%20existing%20studies%20on%20driver%20attention%20prediction%0Aimproved%20performance%20by%20incorporating%20advanced%20saliency%20detection%20techniques%2C%0Athey%20overlooked%20the%20opportunity%20to%20achieve%20human-inspired%20prediction%20by%0Aanalyzing%20driving%20tasks%20from%20a%20cognitive%20science%20perspective.%20During%20driving%2C%0Adrivers%27%20working%20memory%20and%20long-term%20memory%20play%20crucial%20roles%20in%20scene%0Acomprehension%20and%20experience%20retrieval%2C%20respectively.%20Together%2C%20they%20form%0Asituational%20awareness%2C%20facilitating%20drivers%20to%20quickly%20understand%20the%20current%0Atraffic%20situation%20and%20make%20optimal%20decisions%20based%20on%20past%20driving%20experiences.%0ATo%20explicitly%20integrate%20these%20two%20types%20of%20memory%2C%20this%20paper%20proposes%20an%0AAdaptive%20Hybrid-Memory-Fusion%20%28AHMF%29%20driver%20attention%20prediction%20model%20to%0Aachieve%20more%20human-like%20predictions.%20Specifically%2C%20the%20model%20first%20encodes%0Ainformation%20about%20specific%20hazardous%20stimuli%20in%20the%20current%20scene%20to%20form%0Aworking%20memories.%20Then%2C%20it%20adaptively%20retrieves%20similar%20situational%20experiences%0Afrom%20the%20long-term%20memory%20for%20final%20prediction.%20Utilizing%20domain%20adaptation%0Atechniques%2C%20the%20model%20performs%20parallel%20training%20across%20multiple%20datasets%2C%0Athereby%20enriching%20the%20accumulated%20driving%20experience%20within%20the%20long-term%0Amemory%20module.%20Compared%20to%20existing%20models%2C%20our%20model%20demonstrates%20significant%0Aimprovements%20across%20various%20metrics%20on%20multiple%20public%20datasets%2C%20proving%20the%0Aeffectiveness%20of%20integrating%20hybrid%20memories%20in%20driver%20attention%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17442v1&entry.124074799=Read"},
{"title": "How Good (Or Bad) Are LLMs at Detecting Misleading Visualizations?", "author": "Leo Yu-Ho Lo and Huamin Qu", "abstract": "  In this study, we address the growing issue of misleading charts, a prevalent\nproblem that undermines the integrity of information dissemination. Misleading\ncharts can distort the viewer's perception of data, leading to\nmisinterpretations and decisions based on false information. The development of\neffective automatic detection methods for misleading charts is an urgent field\nof research. The recent advancement of multimodal Large Language Models (LLMs)\nhas introduced a promising direction for addressing this challenge. We explored\nthe capabilities of these models in analyzing complex charts and assessing the\nimpact of different prompting strategies on the models' analyses. We utilized a\ndataset of misleading charts collected from the internet by prior research and\ncrafted nine distinct prompts, ranging from simple to complex, to test the\nability of four different multimodal LLMs in detecting over 21 different chart\nissues. Through three experiments--from initial exploration to detailed\nanalysis--we progressively gained insights into how to effectively prompt LLMs\nto identify misleading charts and developed strategies to address the\nscalability challenges encountered as we expanded our detection range from the\ninitial five issues to 21 issues in the final experiment. Our findings reveal\nthat multimodal LLMs possess a strong capability for chart comprehension and\ncritical thinking in data interpretation. There is significant potential in\nemploying multimodal LLMs to counter misleading information by supporting\ncritical thinking and enhancing visualization literacy. This study demonstrates\nthe applicability of LLMs in addressing the pressing concern of misleading\ncharts.\n", "link": "http://arxiv.org/abs/2407.17291v1", "date": "2024-07-24", "relevancy": 1.9919, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5063}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4948}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Good%20%28Or%20Bad%29%20Are%20LLMs%20at%20Detecting%20Misleading%20Visualizations%3F&body=Title%3A%20How%20Good%20%28Or%20Bad%29%20Are%20LLMs%20at%20Detecting%20Misleading%20Visualizations%3F%0AAuthor%3A%20Leo%20Yu-Ho%20Lo%20and%20Huamin%20Qu%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20address%20the%20growing%20issue%20of%20misleading%20charts%2C%20a%20prevalent%0Aproblem%20that%20undermines%20the%20integrity%20of%20information%20dissemination.%20Misleading%0Acharts%20can%20distort%20the%20viewer%27s%20perception%20of%20data%2C%20leading%20to%0Amisinterpretations%20and%20decisions%20based%20on%20false%20information.%20The%20development%20of%0Aeffective%20automatic%20detection%20methods%20for%20misleading%20charts%20is%20an%20urgent%20field%0Aof%20research.%20The%20recent%20advancement%20of%20multimodal%20Large%20Language%20Models%20%28LLMs%29%0Ahas%20introduced%20a%20promising%20direction%20for%20addressing%20this%20challenge.%20We%20explored%0Athe%20capabilities%20of%20these%20models%20in%20analyzing%20complex%20charts%20and%20assessing%20the%0Aimpact%20of%20different%20prompting%20strategies%20on%20the%20models%27%20analyses.%20We%20utilized%20a%0Adataset%20of%20misleading%20charts%20collected%20from%20the%20internet%20by%20prior%20research%20and%0Acrafted%20nine%20distinct%20prompts%2C%20ranging%20from%20simple%20to%20complex%2C%20to%20test%20the%0Aability%20of%20four%20different%20multimodal%20LLMs%20in%20detecting%20over%2021%20different%20chart%0Aissues.%20Through%20three%20experiments--from%20initial%20exploration%20to%20detailed%0Aanalysis--we%20progressively%20gained%20insights%20into%20how%20to%20effectively%20prompt%20LLMs%0Ato%20identify%20misleading%20charts%20and%20developed%20strategies%20to%20address%20the%0Ascalability%20challenges%20encountered%20as%20we%20expanded%20our%20detection%20range%20from%20the%0Ainitial%20five%20issues%20to%2021%20issues%20in%20the%20final%20experiment.%20Our%20findings%20reveal%0Athat%20multimodal%20LLMs%20possess%20a%20strong%20capability%20for%20chart%20comprehension%20and%0Acritical%20thinking%20in%20data%20interpretation.%20There%20is%20significant%20potential%20in%0Aemploying%20multimodal%20LLMs%20to%20counter%20misleading%20information%20by%20supporting%0Acritical%20thinking%20and%20enhancing%20visualization%20literacy.%20This%20study%20demonstrates%0Athe%20applicability%20of%20LLMs%20in%20addressing%20the%20pressing%20concern%20of%20misleading%0Acharts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17291v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Good%2520%2528Or%2520Bad%2529%2520Are%2520LLMs%2520at%2520Detecting%2520Misleading%2520Visualizations%253F%26entry.906535625%3DLeo%2520Yu-Ho%2520Lo%2520and%2520Huamin%2520Qu%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520address%2520the%2520growing%2520issue%2520of%2520misleading%2520charts%252C%2520a%2520prevalent%250Aproblem%2520that%2520undermines%2520the%2520integrity%2520of%2520information%2520dissemination.%2520Misleading%250Acharts%2520can%2520distort%2520the%2520viewer%2527s%2520perception%2520of%2520data%252C%2520leading%2520to%250Amisinterpretations%2520and%2520decisions%2520based%2520on%2520false%2520information.%2520The%2520development%2520of%250Aeffective%2520automatic%2520detection%2520methods%2520for%2520misleading%2520charts%2520is%2520an%2520urgent%2520field%250Aof%2520research.%2520The%2520recent%2520advancement%2520of%2520multimodal%2520Large%2520Language%2520Models%2520%2528LLMs%2529%250Ahas%2520introduced%2520a%2520promising%2520direction%2520for%2520addressing%2520this%2520challenge.%2520We%2520explored%250Athe%2520capabilities%2520of%2520these%2520models%2520in%2520analyzing%2520complex%2520charts%2520and%2520assessing%2520the%250Aimpact%2520of%2520different%2520prompting%2520strategies%2520on%2520the%2520models%2527%2520analyses.%2520We%2520utilized%2520a%250Adataset%2520of%2520misleading%2520charts%2520collected%2520from%2520the%2520internet%2520by%2520prior%2520research%2520and%250Acrafted%2520nine%2520distinct%2520prompts%252C%2520ranging%2520from%2520simple%2520to%2520complex%252C%2520to%2520test%2520the%250Aability%2520of%2520four%2520different%2520multimodal%2520LLMs%2520in%2520detecting%2520over%252021%2520different%2520chart%250Aissues.%2520Through%2520three%2520experiments--from%2520initial%2520exploration%2520to%2520detailed%250Aanalysis--we%2520progressively%2520gained%2520insights%2520into%2520how%2520to%2520effectively%2520prompt%2520LLMs%250Ato%2520identify%2520misleading%2520charts%2520and%2520developed%2520strategies%2520to%2520address%2520the%250Ascalability%2520challenges%2520encountered%2520as%2520we%2520expanded%2520our%2520detection%2520range%2520from%2520the%250Ainitial%2520five%2520issues%2520to%252021%2520issues%2520in%2520the%2520final%2520experiment.%2520Our%2520findings%2520reveal%250Athat%2520multimodal%2520LLMs%2520possess%2520a%2520strong%2520capability%2520for%2520chart%2520comprehension%2520and%250Acritical%2520thinking%2520in%2520data%2520interpretation.%2520There%2520is%2520significant%2520potential%2520in%250Aemploying%2520multimodal%2520LLMs%2520to%2520counter%2520misleading%2520information%2520by%2520supporting%250Acritical%2520thinking%2520and%2520enhancing%2520visualization%2520literacy.%2520This%2520study%2520demonstrates%250Athe%2520applicability%2520of%2520LLMs%2520in%2520addressing%2520the%2520pressing%2520concern%2520of%2520misleading%250Acharts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17291v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Good%20%28Or%20Bad%29%20Are%20LLMs%20at%20Detecting%20Misleading%20Visualizations%3F&entry.906535625=Leo%20Yu-Ho%20Lo%20and%20Huamin%20Qu&entry.1292438233=%20%20In%20this%20study%2C%20we%20address%20the%20growing%20issue%20of%20misleading%20charts%2C%20a%20prevalent%0Aproblem%20that%20undermines%20the%20integrity%20of%20information%20dissemination.%20Misleading%0Acharts%20can%20distort%20the%20viewer%27s%20perception%20of%20data%2C%20leading%20to%0Amisinterpretations%20and%20decisions%20based%20on%20false%20information.%20The%20development%20of%0Aeffective%20automatic%20detection%20methods%20for%20misleading%20charts%20is%20an%20urgent%20field%0Aof%20research.%20The%20recent%20advancement%20of%20multimodal%20Large%20Language%20Models%20%28LLMs%29%0Ahas%20introduced%20a%20promising%20direction%20for%20addressing%20this%20challenge.%20We%20explored%0Athe%20capabilities%20of%20these%20models%20in%20analyzing%20complex%20charts%20and%20assessing%20the%0Aimpact%20of%20different%20prompting%20strategies%20on%20the%20models%27%20analyses.%20We%20utilized%20a%0Adataset%20of%20misleading%20charts%20collected%20from%20the%20internet%20by%20prior%20research%20and%0Acrafted%20nine%20distinct%20prompts%2C%20ranging%20from%20simple%20to%20complex%2C%20to%20test%20the%0Aability%20of%20four%20different%20multimodal%20LLMs%20in%20detecting%20over%2021%20different%20chart%0Aissues.%20Through%20three%20experiments--from%20initial%20exploration%20to%20detailed%0Aanalysis--we%20progressively%20gained%20insights%20into%20how%20to%20effectively%20prompt%20LLMs%0Ato%20identify%20misleading%20charts%20and%20developed%20strategies%20to%20address%20the%0Ascalability%20challenges%20encountered%20as%20we%20expanded%20our%20detection%20range%20from%20the%0Ainitial%20five%20issues%20to%2021%20issues%20in%20the%20final%20experiment.%20Our%20findings%20reveal%0Athat%20multimodal%20LLMs%20possess%20a%20strong%20capability%20for%20chart%20comprehension%20and%0Acritical%20thinking%20in%20data%20interpretation.%20There%20is%20significant%20potential%20in%0Aemploying%20multimodal%20LLMs%20to%20counter%20misleading%20information%20by%20supporting%0Acritical%20thinking%20and%20enhancing%20visualization%20literacy.%20This%20study%20demonstrates%0Athe%20applicability%20of%20LLMs%20in%20addressing%20the%20pressing%20concern%20of%20misleading%0Acharts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17291v1&entry.124074799=Read"},
{"title": "An Experimental Study on the Rashomon Effect of Balancing Methods in\n  Imbalanced Classification", "author": "Mustafa Cavus and Przemys\u0142aw Biecek", "abstract": "  Predictive models may generate biased predictions when classifying imbalanced\ndatasets. This happens when the model favors the majority class, leading to low\nperformance in accurately predicting the minority class. To address this issue,\nbalancing or resampling methods are critical data-centric AI approaches in the\nmodeling process to improve prediction performance. However, there have been\ndebates and questions about the functionality of these methods in recent years.\nIn particular, many candidate models may exhibit very similar predictive\nperformance, called the Rashomon effect, in model selection, and they may even\nproduce different predictions for the same observations. Selecting one of these\nmodels without considering the predictive multiplicity -- which is the case of\nyielding conflicting models' predictions for any sample -- can result in blind\nselection. In this paper, the impact of balancing methods on predictive\nmultiplicity is examined using the Rashomon effect. It is crucial because the\nblind model selection in data-centric AI is risky from a set of approximately\nequally accurate models. This may lead to severe problems in model selection,\nvalidation, and explanation. To tackle this matter, we conducted real dataset\nexperiments to observe the impact of balancing methods on predictive\nmultiplicity through the Rashomon effect by using a newly proposed metric\nobscurity in addition to the existing ones: ambiguity and discrepancy. Our\nfindings showed that balancing methods inflate the predictive multiplicity and\nyield varying results. To monitor the trade-off between the prediction\nperformance and predictive multiplicity for conducting the modeling process\nresponsibly, we proposed using the extended version of the performance-gain\nplot when balancing the training data.\n", "link": "http://arxiv.org/abs/2405.01557v4", "date": "2024-07-24", "relevancy": 1.989, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5079}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5054}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Experimental%20Study%20on%20the%20Rashomon%20Effect%20of%20Balancing%20Methods%20in%0A%20%20Imbalanced%20Classification&body=Title%3A%20An%20Experimental%20Study%20on%20the%20Rashomon%20Effect%20of%20Balancing%20Methods%20in%0A%20%20Imbalanced%20Classification%0AAuthor%3A%20Mustafa%20Cavus%20and%20Przemys%C5%82aw%20Biecek%0AAbstract%3A%20%20%20Predictive%20models%20may%20generate%20biased%20predictions%20when%20classifying%20imbalanced%0Adatasets.%20This%20happens%20when%20the%20model%20favors%20the%20majority%20class%2C%20leading%20to%20low%0Aperformance%20in%20accurately%20predicting%20the%20minority%20class.%20To%20address%20this%20issue%2C%0Abalancing%20or%20resampling%20methods%20are%20critical%20data-centric%20AI%20approaches%20in%20the%0Amodeling%20process%20to%20improve%20prediction%20performance.%20However%2C%20there%20have%20been%0Adebates%20and%20questions%20about%20the%20functionality%20of%20these%20methods%20in%20recent%20years.%0AIn%20particular%2C%20many%20candidate%20models%20may%20exhibit%20very%20similar%20predictive%0Aperformance%2C%20called%20the%20Rashomon%20effect%2C%20in%20model%20selection%2C%20and%20they%20may%20even%0Aproduce%20different%20predictions%20for%20the%20same%20observations.%20Selecting%20one%20of%20these%0Amodels%20without%20considering%20the%20predictive%20multiplicity%20--%20which%20is%20the%20case%20of%0Ayielding%20conflicting%20models%27%20predictions%20for%20any%20sample%20--%20can%20result%20in%20blind%0Aselection.%20In%20this%20paper%2C%20the%20impact%20of%20balancing%20methods%20on%20predictive%0Amultiplicity%20is%20examined%20using%20the%20Rashomon%20effect.%20It%20is%20crucial%20because%20the%0Ablind%20model%20selection%20in%20data-centric%20AI%20is%20risky%20from%20a%20set%20of%20approximately%0Aequally%20accurate%20models.%20This%20may%20lead%20to%20severe%20problems%20in%20model%20selection%2C%0Avalidation%2C%20and%20explanation.%20To%20tackle%20this%20matter%2C%20we%20conducted%20real%20dataset%0Aexperiments%20to%20observe%20the%20impact%20of%20balancing%20methods%20on%20predictive%0Amultiplicity%20through%20the%20Rashomon%20effect%20by%20using%20a%20newly%20proposed%20metric%0Aobscurity%20in%20addition%20to%20the%20existing%20ones%3A%20ambiguity%20and%20discrepancy.%20Our%0Afindings%20showed%20that%20balancing%20methods%20inflate%20the%20predictive%20multiplicity%20and%0Ayield%20varying%20results.%20To%20monitor%20the%20trade-off%20between%20the%20prediction%0Aperformance%20and%20predictive%20multiplicity%20for%20conducting%20the%20modeling%20process%0Aresponsibly%2C%20we%20proposed%20using%20the%20extended%20version%20of%20the%20performance-gain%0Aplot%20when%20balancing%20the%20training%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01557v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Experimental%2520Study%2520on%2520the%2520Rashomon%2520Effect%2520of%2520Balancing%2520Methods%2520in%250A%2520%2520Imbalanced%2520Classification%26entry.906535625%3DMustafa%2520Cavus%2520and%2520Przemys%25C5%2582aw%2520Biecek%26entry.1292438233%3D%2520%2520Predictive%2520models%2520may%2520generate%2520biased%2520predictions%2520when%2520classifying%2520imbalanced%250Adatasets.%2520This%2520happens%2520when%2520the%2520model%2520favors%2520the%2520majority%2520class%252C%2520leading%2520to%2520low%250Aperformance%2520in%2520accurately%2520predicting%2520the%2520minority%2520class.%2520To%2520address%2520this%2520issue%252C%250Abalancing%2520or%2520resampling%2520methods%2520are%2520critical%2520data-centric%2520AI%2520approaches%2520in%2520the%250Amodeling%2520process%2520to%2520improve%2520prediction%2520performance.%2520However%252C%2520there%2520have%2520been%250Adebates%2520and%2520questions%2520about%2520the%2520functionality%2520of%2520these%2520methods%2520in%2520recent%2520years.%250AIn%2520particular%252C%2520many%2520candidate%2520models%2520may%2520exhibit%2520very%2520similar%2520predictive%250Aperformance%252C%2520called%2520the%2520Rashomon%2520effect%252C%2520in%2520model%2520selection%252C%2520and%2520they%2520may%2520even%250Aproduce%2520different%2520predictions%2520for%2520the%2520same%2520observations.%2520Selecting%2520one%2520of%2520these%250Amodels%2520without%2520considering%2520the%2520predictive%2520multiplicity%2520--%2520which%2520is%2520the%2520case%2520of%250Ayielding%2520conflicting%2520models%2527%2520predictions%2520for%2520any%2520sample%2520--%2520can%2520result%2520in%2520blind%250Aselection.%2520In%2520this%2520paper%252C%2520the%2520impact%2520of%2520balancing%2520methods%2520on%2520predictive%250Amultiplicity%2520is%2520examined%2520using%2520the%2520Rashomon%2520effect.%2520It%2520is%2520crucial%2520because%2520the%250Ablind%2520model%2520selection%2520in%2520data-centric%2520AI%2520is%2520risky%2520from%2520a%2520set%2520of%2520approximately%250Aequally%2520accurate%2520models.%2520This%2520may%2520lead%2520to%2520severe%2520problems%2520in%2520model%2520selection%252C%250Avalidation%252C%2520and%2520explanation.%2520To%2520tackle%2520this%2520matter%252C%2520we%2520conducted%2520real%2520dataset%250Aexperiments%2520to%2520observe%2520the%2520impact%2520of%2520balancing%2520methods%2520on%2520predictive%250Amultiplicity%2520through%2520the%2520Rashomon%2520effect%2520by%2520using%2520a%2520newly%2520proposed%2520metric%250Aobscurity%2520in%2520addition%2520to%2520the%2520existing%2520ones%253A%2520ambiguity%2520and%2520discrepancy.%2520Our%250Afindings%2520showed%2520that%2520balancing%2520methods%2520inflate%2520the%2520predictive%2520multiplicity%2520and%250Ayield%2520varying%2520results.%2520To%2520monitor%2520the%2520trade-off%2520between%2520the%2520prediction%250Aperformance%2520and%2520predictive%2520multiplicity%2520for%2520conducting%2520the%2520modeling%2520process%250Aresponsibly%252C%2520we%2520proposed%2520using%2520the%2520extended%2520version%2520of%2520the%2520performance-gain%250Aplot%2520when%2520balancing%2520the%2520training%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01557v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Experimental%20Study%20on%20the%20Rashomon%20Effect%20of%20Balancing%20Methods%20in%0A%20%20Imbalanced%20Classification&entry.906535625=Mustafa%20Cavus%20and%20Przemys%C5%82aw%20Biecek&entry.1292438233=%20%20Predictive%20models%20may%20generate%20biased%20predictions%20when%20classifying%20imbalanced%0Adatasets.%20This%20happens%20when%20the%20model%20favors%20the%20majority%20class%2C%20leading%20to%20low%0Aperformance%20in%20accurately%20predicting%20the%20minority%20class.%20To%20address%20this%20issue%2C%0Abalancing%20or%20resampling%20methods%20are%20critical%20data-centric%20AI%20approaches%20in%20the%0Amodeling%20process%20to%20improve%20prediction%20performance.%20However%2C%20there%20have%20been%0Adebates%20and%20questions%20about%20the%20functionality%20of%20these%20methods%20in%20recent%20years.%0AIn%20particular%2C%20many%20candidate%20models%20may%20exhibit%20very%20similar%20predictive%0Aperformance%2C%20called%20the%20Rashomon%20effect%2C%20in%20model%20selection%2C%20and%20they%20may%20even%0Aproduce%20different%20predictions%20for%20the%20same%20observations.%20Selecting%20one%20of%20these%0Amodels%20without%20considering%20the%20predictive%20multiplicity%20--%20which%20is%20the%20case%20of%0Ayielding%20conflicting%20models%27%20predictions%20for%20any%20sample%20--%20can%20result%20in%20blind%0Aselection.%20In%20this%20paper%2C%20the%20impact%20of%20balancing%20methods%20on%20predictive%0Amultiplicity%20is%20examined%20using%20the%20Rashomon%20effect.%20It%20is%20crucial%20because%20the%0Ablind%20model%20selection%20in%20data-centric%20AI%20is%20risky%20from%20a%20set%20of%20approximately%0Aequally%20accurate%20models.%20This%20may%20lead%20to%20severe%20problems%20in%20model%20selection%2C%0Avalidation%2C%20and%20explanation.%20To%20tackle%20this%20matter%2C%20we%20conducted%20real%20dataset%0Aexperiments%20to%20observe%20the%20impact%20of%20balancing%20methods%20on%20predictive%0Amultiplicity%20through%20the%20Rashomon%20effect%20by%20using%20a%20newly%20proposed%20metric%0Aobscurity%20in%20addition%20to%20the%20existing%20ones%3A%20ambiguity%20and%20discrepancy.%20Our%0Afindings%20showed%20that%20balancing%20methods%20inflate%20the%20predictive%20multiplicity%20and%0Ayield%20varying%20results.%20To%20monitor%20the%20trade-off%20between%20the%20prediction%0Aperformance%20and%20predictive%20multiplicity%20for%20conducting%20the%20modeling%20process%0Aresponsibly%2C%20we%20proposed%20using%20the%20extended%20version%20of%20the%20performance-gain%0Aplot%20when%20balancing%20the%20training%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01557v4&entry.124074799=Read"},
{"title": "M4: Multi-Proxy Multi-Gate Mixture of Experts Network for Multiple\n  Instance Learning in Histopathology Image Analysis", "author": "Junyu Li and Ye Zhang and Wen Shu and Xiaobing Feng and Yingchun Wang and Pengju Yan and Xiaolin Li and Chulin Sha and Min He", "abstract": "  Multiple instance learning (MIL) has been successfully applied for whole\nslide images (WSIs) analysis in computational pathology, enabling a wide range\nof prediction tasks from tumor subtyping to inferring genetic mutations and\nmulti-omics biomarkers. However, existing MIL methods predominantly focus on\nsingle-task learning, resulting in not only overall low efficiency but also the\noverlook of inter-task relatedness. To address these issues, we proposed an\nadapted architecture of Multi-gate Mixture-of-experts with Multi-proxy for\nMultiple instance learning (M4), and applied this framework for simultaneous\nprediction of multiple genetic mutations from WSIs. The proposed M4 model has\ntwo main innovations: (1) utilizing a mixture of experts with multiple gating\nstrategies for multi-genetic mutation prediction on a single pathological\nslide; (2) constructing multi-proxy expert network and gate network for\ncomprehensive and effective modeling of pathological image information. Our\nmodel achieved significant improvements across five tested TCGA datasets in\ncomparison to current state-of-the-art single-task methods. The code is\navailable at:https://github.com/Bigyehahaha/M4.\n", "link": "http://arxiv.org/abs/2407.17267v1", "date": "2024-07-24", "relevancy": 1.9776, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5223}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4837}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M4%3A%20Multi-Proxy%20Multi-Gate%20Mixture%20of%20Experts%20Network%20for%20Multiple%0A%20%20Instance%20Learning%20in%20Histopathology%20Image%20Analysis&body=Title%3A%20M4%3A%20Multi-Proxy%20Multi-Gate%20Mixture%20of%20Experts%20Network%20for%20Multiple%0A%20%20Instance%20Learning%20in%20Histopathology%20Image%20Analysis%0AAuthor%3A%20Junyu%20Li%20and%20Ye%20Zhang%20and%20Wen%20Shu%20and%20Xiaobing%20Feng%20and%20Yingchun%20Wang%20and%20Pengju%20Yan%20and%20Xiaolin%20Li%20and%20Chulin%20Sha%20and%20Min%20He%0AAbstract%3A%20%20%20Multiple%20instance%20learning%20%28MIL%29%20has%20been%20successfully%20applied%20for%20whole%0Aslide%20images%20%28WSIs%29%20analysis%20in%20computational%20pathology%2C%20enabling%20a%20wide%20range%0Aof%20prediction%20tasks%20from%20tumor%20subtyping%20to%20inferring%20genetic%20mutations%20and%0Amulti-omics%20biomarkers.%20However%2C%20existing%20MIL%20methods%20predominantly%20focus%20on%0Asingle-task%20learning%2C%20resulting%20in%20not%20only%20overall%20low%20efficiency%20but%20also%20the%0Aoverlook%20of%20inter-task%20relatedness.%20To%20address%20these%20issues%2C%20we%20proposed%20an%0Aadapted%20architecture%20of%20Multi-gate%20Mixture-of-experts%20with%20Multi-proxy%20for%0AMultiple%20instance%20learning%20%28M4%29%2C%20and%20applied%20this%20framework%20for%20simultaneous%0Aprediction%20of%20multiple%20genetic%20mutations%20from%20WSIs.%20The%20proposed%20M4%20model%20has%0Atwo%20main%20innovations%3A%20%281%29%20utilizing%20a%20mixture%20of%20experts%20with%20multiple%20gating%0Astrategies%20for%20multi-genetic%20mutation%20prediction%20on%20a%20single%20pathological%0Aslide%3B%20%282%29%20constructing%20multi-proxy%20expert%20network%20and%20gate%20network%20for%0Acomprehensive%20and%20effective%20modeling%20of%20pathological%20image%20information.%20Our%0Amodel%20achieved%20significant%20improvements%20across%20five%20tested%20TCGA%20datasets%20in%0Acomparison%20to%20current%20state-of-the-art%20single-task%20methods.%20The%20code%20is%0Aavailable%20at%3Ahttps%3A//github.com/Bigyehahaha/M4.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17267v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM4%253A%2520Multi-Proxy%2520Multi-Gate%2520Mixture%2520of%2520Experts%2520Network%2520for%2520Multiple%250A%2520%2520Instance%2520Learning%2520in%2520Histopathology%2520Image%2520Analysis%26entry.906535625%3DJunyu%2520Li%2520and%2520Ye%2520Zhang%2520and%2520Wen%2520Shu%2520and%2520Xiaobing%2520Feng%2520and%2520Yingchun%2520Wang%2520and%2520Pengju%2520Yan%2520and%2520Xiaolin%2520Li%2520and%2520Chulin%2520Sha%2520and%2520Min%2520He%26entry.1292438233%3D%2520%2520Multiple%2520instance%2520learning%2520%2528MIL%2529%2520has%2520been%2520successfully%2520applied%2520for%2520whole%250Aslide%2520images%2520%2528WSIs%2529%2520analysis%2520in%2520computational%2520pathology%252C%2520enabling%2520a%2520wide%2520range%250Aof%2520prediction%2520tasks%2520from%2520tumor%2520subtyping%2520to%2520inferring%2520genetic%2520mutations%2520and%250Amulti-omics%2520biomarkers.%2520However%252C%2520existing%2520MIL%2520methods%2520predominantly%2520focus%2520on%250Asingle-task%2520learning%252C%2520resulting%2520in%2520not%2520only%2520overall%2520low%2520efficiency%2520but%2520also%2520the%250Aoverlook%2520of%2520inter-task%2520relatedness.%2520To%2520address%2520these%2520issues%252C%2520we%2520proposed%2520an%250Aadapted%2520architecture%2520of%2520Multi-gate%2520Mixture-of-experts%2520with%2520Multi-proxy%2520for%250AMultiple%2520instance%2520learning%2520%2528M4%2529%252C%2520and%2520applied%2520this%2520framework%2520for%2520simultaneous%250Aprediction%2520of%2520multiple%2520genetic%2520mutations%2520from%2520WSIs.%2520The%2520proposed%2520M4%2520model%2520has%250Atwo%2520main%2520innovations%253A%2520%25281%2529%2520utilizing%2520a%2520mixture%2520of%2520experts%2520with%2520multiple%2520gating%250Astrategies%2520for%2520multi-genetic%2520mutation%2520prediction%2520on%2520a%2520single%2520pathological%250Aslide%253B%2520%25282%2529%2520constructing%2520multi-proxy%2520expert%2520network%2520and%2520gate%2520network%2520for%250Acomprehensive%2520and%2520effective%2520modeling%2520of%2520pathological%2520image%2520information.%2520Our%250Amodel%2520achieved%2520significant%2520improvements%2520across%2520five%2520tested%2520TCGA%2520datasets%2520in%250Acomparison%2520to%2520current%2520state-of-the-art%2520single-task%2520methods.%2520The%2520code%2520is%250Aavailable%2520at%253Ahttps%253A//github.com/Bigyehahaha/M4.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17267v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M4%3A%20Multi-Proxy%20Multi-Gate%20Mixture%20of%20Experts%20Network%20for%20Multiple%0A%20%20Instance%20Learning%20in%20Histopathology%20Image%20Analysis&entry.906535625=Junyu%20Li%20and%20Ye%20Zhang%20and%20Wen%20Shu%20and%20Xiaobing%20Feng%20and%20Yingchun%20Wang%20and%20Pengju%20Yan%20and%20Xiaolin%20Li%20and%20Chulin%20Sha%20and%20Min%20He&entry.1292438233=%20%20Multiple%20instance%20learning%20%28MIL%29%20has%20been%20successfully%20applied%20for%20whole%0Aslide%20images%20%28WSIs%29%20analysis%20in%20computational%20pathology%2C%20enabling%20a%20wide%20range%0Aof%20prediction%20tasks%20from%20tumor%20subtyping%20to%20inferring%20genetic%20mutations%20and%0Amulti-omics%20biomarkers.%20However%2C%20existing%20MIL%20methods%20predominantly%20focus%20on%0Asingle-task%20learning%2C%20resulting%20in%20not%20only%20overall%20low%20efficiency%20but%20also%20the%0Aoverlook%20of%20inter-task%20relatedness.%20To%20address%20these%20issues%2C%20we%20proposed%20an%0Aadapted%20architecture%20of%20Multi-gate%20Mixture-of-experts%20with%20Multi-proxy%20for%0AMultiple%20instance%20learning%20%28M4%29%2C%20and%20applied%20this%20framework%20for%20simultaneous%0Aprediction%20of%20multiple%20genetic%20mutations%20from%20WSIs.%20The%20proposed%20M4%20model%20has%0Atwo%20main%20innovations%3A%20%281%29%20utilizing%20a%20mixture%20of%20experts%20with%20multiple%20gating%0Astrategies%20for%20multi-genetic%20mutation%20prediction%20on%20a%20single%20pathological%0Aslide%3B%20%282%29%20constructing%20multi-proxy%20expert%20network%20and%20gate%20network%20for%0Acomprehensive%20and%20effective%20modeling%20of%20pathological%20image%20information.%20Our%0Amodel%20achieved%20significant%20improvements%20across%20five%20tested%20TCGA%20datasets%20in%0Acomparison%20to%20current%20state-of-the-art%20single-task%20methods.%20The%20code%20is%0Aavailable%20at%3Ahttps%3A//github.com/Bigyehahaha/M4.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17267v1&entry.124074799=Read"},
{"title": "Pretrained Visual Representations in Reinforcement Learning", "author": "Emlyn Williams and Athanasios Polydoros", "abstract": "  Visual reinforcement learning (RL) has made significant progress in recent\nyears, but the choice of visual feature extractor remains a crucial design\ndecision. This paper compares the performance of RL algorithms that train a\nconvolutional neural network (CNN) from scratch with those that utilize\npre-trained visual representations (PVRs). We evaluate the Dormant Ratio\nMinimization (DRM) algorithm, a state-of-the-art visual RL method, against\nthree PVRs: ResNet18, DINOv2, and Visual Cortex (VC). We use the Metaworld\nPush-v2 and Drawer-Open-v2 tasks for our comparison. Our results show that the\nchoice of training from scratch compared to using PVRs for maximising\nperformance is task-dependent, but PVRs offer advantages in terms of reduced\nreplay buffer size and faster training times. We also identify a strong\ncorrelation between the dormant ratio and model performance, highlighting the\nimportance of exploration in visual RL. Our study provides insights into the\ntrade-offs between training from scratch and using PVRs, informing the design\nof future visual RL algorithms.\n", "link": "http://arxiv.org/abs/2407.17238v1", "date": "2024-07-24", "relevancy": 1.9663, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5038}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4868}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pretrained%20Visual%20Representations%20in%20Reinforcement%20Learning&body=Title%3A%20Pretrained%20Visual%20Representations%20in%20Reinforcement%20Learning%0AAuthor%3A%20Emlyn%20Williams%20and%20Athanasios%20Polydoros%0AAbstract%3A%20%20%20Visual%20reinforcement%20learning%20%28RL%29%20has%20made%20significant%20progress%20in%20recent%0Ayears%2C%20but%20the%20choice%20of%20visual%20feature%20extractor%20remains%20a%20crucial%20design%0Adecision.%20This%20paper%20compares%20the%20performance%20of%20RL%20algorithms%20that%20train%20a%0Aconvolutional%20neural%20network%20%28CNN%29%20from%20scratch%20with%20those%20that%20utilize%0Apre-trained%20visual%20representations%20%28PVRs%29.%20We%20evaluate%20the%20Dormant%20Ratio%0AMinimization%20%28DRM%29%20algorithm%2C%20a%20state-of-the-art%20visual%20RL%20method%2C%20against%0Athree%20PVRs%3A%20ResNet18%2C%20DINOv2%2C%20and%20Visual%20Cortex%20%28VC%29.%20We%20use%20the%20Metaworld%0APush-v2%20and%20Drawer-Open-v2%20tasks%20for%20our%20comparison.%20Our%20results%20show%20that%20the%0Achoice%20of%20training%20from%20scratch%20compared%20to%20using%20PVRs%20for%20maximising%0Aperformance%20is%20task-dependent%2C%20but%20PVRs%20offer%20advantages%20in%20terms%20of%20reduced%0Areplay%20buffer%20size%20and%20faster%20training%20times.%20We%20also%20identify%20a%20strong%0Acorrelation%20between%20the%20dormant%20ratio%20and%20model%20performance%2C%20highlighting%20the%0Aimportance%20of%20exploration%20in%20visual%20RL.%20Our%20study%20provides%20insights%20into%20the%0Atrade-offs%20between%20training%20from%20scratch%20and%20using%20PVRs%2C%20informing%20the%20design%0Aof%20future%20visual%20RL%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17238v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPretrained%2520Visual%2520Representations%2520in%2520Reinforcement%2520Learning%26entry.906535625%3DEmlyn%2520Williams%2520and%2520Athanasios%2520Polydoros%26entry.1292438233%3D%2520%2520Visual%2520reinforcement%2520learning%2520%2528RL%2529%2520has%2520made%2520significant%2520progress%2520in%2520recent%250Ayears%252C%2520but%2520the%2520choice%2520of%2520visual%2520feature%2520extractor%2520remains%2520a%2520crucial%2520design%250Adecision.%2520This%2520paper%2520compares%2520the%2520performance%2520of%2520RL%2520algorithms%2520that%2520train%2520a%250Aconvolutional%2520neural%2520network%2520%2528CNN%2529%2520from%2520scratch%2520with%2520those%2520that%2520utilize%250Apre-trained%2520visual%2520representations%2520%2528PVRs%2529.%2520We%2520evaluate%2520the%2520Dormant%2520Ratio%250AMinimization%2520%2528DRM%2529%2520algorithm%252C%2520a%2520state-of-the-art%2520visual%2520RL%2520method%252C%2520against%250Athree%2520PVRs%253A%2520ResNet18%252C%2520DINOv2%252C%2520and%2520Visual%2520Cortex%2520%2528VC%2529.%2520We%2520use%2520the%2520Metaworld%250APush-v2%2520and%2520Drawer-Open-v2%2520tasks%2520for%2520our%2520comparison.%2520Our%2520results%2520show%2520that%2520the%250Achoice%2520of%2520training%2520from%2520scratch%2520compared%2520to%2520using%2520PVRs%2520for%2520maximising%250Aperformance%2520is%2520task-dependent%252C%2520but%2520PVRs%2520offer%2520advantages%2520in%2520terms%2520of%2520reduced%250Areplay%2520buffer%2520size%2520and%2520faster%2520training%2520times.%2520We%2520also%2520identify%2520a%2520strong%250Acorrelation%2520between%2520the%2520dormant%2520ratio%2520and%2520model%2520performance%252C%2520highlighting%2520the%250Aimportance%2520of%2520exploration%2520in%2520visual%2520RL.%2520Our%2520study%2520provides%2520insights%2520into%2520the%250Atrade-offs%2520between%2520training%2520from%2520scratch%2520and%2520using%2520PVRs%252C%2520informing%2520the%2520design%250Aof%2520future%2520visual%2520RL%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17238v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pretrained%20Visual%20Representations%20in%20Reinforcement%20Learning&entry.906535625=Emlyn%20Williams%20and%20Athanasios%20Polydoros&entry.1292438233=%20%20Visual%20reinforcement%20learning%20%28RL%29%20has%20made%20significant%20progress%20in%20recent%0Ayears%2C%20but%20the%20choice%20of%20visual%20feature%20extractor%20remains%20a%20crucial%20design%0Adecision.%20This%20paper%20compares%20the%20performance%20of%20RL%20algorithms%20that%20train%20a%0Aconvolutional%20neural%20network%20%28CNN%29%20from%20scratch%20with%20those%20that%20utilize%0Apre-trained%20visual%20representations%20%28PVRs%29.%20We%20evaluate%20the%20Dormant%20Ratio%0AMinimization%20%28DRM%29%20algorithm%2C%20a%20state-of-the-art%20visual%20RL%20method%2C%20against%0Athree%20PVRs%3A%20ResNet18%2C%20DINOv2%2C%20and%20Visual%20Cortex%20%28VC%29.%20We%20use%20the%20Metaworld%0APush-v2%20and%20Drawer-Open-v2%20tasks%20for%20our%20comparison.%20Our%20results%20show%20that%20the%0Achoice%20of%20training%20from%20scratch%20compared%20to%20using%20PVRs%20for%20maximising%0Aperformance%20is%20task-dependent%2C%20but%20PVRs%20offer%20advantages%20in%20terms%20of%20reduced%0Areplay%20buffer%20size%20and%20faster%20training%20times.%20We%20also%20identify%20a%20strong%0Acorrelation%20between%20the%20dormant%20ratio%20and%20model%20performance%2C%20highlighting%20the%0Aimportance%20of%20exploration%20in%20visual%20RL.%20Our%20study%20provides%20insights%20into%20the%0Atrade-offs%20between%20training%20from%20scratch%20and%20using%20PVRs%2C%20informing%20the%20design%0Aof%20future%20visual%20RL%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17238v1&entry.124074799=Read"},
{"title": "High-Probability Convergence for Composite and Distributed Stochastic\n  Minimization and Variational Inequalities with Heavy-Tailed Noise", "author": "Eduard Gorbunov and Abdurakhmon Sadiev and Marina Danilova and Samuel Horv\u00e1th and Gauthier Gidel and Pavel Dvurechensky and Alexander Gasnikov and Peter Richt\u00e1rik", "abstract": "  High-probability analysis of stochastic first-order optimization methods\nunder mild assumptions on the noise has been gaining a lot of attention in\nrecent years. Typically, gradient clipping is one of the key algorithmic\ningredients to derive good high-probability guarantees when the noise is\nheavy-tailed. However, if implemented na\\\"ively, clipping can spoil the\nconvergence of the popular methods for composite and distributed optimization\n(Prox-SGD/Parallel SGD) even in the absence of any noise. Due to this reason,\nmany works on high-probability analysis consider only unconstrained\nnon-distributed problems, and the existing results for composite/distributed\nproblems do not include some important special cases (like strongly convex\nproblems) and are not optimal. To address this issue, we propose new stochastic\nmethods for composite and distributed optimization based on the clipping of\nstochastic gradient differences and prove tight high-probability convergence\nresults (including nearly optimal ones) for the new methods. Using similar\nideas, we also develop new methods for composite and distributed variational\ninequalities and analyze the high-probability convergence of these methods.\n", "link": "http://arxiv.org/abs/2310.01860v2", "date": "2024-07-24", "relevancy": 1.9573, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5111}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4918}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Probability%20Convergence%20for%20Composite%20and%20Distributed%20Stochastic%0A%20%20Minimization%20and%20Variational%20Inequalities%20with%20Heavy-Tailed%20Noise&body=Title%3A%20High-Probability%20Convergence%20for%20Composite%20and%20Distributed%20Stochastic%0A%20%20Minimization%20and%20Variational%20Inequalities%20with%20Heavy-Tailed%20Noise%0AAuthor%3A%20Eduard%20Gorbunov%20and%20Abdurakhmon%20Sadiev%20and%20Marina%20Danilova%20and%20Samuel%20Horv%C3%A1th%20and%20Gauthier%20Gidel%20and%20Pavel%20Dvurechensky%20and%20Alexander%20Gasnikov%20and%20Peter%20Richt%C3%A1rik%0AAbstract%3A%20%20%20High-probability%20analysis%20of%20stochastic%20first-order%20optimization%20methods%0Aunder%20mild%20assumptions%20on%20the%20noise%20has%20been%20gaining%20a%20lot%20of%20attention%20in%0Arecent%20years.%20Typically%2C%20gradient%20clipping%20is%20one%20of%20the%20key%20algorithmic%0Aingredients%20to%20derive%20good%20high-probability%20guarantees%20when%20the%20noise%20is%0Aheavy-tailed.%20However%2C%20if%20implemented%20na%5C%22ively%2C%20clipping%20can%20spoil%20the%0Aconvergence%20of%20the%20popular%20methods%20for%20composite%20and%20distributed%20optimization%0A%28Prox-SGD/Parallel%20SGD%29%20even%20in%20the%20absence%20of%20any%20noise.%20Due%20to%20this%20reason%2C%0Amany%20works%20on%20high-probability%20analysis%20consider%20only%20unconstrained%0Anon-distributed%20problems%2C%20and%20the%20existing%20results%20for%20composite/distributed%0Aproblems%20do%20not%20include%20some%20important%20special%20cases%20%28like%20strongly%20convex%0Aproblems%29%20and%20are%20not%20optimal.%20To%20address%20this%20issue%2C%20we%20propose%20new%20stochastic%0Amethods%20for%20composite%20and%20distributed%20optimization%20based%20on%20the%20clipping%20of%0Astochastic%20gradient%20differences%20and%20prove%20tight%20high-probability%20convergence%0Aresults%20%28including%20nearly%20optimal%20ones%29%20for%20the%20new%20methods.%20Using%20similar%0Aideas%2C%20we%20also%20develop%20new%20methods%20for%20composite%20and%20distributed%20variational%0Ainequalities%20and%20analyze%20the%20high-probability%20convergence%20of%20these%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.01860v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Probability%2520Convergence%2520for%2520Composite%2520and%2520Distributed%2520Stochastic%250A%2520%2520Minimization%2520and%2520Variational%2520Inequalities%2520with%2520Heavy-Tailed%2520Noise%26entry.906535625%3DEduard%2520Gorbunov%2520and%2520Abdurakhmon%2520Sadiev%2520and%2520Marina%2520Danilova%2520and%2520Samuel%2520Horv%25C3%25A1th%2520and%2520Gauthier%2520Gidel%2520and%2520Pavel%2520Dvurechensky%2520and%2520Alexander%2520Gasnikov%2520and%2520Peter%2520Richt%25C3%25A1rik%26entry.1292438233%3D%2520%2520High-probability%2520analysis%2520of%2520stochastic%2520first-order%2520optimization%2520methods%250Aunder%2520mild%2520assumptions%2520on%2520the%2520noise%2520has%2520been%2520gaining%2520a%2520lot%2520of%2520attention%2520in%250Arecent%2520years.%2520Typically%252C%2520gradient%2520clipping%2520is%2520one%2520of%2520the%2520key%2520algorithmic%250Aingredients%2520to%2520derive%2520good%2520high-probability%2520guarantees%2520when%2520the%2520noise%2520is%250Aheavy-tailed.%2520However%252C%2520if%2520implemented%2520na%255C%2522ively%252C%2520clipping%2520can%2520spoil%2520the%250Aconvergence%2520of%2520the%2520popular%2520methods%2520for%2520composite%2520and%2520distributed%2520optimization%250A%2528Prox-SGD/Parallel%2520SGD%2529%2520even%2520in%2520the%2520absence%2520of%2520any%2520noise.%2520Due%2520to%2520this%2520reason%252C%250Amany%2520works%2520on%2520high-probability%2520analysis%2520consider%2520only%2520unconstrained%250Anon-distributed%2520problems%252C%2520and%2520the%2520existing%2520results%2520for%2520composite/distributed%250Aproblems%2520do%2520not%2520include%2520some%2520important%2520special%2520cases%2520%2528like%2520strongly%2520convex%250Aproblems%2529%2520and%2520are%2520not%2520optimal.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520new%2520stochastic%250Amethods%2520for%2520composite%2520and%2520distributed%2520optimization%2520based%2520on%2520the%2520clipping%2520of%250Astochastic%2520gradient%2520differences%2520and%2520prove%2520tight%2520high-probability%2520convergence%250Aresults%2520%2528including%2520nearly%2520optimal%2520ones%2529%2520for%2520the%2520new%2520methods.%2520Using%2520similar%250Aideas%252C%2520we%2520also%2520develop%2520new%2520methods%2520for%2520composite%2520and%2520distributed%2520variational%250Ainequalities%2520and%2520analyze%2520the%2520high-probability%2520convergence%2520of%2520these%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.01860v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Probability%20Convergence%20for%20Composite%20and%20Distributed%20Stochastic%0A%20%20Minimization%20and%20Variational%20Inequalities%20with%20Heavy-Tailed%20Noise&entry.906535625=Eduard%20Gorbunov%20and%20Abdurakhmon%20Sadiev%20and%20Marina%20Danilova%20and%20Samuel%20Horv%C3%A1th%20and%20Gauthier%20Gidel%20and%20Pavel%20Dvurechensky%20and%20Alexander%20Gasnikov%20and%20Peter%20Richt%C3%A1rik&entry.1292438233=%20%20High-probability%20analysis%20of%20stochastic%20first-order%20optimization%20methods%0Aunder%20mild%20assumptions%20on%20the%20noise%20has%20been%20gaining%20a%20lot%20of%20attention%20in%0Arecent%20years.%20Typically%2C%20gradient%20clipping%20is%20one%20of%20the%20key%20algorithmic%0Aingredients%20to%20derive%20good%20high-probability%20guarantees%20when%20the%20noise%20is%0Aheavy-tailed.%20However%2C%20if%20implemented%20na%5C%22ively%2C%20clipping%20can%20spoil%20the%0Aconvergence%20of%20the%20popular%20methods%20for%20composite%20and%20distributed%20optimization%0A%28Prox-SGD/Parallel%20SGD%29%20even%20in%20the%20absence%20of%20any%20noise.%20Due%20to%20this%20reason%2C%0Amany%20works%20on%20high-probability%20analysis%20consider%20only%20unconstrained%0Anon-distributed%20problems%2C%20and%20the%20existing%20results%20for%20composite/distributed%0Aproblems%20do%20not%20include%20some%20important%20special%20cases%20%28like%20strongly%20convex%0Aproblems%29%20and%20are%20not%20optimal.%20To%20address%20this%20issue%2C%20we%20propose%20new%20stochastic%0Amethods%20for%20composite%20and%20distributed%20optimization%20based%20on%20the%20clipping%20of%0Astochastic%20gradient%20differences%20and%20prove%20tight%20high-probability%20convergence%0Aresults%20%28including%20nearly%20optimal%20ones%29%20for%20the%20new%20methods.%20Using%20similar%0Aideas%2C%20we%20also%20develop%20new%20methods%20for%20composite%20and%20distributed%20variational%0Ainequalities%20and%20analyze%20the%20high-probability%20convergence%20of%20these%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.01860v2&entry.124074799=Read"},
{"title": "Sampling-Based Hierarchical Trajectory Planning for Formation Flight", "author": "Qingzhao Liu and Bailing Tian and Xuewei Zhang and Junjie Lu and Zhiyu Li", "abstract": "  Formation flight of unmanned aerial vehicles (UAVs) poses significant\nchallenges in terms of safety and formation keeping, particularly in cluttered\nenvironments. However, existing methods often struggle to simultaneously\nsatisfy these two critical requirements. To address this issue, this paper\nproposes a sampling-based trajectory planning method with a hierarchical\nstructure for formation flight in dense obstacle environments. To ensure\nreliable local sensing information sharing among UAVs, each UAV generates a\nsafe flight corridor (SFC), which is transmitted to the leader UAV.\nSubsequently, a sampling-based formation guidance path generation method is\ndesigned as the front-end strategy, steering the formation to fly in the\ndesired shape safely with the formation connectivity provided by the SFCs.\nFurthermore, a model predictive path integral (MPPI) based distributed\ntrajectory optimization method is developed as the back-end part, which ensures\nthe smoothness, safety and dynamics feasibility of the executable trajectory.\nTo validate the efficiency of the developed algorithm, comprehensive simulation\ncomparisons are conducted. The supplementary simulation video can be seen at\nhttps://www.youtube.com/watch?v=xSxbUN0tn1M.\n", "link": "http://arxiv.org/abs/2407.17392v1", "date": "2024-07-24", "relevancy": 1.9266, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5069}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4692}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sampling-Based%20Hierarchical%20Trajectory%20Planning%20for%20Formation%20Flight&body=Title%3A%20Sampling-Based%20Hierarchical%20Trajectory%20Planning%20for%20Formation%20Flight%0AAuthor%3A%20Qingzhao%20Liu%20and%20Bailing%20Tian%20and%20Xuewei%20Zhang%20and%20Junjie%20Lu%20and%20Zhiyu%20Li%0AAbstract%3A%20%20%20Formation%20flight%20of%20unmanned%20aerial%20vehicles%20%28UAVs%29%20poses%20significant%0Achallenges%20in%20terms%20of%20safety%20and%20formation%20keeping%2C%20particularly%20in%20cluttered%0Aenvironments.%20However%2C%20existing%20methods%20often%20struggle%20to%20simultaneously%0Asatisfy%20these%20two%20critical%20requirements.%20To%20address%20this%20issue%2C%20this%20paper%0Aproposes%20a%20sampling-based%20trajectory%20planning%20method%20with%20a%20hierarchical%0Astructure%20for%20formation%20flight%20in%20dense%20obstacle%20environments.%20To%20ensure%0Areliable%20local%20sensing%20information%20sharing%20among%20UAVs%2C%20each%20UAV%20generates%20a%0Asafe%20flight%20corridor%20%28SFC%29%2C%20which%20is%20transmitted%20to%20the%20leader%20UAV.%0ASubsequently%2C%20a%20sampling-based%20formation%20guidance%20path%20generation%20method%20is%0Adesigned%20as%20the%20front-end%20strategy%2C%20steering%20the%20formation%20to%20fly%20in%20the%0Adesired%20shape%20safely%20with%20the%20formation%20connectivity%20provided%20by%20the%20SFCs.%0AFurthermore%2C%20a%20model%20predictive%20path%20integral%20%28MPPI%29%20based%20distributed%0Atrajectory%20optimization%20method%20is%20developed%20as%20the%20back-end%20part%2C%20which%20ensures%0Athe%20smoothness%2C%20safety%20and%20dynamics%20feasibility%20of%20the%20executable%20trajectory.%0ATo%20validate%20the%20efficiency%20of%20the%20developed%20algorithm%2C%20comprehensive%20simulation%0Acomparisons%20are%20conducted.%20The%20supplementary%20simulation%20video%20can%20be%20seen%20at%0Ahttps%3A//www.youtube.com/watch%3Fv%3DxSxbUN0tn1M.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17392v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSampling-Based%2520Hierarchical%2520Trajectory%2520Planning%2520for%2520Formation%2520Flight%26entry.906535625%3DQingzhao%2520Liu%2520and%2520Bailing%2520Tian%2520and%2520Xuewei%2520Zhang%2520and%2520Junjie%2520Lu%2520and%2520Zhiyu%2520Li%26entry.1292438233%3D%2520%2520Formation%2520flight%2520of%2520unmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%2520poses%2520significant%250Achallenges%2520in%2520terms%2520of%2520safety%2520and%2520formation%2520keeping%252C%2520particularly%2520in%2520cluttered%250Aenvironments.%2520However%252C%2520existing%2520methods%2520often%2520struggle%2520to%2520simultaneously%250Asatisfy%2520these%2520two%2520critical%2520requirements.%2520To%2520address%2520this%2520issue%252C%2520this%2520paper%250Aproposes%2520a%2520sampling-based%2520trajectory%2520planning%2520method%2520with%2520a%2520hierarchical%250Astructure%2520for%2520formation%2520flight%2520in%2520dense%2520obstacle%2520environments.%2520To%2520ensure%250Areliable%2520local%2520sensing%2520information%2520sharing%2520among%2520UAVs%252C%2520each%2520UAV%2520generates%2520a%250Asafe%2520flight%2520corridor%2520%2528SFC%2529%252C%2520which%2520is%2520transmitted%2520to%2520the%2520leader%2520UAV.%250ASubsequently%252C%2520a%2520sampling-based%2520formation%2520guidance%2520path%2520generation%2520method%2520is%250Adesigned%2520as%2520the%2520front-end%2520strategy%252C%2520steering%2520the%2520formation%2520to%2520fly%2520in%2520the%250Adesired%2520shape%2520safely%2520with%2520the%2520formation%2520connectivity%2520provided%2520by%2520the%2520SFCs.%250AFurthermore%252C%2520a%2520model%2520predictive%2520path%2520integral%2520%2528MPPI%2529%2520based%2520distributed%250Atrajectory%2520optimization%2520method%2520is%2520developed%2520as%2520the%2520back-end%2520part%252C%2520which%2520ensures%250Athe%2520smoothness%252C%2520safety%2520and%2520dynamics%2520feasibility%2520of%2520the%2520executable%2520trajectory.%250ATo%2520validate%2520the%2520efficiency%2520of%2520the%2520developed%2520algorithm%252C%2520comprehensive%2520simulation%250Acomparisons%2520are%2520conducted.%2520The%2520supplementary%2520simulation%2520video%2520can%2520be%2520seen%2520at%250Ahttps%253A//www.youtube.com/watch%253Fv%253DxSxbUN0tn1M.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17392v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sampling-Based%20Hierarchical%20Trajectory%20Planning%20for%20Formation%20Flight&entry.906535625=Qingzhao%20Liu%20and%20Bailing%20Tian%20and%20Xuewei%20Zhang%20and%20Junjie%20Lu%20and%20Zhiyu%20Li&entry.1292438233=%20%20Formation%20flight%20of%20unmanned%20aerial%20vehicles%20%28UAVs%29%20poses%20significant%0Achallenges%20in%20terms%20of%20safety%20and%20formation%20keeping%2C%20particularly%20in%20cluttered%0Aenvironments.%20However%2C%20existing%20methods%20often%20struggle%20to%20simultaneously%0Asatisfy%20these%20two%20critical%20requirements.%20To%20address%20this%20issue%2C%20this%20paper%0Aproposes%20a%20sampling-based%20trajectory%20planning%20method%20with%20a%20hierarchical%0Astructure%20for%20formation%20flight%20in%20dense%20obstacle%20environments.%20To%20ensure%0Areliable%20local%20sensing%20information%20sharing%20among%20UAVs%2C%20each%20UAV%20generates%20a%0Asafe%20flight%20corridor%20%28SFC%29%2C%20which%20is%20transmitted%20to%20the%20leader%20UAV.%0ASubsequently%2C%20a%20sampling-based%20formation%20guidance%20path%20generation%20method%20is%0Adesigned%20as%20the%20front-end%20strategy%2C%20steering%20the%20formation%20to%20fly%20in%20the%0Adesired%20shape%20safely%20with%20the%20formation%20connectivity%20provided%20by%20the%20SFCs.%0AFurthermore%2C%20a%20model%20predictive%20path%20integral%20%28MPPI%29%20based%20distributed%0Atrajectory%20optimization%20method%20is%20developed%20as%20the%20back-end%20part%2C%20which%20ensures%0Athe%20smoothness%2C%20safety%20and%20dynamics%20feasibility%20of%20the%20executable%20trajectory.%0ATo%20validate%20the%20efficiency%20of%20the%20developed%20algorithm%2C%20comprehensive%20simulation%0Acomparisons%20are%20conducted.%20The%20supplementary%20simulation%20video%20can%20be%20seen%20at%0Ahttps%3A//www.youtube.com/watch%3Fv%3DxSxbUN0tn1M.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17392v1&entry.124074799=Read"},
{"title": "Channel-Aware Low-Rank Adaptation in Time Series Forecasting", "author": "Tong Nie and Yuewen Mei and Guoyang Qin and Jian Sun and Wei Ma", "abstract": "  The balance between model capacity and generalization has been a key focus of\nrecent discussions in long-term time series forecasting. Two representative\nchannel strategies are closely associated with model expressivity and\nrobustness, including channel independence (CI) and channel dependence (CD).\nThe former adopts individual channel treatment and has been shown to be more\nrobust to distribution shifts, but lacks sufficient capacity to model\nmeaningful channel interactions. The latter is more expressive for representing\ncomplex cross-channel dependencies, but is prone to overfitting. To balance the\ntwo strategies, we present a channel-aware low-rank adaptation method to\ncondition CD models on identity-aware individual components. As a plug-in\nsolution, it is adaptable for a wide range of backbone architectures. Extensive\nexperiments show that it can consistently and significantly improve the\nperformance of both CI and CD models with demonstrated efficiency and\nflexibility. The code is available at https://github.com/tongnie/C-LoRA.\n", "link": "http://arxiv.org/abs/2407.17246v1", "date": "2024-07-24", "relevancy": 1.9241, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5058}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4786}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Channel-Aware%20Low-Rank%20Adaptation%20in%20Time%20Series%20Forecasting&body=Title%3A%20Channel-Aware%20Low-Rank%20Adaptation%20in%20Time%20Series%20Forecasting%0AAuthor%3A%20Tong%20Nie%20and%20Yuewen%20Mei%20and%20Guoyang%20Qin%20and%20Jian%20Sun%20and%20Wei%20Ma%0AAbstract%3A%20%20%20The%20balance%20between%20model%20capacity%20and%20generalization%20has%20been%20a%20key%20focus%20of%0Arecent%20discussions%20in%20long-term%20time%20series%20forecasting.%20Two%20representative%0Achannel%20strategies%20are%20closely%20associated%20with%20model%20expressivity%20and%0Arobustness%2C%20including%20channel%20independence%20%28CI%29%20and%20channel%20dependence%20%28CD%29.%0AThe%20former%20adopts%20individual%20channel%20treatment%20and%20has%20been%20shown%20to%20be%20more%0Arobust%20to%20distribution%20shifts%2C%20but%20lacks%20sufficient%20capacity%20to%20model%0Ameaningful%20channel%20interactions.%20The%20latter%20is%20more%20expressive%20for%20representing%0Acomplex%20cross-channel%20dependencies%2C%20but%20is%20prone%20to%20overfitting.%20To%20balance%20the%0Atwo%20strategies%2C%20we%20present%20a%20channel-aware%20low-rank%20adaptation%20method%20to%0Acondition%20CD%20models%20on%20identity-aware%20individual%20components.%20As%20a%20plug-in%0Asolution%2C%20it%20is%20adaptable%20for%20a%20wide%20range%20of%20backbone%20architectures.%20Extensive%0Aexperiments%20show%20that%20it%20can%20consistently%20and%20significantly%20improve%20the%0Aperformance%20of%20both%20CI%20and%20CD%20models%20with%20demonstrated%20efficiency%20and%0Aflexibility.%20The%20code%20is%20available%20at%20https%3A//github.com/tongnie/C-LoRA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17246v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChannel-Aware%2520Low-Rank%2520Adaptation%2520in%2520Time%2520Series%2520Forecasting%26entry.906535625%3DTong%2520Nie%2520and%2520Yuewen%2520Mei%2520and%2520Guoyang%2520Qin%2520and%2520Jian%2520Sun%2520and%2520Wei%2520Ma%26entry.1292438233%3D%2520%2520The%2520balance%2520between%2520model%2520capacity%2520and%2520generalization%2520has%2520been%2520a%2520key%2520focus%2520of%250Arecent%2520discussions%2520in%2520long-term%2520time%2520series%2520forecasting.%2520Two%2520representative%250Achannel%2520strategies%2520are%2520closely%2520associated%2520with%2520model%2520expressivity%2520and%250Arobustness%252C%2520including%2520channel%2520independence%2520%2528CI%2529%2520and%2520channel%2520dependence%2520%2528CD%2529.%250AThe%2520former%2520adopts%2520individual%2520channel%2520treatment%2520and%2520has%2520been%2520shown%2520to%2520be%2520more%250Arobust%2520to%2520distribution%2520shifts%252C%2520but%2520lacks%2520sufficient%2520capacity%2520to%2520model%250Ameaningful%2520channel%2520interactions.%2520The%2520latter%2520is%2520more%2520expressive%2520for%2520representing%250Acomplex%2520cross-channel%2520dependencies%252C%2520but%2520is%2520prone%2520to%2520overfitting.%2520To%2520balance%2520the%250Atwo%2520strategies%252C%2520we%2520present%2520a%2520channel-aware%2520low-rank%2520adaptation%2520method%2520to%250Acondition%2520CD%2520models%2520on%2520identity-aware%2520individual%2520components.%2520As%2520a%2520plug-in%250Asolution%252C%2520it%2520is%2520adaptable%2520for%2520a%2520wide%2520range%2520of%2520backbone%2520architectures.%2520Extensive%250Aexperiments%2520show%2520that%2520it%2520can%2520consistently%2520and%2520significantly%2520improve%2520the%250Aperformance%2520of%2520both%2520CI%2520and%2520CD%2520models%2520with%2520demonstrated%2520efficiency%2520and%250Aflexibility.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/tongnie/C-LoRA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17246v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Channel-Aware%20Low-Rank%20Adaptation%20in%20Time%20Series%20Forecasting&entry.906535625=Tong%20Nie%20and%20Yuewen%20Mei%20and%20Guoyang%20Qin%20and%20Jian%20Sun%20and%20Wei%20Ma&entry.1292438233=%20%20The%20balance%20between%20model%20capacity%20and%20generalization%20has%20been%20a%20key%20focus%20of%0Arecent%20discussions%20in%20long-term%20time%20series%20forecasting.%20Two%20representative%0Achannel%20strategies%20are%20closely%20associated%20with%20model%20expressivity%20and%0Arobustness%2C%20including%20channel%20independence%20%28CI%29%20and%20channel%20dependence%20%28CD%29.%0AThe%20former%20adopts%20individual%20channel%20treatment%20and%20has%20been%20shown%20to%20be%20more%0Arobust%20to%20distribution%20shifts%2C%20but%20lacks%20sufficient%20capacity%20to%20model%0Ameaningful%20channel%20interactions.%20The%20latter%20is%20more%20expressive%20for%20representing%0Acomplex%20cross-channel%20dependencies%2C%20but%20is%20prone%20to%20overfitting.%20To%20balance%20the%0Atwo%20strategies%2C%20we%20present%20a%20channel-aware%20low-rank%20adaptation%20method%20to%0Acondition%20CD%20models%20on%20identity-aware%20individual%20components.%20As%20a%20plug-in%0Asolution%2C%20it%20is%20adaptable%20for%20a%20wide%20range%20of%20backbone%20architectures.%20Extensive%0Aexperiments%20show%20that%20it%20can%20consistently%20and%20significantly%20improve%20the%0Aperformance%20of%20both%20CI%20and%20CD%20models%20with%20demonstrated%20efficiency%20and%0Aflexibility.%20The%20code%20is%20available%20at%20https%3A//github.com/tongnie/C-LoRA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17246v1&entry.124074799=Read"},
{"title": "Trans2Unet: Neural fusion for Nuclei Semantic Segmentation", "author": "Dinh-Phu Tran and Quoc-Anh Nguyen and Van-Truong Pham and Thi-Thao Tran", "abstract": "  Nuclei segmentation, despite its fundamental role in histopathological image\nanalysis, is still a challenge work. The main challenge of this task is the\nexistence of overlapping areas, which makes separating independent nuclei more\ncomplicated. In this paper, we propose a new two-branch architecture by\ncombining the Unet and TransUnet networks for nuclei segmentation task. In the\nproposed architecture, namely Trans2Unet, the input image is first sent into\nthe Unet branch whose the last convolution layer is removed. This branch makes\nthe network combine features from different spatial regions of the input image\nand localizes more precisely the regions of interest. The input image is also\nfed into the second branch. In the second branch, which is called TransUnet\nbranch, the input image will be divided into patches of images. With Vision\ntransformer (ViT) in architecture, TransUnet can serve as a powerful encoder\nfor medical image segmentation tasks and enhance image details by recovering\nlocalized spatial information. To boost up Trans2Unet efficiency and\nperformance, we proposed to infuse TransUnet with a computational-efficient\nvariation called \"Waterfall\" Atrous Spatial Pooling with Skip Connection\n(WASP-KC) module, which is inspired by the \"Waterfall\" Atrous Spatial Pooling\n(WASP) module. Experiment results on the 2018 Data Science Bowl benchmark show\nthe effectiveness and performance of the proposed architecture while compared\nwith previous segmentation models.\n", "link": "http://arxiv.org/abs/2407.17181v1", "date": "2024-07-24", "relevancy": 1.918, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4843}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4774}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trans2Unet%3A%20Neural%20fusion%20for%20Nuclei%20Semantic%20Segmentation&body=Title%3A%20Trans2Unet%3A%20Neural%20fusion%20for%20Nuclei%20Semantic%20Segmentation%0AAuthor%3A%20Dinh-Phu%20Tran%20and%20Quoc-Anh%20Nguyen%20and%20Van-Truong%20Pham%20and%20Thi-Thao%20Tran%0AAbstract%3A%20%20%20Nuclei%20segmentation%2C%20despite%20its%20fundamental%20role%20in%20histopathological%20image%0Aanalysis%2C%20is%20still%20a%20challenge%20work.%20The%20main%20challenge%20of%20this%20task%20is%20the%0Aexistence%20of%20overlapping%20areas%2C%20which%20makes%20separating%20independent%20nuclei%20more%0Acomplicated.%20In%20this%20paper%2C%20we%20propose%20a%20new%20two-branch%20architecture%20by%0Acombining%20the%20Unet%20and%20TransUnet%20networks%20for%20nuclei%20segmentation%20task.%20In%20the%0Aproposed%20architecture%2C%20namely%20Trans2Unet%2C%20the%20input%20image%20is%20first%20sent%20into%0Athe%20Unet%20branch%20whose%20the%20last%20convolution%20layer%20is%20removed.%20This%20branch%20makes%0Athe%20network%20combine%20features%20from%20different%20spatial%20regions%20of%20the%20input%20image%0Aand%20localizes%20more%20precisely%20the%20regions%20of%20interest.%20The%20input%20image%20is%20also%0Afed%20into%20the%20second%20branch.%20In%20the%20second%20branch%2C%20which%20is%20called%20TransUnet%0Abranch%2C%20the%20input%20image%20will%20be%20divided%20into%20patches%20of%20images.%20With%20Vision%0Atransformer%20%28ViT%29%20in%20architecture%2C%20TransUnet%20can%20serve%20as%20a%20powerful%20encoder%0Afor%20medical%20image%20segmentation%20tasks%20and%20enhance%20image%20details%20by%20recovering%0Alocalized%20spatial%20information.%20To%20boost%20up%20Trans2Unet%20efficiency%20and%0Aperformance%2C%20we%20proposed%20to%20infuse%20TransUnet%20with%20a%20computational-efficient%0Avariation%20called%20%22Waterfall%22%20Atrous%20Spatial%20Pooling%20with%20Skip%20Connection%0A%28WASP-KC%29%20module%2C%20which%20is%20inspired%20by%20the%20%22Waterfall%22%20Atrous%20Spatial%20Pooling%0A%28WASP%29%20module.%20Experiment%20results%20on%20the%202018%20Data%20Science%20Bowl%20benchmark%20show%0Athe%20effectiveness%20and%20performance%20of%20the%20proposed%20architecture%20while%20compared%0Awith%20previous%20segmentation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17181v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrans2Unet%253A%2520Neural%2520fusion%2520for%2520Nuclei%2520Semantic%2520Segmentation%26entry.906535625%3DDinh-Phu%2520Tran%2520and%2520Quoc-Anh%2520Nguyen%2520and%2520Van-Truong%2520Pham%2520and%2520Thi-Thao%2520Tran%26entry.1292438233%3D%2520%2520Nuclei%2520segmentation%252C%2520despite%2520its%2520fundamental%2520role%2520in%2520histopathological%2520image%250Aanalysis%252C%2520is%2520still%2520a%2520challenge%2520work.%2520The%2520main%2520challenge%2520of%2520this%2520task%2520is%2520the%250Aexistence%2520of%2520overlapping%2520areas%252C%2520which%2520makes%2520separating%2520independent%2520nuclei%2520more%250Acomplicated.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520two-branch%2520architecture%2520by%250Acombining%2520the%2520Unet%2520and%2520TransUnet%2520networks%2520for%2520nuclei%2520segmentation%2520task.%2520In%2520the%250Aproposed%2520architecture%252C%2520namely%2520Trans2Unet%252C%2520the%2520input%2520image%2520is%2520first%2520sent%2520into%250Athe%2520Unet%2520branch%2520whose%2520the%2520last%2520convolution%2520layer%2520is%2520removed.%2520This%2520branch%2520makes%250Athe%2520network%2520combine%2520features%2520from%2520different%2520spatial%2520regions%2520of%2520the%2520input%2520image%250Aand%2520localizes%2520more%2520precisely%2520the%2520regions%2520of%2520interest.%2520The%2520input%2520image%2520is%2520also%250Afed%2520into%2520the%2520second%2520branch.%2520In%2520the%2520second%2520branch%252C%2520which%2520is%2520called%2520TransUnet%250Abranch%252C%2520the%2520input%2520image%2520will%2520be%2520divided%2520into%2520patches%2520of%2520images.%2520With%2520Vision%250Atransformer%2520%2528ViT%2529%2520in%2520architecture%252C%2520TransUnet%2520can%2520serve%2520as%2520a%2520powerful%2520encoder%250Afor%2520medical%2520image%2520segmentation%2520tasks%2520and%2520enhance%2520image%2520details%2520by%2520recovering%250Alocalized%2520spatial%2520information.%2520To%2520boost%2520up%2520Trans2Unet%2520efficiency%2520and%250Aperformance%252C%2520we%2520proposed%2520to%2520infuse%2520TransUnet%2520with%2520a%2520computational-efficient%250Avariation%2520called%2520%2522Waterfall%2522%2520Atrous%2520Spatial%2520Pooling%2520with%2520Skip%2520Connection%250A%2528WASP-KC%2529%2520module%252C%2520which%2520is%2520inspired%2520by%2520the%2520%2522Waterfall%2522%2520Atrous%2520Spatial%2520Pooling%250A%2528WASP%2529%2520module.%2520Experiment%2520results%2520on%2520the%25202018%2520Data%2520Science%2520Bowl%2520benchmark%2520show%250Athe%2520effectiveness%2520and%2520performance%2520of%2520the%2520proposed%2520architecture%2520while%2520compared%250Awith%2520previous%2520segmentation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17181v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trans2Unet%3A%20Neural%20fusion%20for%20Nuclei%20Semantic%20Segmentation&entry.906535625=Dinh-Phu%20Tran%20and%20Quoc-Anh%20Nguyen%20and%20Van-Truong%20Pham%20and%20Thi-Thao%20Tran&entry.1292438233=%20%20Nuclei%20segmentation%2C%20despite%20its%20fundamental%20role%20in%20histopathological%20image%0Aanalysis%2C%20is%20still%20a%20challenge%20work.%20The%20main%20challenge%20of%20this%20task%20is%20the%0Aexistence%20of%20overlapping%20areas%2C%20which%20makes%20separating%20independent%20nuclei%20more%0Acomplicated.%20In%20this%20paper%2C%20we%20propose%20a%20new%20two-branch%20architecture%20by%0Acombining%20the%20Unet%20and%20TransUnet%20networks%20for%20nuclei%20segmentation%20task.%20In%20the%0Aproposed%20architecture%2C%20namely%20Trans2Unet%2C%20the%20input%20image%20is%20first%20sent%20into%0Athe%20Unet%20branch%20whose%20the%20last%20convolution%20layer%20is%20removed.%20This%20branch%20makes%0Athe%20network%20combine%20features%20from%20different%20spatial%20regions%20of%20the%20input%20image%0Aand%20localizes%20more%20precisely%20the%20regions%20of%20interest.%20The%20input%20image%20is%20also%0Afed%20into%20the%20second%20branch.%20In%20the%20second%20branch%2C%20which%20is%20called%20TransUnet%0Abranch%2C%20the%20input%20image%20will%20be%20divided%20into%20patches%20of%20images.%20With%20Vision%0Atransformer%20%28ViT%29%20in%20architecture%2C%20TransUnet%20can%20serve%20as%20a%20powerful%20encoder%0Afor%20medical%20image%20segmentation%20tasks%20and%20enhance%20image%20details%20by%20recovering%0Alocalized%20spatial%20information.%20To%20boost%20up%20Trans2Unet%20efficiency%20and%0Aperformance%2C%20we%20proposed%20to%20infuse%20TransUnet%20with%20a%20computational-efficient%0Avariation%20called%20%22Waterfall%22%20Atrous%20Spatial%20Pooling%20with%20Skip%20Connection%0A%28WASP-KC%29%20module%2C%20which%20is%20inspired%20by%20the%20%22Waterfall%22%20Atrous%20Spatial%20Pooling%0A%28WASP%29%20module.%20Experiment%20results%20on%20the%202018%20Data%20Science%20Bowl%20benchmark%20show%0Athe%20effectiveness%20and%20performance%20of%20the%20proposed%20architecture%20while%20compared%0Awith%20previous%20segmentation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17181v1&entry.124074799=Read"},
{"title": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations", "author": "Craig Innes and Subramanian Ramamoorthy", "abstract": "  Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations.\n", "link": "http://arxiv.org/abs/2405.15771v2", "date": "2024-07-24", "relevancy": 1.9121, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5131}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4793}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4627}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Splitting%20of%20Reusable%20Temporal%20Monitors%20for%20Rare%20Traffic%0A%20%20Violations&body=Title%3A%20Adaptive%20Splitting%20of%20Reusable%20Temporal%20Monitors%20for%20Rare%20Traffic%0A%20%20Violations%0AAuthor%3A%20Craig%20Innes%20and%20Subramanian%20Ramamoorthy%0AAbstract%3A%20%20%20Autonomous%20Vehicles%20%28AVs%29%20are%20often%20tested%20in%20simulation%20to%20estimate%20the%0Aprobability%20they%20will%20violate%20safety%20specifications.%20Two%20common%20issues%20arise%0Awhen%20using%20existing%20techniques%20to%20produce%20this%20estimation%3A%20If%20violations%20occur%0Ararely%2C%20simple%20Monte-Carlo%20sampling%20techniques%20can%20fail%20to%20produce%20efficient%0Aestimates%3B%20if%20simulation%20horizons%20are%20too%20long%2C%20importance%20sampling%20techniques%0A%28which%20learn%20proposal%20distributions%20from%20past%20simulations%29%20can%20fail%20to%0Aconverge.%20This%20paper%20addresses%20both%20issues%20by%20interleaving%20rare-event%20sampling%0Atechniques%20with%20online%20specification%20monitoring%20algorithms.%20We%20use%20adaptive%0Amulti-level%20splitting%20to%20decompose%20simulations%20into%20partial%20trajectories%2C%20then%0Acalculate%20the%20distance%20of%20those%20partial%20trajectories%20to%20failure%20by%20leveraging%0Arobustness%20metrics%20from%20Signal%20Temporal%20Logic%20%28STL%29.%20By%20caching%20those%20partial%0Arobustness%20metric%20values%2C%20we%20can%20efficiently%20re-use%20computations%20across%0Amultiple%20sampling%20stages.%20Our%20experiments%20on%20an%20interstate%20lane-change%20scenario%0Ashow%20our%20method%20is%20viable%20for%20testing%20simulated%20AV-pipelines%2C%20efficiently%0Aestimating%20failure%20probabilities%20for%20STL%20specifications%20based%20on%20real%20traffic%0Arules.%20We%20produce%20better%20estimates%20than%20Monte-Carlo%20and%20importance%20sampling%20in%0Afewer%20simulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15771v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Splitting%2520of%2520Reusable%2520Temporal%2520Monitors%2520for%2520Rare%2520Traffic%250A%2520%2520Violations%26entry.906535625%3DCraig%2520Innes%2520and%2520Subramanian%2520Ramamoorthy%26entry.1292438233%3D%2520%2520Autonomous%2520Vehicles%2520%2528AVs%2529%2520are%2520often%2520tested%2520in%2520simulation%2520to%2520estimate%2520the%250Aprobability%2520they%2520will%2520violate%2520safety%2520specifications.%2520Two%2520common%2520issues%2520arise%250Awhen%2520using%2520existing%2520techniques%2520to%2520produce%2520this%2520estimation%253A%2520If%2520violations%2520occur%250Ararely%252C%2520simple%2520Monte-Carlo%2520sampling%2520techniques%2520can%2520fail%2520to%2520produce%2520efficient%250Aestimates%253B%2520if%2520simulation%2520horizons%2520are%2520too%2520long%252C%2520importance%2520sampling%2520techniques%250A%2528which%2520learn%2520proposal%2520distributions%2520from%2520past%2520simulations%2529%2520can%2520fail%2520to%250Aconverge.%2520This%2520paper%2520addresses%2520both%2520issues%2520by%2520interleaving%2520rare-event%2520sampling%250Atechniques%2520with%2520online%2520specification%2520monitoring%2520algorithms.%2520We%2520use%2520adaptive%250Amulti-level%2520splitting%2520to%2520decompose%2520simulations%2520into%2520partial%2520trajectories%252C%2520then%250Acalculate%2520the%2520distance%2520of%2520those%2520partial%2520trajectories%2520to%2520failure%2520by%2520leveraging%250Arobustness%2520metrics%2520from%2520Signal%2520Temporal%2520Logic%2520%2528STL%2529.%2520By%2520caching%2520those%2520partial%250Arobustness%2520metric%2520values%252C%2520we%2520can%2520efficiently%2520re-use%2520computations%2520across%250Amultiple%2520sampling%2520stages.%2520Our%2520experiments%2520on%2520an%2520interstate%2520lane-change%2520scenario%250Ashow%2520our%2520method%2520is%2520viable%2520for%2520testing%2520simulated%2520AV-pipelines%252C%2520efficiently%250Aestimating%2520failure%2520probabilities%2520for%2520STL%2520specifications%2520based%2520on%2520real%2520traffic%250Arules.%2520We%2520produce%2520better%2520estimates%2520than%2520Monte-Carlo%2520and%2520importance%2520sampling%2520in%250Afewer%2520simulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15771v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Splitting%20of%20Reusable%20Temporal%20Monitors%20for%20Rare%20Traffic%0A%20%20Violations&entry.906535625=Craig%20Innes%20and%20Subramanian%20Ramamoorthy&entry.1292438233=%20%20Autonomous%20Vehicles%20%28AVs%29%20are%20often%20tested%20in%20simulation%20to%20estimate%20the%0Aprobability%20they%20will%20violate%20safety%20specifications.%20Two%20common%20issues%20arise%0Awhen%20using%20existing%20techniques%20to%20produce%20this%20estimation%3A%20If%20violations%20occur%0Ararely%2C%20simple%20Monte-Carlo%20sampling%20techniques%20can%20fail%20to%20produce%20efficient%0Aestimates%3B%20if%20simulation%20horizons%20are%20too%20long%2C%20importance%20sampling%20techniques%0A%28which%20learn%20proposal%20distributions%20from%20past%20simulations%29%20can%20fail%20to%0Aconverge.%20This%20paper%20addresses%20both%20issues%20by%20interleaving%20rare-event%20sampling%0Atechniques%20with%20online%20specification%20monitoring%20algorithms.%20We%20use%20adaptive%0Amulti-level%20splitting%20to%20decompose%20simulations%20into%20partial%20trajectories%2C%20then%0Acalculate%20the%20distance%20of%20those%20partial%20trajectories%20to%20failure%20by%20leveraging%0Arobustness%20metrics%20from%20Signal%20Temporal%20Logic%20%28STL%29.%20By%20caching%20those%20partial%0Arobustness%20metric%20values%2C%20we%20can%20efficiently%20re-use%20computations%20across%0Amultiple%20sampling%20stages.%20Our%20experiments%20on%20an%20interstate%20lane-change%20scenario%0Ashow%20our%20method%20is%20viable%20for%20testing%20simulated%20AV-pipelines%2C%20efficiently%0Aestimating%20failure%20probabilities%20for%20STL%20specifications%20based%20on%20real%20traffic%0Arules.%20We%20produce%20better%20estimates%20than%20Monte-Carlo%20and%20importance%20sampling%20in%0Afewer%20simulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15771v2&entry.124074799=Read"},
{"title": "Entropy Reweighted Conformal Classification", "author": "Rui Luo and Nicolo Colombo", "abstract": "  Conformal Prediction (CP) is a powerful framework for constructing prediction\nsets with guaranteed coverage. However, recent studies have shown that\nintegrating confidence calibration with CP can lead to a degradation in\nefficiency. In this paper, We propose an adaptive approach that considers the\nclassifier's uncertainty and employs entropy-based reweighting to enhance the\nefficiency of prediction sets for conformal classification. Our experimental\nresults demonstrate that this method significantly improves efficiency.\n", "link": "http://arxiv.org/abs/2407.17377v1", "date": "2024-07-24", "relevancy": 1.8981, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4926}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4645}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Entropy%20Reweighted%20Conformal%20Classification&body=Title%3A%20Entropy%20Reweighted%20Conformal%20Classification%0AAuthor%3A%20Rui%20Luo%20and%20Nicolo%20Colombo%0AAbstract%3A%20%20%20Conformal%20Prediction%20%28CP%29%20is%20a%20powerful%20framework%20for%20constructing%20prediction%0Asets%20with%20guaranteed%20coverage.%20However%2C%20recent%20studies%20have%20shown%20that%0Aintegrating%20confidence%20calibration%20with%20CP%20can%20lead%20to%20a%20degradation%20in%0Aefficiency.%20In%20this%20paper%2C%20We%20propose%20an%20adaptive%20approach%20that%20considers%20the%0Aclassifier%27s%20uncertainty%20and%20employs%20entropy-based%20reweighting%20to%20enhance%20the%0Aefficiency%20of%20prediction%20sets%20for%20conformal%20classification.%20Our%20experimental%0Aresults%20demonstrate%20that%20this%20method%20significantly%20improves%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17377v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEntropy%2520Reweighted%2520Conformal%2520Classification%26entry.906535625%3DRui%2520Luo%2520and%2520Nicolo%2520Colombo%26entry.1292438233%3D%2520%2520Conformal%2520Prediction%2520%2528CP%2529%2520is%2520a%2520powerful%2520framework%2520for%2520constructing%2520prediction%250Asets%2520with%2520guaranteed%2520coverage.%2520However%252C%2520recent%2520studies%2520have%2520shown%2520that%250Aintegrating%2520confidence%2520calibration%2520with%2520CP%2520can%2520lead%2520to%2520a%2520degradation%2520in%250Aefficiency.%2520In%2520this%2520paper%252C%2520We%2520propose%2520an%2520adaptive%2520approach%2520that%2520considers%2520the%250Aclassifier%2527s%2520uncertainty%2520and%2520employs%2520entropy-based%2520reweighting%2520to%2520enhance%2520the%250Aefficiency%2520of%2520prediction%2520sets%2520for%2520conformal%2520classification.%2520Our%2520experimental%250Aresults%2520demonstrate%2520that%2520this%2520method%2520significantly%2520improves%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17377v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Entropy%20Reweighted%20Conformal%20Classification&entry.906535625=Rui%20Luo%20and%20Nicolo%20Colombo&entry.1292438233=%20%20Conformal%20Prediction%20%28CP%29%20is%20a%20powerful%20framework%20for%20constructing%20prediction%0Asets%20with%20guaranteed%20coverage.%20However%2C%20recent%20studies%20have%20shown%20that%0Aintegrating%20confidence%20calibration%20with%20CP%20can%20lead%20to%20a%20degradation%20in%0Aefficiency.%20In%20this%20paper%2C%20We%20propose%20an%20adaptive%20approach%20that%20considers%20the%0Aclassifier%27s%20uncertainty%20and%20employs%20entropy-based%20reweighting%20to%20enhance%20the%0Aefficiency%20of%20prediction%20sets%20for%20conformal%20classification.%20Our%20experimental%0Aresults%20demonstrate%20that%20this%20method%20significantly%20improves%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17377v1&entry.124074799=Read"},
{"title": "FIIH: Fully Invertible Image Hiding for Secure and Robust", "author": "Lang Huang and Lin Huo and Zheng Gan and Xinrong He", "abstract": "  Image hiding is the study of techniques for covert storage and transmission,\nwhich embeds a secret image into a container image and generates stego image to\nmake it similar in appearance to a normal image. However, existing image hiding\nmethods have a serious problem that the hiding and revealing process cannot be\nfully invertible, which results in the revealing network not being able to\nrecover the secret image losslessly, which makes it impossible to\nsimultaneously achieve high fidelity and secure transmission of the secret\nimage in an insecure network environment. To solve this problem,this paper\nproposes a fully invertible image hiding architecture based on invertible\nneural network,aiming to realize invertible hiding of secret images,which is\ninvertible on both data and network. Based on this ingenious architecture, the\nmethod can withstand deep learning based image steganalysis. In addition, we\npropose a new method for enhancing the robustness of stego images after\ninterference during transmission. Experiments demonstrate that the FIIH\nproposed in this paper significantly outperforms other state-of-the-art image\nhiding methods in hiding a single image, and also significantly outperforms\nother state-of-the-art methods in robustness and security.\n", "link": "http://arxiv.org/abs/2407.17155v1", "date": "2024-07-24", "relevancy": 1.8974, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4987}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4725}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FIIH%3A%20Fully%20Invertible%20Image%20Hiding%20for%20Secure%20and%20Robust&body=Title%3A%20FIIH%3A%20Fully%20Invertible%20Image%20Hiding%20for%20Secure%20and%20Robust%0AAuthor%3A%20Lang%20Huang%20and%20Lin%20Huo%20and%20Zheng%20Gan%20and%20Xinrong%20He%0AAbstract%3A%20%20%20Image%20hiding%20is%20the%20study%20of%20techniques%20for%20covert%20storage%20and%20transmission%2C%0Awhich%20embeds%20a%20secret%20image%20into%20a%20container%20image%20and%20generates%20stego%20image%20to%0Amake%20it%20similar%20in%20appearance%20to%20a%20normal%20image.%20However%2C%20existing%20image%20hiding%0Amethods%20have%20a%20serious%20problem%20that%20the%20hiding%20and%20revealing%20process%20cannot%20be%0Afully%20invertible%2C%20which%20results%20in%20the%20revealing%20network%20not%20being%20able%20to%0Arecover%20the%20secret%20image%20losslessly%2C%20which%20makes%20it%20impossible%20to%0Asimultaneously%20achieve%20high%20fidelity%20and%20secure%20transmission%20of%20the%20secret%0Aimage%20in%20an%20insecure%20network%20environment.%20To%20solve%20this%20problem%2Cthis%20paper%0Aproposes%20a%20fully%20invertible%20image%20hiding%20architecture%20based%20on%20invertible%0Aneural%20network%2Caiming%20to%20realize%20invertible%20hiding%20of%20secret%20images%2Cwhich%20is%0Ainvertible%20on%20both%20data%20and%20network.%20Based%20on%20this%20ingenious%20architecture%2C%20the%0Amethod%20can%20withstand%20deep%20learning%20based%20image%20steganalysis.%20In%20addition%2C%20we%0Apropose%20a%20new%20method%20for%20enhancing%20the%20robustness%20of%20stego%20images%20after%0Ainterference%20during%20transmission.%20Experiments%20demonstrate%20that%20the%20FIIH%0Aproposed%20in%20this%20paper%20significantly%20outperforms%20other%20state-of-the-art%20image%0Ahiding%20methods%20in%20hiding%20a%20single%20image%2C%20and%20also%20significantly%20outperforms%0Aother%20state-of-the-art%20methods%20in%20robustness%20and%20security.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17155v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFIIH%253A%2520Fully%2520Invertible%2520Image%2520Hiding%2520for%2520Secure%2520and%2520Robust%26entry.906535625%3DLang%2520Huang%2520and%2520Lin%2520Huo%2520and%2520Zheng%2520Gan%2520and%2520Xinrong%2520He%26entry.1292438233%3D%2520%2520Image%2520hiding%2520is%2520the%2520study%2520of%2520techniques%2520for%2520covert%2520storage%2520and%2520transmission%252C%250Awhich%2520embeds%2520a%2520secret%2520image%2520into%2520a%2520container%2520image%2520and%2520generates%2520stego%2520image%2520to%250Amake%2520it%2520similar%2520in%2520appearance%2520to%2520a%2520normal%2520image.%2520However%252C%2520existing%2520image%2520hiding%250Amethods%2520have%2520a%2520serious%2520problem%2520that%2520the%2520hiding%2520and%2520revealing%2520process%2520cannot%2520be%250Afully%2520invertible%252C%2520which%2520results%2520in%2520the%2520revealing%2520network%2520not%2520being%2520able%2520to%250Arecover%2520the%2520secret%2520image%2520losslessly%252C%2520which%2520makes%2520it%2520impossible%2520to%250Asimultaneously%2520achieve%2520high%2520fidelity%2520and%2520secure%2520transmission%2520of%2520the%2520secret%250Aimage%2520in%2520an%2520insecure%2520network%2520environment.%2520To%2520solve%2520this%2520problem%252Cthis%2520paper%250Aproposes%2520a%2520fully%2520invertible%2520image%2520hiding%2520architecture%2520based%2520on%2520invertible%250Aneural%2520network%252Caiming%2520to%2520realize%2520invertible%2520hiding%2520of%2520secret%2520images%252Cwhich%2520is%250Ainvertible%2520on%2520both%2520data%2520and%2520network.%2520Based%2520on%2520this%2520ingenious%2520architecture%252C%2520the%250Amethod%2520can%2520withstand%2520deep%2520learning%2520based%2520image%2520steganalysis.%2520In%2520addition%252C%2520we%250Apropose%2520a%2520new%2520method%2520for%2520enhancing%2520the%2520robustness%2520of%2520stego%2520images%2520after%250Ainterference%2520during%2520transmission.%2520Experiments%2520demonstrate%2520that%2520the%2520FIIH%250Aproposed%2520in%2520this%2520paper%2520significantly%2520outperforms%2520other%2520state-of-the-art%2520image%250Ahiding%2520methods%2520in%2520hiding%2520a%2520single%2520image%252C%2520and%2520also%2520significantly%2520outperforms%250Aother%2520state-of-the-art%2520methods%2520in%2520robustness%2520and%2520security.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17155v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FIIH%3A%20Fully%20Invertible%20Image%20Hiding%20for%20Secure%20and%20Robust&entry.906535625=Lang%20Huang%20and%20Lin%20Huo%20and%20Zheng%20Gan%20and%20Xinrong%20He&entry.1292438233=%20%20Image%20hiding%20is%20the%20study%20of%20techniques%20for%20covert%20storage%20and%20transmission%2C%0Awhich%20embeds%20a%20secret%20image%20into%20a%20container%20image%20and%20generates%20stego%20image%20to%0Amake%20it%20similar%20in%20appearance%20to%20a%20normal%20image.%20However%2C%20existing%20image%20hiding%0Amethods%20have%20a%20serious%20problem%20that%20the%20hiding%20and%20revealing%20process%20cannot%20be%0Afully%20invertible%2C%20which%20results%20in%20the%20revealing%20network%20not%20being%20able%20to%0Arecover%20the%20secret%20image%20losslessly%2C%20which%20makes%20it%20impossible%20to%0Asimultaneously%20achieve%20high%20fidelity%20and%20secure%20transmission%20of%20the%20secret%0Aimage%20in%20an%20insecure%20network%20environment.%20To%20solve%20this%20problem%2Cthis%20paper%0Aproposes%20a%20fully%20invertible%20image%20hiding%20architecture%20based%20on%20invertible%0Aneural%20network%2Caiming%20to%20realize%20invertible%20hiding%20of%20secret%20images%2Cwhich%20is%0Ainvertible%20on%20both%20data%20and%20network.%20Based%20on%20this%20ingenious%20architecture%2C%20the%0Amethod%20can%20withstand%20deep%20learning%20based%20image%20steganalysis.%20In%20addition%2C%20we%0Apropose%20a%20new%20method%20for%20enhancing%20the%20robustness%20of%20stego%20images%20after%0Ainterference%20during%20transmission.%20Experiments%20demonstrate%20that%20the%20FIIH%0Aproposed%20in%20this%20paper%20significantly%20outperforms%20other%20state-of-the-art%20image%0Ahiding%20methods%20in%20hiding%20a%20single%20image%2C%20and%20also%20significantly%20outperforms%0Aother%20state-of-the-art%20methods%20in%20robustness%20and%20security.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17155v1&entry.124074799=Read"},
{"title": "MoveLight: Enhancing Traffic Signal Control through Movement-Centric\n  Deep Reinforcement Learning", "author": "Junqi Shao and Chenhao Zheng and Yuxuan Chen and Yucheng Huang and Rui Zhang", "abstract": "  This paper introduces MoveLight, a novel traffic signal control system that\nenhances urban traffic management through movement-centric deep reinforcement\nlearning. By leveraging detailed real-time data and advanced machine learning\ntechniques, MoveLight overcomes the limitations of traditional traffic signal\ncontrol methods. It employs a lane-level control approach using the FRAP\nalgorithm to achieve dynamic and adaptive traffic signal control, optimizing\ntraffic flow, reducing congestion, and improving overall efficiency. Our\nresearch demonstrates the scalability and effectiveness of MoveLight across\nsingle intersections, arterial roads, and network levels. Experimental results\nusing real-world datasets from Cologne and Hangzhou show significant\nimprovements in metrics such as queue length, delay, and throughput compared to\nexisting methods. This study highlights the transformative potential of deep\nreinforcement learning in intelligent traffic signal control, setting a new\nstandard for sustainable and efficient urban transportation systems.\n", "link": "http://arxiv.org/abs/2407.17303v1", "date": "2024-07-24", "relevancy": 1.8963, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4824}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.469}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoveLight%3A%20Enhancing%20Traffic%20Signal%20Control%20through%20Movement-Centric%0A%20%20Deep%20Reinforcement%20Learning&body=Title%3A%20MoveLight%3A%20Enhancing%20Traffic%20Signal%20Control%20through%20Movement-Centric%0A%20%20Deep%20Reinforcement%20Learning%0AAuthor%3A%20Junqi%20Shao%20and%20Chenhao%20Zheng%20and%20Yuxuan%20Chen%20and%20Yucheng%20Huang%20and%20Rui%20Zhang%0AAbstract%3A%20%20%20This%20paper%20introduces%20MoveLight%2C%20a%20novel%20traffic%20signal%20control%20system%20that%0Aenhances%20urban%20traffic%20management%20through%20movement-centric%20deep%20reinforcement%0Alearning.%20By%20leveraging%20detailed%20real-time%20data%20and%20advanced%20machine%20learning%0Atechniques%2C%20MoveLight%20overcomes%20the%20limitations%20of%20traditional%20traffic%20signal%0Acontrol%20methods.%20It%20employs%20a%20lane-level%20control%20approach%20using%20the%20FRAP%0Aalgorithm%20to%20achieve%20dynamic%20and%20adaptive%20traffic%20signal%20control%2C%20optimizing%0Atraffic%20flow%2C%20reducing%20congestion%2C%20and%20improving%20overall%20efficiency.%20Our%0Aresearch%20demonstrates%20the%20scalability%20and%20effectiveness%20of%20MoveLight%20across%0Asingle%20intersections%2C%20arterial%20roads%2C%20and%20network%20levels.%20Experimental%20results%0Ausing%20real-world%20datasets%20from%20Cologne%20and%20Hangzhou%20show%20significant%0Aimprovements%20in%20metrics%20such%20as%20queue%20length%2C%20delay%2C%20and%20throughput%20compared%20to%0Aexisting%20methods.%20This%20study%20highlights%20the%20transformative%20potential%20of%20deep%0Areinforcement%20learning%20in%20intelligent%20traffic%20signal%20control%2C%20setting%20a%20new%0Astandard%20for%20sustainable%20and%20efficient%20urban%20transportation%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17303v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoveLight%253A%2520Enhancing%2520Traffic%2520Signal%2520Control%2520through%2520Movement-Centric%250A%2520%2520Deep%2520Reinforcement%2520Learning%26entry.906535625%3DJunqi%2520Shao%2520and%2520Chenhao%2520Zheng%2520and%2520Yuxuan%2520Chen%2520and%2520Yucheng%2520Huang%2520and%2520Rui%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520MoveLight%252C%2520a%2520novel%2520traffic%2520signal%2520control%2520system%2520that%250Aenhances%2520urban%2520traffic%2520management%2520through%2520movement-centric%2520deep%2520reinforcement%250Alearning.%2520By%2520leveraging%2520detailed%2520real-time%2520data%2520and%2520advanced%2520machine%2520learning%250Atechniques%252C%2520MoveLight%2520overcomes%2520the%2520limitations%2520of%2520traditional%2520traffic%2520signal%250Acontrol%2520methods.%2520It%2520employs%2520a%2520lane-level%2520control%2520approach%2520using%2520the%2520FRAP%250Aalgorithm%2520to%2520achieve%2520dynamic%2520and%2520adaptive%2520traffic%2520signal%2520control%252C%2520optimizing%250Atraffic%2520flow%252C%2520reducing%2520congestion%252C%2520and%2520improving%2520overall%2520efficiency.%2520Our%250Aresearch%2520demonstrates%2520the%2520scalability%2520and%2520effectiveness%2520of%2520MoveLight%2520across%250Asingle%2520intersections%252C%2520arterial%2520roads%252C%2520and%2520network%2520levels.%2520Experimental%2520results%250Ausing%2520real-world%2520datasets%2520from%2520Cologne%2520and%2520Hangzhou%2520show%2520significant%250Aimprovements%2520in%2520metrics%2520such%2520as%2520queue%2520length%252C%2520delay%252C%2520and%2520throughput%2520compared%2520to%250Aexisting%2520methods.%2520This%2520study%2520highlights%2520the%2520transformative%2520potential%2520of%2520deep%250Areinforcement%2520learning%2520in%2520intelligent%2520traffic%2520signal%2520control%252C%2520setting%2520a%2520new%250Astandard%2520for%2520sustainable%2520and%2520efficient%2520urban%2520transportation%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17303v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoveLight%3A%20Enhancing%20Traffic%20Signal%20Control%20through%20Movement-Centric%0A%20%20Deep%20Reinforcement%20Learning&entry.906535625=Junqi%20Shao%20and%20Chenhao%20Zheng%20and%20Yuxuan%20Chen%20and%20Yucheng%20Huang%20and%20Rui%20Zhang&entry.1292438233=%20%20This%20paper%20introduces%20MoveLight%2C%20a%20novel%20traffic%20signal%20control%20system%20that%0Aenhances%20urban%20traffic%20management%20through%20movement-centric%20deep%20reinforcement%0Alearning.%20By%20leveraging%20detailed%20real-time%20data%20and%20advanced%20machine%20learning%0Atechniques%2C%20MoveLight%20overcomes%20the%20limitations%20of%20traditional%20traffic%20signal%0Acontrol%20methods.%20It%20employs%20a%20lane-level%20control%20approach%20using%20the%20FRAP%0Aalgorithm%20to%20achieve%20dynamic%20and%20adaptive%20traffic%20signal%20control%2C%20optimizing%0Atraffic%20flow%2C%20reducing%20congestion%2C%20and%20improving%20overall%20efficiency.%20Our%0Aresearch%20demonstrates%20the%20scalability%20and%20effectiveness%20of%20MoveLight%20across%0Asingle%20intersections%2C%20arterial%20roads%2C%20and%20network%20levels.%20Experimental%20results%0Ausing%20real-world%20datasets%20from%20Cologne%20and%20Hangzhou%20show%20significant%0Aimprovements%20in%20metrics%20such%20as%20queue%20length%2C%20delay%2C%20and%20throughput%20compared%20to%0Aexisting%20methods.%20This%20study%20highlights%20the%20transformative%20potential%20of%20deep%0Areinforcement%20learning%20in%20intelligent%20traffic%20signal%20control%2C%20setting%20a%20new%0Astandard%20for%20sustainable%20and%20efficient%20urban%20transportation%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17303v1&entry.124074799=Read"},
{"title": "Generalization Bounds of Surrogate Policies for Combinatorial\n  Optimization Problems", "author": "Pierre-Cyril Aubin-Frankowski and Yohann De Castro and Axel Parmentier and Alessandro Rudi", "abstract": "  A recent stream of structured learning approaches has improved the practical\nstate of the art for a range of combinatorial optimization problems with\ncomplex objectives encountered in operations research. Such approaches train\npolicies that chain a statistical model with a surrogate combinatorial\noptimization oracle to map any instance of the problem to a feasible solution.\nThe key idea is to exploit the statistical distribution over instances instead\nof dealing with instances separately. However learning such policies by risk\nminimization is challenging because the empirical risk is piecewise constant in\nthe parameters, and few theoretical guarantees have been provided so far. In\nthis article, we investigate methods that smooth the risk by perturbing the\npolicy, which eases optimization and improves generalization. Our main\ncontribution is a generalization bound that controls the perturbation bias, the\nstatistical learning error, and the optimization error. Our analysis relies on\nthe introduction of a uniform weak property, which captures and quantifies the\ninterplay of the statistical model and the surrogate combinatorial optimization\noracle. This property holds under mild assumptions on the statistical model,\nthe surrogate optimization, and the instance data distribution. We illustrate\nthe result on a range of applications such as stochastic vehicle scheduling. In\nparticular, such policies are relevant for contextual stochastic optimization\nand our results cover this case.\n", "link": "http://arxiv.org/abs/2407.17200v1", "date": "2024-07-24", "relevancy": 1.8866, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5007}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4751}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalization%20Bounds%20of%20Surrogate%20Policies%20for%20Combinatorial%0A%20%20Optimization%20Problems&body=Title%3A%20Generalization%20Bounds%20of%20Surrogate%20Policies%20for%20Combinatorial%0A%20%20Optimization%20Problems%0AAuthor%3A%20Pierre-Cyril%20Aubin-Frankowski%20and%20Yohann%20De%20Castro%20and%20Axel%20Parmentier%20and%20Alessandro%20Rudi%0AAbstract%3A%20%20%20A%20recent%20stream%20of%20structured%20learning%20approaches%20has%20improved%20the%20practical%0Astate%20of%20the%20art%20for%20a%20range%20of%20combinatorial%20optimization%20problems%20with%0Acomplex%20objectives%20encountered%20in%20operations%20research.%20Such%20approaches%20train%0Apolicies%20that%20chain%20a%20statistical%20model%20with%20a%20surrogate%20combinatorial%0Aoptimization%20oracle%20to%20map%20any%20instance%20of%20the%20problem%20to%20a%20feasible%20solution.%0AThe%20key%20idea%20is%20to%20exploit%20the%20statistical%20distribution%20over%20instances%20instead%0Aof%20dealing%20with%20instances%20separately.%20However%20learning%20such%20policies%20by%20risk%0Aminimization%20is%20challenging%20because%20the%20empirical%20risk%20is%20piecewise%20constant%20in%0Athe%20parameters%2C%20and%20few%20theoretical%20guarantees%20have%20been%20provided%20so%20far.%20In%0Athis%20article%2C%20we%20investigate%20methods%20that%20smooth%20the%20risk%20by%20perturbing%20the%0Apolicy%2C%20which%20eases%20optimization%20and%20improves%20generalization.%20Our%20main%0Acontribution%20is%20a%20generalization%20bound%20that%20controls%20the%20perturbation%20bias%2C%20the%0Astatistical%20learning%20error%2C%20and%20the%20optimization%20error.%20Our%20analysis%20relies%20on%0Athe%20introduction%20of%20a%20uniform%20weak%20property%2C%20which%20captures%20and%20quantifies%20the%0Ainterplay%20of%20the%20statistical%20model%20and%20the%20surrogate%20combinatorial%20optimization%0Aoracle.%20This%20property%20holds%20under%20mild%20assumptions%20on%20the%20statistical%20model%2C%0Athe%20surrogate%20optimization%2C%20and%20the%20instance%20data%20distribution.%20We%20illustrate%0Athe%20result%20on%20a%20range%20of%20applications%20such%20as%20stochastic%20vehicle%20scheduling.%20In%0Aparticular%2C%20such%20policies%20are%20relevant%20for%20contextual%20stochastic%20optimization%0Aand%20our%20results%20cover%20this%20case.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17200v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralization%2520Bounds%2520of%2520Surrogate%2520Policies%2520for%2520Combinatorial%250A%2520%2520Optimization%2520Problems%26entry.906535625%3DPierre-Cyril%2520Aubin-Frankowski%2520and%2520Yohann%2520De%2520Castro%2520and%2520Axel%2520Parmentier%2520and%2520Alessandro%2520Rudi%26entry.1292438233%3D%2520%2520A%2520recent%2520stream%2520of%2520structured%2520learning%2520approaches%2520has%2520improved%2520the%2520practical%250Astate%2520of%2520the%2520art%2520for%2520a%2520range%2520of%2520combinatorial%2520optimization%2520problems%2520with%250Acomplex%2520objectives%2520encountered%2520in%2520operations%2520research.%2520Such%2520approaches%2520train%250Apolicies%2520that%2520chain%2520a%2520statistical%2520model%2520with%2520a%2520surrogate%2520combinatorial%250Aoptimization%2520oracle%2520to%2520map%2520any%2520instance%2520of%2520the%2520problem%2520to%2520a%2520feasible%2520solution.%250AThe%2520key%2520idea%2520is%2520to%2520exploit%2520the%2520statistical%2520distribution%2520over%2520instances%2520instead%250Aof%2520dealing%2520with%2520instances%2520separately.%2520However%2520learning%2520such%2520policies%2520by%2520risk%250Aminimization%2520is%2520challenging%2520because%2520the%2520empirical%2520risk%2520is%2520piecewise%2520constant%2520in%250Athe%2520parameters%252C%2520and%2520few%2520theoretical%2520guarantees%2520have%2520been%2520provided%2520so%2520far.%2520In%250Athis%2520article%252C%2520we%2520investigate%2520methods%2520that%2520smooth%2520the%2520risk%2520by%2520perturbing%2520the%250Apolicy%252C%2520which%2520eases%2520optimization%2520and%2520improves%2520generalization.%2520Our%2520main%250Acontribution%2520is%2520a%2520generalization%2520bound%2520that%2520controls%2520the%2520perturbation%2520bias%252C%2520the%250Astatistical%2520learning%2520error%252C%2520and%2520the%2520optimization%2520error.%2520Our%2520analysis%2520relies%2520on%250Athe%2520introduction%2520of%2520a%2520uniform%2520weak%2520property%252C%2520which%2520captures%2520and%2520quantifies%2520the%250Ainterplay%2520of%2520the%2520statistical%2520model%2520and%2520the%2520surrogate%2520combinatorial%2520optimization%250Aoracle.%2520This%2520property%2520holds%2520under%2520mild%2520assumptions%2520on%2520the%2520statistical%2520model%252C%250Athe%2520surrogate%2520optimization%252C%2520and%2520the%2520instance%2520data%2520distribution.%2520We%2520illustrate%250Athe%2520result%2520on%2520a%2520range%2520of%2520applications%2520such%2520as%2520stochastic%2520vehicle%2520scheduling.%2520In%250Aparticular%252C%2520such%2520policies%2520are%2520relevant%2520for%2520contextual%2520stochastic%2520optimization%250Aand%2520our%2520results%2520cover%2520this%2520case.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17200v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalization%20Bounds%20of%20Surrogate%20Policies%20for%20Combinatorial%0A%20%20Optimization%20Problems&entry.906535625=Pierre-Cyril%20Aubin-Frankowski%20and%20Yohann%20De%20Castro%20and%20Axel%20Parmentier%20and%20Alessandro%20Rudi&entry.1292438233=%20%20A%20recent%20stream%20of%20structured%20learning%20approaches%20has%20improved%20the%20practical%0Astate%20of%20the%20art%20for%20a%20range%20of%20combinatorial%20optimization%20problems%20with%0Acomplex%20objectives%20encountered%20in%20operations%20research.%20Such%20approaches%20train%0Apolicies%20that%20chain%20a%20statistical%20model%20with%20a%20surrogate%20combinatorial%0Aoptimization%20oracle%20to%20map%20any%20instance%20of%20the%20problem%20to%20a%20feasible%20solution.%0AThe%20key%20idea%20is%20to%20exploit%20the%20statistical%20distribution%20over%20instances%20instead%0Aof%20dealing%20with%20instances%20separately.%20However%20learning%20such%20policies%20by%20risk%0Aminimization%20is%20challenging%20because%20the%20empirical%20risk%20is%20piecewise%20constant%20in%0Athe%20parameters%2C%20and%20few%20theoretical%20guarantees%20have%20been%20provided%20so%20far.%20In%0Athis%20article%2C%20we%20investigate%20methods%20that%20smooth%20the%20risk%20by%20perturbing%20the%0Apolicy%2C%20which%20eases%20optimization%20and%20improves%20generalization.%20Our%20main%0Acontribution%20is%20a%20generalization%20bound%20that%20controls%20the%20perturbation%20bias%2C%20the%0Astatistical%20learning%20error%2C%20and%20the%20optimization%20error.%20Our%20analysis%20relies%20on%0Athe%20introduction%20of%20a%20uniform%20weak%20property%2C%20which%20captures%20and%20quantifies%20the%0Ainterplay%20of%20the%20statistical%20model%20and%20the%20surrogate%20combinatorial%20optimization%0Aoracle.%20This%20property%20holds%20under%20mild%20assumptions%20on%20the%20statistical%20model%2C%0Athe%20surrogate%20optimization%2C%20and%20the%20instance%20data%20distribution.%20We%20illustrate%0Athe%20result%20on%20a%20range%20of%20applications%20such%20as%20stochastic%20vehicle%20scheduling.%20In%0Aparticular%2C%20such%20policies%20are%20relevant%20for%20contextual%20stochastic%20optimization%0Aand%20our%20results%20cover%20this%20case.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17200v1&entry.124074799=Read"},
{"title": "A Unified Framework for Model Editing", "author": "Akshat Gupta and Dev Sajnani and Gopala Anumanchipalli", "abstract": "  ROME and MEMIT are largely believed to be two different model editing\nalgorithms, with the major difference between them being the ability to perform\nbatched edits. In this paper, we unify these two algorithms under a single\nconceptual umbrella, optimizing for the same goal, which we call the\npreservation-memorization objective. ROME uses an equality constraint to\noptimize this objective to perform one edit at a time, whereas MEMIT employs a\nmore flexible least-square constraint that allows for batched edits. We\ngeneralize ROME and enable batched editing with equality constraint in the form\nof EMMET - an Equality-constrained Mass Model Editing algorithm for\nTransformers, a new batched memory-editing algorithm. EMMET can perform\nbatched-edits up to a batch-size of 10,000, with very similar performance to\nMEMIT across multiple dimensions. With the introduction of EMMET, we truly\nunify ROME and MEMIT and show that both algorithms are equivalent in terms of\ntheir optimization objective, their abilities (singular and batched editing),\ntheir model editing performance and their limitations.\n", "link": "http://arxiv.org/abs/2403.14236v3", "date": "2024-07-24", "relevancy": 1.8814, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4727}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4699}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Framework%20for%20Model%20Editing&body=Title%3A%20A%20Unified%20Framework%20for%20Model%20Editing%0AAuthor%3A%20Akshat%20Gupta%20and%20Dev%20Sajnani%20and%20Gopala%20Anumanchipalli%0AAbstract%3A%20%20%20ROME%20and%20MEMIT%20are%20largely%20believed%20to%20be%20two%20different%20model%20editing%0Aalgorithms%2C%20with%20the%20major%20difference%20between%20them%20being%20the%20ability%20to%20perform%0Abatched%20edits.%20In%20this%20paper%2C%20we%20unify%20these%20two%20algorithms%20under%20a%20single%0Aconceptual%20umbrella%2C%20optimizing%20for%20the%20same%20goal%2C%20which%20we%20call%20the%0Apreservation-memorization%20objective.%20ROME%20uses%20an%20equality%20constraint%20to%0Aoptimize%20this%20objective%20to%20perform%20one%20edit%20at%20a%20time%2C%20whereas%20MEMIT%20employs%20a%0Amore%20flexible%20least-square%20constraint%20that%20allows%20for%20batched%20edits.%20We%0Ageneralize%20ROME%20and%20enable%20batched%20editing%20with%20equality%20constraint%20in%20the%20form%0Aof%20EMMET%20-%20an%20Equality-constrained%20Mass%20Model%20Editing%20algorithm%20for%0ATransformers%2C%20a%20new%20batched%20memory-editing%20algorithm.%20EMMET%20can%20perform%0Abatched-edits%20up%20to%20a%20batch-size%20of%2010%2C000%2C%20with%20very%20similar%20performance%20to%0AMEMIT%20across%20multiple%20dimensions.%20With%20the%20introduction%20of%20EMMET%2C%20we%20truly%0Aunify%20ROME%20and%20MEMIT%20and%20show%20that%20both%20algorithms%20are%20equivalent%20in%20terms%20of%0Atheir%20optimization%20objective%2C%20their%20abilities%20%28singular%20and%20batched%20editing%29%2C%0Atheir%20model%20editing%20performance%20and%20their%20limitations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14236v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Framework%2520for%2520Model%2520Editing%26entry.906535625%3DAkshat%2520Gupta%2520and%2520Dev%2520Sajnani%2520and%2520Gopala%2520Anumanchipalli%26entry.1292438233%3D%2520%2520ROME%2520and%2520MEMIT%2520are%2520largely%2520believed%2520to%2520be%2520two%2520different%2520model%2520editing%250Aalgorithms%252C%2520with%2520the%2520major%2520difference%2520between%2520them%2520being%2520the%2520ability%2520to%2520perform%250Abatched%2520edits.%2520In%2520this%2520paper%252C%2520we%2520unify%2520these%2520two%2520algorithms%2520under%2520a%2520single%250Aconceptual%2520umbrella%252C%2520optimizing%2520for%2520the%2520same%2520goal%252C%2520which%2520we%2520call%2520the%250Apreservation-memorization%2520objective.%2520ROME%2520uses%2520an%2520equality%2520constraint%2520to%250Aoptimize%2520this%2520objective%2520to%2520perform%2520one%2520edit%2520at%2520a%2520time%252C%2520whereas%2520MEMIT%2520employs%2520a%250Amore%2520flexible%2520least-square%2520constraint%2520that%2520allows%2520for%2520batched%2520edits.%2520We%250Ageneralize%2520ROME%2520and%2520enable%2520batched%2520editing%2520with%2520equality%2520constraint%2520in%2520the%2520form%250Aof%2520EMMET%2520-%2520an%2520Equality-constrained%2520Mass%2520Model%2520Editing%2520algorithm%2520for%250ATransformers%252C%2520a%2520new%2520batched%2520memory-editing%2520algorithm.%2520EMMET%2520can%2520perform%250Abatched-edits%2520up%2520to%2520a%2520batch-size%2520of%252010%252C000%252C%2520with%2520very%2520similar%2520performance%2520to%250AMEMIT%2520across%2520multiple%2520dimensions.%2520With%2520the%2520introduction%2520of%2520EMMET%252C%2520we%2520truly%250Aunify%2520ROME%2520and%2520MEMIT%2520and%2520show%2520that%2520both%2520algorithms%2520are%2520equivalent%2520in%2520terms%2520of%250Atheir%2520optimization%2520objective%252C%2520their%2520abilities%2520%2528singular%2520and%2520batched%2520editing%2529%252C%250Atheir%2520model%2520editing%2520performance%2520and%2520their%2520limitations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.14236v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Framework%20for%20Model%20Editing&entry.906535625=Akshat%20Gupta%20and%20Dev%20Sajnani%20and%20Gopala%20Anumanchipalli&entry.1292438233=%20%20ROME%20and%20MEMIT%20are%20largely%20believed%20to%20be%20two%20different%20model%20editing%0Aalgorithms%2C%20with%20the%20major%20difference%20between%20them%20being%20the%20ability%20to%20perform%0Abatched%20edits.%20In%20this%20paper%2C%20we%20unify%20these%20two%20algorithms%20under%20a%20single%0Aconceptual%20umbrella%2C%20optimizing%20for%20the%20same%20goal%2C%20which%20we%20call%20the%0Apreservation-memorization%20objective.%20ROME%20uses%20an%20equality%20constraint%20to%0Aoptimize%20this%20objective%20to%20perform%20one%20edit%20at%20a%20time%2C%20whereas%20MEMIT%20employs%20a%0Amore%20flexible%20least-square%20constraint%20that%20allows%20for%20batched%20edits.%20We%0Ageneralize%20ROME%20and%20enable%20batched%20editing%20with%20equality%20constraint%20in%20the%20form%0Aof%20EMMET%20-%20an%20Equality-constrained%20Mass%20Model%20Editing%20algorithm%20for%0ATransformers%2C%20a%20new%20batched%20memory-editing%20algorithm.%20EMMET%20can%20perform%0Abatched-edits%20up%20to%20a%20batch-size%20of%2010%2C000%2C%20with%20very%20similar%20performance%20to%0AMEMIT%20across%20multiple%20dimensions.%20With%20the%20introduction%20of%20EMMET%2C%20we%20truly%0Aunify%20ROME%20and%20MEMIT%20and%20show%20that%20both%20algorithms%20are%20equivalent%20in%20terms%20of%0Atheir%20optimization%20objective%2C%20their%20abilities%20%28singular%20and%20batched%20editing%29%2C%0Atheir%20model%20editing%20performance%20and%20their%20limitations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14236v3&entry.124074799=Read"},
{"title": "Tree-Planner: Efficient Close-loop Task Planning with Large Language\n  Models", "author": "Mengkang Hu and Yao Mu and Xinmiao Yu and Mingyu Ding and Shiguang Wu and Wenqi Shao and Qiguang Chen and Bin Wang and Yu Qiao and Ping Luo", "abstract": "  This paper studies close-loop task planning, which refers to the process of\ngenerating a sequence of skills (a plan) to accomplish a specific goal while\nadapting the plan based on real-time observations. Recently, prompting Large\nLanguage Models (LLMs) to generate actions iteratively has become a prevalent\nparadigm due to its superior performance and user-friendliness. However, this\nparadigm is plagued by two inefficiencies: high token consumption and redundant\nerror correction, both of which hinder its scalability for large-scale testing\nand applications. To address these issues, we propose Tree-Planner, which\nreframes task planning with LLMs into three distinct phases: plan sampling,\naction tree construction, and grounded deciding. Tree-Planner starts by using\nan LLM to sample a set of potential plans before execution, followed by the\naggregation of them to form an action tree. Finally, the LLM performs a\ntop-down decision-making process on the tree, taking into account real-time\nenvironmental information. Experiments show that Tree-Planner achieves\nstate-of-the-art performance while maintaining high efficiency. By decomposing\nLLM queries into a single plan-sampling call and multiple grounded-deciding\ncalls, a considerable part of the prompt are less likely to be repeatedly\nconsumed. As a result, token consumption is reduced by 92.2% compared to the\npreviously best-performing model. Additionally, by enabling backtracking on the\naction tree as needed, the correction process becomes more flexible, leading to\na 40.5% decrease in error corrections.\n", "link": "http://arxiv.org/abs/2310.08582v2", "date": "2024-07-24", "relevancy": 1.8786, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5057}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4731}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tree-Planner%3A%20Efficient%20Close-loop%20Task%20Planning%20with%20Large%20Language%0A%20%20Models&body=Title%3A%20Tree-Planner%3A%20Efficient%20Close-loop%20Task%20Planning%20with%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Mengkang%20Hu%20and%20Yao%20Mu%20and%20Xinmiao%20Yu%20and%20Mingyu%20Ding%20and%20Shiguang%20Wu%20and%20Wenqi%20Shao%20and%20Qiguang%20Chen%20and%20Bin%20Wang%20and%20Yu%20Qiao%20and%20Ping%20Luo%0AAbstract%3A%20%20%20This%20paper%20studies%20close-loop%20task%20planning%2C%20which%20refers%20to%20the%20process%20of%0Agenerating%20a%20sequence%20of%20skills%20%28a%20plan%29%20to%20accomplish%20a%20specific%20goal%20while%0Aadapting%20the%20plan%20based%20on%20real-time%20observations.%20Recently%2C%20prompting%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20generate%20actions%20iteratively%20has%20become%20a%20prevalent%0Aparadigm%20due%20to%20its%20superior%20performance%20and%20user-friendliness.%20However%2C%20this%0Aparadigm%20is%20plagued%20by%20two%20inefficiencies%3A%20high%20token%20consumption%20and%20redundant%0Aerror%20correction%2C%20both%20of%20which%20hinder%20its%20scalability%20for%20large-scale%20testing%0Aand%20applications.%20To%20address%20these%20issues%2C%20we%20propose%20Tree-Planner%2C%20which%0Areframes%20task%20planning%20with%20LLMs%20into%20three%20distinct%20phases%3A%20plan%20sampling%2C%0Aaction%20tree%20construction%2C%20and%20grounded%20deciding.%20Tree-Planner%20starts%20by%20using%0Aan%20LLM%20to%20sample%20a%20set%20of%20potential%20plans%20before%20execution%2C%20followed%20by%20the%0Aaggregation%20of%20them%20to%20form%20an%20action%20tree.%20Finally%2C%20the%20LLM%20performs%20a%0Atop-down%20decision-making%20process%20on%20the%20tree%2C%20taking%20into%20account%20real-time%0Aenvironmental%20information.%20Experiments%20show%20that%20Tree-Planner%20achieves%0Astate-of-the-art%20performance%20while%20maintaining%20high%20efficiency.%20By%20decomposing%0ALLM%20queries%20into%20a%20single%20plan-sampling%20call%20and%20multiple%20grounded-deciding%0Acalls%2C%20a%20considerable%20part%20of%20the%20prompt%20are%20less%20likely%20to%20be%20repeatedly%0Aconsumed.%20As%20a%20result%2C%20token%20consumption%20is%20reduced%20by%2092.2%25%20compared%20to%20the%0Apreviously%20best-performing%20model.%20Additionally%2C%20by%20enabling%20backtracking%20on%20the%0Aaction%20tree%20as%20needed%2C%20the%20correction%20process%20becomes%20more%20flexible%2C%20leading%20to%0Aa%2040.5%25%20decrease%20in%20error%20corrections.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.08582v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTree-Planner%253A%2520Efficient%2520Close-loop%2520Task%2520Planning%2520with%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DMengkang%2520Hu%2520and%2520Yao%2520Mu%2520and%2520Xinmiao%2520Yu%2520and%2520Mingyu%2520Ding%2520and%2520Shiguang%2520Wu%2520and%2520Wenqi%2520Shao%2520and%2520Qiguang%2520Chen%2520and%2520Bin%2520Wang%2520and%2520Yu%2520Qiao%2520and%2520Ping%2520Luo%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520close-loop%2520task%2520planning%252C%2520which%2520refers%2520to%2520the%2520process%2520of%250Agenerating%2520a%2520sequence%2520of%2520skills%2520%2528a%2520plan%2529%2520to%2520accomplish%2520a%2520specific%2520goal%2520while%250Aadapting%2520the%2520plan%2520based%2520on%2520real-time%2520observations.%2520Recently%252C%2520prompting%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520to%2520generate%2520actions%2520iteratively%2520has%2520become%2520a%2520prevalent%250Aparadigm%2520due%2520to%2520its%2520superior%2520performance%2520and%2520user-friendliness.%2520However%252C%2520this%250Aparadigm%2520is%2520plagued%2520by%2520two%2520inefficiencies%253A%2520high%2520token%2520consumption%2520and%2520redundant%250Aerror%2520correction%252C%2520both%2520of%2520which%2520hinder%2520its%2520scalability%2520for%2520large-scale%2520testing%250Aand%2520applications.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520Tree-Planner%252C%2520which%250Areframes%2520task%2520planning%2520with%2520LLMs%2520into%2520three%2520distinct%2520phases%253A%2520plan%2520sampling%252C%250Aaction%2520tree%2520construction%252C%2520and%2520grounded%2520deciding.%2520Tree-Planner%2520starts%2520by%2520using%250Aan%2520LLM%2520to%2520sample%2520a%2520set%2520of%2520potential%2520plans%2520before%2520execution%252C%2520followed%2520by%2520the%250Aaggregation%2520of%2520them%2520to%2520form%2520an%2520action%2520tree.%2520Finally%252C%2520the%2520LLM%2520performs%2520a%250Atop-down%2520decision-making%2520process%2520on%2520the%2520tree%252C%2520taking%2520into%2520account%2520real-time%250Aenvironmental%2520information.%2520Experiments%2520show%2520that%2520Tree-Planner%2520achieves%250Astate-of-the-art%2520performance%2520while%2520maintaining%2520high%2520efficiency.%2520By%2520decomposing%250ALLM%2520queries%2520into%2520a%2520single%2520plan-sampling%2520call%2520and%2520multiple%2520grounded-deciding%250Acalls%252C%2520a%2520considerable%2520part%2520of%2520the%2520prompt%2520are%2520less%2520likely%2520to%2520be%2520repeatedly%250Aconsumed.%2520As%2520a%2520result%252C%2520token%2520consumption%2520is%2520reduced%2520by%252092.2%2525%2520compared%2520to%2520the%250Apreviously%2520best-performing%2520model.%2520Additionally%252C%2520by%2520enabling%2520backtracking%2520on%2520the%250Aaction%2520tree%2520as%2520needed%252C%2520the%2520correction%2520process%2520becomes%2520more%2520flexible%252C%2520leading%2520to%250Aa%252040.5%2525%2520decrease%2520in%2520error%2520corrections.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.08582v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tree-Planner%3A%20Efficient%20Close-loop%20Task%20Planning%20with%20Large%20Language%0A%20%20Models&entry.906535625=Mengkang%20Hu%20and%20Yao%20Mu%20and%20Xinmiao%20Yu%20and%20Mingyu%20Ding%20and%20Shiguang%20Wu%20and%20Wenqi%20Shao%20and%20Qiguang%20Chen%20and%20Bin%20Wang%20and%20Yu%20Qiao%20and%20Ping%20Luo&entry.1292438233=%20%20This%20paper%20studies%20close-loop%20task%20planning%2C%20which%20refers%20to%20the%20process%20of%0Agenerating%20a%20sequence%20of%20skills%20%28a%20plan%29%20to%20accomplish%20a%20specific%20goal%20while%0Aadapting%20the%20plan%20based%20on%20real-time%20observations.%20Recently%2C%20prompting%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20generate%20actions%20iteratively%20has%20become%20a%20prevalent%0Aparadigm%20due%20to%20its%20superior%20performance%20and%20user-friendliness.%20However%2C%20this%0Aparadigm%20is%20plagued%20by%20two%20inefficiencies%3A%20high%20token%20consumption%20and%20redundant%0Aerror%20correction%2C%20both%20of%20which%20hinder%20its%20scalability%20for%20large-scale%20testing%0Aand%20applications.%20To%20address%20these%20issues%2C%20we%20propose%20Tree-Planner%2C%20which%0Areframes%20task%20planning%20with%20LLMs%20into%20three%20distinct%20phases%3A%20plan%20sampling%2C%0Aaction%20tree%20construction%2C%20and%20grounded%20deciding.%20Tree-Planner%20starts%20by%20using%0Aan%20LLM%20to%20sample%20a%20set%20of%20potential%20plans%20before%20execution%2C%20followed%20by%20the%0Aaggregation%20of%20them%20to%20form%20an%20action%20tree.%20Finally%2C%20the%20LLM%20performs%20a%0Atop-down%20decision-making%20process%20on%20the%20tree%2C%20taking%20into%20account%20real-time%0Aenvironmental%20information.%20Experiments%20show%20that%20Tree-Planner%20achieves%0Astate-of-the-art%20performance%20while%20maintaining%20high%20efficiency.%20By%20decomposing%0ALLM%20queries%20into%20a%20single%20plan-sampling%20call%20and%20multiple%20grounded-deciding%0Acalls%2C%20a%20considerable%20part%20of%20the%20prompt%20are%20less%20likely%20to%20be%20repeatedly%0Aconsumed.%20As%20a%20result%2C%20token%20consumption%20is%20reduced%20by%2092.2%25%20compared%20to%20the%0Apreviously%20best-performing%20model.%20Additionally%2C%20by%20enabling%20backtracking%20on%20the%0Aaction%20tree%20as%20needed%2C%20the%20correction%20process%20becomes%20more%20flexible%2C%20leading%20to%0Aa%2040.5%25%20decrease%20in%20error%20corrections.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.08582v2&entry.124074799=Read"},
{"title": "Q-Sparse: All Large Language Models can be Fully Sparsely-Activated", "author": "Hongyu Wang and Shuming Ma and Ruiping Wang and Furu Wei", "abstract": "  We introduce, Q-Sparse, a simple yet effective approach to training\nsparsely-activated large language models (LLMs). Q-Sparse enables full sparsity\nof activations in LLMs which can bring significant efficiency gains in\ninference. This is achieved by applying top-K sparsification to the activations\nand the straight-through-estimator to the training. We also introduce Block\nQ-Sparse for batch training and inference. The key results from this work are,\n(1) Q-Sparse can achieve results comparable to those of baseline LLMs while\nbeing much more efficient at inference time; (2) We present an\ninference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is\neffective in different settings, including training-from-scratch,\ncontinue-training of off-the-shelf LLMs, and finetuning; (4) Q-Sparse works for\nboth full-precision and 1-bit LLMs (e.g., BitNet b1.58). Particularly, the\nsynergy of BitNet b1.58 and Q-Sparse (can be equipped with MoE) provides the\ncornerstone and a clear path to revolutionize the efficiency, including cost\nand energy consumption, of future LLMs.\n", "link": "http://arxiv.org/abs/2407.10969v3", "date": "2024-07-24", "relevancy": 1.8606, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4923}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4831}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Q-Sparse%3A%20All%20Large%20Language%20Models%20can%20be%20Fully%20Sparsely-Activated&body=Title%3A%20Q-Sparse%3A%20All%20Large%20Language%20Models%20can%20be%20Fully%20Sparsely-Activated%0AAuthor%3A%20Hongyu%20Wang%20and%20Shuming%20Ma%20and%20Ruiping%20Wang%20and%20Furu%20Wei%0AAbstract%3A%20%20%20We%20introduce%2C%20Q-Sparse%2C%20a%20simple%20yet%20effective%20approach%20to%20training%0Asparsely-activated%20large%20language%20models%20%28LLMs%29.%20Q-Sparse%20enables%20full%20sparsity%0Aof%20activations%20in%20LLMs%20which%20can%20bring%20significant%20efficiency%20gains%20in%0Ainference.%20This%20is%20achieved%20by%20applying%20top-K%20sparsification%20to%20the%20activations%0Aand%20the%20straight-through-estimator%20to%20the%20training.%20We%20also%20introduce%20Block%0AQ-Sparse%20for%20batch%20training%20and%20inference.%20The%20key%20results%20from%20this%20work%20are%2C%0A%281%29%20Q-Sparse%20can%20achieve%20results%20comparable%20to%20those%20of%20baseline%20LLMs%20while%0Abeing%20much%20more%20efficient%20at%20inference%20time%3B%20%282%29%20We%20present%20an%0Ainference-optimal%20scaling%20law%20for%20sparsely-activated%20LLMs%3B%20%283%29%20Q-Sparse%20is%0Aeffective%20in%20different%20settings%2C%20including%20training-from-scratch%2C%0Acontinue-training%20of%20off-the-shelf%20LLMs%2C%20and%20finetuning%3B%20%284%29%20Q-Sparse%20works%20for%0Aboth%20full-precision%20and%201-bit%20LLMs%20%28e.g.%2C%20BitNet%20b1.58%29.%20Particularly%2C%20the%0Asynergy%20of%20BitNet%20b1.58%20and%20Q-Sparse%20%28can%20be%20equipped%20with%20MoE%29%20provides%20the%0Acornerstone%20and%20a%20clear%20path%20to%20revolutionize%20the%20efficiency%2C%20including%20cost%0Aand%20energy%20consumption%2C%20of%20future%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10969v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQ-Sparse%253A%2520All%2520Large%2520Language%2520Models%2520can%2520be%2520Fully%2520Sparsely-Activated%26entry.906535625%3DHongyu%2520Wang%2520and%2520Shuming%2520Ma%2520and%2520Ruiping%2520Wang%2520and%2520Furu%2520Wei%26entry.1292438233%3D%2520%2520We%2520introduce%252C%2520Q-Sparse%252C%2520a%2520simple%2520yet%2520effective%2520approach%2520to%2520training%250Asparsely-activated%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Q-Sparse%2520enables%2520full%2520sparsity%250Aof%2520activations%2520in%2520LLMs%2520which%2520can%2520bring%2520significant%2520efficiency%2520gains%2520in%250Ainference.%2520This%2520is%2520achieved%2520by%2520applying%2520top-K%2520sparsification%2520to%2520the%2520activations%250Aand%2520the%2520straight-through-estimator%2520to%2520the%2520training.%2520We%2520also%2520introduce%2520Block%250AQ-Sparse%2520for%2520batch%2520training%2520and%2520inference.%2520The%2520key%2520results%2520from%2520this%2520work%2520are%252C%250A%25281%2529%2520Q-Sparse%2520can%2520achieve%2520results%2520comparable%2520to%2520those%2520of%2520baseline%2520LLMs%2520while%250Abeing%2520much%2520more%2520efficient%2520at%2520inference%2520time%253B%2520%25282%2529%2520We%2520present%2520an%250Ainference-optimal%2520scaling%2520law%2520for%2520sparsely-activated%2520LLMs%253B%2520%25283%2529%2520Q-Sparse%2520is%250Aeffective%2520in%2520different%2520settings%252C%2520including%2520training-from-scratch%252C%250Acontinue-training%2520of%2520off-the-shelf%2520LLMs%252C%2520and%2520finetuning%253B%2520%25284%2529%2520Q-Sparse%2520works%2520for%250Aboth%2520full-precision%2520and%25201-bit%2520LLMs%2520%2528e.g.%252C%2520BitNet%2520b1.58%2529.%2520Particularly%252C%2520the%250Asynergy%2520of%2520BitNet%2520b1.58%2520and%2520Q-Sparse%2520%2528can%2520be%2520equipped%2520with%2520MoE%2529%2520provides%2520the%250Acornerstone%2520and%2520a%2520clear%2520path%2520to%2520revolutionize%2520the%2520efficiency%252C%2520including%2520cost%250Aand%2520energy%2520consumption%252C%2520of%2520future%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10969v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Q-Sparse%3A%20All%20Large%20Language%20Models%20can%20be%20Fully%20Sparsely-Activated&entry.906535625=Hongyu%20Wang%20and%20Shuming%20Ma%20and%20Ruiping%20Wang%20and%20Furu%20Wei&entry.1292438233=%20%20We%20introduce%2C%20Q-Sparse%2C%20a%20simple%20yet%20effective%20approach%20to%20training%0Asparsely-activated%20large%20language%20models%20%28LLMs%29.%20Q-Sparse%20enables%20full%20sparsity%0Aof%20activations%20in%20LLMs%20which%20can%20bring%20significant%20efficiency%20gains%20in%0Ainference.%20This%20is%20achieved%20by%20applying%20top-K%20sparsification%20to%20the%20activations%0Aand%20the%20straight-through-estimator%20to%20the%20training.%20We%20also%20introduce%20Block%0AQ-Sparse%20for%20batch%20training%20and%20inference.%20The%20key%20results%20from%20this%20work%20are%2C%0A%281%29%20Q-Sparse%20can%20achieve%20results%20comparable%20to%20those%20of%20baseline%20LLMs%20while%0Abeing%20much%20more%20efficient%20at%20inference%20time%3B%20%282%29%20We%20present%20an%0Ainference-optimal%20scaling%20law%20for%20sparsely-activated%20LLMs%3B%20%283%29%20Q-Sparse%20is%0Aeffective%20in%20different%20settings%2C%20including%20training-from-scratch%2C%0Acontinue-training%20of%20off-the-shelf%20LLMs%2C%20and%20finetuning%3B%20%284%29%20Q-Sparse%20works%20for%0Aboth%20full-precision%20and%201-bit%20LLMs%20%28e.g.%2C%20BitNet%20b1.58%29.%20Particularly%2C%20the%0Asynergy%20of%20BitNet%20b1.58%20and%20Q-Sparse%20%28can%20be%20equipped%20with%20MoE%29%20provides%20the%0Acornerstone%20and%20a%20clear%20path%20to%20revolutionize%20the%20efficiency%2C%20including%20cost%0Aand%20energy%20consumption%2C%20of%20future%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10969v3&entry.124074799=Read"},
{"title": "On the Utility of Speech and Audio Foundation Models for Marmoset Call\n  Analysis", "author": "Eklavya Sarkar and Mathew Magimai. -Doss", "abstract": "  Marmoset monkeys encode vital information in their calls and serve as a\nsurrogate model for neuro-biologists to understand the evolutionary origins of\nhuman vocal communication. Traditionally analyzed with signal processing-based\nfeatures, recent approaches have utilized self-supervised models pre-trained on\nhuman speech for feature extraction, capitalizing on their ability to learn a\nsignal's intrinsic structure independently of its acoustic domain. However, the\nutility of such foundation models remains unclear for marmoset call analysis in\nterms of multi-class classification, bandwidth, and pre-training domain. This\nstudy assesses feature representations derived from speech and general audio\ndomains, across pre-training bandwidths of 4, 8, and 16 kHz for marmoset\ncall-type and caller classification tasks. Results show that models with higher\nbandwidth improve performance, and pre-training on speech or general audio\nyields comparable results, improving over a spectral baseline.\n", "link": "http://arxiv.org/abs/2407.16417v2", "date": "2024-07-24", "relevancy": 1.8604, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4774}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4697}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Utility%20of%20Speech%20and%20Audio%20Foundation%20Models%20for%20Marmoset%20Call%0A%20%20Analysis&body=Title%3A%20On%20the%20Utility%20of%20Speech%20and%20Audio%20Foundation%20Models%20for%20Marmoset%20Call%0A%20%20Analysis%0AAuthor%3A%20Eklavya%20Sarkar%20and%20Mathew%20Magimai.%20-Doss%0AAbstract%3A%20%20%20Marmoset%20monkeys%20encode%20vital%20information%20in%20their%20calls%20and%20serve%20as%20a%0Asurrogate%20model%20for%20neuro-biologists%20to%20understand%20the%20evolutionary%20origins%20of%0Ahuman%20vocal%20communication.%20Traditionally%20analyzed%20with%20signal%20processing-based%0Afeatures%2C%20recent%20approaches%20have%20utilized%20self-supervised%20models%20pre-trained%20on%0Ahuman%20speech%20for%20feature%20extraction%2C%20capitalizing%20on%20their%20ability%20to%20learn%20a%0Asignal%27s%20intrinsic%20structure%20independently%20of%20its%20acoustic%20domain.%20However%2C%20the%0Autility%20of%20such%20foundation%20models%20remains%20unclear%20for%20marmoset%20call%20analysis%20in%0Aterms%20of%20multi-class%20classification%2C%20bandwidth%2C%20and%20pre-training%20domain.%20This%0Astudy%20assesses%20feature%20representations%20derived%20from%20speech%20and%20general%20audio%0Adomains%2C%20across%20pre-training%20bandwidths%20of%204%2C%208%2C%20and%2016%20kHz%20for%20marmoset%0Acall-type%20and%20caller%20classification%20tasks.%20Results%20show%20that%20models%20with%20higher%0Abandwidth%20improve%20performance%2C%20and%20pre-training%20on%20speech%20or%20general%20audio%0Ayields%20comparable%20results%2C%20improving%20over%20a%20spectral%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16417v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Utility%2520of%2520Speech%2520and%2520Audio%2520Foundation%2520Models%2520for%2520Marmoset%2520Call%250A%2520%2520Analysis%26entry.906535625%3DEklavya%2520Sarkar%2520and%2520Mathew%2520Magimai.%2520-Doss%26entry.1292438233%3D%2520%2520Marmoset%2520monkeys%2520encode%2520vital%2520information%2520in%2520their%2520calls%2520and%2520serve%2520as%2520a%250Asurrogate%2520model%2520for%2520neuro-biologists%2520to%2520understand%2520the%2520evolutionary%2520origins%2520of%250Ahuman%2520vocal%2520communication.%2520Traditionally%2520analyzed%2520with%2520signal%2520processing-based%250Afeatures%252C%2520recent%2520approaches%2520have%2520utilized%2520self-supervised%2520models%2520pre-trained%2520on%250Ahuman%2520speech%2520for%2520feature%2520extraction%252C%2520capitalizing%2520on%2520their%2520ability%2520to%2520learn%2520a%250Asignal%2527s%2520intrinsic%2520structure%2520independently%2520of%2520its%2520acoustic%2520domain.%2520However%252C%2520the%250Autility%2520of%2520such%2520foundation%2520models%2520remains%2520unclear%2520for%2520marmoset%2520call%2520analysis%2520in%250Aterms%2520of%2520multi-class%2520classification%252C%2520bandwidth%252C%2520and%2520pre-training%2520domain.%2520This%250Astudy%2520assesses%2520feature%2520representations%2520derived%2520from%2520speech%2520and%2520general%2520audio%250Adomains%252C%2520across%2520pre-training%2520bandwidths%2520of%25204%252C%25208%252C%2520and%252016%2520kHz%2520for%2520marmoset%250Acall-type%2520and%2520caller%2520classification%2520tasks.%2520Results%2520show%2520that%2520models%2520with%2520higher%250Abandwidth%2520improve%2520performance%252C%2520and%2520pre-training%2520on%2520speech%2520or%2520general%2520audio%250Ayields%2520comparable%2520results%252C%2520improving%2520over%2520a%2520spectral%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16417v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Utility%20of%20Speech%20and%20Audio%20Foundation%20Models%20for%20Marmoset%20Call%0A%20%20Analysis&entry.906535625=Eklavya%20Sarkar%20and%20Mathew%20Magimai.%20-Doss&entry.1292438233=%20%20Marmoset%20monkeys%20encode%20vital%20information%20in%20their%20calls%20and%20serve%20as%20a%0Asurrogate%20model%20for%20neuro-biologists%20to%20understand%20the%20evolutionary%20origins%20of%0Ahuman%20vocal%20communication.%20Traditionally%20analyzed%20with%20signal%20processing-based%0Afeatures%2C%20recent%20approaches%20have%20utilized%20self-supervised%20models%20pre-trained%20on%0Ahuman%20speech%20for%20feature%20extraction%2C%20capitalizing%20on%20their%20ability%20to%20learn%20a%0Asignal%27s%20intrinsic%20structure%20independently%20of%20its%20acoustic%20domain.%20However%2C%20the%0Autility%20of%20such%20foundation%20models%20remains%20unclear%20for%20marmoset%20call%20analysis%20in%0Aterms%20of%20multi-class%20classification%2C%20bandwidth%2C%20and%20pre-training%20domain.%20This%0Astudy%20assesses%20feature%20representations%20derived%20from%20speech%20and%20general%20audio%0Adomains%2C%20across%20pre-training%20bandwidths%20of%204%2C%208%2C%20and%2016%20kHz%20for%20marmoset%0Acall-type%20and%20caller%20classification%20tasks.%20Results%20show%20that%20models%20with%20higher%0Abandwidth%20improve%20performance%2C%20and%20pre-training%20on%20speech%20or%20general%20audio%0Ayields%20comparable%20results%2C%20improving%20over%20a%20spectral%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16417v2&entry.124074799=Read"},
{"title": "Improving ICD coding using Chapter based Named Entities and Attentional\n  Models", "author": "Abhijith R. Beeravolu and Mirjam Jonkman and Sami Azam and Friso De Boer", "abstract": "  Recent advancements in natural language processing (NLP) have led to\nautomation in various domains. However, clinical NLP often relies on benchmark\ndatasets that may not reflect real-world scenarios accurately. Automatic ICD\ncoding, a vital NLP task, typically uses outdated and imbalanced datasets like\nMIMIC-III, with existing methods yielding micro-averaged F1 scores between 0.4\nand 0.7 due to many false positives. Our research introduces an enhanced\napproach to ICD coding that improves F1 scores by using chapter-based named\nentities and attentional models. This method categorizes discharge summaries\ninto ICD-9 Chapters and develops attentional models with chapter-specific data,\neliminating the need to consider external data for code identification. For\ncategorization, we use Chapter-IV to de-bias and influence key entities and\nweights without neural networks, creating accurate thresholds and providing\ninterpretability for human validation. Post-validation, we develop attentional\nmodels for three frequent and three non-frequent codes from Chapter-IV using\nBidirectional-Gated Recurrent Units (GRUs) with Attention and Transformer with\nMulti-head Attention architectures. The average Micro-F1 scores of 0.79 and\n0.81 from these models demonstrate significant performance improvements in ICD\ncoding.\n", "link": "http://arxiv.org/abs/2407.17230v1", "date": "2024-07-24", "relevancy": 1.8556, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4657}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4647}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20ICD%20coding%20using%20Chapter%20based%20Named%20Entities%20and%20Attentional%0A%20%20Models&body=Title%3A%20Improving%20ICD%20coding%20using%20Chapter%20based%20Named%20Entities%20and%20Attentional%0A%20%20Models%0AAuthor%3A%20Abhijith%20R.%20Beeravolu%20and%20Mirjam%20Jonkman%20and%20Sami%20Azam%20and%20Friso%20De%20Boer%0AAbstract%3A%20%20%20Recent%20advancements%20in%20natural%20language%20processing%20%28NLP%29%20have%20led%20to%0Aautomation%20in%20various%20domains.%20However%2C%20clinical%20NLP%20often%20relies%20on%20benchmark%0Adatasets%20that%20may%20not%20reflect%20real-world%20scenarios%20accurately.%20Automatic%20ICD%0Acoding%2C%20a%20vital%20NLP%20task%2C%20typically%20uses%20outdated%20and%20imbalanced%20datasets%20like%0AMIMIC-III%2C%20with%20existing%20methods%20yielding%20micro-averaged%20F1%20scores%20between%200.4%0Aand%200.7%20due%20to%20many%20false%20positives.%20Our%20research%20introduces%20an%20enhanced%0Aapproach%20to%20ICD%20coding%20that%20improves%20F1%20scores%20by%20using%20chapter-based%20named%0Aentities%20and%20attentional%20models.%20This%20method%20categorizes%20discharge%20summaries%0Ainto%20ICD-9%20Chapters%20and%20develops%20attentional%20models%20with%20chapter-specific%20data%2C%0Aeliminating%20the%20need%20to%20consider%20external%20data%20for%20code%20identification.%20For%0Acategorization%2C%20we%20use%20Chapter-IV%20to%20de-bias%20and%20influence%20key%20entities%20and%0Aweights%20without%20neural%20networks%2C%20creating%20accurate%20thresholds%20and%20providing%0Ainterpretability%20for%20human%20validation.%20Post-validation%2C%20we%20develop%20attentional%0Amodels%20for%20three%20frequent%20and%20three%20non-frequent%20codes%20from%20Chapter-IV%20using%0ABidirectional-Gated%20Recurrent%20Units%20%28GRUs%29%20with%20Attention%20and%20Transformer%20with%0AMulti-head%20Attention%20architectures.%20The%20average%20Micro-F1%20scores%20of%200.79%20and%0A0.81%20from%20these%20models%20demonstrate%20significant%20performance%20improvements%20in%20ICD%0Acoding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17230v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520ICD%2520coding%2520using%2520Chapter%2520based%2520Named%2520Entities%2520and%2520Attentional%250A%2520%2520Models%26entry.906535625%3DAbhijith%2520R.%2520Beeravolu%2520and%2520Mirjam%2520Jonkman%2520and%2520Sami%2520Azam%2520and%2520Friso%2520De%2520Boer%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520natural%2520language%2520processing%2520%2528NLP%2529%2520have%2520led%2520to%250Aautomation%2520in%2520various%2520domains.%2520However%252C%2520clinical%2520NLP%2520often%2520relies%2520on%2520benchmark%250Adatasets%2520that%2520may%2520not%2520reflect%2520real-world%2520scenarios%2520accurately.%2520Automatic%2520ICD%250Acoding%252C%2520a%2520vital%2520NLP%2520task%252C%2520typically%2520uses%2520outdated%2520and%2520imbalanced%2520datasets%2520like%250AMIMIC-III%252C%2520with%2520existing%2520methods%2520yielding%2520micro-averaged%2520F1%2520scores%2520between%25200.4%250Aand%25200.7%2520due%2520to%2520many%2520false%2520positives.%2520Our%2520research%2520introduces%2520an%2520enhanced%250Aapproach%2520to%2520ICD%2520coding%2520that%2520improves%2520F1%2520scores%2520by%2520using%2520chapter-based%2520named%250Aentities%2520and%2520attentional%2520models.%2520This%2520method%2520categorizes%2520discharge%2520summaries%250Ainto%2520ICD-9%2520Chapters%2520and%2520develops%2520attentional%2520models%2520with%2520chapter-specific%2520data%252C%250Aeliminating%2520the%2520need%2520to%2520consider%2520external%2520data%2520for%2520code%2520identification.%2520For%250Acategorization%252C%2520we%2520use%2520Chapter-IV%2520to%2520de-bias%2520and%2520influence%2520key%2520entities%2520and%250Aweights%2520without%2520neural%2520networks%252C%2520creating%2520accurate%2520thresholds%2520and%2520providing%250Ainterpretability%2520for%2520human%2520validation.%2520Post-validation%252C%2520we%2520develop%2520attentional%250Amodels%2520for%2520three%2520frequent%2520and%2520three%2520non-frequent%2520codes%2520from%2520Chapter-IV%2520using%250ABidirectional-Gated%2520Recurrent%2520Units%2520%2528GRUs%2529%2520with%2520Attention%2520and%2520Transformer%2520with%250AMulti-head%2520Attention%2520architectures.%2520The%2520average%2520Micro-F1%2520scores%2520of%25200.79%2520and%250A0.81%2520from%2520these%2520models%2520demonstrate%2520significant%2520performance%2520improvements%2520in%2520ICD%250Acoding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17230v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20ICD%20coding%20using%20Chapter%20based%20Named%20Entities%20and%20Attentional%0A%20%20Models&entry.906535625=Abhijith%20R.%20Beeravolu%20and%20Mirjam%20Jonkman%20and%20Sami%20Azam%20and%20Friso%20De%20Boer&entry.1292438233=%20%20Recent%20advancements%20in%20natural%20language%20processing%20%28NLP%29%20have%20led%20to%0Aautomation%20in%20various%20domains.%20However%2C%20clinical%20NLP%20often%20relies%20on%20benchmark%0Adatasets%20that%20may%20not%20reflect%20real-world%20scenarios%20accurately.%20Automatic%20ICD%0Acoding%2C%20a%20vital%20NLP%20task%2C%20typically%20uses%20outdated%20and%20imbalanced%20datasets%20like%0AMIMIC-III%2C%20with%20existing%20methods%20yielding%20micro-averaged%20F1%20scores%20between%200.4%0Aand%200.7%20due%20to%20many%20false%20positives.%20Our%20research%20introduces%20an%20enhanced%0Aapproach%20to%20ICD%20coding%20that%20improves%20F1%20scores%20by%20using%20chapter-based%20named%0Aentities%20and%20attentional%20models.%20This%20method%20categorizes%20discharge%20summaries%0Ainto%20ICD-9%20Chapters%20and%20develops%20attentional%20models%20with%20chapter-specific%20data%2C%0Aeliminating%20the%20need%20to%20consider%20external%20data%20for%20code%20identification.%20For%0Acategorization%2C%20we%20use%20Chapter-IV%20to%20de-bias%20and%20influence%20key%20entities%20and%0Aweights%20without%20neural%20networks%2C%20creating%20accurate%20thresholds%20and%20providing%0Ainterpretability%20for%20human%20validation.%20Post-validation%2C%20we%20develop%20attentional%0Amodels%20for%20three%20frequent%20and%20three%20non-frequent%20codes%20from%20Chapter-IV%20using%0ABidirectional-Gated%20Recurrent%20Units%20%28GRUs%29%20with%20Attention%20and%20Transformer%20with%0AMulti-head%20Attention%20architectures.%20The%20average%20Micro-F1%20scores%20of%200.79%20and%0A0.81%20from%20these%20models%20demonstrate%20significant%20performance%20improvements%20in%20ICD%0Acoding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17230v1&entry.124074799=Read"},
{"title": "Solving Deep Reinforcement Learning Tasks with Evolution Strategies and\n  Linear Policy Networks", "author": "Annie Wong and Jacob de Nobel and Thomas B\u00e4ck and Aske Plaat and Anna V. Kononova", "abstract": "  Although deep reinforcement learning methods can learn effective policies for\nchallenging problems such as Atari games and robotics tasks, algorithms are\ncomplex, and training times are often long. This study investigates how\nEvolution Strategies perform compared to gradient-based deep reinforcement\nlearning methods. We use Evolution Strategies to optimize the weights of a\nneural network via neuroevolution, performing direct policy search. We\nbenchmark both deep policy networks and networks consisting of a single linear\nlayer from observations to actions for three gradient-based methods, such as\nProximal Policy Optimization. These methods are evaluated against three\nclassical Evolution Strategies and Augmented Random Search, which all use\nlinear policy networks. Our results reveal that Evolution Strategies can find\neffective linear policies for many reinforcement learning benchmark tasks,\nunlike deep reinforcement learning methods that can only find successful\npolicies using much larger networks, suggesting that current benchmarks are\neasier to solve than previously assumed. Interestingly, Evolution Strategies\nalso achieve results comparable to gradient-based deep reinforcement learning\nalgorithms for higher-complexity tasks. Furthermore, we find that by directly\naccessing the memory state of the game, Evolution Strategies can find\nsuccessful policies in Atari that outperform the policies found by Deep\nQ-Learning. Evolution Strategies also outperform Augmented Random Search in\nmost benchmarks, demonstrating superior sample efficiency and robustness in\ntraining linear policy networks.\n", "link": "http://arxiv.org/abs/2402.06912v2", "date": "2024-07-24", "relevancy": 1.8471, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4772}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4665}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solving%20Deep%20Reinforcement%20Learning%20Tasks%20with%20Evolution%20Strategies%20and%0A%20%20Linear%20Policy%20Networks&body=Title%3A%20Solving%20Deep%20Reinforcement%20Learning%20Tasks%20with%20Evolution%20Strategies%20and%0A%20%20Linear%20Policy%20Networks%0AAuthor%3A%20Annie%20Wong%20and%20Jacob%20de%20Nobel%20and%20Thomas%20B%C3%A4ck%20and%20Aske%20Plaat%20and%20Anna%20V.%20Kononova%0AAbstract%3A%20%20%20Although%20deep%20reinforcement%20learning%20methods%20can%20learn%20effective%20policies%20for%0Achallenging%20problems%20such%20as%20Atari%20games%20and%20robotics%20tasks%2C%20algorithms%20are%0Acomplex%2C%20and%20training%20times%20are%20often%20long.%20This%20study%20investigates%20how%0AEvolution%20Strategies%20perform%20compared%20to%20gradient-based%20deep%20reinforcement%0Alearning%20methods.%20We%20use%20Evolution%20Strategies%20to%20optimize%20the%20weights%20of%20a%0Aneural%20network%20via%20neuroevolution%2C%20performing%20direct%20policy%20search.%20We%0Abenchmark%20both%20deep%20policy%20networks%20and%20networks%20consisting%20of%20a%20single%20linear%0Alayer%20from%20observations%20to%20actions%20for%20three%20gradient-based%20methods%2C%20such%20as%0AProximal%20Policy%20Optimization.%20These%20methods%20are%20evaluated%20against%20three%0Aclassical%20Evolution%20Strategies%20and%20Augmented%20Random%20Search%2C%20which%20all%20use%0Alinear%20policy%20networks.%20Our%20results%20reveal%20that%20Evolution%20Strategies%20can%20find%0Aeffective%20linear%20policies%20for%20many%20reinforcement%20learning%20benchmark%20tasks%2C%0Aunlike%20deep%20reinforcement%20learning%20methods%20that%20can%20only%20find%20successful%0Apolicies%20using%20much%20larger%20networks%2C%20suggesting%20that%20current%20benchmarks%20are%0Aeasier%20to%20solve%20than%20previously%20assumed.%20Interestingly%2C%20Evolution%20Strategies%0Aalso%20achieve%20results%20comparable%20to%20gradient-based%20deep%20reinforcement%20learning%0Aalgorithms%20for%20higher-complexity%20tasks.%20Furthermore%2C%20we%20find%20that%20by%20directly%0Aaccessing%20the%20memory%20state%20of%20the%20game%2C%20Evolution%20Strategies%20can%20find%0Asuccessful%20policies%20in%20Atari%20that%20outperform%20the%20policies%20found%20by%20Deep%0AQ-Learning.%20Evolution%20Strategies%20also%20outperform%20Augmented%20Random%20Search%20in%0Amost%20benchmarks%2C%20demonstrating%20superior%20sample%20efficiency%20and%20robustness%20in%0Atraining%20linear%20policy%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.06912v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolving%2520Deep%2520Reinforcement%2520Learning%2520Tasks%2520with%2520Evolution%2520Strategies%2520and%250A%2520%2520Linear%2520Policy%2520Networks%26entry.906535625%3DAnnie%2520Wong%2520and%2520Jacob%2520de%2520Nobel%2520and%2520Thomas%2520B%25C3%25A4ck%2520and%2520Aske%2520Plaat%2520and%2520Anna%2520V.%2520Kononova%26entry.1292438233%3D%2520%2520Although%2520deep%2520reinforcement%2520learning%2520methods%2520can%2520learn%2520effective%2520policies%2520for%250Achallenging%2520problems%2520such%2520as%2520Atari%2520games%2520and%2520robotics%2520tasks%252C%2520algorithms%2520are%250Acomplex%252C%2520and%2520training%2520times%2520are%2520often%2520long.%2520This%2520study%2520investigates%2520how%250AEvolution%2520Strategies%2520perform%2520compared%2520to%2520gradient-based%2520deep%2520reinforcement%250Alearning%2520methods.%2520We%2520use%2520Evolution%2520Strategies%2520to%2520optimize%2520the%2520weights%2520of%2520a%250Aneural%2520network%2520via%2520neuroevolution%252C%2520performing%2520direct%2520policy%2520search.%2520We%250Abenchmark%2520both%2520deep%2520policy%2520networks%2520and%2520networks%2520consisting%2520of%2520a%2520single%2520linear%250Alayer%2520from%2520observations%2520to%2520actions%2520for%2520three%2520gradient-based%2520methods%252C%2520such%2520as%250AProximal%2520Policy%2520Optimization.%2520These%2520methods%2520are%2520evaluated%2520against%2520three%250Aclassical%2520Evolution%2520Strategies%2520and%2520Augmented%2520Random%2520Search%252C%2520which%2520all%2520use%250Alinear%2520policy%2520networks.%2520Our%2520results%2520reveal%2520that%2520Evolution%2520Strategies%2520can%2520find%250Aeffective%2520linear%2520policies%2520for%2520many%2520reinforcement%2520learning%2520benchmark%2520tasks%252C%250Aunlike%2520deep%2520reinforcement%2520learning%2520methods%2520that%2520can%2520only%2520find%2520successful%250Apolicies%2520using%2520much%2520larger%2520networks%252C%2520suggesting%2520that%2520current%2520benchmarks%2520are%250Aeasier%2520to%2520solve%2520than%2520previously%2520assumed.%2520Interestingly%252C%2520Evolution%2520Strategies%250Aalso%2520achieve%2520results%2520comparable%2520to%2520gradient-based%2520deep%2520reinforcement%2520learning%250Aalgorithms%2520for%2520higher-complexity%2520tasks.%2520Furthermore%252C%2520we%2520find%2520that%2520by%2520directly%250Aaccessing%2520the%2520memory%2520state%2520of%2520the%2520game%252C%2520Evolution%2520Strategies%2520can%2520find%250Asuccessful%2520policies%2520in%2520Atari%2520that%2520outperform%2520the%2520policies%2520found%2520by%2520Deep%250AQ-Learning.%2520Evolution%2520Strategies%2520also%2520outperform%2520Augmented%2520Random%2520Search%2520in%250Amost%2520benchmarks%252C%2520demonstrating%2520superior%2520sample%2520efficiency%2520and%2520robustness%2520in%250Atraining%2520linear%2520policy%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.06912v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20Deep%20Reinforcement%20Learning%20Tasks%20with%20Evolution%20Strategies%20and%0A%20%20Linear%20Policy%20Networks&entry.906535625=Annie%20Wong%20and%20Jacob%20de%20Nobel%20and%20Thomas%20B%C3%A4ck%20and%20Aske%20Plaat%20and%20Anna%20V.%20Kononova&entry.1292438233=%20%20Although%20deep%20reinforcement%20learning%20methods%20can%20learn%20effective%20policies%20for%0Achallenging%20problems%20such%20as%20Atari%20games%20and%20robotics%20tasks%2C%20algorithms%20are%0Acomplex%2C%20and%20training%20times%20are%20often%20long.%20This%20study%20investigates%20how%0AEvolution%20Strategies%20perform%20compared%20to%20gradient-based%20deep%20reinforcement%0Alearning%20methods.%20We%20use%20Evolution%20Strategies%20to%20optimize%20the%20weights%20of%20a%0Aneural%20network%20via%20neuroevolution%2C%20performing%20direct%20policy%20search.%20We%0Abenchmark%20both%20deep%20policy%20networks%20and%20networks%20consisting%20of%20a%20single%20linear%0Alayer%20from%20observations%20to%20actions%20for%20three%20gradient-based%20methods%2C%20such%20as%0AProximal%20Policy%20Optimization.%20These%20methods%20are%20evaluated%20against%20three%0Aclassical%20Evolution%20Strategies%20and%20Augmented%20Random%20Search%2C%20which%20all%20use%0Alinear%20policy%20networks.%20Our%20results%20reveal%20that%20Evolution%20Strategies%20can%20find%0Aeffective%20linear%20policies%20for%20many%20reinforcement%20learning%20benchmark%20tasks%2C%0Aunlike%20deep%20reinforcement%20learning%20methods%20that%20can%20only%20find%20successful%0Apolicies%20using%20much%20larger%20networks%2C%20suggesting%20that%20current%20benchmarks%20are%0Aeasier%20to%20solve%20than%20previously%20assumed.%20Interestingly%2C%20Evolution%20Strategies%0Aalso%20achieve%20results%20comparable%20to%20gradient-based%20deep%20reinforcement%20learning%0Aalgorithms%20for%20higher-complexity%20tasks.%20Furthermore%2C%20we%20find%20that%20by%20directly%0Aaccessing%20the%20memory%20state%20of%20the%20game%2C%20Evolution%20Strategies%20can%20find%0Asuccessful%20policies%20in%20Atari%20that%20outperform%20the%20policies%20found%20by%20Deep%0AQ-Learning.%20Evolution%20Strategies%20also%20outperform%20Augmented%20Random%20Search%20in%0Amost%20benchmarks%2C%20demonstrating%20superior%20sample%20efficiency%20and%20robustness%20in%0Atraining%20linear%20policy%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.06912v2&entry.124074799=Read"},
{"title": "Dissecting Language Models: Machine Unlearning via Selective Pruning", "author": "Nicholas Pochinkov and Nandi Schoots", "abstract": "  Understanding and shaping the behaviour of Large Language Models (LLMs) is\nincreasingly important as applications become more powerful and more frequently\nadopted. This paper introduces a machine unlearning method specifically\ndesigned for LLMs. We introduce a selective pruning method for LLMs that\nremoves neurons based on their relative importance on a targeted capability\ncompared to overall network performance. This approach is a compute- and\ndata-efficient method for identifying and removing neurons that enable specific\nbehaviours. Our findings reveal that both feed-forward and attention neurons in\nLLMs are specialized; that is, for specific tasks, certain neurons are more\ncrucial than others. Code from all experiments is available at\nhttps://github.com/nickypro/selective-pruning\n", "link": "http://arxiv.org/abs/2403.01267v2", "date": "2024-07-24", "relevancy": 1.8351, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4701}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4518}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dissecting%20Language%20Models%3A%20Machine%20Unlearning%20via%20Selective%20Pruning&body=Title%3A%20Dissecting%20Language%20Models%3A%20Machine%20Unlearning%20via%20Selective%20Pruning%0AAuthor%3A%20Nicholas%20Pochinkov%20and%20Nandi%20Schoots%0AAbstract%3A%20%20%20Understanding%20and%20shaping%20the%20behaviour%20of%20Large%20Language%20Models%20%28LLMs%29%20is%0Aincreasingly%20important%20as%20applications%20become%20more%20powerful%20and%20more%20frequently%0Aadopted.%20This%20paper%20introduces%20a%20machine%20unlearning%20method%20specifically%0Adesigned%20for%20LLMs.%20We%20introduce%20a%20selective%20pruning%20method%20for%20LLMs%20that%0Aremoves%20neurons%20based%20on%20their%20relative%20importance%20on%20a%20targeted%20capability%0Acompared%20to%20overall%20network%20performance.%20This%20approach%20is%20a%20compute-%20and%0Adata-efficient%20method%20for%20identifying%20and%20removing%20neurons%20that%20enable%20specific%0Abehaviours.%20Our%20findings%20reveal%20that%20both%20feed-forward%20and%20attention%20neurons%20in%0ALLMs%20are%20specialized%3B%20that%20is%2C%20for%20specific%20tasks%2C%20certain%20neurons%20are%20more%0Acrucial%20than%20others.%20Code%20from%20all%20experiments%20is%20available%20at%0Ahttps%3A//github.com/nickypro/selective-pruning%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01267v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDissecting%2520Language%2520Models%253A%2520Machine%2520Unlearning%2520via%2520Selective%2520Pruning%26entry.906535625%3DNicholas%2520Pochinkov%2520and%2520Nandi%2520Schoots%26entry.1292438233%3D%2520%2520Understanding%2520and%2520shaping%2520the%2520behaviour%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520is%250Aincreasingly%2520important%2520as%2520applications%2520become%2520more%2520powerful%2520and%2520more%2520frequently%250Aadopted.%2520This%2520paper%2520introduces%2520a%2520machine%2520unlearning%2520method%2520specifically%250Adesigned%2520for%2520LLMs.%2520We%2520introduce%2520a%2520selective%2520pruning%2520method%2520for%2520LLMs%2520that%250Aremoves%2520neurons%2520based%2520on%2520their%2520relative%2520importance%2520on%2520a%2520targeted%2520capability%250Acompared%2520to%2520overall%2520network%2520performance.%2520This%2520approach%2520is%2520a%2520compute-%2520and%250Adata-efficient%2520method%2520for%2520identifying%2520and%2520removing%2520neurons%2520that%2520enable%2520specific%250Abehaviours.%2520Our%2520findings%2520reveal%2520that%2520both%2520feed-forward%2520and%2520attention%2520neurons%2520in%250ALLMs%2520are%2520specialized%253B%2520that%2520is%252C%2520for%2520specific%2520tasks%252C%2520certain%2520neurons%2520are%2520more%250Acrucial%2520than%2520others.%2520Code%2520from%2520all%2520experiments%2520is%2520available%2520at%250Ahttps%253A//github.com/nickypro/selective-pruning%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.01267v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dissecting%20Language%20Models%3A%20Machine%20Unlearning%20via%20Selective%20Pruning&entry.906535625=Nicholas%20Pochinkov%20and%20Nandi%20Schoots&entry.1292438233=%20%20Understanding%20and%20shaping%20the%20behaviour%20of%20Large%20Language%20Models%20%28LLMs%29%20is%0Aincreasingly%20important%20as%20applications%20become%20more%20powerful%20and%20more%20frequently%0Aadopted.%20This%20paper%20introduces%20a%20machine%20unlearning%20method%20specifically%0Adesigned%20for%20LLMs.%20We%20introduce%20a%20selective%20pruning%20method%20for%20LLMs%20that%0Aremoves%20neurons%20based%20on%20their%20relative%20importance%20on%20a%20targeted%20capability%0Acompared%20to%20overall%20network%20performance.%20This%20approach%20is%20a%20compute-%20and%0Adata-efficient%20method%20for%20identifying%20and%20removing%20neurons%20that%20enable%20specific%0Abehaviours.%20Our%20findings%20reveal%20that%20both%20feed-forward%20and%20attention%20neurons%20in%0ALLMs%20are%20specialized%3B%20that%20is%2C%20for%20specific%20tasks%2C%20certain%20neurons%20are%20more%0Acrucial%20than%20others.%20Code%20from%20all%20experiments%20is%20available%20at%0Ahttps%3A//github.com/nickypro/selective-pruning%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01267v2&entry.124074799=Read"},
{"title": "Self-Calibrated Variance-Stabilizing Transformations for Real-World\n  Image Denoising", "author": "S\u00e9bastien Herbreteau and Michael Unser", "abstract": "  Supervised deep learning has become the method of choice for image denoising.\nIt involves the training of neural networks on large datasets composed of pairs\nof noisy and clean images. However, the necessity of training data that are\nspecific to the targeted application constrains the widespread use of denoising\nnetworks. Recently, several approaches have been developed to overcome this\ndifficulty by whether artificially generating realistic clean/noisy image\npairs, or training exclusively on noisy images. In this paper, we show that,\ncontrary to popular belief, denoising networks specialized in the removal of\nGaussian noise can be efficiently leveraged in favor of real-world image\ndenoising, even without additional training. For this to happen, an appropriate\nvariance-stabilizing transform (VST) has to be applied beforehand. We propose\nan algorithm termed Noise2VST for the learning of such a model-free VST. Our\napproach requires only the input noisy image and an off-the-shelf Gaussian\ndenoiser. We demonstrate through extensive experiments the efficiency and\nsuperiority of Noise2VST in comparison to existing methods trained in the\nabsence of specific clean/noisy pairs.\n", "link": "http://arxiv.org/abs/2407.17399v1", "date": "2024-07-24", "relevancy": 1.8275, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6543}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5617}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Calibrated%20Variance-Stabilizing%20Transformations%20for%20Real-World%0A%20%20Image%20Denoising&body=Title%3A%20Self-Calibrated%20Variance-Stabilizing%20Transformations%20for%20Real-World%0A%20%20Image%20Denoising%0AAuthor%3A%20S%C3%A9bastien%20Herbreteau%20and%20Michael%20Unser%0AAbstract%3A%20%20%20Supervised%20deep%20learning%20has%20become%20the%20method%20of%20choice%20for%20image%20denoising.%0AIt%20involves%20the%20training%20of%20neural%20networks%20on%20large%20datasets%20composed%20of%20pairs%0Aof%20noisy%20and%20clean%20images.%20However%2C%20the%20necessity%20of%20training%20data%20that%20are%0Aspecific%20to%20the%20targeted%20application%20constrains%20the%20widespread%20use%20of%20denoising%0Anetworks.%20Recently%2C%20several%20approaches%20have%20been%20developed%20to%20overcome%20this%0Adifficulty%20by%20whether%20artificially%20generating%20realistic%20clean/noisy%20image%0Apairs%2C%20or%20training%20exclusively%20on%20noisy%20images.%20In%20this%20paper%2C%20we%20show%20that%2C%0Acontrary%20to%20popular%20belief%2C%20denoising%20networks%20specialized%20in%20the%20removal%20of%0AGaussian%20noise%20can%20be%20efficiently%20leveraged%20in%20favor%20of%20real-world%20image%0Adenoising%2C%20even%20without%20additional%20training.%20For%20this%20to%20happen%2C%20an%20appropriate%0Avariance-stabilizing%20transform%20%28VST%29%20has%20to%20be%20applied%20beforehand.%20We%20propose%0Aan%20algorithm%20termed%20Noise2VST%20for%20the%20learning%20of%20such%20a%20model-free%20VST.%20Our%0Aapproach%20requires%20only%20the%20input%20noisy%20image%20and%20an%20off-the-shelf%20Gaussian%0Adenoiser.%20We%20demonstrate%20through%20extensive%20experiments%20the%20efficiency%20and%0Asuperiority%20of%20Noise2VST%20in%20comparison%20to%20existing%20methods%20trained%20in%20the%0Aabsence%20of%20specific%20clean/noisy%20pairs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17399v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Calibrated%2520Variance-Stabilizing%2520Transformations%2520for%2520Real-World%250A%2520%2520Image%2520Denoising%26entry.906535625%3DS%25C3%25A9bastien%2520Herbreteau%2520and%2520Michael%2520Unser%26entry.1292438233%3D%2520%2520Supervised%2520deep%2520learning%2520has%2520become%2520the%2520method%2520of%2520choice%2520for%2520image%2520denoising.%250AIt%2520involves%2520the%2520training%2520of%2520neural%2520networks%2520on%2520large%2520datasets%2520composed%2520of%2520pairs%250Aof%2520noisy%2520and%2520clean%2520images.%2520However%252C%2520the%2520necessity%2520of%2520training%2520data%2520that%2520are%250Aspecific%2520to%2520the%2520targeted%2520application%2520constrains%2520the%2520widespread%2520use%2520of%2520denoising%250Anetworks.%2520Recently%252C%2520several%2520approaches%2520have%2520been%2520developed%2520to%2520overcome%2520this%250Adifficulty%2520by%2520whether%2520artificially%2520generating%2520realistic%2520clean/noisy%2520image%250Apairs%252C%2520or%2520training%2520exclusively%2520on%2520noisy%2520images.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%252C%250Acontrary%2520to%2520popular%2520belief%252C%2520denoising%2520networks%2520specialized%2520in%2520the%2520removal%2520of%250AGaussian%2520noise%2520can%2520be%2520efficiently%2520leveraged%2520in%2520favor%2520of%2520real-world%2520image%250Adenoising%252C%2520even%2520without%2520additional%2520training.%2520For%2520this%2520to%2520happen%252C%2520an%2520appropriate%250Avariance-stabilizing%2520transform%2520%2528VST%2529%2520has%2520to%2520be%2520applied%2520beforehand.%2520We%2520propose%250Aan%2520algorithm%2520termed%2520Noise2VST%2520for%2520the%2520learning%2520of%2520such%2520a%2520model-free%2520VST.%2520Our%250Aapproach%2520requires%2520only%2520the%2520input%2520noisy%2520image%2520and%2520an%2520off-the-shelf%2520Gaussian%250Adenoiser.%2520We%2520demonstrate%2520through%2520extensive%2520experiments%2520the%2520efficiency%2520and%250Asuperiority%2520of%2520Noise2VST%2520in%2520comparison%2520to%2520existing%2520methods%2520trained%2520in%2520the%250Aabsence%2520of%2520specific%2520clean/noisy%2520pairs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17399v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Calibrated%20Variance-Stabilizing%20Transformations%20for%20Real-World%0A%20%20Image%20Denoising&entry.906535625=S%C3%A9bastien%20Herbreteau%20and%20Michael%20Unser&entry.1292438233=%20%20Supervised%20deep%20learning%20has%20become%20the%20method%20of%20choice%20for%20image%20denoising.%0AIt%20involves%20the%20training%20of%20neural%20networks%20on%20large%20datasets%20composed%20of%20pairs%0Aof%20noisy%20and%20clean%20images.%20However%2C%20the%20necessity%20of%20training%20data%20that%20are%0Aspecific%20to%20the%20targeted%20application%20constrains%20the%20widespread%20use%20of%20denoising%0Anetworks.%20Recently%2C%20several%20approaches%20have%20been%20developed%20to%20overcome%20this%0Adifficulty%20by%20whether%20artificially%20generating%20realistic%20clean/noisy%20image%0Apairs%2C%20or%20training%20exclusively%20on%20noisy%20images.%20In%20this%20paper%2C%20we%20show%20that%2C%0Acontrary%20to%20popular%20belief%2C%20denoising%20networks%20specialized%20in%20the%20removal%20of%0AGaussian%20noise%20can%20be%20efficiently%20leveraged%20in%20favor%20of%20real-world%20image%0Adenoising%2C%20even%20without%20additional%20training.%20For%20this%20to%20happen%2C%20an%20appropriate%0Avariance-stabilizing%20transform%20%28VST%29%20has%20to%20be%20applied%20beforehand.%20We%20propose%0Aan%20algorithm%20termed%20Noise2VST%20for%20the%20learning%20of%20such%20a%20model-free%20VST.%20Our%0Aapproach%20requires%20only%20the%20input%20noisy%20image%20and%20an%20off-the-shelf%20Gaussian%0Adenoiser.%20We%20demonstrate%20through%20extensive%20experiments%20the%20efficiency%20and%0Asuperiority%20of%20Noise2VST%20in%20comparison%20to%20existing%20methods%20trained%20in%20the%0Aabsence%20of%20specific%20clean/noisy%20pairs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17399v1&entry.124074799=Read"},
{"title": "Description-Based Text Similarity", "author": "Shauli Ravfogel and Valentina Pyatkin and Amir DN Cohen and Avshalom Manevich and Yoav Goldberg", "abstract": "  Identifying texts with a given semantics is central for many information\nseeking scenarios. Similarity search over vector embeddings appear to be\ncentral to this ability, yet the similarity reflected in current text\nembeddings is corpus-driven, and is inconsistent and sub-optimal for many use\ncases. What, then, is a good notion of similarity for effective retrieval of\ntext?\n  We identify the need to search for texts based on abstract descriptions of\ntheir content, and the corresponding notion of \\emph{description based\nsimilarity}. We demonstrate the inadequacy of current text embeddings and\npropose an alternative model that significantly improves when used in standard\nnearest neighbor search. The model is trained using positive and negative pairs\nsourced through prompting a LLM, demonstrating how data from LLMs can be used\nfor creating new capabilities not immediately possible using the original\nmodel.\n", "link": "http://arxiv.org/abs/2305.12517v5", "date": "2024-07-24", "relevancy": 1.8269, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4759}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4571}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Description-Based%20Text%20Similarity&body=Title%3A%20Description-Based%20Text%20Similarity%0AAuthor%3A%20Shauli%20Ravfogel%20and%20Valentina%20Pyatkin%20and%20Amir%20DN%20Cohen%20and%20Avshalom%20Manevich%20and%20Yoav%20Goldberg%0AAbstract%3A%20%20%20Identifying%20texts%20with%20a%20given%20semantics%20is%20central%20for%20many%20information%0Aseeking%20scenarios.%20Similarity%20search%20over%20vector%20embeddings%20appear%20to%20be%0Acentral%20to%20this%20ability%2C%20yet%20the%20similarity%20reflected%20in%20current%20text%0Aembeddings%20is%20corpus-driven%2C%20and%20is%20inconsistent%20and%20sub-optimal%20for%20many%20use%0Acases.%20What%2C%20then%2C%20is%20a%20good%20notion%20of%20similarity%20for%20effective%20retrieval%20of%0Atext%3F%0A%20%20We%20identify%20the%20need%20to%20search%20for%20texts%20based%20on%20abstract%20descriptions%20of%0Atheir%20content%2C%20and%20the%20corresponding%20notion%20of%20%5Cemph%7Bdescription%20based%0Asimilarity%7D.%20We%20demonstrate%20the%20inadequacy%20of%20current%20text%20embeddings%20and%0Apropose%20an%20alternative%20model%20that%20significantly%20improves%20when%20used%20in%20standard%0Anearest%20neighbor%20search.%20The%20model%20is%20trained%20using%20positive%20and%20negative%20pairs%0Asourced%20through%20prompting%20a%20LLM%2C%20demonstrating%20how%20data%20from%20LLMs%20can%20be%20used%0Afor%20creating%20new%20capabilities%20not%20immediately%20possible%20using%20the%20original%0Amodel.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.12517v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDescription-Based%2520Text%2520Similarity%26entry.906535625%3DShauli%2520Ravfogel%2520and%2520Valentina%2520Pyatkin%2520and%2520Amir%2520DN%2520Cohen%2520and%2520Avshalom%2520Manevich%2520and%2520Yoav%2520Goldberg%26entry.1292438233%3D%2520%2520Identifying%2520texts%2520with%2520a%2520given%2520semantics%2520is%2520central%2520for%2520many%2520information%250Aseeking%2520scenarios.%2520Similarity%2520search%2520over%2520vector%2520embeddings%2520appear%2520to%2520be%250Acentral%2520to%2520this%2520ability%252C%2520yet%2520the%2520similarity%2520reflected%2520in%2520current%2520text%250Aembeddings%2520is%2520corpus-driven%252C%2520and%2520is%2520inconsistent%2520and%2520sub-optimal%2520for%2520many%2520use%250Acases.%2520What%252C%2520then%252C%2520is%2520a%2520good%2520notion%2520of%2520similarity%2520for%2520effective%2520retrieval%2520of%250Atext%253F%250A%2520%2520We%2520identify%2520the%2520need%2520to%2520search%2520for%2520texts%2520based%2520on%2520abstract%2520descriptions%2520of%250Atheir%2520content%252C%2520and%2520the%2520corresponding%2520notion%2520of%2520%255Cemph%257Bdescription%2520based%250Asimilarity%257D.%2520We%2520demonstrate%2520the%2520inadequacy%2520of%2520current%2520text%2520embeddings%2520and%250Apropose%2520an%2520alternative%2520model%2520that%2520significantly%2520improves%2520when%2520used%2520in%2520standard%250Anearest%2520neighbor%2520search.%2520The%2520model%2520is%2520trained%2520using%2520positive%2520and%2520negative%2520pairs%250Asourced%2520through%2520prompting%2520a%2520LLM%252C%2520demonstrating%2520how%2520data%2520from%2520LLMs%2520can%2520be%2520used%250Afor%2520creating%2520new%2520capabilities%2520not%2520immediately%2520possible%2520using%2520the%2520original%250Amodel.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.12517v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Description-Based%20Text%20Similarity&entry.906535625=Shauli%20Ravfogel%20and%20Valentina%20Pyatkin%20and%20Amir%20DN%20Cohen%20and%20Avshalom%20Manevich%20and%20Yoav%20Goldberg&entry.1292438233=%20%20Identifying%20texts%20with%20a%20given%20semantics%20is%20central%20for%20many%20information%0Aseeking%20scenarios.%20Similarity%20search%20over%20vector%20embeddings%20appear%20to%20be%0Acentral%20to%20this%20ability%2C%20yet%20the%20similarity%20reflected%20in%20current%20text%0Aembeddings%20is%20corpus-driven%2C%20and%20is%20inconsistent%20and%20sub-optimal%20for%20many%20use%0Acases.%20What%2C%20then%2C%20is%20a%20good%20notion%20of%20similarity%20for%20effective%20retrieval%20of%0Atext%3F%0A%20%20We%20identify%20the%20need%20to%20search%20for%20texts%20based%20on%20abstract%20descriptions%20of%0Atheir%20content%2C%20and%20the%20corresponding%20notion%20of%20%5Cemph%7Bdescription%20based%0Asimilarity%7D.%20We%20demonstrate%20the%20inadequacy%20of%20current%20text%20embeddings%20and%0Apropose%20an%20alternative%20model%20that%20significantly%20improves%20when%20used%20in%20standard%0Anearest%20neighbor%20search.%20The%20model%20is%20trained%20using%20positive%20and%20negative%20pairs%0Asourced%20through%20prompting%20a%20LLM%2C%20demonstrating%20how%20data%20from%20LLMs%20can%20be%20used%0Afor%20creating%20new%20capabilities%20not%20immediately%20possible%20using%20the%20original%0Amodel.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.12517v5&entry.124074799=Read"},
{"title": "A Hybrid Federated Kernel Regularized Least Squares Algorithm", "author": "Celeste Damiani and Yulia Rodina and Sergio Decherchi", "abstract": "  Federated learning is becoming an increasingly viable and accepted strategy\nfor building machine learning models in critical privacy-preserving scenarios\nsuch as clinical settings. Often, the data involved is not limited to clinical\ndata but also includes additional omics features (e.g. proteomics).\nConsequently, data is distributed not only across hospitals but also across\nomics centers, which are labs capable of generating such additional features\nfrom biosamples. This scenario leads to a hybrid setting where data is\nscattered both in terms of samples and features. In this hybrid setting, we\npresent an efficient reformulation of the Kernel Regularized Least Squares\nalgorithm, introduce two variants and validate them using well-established\ndatasets. Lastly, we discuss security measures to defend against possible\nattacks.\n", "link": "http://arxiv.org/abs/2407.17228v1", "date": "2024-07-24", "relevancy": 1.8153, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4565}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4554}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Hybrid%20Federated%20Kernel%20Regularized%20Least%20Squares%20Algorithm&body=Title%3A%20A%20Hybrid%20Federated%20Kernel%20Regularized%20Least%20Squares%20Algorithm%0AAuthor%3A%20Celeste%20Damiani%20and%20Yulia%20Rodina%20and%20Sergio%20Decherchi%0AAbstract%3A%20%20%20Federated%20learning%20is%20becoming%20an%20increasingly%20viable%20and%20accepted%20strategy%0Afor%20building%20machine%20learning%20models%20in%20critical%20privacy-preserving%20scenarios%0Asuch%20as%20clinical%20settings.%20Often%2C%20the%20data%20involved%20is%20not%20limited%20to%20clinical%0Adata%20but%20also%20includes%20additional%20omics%20features%20%28e.g.%20proteomics%29.%0AConsequently%2C%20data%20is%20distributed%20not%20only%20across%20hospitals%20but%20also%20across%0Aomics%20centers%2C%20which%20are%20labs%20capable%20of%20generating%20such%20additional%20features%0Afrom%20biosamples.%20This%20scenario%20leads%20to%20a%20hybrid%20setting%20where%20data%20is%0Ascattered%20both%20in%20terms%20of%20samples%20and%20features.%20In%20this%20hybrid%20setting%2C%20we%0Apresent%20an%20efficient%20reformulation%20of%20the%20Kernel%20Regularized%20Least%20Squares%0Aalgorithm%2C%20introduce%20two%20variants%20and%20validate%20them%20using%20well-established%0Adatasets.%20Lastly%2C%20we%20discuss%20security%20measures%20to%20defend%20against%20possible%0Aattacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17228v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Hybrid%2520Federated%2520Kernel%2520Regularized%2520Least%2520Squares%2520Algorithm%26entry.906535625%3DCeleste%2520Damiani%2520and%2520Yulia%2520Rodina%2520and%2520Sergio%2520Decherchi%26entry.1292438233%3D%2520%2520Federated%2520learning%2520is%2520becoming%2520an%2520increasingly%2520viable%2520and%2520accepted%2520strategy%250Afor%2520building%2520machine%2520learning%2520models%2520in%2520critical%2520privacy-preserving%2520scenarios%250Asuch%2520as%2520clinical%2520settings.%2520Often%252C%2520the%2520data%2520involved%2520is%2520not%2520limited%2520to%2520clinical%250Adata%2520but%2520also%2520includes%2520additional%2520omics%2520features%2520%2528e.g.%2520proteomics%2529.%250AConsequently%252C%2520data%2520is%2520distributed%2520not%2520only%2520across%2520hospitals%2520but%2520also%2520across%250Aomics%2520centers%252C%2520which%2520are%2520labs%2520capable%2520of%2520generating%2520such%2520additional%2520features%250Afrom%2520biosamples.%2520This%2520scenario%2520leads%2520to%2520a%2520hybrid%2520setting%2520where%2520data%2520is%250Ascattered%2520both%2520in%2520terms%2520of%2520samples%2520and%2520features.%2520In%2520this%2520hybrid%2520setting%252C%2520we%250Apresent%2520an%2520efficient%2520reformulation%2520of%2520the%2520Kernel%2520Regularized%2520Least%2520Squares%250Aalgorithm%252C%2520introduce%2520two%2520variants%2520and%2520validate%2520them%2520using%2520well-established%250Adatasets.%2520Lastly%252C%2520we%2520discuss%2520security%2520measures%2520to%2520defend%2520against%2520possible%250Aattacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17228v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hybrid%20Federated%20Kernel%20Regularized%20Least%20Squares%20Algorithm&entry.906535625=Celeste%20Damiani%20and%20Yulia%20Rodina%20and%20Sergio%20Decherchi&entry.1292438233=%20%20Federated%20learning%20is%20becoming%20an%20increasingly%20viable%20and%20accepted%20strategy%0Afor%20building%20machine%20learning%20models%20in%20critical%20privacy-preserving%20scenarios%0Asuch%20as%20clinical%20settings.%20Often%2C%20the%20data%20involved%20is%20not%20limited%20to%20clinical%0Adata%20but%20also%20includes%20additional%20omics%20features%20%28e.g.%20proteomics%29.%0AConsequently%2C%20data%20is%20distributed%20not%20only%20across%20hospitals%20but%20also%20across%0Aomics%20centers%2C%20which%20are%20labs%20capable%20of%20generating%20such%20additional%20features%0Afrom%20biosamples.%20This%20scenario%20leads%20to%20a%20hybrid%20setting%20where%20data%20is%0Ascattered%20both%20in%20terms%20of%20samples%20and%20features.%20In%20this%20hybrid%20setting%2C%20we%0Apresent%20an%20efficient%20reformulation%20of%20the%20Kernel%20Regularized%20Least%20Squares%0Aalgorithm%2C%20introduce%20two%20variants%20and%20validate%20them%20using%20well-established%0Adatasets.%20Lastly%2C%20we%20discuss%20security%20measures%20to%20defend%20against%20possible%0Aattacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17228v1&entry.124074799=Read"},
{"title": "Quantum Supervised Learning", "author": "Antonio Macaluso", "abstract": "  Recent advancements in quantum computing have positioned it as a prospective\nsolution for tackling intricate computational challenges, with supervised\nlearning emerging as a promising domain for its application. Despite this\npotential, the field of quantum machine learning is still in its early stages,\nand there persists a level of skepticism regarding a possible near-term quantum\nadvantage. This paper aims to provide a classical perspective on current\nquantum algorithms for supervised learning, effectively bridging traditional\nmachine learning principles with advancements in quantum machine learning.\nSpecifically, this study charts a research trajectory that diverges from the\npredominant focus of quantum machine learning literature, originating from the\nprerequisites of classical methodologies and elucidating the potential impact\nof quantum approaches. Through this exploration, our objective is to deepen the\nunderstanding of the convergence between classical and quantum methods, thereby\nlaying the groundwork for future advancements in both domains and fostering the\ninvolvement of classical practitioners in the field of quantum machine\nlearning.\n", "link": "http://arxiv.org/abs/2407.17161v1", "date": "2024-07-24", "relevancy": 1.8112, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4762}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4416}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum%20Supervised%20Learning&body=Title%3A%20Quantum%20Supervised%20Learning%0AAuthor%3A%20Antonio%20Macaluso%0AAbstract%3A%20%20%20Recent%20advancements%20in%20quantum%20computing%20have%20positioned%20it%20as%20a%20prospective%0Asolution%20for%20tackling%20intricate%20computational%20challenges%2C%20with%20supervised%0Alearning%20emerging%20as%20a%20promising%20domain%20for%20its%20application.%20Despite%20this%0Apotential%2C%20the%20field%20of%20quantum%20machine%20learning%20is%20still%20in%20its%20early%20stages%2C%0Aand%20there%20persists%20a%20level%20of%20skepticism%20regarding%20a%20possible%20near-term%20quantum%0Aadvantage.%20This%20paper%20aims%20to%20provide%20a%20classical%20perspective%20on%20current%0Aquantum%20algorithms%20for%20supervised%20learning%2C%20effectively%20bridging%20traditional%0Amachine%20learning%20principles%20with%20advancements%20in%20quantum%20machine%20learning.%0ASpecifically%2C%20this%20study%20charts%20a%20research%20trajectory%20that%20diverges%20from%20the%0Apredominant%20focus%20of%20quantum%20machine%20learning%20literature%2C%20originating%20from%20the%0Aprerequisites%20of%20classical%20methodologies%20and%20elucidating%20the%20potential%20impact%0Aof%20quantum%20approaches.%20Through%20this%20exploration%2C%20our%20objective%20is%20to%20deepen%20the%0Aunderstanding%20of%20the%20convergence%20between%20classical%20and%20quantum%20methods%2C%20thereby%0Alaying%20the%20groundwork%20for%20future%20advancements%20in%20both%20domains%20and%20fostering%20the%0Ainvolvement%20of%20classical%20practitioners%20in%20the%20field%20of%20quantum%20machine%0Alearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17161v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum%2520Supervised%2520Learning%26entry.906535625%3DAntonio%2520Macaluso%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520quantum%2520computing%2520have%2520positioned%2520it%2520as%2520a%2520prospective%250Asolution%2520for%2520tackling%2520intricate%2520computational%2520challenges%252C%2520with%2520supervised%250Alearning%2520emerging%2520as%2520a%2520promising%2520domain%2520for%2520its%2520application.%2520Despite%2520this%250Apotential%252C%2520the%2520field%2520of%2520quantum%2520machine%2520learning%2520is%2520still%2520in%2520its%2520early%2520stages%252C%250Aand%2520there%2520persists%2520a%2520level%2520of%2520skepticism%2520regarding%2520a%2520possible%2520near-term%2520quantum%250Aadvantage.%2520This%2520paper%2520aims%2520to%2520provide%2520a%2520classical%2520perspective%2520on%2520current%250Aquantum%2520algorithms%2520for%2520supervised%2520learning%252C%2520effectively%2520bridging%2520traditional%250Amachine%2520learning%2520principles%2520with%2520advancements%2520in%2520quantum%2520machine%2520learning.%250ASpecifically%252C%2520this%2520study%2520charts%2520a%2520research%2520trajectory%2520that%2520diverges%2520from%2520the%250Apredominant%2520focus%2520of%2520quantum%2520machine%2520learning%2520literature%252C%2520originating%2520from%2520the%250Aprerequisites%2520of%2520classical%2520methodologies%2520and%2520elucidating%2520the%2520potential%2520impact%250Aof%2520quantum%2520approaches.%2520Through%2520this%2520exploration%252C%2520our%2520objective%2520is%2520to%2520deepen%2520the%250Aunderstanding%2520of%2520the%2520convergence%2520between%2520classical%2520and%2520quantum%2520methods%252C%2520thereby%250Alaying%2520the%2520groundwork%2520for%2520future%2520advancements%2520in%2520both%2520domains%2520and%2520fostering%2520the%250Ainvolvement%2520of%2520classical%2520practitioners%2520in%2520the%2520field%2520of%2520quantum%2520machine%250Alearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17161v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20Supervised%20Learning&entry.906535625=Antonio%20Macaluso&entry.1292438233=%20%20Recent%20advancements%20in%20quantum%20computing%20have%20positioned%20it%20as%20a%20prospective%0Asolution%20for%20tackling%20intricate%20computational%20challenges%2C%20with%20supervised%0Alearning%20emerging%20as%20a%20promising%20domain%20for%20its%20application.%20Despite%20this%0Apotential%2C%20the%20field%20of%20quantum%20machine%20learning%20is%20still%20in%20its%20early%20stages%2C%0Aand%20there%20persists%20a%20level%20of%20skepticism%20regarding%20a%20possible%20near-term%20quantum%0Aadvantage.%20This%20paper%20aims%20to%20provide%20a%20classical%20perspective%20on%20current%0Aquantum%20algorithms%20for%20supervised%20learning%2C%20effectively%20bridging%20traditional%0Amachine%20learning%20principles%20with%20advancements%20in%20quantum%20machine%20learning.%0ASpecifically%2C%20this%20study%20charts%20a%20research%20trajectory%20that%20diverges%20from%20the%0Apredominant%20focus%20of%20quantum%20machine%20learning%20literature%2C%20originating%20from%20the%0Aprerequisites%20of%20classical%20methodologies%20and%20elucidating%20the%20potential%20impact%0Aof%20quantum%20approaches.%20Through%20this%20exploration%2C%20our%20objective%20is%20to%20deepen%20the%0Aunderstanding%20of%20the%20convergence%20between%20classical%20and%20quantum%20methods%2C%20thereby%0Alaying%20the%20groundwork%20for%20future%20advancements%20in%20both%20domains%20and%20fostering%20the%0Ainvolvement%20of%20classical%20practitioners%20in%20the%20field%20of%20quantum%20machine%0Alearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17161v1&entry.124074799=Read"},
{"title": "Sublinear Regret for An Actor-Critic Algorithm in Continuous-Time\n  Linear-Quadratic Reinforcement Learning", "author": "Yilie Huang and Yanwei Jia and Xun Yu Zhou", "abstract": "  We study reinforcement learning (RL) for a class of continuous-time\nlinear-quadratic (LQ) control problems for diffusions where volatility of the\nstate processes depends on both state and control variables. We apply a\nmodel-free approach that relies neither on knowledge of model parameters nor on\ntheir estimations, and devise an actor-critic algorithm to learn the optimal\npolicy parameter directly. Our main contributions include the introduction of a\nnovel exploration schedule and a regret analysis of the proposed algorithm. We\nprovide the convergence rate of the policy parameter to the optimal one, and\nprove that the algorithm achieves a regret bound of $O(N^{\\frac{3}{4}})$ up to\na logarithmic factor. We conduct a simulation study to validate the theoretical\nresults and demonstrate the effectiveness and reliability of the proposed\nalgorithm. We also perform numerical comparisons between our method and those\nof the recent model-based stochastic LQ RL studies adapted to the state- and\ncontrol-dependent volatility setting, demonstrating a better performance of the\nformer in terms of regret bounds.\n", "link": "http://arxiv.org/abs/2407.17226v1", "date": "2024-07-24", "relevancy": 1.4029, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4775}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4624}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sublinear%20Regret%20for%20An%20Actor-Critic%20Algorithm%20in%20Continuous-Time%0A%20%20Linear-Quadratic%20Reinforcement%20Learning&body=Title%3A%20Sublinear%20Regret%20for%20An%20Actor-Critic%20Algorithm%20in%20Continuous-Time%0A%20%20Linear-Quadratic%20Reinforcement%20Learning%0AAuthor%3A%20Yilie%20Huang%20and%20Yanwei%20Jia%20and%20Xun%20Yu%20Zhou%0AAbstract%3A%20%20%20We%20study%20reinforcement%20learning%20%28RL%29%20for%20a%20class%20of%20continuous-time%0Alinear-quadratic%20%28LQ%29%20control%20problems%20for%20diffusions%20where%20volatility%20of%20the%0Astate%20processes%20depends%20on%20both%20state%20and%20control%20variables.%20We%20apply%20a%0Amodel-free%20approach%20that%20relies%20neither%20on%20knowledge%20of%20model%20parameters%20nor%20on%0Atheir%20estimations%2C%20and%20devise%20an%20actor-critic%20algorithm%20to%20learn%20the%20optimal%0Apolicy%20parameter%20directly.%20Our%20main%20contributions%20include%20the%20introduction%20of%20a%0Anovel%20exploration%20schedule%20and%20a%20regret%20analysis%20of%20the%20proposed%20algorithm.%20We%0Aprovide%20the%20convergence%20rate%20of%20the%20policy%20parameter%20to%20the%20optimal%20one%2C%20and%0Aprove%20that%20the%20algorithm%20achieves%20a%20regret%20bound%20of%20%24O%28N%5E%7B%5Cfrac%7B3%7D%7B4%7D%7D%29%24%20up%20to%0Aa%20logarithmic%20factor.%20We%20conduct%20a%20simulation%20study%20to%20validate%20the%20theoretical%0Aresults%20and%20demonstrate%20the%20effectiveness%20and%20reliability%20of%20the%20proposed%0Aalgorithm.%20We%20also%20perform%20numerical%20comparisons%20between%20our%20method%20and%20those%0Aof%20the%20recent%20model-based%20stochastic%20LQ%20RL%20studies%20adapted%20to%20the%20state-%20and%0Acontrol-dependent%20volatility%20setting%2C%20demonstrating%20a%20better%20performance%20of%20the%0Aformer%20in%20terms%20of%20regret%20bounds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17226v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSublinear%2520Regret%2520for%2520An%2520Actor-Critic%2520Algorithm%2520in%2520Continuous-Time%250A%2520%2520Linear-Quadratic%2520Reinforcement%2520Learning%26entry.906535625%3DYilie%2520Huang%2520and%2520Yanwei%2520Jia%2520and%2520Xun%2520Yu%2520Zhou%26entry.1292438233%3D%2520%2520We%2520study%2520reinforcement%2520learning%2520%2528RL%2529%2520for%2520a%2520class%2520of%2520continuous-time%250Alinear-quadratic%2520%2528LQ%2529%2520control%2520problems%2520for%2520diffusions%2520where%2520volatility%2520of%2520the%250Astate%2520processes%2520depends%2520on%2520both%2520state%2520and%2520control%2520variables.%2520We%2520apply%2520a%250Amodel-free%2520approach%2520that%2520relies%2520neither%2520on%2520knowledge%2520of%2520model%2520parameters%2520nor%2520on%250Atheir%2520estimations%252C%2520and%2520devise%2520an%2520actor-critic%2520algorithm%2520to%2520learn%2520the%2520optimal%250Apolicy%2520parameter%2520directly.%2520Our%2520main%2520contributions%2520include%2520the%2520introduction%2520of%2520a%250Anovel%2520exploration%2520schedule%2520and%2520a%2520regret%2520analysis%2520of%2520the%2520proposed%2520algorithm.%2520We%250Aprovide%2520the%2520convergence%2520rate%2520of%2520the%2520policy%2520parameter%2520to%2520the%2520optimal%2520one%252C%2520and%250Aprove%2520that%2520the%2520algorithm%2520achieves%2520a%2520regret%2520bound%2520of%2520%2524O%2528N%255E%257B%255Cfrac%257B3%257D%257B4%257D%257D%2529%2524%2520up%2520to%250Aa%2520logarithmic%2520factor.%2520We%2520conduct%2520a%2520simulation%2520study%2520to%2520validate%2520the%2520theoretical%250Aresults%2520and%2520demonstrate%2520the%2520effectiveness%2520and%2520reliability%2520of%2520the%2520proposed%250Aalgorithm.%2520We%2520also%2520perform%2520numerical%2520comparisons%2520between%2520our%2520method%2520and%2520those%250Aof%2520the%2520recent%2520model-based%2520stochastic%2520LQ%2520RL%2520studies%2520adapted%2520to%2520the%2520state-%2520and%250Acontrol-dependent%2520volatility%2520setting%252C%2520demonstrating%2520a%2520better%2520performance%2520of%2520the%250Aformer%2520in%2520terms%2520of%2520regret%2520bounds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17226v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sublinear%20Regret%20for%20An%20Actor-Critic%20Algorithm%20in%20Continuous-Time%0A%20%20Linear-Quadratic%20Reinforcement%20Learning&entry.906535625=Yilie%20Huang%20and%20Yanwei%20Jia%20and%20Xun%20Yu%20Zhou&entry.1292438233=%20%20We%20study%20reinforcement%20learning%20%28RL%29%20for%20a%20class%20of%20continuous-time%0Alinear-quadratic%20%28LQ%29%20control%20problems%20for%20diffusions%20where%20volatility%20of%20the%0Astate%20processes%20depends%20on%20both%20state%20and%20control%20variables.%20We%20apply%20a%0Amodel-free%20approach%20that%20relies%20neither%20on%20knowledge%20of%20model%20parameters%20nor%20on%0Atheir%20estimations%2C%20and%20devise%20an%20actor-critic%20algorithm%20to%20learn%20the%20optimal%0Apolicy%20parameter%20directly.%20Our%20main%20contributions%20include%20the%20introduction%20of%20a%0Anovel%20exploration%20schedule%20and%20a%20regret%20analysis%20of%20the%20proposed%20algorithm.%20We%0Aprovide%20the%20convergence%20rate%20of%20the%20policy%20parameter%20to%20the%20optimal%20one%2C%20and%0Aprove%20that%20the%20algorithm%20achieves%20a%20regret%20bound%20of%20%24O%28N%5E%7B%5Cfrac%7B3%7D%7B4%7D%7D%29%24%20up%20to%0Aa%20logarithmic%20factor.%20We%20conduct%20a%20simulation%20study%20to%20validate%20the%20theoretical%0Aresults%20and%20demonstrate%20the%20effectiveness%20and%20reliability%20of%20the%20proposed%0Aalgorithm.%20We%20also%20perform%20numerical%20comparisons%20between%20our%20method%20and%20those%0Aof%20the%20recent%20model-based%20stochastic%20LQ%20RL%20studies%20adapted%20to%20the%20state-%20and%0Acontrol-dependent%20volatility%20setting%2C%20demonstrating%20a%20better%20performance%20of%20the%0Aformer%20in%20terms%20of%20regret%20bounds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17226v1&entry.124074799=Read"},
{"title": "Can Watermarking Large Language Models Prevent Copyrighted Text\n  Generation and Hide Training Data?", "author": "Michael-Andrei Panaitescu-Liess and Zora Che and Bang An and Yuancheng Xu and Pankayaraj Pathmanathan and Souradip Chakraborty and Sicheng Zhu and Tom Goldstein and Furong Huang", "abstract": "  Large Language Models (LLMs) have demonstrated impressive capabilities in\ngenerating diverse and contextually rich text. However, concerns regarding\ncopyright infringement arise as LLMs may inadvertently produce copyrighted\nmaterial. In this paper, we first investigate the effectiveness of watermarking\nLLMs as a deterrent against the generation of copyrighted texts. Through\ntheoretical analysis and empirical evaluation, we demonstrate that\nincorporating watermarks into LLMs significantly reduces the likelihood of\ngenerating copyrighted content, thereby addressing a critical concern in the\ndeployment of LLMs. Additionally, we explore the impact of watermarking on\nMembership Inference Attacks (MIAs), which aim to discern whether a sample was\npart of the pretraining dataset and may be used to detect copyright violations.\nSurprisingly, we find that watermarking adversely affects the success rate of\nMIAs, complicating the task of detecting copyrighted text in the pretraining\ndataset. Finally, we propose an adaptive technique to improve the success rate\nof a recent MIA under watermarking. Our findings underscore the importance of\ndeveloping adaptive methods to study critical problems in LLMs with potential\nlegal implications.\n", "link": "http://arxiv.org/abs/2407.17417v1", "date": "2024-07-24", "relevancy": 1.3822, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.473}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4698}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Watermarking%20Large%20Language%20Models%20Prevent%20Copyrighted%20Text%0A%20%20Generation%20and%20Hide%20Training%20Data%3F&body=Title%3A%20Can%20Watermarking%20Large%20Language%20Models%20Prevent%20Copyrighted%20Text%0A%20%20Generation%20and%20Hide%20Training%20Data%3F%0AAuthor%3A%20Michael-Andrei%20Panaitescu-Liess%20and%20Zora%20Che%20and%20Bang%20An%20and%20Yuancheng%20Xu%20and%20Pankayaraj%20Pathmanathan%20and%20Souradip%20Chakraborty%20and%20Sicheng%20Zhu%20and%20Tom%20Goldstein%20and%20Furong%20Huang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20impressive%20capabilities%20in%0Agenerating%20diverse%20and%20contextually%20rich%20text.%20However%2C%20concerns%20regarding%0Acopyright%20infringement%20arise%20as%20LLMs%20may%20inadvertently%20produce%20copyrighted%0Amaterial.%20In%20this%20paper%2C%20we%20first%20investigate%20the%20effectiveness%20of%20watermarking%0ALLMs%20as%20a%20deterrent%20against%20the%20generation%20of%20copyrighted%20texts.%20Through%0Atheoretical%20analysis%20and%20empirical%20evaluation%2C%20we%20demonstrate%20that%0Aincorporating%20watermarks%20into%20LLMs%20significantly%20reduces%20the%20likelihood%20of%0Agenerating%20copyrighted%20content%2C%20thereby%20addressing%20a%20critical%20concern%20in%20the%0Adeployment%20of%20LLMs.%20Additionally%2C%20we%20explore%20the%20impact%20of%20watermarking%20on%0AMembership%20Inference%20Attacks%20%28MIAs%29%2C%20which%20aim%20to%20discern%20whether%20a%20sample%20was%0Apart%20of%20the%20pretraining%20dataset%20and%20may%20be%20used%20to%20detect%20copyright%20violations.%0ASurprisingly%2C%20we%20find%20that%20watermarking%20adversely%20affects%20the%20success%20rate%20of%0AMIAs%2C%20complicating%20the%20task%20of%20detecting%20copyrighted%20text%20in%20the%20pretraining%0Adataset.%20Finally%2C%20we%20propose%20an%20adaptive%20technique%20to%20improve%20the%20success%20rate%0Aof%20a%20recent%20MIA%20under%20watermarking.%20Our%20findings%20underscore%20the%20importance%20of%0Adeveloping%20adaptive%20methods%20to%20study%20critical%20problems%20in%20LLMs%20with%20potential%0Alegal%20implications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17417v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Watermarking%2520Large%2520Language%2520Models%2520Prevent%2520Copyrighted%2520Text%250A%2520%2520Generation%2520and%2520Hide%2520Training%2520Data%253F%26entry.906535625%3DMichael-Andrei%2520Panaitescu-Liess%2520and%2520Zora%2520Che%2520and%2520Bang%2520An%2520and%2520Yuancheng%2520Xu%2520and%2520Pankayaraj%2520Pathmanathan%2520and%2520Souradip%2520Chakraborty%2520and%2520Sicheng%2520Zhu%2520and%2520Tom%2520Goldstein%2520and%2520Furong%2520Huang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520impressive%2520capabilities%2520in%250Agenerating%2520diverse%2520and%2520contextually%2520rich%2520text.%2520However%252C%2520concerns%2520regarding%250Acopyright%2520infringement%2520arise%2520as%2520LLMs%2520may%2520inadvertently%2520produce%2520copyrighted%250Amaterial.%2520In%2520this%2520paper%252C%2520we%2520first%2520investigate%2520the%2520effectiveness%2520of%2520watermarking%250ALLMs%2520as%2520a%2520deterrent%2520against%2520the%2520generation%2520of%2520copyrighted%2520texts.%2520Through%250Atheoretical%2520analysis%2520and%2520empirical%2520evaluation%252C%2520we%2520demonstrate%2520that%250Aincorporating%2520watermarks%2520into%2520LLMs%2520significantly%2520reduces%2520the%2520likelihood%2520of%250Agenerating%2520copyrighted%2520content%252C%2520thereby%2520addressing%2520a%2520critical%2520concern%2520in%2520the%250Adeployment%2520of%2520LLMs.%2520Additionally%252C%2520we%2520explore%2520the%2520impact%2520of%2520watermarking%2520on%250AMembership%2520Inference%2520Attacks%2520%2528MIAs%2529%252C%2520which%2520aim%2520to%2520discern%2520whether%2520a%2520sample%2520was%250Apart%2520of%2520the%2520pretraining%2520dataset%2520and%2520may%2520be%2520used%2520to%2520detect%2520copyright%2520violations.%250ASurprisingly%252C%2520we%2520find%2520that%2520watermarking%2520adversely%2520affects%2520the%2520success%2520rate%2520of%250AMIAs%252C%2520complicating%2520the%2520task%2520of%2520detecting%2520copyrighted%2520text%2520in%2520the%2520pretraining%250Adataset.%2520Finally%252C%2520we%2520propose%2520an%2520adaptive%2520technique%2520to%2520improve%2520the%2520success%2520rate%250Aof%2520a%2520recent%2520MIA%2520under%2520watermarking.%2520Our%2520findings%2520underscore%2520the%2520importance%2520of%250Adeveloping%2520adaptive%2520methods%2520to%2520study%2520critical%2520problems%2520in%2520LLMs%2520with%2520potential%250Alegal%2520implications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17417v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Watermarking%20Large%20Language%20Models%20Prevent%20Copyrighted%20Text%0A%20%20Generation%20and%20Hide%20Training%20Data%3F&entry.906535625=Michael-Andrei%20Panaitescu-Liess%20and%20Zora%20Che%20and%20Bang%20An%20and%20Yuancheng%20Xu%20and%20Pankayaraj%20Pathmanathan%20and%20Souradip%20Chakraborty%20and%20Sicheng%20Zhu%20and%20Tom%20Goldstein%20and%20Furong%20Huang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20impressive%20capabilities%20in%0Agenerating%20diverse%20and%20contextually%20rich%20text.%20However%2C%20concerns%20regarding%0Acopyright%20infringement%20arise%20as%20LLMs%20may%20inadvertently%20produce%20copyrighted%0Amaterial.%20In%20this%20paper%2C%20we%20first%20investigate%20the%20effectiveness%20of%20watermarking%0ALLMs%20as%20a%20deterrent%20against%20the%20generation%20of%20copyrighted%20texts.%20Through%0Atheoretical%20analysis%20and%20empirical%20evaluation%2C%20we%20demonstrate%20that%0Aincorporating%20watermarks%20into%20LLMs%20significantly%20reduces%20the%20likelihood%20of%0Agenerating%20copyrighted%20content%2C%20thereby%20addressing%20a%20critical%20concern%20in%20the%0Adeployment%20of%20LLMs.%20Additionally%2C%20we%20explore%20the%20impact%20of%20watermarking%20on%0AMembership%20Inference%20Attacks%20%28MIAs%29%2C%20which%20aim%20to%20discern%20whether%20a%20sample%20was%0Apart%20of%20the%20pretraining%20dataset%20and%20may%20be%20used%20to%20detect%20copyright%20violations.%0ASurprisingly%2C%20we%20find%20that%20watermarking%20adversely%20affects%20the%20success%20rate%20of%0AMIAs%2C%20complicating%20the%20task%20of%20detecting%20copyrighted%20text%20in%20the%20pretraining%0Adataset.%20Finally%2C%20we%20propose%20an%20adaptive%20technique%20to%20improve%20the%20success%20rate%0Aof%20a%20recent%20MIA%20under%20watermarking.%20Our%20findings%20underscore%20the%20importance%20of%0Adeveloping%20adaptive%20methods%20to%20study%20critical%20problems%20in%20LLMs%20with%20potential%0Alegal%20implications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17417v1&entry.124074799=Read"},
{"title": "Efficiently Reconfiguring a Connected Swarm of Labeled Robots", "author": "S\u00e1ndor P. Fekete and Peter Kramer and Christian Rieck and Christian Scheffer and Arne Schmidt", "abstract": "  When considering motion planning for a swarm of $n$ labeled robots, we need\nto rearrange a given start configuration into a desired target configuration\nvia a sequence of parallel, collision-free robot motions. The objective is to\nreach the new configuration in a minimum amount of time; an important\nconstraint is to keep the swarm connected at all times. Problems of this type\nhave been considered before, with recent notable results achieving constant\nstretch for not necessarily connected reconfiguration: If mapping the start\nconfiguration to the target configuration requires a maximum Manhattan distance\nof $d$, the total duration of an overall schedule can be bounded to\n$\\mathcal{O}(d)$, which is optimal up to constant factors. However, constant\nstretch could only be achieved if disconnected reconfiguration is allowed, or\nfor scaled configurations (which arise by increasing all dimensions of a given\nobject by the same multiplicative factor) of unlabeled robots.\n  We resolve these major open problems by (1) establishing a lower bound of\n$\\Omega(\\sqrt{n})$ for connected, labeled reconfiguration and, most\nimportantly, by (2) proving that for scaled arrangements, constant stretch for\nconnected reconfiguration can be achieved. In addition, we show that (3) it is\nNP-complete to decide whether a makespan of 2 can be achieved, while it is\npossible to check in polynomial time whether a makespan of 1 can be achieved.\n", "link": "http://arxiv.org/abs/2209.11028v2", "date": "2024-07-24", "relevancy": 1.3731, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5199}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4413}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficiently%20Reconfiguring%20a%20Connected%20Swarm%20of%20Labeled%20Robots&body=Title%3A%20Efficiently%20Reconfiguring%20a%20Connected%20Swarm%20of%20Labeled%20Robots%0AAuthor%3A%20S%C3%A1ndor%20P.%20Fekete%20and%20Peter%20Kramer%20and%20Christian%20Rieck%20and%20Christian%20Scheffer%20and%20Arne%20Schmidt%0AAbstract%3A%20%20%20When%20considering%20motion%20planning%20for%20a%20swarm%20of%20%24n%24%20labeled%20robots%2C%20we%20need%0Ato%20rearrange%20a%20given%20start%20configuration%20into%20a%20desired%20target%20configuration%0Avia%20a%20sequence%20of%20parallel%2C%20collision-free%20robot%20motions.%20The%20objective%20is%20to%0Areach%20the%20new%20configuration%20in%20a%20minimum%20amount%20of%20time%3B%20an%20important%0Aconstraint%20is%20to%20keep%20the%20swarm%20connected%20at%20all%20times.%20Problems%20of%20this%20type%0Ahave%20been%20considered%20before%2C%20with%20recent%20notable%20results%20achieving%20constant%0Astretch%20for%20not%20necessarily%20connected%20reconfiguration%3A%20If%20mapping%20the%20start%0Aconfiguration%20to%20the%20target%20configuration%20requires%20a%20maximum%20Manhattan%20distance%0Aof%20%24d%24%2C%20the%20total%20duration%20of%20an%20overall%20schedule%20can%20be%20bounded%20to%0A%24%5Cmathcal%7BO%7D%28d%29%24%2C%20which%20is%20optimal%20up%20to%20constant%20factors.%20However%2C%20constant%0Astretch%20could%20only%20be%20achieved%20if%20disconnected%20reconfiguration%20is%20allowed%2C%20or%0Afor%20scaled%20configurations%20%28which%20arise%20by%20increasing%20all%20dimensions%20of%20a%20given%0Aobject%20by%20the%20same%20multiplicative%20factor%29%20of%20unlabeled%20robots.%0A%20%20We%20resolve%20these%20major%20open%20problems%20by%20%281%29%20establishing%20a%20lower%20bound%20of%0A%24%5COmega%28%5Csqrt%7Bn%7D%29%24%20for%20connected%2C%20labeled%20reconfiguration%20and%2C%20most%0Aimportantly%2C%20by%20%282%29%20proving%20that%20for%20scaled%20arrangements%2C%20constant%20stretch%20for%0Aconnected%20reconfiguration%20can%20be%20achieved.%20In%20addition%2C%20we%20show%20that%20%283%29%20it%20is%0ANP-complete%20to%20decide%20whether%20a%20makespan%20of%202%20can%20be%20achieved%2C%20while%20it%20is%0Apossible%20to%20check%20in%20polynomial%20time%20whether%20a%20makespan%20of%201%20can%20be%20achieved.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.11028v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficiently%2520Reconfiguring%2520a%2520Connected%2520Swarm%2520of%2520Labeled%2520Robots%26entry.906535625%3DS%25C3%25A1ndor%2520P.%2520Fekete%2520and%2520Peter%2520Kramer%2520and%2520Christian%2520Rieck%2520and%2520Christian%2520Scheffer%2520and%2520Arne%2520Schmidt%26entry.1292438233%3D%2520%2520When%2520considering%2520motion%2520planning%2520for%2520a%2520swarm%2520of%2520%2524n%2524%2520labeled%2520robots%252C%2520we%2520need%250Ato%2520rearrange%2520a%2520given%2520start%2520configuration%2520into%2520a%2520desired%2520target%2520configuration%250Avia%2520a%2520sequence%2520of%2520parallel%252C%2520collision-free%2520robot%2520motions.%2520The%2520objective%2520is%2520to%250Areach%2520the%2520new%2520configuration%2520in%2520a%2520minimum%2520amount%2520of%2520time%253B%2520an%2520important%250Aconstraint%2520is%2520to%2520keep%2520the%2520swarm%2520connected%2520at%2520all%2520times.%2520Problems%2520of%2520this%2520type%250Ahave%2520been%2520considered%2520before%252C%2520with%2520recent%2520notable%2520results%2520achieving%2520constant%250Astretch%2520for%2520not%2520necessarily%2520connected%2520reconfiguration%253A%2520If%2520mapping%2520the%2520start%250Aconfiguration%2520to%2520the%2520target%2520configuration%2520requires%2520a%2520maximum%2520Manhattan%2520distance%250Aof%2520%2524d%2524%252C%2520the%2520total%2520duration%2520of%2520an%2520overall%2520schedule%2520can%2520be%2520bounded%2520to%250A%2524%255Cmathcal%257BO%257D%2528d%2529%2524%252C%2520which%2520is%2520optimal%2520up%2520to%2520constant%2520factors.%2520However%252C%2520constant%250Astretch%2520could%2520only%2520be%2520achieved%2520if%2520disconnected%2520reconfiguration%2520is%2520allowed%252C%2520or%250Afor%2520scaled%2520configurations%2520%2528which%2520arise%2520by%2520increasing%2520all%2520dimensions%2520of%2520a%2520given%250Aobject%2520by%2520the%2520same%2520multiplicative%2520factor%2529%2520of%2520unlabeled%2520robots.%250A%2520%2520We%2520resolve%2520these%2520major%2520open%2520problems%2520by%2520%25281%2529%2520establishing%2520a%2520lower%2520bound%2520of%250A%2524%255COmega%2528%255Csqrt%257Bn%257D%2529%2524%2520for%2520connected%252C%2520labeled%2520reconfiguration%2520and%252C%2520most%250Aimportantly%252C%2520by%2520%25282%2529%2520proving%2520that%2520for%2520scaled%2520arrangements%252C%2520constant%2520stretch%2520for%250Aconnected%2520reconfiguration%2520can%2520be%2520achieved.%2520In%2520addition%252C%2520we%2520show%2520that%2520%25283%2529%2520it%2520is%250ANP-complete%2520to%2520decide%2520whether%2520a%2520makespan%2520of%25202%2520can%2520be%2520achieved%252C%2520while%2520it%2520is%250Apossible%2520to%2520check%2520in%2520polynomial%2520time%2520whether%2520a%2520makespan%2520of%25201%2520can%2520be%2520achieved.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.11028v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficiently%20Reconfiguring%20a%20Connected%20Swarm%20of%20Labeled%20Robots&entry.906535625=S%C3%A1ndor%20P.%20Fekete%20and%20Peter%20Kramer%20and%20Christian%20Rieck%20and%20Christian%20Scheffer%20and%20Arne%20Schmidt&entry.1292438233=%20%20When%20considering%20motion%20planning%20for%20a%20swarm%20of%20%24n%24%20labeled%20robots%2C%20we%20need%0Ato%20rearrange%20a%20given%20start%20configuration%20into%20a%20desired%20target%20configuration%0Avia%20a%20sequence%20of%20parallel%2C%20collision-free%20robot%20motions.%20The%20objective%20is%20to%0Areach%20the%20new%20configuration%20in%20a%20minimum%20amount%20of%20time%3B%20an%20important%0Aconstraint%20is%20to%20keep%20the%20swarm%20connected%20at%20all%20times.%20Problems%20of%20this%20type%0Ahave%20been%20considered%20before%2C%20with%20recent%20notable%20results%20achieving%20constant%0Astretch%20for%20not%20necessarily%20connected%20reconfiguration%3A%20If%20mapping%20the%20start%0Aconfiguration%20to%20the%20target%20configuration%20requires%20a%20maximum%20Manhattan%20distance%0Aof%20%24d%24%2C%20the%20total%20duration%20of%20an%20overall%20schedule%20can%20be%20bounded%20to%0A%24%5Cmathcal%7BO%7D%28d%29%24%2C%20which%20is%20optimal%20up%20to%20constant%20factors.%20However%2C%20constant%0Astretch%20could%20only%20be%20achieved%20if%20disconnected%20reconfiguration%20is%20allowed%2C%20or%0Afor%20scaled%20configurations%20%28which%20arise%20by%20increasing%20all%20dimensions%20of%20a%20given%0Aobject%20by%20the%20same%20multiplicative%20factor%29%20of%20unlabeled%20robots.%0A%20%20We%20resolve%20these%20major%20open%20problems%20by%20%281%29%20establishing%20a%20lower%20bound%20of%0A%24%5COmega%28%5Csqrt%7Bn%7D%29%24%20for%20connected%2C%20labeled%20reconfiguration%20and%2C%20most%0Aimportantly%2C%20by%20%282%29%20proving%20that%20for%20scaled%20arrangements%2C%20constant%20stretch%20for%0Aconnected%20reconfiguration%20can%20be%20achieved.%20In%20addition%2C%20we%20show%20that%20%283%29%20it%20is%0ANP-complete%20to%20decide%20whether%20a%20makespan%20of%202%20can%20be%20achieved%2C%20while%20it%20is%0Apossible%20to%20check%20in%20polynomial%20time%20whether%20a%20makespan%20of%201%20can%20be%20achieved.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.11028v2&entry.124074799=Read"},
{"title": "Pacer and Runner: Cooperative Learning Framework between Single- and\n  Cross-Domain Sequential Recommendation", "author": "Chung Park and Taesan Kim and Hyungjun Yoon and Junui Hong and Yelim Yu and Mincheol Cho and Minsung Choi and Jaegul Choo", "abstract": "  Cross-Domain Sequential Recommendation (CDSR) improves recommendation\nperformance by utilizing information from multiple domains, which contrasts\nwith Single-Domain Sequential Recommendation (SDSR) that relies on a historical\ninteraction within a specific domain. However, CDSR may underperform compared\nto the SDSR approach in certain domains due to negative transfer, which occurs\nwhen there is a lack of relation between domains or different levels of data\nsparsity. To address the issue of negative transfer, our proposed CDSR model\nestimates the degree of negative transfer of each domain and adaptively assigns\nit as a weight factor to the prediction loss, to control gradient flows through\ndomains with significant negative transfer. To this end, our model compares the\nperformance of a model trained on multiple domains (CDSR) with a model trained\nsolely on the specific domain (SDSR) to evaluate the negative transfer of each\ndomain using our asymmetric cooperative network. In addition, to facilitate the\ntransfer of valuable cues between the SDSR and CDSR tasks, we developed an\nauxiliary loss that maximizes the mutual information between the representation\npairs from both tasks on a per-domain basis. This cooperative learning between\nSDSR and CDSR tasks is similar to the collaborative dynamics between pacers and\nrunners in a marathon. Our model outperformed numerous previous works in\nextensive experiments on two real-world industrial datasets across ten service\ndomains. We also have deployed our model in the recommendation system of our\npersonal assistant app service, resulting in 21.4% increase in click-through\nrate compared to existing models, which is valuable to real-world business.\n", "link": "http://arxiv.org/abs/2407.11245v2", "date": "2024-07-24", "relevancy": 1.4376, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5004}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4755}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pacer%20and%20Runner%3A%20Cooperative%20Learning%20Framework%20between%20Single-%20and%0A%20%20Cross-Domain%20Sequential%20Recommendation&body=Title%3A%20Pacer%20and%20Runner%3A%20Cooperative%20Learning%20Framework%20between%20Single-%20and%0A%20%20Cross-Domain%20Sequential%20Recommendation%0AAuthor%3A%20Chung%20Park%20and%20Taesan%20Kim%20and%20Hyungjun%20Yoon%20and%20Junui%20Hong%20and%20Yelim%20Yu%20and%20Mincheol%20Cho%20and%20Minsung%20Choi%20and%20Jaegul%20Choo%0AAbstract%3A%20%20%20Cross-Domain%20Sequential%20Recommendation%20%28CDSR%29%20improves%20recommendation%0Aperformance%20by%20utilizing%20information%20from%20multiple%20domains%2C%20which%20contrasts%0Awith%20Single-Domain%20Sequential%20Recommendation%20%28SDSR%29%20that%20relies%20on%20a%20historical%0Ainteraction%20within%20a%20specific%20domain.%20However%2C%20CDSR%20may%20underperform%20compared%0Ato%20the%20SDSR%20approach%20in%20certain%20domains%20due%20to%20negative%20transfer%2C%20which%20occurs%0Awhen%20there%20is%20a%20lack%20of%20relation%20between%20domains%20or%20different%20levels%20of%20data%0Asparsity.%20To%20address%20the%20issue%20of%20negative%20transfer%2C%20our%20proposed%20CDSR%20model%0Aestimates%20the%20degree%20of%20negative%20transfer%20of%20each%20domain%20and%20adaptively%20assigns%0Ait%20as%20a%20weight%20factor%20to%20the%20prediction%20loss%2C%20to%20control%20gradient%20flows%20through%0Adomains%20with%20significant%20negative%20transfer.%20To%20this%20end%2C%20our%20model%20compares%20the%0Aperformance%20of%20a%20model%20trained%20on%20multiple%20domains%20%28CDSR%29%20with%20a%20model%20trained%0Asolely%20on%20the%20specific%20domain%20%28SDSR%29%20to%20evaluate%20the%20negative%20transfer%20of%20each%0Adomain%20using%20our%20asymmetric%20cooperative%20network.%20In%20addition%2C%20to%20facilitate%20the%0Atransfer%20of%20valuable%20cues%20between%20the%20SDSR%20and%20CDSR%20tasks%2C%20we%20developed%20an%0Aauxiliary%20loss%20that%20maximizes%20the%20mutual%20information%20between%20the%20representation%0Apairs%20from%20both%20tasks%20on%20a%20per-domain%20basis.%20This%20cooperative%20learning%20between%0ASDSR%20and%20CDSR%20tasks%20is%20similar%20to%20the%20collaborative%20dynamics%20between%20pacers%20and%0Arunners%20in%20a%20marathon.%20Our%20model%20outperformed%20numerous%20previous%20works%20in%0Aextensive%20experiments%20on%20two%20real-world%20industrial%20datasets%20across%20ten%20service%0Adomains.%20We%20also%20have%20deployed%20our%20model%20in%20the%20recommendation%20system%20of%20our%0Apersonal%20assistant%20app%20service%2C%20resulting%20in%2021.4%25%20increase%20in%20click-through%0Arate%20compared%20to%20existing%20models%2C%20which%20is%20valuable%20to%20real-world%20business.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11245v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPacer%2520and%2520Runner%253A%2520Cooperative%2520Learning%2520Framework%2520between%2520Single-%2520and%250A%2520%2520Cross-Domain%2520Sequential%2520Recommendation%26entry.906535625%3DChung%2520Park%2520and%2520Taesan%2520Kim%2520and%2520Hyungjun%2520Yoon%2520and%2520Junui%2520Hong%2520and%2520Yelim%2520Yu%2520and%2520Mincheol%2520Cho%2520and%2520Minsung%2520Choi%2520and%2520Jaegul%2520Choo%26entry.1292438233%3D%2520%2520Cross-Domain%2520Sequential%2520Recommendation%2520%2528CDSR%2529%2520improves%2520recommendation%250Aperformance%2520by%2520utilizing%2520information%2520from%2520multiple%2520domains%252C%2520which%2520contrasts%250Awith%2520Single-Domain%2520Sequential%2520Recommendation%2520%2528SDSR%2529%2520that%2520relies%2520on%2520a%2520historical%250Ainteraction%2520within%2520a%2520specific%2520domain.%2520However%252C%2520CDSR%2520may%2520underperform%2520compared%250Ato%2520the%2520SDSR%2520approach%2520in%2520certain%2520domains%2520due%2520to%2520negative%2520transfer%252C%2520which%2520occurs%250Awhen%2520there%2520is%2520a%2520lack%2520of%2520relation%2520between%2520domains%2520or%2520different%2520levels%2520of%2520data%250Asparsity.%2520To%2520address%2520the%2520issue%2520of%2520negative%2520transfer%252C%2520our%2520proposed%2520CDSR%2520model%250Aestimates%2520the%2520degree%2520of%2520negative%2520transfer%2520of%2520each%2520domain%2520and%2520adaptively%2520assigns%250Ait%2520as%2520a%2520weight%2520factor%2520to%2520the%2520prediction%2520loss%252C%2520to%2520control%2520gradient%2520flows%2520through%250Adomains%2520with%2520significant%2520negative%2520transfer.%2520To%2520this%2520end%252C%2520our%2520model%2520compares%2520the%250Aperformance%2520of%2520a%2520model%2520trained%2520on%2520multiple%2520domains%2520%2528CDSR%2529%2520with%2520a%2520model%2520trained%250Asolely%2520on%2520the%2520specific%2520domain%2520%2528SDSR%2529%2520to%2520evaluate%2520the%2520negative%2520transfer%2520of%2520each%250Adomain%2520using%2520our%2520asymmetric%2520cooperative%2520network.%2520In%2520addition%252C%2520to%2520facilitate%2520the%250Atransfer%2520of%2520valuable%2520cues%2520between%2520the%2520SDSR%2520and%2520CDSR%2520tasks%252C%2520we%2520developed%2520an%250Aauxiliary%2520loss%2520that%2520maximizes%2520the%2520mutual%2520information%2520between%2520the%2520representation%250Apairs%2520from%2520both%2520tasks%2520on%2520a%2520per-domain%2520basis.%2520This%2520cooperative%2520learning%2520between%250ASDSR%2520and%2520CDSR%2520tasks%2520is%2520similar%2520to%2520the%2520collaborative%2520dynamics%2520between%2520pacers%2520and%250Arunners%2520in%2520a%2520marathon.%2520Our%2520model%2520outperformed%2520numerous%2520previous%2520works%2520in%250Aextensive%2520experiments%2520on%2520two%2520real-world%2520industrial%2520datasets%2520across%2520ten%2520service%250Adomains.%2520We%2520also%2520have%2520deployed%2520our%2520model%2520in%2520the%2520recommendation%2520system%2520of%2520our%250Apersonal%2520assistant%2520app%2520service%252C%2520resulting%2520in%252021.4%2525%2520increase%2520in%2520click-through%250Arate%2520compared%2520to%2520existing%2520models%252C%2520which%2520is%2520valuable%2520to%2520real-world%2520business.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11245v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pacer%20and%20Runner%3A%20Cooperative%20Learning%20Framework%20between%20Single-%20and%0A%20%20Cross-Domain%20Sequential%20Recommendation&entry.906535625=Chung%20Park%20and%20Taesan%20Kim%20and%20Hyungjun%20Yoon%20and%20Junui%20Hong%20and%20Yelim%20Yu%20and%20Mincheol%20Cho%20and%20Minsung%20Choi%20and%20Jaegul%20Choo&entry.1292438233=%20%20Cross-Domain%20Sequential%20Recommendation%20%28CDSR%29%20improves%20recommendation%0Aperformance%20by%20utilizing%20information%20from%20multiple%20domains%2C%20which%20contrasts%0Awith%20Single-Domain%20Sequential%20Recommendation%20%28SDSR%29%20that%20relies%20on%20a%20historical%0Ainteraction%20within%20a%20specific%20domain.%20However%2C%20CDSR%20may%20underperform%20compared%0Ato%20the%20SDSR%20approach%20in%20certain%20domains%20due%20to%20negative%20transfer%2C%20which%20occurs%0Awhen%20there%20is%20a%20lack%20of%20relation%20between%20domains%20or%20different%20levels%20of%20data%0Asparsity.%20To%20address%20the%20issue%20of%20negative%20transfer%2C%20our%20proposed%20CDSR%20model%0Aestimates%20the%20degree%20of%20negative%20transfer%20of%20each%20domain%20and%20adaptively%20assigns%0Ait%20as%20a%20weight%20factor%20to%20the%20prediction%20loss%2C%20to%20control%20gradient%20flows%20through%0Adomains%20with%20significant%20negative%20transfer.%20To%20this%20end%2C%20our%20model%20compares%20the%0Aperformance%20of%20a%20model%20trained%20on%20multiple%20domains%20%28CDSR%29%20with%20a%20model%20trained%0Asolely%20on%20the%20specific%20domain%20%28SDSR%29%20to%20evaluate%20the%20negative%20transfer%20of%20each%0Adomain%20using%20our%20asymmetric%20cooperative%20network.%20In%20addition%2C%20to%20facilitate%20the%0Atransfer%20of%20valuable%20cues%20between%20the%20SDSR%20and%20CDSR%20tasks%2C%20we%20developed%20an%0Aauxiliary%20loss%20that%20maximizes%20the%20mutual%20information%20between%20the%20representation%0Apairs%20from%20both%20tasks%20on%20a%20per-domain%20basis.%20This%20cooperative%20learning%20between%0ASDSR%20and%20CDSR%20tasks%20is%20similar%20to%20the%20collaborative%20dynamics%20between%20pacers%20and%0Arunners%20in%20a%20marathon.%20Our%20model%20outperformed%20numerous%20previous%20works%20in%0Aextensive%20experiments%20on%20two%20real-world%20industrial%20datasets%20across%20ten%20service%0Adomains.%20We%20also%20have%20deployed%20our%20model%20in%20the%20recommendation%20system%20of%20our%0Apersonal%20assistant%20app%20service%2C%20resulting%20in%2021.4%25%20increase%20in%20click-through%0Arate%20compared%20to%20existing%20models%2C%20which%20is%20valuable%20to%20real-world%20business.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11245v2&entry.124074799=Read"},
{"title": "Comparison of Waymo Rider-Only Crash Data to Human Benchmarks at 7.1\n  Million Miles", "author": "Kristofer D. Kusano and John M. Scanlon and Yin-Hsiu Chen and Timothy L. McMurry and Ruoshu Chen and Tilia Gode and Trent Victor", "abstract": "  This paper examines the safety performance of the Waymo Driver, an SAE level\n4 automated driving system (ADS) used in a rider-only (RO) ride-hailing\napplication without a human driver, either in the vehicle or remotely. ADS\ncrash data was derived from NHTSA's Standing General Order (SGO) reporting over\n7.14 million RO miles through the end of October 2023 in Phoenix, AZ, San\nFrancisco, CA, and Los Angeles, CA. When considering all locations together,\nthe any-injury-reported crashed vehicle rate was 0.41 incidents per million\nmiles (IPMM) for the ADS vs 2.80 IPMM for the human benchmark, an 85% reduction\nor a human crash rate that is 6.7 times higher than the ADS rate.\nPolice-reported crashed vehicle rates for all locations together were 2.1 IPMM\nfor the ADS vs. 4.68 IPMM for the human benchmark, a 55% reduction or a human\ncrash rate that was 2.2 times higher than the ADS rate. Police-reported and\nany-injury-reported crashed vehicle rate reductions for the ADS were\nstatistically significant when compared in San Francisco and Phoenix, as well\nas combined across all locations. The any property damage or injury comparison\nhad statistically significant decrease in 3 comparisons, but also\nnon-significant results in 3 other benchmarks. Given imprecision in the\nbenchmark estimate and multiple potential sources of underreporting biasing the\nbenchmarks, caution should be taken when interpreting the results of the any\nproperty damage or injury comparison. Together, these crash-rate results should\nbe interpreted as a directional and continuous confidence growth indicator,\ntogether with other methodologies, in a safety case approach.\n", "link": "http://arxiv.org/abs/2312.12675v2", "date": "2024-07-24", "relevancy": 1.6401, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4564}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4245}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparison%20of%20Waymo%20Rider-Only%20Crash%20Data%20to%20Human%20Benchmarks%20at%207.1%0A%20%20Million%20Miles&body=Title%3A%20Comparison%20of%20Waymo%20Rider-Only%20Crash%20Data%20to%20Human%20Benchmarks%20at%207.1%0A%20%20Million%20Miles%0AAuthor%3A%20Kristofer%20D.%20Kusano%20and%20John%20M.%20Scanlon%20and%20Yin-Hsiu%20Chen%20and%20Timothy%20L.%20McMurry%20and%20Ruoshu%20Chen%20and%20Tilia%20Gode%20and%20Trent%20Victor%0AAbstract%3A%20%20%20This%20paper%20examines%20the%20safety%20performance%20of%20the%20Waymo%20Driver%2C%20an%20SAE%20level%0A4%20automated%20driving%20system%20%28ADS%29%20used%20in%20a%20rider-only%20%28RO%29%20ride-hailing%0Aapplication%20without%20a%20human%20driver%2C%20either%20in%20the%20vehicle%20or%20remotely.%20ADS%0Acrash%20data%20was%20derived%20from%20NHTSA%27s%20Standing%20General%20Order%20%28SGO%29%20reporting%20over%0A7.14%20million%20RO%20miles%20through%20the%20end%20of%20October%202023%20in%20Phoenix%2C%20AZ%2C%20San%0AFrancisco%2C%20CA%2C%20and%20Los%20Angeles%2C%20CA.%20When%20considering%20all%20locations%20together%2C%0Athe%20any-injury-reported%20crashed%20vehicle%20rate%20was%200.41%20incidents%20per%20million%0Amiles%20%28IPMM%29%20for%20the%20ADS%20vs%202.80%20IPMM%20for%20the%20human%20benchmark%2C%20an%2085%25%20reduction%0Aor%20a%20human%20crash%20rate%20that%20is%206.7%20times%20higher%20than%20the%20ADS%20rate.%0APolice-reported%20crashed%20vehicle%20rates%20for%20all%20locations%20together%20were%202.1%20IPMM%0Afor%20the%20ADS%20vs.%204.68%20IPMM%20for%20the%20human%20benchmark%2C%20a%2055%25%20reduction%20or%20a%20human%0Acrash%20rate%20that%20was%202.2%20times%20higher%20than%20the%20ADS%20rate.%20Police-reported%20and%0Aany-injury-reported%20crashed%20vehicle%20rate%20reductions%20for%20the%20ADS%20were%0Astatistically%20significant%20when%20compared%20in%20San%20Francisco%20and%20Phoenix%2C%20as%20well%0Aas%20combined%20across%20all%20locations.%20The%20any%20property%20damage%20or%20injury%20comparison%0Ahad%20statistically%20significant%20decrease%20in%203%20comparisons%2C%20but%20also%0Anon-significant%20results%20in%203%20other%20benchmarks.%20Given%20imprecision%20in%20the%0Abenchmark%20estimate%20and%20multiple%20potential%20sources%20of%20underreporting%20biasing%20the%0Abenchmarks%2C%20caution%20should%20be%20taken%20when%20interpreting%20the%20results%20of%20the%20any%0Aproperty%20damage%20or%20injury%20comparison.%20Together%2C%20these%20crash-rate%20results%20should%0Abe%20interpreted%20as%20a%20directional%20and%20continuous%20confidence%20growth%20indicator%2C%0Atogether%20with%20other%20methodologies%2C%20in%20a%20safety%20case%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.12675v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparison%2520of%2520Waymo%2520Rider-Only%2520Crash%2520Data%2520to%2520Human%2520Benchmarks%2520at%25207.1%250A%2520%2520Million%2520Miles%26entry.906535625%3DKristofer%2520D.%2520Kusano%2520and%2520John%2520M.%2520Scanlon%2520and%2520Yin-Hsiu%2520Chen%2520and%2520Timothy%2520L.%2520McMurry%2520and%2520Ruoshu%2520Chen%2520and%2520Tilia%2520Gode%2520and%2520Trent%2520Victor%26entry.1292438233%3D%2520%2520This%2520paper%2520examines%2520the%2520safety%2520performance%2520of%2520the%2520Waymo%2520Driver%252C%2520an%2520SAE%2520level%250A4%2520automated%2520driving%2520system%2520%2528ADS%2529%2520used%2520in%2520a%2520rider-only%2520%2528RO%2529%2520ride-hailing%250Aapplication%2520without%2520a%2520human%2520driver%252C%2520either%2520in%2520the%2520vehicle%2520or%2520remotely.%2520ADS%250Acrash%2520data%2520was%2520derived%2520from%2520NHTSA%2527s%2520Standing%2520General%2520Order%2520%2528SGO%2529%2520reporting%2520over%250A7.14%2520million%2520RO%2520miles%2520through%2520the%2520end%2520of%2520October%25202023%2520in%2520Phoenix%252C%2520AZ%252C%2520San%250AFrancisco%252C%2520CA%252C%2520and%2520Los%2520Angeles%252C%2520CA.%2520When%2520considering%2520all%2520locations%2520together%252C%250Athe%2520any-injury-reported%2520crashed%2520vehicle%2520rate%2520was%25200.41%2520incidents%2520per%2520million%250Amiles%2520%2528IPMM%2529%2520for%2520the%2520ADS%2520vs%25202.80%2520IPMM%2520for%2520the%2520human%2520benchmark%252C%2520an%252085%2525%2520reduction%250Aor%2520a%2520human%2520crash%2520rate%2520that%2520is%25206.7%2520times%2520higher%2520than%2520the%2520ADS%2520rate.%250APolice-reported%2520crashed%2520vehicle%2520rates%2520for%2520all%2520locations%2520together%2520were%25202.1%2520IPMM%250Afor%2520the%2520ADS%2520vs.%25204.68%2520IPMM%2520for%2520the%2520human%2520benchmark%252C%2520a%252055%2525%2520reduction%2520or%2520a%2520human%250Acrash%2520rate%2520that%2520was%25202.2%2520times%2520higher%2520than%2520the%2520ADS%2520rate.%2520Police-reported%2520and%250Aany-injury-reported%2520crashed%2520vehicle%2520rate%2520reductions%2520for%2520the%2520ADS%2520were%250Astatistically%2520significant%2520when%2520compared%2520in%2520San%2520Francisco%2520and%2520Phoenix%252C%2520as%2520well%250Aas%2520combined%2520across%2520all%2520locations.%2520The%2520any%2520property%2520damage%2520or%2520injury%2520comparison%250Ahad%2520statistically%2520significant%2520decrease%2520in%25203%2520comparisons%252C%2520but%2520also%250Anon-significant%2520results%2520in%25203%2520other%2520benchmarks.%2520Given%2520imprecision%2520in%2520the%250Abenchmark%2520estimate%2520and%2520multiple%2520potential%2520sources%2520of%2520underreporting%2520biasing%2520the%250Abenchmarks%252C%2520caution%2520should%2520be%2520taken%2520when%2520interpreting%2520the%2520results%2520of%2520the%2520any%250Aproperty%2520damage%2520or%2520injury%2520comparison.%2520Together%252C%2520these%2520crash-rate%2520results%2520should%250Abe%2520interpreted%2520as%2520a%2520directional%2520and%2520continuous%2520confidence%2520growth%2520indicator%252C%250Atogether%2520with%2520other%2520methodologies%252C%2520in%2520a%2520safety%2520case%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.12675v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparison%20of%20Waymo%20Rider-Only%20Crash%20Data%20to%20Human%20Benchmarks%20at%207.1%0A%20%20Million%20Miles&entry.906535625=Kristofer%20D.%20Kusano%20and%20John%20M.%20Scanlon%20and%20Yin-Hsiu%20Chen%20and%20Timothy%20L.%20McMurry%20and%20Ruoshu%20Chen%20and%20Tilia%20Gode%20and%20Trent%20Victor&entry.1292438233=%20%20This%20paper%20examines%20the%20safety%20performance%20of%20the%20Waymo%20Driver%2C%20an%20SAE%20level%0A4%20automated%20driving%20system%20%28ADS%29%20used%20in%20a%20rider-only%20%28RO%29%20ride-hailing%0Aapplication%20without%20a%20human%20driver%2C%20either%20in%20the%20vehicle%20or%20remotely.%20ADS%0Acrash%20data%20was%20derived%20from%20NHTSA%27s%20Standing%20General%20Order%20%28SGO%29%20reporting%20over%0A7.14%20million%20RO%20miles%20through%20the%20end%20of%20October%202023%20in%20Phoenix%2C%20AZ%2C%20San%0AFrancisco%2C%20CA%2C%20and%20Los%20Angeles%2C%20CA.%20When%20considering%20all%20locations%20together%2C%0Athe%20any-injury-reported%20crashed%20vehicle%20rate%20was%200.41%20incidents%20per%20million%0Amiles%20%28IPMM%29%20for%20the%20ADS%20vs%202.80%20IPMM%20for%20the%20human%20benchmark%2C%20an%2085%25%20reduction%0Aor%20a%20human%20crash%20rate%20that%20is%206.7%20times%20higher%20than%20the%20ADS%20rate.%0APolice-reported%20crashed%20vehicle%20rates%20for%20all%20locations%20together%20were%202.1%20IPMM%0Afor%20the%20ADS%20vs.%204.68%20IPMM%20for%20the%20human%20benchmark%2C%20a%2055%25%20reduction%20or%20a%20human%0Acrash%20rate%20that%20was%202.2%20times%20higher%20than%20the%20ADS%20rate.%20Police-reported%20and%0Aany-injury-reported%20crashed%20vehicle%20rate%20reductions%20for%20the%20ADS%20were%0Astatistically%20significant%20when%20compared%20in%20San%20Francisco%20and%20Phoenix%2C%20as%20well%0Aas%20combined%20across%20all%20locations.%20The%20any%20property%20damage%20or%20injury%20comparison%0Ahad%20statistically%20significant%20decrease%20in%203%20comparisons%2C%20but%20also%0Anon-significant%20results%20in%203%20other%20benchmarks.%20Given%20imprecision%20in%20the%0Abenchmark%20estimate%20and%20multiple%20potential%20sources%20of%20underreporting%20biasing%20the%0Abenchmarks%2C%20caution%20should%20be%20taken%20when%20interpreting%20the%20results%20of%20the%20any%0Aproperty%20damage%20or%20injury%20comparison.%20Together%2C%20these%20crash-rate%20results%20should%0Abe%20interpreted%20as%20a%20directional%20and%20continuous%20confidence%20growth%20indicator%2C%0Atogether%20with%20other%20methodologies%2C%20in%20a%20safety%20case%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.12675v2&entry.124074799=Read"},
{"title": "SCIsegV2: A Universal Tool for Segmentation of Intramedullary Lesions in\n  Spinal Cord Injury", "author": "Enamundram Naga Karthik and Jan Valo\u0161ek and Lynn Farner and Dario Pfyffer and Simon Schading-Sassenhausen and Anna Lebret and Gergely David and Andrew C. Smith and Kenneth A. Weber II and Maryam Seif and RHSCIR Network Imaging Group and Patrick Freund and Julien Cohen-Adad", "abstract": "  Spinal cord injury (SCI) is a devastating incidence leading to permanent\nparalysis and loss of sensory-motor functions potentially resulting in the\nformation of lesions within the spinal cord. Imaging biomarkers obtained from\nmagnetic resonance imaging (MRI) scans can predict the functional recovery of\nindividuals with SCI and help choose the optimal treatment strategy. Currently,\nmost studies employ manual quantification of these MRI-derived biomarkers,\nwhich is a subjective and tedious task. In this work, we propose (i) a\nuniversal tool for the automatic segmentation of intramedullary SCI lesions,\ndubbed \\texttt{SCIsegV2}, and (ii) a method to automatically compute the width\nof the tissue bridges from the segmented lesion. Tissue bridges represent the\nspared spinal tissue adjacent to the lesion, which is associated with\nfunctional recovery in SCI patients. The tool was trained and validated on a\nheterogeneous dataset from 7 sites comprising patients from different SCI\nphases (acute, sub-acute, and chronic) and etiologies (traumatic SCI, ischemic\nSCI, and degenerative cervical myelopathy). Tissue bridges quantified\nautomatically did not significantly differ from those computed manually,\nsuggesting that the proposed automatic tool can be used to derive relevant MRI\nbiomarkers. \\texttt{SCIsegV2} and the automatic tissue bridges computation are\nopen-source and available in Spinal Cord Toolbox (v6.4 and above) via the\n\\texttt{sct\\_deepseg -task seg\\_sc\\_lesion\\_t2w\\_sci} and\n\\texttt{sct\\_analyze\\_lesion} functions, respectively.\n", "link": "http://arxiv.org/abs/2407.17265v1", "date": "2024-07-24", "relevancy": 1.34, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4709}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4479}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCIsegV2%3A%20A%20Universal%20Tool%20for%20Segmentation%20of%20Intramedullary%20Lesions%20in%0A%20%20Spinal%20Cord%20Injury&body=Title%3A%20SCIsegV2%3A%20A%20Universal%20Tool%20for%20Segmentation%20of%20Intramedullary%20Lesions%20in%0A%20%20Spinal%20Cord%20Injury%0AAuthor%3A%20Enamundram%20Naga%20Karthik%20and%20Jan%20Valo%C5%A1ek%20and%20Lynn%20Farner%20and%20Dario%20Pfyffer%20and%20Simon%20Schading-Sassenhausen%20and%20Anna%20Lebret%20and%20Gergely%20David%20and%20Andrew%20C.%20Smith%20and%20Kenneth%20A.%20Weber%20II%20and%20Maryam%20Seif%20and%20RHSCIR%20Network%20Imaging%20Group%20and%20Patrick%20Freund%20and%20Julien%20Cohen-Adad%0AAbstract%3A%20%20%20Spinal%20cord%20injury%20%28SCI%29%20is%20a%20devastating%20incidence%20leading%20to%20permanent%0Aparalysis%20and%20loss%20of%20sensory-motor%20functions%20potentially%20resulting%20in%20the%0Aformation%20of%20lesions%20within%20the%20spinal%20cord.%20Imaging%20biomarkers%20obtained%20from%0Amagnetic%20resonance%20imaging%20%28MRI%29%20scans%20can%20predict%20the%20functional%20recovery%20of%0Aindividuals%20with%20SCI%20and%20help%20choose%20the%20optimal%20treatment%20strategy.%20Currently%2C%0Amost%20studies%20employ%20manual%20quantification%20of%20these%20MRI-derived%20biomarkers%2C%0Awhich%20is%20a%20subjective%20and%20tedious%20task.%20In%20this%20work%2C%20we%20propose%20%28i%29%20a%0Auniversal%20tool%20for%20the%20automatic%20segmentation%20of%20intramedullary%20SCI%20lesions%2C%0Adubbed%20%5Ctexttt%7BSCIsegV2%7D%2C%20and%20%28ii%29%20a%20method%20to%20automatically%20compute%20the%20width%0Aof%20the%20tissue%20bridges%20from%20the%20segmented%20lesion.%20Tissue%20bridges%20represent%20the%0Aspared%20spinal%20tissue%20adjacent%20to%20the%20lesion%2C%20which%20is%20associated%20with%0Afunctional%20recovery%20in%20SCI%20patients.%20The%20tool%20was%20trained%20and%20validated%20on%20a%0Aheterogeneous%20dataset%20from%207%20sites%20comprising%20patients%20from%20different%20SCI%0Aphases%20%28acute%2C%20sub-acute%2C%20and%20chronic%29%20and%20etiologies%20%28traumatic%20SCI%2C%20ischemic%0ASCI%2C%20and%20degenerative%20cervical%20myelopathy%29.%20Tissue%20bridges%20quantified%0Aautomatically%20did%20not%20significantly%20differ%20from%20those%20computed%20manually%2C%0Asuggesting%20that%20the%20proposed%20automatic%20tool%20can%20be%20used%20to%20derive%20relevant%20MRI%0Abiomarkers.%20%5Ctexttt%7BSCIsegV2%7D%20and%20the%20automatic%20tissue%20bridges%20computation%20are%0Aopen-source%20and%20available%20in%20Spinal%20Cord%20Toolbox%20%28v6.4%20and%20above%29%20via%20the%0A%5Ctexttt%7Bsct%5C_deepseg%20-task%20seg%5C_sc%5C_lesion%5C_t2w%5C_sci%7D%20and%0A%5Ctexttt%7Bsct%5C_analyze%5C_lesion%7D%20functions%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17265v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCIsegV2%253A%2520A%2520Universal%2520Tool%2520for%2520Segmentation%2520of%2520Intramedullary%2520Lesions%2520in%250A%2520%2520Spinal%2520Cord%2520Injury%26entry.906535625%3DEnamundram%2520Naga%2520Karthik%2520and%2520Jan%2520Valo%25C5%25A1ek%2520and%2520Lynn%2520Farner%2520and%2520Dario%2520Pfyffer%2520and%2520Simon%2520Schading-Sassenhausen%2520and%2520Anna%2520Lebret%2520and%2520Gergely%2520David%2520and%2520Andrew%2520C.%2520Smith%2520and%2520Kenneth%2520A.%2520Weber%2520II%2520and%2520Maryam%2520Seif%2520and%2520RHSCIR%2520Network%2520Imaging%2520Group%2520and%2520Patrick%2520Freund%2520and%2520Julien%2520Cohen-Adad%26entry.1292438233%3D%2520%2520Spinal%2520cord%2520injury%2520%2528SCI%2529%2520is%2520a%2520devastating%2520incidence%2520leading%2520to%2520permanent%250Aparalysis%2520and%2520loss%2520of%2520sensory-motor%2520functions%2520potentially%2520resulting%2520in%2520the%250Aformation%2520of%2520lesions%2520within%2520the%2520spinal%2520cord.%2520Imaging%2520biomarkers%2520obtained%2520from%250Amagnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520scans%2520can%2520predict%2520the%2520functional%2520recovery%2520of%250Aindividuals%2520with%2520SCI%2520and%2520help%2520choose%2520the%2520optimal%2520treatment%2520strategy.%2520Currently%252C%250Amost%2520studies%2520employ%2520manual%2520quantification%2520of%2520these%2520MRI-derived%2520biomarkers%252C%250Awhich%2520is%2520a%2520subjective%2520and%2520tedious%2520task.%2520In%2520this%2520work%252C%2520we%2520propose%2520%2528i%2529%2520a%250Auniversal%2520tool%2520for%2520the%2520automatic%2520segmentation%2520of%2520intramedullary%2520SCI%2520lesions%252C%250Adubbed%2520%255Ctexttt%257BSCIsegV2%257D%252C%2520and%2520%2528ii%2529%2520a%2520method%2520to%2520automatically%2520compute%2520the%2520width%250Aof%2520the%2520tissue%2520bridges%2520from%2520the%2520segmented%2520lesion.%2520Tissue%2520bridges%2520represent%2520the%250Aspared%2520spinal%2520tissue%2520adjacent%2520to%2520the%2520lesion%252C%2520which%2520is%2520associated%2520with%250Afunctional%2520recovery%2520in%2520SCI%2520patients.%2520The%2520tool%2520was%2520trained%2520and%2520validated%2520on%2520a%250Aheterogeneous%2520dataset%2520from%25207%2520sites%2520comprising%2520patients%2520from%2520different%2520SCI%250Aphases%2520%2528acute%252C%2520sub-acute%252C%2520and%2520chronic%2529%2520and%2520etiologies%2520%2528traumatic%2520SCI%252C%2520ischemic%250ASCI%252C%2520and%2520degenerative%2520cervical%2520myelopathy%2529.%2520Tissue%2520bridges%2520quantified%250Aautomatically%2520did%2520not%2520significantly%2520differ%2520from%2520those%2520computed%2520manually%252C%250Asuggesting%2520that%2520the%2520proposed%2520automatic%2520tool%2520can%2520be%2520used%2520to%2520derive%2520relevant%2520MRI%250Abiomarkers.%2520%255Ctexttt%257BSCIsegV2%257D%2520and%2520the%2520automatic%2520tissue%2520bridges%2520computation%2520are%250Aopen-source%2520and%2520available%2520in%2520Spinal%2520Cord%2520Toolbox%2520%2528v6.4%2520and%2520above%2529%2520via%2520the%250A%255Ctexttt%257Bsct%255C_deepseg%2520-task%2520seg%255C_sc%255C_lesion%255C_t2w%255C_sci%257D%2520and%250A%255Ctexttt%257Bsct%255C_analyze%255C_lesion%257D%2520functions%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17265v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCIsegV2%3A%20A%20Universal%20Tool%20for%20Segmentation%20of%20Intramedullary%20Lesions%20in%0A%20%20Spinal%20Cord%20Injury&entry.906535625=Enamundram%20Naga%20Karthik%20and%20Jan%20Valo%C5%A1ek%20and%20Lynn%20Farner%20and%20Dario%20Pfyffer%20and%20Simon%20Schading-Sassenhausen%20and%20Anna%20Lebret%20and%20Gergely%20David%20and%20Andrew%20C.%20Smith%20and%20Kenneth%20A.%20Weber%20II%20and%20Maryam%20Seif%20and%20RHSCIR%20Network%20Imaging%20Group%20and%20Patrick%20Freund%20and%20Julien%20Cohen-Adad&entry.1292438233=%20%20Spinal%20cord%20injury%20%28SCI%29%20is%20a%20devastating%20incidence%20leading%20to%20permanent%0Aparalysis%20and%20loss%20of%20sensory-motor%20functions%20potentially%20resulting%20in%20the%0Aformation%20of%20lesions%20within%20the%20spinal%20cord.%20Imaging%20biomarkers%20obtained%20from%0Amagnetic%20resonance%20imaging%20%28MRI%29%20scans%20can%20predict%20the%20functional%20recovery%20of%0Aindividuals%20with%20SCI%20and%20help%20choose%20the%20optimal%20treatment%20strategy.%20Currently%2C%0Amost%20studies%20employ%20manual%20quantification%20of%20these%20MRI-derived%20biomarkers%2C%0Awhich%20is%20a%20subjective%20and%20tedious%20task.%20In%20this%20work%2C%20we%20propose%20%28i%29%20a%0Auniversal%20tool%20for%20the%20automatic%20segmentation%20of%20intramedullary%20SCI%20lesions%2C%0Adubbed%20%5Ctexttt%7BSCIsegV2%7D%2C%20and%20%28ii%29%20a%20method%20to%20automatically%20compute%20the%20width%0Aof%20the%20tissue%20bridges%20from%20the%20segmented%20lesion.%20Tissue%20bridges%20represent%20the%0Aspared%20spinal%20tissue%20adjacent%20to%20the%20lesion%2C%20which%20is%20associated%20with%0Afunctional%20recovery%20in%20SCI%20patients.%20The%20tool%20was%20trained%20and%20validated%20on%20a%0Aheterogeneous%20dataset%20from%207%20sites%20comprising%20patients%20from%20different%20SCI%0Aphases%20%28acute%2C%20sub-acute%2C%20and%20chronic%29%20and%20etiologies%20%28traumatic%20SCI%2C%20ischemic%0ASCI%2C%20and%20degenerative%20cervical%20myelopathy%29.%20Tissue%20bridges%20quantified%0Aautomatically%20did%20not%20significantly%20differ%20from%20those%20computed%20manually%2C%0Asuggesting%20that%20the%20proposed%20automatic%20tool%20can%20be%20used%20to%20derive%20relevant%20MRI%0Abiomarkers.%20%5Ctexttt%7BSCIsegV2%7D%20and%20the%20automatic%20tissue%20bridges%20computation%20are%0Aopen-source%20and%20available%20in%20Spinal%20Cord%20Toolbox%20%28v6.4%20and%20above%29%20via%20the%0A%5Ctexttt%7Bsct%5C_deepseg%20-task%20seg%5C_sc%5C_lesion%5C_t2w%5C_sci%7D%20and%0A%5Ctexttt%7Bsct%5C_analyze%5C_lesion%7D%20functions%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17265v1&entry.124074799=Read"},
{"title": "Enhanced SMC$^2$: Leveraging Gradient Information from Differentiable\n  Particle Filters Within Langevin Proposals", "author": "Conor Rosato and Joshua Murphy and Alessandro Varsi and Paul Horridge and Simon Maskell", "abstract": "  Sequential Monte Carlo Squared (SMC$^2$) is a Bayesian method which can infer\nthe states and parameters of non-linear, non-Gaussian state-space models. The\nstandard random-walk proposal in SMC$^2$ faces challenges, particularly with\nhigh-dimensional parameter spaces. This study outlines a novel approach by\nharnessing first-order gradients derived from a Common Random Numbers -\nParticle Filter (CRN-PF) using PyTorch. The resulting gradients can be\nleveraged within a Langevin proposal without accept/reject. Including Langevin\ndynamics within the proposal can result in a higher effective sample size and\nmore accurate parameter estimates when compared with the random-walk. The\nresulting algorithm is parallelized on distributed memory using Message Passing\nInterface (MPI) and runs in $\\mathcal{O}(\\log_2N)$ time complexity. Utilizing\n64 computational cores we obtain a 51x speed-up when compared to a single core.\nA GitHub link is given which provides access to the code.\n", "link": "http://arxiv.org/abs/2407.17296v1", "date": "2024-07-24", "relevancy": 1.0194, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5399}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5018}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20SMC%24%5E2%24%3A%20Leveraging%20Gradient%20Information%20from%20Differentiable%0A%20%20Particle%20Filters%20Within%20Langevin%20Proposals&body=Title%3A%20Enhanced%20SMC%24%5E2%24%3A%20Leveraging%20Gradient%20Information%20from%20Differentiable%0A%20%20Particle%20Filters%20Within%20Langevin%20Proposals%0AAuthor%3A%20Conor%20Rosato%20and%20Joshua%20Murphy%20and%20Alessandro%20Varsi%20and%20Paul%20Horridge%20and%20Simon%20Maskell%0AAbstract%3A%20%20%20Sequential%20Monte%20Carlo%20Squared%20%28SMC%24%5E2%24%29%20is%20a%20Bayesian%20method%20which%20can%20infer%0Athe%20states%20and%20parameters%20of%20non-linear%2C%20non-Gaussian%20state-space%20models.%20The%0Astandard%20random-walk%20proposal%20in%20SMC%24%5E2%24%20faces%20challenges%2C%20particularly%20with%0Ahigh-dimensional%20parameter%20spaces.%20This%20study%20outlines%20a%20novel%20approach%20by%0Aharnessing%20first-order%20gradients%20derived%20from%20a%20Common%20Random%20Numbers%20-%0AParticle%20Filter%20%28CRN-PF%29%20using%20PyTorch.%20The%20resulting%20gradients%20can%20be%0Aleveraged%20within%20a%20Langevin%20proposal%20without%20accept/reject.%20Including%20Langevin%0Adynamics%20within%20the%20proposal%20can%20result%20in%20a%20higher%20effective%20sample%20size%20and%0Amore%20accurate%20parameter%20estimates%20when%20compared%20with%20the%20random-walk.%20The%0Aresulting%20algorithm%20is%20parallelized%20on%20distributed%20memory%20using%20Message%20Passing%0AInterface%20%28MPI%29%20and%20runs%20in%20%24%5Cmathcal%7BO%7D%28%5Clog_2N%29%24%20time%20complexity.%20Utilizing%0A64%20computational%20cores%20we%20obtain%20a%2051x%20speed-up%20when%20compared%20to%20a%20single%20core.%0AA%20GitHub%20link%20is%20given%20which%20provides%20access%20to%20the%20code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17296v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520SMC%2524%255E2%2524%253A%2520Leveraging%2520Gradient%2520Information%2520from%2520Differentiable%250A%2520%2520Particle%2520Filters%2520Within%2520Langevin%2520Proposals%26entry.906535625%3DConor%2520Rosato%2520and%2520Joshua%2520Murphy%2520and%2520Alessandro%2520Varsi%2520and%2520Paul%2520Horridge%2520and%2520Simon%2520Maskell%26entry.1292438233%3D%2520%2520Sequential%2520Monte%2520Carlo%2520Squared%2520%2528SMC%2524%255E2%2524%2529%2520is%2520a%2520Bayesian%2520method%2520which%2520can%2520infer%250Athe%2520states%2520and%2520parameters%2520of%2520non-linear%252C%2520non-Gaussian%2520state-space%2520models.%2520The%250Astandard%2520random-walk%2520proposal%2520in%2520SMC%2524%255E2%2524%2520faces%2520challenges%252C%2520particularly%2520with%250Ahigh-dimensional%2520parameter%2520spaces.%2520This%2520study%2520outlines%2520a%2520novel%2520approach%2520by%250Aharnessing%2520first-order%2520gradients%2520derived%2520from%2520a%2520Common%2520Random%2520Numbers%2520-%250AParticle%2520Filter%2520%2528CRN-PF%2529%2520using%2520PyTorch.%2520The%2520resulting%2520gradients%2520can%2520be%250Aleveraged%2520within%2520a%2520Langevin%2520proposal%2520without%2520accept/reject.%2520Including%2520Langevin%250Adynamics%2520within%2520the%2520proposal%2520can%2520result%2520in%2520a%2520higher%2520effective%2520sample%2520size%2520and%250Amore%2520accurate%2520parameter%2520estimates%2520when%2520compared%2520with%2520the%2520random-walk.%2520The%250Aresulting%2520algorithm%2520is%2520parallelized%2520on%2520distributed%2520memory%2520using%2520Message%2520Passing%250AInterface%2520%2528MPI%2529%2520and%2520runs%2520in%2520%2524%255Cmathcal%257BO%257D%2528%255Clog_2N%2529%2524%2520time%2520complexity.%2520Utilizing%250A64%2520computational%2520cores%2520we%2520obtain%2520a%252051x%2520speed-up%2520when%2520compared%2520to%2520a%2520single%2520core.%250AA%2520GitHub%2520link%2520is%2520given%2520which%2520provides%2520access%2520to%2520the%2520code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17296v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20SMC%24%5E2%24%3A%20Leveraging%20Gradient%20Information%20from%20Differentiable%0A%20%20Particle%20Filters%20Within%20Langevin%20Proposals&entry.906535625=Conor%20Rosato%20and%20Joshua%20Murphy%20and%20Alessandro%20Varsi%20and%20Paul%20Horridge%20and%20Simon%20Maskell&entry.1292438233=%20%20Sequential%20Monte%20Carlo%20Squared%20%28SMC%24%5E2%24%29%20is%20a%20Bayesian%20method%20which%20can%20infer%0Athe%20states%20and%20parameters%20of%20non-linear%2C%20non-Gaussian%20state-space%20models.%20The%0Astandard%20random-walk%20proposal%20in%20SMC%24%5E2%24%20faces%20challenges%2C%20particularly%20with%0Ahigh-dimensional%20parameter%20spaces.%20This%20study%20outlines%20a%20novel%20approach%20by%0Aharnessing%20first-order%20gradients%20derived%20from%20a%20Common%20Random%20Numbers%20-%0AParticle%20Filter%20%28CRN-PF%29%20using%20PyTorch.%20The%20resulting%20gradients%20can%20be%0Aleveraged%20within%20a%20Langevin%20proposal%20without%20accept/reject.%20Including%20Langevin%0Adynamics%20within%20the%20proposal%20can%20result%20in%20a%20higher%20effective%20sample%20size%20and%0Amore%20accurate%20parameter%20estimates%20when%20compared%20with%20the%20random-walk.%20The%0Aresulting%20algorithm%20is%20parallelized%20on%20distributed%20memory%20using%20Message%20Passing%0AInterface%20%28MPI%29%20and%20runs%20in%20%24%5Cmathcal%7BO%7D%28%5Clog_2N%29%24%20time%20complexity.%20Utilizing%0A64%20computational%20cores%20we%20obtain%20a%2051x%20speed-up%20when%20compared%20to%20a%20single%20core.%0AA%20GitHub%20link%20is%20given%20which%20provides%20access%20to%20the%20code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17296v1&entry.124074799=Read"},
{"title": "Take a Step and Reconsider: Sequence Decoding for Self-Improved Neural\n  Combinatorial Optimization", "author": "Jonathan Pirnay and Dominik G. Grimm", "abstract": "  The constructive approach within Neural Combinatorial Optimization (NCO)\ntreats a combinatorial optimization problem as a finite Markov decision\nprocess, where solutions are built incrementally through a sequence of\ndecisions guided by a neural policy network. To train the policy, recent\nresearch is shifting toward a 'self-improved' learning methodology that\naddresses the limitations of reinforcement learning and supervised approaches.\nHere, the policy is iteratively trained in a supervised manner, with solutions\nderived from the current policy serving as pseudo-labels. The way these\nsolutions are obtained from the policy determines the quality of the\npseudo-labels. In this paper, we present a simple and problem-independent\nsequence decoding method for self-improved learning based on sampling sequences\nwithout replacement. We incrementally follow the best solution found and repeat\nthe sampling process from intermediate partial solutions. By modifying the\npolicy to ignore previously sampled sequences, we force it to consider only\nunseen alternatives, thereby increasing solution diversity. Experimental\nresults for the Traveling Salesman and Capacitated Vehicle Routing Problem\ndemonstrate its strong performance. Furthermore, our method outperforms\nprevious NCO approaches on the Job Shop Scheduling Problem.\n", "link": "http://arxiv.org/abs/2407.17206v1", "date": "2024-07-24", "relevancy": 1.455, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4877}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4876}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Take%20a%20Step%20and%20Reconsider%3A%20Sequence%20Decoding%20for%20Self-Improved%20Neural%0A%20%20Combinatorial%20Optimization&body=Title%3A%20Take%20a%20Step%20and%20Reconsider%3A%20Sequence%20Decoding%20for%20Self-Improved%20Neural%0A%20%20Combinatorial%20Optimization%0AAuthor%3A%20Jonathan%20Pirnay%20and%20Dominik%20G.%20Grimm%0AAbstract%3A%20%20%20The%20constructive%20approach%20within%20Neural%20Combinatorial%20Optimization%20%28NCO%29%0Atreats%20a%20combinatorial%20optimization%20problem%20as%20a%20finite%20Markov%20decision%0Aprocess%2C%20where%20solutions%20are%20built%20incrementally%20through%20a%20sequence%20of%0Adecisions%20guided%20by%20a%20neural%20policy%20network.%20To%20train%20the%20policy%2C%20recent%0Aresearch%20is%20shifting%20toward%20a%20%27self-improved%27%20learning%20methodology%20that%0Aaddresses%20the%20limitations%20of%20reinforcement%20learning%20and%20supervised%20approaches.%0AHere%2C%20the%20policy%20is%20iteratively%20trained%20in%20a%20supervised%20manner%2C%20with%20solutions%0Aderived%20from%20the%20current%20policy%20serving%20as%20pseudo-labels.%20The%20way%20these%0Asolutions%20are%20obtained%20from%20the%20policy%20determines%20the%20quality%20of%20the%0Apseudo-labels.%20In%20this%20paper%2C%20we%20present%20a%20simple%20and%20problem-independent%0Asequence%20decoding%20method%20for%20self-improved%20learning%20based%20on%20sampling%20sequences%0Awithout%20replacement.%20We%20incrementally%20follow%20the%20best%20solution%20found%20and%20repeat%0Athe%20sampling%20process%20from%20intermediate%20partial%20solutions.%20By%20modifying%20the%0Apolicy%20to%20ignore%20previously%20sampled%20sequences%2C%20we%20force%20it%20to%20consider%20only%0Aunseen%20alternatives%2C%20thereby%20increasing%20solution%20diversity.%20Experimental%0Aresults%20for%20the%20Traveling%20Salesman%20and%20Capacitated%20Vehicle%20Routing%20Problem%0Ademonstrate%20its%20strong%20performance.%20Furthermore%2C%20our%20method%20outperforms%0Aprevious%20NCO%20approaches%20on%20the%20Job%20Shop%20Scheduling%20Problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17206v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTake%2520a%2520Step%2520and%2520Reconsider%253A%2520Sequence%2520Decoding%2520for%2520Self-Improved%2520Neural%250A%2520%2520Combinatorial%2520Optimization%26entry.906535625%3DJonathan%2520Pirnay%2520and%2520Dominik%2520G.%2520Grimm%26entry.1292438233%3D%2520%2520The%2520constructive%2520approach%2520within%2520Neural%2520Combinatorial%2520Optimization%2520%2528NCO%2529%250Atreats%2520a%2520combinatorial%2520optimization%2520problem%2520as%2520a%2520finite%2520Markov%2520decision%250Aprocess%252C%2520where%2520solutions%2520are%2520built%2520incrementally%2520through%2520a%2520sequence%2520of%250Adecisions%2520guided%2520by%2520a%2520neural%2520policy%2520network.%2520To%2520train%2520the%2520policy%252C%2520recent%250Aresearch%2520is%2520shifting%2520toward%2520a%2520%2527self-improved%2527%2520learning%2520methodology%2520that%250Aaddresses%2520the%2520limitations%2520of%2520reinforcement%2520learning%2520and%2520supervised%2520approaches.%250AHere%252C%2520the%2520policy%2520is%2520iteratively%2520trained%2520in%2520a%2520supervised%2520manner%252C%2520with%2520solutions%250Aderived%2520from%2520the%2520current%2520policy%2520serving%2520as%2520pseudo-labels.%2520The%2520way%2520these%250Asolutions%2520are%2520obtained%2520from%2520the%2520policy%2520determines%2520the%2520quality%2520of%2520the%250Apseudo-labels.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520simple%2520and%2520problem-independent%250Asequence%2520decoding%2520method%2520for%2520self-improved%2520learning%2520based%2520on%2520sampling%2520sequences%250Awithout%2520replacement.%2520We%2520incrementally%2520follow%2520the%2520best%2520solution%2520found%2520and%2520repeat%250Athe%2520sampling%2520process%2520from%2520intermediate%2520partial%2520solutions.%2520By%2520modifying%2520the%250Apolicy%2520to%2520ignore%2520previously%2520sampled%2520sequences%252C%2520we%2520force%2520it%2520to%2520consider%2520only%250Aunseen%2520alternatives%252C%2520thereby%2520increasing%2520solution%2520diversity.%2520Experimental%250Aresults%2520for%2520the%2520Traveling%2520Salesman%2520and%2520Capacitated%2520Vehicle%2520Routing%2520Problem%250Ademonstrate%2520its%2520strong%2520performance.%2520Furthermore%252C%2520our%2520method%2520outperforms%250Aprevious%2520NCO%2520approaches%2520on%2520the%2520Job%2520Shop%2520Scheduling%2520Problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17206v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Take%20a%20Step%20and%20Reconsider%3A%20Sequence%20Decoding%20for%20Self-Improved%20Neural%0A%20%20Combinatorial%20Optimization&entry.906535625=Jonathan%20Pirnay%20and%20Dominik%20G.%20Grimm&entry.1292438233=%20%20The%20constructive%20approach%20within%20Neural%20Combinatorial%20Optimization%20%28NCO%29%0Atreats%20a%20combinatorial%20optimization%20problem%20as%20a%20finite%20Markov%20decision%0Aprocess%2C%20where%20solutions%20are%20built%20incrementally%20through%20a%20sequence%20of%0Adecisions%20guided%20by%20a%20neural%20policy%20network.%20To%20train%20the%20policy%2C%20recent%0Aresearch%20is%20shifting%20toward%20a%20%27self-improved%27%20learning%20methodology%20that%0Aaddresses%20the%20limitations%20of%20reinforcement%20learning%20and%20supervised%20approaches.%0AHere%2C%20the%20policy%20is%20iteratively%20trained%20in%20a%20supervised%20manner%2C%20with%20solutions%0Aderived%20from%20the%20current%20policy%20serving%20as%20pseudo-labels.%20The%20way%20these%0Asolutions%20are%20obtained%20from%20the%20policy%20determines%20the%20quality%20of%20the%0Apseudo-labels.%20In%20this%20paper%2C%20we%20present%20a%20simple%20and%20problem-independent%0Asequence%20decoding%20method%20for%20self-improved%20learning%20based%20on%20sampling%20sequences%0Awithout%20replacement.%20We%20incrementally%20follow%20the%20best%20solution%20found%20and%20repeat%0Athe%20sampling%20process%20from%20intermediate%20partial%20solutions.%20By%20modifying%20the%0Apolicy%20to%20ignore%20previously%20sampled%20sequences%2C%20we%20force%20it%20to%20consider%20only%0Aunseen%20alternatives%2C%20thereby%20increasing%20solution%20diversity.%20Experimental%0Aresults%20for%20the%20Traveling%20Salesman%20and%20Capacitated%20Vehicle%20Routing%20Problem%0Ademonstrate%20its%20strong%20performance.%20Furthermore%2C%20our%20method%20outperforms%0Aprevious%20NCO%20approaches%20on%20the%20Job%20Shop%20Scheduling%20Problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17206v1&entry.124074799=Read"},
{"title": "CycleMix: Mixing Source Domains for Domain Generalization in\n  Style-Dependent Data", "author": "Aristotelis Ballas and Christos Diou", "abstract": "  As deep learning-based systems have become an integral part of everyday life,\nlimitations in their generalization ability have begun to emerge. Machine\nlearning algorithms typically rely on the i.i.d. assumption, meaning that their\ntraining and validation data are expected to follow the same distribution,\nwhich does not necessarily hold in practice. In the case of image\nclassification, one frequent reason that algorithms fail to generalize is that\nthey rely on spurious correlations present in training data, such as\nassociating image styles with target classes. These associations may not be\npresent in the unseen test data, leading to significant degradation of their\neffectiveness. In this work, we attempt to mitigate this Domain Generalization\n(DG) problem by training a robust feature extractor which disregards features\nattributed to image-style but infers based on style-invariant image\nrepresentations. To achieve this, we train CycleGAN models to learn the\ndifferent styles present in the training data and randomly mix them together to\ncreate samples with novel style attributes to improve generalization.\nExperimental results on the PACS DG benchmark validate the proposed method.\n", "link": "http://arxiv.org/abs/2407.13421v2", "date": "2024-07-24", "relevancy": 1.5561, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5218}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5186}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CycleMix%3A%20Mixing%20Source%20Domains%20for%20Domain%20Generalization%20in%0A%20%20Style-Dependent%20Data&body=Title%3A%20CycleMix%3A%20Mixing%20Source%20Domains%20for%20Domain%20Generalization%20in%0A%20%20Style-Dependent%20Data%0AAuthor%3A%20Aristotelis%20Ballas%20and%20Christos%20Diou%0AAbstract%3A%20%20%20As%20deep%20learning-based%20systems%20have%20become%20an%20integral%20part%20of%20everyday%20life%2C%0Alimitations%20in%20their%20generalization%20ability%20have%20begun%20to%20emerge.%20Machine%0Alearning%20algorithms%20typically%20rely%20on%20the%20i.i.d.%20assumption%2C%20meaning%20that%20their%0Atraining%20and%20validation%20data%20are%20expected%20to%20follow%20the%20same%20distribution%2C%0Awhich%20does%20not%20necessarily%20hold%20in%20practice.%20In%20the%20case%20of%20image%0Aclassification%2C%20one%20frequent%20reason%20that%20algorithms%20fail%20to%20generalize%20is%20that%0Athey%20rely%20on%20spurious%20correlations%20present%20in%20training%20data%2C%20such%20as%0Aassociating%20image%20styles%20with%20target%20classes.%20These%20associations%20may%20not%20be%0Apresent%20in%20the%20unseen%20test%20data%2C%20leading%20to%20significant%20degradation%20of%20their%0Aeffectiveness.%20In%20this%20work%2C%20we%20attempt%20to%20mitigate%20this%20Domain%20Generalization%0A%28DG%29%20problem%20by%20training%20a%20robust%20feature%20extractor%20which%20disregards%20features%0Aattributed%20to%20image-style%20but%20infers%20based%20on%20style-invariant%20image%0Arepresentations.%20To%20achieve%20this%2C%20we%20train%20CycleGAN%20models%20to%20learn%20the%0Adifferent%20styles%20present%20in%20the%20training%20data%20and%20randomly%20mix%20them%20together%20to%0Acreate%20samples%20with%20novel%20style%20attributes%20to%20improve%20generalization.%0AExperimental%20results%20on%20the%20PACS%20DG%20benchmark%20validate%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13421v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCycleMix%253A%2520Mixing%2520Source%2520Domains%2520for%2520Domain%2520Generalization%2520in%250A%2520%2520Style-Dependent%2520Data%26entry.906535625%3DAristotelis%2520Ballas%2520and%2520Christos%2520Diou%26entry.1292438233%3D%2520%2520As%2520deep%2520learning-based%2520systems%2520have%2520become%2520an%2520integral%2520part%2520of%2520everyday%2520life%252C%250Alimitations%2520in%2520their%2520generalization%2520ability%2520have%2520begun%2520to%2520emerge.%2520Machine%250Alearning%2520algorithms%2520typically%2520rely%2520on%2520the%2520i.i.d.%2520assumption%252C%2520meaning%2520that%2520their%250Atraining%2520and%2520validation%2520data%2520are%2520expected%2520to%2520follow%2520the%2520same%2520distribution%252C%250Awhich%2520does%2520not%2520necessarily%2520hold%2520in%2520practice.%2520In%2520the%2520case%2520of%2520image%250Aclassification%252C%2520one%2520frequent%2520reason%2520that%2520algorithms%2520fail%2520to%2520generalize%2520is%2520that%250Athey%2520rely%2520on%2520spurious%2520correlations%2520present%2520in%2520training%2520data%252C%2520such%2520as%250Aassociating%2520image%2520styles%2520with%2520target%2520classes.%2520These%2520associations%2520may%2520not%2520be%250Apresent%2520in%2520the%2520unseen%2520test%2520data%252C%2520leading%2520to%2520significant%2520degradation%2520of%2520their%250Aeffectiveness.%2520In%2520this%2520work%252C%2520we%2520attempt%2520to%2520mitigate%2520this%2520Domain%2520Generalization%250A%2528DG%2529%2520problem%2520by%2520training%2520a%2520robust%2520feature%2520extractor%2520which%2520disregards%2520features%250Aattributed%2520to%2520image-style%2520but%2520infers%2520based%2520on%2520style-invariant%2520image%250Arepresentations.%2520To%2520achieve%2520this%252C%2520we%2520train%2520CycleGAN%2520models%2520to%2520learn%2520the%250Adifferent%2520styles%2520present%2520in%2520the%2520training%2520data%2520and%2520randomly%2520mix%2520them%2520together%2520to%250Acreate%2520samples%2520with%2520novel%2520style%2520attributes%2520to%2520improve%2520generalization.%250AExperimental%2520results%2520on%2520the%2520PACS%2520DG%2520benchmark%2520validate%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13421v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CycleMix%3A%20Mixing%20Source%20Domains%20for%20Domain%20Generalization%20in%0A%20%20Style-Dependent%20Data&entry.906535625=Aristotelis%20Ballas%20and%20Christos%20Diou&entry.1292438233=%20%20As%20deep%20learning-based%20systems%20have%20become%20an%20integral%20part%20of%20everyday%20life%2C%0Alimitations%20in%20their%20generalization%20ability%20have%20begun%20to%20emerge.%20Machine%0Alearning%20algorithms%20typically%20rely%20on%20the%20i.i.d.%20assumption%2C%20meaning%20that%20their%0Atraining%20and%20validation%20data%20are%20expected%20to%20follow%20the%20same%20distribution%2C%0Awhich%20does%20not%20necessarily%20hold%20in%20practice.%20In%20the%20case%20of%20image%0Aclassification%2C%20one%20frequent%20reason%20that%20algorithms%20fail%20to%20generalize%20is%20that%0Athey%20rely%20on%20spurious%20correlations%20present%20in%20training%20data%2C%20such%20as%0Aassociating%20image%20styles%20with%20target%20classes.%20These%20associations%20may%20not%20be%0Apresent%20in%20the%20unseen%20test%20data%2C%20leading%20to%20significant%20degradation%20of%20their%0Aeffectiveness.%20In%20this%20work%2C%20we%20attempt%20to%20mitigate%20this%20Domain%20Generalization%0A%28DG%29%20problem%20by%20training%20a%20robust%20feature%20extractor%20which%20disregards%20features%0Aattributed%20to%20image-style%20but%20infers%20based%20on%20style-invariant%20image%0Arepresentations.%20To%20achieve%20this%2C%20we%20train%20CycleGAN%20models%20to%20learn%20the%0Adifferent%20styles%20present%20in%20the%20training%20data%20and%20randomly%20mix%20them%20together%20to%0Acreate%20samples%20with%20novel%20style%20attributes%20to%20improve%20generalization.%0AExperimental%20results%20on%20the%20PACS%20DG%20benchmark%20validate%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13421v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


