<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240721.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GSD: View-Guided Gaussian Splatting Diffusion for 3D Reconstruction", "author": "Yuxuan Mu and Xinxin Zuo and Chuan Guo and Yilin Wang and Juwei Lu and Xiaofeng Wu and Songcen Xu and Peng Dai and Youliang Yan and Li Cheng", "abstract": "  We present GSD, a diffusion model approach based on Gaussian Splatting (GS)\nrepresentation for 3D object reconstruction from a single view. Prior works\nsuffer from inconsistent 3D geometry or mediocre rendering quality due to\nimproper representations. We take a step towards resolving these shortcomings\nby utilizing the recent state-of-the-art 3D explicit representation, Gaussian\nSplatting, and an unconditional diffusion model. This model learns to generate\n3D objects represented by sets of GS ellipsoids. With these strong generative\n3D priors, though learning unconditionally, the diffusion model is ready for\nview-guided reconstruction without further model fine-tuning. This is achieved\nby propagating fine-grained 2D features through the efficient yet flexible\nsplatting function and the guided denoising sampling process. In addition, a 2D\ndiffusion model is further employed to enhance rendering fidelity, and improve\nreconstructed GS quality by polishing and re-using the rendered images. The\nfinal reconstructed objects explicitly come with high-quality 3D structure and\ntexture, and can be efficiently rendered in arbitrary views. Experiments on the\nchallenging real-world CO3D dataset demonstrate the superiority of our\napproach. Project page: $\\href{https://yxmu.foo/GSD/}{\\text{this https URL}}$\n", "link": "http://arxiv.org/abs/2407.04237v3", "date": "2024-07-19", "relevancy": 3.4621, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7179}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6797}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GSD%3A%20View-Guided%20Gaussian%20Splatting%20Diffusion%20for%203D%20Reconstruction&body=Title%3A%20GSD%3A%20View-Guided%20Gaussian%20Splatting%20Diffusion%20for%203D%20Reconstruction%0AAuthor%3A%20Yuxuan%20Mu%20and%20Xinxin%20Zuo%20and%20Chuan%20Guo%20and%20Yilin%20Wang%20and%20Juwei%20Lu%20and%20Xiaofeng%20Wu%20and%20Songcen%20Xu%20and%20Peng%20Dai%20and%20Youliang%20Yan%20and%20Li%20Cheng%0AAbstract%3A%20%20%20We%20present%20GSD%2C%20a%20diffusion%20model%20approach%20based%20on%20Gaussian%20Splatting%20%28GS%29%0Arepresentation%20for%203D%20object%20reconstruction%20from%20a%20single%20view.%20Prior%20works%0Asuffer%20from%20inconsistent%203D%20geometry%20or%20mediocre%20rendering%20quality%20due%20to%0Aimproper%20representations.%20We%20take%20a%20step%20towards%20resolving%20these%20shortcomings%0Aby%20utilizing%20the%20recent%20state-of-the-art%203D%20explicit%20representation%2C%20Gaussian%0ASplatting%2C%20and%20an%20unconditional%20diffusion%20model.%20This%20model%20learns%20to%20generate%0A3D%20objects%20represented%20by%20sets%20of%20GS%20ellipsoids.%20With%20these%20strong%20generative%0A3D%20priors%2C%20though%20learning%20unconditionally%2C%20the%20diffusion%20model%20is%20ready%20for%0Aview-guided%20reconstruction%20without%20further%20model%20fine-tuning.%20This%20is%20achieved%0Aby%20propagating%20fine-grained%202D%20features%20through%20the%20efficient%20yet%20flexible%0Asplatting%20function%20and%20the%20guided%20denoising%20sampling%20process.%20In%20addition%2C%20a%202D%0Adiffusion%20model%20is%20further%20employed%20to%20enhance%20rendering%20fidelity%2C%20and%20improve%0Areconstructed%20GS%20quality%20by%20polishing%20and%20re-using%20the%20rendered%20images.%20The%0Afinal%20reconstructed%20objects%20explicitly%20come%20with%20high-quality%203D%20structure%20and%0Atexture%2C%20and%20can%20be%20efficiently%20rendered%20in%20arbitrary%20views.%20Experiments%20on%20the%0Achallenging%20real-world%20CO3D%20dataset%20demonstrate%20the%20superiority%20of%20our%0Aapproach.%20Project%20page%3A%20%24%5Chref%7Bhttps%3A//yxmu.foo/GSD/%7D%7B%5Ctext%7Bthis%20https%20URL%7D%7D%24%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04237v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGSD%253A%2520View-Guided%2520Gaussian%2520Splatting%2520Diffusion%2520for%25203D%2520Reconstruction%26entry.906535625%3DYuxuan%2520Mu%2520and%2520Xinxin%2520Zuo%2520and%2520Chuan%2520Guo%2520and%2520Yilin%2520Wang%2520and%2520Juwei%2520Lu%2520and%2520Xiaofeng%2520Wu%2520and%2520Songcen%2520Xu%2520and%2520Peng%2520Dai%2520and%2520Youliang%2520Yan%2520and%2520Li%2520Cheng%26entry.1292438233%3D%2520%2520We%2520present%2520GSD%252C%2520a%2520diffusion%2520model%2520approach%2520based%2520on%2520Gaussian%2520Splatting%2520%2528GS%2529%250Arepresentation%2520for%25203D%2520object%2520reconstruction%2520from%2520a%2520single%2520view.%2520Prior%2520works%250Asuffer%2520from%2520inconsistent%25203D%2520geometry%2520or%2520mediocre%2520rendering%2520quality%2520due%2520to%250Aimproper%2520representations.%2520We%2520take%2520a%2520step%2520towards%2520resolving%2520these%2520shortcomings%250Aby%2520utilizing%2520the%2520recent%2520state-of-the-art%25203D%2520explicit%2520representation%252C%2520Gaussian%250ASplatting%252C%2520and%2520an%2520unconditional%2520diffusion%2520model.%2520This%2520model%2520learns%2520to%2520generate%250A3D%2520objects%2520represented%2520by%2520sets%2520of%2520GS%2520ellipsoids.%2520With%2520these%2520strong%2520generative%250A3D%2520priors%252C%2520though%2520learning%2520unconditionally%252C%2520the%2520diffusion%2520model%2520is%2520ready%2520for%250Aview-guided%2520reconstruction%2520without%2520further%2520model%2520fine-tuning.%2520This%2520is%2520achieved%250Aby%2520propagating%2520fine-grained%25202D%2520features%2520through%2520the%2520efficient%2520yet%2520flexible%250Asplatting%2520function%2520and%2520the%2520guided%2520denoising%2520sampling%2520process.%2520In%2520addition%252C%2520a%25202D%250Adiffusion%2520model%2520is%2520further%2520employed%2520to%2520enhance%2520rendering%2520fidelity%252C%2520and%2520improve%250Areconstructed%2520GS%2520quality%2520by%2520polishing%2520and%2520re-using%2520the%2520rendered%2520images.%2520The%250Afinal%2520reconstructed%2520objects%2520explicitly%2520come%2520with%2520high-quality%25203D%2520structure%2520and%250Atexture%252C%2520and%2520can%2520be%2520efficiently%2520rendered%2520in%2520arbitrary%2520views.%2520Experiments%2520on%2520the%250Achallenging%2520real-world%2520CO3D%2520dataset%2520demonstrate%2520the%2520superiority%2520of%2520our%250Aapproach.%2520Project%2520page%253A%2520%2524%255Chref%257Bhttps%253A//yxmu.foo/GSD/%257D%257B%255Ctext%257Bthis%2520https%2520URL%257D%257D%2524%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04237v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GSD%3A%20View-Guided%20Gaussian%20Splatting%20Diffusion%20for%203D%20Reconstruction&entry.906535625=Yuxuan%20Mu%20and%20Xinxin%20Zuo%20and%20Chuan%20Guo%20and%20Yilin%20Wang%20and%20Juwei%20Lu%20and%20Xiaofeng%20Wu%20and%20Songcen%20Xu%20and%20Peng%20Dai%20and%20Youliang%20Yan%20and%20Li%20Cheng&entry.1292438233=%20%20We%20present%20GSD%2C%20a%20diffusion%20model%20approach%20based%20on%20Gaussian%20Splatting%20%28GS%29%0Arepresentation%20for%203D%20object%20reconstruction%20from%20a%20single%20view.%20Prior%20works%0Asuffer%20from%20inconsistent%203D%20geometry%20or%20mediocre%20rendering%20quality%20due%20to%0Aimproper%20representations.%20We%20take%20a%20step%20towards%20resolving%20these%20shortcomings%0Aby%20utilizing%20the%20recent%20state-of-the-art%203D%20explicit%20representation%2C%20Gaussian%0ASplatting%2C%20and%20an%20unconditional%20diffusion%20model.%20This%20model%20learns%20to%20generate%0A3D%20objects%20represented%20by%20sets%20of%20GS%20ellipsoids.%20With%20these%20strong%20generative%0A3D%20priors%2C%20though%20learning%20unconditionally%2C%20the%20diffusion%20model%20is%20ready%20for%0Aview-guided%20reconstruction%20without%20further%20model%20fine-tuning.%20This%20is%20achieved%0Aby%20propagating%20fine-grained%202D%20features%20through%20the%20efficient%20yet%20flexible%0Asplatting%20function%20and%20the%20guided%20denoising%20sampling%20process.%20In%20addition%2C%20a%202D%0Adiffusion%20model%20is%20further%20employed%20to%20enhance%20rendering%20fidelity%2C%20and%20improve%0Areconstructed%20GS%20quality%20by%20polishing%20and%20re-using%20the%20rendered%20images.%20The%0Afinal%20reconstructed%20objects%20explicitly%20come%20with%20high-quality%203D%20structure%20and%0Atexture%2C%20and%20can%20be%20efficiently%20rendered%20in%20arbitrary%20views.%20Experiments%20on%20the%0Achallenging%20real-world%20CO3D%20dataset%20demonstrate%20the%20superiority%20of%20our%0Aapproach.%20Project%20page%3A%20%24%5Chref%7Bhttps%3A//yxmu.foo/GSD/%7D%7B%5Ctext%7Bthis%20https%20URL%7D%7D%24%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04237v3&entry.124074799=Read"},
{"title": "Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D\n  Gaussians", "author": "Licheng Zhong and Hong-Xing Yu and Jiajun Wu and Yunzhu Li", "abstract": "  Reconstructing and simulating elastic objects from visual observations is\ncrucial for applications in computer vision and robotics. Existing methods,\nsuch as 3D Gaussians, model 3D appearance and geometry, but lack the ability to\nestimate physical properties for objects and simulate them. The core challenge\nlies in integrating an expressive yet efficient physical dynamics model. We\npropose Spring-Gaus, a 3D physical object representation for reconstructing and\nsimulating elastic objects from videos of the object from multiple viewpoints.\nIn particular, we develop and integrate a 3D Spring-Mass model into 3D Gaussian\nkernels, enabling the reconstruction of the visual appearance, shape, and\nphysical dynamics of the object. Our approach enables future prediction and\nsimulation under various initial states and environmental properties. We\nevaluate Spring-Gaus on both synthetic and real-world datasets, demonstrating\naccurate reconstruction and simulation of elastic objects. Project page:\nhttps://zlicheng.com/spring_gaus/.\n", "link": "http://arxiv.org/abs/2403.09434v3", "date": "2024-07-19", "relevancy": 3.0591, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6683}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6089}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reconstruction%20and%20Simulation%20of%20Elastic%20Objects%20with%20Spring-Mass%203D%0A%20%20Gaussians&body=Title%3A%20Reconstruction%20and%20Simulation%20of%20Elastic%20Objects%20with%20Spring-Mass%203D%0A%20%20Gaussians%0AAuthor%3A%20Licheng%20Zhong%20and%20Hong-Xing%20Yu%20and%20Jiajun%20Wu%20and%20Yunzhu%20Li%0AAbstract%3A%20%20%20Reconstructing%20and%20simulating%20elastic%20objects%20from%20visual%20observations%20is%0Acrucial%20for%20applications%20in%20computer%20vision%20and%20robotics.%20Existing%20methods%2C%0Asuch%20as%203D%20Gaussians%2C%20model%203D%20appearance%20and%20geometry%2C%20but%20lack%20the%20ability%20to%0Aestimate%20physical%20properties%20for%20objects%20and%20simulate%20them.%20The%20core%20challenge%0Alies%20in%20integrating%20an%20expressive%20yet%20efficient%20physical%20dynamics%20model.%20We%0Apropose%20Spring-Gaus%2C%20a%203D%20physical%20object%20representation%20for%20reconstructing%20and%0Asimulating%20elastic%20objects%20from%20videos%20of%20the%20object%20from%20multiple%20viewpoints.%0AIn%20particular%2C%20we%20develop%20and%20integrate%20a%203D%20Spring-Mass%20model%20into%203D%20Gaussian%0Akernels%2C%20enabling%20the%20reconstruction%20of%20the%20visual%20appearance%2C%20shape%2C%20and%0Aphysical%20dynamics%20of%20the%20object.%20Our%20approach%20enables%20future%20prediction%20and%0Asimulation%20under%20various%20initial%20states%20and%20environmental%20properties.%20We%0Aevaluate%20Spring-Gaus%20on%20both%20synthetic%20and%20real-world%20datasets%2C%20demonstrating%0Aaccurate%20reconstruction%20and%20simulation%20of%20elastic%20objects.%20Project%20page%3A%0Ahttps%3A//zlicheng.com/spring_gaus/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09434v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReconstruction%2520and%2520Simulation%2520of%2520Elastic%2520Objects%2520with%2520Spring-Mass%25203D%250A%2520%2520Gaussians%26entry.906535625%3DLicheng%2520Zhong%2520and%2520Hong-Xing%2520Yu%2520and%2520Jiajun%2520Wu%2520and%2520Yunzhu%2520Li%26entry.1292438233%3D%2520%2520Reconstructing%2520and%2520simulating%2520elastic%2520objects%2520from%2520visual%2520observations%2520is%250Acrucial%2520for%2520applications%2520in%2520computer%2520vision%2520and%2520robotics.%2520Existing%2520methods%252C%250Asuch%2520as%25203D%2520Gaussians%252C%2520model%25203D%2520appearance%2520and%2520geometry%252C%2520but%2520lack%2520the%2520ability%2520to%250Aestimate%2520physical%2520properties%2520for%2520objects%2520and%2520simulate%2520them.%2520The%2520core%2520challenge%250Alies%2520in%2520integrating%2520an%2520expressive%2520yet%2520efficient%2520physical%2520dynamics%2520model.%2520We%250Apropose%2520Spring-Gaus%252C%2520a%25203D%2520physical%2520object%2520representation%2520for%2520reconstructing%2520and%250Asimulating%2520elastic%2520objects%2520from%2520videos%2520of%2520the%2520object%2520from%2520multiple%2520viewpoints.%250AIn%2520particular%252C%2520we%2520develop%2520and%2520integrate%2520a%25203D%2520Spring-Mass%2520model%2520into%25203D%2520Gaussian%250Akernels%252C%2520enabling%2520the%2520reconstruction%2520of%2520the%2520visual%2520appearance%252C%2520shape%252C%2520and%250Aphysical%2520dynamics%2520of%2520the%2520object.%2520Our%2520approach%2520enables%2520future%2520prediction%2520and%250Asimulation%2520under%2520various%2520initial%2520states%2520and%2520environmental%2520properties.%2520We%250Aevaluate%2520Spring-Gaus%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%252C%2520demonstrating%250Aaccurate%2520reconstruction%2520and%2520simulation%2520of%2520elastic%2520objects.%2520Project%2520page%253A%250Ahttps%253A//zlicheng.com/spring_gaus/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09434v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reconstruction%20and%20Simulation%20of%20Elastic%20Objects%20with%20Spring-Mass%203D%0A%20%20Gaussians&entry.906535625=Licheng%20Zhong%20and%20Hong-Xing%20Yu%20and%20Jiajun%20Wu%20and%20Yunzhu%20Li&entry.1292438233=%20%20Reconstructing%20and%20simulating%20elastic%20objects%20from%20visual%20observations%20is%0Acrucial%20for%20applications%20in%20computer%20vision%20and%20robotics.%20Existing%20methods%2C%0Asuch%20as%203D%20Gaussians%2C%20model%203D%20appearance%20and%20geometry%2C%20but%20lack%20the%20ability%20to%0Aestimate%20physical%20properties%20for%20objects%20and%20simulate%20them.%20The%20core%20challenge%0Alies%20in%20integrating%20an%20expressive%20yet%20efficient%20physical%20dynamics%20model.%20We%0Apropose%20Spring-Gaus%2C%20a%203D%20physical%20object%20representation%20for%20reconstructing%20and%0Asimulating%20elastic%20objects%20from%20videos%20of%20the%20object%20from%20multiple%20viewpoints.%0AIn%20particular%2C%20we%20develop%20and%20integrate%20a%203D%20Spring-Mass%20model%20into%203D%20Gaussian%0Akernels%2C%20enabling%20the%20reconstruction%20of%20the%20visual%20appearance%2C%20shape%2C%20and%0Aphysical%20dynamics%20of%20the%20object.%20Our%20approach%20enables%20future%20prediction%20and%0Asimulation%20under%20various%20initial%20states%20and%20environmental%20properties.%20We%0Aevaluate%20Spring-Gaus%20on%20both%20synthetic%20and%20real-world%20datasets%2C%20demonstrating%0Aaccurate%20reconstruction%20and%20simulation%20of%20elastic%20objects.%20Project%20page%3A%0Ahttps%3A//zlicheng.com/spring_gaus/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09434v3&entry.124074799=Read"},
{"title": "OpenSU3D: Open World 3D Scene Understanding using Foundation Models", "author": "Rafay Mohiuddin and Sai Manoj Prakhya and Fiona Collins and Ziyuan Liu and Andr\u00e9 Borrmann", "abstract": "  In this paper, we present a novel, scalable approach for constructing open\nset, instance-level 3D scene representations, advancing open world\nunderstanding of 3D environments. Existing methods require pre-constructed 3D\nscenes and face scalability issues due to per-point feature vector learning,\nlimiting their efficacy with complex queries. Our method overcomes these\nlimitations by incrementally building instance-level 3D scene representations\nusing 2D foundation models, efficiently aggregating instance-level details such\nas masks, feature vectors, names, and captions. We introduce fusion schemes for\nfeature vectors to enhance their contextual knowledge and performance on\ncomplex queries. Additionally, we explore large language models for robust\nautomatic annotation and spatial reasoning tasks. We evaluate our proposed\napproach on multiple scenes from ScanNet and Replica datasets demonstrating\nzero-shot generalization capabilities, exceeding current state-of-the-art\nmethods in open world 3D scene understanding.\n", "link": "http://arxiv.org/abs/2407.14279v1", "date": "2024-07-19", "relevancy": 3.0245, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6143}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6143}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenSU3D%3A%20Open%20World%203D%20Scene%20Understanding%20using%20Foundation%20Models&body=Title%3A%20OpenSU3D%3A%20Open%20World%203D%20Scene%20Understanding%20using%20Foundation%20Models%0AAuthor%3A%20Rafay%20Mohiuddin%20and%20Sai%20Manoj%20Prakhya%20and%20Fiona%20Collins%20and%20Ziyuan%20Liu%20and%20Andr%C3%A9%20Borrmann%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20novel%2C%20scalable%20approach%20for%20constructing%20open%0Aset%2C%20instance-level%203D%20scene%20representations%2C%20advancing%20open%20world%0Aunderstanding%20of%203D%20environments.%20Existing%20methods%20require%20pre-constructed%203D%0Ascenes%20and%20face%20scalability%20issues%20due%20to%20per-point%20feature%20vector%20learning%2C%0Alimiting%20their%20efficacy%20with%20complex%20queries.%20Our%20method%20overcomes%20these%0Alimitations%20by%20incrementally%20building%20instance-level%203D%20scene%20representations%0Ausing%202D%20foundation%20models%2C%20efficiently%20aggregating%20instance-level%20details%20such%0Aas%20masks%2C%20feature%20vectors%2C%20names%2C%20and%20captions.%20We%20introduce%20fusion%20schemes%20for%0Afeature%20vectors%20to%20enhance%20their%20contextual%20knowledge%20and%20performance%20on%0Acomplex%20queries.%20Additionally%2C%20we%20explore%20large%20language%20models%20for%20robust%0Aautomatic%20annotation%20and%20spatial%20reasoning%20tasks.%20We%20evaluate%20our%20proposed%0Aapproach%20on%20multiple%20scenes%20from%20ScanNet%20and%20Replica%20datasets%20demonstrating%0Azero-shot%20generalization%20capabilities%2C%20exceeding%20current%20state-of-the-art%0Amethods%20in%20open%20world%203D%20scene%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14279v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenSU3D%253A%2520Open%2520World%25203D%2520Scene%2520Understanding%2520using%2520Foundation%2520Models%26entry.906535625%3DRafay%2520Mohiuddin%2520and%2520Sai%2520Manoj%2520Prakhya%2520and%2520Fiona%2520Collins%2520and%2520Ziyuan%2520Liu%2520and%2520Andr%25C3%25A9%2520Borrmann%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%252C%2520scalable%2520approach%2520for%2520constructing%2520open%250Aset%252C%2520instance-level%25203D%2520scene%2520representations%252C%2520advancing%2520open%2520world%250Aunderstanding%2520of%25203D%2520environments.%2520Existing%2520methods%2520require%2520pre-constructed%25203D%250Ascenes%2520and%2520face%2520scalability%2520issues%2520due%2520to%2520per-point%2520feature%2520vector%2520learning%252C%250Alimiting%2520their%2520efficacy%2520with%2520complex%2520queries.%2520Our%2520method%2520overcomes%2520these%250Alimitations%2520by%2520incrementally%2520building%2520instance-level%25203D%2520scene%2520representations%250Ausing%25202D%2520foundation%2520models%252C%2520efficiently%2520aggregating%2520instance-level%2520details%2520such%250Aas%2520masks%252C%2520feature%2520vectors%252C%2520names%252C%2520and%2520captions.%2520We%2520introduce%2520fusion%2520schemes%2520for%250Afeature%2520vectors%2520to%2520enhance%2520their%2520contextual%2520knowledge%2520and%2520performance%2520on%250Acomplex%2520queries.%2520Additionally%252C%2520we%2520explore%2520large%2520language%2520models%2520for%2520robust%250Aautomatic%2520annotation%2520and%2520spatial%2520reasoning%2520tasks.%2520We%2520evaluate%2520our%2520proposed%250Aapproach%2520on%2520multiple%2520scenes%2520from%2520ScanNet%2520and%2520Replica%2520datasets%2520demonstrating%250Azero-shot%2520generalization%2520capabilities%252C%2520exceeding%2520current%2520state-of-the-art%250Amethods%2520in%2520open%2520world%25203D%2520scene%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14279v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenSU3D%3A%20Open%20World%203D%20Scene%20Understanding%20using%20Foundation%20Models&entry.906535625=Rafay%20Mohiuddin%20and%20Sai%20Manoj%20Prakhya%20and%20Fiona%20Collins%20and%20Ziyuan%20Liu%20and%20Andr%C3%A9%20Borrmann&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20novel%2C%20scalable%20approach%20for%20constructing%20open%0Aset%2C%20instance-level%203D%20scene%20representations%2C%20advancing%20open%20world%0Aunderstanding%20of%203D%20environments.%20Existing%20methods%20require%20pre-constructed%203D%0Ascenes%20and%20face%20scalability%20issues%20due%20to%20per-point%20feature%20vector%20learning%2C%0Alimiting%20their%20efficacy%20with%20complex%20queries.%20Our%20method%20overcomes%20these%0Alimitations%20by%20incrementally%20building%20instance-level%203D%20scene%20representations%0Ausing%202D%20foundation%20models%2C%20efficiently%20aggregating%20instance-level%20details%20such%0Aas%20masks%2C%20feature%20vectors%2C%20names%2C%20and%20captions.%20We%20introduce%20fusion%20schemes%20for%0Afeature%20vectors%20to%20enhance%20their%20contextual%20knowledge%20and%20performance%20on%0Acomplex%20queries.%20Additionally%2C%20we%20explore%20large%20language%20models%20for%20robust%0Aautomatic%20annotation%20and%20spatial%20reasoning%20tasks.%20We%20evaluate%20our%20proposed%0Aapproach%20on%20multiple%20scenes%20from%20ScanNet%20and%20Replica%20datasets%20demonstrating%0Azero-shot%20generalization%20capabilities%2C%20exceeding%20current%20state-of-the-art%0Amethods%20in%20open%20world%203D%20scene%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14279v1&entry.124074799=Read"},
{"title": "SparseCraft: Few-Shot Neural Reconstruction through Stereopsis Guided\n  Geometric Linearization", "author": "Mae Younes and Amine Ouasfi and Adnane Boukhayma", "abstract": "  We present a novel approach for recovering 3D shape and view dependent\nappearance from a few colored images, enabling efficient 3D reconstruction and\nnovel view synthesis. Our method learns an implicit neural representation in\nthe form of a Signed Distance Function (SDF) and a radiance field. The model is\ntrained progressively through ray marching enabled volumetric rendering, and\nregularized with learning-free multi-view stereo (MVS) cues. Key to our\ncontribution is a novel implicit neural shape function learning strategy that\nencourages our SDF field to be as linear as possible near the level-set, hence\nrobustifying the training against noise emanating from the supervision and\nregularization signals. Without using any pretrained priors, our method, called\nSparseCraft, achieves state-of-the-art performances both in novel-view\nsynthesis and reconstruction from sparse views in standard benchmarks, while\nrequiring less than 10 minutes for training.\n", "link": "http://arxiv.org/abs/2407.14257v1", "date": "2024-07-19", "relevancy": 2.9915, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6234}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5949}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SparseCraft%3A%20Few-Shot%20Neural%20Reconstruction%20through%20Stereopsis%20Guided%0A%20%20Geometric%20Linearization&body=Title%3A%20SparseCraft%3A%20Few-Shot%20Neural%20Reconstruction%20through%20Stereopsis%20Guided%0A%20%20Geometric%20Linearization%0AAuthor%3A%20Mae%20Younes%20and%20Amine%20Ouasfi%20and%20Adnane%20Boukhayma%0AAbstract%3A%20%20%20We%20present%20a%20novel%20approach%20for%20recovering%203D%20shape%20and%20view%20dependent%0Aappearance%20from%20a%20few%20colored%20images%2C%20enabling%20efficient%203D%20reconstruction%20and%0Anovel%20view%20synthesis.%20Our%20method%20learns%20an%20implicit%20neural%20representation%20in%0Athe%20form%20of%20a%20Signed%20Distance%20Function%20%28SDF%29%20and%20a%20radiance%20field.%20The%20model%20is%0Atrained%20progressively%20through%20ray%20marching%20enabled%20volumetric%20rendering%2C%20and%0Aregularized%20with%20learning-free%20multi-view%20stereo%20%28MVS%29%20cues.%20Key%20to%20our%0Acontribution%20is%20a%20novel%20implicit%20neural%20shape%20function%20learning%20strategy%20that%0Aencourages%20our%20SDF%20field%20to%20be%20as%20linear%20as%20possible%20near%20the%20level-set%2C%20hence%0Arobustifying%20the%20training%20against%20noise%20emanating%20from%20the%20supervision%20and%0Aregularization%20signals.%20Without%20using%20any%20pretrained%20priors%2C%20our%20method%2C%20called%0ASparseCraft%2C%20achieves%20state-of-the-art%20performances%20both%20in%20novel-view%0Asynthesis%20and%20reconstruction%20from%20sparse%20views%20in%20standard%20benchmarks%2C%20while%0Arequiring%20less%20than%2010%20minutes%20for%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14257v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparseCraft%253A%2520Few-Shot%2520Neural%2520Reconstruction%2520through%2520Stereopsis%2520Guided%250A%2520%2520Geometric%2520Linearization%26entry.906535625%3DMae%2520Younes%2520and%2520Amine%2520Ouasfi%2520and%2520Adnane%2520Boukhayma%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520approach%2520for%2520recovering%25203D%2520shape%2520and%2520view%2520dependent%250Aappearance%2520from%2520a%2520few%2520colored%2520images%252C%2520enabling%2520efficient%25203D%2520reconstruction%2520and%250Anovel%2520view%2520synthesis.%2520Our%2520method%2520learns%2520an%2520implicit%2520neural%2520representation%2520in%250Athe%2520form%2520of%2520a%2520Signed%2520Distance%2520Function%2520%2528SDF%2529%2520and%2520a%2520radiance%2520field.%2520The%2520model%2520is%250Atrained%2520progressively%2520through%2520ray%2520marching%2520enabled%2520volumetric%2520rendering%252C%2520and%250Aregularized%2520with%2520learning-free%2520multi-view%2520stereo%2520%2528MVS%2529%2520cues.%2520Key%2520to%2520our%250Acontribution%2520is%2520a%2520novel%2520implicit%2520neural%2520shape%2520function%2520learning%2520strategy%2520that%250Aencourages%2520our%2520SDF%2520field%2520to%2520be%2520as%2520linear%2520as%2520possible%2520near%2520the%2520level-set%252C%2520hence%250Arobustifying%2520the%2520training%2520against%2520noise%2520emanating%2520from%2520the%2520supervision%2520and%250Aregularization%2520signals.%2520Without%2520using%2520any%2520pretrained%2520priors%252C%2520our%2520method%252C%2520called%250ASparseCraft%252C%2520achieves%2520state-of-the-art%2520performances%2520both%2520in%2520novel-view%250Asynthesis%2520and%2520reconstruction%2520from%2520sparse%2520views%2520in%2520standard%2520benchmarks%252C%2520while%250Arequiring%2520less%2520than%252010%2520minutes%2520for%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14257v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SparseCraft%3A%20Few-Shot%20Neural%20Reconstruction%20through%20Stereopsis%20Guided%0A%20%20Geometric%20Linearization&entry.906535625=Mae%20Younes%20and%20Amine%20Ouasfi%20and%20Adnane%20Boukhayma&entry.1292438233=%20%20We%20present%20a%20novel%20approach%20for%20recovering%203D%20shape%20and%20view%20dependent%0Aappearance%20from%20a%20few%20colored%20images%2C%20enabling%20efficient%203D%20reconstruction%20and%0Anovel%20view%20synthesis.%20Our%20method%20learns%20an%20implicit%20neural%20representation%20in%0Athe%20form%20of%20a%20Signed%20Distance%20Function%20%28SDF%29%20and%20a%20radiance%20field.%20The%20model%20is%0Atrained%20progressively%20through%20ray%20marching%20enabled%20volumetric%20rendering%2C%20and%0Aregularized%20with%20learning-free%20multi-view%20stereo%20%28MVS%29%20cues.%20Key%20to%20our%0Acontribution%20is%20a%20novel%20implicit%20neural%20shape%20function%20learning%20strategy%20that%0Aencourages%20our%20SDF%20field%20to%20be%20as%20linear%20as%20possible%20near%20the%20level-set%2C%20hence%0Arobustifying%20the%20training%20against%20noise%20emanating%20from%20the%20supervision%20and%0Aregularization%20signals.%20Without%20using%20any%20pretrained%20priors%2C%20our%20method%2C%20called%0ASparseCraft%2C%20achieves%20state-of-the-art%20performances%20both%20in%20novel-view%0Asynthesis%20and%20reconstruction%20from%20sparse%20views%20in%20standard%20benchmarks%2C%20while%0Arequiring%20less%20than%2010%20minutes%20for%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14257v1&entry.124074799=Read"},
{"title": "Double-Shot 3D Shape Measurement with a Dual-Branch Network", "author": "Mingyang Lei and Jingfan Fan and Long Shao and Hong Song and Deqiang Xiao and Danni Ai and Tianyu Fu and Ying Gu and Jian Yang", "abstract": "  The structured light (SL)-based 3D measurement techniques with deep learning\nhave been widely studied, among which speckle projection profilometry (SPP) and\nfringe projection profilometry (FPP) are two popular methods. However, they\ngenerally use a single projection pattern for reconstruction, resulting in\nfringe order ambiguity or poor reconstruction accuracy. To alleviate these\nproblems, we propose a parallel dual-branch Convolutional Neural Network\n(CNN)-Transformer network (PDCNet), to take advantage of convolutional\noperations and self-attention mechanisms for processing different SL\nmodalities. Within PDCNet, a Transformer branch is used to capture global\nperception in the fringe images, while a CNN branch is designed to collect\nlocal details in the speckle images. To fully integrate complementary features,\nwe design a double-stream attention aggregation module (DAAM) that consist of a\nparallel attention subnetwork for aggregating multi-scale spatial structure\ninformation. This module can dynamically retain local and global\nrepresentations to the maximum extent. Moreover, an adaptive mixture density\nhead with bimodal Gaussian distribution is proposed for learning a\nrepresentation that is precise near discontinuities. Compared to the standard\ndisparity regression strategy, this adaptive mixture head can effectively\nimproves performance at object boundaries. Extensive experiments demonstrate\nthat our method can reduce fringe order ambiguity while producing high-accuracy\nresults on a self-made dataset. We also show that the proposed architecture\nreveals the potential in infrared-visible image fusion task.\n", "link": "http://arxiv.org/abs/2407.14198v1", "date": "2024-07-19", "relevancy": 2.9907, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6182}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5999}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5764}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Double-Shot%203D%20Shape%20Measurement%20with%20a%20Dual-Branch%20Network&body=Title%3A%20Double-Shot%203D%20Shape%20Measurement%20with%20a%20Dual-Branch%20Network%0AAuthor%3A%20Mingyang%20Lei%20and%20Jingfan%20Fan%20and%20Long%20Shao%20and%20Hong%20Song%20and%20Deqiang%20Xiao%20and%20Danni%20Ai%20and%20Tianyu%20Fu%20and%20Ying%20Gu%20and%20Jian%20Yang%0AAbstract%3A%20%20%20The%20structured%20light%20%28SL%29-based%203D%20measurement%20techniques%20with%20deep%20learning%0Ahave%20been%20widely%20studied%2C%20among%20which%20speckle%20projection%20profilometry%20%28SPP%29%20and%0Afringe%20projection%20profilometry%20%28FPP%29%20are%20two%20popular%20methods.%20However%2C%20they%0Agenerally%20use%20a%20single%20projection%20pattern%20for%20reconstruction%2C%20resulting%20in%0Afringe%20order%20ambiguity%20or%20poor%20reconstruction%20accuracy.%20To%20alleviate%20these%0Aproblems%2C%20we%20propose%20a%20parallel%20dual-branch%20Convolutional%20Neural%20Network%0A%28CNN%29-Transformer%20network%20%28PDCNet%29%2C%20to%20take%20advantage%20of%20convolutional%0Aoperations%20and%20self-attention%20mechanisms%20for%20processing%20different%20SL%0Amodalities.%20Within%20PDCNet%2C%20a%20Transformer%20branch%20is%20used%20to%20capture%20global%0Aperception%20in%20the%20fringe%20images%2C%20while%20a%20CNN%20branch%20is%20designed%20to%20collect%0Alocal%20details%20in%20the%20speckle%20images.%20To%20fully%20integrate%20complementary%20features%2C%0Awe%20design%20a%20double-stream%20attention%20aggregation%20module%20%28DAAM%29%20that%20consist%20of%20a%0Aparallel%20attention%20subnetwork%20for%20aggregating%20multi-scale%20spatial%20structure%0Ainformation.%20This%20module%20can%20dynamically%20retain%20local%20and%20global%0Arepresentations%20to%20the%20maximum%20extent.%20Moreover%2C%20an%20adaptive%20mixture%20density%0Ahead%20with%20bimodal%20Gaussian%20distribution%20is%20proposed%20for%20learning%20a%0Arepresentation%20that%20is%20precise%20near%20discontinuities.%20Compared%20to%20the%20standard%0Adisparity%20regression%20strategy%2C%20this%20adaptive%20mixture%20head%20can%20effectively%0Aimproves%20performance%20at%20object%20boundaries.%20Extensive%20experiments%20demonstrate%0Athat%20our%20method%20can%20reduce%20fringe%20order%20ambiguity%20while%20producing%20high-accuracy%0Aresults%20on%20a%20self-made%20dataset.%20We%20also%20show%20that%20the%20proposed%20architecture%0Areveals%20the%20potential%20in%20infrared-visible%20image%20fusion%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14198v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDouble-Shot%25203D%2520Shape%2520Measurement%2520with%2520a%2520Dual-Branch%2520Network%26entry.906535625%3DMingyang%2520Lei%2520and%2520Jingfan%2520Fan%2520and%2520Long%2520Shao%2520and%2520Hong%2520Song%2520and%2520Deqiang%2520Xiao%2520and%2520Danni%2520Ai%2520and%2520Tianyu%2520Fu%2520and%2520Ying%2520Gu%2520and%2520Jian%2520Yang%26entry.1292438233%3D%2520%2520The%2520structured%2520light%2520%2528SL%2529-based%25203D%2520measurement%2520techniques%2520with%2520deep%2520learning%250Ahave%2520been%2520widely%2520studied%252C%2520among%2520which%2520speckle%2520projection%2520profilometry%2520%2528SPP%2529%2520and%250Afringe%2520projection%2520profilometry%2520%2528FPP%2529%2520are%2520two%2520popular%2520methods.%2520However%252C%2520they%250Agenerally%2520use%2520a%2520single%2520projection%2520pattern%2520for%2520reconstruction%252C%2520resulting%2520in%250Afringe%2520order%2520ambiguity%2520or%2520poor%2520reconstruction%2520accuracy.%2520To%2520alleviate%2520these%250Aproblems%252C%2520we%2520propose%2520a%2520parallel%2520dual-branch%2520Convolutional%2520Neural%2520Network%250A%2528CNN%2529-Transformer%2520network%2520%2528PDCNet%2529%252C%2520to%2520take%2520advantage%2520of%2520convolutional%250Aoperations%2520and%2520self-attention%2520mechanisms%2520for%2520processing%2520different%2520SL%250Amodalities.%2520Within%2520PDCNet%252C%2520a%2520Transformer%2520branch%2520is%2520used%2520to%2520capture%2520global%250Aperception%2520in%2520the%2520fringe%2520images%252C%2520while%2520a%2520CNN%2520branch%2520is%2520designed%2520to%2520collect%250Alocal%2520details%2520in%2520the%2520speckle%2520images.%2520To%2520fully%2520integrate%2520complementary%2520features%252C%250Awe%2520design%2520a%2520double-stream%2520attention%2520aggregation%2520module%2520%2528DAAM%2529%2520that%2520consist%2520of%2520a%250Aparallel%2520attention%2520subnetwork%2520for%2520aggregating%2520multi-scale%2520spatial%2520structure%250Ainformation.%2520This%2520module%2520can%2520dynamically%2520retain%2520local%2520and%2520global%250Arepresentations%2520to%2520the%2520maximum%2520extent.%2520Moreover%252C%2520an%2520adaptive%2520mixture%2520density%250Ahead%2520with%2520bimodal%2520Gaussian%2520distribution%2520is%2520proposed%2520for%2520learning%2520a%250Arepresentation%2520that%2520is%2520precise%2520near%2520discontinuities.%2520Compared%2520to%2520the%2520standard%250Adisparity%2520regression%2520strategy%252C%2520this%2520adaptive%2520mixture%2520head%2520can%2520effectively%250Aimproves%2520performance%2520at%2520object%2520boundaries.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520our%2520method%2520can%2520reduce%2520fringe%2520order%2520ambiguity%2520while%2520producing%2520high-accuracy%250Aresults%2520on%2520a%2520self-made%2520dataset.%2520We%2520also%2520show%2520that%2520the%2520proposed%2520architecture%250Areveals%2520the%2520potential%2520in%2520infrared-visible%2520image%2520fusion%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14198v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Double-Shot%203D%20Shape%20Measurement%20with%20a%20Dual-Branch%20Network&entry.906535625=Mingyang%20Lei%20and%20Jingfan%20Fan%20and%20Long%20Shao%20and%20Hong%20Song%20and%20Deqiang%20Xiao%20and%20Danni%20Ai%20and%20Tianyu%20Fu%20and%20Ying%20Gu%20and%20Jian%20Yang&entry.1292438233=%20%20The%20structured%20light%20%28SL%29-based%203D%20measurement%20techniques%20with%20deep%20learning%0Ahave%20been%20widely%20studied%2C%20among%20which%20speckle%20projection%20profilometry%20%28SPP%29%20and%0Afringe%20projection%20profilometry%20%28FPP%29%20are%20two%20popular%20methods.%20However%2C%20they%0Agenerally%20use%20a%20single%20projection%20pattern%20for%20reconstruction%2C%20resulting%20in%0Afringe%20order%20ambiguity%20or%20poor%20reconstruction%20accuracy.%20To%20alleviate%20these%0Aproblems%2C%20we%20propose%20a%20parallel%20dual-branch%20Convolutional%20Neural%20Network%0A%28CNN%29-Transformer%20network%20%28PDCNet%29%2C%20to%20take%20advantage%20of%20convolutional%0Aoperations%20and%20self-attention%20mechanisms%20for%20processing%20different%20SL%0Amodalities.%20Within%20PDCNet%2C%20a%20Transformer%20branch%20is%20used%20to%20capture%20global%0Aperception%20in%20the%20fringe%20images%2C%20while%20a%20CNN%20branch%20is%20designed%20to%20collect%0Alocal%20details%20in%20the%20speckle%20images.%20To%20fully%20integrate%20complementary%20features%2C%0Awe%20design%20a%20double-stream%20attention%20aggregation%20module%20%28DAAM%29%20that%20consist%20of%20a%0Aparallel%20attention%20subnetwork%20for%20aggregating%20multi-scale%20spatial%20structure%0Ainformation.%20This%20module%20can%20dynamically%20retain%20local%20and%20global%0Arepresentations%20to%20the%20maximum%20extent.%20Moreover%2C%20an%20adaptive%20mixture%20density%0Ahead%20with%20bimodal%20Gaussian%20distribution%20is%20proposed%20for%20learning%20a%0Arepresentation%20that%20is%20precise%20near%20discontinuities.%20Compared%20to%20the%20standard%0Adisparity%20regression%20strategy%2C%20this%20adaptive%20mixture%20head%20can%20effectively%0Aimproves%20performance%20at%20object%20boundaries.%20Extensive%20experiments%20demonstrate%0Athat%20our%20method%20can%20reduce%20fringe%20order%20ambiguity%20while%20producing%20high-accuracy%0Aresults%20on%20a%20self-made%20dataset.%20We%20also%20show%20that%20the%20proposed%20architecture%0Areveals%20the%20potential%20in%20infrared-visible%20image%20fusion%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14198v1&entry.124074799=Read"},
{"title": "HOTS3D: Hyper-Spherical Optimal Transport for Semantic Alignment of\n  Text-to-3D Generation", "author": "Zezeng Li and Weimin Wang and WenHai Li and Na Lei and Xianfeng Gu", "abstract": "  Recent CLIP-guided 3D generation methods have achieved promising results but\nstruggle with generating faithful 3D shapes that conform with input text due to\nthe gap between text and image embeddings. To this end, this paper proposes\nHOTS3D which makes the first attempt to effectively bridge this gap by aligning\ntext features to the image features with spherical optimal transport (SOT).\nHowever, in high-dimensional situations, solving the SOT remains a challenge.\nTo obtain the SOT map for high-dimensional features obtained from CLIP encoding\nof two modalities, we mathematically formulate and derive the solution based on\nVillani's theorem, which can directly align two hyper-sphere distributions\nwithout manifold exponential maps. Furthermore, we implement it by leveraging\ninput convex neural networks (ICNNs) for the optimal Kantorovich potential.\nWith the optimally mapped features, a diffusion-based generator and a\nNerf-based decoder are subsequently utilized to transform them into 3D shapes.\nExtensive qualitative and qualitative comparisons with state-of-the-arts\ndemonstrate the superiority of the proposed HOTS3D for 3D shape generation,\nespecially on the consistency with text semantics.\n", "link": "http://arxiv.org/abs/2407.14419v1", "date": "2024-07-19", "relevancy": 2.9296, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6313}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5632}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HOTS3D%3A%20Hyper-Spherical%20Optimal%20Transport%20for%20Semantic%20Alignment%20of%0A%20%20Text-to-3D%20Generation&body=Title%3A%20HOTS3D%3A%20Hyper-Spherical%20Optimal%20Transport%20for%20Semantic%20Alignment%20of%0A%20%20Text-to-3D%20Generation%0AAuthor%3A%20Zezeng%20Li%20and%20Weimin%20Wang%20and%20WenHai%20Li%20and%20Na%20Lei%20and%20Xianfeng%20Gu%0AAbstract%3A%20%20%20Recent%20CLIP-guided%203D%20generation%20methods%20have%20achieved%20promising%20results%20but%0Astruggle%20with%20generating%20faithful%203D%20shapes%20that%20conform%20with%20input%20text%20due%20to%0Athe%20gap%20between%20text%20and%20image%20embeddings.%20To%20this%20end%2C%20this%20paper%20proposes%0AHOTS3D%20which%20makes%20the%20first%20attempt%20to%20effectively%20bridge%20this%20gap%20by%20aligning%0Atext%20features%20to%20the%20image%20features%20with%20spherical%20optimal%20transport%20%28SOT%29.%0AHowever%2C%20in%20high-dimensional%20situations%2C%20solving%20the%20SOT%20remains%20a%20challenge.%0ATo%20obtain%20the%20SOT%20map%20for%20high-dimensional%20features%20obtained%20from%20CLIP%20encoding%0Aof%20two%20modalities%2C%20we%20mathematically%20formulate%20and%20derive%20the%20solution%20based%20on%0AVillani%27s%20theorem%2C%20which%20can%20directly%20align%20two%20hyper-sphere%20distributions%0Awithout%20manifold%20exponential%20maps.%20Furthermore%2C%20we%20implement%20it%20by%20leveraging%0Ainput%20convex%20neural%20networks%20%28ICNNs%29%20for%20the%20optimal%20Kantorovich%20potential.%0AWith%20the%20optimally%20mapped%20features%2C%20a%20diffusion-based%20generator%20and%20a%0ANerf-based%20decoder%20are%20subsequently%20utilized%20to%20transform%20them%20into%203D%20shapes.%0AExtensive%20qualitative%20and%20qualitative%20comparisons%20with%20state-of-the-arts%0Ademonstrate%20the%20superiority%20of%20the%20proposed%20HOTS3D%20for%203D%20shape%20generation%2C%0Aespecially%20on%20the%20consistency%20with%20text%20semantics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14419v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHOTS3D%253A%2520Hyper-Spherical%2520Optimal%2520Transport%2520for%2520Semantic%2520Alignment%2520of%250A%2520%2520Text-to-3D%2520Generation%26entry.906535625%3DZezeng%2520Li%2520and%2520Weimin%2520Wang%2520and%2520WenHai%2520Li%2520and%2520Na%2520Lei%2520and%2520Xianfeng%2520Gu%26entry.1292438233%3D%2520%2520Recent%2520CLIP-guided%25203D%2520generation%2520methods%2520have%2520achieved%2520promising%2520results%2520but%250Astruggle%2520with%2520generating%2520faithful%25203D%2520shapes%2520that%2520conform%2520with%2520input%2520text%2520due%2520to%250Athe%2520gap%2520between%2520text%2520and%2520image%2520embeddings.%2520To%2520this%2520end%252C%2520this%2520paper%2520proposes%250AHOTS3D%2520which%2520makes%2520the%2520first%2520attempt%2520to%2520effectively%2520bridge%2520this%2520gap%2520by%2520aligning%250Atext%2520features%2520to%2520the%2520image%2520features%2520with%2520spherical%2520optimal%2520transport%2520%2528SOT%2529.%250AHowever%252C%2520in%2520high-dimensional%2520situations%252C%2520solving%2520the%2520SOT%2520remains%2520a%2520challenge.%250ATo%2520obtain%2520the%2520SOT%2520map%2520for%2520high-dimensional%2520features%2520obtained%2520from%2520CLIP%2520encoding%250Aof%2520two%2520modalities%252C%2520we%2520mathematically%2520formulate%2520and%2520derive%2520the%2520solution%2520based%2520on%250AVillani%2527s%2520theorem%252C%2520which%2520can%2520directly%2520align%2520two%2520hyper-sphere%2520distributions%250Awithout%2520manifold%2520exponential%2520maps.%2520Furthermore%252C%2520we%2520implement%2520it%2520by%2520leveraging%250Ainput%2520convex%2520neural%2520networks%2520%2528ICNNs%2529%2520for%2520the%2520optimal%2520Kantorovich%2520potential.%250AWith%2520the%2520optimally%2520mapped%2520features%252C%2520a%2520diffusion-based%2520generator%2520and%2520a%250ANerf-based%2520decoder%2520are%2520subsequently%2520utilized%2520to%2520transform%2520them%2520into%25203D%2520shapes.%250AExtensive%2520qualitative%2520and%2520qualitative%2520comparisons%2520with%2520state-of-the-arts%250Ademonstrate%2520the%2520superiority%2520of%2520the%2520proposed%2520HOTS3D%2520for%25203D%2520shape%2520generation%252C%250Aespecially%2520on%2520the%2520consistency%2520with%2520text%2520semantics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14419v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HOTS3D%3A%20Hyper-Spherical%20Optimal%20Transport%20for%20Semantic%20Alignment%20of%0A%20%20Text-to-3D%20Generation&entry.906535625=Zezeng%20Li%20and%20Weimin%20Wang%20and%20WenHai%20Li%20and%20Na%20Lei%20and%20Xianfeng%20Gu&entry.1292438233=%20%20Recent%20CLIP-guided%203D%20generation%20methods%20have%20achieved%20promising%20results%20but%0Astruggle%20with%20generating%20faithful%203D%20shapes%20that%20conform%20with%20input%20text%20due%20to%0Athe%20gap%20between%20text%20and%20image%20embeddings.%20To%20this%20end%2C%20this%20paper%20proposes%0AHOTS3D%20which%20makes%20the%20first%20attempt%20to%20effectively%20bridge%20this%20gap%20by%20aligning%0Atext%20features%20to%20the%20image%20features%20with%20spherical%20optimal%20transport%20%28SOT%29.%0AHowever%2C%20in%20high-dimensional%20situations%2C%20solving%20the%20SOT%20remains%20a%20challenge.%0ATo%20obtain%20the%20SOT%20map%20for%20high-dimensional%20features%20obtained%20from%20CLIP%20encoding%0Aof%20two%20modalities%2C%20we%20mathematically%20formulate%20and%20derive%20the%20solution%20based%20on%0AVillani%27s%20theorem%2C%20which%20can%20directly%20align%20two%20hyper-sphere%20distributions%0Awithout%20manifold%20exponential%20maps.%20Furthermore%2C%20we%20implement%20it%20by%20leveraging%0Ainput%20convex%20neural%20networks%20%28ICNNs%29%20for%20the%20optimal%20Kantorovich%20potential.%0AWith%20the%20optimally%20mapped%20features%2C%20a%20diffusion-based%20generator%20and%20a%0ANerf-based%20decoder%20are%20subsequently%20utilized%20to%20transform%20them%20into%203D%20shapes.%0AExtensive%20qualitative%20and%20qualitative%20comparisons%20with%20state-of-the-arts%0Ademonstrate%20the%20superiority%20of%20the%20proposed%20HOTS3D%20for%203D%20shape%20generation%2C%0Aespecially%20on%20the%20consistency%20with%20text%20semantics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14419v1&entry.124074799=Read"},
{"title": "SMC-NCA: Semantic-guided Multi-level Contrast for Semi-supervised\n  Temporal Action Segmentation", "author": "Feixiang Zhou and Zheheng Jiang and Huiyu Zhou and Xuelong Li", "abstract": "  Semi-supervised temporal action segmentation (SS-TA) aims to perform\nframe-wise classification in long untrimmed videos, where only a fraction of\nvideos in the training set have labels. Recent studies have shown the potential\nof contrastive learning in unsupervised representation learning using\nunlabelled data. However, learning the representation of each frame by\nunsupervised contrastive learning for action segmentation remains an open and\nchallenging problem. In this paper, we propose a novel Semantic-guided\nMulti-level Contrast scheme with a Neighbourhood-Consistency-Aware unit\n(SMC-NCA) to extract strong frame-wise representations for SS-TAS.\nSpecifically, for representation learning, SMC is first used to explore intra-\nand inter-information variations in a unified and contrastive way, based on\naction-specific semantic information and temporal information highlighting\nrelations between actions. Then, the NCA module, which is responsible for\nenforcing spatial consistency between neighbourhoods centered at different\nframes to alleviate over-segmentation issues, works alongside SMC for\nsemi-supervised learning (SSL). Our SMC outperforms the other state-of-the-art\nmethods on three benchmarks, offering improvements of up to 17.8% and 12.6% in\nterms of Edit distance and accuracy, respectively. Additionally, the NCA unit\nresults in significantly better segmentation performance in the presence of\nonly 5% labelled videos. We also demonstrate the generalizability and\neffectiveness of the proposed method on our Parkinson Disease's Mouse Behaviour\n(PDMB) dataset. Code is available at https://github.com/FeixiangZhou/SMC-NCA.\n", "link": "http://arxiv.org/abs/2312.12347v3", "date": "2024-07-19", "relevancy": 2.9086, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6302}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5798}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMC-NCA%3A%20Semantic-guided%20Multi-level%20Contrast%20for%20Semi-supervised%0A%20%20Temporal%20Action%20Segmentation&body=Title%3A%20SMC-NCA%3A%20Semantic-guided%20Multi-level%20Contrast%20for%20Semi-supervised%0A%20%20Temporal%20Action%20Segmentation%0AAuthor%3A%20Feixiang%20Zhou%20and%20Zheheng%20Jiang%20and%20Huiyu%20Zhou%20and%20Xuelong%20Li%0AAbstract%3A%20%20%20Semi-supervised%20temporal%20action%20segmentation%20%28SS-TA%29%20aims%20to%20perform%0Aframe-wise%20classification%20in%20long%20untrimmed%20videos%2C%20where%20only%20a%20fraction%20of%0Avideos%20in%20the%20training%20set%20have%20labels.%20Recent%20studies%20have%20shown%20the%20potential%0Aof%20contrastive%20learning%20in%20unsupervised%20representation%20learning%20using%0Aunlabelled%20data.%20However%2C%20learning%20the%20representation%20of%20each%20frame%20by%0Aunsupervised%20contrastive%20learning%20for%20action%20segmentation%20remains%20an%20open%20and%0Achallenging%20problem.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Semantic-guided%0AMulti-level%20Contrast%20scheme%20with%20a%20Neighbourhood-Consistency-Aware%20unit%0A%28SMC-NCA%29%20to%20extract%20strong%20frame-wise%20representations%20for%20SS-TAS.%0ASpecifically%2C%20for%20representation%20learning%2C%20SMC%20is%20first%20used%20to%20explore%20intra-%0Aand%20inter-information%20variations%20in%20a%20unified%20and%20contrastive%20way%2C%20based%20on%0Aaction-specific%20semantic%20information%20and%20temporal%20information%20highlighting%0Arelations%20between%20actions.%20Then%2C%20the%20NCA%20module%2C%20which%20is%20responsible%20for%0Aenforcing%20spatial%20consistency%20between%20neighbourhoods%20centered%20at%20different%0Aframes%20to%20alleviate%20over-segmentation%20issues%2C%20works%20alongside%20SMC%20for%0Asemi-supervised%20learning%20%28SSL%29.%20Our%20SMC%20outperforms%20the%20other%20state-of-the-art%0Amethods%20on%20three%20benchmarks%2C%20offering%20improvements%20of%20up%20to%2017.8%25%20and%2012.6%25%20in%0Aterms%20of%20Edit%20distance%20and%20accuracy%2C%20respectively.%20Additionally%2C%20the%20NCA%20unit%0Aresults%20in%20significantly%20better%20segmentation%20performance%20in%20the%20presence%20of%0Aonly%205%25%20labelled%20videos.%20We%20also%20demonstrate%20the%20generalizability%20and%0Aeffectiveness%20of%20the%20proposed%20method%20on%20our%20Parkinson%20Disease%27s%20Mouse%20Behaviour%0A%28PDMB%29%20dataset.%20Code%20is%20available%20at%20https%3A//github.com/FeixiangZhou/SMC-NCA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.12347v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMC-NCA%253A%2520Semantic-guided%2520Multi-level%2520Contrast%2520for%2520Semi-supervised%250A%2520%2520Temporal%2520Action%2520Segmentation%26entry.906535625%3DFeixiang%2520Zhou%2520and%2520Zheheng%2520Jiang%2520and%2520Huiyu%2520Zhou%2520and%2520Xuelong%2520Li%26entry.1292438233%3D%2520%2520Semi-supervised%2520temporal%2520action%2520segmentation%2520%2528SS-TA%2529%2520aims%2520to%2520perform%250Aframe-wise%2520classification%2520in%2520long%2520untrimmed%2520videos%252C%2520where%2520only%2520a%2520fraction%2520of%250Avideos%2520in%2520the%2520training%2520set%2520have%2520labels.%2520Recent%2520studies%2520have%2520shown%2520the%2520potential%250Aof%2520contrastive%2520learning%2520in%2520unsupervised%2520representation%2520learning%2520using%250Aunlabelled%2520data.%2520However%252C%2520learning%2520the%2520representation%2520of%2520each%2520frame%2520by%250Aunsupervised%2520contrastive%2520learning%2520for%2520action%2520segmentation%2520remains%2520an%2520open%2520and%250Achallenging%2520problem.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520Semantic-guided%250AMulti-level%2520Contrast%2520scheme%2520with%2520a%2520Neighbourhood-Consistency-Aware%2520unit%250A%2528SMC-NCA%2529%2520to%2520extract%2520strong%2520frame-wise%2520representations%2520for%2520SS-TAS.%250ASpecifically%252C%2520for%2520representation%2520learning%252C%2520SMC%2520is%2520first%2520used%2520to%2520explore%2520intra-%250Aand%2520inter-information%2520variations%2520in%2520a%2520unified%2520and%2520contrastive%2520way%252C%2520based%2520on%250Aaction-specific%2520semantic%2520information%2520and%2520temporal%2520information%2520highlighting%250Arelations%2520between%2520actions.%2520Then%252C%2520the%2520NCA%2520module%252C%2520which%2520is%2520responsible%2520for%250Aenforcing%2520spatial%2520consistency%2520between%2520neighbourhoods%2520centered%2520at%2520different%250Aframes%2520to%2520alleviate%2520over-segmentation%2520issues%252C%2520works%2520alongside%2520SMC%2520for%250Asemi-supervised%2520learning%2520%2528SSL%2529.%2520Our%2520SMC%2520outperforms%2520the%2520other%2520state-of-the-art%250Amethods%2520on%2520three%2520benchmarks%252C%2520offering%2520improvements%2520of%2520up%2520to%252017.8%2525%2520and%252012.6%2525%2520in%250Aterms%2520of%2520Edit%2520distance%2520and%2520accuracy%252C%2520respectively.%2520Additionally%252C%2520the%2520NCA%2520unit%250Aresults%2520in%2520significantly%2520better%2520segmentation%2520performance%2520in%2520the%2520presence%2520of%250Aonly%25205%2525%2520labelled%2520videos.%2520We%2520also%2520demonstrate%2520the%2520generalizability%2520and%250Aeffectiveness%2520of%2520the%2520proposed%2520method%2520on%2520our%2520Parkinson%2520Disease%2527s%2520Mouse%2520Behaviour%250A%2528PDMB%2529%2520dataset.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/FeixiangZhou/SMC-NCA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.12347v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMC-NCA%3A%20Semantic-guided%20Multi-level%20Contrast%20for%20Semi-supervised%0A%20%20Temporal%20Action%20Segmentation&entry.906535625=Feixiang%20Zhou%20and%20Zheheng%20Jiang%20and%20Huiyu%20Zhou%20and%20Xuelong%20Li&entry.1292438233=%20%20Semi-supervised%20temporal%20action%20segmentation%20%28SS-TA%29%20aims%20to%20perform%0Aframe-wise%20classification%20in%20long%20untrimmed%20videos%2C%20where%20only%20a%20fraction%20of%0Avideos%20in%20the%20training%20set%20have%20labels.%20Recent%20studies%20have%20shown%20the%20potential%0Aof%20contrastive%20learning%20in%20unsupervised%20representation%20learning%20using%0Aunlabelled%20data.%20However%2C%20learning%20the%20representation%20of%20each%20frame%20by%0Aunsupervised%20contrastive%20learning%20for%20action%20segmentation%20remains%20an%20open%20and%0Achallenging%20problem.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Semantic-guided%0AMulti-level%20Contrast%20scheme%20with%20a%20Neighbourhood-Consistency-Aware%20unit%0A%28SMC-NCA%29%20to%20extract%20strong%20frame-wise%20representations%20for%20SS-TAS.%0ASpecifically%2C%20for%20representation%20learning%2C%20SMC%20is%20first%20used%20to%20explore%20intra-%0Aand%20inter-information%20variations%20in%20a%20unified%20and%20contrastive%20way%2C%20based%20on%0Aaction-specific%20semantic%20information%20and%20temporal%20information%20highlighting%0Arelations%20between%20actions.%20Then%2C%20the%20NCA%20module%2C%20which%20is%20responsible%20for%0Aenforcing%20spatial%20consistency%20between%20neighbourhoods%20centered%20at%20different%0Aframes%20to%20alleviate%20over-segmentation%20issues%2C%20works%20alongside%20SMC%20for%0Asemi-supervised%20learning%20%28SSL%29.%20Our%20SMC%20outperforms%20the%20other%20state-of-the-art%0Amethods%20on%20three%20benchmarks%2C%20offering%20improvements%20of%20up%20to%2017.8%25%20and%2012.6%25%20in%0Aterms%20of%20Edit%20distance%20and%20accuracy%2C%20respectively.%20Additionally%2C%20the%20NCA%20unit%0Aresults%20in%20significantly%20better%20segmentation%20performance%20in%20the%20presence%20of%0Aonly%205%25%20labelled%20videos.%20We%20also%20demonstrate%20the%20generalizability%20and%0Aeffectiveness%20of%20the%20proposed%20method%20on%20our%20Parkinson%20Disease%27s%20Mouse%20Behaviour%0A%28PDMB%29%20dataset.%20Code%20is%20available%20at%20https%3A//github.com/FeixiangZhou/SMC-NCA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.12347v3&entry.124074799=Read"},
{"title": "Dyn-Adapter: Towards Disentangled Representation for Efficient Visual\n  Recognition", "author": "Yurong Zhang and Honghao Chen and Xinyu Zhang and Xiangxiang Chu and Li Song", "abstract": "  Parameter-efficient transfer learning (PETL) is a promising task, aiming to\nadapt the large-scale pre-trained model to downstream tasks with a relatively\nmodest cost. However, current PETL methods struggle in compressing\ncomputational complexity and bear a heavy inference burden due to the complete\nforward process. This paper presents an efficient visual recognition paradigm,\ncalled Dynamic Adapter (Dyn-Adapter), that boosts PETL efficiency by subtly\ndisentangling features in multiple levels. Our approach is simple: first, we\ndevise a dynamic architecture with balanced early heads for multi-level feature\nextraction, along with adaptive training strategy. Second, we introduce a\nbidirectional sparsity strategy driven by the pursuit of powerful\ngeneralization ability. These qualities enable us to fine-tune efficiently and\neffectively: we reduce FLOPs during inference by 50%, while maintaining or even\nyielding higher recognition accuracy. Extensive experiments on diverse datasets\nand pretrained backbones demonstrate the potential of Dyn-Adapter serving as a\ngeneral efficiency booster for PETL in vision recognition tasks.\n", "link": "http://arxiv.org/abs/2407.14302v1", "date": "2024-07-19", "relevancy": 2.8697, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.61}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5616}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dyn-Adapter%3A%20Towards%20Disentangled%20Representation%20for%20Efficient%20Visual%0A%20%20Recognition&body=Title%3A%20Dyn-Adapter%3A%20Towards%20Disentangled%20Representation%20for%20Efficient%20Visual%0A%20%20Recognition%0AAuthor%3A%20Yurong%20Zhang%20and%20Honghao%20Chen%20and%20Xinyu%20Zhang%20and%20Xiangxiang%20Chu%20and%20Li%20Song%0AAbstract%3A%20%20%20Parameter-efficient%20transfer%20learning%20%28PETL%29%20is%20a%20promising%20task%2C%20aiming%20to%0Aadapt%20the%20large-scale%20pre-trained%20model%20to%20downstream%20tasks%20with%20a%20relatively%0Amodest%20cost.%20However%2C%20current%20PETL%20methods%20struggle%20in%20compressing%0Acomputational%20complexity%20and%20bear%20a%20heavy%20inference%20burden%20due%20to%20the%20complete%0Aforward%20process.%20This%20paper%20presents%20an%20efficient%20visual%20recognition%20paradigm%2C%0Acalled%20Dynamic%20Adapter%20%28Dyn-Adapter%29%2C%20that%20boosts%20PETL%20efficiency%20by%20subtly%0Adisentangling%20features%20in%20multiple%20levels.%20Our%20approach%20is%20simple%3A%20first%2C%20we%0Adevise%20a%20dynamic%20architecture%20with%20balanced%20early%20heads%20for%20multi-level%20feature%0Aextraction%2C%20along%20with%20adaptive%20training%20strategy.%20Second%2C%20we%20introduce%20a%0Abidirectional%20sparsity%20strategy%20driven%20by%20the%20pursuit%20of%20powerful%0Ageneralization%20ability.%20These%20qualities%20enable%20us%20to%20fine-tune%20efficiently%20and%0Aeffectively%3A%20we%20reduce%20FLOPs%20during%20inference%20by%2050%25%2C%20while%20maintaining%20or%20even%0Ayielding%20higher%20recognition%20accuracy.%20Extensive%20experiments%20on%20diverse%20datasets%0Aand%20pretrained%20backbones%20demonstrate%20the%20potential%20of%20Dyn-Adapter%20serving%20as%20a%0Ageneral%20efficiency%20booster%20for%20PETL%20in%20vision%20recognition%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14302v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDyn-Adapter%253A%2520Towards%2520Disentangled%2520Representation%2520for%2520Efficient%2520Visual%250A%2520%2520Recognition%26entry.906535625%3DYurong%2520Zhang%2520and%2520Honghao%2520Chen%2520and%2520Xinyu%2520Zhang%2520and%2520Xiangxiang%2520Chu%2520and%2520Li%2520Song%26entry.1292438233%3D%2520%2520Parameter-efficient%2520transfer%2520learning%2520%2528PETL%2529%2520is%2520a%2520promising%2520task%252C%2520aiming%2520to%250Aadapt%2520the%2520large-scale%2520pre-trained%2520model%2520to%2520downstream%2520tasks%2520with%2520a%2520relatively%250Amodest%2520cost.%2520However%252C%2520current%2520PETL%2520methods%2520struggle%2520in%2520compressing%250Acomputational%2520complexity%2520and%2520bear%2520a%2520heavy%2520inference%2520burden%2520due%2520to%2520the%2520complete%250Aforward%2520process.%2520This%2520paper%2520presents%2520an%2520efficient%2520visual%2520recognition%2520paradigm%252C%250Acalled%2520Dynamic%2520Adapter%2520%2528Dyn-Adapter%2529%252C%2520that%2520boosts%2520PETL%2520efficiency%2520by%2520subtly%250Adisentangling%2520features%2520in%2520multiple%2520levels.%2520Our%2520approach%2520is%2520simple%253A%2520first%252C%2520we%250Adevise%2520a%2520dynamic%2520architecture%2520with%2520balanced%2520early%2520heads%2520for%2520multi-level%2520feature%250Aextraction%252C%2520along%2520with%2520adaptive%2520training%2520strategy.%2520Second%252C%2520we%2520introduce%2520a%250Abidirectional%2520sparsity%2520strategy%2520driven%2520by%2520the%2520pursuit%2520of%2520powerful%250Ageneralization%2520ability.%2520These%2520qualities%2520enable%2520us%2520to%2520fine-tune%2520efficiently%2520and%250Aeffectively%253A%2520we%2520reduce%2520FLOPs%2520during%2520inference%2520by%252050%2525%252C%2520while%2520maintaining%2520or%2520even%250Ayielding%2520higher%2520recognition%2520accuracy.%2520Extensive%2520experiments%2520on%2520diverse%2520datasets%250Aand%2520pretrained%2520backbones%2520demonstrate%2520the%2520potential%2520of%2520Dyn-Adapter%2520serving%2520as%2520a%250Ageneral%2520efficiency%2520booster%2520for%2520PETL%2520in%2520vision%2520recognition%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14302v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dyn-Adapter%3A%20Towards%20Disentangled%20Representation%20for%20Efficient%20Visual%0A%20%20Recognition&entry.906535625=Yurong%20Zhang%20and%20Honghao%20Chen%20and%20Xinyu%20Zhang%20and%20Xiangxiang%20Chu%20and%20Li%20Song&entry.1292438233=%20%20Parameter-efficient%20transfer%20learning%20%28PETL%29%20is%20a%20promising%20task%2C%20aiming%20to%0Aadapt%20the%20large-scale%20pre-trained%20model%20to%20downstream%20tasks%20with%20a%20relatively%0Amodest%20cost.%20However%2C%20current%20PETL%20methods%20struggle%20in%20compressing%0Acomputational%20complexity%20and%20bear%20a%20heavy%20inference%20burden%20due%20to%20the%20complete%0Aforward%20process.%20This%20paper%20presents%20an%20efficient%20visual%20recognition%20paradigm%2C%0Acalled%20Dynamic%20Adapter%20%28Dyn-Adapter%29%2C%20that%20boosts%20PETL%20efficiency%20by%20subtly%0Adisentangling%20features%20in%20multiple%20levels.%20Our%20approach%20is%20simple%3A%20first%2C%20we%0Adevise%20a%20dynamic%20architecture%20with%20balanced%20early%20heads%20for%20multi-level%20feature%0Aextraction%2C%20along%20with%20adaptive%20training%20strategy.%20Second%2C%20we%20introduce%20a%0Abidirectional%20sparsity%20strategy%20driven%20by%20the%20pursuit%20of%20powerful%0Ageneralization%20ability.%20These%20qualities%20enable%20us%20to%20fine-tune%20efficiently%20and%0Aeffectively%3A%20we%20reduce%20FLOPs%20during%20inference%20by%2050%25%2C%20while%20maintaining%20or%20even%0Ayielding%20higher%20recognition%20accuracy.%20Extensive%20experiments%20on%20diverse%20datasets%0Aand%20pretrained%20backbones%20demonstrate%20the%20potential%20of%20Dyn-Adapter%20serving%20as%20a%0Ageneral%20efficiency%20booster%20for%20PETL%20in%20vision%20recognition%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14302v1&entry.124074799=Read"},
{"title": "An Attention-based Representation Distillation Baseline for Multi-Label\n  Continual Learning", "author": "Martin Menabue and Emanuele Frascaroli and Matteo Boschini and Lorenzo Bonicelli and Angelo Porrello and Simone Calderara", "abstract": "  The field of Continual Learning (CL) has inspired numerous researchers over\nthe years, leading to increasingly advanced countermeasures to the issue of\ncatastrophic forgetting. Most studies have focused on the single-class\nscenario, where each example comes with a single label. The recent literature\nhas successfully tackled such a setting, with impressive results. Differently,\nwe shift our attention to the multi-label scenario, as we feel it to be more\nrepresentative of real-world open problems. In our work, we show that existing\nstate-of-the-art CL methods fail to achieve satisfactory performance, thus\nquestioning the real advance claimed in recent years. Therefore, we assess both\nold-style and novel strategies and propose, on top of them, an approach called\nSelective Class Attention Distillation (SCAD). It relies on a knowledge\ntransfer technique that seeks to align the representations of the student\nnetwork -- which trains continuously and is subject to forgetting -- with the\nteacher ones, which is pretrained and kept frozen. Importantly, our method is\nable to selectively transfer the relevant information from the teacher to the\nstudent, thereby preventing irrelevant information from harming the student's\nperformance during online training. To demonstrate the merits of our approach,\nwe conduct experiments on two different multi-label datasets, showing that our\nmethod outperforms the current state-of-the-art Continual Learning methods. Our\nfindings highlight the importance of addressing the unique challenges posed by\nmulti-label environments in the field of Continual Learning. The code of SCAD\nis available at https://github.com/aimagelab/SCAD-LOD-2024.\n", "link": "http://arxiv.org/abs/2407.14249v1", "date": "2024-07-19", "relevancy": 2.8233, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5911}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5568}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Attention-based%20Representation%20Distillation%20Baseline%20for%20Multi-Label%0A%20%20Continual%20Learning&body=Title%3A%20An%20Attention-based%20Representation%20Distillation%20Baseline%20for%20Multi-Label%0A%20%20Continual%20Learning%0AAuthor%3A%20Martin%20Menabue%20and%20Emanuele%20Frascaroli%20and%20Matteo%20Boschini%20and%20Lorenzo%20Bonicelli%20and%20Angelo%20Porrello%20and%20Simone%20Calderara%0AAbstract%3A%20%20%20The%20field%20of%20Continual%20Learning%20%28CL%29%20has%20inspired%20numerous%20researchers%20over%0Athe%20years%2C%20leading%20to%20increasingly%20advanced%20countermeasures%20to%20the%20issue%20of%0Acatastrophic%20forgetting.%20Most%20studies%20have%20focused%20on%20the%20single-class%0Ascenario%2C%20where%20each%20example%20comes%20with%20a%20single%20label.%20The%20recent%20literature%0Ahas%20successfully%20tackled%20such%20a%20setting%2C%20with%20impressive%20results.%20Differently%2C%0Awe%20shift%20our%20attention%20to%20the%20multi-label%20scenario%2C%20as%20we%20feel%20it%20to%20be%20more%0Arepresentative%20of%20real-world%20open%20problems.%20In%20our%20work%2C%20we%20show%20that%20existing%0Astate-of-the-art%20CL%20methods%20fail%20to%20achieve%20satisfactory%20performance%2C%20thus%0Aquestioning%20the%20real%20advance%20claimed%20in%20recent%20years.%20Therefore%2C%20we%20assess%20both%0Aold-style%20and%20novel%20strategies%20and%20propose%2C%20on%20top%20of%20them%2C%20an%20approach%20called%0ASelective%20Class%20Attention%20Distillation%20%28SCAD%29.%20It%20relies%20on%20a%20knowledge%0Atransfer%20technique%20that%20seeks%20to%20align%20the%20representations%20of%20the%20student%0Anetwork%20--%20which%20trains%20continuously%20and%20is%20subject%20to%20forgetting%20--%20with%20the%0Ateacher%20ones%2C%20which%20is%20pretrained%20and%20kept%20frozen.%20Importantly%2C%20our%20method%20is%0Aable%20to%20selectively%20transfer%20the%20relevant%20information%20from%20the%20teacher%20to%20the%0Astudent%2C%20thereby%20preventing%20irrelevant%20information%20from%20harming%20the%20student%27s%0Aperformance%20during%20online%20training.%20To%20demonstrate%20the%20merits%20of%20our%20approach%2C%0Awe%20conduct%20experiments%20on%20two%20different%20multi-label%20datasets%2C%20showing%20that%20our%0Amethod%20outperforms%20the%20current%20state-of-the-art%20Continual%20Learning%20methods.%20Our%0Afindings%20highlight%20the%20importance%20of%20addressing%20the%20unique%20challenges%20posed%20by%0Amulti-label%20environments%20in%20the%20field%20of%20Continual%20Learning.%20The%20code%20of%20SCAD%0Ais%20available%20at%20https%3A//github.com/aimagelab/SCAD-LOD-2024.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14249v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Attention-based%2520Representation%2520Distillation%2520Baseline%2520for%2520Multi-Label%250A%2520%2520Continual%2520Learning%26entry.906535625%3DMartin%2520Menabue%2520and%2520Emanuele%2520Frascaroli%2520and%2520Matteo%2520Boschini%2520and%2520Lorenzo%2520Bonicelli%2520and%2520Angelo%2520Porrello%2520and%2520Simone%2520Calderara%26entry.1292438233%3D%2520%2520The%2520field%2520of%2520Continual%2520Learning%2520%2528CL%2529%2520has%2520inspired%2520numerous%2520researchers%2520over%250Athe%2520years%252C%2520leading%2520to%2520increasingly%2520advanced%2520countermeasures%2520to%2520the%2520issue%2520of%250Acatastrophic%2520forgetting.%2520Most%2520studies%2520have%2520focused%2520on%2520the%2520single-class%250Ascenario%252C%2520where%2520each%2520example%2520comes%2520with%2520a%2520single%2520label.%2520The%2520recent%2520literature%250Ahas%2520successfully%2520tackled%2520such%2520a%2520setting%252C%2520with%2520impressive%2520results.%2520Differently%252C%250Awe%2520shift%2520our%2520attention%2520to%2520the%2520multi-label%2520scenario%252C%2520as%2520we%2520feel%2520it%2520to%2520be%2520more%250Arepresentative%2520of%2520real-world%2520open%2520problems.%2520In%2520our%2520work%252C%2520we%2520show%2520that%2520existing%250Astate-of-the-art%2520CL%2520methods%2520fail%2520to%2520achieve%2520satisfactory%2520performance%252C%2520thus%250Aquestioning%2520the%2520real%2520advance%2520claimed%2520in%2520recent%2520years.%2520Therefore%252C%2520we%2520assess%2520both%250Aold-style%2520and%2520novel%2520strategies%2520and%2520propose%252C%2520on%2520top%2520of%2520them%252C%2520an%2520approach%2520called%250ASelective%2520Class%2520Attention%2520Distillation%2520%2528SCAD%2529.%2520It%2520relies%2520on%2520a%2520knowledge%250Atransfer%2520technique%2520that%2520seeks%2520to%2520align%2520the%2520representations%2520of%2520the%2520student%250Anetwork%2520--%2520which%2520trains%2520continuously%2520and%2520is%2520subject%2520to%2520forgetting%2520--%2520with%2520the%250Ateacher%2520ones%252C%2520which%2520is%2520pretrained%2520and%2520kept%2520frozen.%2520Importantly%252C%2520our%2520method%2520is%250Aable%2520to%2520selectively%2520transfer%2520the%2520relevant%2520information%2520from%2520the%2520teacher%2520to%2520the%250Astudent%252C%2520thereby%2520preventing%2520irrelevant%2520information%2520from%2520harming%2520the%2520student%2527s%250Aperformance%2520during%2520online%2520training.%2520To%2520demonstrate%2520the%2520merits%2520of%2520our%2520approach%252C%250Awe%2520conduct%2520experiments%2520on%2520two%2520different%2520multi-label%2520datasets%252C%2520showing%2520that%2520our%250Amethod%2520outperforms%2520the%2520current%2520state-of-the-art%2520Continual%2520Learning%2520methods.%2520Our%250Afindings%2520highlight%2520the%2520importance%2520of%2520addressing%2520the%2520unique%2520challenges%2520posed%2520by%250Amulti-label%2520environments%2520in%2520the%2520field%2520of%2520Continual%2520Learning.%2520The%2520code%2520of%2520SCAD%250Ais%2520available%2520at%2520https%253A//github.com/aimagelab/SCAD-LOD-2024.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14249v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Attention-based%20Representation%20Distillation%20Baseline%20for%20Multi-Label%0A%20%20Continual%20Learning&entry.906535625=Martin%20Menabue%20and%20Emanuele%20Frascaroli%20and%20Matteo%20Boschini%20and%20Lorenzo%20Bonicelli%20and%20Angelo%20Porrello%20and%20Simone%20Calderara&entry.1292438233=%20%20The%20field%20of%20Continual%20Learning%20%28CL%29%20has%20inspired%20numerous%20researchers%20over%0Athe%20years%2C%20leading%20to%20increasingly%20advanced%20countermeasures%20to%20the%20issue%20of%0Acatastrophic%20forgetting.%20Most%20studies%20have%20focused%20on%20the%20single-class%0Ascenario%2C%20where%20each%20example%20comes%20with%20a%20single%20label.%20The%20recent%20literature%0Ahas%20successfully%20tackled%20such%20a%20setting%2C%20with%20impressive%20results.%20Differently%2C%0Awe%20shift%20our%20attention%20to%20the%20multi-label%20scenario%2C%20as%20we%20feel%20it%20to%20be%20more%0Arepresentative%20of%20real-world%20open%20problems.%20In%20our%20work%2C%20we%20show%20that%20existing%0Astate-of-the-art%20CL%20methods%20fail%20to%20achieve%20satisfactory%20performance%2C%20thus%0Aquestioning%20the%20real%20advance%20claimed%20in%20recent%20years.%20Therefore%2C%20we%20assess%20both%0Aold-style%20and%20novel%20strategies%20and%20propose%2C%20on%20top%20of%20them%2C%20an%20approach%20called%0ASelective%20Class%20Attention%20Distillation%20%28SCAD%29.%20It%20relies%20on%20a%20knowledge%0Atransfer%20technique%20that%20seeks%20to%20align%20the%20representations%20of%20the%20student%0Anetwork%20--%20which%20trains%20continuously%20and%20is%20subject%20to%20forgetting%20--%20with%20the%0Ateacher%20ones%2C%20which%20is%20pretrained%20and%20kept%20frozen.%20Importantly%2C%20our%20method%20is%0Aable%20to%20selectively%20transfer%20the%20relevant%20information%20from%20the%20teacher%20to%20the%0Astudent%2C%20thereby%20preventing%20irrelevant%20information%20from%20harming%20the%20student%27s%0Aperformance%20during%20online%20training.%20To%20demonstrate%20the%20merits%20of%20our%20approach%2C%0Awe%20conduct%20experiments%20on%20two%20different%20multi-label%20datasets%2C%20showing%20that%20our%0Amethod%20outperforms%20the%20current%20state-of-the-art%20Continual%20Learning%20methods.%20Our%0Afindings%20highlight%20the%20importance%20of%20addressing%20the%20unique%20challenges%20posed%20by%0Amulti-label%20environments%20in%20the%20field%20of%20Continual%20Learning.%20The%20code%20of%20SCAD%0Ais%20available%20at%20https%3A//github.com/aimagelab/SCAD-LOD-2024.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14249v1&entry.124074799=Read"},
{"title": "A Curriculum-style Self-training Approach for Source-Free Semantic\n  Segmentation", "author": "Yuxi Wang and Jian Liang and Zhaoxiang Zhang", "abstract": "  Source-free domain adaptation has developed rapidly in recent years, where\nthe well-trained source model is adapted to the target domain instead of the\nsource data, offering the potential for privacy concerns and intellectual\nproperty protection. However, a number of feature alignment techniques in prior\ndomain adaptation methods are not feasible in this challenging problem setting.\nThereby, we resort to probing inherent domain-invariant feature learning and\npropose a curriculum-style self-training approach for source-free domain\nadaptive semantic segmentation. In particular, we introduce a curriculum-style\nentropy minimization method to explore the implicit knowledge from the source\nmodel, which fits the trained source model to the target data using certain\ninformation from easy-to-hard predictions. We then train the segmentation\nnetwork by the proposed complementary curriculum-style self-training, which\nutilizes the negative and positive pseudo labels following the\ncurriculum-learning manner. Although negative pseudo-labels with high\nuncertainty cannot be identified with the correct labels, they can definitely\nindicate absent classes. Moreover, we employ an information propagation scheme\nto further reduce the intra-domain discrepancy within the target domain, which\ncould act as a standard post-processing method for the domain adaptation field.\nFurthermore, we extend the proposed method to a more challenging black-box\nsource model scenario where only the source model's predictions are available.\nExtensive experiments validate that our method yields state-of-the-art\nperformance on source-free semantic segmentation tasks for both\nsynthetic-to-real and adverse conditions datasets. The code and corresponding\ntrained models are released at \\url{https://github.com/yxiwang/ATP}.\n", "link": "http://arxiv.org/abs/2106.11653v5", "date": "2024-07-19", "relevancy": 2.8046, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5653}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5628}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Curriculum-style%20Self-training%20Approach%20for%20Source-Free%20Semantic%0A%20%20Segmentation&body=Title%3A%20A%20Curriculum-style%20Self-training%20Approach%20for%20Source-Free%20Semantic%0A%20%20Segmentation%0AAuthor%3A%20Yuxi%20Wang%20and%20Jian%20Liang%20and%20Zhaoxiang%20Zhang%0AAbstract%3A%20%20%20Source-free%20domain%20adaptation%20has%20developed%20rapidly%20in%20recent%20years%2C%20where%0Athe%20well-trained%20source%20model%20is%20adapted%20to%20the%20target%20domain%20instead%20of%20the%0Asource%20data%2C%20offering%20the%20potential%20for%20privacy%20concerns%20and%20intellectual%0Aproperty%20protection.%20However%2C%20a%20number%20of%20feature%20alignment%20techniques%20in%20prior%0Adomain%20adaptation%20methods%20are%20not%20feasible%20in%20this%20challenging%20problem%20setting.%0AThereby%2C%20we%20resort%20to%20probing%20inherent%20domain-invariant%20feature%20learning%20and%0Apropose%20a%20curriculum-style%20self-training%20approach%20for%20source-free%20domain%0Aadaptive%20semantic%20segmentation.%20In%20particular%2C%20we%20introduce%20a%20curriculum-style%0Aentropy%20minimization%20method%20to%20explore%20the%20implicit%20knowledge%20from%20the%20source%0Amodel%2C%20which%20fits%20the%20trained%20source%20model%20to%20the%20target%20data%20using%20certain%0Ainformation%20from%20easy-to-hard%20predictions.%20We%20then%20train%20the%20segmentation%0Anetwork%20by%20the%20proposed%20complementary%20curriculum-style%20self-training%2C%20which%0Autilizes%20the%20negative%20and%20positive%20pseudo%20labels%20following%20the%0Acurriculum-learning%20manner.%20Although%20negative%20pseudo-labels%20with%20high%0Auncertainty%20cannot%20be%20identified%20with%20the%20correct%20labels%2C%20they%20can%20definitely%0Aindicate%20absent%20classes.%20Moreover%2C%20we%20employ%20an%20information%20propagation%20scheme%0Ato%20further%20reduce%20the%20intra-domain%20discrepancy%20within%20the%20target%20domain%2C%20which%0Acould%20act%20as%20a%20standard%20post-processing%20method%20for%20the%20domain%20adaptation%20field.%0AFurthermore%2C%20we%20extend%20the%20proposed%20method%20to%20a%20more%20challenging%20black-box%0Asource%20model%20scenario%20where%20only%20the%20source%20model%27s%20predictions%20are%20available.%0AExtensive%20experiments%20validate%20that%20our%20method%20yields%20state-of-the-art%0Aperformance%20on%20source-free%20semantic%20segmentation%20tasks%20for%20both%0Asynthetic-to-real%20and%20adverse%20conditions%20datasets.%20The%20code%20and%20corresponding%0Atrained%20models%20are%20released%20at%20%5Curl%7Bhttps%3A//github.com/yxiwang/ATP%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2106.11653v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Curriculum-style%2520Self-training%2520Approach%2520for%2520Source-Free%2520Semantic%250A%2520%2520Segmentation%26entry.906535625%3DYuxi%2520Wang%2520and%2520Jian%2520Liang%2520and%2520Zhaoxiang%2520Zhang%26entry.1292438233%3D%2520%2520Source-free%2520domain%2520adaptation%2520has%2520developed%2520rapidly%2520in%2520recent%2520years%252C%2520where%250Athe%2520well-trained%2520source%2520model%2520is%2520adapted%2520to%2520the%2520target%2520domain%2520instead%2520of%2520the%250Asource%2520data%252C%2520offering%2520the%2520potential%2520for%2520privacy%2520concerns%2520and%2520intellectual%250Aproperty%2520protection.%2520However%252C%2520a%2520number%2520of%2520feature%2520alignment%2520techniques%2520in%2520prior%250Adomain%2520adaptation%2520methods%2520are%2520not%2520feasible%2520in%2520this%2520challenging%2520problem%2520setting.%250AThereby%252C%2520we%2520resort%2520to%2520probing%2520inherent%2520domain-invariant%2520feature%2520learning%2520and%250Apropose%2520a%2520curriculum-style%2520self-training%2520approach%2520for%2520source-free%2520domain%250Aadaptive%2520semantic%2520segmentation.%2520In%2520particular%252C%2520we%2520introduce%2520a%2520curriculum-style%250Aentropy%2520minimization%2520method%2520to%2520explore%2520the%2520implicit%2520knowledge%2520from%2520the%2520source%250Amodel%252C%2520which%2520fits%2520the%2520trained%2520source%2520model%2520to%2520the%2520target%2520data%2520using%2520certain%250Ainformation%2520from%2520easy-to-hard%2520predictions.%2520We%2520then%2520train%2520the%2520segmentation%250Anetwork%2520by%2520the%2520proposed%2520complementary%2520curriculum-style%2520self-training%252C%2520which%250Autilizes%2520the%2520negative%2520and%2520positive%2520pseudo%2520labels%2520following%2520the%250Acurriculum-learning%2520manner.%2520Although%2520negative%2520pseudo-labels%2520with%2520high%250Auncertainty%2520cannot%2520be%2520identified%2520with%2520the%2520correct%2520labels%252C%2520they%2520can%2520definitely%250Aindicate%2520absent%2520classes.%2520Moreover%252C%2520we%2520employ%2520an%2520information%2520propagation%2520scheme%250Ato%2520further%2520reduce%2520the%2520intra-domain%2520discrepancy%2520within%2520the%2520target%2520domain%252C%2520which%250Acould%2520act%2520as%2520a%2520standard%2520post-processing%2520method%2520for%2520the%2520domain%2520adaptation%2520field.%250AFurthermore%252C%2520we%2520extend%2520the%2520proposed%2520method%2520to%2520a%2520more%2520challenging%2520black-box%250Asource%2520model%2520scenario%2520where%2520only%2520the%2520source%2520model%2527s%2520predictions%2520are%2520available.%250AExtensive%2520experiments%2520validate%2520that%2520our%2520method%2520yields%2520state-of-the-art%250Aperformance%2520on%2520source-free%2520semantic%2520segmentation%2520tasks%2520for%2520both%250Asynthetic-to-real%2520and%2520adverse%2520conditions%2520datasets.%2520The%2520code%2520and%2520corresponding%250Atrained%2520models%2520are%2520released%2520at%2520%255Curl%257Bhttps%253A//github.com/yxiwang/ATP%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2106.11653v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Curriculum-style%20Self-training%20Approach%20for%20Source-Free%20Semantic%0A%20%20Segmentation&entry.906535625=Yuxi%20Wang%20and%20Jian%20Liang%20and%20Zhaoxiang%20Zhang&entry.1292438233=%20%20Source-free%20domain%20adaptation%20has%20developed%20rapidly%20in%20recent%20years%2C%20where%0Athe%20well-trained%20source%20model%20is%20adapted%20to%20the%20target%20domain%20instead%20of%20the%0Asource%20data%2C%20offering%20the%20potential%20for%20privacy%20concerns%20and%20intellectual%0Aproperty%20protection.%20However%2C%20a%20number%20of%20feature%20alignment%20techniques%20in%20prior%0Adomain%20adaptation%20methods%20are%20not%20feasible%20in%20this%20challenging%20problem%20setting.%0AThereby%2C%20we%20resort%20to%20probing%20inherent%20domain-invariant%20feature%20learning%20and%0Apropose%20a%20curriculum-style%20self-training%20approach%20for%20source-free%20domain%0Aadaptive%20semantic%20segmentation.%20In%20particular%2C%20we%20introduce%20a%20curriculum-style%0Aentropy%20minimization%20method%20to%20explore%20the%20implicit%20knowledge%20from%20the%20source%0Amodel%2C%20which%20fits%20the%20trained%20source%20model%20to%20the%20target%20data%20using%20certain%0Ainformation%20from%20easy-to-hard%20predictions.%20We%20then%20train%20the%20segmentation%0Anetwork%20by%20the%20proposed%20complementary%20curriculum-style%20self-training%2C%20which%0Autilizes%20the%20negative%20and%20positive%20pseudo%20labels%20following%20the%0Acurriculum-learning%20manner.%20Although%20negative%20pseudo-labels%20with%20high%0Auncertainty%20cannot%20be%20identified%20with%20the%20correct%20labels%2C%20they%20can%20definitely%0Aindicate%20absent%20classes.%20Moreover%2C%20we%20employ%20an%20information%20propagation%20scheme%0Ato%20further%20reduce%20the%20intra-domain%20discrepancy%20within%20the%20target%20domain%2C%20which%0Acould%20act%20as%20a%20standard%20post-processing%20method%20for%20the%20domain%20adaptation%20field.%0AFurthermore%2C%20we%20extend%20the%20proposed%20method%20to%20a%20more%20challenging%20black-box%0Asource%20model%20scenario%20where%20only%20the%20source%20model%27s%20predictions%20are%20available.%0AExtensive%20experiments%20validate%20that%20our%20method%20yields%20state-of-the-art%0Aperformance%20on%20source-free%20semantic%20segmentation%20tasks%20for%20both%0Asynthetic-to-real%20and%20adverse%20conditions%20datasets.%20The%20code%20and%20corresponding%0Atrained%20models%20are%20released%20at%20%5Curl%7Bhttps%3A//github.com/yxiwang/ATP%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2106.11653v5&entry.124074799=Read"},
{"title": "TaGAT: Topology-Aware Graph Attention Network For Multi-modal Retinal\n  Image Fusion", "author": "Xin Tian and Nantheera Anantrasirichai and Lindsay Nicholson and Alin Achim", "abstract": "  In the realm of medical image fusion, integrating information from various\nmodalities is crucial for improving diagnostics and treatment planning,\nespecially in retinal health, where the important features exhibit differently\nin different imaging modalities. Existing deep learning-based approaches\ninsufficiently focus on retinal image fusion, and thus fail to preserve enough\nanatomical structure and fine vessel details in retinal image fusion. To\naddress this, we propose the Topology-Aware Graph Attention Network (TaGAT) for\nmulti-modal retinal image fusion, leveraging a novel Topology-Aware Encoder\n(TAE) with Graph Attention Networks (GAT) to effectively enhance spatial\nfeatures with retinal vasculature's graph topology across modalities. The TAE\nencodes the base and detail features, extracted via a Long-short Range (LSR)\nencoder from retinal images, into the graph extracted from the retinal vessel.\nWithin the TAE, the GAT-based Graph Information Update (GIU) block dynamically\nrefines and aggregates the node features to generate topology-aware graph\nfeatures. The updated graph features with base and detail features are combined\nand decoded as a fused image. Our model outperforms state-of-the-art methods in\nFluorescein Fundus Angiography (FFA) with Color Fundus (CF) and Optical\nCoherence Tomography (OCT) with confocal microscopy retinal image fusion. The\nsource code can be accessed via https://github.com/xintian-99/TaGAT.\n", "link": "http://arxiv.org/abs/2407.14188v1", "date": "2024-07-19", "relevancy": 2.7221, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5707}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5342}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TaGAT%3A%20Topology-Aware%20Graph%20Attention%20Network%20For%20Multi-modal%20Retinal%0A%20%20Image%20Fusion&body=Title%3A%20TaGAT%3A%20Topology-Aware%20Graph%20Attention%20Network%20For%20Multi-modal%20Retinal%0A%20%20Image%20Fusion%0AAuthor%3A%20Xin%20Tian%20and%20Nantheera%20Anantrasirichai%20and%20Lindsay%20Nicholson%20and%20Alin%20Achim%0AAbstract%3A%20%20%20In%20the%20realm%20of%20medical%20image%20fusion%2C%20integrating%20information%20from%20various%0Amodalities%20is%20crucial%20for%20improving%20diagnostics%20and%20treatment%20planning%2C%0Aespecially%20in%20retinal%20health%2C%20where%20the%20important%20features%20exhibit%20differently%0Ain%20different%20imaging%20modalities.%20Existing%20deep%20learning-based%20approaches%0Ainsufficiently%20focus%20on%20retinal%20image%20fusion%2C%20and%20thus%20fail%20to%20preserve%20enough%0Aanatomical%20structure%20and%20fine%20vessel%20details%20in%20retinal%20image%20fusion.%20To%0Aaddress%20this%2C%20we%20propose%20the%20Topology-Aware%20Graph%20Attention%20Network%20%28TaGAT%29%20for%0Amulti-modal%20retinal%20image%20fusion%2C%20leveraging%20a%20novel%20Topology-Aware%20Encoder%0A%28TAE%29%20with%20Graph%20Attention%20Networks%20%28GAT%29%20to%20effectively%20enhance%20spatial%0Afeatures%20with%20retinal%20vasculature%27s%20graph%20topology%20across%20modalities.%20The%20TAE%0Aencodes%20the%20base%20and%20detail%20features%2C%20extracted%20via%20a%20Long-short%20Range%20%28LSR%29%0Aencoder%20from%20retinal%20images%2C%20into%20the%20graph%20extracted%20from%20the%20retinal%20vessel.%0AWithin%20the%20TAE%2C%20the%20GAT-based%20Graph%20Information%20Update%20%28GIU%29%20block%20dynamically%0Arefines%20and%20aggregates%20the%20node%20features%20to%20generate%20topology-aware%20graph%0Afeatures.%20The%20updated%20graph%20features%20with%20base%20and%20detail%20features%20are%20combined%0Aand%20decoded%20as%20a%20fused%20image.%20Our%20model%20outperforms%20state-of-the-art%20methods%20in%0AFluorescein%20Fundus%20Angiography%20%28FFA%29%20with%20Color%20Fundus%20%28CF%29%20and%20Optical%0ACoherence%20Tomography%20%28OCT%29%20with%20confocal%20microscopy%20retinal%20image%20fusion.%20The%0Asource%20code%20can%20be%20accessed%20via%20https%3A//github.com/xintian-99/TaGAT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14188v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaGAT%253A%2520Topology-Aware%2520Graph%2520Attention%2520Network%2520For%2520Multi-modal%2520Retinal%250A%2520%2520Image%2520Fusion%26entry.906535625%3DXin%2520Tian%2520and%2520Nantheera%2520Anantrasirichai%2520and%2520Lindsay%2520Nicholson%2520and%2520Alin%2520Achim%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520medical%2520image%2520fusion%252C%2520integrating%2520information%2520from%2520various%250Amodalities%2520is%2520crucial%2520for%2520improving%2520diagnostics%2520and%2520treatment%2520planning%252C%250Aespecially%2520in%2520retinal%2520health%252C%2520where%2520the%2520important%2520features%2520exhibit%2520differently%250Ain%2520different%2520imaging%2520modalities.%2520Existing%2520deep%2520learning-based%2520approaches%250Ainsufficiently%2520focus%2520on%2520retinal%2520image%2520fusion%252C%2520and%2520thus%2520fail%2520to%2520preserve%2520enough%250Aanatomical%2520structure%2520and%2520fine%2520vessel%2520details%2520in%2520retinal%2520image%2520fusion.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520the%2520Topology-Aware%2520Graph%2520Attention%2520Network%2520%2528TaGAT%2529%2520for%250Amulti-modal%2520retinal%2520image%2520fusion%252C%2520leveraging%2520a%2520novel%2520Topology-Aware%2520Encoder%250A%2528TAE%2529%2520with%2520Graph%2520Attention%2520Networks%2520%2528GAT%2529%2520to%2520effectively%2520enhance%2520spatial%250Afeatures%2520with%2520retinal%2520vasculature%2527s%2520graph%2520topology%2520across%2520modalities.%2520The%2520TAE%250Aencodes%2520the%2520base%2520and%2520detail%2520features%252C%2520extracted%2520via%2520a%2520Long-short%2520Range%2520%2528LSR%2529%250Aencoder%2520from%2520retinal%2520images%252C%2520into%2520the%2520graph%2520extracted%2520from%2520the%2520retinal%2520vessel.%250AWithin%2520the%2520TAE%252C%2520the%2520GAT-based%2520Graph%2520Information%2520Update%2520%2528GIU%2529%2520block%2520dynamically%250Arefines%2520and%2520aggregates%2520the%2520node%2520features%2520to%2520generate%2520topology-aware%2520graph%250Afeatures.%2520The%2520updated%2520graph%2520features%2520with%2520base%2520and%2520detail%2520features%2520are%2520combined%250Aand%2520decoded%2520as%2520a%2520fused%2520image.%2520Our%2520model%2520outperforms%2520state-of-the-art%2520methods%2520in%250AFluorescein%2520Fundus%2520Angiography%2520%2528FFA%2529%2520with%2520Color%2520Fundus%2520%2528CF%2529%2520and%2520Optical%250ACoherence%2520Tomography%2520%2528OCT%2529%2520with%2520confocal%2520microscopy%2520retinal%2520image%2520fusion.%2520The%250Asource%2520code%2520can%2520be%2520accessed%2520via%2520https%253A//github.com/xintian-99/TaGAT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14188v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TaGAT%3A%20Topology-Aware%20Graph%20Attention%20Network%20For%20Multi-modal%20Retinal%0A%20%20Image%20Fusion&entry.906535625=Xin%20Tian%20and%20Nantheera%20Anantrasirichai%20and%20Lindsay%20Nicholson%20and%20Alin%20Achim&entry.1292438233=%20%20In%20the%20realm%20of%20medical%20image%20fusion%2C%20integrating%20information%20from%20various%0Amodalities%20is%20crucial%20for%20improving%20diagnostics%20and%20treatment%20planning%2C%0Aespecially%20in%20retinal%20health%2C%20where%20the%20important%20features%20exhibit%20differently%0Ain%20different%20imaging%20modalities.%20Existing%20deep%20learning-based%20approaches%0Ainsufficiently%20focus%20on%20retinal%20image%20fusion%2C%20and%20thus%20fail%20to%20preserve%20enough%0Aanatomical%20structure%20and%20fine%20vessel%20details%20in%20retinal%20image%20fusion.%20To%0Aaddress%20this%2C%20we%20propose%20the%20Topology-Aware%20Graph%20Attention%20Network%20%28TaGAT%29%20for%0Amulti-modal%20retinal%20image%20fusion%2C%20leveraging%20a%20novel%20Topology-Aware%20Encoder%0A%28TAE%29%20with%20Graph%20Attention%20Networks%20%28GAT%29%20to%20effectively%20enhance%20spatial%0Afeatures%20with%20retinal%20vasculature%27s%20graph%20topology%20across%20modalities.%20The%20TAE%0Aencodes%20the%20base%20and%20detail%20features%2C%20extracted%20via%20a%20Long-short%20Range%20%28LSR%29%0Aencoder%20from%20retinal%20images%2C%20into%20the%20graph%20extracted%20from%20the%20retinal%20vessel.%0AWithin%20the%20TAE%2C%20the%20GAT-based%20Graph%20Information%20Update%20%28GIU%29%20block%20dynamically%0Arefines%20and%20aggregates%20the%20node%20features%20to%20generate%20topology-aware%20graph%0Afeatures.%20The%20updated%20graph%20features%20with%20base%20and%20detail%20features%20are%20combined%0Aand%20decoded%20as%20a%20fused%20image.%20Our%20model%20outperforms%20state-of-the-art%20methods%20in%0AFluorescein%20Fundus%20Angiography%20%28FFA%29%20with%20Color%20Fundus%20%28CF%29%20and%20Optical%0ACoherence%20Tomography%20%28OCT%29%20with%20confocal%20microscopy%20retinal%20image%20fusion.%20The%0Asource%20code%20can%20be%20accessed%20via%20https%3A//github.com/xintian-99/TaGAT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14188v1&entry.124074799=Read"},
{"title": "Enhancing Layout Hotspot Detection Efficiency with YOLOv8 and PCA-Guided\n  Augmentation", "author": "Dongyang Wu and Siyang Wang and Mehdi Kamal and Massoud Pedram", "abstract": "  In this paper, we present a YOLO-based framework for layout hotspot\ndetection, aiming to enhance the efficiency and performance of the design rule\nchecking (DRC) process. Our approach leverages the YOLOv8 vision model to\ndetect multiple hotspots within each layout image, even when dealing with large\nlayout image sizes. Additionally, to enhance pattern-matching effectiveness, we\nintroduce a novel approach to augment the layout image using information\nextracted through Principal Component Analysis (PCA). The core of our proposed\nmethod is an algorithm that utilizes PCA to extract valuable auxiliary\ninformation from the layout image. This extracted information is then\nincorporated into the layout image as an additional color channel. This\naugmentation significantly improves the accuracy of multi-hotspot detection\nwhile reducing the false alarm rate of the object detection algorithm. We\nevaluate the effectiveness of our framework using four datasets generated from\nlayouts found in the ICCAD-2019 benchmark dataset. The results demonstrate that\nour framework achieves a precision (recall) of approximately 83% (86%) while\nmaintaining a false alarm rate of less than 7.4\\%. Also, the studies show that\nthe proposed augmentation approach could improve the detection ability of\nnever-seen-before (NSB) hotspots by about 10%.\n", "link": "http://arxiv.org/abs/2407.14498v1", "date": "2024-07-19", "relevancy": 2.685, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6188}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4996}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Layout%20Hotspot%20Detection%20Efficiency%20with%20YOLOv8%20and%20PCA-Guided%0A%20%20Augmentation&body=Title%3A%20Enhancing%20Layout%20Hotspot%20Detection%20Efficiency%20with%20YOLOv8%20and%20PCA-Guided%0A%20%20Augmentation%0AAuthor%3A%20Dongyang%20Wu%20and%20Siyang%20Wang%20and%20Mehdi%20Kamal%20and%20Massoud%20Pedram%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20YOLO-based%20framework%20for%20layout%20hotspot%0Adetection%2C%20aiming%20to%20enhance%20the%20efficiency%20and%20performance%20of%20the%20design%20rule%0Achecking%20%28DRC%29%20process.%20Our%20approach%20leverages%20the%20YOLOv8%20vision%20model%20to%0Adetect%20multiple%20hotspots%20within%20each%20layout%20image%2C%20even%20when%20dealing%20with%20large%0Alayout%20image%20sizes.%20Additionally%2C%20to%20enhance%20pattern-matching%20effectiveness%2C%20we%0Aintroduce%20a%20novel%20approach%20to%20augment%20the%20layout%20image%20using%20information%0Aextracted%20through%20Principal%20Component%20Analysis%20%28PCA%29.%20The%20core%20of%20our%20proposed%0Amethod%20is%20an%20algorithm%20that%20utilizes%20PCA%20to%20extract%20valuable%20auxiliary%0Ainformation%20from%20the%20layout%20image.%20This%20extracted%20information%20is%20then%0Aincorporated%20into%20the%20layout%20image%20as%20an%20additional%20color%20channel.%20This%0Aaugmentation%20significantly%20improves%20the%20accuracy%20of%20multi-hotspot%20detection%0Awhile%20reducing%20the%20false%20alarm%20rate%20of%20the%20object%20detection%20algorithm.%20We%0Aevaluate%20the%20effectiveness%20of%20our%20framework%20using%20four%20datasets%20generated%20from%0Alayouts%20found%20in%20the%20ICCAD-2019%20benchmark%20dataset.%20The%20results%20demonstrate%20that%0Aour%20framework%20achieves%20a%20precision%20%28recall%29%20of%20approximately%2083%25%20%2886%25%29%20while%0Amaintaining%20a%20false%20alarm%20rate%20of%20less%20than%207.4%5C%25.%20Also%2C%20the%20studies%20show%20that%0Athe%20proposed%20augmentation%20approach%20could%20improve%20the%20detection%20ability%20of%0Anever-seen-before%20%28NSB%29%20hotspots%20by%20about%2010%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14498v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Layout%2520Hotspot%2520Detection%2520Efficiency%2520with%2520YOLOv8%2520and%2520PCA-Guided%250A%2520%2520Augmentation%26entry.906535625%3DDongyang%2520Wu%2520and%2520Siyang%2520Wang%2520and%2520Mehdi%2520Kamal%2520and%2520Massoud%2520Pedram%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520YOLO-based%2520framework%2520for%2520layout%2520hotspot%250Adetection%252C%2520aiming%2520to%2520enhance%2520the%2520efficiency%2520and%2520performance%2520of%2520the%2520design%2520rule%250Achecking%2520%2528DRC%2529%2520process.%2520Our%2520approach%2520leverages%2520the%2520YOLOv8%2520vision%2520model%2520to%250Adetect%2520multiple%2520hotspots%2520within%2520each%2520layout%2520image%252C%2520even%2520when%2520dealing%2520with%2520large%250Alayout%2520image%2520sizes.%2520Additionally%252C%2520to%2520enhance%2520pattern-matching%2520effectiveness%252C%2520we%250Aintroduce%2520a%2520novel%2520approach%2520to%2520augment%2520the%2520layout%2520image%2520using%2520information%250Aextracted%2520through%2520Principal%2520Component%2520Analysis%2520%2528PCA%2529.%2520The%2520core%2520of%2520our%2520proposed%250Amethod%2520is%2520an%2520algorithm%2520that%2520utilizes%2520PCA%2520to%2520extract%2520valuable%2520auxiliary%250Ainformation%2520from%2520the%2520layout%2520image.%2520This%2520extracted%2520information%2520is%2520then%250Aincorporated%2520into%2520the%2520layout%2520image%2520as%2520an%2520additional%2520color%2520channel.%2520This%250Aaugmentation%2520significantly%2520improves%2520the%2520accuracy%2520of%2520multi-hotspot%2520detection%250Awhile%2520reducing%2520the%2520false%2520alarm%2520rate%2520of%2520the%2520object%2520detection%2520algorithm.%2520We%250Aevaluate%2520the%2520effectiveness%2520of%2520our%2520framework%2520using%2520four%2520datasets%2520generated%2520from%250Alayouts%2520found%2520in%2520the%2520ICCAD-2019%2520benchmark%2520dataset.%2520The%2520results%2520demonstrate%2520that%250Aour%2520framework%2520achieves%2520a%2520precision%2520%2528recall%2529%2520of%2520approximately%252083%2525%2520%252886%2525%2529%2520while%250Amaintaining%2520a%2520false%2520alarm%2520rate%2520of%2520less%2520than%25207.4%255C%2525.%2520Also%252C%2520the%2520studies%2520show%2520that%250Athe%2520proposed%2520augmentation%2520approach%2520could%2520improve%2520the%2520detection%2520ability%2520of%250Anever-seen-before%2520%2528NSB%2529%2520hotspots%2520by%2520about%252010%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14498v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Layout%20Hotspot%20Detection%20Efficiency%20with%20YOLOv8%20and%20PCA-Guided%0A%20%20Augmentation&entry.906535625=Dongyang%20Wu%20and%20Siyang%20Wang%20and%20Mehdi%20Kamal%20and%20Massoud%20Pedram&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20YOLO-based%20framework%20for%20layout%20hotspot%0Adetection%2C%20aiming%20to%20enhance%20the%20efficiency%20and%20performance%20of%20the%20design%20rule%0Achecking%20%28DRC%29%20process.%20Our%20approach%20leverages%20the%20YOLOv8%20vision%20model%20to%0Adetect%20multiple%20hotspots%20within%20each%20layout%20image%2C%20even%20when%20dealing%20with%20large%0Alayout%20image%20sizes.%20Additionally%2C%20to%20enhance%20pattern-matching%20effectiveness%2C%20we%0Aintroduce%20a%20novel%20approach%20to%20augment%20the%20layout%20image%20using%20information%0Aextracted%20through%20Principal%20Component%20Analysis%20%28PCA%29.%20The%20core%20of%20our%20proposed%0Amethod%20is%20an%20algorithm%20that%20utilizes%20PCA%20to%20extract%20valuable%20auxiliary%0Ainformation%20from%20the%20layout%20image.%20This%20extracted%20information%20is%20then%0Aincorporated%20into%20the%20layout%20image%20as%20an%20additional%20color%20channel.%20This%0Aaugmentation%20significantly%20improves%20the%20accuracy%20of%20multi-hotspot%20detection%0Awhile%20reducing%20the%20false%20alarm%20rate%20of%20the%20object%20detection%20algorithm.%20We%0Aevaluate%20the%20effectiveness%20of%20our%20framework%20using%20four%20datasets%20generated%20from%0Alayouts%20found%20in%20the%20ICCAD-2019%20benchmark%20dataset.%20The%20results%20demonstrate%20that%0Aour%20framework%20achieves%20a%20precision%20%28recall%29%20of%20approximately%2083%25%20%2886%25%29%20while%0Amaintaining%20a%20false%20alarm%20rate%20of%20less%20than%207.4%5C%25.%20Also%2C%20the%20studies%20show%20that%0Athe%20proposed%20augmentation%20approach%20could%20improve%20the%20detection%20ability%20of%0Anever-seen-before%20%28NSB%29%20hotspots%20by%20about%2010%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14498v1&entry.124074799=Read"},
{"title": "Open-Set Recognition in the Age of Vision-Language Models", "author": "Dimity Miller and Niko S\u00fcnderhauf and Alex Kenna and Keita Mason", "abstract": "  Are vision-language models (VLMs) for open-vocabulary perception inherently\nopen-set models because they are trained on internet-scale datasets? We answer\nthis question with a clear no - VLMs introduce closed-set assumptions via their\nfinite query set, making them vulnerable to open-set conditions. We\nsystematically evaluate VLMs for open-set recognition and find they frequently\nmisclassify objects not contained in their query set, leading to alarmingly low\nprecision when tuned for high recall and vice versa. We show that naively\nincreasing the size of the query set to contain more and more classes does not\nmitigate this problem, but instead causes diminishing task performance and\nopen-set performance. We establish a revised definition of the open-set problem\nfor the age of VLMs, define a new benchmark and evaluation protocol to\nfacilitate standardised evaluation and research in this important area, and\nevaluate promising baseline approaches based on predictive uncertainty and\ndedicated negative embeddings on a range of open-vocabulary VLM classifiers and\nobject detectors.\n", "link": "http://arxiv.org/abs/2403.16528v2", "date": "2024-07-19", "relevancy": 2.677, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5707}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5212}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-Set%20Recognition%20in%20the%20Age%20of%20Vision-Language%20Models&body=Title%3A%20Open-Set%20Recognition%20in%20the%20Age%20of%20Vision-Language%20Models%0AAuthor%3A%20Dimity%20Miller%20and%20Niko%20S%C3%BCnderhauf%20and%20Alex%20Kenna%20and%20Keita%20Mason%0AAbstract%3A%20%20%20Are%20vision-language%20models%20%28VLMs%29%20for%20open-vocabulary%20perception%20inherently%0Aopen-set%20models%20because%20they%20are%20trained%20on%20internet-scale%20datasets%3F%20We%20answer%0Athis%20question%20with%20a%20clear%20no%20-%20VLMs%20introduce%20closed-set%20assumptions%20via%20their%0Afinite%20query%20set%2C%20making%20them%20vulnerable%20to%20open-set%20conditions.%20We%0Asystematically%20evaluate%20VLMs%20for%20open-set%20recognition%20and%20find%20they%20frequently%0Amisclassify%20objects%20not%20contained%20in%20their%20query%20set%2C%20leading%20to%20alarmingly%20low%0Aprecision%20when%20tuned%20for%20high%20recall%20and%20vice%20versa.%20We%20show%20that%20naively%0Aincreasing%20the%20size%20of%20the%20query%20set%20to%20contain%20more%20and%20more%20classes%20does%20not%0Amitigate%20this%20problem%2C%20but%20instead%20causes%20diminishing%20task%20performance%20and%0Aopen-set%20performance.%20We%20establish%20a%20revised%20definition%20of%20the%20open-set%20problem%0Afor%20the%20age%20of%20VLMs%2C%20define%20a%20new%20benchmark%20and%20evaluation%20protocol%20to%0Afacilitate%20standardised%20evaluation%20and%20research%20in%20this%20important%20area%2C%20and%0Aevaluate%20promising%20baseline%20approaches%20based%20on%20predictive%20uncertainty%20and%0Adedicated%20negative%20embeddings%20on%20a%20range%20of%20open-vocabulary%20VLM%20classifiers%20and%0Aobject%20detectors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16528v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-Set%2520Recognition%2520in%2520the%2520Age%2520of%2520Vision-Language%2520Models%26entry.906535625%3DDimity%2520Miller%2520and%2520Niko%2520S%25C3%25BCnderhauf%2520and%2520Alex%2520Kenna%2520and%2520Keita%2520Mason%26entry.1292438233%3D%2520%2520Are%2520vision-language%2520models%2520%2528VLMs%2529%2520for%2520open-vocabulary%2520perception%2520inherently%250Aopen-set%2520models%2520because%2520they%2520are%2520trained%2520on%2520internet-scale%2520datasets%253F%2520We%2520answer%250Athis%2520question%2520with%2520a%2520clear%2520no%2520-%2520VLMs%2520introduce%2520closed-set%2520assumptions%2520via%2520their%250Afinite%2520query%2520set%252C%2520making%2520them%2520vulnerable%2520to%2520open-set%2520conditions.%2520We%250Asystematically%2520evaluate%2520VLMs%2520for%2520open-set%2520recognition%2520and%2520find%2520they%2520frequently%250Amisclassify%2520objects%2520not%2520contained%2520in%2520their%2520query%2520set%252C%2520leading%2520to%2520alarmingly%2520low%250Aprecision%2520when%2520tuned%2520for%2520high%2520recall%2520and%2520vice%2520versa.%2520We%2520show%2520that%2520naively%250Aincreasing%2520the%2520size%2520of%2520the%2520query%2520set%2520to%2520contain%2520more%2520and%2520more%2520classes%2520does%2520not%250Amitigate%2520this%2520problem%252C%2520but%2520instead%2520causes%2520diminishing%2520task%2520performance%2520and%250Aopen-set%2520performance.%2520We%2520establish%2520a%2520revised%2520definition%2520of%2520the%2520open-set%2520problem%250Afor%2520the%2520age%2520of%2520VLMs%252C%2520define%2520a%2520new%2520benchmark%2520and%2520evaluation%2520protocol%2520to%250Afacilitate%2520standardised%2520evaluation%2520and%2520research%2520in%2520this%2520important%2520area%252C%2520and%250Aevaluate%2520promising%2520baseline%2520approaches%2520based%2520on%2520predictive%2520uncertainty%2520and%250Adedicated%2520negative%2520embeddings%2520on%2520a%2520range%2520of%2520open-vocabulary%2520VLM%2520classifiers%2520and%250Aobject%2520detectors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16528v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-Set%20Recognition%20in%20the%20Age%20of%20Vision-Language%20Models&entry.906535625=Dimity%20Miller%20and%20Niko%20S%C3%BCnderhauf%20and%20Alex%20Kenna%20and%20Keita%20Mason&entry.1292438233=%20%20Are%20vision-language%20models%20%28VLMs%29%20for%20open-vocabulary%20perception%20inherently%0Aopen-set%20models%20because%20they%20are%20trained%20on%20internet-scale%20datasets%3F%20We%20answer%0Athis%20question%20with%20a%20clear%20no%20-%20VLMs%20introduce%20closed-set%20assumptions%20via%20their%0Afinite%20query%20set%2C%20making%20them%20vulnerable%20to%20open-set%20conditions.%20We%0Asystematically%20evaluate%20VLMs%20for%20open-set%20recognition%20and%20find%20they%20frequently%0Amisclassify%20objects%20not%20contained%20in%20their%20query%20set%2C%20leading%20to%20alarmingly%20low%0Aprecision%20when%20tuned%20for%20high%20recall%20and%20vice%20versa.%20We%20show%20that%20naively%0Aincreasing%20the%20size%20of%20the%20query%20set%20to%20contain%20more%20and%20more%20classes%20does%20not%0Amitigate%20this%20problem%2C%20but%20instead%20causes%20diminishing%20task%20performance%20and%0Aopen-set%20performance.%20We%20establish%20a%20revised%20definition%20of%20the%20open-set%20problem%0Afor%20the%20age%20of%20VLMs%2C%20define%20a%20new%20benchmark%20and%20evaluation%20protocol%20to%0Afacilitate%20standardised%20evaluation%20and%20research%20in%20this%20important%20area%2C%20and%0Aevaluate%20promising%20baseline%20approaches%20based%20on%20predictive%20uncertainty%20and%0Adedicated%20negative%20embeddings%20on%20a%20range%20of%20open-vocabulary%20VLM%20classifiers%20and%0Aobject%20detectors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16528v2&entry.124074799=Read"},
{"title": "Enhancing Facial Expression Recognition through Dual-Direction Attention\n  Mixed Feature Networks: Application to 7th ABAW Challenge", "author": "Josep Cabacas-Maso and Elena Ortega-Beltr\u00e1n and Ismael Benito-Altamirano and Carles Ventura", "abstract": "  We present our contribution to the 7th ABAW challenge at ECCV 2024, by\nutilizing a Dual-Direction Attention Mixed Feature Network for multitask facial\nexpression recognition we achieve results far beyond the proposed baseline for\nthe Multi-Task ABAW challenge. Our proposal uses the well-known DDAMFN\narchitecture as base to effectively predict valence-arousal, emotion\nrecognition, and action units. We demonstrate the architecture ability to\nhandle these tasks simultaneously, providing insights into its architecture and\nthe rationale behind its design. Additionally, we compare our results for a\nmultitask solution with independent single-task performance.\n", "link": "http://arxiv.org/abs/2407.12390v2", "date": "2024-07-19", "relevancy": 2.6658, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.535}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5332}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Facial%20Expression%20Recognition%20through%20Dual-Direction%20Attention%0A%20%20Mixed%20Feature%20Networks%3A%20Application%20to%207th%20ABAW%20Challenge&body=Title%3A%20Enhancing%20Facial%20Expression%20Recognition%20through%20Dual-Direction%20Attention%0A%20%20Mixed%20Feature%20Networks%3A%20Application%20to%207th%20ABAW%20Challenge%0AAuthor%3A%20Josep%20Cabacas-Maso%20and%20Elena%20Ortega-Beltr%C3%A1n%20and%20Ismael%20Benito-Altamirano%20and%20Carles%20Ventura%0AAbstract%3A%20%20%20We%20present%20our%20contribution%20to%20the%207th%20ABAW%20challenge%20at%20ECCV%202024%2C%20by%0Autilizing%20a%20Dual-Direction%20Attention%20Mixed%20Feature%20Network%20for%20multitask%20facial%0Aexpression%20recognition%20we%20achieve%20results%20far%20beyond%20the%20proposed%20baseline%20for%0Athe%20Multi-Task%20ABAW%20challenge.%20Our%20proposal%20uses%20the%20well-known%20DDAMFN%0Aarchitecture%20as%20base%20to%20effectively%20predict%20valence-arousal%2C%20emotion%0Arecognition%2C%20and%20action%20units.%20We%20demonstrate%20the%20architecture%20ability%20to%0Ahandle%20these%20tasks%20simultaneously%2C%20providing%20insights%20into%20its%20architecture%20and%0Athe%20rationale%20behind%20its%20design.%20Additionally%2C%20we%20compare%20our%20results%20for%20a%0Amultitask%20solution%20with%20independent%20single-task%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12390v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Facial%2520Expression%2520Recognition%2520through%2520Dual-Direction%2520Attention%250A%2520%2520Mixed%2520Feature%2520Networks%253A%2520Application%2520to%25207th%2520ABAW%2520Challenge%26entry.906535625%3DJosep%2520Cabacas-Maso%2520and%2520Elena%2520Ortega-Beltr%25C3%25A1n%2520and%2520Ismael%2520Benito-Altamirano%2520and%2520Carles%2520Ventura%26entry.1292438233%3D%2520%2520We%2520present%2520our%2520contribution%2520to%2520the%25207th%2520ABAW%2520challenge%2520at%2520ECCV%25202024%252C%2520by%250Autilizing%2520a%2520Dual-Direction%2520Attention%2520Mixed%2520Feature%2520Network%2520for%2520multitask%2520facial%250Aexpression%2520recognition%2520we%2520achieve%2520results%2520far%2520beyond%2520the%2520proposed%2520baseline%2520for%250Athe%2520Multi-Task%2520ABAW%2520challenge.%2520Our%2520proposal%2520uses%2520the%2520well-known%2520DDAMFN%250Aarchitecture%2520as%2520base%2520to%2520effectively%2520predict%2520valence-arousal%252C%2520emotion%250Arecognition%252C%2520and%2520action%2520units.%2520We%2520demonstrate%2520the%2520architecture%2520ability%2520to%250Ahandle%2520these%2520tasks%2520simultaneously%252C%2520providing%2520insights%2520into%2520its%2520architecture%2520and%250Athe%2520rationale%2520behind%2520its%2520design.%2520Additionally%252C%2520we%2520compare%2520our%2520results%2520for%2520a%250Amultitask%2520solution%2520with%2520independent%2520single-task%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12390v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Facial%20Expression%20Recognition%20through%20Dual-Direction%20Attention%0A%20%20Mixed%20Feature%20Networks%3A%20Application%20to%207th%20ABAW%20Challenge&entry.906535625=Josep%20Cabacas-Maso%20and%20Elena%20Ortega-Beltr%C3%A1n%20and%20Ismael%20Benito-Altamirano%20and%20Carles%20Ventura&entry.1292438233=%20%20We%20present%20our%20contribution%20to%20the%207th%20ABAW%20challenge%20at%20ECCV%202024%2C%20by%0Autilizing%20a%20Dual-Direction%20Attention%20Mixed%20Feature%20Network%20for%20multitask%20facial%0Aexpression%20recognition%20we%20achieve%20results%20far%20beyond%20the%20proposed%20baseline%20for%0Athe%20Multi-Task%20ABAW%20challenge.%20Our%20proposal%20uses%20the%20well-known%20DDAMFN%0Aarchitecture%20as%20base%20to%20effectively%20predict%20valence-arousal%2C%20emotion%0Arecognition%2C%20and%20action%20units.%20We%20demonstrate%20the%20architecture%20ability%20to%0Ahandle%20these%20tasks%20simultaneously%2C%20providing%20insights%20into%20its%20architecture%20and%0Athe%20rationale%20behind%20its%20design.%20Additionally%2C%20we%20compare%20our%20results%20for%20a%0Amultitask%20solution%20with%20independent%20single-task%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12390v2&entry.124074799=Read"},
{"title": "Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse\n  Autoencoders", "author": "Senthooran Rajamanoharan and Tom Lieberum and Nicolas Sonnerat and Arthur Conmy and Vikrant Varma and J\u00e1nos Kram\u00e1r and Neel Nanda", "abstract": "  Sparse autoencoders (SAEs) are a promising unsupervised approach for\nidentifying causally relevant and interpretable linear features in a language\nmodel's (LM) activations. To be useful for downstream tasks, SAEs need to\ndecompose LM activations faithfully; yet to be interpretable the decomposition\nmust be sparse -- two objectives that are in tension. In this paper, we\nintroduce JumpReLU SAEs, which achieve state-of-the-art reconstruction fidelity\nat a given sparsity level on Gemma 2 9B activations, compared to other recent\nadvances such as Gated and TopK SAEs. We also show that this improvement does\nnot come at the cost of interpretability through manual and automated\ninterpretability studies. JumpReLU SAEs are a simple modification of vanilla\n(ReLU) SAEs -- where we replace the ReLU with a discontinuous JumpReLU\nactivation function -- and are similarly efficient to train and run. By\nutilising straight-through-estimators (STEs) in a principled manner, we show\nhow it is possible to train JumpReLU SAEs effectively despite the discontinuous\nJumpReLU function introduced in the SAE's forward pass. Similarly, we use STEs\nto directly train L0 to be sparse, instead of training on proxies such as L1,\navoiding problems like shrinkage.\n", "link": "http://arxiv.org/abs/2407.14435v1", "date": "2024-07-19", "relevancy": 2.6151, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5606}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5082}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Jumping%20Ahead%3A%20Improving%20Reconstruction%20Fidelity%20with%20JumpReLU%20Sparse%0A%20%20Autoencoders&body=Title%3A%20Jumping%20Ahead%3A%20Improving%20Reconstruction%20Fidelity%20with%20JumpReLU%20Sparse%0A%20%20Autoencoders%0AAuthor%3A%20Senthooran%20Rajamanoharan%20and%20Tom%20Lieberum%20and%20Nicolas%20Sonnerat%20and%20Arthur%20Conmy%20and%20Vikrant%20Varma%20and%20J%C3%A1nos%20Kram%C3%A1r%20and%20Neel%20Nanda%0AAbstract%3A%20%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20a%20promising%20unsupervised%20approach%20for%0Aidentifying%20causally%20relevant%20and%20interpretable%20linear%20features%20in%20a%20language%0Amodel%27s%20%28LM%29%20activations.%20To%20be%20useful%20for%20downstream%20tasks%2C%20SAEs%20need%20to%0Adecompose%20LM%20activations%20faithfully%3B%20yet%20to%20be%20interpretable%20the%20decomposition%0Amust%20be%20sparse%20--%20two%20objectives%20that%20are%20in%20tension.%20In%20this%20paper%2C%20we%0Aintroduce%20JumpReLU%20SAEs%2C%20which%20achieve%20state-of-the-art%20reconstruction%20fidelity%0Aat%20a%20given%20sparsity%20level%20on%20Gemma%202%209B%20activations%2C%20compared%20to%20other%20recent%0Aadvances%20such%20as%20Gated%20and%20TopK%20SAEs.%20We%20also%20show%20that%20this%20improvement%20does%0Anot%20come%20at%20the%20cost%20of%20interpretability%20through%20manual%20and%20automated%0Ainterpretability%20studies.%20JumpReLU%20SAEs%20are%20a%20simple%20modification%20of%20vanilla%0A%28ReLU%29%20SAEs%20--%20where%20we%20replace%20the%20ReLU%20with%20a%20discontinuous%20JumpReLU%0Aactivation%20function%20--%20and%20are%20similarly%20efficient%20to%20train%20and%20run.%20By%0Autilising%20straight-through-estimators%20%28STEs%29%20in%20a%20principled%20manner%2C%20we%20show%0Ahow%20it%20is%20possible%20to%20train%20JumpReLU%20SAEs%20effectively%20despite%20the%20discontinuous%0AJumpReLU%20function%20introduced%20in%20the%20SAE%27s%20forward%20pass.%20Similarly%2C%20we%20use%20STEs%0Ato%20directly%20train%20L0%20to%20be%20sparse%2C%20instead%20of%20training%20on%20proxies%20such%20as%20L1%2C%0Aavoiding%20problems%20like%20shrinkage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14435v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJumping%2520Ahead%253A%2520Improving%2520Reconstruction%2520Fidelity%2520with%2520JumpReLU%2520Sparse%250A%2520%2520Autoencoders%26entry.906535625%3DSenthooran%2520Rajamanoharan%2520and%2520Tom%2520Lieberum%2520and%2520Nicolas%2520Sonnerat%2520and%2520Arthur%2520Conmy%2520and%2520Vikrant%2520Varma%2520and%2520J%25C3%25A1nos%2520Kram%25C3%25A1r%2520and%2520Neel%2520Nanda%26entry.1292438233%3D%2520%2520Sparse%2520autoencoders%2520%2528SAEs%2529%2520are%2520a%2520promising%2520unsupervised%2520approach%2520for%250Aidentifying%2520causally%2520relevant%2520and%2520interpretable%2520linear%2520features%2520in%2520a%2520language%250Amodel%2527s%2520%2528LM%2529%2520activations.%2520To%2520be%2520useful%2520for%2520downstream%2520tasks%252C%2520SAEs%2520need%2520to%250Adecompose%2520LM%2520activations%2520faithfully%253B%2520yet%2520to%2520be%2520interpretable%2520the%2520decomposition%250Amust%2520be%2520sparse%2520--%2520two%2520objectives%2520that%2520are%2520in%2520tension.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520JumpReLU%2520SAEs%252C%2520which%2520achieve%2520state-of-the-art%2520reconstruction%2520fidelity%250Aat%2520a%2520given%2520sparsity%2520level%2520on%2520Gemma%25202%25209B%2520activations%252C%2520compared%2520to%2520other%2520recent%250Aadvances%2520such%2520as%2520Gated%2520and%2520TopK%2520SAEs.%2520We%2520also%2520show%2520that%2520this%2520improvement%2520does%250Anot%2520come%2520at%2520the%2520cost%2520of%2520interpretability%2520through%2520manual%2520and%2520automated%250Ainterpretability%2520studies.%2520JumpReLU%2520SAEs%2520are%2520a%2520simple%2520modification%2520of%2520vanilla%250A%2528ReLU%2529%2520SAEs%2520--%2520where%2520we%2520replace%2520the%2520ReLU%2520with%2520a%2520discontinuous%2520JumpReLU%250Aactivation%2520function%2520--%2520and%2520are%2520similarly%2520efficient%2520to%2520train%2520and%2520run.%2520By%250Autilising%2520straight-through-estimators%2520%2528STEs%2529%2520in%2520a%2520principled%2520manner%252C%2520we%2520show%250Ahow%2520it%2520is%2520possible%2520to%2520train%2520JumpReLU%2520SAEs%2520effectively%2520despite%2520the%2520discontinuous%250AJumpReLU%2520function%2520introduced%2520in%2520the%2520SAE%2527s%2520forward%2520pass.%2520Similarly%252C%2520we%2520use%2520STEs%250Ato%2520directly%2520train%2520L0%2520to%2520be%2520sparse%252C%2520instead%2520of%2520training%2520on%2520proxies%2520such%2520as%2520L1%252C%250Aavoiding%2520problems%2520like%2520shrinkage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14435v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Jumping%20Ahead%3A%20Improving%20Reconstruction%20Fidelity%20with%20JumpReLU%20Sparse%0A%20%20Autoencoders&entry.906535625=Senthooran%20Rajamanoharan%20and%20Tom%20Lieberum%20and%20Nicolas%20Sonnerat%20and%20Arthur%20Conmy%20and%20Vikrant%20Varma%20and%20J%C3%A1nos%20Kram%C3%A1r%20and%20Neel%20Nanda&entry.1292438233=%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20a%20promising%20unsupervised%20approach%20for%0Aidentifying%20causally%20relevant%20and%20interpretable%20linear%20features%20in%20a%20language%0Amodel%27s%20%28LM%29%20activations.%20To%20be%20useful%20for%20downstream%20tasks%2C%20SAEs%20need%20to%0Adecompose%20LM%20activations%20faithfully%3B%20yet%20to%20be%20interpretable%20the%20decomposition%0Amust%20be%20sparse%20--%20two%20objectives%20that%20are%20in%20tension.%20In%20this%20paper%2C%20we%0Aintroduce%20JumpReLU%20SAEs%2C%20which%20achieve%20state-of-the-art%20reconstruction%20fidelity%0Aat%20a%20given%20sparsity%20level%20on%20Gemma%202%209B%20activations%2C%20compared%20to%20other%20recent%0Aadvances%20such%20as%20Gated%20and%20TopK%20SAEs.%20We%20also%20show%20that%20this%20improvement%20does%0Anot%20come%20at%20the%20cost%20of%20interpretability%20through%20manual%20and%20automated%0Ainterpretability%20studies.%20JumpReLU%20SAEs%20are%20a%20simple%20modification%20of%20vanilla%0A%28ReLU%29%20SAEs%20--%20where%20we%20replace%20the%20ReLU%20with%20a%20discontinuous%20JumpReLU%0Aactivation%20function%20--%20and%20are%20similarly%20efficient%20to%20train%20and%20run.%20By%0Autilising%20straight-through-estimators%20%28STEs%29%20in%20a%20principled%20manner%2C%20we%20show%0Ahow%20it%20is%20possible%20to%20train%20JumpReLU%20SAEs%20effectively%20despite%20the%20discontinuous%0AJumpReLU%20function%20introduced%20in%20the%20SAE%27s%20forward%20pass.%20Similarly%2C%20we%20use%20STEs%0Ato%20directly%20train%20L0%20to%20be%20sparse%2C%20instead%20of%20training%20on%20proxies%20such%20as%20L1%2C%0Aavoiding%20problems%20like%20shrinkage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14435v1&entry.124074799=Read"},
{"title": "DUPLEX: Dual GAT for Complex Embedding of Directed Graphs", "author": "Zhaoru Ke and Hang Yu and Jianguo Li and Haipeng Zhang", "abstract": "  Current directed graph embedding methods build upon undirected techniques but\noften inadequately capture directed edge information, leading to challenges\nsuch as: (1) Suboptimal representations for nodes with low in/out-degrees, due\nto the insufficient neighbor interactions; (2) Limited inductive ability for\nrepresenting new nodes post-training; (3) Narrow generalizability, as training\nis overly coupled with specific tasks. In response, we propose DUPLEX, an\ninductive framework for complex embeddings of directed graphs. It (1) leverages\nHermitian adjacency matrix decomposition for comprehensive neighbor\nintegration, (2) employs a dual GAT encoder for directional neighbor modeling,\nand (3) features two parameter-free decoders to decouple training from\nparticular tasks. DUPLEX outperforms state-of-the-art models, especially for\nnodes with sparse connectivity, and demonstrates robust inductive capability\nand adaptability across various tasks. The code is available at\nhttps://github.com/alipay/DUPLEX.\n", "link": "http://arxiv.org/abs/2406.05391v2", "date": "2024-07-19", "relevancy": 2.5663, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5224}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5137}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DUPLEX%3A%20Dual%20GAT%20for%20Complex%20Embedding%20of%20Directed%20Graphs&body=Title%3A%20DUPLEX%3A%20Dual%20GAT%20for%20Complex%20Embedding%20of%20Directed%20Graphs%0AAuthor%3A%20Zhaoru%20Ke%20and%20Hang%20Yu%20and%20Jianguo%20Li%20and%20Haipeng%20Zhang%0AAbstract%3A%20%20%20Current%20directed%20graph%20embedding%20methods%20build%20upon%20undirected%20techniques%20but%0Aoften%20inadequately%20capture%20directed%20edge%20information%2C%20leading%20to%20challenges%0Asuch%20as%3A%20%281%29%20Suboptimal%20representations%20for%20nodes%20with%20low%20in/out-degrees%2C%20due%0Ato%20the%20insufficient%20neighbor%20interactions%3B%20%282%29%20Limited%20inductive%20ability%20for%0Arepresenting%20new%20nodes%20post-training%3B%20%283%29%20Narrow%20generalizability%2C%20as%20training%0Ais%20overly%20coupled%20with%20specific%20tasks.%20In%20response%2C%20we%20propose%20DUPLEX%2C%20an%0Ainductive%20framework%20for%20complex%20embeddings%20of%20directed%20graphs.%20It%20%281%29%20leverages%0AHermitian%20adjacency%20matrix%20decomposition%20for%20comprehensive%20neighbor%0Aintegration%2C%20%282%29%20employs%20a%20dual%20GAT%20encoder%20for%20directional%20neighbor%20modeling%2C%0Aand%20%283%29%20features%20two%20parameter-free%20decoders%20to%20decouple%20training%20from%0Aparticular%20tasks.%20DUPLEX%20outperforms%20state-of-the-art%20models%2C%20especially%20for%0Anodes%20with%20sparse%20connectivity%2C%20and%20demonstrates%20robust%20inductive%20capability%0Aand%20adaptability%20across%20various%20tasks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/alipay/DUPLEX.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05391v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDUPLEX%253A%2520Dual%2520GAT%2520for%2520Complex%2520Embedding%2520of%2520Directed%2520Graphs%26entry.906535625%3DZhaoru%2520Ke%2520and%2520Hang%2520Yu%2520and%2520Jianguo%2520Li%2520and%2520Haipeng%2520Zhang%26entry.1292438233%3D%2520%2520Current%2520directed%2520graph%2520embedding%2520methods%2520build%2520upon%2520undirected%2520techniques%2520but%250Aoften%2520inadequately%2520capture%2520directed%2520edge%2520information%252C%2520leading%2520to%2520challenges%250Asuch%2520as%253A%2520%25281%2529%2520Suboptimal%2520representations%2520for%2520nodes%2520with%2520low%2520in/out-degrees%252C%2520due%250Ato%2520the%2520insufficient%2520neighbor%2520interactions%253B%2520%25282%2529%2520Limited%2520inductive%2520ability%2520for%250Arepresenting%2520new%2520nodes%2520post-training%253B%2520%25283%2529%2520Narrow%2520generalizability%252C%2520as%2520training%250Ais%2520overly%2520coupled%2520with%2520specific%2520tasks.%2520In%2520response%252C%2520we%2520propose%2520DUPLEX%252C%2520an%250Ainductive%2520framework%2520for%2520complex%2520embeddings%2520of%2520directed%2520graphs.%2520It%2520%25281%2529%2520leverages%250AHermitian%2520adjacency%2520matrix%2520decomposition%2520for%2520comprehensive%2520neighbor%250Aintegration%252C%2520%25282%2529%2520employs%2520a%2520dual%2520GAT%2520encoder%2520for%2520directional%2520neighbor%2520modeling%252C%250Aand%2520%25283%2529%2520features%2520two%2520parameter-free%2520decoders%2520to%2520decouple%2520training%2520from%250Aparticular%2520tasks.%2520DUPLEX%2520outperforms%2520state-of-the-art%2520models%252C%2520especially%2520for%250Anodes%2520with%2520sparse%2520connectivity%252C%2520and%2520demonstrates%2520robust%2520inductive%2520capability%250Aand%2520adaptability%2520across%2520various%2520tasks.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/alipay/DUPLEX.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05391v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DUPLEX%3A%20Dual%20GAT%20for%20Complex%20Embedding%20of%20Directed%20Graphs&entry.906535625=Zhaoru%20Ke%20and%20Hang%20Yu%20and%20Jianguo%20Li%20and%20Haipeng%20Zhang&entry.1292438233=%20%20Current%20directed%20graph%20embedding%20methods%20build%20upon%20undirected%20techniques%20but%0Aoften%20inadequately%20capture%20directed%20edge%20information%2C%20leading%20to%20challenges%0Asuch%20as%3A%20%281%29%20Suboptimal%20representations%20for%20nodes%20with%20low%20in/out-degrees%2C%20due%0Ato%20the%20insufficient%20neighbor%20interactions%3B%20%282%29%20Limited%20inductive%20ability%20for%0Arepresenting%20new%20nodes%20post-training%3B%20%283%29%20Narrow%20generalizability%2C%20as%20training%0Ais%20overly%20coupled%20with%20specific%20tasks.%20In%20response%2C%20we%20propose%20DUPLEX%2C%20an%0Ainductive%20framework%20for%20complex%20embeddings%20of%20directed%20graphs.%20It%20%281%29%20leverages%0AHermitian%20adjacency%20matrix%20decomposition%20for%20comprehensive%20neighbor%0Aintegration%2C%20%282%29%20employs%20a%20dual%20GAT%20encoder%20for%20directional%20neighbor%20modeling%2C%0Aand%20%283%29%20features%20two%20parameter-free%20decoders%20to%20decouple%20training%20from%0Aparticular%20tasks.%20DUPLEX%20outperforms%20state-of-the-art%20models%2C%20especially%20for%0Anodes%20with%20sparse%20connectivity%2C%20and%20demonstrates%20robust%20inductive%20capability%0Aand%20adaptability%20across%20various%20tasks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/alipay/DUPLEX.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05391v2&entry.124074799=Read"},
{"title": "L^2CL: Embarrassingly Simple Layer-to-Layer Contrastive Learning for\n  Graph Collaborative Filtering", "author": "Xinzhou Jin and Jintang Li and Liang Chen and Chenyun Yu and Yuanzhen Xie and Tao Xie and Chengxiang Zhuo and Zang Li and Zibin Zheng", "abstract": "  Graph neural networks (GNNs) have recently emerged as an effective approach\nto model neighborhood signals in collaborative filtering. Towards this research\nline, graph contrastive learning (GCL) demonstrates robust capabilities to\naddress the supervision label shortage issue through generating massive\nself-supervised signals. Despite its effectiveness, GCL for recommendation\nsuffers seriously from two main challenges: i) GCL relies on graph augmentation\nto generate semantically different views for contrasting, which could\npotentially disrupt key information and introduce unwanted noise; ii) current\nworks for GCL primarily focus on contrasting representations using\nsophisticated networks architecture (usually deep) to capture high-order\ninteractions, which leads to increased computational complexity and suboptimal\ntraining efficiency. To this end, we propose L2CL, a principled Layer-to-Layer\nContrastive Learning framework that contrasts representations from different\nlayers. By aligning the semantic similarities between different layers, L2CL\nenables the learning of complex structural relationships and gets rid of the\nnoise perturbation in stochastic data augmentation. Surprisingly, we find that\nL2CL, using only one-hop contrastive learning paradigm, is able to capture\nintrinsic semantic structures and improve the quality of node representation,\nleading to a simple yet effective architecture. We also provide theoretical\nguarantees for L2CL in minimizing task-irrelevant information. Extensive\nexperiments on five real-world datasets demonstrate the superiority of our\nmodel over various state-of-the-art collaborative filtering methods. Our code\nis available at https://github.com/downeykking/L2CL.\n", "link": "http://arxiv.org/abs/2407.14266v1", "date": "2024-07-19", "relevancy": 2.5585, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5576}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4933}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20L%5E2CL%3A%20Embarrassingly%20Simple%20Layer-to-Layer%20Contrastive%20Learning%20for%0A%20%20Graph%20Collaborative%20Filtering&body=Title%3A%20L%5E2CL%3A%20Embarrassingly%20Simple%20Layer-to-Layer%20Contrastive%20Learning%20for%0A%20%20Graph%20Collaborative%20Filtering%0AAuthor%3A%20Xinzhou%20Jin%20and%20Jintang%20Li%20and%20Liang%20Chen%20and%20Chenyun%20Yu%20and%20Yuanzhen%20Xie%20and%20Tao%20Xie%20and%20Chengxiang%20Zhuo%20and%20Zang%20Li%20and%20Zibin%20Zheng%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20recently%20emerged%20as%20an%20effective%20approach%0Ato%20model%20neighborhood%20signals%20in%20collaborative%20filtering.%20Towards%20this%20research%0Aline%2C%20graph%20contrastive%20learning%20%28GCL%29%20demonstrates%20robust%20capabilities%20to%0Aaddress%20the%20supervision%20label%20shortage%20issue%20through%20generating%20massive%0Aself-supervised%20signals.%20Despite%20its%20effectiveness%2C%20GCL%20for%20recommendation%0Asuffers%20seriously%20from%20two%20main%20challenges%3A%20i%29%20GCL%20relies%20on%20graph%20augmentation%0Ato%20generate%20semantically%20different%20views%20for%20contrasting%2C%20which%20could%0Apotentially%20disrupt%20key%20information%20and%20introduce%20unwanted%20noise%3B%20ii%29%20current%0Aworks%20for%20GCL%20primarily%20focus%20on%20contrasting%20representations%20using%0Asophisticated%20networks%20architecture%20%28usually%20deep%29%20to%20capture%20high-order%0Ainteractions%2C%20which%20leads%20to%20increased%20computational%20complexity%20and%20suboptimal%0Atraining%20efficiency.%20To%20this%20end%2C%20we%20propose%20L2CL%2C%20a%20principled%20Layer-to-Layer%0AContrastive%20Learning%20framework%20that%20contrasts%20representations%20from%20different%0Alayers.%20By%20aligning%20the%20semantic%20similarities%20between%20different%20layers%2C%20L2CL%0Aenables%20the%20learning%20of%20complex%20structural%20relationships%20and%20gets%20rid%20of%20the%0Anoise%20perturbation%20in%20stochastic%20data%20augmentation.%20Surprisingly%2C%20we%20find%20that%0AL2CL%2C%20using%20only%20one-hop%20contrastive%20learning%20paradigm%2C%20is%20able%20to%20capture%0Aintrinsic%20semantic%20structures%20and%20improve%20the%20quality%20of%20node%20representation%2C%0Aleading%20to%20a%20simple%20yet%20effective%20architecture.%20We%20also%20provide%20theoretical%0Aguarantees%20for%20L2CL%20in%20minimizing%20task-irrelevant%20information.%20Extensive%0Aexperiments%20on%20five%20real-world%20datasets%20demonstrate%20the%20superiority%20of%20our%0Amodel%20over%20various%20state-of-the-art%20collaborative%20filtering%20methods.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/downeykking/L2CL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14266v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DL%255E2CL%253A%2520Embarrassingly%2520Simple%2520Layer-to-Layer%2520Contrastive%2520Learning%2520for%250A%2520%2520Graph%2520Collaborative%2520Filtering%26entry.906535625%3DXinzhou%2520Jin%2520and%2520Jintang%2520Li%2520and%2520Liang%2520Chen%2520and%2520Chenyun%2520Yu%2520and%2520Yuanzhen%2520Xie%2520and%2520Tao%2520Xie%2520and%2520Chengxiang%2520Zhuo%2520and%2520Zang%2520Li%2520and%2520Zibin%2520Zheng%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520recently%2520emerged%2520as%2520an%2520effective%2520approach%250Ato%2520model%2520neighborhood%2520signals%2520in%2520collaborative%2520filtering.%2520Towards%2520this%2520research%250Aline%252C%2520graph%2520contrastive%2520learning%2520%2528GCL%2529%2520demonstrates%2520robust%2520capabilities%2520to%250Aaddress%2520the%2520supervision%2520label%2520shortage%2520issue%2520through%2520generating%2520massive%250Aself-supervised%2520signals.%2520Despite%2520its%2520effectiveness%252C%2520GCL%2520for%2520recommendation%250Asuffers%2520seriously%2520from%2520two%2520main%2520challenges%253A%2520i%2529%2520GCL%2520relies%2520on%2520graph%2520augmentation%250Ato%2520generate%2520semantically%2520different%2520views%2520for%2520contrasting%252C%2520which%2520could%250Apotentially%2520disrupt%2520key%2520information%2520and%2520introduce%2520unwanted%2520noise%253B%2520ii%2529%2520current%250Aworks%2520for%2520GCL%2520primarily%2520focus%2520on%2520contrasting%2520representations%2520using%250Asophisticated%2520networks%2520architecture%2520%2528usually%2520deep%2529%2520to%2520capture%2520high-order%250Ainteractions%252C%2520which%2520leads%2520to%2520increased%2520computational%2520complexity%2520and%2520suboptimal%250Atraining%2520efficiency.%2520To%2520this%2520end%252C%2520we%2520propose%2520L2CL%252C%2520a%2520principled%2520Layer-to-Layer%250AContrastive%2520Learning%2520framework%2520that%2520contrasts%2520representations%2520from%2520different%250Alayers.%2520By%2520aligning%2520the%2520semantic%2520similarities%2520between%2520different%2520layers%252C%2520L2CL%250Aenables%2520the%2520learning%2520of%2520complex%2520structural%2520relationships%2520and%2520gets%2520rid%2520of%2520the%250Anoise%2520perturbation%2520in%2520stochastic%2520data%2520augmentation.%2520Surprisingly%252C%2520we%2520find%2520that%250AL2CL%252C%2520using%2520only%2520one-hop%2520contrastive%2520learning%2520paradigm%252C%2520is%2520able%2520to%2520capture%250Aintrinsic%2520semantic%2520structures%2520and%2520improve%2520the%2520quality%2520of%2520node%2520representation%252C%250Aleading%2520to%2520a%2520simple%2520yet%2520effective%2520architecture.%2520We%2520also%2520provide%2520theoretical%250Aguarantees%2520for%2520L2CL%2520in%2520minimizing%2520task-irrelevant%2520information.%2520Extensive%250Aexperiments%2520on%2520five%2520real-world%2520datasets%2520demonstrate%2520the%2520superiority%2520of%2520our%250Amodel%2520over%2520various%2520state-of-the-art%2520collaborative%2520filtering%2520methods.%2520Our%2520code%250Ais%2520available%2520at%2520https%253A//github.com/downeykking/L2CL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14266v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=L%5E2CL%3A%20Embarrassingly%20Simple%20Layer-to-Layer%20Contrastive%20Learning%20for%0A%20%20Graph%20Collaborative%20Filtering&entry.906535625=Xinzhou%20Jin%20and%20Jintang%20Li%20and%20Liang%20Chen%20and%20Chenyun%20Yu%20and%20Yuanzhen%20Xie%20and%20Tao%20Xie%20and%20Chengxiang%20Zhuo%20and%20Zang%20Li%20and%20Zibin%20Zheng&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20recently%20emerged%20as%20an%20effective%20approach%0Ato%20model%20neighborhood%20signals%20in%20collaborative%20filtering.%20Towards%20this%20research%0Aline%2C%20graph%20contrastive%20learning%20%28GCL%29%20demonstrates%20robust%20capabilities%20to%0Aaddress%20the%20supervision%20label%20shortage%20issue%20through%20generating%20massive%0Aself-supervised%20signals.%20Despite%20its%20effectiveness%2C%20GCL%20for%20recommendation%0Asuffers%20seriously%20from%20two%20main%20challenges%3A%20i%29%20GCL%20relies%20on%20graph%20augmentation%0Ato%20generate%20semantically%20different%20views%20for%20contrasting%2C%20which%20could%0Apotentially%20disrupt%20key%20information%20and%20introduce%20unwanted%20noise%3B%20ii%29%20current%0Aworks%20for%20GCL%20primarily%20focus%20on%20contrasting%20representations%20using%0Asophisticated%20networks%20architecture%20%28usually%20deep%29%20to%20capture%20high-order%0Ainteractions%2C%20which%20leads%20to%20increased%20computational%20complexity%20and%20suboptimal%0Atraining%20efficiency.%20To%20this%20end%2C%20we%20propose%20L2CL%2C%20a%20principled%20Layer-to-Layer%0AContrastive%20Learning%20framework%20that%20contrasts%20representations%20from%20different%0Alayers.%20By%20aligning%20the%20semantic%20similarities%20between%20different%20layers%2C%20L2CL%0Aenables%20the%20learning%20of%20complex%20structural%20relationships%20and%20gets%20rid%20of%20the%0Anoise%20perturbation%20in%20stochastic%20data%20augmentation.%20Surprisingly%2C%20we%20find%20that%0AL2CL%2C%20using%20only%20one-hop%20contrastive%20learning%20paradigm%2C%20is%20able%20to%20capture%0Aintrinsic%20semantic%20structures%20and%20improve%20the%20quality%20of%20node%20representation%2C%0Aleading%20to%20a%20simple%20yet%20effective%20architecture.%20We%20also%20provide%20theoretical%0Aguarantees%20for%20L2CL%20in%20minimizing%20task-irrelevant%20information.%20Extensive%0Aexperiments%20on%20five%20real-world%20datasets%20demonstrate%20the%20superiority%20of%20our%0Amodel%20over%20various%20state-of-the-art%20collaborative%20filtering%20methods.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/downeykking/L2CL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14266v1&entry.124074799=Read"},
{"title": "PACE: A Large-Scale Dataset with Pose Annotations in Cluttered\n  Environments", "author": "Yang You and Kai Xiong and Zhening Yang and Zhengxiang Huang and Junwei Zhou and Ruoxi Shi and Zhou Fang and Adam W. Harley and Leonidas Guibas and Cewu Lu", "abstract": "  We introduce PACE (Pose Annotations in Cluttered Environments), a large-scale\nbenchmark designed to advance the development and evaluation of pose estimation\nmethods in cluttered scenarios. PACE provides a large-scale real-world\nbenchmark for both instance-level and category-level settings. The benchmark\nconsists of 55K frames with 258K annotations across 300 videos, covering 238\nobjects from 43 categories and featuring a mix of rigid and articulated items\nin cluttered scenes. To annotate the real-world data efficiently, we develop an\ninnovative annotation system with a calibrated 3-camera setup. Additionally, we\noffer PACE-Sim, which contains 100K photo-realistic simulated frames with 2.4M\nannotations across 931 objects. We test state-of-the-art algorithms in PACE\nalong two tracks: pose estimation, and object pose tracking, revealing the\nbenchmark's challenges and research opportunities. Our benchmark code and data\nis available on https://github.com/qq456cvb/PACE.\n", "link": "http://arxiv.org/abs/2312.15130v3", "date": "2024-07-19", "relevancy": 2.5582, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5199}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5108}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PACE%3A%20A%20Large-Scale%20Dataset%20with%20Pose%20Annotations%20in%20Cluttered%0A%20%20Environments&body=Title%3A%20PACE%3A%20A%20Large-Scale%20Dataset%20with%20Pose%20Annotations%20in%20Cluttered%0A%20%20Environments%0AAuthor%3A%20Yang%20You%20and%20Kai%20Xiong%20and%20Zhening%20Yang%20and%20Zhengxiang%20Huang%20and%20Junwei%20Zhou%20and%20Ruoxi%20Shi%20and%20Zhou%20Fang%20and%20Adam%20W.%20Harley%20and%20Leonidas%20Guibas%20and%20Cewu%20Lu%0AAbstract%3A%20%20%20We%20introduce%20PACE%20%28Pose%20Annotations%20in%20Cluttered%20Environments%29%2C%20a%20large-scale%0Abenchmark%20designed%20to%20advance%20the%20development%20and%20evaluation%20of%20pose%20estimation%0Amethods%20in%20cluttered%20scenarios.%20PACE%20provides%20a%20large-scale%20real-world%0Abenchmark%20for%20both%20instance-level%20and%20category-level%20settings.%20The%20benchmark%0Aconsists%20of%2055K%20frames%20with%20258K%20annotations%20across%20300%20videos%2C%20covering%20238%0Aobjects%20from%2043%20categories%20and%20featuring%20a%20mix%20of%20rigid%20and%20articulated%20items%0Ain%20cluttered%20scenes.%20To%20annotate%20the%20real-world%20data%20efficiently%2C%20we%20develop%20an%0Ainnovative%20annotation%20system%20with%20a%20calibrated%203-camera%20setup.%20Additionally%2C%20we%0Aoffer%20PACE-Sim%2C%20which%20contains%20100K%20photo-realistic%20simulated%20frames%20with%202.4M%0Aannotations%20across%20931%20objects.%20We%20test%20state-of-the-art%20algorithms%20in%20PACE%0Aalong%20two%20tracks%3A%20pose%20estimation%2C%20and%20object%20pose%20tracking%2C%20revealing%20the%0Abenchmark%27s%20challenges%20and%20research%20opportunities.%20Our%20benchmark%20code%20and%20data%0Ais%20available%20on%20https%3A//github.com/qq456cvb/PACE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.15130v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPACE%253A%2520A%2520Large-Scale%2520Dataset%2520with%2520Pose%2520Annotations%2520in%2520Cluttered%250A%2520%2520Environments%26entry.906535625%3DYang%2520You%2520and%2520Kai%2520Xiong%2520and%2520Zhening%2520Yang%2520and%2520Zhengxiang%2520Huang%2520and%2520Junwei%2520Zhou%2520and%2520Ruoxi%2520Shi%2520and%2520Zhou%2520Fang%2520and%2520Adam%2520W.%2520Harley%2520and%2520Leonidas%2520Guibas%2520and%2520Cewu%2520Lu%26entry.1292438233%3D%2520%2520We%2520introduce%2520PACE%2520%2528Pose%2520Annotations%2520in%2520Cluttered%2520Environments%2529%252C%2520a%2520large-scale%250Abenchmark%2520designed%2520to%2520advance%2520the%2520development%2520and%2520evaluation%2520of%2520pose%2520estimation%250Amethods%2520in%2520cluttered%2520scenarios.%2520PACE%2520provides%2520a%2520large-scale%2520real-world%250Abenchmark%2520for%2520both%2520instance-level%2520and%2520category-level%2520settings.%2520The%2520benchmark%250Aconsists%2520of%252055K%2520frames%2520with%2520258K%2520annotations%2520across%2520300%2520videos%252C%2520covering%2520238%250Aobjects%2520from%252043%2520categories%2520and%2520featuring%2520a%2520mix%2520of%2520rigid%2520and%2520articulated%2520items%250Ain%2520cluttered%2520scenes.%2520To%2520annotate%2520the%2520real-world%2520data%2520efficiently%252C%2520we%2520develop%2520an%250Ainnovative%2520annotation%2520system%2520with%2520a%2520calibrated%25203-camera%2520setup.%2520Additionally%252C%2520we%250Aoffer%2520PACE-Sim%252C%2520which%2520contains%2520100K%2520photo-realistic%2520simulated%2520frames%2520with%25202.4M%250Aannotations%2520across%2520931%2520objects.%2520We%2520test%2520state-of-the-art%2520algorithms%2520in%2520PACE%250Aalong%2520two%2520tracks%253A%2520pose%2520estimation%252C%2520and%2520object%2520pose%2520tracking%252C%2520revealing%2520the%250Abenchmark%2527s%2520challenges%2520and%2520research%2520opportunities.%2520Our%2520benchmark%2520code%2520and%2520data%250Ais%2520available%2520on%2520https%253A//github.com/qq456cvb/PACE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.15130v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PACE%3A%20A%20Large-Scale%20Dataset%20with%20Pose%20Annotations%20in%20Cluttered%0A%20%20Environments&entry.906535625=Yang%20You%20and%20Kai%20Xiong%20and%20Zhening%20Yang%20and%20Zhengxiang%20Huang%20and%20Junwei%20Zhou%20and%20Ruoxi%20Shi%20and%20Zhou%20Fang%20and%20Adam%20W.%20Harley%20and%20Leonidas%20Guibas%20and%20Cewu%20Lu&entry.1292438233=%20%20We%20introduce%20PACE%20%28Pose%20Annotations%20in%20Cluttered%20Environments%29%2C%20a%20large-scale%0Abenchmark%20designed%20to%20advance%20the%20development%20and%20evaluation%20of%20pose%20estimation%0Amethods%20in%20cluttered%20scenarios.%20PACE%20provides%20a%20large-scale%20real-world%0Abenchmark%20for%20both%20instance-level%20and%20category-level%20settings.%20The%20benchmark%0Aconsists%20of%2055K%20frames%20with%20258K%20annotations%20across%20300%20videos%2C%20covering%20238%0Aobjects%20from%2043%20categories%20and%20featuring%20a%20mix%20of%20rigid%20and%20articulated%20items%0Ain%20cluttered%20scenes.%20To%20annotate%20the%20real-world%20data%20efficiently%2C%20we%20develop%20an%0Ainnovative%20annotation%20system%20with%20a%20calibrated%203-camera%20setup.%20Additionally%2C%20we%0Aoffer%20PACE-Sim%2C%20which%20contains%20100K%20photo-realistic%20simulated%20frames%20with%202.4M%0Aannotations%20across%20931%20objects.%20We%20test%20state-of-the-art%20algorithms%20in%20PACE%0Aalong%20two%20tracks%3A%20pose%20estimation%2C%20and%20object%20pose%20tracking%2C%20revealing%20the%0Abenchmark%27s%20challenges%20and%20research%20opportunities.%20Our%20benchmark%20code%20and%20data%0Ais%20available%20on%20https%3A//github.com/qq456cvb/PACE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.15130v3&entry.124074799=Read"},
{"title": "Towards Responsible Development of Generative AI for Education: An\n  Evaluation-Driven Approach", "author": "Irina Jurenka and Markus Kunesch and Kevin R. McKee and Daniel Gillick and Shaojian Zhu and Sara Wiltberger and Shubham Milind Phal and Katherine Hermann and Daniel Kasenberg and Avishkar Bhoopchand and Ankit Anand and Miruna P\u00eeslar and Stephanie Chan and Lisa Wang and Jennifer She and Parsa Mahmoudieh and Aliya Rysbek and Wei-Jen Ko and Andrea Huber and Brett Wiltshire and Gal Elidan and Roni Rabin and Jasmin Rubinovitz and Amit Pitaru and Mac McAllister and Julia Wilkowski and David Choi and Roee Engelberg and Lidan Hackmon and Adva Levin and Rachel Griffin and Michael Sears and Filip Bar and Mia Mesar and Mana Jabbour and Arslan Chaudhry and James Cohan and Sridhar Thiagarajan and Nir Levine and Ben Brown and Dilan Gorur and Svetlana Grant and Rachel Hashimshoni and Laura Weidinger and Jieru Hu and Dawn Chen and Kuba Dolecki and Canfer Akbulut and Maxwell Bileschi and Laura Culp and Wen-Xin Dong and Nahema Marchal and Kelsie Van Deman and Hema Bajaj Misra and Michael Duah and Moran Ambar and Avi Caciularu and Sandra Lefdal and Chris Summerfield and James An and Pierre-Alexandre Kamienny and Abhinit Mohdi and Theofilos Strinopoulous and Annie Hale and Wayne Anderson and Luis C. Cobo and Niv Efron and Muktha Ananda and Shakir Mohamed and Maureen Heymans and Zoubin Ghahramani and Yossi Matias and Ben Gomes and Lila Ibrahim", "abstract": "  A major challenge facing the world is the provision of equitable and\nuniversal access to quality education. Recent advances in generative AI (gen\nAI) have created excitement about the potential of new technologies to offer a\npersonal tutor for every learner and a teaching assistant for every teacher.\nThe full extent of this dream, however, has not yet materialised. We argue that\nthis is primarily due to the difficulties with verbalising pedagogical\nintuitions into gen AI prompts and the lack of good evaluation practices,\nreinforced by the challenges in defining excellent pedagogy. Here we present\nour work collaborating with learners and educators to translate high level\nprinciples from learning science into a pragmatic set of seven diverse\neducational benchmarks, spanning quantitative, qualitative, automatic and human\nevaluations; and to develop a new set of fine-tuning datasets to improve the\npedagogical capabilities of Gemini, introducing LearnLM-Tutor. Our evaluations\nshow that LearnLM-Tutor is consistently preferred over a prompt tuned Gemini by\neducators and learners on a number of pedagogical dimensions. We hope that this\nwork can serve as a first step towards developing a comprehensive educational\nevaluation framework, and that this can enable rapid progress within the AI and\nEdTech communities towards maximising the positive impact of gen AI in\neducation.\n", "link": "http://arxiv.org/abs/2407.12687v2", "date": "2024-07-19", "relevancy": 2.5288, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5631}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4789}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Responsible%20Development%20of%20Generative%20AI%20for%20Education%3A%20An%0A%20%20Evaluation-Driven%20Approach&body=Title%3A%20Towards%20Responsible%20Development%20of%20Generative%20AI%20for%20Education%3A%20An%0A%20%20Evaluation-Driven%20Approach%0AAuthor%3A%20Irina%20Jurenka%20and%20Markus%20Kunesch%20and%20Kevin%20R.%20McKee%20and%20Daniel%20Gillick%20and%20Shaojian%20Zhu%20and%20Sara%20Wiltberger%20and%20Shubham%20Milind%20Phal%20and%20Katherine%20Hermann%20and%20Daniel%20Kasenberg%20and%20Avishkar%20Bhoopchand%20and%20Ankit%20Anand%20and%20Miruna%20P%C3%AEslar%20and%20Stephanie%20Chan%20and%20Lisa%20Wang%20and%20Jennifer%20She%20and%20Parsa%20Mahmoudieh%20and%20Aliya%20Rysbek%20and%20Wei-Jen%20Ko%20and%20Andrea%20Huber%20and%20Brett%20Wiltshire%20and%20Gal%20Elidan%20and%20Roni%20Rabin%20and%20Jasmin%20Rubinovitz%20and%20Amit%20Pitaru%20and%20Mac%20McAllister%20and%20Julia%20Wilkowski%20and%20David%20Choi%20and%20Roee%20Engelberg%20and%20Lidan%20Hackmon%20and%20Adva%20Levin%20and%20Rachel%20Griffin%20and%20Michael%20Sears%20and%20Filip%20Bar%20and%20Mia%20Mesar%20and%20Mana%20Jabbour%20and%20Arslan%20Chaudhry%20and%20James%20Cohan%20and%20Sridhar%20Thiagarajan%20and%20Nir%20Levine%20and%20Ben%20Brown%20and%20Dilan%20Gorur%20and%20Svetlana%20Grant%20and%20Rachel%20Hashimshoni%20and%20Laura%20Weidinger%20and%20Jieru%20Hu%20and%20Dawn%20Chen%20and%20Kuba%20Dolecki%20and%20Canfer%20Akbulut%20and%20Maxwell%20Bileschi%20and%20Laura%20Culp%20and%20Wen-Xin%20Dong%20and%20Nahema%20Marchal%20and%20Kelsie%20Van%20Deman%20and%20Hema%20Bajaj%20Misra%20and%20Michael%20Duah%20and%20Moran%20Ambar%20and%20Avi%20Caciularu%20and%20Sandra%20Lefdal%20and%20Chris%20Summerfield%20and%20James%20An%20and%20Pierre-Alexandre%20Kamienny%20and%20Abhinit%20Mohdi%20and%20Theofilos%20Strinopoulous%20and%20Annie%20Hale%20and%20Wayne%20Anderson%20and%20Luis%20C.%20Cobo%20and%20Niv%20Efron%20and%20Muktha%20Ananda%20and%20Shakir%20Mohamed%20and%20Maureen%20Heymans%20and%20Zoubin%20Ghahramani%20and%20Yossi%20Matias%20and%20Ben%20Gomes%20and%20Lila%20Ibrahim%0AAbstract%3A%20%20%20A%20major%20challenge%20facing%20the%20world%20is%20the%20provision%20of%20equitable%20and%0Auniversal%20access%20to%20quality%20education.%20Recent%20advances%20in%20generative%20AI%20%28gen%0AAI%29%20have%20created%20excitement%20about%20the%20potential%20of%20new%20technologies%20to%20offer%20a%0Apersonal%20tutor%20for%20every%20learner%20and%20a%20teaching%20assistant%20for%20every%20teacher.%0AThe%20full%20extent%20of%20this%20dream%2C%20however%2C%20has%20not%20yet%20materialised.%20We%20argue%20that%0Athis%20is%20primarily%20due%20to%20the%20difficulties%20with%20verbalising%20pedagogical%0Aintuitions%20into%20gen%20AI%20prompts%20and%20the%20lack%20of%20good%20evaluation%20practices%2C%0Areinforced%20by%20the%20challenges%20in%20defining%20excellent%20pedagogy.%20Here%20we%20present%0Aour%20work%20collaborating%20with%20learners%20and%20educators%20to%20translate%20high%20level%0Aprinciples%20from%20learning%20science%20into%20a%20pragmatic%20set%20of%20seven%20diverse%0Aeducational%20benchmarks%2C%20spanning%20quantitative%2C%20qualitative%2C%20automatic%20and%20human%0Aevaluations%3B%20and%20to%20develop%20a%20new%20set%20of%20fine-tuning%20datasets%20to%20improve%20the%0Apedagogical%20capabilities%20of%20Gemini%2C%20introducing%20LearnLM-Tutor.%20Our%20evaluations%0Ashow%20that%20LearnLM-Tutor%20is%20consistently%20preferred%20over%20a%20prompt%20tuned%20Gemini%20by%0Aeducators%20and%20learners%20on%20a%20number%20of%20pedagogical%20dimensions.%20We%20hope%20that%20this%0Awork%20can%20serve%20as%20a%20first%20step%20towards%20developing%20a%20comprehensive%20educational%0Aevaluation%20framework%2C%20and%20that%20this%20can%20enable%20rapid%20progress%20within%20the%20AI%20and%0AEdTech%20communities%20towards%20maximising%20the%20positive%20impact%20of%20gen%20AI%20in%0Aeducation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12687v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Responsible%2520Development%2520of%2520Generative%2520AI%2520for%2520Education%253A%2520An%250A%2520%2520Evaluation-Driven%2520Approach%26entry.906535625%3DIrina%2520Jurenka%2520and%2520Markus%2520Kunesch%2520and%2520Kevin%2520R.%2520McKee%2520and%2520Daniel%2520Gillick%2520and%2520Shaojian%2520Zhu%2520and%2520Sara%2520Wiltberger%2520and%2520Shubham%2520Milind%2520Phal%2520and%2520Katherine%2520Hermann%2520and%2520Daniel%2520Kasenberg%2520and%2520Avishkar%2520Bhoopchand%2520and%2520Ankit%2520Anand%2520and%2520Miruna%2520P%25C3%25AEslar%2520and%2520Stephanie%2520Chan%2520and%2520Lisa%2520Wang%2520and%2520Jennifer%2520She%2520and%2520Parsa%2520Mahmoudieh%2520and%2520Aliya%2520Rysbek%2520and%2520Wei-Jen%2520Ko%2520and%2520Andrea%2520Huber%2520and%2520Brett%2520Wiltshire%2520and%2520Gal%2520Elidan%2520and%2520Roni%2520Rabin%2520and%2520Jasmin%2520Rubinovitz%2520and%2520Amit%2520Pitaru%2520and%2520Mac%2520McAllister%2520and%2520Julia%2520Wilkowski%2520and%2520David%2520Choi%2520and%2520Roee%2520Engelberg%2520and%2520Lidan%2520Hackmon%2520and%2520Adva%2520Levin%2520and%2520Rachel%2520Griffin%2520and%2520Michael%2520Sears%2520and%2520Filip%2520Bar%2520and%2520Mia%2520Mesar%2520and%2520Mana%2520Jabbour%2520and%2520Arslan%2520Chaudhry%2520and%2520James%2520Cohan%2520and%2520Sridhar%2520Thiagarajan%2520and%2520Nir%2520Levine%2520and%2520Ben%2520Brown%2520and%2520Dilan%2520Gorur%2520and%2520Svetlana%2520Grant%2520and%2520Rachel%2520Hashimshoni%2520and%2520Laura%2520Weidinger%2520and%2520Jieru%2520Hu%2520and%2520Dawn%2520Chen%2520and%2520Kuba%2520Dolecki%2520and%2520Canfer%2520Akbulut%2520and%2520Maxwell%2520Bileschi%2520and%2520Laura%2520Culp%2520and%2520Wen-Xin%2520Dong%2520and%2520Nahema%2520Marchal%2520and%2520Kelsie%2520Van%2520Deman%2520and%2520Hema%2520Bajaj%2520Misra%2520and%2520Michael%2520Duah%2520and%2520Moran%2520Ambar%2520and%2520Avi%2520Caciularu%2520and%2520Sandra%2520Lefdal%2520and%2520Chris%2520Summerfield%2520and%2520James%2520An%2520and%2520Pierre-Alexandre%2520Kamienny%2520and%2520Abhinit%2520Mohdi%2520and%2520Theofilos%2520Strinopoulous%2520and%2520Annie%2520Hale%2520and%2520Wayne%2520Anderson%2520and%2520Luis%2520C.%2520Cobo%2520and%2520Niv%2520Efron%2520and%2520Muktha%2520Ananda%2520and%2520Shakir%2520Mohamed%2520and%2520Maureen%2520Heymans%2520and%2520Zoubin%2520Ghahramani%2520and%2520Yossi%2520Matias%2520and%2520Ben%2520Gomes%2520and%2520Lila%2520Ibrahim%26entry.1292438233%3D%2520%2520A%2520major%2520challenge%2520facing%2520the%2520world%2520is%2520the%2520provision%2520of%2520equitable%2520and%250Auniversal%2520access%2520to%2520quality%2520education.%2520Recent%2520advances%2520in%2520generative%2520AI%2520%2528gen%250AAI%2529%2520have%2520created%2520excitement%2520about%2520the%2520potential%2520of%2520new%2520technologies%2520to%2520offer%2520a%250Apersonal%2520tutor%2520for%2520every%2520learner%2520and%2520a%2520teaching%2520assistant%2520for%2520every%2520teacher.%250AThe%2520full%2520extent%2520of%2520this%2520dream%252C%2520however%252C%2520has%2520not%2520yet%2520materialised.%2520We%2520argue%2520that%250Athis%2520is%2520primarily%2520due%2520to%2520the%2520difficulties%2520with%2520verbalising%2520pedagogical%250Aintuitions%2520into%2520gen%2520AI%2520prompts%2520and%2520the%2520lack%2520of%2520good%2520evaluation%2520practices%252C%250Areinforced%2520by%2520the%2520challenges%2520in%2520defining%2520excellent%2520pedagogy.%2520Here%2520we%2520present%250Aour%2520work%2520collaborating%2520with%2520learners%2520and%2520educators%2520to%2520translate%2520high%2520level%250Aprinciples%2520from%2520learning%2520science%2520into%2520a%2520pragmatic%2520set%2520of%2520seven%2520diverse%250Aeducational%2520benchmarks%252C%2520spanning%2520quantitative%252C%2520qualitative%252C%2520automatic%2520and%2520human%250Aevaluations%253B%2520and%2520to%2520develop%2520a%2520new%2520set%2520of%2520fine-tuning%2520datasets%2520to%2520improve%2520the%250Apedagogical%2520capabilities%2520of%2520Gemini%252C%2520introducing%2520LearnLM-Tutor.%2520Our%2520evaluations%250Ashow%2520that%2520LearnLM-Tutor%2520is%2520consistently%2520preferred%2520over%2520a%2520prompt%2520tuned%2520Gemini%2520by%250Aeducators%2520and%2520learners%2520on%2520a%2520number%2520of%2520pedagogical%2520dimensions.%2520We%2520hope%2520that%2520this%250Awork%2520can%2520serve%2520as%2520a%2520first%2520step%2520towards%2520developing%2520a%2520comprehensive%2520educational%250Aevaluation%2520framework%252C%2520and%2520that%2520this%2520can%2520enable%2520rapid%2520progress%2520within%2520the%2520AI%2520and%250AEdTech%2520communities%2520towards%2520maximising%2520the%2520positive%2520impact%2520of%2520gen%2520AI%2520in%250Aeducation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12687v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Responsible%20Development%20of%20Generative%20AI%20for%20Education%3A%20An%0A%20%20Evaluation-Driven%20Approach&entry.906535625=Irina%20Jurenka%20and%20Markus%20Kunesch%20and%20Kevin%20R.%20McKee%20and%20Daniel%20Gillick%20and%20Shaojian%20Zhu%20and%20Sara%20Wiltberger%20and%20Shubham%20Milind%20Phal%20and%20Katherine%20Hermann%20and%20Daniel%20Kasenberg%20and%20Avishkar%20Bhoopchand%20and%20Ankit%20Anand%20and%20Miruna%20P%C3%AEslar%20and%20Stephanie%20Chan%20and%20Lisa%20Wang%20and%20Jennifer%20She%20and%20Parsa%20Mahmoudieh%20and%20Aliya%20Rysbek%20and%20Wei-Jen%20Ko%20and%20Andrea%20Huber%20and%20Brett%20Wiltshire%20and%20Gal%20Elidan%20and%20Roni%20Rabin%20and%20Jasmin%20Rubinovitz%20and%20Amit%20Pitaru%20and%20Mac%20McAllister%20and%20Julia%20Wilkowski%20and%20David%20Choi%20and%20Roee%20Engelberg%20and%20Lidan%20Hackmon%20and%20Adva%20Levin%20and%20Rachel%20Griffin%20and%20Michael%20Sears%20and%20Filip%20Bar%20and%20Mia%20Mesar%20and%20Mana%20Jabbour%20and%20Arslan%20Chaudhry%20and%20James%20Cohan%20and%20Sridhar%20Thiagarajan%20and%20Nir%20Levine%20and%20Ben%20Brown%20and%20Dilan%20Gorur%20and%20Svetlana%20Grant%20and%20Rachel%20Hashimshoni%20and%20Laura%20Weidinger%20and%20Jieru%20Hu%20and%20Dawn%20Chen%20and%20Kuba%20Dolecki%20and%20Canfer%20Akbulut%20and%20Maxwell%20Bileschi%20and%20Laura%20Culp%20and%20Wen-Xin%20Dong%20and%20Nahema%20Marchal%20and%20Kelsie%20Van%20Deman%20and%20Hema%20Bajaj%20Misra%20and%20Michael%20Duah%20and%20Moran%20Ambar%20and%20Avi%20Caciularu%20and%20Sandra%20Lefdal%20and%20Chris%20Summerfield%20and%20James%20An%20and%20Pierre-Alexandre%20Kamienny%20and%20Abhinit%20Mohdi%20and%20Theofilos%20Strinopoulous%20and%20Annie%20Hale%20and%20Wayne%20Anderson%20and%20Luis%20C.%20Cobo%20and%20Niv%20Efron%20and%20Muktha%20Ananda%20and%20Shakir%20Mohamed%20and%20Maureen%20Heymans%20and%20Zoubin%20Ghahramani%20and%20Yossi%20Matias%20and%20Ben%20Gomes%20and%20Lila%20Ibrahim&entry.1292438233=%20%20A%20major%20challenge%20facing%20the%20world%20is%20the%20provision%20of%20equitable%20and%0Auniversal%20access%20to%20quality%20education.%20Recent%20advances%20in%20generative%20AI%20%28gen%0AAI%29%20have%20created%20excitement%20about%20the%20potential%20of%20new%20technologies%20to%20offer%20a%0Apersonal%20tutor%20for%20every%20learner%20and%20a%20teaching%20assistant%20for%20every%20teacher.%0AThe%20full%20extent%20of%20this%20dream%2C%20however%2C%20has%20not%20yet%20materialised.%20We%20argue%20that%0Athis%20is%20primarily%20due%20to%20the%20difficulties%20with%20verbalising%20pedagogical%0Aintuitions%20into%20gen%20AI%20prompts%20and%20the%20lack%20of%20good%20evaluation%20practices%2C%0Areinforced%20by%20the%20challenges%20in%20defining%20excellent%20pedagogy.%20Here%20we%20present%0Aour%20work%20collaborating%20with%20learners%20and%20educators%20to%20translate%20high%20level%0Aprinciples%20from%20learning%20science%20into%20a%20pragmatic%20set%20of%20seven%20diverse%0Aeducational%20benchmarks%2C%20spanning%20quantitative%2C%20qualitative%2C%20automatic%20and%20human%0Aevaluations%3B%20and%20to%20develop%20a%20new%20set%20of%20fine-tuning%20datasets%20to%20improve%20the%0Apedagogical%20capabilities%20of%20Gemini%2C%20introducing%20LearnLM-Tutor.%20Our%20evaluations%0Ashow%20that%20LearnLM-Tutor%20is%20consistently%20preferred%20over%20a%20prompt%20tuned%20Gemini%20by%0Aeducators%20and%20learners%20on%20a%20number%20of%20pedagogical%20dimensions.%20We%20hope%20that%20this%0Awork%20can%20serve%20as%20a%20first%20step%20towards%20developing%20a%20comprehensive%20educational%0Aevaluation%20framework%2C%20and%20that%20this%20can%20enable%20rapid%20progress%20within%20the%20AI%20and%0AEdTech%20communities%20towards%20maximising%20the%20positive%20impact%20of%20gen%20AI%20in%0Aeducation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12687v2&entry.124074799=Read"},
{"title": "DeepHGCN: Toward Deeper Hyperbolic Graph Convolutional Networks", "author": "Jiaxu Liu and Xinping Yi and Xiaowei Huang", "abstract": "  Hyperbolic graph convolutional networks (HGCNs) have demonstrated significant\npotential in extracting information from hierarchical graphs. However, existing\nHGCNs are limited to shallow architectures due to the computational expense of\nhyperbolic operations and the issue of over-smoothing as depth increases.\nAlthough treatments have been applied to alleviate over-smoothing in GCNs,\ndeveloping a hyperbolic solution presents distinct challenges since operations\nmust be carefully designed to fit the hyperbolic nature. Addressing these\nchallenges, we propose DeepHGCN, the first deep multi-layer HGCN architecture\nwith dramatically improved computational efficiency and substantially reduced\nover-smoothing. DeepHGCN features two key innovations: (1) a novel hyperbolic\nfeature transformation layer that enables fast and accurate linear mappings,\nand (2) techniques such as hyperbolic residual connections and regularization\nfor both weights and features, facilitated by an efficient hyperbolic midpoint\nmethod. Extensive experiments demonstrate that DeepHGCN achieves significant\nimprovements in link prediction and node classification tasks compared to both\nEuclidean and shallow hyperbolic GCN variants.\n", "link": "http://arxiv.org/abs/2310.02027v4", "date": "2024-07-19", "relevancy": 2.5066, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5248}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5023}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepHGCN%3A%20Toward%20Deeper%20Hyperbolic%20Graph%20Convolutional%20Networks&body=Title%3A%20DeepHGCN%3A%20Toward%20Deeper%20Hyperbolic%20Graph%20Convolutional%20Networks%0AAuthor%3A%20Jiaxu%20Liu%20and%20Xinping%20Yi%20and%20Xiaowei%20Huang%0AAbstract%3A%20%20%20Hyperbolic%20graph%20convolutional%20networks%20%28HGCNs%29%20have%20demonstrated%20significant%0Apotential%20in%20extracting%20information%20from%20hierarchical%20graphs.%20However%2C%20existing%0AHGCNs%20are%20limited%20to%20shallow%20architectures%20due%20to%20the%20computational%20expense%20of%0Ahyperbolic%20operations%20and%20the%20issue%20of%20over-smoothing%20as%20depth%20increases.%0AAlthough%20treatments%20have%20been%20applied%20to%20alleviate%20over-smoothing%20in%20GCNs%2C%0Adeveloping%20a%20hyperbolic%20solution%20presents%20distinct%20challenges%20since%20operations%0Amust%20be%20carefully%20designed%20to%20fit%20the%20hyperbolic%20nature.%20Addressing%20these%0Achallenges%2C%20we%20propose%20DeepHGCN%2C%20the%20first%20deep%20multi-layer%20HGCN%20architecture%0Awith%20dramatically%20improved%20computational%20efficiency%20and%20substantially%20reduced%0Aover-smoothing.%20DeepHGCN%20features%20two%20key%20innovations%3A%20%281%29%20a%20novel%20hyperbolic%0Afeature%20transformation%20layer%20that%20enables%20fast%20and%20accurate%20linear%20mappings%2C%0Aand%20%282%29%20techniques%20such%20as%20hyperbolic%20residual%20connections%20and%20regularization%0Afor%20both%20weights%20and%20features%2C%20facilitated%20by%20an%20efficient%20hyperbolic%20midpoint%0Amethod.%20Extensive%20experiments%20demonstrate%20that%20DeepHGCN%20achieves%20significant%0Aimprovements%20in%20link%20prediction%20and%20node%20classification%20tasks%20compared%20to%20both%0AEuclidean%20and%20shallow%20hyperbolic%20GCN%20variants.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02027v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepHGCN%253A%2520Toward%2520Deeper%2520Hyperbolic%2520Graph%2520Convolutional%2520Networks%26entry.906535625%3DJiaxu%2520Liu%2520and%2520Xinping%2520Yi%2520and%2520Xiaowei%2520Huang%26entry.1292438233%3D%2520%2520Hyperbolic%2520graph%2520convolutional%2520networks%2520%2528HGCNs%2529%2520have%2520demonstrated%2520significant%250Apotential%2520in%2520extracting%2520information%2520from%2520hierarchical%2520graphs.%2520However%252C%2520existing%250AHGCNs%2520are%2520limited%2520to%2520shallow%2520architectures%2520due%2520to%2520the%2520computational%2520expense%2520of%250Ahyperbolic%2520operations%2520and%2520the%2520issue%2520of%2520over-smoothing%2520as%2520depth%2520increases.%250AAlthough%2520treatments%2520have%2520been%2520applied%2520to%2520alleviate%2520over-smoothing%2520in%2520GCNs%252C%250Adeveloping%2520a%2520hyperbolic%2520solution%2520presents%2520distinct%2520challenges%2520since%2520operations%250Amust%2520be%2520carefully%2520designed%2520to%2520fit%2520the%2520hyperbolic%2520nature.%2520Addressing%2520these%250Achallenges%252C%2520we%2520propose%2520DeepHGCN%252C%2520the%2520first%2520deep%2520multi-layer%2520HGCN%2520architecture%250Awith%2520dramatically%2520improved%2520computational%2520efficiency%2520and%2520substantially%2520reduced%250Aover-smoothing.%2520DeepHGCN%2520features%2520two%2520key%2520innovations%253A%2520%25281%2529%2520a%2520novel%2520hyperbolic%250Afeature%2520transformation%2520layer%2520that%2520enables%2520fast%2520and%2520accurate%2520linear%2520mappings%252C%250Aand%2520%25282%2529%2520techniques%2520such%2520as%2520hyperbolic%2520residual%2520connections%2520and%2520regularization%250Afor%2520both%2520weights%2520and%2520features%252C%2520facilitated%2520by%2520an%2520efficient%2520hyperbolic%2520midpoint%250Amethod.%2520Extensive%2520experiments%2520demonstrate%2520that%2520DeepHGCN%2520achieves%2520significant%250Aimprovements%2520in%2520link%2520prediction%2520and%2520node%2520classification%2520tasks%2520compared%2520to%2520both%250AEuclidean%2520and%2520shallow%2520hyperbolic%2520GCN%2520variants.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.02027v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepHGCN%3A%20Toward%20Deeper%20Hyperbolic%20Graph%20Convolutional%20Networks&entry.906535625=Jiaxu%20Liu%20and%20Xinping%20Yi%20and%20Xiaowei%20Huang&entry.1292438233=%20%20Hyperbolic%20graph%20convolutional%20networks%20%28HGCNs%29%20have%20demonstrated%20significant%0Apotential%20in%20extracting%20information%20from%20hierarchical%20graphs.%20However%2C%20existing%0AHGCNs%20are%20limited%20to%20shallow%20architectures%20due%20to%20the%20computational%20expense%20of%0Ahyperbolic%20operations%20and%20the%20issue%20of%20over-smoothing%20as%20depth%20increases.%0AAlthough%20treatments%20have%20been%20applied%20to%20alleviate%20over-smoothing%20in%20GCNs%2C%0Adeveloping%20a%20hyperbolic%20solution%20presents%20distinct%20challenges%20since%20operations%0Amust%20be%20carefully%20designed%20to%20fit%20the%20hyperbolic%20nature.%20Addressing%20these%0Achallenges%2C%20we%20propose%20DeepHGCN%2C%20the%20first%20deep%20multi-layer%20HGCN%20architecture%0Awith%20dramatically%20improved%20computational%20efficiency%20and%20substantially%20reduced%0Aover-smoothing.%20DeepHGCN%20features%20two%20key%20innovations%3A%20%281%29%20a%20novel%20hyperbolic%0Afeature%20transformation%20layer%20that%20enables%20fast%20and%20accurate%20linear%20mappings%2C%0Aand%20%282%29%20techniques%20such%20as%20hyperbolic%20residual%20connections%20and%20regularization%0Afor%20both%20weights%20and%20features%2C%20facilitated%20by%20an%20efficient%20hyperbolic%20midpoint%0Amethod.%20Extensive%20experiments%20demonstrate%20that%20DeepHGCN%20achieves%20significant%0Aimprovements%20in%20link%20prediction%20and%20node%20classification%20tasks%20compared%20to%20both%0AEuclidean%20and%20shallow%20hyperbolic%20GCN%20variants.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02027v4&entry.124074799=Read"},
{"title": "EmoCAM: Toward Understanding What Drives CNN-based Emotion Recognition", "author": "Youssef Doulfoukar and Laurent Mertens and Joost Vennekens", "abstract": "  Convolutional Neural Networks are particularly suited for image analysis\ntasks, such as Image Classification, Object Recognition or Image Segmentation.\nLike all Artificial Neural Networks, however, they are \"black box\" models, and\nsuffer from poor explainability. This work is concerned with the specific\ndownstream task of Emotion Recognition from images, and proposes a framework\nthat combines CAM-based techniques with Object Detection on a corpus level to\nbetter understand on which image cues a particular model, in our case EmoNet,\nrelies to assign a specific emotion to an image. We demonstrate that the model\nmostly focuses on human characteristics, but also explore the pronounced effect\nof specific image modifications.\n", "link": "http://arxiv.org/abs/2407.14314v1", "date": "2024-07-19", "relevancy": 2.4995, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5063}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4987}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EmoCAM%3A%20Toward%20Understanding%20What%20Drives%20CNN-based%20Emotion%20Recognition&body=Title%3A%20EmoCAM%3A%20Toward%20Understanding%20What%20Drives%20CNN-based%20Emotion%20Recognition%0AAuthor%3A%20Youssef%20Doulfoukar%20and%20Laurent%20Mertens%20and%20Joost%20Vennekens%0AAbstract%3A%20%20%20Convolutional%20Neural%20Networks%20are%20particularly%20suited%20for%20image%20analysis%0Atasks%2C%20such%20as%20Image%20Classification%2C%20Object%20Recognition%20or%20Image%20Segmentation.%0ALike%20all%20Artificial%20Neural%20Networks%2C%20however%2C%20they%20are%20%22black%20box%22%20models%2C%20and%0Asuffer%20from%20poor%20explainability.%20This%20work%20is%20concerned%20with%20the%20specific%0Adownstream%20task%20of%20Emotion%20Recognition%20from%20images%2C%20and%20proposes%20a%20framework%0Athat%20combines%20CAM-based%20techniques%20with%20Object%20Detection%20on%20a%20corpus%20level%20to%0Abetter%20understand%20on%20which%20image%20cues%20a%20particular%20model%2C%20in%20our%20case%20EmoNet%2C%0Arelies%20to%20assign%20a%20specific%20emotion%20to%20an%20image.%20We%20demonstrate%20that%20the%20model%0Amostly%20focuses%20on%20human%20characteristics%2C%20but%20also%20explore%20the%20pronounced%20effect%0Aof%20specific%20image%20modifications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14314v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmoCAM%253A%2520Toward%2520Understanding%2520What%2520Drives%2520CNN-based%2520Emotion%2520Recognition%26entry.906535625%3DYoussef%2520Doulfoukar%2520and%2520Laurent%2520Mertens%2520and%2520Joost%2520Vennekens%26entry.1292438233%3D%2520%2520Convolutional%2520Neural%2520Networks%2520are%2520particularly%2520suited%2520for%2520image%2520analysis%250Atasks%252C%2520such%2520as%2520Image%2520Classification%252C%2520Object%2520Recognition%2520or%2520Image%2520Segmentation.%250ALike%2520all%2520Artificial%2520Neural%2520Networks%252C%2520however%252C%2520they%2520are%2520%2522black%2520box%2522%2520models%252C%2520and%250Asuffer%2520from%2520poor%2520explainability.%2520This%2520work%2520is%2520concerned%2520with%2520the%2520specific%250Adownstream%2520task%2520of%2520Emotion%2520Recognition%2520from%2520images%252C%2520and%2520proposes%2520a%2520framework%250Athat%2520combines%2520CAM-based%2520techniques%2520with%2520Object%2520Detection%2520on%2520a%2520corpus%2520level%2520to%250Abetter%2520understand%2520on%2520which%2520image%2520cues%2520a%2520particular%2520model%252C%2520in%2520our%2520case%2520EmoNet%252C%250Arelies%2520to%2520assign%2520a%2520specific%2520emotion%2520to%2520an%2520image.%2520We%2520demonstrate%2520that%2520the%2520model%250Amostly%2520focuses%2520on%2520human%2520characteristics%252C%2520but%2520also%2520explore%2520the%2520pronounced%2520effect%250Aof%2520specific%2520image%2520modifications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14314v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EmoCAM%3A%20Toward%20Understanding%20What%20Drives%20CNN-based%20Emotion%20Recognition&entry.906535625=Youssef%20Doulfoukar%20and%20Laurent%20Mertens%20and%20Joost%20Vennekens&entry.1292438233=%20%20Convolutional%20Neural%20Networks%20are%20particularly%20suited%20for%20image%20analysis%0Atasks%2C%20such%20as%20Image%20Classification%2C%20Object%20Recognition%20or%20Image%20Segmentation.%0ALike%20all%20Artificial%20Neural%20Networks%2C%20however%2C%20they%20are%20%22black%20box%22%20models%2C%20and%0Asuffer%20from%20poor%20explainability.%20This%20work%20is%20concerned%20with%20the%20specific%0Adownstream%20task%20of%20Emotion%20Recognition%20from%20images%2C%20and%20proposes%20a%20framework%0Athat%20combines%20CAM-based%20techniques%20with%20Object%20Detection%20on%20a%20corpus%20level%20to%0Abetter%20understand%20on%20which%20image%20cues%20a%20particular%20model%2C%20in%20our%20case%20EmoNet%2C%0Arelies%20to%20assign%20a%20specific%20emotion%20to%20an%20image.%20We%20demonstrate%20that%20the%20model%0Amostly%20focuses%20on%20human%20characteristics%2C%20but%20also%20explore%20the%20pronounced%20effect%0Aof%20specific%20image%20modifications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14314v1&entry.124074799=Read"},
{"title": "Fair Overlap Number of Balls (Fair-ONB): A Data-Morphology-based\n  Undersampling Method for Bias Reduction", "author": "Jos\u00e9 Daniel Pascual-Triana and Alberto Fern\u00e1ndez and Paulo Novais and Francisco Herrera", "abstract": "  Given the magnitude of data generation currently, both in quantity and speed,\nthe use of machine learning is increasingly important. When data include\nprotected features that might give rise to discrimination, special care must be\ntaken. Data quality is critical in these cases, as biases in training data can\nbe reflected in classification models. This has devastating consequences and\nfails to comply with current regulations. Data-Centric Artificial Intelligence\nproposes dataset modifications to improve its quality. Instance selection via\nundersampling can foster balanced learning of classes and protected feature\nvalues in the classifier. When such undersampling is done close to the decision\nboundary, the effect on the classifier would be bolstered. This work proposes\nFair Overlap Number of Balls (Fair-ONB), an undersampling method that harnesses\nthe data morphology of the different data groups (obtained from the combination\nof classes and protected feature values) to perform guided undersampling in the\nareas where they overlap. It employs attributes of the ball coverage of the\ngroups, such as the radius, number of covered instances and density, to select\nthe most suitable areas for undersampling and reduce bias. Results show that\nthe Fair-ONB method reduces bias with low impact on the classifier's predictive\nperformance.\n", "link": "http://arxiv.org/abs/2407.14210v1", "date": "2024-07-19", "relevancy": 2.4673, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5222}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4914}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fair%20Overlap%20Number%20of%20Balls%20%28Fair-ONB%29%3A%20A%20Data-Morphology-based%0A%20%20Undersampling%20Method%20for%20Bias%20Reduction&body=Title%3A%20Fair%20Overlap%20Number%20of%20Balls%20%28Fair-ONB%29%3A%20A%20Data-Morphology-based%0A%20%20Undersampling%20Method%20for%20Bias%20Reduction%0AAuthor%3A%20Jos%C3%A9%20Daniel%20Pascual-Triana%20and%20Alberto%20Fern%C3%A1ndez%20and%20Paulo%20Novais%20and%20Francisco%20Herrera%0AAbstract%3A%20%20%20Given%20the%20magnitude%20of%20data%20generation%20currently%2C%20both%20in%20quantity%20and%20speed%2C%0Athe%20use%20of%20machine%20learning%20is%20increasingly%20important.%20When%20data%20include%0Aprotected%20features%20that%20might%20give%20rise%20to%20discrimination%2C%20special%20care%20must%20be%0Ataken.%20Data%20quality%20is%20critical%20in%20these%20cases%2C%20as%20biases%20in%20training%20data%20can%0Abe%20reflected%20in%20classification%20models.%20This%20has%20devastating%20consequences%20and%0Afails%20to%20comply%20with%20current%20regulations.%20Data-Centric%20Artificial%20Intelligence%0Aproposes%20dataset%20modifications%20to%20improve%20its%20quality.%20Instance%20selection%20via%0Aundersampling%20can%20foster%20balanced%20learning%20of%20classes%20and%20protected%20feature%0Avalues%20in%20the%20classifier.%20When%20such%20undersampling%20is%20done%20close%20to%20the%20decision%0Aboundary%2C%20the%20effect%20on%20the%20classifier%20would%20be%20bolstered.%20This%20work%20proposes%0AFair%20Overlap%20Number%20of%20Balls%20%28Fair-ONB%29%2C%20an%20undersampling%20method%20that%20harnesses%0Athe%20data%20morphology%20of%20the%20different%20data%20groups%20%28obtained%20from%20the%20combination%0Aof%20classes%20and%20protected%20feature%20values%29%20to%20perform%20guided%20undersampling%20in%20the%0Aareas%20where%20they%20overlap.%20It%20employs%20attributes%20of%20the%20ball%20coverage%20of%20the%0Agroups%2C%20such%20as%20the%20radius%2C%20number%20of%20covered%20instances%20and%20density%2C%20to%20select%0Athe%20most%20suitable%20areas%20for%20undersampling%20and%20reduce%20bias.%20Results%20show%20that%0Athe%20Fair-ONB%20method%20reduces%20bias%20with%20low%20impact%20on%20the%20classifier%27s%20predictive%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14210v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFair%2520Overlap%2520Number%2520of%2520Balls%2520%2528Fair-ONB%2529%253A%2520A%2520Data-Morphology-based%250A%2520%2520Undersampling%2520Method%2520for%2520Bias%2520Reduction%26entry.906535625%3DJos%25C3%25A9%2520Daniel%2520Pascual-Triana%2520and%2520Alberto%2520Fern%25C3%25A1ndez%2520and%2520Paulo%2520Novais%2520and%2520Francisco%2520Herrera%26entry.1292438233%3D%2520%2520Given%2520the%2520magnitude%2520of%2520data%2520generation%2520currently%252C%2520both%2520in%2520quantity%2520and%2520speed%252C%250Athe%2520use%2520of%2520machine%2520learning%2520is%2520increasingly%2520important.%2520When%2520data%2520include%250Aprotected%2520features%2520that%2520might%2520give%2520rise%2520to%2520discrimination%252C%2520special%2520care%2520must%2520be%250Ataken.%2520Data%2520quality%2520is%2520critical%2520in%2520these%2520cases%252C%2520as%2520biases%2520in%2520training%2520data%2520can%250Abe%2520reflected%2520in%2520classification%2520models.%2520This%2520has%2520devastating%2520consequences%2520and%250Afails%2520to%2520comply%2520with%2520current%2520regulations.%2520Data-Centric%2520Artificial%2520Intelligence%250Aproposes%2520dataset%2520modifications%2520to%2520improve%2520its%2520quality.%2520Instance%2520selection%2520via%250Aundersampling%2520can%2520foster%2520balanced%2520learning%2520of%2520classes%2520and%2520protected%2520feature%250Avalues%2520in%2520the%2520classifier.%2520When%2520such%2520undersampling%2520is%2520done%2520close%2520to%2520the%2520decision%250Aboundary%252C%2520the%2520effect%2520on%2520the%2520classifier%2520would%2520be%2520bolstered.%2520This%2520work%2520proposes%250AFair%2520Overlap%2520Number%2520of%2520Balls%2520%2528Fair-ONB%2529%252C%2520an%2520undersampling%2520method%2520that%2520harnesses%250Athe%2520data%2520morphology%2520of%2520the%2520different%2520data%2520groups%2520%2528obtained%2520from%2520the%2520combination%250Aof%2520classes%2520and%2520protected%2520feature%2520values%2529%2520to%2520perform%2520guided%2520undersampling%2520in%2520the%250Aareas%2520where%2520they%2520overlap.%2520It%2520employs%2520attributes%2520of%2520the%2520ball%2520coverage%2520of%2520the%250Agroups%252C%2520such%2520as%2520the%2520radius%252C%2520number%2520of%2520covered%2520instances%2520and%2520density%252C%2520to%2520select%250Athe%2520most%2520suitable%2520areas%2520for%2520undersampling%2520and%2520reduce%2520bias.%2520Results%2520show%2520that%250Athe%2520Fair-ONB%2520method%2520reduces%2520bias%2520with%2520low%2520impact%2520on%2520the%2520classifier%2527s%2520predictive%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14210v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fair%20Overlap%20Number%20of%20Balls%20%28Fair-ONB%29%3A%20A%20Data-Morphology-based%0A%20%20Undersampling%20Method%20for%20Bias%20Reduction&entry.906535625=Jos%C3%A9%20Daniel%20Pascual-Triana%20and%20Alberto%20Fern%C3%A1ndez%20and%20Paulo%20Novais%20and%20Francisco%20Herrera&entry.1292438233=%20%20Given%20the%20magnitude%20of%20data%20generation%20currently%2C%20both%20in%20quantity%20and%20speed%2C%0Athe%20use%20of%20machine%20learning%20is%20increasingly%20important.%20When%20data%20include%0Aprotected%20features%20that%20might%20give%20rise%20to%20discrimination%2C%20special%20care%20must%20be%0Ataken.%20Data%20quality%20is%20critical%20in%20these%20cases%2C%20as%20biases%20in%20training%20data%20can%0Abe%20reflected%20in%20classification%20models.%20This%20has%20devastating%20consequences%20and%0Afails%20to%20comply%20with%20current%20regulations.%20Data-Centric%20Artificial%20Intelligence%0Aproposes%20dataset%20modifications%20to%20improve%20its%20quality.%20Instance%20selection%20via%0Aundersampling%20can%20foster%20balanced%20learning%20of%20classes%20and%20protected%20feature%0Avalues%20in%20the%20classifier.%20When%20such%20undersampling%20is%20done%20close%20to%20the%20decision%0Aboundary%2C%20the%20effect%20on%20the%20classifier%20would%20be%20bolstered.%20This%20work%20proposes%0AFair%20Overlap%20Number%20of%20Balls%20%28Fair-ONB%29%2C%20an%20undersampling%20method%20that%20harnesses%0Athe%20data%20morphology%20of%20the%20different%20data%20groups%20%28obtained%20from%20the%20combination%0Aof%20classes%20and%20protected%20feature%20values%29%20to%20perform%20guided%20undersampling%20in%20the%0Aareas%20where%20they%20overlap.%20It%20employs%20attributes%20of%20the%20ball%20coverage%20of%20the%0Agroups%2C%20such%20as%20the%20radius%2C%20number%20of%20covered%20instances%20and%20density%2C%20to%20select%0Athe%20most%20suitable%20areas%20for%20undersampling%20and%20reduce%20bias.%20Results%20show%20that%0Athe%20Fair-ONB%20method%20reduces%20bias%20with%20low%20impact%20on%20the%20classifier%27s%20predictive%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14210v1&entry.124074799=Read"},
{"title": "Continual Panoptic Perception: Towards Multi-modal Incremental\n  Interpretation of Remote Sensing Images", "author": "Bo Yuan and Danpei Zhao and Zhuoran Liu and Wentao Li and Tian Li", "abstract": "  Continual learning (CL) breaks off the one-way training manner and enables a\nmodel to adapt to new data, semantics and tasks continuously. However, current\nCL methods mainly focus on single tasks. Besides, CL models are plagued by\ncatastrophic forgetting and semantic drift since the lack of old data, which\noften occurs in remote-sensing interpretation due to the intricate fine-grained\nsemantics. In this paper, we propose Continual Panoptic Perception (CPP), a\nunified continual learning model that leverages multi-task joint learning\ncovering pixel-level classification, instance-level segmentation and\nimage-level perception for universal interpretation in remote sensing images.\nConcretely, we propose a collaborative cross-modal encoder (CCE) to extract the\ninput image features, which supports pixel classification and caption\ngeneration synchronously. To inherit the knowledge from the old model without\nexemplar memory, we propose a task-interactive knowledge distillation (TKD)\nmethod, which leverages cross-modal optimization and task-asymmetric\npseudo-labeling (TPL) to alleviate catastrophic forgetting. Furthermore, we\nalso propose a joint optimization mechanism to achieve end-to-end multi-modal\npanoptic perception. Experimental results on the fine-grained panoptic\nperception dataset validate the effectiveness of the proposed model, and also\nprove that joint optimization can boost sub-task CL efficiency with over 13\\%\nrelative improvement on panoptic quality.\n", "link": "http://arxiv.org/abs/2407.14242v1", "date": "2024-07-19", "relevancy": 2.4647, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6305}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6151}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Panoptic%20Perception%3A%20Towards%20Multi-modal%20Incremental%0A%20%20Interpretation%20of%20Remote%20Sensing%20Images&body=Title%3A%20Continual%20Panoptic%20Perception%3A%20Towards%20Multi-modal%20Incremental%0A%20%20Interpretation%20of%20Remote%20Sensing%20Images%0AAuthor%3A%20Bo%20Yuan%20and%20Danpei%20Zhao%20and%20Zhuoran%20Liu%20and%20Wentao%20Li%20and%20Tian%20Li%0AAbstract%3A%20%20%20Continual%20learning%20%28CL%29%20breaks%20off%20the%20one-way%20training%20manner%20and%20enables%20a%0Amodel%20to%20adapt%20to%20new%20data%2C%20semantics%20and%20tasks%20continuously.%20However%2C%20current%0ACL%20methods%20mainly%20focus%20on%20single%20tasks.%20Besides%2C%20CL%20models%20are%20plagued%20by%0Acatastrophic%20forgetting%20and%20semantic%20drift%20since%20the%20lack%20of%20old%20data%2C%20which%0Aoften%20occurs%20in%20remote-sensing%20interpretation%20due%20to%20the%20intricate%20fine-grained%0Asemantics.%20In%20this%20paper%2C%20we%20propose%20Continual%20Panoptic%20Perception%20%28CPP%29%2C%20a%0Aunified%20continual%20learning%20model%20that%20leverages%20multi-task%20joint%20learning%0Acovering%20pixel-level%20classification%2C%20instance-level%20segmentation%20and%0Aimage-level%20perception%20for%20universal%20interpretation%20in%20remote%20sensing%20images.%0AConcretely%2C%20we%20propose%20a%20collaborative%20cross-modal%20encoder%20%28CCE%29%20to%20extract%20the%0Ainput%20image%20features%2C%20which%20supports%20pixel%20classification%20and%20caption%0Ageneration%20synchronously.%20To%20inherit%20the%20knowledge%20from%20the%20old%20model%20without%0Aexemplar%20memory%2C%20we%20propose%20a%20task-interactive%20knowledge%20distillation%20%28TKD%29%0Amethod%2C%20which%20leverages%20cross-modal%20optimization%20and%20task-asymmetric%0Apseudo-labeling%20%28TPL%29%20to%20alleviate%20catastrophic%20forgetting.%20Furthermore%2C%20we%0Aalso%20propose%20a%20joint%20optimization%20mechanism%20to%20achieve%20end-to-end%20multi-modal%0Apanoptic%20perception.%20Experimental%20results%20on%20the%20fine-grained%20panoptic%0Aperception%20dataset%20validate%20the%20effectiveness%20of%20the%20proposed%20model%2C%20and%20also%0Aprove%20that%20joint%20optimization%20can%20boost%20sub-task%20CL%20efficiency%20with%20over%2013%5C%25%0Arelative%20improvement%20on%20panoptic%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14242v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Panoptic%2520Perception%253A%2520Towards%2520Multi-modal%2520Incremental%250A%2520%2520Interpretation%2520of%2520Remote%2520Sensing%2520Images%26entry.906535625%3DBo%2520Yuan%2520and%2520Danpei%2520Zhao%2520and%2520Zhuoran%2520Liu%2520and%2520Wentao%2520Li%2520and%2520Tian%2520Li%26entry.1292438233%3D%2520%2520Continual%2520learning%2520%2528CL%2529%2520breaks%2520off%2520the%2520one-way%2520training%2520manner%2520and%2520enables%2520a%250Amodel%2520to%2520adapt%2520to%2520new%2520data%252C%2520semantics%2520and%2520tasks%2520continuously.%2520However%252C%2520current%250ACL%2520methods%2520mainly%2520focus%2520on%2520single%2520tasks.%2520Besides%252C%2520CL%2520models%2520are%2520plagued%2520by%250Acatastrophic%2520forgetting%2520and%2520semantic%2520drift%2520since%2520the%2520lack%2520of%2520old%2520data%252C%2520which%250Aoften%2520occurs%2520in%2520remote-sensing%2520interpretation%2520due%2520to%2520the%2520intricate%2520fine-grained%250Asemantics.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Continual%2520Panoptic%2520Perception%2520%2528CPP%2529%252C%2520a%250Aunified%2520continual%2520learning%2520model%2520that%2520leverages%2520multi-task%2520joint%2520learning%250Acovering%2520pixel-level%2520classification%252C%2520instance-level%2520segmentation%2520and%250Aimage-level%2520perception%2520for%2520universal%2520interpretation%2520in%2520remote%2520sensing%2520images.%250AConcretely%252C%2520we%2520propose%2520a%2520collaborative%2520cross-modal%2520encoder%2520%2528CCE%2529%2520to%2520extract%2520the%250Ainput%2520image%2520features%252C%2520which%2520supports%2520pixel%2520classification%2520and%2520caption%250Ageneration%2520synchronously.%2520To%2520inherit%2520the%2520knowledge%2520from%2520the%2520old%2520model%2520without%250Aexemplar%2520memory%252C%2520we%2520propose%2520a%2520task-interactive%2520knowledge%2520distillation%2520%2528TKD%2529%250Amethod%252C%2520which%2520leverages%2520cross-modal%2520optimization%2520and%2520task-asymmetric%250Apseudo-labeling%2520%2528TPL%2529%2520to%2520alleviate%2520catastrophic%2520forgetting.%2520Furthermore%252C%2520we%250Aalso%2520propose%2520a%2520joint%2520optimization%2520mechanism%2520to%2520achieve%2520end-to-end%2520multi-modal%250Apanoptic%2520perception.%2520Experimental%2520results%2520on%2520the%2520fine-grained%2520panoptic%250Aperception%2520dataset%2520validate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520model%252C%2520and%2520also%250Aprove%2520that%2520joint%2520optimization%2520can%2520boost%2520sub-task%2520CL%2520efficiency%2520with%2520over%252013%255C%2525%250Arelative%2520improvement%2520on%2520panoptic%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14242v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Panoptic%20Perception%3A%20Towards%20Multi-modal%20Incremental%0A%20%20Interpretation%20of%20Remote%20Sensing%20Images&entry.906535625=Bo%20Yuan%20and%20Danpei%20Zhao%20and%20Zhuoran%20Liu%20and%20Wentao%20Li%20and%20Tian%20Li&entry.1292438233=%20%20Continual%20learning%20%28CL%29%20breaks%20off%20the%20one-way%20training%20manner%20and%20enables%20a%0Amodel%20to%20adapt%20to%20new%20data%2C%20semantics%20and%20tasks%20continuously.%20However%2C%20current%0ACL%20methods%20mainly%20focus%20on%20single%20tasks.%20Besides%2C%20CL%20models%20are%20plagued%20by%0Acatastrophic%20forgetting%20and%20semantic%20drift%20since%20the%20lack%20of%20old%20data%2C%20which%0Aoften%20occurs%20in%20remote-sensing%20interpretation%20due%20to%20the%20intricate%20fine-grained%0Asemantics.%20In%20this%20paper%2C%20we%20propose%20Continual%20Panoptic%20Perception%20%28CPP%29%2C%20a%0Aunified%20continual%20learning%20model%20that%20leverages%20multi-task%20joint%20learning%0Acovering%20pixel-level%20classification%2C%20instance-level%20segmentation%20and%0Aimage-level%20perception%20for%20universal%20interpretation%20in%20remote%20sensing%20images.%0AConcretely%2C%20we%20propose%20a%20collaborative%20cross-modal%20encoder%20%28CCE%29%20to%20extract%20the%0Ainput%20image%20features%2C%20which%20supports%20pixel%20classification%20and%20caption%0Ageneration%20synchronously.%20To%20inherit%20the%20knowledge%20from%20the%20old%20model%20without%0Aexemplar%20memory%2C%20we%20propose%20a%20task-interactive%20knowledge%20distillation%20%28TKD%29%0Amethod%2C%20which%20leverages%20cross-modal%20optimization%20and%20task-asymmetric%0Apseudo-labeling%20%28TPL%29%20to%20alleviate%20catastrophic%20forgetting.%20Furthermore%2C%20we%0Aalso%20propose%20a%20joint%20optimization%20mechanism%20to%20achieve%20end-to-end%20multi-modal%0Apanoptic%20perception.%20Experimental%20results%20on%20the%20fine-grained%20panoptic%0Aperception%20dataset%20validate%20the%20effectiveness%20of%20the%20proposed%20model%2C%20and%20also%0Aprove%20that%20joint%20optimization%20can%20boost%20sub-task%20CL%20efficiency%20with%20over%2013%5C%25%0Arelative%20improvement%20on%20panoptic%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14242v1&entry.124074799=Read"},
{"title": "Interior Object Geometry via Fitted Frames", "author": "Stephen M. Pizer and Zhiyuan Liu and Junjie Zhao and Nicholas Tapp-Hughes and James Damon and Miaomiao Zhang and JS Marron and Jared Vicory", "abstract": "  We describe a representation targeted for anatomic objects which is designed\nto enable strong locational correspondence within object populations and thus\nto provide powerful object statistics. The method generates fitted frames on\nthe boundary and in the interior of objects and produces alignment-free\ngeometric features from them. It accomplishes this by understanding an object\nas the diffeomorphic deformation of an ellipsoid and using a skeletal\nrepresentation fitted throughout the deformation to produce a model of the\ntarget object, where the object is provided initially in the form of a boundary\nmesh. Via classification performance on hippocampi shape between individuals\nwith a disorder vs. others, we compare our method to two state-of-the-art\nmethods for producing object representations that are intended to capture\ngeometric correspondence across a population of objects and to yield geometric\nfeatures useful for statistics, and we show improved classification performance\nby this new representation, which we call the evolutionary s-rep. The geometric\nfeatures that are derived from each of the representations, especially via\nfitted frames, is discussed.\n", "link": "http://arxiv.org/abs/2407.14357v1", "date": "2024-07-19", "relevancy": 2.4534, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5309}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4792}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interior%20Object%20Geometry%20via%20Fitted%20Frames&body=Title%3A%20Interior%20Object%20Geometry%20via%20Fitted%20Frames%0AAuthor%3A%20Stephen%20M.%20Pizer%20and%20Zhiyuan%20Liu%20and%20Junjie%20Zhao%20and%20Nicholas%20Tapp-Hughes%20and%20James%20Damon%20and%20Miaomiao%20Zhang%20and%20JS%20Marron%20and%20Jared%20Vicory%0AAbstract%3A%20%20%20We%20describe%20a%20representation%20targeted%20for%20anatomic%20objects%20which%20is%20designed%0Ato%20enable%20strong%20locational%20correspondence%20within%20object%20populations%20and%20thus%0Ato%20provide%20powerful%20object%20statistics.%20The%20method%20generates%20fitted%20frames%20on%0Athe%20boundary%20and%20in%20the%20interior%20of%20objects%20and%20produces%20alignment-free%0Ageometric%20features%20from%20them.%20It%20accomplishes%20this%20by%20understanding%20an%20object%0Aas%20the%20diffeomorphic%20deformation%20of%20an%20ellipsoid%20and%20using%20a%20skeletal%0Arepresentation%20fitted%20throughout%20the%20deformation%20to%20produce%20a%20model%20of%20the%0Atarget%20object%2C%20where%20the%20object%20is%20provided%20initially%20in%20the%20form%20of%20a%20boundary%0Amesh.%20Via%20classification%20performance%20on%20hippocampi%20shape%20between%20individuals%0Awith%20a%20disorder%20vs.%20others%2C%20we%20compare%20our%20method%20to%20two%20state-of-the-art%0Amethods%20for%20producing%20object%20representations%20that%20are%20intended%20to%20capture%0Ageometric%20correspondence%20across%20a%20population%20of%20objects%20and%20to%20yield%20geometric%0Afeatures%20useful%20for%20statistics%2C%20and%20we%20show%20improved%20classification%20performance%0Aby%20this%20new%20representation%2C%20which%20we%20call%20the%20evolutionary%20s-rep.%20The%20geometric%0Afeatures%20that%20are%20derived%20from%20each%20of%20the%20representations%2C%20especially%20via%0Afitted%20frames%2C%20is%20discussed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14357v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterior%2520Object%2520Geometry%2520via%2520Fitted%2520Frames%26entry.906535625%3DStephen%2520M.%2520Pizer%2520and%2520Zhiyuan%2520Liu%2520and%2520Junjie%2520Zhao%2520and%2520Nicholas%2520Tapp-Hughes%2520and%2520James%2520Damon%2520and%2520Miaomiao%2520Zhang%2520and%2520JS%2520Marron%2520and%2520Jared%2520Vicory%26entry.1292438233%3D%2520%2520We%2520describe%2520a%2520representation%2520targeted%2520for%2520anatomic%2520objects%2520which%2520is%2520designed%250Ato%2520enable%2520strong%2520locational%2520correspondence%2520within%2520object%2520populations%2520and%2520thus%250Ato%2520provide%2520powerful%2520object%2520statistics.%2520The%2520method%2520generates%2520fitted%2520frames%2520on%250Athe%2520boundary%2520and%2520in%2520the%2520interior%2520of%2520objects%2520and%2520produces%2520alignment-free%250Ageometric%2520features%2520from%2520them.%2520It%2520accomplishes%2520this%2520by%2520understanding%2520an%2520object%250Aas%2520the%2520diffeomorphic%2520deformation%2520of%2520an%2520ellipsoid%2520and%2520using%2520a%2520skeletal%250Arepresentation%2520fitted%2520throughout%2520the%2520deformation%2520to%2520produce%2520a%2520model%2520of%2520the%250Atarget%2520object%252C%2520where%2520the%2520object%2520is%2520provided%2520initially%2520in%2520the%2520form%2520of%2520a%2520boundary%250Amesh.%2520Via%2520classification%2520performance%2520on%2520hippocampi%2520shape%2520between%2520individuals%250Awith%2520a%2520disorder%2520vs.%2520others%252C%2520we%2520compare%2520our%2520method%2520to%2520two%2520state-of-the-art%250Amethods%2520for%2520producing%2520object%2520representations%2520that%2520are%2520intended%2520to%2520capture%250Ageometric%2520correspondence%2520across%2520a%2520population%2520of%2520objects%2520and%2520to%2520yield%2520geometric%250Afeatures%2520useful%2520for%2520statistics%252C%2520and%2520we%2520show%2520improved%2520classification%2520performance%250Aby%2520this%2520new%2520representation%252C%2520which%2520we%2520call%2520the%2520evolutionary%2520s-rep.%2520The%2520geometric%250Afeatures%2520that%2520are%2520derived%2520from%2520each%2520of%2520the%2520representations%252C%2520especially%2520via%250Afitted%2520frames%252C%2520is%2520discussed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14357v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interior%20Object%20Geometry%20via%20Fitted%20Frames&entry.906535625=Stephen%20M.%20Pizer%20and%20Zhiyuan%20Liu%20and%20Junjie%20Zhao%20and%20Nicholas%20Tapp-Hughes%20and%20James%20Damon%20and%20Miaomiao%20Zhang%20and%20JS%20Marron%20and%20Jared%20Vicory&entry.1292438233=%20%20We%20describe%20a%20representation%20targeted%20for%20anatomic%20objects%20which%20is%20designed%0Ato%20enable%20strong%20locational%20correspondence%20within%20object%20populations%20and%20thus%0Ato%20provide%20powerful%20object%20statistics.%20The%20method%20generates%20fitted%20frames%20on%0Athe%20boundary%20and%20in%20the%20interior%20of%20objects%20and%20produces%20alignment-free%0Ageometric%20features%20from%20them.%20It%20accomplishes%20this%20by%20understanding%20an%20object%0Aas%20the%20diffeomorphic%20deformation%20of%20an%20ellipsoid%20and%20using%20a%20skeletal%0Arepresentation%20fitted%20throughout%20the%20deformation%20to%20produce%20a%20model%20of%20the%0Atarget%20object%2C%20where%20the%20object%20is%20provided%20initially%20in%20the%20form%20of%20a%20boundary%0Amesh.%20Via%20classification%20performance%20on%20hippocampi%20shape%20between%20individuals%0Awith%20a%20disorder%20vs.%20others%2C%20we%20compare%20our%20method%20to%20two%20state-of-the-art%0Amethods%20for%20producing%20object%20representations%20that%20are%20intended%20to%20capture%0Ageometric%20correspondence%20across%20a%20population%20of%20objects%20and%20to%20yield%20geometric%0Afeatures%20useful%20for%20statistics%2C%20and%20we%20show%20improved%20classification%20performance%0Aby%20this%20new%20representation%2C%20which%20we%20call%20the%20evolutionary%20s-rep.%20The%20geometric%0Afeatures%20that%20are%20derived%20from%20each%20of%20the%20representations%2C%20especially%20via%0Afitted%20frames%2C%20is%20discussed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14357v1&entry.124074799=Read"},
{"title": "Hierarchical Windowed Graph Attention Network and a Large Scale Dataset\n  for Isolated Indian Sign Language Recognition", "author": "Suvajit Patra and Arkadip Maitra and Megha Tiwari and K. Kumaran and Swathy Prabhu and Swami Punyeshwarananda and Soumitra Samanta", "abstract": "  Automatic Sign Language (SL) recognition is an important task in the computer\nvision community. To build a robust SL recognition system, we need a\nconsiderable amount of data which is lacking particularly in Indian sign\nlanguage (ISL). In this paper, we propose a large-scale isolated ISL dataset\nand a novel SL recognition model based on skeleton graph structure. The dataset\ncovers 2,002 daily used common words in the deaf community recorded by 20 (10\nmale and 10 female) deaf adult signers (contains 40033 videos). We propose a SL\nrecognition model namely Hierarchical Windowed Graph Attention Network (HWGAT)\nby utilizing the human upper body skeleton graph structure. The HWGAT tries to\ncapture distinctive motions by giving attention to different body parts induced\nby the human skeleton graph structure. The utility of the proposed dataset and\nthe usefulness of our model are evaluated through extensive experiments. We\npre-trained the proposed model on the proposed dataset and fine-tuned it across\ndifferent sign language datasets further boosting the performance of 1.10,\n0.46, 0.78, and 6.84 percentage points on INCLUDE, LSA64, AUTSL and WLASL\nrespectively compared to the existing state-of-the-art skeleton-based models.\n", "link": "http://arxiv.org/abs/2407.14224v1", "date": "2024-07-19", "relevancy": 2.4102, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4876}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4818}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Windowed%20Graph%20Attention%20Network%20and%20a%20Large%20Scale%20Dataset%0A%20%20for%20Isolated%20Indian%20Sign%20Language%20Recognition&body=Title%3A%20Hierarchical%20Windowed%20Graph%20Attention%20Network%20and%20a%20Large%20Scale%20Dataset%0A%20%20for%20Isolated%20Indian%20Sign%20Language%20Recognition%0AAuthor%3A%20Suvajit%20Patra%20and%20Arkadip%20Maitra%20and%20Megha%20Tiwari%20and%20K.%20Kumaran%20and%20Swathy%20Prabhu%20and%20Swami%20Punyeshwarananda%20and%20Soumitra%20Samanta%0AAbstract%3A%20%20%20Automatic%20Sign%20Language%20%28SL%29%20recognition%20is%20an%20important%20task%20in%20the%20computer%0Avision%20community.%20To%20build%20a%20robust%20SL%20recognition%20system%2C%20we%20need%20a%0Aconsiderable%20amount%20of%20data%20which%20is%20lacking%20particularly%20in%20Indian%20sign%0Alanguage%20%28ISL%29.%20In%20this%20paper%2C%20we%20propose%20a%20large-scale%20isolated%20ISL%20dataset%0Aand%20a%20novel%20SL%20recognition%20model%20based%20on%20skeleton%20graph%20structure.%20The%20dataset%0Acovers%202%2C002%20daily%20used%20common%20words%20in%20the%20deaf%20community%20recorded%20by%2020%20%2810%0Amale%20and%2010%20female%29%20deaf%20adult%20signers%20%28contains%2040033%20videos%29.%20We%20propose%20a%20SL%0Arecognition%20model%20namely%20Hierarchical%20Windowed%20Graph%20Attention%20Network%20%28HWGAT%29%0Aby%20utilizing%20the%20human%20upper%20body%20skeleton%20graph%20structure.%20The%20HWGAT%20tries%20to%0Acapture%20distinctive%20motions%20by%20giving%20attention%20to%20different%20body%20parts%20induced%0Aby%20the%20human%20skeleton%20graph%20structure.%20The%20utility%20of%20the%20proposed%20dataset%20and%0Athe%20usefulness%20of%20our%20model%20are%20evaluated%20through%20extensive%20experiments.%20We%0Apre-trained%20the%20proposed%20model%20on%20the%20proposed%20dataset%20and%20fine-tuned%20it%20across%0Adifferent%20sign%20language%20datasets%20further%20boosting%20the%20performance%20of%201.10%2C%0A0.46%2C%200.78%2C%20and%206.84%20percentage%20points%20on%20INCLUDE%2C%20LSA64%2C%20AUTSL%20and%20WLASL%0Arespectively%20compared%20to%20the%20existing%20state-of-the-art%20skeleton-based%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14224v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Windowed%2520Graph%2520Attention%2520Network%2520and%2520a%2520Large%2520Scale%2520Dataset%250A%2520%2520for%2520Isolated%2520Indian%2520Sign%2520Language%2520Recognition%26entry.906535625%3DSuvajit%2520Patra%2520and%2520Arkadip%2520Maitra%2520and%2520Megha%2520Tiwari%2520and%2520K.%2520Kumaran%2520and%2520Swathy%2520Prabhu%2520and%2520Swami%2520Punyeshwarananda%2520and%2520Soumitra%2520Samanta%26entry.1292438233%3D%2520%2520Automatic%2520Sign%2520Language%2520%2528SL%2529%2520recognition%2520is%2520an%2520important%2520task%2520in%2520the%2520computer%250Avision%2520community.%2520To%2520build%2520a%2520robust%2520SL%2520recognition%2520system%252C%2520we%2520need%2520a%250Aconsiderable%2520amount%2520of%2520data%2520which%2520is%2520lacking%2520particularly%2520in%2520Indian%2520sign%250Alanguage%2520%2528ISL%2529.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520large-scale%2520isolated%2520ISL%2520dataset%250Aand%2520a%2520novel%2520SL%2520recognition%2520model%2520based%2520on%2520skeleton%2520graph%2520structure.%2520The%2520dataset%250Acovers%25202%252C002%2520daily%2520used%2520common%2520words%2520in%2520the%2520deaf%2520community%2520recorded%2520by%252020%2520%252810%250Amale%2520and%252010%2520female%2529%2520deaf%2520adult%2520signers%2520%2528contains%252040033%2520videos%2529.%2520We%2520propose%2520a%2520SL%250Arecognition%2520model%2520namely%2520Hierarchical%2520Windowed%2520Graph%2520Attention%2520Network%2520%2528HWGAT%2529%250Aby%2520utilizing%2520the%2520human%2520upper%2520body%2520skeleton%2520graph%2520structure.%2520The%2520HWGAT%2520tries%2520to%250Acapture%2520distinctive%2520motions%2520by%2520giving%2520attention%2520to%2520different%2520body%2520parts%2520induced%250Aby%2520the%2520human%2520skeleton%2520graph%2520structure.%2520The%2520utility%2520of%2520the%2520proposed%2520dataset%2520and%250Athe%2520usefulness%2520of%2520our%2520model%2520are%2520evaluated%2520through%2520extensive%2520experiments.%2520We%250Apre-trained%2520the%2520proposed%2520model%2520on%2520the%2520proposed%2520dataset%2520and%2520fine-tuned%2520it%2520across%250Adifferent%2520sign%2520language%2520datasets%2520further%2520boosting%2520the%2520performance%2520of%25201.10%252C%250A0.46%252C%25200.78%252C%2520and%25206.84%2520percentage%2520points%2520on%2520INCLUDE%252C%2520LSA64%252C%2520AUTSL%2520and%2520WLASL%250Arespectively%2520compared%2520to%2520the%2520existing%2520state-of-the-art%2520skeleton-based%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14224v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Windowed%20Graph%20Attention%20Network%20and%20a%20Large%20Scale%20Dataset%0A%20%20for%20Isolated%20Indian%20Sign%20Language%20Recognition&entry.906535625=Suvajit%20Patra%20and%20Arkadip%20Maitra%20and%20Megha%20Tiwari%20and%20K.%20Kumaran%20and%20Swathy%20Prabhu%20and%20Swami%20Punyeshwarananda%20and%20Soumitra%20Samanta&entry.1292438233=%20%20Automatic%20Sign%20Language%20%28SL%29%20recognition%20is%20an%20important%20task%20in%20the%20computer%0Avision%20community.%20To%20build%20a%20robust%20SL%20recognition%20system%2C%20we%20need%20a%0Aconsiderable%20amount%20of%20data%20which%20is%20lacking%20particularly%20in%20Indian%20sign%0Alanguage%20%28ISL%29.%20In%20this%20paper%2C%20we%20propose%20a%20large-scale%20isolated%20ISL%20dataset%0Aand%20a%20novel%20SL%20recognition%20model%20based%20on%20skeleton%20graph%20structure.%20The%20dataset%0Acovers%202%2C002%20daily%20used%20common%20words%20in%20the%20deaf%20community%20recorded%20by%2020%20%2810%0Amale%20and%2010%20female%29%20deaf%20adult%20signers%20%28contains%2040033%20videos%29.%20We%20propose%20a%20SL%0Arecognition%20model%20namely%20Hierarchical%20Windowed%20Graph%20Attention%20Network%20%28HWGAT%29%0Aby%20utilizing%20the%20human%20upper%20body%20skeleton%20graph%20structure.%20The%20HWGAT%20tries%20to%0Acapture%20distinctive%20motions%20by%20giving%20attention%20to%20different%20body%20parts%20induced%0Aby%20the%20human%20skeleton%20graph%20structure.%20The%20utility%20of%20the%20proposed%20dataset%20and%0Athe%20usefulness%20of%20our%20model%20are%20evaluated%20through%20extensive%20experiments.%20We%0Apre-trained%20the%20proposed%20model%20on%20the%20proposed%20dataset%20and%20fine-tuned%20it%20across%0Adifferent%20sign%20language%20datasets%20further%20boosting%20the%20performance%20of%201.10%2C%0A0.46%2C%200.78%2C%20and%206.84%20percentage%20points%20on%20INCLUDE%2C%20LSA64%2C%20AUTSL%20and%20WLASL%0Arespectively%20compared%20to%20the%20existing%20state-of-the-art%20skeleton-based%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14224v1&entry.124074799=Read"},
{"title": "PD-TPE: Parallel Decoder with Text-guided Position Encoding for 3D\n  Visual Grounding", "author": "Chenshu Hou and Liang Peng and Xiaopei Wu and Wenxiao Wang and Xiaofei He", "abstract": "  3D visual grounding aims to locate the target object mentioned by free-formed\nnatural language descriptions in 3D point cloud scenes. Most previous work\nrequires the encoder-decoder to simultaneously align the attribute information\nof the target object and its relational information with the surrounding\nenvironment across modalities. This causes the queries' attention to be\ndispersed, potentially leading to an excessive focus on points irrelevant to\nthe input language descriptions. To alleviate these issues, we propose PD-TPE,\na visual-language model with a double-branch decoder. The two branches perform\nproposal feature decoding and surrounding layout awareness in parallel. Since\ntheir attention maps are not influenced by each other, the queries focus on\ntokens relevant to each branch's specific objective. In particular, we design a\nnovel Text-guided Position Encoding method, which differs between the two\nbranches. In the main branch, the priori relies on the relative positions\nbetween tokens and predicted 3D boxes, which direct the model to pay more\nattention to tokens near the object; in the surrounding branch, it is guided by\nthe similarity between visual and text features, so that the queries attend to\ntokens that can provide effective layout information. Extensive experiments\ndemonstrate that we surpass the state-of-the-art on two widely adopted 3D\nvisual grounding datasets, ScanRefer and NR3D, by 1.8% and 2.2%, respectively.\nCodes will be made publicly available.\n", "link": "http://arxiv.org/abs/2407.14491v1", "date": "2024-07-19", "relevancy": 2.3944, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6067}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.604}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PD-TPE%3A%20Parallel%20Decoder%20with%20Text-guided%20Position%20Encoding%20for%203D%0A%20%20Visual%20Grounding&body=Title%3A%20PD-TPE%3A%20Parallel%20Decoder%20with%20Text-guided%20Position%20Encoding%20for%203D%0A%20%20Visual%20Grounding%0AAuthor%3A%20Chenshu%20Hou%20and%20Liang%20Peng%20and%20Xiaopei%20Wu%20and%20Wenxiao%20Wang%20and%20Xiaofei%20He%0AAbstract%3A%20%20%203D%20visual%20grounding%20aims%20to%20locate%20the%20target%20object%20mentioned%20by%20free-formed%0Anatural%20language%20descriptions%20in%203D%20point%20cloud%20scenes.%20Most%20previous%20work%0Arequires%20the%20encoder-decoder%20to%20simultaneously%20align%20the%20attribute%20information%0Aof%20the%20target%20object%20and%20its%20relational%20information%20with%20the%20surrounding%0Aenvironment%20across%20modalities.%20This%20causes%20the%20queries%27%20attention%20to%20be%0Adispersed%2C%20potentially%20leading%20to%20an%20excessive%20focus%20on%20points%20irrelevant%20to%0Athe%20input%20language%20descriptions.%20To%20alleviate%20these%20issues%2C%20we%20propose%20PD-TPE%2C%0Aa%20visual-language%20model%20with%20a%20double-branch%20decoder.%20The%20two%20branches%20perform%0Aproposal%20feature%20decoding%20and%20surrounding%20layout%20awareness%20in%20parallel.%20Since%0Atheir%20attention%20maps%20are%20not%20influenced%20by%20each%20other%2C%20the%20queries%20focus%20on%0Atokens%20relevant%20to%20each%20branch%27s%20specific%20objective.%20In%20particular%2C%20we%20design%20a%0Anovel%20Text-guided%20Position%20Encoding%20method%2C%20which%20differs%20between%20the%20two%0Abranches.%20In%20the%20main%20branch%2C%20the%20priori%20relies%20on%20the%20relative%20positions%0Abetween%20tokens%20and%20predicted%203D%20boxes%2C%20which%20direct%20the%20model%20to%20pay%20more%0Aattention%20to%20tokens%20near%20the%20object%3B%20in%20the%20surrounding%20branch%2C%20it%20is%20guided%20by%0Athe%20similarity%20between%20visual%20and%20text%20features%2C%20so%20that%20the%20queries%20attend%20to%0Atokens%20that%20can%20provide%20effective%20layout%20information.%20Extensive%20experiments%0Ademonstrate%20that%20we%20surpass%20the%20state-of-the-art%20on%20two%20widely%20adopted%203D%0Avisual%20grounding%20datasets%2C%20ScanRefer%20and%20NR3D%2C%20by%201.8%25%20and%202.2%25%2C%20respectively.%0ACodes%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14491v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPD-TPE%253A%2520Parallel%2520Decoder%2520with%2520Text-guided%2520Position%2520Encoding%2520for%25203D%250A%2520%2520Visual%2520Grounding%26entry.906535625%3DChenshu%2520Hou%2520and%2520Liang%2520Peng%2520and%2520Xiaopei%2520Wu%2520and%2520Wenxiao%2520Wang%2520and%2520Xiaofei%2520He%26entry.1292438233%3D%2520%25203D%2520visual%2520grounding%2520aims%2520to%2520locate%2520the%2520target%2520object%2520mentioned%2520by%2520free-formed%250Anatural%2520language%2520descriptions%2520in%25203D%2520point%2520cloud%2520scenes.%2520Most%2520previous%2520work%250Arequires%2520the%2520encoder-decoder%2520to%2520simultaneously%2520align%2520the%2520attribute%2520information%250Aof%2520the%2520target%2520object%2520and%2520its%2520relational%2520information%2520with%2520the%2520surrounding%250Aenvironment%2520across%2520modalities.%2520This%2520causes%2520the%2520queries%2527%2520attention%2520to%2520be%250Adispersed%252C%2520potentially%2520leading%2520to%2520an%2520excessive%2520focus%2520on%2520points%2520irrelevant%2520to%250Athe%2520input%2520language%2520descriptions.%2520To%2520alleviate%2520these%2520issues%252C%2520we%2520propose%2520PD-TPE%252C%250Aa%2520visual-language%2520model%2520with%2520a%2520double-branch%2520decoder.%2520The%2520two%2520branches%2520perform%250Aproposal%2520feature%2520decoding%2520and%2520surrounding%2520layout%2520awareness%2520in%2520parallel.%2520Since%250Atheir%2520attention%2520maps%2520are%2520not%2520influenced%2520by%2520each%2520other%252C%2520the%2520queries%2520focus%2520on%250Atokens%2520relevant%2520to%2520each%2520branch%2527s%2520specific%2520objective.%2520In%2520particular%252C%2520we%2520design%2520a%250Anovel%2520Text-guided%2520Position%2520Encoding%2520method%252C%2520which%2520differs%2520between%2520the%2520two%250Abranches.%2520In%2520the%2520main%2520branch%252C%2520the%2520priori%2520relies%2520on%2520the%2520relative%2520positions%250Abetween%2520tokens%2520and%2520predicted%25203D%2520boxes%252C%2520which%2520direct%2520the%2520model%2520to%2520pay%2520more%250Aattention%2520to%2520tokens%2520near%2520the%2520object%253B%2520in%2520the%2520surrounding%2520branch%252C%2520it%2520is%2520guided%2520by%250Athe%2520similarity%2520between%2520visual%2520and%2520text%2520features%252C%2520so%2520that%2520the%2520queries%2520attend%2520to%250Atokens%2520that%2520can%2520provide%2520effective%2520layout%2520information.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520we%2520surpass%2520the%2520state-of-the-art%2520on%2520two%2520widely%2520adopted%25203D%250Avisual%2520grounding%2520datasets%252C%2520ScanRefer%2520and%2520NR3D%252C%2520by%25201.8%2525%2520and%25202.2%2525%252C%2520respectively.%250ACodes%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14491v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PD-TPE%3A%20Parallel%20Decoder%20with%20Text-guided%20Position%20Encoding%20for%203D%0A%20%20Visual%20Grounding&entry.906535625=Chenshu%20Hou%20and%20Liang%20Peng%20and%20Xiaopei%20Wu%20and%20Wenxiao%20Wang%20and%20Xiaofei%20He&entry.1292438233=%20%203D%20visual%20grounding%20aims%20to%20locate%20the%20target%20object%20mentioned%20by%20free-formed%0Anatural%20language%20descriptions%20in%203D%20point%20cloud%20scenes.%20Most%20previous%20work%0Arequires%20the%20encoder-decoder%20to%20simultaneously%20align%20the%20attribute%20information%0Aof%20the%20target%20object%20and%20its%20relational%20information%20with%20the%20surrounding%0Aenvironment%20across%20modalities.%20This%20causes%20the%20queries%27%20attention%20to%20be%0Adispersed%2C%20potentially%20leading%20to%20an%20excessive%20focus%20on%20points%20irrelevant%20to%0Athe%20input%20language%20descriptions.%20To%20alleviate%20these%20issues%2C%20we%20propose%20PD-TPE%2C%0Aa%20visual-language%20model%20with%20a%20double-branch%20decoder.%20The%20two%20branches%20perform%0Aproposal%20feature%20decoding%20and%20surrounding%20layout%20awareness%20in%20parallel.%20Since%0Atheir%20attention%20maps%20are%20not%20influenced%20by%20each%20other%2C%20the%20queries%20focus%20on%0Atokens%20relevant%20to%20each%20branch%27s%20specific%20objective.%20In%20particular%2C%20we%20design%20a%0Anovel%20Text-guided%20Position%20Encoding%20method%2C%20which%20differs%20between%20the%20two%0Abranches.%20In%20the%20main%20branch%2C%20the%20priori%20relies%20on%20the%20relative%20positions%0Abetween%20tokens%20and%20predicted%203D%20boxes%2C%20which%20direct%20the%20model%20to%20pay%20more%0Aattention%20to%20tokens%20near%20the%20object%3B%20in%20the%20surrounding%20branch%2C%20it%20is%20guided%20by%0Athe%20similarity%20between%20visual%20and%20text%20features%2C%20so%20that%20the%20queries%20attend%20to%0Atokens%20that%20can%20provide%20effective%20layout%20information.%20Extensive%20experiments%0Ademonstrate%20that%20we%20surpass%20the%20state-of-the-art%20on%20two%20widely%20adopted%203D%0Avisual%20grounding%20datasets%2C%20ScanRefer%20and%20NR3D%2C%20by%201.8%25%20and%202.2%25%2C%20respectively.%0ACodes%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14491v1&entry.124074799=Read"},
{"title": "Equivariant Symmetries for Aided Inertial Navigation", "author": "Alessandro Fornasier", "abstract": "  Respecting the geometry of the underlying system and exploiting its symmetry\nhave been driving concepts in deriving modern geometric filters for inertial\nnavigation systems (INSs). Despite their success, the explicit treatment of\ninertial measurement unit (IMU) biases remains challenging, unveiling a gap in\nthe current theory of filter design. In response to this gap, this dissertation\nbuilds upon the recent theory of equivariant systems to address and overcome\nthe limitations in existing methodologies. The goal is to identify new\nsymmetries of inertial navigation systems that include a geometric treatment of\nIMU biases and exploit them to design filtering algorithms that outperform\nstate-of-the-art solutions in terms of accuracy, convergence rate, robustness,\nand consistency. This dissertation leverages the semi-direct product rule and\nintroduces the tangent group for inertial navigation systems as the first\nequivariant symmetry that properly accounts for IMU biases. Based on that, we\nshow that it is possible to derive an equivariant filter (EqF) algorithm with\nautonomous navigation error dynamics. The resulting filter demonstrates\nsuperior to state-of-the-art solutions. Through a comprehensive analysis of\nvarious symmetries of inertial navigation systems, we formalized the concept\nthat every filter can be derived as an EqF with a specific choice of symmetry.\nThis underlines the fundamental role of symmetry in determining filter\nperformance. This dissertation advances the understanding of equivariant\nsymmetries in the context of inertial navigation systems and serves as a basis\nfor the next generation of equivariant estimators, marking a significant leap\ntoward more reliable navigation solutions.\n", "link": "http://arxiv.org/abs/2407.14297v1", "date": "2024-07-19", "relevancy": 2.3394, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.502}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4634}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Equivariant%20Symmetries%20for%20Aided%20Inertial%20Navigation&body=Title%3A%20Equivariant%20Symmetries%20for%20Aided%20Inertial%20Navigation%0AAuthor%3A%20Alessandro%20Fornasier%0AAbstract%3A%20%20%20Respecting%20the%20geometry%20of%20the%20underlying%20system%20and%20exploiting%20its%20symmetry%0Ahave%20been%20driving%20concepts%20in%20deriving%20modern%20geometric%20filters%20for%20inertial%0Anavigation%20systems%20%28INSs%29.%20Despite%20their%20success%2C%20the%20explicit%20treatment%20of%0Ainertial%20measurement%20unit%20%28IMU%29%20biases%20remains%20challenging%2C%20unveiling%20a%20gap%20in%0Athe%20current%20theory%20of%20filter%20design.%20In%20response%20to%20this%20gap%2C%20this%20dissertation%0Abuilds%20upon%20the%20recent%20theory%20of%20equivariant%20systems%20to%20address%20and%20overcome%0Athe%20limitations%20in%20existing%20methodologies.%20The%20goal%20is%20to%20identify%20new%0Asymmetries%20of%20inertial%20navigation%20systems%20that%20include%20a%20geometric%20treatment%20of%0AIMU%20biases%20and%20exploit%20them%20to%20design%20filtering%20algorithms%20that%20outperform%0Astate-of-the-art%20solutions%20in%20terms%20of%20accuracy%2C%20convergence%20rate%2C%20robustness%2C%0Aand%20consistency.%20This%20dissertation%20leverages%20the%20semi-direct%20product%20rule%20and%0Aintroduces%20the%20tangent%20group%20for%20inertial%20navigation%20systems%20as%20the%20first%0Aequivariant%20symmetry%20that%20properly%20accounts%20for%20IMU%20biases.%20Based%20on%20that%2C%20we%0Ashow%20that%20it%20is%20possible%20to%20derive%20an%20equivariant%20filter%20%28EqF%29%20algorithm%20with%0Aautonomous%20navigation%20error%20dynamics.%20The%20resulting%20filter%20demonstrates%0Asuperior%20to%20state-of-the-art%20solutions.%20Through%20a%20comprehensive%20analysis%20of%0Avarious%20symmetries%20of%20inertial%20navigation%20systems%2C%20we%20formalized%20the%20concept%0Athat%20every%20filter%20can%20be%20derived%20as%20an%20EqF%20with%20a%20specific%20choice%20of%20symmetry.%0AThis%20underlines%20the%20fundamental%20role%20of%20symmetry%20in%20determining%20filter%0Aperformance.%20This%20dissertation%20advances%20the%20understanding%20of%20equivariant%0Asymmetries%20in%20the%20context%20of%20inertial%20navigation%20systems%20and%20serves%20as%20a%20basis%0Afor%20the%20next%20generation%20of%20equivariant%20estimators%2C%20marking%20a%20significant%20leap%0Atoward%20more%20reliable%20navigation%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14297v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquivariant%2520Symmetries%2520for%2520Aided%2520Inertial%2520Navigation%26entry.906535625%3DAlessandro%2520Fornasier%26entry.1292438233%3D%2520%2520Respecting%2520the%2520geometry%2520of%2520the%2520underlying%2520system%2520and%2520exploiting%2520its%2520symmetry%250Ahave%2520been%2520driving%2520concepts%2520in%2520deriving%2520modern%2520geometric%2520filters%2520for%2520inertial%250Anavigation%2520systems%2520%2528INSs%2529.%2520Despite%2520their%2520success%252C%2520the%2520explicit%2520treatment%2520of%250Ainertial%2520measurement%2520unit%2520%2528IMU%2529%2520biases%2520remains%2520challenging%252C%2520unveiling%2520a%2520gap%2520in%250Athe%2520current%2520theory%2520of%2520filter%2520design.%2520In%2520response%2520to%2520this%2520gap%252C%2520this%2520dissertation%250Abuilds%2520upon%2520the%2520recent%2520theory%2520of%2520equivariant%2520systems%2520to%2520address%2520and%2520overcome%250Athe%2520limitations%2520in%2520existing%2520methodologies.%2520The%2520goal%2520is%2520to%2520identify%2520new%250Asymmetries%2520of%2520inertial%2520navigation%2520systems%2520that%2520include%2520a%2520geometric%2520treatment%2520of%250AIMU%2520biases%2520and%2520exploit%2520them%2520to%2520design%2520filtering%2520algorithms%2520that%2520outperform%250Astate-of-the-art%2520solutions%2520in%2520terms%2520of%2520accuracy%252C%2520convergence%2520rate%252C%2520robustness%252C%250Aand%2520consistency.%2520This%2520dissertation%2520leverages%2520the%2520semi-direct%2520product%2520rule%2520and%250Aintroduces%2520the%2520tangent%2520group%2520for%2520inertial%2520navigation%2520systems%2520as%2520the%2520first%250Aequivariant%2520symmetry%2520that%2520properly%2520accounts%2520for%2520IMU%2520biases.%2520Based%2520on%2520that%252C%2520we%250Ashow%2520that%2520it%2520is%2520possible%2520to%2520derive%2520an%2520equivariant%2520filter%2520%2528EqF%2529%2520algorithm%2520with%250Aautonomous%2520navigation%2520error%2520dynamics.%2520The%2520resulting%2520filter%2520demonstrates%250Asuperior%2520to%2520state-of-the-art%2520solutions.%2520Through%2520a%2520comprehensive%2520analysis%2520of%250Avarious%2520symmetries%2520of%2520inertial%2520navigation%2520systems%252C%2520we%2520formalized%2520the%2520concept%250Athat%2520every%2520filter%2520can%2520be%2520derived%2520as%2520an%2520EqF%2520with%2520a%2520specific%2520choice%2520of%2520symmetry.%250AThis%2520underlines%2520the%2520fundamental%2520role%2520of%2520symmetry%2520in%2520determining%2520filter%250Aperformance.%2520This%2520dissertation%2520advances%2520the%2520understanding%2520of%2520equivariant%250Asymmetries%2520in%2520the%2520context%2520of%2520inertial%2520navigation%2520systems%2520and%2520serves%2520as%2520a%2520basis%250Afor%2520the%2520next%2520generation%2520of%2520equivariant%2520estimators%252C%2520marking%2520a%2520significant%2520leap%250Atoward%2520more%2520reliable%2520navigation%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14297v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equivariant%20Symmetries%20for%20Aided%20Inertial%20Navigation&entry.906535625=Alessandro%20Fornasier&entry.1292438233=%20%20Respecting%20the%20geometry%20of%20the%20underlying%20system%20and%20exploiting%20its%20symmetry%0Ahave%20been%20driving%20concepts%20in%20deriving%20modern%20geometric%20filters%20for%20inertial%0Anavigation%20systems%20%28INSs%29.%20Despite%20their%20success%2C%20the%20explicit%20treatment%20of%0Ainertial%20measurement%20unit%20%28IMU%29%20biases%20remains%20challenging%2C%20unveiling%20a%20gap%20in%0Athe%20current%20theory%20of%20filter%20design.%20In%20response%20to%20this%20gap%2C%20this%20dissertation%0Abuilds%20upon%20the%20recent%20theory%20of%20equivariant%20systems%20to%20address%20and%20overcome%0Athe%20limitations%20in%20existing%20methodologies.%20The%20goal%20is%20to%20identify%20new%0Asymmetries%20of%20inertial%20navigation%20systems%20that%20include%20a%20geometric%20treatment%20of%0AIMU%20biases%20and%20exploit%20them%20to%20design%20filtering%20algorithms%20that%20outperform%0Astate-of-the-art%20solutions%20in%20terms%20of%20accuracy%2C%20convergence%20rate%2C%20robustness%2C%0Aand%20consistency.%20This%20dissertation%20leverages%20the%20semi-direct%20product%20rule%20and%0Aintroduces%20the%20tangent%20group%20for%20inertial%20navigation%20systems%20as%20the%20first%0Aequivariant%20symmetry%20that%20properly%20accounts%20for%20IMU%20biases.%20Based%20on%20that%2C%20we%0Ashow%20that%20it%20is%20possible%20to%20derive%20an%20equivariant%20filter%20%28EqF%29%20algorithm%20with%0Aautonomous%20navigation%20error%20dynamics.%20The%20resulting%20filter%20demonstrates%0Asuperior%20to%20state-of-the-art%20solutions.%20Through%20a%20comprehensive%20analysis%20of%0Avarious%20symmetries%20of%20inertial%20navigation%20systems%2C%20we%20formalized%20the%20concept%0Athat%20every%20filter%20can%20be%20derived%20as%20an%20EqF%20with%20a%20specific%20choice%20of%20symmetry.%0AThis%20underlines%20the%20fundamental%20role%20of%20symmetry%20in%20determining%20filter%0Aperformance.%20This%20dissertation%20advances%20the%20understanding%20of%20equivariant%0Asymmetries%20in%20the%20context%20of%20inertial%20navigation%20systems%20and%20serves%20as%20a%20basis%0Afor%20the%20next%20generation%20of%20equivariant%20estimators%2C%20marking%20a%20significant%20leap%0Atoward%20more%20reliable%20navigation%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14297v1&entry.124074799=Read"},
{"title": "Panoptic Segmentation of Mammograms with Text-To-Image Diffusion Model", "author": "Kun Zhao and Jakub Prokop and Javier Montalt Tordera and Sadegh Mohammadi", "abstract": "  Mammography is crucial for breast cancer surveillance and early diagnosis.\nHowever, analyzing mammography images is a demanding task for radiologists, who\noften review hundreds of mammograms daily, leading to overdiagnosis and\novertreatment. Computer-Aided Diagnosis (CAD) systems have been developed to\nassist in this process, but their capabilities, particularly in lesion\nsegmentation, remained limited. With the contemporary advances in deep learning\ntheir performance may be improved. Recently, vision-language diffusion models\nemerged, demonstrating outstanding performance in image generation and\ntransferability to various downstream tasks. We aim to harness their\ncapabilities for breast lesion segmentation in a panoptic setting, which\nencompasses both semantic and instance-level predictions. Specifically, we\npropose leveraging pretrained features from a Stable Diffusion model as inputs\nto a state-of-the-art panoptic segmentation architecture, resulting in accurate\ndelineation of individual breast lesions. To bridge the gap between natural and\nmedical imaging domains, we incorporated a mammography-specific MAM-E diffusion\nmodel and BiomedCLIP image and text encoders into this framework. We evaluated\nour approach on two recently published mammography datasets, CDD-CESM and\nVinDr-Mammo. For the instance segmentation task, we noted 40.25 AP0.1 and 46.82\nAP0.05, as well as 25.44 PQ0.1 and 26.92 PQ0.05. For the semantic segmentation\ntask, we achieved Dice scores of 38.86 and 40.92, respectively.\n", "link": "http://arxiv.org/abs/2407.14326v1", "date": "2024-07-19", "relevancy": 2.292, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.611}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5654}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Panoptic%20Segmentation%20of%20Mammograms%20with%20Text-To-Image%20Diffusion%20Model&body=Title%3A%20Panoptic%20Segmentation%20of%20Mammograms%20with%20Text-To-Image%20Diffusion%20Model%0AAuthor%3A%20Kun%20Zhao%20and%20Jakub%20Prokop%20and%20Javier%20Montalt%20Tordera%20and%20Sadegh%20Mohammadi%0AAbstract%3A%20%20%20Mammography%20is%20crucial%20for%20breast%20cancer%20surveillance%20and%20early%20diagnosis.%0AHowever%2C%20analyzing%20mammography%20images%20is%20a%20demanding%20task%20for%20radiologists%2C%20who%0Aoften%20review%20hundreds%20of%20mammograms%20daily%2C%20leading%20to%20overdiagnosis%20and%0Aovertreatment.%20Computer-Aided%20Diagnosis%20%28CAD%29%20systems%20have%20been%20developed%20to%0Aassist%20in%20this%20process%2C%20but%20their%20capabilities%2C%20particularly%20in%20lesion%0Asegmentation%2C%20remained%20limited.%20With%20the%20contemporary%20advances%20in%20deep%20learning%0Atheir%20performance%20may%20be%20improved.%20Recently%2C%20vision-language%20diffusion%20models%0Aemerged%2C%20demonstrating%20outstanding%20performance%20in%20image%20generation%20and%0Atransferability%20to%20various%20downstream%20tasks.%20We%20aim%20to%20harness%20their%0Acapabilities%20for%20breast%20lesion%20segmentation%20in%20a%20panoptic%20setting%2C%20which%0Aencompasses%20both%20semantic%20and%20instance-level%20predictions.%20Specifically%2C%20we%0Apropose%20leveraging%20pretrained%20features%20from%20a%20Stable%20Diffusion%20model%20as%20inputs%0Ato%20a%20state-of-the-art%20panoptic%20segmentation%20architecture%2C%20resulting%20in%20accurate%0Adelineation%20of%20individual%20breast%20lesions.%20To%20bridge%20the%20gap%20between%20natural%20and%0Amedical%20imaging%20domains%2C%20we%20incorporated%20a%20mammography-specific%20MAM-E%20diffusion%0Amodel%20and%20BiomedCLIP%20image%20and%20text%20encoders%20into%20this%20framework.%20We%20evaluated%0Aour%20approach%20on%20two%20recently%20published%20mammography%20datasets%2C%20CDD-CESM%20and%0AVinDr-Mammo.%20For%20the%20instance%20segmentation%20task%2C%20we%20noted%2040.25%20AP0.1%20and%2046.82%0AAP0.05%2C%20as%20well%20as%2025.44%20PQ0.1%20and%2026.92%20PQ0.05.%20For%20the%20semantic%20segmentation%0Atask%2C%20we%20achieved%20Dice%20scores%20of%2038.86%20and%2040.92%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14326v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPanoptic%2520Segmentation%2520of%2520Mammograms%2520with%2520Text-To-Image%2520Diffusion%2520Model%26entry.906535625%3DKun%2520Zhao%2520and%2520Jakub%2520Prokop%2520and%2520Javier%2520Montalt%2520Tordera%2520and%2520Sadegh%2520Mohammadi%26entry.1292438233%3D%2520%2520Mammography%2520is%2520crucial%2520for%2520breast%2520cancer%2520surveillance%2520and%2520early%2520diagnosis.%250AHowever%252C%2520analyzing%2520mammography%2520images%2520is%2520a%2520demanding%2520task%2520for%2520radiologists%252C%2520who%250Aoften%2520review%2520hundreds%2520of%2520mammograms%2520daily%252C%2520leading%2520to%2520overdiagnosis%2520and%250Aovertreatment.%2520Computer-Aided%2520Diagnosis%2520%2528CAD%2529%2520systems%2520have%2520been%2520developed%2520to%250Aassist%2520in%2520this%2520process%252C%2520but%2520their%2520capabilities%252C%2520particularly%2520in%2520lesion%250Asegmentation%252C%2520remained%2520limited.%2520With%2520the%2520contemporary%2520advances%2520in%2520deep%2520learning%250Atheir%2520performance%2520may%2520be%2520improved.%2520Recently%252C%2520vision-language%2520diffusion%2520models%250Aemerged%252C%2520demonstrating%2520outstanding%2520performance%2520in%2520image%2520generation%2520and%250Atransferability%2520to%2520various%2520downstream%2520tasks.%2520We%2520aim%2520to%2520harness%2520their%250Acapabilities%2520for%2520breast%2520lesion%2520segmentation%2520in%2520a%2520panoptic%2520setting%252C%2520which%250Aencompasses%2520both%2520semantic%2520and%2520instance-level%2520predictions.%2520Specifically%252C%2520we%250Apropose%2520leveraging%2520pretrained%2520features%2520from%2520a%2520Stable%2520Diffusion%2520model%2520as%2520inputs%250Ato%2520a%2520state-of-the-art%2520panoptic%2520segmentation%2520architecture%252C%2520resulting%2520in%2520accurate%250Adelineation%2520of%2520individual%2520breast%2520lesions.%2520To%2520bridge%2520the%2520gap%2520between%2520natural%2520and%250Amedical%2520imaging%2520domains%252C%2520we%2520incorporated%2520a%2520mammography-specific%2520MAM-E%2520diffusion%250Amodel%2520and%2520BiomedCLIP%2520image%2520and%2520text%2520encoders%2520into%2520this%2520framework.%2520We%2520evaluated%250Aour%2520approach%2520on%2520two%2520recently%2520published%2520mammography%2520datasets%252C%2520CDD-CESM%2520and%250AVinDr-Mammo.%2520For%2520the%2520instance%2520segmentation%2520task%252C%2520we%2520noted%252040.25%2520AP0.1%2520and%252046.82%250AAP0.05%252C%2520as%2520well%2520as%252025.44%2520PQ0.1%2520and%252026.92%2520PQ0.05.%2520For%2520the%2520semantic%2520segmentation%250Atask%252C%2520we%2520achieved%2520Dice%2520scores%2520of%252038.86%2520and%252040.92%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14326v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Panoptic%20Segmentation%20of%20Mammograms%20with%20Text-To-Image%20Diffusion%20Model&entry.906535625=Kun%20Zhao%20and%20Jakub%20Prokop%20and%20Javier%20Montalt%20Tordera%20and%20Sadegh%20Mohammadi&entry.1292438233=%20%20Mammography%20is%20crucial%20for%20breast%20cancer%20surveillance%20and%20early%20diagnosis.%0AHowever%2C%20analyzing%20mammography%20images%20is%20a%20demanding%20task%20for%20radiologists%2C%20who%0Aoften%20review%20hundreds%20of%20mammograms%20daily%2C%20leading%20to%20overdiagnosis%20and%0Aovertreatment.%20Computer-Aided%20Diagnosis%20%28CAD%29%20systems%20have%20been%20developed%20to%0Aassist%20in%20this%20process%2C%20but%20their%20capabilities%2C%20particularly%20in%20lesion%0Asegmentation%2C%20remained%20limited.%20With%20the%20contemporary%20advances%20in%20deep%20learning%0Atheir%20performance%20may%20be%20improved.%20Recently%2C%20vision-language%20diffusion%20models%0Aemerged%2C%20demonstrating%20outstanding%20performance%20in%20image%20generation%20and%0Atransferability%20to%20various%20downstream%20tasks.%20We%20aim%20to%20harness%20their%0Acapabilities%20for%20breast%20lesion%20segmentation%20in%20a%20panoptic%20setting%2C%20which%0Aencompasses%20both%20semantic%20and%20instance-level%20predictions.%20Specifically%2C%20we%0Apropose%20leveraging%20pretrained%20features%20from%20a%20Stable%20Diffusion%20model%20as%20inputs%0Ato%20a%20state-of-the-art%20panoptic%20segmentation%20architecture%2C%20resulting%20in%20accurate%0Adelineation%20of%20individual%20breast%20lesions.%20To%20bridge%20the%20gap%20between%20natural%20and%0Amedical%20imaging%20domains%2C%20we%20incorporated%20a%20mammography-specific%20MAM-E%20diffusion%0Amodel%20and%20BiomedCLIP%20image%20and%20text%20encoders%20into%20this%20framework.%20We%20evaluated%0Aour%20approach%20on%20two%20recently%20published%20mammography%20datasets%2C%20CDD-CESM%20and%0AVinDr-Mammo.%20For%20the%20instance%20segmentation%20task%2C%20we%20noted%2040.25%20AP0.1%20and%2046.82%0AAP0.05%2C%20as%20well%20as%2025.44%20PQ0.1%20and%2026.92%20PQ0.05.%20For%20the%20semantic%20segmentation%0Atask%2C%20we%20achieved%20Dice%20scores%20of%2038.86%20and%2040.92%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14326v1&entry.124074799=Read"},
{"title": "A Benchmark for Gaussian Splatting Compression and Quality Assessment\n  Study", "author": "Qi Yang and Kaifa Yang and Yuke Xing and Yiling Xu and Zhu Li", "abstract": "  To fill the gap of traditional GS compression method, in this paper, we first\npropose a simple and effective GS data compression anchor called Graph-based GS\nCompression (GGSC). GGSC is inspired by graph signal processing theory and uses\ntwo branches to compress the primitive center and attributes. We split the\nwhole GS sample via KDTree and clip the high-frequency components after the\ngraph Fourier transform. Followed by quantization, G-PCC and adaptive\narithmetic coding are used to compress the primitive center and attribute\nresidual matrix to generate the bitrate file. GGSS is the first work to explore\ntraditional GS compression, with advantages that can reveal the GS distortion\ncharacteristics corresponding to typical compression operation, such as\nhigh-frequency clipping and quantization. Second, based on GGSC, we create a GS\nQuality Assessment dataset (GSQA) with 120 samples. A subjective experiment is\nconducted in a laboratory environment to collect subjective scores after\nrendering GS into Processed Video Sequences (PVS). We analyze the\ncharacteristics of different GS distortions based on Mean Opinion Scores (MOS),\ndemonstrating the sensitivity of different attributes distortion to visual\nquality. The GGSC code and the dataset, including GS samples, MOS, and PVS, are\nmade publicly available at https://github.com/Qi-Yangsjtu/GGSC.\n", "link": "http://arxiv.org/abs/2407.14197v1", "date": "2024-07-19", "relevancy": 2.2816, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5956}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5915}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Benchmark%20for%20Gaussian%20Splatting%20Compression%20and%20Quality%20Assessment%0A%20%20Study&body=Title%3A%20A%20Benchmark%20for%20Gaussian%20Splatting%20Compression%20and%20Quality%20Assessment%0A%20%20Study%0AAuthor%3A%20Qi%20Yang%20and%20Kaifa%20Yang%20and%20Yuke%20Xing%20and%20Yiling%20Xu%20and%20Zhu%20Li%0AAbstract%3A%20%20%20To%20fill%20the%20gap%20of%20traditional%20GS%20compression%20method%2C%20in%20this%20paper%2C%20we%20first%0Apropose%20a%20simple%20and%20effective%20GS%20data%20compression%20anchor%20called%20Graph-based%20GS%0ACompression%20%28GGSC%29.%20GGSC%20is%20inspired%20by%20graph%20signal%20processing%20theory%20and%20uses%0Atwo%20branches%20to%20compress%20the%20primitive%20center%20and%20attributes.%20We%20split%20the%0Awhole%20GS%20sample%20via%20KDTree%20and%20clip%20the%20high-frequency%20components%20after%20the%0Agraph%20Fourier%20transform.%20Followed%20by%20quantization%2C%20G-PCC%20and%20adaptive%0Aarithmetic%20coding%20are%20used%20to%20compress%20the%20primitive%20center%20and%20attribute%0Aresidual%20matrix%20to%20generate%20the%20bitrate%20file.%20GGSS%20is%20the%20first%20work%20to%20explore%0Atraditional%20GS%20compression%2C%20with%20advantages%20that%20can%20reveal%20the%20GS%20distortion%0Acharacteristics%20corresponding%20to%20typical%20compression%20operation%2C%20such%20as%0Ahigh-frequency%20clipping%20and%20quantization.%20Second%2C%20based%20on%20GGSC%2C%20we%20create%20a%20GS%0AQuality%20Assessment%20dataset%20%28GSQA%29%20with%20120%20samples.%20A%20subjective%20experiment%20is%0Aconducted%20in%20a%20laboratory%20environment%20to%20collect%20subjective%20scores%20after%0Arendering%20GS%20into%20Processed%20Video%20Sequences%20%28PVS%29.%20We%20analyze%20the%0Acharacteristics%20of%20different%20GS%20distortions%20based%20on%20Mean%20Opinion%20Scores%20%28MOS%29%2C%0Ademonstrating%20the%20sensitivity%20of%20different%20attributes%20distortion%20to%20visual%0Aquality.%20The%20GGSC%20code%20and%20the%20dataset%2C%20including%20GS%20samples%2C%20MOS%2C%20and%20PVS%2C%20are%0Amade%20publicly%20available%20at%20https%3A//github.com/Qi-Yangsjtu/GGSC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14197v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Benchmark%2520for%2520Gaussian%2520Splatting%2520Compression%2520and%2520Quality%2520Assessment%250A%2520%2520Study%26entry.906535625%3DQi%2520Yang%2520and%2520Kaifa%2520Yang%2520and%2520Yuke%2520Xing%2520and%2520Yiling%2520Xu%2520and%2520Zhu%2520Li%26entry.1292438233%3D%2520%2520To%2520fill%2520the%2520gap%2520of%2520traditional%2520GS%2520compression%2520method%252C%2520in%2520this%2520paper%252C%2520we%2520first%250Apropose%2520a%2520simple%2520and%2520effective%2520GS%2520data%2520compression%2520anchor%2520called%2520Graph-based%2520GS%250ACompression%2520%2528GGSC%2529.%2520GGSC%2520is%2520inspired%2520by%2520graph%2520signal%2520processing%2520theory%2520and%2520uses%250Atwo%2520branches%2520to%2520compress%2520the%2520primitive%2520center%2520and%2520attributes.%2520We%2520split%2520the%250Awhole%2520GS%2520sample%2520via%2520KDTree%2520and%2520clip%2520the%2520high-frequency%2520components%2520after%2520the%250Agraph%2520Fourier%2520transform.%2520Followed%2520by%2520quantization%252C%2520G-PCC%2520and%2520adaptive%250Aarithmetic%2520coding%2520are%2520used%2520to%2520compress%2520the%2520primitive%2520center%2520and%2520attribute%250Aresidual%2520matrix%2520to%2520generate%2520the%2520bitrate%2520file.%2520GGSS%2520is%2520the%2520first%2520work%2520to%2520explore%250Atraditional%2520GS%2520compression%252C%2520with%2520advantages%2520that%2520can%2520reveal%2520the%2520GS%2520distortion%250Acharacteristics%2520corresponding%2520to%2520typical%2520compression%2520operation%252C%2520such%2520as%250Ahigh-frequency%2520clipping%2520and%2520quantization.%2520Second%252C%2520based%2520on%2520GGSC%252C%2520we%2520create%2520a%2520GS%250AQuality%2520Assessment%2520dataset%2520%2528GSQA%2529%2520with%2520120%2520samples.%2520A%2520subjective%2520experiment%2520is%250Aconducted%2520in%2520a%2520laboratory%2520environment%2520to%2520collect%2520subjective%2520scores%2520after%250Arendering%2520GS%2520into%2520Processed%2520Video%2520Sequences%2520%2528PVS%2529.%2520We%2520analyze%2520the%250Acharacteristics%2520of%2520different%2520GS%2520distortions%2520based%2520on%2520Mean%2520Opinion%2520Scores%2520%2528MOS%2529%252C%250Ademonstrating%2520the%2520sensitivity%2520of%2520different%2520attributes%2520distortion%2520to%2520visual%250Aquality.%2520The%2520GGSC%2520code%2520and%2520the%2520dataset%252C%2520including%2520GS%2520samples%252C%2520MOS%252C%2520and%2520PVS%252C%2520are%250Amade%2520publicly%2520available%2520at%2520https%253A//github.com/Qi-Yangsjtu/GGSC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14197v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Benchmark%20for%20Gaussian%20Splatting%20Compression%20and%20Quality%20Assessment%0A%20%20Study&entry.906535625=Qi%20Yang%20and%20Kaifa%20Yang%20and%20Yuke%20Xing%20and%20Yiling%20Xu%20and%20Zhu%20Li&entry.1292438233=%20%20To%20fill%20the%20gap%20of%20traditional%20GS%20compression%20method%2C%20in%20this%20paper%2C%20we%20first%0Apropose%20a%20simple%20and%20effective%20GS%20data%20compression%20anchor%20called%20Graph-based%20GS%0ACompression%20%28GGSC%29.%20GGSC%20is%20inspired%20by%20graph%20signal%20processing%20theory%20and%20uses%0Atwo%20branches%20to%20compress%20the%20primitive%20center%20and%20attributes.%20We%20split%20the%0Awhole%20GS%20sample%20via%20KDTree%20and%20clip%20the%20high-frequency%20components%20after%20the%0Agraph%20Fourier%20transform.%20Followed%20by%20quantization%2C%20G-PCC%20and%20adaptive%0Aarithmetic%20coding%20are%20used%20to%20compress%20the%20primitive%20center%20and%20attribute%0Aresidual%20matrix%20to%20generate%20the%20bitrate%20file.%20GGSS%20is%20the%20first%20work%20to%20explore%0Atraditional%20GS%20compression%2C%20with%20advantages%20that%20can%20reveal%20the%20GS%20distortion%0Acharacteristics%20corresponding%20to%20typical%20compression%20operation%2C%20such%20as%0Ahigh-frequency%20clipping%20and%20quantization.%20Second%2C%20based%20on%20GGSC%2C%20we%20create%20a%20GS%0AQuality%20Assessment%20dataset%20%28GSQA%29%20with%20120%20samples.%20A%20subjective%20experiment%20is%0Aconducted%20in%20a%20laboratory%20environment%20to%20collect%20subjective%20scores%20after%0Arendering%20GS%20into%20Processed%20Video%20Sequences%20%28PVS%29.%20We%20analyze%20the%0Acharacteristics%20of%20different%20GS%20distortions%20based%20on%20Mean%20Opinion%20Scores%20%28MOS%29%2C%0Ademonstrating%20the%20sensitivity%20of%20different%20attributes%20distortion%20to%20visual%0Aquality.%20The%20GGSC%20code%20and%20the%20dataset%2C%20including%20GS%20samples%2C%20MOS%2C%20and%20PVS%2C%20are%0Amade%20publicly%20available%20at%20https%3A//github.com/Qi-Yangsjtu/GGSC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14197v1&entry.124074799=Read"},
{"title": "T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video\n  Generation", "author": "Kaiyue Sun and Kaiyi Huang and Xian Liu and Yue Wu and Zihan Xu and Zhenguo Li and Xihui Liu", "abstract": "  Text-to-video (T2V) generation models have advanced significantly, yet their\nability to compose different objects, attributes, actions, and motions into a\nvideo remains unexplored. Previous text-to-video benchmarks also neglect this\nimportant ability for evaluation. In this work, we conduct the first systematic\nstudy on compositional text-to-video generation. We propose T2V-CompBench, the\nfirst benchmark tailored for compositional text-to-video generation.\nT2V-CompBench encompasses diverse aspects of compositionality, including\nconsistent attribute binding, dynamic attribute binding, spatial relationships,\nmotion binding, action binding, object interactions, and generative numeracy.\nWe further carefully design evaluation metrics of MLLM-based metrics,\ndetection-based metrics, and tracking-based metrics, which can better reflect\nthe compositional text-to-video generation quality of seven proposed categories\nwith 700 text prompts. The effectiveness of the proposed metrics is verified by\ncorrelation with human evaluations. We also benchmark various text-to-video\ngenerative models and conduct in-depth analysis across different models and\ndifferent compositional categories. We find that compositional text-to-video\ngeneration is highly challenging for current models, and we hope that our\nattempt will shed light on future research in this direction.\n", "link": "http://arxiv.org/abs/2407.14505v1", "date": "2024-07-19", "relevancy": 2.2768, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6003}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5935}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T2V-CompBench%3A%20A%20Comprehensive%20Benchmark%20for%20Compositional%20Text-to-video%0A%20%20Generation&body=Title%3A%20T2V-CompBench%3A%20A%20Comprehensive%20Benchmark%20for%20Compositional%20Text-to-video%0A%20%20Generation%0AAuthor%3A%20Kaiyue%20Sun%20and%20Kaiyi%20Huang%20and%20Xian%20Liu%20and%20Yue%20Wu%20and%20Zihan%20Xu%20and%20Zhenguo%20Li%20and%20Xihui%20Liu%0AAbstract%3A%20%20%20Text-to-video%20%28T2V%29%20generation%20models%20have%20advanced%20significantly%2C%20yet%20their%0Aability%20to%20compose%20different%20objects%2C%20attributes%2C%20actions%2C%20and%20motions%20into%20a%0Avideo%20remains%20unexplored.%20Previous%20text-to-video%20benchmarks%20also%20neglect%20this%0Aimportant%20ability%20for%20evaluation.%20In%20this%20work%2C%20we%20conduct%20the%20first%20systematic%0Astudy%20on%20compositional%20text-to-video%20generation.%20We%20propose%20T2V-CompBench%2C%20the%0Afirst%20benchmark%20tailored%20for%20compositional%20text-to-video%20generation.%0AT2V-CompBench%20encompasses%20diverse%20aspects%20of%20compositionality%2C%20including%0Aconsistent%20attribute%20binding%2C%20dynamic%20attribute%20binding%2C%20spatial%20relationships%2C%0Amotion%20binding%2C%20action%20binding%2C%20object%20interactions%2C%20and%20generative%20numeracy.%0AWe%20further%20carefully%20design%20evaluation%20metrics%20of%20MLLM-based%20metrics%2C%0Adetection-based%20metrics%2C%20and%20tracking-based%20metrics%2C%20which%20can%20better%20reflect%0Athe%20compositional%20text-to-video%20generation%20quality%20of%20seven%20proposed%20categories%0Awith%20700%20text%20prompts.%20The%20effectiveness%20of%20the%20proposed%20metrics%20is%20verified%20by%0Acorrelation%20with%20human%20evaluations.%20We%20also%20benchmark%20various%20text-to-video%0Agenerative%20models%20and%20conduct%20in-depth%20analysis%20across%20different%20models%20and%0Adifferent%20compositional%20categories.%20We%20find%20that%20compositional%20text-to-video%0Ageneration%20is%20highly%20challenging%20for%20current%20models%2C%20and%20we%20hope%20that%20our%0Aattempt%20will%20shed%20light%20on%20future%20research%20in%20this%20direction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14505v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT2V-CompBench%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Compositional%2520Text-to-video%250A%2520%2520Generation%26entry.906535625%3DKaiyue%2520Sun%2520and%2520Kaiyi%2520Huang%2520and%2520Xian%2520Liu%2520and%2520Yue%2520Wu%2520and%2520Zihan%2520Xu%2520and%2520Zhenguo%2520Li%2520and%2520Xihui%2520Liu%26entry.1292438233%3D%2520%2520Text-to-video%2520%2528T2V%2529%2520generation%2520models%2520have%2520advanced%2520significantly%252C%2520yet%2520their%250Aability%2520to%2520compose%2520different%2520objects%252C%2520attributes%252C%2520actions%252C%2520and%2520motions%2520into%2520a%250Avideo%2520remains%2520unexplored.%2520Previous%2520text-to-video%2520benchmarks%2520also%2520neglect%2520this%250Aimportant%2520ability%2520for%2520evaluation.%2520In%2520this%2520work%252C%2520we%2520conduct%2520the%2520first%2520systematic%250Astudy%2520on%2520compositional%2520text-to-video%2520generation.%2520We%2520propose%2520T2V-CompBench%252C%2520the%250Afirst%2520benchmark%2520tailored%2520for%2520compositional%2520text-to-video%2520generation.%250AT2V-CompBench%2520encompasses%2520diverse%2520aspects%2520of%2520compositionality%252C%2520including%250Aconsistent%2520attribute%2520binding%252C%2520dynamic%2520attribute%2520binding%252C%2520spatial%2520relationships%252C%250Amotion%2520binding%252C%2520action%2520binding%252C%2520object%2520interactions%252C%2520and%2520generative%2520numeracy.%250AWe%2520further%2520carefully%2520design%2520evaluation%2520metrics%2520of%2520MLLM-based%2520metrics%252C%250Adetection-based%2520metrics%252C%2520and%2520tracking-based%2520metrics%252C%2520which%2520can%2520better%2520reflect%250Athe%2520compositional%2520text-to-video%2520generation%2520quality%2520of%2520seven%2520proposed%2520categories%250Awith%2520700%2520text%2520prompts.%2520The%2520effectiveness%2520of%2520the%2520proposed%2520metrics%2520is%2520verified%2520by%250Acorrelation%2520with%2520human%2520evaluations.%2520We%2520also%2520benchmark%2520various%2520text-to-video%250Agenerative%2520models%2520and%2520conduct%2520in-depth%2520analysis%2520across%2520different%2520models%2520and%250Adifferent%2520compositional%2520categories.%2520We%2520find%2520that%2520compositional%2520text-to-video%250Ageneration%2520is%2520highly%2520challenging%2520for%2520current%2520models%252C%2520and%2520we%2520hope%2520that%2520our%250Aattempt%2520will%2520shed%2520light%2520on%2520future%2520research%2520in%2520this%2520direction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14505v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T2V-CompBench%3A%20A%20Comprehensive%20Benchmark%20for%20Compositional%20Text-to-video%0A%20%20Generation&entry.906535625=Kaiyue%20Sun%20and%20Kaiyi%20Huang%20and%20Xian%20Liu%20and%20Yue%20Wu%20and%20Zihan%20Xu%20and%20Zhenguo%20Li%20and%20Xihui%20Liu&entry.1292438233=%20%20Text-to-video%20%28T2V%29%20generation%20models%20have%20advanced%20significantly%2C%20yet%20their%0Aability%20to%20compose%20different%20objects%2C%20attributes%2C%20actions%2C%20and%20motions%20into%20a%0Avideo%20remains%20unexplored.%20Previous%20text-to-video%20benchmarks%20also%20neglect%20this%0Aimportant%20ability%20for%20evaluation.%20In%20this%20work%2C%20we%20conduct%20the%20first%20systematic%0Astudy%20on%20compositional%20text-to-video%20generation.%20We%20propose%20T2V-CompBench%2C%20the%0Afirst%20benchmark%20tailored%20for%20compositional%20text-to-video%20generation.%0AT2V-CompBench%20encompasses%20diverse%20aspects%20of%20compositionality%2C%20including%0Aconsistent%20attribute%20binding%2C%20dynamic%20attribute%20binding%2C%20spatial%20relationships%2C%0Amotion%20binding%2C%20action%20binding%2C%20object%20interactions%2C%20and%20generative%20numeracy.%0AWe%20further%20carefully%20design%20evaluation%20metrics%20of%20MLLM-based%20metrics%2C%0Adetection-based%20metrics%2C%20and%20tracking-based%20metrics%2C%20which%20can%20better%20reflect%0Athe%20compositional%20text-to-video%20generation%20quality%20of%20seven%20proposed%20categories%0Awith%20700%20text%20prompts.%20The%20effectiveness%20of%20the%20proposed%20metrics%20is%20verified%20by%0Acorrelation%20with%20human%20evaluations.%20We%20also%20benchmark%20various%20text-to-video%0Agenerative%20models%20and%20conduct%20in-depth%20analysis%20across%20different%20models%20and%0Adifferent%20compositional%20categories.%20We%20find%20that%20compositional%20text-to-video%0Ageneration%20is%20highly%20challenging%20for%20current%20models%2C%20and%20we%20hope%20that%20our%0Aattempt%20will%20shed%20light%20on%20future%20research%20in%20this%20direction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14505v1&entry.124074799=Read"},
{"title": "Large Kernel Distillation Network for Efficient Single Image\n  Super-Resolution", "author": "Chengxing Xie and Xiaoming Zhang and Linze Li and Haiteng Meng and Tianlin Zhang and Tianrui Li and Xiaole Zhao", "abstract": "  Efficient and lightweight single-image super-resolution (SISR) has achieved\nremarkable performance in recent years. One effective approach is the use of\nlarge kernel designs, which have been shown to improve the performance of SISR\nmodels while reducing their computational requirements. However, current\nstate-of-the-art (SOTA) models still face problems such as high computational\ncosts. To address these issues, we propose the Large Kernel Distillation\nNetwork (LKDN) in this paper. Our approach simplifies the model structure and\nintroduces more efficient attention modules to reduce computational costs while\nalso improving performance. Specifically, we employ the reparameterization\ntechnique to enhance model performance without adding extra cost. We also\nintroduce a new optimizer from other tasks to SISR, which improves training\nspeed and performance. Our experimental results demonstrate that LKDN\noutperforms existing lightweight SR methods and achieves SOTA performance.\n", "link": "http://arxiv.org/abs/2407.14340v1", "date": "2024-07-19", "relevancy": 2.2754, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5957}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5805}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Kernel%20Distillation%20Network%20for%20Efficient%20Single%20Image%0A%20%20Super-Resolution&body=Title%3A%20Large%20Kernel%20Distillation%20Network%20for%20Efficient%20Single%20Image%0A%20%20Super-Resolution%0AAuthor%3A%20Chengxing%20Xie%20and%20Xiaoming%20Zhang%20and%20Linze%20Li%20and%20Haiteng%20Meng%20and%20Tianlin%20Zhang%20and%20Tianrui%20Li%20and%20Xiaole%20Zhao%0AAbstract%3A%20%20%20Efficient%20and%20lightweight%20single-image%20super-resolution%20%28SISR%29%20has%20achieved%0Aremarkable%20performance%20in%20recent%20years.%20One%20effective%20approach%20is%20the%20use%20of%0Alarge%20kernel%20designs%2C%20which%20have%20been%20shown%20to%20improve%20the%20performance%20of%20SISR%0Amodels%20while%20reducing%20their%20computational%20requirements.%20However%2C%20current%0Astate-of-the-art%20%28SOTA%29%20models%20still%20face%20problems%20such%20as%20high%20computational%0Acosts.%20To%20address%20these%20issues%2C%20we%20propose%20the%20Large%20Kernel%20Distillation%0ANetwork%20%28LKDN%29%20in%20this%20paper.%20Our%20approach%20simplifies%20the%20model%20structure%20and%0Aintroduces%20more%20efficient%20attention%20modules%20to%20reduce%20computational%20costs%20while%0Aalso%20improving%20performance.%20Specifically%2C%20we%20employ%20the%20reparameterization%0Atechnique%20to%20enhance%20model%20performance%20without%20adding%20extra%20cost.%20We%20also%0Aintroduce%20a%20new%20optimizer%20from%20other%20tasks%20to%20SISR%2C%20which%20improves%20training%0Aspeed%20and%20performance.%20Our%20experimental%20results%20demonstrate%20that%20LKDN%0Aoutperforms%20existing%20lightweight%20SR%20methods%20and%20achieves%20SOTA%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14340v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Kernel%2520Distillation%2520Network%2520for%2520Efficient%2520Single%2520Image%250A%2520%2520Super-Resolution%26entry.906535625%3DChengxing%2520Xie%2520and%2520Xiaoming%2520Zhang%2520and%2520Linze%2520Li%2520and%2520Haiteng%2520Meng%2520and%2520Tianlin%2520Zhang%2520and%2520Tianrui%2520Li%2520and%2520Xiaole%2520Zhao%26entry.1292438233%3D%2520%2520Efficient%2520and%2520lightweight%2520single-image%2520super-resolution%2520%2528SISR%2529%2520has%2520achieved%250Aremarkable%2520performance%2520in%2520recent%2520years.%2520One%2520effective%2520approach%2520is%2520the%2520use%2520of%250Alarge%2520kernel%2520designs%252C%2520which%2520have%2520been%2520shown%2520to%2520improve%2520the%2520performance%2520of%2520SISR%250Amodels%2520while%2520reducing%2520their%2520computational%2520requirements.%2520However%252C%2520current%250Astate-of-the-art%2520%2528SOTA%2529%2520models%2520still%2520face%2520problems%2520such%2520as%2520high%2520computational%250Acosts.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520the%2520Large%2520Kernel%2520Distillation%250ANetwork%2520%2528LKDN%2529%2520in%2520this%2520paper.%2520Our%2520approach%2520simplifies%2520the%2520model%2520structure%2520and%250Aintroduces%2520more%2520efficient%2520attention%2520modules%2520to%2520reduce%2520computational%2520costs%2520while%250Aalso%2520improving%2520performance.%2520Specifically%252C%2520we%2520employ%2520the%2520reparameterization%250Atechnique%2520to%2520enhance%2520model%2520performance%2520without%2520adding%2520extra%2520cost.%2520We%2520also%250Aintroduce%2520a%2520new%2520optimizer%2520from%2520other%2520tasks%2520to%2520SISR%252C%2520which%2520improves%2520training%250Aspeed%2520and%2520performance.%2520Our%2520experimental%2520results%2520demonstrate%2520that%2520LKDN%250Aoutperforms%2520existing%2520lightweight%2520SR%2520methods%2520and%2520achieves%2520SOTA%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14340v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Kernel%20Distillation%20Network%20for%20Efficient%20Single%20Image%0A%20%20Super-Resolution&entry.906535625=Chengxing%20Xie%20and%20Xiaoming%20Zhang%20and%20Linze%20Li%20and%20Haiteng%20Meng%20and%20Tianlin%20Zhang%20and%20Tianrui%20Li%20and%20Xiaole%20Zhao&entry.1292438233=%20%20Efficient%20and%20lightweight%20single-image%20super-resolution%20%28SISR%29%20has%20achieved%0Aremarkable%20performance%20in%20recent%20years.%20One%20effective%20approach%20is%20the%20use%20of%0Alarge%20kernel%20designs%2C%20which%20have%20been%20shown%20to%20improve%20the%20performance%20of%20SISR%0Amodels%20while%20reducing%20their%20computational%20requirements.%20However%2C%20current%0Astate-of-the-art%20%28SOTA%29%20models%20still%20face%20problems%20such%20as%20high%20computational%0Acosts.%20To%20address%20these%20issues%2C%20we%20propose%20the%20Large%20Kernel%20Distillation%0ANetwork%20%28LKDN%29%20in%20this%20paper.%20Our%20approach%20simplifies%20the%20model%20structure%20and%0Aintroduces%20more%20efficient%20attention%20modules%20to%20reduce%20computational%20costs%20while%0Aalso%20improving%20performance.%20Specifically%2C%20we%20employ%20the%20reparameterization%0Atechnique%20to%20enhance%20model%20performance%20without%20adding%20extra%20cost.%20We%20also%0Aintroduce%20a%20new%20optimizer%20from%20other%20tasks%20to%20SISR%2C%20which%20improves%20training%0Aspeed%20and%20performance.%20Our%20experimental%20results%20demonstrate%20that%20LKDN%0Aoutperforms%20existing%20lightweight%20SR%20methods%20and%20achieves%20SOTA%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14340v1&entry.124074799=Read"},
{"title": "Token-level Correlation-guided Compression for Efficient Multimodal\n  Document Understanding", "author": "Renshan Zhang and Yibo Lyu and Rui Shao and Gongwei Chen and Weili Guan and Liqiang Nie", "abstract": "  Cropping high-resolution document images into multiple sub-images is the most\nwidely used approach for current Multimodal Large Language Models (MLLMs) to do\ndocument understanding. Most of current document understanding methods preserve\nall tokens within sub-images and treat them equally. This neglects their\ndifferent informativeness and leads to a significant increase in the number of\nimage tokens. To perform a more adaptive and efficient document understanding,\nwe propose Token-level Correlation-guided Compression, a parameter-free and\nplug-and-play methodology to optimize token processing. Firstly, we propose an\ninnovative approach for assessing the pattern repetitiveness based on the\ncorrelation between each patch tokens. This method identifies redundant tokens,\nallowing for the determination of the sub-image's information density.\nSecondly, we present a token-level sampling method that efficiently captures\nthe most informative tokens by delving into the correlation between the [CLS]\ntoken and patch tokens. By integrating these strategies, we develop a\nplug-and-play adaptive compressor module that can be seamlessly incorporated\ninto MLLMs utilizing cropping techniques. This module not only enhances the\nprocessing speed during training and inference but also maintains comparable\nperformance. We conduct experiments with the SOTA document understanding model\nmPLUG-DocOwl1.5 and the effectiveness is demonstrated through extensive\ncomparisons with other compression methods.\n", "link": "http://arxiv.org/abs/2407.14439v1", "date": "2024-07-19", "relevancy": 2.2737, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5781}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5744}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Token-level%20Correlation-guided%20Compression%20for%20Efficient%20Multimodal%0A%20%20Document%20Understanding&body=Title%3A%20Token-level%20Correlation-guided%20Compression%20for%20Efficient%20Multimodal%0A%20%20Document%20Understanding%0AAuthor%3A%20Renshan%20Zhang%20and%20Yibo%20Lyu%20and%20Rui%20Shao%20and%20Gongwei%20Chen%20and%20Weili%20Guan%20and%20Liqiang%20Nie%0AAbstract%3A%20%20%20Cropping%20high-resolution%20document%20images%20into%20multiple%20sub-images%20is%20the%20most%0Awidely%20used%20approach%20for%20current%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20to%20do%0Adocument%20understanding.%20Most%20of%20current%20document%20understanding%20methods%20preserve%0Aall%20tokens%20within%20sub-images%20and%20treat%20them%20equally.%20This%20neglects%20their%0Adifferent%20informativeness%20and%20leads%20to%20a%20significant%20increase%20in%20the%20number%20of%0Aimage%20tokens.%20To%20perform%20a%20more%20adaptive%20and%20efficient%20document%20understanding%2C%0Awe%20propose%20Token-level%20Correlation-guided%20Compression%2C%20a%20parameter-free%20and%0Aplug-and-play%20methodology%20to%20optimize%20token%20processing.%20Firstly%2C%20we%20propose%20an%0Ainnovative%20approach%20for%20assessing%20the%20pattern%20repetitiveness%20based%20on%20the%0Acorrelation%20between%20each%20patch%20tokens.%20This%20method%20identifies%20redundant%20tokens%2C%0Aallowing%20for%20the%20determination%20of%20the%20sub-image%27s%20information%20density.%0ASecondly%2C%20we%20present%20a%20token-level%20sampling%20method%20that%20efficiently%20captures%0Athe%20most%20informative%20tokens%20by%20delving%20into%20the%20correlation%20between%20the%20%5BCLS%5D%0Atoken%20and%20patch%20tokens.%20By%20integrating%20these%20strategies%2C%20we%20develop%20a%0Aplug-and-play%20adaptive%20compressor%20module%20that%20can%20be%20seamlessly%20incorporated%0Ainto%20MLLMs%20utilizing%20cropping%20techniques.%20This%20module%20not%20only%20enhances%20the%0Aprocessing%20speed%20during%20training%20and%20inference%20but%20also%20maintains%20comparable%0Aperformance.%20We%20conduct%20experiments%20with%20the%20SOTA%20document%20understanding%20model%0AmPLUG-DocOwl1.5%20and%20the%20effectiveness%20is%20demonstrated%20through%20extensive%0Acomparisons%20with%20other%20compression%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14439v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToken-level%2520Correlation-guided%2520Compression%2520for%2520Efficient%2520Multimodal%250A%2520%2520Document%2520Understanding%26entry.906535625%3DRenshan%2520Zhang%2520and%2520Yibo%2520Lyu%2520and%2520Rui%2520Shao%2520and%2520Gongwei%2520Chen%2520and%2520Weili%2520Guan%2520and%2520Liqiang%2520Nie%26entry.1292438233%3D%2520%2520Cropping%2520high-resolution%2520document%2520images%2520into%2520multiple%2520sub-images%2520is%2520the%2520most%250Awidely%2520used%2520approach%2520for%2520current%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520to%2520do%250Adocument%2520understanding.%2520Most%2520of%2520current%2520document%2520understanding%2520methods%2520preserve%250Aall%2520tokens%2520within%2520sub-images%2520and%2520treat%2520them%2520equally.%2520This%2520neglects%2520their%250Adifferent%2520informativeness%2520and%2520leads%2520to%2520a%2520significant%2520increase%2520in%2520the%2520number%2520of%250Aimage%2520tokens.%2520To%2520perform%2520a%2520more%2520adaptive%2520and%2520efficient%2520document%2520understanding%252C%250Awe%2520propose%2520Token-level%2520Correlation-guided%2520Compression%252C%2520a%2520parameter-free%2520and%250Aplug-and-play%2520methodology%2520to%2520optimize%2520token%2520processing.%2520Firstly%252C%2520we%2520propose%2520an%250Ainnovative%2520approach%2520for%2520assessing%2520the%2520pattern%2520repetitiveness%2520based%2520on%2520the%250Acorrelation%2520between%2520each%2520patch%2520tokens.%2520This%2520method%2520identifies%2520redundant%2520tokens%252C%250Aallowing%2520for%2520the%2520determination%2520of%2520the%2520sub-image%2527s%2520information%2520density.%250ASecondly%252C%2520we%2520present%2520a%2520token-level%2520sampling%2520method%2520that%2520efficiently%2520captures%250Athe%2520most%2520informative%2520tokens%2520by%2520delving%2520into%2520the%2520correlation%2520between%2520the%2520%255BCLS%255D%250Atoken%2520and%2520patch%2520tokens.%2520By%2520integrating%2520these%2520strategies%252C%2520we%2520develop%2520a%250Aplug-and-play%2520adaptive%2520compressor%2520module%2520that%2520can%2520be%2520seamlessly%2520incorporated%250Ainto%2520MLLMs%2520utilizing%2520cropping%2520techniques.%2520This%2520module%2520not%2520only%2520enhances%2520the%250Aprocessing%2520speed%2520during%2520training%2520and%2520inference%2520but%2520also%2520maintains%2520comparable%250Aperformance.%2520We%2520conduct%2520experiments%2520with%2520the%2520SOTA%2520document%2520understanding%2520model%250AmPLUG-DocOwl1.5%2520and%2520the%2520effectiveness%2520is%2520demonstrated%2520through%2520extensive%250Acomparisons%2520with%2520other%2520compression%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14439v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Token-level%20Correlation-guided%20Compression%20for%20Efficient%20Multimodal%0A%20%20Document%20Understanding&entry.906535625=Renshan%20Zhang%20and%20Yibo%20Lyu%20and%20Rui%20Shao%20and%20Gongwei%20Chen%20and%20Weili%20Guan%20and%20Liqiang%20Nie&entry.1292438233=%20%20Cropping%20high-resolution%20document%20images%20into%20multiple%20sub-images%20is%20the%20most%0Awidely%20used%20approach%20for%20current%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20to%20do%0Adocument%20understanding.%20Most%20of%20current%20document%20understanding%20methods%20preserve%0Aall%20tokens%20within%20sub-images%20and%20treat%20them%20equally.%20This%20neglects%20their%0Adifferent%20informativeness%20and%20leads%20to%20a%20significant%20increase%20in%20the%20number%20of%0Aimage%20tokens.%20To%20perform%20a%20more%20adaptive%20and%20efficient%20document%20understanding%2C%0Awe%20propose%20Token-level%20Correlation-guided%20Compression%2C%20a%20parameter-free%20and%0Aplug-and-play%20methodology%20to%20optimize%20token%20processing.%20Firstly%2C%20we%20propose%20an%0Ainnovative%20approach%20for%20assessing%20the%20pattern%20repetitiveness%20based%20on%20the%0Acorrelation%20between%20each%20patch%20tokens.%20This%20method%20identifies%20redundant%20tokens%2C%0Aallowing%20for%20the%20determination%20of%20the%20sub-image%27s%20information%20density.%0ASecondly%2C%20we%20present%20a%20token-level%20sampling%20method%20that%20efficiently%20captures%0Athe%20most%20informative%20tokens%20by%20delving%20into%20the%20correlation%20between%20the%20%5BCLS%5D%0Atoken%20and%20patch%20tokens.%20By%20integrating%20these%20strategies%2C%20we%20develop%20a%0Aplug-and-play%20adaptive%20compressor%20module%20that%20can%20be%20seamlessly%20incorporated%0Ainto%20MLLMs%20utilizing%20cropping%20techniques.%20This%20module%20not%20only%20enhances%20the%0Aprocessing%20speed%20during%20training%20and%20inference%20but%20also%20maintains%20comparable%0Aperformance.%20We%20conduct%20experiments%20with%20the%20SOTA%20document%20understanding%20model%0AmPLUG-DocOwl1.5%20and%20the%20effectiveness%20is%20demonstrated%20through%20extensive%0Acomparisons%20with%20other%20compression%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14439v1&entry.124074799=Read"},
{"title": "Complementary Learning for Real-World Model Failure Detection", "author": "Daniel Bogdoll and Finn Sartoris and Vincent Geppert and Svetlana Pavlitska and J. Marius Z\u00f6llner", "abstract": "  In real-world autonomous driving, deep learning models can experience\nperformance degradation due to distributional shifts between the training data\nand the driving conditions encountered. As is typical in machine learning, it\nis difficult to acquire a large and potentially representative labeled test set\nto validate models in preparation for deployment in the wild. In this work, we\nintroduce complementary learning, where we use learned characteristics from\ndifferent training paradigms to detect model errors. We demonstrate our\napproach by learning semantic and predictive motion labels in point clouds in a\nsupervised and self-supervised manner and detect and classify model\ndiscrepancies subsequently. We perform a large-scale qualitative analysis and\npresent LidarCODA, the first dataset with labeled anomalies in lidar point\nclouds, for an extensive quantitative analysis.\n", "link": "http://arxiv.org/abs/2407.14306v1", "date": "2024-07-19", "relevancy": 2.266, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5768}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.568}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Complementary%20Learning%20for%20Real-World%20Model%20Failure%20Detection&body=Title%3A%20Complementary%20Learning%20for%20Real-World%20Model%20Failure%20Detection%0AAuthor%3A%20Daniel%20Bogdoll%20and%20Finn%20Sartoris%20and%20Vincent%20Geppert%20and%20Svetlana%20Pavlitska%20and%20J.%20Marius%20Z%C3%B6llner%0AAbstract%3A%20%20%20In%20real-world%20autonomous%20driving%2C%20deep%20learning%20models%20can%20experience%0Aperformance%20degradation%20due%20to%20distributional%20shifts%20between%20the%20training%20data%0Aand%20the%20driving%20conditions%20encountered.%20As%20is%20typical%20in%20machine%20learning%2C%20it%0Ais%20difficult%20to%20acquire%20a%20large%20and%20potentially%20representative%20labeled%20test%20set%0Ato%20validate%20models%20in%20preparation%20for%20deployment%20in%20the%20wild.%20In%20this%20work%2C%20we%0Aintroduce%20complementary%20learning%2C%20where%20we%20use%20learned%20characteristics%20from%0Adifferent%20training%20paradigms%20to%20detect%20model%20errors.%20We%20demonstrate%20our%0Aapproach%20by%20learning%20semantic%20and%20predictive%20motion%20labels%20in%20point%20clouds%20in%20a%0Asupervised%20and%20self-supervised%20manner%20and%20detect%20and%20classify%20model%0Adiscrepancies%20subsequently.%20We%20perform%20a%20large-scale%20qualitative%20analysis%20and%0Apresent%20LidarCODA%2C%20the%20first%20dataset%20with%20labeled%20anomalies%20in%20lidar%20point%0Aclouds%2C%20for%20an%20extensive%20quantitative%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14306v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComplementary%2520Learning%2520for%2520Real-World%2520Model%2520Failure%2520Detection%26entry.906535625%3DDaniel%2520Bogdoll%2520and%2520Finn%2520Sartoris%2520and%2520Vincent%2520Geppert%2520and%2520Svetlana%2520Pavlitska%2520and%2520J.%2520Marius%2520Z%25C3%25B6llner%26entry.1292438233%3D%2520%2520In%2520real-world%2520autonomous%2520driving%252C%2520deep%2520learning%2520models%2520can%2520experience%250Aperformance%2520degradation%2520due%2520to%2520distributional%2520shifts%2520between%2520the%2520training%2520data%250Aand%2520the%2520driving%2520conditions%2520encountered.%2520As%2520is%2520typical%2520in%2520machine%2520learning%252C%2520it%250Ais%2520difficult%2520to%2520acquire%2520a%2520large%2520and%2520potentially%2520representative%2520labeled%2520test%2520set%250Ato%2520validate%2520models%2520in%2520preparation%2520for%2520deployment%2520in%2520the%2520wild.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520complementary%2520learning%252C%2520where%2520we%2520use%2520learned%2520characteristics%2520from%250Adifferent%2520training%2520paradigms%2520to%2520detect%2520model%2520errors.%2520We%2520demonstrate%2520our%250Aapproach%2520by%2520learning%2520semantic%2520and%2520predictive%2520motion%2520labels%2520in%2520point%2520clouds%2520in%2520a%250Asupervised%2520and%2520self-supervised%2520manner%2520and%2520detect%2520and%2520classify%2520model%250Adiscrepancies%2520subsequently.%2520We%2520perform%2520a%2520large-scale%2520qualitative%2520analysis%2520and%250Apresent%2520LidarCODA%252C%2520the%2520first%2520dataset%2520with%2520labeled%2520anomalies%2520in%2520lidar%2520point%250Aclouds%252C%2520for%2520an%2520extensive%2520quantitative%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14306v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Complementary%20Learning%20for%20Real-World%20Model%20Failure%20Detection&entry.906535625=Daniel%20Bogdoll%20and%20Finn%20Sartoris%20and%20Vincent%20Geppert%20and%20Svetlana%20Pavlitska%20and%20J.%20Marius%20Z%C3%B6llner&entry.1292438233=%20%20In%20real-world%20autonomous%20driving%2C%20deep%20learning%20models%20can%20experience%0Aperformance%20degradation%20due%20to%20distributional%20shifts%20between%20the%20training%20data%0Aand%20the%20driving%20conditions%20encountered.%20As%20is%20typical%20in%20machine%20learning%2C%20it%0Ais%20difficult%20to%20acquire%20a%20large%20and%20potentially%20representative%20labeled%20test%20set%0Ato%20validate%20models%20in%20preparation%20for%20deployment%20in%20the%20wild.%20In%20this%20work%2C%20we%0Aintroduce%20complementary%20learning%2C%20where%20we%20use%20learned%20characteristics%20from%0Adifferent%20training%20paradigms%20to%20detect%20model%20errors.%20We%20demonstrate%20our%0Aapproach%20by%20learning%20semantic%20and%20predictive%20motion%20labels%20in%20point%20clouds%20in%20a%0Asupervised%20and%20self-supervised%20manner%20and%20detect%20and%20classify%20model%0Adiscrepancies%20subsequently.%20We%20perform%20a%20large-scale%20qualitative%20analysis%20and%0Apresent%20LidarCODA%2C%20the%20first%20dataset%20with%20labeled%20anomalies%20in%20lidar%20point%0Aclouds%2C%20for%20an%20extensive%20quantitative%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14306v1&entry.124074799=Read"},
{"title": "EVLM: An Efficient Vision-Language Model for Visual Understanding", "author": "Kaibing Chen and Dong Shen and Hanwen Zhong and Huasong Zhong and Kui Xia and Di Xu and Wei Yuan and Yifei Hu and Bin Wen and Tianke Zhang and Changyi Liu and Dewen Fan and Huihui Xiao and Jiahong Wu and Fan Yang and Size Li and Di Zhang", "abstract": "  In the field of multi-modal language models, the majority of methods are\nbuilt on an architecture similar to LLaVA. These models use a single-layer ViT\nfeature as a visual prompt, directly feeding it into the language models\nalongside textual tokens. However, when dealing with long sequences of visual\nsignals or inputs such as videos, the self-attention mechanism of language\nmodels can lead to significant computational overhead. Additionally, using\nsingle-layer ViT features makes it challenging for large language models to\nperceive visual signals fully. This paper proposes an efficient multi-modal\nlanguage model to minimize computational costs while enabling the model to\nperceive visual signals as comprehensively as possible. Our method primarily\nincludes: (1) employing cross-attention to image-text interaction similar to\nFlamingo. (2) utilize hierarchical ViT features. (3) introduce the Mixture of\nExperts (MoE) mechanism to enhance model effectiveness. Our model achieves\ncompetitive scores on public multi-modal benchmarks and performs well in tasks\nsuch as image captioning and video captioning.\n", "link": "http://arxiv.org/abs/2407.14177v1", "date": "2024-07-19", "relevancy": 2.2462, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5762}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5594}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EVLM%3A%20An%20Efficient%20Vision-Language%20Model%20for%20Visual%20Understanding&body=Title%3A%20EVLM%3A%20An%20Efficient%20Vision-Language%20Model%20for%20Visual%20Understanding%0AAuthor%3A%20Kaibing%20Chen%20and%20Dong%20Shen%20and%20Hanwen%20Zhong%20and%20Huasong%20Zhong%20and%20Kui%20Xia%20and%20Di%20Xu%20and%20Wei%20Yuan%20and%20Yifei%20Hu%20and%20Bin%20Wen%20and%20Tianke%20Zhang%20and%20Changyi%20Liu%20and%20Dewen%20Fan%20and%20Huihui%20Xiao%20and%20Jiahong%20Wu%20and%20Fan%20Yang%20and%20Size%20Li%20and%20Di%20Zhang%0AAbstract%3A%20%20%20In%20the%20field%20of%20multi-modal%20language%20models%2C%20the%20majority%20of%20methods%20are%0Abuilt%20on%20an%20architecture%20similar%20to%20LLaVA.%20These%20models%20use%20a%20single-layer%20ViT%0Afeature%20as%20a%20visual%20prompt%2C%20directly%20feeding%20it%20into%20the%20language%20models%0Aalongside%20textual%20tokens.%20However%2C%20when%20dealing%20with%20long%20sequences%20of%20visual%0Asignals%20or%20inputs%20such%20as%20videos%2C%20the%20self-attention%20mechanism%20of%20language%0Amodels%20can%20lead%20to%20significant%20computational%20overhead.%20Additionally%2C%20using%0Asingle-layer%20ViT%20features%20makes%20it%20challenging%20for%20large%20language%20models%20to%0Aperceive%20visual%20signals%20fully.%20This%20paper%20proposes%20an%20efficient%20multi-modal%0Alanguage%20model%20to%20minimize%20computational%20costs%20while%20enabling%20the%20model%20to%0Aperceive%20visual%20signals%20as%20comprehensively%20as%20possible.%20Our%20method%20primarily%0Aincludes%3A%20%281%29%20employing%20cross-attention%20to%20image-text%20interaction%20similar%20to%0AFlamingo.%20%282%29%20utilize%20hierarchical%20ViT%20features.%20%283%29%20introduce%20the%20Mixture%20of%0AExperts%20%28MoE%29%20mechanism%20to%20enhance%20model%20effectiveness.%20Our%20model%20achieves%0Acompetitive%20scores%20on%20public%20multi-modal%20benchmarks%20and%20performs%20well%20in%20tasks%0Asuch%20as%20image%20captioning%20and%20video%20captioning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEVLM%253A%2520An%2520Efficient%2520Vision-Language%2520Model%2520for%2520Visual%2520Understanding%26entry.906535625%3DKaibing%2520Chen%2520and%2520Dong%2520Shen%2520and%2520Hanwen%2520Zhong%2520and%2520Huasong%2520Zhong%2520and%2520Kui%2520Xia%2520and%2520Di%2520Xu%2520and%2520Wei%2520Yuan%2520and%2520Yifei%2520Hu%2520and%2520Bin%2520Wen%2520and%2520Tianke%2520Zhang%2520and%2520Changyi%2520Liu%2520and%2520Dewen%2520Fan%2520and%2520Huihui%2520Xiao%2520and%2520Jiahong%2520Wu%2520and%2520Fan%2520Yang%2520and%2520Size%2520Li%2520and%2520Di%2520Zhang%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520multi-modal%2520language%2520models%252C%2520the%2520majority%2520of%2520methods%2520are%250Abuilt%2520on%2520an%2520architecture%2520similar%2520to%2520LLaVA.%2520These%2520models%2520use%2520a%2520single-layer%2520ViT%250Afeature%2520as%2520a%2520visual%2520prompt%252C%2520directly%2520feeding%2520it%2520into%2520the%2520language%2520models%250Aalongside%2520textual%2520tokens.%2520However%252C%2520when%2520dealing%2520with%2520long%2520sequences%2520of%2520visual%250Asignals%2520or%2520inputs%2520such%2520as%2520videos%252C%2520the%2520self-attention%2520mechanism%2520of%2520language%250Amodels%2520can%2520lead%2520to%2520significant%2520computational%2520overhead.%2520Additionally%252C%2520using%250Asingle-layer%2520ViT%2520features%2520makes%2520it%2520challenging%2520for%2520large%2520language%2520models%2520to%250Aperceive%2520visual%2520signals%2520fully.%2520This%2520paper%2520proposes%2520an%2520efficient%2520multi-modal%250Alanguage%2520model%2520to%2520minimize%2520computational%2520costs%2520while%2520enabling%2520the%2520model%2520to%250Aperceive%2520visual%2520signals%2520as%2520comprehensively%2520as%2520possible.%2520Our%2520method%2520primarily%250Aincludes%253A%2520%25281%2529%2520employing%2520cross-attention%2520to%2520image-text%2520interaction%2520similar%2520to%250AFlamingo.%2520%25282%2529%2520utilize%2520hierarchical%2520ViT%2520features.%2520%25283%2529%2520introduce%2520the%2520Mixture%2520of%250AExperts%2520%2528MoE%2529%2520mechanism%2520to%2520enhance%2520model%2520effectiveness.%2520Our%2520model%2520achieves%250Acompetitive%2520scores%2520on%2520public%2520multi-modal%2520benchmarks%2520and%2520performs%2520well%2520in%2520tasks%250Asuch%2520as%2520image%2520captioning%2520and%2520video%2520captioning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EVLM%3A%20An%20Efficient%20Vision-Language%20Model%20for%20Visual%20Understanding&entry.906535625=Kaibing%20Chen%20and%20Dong%20Shen%20and%20Hanwen%20Zhong%20and%20Huasong%20Zhong%20and%20Kui%20Xia%20and%20Di%20Xu%20and%20Wei%20Yuan%20and%20Yifei%20Hu%20and%20Bin%20Wen%20and%20Tianke%20Zhang%20and%20Changyi%20Liu%20and%20Dewen%20Fan%20and%20Huihui%20Xiao%20and%20Jiahong%20Wu%20and%20Fan%20Yang%20and%20Size%20Li%20and%20Di%20Zhang&entry.1292438233=%20%20In%20the%20field%20of%20multi-modal%20language%20models%2C%20the%20majority%20of%20methods%20are%0Abuilt%20on%20an%20architecture%20similar%20to%20LLaVA.%20These%20models%20use%20a%20single-layer%20ViT%0Afeature%20as%20a%20visual%20prompt%2C%20directly%20feeding%20it%20into%20the%20language%20models%0Aalongside%20textual%20tokens.%20However%2C%20when%20dealing%20with%20long%20sequences%20of%20visual%0Asignals%20or%20inputs%20such%20as%20videos%2C%20the%20self-attention%20mechanism%20of%20language%0Amodels%20can%20lead%20to%20significant%20computational%20overhead.%20Additionally%2C%20using%0Asingle-layer%20ViT%20features%20makes%20it%20challenging%20for%20large%20language%20models%20to%0Aperceive%20visual%20signals%20fully.%20This%20paper%20proposes%20an%20efficient%20multi-modal%0Alanguage%20model%20to%20minimize%20computational%20costs%20while%20enabling%20the%20model%20to%0Aperceive%20visual%20signals%20as%20comprehensively%20as%20possible.%20Our%20method%20primarily%0Aincludes%3A%20%281%29%20employing%20cross-attention%20to%20image-text%20interaction%20similar%20to%0AFlamingo.%20%282%29%20utilize%20hierarchical%20ViT%20features.%20%283%29%20introduce%20the%20Mixture%20of%0AExperts%20%28MoE%29%20mechanism%20to%20enhance%20model%20effectiveness.%20Our%20model%20achieves%0Acompetitive%20scores%20on%20public%20multi-modal%20benchmarks%20and%20performs%20well%20in%20tasks%0Asuch%20as%20image%20captioning%20and%20video%20captioning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14177v1&entry.124074799=Read"},
{"title": "Motion and Structure from Event-based Normal Flow", "author": "Zhongyang Ren and Bangyan Liao and Delei Kong and Jinghang Li and Peidong Liu and Laurent Kneip and Guillermo Gallego and Yi Zhou", "abstract": "  Recovering the camera motion and scene geometry from visual data is a\nfundamental problem in the field of computer vision. Its success in standard\nvision is attributed to the maturity of feature extraction, data association\nand multi-view geometry. The recent emergence of neuromorphic event-based\ncameras places great demands on approaches that use raw event data as input to\nsolve this fundamental problem.Existing state-of-the-art solutions typically\ninfer implicitly data association by iteratively reversing the event data\ngeneration process. However, the nonlinear nature of these methods limits their\napplicability in real-time tasks, and the constant-motion assumption leads to\nunstable results under agile motion. To this end, we rethink the problem\nformulation in a way that aligns better with the differential working principle\nof event cameras.We show that the event-based normal flow can be used, via the\nproposed geometric error term, as an alternative to the full flow in solving a\nfamily of geometric problems that involve instantaneous first-order kinematics\nand scene geometry. Furthermore, we develop a fast linear solver and a\ncontinuous-time nonlinear solver on top of the proposed geometric error\nterm.Experiments on both synthetic and real data show the superiority of our\nlinear solver in terms of accuracy and efficiency, and indicate its\ncomplementary feature as an initialization method for existing nonlinear\nsolvers. Besides, our continuous-time non-linear solver exhibits exceptional\ncapability in accommodating sudden variations in motion since it does not rely\non the constant-motion assumption.\n", "link": "http://arxiv.org/abs/2407.12239v2", "date": "2024-07-19", "relevancy": 2.2182, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5658}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5499}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Motion%20and%20Structure%20from%20Event-based%20Normal%20Flow&body=Title%3A%20Motion%20and%20Structure%20from%20Event-based%20Normal%20Flow%0AAuthor%3A%20Zhongyang%20Ren%20and%20Bangyan%20Liao%20and%20Delei%20Kong%20and%20Jinghang%20Li%20and%20Peidong%20Liu%20and%20Laurent%20Kneip%20and%20Guillermo%20Gallego%20and%20Yi%20Zhou%0AAbstract%3A%20%20%20Recovering%20the%20camera%20motion%20and%20scene%20geometry%20from%20visual%20data%20is%20a%0Afundamental%20problem%20in%20the%20field%20of%20computer%20vision.%20Its%20success%20in%20standard%0Avision%20is%20attributed%20to%20the%20maturity%20of%20feature%20extraction%2C%20data%20association%0Aand%20multi-view%20geometry.%20The%20recent%20emergence%20of%20neuromorphic%20event-based%0Acameras%20places%20great%20demands%20on%20approaches%20that%20use%20raw%20event%20data%20as%20input%20to%0Asolve%20this%20fundamental%20problem.Existing%20state-of-the-art%20solutions%20typically%0Ainfer%20implicitly%20data%20association%20by%20iteratively%20reversing%20the%20event%20data%0Ageneration%20process.%20However%2C%20the%20nonlinear%20nature%20of%20these%20methods%20limits%20their%0Aapplicability%20in%20real-time%20tasks%2C%20and%20the%20constant-motion%20assumption%20leads%20to%0Aunstable%20results%20under%20agile%20motion.%20To%20this%20end%2C%20we%20rethink%20the%20problem%0Aformulation%20in%20a%20way%20that%20aligns%20better%20with%20the%20differential%20working%20principle%0Aof%20event%20cameras.We%20show%20that%20the%20event-based%20normal%20flow%20can%20be%20used%2C%20via%20the%0Aproposed%20geometric%20error%20term%2C%20as%20an%20alternative%20to%20the%20full%20flow%20in%20solving%20a%0Afamily%20of%20geometric%20problems%20that%20involve%20instantaneous%20first-order%20kinematics%0Aand%20scene%20geometry.%20Furthermore%2C%20we%20develop%20a%20fast%20linear%20solver%20and%20a%0Acontinuous-time%20nonlinear%20solver%20on%20top%20of%20the%20proposed%20geometric%20error%0Aterm.Experiments%20on%20both%20synthetic%20and%20real%20data%20show%20the%20superiority%20of%20our%0Alinear%20solver%20in%20terms%20of%20accuracy%20and%20efficiency%2C%20and%20indicate%20its%0Acomplementary%20feature%20as%20an%20initialization%20method%20for%20existing%20nonlinear%0Asolvers.%20Besides%2C%20our%20continuous-time%20non-linear%20solver%20exhibits%20exceptional%0Acapability%20in%20accommodating%20sudden%20variations%20in%20motion%20since%20it%20does%20not%20rely%0Aon%20the%20constant-motion%20assumption.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12239v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotion%2520and%2520Structure%2520from%2520Event-based%2520Normal%2520Flow%26entry.906535625%3DZhongyang%2520Ren%2520and%2520Bangyan%2520Liao%2520and%2520Delei%2520Kong%2520and%2520Jinghang%2520Li%2520and%2520Peidong%2520Liu%2520and%2520Laurent%2520Kneip%2520and%2520Guillermo%2520Gallego%2520and%2520Yi%2520Zhou%26entry.1292438233%3D%2520%2520Recovering%2520the%2520camera%2520motion%2520and%2520scene%2520geometry%2520from%2520visual%2520data%2520is%2520a%250Afundamental%2520problem%2520in%2520the%2520field%2520of%2520computer%2520vision.%2520Its%2520success%2520in%2520standard%250Avision%2520is%2520attributed%2520to%2520the%2520maturity%2520of%2520feature%2520extraction%252C%2520data%2520association%250Aand%2520multi-view%2520geometry.%2520The%2520recent%2520emergence%2520of%2520neuromorphic%2520event-based%250Acameras%2520places%2520great%2520demands%2520on%2520approaches%2520that%2520use%2520raw%2520event%2520data%2520as%2520input%2520to%250Asolve%2520this%2520fundamental%2520problem.Existing%2520state-of-the-art%2520solutions%2520typically%250Ainfer%2520implicitly%2520data%2520association%2520by%2520iteratively%2520reversing%2520the%2520event%2520data%250Ageneration%2520process.%2520However%252C%2520the%2520nonlinear%2520nature%2520of%2520these%2520methods%2520limits%2520their%250Aapplicability%2520in%2520real-time%2520tasks%252C%2520and%2520the%2520constant-motion%2520assumption%2520leads%2520to%250Aunstable%2520results%2520under%2520agile%2520motion.%2520To%2520this%2520end%252C%2520we%2520rethink%2520the%2520problem%250Aformulation%2520in%2520a%2520way%2520that%2520aligns%2520better%2520with%2520the%2520differential%2520working%2520principle%250Aof%2520event%2520cameras.We%2520show%2520that%2520the%2520event-based%2520normal%2520flow%2520can%2520be%2520used%252C%2520via%2520the%250Aproposed%2520geometric%2520error%2520term%252C%2520as%2520an%2520alternative%2520to%2520the%2520full%2520flow%2520in%2520solving%2520a%250Afamily%2520of%2520geometric%2520problems%2520that%2520involve%2520instantaneous%2520first-order%2520kinematics%250Aand%2520scene%2520geometry.%2520Furthermore%252C%2520we%2520develop%2520a%2520fast%2520linear%2520solver%2520and%2520a%250Acontinuous-time%2520nonlinear%2520solver%2520on%2520top%2520of%2520the%2520proposed%2520geometric%2520error%250Aterm.Experiments%2520on%2520both%2520synthetic%2520and%2520real%2520data%2520show%2520the%2520superiority%2520of%2520our%250Alinear%2520solver%2520in%2520terms%2520of%2520accuracy%2520and%2520efficiency%252C%2520and%2520indicate%2520its%250Acomplementary%2520feature%2520as%2520an%2520initialization%2520method%2520for%2520existing%2520nonlinear%250Asolvers.%2520Besides%252C%2520our%2520continuous-time%2520non-linear%2520solver%2520exhibits%2520exceptional%250Acapability%2520in%2520accommodating%2520sudden%2520variations%2520in%2520motion%2520since%2520it%2520does%2520not%2520rely%250Aon%2520the%2520constant-motion%2520assumption.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12239v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Motion%20and%20Structure%20from%20Event-based%20Normal%20Flow&entry.906535625=Zhongyang%20Ren%20and%20Bangyan%20Liao%20and%20Delei%20Kong%20and%20Jinghang%20Li%20and%20Peidong%20Liu%20and%20Laurent%20Kneip%20and%20Guillermo%20Gallego%20and%20Yi%20Zhou&entry.1292438233=%20%20Recovering%20the%20camera%20motion%20and%20scene%20geometry%20from%20visual%20data%20is%20a%0Afundamental%20problem%20in%20the%20field%20of%20computer%20vision.%20Its%20success%20in%20standard%0Avision%20is%20attributed%20to%20the%20maturity%20of%20feature%20extraction%2C%20data%20association%0Aand%20multi-view%20geometry.%20The%20recent%20emergence%20of%20neuromorphic%20event-based%0Acameras%20places%20great%20demands%20on%20approaches%20that%20use%20raw%20event%20data%20as%20input%20to%0Asolve%20this%20fundamental%20problem.Existing%20state-of-the-art%20solutions%20typically%0Ainfer%20implicitly%20data%20association%20by%20iteratively%20reversing%20the%20event%20data%0Ageneration%20process.%20However%2C%20the%20nonlinear%20nature%20of%20these%20methods%20limits%20their%0Aapplicability%20in%20real-time%20tasks%2C%20and%20the%20constant-motion%20assumption%20leads%20to%0Aunstable%20results%20under%20agile%20motion.%20To%20this%20end%2C%20we%20rethink%20the%20problem%0Aformulation%20in%20a%20way%20that%20aligns%20better%20with%20the%20differential%20working%20principle%0Aof%20event%20cameras.We%20show%20that%20the%20event-based%20normal%20flow%20can%20be%20used%2C%20via%20the%0Aproposed%20geometric%20error%20term%2C%20as%20an%20alternative%20to%20the%20full%20flow%20in%20solving%20a%0Afamily%20of%20geometric%20problems%20that%20involve%20instantaneous%20first-order%20kinematics%0Aand%20scene%20geometry.%20Furthermore%2C%20we%20develop%20a%20fast%20linear%20solver%20and%20a%0Acontinuous-time%20nonlinear%20solver%20on%20top%20of%20the%20proposed%20geometric%20error%0Aterm.Experiments%20on%20both%20synthetic%20and%20real%20data%20show%20the%20superiority%20of%20our%0Alinear%20solver%20in%20terms%20of%20accuracy%20and%20efficiency%2C%20and%20indicate%20its%0Acomplementary%20feature%20as%20an%20initialization%20method%20for%20existing%20nonlinear%0Asolvers.%20Besides%2C%20our%20continuous-time%20non-linear%20solver%20exhibits%20exceptional%0Acapability%20in%20accommodating%20sudden%20variations%20in%20motion%20since%20it%20does%20not%20rely%0Aon%20the%20constant-motion%20assumption.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12239v2&entry.124074799=Read"},
{"title": "Decoupling Common and Unique Representations for Multimodal\n  Self-supervised Learning", "author": "Yi Wang and Conrad M Albrecht and Nassim Ait Ali Braham and Chenying Liu and Zhitong Xiong and Xiao Xiang Zhu", "abstract": "  The increasing availability of multi-sensor data sparks wide interest in\nmultimodal self-supervised learning. However, most existing approaches learn\nonly common representations across modalities while ignoring intra-modal\ntraining and modality-unique representations. We propose Decoupling Common and\nUnique Representations (DeCUR), a simple yet effective method for multimodal\nself-supervised learning. By distinguishing inter- and intra-modal embeddings\nthrough multimodal redundancy reduction, DeCUR can integrate complementary\ninformation across different modalities. We evaluate DeCUR in three common\nmultimodal scenarios (radar-optical, RGB-elevation, and RGB-depth), and\ndemonstrate its consistent improvement regardless of architectures and for both\nmultimodal and modality-missing settings. With thorough experiments and\ncomprehensive analysis, we hope this work can provide valuable insights and\nraise more interest in researching the hidden relationships of multimodal\nrepresentations.\n", "link": "http://arxiv.org/abs/2309.05300v3", "date": "2024-07-19", "relevancy": 2.2087, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5549}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5544}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoupling%20Common%20and%20Unique%20Representations%20for%20Multimodal%0A%20%20Self-supervised%20Learning&body=Title%3A%20Decoupling%20Common%20and%20Unique%20Representations%20for%20Multimodal%0A%20%20Self-supervised%20Learning%0AAuthor%3A%20Yi%20Wang%20and%20Conrad%20M%20Albrecht%20and%20Nassim%20Ait%20Ali%20Braham%20and%20Chenying%20Liu%20and%20Zhitong%20Xiong%20and%20Xiao%20Xiang%20Zhu%0AAbstract%3A%20%20%20The%20increasing%20availability%20of%20multi-sensor%20data%20sparks%20wide%20interest%20in%0Amultimodal%20self-supervised%20learning.%20However%2C%20most%20existing%20approaches%20learn%0Aonly%20common%20representations%20across%20modalities%20while%20ignoring%20intra-modal%0Atraining%20and%20modality-unique%20representations.%20We%20propose%20Decoupling%20Common%20and%0AUnique%20Representations%20%28DeCUR%29%2C%20a%20simple%20yet%20effective%20method%20for%20multimodal%0Aself-supervised%20learning.%20By%20distinguishing%20inter-%20and%20intra-modal%20embeddings%0Athrough%20multimodal%20redundancy%20reduction%2C%20DeCUR%20can%20integrate%20complementary%0Ainformation%20across%20different%20modalities.%20We%20evaluate%20DeCUR%20in%20three%20common%0Amultimodal%20scenarios%20%28radar-optical%2C%20RGB-elevation%2C%20and%20RGB-depth%29%2C%20and%0Ademonstrate%20its%20consistent%20improvement%20regardless%20of%20architectures%20and%20for%20both%0Amultimodal%20and%20modality-missing%20settings.%20With%20thorough%20experiments%20and%0Acomprehensive%20analysis%2C%20we%20hope%20this%20work%20can%20provide%20valuable%20insights%20and%0Araise%20more%20interest%20in%20researching%20the%20hidden%20relationships%20of%20multimodal%0Arepresentations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.05300v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoupling%2520Common%2520and%2520Unique%2520Representations%2520for%2520Multimodal%250A%2520%2520Self-supervised%2520Learning%26entry.906535625%3DYi%2520Wang%2520and%2520Conrad%2520M%2520Albrecht%2520and%2520Nassim%2520Ait%2520Ali%2520Braham%2520and%2520Chenying%2520Liu%2520and%2520Zhitong%2520Xiong%2520and%2520Xiao%2520Xiang%2520Zhu%26entry.1292438233%3D%2520%2520The%2520increasing%2520availability%2520of%2520multi-sensor%2520data%2520sparks%2520wide%2520interest%2520in%250Amultimodal%2520self-supervised%2520learning.%2520However%252C%2520most%2520existing%2520approaches%2520learn%250Aonly%2520common%2520representations%2520across%2520modalities%2520while%2520ignoring%2520intra-modal%250Atraining%2520and%2520modality-unique%2520representations.%2520We%2520propose%2520Decoupling%2520Common%2520and%250AUnique%2520Representations%2520%2528DeCUR%2529%252C%2520a%2520simple%2520yet%2520effective%2520method%2520for%2520multimodal%250Aself-supervised%2520learning.%2520By%2520distinguishing%2520inter-%2520and%2520intra-modal%2520embeddings%250Athrough%2520multimodal%2520redundancy%2520reduction%252C%2520DeCUR%2520can%2520integrate%2520complementary%250Ainformation%2520across%2520different%2520modalities.%2520We%2520evaluate%2520DeCUR%2520in%2520three%2520common%250Amultimodal%2520scenarios%2520%2528radar-optical%252C%2520RGB-elevation%252C%2520and%2520RGB-depth%2529%252C%2520and%250Ademonstrate%2520its%2520consistent%2520improvement%2520regardless%2520of%2520architectures%2520and%2520for%2520both%250Amultimodal%2520and%2520modality-missing%2520settings.%2520With%2520thorough%2520experiments%2520and%250Acomprehensive%2520analysis%252C%2520we%2520hope%2520this%2520work%2520can%2520provide%2520valuable%2520insights%2520and%250Araise%2520more%2520interest%2520in%2520researching%2520the%2520hidden%2520relationships%2520of%2520multimodal%250Arepresentations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.05300v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoupling%20Common%20and%20Unique%20Representations%20for%20Multimodal%0A%20%20Self-supervised%20Learning&entry.906535625=Yi%20Wang%20and%20Conrad%20M%20Albrecht%20and%20Nassim%20Ait%20Ali%20Braham%20and%20Chenying%20Liu%20and%20Zhitong%20Xiong%20and%20Xiao%20Xiang%20Zhu&entry.1292438233=%20%20The%20increasing%20availability%20of%20multi-sensor%20data%20sparks%20wide%20interest%20in%0Amultimodal%20self-supervised%20learning.%20However%2C%20most%20existing%20approaches%20learn%0Aonly%20common%20representations%20across%20modalities%20while%20ignoring%20intra-modal%0Atraining%20and%20modality-unique%20representations.%20We%20propose%20Decoupling%20Common%20and%0AUnique%20Representations%20%28DeCUR%29%2C%20a%20simple%20yet%20effective%20method%20for%20multimodal%0Aself-supervised%20learning.%20By%20distinguishing%20inter-%20and%20intra-modal%20embeddings%0Athrough%20multimodal%20redundancy%20reduction%2C%20DeCUR%20can%20integrate%20complementary%0Ainformation%20across%20different%20modalities.%20We%20evaluate%20DeCUR%20in%20three%20common%0Amultimodal%20scenarios%20%28radar-optical%2C%20RGB-elevation%2C%20and%20RGB-depth%29%2C%20and%0Ademonstrate%20its%20consistent%20improvement%20regardless%20of%20architectures%20and%20for%20both%0Amultimodal%20and%20modality-missing%20settings.%20With%20thorough%20experiments%20and%0Acomprehensive%20analysis%2C%20we%20hope%20this%20work%20can%20provide%20valuable%20insights%20and%0Araise%20more%20interest%20in%20researching%20the%20hidden%20relationships%20of%20multimodal%0Arepresentations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.05300v3&entry.124074799=Read"},
{"title": "Context Matters: Leveraging Spatiotemporal Metadata for Semi-Supervised\n  Learning on Remote Sensing Images", "author": "Maximilian Bernhard and Tanveer Hannan and Niklas Strau\u00df and Matthias Schubert", "abstract": "  Remote sensing projects typically generate large amounts of imagery that can\nbe used to train powerful deep neural networks. However, the amount of labeled\nimages is often small, as remote sensing applications generally require expert\nlabelers. Thus, semi-supervised learning (SSL), i.e., learning with a small\npool of labeled and a larger pool of unlabeled data, is particularly useful in\nthis domain. Current SSL approaches generate pseudo-labels from model\npredictions for unlabeled samples. As the quality of these pseudo-labels is\ncrucial for performance, utilizing additional information to improve\npseudo-label quality yields a promising direction. For remote sensing images,\ngeolocation and recording time are generally available and provide a valuable\nsource of information as semantic concepts, such as land cover, are highly\ndependent on spatiotemporal context, e.g., due to seasonal effects and\nvegetation zones. In this paper, we propose to exploit spatiotemporal\nmetainformation in SSL to improve the quality of pseudo-labels and, therefore,\nthe final model performance. We show that directly adding the available\nmetadata to the input of the predictor at test time degenerates the prediction\nquality for metadata outside the spatiotemporal distribution of the training\nset. Thus, we propose a teacher-student SSL framework where only the teacher\nnetwork uses metainformation to improve the quality of pseudo-labels on the\ntraining set. Correspondingly, our student network benefits from the improved\npseudo-labels but does not receive metadata as input, making it invariant to\nspatiotemporal shifts at test time. Furthermore, we propose methods for\nencoding and injecting spatiotemporal information into the model and introduce\na novel distillation mechanism to enhance the knowledge transfer between\nteacher and student. Our framework dubbed Spatiotemporal SSL can be easily\ncombined with several stat...\n", "link": "http://arxiv.org/abs/2404.18583v2", "date": "2024-07-19", "relevancy": 2.2051, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5804}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5321}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context%20Matters%3A%20Leveraging%20Spatiotemporal%20Metadata%20for%20Semi-Supervised%0A%20%20Learning%20on%20Remote%20Sensing%20Images&body=Title%3A%20Context%20Matters%3A%20Leveraging%20Spatiotemporal%20Metadata%20for%20Semi-Supervised%0A%20%20Learning%20on%20Remote%20Sensing%20Images%0AAuthor%3A%20Maximilian%20Bernhard%20and%20Tanveer%20Hannan%20and%20Niklas%20Strau%C3%9F%20and%20Matthias%20Schubert%0AAbstract%3A%20%20%20Remote%20sensing%20projects%20typically%20generate%20large%20amounts%20of%20imagery%20that%20can%0Abe%20used%20to%20train%20powerful%20deep%20neural%20networks.%20However%2C%20the%20amount%20of%20labeled%0Aimages%20is%20often%20small%2C%20as%20remote%20sensing%20applications%20generally%20require%20expert%0Alabelers.%20Thus%2C%20semi-supervised%20learning%20%28SSL%29%2C%20i.e.%2C%20learning%20with%20a%20small%0Apool%20of%20labeled%20and%20a%20larger%20pool%20of%20unlabeled%20data%2C%20is%20particularly%20useful%20in%0Athis%20domain.%20Current%20SSL%20approaches%20generate%20pseudo-labels%20from%20model%0Apredictions%20for%20unlabeled%20samples.%20As%20the%20quality%20of%20these%20pseudo-labels%20is%0Acrucial%20for%20performance%2C%20utilizing%20additional%20information%20to%20improve%0Apseudo-label%20quality%20yields%20a%20promising%20direction.%20For%20remote%20sensing%20images%2C%0Ageolocation%20and%20recording%20time%20are%20generally%20available%20and%20provide%20a%20valuable%0Asource%20of%20information%20as%20semantic%20concepts%2C%20such%20as%20land%20cover%2C%20are%20highly%0Adependent%20on%20spatiotemporal%20context%2C%20e.g.%2C%20due%20to%20seasonal%20effects%20and%0Avegetation%20zones.%20In%20this%20paper%2C%20we%20propose%20to%20exploit%20spatiotemporal%0Ametainformation%20in%20SSL%20to%20improve%20the%20quality%20of%20pseudo-labels%20and%2C%20therefore%2C%0Athe%20final%20model%20performance.%20We%20show%20that%20directly%20adding%20the%20available%0Ametadata%20to%20the%20input%20of%20the%20predictor%20at%20test%20time%20degenerates%20the%20prediction%0Aquality%20for%20metadata%20outside%20the%20spatiotemporal%20distribution%20of%20the%20training%0Aset.%20Thus%2C%20we%20propose%20a%20teacher-student%20SSL%20framework%20where%20only%20the%20teacher%0Anetwork%20uses%20metainformation%20to%20improve%20the%20quality%20of%20pseudo-labels%20on%20the%0Atraining%20set.%20Correspondingly%2C%20our%20student%20network%20benefits%20from%20the%20improved%0Apseudo-labels%20but%20does%20not%20receive%20metadata%20as%20input%2C%20making%20it%20invariant%20to%0Aspatiotemporal%20shifts%20at%20test%20time.%20Furthermore%2C%20we%20propose%20methods%20for%0Aencoding%20and%20injecting%20spatiotemporal%20information%20into%20the%20model%20and%20introduce%0Aa%20novel%20distillation%20mechanism%20to%20enhance%20the%20knowledge%20transfer%20between%0Ateacher%20and%20student.%20Our%20framework%20dubbed%20Spatiotemporal%20SSL%20can%20be%20easily%0Acombined%20with%20several%20stat...%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18583v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext%2520Matters%253A%2520Leveraging%2520Spatiotemporal%2520Metadata%2520for%2520Semi-Supervised%250A%2520%2520Learning%2520on%2520Remote%2520Sensing%2520Images%26entry.906535625%3DMaximilian%2520Bernhard%2520and%2520Tanveer%2520Hannan%2520and%2520Niklas%2520Strau%25C3%259F%2520and%2520Matthias%2520Schubert%26entry.1292438233%3D%2520%2520Remote%2520sensing%2520projects%2520typically%2520generate%2520large%2520amounts%2520of%2520imagery%2520that%2520can%250Abe%2520used%2520to%2520train%2520powerful%2520deep%2520neural%2520networks.%2520However%252C%2520the%2520amount%2520of%2520labeled%250Aimages%2520is%2520often%2520small%252C%2520as%2520remote%2520sensing%2520applications%2520generally%2520require%2520expert%250Alabelers.%2520Thus%252C%2520semi-supervised%2520learning%2520%2528SSL%2529%252C%2520i.e.%252C%2520learning%2520with%2520a%2520small%250Apool%2520of%2520labeled%2520and%2520a%2520larger%2520pool%2520of%2520unlabeled%2520data%252C%2520is%2520particularly%2520useful%2520in%250Athis%2520domain.%2520Current%2520SSL%2520approaches%2520generate%2520pseudo-labels%2520from%2520model%250Apredictions%2520for%2520unlabeled%2520samples.%2520As%2520the%2520quality%2520of%2520these%2520pseudo-labels%2520is%250Acrucial%2520for%2520performance%252C%2520utilizing%2520additional%2520information%2520to%2520improve%250Apseudo-label%2520quality%2520yields%2520a%2520promising%2520direction.%2520For%2520remote%2520sensing%2520images%252C%250Ageolocation%2520and%2520recording%2520time%2520are%2520generally%2520available%2520and%2520provide%2520a%2520valuable%250Asource%2520of%2520information%2520as%2520semantic%2520concepts%252C%2520such%2520as%2520land%2520cover%252C%2520are%2520highly%250Adependent%2520on%2520spatiotemporal%2520context%252C%2520e.g.%252C%2520due%2520to%2520seasonal%2520effects%2520and%250Avegetation%2520zones.%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%2520exploit%2520spatiotemporal%250Ametainformation%2520in%2520SSL%2520to%2520improve%2520the%2520quality%2520of%2520pseudo-labels%2520and%252C%2520therefore%252C%250Athe%2520final%2520model%2520performance.%2520We%2520show%2520that%2520directly%2520adding%2520the%2520available%250Ametadata%2520to%2520the%2520input%2520of%2520the%2520predictor%2520at%2520test%2520time%2520degenerates%2520the%2520prediction%250Aquality%2520for%2520metadata%2520outside%2520the%2520spatiotemporal%2520distribution%2520of%2520the%2520training%250Aset.%2520Thus%252C%2520we%2520propose%2520a%2520teacher-student%2520SSL%2520framework%2520where%2520only%2520the%2520teacher%250Anetwork%2520uses%2520metainformation%2520to%2520improve%2520the%2520quality%2520of%2520pseudo-labels%2520on%2520the%250Atraining%2520set.%2520Correspondingly%252C%2520our%2520student%2520network%2520benefits%2520from%2520the%2520improved%250Apseudo-labels%2520but%2520does%2520not%2520receive%2520metadata%2520as%2520input%252C%2520making%2520it%2520invariant%2520to%250Aspatiotemporal%2520shifts%2520at%2520test%2520time.%2520Furthermore%252C%2520we%2520propose%2520methods%2520for%250Aencoding%2520and%2520injecting%2520spatiotemporal%2520information%2520into%2520the%2520model%2520and%2520introduce%250Aa%2520novel%2520distillation%2520mechanism%2520to%2520enhance%2520the%2520knowledge%2520transfer%2520between%250Ateacher%2520and%2520student.%2520Our%2520framework%2520dubbed%2520Spatiotemporal%2520SSL%2520can%2520be%2520easily%250Acombined%2520with%2520several%2520stat...%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18583v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context%20Matters%3A%20Leveraging%20Spatiotemporal%20Metadata%20for%20Semi-Supervised%0A%20%20Learning%20on%20Remote%20Sensing%20Images&entry.906535625=Maximilian%20Bernhard%20and%20Tanveer%20Hannan%20and%20Niklas%20Strau%C3%9F%20and%20Matthias%20Schubert&entry.1292438233=%20%20Remote%20sensing%20projects%20typically%20generate%20large%20amounts%20of%20imagery%20that%20can%0Abe%20used%20to%20train%20powerful%20deep%20neural%20networks.%20However%2C%20the%20amount%20of%20labeled%0Aimages%20is%20often%20small%2C%20as%20remote%20sensing%20applications%20generally%20require%20expert%0Alabelers.%20Thus%2C%20semi-supervised%20learning%20%28SSL%29%2C%20i.e.%2C%20learning%20with%20a%20small%0Apool%20of%20labeled%20and%20a%20larger%20pool%20of%20unlabeled%20data%2C%20is%20particularly%20useful%20in%0Athis%20domain.%20Current%20SSL%20approaches%20generate%20pseudo-labels%20from%20model%0Apredictions%20for%20unlabeled%20samples.%20As%20the%20quality%20of%20these%20pseudo-labels%20is%0Acrucial%20for%20performance%2C%20utilizing%20additional%20information%20to%20improve%0Apseudo-label%20quality%20yields%20a%20promising%20direction.%20For%20remote%20sensing%20images%2C%0Ageolocation%20and%20recording%20time%20are%20generally%20available%20and%20provide%20a%20valuable%0Asource%20of%20information%20as%20semantic%20concepts%2C%20such%20as%20land%20cover%2C%20are%20highly%0Adependent%20on%20spatiotemporal%20context%2C%20e.g.%2C%20due%20to%20seasonal%20effects%20and%0Avegetation%20zones.%20In%20this%20paper%2C%20we%20propose%20to%20exploit%20spatiotemporal%0Ametainformation%20in%20SSL%20to%20improve%20the%20quality%20of%20pseudo-labels%20and%2C%20therefore%2C%0Athe%20final%20model%20performance.%20We%20show%20that%20directly%20adding%20the%20available%0Ametadata%20to%20the%20input%20of%20the%20predictor%20at%20test%20time%20degenerates%20the%20prediction%0Aquality%20for%20metadata%20outside%20the%20spatiotemporal%20distribution%20of%20the%20training%0Aset.%20Thus%2C%20we%20propose%20a%20teacher-student%20SSL%20framework%20where%20only%20the%20teacher%0Anetwork%20uses%20metainformation%20to%20improve%20the%20quality%20of%20pseudo-labels%20on%20the%0Atraining%20set.%20Correspondingly%2C%20our%20student%20network%20benefits%20from%20the%20improved%0Apseudo-labels%20but%20does%20not%20receive%20metadata%20as%20input%2C%20making%20it%20invariant%20to%0Aspatiotemporal%20shifts%20at%20test%20time.%20Furthermore%2C%20we%20propose%20methods%20for%0Aencoding%20and%20injecting%20spatiotemporal%20information%20into%20the%20model%20and%20introduce%0Aa%20novel%20distillation%20mechanism%20to%20enhance%20the%20knowledge%20transfer%20between%0Ateacher%20and%20student.%20Our%20framework%20dubbed%20Spatiotemporal%20SSL%20can%20be%20easily%0Acombined%20with%20several%20stat...%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18583v2&entry.124074799=Read"},
{"title": "Towards Scene Graph Anticipation", "author": "Rohith Peddi and Saksham Singh and  Saurabh and Parag Singla and Vibhav Gogate", "abstract": "  Spatio-temporal scene graphs represent interactions in a video by decomposing\nscenes into individual objects and their pair-wise temporal relationships.\nLong-term anticipation of the fine-grained pair-wise relationships between\nobjects is a challenging problem. To this end, we introduce the task of Scene\nGraph Anticipation (SGA). We adapt state-of-the-art scene graph generation\nmethods as baselines to anticipate future pair-wise relationships between\nobjects and propose a novel approach SceneSayer. In SceneSayer, we leverage\nobject-centric representations of relationships to reason about the observed\nvideo frames and model the evolution of relationships between objects. We take\na continuous time perspective and model the latent dynamics of the evolution of\nobject interactions using concepts of NeuralODE and NeuralSDE, respectively. We\ninfer representations of future relationships by solving an Ordinary\nDifferential Equation and a Stochastic Differential Equation, respectively.\nExtensive experimentation on the Action Genome dataset validates the efficacy\nof the proposed methods.\n", "link": "http://arxiv.org/abs/2403.04899v2", "date": "2024-07-19", "relevancy": 2.1997, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6187}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5434}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Scene%20Graph%20Anticipation&body=Title%3A%20Towards%20Scene%20Graph%20Anticipation%0AAuthor%3A%20Rohith%20Peddi%20and%20Saksham%20Singh%20and%20%20Saurabh%20and%20Parag%20Singla%20and%20Vibhav%20Gogate%0AAbstract%3A%20%20%20Spatio-temporal%20scene%20graphs%20represent%20interactions%20in%20a%20video%20by%20decomposing%0Ascenes%20into%20individual%20objects%20and%20their%20pair-wise%20temporal%20relationships.%0ALong-term%20anticipation%20of%20the%20fine-grained%20pair-wise%20relationships%20between%0Aobjects%20is%20a%20challenging%20problem.%20To%20this%20end%2C%20we%20introduce%20the%20task%20of%20Scene%0AGraph%20Anticipation%20%28SGA%29.%20We%20adapt%20state-of-the-art%20scene%20graph%20generation%0Amethods%20as%20baselines%20to%20anticipate%20future%20pair-wise%20relationships%20between%0Aobjects%20and%20propose%20a%20novel%20approach%20SceneSayer.%20In%20SceneSayer%2C%20we%20leverage%0Aobject-centric%20representations%20of%20relationships%20to%20reason%20about%20the%20observed%0Avideo%20frames%20and%20model%20the%20evolution%20of%20relationships%20between%20objects.%20We%20take%0Aa%20continuous%20time%20perspective%20and%20model%20the%20latent%20dynamics%20of%20the%20evolution%20of%0Aobject%20interactions%20using%20concepts%20of%20NeuralODE%20and%20NeuralSDE%2C%20respectively.%20We%0Ainfer%20representations%20of%20future%20relationships%20by%20solving%20an%20Ordinary%0ADifferential%20Equation%20and%20a%20Stochastic%20Differential%20Equation%2C%20respectively.%0AExtensive%20experimentation%20on%20the%20Action%20Genome%20dataset%20validates%20the%20efficacy%0Aof%20the%20proposed%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04899v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Scene%2520Graph%2520Anticipation%26entry.906535625%3DRohith%2520Peddi%2520and%2520Saksham%2520Singh%2520and%2520%2520Saurabh%2520and%2520Parag%2520Singla%2520and%2520Vibhav%2520Gogate%26entry.1292438233%3D%2520%2520Spatio-temporal%2520scene%2520graphs%2520represent%2520interactions%2520in%2520a%2520video%2520by%2520decomposing%250Ascenes%2520into%2520individual%2520objects%2520and%2520their%2520pair-wise%2520temporal%2520relationships.%250ALong-term%2520anticipation%2520of%2520the%2520fine-grained%2520pair-wise%2520relationships%2520between%250Aobjects%2520is%2520a%2520challenging%2520problem.%2520To%2520this%2520end%252C%2520we%2520introduce%2520the%2520task%2520of%2520Scene%250AGraph%2520Anticipation%2520%2528SGA%2529.%2520We%2520adapt%2520state-of-the-art%2520scene%2520graph%2520generation%250Amethods%2520as%2520baselines%2520to%2520anticipate%2520future%2520pair-wise%2520relationships%2520between%250Aobjects%2520and%2520propose%2520a%2520novel%2520approach%2520SceneSayer.%2520In%2520SceneSayer%252C%2520we%2520leverage%250Aobject-centric%2520representations%2520of%2520relationships%2520to%2520reason%2520about%2520the%2520observed%250Avideo%2520frames%2520and%2520model%2520the%2520evolution%2520of%2520relationships%2520between%2520objects.%2520We%2520take%250Aa%2520continuous%2520time%2520perspective%2520and%2520model%2520the%2520latent%2520dynamics%2520of%2520the%2520evolution%2520of%250Aobject%2520interactions%2520using%2520concepts%2520of%2520NeuralODE%2520and%2520NeuralSDE%252C%2520respectively.%2520We%250Ainfer%2520representations%2520of%2520future%2520relationships%2520by%2520solving%2520an%2520Ordinary%250ADifferential%2520Equation%2520and%2520a%2520Stochastic%2520Differential%2520Equation%252C%2520respectively.%250AExtensive%2520experimentation%2520on%2520the%2520Action%2520Genome%2520dataset%2520validates%2520the%2520efficacy%250Aof%2520the%2520proposed%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04899v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Scene%20Graph%20Anticipation&entry.906535625=Rohith%20Peddi%20and%20Saksham%20Singh%20and%20%20Saurabh%20and%20Parag%20Singla%20and%20Vibhav%20Gogate&entry.1292438233=%20%20Spatio-temporal%20scene%20graphs%20represent%20interactions%20in%20a%20video%20by%20decomposing%0Ascenes%20into%20individual%20objects%20and%20their%20pair-wise%20temporal%20relationships.%0ALong-term%20anticipation%20of%20the%20fine-grained%20pair-wise%20relationships%20between%0Aobjects%20is%20a%20challenging%20problem.%20To%20this%20end%2C%20we%20introduce%20the%20task%20of%20Scene%0AGraph%20Anticipation%20%28SGA%29.%20We%20adapt%20state-of-the-art%20scene%20graph%20generation%0Amethods%20as%20baselines%20to%20anticipate%20future%20pair-wise%20relationships%20between%0Aobjects%20and%20propose%20a%20novel%20approach%20SceneSayer.%20In%20SceneSayer%2C%20we%20leverage%0Aobject-centric%20representations%20of%20relationships%20to%20reason%20about%20the%20observed%0Avideo%20frames%20and%20model%20the%20evolution%20of%20relationships%20between%20objects.%20We%20take%0Aa%20continuous%20time%20perspective%20and%20model%20the%20latent%20dynamics%20of%20the%20evolution%20of%0Aobject%20interactions%20using%20concepts%20of%20NeuralODE%20and%20NeuralSDE%2C%20respectively.%20We%0Ainfer%20representations%20of%20future%20relationships%20by%20solving%20an%20Ordinary%0ADifferential%20Equation%20and%20a%20Stochastic%20Differential%20Equation%2C%20respectively.%0AExtensive%20experimentation%20on%20the%20Action%20Genome%20dataset%20validates%20the%20efficacy%0Aof%20the%20proposed%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04899v2&entry.124074799=Read"},
{"title": "Vista: A Generalizable Driving World Model with High Fidelity and\n  Versatile Controllability", "author": "Shenyuan Gao and Jiazhi Yang and Li Chen and Kashyap Chitta and Yihang Qiu and Andreas Geiger and Jun Zhang and Hongyang Li", "abstract": "  World models can foresee the outcomes of different actions, which is of\nparamount importance for autonomous driving. Nevertheless, existing driving\nworld models still have limitations in generalization to unseen environments,\nprediction fidelity of critical details, and action controllability for\nflexible application. In this paper, we present Vista, a generalizable driving\nworld model with high fidelity and versatile controllability. Based on a\nsystematic diagnosis of existing methods, we introduce several key ingredients\nto address these limitations. To accurately predict real-world dynamics at high\nresolution, we propose two novel losses to promote the learning of moving\ninstances and structural information. We also devise an effective latent\nreplacement approach to inject historical frames as priors for coherent\nlong-horizon rollouts. For action controllability, we incorporate a versatile\nset of controls from high-level intentions (command, goal point) to low-level\nmaneuvers (trajectory, angle, and speed) through an efficient learning\nstrategy. After large-scale training, the capabilities of Vista can seamlessly\ngeneralize to different scenarios. Extensive experiments on multiple datasets\nshow that Vista outperforms the most advanced general-purpose video generator\nin over 70% of comparisons and surpasses the best-performing driving world\nmodel by 55% in FID and 27% in FVD. Moreover, for the first time, we utilize\nthe capacity of Vista itself to establish a generalizable reward for real-world\naction evaluation without accessing the ground truth actions.\n", "link": "http://arxiv.org/abs/2405.17398v3", "date": "2024-07-19", "relevancy": 2.1899, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5508}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5477}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vista%3A%20A%20Generalizable%20Driving%20World%20Model%20with%20High%20Fidelity%20and%0A%20%20Versatile%20Controllability&body=Title%3A%20Vista%3A%20A%20Generalizable%20Driving%20World%20Model%20with%20High%20Fidelity%20and%0A%20%20Versatile%20Controllability%0AAuthor%3A%20Shenyuan%20Gao%20and%20Jiazhi%20Yang%20and%20Li%20Chen%20and%20Kashyap%20Chitta%20and%20Yihang%20Qiu%20and%20Andreas%20Geiger%20and%20Jun%20Zhang%20and%20Hongyang%20Li%0AAbstract%3A%20%20%20World%20models%20can%20foresee%20the%20outcomes%20of%20different%20actions%2C%20which%20is%20of%0Aparamount%20importance%20for%20autonomous%20driving.%20Nevertheless%2C%20existing%20driving%0Aworld%20models%20still%20have%20limitations%20in%20generalization%20to%20unseen%20environments%2C%0Aprediction%20fidelity%20of%20critical%20details%2C%20and%20action%20controllability%20for%0Aflexible%20application.%20In%20this%20paper%2C%20we%20present%20Vista%2C%20a%20generalizable%20driving%0Aworld%20model%20with%20high%20fidelity%20and%20versatile%20controllability.%20Based%20on%20a%0Asystematic%20diagnosis%20of%20existing%20methods%2C%20we%20introduce%20several%20key%20ingredients%0Ato%20address%20these%20limitations.%20To%20accurately%20predict%20real-world%20dynamics%20at%20high%0Aresolution%2C%20we%20propose%20two%20novel%20losses%20to%20promote%20the%20learning%20of%20moving%0Ainstances%20and%20structural%20information.%20We%20also%20devise%20an%20effective%20latent%0Areplacement%20approach%20to%20inject%20historical%20frames%20as%20priors%20for%20coherent%0Along-horizon%20rollouts.%20For%20action%20controllability%2C%20we%20incorporate%20a%20versatile%0Aset%20of%20controls%20from%20high-level%20intentions%20%28command%2C%20goal%20point%29%20to%20low-level%0Amaneuvers%20%28trajectory%2C%20angle%2C%20and%20speed%29%20through%20an%20efficient%20learning%0Astrategy.%20After%20large-scale%20training%2C%20the%20capabilities%20of%20Vista%20can%20seamlessly%0Ageneralize%20to%20different%20scenarios.%20Extensive%20experiments%20on%20multiple%20datasets%0Ashow%20that%20Vista%20outperforms%20the%20most%20advanced%20general-purpose%20video%20generator%0Ain%20over%2070%25%20of%20comparisons%20and%20surpasses%20the%20best-performing%20driving%20world%0Amodel%20by%2055%25%20in%20FID%20and%2027%25%20in%20FVD.%20Moreover%2C%20for%20the%20first%20time%2C%20we%20utilize%0Athe%20capacity%20of%20Vista%20itself%20to%20establish%20a%20generalizable%20reward%20for%20real-world%0Aaction%20evaluation%20without%20accessing%20the%20ground%20truth%20actions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17398v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVista%253A%2520A%2520Generalizable%2520Driving%2520World%2520Model%2520with%2520High%2520Fidelity%2520and%250A%2520%2520Versatile%2520Controllability%26entry.906535625%3DShenyuan%2520Gao%2520and%2520Jiazhi%2520Yang%2520and%2520Li%2520Chen%2520and%2520Kashyap%2520Chitta%2520and%2520Yihang%2520Qiu%2520and%2520Andreas%2520Geiger%2520and%2520Jun%2520Zhang%2520and%2520Hongyang%2520Li%26entry.1292438233%3D%2520%2520World%2520models%2520can%2520foresee%2520the%2520outcomes%2520of%2520different%2520actions%252C%2520which%2520is%2520of%250Aparamount%2520importance%2520for%2520autonomous%2520driving.%2520Nevertheless%252C%2520existing%2520driving%250Aworld%2520models%2520still%2520have%2520limitations%2520in%2520generalization%2520to%2520unseen%2520environments%252C%250Aprediction%2520fidelity%2520of%2520critical%2520details%252C%2520and%2520action%2520controllability%2520for%250Aflexible%2520application.%2520In%2520this%2520paper%252C%2520we%2520present%2520Vista%252C%2520a%2520generalizable%2520driving%250Aworld%2520model%2520with%2520high%2520fidelity%2520and%2520versatile%2520controllability.%2520Based%2520on%2520a%250Asystematic%2520diagnosis%2520of%2520existing%2520methods%252C%2520we%2520introduce%2520several%2520key%2520ingredients%250Ato%2520address%2520these%2520limitations.%2520To%2520accurately%2520predict%2520real-world%2520dynamics%2520at%2520high%250Aresolution%252C%2520we%2520propose%2520two%2520novel%2520losses%2520to%2520promote%2520the%2520learning%2520of%2520moving%250Ainstances%2520and%2520structural%2520information.%2520We%2520also%2520devise%2520an%2520effective%2520latent%250Areplacement%2520approach%2520to%2520inject%2520historical%2520frames%2520as%2520priors%2520for%2520coherent%250Along-horizon%2520rollouts.%2520For%2520action%2520controllability%252C%2520we%2520incorporate%2520a%2520versatile%250Aset%2520of%2520controls%2520from%2520high-level%2520intentions%2520%2528command%252C%2520goal%2520point%2529%2520to%2520low-level%250Amaneuvers%2520%2528trajectory%252C%2520angle%252C%2520and%2520speed%2529%2520through%2520an%2520efficient%2520learning%250Astrategy.%2520After%2520large-scale%2520training%252C%2520the%2520capabilities%2520of%2520Vista%2520can%2520seamlessly%250Ageneralize%2520to%2520different%2520scenarios.%2520Extensive%2520experiments%2520on%2520multiple%2520datasets%250Ashow%2520that%2520Vista%2520outperforms%2520the%2520most%2520advanced%2520general-purpose%2520video%2520generator%250Ain%2520over%252070%2525%2520of%2520comparisons%2520and%2520surpasses%2520the%2520best-performing%2520driving%2520world%250Amodel%2520by%252055%2525%2520in%2520FID%2520and%252027%2525%2520in%2520FVD.%2520Moreover%252C%2520for%2520the%2520first%2520time%252C%2520we%2520utilize%250Athe%2520capacity%2520of%2520Vista%2520itself%2520to%2520establish%2520a%2520generalizable%2520reward%2520for%2520real-world%250Aaction%2520evaluation%2520without%2520accessing%2520the%2520ground%2520truth%2520actions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17398v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vista%3A%20A%20Generalizable%20Driving%20World%20Model%20with%20High%20Fidelity%20and%0A%20%20Versatile%20Controllability&entry.906535625=Shenyuan%20Gao%20and%20Jiazhi%20Yang%20and%20Li%20Chen%20and%20Kashyap%20Chitta%20and%20Yihang%20Qiu%20and%20Andreas%20Geiger%20and%20Jun%20Zhang%20and%20Hongyang%20Li&entry.1292438233=%20%20World%20models%20can%20foresee%20the%20outcomes%20of%20different%20actions%2C%20which%20is%20of%0Aparamount%20importance%20for%20autonomous%20driving.%20Nevertheless%2C%20existing%20driving%0Aworld%20models%20still%20have%20limitations%20in%20generalization%20to%20unseen%20environments%2C%0Aprediction%20fidelity%20of%20critical%20details%2C%20and%20action%20controllability%20for%0Aflexible%20application.%20In%20this%20paper%2C%20we%20present%20Vista%2C%20a%20generalizable%20driving%0Aworld%20model%20with%20high%20fidelity%20and%20versatile%20controllability.%20Based%20on%20a%0Asystematic%20diagnosis%20of%20existing%20methods%2C%20we%20introduce%20several%20key%20ingredients%0Ato%20address%20these%20limitations.%20To%20accurately%20predict%20real-world%20dynamics%20at%20high%0Aresolution%2C%20we%20propose%20two%20novel%20losses%20to%20promote%20the%20learning%20of%20moving%0Ainstances%20and%20structural%20information.%20We%20also%20devise%20an%20effective%20latent%0Areplacement%20approach%20to%20inject%20historical%20frames%20as%20priors%20for%20coherent%0Along-horizon%20rollouts.%20For%20action%20controllability%2C%20we%20incorporate%20a%20versatile%0Aset%20of%20controls%20from%20high-level%20intentions%20%28command%2C%20goal%20point%29%20to%20low-level%0Amaneuvers%20%28trajectory%2C%20angle%2C%20and%20speed%29%20through%20an%20efficient%20learning%0Astrategy.%20After%20large-scale%20training%2C%20the%20capabilities%20of%20Vista%20can%20seamlessly%0Ageneralize%20to%20different%20scenarios.%20Extensive%20experiments%20on%20multiple%20datasets%0Ashow%20that%20Vista%20outperforms%20the%20most%20advanced%20general-purpose%20video%20generator%0Ain%20over%2070%25%20of%20comparisons%20and%20surpasses%20the%20best-performing%20driving%20world%0Amodel%20by%2055%25%20in%20FID%20and%2027%25%20in%20FVD.%20Moreover%2C%20for%20the%20first%20time%2C%20we%20utilize%0Athe%20capacity%20of%20Vista%20itself%20to%20establish%20a%20generalizable%20reward%20for%20real-world%0Aaction%20evaluation%20without%20accessing%20the%20ground%20truth%20actions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17398v3&entry.124074799=Read"},
{"title": "AttentNet: Fully Convolutional 3D Attention for Lung Nodule Detection", "author": "Majedaldein Almahasneh and Xianghua Xie and Adeline Paiement", "abstract": "  Motivated by the increasing popularity of attention mechanisms, we observe\nthat popular convolutional (conv.) attention models like Squeeze-and-Excite\n(SE) and Convolutional Block Attention Module (CBAM) rely on expensive\nmulti-layer perception (MLP) layers. These MLP layers significantly increase\ncomputational complexity, making such models less applicable to 3D image\ncontexts, where data dimensionality and computational costs are higher. In 3D\nmedical imaging, such as 3D pulmonary CT scans, efficient processing is crucial\ndue to the large data volume. Traditional 2D attention generalized to 3D\nincreases the computational load, creating demand for more efficient attention\nmechanisms for 3D tasks. We investigate the possibility of incorporating fully\nconvolutional (conv.) attention in 3D context. We present two 3D fully conv.\nattention blocks, demonstrating their effectiveness in 3D context. Using\npulmonary CT scans for 3D lung nodule detection, we present AttentNet, an\nautomated lung nodule detection framework from CT images, performing detection\nas an ensemble of two stages, candidate proposal and false positive (FP)\nreduction. We compare the proposed 3D attention blocks to popular 2D conv.\nattention methods generalized to 3D modules and to self-attention units. For\nthe FP reduction stage, we also use a joint analysis approach to aggregate\nspatial information from different contextual levels. We use LUNA-16 lung\nnodule detection dataset to demonstrate the benefits of the proposed fully\nconv. attention blocks compared to baseline popular lung nodule detection\nmethods when no attention is used. Our work does not aim at achieving\nstate-of-the-art results in the lung nodule detection task, rather to\ndemonstrate the benefits of incorporating fully conv. attention within a 3D\ncontext.\n", "link": "http://arxiv.org/abs/2407.14464v1", "date": "2024-07-19", "relevancy": 2.1741, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5618}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5493}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AttentNet%3A%20Fully%20Convolutional%203D%20Attention%20for%20Lung%20Nodule%20Detection&body=Title%3A%20AttentNet%3A%20Fully%20Convolutional%203D%20Attention%20for%20Lung%20Nodule%20Detection%0AAuthor%3A%20Majedaldein%20Almahasneh%20and%20Xianghua%20Xie%20and%20Adeline%20Paiement%0AAbstract%3A%20%20%20Motivated%20by%20the%20increasing%20popularity%20of%20attention%20mechanisms%2C%20we%20observe%0Athat%20popular%20convolutional%20%28conv.%29%20attention%20models%20like%20Squeeze-and-Excite%0A%28SE%29%20and%20Convolutional%20Block%20Attention%20Module%20%28CBAM%29%20rely%20on%20expensive%0Amulti-layer%20perception%20%28MLP%29%20layers.%20These%20MLP%20layers%20significantly%20increase%0Acomputational%20complexity%2C%20making%20such%20models%20less%20applicable%20to%203D%20image%0Acontexts%2C%20where%20data%20dimensionality%20and%20computational%20costs%20are%20higher.%20In%203D%0Amedical%20imaging%2C%20such%20as%203D%20pulmonary%20CT%20scans%2C%20efficient%20processing%20is%20crucial%0Adue%20to%20the%20large%20data%20volume.%20Traditional%202D%20attention%20generalized%20to%203D%0Aincreases%20the%20computational%20load%2C%20creating%20demand%20for%20more%20efficient%20attention%0Amechanisms%20for%203D%20tasks.%20We%20investigate%20the%20possibility%20of%20incorporating%20fully%0Aconvolutional%20%28conv.%29%20attention%20in%203D%20context.%20We%20present%20two%203D%20fully%20conv.%0Aattention%20blocks%2C%20demonstrating%20their%20effectiveness%20in%203D%20context.%20Using%0Apulmonary%20CT%20scans%20for%203D%20lung%20nodule%20detection%2C%20we%20present%20AttentNet%2C%20an%0Aautomated%20lung%20nodule%20detection%20framework%20from%20CT%20images%2C%20performing%20detection%0Aas%20an%20ensemble%20of%20two%20stages%2C%20candidate%20proposal%20and%20false%20positive%20%28FP%29%0Areduction.%20We%20compare%20the%20proposed%203D%20attention%20blocks%20to%20popular%202D%20conv.%0Aattention%20methods%20generalized%20to%203D%20modules%20and%20to%20self-attention%20units.%20For%0Athe%20FP%20reduction%20stage%2C%20we%20also%20use%20a%20joint%20analysis%20approach%20to%20aggregate%0Aspatial%20information%20from%20different%20contextual%20levels.%20We%20use%20LUNA-16%20lung%0Anodule%20detection%20dataset%20to%20demonstrate%20the%20benefits%20of%20the%20proposed%20fully%0Aconv.%20attention%20blocks%20compared%20to%20baseline%20popular%20lung%20nodule%20detection%0Amethods%20when%20no%20attention%20is%20used.%20Our%20work%20does%20not%20aim%20at%20achieving%0Astate-of-the-art%20results%20in%20the%20lung%20nodule%20detection%20task%2C%20rather%20to%0Ademonstrate%20the%20benefits%20of%20incorporating%20fully%20conv.%20attention%20within%20a%203D%0Acontext.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14464v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttentNet%253A%2520Fully%2520Convolutional%25203D%2520Attention%2520for%2520Lung%2520Nodule%2520Detection%26entry.906535625%3DMajedaldein%2520Almahasneh%2520and%2520Xianghua%2520Xie%2520and%2520Adeline%2520Paiement%26entry.1292438233%3D%2520%2520Motivated%2520by%2520the%2520increasing%2520popularity%2520of%2520attention%2520mechanisms%252C%2520we%2520observe%250Athat%2520popular%2520convolutional%2520%2528conv.%2529%2520attention%2520models%2520like%2520Squeeze-and-Excite%250A%2528SE%2529%2520and%2520Convolutional%2520Block%2520Attention%2520Module%2520%2528CBAM%2529%2520rely%2520on%2520expensive%250Amulti-layer%2520perception%2520%2528MLP%2529%2520layers.%2520These%2520MLP%2520layers%2520significantly%2520increase%250Acomputational%2520complexity%252C%2520making%2520such%2520models%2520less%2520applicable%2520to%25203D%2520image%250Acontexts%252C%2520where%2520data%2520dimensionality%2520and%2520computational%2520costs%2520are%2520higher.%2520In%25203D%250Amedical%2520imaging%252C%2520such%2520as%25203D%2520pulmonary%2520CT%2520scans%252C%2520efficient%2520processing%2520is%2520crucial%250Adue%2520to%2520the%2520large%2520data%2520volume.%2520Traditional%25202D%2520attention%2520generalized%2520to%25203D%250Aincreases%2520the%2520computational%2520load%252C%2520creating%2520demand%2520for%2520more%2520efficient%2520attention%250Amechanisms%2520for%25203D%2520tasks.%2520We%2520investigate%2520the%2520possibility%2520of%2520incorporating%2520fully%250Aconvolutional%2520%2528conv.%2529%2520attention%2520in%25203D%2520context.%2520We%2520present%2520two%25203D%2520fully%2520conv.%250Aattention%2520blocks%252C%2520demonstrating%2520their%2520effectiveness%2520in%25203D%2520context.%2520Using%250Apulmonary%2520CT%2520scans%2520for%25203D%2520lung%2520nodule%2520detection%252C%2520we%2520present%2520AttentNet%252C%2520an%250Aautomated%2520lung%2520nodule%2520detection%2520framework%2520from%2520CT%2520images%252C%2520performing%2520detection%250Aas%2520an%2520ensemble%2520of%2520two%2520stages%252C%2520candidate%2520proposal%2520and%2520false%2520positive%2520%2528FP%2529%250Areduction.%2520We%2520compare%2520the%2520proposed%25203D%2520attention%2520blocks%2520to%2520popular%25202D%2520conv.%250Aattention%2520methods%2520generalized%2520to%25203D%2520modules%2520and%2520to%2520self-attention%2520units.%2520For%250Athe%2520FP%2520reduction%2520stage%252C%2520we%2520also%2520use%2520a%2520joint%2520analysis%2520approach%2520to%2520aggregate%250Aspatial%2520information%2520from%2520different%2520contextual%2520levels.%2520We%2520use%2520LUNA-16%2520lung%250Anodule%2520detection%2520dataset%2520to%2520demonstrate%2520the%2520benefits%2520of%2520the%2520proposed%2520fully%250Aconv.%2520attention%2520blocks%2520compared%2520to%2520baseline%2520popular%2520lung%2520nodule%2520detection%250Amethods%2520when%2520no%2520attention%2520is%2520used.%2520Our%2520work%2520does%2520not%2520aim%2520at%2520achieving%250Astate-of-the-art%2520results%2520in%2520the%2520lung%2520nodule%2520detection%2520task%252C%2520rather%2520to%250Ademonstrate%2520the%2520benefits%2520of%2520incorporating%2520fully%2520conv.%2520attention%2520within%2520a%25203D%250Acontext.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14464v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AttentNet%3A%20Fully%20Convolutional%203D%20Attention%20for%20Lung%20Nodule%20Detection&entry.906535625=Majedaldein%20Almahasneh%20and%20Xianghua%20Xie%20and%20Adeline%20Paiement&entry.1292438233=%20%20Motivated%20by%20the%20increasing%20popularity%20of%20attention%20mechanisms%2C%20we%20observe%0Athat%20popular%20convolutional%20%28conv.%29%20attention%20models%20like%20Squeeze-and-Excite%0A%28SE%29%20and%20Convolutional%20Block%20Attention%20Module%20%28CBAM%29%20rely%20on%20expensive%0Amulti-layer%20perception%20%28MLP%29%20layers.%20These%20MLP%20layers%20significantly%20increase%0Acomputational%20complexity%2C%20making%20such%20models%20less%20applicable%20to%203D%20image%0Acontexts%2C%20where%20data%20dimensionality%20and%20computational%20costs%20are%20higher.%20In%203D%0Amedical%20imaging%2C%20such%20as%203D%20pulmonary%20CT%20scans%2C%20efficient%20processing%20is%20crucial%0Adue%20to%20the%20large%20data%20volume.%20Traditional%202D%20attention%20generalized%20to%203D%0Aincreases%20the%20computational%20load%2C%20creating%20demand%20for%20more%20efficient%20attention%0Amechanisms%20for%203D%20tasks.%20We%20investigate%20the%20possibility%20of%20incorporating%20fully%0Aconvolutional%20%28conv.%29%20attention%20in%203D%20context.%20We%20present%20two%203D%20fully%20conv.%0Aattention%20blocks%2C%20demonstrating%20their%20effectiveness%20in%203D%20context.%20Using%0Apulmonary%20CT%20scans%20for%203D%20lung%20nodule%20detection%2C%20we%20present%20AttentNet%2C%20an%0Aautomated%20lung%20nodule%20detection%20framework%20from%20CT%20images%2C%20performing%20detection%0Aas%20an%20ensemble%20of%20two%20stages%2C%20candidate%20proposal%20and%20false%20positive%20%28FP%29%0Areduction.%20We%20compare%20the%20proposed%203D%20attention%20blocks%20to%20popular%202D%20conv.%0Aattention%20methods%20generalized%20to%203D%20modules%20and%20to%20self-attention%20units.%20For%0Athe%20FP%20reduction%20stage%2C%20we%20also%20use%20a%20joint%20analysis%20approach%20to%20aggregate%0Aspatial%20information%20from%20different%20contextual%20levels.%20We%20use%20LUNA-16%20lung%0Anodule%20detection%20dataset%20to%20demonstrate%20the%20benefits%20of%20the%20proposed%20fully%0Aconv.%20attention%20blocks%20compared%20to%20baseline%20popular%20lung%20nodule%20detection%0Amethods%20when%20no%20attention%20is%20used.%20Our%20work%20does%20not%20aim%20at%20achieving%0Astate-of-the-art%20results%20in%20the%20lung%20nodule%20detection%20task%2C%20rather%20to%0Ademonstrate%20the%20benefits%20of%20incorporating%20fully%20conv.%20attention%20within%20a%203D%0Acontext.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14464v1&entry.124074799=Read"},
{"title": "Unsupervised Spike Depth Estimation via Cross-modality Cross-domain\n  Knowledge Transfer", "author": "Jiaming Liu and Qizhe Zhang and Xiaoqi Li and Jianing Li and Guanqun Wang and Ming Lu and Tiejun Huang and Shanghang Zhang", "abstract": "  Neuromorphic spike data, an upcoming modality with high temporal resolution,\nhas shown promising potential in autonomous driving by mitigating the\nchallenges posed by high-velocity motion blur. However, training the spike\ndepth estimation network holds significant challenges in two aspects: sparse\nspatial information for pixel-wise tasks and difficulties in achieving paired\ndepth labels for temporally intensive spike streams. Therefore, we introduce\nopen-source RGB data to support spike depth estimation, leveraging its\nannotations and spatial information. The inherent differences in modalities and\ndata distribution make it challenging to directly apply transfer learning from\nopen-source RGB to target spike data. To this end, we propose a cross-modality\ncross-domain (BiCross) framework to realize unsupervised spike depth estimation\nby introducing simulated mediate source spike data. Specifically, we design a\nCoarse-to-Fine Knowledge Distillation (CFKD) approach to facilitate\ncomprehensive cross-modality knowledge transfer while preserving the unique\nstrengths of both modalities, utilizing a spike-oriented uncertainty scheme.\nThen, we propose a Self-Correcting Teacher-Student (SCTS) mechanism to screen\nout reliable pixel-wise pseudo labels and ease the domain shift of the student\nmodel, which avoids error accumulation in target spike data. To verify the\neffectiveness of BiCross, we conduct extensive experiments on four scenarios,\nincluding Synthetic to Real, Extreme Weather, Scene Changing, and Real Spike.\nOur method achieves state-of-the-art (SOTA) performances, compared with\nRGB-oriented unsupervised depth estimation methods. Code and dataset:\nhttps://github.com/Theia-4869/BiCross\n", "link": "http://arxiv.org/abs/2208.12527v3", "date": "2024-07-19", "relevancy": 2.161, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5443}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5434}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Spike%20Depth%20Estimation%20via%20Cross-modality%20Cross-domain%0A%20%20Knowledge%20Transfer&body=Title%3A%20Unsupervised%20Spike%20Depth%20Estimation%20via%20Cross-modality%20Cross-domain%0A%20%20Knowledge%20Transfer%0AAuthor%3A%20Jiaming%20Liu%20and%20Qizhe%20Zhang%20and%20Xiaoqi%20Li%20and%20Jianing%20Li%20and%20Guanqun%20Wang%20and%20Ming%20Lu%20and%20Tiejun%20Huang%20and%20Shanghang%20Zhang%0AAbstract%3A%20%20%20Neuromorphic%20spike%20data%2C%20an%20upcoming%20modality%20with%20high%20temporal%20resolution%2C%0Ahas%20shown%20promising%20potential%20in%20autonomous%20driving%20by%20mitigating%20the%0Achallenges%20posed%20by%20high-velocity%20motion%20blur.%20However%2C%20training%20the%20spike%0Adepth%20estimation%20network%20holds%20significant%20challenges%20in%20two%20aspects%3A%20sparse%0Aspatial%20information%20for%20pixel-wise%20tasks%20and%20difficulties%20in%20achieving%20paired%0Adepth%20labels%20for%20temporally%20intensive%20spike%20streams.%20Therefore%2C%20we%20introduce%0Aopen-source%20RGB%20data%20to%20support%20spike%20depth%20estimation%2C%20leveraging%20its%0Aannotations%20and%20spatial%20information.%20The%20inherent%20differences%20in%20modalities%20and%0Adata%20distribution%20make%20it%20challenging%20to%20directly%20apply%20transfer%20learning%20from%0Aopen-source%20RGB%20to%20target%20spike%20data.%20To%20this%20end%2C%20we%20propose%20a%20cross-modality%0Across-domain%20%28BiCross%29%20framework%20to%20realize%20unsupervised%20spike%20depth%20estimation%0Aby%20introducing%20simulated%20mediate%20source%20spike%20data.%20Specifically%2C%20we%20design%20a%0ACoarse-to-Fine%20Knowledge%20Distillation%20%28CFKD%29%20approach%20to%20facilitate%0Acomprehensive%20cross-modality%20knowledge%20transfer%20while%20preserving%20the%20unique%0Astrengths%20of%20both%20modalities%2C%20utilizing%20a%20spike-oriented%20uncertainty%20scheme.%0AThen%2C%20we%20propose%20a%20Self-Correcting%20Teacher-Student%20%28SCTS%29%20mechanism%20to%20screen%0Aout%20reliable%20pixel-wise%20pseudo%20labels%20and%20ease%20the%20domain%20shift%20of%20the%20student%0Amodel%2C%20which%20avoids%20error%20accumulation%20in%20target%20spike%20data.%20To%20verify%20the%0Aeffectiveness%20of%20BiCross%2C%20we%20conduct%20extensive%20experiments%20on%20four%20scenarios%2C%0Aincluding%20Synthetic%20to%20Real%2C%20Extreme%20Weather%2C%20Scene%20Changing%2C%20and%20Real%20Spike.%0AOur%20method%20achieves%20state-of-the-art%20%28SOTA%29%20performances%2C%20compared%20with%0ARGB-oriented%20unsupervised%20depth%20estimation%20methods.%20Code%20and%20dataset%3A%0Ahttps%3A//github.com/Theia-4869/BiCross%0A%0ALink%3A%20http%3A//arxiv.org/abs/2208.12527v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Spike%2520Depth%2520Estimation%2520via%2520Cross-modality%2520Cross-domain%250A%2520%2520Knowledge%2520Transfer%26entry.906535625%3DJiaming%2520Liu%2520and%2520Qizhe%2520Zhang%2520and%2520Xiaoqi%2520Li%2520and%2520Jianing%2520Li%2520and%2520Guanqun%2520Wang%2520and%2520Ming%2520Lu%2520and%2520Tiejun%2520Huang%2520and%2520Shanghang%2520Zhang%26entry.1292438233%3D%2520%2520Neuromorphic%2520spike%2520data%252C%2520an%2520upcoming%2520modality%2520with%2520high%2520temporal%2520resolution%252C%250Ahas%2520shown%2520promising%2520potential%2520in%2520autonomous%2520driving%2520by%2520mitigating%2520the%250Achallenges%2520posed%2520by%2520high-velocity%2520motion%2520blur.%2520However%252C%2520training%2520the%2520spike%250Adepth%2520estimation%2520network%2520holds%2520significant%2520challenges%2520in%2520two%2520aspects%253A%2520sparse%250Aspatial%2520information%2520for%2520pixel-wise%2520tasks%2520and%2520difficulties%2520in%2520achieving%2520paired%250Adepth%2520labels%2520for%2520temporally%2520intensive%2520spike%2520streams.%2520Therefore%252C%2520we%2520introduce%250Aopen-source%2520RGB%2520data%2520to%2520support%2520spike%2520depth%2520estimation%252C%2520leveraging%2520its%250Aannotations%2520and%2520spatial%2520information.%2520The%2520inherent%2520differences%2520in%2520modalities%2520and%250Adata%2520distribution%2520make%2520it%2520challenging%2520to%2520directly%2520apply%2520transfer%2520learning%2520from%250Aopen-source%2520RGB%2520to%2520target%2520spike%2520data.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520cross-modality%250Across-domain%2520%2528BiCross%2529%2520framework%2520to%2520realize%2520unsupervised%2520spike%2520depth%2520estimation%250Aby%2520introducing%2520simulated%2520mediate%2520source%2520spike%2520data.%2520Specifically%252C%2520we%2520design%2520a%250ACoarse-to-Fine%2520Knowledge%2520Distillation%2520%2528CFKD%2529%2520approach%2520to%2520facilitate%250Acomprehensive%2520cross-modality%2520knowledge%2520transfer%2520while%2520preserving%2520the%2520unique%250Astrengths%2520of%2520both%2520modalities%252C%2520utilizing%2520a%2520spike-oriented%2520uncertainty%2520scheme.%250AThen%252C%2520we%2520propose%2520a%2520Self-Correcting%2520Teacher-Student%2520%2528SCTS%2529%2520mechanism%2520to%2520screen%250Aout%2520reliable%2520pixel-wise%2520pseudo%2520labels%2520and%2520ease%2520the%2520domain%2520shift%2520of%2520the%2520student%250Amodel%252C%2520which%2520avoids%2520error%2520accumulation%2520in%2520target%2520spike%2520data.%2520To%2520verify%2520the%250Aeffectiveness%2520of%2520BiCross%252C%2520we%2520conduct%2520extensive%2520experiments%2520on%2520four%2520scenarios%252C%250Aincluding%2520Synthetic%2520to%2520Real%252C%2520Extreme%2520Weather%252C%2520Scene%2520Changing%252C%2520and%2520Real%2520Spike.%250AOur%2520method%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520performances%252C%2520compared%2520with%250ARGB-oriented%2520unsupervised%2520depth%2520estimation%2520methods.%2520Code%2520and%2520dataset%253A%250Ahttps%253A//github.com/Theia-4869/BiCross%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2208.12527v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Spike%20Depth%20Estimation%20via%20Cross-modality%20Cross-domain%0A%20%20Knowledge%20Transfer&entry.906535625=Jiaming%20Liu%20and%20Qizhe%20Zhang%20and%20Xiaoqi%20Li%20and%20Jianing%20Li%20and%20Guanqun%20Wang%20and%20Ming%20Lu%20and%20Tiejun%20Huang%20and%20Shanghang%20Zhang&entry.1292438233=%20%20Neuromorphic%20spike%20data%2C%20an%20upcoming%20modality%20with%20high%20temporal%20resolution%2C%0Ahas%20shown%20promising%20potential%20in%20autonomous%20driving%20by%20mitigating%20the%0Achallenges%20posed%20by%20high-velocity%20motion%20blur.%20However%2C%20training%20the%20spike%0Adepth%20estimation%20network%20holds%20significant%20challenges%20in%20two%20aspects%3A%20sparse%0Aspatial%20information%20for%20pixel-wise%20tasks%20and%20difficulties%20in%20achieving%20paired%0Adepth%20labels%20for%20temporally%20intensive%20spike%20streams.%20Therefore%2C%20we%20introduce%0Aopen-source%20RGB%20data%20to%20support%20spike%20depth%20estimation%2C%20leveraging%20its%0Aannotations%20and%20spatial%20information.%20The%20inherent%20differences%20in%20modalities%20and%0Adata%20distribution%20make%20it%20challenging%20to%20directly%20apply%20transfer%20learning%20from%0Aopen-source%20RGB%20to%20target%20spike%20data.%20To%20this%20end%2C%20we%20propose%20a%20cross-modality%0Across-domain%20%28BiCross%29%20framework%20to%20realize%20unsupervised%20spike%20depth%20estimation%0Aby%20introducing%20simulated%20mediate%20source%20spike%20data.%20Specifically%2C%20we%20design%20a%0ACoarse-to-Fine%20Knowledge%20Distillation%20%28CFKD%29%20approach%20to%20facilitate%0Acomprehensive%20cross-modality%20knowledge%20transfer%20while%20preserving%20the%20unique%0Astrengths%20of%20both%20modalities%2C%20utilizing%20a%20spike-oriented%20uncertainty%20scheme.%0AThen%2C%20we%20propose%20a%20Self-Correcting%20Teacher-Student%20%28SCTS%29%20mechanism%20to%20screen%0Aout%20reliable%20pixel-wise%20pseudo%20labels%20and%20ease%20the%20domain%20shift%20of%20the%20student%0Amodel%2C%20which%20avoids%20error%20accumulation%20in%20target%20spike%20data.%20To%20verify%20the%0Aeffectiveness%20of%20BiCross%2C%20we%20conduct%20extensive%20experiments%20on%20four%20scenarios%2C%0Aincluding%20Synthetic%20to%20Real%2C%20Extreme%20Weather%2C%20Scene%20Changing%2C%20and%20Real%20Spike.%0AOur%20method%20achieves%20state-of-the-art%20%28SOTA%29%20performances%2C%20compared%20with%0ARGB-oriented%20unsupervised%20depth%20estimation%20methods.%20Code%20and%20dataset%3A%0Ahttps%3A//github.com/Theia-4869/BiCross%0A&entry.1838667208=http%3A//arxiv.org/abs/2208.12527v3&entry.124074799=Read"},
{"title": "PIPNet3D: Interpretable Detection of Alzheimer in MRI Scans", "author": "Lisa Anita De Santi and J\u00f6rg Schl\u00f6tterer and Michael Scheschenja and Joel Wessendorf and Meike Nauta and Vincenzo Positano and Christin Seifert", "abstract": "  Information from neuroimaging examinations is increasingly used to support\ndiagnoses of dementia, e.g., Alzheimer's disease. While current clinical\npractice is mainly based on visual inspection and feature engineering, Deep\nLearning approaches can be used to automate the analysis and to discover new\nimage-biomarkers. Part-prototype neural networks (PP-NN) are an alternative to\nstandard blackbox models, and have shown promising results in general computer\nvision. PP-NN's base their reasoning on prototypical image regions that are\nlearned fully unsupervised, and combined with a simple-to-understand decision\nlayer. We present PIPNet3D, a PP-NN for volumetric images. We apply PIPNet3D to\nthe clinical diagnosis of Alzheimer's Disease from structural Magnetic\nResonance Imaging (sMRI). We assess the quality of prototypes under a\nsystematic evaluation framework, propose new functionally grounded metrics to\nevaluate brain prototypes and develop an evaluation scheme to assess their\ncoherency with domain experts. Our results show that PIPNet3D is an\ninterpretable, compact model for Alzheimer's diagnosis with its reasoning well\naligned to medical domain knowledge. Notably, PIPNet3D achieves the same\naccuracy as its blackbox counterpart; and removing the remaining clinically\nirrelevant prototypes from its decision process does not decrease predictive\nperformance.\n", "link": "http://arxiv.org/abs/2403.18328v2", "date": "2024-07-19", "relevancy": 2.1586, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5494}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5333}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PIPNet3D%3A%20Interpretable%20Detection%20of%20Alzheimer%20in%20MRI%20Scans&body=Title%3A%20PIPNet3D%3A%20Interpretable%20Detection%20of%20Alzheimer%20in%20MRI%20Scans%0AAuthor%3A%20Lisa%20Anita%20De%20Santi%20and%20J%C3%B6rg%20Schl%C3%B6tterer%20and%20Michael%20Scheschenja%20and%20Joel%20Wessendorf%20and%20Meike%20Nauta%20and%20Vincenzo%20Positano%20and%20Christin%20Seifert%0AAbstract%3A%20%20%20Information%20from%20neuroimaging%20examinations%20is%20increasingly%20used%20to%20support%0Adiagnoses%20of%20dementia%2C%20e.g.%2C%20Alzheimer%27s%20disease.%20While%20current%20clinical%0Apractice%20is%20mainly%20based%20on%20visual%20inspection%20and%20feature%20engineering%2C%20Deep%0ALearning%20approaches%20can%20be%20used%20to%20automate%20the%20analysis%20and%20to%20discover%20new%0Aimage-biomarkers.%20Part-prototype%20neural%20networks%20%28PP-NN%29%20are%20an%20alternative%20to%0Astandard%20blackbox%20models%2C%20and%20have%20shown%20promising%20results%20in%20general%20computer%0Avision.%20PP-NN%27s%20base%20their%20reasoning%20on%20prototypical%20image%20regions%20that%20are%0Alearned%20fully%20unsupervised%2C%20and%20combined%20with%20a%20simple-to-understand%20decision%0Alayer.%20We%20present%20PIPNet3D%2C%20a%20PP-NN%20for%20volumetric%20images.%20We%20apply%20PIPNet3D%20to%0Athe%20clinical%20diagnosis%20of%20Alzheimer%27s%20Disease%20from%20structural%20Magnetic%0AResonance%20Imaging%20%28sMRI%29.%20We%20assess%20the%20quality%20of%20prototypes%20under%20a%0Asystematic%20evaluation%20framework%2C%20propose%20new%20functionally%20grounded%20metrics%20to%0Aevaluate%20brain%20prototypes%20and%20develop%20an%20evaluation%20scheme%20to%20assess%20their%0Acoherency%20with%20domain%20experts.%20Our%20results%20show%20that%20PIPNet3D%20is%20an%0Ainterpretable%2C%20compact%20model%20for%20Alzheimer%27s%20diagnosis%20with%20its%20reasoning%20well%0Aaligned%20to%20medical%20domain%20knowledge.%20Notably%2C%20PIPNet3D%20achieves%20the%20same%0Aaccuracy%20as%20its%20blackbox%20counterpart%3B%20and%20removing%20the%20remaining%20clinically%0Airrelevant%20prototypes%20from%20its%20decision%20process%20does%20not%20decrease%20predictive%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18328v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPIPNet3D%253A%2520Interpretable%2520Detection%2520of%2520Alzheimer%2520in%2520MRI%2520Scans%26entry.906535625%3DLisa%2520Anita%2520De%2520Santi%2520and%2520J%25C3%25B6rg%2520Schl%25C3%25B6tterer%2520and%2520Michael%2520Scheschenja%2520and%2520Joel%2520Wessendorf%2520and%2520Meike%2520Nauta%2520and%2520Vincenzo%2520Positano%2520and%2520Christin%2520Seifert%26entry.1292438233%3D%2520%2520Information%2520from%2520neuroimaging%2520examinations%2520is%2520increasingly%2520used%2520to%2520support%250Adiagnoses%2520of%2520dementia%252C%2520e.g.%252C%2520Alzheimer%2527s%2520disease.%2520While%2520current%2520clinical%250Apractice%2520is%2520mainly%2520based%2520on%2520visual%2520inspection%2520and%2520feature%2520engineering%252C%2520Deep%250ALearning%2520approaches%2520can%2520be%2520used%2520to%2520automate%2520the%2520analysis%2520and%2520to%2520discover%2520new%250Aimage-biomarkers.%2520Part-prototype%2520neural%2520networks%2520%2528PP-NN%2529%2520are%2520an%2520alternative%2520to%250Astandard%2520blackbox%2520models%252C%2520and%2520have%2520shown%2520promising%2520results%2520in%2520general%2520computer%250Avision.%2520PP-NN%2527s%2520base%2520their%2520reasoning%2520on%2520prototypical%2520image%2520regions%2520that%2520are%250Alearned%2520fully%2520unsupervised%252C%2520and%2520combined%2520with%2520a%2520simple-to-understand%2520decision%250Alayer.%2520We%2520present%2520PIPNet3D%252C%2520a%2520PP-NN%2520for%2520volumetric%2520images.%2520We%2520apply%2520PIPNet3D%2520to%250Athe%2520clinical%2520diagnosis%2520of%2520Alzheimer%2527s%2520Disease%2520from%2520structural%2520Magnetic%250AResonance%2520Imaging%2520%2528sMRI%2529.%2520We%2520assess%2520the%2520quality%2520of%2520prototypes%2520under%2520a%250Asystematic%2520evaluation%2520framework%252C%2520propose%2520new%2520functionally%2520grounded%2520metrics%2520to%250Aevaluate%2520brain%2520prototypes%2520and%2520develop%2520an%2520evaluation%2520scheme%2520to%2520assess%2520their%250Acoherency%2520with%2520domain%2520experts.%2520Our%2520results%2520show%2520that%2520PIPNet3D%2520is%2520an%250Ainterpretable%252C%2520compact%2520model%2520for%2520Alzheimer%2527s%2520diagnosis%2520with%2520its%2520reasoning%2520well%250Aaligned%2520to%2520medical%2520domain%2520knowledge.%2520Notably%252C%2520PIPNet3D%2520achieves%2520the%2520same%250Aaccuracy%2520as%2520its%2520blackbox%2520counterpart%253B%2520and%2520removing%2520the%2520remaining%2520clinically%250Airrelevant%2520prototypes%2520from%2520its%2520decision%2520process%2520does%2520not%2520decrease%2520predictive%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.18328v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PIPNet3D%3A%20Interpretable%20Detection%20of%20Alzheimer%20in%20MRI%20Scans&entry.906535625=Lisa%20Anita%20De%20Santi%20and%20J%C3%B6rg%20Schl%C3%B6tterer%20and%20Michael%20Scheschenja%20and%20Joel%20Wessendorf%20and%20Meike%20Nauta%20and%20Vincenzo%20Positano%20and%20Christin%20Seifert&entry.1292438233=%20%20Information%20from%20neuroimaging%20examinations%20is%20increasingly%20used%20to%20support%0Adiagnoses%20of%20dementia%2C%20e.g.%2C%20Alzheimer%27s%20disease.%20While%20current%20clinical%0Apractice%20is%20mainly%20based%20on%20visual%20inspection%20and%20feature%20engineering%2C%20Deep%0ALearning%20approaches%20can%20be%20used%20to%20automate%20the%20analysis%20and%20to%20discover%20new%0Aimage-biomarkers.%20Part-prototype%20neural%20networks%20%28PP-NN%29%20are%20an%20alternative%20to%0Astandard%20blackbox%20models%2C%20and%20have%20shown%20promising%20results%20in%20general%20computer%0Avision.%20PP-NN%27s%20base%20their%20reasoning%20on%20prototypical%20image%20regions%20that%20are%0Alearned%20fully%20unsupervised%2C%20and%20combined%20with%20a%20simple-to-understand%20decision%0Alayer.%20We%20present%20PIPNet3D%2C%20a%20PP-NN%20for%20volumetric%20images.%20We%20apply%20PIPNet3D%20to%0Athe%20clinical%20diagnosis%20of%20Alzheimer%27s%20Disease%20from%20structural%20Magnetic%0AResonance%20Imaging%20%28sMRI%29.%20We%20assess%20the%20quality%20of%20prototypes%20under%20a%0Asystematic%20evaluation%20framework%2C%20propose%20new%20functionally%20grounded%20metrics%20to%0Aevaluate%20brain%20prototypes%20and%20develop%20an%20evaluation%20scheme%20to%20assess%20their%0Acoherency%20with%20domain%20experts.%20Our%20results%20show%20that%20PIPNet3D%20is%20an%0Ainterpretable%2C%20compact%20model%20for%20Alzheimer%27s%20diagnosis%20with%20its%20reasoning%20well%0Aaligned%20to%20medical%20domain%20knowledge.%20Notably%2C%20PIPNet3D%20achieves%20the%20same%0Aaccuracy%20as%20its%20blackbox%20counterpart%3B%20and%20removing%20the%20remaining%20clinically%0Airrelevant%20prototypes%20from%20its%20decision%20process%20does%20not%20decrease%20predictive%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18328v2&entry.124074799=Read"},
{"title": "KoMA: Knowledge-driven Multi-agent Framework for Autonomous Driving with\n  Large Language Models", "author": "Kemou Jiang and Xuan Cai and Zhiyong Cui and Aoyong Li and Yilong Ren and Haiyang Yu and Hao Yang and Daocheng Fu and Licheng Wen and Pinlong Cai", "abstract": "  Large language models (LLMs) as autonomous agents offer a novel avenue for\ntackling real-world challenges through a knowledge-driven manner. These\nLLM-enhanced methodologies excel in generalization and interpretability.\nHowever, the complexity of driving tasks often necessitates the collaboration\nof multiple, heterogeneous agents, underscoring the need for such LLM-driven\nagents to engage in cooperative knowledge sharing and cognitive synergy.\nDespite the promise of LLMs, current applications predominantly center around\nsingle agent scenarios. To broaden the horizons of knowledge-driven strategies\nand bolster the generalization capabilities of autonomous agents, we propose\nthe KoMA framework consisting of multi-agent interaction, multi-step planning,\nshared-memory, and ranking-based reflection modules to enhance multi-agents'\ndecision-making in complex driving scenarios. Based on the framework's\ngenerated text descriptions of driving scenarios, the multi-agent interaction\nmodule enables LLM agents to analyze and infer the intentions of surrounding\nvehicles, akin to human cognition. The multi-step planning module enables LLM\nagents to analyze and obtain final action decisions layer by layer to ensure\nconsistent goals for short-term action decisions. The shared memory module can\naccumulate collective experience to make superior decisions, and the\nranking-based reflection module can evaluate and improve agent behavior with\nthe aim of enhancing driving safety and efficiency. The KoMA framework not only\nenhances the robustness and adaptability of autonomous driving agents but also\nsignificantly elevates their generalization capabilities across diverse\nscenarios. Empirical results demonstrate the superiority of our approach over\ntraditional methods, particularly in its ability to handle complex,\nunpredictable driving environments without extensive retraining.\n", "link": "http://arxiv.org/abs/2407.14239v1", "date": "2024-07-19", "relevancy": 2.1558, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5478}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.544}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KoMA%3A%20Knowledge-driven%20Multi-agent%20Framework%20for%20Autonomous%20Driving%20with%0A%20%20Large%20Language%20Models&body=Title%3A%20KoMA%3A%20Knowledge-driven%20Multi-agent%20Framework%20for%20Autonomous%20Driving%20with%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Kemou%20Jiang%20and%20Xuan%20Cai%20and%20Zhiyong%20Cui%20and%20Aoyong%20Li%20and%20Yilong%20Ren%20and%20Haiyang%20Yu%20and%20Hao%20Yang%20and%20Daocheng%20Fu%20and%20Licheng%20Wen%20and%20Pinlong%20Cai%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20as%20autonomous%20agents%20offer%20a%20novel%20avenue%20for%0Atackling%20real-world%20challenges%20through%20a%20knowledge-driven%20manner.%20These%0ALLM-enhanced%20methodologies%20excel%20in%20generalization%20and%20interpretability.%0AHowever%2C%20the%20complexity%20of%20driving%20tasks%20often%20necessitates%20the%20collaboration%0Aof%20multiple%2C%20heterogeneous%20agents%2C%20underscoring%20the%20need%20for%20such%20LLM-driven%0Aagents%20to%20engage%20in%20cooperative%20knowledge%20sharing%20and%20cognitive%20synergy.%0ADespite%20the%20promise%20of%20LLMs%2C%20current%20applications%20predominantly%20center%20around%0Asingle%20agent%20scenarios.%20To%20broaden%20the%20horizons%20of%20knowledge-driven%20strategies%0Aand%20bolster%20the%20generalization%20capabilities%20of%20autonomous%20agents%2C%20we%20propose%0Athe%20KoMA%20framework%20consisting%20of%20multi-agent%20interaction%2C%20multi-step%20planning%2C%0Ashared-memory%2C%20and%20ranking-based%20reflection%20modules%20to%20enhance%20multi-agents%27%0Adecision-making%20in%20complex%20driving%20scenarios.%20Based%20on%20the%20framework%27s%0Agenerated%20text%20descriptions%20of%20driving%20scenarios%2C%20the%20multi-agent%20interaction%0Amodule%20enables%20LLM%20agents%20to%20analyze%20and%20infer%20the%20intentions%20of%20surrounding%0Avehicles%2C%20akin%20to%20human%20cognition.%20The%20multi-step%20planning%20module%20enables%20LLM%0Aagents%20to%20analyze%20and%20obtain%20final%20action%20decisions%20layer%20by%20layer%20to%20ensure%0Aconsistent%20goals%20for%20short-term%20action%20decisions.%20The%20shared%20memory%20module%20can%0Aaccumulate%20collective%20experience%20to%20make%20superior%20decisions%2C%20and%20the%0Aranking-based%20reflection%20module%20can%20evaluate%20and%20improve%20agent%20behavior%20with%0Athe%20aim%20of%20enhancing%20driving%20safety%20and%20efficiency.%20The%20KoMA%20framework%20not%20only%0Aenhances%20the%20robustness%20and%20adaptability%20of%20autonomous%20driving%20agents%20but%20also%0Asignificantly%20elevates%20their%20generalization%20capabilities%20across%20diverse%0Ascenarios.%20Empirical%20results%20demonstrate%20the%20superiority%20of%20our%20approach%20over%0Atraditional%20methods%2C%20particularly%20in%20its%20ability%20to%20handle%20complex%2C%0Aunpredictable%20driving%20environments%20without%20extensive%20retraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKoMA%253A%2520Knowledge-driven%2520Multi-agent%2520Framework%2520for%2520Autonomous%2520Driving%2520with%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DKemou%2520Jiang%2520and%2520Xuan%2520Cai%2520and%2520Zhiyong%2520Cui%2520and%2520Aoyong%2520Li%2520and%2520Yilong%2520Ren%2520and%2520Haiyang%2520Yu%2520and%2520Hao%2520Yang%2520and%2520Daocheng%2520Fu%2520and%2520Licheng%2520Wen%2520and%2520Pinlong%2520Cai%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520as%2520autonomous%2520agents%2520offer%2520a%2520novel%2520avenue%2520for%250Atackling%2520real-world%2520challenges%2520through%2520a%2520knowledge-driven%2520manner.%2520These%250ALLM-enhanced%2520methodologies%2520excel%2520in%2520generalization%2520and%2520interpretability.%250AHowever%252C%2520the%2520complexity%2520of%2520driving%2520tasks%2520often%2520necessitates%2520the%2520collaboration%250Aof%2520multiple%252C%2520heterogeneous%2520agents%252C%2520underscoring%2520the%2520need%2520for%2520such%2520LLM-driven%250Aagents%2520to%2520engage%2520in%2520cooperative%2520knowledge%2520sharing%2520and%2520cognitive%2520synergy.%250ADespite%2520the%2520promise%2520of%2520LLMs%252C%2520current%2520applications%2520predominantly%2520center%2520around%250Asingle%2520agent%2520scenarios.%2520To%2520broaden%2520the%2520horizons%2520of%2520knowledge-driven%2520strategies%250Aand%2520bolster%2520the%2520generalization%2520capabilities%2520of%2520autonomous%2520agents%252C%2520we%2520propose%250Athe%2520KoMA%2520framework%2520consisting%2520of%2520multi-agent%2520interaction%252C%2520multi-step%2520planning%252C%250Ashared-memory%252C%2520and%2520ranking-based%2520reflection%2520modules%2520to%2520enhance%2520multi-agents%2527%250Adecision-making%2520in%2520complex%2520driving%2520scenarios.%2520Based%2520on%2520the%2520framework%2527s%250Agenerated%2520text%2520descriptions%2520of%2520driving%2520scenarios%252C%2520the%2520multi-agent%2520interaction%250Amodule%2520enables%2520LLM%2520agents%2520to%2520analyze%2520and%2520infer%2520the%2520intentions%2520of%2520surrounding%250Avehicles%252C%2520akin%2520to%2520human%2520cognition.%2520The%2520multi-step%2520planning%2520module%2520enables%2520LLM%250Aagents%2520to%2520analyze%2520and%2520obtain%2520final%2520action%2520decisions%2520layer%2520by%2520layer%2520to%2520ensure%250Aconsistent%2520goals%2520for%2520short-term%2520action%2520decisions.%2520The%2520shared%2520memory%2520module%2520can%250Aaccumulate%2520collective%2520experience%2520to%2520make%2520superior%2520decisions%252C%2520and%2520the%250Aranking-based%2520reflection%2520module%2520can%2520evaluate%2520and%2520improve%2520agent%2520behavior%2520with%250Athe%2520aim%2520of%2520enhancing%2520driving%2520safety%2520and%2520efficiency.%2520The%2520KoMA%2520framework%2520not%2520only%250Aenhances%2520the%2520robustness%2520and%2520adaptability%2520of%2520autonomous%2520driving%2520agents%2520but%2520also%250Asignificantly%2520elevates%2520their%2520generalization%2520capabilities%2520across%2520diverse%250Ascenarios.%2520Empirical%2520results%2520demonstrate%2520the%2520superiority%2520of%2520our%2520approach%2520over%250Atraditional%2520methods%252C%2520particularly%2520in%2520its%2520ability%2520to%2520handle%2520complex%252C%250Aunpredictable%2520driving%2520environments%2520without%2520extensive%2520retraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KoMA%3A%20Knowledge-driven%20Multi-agent%20Framework%20for%20Autonomous%20Driving%20with%0A%20%20Large%20Language%20Models&entry.906535625=Kemou%20Jiang%20and%20Xuan%20Cai%20and%20Zhiyong%20Cui%20and%20Aoyong%20Li%20and%20Yilong%20Ren%20and%20Haiyang%20Yu%20and%20Hao%20Yang%20and%20Daocheng%20Fu%20and%20Licheng%20Wen%20and%20Pinlong%20Cai&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20as%20autonomous%20agents%20offer%20a%20novel%20avenue%20for%0Atackling%20real-world%20challenges%20through%20a%20knowledge-driven%20manner.%20These%0ALLM-enhanced%20methodologies%20excel%20in%20generalization%20and%20interpretability.%0AHowever%2C%20the%20complexity%20of%20driving%20tasks%20often%20necessitates%20the%20collaboration%0Aof%20multiple%2C%20heterogeneous%20agents%2C%20underscoring%20the%20need%20for%20such%20LLM-driven%0Aagents%20to%20engage%20in%20cooperative%20knowledge%20sharing%20and%20cognitive%20synergy.%0ADespite%20the%20promise%20of%20LLMs%2C%20current%20applications%20predominantly%20center%20around%0Asingle%20agent%20scenarios.%20To%20broaden%20the%20horizons%20of%20knowledge-driven%20strategies%0Aand%20bolster%20the%20generalization%20capabilities%20of%20autonomous%20agents%2C%20we%20propose%0Athe%20KoMA%20framework%20consisting%20of%20multi-agent%20interaction%2C%20multi-step%20planning%2C%0Ashared-memory%2C%20and%20ranking-based%20reflection%20modules%20to%20enhance%20multi-agents%27%0Adecision-making%20in%20complex%20driving%20scenarios.%20Based%20on%20the%20framework%27s%0Agenerated%20text%20descriptions%20of%20driving%20scenarios%2C%20the%20multi-agent%20interaction%0Amodule%20enables%20LLM%20agents%20to%20analyze%20and%20infer%20the%20intentions%20of%20surrounding%0Avehicles%2C%20akin%20to%20human%20cognition.%20The%20multi-step%20planning%20module%20enables%20LLM%0Aagents%20to%20analyze%20and%20obtain%20final%20action%20decisions%20layer%20by%20layer%20to%20ensure%0Aconsistent%20goals%20for%20short-term%20action%20decisions.%20The%20shared%20memory%20module%20can%0Aaccumulate%20collective%20experience%20to%20make%20superior%20decisions%2C%20and%20the%0Aranking-based%20reflection%20module%20can%20evaluate%20and%20improve%20agent%20behavior%20with%0Athe%20aim%20of%20enhancing%20driving%20safety%20and%20efficiency.%20The%20KoMA%20framework%20not%20only%0Aenhances%20the%20robustness%20and%20adaptability%20of%20autonomous%20driving%20agents%20but%20also%0Asignificantly%20elevates%20their%20generalization%20capabilities%20across%20diverse%0Ascenarios.%20Empirical%20results%20demonstrate%20the%20superiority%20of%20our%20approach%20over%0Atraditional%20methods%2C%20particularly%20in%20its%20ability%20to%20handle%20complex%2C%0Aunpredictable%20driving%20environments%20without%20extensive%20retraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14239v1&entry.124074799=Read"},
{"title": "On Pre-training of Multimodal Language Models Customized for Chart\n  Understanding", "author": "Wan-Cyuan Fan and Yen-Chun Chen and Mengchen Liu and Lu Yuan and Leonid Sigal", "abstract": "  Recent studies customizing Multimodal Large Language Models (MLLMs) for\ndomain-specific tasks have yielded promising results, especially in the field\nof scientific chart comprehension. These studies generally utilize visual\ninstruction tuning with specialized datasets to enhance question and answer\n(QA) accuracy within the chart domain. However, they often neglect the\nfundamental discrepancy between natural image-caption pre-training data and\ndigital chart image-QA data, particularly in the models' capacity to extract\nunderlying numeric values from charts. This paper tackles this oversight by\nexploring the training processes necessary to improve MLLMs' comprehension of\ncharts. We present three key findings: (1) Incorporating raw data values in\nalignment pre-training markedly improves comprehension of chart data. (2)\nReplacing images with their textual representation randomly during end-to-end\nfine-tuning transfer the language reasoning capability to chart interpretation\nskills. (3) Requiring the model to first extract the underlying chart data and\nthen answer the question in the fine-tuning can further improve the accuracy.\nConsequently, we introduce CHOPINLLM, an MLLM tailored for in-depth chart\ncomprehension. CHOPINLLM effectively interprets various types of charts,\nincluding unannotated ones, while maintaining robust reasoning abilities.\nFurthermore, we establish a new benchmark to evaluate MLLMs' understanding of\ndifferent chart types across various comprehension levels. Experimental results\nshow that CHOPINLLM exhibits strong performance in understanding both annotated\nand unannotated charts across a wide range of types.\n", "link": "http://arxiv.org/abs/2407.14506v1", "date": "2024-07-19", "relevancy": 2.1514, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5932}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4985}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4977}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Pre-training%20of%20Multimodal%20Language%20Models%20Customized%20for%20Chart%0A%20%20Understanding&body=Title%3A%20On%20Pre-training%20of%20Multimodal%20Language%20Models%20Customized%20for%20Chart%0A%20%20Understanding%0AAuthor%3A%20Wan-Cyuan%20Fan%20and%20Yen-Chun%20Chen%20and%20Mengchen%20Liu%20and%20Lu%20Yuan%20and%20Leonid%20Sigal%0AAbstract%3A%20%20%20Recent%20studies%20customizing%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20for%0Adomain-specific%20tasks%20have%20yielded%20promising%20results%2C%20especially%20in%20the%20field%0Aof%20scientific%20chart%20comprehension.%20These%20studies%20generally%20utilize%20visual%0Ainstruction%20tuning%20with%20specialized%20datasets%20to%20enhance%20question%20and%20answer%0A%28QA%29%20accuracy%20within%20the%20chart%20domain.%20However%2C%20they%20often%20neglect%20the%0Afundamental%20discrepancy%20between%20natural%20image-caption%20pre-training%20data%20and%0Adigital%20chart%20image-QA%20data%2C%20particularly%20in%20the%20models%27%20capacity%20to%20extract%0Aunderlying%20numeric%20values%20from%20charts.%20This%20paper%20tackles%20this%20oversight%20by%0Aexploring%20the%20training%20processes%20necessary%20to%20improve%20MLLMs%27%20comprehension%20of%0Acharts.%20We%20present%20three%20key%20findings%3A%20%281%29%20Incorporating%20raw%20data%20values%20in%0Aalignment%20pre-training%20markedly%20improves%20comprehension%20of%20chart%20data.%20%282%29%0AReplacing%20images%20with%20their%20textual%20representation%20randomly%20during%20end-to-end%0Afine-tuning%20transfer%20the%20language%20reasoning%20capability%20to%20chart%20interpretation%0Askills.%20%283%29%20Requiring%20the%20model%20to%20first%20extract%20the%20underlying%20chart%20data%20and%0Athen%20answer%20the%20question%20in%20the%20fine-tuning%20can%20further%20improve%20the%20accuracy.%0AConsequently%2C%20we%20introduce%20CHOPINLLM%2C%20an%20MLLM%20tailored%20for%20in-depth%20chart%0Acomprehension.%20CHOPINLLM%20effectively%20interprets%20various%20types%20of%20charts%2C%0Aincluding%20unannotated%20ones%2C%20while%20maintaining%20robust%20reasoning%20abilities.%0AFurthermore%2C%20we%20establish%20a%20new%20benchmark%20to%20evaluate%20MLLMs%27%20understanding%20of%0Adifferent%20chart%20types%20across%20various%20comprehension%20levels.%20Experimental%20results%0Ashow%20that%20CHOPINLLM%20exhibits%20strong%20performance%20in%20understanding%20both%20annotated%0Aand%20unannotated%20charts%20across%20a%20wide%20range%20of%20types.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14506v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Pre-training%2520of%2520Multimodal%2520Language%2520Models%2520Customized%2520for%2520Chart%250A%2520%2520Understanding%26entry.906535625%3DWan-Cyuan%2520Fan%2520and%2520Yen-Chun%2520Chen%2520and%2520Mengchen%2520Liu%2520and%2520Lu%2520Yuan%2520and%2520Leonid%2520Sigal%26entry.1292438233%3D%2520%2520Recent%2520studies%2520customizing%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520for%250Adomain-specific%2520tasks%2520have%2520yielded%2520promising%2520results%252C%2520especially%2520in%2520the%2520field%250Aof%2520scientific%2520chart%2520comprehension.%2520These%2520studies%2520generally%2520utilize%2520visual%250Ainstruction%2520tuning%2520with%2520specialized%2520datasets%2520to%2520enhance%2520question%2520and%2520answer%250A%2528QA%2529%2520accuracy%2520within%2520the%2520chart%2520domain.%2520However%252C%2520they%2520often%2520neglect%2520the%250Afundamental%2520discrepancy%2520between%2520natural%2520image-caption%2520pre-training%2520data%2520and%250Adigital%2520chart%2520image-QA%2520data%252C%2520particularly%2520in%2520the%2520models%2527%2520capacity%2520to%2520extract%250Aunderlying%2520numeric%2520values%2520from%2520charts.%2520This%2520paper%2520tackles%2520this%2520oversight%2520by%250Aexploring%2520the%2520training%2520processes%2520necessary%2520to%2520improve%2520MLLMs%2527%2520comprehension%2520of%250Acharts.%2520We%2520present%2520three%2520key%2520findings%253A%2520%25281%2529%2520Incorporating%2520raw%2520data%2520values%2520in%250Aalignment%2520pre-training%2520markedly%2520improves%2520comprehension%2520of%2520chart%2520data.%2520%25282%2529%250AReplacing%2520images%2520with%2520their%2520textual%2520representation%2520randomly%2520during%2520end-to-end%250Afine-tuning%2520transfer%2520the%2520language%2520reasoning%2520capability%2520to%2520chart%2520interpretation%250Askills.%2520%25283%2529%2520Requiring%2520the%2520model%2520to%2520first%2520extract%2520the%2520underlying%2520chart%2520data%2520and%250Athen%2520answer%2520the%2520question%2520in%2520the%2520fine-tuning%2520can%2520further%2520improve%2520the%2520accuracy.%250AConsequently%252C%2520we%2520introduce%2520CHOPINLLM%252C%2520an%2520MLLM%2520tailored%2520for%2520in-depth%2520chart%250Acomprehension.%2520CHOPINLLM%2520effectively%2520interprets%2520various%2520types%2520of%2520charts%252C%250Aincluding%2520unannotated%2520ones%252C%2520while%2520maintaining%2520robust%2520reasoning%2520abilities.%250AFurthermore%252C%2520we%2520establish%2520a%2520new%2520benchmark%2520to%2520evaluate%2520MLLMs%2527%2520understanding%2520of%250Adifferent%2520chart%2520types%2520across%2520various%2520comprehension%2520levels.%2520Experimental%2520results%250Ashow%2520that%2520CHOPINLLM%2520exhibits%2520strong%2520performance%2520in%2520understanding%2520both%2520annotated%250Aand%2520unannotated%2520charts%2520across%2520a%2520wide%2520range%2520of%2520types.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14506v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Pre-training%20of%20Multimodal%20Language%20Models%20Customized%20for%20Chart%0A%20%20Understanding&entry.906535625=Wan-Cyuan%20Fan%20and%20Yen-Chun%20Chen%20and%20Mengchen%20Liu%20and%20Lu%20Yuan%20and%20Leonid%20Sigal&entry.1292438233=%20%20Recent%20studies%20customizing%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20for%0Adomain-specific%20tasks%20have%20yielded%20promising%20results%2C%20especially%20in%20the%20field%0Aof%20scientific%20chart%20comprehension.%20These%20studies%20generally%20utilize%20visual%0Ainstruction%20tuning%20with%20specialized%20datasets%20to%20enhance%20question%20and%20answer%0A%28QA%29%20accuracy%20within%20the%20chart%20domain.%20However%2C%20they%20often%20neglect%20the%0Afundamental%20discrepancy%20between%20natural%20image-caption%20pre-training%20data%20and%0Adigital%20chart%20image-QA%20data%2C%20particularly%20in%20the%20models%27%20capacity%20to%20extract%0Aunderlying%20numeric%20values%20from%20charts.%20This%20paper%20tackles%20this%20oversight%20by%0Aexploring%20the%20training%20processes%20necessary%20to%20improve%20MLLMs%27%20comprehension%20of%0Acharts.%20We%20present%20three%20key%20findings%3A%20%281%29%20Incorporating%20raw%20data%20values%20in%0Aalignment%20pre-training%20markedly%20improves%20comprehension%20of%20chart%20data.%20%282%29%0AReplacing%20images%20with%20their%20textual%20representation%20randomly%20during%20end-to-end%0Afine-tuning%20transfer%20the%20language%20reasoning%20capability%20to%20chart%20interpretation%0Askills.%20%283%29%20Requiring%20the%20model%20to%20first%20extract%20the%20underlying%20chart%20data%20and%0Athen%20answer%20the%20question%20in%20the%20fine-tuning%20can%20further%20improve%20the%20accuracy.%0AConsequently%2C%20we%20introduce%20CHOPINLLM%2C%20an%20MLLM%20tailored%20for%20in-depth%20chart%0Acomprehension.%20CHOPINLLM%20effectively%20interprets%20various%20types%20of%20charts%2C%0Aincluding%20unannotated%20ones%2C%20while%20maintaining%20robust%20reasoning%20abilities.%0AFurthermore%2C%20we%20establish%20a%20new%20benchmark%20to%20evaluate%20MLLMs%27%20understanding%20of%0Adifferent%20chart%20types%20across%20various%20comprehension%20levels.%20Experimental%20results%0Ashow%20that%20CHOPINLLM%20exhibits%20strong%20performance%20in%20understanding%20both%20annotated%0Aand%20unannotated%20charts%20across%20a%20wide%20range%20of%20types.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14506v1&entry.124074799=Read"},
{"title": "HEROS: Hierarchical Exploration with Online Subregion Updating for 3D\n  Environment Coverage", "author": "Shijun Long and Ying Li and Chenming Wu and Bin Xu and Wei Fan", "abstract": "  We present an autonomous exploration system for efficient coverage of unknown\nenvironments. First, a rapid environment preprocessing method is introduced to\nprovide environmental information for subsequent exploration planning. Then,\nthe whole exploration space is divided into multiple subregion cells, each with\nvarying levels of detail. The subregion cells are capable of decomposition and\nupdating online, effectively characterizing dynamic unknown regions with\nvariable resolution. Finally, the hierarchical planning strategy treats\nsubregions as basic planning units and computes an efficient global coverage\npath. Guided by the global path, the local path that sequentially visits the\nviewpoint set is refined to provide an executable path for the robot. This\nhierarchical planning from coarse to fine steps reduces the complexity of the\nplanning scheme while improving exploration efficiency. The proposed method is\ncompared with state-of-art methods in benchmark environments. Our approach\ndemonstrates superior efficiency in completing exploration while using lower\ncomputational resources.\n", "link": "http://arxiv.org/abs/2407.11326v2", "date": "2024-07-19", "relevancy": 2.1508, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6335}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5202}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HEROS%3A%20Hierarchical%20Exploration%20with%20Online%20Subregion%20Updating%20for%203D%0A%20%20Environment%20Coverage&body=Title%3A%20HEROS%3A%20Hierarchical%20Exploration%20with%20Online%20Subregion%20Updating%20for%203D%0A%20%20Environment%20Coverage%0AAuthor%3A%20Shijun%20Long%20and%20Ying%20Li%20and%20Chenming%20Wu%20and%20Bin%20Xu%20and%20Wei%20Fan%0AAbstract%3A%20%20%20We%20present%20an%20autonomous%20exploration%20system%20for%20efficient%20coverage%20of%20unknown%0Aenvironments.%20First%2C%20a%20rapid%20environment%20preprocessing%20method%20is%20introduced%20to%0Aprovide%20environmental%20information%20for%20subsequent%20exploration%20planning.%20Then%2C%0Athe%20whole%20exploration%20space%20is%20divided%20into%20multiple%20subregion%20cells%2C%20each%20with%0Avarying%20levels%20of%20detail.%20The%20subregion%20cells%20are%20capable%20of%20decomposition%20and%0Aupdating%20online%2C%20effectively%20characterizing%20dynamic%20unknown%20regions%20with%0Avariable%20resolution.%20Finally%2C%20the%20hierarchical%20planning%20strategy%20treats%0Asubregions%20as%20basic%20planning%20units%20and%20computes%20an%20efficient%20global%20coverage%0Apath.%20Guided%20by%20the%20global%20path%2C%20the%20local%20path%20that%20sequentially%20visits%20the%0Aviewpoint%20set%20is%20refined%20to%20provide%20an%20executable%20path%20for%20the%20robot.%20This%0Ahierarchical%20planning%20from%20coarse%20to%20fine%20steps%20reduces%20the%20complexity%20of%20the%0Aplanning%20scheme%20while%20improving%20exploration%20efficiency.%20The%20proposed%20method%20is%0Acompared%20with%20state-of-art%20methods%20in%20benchmark%20environments.%20Our%20approach%0Ademonstrates%20superior%20efficiency%20in%20completing%20exploration%20while%20using%20lower%0Acomputational%20resources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11326v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHEROS%253A%2520Hierarchical%2520Exploration%2520with%2520Online%2520Subregion%2520Updating%2520for%25203D%250A%2520%2520Environment%2520Coverage%26entry.906535625%3DShijun%2520Long%2520and%2520Ying%2520Li%2520and%2520Chenming%2520Wu%2520and%2520Bin%2520Xu%2520and%2520Wei%2520Fan%26entry.1292438233%3D%2520%2520We%2520present%2520an%2520autonomous%2520exploration%2520system%2520for%2520efficient%2520coverage%2520of%2520unknown%250Aenvironments.%2520First%252C%2520a%2520rapid%2520environment%2520preprocessing%2520method%2520is%2520introduced%2520to%250Aprovide%2520environmental%2520information%2520for%2520subsequent%2520exploration%2520planning.%2520Then%252C%250Athe%2520whole%2520exploration%2520space%2520is%2520divided%2520into%2520multiple%2520subregion%2520cells%252C%2520each%2520with%250Avarying%2520levels%2520of%2520detail.%2520The%2520subregion%2520cells%2520are%2520capable%2520of%2520decomposition%2520and%250Aupdating%2520online%252C%2520effectively%2520characterizing%2520dynamic%2520unknown%2520regions%2520with%250Avariable%2520resolution.%2520Finally%252C%2520the%2520hierarchical%2520planning%2520strategy%2520treats%250Asubregions%2520as%2520basic%2520planning%2520units%2520and%2520computes%2520an%2520efficient%2520global%2520coverage%250Apath.%2520Guided%2520by%2520the%2520global%2520path%252C%2520the%2520local%2520path%2520that%2520sequentially%2520visits%2520the%250Aviewpoint%2520set%2520is%2520refined%2520to%2520provide%2520an%2520executable%2520path%2520for%2520the%2520robot.%2520This%250Ahierarchical%2520planning%2520from%2520coarse%2520to%2520fine%2520steps%2520reduces%2520the%2520complexity%2520of%2520the%250Aplanning%2520scheme%2520while%2520improving%2520exploration%2520efficiency.%2520The%2520proposed%2520method%2520is%250Acompared%2520with%2520state-of-art%2520methods%2520in%2520benchmark%2520environments.%2520Our%2520approach%250Ademonstrates%2520superior%2520efficiency%2520in%2520completing%2520exploration%2520while%2520using%2520lower%250Acomputational%2520resources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11326v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HEROS%3A%20Hierarchical%20Exploration%20with%20Online%20Subregion%20Updating%20for%203D%0A%20%20Environment%20Coverage&entry.906535625=Shijun%20Long%20and%20Ying%20Li%20and%20Chenming%20Wu%20and%20Bin%20Xu%20and%20Wei%20Fan&entry.1292438233=%20%20We%20present%20an%20autonomous%20exploration%20system%20for%20efficient%20coverage%20of%20unknown%0Aenvironments.%20First%2C%20a%20rapid%20environment%20preprocessing%20method%20is%20introduced%20to%0Aprovide%20environmental%20information%20for%20subsequent%20exploration%20planning.%20Then%2C%0Athe%20whole%20exploration%20space%20is%20divided%20into%20multiple%20subregion%20cells%2C%20each%20with%0Avarying%20levels%20of%20detail.%20The%20subregion%20cells%20are%20capable%20of%20decomposition%20and%0Aupdating%20online%2C%20effectively%20characterizing%20dynamic%20unknown%20regions%20with%0Avariable%20resolution.%20Finally%2C%20the%20hierarchical%20planning%20strategy%20treats%0Asubregions%20as%20basic%20planning%20units%20and%20computes%20an%20efficient%20global%20coverage%0Apath.%20Guided%20by%20the%20global%20path%2C%20the%20local%20path%20that%20sequentially%20visits%20the%0Aviewpoint%20set%20is%20refined%20to%20provide%20an%20executable%20path%20for%20the%20robot.%20This%0Ahierarchical%20planning%20from%20coarse%20to%20fine%20steps%20reduces%20the%20complexity%20of%20the%0Aplanning%20scheme%20while%20improving%20exploration%20efficiency.%20The%20proposed%20method%20is%0Acompared%20with%20state-of-art%20methods%20in%20benchmark%20environments.%20Our%20approach%0Ademonstrates%20superior%20efficiency%20in%20completing%20exploration%20while%20using%20lower%0Acomputational%20resources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11326v2&entry.124074799=Read"},
{"title": "MLMT-CNN for Object Detection and Segmentation in Multi-layer and\n  Multi-spectral Images", "author": "Majedaldein Almahasneh and Adeline Paiement and Xianghua Xie and Jean Aboudarham", "abstract": "  Precisely localising solar Active Regions (AR) from multi-spectral images is\na challenging but important task in understanding solar activity and its\ninfluence on space weather. A main challenge comes from each modality capturing\na different location of the 3D objects, as opposed to typical multi-spectral\nimaging scenarios where all image bands observe the same scene. Thus, we refer\nto this special multi-spectral scenario as multi-layer. We present a multi-task\ndeep learning framework that exploits the dependencies between image bands to\nproduce 3D AR localisation (segmentation and detection) where different image\nbands (and physical locations) have their own set of results. Furthermore, to\naddress the difficulty of producing dense AR annotations for training\nsupervised machine learning (ML) algorithms, we adapt a training strategy based\non weak labels (i.e. bounding boxes) in a recursive manner. We compare our\ndetection and segmentation stages against baseline approaches for solar image\nanalysis (multi-channel coronal hole detection, SPOCA for ARs) and\nstate-of-the-art deep learning methods (Faster RCNN, U-Net). Additionally, both\ndetection a nd segmentation stages are quantitatively validated on artificially\ncreated data of similar spatial configurations made from annotated multi-modal\nmagnetic resonance images. Our framework achieves an average of 0.72 IoU\n(segmentation) and 0.90 F1 score (detection) across all modalities, comparing\nto the best performing baseline methods with scores of 0.53 and 0.58,\nrespectively, on the artificial dataset, and 0.84 F1 score in the AR detection\ntask comparing to baseline of 0.82 F1 score. Our segmentation results are\nqualitatively validated by an expert on real ARs.\n", "link": "http://arxiv.org/abs/2407.14473v1", "date": "2024-07-19", "relevancy": 2.1449, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5419}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5339}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MLMT-CNN%20for%20Object%20Detection%20and%20Segmentation%20in%20Multi-layer%20and%0A%20%20Multi-spectral%20Images&body=Title%3A%20MLMT-CNN%20for%20Object%20Detection%20and%20Segmentation%20in%20Multi-layer%20and%0A%20%20Multi-spectral%20Images%0AAuthor%3A%20Majedaldein%20Almahasneh%20and%20Adeline%20Paiement%20and%20Xianghua%20Xie%20and%20Jean%20Aboudarham%0AAbstract%3A%20%20%20Precisely%20localising%20solar%20Active%20Regions%20%28AR%29%20from%20multi-spectral%20images%20is%0Aa%20challenging%20but%20important%20task%20in%20understanding%20solar%20activity%20and%20its%0Ainfluence%20on%20space%20weather.%20A%20main%20challenge%20comes%20from%20each%20modality%20capturing%0Aa%20different%20location%20of%20the%203D%20objects%2C%20as%20opposed%20to%20typical%20multi-spectral%0Aimaging%20scenarios%20where%20all%20image%20bands%20observe%20the%20same%20scene.%20Thus%2C%20we%20refer%0Ato%20this%20special%20multi-spectral%20scenario%20as%20multi-layer.%20We%20present%20a%20multi-task%0Adeep%20learning%20framework%20that%20exploits%20the%20dependencies%20between%20image%20bands%20to%0Aproduce%203D%20AR%20localisation%20%28segmentation%20and%20detection%29%20where%20different%20image%0Abands%20%28and%20physical%20locations%29%20have%20their%20own%20set%20of%20results.%20Furthermore%2C%20to%0Aaddress%20the%20difficulty%20of%20producing%20dense%20AR%20annotations%20for%20training%0Asupervised%20machine%20learning%20%28ML%29%20algorithms%2C%20we%20adapt%20a%20training%20strategy%20based%0Aon%20weak%20labels%20%28i.e.%20bounding%20boxes%29%20in%20a%20recursive%20manner.%20We%20compare%20our%0Adetection%20and%20segmentation%20stages%20against%20baseline%20approaches%20for%20solar%20image%0Aanalysis%20%28multi-channel%20coronal%20hole%20detection%2C%20SPOCA%20for%20ARs%29%20and%0Astate-of-the-art%20deep%20learning%20methods%20%28Faster%20RCNN%2C%20U-Net%29.%20Additionally%2C%20both%0Adetection%20a%20nd%20segmentation%20stages%20are%20quantitatively%20validated%20on%20artificially%0Acreated%20data%20of%20similar%20spatial%20configurations%20made%20from%20annotated%20multi-modal%0Amagnetic%20resonance%20images.%20Our%20framework%20achieves%20an%20average%20of%200.72%20IoU%0A%28segmentation%29%20and%200.90%20F1%20score%20%28detection%29%20across%20all%20modalities%2C%20comparing%0Ato%20the%20best%20performing%20baseline%20methods%20with%20scores%20of%200.53%20and%200.58%2C%0Arespectively%2C%20on%20the%20artificial%20dataset%2C%20and%200.84%20F1%20score%20in%20the%20AR%20detection%0Atask%20comparing%20to%20baseline%20of%200.82%20F1%20score.%20Our%20segmentation%20results%20are%0Aqualitatively%20validated%20by%20an%20expert%20on%20real%20ARs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMLMT-CNN%2520for%2520Object%2520Detection%2520and%2520Segmentation%2520in%2520Multi-layer%2520and%250A%2520%2520Multi-spectral%2520Images%26entry.906535625%3DMajedaldein%2520Almahasneh%2520and%2520Adeline%2520Paiement%2520and%2520Xianghua%2520Xie%2520and%2520Jean%2520Aboudarham%26entry.1292438233%3D%2520%2520Precisely%2520localising%2520solar%2520Active%2520Regions%2520%2528AR%2529%2520from%2520multi-spectral%2520images%2520is%250Aa%2520challenging%2520but%2520important%2520task%2520in%2520understanding%2520solar%2520activity%2520and%2520its%250Ainfluence%2520on%2520space%2520weather.%2520A%2520main%2520challenge%2520comes%2520from%2520each%2520modality%2520capturing%250Aa%2520different%2520location%2520of%2520the%25203D%2520objects%252C%2520as%2520opposed%2520to%2520typical%2520multi-spectral%250Aimaging%2520scenarios%2520where%2520all%2520image%2520bands%2520observe%2520the%2520same%2520scene.%2520Thus%252C%2520we%2520refer%250Ato%2520this%2520special%2520multi-spectral%2520scenario%2520as%2520multi-layer.%2520We%2520present%2520a%2520multi-task%250Adeep%2520learning%2520framework%2520that%2520exploits%2520the%2520dependencies%2520between%2520image%2520bands%2520to%250Aproduce%25203D%2520AR%2520localisation%2520%2528segmentation%2520and%2520detection%2529%2520where%2520different%2520image%250Abands%2520%2528and%2520physical%2520locations%2529%2520have%2520their%2520own%2520set%2520of%2520results.%2520Furthermore%252C%2520to%250Aaddress%2520the%2520difficulty%2520of%2520producing%2520dense%2520AR%2520annotations%2520for%2520training%250Asupervised%2520machine%2520learning%2520%2528ML%2529%2520algorithms%252C%2520we%2520adapt%2520a%2520training%2520strategy%2520based%250Aon%2520weak%2520labels%2520%2528i.e.%2520bounding%2520boxes%2529%2520in%2520a%2520recursive%2520manner.%2520We%2520compare%2520our%250Adetection%2520and%2520segmentation%2520stages%2520against%2520baseline%2520approaches%2520for%2520solar%2520image%250Aanalysis%2520%2528multi-channel%2520coronal%2520hole%2520detection%252C%2520SPOCA%2520for%2520ARs%2529%2520and%250Astate-of-the-art%2520deep%2520learning%2520methods%2520%2528Faster%2520RCNN%252C%2520U-Net%2529.%2520Additionally%252C%2520both%250Adetection%2520a%2520nd%2520segmentation%2520stages%2520are%2520quantitatively%2520validated%2520on%2520artificially%250Acreated%2520data%2520of%2520similar%2520spatial%2520configurations%2520made%2520from%2520annotated%2520multi-modal%250Amagnetic%2520resonance%2520images.%2520Our%2520framework%2520achieves%2520an%2520average%2520of%25200.72%2520IoU%250A%2528segmentation%2529%2520and%25200.90%2520F1%2520score%2520%2528detection%2529%2520across%2520all%2520modalities%252C%2520comparing%250Ato%2520the%2520best%2520performing%2520baseline%2520methods%2520with%2520scores%2520of%25200.53%2520and%25200.58%252C%250Arespectively%252C%2520on%2520the%2520artificial%2520dataset%252C%2520and%25200.84%2520F1%2520score%2520in%2520the%2520AR%2520detection%250Atask%2520comparing%2520to%2520baseline%2520of%25200.82%2520F1%2520score.%2520Our%2520segmentation%2520results%2520are%250Aqualitatively%2520validated%2520by%2520an%2520expert%2520on%2520real%2520ARs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MLMT-CNN%20for%20Object%20Detection%20and%20Segmentation%20in%20Multi-layer%20and%0A%20%20Multi-spectral%20Images&entry.906535625=Majedaldein%20Almahasneh%20and%20Adeline%20Paiement%20and%20Xianghua%20Xie%20and%20Jean%20Aboudarham&entry.1292438233=%20%20Precisely%20localising%20solar%20Active%20Regions%20%28AR%29%20from%20multi-spectral%20images%20is%0Aa%20challenging%20but%20important%20task%20in%20understanding%20solar%20activity%20and%20its%0Ainfluence%20on%20space%20weather.%20A%20main%20challenge%20comes%20from%20each%20modality%20capturing%0Aa%20different%20location%20of%20the%203D%20objects%2C%20as%20opposed%20to%20typical%20multi-spectral%0Aimaging%20scenarios%20where%20all%20image%20bands%20observe%20the%20same%20scene.%20Thus%2C%20we%20refer%0Ato%20this%20special%20multi-spectral%20scenario%20as%20multi-layer.%20We%20present%20a%20multi-task%0Adeep%20learning%20framework%20that%20exploits%20the%20dependencies%20between%20image%20bands%20to%0Aproduce%203D%20AR%20localisation%20%28segmentation%20and%20detection%29%20where%20different%20image%0Abands%20%28and%20physical%20locations%29%20have%20their%20own%20set%20of%20results.%20Furthermore%2C%20to%0Aaddress%20the%20difficulty%20of%20producing%20dense%20AR%20annotations%20for%20training%0Asupervised%20machine%20learning%20%28ML%29%20algorithms%2C%20we%20adapt%20a%20training%20strategy%20based%0Aon%20weak%20labels%20%28i.e.%20bounding%20boxes%29%20in%20a%20recursive%20manner.%20We%20compare%20our%0Adetection%20and%20segmentation%20stages%20against%20baseline%20approaches%20for%20solar%20image%0Aanalysis%20%28multi-channel%20coronal%20hole%20detection%2C%20SPOCA%20for%20ARs%29%20and%0Astate-of-the-art%20deep%20learning%20methods%20%28Faster%20RCNN%2C%20U-Net%29.%20Additionally%2C%20both%0Adetection%20a%20nd%20segmentation%20stages%20are%20quantitatively%20validated%20on%20artificially%0Acreated%20data%20of%20similar%20spatial%20configurations%20made%20from%20annotated%20multi-modal%0Amagnetic%20resonance%20images.%20Our%20framework%20achieves%20an%20average%20of%200.72%20IoU%0A%28segmentation%29%20and%200.90%20F1%20score%20%28detection%29%20across%20all%20modalities%2C%20comparing%0Ato%20the%20best%20performing%20baseline%20methods%20with%20scores%20of%200.53%20and%200.58%2C%0Arespectively%2C%20on%20the%20artificial%20dataset%2C%20and%200.84%20F1%20score%20in%20the%20AR%20detection%0Atask%20comparing%20to%20baseline%20of%200.82%20F1%20score.%20Our%20segmentation%20results%20are%0Aqualitatively%20validated%20by%20an%20expert%20on%20real%20ARs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14473v1&entry.124074799=Read"},
{"title": "Stochastic Model Predictive Control with Optimal Linear Feedback for\n  Mobile Robots in Dynamic Environments", "author": "Yunfan Gao and Florian Messerer and Niels van Duijkeren and Moritz Diehl", "abstract": "  Robot navigation around humans can be a challenging problem since human\nmovements are hard to predict. Stochastic model predictive control (MPC) can\naccount for such uncertainties and approximately bound the probability of a\ncollision to take place. In this paper, to counteract the rapidly growing human\nmotion uncertainty over time, we incorporate state feedback in the stochastic\nMPC. This allows the robot to more closely track reference trajectories. To\nthis end the feedback policy is left as a degree of freedom in the optimal\ncontrol problem. The stochastic MPC with feedback is validated in simulation\nexperiments and is compared against nominal MPC and stochastic MPC without\nfeedback. The added computation time can be limited by reducing the number of\nadditional variables for the feedback law with a small compromise in control\nperformance.\n", "link": "http://arxiv.org/abs/2407.14220v1", "date": "2024-07-19", "relevancy": 2.1409, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5832}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5313}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.52}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stochastic%20Model%20Predictive%20Control%20with%20Optimal%20Linear%20Feedback%20for%0A%20%20Mobile%20Robots%20in%20Dynamic%20Environments&body=Title%3A%20Stochastic%20Model%20Predictive%20Control%20with%20Optimal%20Linear%20Feedback%20for%0A%20%20Mobile%20Robots%20in%20Dynamic%20Environments%0AAuthor%3A%20Yunfan%20Gao%20and%20Florian%20Messerer%20and%20Niels%20van%20Duijkeren%20and%20Moritz%20Diehl%0AAbstract%3A%20%20%20Robot%20navigation%20around%20humans%20can%20be%20a%20challenging%20problem%20since%20human%0Amovements%20are%20hard%20to%20predict.%20Stochastic%20model%20predictive%20control%20%28MPC%29%20can%0Aaccount%20for%20such%20uncertainties%20and%20approximately%20bound%20the%20probability%20of%20a%0Acollision%20to%20take%20place.%20In%20this%20paper%2C%20to%20counteract%20the%20rapidly%20growing%20human%0Amotion%20uncertainty%20over%20time%2C%20we%20incorporate%20state%20feedback%20in%20the%20stochastic%0AMPC.%20This%20allows%20the%20robot%20to%20more%20closely%20track%20reference%20trajectories.%20To%0Athis%20end%20the%20feedback%20policy%20is%20left%20as%20a%20degree%20of%20freedom%20in%20the%20optimal%0Acontrol%20problem.%20The%20stochastic%20MPC%20with%20feedback%20is%20validated%20in%20simulation%0Aexperiments%20and%20is%20compared%20against%20nominal%20MPC%20and%20stochastic%20MPC%20without%0Afeedback.%20The%20added%20computation%20time%20can%20be%20limited%20by%20reducing%20the%20number%20of%0Aadditional%20variables%20for%20the%20feedback%20law%20with%20a%20small%20compromise%20in%20control%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14220v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStochastic%2520Model%2520Predictive%2520Control%2520with%2520Optimal%2520Linear%2520Feedback%2520for%250A%2520%2520Mobile%2520Robots%2520in%2520Dynamic%2520Environments%26entry.906535625%3DYunfan%2520Gao%2520and%2520Florian%2520Messerer%2520and%2520Niels%2520van%2520Duijkeren%2520and%2520Moritz%2520Diehl%26entry.1292438233%3D%2520%2520Robot%2520navigation%2520around%2520humans%2520can%2520be%2520a%2520challenging%2520problem%2520since%2520human%250Amovements%2520are%2520hard%2520to%2520predict.%2520Stochastic%2520model%2520predictive%2520control%2520%2528MPC%2529%2520can%250Aaccount%2520for%2520such%2520uncertainties%2520and%2520approximately%2520bound%2520the%2520probability%2520of%2520a%250Acollision%2520to%2520take%2520place.%2520In%2520this%2520paper%252C%2520to%2520counteract%2520the%2520rapidly%2520growing%2520human%250Amotion%2520uncertainty%2520over%2520time%252C%2520we%2520incorporate%2520state%2520feedback%2520in%2520the%2520stochastic%250AMPC.%2520This%2520allows%2520the%2520robot%2520to%2520more%2520closely%2520track%2520reference%2520trajectories.%2520To%250Athis%2520end%2520the%2520feedback%2520policy%2520is%2520left%2520as%2520a%2520degree%2520of%2520freedom%2520in%2520the%2520optimal%250Acontrol%2520problem.%2520The%2520stochastic%2520MPC%2520with%2520feedback%2520is%2520validated%2520in%2520simulation%250Aexperiments%2520and%2520is%2520compared%2520against%2520nominal%2520MPC%2520and%2520stochastic%2520MPC%2520without%250Afeedback.%2520The%2520added%2520computation%2520time%2520can%2520be%2520limited%2520by%2520reducing%2520the%2520number%2520of%250Aadditional%2520variables%2520for%2520the%2520feedback%2520law%2520with%2520a%2520small%2520compromise%2520in%2520control%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14220v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stochastic%20Model%20Predictive%20Control%20with%20Optimal%20Linear%20Feedback%20for%0A%20%20Mobile%20Robots%20in%20Dynamic%20Environments&entry.906535625=Yunfan%20Gao%20and%20Florian%20Messerer%20and%20Niels%20van%20Duijkeren%20and%20Moritz%20Diehl&entry.1292438233=%20%20Robot%20navigation%20around%20humans%20can%20be%20a%20challenging%20problem%20since%20human%0Amovements%20are%20hard%20to%20predict.%20Stochastic%20model%20predictive%20control%20%28MPC%29%20can%0Aaccount%20for%20such%20uncertainties%20and%20approximately%20bound%20the%20probability%20of%20a%0Acollision%20to%20take%20place.%20In%20this%20paper%2C%20to%20counteract%20the%20rapidly%20growing%20human%0Amotion%20uncertainty%20over%20time%2C%20we%20incorporate%20state%20feedback%20in%20the%20stochastic%0AMPC.%20This%20allows%20the%20robot%20to%20more%20closely%20track%20reference%20trajectories.%20To%0Athis%20end%20the%20feedback%20policy%20is%20left%20as%20a%20degree%20of%20freedom%20in%20the%20optimal%0Acontrol%20problem.%20The%20stochastic%20MPC%20with%20feedback%20is%20validated%20in%20simulation%0Aexperiments%20and%20is%20compared%20against%20nominal%20MPC%20and%20stochastic%20MPC%20without%0Afeedback.%20The%20added%20computation%20time%20can%20be%20limited%20by%20reducing%20the%20number%20of%0Aadditional%20variables%20for%20the%20feedback%20law%20with%20a%20small%20compromise%20in%20control%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14220v1&entry.124074799=Read"},
{"title": "Learn and Don't Forget: Adding a New Language to ASR Foundation Models", "author": "Mengjie Qian and Siyuan Tang and Rao Ma and Kate M. Knill and Mark J. F. Gales", "abstract": "  Foundation ASR models often support many languages, e.g. 100 languages in\nWhisper. However, there has been limited work on integrating an additional,\ntypically low-resource, language, while maintaining performance on the original\nlanguage set. Fine-tuning, while simple, may degrade the accuracy of the\noriginal set. We compare three approaches that exploit adaptation parameters:\nsoft language code tuning, train only the language code; soft prompt tuning,\ntrain prepended tokens; and LoRA where a small set of additional parameters are\noptimised. Elastic Weight Consolidation (EWC) offers an alternative compromise\nwith the potential to maintain performance in specific target languages.\nResults show that direct fine-tuning yields the best performance for the new\nlanguage but degrades existing language capabilities. EWC can address this\nissue for specific languages. If only adaptation parameters are used, the\nlanguage capabilities are maintained but at the cost of performance in the new\nlanguage.\n", "link": "http://arxiv.org/abs/2407.06800v2", "date": "2024-07-19", "relevancy": 2.1386, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4373}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4319}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learn%20and%20Don%27t%20Forget%3A%20Adding%20a%20New%20Language%20to%20ASR%20Foundation%20Models&body=Title%3A%20Learn%20and%20Don%27t%20Forget%3A%20Adding%20a%20New%20Language%20to%20ASR%20Foundation%20Models%0AAuthor%3A%20Mengjie%20Qian%20and%20Siyuan%20Tang%20and%20Rao%20Ma%20and%20Kate%20M.%20Knill%20and%20Mark%20J.%20F.%20Gales%0AAbstract%3A%20%20%20Foundation%20ASR%20models%20often%20support%20many%20languages%2C%20e.g.%20100%20languages%20in%0AWhisper.%20However%2C%20there%20has%20been%20limited%20work%20on%20integrating%20an%20additional%2C%0Atypically%20low-resource%2C%20language%2C%20while%20maintaining%20performance%20on%20the%20original%0Alanguage%20set.%20Fine-tuning%2C%20while%20simple%2C%20may%20degrade%20the%20accuracy%20of%20the%0Aoriginal%20set.%20We%20compare%20three%20approaches%20that%20exploit%20adaptation%20parameters%3A%0Asoft%20language%20code%20tuning%2C%20train%20only%20the%20language%20code%3B%20soft%20prompt%20tuning%2C%0Atrain%20prepended%20tokens%3B%20and%20LoRA%20where%20a%20small%20set%20of%20additional%20parameters%20are%0Aoptimised.%20Elastic%20Weight%20Consolidation%20%28EWC%29%20offers%20an%20alternative%20compromise%0Awith%20the%20potential%20to%20maintain%20performance%20in%20specific%20target%20languages.%0AResults%20show%20that%20direct%20fine-tuning%20yields%20the%20best%20performance%20for%20the%20new%0Alanguage%20but%20degrades%20existing%20language%20capabilities.%20EWC%20can%20address%20this%0Aissue%20for%20specific%20languages.%20If%20only%20adaptation%20parameters%20are%20used%2C%20the%0Alanguage%20capabilities%20are%20maintained%20but%20at%20the%20cost%20of%20performance%20in%20the%20new%0Alanguage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06800v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearn%2520and%2520Don%2527t%2520Forget%253A%2520Adding%2520a%2520New%2520Language%2520to%2520ASR%2520Foundation%2520Models%26entry.906535625%3DMengjie%2520Qian%2520and%2520Siyuan%2520Tang%2520and%2520Rao%2520Ma%2520and%2520Kate%2520M.%2520Knill%2520and%2520Mark%2520J.%2520F.%2520Gales%26entry.1292438233%3D%2520%2520Foundation%2520ASR%2520models%2520often%2520support%2520many%2520languages%252C%2520e.g.%2520100%2520languages%2520in%250AWhisper.%2520However%252C%2520there%2520has%2520been%2520limited%2520work%2520on%2520integrating%2520an%2520additional%252C%250Atypically%2520low-resource%252C%2520language%252C%2520while%2520maintaining%2520performance%2520on%2520the%2520original%250Alanguage%2520set.%2520Fine-tuning%252C%2520while%2520simple%252C%2520may%2520degrade%2520the%2520accuracy%2520of%2520the%250Aoriginal%2520set.%2520We%2520compare%2520three%2520approaches%2520that%2520exploit%2520adaptation%2520parameters%253A%250Asoft%2520language%2520code%2520tuning%252C%2520train%2520only%2520the%2520language%2520code%253B%2520soft%2520prompt%2520tuning%252C%250Atrain%2520prepended%2520tokens%253B%2520and%2520LoRA%2520where%2520a%2520small%2520set%2520of%2520additional%2520parameters%2520are%250Aoptimised.%2520Elastic%2520Weight%2520Consolidation%2520%2528EWC%2529%2520offers%2520an%2520alternative%2520compromise%250Awith%2520the%2520potential%2520to%2520maintain%2520performance%2520in%2520specific%2520target%2520languages.%250AResults%2520show%2520that%2520direct%2520fine-tuning%2520yields%2520the%2520best%2520performance%2520for%2520the%2520new%250Alanguage%2520but%2520degrades%2520existing%2520language%2520capabilities.%2520EWC%2520can%2520address%2520this%250Aissue%2520for%2520specific%2520languages.%2520If%2520only%2520adaptation%2520parameters%2520are%2520used%252C%2520the%250Alanguage%2520capabilities%2520are%2520maintained%2520but%2520at%2520the%2520cost%2520of%2520performance%2520in%2520the%2520new%250Alanguage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06800v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learn%20and%20Don%27t%20Forget%3A%20Adding%20a%20New%20Language%20to%20ASR%20Foundation%20Models&entry.906535625=Mengjie%20Qian%20and%20Siyuan%20Tang%20and%20Rao%20Ma%20and%20Kate%20M.%20Knill%20and%20Mark%20J.%20F.%20Gales&entry.1292438233=%20%20Foundation%20ASR%20models%20often%20support%20many%20languages%2C%20e.g.%20100%20languages%20in%0AWhisper.%20However%2C%20there%20has%20been%20limited%20work%20on%20integrating%20an%20additional%2C%0Atypically%20low-resource%2C%20language%2C%20while%20maintaining%20performance%20on%20the%20original%0Alanguage%20set.%20Fine-tuning%2C%20while%20simple%2C%20may%20degrade%20the%20accuracy%20of%20the%0Aoriginal%20set.%20We%20compare%20three%20approaches%20that%20exploit%20adaptation%20parameters%3A%0Asoft%20language%20code%20tuning%2C%20train%20only%20the%20language%20code%3B%20soft%20prompt%20tuning%2C%0Atrain%20prepended%20tokens%3B%20and%20LoRA%20where%20a%20small%20set%20of%20additional%20parameters%20are%0Aoptimised.%20Elastic%20Weight%20Consolidation%20%28EWC%29%20offers%20an%20alternative%20compromise%0Awith%20the%20potential%20to%20maintain%20performance%20in%20specific%20target%20languages.%0AResults%20show%20that%20direct%20fine-tuning%20yields%20the%20best%20performance%20for%20the%20new%0Alanguage%20but%20degrades%20existing%20language%20capabilities.%20EWC%20can%20address%20this%0Aissue%20for%20specific%20languages.%20If%20only%20adaptation%20parameters%20are%20used%2C%20the%0Alanguage%20capabilities%20are%20maintained%20but%20at%20the%20cost%20of%20performance%20in%20the%20new%0Alanguage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06800v2&entry.124074799=Read"},
{"title": "Multi-Source and Test-Time Domain Adaptation on Multivariate Signals\n  using Spatio-Temporal Monge Alignment", "author": "Th\u00e9o Gnassounou and Antoine Collas and R\u00e9mi Flamary and Karim Lounici and Alexandre Gramfort", "abstract": "  Machine learning applications on signals such as computer vision or\nbiomedical data often face significant challenges due to the variability that\nexists across hardware devices or session recordings. This variability poses a\nDomain Adaptation (DA) problem, as training and testing data distributions\noften differ. In this work, we propose Spatio-Temporal Monge Alignment (STMA)\nto mitigate these variabilities. This Optimal Transport (OT) based method\nadapts the cross-power spectrum density (cross-PSD) of multivariate signals by\nmapping them to the Wasserstein barycenter of source domains (multi-source DA).\nPredictions for new domains can be done with a filtering without the need for\nretraining a model with source data (test-time DA). We also study and discuss\ntwo special cases of the method, Temporal Monge Alignment (TMA) and Spatial\nMonge Alignment (SMA). Non-asymptotic concentration bounds are derived for the\nmappings estimation, which reveals a bias-plus-variance error structure with a\nvariance decay rate of $\\mathcal{O}(n_\\ell^{-1/2})$ with $n_\\ell$ the signal\nlength. This theoretical guarantee demonstrates the efficiency of the proposed\ncomputational schema. Numerical experiments on multivariate biosignals and\nimage data show that STMA leads to significant and consistent performance gains\nbetween datasets acquired with very different settings. Notably, STMA is a\npre-processing step complementary to state-of-the-art deep learning methods.\n", "link": "http://arxiv.org/abs/2407.14303v1", "date": "2024-07-19", "relevancy": 2.132, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5669}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5131}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4981}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Source%20and%20Test-Time%20Domain%20Adaptation%20on%20Multivariate%20Signals%0A%20%20using%20Spatio-Temporal%20Monge%20Alignment&body=Title%3A%20Multi-Source%20and%20Test-Time%20Domain%20Adaptation%20on%20Multivariate%20Signals%0A%20%20using%20Spatio-Temporal%20Monge%20Alignment%0AAuthor%3A%20Th%C3%A9o%20Gnassounou%20and%20Antoine%20Collas%20and%20R%C3%A9mi%20Flamary%20and%20Karim%20Lounici%20and%20Alexandre%20Gramfort%0AAbstract%3A%20%20%20Machine%20learning%20applications%20on%20signals%20such%20as%20computer%20vision%20or%0Abiomedical%20data%20often%20face%20significant%20challenges%20due%20to%20the%20variability%20that%0Aexists%20across%20hardware%20devices%20or%20session%20recordings.%20This%20variability%20poses%20a%0ADomain%20Adaptation%20%28DA%29%20problem%2C%20as%20training%20and%20testing%20data%20distributions%0Aoften%20differ.%20In%20this%20work%2C%20we%20propose%20Spatio-Temporal%20Monge%20Alignment%20%28STMA%29%0Ato%20mitigate%20these%20variabilities.%20This%20Optimal%20Transport%20%28OT%29%20based%20method%0Aadapts%20the%20cross-power%20spectrum%20density%20%28cross-PSD%29%20of%20multivariate%20signals%20by%0Amapping%20them%20to%20the%20Wasserstein%20barycenter%20of%20source%20domains%20%28multi-source%20DA%29.%0APredictions%20for%20new%20domains%20can%20be%20done%20with%20a%20filtering%20without%20the%20need%20for%0Aretraining%20a%20model%20with%20source%20data%20%28test-time%20DA%29.%20We%20also%20study%20and%20discuss%0Atwo%20special%20cases%20of%20the%20method%2C%20Temporal%20Monge%20Alignment%20%28TMA%29%20and%20Spatial%0AMonge%20Alignment%20%28SMA%29.%20Non-asymptotic%20concentration%20bounds%20are%20derived%20for%20the%0Amappings%20estimation%2C%20which%20reveals%20a%20bias-plus-variance%20error%20structure%20with%20a%0Avariance%20decay%20rate%20of%20%24%5Cmathcal%7BO%7D%28n_%5Cell%5E%7B-1/2%7D%29%24%20with%20%24n_%5Cell%24%20the%20signal%0Alength.%20This%20theoretical%20guarantee%20demonstrates%20the%20efficiency%20of%20the%20proposed%0Acomputational%20schema.%20Numerical%20experiments%20on%20multivariate%20biosignals%20and%0Aimage%20data%20show%20that%20STMA%20leads%20to%20significant%20and%20consistent%20performance%20gains%0Abetween%20datasets%20acquired%20with%20very%20different%20settings.%20Notably%2C%20STMA%20is%20a%0Apre-processing%20step%20complementary%20to%20state-of-the-art%20deep%20learning%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14303v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Source%2520and%2520Test-Time%2520Domain%2520Adaptation%2520on%2520Multivariate%2520Signals%250A%2520%2520using%2520Spatio-Temporal%2520Monge%2520Alignment%26entry.906535625%3DTh%25C3%25A9o%2520Gnassounou%2520and%2520Antoine%2520Collas%2520and%2520R%25C3%25A9mi%2520Flamary%2520and%2520Karim%2520Lounici%2520and%2520Alexandre%2520Gramfort%26entry.1292438233%3D%2520%2520Machine%2520learning%2520applications%2520on%2520signals%2520such%2520as%2520computer%2520vision%2520or%250Abiomedical%2520data%2520often%2520face%2520significant%2520challenges%2520due%2520to%2520the%2520variability%2520that%250Aexists%2520across%2520hardware%2520devices%2520or%2520session%2520recordings.%2520This%2520variability%2520poses%2520a%250ADomain%2520Adaptation%2520%2528DA%2529%2520problem%252C%2520as%2520training%2520and%2520testing%2520data%2520distributions%250Aoften%2520differ.%2520In%2520this%2520work%252C%2520we%2520propose%2520Spatio-Temporal%2520Monge%2520Alignment%2520%2528STMA%2529%250Ato%2520mitigate%2520these%2520variabilities.%2520This%2520Optimal%2520Transport%2520%2528OT%2529%2520based%2520method%250Aadapts%2520the%2520cross-power%2520spectrum%2520density%2520%2528cross-PSD%2529%2520of%2520multivariate%2520signals%2520by%250Amapping%2520them%2520to%2520the%2520Wasserstein%2520barycenter%2520of%2520source%2520domains%2520%2528multi-source%2520DA%2529.%250APredictions%2520for%2520new%2520domains%2520can%2520be%2520done%2520with%2520a%2520filtering%2520without%2520the%2520need%2520for%250Aretraining%2520a%2520model%2520with%2520source%2520data%2520%2528test-time%2520DA%2529.%2520We%2520also%2520study%2520and%2520discuss%250Atwo%2520special%2520cases%2520of%2520the%2520method%252C%2520Temporal%2520Monge%2520Alignment%2520%2528TMA%2529%2520and%2520Spatial%250AMonge%2520Alignment%2520%2528SMA%2529.%2520Non-asymptotic%2520concentration%2520bounds%2520are%2520derived%2520for%2520the%250Amappings%2520estimation%252C%2520which%2520reveals%2520a%2520bias-plus-variance%2520error%2520structure%2520with%2520a%250Avariance%2520decay%2520rate%2520of%2520%2524%255Cmathcal%257BO%257D%2528n_%255Cell%255E%257B-1/2%257D%2529%2524%2520with%2520%2524n_%255Cell%2524%2520the%2520signal%250Alength.%2520This%2520theoretical%2520guarantee%2520demonstrates%2520the%2520efficiency%2520of%2520the%2520proposed%250Acomputational%2520schema.%2520Numerical%2520experiments%2520on%2520multivariate%2520biosignals%2520and%250Aimage%2520data%2520show%2520that%2520STMA%2520leads%2520to%2520significant%2520and%2520consistent%2520performance%2520gains%250Abetween%2520datasets%2520acquired%2520with%2520very%2520different%2520settings.%2520Notably%252C%2520STMA%2520is%2520a%250Apre-processing%2520step%2520complementary%2520to%2520state-of-the-art%2520deep%2520learning%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14303v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Source%20and%20Test-Time%20Domain%20Adaptation%20on%20Multivariate%20Signals%0A%20%20using%20Spatio-Temporal%20Monge%20Alignment&entry.906535625=Th%C3%A9o%20Gnassounou%20and%20Antoine%20Collas%20and%20R%C3%A9mi%20Flamary%20and%20Karim%20Lounici%20and%20Alexandre%20Gramfort&entry.1292438233=%20%20Machine%20learning%20applications%20on%20signals%20such%20as%20computer%20vision%20or%0Abiomedical%20data%20often%20face%20significant%20challenges%20due%20to%20the%20variability%20that%0Aexists%20across%20hardware%20devices%20or%20session%20recordings.%20This%20variability%20poses%20a%0ADomain%20Adaptation%20%28DA%29%20problem%2C%20as%20training%20and%20testing%20data%20distributions%0Aoften%20differ.%20In%20this%20work%2C%20we%20propose%20Spatio-Temporal%20Monge%20Alignment%20%28STMA%29%0Ato%20mitigate%20these%20variabilities.%20This%20Optimal%20Transport%20%28OT%29%20based%20method%0Aadapts%20the%20cross-power%20spectrum%20density%20%28cross-PSD%29%20of%20multivariate%20signals%20by%0Amapping%20them%20to%20the%20Wasserstein%20barycenter%20of%20source%20domains%20%28multi-source%20DA%29.%0APredictions%20for%20new%20domains%20can%20be%20done%20with%20a%20filtering%20without%20the%20need%20for%0Aretraining%20a%20model%20with%20source%20data%20%28test-time%20DA%29.%20We%20also%20study%20and%20discuss%0Atwo%20special%20cases%20of%20the%20method%2C%20Temporal%20Monge%20Alignment%20%28TMA%29%20and%20Spatial%0AMonge%20Alignment%20%28SMA%29.%20Non-asymptotic%20concentration%20bounds%20are%20derived%20for%20the%0Amappings%20estimation%2C%20which%20reveals%20a%20bias-plus-variance%20error%20structure%20with%20a%0Avariance%20decay%20rate%20of%20%24%5Cmathcal%7BO%7D%28n_%5Cell%5E%7B-1/2%7D%29%24%20with%20%24n_%5Cell%24%20the%20signal%0Alength.%20This%20theoretical%20guarantee%20demonstrates%20the%20efficiency%20of%20the%20proposed%0Acomputational%20schema.%20Numerical%20experiments%20on%20multivariate%20biosignals%20and%0Aimage%20data%20show%20that%20STMA%20leads%20to%20significant%20and%20consistent%20performance%20gains%0Abetween%20datasets%20acquired%20with%20very%20different%20settings.%20Notably%2C%20STMA%20is%20a%0Apre-processing%20step%20complementary%20to%20state-of-the-art%20deep%20learning%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14303v1&entry.124074799=Read"},
{"title": "Words2Contact: Identifying Support Contacts from Verbal Instructions\n  Using Foundation Models", "author": "Dionis Totsila and Quentin Rouxel and Jean-Baptiste Mouret and Serena Ivaldi", "abstract": "  This paper presents Words2Contact, a language-guided multi-contact placement\npipeline leveraging large language models and vision language models. Our\nmethod is a key component for language-assisted teleoperation and human-robot\ncooperation, where human operators can instruct the robots where to place their\nsupport contacts before whole-body reaching or manipulation using natural\nlanguage. Words2Contact transforms the verbal instructions of a human operator\ninto contact placement predictions; it also deals with iterative corrections,\nuntil the human is satisfied with the contact location identified in the\nrobot's field of view. We benchmark state-of-the-art LLMs and VLMs for size and\nperformance in contact prediction. We demonstrate the effectiveness of the\niterative correction process, showing that users, even naive, quickly learn how\nto instruct the system to obtain accurate locations. Finally, we validate\nWords2Contact in real-world experiments with the Talos humanoid robot,\ninstructed by human operators to place support contacts on different locations\nand surfaces to avoid falling when reaching for distant objects.\n", "link": "http://arxiv.org/abs/2407.14229v1", "date": "2024-07-19", "relevancy": 2.1308, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.572}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5283}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Words2Contact%3A%20Identifying%20Support%20Contacts%20from%20Verbal%20Instructions%0A%20%20Using%20Foundation%20Models&body=Title%3A%20Words2Contact%3A%20Identifying%20Support%20Contacts%20from%20Verbal%20Instructions%0A%20%20Using%20Foundation%20Models%0AAuthor%3A%20Dionis%20Totsila%20and%20Quentin%20Rouxel%20and%20Jean-Baptiste%20Mouret%20and%20Serena%20Ivaldi%0AAbstract%3A%20%20%20This%20paper%20presents%20Words2Contact%2C%20a%20language-guided%20multi-contact%20placement%0Apipeline%20leveraging%20large%20language%20models%20and%20vision%20language%20models.%20Our%0Amethod%20is%20a%20key%20component%20for%20language-assisted%20teleoperation%20and%20human-robot%0Acooperation%2C%20where%20human%20operators%20can%20instruct%20the%20robots%20where%20to%20place%20their%0Asupport%20contacts%20before%20whole-body%20reaching%20or%20manipulation%20using%20natural%0Alanguage.%20Words2Contact%20transforms%20the%20verbal%20instructions%20of%20a%20human%20operator%0Ainto%20contact%20placement%20predictions%3B%20it%20also%20deals%20with%20iterative%20corrections%2C%0Auntil%20the%20human%20is%20satisfied%20with%20the%20contact%20location%20identified%20in%20the%0Arobot%27s%20field%20of%20view.%20We%20benchmark%20state-of-the-art%20LLMs%20and%20VLMs%20for%20size%20and%0Aperformance%20in%20contact%20prediction.%20We%20demonstrate%20the%20effectiveness%20of%20the%0Aiterative%20correction%20process%2C%20showing%20that%20users%2C%20even%20naive%2C%20quickly%20learn%20how%0Ato%20instruct%20the%20system%20to%20obtain%20accurate%20locations.%20Finally%2C%20we%20validate%0AWords2Contact%20in%20real-world%20experiments%20with%20the%20Talos%20humanoid%20robot%2C%0Ainstructed%20by%20human%20operators%20to%20place%20support%20contacts%20on%20different%20locations%0Aand%20surfaces%20to%20avoid%20falling%20when%20reaching%20for%20distant%20objects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14229v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWords2Contact%253A%2520Identifying%2520Support%2520Contacts%2520from%2520Verbal%2520Instructions%250A%2520%2520Using%2520Foundation%2520Models%26entry.906535625%3DDionis%2520Totsila%2520and%2520Quentin%2520Rouxel%2520and%2520Jean-Baptiste%2520Mouret%2520and%2520Serena%2520Ivaldi%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520Words2Contact%252C%2520a%2520language-guided%2520multi-contact%2520placement%250Apipeline%2520leveraging%2520large%2520language%2520models%2520and%2520vision%2520language%2520models.%2520Our%250Amethod%2520is%2520a%2520key%2520component%2520for%2520language-assisted%2520teleoperation%2520and%2520human-robot%250Acooperation%252C%2520where%2520human%2520operators%2520can%2520instruct%2520the%2520robots%2520where%2520to%2520place%2520their%250Asupport%2520contacts%2520before%2520whole-body%2520reaching%2520or%2520manipulation%2520using%2520natural%250Alanguage.%2520Words2Contact%2520transforms%2520the%2520verbal%2520instructions%2520of%2520a%2520human%2520operator%250Ainto%2520contact%2520placement%2520predictions%253B%2520it%2520also%2520deals%2520with%2520iterative%2520corrections%252C%250Auntil%2520the%2520human%2520is%2520satisfied%2520with%2520the%2520contact%2520location%2520identified%2520in%2520the%250Arobot%2527s%2520field%2520of%2520view.%2520We%2520benchmark%2520state-of-the-art%2520LLMs%2520and%2520VLMs%2520for%2520size%2520and%250Aperformance%2520in%2520contact%2520prediction.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520the%250Aiterative%2520correction%2520process%252C%2520showing%2520that%2520users%252C%2520even%2520naive%252C%2520quickly%2520learn%2520how%250Ato%2520instruct%2520the%2520system%2520to%2520obtain%2520accurate%2520locations.%2520Finally%252C%2520we%2520validate%250AWords2Contact%2520in%2520real-world%2520experiments%2520with%2520the%2520Talos%2520humanoid%2520robot%252C%250Ainstructed%2520by%2520human%2520operators%2520to%2520place%2520support%2520contacts%2520on%2520different%2520locations%250Aand%2520surfaces%2520to%2520avoid%2520falling%2520when%2520reaching%2520for%2520distant%2520objects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14229v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Words2Contact%3A%20Identifying%20Support%20Contacts%20from%20Verbal%20Instructions%0A%20%20Using%20Foundation%20Models&entry.906535625=Dionis%20Totsila%20and%20Quentin%20Rouxel%20and%20Jean-Baptiste%20Mouret%20and%20Serena%20Ivaldi&entry.1292438233=%20%20This%20paper%20presents%20Words2Contact%2C%20a%20language-guided%20multi-contact%20placement%0Apipeline%20leveraging%20large%20language%20models%20and%20vision%20language%20models.%20Our%0Amethod%20is%20a%20key%20component%20for%20language-assisted%20teleoperation%20and%20human-robot%0Acooperation%2C%20where%20human%20operators%20can%20instruct%20the%20robots%20where%20to%20place%20their%0Asupport%20contacts%20before%20whole-body%20reaching%20or%20manipulation%20using%20natural%0Alanguage.%20Words2Contact%20transforms%20the%20verbal%20instructions%20of%20a%20human%20operator%0Ainto%20contact%20placement%20predictions%3B%20it%20also%20deals%20with%20iterative%20corrections%2C%0Auntil%20the%20human%20is%20satisfied%20with%20the%20contact%20location%20identified%20in%20the%0Arobot%27s%20field%20of%20view.%20We%20benchmark%20state-of-the-art%20LLMs%20and%20VLMs%20for%20size%20and%0Aperformance%20in%20contact%20prediction.%20We%20demonstrate%20the%20effectiveness%20of%20the%0Aiterative%20correction%20process%2C%20showing%20that%20users%2C%20even%20naive%2C%20quickly%20learn%20how%0Ato%20instruct%20the%20system%20to%20obtain%20accurate%20locations.%20Finally%2C%20we%20validate%0AWords2Contact%20in%20real-world%20experiments%20with%20the%20Talos%20humanoid%20robot%2C%0Ainstructed%20by%20human%20operators%20to%20place%20support%20contacts%20on%20different%20locations%0Aand%20surfaces%20to%20avoid%20falling%20when%20reaching%20for%20distant%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14229v1&entry.124074799=Read"},
{"title": "Multi-Attribute Vision Transformers are Efficient and Robust Learners", "author": "Hanan Gani and Nada Saadi and Noor Hussein and Karthik Nandakumar", "abstract": "  Since their inception, Vision Transformers (ViTs) have emerged as a\ncompelling alternative to Convolutional Neural Networks (CNNs) across a wide\nspectrum of tasks. ViTs exhibit notable characteristics, including global\nattention, resilience against occlusions, and adaptability to distribution\nshifts. One underexplored aspect of ViTs is their potential for multi-attribute\nlearning, referring to their ability to simultaneously grasp multiple\nattribute-related tasks. In this paper, we delve into the multi-attribute\nlearning capability of ViTs, presenting a straightforward yet effective\nstrategy for training various attributes through a single ViT network as\ndistinct tasks. We assess the resilience of multi-attribute ViTs against\nadversarial attacks and compare their performance against ViTs designed for\nsingle attributes. Moreover, we further evaluate the robustness of\nmulti-attribute ViTs against a recent transformer based attack called\nPatch-Fool. Our empirical findings on the CelebA dataset provide validation for\nour assertion. Our code is available at https://github.com/hananshafi/MTL-ViT\n", "link": "http://arxiv.org/abs/2402.08070v2", "date": "2024-07-19", "relevancy": 2.1251, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5707}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5196}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Attribute%20Vision%20Transformers%20are%20Efficient%20and%20Robust%20Learners&body=Title%3A%20Multi-Attribute%20Vision%20Transformers%20are%20Efficient%20and%20Robust%20Learners%0AAuthor%3A%20Hanan%20Gani%20and%20Nada%20Saadi%20and%20Noor%20Hussein%20and%20Karthik%20Nandakumar%0AAbstract%3A%20%20%20Since%20their%20inception%2C%20Vision%20Transformers%20%28ViTs%29%20have%20emerged%20as%20a%0Acompelling%20alternative%20to%20Convolutional%20Neural%20Networks%20%28CNNs%29%20across%20a%20wide%0Aspectrum%20of%20tasks.%20ViTs%20exhibit%20notable%20characteristics%2C%20including%20global%0Aattention%2C%20resilience%20against%20occlusions%2C%20and%20adaptability%20to%20distribution%0Ashifts.%20One%20underexplored%20aspect%20of%20ViTs%20is%20their%20potential%20for%20multi-attribute%0Alearning%2C%20referring%20to%20their%20ability%20to%20simultaneously%20grasp%20multiple%0Aattribute-related%20tasks.%20In%20this%20paper%2C%20we%20delve%20into%20the%20multi-attribute%0Alearning%20capability%20of%20ViTs%2C%20presenting%20a%20straightforward%20yet%20effective%0Astrategy%20for%20training%20various%20attributes%20through%20a%20single%20ViT%20network%20as%0Adistinct%20tasks.%20We%20assess%20the%20resilience%20of%20multi-attribute%20ViTs%20against%0Aadversarial%20attacks%20and%20compare%20their%20performance%20against%20ViTs%20designed%20for%0Asingle%20attributes.%20Moreover%2C%20we%20further%20evaluate%20the%20robustness%20of%0Amulti-attribute%20ViTs%20against%20a%20recent%20transformer%20based%20attack%20called%0APatch-Fool.%20Our%20empirical%20findings%20on%20the%20CelebA%20dataset%20provide%20validation%20for%0Aour%20assertion.%20Our%20code%20is%20available%20at%20https%3A//github.com/hananshafi/MTL-ViT%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.08070v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Attribute%2520Vision%2520Transformers%2520are%2520Efficient%2520and%2520Robust%2520Learners%26entry.906535625%3DHanan%2520Gani%2520and%2520Nada%2520Saadi%2520and%2520Noor%2520Hussein%2520and%2520Karthik%2520Nandakumar%26entry.1292438233%3D%2520%2520Since%2520their%2520inception%252C%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520emerged%2520as%2520a%250Acompelling%2520alternative%2520to%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520across%2520a%2520wide%250Aspectrum%2520of%2520tasks.%2520ViTs%2520exhibit%2520notable%2520characteristics%252C%2520including%2520global%250Aattention%252C%2520resilience%2520against%2520occlusions%252C%2520and%2520adaptability%2520to%2520distribution%250Ashifts.%2520One%2520underexplored%2520aspect%2520of%2520ViTs%2520is%2520their%2520potential%2520for%2520multi-attribute%250Alearning%252C%2520referring%2520to%2520their%2520ability%2520to%2520simultaneously%2520grasp%2520multiple%250Aattribute-related%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520delve%2520into%2520the%2520multi-attribute%250Alearning%2520capability%2520of%2520ViTs%252C%2520presenting%2520a%2520straightforward%2520yet%2520effective%250Astrategy%2520for%2520training%2520various%2520attributes%2520through%2520a%2520single%2520ViT%2520network%2520as%250Adistinct%2520tasks.%2520We%2520assess%2520the%2520resilience%2520of%2520multi-attribute%2520ViTs%2520against%250Aadversarial%2520attacks%2520and%2520compare%2520their%2520performance%2520against%2520ViTs%2520designed%2520for%250Asingle%2520attributes.%2520Moreover%252C%2520we%2520further%2520evaluate%2520the%2520robustness%2520of%250Amulti-attribute%2520ViTs%2520against%2520a%2520recent%2520transformer%2520based%2520attack%2520called%250APatch-Fool.%2520Our%2520empirical%2520findings%2520on%2520the%2520CelebA%2520dataset%2520provide%2520validation%2520for%250Aour%2520assertion.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/hananshafi/MTL-ViT%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.08070v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Attribute%20Vision%20Transformers%20are%20Efficient%20and%20Robust%20Learners&entry.906535625=Hanan%20Gani%20and%20Nada%20Saadi%20and%20Noor%20Hussein%20and%20Karthik%20Nandakumar&entry.1292438233=%20%20Since%20their%20inception%2C%20Vision%20Transformers%20%28ViTs%29%20have%20emerged%20as%20a%0Acompelling%20alternative%20to%20Convolutional%20Neural%20Networks%20%28CNNs%29%20across%20a%20wide%0Aspectrum%20of%20tasks.%20ViTs%20exhibit%20notable%20characteristics%2C%20including%20global%0Aattention%2C%20resilience%20against%20occlusions%2C%20and%20adaptability%20to%20distribution%0Ashifts.%20One%20underexplored%20aspect%20of%20ViTs%20is%20their%20potential%20for%20multi-attribute%0Alearning%2C%20referring%20to%20their%20ability%20to%20simultaneously%20grasp%20multiple%0Aattribute-related%20tasks.%20In%20this%20paper%2C%20we%20delve%20into%20the%20multi-attribute%0Alearning%20capability%20of%20ViTs%2C%20presenting%20a%20straightforward%20yet%20effective%0Astrategy%20for%20training%20various%20attributes%20through%20a%20single%20ViT%20network%20as%0Adistinct%20tasks.%20We%20assess%20the%20resilience%20of%20multi-attribute%20ViTs%20against%0Aadversarial%20attacks%20and%20compare%20their%20performance%20against%20ViTs%20designed%20for%0Asingle%20attributes.%20Moreover%2C%20we%20further%20evaluate%20the%20robustness%20of%0Amulti-attribute%20ViTs%20against%20a%20recent%20transformer%20based%20attack%20called%0APatch-Fool.%20Our%20empirical%20findings%20on%20the%20CelebA%20dataset%20provide%20validation%20for%0Aour%20assertion.%20Our%20code%20is%20available%20at%20https%3A//github.com/hananshafi/MTL-ViT%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08070v2&entry.124074799=Read"},
{"title": "From Principles to Practices: Lessons Learned from Applying Partnership\n  on AI's (PAI) Synthetic Media Framework to 11 Use Cases", "author": "Claire R. Leibowicz and Christian H. Cardona", "abstract": "  2023 was the year the world woke up to generative AI, and 2024 is the year\npolicymakers are responding more firmly. Importantly, this policy momentum is\ntaking place alongside real world creation and distribution of synthetic media.\nSocial media platforms, news organizations, dating apps, image generation\ncompanies, and more are already navigating a world of AI-generated visuals and\nsounds, already changing hearts and minds, as policymakers try to catch up.\nHow, then, can AI governance capture the complexity of the synthetic media\nlandscape? How can it attend to synthetic media's myriad uses, ranging from\nstorytelling to privacy preservation, to deception, fraud, and defamation,\ntaking into account the many stakeholders involved in its development,\ncreation, and distribution? And what might it mean to govern synthetic media in\na manner that upholds the truth while bolstering freedom of expression? What\nfollows is the first known collection of diverse examples of the implementation\nof synthetic media governance that responds to these questions, specifically\nthrough Partnership on AI's (PAI) Responsible Practices for Synthetic Media - a\nvoluntary, normative Framework for creating, distributing, and building\ntechnology for synthetic media responsibly, launched in February 2023. In this\npaper, we present a case bank of real world examples that help operationalize\nthe Framework - highlighting areas synthetic media governance can be applied,\naugmented, expanded, and refined for use, in practice. Read together, the cases\nemphasize distinct elements of AI policymaking and seven emergent best\npractices supporting transparency, safety, expression, and digital dignity\nonline: consent, disclosure, and differentiation between harmful and creative\nuse cases.\n", "link": "http://arxiv.org/abs/2407.13025v2", "date": "2024-07-19", "relevancy": 2.1164, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4363}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4271}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Principles%20to%20Practices%3A%20Lessons%20Learned%20from%20Applying%20Partnership%0A%20%20on%20AI%27s%20%28PAI%29%20Synthetic%20Media%20Framework%20to%2011%20Use%20Cases&body=Title%3A%20From%20Principles%20to%20Practices%3A%20Lessons%20Learned%20from%20Applying%20Partnership%0A%20%20on%20AI%27s%20%28PAI%29%20Synthetic%20Media%20Framework%20to%2011%20Use%20Cases%0AAuthor%3A%20Claire%20R.%20Leibowicz%20and%20Christian%20H.%20Cardona%0AAbstract%3A%20%20%202023%20was%20the%20year%20the%20world%20woke%20up%20to%20generative%20AI%2C%20and%202024%20is%20the%20year%0Apolicymakers%20are%20responding%20more%20firmly.%20Importantly%2C%20this%20policy%20momentum%20is%0Ataking%20place%20alongside%20real%20world%20creation%20and%20distribution%20of%20synthetic%20media.%0ASocial%20media%20platforms%2C%20news%20organizations%2C%20dating%20apps%2C%20image%20generation%0Acompanies%2C%20and%20more%20are%20already%20navigating%20a%20world%20of%20AI-generated%20visuals%20and%0Asounds%2C%20already%20changing%20hearts%20and%20minds%2C%20as%20policymakers%20try%20to%20catch%20up.%0AHow%2C%20then%2C%20can%20AI%20governance%20capture%20the%20complexity%20of%20the%20synthetic%20media%0Alandscape%3F%20How%20can%20it%20attend%20to%20synthetic%20media%27s%20myriad%20uses%2C%20ranging%20from%0Astorytelling%20to%20privacy%20preservation%2C%20to%20deception%2C%20fraud%2C%20and%20defamation%2C%0Ataking%20into%20account%20the%20many%20stakeholders%20involved%20in%20its%20development%2C%0Acreation%2C%20and%20distribution%3F%20And%20what%20might%20it%20mean%20to%20govern%20synthetic%20media%20in%0Aa%20manner%20that%20upholds%20the%20truth%20while%20bolstering%20freedom%20of%20expression%3F%20What%0Afollows%20is%20the%20first%20known%20collection%20of%20diverse%20examples%20of%20the%20implementation%0Aof%20synthetic%20media%20governance%20that%20responds%20to%20these%20questions%2C%20specifically%0Athrough%20Partnership%20on%20AI%27s%20%28PAI%29%20Responsible%20Practices%20for%20Synthetic%20Media%20-%20a%0Avoluntary%2C%20normative%20Framework%20for%20creating%2C%20distributing%2C%20and%20building%0Atechnology%20for%20synthetic%20media%20responsibly%2C%20launched%20in%20February%202023.%20In%20this%0Apaper%2C%20we%20present%20a%20case%20bank%20of%20real%20world%20examples%20that%20help%20operationalize%0Athe%20Framework%20-%20highlighting%20areas%20synthetic%20media%20governance%20can%20be%20applied%2C%0Aaugmented%2C%20expanded%2C%20and%20refined%20for%20use%2C%20in%20practice.%20Read%20together%2C%20the%20cases%0Aemphasize%20distinct%20elements%20of%20AI%20policymaking%20and%20seven%20emergent%20best%0Apractices%20supporting%20transparency%2C%20safety%2C%20expression%2C%20and%20digital%20dignity%0Aonline%3A%20consent%2C%20disclosure%2C%20and%20differentiation%20between%20harmful%20and%20creative%0Ause%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13025v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Principles%2520to%2520Practices%253A%2520Lessons%2520Learned%2520from%2520Applying%2520Partnership%250A%2520%2520on%2520AI%2527s%2520%2528PAI%2529%2520Synthetic%2520Media%2520Framework%2520to%252011%2520Use%2520Cases%26entry.906535625%3DClaire%2520R.%2520Leibowicz%2520and%2520Christian%2520H.%2520Cardona%26entry.1292438233%3D%2520%25202023%2520was%2520the%2520year%2520the%2520world%2520woke%2520up%2520to%2520generative%2520AI%252C%2520and%25202024%2520is%2520the%2520year%250Apolicymakers%2520are%2520responding%2520more%2520firmly.%2520Importantly%252C%2520this%2520policy%2520momentum%2520is%250Ataking%2520place%2520alongside%2520real%2520world%2520creation%2520and%2520distribution%2520of%2520synthetic%2520media.%250ASocial%2520media%2520platforms%252C%2520news%2520organizations%252C%2520dating%2520apps%252C%2520image%2520generation%250Acompanies%252C%2520and%2520more%2520are%2520already%2520navigating%2520a%2520world%2520of%2520AI-generated%2520visuals%2520and%250Asounds%252C%2520already%2520changing%2520hearts%2520and%2520minds%252C%2520as%2520policymakers%2520try%2520to%2520catch%2520up.%250AHow%252C%2520then%252C%2520can%2520AI%2520governance%2520capture%2520the%2520complexity%2520of%2520the%2520synthetic%2520media%250Alandscape%253F%2520How%2520can%2520it%2520attend%2520to%2520synthetic%2520media%2527s%2520myriad%2520uses%252C%2520ranging%2520from%250Astorytelling%2520to%2520privacy%2520preservation%252C%2520to%2520deception%252C%2520fraud%252C%2520and%2520defamation%252C%250Ataking%2520into%2520account%2520the%2520many%2520stakeholders%2520involved%2520in%2520its%2520development%252C%250Acreation%252C%2520and%2520distribution%253F%2520And%2520what%2520might%2520it%2520mean%2520to%2520govern%2520synthetic%2520media%2520in%250Aa%2520manner%2520that%2520upholds%2520the%2520truth%2520while%2520bolstering%2520freedom%2520of%2520expression%253F%2520What%250Afollows%2520is%2520the%2520first%2520known%2520collection%2520of%2520diverse%2520examples%2520of%2520the%2520implementation%250Aof%2520synthetic%2520media%2520governance%2520that%2520responds%2520to%2520these%2520questions%252C%2520specifically%250Athrough%2520Partnership%2520on%2520AI%2527s%2520%2528PAI%2529%2520Responsible%2520Practices%2520for%2520Synthetic%2520Media%2520-%2520a%250Avoluntary%252C%2520normative%2520Framework%2520for%2520creating%252C%2520distributing%252C%2520and%2520building%250Atechnology%2520for%2520synthetic%2520media%2520responsibly%252C%2520launched%2520in%2520February%25202023.%2520In%2520this%250Apaper%252C%2520we%2520present%2520a%2520case%2520bank%2520of%2520real%2520world%2520examples%2520that%2520help%2520operationalize%250Athe%2520Framework%2520-%2520highlighting%2520areas%2520synthetic%2520media%2520governance%2520can%2520be%2520applied%252C%250Aaugmented%252C%2520expanded%252C%2520and%2520refined%2520for%2520use%252C%2520in%2520practice.%2520Read%2520together%252C%2520the%2520cases%250Aemphasize%2520distinct%2520elements%2520of%2520AI%2520policymaking%2520and%2520seven%2520emergent%2520best%250Apractices%2520supporting%2520transparency%252C%2520safety%252C%2520expression%252C%2520and%2520digital%2520dignity%250Aonline%253A%2520consent%252C%2520disclosure%252C%2520and%2520differentiation%2520between%2520harmful%2520and%2520creative%250Ause%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13025v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Principles%20to%20Practices%3A%20Lessons%20Learned%20from%20Applying%20Partnership%0A%20%20on%20AI%27s%20%28PAI%29%20Synthetic%20Media%20Framework%20to%2011%20Use%20Cases&entry.906535625=Claire%20R.%20Leibowicz%20and%20Christian%20H.%20Cardona&entry.1292438233=%20%202023%20was%20the%20year%20the%20world%20woke%20up%20to%20generative%20AI%2C%20and%202024%20is%20the%20year%0Apolicymakers%20are%20responding%20more%20firmly.%20Importantly%2C%20this%20policy%20momentum%20is%0Ataking%20place%20alongside%20real%20world%20creation%20and%20distribution%20of%20synthetic%20media.%0ASocial%20media%20platforms%2C%20news%20organizations%2C%20dating%20apps%2C%20image%20generation%0Acompanies%2C%20and%20more%20are%20already%20navigating%20a%20world%20of%20AI-generated%20visuals%20and%0Asounds%2C%20already%20changing%20hearts%20and%20minds%2C%20as%20policymakers%20try%20to%20catch%20up.%0AHow%2C%20then%2C%20can%20AI%20governance%20capture%20the%20complexity%20of%20the%20synthetic%20media%0Alandscape%3F%20How%20can%20it%20attend%20to%20synthetic%20media%27s%20myriad%20uses%2C%20ranging%20from%0Astorytelling%20to%20privacy%20preservation%2C%20to%20deception%2C%20fraud%2C%20and%20defamation%2C%0Ataking%20into%20account%20the%20many%20stakeholders%20involved%20in%20its%20development%2C%0Acreation%2C%20and%20distribution%3F%20And%20what%20might%20it%20mean%20to%20govern%20synthetic%20media%20in%0Aa%20manner%20that%20upholds%20the%20truth%20while%20bolstering%20freedom%20of%20expression%3F%20What%0Afollows%20is%20the%20first%20known%20collection%20of%20diverse%20examples%20of%20the%20implementation%0Aof%20synthetic%20media%20governance%20that%20responds%20to%20these%20questions%2C%20specifically%0Athrough%20Partnership%20on%20AI%27s%20%28PAI%29%20Responsible%20Practices%20for%20Synthetic%20Media%20-%20a%0Avoluntary%2C%20normative%20Framework%20for%20creating%2C%20distributing%2C%20and%20building%0Atechnology%20for%20synthetic%20media%20responsibly%2C%20launched%20in%20February%202023.%20In%20this%0Apaper%2C%20we%20present%20a%20case%20bank%20of%20real%20world%20examples%20that%20help%20operationalize%0Athe%20Framework%20-%20highlighting%20areas%20synthetic%20media%20governance%20can%20be%20applied%2C%0Aaugmented%2C%20expanded%2C%20and%20refined%20for%20use%2C%20in%20practice.%20Read%20together%2C%20the%20cases%0Aemphasize%20distinct%20elements%20of%20AI%20policymaking%20and%20seven%20emergent%20best%0Apractices%20supporting%20transparency%2C%20safety%2C%20expression%2C%20and%20digital%20dignity%0Aonline%3A%20consent%2C%20disclosure%2C%20and%20differentiation%20between%20harmful%20and%20creative%0Ause%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13025v2&entry.124074799=Read"},
{"title": "Vision-Based Power Line Cables and Pylons Detection for Low Flying\n  Aircrafts", "author": "Jakub Gwizda\u0142a and Doruk Oner and Soumava Kumar Roy and Mian Akbar Shah and Ad Eberhard and Ivan Egorov and Philipp Kr\u00fcsi and Grigory Yakushev", "abstract": "  Power lines are dangerous for low-flying aircrafts, especially in\nlow-visibility conditions. Thus, a vision-based system able to analyze the\naircraft's surroundings and to provide the pilots with a \"second pair of eyes\"\ncan contribute to enhancing their safety. To this end, we have developed a deep\nlearning approach to jointly detect power line cables and pylons from images\ncaptured at distances of several hundred meters by aircraft-mounted cameras. In\ndoing so, we have combined a modern convolutional architecture with transfer\nlearning and a loss function adapted to curvilinear structure delineation. We\nuse a single network for both detection tasks and demonstrated its performance\non two benchmarking datasets. We have integrated it within an onboard system\nand run it in flight, and have demonstrated with our experiments that it\noutperforms the prior distant cable detection method on both datasets, while\nalso successfully detecting pylons, given their annotations are available for\nthe data.\n", "link": "http://arxiv.org/abs/2407.14352v1", "date": "2024-07-19", "relevancy": 2.0987, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5347}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5296}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-Based%20Power%20Line%20Cables%20and%20Pylons%20Detection%20for%20Low%20Flying%0A%20%20Aircrafts&body=Title%3A%20Vision-Based%20Power%20Line%20Cables%20and%20Pylons%20Detection%20for%20Low%20Flying%0A%20%20Aircrafts%0AAuthor%3A%20Jakub%20Gwizda%C5%82a%20and%20Doruk%20Oner%20and%20Soumava%20Kumar%20Roy%20and%20Mian%20Akbar%20Shah%20and%20Ad%20Eberhard%20and%20Ivan%20Egorov%20and%20Philipp%20Kr%C3%BCsi%20and%20Grigory%20Yakushev%0AAbstract%3A%20%20%20Power%20lines%20are%20dangerous%20for%20low-flying%20aircrafts%2C%20especially%20in%0Alow-visibility%20conditions.%20Thus%2C%20a%20vision-based%20system%20able%20to%20analyze%20the%0Aaircraft%27s%20surroundings%20and%20to%20provide%20the%20pilots%20with%20a%20%22second%20pair%20of%20eyes%22%0Acan%20contribute%20to%20enhancing%20their%20safety.%20To%20this%20end%2C%20we%20have%20developed%20a%20deep%0Alearning%20approach%20to%20jointly%20detect%20power%20line%20cables%20and%20pylons%20from%20images%0Acaptured%20at%20distances%20of%20several%20hundred%20meters%20by%20aircraft-mounted%20cameras.%20In%0Adoing%20so%2C%20we%20have%20combined%20a%20modern%20convolutional%20architecture%20with%20transfer%0Alearning%20and%20a%20loss%20function%20adapted%20to%20curvilinear%20structure%20delineation.%20We%0Ause%20a%20single%20network%20for%20both%20detection%20tasks%20and%20demonstrated%20its%20performance%0Aon%20two%20benchmarking%20datasets.%20We%20have%20integrated%20it%20within%20an%20onboard%20system%0Aand%20run%20it%20in%20flight%2C%20and%20have%20demonstrated%20with%20our%20experiments%20that%20it%0Aoutperforms%20the%20prior%20distant%20cable%20detection%20method%20on%20both%20datasets%2C%20while%0Aalso%20successfully%20detecting%20pylons%2C%20given%20their%20annotations%20are%20available%20for%0Athe%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14352v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-Based%2520Power%2520Line%2520Cables%2520and%2520Pylons%2520Detection%2520for%2520Low%2520Flying%250A%2520%2520Aircrafts%26entry.906535625%3DJakub%2520Gwizda%25C5%2582a%2520and%2520Doruk%2520Oner%2520and%2520Soumava%2520Kumar%2520Roy%2520and%2520Mian%2520Akbar%2520Shah%2520and%2520Ad%2520Eberhard%2520and%2520Ivan%2520Egorov%2520and%2520Philipp%2520Kr%25C3%25BCsi%2520and%2520Grigory%2520Yakushev%26entry.1292438233%3D%2520%2520Power%2520lines%2520are%2520dangerous%2520for%2520low-flying%2520aircrafts%252C%2520especially%2520in%250Alow-visibility%2520conditions.%2520Thus%252C%2520a%2520vision-based%2520system%2520able%2520to%2520analyze%2520the%250Aaircraft%2527s%2520surroundings%2520and%2520to%2520provide%2520the%2520pilots%2520with%2520a%2520%2522second%2520pair%2520of%2520eyes%2522%250Acan%2520contribute%2520to%2520enhancing%2520their%2520safety.%2520To%2520this%2520end%252C%2520we%2520have%2520developed%2520a%2520deep%250Alearning%2520approach%2520to%2520jointly%2520detect%2520power%2520line%2520cables%2520and%2520pylons%2520from%2520images%250Acaptured%2520at%2520distances%2520of%2520several%2520hundred%2520meters%2520by%2520aircraft-mounted%2520cameras.%2520In%250Adoing%2520so%252C%2520we%2520have%2520combined%2520a%2520modern%2520convolutional%2520architecture%2520with%2520transfer%250Alearning%2520and%2520a%2520loss%2520function%2520adapted%2520to%2520curvilinear%2520structure%2520delineation.%2520We%250Ause%2520a%2520single%2520network%2520for%2520both%2520detection%2520tasks%2520and%2520demonstrated%2520its%2520performance%250Aon%2520two%2520benchmarking%2520datasets.%2520We%2520have%2520integrated%2520it%2520within%2520an%2520onboard%2520system%250Aand%2520run%2520it%2520in%2520flight%252C%2520and%2520have%2520demonstrated%2520with%2520our%2520experiments%2520that%2520it%250Aoutperforms%2520the%2520prior%2520distant%2520cable%2520detection%2520method%2520on%2520both%2520datasets%252C%2520while%250Aalso%2520successfully%2520detecting%2520pylons%252C%2520given%2520their%2520annotations%2520are%2520available%2520for%250Athe%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14352v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-Based%20Power%20Line%20Cables%20and%20Pylons%20Detection%20for%20Low%20Flying%0A%20%20Aircrafts&entry.906535625=Jakub%20Gwizda%C5%82a%20and%20Doruk%20Oner%20and%20Soumava%20Kumar%20Roy%20and%20Mian%20Akbar%20Shah%20and%20Ad%20Eberhard%20and%20Ivan%20Egorov%20and%20Philipp%20Kr%C3%BCsi%20and%20Grigory%20Yakushev&entry.1292438233=%20%20Power%20lines%20are%20dangerous%20for%20low-flying%20aircrafts%2C%20especially%20in%0Alow-visibility%20conditions.%20Thus%2C%20a%20vision-based%20system%20able%20to%20analyze%20the%0Aaircraft%27s%20surroundings%20and%20to%20provide%20the%20pilots%20with%20a%20%22second%20pair%20of%20eyes%22%0Acan%20contribute%20to%20enhancing%20their%20safety.%20To%20this%20end%2C%20we%20have%20developed%20a%20deep%0Alearning%20approach%20to%20jointly%20detect%20power%20line%20cables%20and%20pylons%20from%20images%0Acaptured%20at%20distances%20of%20several%20hundred%20meters%20by%20aircraft-mounted%20cameras.%20In%0Adoing%20so%2C%20we%20have%20combined%20a%20modern%20convolutional%20architecture%20with%20transfer%0Alearning%20and%20a%20loss%20function%20adapted%20to%20curvilinear%20structure%20delineation.%20We%0Ause%20a%20single%20network%20for%20both%20detection%20tasks%20and%20demonstrated%20its%20performance%0Aon%20two%20benchmarking%20datasets.%20We%20have%20integrated%20it%20within%20an%20onboard%20system%0Aand%20run%20it%20in%20flight%2C%20and%20have%20demonstrated%20with%20our%20experiments%20that%20it%0Aoutperforms%20the%20prior%20distant%20cable%20detection%20method%20on%20both%20datasets%2C%20while%0Aalso%20successfully%20detecting%20pylons%2C%20given%20their%20annotations%20are%20available%20for%0Athe%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14352v1&entry.124074799=Read"},
{"title": "LLM See, LLM Do: Guiding Data Generation to Target Non-Differentiable\n  Objectives", "author": "Lu\u00edsa Shimabucoro and Sebastian Ruder and Julia Kreutzer and Marzieh Fadaee and Sara Hooker", "abstract": "  The widespread adoption of synthetic data raises new questions about how\nmodels generating the data can influence other large language models (LLMs) via\ndistilled data. To start, our work exhaustively characterizes the impact of\npassive inheritance of model properties by systematically studying the\nconsequences of synthetic data integration. We provide one of the most\ncomprehensive studies to-date of how the source of synthetic data shapes\nmodels' internal biases, calibration and generations' textual attributes and\npreferences. We find that models are surprisingly sensitive towards certain\nattributes even when the synthetic data prompts appear \"neutral\". which invites\nthe question whether this sensitivity can be exploited for good.\n  Our findings invite the question can we explicitly steer the models towards\nthe properties we want at test time by exploiting the data generation process?\nThis would have historically been considered infeasible due to the cost of\ncollecting data with a specific characteristic or objective in mind. However,\nimprovement in the quality of synthetic data, as well as a shift towards\ngeneral-purpose models designed to follow a diverse way of instructions, means\nthis question is timely. We propose active inheritance as a term to describe\nintentionally constraining synthetic data according to a non-differentiable\nobjective. We demonstrate how active inheritance can steer the generation\nprofiles of models towards desirable non-differentiable attributes, e.g. high\nlexical diversity or low toxicity.\n", "link": "http://arxiv.org/abs/2407.01490v2", "date": "2024-07-19", "relevancy": 2.0772, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5249}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5236}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM%20See%2C%20LLM%20Do%3A%20Guiding%20Data%20Generation%20to%20Target%20Non-Differentiable%0A%20%20Objectives&body=Title%3A%20LLM%20See%2C%20LLM%20Do%3A%20Guiding%20Data%20Generation%20to%20Target%20Non-Differentiable%0A%20%20Objectives%0AAuthor%3A%20Lu%C3%ADsa%20Shimabucoro%20and%20Sebastian%20Ruder%20and%20Julia%20Kreutzer%20and%20Marzieh%20Fadaee%20and%20Sara%20Hooker%0AAbstract%3A%20%20%20The%20widespread%20adoption%20of%20synthetic%20data%20raises%20new%20questions%20about%20how%0Amodels%20generating%20the%20data%20can%20influence%20other%20large%20language%20models%20%28LLMs%29%20via%0Adistilled%20data.%20To%20start%2C%20our%20work%20exhaustively%20characterizes%20the%20impact%20of%0Apassive%20inheritance%20of%20model%20properties%20by%20systematically%20studying%20the%0Aconsequences%20of%20synthetic%20data%20integration.%20We%20provide%20one%20of%20the%20most%0Acomprehensive%20studies%20to-date%20of%20how%20the%20source%20of%20synthetic%20data%20shapes%0Amodels%27%20internal%20biases%2C%20calibration%20and%20generations%27%20textual%20attributes%20and%0Apreferences.%20We%20find%20that%20models%20are%20surprisingly%20sensitive%20towards%20certain%0Aattributes%20even%20when%20the%20synthetic%20data%20prompts%20appear%20%22neutral%22.%20which%20invites%0Athe%20question%20whether%20this%20sensitivity%20can%20be%20exploited%20for%20good.%0A%20%20Our%20findings%20invite%20the%20question%20can%20we%20explicitly%20steer%20the%20models%20towards%0Athe%20properties%20we%20want%20at%20test%20time%20by%20exploiting%20the%20data%20generation%20process%3F%0AThis%20would%20have%20historically%20been%20considered%20infeasible%20due%20to%20the%20cost%20of%0Acollecting%20data%20with%20a%20specific%20characteristic%20or%20objective%20in%20mind.%20However%2C%0Aimprovement%20in%20the%20quality%20of%20synthetic%20data%2C%20as%20well%20as%20a%20shift%20towards%0Ageneral-purpose%20models%20designed%20to%20follow%20a%20diverse%20way%20of%20instructions%2C%20means%0Athis%20question%20is%20timely.%20We%20propose%20active%20inheritance%20as%20a%20term%20to%20describe%0Aintentionally%20constraining%20synthetic%20data%20according%20to%20a%20non-differentiable%0Aobjective.%20We%20demonstrate%20how%20active%20inheritance%20can%20steer%20the%20generation%0Aprofiles%20of%20models%20towards%20desirable%20non-differentiable%20attributes%2C%20e.g.%20high%0Alexical%20diversity%20or%20low%20toxicity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01490v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM%2520See%252C%2520LLM%2520Do%253A%2520Guiding%2520Data%2520Generation%2520to%2520Target%2520Non-Differentiable%250A%2520%2520Objectives%26entry.906535625%3DLu%25C3%25ADsa%2520Shimabucoro%2520and%2520Sebastian%2520Ruder%2520and%2520Julia%2520Kreutzer%2520and%2520Marzieh%2520Fadaee%2520and%2520Sara%2520Hooker%26entry.1292438233%3D%2520%2520The%2520widespread%2520adoption%2520of%2520synthetic%2520data%2520raises%2520new%2520questions%2520about%2520how%250Amodels%2520generating%2520the%2520data%2520can%2520influence%2520other%2520large%2520language%2520models%2520%2528LLMs%2529%2520via%250Adistilled%2520data.%2520To%2520start%252C%2520our%2520work%2520exhaustively%2520characterizes%2520the%2520impact%2520of%250Apassive%2520inheritance%2520of%2520model%2520properties%2520by%2520systematically%2520studying%2520the%250Aconsequences%2520of%2520synthetic%2520data%2520integration.%2520We%2520provide%2520one%2520of%2520the%2520most%250Acomprehensive%2520studies%2520to-date%2520of%2520how%2520the%2520source%2520of%2520synthetic%2520data%2520shapes%250Amodels%2527%2520internal%2520biases%252C%2520calibration%2520and%2520generations%2527%2520textual%2520attributes%2520and%250Apreferences.%2520We%2520find%2520that%2520models%2520are%2520surprisingly%2520sensitive%2520towards%2520certain%250Aattributes%2520even%2520when%2520the%2520synthetic%2520data%2520prompts%2520appear%2520%2522neutral%2522.%2520which%2520invites%250Athe%2520question%2520whether%2520this%2520sensitivity%2520can%2520be%2520exploited%2520for%2520good.%250A%2520%2520Our%2520findings%2520invite%2520the%2520question%2520can%2520we%2520explicitly%2520steer%2520the%2520models%2520towards%250Athe%2520properties%2520we%2520want%2520at%2520test%2520time%2520by%2520exploiting%2520the%2520data%2520generation%2520process%253F%250AThis%2520would%2520have%2520historically%2520been%2520considered%2520infeasible%2520due%2520to%2520the%2520cost%2520of%250Acollecting%2520data%2520with%2520a%2520specific%2520characteristic%2520or%2520objective%2520in%2520mind.%2520However%252C%250Aimprovement%2520in%2520the%2520quality%2520of%2520synthetic%2520data%252C%2520as%2520well%2520as%2520a%2520shift%2520towards%250Ageneral-purpose%2520models%2520designed%2520to%2520follow%2520a%2520diverse%2520way%2520of%2520instructions%252C%2520means%250Athis%2520question%2520is%2520timely.%2520We%2520propose%2520active%2520inheritance%2520as%2520a%2520term%2520to%2520describe%250Aintentionally%2520constraining%2520synthetic%2520data%2520according%2520to%2520a%2520non-differentiable%250Aobjective.%2520We%2520demonstrate%2520how%2520active%2520inheritance%2520can%2520steer%2520the%2520generation%250Aprofiles%2520of%2520models%2520towards%2520desirable%2520non-differentiable%2520attributes%252C%2520e.g.%2520high%250Alexical%2520diversity%2520or%2520low%2520toxicity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01490v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM%20See%2C%20LLM%20Do%3A%20Guiding%20Data%20Generation%20to%20Target%20Non-Differentiable%0A%20%20Objectives&entry.906535625=Lu%C3%ADsa%20Shimabucoro%20and%20Sebastian%20Ruder%20and%20Julia%20Kreutzer%20and%20Marzieh%20Fadaee%20and%20Sara%20Hooker&entry.1292438233=%20%20The%20widespread%20adoption%20of%20synthetic%20data%20raises%20new%20questions%20about%20how%0Amodels%20generating%20the%20data%20can%20influence%20other%20large%20language%20models%20%28LLMs%29%20via%0Adistilled%20data.%20To%20start%2C%20our%20work%20exhaustively%20characterizes%20the%20impact%20of%0Apassive%20inheritance%20of%20model%20properties%20by%20systematically%20studying%20the%0Aconsequences%20of%20synthetic%20data%20integration.%20We%20provide%20one%20of%20the%20most%0Acomprehensive%20studies%20to-date%20of%20how%20the%20source%20of%20synthetic%20data%20shapes%0Amodels%27%20internal%20biases%2C%20calibration%20and%20generations%27%20textual%20attributes%20and%0Apreferences.%20We%20find%20that%20models%20are%20surprisingly%20sensitive%20towards%20certain%0Aattributes%20even%20when%20the%20synthetic%20data%20prompts%20appear%20%22neutral%22.%20which%20invites%0Athe%20question%20whether%20this%20sensitivity%20can%20be%20exploited%20for%20good.%0A%20%20Our%20findings%20invite%20the%20question%20can%20we%20explicitly%20steer%20the%20models%20towards%0Athe%20properties%20we%20want%20at%20test%20time%20by%20exploiting%20the%20data%20generation%20process%3F%0AThis%20would%20have%20historically%20been%20considered%20infeasible%20due%20to%20the%20cost%20of%0Acollecting%20data%20with%20a%20specific%20characteristic%20or%20objective%20in%20mind.%20However%2C%0Aimprovement%20in%20the%20quality%20of%20synthetic%20data%2C%20as%20well%20as%20a%20shift%20towards%0Ageneral-purpose%20models%20designed%20to%20follow%20a%20diverse%20way%20of%20instructions%2C%20means%0Athis%20question%20is%20timely.%20We%20propose%20active%20inheritance%20as%20a%20term%20to%20describe%0Aintentionally%20constraining%20synthetic%20data%20according%20to%20a%20non-differentiable%0Aobjective.%20We%20demonstrate%20how%20active%20inheritance%20can%20steer%20the%20generation%0Aprofiles%20of%20models%20towards%20desirable%20non-differentiable%20attributes%2C%20e.g.%20high%0Alexical%20diversity%20or%20low%20toxicity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01490v2&entry.124074799=Read"},
{"title": "EdgeSAM: Prompt-In-the-Loop Distillation for On-Device Deployment of SAM", "author": "Chong Zhou and Xiangtai Li and Chen Change Loy and Bo Dai", "abstract": "  This paper presents EdgeSAM, an accelerated variant of the Segment Anything\nModel (SAM), optimized for efficient execution on edge devices with minimal\ncompromise in performance. Our approach involves distilling the original\nViT-based SAM image encoder into a purely CNN-based architecture, better suited\nfor edge devices. We carefully benchmark various distillation strategies and\ndemonstrate that taskagnostic encoder distillation fails to capture the full\nknowledge embodied in SAM. To overcome this bottleneck, we include both the\nprompt encoder and mask decoder in the distillation process, with box and point\nprompts in the loop, so that the distilled model can accurately capture the\nintricate dynamics between user input and mask generation. To mitigate dataset\nbias issues stemming from point prompt distillation, we incorporate a\nlightweight module within the encoder. As a result, EdgeSAM achieves a 37-fold\nspeed increase compared to the original SAM, and it also outperforms\nMobileSAM/EfficientSAM, being over 7 times as fast when deployed on edge\ndevices while enhancing the mIoUs on COCO and LVIS by 2.3/1.5 and 3.1/1.6,\nrespectively. It is also the first SAM variant that can run at over 30 FPS on\nan iPhone 14. Code and demo are available at\nhttps://www.mmlab-ntu.com/project/edgesam.\n", "link": "http://arxiv.org/abs/2312.06660v2", "date": "2024-07-19", "relevancy": 2.0751, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5645}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5118}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EdgeSAM%3A%20Prompt-In-the-Loop%20Distillation%20for%20On-Device%20Deployment%20of%20SAM&body=Title%3A%20EdgeSAM%3A%20Prompt-In-the-Loop%20Distillation%20for%20On-Device%20Deployment%20of%20SAM%0AAuthor%3A%20Chong%20Zhou%20and%20Xiangtai%20Li%20and%20Chen%20Change%20Loy%20and%20Bo%20Dai%0AAbstract%3A%20%20%20This%20paper%20presents%20EdgeSAM%2C%20an%20accelerated%20variant%20of%20the%20Segment%20Anything%0AModel%20%28SAM%29%2C%20optimized%20for%20efficient%20execution%20on%20edge%20devices%20with%20minimal%0Acompromise%20in%20performance.%20Our%20approach%20involves%20distilling%20the%20original%0AViT-based%20SAM%20image%20encoder%20into%20a%20purely%20CNN-based%20architecture%2C%20better%20suited%0Afor%20edge%20devices.%20We%20carefully%20benchmark%20various%20distillation%20strategies%20and%0Ademonstrate%20that%20taskagnostic%20encoder%20distillation%20fails%20to%20capture%20the%20full%0Aknowledge%20embodied%20in%20SAM.%20To%20overcome%20this%20bottleneck%2C%20we%20include%20both%20the%0Aprompt%20encoder%20and%20mask%20decoder%20in%20the%20distillation%20process%2C%20with%20box%20and%20point%0Aprompts%20in%20the%20loop%2C%20so%20that%20the%20distilled%20model%20can%20accurately%20capture%20the%0Aintricate%20dynamics%20between%20user%20input%20and%20mask%20generation.%20To%20mitigate%20dataset%0Abias%20issues%20stemming%20from%20point%20prompt%20distillation%2C%20we%20incorporate%20a%0Alightweight%20module%20within%20the%20encoder.%20As%20a%20result%2C%20EdgeSAM%20achieves%20a%2037-fold%0Aspeed%20increase%20compared%20to%20the%20original%20SAM%2C%20and%20it%20also%20outperforms%0AMobileSAM/EfficientSAM%2C%20being%20over%207%20times%20as%20fast%20when%20deployed%20on%20edge%0Adevices%20while%20enhancing%20the%20mIoUs%20on%20COCO%20and%20LVIS%20by%202.3/1.5%20and%203.1/1.6%2C%0Arespectively.%20It%20is%20also%20the%20first%20SAM%20variant%20that%20can%20run%20at%20over%2030%20FPS%20on%0Aan%20iPhone%2014.%20Code%20and%20demo%20are%20available%20at%0Ahttps%3A//www.mmlab-ntu.com/project/edgesam.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.06660v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdgeSAM%253A%2520Prompt-In-the-Loop%2520Distillation%2520for%2520On-Device%2520Deployment%2520of%2520SAM%26entry.906535625%3DChong%2520Zhou%2520and%2520Xiangtai%2520Li%2520and%2520Chen%2520Change%2520Loy%2520and%2520Bo%2520Dai%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520EdgeSAM%252C%2520an%2520accelerated%2520variant%2520of%2520the%2520Segment%2520Anything%250AModel%2520%2528SAM%2529%252C%2520optimized%2520for%2520efficient%2520execution%2520on%2520edge%2520devices%2520with%2520minimal%250Acompromise%2520in%2520performance.%2520Our%2520approach%2520involves%2520distilling%2520the%2520original%250AViT-based%2520SAM%2520image%2520encoder%2520into%2520a%2520purely%2520CNN-based%2520architecture%252C%2520better%2520suited%250Afor%2520edge%2520devices.%2520We%2520carefully%2520benchmark%2520various%2520distillation%2520strategies%2520and%250Ademonstrate%2520that%2520taskagnostic%2520encoder%2520distillation%2520fails%2520to%2520capture%2520the%2520full%250Aknowledge%2520embodied%2520in%2520SAM.%2520To%2520overcome%2520this%2520bottleneck%252C%2520we%2520include%2520both%2520the%250Aprompt%2520encoder%2520and%2520mask%2520decoder%2520in%2520the%2520distillation%2520process%252C%2520with%2520box%2520and%2520point%250Aprompts%2520in%2520the%2520loop%252C%2520so%2520that%2520the%2520distilled%2520model%2520can%2520accurately%2520capture%2520the%250Aintricate%2520dynamics%2520between%2520user%2520input%2520and%2520mask%2520generation.%2520To%2520mitigate%2520dataset%250Abias%2520issues%2520stemming%2520from%2520point%2520prompt%2520distillation%252C%2520we%2520incorporate%2520a%250Alightweight%2520module%2520within%2520the%2520encoder.%2520As%2520a%2520result%252C%2520EdgeSAM%2520achieves%2520a%252037-fold%250Aspeed%2520increase%2520compared%2520to%2520the%2520original%2520SAM%252C%2520and%2520it%2520also%2520outperforms%250AMobileSAM/EfficientSAM%252C%2520being%2520over%25207%2520times%2520as%2520fast%2520when%2520deployed%2520on%2520edge%250Adevices%2520while%2520enhancing%2520the%2520mIoUs%2520on%2520COCO%2520and%2520LVIS%2520by%25202.3/1.5%2520and%25203.1/1.6%252C%250Arespectively.%2520It%2520is%2520also%2520the%2520first%2520SAM%2520variant%2520that%2520can%2520run%2520at%2520over%252030%2520FPS%2520on%250Aan%2520iPhone%252014.%2520Code%2520and%2520demo%2520are%2520available%2520at%250Ahttps%253A//www.mmlab-ntu.com/project/edgesam.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.06660v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EdgeSAM%3A%20Prompt-In-the-Loop%20Distillation%20for%20On-Device%20Deployment%20of%20SAM&entry.906535625=Chong%20Zhou%20and%20Xiangtai%20Li%20and%20Chen%20Change%20Loy%20and%20Bo%20Dai&entry.1292438233=%20%20This%20paper%20presents%20EdgeSAM%2C%20an%20accelerated%20variant%20of%20the%20Segment%20Anything%0AModel%20%28SAM%29%2C%20optimized%20for%20efficient%20execution%20on%20edge%20devices%20with%20minimal%0Acompromise%20in%20performance.%20Our%20approach%20involves%20distilling%20the%20original%0AViT-based%20SAM%20image%20encoder%20into%20a%20purely%20CNN-based%20architecture%2C%20better%20suited%0Afor%20edge%20devices.%20We%20carefully%20benchmark%20various%20distillation%20strategies%20and%0Ademonstrate%20that%20taskagnostic%20encoder%20distillation%20fails%20to%20capture%20the%20full%0Aknowledge%20embodied%20in%20SAM.%20To%20overcome%20this%20bottleneck%2C%20we%20include%20both%20the%0Aprompt%20encoder%20and%20mask%20decoder%20in%20the%20distillation%20process%2C%20with%20box%20and%20point%0Aprompts%20in%20the%20loop%2C%20so%20that%20the%20distilled%20model%20can%20accurately%20capture%20the%0Aintricate%20dynamics%20between%20user%20input%20and%20mask%20generation.%20To%20mitigate%20dataset%0Abias%20issues%20stemming%20from%20point%20prompt%20distillation%2C%20we%20incorporate%20a%0Alightweight%20module%20within%20the%20encoder.%20As%20a%20result%2C%20EdgeSAM%20achieves%20a%2037-fold%0Aspeed%20increase%20compared%20to%20the%20original%20SAM%2C%20and%20it%20also%20outperforms%0AMobileSAM/EfficientSAM%2C%20being%20over%207%20times%20as%20fast%20when%20deployed%20on%20edge%0Adevices%20while%20enhancing%20the%20mIoUs%20on%20COCO%20and%20LVIS%20by%202.3/1.5%20and%203.1/1.6%2C%0Arespectively.%20It%20is%20also%20the%20first%20SAM%20variant%20that%20can%20run%20at%20over%2030%20FPS%20on%0Aan%20iPhone%2014.%20Code%20and%20demo%20are%20available%20at%0Ahttps%3A//www.mmlab-ntu.com/project/edgesam.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.06660v2&entry.124074799=Read"},
{"title": "Learning Collective Variables with Synthetic Data Augmentation through\n  Physics-Inspired Geodesic Interpolation", "author": "Soojung Yang and Juno Nam and Johannes C. B. Dietschreit and Rafael G\u00f3mez-Bombarelli", "abstract": "  In molecular dynamics simulations, rare events, such as protein folding, are\ntypically studied using enhanced sampling techniques, most of which are based\non the definition of a collective variable (CV) along which acceleration\noccurs. Obtaining an expressive CV is crucial, but often hindered by the lack\nof information about the particular event, e.g., the transition from unfolded\nto folded conformation. We propose a simulation-free data augmentation strategy\nusing physics-inspired metrics to generate geodesic interpolations resembling\nprotein folding transitions, thereby improving sampling efficiency without true\ntransition state samples. This new data can be used to improve the accuracy of\nclassifier-based methods. Alternatively, a regression-based learning scheme for\nCV models can be adopted by leveraging the interpolation progress parameter.\n", "link": "http://arxiv.org/abs/2402.01542v4", "date": "2024-07-19", "relevancy": 2.0666, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5283}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5271}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Collective%20Variables%20with%20Synthetic%20Data%20Augmentation%20through%0A%20%20Physics-Inspired%20Geodesic%20Interpolation&body=Title%3A%20Learning%20Collective%20Variables%20with%20Synthetic%20Data%20Augmentation%20through%0A%20%20Physics-Inspired%20Geodesic%20Interpolation%0AAuthor%3A%20Soojung%20Yang%20and%20Juno%20Nam%20and%20Johannes%20C.%20B.%20Dietschreit%20and%20Rafael%20G%C3%B3mez-Bombarelli%0AAbstract%3A%20%20%20In%20molecular%20dynamics%20simulations%2C%20rare%20events%2C%20such%20as%20protein%20folding%2C%20are%0Atypically%20studied%20using%20enhanced%20sampling%20techniques%2C%20most%20of%20which%20are%20based%0Aon%20the%20definition%20of%20a%20collective%20variable%20%28CV%29%20along%20which%20acceleration%0Aoccurs.%20Obtaining%20an%20expressive%20CV%20is%20crucial%2C%20but%20often%20hindered%20by%20the%20lack%0Aof%20information%20about%20the%20particular%20event%2C%20e.g.%2C%20the%20transition%20from%20unfolded%0Ato%20folded%20conformation.%20We%20propose%20a%20simulation-free%20data%20augmentation%20strategy%0Ausing%20physics-inspired%20metrics%20to%20generate%20geodesic%20interpolations%20resembling%0Aprotein%20folding%20transitions%2C%20thereby%20improving%20sampling%20efficiency%20without%20true%0Atransition%20state%20samples.%20This%20new%20data%20can%20be%20used%20to%20improve%20the%20accuracy%20of%0Aclassifier-based%20methods.%20Alternatively%2C%20a%20regression-based%20learning%20scheme%20for%0ACV%20models%20can%20be%20adopted%20by%20leveraging%20the%20interpolation%20progress%20parameter.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01542v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Collective%2520Variables%2520with%2520Synthetic%2520Data%2520Augmentation%2520through%250A%2520%2520Physics-Inspired%2520Geodesic%2520Interpolation%26entry.906535625%3DSoojung%2520Yang%2520and%2520Juno%2520Nam%2520and%2520Johannes%2520C.%2520B.%2520Dietschreit%2520and%2520Rafael%2520G%25C3%25B3mez-Bombarelli%26entry.1292438233%3D%2520%2520In%2520molecular%2520dynamics%2520simulations%252C%2520rare%2520events%252C%2520such%2520as%2520protein%2520folding%252C%2520are%250Atypically%2520studied%2520using%2520enhanced%2520sampling%2520techniques%252C%2520most%2520of%2520which%2520are%2520based%250Aon%2520the%2520definition%2520of%2520a%2520collective%2520variable%2520%2528CV%2529%2520along%2520which%2520acceleration%250Aoccurs.%2520Obtaining%2520an%2520expressive%2520CV%2520is%2520crucial%252C%2520but%2520often%2520hindered%2520by%2520the%2520lack%250Aof%2520information%2520about%2520the%2520particular%2520event%252C%2520e.g.%252C%2520the%2520transition%2520from%2520unfolded%250Ato%2520folded%2520conformation.%2520We%2520propose%2520a%2520simulation-free%2520data%2520augmentation%2520strategy%250Ausing%2520physics-inspired%2520metrics%2520to%2520generate%2520geodesic%2520interpolations%2520resembling%250Aprotein%2520folding%2520transitions%252C%2520thereby%2520improving%2520sampling%2520efficiency%2520without%2520true%250Atransition%2520state%2520samples.%2520This%2520new%2520data%2520can%2520be%2520used%2520to%2520improve%2520the%2520accuracy%2520of%250Aclassifier-based%2520methods.%2520Alternatively%252C%2520a%2520regression-based%2520learning%2520scheme%2520for%250ACV%2520models%2520can%2520be%2520adopted%2520by%2520leveraging%2520the%2520interpolation%2520progress%2520parameter.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01542v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Collective%20Variables%20with%20Synthetic%20Data%20Augmentation%20through%0A%20%20Physics-Inspired%20Geodesic%20Interpolation&entry.906535625=Soojung%20Yang%20and%20Juno%20Nam%20and%20Johannes%20C.%20B.%20Dietschreit%20and%20Rafael%20G%C3%B3mez-Bombarelli&entry.1292438233=%20%20In%20molecular%20dynamics%20simulations%2C%20rare%20events%2C%20such%20as%20protein%20folding%2C%20are%0Atypically%20studied%20using%20enhanced%20sampling%20techniques%2C%20most%20of%20which%20are%20based%0Aon%20the%20definition%20of%20a%20collective%20variable%20%28CV%29%20along%20which%20acceleration%0Aoccurs.%20Obtaining%20an%20expressive%20CV%20is%20crucial%2C%20but%20often%20hindered%20by%20the%20lack%0Aof%20information%20about%20the%20particular%20event%2C%20e.g.%2C%20the%20transition%20from%20unfolded%0Ato%20folded%20conformation.%20We%20propose%20a%20simulation-free%20data%20augmentation%20strategy%0Ausing%20physics-inspired%20metrics%20to%20generate%20geodesic%20interpolations%20resembling%0Aprotein%20folding%20transitions%2C%20thereby%20improving%20sampling%20efficiency%20without%20true%0Atransition%20state%20samples.%20This%20new%20data%20can%20be%20used%20to%20improve%20the%20accuracy%20of%0Aclassifier-based%20methods.%20Alternatively%2C%20a%20regression-based%20learning%20scheme%20for%0ACV%20models%20can%20be%20adopted%20by%20leveraging%20the%20interpolation%20progress%20parameter.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01542v4&entry.124074799=Read"},
{"title": "The Extrapolation Power of Implicit Models", "author": "Juliette Decugis and Alicia Y. Tsai and Max Emerling and Ashwin Ganesh and Laurent El Ghaoui", "abstract": "  In this paper, we investigate the extrapolation capabilities of implicit deep\nlearning models in handling unobserved data, where traditional deep neural\nnetworks may falter. Implicit models, distinguished by their adaptability in\nlayer depth and incorporation of feedback within their computational graph, are\nput to the test across various extrapolation scenarios: out-of-distribution,\ngeographical, and temporal shifts. Our experiments consistently demonstrate\nsignificant performance advantage with implicit models. Unlike their\nnon-implicit counterparts, which often rely on meticulous architectural design\nfor each task, implicit models demonstrate the ability to learn complex model\nstructures without the need for task-specific design, highlighting their\nrobustness in handling unseen data.\n", "link": "http://arxiv.org/abs/2407.14430v1", "date": "2024-07-19", "relevancy": 2.0468, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5349}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5014}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4796}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Extrapolation%20Power%20of%20Implicit%20Models&body=Title%3A%20The%20Extrapolation%20Power%20of%20Implicit%20Models%0AAuthor%3A%20Juliette%20Decugis%20and%20Alicia%20Y.%20Tsai%20and%20Max%20Emerling%20and%20Ashwin%20Ganesh%20and%20Laurent%20El%20Ghaoui%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20investigate%20the%20extrapolation%20capabilities%20of%20implicit%20deep%0Alearning%20models%20in%20handling%20unobserved%20data%2C%20where%20traditional%20deep%20neural%0Anetworks%20may%20falter.%20Implicit%20models%2C%20distinguished%20by%20their%20adaptability%20in%0Alayer%20depth%20and%20incorporation%20of%20feedback%20within%20their%20computational%20graph%2C%20are%0Aput%20to%20the%20test%20across%20various%20extrapolation%20scenarios%3A%20out-of-distribution%2C%0Ageographical%2C%20and%20temporal%20shifts.%20Our%20experiments%20consistently%20demonstrate%0Asignificant%20performance%20advantage%20with%20implicit%20models.%20Unlike%20their%0Anon-implicit%20counterparts%2C%20which%20often%20rely%20on%20meticulous%20architectural%20design%0Afor%20each%20task%2C%20implicit%20models%20demonstrate%20the%20ability%20to%20learn%20complex%20model%0Astructures%20without%20the%20need%20for%20task-specific%20design%2C%20highlighting%20their%0Arobustness%20in%20handling%20unseen%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14430v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Extrapolation%2520Power%2520of%2520Implicit%2520Models%26entry.906535625%3DJuliette%2520Decugis%2520and%2520Alicia%2520Y.%2520Tsai%2520and%2520Max%2520Emerling%2520and%2520Ashwin%2520Ganesh%2520and%2520Laurent%2520El%2520Ghaoui%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520extrapolation%2520capabilities%2520of%2520implicit%2520deep%250Alearning%2520models%2520in%2520handling%2520unobserved%2520data%252C%2520where%2520traditional%2520deep%2520neural%250Anetworks%2520may%2520falter.%2520Implicit%2520models%252C%2520distinguished%2520by%2520their%2520adaptability%2520in%250Alayer%2520depth%2520and%2520incorporation%2520of%2520feedback%2520within%2520their%2520computational%2520graph%252C%2520are%250Aput%2520to%2520the%2520test%2520across%2520various%2520extrapolation%2520scenarios%253A%2520out-of-distribution%252C%250Ageographical%252C%2520and%2520temporal%2520shifts.%2520Our%2520experiments%2520consistently%2520demonstrate%250Asignificant%2520performance%2520advantage%2520with%2520implicit%2520models.%2520Unlike%2520their%250Anon-implicit%2520counterparts%252C%2520which%2520often%2520rely%2520on%2520meticulous%2520architectural%2520design%250Afor%2520each%2520task%252C%2520implicit%2520models%2520demonstrate%2520the%2520ability%2520to%2520learn%2520complex%2520model%250Astructures%2520without%2520the%2520need%2520for%2520task-specific%2520design%252C%2520highlighting%2520their%250Arobustness%2520in%2520handling%2520unseen%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14430v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Extrapolation%20Power%20of%20Implicit%20Models&entry.906535625=Juliette%20Decugis%20and%20Alicia%20Y.%20Tsai%20and%20Max%20Emerling%20and%20Ashwin%20Ganesh%20and%20Laurent%20El%20Ghaoui&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20the%20extrapolation%20capabilities%20of%20implicit%20deep%0Alearning%20models%20in%20handling%20unobserved%20data%2C%20where%20traditional%20deep%20neural%0Anetworks%20may%20falter.%20Implicit%20models%2C%20distinguished%20by%20their%20adaptability%20in%0Alayer%20depth%20and%20incorporation%20of%20feedback%20within%20their%20computational%20graph%2C%20are%0Aput%20to%20the%20test%20across%20various%20extrapolation%20scenarios%3A%20out-of-distribution%2C%0Ageographical%2C%20and%20temporal%20shifts.%20Our%20experiments%20consistently%20demonstrate%0Asignificant%20performance%20advantage%20with%20implicit%20models.%20Unlike%20their%0Anon-implicit%20counterparts%2C%20which%20often%20rely%20on%20meticulous%20architectural%20design%0Afor%20each%20task%2C%20implicit%20models%20demonstrate%20the%20ability%20to%20learn%20complex%20model%0Astructures%20without%20the%20need%20for%20task-specific%20design%2C%20highlighting%20their%0Arobustness%20in%20handling%20unseen%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14430v1&entry.124074799=Read"},
{"title": "Joint or Disjoint: Mixing Training Regimes for Early-Exit Models", "author": "Bart\u0142omiej Krzepkowski and Monika Michaluk and Franciszek Szarwacki and Piotr Kubaty and Jary Pomponi and Tomasz Trzci\u0144ski and Bartosz W\u00f3jcik and Kamil Adamczewski", "abstract": "  Early exits are an important efficiency mechanism integrated into deep neural\nnetworks that allows for the termination of the network's forward pass before\nprocessing through all its layers. By allowing early halting of the inference\nprocess for less complex inputs that reached high confidence, early exits\nsignificantly reduce the amount of computation required. Early exit methods add\ntrainable internal classifiers which leads to more intricacy in the training\nprocess. However, there is no consistent verification of the approaches of\ntraining of early exit methods, and no unified scheme of training such models.\nMost early exit methods employ a training strategy that either simultaneously\ntrains the backbone network and the exit heads or trains the exit heads\nseparately. We propose a training approach where the backbone is initially\ntrained on its own, followed by a phase where both the backbone and the exit\nheads are trained together. Thus, we advocate for organizing early-exit\ntraining strategies into three distinct categories, and then validate them for\ntheir performance and efficiency. In this benchmark, we perform both\ntheoretical and empirical analysis of early-exit training regimes. We study the\nmethods in terms of information flow, loss landscape and numerical rank of\nactivations and gauge the suitability of regimes for various architectures and\ndatasets.\n", "link": "http://arxiv.org/abs/2407.14320v1", "date": "2024-07-19", "relevancy": 2.0425, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5767}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.471}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20or%20Disjoint%3A%20Mixing%20Training%20Regimes%20for%20Early-Exit%20Models&body=Title%3A%20Joint%20or%20Disjoint%3A%20Mixing%20Training%20Regimes%20for%20Early-Exit%20Models%0AAuthor%3A%20Bart%C5%82omiej%20Krzepkowski%20and%20Monika%20Michaluk%20and%20Franciszek%20Szarwacki%20and%20Piotr%20Kubaty%20and%20Jary%20Pomponi%20and%20Tomasz%20Trzci%C5%84ski%20and%20Bartosz%20W%C3%B3jcik%20and%20Kamil%20Adamczewski%0AAbstract%3A%20%20%20Early%20exits%20are%20an%20important%20efficiency%20mechanism%20integrated%20into%20deep%20neural%0Anetworks%20that%20allows%20for%20the%20termination%20of%20the%20network%27s%20forward%20pass%20before%0Aprocessing%20through%20all%20its%20layers.%20By%20allowing%20early%20halting%20of%20the%20inference%0Aprocess%20for%20less%20complex%20inputs%20that%20reached%20high%20confidence%2C%20early%20exits%0Asignificantly%20reduce%20the%20amount%20of%20computation%20required.%20Early%20exit%20methods%20add%0Atrainable%20internal%20classifiers%20which%20leads%20to%20more%20intricacy%20in%20the%20training%0Aprocess.%20However%2C%20there%20is%20no%20consistent%20verification%20of%20the%20approaches%20of%0Atraining%20of%20early%20exit%20methods%2C%20and%20no%20unified%20scheme%20of%20training%20such%20models.%0AMost%20early%20exit%20methods%20employ%20a%20training%20strategy%20that%20either%20simultaneously%0Atrains%20the%20backbone%20network%20and%20the%20exit%20heads%20or%20trains%20the%20exit%20heads%0Aseparately.%20We%20propose%20a%20training%20approach%20where%20the%20backbone%20is%20initially%0Atrained%20on%20its%20own%2C%20followed%20by%20a%20phase%20where%20both%20the%20backbone%20and%20the%20exit%0Aheads%20are%20trained%20together.%20Thus%2C%20we%20advocate%20for%20organizing%20early-exit%0Atraining%20strategies%20into%20three%20distinct%20categories%2C%20and%20then%20validate%20them%20for%0Atheir%20performance%20and%20efficiency.%20In%20this%20benchmark%2C%20we%20perform%20both%0Atheoretical%20and%20empirical%20analysis%20of%20early-exit%20training%20regimes.%20We%20study%20the%0Amethods%20in%20terms%20of%20information%20flow%2C%20loss%20landscape%20and%20numerical%20rank%20of%0Aactivations%20and%20gauge%20the%20suitability%20of%20regimes%20for%20various%20architectures%20and%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14320v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520or%2520Disjoint%253A%2520Mixing%2520Training%2520Regimes%2520for%2520Early-Exit%2520Models%26entry.906535625%3DBart%25C5%2582omiej%2520Krzepkowski%2520and%2520Monika%2520Michaluk%2520and%2520Franciszek%2520Szarwacki%2520and%2520Piotr%2520Kubaty%2520and%2520Jary%2520Pomponi%2520and%2520Tomasz%2520Trzci%25C5%2584ski%2520and%2520Bartosz%2520W%25C3%25B3jcik%2520and%2520Kamil%2520Adamczewski%26entry.1292438233%3D%2520%2520Early%2520exits%2520are%2520an%2520important%2520efficiency%2520mechanism%2520integrated%2520into%2520deep%2520neural%250Anetworks%2520that%2520allows%2520for%2520the%2520termination%2520of%2520the%2520network%2527s%2520forward%2520pass%2520before%250Aprocessing%2520through%2520all%2520its%2520layers.%2520By%2520allowing%2520early%2520halting%2520of%2520the%2520inference%250Aprocess%2520for%2520less%2520complex%2520inputs%2520that%2520reached%2520high%2520confidence%252C%2520early%2520exits%250Asignificantly%2520reduce%2520the%2520amount%2520of%2520computation%2520required.%2520Early%2520exit%2520methods%2520add%250Atrainable%2520internal%2520classifiers%2520which%2520leads%2520to%2520more%2520intricacy%2520in%2520the%2520training%250Aprocess.%2520However%252C%2520there%2520is%2520no%2520consistent%2520verification%2520of%2520the%2520approaches%2520of%250Atraining%2520of%2520early%2520exit%2520methods%252C%2520and%2520no%2520unified%2520scheme%2520of%2520training%2520such%2520models.%250AMost%2520early%2520exit%2520methods%2520employ%2520a%2520training%2520strategy%2520that%2520either%2520simultaneously%250Atrains%2520the%2520backbone%2520network%2520and%2520the%2520exit%2520heads%2520or%2520trains%2520the%2520exit%2520heads%250Aseparately.%2520We%2520propose%2520a%2520training%2520approach%2520where%2520the%2520backbone%2520is%2520initially%250Atrained%2520on%2520its%2520own%252C%2520followed%2520by%2520a%2520phase%2520where%2520both%2520the%2520backbone%2520and%2520the%2520exit%250Aheads%2520are%2520trained%2520together.%2520Thus%252C%2520we%2520advocate%2520for%2520organizing%2520early-exit%250Atraining%2520strategies%2520into%2520three%2520distinct%2520categories%252C%2520and%2520then%2520validate%2520them%2520for%250Atheir%2520performance%2520and%2520efficiency.%2520In%2520this%2520benchmark%252C%2520we%2520perform%2520both%250Atheoretical%2520and%2520empirical%2520analysis%2520of%2520early-exit%2520training%2520regimes.%2520We%2520study%2520the%250Amethods%2520in%2520terms%2520of%2520information%2520flow%252C%2520loss%2520landscape%2520and%2520numerical%2520rank%2520of%250Aactivations%2520and%2520gauge%2520the%2520suitability%2520of%2520regimes%2520for%2520various%2520architectures%2520and%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14320v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20or%20Disjoint%3A%20Mixing%20Training%20Regimes%20for%20Early-Exit%20Models&entry.906535625=Bart%C5%82omiej%20Krzepkowski%20and%20Monika%20Michaluk%20and%20Franciszek%20Szarwacki%20and%20Piotr%20Kubaty%20and%20Jary%20Pomponi%20and%20Tomasz%20Trzci%C5%84ski%20and%20Bartosz%20W%C3%B3jcik%20and%20Kamil%20Adamczewski&entry.1292438233=%20%20Early%20exits%20are%20an%20important%20efficiency%20mechanism%20integrated%20into%20deep%20neural%0Anetworks%20that%20allows%20for%20the%20termination%20of%20the%20network%27s%20forward%20pass%20before%0Aprocessing%20through%20all%20its%20layers.%20By%20allowing%20early%20halting%20of%20the%20inference%0Aprocess%20for%20less%20complex%20inputs%20that%20reached%20high%20confidence%2C%20early%20exits%0Asignificantly%20reduce%20the%20amount%20of%20computation%20required.%20Early%20exit%20methods%20add%0Atrainable%20internal%20classifiers%20which%20leads%20to%20more%20intricacy%20in%20the%20training%0Aprocess.%20However%2C%20there%20is%20no%20consistent%20verification%20of%20the%20approaches%20of%0Atraining%20of%20early%20exit%20methods%2C%20and%20no%20unified%20scheme%20of%20training%20such%20models.%0AMost%20early%20exit%20methods%20employ%20a%20training%20strategy%20that%20either%20simultaneously%0Atrains%20the%20backbone%20network%20and%20the%20exit%20heads%20or%20trains%20the%20exit%20heads%0Aseparately.%20We%20propose%20a%20training%20approach%20where%20the%20backbone%20is%20initially%0Atrained%20on%20its%20own%2C%20followed%20by%20a%20phase%20where%20both%20the%20backbone%20and%20the%20exit%0Aheads%20are%20trained%20together.%20Thus%2C%20we%20advocate%20for%20organizing%20early-exit%0Atraining%20strategies%20into%20three%20distinct%20categories%2C%20and%20then%20validate%20them%20for%0Atheir%20performance%20and%20efficiency.%20In%20this%20benchmark%2C%20we%20perform%20both%0Atheoretical%20and%20empirical%20analysis%20of%20early-exit%20training%20regimes.%20We%20study%20the%0Amethods%20in%20terms%20of%20information%20flow%2C%20loss%20landscape%20and%20numerical%20rank%20of%0Aactivations%20and%20gauge%20the%20suitability%20of%20regimes%20for%20various%20architectures%20and%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14320v1&entry.124074799=Read"},
{"title": "Exploring the Causality of End-to-End Autonomous Driving", "author": "Jiankun Li and Hao Li and Jiangjiang Liu and Zhikang Zou and Xiaoqing Ye and Fan Wang and Jizhou Huang and Hua Wu and Haifeng Wang", "abstract": "  Deep learning-based models are widely deployed in autonomous driving areas,\nespecially the increasingly noticed end-to-end solutions. However, the\nblack-box property of these models raises concerns about their trustworthiness\nand safety for autonomous driving, and how to debug the causality has become a\npressing concern. Despite some existing research on the explainability of\nautonomous driving, there is currently no systematic solution to help\nresearchers debug and identify the key factors that lead to the final predicted\naction of end-to-end autonomous driving. In this work, we propose a\ncomprehensive approach to explore and analyze the causality of end-to-end\nautonomous driving. First, we validate the essential information that the final\nplanning depends on by using controlled variables and counterfactual\ninterventions for qualitative analysis. Then, we quantitatively assess the\nfactors influencing model decisions by visualizing and statistically analyzing\nthe response of key model inputs. Finally, based on the comprehensive study of\nthe multi-factorial end-to-end autonomous driving system, we have developed a\nstrong baseline and a tool for exploring causality in the close-loop simulator\nCARLA. It leverages the essential input sources to obtain a well-designed\nmodel, resulting in highly competitive capabilities. As far as we know, our\nwork is the first to unveil the mystery of end-to-end autonomous driving and\nturn the black box into a white one. Thorough close-loop experiments\ndemonstrate that our method can be applied to end-to-end autonomous driving\nsolutions for causality debugging. Code will be available at\nhttps://github.com/bdvisl/DriveInsight.\n", "link": "http://arxiv.org/abs/2407.06546v2", "date": "2024-07-19", "relevancy": 2.0425, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5201}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5127}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Causality%20of%20End-to-End%20Autonomous%20Driving&body=Title%3A%20Exploring%20the%20Causality%20of%20End-to-End%20Autonomous%20Driving%0AAuthor%3A%20Jiankun%20Li%20and%20Hao%20Li%20and%20Jiangjiang%20Liu%20and%20Zhikang%20Zou%20and%20Xiaoqing%20Ye%20and%20Fan%20Wang%20and%20Jizhou%20Huang%20and%20Hua%20Wu%20and%20Haifeng%20Wang%0AAbstract%3A%20%20%20Deep%20learning-based%20models%20are%20widely%20deployed%20in%20autonomous%20driving%20areas%2C%0Aespecially%20the%20increasingly%20noticed%20end-to-end%20solutions.%20However%2C%20the%0Ablack-box%20property%20of%20these%20models%20raises%20concerns%20about%20their%20trustworthiness%0Aand%20safety%20for%20autonomous%20driving%2C%20and%20how%20to%20debug%20the%20causality%20has%20become%20a%0Apressing%20concern.%20Despite%20some%20existing%20research%20on%20the%20explainability%20of%0Aautonomous%20driving%2C%20there%20is%20currently%20no%20systematic%20solution%20to%20help%0Aresearchers%20debug%20and%20identify%20the%20key%20factors%20that%20lead%20to%20the%20final%20predicted%0Aaction%20of%20end-to-end%20autonomous%20driving.%20In%20this%20work%2C%20we%20propose%20a%0Acomprehensive%20approach%20to%20explore%20and%20analyze%20the%20causality%20of%20end-to-end%0Aautonomous%20driving.%20First%2C%20we%20validate%20the%20essential%20information%20that%20the%20final%0Aplanning%20depends%20on%20by%20using%20controlled%20variables%20and%20counterfactual%0Ainterventions%20for%20qualitative%20analysis.%20Then%2C%20we%20quantitatively%20assess%20the%0Afactors%20influencing%20model%20decisions%20by%20visualizing%20and%20statistically%20analyzing%0Athe%20response%20of%20key%20model%20inputs.%20Finally%2C%20based%20on%20the%20comprehensive%20study%20of%0Athe%20multi-factorial%20end-to-end%20autonomous%20driving%20system%2C%20we%20have%20developed%20a%0Astrong%20baseline%20and%20a%20tool%20for%20exploring%20causality%20in%20the%20close-loop%20simulator%0ACARLA.%20It%20leverages%20the%20essential%20input%20sources%20to%20obtain%20a%20well-designed%0Amodel%2C%20resulting%20in%20highly%20competitive%20capabilities.%20As%20far%20as%20we%20know%2C%20our%0Awork%20is%20the%20first%20to%20unveil%20the%20mystery%20of%20end-to-end%20autonomous%20driving%20and%0Aturn%20the%20black%20box%20into%20a%20white%20one.%20Thorough%20close-loop%20experiments%0Ademonstrate%20that%20our%20method%20can%20be%20applied%20to%20end-to-end%20autonomous%20driving%0Asolutions%20for%20causality%20debugging.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/bdvisl/DriveInsight.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06546v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Causality%2520of%2520End-to-End%2520Autonomous%2520Driving%26entry.906535625%3DJiankun%2520Li%2520and%2520Hao%2520Li%2520and%2520Jiangjiang%2520Liu%2520and%2520Zhikang%2520Zou%2520and%2520Xiaoqing%2520Ye%2520and%2520Fan%2520Wang%2520and%2520Jizhou%2520Huang%2520and%2520Hua%2520Wu%2520and%2520Haifeng%2520Wang%26entry.1292438233%3D%2520%2520Deep%2520learning-based%2520models%2520are%2520widely%2520deployed%2520in%2520autonomous%2520driving%2520areas%252C%250Aespecially%2520the%2520increasingly%2520noticed%2520end-to-end%2520solutions.%2520However%252C%2520the%250Ablack-box%2520property%2520of%2520these%2520models%2520raises%2520concerns%2520about%2520their%2520trustworthiness%250Aand%2520safety%2520for%2520autonomous%2520driving%252C%2520and%2520how%2520to%2520debug%2520the%2520causality%2520has%2520become%2520a%250Apressing%2520concern.%2520Despite%2520some%2520existing%2520research%2520on%2520the%2520explainability%2520of%250Aautonomous%2520driving%252C%2520there%2520is%2520currently%2520no%2520systematic%2520solution%2520to%2520help%250Aresearchers%2520debug%2520and%2520identify%2520the%2520key%2520factors%2520that%2520lead%2520to%2520the%2520final%2520predicted%250Aaction%2520of%2520end-to-end%2520autonomous%2520driving.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Acomprehensive%2520approach%2520to%2520explore%2520and%2520analyze%2520the%2520causality%2520of%2520end-to-end%250Aautonomous%2520driving.%2520First%252C%2520we%2520validate%2520the%2520essential%2520information%2520that%2520the%2520final%250Aplanning%2520depends%2520on%2520by%2520using%2520controlled%2520variables%2520and%2520counterfactual%250Ainterventions%2520for%2520qualitative%2520analysis.%2520Then%252C%2520we%2520quantitatively%2520assess%2520the%250Afactors%2520influencing%2520model%2520decisions%2520by%2520visualizing%2520and%2520statistically%2520analyzing%250Athe%2520response%2520of%2520key%2520model%2520inputs.%2520Finally%252C%2520based%2520on%2520the%2520comprehensive%2520study%2520of%250Athe%2520multi-factorial%2520end-to-end%2520autonomous%2520driving%2520system%252C%2520we%2520have%2520developed%2520a%250Astrong%2520baseline%2520and%2520a%2520tool%2520for%2520exploring%2520causality%2520in%2520the%2520close-loop%2520simulator%250ACARLA.%2520It%2520leverages%2520the%2520essential%2520input%2520sources%2520to%2520obtain%2520a%2520well-designed%250Amodel%252C%2520resulting%2520in%2520highly%2520competitive%2520capabilities.%2520As%2520far%2520as%2520we%2520know%252C%2520our%250Awork%2520is%2520the%2520first%2520to%2520unveil%2520the%2520mystery%2520of%2520end-to-end%2520autonomous%2520driving%2520and%250Aturn%2520the%2520black%2520box%2520into%2520a%2520white%2520one.%2520Thorough%2520close-loop%2520experiments%250Ademonstrate%2520that%2520our%2520method%2520can%2520be%2520applied%2520to%2520end-to-end%2520autonomous%2520driving%250Asolutions%2520for%2520causality%2520debugging.%2520Code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/bdvisl/DriveInsight.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06546v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Causality%20of%20End-to-End%20Autonomous%20Driving&entry.906535625=Jiankun%20Li%20and%20Hao%20Li%20and%20Jiangjiang%20Liu%20and%20Zhikang%20Zou%20and%20Xiaoqing%20Ye%20and%20Fan%20Wang%20and%20Jizhou%20Huang%20and%20Hua%20Wu%20and%20Haifeng%20Wang&entry.1292438233=%20%20Deep%20learning-based%20models%20are%20widely%20deployed%20in%20autonomous%20driving%20areas%2C%0Aespecially%20the%20increasingly%20noticed%20end-to-end%20solutions.%20However%2C%20the%0Ablack-box%20property%20of%20these%20models%20raises%20concerns%20about%20their%20trustworthiness%0Aand%20safety%20for%20autonomous%20driving%2C%20and%20how%20to%20debug%20the%20causality%20has%20become%20a%0Apressing%20concern.%20Despite%20some%20existing%20research%20on%20the%20explainability%20of%0Aautonomous%20driving%2C%20there%20is%20currently%20no%20systematic%20solution%20to%20help%0Aresearchers%20debug%20and%20identify%20the%20key%20factors%20that%20lead%20to%20the%20final%20predicted%0Aaction%20of%20end-to-end%20autonomous%20driving.%20In%20this%20work%2C%20we%20propose%20a%0Acomprehensive%20approach%20to%20explore%20and%20analyze%20the%20causality%20of%20end-to-end%0Aautonomous%20driving.%20First%2C%20we%20validate%20the%20essential%20information%20that%20the%20final%0Aplanning%20depends%20on%20by%20using%20controlled%20variables%20and%20counterfactual%0Ainterventions%20for%20qualitative%20analysis.%20Then%2C%20we%20quantitatively%20assess%20the%0Afactors%20influencing%20model%20decisions%20by%20visualizing%20and%20statistically%20analyzing%0Athe%20response%20of%20key%20model%20inputs.%20Finally%2C%20based%20on%20the%20comprehensive%20study%20of%0Athe%20multi-factorial%20end-to-end%20autonomous%20driving%20system%2C%20we%20have%20developed%20a%0Astrong%20baseline%20and%20a%20tool%20for%20exploring%20causality%20in%20the%20close-loop%20simulator%0ACARLA.%20It%20leverages%20the%20essential%20input%20sources%20to%20obtain%20a%20well-designed%0Amodel%2C%20resulting%20in%20highly%20competitive%20capabilities.%20As%20far%20as%20we%20know%2C%20our%0Awork%20is%20the%20first%20to%20unveil%20the%20mystery%20of%20end-to-end%20autonomous%20driving%20and%0Aturn%20the%20black%20box%20into%20a%20white%20one.%20Thorough%20close-loop%20experiments%0Ademonstrate%20that%20our%20method%20can%20be%20applied%20to%20end-to-end%20autonomous%20driving%0Asolutions%20for%20causality%20debugging.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/bdvisl/DriveInsight.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06546v2&entry.124074799=Read"},
{"title": "Deep Domain Adaptation Regression for Force Calibration of Optical\n  Tactile Sensors", "author": "Zhuo Chen and Ni Ou and Jiaqi Jiang and Shan Luo", "abstract": "  Optical tactile sensors provide robots with rich force information for robot\ngrasping in unstructured environments. The fast and accurate calibration of\nthree-dimensional contact forces holds significance for new sensors and\nexisting tactile sensors which may have incurred damage or aging. However, the\nconventional neural-network-based force calibration method necessitates a large\nvolume of force-labeled tactile images to minimize force prediction errors,\nwith the need for accurate Force/Torque measurement tools as well as a\ntime-consuming data collection process. To address this challenge, we propose a\nnovel deep domain-adaptation force calibration method, designed to transfer the\nforce prediction ability from a calibrated optical tactile sensor to\nuncalibrated ones with various combinations of domain gaps, including marker\npresence, illumination condition, and elastomer modulus. Experimental results\nshow the effectiveness of the proposed unsupervised force calibration method,\nwith lowest force prediction errors of 0.102N (3.4\\% in full force range) for\nnormal force, and 0.095N (6.3\\%) and 0.062N (4.1\\%) for shear forces along the\nx-axis and y-axis, respectively. This study presents a promising, general force\ncalibration methodology for optical tactile sensors.\n", "link": "http://arxiv.org/abs/2407.14380v1", "date": "2024-07-19", "relevancy": 2.0304, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5664}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5061}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Domain%20Adaptation%20Regression%20for%20Force%20Calibration%20of%20Optical%0A%20%20Tactile%20Sensors&body=Title%3A%20Deep%20Domain%20Adaptation%20Regression%20for%20Force%20Calibration%20of%20Optical%0A%20%20Tactile%20Sensors%0AAuthor%3A%20Zhuo%20Chen%20and%20Ni%20Ou%20and%20Jiaqi%20Jiang%20and%20Shan%20Luo%0AAbstract%3A%20%20%20Optical%20tactile%20sensors%20provide%20robots%20with%20rich%20force%20information%20for%20robot%0Agrasping%20in%20unstructured%20environments.%20The%20fast%20and%20accurate%20calibration%20of%0Athree-dimensional%20contact%20forces%20holds%20significance%20for%20new%20sensors%20and%0Aexisting%20tactile%20sensors%20which%20may%20have%20incurred%20damage%20or%20aging.%20However%2C%20the%0Aconventional%20neural-network-based%20force%20calibration%20method%20necessitates%20a%20large%0Avolume%20of%20force-labeled%20tactile%20images%20to%20minimize%20force%20prediction%20errors%2C%0Awith%20the%20need%20for%20accurate%20Force/Torque%20measurement%20tools%20as%20well%20as%20a%0Atime-consuming%20data%20collection%20process.%20To%20address%20this%20challenge%2C%20we%20propose%20a%0Anovel%20deep%20domain-adaptation%20force%20calibration%20method%2C%20designed%20to%20transfer%20the%0Aforce%20prediction%20ability%20from%20a%20calibrated%20optical%20tactile%20sensor%20to%0Auncalibrated%20ones%20with%20various%20combinations%20of%20domain%20gaps%2C%20including%20marker%0Apresence%2C%20illumination%20condition%2C%20and%20elastomer%20modulus.%20Experimental%20results%0Ashow%20the%20effectiveness%20of%20the%20proposed%20unsupervised%20force%20calibration%20method%2C%0Awith%20lowest%20force%20prediction%20errors%20of%200.102N%20%283.4%5C%25%20in%20full%20force%20range%29%20for%0Anormal%20force%2C%20and%200.095N%20%286.3%5C%25%29%20and%200.062N%20%284.1%5C%25%29%20for%20shear%20forces%20along%20the%0Ax-axis%20and%20y-axis%2C%20respectively.%20This%20study%20presents%20a%20promising%2C%20general%20force%0Acalibration%20methodology%20for%20optical%20tactile%20sensors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14380v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Domain%2520Adaptation%2520Regression%2520for%2520Force%2520Calibration%2520of%2520Optical%250A%2520%2520Tactile%2520Sensors%26entry.906535625%3DZhuo%2520Chen%2520and%2520Ni%2520Ou%2520and%2520Jiaqi%2520Jiang%2520and%2520Shan%2520Luo%26entry.1292438233%3D%2520%2520Optical%2520tactile%2520sensors%2520provide%2520robots%2520with%2520rich%2520force%2520information%2520for%2520robot%250Agrasping%2520in%2520unstructured%2520environments.%2520The%2520fast%2520and%2520accurate%2520calibration%2520of%250Athree-dimensional%2520contact%2520forces%2520holds%2520significance%2520for%2520new%2520sensors%2520and%250Aexisting%2520tactile%2520sensors%2520which%2520may%2520have%2520incurred%2520damage%2520or%2520aging.%2520However%252C%2520the%250Aconventional%2520neural-network-based%2520force%2520calibration%2520method%2520necessitates%2520a%2520large%250Avolume%2520of%2520force-labeled%2520tactile%2520images%2520to%2520minimize%2520force%2520prediction%2520errors%252C%250Awith%2520the%2520need%2520for%2520accurate%2520Force/Torque%2520measurement%2520tools%2520as%2520well%2520as%2520a%250Atime-consuming%2520data%2520collection%2520process.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%250Anovel%2520deep%2520domain-adaptation%2520force%2520calibration%2520method%252C%2520designed%2520to%2520transfer%2520the%250Aforce%2520prediction%2520ability%2520from%2520a%2520calibrated%2520optical%2520tactile%2520sensor%2520to%250Auncalibrated%2520ones%2520with%2520various%2520combinations%2520of%2520domain%2520gaps%252C%2520including%2520marker%250Apresence%252C%2520illumination%2520condition%252C%2520and%2520elastomer%2520modulus.%2520Experimental%2520results%250Ashow%2520the%2520effectiveness%2520of%2520the%2520proposed%2520unsupervised%2520force%2520calibration%2520method%252C%250Awith%2520lowest%2520force%2520prediction%2520errors%2520of%25200.102N%2520%25283.4%255C%2525%2520in%2520full%2520force%2520range%2529%2520for%250Anormal%2520force%252C%2520and%25200.095N%2520%25286.3%255C%2525%2529%2520and%25200.062N%2520%25284.1%255C%2525%2529%2520for%2520shear%2520forces%2520along%2520the%250Ax-axis%2520and%2520y-axis%252C%2520respectively.%2520This%2520study%2520presents%2520a%2520promising%252C%2520general%2520force%250Acalibration%2520methodology%2520for%2520optical%2520tactile%2520sensors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14380v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Domain%20Adaptation%20Regression%20for%20Force%20Calibration%20of%20Optical%0A%20%20Tactile%20Sensors&entry.906535625=Zhuo%20Chen%20and%20Ni%20Ou%20and%20Jiaqi%20Jiang%20and%20Shan%20Luo&entry.1292438233=%20%20Optical%20tactile%20sensors%20provide%20robots%20with%20rich%20force%20information%20for%20robot%0Agrasping%20in%20unstructured%20environments.%20The%20fast%20and%20accurate%20calibration%20of%0Athree-dimensional%20contact%20forces%20holds%20significance%20for%20new%20sensors%20and%0Aexisting%20tactile%20sensors%20which%20may%20have%20incurred%20damage%20or%20aging.%20However%2C%20the%0Aconventional%20neural-network-based%20force%20calibration%20method%20necessitates%20a%20large%0Avolume%20of%20force-labeled%20tactile%20images%20to%20minimize%20force%20prediction%20errors%2C%0Awith%20the%20need%20for%20accurate%20Force/Torque%20measurement%20tools%20as%20well%20as%20a%0Atime-consuming%20data%20collection%20process.%20To%20address%20this%20challenge%2C%20we%20propose%20a%0Anovel%20deep%20domain-adaptation%20force%20calibration%20method%2C%20designed%20to%20transfer%20the%0Aforce%20prediction%20ability%20from%20a%20calibrated%20optical%20tactile%20sensor%20to%0Auncalibrated%20ones%20with%20various%20combinations%20of%20domain%20gaps%2C%20including%20marker%0Apresence%2C%20illumination%20condition%2C%20and%20elastomer%20modulus.%20Experimental%20results%0Ashow%20the%20effectiveness%20of%20the%20proposed%20unsupervised%20force%20calibration%20method%2C%0Awith%20lowest%20force%20prediction%20errors%20of%200.102N%20%283.4%5C%25%20in%20full%20force%20range%29%20for%0Anormal%20force%2C%20and%200.095N%20%286.3%5C%25%29%20and%200.062N%20%284.1%5C%25%29%20for%20shear%20forces%20along%20the%0Ax-axis%20and%20y-axis%2C%20respectively.%20This%20study%20presents%20a%20promising%2C%20general%20force%0Acalibration%20methodology%20for%20optical%20tactile%20sensors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14380v1&entry.124074799=Read"},
{"title": "Relational Representation Distillation", "author": "Nikolaos Giakoumoglou and Tania Stathaki", "abstract": "  Knowledge distillation (KD) is an effective method for transferring knowledge\nfrom a large, well-trained teacher model to a smaller, more efficient student\nmodel. Despite its success, one of the main challenges in KD is ensuring the\nefficient transfer of complex knowledge while maintaining the student's\ncomputational efficiency. Unlike previous works that applied contrastive\nobjectives promoting explicit negative instances, we introduce Relational\nRepresentation Distillation (RRD). Our approach leverages pairwise similarities\nto explore and reinforce the relationships between the teacher and student\nmodels. Inspired by self-supervised learning principles, it uses a relaxed\ncontrastive loss that focuses on similarity rather than exact replication. This\nmethod aligns the output distributions of teacher samples in a large memory\nbuffer, improving the robustness and performance of the student model without\nthe need for strict negative instance differentiation. Our approach\ndemonstrates superior performance on CIFAR-100, outperforming traditional KD\ntechniques and surpassing 13 state-of-the-art methods. It also transfers\nsuccessfully to other datasets like Tiny ImageNet and STL-10. The code will be\nmade public soon.\n", "link": "http://arxiv.org/abs/2407.12073v2", "date": "2024-07-19", "relevancy": 2.0215, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.512}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5013}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relational%20Representation%20Distillation&body=Title%3A%20Relational%20Representation%20Distillation%0AAuthor%3A%20Nikolaos%20Giakoumoglou%20and%20Tania%20Stathaki%0AAbstract%3A%20%20%20Knowledge%20distillation%20%28KD%29%20is%20an%20effective%20method%20for%20transferring%20knowledge%0Afrom%20a%20large%2C%20well-trained%20teacher%20model%20to%20a%20smaller%2C%20more%20efficient%20student%0Amodel.%20Despite%20its%20success%2C%20one%20of%20the%20main%20challenges%20in%20KD%20is%20ensuring%20the%0Aefficient%20transfer%20of%20complex%20knowledge%20while%20maintaining%20the%20student%27s%0Acomputational%20efficiency.%20Unlike%20previous%20works%20that%20applied%20contrastive%0Aobjectives%20promoting%20explicit%20negative%20instances%2C%20we%20introduce%20Relational%0ARepresentation%20Distillation%20%28RRD%29.%20Our%20approach%20leverages%20pairwise%20similarities%0Ato%20explore%20and%20reinforce%20the%20relationships%20between%20the%20teacher%20and%20student%0Amodels.%20Inspired%20by%20self-supervised%20learning%20principles%2C%20it%20uses%20a%20relaxed%0Acontrastive%20loss%20that%20focuses%20on%20similarity%20rather%20than%20exact%20replication.%20This%0Amethod%20aligns%20the%20output%20distributions%20of%20teacher%20samples%20in%20a%20large%20memory%0Abuffer%2C%20improving%20the%20robustness%20and%20performance%20of%20the%20student%20model%20without%0Athe%20need%20for%20strict%20negative%20instance%20differentiation.%20Our%20approach%0Ademonstrates%20superior%20performance%20on%20CIFAR-100%2C%20outperforming%20traditional%20KD%0Atechniques%20and%20surpassing%2013%20state-of-the-art%20methods.%20It%20also%20transfers%0Asuccessfully%20to%20other%20datasets%20like%20Tiny%20ImageNet%20and%20STL-10.%20The%20code%20will%20be%0Amade%20public%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12073v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelational%2520Representation%2520Distillation%26entry.906535625%3DNikolaos%2520Giakoumoglou%2520and%2520Tania%2520Stathaki%26entry.1292438233%3D%2520%2520Knowledge%2520distillation%2520%2528KD%2529%2520is%2520an%2520effective%2520method%2520for%2520transferring%2520knowledge%250Afrom%2520a%2520large%252C%2520well-trained%2520teacher%2520model%2520to%2520a%2520smaller%252C%2520more%2520efficient%2520student%250Amodel.%2520Despite%2520its%2520success%252C%2520one%2520of%2520the%2520main%2520challenges%2520in%2520KD%2520is%2520ensuring%2520the%250Aefficient%2520transfer%2520of%2520complex%2520knowledge%2520while%2520maintaining%2520the%2520student%2527s%250Acomputational%2520efficiency.%2520Unlike%2520previous%2520works%2520that%2520applied%2520contrastive%250Aobjectives%2520promoting%2520explicit%2520negative%2520instances%252C%2520we%2520introduce%2520Relational%250ARepresentation%2520Distillation%2520%2528RRD%2529.%2520Our%2520approach%2520leverages%2520pairwise%2520similarities%250Ato%2520explore%2520and%2520reinforce%2520the%2520relationships%2520between%2520the%2520teacher%2520and%2520student%250Amodels.%2520Inspired%2520by%2520self-supervised%2520learning%2520principles%252C%2520it%2520uses%2520a%2520relaxed%250Acontrastive%2520loss%2520that%2520focuses%2520on%2520similarity%2520rather%2520than%2520exact%2520replication.%2520This%250Amethod%2520aligns%2520the%2520output%2520distributions%2520of%2520teacher%2520samples%2520in%2520a%2520large%2520memory%250Abuffer%252C%2520improving%2520the%2520robustness%2520and%2520performance%2520of%2520the%2520student%2520model%2520without%250Athe%2520need%2520for%2520strict%2520negative%2520instance%2520differentiation.%2520Our%2520approach%250Ademonstrates%2520superior%2520performance%2520on%2520CIFAR-100%252C%2520outperforming%2520traditional%2520KD%250Atechniques%2520and%2520surpassing%252013%2520state-of-the-art%2520methods.%2520It%2520also%2520transfers%250Asuccessfully%2520to%2520other%2520datasets%2520like%2520Tiny%2520ImageNet%2520and%2520STL-10.%2520The%2520code%2520will%2520be%250Amade%2520public%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12073v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relational%20Representation%20Distillation&entry.906535625=Nikolaos%20Giakoumoglou%20and%20Tania%20Stathaki&entry.1292438233=%20%20Knowledge%20distillation%20%28KD%29%20is%20an%20effective%20method%20for%20transferring%20knowledge%0Afrom%20a%20large%2C%20well-trained%20teacher%20model%20to%20a%20smaller%2C%20more%20efficient%20student%0Amodel.%20Despite%20its%20success%2C%20one%20of%20the%20main%20challenges%20in%20KD%20is%20ensuring%20the%0Aefficient%20transfer%20of%20complex%20knowledge%20while%20maintaining%20the%20student%27s%0Acomputational%20efficiency.%20Unlike%20previous%20works%20that%20applied%20contrastive%0Aobjectives%20promoting%20explicit%20negative%20instances%2C%20we%20introduce%20Relational%0ARepresentation%20Distillation%20%28RRD%29.%20Our%20approach%20leverages%20pairwise%20similarities%0Ato%20explore%20and%20reinforce%20the%20relationships%20between%20the%20teacher%20and%20student%0Amodels.%20Inspired%20by%20self-supervised%20learning%20principles%2C%20it%20uses%20a%20relaxed%0Acontrastive%20loss%20that%20focuses%20on%20similarity%20rather%20than%20exact%20replication.%20This%0Amethod%20aligns%20the%20output%20distributions%20of%20teacher%20samples%20in%20a%20large%20memory%0Abuffer%2C%20improving%20the%20robustness%20and%20performance%20of%20the%20student%20model%20without%0Athe%20need%20for%20strict%20negative%20instance%20differentiation.%20Our%20approach%0Ademonstrates%20superior%20performance%20on%20CIFAR-100%2C%20outperforming%20traditional%20KD%0Atechniques%20and%20surpassing%2013%20state-of-the-art%20methods.%20It%20also%20transfers%0Asuccessfully%20to%20other%20datasets%20like%20Tiny%20ImageNet%20and%20STL-10.%20The%20code%20will%20be%0Amade%20public%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12073v2&entry.124074799=Read"},
{"title": "DEAL: Disentangle and Localize Concept-level Explanations for VLMs", "author": "Tang Li and Mengmeng Ma and Xi Peng", "abstract": "  Large pre-trained Vision-Language Models (VLMs) have become ubiquitous\nfoundational components of other models and downstream tasks. Although\npowerful, our empirical results reveal that such models might not be able to\nidentify fine-grained concepts. Specifically, the explanations of VLMs with\nrespect to fine-grained concepts are entangled and mislocalized. To address\nthis issue, we propose to DisEntAngle and Localize (DEAL) the concept-level\nexplanations for VLMs without human annotations. The key idea is encouraging\nthe concept-level explanations to be distinct while maintaining consistency\nwith category-level explanations. We conduct extensive experiments and ablation\nstudies on a wide range of benchmark datasets and vision-language models. Our\nempirical results demonstrate that the proposed method significantly improves\nthe concept-level explanations of the model in terms of disentanglability and\nlocalizability. Surprisingly, the improved explainability alleviates the\nmodel's reliance on spurious correlations, which further benefits the\nprediction accuracy.\n", "link": "http://arxiv.org/abs/2407.14412v1", "date": "2024-07-19", "relevancy": 2.0087, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5107}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5018}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DEAL%3A%20Disentangle%20and%20Localize%20Concept-level%20Explanations%20for%20VLMs&body=Title%3A%20DEAL%3A%20Disentangle%20and%20Localize%20Concept-level%20Explanations%20for%20VLMs%0AAuthor%3A%20Tang%20Li%20and%20Mengmeng%20Ma%20and%20Xi%20Peng%0AAbstract%3A%20%20%20Large%20pre-trained%20Vision-Language%20Models%20%28VLMs%29%20have%20become%20ubiquitous%0Afoundational%20components%20of%20other%20models%20and%20downstream%20tasks.%20Although%0Apowerful%2C%20our%20empirical%20results%20reveal%20that%20such%20models%20might%20not%20be%20able%20to%0Aidentify%20fine-grained%20concepts.%20Specifically%2C%20the%20explanations%20of%20VLMs%20with%0Arespect%20to%20fine-grained%20concepts%20are%20entangled%20and%20mislocalized.%20To%20address%0Athis%20issue%2C%20we%20propose%20to%20DisEntAngle%20and%20Localize%20%28DEAL%29%20the%20concept-level%0Aexplanations%20for%20VLMs%20without%20human%20annotations.%20The%20key%20idea%20is%20encouraging%0Athe%20concept-level%20explanations%20to%20be%20distinct%20while%20maintaining%20consistency%0Awith%20category-level%20explanations.%20We%20conduct%20extensive%20experiments%20and%20ablation%0Astudies%20on%20a%20wide%20range%20of%20benchmark%20datasets%20and%20vision-language%20models.%20Our%0Aempirical%20results%20demonstrate%20that%20the%20proposed%20method%20significantly%20improves%0Athe%20concept-level%20explanations%20of%20the%20model%20in%20terms%20of%20disentanglability%20and%0Alocalizability.%20Surprisingly%2C%20the%20improved%20explainability%20alleviates%20the%0Amodel%27s%20reliance%20on%20spurious%20correlations%2C%20which%20further%20benefits%20the%0Aprediction%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14412v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDEAL%253A%2520Disentangle%2520and%2520Localize%2520Concept-level%2520Explanations%2520for%2520VLMs%26entry.906535625%3DTang%2520Li%2520and%2520Mengmeng%2520Ma%2520and%2520Xi%2520Peng%26entry.1292438233%3D%2520%2520Large%2520pre-trained%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520become%2520ubiquitous%250Afoundational%2520components%2520of%2520other%2520models%2520and%2520downstream%2520tasks.%2520Although%250Apowerful%252C%2520our%2520empirical%2520results%2520reveal%2520that%2520such%2520models%2520might%2520not%2520be%2520able%2520to%250Aidentify%2520fine-grained%2520concepts.%2520Specifically%252C%2520the%2520explanations%2520of%2520VLMs%2520with%250Arespect%2520to%2520fine-grained%2520concepts%2520are%2520entangled%2520and%2520mislocalized.%2520To%2520address%250Athis%2520issue%252C%2520we%2520propose%2520to%2520DisEntAngle%2520and%2520Localize%2520%2528DEAL%2529%2520the%2520concept-level%250Aexplanations%2520for%2520VLMs%2520without%2520human%2520annotations.%2520The%2520key%2520idea%2520is%2520encouraging%250Athe%2520concept-level%2520explanations%2520to%2520be%2520distinct%2520while%2520maintaining%2520consistency%250Awith%2520category-level%2520explanations.%2520We%2520conduct%2520extensive%2520experiments%2520and%2520ablation%250Astudies%2520on%2520a%2520wide%2520range%2520of%2520benchmark%2520datasets%2520and%2520vision-language%2520models.%2520Our%250Aempirical%2520results%2520demonstrate%2520that%2520the%2520proposed%2520method%2520significantly%2520improves%250Athe%2520concept-level%2520explanations%2520of%2520the%2520model%2520in%2520terms%2520of%2520disentanglability%2520and%250Alocalizability.%2520Surprisingly%252C%2520the%2520improved%2520explainability%2520alleviates%2520the%250Amodel%2527s%2520reliance%2520on%2520spurious%2520correlations%252C%2520which%2520further%2520benefits%2520the%250Aprediction%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14412v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DEAL%3A%20Disentangle%20and%20Localize%20Concept-level%20Explanations%20for%20VLMs&entry.906535625=Tang%20Li%20and%20Mengmeng%20Ma%20and%20Xi%20Peng&entry.1292438233=%20%20Large%20pre-trained%20Vision-Language%20Models%20%28VLMs%29%20have%20become%20ubiquitous%0Afoundational%20components%20of%20other%20models%20and%20downstream%20tasks.%20Although%0Apowerful%2C%20our%20empirical%20results%20reveal%20that%20such%20models%20might%20not%20be%20able%20to%0Aidentify%20fine-grained%20concepts.%20Specifically%2C%20the%20explanations%20of%20VLMs%20with%0Arespect%20to%20fine-grained%20concepts%20are%20entangled%20and%20mislocalized.%20To%20address%0Athis%20issue%2C%20we%20propose%20to%20DisEntAngle%20and%20Localize%20%28DEAL%29%20the%20concept-level%0Aexplanations%20for%20VLMs%20without%20human%20annotations.%20The%20key%20idea%20is%20encouraging%0Athe%20concept-level%20explanations%20to%20be%20distinct%20while%20maintaining%20consistency%0Awith%20category-level%20explanations.%20We%20conduct%20extensive%20experiments%20and%20ablation%0Astudies%20on%20a%20wide%20range%20of%20benchmark%20datasets%20and%20vision-language%20models.%20Our%0Aempirical%20results%20demonstrate%20that%20the%20proposed%20method%20significantly%20improves%0Athe%20concept-level%20explanations%20of%20the%20model%20in%20terms%20of%20disentanglability%20and%0Alocalizability.%20Surprisingly%2C%20the%20improved%20explainability%20alleviates%20the%0Amodel%27s%20reliance%20on%20spurious%20correlations%2C%20which%20further%20benefits%20the%0Aprediction%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14412v1&entry.124074799=Read"},
{"title": "Dataset Distillation by Automatic Training Trajectories", "author": "Dai Liu and Jindong Gu and Hu Cao and Carsten Trinitis and Martin Schulz", "abstract": "  Dataset Distillation is used to create a concise, yet informative, synthetic\ndataset that can replace the original dataset for training purposes. Some\nleading methods in this domain prioritize long-range matching, involving the\nunrolling of training trajectories with a fixed number of steps (NS) on the\nsynthetic dataset to align with various expert training trajectories. However,\ntraditional long-range matching methods possess an overfitting-like problem,\nthe fixed step size NS forces synthetic dataset to distortedly conform seen\nexpert training trajectories, resulting in a loss of generality-especially to\nthose from unencountered architecture. We refer to this as the Accumulated\nMismatching Problem (AMP), and propose a new approach, Automatic Training\nTrajectories (ATT), which dynamically and adaptively adjusts trajectory length\nNS to address the AMP. Our method outperforms existing methods particularly in\ntests involving cross-architectures. Moreover, owing to its adaptive nature, it\nexhibits enhanced stability in the face of parameter variations.\n", "link": "http://arxiv.org/abs/2407.14245v1", "date": "2024-07-19", "relevancy": 2.0083, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5168}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4944}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dataset%20Distillation%20by%20Automatic%20Training%20Trajectories&body=Title%3A%20Dataset%20Distillation%20by%20Automatic%20Training%20Trajectories%0AAuthor%3A%20Dai%20Liu%20and%20Jindong%20Gu%20and%20Hu%20Cao%20and%20Carsten%20Trinitis%20and%20Martin%20Schulz%0AAbstract%3A%20%20%20Dataset%20Distillation%20is%20used%20to%20create%20a%20concise%2C%20yet%20informative%2C%20synthetic%0Adataset%20that%20can%20replace%20the%20original%20dataset%20for%20training%20purposes.%20Some%0Aleading%20methods%20in%20this%20domain%20prioritize%20long-range%20matching%2C%20involving%20the%0Aunrolling%20of%20training%20trajectories%20with%20a%20fixed%20number%20of%20steps%20%28NS%29%20on%20the%0Asynthetic%20dataset%20to%20align%20with%20various%20expert%20training%20trajectories.%20However%2C%0Atraditional%20long-range%20matching%20methods%20possess%20an%20overfitting-like%20problem%2C%0Athe%20fixed%20step%20size%20NS%20forces%20synthetic%20dataset%20to%20distortedly%20conform%20seen%0Aexpert%20training%20trajectories%2C%20resulting%20in%20a%20loss%20of%20generality-especially%20to%0Athose%20from%20unencountered%20architecture.%20We%20refer%20to%20this%20as%20the%20Accumulated%0AMismatching%20Problem%20%28AMP%29%2C%20and%20propose%20a%20new%20approach%2C%20Automatic%20Training%0ATrajectories%20%28ATT%29%2C%20which%20dynamically%20and%20adaptively%20adjusts%20trajectory%20length%0ANS%20to%20address%20the%20AMP.%20Our%20method%20outperforms%20existing%20methods%20particularly%20in%0Atests%20involving%20cross-architectures.%20Moreover%2C%20owing%20to%20its%20adaptive%20nature%2C%20it%0Aexhibits%20enhanced%20stability%20in%20the%20face%20of%20parameter%20variations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14245v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDataset%2520Distillation%2520by%2520Automatic%2520Training%2520Trajectories%26entry.906535625%3DDai%2520Liu%2520and%2520Jindong%2520Gu%2520and%2520Hu%2520Cao%2520and%2520Carsten%2520Trinitis%2520and%2520Martin%2520Schulz%26entry.1292438233%3D%2520%2520Dataset%2520Distillation%2520is%2520used%2520to%2520create%2520a%2520concise%252C%2520yet%2520informative%252C%2520synthetic%250Adataset%2520that%2520can%2520replace%2520the%2520original%2520dataset%2520for%2520training%2520purposes.%2520Some%250Aleading%2520methods%2520in%2520this%2520domain%2520prioritize%2520long-range%2520matching%252C%2520involving%2520the%250Aunrolling%2520of%2520training%2520trajectories%2520with%2520a%2520fixed%2520number%2520of%2520steps%2520%2528NS%2529%2520on%2520the%250Asynthetic%2520dataset%2520to%2520align%2520with%2520various%2520expert%2520training%2520trajectories.%2520However%252C%250Atraditional%2520long-range%2520matching%2520methods%2520possess%2520an%2520overfitting-like%2520problem%252C%250Athe%2520fixed%2520step%2520size%2520NS%2520forces%2520synthetic%2520dataset%2520to%2520distortedly%2520conform%2520seen%250Aexpert%2520training%2520trajectories%252C%2520resulting%2520in%2520a%2520loss%2520of%2520generality-especially%2520to%250Athose%2520from%2520unencountered%2520architecture.%2520We%2520refer%2520to%2520this%2520as%2520the%2520Accumulated%250AMismatching%2520Problem%2520%2528AMP%2529%252C%2520and%2520propose%2520a%2520new%2520approach%252C%2520Automatic%2520Training%250ATrajectories%2520%2528ATT%2529%252C%2520which%2520dynamically%2520and%2520adaptively%2520adjusts%2520trajectory%2520length%250ANS%2520to%2520address%2520the%2520AMP.%2520Our%2520method%2520outperforms%2520existing%2520methods%2520particularly%2520in%250Atests%2520involving%2520cross-architectures.%2520Moreover%252C%2520owing%2520to%2520its%2520adaptive%2520nature%252C%2520it%250Aexhibits%2520enhanced%2520stability%2520in%2520the%2520face%2520of%2520parameter%2520variations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14245v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dataset%20Distillation%20by%20Automatic%20Training%20Trajectories&entry.906535625=Dai%20Liu%20and%20Jindong%20Gu%20and%20Hu%20Cao%20and%20Carsten%20Trinitis%20and%20Martin%20Schulz&entry.1292438233=%20%20Dataset%20Distillation%20is%20used%20to%20create%20a%20concise%2C%20yet%20informative%2C%20synthetic%0Adataset%20that%20can%20replace%20the%20original%20dataset%20for%20training%20purposes.%20Some%0Aleading%20methods%20in%20this%20domain%20prioritize%20long-range%20matching%2C%20involving%20the%0Aunrolling%20of%20training%20trajectories%20with%20a%20fixed%20number%20of%20steps%20%28NS%29%20on%20the%0Asynthetic%20dataset%20to%20align%20with%20various%20expert%20training%20trajectories.%20However%2C%0Atraditional%20long-range%20matching%20methods%20possess%20an%20overfitting-like%20problem%2C%0Athe%20fixed%20step%20size%20NS%20forces%20synthetic%20dataset%20to%20distortedly%20conform%20seen%0Aexpert%20training%20trajectories%2C%20resulting%20in%20a%20loss%20of%20generality-especially%20to%0Athose%20from%20unencountered%20architecture.%20We%20refer%20to%20this%20as%20the%20Accumulated%0AMismatching%20Problem%20%28AMP%29%2C%20and%20propose%20a%20new%20approach%2C%20Automatic%20Training%0ATrajectories%20%28ATT%29%2C%20which%20dynamically%20and%20adaptively%20adjusts%20trajectory%20length%0ANS%20to%20address%20the%20AMP.%20Our%20method%20outperforms%20existing%20methods%20particularly%20in%0Atests%20involving%20cross-architectures.%20Moreover%2C%20owing%20to%20its%20adaptive%20nature%2C%20it%0Aexhibits%20enhanced%20stability%20in%20the%20face%20of%20parameter%20variations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14245v1&entry.124074799=Read"},
{"title": "Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated\n  Concept Discovery", "author": "Sukrut Rao and Sweta Mahajan and Moritz B\u00f6hle and Bernt Schiele", "abstract": "  Concept Bottleneck Models (CBMs) have recently been proposed to address the\n'black-box' problem of deep neural networks, by first mapping images to a\nhuman-understandable concept space and then linearly combining concepts for\nclassification. Such models typically require first coming up with a set of\nconcepts relevant to the task and then aligning the representations of a\nfeature extractor to map to these concepts. However, even with powerful\nfoundational feature extractors like CLIP, there are no guarantees that the\nspecified concepts are detectable. In this work, we leverage recent advances in\nmechanistic interpretability and propose a novel CBM approach -- called\nDiscover-then-Name-CBM (DN-CBM) -- that inverts the typical paradigm: instead\nof pre-selecting concepts based on the downstream classification task, we use\nsparse autoencoders to first discover concepts learnt by the model, and then\nname them and train linear probes for classification. Our concept extraction\nstrategy is efficient, since it is agnostic to the downstream task, and uses\nconcepts already known to the model. We perform a comprehensive evaluation\nacross multiple datasets and CLIP architectures and show that our method yields\nsemantically meaningful concepts, assigns appropriate names to them that make\nthem easy to interpret, and yields performant and interpretable CBMs. Code\navailable at https://github.com/neuroexplicit-saar/discover-then-name.\n", "link": "http://arxiv.org/abs/2407.14499v1", "date": "2024-07-19", "relevancy": 1.9957, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5082}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4941}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discover-then-Name%3A%20Task-Agnostic%20Concept%20Bottlenecks%20via%20Automated%0A%20%20Concept%20Discovery&body=Title%3A%20Discover-then-Name%3A%20Task-Agnostic%20Concept%20Bottlenecks%20via%20Automated%0A%20%20Concept%20Discovery%0AAuthor%3A%20Sukrut%20Rao%20and%20Sweta%20Mahajan%20and%20Moritz%20B%C3%B6hle%20and%20Bernt%20Schiele%0AAbstract%3A%20%20%20Concept%20Bottleneck%20Models%20%28CBMs%29%20have%20recently%20been%20proposed%20to%20address%20the%0A%27black-box%27%20problem%20of%20deep%20neural%20networks%2C%20by%20first%20mapping%20images%20to%20a%0Ahuman-understandable%20concept%20space%20and%20then%20linearly%20combining%20concepts%20for%0Aclassification.%20Such%20models%20typically%20require%20first%20coming%20up%20with%20a%20set%20of%0Aconcepts%20relevant%20to%20the%20task%20and%20then%20aligning%20the%20representations%20of%20a%0Afeature%20extractor%20to%20map%20to%20these%20concepts.%20However%2C%20even%20with%20powerful%0Afoundational%20feature%20extractors%20like%20CLIP%2C%20there%20are%20no%20guarantees%20that%20the%0Aspecified%20concepts%20are%20detectable.%20In%20this%20work%2C%20we%20leverage%20recent%20advances%20in%0Amechanistic%20interpretability%20and%20propose%20a%20novel%20CBM%20approach%20--%20called%0ADiscover-then-Name-CBM%20%28DN-CBM%29%20--%20that%20inverts%20the%20typical%20paradigm%3A%20instead%0Aof%20pre-selecting%20concepts%20based%20on%20the%20downstream%20classification%20task%2C%20we%20use%0Asparse%20autoencoders%20to%20first%20discover%20concepts%20learnt%20by%20the%20model%2C%20and%20then%0Aname%20them%20and%20train%20linear%20probes%20for%20classification.%20Our%20concept%20extraction%0Astrategy%20is%20efficient%2C%20since%20it%20is%20agnostic%20to%20the%20downstream%20task%2C%20and%20uses%0Aconcepts%20already%20known%20to%20the%20model.%20We%20perform%20a%20comprehensive%20evaluation%0Aacross%20multiple%20datasets%20and%20CLIP%20architectures%20and%20show%20that%20our%20method%20yields%0Asemantically%20meaningful%20concepts%2C%20assigns%20appropriate%20names%20to%20them%20that%20make%0Athem%20easy%20to%20interpret%2C%20and%20yields%20performant%20and%20interpretable%20CBMs.%20Code%0Aavailable%20at%20https%3A//github.com/neuroexplicit-saar/discover-then-name.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14499v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscover-then-Name%253A%2520Task-Agnostic%2520Concept%2520Bottlenecks%2520via%2520Automated%250A%2520%2520Concept%2520Discovery%26entry.906535625%3DSukrut%2520Rao%2520and%2520Sweta%2520Mahajan%2520and%2520Moritz%2520B%25C3%25B6hle%2520and%2520Bernt%2520Schiele%26entry.1292438233%3D%2520%2520Concept%2520Bottleneck%2520Models%2520%2528CBMs%2529%2520have%2520recently%2520been%2520proposed%2520to%2520address%2520the%250A%2527black-box%2527%2520problem%2520of%2520deep%2520neural%2520networks%252C%2520by%2520first%2520mapping%2520images%2520to%2520a%250Ahuman-understandable%2520concept%2520space%2520and%2520then%2520linearly%2520combining%2520concepts%2520for%250Aclassification.%2520Such%2520models%2520typically%2520require%2520first%2520coming%2520up%2520with%2520a%2520set%2520of%250Aconcepts%2520relevant%2520to%2520the%2520task%2520and%2520then%2520aligning%2520the%2520representations%2520of%2520a%250Afeature%2520extractor%2520to%2520map%2520to%2520these%2520concepts.%2520However%252C%2520even%2520with%2520powerful%250Afoundational%2520feature%2520extractors%2520like%2520CLIP%252C%2520there%2520are%2520no%2520guarantees%2520that%2520the%250Aspecified%2520concepts%2520are%2520detectable.%2520In%2520this%2520work%252C%2520we%2520leverage%2520recent%2520advances%2520in%250Amechanistic%2520interpretability%2520and%2520propose%2520a%2520novel%2520CBM%2520approach%2520--%2520called%250ADiscover-then-Name-CBM%2520%2528DN-CBM%2529%2520--%2520that%2520inverts%2520the%2520typical%2520paradigm%253A%2520instead%250Aof%2520pre-selecting%2520concepts%2520based%2520on%2520the%2520downstream%2520classification%2520task%252C%2520we%2520use%250Asparse%2520autoencoders%2520to%2520first%2520discover%2520concepts%2520learnt%2520by%2520the%2520model%252C%2520and%2520then%250Aname%2520them%2520and%2520train%2520linear%2520probes%2520for%2520classification.%2520Our%2520concept%2520extraction%250Astrategy%2520is%2520efficient%252C%2520since%2520it%2520is%2520agnostic%2520to%2520the%2520downstream%2520task%252C%2520and%2520uses%250Aconcepts%2520already%2520known%2520to%2520the%2520model.%2520We%2520perform%2520a%2520comprehensive%2520evaluation%250Aacross%2520multiple%2520datasets%2520and%2520CLIP%2520architectures%2520and%2520show%2520that%2520our%2520method%2520yields%250Asemantically%2520meaningful%2520concepts%252C%2520assigns%2520appropriate%2520names%2520to%2520them%2520that%2520make%250Athem%2520easy%2520to%2520interpret%252C%2520and%2520yields%2520performant%2520and%2520interpretable%2520CBMs.%2520Code%250Aavailable%2520at%2520https%253A//github.com/neuroexplicit-saar/discover-then-name.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14499v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discover-then-Name%3A%20Task-Agnostic%20Concept%20Bottlenecks%20via%20Automated%0A%20%20Concept%20Discovery&entry.906535625=Sukrut%20Rao%20and%20Sweta%20Mahajan%20and%20Moritz%20B%C3%B6hle%20and%20Bernt%20Schiele&entry.1292438233=%20%20Concept%20Bottleneck%20Models%20%28CBMs%29%20have%20recently%20been%20proposed%20to%20address%20the%0A%27black-box%27%20problem%20of%20deep%20neural%20networks%2C%20by%20first%20mapping%20images%20to%20a%0Ahuman-understandable%20concept%20space%20and%20then%20linearly%20combining%20concepts%20for%0Aclassification.%20Such%20models%20typically%20require%20first%20coming%20up%20with%20a%20set%20of%0Aconcepts%20relevant%20to%20the%20task%20and%20then%20aligning%20the%20representations%20of%20a%0Afeature%20extractor%20to%20map%20to%20these%20concepts.%20However%2C%20even%20with%20powerful%0Afoundational%20feature%20extractors%20like%20CLIP%2C%20there%20are%20no%20guarantees%20that%20the%0Aspecified%20concepts%20are%20detectable.%20In%20this%20work%2C%20we%20leverage%20recent%20advances%20in%0Amechanistic%20interpretability%20and%20propose%20a%20novel%20CBM%20approach%20--%20called%0ADiscover-then-Name-CBM%20%28DN-CBM%29%20--%20that%20inverts%20the%20typical%20paradigm%3A%20instead%0Aof%20pre-selecting%20concepts%20based%20on%20the%20downstream%20classification%20task%2C%20we%20use%0Asparse%20autoencoders%20to%20first%20discover%20concepts%20learnt%20by%20the%20model%2C%20and%20then%0Aname%20them%20and%20train%20linear%20probes%20for%20classification.%20Our%20concept%20extraction%0Astrategy%20is%20efficient%2C%20since%20it%20is%20agnostic%20to%20the%20downstream%20task%2C%20and%20uses%0Aconcepts%20already%20known%20to%20the%20model.%20We%20perform%20a%20comprehensive%20evaluation%0Aacross%20multiple%20datasets%20and%20CLIP%20architectures%20and%20show%20that%20our%20method%20yields%0Asemantically%20meaningful%20concepts%2C%20assigns%20appropriate%20names%20to%20them%20that%20make%0Athem%20easy%20to%20interpret%2C%20and%20yields%20performant%20and%20interpretable%20CBMs.%20Code%0Aavailable%20at%20https%3A//github.com/neuroexplicit-saar/discover-then-name.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14499v1&entry.124074799=Read"},
{"title": "How to Engage Your Readers? Generating Guiding Questions to Promote\n  Active Reading", "author": "Peng Cui and Vil\u00e9m Zouhar and Xiaoyu Zhang and Mrinmaya Sachan", "abstract": "  Using questions in written text is an effective strategy to enhance\nreadability. However, what makes an active reading question good, what the\nlinguistic role of these questions is, and what is their impact on human\nreading remains understudied. We introduce GuidingQ, a dataset of 10K in-text\nquestions from textbooks and scientific articles. By analyzing the dataset, we\npresent a comprehensive understanding of the use, distribution, and linguistic\ncharacteristics of these questions. Then, we explore various approaches to\ngenerate such questions using language models. Our results highlight the\nimportance of capturing inter-question relationships and the challenge of\nquestion position identification in generating these questions. Finally, we\nconduct a human study to understand the implication of such questions on\nreading comprehension. We find that the generated questions are of high quality\nand are almost as effective as human-written questions in terms of improving\nreaders' memorization and comprehension.\n", "link": "http://arxiv.org/abs/2407.14309v1", "date": "2024-07-19", "relevancy": 1.9919, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4236}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.3871}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.3844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20to%20Engage%20Your%20Readers%3F%20Generating%20Guiding%20Questions%20to%20Promote%0A%20%20Active%20Reading&body=Title%3A%20How%20to%20Engage%20Your%20Readers%3F%20Generating%20Guiding%20Questions%20to%20Promote%0A%20%20Active%20Reading%0AAuthor%3A%20Peng%20Cui%20and%20Vil%C3%A9m%20Zouhar%20and%20Xiaoyu%20Zhang%20and%20Mrinmaya%20Sachan%0AAbstract%3A%20%20%20Using%20questions%20in%20written%20text%20is%20an%20effective%20strategy%20to%20enhance%0Areadability.%20However%2C%20what%20makes%20an%20active%20reading%20question%20good%2C%20what%20the%0Alinguistic%20role%20of%20these%20questions%20is%2C%20and%20what%20is%20their%20impact%20on%20human%0Areading%20remains%20understudied.%20We%20introduce%20GuidingQ%2C%20a%20dataset%20of%2010K%20in-text%0Aquestions%20from%20textbooks%20and%20scientific%20articles.%20By%20analyzing%20the%20dataset%2C%20we%0Apresent%20a%20comprehensive%20understanding%20of%20the%20use%2C%20distribution%2C%20and%20linguistic%0Acharacteristics%20of%20these%20questions.%20Then%2C%20we%20explore%20various%20approaches%20to%0Agenerate%20such%20questions%20using%20language%20models.%20Our%20results%20highlight%20the%0Aimportance%20of%20capturing%20inter-question%20relationships%20and%20the%20challenge%20of%0Aquestion%20position%20identification%20in%20generating%20these%20questions.%20Finally%2C%20we%0Aconduct%20a%20human%20study%20to%20understand%20the%20implication%20of%20such%20questions%20on%0Areading%20comprehension.%20We%20find%20that%20the%20generated%20questions%20are%20of%20high%20quality%0Aand%20are%20almost%20as%20effective%20as%20human-written%20questions%20in%20terms%20of%20improving%0Areaders%27%20memorization%20and%20comprehension.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14309v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520to%2520Engage%2520Your%2520Readers%253F%2520Generating%2520Guiding%2520Questions%2520to%2520Promote%250A%2520%2520Active%2520Reading%26entry.906535625%3DPeng%2520Cui%2520and%2520Vil%25C3%25A9m%2520Zouhar%2520and%2520Xiaoyu%2520Zhang%2520and%2520Mrinmaya%2520Sachan%26entry.1292438233%3D%2520%2520Using%2520questions%2520in%2520written%2520text%2520is%2520an%2520effective%2520strategy%2520to%2520enhance%250Areadability.%2520However%252C%2520what%2520makes%2520an%2520active%2520reading%2520question%2520good%252C%2520what%2520the%250Alinguistic%2520role%2520of%2520these%2520questions%2520is%252C%2520and%2520what%2520is%2520their%2520impact%2520on%2520human%250Areading%2520remains%2520understudied.%2520We%2520introduce%2520GuidingQ%252C%2520a%2520dataset%2520of%252010K%2520in-text%250Aquestions%2520from%2520textbooks%2520and%2520scientific%2520articles.%2520By%2520analyzing%2520the%2520dataset%252C%2520we%250Apresent%2520a%2520comprehensive%2520understanding%2520of%2520the%2520use%252C%2520distribution%252C%2520and%2520linguistic%250Acharacteristics%2520of%2520these%2520questions.%2520Then%252C%2520we%2520explore%2520various%2520approaches%2520to%250Agenerate%2520such%2520questions%2520using%2520language%2520models.%2520Our%2520results%2520highlight%2520the%250Aimportance%2520of%2520capturing%2520inter-question%2520relationships%2520and%2520the%2520challenge%2520of%250Aquestion%2520position%2520identification%2520in%2520generating%2520these%2520questions.%2520Finally%252C%2520we%250Aconduct%2520a%2520human%2520study%2520to%2520understand%2520the%2520implication%2520of%2520such%2520questions%2520on%250Areading%2520comprehension.%2520We%2520find%2520that%2520the%2520generated%2520questions%2520are%2520of%2520high%2520quality%250Aand%2520are%2520almost%2520as%2520effective%2520as%2520human-written%2520questions%2520in%2520terms%2520of%2520improving%250Areaders%2527%2520memorization%2520and%2520comprehension.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14309v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20to%20Engage%20Your%20Readers%3F%20Generating%20Guiding%20Questions%20to%20Promote%0A%20%20Active%20Reading&entry.906535625=Peng%20Cui%20and%20Vil%C3%A9m%20Zouhar%20and%20Xiaoyu%20Zhang%20and%20Mrinmaya%20Sachan&entry.1292438233=%20%20Using%20questions%20in%20written%20text%20is%20an%20effective%20strategy%20to%20enhance%0Areadability.%20However%2C%20what%20makes%20an%20active%20reading%20question%20good%2C%20what%20the%0Alinguistic%20role%20of%20these%20questions%20is%2C%20and%20what%20is%20their%20impact%20on%20human%0Areading%20remains%20understudied.%20We%20introduce%20GuidingQ%2C%20a%20dataset%20of%2010K%20in-text%0Aquestions%20from%20textbooks%20and%20scientific%20articles.%20By%20analyzing%20the%20dataset%2C%20we%0Apresent%20a%20comprehensive%20understanding%20of%20the%20use%2C%20distribution%2C%20and%20linguistic%0Acharacteristics%20of%20these%20questions.%20Then%2C%20we%20explore%20various%20approaches%20to%0Agenerate%20such%20questions%20using%20language%20models.%20Our%20results%20highlight%20the%0Aimportance%20of%20capturing%20inter-question%20relationships%20and%20the%20challenge%20of%0Aquestion%20position%20identification%20in%20generating%20these%20questions.%20Finally%2C%20we%0Aconduct%20a%20human%20study%20to%20understand%20the%20implication%20of%20such%20questions%20on%0Areading%20comprehension.%20We%20find%20that%20the%20generated%20questions%20are%20of%20high%20quality%0Aand%20are%20almost%20as%20effective%20as%20human-written%20questions%20in%20terms%20of%20improving%0Areaders%27%20memorization%20and%20comprehension.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14309v1&entry.124074799=Read"},
{"title": "The need of a self for self-driving cars a theoretical model applying\n  homeostasis to self driving", "author": "Martin Schmalzried", "abstract": "  This paper explores the concept of creating a \"self\" for self-driving cars\nthrough a homeostatic architecture designed to enhance their autonomy, safety,\nand efficiency. The proposed system integrates inward focused sensors to\nmonitor the car's internal state, such as the condition of its metal bodywork,\nwheels, engine, and battery, establishing a baseline homeostatic state\nrepresenting optimal functionality. Outward facing sensors, like cameras and\nLIDAR, are then interpreted via their impact on the car's homeostatic state by\nquantifying deviations from homeostasis. This contrasts with the approach of\ntrying to make cars \"see\" reality in a similar way to humans and identify\nelements in their reality in the same way humans. Virtual environments would be\nleveraged to accelerate training. Additionally, cars are programmed to\ncommunicate and share experiences via blockchain technology, learning from each\nother's mistakes while maintaining individualized training models. A dedicated\nlanguage for self-driving cars is proposed to enable nuanced interpretation and\nresponse to environmental data. This architecture allows self-driving cars to\ndynamically adjust their behavior based on internal and external feedback,\npromoting cooperation and continuous improvement. The study concludes by\ndiscussing the broader implications for AI development, potential real-world\napplications, and future research directions.\n", "link": "http://arxiv.org/abs/2407.12795v2", "date": "2024-07-19", "relevancy": 1.989, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5446}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4842}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20need%20of%20a%20self%20for%20self-driving%20cars%20a%20theoretical%20model%20applying%0A%20%20homeostasis%20to%20self%20driving&body=Title%3A%20The%20need%20of%20a%20self%20for%20self-driving%20cars%20a%20theoretical%20model%20applying%0A%20%20homeostasis%20to%20self%20driving%0AAuthor%3A%20Martin%20Schmalzried%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20concept%20of%20creating%20a%20%22self%22%20for%20self-driving%20cars%0Athrough%20a%20homeostatic%20architecture%20designed%20to%20enhance%20their%20autonomy%2C%20safety%2C%0Aand%20efficiency.%20The%20proposed%20system%20integrates%20inward%20focused%20sensors%20to%0Amonitor%20the%20car%27s%20internal%20state%2C%20such%20as%20the%20condition%20of%20its%20metal%20bodywork%2C%0Awheels%2C%20engine%2C%20and%20battery%2C%20establishing%20a%20baseline%20homeostatic%20state%0Arepresenting%20optimal%20functionality.%20Outward%20facing%20sensors%2C%20like%20cameras%20and%0ALIDAR%2C%20are%20then%20interpreted%20via%20their%20impact%20on%20the%20car%27s%20homeostatic%20state%20by%0Aquantifying%20deviations%20from%20homeostasis.%20This%20contrasts%20with%20the%20approach%20of%0Atrying%20to%20make%20cars%20%22see%22%20reality%20in%20a%20similar%20way%20to%20humans%20and%20identify%0Aelements%20in%20their%20reality%20in%20the%20same%20way%20humans.%20Virtual%20environments%20would%20be%0Aleveraged%20to%20accelerate%20training.%20Additionally%2C%20cars%20are%20programmed%20to%0Acommunicate%20and%20share%20experiences%20via%20blockchain%20technology%2C%20learning%20from%20each%0Aother%27s%20mistakes%20while%20maintaining%20individualized%20training%20models.%20A%20dedicated%0Alanguage%20for%20self-driving%20cars%20is%20proposed%20to%20enable%20nuanced%20interpretation%20and%0Aresponse%20to%20environmental%20data.%20This%20architecture%20allows%20self-driving%20cars%20to%0Adynamically%20adjust%20their%20behavior%20based%20on%20internal%20and%20external%20feedback%2C%0Apromoting%20cooperation%20and%20continuous%20improvement.%20The%20study%20concludes%20by%0Adiscussing%20the%20broader%20implications%20for%20AI%20development%2C%20potential%20real-world%0Aapplications%2C%20and%20future%20research%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12795v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520need%2520of%2520a%2520self%2520for%2520self-driving%2520cars%2520a%2520theoretical%2520model%2520applying%250A%2520%2520homeostasis%2520to%2520self%2520driving%26entry.906535625%3DMartin%2520Schmalzried%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520concept%2520of%2520creating%2520a%2520%2522self%2522%2520for%2520self-driving%2520cars%250Athrough%2520a%2520homeostatic%2520architecture%2520designed%2520to%2520enhance%2520their%2520autonomy%252C%2520safety%252C%250Aand%2520efficiency.%2520The%2520proposed%2520system%2520integrates%2520inward%2520focused%2520sensors%2520to%250Amonitor%2520the%2520car%2527s%2520internal%2520state%252C%2520such%2520as%2520the%2520condition%2520of%2520its%2520metal%2520bodywork%252C%250Awheels%252C%2520engine%252C%2520and%2520battery%252C%2520establishing%2520a%2520baseline%2520homeostatic%2520state%250Arepresenting%2520optimal%2520functionality.%2520Outward%2520facing%2520sensors%252C%2520like%2520cameras%2520and%250ALIDAR%252C%2520are%2520then%2520interpreted%2520via%2520their%2520impact%2520on%2520the%2520car%2527s%2520homeostatic%2520state%2520by%250Aquantifying%2520deviations%2520from%2520homeostasis.%2520This%2520contrasts%2520with%2520the%2520approach%2520of%250Atrying%2520to%2520make%2520cars%2520%2522see%2522%2520reality%2520in%2520a%2520similar%2520way%2520to%2520humans%2520and%2520identify%250Aelements%2520in%2520their%2520reality%2520in%2520the%2520same%2520way%2520humans.%2520Virtual%2520environments%2520would%2520be%250Aleveraged%2520to%2520accelerate%2520training.%2520Additionally%252C%2520cars%2520are%2520programmed%2520to%250Acommunicate%2520and%2520share%2520experiences%2520via%2520blockchain%2520technology%252C%2520learning%2520from%2520each%250Aother%2527s%2520mistakes%2520while%2520maintaining%2520individualized%2520training%2520models.%2520A%2520dedicated%250Alanguage%2520for%2520self-driving%2520cars%2520is%2520proposed%2520to%2520enable%2520nuanced%2520interpretation%2520and%250Aresponse%2520to%2520environmental%2520data.%2520This%2520architecture%2520allows%2520self-driving%2520cars%2520to%250Adynamically%2520adjust%2520their%2520behavior%2520based%2520on%2520internal%2520and%2520external%2520feedback%252C%250Apromoting%2520cooperation%2520and%2520continuous%2520improvement.%2520The%2520study%2520concludes%2520by%250Adiscussing%2520the%2520broader%2520implications%2520for%2520AI%2520development%252C%2520potential%2520real-world%250Aapplications%252C%2520and%2520future%2520research%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12795v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20need%20of%20a%20self%20for%20self-driving%20cars%20a%20theoretical%20model%20applying%0A%20%20homeostasis%20to%20self%20driving&entry.906535625=Martin%20Schmalzried&entry.1292438233=%20%20This%20paper%20explores%20the%20concept%20of%20creating%20a%20%22self%22%20for%20self-driving%20cars%0Athrough%20a%20homeostatic%20architecture%20designed%20to%20enhance%20their%20autonomy%2C%20safety%2C%0Aand%20efficiency.%20The%20proposed%20system%20integrates%20inward%20focused%20sensors%20to%0Amonitor%20the%20car%27s%20internal%20state%2C%20such%20as%20the%20condition%20of%20its%20metal%20bodywork%2C%0Awheels%2C%20engine%2C%20and%20battery%2C%20establishing%20a%20baseline%20homeostatic%20state%0Arepresenting%20optimal%20functionality.%20Outward%20facing%20sensors%2C%20like%20cameras%20and%0ALIDAR%2C%20are%20then%20interpreted%20via%20their%20impact%20on%20the%20car%27s%20homeostatic%20state%20by%0Aquantifying%20deviations%20from%20homeostasis.%20This%20contrasts%20with%20the%20approach%20of%0Atrying%20to%20make%20cars%20%22see%22%20reality%20in%20a%20similar%20way%20to%20humans%20and%20identify%0Aelements%20in%20their%20reality%20in%20the%20same%20way%20humans.%20Virtual%20environments%20would%20be%0Aleveraged%20to%20accelerate%20training.%20Additionally%2C%20cars%20are%20programmed%20to%0Acommunicate%20and%20share%20experiences%20via%20blockchain%20technology%2C%20learning%20from%20each%0Aother%27s%20mistakes%20while%20maintaining%20individualized%20training%20models.%20A%20dedicated%0Alanguage%20for%20self-driving%20cars%20is%20proposed%20to%20enable%20nuanced%20interpretation%20and%0Aresponse%20to%20environmental%20data.%20This%20architecture%20allows%20self-driving%20cars%20to%0Adynamically%20adjust%20their%20behavior%20based%20on%20internal%20and%20external%20feedback%2C%0Apromoting%20cooperation%20and%20continuous%20improvement.%20The%20study%20concludes%20by%0Adiscussing%20the%20broader%20implications%20for%20AI%20development%2C%20potential%20real-world%0Aapplications%2C%20and%20future%20research%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12795v2&entry.124074799=Read"},
{"title": "Bucketed Ranking-based Losses for Efficient Training of Object Detectors", "author": "Feyza Yavuz and Baris Can Cam and Adnan Harun Dogan and Kemal Oksuz and Emre Akbas and Sinan Kalkan", "abstract": "  Ranking-based loss functions, such as Average Precision Loss and Rank&Sort\nLoss, outperform widely used score-based losses in object detection. These loss\nfunctions better align with the evaluation criteria, have fewer\nhyperparameters, and offer robustness against the imbalance between positive\nand negative classes. However, they require pairwise comparisons among $P$\npositive and $N$ negative predictions, introducing a time complexity of\n$\\mathcal{O}(PN)$, which is prohibitive since $N$ is often large (e.g., $10^8$\nin ATSS). Despite their advantages, the widespread adoption of ranking-based\nlosses has been hindered by their high time and space complexities.\n  In this paper, we focus on improving the efficiency of ranking-based loss\nfunctions. To this end, we propose Bucketed Ranking-based Losses which group\nnegative predictions into $B$ buckets ($B \\ll N$) in order to reduce the number\nof pairwise comparisons so that time complexity can be reduced. Our method\nenhances the time complexity, reducing it to $\\mathcal{O}(\\max (N \\log(N),\nP^2))$. To validate our method and show its generality, we conducted\nexperiments on 2 different tasks, 3 different datasets, 7 different detectors.\nWe show that Bucketed Ranking-based (BR) Losses yield the same accuracy with\nthe unbucketed versions and provide $2\\times$ faster training on average. We\nalso train, for the first time, transformer-based object detectors using\nranking-based losses, thanks to the efficiency of our BR. When we train CoDETR,\na state-of-the-art transformer-based object detector, using our BR Loss, we\nconsistently outperform its original results over several different backbones.\nCode is available at https://github.com/blisgard/BucketedRankingBasedLosses\n", "link": "http://arxiv.org/abs/2407.14204v1", "date": "2024-07-19", "relevancy": 1.9845, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5051}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5038}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bucketed%20Ranking-based%20Losses%20for%20Efficient%20Training%20of%20Object%20Detectors&body=Title%3A%20Bucketed%20Ranking-based%20Losses%20for%20Efficient%20Training%20of%20Object%20Detectors%0AAuthor%3A%20Feyza%20Yavuz%20and%20Baris%20Can%20Cam%20and%20Adnan%20Harun%20Dogan%20and%20Kemal%20Oksuz%20and%20Emre%20Akbas%20and%20Sinan%20Kalkan%0AAbstract%3A%20%20%20Ranking-based%20loss%20functions%2C%20such%20as%20Average%20Precision%20Loss%20and%20Rank%26Sort%0ALoss%2C%20outperform%20widely%20used%20score-based%20losses%20in%20object%20detection.%20These%20loss%0Afunctions%20better%20align%20with%20the%20evaluation%20criteria%2C%20have%20fewer%0Ahyperparameters%2C%20and%20offer%20robustness%20against%20the%20imbalance%20between%20positive%0Aand%20negative%20classes.%20However%2C%20they%20require%20pairwise%20comparisons%20among%20%24P%24%0Apositive%20and%20%24N%24%20negative%20predictions%2C%20introducing%20a%20time%20complexity%20of%0A%24%5Cmathcal%7BO%7D%28PN%29%24%2C%20which%20is%20prohibitive%20since%20%24N%24%20is%20often%20large%20%28e.g.%2C%20%2410%5E8%24%0Ain%20ATSS%29.%20Despite%20their%20advantages%2C%20the%20widespread%20adoption%20of%20ranking-based%0Alosses%20has%20been%20hindered%20by%20their%20high%20time%20and%20space%20complexities.%0A%20%20In%20this%20paper%2C%20we%20focus%20on%20improving%20the%20efficiency%20of%20ranking-based%20loss%0Afunctions.%20To%20this%20end%2C%20we%20propose%20Bucketed%20Ranking-based%20Losses%20which%20group%0Anegative%20predictions%20into%20%24B%24%20buckets%20%28%24B%20%5Cll%20N%24%29%20in%20order%20to%20reduce%20the%20number%0Aof%20pairwise%20comparisons%20so%20that%20time%20complexity%20can%20be%20reduced.%20Our%20method%0Aenhances%20the%20time%20complexity%2C%20reducing%20it%20to%20%24%5Cmathcal%7BO%7D%28%5Cmax%20%28N%20%5Clog%28N%29%2C%0AP%5E2%29%29%24.%20To%20validate%20our%20method%20and%20show%20its%20generality%2C%20we%20conducted%0Aexperiments%20on%202%20different%20tasks%2C%203%20different%20datasets%2C%207%20different%20detectors.%0AWe%20show%20that%20Bucketed%20Ranking-based%20%28BR%29%20Losses%20yield%20the%20same%20accuracy%20with%0Athe%20unbucketed%20versions%20and%20provide%20%242%5Ctimes%24%20faster%20training%20on%20average.%20We%0Aalso%20train%2C%20for%20the%20first%20time%2C%20transformer-based%20object%20detectors%20using%0Aranking-based%20losses%2C%20thanks%20to%20the%20efficiency%20of%20our%20BR.%20When%20we%20train%20CoDETR%2C%0Aa%20state-of-the-art%20transformer-based%20object%20detector%2C%20using%20our%20BR%20Loss%2C%20we%0Aconsistently%20outperform%20its%20original%20results%20over%20several%20different%20backbones.%0ACode%20is%20available%20at%20https%3A//github.com/blisgard/BucketedRankingBasedLosses%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14204v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBucketed%2520Ranking-based%2520Losses%2520for%2520Efficient%2520Training%2520of%2520Object%2520Detectors%26entry.906535625%3DFeyza%2520Yavuz%2520and%2520Baris%2520Can%2520Cam%2520and%2520Adnan%2520Harun%2520Dogan%2520and%2520Kemal%2520Oksuz%2520and%2520Emre%2520Akbas%2520and%2520Sinan%2520Kalkan%26entry.1292438233%3D%2520%2520Ranking-based%2520loss%2520functions%252C%2520such%2520as%2520Average%2520Precision%2520Loss%2520and%2520Rank%2526Sort%250ALoss%252C%2520outperform%2520widely%2520used%2520score-based%2520losses%2520in%2520object%2520detection.%2520These%2520loss%250Afunctions%2520better%2520align%2520with%2520the%2520evaluation%2520criteria%252C%2520have%2520fewer%250Ahyperparameters%252C%2520and%2520offer%2520robustness%2520against%2520the%2520imbalance%2520between%2520positive%250Aand%2520negative%2520classes.%2520However%252C%2520they%2520require%2520pairwise%2520comparisons%2520among%2520%2524P%2524%250Apositive%2520and%2520%2524N%2524%2520negative%2520predictions%252C%2520introducing%2520a%2520time%2520complexity%2520of%250A%2524%255Cmathcal%257BO%257D%2528PN%2529%2524%252C%2520which%2520is%2520prohibitive%2520since%2520%2524N%2524%2520is%2520often%2520large%2520%2528e.g.%252C%2520%252410%255E8%2524%250Ain%2520ATSS%2529.%2520Despite%2520their%2520advantages%252C%2520the%2520widespread%2520adoption%2520of%2520ranking-based%250Alosses%2520has%2520been%2520hindered%2520by%2520their%2520high%2520time%2520and%2520space%2520complexities.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520improving%2520the%2520efficiency%2520of%2520ranking-based%2520loss%250Afunctions.%2520To%2520this%2520end%252C%2520we%2520propose%2520Bucketed%2520Ranking-based%2520Losses%2520which%2520group%250Anegative%2520predictions%2520into%2520%2524B%2524%2520buckets%2520%2528%2524B%2520%255Cll%2520N%2524%2529%2520in%2520order%2520to%2520reduce%2520the%2520number%250Aof%2520pairwise%2520comparisons%2520so%2520that%2520time%2520complexity%2520can%2520be%2520reduced.%2520Our%2520method%250Aenhances%2520the%2520time%2520complexity%252C%2520reducing%2520it%2520to%2520%2524%255Cmathcal%257BO%257D%2528%255Cmax%2520%2528N%2520%255Clog%2528N%2529%252C%250AP%255E2%2529%2529%2524.%2520To%2520validate%2520our%2520method%2520and%2520show%2520its%2520generality%252C%2520we%2520conducted%250Aexperiments%2520on%25202%2520different%2520tasks%252C%25203%2520different%2520datasets%252C%25207%2520different%2520detectors.%250AWe%2520show%2520that%2520Bucketed%2520Ranking-based%2520%2528BR%2529%2520Losses%2520yield%2520the%2520same%2520accuracy%2520with%250Athe%2520unbucketed%2520versions%2520and%2520provide%2520%25242%255Ctimes%2524%2520faster%2520training%2520on%2520average.%2520We%250Aalso%2520train%252C%2520for%2520the%2520first%2520time%252C%2520transformer-based%2520object%2520detectors%2520using%250Aranking-based%2520losses%252C%2520thanks%2520to%2520the%2520efficiency%2520of%2520our%2520BR.%2520When%2520we%2520train%2520CoDETR%252C%250Aa%2520state-of-the-art%2520transformer-based%2520object%2520detector%252C%2520using%2520our%2520BR%2520Loss%252C%2520we%250Aconsistently%2520outperform%2520its%2520original%2520results%2520over%2520several%2520different%2520backbones.%250ACode%2520is%2520available%2520at%2520https%253A//github.com/blisgard/BucketedRankingBasedLosses%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14204v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bucketed%20Ranking-based%20Losses%20for%20Efficient%20Training%20of%20Object%20Detectors&entry.906535625=Feyza%20Yavuz%20and%20Baris%20Can%20Cam%20and%20Adnan%20Harun%20Dogan%20and%20Kemal%20Oksuz%20and%20Emre%20Akbas%20and%20Sinan%20Kalkan&entry.1292438233=%20%20Ranking-based%20loss%20functions%2C%20such%20as%20Average%20Precision%20Loss%20and%20Rank%26Sort%0ALoss%2C%20outperform%20widely%20used%20score-based%20losses%20in%20object%20detection.%20These%20loss%0Afunctions%20better%20align%20with%20the%20evaluation%20criteria%2C%20have%20fewer%0Ahyperparameters%2C%20and%20offer%20robustness%20against%20the%20imbalance%20between%20positive%0Aand%20negative%20classes.%20However%2C%20they%20require%20pairwise%20comparisons%20among%20%24P%24%0Apositive%20and%20%24N%24%20negative%20predictions%2C%20introducing%20a%20time%20complexity%20of%0A%24%5Cmathcal%7BO%7D%28PN%29%24%2C%20which%20is%20prohibitive%20since%20%24N%24%20is%20often%20large%20%28e.g.%2C%20%2410%5E8%24%0Ain%20ATSS%29.%20Despite%20their%20advantages%2C%20the%20widespread%20adoption%20of%20ranking-based%0Alosses%20has%20been%20hindered%20by%20their%20high%20time%20and%20space%20complexities.%0A%20%20In%20this%20paper%2C%20we%20focus%20on%20improving%20the%20efficiency%20of%20ranking-based%20loss%0Afunctions.%20To%20this%20end%2C%20we%20propose%20Bucketed%20Ranking-based%20Losses%20which%20group%0Anegative%20predictions%20into%20%24B%24%20buckets%20%28%24B%20%5Cll%20N%24%29%20in%20order%20to%20reduce%20the%20number%0Aof%20pairwise%20comparisons%20so%20that%20time%20complexity%20can%20be%20reduced.%20Our%20method%0Aenhances%20the%20time%20complexity%2C%20reducing%20it%20to%20%24%5Cmathcal%7BO%7D%28%5Cmax%20%28N%20%5Clog%28N%29%2C%0AP%5E2%29%29%24.%20To%20validate%20our%20method%20and%20show%20its%20generality%2C%20we%20conducted%0Aexperiments%20on%202%20different%20tasks%2C%203%20different%20datasets%2C%207%20different%20detectors.%0AWe%20show%20that%20Bucketed%20Ranking-based%20%28BR%29%20Losses%20yield%20the%20same%20accuracy%20with%0Athe%20unbucketed%20versions%20and%20provide%20%242%5Ctimes%24%20faster%20training%20on%20average.%20We%0Aalso%20train%2C%20for%20the%20first%20time%2C%20transformer-based%20object%20detectors%20using%0Aranking-based%20losses%2C%20thanks%20to%20the%20efficiency%20of%20our%20BR.%20When%20we%20train%20CoDETR%2C%0Aa%20state-of-the-art%20transformer-based%20object%20detector%2C%20using%20our%20BR%20Loss%2C%20we%0Aconsistently%20outperform%20its%20original%20results%20over%20several%20different%20backbones.%0ACode%20is%20available%20at%20https%3A//github.com/blisgard/BucketedRankingBasedLosses%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14204v1&entry.124074799=Read"},
{"title": "Regularized KL-Divergence for Well-Defined Function-Space Variational\n  Inference in Bayesian neural networks", "author": "Tristan Cinquin and Robert Bamler", "abstract": "  Bayesian neural networks (BNN) promise to combine the predictive performance\nof neural networks with principled uncertainty modeling important for\nsafety-critical systems and decision making. However, posterior uncertainty\nestimates depend on the choice of prior, and finding informative priors in\nweight-space has proven difficult. This has motivated variational inference\n(VI) methods that pose priors directly on the function generated by the BNN\nrather than on weights. In this paper, we address a fundamental issue with such\nfunction-space VI approaches pointed out by Burt et al. (2020), who showed that\nthe objective function (ELBO) is negative infinite for most priors of interest.\nOur solution builds on generalized VI (Knoblauch et al., 2019) with the\nregularized KL divergence (Quang, 2019) and is, to the best of our knowledge,\nthe first well-defined variational objective for function-space inference in\nBNNs with Gaussian process (GP) priors. Experiments show that our method\nincorporates the properties specified by the GP prior on synthetic and small\nreal-world data sets, and provides competitive uncertainty estimates for\nregression, classification and out-of-distribution detection compared to BNN\nbaselines with both function and weight-space priors.\n", "link": "http://arxiv.org/abs/2406.04317v2", "date": "2024-07-19", "relevancy": 1.963, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.528}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4912}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Regularized%20KL-Divergence%20for%20Well-Defined%20Function-Space%20Variational%0A%20%20Inference%20in%20Bayesian%20neural%20networks&body=Title%3A%20Regularized%20KL-Divergence%20for%20Well-Defined%20Function-Space%20Variational%0A%20%20Inference%20in%20Bayesian%20neural%20networks%0AAuthor%3A%20Tristan%20Cinquin%20and%20Robert%20Bamler%0AAbstract%3A%20%20%20Bayesian%20neural%20networks%20%28BNN%29%20promise%20to%20combine%20the%20predictive%20performance%0Aof%20neural%20networks%20with%20principled%20uncertainty%20modeling%20important%20for%0Asafety-critical%20systems%20and%20decision%20making.%20However%2C%20posterior%20uncertainty%0Aestimates%20depend%20on%20the%20choice%20of%20prior%2C%20and%20finding%20informative%20priors%20in%0Aweight-space%20has%20proven%20difficult.%20This%20has%20motivated%20variational%20inference%0A%28VI%29%20methods%20that%20pose%20priors%20directly%20on%20the%20function%20generated%20by%20the%20BNN%0Arather%20than%20on%20weights.%20In%20this%20paper%2C%20we%20address%20a%20fundamental%20issue%20with%20such%0Afunction-space%20VI%20approaches%20pointed%20out%20by%20Burt%20et%20al.%20%282020%29%2C%20who%20showed%20that%0Athe%20objective%20function%20%28ELBO%29%20is%20negative%20infinite%20for%20most%20priors%20of%20interest.%0AOur%20solution%20builds%20on%20generalized%20VI%20%28Knoblauch%20et%20al.%2C%202019%29%20with%20the%0Aregularized%20KL%20divergence%20%28Quang%2C%202019%29%20and%20is%2C%20to%20the%20best%20of%20our%20knowledge%2C%0Athe%20first%20well-defined%20variational%20objective%20for%20function-space%20inference%20in%0ABNNs%20with%20Gaussian%20process%20%28GP%29%20priors.%20Experiments%20show%20that%20our%20method%0Aincorporates%20the%20properties%20specified%20by%20the%20GP%20prior%20on%20synthetic%20and%20small%0Areal-world%20data%20sets%2C%20and%20provides%20competitive%20uncertainty%20estimates%20for%0Aregression%2C%20classification%20and%20out-of-distribution%20detection%20compared%20to%20BNN%0Abaselines%20with%20both%20function%20and%20weight-space%20priors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04317v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegularized%2520KL-Divergence%2520for%2520Well-Defined%2520Function-Space%2520Variational%250A%2520%2520Inference%2520in%2520Bayesian%2520neural%2520networks%26entry.906535625%3DTristan%2520Cinquin%2520and%2520Robert%2520Bamler%26entry.1292438233%3D%2520%2520Bayesian%2520neural%2520networks%2520%2528BNN%2529%2520promise%2520to%2520combine%2520the%2520predictive%2520performance%250Aof%2520neural%2520networks%2520with%2520principled%2520uncertainty%2520modeling%2520important%2520for%250Asafety-critical%2520systems%2520and%2520decision%2520making.%2520However%252C%2520posterior%2520uncertainty%250Aestimates%2520depend%2520on%2520the%2520choice%2520of%2520prior%252C%2520and%2520finding%2520informative%2520priors%2520in%250Aweight-space%2520has%2520proven%2520difficult.%2520This%2520has%2520motivated%2520variational%2520inference%250A%2528VI%2529%2520methods%2520that%2520pose%2520priors%2520directly%2520on%2520the%2520function%2520generated%2520by%2520the%2520BNN%250Arather%2520than%2520on%2520weights.%2520In%2520this%2520paper%252C%2520we%2520address%2520a%2520fundamental%2520issue%2520with%2520such%250Afunction-space%2520VI%2520approaches%2520pointed%2520out%2520by%2520Burt%2520et%2520al.%2520%25282020%2529%252C%2520who%2520showed%2520that%250Athe%2520objective%2520function%2520%2528ELBO%2529%2520is%2520negative%2520infinite%2520for%2520most%2520priors%2520of%2520interest.%250AOur%2520solution%2520builds%2520on%2520generalized%2520VI%2520%2528Knoblauch%2520et%2520al.%252C%25202019%2529%2520with%2520the%250Aregularized%2520KL%2520divergence%2520%2528Quang%252C%25202019%2529%2520and%2520is%252C%2520to%2520the%2520best%2520of%2520our%2520knowledge%252C%250Athe%2520first%2520well-defined%2520variational%2520objective%2520for%2520function-space%2520inference%2520in%250ABNNs%2520with%2520Gaussian%2520process%2520%2528GP%2529%2520priors.%2520Experiments%2520show%2520that%2520our%2520method%250Aincorporates%2520the%2520properties%2520specified%2520by%2520the%2520GP%2520prior%2520on%2520synthetic%2520and%2520small%250Areal-world%2520data%2520sets%252C%2520and%2520provides%2520competitive%2520uncertainty%2520estimates%2520for%250Aregression%252C%2520classification%2520and%2520out-of-distribution%2520detection%2520compared%2520to%2520BNN%250Abaselines%2520with%2520both%2520function%2520and%2520weight-space%2520priors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04317v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Regularized%20KL-Divergence%20for%20Well-Defined%20Function-Space%20Variational%0A%20%20Inference%20in%20Bayesian%20neural%20networks&entry.906535625=Tristan%20Cinquin%20and%20Robert%20Bamler&entry.1292438233=%20%20Bayesian%20neural%20networks%20%28BNN%29%20promise%20to%20combine%20the%20predictive%20performance%0Aof%20neural%20networks%20with%20principled%20uncertainty%20modeling%20important%20for%0Asafety-critical%20systems%20and%20decision%20making.%20However%2C%20posterior%20uncertainty%0Aestimates%20depend%20on%20the%20choice%20of%20prior%2C%20and%20finding%20informative%20priors%20in%0Aweight-space%20has%20proven%20difficult.%20This%20has%20motivated%20variational%20inference%0A%28VI%29%20methods%20that%20pose%20priors%20directly%20on%20the%20function%20generated%20by%20the%20BNN%0Arather%20than%20on%20weights.%20In%20this%20paper%2C%20we%20address%20a%20fundamental%20issue%20with%20such%0Afunction-space%20VI%20approaches%20pointed%20out%20by%20Burt%20et%20al.%20%282020%29%2C%20who%20showed%20that%0Athe%20objective%20function%20%28ELBO%29%20is%20negative%20infinite%20for%20most%20priors%20of%20interest.%0AOur%20solution%20builds%20on%20generalized%20VI%20%28Knoblauch%20et%20al.%2C%202019%29%20with%20the%0Aregularized%20KL%20divergence%20%28Quang%2C%202019%29%20and%20is%2C%20to%20the%20best%20of%20our%20knowledge%2C%0Athe%20first%20well-defined%20variational%20objective%20for%20function-space%20inference%20in%0ABNNs%20with%20Gaussian%20process%20%28GP%29%20priors.%20Experiments%20show%20that%20our%20method%0Aincorporates%20the%20properties%20specified%20by%20the%20GP%20prior%20on%20synthetic%20and%20small%0Areal-world%20data%20sets%2C%20and%20provides%20competitive%20uncertainty%20estimates%20for%0Aregression%2C%20classification%20and%20out-of-distribution%20detection%20compared%20to%20BNN%0Abaselines%20with%20both%20function%20and%20weight-space%20priors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04317v2&entry.124074799=Read"},
{"title": "ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG\n  Capabilities", "author": "Peng Xu and Wei Ping and Xianchao Wu and Zihan Liu and Mohammad Shoeybi and Bryan Catanzaro", "abstract": "  In this work, we introduce ChatQA 2, a Llama3-based model designed to bridge\nthe gap between open-access LLMs and leading proprietary models (e.g.,\nGPT-4-Turbo) in long-context understanding and retrieval-augmented generation\n(RAG) capabilities. These two capabilities are essential for LLMs to process\nlarge volumes of information that cannot fit into a single prompt and are\ncomplementary to each other, depending on the downstream tasks and\ncomputational budgets. We present a detailed continued training recipe to\nextend the context window of Llama3-70B-base from 8K to 128K tokens, along with\na three-stage instruction tuning process to enhance the model's\ninstruction-following, RAG performance, and long-context understanding\ncapabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model\nachieves accuracy comparable to GPT-4-Turbo-2024-0409 on many long-context\nunderstanding tasks and surpasses it on the RAG benchmark. Interestingly, we\nfind that the state-of-the-art long-context retriever can alleviate the top-k\ncontext fragmentation issue in RAG, further improving RAG-based results for\nlong-context understanding tasks. We also provide extensive comparisons between\nRAG and long-context solutions using state-of-the-art long-context LLMs.\n", "link": "http://arxiv.org/abs/2407.14482v1", "date": "2024-07-19", "relevancy": 1.9559, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5256}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4644}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChatQA%202%3A%20Bridging%20the%20Gap%20to%20Proprietary%20LLMs%20in%20Long%20Context%20and%20RAG%0A%20%20Capabilities&body=Title%3A%20ChatQA%202%3A%20Bridging%20the%20Gap%20to%20Proprietary%20LLMs%20in%20Long%20Context%20and%20RAG%0A%20%20Capabilities%0AAuthor%3A%20Peng%20Xu%20and%20Wei%20Ping%20and%20Xianchao%20Wu%20and%20Zihan%20Liu%20and%20Mohammad%20Shoeybi%20and%20Bryan%20Catanzaro%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20ChatQA%202%2C%20a%20Llama3-based%20model%20designed%20to%20bridge%0Athe%20gap%20between%20open-access%20LLMs%20and%20leading%20proprietary%20models%20%28e.g.%2C%0AGPT-4-Turbo%29%20in%20long-context%20understanding%20and%20retrieval-augmented%20generation%0A%28RAG%29%20capabilities.%20These%20two%20capabilities%20are%20essential%20for%20LLMs%20to%20process%0Alarge%20volumes%20of%20information%20that%20cannot%20fit%20into%20a%20single%20prompt%20and%20are%0Acomplementary%20to%20each%20other%2C%20depending%20on%20the%20downstream%20tasks%20and%0Acomputational%20budgets.%20We%20present%20a%20detailed%20continued%20training%20recipe%20to%0Aextend%20the%20context%20window%20of%20Llama3-70B-base%20from%208K%20to%20128K%20tokens%2C%20along%20with%0Aa%20three-stage%20instruction%20tuning%20process%20to%20enhance%20the%20model%27s%0Ainstruction-following%2C%20RAG%20performance%2C%20and%20long-context%20understanding%0Acapabilities.%20Our%20results%20demonstrate%20that%20the%20Llama3-ChatQA-2-70B%20model%0Aachieves%20accuracy%20comparable%20to%20GPT-4-Turbo-2024-0409%20on%20many%20long-context%0Aunderstanding%20tasks%20and%20surpasses%20it%20on%20the%20RAG%20benchmark.%20Interestingly%2C%20we%0Afind%20that%20the%20state-of-the-art%20long-context%20retriever%20can%20alleviate%20the%20top-k%0Acontext%20fragmentation%20issue%20in%20RAG%2C%20further%20improving%20RAG-based%20results%20for%0Along-context%20understanding%20tasks.%20We%20also%20provide%20extensive%20comparisons%20between%0ARAG%20and%20long-context%20solutions%20using%20state-of-the-art%20long-context%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14482v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChatQA%25202%253A%2520Bridging%2520the%2520Gap%2520to%2520Proprietary%2520LLMs%2520in%2520Long%2520Context%2520and%2520RAG%250A%2520%2520Capabilities%26entry.906535625%3DPeng%2520Xu%2520and%2520Wei%2520Ping%2520and%2520Xianchao%2520Wu%2520and%2520Zihan%2520Liu%2520and%2520Mohammad%2520Shoeybi%2520and%2520Bryan%2520Catanzaro%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520ChatQA%25202%252C%2520a%2520Llama3-based%2520model%2520designed%2520to%2520bridge%250Athe%2520gap%2520between%2520open-access%2520LLMs%2520and%2520leading%2520proprietary%2520models%2520%2528e.g.%252C%250AGPT-4-Turbo%2529%2520in%2520long-context%2520understanding%2520and%2520retrieval-augmented%2520generation%250A%2528RAG%2529%2520capabilities.%2520These%2520two%2520capabilities%2520are%2520essential%2520for%2520LLMs%2520to%2520process%250Alarge%2520volumes%2520of%2520information%2520that%2520cannot%2520fit%2520into%2520a%2520single%2520prompt%2520and%2520are%250Acomplementary%2520to%2520each%2520other%252C%2520depending%2520on%2520the%2520downstream%2520tasks%2520and%250Acomputational%2520budgets.%2520We%2520present%2520a%2520detailed%2520continued%2520training%2520recipe%2520to%250Aextend%2520the%2520context%2520window%2520of%2520Llama3-70B-base%2520from%25208K%2520to%2520128K%2520tokens%252C%2520along%2520with%250Aa%2520three-stage%2520instruction%2520tuning%2520process%2520to%2520enhance%2520the%2520model%2527s%250Ainstruction-following%252C%2520RAG%2520performance%252C%2520and%2520long-context%2520understanding%250Acapabilities.%2520Our%2520results%2520demonstrate%2520that%2520the%2520Llama3-ChatQA-2-70B%2520model%250Aachieves%2520accuracy%2520comparable%2520to%2520GPT-4-Turbo-2024-0409%2520on%2520many%2520long-context%250Aunderstanding%2520tasks%2520and%2520surpasses%2520it%2520on%2520the%2520RAG%2520benchmark.%2520Interestingly%252C%2520we%250Afind%2520that%2520the%2520state-of-the-art%2520long-context%2520retriever%2520can%2520alleviate%2520the%2520top-k%250Acontext%2520fragmentation%2520issue%2520in%2520RAG%252C%2520further%2520improving%2520RAG-based%2520results%2520for%250Along-context%2520understanding%2520tasks.%2520We%2520also%2520provide%2520extensive%2520comparisons%2520between%250ARAG%2520and%2520long-context%2520solutions%2520using%2520state-of-the-art%2520long-context%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14482v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChatQA%202%3A%20Bridging%20the%20Gap%20to%20Proprietary%20LLMs%20in%20Long%20Context%20and%20RAG%0A%20%20Capabilities&entry.906535625=Peng%20Xu%20and%20Wei%20Ping%20and%20Xianchao%20Wu%20and%20Zihan%20Liu%20and%20Mohammad%20Shoeybi%20and%20Bryan%20Catanzaro&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20ChatQA%202%2C%20a%20Llama3-based%20model%20designed%20to%20bridge%0Athe%20gap%20between%20open-access%20LLMs%20and%20leading%20proprietary%20models%20%28e.g.%2C%0AGPT-4-Turbo%29%20in%20long-context%20understanding%20and%20retrieval-augmented%20generation%0A%28RAG%29%20capabilities.%20These%20two%20capabilities%20are%20essential%20for%20LLMs%20to%20process%0Alarge%20volumes%20of%20information%20that%20cannot%20fit%20into%20a%20single%20prompt%20and%20are%0Acomplementary%20to%20each%20other%2C%20depending%20on%20the%20downstream%20tasks%20and%0Acomputational%20budgets.%20We%20present%20a%20detailed%20continued%20training%20recipe%20to%0Aextend%20the%20context%20window%20of%20Llama3-70B-base%20from%208K%20to%20128K%20tokens%2C%20along%20with%0Aa%20three-stage%20instruction%20tuning%20process%20to%20enhance%20the%20model%27s%0Ainstruction-following%2C%20RAG%20performance%2C%20and%20long-context%20understanding%0Acapabilities.%20Our%20results%20demonstrate%20that%20the%20Llama3-ChatQA-2-70B%20model%0Aachieves%20accuracy%20comparable%20to%20GPT-4-Turbo-2024-0409%20on%20many%20long-context%0Aunderstanding%20tasks%20and%20surpasses%20it%20on%20the%20RAG%20benchmark.%20Interestingly%2C%20we%0Afind%20that%20the%20state-of-the-art%20long-context%20retriever%20can%20alleviate%20the%20top-k%0Acontext%20fragmentation%20issue%20in%20RAG%2C%20further%20improving%20RAG-based%20results%20for%0Along-context%20understanding%20tasks.%20We%20also%20provide%20extensive%20comparisons%20between%0ARAG%20and%20long-context%20solutions%20using%20state-of-the-art%20long-context%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14482v1&entry.124074799=Read"},
{"title": "Forbes: Face Obfuscation Rendering via Backpropagation Refinement Scheme", "author": "Jintae Kim and Seungwon yang and Seong-Gyun Jeong and Chang-Su Kim", "abstract": "  A novel algorithm for face obfuscation, called Forbes, which aims to\nobfuscate facial appearance recognizable by humans but preserve the identity\nand attributes decipherable by machines, is proposed in this paper. Forbes\nfirst applies multiple obfuscating transformations with random parameters to an\nimage to remove the identity information distinguishable by humans. Then, it\noptimizes the parameters to make the transformed image decipherable by machines\nbased on the backpropagation refinement scheme. Finally, it renders an\nobfuscated image by applying the transformations with the optimized parameters.\nExperimental results on various datasets demonstrate that Forbes achieves both\nhuman indecipherability and machine decipherability excellently. The source\ncodes are available at https://github.com/mcljtkim/Forbes.\n", "link": "http://arxiv.org/abs/2407.14170v1", "date": "2024-07-19", "relevancy": 1.9504, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4907}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.4888}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forbes%3A%20Face%20Obfuscation%20Rendering%20via%20Backpropagation%20Refinement%20Scheme&body=Title%3A%20Forbes%3A%20Face%20Obfuscation%20Rendering%20via%20Backpropagation%20Refinement%20Scheme%0AAuthor%3A%20Jintae%20Kim%20and%20Seungwon%20yang%20and%20Seong-Gyun%20Jeong%20and%20Chang-Su%20Kim%0AAbstract%3A%20%20%20A%20novel%20algorithm%20for%20face%20obfuscation%2C%20called%20Forbes%2C%20which%20aims%20to%0Aobfuscate%20facial%20appearance%20recognizable%20by%20humans%20but%20preserve%20the%20identity%0Aand%20attributes%20decipherable%20by%20machines%2C%20is%20proposed%20in%20this%20paper.%20Forbes%0Afirst%20applies%20multiple%20obfuscating%20transformations%20with%20random%20parameters%20to%20an%0Aimage%20to%20remove%20the%20identity%20information%20distinguishable%20by%20humans.%20Then%2C%20it%0Aoptimizes%20the%20parameters%20to%20make%20the%20transformed%20image%20decipherable%20by%20machines%0Abased%20on%20the%20backpropagation%20refinement%20scheme.%20Finally%2C%20it%20renders%20an%0Aobfuscated%20image%20by%20applying%20the%20transformations%20with%20the%20optimized%20parameters.%0AExperimental%20results%20on%20various%20datasets%20demonstrate%20that%20Forbes%20achieves%20both%0Ahuman%20indecipherability%20and%20machine%20decipherability%20excellently.%20The%20source%0Acodes%20are%20available%20at%20https%3A//github.com/mcljtkim/Forbes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14170v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForbes%253A%2520Face%2520Obfuscation%2520Rendering%2520via%2520Backpropagation%2520Refinement%2520Scheme%26entry.906535625%3DJintae%2520Kim%2520and%2520Seungwon%2520yang%2520and%2520Seong-Gyun%2520Jeong%2520and%2520Chang-Su%2520Kim%26entry.1292438233%3D%2520%2520A%2520novel%2520algorithm%2520for%2520face%2520obfuscation%252C%2520called%2520Forbes%252C%2520which%2520aims%2520to%250Aobfuscate%2520facial%2520appearance%2520recognizable%2520by%2520humans%2520but%2520preserve%2520the%2520identity%250Aand%2520attributes%2520decipherable%2520by%2520machines%252C%2520is%2520proposed%2520in%2520this%2520paper.%2520Forbes%250Afirst%2520applies%2520multiple%2520obfuscating%2520transformations%2520with%2520random%2520parameters%2520to%2520an%250Aimage%2520to%2520remove%2520the%2520identity%2520information%2520distinguishable%2520by%2520humans.%2520Then%252C%2520it%250Aoptimizes%2520the%2520parameters%2520to%2520make%2520the%2520transformed%2520image%2520decipherable%2520by%2520machines%250Abased%2520on%2520the%2520backpropagation%2520refinement%2520scheme.%2520Finally%252C%2520it%2520renders%2520an%250Aobfuscated%2520image%2520by%2520applying%2520the%2520transformations%2520with%2520the%2520optimized%2520parameters.%250AExperimental%2520results%2520on%2520various%2520datasets%2520demonstrate%2520that%2520Forbes%2520achieves%2520both%250Ahuman%2520indecipherability%2520and%2520machine%2520decipherability%2520excellently.%2520The%2520source%250Acodes%2520are%2520available%2520at%2520https%253A//github.com/mcljtkim/Forbes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14170v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forbes%3A%20Face%20Obfuscation%20Rendering%20via%20Backpropagation%20Refinement%20Scheme&entry.906535625=Jintae%20Kim%20and%20Seungwon%20yang%20and%20Seong-Gyun%20Jeong%20and%20Chang-Su%20Kim&entry.1292438233=%20%20A%20novel%20algorithm%20for%20face%20obfuscation%2C%20called%20Forbes%2C%20which%20aims%20to%0Aobfuscate%20facial%20appearance%20recognizable%20by%20humans%20but%20preserve%20the%20identity%0Aand%20attributes%20decipherable%20by%20machines%2C%20is%20proposed%20in%20this%20paper.%20Forbes%0Afirst%20applies%20multiple%20obfuscating%20transformations%20with%20random%20parameters%20to%20an%0Aimage%20to%20remove%20the%20identity%20information%20distinguishable%20by%20humans.%20Then%2C%20it%0Aoptimizes%20the%20parameters%20to%20make%20the%20transformed%20image%20decipherable%20by%20machines%0Abased%20on%20the%20backpropagation%20refinement%20scheme.%20Finally%2C%20it%20renders%20an%0Aobfuscated%20image%20by%20applying%20the%20transformations%20with%20the%20optimized%20parameters.%0AExperimental%20results%20on%20various%20datasets%20demonstrate%20that%20Forbes%20achieves%20both%0Ahuman%20indecipherability%20and%20machine%20decipherability%20excellently.%20The%20source%0Acodes%20are%20available%20at%20https%3A//github.com/mcljtkim/Forbes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14170v1&entry.124074799=Read"},
{"title": "Simple, unified analysis of Johnson-Lindenstrauss with applications", "author": "Yingru Li", "abstract": "  We present a simplified and unified analysis of the Johnson-Lindenstrauss\n(JL) lemma, a cornerstone of dimensionality reduction for managing\nhigh-dimensional data. Our approach simplifies understanding and unifies\nvarious constructions under the JL framework, including spherical, binary-coin,\nsparse JL, Gaussian, and sub-Gaussian models. This unification preserves the\nintrinsic geometry of data, essential for applications from streaming\nalgorithms to reinforcement learning. We provide the first rigorous proof of\nthe spherical construction's effectiveness and introduce a general class of\nsub-Gaussian constructions within this simplified framework. Central to our\ncontribution is an innovative extension of the Hanson-Wright inequality to high\ndimensions, complete with explicit constants. By using simple yet powerful\nprobabilistic tools and analytical techniques, such as an enhanced\ndiagonalization process, our analysis solidifies the theoretical foundation of\nthe JL lemma by removing an independence assumption and extends its practical\napplicability to contemporary algorithms.\n", "link": "http://arxiv.org/abs/2402.10232v4", "date": "2024-07-19", "relevancy": 1.9485, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5119}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4764}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simple%2C%20unified%20analysis%20of%20Johnson-Lindenstrauss%20with%20applications&body=Title%3A%20Simple%2C%20unified%20analysis%20of%20Johnson-Lindenstrauss%20with%20applications%0AAuthor%3A%20Yingru%20Li%0AAbstract%3A%20%20%20We%20present%20a%20simplified%20and%20unified%20analysis%20of%20the%20Johnson-Lindenstrauss%0A%28JL%29%20lemma%2C%20a%20cornerstone%20of%20dimensionality%20reduction%20for%20managing%0Ahigh-dimensional%20data.%20Our%20approach%20simplifies%20understanding%20and%20unifies%0Avarious%20constructions%20under%20the%20JL%20framework%2C%20including%20spherical%2C%20binary-coin%2C%0Asparse%20JL%2C%20Gaussian%2C%20and%20sub-Gaussian%20models.%20This%20unification%20preserves%20the%0Aintrinsic%20geometry%20of%20data%2C%20essential%20for%20applications%20from%20streaming%0Aalgorithms%20to%20reinforcement%20learning.%20We%20provide%20the%20first%20rigorous%20proof%20of%0Athe%20spherical%20construction%27s%20effectiveness%20and%20introduce%20a%20general%20class%20of%0Asub-Gaussian%20constructions%20within%20this%20simplified%20framework.%20Central%20to%20our%0Acontribution%20is%20an%20innovative%20extension%20of%20the%20Hanson-Wright%20inequality%20to%20high%0Adimensions%2C%20complete%20with%20explicit%20constants.%20By%20using%20simple%20yet%20powerful%0Aprobabilistic%20tools%20and%20analytical%20techniques%2C%20such%20as%20an%20enhanced%0Adiagonalization%20process%2C%20our%20analysis%20solidifies%20the%20theoretical%20foundation%20of%0Athe%20JL%20lemma%20by%20removing%20an%20independence%20assumption%20and%20extends%20its%20practical%0Aapplicability%20to%20contemporary%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10232v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimple%252C%2520unified%2520analysis%2520of%2520Johnson-Lindenstrauss%2520with%2520applications%26entry.906535625%3DYingru%2520Li%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520simplified%2520and%2520unified%2520analysis%2520of%2520the%2520Johnson-Lindenstrauss%250A%2528JL%2529%2520lemma%252C%2520a%2520cornerstone%2520of%2520dimensionality%2520reduction%2520for%2520managing%250Ahigh-dimensional%2520data.%2520Our%2520approach%2520simplifies%2520understanding%2520and%2520unifies%250Avarious%2520constructions%2520under%2520the%2520JL%2520framework%252C%2520including%2520spherical%252C%2520binary-coin%252C%250Asparse%2520JL%252C%2520Gaussian%252C%2520and%2520sub-Gaussian%2520models.%2520This%2520unification%2520preserves%2520the%250Aintrinsic%2520geometry%2520of%2520data%252C%2520essential%2520for%2520applications%2520from%2520streaming%250Aalgorithms%2520to%2520reinforcement%2520learning.%2520We%2520provide%2520the%2520first%2520rigorous%2520proof%2520of%250Athe%2520spherical%2520construction%2527s%2520effectiveness%2520and%2520introduce%2520a%2520general%2520class%2520of%250Asub-Gaussian%2520constructions%2520within%2520this%2520simplified%2520framework.%2520Central%2520to%2520our%250Acontribution%2520is%2520an%2520innovative%2520extension%2520of%2520the%2520Hanson-Wright%2520inequality%2520to%2520high%250Adimensions%252C%2520complete%2520with%2520explicit%2520constants.%2520By%2520using%2520simple%2520yet%2520powerful%250Aprobabilistic%2520tools%2520and%2520analytical%2520techniques%252C%2520such%2520as%2520an%2520enhanced%250Adiagonalization%2520process%252C%2520our%2520analysis%2520solidifies%2520the%2520theoretical%2520foundation%2520of%250Athe%2520JL%2520lemma%2520by%2520removing%2520an%2520independence%2520assumption%2520and%2520extends%2520its%2520practical%250Aapplicability%2520to%2520contemporary%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10232v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simple%2C%20unified%20analysis%20of%20Johnson-Lindenstrauss%20with%20applications&entry.906535625=Yingru%20Li&entry.1292438233=%20%20We%20present%20a%20simplified%20and%20unified%20analysis%20of%20the%20Johnson-Lindenstrauss%0A%28JL%29%20lemma%2C%20a%20cornerstone%20of%20dimensionality%20reduction%20for%20managing%0Ahigh-dimensional%20data.%20Our%20approach%20simplifies%20understanding%20and%20unifies%0Avarious%20constructions%20under%20the%20JL%20framework%2C%20including%20spherical%2C%20binary-coin%2C%0Asparse%20JL%2C%20Gaussian%2C%20and%20sub-Gaussian%20models.%20This%20unification%20preserves%20the%0Aintrinsic%20geometry%20of%20data%2C%20essential%20for%20applications%20from%20streaming%0Aalgorithms%20to%20reinforcement%20learning.%20We%20provide%20the%20first%20rigorous%20proof%20of%0Athe%20spherical%20construction%27s%20effectiveness%20and%20introduce%20a%20general%20class%20of%0Asub-Gaussian%20constructions%20within%20this%20simplified%20framework.%20Central%20to%20our%0Acontribution%20is%20an%20innovative%20extension%20of%20the%20Hanson-Wright%20inequality%20to%20high%0Adimensions%2C%20complete%20with%20explicit%20constants.%20By%20using%20simple%20yet%20powerful%0Aprobabilistic%20tools%20and%20analytical%20techniques%2C%20such%20as%20an%20enhanced%0Adiagonalization%20process%2C%20our%20analysis%20solidifies%20the%20theoretical%20foundation%20of%0Athe%20JL%20lemma%20by%20removing%20an%20independence%20assumption%20and%20extends%20its%20practical%0Aapplicability%20to%20contemporary%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10232v4&entry.124074799=Read"},
{"title": "Safety-Oriented Calibration and Evaluation of the Intelligent Driver\n  Model", "author": "Kingsley Adjenughwure and Arturo Tejada and Pedro F. V. Oliveira and Jeroen Hogema and Gerdien Klunder", "abstract": "  Many car-following models like the Intelligent Driver Model (IDM) incorporate\nimportant aspects of safety in their definitions, such as collision-free\ndriving and keeping safe distances, implying that drivers are safety conscious\nwhen driving. Despite their safety-oriented nature, when calibrating and\nevaluating these models, the main objective of most studies is to find model\nparameters that minimize the error in observed measurements like spacing and\nspeed while studies specifically focused on calibrating and evaluating\nunobserved safe behavior captured by the parameters of the model are scarce.\nMost studies on calibration and evaluation of the IDM do not check if the\nobserved driving behavior (i.e. spacing) are within the model estimated\nunobserved safety thresholds (i.e. desired safety spacing) or what parameters\nare important for safety. This limits their application for safety driven\ntraffic simulations. To fill this gap, this paper first proposes a simple\nmetric to evaluate driver compliance with the safety thresholds of the IDM\nmodel. Specifically, we evaluate driver compliance to their desired safety\nspacing, speed and safe time gap. Next, a method to enforce compliance to the\nsafety threshold during model calibration is proposed. The proposed compliance\nmetric and the calibration approach is tested using Dutch highway trajectory\ndata obtained from a driving simulator experiment and two drones. The results\nshow that compliance to the IDM safety threshold greatly depends on braking\ncapability with a median compliance between 38% and 90% of driving time,\nindicating that drivers can only partially follow the IDM safety threshold in\nreality.\n", "link": "http://arxiv.org/abs/2310.04259v2", "date": "2024-07-19", "relevancy": 1.9485, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5055}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4893}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safety-Oriented%20Calibration%20and%20Evaluation%20of%20the%20Intelligent%20Driver%0A%20%20Model&body=Title%3A%20Safety-Oriented%20Calibration%20and%20Evaluation%20of%20the%20Intelligent%20Driver%0A%20%20Model%0AAuthor%3A%20Kingsley%20Adjenughwure%20and%20Arturo%20Tejada%20and%20Pedro%20F.%20V.%20Oliveira%20and%20Jeroen%20Hogema%20and%20Gerdien%20Klunder%0AAbstract%3A%20%20%20Many%20car-following%20models%20like%20the%20Intelligent%20Driver%20Model%20%28IDM%29%20incorporate%0Aimportant%20aspects%20of%20safety%20in%20their%20definitions%2C%20such%20as%20collision-free%0Adriving%20and%20keeping%20safe%20distances%2C%20implying%20that%20drivers%20are%20safety%20conscious%0Awhen%20driving.%20Despite%20their%20safety-oriented%20nature%2C%20when%20calibrating%20and%0Aevaluating%20these%20models%2C%20the%20main%20objective%20of%20most%20studies%20is%20to%20find%20model%0Aparameters%20that%20minimize%20the%20error%20in%20observed%20measurements%20like%20spacing%20and%0Aspeed%20while%20studies%20specifically%20focused%20on%20calibrating%20and%20evaluating%0Aunobserved%20safe%20behavior%20captured%20by%20the%20parameters%20of%20the%20model%20are%20scarce.%0AMost%20studies%20on%20calibration%20and%20evaluation%20of%20the%20IDM%20do%20not%20check%20if%20the%0Aobserved%20driving%20behavior%20%28i.e.%20spacing%29%20are%20within%20the%20model%20estimated%0Aunobserved%20safety%20thresholds%20%28i.e.%20desired%20safety%20spacing%29%20or%20what%20parameters%0Aare%20important%20for%20safety.%20This%20limits%20their%20application%20for%20safety%20driven%0Atraffic%20simulations.%20To%20fill%20this%20gap%2C%20this%20paper%20first%20proposes%20a%20simple%0Ametric%20to%20evaluate%20driver%20compliance%20with%20the%20safety%20thresholds%20of%20the%20IDM%0Amodel.%20Specifically%2C%20we%20evaluate%20driver%20compliance%20to%20their%20desired%20safety%0Aspacing%2C%20speed%20and%20safe%20time%20gap.%20Next%2C%20a%20method%20to%20enforce%20compliance%20to%20the%0Asafety%20threshold%20during%20model%20calibration%20is%20proposed.%20The%20proposed%20compliance%0Ametric%20and%20the%20calibration%20approach%20is%20tested%20using%20Dutch%20highway%20trajectory%0Adata%20obtained%20from%20a%20driving%20simulator%20experiment%20and%20two%20drones.%20The%20results%0Ashow%20that%20compliance%20to%20the%20IDM%20safety%20threshold%20greatly%20depends%20on%20braking%0Acapability%20with%20a%20median%20compliance%20between%2038%25%20and%2090%25%20of%20driving%20time%2C%0Aindicating%20that%20drivers%20can%20only%20partially%20follow%20the%20IDM%20safety%20threshold%20in%0Areality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.04259v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafety-Oriented%2520Calibration%2520and%2520Evaluation%2520of%2520the%2520Intelligent%2520Driver%250A%2520%2520Model%26entry.906535625%3DKingsley%2520Adjenughwure%2520and%2520Arturo%2520Tejada%2520and%2520Pedro%2520F.%2520V.%2520Oliveira%2520and%2520Jeroen%2520Hogema%2520and%2520Gerdien%2520Klunder%26entry.1292438233%3D%2520%2520Many%2520car-following%2520models%2520like%2520the%2520Intelligent%2520Driver%2520Model%2520%2528IDM%2529%2520incorporate%250Aimportant%2520aspects%2520of%2520safety%2520in%2520their%2520definitions%252C%2520such%2520as%2520collision-free%250Adriving%2520and%2520keeping%2520safe%2520distances%252C%2520implying%2520that%2520drivers%2520are%2520safety%2520conscious%250Awhen%2520driving.%2520Despite%2520their%2520safety-oriented%2520nature%252C%2520when%2520calibrating%2520and%250Aevaluating%2520these%2520models%252C%2520the%2520main%2520objective%2520of%2520most%2520studies%2520is%2520to%2520find%2520model%250Aparameters%2520that%2520minimize%2520the%2520error%2520in%2520observed%2520measurements%2520like%2520spacing%2520and%250Aspeed%2520while%2520studies%2520specifically%2520focused%2520on%2520calibrating%2520and%2520evaluating%250Aunobserved%2520safe%2520behavior%2520captured%2520by%2520the%2520parameters%2520of%2520the%2520model%2520are%2520scarce.%250AMost%2520studies%2520on%2520calibration%2520and%2520evaluation%2520of%2520the%2520IDM%2520do%2520not%2520check%2520if%2520the%250Aobserved%2520driving%2520behavior%2520%2528i.e.%2520spacing%2529%2520are%2520within%2520the%2520model%2520estimated%250Aunobserved%2520safety%2520thresholds%2520%2528i.e.%2520desired%2520safety%2520spacing%2529%2520or%2520what%2520parameters%250Aare%2520important%2520for%2520safety.%2520This%2520limits%2520their%2520application%2520for%2520safety%2520driven%250Atraffic%2520simulations.%2520To%2520fill%2520this%2520gap%252C%2520this%2520paper%2520first%2520proposes%2520a%2520simple%250Ametric%2520to%2520evaluate%2520driver%2520compliance%2520with%2520the%2520safety%2520thresholds%2520of%2520the%2520IDM%250Amodel.%2520Specifically%252C%2520we%2520evaluate%2520driver%2520compliance%2520to%2520their%2520desired%2520safety%250Aspacing%252C%2520speed%2520and%2520safe%2520time%2520gap.%2520Next%252C%2520a%2520method%2520to%2520enforce%2520compliance%2520to%2520the%250Asafety%2520threshold%2520during%2520model%2520calibration%2520is%2520proposed.%2520The%2520proposed%2520compliance%250Ametric%2520and%2520the%2520calibration%2520approach%2520is%2520tested%2520using%2520Dutch%2520highway%2520trajectory%250Adata%2520obtained%2520from%2520a%2520driving%2520simulator%2520experiment%2520and%2520two%2520drones.%2520The%2520results%250Ashow%2520that%2520compliance%2520to%2520the%2520IDM%2520safety%2520threshold%2520greatly%2520depends%2520on%2520braking%250Acapability%2520with%2520a%2520median%2520compliance%2520between%252038%2525%2520and%252090%2525%2520of%2520driving%2520time%252C%250Aindicating%2520that%2520drivers%2520can%2520only%2520partially%2520follow%2520the%2520IDM%2520safety%2520threshold%2520in%250Areality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.04259v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safety-Oriented%20Calibration%20and%20Evaluation%20of%20the%20Intelligent%20Driver%0A%20%20Model&entry.906535625=Kingsley%20Adjenughwure%20and%20Arturo%20Tejada%20and%20Pedro%20F.%20V.%20Oliveira%20and%20Jeroen%20Hogema%20and%20Gerdien%20Klunder&entry.1292438233=%20%20Many%20car-following%20models%20like%20the%20Intelligent%20Driver%20Model%20%28IDM%29%20incorporate%0Aimportant%20aspects%20of%20safety%20in%20their%20definitions%2C%20such%20as%20collision-free%0Adriving%20and%20keeping%20safe%20distances%2C%20implying%20that%20drivers%20are%20safety%20conscious%0Awhen%20driving.%20Despite%20their%20safety-oriented%20nature%2C%20when%20calibrating%20and%0Aevaluating%20these%20models%2C%20the%20main%20objective%20of%20most%20studies%20is%20to%20find%20model%0Aparameters%20that%20minimize%20the%20error%20in%20observed%20measurements%20like%20spacing%20and%0Aspeed%20while%20studies%20specifically%20focused%20on%20calibrating%20and%20evaluating%0Aunobserved%20safe%20behavior%20captured%20by%20the%20parameters%20of%20the%20model%20are%20scarce.%0AMost%20studies%20on%20calibration%20and%20evaluation%20of%20the%20IDM%20do%20not%20check%20if%20the%0Aobserved%20driving%20behavior%20%28i.e.%20spacing%29%20are%20within%20the%20model%20estimated%0Aunobserved%20safety%20thresholds%20%28i.e.%20desired%20safety%20spacing%29%20or%20what%20parameters%0Aare%20important%20for%20safety.%20This%20limits%20their%20application%20for%20safety%20driven%0Atraffic%20simulations.%20To%20fill%20this%20gap%2C%20this%20paper%20first%20proposes%20a%20simple%0Ametric%20to%20evaluate%20driver%20compliance%20with%20the%20safety%20thresholds%20of%20the%20IDM%0Amodel.%20Specifically%2C%20we%20evaluate%20driver%20compliance%20to%20their%20desired%20safety%0Aspacing%2C%20speed%20and%20safe%20time%20gap.%20Next%2C%20a%20method%20to%20enforce%20compliance%20to%20the%0Asafety%20threshold%20during%20model%20calibration%20is%20proposed.%20The%20proposed%20compliance%0Ametric%20and%20the%20calibration%20approach%20is%20tested%20using%20Dutch%20highway%20trajectory%0Adata%20obtained%20from%20a%20driving%20simulator%20experiment%20and%20two%20drones.%20The%20results%0Ashow%20that%20compliance%20to%20the%20IDM%20safety%20threshold%20greatly%20depends%20on%20braking%0Acapability%20with%20a%20median%20compliance%20between%2038%25%20and%2090%25%20of%20driving%20time%2C%0Aindicating%20that%20drivers%20can%20only%20partially%20follow%20the%20IDM%20safety%20threshold%20in%0Areality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.04259v2&entry.124074799=Read"},
{"title": "Improving GBDT Performance on Imbalanced Datasets: An Empirical Study of\n  Class-Balanced Loss Functions", "author": "Jiaqi Luo and Yuan Yuan and Shixin Xu", "abstract": "  Class imbalance remains a significant challenge in machine learning,\nparticularly for tabular data classification tasks. While Gradient Boosting\nDecision Trees (GBDT) models have proven highly effective for such tasks, their\nperformance can be compromised when dealing with imbalanced datasets. This\npaper presents the first comprehensive study on adapting class-balanced loss\nfunctions to three GBDT algorithms across various tabular classification tasks,\nincluding binary, multi-class, and multi-label classification. We conduct\nextensive experiments on multiple datasets to evaluate the impact of\nclass-balanced losses on different GBDT models, establishing a valuable\nbenchmark. Our results demonstrate the potential of class-balanced loss\nfunctions to enhance GBDT performance on imbalanced datasets, offering a robust\napproach for practitioners facing class imbalance challenges in real-world\napplications. Additionally, we introduce a Python package that facilitates the\nintegration of class-balanced loss functions into GBDT workflows, making these\nadvanced techniques accessible to a wider audience.\n", "link": "http://arxiv.org/abs/2407.14381v1", "date": "2024-07-19", "relevancy": 1.9411, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5183}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.471}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4385}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20GBDT%20Performance%20on%20Imbalanced%20Datasets%3A%20An%20Empirical%20Study%20of%0A%20%20Class-Balanced%20Loss%20Functions&body=Title%3A%20Improving%20GBDT%20Performance%20on%20Imbalanced%20Datasets%3A%20An%20Empirical%20Study%20of%0A%20%20Class-Balanced%20Loss%20Functions%0AAuthor%3A%20Jiaqi%20Luo%20and%20Yuan%20Yuan%20and%20Shixin%20Xu%0AAbstract%3A%20%20%20Class%20imbalance%20remains%20a%20significant%20challenge%20in%20machine%20learning%2C%0Aparticularly%20for%20tabular%20data%20classification%20tasks.%20While%20Gradient%20Boosting%0ADecision%20Trees%20%28GBDT%29%20models%20have%20proven%20highly%20effective%20for%20such%20tasks%2C%20their%0Aperformance%20can%20be%20compromised%20when%20dealing%20with%20imbalanced%20datasets.%20This%0Apaper%20presents%20the%20first%20comprehensive%20study%20on%20adapting%20class-balanced%20loss%0Afunctions%20to%20three%20GBDT%20algorithms%20across%20various%20tabular%20classification%20tasks%2C%0Aincluding%20binary%2C%20multi-class%2C%20and%20multi-label%20classification.%20We%20conduct%0Aextensive%20experiments%20on%20multiple%20datasets%20to%20evaluate%20the%20impact%20of%0Aclass-balanced%20losses%20on%20different%20GBDT%20models%2C%20establishing%20a%20valuable%0Abenchmark.%20Our%20results%20demonstrate%20the%20potential%20of%20class-balanced%20loss%0Afunctions%20to%20enhance%20GBDT%20performance%20on%20imbalanced%20datasets%2C%20offering%20a%20robust%0Aapproach%20for%20practitioners%20facing%20class%20imbalance%20challenges%20in%20real-world%0Aapplications.%20Additionally%2C%20we%20introduce%20a%20Python%20package%20that%20facilitates%20the%0Aintegration%20of%20class-balanced%20loss%20functions%20into%20GBDT%20workflows%2C%20making%20these%0Aadvanced%20techniques%20accessible%20to%20a%20wider%20audience.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14381v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520GBDT%2520Performance%2520on%2520Imbalanced%2520Datasets%253A%2520An%2520Empirical%2520Study%2520of%250A%2520%2520Class-Balanced%2520Loss%2520Functions%26entry.906535625%3DJiaqi%2520Luo%2520and%2520Yuan%2520Yuan%2520and%2520Shixin%2520Xu%26entry.1292438233%3D%2520%2520Class%2520imbalance%2520remains%2520a%2520significant%2520challenge%2520in%2520machine%2520learning%252C%250Aparticularly%2520for%2520tabular%2520data%2520classification%2520tasks.%2520While%2520Gradient%2520Boosting%250ADecision%2520Trees%2520%2528GBDT%2529%2520models%2520have%2520proven%2520highly%2520effective%2520for%2520such%2520tasks%252C%2520their%250Aperformance%2520can%2520be%2520compromised%2520when%2520dealing%2520with%2520imbalanced%2520datasets.%2520This%250Apaper%2520presents%2520the%2520first%2520comprehensive%2520study%2520on%2520adapting%2520class-balanced%2520loss%250Afunctions%2520to%2520three%2520GBDT%2520algorithms%2520across%2520various%2520tabular%2520classification%2520tasks%252C%250Aincluding%2520binary%252C%2520multi-class%252C%2520and%2520multi-label%2520classification.%2520We%2520conduct%250Aextensive%2520experiments%2520on%2520multiple%2520datasets%2520to%2520evaluate%2520the%2520impact%2520of%250Aclass-balanced%2520losses%2520on%2520different%2520GBDT%2520models%252C%2520establishing%2520a%2520valuable%250Abenchmark.%2520Our%2520results%2520demonstrate%2520the%2520potential%2520of%2520class-balanced%2520loss%250Afunctions%2520to%2520enhance%2520GBDT%2520performance%2520on%2520imbalanced%2520datasets%252C%2520offering%2520a%2520robust%250Aapproach%2520for%2520practitioners%2520facing%2520class%2520imbalance%2520challenges%2520in%2520real-world%250Aapplications.%2520Additionally%252C%2520we%2520introduce%2520a%2520Python%2520package%2520that%2520facilitates%2520the%250Aintegration%2520of%2520class-balanced%2520loss%2520functions%2520into%2520GBDT%2520workflows%252C%2520making%2520these%250Aadvanced%2520techniques%2520accessible%2520to%2520a%2520wider%2520audience.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14381v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20GBDT%20Performance%20on%20Imbalanced%20Datasets%3A%20An%20Empirical%20Study%20of%0A%20%20Class-Balanced%20Loss%20Functions&entry.906535625=Jiaqi%20Luo%20and%20Yuan%20Yuan%20and%20Shixin%20Xu&entry.1292438233=%20%20Class%20imbalance%20remains%20a%20significant%20challenge%20in%20machine%20learning%2C%0Aparticularly%20for%20tabular%20data%20classification%20tasks.%20While%20Gradient%20Boosting%0ADecision%20Trees%20%28GBDT%29%20models%20have%20proven%20highly%20effective%20for%20such%20tasks%2C%20their%0Aperformance%20can%20be%20compromised%20when%20dealing%20with%20imbalanced%20datasets.%20This%0Apaper%20presents%20the%20first%20comprehensive%20study%20on%20adapting%20class-balanced%20loss%0Afunctions%20to%20three%20GBDT%20algorithms%20across%20various%20tabular%20classification%20tasks%2C%0Aincluding%20binary%2C%20multi-class%2C%20and%20multi-label%20classification.%20We%20conduct%0Aextensive%20experiments%20on%20multiple%20datasets%20to%20evaluate%20the%20impact%20of%0Aclass-balanced%20losses%20on%20different%20GBDT%20models%2C%20establishing%20a%20valuable%0Abenchmark.%20Our%20results%20demonstrate%20the%20potential%20of%20class-balanced%20loss%0Afunctions%20to%20enhance%20GBDT%20performance%20on%20imbalanced%20datasets%2C%20offering%20a%20robust%0Aapproach%20for%20practitioners%20facing%20class%20imbalance%20challenges%20in%20real-world%0Aapplications.%20Additionally%2C%20we%20introduce%20a%20Python%20package%20that%20facilitates%20the%0Aintegration%20of%20class-balanced%20loss%20functions%20into%20GBDT%20workflows%2C%20making%20these%0Aadvanced%20techniques%20accessible%20to%20a%20wider%20audience.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14381v1&entry.124074799=Read"},
{"title": "A review on vision-based motion estimation", "author": "Hongyi Liu and Haifeng Wang", "abstract": "  Compared to contact sensors-based motion measurement, vision-based motion\nmeasurement has advantages of low cost and high efficiency and have been under\nactive development in the past decades. This paper provides a review on\nexisting motion measurement methods. In addition to the development of each\nbranch of vision-based motion measurement methods, this paper also discussed\nthe advantages and disadvantages of existing methods. Based on this discussion,\nit was identified that existing methods have a common limitation in optimally\nbalancing accuracy and robustness. To address issue, we developed the Gaussian\nkernel-based motion measurement method. Preliminary study shows that the\ndeveloped method can achieve high accuracy on simple synthesized images.\n", "link": "http://arxiv.org/abs/2407.14478v1", "date": "2024-07-19", "relevancy": 1.941, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5032}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4883}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20review%20on%20vision-based%20motion%20estimation&body=Title%3A%20A%20review%20on%20vision-based%20motion%20estimation%0AAuthor%3A%20Hongyi%20Liu%20and%20Haifeng%20Wang%0AAbstract%3A%20%20%20Compared%20to%20contact%20sensors-based%20motion%20measurement%2C%20vision-based%20motion%0Ameasurement%20has%20advantages%20of%20low%20cost%20and%20high%20efficiency%20and%20have%20been%20under%0Aactive%20development%20in%20the%20past%20decades.%20This%20paper%20provides%20a%20review%20on%0Aexisting%20motion%20measurement%20methods.%20In%20addition%20to%20the%20development%20of%20each%0Abranch%20of%20vision-based%20motion%20measurement%20methods%2C%20this%20paper%20also%20discussed%0Athe%20advantages%20and%20disadvantages%20of%20existing%20methods.%20Based%20on%20this%20discussion%2C%0Ait%20was%20identified%20that%20existing%20methods%20have%20a%20common%20limitation%20in%20optimally%0Abalancing%20accuracy%20and%20robustness.%20To%20address%20issue%2C%20we%20developed%20the%20Gaussian%0Akernel-based%20motion%20measurement%20method.%20Preliminary%20study%20shows%20that%20the%0Adeveloped%20method%20can%20achieve%20high%20accuracy%20on%20simple%20synthesized%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14478v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520review%2520on%2520vision-based%2520motion%2520estimation%26entry.906535625%3DHongyi%2520Liu%2520and%2520Haifeng%2520Wang%26entry.1292438233%3D%2520%2520Compared%2520to%2520contact%2520sensors-based%2520motion%2520measurement%252C%2520vision-based%2520motion%250Ameasurement%2520has%2520advantages%2520of%2520low%2520cost%2520and%2520high%2520efficiency%2520and%2520have%2520been%2520under%250Aactive%2520development%2520in%2520the%2520past%2520decades.%2520This%2520paper%2520provides%2520a%2520review%2520on%250Aexisting%2520motion%2520measurement%2520methods.%2520In%2520addition%2520to%2520the%2520development%2520of%2520each%250Abranch%2520of%2520vision-based%2520motion%2520measurement%2520methods%252C%2520this%2520paper%2520also%2520discussed%250Athe%2520advantages%2520and%2520disadvantages%2520of%2520existing%2520methods.%2520Based%2520on%2520this%2520discussion%252C%250Ait%2520was%2520identified%2520that%2520existing%2520methods%2520have%2520a%2520common%2520limitation%2520in%2520optimally%250Abalancing%2520accuracy%2520and%2520robustness.%2520To%2520address%2520issue%252C%2520we%2520developed%2520the%2520Gaussian%250Akernel-based%2520motion%2520measurement%2520method.%2520Preliminary%2520study%2520shows%2520that%2520the%250Adeveloped%2520method%2520can%2520achieve%2520high%2520accuracy%2520on%2520simple%2520synthesized%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14478v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20review%20on%20vision-based%20motion%20estimation&entry.906535625=Hongyi%20Liu%20and%20Haifeng%20Wang&entry.1292438233=%20%20Compared%20to%20contact%20sensors-based%20motion%20measurement%2C%20vision-based%20motion%0Ameasurement%20has%20advantages%20of%20low%20cost%20and%20high%20efficiency%20and%20have%20been%20under%0Aactive%20development%20in%20the%20past%20decades.%20This%20paper%20provides%20a%20review%20on%0Aexisting%20motion%20measurement%20methods.%20In%20addition%20to%20the%20development%20of%20each%0Abranch%20of%20vision-based%20motion%20measurement%20methods%2C%20this%20paper%20also%20discussed%0Athe%20advantages%20and%20disadvantages%20of%20existing%20methods.%20Based%20on%20this%20discussion%2C%0Ait%20was%20identified%20that%20existing%20methods%20have%20a%20common%20limitation%20in%20optimally%0Abalancing%20accuracy%20and%20robustness.%20To%20address%20issue%2C%20we%20developed%20the%20Gaussian%0Akernel-based%20motion%20measurement%20method.%20Preliminary%20study%20shows%20that%20the%0Adeveloped%20method%20can%20achieve%20high%20accuracy%20on%20simple%20synthesized%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14478v1&entry.124074799=Read"},
{"title": "Hyperparameter Optimization for Driving Strategies Based on\n  Reinforcement Learning", "author": "Nihal Acharya Adde and Hanno Gottschalk and Andreas Ebert", "abstract": "  This paper focuses on hyperparameter optimization for autonomous driving\nstrategies based on Reinforcement Learning. We provide a detailed description\nof training the RL agent in a simulation environment. Subsequently, we employ\nEfficient Global Optimization algorithm that uses Gaussian Process fitting for\nhyperparameter optimization in RL. Before this optimization phase, Gaussian\nprocess interpolation is applied to fit the surrogate model, for which the\nhyperparameter set is generated using Latin hypercube sampling. To accelerate\nthe evaluation, parallelization techniques are employed. Following the\nhyperparameter optimization procedure, a set of hyperparameters is identified,\nresulting in a noteworthy enhancement in overall driving performance. There is\na substantial increase of 4\\% when compared to existing manually tuned\nparameters and the hyperparameters discovered during the initialization process\nusing Latin hypercube sampling. After the optimization, we analyze the obtained\nresults thoroughly and conduct a sensitivity analysis to assess the robustness\nand generalization capabilities of the learned autonomous driving strategies.\nThe findings from this study contribute to the advancement of Gaussian process\nbased Bayesian optimization to optimize the hyperparameters for autonomous\ndriving in RL, providing valuable insights for the development of efficient and\nreliable autonomous driving systems.\n", "link": "http://arxiv.org/abs/2407.14262v1", "date": "2024-07-19", "relevancy": 1.9374, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.49}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.485}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hyperparameter%20Optimization%20for%20Driving%20Strategies%20Based%20on%0A%20%20Reinforcement%20Learning&body=Title%3A%20Hyperparameter%20Optimization%20for%20Driving%20Strategies%20Based%20on%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Nihal%20Acharya%20Adde%20and%20Hanno%20Gottschalk%20and%20Andreas%20Ebert%0AAbstract%3A%20%20%20This%20paper%20focuses%20on%20hyperparameter%20optimization%20for%20autonomous%20driving%0Astrategies%20based%20on%20Reinforcement%20Learning.%20We%20provide%20a%20detailed%20description%0Aof%20training%20the%20RL%20agent%20in%20a%20simulation%20environment.%20Subsequently%2C%20we%20employ%0AEfficient%20Global%20Optimization%20algorithm%20that%20uses%20Gaussian%20Process%20fitting%20for%0Ahyperparameter%20optimization%20in%20RL.%20Before%20this%20optimization%20phase%2C%20Gaussian%0Aprocess%20interpolation%20is%20applied%20to%20fit%20the%20surrogate%20model%2C%20for%20which%20the%0Ahyperparameter%20set%20is%20generated%20using%20Latin%20hypercube%20sampling.%20To%20accelerate%0Athe%20evaluation%2C%20parallelization%20techniques%20are%20employed.%20Following%20the%0Ahyperparameter%20optimization%20procedure%2C%20a%20set%20of%20hyperparameters%20is%20identified%2C%0Aresulting%20in%20a%20noteworthy%20enhancement%20in%20overall%20driving%20performance.%20There%20is%0Aa%20substantial%20increase%20of%204%5C%25%20when%20compared%20to%20existing%20manually%20tuned%0Aparameters%20and%20the%20hyperparameters%20discovered%20during%20the%20initialization%20process%0Ausing%20Latin%20hypercube%20sampling.%20After%20the%20optimization%2C%20we%20analyze%20the%20obtained%0Aresults%20thoroughly%20and%20conduct%20a%20sensitivity%20analysis%20to%20assess%20the%20robustness%0Aand%20generalization%20capabilities%20of%20the%20learned%20autonomous%20driving%20strategies.%0AThe%20findings%20from%20this%20study%20contribute%20to%20the%20advancement%20of%20Gaussian%20process%0Abased%20Bayesian%20optimization%20to%20optimize%20the%20hyperparameters%20for%20autonomous%0Adriving%20in%20RL%2C%20providing%20valuable%20insights%20for%20the%20development%20of%20efficient%20and%0Areliable%20autonomous%20driving%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14262v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperparameter%2520Optimization%2520for%2520Driving%2520Strategies%2520Based%2520on%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DNihal%2520Acharya%2520Adde%2520and%2520Hanno%2520Gottschalk%2520and%2520Andreas%2520Ebert%26entry.1292438233%3D%2520%2520This%2520paper%2520focuses%2520on%2520hyperparameter%2520optimization%2520for%2520autonomous%2520driving%250Astrategies%2520based%2520on%2520Reinforcement%2520Learning.%2520We%2520provide%2520a%2520detailed%2520description%250Aof%2520training%2520the%2520RL%2520agent%2520in%2520a%2520simulation%2520environment.%2520Subsequently%252C%2520we%2520employ%250AEfficient%2520Global%2520Optimization%2520algorithm%2520that%2520uses%2520Gaussian%2520Process%2520fitting%2520for%250Ahyperparameter%2520optimization%2520in%2520RL.%2520Before%2520this%2520optimization%2520phase%252C%2520Gaussian%250Aprocess%2520interpolation%2520is%2520applied%2520to%2520fit%2520the%2520surrogate%2520model%252C%2520for%2520which%2520the%250Ahyperparameter%2520set%2520is%2520generated%2520using%2520Latin%2520hypercube%2520sampling.%2520To%2520accelerate%250Athe%2520evaluation%252C%2520parallelization%2520techniques%2520are%2520employed.%2520Following%2520the%250Ahyperparameter%2520optimization%2520procedure%252C%2520a%2520set%2520of%2520hyperparameters%2520is%2520identified%252C%250Aresulting%2520in%2520a%2520noteworthy%2520enhancement%2520in%2520overall%2520driving%2520performance.%2520There%2520is%250Aa%2520substantial%2520increase%2520of%25204%255C%2525%2520when%2520compared%2520to%2520existing%2520manually%2520tuned%250Aparameters%2520and%2520the%2520hyperparameters%2520discovered%2520during%2520the%2520initialization%2520process%250Ausing%2520Latin%2520hypercube%2520sampling.%2520After%2520the%2520optimization%252C%2520we%2520analyze%2520the%2520obtained%250Aresults%2520thoroughly%2520and%2520conduct%2520a%2520sensitivity%2520analysis%2520to%2520assess%2520the%2520robustness%250Aand%2520generalization%2520capabilities%2520of%2520the%2520learned%2520autonomous%2520driving%2520strategies.%250AThe%2520findings%2520from%2520this%2520study%2520contribute%2520to%2520the%2520advancement%2520of%2520Gaussian%2520process%250Abased%2520Bayesian%2520optimization%2520to%2520optimize%2520the%2520hyperparameters%2520for%2520autonomous%250Adriving%2520in%2520RL%252C%2520providing%2520valuable%2520insights%2520for%2520the%2520development%2520of%2520efficient%2520and%250Areliable%2520autonomous%2520driving%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14262v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyperparameter%20Optimization%20for%20Driving%20Strategies%20Based%20on%0A%20%20Reinforcement%20Learning&entry.906535625=Nihal%20Acharya%20Adde%20and%20Hanno%20Gottschalk%20and%20Andreas%20Ebert&entry.1292438233=%20%20This%20paper%20focuses%20on%20hyperparameter%20optimization%20for%20autonomous%20driving%0Astrategies%20based%20on%20Reinforcement%20Learning.%20We%20provide%20a%20detailed%20description%0Aof%20training%20the%20RL%20agent%20in%20a%20simulation%20environment.%20Subsequently%2C%20we%20employ%0AEfficient%20Global%20Optimization%20algorithm%20that%20uses%20Gaussian%20Process%20fitting%20for%0Ahyperparameter%20optimization%20in%20RL.%20Before%20this%20optimization%20phase%2C%20Gaussian%0Aprocess%20interpolation%20is%20applied%20to%20fit%20the%20surrogate%20model%2C%20for%20which%20the%0Ahyperparameter%20set%20is%20generated%20using%20Latin%20hypercube%20sampling.%20To%20accelerate%0Athe%20evaluation%2C%20parallelization%20techniques%20are%20employed.%20Following%20the%0Ahyperparameter%20optimization%20procedure%2C%20a%20set%20of%20hyperparameters%20is%20identified%2C%0Aresulting%20in%20a%20noteworthy%20enhancement%20in%20overall%20driving%20performance.%20There%20is%0Aa%20substantial%20increase%20of%204%5C%25%20when%20compared%20to%20existing%20manually%20tuned%0Aparameters%20and%20the%20hyperparameters%20discovered%20during%20the%20initialization%20process%0Ausing%20Latin%20hypercube%20sampling.%20After%20the%20optimization%2C%20we%20analyze%20the%20obtained%0Aresults%20thoroughly%20and%20conduct%20a%20sensitivity%20analysis%20to%20assess%20the%20robustness%0Aand%20generalization%20capabilities%20of%20the%20learned%20autonomous%20driving%20strategies.%0AThe%20findings%20from%20this%20study%20contribute%20to%20the%20advancement%20of%20Gaussian%20process%0Abased%20Bayesian%20optimization%20to%20optimize%20the%20hyperparameters%20for%20autonomous%0Adriving%20in%20RL%2C%20providing%20valuable%20insights%20for%20the%20development%20of%20efficient%20and%0Areliable%20autonomous%20driving%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14262v1&entry.124074799=Read"},
{"title": "Straightforward Layer-wise Pruning for More Efficient Visual Adaptation", "author": "Ruizi Han and Jinglei Tang", "abstract": "  Parameter-efficient transfer learning (PETL) aims to adapt large pre-trained\nmodels using limited parameters. While most PETL approaches update the added\nparameters and freeze pre-trained weights during training, the minimal impact\nof task-specific deep layers on cross-domain data poses a challenge as PETL\ncannot modify them, resulting in redundant model structures. Structural pruning\neffectively reduces model redundancy; however, common pruning methods often\nlead to an excessive increase in stored parameters due to varying pruning\nstructures based on pruning rates and data. Recognizing the storage parameter\nvolume issue, we propose a Straightforward layer-wise pruning method, called\nSLS, for pruning PETL-transferred models. By evaluating parameters from a\nfeature perspective of each layer and utilizing clustering metrics to assess\ncurrent parameters based on clustering phenomena in low-dimensional space\nobtained through t-SNE, SLS facilitates informed pruning decisions. Our study\nreveals that layer-wise pruning, with a focus on storing pruning indices,\naddresses storage volume concerns. Notably, mainstream Layer-wise pruning\nmethods may not be suitable for assessing layer importance in PETL-transferred\nmodels, where the majority of parameters are pre-trained and have limited\nrelevance to downstream datasets. Comparative analysis against state-of-the-art\nPETL methods demonstrates that the pruned model achieved a notable balance\nbetween model throughput and accuracy. Moreover, SLS effectively reduces\nstorage overhead arising from varying pruned structures while enhancing the\naccuracy and speed of pruned models compared to conventional pruning methods.\n", "link": "http://arxiv.org/abs/2407.14330v1", "date": "2024-07-19", "relevancy": 1.9257, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5047}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4691}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Straightforward%20Layer-wise%20Pruning%20for%20More%20Efficient%20Visual%20Adaptation&body=Title%3A%20Straightforward%20Layer-wise%20Pruning%20for%20More%20Efficient%20Visual%20Adaptation%0AAuthor%3A%20Ruizi%20Han%20and%20Jinglei%20Tang%0AAbstract%3A%20%20%20Parameter-efficient%20transfer%20learning%20%28PETL%29%20aims%20to%20adapt%20large%20pre-trained%0Amodels%20using%20limited%20parameters.%20While%20most%20PETL%20approaches%20update%20the%20added%0Aparameters%20and%20freeze%20pre-trained%20weights%20during%20training%2C%20the%20minimal%20impact%0Aof%20task-specific%20deep%20layers%20on%20cross-domain%20data%20poses%20a%20challenge%20as%20PETL%0Acannot%20modify%20them%2C%20resulting%20in%20redundant%20model%20structures.%20Structural%20pruning%0Aeffectively%20reduces%20model%20redundancy%3B%20however%2C%20common%20pruning%20methods%20often%0Alead%20to%20an%20excessive%20increase%20in%20stored%20parameters%20due%20to%20varying%20pruning%0Astructures%20based%20on%20pruning%20rates%20and%20data.%20Recognizing%20the%20storage%20parameter%0Avolume%20issue%2C%20we%20propose%20a%20Straightforward%20layer-wise%20pruning%20method%2C%20called%0ASLS%2C%20for%20pruning%20PETL-transferred%20models.%20By%20evaluating%20parameters%20from%20a%0Afeature%20perspective%20of%20each%20layer%20and%20utilizing%20clustering%20metrics%20to%20assess%0Acurrent%20parameters%20based%20on%20clustering%20phenomena%20in%20low-dimensional%20space%0Aobtained%20through%20t-SNE%2C%20SLS%20facilitates%20informed%20pruning%20decisions.%20Our%20study%0Areveals%20that%20layer-wise%20pruning%2C%20with%20a%20focus%20on%20storing%20pruning%20indices%2C%0Aaddresses%20storage%20volume%20concerns.%20Notably%2C%20mainstream%20Layer-wise%20pruning%0Amethods%20may%20not%20be%20suitable%20for%20assessing%20layer%20importance%20in%20PETL-transferred%0Amodels%2C%20where%20the%20majority%20of%20parameters%20are%20pre-trained%20and%20have%20limited%0Arelevance%20to%20downstream%20datasets.%20Comparative%20analysis%20against%20state-of-the-art%0APETL%20methods%20demonstrates%20that%20the%20pruned%20model%20achieved%20a%20notable%20balance%0Abetween%20model%20throughput%20and%20accuracy.%20Moreover%2C%20SLS%20effectively%20reduces%0Astorage%20overhead%20arising%20from%20varying%20pruned%20structures%20while%20enhancing%20the%0Aaccuracy%20and%20speed%20of%20pruned%20models%20compared%20to%20conventional%20pruning%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14330v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStraightforward%2520Layer-wise%2520Pruning%2520for%2520More%2520Efficient%2520Visual%2520Adaptation%26entry.906535625%3DRuizi%2520Han%2520and%2520Jinglei%2520Tang%26entry.1292438233%3D%2520%2520Parameter-efficient%2520transfer%2520learning%2520%2528PETL%2529%2520aims%2520to%2520adapt%2520large%2520pre-trained%250Amodels%2520using%2520limited%2520parameters.%2520While%2520most%2520PETL%2520approaches%2520update%2520the%2520added%250Aparameters%2520and%2520freeze%2520pre-trained%2520weights%2520during%2520training%252C%2520the%2520minimal%2520impact%250Aof%2520task-specific%2520deep%2520layers%2520on%2520cross-domain%2520data%2520poses%2520a%2520challenge%2520as%2520PETL%250Acannot%2520modify%2520them%252C%2520resulting%2520in%2520redundant%2520model%2520structures.%2520Structural%2520pruning%250Aeffectively%2520reduces%2520model%2520redundancy%253B%2520however%252C%2520common%2520pruning%2520methods%2520often%250Alead%2520to%2520an%2520excessive%2520increase%2520in%2520stored%2520parameters%2520due%2520to%2520varying%2520pruning%250Astructures%2520based%2520on%2520pruning%2520rates%2520and%2520data.%2520Recognizing%2520the%2520storage%2520parameter%250Avolume%2520issue%252C%2520we%2520propose%2520a%2520Straightforward%2520layer-wise%2520pruning%2520method%252C%2520called%250ASLS%252C%2520for%2520pruning%2520PETL-transferred%2520models.%2520By%2520evaluating%2520parameters%2520from%2520a%250Afeature%2520perspective%2520of%2520each%2520layer%2520and%2520utilizing%2520clustering%2520metrics%2520to%2520assess%250Acurrent%2520parameters%2520based%2520on%2520clustering%2520phenomena%2520in%2520low-dimensional%2520space%250Aobtained%2520through%2520t-SNE%252C%2520SLS%2520facilitates%2520informed%2520pruning%2520decisions.%2520Our%2520study%250Areveals%2520that%2520layer-wise%2520pruning%252C%2520with%2520a%2520focus%2520on%2520storing%2520pruning%2520indices%252C%250Aaddresses%2520storage%2520volume%2520concerns.%2520Notably%252C%2520mainstream%2520Layer-wise%2520pruning%250Amethods%2520may%2520not%2520be%2520suitable%2520for%2520assessing%2520layer%2520importance%2520in%2520PETL-transferred%250Amodels%252C%2520where%2520the%2520majority%2520of%2520parameters%2520are%2520pre-trained%2520and%2520have%2520limited%250Arelevance%2520to%2520downstream%2520datasets.%2520Comparative%2520analysis%2520against%2520state-of-the-art%250APETL%2520methods%2520demonstrates%2520that%2520the%2520pruned%2520model%2520achieved%2520a%2520notable%2520balance%250Abetween%2520model%2520throughput%2520and%2520accuracy.%2520Moreover%252C%2520SLS%2520effectively%2520reduces%250Astorage%2520overhead%2520arising%2520from%2520varying%2520pruned%2520structures%2520while%2520enhancing%2520the%250Aaccuracy%2520and%2520speed%2520of%2520pruned%2520models%2520compared%2520to%2520conventional%2520pruning%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14330v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Straightforward%20Layer-wise%20Pruning%20for%20More%20Efficient%20Visual%20Adaptation&entry.906535625=Ruizi%20Han%20and%20Jinglei%20Tang&entry.1292438233=%20%20Parameter-efficient%20transfer%20learning%20%28PETL%29%20aims%20to%20adapt%20large%20pre-trained%0Amodels%20using%20limited%20parameters.%20While%20most%20PETL%20approaches%20update%20the%20added%0Aparameters%20and%20freeze%20pre-trained%20weights%20during%20training%2C%20the%20minimal%20impact%0Aof%20task-specific%20deep%20layers%20on%20cross-domain%20data%20poses%20a%20challenge%20as%20PETL%0Acannot%20modify%20them%2C%20resulting%20in%20redundant%20model%20structures.%20Structural%20pruning%0Aeffectively%20reduces%20model%20redundancy%3B%20however%2C%20common%20pruning%20methods%20often%0Alead%20to%20an%20excessive%20increase%20in%20stored%20parameters%20due%20to%20varying%20pruning%0Astructures%20based%20on%20pruning%20rates%20and%20data.%20Recognizing%20the%20storage%20parameter%0Avolume%20issue%2C%20we%20propose%20a%20Straightforward%20layer-wise%20pruning%20method%2C%20called%0ASLS%2C%20for%20pruning%20PETL-transferred%20models.%20By%20evaluating%20parameters%20from%20a%0Afeature%20perspective%20of%20each%20layer%20and%20utilizing%20clustering%20metrics%20to%20assess%0Acurrent%20parameters%20based%20on%20clustering%20phenomena%20in%20low-dimensional%20space%0Aobtained%20through%20t-SNE%2C%20SLS%20facilitates%20informed%20pruning%20decisions.%20Our%20study%0Areveals%20that%20layer-wise%20pruning%2C%20with%20a%20focus%20on%20storing%20pruning%20indices%2C%0Aaddresses%20storage%20volume%20concerns.%20Notably%2C%20mainstream%20Layer-wise%20pruning%0Amethods%20may%20not%20be%20suitable%20for%20assessing%20layer%20importance%20in%20PETL-transferred%0Amodels%2C%20where%20the%20majority%20of%20parameters%20are%20pre-trained%20and%20have%20limited%0Arelevance%20to%20downstream%20datasets.%20Comparative%20analysis%20against%20state-of-the-art%0APETL%20methods%20demonstrates%20that%20the%20pruned%20model%20achieved%20a%20notable%20balance%0Abetween%20model%20throughput%20and%20accuracy.%20Moreover%2C%20SLS%20effectively%20reduces%0Astorage%20overhead%20arising%20from%20varying%20pruned%20structures%20while%20enhancing%20the%0Aaccuracy%20and%20speed%20of%20pruned%20models%20compared%20to%20conventional%20pruning%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14330v1&entry.124074799=Read"},
{"title": "PolyFormer: Scalable Node-wise Filters via Polynomial Graph Transformer", "author": "Jiahong Ma and Mingguo He and Zhewei Wei", "abstract": "  Spectral Graph Neural Networks have demonstrated superior performance in\ngraph representation learning. However, many current methods focus on employing\nshared polynomial coefficients for all nodes, i.e., learning node-unified\nfilters, which limits the filters' flexibility for node-level tasks. The recent\nDSF attempts to overcome this limitation by learning node-wise coefficients\nbased on positional encoding. However, the initialization and updating process\nof the positional encoding are burdensome, hindering scalability on large-scale\ngraphs. In this work, we propose a scalable node-wise filter, PolyAttn.\nLeveraging the attention mechanism, PolyAttn can directly learn node-wise\nfilters in an efficient manner, offering powerful representation capabilities.\nBuilding on PolyAttn, we introduce the whole model, named PolyFormer. In the\nlens of Graph Transformer models, PolyFormer, which calculates attention scores\nwithin nodes, shows great scalability. Moreover, the model captures spectral\ninformation, enhancing expressiveness while maintaining efficiency. With these\nadvantages, PolyFormer offers a desirable balance between scalability and\nexpressiveness for node-level tasks. Extensive experiments demonstrate that our\nproposed methods excel at learning arbitrary node-wise filters, showing\nsuperior performance on both homophilic and heterophilic graphs, and handling\ngraphs containing up to 100 million nodes. The code is available at\nhttps://github.com/air029/PolyFormer.\n", "link": "http://arxiv.org/abs/2407.14459v1", "date": "2024-07-19", "relevancy": 1.9246, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5503}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.477}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PolyFormer%3A%20Scalable%20Node-wise%20Filters%20via%20Polynomial%20Graph%20Transformer&body=Title%3A%20PolyFormer%3A%20Scalable%20Node-wise%20Filters%20via%20Polynomial%20Graph%20Transformer%0AAuthor%3A%20Jiahong%20Ma%20and%20Mingguo%20He%20and%20Zhewei%20Wei%0AAbstract%3A%20%20%20Spectral%20Graph%20Neural%20Networks%20have%20demonstrated%20superior%20performance%20in%0Agraph%20representation%20learning.%20However%2C%20many%20current%20methods%20focus%20on%20employing%0Ashared%20polynomial%20coefficients%20for%20all%20nodes%2C%20i.e.%2C%20learning%20node-unified%0Afilters%2C%20which%20limits%20the%20filters%27%20flexibility%20for%20node-level%20tasks.%20The%20recent%0ADSF%20attempts%20to%20overcome%20this%20limitation%20by%20learning%20node-wise%20coefficients%0Abased%20on%20positional%20encoding.%20However%2C%20the%20initialization%20and%20updating%20process%0Aof%20the%20positional%20encoding%20are%20burdensome%2C%20hindering%20scalability%20on%20large-scale%0Agraphs.%20In%20this%20work%2C%20we%20propose%20a%20scalable%20node-wise%20filter%2C%20PolyAttn.%0ALeveraging%20the%20attention%20mechanism%2C%20PolyAttn%20can%20directly%20learn%20node-wise%0Afilters%20in%20an%20efficient%20manner%2C%20offering%20powerful%20representation%20capabilities.%0ABuilding%20on%20PolyAttn%2C%20we%20introduce%20the%20whole%20model%2C%20named%20PolyFormer.%20In%20the%0Alens%20of%20Graph%20Transformer%20models%2C%20PolyFormer%2C%20which%20calculates%20attention%20scores%0Awithin%20nodes%2C%20shows%20great%20scalability.%20Moreover%2C%20the%20model%20captures%20spectral%0Ainformation%2C%20enhancing%20expressiveness%20while%20maintaining%20efficiency.%20With%20these%0Aadvantages%2C%20PolyFormer%20offers%20a%20desirable%20balance%20between%20scalability%20and%0Aexpressiveness%20for%20node-level%20tasks.%20Extensive%20experiments%20demonstrate%20that%20our%0Aproposed%20methods%20excel%20at%20learning%20arbitrary%20node-wise%20filters%2C%20showing%0Asuperior%20performance%20on%20both%20homophilic%20and%20heterophilic%20graphs%2C%20and%20handling%0Agraphs%20containing%20up%20to%20100%20million%20nodes.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/air029/PolyFormer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14459v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPolyFormer%253A%2520Scalable%2520Node-wise%2520Filters%2520via%2520Polynomial%2520Graph%2520Transformer%26entry.906535625%3DJiahong%2520Ma%2520and%2520Mingguo%2520He%2520and%2520Zhewei%2520Wei%26entry.1292438233%3D%2520%2520Spectral%2520Graph%2520Neural%2520Networks%2520have%2520demonstrated%2520superior%2520performance%2520in%250Agraph%2520representation%2520learning.%2520However%252C%2520many%2520current%2520methods%2520focus%2520on%2520employing%250Ashared%2520polynomial%2520coefficients%2520for%2520all%2520nodes%252C%2520i.e.%252C%2520learning%2520node-unified%250Afilters%252C%2520which%2520limits%2520the%2520filters%2527%2520flexibility%2520for%2520node-level%2520tasks.%2520The%2520recent%250ADSF%2520attempts%2520to%2520overcome%2520this%2520limitation%2520by%2520learning%2520node-wise%2520coefficients%250Abased%2520on%2520positional%2520encoding.%2520However%252C%2520the%2520initialization%2520and%2520updating%2520process%250Aof%2520the%2520positional%2520encoding%2520are%2520burdensome%252C%2520hindering%2520scalability%2520on%2520large-scale%250Agraphs.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520scalable%2520node-wise%2520filter%252C%2520PolyAttn.%250ALeveraging%2520the%2520attention%2520mechanism%252C%2520PolyAttn%2520can%2520directly%2520learn%2520node-wise%250Afilters%2520in%2520an%2520efficient%2520manner%252C%2520offering%2520powerful%2520representation%2520capabilities.%250ABuilding%2520on%2520PolyAttn%252C%2520we%2520introduce%2520the%2520whole%2520model%252C%2520named%2520PolyFormer.%2520In%2520the%250Alens%2520of%2520Graph%2520Transformer%2520models%252C%2520PolyFormer%252C%2520which%2520calculates%2520attention%2520scores%250Awithin%2520nodes%252C%2520shows%2520great%2520scalability.%2520Moreover%252C%2520the%2520model%2520captures%2520spectral%250Ainformation%252C%2520enhancing%2520expressiveness%2520while%2520maintaining%2520efficiency.%2520With%2520these%250Aadvantages%252C%2520PolyFormer%2520offers%2520a%2520desirable%2520balance%2520between%2520scalability%2520and%250Aexpressiveness%2520for%2520node-level%2520tasks.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250Aproposed%2520methods%2520excel%2520at%2520learning%2520arbitrary%2520node-wise%2520filters%252C%2520showing%250Asuperior%2520performance%2520on%2520both%2520homophilic%2520and%2520heterophilic%2520graphs%252C%2520and%2520handling%250Agraphs%2520containing%2520up%2520to%2520100%2520million%2520nodes.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/air029/PolyFormer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14459v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PolyFormer%3A%20Scalable%20Node-wise%20Filters%20via%20Polynomial%20Graph%20Transformer&entry.906535625=Jiahong%20Ma%20and%20Mingguo%20He%20and%20Zhewei%20Wei&entry.1292438233=%20%20Spectral%20Graph%20Neural%20Networks%20have%20demonstrated%20superior%20performance%20in%0Agraph%20representation%20learning.%20However%2C%20many%20current%20methods%20focus%20on%20employing%0Ashared%20polynomial%20coefficients%20for%20all%20nodes%2C%20i.e.%2C%20learning%20node-unified%0Afilters%2C%20which%20limits%20the%20filters%27%20flexibility%20for%20node-level%20tasks.%20The%20recent%0ADSF%20attempts%20to%20overcome%20this%20limitation%20by%20learning%20node-wise%20coefficients%0Abased%20on%20positional%20encoding.%20However%2C%20the%20initialization%20and%20updating%20process%0Aof%20the%20positional%20encoding%20are%20burdensome%2C%20hindering%20scalability%20on%20large-scale%0Agraphs.%20In%20this%20work%2C%20we%20propose%20a%20scalable%20node-wise%20filter%2C%20PolyAttn.%0ALeveraging%20the%20attention%20mechanism%2C%20PolyAttn%20can%20directly%20learn%20node-wise%0Afilters%20in%20an%20efficient%20manner%2C%20offering%20powerful%20representation%20capabilities.%0ABuilding%20on%20PolyAttn%2C%20we%20introduce%20the%20whole%20model%2C%20named%20PolyFormer.%20In%20the%0Alens%20of%20Graph%20Transformer%20models%2C%20PolyFormer%2C%20which%20calculates%20attention%20scores%0Awithin%20nodes%2C%20shows%20great%20scalability.%20Moreover%2C%20the%20model%20captures%20spectral%0Ainformation%2C%20enhancing%20expressiveness%20while%20maintaining%20efficiency.%20With%20these%0Aadvantages%2C%20PolyFormer%20offers%20a%20desirable%20balance%20between%20scalability%20and%0Aexpressiveness%20for%20node-level%20tasks.%20Extensive%20experiments%20demonstrate%20that%20our%0Aproposed%20methods%20excel%20at%20learning%20arbitrary%20node-wise%20filters%2C%20showing%0Asuperior%20performance%20on%20both%20homophilic%20and%20heterophilic%20graphs%2C%20and%20handling%0Agraphs%20containing%20up%20to%20100%20million%20nodes.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/air029/PolyFormer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14459v1&entry.124074799=Read"},
{"title": "Voices in a Crowd: Searching for Clusters of Unique Perspectives", "author": "Nikolas Vitsakis and Amit Parekh and Ioannis Konstas", "abstract": "  Language models have been shown to reproduce underlying biases existing in\ntheir training data, which is the majority perspective by default. Proposed\nsolutions aim to capture minority perspectives by either modelling annotator\ndisagreements or grouping annotators based on shared metadata, both of which\nface significant challenges. We propose a framework that trains models without\nencoding annotator metadata, extracts latent embeddings informed by annotator\nbehaviour, and creates clusters of similar opinions, that we refer to as\nvoices. Resulting clusters are validated post-hoc via internal and external\nquantitative metrics, as well a qualitative analysis to identify the type of\nvoice that each cluster represents. Our results demonstrate the strong\ngeneralisation capability of our framework, indicated by resulting clusters\nbeing adequately robust, while also capturing minority perspectives based on\ndifferent demographic factors throughout two distinct datasets.\n", "link": "http://arxiv.org/abs/2407.14259v1", "date": "2024-07-19", "relevancy": 1.9235, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4919}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4737}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Voices%20in%20a%20Crowd%3A%20Searching%20for%20Clusters%20of%20Unique%20Perspectives&body=Title%3A%20Voices%20in%20a%20Crowd%3A%20Searching%20for%20Clusters%20of%20Unique%20Perspectives%0AAuthor%3A%20Nikolas%20Vitsakis%20and%20Amit%20Parekh%20and%20Ioannis%20Konstas%0AAbstract%3A%20%20%20Language%20models%20have%20been%20shown%20to%20reproduce%20underlying%20biases%20existing%20in%0Atheir%20training%20data%2C%20which%20is%20the%20majority%20perspective%20by%20default.%20Proposed%0Asolutions%20aim%20to%20capture%20minority%20perspectives%20by%20either%20modelling%20annotator%0Adisagreements%20or%20grouping%20annotators%20based%20on%20shared%20metadata%2C%20both%20of%20which%0Aface%20significant%20challenges.%20We%20propose%20a%20framework%20that%20trains%20models%20without%0Aencoding%20annotator%20metadata%2C%20extracts%20latent%20embeddings%20informed%20by%20annotator%0Abehaviour%2C%20and%20creates%20clusters%20of%20similar%20opinions%2C%20that%20we%20refer%20to%20as%0Avoices.%20Resulting%20clusters%20are%20validated%20post-hoc%20via%20internal%20and%20external%0Aquantitative%20metrics%2C%20as%20well%20a%20qualitative%20analysis%20to%20identify%20the%20type%20of%0Avoice%20that%20each%20cluster%20represents.%20Our%20results%20demonstrate%20the%20strong%0Ageneralisation%20capability%20of%20our%20framework%2C%20indicated%20by%20resulting%20clusters%0Abeing%20adequately%20robust%2C%20while%20also%20capturing%20minority%20perspectives%20based%20on%0Adifferent%20demographic%20factors%20throughout%20two%20distinct%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14259v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVoices%2520in%2520a%2520Crowd%253A%2520Searching%2520for%2520Clusters%2520of%2520Unique%2520Perspectives%26entry.906535625%3DNikolas%2520Vitsakis%2520and%2520Amit%2520Parekh%2520and%2520Ioannis%2520Konstas%26entry.1292438233%3D%2520%2520Language%2520models%2520have%2520been%2520shown%2520to%2520reproduce%2520underlying%2520biases%2520existing%2520in%250Atheir%2520training%2520data%252C%2520which%2520is%2520the%2520majority%2520perspective%2520by%2520default.%2520Proposed%250Asolutions%2520aim%2520to%2520capture%2520minority%2520perspectives%2520by%2520either%2520modelling%2520annotator%250Adisagreements%2520or%2520grouping%2520annotators%2520based%2520on%2520shared%2520metadata%252C%2520both%2520of%2520which%250Aface%2520significant%2520challenges.%2520We%2520propose%2520a%2520framework%2520that%2520trains%2520models%2520without%250Aencoding%2520annotator%2520metadata%252C%2520extracts%2520latent%2520embeddings%2520informed%2520by%2520annotator%250Abehaviour%252C%2520and%2520creates%2520clusters%2520of%2520similar%2520opinions%252C%2520that%2520we%2520refer%2520to%2520as%250Avoices.%2520Resulting%2520clusters%2520are%2520validated%2520post-hoc%2520via%2520internal%2520and%2520external%250Aquantitative%2520metrics%252C%2520as%2520well%2520a%2520qualitative%2520analysis%2520to%2520identify%2520the%2520type%2520of%250Avoice%2520that%2520each%2520cluster%2520represents.%2520Our%2520results%2520demonstrate%2520the%2520strong%250Ageneralisation%2520capability%2520of%2520our%2520framework%252C%2520indicated%2520by%2520resulting%2520clusters%250Abeing%2520adequately%2520robust%252C%2520while%2520also%2520capturing%2520minority%2520perspectives%2520based%2520on%250Adifferent%2520demographic%2520factors%2520throughout%2520two%2520distinct%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14259v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Voices%20in%20a%20Crowd%3A%20Searching%20for%20Clusters%20of%20Unique%20Perspectives&entry.906535625=Nikolas%20Vitsakis%20and%20Amit%20Parekh%20and%20Ioannis%20Konstas&entry.1292438233=%20%20Language%20models%20have%20been%20shown%20to%20reproduce%20underlying%20biases%20existing%20in%0Atheir%20training%20data%2C%20which%20is%20the%20majority%20perspective%20by%20default.%20Proposed%0Asolutions%20aim%20to%20capture%20minority%20perspectives%20by%20either%20modelling%20annotator%0Adisagreements%20or%20grouping%20annotators%20based%20on%20shared%20metadata%2C%20both%20of%20which%0Aface%20significant%20challenges.%20We%20propose%20a%20framework%20that%20trains%20models%20without%0Aencoding%20annotator%20metadata%2C%20extracts%20latent%20embeddings%20informed%20by%20annotator%0Abehaviour%2C%20and%20creates%20clusters%20of%20similar%20opinions%2C%20that%20we%20refer%20to%20as%0Avoices.%20Resulting%20clusters%20are%20validated%20post-hoc%20via%20internal%20and%20external%0Aquantitative%20metrics%2C%20as%20well%20a%20qualitative%20analysis%20to%20identify%20the%20type%20of%0Avoice%20that%20each%20cluster%20represents.%20Our%20results%20demonstrate%20the%20strong%0Ageneralisation%20capability%20of%20our%20framework%2C%20indicated%20by%20resulting%20clusters%0Abeing%20adequately%20robust%2C%20while%20also%20capturing%20minority%20perspectives%20based%20on%0Adifferent%20demographic%20factors%20throughout%20two%20distinct%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14259v1&entry.124074799=Read"},
{"title": "Power Variable Projection for Initialization-Free Large-Scale Bundle\n  Adjustment", "author": "Simon Weber and Je Hyeong Hong and Daniel Cremers", "abstract": "  Most Bundle Adjustment (BA) solvers like the Levenberg-Marquardt algorithm\nrequire a good initialization. Instead, initialization-free BA remains a\nlargely uncharted territory. The under-explored Variable Projection algorithm\n(VarPro) exhibits a wide convergence basin even without initialization. Coupled\nwith object space error formulation, recent works have shown its ability to\nsolve small-scale initialization-free bundle adjustment problem. To make such\ninitialization-free BA approaches scalable, we introduce Power Variable\nProjection (PoVar), extending a recent inverse expansion method based on power\nseries. Importantly, we link the power series expansion to Riemannian manifold\noptimization. This projective framework is crucial to solve large-scale bundle\nadjustment problems without initialization. Using the real-world BAL dataset,\nwe experimentally demonstrate that our solver achieves state-of-the-art results\nin terms of speed and accuracy. To our knowledge, this work is the first to\naddress the scalability of BA without initialization opening new venues for\ninitialization-free structure-from-motion.\n", "link": "http://arxiv.org/abs/2405.05079v4", "date": "2024-07-19", "relevancy": 1.9174, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4915}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4727}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Power%20Variable%20Projection%20for%20Initialization-Free%20Large-Scale%20Bundle%0A%20%20Adjustment&body=Title%3A%20Power%20Variable%20Projection%20for%20Initialization-Free%20Large-Scale%20Bundle%0A%20%20Adjustment%0AAuthor%3A%20Simon%20Weber%20and%20Je%20Hyeong%20Hong%20and%20Daniel%20Cremers%0AAbstract%3A%20%20%20Most%20Bundle%20Adjustment%20%28BA%29%20solvers%20like%20the%20Levenberg-Marquardt%20algorithm%0Arequire%20a%20good%20initialization.%20Instead%2C%20initialization-free%20BA%20remains%20a%0Alargely%20uncharted%20territory.%20The%20under-explored%20Variable%20Projection%20algorithm%0A%28VarPro%29%20exhibits%20a%20wide%20convergence%20basin%20even%20without%20initialization.%20Coupled%0Awith%20object%20space%20error%20formulation%2C%20recent%20works%20have%20shown%20its%20ability%20to%0Asolve%20small-scale%20initialization-free%20bundle%20adjustment%20problem.%20To%20make%20such%0Ainitialization-free%20BA%20approaches%20scalable%2C%20we%20introduce%20Power%20Variable%0AProjection%20%28PoVar%29%2C%20extending%20a%20recent%20inverse%20expansion%20method%20based%20on%20power%0Aseries.%20Importantly%2C%20we%20link%20the%20power%20series%20expansion%20to%20Riemannian%20manifold%0Aoptimization.%20This%20projective%20framework%20is%20crucial%20to%20solve%20large-scale%20bundle%0Aadjustment%20problems%20without%20initialization.%20Using%20the%20real-world%20BAL%20dataset%2C%0Awe%20experimentally%20demonstrate%20that%20our%20solver%20achieves%20state-of-the-art%20results%0Ain%20terms%20of%20speed%20and%20accuracy.%20To%20our%20knowledge%2C%20this%20work%20is%20the%20first%20to%0Aaddress%20the%20scalability%20of%20BA%20without%20initialization%20opening%20new%20venues%20for%0Ainitialization-free%20structure-from-motion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05079v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPower%2520Variable%2520Projection%2520for%2520Initialization-Free%2520Large-Scale%2520Bundle%250A%2520%2520Adjustment%26entry.906535625%3DSimon%2520Weber%2520and%2520Je%2520Hyeong%2520Hong%2520and%2520Daniel%2520Cremers%26entry.1292438233%3D%2520%2520Most%2520Bundle%2520Adjustment%2520%2528BA%2529%2520solvers%2520like%2520the%2520Levenberg-Marquardt%2520algorithm%250Arequire%2520a%2520good%2520initialization.%2520Instead%252C%2520initialization-free%2520BA%2520remains%2520a%250Alargely%2520uncharted%2520territory.%2520The%2520under-explored%2520Variable%2520Projection%2520algorithm%250A%2528VarPro%2529%2520exhibits%2520a%2520wide%2520convergence%2520basin%2520even%2520without%2520initialization.%2520Coupled%250Awith%2520object%2520space%2520error%2520formulation%252C%2520recent%2520works%2520have%2520shown%2520its%2520ability%2520to%250Asolve%2520small-scale%2520initialization-free%2520bundle%2520adjustment%2520problem.%2520To%2520make%2520such%250Ainitialization-free%2520BA%2520approaches%2520scalable%252C%2520we%2520introduce%2520Power%2520Variable%250AProjection%2520%2528PoVar%2529%252C%2520extending%2520a%2520recent%2520inverse%2520expansion%2520method%2520based%2520on%2520power%250Aseries.%2520Importantly%252C%2520we%2520link%2520the%2520power%2520series%2520expansion%2520to%2520Riemannian%2520manifold%250Aoptimization.%2520This%2520projective%2520framework%2520is%2520crucial%2520to%2520solve%2520large-scale%2520bundle%250Aadjustment%2520problems%2520without%2520initialization.%2520Using%2520the%2520real-world%2520BAL%2520dataset%252C%250Awe%2520experimentally%2520demonstrate%2520that%2520our%2520solver%2520achieves%2520state-of-the-art%2520results%250Ain%2520terms%2520of%2520speed%2520and%2520accuracy.%2520To%2520our%2520knowledge%252C%2520this%2520work%2520is%2520the%2520first%2520to%250Aaddress%2520the%2520scalability%2520of%2520BA%2520without%2520initialization%2520opening%2520new%2520venues%2520for%250Ainitialization-free%2520structure-from-motion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05079v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Power%20Variable%20Projection%20for%20Initialization-Free%20Large-Scale%20Bundle%0A%20%20Adjustment&entry.906535625=Simon%20Weber%20and%20Je%20Hyeong%20Hong%20and%20Daniel%20Cremers&entry.1292438233=%20%20Most%20Bundle%20Adjustment%20%28BA%29%20solvers%20like%20the%20Levenberg-Marquardt%20algorithm%0Arequire%20a%20good%20initialization.%20Instead%2C%20initialization-free%20BA%20remains%20a%0Alargely%20uncharted%20territory.%20The%20under-explored%20Variable%20Projection%20algorithm%0A%28VarPro%29%20exhibits%20a%20wide%20convergence%20basin%20even%20without%20initialization.%20Coupled%0Awith%20object%20space%20error%20formulation%2C%20recent%20works%20have%20shown%20its%20ability%20to%0Asolve%20small-scale%20initialization-free%20bundle%20adjustment%20problem.%20To%20make%20such%0Ainitialization-free%20BA%20approaches%20scalable%2C%20we%20introduce%20Power%20Variable%0AProjection%20%28PoVar%29%2C%20extending%20a%20recent%20inverse%20expansion%20method%20based%20on%20power%0Aseries.%20Importantly%2C%20we%20link%20the%20power%20series%20expansion%20to%20Riemannian%20manifold%0Aoptimization.%20This%20projective%20framework%20is%20crucial%20to%20solve%20large-scale%20bundle%0Aadjustment%20problems%20without%20initialization.%20Using%20the%20real-world%20BAL%20dataset%2C%0Awe%20experimentally%20demonstrate%20that%20our%20solver%20achieves%20state-of-the-art%20results%0Ain%20terms%20of%20speed%20and%20accuracy.%20To%20our%20knowledge%2C%20this%20work%20is%20the%20first%20to%0Aaddress%20the%20scalability%20of%20BA%20without%20initialization%20opening%20new%20venues%20for%0Ainitialization-free%20structure-from-motion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05079v4&entry.124074799=Read"},
{"title": "The Future of Large Language Model Pre-training is Federated", "author": "Lorenzo Sani and Alex Iacob and Zeyu Cao and Bill Marino and Yan Gao and Tomas Paulik and Wanru Zhao and William F. Shen and Preslav Aleksandrov and Xinchi Qiu and Nicholas D. Lane", "abstract": "  Generative pre-trained large language models (LLMs) have demonstrated\nimpressive performance over a wide range of tasks, thanks to the unprecedented\namount of data they have been trained on. As established scaling laws indicate,\nLLMs' future performance improvement depends on the amount of computing and\ndata sources they can leverage for pre-training. Federated learning (FL) has\nthe potential to unleash the majority of the planet's data and computational\nresources, which are underutilized by the data-center-focused training\nmethodology of current LLM practice. Our work presents a robust, flexible,\nreproducible FL approach that enables large-scale collaboration across\ninstitutions to train LLMs. We propose a scalable deployment system called\nPhoton to enable the investigation and development of this new training\nparadigm for LLM pre-training. We show that Photon can be used by organizations\ninterested in collaborating with their private data sources and computational\nresources for pre-training LLMs with billions of parameters. This paradigm\nwould mobilize more computational and data resources while matching or\npotentially exceeding centralized performance. We further show the\neffectiveness of the federated training scales with model size and present our\napproach for training a billion-scale federated LLM using limited resources.\nFinally, we show that LLM training is highly resilient to the classical\nchallenges of federated statistical and hardware heterogeneity. Furthermore, we\nshow that convergence is robust to partial participation, opening the avenue\nfor compute-efficient collaborative training. Photon will help data-rich actors\nto become the protagonists of LLMs pre-training instead of leaving the stage to\ncompute-rich actors alone.\n", "link": "http://arxiv.org/abs/2405.10853v2", "date": "2024-07-19", "relevancy": 1.9144, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5034}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4883}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Future%20of%20Large%20Language%20Model%20Pre-training%20is%20Federated&body=Title%3A%20The%20Future%20of%20Large%20Language%20Model%20Pre-training%20is%20Federated%0AAuthor%3A%20Lorenzo%20Sani%20and%20Alex%20Iacob%20and%20Zeyu%20Cao%20and%20Bill%20Marino%20and%20Yan%20Gao%20and%20Tomas%20Paulik%20and%20Wanru%20Zhao%20and%20William%20F.%20Shen%20and%20Preslav%20Aleksandrov%20and%20Xinchi%20Qiu%20and%20Nicholas%20D.%20Lane%0AAbstract%3A%20%20%20Generative%20pre-trained%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%0Aimpressive%20performance%20over%20a%20wide%20range%20of%20tasks%2C%20thanks%20to%20the%20unprecedented%0Aamount%20of%20data%20they%20have%20been%20trained%20on.%20As%20established%20scaling%20laws%20indicate%2C%0ALLMs%27%20future%20performance%20improvement%20depends%20on%20the%20amount%20of%20computing%20and%0Adata%20sources%20they%20can%20leverage%20for%20pre-training.%20Federated%20learning%20%28FL%29%20has%0Athe%20potential%20to%20unleash%20the%20majority%20of%20the%20planet%27s%20data%20and%20computational%0Aresources%2C%20which%20are%20underutilized%20by%20the%20data-center-focused%20training%0Amethodology%20of%20current%20LLM%20practice.%20Our%20work%20presents%20a%20robust%2C%20flexible%2C%0Areproducible%20FL%20approach%20that%20enables%20large-scale%20collaboration%20across%0Ainstitutions%20to%20train%20LLMs.%20We%20propose%20a%20scalable%20deployment%20system%20called%0APhoton%20to%20enable%20the%20investigation%20and%20development%20of%20this%20new%20training%0Aparadigm%20for%20LLM%20pre-training.%20We%20show%20that%20Photon%20can%20be%20used%20by%20organizations%0Ainterested%20in%20collaborating%20with%20their%20private%20data%20sources%20and%20computational%0Aresources%20for%20pre-training%20LLMs%20with%20billions%20of%20parameters.%20This%20paradigm%0Awould%20mobilize%20more%20computational%20and%20data%20resources%20while%20matching%20or%0Apotentially%20exceeding%20centralized%20performance.%20We%20further%20show%20the%0Aeffectiveness%20of%20the%20federated%20training%20scales%20with%20model%20size%20and%20present%20our%0Aapproach%20for%20training%20a%20billion-scale%20federated%20LLM%20using%20limited%20resources.%0AFinally%2C%20we%20show%20that%20LLM%20training%20is%20highly%20resilient%20to%20the%20classical%0Achallenges%20of%20federated%20statistical%20and%20hardware%20heterogeneity.%20Furthermore%2C%20we%0Ashow%20that%20convergence%20is%20robust%20to%20partial%20participation%2C%20opening%20the%20avenue%0Afor%20compute-efficient%20collaborative%20training.%20Photon%20will%20help%20data-rich%20actors%0Ato%20become%20the%20protagonists%20of%20LLMs%20pre-training%20instead%20of%20leaving%20the%20stage%20to%0Acompute-rich%20actors%20alone.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10853v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Future%2520of%2520Large%2520Language%2520Model%2520Pre-training%2520is%2520Federated%26entry.906535625%3DLorenzo%2520Sani%2520and%2520Alex%2520Iacob%2520and%2520Zeyu%2520Cao%2520and%2520Bill%2520Marino%2520and%2520Yan%2520Gao%2520and%2520Tomas%2520Paulik%2520and%2520Wanru%2520Zhao%2520and%2520William%2520F.%2520Shen%2520and%2520Preslav%2520Aleksandrov%2520and%2520Xinchi%2520Qiu%2520and%2520Nicholas%2520D.%2520Lane%26entry.1292438233%3D%2520%2520Generative%2520pre-trained%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%250Aimpressive%2520performance%2520over%2520a%2520wide%2520range%2520of%2520tasks%252C%2520thanks%2520to%2520the%2520unprecedented%250Aamount%2520of%2520data%2520they%2520have%2520been%2520trained%2520on.%2520As%2520established%2520scaling%2520laws%2520indicate%252C%250ALLMs%2527%2520future%2520performance%2520improvement%2520depends%2520on%2520the%2520amount%2520of%2520computing%2520and%250Adata%2520sources%2520they%2520can%2520leverage%2520for%2520pre-training.%2520Federated%2520learning%2520%2528FL%2529%2520has%250Athe%2520potential%2520to%2520unleash%2520the%2520majority%2520of%2520the%2520planet%2527s%2520data%2520and%2520computational%250Aresources%252C%2520which%2520are%2520underutilized%2520by%2520the%2520data-center-focused%2520training%250Amethodology%2520of%2520current%2520LLM%2520practice.%2520Our%2520work%2520presents%2520a%2520robust%252C%2520flexible%252C%250Areproducible%2520FL%2520approach%2520that%2520enables%2520large-scale%2520collaboration%2520across%250Ainstitutions%2520to%2520train%2520LLMs.%2520We%2520propose%2520a%2520scalable%2520deployment%2520system%2520called%250APhoton%2520to%2520enable%2520the%2520investigation%2520and%2520development%2520of%2520this%2520new%2520training%250Aparadigm%2520for%2520LLM%2520pre-training.%2520We%2520show%2520that%2520Photon%2520can%2520be%2520used%2520by%2520organizations%250Ainterested%2520in%2520collaborating%2520with%2520their%2520private%2520data%2520sources%2520and%2520computational%250Aresources%2520for%2520pre-training%2520LLMs%2520with%2520billions%2520of%2520parameters.%2520This%2520paradigm%250Awould%2520mobilize%2520more%2520computational%2520and%2520data%2520resources%2520while%2520matching%2520or%250Apotentially%2520exceeding%2520centralized%2520performance.%2520We%2520further%2520show%2520the%250Aeffectiveness%2520of%2520the%2520federated%2520training%2520scales%2520with%2520model%2520size%2520and%2520present%2520our%250Aapproach%2520for%2520training%2520a%2520billion-scale%2520federated%2520LLM%2520using%2520limited%2520resources.%250AFinally%252C%2520we%2520show%2520that%2520LLM%2520training%2520is%2520highly%2520resilient%2520to%2520the%2520classical%250Achallenges%2520of%2520federated%2520statistical%2520and%2520hardware%2520heterogeneity.%2520Furthermore%252C%2520we%250Ashow%2520that%2520convergence%2520is%2520robust%2520to%2520partial%2520participation%252C%2520opening%2520the%2520avenue%250Afor%2520compute-efficient%2520collaborative%2520training.%2520Photon%2520will%2520help%2520data-rich%2520actors%250Ato%2520become%2520the%2520protagonists%2520of%2520LLMs%2520pre-training%2520instead%2520of%2520leaving%2520the%2520stage%2520to%250Acompute-rich%2520actors%2520alone.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10853v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Future%20of%20Large%20Language%20Model%20Pre-training%20is%20Federated&entry.906535625=Lorenzo%20Sani%20and%20Alex%20Iacob%20and%20Zeyu%20Cao%20and%20Bill%20Marino%20and%20Yan%20Gao%20and%20Tomas%20Paulik%20and%20Wanru%20Zhao%20and%20William%20F.%20Shen%20and%20Preslav%20Aleksandrov%20and%20Xinchi%20Qiu%20and%20Nicholas%20D.%20Lane&entry.1292438233=%20%20Generative%20pre-trained%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%0Aimpressive%20performance%20over%20a%20wide%20range%20of%20tasks%2C%20thanks%20to%20the%20unprecedented%0Aamount%20of%20data%20they%20have%20been%20trained%20on.%20As%20established%20scaling%20laws%20indicate%2C%0ALLMs%27%20future%20performance%20improvement%20depends%20on%20the%20amount%20of%20computing%20and%0Adata%20sources%20they%20can%20leverage%20for%20pre-training.%20Federated%20learning%20%28FL%29%20has%0Athe%20potential%20to%20unleash%20the%20majority%20of%20the%20planet%27s%20data%20and%20computational%0Aresources%2C%20which%20are%20underutilized%20by%20the%20data-center-focused%20training%0Amethodology%20of%20current%20LLM%20practice.%20Our%20work%20presents%20a%20robust%2C%20flexible%2C%0Areproducible%20FL%20approach%20that%20enables%20large-scale%20collaboration%20across%0Ainstitutions%20to%20train%20LLMs.%20We%20propose%20a%20scalable%20deployment%20system%20called%0APhoton%20to%20enable%20the%20investigation%20and%20development%20of%20this%20new%20training%0Aparadigm%20for%20LLM%20pre-training.%20We%20show%20that%20Photon%20can%20be%20used%20by%20organizations%0Ainterested%20in%20collaborating%20with%20their%20private%20data%20sources%20and%20computational%0Aresources%20for%20pre-training%20LLMs%20with%20billions%20of%20parameters.%20This%20paradigm%0Awould%20mobilize%20more%20computational%20and%20data%20resources%20while%20matching%20or%0Apotentially%20exceeding%20centralized%20performance.%20We%20further%20show%20the%0Aeffectiveness%20of%20the%20federated%20training%20scales%20with%20model%20size%20and%20present%20our%0Aapproach%20for%20training%20a%20billion-scale%20federated%20LLM%20using%20limited%20resources.%0AFinally%2C%20we%20show%20that%20LLM%20training%20is%20highly%20resilient%20to%20the%20classical%0Achallenges%20of%20federated%20statistical%20and%20hardware%20heterogeneity.%20Furthermore%2C%20we%0Ashow%20that%20convergence%20is%20robust%20to%20partial%20participation%2C%20opening%20the%20avenue%0Afor%20compute-efficient%20collaborative%20training.%20Photon%20will%20help%20data-rich%20actors%0Ato%20become%20the%20protagonists%20of%20LLMs%20pre-training%20instead%20of%20leaving%20the%20stage%20to%0Acompute-rich%20actors%20alone.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10853v2&entry.124074799=Read"},
{"title": "Stable Audio Open", "author": "Zach Evans and Julian D. Parker and CJ Carr and Zack Zukowski and Josiah Taylor and Jordi Pons", "abstract": "  Open generative models are vitally important for the community, allowing for\nfine-tunes and serving as baselines when presenting new models. However, most\ncurrent text-to-audio models are private and not accessible for artists and\nresearchers to build upon. Here we describe the architecture and training\nprocess of a new open-weights text-to-audio model trained with Creative Commons\ndata. Our evaluation shows that the model's performance is competitive with the\nstate-of-the-art across various metrics. Notably, the reported FDopenl3 results\n(measuring the realism of the generations) showcase its potential for\nhigh-quality stereo sound synthesis at 44.1kHz.\n", "link": "http://arxiv.org/abs/2407.14358v1", "date": "2024-07-19", "relevancy": 1.9075, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5006}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4876}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stable%20Audio%20Open&body=Title%3A%20Stable%20Audio%20Open%0AAuthor%3A%20Zach%20Evans%20and%20Julian%20D.%20Parker%20and%20CJ%20Carr%20and%20Zack%20Zukowski%20and%20Josiah%20Taylor%20and%20Jordi%20Pons%0AAbstract%3A%20%20%20Open%20generative%20models%20are%20vitally%20important%20for%20the%20community%2C%20allowing%20for%0Afine-tunes%20and%20serving%20as%20baselines%20when%20presenting%20new%20models.%20However%2C%20most%0Acurrent%20text-to-audio%20models%20are%20private%20and%20not%20accessible%20for%20artists%20and%0Aresearchers%20to%20build%20upon.%20Here%20we%20describe%20the%20architecture%20and%20training%0Aprocess%20of%20a%20new%20open-weights%20text-to-audio%20model%20trained%20with%20Creative%20Commons%0Adata.%20Our%20evaluation%20shows%20that%20the%20model%27s%20performance%20is%20competitive%20with%20the%0Astate-of-the-art%20across%20various%20metrics.%20Notably%2C%20the%20reported%20FDopenl3%20results%0A%28measuring%20the%20realism%20of%20the%20generations%29%20showcase%20its%20potential%20for%0Ahigh-quality%20stereo%20sound%20synthesis%20at%2044.1kHz.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14358v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStable%2520Audio%2520Open%26entry.906535625%3DZach%2520Evans%2520and%2520Julian%2520D.%2520Parker%2520and%2520CJ%2520Carr%2520and%2520Zack%2520Zukowski%2520and%2520Josiah%2520Taylor%2520and%2520Jordi%2520Pons%26entry.1292438233%3D%2520%2520Open%2520generative%2520models%2520are%2520vitally%2520important%2520for%2520the%2520community%252C%2520allowing%2520for%250Afine-tunes%2520and%2520serving%2520as%2520baselines%2520when%2520presenting%2520new%2520models.%2520However%252C%2520most%250Acurrent%2520text-to-audio%2520models%2520are%2520private%2520and%2520not%2520accessible%2520for%2520artists%2520and%250Aresearchers%2520to%2520build%2520upon.%2520Here%2520we%2520describe%2520the%2520architecture%2520and%2520training%250Aprocess%2520of%2520a%2520new%2520open-weights%2520text-to-audio%2520model%2520trained%2520with%2520Creative%2520Commons%250Adata.%2520Our%2520evaluation%2520shows%2520that%2520the%2520model%2527s%2520performance%2520is%2520competitive%2520with%2520the%250Astate-of-the-art%2520across%2520various%2520metrics.%2520Notably%252C%2520the%2520reported%2520FDopenl3%2520results%250A%2528measuring%2520the%2520realism%2520of%2520the%2520generations%2529%2520showcase%2520its%2520potential%2520for%250Ahigh-quality%2520stereo%2520sound%2520synthesis%2520at%252044.1kHz.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14358v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stable%20Audio%20Open&entry.906535625=Zach%20Evans%20and%20Julian%20D.%20Parker%20and%20CJ%20Carr%20and%20Zack%20Zukowski%20and%20Josiah%20Taylor%20and%20Jordi%20Pons&entry.1292438233=%20%20Open%20generative%20models%20are%20vitally%20important%20for%20the%20community%2C%20allowing%20for%0Afine-tunes%20and%20serving%20as%20baselines%20when%20presenting%20new%20models.%20However%2C%20most%0Acurrent%20text-to-audio%20models%20are%20private%20and%20not%20accessible%20for%20artists%20and%0Aresearchers%20to%20build%20upon.%20Here%20we%20describe%20the%20architecture%20and%20training%0Aprocess%20of%20a%20new%20open-weights%20text-to-audio%20model%20trained%20with%20Creative%20Commons%0Adata.%20Our%20evaluation%20shows%20that%20the%20model%27s%20performance%20is%20competitive%20with%20the%0Astate-of-the-art%20across%20various%20metrics.%20Notably%2C%20the%20reported%20FDopenl3%20results%0A%28measuring%20the%20realism%20of%20the%20generations%29%20showcase%20its%20potential%20for%0Ahigh-quality%20stereo%20sound%20synthesis%20at%2044.1kHz.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14358v1&entry.124074799=Read"},
{"title": "Robust agents learn causal world models", "author": "Jonathan Richens and Tom Everitt", "abstract": "  It has long been hypothesised that causal reasoning plays a fundamental role\nin robust and general intelligence. However, it is not known if agents must\nlearn causal models in order to generalise to new domains, or if other\ninductive biases are sufficient. We answer this question, showing that any\nagent capable of satisfying a regret bound under a large set of distributional\nshifts must have learned an approximate causal model of the data generating\nprocess, which converges to the true causal model for optimal agents. We\ndiscuss the implications of this result for several research areas including\ntransfer learning and causal inference.\n", "link": "http://arxiv.org/abs/2402.10877v7", "date": "2024-07-19", "relevancy": 1.9012, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4989}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4871}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20agents%20learn%20causal%20world%20models&body=Title%3A%20Robust%20agents%20learn%20causal%20world%20models%0AAuthor%3A%20Jonathan%20Richens%20and%20Tom%20Everitt%0AAbstract%3A%20%20%20It%20has%20long%20been%20hypothesised%20that%20causal%20reasoning%20plays%20a%20fundamental%20role%0Ain%20robust%20and%20general%20intelligence.%20However%2C%20it%20is%20not%20known%20if%20agents%20must%0Alearn%20causal%20models%20in%20order%20to%20generalise%20to%20new%20domains%2C%20or%20if%20other%0Ainductive%20biases%20are%20sufficient.%20We%20answer%20this%20question%2C%20showing%20that%20any%0Aagent%20capable%20of%20satisfying%20a%20regret%20bound%20under%20a%20large%20set%20of%20distributional%0Ashifts%20must%20have%20learned%20an%20approximate%20causal%20model%20of%20the%20data%20generating%0Aprocess%2C%20which%20converges%20to%20the%20true%20causal%20model%20for%20optimal%20agents.%20We%0Adiscuss%20the%20implications%20of%20this%20result%20for%20several%20research%20areas%20including%0Atransfer%20learning%20and%20causal%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10877v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520agents%2520learn%2520causal%2520world%2520models%26entry.906535625%3DJonathan%2520Richens%2520and%2520Tom%2520Everitt%26entry.1292438233%3D%2520%2520It%2520has%2520long%2520been%2520hypothesised%2520that%2520causal%2520reasoning%2520plays%2520a%2520fundamental%2520role%250Ain%2520robust%2520and%2520general%2520intelligence.%2520However%252C%2520it%2520is%2520not%2520known%2520if%2520agents%2520must%250Alearn%2520causal%2520models%2520in%2520order%2520to%2520generalise%2520to%2520new%2520domains%252C%2520or%2520if%2520other%250Ainductive%2520biases%2520are%2520sufficient.%2520We%2520answer%2520this%2520question%252C%2520showing%2520that%2520any%250Aagent%2520capable%2520of%2520satisfying%2520a%2520regret%2520bound%2520under%2520a%2520large%2520set%2520of%2520distributional%250Ashifts%2520must%2520have%2520learned%2520an%2520approximate%2520causal%2520model%2520of%2520the%2520data%2520generating%250Aprocess%252C%2520which%2520converges%2520to%2520the%2520true%2520causal%2520model%2520for%2520optimal%2520agents.%2520We%250Adiscuss%2520the%2520implications%2520of%2520this%2520result%2520for%2520several%2520research%2520areas%2520including%250Atransfer%2520learning%2520and%2520causal%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10877v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20agents%20learn%20causal%20world%20models&entry.906535625=Jonathan%20Richens%20and%20Tom%20Everitt&entry.1292438233=%20%20It%20has%20long%20been%20hypothesised%20that%20causal%20reasoning%20plays%20a%20fundamental%20role%0Ain%20robust%20and%20general%20intelligence.%20However%2C%20it%20is%20not%20known%20if%20agents%20must%0Alearn%20causal%20models%20in%20order%20to%20generalise%20to%20new%20domains%2C%20or%20if%20other%0Ainductive%20biases%20are%20sufficient.%20We%20answer%20this%20question%2C%20showing%20that%20any%0Aagent%20capable%20of%20satisfying%20a%20regret%20bound%20under%20a%20large%20set%20of%20distributional%0Ashifts%20must%20have%20learned%20an%20approximate%20causal%20model%20of%20the%20data%20generating%0Aprocess%2C%20which%20converges%20to%20the%20true%20causal%20model%20for%20optimal%20agents.%20We%0Adiscuss%20the%20implications%20of%20this%20result%20for%20several%20research%20areas%20including%0Atransfer%20learning%20and%20causal%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10877v7&entry.124074799=Read"},
{"title": "Superposition Prompting: Improving and Accelerating Retrieval-Augmented\n  Generation", "author": "Thomas Merth and Qichen Fu and Mohammad Rastegari and Mahyar Najibi", "abstract": "  Despite the successes of large language models (LLMs), they exhibit\nsignificant drawbacks, particularly when processing long contexts. Their\ninference cost scales quadratically with respect to sequence length, making it\nexpensive for deployment in some real-world text processing applications, such\nas retrieval-augmented generation (RAG). Additionally, LLMs also exhibit the\n\"distraction phenomenon\", where irrelevant context in the prompt degrades\noutput quality. To address these drawbacks, we propose a novel RAG prompting\nmethodology, *superposition prompting*, which can be directly applied to\npre-trained transformer-based LLMs *without the need for fine-tuning*. At a\nhigh level, superposition prompting allows the LLM to process input documents\nin parallel *prompt paths*, discarding paths once they are deemed irrelevant.\nWe demonstrate the capability of our method to simultaneously enhance time\nefficiency across a variety of question-answering benchmarks using multiple\npre-trained LLMs. Furthermore, our technique significantly improves accuracy\nwhen the retrieved context is large relative the context the model was trained\non. For example, our approach facilitates a 93x reduction in compute time while\n*improving* accuracy by 43% on the NaturalQuestions-Open dataset with the\nMPT-7B instruction-tuned model over naive RAG.\n", "link": "http://arxiv.org/abs/2404.06910v2", "date": "2024-07-19", "relevancy": 1.8879, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4829}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4781}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Superposition%20Prompting%3A%20Improving%20and%20Accelerating%20Retrieval-Augmented%0A%20%20Generation&body=Title%3A%20Superposition%20Prompting%3A%20Improving%20and%20Accelerating%20Retrieval-Augmented%0A%20%20Generation%0AAuthor%3A%20Thomas%20Merth%20and%20Qichen%20Fu%20and%20Mohammad%20Rastegari%20and%20Mahyar%20Najibi%0AAbstract%3A%20%20%20Despite%20the%20successes%20of%20large%20language%20models%20%28LLMs%29%2C%20they%20exhibit%0Asignificant%20drawbacks%2C%20particularly%20when%20processing%20long%20contexts.%20Their%0Ainference%20cost%20scales%20quadratically%20with%20respect%20to%20sequence%20length%2C%20making%20it%0Aexpensive%20for%20deployment%20in%20some%20real-world%20text%20processing%20applications%2C%20such%0Aas%20retrieval-augmented%20generation%20%28RAG%29.%20Additionally%2C%20LLMs%20also%20exhibit%20the%0A%22distraction%20phenomenon%22%2C%20where%20irrelevant%20context%20in%20the%20prompt%20degrades%0Aoutput%20quality.%20To%20address%20these%20drawbacks%2C%20we%20propose%20a%20novel%20RAG%20prompting%0Amethodology%2C%20%2Asuperposition%20prompting%2A%2C%20which%20can%20be%20directly%20applied%20to%0Apre-trained%20transformer-based%20LLMs%20%2Awithout%20the%20need%20for%20fine-tuning%2A.%20At%20a%0Ahigh%20level%2C%20superposition%20prompting%20allows%20the%20LLM%20to%20process%20input%20documents%0Ain%20parallel%20%2Aprompt%20paths%2A%2C%20discarding%20paths%20once%20they%20are%20deemed%20irrelevant.%0AWe%20demonstrate%20the%20capability%20of%20our%20method%20to%20simultaneously%20enhance%20time%0Aefficiency%20across%20a%20variety%20of%20question-answering%20benchmarks%20using%20multiple%0Apre-trained%20LLMs.%20Furthermore%2C%20our%20technique%20significantly%20improves%20accuracy%0Awhen%20the%20retrieved%20context%20is%20large%20relative%20the%20context%20the%20model%20was%20trained%0Aon.%20For%20example%2C%20our%20approach%20facilitates%20a%2093x%20reduction%20in%20compute%20time%20while%0A%2Aimproving%2A%20accuracy%20by%2043%25%20on%20the%20NaturalQuestions-Open%20dataset%20with%20the%0AMPT-7B%20instruction-tuned%20model%20over%20naive%20RAG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06910v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuperposition%2520Prompting%253A%2520Improving%2520and%2520Accelerating%2520Retrieval-Augmented%250A%2520%2520Generation%26entry.906535625%3DThomas%2520Merth%2520and%2520Qichen%2520Fu%2520and%2520Mohammad%2520Rastegari%2520and%2520Mahyar%2520Najibi%26entry.1292438233%3D%2520%2520Despite%2520the%2520successes%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520they%2520exhibit%250Asignificant%2520drawbacks%252C%2520particularly%2520when%2520processing%2520long%2520contexts.%2520Their%250Ainference%2520cost%2520scales%2520quadratically%2520with%2520respect%2520to%2520sequence%2520length%252C%2520making%2520it%250Aexpensive%2520for%2520deployment%2520in%2520some%2520real-world%2520text%2520processing%2520applications%252C%2520such%250Aas%2520retrieval-augmented%2520generation%2520%2528RAG%2529.%2520Additionally%252C%2520LLMs%2520also%2520exhibit%2520the%250A%2522distraction%2520phenomenon%2522%252C%2520where%2520irrelevant%2520context%2520in%2520the%2520prompt%2520degrades%250Aoutput%2520quality.%2520To%2520address%2520these%2520drawbacks%252C%2520we%2520propose%2520a%2520novel%2520RAG%2520prompting%250Amethodology%252C%2520%252Asuperposition%2520prompting%252A%252C%2520which%2520can%2520be%2520directly%2520applied%2520to%250Apre-trained%2520transformer-based%2520LLMs%2520%252Awithout%2520the%2520need%2520for%2520fine-tuning%252A.%2520At%2520a%250Ahigh%2520level%252C%2520superposition%2520prompting%2520allows%2520the%2520LLM%2520to%2520process%2520input%2520documents%250Ain%2520parallel%2520%252Aprompt%2520paths%252A%252C%2520discarding%2520paths%2520once%2520they%2520are%2520deemed%2520irrelevant.%250AWe%2520demonstrate%2520the%2520capability%2520of%2520our%2520method%2520to%2520simultaneously%2520enhance%2520time%250Aefficiency%2520across%2520a%2520variety%2520of%2520question-answering%2520benchmarks%2520using%2520multiple%250Apre-trained%2520LLMs.%2520Furthermore%252C%2520our%2520technique%2520significantly%2520improves%2520accuracy%250Awhen%2520the%2520retrieved%2520context%2520is%2520large%2520relative%2520the%2520context%2520the%2520model%2520was%2520trained%250Aon.%2520For%2520example%252C%2520our%2520approach%2520facilitates%2520a%252093x%2520reduction%2520in%2520compute%2520time%2520while%250A%252Aimproving%252A%2520accuracy%2520by%252043%2525%2520on%2520the%2520NaturalQuestions-Open%2520dataset%2520with%2520the%250AMPT-7B%2520instruction-tuned%2520model%2520over%2520naive%2520RAG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.06910v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Superposition%20Prompting%3A%20Improving%20and%20Accelerating%20Retrieval-Augmented%0A%20%20Generation&entry.906535625=Thomas%20Merth%20and%20Qichen%20Fu%20and%20Mohammad%20Rastegari%20and%20Mahyar%20Najibi&entry.1292438233=%20%20Despite%20the%20successes%20of%20large%20language%20models%20%28LLMs%29%2C%20they%20exhibit%0Asignificant%20drawbacks%2C%20particularly%20when%20processing%20long%20contexts.%20Their%0Ainference%20cost%20scales%20quadratically%20with%20respect%20to%20sequence%20length%2C%20making%20it%0Aexpensive%20for%20deployment%20in%20some%20real-world%20text%20processing%20applications%2C%20such%0Aas%20retrieval-augmented%20generation%20%28RAG%29.%20Additionally%2C%20LLMs%20also%20exhibit%20the%0A%22distraction%20phenomenon%22%2C%20where%20irrelevant%20context%20in%20the%20prompt%20degrades%0Aoutput%20quality.%20To%20address%20these%20drawbacks%2C%20we%20propose%20a%20novel%20RAG%20prompting%0Amethodology%2C%20%2Asuperposition%20prompting%2A%2C%20which%20can%20be%20directly%20applied%20to%0Apre-trained%20transformer-based%20LLMs%20%2Awithout%20the%20need%20for%20fine-tuning%2A.%20At%20a%0Ahigh%20level%2C%20superposition%20prompting%20allows%20the%20LLM%20to%20process%20input%20documents%0Ain%20parallel%20%2Aprompt%20paths%2A%2C%20discarding%20paths%20once%20they%20are%20deemed%20irrelevant.%0AWe%20demonstrate%20the%20capability%20of%20our%20method%20to%20simultaneously%20enhance%20time%0Aefficiency%20across%20a%20variety%20of%20question-answering%20benchmarks%20using%20multiple%0Apre-trained%20LLMs.%20Furthermore%2C%20our%20technique%20significantly%20improves%20accuracy%0Awhen%20the%20retrieved%20context%20is%20large%20relative%20the%20context%20the%20model%20was%20trained%0Aon.%20For%20example%2C%20our%20approach%20facilitates%20a%2093x%20reduction%20in%20compute%20time%20while%0A%2Aimproving%2A%20accuracy%20by%2043%25%20on%20the%20NaturalQuestions-Open%20dataset%20with%20the%0AMPT-7B%20instruction-tuned%20model%20over%20naive%20RAG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06910v2&entry.124074799=Read"},
{"title": "Safe-SD: Safe and Traceable Stable Diffusion with Text Prompt Trigger\n  for Invisible Generative Watermarking", "author": "Zhiyuan Ma and Guoli Jia and Biqing Qi and Bowen Zhou", "abstract": "  Recently, stable diffusion (SD) models have typically flourished in the field\nof image synthesis and personalized editing, with a range of photorealistic and\nunprecedented images being successfully generated. As a result, widespread\ninterest has been ignited to develop and use various SD-based tools for visual\ncontent creation. However, the exposure of AI-created content on public\nplatforms could raise both legal and ethical risks. In this regard, the\ntraditional methods of adding watermarks to the already generated images (i.e.\npost-processing) may face a dilemma (e.g., being erased or modified) in terms\nof copyright protection and content monitoring, since the powerful image\ninversion and text-to-image editing techniques have been widely explored in\nSD-based methods. In this work, we propose a Safe and high-traceable Stable\nDiffusion framework (namely Safe-SD) to adaptively implant the graphical\nwatermarks (e.g., QR code) into the imperceptible structure-related pixels\nduring the generative diffusion process for supporting text-driven invisible\nwatermarking and detection. Different from the previous high-cost\ninjection-then-detection training framework, we design a simple and unified\narchitecture, which makes it possible to simultaneously train watermark\ninjection and detection in a single network, greatly improving the efficiency\nand convenience of use. Moreover, to further support text-driven generative\nwatermarking and deeply explore its robustness and high-traceability, we\nelaborately design lambda sampling and encryption algorithm to fine-tune a\nlatent diffuser wrapped by a VAE for balancing high-fidelity image synthesis\nand high-traceable watermark detection. We present our quantitative and\nqualitative results on two representative datasets LSUN, COCO and FFHQ,\ndemonstrating state-of-the-art performance of Safe-SD and showing it\nsignificantly outperforms the previous approaches.\n", "link": "http://arxiv.org/abs/2407.13188v2", "date": "2024-07-19", "relevancy": 1.8871, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6362}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.634}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe-SD%3A%20Safe%20and%20Traceable%20Stable%20Diffusion%20with%20Text%20Prompt%20Trigger%0A%20%20for%20Invisible%20Generative%20Watermarking&body=Title%3A%20Safe-SD%3A%20Safe%20and%20Traceable%20Stable%20Diffusion%20with%20Text%20Prompt%20Trigger%0A%20%20for%20Invisible%20Generative%20Watermarking%0AAuthor%3A%20Zhiyuan%20Ma%20and%20Guoli%20Jia%20and%20Biqing%20Qi%20and%20Bowen%20Zhou%0AAbstract%3A%20%20%20Recently%2C%20stable%20diffusion%20%28SD%29%20models%20have%20typically%20flourished%20in%20the%20field%0Aof%20image%20synthesis%20and%20personalized%20editing%2C%20with%20a%20range%20of%20photorealistic%20and%0Aunprecedented%20images%20being%20successfully%20generated.%20As%20a%20result%2C%20widespread%0Ainterest%20has%20been%20ignited%20to%20develop%20and%20use%20various%20SD-based%20tools%20for%20visual%0Acontent%20creation.%20However%2C%20the%20exposure%20of%20AI-created%20content%20on%20public%0Aplatforms%20could%20raise%20both%20legal%20and%20ethical%20risks.%20In%20this%20regard%2C%20the%0Atraditional%20methods%20of%20adding%20watermarks%20to%20the%20already%20generated%20images%20%28i.e.%0Apost-processing%29%20may%20face%20a%20dilemma%20%28e.g.%2C%20being%20erased%20or%20modified%29%20in%20terms%0Aof%20copyright%20protection%20and%20content%20monitoring%2C%20since%20the%20powerful%20image%0Ainversion%20and%20text-to-image%20editing%20techniques%20have%20been%20widely%20explored%20in%0ASD-based%20methods.%20In%20this%20work%2C%20we%20propose%20a%20Safe%20and%20high-traceable%20Stable%0ADiffusion%20framework%20%28namely%20Safe-SD%29%20to%20adaptively%20implant%20the%20graphical%0Awatermarks%20%28e.g.%2C%20QR%20code%29%20into%20the%20imperceptible%20structure-related%20pixels%0Aduring%20the%20generative%20diffusion%20process%20for%20supporting%20text-driven%20invisible%0Awatermarking%20and%20detection.%20Different%20from%20the%20previous%20high-cost%0Ainjection-then-detection%20training%20framework%2C%20we%20design%20a%20simple%20and%20unified%0Aarchitecture%2C%20which%20makes%20it%20possible%20to%20simultaneously%20train%20watermark%0Ainjection%20and%20detection%20in%20a%20single%20network%2C%20greatly%20improving%20the%20efficiency%0Aand%20convenience%20of%20use.%20Moreover%2C%20to%20further%20support%20text-driven%20generative%0Awatermarking%20and%20deeply%20explore%20its%20robustness%20and%20high-traceability%2C%20we%0Aelaborately%20design%20lambda%20sampling%20and%20encryption%20algorithm%20to%20fine-tune%20a%0Alatent%20diffuser%20wrapped%20by%20a%20VAE%20for%20balancing%20high-fidelity%20image%20synthesis%0Aand%20high-traceable%20watermark%20detection.%20We%20present%20our%20quantitative%20and%0Aqualitative%20results%20on%20two%20representative%20datasets%20LSUN%2C%20COCO%20and%20FFHQ%2C%0Ademonstrating%20state-of-the-art%20performance%20of%20Safe-SD%20and%20showing%20it%0Asignificantly%20outperforms%20the%20previous%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13188v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe-SD%253A%2520Safe%2520and%2520Traceable%2520Stable%2520Diffusion%2520with%2520Text%2520Prompt%2520Trigger%250A%2520%2520for%2520Invisible%2520Generative%2520Watermarking%26entry.906535625%3DZhiyuan%2520Ma%2520and%2520Guoli%2520Jia%2520and%2520Biqing%2520Qi%2520and%2520Bowen%2520Zhou%26entry.1292438233%3D%2520%2520Recently%252C%2520stable%2520diffusion%2520%2528SD%2529%2520models%2520have%2520typically%2520flourished%2520in%2520the%2520field%250Aof%2520image%2520synthesis%2520and%2520personalized%2520editing%252C%2520with%2520a%2520range%2520of%2520photorealistic%2520and%250Aunprecedented%2520images%2520being%2520successfully%2520generated.%2520As%2520a%2520result%252C%2520widespread%250Ainterest%2520has%2520been%2520ignited%2520to%2520develop%2520and%2520use%2520various%2520SD-based%2520tools%2520for%2520visual%250Acontent%2520creation.%2520However%252C%2520the%2520exposure%2520of%2520AI-created%2520content%2520on%2520public%250Aplatforms%2520could%2520raise%2520both%2520legal%2520and%2520ethical%2520risks.%2520In%2520this%2520regard%252C%2520the%250Atraditional%2520methods%2520of%2520adding%2520watermarks%2520to%2520the%2520already%2520generated%2520images%2520%2528i.e.%250Apost-processing%2529%2520may%2520face%2520a%2520dilemma%2520%2528e.g.%252C%2520being%2520erased%2520or%2520modified%2529%2520in%2520terms%250Aof%2520copyright%2520protection%2520and%2520content%2520monitoring%252C%2520since%2520the%2520powerful%2520image%250Ainversion%2520and%2520text-to-image%2520editing%2520techniques%2520have%2520been%2520widely%2520explored%2520in%250ASD-based%2520methods.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520Safe%2520and%2520high-traceable%2520Stable%250ADiffusion%2520framework%2520%2528namely%2520Safe-SD%2529%2520to%2520adaptively%2520implant%2520the%2520graphical%250Awatermarks%2520%2528e.g.%252C%2520QR%2520code%2529%2520into%2520the%2520imperceptible%2520structure-related%2520pixels%250Aduring%2520the%2520generative%2520diffusion%2520process%2520for%2520supporting%2520text-driven%2520invisible%250Awatermarking%2520and%2520detection.%2520Different%2520from%2520the%2520previous%2520high-cost%250Ainjection-then-detection%2520training%2520framework%252C%2520we%2520design%2520a%2520simple%2520and%2520unified%250Aarchitecture%252C%2520which%2520makes%2520it%2520possible%2520to%2520simultaneously%2520train%2520watermark%250Ainjection%2520and%2520detection%2520in%2520a%2520single%2520network%252C%2520greatly%2520improving%2520the%2520efficiency%250Aand%2520convenience%2520of%2520use.%2520Moreover%252C%2520to%2520further%2520support%2520text-driven%2520generative%250Awatermarking%2520and%2520deeply%2520explore%2520its%2520robustness%2520and%2520high-traceability%252C%2520we%250Aelaborately%2520design%2520lambda%2520sampling%2520and%2520encryption%2520algorithm%2520to%2520fine-tune%2520a%250Alatent%2520diffuser%2520wrapped%2520by%2520a%2520VAE%2520for%2520balancing%2520high-fidelity%2520image%2520synthesis%250Aand%2520high-traceable%2520watermark%2520detection.%2520We%2520present%2520our%2520quantitative%2520and%250Aqualitative%2520results%2520on%2520two%2520representative%2520datasets%2520LSUN%252C%2520COCO%2520and%2520FFHQ%252C%250Ademonstrating%2520state-of-the-art%2520performance%2520of%2520Safe-SD%2520and%2520showing%2520it%250Asignificantly%2520outperforms%2520the%2520previous%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13188v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe-SD%3A%20Safe%20and%20Traceable%20Stable%20Diffusion%20with%20Text%20Prompt%20Trigger%0A%20%20for%20Invisible%20Generative%20Watermarking&entry.906535625=Zhiyuan%20Ma%20and%20Guoli%20Jia%20and%20Biqing%20Qi%20and%20Bowen%20Zhou&entry.1292438233=%20%20Recently%2C%20stable%20diffusion%20%28SD%29%20models%20have%20typically%20flourished%20in%20the%20field%0Aof%20image%20synthesis%20and%20personalized%20editing%2C%20with%20a%20range%20of%20photorealistic%20and%0Aunprecedented%20images%20being%20successfully%20generated.%20As%20a%20result%2C%20widespread%0Ainterest%20has%20been%20ignited%20to%20develop%20and%20use%20various%20SD-based%20tools%20for%20visual%0Acontent%20creation.%20However%2C%20the%20exposure%20of%20AI-created%20content%20on%20public%0Aplatforms%20could%20raise%20both%20legal%20and%20ethical%20risks.%20In%20this%20regard%2C%20the%0Atraditional%20methods%20of%20adding%20watermarks%20to%20the%20already%20generated%20images%20%28i.e.%0Apost-processing%29%20may%20face%20a%20dilemma%20%28e.g.%2C%20being%20erased%20or%20modified%29%20in%20terms%0Aof%20copyright%20protection%20and%20content%20monitoring%2C%20since%20the%20powerful%20image%0Ainversion%20and%20text-to-image%20editing%20techniques%20have%20been%20widely%20explored%20in%0ASD-based%20methods.%20In%20this%20work%2C%20we%20propose%20a%20Safe%20and%20high-traceable%20Stable%0ADiffusion%20framework%20%28namely%20Safe-SD%29%20to%20adaptively%20implant%20the%20graphical%0Awatermarks%20%28e.g.%2C%20QR%20code%29%20into%20the%20imperceptible%20structure-related%20pixels%0Aduring%20the%20generative%20diffusion%20process%20for%20supporting%20text-driven%20invisible%0Awatermarking%20and%20detection.%20Different%20from%20the%20previous%20high-cost%0Ainjection-then-detection%20training%20framework%2C%20we%20design%20a%20simple%20and%20unified%0Aarchitecture%2C%20which%20makes%20it%20possible%20to%20simultaneously%20train%20watermark%0Ainjection%20and%20detection%20in%20a%20single%20network%2C%20greatly%20improving%20the%20efficiency%0Aand%20convenience%20of%20use.%20Moreover%2C%20to%20further%20support%20text-driven%20generative%0Awatermarking%20and%20deeply%20explore%20its%20robustness%20and%20high-traceability%2C%20we%0Aelaborately%20design%20lambda%20sampling%20and%20encryption%20algorithm%20to%20fine-tune%20a%0Alatent%20diffuser%20wrapped%20by%20a%20VAE%20for%20balancing%20high-fidelity%20image%20synthesis%0Aand%20high-traceable%20watermark%20detection.%20We%20present%20our%20quantitative%20and%0Aqualitative%20results%20on%20two%20representative%20datasets%20LSUN%2C%20COCO%20and%20FFHQ%2C%0Ademonstrating%20state-of-the-art%20performance%20of%20Safe-SD%20and%20showing%20it%0Asignificantly%20outperforms%20the%20previous%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13188v2&entry.124074799=Read"},
{"title": "DiffIR2VR-Zero: Zero-Shot Video Restoration with Diffusion-based Image\n  Restoration Models", "author": "Chang-Han Yeh and Chin-Yang Lin and Zhixiang Wang and Chi-Wei Hsiao and Ting-Hsuan Chen and Yu-Lun Liu", "abstract": "  This paper introduces a method for zero-shot video restoration using\npre-trained image restoration diffusion models. Traditional video restoration\nmethods often need retraining for different settings and struggle with limited\ngeneralization across various degradation types and datasets. Our approach uses\na hierarchical token merging strategy for keyframes and local frames, combined\nwith a hybrid correspondence mechanism that blends optical flow and\nfeature-based nearest neighbor matching (latent merging). We show that our\nmethod not only achieves top performance in zero-shot video restoration but\nalso significantly surpasses trained models in generalization across diverse\ndatasets and extreme degradations (8$\\times$ super-resolution and high-standard\ndeviation video denoising). We present evidence through quantitative metrics\nand visual comparisons on various challenging datasets. Additionally, our\ntechnique works with any 2D restoration diffusion model, offering a versatile\nand powerful tool for video enhancement tasks without extensive retraining.\nThis research leads to more efficient and widely applicable video restoration\ntechnologies, supporting advancements in fields that require high-quality video\noutput. See our project page for video results at\nhttps://jimmycv07.github.io/DiffIR2VR_web/.\n", "link": "http://arxiv.org/abs/2407.01519v2", "date": "2024-07-19", "relevancy": 1.8832, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6489}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6069}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffIR2VR-Zero%3A%20Zero-Shot%20Video%20Restoration%20with%20Diffusion-based%20Image%0A%20%20Restoration%20Models&body=Title%3A%20DiffIR2VR-Zero%3A%20Zero-Shot%20Video%20Restoration%20with%20Diffusion-based%20Image%0A%20%20Restoration%20Models%0AAuthor%3A%20Chang-Han%20Yeh%20and%20Chin-Yang%20Lin%20and%20Zhixiang%20Wang%20and%20Chi-Wei%20Hsiao%20and%20Ting-Hsuan%20Chen%20and%20Yu-Lun%20Liu%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20method%20for%20zero-shot%20video%20restoration%20using%0Apre-trained%20image%20restoration%20diffusion%20models.%20Traditional%20video%20restoration%0Amethods%20often%20need%20retraining%20for%20different%20settings%20and%20struggle%20with%20limited%0Ageneralization%20across%20various%20degradation%20types%20and%20datasets.%20Our%20approach%20uses%0Aa%20hierarchical%20token%20merging%20strategy%20for%20keyframes%20and%20local%20frames%2C%20combined%0Awith%20a%20hybrid%20correspondence%20mechanism%20that%20blends%20optical%20flow%20and%0Afeature-based%20nearest%20neighbor%20matching%20%28latent%20merging%29.%20We%20show%20that%20our%0Amethod%20not%20only%20achieves%20top%20performance%20in%20zero-shot%20video%20restoration%20but%0Aalso%20significantly%20surpasses%20trained%20models%20in%20generalization%20across%20diverse%0Adatasets%20and%20extreme%20degradations%20%288%24%5Ctimes%24%20super-resolution%20and%20high-standard%0Adeviation%20video%20denoising%29.%20We%20present%20evidence%20through%20quantitative%20metrics%0Aand%20visual%20comparisons%20on%20various%20challenging%20datasets.%20Additionally%2C%20our%0Atechnique%20works%20with%20any%202D%20restoration%20diffusion%20model%2C%20offering%20a%20versatile%0Aand%20powerful%20tool%20for%20video%20enhancement%20tasks%20without%20extensive%20retraining.%0AThis%20research%20leads%20to%20more%20efficient%20and%20widely%20applicable%20video%20restoration%0Atechnologies%2C%20supporting%20advancements%20in%20fields%20that%20require%20high-quality%20video%0Aoutput.%20See%20our%20project%20page%20for%20video%20results%20at%0Ahttps%3A//jimmycv07.github.io/DiffIR2VR_web/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01519v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffIR2VR-Zero%253A%2520Zero-Shot%2520Video%2520Restoration%2520with%2520Diffusion-based%2520Image%250A%2520%2520Restoration%2520Models%26entry.906535625%3DChang-Han%2520Yeh%2520and%2520Chin-Yang%2520Lin%2520and%2520Zhixiang%2520Wang%2520and%2520Chi-Wei%2520Hsiao%2520and%2520Ting-Hsuan%2520Chen%2520and%2520Yu-Lun%2520Liu%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520method%2520for%2520zero-shot%2520video%2520restoration%2520using%250Apre-trained%2520image%2520restoration%2520diffusion%2520models.%2520Traditional%2520video%2520restoration%250Amethods%2520often%2520need%2520retraining%2520for%2520different%2520settings%2520and%2520struggle%2520with%2520limited%250Ageneralization%2520across%2520various%2520degradation%2520types%2520and%2520datasets.%2520Our%2520approach%2520uses%250Aa%2520hierarchical%2520token%2520merging%2520strategy%2520for%2520keyframes%2520and%2520local%2520frames%252C%2520combined%250Awith%2520a%2520hybrid%2520correspondence%2520mechanism%2520that%2520blends%2520optical%2520flow%2520and%250Afeature-based%2520nearest%2520neighbor%2520matching%2520%2528latent%2520merging%2529.%2520We%2520show%2520that%2520our%250Amethod%2520not%2520only%2520achieves%2520top%2520performance%2520in%2520zero-shot%2520video%2520restoration%2520but%250Aalso%2520significantly%2520surpasses%2520trained%2520models%2520in%2520generalization%2520across%2520diverse%250Adatasets%2520and%2520extreme%2520degradations%2520%25288%2524%255Ctimes%2524%2520super-resolution%2520and%2520high-standard%250Adeviation%2520video%2520denoising%2529.%2520We%2520present%2520evidence%2520through%2520quantitative%2520metrics%250Aand%2520visual%2520comparisons%2520on%2520various%2520challenging%2520datasets.%2520Additionally%252C%2520our%250Atechnique%2520works%2520with%2520any%25202D%2520restoration%2520diffusion%2520model%252C%2520offering%2520a%2520versatile%250Aand%2520powerful%2520tool%2520for%2520video%2520enhancement%2520tasks%2520without%2520extensive%2520retraining.%250AThis%2520research%2520leads%2520to%2520more%2520efficient%2520and%2520widely%2520applicable%2520video%2520restoration%250Atechnologies%252C%2520supporting%2520advancements%2520in%2520fields%2520that%2520require%2520high-quality%2520video%250Aoutput.%2520See%2520our%2520project%2520page%2520for%2520video%2520results%2520at%250Ahttps%253A//jimmycv07.github.io/DiffIR2VR_web/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01519v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffIR2VR-Zero%3A%20Zero-Shot%20Video%20Restoration%20with%20Diffusion-based%20Image%0A%20%20Restoration%20Models&entry.906535625=Chang-Han%20Yeh%20and%20Chin-Yang%20Lin%20and%20Zhixiang%20Wang%20and%20Chi-Wei%20Hsiao%20and%20Ting-Hsuan%20Chen%20and%20Yu-Lun%20Liu&entry.1292438233=%20%20This%20paper%20introduces%20a%20method%20for%20zero-shot%20video%20restoration%20using%0Apre-trained%20image%20restoration%20diffusion%20models.%20Traditional%20video%20restoration%0Amethods%20often%20need%20retraining%20for%20different%20settings%20and%20struggle%20with%20limited%0Ageneralization%20across%20various%20degradation%20types%20and%20datasets.%20Our%20approach%20uses%0Aa%20hierarchical%20token%20merging%20strategy%20for%20keyframes%20and%20local%20frames%2C%20combined%0Awith%20a%20hybrid%20correspondence%20mechanism%20that%20blends%20optical%20flow%20and%0Afeature-based%20nearest%20neighbor%20matching%20%28latent%20merging%29.%20We%20show%20that%20our%0Amethod%20not%20only%20achieves%20top%20performance%20in%20zero-shot%20video%20restoration%20but%0Aalso%20significantly%20surpasses%20trained%20models%20in%20generalization%20across%20diverse%0Adatasets%20and%20extreme%20degradations%20%288%24%5Ctimes%24%20super-resolution%20and%20high-standard%0Adeviation%20video%20denoising%29.%20We%20present%20evidence%20through%20quantitative%20metrics%0Aand%20visual%20comparisons%20on%20various%20challenging%20datasets.%20Additionally%2C%20our%0Atechnique%20works%20with%20any%202D%20restoration%20diffusion%20model%2C%20offering%20a%20versatile%0Aand%20powerful%20tool%20for%20video%20enhancement%20tasks%20without%20extensive%20retraining.%0AThis%20research%20leads%20to%20more%20efficient%20and%20widely%20applicable%20video%20restoration%0Atechnologies%2C%20supporting%20advancements%20in%20fields%20that%20require%20high-quality%20video%0Aoutput.%20See%20our%20project%20page%20for%20video%20results%20at%0Ahttps%3A//jimmycv07.github.io/DiffIR2VR_web/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01519v2&entry.124074799=Read"},
{"title": "TTT: A Temporal Refinement Heuristic for Tenuously Tractable Discrete\n  Time Reachability Problems", "author": "Chelsea Sidrane and Jana Tumova", "abstract": "  Reachable set computation is an important tool for analyzing control systems.\nSimulating a control system can show that the system is generally functioning\nas desired, but a formal tool like reachability analysis can provide a\nguarantee of correctness. For linear systems, reachability analysis is\nstraightforward and fast, but as more complex components are added to the\ncontrol system such as nonlinear dynamics or a neural network controller,\nreachability analysis may slow down or become overly conservative. To address\nthese challenges, much literature has focused on spatial refinement, e.g.,\ntuning the discretization of the input sets and intermediate reachable sets.\nHowever, this paper addresses a different dimension: temporal refinement. The\nbasic idea of temporal refinement is to automatically choose when along the\nhorizon of the reachability problem to execute slow symbolic queries which\nincur less approximation error versus fast concrete queries which incur more\napproximation error. Temporal refinement can be combined with other refinement\napproaches and offers an additional ``tuning knob'' with which to trade off\ntractability and tightness in approximate reachable set computation. Here, we\nintroduce an automatic framework for performing temporal refinement and we\ndemonstrate the effectiveness of this technique on computing approximate\nreachable sets for nonlinear systems with neural network control policies. We\ndemonstrate the calculation of reachable sets of varying approximation error\nunder varying computational budget and show that our algorithm is able to\ngenerate approximate reachable sets with a similar amount of error to the\nbaseline approach in 20-70% less time.\n", "link": "http://arxiv.org/abs/2407.14394v1", "date": "2024-07-19", "relevancy": 1.8733, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4928}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4579}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TTT%3A%20A%20Temporal%20Refinement%20Heuristic%20for%20Tenuously%20Tractable%20Discrete%0A%20%20Time%20Reachability%20Problems&body=Title%3A%20TTT%3A%20A%20Temporal%20Refinement%20Heuristic%20for%20Tenuously%20Tractable%20Discrete%0A%20%20Time%20Reachability%20Problems%0AAuthor%3A%20Chelsea%20Sidrane%20and%20Jana%20Tumova%0AAbstract%3A%20%20%20Reachable%20set%20computation%20is%20an%20important%20tool%20for%20analyzing%20control%20systems.%0ASimulating%20a%20control%20system%20can%20show%20that%20the%20system%20is%20generally%20functioning%0Aas%20desired%2C%20but%20a%20formal%20tool%20like%20reachability%20analysis%20can%20provide%20a%0Aguarantee%20of%20correctness.%20For%20linear%20systems%2C%20reachability%20analysis%20is%0Astraightforward%20and%20fast%2C%20but%20as%20more%20complex%20components%20are%20added%20to%20the%0Acontrol%20system%20such%20as%20nonlinear%20dynamics%20or%20a%20neural%20network%20controller%2C%0Areachability%20analysis%20may%20slow%20down%20or%20become%20overly%20conservative.%20To%20address%0Athese%20challenges%2C%20much%20literature%20has%20focused%20on%20spatial%20refinement%2C%20e.g.%2C%0Atuning%20the%20discretization%20of%20the%20input%20sets%20and%20intermediate%20reachable%20sets.%0AHowever%2C%20this%20paper%20addresses%20a%20different%20dimension%3A%20temporal%20refinement.%20The%0Abasic%20idea%20of%20temporal%20refinement%20is%20to%20automatically%20choose%20when%20along%20the%0Ahorizon%20of%20the%20reachability%20problem%20to%20execute%20slow%20symbolic%20queries%20which%0Aincur%20less%20approximation%20error%20versus%20fast%20concrete%20queries%20which%20incur%20more%0Aapproximation%20error.%20Temporal%20refinement%20can%20be%20combined%20with%20other%20refinement%0Aapproaches%20and%20offers%20an%20additional%20%60%60tuning%20knob%27%27%20with%20which%20to%20trade%20off%0Atractability%20and%20tightness%20in%20approximate%20reachable%20set%20computation.%20Here%2C%20we%0Aintroduce%20an%20automatic%20framework%20for%20performing%20temporal%20refinement%20and%20we%0Ademonstrate%20the%20effectiveness%20of%20this%20technique%20on%20computing%20approximate%0Areachable%20sets%20for%20nonlinear%20systems%20with%20neural%20network%20control%20policies.%20We%0Ademonstrate%20the%20calculation%20of%20reachable%20sets%20of%20varying%20approximation%20error%0Aunder%20varying%20computational%20budget%20and%20show%20that%20our%20algorithm%20is%20able%20to%0Agenerate%20approximate%20reachable%20sets%20with%20a%20similar%20amount%20of%20error%20to%20the%0Abaseline%20approach%20in%2020-70%25%20less%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTTT%253A%2520A%2520Temporal%2520Refinement%2520Heuristic%2520for%2520Tenuously%2520Tractable%2520Discrete%250A%2520%2520Time%2520Reachability%2520Problems%26entry.906535625%3DChelsea%2520Sidrane%2520and%2520Jana%2520Tumova%26entry.1292438233%3D%2520%2520Reachable%2520set%2520computation%2520is%2520an%2520important%2520tool%2520for%2520analyzing%2520control%2520systems.%250ASimulating%2520a%2520control%2520system%2520can%2520show%2520that%2520the%2520system%2520is%2520generally%2520functioning%250Aas%2520desired%252C%2520but%2520a%2520formal%2520tool%2520like%2520reachability%2520analysis%2520can%2520provide%2520a%250Aguarantee%2520of%2520correctness.%2520For%2520linear%2520systems%252C%2520reachability%2520analysis%2520is%250Astraightforward%2520and%2520fast%252C%2520but%2520as%2520more%2520complex%2520components%2520are%2520added%2520to%2520the%250Acontrol%2520system%2520such%2520as%2520nonlinear%2520dynamics%2520or%2520a%2520neural%2520network%2520controller%252C%250Areachability%2520analysis%2520may%2520slow%2520down%2520or%2520become%2520overly%2520conservative.%2520To%2520address%250Athese%2520challenges%252C%2520much%2520literature%2520has%2520focused%2520on%2520spatial%2520refinement%252C%2520e.g.%252C%250Atuning%2520the%2520discretization%2520of%2520the%2520input%2520sets%2520and%2520intermediate%2520reachable%2520sets.%250AHowever%252C%2520this%2520paper%2520addresses%2520a%2520different%2520dimension%253A%2520temporal%2520refinement.%2520The%250Abasic%2520idea%2520of%2520temporal%2520refinement%2520is%2520to%2520automatically%2520choose%2520when%2520along%2520the%250Ahorizon%2520of%2520the%2520reachability%2520problem%2520to%2520execute%2520slow%2520symbolic%2520queries%2520which%250Aincur%2520less%2520approximation%2520error%2520versus%2520fast%2520concrete%2520queries%2520which%2520incur%2520more%250Aapproximation%2520error.%2520Temporal%2520refinement%2520can%2520be%2520combined%2520with%2520other%2520refinement%250Aapproaches%2520and%2520offers%2520an%2520additional%2520%2560%2560tuning%2520knob%2527%2527%2520with%2520which%2520to%2520trade%2520off%250Atractability%2520and%2520tightness%2520in%2520approximate%2520reachable%2520set%2520computation.%2520Here%252C%2520we%250Aintroduce%2520an%2520automatic%2520framework%2520for%2520performing%2520temporal%2520refinement%2520and%2520we%250Ademonstrate%2520the%2520effectiveness%2520of%2520this%2520technique%2520on%2520computing%2520approximate%250Areachable%2520sets%2520for%2520nonlinear%2520systems%2520with%2520neural%2520network%2520control%2520policies.%2520We%250Ademonstrate%2520the%2520calculation%2520of%2520reachable%2520sets%2520of%2520varying%2520approximation%2520error%250Aunder%2520varying%2520computational%2520budget%2520and%2520show%2520that%2520our%2520algorithm%2520is%2520able%2520to%250Agenerate%2520approximate%2520reachable%2520sets%2520with%2520a%2520similar%2520amount%2520of%2520error%2520to%2520the%250Abaseline%2520approach%2520in%252020-70%2525%2520less%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TTT%3A%20A%20Temporal%20Refinement%20Heuristic%20for%20Tenuously%20Tractable%20Discrete%0A%20%20Time%20Reachability%20Problems&entry.906535625=Chelsea%20Sidrane%20and%20Jana%20Tumova&entry.1292438233=%20%20Reachable%20set%20computation%20is%20an%20important%20tool%20for%20analyzing%20control%20systems.%0ASimulating%20a%20control%20system%20can%20show%20that%20the%20system%20is%20generally%20functioning%0Aas%20desired%2C%20but%20a%20formal%20tool%20like%20reachability%20analysis%20can%20provide%20a%0Aguarantee%20of%20correctness.%20For%20linear%20systems%2C%20reachability%20analysis%20is%0Astraightforward%20and%20fast%2C%20but%20as%20more%20complex%20components%20are%20added%20to%20the%0Acontrol%20system%20such%20as%20nonlinear%20dynamics%20or%20a%20neural%20network%20controller%2C%0Areachability%20analysis%20may%20slow%20down%20or%20become%20overly%20conservative.%20To%20address%0Athese%20challenges%2C%20much%20literature%20has%20focused%20on%20spatial%20refinement%2C%20e.g.%2C%0Atuning%20the%20discretization%20of%20the%20input%20sets%20and%20intermediate%20reachable%20sets.%0AHowever%2C%20this%20paper%20addresses%20a%20different%20dimension%3A%20temporal%20refinement.%20The%0Abasic%20idea%20of%20temporal%20refinement%20is%20to%20automatically%20choose%20when%20along%20the%0Ahorizon%20of%20the%20reachability%20problem%20to%20execute%20slow%20symbolic%20queries%20which%0Aincur%20less%20approximation%20error%20versus%20fast%20concrete%20queries%20which%20incur%20more%0Aapproximation%20error.%20Temporal%20refinement%20can%20be%20combined%20with%20other%20refinement%0Aapproaches%20and%20offers%20an%20additional%20%60%60tuning%20knob%27%27%20with%20which%20to%20trade%20off%0Atractability%20and%20tightness%20in%20approximate%20reachable%20set%20computation.%20Here%2C%20we%0Aintroduce%20an%20automatic%20framework%20for%20performing%20temporal%20refinement%20and%20we%0Ademonstrate%20the%20effectiveness%20of%20this%20technique%20on%20computing%20approximate%0Areachable%20sets%20for%20nonlinear%20systems%20with%20neural%20network%20control%20policies.%20We%0Ademonstrate%20the%20calculation%20of%20reachable%20sets%20of%20varying%20approximation%20error%0Aunder%20varying%20computational%20budget%20and%20show%20that%20our%20algorithm%20is%20able%20to%0Agenerate%20approximate%20reachable%20sets%20with%20a%20similar%20amount%20of%20error%20to%20the%0Abaseline%20approach%20in%2020-70%25%20less%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14394v1&entry.124074799=Read"},
{"title": "Unlearning Concepts from Text-to-Video Diffusion Models", "author": "Shiqi Liu and Yihua Tan", "abstract": "  With the advancement of computer vision and natural language processing,\ntext-to-video generation, enabled by text-to-video diffusion models, has become\nmore prevalent. These models are trained using a large amount of data from the\ninternet. However, the training data often contain copyrighted content,\nincluding cartoon character icons and artist styles, private portraits, and\nunsafe videos. Since filtering the data and retraining the model is\nchallenging, methods for unlearning specific concepts from text-to-video\ndiffusion models have been investigated. However, due to the high computational\ncomplexity and relative large optimization scale, there is little work on\nunlearning methods for text-to-video diffusion models. We propose a novel\nconcept-unlearning method by transferring the unlearning capability of the text\nencoder of text-to-image diffusion models to text-to-video diffusion models.\nSpecifically, the method optimizes the text encoder using few-shot unlearning,\nwhere several generated images are used. We then use the optimized text encoder\nin text-to-video diffusion models to generate videos. Our method costs low\ncomputation resources and has small optimization scale. We discuss the\ngenerated videos after unlearning a concept. The experiments demonstrates that\nour method can unlearn copyrighted cartoon characters, artist styles, objects\nand people's facial characteristics. Our method can unlearn a concept within\nabout 100 seconds on an RTX 3070. Since there was no concept unlearning method\nfor text-to-video diffusion models before, we make concept unlearning feasible\nand more accessible in the text-to-video domain.\n", "link": "http://arxiv.org/abs/2407.14209v1", "date": "2024-07-19", "relevancy": 1.8655, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6668}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6112}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unlearning%20Concepts%20from%20Text-to-Video%20Diffusion%20Models&body=Title%3A%20Unlearning%20Concepts%20from%20Text-to-Video%20Diffusion%20Models%0AAuthor%3A%20Shiqi%20Liu%20and%20Yihua%20Tan%0AAbstract%3A%20%20%20With%20the%20advancement%20of%20computer%20vision%20and%20natural%20language%20processing%2C%0Atext-to-video%20generation%2C%20enabled%20by%20text-to-video%20diffusion%20models%2C%20has%20become%0Amore%20prevalent.%20These%20models%20are%20trained%20using%20a%20large%20amount%20of%20data%20from%20the%0Ainternet.%20However%2C%20the%20training%20data%20often%20contain%20copyrighted%20content%2C%0Aincluding%20cartoon%20character%20icons%20and%20artist%20styles%2C%20private%20portraits%2C%20and%0Aunsafe%20videos.%20Since%20filtering%20the%20data%20and%20retraining%20the%20model%20is%0Achallenging%2C%20methods%20for%20unlearning%20specific%20concepts%20from%20text-to-video%0Adiffusion%20models%20have%20been%20investigated.%20However%2C%20due%20to%20the%20high%20computational%0Acomplexity%20and%20relative%20large%20optimization%20scale%2C%20there%20is%20little%20work%20on%0Aunlearning%20methods%20for%20text-to-video%20diffusion%20models.%20We%20propose%20a%20novel%0Aconcept-unlearning%20method%20by%20transferring%20the%20unlearning%20capability%20of%20the%20text%0Aencoder%20of%20text-to-image%20diffusion%20models%20to%20text-to-video%20diffusion%20models.%0ASpecifically%2C%20the%20method%20optimizes%20the%20text%20encoder%20using%20few-shot%20unlearning%2C%0Awhere%20several%20generated%20images%20are%20used.%20We%20then%20use%20the%20optimized%20text%20encoder%0Ain%20text-to-video%20diffusion%20models%20to%20generate%20videos.%20Our%20method%20costs%20low%0Acomputation%20resources%20and%20has%20small%20optimization%20scale.%20We%20discuss%20the%0Agenerated%20videos%20after%20unlearning%20a%20concept.%20The%20experiments%20demonstrates%20that%0Aour%20method%20can%20unlearn%20copyrighted%20cartoon%20characters%2C%20artist%20styles%2C%20objects%0Aand%20people%27s%20facial%20characteristics.%20Our%20method%20can%20unlearn%20a%20concept%20within%0Aabout%20100%20seconds%20on%20an%20RTX%203070.%20Since%20there%20was%20no%20concept%20unlearning%20method%0Afor%20text-to-video%20diffusion%20models%20before%2C%20we%20make%20concept%20unlearning%20feasible%0Aand%20more%20accessible%20in%20the%20text-to-video%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14209v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnlearning%2520Concepts%2520from%2520Text-to-Video%2520Diffusion%2520Models%26entry.906535625%3DShiqi%2520Liu%2520and%2520Yihua%2520Tan%26entry.1292438233%3D%2520%2520With%2520the%2520advancement%2520of%2520computer%2520vision%2520and%2520natural%2520language%2520processing%252C%250Atext-to-video%2520generation%252C%2520enabled%2520by%2520text-to-video%2520diffusion%2520models%252C%2520has%2520become%250Amore%2520prevalent.%2520These%2520models%2520are%2520trained%2520using%2520a%2520large%2520amount%2520of%2520data%2520from%2520the%250Ainternet.%2520However%252C%2520the%2520training%2520data%2520often%2520contain%2520copyrighted%2520content%252C%250Aincluding%2520cartoon%2520character%2520icons%2520and%2520artist%2520styles%252C%2520private%2520portraits%252C%2520and%250Aunsafe%2520videos.%2520Since%2520filtering%2520the%2520data%2520and%2520retraining%2520the%2520model%2520is%250Achallenging%252C%2520methods%2520for%2520unlearning%2520specific%2520concepts%2520from%2520text-to-video%250Adiffusion%2520models%2520have%2520been%2520investigated.%2520However%252C%2520due%2520to%2520the%2520high%2520computational%250Acomplexity%2520and%2520relative%2520large%2520optimization%2520scale%252C%2520there%2520is%2520little%2520work%2520on%250Aunlearning%2520methods%2520for%2520text-to-video%2520diffusion%2520models.%2520We%2520propose%2520a%2520novel%250Aconcept-unlearning%2520method%2520by%2520transferring%2520the%2520unlearning%2520capability%2520of%2520the%2520text%250Aencoder%2520of%2520text-to-image%2520diffusion%2520models%2520to%2520text-to-video%2520diffusion%2520models.%250ASpecifically%252C%2520the%2520method%2520optimizes%2520the%2520text%2520encoder%2520using%2520few-shot%2520unlearning%252C%250Awhere%2520several%2520generated%2520images%2520are%2520used.%2520We%2520then%2520use%2520the%2520optimized%2520text%2520encoder%250Ain%2520text-to-video%2520diffusion%2520models%2520to%2520generate%2520videos.%2520Our%2520method%2520costs%2520low%250Acomputation%2520resources%2520and%2520has%2520small%2520optimization%2520scale.%2520We%2520discuss%2520the%250Agenerated%2520videos%2520after%2520unlearning%2520a%2520concept.%2520The%2520experiments%2520demonstrates%2520that%250Aour%2520method%2520can%2520unlearn%2520copyrighted%2520cartoon%2520characters%252C%2520artist%2520styles%252C%2520objects%250Aand%2520people%2527s%2520facial%2520characteristics.%2520Our%2520method%2520can%2520unlearn%2520a%2520concept%2520within%250Aabout%2520100%2520seconds%2520on%2520an%2520RTX%25203070.%2520Since%2520there%2520was%2520no%2520concept%2520unlearning%2520method%250Afor%2520text-to-video%2520diffusion%2520models%2520before%252C%2520we%2520make%2520concept%2520unlearning%2520feasible%250Aand%2520more%2520accessible%2520in%2520the%2520text-to-video%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14209v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlearning%20Concepts%20from%20Text-to-Video%20Diffusion%20Models&entry.906535625=Shiqi%20Liu%20and%20Yihua%20Tan&entry.1292438233=%20%20With%20the%20advancement%20of%20computer%20vision%20and%20natural%20language%20processing%2C%0Atext-to-video%20generation%2C%20enabled%20by%20text-to-video%20diffusion%20models%2C%20has%20become%0Amore%20prevalent.%20These%20models%20are%20trained%20using%20a%20large%20amount%20of%20data%20from%20the%0Ainternet.%20However%2C%20the%20training%20data%20often%20contain%20copyrighted%20content%2C%0Aincluding%20cartoon%20character%20icons%20and%20artist%20styles%2C%20private%20portraits%2C%20and%0Aunsafe%20videos.%20Since%20filtering%20the%20data%20and%20retraining%20the%20model%20is%0Achallenging%2C%20methods%20for%20unlearning%20specific%20concepts%20from%20text-to-video%0Adiffusion%20models%20have%20been%20investigated.%20However%2C%20due%20to%20the%20high%20computational%0Acomplexity%20and%20relative%20large%20optimization%20scale%2C%20there%20is%20little%20work%20on%0Aunlearning%20methods%20for%20text-to-video%20diffusion%20models.%20We%20propose%20a%20novel%0Aconcept-unlearning%20method%20by%20transferring%20the%20unlearning%20capability%20of%20the%20text%0Aencoder%20of%20text-to-image%20diffusion%20models%20to%20text-to-video%20diffusion%20models.%0ASpecifically%2C%20the%20method%20optimizes%20the%20text%20encoder%20using%20few-shot%20unlearning%2C%0Awhere%20several%20generated%20images%20are%20used.%20We%20then%20use%20the%20optimized%20text%20encoder%0Ain%20text-to-video%20diffusion%20models%20to%20generate%20videos.%20Our%20method%20costs%20low%0Acomputation%20resources%20and%20has%20small%20optimization%20scale.%20We%20discuss%20the%0Agenerated%20videos%20after%20unlearning%20a%20concept.%20The%20experiments%20demonstrates%20that%0Aour%20method%20can%20unlearn%20copyrighted%20cartoon%20characters%2C%20artist%20styles%2C%20objects%0Aand%20people%27s%20facial%20characteristics.%20Our%20method%20can%20unlearn%20a%20concept%20within%0Aabout%20100%20seconds%20on%20an%20RTX%203070.%20Since%20there%20was%20no%20concept%20unlearning%20method%0Afor%20text-to-video%20diffusion%20models%20before%2C%20we%20make%20concept%20unlearning%20feasible%0Aand%20more%20accessible%20in%20the%20text-to-video%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14209v1&entry.124074799=Read"},
{"title": "Open Artificial Knowledge", "author": "Vadim Borisov and Richard H. Schreiber", "abstract": "  The tremendous success of chat-based AI systems like ChatGPT, Claude, and\nGemini stems from Large Language Models (LLMs) trained on vast amount of\ndatasets. However, acquiring high-quality, diverse, and ethically sourced\ntraining data remains a significant challenge. We introduce the Open Artificial\nKnowledge (OAK) dataset, a large-scale resource of over 500 million tokens (at\nthe moment of writing) designed to address this issue. OAK leverages an\nensemble of state-of-the-art LLMs, including GPT4o, LLaMa3-70B, LLaMa3-8B,\nMixtral-8x7B, Gemma-7B, and Gemma-2-9B , to generate high-quality text across\ndiverse domains, guided by Wikipedia's main categories. Our methodology ensures\nbroad knowledge coverage while maintaining coherence and factual accuracy. The\nOAK dataset aims to foster the development of more capable and aligned language\nmodels while addressing critical issues of data scarcity and privacy in LLM\ntraining, and it is freely available on www.oakdataset.org.\n", "link": "http://arxiv.org/abs/2407.14371v1", "date": "2024-07-19", "relevancy": 1.8632, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4765}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4596}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open%20Artificial%20Knowledge&body=Title%3A%20Open%20Artificial%20Knowledge%0AAuthor%3A%20Vadim%20Borisov%20and%20Richard%20H.%20Schreiber%0AAbstract%3A%20%20%20The%20tremendous%20success%20of%20chat-based%20AI%20systems%20like%20ChatGPT%2C%20Claude%2C%20and%0AGemini%20stems%20from%20Large%20Language%20Models%20%28LLMs%29%20trained%20on%20vast%20amount%20of%0Adatasets.%20However%2C%20acquiring%20high-quality%2C%20diverse%2C%20and%20ethically%20sourced%0Atraining%20data%20remains%20a%20significant%20challenge.%20We%20introduce%20the%20Open%20Artificial%0AKnowledge%20%28OAK%29%20dataset%2C%20a%20large-scale%20resource%20of%20over%20500%20million%20tokens%20%28at%0Athe%20moment%20of%20writing%29%20designed%20to%20address%20this%20issue.%20OAK%20leverages%20an%0Aensemble%20of%20state-of-the-art%20LLMs%2C%20including%20GPT4o%2C%20LLaMa3-70B%2C%20LLaMa3-8B%2C%0AMixtral-8x7B%2C%20Gemma-7B%2C%20and%20Gemma-2-9B%20%2C%20to%20generate%20high-quality%20text%20across%0Adiverse%20domains%2C%20guided%20by%20Wikipedia%27s%20main%20categories.%20Our%20methodology%20ensures%0Abroad%20knowledge%20coverage%20while%20maintaining%20coherence%20and%20factual%20accuracy.%20The%0AOAK%20dataset%20aims%20to%20foster%20the%20development%20of%20more%20capable%20and%20aligned%20language%0Amodels%20while%20addressing%20critical%20issues%20of%20data%20scarcity%20and%20privacy%20in%20LLM%0Atraining%2C%20and%20it%20is%20freely%20available%20on%20www.oakdataset.org.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14371v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen%2520Artificial%2520Knowledge%26entry.906535625%3DVadim%2520Borisov%2520and%2520Richard%2520H.%2520Schreiber%26entry.1292438233%3D%2520%2520The%2520tremendous%2520success%2520of%2520chat-based%2520AI%2520systems%2520like%2520ChatGPT%252C%2520Claude%252C%2520and%250AGemini%2520stems%2520from%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520trained%2520on%2520vast%2520amount%2520of%250Adatasets.%2520However%252C%2520acquiring%2520high-quality%252C%2520diverse%252C%2520and%2520ethically%2520sourced%250Atraining%2520data%2520remains%2520a%2520significant%2520challenge.%2520We%2520introduce%2520the%2520Open%2520Artificial%250AKnowledge%2520%2528OAK%2529%2520dataset%252C%2520a%2520large-scale%2520resource%2520of%2520over%2520500%2520million%2520tokens%2520%2528at%250Athe%2520moment%2520of%2520writing%2529%2520designed%2520to%2520address%2520this%2520issue.%2520OAK%2520leverages%2520an%250Aensemble%2520of%2520state-of-the-art%2520LLMs%252C%2520including%2520GPT4o%252C%2520LLaMa3-70B%252C%2520LLaMa3-8B%252C%250AMixtral-8x7B%252C%2520Gemma-7B%252C%2520and%2520Gemma-2-9B%2520%252C%2520to%2520generate%2520high-quality%2520text%2520across%250Adiverse%2520domains%252C%2520guided%2520by%2520Wikipedia%2527s%2520main%2520categories.%2520Our%2520methodology%2520ensures%250Abroad%2520knowledge%2520coverage%2520while%2520maintaining%2520coherence%2520and%2520factual%2520accuracy.%2520The%250AOAK%2520dataset%2520aims%2520to%2520foster%2520the%2520development%2520of%2520more%2520capable%2520and%2520aligned%2520language%250Amodels%2520while%2520addressing%2520critical%2520issues%2520of%2520data%2520scarcity%2520and%2520privacy%2520in%2520LLM%250Atraining%252C%2520and%2520it%2520is%2520freely%2520available%2520on%2520www.oakdataset.org.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14371v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open%20Artificial%20Knowledge&entry.906535625=Vadim%20Borisov%20and%20Richard%20H.%20Schreiber&entry.1292438233=%20%20The%20tremendous%20success%20of%20chat-based%20AI%20systems%20like%20ChatGPT%2C%20Claude%2C%20and%0AGemini%20stems%20from%20Large%20Language%20Models%20%28LLMs%29%20trained%20on%20vast%20amount%20of%0Adatasets.%20However%2C%20acquiring%20high-quality%2C%20diverse%2C%20and%20ethically%20sourced%0Atraining%20data%20remains%20a%20significant%20challenge.%20We%20introduce%20the%20Open%20Artificial%0AKnowledge%20%28OAK%29%20dataset%2C%20a%20large-scale%20resource%20of%20over%20500%20million%20tokens%20%28at%0Athe%20moment%20of%20writing%29%20designed%20to%20address%20this%20issue.%20OAK%20leverages%20an%0Aensemble%20of%20state-of-the-art%20LLMs%2C%20including%20GPT4o%2C%20LLaMa3-70B%2C%20LLaMa3-8B%2C%0AMixtral-8x7B%2C%20Gemma-7B%2C%20and%20Gemma-2-9B%20%2C%20to%20generate%20high-quality%20text%20across%0Adiverse%20domains%2C%20guided%20by%20Wikipedia%27s%20main%20categories.%20Our%20methodology%20ensures%0Abroad%20knowledge%20coverage%20while%20maintaining%20coherence%20and%20factual%20accuracy.%20The%0AOAK%20dataset%20aims%20to%20foster%20the%20development%20of%20more%20capable%20and%20aligned%20language%0Amodels%20while%20addressing%20critical%20issues%20of%20data%20scarcity%20and%20privacy%20in%20LLM%0Atraining%2C%20and%20it%20is%20freely%20available%20on%20www.oakdataset.org.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14371v1&entry.124074799=Read"},
{"title": "Democratizing LLMs for Low-Resource Languages by Leveraging their\n  English Dominant Abilities with Linguistically-Diverse Prompts", "author": "Xuan-Phi Nguyen and Sharifah Mahani Aljunied and Shafiq Joty and Lidong Bing", "abstract": "  Large language models (LLMs) are known to effectively perform tasks by simply\nobserving few exemplars. However, in low-resource languages, obtaining such\nhand-picked exemplars can still be challenging, where unsupervised techniques\nmay be necessary. Moreover, competent generative capabilities of LLMs are\nobserved only in high-resource languages, while their performances among\nunder-represented languages fall behind due to pre-training data imbalance. To\nelicit LLMs' ability onto low-resource languages without any supervised data,\nwe propose to assemble synthetic exemplars from a diverse set of high-resource\nlanguages to prompt the LLMs to translate from any language into English. These\nprompts are then used to create intra-lingual exemplars to perform tasks in the\ntarget languages. Our unsupervised prompting method performs on par with\nsupervised few-shot learning in LLMs of different sizes for translations\nbetween English and 13 Indic and 21 African low-resource languages. We also\nshow that fine-tuning a 7B model on data generated from our method helps it\nperform competitively with a 175B model. In non-English translation tasks, our\nmethod even outperforms supervised prompting by up to 3 chrF++ in many\nlow-resource languages. When evaluated on zero-shot multilingual summarization,\nour method surpasses other English-pivoting baselines by up to 4 ROUGE-L and is\nalso favored by GPT-4.\n", "link": "http://arxiv.org/abs/2306.11372v2", "date": "2024-07-19", "relevancy": 1.854, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4733}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4671}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Democratizing%20LLMs%20for%20Low-Resource%20Languages%20by%20Leveraging%20their%0A%20%20English%20Dominant%20Abilities%20with%20Linguistically-Diverse%20Prompts&body=Title%3A%20Democratizing%20LLMs%20for%20Low-Resource%20Languages%20by%20Leveraging%20their%0A%20%20English%20Dominant%20Abilities%20with%20Linguistically-Diverse%20Prompts%0AAuthor%3A%20Xuan-Phi%20Nguyen%20and%20Sharifah%20Mahani%20Aljunied%20and%20Shafiq%20Joty%20and%20Lidong%20Bing%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20known%20to%20effectively%20perform%20tasks%20by%20simply%0Aobserving%20few%20exemplars.%20However%2C%20in%20low-resource%20languages%2C%20obtaining%20such%0Ahand-picked%20exemplars%20can%20still%20be%20challenging%2C%20where%20unsupervised%20techniques%0Amay%20be%20necessary.%20Moreover%2C%20competent%20generative%20capabilities%20of%20LLMs%20are%0Aobserved%20only%20in%20high-resource%20languages%2C%20while%20their%20performances%20among%0Aunder-represented%20languages%20fall%20behind%20due%20to%20pre-training%20data%20imbalance.%20To%0Aelicit%20LLMs%27%20ability%20onto%20low-resource%20languages%20without%20any%20supervised%20data%2C%0Awe%20propose%20to%20assemble%20synthetic%20exemplars%20from%20a%20diverse%20set%20of%20high-resource%0Alanguages%20to%20prompt%20the%20LLMs%20to%20translate%20from%20any%20language%20into%20English.%20These%0Aprompts%20are%20then%20used%20to%20create%20intra-lingual%20exemplars%20to%20perform%20tasks%20in%20the%0Atarget%20languages.%20Our%20unsupervised%20prompting%20method%20performs%20on%20par%20with%0Asupervised%20few-shot%20learning%20in%20LLMs%20of%20different%20sizes%20for%20translations%0Abetween%20English%20and%2013%20Indic%20and%2021%20African%20low-resource%20languages.%20We%20also%0Ashow%20that%20fine-tuning%20a%207B%20model%20on%20data%20generated%20from%20our%20method%20helps%20it%0Aperform%20competitively%20with%20a%20175B%20model.%20In%20non-English%20translation%20tasks%2C%20our%0Amethod%20even%20outperforms%20supervised%20prompting%20by%20up%20to%203%20chrF%2B%2B%20in%20many%0Alow-resource%20languages.%20When%20evaluated%20on%20zero-shot%20multilingual%20summarization%2C%0Aour%20method%20surpasses%20other%20English-pivoting%20baselines%20by%20up%20to%204%20ROUGE-L%20and%20is%0Aalso%20favored%20by%20GPT-4.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.11372v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDemocratizing%2520LLMs%2520for%2520Low-Resource%2520Languages%2520by%2520Leveraging%2520their%250A%2520%2520English%2520Dominant%2520Abilities%2520with%2520Linguistically-Diverse%2520Prompts%26entry.906535625%3DXuan-Phi%2520Nguyen%2520and%2520Sharifah%2520Mahani%2520Aljunied%2520and%2520Shafiq%2520Joty%2520and%2520Lidong%2520Bing%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520known%2520to%2520effectively%2520perform%2520tasks%2520by%2520simply%250Aobserving%2520few%2520exemplars.%2520However%252C%2520in%2520low-resource%2520languages%252C%2520obtaining%2520such%250Ahand-picked%2520exemplars%2520can%2520still%2520be%2520challenging%252C%2520where%2520unsupervised%2520techniques%250Amay%2520be%2520necessary.%2520Moreover%252C%2520competent%2520generative%2520capabilities%2520of%2520LLMs%2520are%250Aobserved%2520only%2520in%2520high-resource%2520languages%252C%2520while%2520their%2520performances%2520among%250Aunder-represented%2520languages%2520fall%2520behind%2520due%2520to%2520pre-training%2520data%2520imbalance.%2520To%250Aelicit%2520LLMs%2527%2520ability%2520onto%2520low-resource%2520languages%2520without%2520any%2520supervised%2520data%252C%250Awe%2520propose%2520to%2520assemble%2520synthetic%2520exemplars%2520from%2520a%2520diverse%2520set%2520of%2520high-resource%250Alanguages%2520to%2520prompt%2520the%2520LLMs%2520to%2520translate%2520from%2520any%2520language%2520into%2520English.%2520These%250Aprompts%2520are%2520then%2520used%2520to%2520create%2520intra-lingual%2520exemplars%2520to%2520perform%2520tasks%2520in%2520the%250Atarget%2520languages.%2520Our%2520unsupervised%2520prompting%2520method%2520performs%2520on%2520par%2520with%250Asupervised%2520few-shot%2520learning%2520in%2520LLMs%2520of%2520different%2520sizes%2520for%2520translations%250Abetween%2520English%2520and%252013%2520Indic%2520and%252021%2520African%2520low-resource%2520languages.%2520We%2520also%250Ashow%2520that%2520fine-tuning%2520a%25207B%2520model%2520on%2520data%2520generated%2520from%2520our%2520method%2520helps%2520it%250Aperform%2520competitively%2520with%2520a%2520175B%2520model.%2520In%2520non-English%2520translation%2520tasks%252C%2520our%250Amethod%2520even%2520outperforms%2520supervised%2520prompting%2520by%2520up%2520to%25203%2520chrF%252B%252B%2520in%2520many%250Alow-resource%2520languages.%2520When%2520evaluated%2520on%2520zero-shot%2520multilingual%2520summarization%252C%250Aour%2520method%2520surpasses%2520other%2520English-pivoting%2520baselines%2520by%2520up%2520to%25204%2520ROUGE-L%2520and%2520is%250Aalso%2520favored%2520by%2520GPT-4.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.11372v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Democratizing%20LLMs%20for%20Low-Resource%20Languages%20by%20Leveraging%20their%0A%20%20English%20Dominant%20Abilities%20with%20Linguistically-Diverse%20Prompts&entry.906535625=Xuan-Phi%20Nguyen%20and%20Sharifah%20Mahani%20Aljunied%20and%20Shafiq%20Joty%20and%20Lidong%20Bing&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20known%20to%20effectively%20perform%20tasks%20by%20simply%0Aobserving%20few%20exemplars.%20However%2C%20in%20low-resource%20languages%2C%20obtaining%20such%0Ahand-picked%20exemplars%20can%20still%20be%20challenging%2C%20where%20unsupervised%20techniques%0Amay%20be%20necessary.%20Moreover%2C%20competent%20generative%20capabilities%20of%20LLMs%20are%0Aobserved%20only%20in%20high-resource%20languages%2C%20while%20their%20performances%20among%0Aunder-represented%20languages%20fall%20behind%20due%20to%20pre-training%20data%20imbalance.%20To%0Aelicit%20LLMs%27%20ability%20onto%20low-resource%20languages%20without%20any%20supervised%20data%2C%0Awe%20propose%20to%20assemble%20synthetic%20exemplars%20from%20a%20diverse%20set%20of%20high-resource%0Alanguages%20to%20prompt%20the%20LLMs%20to%20translate%20from%20any%20language%20into%20English.%20These%0Aprompts%20are%20then%20used%20to%20create%20intra-lingual%20exemplars%20to%20perform%20tasks%20in%20the%0Atarget%20languages.%20Our%20unsupervised%20prompting%20method%20performs%20on%20par%20with%0Asupervised%20few-shot%20learning%20in%20LLMs%20of%20different%20sizes%20for%20translations%0Abetween%20English%20and%2013%20Indic%20and%2021%20African%20low-resource%20languages.%20We%20also%0Ashow%20that%20fine-tuning%20a%207B%20model%20on%20data%20generated%20from%20our%20method%20helps%20it%0Aperform%20competitively%20with%20a%20175B%20model.%20In%20non-English%20translation%20tasks%2C%20our%0Amethod%20even%20outperforms%20supervised%20prompting%20by%20up%20to%203%20chrF%2B%2B%20in%20many%0Alow-resource%20languages.%20When%20evaluated%20on%20zero-shot%20multilingual%20summarization%2C%0Aour%20method%20surpasses%20other%20English-pivoting%20baselines%20by%20up%20to%204%20ROUGE-L%20and%20is%0Aalso%20favored%20by%20GPT-4.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.11372v2&entry.124074799=Read"},
{"title": "Realistic Evaluation of Test-Time Adaptation Algorithms: Unsupervised\n  Hyperparameter Selection", "author": "Sebastian Cygert and Damian S\u00f3jka and Tomasz Trzci\u0144ski and Bart\u0142omiej Twardowski", "abstract": "  Test-Time Adaptation (TTA) has recently emerged as a promising strategy for\ntackling the problem of machine learning model robustness under distribution\nshifts by adapting the model during inference without access to any labels.\nBecause of task difficulty, hyperparameters strongly influence the\neffectiveness of adaptation. However, the literature has provided little\nexploration into optimal hyperparameter selection. In this work, we tackle this\nproblem by evaluating existing TTA methods using surrogate-based hp-selection\nstrategies (which do not assume access to the test labels) to obtain a more\nrealistic evaluation of their performance. We show that some of the recent\nstate-of-the-art methods exhibit inferior performance compared to the previous\nalgorithms when using our more realistic evaluation setup. Further, we show\nthat forgetting is still a problem in TTA as the only method that is robust to\nhp-selection resets the model to the initial state at every step. We analyze\ndifferent types of unsupervised selection strategies, and while they work\nreasonably well in most scenarios, the only strategies that work consistently\nwell use some kind of supervision (either by a limited number of annotated test\nsamples or by using pretraining data). Our findings underscore the need for\nfurther research with more rigorous benchmarking by explicitly stating model\nselection strategies, to facilitate which we open-source our code.\n", "link": "http://arxiv.org/abs/2407.14231v1", "date": "2024-07-19", "relevancy": 1.8539, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4675}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4669}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Realistic%20Evaluation%20of%20Test-Time%20Adaptation%20Algorithms%3A%20Unsupervised%0A%20%20Hyperparameter%20Selection&body=Title%3A%20Realistic%20Evaluation%20of%20Test-Time%20Adaptation%20Algorithms%3A%20Unsupervised%0A%20%20Hyperparameter%20Selection%0AAuthor%3A%20Sebastian%20Cygert%20and%20Damian%20S%C3%B3jka%20and%20Tomasz%20Trzci%C5%84ski%20and%20Bart%C5%82omiej%20Twardowski%0AAbstract%3A%20%20%20Test-Time%20Adaptation%20%28TTA%29%20has%20recently%20emerged%20as%20a%20promising%20strategy%20for%0Atackling%20the%20problem%20of%20machine%20learning%20model%20robustness%20under%20distribution%0Ashifts%20by%20adapting%20the%20model%20during%20inference%20without%20access%20to%20any%20labels.%0ABecause%20of%20task%20difficulty%2C%20hyperparameters%20strongly%20influence%20the%0Aeffectiveness%20of%20adaptation.%20However%2C%20the%20literature%20has%20provided%20little%0Aexploration%20into%20optimal%20hyperparameter%20selection.%20In%20this%20work%2C%20we%20tackle%20this%0Aproblem%20by%20evaluating%20existing%20TTA%20methods%20using%20surrogate-based%20hp-selection%0Astrategies%20%28which%20do%20not%20assume%20access%20to%20the%20test%20labels%29%20to%20obtain%20a%20more%0Arealistic%20evaluation%20of%20their%20performance.%20We%20show%20that%20some%20of%20the%20recent%0Astate-of-the-art%20methods%20exhibit%20inferior%20performance%20compared%20to%20the%20previous%0Aalgorithms%20when%20using%20our%20more%20realistic%20evaluation%20setup.%20Further%2C%20we%20show%0Athat%20forgetting%20is%20still%20a%20problem%20in%20TTA%20as%20the%20only%20method%20that%20is%20robust%20to%0Ahp-selection%20resets%20the%20model%20to%20the%20initial%20state%20at%20every%20step.%20We%20analyze%0Adifferent%20types%20of%20unsupervised%20selection%20strategies%2C%20and%20while%20they%20work%0Areasonably%20well%20in%20most%20scenarios%2C%20the%20only%20strategies%20that%20work%20consistently%0Awell%20use%20some%20kind%20of%20supervision%20%28either%20by%20a%20limited%20number%20of%20annotated%20test%0Asamples%20or%20by%20using%20pretraining%20data%29.%20Our%20findings%20underscore%20the%20need%20for%0Afurther%20research%20with%20more%20rigorous%20benchmarking%20by%20explicitly%20stating%20model%0Aselection%20strategies%2C%20to%20facilitate%20which%20we%20open-source%20our%20code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14231v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealistic%2520Evaluation%2520of%2520Test-Time%2520Adaptation%2520Algorithms%253A%2520Unsupervised%250A%2520%2520Hyperparameter%2520Selection%26entry.906535625%3DSebastian%2520Cygert%2520and%2520Damian%2520S%25C3%25B3jka%2520and%2520Tomasz%2520Trzci%25C5%2584ski%2520and%2520Bart%25C5%2582omiej%2520Twardowski%26entry.1292438233%3D%2520%2520Test-Time%2520Adaptation%2520%2528TTA%2529%2520has%2520recently%2520emerged%2520as%2520a%2520promising%2520strategy%2520for%250Atackling%2520the%2520problem%2520of%2520machine%2520learning%2520model%2520robustness%2520under%2520distribution%250Ashifts%2520by%2520adapting%2520the%2520model%2520during%2520inference%2520without%2520access%2520to%2520any%2520labels.%250ABecause%2520of%2520task%2520difficulty%252C%2520hyperparameters%2520strongly%2520influence%2520the%250Aeffectiveness%2520of%2520adaptation.%2520However%252C%2520the%2520literature%2520has%2520provided%2520little%250Aexploration%2520into%2520optimal%2520hyperparameter%2520selection.%2520In%2520this%2520work%252C%2520we%2520tackle%2520this%250Aproblem%2520by%2520evaluating%2520existing%2520TTA%2520methods%2520using%2520surrogate-based%2520hp-selection%250Astrategies%2520%2528which%2520do%2520not%2520assume%2520access%2520to%2520the%2520test%2520labels%2529%2520to%2520obtain%2520a%2520more%250Arealistic%2520evaluation%2520of%2520their%2520performance.%2520We%2520show%2520that%2520some%2520of%2520the%2520recent%250Astate-of-the-art%2520methods%2520exhibit%2520inferior%2520performance%2520compared%2520to%2520the%2520previous%250Aalgorithms%2520when%2520using%2520our%2520more%2520realistic%2520evaluation%2520setup.%2520Further%252C%2520we%2520show%250Athat%2520forgetting%2520is%2520still%2520a%2520problem%2520in%2520TTA%2520as%2520the%2520only%2520method%2520that%2520is%2520robust%2520to%250Ahp-selection%2520resets%2520the%2520model%2520to%2520the%2520initial%2520state%2520at%2520every%2520step.%2520We%2520analyze%250Adifferent%2520types%2520of%2520unsupervised%2520selection%2520strategies%252C%2520and%2520while%2520they%2520work%250Areasonably%2520well%2520in%2520most%2520scenarios%252C%2520the%2520only%2520strategies%2520that%2520work%2520consistently%250Awell%2520use%2520some%2520kind%2520of%2520supervision%2520%2528either%2520by%2520a%2520limited%2520number%2520of%2520annotated%2520test%250Asamples%2520or%2520by%2520using%2520pretraining%2520data%2529.%2520Our%2520findings%2520underscore%2520the%2520need%2520for%250Afurther%2520research%2520with%2520more%2520rigorous%2520benchmarking%2520by%2520explicitly%2520stating%2520model%250Aselection%2520strategies%252C%2520to%2520facilitate%2520which%2520we%2520open-source%2520our%2520code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14231v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Realistic%20Evaluation%20of%20Test-Time%20Adaptation%20Algorithms%3A%20Unsupervised%0A%20%20Hyperparameter%20Selection&entry.906535625=Sebastian%20Cygert%20and%20Damian%20S%C3%B3jka%20and%20Tomasz%20Trzci%C5%84ski%20and%20Bart%C5%82omiej%20Twardowski&entry.1292438233=%20%20Test-Time%20Adaptation%20%28TTA%29%20has%20recently%20emerged%20as%20a%20promising%20strategy%20for%0Atackling%20the%20problem%20of%20machine%20learning%20model%20robustness%20under%20distribution%0Ashifts%20by%20adapting%20the%20model%20during%20inference%20without%20access%20to%20any%20labels.%0ABecause%20of%20task%20difficulty%2C%20hyperparameters%20strongly%20influence%20the%0Aeffectiveness%20of%20adaptation.%20However%2C%20the%20literature%20has%20provided%20little%0Aexploration%20into%20optimal%20hyperparameter%20selection.%20In%20this%20work%2C%20we%20tackle%20this%0Aproblem%20by%20evaluating%20existing%20TTA%20methods%20using%20surrogate-based%20hp-selection%0Astrategies%20%28which%20do%20not%20assume%20access%20to%20the%20test%20labels%29%20to%20obtain%20a%20more%0Arealistic%20evaluation%20of%20their%20performance.%20We%20show%20that%20some%20of%20the%20recent%0Astate-of-the-art%20methods%20exhibit%20inferior%20performance%20compared%20to%20the%20previous%0Aalgorithms%20when%20using%20our%20more%20realistic%20evaluation%20setup.%20Further%2C%20we%20show%0Athat%20forgetting%20is%20still%20a%20problem%20in%20TTA%20as%20the%20only%20method%20that%20is%20robust%20to%0Ahp-selection%20resets%20the%20model%20to%20the%20initial%20state%20at%20every%20step.%20We%20analyze%0Adifferent%20types%20of%20unsupervised%20selection%20strategies%2C%20and%20while%20they%20work%0Areasonably%20well%20in%20most%20scenarios%2C%20the%20only%20strategies%20that%20work%20consistently%0Awell%20use%20some%20kind%20of%20supervision%20%28either%20by%20a%20limited%20number%20of%20annotated%20test%0Asamples%20or%20by%20using%20pretraining%20data%29.%20Our%20findings%20underscore%20the%20need%20for%0Afurther%20research%20with%20more%20rigorous%20benchmarking%20by%20explicitly%20stating%20model%0Aselection%20strategies%2C%20to%20facilitate%20which%20we%20open-source%20our%20code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14231v1&entry.124074799=Read"},
{"title": "LeKUBE: A Legal Knowledge Update BEnchmark", "author": "Changyue Wang and Weihang Su and Hu Yiran and Qingyao Ai and Yueyue Wu and Cheng Luo and Yiqun Liu and Min Zhang and Shaoping Ma", "abstract": "  Recent advances in Large Language Models (LLMs) have significantly shaped the\napplications of AI in multiple fields, including the studies of legal\nintelligence. Trained on extensive legal texts, including statutes and legal\ndocuments, the legal LLMs can capture important legal knowledge/concepts\neffectively and provide important support for downstream legal applications\nsuch as legal consultancy. Yet, the dynamic nature of legal statutes and\ninterpretations also poses new challenges to the use of LLMs in legal\napplications. Particularly, how to update the legal knowledge of LLMs\neffectively and efficiently has become an important research problem in\npractice. Existing benchmarks for evaluating knowledge update methods are\nmostly designed for the open domain and cannot address the specific challenges\nof the legal domain, such as the nuanced application of new legal knowledge,\nthe complexity and lengthiness of legal regulations, and the intricate nature\nof legal reasoning. To address this gap, we introduce the Legal Knowledge\nUpdate BEnchmark, i.e. LeKUBE, which evaluates knowledge update methods for\nlegal LLMs across five dimensions. Specifically, we categorize the needs of\nknowledge updates in the legal domain with the help of legal professionals, and\nthen hire annotators from law schools to create synthetic updates to the\nChinese Criminal and Civil Code as well as sets of questions of which the\nanswers would change after the updates. Through a comprehensive evaluation of\nstate-of-the-art knowledge update methods, we reveal a notable gap between\nexisting knowledge update methods and the unique needs of the legal domain,\nemphasizing the need for further research and development of knowledge update\nmechanisms tailored for legal LLMs.\n", "link": "http://arxiv.org/abs/2407.14192v1", "date": "2024-07-19", "relevancy": 1.3294, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.481}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4526}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4242}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LeKUBE%3A%20A%20Legal%20Knowledge%20Update%20BEnchmark&body=Title%3A%20LeKUBE%3A%20A%20Legal%20Knowledge%20Update%20BEnchmark%0AAuthor%3A%20Changyue%20Wang%20and%20Weihang%20Su%20and%20Hu%20Yiran%20and%20Qingyao%20Ai%20and%20Yueyue%20Wu%20and%20Cheng%20Luo%20and%20Yiqun%20Liu%20and%20Min%20Zhang%20and%20Shaoping%20Ma%0AAbstract%3A%20%20%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20significantly%20shaped%20the%0Aapplications%20of%20AI%20in%20multiple%20fields%2C%20including%20the%20studies%20of%20legal%0Aintelligence.%20Trained%20on%20extensive%20legal%20texts%2C%20including%20statutes%20and%20legal%0Adocuments%2C%20the%20legal%20LLMs%20can%20capture%20important%20legal%20knowledge/concepts%0Aeffectively%20and%20provide%20important%20support%20for%20downstream%20legal%20applications%0Asuch%20as%20legal%20consultancy.%20Yet%2C%20the%20dynamic%20nature%20of%20legal%20statutes%20and%0Ainterpretations%20also%20poses%20new%20challenges%20to%20the%20use%20of%20LLMs%20in%20legal%0Aapplications.%20Particularly%2C%20how%20to%20update%20the%20legal%20knowledge%20of%20LLMs%0Aeffectively%20and%20efficiently%20has%20become%20an%20important%20research%20problem%20in%0Apractice.%20Existing%20benchmarks%20for%20evaluating%20knowledge%20update%20methods%20are%0Amostly%20designed%20for%20the%20open%20domain%20and%20cannot%20address%20the%20specific%20challenges%0Aof%20the%20legal%20domain%2C%20such%20as%20the%20nuanced%20application%20of%20new%20legal%20knowledge%2C%0Athe%20complexity%20and%20lengthiness%20of%20legal%20regulations%2C%20and%20the%20intricate%20nature%0Aof%20legal%20reasoning.%20To%20address%20this%20gap%2C%20we%20introduce%20the%20Legal%20Knowledge%0AUpdate%20BEnchmark%2C%20i.e.%20LeKUBE%2C%20which%20evaluates%20knowledge%20update%20methods%20for%0Alegal%20LLMs%20across%20five%20dimensions.%20Specifically%2C%20we%20categorize%20the%20needs%20of%0Aknowledge%20updates%20in%20the%20legal%20domain%20with%20the%20help%20of%20legal%20professionals%2C%20and%0Athen%20hire%20annotators%20from%20law%20schools%20to%20create%20synthetic%20updates%20to%20the%0AChinese%20Criminal%20and%20Civil%20Code%20as%20well%20as%20sets%20of%20questions%20of%20which%20the%0Aanswers%20would%20change%20after%20the%20updates.%20Through%20a%20comprehensive%20evaluation%20of%0Astate-of-the-art%20knowledge%20update%20methods%2C%20we%20reveal%20a%20notable%20gap%20between%0Aexisting%20knowledge%20update%20methods%20and%20the%20unique%20needs%20of%20the%20legal%20domain%2C%0Aemphasizing%20the%20need%20for%20further%20research%20and%20development%20of%20knowledge%20update%0Amechanisms%20tailored%20for%20legal%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14192v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeKUBE%253A%2520A%2520Legal%2520Knowledge%2520Update%2520BEnchmark%26entry.906535625%3DChangyue%2520Wang%2520and%2520Weihang%2520Su%2520and%2520Hu%2520Yiran%2520and%2520Qingyao%2520Ai%2520and%2520Yueyue%2520Wu%2520and%2520Cheng%2520Luo%2520and%2520Yiqun%2520Liu%2520and%2520Min%2520Zhang%2520and%2520Shaoping%2520Ma%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520significantly%2520shaped%2520the%250Aapplications%2520of%2520AI%2520in%2520multiple%2520fields%252C%2520including%2520the%2520studies%2520of%2520legal%250Aintelligence.%2520Trained%2520on%2520extensive%2520legal%2520texts%252C%2520including%2520statutes%2520and%2520legal%250Adocuments%252C%2520the%2520legal%2520LLMs%2520can%2520capture%2520important%2520legal%2520knowledge/concepts%250Aeffectively%2520and%2520provide%2520important%2520support%2520for%2520downstream%2520legal%2520applications%250Asuch%2520as%2520legal%2520consultancy.%2520Yet%252C%2520the%2520dynamic%2520nature%2520of%2520legal%2520statutes%2520and%250Ainterpretations%2520also%2520poses%2520new%2520challenges%2520to%2520the%2520use%2520of%2520LLMs%2520in%2520legal%250Aapplications.%2520Particularly%252C%2520how%2520to%2520update%2520the%2520legal%2520knowledge%2520of%2520LLMs%250Aeffectively%2520and%2520efficiently%2520has%2520become%2520an%2520important%2520research%2520problem%2520in%250Apractice.%2520Existing%2520benchmarks%2520for%2520evaluating%2520knowledge%2520update%2520methods%2520are%250Amostly%2520designed%2520for%2520the%2520open%2520domain%2520and%2520cannot%2520address%2520the%2520specific%2520challenges%250Aof%2520the%2520legal%2520domain%252C%2520such%2520as%2520the%2520nuanced%2520application%2520of%2520new%2520legal%2520knowledge%252C%250Athe%2520complexity%2520and%2520lengthiness%2520of%2520legal%2520regulations%252C%2520and%2520the%2520intricate%2520nature%250Aof%2520legal%2520reasoning.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520the%2520Legal%2520Knowledge%250AUpdate%2520BEnchmark%252C%2520i.e.%2520LeKUBE%252C%2520which%2520evaluates%2520knowledge%2520update%2520methods%2520for%250Alegal%2520LLMs%2520across%2520five%2520dimensions.%2520Specifically%252C%2520we%2520categorize%2520the%2520needs%2520of%250Aknowledge%2520updates%2520in%2520the%2520legal%2520domain%2520with%2520the%2520help%2520of%2520legal%2520professionals%252C%2520and%250Athen%2520hire%2520annotators%2520from%2520law%2520schools%2520to%2520create%2520synthetic%2520updates%2520to%2520the%250AChinese%2520Criminal%2520and%2520Civil%2520Code%2520as%2520well%2520as%2520sets%2520of%2520questions%2520of%2520which%2520the%250Aanswers%2520would%2520change%2520after%2520the%2520updates.%2520Through%2520a%2520comprehensive%2520evaluation%2520of%250Astate-of-the-art%2520knowledge%2520update%2520methods%252C%2520we%2520reveal%2520a%2520notable%2520gap%2520between%250Aexisting%2520knowledge%2520update%2520methods%2520and%2520the%2520unique%2520needs%2520of%2520the%2520legal%2520domain%252C%250Aemphasizing%2520the%2520need%2520for%2520further%2520research%2520and%2520development%2520of%2520knowledge%2520update%250Amechanisms%2520tailored%2520for%2520legal%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14192v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LeKUBE%3A%20A%20Legal%20Knowledge%20Update%20BEnchmark&entry.906535625=Changyue%20Wang%20and%20Weihang%20Su%20and%20Hu%20Yiran%20and%20Qingyao%20Ai%20and%20Yueyue%20Wu%20and%20Cheng%20Luo%20and%20Yiqun%20Liu%20and%20Min%20Zhang%20and%20Shaoping%20Ma&entry.1292438233=%20%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20significantly%20shaped%20the%0Aapplications%20of%20AI%20in%20multiple%20fields%2C%20including%20the%20studies%20of%20legal%0Aintelligence.%20Trained%20on%20extensive%20legal%20texts%2C%20including%20statutes%20and%20legal%0Adocuments%2C%20the%20legal%20LLMs%20can%20capture%20important%20legal%20knowledge/concepts%0Aeffectively%20and%20provide%20important%20support%20for%20downstream%20legal%20applications%0Asuch%20as%20legal%20consultancy.%20Yet%2C%20the%20dynamic%20nature%20of%20legal%20statutes%20and%0Ainterpretations%20also%20poses%20new%20challenges%20to%20the%20use%20of%20LLMs%20in%20legal%0Aapplications.%20Particularly%2C%20how%20to%20update%20the%20legal%20knowledge%20of%20LLMs%0Aeffectively%20and%20efficiently%20has%20become%20an%20important%20research%20problem%20in%0Apractice.%20Existing%20benchmarks%20for%20evaluating%20knowledge%20update%20methods%20are%0Amostly%20designed%20for%20the%20open%20domain%20and%20cannot%20address%20the%20specific%20challenges%0Aof%20the%20legal%20domain%2C%20such%20as%20the%20nuanced%20application%20of%20new%20legal%20knowledge%2C%0Athe%20complexity%20and%20lengthiness%20of%20legal%20regulations%2C%20and%20the%20intricate%20nature%0Aof%20legal%20reasoning.%20To%20address%20this%20gap%2C%20we%20introduce%20the%20Legal%20Knowledge%0AUpdate%20BEnchmark%2C%20i.e.%20LeKUBE%2C%20which%20evaluates%20knowledge%20update%20methods%20for%0Alegal%20LLMs%20across%20five%20dimensions.%20Specifically%2C%20we%20categorize%20the%20needs%20of%0Aknowledge%20updates%20in%20the%20legal%20domain%20with%20the%20help%20of%20legal%20professionals%2C%20and%0Athen%20hire%20annotators%20from%20law%20schools%20to%20create%20synthetic%20updates%20to%20the%0AChinese%20Criminal%20and%20Civil%20Code%20as%20well%20as%20sets%20of%20questions%20of%20which%20the%0Aanswers%20would%20change%20after%20the%20updates.%20Through%20a%20comprehensive%20evaluation%20of%0Astate-of-the-art%20knowledge%20update%20methods%2C%20we%20reveal%20a%20notable%20gap%20between%0Aexisting%20knowledge%20update%20methods%20and%20the%20unique%20needs%20of%20the%20legal%20domain%2C%0Aemphasizing%20the%20need%20for%20further%20research%20and%20development%20of%20knowledge%20update%0Amechanisms%20tailored%20for%20legal%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14192v1&entry.124074799=Read"},
{"title": "Frontiers of Deep Learning: From Novel Application to Real-World\n  Deployment", "author": "Rui Xie", "abstract": "  Deep learning continues to re-shape numerous fields, from natural language\nprocessing and imaging to data analytics and recommendation systems. This\nreport studies two research papers that represent recent progress on deep\nlearning from two largely different aspects: The first paper applied the\ntransformer networks, which are typically used in language models, to improve\nthe quality of synthetic aperture radar image by effectively reducing the\nspeckle noise. The second paper presents an in-storage computing design\nsolution to enable cost-efficient and high-performance implementations of deep\nlearning recommendation systems. In addition to summarizing each paper in terms\nof motivation, key ideas and techniques, and evaluation results, this report\nalso presents thoughts and discussions about possible future research\ndirections. By carrying out in-depth study on these two representative papers\nand related references, this doctoral candidate has developed better\nunderstanding on the far-reaching impact and efficient implementation of deep\nlearning models.\n", "link": "http://arxiv.org/abs/2407.14386v1", "date": "2024-07-19", "relevancy": 1.5426, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5316}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5267}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frontiers%20of%20Deep%20Learning%3A%20From%20Novel%20Application%20to%20Real-World%0A%20%20Deployment&body=Title%3A%20Frontiers%20of%20Deep%20Learning%3A%20From%20Novel%20Application%20to%20Real-World%0A%20%20Deployment%0AAuthor%3A%20Rui%20Xie%0AAbstract%3A%20%20%20Deep%20learning%20continues%20to%20re-shape%20numerous%20fields%2C%20from%20natural%20language%0Aprocessing%20and%20imaging%20to%20data%20analytics%20and%20recommendation%20systems.%20This%0Areport%20studies%20two%20research%20papers%20that%20represent%20recent%20progress%20on%20deep%0Alearning%20from%20two%20largely%20different%20aspects%3A%20The%20first%20paper%20applied%20the%0Atransformer%20networks%2C%20which%20are%20typically%20used%20in%20language%20models%2C%20to%20improve%0Athe%20quality%20of%20synthetic%20aperture%20radar%20image%20by%20effectively%20reducing%20the%0Aspeckle%20noise.%20The%20second%20paper%20presents%20an%20in-storage%20computing%20design%0Asolution%20to%20enable%20cost-efficient%20and%20high-performance%20implementations%20of%20deep%0Alearning%20recommendation%20systems.%20In%20addition%20to%20summarizing%20each%20paper%20in%20terms%0Aof%20motivation%2C%20key%20ideas%20and%20techniques%2C%20and%20evaluation%20results%2C%20this%20report%0Aalso%20presents%20thoughts%20and%20discussions%20about%20possible%20future%20research%0Adirections.%20By%20carrying%20out%20in-depth%20study%20on%20these%20two%20representative%20papers%0Aand%20related%20references%2C%20this%20doctoral%20candidate%20has%20developed%20better%0Aunderstanding%20on%20the%20far-reaching%20impact%20and%20efficient%20implementation%20of%20deep%0Alearning%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14386v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrontiers%2520of%2520Deep%2520Learning%253A%2520From%2520Novel%2520Application%2520to%2520Real-World%250A%2520%2520Deployment%26entry.906535625%3DRui%2520Xie%26entry.1292438233%3D%2520%2520Deep%2520learning%2520continues%2520to%2520re-shape%2520numerous%2520fields%252C%2520from%2520natural%2520language%250Aprocessing%2520and%2520imaging%2520to%2520data%2520analytics%2520and%2520recommendation%2520systems.%2520This%250Areport%2520studies%2520two%2520research%2520papers%2520that%2520represent%2520recent%2520progress%2520on%2520deep%250Alearning%2520from%2520two%2520largely%2520different%2520aspects%253A%2520The%2520first%2520paper%2520applied%2520the%250Atransformer%2520networks%252C%2520which%2520are%2520typically%2520used%2520in%2520language%2520models%252C%2520to%2520improve%250Athe%2520quality%2520of%2520synthetic%2520aperture%2520radar%2520image%2520by%2520effectively%2520reducing%2520the%250Aspeckle%2520noise.%2520The%2520second%2520paper%2520presents%2520an%2520in-storage%2520computing%2520design%250Asolution%2520to%2520enable%2520cost-efficient%2520and%2520high-performance%2520implementations%2520of%2520deep%250Alearning%2520recommendation%2520systems.%2520In%2520addition%2520to%2520summarizing%2520each%2520paper%2520in%2520terms%250Aof%2520motivation%252C%2520key%2520ideas%2520and%2520techniques%252C%2520and%2520evaluation%2520results%252C%2520this%2520report%250Aalso%2520presents%2520thoughts%2520and%2520discussions%2520about%2520possible%2520future%2520research%250Adirections.%2520By%2520carrying%2520out%2520in-depth%2520study%2520on%2520these%2520two%2520representative%2520papers%250Aand%2520related%2520references%252C%2520this%2520doctoral%2520candidate%2520has%2520developed%2520better%250Aunderstanding%2520on%2520the%2520far-reaching%2520impact%2520and%2520efficient%2520implementation%2520of%2520deep%250Alearning%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14386v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frontiers%20of%20Deep%20Learning%3A%20From%20Novel%20Application%20to%20Real-World%0A%20%20Deployment&entry.906535625=Rui%20Xie&entry.1292438233=%20%20Deep%20learning%20continues%20to%20re-shape%20numerous%20fields%2C%20from%20natural%20language%0Aprocessing%20and%20imaging%20to%20data%20analytics%20and%20recommendation%20systems.%20This%0Areport%20studies%20two%20research%20papers%20that%20represent%20recent%20progress%20on%20deep%0Alearning%20from%20two%20largely%20different%20aspects%3A%20The%20first%20paper%20applied%20the%0Atransformer%20networks%2C%20which%20are%20typically%20used%20in%20language%20models%2C%20to%20improve%0Athe%20quality%20of%20synthetic%20aperture%20radar%20image%20by%20effectively%20reducing%20the%0Aspeckle%20noise.%20The%20second%20paper%20presents%20an%20in-storage%20computing%20design%0Asolution%20to%20enable%20cost-efficient%20and%20high-performance%20implementations%20of%20deep%0Alearning%20recommendation%20systems.%20In%20addition%20to%20summarizing%20each%20paper%20in%20terms%0Aof%20motivation%2C%20key%20ideas%20and%20techniques%2C%20and%20evaluation%20results%2C%20this%20report%0Aalso%20presents%20thoughts%20and%20discussions%20about%20possible%20future%20research%0Adirections.%20By%20carrying%20out%20in-depth%20study%20on%20these%20two%20representative%20papers%0Aand%20related%20references%2C%20this%20doctoral%20candidate%20has%20developed%20better%0Aunderstanding%20on%20the%20far-reaching%20impact%20and%20efficient%20implementation%20of%20deep%0Alearning%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14386v1&entry.124074799=Read"},
{"title": "Achieving Well-Informed Decision-Making in Drug Discovery: A\n  Comprehensive Calibration Study using Neural Network-Based Structure-Activity\n  Models", "author": "Hannah Rosa Friesacher and Ola Engkvist and Lewis Mervin and Yves Moreau and Adam Arany", "abstract": "  In the drug discovery process, where experiments can be costly and\ntime-consuming, computational models that predict drug-target interactions are\nvaluable tools to accelerate the development of new therapeutic agents.\nEstimating the uncertainty inherent in these neural network predictions\nprovides valuable information that facilitates optimal decision-making when\nrisk assessment is crucial. However, such models can be poorly calibrated,\nwhich results in unreliable uncertainty estimates that do not reflect the true\npredictive uncertainty. In this study, we compare different metrics, including\naccuracy and calibration scores, used for model hyperparameter tuning to\ninvestigate which model selection strategy achieves well-calibrated models.\nFurthermore, we propose to use a computationally efficient Bayesian uncertainty\nestimation method named Bayesian Linear Probing (BLP), which generates\nHamiltonian Monte Carlo (HMC) trajectories to obtain samples for the parameters\nof a Bayesian Logistic Regression fitted to the hidden layer of the baseline\nneural network. We report that BLP improves model calibration and achieves the\nperformance of common uncertainty quantification methods by combining the\nbenefits of uncertainty estimation and probability calibration methods.\nFinally, we show that combining post hoc calibration method with\nwell-performing uncertainty quantification approaches can boost model accuracy\nand calibration.\n", "link": "http://arxiv.org/abs/2407.14185v1", "date": "2024-07-19", "relevancy": 1.5694, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5258}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5226}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Achieving%20Well-Informed%20Decision-Making%20in%20Drug%20Discovery%3A%20A%0A%20%20Comprehensive%20Calibration%20Study%20using%20Neural%20Network-Based%20Structure-Activity%0A%20%20Models&body=Title%3A%20Achieving%20Well-Informed%20Decision-Making%20in%20Drug%20Discovery%3A%20A%0A%20%20Comprehensive%20Calibration%20Study%20using%20Neural%20Network-Based%20Structure-Activity%0A%20%20Models%0AAuthor%3A%20Hannah%20Rosa%20Friesacher%20and%20Ola%20Engkvist%20and%20Lewis%20Mervin%20and%20Yves%20Moreau%20and%20Adam%20Arany%0AAbstract%3A%20%20%20In%20the%20drug%20discovery%20process%2C%20where%20experiments%20can%20be%20costly%20and%0Atime-consuming%2C%20computational%20models%20that%20predict%20drug-target%20interactions%20are%0Avaluable%20tools%20to%20accelerate%20the%20development%20of%20new%20therapeutic%20agents.%0AEstimating%20the%20uncertainty%20inherent%20in%20these%20neural%20network%20predictions%0Aprovides%20valuable%20information%20that%20facilitates%20optimal%20decision-making%20when%0Arisk%20assessment%20is%20crucial.%20However%2C%20such%20models%20can%20be%20poorly%20calibrated%2C%0Awhich%20results%20in%20unreliable%20uncertainty%20estimates%20that%20do%20not%20reflect%20the%20true%0Apredictive%20uncertainty.%20In%20this%20study%2C%20we%20compare%20different%20metrics%2C%20including%0Aaccuracy%20and%20calibration%20scores%2C%20used%20for%20model%20hyperparameter%20tuning%20to%0Ainvestigate%20which%20model%20selection%20strategy%20achieves%20well-calibrated%20models.%0AFurthermore%2C%20we%20propose%20to%20use%20a%20computationally%20efficient%20Bayesian%20uncertainty%0Aestimation%20method%20named%20Bayesian%20Linear%20Probing%20%28BLP%29%2C%20which%20generates%0AHamiltonian%20Monte%20Carlo%20%28HMC%29%20trajectories%20to%20obtain%20samples%20for%20the%20parameters%0Aof%20a%20Bayesian%20Logistic%20Regression%20fitted%20to%20the%20hidden%20layer%20of%20the%20baseline%0Aneural%20network.%20We%20report%20that%20BLP%20improves%20model%20calibration%20and%20achieves%20the%0Aperformance%20of%20common%20uncertainty%20quantification%20methods%20by%20combining%20the%0Abenefits%20of%20uncertainty%20estimation%20and%20probability%20calibration%20methods.%0AFinally%2C%20we%20show%20that%20combining%20post%20hoc%20calibration%20method%20with%0Awell-performing%20uncertainty%20quantification%20approaches%20can%20boost%20model%20accuracy%0Aand%20calibration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14185v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAchieving%2520Well-Informed%2520Decision-Making%2520in%2520Drug%2520Discovery%253A%2520A%250A%2520%2520Comprehensive%2520Calibration%2520Study%2520using%2520Neural%2520Network-Based%2520Structure-Activity%250A%2520%2520Models%26entry.906535625%3DHannah%2520Rosa%2520Friesacher%2520and%2520Ola%2520Engkvist%2520and%2520Lewis%2520Mervin%2520and%2520Yves%2520Moreau%2520and%2520Adam%2520Arany%26entry.1292438233%3D%2520%2520In%2520the%2520drug%2520discovery%2520process%252C%2520where%2520experiments%2520can%2520be%2520costly%2520and%250Atime-consuming%252C%2520computational%2520models%2520that%2520predict%2520drug-target%2520interactions%2520are%250Avaluable%2520tools%2520to%2520accelerate%2520the%2520development%2520of%2520new%2520therapeutic%2520agents.%250AEstimating%2520the%2520uncertainty%2520inherent%2520in%2520these%2520neural%2520network%2520predictions%250Aprovides%2520valuable%2520information%2520that%2520facilitates%2520optimal%2520decision-making%2520when%250Arisk%2520assessment%2520is%2520crucial.%2520However%252C%2520such%2520models%2520can%2520be%2520poorly%2520calibrated%252C%250Awhich%2520results%2520in%2520unreliable%2520uncertainty%2520estimates%2520that%2520do%2520not%2520reflect%2520the%2520true%250Apredictive%2520uncertainty.%2520In%2520this%2520study%252C%2520we%2520compare%2520different%2520metrics%252C%2520including%250Aaccuracy%2520and%2520calibration%2520scores%252C%2520used%2520for%2520model%2520hyperparameter%2520tuning%2520to%250Ainvestigate%2520which%2520model%2520selection%2520strategy%2520achieves%2520well-calibrated%2520models.%250AFurthermore%252C%2520we%2520propose%2520to%2520use%2520a%2520computationally%2520efficient%2520Bayesian%2520uncertainty%250Aestimation%2520method%2520named%2520Bayesian%2520Linear%2520Probing%2520%2528BLP%2529%252C%2520which%2520generates%250AHamiltonian%2520Monte%2520Carlo%2520%2528HMC%2529%2520trajectories%2520to%2520obtain%2520samples%2520for%2520the%2520parameters%250Aof%2520a%2520Bayesian%2520Logistic%2520Regression%2520fitted%2520to%2520the%2520hidden%2520layer%2520of%2520the%2520baseline%250Aneural%2520network.%2520We%2520report%2520that%2520BLP%2520improves%2520model%2520calibration%2520and%2520achieves%2520the%250Aperformance%2520of%2520common%2520uncertainty%2520quantification%2520methods%2520by%2520combining%2520the%250Abenefits%2520of%2520uncertainty%2520estimation%2520and%2520probability%2520calibration%2520methods.%250AFinally%252C%2520we%2520show%2520that%2520combining%2520post%2520hoc%2520calibration%2520method%2520with%250Awell-performing%2520uncertainty%2520quantification%2520approaches%2520can%2520boost%2520model%2520accuracy%250Aand%2520calibration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14185v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Achieving%20Well-Informed%20Decision-Making%20in%20Drug%20Discovery%3A%20A%0A%20%20Comprehensive%20Calibration%20Study%20using%20Neural%20Network-Based%20Structure-Activity%0A%20%20Models&entry.906535625=Hannah%20Rosa%20Friesacher%20and%20Ola%20Engkvist%20and%20Lewis%20Mervin%20and%20Yves%20Moreau%20and%20Adam%20Arany&entry.1292438233=%20%20In%20the%20drug%20discovery%20process%2C%20where%20experiments%20can%20be%20costly%20and%0Atime-consuming%2C%20computational%20models%20that%20predict%20drug-target%20interactions%20are%0Avaluable%20tools%20to%20accelerate%20the%20development%20of%20new%20therapeutic%20agents.%0AEstimating%20the%20uncertainty%20inherent%20in%20these%20neural%20network%20predictions%0Aprovides%20valuable%20information%20that%20facilitates%20optimal%20decision-making%20when%0Arisk%20assessment%20is%20crucial.%20However%2C%20such%20models%20can%20be%20poorly%20calibrated%2C%0Awhich%20results%20in%20unreliable%20uncertainty%20estimates%20that%20do%20not%20reflect%20the%20true%0Apredictive%20uncertainty.%20In%20this%20study%2C%20we%20compare%20different%20metrics%2C%20including%0Aaccuracy%20and%20calibration%20scores%2C%20used%20for%20model%20hyperparameter%20tuning%20to%0Ainvestigate%20which%20model%20selection%20strategy%20achieves%20well-calibrated%20models.%0AFurthermore%2C%20we%20propose%20to%20use%20a%20computationally%20efficient%20Bayesian%20uncertainty%0Aestimation%20method%20named%20Bayesian%20Linear%20Probing%20%28BLP%29%2C%20which%20generates%0AHamiltonian%20Monte%20Carlo%20%28HMC%29%20trajectories%20to%20obtain%20samples%20for%20the%20parameters%0Aof%20a%20Bayesian%20Logistic%20Regression%20fitted%20to%20the%20hidden%20layer%20of%20the%20baseline%0Aneural%20network.%20We%20report%20that%20BLP%20improves%20model%20calibration%20and%20achieves%20the%0Aperformance%20of%20common%20uncertainty%20quantification%20methods%20by%20combining%20the%0Abenefits%20of%20uncertainty%20estimation%20and%20probability%20calibration%20methods.%0AFinally%2C%20we%20show%20that%20combining%20post%20hoc%20calibration%20method%20with%0Awell-performing%20uncertainty%20quantification%20approaches%20can%20boost%20model%20accuracy%0Aand%20calibration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14185v1&entry.124074799=Read"},
{"title": "DEPICT: Diffusion-Enabled Permutation Importance for Image\n  Classification Tasks", "author": "Sarah Jabbour and Gregory Kondas and Ella Kazerooni and Michael Sjoding and David Fouhey and Jenna Wiens", "abstract": "  We propose a permutation-based explanation method for image classifiers.\nCurrent image-model explanations like activation maps are limited to\ninstance-based explanations in the pixel space, making it difficult to\nunderstand global model behavior. In contrast, permutation based explanations\nfor tabular data classifiers measure feature importance by comparing model\nperformance on data before and after permuting a feature. We propose an\nexplanation method for image-based models that permutes interpretable concepts\nacross dataset images. Given a dataset of images labeled with specific concepts\nlike captions, we permute a concept across examples in the text space and then\ngenerate images via a text-conditioned diffusion model. Feature importance is\nthen reflected by the change in model performance relative to unpermuted data.\nWhen applied to a set of concepts, the method generates a ranking of feature\nimportance. We show this approach recovers underlying model feature importance\non synthetic and real-world image classification tasks.\n", "link": "http://arxiv.org/abs/2407.14509v1", "date": "2024-07-19", "relevancy": 1.0639, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5403}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5315}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DEPICT%3A%20Diffusion-Enabled%20Permutation%20Importance%20for%20Image%0A%20%20Classification%20Tasks&body=Title%3A%20DEPICT%3A%20Diffusion-Enabled%20Permutation%20Importance%20for%20Image%0A%20%20Classification%20Tasks%0AAuthor%3A%20Sarah%20Jabbour%20and%20Gregory%20Kondas%20and%20Ella%20Kazerooni%20and%20Michael%20Sjoding%20and%20David%20Fouhey%20and%20Jenna%20Wiens%0AAbstract%3A%20%20%20We%20propose%20a%20permutation-based%20explanation%20method%20for%20image%20classifiers.%0ACurrent%20image-model%20explanations%20like%20activation%20maps%20are%20limited%20to%0Ainstance-based%20explanations%20in%20the%20pixel%20space%2C%20making%20it%20difficult%20to%0Aunderstand%20global%20model%20behavior.%20In%20contrast%2C%20permutation%20based%20explanations%0Afor%20tabular%20data%20classifiers%20measure%20feature%20importance%20by%20comparing%20model%0Aperformance%20on%20data%20before%20and%20after%20permuting%20a%20feature.%20We%20propose%20an%0Aexplanation%20method%20for%20image-based%20models%20that%20permutes%20interpretable%20concepts%0Aacross%20dataset%20images.%20Given%20a%20dataset%20of%20images%20labeled%20with%20specific%20concepts%0Alike%20captions%2C%20we%20permute%20a%20concept%20across%20examples%20in%20the%20text%20space%20and%20then%0Agenerate%20images%20via%20a%20text-conditioned%20diffusion%20model.%20Feature%20importance%20is%0Athen%20reflected%20by%20the%20change%20in%20model%20performance%20relative%20to%20unpermuted%20data.%0AWhen%20applied%20to%20a%20set%20of%20concepts%2C%20the%20method%20generates%20a%20ranking%20of%20feature%0Aimportance.%20We%20show%20this%20approach%20recovers%20underlying%20model%20feature%20importance%0Aon%20synthetic%20and%20real-world%20image%20classification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14509v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDEPICT%253A%2520Diffusion-Enabled%2520Permutation%2520Importance%2520for%2520Image%250A%2520%2520Classification%2520Tasks%26entry.906535625%3DSarah%2520Jabbour%2520and%2520Gregory%2520Kondas%2520and%2520Ella%2520Kazerooni%2520and%2520Michael%2520Sjoding%2520and%2520David%2520Fouhey%2520and%2520Jenna%2520Wiens%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520permutation-based%2520explanation%2520method%2520for%2520image%2520classifiers.%250ACurrent%2520image-model%2520explanations%2520like%2520activation%2520maps%2520are%2520limited%2520to%250Ainstance-based%2520explanations%2520in%2520the%2520pixel%2520space%252C%2520making%2520it%2520difficult%2520to%250Aunderstand%2520global%2520model%2520behavior.%2520In%2520contrast%252C%2520permutation%2520based%2520explanations%250Afor%2520tabular%2520data%2520classifiers%2520measure%2520feature%2520importance%2520by%2520comparing%2520model%250Aperformance%2520on%2520data%2520before%2520and%2520after%2520permuting%2520a%2520feature.%2520We%2520propose%2520an%250Aexplanation%2520method%2520for%2520image-based%2520models%2520that%2520permutes%2520interpretable%2520concepts%250Aacross%2520dataset%2520images.%2520Given%2520a%2520dataset%2520of%2520images%2520labeled%2520with%2520specific%2520concepts%250Alike%2520captions%252C%2520we%2520permute%2520a%2520concept%2520across%2520examples%2520in%2520the%2520text%2520space%2520and%2520then%250Agenerate%2520images%2520via%2520a%2520text-conditioned%2520diffusion%2520model.%2520Feature%2520importance%2520is%250Athen%2520reflected%2520by%2520the%2520change%2520in%2520model%2520performance%2520relative%2520to%2520unpermuted%2520data.%250AWhen%2520applied%2520to%2520a%2520set%2520of%2520concepts%252C%2520the%2520method%2520generates%2520a%2520ranking%2520of%2520feature%250Aimportance.%2520We%2520show%2520this%2520approach%2520recovers%2520underlying%2520model%2520feature%2520importance%250Aon%2520synthetic%2520and%2520real-world%2520image%2520classification%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14509v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DEPICT%3A%20Diffusion-Enabled%20Permutation%20Importance%20for%20Image%0A%20%20Classification%20Tasks&entry.906535625=Sarah%20Jabbour%20and%20Gregory%20Kondas%20and%20Ella%20Kazerooni%20and%20Michael%20Sjoding%20and%20David%20Fouhey%20and%20Jenna%20Wiens&entry.1292438233=%20%20We%20propose%20a%20permutation-based%20explanation%20method%20for%20image%20classifiers.%0ACurrent%20image-model%20explanations%20like%20activation%20maps%20are%20limited%20to%0Ainstance-based%20explanations%20in%20the%20pixel%20space%2C%20making%20it%20difficult%20to%0Aunderstand%20global%20model%20behavior.%20In%20contrast%2C%20permutation%20based%20explanations%0Afor%20tabular%20data%20classifiers%20measure%20feature%20importance%20by%20comparing%20model%0Aperformance%20on%20data%20before%20and%20after%20permuting%20a%20feature.%20We%20propose%20an%0Aexplanation%20method%20for%20image-based%20models%20that%20permutes%20interpretable%20concepts%0Aacross%20dataset%20images.%20Given%20a%20dataset%20of%20images%20labeled%20with%20specific%20concepts%0Alike%20captions%2C%20we%20permute%20a%20concept%20across%20examples%20in%20the%20text%20space%20and%20then%0Agenerate%20images%20via%20a%20text-conditioned%20diffusion%20model.%20Feature%20importance%20is%0Athen%20reflected%20by%20the%20change%20in%20model%20performance%20relative%20to%20unpermuted%20data.%0AWhen%20applied%20to%20a%20set%20of%20concepts%2C%20the%20method%20generates%20a%20ranking%20of%20feature%0Aimportance.%20We%20show%20this%20approach%20recovers%20underlying%20model%20feature%20importance%0Aon%20synthetic%20and%20real-world%20image%20classification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14509v1&entry.124074799=Read"},
{"title": "Discovering environments with XRM", "author": "Mohammad Pezeshki and Diane Bouchacourt and Mark Ibrahim and Nicolas Ballas and Pascal Vincent and David Lopez-Paz", "abstract": "  Environment annotations are essential for the success of many\nout-of-distribution (OOD) generalization methods. Unfortunately, these are\ncostly to obtain and often limited by human annotators' biases. To achieve\nrobust generalization, it is essential to develop algorithms for automatic\nenvironment discovery within datasets. Current proposals, which divide examples\nbased on their training error, suffer from one fundamental problem. These\nmethods introduce hyper-parameters and early-stopping criteria, which require a\nvalidation set with human-annotated environments, the very information subject\nto discovery. In this paper, we propose Cross-Risk-Minimization (XRM) to\naddress this issue. XRM trains twin networks, each learning from one random\nhalf of the training data, while imitating confident held-out mistakes made by\nits sibling. XRM provides a recipe for hyper-parameter tuning, does not require\nearly-stopping, and can discover environments for all training and validation\ndata. Algorithms built on top of XRM environments achieve oracle\nworst-group-accuracy, addressing a long-standing challenge in OOD\ngeneralization. Code available at\n\\url{https://github.com/facebookresearch/XRM}.\n", "link": "http://arxiv.org/abs/2309.16748v2", "date": "2024-07-19", "relevancy": 1.4734, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5391}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.486}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discovering%20environments%20with%20XRM&body=Title%3A%20Discovering%20environments%20with%20XRM%0AAuthor%3A%20Mohammad%20Pezeshki%20and%20Diane%20Bouchacourt%20and%20Mark%20Ibrahim%20and%20Nicolas%20Ballas%20and%20Pascal%20Vincent%20and%20David%20Lopez-Paz%0AAbstract%3A%20%20%20Environment%20annotations%20are%20essential%20for%20the%20success%20of%20many%0Aout-of-distribution%20%28OOD%29%20generalization%20methods.%20Unfortunately%2C%20these%20are%0Acostly%20to%20obtain%20and%20often%20limited%20by%20human%20annotators%27%20biases.%20To%20achieve%0Arobust%20generalization%2C%20it%20is%20essential%20to%20develop%20algorithms%20for%20automatic%0Aenvironment%20discovery%20within%20datasets.%20Current%20proposals%2C%20which%20divide%20examples%0Abased%20on%20their%20training%20error%2C%20suffer%20from%20one%20fundamental%20problem.%20These%0Amethods%20introduce%20hyper-parameters%20and%20early-stopping%20criteria%2C%20which%20require%20a%0Avalidation%20set%20with%20human-annotated%20environments%2C%20the%20very%20information%20subject%0Ato%20discovery.%20In%20this%20paper%2C%20we%20propose%20Cross-Risk-Minimization%20%28XRM%29%20to%0Aaddress%20this%20issue.%20XRM%20trains%20twin%20networks%2C%20each%20learning%20from%20one%20random%0Ahalf%20of%20the%20training%20data%2C%20while%20imitating%20confident%20held-out%20mistakes%20made%20by%0Aits%20sibling.%20XRM%20provides%20a%20recipe%20for%20hyper-parameter%20tuning%2C%20does%20not%20require%0Aearly-stopping%2C%20and%20can%20discover%20environments%20for%20all%20training%20and%20validation%0Adata.%20Algorithms%20built%20on%20top%20of%20XRM%20environments%20achieve%20oracle%0Aworst-group-accuracy%2C%20addressing%20a%20long-standing%20challenge%20in%20OOD%0Ageneralization.%20Code%20available%20at%0A%5Curl%7Bhttps%3A//github.com/facebookresearch/XRM%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.16748v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscovering%2520environments%2520with%2520XRM%26entry.906535625%3DMohammad%2520Pezeshki%2520and%2520Diane%2520Bouchacourt%2520and%2520Mark%2520Ibrahim%2520and%2520Nicolas%2520Ballas%2520and%2520Pascal%2520Vincent%2520and%2520David%2520Lopez-Paz%26entry.1292438233%3D%2520%2520Environment%2520annotations%2520are%2520essential%2520for%2520the%2520success%2520of%2520many%250Aout-of-distribution%2520%2528OOD%2529%2520generalization%2520methods.%2520Unfortunately%252C%2520these%2520are%250Acostly%2520to%2520obtain%2520and%2520often%2520limited%2520by%2520human%2520annotators%2527%2520biases.%2520To%2520achieve%250Arobust%2520generalization%252C%2520it%2520is%2520essential%2520to%2520develop%2520algorithms%2520for%2520automatic%250Aenvironment%2520discovery%2520within%2520datasets.%2520Current%2520proposals%252C%2520which%2520divide%2520examples%250Abased%2520on%2520their%2520training%2520error%252C%2520suffer%2520from%2520one%2520fundamental%2520problem.%2520These%250Amethods%2520introduce%2520hyper-parameters%2520and%2520early-stopping%2520criteria%252C%2520which%2520require%2520a%250Avalidation%2520set%2520with%2520human-annotated%2520environments%252C%2520the%2520very%2520information%2520subject%250Ato%2520discovery.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Cross-Risk-Minimization%2520%2528XRM%2529%2520to%250Aaddress%2520this%2520issue.%2520XRM%2520trains%2520twin%2520networks%252C%2520each%2520learning%2520from%2520one%2520random%250Ahalf%2520of%2520the%2520training%2520data%252C%2520while%2520imitating%2520confident%2520held-out%2520mistakes%2520made%2520by%250Aits%2520sibling.%2520XRM%2520provides%2520a%2520recipe%2520for%2520hyper-parameter%2520tuning%252C%2520does%2520not%2520require%250Aearly-stopping%252C%2520and%2520can%2520discover%2520environments%2520for%2520all%2520training%2520and%2520validation%250Adata.%2520Algorithms%2520built%2520on%2520top%2520of%2520XRM%2520environments%2520achieve%2520oracle%250Aworst-group-accuracy%252C%2520addressing%2520a%2520long-standing%2520challenge%2520in%2520OOD%250Ageneralization.%2520Code%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/facebookresearch/XRM%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.16748v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discovering%20environments%20with%20XRM&entry.906535625=Mohammad%20Pezeshki%20and%20Diane%20Bouchacourt%20and%20Mark%20Ibrahim%20and%20Nicolas%20Ballas%20and%20Pascal%20Vincent%20and%20David%20Lopez-Paz&entry.1292438233=%20%20Environment%20annotations%20are%20essential%20for%20the%20success%20of%20many%0Aout-of-distribution%20%28OOD%29%20generalization%20methods.%20Unfortunately%2C%20these%20are%0Acostly%20to%20obtain%20and%20often%20limited%20by%20human%20annotators%27%20biases.%20To%20achieve%0Arobust%20generalization%2C%20it%20is%20essential%20to%20develop%20algorithms%20for%20automatic%0Aenvironment%20discovery%20within%20datasets.%20Current%20proposals%2C%20which%20divide%20examples%0Abased%20on%20their%20training%20error%2C%20suffer%20from%20one%20fundamental%20problem.%20These%0Amethods%20introduce%20hyper-parameters%20and%20early-stopping%20criteria%2C%20which%20require%20a%0Avalidation%20set%20with%20human-annotated%20environments%2C%20the%20very%20information%20subject%0Ato%20discovery.%20In%20this%20paper%2C%20we%20propose%20Cross-Risk-Minimization%20%28XRM%29%20to%0Aaddress%20this%20issue.%20XRM%20trains%20twin%20networks%2C%20each%20learning%20from%20one%20random%0Ahalf%20of%20the%20training%20data%2C%20while%20imitating%20confident%20held-out%20mistakes%20made%20by%0Aits%20sibling.%20XRM%20provides%20a%20recipe%20for%20hyper-parameter%20tuning%2C%20does%20not%20require%0Aearly-stopping%2C%20and%20can%20discover%20environments%20for%20all%20training%20and%20validation%0Adata.%20Algorithms%20built%20on%20top%20of%20XRM%20environments%20achieve%20oracle%0Aworst-group-accuracy%2C%20addressing%20a%20long-standing%20challenge%20in%20OOD%0Ageneralization.%20Code%20available%20at%0A%5Curl%7Bhttps%3A//github.com/facebookresearch/XRM%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.16748v2&entry.124074799=Read"},
{"title": "SHS: Scorpion Hunting Strategy Swarm Algorithm", "author": "Abhilash Singh and Seyed Muhammad Hossein Mousavi and Kumar Gaurav", "abstract": "  We introduced the Scorpion Hunting Strategy (SHS), a novel population-based,\nnature-inspired optimisation algorithm. This algorithm draws inspiration from\nthe hunting strategy of scorpions, which identify, locate, and capture their\nprey using the alpha and beta vibration operators. These operators control the\nSHS algorithm's exploitation and exploration abilities. To formulate an\noptimisation method, we mathematically simulate these dynamic events and\nbehaviors. We evaluate the effectiveness of the SHS algorithm by employing 20\nbenchmark functions (including 10 conventional and 10 CEC2020 functions), using\nboth qualitative and quantitative analyses. Through a comparative analysis with\n12 state-of-the-art meta-heuristic algorithms, we demonstrate that the proposed\nSHS algorithm yields exceptionally promising results. These findings are\nfurther supported by statistically significant results obtained through the\nWilcoxon rank sum test. Additionally, the ranking of SHS, as determined by the\naverage rank derived from the Friedman test, positions it at the forefront when\ncompared to other algorithms. Going beyond theoretical validation, we showcase\nthe practical utility of the SHS algorithm by applying it to six distinct\nreal-world optimisation tasks. These applications illustrate the algorithm's\npotential in addressing complex optimisation challenges. In summary, this work\nnot only introduces the innovative SHS algorithm but also substantiates its\neffectiveness and versatility through rigorous benchmarking and real-world\nproblem-solving scenarios.\n", "link": "http://arxiv.org/abs/2407.14202v1", "date": "2024-07-19", "relevancy": 1.2084, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4277}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4005}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SHS%3A%20Scorpion%20Hunting%20Strategy%20Swarm%20Algorithm&body=Title%3A%20SHS%3A%20Scorpion%20Hunting%20Strategy%20Swarm%20Algorithm%0AAuthor%3A%20Abhilash%20Singh%20and%20Seyed%20Muhammad%20Hossein%20Mousavi%20and%20Kumar%20Gaurav%0AAbstract%3A%20%20%20We%20introduced%20the%20Scorpion%20Hunting%20Strategy%20%28SHS%29%2C%20a%20novel%20population-based%2C%0Anature-inspired%20optimisation%20algorithm.%20This%20algorithm%20draws%20inspiration%20from%0Athe%20hunting%20strategy%20of%20scorpions%2C%20which%20identify%2C%20locate%2C%20and%20capture%20their%0Aprey%20using%20the%20alpha%20and%20beta%20vibration%20operators.%20These%20operators%20control%20the%0ASHS%20algorithm%27s%20exploitation%20and%20exploration%20abilities.%20To%20formulate%20an%0Aoptimisation%20method%2C%20we%20mathematically%20simulate%20these%20dynamic%20events%20and%0Abehaviors.%20We%20evaluate%20the%20effectiveness%20of%20the%20SHS%20algorithm%20by%20employing%2020%0Abenchmark%20functions%20%28including%2010%20conventional%20and%2010%20CEC2020%20functions%29%2C%20using%0Aboth%20qualitative%20and%20quantitative%20analyses.%20Through%20a%20comparative%20analysis%20with%0A12%20state-of-the-art%20meta-heuristic%20algorithms%2C%20we%20demonstrate%20that%20the%20proposed%0ASHS%20algorithm%20yields%20exceptionally%20promising%20results.%20These%20findings%20are%0Afurther%20supported%20by%20statistically%20significant%20results%20obtained%20through%20the%0AWilcoxon%20rank%20sum%20test.%20Additionally%2C%20the%20ranking%20of%20SHS%2C%20as%20determined%20by%20the%0Aaverage%20rank%20derived%20from%20the%20Friedman%20test%2C%20positions%20it%20at%20the%20forefront%20when%0Acompared%20to%20other%20algorithms.%20Going%20beyond%20theoretical%20validation%2C%20we%20showcase%0Athe%20practical%20utility%20of%20the%20SHS%20algorithm%20by%20applying%20it%20to%20six%20distinct%0Areal-world%20optimisation%20tasks.%20These%20applications%20illustrate%20the%20algorithm%27s%0Apotential%20in%20addressing%20complex%20optimisation%20challenges.%20In%20summary%2C%20this%20work%0Anot%20only%20introduces%20the%20innovative%20SHS%20algorithm%20but%20also%20substantiates%20its%0Aeffectiveness%20and%20versatility%20through%20rigorous%20benchmarking%20and%20real-world%0Aproblem-solving%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14202v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSHS%253A%2520Scorpion%2520Hunting%2520Strategy%2520Swarm%2520Algorithm%26entry.906535625%3DAbhilash%2520Singh%2520and%2520Seyed%2520Muhammad%2520Hossein%2520Mousavi%2520and%2520Kumar%2520Gaurav%26entry.1292438233%3D%2520%2520We%2520introduced%2520the%2520Scorpion%2520Hunting%2520Strategy%2520%2528SHS%2529%252C%2520a%2520novel%2520population-based%252C%250Anature-inspired%2520optimisation%2520algorithm.%2520This%2520algorithm%2520draws%2520inspiration%2520from%250Athe%2520hunting%2520strategy%2520of%2520scorpions%252C%2520which%2520identify%252C%2520locate%252C%2520and%2520capture%2520their%250Aprey%2520using%2520the%2520alpha%2520and%2520beta%2520vibration%2520operators.%2520These%2520operators%2520control%2520the%250ASHS%2520algorithm%2527s%2520exploitation%2520and%2520exploration%2520abilities.%2520To%2520formulate%2520an%250Aoptimisation%2520method%252C%2520we%2520mathematically%2520simulate%2520these%2520dynamic%2520events%2520and%250Abehaviors.%2520We%2520evaluate%2520the%2520effectiveness%2520of%2520the%2520SHS%2520algorithm%2520by%2520employing%252020%250Abenchmark%2520functions%2520%2528including%252010%2520conventional%2520and%252010%2520CEC2020%2520functions%2529%252C%2520using%250Aboth%2520qualitative%2520and%2520quantitative%2520analyses.%2520Through%2520a%2520comparative%2520analysis%2520with%250A12%2520state-of-the-art%2520meta-heuristic%2520algorithms%252C%2520we%2520demonstrate%2520that%2520the%2520proposed%250ASHS%2520algorithm%2520yields%2520exceptionally%2520promising%2520results.%2520These%2520findings%2520are%250Afurther%2520supported%2520by%2520statistically%2520significant%2520results%2520obtained%2520through%2520the%250AWilcoxon%2520rank%2520sum%2520test.%2520Additionally%252C%2520the%2520ranking%2520of%2520SHS%252C%2520as%2520determined%2520by%2520the%250Aaverage%2520rank%2520derived%2520from%2520the%2520Friedman%2520test%252C%2520positions%2520it%2520at%2520the%2520forefront%2520when%250Acompared%2520to%2520other%2520algorithms.%2520Going%2520beyond%2520theoretical%2520validation%252C%2520we%2520showcase%250Athe%2520practical%2520utility%2520of%2520the%2520SHS%2520algorithm%2520by%2520applying%2520it%2520to%2520six%2520distinct%250Areal-world%2520optimisation%2520tasks.%2520These%2520applications%2520illustrate%2520the%2520algorithm%2527s%250Apotential%2520in%2520addressing%2520complex%2520optimisation%2520challenges.%2520In%2520summary%252C%2520this%2520work%250Anot%2520only%2520introduces%2520the%2520innovative%2520SHS%2520algorithm%2520but%2520also%2520substantiates%2520its%250Aeffectiveness%2520and%2520versatility%2520through%2520rigorous%2520benchmarking%2520and%2520real-world%250Aproblem-solving%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14202v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SHS%3A%20Scorpion%20Hunting%20Strategy%20Swarm%20Algorithm&entry.906535625=Abhilash%20Singh%20and%20Seyed%20Muhammad%20Hossein%20Mousavi%20and%20Kumar%20Gaurav&entry.1292438233=%20%20We%20introduced%20the%20Scorpion%20Hunting%20Strategy%20%28SHS%29%2C%20a%20novel%20population-based%2C%0Anature-inspired%20optimisation%20algorithm.%20This%20algorithm%20draws%20inspiration%20from%0Athe%20hunting%20strategy%20of%20scorpions%2C%20which%20identify%2C%20locate%2C%20and%20capture%20their%0Aprey%20using%20the%20alpha%20and%20beta%20vibration%20operators.%20These%20operators%20control%20the%0ASHS%20algorithm%27s%20exploitation%20and%20exploration%20abilities.%20To%20formulate%20an%0Aoptimisation%20method%2C%20we%20mathematically%20simulate%20these%20dynamic%20events%20and%0Abehaviors.%20We%20evaluate%20the%20effectiveness%20of%20the%20SHS%20algorithm%20by%20employing%2020%0Abenchmark%20functions%20%28including%2010%20conventional%20and%2010%20CEC2020%20functions%29%2C%20using%0Aboth%20qualitative%20and%20quantitative%20analyses.%20Through%20a%20comparative%20analysis%20with%0A12%20state-of-the-art%20meta-heuristic%20algorithms%2C%20we%20demonstrate%20that%20the%20proposed%0ASHS%20algorithm%20yields%20exceptionally%20promising%20results.%20These%20findings%20are%0Afurther%20supported%20by%20statistically%20significant%20results%20obtained%20through%20the%0AWilcoxon%20rank%20sum%20test.%20Additionally%2C%20the%20ranking%20of%20SHS%2C%20as%20determined%20by%20the%0Aaverage%20rank%20derived%20from%20the%20Friedman%20test%2C%20positions%20it%20at%20the%20forefront%20when%0Acompared%20to%20other%20algorithms.%20Going%20beyond%20theoretical%20validation%2C%20we%20showcase%0Athe%20practical%20utility%20of%20the%20SHS%20algorithm%20by%20applying%20it%20to%20six%20distinct%0Areal-world%20optimisation%20tasks.%20These%20applications%20illustrate%20the%20algorithm%27s%0Apotential%20in%20addressing%20complex%20optimisation%20challenges.%20In%20summary%2C%20this%20work%0Anot%20only%20introduces%20the%20innovative%20SHS%20algorithm%20but%20also%20substantiates%20its%0Aeffectiveness%20and%20versatility%20through%20rigorous%20benchmarking%20and%20real-world%0Aproblem-solving%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14202v1&entry.124074799=Read"},
{"title": "Quantifying the value of positive transfer: An experimental case study", "author": "Aidan J. Hughes and Giulia Delo and Jack Poole and Nikolaos Dervilis and Keith Worden", "abstract": "  In traditional approaches to structural health monitoring, challenges often\narise associated with the availability of labelled data. Population-based\nstructural health monitoring seeks to overcomes these challenges by leveraging\ndata/information from similar structures via technologies such as transfer\nlearning. The current paper demonstrate a methodology for quantifying the value\nof information transfer in the context of operation and maintenance\ndecision-making. This demonstration, based on a population of laboratory-scale\naircraft models, highlights the steps required to evaluate the expected value\nof information transfer including similarity assessment and prediction of\ntransfer efficacy. Once evaluated for a given population, the value of\ninformation transfer can be used to optimise transfer-learning strategies for\nnewly-acquired target domains.\n", "link": "http://arxiv.org/abs/2407.14342v1", "date": "2024-07-19", "relevancy": 1.3893, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4924}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4738}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantifying%20the%20value%20of%20positive%20transfer%3A%20An%20experimental%20case%20study&body=Title%3A%20Quantifying%20the%20value%20of%20positive%20transfer%3A%20An%20experimental%20case%20study%0AAuthor%3A%20Aidan%20J.%20Hughes%20and%20Giulia%20Delo%20and%20Jack%20Poole%20and%20Nikolaos%20Dervilis%20and%20Keith%20Worden%0AAbstract%3A%20%20%20In%20traditional%20approaches%20to%20structural%20health%20monitoring%2C%20challenges%20often%0Aarise%20associated%20with%20the%20availability%20of%20labelled%20data.%20Population-based%0Astructural%20health%20monitoring%20seeks%20to%20overcomes%20these%20challenges%20by%20leveraging%0Adata/information%20from%20similar%20structures%20via%20technologies%20such%20as%20transfer%0Alearning.%20The%20current%20paper%20demonstrate%20a%20methodology%20for%20quantifying%20the%20value%0Aof%20information%20transfer%20in%20the%20context%20of%20operation%20and%20maintenance%0Adecision-making.%20This%20demonstration%2C%20based%20on%20a%20population%20of%20laboratory-scale%0Aaircraft%20models%2C%20highlights%20the%20steps%20required%20to%20evaluate%20the%20expected%20value%0Aof%20information%20transfer%20including%20similarity%20assessment%20and%20prediction%20of%0Atransfer%20efficacy.%20Once%20evaluated%20for%20a%20given%20population%2C%20the%20value%20of%0Ainformation%20transfer%20can%20be%20used%20to%20optimise%20transfer-learning%20strategies%20for%0Anewly-acquired%20target%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14342v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantifying%2520the%2520value%2520of%2520positive%2520transfer%253A%2520An%2520experimental%2520case%2520study%26entry.906535625%3DAidan%2520J.%2520Hughes%2520and%2520Giulia%2520Delo%2520and%2520Jack%2520Poole%2520and%2520Nikolaos%2520Dervilis%2520and%2520Keith%2520Worden%26entry.1292438233%3D%2520%2520In%2520traditional%2520approaches%2520to%2520structural%2520health%2520monitoring%252C%2520challenges%2520often%250Aarise%2520associated%2520with%2520the%2520availability%2520of%2520labelled%2520data.%2520Population-based%250Astructural%2520health%2520monitoring%2520seeks%2520to%2520overcomes%2520these%2520challenges%2520by%2520leveraging%250Adata/information%2520from%2520similar%2520structures%2520via%2520technologies%2520such%2520as%2520transfer%250Alearning.%2520The%2520current%2520paper%2520demonstrate%2520a%2520methodology%2520for%2520quantifying%2520the%2520value%250Aof%2520information%2520transfer%2520in%2520the%2520context%2520of%2520operation%2520and%2520maintenance%250Adecision-making.%2520This%2520demonstration%252C%2520based%2520on%2520a%2520population%2520of%2520laboratory-scale%250Aaircraft%2520models%252C%2520highlights%2520the%2520steps%2520required%2520to%2520evaluate%2520the%2520expected%2520value%250Aof%2520information%2520transfer%2520including%2520similarity%2520assessment%2520and%2520prediction%2520of%250Atransfer%2520efficacy.%2520Once%2520evaluated%2520for%2520a%2520given%2520population%252C%2520the%2520value%2520of%250Ainformation%2520transfer%2520can%2520be%2520used%2520to%2520optimise%2520transfer-learning%2520strategies%2520for%250Anewly-acquired%2520target%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14342v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20the%20value%20of%20positive%20transfer%3A%20An%20experimental%20case%20study&entry.906535625=Aidan%20J.%20Hughes%20and%20Giulia%20Delo%20and%20Jack%20Poole%20and%20Nikolaos%20Dervilis%20and%20Keith%20Worden&entry.1292438233=%20%20In%20traditional%20approaches%20to%20structural%20health%20monitoring%2C%20challenges%20often%0Aarise%20associated%20with%20the%20availability%20of%20labelled%20data.%20Population-based%0Astructural%20health%20monitoring%20seeks%20to%20overcomes%20these%20challenges%20by%20leveraging%0Adata/information%20from%20similar%20structures%20via%20technologies%20such%20as%20transfer%0Alearning.%20The%20current%20paper%20demonstrate%20a%20methodology%20for%20quantifying%20the%20value%0Aof%20information%20transfer%20in%20the%20context%20of%20operation%20and%20maintenance%0Adecision-making.%20This%20demonstration%2C%20based%20on%20a%20population%20of%20laboratory-scale%0Aaircraft%20models%2C%20highlights%20the%20steps%20required%20to%20evaluate%20the%20expected%20value%0Aof%20information%20transfer%20including%20similarity%20assessment%20and%20prediction%20of%0Atransfer%20efficacy.%20Once%20evaluated%20for%20a%20given%20population%2C%20the%20value%20of%0Ainformation%20transfer%20can%20be%20used%20to%20optimise%20transfer-learning%20strategies%20for%0Anewly-acquired%20target%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14342v1&entry.124074799=Read"},
{"title": "Do LLMs have Consistent Values?", "author": "Naama Rozen and Gal Elidan and Amir Globerson and Ella Daniel", "abstract": "  Values are a basic driving force underlying human behavior. Large Language\nModels (LLM) technology is constantly improving towards human-like dialogue.\nHowever, little research has been done to study the values exhibited in text\ngenerated by LLMs. Here we study this question by turning to the rich\nliterature on value structure in psychology. We ask whether LLMs exhibit the\nsame value structure that has been demonstrated in humans, including the\nranking of values, and correlation between values. We show that the results of\nthis analysis strongly depend on how the LLM is prompted, and that under a\nparticular prompting strategy (referred to as 'Value Anchoring') the agreement\nwith human data is quite compelling. Our results serve both to improve our\nunderstanding of values in LLMs, as well as introduce novel methods for\nassessing consistency in LLM responses.\n", "link": "http://arxiv.org/abs/2407.12878v2", "date": "2024-07-19", "relevancy": 1.4583, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3689}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3641}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20LLMs%20have%20Consistent%20Values%3F&body=Title%3A%20Do%20LLMs%20have%20Consistent%20Values%3F%0AAuthor%3A%20Naama%20Rozen%20and%20Gal%20Elidan%20and%20Amir%20Globerson%20and%20Ella%20Daniel%0AAbstract%3A%20%20%20Values%20are%20a%20basic%20driving%20force%20underlying%20human%20behavior.%20Large%20Language%0AModels%20%28LLM%29%20technology%20is%20constantly%20improving%20towards%20human-like%20dialogue.%0AHowever%2C%20little%20research%20has%20been%20done%20to%20study%20the%20values%20exhibited%20in%20text%0Agenerated%20by%20LLMs.%20Here%20we%20study%20this%20question%20by%20turning%20to%20the%20rich%0Aliterature%20on%20value%20structure%20in%20psychology.%20We%20ask%20whether%20LLMs%20exhibit%20the%0Asame%20value%20structure%20that%20has%20been%20demonstrated%20in%20humans%2C%20including%20the%0Aranking%20of%20values%2C%20and%20correlation%20between%20values.%20We%20show%20that%20the%20results%20of%0Athis%20analysis%20strongly%20depend%20on%20how%20the%20LLM%20is%20prompted%2C%20and%20that%20under%20a%0Aparticular%20prompting%20strategy%20%28referred%20to%20as%20%27Value%20Anchoring%27%29%20the%20agreement%0Awith%20human%20data%20is%20quite%20compelling.%20Our%20results%20serve%20both%20to%20improve%20our%0Aunderstanding%20of%20values%20in%20LLMs%2C%20as%20well%20as%20introduce%20novel%20methods%20for%0Aassessing%20consistency%20in%20LLM%20responses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12878v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520LLMs%2520have%2520Consistent%2520Values%253F%26entry.906535625%3DNaama%2520Rozen%2520and%2520Gal%2520Elidan%2520and%2520Amir%2520Globerson%2520and%2520Ella%2520Daniel%26entry.1292438233%3D%2520%2520Values%2520are%2520a%2520basic%2520driving%2520force%2520underlying%2520human%2520behavior.%2520Large%2520Language%250AModels%2520%2528LLM%2529%2520technology%2520is%2520constantly%2520improving%2520towards%2520human-like%2520dialogue.%250AHowever%252C%2520little%2520research%2520has%2520been%2520done%2520to%2520study%2520the%2520values%2520exhibited%2520in%2520text%250Agenerated%2520by%2520LLMs.%2520Here%2520we%2520study%2520this%2520question%2520by%2520turning%2520to%2520the%2520rich%250Aliterature%2520on%2520value%2520structure%2520in%2520psychology.%2520We%2520ask%2520whether%2520LLMs%2520exhibit%2520the%250Asame%2520value%2520structure%2520that%2520has%2520been%2520demonstrated%2520in%2520humans%252C%2520including%2520the%250Aranking%2520of%2520values%252C%2520and%2520correlation%2520between%2520values.%2520We%2520show%2520that%2520the%2520results%2520of%250Athis%2520analysis%2520strongly%2520depend%2520on%2520how%2520the%2520LLM%2520is%2520prompted%252C%2520and%2520that%2520under%2520a%250Aparticular%2520prompting%2520strategy%2520%2528referred%2520to%2520as%2520%2527Value%2520Anchoring%2527%2529%2520the%2520agreement%250Awith%2520human%2520data%2520is%2520quite%2520compelling.%2520Our%2520results%2520serve%2520both%2520to%2520improve%2520our%250Aunderstanding%2520of%2520values%2520in%2520LLMs%252C%2520as%2520well%2520as%2520introduce%2520novel%2520methods%2520for%250Aassessing%2520consistency%2520in%2520LLM%2520responses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12878v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20LLMs%20have%20Consistent%20Values%3F&entry.906535625=Naama%20Rozen%20and%20Gal%20Elidan%20and%20Amir%20Globerson%20and%20Ella%20Daniel&entry.1292438233=%20%20Values%20are%20a%20basic%20driving%20force%20underlying%20human%20behavior.%20Large%20Language%0AModels%20%28LLM%29%20technology%20is%20constantly%20improving%20towards%20human-like%20dialogue.%0AHowever%2C%20little%20research%20has%20been%20done%20to%20study%20the%20values%20exhibited%20in%20text%0Agenerated%20by%20LLMs.%20Here%20we%20study%20this%20question%20by%20turning%20to%20the%20rich%0Aliterature%20on%20value%20structure%20in%20psychology.%20We%20ask%20whether%20LLMs%20exhibit%20the%0Asame%20value%20structure%20that%20has%20been%20demonstrated%20in%20humans%2C%20including%20the%0Aranking%20of%20values%2C%20and%20correlation%20between%20values.%20We%20show%20that%20the%20results%20of%0Athis%20analysis%20strongly%20depend%20on%20how%20the%20LLM%20is%20prompted%2C%20and%20that%20under%20a%0Aparticular%20prompting%20strategy%20%28referred%20to%20as%20%27Value%20Anchoring%27%29%20the%20agreement%0Awith%20human%20data%20is%20quite%20compelling.%20Our%20results%20serve%20both%20to%20improve%20our%0Aunderstanding%20of%20values%20in%20LLMs%2C%20as%20well%20as%20introduce%20novel%20methods%20for%0Aassessing%20consistency%20in%20LLM%20responses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12878v2&entry.124074799=Read"},
{"title": "FuzzTheREST: An Intelligent Automated Black-box RESTful API Fuzzer", "author": "Tiago Dias and Eva Maia and Isabel Pra\u00e7a", "abstract": "  Software's pervasive impact and increasing reliance in the era of digital\ntransformation raise concerns about vulnerabilities, emphasizing the need for\nsoftware security. Fuzzy testing is a dynamic analysis software testing\ntechnique that consists of feeding faulty input data to a System Under Test\n(SUT) and observing its behavior. Specifically regarding black-box RESTful API\ntesting, recent literature has attempted to automate this technique using\nheuristics to perform the input search and using the HTTP response status codes\nfor classification. However, most approaches do not keep track of code\ncoverage, which is important to validate the solution. This work introduces a\nblack-box RESTful API fuzzy testing tool that employs Reinforcement Learning\n(RL) for vulnerability detection. The fuzzer operates via the OpenAPI\nSpecification (OAS) file and a scenarios file, which includes information to\ncommunicate with the SUT and the sequences of functionalities to test,\nrespectively. To evaluate its effectiveness, the tool was tested on the\nPetstore API. The tool found a total of six unique vulnerabilities and achieved\n55\\% code coverage.\n", "link": "http://arxiv.org/abs/2407.14361v1", "date": "2024-07-19", "relevancy": 1.1582, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3931}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3846}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.37}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FuzzTheREST%3A%20An%20Intelligent%20Automated%20Black-box%20RESTful%20API%20Fuzzer&body=Title%3A%20FuzzTheREST%3A%20An%20Intelligent%20Automated%20Black-box%20RESTful%20API%20Fuzzer%0AAuthor%3A%20Tiago%20Dias%20and%20Eva%20Maia%20and%20Isabel%20Pra%C3%A7a%0AAbstract%3A%20%20%20Software%27s%20pervasive%20impact%20and%20increasing%20reliance%20in%20the%20era%20of%20digital%0Atransformation%20raise%20concerns%20about%20vulnerabilities%2C%20emphasizing%20the%20need%20for%0Asoftware%20security.%20Fuzzy%20testing%20is%20a%20dynamic%20analysis%20software%20testing%0Atechnique%20that%20consists%20of%20feeding%20faulty%20input%20data%20to%20a%20System%20Under%20Test%0A%28SUT%29%20and%20observing%20its%20behavior.%20Specifically%20regarding%20black-box%20RESTful%20API%0Atesting%2C%20recent%20literature%20has%20attempted%20to%20automate%20this%20technique%20using%0Aheuristics%20to%20perform%20the%20input%20search%20and%20using%20the%20HTTP%20response%20status%20codes%0Afor%20classification.%20However%2C%20most%20approaches%20do%20not%20keep%20track%20of%20code%0Acoverage%2C%20which%20is%20important%20to%20validate%20the%20solution.%20This%20work%20introduces%20a%0Ablack-box%20RESTful%20API%20fuzzy%20testing%20tool%20that%20employs%20Reinforcement%20Learning%0A%28RL%29%20for%20vulnerability%20detection.%20The%20fuzzer%20operates%20via%20the%20OpenAPI%0ASpecification%20%28OAS%29%20file%20and%20a%20scenarios%20file%2C%20which%20includes%20information%20to%0Acommunicate%20with%20the%20SUT%20and%20the%20sequences%20of%20functionalities%20to%20test%2C%0Arespectively.%20To%20evaluate%20its%20effectiveness%2C%20the%20tool%20was%20tested%20on%20the%0APetstore%20API.%20The%20tool%20found%20a%20total%20of%20six%20unique%20vulnerabilities%20and%20achieved%0A55%5C%25%20code%20coverage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14361v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFuzzTheREST%253A%2520An%2520Intelligent%2520Automated%2520Black-box%2520RESTful%2520API%2520Fuzzer%26entry.906535625%3DTiago%2520Dias%2520and%2520Eva%2520Maia%2520and%2520Isabel%2520Pra%25C3%25A7a%26entry.1292438233%3D%2520%2520Software%2527s%2520pervasive%2520impact%2520and%2520increasing%2520reliance%2520in%2520the%2520era%2520of%2520digital%250Atransformation%2520raise%2520concerns%2520about%2520vulnerabilities%252C%2520emphasizing%2520the%2520need%2520for%250Asoftware%2520security.%2520Fuzzy%2520testing%2520is%2520a%2520dynamic%2520analysis%2520software%2520testing%250Atechnique%2520that%2520consists%2520of%2520feeding%2520faulty%2520input%2520data%2520to%2520a%2520System%2520Under%2520Test%250A%2528SUT%2529%2520and%2520observing%2520its%2520behavior.%2520Specifically%2520regarding%2520black-box%2520RESTful%2520API%250Atesting%252C%2520recent%2520literature%2520has%2520attempted%2520to%2520automate%2520this%2520technique%2520using%250Aheuristics%2520to%2520perform%2520the%2520input%2520search%2520and%2520using%2520the%2520HTTP%2520response%2520status%2520codes%250Afor%2520classification.%2520However%252C%2520most%2520approaches%2520do%2520not%2520keep%2520track%2520of%2520code%250Acoverage%252C%2520which%2520is%2520important%2520to%2520validate%2520the%2520solution.%2520This%2520work%2520introduces%2520a%250Ablack-box%2520RESTful%2520API%2520fuzzy%2520testing%2520tool%2520that%2520employs%2520Reinforcement%2520Learning%250A%2528RL%2529%2520for%2520vulnerability%2520detection.%2520The%2520fuzzer%2520operates%2520via%2520the%2520OpenAPI%250ASpecification%2520%2528OAS%2529%2520file%2520and%2520a%2520scenarios%2520file%252C%2520which%2520includes%2520information%2520to%250Acommunicate%2520with%2520the%2520SUT%2520and%2520the%2520sequences%2520of%2520functionalities%2520to%2520test%252C%250Arespectively.%2520To%2520evaluate%2520its%2520effectiveness%252C%2520the%2520tool%2520was%2520tested%2520on%2520the%250APetstore%2520API.%2520The%2520tool%2520found%2520a%2520total%2520of%2520six%2520unique%2520vulnerabilities%2520and%2520achieved%250A55%255C%2525%2520code%2520coverage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14361v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FuzzTheREST%3A%20An%20Intelligent%20Automated%20Black-box%20RESTful%20API%20Fuzzer&entry.906535625=Tiago%20Dias%20and%20Eva%20Maia%20and%20Isabel%20Pra%C3%A7a&entry.1292438233=%20%20Software%27s%20pervasive%20impact%20and%20increasing%20reliance%20in%20the%20era%20of%20digital%0Atransformation%20raise%20concerns%20about%20vulnerabilities%2C%20emphasizing%20the%20need%20for%0Asoftware%20security.%20Fuzzy%20testing%20is%20a%20dynamic%20analysis%20software%20testing%0Atechnique%20that%20consists%20of%20feeding%20faulty%20input%20data%20to%20a%20System%20Under%20Test%0A%28SUT%29%20and%20observing%20its%20behavior.%20Specifically%20regarding%20black-box%20RESTful%20API%0Atesting%2C%20recent%20literature%20has%20attempted%20to%20automate%20this%20technique%20using%0Aheuristics%20to%20perform%20the%20input%20search%20and%20using%20the%20HTTP%20response%20status%20codes%0Afor%20classification.%20However%2C%20most%20approaches%20do%20not%20keep%20track%20of%20code%0Acoverage%2C%20which%20is%20important%20to%20validate%20the%20solution.%20This%20work%20introduces%20a%0Ablack-box%20RESTful%20API%20fuzzy%20testing%20tool%20that%20employs%20Reinforcement%20Learning%0A%28RL%29%20for%20vulnerability%20detection.%20The%20fuzzer%20operates%20via%20the%20OpenAPI%0ASpecification%20%28OAS%29%20file%20and%20a%20scenarios%20file%2C%20which%20includes%20information%20to%0Acommunicate%20with%20the%20SUT%20and%20the%20sequences%20of%20functionalities%20to%20test%2C%0Arespectively.%20To%20evaluate%20its%20effectiveness%2C%20the%20tool%20was%20tested%20on%20the%0APetstore%20API.%20The%20tool%20found%20a%20total%20of%20six%20unique%20vulnerabilities%20and%20achieved%0A55%5C%25%20code%20coverage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14361v1&entry.124074799=Read"},
{"title": "Domain Adaptation for Industrial Time-series Forecasting via\n  Counterfactual Inference", "author": "Chao Min and Guoquan Wen and Jiangru Yuan and Jun Yi and Xing Guo", "abstract": "  Industrial time-series, as a structural data responds to production process\ninformation, can be utilized to perform data-driven decision-making for\neffective monitoring of industrial production process. However, there are some\nchallenges for time-series forecasting in industry, e.g., predicting few-shot\ncaused by data shortage, and decision-confusing caused by unknown treatment\npolicy. To cope with the problems, we propose a novel causal domain adaptation\nframework, Causal Domain Adaptation (CDA) forecaster to improve the performance\non the interested domain with limited data (target). Firstly, we analyze the\ncausality existing along with treatments, and thus ensure the shared causality\nover time. Subsequently, we propose an answer-based attention mechanism to\nachieve domain-invariant representation by the shared causality in both\ndomains. Then, a novel domain-adaptation is built to model treatments and\noutcomes jointly training on source and target domain. The main insights are\nthat our designed answer-based attention mechanism allows the target domain to\nleverage the existed causality in source time-series even with different\ntreatments, and our forecaster can predict the counterfactual outcome of\nindustrial time-series, meaning a guidance in production process. Compared with\ncommonly baselines, our method on real-world and synthetic oilfield datasets\ndemonstrates the effectiveness in across-domain prediction and the practicality\nin guiding production process\n", "link": "http://arxiv.org/abs/2407.14214v1", "date": "2024-07-19", "relevancy": 1.342, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.462}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4574}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain%20Adaptation%20for%20Industrial%20Time-series%20Forecasting%20via%0A%20%20Counterfactual%20Inference&body=Title%3A%20Domain%20Adaptation%20for%20Industrial%20Time-series%20Forecasting%20via%0A%20%20Counterfactual%20Inference%0AAuthor%3A%20Chao%20Min%20and%20Guoquan%20Wen%20and%20Jiangru%20Yuan%20and%20Jun%20Yi%20and%20Xing%20Guo%0AAbstract%3A%20%20%20Industrial%20time-series%2C%20as%20a%20structural%20data%20responds%20to%20production%20process%0Ainformation%2C%20can%20be%20utilized%20to%20perform%20data-driven%20decision-making%20for%0Aeffective%20monitoring%20of%20industrial%20production%20process.%20However%2C%20there%20are%20some%0Achallenges%20for%20time-series%20forecasting%20in%20industry%2C%20e.g.%2C%20predicting%20few-shot%0Acaused%20by%20data%20shortage%2C%20and%20decision-confusing%20caused%20by%20unknown%20treatment%0Apolicy.%20To%20cope%20with%20the%20problems%2C%20we%20propose%20a%20novel%20causal%20domain%20adaptation%0Aframework%2C%20Causal%20Domain%20Adaptation%20%28CDA%29%20forecaster%20to%20improve%20the%20performance%0Aon%20the%20interested%20domain%20with%20limited%20data%20%28target%29.%20Firstly%2C%20we%20analyze%20the%0Acausality%20existing%20along%20with%20treatments%2C%20and%20thus%20ensure%20the%20shared%20causality%0Aover%20time.%20Subsequently%2C%20we%20propose%20an%20answer-based%20attention%20mechanism%20to%0Aachieve%20domain-invariant%20representation%20by%20the%20shared%20causality%20in%20both%0Adomains.%20Then%2C%20a%20novel%20domain-adaptation%20is%20built%20to%20model%20treatments%20and%0Aoutcomes%20jointly%20training%20on%20source%20and%20target%20domain.%20The%20main%20insights%20are%0Athat%20our%20designed%20answer-based%20attention%20mechanism%20allows%20the%20target%20domain%20to%0Aleverage%20the%20existed%20causality%20in%20source%20time-series%20even%20with%20different%0Atreatments%2C%20and%20our%20forecaster%20can%20predict%20the%20counterfactual%20outcome%20of%0Aindustrial%20time-series%2C%20meaning%20a%20guidance%20in%20production%20process.%20Compared%20with%0Acommonly%20baselines%2C%20our%20method%20on%20real-world%20and%20synthetic%20oilfield%20datasets%0Ademonstrates%20the%20effectiveness%20in%20across-domain%20prediction%20and%20the%20practicality%0Ain%20guiding%20production%20process%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14214v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain%2520Adaptation%2520for%2520Industrial%2520Time-series%2520Forecasting%2520via%250A%2520%2520Counterfactual%2520Inference%26entry.906535625%3DChao%2520Min%2520and%2520Guoquan%2520Wen%2520and%2520Jiangru%2520Yuan%2520and%2520Jun%2520Yi%2520and%2520Xing%2520Guo%26entry.1292438233%3D%2520%2520Industrial%2520time-series%252C%2520as%2520a%2520structural%2520data%2520responds%2520to%2520production%2520process%250Ainformation%252C%2520can%2520be%2520utilized%2520to%2520perform%2520data-driven%2520decision-making%2520for%250Aeffective%2520monitoring%2520of%2520industrial%2520production%2520process.%2520However%252C%2520there%2520are%2520some%250Achallenges%2520for%2520time-series%2520forecasting%2520in%2520industry%252C%2520e.g.%252C%2520predicting%2520few-shot%250Acaused%2520by%2520data%2520shortage%252C%2520and%2520decision-confusing%2520caused%2520by%2520unknown%2520treatment%250Apolicy.%2520To%2520cope%2520with%2520the%2520problems%252C%2520we%2520propose%2520a%2520novel%2520causal%2520domain%2520adaptation%250Aframework%252C%2520Causal%2520Domain%2520Adaptation%2520%2528CDA%2529%2520forecaster%2520to%2520improve%2520the%2520performance%250Aon%2520the%2520interested%2520domain%2520with%2520limited%2520data%2520%2528target%2529.%2520Firstly%252C%2520we%2520analyze%2520the%250Acausality%2520existing%2520along%2520with%2520treatments%252C%2520and%2520thus%2520ensure%2520the%2520shared%2520causality%250Aover%2520time.%2520Subsequently%252C%2520we%2520propose%2520an%2520answer-based%2520attention%2520mechanism%2520to%250Aachieve%2520domain-invariant%2520representation%2520by%2520the%2520shared%2520causality%2520in%2520both%250Adomains.%2520Then%252C%2520a%2520novel%2520domain-adaptation%2520is%2520built%2520to%2520model%2520treatments%2520and%250Aoutcomes%2520jointly%2520training%2520on%2520source%2520and%2520target%2520domain.%2520The%2520main%2520insights%2520are%250Athat%2520our%2520designed%2520answer-based%2520attention%2520mechanism%2520allows%2520the%2520target%2520domain%2520to%250Aleverage%2520the%2520existed%2520causality%2520in%2520source%2520time-series%2520even%2520with%2520different%250Atreatments%252C%2520and%2520our%2520forecaster%2520can%2520predict%2520the%2520counterfactual%2520outcome%2520of%250Aindustrial%2520time-series%252C%2520meaning%2520a%2520guidance%2520in%2520production%2520process.%2520Compared%2520with%250Acommonly%2520baselines%252C%2520our%2520method%2520on%2520real-world%2520and%2520synthetic%2520oilfield%2520datasets%250Ademonstrates%2520the%2520effectiveness%2520in%2520across-domain%2520prediction%2520and%2520the%2520practicality%250Ain%2520guiding%2520production%2520process%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14214v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain%20Adaptation%20for%20Industrial%20Time-series%20Forecasting%20via%0A%20%20Counterfactual%20Inference&entry.906535625=Chao%20Min%20and%20Guoquan%20Wen%20and%20Jiangru%20Yuan%20and%20Jun%20Yi%20and%20Xing%20Guo&entry.1292438233=%20%20Industrial%20time-series%2C%20as%20a%20structural%20data%20responds%20to%20production%20process%0Ainformation%2C%20can%20be%20utilized%20to%20perform%20data-driven%20decision-making%20for%0Aeffective%20monitoring%20of%20industrial%20production%20process.%20However%2C%20there%20are%20some%0Achallenges%20for%20time-series%20forecasting%20in%20industry%2C%20e.g.%2C%20predicting%20few-shot%0Acaused%20by%20data%20shortage%2C%20and%20decision-confusing%20caused%20by%20unknown%20treatment%0Apolicy.%20To%20cope%20with%20the%20problems%2C%20we%20propose%20a%20novel%20causal%20domain%20adaptation%0Aframework%2C%20Causal%20Domain%20Adaptation%20%28CDA%29%20forecaster%20to%20improve%20the%20performance%0Aon%20the%20interested%20domain%20with%20limited%20data%20%28target%29.%20Firstly%2C%20we%20analyze%20the%0Acausality%20existing%20along%20with%20treatments%2C%20and%20thus%20ensure%20the%20shared%20causality%0Aover%20time.%20Subsequently%2C%20we%20propose%20an%20answer-based%20attention%20mechanism%20to%0Aachieve%20domain-invariant%20representation%20by%20the%20shared%20causality%20in%20both%0Adomains.%20Then%2C%20a%20novel%20domain-adaptation%20is%20built%20to%20model%20treatments%20and%0Aoutcomes%20jointly%20training%20on%20source%20and%20target%20domain.%20The%20main%20insights%20are%0Athat%20our%20designed%20answer-based%20attention%20mechanism%20allows%20the%20target%20domain%20to%0Aleverage%20the%20existed%20causality%20in%20source%20time-series%20even%20with%20different%0Atreatments%2C%20and%20our%20forecaster%20can%20predict%20the%20counterfactual%20outcome%20of%0Aindustrial%20time-series%2C%20meaning%20a%20guidance%20in%20production%20process.%20Compared%20with%0Acommonly%20baselines%2C%20our%20method%20on%20real-world%20and%20synthetic%20oilfield%20datasets%0Ademonstrates%20the%20effectiveness%20in%20across-domain%20prediction%20and%20the%20practicality%0Ain%20guiding%20production%20process%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14214v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


