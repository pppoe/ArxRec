<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250206.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GaussRender: Learning 3D Occupancy with Gaussian Rendering", "author": "Loick Chambon and Eloi Zablocki and Alexandre Boulch and Mickael Chen and Matthieu Cord", "abstract": "  Understanding the 3D geometry and semantics of driving scenes is critical for\ndeveloping of safe autonomous vehicles. While 3D occupancy models are typically\ntrained using voxel-based supervision with standard losses (e.g.,\ncross-entropy, Lovasz, dice), these approaches treat voxel predictions\nindependently, neglecting their spatial relationships. In this paper, we\npropose GaussRender, a plug-and-play 3D-to-2D reprojection loss that enhances\nvoxel-based supervision. Our method projects 3D voxel representations into\narbitrary 2D perspectives and leverages Gaussian splatting as an efficient,\ndifferentiable rendering proxy of voxels, introducing spatial dependencies\nacross projected elements. This approach improves semantic and geometric\nconsistency, handles occlusions more efficiently, and requires no architectural\nmodifications. Extensive experiments on multiple benchmarks\n(SurroundOcc-nuScenes, Occ3D-nuScenes, SSCBench-KITTI360) demonstrate\nconsistent performance gains across various 3D occupancy models (TPVFormer,\nSurroundOcc, Symphonies), highlighting the robustness and versatility of our\nframework. The code is available at https://github.com/valeoai/GaussRender.\n", "link": "http://arxiv.org/abs/2502.05040v1", "date": "2025-02-07", "relevancy": 3.3008, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6751}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6676}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussRender%3A%20Learning%203D%20Occupancy%20with%20Gaussian%20Rendering&body=Title%3A%20GaussRender%3A%20Learning%203D%20Occupancy%20with%20Gaussian%20Rendering%0AAuthor%3A%20Loick%20Chambon%20and%20Eloi%20Zablocki%20and%20Alexandre%20Boulch%20and%20Mickael%20Chen%20and%20Matthieu%20Cord%0AAbstract%3A%20%20%20Understanding%20the%203D%20geometry%20and%20semantics%20of%20driving%20scenes%20is%20critical%20for%0Adeveloping%20of%20safe%20autonomous%20vehicles.%20While%203D%20occupancy%20models%20are%20typically%0Atrained%20using%20voxel-based%20supervision%20with%20standard%20losses%20%28e.g.%2C%0Across-entropy%2C%20Lovasz%2C%20dice%29%2C%20these%20approaches%20treat%20voxel%20predictions%0Aindependently%2C%20neglecting%20their%20spatial%20relationships.%20In%20this%20paper%2C%20we%0Apropose%20GaussRender%2C%20a%20plug-and-play%203D-to-2D%20reprojection%20loss%20that%20enhances%0Avoxel-based%20supervision.%20Our%20method%20projects%203D%20voxel%20representations%20into%0Aarbitrary%202D%20perspectives%20and%20leverages%20Gaussian%20splatting%20as%20an%20efficient%2C%0Adifferentiable%20rendering%20proxy%20of%20voxels%2C%20introducing%20spatial%20dependencies%0Aacross%20projected%20elements.%20This%20approach%20improves%20semantic%20and%20geometric%0Aconsistency%2C%20handles%20occlusions%20more%20efficiently%2C%20and%20requires%20no%20architectural%0Amodifications.%20Extensive%20experiments%20on%20multiple%20benchmarks%0A%28SurroundOcc-nuScenes%2C%20Occ3D-nuScenes%2C%20SSCBench-KITTI360%29%20demonstrate%0Aconsistent%20performance%20gains%20across%20various%203D%20occupancy%20models%20%28TPVFormer%2C%0ASurroundOcc%2C%20Symphonies%29%2C%20highlighting%20the%20robustness%20and%20versatility%20of%20our%0Aframework.%20The%20code%20is%20available%20at%20https%3A//github.com/valeoai/GaussRender.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05040v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussRender%253A%2520Learning%25203D%2520Occupancy%2520with%2520Gaussian%2520Rendering%26entry.906535625%3DLoick%2520Chambon%2520and%2520Eloi%2520Zablocki%2520and%2520Alexandre%2520Boulch%2520and%2520Mickael%2520Chen%2520and%2520Matthieu%2520Cord%26entry.1292438233%3D%2520%2520Understanding%2520the%25203D%2520geometry%2520and%2520semantics%2520of%2520driving%2520scenes%2520is%2520critical%2520for%250Adeveloping%2520of%2520safe%2520autonomous%2520vehicles.%2520While%25203D%2520occupancy%2520models%2520are%2520typically%250Atrained%2520using%2520voxel-based%2520supervision%2520with%2520standard%2520losses%2520%2528e.g.%252C%250Across-entropy%252C%2520Lovasz%252C%2520dice%2529%252C%2520these%2520approaches%2520treat%2520voxel%2520predictions%250Aindependently%252C%2520neglecting%2520their%2520spatial%2520relationships.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520GaussRender%252C%2520a%2520plug-and-play%25203D-to-2D%2520reprojection%2520loss%2520that%2520enhances%250Avoxel-based%2520supervision.%2520Our%2520method%2520projects%25203D%2520voxel%2520representations%2520into%250Aarbitrary%25202D%2520perspectives%2520and%2520leverages%2520Gaussian%2520splatting%2520as%2520an%2520efficient%252C%250Adifferentiable%2520rendering%2520proxy%2520of%2520voxels%252C%2520introducing%2520spatial%2520dependencies%250Aacross%2520projected%2520elements.%2520This%2520approach%2520improves%2520semantic%2520and%2520geometric%250Aconsistency%252C%2520handles%2520occlusions%2520more%2520efficiently%252C%2520and%2520requires%2520no%2520architectural%250Amodifications.%2520Extensive%2520experiments%2520on%2520multiple%2520benchmarks%250A%2528SurroundOcc-nuScenes%252C%2520Occ3D-nuScenes%252C%2520SSCBench-KITTI360%2529%2520demonstrate%250Aconsistent%2520performance%2520gains%2520across%2520various%25203D%2520occupancy%2520models%2520%2528TPVFormer%252C%250ASurroundOcc%252C%2520Symphonies%2529%252C%2520highlighting%2520the%2520robustness%2520and%2520versatility%2520of%2520our%250Aframework.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/valeoai/GaussRender.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05040v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussRender%3A%20Learning%203D%20Occupancy%20with%20Gaussian%20Rendering&entry.906535625=Loick%20Chambon%20and%20Eloi%20Zablocki%20and%20Alexandre%20Boulch%20and%20Mickael%20Chen%20and%20Matthieu%20Cord&entry.1292438233=%20%20Understanding%20the%203D%20geometry%20and%20semantics%20of%20driving%20scenes%20is%20critical%20for%0Adeveloping%20of%20safe%20autonomous%20vehicles.%20While%203D%20occupancy%20models%20are%20typically%0Atrained%20using%20voxel-based%20supervision%20with%20standard%20losses%20%28e.g.%2C%0Across-entropy%2C%20Lovasz%2C%20dice%29%2C%20these%20approaches%20treat%20voxel%20predictions%0Aindependently%2C%20neglecting%20their%20spatial%20relationships.%20In%20this%20paper%2C%20we%0Apropose%20GaussRender%2C%20a%20plug-and-play%203D-to-2D%20reprojection%20loss%20that%20enhances%0Avoxel-based%20supervision.%20Our%20method%20projects%203D%20voxel%20representations%20into%0Aarbitrary%202D%20perspectives%20and%20leverages%20Gaussian%20splatting%20as%20an%20efficient%2C%0Adifferentiable%20rendering%20proxy%20of%20voxels%2C%20introducing%20spatial%20dependencies%0Aacross%20projected%20elements.%20This%20approach%20improves%20semantic%20and%20geometric%0Aconsistency%2C%20handles%20occlusions%20more%20efficiently%2C%20and%20requires%20no%20architectural%0Amodifications.%20Extensive%20experiments%20on%20multiple%20benchmarks%0A%28SurroundOcc-nuScenes%2C%20Occ3D-nuScenes%2C%20SSCBench-KITTI360%29%20demonstrate%0Aconsistent%20performance%20gains%20across%20various%203D%20occupancy%20models%20%28TPVFormer%2C%0ASurroundOcc%2C%20Symphonies%29%2C%20highlighting%20the%20robustness%20and%20versatility%20of%20our%0Aframework.%20The%20code%20is%20available%20at%20https%3A//github.com/valeoai/GaussRender.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05040v1&entry.124074799=Read"},
{"title": "OccGS: Zero-shot 3D Occupancy Reconstruction with Semantic and\n  Geometric-Aware Gaussian Splatting", "author": "Xiaoyu Zhou and Jingqi Wang and Yongtao Wang and Yufei Wei and Nan Dong and Ming-Hsuan Yang", "abstract": "  Obtaining semantic 3D occupancy from raw sensor data without manual\nannotations remains an essential yet challenging task. While prior works have\napproached this as a perception prediction problem, we formulate it as\nscene-aware 3D occupancy reconstruction with geometry and semantics. In this\nwork, we propose OccGS, a novel 3D Occupancy reconstruction framework utilizing\nSemantic and Geometric-Aware Gaussian Splatting in a zero-shot manner.\nLeveraging semantics extracted from vision-language models and geometry guided\nby LiDAR points, OccGS constructs Semantic and Geometric-Aware Gaussians from\nraw multisensor data. We also develop a cumulative Gaussian-to-3D voxel\nsplatting method for reconstructing occupancy from the Gaussians. OccGS\nperforms favorably against self-supervised methods in occupancy prediction,\nachieving comparable performance to fully supervised approaches and achieving\nstate-of-the-art performance on zero-shot semantic 3D occupancy estimation.\n", "link": "http://arxiv.org/abs/2502.04981v1", "date": "2025-02-07", "relevancy": 3.2829, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6927}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6626}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OccGS%3A%20Zero-shot%203D%20Occupancy%20Reconstruction%20with%20Semantic%20and%0A%20%20Geometric-Aware%20Gaussian%20Splatting&body=Title%3A%20OccGS%3A%20Zero-shot%203D%20Occupancy%20Reconstruction%20with%20Semantic%20and%0A%20%20Geometric-Aware%20Gaussian%20Splatting%0AAuthor%3A%20Xiaoyu%20Zhou%20and%20Jingqi%20Wang%20and%20Yongtao%20Wang%20and%20Yufei%20Wei%20and%20Nan%20Dong%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20%20%20Obtaining%20semantic%203D%20occupancy%20from%20raw%20sensor%20data%20without%20manual%0Aannotations%20remains%20an%20essential%20yet%20challenging%20task.%20While%20prior%20works%20have%0Aapproached%20this%20as%20a%20perception%20prediction%20problem%2C%20we%20formulate%20it%20as%0Ascene-aware%203D%20occupancy%20reconstruction%20with%20geometry%20and%20semantics.%20In%20this%0Awork%2C%20we%20propose%20OccGS%2C%20a%20novel%203D%20Occupancy%20reconstruction%20framework%20utilizing%0ASemantic%20and%20Geometric-Aware%20Gaussian%20Splatting%20in%20a%20zero-shot%20manner.%0ALeveraging%20semantics%20extracted%20from%20vision-language%20models%20and%20geometry%20guided%0Aby%20LiDAR%20points%2C%20OccGS%20constructs%20Semantic%20and%20Geometric-Aware%20Gaussians%20from%0Araw%20multisensor%20data.%20We%20also%20develop%20a%20cumulative%20Gaussian-to-3D%20voxel%0Asplatting%20method%20for%20reconstructing%20occupancy%20from%20the%20Gaussians.%20OccGS%0Aperforms%20favorably%20against%20self-supervised%20methods%20in%20occupancy%20prediction%2C%0Aachieving%20comparable%20performance%20to%20fully%20supervised%20approaches%20and%20achieving%0Astate-of-the-art%20performance%20on%20zero-shot%20semantic%203D%20occupancy%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOccGS%253A%2520Zero-shot%25203D%2520Occupancy%2520Reconstruction%2520with%2520Semantic%2520and%250A%2520%2520Geometric-Aware%2520Gaussian%2520Splatting%26entry.906535625%3DXiaoyu%2520Zhou%2520and%2520Jingqi%2520Wang%2520and%2520Yongtao%2520Wang%2520and%2520Yufei%2520Wei%2520and%2520Nan%2520Dong%2520and%2520Ming-Hsuan%2520Yang%26entry.1292438233%3D%2520%2520Obtaining%2520semantic%25203D%2520occupancy%2520from%2520raw%2520sensor%2520data%2520without%2520manual%250Aannotations%2520remains%2520an%2520essential%2520yet%2520challenging%2520task.%2520While%2520prior%2520works%2520have%250Aapproached%2520this%2520as%2520a%2520perception%2520prediction%2520problem%252C%2520we%2520formulate%2520it%2520as%250Ascene-aware%25203D%2520occupancy%2520reconstruction%2520with%2520geometry%2520and%2520semantics.%2520In%2520this%250Awork%252C%2520we%2520propose%2520OccGS%252C%2520a%2520novel%25203D%2520Occupancy%2520reconstruction%2520framework%2520utilizing%250ASemantic%2520and%2520Geometric-Aware%2520Gaussian%2520Splatting%2520in%2520a%2520zero-shot%2520manner.%250ALeveraging%2520semantics%2520extracted%2520from%2520vision-language%2520models%2520and%2520geometry%2520guided%250Aby%2520LiDAR%2520points%252C%2520OccGS%2520constructs%2520Semantic%2520and%2520Geometric-Aware%2520Gaussians%2520from%250Araw%2520multisensor%2520data.%2520We%2520also%2520develop%2520a%2520cumulative%2520Gaussian-to-3D%2520voxel%250Asplatting%2520method%2520for%2520reconstructing%2520occupancy%2520from%2520the%2520Gaussians.%2520OccGS%250Aperforms%2520favorably%2520against%2520self-supervised%2520methods%2520in%2520occupancy%2520prediction%252C%250Aachieving%2520comparable%2520performance%2520to%2520fully%2520supervised%2520approaches%2520and%2520achieving%250Astate-of-the-art%2520performance%2520on%2520zero-shot%2520semantic%25203D%2520occupancy%2520estimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OccGS%3A%20Zero-shot%203D%20Occupancy%20Reconstruction%20with%20Semantic%20and%0A%20%20Geometric-Aware%20Gaussian%20Splatting&entry.906535625=Xiaoyu%20Zhou%20and%20Jingqi%20Wang%20and%20Yongtao%20Wang%20and%20Yufei%20Wei%20and%20Nan%20Dong%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%20Obtaining%20semantic%203D%20occupancy%20from%20raw%20sensor%20data%20without%20manual%0Aannotations%20remains%20an%20essential%20yet%20challenging%20task.%20While%20prior%20works%20have%0Aapproached%20this%20as%20a%20perception%20prediction%20problem%2C%20we%20formulate%20it%20as%0Ascene-aware%203D%20occupancy%20reconstruction%20with%20geometry%20and%20semantics.%20In%20this%0Awork%2C%20we%20propose%20OccGS%2C%20a%20novel%203D%20Occupancy%20reconstruction%20framework%20utilizing%0ASemantic%20and%20Geometric-Aware%20Gaussian%20Splatting%20in%20a%20zero-shot%20manner.%0ALeveraging%20semantics%20extracted%20from%20vision-language%20models%20and%20geometry%20guided%0Aby%20LiDAR%20points%2C%20OccGS%20constructs%20Semantic%20and%20Geometric-Aware%20Gaussians%20from%0Araw%20multisensor%20data.%20We%20also%20develop%20a%20cumulative%20Gaussian-to-3D%20voxel%0Asplatting%20method%20for%20reconstructing%20occupancy%20from%20the%20Gaussians.%20OccGS%0Aperforms%20favorably%20against%20self-supervised%20methods%20in%20occupancy%20prediction%2C%0Aachieving%20comparable%20performance%20to%20fully%20supervised%20approaches%20and%20achieving%0Astate-of-the-art%20performance%20on%20zero-shot%20semantic%203D%20occupancy%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04981v1&entry.124074799=Read"},
{"title": "Drag Your Gaussian: Effective Drag-Based Editing with Score Distillation\n  for 3D Gaussian Splatting", "author": "Yansong Qu and Dian Chen and Xinyang Li and Xiaofan Li and Shengchuan Zhang and Liujuan Cao and Rongrong Ji", "abstract": "  Recent advancements in 3D scene editing have been propelled by the rapid\ndevelopment of generative models. Existing methods typically utilize generative\nmodels to perform text-guided editing on 3D representations, such as 3D\nGaussian Splatting (3DGS). However, these methods are often limited to texture\nmodifications and fail when addressing geometric changes, such as editing a\ncharacter's head to turn around. Moreover, such methods lack accurate control\nover the spatial position of editing results, as language struggles to\nprecisely describe the extent of edits. To overcome these limitations, we\nintroduce DYG, an effective 3D drag-based editing method for 3D Gaussian\nSplatting. It enables users to conveniently specify the desired editing region\nand the desired dragging direction through the input of 3D masks and pairs of\ncontrol points, thereby enabling precise control over the extent of editing.\nDYG integrates the strengths of the implicit triplane representation to\nestablish the geometric scaffold of the editing results, effectively overcoming\nsuboptimal editing outcomes caused by the sparsity of 3DGS in the desired\nediting regions. Additionally, we incorporate a drag-based Latent Diffusion\nModel into our method through the proposed Drag-SDS loss function, enabling\nflexible, multi-view consistent, and fine-grained editing. Extensive\nexperiments demonstrate that DYG conducts effective drag-based editing guided\nby control point prompts, surpassing other baselines in terms of editing effect\nand quality, both qualitatively and quantitatively. Visit our project page at\nhttps://quyans.github.io/Drag-Your-Gaussian.\n", "link": "http://arxiv.org/abs/2501.18672v2", "date": "2025-02-07", "relevancy": 3.2497, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6515}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.65}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Drag%20Your%20Gaussian%3A%20Effective%20Drag-Based%20Editing%20with%20Score%20Distillation%0A%20%20for%203D%20Gaussian%20Splatting&body=Title%3A%20Drag%20Your%20Gaussian%3A%20Effective%20Drag-Based%20Editing%20with%20Score%20Distillation%0A%20%20for%203D%20Gaussian%20Splatting%0AAuthor%3A%20Yansong%20Qu%20and%20Dian%20Chen%20and%20Xinyang%20Li%20and%20Xiaofan%20Li%20and%20Shengchuan%20Zhang%20and%20Liujuan%20Cao%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20Recent%20advancements%20in%203D%20scene%20editing%20have%20been%20propelled%20by%20the%20rapid%0Adevelopment%20of%20generative%20models.%20Existing%20methods%20typically%20utilize%20generative%0Amodels%20to%20perform%20text-guided%20editing%20on%203D%20representations%2C%20such%20as%203D%0AGaussian%20Splatting%20%283DGS%29.%20However%2C%20these%20methods%20are%20often%20limited%20to%20texture%0Amodifications%20and%20fail%20when%20addressing%20geometric%20changes%2C%20such%20as%20editing%20a%0Acharacter%27s%20head%20to%20turn%20around.%20Moreover%2C%20such%20methods%20lack%20accurate%20control%0Aover%20the%20spatial%20position%20of%20editing%20results%2C%20as%20language%20struggles%20to%0Aprecisely%20describe%20the%20extent%20of%20edits.%20To%20overcome%20these%20limitations%2C%20we%0Aintroduce%20DYG%2C%20an%20effective%203D%20drag-based%20editing%20method%20for%203D%20Gaussian%0ASplatting.%20It%20enables%20users%20to%20conveniently%20specify%20the%20desired%20editing%20region%0Aand%20the%20desired%20dragging%20direction%20through%20the%20input%20of%203D%20masks%20and%20pairs%20of%0Acontrol%20points%2C%20thereby%20enabling%20precise%20control%20over%20the%20extent%20of%20editing.%0ADYG%20integrates%20the%20strengths%20of%20the%20implicit%20triplane%20representation%20to%0Aestablish%20the%20geometric%20scaffold%20of%20the%20editing%20results%2C%20effectively%20overcoming%0Asuboptimal%20editing%20outcomes%20caused%20by%20the%20sparsity%20of%203DGS%20in%20the%20desired%0Aediting%20regions.%20Additionally%2C%20we%20incorporate%20a%20drag-based%20Latent%20Diffusion%0AModel%20into%20our%20method%20through%20the%20proposed%20Drag-SDS%20loss%20function%2C%20enabling%0Aflexible%2C%20multi-view%20consistent%2C%20and%20fine-grained%20editing.%20Extensive%0Aexperiments%20demonstrate%20that%20DYG%20conducts%20effective%20drag-based%20editing%20guided%0Aby%20control%20point%20prompts%2C%20surpassing%20other%20baselines%20in%20terms%20of%20editing%20effect%0Aand%20quality%2C%20both%20qualitatively%20and%20quantitatively.%20Visit%20our%20project%20page%20at%0Ahttps%3A//quyans.github.io/Drag-Your-Gaussian.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18672v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDrag%2520Your%2520Gaussian%253A%2520Effective%2520Drag-Based%2520Editing%2520with%2520Score%2520Distillation%250A%2520%2520for%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DYansong%2520Qu%2520and%2520Dian%2520Chen%2520and%2520Xinyang%2520Li%2520and%2520Xiaofan%2520Li%2520and%2520Shengchuan%2520Zhang%2520and%2520Liujuan%2520Cao%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%25203D%2520scene%2520editing%2520have%2520been%2520propelled%2520by%2520the%2520rapid%250Adevelopment%2520of%2520generative%2520models.%2520Existing%2520methods%2520typically%2520utilize%2520generative%250Amodels%2520to%2520perform%2520text-guided%2520editing%2520on%25203D%2520representations%252C%2520such%2520as%25203D%250AGaussian%2520Splatting%2520%25283DGS%2529.%2520However%252C%2520these%2520methods%2520are%2520often%2520limited%2520to%2520texture%250Amodifications%2520and%2520fail%2520when%2520addressing%2520geometric%2520changes%252C%2520such%2520as%2520editing%2520a%250Acharacter%2527s%2520head%2520to%2520turn%2520around.%2520Moreover%252C%2520such%2520methods%2520lack%2520accurate%2520control%250Aover%2520the%2520spatial%2520position%2520of%2520editing%2520results%252C%2520as%2520language%2520struggles%2520to%250Aprecisely%2520describe%2520the%2520extent%2520of%2520edits.%2520To%2520overcome%2520these%2520limitations%252C%2520we%250Aintroduce%2520DYG%252C%2520an%2520effective%25203D%2520drag-based%2520editing%2520method%2520for%25203D%2520Gaussian%250ASplatting.%2520It%2520enables%2520users%2520to%2520conveniently%2520specify%2520the%2520desired%2520editing%2520region%250Aand%2520the%2520desired%2520dragging%2520direction%2520through%2520the%2520input%2520of%25203D%2520masks%2520and%2520pairs%2520of%250Acontrol%2520points%252C%2520thereby%2520enabling%2520precise%2520control%2520over%2520the%2520extent%2520of%2520editing.%250ADYG%2520integrates%2520the%2520strengths%2520of%2520the%2520implicit%2520triplane%2520representation%2520to%250Aestablish%2520the%2520geometric%2520scaffold%2520of%2520the%2520editing%2520results%252C%2520effectively%2520overcoming%250Asuboptimal%2520editing%2520outcomes%2520caused%2520by%2520the%2520sparsity%2520of%25203DGS%2520in%2520the%2520desired%250Aediting%2520regions.%2520Additionally%252C%2520we%2520incorporate%2520a%2520drag-based%2520Latent%2520Diffusion%250AModel%2520into%2520our%2520method%2520through%2520the%2520proposed%2520Drag-SDS%2520loss%2520function%252C%2520enabling%250Aflexible%252C%2520multi-view%2520consistent%252C%2520and%2520fine-grained%2520editing.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520DYG%2520conducts%2520effective%2520drag-based%2520editing%2520guided%250Aby%2520control%2520point%2520prompts%252C%2520surpassing%2520other%2520baselines%2520in%2520terms%2520of%2520editing%2520effect%250Aand%2520quality%252C%2520both%2520qualitatively%2520and%2520quantitatively.%2520Visit%2520our%2520project%2520page%2520at%250Ahttps%253A//quyans.github.io/Drag-Your-Gaussian.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18672v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Drag%20Your%20Gaussian%3A%20Effective%20Drag-Based%20Editing%20with%20Score%20Distillation%0A%20%20for%203D%20Gaussian%20Splatting&entry.906535625=Yansong%20Qu%20and%20Dian%20Chen%20and%20Xinyang%20Li%20and%20Xiaofan%20Li%20and%20Shengchuan%20Zhang%20and%20Liujuan%20Cao%20and%20Rongrong%20Ji&entry.1292438233=%20%20Recent%20advancements%20in%203D%20scene%20editing%20have%20been%20propelled%20by%20the%20rapid%0Adevelopment%20of%20generative%20models.%20Existing%20methods%20typically%20utilize%20generative%0Amodels%20to%20perform%20text-guided%20editing%20on%203D%20representations%2C%20such%20as%203D%0AGaussian%20Splatting%20%283DGS%29.%20However%2C%20these%20methods%20are%20often%20limited%20to%20texture%0Amodifications%20and%20fail%20when%20addressing%20geometric%20changes%2C%20such%20as%20editing%20a%0Acharacter%27s%20head%20to%20turn%20around.%20Moreover%2C%20such%20methods%20lack%20accurate%20control%0Aover%20the%20spatial%20position%20of%20editing%20results%2C%20as%20language%20struggles%20to%0Aprecisely%20describe%20the%20extent%20of%20edits.%20To%20overcome%20these%20limitations%2C%20we%0Aintroduce%20DYG%2C%20an%20effective%203D%20drag-based%20editing%20method%20for%203D%20Gaussian%0ASplatting.%20It%20enables%20users%20to%20conveniently%20specify%20the%20desired%20editing%20region%0Aand%20the%20desired%20dragging%20direction%20through%20the%20input%20of%203D%20masks%20and%20pairs%20of%0Acontrol%20points%2C%20thereby%20enabling%20precise%20control%20over%20the%20extent%20of%20editing.%0ADYG%20integrates%20the%20strengths%20of%20the%20implicit%20triplane%20representation%20to%0Aestablish%20the%20geometric%20scaffold%20of%20the%20editing%20results%2C%20effectively%20overcoming%0Asuboptimal%20editing%20outcomes%20caused%20by%20the%20sparsity%20of%203DGS%20in%20the%20desired%0Aediting%20regions.%20Additionally%2C%20we%20incorporate%20a%20drag-based%20Latent%20Diffusion%0AModel%20into%20our%20method%20through%20the%20proposed%20Drag-SDS%20loss%20function%2C%20enabling%0Aflexible%2C%20multi-view%20consistent%2C%20and%20fine-grained%20editing.%20Extensive%0Aexperiments%20demonstrate%20that%20DYG%20conducts%20effective%20drag-based%20editing%20guided%0Aby%20control%20point%20prompts%2C%20surpassing%20other%20baselines%20in%20terms%20of%20editing%20effect%0Aand%20quality%2C%20both%20qualitatively%20and%20quantitatively.%20Visit%20our%20project%20page%20at%0Ahttps%3A//quyans.github.io/Drag-Your-Gaussian.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18672v2&entry.124074799=Read"},
{"title": "HAC++: Towards 100X Compression of 3D Gaussian Splatting", "author": "Yihang Chen and Qianyi Wu and Weiyao Lin and Mehrtash Harandi and Jianfei Cai", "abstract": "  3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel\nview synthesis, boasting rapid rendering speed with high fidelity. However, the\nsubstantial Gaussians and their associated attributes necessitate effective\ncompression techniques. Nevertheless, the sparse and unorganized nature of the\npoint cloud of Gaussians (or anchors in our paper) presents challenges for\ncompression. To achieve a compact size, we propose HAC++, which leverages the\nrelationships between unorganized anchors and a structured hash grid, utilizing\ntheir mutual information for context modeling. Additionally, HAC++ captures\nintra-anchor contextual relationships to further enhance compression\nperformance. To facilitate entropy coding, we utilize Gaussian distributions to\nprecisely estimate the probability of each quantized attribute, where an\nadaptive quantization module is proposed to enable high-precision quantization\nof these attributes for improved fidelity restoration. Moreover, we incorporate\nan adaptive masking strategy to eliminate invalid Gaussians and anchors.\nOverall, HAC++ achieves a remarkable size reduction of over 100X compared to\nvanilla 3DGS when averaged on all datasets, while simultaneously improving\nfidelity. It also delivers more than 20X size reduction compared to\nScaffold-GS. Our code is available at\nhttps://github.com/YihangChen-ee/HAC-plus.\n", "link": "http://arxiv.org/abs/2501.12255v3", "date": "2025-02-07", "relevancy": 3.2301, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6576}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6541}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HAC%2B%2B%3A%20Towards%20100X%20Compression%20of%203D%20Gaussian%20Splatting&body=Title%3A%20HAC%2B%2B%3A%20Towards%20100X%20Compression%20of%203D%20Gaussian%20Splatting%0AAuthor%3A%20Yihang%20Chen%20and%20Qianyi%20Wu%20and%20Weiyao%20Lin%20and%20Mehrtash%20Harandi%20and%20Jianfei%20Cai%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20promising%20framework%20for%20novel%0Aview%20synthesis%2C%20boasting%20rapid%20rendering%20speed%20with%20high%20fidelity.%20However%2C%20the%0Asubstantial%20Gaussians%20and%20their%20associated%20attributes%20necessitate%20effective%0Acompression%20techniques.%20Nevertheless%2C%20the%20sparse%20and%20unorganized%20nature%20of%20the%0Apoint%20cloud%20of%20Gaussians%20%28or%20anchors%20in%20our%20paper%29%20presents%20challenges%20for%0Acompression.%20To%20achieve%20a%20compact%20size%2C%20we%20propose%20HAC%2B%2B%2C%20which%20leverages%20the%0Arelationships%20between%20unorganized%20anchors%20and%20a%20structured%20hash%20grid%2C%20utilizing%0Atheir%20mutual%20information%20for%20context%20modeling.%20Additionally%2C%20HAC%2B%2B%20captures%0Aintra-anchor%20contextual%20relationships%20to%20further%20enhance%20compression%0Aperformance.%20To%20facilitate%20entropy%20coding%2C%20we%20utilize%20Gaussian%20distributions%20to%0Aprecisely%20estimate%20the%20probability%20of%20each%20quantized%20attribute%2C%20where%20an%0Aadaptive%20quantization%20module%20is%20proposed%20to%20enable%20high-precision%20quantization%0Aof%20these%20attributes%20for%20improved%20fidelity%20restoration.%20Moreover%2C%20we%20incorporate%0Aan%20adaptive%20masking%20strategy%20to%20eliminate%20invalid%20Gaussians%20and%20anchors.%0AOverall%2C%20HAC%2B%2B%20achieves%20a%20remarkable%20size%20reduction%20of%20over%20100X%20compared%20to%0Avanilla%203DGS%20when%20averaged%20on%20all%20datasets%2C%20while%20simultaneously%20improving%0Afidelity.%20It%20also%20delivers%20more%20than%2020X%20size%20reduction%20compared%20to%0AScaffold-GS.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/YihangChen-ee/HAC-plus.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12255v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHAC%252B%252B%253A%2520Towards%2520100X%2520Compression%2520of%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DYihang%2520Chen%2520and%2520Qianyi%2520Wu%2520and%2520Weiyao%2520Lin%2520and%2520Mehrtash%2520Harandi%2520and%2520Jianfei%2520Cai%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520emerged%2520as%2520a%2520promising%2520framework%2520for%2520novel%250Aview%2520synthesis%252C%2520boasting%2520rapid%2520rendering%2520speed%2520with%2520high%2520fidelity.%2520However%252C%2520the%250Asubstantial%2520Gaussians%2520and%2520their%2520associated%2520attributes%2520necessitate%2520effective%250Acompression%2520techniques.%2520Nevertheless%252C%2520the%2520sparse%2520and%2520unorganized%2520nature%2520of%2520the%250Apoint%2520cloud%2520of%2520Gaussians%2520%2528or%2520anchors%2520in%2520our%2520paper%2529%2520presents%2520challenges%2520for%250Acompression.%2520To%2520achieve%2520a%2520compact%2520size%252C%2520we%2520propose%2520HAC%252B%252B%252C%2520which%2520leverages%2520the%250Arelationships%2520between%2520unorganized%2520anchors%2520and%2520a%2520structured%2520hash%2520grid%252C%2520utilizing%250Atheir%2520mutual%2520information%2520for%2520context%2520modeling.%2520Additionally%252C%2520HAC%252B%252B%2520captures%250Aintra-anchor%2520contextual%2520relationships%2520to%2520further%2520enhance%2520compression%250Aperformance.%2520To%2520facilitate%2520entropy%2520coding%252C%2520we%2520utilize%2520Gaussian%2520distributions%2520to%250Aprecisely%2520estimate%2520the%2520probability%2520of%2520each%2520quantized%2520attribute%252C%2520where%2520an%250Aadaptive%2520quantization%2520module%2520is%2520proposed%2520to%2520enable%2520high-precision%2520quantization%250Aof%2520these%2520attributes%2520for%2520improved%2520fidelity%2520restoration.%2520Moreover%252C%2520we%2520incorporate%250Aan%2520adaptive%2520masking%2520strategy%2520to%2520eliminate%2520invalid%2520Gaussians%2520and%2520anchors.%250AOverall%252C%2520HAC%252B%252B%2520achieves%2520a%2520remarkable%2520size%2520reduction%2520of%2520over%2520100X%2520compared%2520to%250Avanilla%25203DGS%2520when%2520averaged%2520on%2520all%2520datasets%252C%2520while%2520simultaneously%2520improving%250Afidelity.%2520It%2520also%2520delivers%2520more%2520than%252020X%2520size%2520reduction%2520compared%2520to%250AScaffold-GS.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/YihangChen-ee/HAC-plus.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12255v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HAC%2B%2B%3A%20Towards%20100X%20Compression%20of%203D%20Gaussian%20Splatting&entry.906535625=Yihang%20Chen%20and%20Qianyi%20Wu%20and%20Weiyao%20Lin%20and%20Mehrtash%20Harandi%20and%20Jianfei%20Cai&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20promising%20framework%20for%20novel%0Aview%20synthesis%2C%20boasting%20rapid%20rendering%20speed%20with%20high%20fidelity.%20However%2C%20the%0Asubstantial%20Gaussians%20and%20their%20associated%20attributes%20necessitate%20effective%0Acompression%20techniques.%20Nevertheless%2C%20the%20sparse%20and%20unorganized%20nature%20of%20the%0Apoint%20cloud%20of%20Gaussians%20%28or%20anchors%20in%20our%20paper%29%20presents%20challenges%20for%0Acompression.%20To%20achieve%20a%20compact%20size%2C%20we%20propose%20HAC%2B%2B%2C%20which%20leverages%20the%0Arelationships%20between%20unorganized%20anchors%20and%20a%20structured%20hash%20grid%2C%20utilizing%0Atheir%20mutual%20information%20for%20context%20modeling.%20Additionally%2C%20HAC%2B%2B%20captures%0Aintra-anchor%20contextual%20relationships%20to%20further%20enhance%20compression%0Aperformance.%20To%20facilitate%20entropy%20coding%2C%20we%20utilize%20Gaussian%20distributions%20to%0Aprecisely%20estimate%20the%20probability%20of%20each%20quantized%20attribute%2C%20where%20an%0Aadaptive%20quantization%20module%20is%20proposed%20to%20enable%20high-precision%20quantization%0Aof%20these%20attributes%20for%20improved%20fidelity%20restoration.%20Moreover%2C%20we%20incorporate%0Aan%20adaptive%20masking%20strategy%20to%20eliminate%20invalid%20Gaussians%20and%20anchors.%0AOverall%2C%20HAC%2B%2B%20achieves%20a%20remarkable%20size%20reduction%20of%20over%20100X%20compared%20to%0Avanilla%203DGS%20when%20averaged%20on%20all%20datasets%2C%20while%20simultaneously%20improving%0Afidelity.%20It%20also%20delivers%20more%20than%2020X%20size%20reduction%20compared%20to%0AScaffold-GS.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/YihangChen-ee/HAC-plus.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12255v3&entry.124074799=Read"},
{"title": "DCFormer: Efficient 3D Vision-Language Modeling with Decomposed\n  Convolutions", "author": "Gorkem Can Ates and Kuang Gong and Wei Shao", "abstract": "  Vision-language models (VLMs) align visual and textual representations,\nenabling high-performance zero-shot classification and image-text retrieval in\n2D medical imaging. However, extending VLMs to 3D medical imaging remains\ncomputationally challenging. Existing 3D VLMs rely on Vision Transformers\n(ViTs), which are computationally expensive due to self-attention's quadratic\ncomplexity, or 3D convolutions, which demand excessive parameters and FLOPs as\nkernel size increases. We introduce DCFormer, an efficient 3D medical image\nencoder that factorizes 3D convolutions into three parallel 1D convolutions\nalong depth, height, and width. This design preserves spatial information while\nsignificantly reducing computational cost. Integrated into a CLIP-based\nvision-language framework, DCFormer is evaluated on CT-RATE, a dataset of\n50,188 paired 3D chest CT volumes and radiology reports, for zero-shot\nmulti-abnormality detection across 18 pathologies. Compared to ViT, ConvNeXt,\nPoolFormer, and TransUNet, DCFormer achieves superior efficiency and accuracy,\nwith DCFormer-Tiny reaching 62.0% accuracy and a 46.3% F1-score while using\nsignificantly fewer parameters. These results highlight DCFormer's potential\nfor scalable, clinically deployable 3D medical VLMs. Our codes will be publicly\navailable.\n", "link": "http://arxiv.org/abs/2502.05091v1", "date": "2025-02-07", "relevancy": 3.0832, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6279}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6279}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DCFormer%3A%20Efficient%203D%20Vision-Language%20Modeling%20with%20Decomposed%0A%20%20Convolutions&body=Title%3A%20DCFormer%3A%20Efficient%203D%20Vision-Language%20Modeling%20with%20Decomposed%0A%20%20Convolutions%0AAuthor%3A%20Gorkem%20Can%20Ates%20and%20Kuang%20Gong%20and%20Wei%20Shao%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20align%20visual%20and%20textual%20representations%2C%0Aenabling%20high-performance%20zero-shot%20classification%20and%20image-text%20retrieval%20in%0A2D%20medical%20imaging.%20However%2C%20extending%20VLMs%20to%203D%20medical%20imaging%20remains%0Acomputationally%20challenging.%20Existing%203D%20VLMs%20rely%20on%20Vision%20Transformers%0A%28ViTs%29%2C%20which%20are%20computationally%20expensive%20due%20to%20self-attention%27s%20quadratic%0Acomplexity%2C%20or%203D%20convolutions%2C%20which%20demand%20excessive%20parameters%20and%20FLOPs%20as%0Akernel%20size%20increases.%20We%20introduce%20DCFormer%2C%20an%20efficient%203D%20medical%20image%0Aencoder%20that%20factorizes%203D%20convolutions%20into%20three%20parallel%201D%20convolutions%0Aalong%20depth%2C%20height%2C%20and%20width.%20This%20design%20preserves%20spatial%20information%20while%0Asignificantly%20reducing%20computational%20cost.%20Integrated%20into%20a%20CLIP-based%0Avision-language%20framework%2C%20DCFormer%20is%20evaluated%20on%20CT-RATE%2C%20a%20dataset%20of%0A50%2C188%20paired%203D%20chest%20CT%20volumes%20and%20radiology%20reports%2C%20for%20zero-shot%0Amulti-abnormality%20detection%20across%2018%20pathologies.%20Compared%20to%20ViT%2C%20ConvNeXt%2C%0APoolFormer%2C%20and%20TransUNet%2C%20DCFormer%20achieves%20superior%20efficiency%20and%20accuracy%2C%0Awith%20DCFormer-Tiny%20reaching%2062.0%25%20accuracy%20and%20a%2046.3%25%20F1-score%20while%20using%0Asignificantly%20fewer%20parameters.%20These%20results%20highlight%20DCFormer%27s%20potential%0Afor%20scalable%2C%20clinically%20deployable%203D%20medical%20VLMs.%20Our%20codes%20will%20be%20publicly%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05091v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDCFormer%253A%2520Efficient%25203D%2520Vision-Language%2520Modeling%2520with%2520Decomposed%250A%2520%2520Convolutions%26entry.906535625%3DGorkem%2520Can%2520Ates%2520and%2520Kuang%2520Gong%2520and%2520Wei%2520Shao%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520align%2520visual%2520and%2520textual%2520representations%252C%250Aenabling%2520high-performance%2520zero-shot%2520classification%2520and%2520image-text%2520retrieval%2520in%250A2D%2520medical%2520imaging.%2520However%252C%2520extending%2520VLMs%2520to%25203D%2520medical%2520imaging%2520remains%250Acomputationally%2520challenging.%2520Existing%25203D%2520VLMs%2520rely%2520on%2520Vision%2520Transformers%250A%2528ViTs%2529%252C%2520which%2520are%2520computationally%2520expensive%2520due%2520to%2520self-attention%2527s%2520quadratic%250Acomplexity%252C%2520or%25203D%2520convolutions%252C%2520which%2520demand%2520excessive%2520parameters%2520and%2520FLOPs%2520as%250Akernel%2520size%2520increases.%2520We%2520introduce%2520DCFormer%252C%2520an%2520efficient%25203D%2520medical%2520image%250Aencoder%2520that%2520factorizes%25203D%2520convolutions%2520into%2520three%2520parallel%25201D%2520convolutions%250Aalong%2520depth%252C%2520height%252C%2520and%2520width.%2520This%2520design%2520preserves%2520spatial%2520information%2520while%250Asignificantly%2520reducing%2520computational%2520cost.%2520Integrated%2520into%2520a%2520CLIP-based%250Avision-language%2520framework%252C%2520DCFormer%2520is%2520evaluated%2520on%2520CT-RATE%252C%2520a%2520dataset%2520of%250A50%252C188%2520paired%25203D%2520chest%2520CT%2520volumes%2520and%2520radiology%2520reports%252C%2520for%2520zero-shot%250Amulti-abnormality%2520detection%2520across%252018%2520pathologies.%2520Compared%2520to%2520ViT%252C%2520ConvNeXt%252C%250APoolFormer%252C%2520and%2520TransUNet%252C%2520DCFormer%2520achieves%2520superior%2520efficiency%2520and%2520accuracy%252C%250Awith%2520DCFormer-Tiny%2520reaching%252062.0%2525%2520accuracy%2520and%2520a%252046.3%2525%2520F1-score%2520while%2520using%250Asignificantly%2520fewer%2520parameters.%2520These%2520results%2520highlight%2520DCFormer%2527s%2520potential%250Afor%2520scalable%252C%2520clinically%2520deployable%25203D%2520medical%2520VLMs.%2520Our%2520codes%2520will%2520be%2520publicly%250Aavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05091v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DCFormer%3A%20Efficient%203D%20Vision-Language%20Modeling%20with%20Decomposed%0A%20%20Convolutions&entry.906535625=Gorkem%20Can%20Ates%20and%20Kuang%20Gong%20and%20Wei%20Shao&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20align%20visual%20and%20textual%20representations%2C%0Aenabling%20high-performance%20zero-shot%20classification%20and%20image-text%20retrieval%20in%0A2D%20medical%20imaging.%20However%2C%20extending%20VLMs%20to%203D%20medical%20imaging%20remains%0Acomputationally%20challenging.%20Existing%203D%20VLMs%20rely%20on%20Vision%20Transformers%0A%28ViTs%29%2C%20which%20are%20computationally%20expensive%20due%20to%20self-attention%27s%20quadratic%0Acomplexity%2C%20or%203D%20convolutions%2C%20which%20demand%20excessive%20parameters%20and%20FLOPs%20as%0Akernel%20size%20increases.%20We%20introduce%20DCFormer%2C%20an%20efficient%203D%20medical%20image%0Aencoder%20that%20factorizes%203D%20convolutions%20into%20three%20parallel%201D%20convolutions%0Aalong%20depth%2C%20height%2C%20and%20width.%20This%20design%20preserves%20spatial%20information%20while%0Asignificantly%20reducing%20computational%20cost.%20Integrated%20into%20a%20CLIP-based%0Avision-language%20framework%2C%20DCFormer%20is%20evaluated%20on%20CT-RATE%2C%20a%20dataset%20of%0A50%2C188%20paired%203D%20chest%20CT%20volumes%20and%20radiology%20reports%2C%20for%20zero-shot%0Amulti-abnormality%20detection%20across%2018%20pathologies.%20Compared%20to%20ViT%2C%20ConvNeXt%2C%0APoolFormer%2C%20and%20TransUNet%2C%20DCFormer%20achieves%20superior%20efficiency%20and%20accuracy%2C%0Awith%20DCFormer-Tiny%20reaching%2062.0%25%20accuracy%20and%20a%2046.3%25%20F1-score%20while%20using%0Asignificantly%20fewer%20parameters.%20These%20results%20highlight%20DCFormer%27s%20potential%0Afor%20scalable%2C%20clinically%20deployable%203D%20medical%20VLMs.%20Our%20codes%20will%20be%20publicly%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05091v1&entry.124074799=Read"},
{"title": "AuraFusion360: Augmented Unseen Region Alignment for Reference-based\n  360\u00b0 Unbounded Scene Inpainting", "author": "Chung-Ho Wu and Yang-Jung Chen and Ying-Huan Chen and Jie-Ying Lee and Bo-Hsu Ke and Chun-Wei Tuan Mu and Yi-Chuan Huang and Chin-Yang Lin and Min-Hung Chen and Yen-Yu Lin and Yu-Lun Liu", "abstract": "  Three-dimensional scene inpainting is crucial for applications from virtual\nreality to architectural visualization, yet existing methods struggle with view\nconsistency and geometric accuracy in 360{\\deg} unbounded scenes. We present\nAuraFusion360, a novel reference-based method that enables high-quality object\nremoval and hole filling in 3D scenes represented by Gaussian Splatting. Our\napproach introduces (1) depth-aware unseen mask generation for accurate\nocclusion identification, (2) Adaptive Guided Depth Diffusion, a zero-shot\nmethod for accurate initial point placement without requiring additional\ntraining, and (3) SDEdit-based detail enhancement for multi-view coherence. We\nalso introduce 360-USID, the first comprehensive dataset for 360{\\deg}\nunbounded scene inpainting with ground truth. Extensive experiments demonstrate\nthat AuraFusion360 significantly outperforms existing methods, achieving\nsuperior perceptual quality while maintaining geometric accuracy across\ndramatic viewpoint changes. See our project page for video results and the\ndataset at https://kkennethwu.github.io/aurafusion360/.\n", "link": "http://arxiv.org/abs/2502.05176v1", "date": "2025-02-07", "relevancy": 3.0137, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6042}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6042}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AuraFusion360%3A%20Augmented%20Unseen%20Region%20Alignment%20for%20Reference-based%0A%20%20360%C2%B0%20Unbounded%20Scene%20Inpainting&body=Title%3A%20AuraFusion360%3A%20Augmented%20Unseen%20Region%20Alignment%20for%20Reference-based%0A%20%20360%C2%B0%20Unbounded%20Scene%20Inpainting%0AAuthor%3A%20Chung-Ho%20Wu%20and%20Yang-Jung%20Chen%20and%20Ying-Huan%20Chen%20and%20Jie-Ying%20Lee%20and%20Bo-Hsu%20Ke%20and%20Chun-Wei%20Tuan%20Mu%20and%20Yi-Chuan%20Huang%20and%20Chin-Yang%20Lin%20and%20Min-Hung%20Chen%20and%20Yen-Yu%20Lin%20and%20Yu-Lun%20Liu%0AAbstract%3A%20%20%20Three-dimensional%20scene%20inpainting%20is%20crucial%20for%20applications%20from%20virtual%0Areality%20to%20architectural%20visualization%2C%20yet%20existing%20methods%20struggle%20with%20view%0Aconsistency%20and%20geometric%20accuracy%20in%20360%7B%5Cdeg%7D%20unbounded%20scenes.%20We%20present%0AAuraFusion360%2C%20a%20novel%20reference-based%20method%20that%20enables%20high-quality%20object%0Aremoval%20and%20hole%20filling%20in%203D%20scenes%20represented%20by%20Gaussian%20Splatting.%20Our%0Aapproach%20introduces%20%281%29%20depth-aware%20unseen%20mask%20generation%20for%20accurate%0Aocclusion%20identification%2C%20%282%29%20Adaptive%20Guided%20Depth%20Diffusion%2C%20a%20zero-shot%0Amethod%20for%20accurate%20initial%20point%20placement%20without%20requiring%20additional%0Atraining%2C%20and%20%283%29%20SDEdit-based%20detail%20enhancement%20for%20multi-view%20coherence.%20We%0Aalso%20introduce%20360-USID%2C%20the%20first%20comprehensive%20dataset%20for%20360%7B%5Cdeg%7D%0Aunbounded%20scene%20inpainting%20with%20ground%20truth.%20Extensive%20experiments%20demonstrate%0Athat%20AuraFusion360%20significantly%20outperforms%20existing%20methods%2C%20achieving%0Asuperior%20perceptual%20quality%20while%20maintaining%20geometric%20accuracy%20across%0Adramatic%20viewpoint%20changes.%20See%20our%20project%20page%20for%20video%20results%20and%20the%0Adataset%20at%20https%3A//kkennethwu.github.io/aurafusion360/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05176v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuraFusion360%253A%2520Augmented%2520Unseen%2520Region%2520Alignment%2520for%2520Reference-based%250A%2520%2520360%25C2%25B0%2520Unbounded%2520Scene%2520Inpainting%26entry.906535625%3DChung-Ho%2520Wu%2520and%2520Yang-Jung%2520Chen%2520and%2520Ying-Huan%2520Chen%2520and%2520Jie-Ying%2520Lee%2520and%2520Bo-Hsu%2520Ke%2520and%2520Chun-Wei%2520Tuan%2520Mu%2520and%2520Yi-Chuan%2520Huang%2520and%2520Chin-Yang%2520Lin%2520and%2520Min-Hung%2520Chen%2520and%2520Yen-Yu%2520Lin%2520and%2520Yu-Lun%2520Liu%26entry.1292438233%3D%2520%2520Three-dimensional%2520scene%2520inpainting%2520is%2520crucial%2520for%2520applications%2520from%2520virtual%250Areality%2520to%2520architectural%2520visualization%252C%2520yet%2520existing%2520methods%2520struggle%2520with%2520view%250Aconsistency%2520and%2520geometric%2520accuracy%2520in%2520360%257B%255Cdeg%257D%2520unbounded%2520scenes.%2520We%2520present%250AAuraFusion360%252C%2520a%2520novel%2520reference-based%2520method%2520that%2520enables%2520high-quality%2520object%250Aremoval%2520and%2520hole%2520filling%2520in%25203D%2520scenes%2520represented%2520by%2520Gaussian%2520Splatting.%2520Our%250Aapproach%2520introduces%2520%25281%2529%2520depth-aware%2520unseen%2520mask%2520generation%2520for%2520accurate%250Aocclusion%2520identification%252C%2520%25282%2529%2520Adaptive%2520Guided%2520Depth%2520Diffusion%252C%2520a%2520zero-shot%250Amethod%2520for%2520accurate%2520initial%2520point%2520placement%2520without%2520requiring%2520additional%250Atraining%252C%2520and%2520%25283%2529%2520SDEdit-based%2520detail%2520enhancement%2520for%2520multi-view%2520coherence.%2520We%250Aalso%2520introduce%2520360-USID%252C%2520the%2520first%2520comprehensive%2520dataset%2520for%2520360%257B%255Cdeg%257D%250Aunbounded%2520scene%2520inpainting%2520with%2520ground%2520truth.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520AuraFusion360%2520significantly%2520outperforms%2520existing%2520methods%252C%2520achieving%250Asuperior%2520perceptual%2520quality%2520while%2520maintaining%2520geometric%2520accuracy%2520across%250Adramatic%2520viewpoint%2520changes.%2520See%2520our%2520project%2520page%2520for%2520video%2520results%2520and%2520the%250Adataset%2520at%2520https%253A//kkennethwu.github.io/aurafusion360/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05176v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AuraFusion360%3A%20Augmented%20Unseen%20Region%20Alignment%20for%20Reference-based%0A%20%20360%C2%B0%20Unbounded%20Scene%20Inpainting&entry.906535625=Chung-Ho%20Wu%20and%20Yang-Jung%20Chen%20and%20Ying-Huan%20Chen%20and%20Jie-Ying%20Lee%20and%20Bo-Hsu%20Ke%20and%20Chun-Wei%20Tuan%20Mu%20and%20Yi-Chuan%20Huang%20and%20Chin-Yang%20Lin%20and%20Min-Hung%20Chen%20and%20Yen-Yu%20Lin%20and%20Yu-Lun%20Liu&entry.1292438233=%20%20Three-dimensional%20scene%20inpainting%20is%20crucial%20for%20applications%20from%20virtual%0Areality%20to%20architectural%20visualization%2C%20yet%20existing%20methods%20struggle%20with%20view%0Aconsistency%20and%20geometric%20accuracy%20in%20360%7B%5Cdeg%7D%20unbounded%20scenes.%20We%20present%0AAuraFusion360%2C%20a%20novel%20reference-based%20method%20that%20enables%20high-quality%20object%0Aremoval%20and%20hole%20filling%20in%203D%20scenes%20represented%20by%20Gaussian%20Splatting.%20Our%0Aapproach%20introduces%20%281%29%20depth-aware%20unseen%20mask%20generation%20for%20accurate%0Aocclusion%20identification%2C%20%282%29%20Adaptive%20Guided%20Depth%20Diffusion%2C%20a%20zero-shot%0Amethod%20for%20accurate%20initial%20point%20placement%20without%20requiring%20additional%0Atraining%2C%20and%20%283%29%20SDEdit-based%20detail%20enhancement%20for%20multi-view%20coherence.%20We%0Aalso%20introduce%20360-USID%2C%20the%20first%20comprehensive%20dataset%20for%20360%7B%5Cdeg%7D%0Aunbounded%20scene%20inpainting%20with%20ground%20truth.%20Extensive%20experiments%20demonstrate%0Athat%20AuraFusion360%20significantly%20outperforms%20existing%20methods%2C%20achieving%0Asuperior%20perceptual%20quality%20while%20maintaining%20geometric%20accuracy%20across%0Adramatic%20viewpoint%20changes.%20See%20our%20project%20page%20for%20video%20results%20and%20the%0Adataset%20at%20https%3A//kkennethwu.github.io/aurafusion360/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05176v1&entry.124074799=Read"},
{"title": "Kronecker Mask and Interpretive Prompts are Language-Action Video\n  Learners", "author": "Jingyi Yang and Zitong Yu and Xiuming Ni and Jia He and Hui Li", "abstract": "  Contrastive language-image pretraining (CLIP) has significantly advanced\nimage-based vision learning. A pressing topic subsequently arises: how can we\neffectively adapt CLIP to the video domain? Recent studies have focused on\nadjusting either the textual or visual branch of CLIP for action recognition.\nHowever, we argue that adaptations of both branches are crucial. In this paper,\nwe propose \\textbf{CLAVER}: a \\textbf{C}ontrastive\n\\textbf{L}anguage-\\textbf{A}ction \\textbf{V}ideo Learn\\textbf{er}, designed to\nshift CLIP's focus from the alignment of static visual objects and concrete\nnouns to the alignment of dynamic action behaviors and abstract verbs.\nSpecifically, we introduce a novel Kronecker mask attention for temporal\nmodeling. Our tailored Kronecker mask offers three benefits 1) it expands the\ntemporal receptive field for each token, 2) it serves as an effective\nspatiotemporal heterogeneity inductive bias, mitigating the issue of\nspatiotemporal homogenization, and 3) it can be seamlessly plugged into\ntransformer-based models. Regarding the textual branch, we leverage large\nlanguage models to generate diverse, sentence-level and semantically rich\ninterpretive prompts of actions, which shift the model's focus towards the verb\ncomprehension. Extensive experiments on various benchmarks and learning\nscenarios demonstrate the superiority and generality of our approach. The code\nwill be available soon.\n", "link": "http://arxiv.org/abs/2502.03549v2", "date": "2025-02-07", "relevancy": 3.001, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6133}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5937}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kronecker%20Mask%20and%20Interpretive%20Prompts%20are%20Language-Action%20Video%0A%20%20Learners&body=Title%3A%20Kronecker%20Mask%20and%20Interpretive%20Prompts%20are%20Language-Action%20Video%0A%20%20Learners%0AAuthor%3A%20Jingyi%20Yang%20and%20Zitong%20Yu%20and%20Xiuming%20Ni%20and%20Jia%20He%20and%20Hui%20Li%0AAbstract%3A%20%20%20Contrastive%20language-image%20pretraining%20%28CLIP%29%20has%20significantly%20advanced%0Aimage-based%20vision%20learning.%20A%20pressing%20topic%20subsequently%20arises%3A%20how%20can%20we%0Aeffectively%20adapt%20CLIP%20to%20the%20video%20domain%3F%20Recent%20studies%20have%20focused%20on%0Aadjusting%20either%20the%20textual%20or%20visual%20branch%20of%20CLIP%20for%20action%20recognition.%0AHowever%2C%20we%20argue%20that%20adaptations%20of%20both%20branches%20are%20crucial.%20In%20this%20paper%2C%0Awe%20propose%20%5Ctextbf%7BCLAVER%7D%3A%20a%20%5Ctextbf%7BC%7Dontrastive%0A%5Ctextbf%7BL%7Danguage-%5Ctextbf%7BA%7Dction%20%5Ctextbf%7BV%7Dideo%20Learn%5Ctextbf%7Ber%7D%2C%20designed%20to%0Ashift%20CLIP%27s%20focus%20from%20the%20alignment%20of%20static%20visual%20objects%20and%20concrete%0Anouns%20to%20the%20alignment%20of%20dynamic%20action%20behaviors%20and%20abstract%20verbs.%0ASpecifically%2C%20we%20introduce%20a%20novel%20Kronecker%20mask%20attention%20for%20temporal%0Amodeling.%20Our%20tailored%20Kronecker%20mask%20offers%20three%20benefits%201%29%20it%20expands%20the%0Atemporal%20receptive%20field%20for%20each%20token%2C%202%29%20it%20serves%20as%20an%20effective%0Aspatiotemporal%20heterogeneity%20inductive%20bias%2C%20mitigating%20the%20issue%20of%0Aspatiotemporal%20homogenization%2C%20and%203%29%20it%20can%20be%20seamlessly%20plugged%20into%0Atransformer-based%20models.%20Regarding%20the%20textual%20branch%2C%20we%20leverage%20large%0Alanguage%20models%20to%20generate%20diverse%2C%20sentence-level%20and%20semantically%20rich%0Ainterpretive%20prompts%20of%20actions%2C%20which%20shift%20the%20model%27s%20focus%20towards%20the%20verb%0Acomprehension.%20Extensive%20experiments%20on%20various%20benchmarks%20and%20learning%0Ascenarios%20demonstrate%20the%20superiority%20and%20generality%20of%20our%20approach.%20The%20code%0Awill%20be%20available%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03549v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKronecker%2520Mask%2520and%2520Interpretive%2520Prompts%2520are%2520Language-Action%2520Video%250A%2520%2520Learners%26entry.906535625%3DJingyi%2520Yang%2520and%2520Zitong%2520Yu%2520and%2520Xiuming%2520Ni%2520and%2520Jia%2520He%2520and%2520Hui%2520Li%26entry.1292438233%3D%2520%2520Contrastive%2520language-image%2520pretraining%2520%2528CLIP%2529%2520has%2520significantly%2520advanced%250Aimage-based%2520vision%2520learning.%2520A%2520pressing%2520topic%2520subsequently%2520arises%253A%2520how%2520can%2520we%250Aeffectively%2520adapt%2520CLIP%2520to%2520the%2520video%2520domain%253F%2520Recent%2520studies%2520have%2520focused%2520on%250Aadjusting%2520either%2520the%2520textual%2520or%2520visual%2520branch%2520of%2520CLIP%2520for%2520action%2520recognition.%250AHowever%252C%2520we%2520argue%2520that%2520adaptations%2520of%2520both%2520branches%2520are%2520crucial.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520%255Ctextbf%257BCLAVER%257D%253A%2520a%2520%255Ctextbf%257BC%257Dontrastive%250A%255Ctextbf%257BL%257Danguage-%255Ctextbf%257BA%257Dction%2520%255Ctextbf%257BV%257Dideo%2520Learn%255Ctextbf%257Ber%257D%252C%2520designed%2520to%250Ashift%2520CLIP%2527s%2520focus%2520from%2520the%2520alignment%2520of%2520static%2520visual%2520objects%2520and%2520concrete%250Anouns%2520to%2520the%2520alignment%2520of%2520dynamic%2520action%2520behaviors%2520and%2520abstract%2520verbs.%250ASpecifically%252C%2520we%2520introduce%2520a%2520novel%2520Kronecker%2520mask%2520attention%2520for%2520temporal%250Amodeling.%2520Our%2520tailored%2520Kronecker%2520mask%2520offers%2520three%2520benefits%25201%2529%2520it%2520expands%2520the%250Atemporal%2520receptive%2520field%2520for%2520each%2520token%252C%25202%2529%2520it%2520serves%2520as%2520an%2520effective%250Aspatiotemporal%2520heterogeneity%2520inductive%2520bias%252C%2520mitigating%2520the%2520issue%2520of%250Aspatiotemporal%2520homogenization%252C%2520and%25203%2529%2520it%2520can%2520be%2520seamlessly%2520plugged%2520into%250Atransformer-based%2520models.%2520Regarding%2520the%2520textual%2520branch%252C%2520we%2520leverage%2520large%250Alanguage%2520models%2520to%2520generate%2520diverse%252C%2520sentence-level%2520and%2520semantically%2520rich%250Ainterpretive%2520prompts%2520of%2520actions%252C%2520which%2520shift%2520the%2520model%2527s%2520focus%2520towards%2520the%2520verb%250Acomprehension.%2520Extensive%2520experiments%2520on%2520various%2520benchmarks%2520and%2520learning%250Ascenarios%2520demonstrate%2520the%2520superiority%2520and%2520generality%2520of%2520our%2520approach.%2520The%2520code%250Awill%2520be%2520available%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03549v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kronecker%20Mask%20and%20Interpretive%20Prompts%20are%20Language-Action%20Video%0A%20%20Learners&entry.906535625=Jingyi%20Yang%20and%20Zitong%20Yu%20and%20Xiuming%20Ni%20and%20Jia%20He%20and%20Hui%20Li&entry.1292438233=%20%20Contrastive%20language-image%20pretraining%20%28CLIP%29%20has%20significantly%20advanced%0Aimage-based%20vision%20learning.%20A%20pressing%20topic%20subsequently%20arises%3A%20how%20can%20we%0Aeffectively%20adapt%20CLIP%20to%20the%20video%20domain%3F%20Recent%20studies%20have%20focused%20on%0Aadjusting%20either%20the%20textual%20or%20visual%20branch%20of%20CLIP%20for%20action%20recognition.%0AHowever%2C%20we%20argue%20that%20adaptations%20of%20both%20branches%20are%20crucial.%20In%20this%20paper%2C%0Awe%20propose%20%5Ctextbf%7BCLAVER%7D%3A%20a%20%5Ctextbf%7BC%7Dontrastive%0A%5Ctextbf%7BL%7Danguage-%5Ctextbf%7BA%7Dction%20%5Ctextbf%7BV%7Dideo%20Learn%5Ctextbf%7Ber%7D%2C%20designed%20to%0Ashift%20CLIP%27s%20focus%20from%20the%20alignment%20of%20static%20visual%20objects%20and%20concrete%0Anouns%20to%20the%20alignment%20of%20dynamic%20action%20behaviors%20and%20abstract%20verbs.%0ASpecifically%2C%20we%20introduce%20a%20novel%20Kronecker%20mask%20attention%20for%20temporal%0Amodeling.%20Our%20tailored%20Kronecker%20mask%20offers%20three%20benefits%201%29%20it%20expands%20the%0Atemporal%20receptive%20field%20for%20each%20token%2C%202%29%20it%20serves%20as%20an%20effective%0Aspatiotemporal%20heterogeneity%20inductive%20bias%2C%20mitigating%20the%20issue%20of%0Aspatiotemporal%20homogenization%2C%20and%203%29%20it%20can%20be%20seamlessly%20plugged%20into%0Atransformer-based%20models.%20Regarding%20the%20textual%20branch%2C%20we%20leverage%20large%0Alanguage%20models%20to%20generate%20diverse%2C%20sentence-level%20and%20semantically%20rich%0Ainterpretive%20prompts%20of%20actions%2C%20which%20shift%20the%20model%27s%20focus%20towards%20the%20verb%0Acomprehension.%20Extensive%20experiments%20on%20various%20benchmarks%20and%20learning%0Ascenarios%20demonstrate%20the%20superiority%20and%20generality%20of%20our%20approach.%20The%20code%0Awill%20be%20available%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03549v2&entry.124074799=Read"},
{"title": "Exploring scalable medical image encoders beyond text supervision", "author": "Fernando P\u00e9rez-Garc\u00eda and Harshita Sharma and Sam Bond-Taylor and Kenza Bouzid and Valentina Salvatelli and Maximilian Ilse and Shruthi Bannur and Daniel C. Castro and Anton Schwaighofer and Matthew P. Lungren and Maria Teodora Wetscherek and Noel Codella and Stephanie L. Hyland and Javier Alvarez-Valle and Ozan Oktay", "abstract": "  Language-supervised pre-training has proven to be a valuable method for\nextracting semantically meaningful features from images, serving as a\nfoundational element in multimodal systems within the computer vision and\nmedical imaging domains. However, the computed features are limited by the\ninformation contained in the text, which is particularly problematic in medical\nimaging, where the findings described by radiologists focus on specific\nobservations. This challenge is compounded by the scarcity of paired\nimaging-text data due to concerns over leakage of personal health information.\nIn this work, we fundamentally challenge the prevailing reliance on language\nsupervision for learning general-purpose biomedical imaging encoders. We\nintroduce RAD-DINO, a biomedical image encoder pre-trained solely on unimodal\nbiomedical imaging data that obtains similar or greater performance than\nstate-of-the-art biomedical language-supervised models on a diverse range of\nbenchmarks. Specifically, the quality of learned representations is evaluated\non standard imaging tasks (classification and semantic segmentation), and a\nvision-language alignment task (text report generation from images). To further\ndemonstrate the drawback of language supervision, we show that features from\nRAD-DINO correlate with other medical records (e.g., sex or age) better than\nlanguage-supervised models, which are generally not mentioned in radiology\nreports. Finally, we conduct a series of ablations determining the factors in\nRAD-DINO's performance; notably, we observe that RAD-DINO's downstream\nperformance scales well with the quantity and diversity of training data,\ndemonstrating that image-only supervision is a scalable approach for training a\nfoundational biomedical image encoder. Model weights of RAD-DINO trained on\npublicly available datasets are available at\nhttps://huggingface.co/microsoft/rad-dino.\n", "link": "http://arxiv.org/abs/2401.10815v3", "date": "2025-02-07", "relevancy": 2.9459, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6122}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6122}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20scalable%20medical%20image%20encoders%20beyond%20text%20supervision&body=Title%3A%20Exploring%20scalable%20medical%20image%20encoders%20beyond%20text%20supervision%0AAuthor%3A%20Fernando%20P%C3%A9rez-Garc%C3%ADa%20and%20Harshita%20Sharma%20and%20Sam%20Bond-Taylor%20and%20Kenza%20Bouzid%20and%20Valentina%20Salvatelli%20and%20Maximilian%20Ilse%20and%20Shruthi%20Bannur%20and%20Daniel%20C.%20Castro%20and%20Anton%20Schwaighofer%20and%20Matthew%20P.%20Lungren%20and%20Maria%20Teodora%20Wetscherek%20and%20Noel%20Codella%20and%20Stephanie%20L.%20Hyland%20and%20Javier%20Alvarez-Valle%20and%20Ozan%20Oktay%0AAbstract%3A%20%20%20Language-supervised%20pre-training%20has%20proven%20to%20be%20a%20valuable%20method%20for%0Aextracting%20semantically%20meaningful%20features%20from%20images%2C%20serving%20as%20a%0Afoundational%20element%20in%20multimodal%20systems%20within%20the%20computer%20vision%20and%0Amedical%20imaging%20domains.%20However%2C%20the%20computed%20features%20are%20limited%20by%20the%0Ainformation%20contained%20in%20the%20text%2C%20which%20is%20particularly%20problematic%20in%20medical%0Aimaging%2C%20where%20the%20findings%20described%20by%20radiologists%20focus%20on%20specific%0Aobservations.%20This%20challenge%20is%20compounded%20by%20the%20scarcity%20of%20paired%0Aimaging-text%20data%20due%20to%20concerns%20over%20leakage%20of%20personal%20health%20information.%0AIn%20this%20work%2C%20we%20fundamentally%20challenge%20the%20prevailing%20reliance%20on%20language%0Asupervision%20for%20learning%20general-purpose%20biomedical%20imaging%20encoders.%20We%0Aintroduce%20RAD-DINO%2C%20a%20biomedical%20image%20encoder%20pre-trained%20solely%20on%20unimodal%0Abiomedical%20imaging%20data%20that%20obtains%20similar%20or%20greater%20performance%20than%0Astate-of-the-art%20biomedical%20language-supervised%20models%20on%20a%20diverse%20range%20of%0Abenchmarks.%20Specifically%2C%20the%20quality%20of%20learned%20representations%20is%20evaluated%0Aon%20standard%20imaging%20tasks%20%28classification%20and%20semantic%20segmentation%29%2C%20and%20a%0Avision-language%20alignment%20task%20%28text%20report%20generation%20from%20images%29.%20To%20further%0Ademonstrate%20the%20drawback%20of%20language%20supervision%2C%20we%20show%20that%20features%20from%0ARAD-DINO%20correlate%20with%20other%20medical%20records%20%28e.g.%2C%20sex%20or%20age%29%20better%20than%0Alanguage-supervised%20models%2C%20which%20are%20generally%20not%20mentioned%20in%20radiology%0Areports.%20Finally%2C%20we%20conduct%20a%20series%20of%20ablations%20determining%20the%20factors%20in%0ARAD-DINO%27s%20performance%3B%20notably%2C%20we%20observe%20that%20RAD-DINO%27s%20downstream%0Aperformance%20scales%20well%20with%20the%20quantity%20and%20diversity%20of%20training%20data%2C%0Ademonstrating%20that%20image-only%20supervision%20is%20a%20scalable%20approach%20for%20training%20a%0Afoundational%20biomedical%20image%20encoder.%20Model%20weights%20of%20RAD-DINO%20trained%20on%0Apublicly%20available%20datasets%20are%20available%20at%0Ahttps%3A//huggingface.co/microsoft/rad-dino.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.10815v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520scalable%2520medical%2520image%2520encoders%2520beyond%2520text%2520supervision%26entry.906535625%3DFernando%2520P%25C3%25A9rez-Garc%25C3%25ADa%2520and%2520Harshita%2520Sharma%2520and%2520Sam%2520Bond-Taylor%2520and%2520Kenza%2520Bouzid%2520and%2520Valentina%2520Salvatelli%2520and%2520Maximilian%2520Ilse%2520and%2520Shruthi%2520Bannur%2520and%2520Daniel%2520C.%2520Castro%2520and%2520Anton%2520Schwaighofer%2520and%2520Matthew%2520P.%2520Lungren%2520and%2520Maria%2520Teodora%2520Wetscherek%2520and%2520Noel%2520Codella%2520and%2520Stephanie%2520L.%2520Hyland%2520and%2520Javier%2520Alvarez-Valle%2520and%2520Ozan%2520Oktay%26entry.1292438233%3D%2520%2520Language-supervised%2520pre-training%2520has%2520proven%2520to%2520be%2520a%2520valuable%2520method%2520for%250Aextracting%2520semantically%2520meaningful%2520features%2520from%2520images%252C%2520serving%2520as%2520a%250Afoundational%2520element%2520in%2520multimodal%2520systems%2520within%2520the%2520computer%2520vision%2520and%250Amedical%2520imaging%2520domains.%2520However%252C%2520the%2520computed%2520features%2520are%2520limited%2520by%2520the%250Ainformation%2520contained%2520in%2520the%2520text%252C%2520which%2520is%2520particularly%2520problematic%2520in%2520medical%250Aimaging%252C%2520where%2520the%2520findings%2520described%2520by%2520radiologists%2520focus%2520on%2520specific%250Aobservations.%2520This%2520challenge%2520is%2520compounded%2520by%2520the%2520scarcity%2520of%2520paired%250Aimaging-text%2520data%2520due%2520to%2520concerns%2520over%2520leakage%2520of%2520personal%2520health%2520information.%250AIn%2520this%2520work%252C%2520we%2520fundamentally%2520challenge%2520the%2520prevailing%2520reliance%2520on%2520language%250Asupervision%2520for%2520learning%2520general-purpose%2520biomedical%2520imaging%2520encoders.%2520We%250Aintroduce%2520RAD-DINO%252C%2520a%2520biomedical%2520image%2520encoder%2520pre-trained%2520solely%2520on%2520unimodal%250Abiomedical%2520imaging%2520data%2520that%2520obtains%2520similar%2520or%2520greater%2520performance%2520than%250Astate-of-the-art%2520biomedical%2520language-supervised%2520models%2520on%2520a%2520diverse%2520range%2520of%250Abenchmarks.%2520Specifically%252C%2520the%2520quality%2520of%2520learned%2520representations%2520is%2520evaluated%250Aon%2520standard%2520imaging%2520tasks%2520%2528classification%2520and%2520semantic%2520segmentation%2529%252C%2520and%2520a%250Avision-language%2520alignment%2520task%2520%2528text%2520report%2520generation%2520from%2520images%2529.%2520To%2520further%250Ademonstrate%2520the%2520drawback%2520of%2520language%2520supervision%252C%2520we%2520show%2520that%2520features%2520from%250ARAD-DINO%2520correlate%2520with%2520other%2520medical%2520records%2520%2528e.g.%252C%2520sex%2520or%2520age%2529%2520better%2520than%250Alanguage-supervised%2520models%252C%2520which%2520are%2520generally%2520not%2520mentioned%2520in%2520radiology%250Areports.%2520Finally%252C%2520we%2520conduct%2520a%2520series%2520of%2520ablations%2520determining%2520the%2520factors%2520in%250ARAD-DINO%2527s%2520performance%253B%2520notably%252C%2520we%2520observe%2520that%2520RAD-DINO%2527s%2520downstream%250Aperformance%2520scales%2520well%2520with%2520the%2520quantity%2520and%2520diversity%2520of%2520training%2520data%252C%250Ademonstrating%2520that%2520image-only%2520supervision%2520is%2520a%2520scalable%2520approach%2520for%2520training%2520a%250Afoundational%2520biomedical%2520image%2520encoder.%2520Model%2520weights%2520of%2520RAD-DINO%2520trained%2520on%250Apublicly%2520available%2520datasets%2520are%2520available%2520at%250Ahttps%253A//huggingface.co/microsoft/rad-dino.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.10815v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20scalable%20medical%20image%20encoders%20beyond%20text%20supervision&entry.906535625=Fernando%20P%C3%A9rez-Garc%C3%ADa%20and%20Harshita%20Sharma%20and%20Sam%20Bond-Taylor%20and%20Kenza%20Bouzid%20and%20Valentina%20Salvatelli%20and%20Maximilian%20Ilse%20and%20Shruthi%20Bannur%20and%20Daniel%20C.%20Castro%20and%20Anton%20Schwaighofer%20and%20Matthew%20P.%20Lungren%20and%20Maria%20Teodora%20Wetscherek%20and%20Noel%20Codella%20and%20Stephanie%20L.%20Hyland%20and%20Javier%20Alvarez-Valle%20and%20Ozan%20Oktay&entry.1292438233=%20%20Language-supervised%20pre-training%20has%20proven%20to%20be%20a%20valuable%20method%20for%0Aextracting%20semantically%20meaningful%20features%20from%20images%2C%20serving%20as%20a%0Afoundational%20element%20in%20multimodal%20systems%20within%20the%20computer%20vision%20and%0Amedical%20imaging%20domains.%20However%2C%20the%20computed%20features%20are%20limited%20by%20the%0Ainformation%20contained%20in%20the%20text%2C%20which%20is%20particularly%20problematic%20in%20medical%0Aimaging%2C%20where%20the%20findings%20described%20by%20radiologists%20focus%20on%20specific%0Aobservations.%20This%20challenge%20is%20compounded%20by%20the%20scarcity%20of%20paired%0Aimaging-text%20data%20due%20to%20concerns%20over%20leakage%20of%20personal%20health%20information.%0AIn%20this%20work%2C%20we%20fundamentally%20challenge%20the%20prevailing%20reliance%20on%20language%0Asupervision%20for%20learning%20general-purpose%20biomedical%20imaging%20encoders.%20We%0Aintroduce%20RAD-DINO%2C%20a%20biomedical%20image%20encoder%20pre-trained%20solely%20on%20unimodal%0Abiomedical%20imaging%20data%20that%20obtains%20similar%20or%20greater%20performance%20than%0Astate-of-the-art%20biomedical%20language-supervised%20models%20on%20a%20diverse%20range%20of%0Abenchmarks.%20Specifically%2C%20the%20quality%20of%20learned%20representations%20is%20evaluated%0Aon%20standard%20imaging%20tasks%20%28classification%20and%20semantic%20segmentation%29%2C%20and%20a%0Avision-language%20alignment%20task%20%28text%20report%20generation%20from%20images%29.%20To%20further%0Ademonstrate%20the%20drawback%20of%20language%20supervision%2C%20we%20show%20that%20features%20from%0ARAD-DINO%20correlate%20with%20other%20medical%20records%20%28e.g.%2C%20sex%20or%20age%29%20better%20than%0Alanguage-supervised%20models%2C%20which%20are%20generally%20not%20mentioned%20in%20radiology%0Areports.%20Finally%2C%20we%20conduct%20a%20series%20of%20ablations%20determining%20the%20factors%20in%0ARAD-DINO%27s%20performance%3B%20notably%2C%20we%20observe%20that%20RAD-DINO%27s%20downstream%0Aperformance%20scales%20well%20with%20the%20quantity%20and%20diversity%20of%20training%20data%2C%0Ademonstrating%20that%20image-only%20supervision%20is%20a%20scalable%20approach%20for%20training%20a%0Afoundational%20biomedical%20image%20encoder.%20Model%20weights%20of%20RAD-DINO%20trained%20on%0Apublicly%20available%20datasets%20are%20available%20at%0Ahttps%3A//huggingface.co/microsoft/rad-dino.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10815v3&entry.124074799=Read"},
{"title": "Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with\n  Leading Short-Context Accuray", "author": "Yunhang Shen and Chaoyou Fu and Shaoqi Dong and Xiong Wang and Peixian Chen and Mengdan Zhang and Haoyu Cao and Ke Li and Xiawu Zheng and Yan Zhang and Yiyi Zhou and Rongrong Ji and Xing Sun", "abstract": "  Establishing the long-context capability of large vision-language models is\ncrucial for video understanding, high-resolution image understanding,\nmulti-modal agents and reasoning. We introduce Long-VITA, a simple yet\neffective large multi-modal model for long-context visual-language\nunderstanding tasks. It is adept at concurrently processing and analyzing\nmodalities of image, video, and text over 4K frames or 1M tokens while\ndelivering advanced performances on short-context multi-modal tasks. We propose\nan effective multi-modal training schema that starts with large language models\nand proceeds through vision-language alignment, general knowledge learning, and\ntwo sequential stages of long-sequence fine-tuning. We further implement\ncontext-parallelism distributed inference and logits-masked language modeling\nhead to scale Long-VITA to infinitely long inputs of images and texts during\nmodel inference. Regarding training data, Long-VITA is built on a mix of $17$M\nsamples from public datasets only and demonstrates the state-of-the-art\nperformance on various multi-modal benchmarks, compared against recent\ncutting-edge models with internal data. Long-VITA is fully reproducible and\nsupports both NPU and GPU platforms for training and testing. We hope Long-VITA\ncan serve as a competitive baseline and offer valuable insights for the\nopen-source community in advancing long-context multi-modal understanding.\n", "link": "http://arxiv.org/abs/2502.05177v1", "date": "2025-02-07", "relevancy": 2.9441, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5895}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5895}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long-VITA%3A%20Scaling%20Large%20Multi-modal%20Models%20to%201%20Million%20Tokens%20with%0A%20%20Leading%20Short-Context%20Accuray&body=Title%3A%20Long-VITA%3A%20Scaling%20Large%20Multi-modal%20Models%20to%201%20Million%20Tokens%20with%0A%20%20Leading%20Short-Context%20Accuray%0AAuthor%3A%20Yunhang%20Shen%20and%20Chaoyou%20Fu%20and%20Shaoqi%20Dong%20and%20Xiong%20Wang%20and%20Peixian%20Chen%20and%20Mengdan%20Zhang%20and%20Haoyu%20Cao%20and%20Ke%20Li%20and%20Xiawu%20Zheng%20and%20Yan%20Zhang%20and%20Yiyi%20Zhou%20and%20Rongrong%20Ji%20and%20Xing%20Sun%0AAbstract%3A%20%20%20Establishing%20the%20long-context%20capability%20of%20large%20vision-language%20models%20is%0Acrucial%20for%20video%20understanding%2C%20high-resolution%20image%20understanding%2C%0Amulti-modal%20agents%20and%20reasoning.%20We%20introduce%20Long-VITA%2C%20a%20simple%20yet%0Aeffective%20large%20multi-modal%20model%20for%20long-context%20visual-language%0Aunderstanding%20tasks.%20It%20is%20adept%20at%20concurrently%20processing%20and%20analyzing%0Amodalities%20of%20image%2C%20video%2C%20and%20text%20over%204K%20frames%20or%201M%20tokens%20while%0Adelivering%20advanced%20performances%20on%20short-context%20multi-modal%20tasks.%20We%20propose%0Aan%20effective%20multi-modal%20training%20schema%20that%20starts%20with%20large%20language%20models%0Aand%20proceeds%20through%20vision-language%20alignment%2C%20general%20knowledge%20learning%2C%20and%0Atwo%20sequential%20stages%20of%20long-sequence%20fine-tuning.%20We%20further%20implement%0Acontext-parallelism%20distributed%20inference%20and%20logits-masked%20language%20modeling%0Ahead%20to%20scale%20Long-VITA%20to%20infinitely%20long%20inputs%20of%20images%20and%20texts%20during%0Amodel%20inference.%20Regarding%20training%20data%2C%20Long-VITA%20is%20built%20on%20a%20mix%20of%20%2417%24M%0Asamples%20from%20public%20datasets%20only%20and%20demonstrates%20the%20state-of-the-art%0Aperformance%20on%20various%20multi-modal%20benchmarks%2C%20compared%20against%20recent%0Acutting-edge%20models%20with%20internal%20data.%20Long-VITA%20is%20fully%20reproducible%20and%0Asupports%20both%20NPU%20and%20GPU%20platforms%20for%20training%20and%20testing.%20We%20hope%20Long-VITA%0Acan%20serve%20as%20a%20competitive%20baseline%20and%20offer%20valuable%20insights%20for%20the%0Aopen-source%20community%20in%20advancing%20long-context%20multi-modal%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong-VITA%253A%2520Scaling%2520Large%2520Multi-modal%2520Models%2520to%25201%2520Million%2520Tokens%2520with%250A%2520%2520Leading%2520Short-Context%2520Accuray%26entry.906535625%3DYunhang%2520Shen%2520and%2520Chaoyou%2520Fu%2520and%2520Shaoqi%2520Dong%2520and%2520Xiong%2520Wang%2520and%2520Peixian%2520Chen%2520and%2520Mengdan%2520Zhang%2520and%2520Haoyu%2520Cao%2520and%2520Ke%2520Li%2520and%2520Xiawu%2520Zheng%2520and%2520Yan%2520Zhang%2520and%2520Yiyi%2520Zhou%2520and%2520Rongrong%2520Ji%2520and%2520Xing%2520Sun%26entry.1292438233%3D%2520%2520Establishing%2520the%2520long-context%2520capability%2520of%2520large%2520vision-language%2520models%2520is%250Acrucial%2520for%2520video%2520understanding%252C%2520high-resolution%2520image%2520understanding%252C%250Amulti-modal%2520agents%2520and%2520reasoning.%2520We%2520introduce%2520Long-VITA%252C%2520a%2520simple%2520yet%250Aeffective%2520large%2520multi-modal%2520model%2520for%2520long-context%2520visual-language%250Aunderstanding%2520tasks.%2520It%2520is%2520adept%2520at%2520concurrently%2520processing%2520and%2520analyzing%250Amodalities%2520of%2520image%252C%2520video%252C%2520and%2520text%2520over%25204K%2520frames%2520or%25201M%2520tokens%2520while%250Adelivering%2520advanced%2520performances%2520on%2520short-context%2520multi-modal%2520tasks.%2520We%2520propose%250Aan%2520effective%2520multi-modal%2520training%2520schema%2520that%2520starts%2520with%2520large%2520language%2520models%250Aand%2520proceeds%2520through%2520vision-language%2520alignment%252C%2520general%2520knowledge%2520learning%252C%2520and%250Atwo%2520sequential%2520stages%2520of%2520long-sequence%2520fine-tuning.%2520We%2520further%2520implement%250Acontext-parallelism%2520distributed%2520inference%2520and%2520logits-masked%2520language%2520modeling%250Ahead%2520to%2520scale%2520Long-VITA%2520to%2520infinitely%2520long%2520inputs%2520of%2520images%2520and%2520texts%2520during%250Amodel%2520inference.%2520Regarding%2520training%2520data%252C%2520Long-VITA%2520is%2520built%2520on%2520a%2520mix%2520of%2520%252417%2524M%250Asamples%2520from%2520public%2520datasets%2520only%2520and%2520demonstrates%2520the%2520state-of-the-art%250Aperformance%2520on%2520various%2520multi-modal%2520benchmarks%252C%2520compared%2520against%2520recent%250Acutting-edge%2520models%2520with%2520internal%2520data.%2520Long-VITA%2520is%2520fully%2520reproducible%2520and%250Asupports%2520both%2520NPU%2520and%2520GPU%2520platforms%2520for%2520training%2520and%2520testing.%2520We%2520hope%2520Long-VITA%250Acan%2520serve%2520as%2520a%2520competitive%2520baseline%2520and%2520offer%2520valuable%2520insights%2520for%2520the%250Aopen-source%2520community%2520in%2520advancing%2520long-context%2520multi-modal%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-VITA%3A%20Scaling%20Large%20Multi-modal%20Models%20to%201%20Million%20Tokens%20with%0A%20%20Leading%20Short-Context%20Accuray&entry.906535625=Yunhang%20Shen%20and%20Chaoyou%20Fu%20and%20Shaoqi%20Dong%20and%20Xiong%20Wang%20and%20Peixian%20Chen%20and%20Mengdan%20Zhang%20and%20Haoyu%20Cao%20and%20Ke%20Li%20and%20Xiawu%20Zheng%20and%20Yan%20Zhang%20and%20Yiyi%20Zhou%20and%20Rongrong%20Ji%20and%20Xing%20Sun&entry.1292438233=%20%20Establishing%20the%20long-context%20capability%20of%20large%20vision-language%20models%20is%0Acrucial%20for%20video%20understanding%2C%20high-resolution%20image%20understanding%2C%0Amulti-modal%20agents%20and%20reasoning.%20We%20introduce%20Long-VITA%2C%20a%20simple%20yet%0Aeffective%20large%20multi-modal%20model%20for%20long-context%20visual-language%0Aunderstanding%20tasks.%20It%20is%20adept%20at%20concurrently%20processing%20and%20analyzing%0Amodalities%20of%20image%2C%20video%2C%20and%20text%20over%204K%20frames%20or%201M%20tokens%20while%0Adelivering%20advanced%20performances%20on%20short-context%20multi-modal%20tasks.%20We%20propose%0Aan%20effective%20multi-modal%20training%20schema%20that%20starts%20with%20large%20language%20models%0Aand%20proceeds%20through%20vision-language%20alignment%2C%20general%20knowledge%20learning%2C%20and%0Atwo%20sequential%20stages%20of%20long-sequence%20fine-tuning.%20We%20further%20implement%0Acontext-parallelism%20distributed%20inference%20and%20logits-masked%20language%20modeling%0Ahead%20to%20scale%20Long-VITA%20to%20infinitely%20long%20inputs%20of%20images%20and%20texts%20during%0Amodel%20inference.%20Regarding%20training%20data%2C%20Long-VITA%20is%20built%20on%20a%20mix%20of%20%2417%24M%0Asamples%20from%20public%20datasets%20only%20and%20demonstrates%20the%20state-of-the-art%0Aperformance%20on%20various%20multi-modal%20benchmarks%2C%20compared%20against%20recent%0Acutting-edge%20models%20with%20internal%20data.%20Long-VITA%20is%20fully%20reproducible%20and%0Asupports%20both%20NPU%20and%20GPU%20platforms%20for%20training%20and%20testing.%20We%20hope%20Long-VITA%0Acan%20serve%20as%20a%20competitive%20baseline%20and%20offer%20valuable%20insights%20for%20the%0Aopen-source%20community%20in%20advancing%20long-context%20multi-modal%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05177v1&entry.124074799=Read"},
{"title": "Efficient Few-Shot Continual Learning in Vision-Language Models", "author": "Aristeidis Panos and Rahaf Aljundi and Daniel Olmeda Reino and Richard E. Turner", "abstract": "  Vision-language models (VLMs) excel in tasks such as visual question\nanswering and image captioning. However, VLMs are often limited by their use of\npretrained image encoders, like CLIP, leading to image understanding errors\nthat hinder overall performance. On top of that, real-world applications often\nrequire the model to be continuously adapted as new and often limited data\ncontinuously arrive. To address this, we propose LoRSU (Low-Rank Adaptation\nwith Structured Updates), a robust and computationally efficient method for\nselectively updating image encoders within VLMs. LoRSU introduces structured\nand localized parameter updates, effectively correcting performance on\npreviously error-prone data while preserving the model's general robustness.\nOur approach leverages theoretical insights to identify and update only the\nmost critical parameters, achieving significant resource efficiency.\nSpecifically, we demonstrate that LoRSU reduces computational overhead by over\n25x compared to full VLM updates, without sacrificing performance. Experimental\nresults on VQA tasks in the few-shot continual learning setting, validate\nLoRSU's scalability, efficiency, and effectiveness, making it a compelling\nsolution for image encoder adaptation in resource-constrained environments.\n", "link": "http://arxiv.org/abs/2502.04098v2", "date": "2025-02-07", "relevancy": 2.9022, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5904}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5904}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Few-Shot%20Continual%20Learning%20in%20Vision-Language%20Models&body=Title%3A%20Efficient%20Few-Shot%20Continual%20Learning%20in%20Vision-Language%20Models%0AAuthor%3A%20Aristeidis%20Panos%20and%20Rahaf%20Aljundi%20and%20Daniel%20Olmeda%20Reino%20and%20Richard%20E.%20Turner%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20excel%20in%20tasks%20such%20as%20visual%20question%0Aanswering%20and%20image%20captioning.%20However%2C%20VLMs%20are%20often%20limited%20by%20their%20use%20of%0Apretrained%20image%20encoders%2C%20like%20CLIP%2C%20leading%20to%20image%20understanding%20errors%0Athat%20hinder%20overall%20performance.%20On%20top%20of%20that%2C%20real-world%20applications%20often%0Arequire%20the%20model%20to%20be%20continuously%20adapted%20as%20new%20and%20often%20limited%20data%0Acontinuously%20arrive.%20To%20address%20this%2C%20we%20propose%20LoRSU%20%28Low-Rank%20Adaptation%0Awith%20Structured%20Updates%29%2C%20a%20robust%20and%20computationally%20efficient%20method%20for%0Aselectively%20updating%20image%20encoders%20within%20VLMs.%20LoRSU%20introduces%20structured%0Aand%20localized%20parameter%20updates%2C%20effectively%20correcting%20performance%20on%0Apreviously%20error-prone%20data%20while%20preserving%20the%20model%27s%20general%20robustness.%0AOur%20approach%20leverages%20theoretical%20insights%20to%20identify%20and%20update%20only%20the%0Amost%20critical%20parameters%2C%20achieving%20significant%20resource%20efficiency.%0ASpecifically%2C%20we%20demonstrate%20that%20LoRSU%20reduces%20computational%20overhead%20by%20over%0A25x%20compared%20to%20full%20VLM%20updates%2C%20without%20sacrificing%20performance.%20Experimental%0Aresults%20on%20VQA%20tasks%20in%20the%20few-shot%20continual%20learning%20setting%2C%20validate%0ALoRSU%27s%20scalability%2C%20efficiency%2C%20and%20effectiveness%2C%20making%20it%20a%20compelling%0Asolution%20for%20image%20encoder%20adaptation%20in%20resource-constrained%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04098v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Few-Shot%2520Continual%2520Learning%2520in%2520Vision-Language%2520Models%26entry.906535625%3DAristeidis%2520Panos%2520and%2520Rahaf%2520Aljundi%2520and%2520Daniel%2520Olmeda%2520Reino%2520and%2520Richard%2520E.%2520Turner%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520excel%2520in%2520tasks%2520such%2520as%2520visual%2520question%250Aanswering%2520and%2520image%2520captioning.%2520However%252C%2520VLMs%2520are%2520often%2520limited%2520by%2520their%2520use%2520of%250Apretrained%2520image%2520encoders%252C%2520like%2520CLIP%252C%2520leading%2520to%2520image%2520understanding%2520errors%250Athat%2520hinder%2520overall%2520performance.%2520On%2520top%2520of%2520that%252C%2520real-world%2520applications%2520often%250Arequire%2520the%2520model%2520to%2520be%2520continuously%2520adapted%2520as%2520new%2520and%2520often%2520limited%2520data%250Acontinuously%2520arrive.%2520To%2520address%2520this%252C%2520we%2520propose%2520LoRSU%2520%2528Low-Rank%2520Adaptation%250Awith%2520Structured%2520Updates%2529%252C%2520a%2520robust%2520and%2520computationally%2520efficient%2520method%2520for%250Aselectively%2520updating%2520image%2520encoders%2520within%2520VLMs.%2520LoRSU%2520introduces%2520structured%250Aand%2520localized%2520parameter%2520updates%252C%2520effectively%2520correcting%2520performance%2520on%250Apreviously%2520error-prone%2520data%2520while%2520preserving%2520the%2520model%2527s%2520general%2520robustness.%250AOur%2520approach%2520leverages%2520theoretical%2520insights%2520to%2520identify%2520and%2520update%2520only%2520the%250Amost%2520critical%2520parameters%252C%2520achieving%2520significant%2520resource%2520efficiency.%250ASpecifically%252C%2520we%2520demonstrate%2520that%2520LoRSU%2520reduces%2520computational%2520overhead%2520by%2520over%250A25x%2520compared%2520to%2520full%2520VLM%2520updates%252C%2520without%2520sacrificing%2520performance.%2520Experimental%250Aresults%2520on%2520VQA%2520tasks%2520in%2520the%2520few-shot%2520continual%2520learning%2520setting%252C%2520validate%250ALoRSU%2527s%2520scalability%252C%2520efficiency%252C%2520and%2520effectiveness%252C%2520making%2520it%2520a%2520compelling%250Asolution%2520for%2520image%2520encoder%2520adaptation%2520in%2520resource-constrained%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04098v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Few-Shot%20Continual%20Learning%20in%20Vision-Language%20Models&entry.906535625=Aristeidis%20Panos%20and%20Rahaf%20Aljundi%20and%20Daniel%20Olmeda%20Reino%20and%20Richard%20E.%20Turner&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20excel%20in%20tasks%20such%20as%20visual%20question%0Aanswering%20and%20image%20captioning.%20However%2C%20VLMs%20are%20often%20limited%20by%20their%20use%20of%0Apretrained%20image%20encoders%2C%20like%20CLIP%2C%20leading%20to%20image%20understanding%20errors%0Athat%20hinder%20overall%20performance.%20On%20top%20of%20that%2C%20real-world%20applications%20often%0Arequire%20the%20model%20to%20be%20continuously%20adapted%20as%20new%20and%20often%20limited%20data%0Acontinuously%20arrive.%20To%20address%20this%2C%20we%20propose%20LoRSU%20%28Low-Rank%20Adaptation%0Awith%20Structured%20Updates%29%2C%20a%20robust%20and%20computationally%20efficient%20method%20for%0Aselectively%20updating%20image%20encoders%20within%20VLMs.%20LoRSU%20introduces%20structured%0Aand%20localized%20parameter%20updates%2C%20effectively%20correcting%20performance%20on%0Apreviously%20error-prone%20data%20while%20preserving%20the%20model%27s%20general%20robustness.%0AOur%20approach%20leverages%20theoretical%20insights%20to%20identify%20and%20update%20only%20the%0Amost%20critical%20parameters%2C%20achieving%20significant%20resource%20efficiency.%0ASpecifically%2C%20we%20demonstrate%20that%20LoRSU%20reduces%20computational%20overhead%20by%20over%0A25x%20compared%20to%20full%20VLM%20updates%2C%20without%20sacrificing%20performance.%20Experimental%0Aresults%20on%20VQA%20tasks%20in%20the%20few-shot%20continual%20learning%20setting%2C%20validate%0ALoRSU%27s%20scalability%2C%20efficiency%2C%20and%20effectiveness%2C%20making%20it%20a%20compelling%0Asolution%20for%20image%20encoder%20adaptation%20in%20resource-constrained%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04098v2&entry.124074799=Read"},
{"title": "A-VL: Adaptive Attention for Large Vision-Language Models", "author": "Junyang Zhang and Mu Yuan and Ruiguang Zhong and Puhan Luo and Huiyou Zhan and Ningkang Zhang and Chengchen Hu and Xiangyang Li", "abstract": "  The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance.\n", "link": "http://arxiv.org/abs/2409.14846v2", "date": "2025-02-07", "relevancy": 2.8642, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5775}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5705}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A-VL%3A%20Adaptive%20Attention%20for%20Large%20Vision-Language%20Models&body=Title%3A%20A-VL%3A%20Adaptive%20Attention%20for%20Large%20Vision-Language%20Models%0AAuthor%3A%20Junyang%20Zhang%20and%20Mu%20Yuan%20and%20Ruiguang%20Zhong%20and%20Puhan%20Luo%20and%20Huiyou%20Zhan%20and%20Ningkang%20Zhang%20and%20Chengchen%20Hu%20and%20Xiangyang%20Li%0AAbstract%3A%20%20%20The%20Large%20Vision-Language%20Model%20%28LVLM%29%20integrates%20computer%20vision%20and%20natural%0Alanguage%20processing%20techniques%2C%20offering%20substantial%20application%20potential.%0AHowever%2C%20these%20models%20demand%20extensive%20resources%20during%20inference.%20Adaptive%0Aattention%20techniques%20can%20dynamically%20reduce%20computational%20redundancy%20and%20thus%0Aimprove%20efficiency.%20Although%20current%20adaptive%20attention%20methods%20significantly%0Areduce%20the%20memory%20requirements%20of%20Transformer-based%20language%20models%2C%20they%20are%0Anot%20tailored%20for%20LVLMs.%20We%20observe%20that%20LVLMs%20generate%20responses%20from%20both%0Aremote%20image%20tokens%20and%20local%20text%20tokens%2C%20and%20different%20modalities%20have%0Adifferent%20attention%20patterns.%20This%20observation%20inspires%20us%20to%20manage%20the%0Aattention%20for%20each%20modality%20separately.%20Specifically%2C%20for%20visual%20input%2C%20we%0Astore%20the%20cache%20of%20potentially%20useful%20information%20but%20only%20compute%20the%20most%0Acritical%20parts.%20For%20language%20input%2C%20we%20care%20more%20about%20local%20information.%20Based%0Aon%20our%20observation%20and%20analysis%20of%20vision-language%20attention%20patterns%2C%20we%0Adevelop%20A-VL%2C%20a%20plug-and-play%20adaptive%20attention%20tailored%20for%20LVLM%20inference.%0AExtensive%20evaluations%20on%20three%20vision-language%20tasks%20and%20five%20datasets%20show%20the%0Aeffectiveness%20of%20our%20designs.%20Our%20approach%20A-VL%20outperforms%20existing%20adaptive%0Aattention%20methods%20in%20reducing%20memory%20usage%20and%20computational%20load%20without%0Acompromising%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.14846v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA-VL%253A%2520Adaptive%2520Attention%2520for%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DJunyang%2520Zhang%2520and%2520Mu%2520Yuan%2520and%2520Ruiguang%2520Zhong%2520and%2520Puhan%2520Luo%2520and%2520Huiyou%2520Zhan%2520and%2520Ningkang%2520Zhang%2520and%2520Chengchen%2520Hu%2520and%2520Xiangyang%2520Li%26entry.1292438233%3D%2520%2520The%2520Large%2520Vision-Language%2520Model%2520%2528LVLM%2529%2520integrates%2520computer%2520vision%2520and%2520natural%250Alanguage%2520processing%2520techniques%252C%2520offering%2520substantial%2520application%2520potential.%250AHowever%252C%2520these%2520models%2520demand%2520extensive%2520resources%2520during%2520inference.%2520Adaptive%250Aattention%2520techniques%2520can%2520dynamically%2520reduce%2520computational%2520redundancy%2520and%2520thus%250Aimprove%2520efficiency.%2520Although%2520current%2520adaptive%2520attention%2520methods%2520significantly%250Areduce%2520the%2520memory%2520requirements%2520of%2520Transformer-based%2520language%2520models%252C%2520they%2520are%250Anot%2520tailored%2520for%2520LVLMs.%2520We%2520observe%2520that%2520LVLMs%2520generate%2520responses%2520from%2520both%250Aremote%2520image%2520tokens%2520and%2520local%2520text%2520tokens%252C%2520and%2520different%2520modalities%2520have%250Adifferent%2520attention%2520patterns.%2520This%2520observation%2520inspires%2520us%2520to%2520manage%2520the%250Aattention%2520for%2520each%2520modality%2520separately.%2520Specifically%252C%2520for%2520visual%2520input%252C%2520we%250Astore%2520the%2520cache%2520of%2520potentially%2520useful%2520information%2520but%2520only%2520compute%2520the%2520most%250Acritical%2520parts.%2520For%2520language%2520input%252C%2520we%2520care%2520more%2520about%2520local%2520information.%2520Based%250Aon%2520our%2520observation%2520and%2520analysis%2520of%2520vision-language%2520attention%2520patterns%252C%2520we%250Adevelop%2520A-VL%252C%2520a%2520plug-and-play%2520adaptive%2520attention%2520tailored%2520for%2520LVLM%2520inference.%250AExtensive%2520evaluations%2520on%2520three%2520vision-language%2520tasks%2520and%2520five%2520datasets%2520show%2520the%250Aeffectiveness%2520of%2520our%2520designs.%2520Our%2520approach%2520A-VL%2520outperforms%2520existing%2520adaptive%250Aattention%2520methods%2520in%2520reducing%2520memory%2520usage%2520and%2520computational%2520load%2520without%250Acompromising%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.14846v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A-VL%3A%20Adaptive%20Attention%20for%20Large%20Vision-Language%20Models&entry.906535625=Junyang%20Zhang%20and%20Mu%20Yuan%20and%20Ruiguang%20Zhong%20and%20Puhan%20Luo%20and%20Huiyou%20Zhan%20and%20Ningkang%20Zhang%20and%20Chengchen%20Hu%20and%20Xiangyang%20Li&entry.1292438233=%20%20The%20Large%20Vision-Language%20Model%20%28LVLM%29%20integrates%20computer%20vision%20and%20natural%0Alanguage%20processing%20techniques%2C%20offering%20substantial%20application%20potential.%0AHowever%2C%20these%20models%20demand%20extensive%20resources%20during%20inference.%20Adaptive%0Aattention%20techniques%20can%20dynamically%20reduce%20computational%20redundancy%20and%20thus%0Aimprove%20efficiency.%20Although%20current%20adaptive%20attention%20methods%20significantly%0Areduce%20the%20memory%20requirements%20of%20Transformer-based%20language%20models%2C%20they%20are%0Anot%20tailored%20for%20LVLMs.%20We%20observe%20that%20LVLMs%20generate%20responses%20from%20both%0Aremote%20image%20tokens%20and%20local%20text%20tokens%2C%20and%20different%20modalities%20have%0Adifferent%20attention%20patterns.%20This%20observation%20inspires%20us%20to%20manage%20the%0Aattention%20for%20each%20modality%20separately.%20Specifically%2C%20for%20visual%20input%2C%20we%0Astore%20the%20cache%20of%20potentially%20useful%20information%20but%20only%20compute%20the%20most%0Acritical%20parts.%20For%20language%20input%2C%20we%20care%20more%20about%20local%20information.%20Based%0Aon%20our%20observation%20and%20analysis%20of%20vision-language%20attention%20patterns%2C%20we%0Adevelop%20A-VL%2C%20a%20plug-and-play%20adaptive%20attention%20tailored%20for%20LVLM%20inference.%0AExtensive%20evaluations%20on%20three%20vision-language%20tasks%20and%20five%20datasets%20show%20the%0Aeffectiveness%20of%20our%20designs.%20Our%20approach%20A-VL%20outperforms%20existing%20adaptive%0Aattention%20methods%20in%20reducing%20memory%20usage%20and%20computational%20load%20without%0Acompromising%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.14846v2&entry.124074799=Read"},
{"title": "IDPro: Flexible Interactive Video Object Segmentation by ID-queried\n  Concurrent Propagation", "author": "Kexin Li and Tao Jiang and Zongxin Yang and Yi Yang and Yueting Zhuang and Jun Xiao", "abstract": "  Interactive Video Object Segmentation (iVOS) is a challenging task that\nrequires real-time human-computer interaction. To improve the user experience,\nit is important to consider the user's input habits, segmentation quality,\nrunning time and memory consumption.However, existing methods compromise user\nexperience with single input mode and slow running speed. Specifically, these\nmethods only allow the user to interact with one single frame, which limits the\nexpression of the user's intent.To overcome these limitations and better align\nwith people's usage habits, we propose a framework that can accept multiple\nframes simultaneously and explore synergistic interaction across frames (SIAF).\nConcretely, we designed the Across-Frame Interaction Module that enables users\nto annotate different objects freely on multiple frames. The AFI module will\nmigrate scribble information among multiple interactive frames and generate\nmulti-frame masks. Additionally, we employ the id-queried mechanism to process\nmultiple objects in batches. Furthermore, for a more efficient propagation and\nlightweight model, we design a truncated re-propagation strategy to replace the\nprevious multi-round fusion module, which employs an across-round memory that\nstores important interaction information. Our SwinB-SIAF achieves new\nstate-of-the-art performance on DAVIS 2017 (89.6%, J&F@60). Moreover, our\nR50-SIAF is more than 3 faster than the state-of-the-art competitor under\nchallenging multi-object scenarios.\n", "link": "http://arxiv.org/abs/2401.12480v3", "date": "2025-02-07", "relevancy": 2.8315, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5759}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5678}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IDPro%3A%20Flexible%20Interactive%20Video%20Object%20Segmentation%20by%20ID-queried%0A%20%20Concurrent%20Propagation&body=Title%3A%20IDPro%3A%20Flexible%20Interactive%20Video%20Object%20Segmentation%20by%20ID-queried%0A%20%20Concurrent%20Propagation%0AAuthor%3A%20Kexin%20Li%20and%20Tao%20Jiang%20and%20Zongxin%20Yang%20and%20Yi%20Yang%20and%20Yueting%20Zhuang%20and%20Jun%20Xiao%0AAbstract%3A%20%20%20Interactive%20Video%20Object%20Segmentation%20%28iVOS%29%20is%20a%20challenging%20task%20that%0Arequires%20real-time%20human-computer%20interaction.%20To%20improve%20the%20user%20experience%2C%0Ait%20is%20important%20to%20consider%20the%20user%27s%20input%20habits%2C%20segmentation%20quality%2C%0Arunning%20time%20and%20memory%20consumption.However%2C%20existing%20methods%20compromise%20user%0Aexperience%20with%20single%20input%20mode%20and%20slow%20running%20speed.%20Specifically%2C%20these%0Amethods%20only%20allow%20the%20user%20to%20interact%20with%20one%20single%20frame%2C%20which%20limits%20the%0Aexpression%20of%20the%20user%27s%20intent.To%20overcome%20these%20limitations%20and%20better%20align%0Awith%20people%27s%20usage%20habits%2C%20we%20propose%20a%20framework%20that%20can%20accept%20multiple%0Aframes%20simultaneously%20and%20explore%20synergistic%20interaction%20across%20frames%20%28SIAF%29.%0AConcretely%2C%20we%20designed%20the%20Across-Frame%20Interaction%20Module%20that%20enables%20users%0Ato%20annotate%20different%20objects%20freely%20on%20multiple%20frames.%20The%20AFI%20module%20will%0Amigrate%20scribble%20information%20among%20multiple%20interactive%20frames%20and%20generate%0Amulti-frame%20masks.%20Additionally%2C%20we%20employ%20the%20id-queried%20mechanism%20to%20process%0Amultiple%20objects%20in%20batches.%20Furthermore%2C%20for%20a%20more%20efficient%20propagation%20and%0Alightweight%20model%2C%20we%20design%20a%20truncated%20re-propagation%20strategy%20to%20replace%20the%0Aprevious%20multi-round%20fusion%20module%2C%20which%20employs%20an%20across-round%20memory%20that%0Astores%20important%20interaction%20information.%20Our%20SwinB-SIAF%20achieves%20new%0Astate-of-the-art%20performance%20on%20DAVIS%202017%20%2889.6%25%2C%20J%26F%4060%29.%20Moreover%2C%20our%0AR50-SIAF%20is%20more%20than%203%20faster%20than%20the%20state-of-the-art%20competitor%20under%0Achallenging%20multi-object%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.12480v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIDPro%253A%2520Flexible%2520Interactive%2520Video%2520Object%2520Segmentation%2520by%2520ID-queried%250A%2520%2520Concurrent%2520Propagation%26entry.906535625%3DKexin%2520Li%2520and%2520Tao%2520Jiang%2520and%2520Zongxin%2520Yang%2520and%2520Yi%2520Yang%2520and%2520Yueting%2520Zhuang%2520and%2520Jun%2520Xiao%26entry.1292438233%3D%2520%2520Interactive%2520Video%2520Object%2520Segmentation%2520%2528iVOS%2529%2520is%2520a%2520challenging%2520task%2520that%250Arequires%2520real-time%2520human-computer%2520interaction.%2520To%2520improve%2520the%2520user%2520experience%252C%250Ait%2520is%2520important%2520to%2520consider%2520the%2520user%2527s%2520input%2520habits%252C%2520segmentation%2520quality%252C%250Arunning%2520time%2520and%2520memory%2520consumption.However%252C%2520existing%2520methods%2520compromise%2520user%250Aexperience%2520with%2520single%2520input%2520mode%2520and%2520slow%2520running%2520speed.%2520Specifically%252C%2520these%250Amethods%2520only%2520allow%2520the%2520user%2520to%2520interact%2520with%2520one%2520single%2520frame%252C%2520which%2520limits%2520the%250Aexpression%2520of%2520the%2520user%2527s%2520intent.To%2520overcome%2520these%2520limitations%2520and%2520better%2520align%250Awith%2520people%2527s%2520usage%2520habits%252C%2520we%2520propose%2520a%2520framework%2520that%2520can%2520accept%2520multiple%250Aframes%2520simultaneously%2520and%2520explore%2520synergistic%2520interaction%2520across%2520frames%2520%2528SIAF%2529.%250AConcretely%252C%2520we%2520designed%2520the%2520Across-Frame%2520Interaction%2520Module%2520that%2520enables%2520users%250Ato%2520annotate%2520different%2520objects%2520freely%2520on%2520multiple%2520frames.%2520The%2520AFI%2520module%2520will%250Amigrate%2520scribble%2520information%2520among%2520multiple%2520interactive%2520frames%2520and%2520generate%250Amulti-frame%2520masks.%2520Additionally%252C%2520we%2520employ%2520the%2520id-queried%2520mechanism%2520to%2520process%250Amultiple%2520objects%2520in%2520batches.%2520Furthermore%252C%2520for%2520a%2520more%2520efficient%2520propagation%2520and%250Alightweight%2520model%252C%2520we%2520design%2520a%2520truncated%2520re-propagation%2520strategy%2520to%2520replace%2520the%250Aprevious%2520multi-round%2520fusion%2520module%252C%2520which%2520employs%2520an%2520across-round%2520memory%2520that%250Astores%2520important%2520interaction%2520information.%2520Our%2520SwinB-SIAF%2520achieves%2520new%250Astate-of-the-art%2520performance%2520on%2520DAVIS%25202017%2520%252889.6%2525%252C%2520J%2526F%254060%2529.%2520Moreover%252C%2520our%250AR50-SIAF%2520is%2520more%2520than%25203%2520faster%2520than%2520the%2520state-of-the-art%2520competitor%2520under%250Achallenging%2520multi-object%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.12480v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IDPro%3A%20Flexible%20Interactive%20Video%20Object%20Segmentation%20by%20ID-queried%0A%20%20Concurrent%20Propagation&entry.906535625=Kexin%20Li%20and%20Tao%20Jiang%20and%20Zongxin%20Yang%20and%20Yi%20Yang%20and%20Yueting%20Zhuang%20and%20Jun%20Xiao&entry.1292438233=%20%20Interactive%20Video%20Object%20Segmentation%20%28iVOS%29%20is%20a%20challenging%20task%20that%0Arequires%20real-time%20human-computer%20interaction.%20To%20improve%20the%20user%20experience%2C%0Ait%20is%20important%20to%20consider%20the%20user%27s%20input%20habits%2C%20segmentation%20quality%2C%0Arunning%20time%20and%20memory%20consumption.However%2C%20existing%20methods%20compromise%20user%0Aexperience%20with%20single%20input%20mode%20and%20slow%20running%20speed.%20Specifically%2C%20these%0Amethods%20only%20allow%20the%20user%20to%20interact%20with%20one%20single%20frame%2C%20which%20limits%20the%0Aexpression%20of%20the%20user%27s%20intent.To%20overcome%20these%20limitations%20and%20better%20align%0Awith%20people%27s%20usage%20habits%2C%20we%20propose%20a%20framework%20that%20can%20accept%20multiple%0Aframes%20simultaneously%20and%20explore%20synergistic%20interaction%20across%20frames%20%28SIAF%29.%0AConcretely%2C%20we%20designed%20the%20Across-Frame%20Interaction%20Module%20that%20enables%20users%0Ato%20annotate%20different%20objects%20freely%20on%20multiple%20frames.%20The%20AFI%20module%20will%0Amigrate%20scribble%20information%20among%20multiple%20interactive%20frames%20and%20generate%0Amulti-frame%20masks.%20Additionally%2C%20we%20employ%20the%20id-queried%20mechanism%20to%20process%0Amultiple%20objects%20in%20batches.%20Furthermore%2C%20for%20a%20more%20efficient%20propagation%20and%0Alightweight%20model%2C%20we%20design%20a%20truncated%20re-propagation%20strategy%20to%20replace%20the%0Aprevious%20multi-round%20fusion%20module%2C%20which%20employs%20an%20across-round%20memory%20that%0Astores%20important%20interaction%20information.%20Our%20SwinB-SIAF%20achieves%20new%0Astate-of-the-art%20performance%20on%20DAVIS%202017%20%2889.6%25%2C%20J%26F%4060%29.%20Moreover%2C%20our%0AR50-SIAF%20is%20more%20than%203%20faster%20than%20the%20state-of-the-art%20competitor%20under%0Achallenging%20multi-object%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.12480v3&entry.124074799=Read"},
{"title": "Gaze-Guided Robotic Vascular Ultrasound Leveraging Human Intention\n  Estimation", "author": "Yuan Bi and Yang Su and Nassir Navab and Zhongliang Jiang", "abstract": "  Medical ultrasound has been widely used to examine vascular structure in\nmodern clinical practice. However, traditional ultrasound examination often\nfaces challenges related to inter- and intra-operator variation. The robotic\nultrasound system (RUSS) appears as a potential solution for such challenges\nbecause of its superiority in stability and reproducibility. Given the complex\nanatomy of human vasculature, multiple vessels often appear in ultrasound\nimages, or a single vessel bifurcates into branches, complicating the\nexamination process. To tackle this challenge, this work presents a gaze-guided\nRUSS for vascular applications. A gaze tracker captures the eye movements of\nthe operator. The extracted gaze signal guides the RUSS to follow the correct\nvessel when it bifurcates. Additionally, a gaze-guided segmentation network is\nproposed to enhance segmentation robustness by exploiting gaze information.\nHowever, gaze signals are often noisy, requiring interpretation to accurately\ndiscern the operator's true intentions. To this end, this study proposes a\nstabilization module to process raw gaze data. The inferred attention heatmap\nis utilized as a region proposal to aid segmentation and serve as a trigger\nsignal when the operator needs to adjust the scanning target, such as when a\nbifurcation appears. To ensure appropriate contact between the probe and\nsurface during scanning, an automatic ultrasound confidence-based orientation\ncorrection method is developed. In experiments, we demonstrated the efficiency\nof the proposed gaze-guided segmentation pipeline by comparing it with other\nmethods. Besides, the performance of the proposed gaze-guided RUSS was also\nvalidated as a whole on a realistic arm phantom with an uneven surface.\n", "link": "http://arxiv.org/abs/2502.05053v1", "date": "2025-02-07", "relevancy": 2.7719, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.561}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5521}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.55}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaze-Guided%20Robotic%20Vascular%20Ultrasound%20Leveraging%20Human%20Intention%0A%20%20Estimation&body=Title%3A%20Gaze-Guided%20Robotic%20Vascular%20Ultrasound%20Leveraging%20Human%20Intention%0A%20%20Estimation%0AAuthor%3A%20Yuan%20Bi%20and%20Yang%20Su%20and%20Nassir%20Navab%20and%20Zhongliang%20Jiang%0AAbstract%3A%20%20%20Medical%20ultrasound%20has%20been%20widely%20used%20to%20examine%20vascular%20structure%20in%0Amodern%20clinical%20practice.%20However%2C%20traditional%20ultrasound%20examination%20often%0Afaces%20challenges%20related%20to%20inter-%20and%20intra-operator%20variation.%20The%20robotic%0Aultrasound%20system%20%28RUSS%29%20appears%20as%20a%20potential%20solution%20for%20such%20challenges%0Abecause%20of%20its%20superiority%20in%20stability%20and%20reproducibility.%20Given%20the%20complex%0Aanatomy%20of%20human%20vasculature%2C%20multiple%20vessels%20often%20appear%20in%20ultrasound%0Aimages%2C%20or%20a%20single%20vessel%20bifurcates%20into%20branches%2C%20complicating%20the%0Aexamination%20process.%20To%20tackle%20this%20challenge%2C%20this%20work%20presents%20a%20gaze-guided%0ARUSS%20for%20vascular%20applications.%20A%20gaze%20tracker%20captures%20the%20eye%20movements%20of%0Athe%20operator.%20The%20extracted%20gaze%20signal%20guides%20the%20RUSS%20to%20follow%20the%20correct%0Avessel%20when%20it%20bifurcates.%20Additionally%2C%20a%20gaze-guided%20segmentation%20network%20is%0Aproposed%20to%20enhance%20segmentation%20robustness%20by%20exploiting%20gaze%20information.%0AHowever%2C%20gaze%20signals%20are%20often%20noisy%2C%20requiring%20interpretation%20to%20accurately%0Adiscern%20the%20operator%27s%20true%20intentions.%20To%20this%20end%2C%20this%20study%20proposes%20a%0Astabilization%20module%20to%20process%20raw%20gaze%20data.%20The%20inferred%20attention%20heatmap%0Ais%20utilized%20as%20a%20region%20proposal%20to%20aid%20segmentation%20and%20serve%20as%20a%20trigger%0Asignal%20when%20the%20operator%20needs%20to%20adjust%20the%20scanning%20target%2C%20such%20as%20when%20a%0Abifurcation%20appears.%20To%20ensure%20appropriate%20contact%20between%20the%20probe%20and%0Asurface%20during%20scanning%2C%20an%20automatic%20ultrasound%20confidence-based%20orientation%0Acorrection%20method%20is%20developed.%20In%20experiments%2C%20we%20demonstrated%20the%20efficiency%0Aof%20the%20proposed%20gaze-guided%20segmentation%20pipeline%20by%20comparing%20it%20with%20other%0Amethods.%20Besides%2C%20the%20performance%20of%20the%20proposed%20gaze-guided%20RUSS%20was%20also%0Avalidated%20as%20a%20whole%20on%20a%20realistic%20arm%20phantom%20with%20an%20uneven%20surface.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05053v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaze-Guided%2520Robotic%2520Vascular%2520Ultrasound%2520Leveraging%2520Human%2520Intention%250A%2520%2520Estimation%26entry.906535625%3DYuan%2520Bi%2520and%2520Yang%2520Su%2520and%2520Nassir%2520Navab%2520and%2520Zhongliang%2520Jiang%26entry.1292438233%3D%2520%2520Medical%2520ultrasound%2520has%2520been%2520widely%2520used%2520to%2520examine%2520vascular%2520structure%2520in%250Amodern%2520clinical%2520practice.%2520However%252C%2520traditional%2520ultrasound%2520examination%2520often%250Afaces%2520challenges%2520related%2520to%2520inter-%2520and%2520intra-operator%2520variation.%2520The%2520robotic%250Aultrasound%2520system%2520%2528RUSS%2529%2520appears%2520as%2520a%2520potential%2520solution%2520for%2520such%2520challenges%250Abecause%2520of%2520its%2520superiority%2520in%2520stability%2520and%2520reproducibility.%2520Given%2520the%2520complex%250Aanatomy%2520of%2520human%2520vasculature%252C%2520multiple%2520vessels%2520often%2520appear%2520in%2520ultrasound%250Aimages%252C%2520or%2520a%2520single%2520vessel%2520bifurcates%2520into%2520branches%252C%2520complicating%2520the%250Aexamination%2520process.%2520To%2520tackle%2520this%2520challenge%252C%2520this%2520work%2520presents%2520a%2520gaze-guided%250ARUSS%2520for%2520vascular%2520applications.%2520A%2520gaze%2520tracker%2520captures%2520the%2520eye%2520movements%2520of%250Athe%2520operator.%2520The%2520extracted%2520gaze%2520signal%2520guides%2520the%2520RUSS%2520to%2520follow%2520the%2520correct%250Avessel%2520when%2520it%2520bifurcates.%2520Additionally%252C%2520a%2520gaze-guided%2520segmentation%2520network%2520is%250Aproposed%2520to%2520enhance%2520segmentation%2520robustness%2520by%2520exploiting%2520gaze%2520information.%250AHowever%252C%2520gaze%2520signals%2520are%2520often%2520noisy%252C%2520requiring%2520interpretation%2520to%2520accurately%250Adiscern%2520the%2520operator%2527s%2520true%2520intentions.%2520To%2520this%2520end%252C%2520this%2520study%2520proposes%2520a%250Astabilization%2520module%2520to%2520process%2520raw%2520gaze%2520data.%2520The%2520inferred%2520attention%2520heatmap%250Ais%2520utilized%2520as%2520a%2520region%2520proposal%2520to%2520aid%2520segmentation%2520and%2520serve%2520as%2520a%2520trigger%250Asignal%2520when%2520the%2520operator%2520needs%2520to%2520adjust%2520the%2520scanning%2520target%252C%2520such%2520as%2520when%2520a%250Abifurcation%2520appears.%2520To%2520ensure%2520appropriate%2520contact%2520between%2520the%2520probe%2520and%250Asurface%2520during%2520scanning%252C%2520an%2520automatic%2520ultrasound%2520confidence-based%2520orientation%250Acorrection%2520method%2520is%2520developed.%2520In%2520experiments%252C%2520we%2520demonstrated%2520the%2520efficiency%250Aof%2520the%2520proposed%2520gaze-guided%2520segmentation%2520pipeline%2520by%2520comparing%2520it%2520with%2520other%250Amethods.%2520Besides%252C%2520the%2520performance%2520of%2520the%2520proposed%2520gaze-guided%2520RUSS%2520was%2520also%250Avalidated%2520as%2520a%2520whole%2520on%2520a%2520realistic%2520arm%2520phantom%2520with%2520an%2520uneven%2520surface.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05053v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaze-Guided%20Robotic%20Vascular%20Ultrasound%20Leveraging%20Human%20Intention%0A%20%20Estimation&entry.906535625=Yuan%20Bi%20and%20Yang%20Su%20and%20Nassir%20Navab%20and%20Zhongliang%20Jiang&entry.1292438233=%20%20Medical%20ultrasound%20has%20been%20widely%20used%20to%20examine%20vascular%20structure%20in%0Amodern%20clinical%20practice.%20However%2C%20traditional%20ultrasound%20examination%20often%0Afaces%20challenges%20related%20to%20inter-%20and%20intra-operator%20variation.%20The%20robotic%0Aultrasound%20system%20%28RUSS%29%20appears%20as%20a%20potential%20solution%20for%20such%20challenges%0Abecause%20of%20its%20superiority%20in%20stability%20and%20reproducibility.%20Given%20the%20complex%0Aanatomy%20of%20human%20vasculature%2C%20multiple%20vessels%20often%20appear%20in%20ultrasound%0Aimages%2C%20or%20a%20single%20vessel%20bifurcates%20into%20branches%2C%20complicating%20the%0Aexamination%20process.%20To%20tackle%20this%20challenge%2C%20this%20work%20presents%20a%20gaze-guided%0ARUSS%20for%20vascular%20applications.%20A%20gaze%20tracker%20captures%20the%20eye%20movements%20of%0Athe%20operator.%20The%20extracted%20gaze%20signal%20guides%20the%20RUSS%20to%20follow%20the%20correct%0Avessel%20when%20it%20bifurcates.%20Additionally%2C%20a%20gaze-guided%20segmentation%20network%20is%0Aproposed%20to%20enhance%20segmentation%20robustness%20by%20exploiting%20gaze%20information.%0AHowever%2C%20gaze%20signals%20are%20often%20noisy%2C%20requiring%20interpretation%20to%20accurately%0Adiscern%20the%20operator%27s%20true%20intentions.%20To%20this%20end%2C%20this%20study%20proposes%20a%0Astabilization%20module%20to%20process%20raw%20gaze%20data.%20The%20inferred%20attention%20heatmap%0Ais%20utilized%20as%20a%20region%20proposal%20to%20aid%20segmentation%20and%20serve%20as%20a%20trigger%0Asignal%20when%20the%20operator%20needs%20to%20adjust%20the%20scanning%20target%2C%20such%20as%20when%20a%0Abifurcation%20appears.%20To%20ensure%20appropriate%20contact%20between%20the%20probe%20and%0Asurface%20during%20scanning%2C%20an%20automatic%20ultrasound%20confidence-based%20orientation%0Acorrection%20method%20is%20developed.%20In%20experiments%2C%20we%20demonstrated%20the%20efficiency%0Aof%20the%20proposed%20gaze-guided%20segmentation%20pipeline%20by%20comparing%20it%20with%20other%0Amethods.%20Besides%2C%20the%20performance%20of%20the%20proposed%20gaze-guided%20RUSS%20was%20also%0Avalidated%20as%20a%20whole%20on%20a%20realistic%20arm%20phantom%20with%20an%20uneven%20surface.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05053v1&entry.124074799=Read"},
{"title": "Enhancing medical vision-language contrastive learning via\n  inter-matching relation modelling", "author": "Mingjian Li and Mingyuan Meng and Michael Fulham and David Dagan Feng and Lei Bi and Jinman Kim", "abstract": "  Medical image representations can be learned through medical vision-language\ncontrastive learning (mVLCL) where medical imaging reports are used as weak\nsupervision through image-text alignment. These learned image representations\ncan be transferred to and benefit various downstream medical vision tasks such\nas disease classification and segmentation. Recent mVLCL methods attempt to\nalign image sub-regions and the report keywords as local-matchings. However,\nthese methods aggregate all local-matchings via simple pooling operations while\nignoring the inherent relations between them. These methods therefore fail to\nreason between local-matchings that are semantically related, e.g.,\nlocal-matchings that correspond to the disease word and the location word\n(semantic-relations), and also fail to differentiate such clinically important\nlocal-matchings from others that correspond to less meaningful words, e.g.,\nconjunction words (importance-relations). Hence, we propose a mVLCL method that\nmodels the inter-matching relations between local-matchings via a\nrelation-enhanced contrastive learning framework (RECLF). In RECLF, we\nintroduce a semantic-relation reasoning module (SRM) and an importance-relation\nreasoning module (IRM) to enable more fine-grained report supervision for image\nrepresentation learning. We evaluated our method using six public benchmark\ndatasets on four downstream tasks, including segmentation, zero-shot\nclassification, linear classification, and cross-modal retrieval. Our results\ndemonstrated the superiority of our RECLF over the state-of-the-art mVLCL\nmethods with consistent improvements across single-modal and cross-modal tasks.\nThese results suggest that our RECLF, by modelling the inter-matching\nrelations, can learn improved medical image representations with better\ngeneralization capabilities.\n", "link": "http://arxiv.org/abs/2401.10501v2", "date": "2025-02-07", "relevancy": 2.7638, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5565}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5509}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20medical%20vision-language%20contrastive%20learning%20via%0A%20%20inter-matching%20relation%20modelling&body=Title%3A%20Enhancing%20medical%20vision-language%20contrastive%20learning%20via%0A%20%20inter-matching%20relation%20modelling%0AAuthor%3A%20Mingjian%20Li%20and%20Mingyuan%20Meng%20and%20Michael%20Fulham%20and%20David%20Dagan%20Feng%20and%20Lei%20Bi%20and%20Jinman%20Kim%0AAbstract%3A%20%20%20Medical%20image%20representations%20can%20be%20learned%20through%20medical%20vision-language%0Acontrastive%20learning%20%28mVLCL%29%20where%20medical%20imaging%20reports%20are%20used%20as%20weak%0Asupervision%20through%20image-text%20alignment.%20These%20learned%20image%20representations%0Acan%20be%20transferred%20to%20and%20benefit%20various%20downstream%20medical%20vision%20tasks%20such%0Aas%20disease%20classification%20and%20segmentation.%20Recent%20mVLCL%20methods%20attempt%20to%0Aalign%20image%20sub-regions%20and%20the%20report%20keywords%20as%20local-matchings.%20However%2C%0Athese%20methods%20aggregate%20all%20local-matchings%20via%20simple%20pooling%20operations%20while%0Aignoring%20the%20inherent%20relations%20between%20them.%20These%20methods%20therefore%20fail%20to%0Areason%20between%20local-matchings%20that%20are%20semantically%20related%2C%20e.g.%2C%0Alocal-matchings%20that%20correspond%20to%20the%20disease%20word%20and%20the%20location%20word%0A%28semantic-relations%29%2C%20and%20also%20fail%20to%20differentiate%20such%20clinically%20important%0Alocal-matchings%20from%20others%20that%20correspond%20to%20less%20meaningful%20words%2C%20e.g.%2C%0Aconjunction%20words%20%28importance-relations%29.%20Hence%2C%20we%20propose%20a%20mVLCL%20method%20that%0Amodels%20the%20inter-matching%20relations%20between%20local-matchings%20via%20a%0Arelation-enhanced%20contrastive%20learning%20framework%20%28RECLF%29.%20In%20RECLF%2C%20we%0Aintroduce%20a%20semantic-relation%20reasoning%20module%20%28SRM%29%20and%20an%20importance-relation%0Areasoning%20module%20%28IRM%29%20to%20enable%20more%20fine-grained%20report%20supervision%20for%20image%0Arepresentation%20learning.%20We%20evaluated%20our%20method%20using%20six%20public%20benchmark%0Adatasets%20on%20four%20downstream%20tasks%2C%20including%20segmentation%2C%20zero-shot%0Aclassification%2C%20linear%20classification%2C%20and%20cross-modal%20retrieval.%20Our%20results%0Ademonstrated%20the%20superiority%20of%20our%20RECLF%20over%20the%20state-of-the-art%20mVLCL%0Amethods%20with%20consistent%20improvements%20across%20single-modal%20and%20cross-modal%20tasks.%0AThese%20results%20suggest%20that%20our%20RECLF%2C%20by%20modelling%20the%20inter-matching%0Arelations%2C%20can%20learn%20improved%20medical%20image%20representations%20with%20better%0Ageneralization%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.10501v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520medical%2520vision-language%2520contrastive%2520learning%2520via%250A%2520%2520inter-matching%2520relation%2520modelling%26entry.906535625%3DMingjian%2520Li%2520and%2520Mingyuan%2520Meng%2520and%2520Michael%2520Fulham%2520and%2520David%2520Dagan%2520Feng%2520and%2520Lei%2520Bi%2520and%2520Jinman%2520Kim%26entry.1292438233%3D%2520%2520Medical%2520image%2520representations%2520can%2520be%2520learned%2520through%2520medical%2520vision-language%250Acontrastive%2520learning%2520%2528mVLCL%2529%2520where%2520medical%2520imaging%2520reports%2520are%2520used%2520as%2520weak%250Asupervision%2520through%2520image-text%2520alignment.%2520These%2520learned%2520image%2520representations%250Acan%2520be%2520transferred%2520to%2520and%2520benefit%2520various%2520downstream%2520medical%2520vision%2520tasks%2520such%250Aas%2520disease%2520classification%2520and%2520segmentation.%2520Recent%2520mVLCL%2520methods%2520attempt%2520to%250Aalign%2520image%2520sub-regions%2520and%2520the%2520report%2520keywords%2520as%2520local-matchings.%2520However%252C%250Athese%2520methods%2520aggregate%2520all%2520local-matchings%2520via%2520simple%2520pooling%2520operations%2520while%250Aignoring%2520the%2520inherent%2520relations%2520between%2520them.%2520These%2520methods%2520therefore%2520fail%2520to%250Areason%2520between%2520local-matchings%2520that%2520are%2520semantically%2520related%252C%2520e.g.%252C%250Alocal-matchings%2520that%2520correspond%2520to%2520the%2520disease%2520word%2520and%2520the%2520location%2520word%250A%2528semantic-relations%2529%252C%2520and%2520also%2520fail%2520to%2520differentiate%2520such%2520clinically%2520important%250Alocal-matchings%2520from%2520others%2520that%2520correspond%2520to%2520less%2520meaningful%2520words%252C%2520e.g.%252C%250Aconjunction%2520words%2520%2528importance-relations%2529.%2520Hence%252C%2520we%2520propose%2520a%2520mVLCL%2520method%2520that%250Amodels%2520the%2520inter-matching%2520relations%2520between%2520local-matchings%2520via%2520a%250Arelation-enhanced%2520contrastive%2520learning%2520framework%2520%2528RECLF%2529.%2520In%2520RECLF%252C%2520we%250Aintroduce%2520a%2520semantic-relation%2520reasoning%2520module%2520%2528SRM%2529%2520and%2520an%2520importance-relation%250Areasoning%2520module%2520%2528IRM%2529%2520to%2520enable%2520more%2520fine-grained%2520report%2520supervision%2520for%2520image%250Arepresentation%2520learning.%2520We%2520evaluated%2520our%2520method%2520using%2520six%2520public%2520benchmark%250Adatasets%2520on%2520four%2520downstream%2520tasks%252C%2520including%2520segmentation%252C%2520zero-shot%250Aclassification%252C%2520linear%2520classification%252C%2520and%2520cross-modal%2520retrieval.%2520Our%2520results%250Ademonstrated%2520the%2520superiority%2520of%2520our%2520RECLF%2520over%2520the%2520state-of-the-art%2520mVLCL%250Amethods%2520with%2520consistent%2520improvements%2520across%2520single-modal%2520and%2520cross-modal%2520tasks.%250AThese%2520results%2520suggest%2520that%2520our%2520RECLF%252C%2520by%2520modelling%2520the%2520inter-matching%250Arelations%252C%2520can%2520learn%2520improved%2520medical%2520image%2520representations%2520with%2520better%250Ageneralization%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.10501v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20medical%20vision-language%20contrastive%20learning%20via%0A%20%20inter-matching%20relation%20modelling&entry.906535625=Mingjian%20Li%20and%20Mingyuan%20Meng%20and%20Michael%20Fulham%20and%20David%20Dagan%20Feng%20and%20Lei%20Bi%20and%20Jinman%20Kim&entry.1292438233=%20%20Medical%20image%20representations%20can%20be%20learned%20through%20medical%20vision-language%0Acontrastive%20learning%20%28mVLCL%29%20where%20medical%20imaging%20reports%20are%20used%20as%20weak%0Asupervision%20through%20image-text%20alignment.%20These%20learned%20image%20representations%0Acan%20be%20transferred%20to%20and%20benefit%20various%20downstream%20medical%20vision%20tasks%20such%0Aas%20disease%20classification%20and%20segmentation.%20Recent%20mVLCL%20methods%20attempt%20to%0Aalign%20image%20sub-regions%20and%20the%20report%20keywords%20as%20local-matchings.%20However%2C%0Athese%20methods%20aggregate%20all%20local-matchings%20via%20simple%20pooling%20operations%20while%0Aignoring%20the%20inherent%20relations%20between%20them.%20These%20methods%20therefore%20fail%20to%0Areason%20between%20local-matchings%20that%20are%20semantically%20related%2C%20e.g.%2C%0Alocal-matchings%20that%20correspond%20to%20the%20disease%20word%20and%20the%20location%20word%0A%28semantic-relations%29%2C%20and%20also%20fail%20to%20differentiate%20such%20clinically%20important%0Alocal-matchings%20from%20others%20that%20correspond%20to%20less%20meaningful%20words%2C%20e.g.%2C%0Aconjunction%20words%20%28importance-relations%29.%20Hence%2C%20we%20propose%20a%20mVLCL%20method%20that%0Amodels%20the%20inter-matching%20relations%20between%20local-matchings%20via%20a%0Arelation-enhanced%20contrastive%20learning%20framework%20%28RECLF%29.%20In%20RECLF%2C%20we%0Aintroduce%20a%20semantic-relation%20reasoning%20module%20%28SRM%29%20and%20an%20importance-relation%0Areasoning%20module%20%28IRM%29%20to%20enable%20more%20fine-grained%20report%20supervision%20for%20image%0Arepresentation%20learning.%20We%20evaluated%20our%20method%20using%20six%20public%20benchmark%0Adatasets%20on%20four%20downstream%20tasks%2C%20including%20segmentation%2C%20zero-shot%0Aclassification%2C%20linear%20classification%2C%20and%20cross-modal%20retrieval.%20Our%20results%0Ademonstrated%20the%20superiority%20of%20our%20RECLF%20over%20the%20state-of-the-art%20mVLCL%0Amethods%20with%20consistent%20improvements%20across%20single-modal%20and%20cross-modal%20tasks.%0AThese%20results%20suggest%20that%20our%20RECLF%2C%20by%20modelling%20the%20inter-matching%0Arelations%2C%20can%20learn%20improved%20medical%20image%20representations%20with%20better%0Ageneralization%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10501v2&entry.124074799=Read"},
{"title": "LP-DETR: Layer-wise Progressive Relations for Object Detection", "author": "Zhengjian Kang and Ye Zhang and Xiaoyu Deng and Xintao Li and Yongzhe Zhang", "abstract": "  This paper presents LP-DETR (Layer-wise Progressive DETR), a novel approach\nthat enhances DETR-based object detection through multi-scale relation\nmodeling. Our method introduces learnable spatial relationships between object\nqueries through a relation-aware self-attention mechanism, which adaptively\nlearns to balance different scales of relations (local, medium and global)\nacross decoder layers. This progressive design enables the model to effectively\ncapture evolving spatial dependencies throughout the detection pipeline.\nExtensive experiments on COCO 2017 dataset demonstrate that our method improves\nboth convergence speed and detection accuracy compared to standard\nself-attention module. The proposed method achieves competitive results,\nreaching 52.3\\% AP with 12 epochs and 52.5\\% AP with 24 epochs using ResNet-50\nbackbone, and further improving to 58.0\\% AP with Swin-L backbone. Furthermore,\nour analysis reveals an interesting pattern: the model naturally learns to\nprioritize local spatial relations in early decoder layers while gradually\nshifting attention to broader contexts in deeper layers, providing valuable\ninsights for future research in object detection.\n", "link": "http://arxiv.org/abs/2502.05147v1", "date": "2025-02-07", "relevancy": 2.7479, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5615}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.556}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LP-DETR%3A%20Layer-wise%20Progressive%20Relations%20for%20Object%20Detection&body=Title%3A%20LP-DETR%3A%20Layer-wise%20Progressive%20Relations%20for%20Object%20Detection%0AAuthor%3A%20Zhengjian%20Kang%20and%20Ye%20Zhang%20and%20Xiaoyu%20Deng%20and%20Xintao%20Li%20and%20Yongzhe%20Zhang%0AAbstract%3A%20%20%20This%20paper%20presents%20LP-DETR%20%28Layer-wise%20Progressive%20DETR%29%2C%20a%20novel%20approach%0Athat%20enhances%20DETR-based%20object%20detection%20through%20multi-scale%20relation%0Amodeling.%20Our%20method%20introduces%20learnable%20spatial%20relationships%20between%20object%0Aqueries%20through%20a%20relation-aware%20self-attention%20mechanism%2C%20which%20adaptively%0Alearns%20to%20balance%20different%20scales%20of%20relations%20%28local%2C%20medium%20and%20global%29%0Aacross%20decoder%20layers.%20This%20progressive%20design%20enables%20the%20model%20to%20effectively%0Acapture%20evolving%20spatial%20dependencies%20throughout%20the%20detection%20pipeline.%0AExtensive%20experiments%20on%20COCO%202017%20dataset%20demonstrate%20that%20our%20method%20improves%0Aboth%20convergence%20speed%20and%20detection%20accuracy%20compared%20to%20standard%0Aself-attention%20module.%20The%20proposed%20method%20achieves%20competitive%20results%2C%0Areaching%2052.3%5C%25%20AP%20with%2012%20epochs%20and%2052.5%5C%25%20AP%20with%2024%20epochs%20using%20ResNet-50%0Abackbone%2C%20and%20further%20improving%20to%2058.0%5C%25%20AP%20with%20Swin-L%20backbone.%20Furthermore%2C%0Aour%20analysis%20reveals%20an%20interesting%20pattern%3A%20the%20model%20naturally%20learns%20to%0Aprioritize%20local%20spatial%20relations%20in%20early%20decoder%20layers%20while%20gradually%0Ashifting%20attention%20to%20broader%20contexts%20in%20deeper%20layers%2C%20providing%20valuable%0Ainsights%20for%20future%20research%20in%20object%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05147v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLP-DETR%253A%2520Layer-wise%2520Progressive%2520Relations%2520for%2520Object%2520Detection%26entry.906535625%3DZhengjian%2520Kang%2520and%2520Ye%2520Zhang%2520and%2520Xiaoyu%2520Deng%2520and%2520Xintao%2520Li%2520and%2520Yongzhe%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520LP-DETR%2520%2528Layer-wise%2520Progressive%2520DETR%2529%252C%2520a%2520novel%2520approach%250Athat%2520enhances%2520DETR-based%2520object%2520detection%2520through%2520multi-scale%2520relation%250Amodeling.%2520Our%2520method%2520introduces%2520learnable%2520spatial%2520relationships%2520between%2520object%250Aqueries%2520through%2520a%2520relation-aware%2520self-attention%2520mechanism%252C%2520which%2520adaptively%250Alearns%2520to%2520balance%2520different%2520scales%2520of%2520relations%2520%2528local%252C%2520medium%2520and%2520global%2529%250Aacross%2520decoder%2520layers.%2520This%2520progressive%2520design%2520enables%2520the%2520model%2520to%2520effectively%250Acapture%2520evolving%2520spatial%2520dependencies%2520throughout%2520the%2520detection%2520pipeline.%250AExtensive%2520experiments%2520on%2520COCO%25202017%2520dataset%2520demonstrate%2520that%2520our%2520method%2520improves%250Aboth%2520convergence%2520speed%2520and%2520detection%2520accuracy%2520compared%2520to%2520standard%250Aself-attention%2520module.%2520The%2520proposed%2520method%2520achieves%2520competitive%2520results%252C%250Areaching%252052.3%255C%2525%2520AP%2520with%252012%2520epochs%2520and%252052.5%255C%2525%2520AP%2520with%252024%2520epochs%2520using%2520ResNet-50%250Abackbone%252C%2520and%2520further%2520improving%2520to%252058.0%255C%2525%2520AP%2520with%2520Swin-L%2520backbone.%2520Furthermore%252C%250Aour%2520analysis%2520reveals%2520an%2520interesting%2520pattern%253A%2520the%2520model%2520naturally%2520learns%2520to%250Aprioritize%2520local%2520spatial%2520relations%2520in%2520early%2520decoder%2520layers%2520while%2520gradually%250Ashifting%2520attention%2520to%2520broader%2520contexts%2520in%2520deeper%2520layers%252C%2520providing%2520valuable%250Ainsights%2520for%2520future%2520research%2520in%2520object%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05147v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LP-DETR%3A%20Layer-wise%20Progressive%20Relations%20for%20Object%20Detection&entry.906535625=Zhengjian%20Kang%20and%20Ye%20Zhang%20and%20Xiaoyu%20Deng%20and%20Xintao%20Li%20and%20Yongzhe%20Zhang&entry.1292438233=%20%20This%20paper%20presents%20LP-DETR%20%28Layer-wise%20Progressive%20DETR%29%2C%20a%20novel%20approach%0Athat%20enhances%20DETR-based%20object%20detection%20through%20multi-scale%20relation%0Amodeling.%20Our%20method%20introduces%20learnable%20spatial%20relationships%20between%20object%0Aqueries%20through%20a%20relation-aware%20self-attention%20mechanism%2C%20which%20adaptively%0Alearns%20to%20balance%20different%20scales%20of%20relations%20%28local%2C%20medium%20and%20global%29%0Aacross%20decoder%20layers.%20This%20progressive%20design%20enables%20the%20model%20to%20effectively%0Acapture%20evolving%20spatial%20dependencies%20throughout%20the%20detection%20pipeline.%0AExtensive%20experiments%20on%20COCO%202017%20dataset%20demonstrate%20that%20our%20method%20improves%0Aboth%20convergence%20speed%20and%20detection%20accuracy%20compared%20to%20standard%0Aself-attention%20module.%20The%20proposed%20method%20achieves%20competitive%20results%2C%0Areaching%2052.3%5C%25%20AP%20with%2012%20epochs%20and%2052.5%5C%25%20AP%20with%2024%20epochs%20using%20ResNet-50%0Abackbone%2C%20and%20further%20improving%20to%2058.0%5C%25%20AP%20with%20Swin-L%20backbone.%20Furthermore%2C%0Aour%20analysis%20reveals%20an%20interesting%20pattern%3A%20the%20model%20naturally%20learns%20to%0Aprioritize%20local%20spatial%20relations%20in%20early%20decoder%20layers%20while%20gradually%0Ashifting%20attention%20to%20broader%20contexts%20in%20deeper%20layers%2C%20providing%20valuable%0Ainsights%20for%20future%20research%20in%20object%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05147v1&entry.124074799=Read"},
{"title": "HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion\n  Video Generation", "author": "Qijun Gan and Yi Ren and Chen Zhang and Zhenhui Ye and Pan Xie and Xiang Yin and Zehuan Yuan and Bingyue Peng and Jianke Zhu", "abstract": "  Human motion video generation has advanced significantly, while existing\nmethods still struggle with accurately rendering detailed body parts like hands\nand faces, especially in long sequences and intricate motions. Current\napproaches also rely on fixed resolution and struggle to maintain visual\nconsistency. To address these limitations, we propose HumanDiT, a pose-guided\nDiffusion Transformer (DiT)-based framework trained on a large and wild dataset\ncontaining 14,000 hours of high-quality video to produce high-fidelity videos\nwith fine-grained body rendering. Specifically, (i) HumanDiT, built on DiT,\nsupports numerous video resolutions and variable sequence lengths, facilitating\nlearning for long-sequence video generation; (ii) we introduce a prefix-latent\nreference strategy to maintain personalized characteristics across extended\nsequences. Furthermore, during inference, HumanDiT leverages Keypoint-DiT to\ngenerate subsequent pose sequences, facilitating video continuation from static\nimages or existing videos. It also utilizes a Pose Adapter to enable pose\ntransfer with given sequences. Extensive experiments demonstrate its superior\nperformance in generating long-form, pose-accurate videos across diverse\nscenarios.\n", "link": "http://arxiv.org/abs/2502.04847v1", "date": "2025-02-07", "relevancy": 2.7208, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7296}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6571}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.64}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HumanDiT%3A%20Pose-Guided%20Diffusion%20Transformer%20for%20Long-form%20Human%20Motion%0A%20%20Video%20Generation&body=Title%3A%20HumanDiT%3A%20Pose-Guided%20Diffusion%20Transformer%20for%20Long-form%20Human%20Motion%0A%20%20Video%20Generation%0AAuthor%3A%20Qijun%20Gan%20and%20Yi%20Ren%20and%20Chen%20Zhang%20and%20Zhenhui%20Ye%20and%20Pan%20Xie%20and%20Xiang%20Yin%20and%20Zehuan%20Yuan%20and%20Bingyue%20Peng%20and%20Jianke%20Zhu%0AAbstract%3A%20%20%20Human%20motion%20video%20generation%20has%20advanced%20significantly%2C%20while%20existing%0Amethods%20still%20struggle%20with%20accurately%20rendering%20detailed%20body%20parts%20like%20hands%0Aand%20faces%2C%20especially%20in%20long%20sequences%20and%20intricate%20motions.%20Current%0Aapproaches%20also%20rely%20on%20fixed%20resolution%20and%20struggle%20to%20maintain%20visual%0Aconsistency.%20To%20address%20these%20limitations%2C%20we%20propose%20HumanDiT%2C%20a%20pose-guided%0ADiffusion%20Transformer%20%28DiT%29-based%20framework%20trained%20on%20a%20large%20and%20wild%20dataset%0Acontaining%2014%2C000%20hours%20of%20high-quality%20video%20to%20produce%20high-fidelity%20videos%0Awith%20fine-grained%20body%20rendering.%20Specifically%2C%20%28i%29%20HumanDiT%2C%20built%20on%20DiT%2C%0Asupports%20numerous%20video%20resolutions%20and%20variable%20sequence%20lengths%2C%20facilitating%0Alearning%20for%20long-sequence%20video%20generation%3B%20%28ii%29%20we%20introduce%20a%20prefix-latent%0Areference%20strategy%20to%20maintain%20personalized%20characteristics%20across%20extended%0Asequences.%20Furthermore%2C%20during%20inference%2C%20HumanDiT%20leverages%20Keypoint-DiT%20to%0Agenerate%20subsequent%20pose%20sequences%2C%20facilitating%20video%20continuation%20from%20static%0Aimages%20or%20existing%20videos.%20It%20also%20utilizes%20a%20Pose%20Adapter%20to%20enable%20pose%0Atransfer%20with%20given%20sequences.%20Extensive%20experiments%20demonstrate%20its%20superior%0Aperformance%20in%20generating%20long-form%2C%20pose-accurate%20videos%20across%20diverse%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumanDiT%253A%2520Pose-Guided%2520Diffusion%2520Transformer%2520for%2520Long-form%2520Human%2520Motion%250A%2520%2520Video%2520Generation%26entry.906535625%3DQijun%2520Gan%2520and%2520Yi%2520Ren%2520and%2520Chen%2520Zhang%2520and%2520Zhenhui%2520Ye%2520and%2520Pan%2520Xie%2520and%2520Xiang%2520Yin%2520and%2520Zehuan%2520Yuan%2520and%2520Bingyue%2520Peng%2520and%2520Jianke%2520Zhu%26entry.1292438233%3D%2520%2520Human%2520motion%2520video%2520generation%2520has%2520advanced%2520significantly%252C%2520while%2520existing%250Amethods%2520still%2520struggle%2520with%2520accurately%2520rendering%2520detailed%2520body%2520parts%2520like%2520hands%250Aand%2520faces%252C%2520especially%2520in%2520long%2520sequences%2520and%2520intricate%2520motions.%2520Current%250Aapproaches%2520also%2520rely%2520on%2520fixed%2520resolution%2520and%2520struggle%2520to%2520maintain%2520visual%250Aconsistency.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520HumanDiT%252C%2520a%2520pose-guided%250ADiffusion%2520Transformer%2520%2528DiT%2529-based%2520framework%2520trained%2520on%2520a%2520large%2520and%2520wild%2520dataset%250Acontaining%252014%252C000%2520hours%2520of%2520high-quality%2520video%2520to%2520produce%2520high-fidelity%2520videos%250Awith%2520fine-grained%2520body%2520rendering.%2520Specifically%252C%2520%2528i%2529%2520HumanDiT%252C%2520built%2520on%2520DiT%252C%250Asupports%2520numerous%2520video%2520resolutions%2520and%2520variable%2520sequence%2520lengths%252C%2520facilitating%250Alearning%2520for%2520long-sequence%2520video%2520generation%253B%2520%2528ii%2529%2520we%2520introduce%2520a%2520prefix-latent%250Areference%2520strategy%2520to%2520maintain%2520personalized%2520characteristics%2520across%2520extended%250Asequences.%2520Furthermore%252C%2520during%2520inference%252C%2520HumanDiT%2520leverages%2520Keypoint-DiT%2520to%250Agenerate%2520subsequent%2520pose%2520sequences%252C%2520facilitating%2520video%2520continuation%2520from%2520static%250Aimages%2520or%2520existing%2520videos.%2520It%2520also%2520utilizes%2520a%2520Pose%2520Adapter%2520to%2520enable%2520pose%250Atransfer%2520with%2520given%2520sequences.%2520Extensive%2520experiments%2520demonstrate%2520its%2520superior%250Aperformance%2520in%2520generating%2520long-form%252C%2520pose-accurate%2520videos%2520across%2520diverse%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HumanDiT%3A%20Pose-Guided%20Diffusion%20Transformer%20for%20Long-form%20Human%20Motion%0A%20%20Video%20Generation&entry.906535625=Qijun%20Gan%20and%20Yi%20Ren%20and%20Chen%20Zhang%20and%20Zhenhui%20Ye%20and%20Pan%20Xie%20and%20Xiang%20Yin%20and%20Zehuan%20Yuan%20and%20Bingyue%20Peng%20and%20Jianke%20Zhu&entry.1292438233=%20%20Human%20motion%20video%20generation%20has%20advanced%20significantly%2C%20while%20existing%0Amethods%20still%20struggle%20with%20accurately%20rendering%20detailed%20body%20parts%20like%20hands%0Aand%20faces%2C%20especially%20in%20long%20sequences%20and%20intricate%20motions.%20Current%0Aapproaches%20also%20rely%20on%20fixed%20resolution%20and%20struggle%20to%20maintain%20visual%0Aconsistency.%20To%20address%20these%20limitations%2C%20we%20propose%20HumanDiT%2C%20a%20pose-guided%0ADiffusion%20Transformer%20%28DiT%29-based%20framework%20trained%20on%20a%20large%20and%20wild%20dataset%0Acontaining%2014%2C000%20hours%20of%20high-quality%20video%20to%20produce%20high-fidelity%20videos%0Awith%20fine-grained%20body%20rendering.%20Specifically%2C%20%28i%29%20HumanDiT%2C%20built%20on%20DiT%2C%0Asupports%20numerous%20video%20resolutions%20and%20variable%20sequence%20lengths%2C%20facilitating%0Alearning%20for%20long-sequence%20video%20generation%3B%20%28ii%29%20we%20introduce%20a%20prefix-latent%0Areference%20strategy%20to%20maintain%20personalized%20characteristics%20across%20extended%0Asequences.%20Furthermore%2C%20during%20inference%2C%20HumanDiT%20leverages%20Keypoint-DiT%20to%0Agenerate%20subsequent%20pose%20sequences%2C%20facilitating%20video%20continuation%20from%20static%0Aimages%20or%20existing%20videos.%20It%20also%20utilizes%20a%20Pose%20Adapter%20to%20enable%20pose%0Atransfer%20with%20given%20sequences.%20Extensive%20experiments%20demonstrate%20its%20superior%0Aperformance%20in%20generating%20long-form%2C%20pose-accurate%20videos%20across%20diverse%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04847v1&entry.124074799=Read"},
{"title": "QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive\n  Multimodal Understanding and Generation", "author": "Yue Zhao and Fuzhao Xue and Scott Reed and Linxi Fan and Yuke Zhu and Jan Kautz and Zhiding Yu and Philipp Kr\u00e4henb\u00fchl and De-An Huang", "abstract": "  We introduce Quantized Language-Image Pretraining (QLIP), a visual\ntokenization method that combines state-of-the-art reconstruction quality with\nstate-of-the-art zero-shot image understanding. QLIP trains a\nbinary-spherical-quantization-based autoencoder with reconstruction and\nlanguage-image alignment objectives. We are the first to show that the two\nobjectives do not need to be at odds. We balance the two loss terms dynamically\nduring training and show that a two-stage training pipeline effectively mixes\nthe large-batch requirements of image-language pre-training with the memory\nbottleneck imposed by the reconstruction objective. We validate the\neffectiveness of QLIP for multimodal understanding and text-conditioned image\ngeneration with a single model. Specifically, QLIP serves as a drop-in\nreplacement for the visual encoder for LLaVA and the image tokenizer for\nLlamaGen with comparable or even better performance. Finally, we demonstrate\nthat QLIP enables a unified mixed-modality auto-regressive model for\nunderstanding and generation.\n", "link": "http://arxiv.org/abs/2502.05178v1", "date": "2025-02-07", "relevancy": 2.7159, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5838}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5229}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5229}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QLIP%3A%20Text-Aligned%20Visual%20Tokenization%20Unifies%20Auto-Regressive%0A%20%20Multimodal%20Understanding%20and%20Generation&body=Title%3A%20QLIP%3A%20Text-Aligned%20Visual%20Tokenization%20Unifies%20Auto-Regressive%0A%20%20Multimodal%20Understanding%20and%20Generation%0AAuthor%3A%20Yue%20Zhao%20and%20Fuzhao%20Xue%20and%20Scott%20Reed%20and%20Linxi%20Fan%20and%20Yuke%20Zhu%20and%20Jan%20Kautz%20and%20Zhiding%20Yu%20and%20Philipp%20Kr%C3%A4henb%C3%BChl%20and%20De-An%20Huang%0AAbstract%3A%20%20%20We%20introduce%20Quantized%20Language-Image%20Pretraining%20%28QLIP%29%2C%20a%20visual%0Atokenization%20method%20that%20combines%20state-of-the-art%20reconstruction%20quality%20with%0Astate-of-the-art%20zero-shot%20image%20understanding.%20QLIP%20trains%20a%0Abinary-spherical-quantization-based%20autoencoder%20with%20reconstruction%20and%0Alanguage-image%20alignment%20objectives.%20We%20are%20the%20first%20to%20show%20that%20the%20two%0Aobjectives%20do%20not%20need%20to%20be%20at%20odds.%20We%20balance%20the%20two%20loss%20terms%20dynamically%0Aduring%20training%20and%20show%20that%20a%20two-stage%20training%20pipeline%20effectively%20mixes%0Athe%20large-batch%20requirements%20of%20image-language%20pre-training%20with%20the%20memory%0Abottleneck%20imposed%20by%20the%20reconstruction%20objective.%20We%20validate%20the%0Aeffectiveness%20of%20QLIP%20for%20multimodal%20understanding%20and%20text-conditioned%20image%0Ageneration%20with%20a%20single%20model.%20Specifically%2C%20QLIP%20serves%20as%20a%20drop-in%0Areplacement%20for%20the%20visual%20encoder%20for%20LLaVA%20and%20the%20image%20tokenizer%20for%0ALlamaGen%20with%20comparable%20or%20even%20better%20performance.%20Finally%2C%20we%20demonstrate%0Athat%20QLIP%20enables%20a%20unified%20mixed-modality%20auto-regressive%20model%20for%0Aunderstanding%20and%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05178v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQLIP%253A%2520Text-Aligned%2520Visual%2520Tokenization%2520Unifies%2520Auto-Regressive%250A%2520%2520Multimodal%2520Understanding%2520and%2520Generation%26entry.906535625%3DYue%2520Zhao%2520and%2520Fuzhao%2520Xue%2520and%2520Scott%2520Reed%2520and%2520Linxi%2520Fan%2520and%2520Yuke%2520Zhu%2520and%2520Jan%2520Kautz%2520and%2520Zhiding%2520Yu%2520and%2520Philipp%2520Kr%25C3%25A4henb%25C3%25BChl%2520and%2520De-An%2520Huang%26entry.1292438233%3D%2520%2520We%2520introduce%2520Quantized%2520Language-Image%2520Pretraining%2520%2528QLIP%2529%252C%2520a%2520visual%250Atokenization%2520method%2520that%2520combines%2520state-of-the-art%2520reconstruction%2520quality%2520with%250Astate-of-the-art%2520zero-shot%2520image%2520understanding.%2520QLIP%2520trains%2520a%250Abinary-spherical-quantization-based%2520autoencoder%2520with%2520reconstruction%2520and%250Alanguage-image%2520alignment%2520objectives.%2520We%2520are%2520the%2520first%2520to%2520show%2520that%2520the%2520two%250Aobjectives%2520do%2520not%2520need%2520to%2520be%2520at%2520odds.%2520We%2520balance%2520the%2520two%2520loss%2520terms%2520dynamically%250Aduring%2520training%2520and%2520show%2520that%2520a%2520two-stage%2520training%2520pipeline%2520effectively%2520mixes%250Athe%2520large-batch%2520requirements%2520of%2520image-language%2520pre-training%2520with%2520the%2520memory%250Abottleneck%2520imposed%2520by%2520the%2520reconstruction%2520objective.%2520We%2520validate%2520the%250Aeffectiveness%2520of%2520QLIP%2520for%2520multimodal%2520understanding%2520and%2520text-conditioned%2520image%250Ageneration%2520with%2520a%2520single%2520model.%2520Specifically%252C%2520QLIP%2520serves%2520as%2520a%2520drop-in%250Areplacement%2520for%2520the%2520visual%2520encoder%2520for%2520LLaVA%2520and%2520the%2520image%2520tokenizer%2520for%250ALlamaGen%2520with%2520comparable%2520or%2520even%2520better%2520performance.%2520Finally%252C%2520we%2520demonstrate%250Athat%2520QLIP%2520enables%2520a%2520unified%2520mixed-modality%2520auto-regressive%2520model%2520for%250Aunderstanding%2520and%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05178v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QLIP%3A%20Text-Aligned%20Visual%20Tokenization%20Unifies%20Auto-Regressive%0A%20%20Multimodal%20Understanding%20and%20Generation&entry.906535625=Yue%20Zhao%20and%20Fuzhao%20Xue%20and%20Scott%20Reed%20and%20Linxi%20Fan%20and%20Yuke%20Zhu%20and%20Jan%20Kautz%20and%20Zhiding%20Yu%20and%20Philipp%20Kr%C3%A4henb%C3%BChl%20and%20De-An%20Huang&entry.1292438233=%20%20We%20introduce%20Quantized%20Language-Image%20Pretraining%20%28QLIP%29%2C%20a%20visual%0Atokenization%20method%20that%20combines%20state-of-the-art%20reconstruction%20quality%20with%0Astate-of-the-art%20zero-shot%20image%20understanding.%20QLIP%20trains%20a%0Abinary-spherical-quantization-based%20autoencoder%20with%20reconstruction%20and%0Alanguage-image%20alignment%20objectives.%20We%20are%20the%20first%20to%20show%20that%20the%20two%0Aobjectives%20do%20not%20need%20to%20be%20at%20odds.%20We%20balance%20the%20two%20loss%20terms%20dynamically%0Aduring%20training%20and%20show%20that%20a%20two-stage%20training%20pipeline%20effectively%20mixes%0Athe%20large-batch%20requirements%20of%20image-language%20pre-training%20with%20the%20memory%0Abottleneck%20imposed%20by%20the%20reconstruction%20objective.%20We%20validate%20the%0Aeffectiveness%20of%20QLIP%20for%20multimodal%20understanding%20and%20text-conditioned%20image%0Ageneration%20with%20a%20single%20model.%20Specifically%2C%20QLIP%20serves%20as%20a%20drop-in%0Areplacement%20for%20the%20visual%20encoder%20for%20LLaVA%20and%20the%20image%20tokenizer%20for%0ALlamaGen%20with%20comparable%20or%20even%20better%20performance.%20Finally%2C%20we%20demonstrate%0Athat%20QLIP%20enables%20a%20unified%20mixed-modality%20auto-regressive%20model%20for%0Aunderstanding%20and%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05178v1&entry.124074799=Read"},
{"title": "Differentiable Mobile Display Photometric Stereo", "author": "Gawoon Ban and Hyeongjun Kim and Seokjun Choi and Seungwoo Yoon and Seung-Hwan Baek", "abstract": "  Display photometric stereo uses a display as a programmable light source to\nilluminate a scene with diverse illumination conditions. Recently,\ndifferentiable display photometric stereo (DDPS) demonstrated improved normal\nreconstruction accuracy by using learned display patterns. However, DDPS faced\nlimitations in practicality, requiring a fixed desktop imaging setup using a\npolarization camera and a desktop-scale monitor. In this paper, we propose a\nmore practical physics-based photometric stereo, differentiable mobile display\nphotometric stereo (DMDPS), that leverages a mobile phone consisting of a\ndisplay and a camera. We overcome the limitations of using a mobile device by\ndeveloping a mobile app and method that simultaneously displays patterns and\ncaptures high-quality HDR images. Using this technique, we capture real-world\n3D-printed objects and learn display patterns via a differentiable learning\nprocess. We demonstrate the effectiveness of DMDPS on both a 3D printed dataset\nand a first dataset of fallen leaves. The leaf dataset contains reconstructed\nsurface normals and albedos of fallen leaves that may enable future research\nbeyond computer graphics and vision. We believe that DMDPS takes a step forward\nfor practical physics-based photometric stereo.\n", "link": "http://arxiv.org/abs/2502.05055v1", "date": "2025-02-07", "relevancy": 2.7048, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.541}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.541}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentiable%20Mobile%20Display%20Photometric%20Stereo&body=Title%3A%20Differentiable%20Mobile%20Display%20Photometric%20Stereo%0AAuthor%3A%20Gawoon%20Ban%20and%20Hyeongjun%20Kim%20and%20Seokjun%20Choi%20and%20Seungwoo%20Yoon%20and%20Seung-Hwan%20Baek%0AAbstract%3A%20%20%20Display%20photometric%20stereo%20uses%20a%20display%20as%20a%20programmable%20light%20source%20to%0Ailluminate%20a%20scene%20with%20diverse%20illumination%20conditions.%20Recently%2C%0Adifferentiable%20display%20photometric%20stereo%20%28DDPS%29%20demonstrated%20improved%20normal%0Areconstruction%20accuracy%20by%20using%20learned%20display%20patterns.%20However%2C%20DDPS%20faced%0Alimitations%20in%20practicality%2C%20requiring%20a%20fixed%20desktop%20imaging%20setup%20using%20a%0Apolarization%20camera%20and%20a%20desktop-scale%20monitor.%20In%20this%20paper%2C%20we%20propose%20a%0Amore%20practical%20physics-based%20photometric%20stereo%2C%20differentiable%20mobile%20display%0Aphotometric%20stereo%20%28DMDPS%29%2C%20that%20leverages%20a%20mobile%20phone%20consisting%20of%20a%0Adisplay%20and%20a%20camera.%20We%20overcome%20the%20limitations%20of%20using%20a%20mobile%20device%20by%0Adeveloping%20a%20mobile%20app%20and%20method%20that%20simultaneously%20displays%20patterns%20and%0Acaptures%20high-quality%20HDR%20images.%20Using%20this%20technique%2C%20we%20capture%20real-world%0A3D-printed%20objects%20and%20learn%20display%20patterns%20via%20a%20differentiable%20learning%0Aprocess.%20We%20demonstrate%20the%20effectiveness%20of%20DMDPS%20on%20both%20a%203D%20printed%20dataset%0Aand%20a%20first%20dataset%20of%20fallen%20leaves.%20The%20leaf%20dataset%20contains%20reconstructed%0Asurface%20normals%20and%20albedos%20of%20fallen%20leaves%20that%20may%20enable%20future%20research%0Abeyond%20computer%20graphics%20and%20vision.%20We%20believe%20that%20DMDPS%20takes%20a%20step%20forward%0Afor%20practical%20physics-based%20photometric%20stereo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05055v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentiable%2520Mobile%2520Display%2520Photometric%2520Stereo%26entry.906535625%3DGawoon%2520Ban%2520and%2520Hyeongjun%2520Kim%2520and%2520Seokjun%2520Choi%2520and%2520Seungwoo%2520Yoon%2520and%2520Seung-Hwan%2520Baek%26entry.1292438233%3D%2520%2520Display%2520photometric%2520stereo%2520uses%2520a%2520display%2520as%2520a%2520programmable%2520light%2520source%2520to%250Ailluminate%2520a%2520scene%2520with%2520diverse%2520illumination%2520conditions.%2520Recently%252C%250Adifferentiable%2520display%2520photometric%2520stereo%2520%2528DDPS%2529%2520demonstrated%2520improved%2520normal%250Areconstruction%2520accuracy%2520by%2520using%2520learned%2520display%2520patterns.%2520However%252C%2520DDPS%2520faced%250Alimitations%2520in%2520practicality%252C%2520requiring%2520a%2520fixed%2520desktop%2520imaging%2520setup%2520using%2520a%250Apolarization%2520camera%2520and%2520a%2520desktop-scale%2520monitor.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Amore%2520practical%2520physics-based%2520photometric%2520stereo%252C%2520differentiable%2520mobile%2520display%250Aphotometric%2520stereo%2520%2528DMDPS%2529%252C%2520that%2520leverages%2520a%2520mobile%2520phone%2520consisting%2520of%2520a%250Adisplay%2520and%2520a%2520camera.%2520We%2520overcome%2520the%2520limitations%2520of%2520using%2520a%2520mobile%2520device%2520by%250Adeveloping%2520a%2520mobile%2520app%2520and%2520method%2520that%2520simultaneously%2520displays%2520patterns%2520and%250Acaptures%2520high-quality%2520HDR%2520images.%2520Using%2520this%2520technique%252C%2520we%2520capture%2520real-world%250A3D-printed%2520objects%2520and%2520learn%2520display%2520patterns%2520via%2520a%2520differentiable%2520learning%250Aprocess.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520DMDPS%2520on%2520both%2520a%25203D%2520printed%2520dataset%250Aand%2520a%2520first%2520dataset%2520of%2520fallen%2520leaves.%2520The%2520leaf%2520dataset%2520contains%2520reconstructed%250Asurface%2520normals%2520and%2520albedos%2520of%2520fallen%2520leaves%2520that%2520may%2520enable%2520future%2520research%250Abeyond%2520computer%2520graphics%2520and%2520vision.%2520We%2520believe%2520that%2520DMDPS%2520takes%2520a%2520step%2520forward%250Afor%2520practical%2520physics-based%2520photometric%2520stereo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05055v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentiable%20Mobile%20Display%20Photometric%20Stereo&entry.906535625=Gawoon%20Ban%20and%20Hyeongjun%20Kim%20and%20Seokjun%20Choi%20and%20Seungwoo%20Yoon%20and%20Seung-Hwan%20Baek&entry.1292438233=%20%20Display%20photometric%20stereo%20uses%20a%20display%20as%20a%20programmable%20light%20source%20to%0Ailluminate%20a%20scene%20with%20diverse%20illumination%20conditions.%20Recently%2C%0Adifferentiable%20display%20photometric%20stereo%20%28DDPS%29%20demonstrated%20improved%20normal%0Areconstruction%20accuracy%20by%20using%20learned%20display%20patterns.%20However%2C%20DDPS%20faced%0Alimitations%20in%20practicality%2C%20requiring%20a%20fixed%20desktop%20imaging%20setup%20using%20a%0Apolarization%20camera%20and%20a%20desktop-scale%20monitor.%20In%20this%20paper%2C%20we%20propose%20a%0Amore%20practical%20physics-based%20photometric%20stereo%2C%20differentiable%20mobile%20display%0Aphotometric%20stereo%20%28DMDPS%29%2C%20that%20leverages%20a%20mobile%20phone%20consisting%20of%20a%0Adisplay%20and%20a%20camera.%20We%20overcome%20the%20limitations%20of%20using%20a%20mobile%20device%20by%0Adeveloping%20a%20mobile%20app%20and%20method%20that%20simultaneously%20displays%20patterns%20and%0Acaptures%20high-quality%20HDR%20images.%20Using%20this%20technique%2C%20we%20capture%20real-world%0A3D-printed%20objects%20and%20learn%20display%20patterns%20via%20a%20differentiable%20learning%0Aprocess.%20We%20demonstrate%20the%20effectiveness%20of%20DMDPS%20on%20both%20a%203D%20printed%20dataset%0Aand%20a%20first%20dataset%20of%20fallen%20leaves.%20The%20leaf%20dataset%20contains%20reconstructed%0Asurface%20normals%20and%20albedos%20of%20fallen%20leaves%20that%20may%20enable%20future%20research%0Abeyond%20computer%20graphics%20and%20vision.%20We%20believe%20that%20DMDPS%20takes%20a%20step%20forward%0Afor%20practical%20physics-based%20photometric%20stereo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05055v1&entry.124074799=Read"},
{"title": "multiGradICON: A Foundation Model for Multimodal Medical Image\n  Registration", "author": "Basar Demir and Lin Tian and Thomas Hastings Greer and Roland Kwitt and Francois-Xavier Vialard and Raul San Jose Estepar and Sylvain Bouix and Richard Jarrett Rushmore and Ebrahim Ebrahim and Marc Niethammer", "abstract": "  Modern medical image registration approaches predict deformations using deep\nnetworks. These approaches achieve state-of-the-art (SOTA) registration\naccuracy and are generally fast. However, deep learning (DL) approaches are, in\ncontrast to conventional non-deep-learning-based approaches, anatomy-specific.\nRecently, a universal deep registration approach, uniGradICON, has been\nproposed. However, uniGradICON focuses on monomodal image registration. In this\nwork, we therefore develop multiGradICON as a first step towards universal\n*multimodal* medical image registration. Specifically, we show that 1) we can\ntrain a DL registration model that is suitable for monomodal *and* multimodal\nregistration; 2) loss function randomization can increase multimodal\nregistration accuracy; and 3) training a model with multimodal data helps\nmultimodal generalization. Our code and the multiGradICON model are available\nat https://github.com/uncbiag/uniGradICON.\n", "link": "http://arxiv.org/abs/2408.00221v2", "date": "2025-02-07", "relevancy": 2.6889, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5523}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5337}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20multiGradICON%3A%20A%20Foundation%20Model%20for%20Multimodal%20Medical%20Image%0A%20%20Registration&body=Title%3A%20multiGradICON%3A%20A%20Foundation%20Model%20for%20Multimodal%20Medical%20Image%0A%20%20Registration%0AAuthor%3A%20Basar%20Demir%20and%20Lin%20Tian%20and%20Thomas%20Hastings%20Greer%20and%20Roland%20Kwitt%20and%20Francois-Xavier%20Vialard%20and%20Raul%20San%20Jose%20Estepar%20and%20Sylvain%20Bouix%20and%20Richard%20Jarrett%20Rushmore%20and%20Ebrahim%20Ebrahim%20and%20Marc%20Niethammer%0AAbstract%3A%20%20%20Modern%20medical%20image%20registration%20approaches%20predict%20deformations%20using%20deep%0Anetworks.%20These%20approaches%20achieve%20state-of-the-art%20%28SOTA%29%20registration%0Aaccuracy%20and%20are%20generally%20fast.%20However%2C%20deep%20learning%20%28DL%29%20approaches%20are%2C%20in%0Acontrast%20to%20conventional%20non-deep-learning-based%20approaches%2C%20anatomy-specific.%0ARecently%2C%20a%20universal%20deep%20registration%20approach%2C%20uniGradICON%2C%20has%20been%0Aproposed.%20However%2C%20uniGradICON%20focuses%20on%20monomodal%20image%20registration.%20In%20this%0Awork%2C%20we%20therefore%20develop%20multiGradICON%20as%20a%20first%20step%20towards%20universal%0A%2Amultimodal%2A%20medical%20image%20registration.%20Specifically%2C%20we%20show%20that%201%29%20we%20can%0Atrain%20a%20DL%20registration%20model%20that%20is%20suitable%20for%20monomodal%20%2Aand%2A%20multimodal%0Aregistration%3B%202%29%20loss%20function%20randomization%20can%20increase%20multimodal%0Aregistration%20accuracy%3B%20and%203%29%20training%20a%20model%20with%20multimodal%20data%20helps%0Amultimodal%20generalization.%20Our%20code%20and%20the%20multiGradICON%20model%20are%20available%0Aat%20https%3A//github.com/uncbiag/uniGradICON.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00221v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DmultiGradICON%253A%2520A%2520Foundation%2520Model%2520for%2520Multimodal%2520Medical%2520Image%250A%2520%2520Registration%26entry.906535625%3DBasar%2520Demir%2520and%2520Lin%2520Tian%2520and%2520Thomas%2520Hastings%2520Greer%2520and%2520Roland%2520Kwitt%2520and%2520Francois-Xavier%2520Vialard%2520and%2520Raul%2520San%2520Jose%2520Estepar%2520and%2520Sylvain%2520Bouix%2520and%2520Richard%2520Jarrett%2520Rushmore%2520and%2520Ebrahim%2520Ebrahim%2520and%2520Marc%2520Niethammer%26entry.1292438233%3D%2520%2520Modern%2520medical%2520image%2520registration%2520approaches%2520predict%2520deformations%2520using%2520deep%250Anetworks.%2520These%2520approaches%2520achieve%2520state-of-the-art%2520%2528SOTA%2529%2520registration%250Aaccuracy%2520and%2520are%2520generally%2520fast.%2520However%252C%2520deep%2520learning%2520%2528DL%2529%2520approaches%2520are%252C%2520in%250Acontrast%2520to%2520conventional%2520non-deep-learning-based%2520approaches%252C%2520anatomy-specific.%250ARecently%252C%2520a%2520universal%2520deep%2520registration%2520approach%252C%2520uniGradICON%252C%2520has%2520been%250Aproposed.%2520However%252C%2520uniGradICON%2520focuses%2520on%2520monomodal%2520image%2520registration.%2520In%2520this%250Awork%252C%2520we%2520therefore%2520develop%2520multiGradICON%2520as%2520a%2520first%2520step%2520towards%2520universal%250A%252Amultimodal%252A%2520medical%2520image%2520registration.%2520Specifically%252C%2520we%2520show%2520that%25201%2529%2520we%2520can%250Atrain%2520a%2520DL%2520registration%2520model%2520that%2520is%2520suitable%2520for%2520monomodal%2520%252Aand%252A%2520multimodal%250Aregistration%253B%25202%2529%2520loss%2520function%2520randomization%2520can%2520increase%2520multimodal%250Aregistration%2520accuracy%253B%2520and%25203%2529%2520training%2520a%2520model%2520with%2520multimodal%2520data%2520helps%250Amultimodal%2520generalization.%2520Our%2520code%2520and%2520the%2520multiGradICON%2520model%2520are%2520available%250Aat%2520https%253A//github.com/uncbiag/uniGradICON.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00221v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=multiGradICON%3A%20A%20Foundation%20Model%20for%20Multimodal%20Medical%20Image%0A%20%20Registration&entry.906535625=Basar%20Demir%20and%20Lin%20Tian%20and%20Thomas%20Hastings%20Greer%20and%20Roland%20Kwitt%20and%20Francois-Xavier%20Vialard%20and%20Raul%20San%20Jose%20Estepar%20and%20Sylvain%20Bouix%20and%20Richard%20Jarrett%20Rushmore%20and%20Ebrahim%20Ebrahim%20and%20Marc%20Niethammer&entry.1292438233=%20%20Modern%20medical%20image%20registration%20approaches%20predict%20deformations%20using%20deep%0Anetworks.%20These%20approaches%20achieve%20state-of-the-art%20%28SOTA%29%20registration%0Aaccuracy%20and%20are%20generally%20fast.%20However%2C%20deep%20learning%20%28DL%29%20approaches%20are%2C%20in%0Acontrast%20to%20conventional%20non-deep-learning-based%20approaches%2C%20anatomy-specific.%0ARecently%2C%20a%20universal%20deep%20registration%20approach%2C%20uniGradICON%2C%20has%20been%0Aproposed.%20However%2C%20uniGradICON%20focuses%20on%20monomodal%20image%20registration.%20In%20this%0Awork%2C%20we%20therefore%20develop%20multiGradICON%20as%20a%20first%20step%20towards%20universal%0A%2Amultimodal%2A%20medical%20image%20registration.%20Specifically%2C%20we%20show%20that%201%29%20we%20can%0Atrain%20a%20DL%20registration%20model%20that%20is%20suitable%20for%20monomodal%20%2Aand%2A%20multimodal%0Aregistration%3B%202%29%20loss%20function%20randomization%20can%20increase%20multimodal%0Aregistration%20accuracy%3B%20and%203%29%20training%20a%20model%20with%20multimodal%20data%20helps%0Amultimodal%20generalization.%20Our%20code%20and%20the%20multiGradICON%20model%20are%20available%0Aat%20https%3A//github.com/uncbiag/uniGradICON.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00221v2&entry.124074799=Read"},
{"title": "MindAligner: Explicit Brain Functional Alignment for Cross-Subject\n  Visual Decoding from Limited fMRI Data", "author": "Yuqin Dai and Zhouheng Yao and Chunfeng Song and Qihao Zheng and Weijian Mai and Kunyu Peng and Shuai Lu and Wanli Ouyang and Jian Yang and Jiamin Wu", "abstract": "  Brain decoding aims to reconstruct visual perception of human subject from\nfMRI signals, which is crucial for understanding brain's perception mechanisms.\nExisting methods are confined to the single-subject paradigm due to substantial\nbrain variability, which leads to weak generalization across individuals and\nincurs high training costs, exacerbated by limited availability of fMRI data.\nTo address these challenges, we propose MindAligner, an explicit functional\nalignment framework for cross-subject brain decoding from limited fMRI data.\nThe proposed MindAligner enjoys several merits. First, we learn a Brain\nTransfer Matrix (BTM) that projects the brain signals of an arbitrary new\nsubject to one of the known subjects, enabling seamless use of pre-trained\ndecoding models. Second, to facilitate reliable BTM learning, a Brain\nFunctional Alignment module is proposed to perform soft cross-subject brain\nalignment under different visual stimuli with a multi-level brain alignment\nloss, uncovering fine-grained functional correspondences with high\ninterpretability. Experiments indicate that MindAligner not only outperforms\nexisting methods in visual decoding under data-limited conditions, but also\nprovides valuable neuroscience insights in cross-subject functional analysis.\nThe code will be made publicly available.\n", "link": "http://arxiv.org/abs/2502.05034v1", "date": "2025-02-07", "relevancy": 2.6618, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5359}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5359}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MindAligner%3A%20Explicit%20Brain%20Functional%20Alignment%20for%20Cross-Subject%0A%20%20Visual%20Decoding%20from%20Limited%20fMRI%20Data&body=Title%3A%20MindAligner%3A%20Explicit%20Brain%20Functional%20Alignment%20for%20Cross-Subject%0A%20%20Visual%20Decoding%20from%20Limited%20fMRI%20Data%0AAuthor%3A%20Yuqin%20Dai%20and%20Zhouheng%20Yao%20and%20Chunfeng%20Song%20and%20Qihao%20Zheng%20and%20Weijian%20Mai%20and%20Kunyu%20Peng%20and%20Shuai%20Lu%20and%20Wanli%20Ouyang%20and%20Jian%20Yang%20and%20Jiamin%20Wu%0AAbstract%3A%20%20%20Brain%20decoding%20aims%20to%20reconstruct%20visual%20perception%20of%20human%20subject%20from%0AfMRI%20signals%2C%20which%20is%20crucial%20for%20understanding%20brain%27s%20perception%20mechanisms.%0AExisting%20methods%20are%20confined%20to%20the%20single-subject%20paradigm%20due%20to%20substantial%0Abrain%20variability%2C%20which%20leads%20to%20weak%20generalization%20across%20individuals%20and%0Aincurs%20high%20training%20costs%2C%20exacerbated%20by%20limited%20availability%20of%20fMRI%20data.%0ATo%20address%20these%20challenges%2C%20we%20propose%20MindAligner%2C%20an%20explicit%20functional%0Aalignment%20framework%20for%20cross-subject%20brain%20decoding%20from%20limited%20fMRI%20data.%0AThe%20proposed%20MindAligner%20enjoys%20several%20merits.%20First%2C%20we%20learn%20a%20Brain%0ATransfer%20Matrix%20%28BTM%29%20that%20projects%20the%20brain%20signals%20of%20an%20arbitrary%20new%0Asubject%20to%20one%20of%20the%20known%20subjects%2C%20enabling%20seamless%20use%20of%20pre-trained%0Adecoding%20models.%20Second%2C%20to%20facilitate%20reliable%20BTM%20learning%2C%20a%20Brain%0AFunctional%20Alignment%20module%20is%20proposed%20to%20perform%20soft%20cross-subject%20brain%0Aalignment%20under%20different%20visual%20stimuli%20with%20a%20multi-level%20brain%20alignment%0Aloss%2C%20uncovering%20fine-grained%20functional%20correspondences%20with%20high%0Ainterpretability.%20Experiments%20indicate%20that%20MindAligner%20not%20only%20outperforms%0Aexisting%20methods%20in%20visual%20decoding%20under%20data-limited%20conditions%2C%20but%20also%0Aprovides%20valuable%20neuroscience%20insights%20in%20cross-subject%20functional%20analysis.%0AThe%20code%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05034v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMindAligner%253A%2520Explicit%2520Brain%2520Functional%2520Alignment%2520for%2520Cross-Subject%250A%2520%2520Visual%2520Decoding%2520from%2520Limited%2520fMRI%2520Data%26entry.906535625%3DYuqin%2520Dai%2520and%2520Zhouheng%2520Yao%2520and%2520Chunfeng%2520Song%2520and%2520Qihao%2520Zheng%2520and%2520Weijian%2520Mai%2520and%2520Kunyu%2520Peng%2520and%2520Shuai%2520Lu%2520and%2520Wanli%2520Ouyang%2520and%2520Jian%2520Yang%2520and%2520Jiamin%2520Wu%26entry.1292438233%3D%2520%2520Brain%2520decoding%2520aims%2520to%2520reconstruct%2520visual%2520perception%2520of%2520human%2520subject%2520from%250AfMRI%2520signals%252C%2520which%2520is%2520crucial%2520for%2520understanding%2520brain%2527s%2520perception%2520mechanisms.%250AExisting%2520methods%2520are%2520confined%2520to%2520the%2520single-subject%2520paradigm%2520due%2520to%2520substantial%250Abrain%2520variability%252C%2520which%2520leads%2520to%2520weak%2520generalization%2520across%2520individuals%2520and%250Aincurs%2520high%2520training%2520costs%252C%2520exacerbated%2520by%2520limited%2520availability%2520of%2520fMRI%2520data.%250ATo%2520address%2520these%2520challenges%252C%2520we%2520propose%2520MindAligner%252C%2520an%2520explicit%2520functional%250Aalignment%2520framework%2520for%2520cross-subject%2520brain%2520decoding%2520from%2520limited%2520fMRI%2520data.%250AThe%2520proposed%2520MindAligner%2520enjoys%2520several%2520merits.%2520First%252C%2520we%2520learn%2520a%2520Brain%250ATransfer%2520Matrix%2520%2528BTM%2529%2520that%2520projects%2520the%2520brain%2520signals%2520of%2520an%2520arbitrary%2520new%250Asubject%2520to%2520one%2520of%2520the%2520known%2520subjects%252C%2520enabling%2520seamless%2520use%2520of%2520pre-trained%250Adecoding%2520models.%2520Second%252C%2520to%2520facilitate%2520reliable%2520BTM%2520learning%252C%2520a%2520Brain%250AFunctional%2520Alignment%2520module%2520is%2520proposed%2520to%2520perform%2520soft%2520cross-subject%2520brain%250Aalignment%2520under%2520different%2520visual%2520stimuli%2520with%2520a%2520multi-level%2520brain%2520alignment%250Aloss%252C%2520uncovering%2520fine-grained%2520functional%2520correspondences%2520with%2520high%250Ainterpretability.%2520Experiments%2520indicate%2520that%2520MindAligner%2520not%2520only%2520outperforms%250Aexisting%2520methods%2520in%2520visual%2520decoding%2520under%2520data-limited%2520conditions%252C%2520but%2520also%250Aprovides%2520valuable%2520neuroscience%2520insights%2520in%2520cross-subject%2520functional%2520analysis.%250AThe%2520code%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05034v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MindAligner%3A%20Explicit%20Brain%20Functional%20Alignment%20for%20Cross-Subject%0A%20%20Visual%20Decoding%20from%20Limited%20fMRI%20Data&entry.906535625=Yuqin%20Dai%20and%20Zhouheng%20Yao%20and%20Chunfeng%20Song%20and%20Qihao%20Zheng%20and%20Weijian%20Mai%20and%20Kunyu%20Peng%20and%20Shuai%20Lu%20and%20Wanli%20Ouyang%20and%20Jian%20Yang%20and%20Jiamin%20Wu&entry.1292438233=%20%20Brain%20decoding%20aims%20to%20reconstruct%20visual%20perception%20of%20human%20subject%20from%0AfMRI%20signals%2C%20which%20is%20crucial%20for%20understanding%20brain%27s%20perception%20mechanisms.%0AExisting%20methods%20are%20confined%20to%20the%20single-subject%20paradigm%20due%20to%20substantial%0Abrain%20variability%2C%20which%20leads%20to%20weak%20generalization%20across%20individuals%20and%0Aincurs%20high%20training%20costs%2C%20exacerbated%20by%20limited%20availability%20of%20fMRI%20data.%0ATo%20address%20these%20challenges%2C%20we%20propose%20MindAligner%2C%20an%20explicit%20functional%0Aalignment%20framework%20for%20cross-subject%20brain%20decoding%20from%20limited%20fMRI%20data.%0AThe%20proposed%20MindAligner%20enjoys%20several%20merits.%20First%2C%20we%20learn%20a%20Brain%0ATransfer%20Matrix%20%28BTM%29%20that%20projects%20the%20brain%20signals%20of%20an%20arbitrary%20new%0Asubject%20to%20one%20of%20the%20known%20subjects%2C%20enabling%20seamless%20use%20of%20pre-trained%0Adecoding%20models.%20Second%2C%20to%20facilitate%20reliable%20BTM%20learning%2C%20a%20Brain%0AFunctional%20Alignment%20module%20is%20proposed%20to%20perform%20soft%20cross-subject%20brain%0Aalignment%20under%20different%20visual%20stimuli%20with%20a%20multi-level%20brain%20alignment%0Aloss%2C%20uncovering%20fine-grained%20functional%20correspondences%20with%20high%0Ainterpretability.%20Experiments%20indicate%20that%20MindAligner%20not%20only%20outperforms%0Aexisting%20methods%20in%20visual%20decoding%20under%20data-limited%20conditions%2C%20but%20also%0Aprovides%20valuable%20neuroscience%20insights%20in%20cross-subject%20functional%20analysis.%0AThe%20code%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05034v1&entry.124074799=Read"},
{"title": "ARTInp: CBCT-to-CT Image Inpainting and Image Translation in\n  Radiotherapy", "author": "Ricardo Coimbra Brioso and Leonardo Crespi and Andrea Seghetto and Damiano Dei and Nicola Lambri and Pietro Mancosu and Marta Scorsetti and Daniele Loiacono", "abstract": "  A key step in Adaptive Radiation Therapy (ART) workflows is the evaluation of\nthe patient's anatomy at treatment time to ensure the accuracy of the delivery.\nTo this end, Cone Beam Computerized Tomography (CBCT) is widely used being\ncost-effective and easy to integrate into the treatment process. Nonetheless,\nCBCT images have lower resolution and more artifacts than CT scans, making them\nless reliable for precise treatment validation. Moreover, in complex treatments\nsuch as Total Marrow and Lymph Node Irradiation (TMLI), where full-body\nvisualization of the patient is critical for accurate dose delivery, the CBCT\nimages are often discontinuous, leaving gaps that could contain relevant\nanatomical information. To address these limitations, we propose ARTInp\n(Adaptive Radiation Therapy Inpainting), a novel deep-learning framework\ncombining image inpainting and CBCT-to-CT translation. ARTInp employs a\ndual-network approach: a completion network that fills anatomical gaps in CBCT\nvolumes and a custom Generative Adversarial Network (GAN) to generate\nhigh-quality synthetic CT (sCT) images. We trained ARTInp on a dataset of\npaired CBCT and CT images from the SynthRad 2023 challenge, and the performance\nachieved on a test set of 18 patients demonstrates its potential for enhancing\nCBCT-based workflows in radiotherapy.\n", "link": "http://arxiv.org/abs/2502.04898v1", "date": "2025-02-07", "relevancy": 2.6534, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5341}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5341}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ARTInp%3A%20CBCT-to-CT%20Image%20Inpainting%20and%20Image%20Translation%20in%0A%20%20Radiotherapy&body=Title%3A%20ARTInp%3A%20CBCT-to-CT%20Image%20Inpainting%20and%20Image%20Translation%20in%0A%20%20Radiotherapy%0AAuthor%3A%20Ricardo%20Coimbra%20Brioso%20and%20Leonardo%20Crespi%20and%20Andrea%20Seghetto%20and%20Damiano%20Dei%20and%20Nicola%20Lambri%20and%20Pietro%20Mancosu%20and%20Marta%20Scorsetti%20and%20Daniele%20Loiacono%0AAbstract%3A%20%20%20A%20key%20step%20in%20Adaptive%20Radiation%20Therapy%20%28ART%29%20workflows%20is%20the%20evaluation%20of%0Athe%20patient%27s%20anatomy%20at%20treatment%20time%20to%20ensure%20the%20accuracy%20of%20the%20delivery.%0ATo%20this%20end%2C%20Cone%20Beam%20Computerized%20Tomography%20%28CBCT%29%20is%20widely%20used%20being%0Acost-effective%20and%20easy%20to%20integrate%20into%20the%20treatment%20process.%20Nonetheless%2C%0ACBCT%20images%20have%20lower%20resolution%20and%20more%20artifacts%20than%20CT%20scans%2C%20making%20them%0Aless%20reliable%20for%20precise%20treatment%20validation.%20Moreover%2C%20in%20complex%20treatments%0Asuch%20as%20Total%20Marrow%20and%20Lymph%20Node%20Irradiation%20%28TMLI%29%2C%20where%20full-body%0Avisualization%20of%20the%20patient%20is%20critical%20for%20accurate%20dose%20delivery%2C%20the%20CBCT%0Aimages%20are%20often%20discontinuous%2C%20leaving%20gaps%20that%20could%20contain%20relevant%0Aanatomical%20information.%20To%20address%20these%20limitations%2C%20we%20propose%20ARTInp%0A%28Adaptive%20Radiation%20Therapy%20Inpainting%29%2C%20a%20novel%20deep-learning%20framework%0Acombining%20image%20inpainting%20and%20CBCT-to-CT%20translation.%20ARTInp%20employs%20a%0Adual-network%20approach%3A%20a%20completion%20network%20that%20fills%20anatomical%20gaps%20in%20CBCT%0Avolumes%20and%20a%20custom%20Generative%20Adversarial%20Network%20%28GAN%29%20to%20generate%0Ahigh-quality%20synthetic%20CT%20%28sCT%29%20images.%20We%20trained%20ARTInp%20on%20a%20dataset%20of%0Apaired%20CBCT%20and%20CT%20images%20from%20the%20SynthRad%202023%20challenge%2C%20and%20the%20performance%0Aachieved%20on%20a%20test%20set%20of%2018%20patients%20demonstrates%20its%20potential%20for%20enhancing%0ACBCT-based%20workflows%20in%20radiotherapy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04898v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DARTInp%253A%2520CBCT-to-CT%2520Image%2520Inpainting%2520and%2520Image%2520Translation%2520in%250A%2520%2520Radiotherapy%26entry.906535625%3DRicardo%2520Coimbra%2520Brioso%2520and%2520Leonardo%2520Crespi%2520and%2520Andrea%2520Seghetto%2520and%2520Damiano%2520Dei%2520and%2520Nicola%2520Lambri%2520and%2520Pietro%2520Mancosu%2520and%2520Marta%2520Scorsetti%2520and%2520Daniele%2520Loiacono%26entry.1292438233%3D%2520%2520A%2520key%2520step%2520in%2520Adaptive%2520Radiation%2520Therapy%2520%2528ART%2529%2520workflows%2520is%2520the%2520evaluation%2520of%250Athe%2520patient%2527s%2520anatomy%2520at%2520treatment%2520time%2520to%2520ensure%2520the%2520accuracy%2520of%2520the%2520delivery.%250ATo%2520this%2520end%252C%2520Cone%2520Beam%2520Computerized%2520Tomography%2520%2528CBCT%2529%2520is%2520widely%2520used%2520being%250Acost-effective%2520and%2520easy%2520to%2520integrate%2520into%2520the%2520treatment%2520process.%2520Nonetheless%252C%250ACBCT%2520images%2520have%2520lower%2520resolution%2520and%2520more%2520artifacts%2520than%2520CT%2520scans%252C%2520making%2520them%250Aless%2520reliable%2520for%2520precise%2520treatment%2520validation.%2520Moreover%252C%2520in%2520complex%2520treatments%250Asuch%2520as%2520Total%2520Marrow%2520and%2520Lymph%2520Node%2520Irradiation%2520%2528TMLI%2529%252C%2520where%2520full-body%250Avisualization%2520of%2520the%2520patient%2520is%2520critical%2520for%2520accurate%2520dose%2520delivery%252C%2520the%2520CBCT%250Aimages%2520are%2520often%2520discontinuous%252C%2520leaving%2520gaps%2520that%2520could%2520contain%2520relevant%250Aanatomical%2520information.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520ARTInp%250A%2528Adaptive%2520Radiation%2520Therapy%2520Inpainting%2529%252C%2520a%2520novel%2520deep-learning%2520framework%250Acombining%2520image%2520inpainting%2520and%2520CBCT-to-CT%2520translation.%2520ARTInp%2520employs%2520a%250Adual-network%2520approach%253A%2520a%2520completion%2520network%2520that%2520fills%2520anatomical%2520gaps%2520in%2520CBCT%250Avolumes%2520and%2520a%2520custom%2520Generative%2520Adversarial%2520Network%2520%2528GAN%2529%2520to%2520generate%250Ahigh-quality%2520synthetic%2520CT%2520%2528sCT%2529%2520images.%2520We%2520trained%2520ARTInp%2520on%2520a%2520dataset%2520of%250Apaired%2520CBCT%2520and%2520CT%2520images%2520from%2520the%2520SynthRad%25202023%2520challenge%252C%2520and%2520the%2520performance%250Aachieved%2520on%2520a%2520test%2520set%2520of%252018%2520patients%2520demonstrates%2520its%2520potential%2520for%2520enhancing%250ACBCT-based%2520workflows%2520in%2520radiotherapy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04898v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ARTInp%3A%20CBCT-to-CT%20Image%20Inpainting%20and%20Image%20Translation%20in%0A%20%20Radiotherapy&entry.906535625=Ricardo%20Coimbra%20Brioso%20and%20Leonardo%20Crespi%20and%20Andrea%20Seghetto%20and%20Damiano%20Dei%20and%20Nicola%20Lambri%20and%20Pietro%20Mancosu%20and%20Marta%20Scorsetti%20and%20Daniele%20Loiacono&entry.1292438233=%20%20A%20key%20step%20in%20Adaptive%20Radiation%20Therapy%20%28ART%29%20workflows%20is%20the%20evaluation%20of%0Athe%20patient%27s%20anatomy%20at%20treatment%20time%20to%20ensure%20the%20accuracy%20of%20the%20delivery.%0ATo%20this%20end%2C%20Cone%20Beam%20Computerized%20Tomography%20%28CBCT%29%20is%20widely%20used%20being%0Acost-effective%20and%20easy%20to%20integrate%20into%20the%20treatment%20process.%20Nonetheless%2C%0ACBCT%20images%20have%20lower%20resolution%20and%20more%20artifacts%20than%20CT%20scans%2C%20making%20them%0Aless%20reliable%20for%20precise%20treatment%20validation.%20Moreover%2C%20in%20complex%20treatments%0Asuch%20as%20Total%20Marrow%20and%20Lymph%20Node%20Irradiation%20%28TMLI%29%2C%20where%20full-body%0Avisualization%20of%20the%20patient%20is%20critical%20for%20accurate%20dose%20delivery%2C%20the%20CBCT%0Aimages%20are%20often%20discontinuous%2C%20leaving%20gaps%20that%20could%20contain%20relevant%0Aanatomical%20information.%20To%20address%20these%20limitations%2C%20we%20propose%20ARTInp%0A%28Adaptive%20Radiation%20Therapy%20Inpainting%29%2C%20a%20novel%20deep-learning%20framework%0Acombining%20image%20inpainting%20and%20CBCT-to-CT%20translation.%20ARTInp%20employs%20a%0Adual-network%20approach%3A%20a%20completion%20network%20that%20fills%20anatomical%20gaps%20in%20CBCT%0Avolumes%20and%20a%20custom%20Generative%20Adversarial%20Network%20%28GAN%29%20to%20generate%0Ahigh-quality%20synthetic%20CT%20%28sCT%29%20images.%20We%20trained%20ARTInp%20on%20a%20dataset%20of%0Apaired%20CBCT%20and%20CT%20images%20from%20the%20SynthRad%202023%20challenge%2C%20and%20the%20performance%0Aachieved%20on%20a%20test%20set%20of%2018%20patients%20demonstrates%20its%20potential%20for%20enhancing%0ACBCT-based%20workflows%20in%20radiotherapy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04898v1&entry.124074799=Read"},
{"title": "3DMolFormer: A Dual-channel Framework for Structure-based Drug Discovery", "author": "Xiuyuan Hu and Guoqing Liu and Can Chen and Yang Zhao and Hao Zhang and Xue Liu", "abstract": "  Structure-based drug discovery, encompassing the tasks of protein-ligand\ndocking and pocket-aware 3D drug design, represents a core challenge in drug\ndiscovery. However, no existing work can deal with both tasks to effectively\nleverage the duality between them, and current methods for each task are\nhindered by challenges in modeling 3D information and the limitations of\navailable data. To address these issues, we propose 3DMolFormer, a unified\ndual-channel transformer-based framework applicable to both docking and 3D drug\ndesign tasks, which exploits their duality by utilizing docking functionalities\nwithin the drug design process. Specifically, we represent 3D pocket-ligand\ncomplexes using parallel sequences of discrete tokens and continuous numbers,\nand we design a corresponding dual-channel transformer model to handle this\nformat, thereby overcoming the challenges of 3D information modeling.\nAdditionally, we alleviate data limitations through large-scale pre-training on\na mixed dataset, followed by supervised and reinforcement learning fine-tuning\ntechniques respectively tailored for the two tasks. Experimental results\ndemonstrate that 3DMolFormer outperforms previous approaches in both\nprotein-ligand docking and pocket-aware 3D drug design, highlighting its\npromising application in structure-based drug discovery. The code is available\nat: https://github.com/HXYfighter/3DMolFormer .\n", "link": "http://arxiv.org/abs/2502.05107v1", "date": "2025-02-07", "relevancy": 2.6281, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5528}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5229}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DMolFormer%3A%20A%20Dual-channel%20Framework%20for%20Structure-based%20Drug%20Discovery&body=Title%3A%203DMolFormer%3A%20A%20Dual-channel%20Framework%20for%20Structure-based%20Drug%20Discovery%0AAuthor%3A%20Xiuyuan%20Hu%20and%20Guoqing%20Liu%20and%20Can%20Chen%20and%20Yang%20Zhao%20and%20Hao%20Zhang%20and%20Xue%20Liu%0AAbstract%3A%20%20%20Structure-based%20drug%20discovery%2C%20encompassing%20the%20tasks%20of%20protein-ligand%0Adocking%20and%20pocket-aware%203D%20drug%20design%2C%20represents%20a%20core%20challenge%20in%20drug%0Adiscovery.%20However%2C%20no%20existing%20work%20can%20deal%20with%20both%20tasks%20to%20effectively%0Aleverage%20the%20duality%20between%20them%2C%20and%20current%20methods%20for%20each%20task%20are%0Ahindered%20by%20challenges%20in%20modeling%203D%20information%20and%20the%20limitations%20of%0Aavailable%20data.%20To%20address%20these%20issues%2C%20we%20propose%203DMolFormer%2C%20a%20unified%0Adual-channel%20transformer-based%20framework%20applicable%20to%20both%20docking%20and%203D%20drug%0Adesign%20tasks%2C%20which%20exploits%20their%20duality%20by%20utilizing%20docking%20functionalities%0Awithin%20the%20drug%20design%20process.%20Specifically%2C%20we%20represent%203D%20pocket-ligand%0Acomplexes%20using%20parallel%20sequences%20of%20discrete%20tokens%20and%20continuous%20numbers%2C%0Aand%20we%20design%20a%20corresponding%20dual-channel%20transformer%20model%20to%20handle%20this%0Aformat%2C%20thereby%20overcoming%20the%20challenges%20of%203D%20information%20modeling.%0AAdditionally%2C%20we%20alleviate%20data%20limitations%20through%20large-scale%20pre-training%20on%0Aa%20mixed%20dataset%2C%20followed%20by%20supervised%20and%20reinforcement%20learning%20fine-tuning%0Atechniques%20respectively%20tailored%20for%20the%20two%20tasks.%20Experimental%20results%0Ademonstrate%20that%203DMolFormer%20outperforms%20previous%20approaches%20in%20both%0Aprotein-ligand%20docking%20and%20pocket-aware%203D%20drug%20design%2C%20highlighting%20its%0Apromising%20application%20in%20structure-based%20drug%20discovery.%20The%20code%20is%20available%0Aat%3A%20https%3A//github.com/HXYfighter/3DMolFormer%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05107v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DMolFormer%253A%2520A%2520Dual-channel%2520Framework%2520for%2520Structure-based%2520Drug%2520Discovery%26entry.906535625%3DXiuyuan%2520Hu%2520and%2520Guoqing%2520Liu%2520and%2520Can%2520Chen%2520and%2520Yang%2520Zhao%2520and%2520Hao%2520Zhang%2520and%2520Xue%2520Liu%26entry.1292438233%3D%2520%2520Structure-based%2520drug%2520discovery%252C%2520encompassing%2520the%2520tasks%2520of%2520protein-ligand%250Adocking%2520and%2520pocket-aware%25203D%2520drug%2520design%252C%2520represents%2520a%2520core%2520challenge%2520in%2520drug%250Adiscovery.%2520However%252C%2520no%2520existing%2520work%2520can%2520deal%2520with%2520both%2520tasks%2520to%2520effectively%250Aleverage%2520the%2520duality%2520between%2520them%252C%2520and%2520current%2520methods%2520for%2520each%2520task%2520are%250Ahindered%2520by%2520challenges%2520in%2520modeling%25203D%2520information%2520and%2520the%2520limitations%2520of%250Aavailable%2520data.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%25203DMolFormer%252C%2520a%2520unified%250Adual-channel%2520transformer-based%2520framework%2520applicable%2520to%2520both%2520docking%2520and%25203D%2520drug%250Adesign%2520tasks%252C%2520which%2520exploits%2520their%2520duality%2520by%2520utilizing%2520docking%2520functionalities%250Awithin%2520the%2520drug%2520design%2520process.%2520Specifically%252C%2520we%2520represent%25203D%2520pocket-ligand%250Acomplexes%2520using%2520parallel%2520sequences%2520of%2520discrete%2520tokens%2520and%2520continuous%2520numbers%252C%250Aand%2520we%2520design%2520a%2520corresponding%2520dual-channel%2520transformer%2520model%2520to%2520handle%2520this%250Aformat%252C%2520thereby%2520overcoming%2520the%2520challenges%2520of%25203D%2520information%2520modeling.%250AAdditionally%252C%2520we%2520alleviate%2520data%2520limitations%2520through%2520large-scale%2520pre-training%2520on%250Aa%2520mixed%2520dataset%252C%2520followed%2520by%2520supervised%2520and%2520reinforcement%2520learning%2520fine-tuning%250Atechniques%2520respectively%2520tailored%2520for%2520the%2520two%2520tasks.%2520Experimental%2520results%250Ademonstrate%2520that%25203DMolFormer%2520outperforms%2520previous%2520approaches%2520in%2520both%250Aprotein-ligand%2520docking%2520and%2520pocket-aware%25203D%2520drug%2520design%252C%2520highlighting%2520its%250Apromising%2520application%2520in%2520structure-based%2520drug%2520discovery.%2520The%2520code%2520is%2520available%250Aat%253A%2520https%253A//github.com/HXYfighter/3DMolFormer%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05107v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DMolFormer%3A%20A%20Dual-channel%20Framework%20for%20Structure-based%20Drug%20Discovery&entry.906535625=Xiuyuan%20Hu%20and%20Guoqing%20Liu%20and%20Can%20Chen%20and%20Yang%20Zhao%20and%20Hao%20Zhang%20and%20Xue%20Liu&entry.1292438233=%20%20Structure-based%20drug%20discovery%2C%20encompassing%20the%20tasks%20of%20protein-ligand%0Adocking%20and%20pocket-aware%203D%20drug%20design%2C%20represents%20a%20core%20challenge%20in%20drug%0Adiscovery.%20However%2C%20no%20existing%20work%20can%20deal%20with%20both%20tasks%20to%20effectively%0Aleverage%20the%20duality%20between%20them%2C%20and%20current%20methods%20for%20each%20task%20are%0Ahindered%20by%20challenges%20in%20modeling%203D%20information%20and%20the%20limitations%20of%0Aavailable%20data.%20To%20address%20these%20issues%2C%20we%20propose%203DMolFormer%2C%20a%20unified%0Adual-channel%20transformer-based%20framework%20applicable%20to%20both%20docking%20and%203D%20drug%0Adesign%20tasks%2C%20which%20exploits%20their%20duality%20by%20utilizing%20docking%20functionalities%0Awithin%20the%20drug%20design%20process.%20Specifically%2C%20we%20represent%203D%20pocket-ligand%0Acomplexes%20using%20parallel%20sequences%20of%20discrete%20tokens%20and%20continuous%20numbers%2C%0Aand%20we%20design%20a%20corresponding%20dual-channel%20transformer%20model%20to%20handle%20this%0Aformat%2C%20thereby%20overcoming%20the%20challenges%20of%203D%20information%20modeling.%0AAdditionally%2C%20we%20alleviate%20data%20limitations%20through%20large-scale%20pre-training%20on%0Aa%20mixed%20dataset%2C%20followed%20by%20supervised%20and%20reinforcement%20learning%20fine-tuning%0Atechniques%20respectively%20tailored%20for%20the%20two%20tasks.%20Experimental%20results%0Ademonstrate%20that%203DMolFormer%20outperforms%20previous%20approaches%20in%20both%0Aprotein-ligand%20docking%20and%20pocket-aware%203D%20drug%20design%2C%20highlighting%20its%0Apromising%20application%20in%20structure-based%20drug%20discovery.%20The%20code%20is%20available%0Aat%3A%20https%3A//github.com/HXYfighter/3DMolFormer%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05107v1&entry.124074799=Read"},
{"title": "ChallengeMe: An Adversarial Learning-enabled Text Summarization\n  Framework", "author": "Xiaoyu Deng and Ye Zhang and Tianmin Guo and Yongzhe Zhang and Zhengjian Kang and Hang Yang", "abstract": "  The astonishing performance of large language models (LLMs) and their\nremarkable achievements in production and daily life have led to their\nwidespread application in collaborative tasks. However, current large models\nface challenges such as hallucination and lack of specificity in content\ngeneration in vertical domain tasks. Inspired by the contrast and\nclassification mechanisms in human cognitive processes, this paper constructs\nan adversarial learning-based prompt framework named ChallengeMe, which\nincludes three cascaded solutions: generation prompts, evaluation prompts, and\nfeedback optimization. In this process, we designed seven core optimization\ndimensions and set the threshold for adversarial learning. The results of mixed\ncase studies on the text summarization task show that the proposed framework\ncan generate more accurate and fluent text summaries compared to the current\nadvanced mainstream LLMs.\n", "link": "http://arxiv.org/abs/2502.05084v1", "date": "2025-02-07", "relevancy": 2.6273, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5344}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.521}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChallengeMe%3A%20An%20Adversarial%20Learning-enabled%20Text%20Summarization%0A%20%20Framework&body=Title%3A%20ChallengeMe%3A%20An%20Adversarial%20Learning-enabled%20Text%20Summarization%0A%20%20Framework%0AAuthor%3A%20Xiaoyu%20Deng%20and%20Ye%20Zhang%20and%20Tianmin%20Guo%20and%20Yongzhe%20Zhang%20and%20Zhengjian%20Kang%20and%20Hang%20Yang%0AAbstract%3A%20%20%20The%20astonishing%20performance%20of%20large%20language%20models%20%28LLMs%29%20and%20their%0Aremarkable%20achievements%20in%20production%20and%20daily%20life%20have%20led%20to%20their%0Awidespread%20application%20in%20collaborative%20tasks.%20However%2C%20current%20large%20models%0Aface%20challenges%20such%20as%20hallucination%20and%20lack%20of%20specificity%20in%20content%0Ageneration%20in%20vertical%20domain%20tasks.%20Inspired%20by%20the%20contrast%20and%0Aclassification%20mechanisms%20in%20human%20cognitive%20processes%2C%20this%20paper%20constructs%0Aan%20adversarial%20learning-based%20prompt%20framework%20named%20ChallengeMe%2C%20which%0Aincludes%20three%20cascaded%20solutions%3A%20generation%20prompts%2C%20evaluation%20prompts%2C%20and%0Afeedback%20optimization.%20In%20this%20process%2C%20we%20designed%20seven%20core%20optimization%0Adimensions%20and%20set%20the%20threshold%20for%20adversarial%20learning.%20The%20results%20of%20mixed%0Acase%20studies%20on%20the%20text%20summarization%20task%20show%20that%20the%20proposed%20framework%0Acan%20generate%20more%20accurate%20and%20fluent%20text%20summaries%20compared%20to%20the%20current%0Aadvanced%20mainstream%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05084v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChallengeMe%253A%2520An%2520Adversarial%2520Learning-enabled%2520Text%2520Summarization%250A%2520%2520Framework%26entry.906535625%3DXiaoyu%2520Deng%2520and%2520Ye%2520Zhang%2520and%2520Tianmin%2520Guo%2520and%2520Yongzhe%2520Zhang%2520and%2520Zhengjian%2520Kang%2520and%2520Hang%2520Yang%26entry.1292438233%3D%2520%2520The%2520astonishing%2520performance%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520their%250Aremarkable%2520achievements%2520in%2520production%2520and%2520daily%2520life%2520have%2520led%2520to%2520their%250Awidespread%2520application%2520in%2520collaborative%2520tasks.%2520However%252C%2520current%2520large%2520models%250Aface%2520challenges%2520such%2520as%2520hallucination%2520and%2520lack%2520of%2520specificity%2520in%2520content%250Ageneration%2520in%2520vertical%2520domain%2520tasks.%2520Inspired%2520by%2520the%2520contrast%2520and%250Aclassification%2520mechanisms%2520in%2520human%2520cognitive%2520processes%252C%2520this%2520paper%2520constructs%250Aan%2520adversarial%2520learning-based%2520prompt%2520framework%2520named%2520ChallengeMe%252C%2520which%250Aincludes%2520three%2520cascaded%2520solutions%253A%2520generation%2520prompts%252C%2520evaluation%2520prompts%252C%2520and%250Afeedback%2520optimization.%2520In%2520this%2520process%252C%2520we%2520designed%2520seven%2520core%2520optimization%250Adimensions%2520and%2520set%2520the%2520threshold%2520for%2520adversarial%2520learning.%2520The%2520results%2520of%2520mixed%250Acase%2520studies%2520on%2520the%2520text%2520summarization%2520task%2520show%2520that%2520the%2520proposed%2520framework%250Acan%2520generate%2520more%2520accurate%2520and%2520fluent%2520text%2520summaries%2520compared%2520to%2520the%2520current%250Aadvanced%2520mainstream%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05084v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChallengeMe%3A%20An%20Adversarial%20Learning-enabled%20Text%20Summarization%0A%20%20Framework&entry.906535625=Xiaoyu%20Deng%20and%20Ye%20Zhang%20and%20Tianmin%20Guo%20and%20Yongzhe%20Zhang%20and%20Zhengjian%20Kang%20and%20Hang%20Yang&entry.1292438233=%20%20The%20astonishing%20performance%20of%20large%20language%20models%20%28LLMs%29%20and%20their%0Aremarkable%20achievements%20in%20production%20and%20daily%20life%20have%20led%20to%20their%0Awidespread%20application%20in%20collaborative%20tasks.%20However%2C%20current%20large%20models%0Aface%20challenges%20such%20as%20hallucination%20and%20lack%20of%20specificity%20in%20content%0Ageneration%20in%20vertical%20domain%20tasks.%20Inspired%20by%20the%20contrast%20and%0Aclassification%20mechanisms%20in%20human%20cognitive%20processes%2C%20this%20paper%20constructs%0Aan%20adversarial%20learning-based%20prompt%20framework%20named%20ChallengeMe%2C%20which%0Aincludes%20three%20cascaded%20solutions%3A%20generation%20prompts%2C%20evaluation%20prompts%2C%20and%0Afeedback%20optimization.%20In%20this%20process%2C%20we%20designed%20seven%20core%20optimization%0Adimensions%20and%20set%20the%20threshold%20for%20adversarial%20learning.%20The%20results%20of%20mixed%0Acase%20studies%20on%20the%20text%20summarization%20task%20show%20that%20the%20proposed%20framework%0Acan%20generate%20more%20accurate%20and%20fluent%20text%20summaries%20compared%20to%20the%20current%0Aadvanced%20mainstream%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05084v1&entry.124074799=Read"},
{"title": "CT-AGRG: Automated Abnormality-Guided Report Generation from 3D Chest CT\n  Volumes", "author": "Theo Di Piazza and Carole Lazarus and Olivier Nempont and Loic Boussel", "abstract": "  The rapid increase of computed tomography (CT) scans and their time-consuming\nmanual analysis have created an urgent need for robust automated analysis\ntechniques in clinical settings. These aim to assist radiologists and help them\nmanaging their growing workload. Existing methods typically generate entire\nreports directly from 3D CT images, without explicitly focusing on observed\nabnormalities. This unguided approach often results in repetitive content or\nincomplete reports, failing to prioritize anomaly-specific descriptions. We\npropose a new anomaly-guided report generation model, which first predicts\nabnormalities and then generates targeted descriptions for each. Evaluation on\na public dataset demonstrates significant improvements in report quality and\nclinical relevance. We extend our work by conducting an ablation study to\ndemonstrate its effectiveness.\n", "link": "http://arxiv.org/abs/2408.11965v6", "date": "2025-02-07", "relevancy": 2.6255, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5377}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5377}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CT-AGRG%3A%20Automated%20Abnormality-Guided%20Report%20Generation%20from%203D%20Chest%20CT%0A%20%20Volumes&body=Title%3A%20CT-AGRG%3A%20Automated%20Abnormality-Guided%20Report%20Generation%20from%203D%20Chest%20CT%0A%20%20Volumes%0AAuthor%3A%20Theo%20Di%20Piazza%20and%20Carole%20Lazarus%20and%20Olivier%20Nempont%20and%20Loic%20Boussel%0AAbstract%3A%20%20%20The%20rapid%20increase%20of%20computed%20tomography%20%28CT%29%20scans%20and%20their%20time-consuming%0Amanual%20analysis%20have%20created%20an%20urgent%20need%20for%20robust%20automated%20analysis%0Atechniques%20in%20clinical%20settings.%20These%20aim%20to%20assist%20radiologists%20and%20help%20them%0Amanaging%20their%20growing%20workload.%20Existing%20methods%20typically%20generate%20entire%0Areports%20directly%20from%203D%20CT%20images%2C%20without%20explicitly%20focusing%20on%20observed%0Aabnormalities.%20This%20unguided%20approach%20often%20results%20in%20repetitive%20content%20or%0Aincomplete%20reports%2C%20failing%20to%20prioritize%20anomaly-specific%20descriptions.%20We%0Apropose%20a%20new%20anomaly-guided%20report%20generation%20model%2C%20which%20first%20predicts%0Aabnormalities%20and%20then%20generates%20targeted%20descriptions%20for%20each.%20Evaluation%20on%0Aa%20public%20dataset%20demonstrates%20significant%20improvements%20in%20report%20quality%20and%0Aclinical%20relevance.%20We%20extend%20our%20work%20by%20conducting%20an%20ablation%20study%20to%0Ademonstrate%20its%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11965v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCT-AGRG%253A%2520Automated%2520Abnormality-Guided%2520Report%2520Generation%2520from%25203D%2520Chest%2520CT%250A%2520%2520Volumes%26entry.906535625%3DTheo%2520Di%2520Piazza%2520and%2520Carole%2520Lazarus%2520and%2520Olivier%2520Nempont%2520and%2520Loic%2520Boussel%26entry.1292438233%3D%2520%2520The%2520rapid%2520increase%2520of%2520computed%2520tomography%2520%2528CT%2529%2520scans%2520and%2520their%2520time-consuming%250Amanual%2520analysis%2520have%2520created%2520an%2520urgent%2520need%2520for%2520robust%2520automated%2520analysis%250Atechniques%2520in%2520clinical%2520settings.%2520These%2520aim%2520to%2520assist%2520radiologists%2520and%2520help%2520them%250Amanaging%2520their%2520growing%2520workload.%2520Existing%2520methods%2520typically%2520generate%2520entire%250Areports%2520directly%2520from%25203D%2520CT%2520images%252C%2520without%2520explicitly%2520focusing%2520on%2520observed%250Aabnormalities.%2520This%2520unguided%2520approach%2520often%2520results%2520in%2520repetitive%2520content%2520or%250Aincomplete%2520reports%252C%2520failing%2520to%2520prioritize%2520anomaly-specific%2520descriptions.%2520We%250Apropose%2520a%2520new%2520anomaly-guided%2520report%2520generation%2520model%252C%2520which%2520first%2520predicts%250Aabnormalities%2520and%2520then%2520generates%2520targeted%2520descriptions%2520for%2520each.%2520Evaluation%2520on%250Aa%2520public%2520dataset%2520demonstrates%2520significant%2520improvements%2520in%2520report%2520quality%2520and%250Aclinical%2520relevance.%2520We%2520extend%2520our%2520work%2520by%2520conducting%2520an%2520ablation%2520study%2520to%250Ademonstrate%2520its%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11965v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CT-AGRG%3A%20Automated%20Abnormality-Guided%20Report%20Generation%20from%203D%20Chest%20CT%0A%20%20Volumes&entry.906535625=Theo%20Di%20Piazza%20and%20Carole%20Lazarus%20and%20Olivier%20Nempont%20and%20Loic%20Boussel&entry.1292438233=%20%20The%20rapid%20increase%20of%20computed%20tomography%20%28CT%29%20scans%20and%20their%20time-consuming%0Amanual%20analysis%20have%20created%20an%20urgent%20need%20for%20robust%20automated%20analysis%0Atechniques%20in%20clinical%20settings.%20These%20aim%20to%20assist%20radiologists%20and%20help%20them%0Amanaging%20their%20growing%20workload.%20Existing%20methods%20typically%20generate%20entire%0Areports%20directly%20from%203D%20CT%20images%2C%20without%20explicitly%20focusing%20on%20observed%0Aabnormalities.%20This%20unguided%20approach%20often%20results%20in%20repetitive%20content%20or%0Aincomplete%20reports%2C%20failing%20to%20prioritize%20anomaly-specific%20descriptions.%20We%0Apropose%20a%20new%20anomaly-guided%20report%20generation%20model%2C%20which%20first%20predicts%0Aabnormalities%20and%20then%20generates%20targeted%20descriptions%20for%20each.%20Evaluation%20on%0Aa%20public%20dataset%20demonstrates%20significant%20improvements%20in%20report%20quality%20and%0Aclinical%20relevance.%20We%20extend%20our%20work%20by%20conducting%20an%20ablation%20study%20to%0Ademonstrate%20its%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11965v6&entry.124074799=Read"},
{"title": "Explicit Relational Reasoning Network for Scene Text Detection", "author": "Yuchen Su and Zhineng Chen and Yongkun Du and Zhilong Ji and Kai Hu and Jinfeng Bai and Xieping Gao", "abstract": "  Connected component (CC) is a proper text shape representation that aligns\nwith human reading intuition. However, CC-based text detection methods have\nrecently faced a developmental bottleneck that their time-consuming\npost-processing is difficult to eliminate. To address this issue, we introduce\nan explicit relational reasoning network (ERRNet) to elegantly model the\ncomponent relationships without post-processing. Concretely, we first represent\neach text instance as multiple ordered text components, and then treat these\ncomponents as objects in sequential movement. In this way, scene text detection\ncan be innovatively viewed as a tracking problem. From this perspective, we\ndesign an end-to-end tracking decoder to achieve a CC-based method dispensing\nwith post-processing entirely. Additionally, we observe that there is an\ninconsistency between classification confidence and localization quality, so we\npropose a Polygon Monte-Carlo method to quickly and accurately evaluate the\nlocalization quality. Based on this, we introduce a position-supervised\nclassification loss to guide the task-aligned learning of ERRNet. Experiments\non challenging benchmarks demonstrate the effectiveness of our ERRNet. It\nconsistently achieves state-of-the-art accuracy while holding highly\ncompetitive inference speed.\n", "link": "http://arxiv.org/abs/2412.14692v3", "date": "2025-02-07", "relevancy": 2.6233, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5261}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5261}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explicit%20Relational%20Reasoning%20Network%20for%20Scene%20Text%20Detection&body=Title%3A%20Explicit%20Relational%20Reasoning%20Network%20for%20Scene%20Text%20Detection%0AAuthor%3A%20Yuchen%20Su%20and%20Zhineng%20Chen%20and%20Yongkun%20Du%20and%20Zhilong%20Ji%20and%20Kai%20Hu%20and%20Jinfeng%20Bai%20and%20Xieping%20Gao%0AAbstract%3A%20%20%20Connected%20component%20%28CC%29%20is%20a%20proper%20text%20shape%20representation%20that%20aligns%0Awith%20human%20reading%20intuition.%20However%2C%20CC-based%20text%20detection%20methods%20have%0Arecently%20faced%20a%20developmental%20bottleneck%20that%20their%20time-consuming%0Apost-processing%20is%20difficult%20to%20eliminate.%20To%20address%20this%20issue%2C%20we%20introduce%0Aan%20explicit%20relational%20reasoning%20network%20%28ERRNet%29%20to%20elegantly%20model%20the%0Acomponent%20relationships%20without%20post-processing.%20Concretely%2C%20we%20first%20represent%0Aeach%20text%20instance%20as%20multiple%20ordered%20text%20components%2C%20and%20then%20treat%20these%0Acomponents%20as%20objects%20in%20sequential%20movement.%20In%20this%20way%2C%20scene%20text%20detection%0Acan%20be%20innovatively%20viewed%20as%20a%20tracking%20problem.%20From%20this%20perspective%2C%20we%0Adesign%20an%20end-to-end%20tracking%20decoder%20to%20achieve%20a%20CC-based%20method%20dispensing%0Awith%20post-processing%20entirely.%20Additionally%2C%20we%20observe%20that%20there%20is%20an%0Ainconsistency%20between%20classification%20confidence%20and%20localization%20quality%2C%20so%20we%0Apropose%20a%20Polygon%20Monte-Carlo%20method%20to%20quickly%20and%20accurately%20evaluate%20the%0Alocalization%20quality.%20Based%20on%20this%2C%20we%20introduce%20a%20position-supervised%0Aclassification%20loss%20to%20guide%20the%20task-aligned%20learning%20of%20ERRNet.%20Experiments%0Aon%20challenging%20benchmarks%20demonstrate%20the%20effectiveness%20of%20our%20ERRNet.%20It%0Aconsistently%20achieves%20state-of-the-art%20accuracy%20while%20holding%20highly%0Acompetitive%20inference%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14692v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplicit%2520Relational%2520Reasoning%2520Network%2520for%2520Scene%2520Text%2520Detection%26entry.906535625%3DYuchen%2520Su%2520and%2520Zhineng%2520Chen%2520and%2520Yongkun%2520Du%2520and%2520Zhilong%2520Ji%2520and%2520Kai%2520Hu%2520and%2520Jinfeng%2520Bai%2520and%2520Xieping%2520Gao%26entry.1292438233%3D%2520%2520Connected%2520component%2520%2528CC%2529%2520is%2520a%2520proper%2520text%2520shape%2520representation%2520that%2520aligns%250Awith%2520human%2520reading%2520intuition.%2520However%252C%2520CC-based%2520text%2520detection%2520methods%2520have%250Arecently%2520faced%2520a%2520developmental%2520bottleneck%2520that%2520their%2520time-consuming%250Apost-processing%2520is%2520difficult%2520to%2520eliminate.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%250Aan%2520explicit%2520relational%2520reasoning%2520network%2520%2528ERRNet%2529%2520to%2520elegantly%2520model%2520the%250Acomponent%2520relationships%2520without%2520post-processing.%2520Concretely%252C%2520we%2520first%2520represent%250Aeach%2520text%2520instance%2520as%2520multiple%2520ordered%2520text%2520components%252C%2520and%2520then%2520treat%2520these%250Acomponents%2520as%2520objects%2520in%2520sequential%2520movement.%2520In%2520this%2520way%252C%2520scene%2520text%2520detection%250Acan%2520be%2520innovatively%2520viewed%2520as%2520a%2520tracking%2520problem.%2520From%2520this%2520perspective%252C%2520we%250Adesign%2520an%2520end-to-end%2520tracking%2520decoder%2520to%2520achieve%2520a%2520CC-based%2520method%2520dispensing%250Awith%2520post-processing%2520entirely.%2520Additionally%252C%2520we%2520observe%2520that%2520there%2520is%2520an%250Ainconsistency%2520between%2520classification%2520confidence%2520and%2520localization%2520quality%252C%2520so%2520we%250Apropose%2520a%2520Polygon%2520Monte-Carlo%2520method%2520to%2520quickly%2520and%2520accurately%2520evaluate%2520the%250Alocalization%2520quality.%2520Based%2520on%2520this%252C%2520we%2520introduce%2520a%2520position-supervised%250Aclassification%2520loss%2520to%2520guide%2520the%2520task-aligned%2520learning%2520of%2520ERRNet.%2520Experiments%250Aon%2520challenging%2520benchmarks%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520ERRNet.%2520It%250Aconsistently%2520achieves%2520state-of-the-art%2520accuracy%2520while%2520holding%2520highly%250Acompetitive%2520inference%2520speed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14692v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explicit%20Relational%20Reasoning%20Network%20for%20Scene%20Text%20Detection&entry.906535625=Yuchen%20Su%20and%20Zhineng%20Chen%20and%20Yongkun%20Du%20and%20Zhilong%20Ji%20and%20Kai%20Hu%20and%20Jinfeng%20Bai%20and%20Xieping%20Gao&entry.1292438233=%20%20Connected%20component%20%28CC%29%20is%20a%20proper%20text%20shape%20representation%20that%20aligns%0Awith%20human%20reading%20intuition.%20However%2C%20CC-based%20text%20detection%20methods%20have%0Arecently%20faced%20a%20developmental%20bottleneck%20that%20their%20time-consuming%0Apost-processing%20is%20difficult%20to%20eliminate.%20To%20address%20this%20issue%2C%20we%20introduce%0Aan%20explicit%20relational%20reasoning%20network%20%28ERRNet%29%20to%20elegantly%20model%20the%0Acomponent%20relationships%20without%20post-processing.%20Concretely%2C%20we%20first%20represent%0Aeach%20text%20instance%20as%20multiple%20ordered%20text%20components%2C%20and%20then%20treat%20these%0Acomponents%20as%20objects%20in%20sequential%20movement.%20In%20this%20way%2C%20scene%20text%20detection%0Acan%20be%20innovatively%20viewed%20as%20a%20tracking%20problem.%20From%20this%20perspective%2C%20we%0Adesign%20an%20end-to-end%20tracking%20decoder%20to%20achieve%20a%20CC-based%20method%20dispensing%0Awith%20post-processing%20entirely.%20Additionally%2C%20we%20observe%20that%20there%20is%20an%0Ainconsistency%20between%20classification%20confidence%20and%20localization%20quality%2C%20so%20we%0Apropose%20a%20Polygon%20Monte-Carlo%20method%20to%20quickly%20and%20accurately%20evaluate%20the%0Alocalization%20quality.%20Based%20on%20this%2C%20we%20introduce%20a%20position-supervised%0Aclassification%20loss%20to%20guide%20the%20task-aligned%20learning%20of%20ERRNet.%20Experiments%0Aon%20challenging%20benchmarks%20demonstrate%20the%20effectiveness%20of%20our%20ERRNet.%20It%0Aconsistently%20achieves%20state-of-the-art%20accuracy%20while%20holding%20highly%0Acompetitive%20inference%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14692v3&entry.124074799=Read"},
{"title": "In-context denoising with one-layer transformers: connections between\n  attention and associative memory retrieval", "author": "Matthew Smart and Alberto Bietti and Anirvan M. Sengupta", "abstract": "  We introduce in-context denoising, a task that refines the connection between\nattention-based architectures and dense associative memory (DAM) networks, also\nknown as modern Hopfield networks. Using a Bayesian framework, we show\ntheoretically and empirically that certain restricted denoising problems can be\nsolved optimally even by a single-layer transformer. We demonstrate that a\ntrained attention layer processes each denoising prompt by performing a single\ngradient descent update on a context-aware DAM energy landscape, where context\ntokens serve as associative memories and the query token acts as an initial\nstate. This one-step update yields better solutions than exact retrieval of\neither a context token or a spurious local minimum, providing a concrete\nexample of DAM networks extending beyond the standard retrieval paradigm.\nOverall, this work solidifies the link between associative memory and attention\nmechanisms first identified by Ramsauer et al., and demonstrates the relevance\nof associative memory models in the study of in-context learning.\n", "link": "http://arxiv.org/abs/2502.05164v1", "date": "2025-02-07", "relevancy": 2.6123, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5412}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5131}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In-context%20denoising%20with%20one-layer%20transformers%3A%20connections%20between%0A%20%20attention%20and%20associative%20memory%20retrieval&body=Title%3A%20In-context%20denoising%20with%20one-layer%20transformers%3A%20connections%20between%0A%20%20attention%20and%20associative%20memory%20retrieval%0AAuthor%3A%20Matthew%20Smart%20and%20Alberto%20Bietti%20and%20Anirvan%20M.%20Sengupta%0AAbstract%3A%20%20%20We%20introduce%20in-context%20denoising%2C%20a%20task%20that%20refines%20the%20connection%20between%0Aattention-based%20architectures%20and%20dense%20associative%20memory%20%28DAM%29%20networks%2C%20also%0Aknown%20as%20modern%20Hopfield%20networks.%20Using%20a%20Bayesian%20framework%2C%20we%20show%0Atheoretically%20and%20empirically%20that%20certain%20restricted%20denoising%20problems%20can%20be%0Asolved%20optimally%20even%20by%20a%20single-layer%20transformer.%20We%20demonstrate%20that%20a%0Atrained%20attention%20layer%20processes%20each%20denoising%20prompt%20by%20performing%20a%20single%0Agradient%20descent%20update%20on%20a%20context-aware%20DAM%20energy%20landscape%2C%20where%20context%0Atokens%20serve%20as%20associative%20memories%20and%20the%20query%20token%20acts%20as%20an%20initial%0Astate.%20This%20one-step%20update%20yields%20better%20solutions%20than%20exact%20retrieval%20of%0Aeither%20a%20context%20token%20or%20a%20spurious%20local%20minimum%2C%20providing%20a%20concrete%0Aexample%20of%20DAM%20networks%20extending%20beyond%20the%20standard%20retrieval%20paradigm.%0AOverall%2C%20this%20work%20solidifies%20the%20link%20between%20associative%20memory%20and%20attention%0Amechanisms%20first%20identified%20by%20Ramsauer%20et%20al.%2C%20and%20demonstrates%20the%20relevance%0Aof%20associative%20memory%20models%20in%20the%20study%20of%20in-context%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05164v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn-context%2520denoising%2520with%2520one-layer%2520transformers%253A%2520connections%2520between%250A%2520%2520attention%2520and%2520associative%2520memory%2520retrieval%26entry.906535625%3DMatthew%2520Smart%2520and%2520Alberto%2520Bietti%2520and%2520Anirvan%2520M.%2520Sengupta%26entry.1292438233%3D%2520%2520We%2520introduce%2520in-context%2520denoising%252C%2520a%2520task%2520that%2520refines%2520the%2520connection%2520between%250Aattention-based%2520architectures%2520and%2520dense%2520associative%2520memory%2520%2528DAM%2529%2520networks%252C%2520also%250Aknown%2520as%2520modern%2520Hopfield%2520networks.%2520Using%2520a%2520Bayesian%2520framework%252C%2520we%2520show%250Atheoretically%2520and%2520empirically%2520that%2520certain%2520restricted%2520denoising%2520problems%2520can%2520be%250Asolved%2520optimally%2520even%2520by%2520a%2520single-layer%2520transformer.%2520We%2520demonstrate%2520that%2520a%250Atrained%2520attention%2520layer%2520processes%2520each%2520denoising%2520prompt%2520by%2520performing%2520a%2520single%250Agradient%2520descent%2520update%2520on%2520a%2520context-aware%2520DAM%2520energy%2520landscape%252C%2520where%2520context%250Atokens%2520serve%2520as%2520associative%2520memories%2520and%2520the%2520query%2520token%2520acts%2520as%2520an%2520initial%250Astate.%2520This%2520one-step%2520update%2520yields%2520better%2520solutions%2520than%2520exact%2520retrieval%2520of%250Aeither%2520a%2520context%2520token%2520or%2520a%2520spurious%2520local%2520minimum%252C%2520providing%2520a%2520concrete%250Aexample%2520of%2520DAM%2520networks%2520extending%2520beyond%2520the%2520standard%2520retrieval%2520paradigm.%250AOverall%252C%2520this%2520work%2520solidifies%2520the%2520link%2520between%2520associative%2520memory%2520and%2520attention%250Amechanisms%2520first%2520identified%2520by%2520Ramsauer%2520et%2520al.%252C%2520and%2520demonstrates%2520the%2520relevance%250Aof%2520associative%2520memory%2520models%2520in%2520the%2520study%2520of%2520in-context%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05164v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-context%20denoising%20with%20one-layer%20transformers%3A%20connections%20between%0A%20%20attention%20and%20associative%20memory%20retrieval&entry.906535625=Matthew%20Smart%20and%20Alberto%20Bietti%20and%20Anirvan%20M.%20Sengupta&entry.1292438233=%20%20We%20introduce%20in-context%20denoising%2C%20a%20task%20that%20refines%20the%20connection%20between%0Aattention-based%20architectures%20and%20dense%20associative%20memory%20%28DAM%29%20networks%2C%20also%0Aknown%20as%20modern%20Hopfield%20networks.%20Using%20a%20Bayesian%20framework%2C%20we%20show%0Atheoretically%20and%20empirically%20that%20certain%20restricted%20denoising%20problems%20can%20be%0Asolved%20optimally%20even%20by%20a%20single-layer%20transformer.%20We%20demonstrate%20that%20a%0Atrained%20attention%20layer%20processes%20each%20denoising%20prompt%20by%20performing%20a%20single%0Agradient%20descent%20update%20on%20a%20context-aware%20DAM%20energy%20landscape%2C%20where%20context%0Atokens%20serve%20as%20associative%20memories%20and%20the%20query%20token%20acts%20as%20an%20initial%0Astate.%20This%20one-step%20update%20yields%20better%20solutions%20than%20exact%20retrieval%20of%0Aeither%20a%20context%20token%20or%20a%20spurious%20local%20minimum%2C%20providing%20a%20concrete%0Aexample%20of%20DAM%20networks%20extending%20beyond%20the%20standard%20retrieval%20paradigm.%0AOverall%2C%20this%20work%20solidifies%20the%20link%20between%20associative%20memory%20and%20attention%0Amechanisms%20first%20identified%20by%20Ramsauer%20et%20al.%2C%20and%20demonstrates%20the%20relevance%0Aof%20associative%20memory%20models%20in%20the%20study%20of%20in-context%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05164v1&entry.124074799=Read"},
{"title": "GLAM: Glomeruli Segmentation for Human Pathological Lesions using\n  Adapted Mouse Model", "author": "Lining Yu and Mengmeng Yin and Ruining Deng and Quan Liu and Tianyuan Yao and Can Cui and Yitian Long and Yu Wang and Yaohong Wang and Shilin Zhao and Haichun Yang and Yuankai Huo", "abstract": "  Moving from animal models to human applications in preclinical research\nencompasses a broad spectrum of disciplines in medical science. A fundamental\nelement in the development of new drugs, treatments, diagnostic methods, and in\ndeepening our understanding of disease processes is the accurate measurement of\nkidney tissues. Past studies have demonstrated the viability of translating\nglomeruli segmentation techniques from mouse models to human applications. Yet,\nthese investigations tend to neglect the complexities involved in segmenting\npathological glomeruli affected by different lesions. Such lesions present a\nwider range of morphological variations compared to healthy glomerular tissue,\nwhich are arguably more valuable than normal glomeruli in clinical practice.\nFurthermore, data on lesions from animal models can be more readily scaled up\nfrom disease models and whole kidney biopsies. This brings up a question:\n``\\textit{Can a pathological segmentation model trained on mouse models be\neffectively applied to human patients?}\" To answer this question, we introduced\nGLAM, a deep learning study for fine-grained segmentation of human kidney\nlesions using a mouse model, addressing mouse-to-human transfer learning, by\nevaluating different learning strategies for segmenting human pathological\nlesions using zero-shot transfer learning and hybrid learning by leveraging\nmouse samples. From the results, the hybrid learning model achieved superior\nperformance.\n", "link": "http://arxiv.org/abs/2407.18390v2", "date": "2025-02-07", "relevancy": 2.6017, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5548}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5121}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4942}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLAM%3A%20Glomeruli%20Segmentation%20for%20Human%20Pathological%20Lesions%20using%0A%20%20Adapted%20Mouse%20Model&body=Title%3A%20GLAM%3A%20Glomeruli%20Segmentation%20for%20Human%20Pathological%20Lesions%20using%0A%20%20Adapted%20Mouse%20Model%0AAuthor%3A%20Lining%20Yu%20and%20Mengmeng%20Yin%20and%20Ruining%20Deng%20and%20Quan%20Liu%20and%20Tianyuan%20Yao%20and%20Can%20Cui%20and%20Yitian%20Long%20and%20Yu%20Wang%20and%20Yaohong%20Wang%20and%20Shilin%20Zhao%20and%20Haichun%20Yang%20and%20Yuankai%20Huo%0AAbstract%3A%20%20%20Moving%20from%20animal%20models%20to%20human%20applications%20in%20preclinical%20research%0Aencompasses%20a%20broad%20spectrum%20of%20disciplines%20in%20medical%20science.%20A%20fundamental%0Aelement%20in%20the%20development%20of%20new%20drugs%2C%20treatments%2C%20diagnostic%20methods%2C%20and%20in%0Adeepening%20our%20understanding%20of%20disease%20processes%20is%20the%20accurate%20measurement%20of%0Akidney%20tissues.%20Past%20studies%20have%20demonstrated%20the%20viability%20of%20translating%0Aglomeruli%20segmentation%20techniques%20from%20mouse%20models%20to%20human%20applications.%20Yet%2C%0Athese%20investigations%20tend%20to%20neglect%20the%20complexities%20involved%20in%20segmenting%0Apathological%20glomeruli%20affected%20by%20different%20lesions.%20Such%20lesions%20present%20a%0Awider%20range%20of%20morphological%20variations%20compared%20to%20healthy%20glomerular%20tissue%2C%0Awhich%20are%20arguably%20more%20valuable%20than%20normal%20glomeruli%20in%20clinical%20practice.%0AFurthermore%2C%20data%20on%20lesions%20from%20animal%20models%20can%20be%20more%20readily%20scaled%20up%0Afrom%20disease%20models%20and%20whole%20kidney%20biopsies.%20This%20brings%20up%20a%20question%3A%0A%60%60%5Ctextit%7BCan%20a%20pathological%20segmentation%20model%20trained%20on%20mouse%20models%20be%0Aeffectively%20applied%20to%20human%20patients%3F%7D%22%20To%20answer%20this%20question%2C%20we%20introduced%0AGLAM%2C%20a%20deep%20learning%20study%20for%20fine-grained%20segmentation%20of%20human%20kidney%0Alesions%20using%20a%20mouse%20model%2C%20addressing%20mouse-to-human%20transfer%20learning%2C%20by%0Aevaluating%20different%20learning%20strategies%20for%20segmenting%20human%20pathological%0Alesions%20using%20zero-shot%20transfer%20learning%20and%20hybrid%20learning%20by%20leveraging%0Amouse%20samples.%20From%20the%20results%2C%20the%20hybrid%20learning%20model%20achieved%20superior%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18390v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLAM%253A%2520Glomeruli%2520Segmentation%2520for%2520Human%2520Pathological%2520Lesions%2520using%250A%2520%2520Adapted%2520Mouse%2520Model%26entry.906535625%3DLining%2520Yu%2520and%2520Mengmeng%2520Yin%2520and%2520Ruining%2520Deng%2520and%2520Quan%2520Liu%2520and%2520Tianyuan%2520Yao%2520and%2520Can%2520Cui%2520and%2520Yitian%2520Long%2520and%2520Yu%2520Wang%2520and%2520Yaohong%2520Wang%2520and%2520Shilin%2520Zhao%2520and%2520Haichun%2520Yang%2520and%2520Yuankai%2520Huo%26entry.1292438233%3D%2520%2520Moving%2520from%2520animal%2520models%2520to%2520human%2520applications%2520in%2520preclinical%2520research%250Aencompasses%2520a%2520broad%2520spectrum%2520of%2520disciplines%2520in%2520medical%2520science.%2520A%2520fundamental%250Aelement%2520in%2520the%2520development%2520of%2520new%2520drugs%252C%2520treatments%252C%2520diagnostic%2520methods%252C%2520and%2520in%250Adeepening%2520our%2520understanding%2520of%2520disease%2520processes%2520is%2520the%2520accurate%2520measurement%2520of%250Akidney%2520tissues.%2520Past%2520studies%2520have%2520demonstrated%2520the%2520viability%2520of%2520translating%250Aglomeruli%2520segmentation%2520techniques%2520from%2520mouse%2520models%2520to%2520human%2520applications.%2520Yet%252C%250Athese%2520investigations%2520tend%2520to%2520neglect%2520the%2520complexities%2520involved%2520in%2520segmenting%250Apathological%2520glomeruli%2520affected%2520by%2520different%2520lesions.%2520Such%2520lesions%2520present%2520a%250Awider%2520range%2520of%2520morphological%2520variations%2520compared%2520to%2520healthy%2520glomerular%2520tissue%252C%250Awhich%2520are%2520arguably%2520more%2520valuable%2520than%2520normal%2520glomeruli%2520in%2520clinical%2520practice.%250AFurthermore%252C%2520data%2520on%2520lesions%2520from%2520animal%2520models%2520can%2520be%2520more%2520readily%2520scaled%2520up%250Afrom%2520disease%2520models%2520and%2520whole%2520kidney%2520biopsies.%2520This%2520brings%2520up%2520a%2520question%253A%250A%2560%2560%255Ctextit%257BCan%2520a%2520pathological%2520segmentation%2520model%2520trained%2520on%2520mouse%2520models%2520be%250Aeffectively%2520applied%2520to%2520human%2520patients%253F%257D%2522%2520To%2520answer%2520this%2520question%252C%2520we%2520introduced%250AGLAM%252C%2520a%2520deep%2520learning%2520study%2520for%2520fine-grained%2520segmentation%2520of%2520human%2520kidney%250Alesions%2520using%2520a%2520mouse%2520model%252C%2520addressing%2520mouse-to-human%2520transfer%2520learning%252C%2520by%250Aevaluating%2520different%2520learning%2520strategies%2520for%2520segmenting%2520human%2520pathological%250Alesions%2520using%2520zero-shot%2520transfer%2520learning%2520and%2520hybrid%2520learning%2520by%2520leveraging%250Amouse%2520samples.%2520From%2520the%2520results%252C%2520the%2520hybrid%2520learning%2520model%2520achieved%2520superior%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18390v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLAM%3A%20Glomeruli%20Segmentation%20for%20Human%20Pathological%20Lesions%20using%0A%20%20Adapted%20Mouse%20Model&entry.906535625=Lining%20Yu%20and%20Mengmeng%20Yin%20and%20Ruining%20Deng%20and%20Quan%20Liu%20and%20Tianyuan%20Yao%20and%20Can%20Cui%20and%20Yitian%20Long%20and%20Yu%20Wang%20and%20Yaohong%20Wang%20and%20Shilin%20Zhao%20and%20Haichun%20Yang%20and%20Yuankai%20Huo&entry.1292438233=%20%20Moving%20from%20animal%20models%20to%20human%20applications%20in%20preclinical%20research%0Aencompasses%20a%20broad%20spectrum%20of%20disciplines%20in%20medical%20science.%20A%20fundamental%0Aelement%20in%20the%20development%20of%20new%20drugs%2C%20treatments%2C%20diagnostic%20methods%2C%20and%20in%0Adeepening%20our%20understanding%20of%20disease%20processes%20is%20the%20accurate%20measurement%20of%0Akidney%20tissues.%20Past%20studies%20have%20demonstrated%20the%20viability%20of%20translating%0Aglomeruli%20segmentation%20techniques%20from%20mouse%20models%20to%20human%20applications.%20Yet%2C%0Athese%20investigations%20tend%20to%20neglect%20the%20complexities%20involved%20in%20segmenting%0Apathological%20glomeruli%20affected%20by%20different%20lesions.%20Such%20lesions%20present%20a%0Awider%20range%20of%20morphological%20variations%20compared%20to%20healthy%20glomerular%20tissue%2C%0Awhich%20are%20arguably%20more%20valuable%20than%20normal%20glomeruli%20in%20clinical%20practice.%0AFurthermore%2C%20data%20on%20lesions%20from%20animal%20models%20can%20be%20more%20readily%20scaled%20up%0Afrom%20disease%20models%20and%20whole%20kidney%20biopsies.%20This%20brings%20up%20a%20question%3A%0A%60%60%5Ctextit%7BCan%20a%20pathological%20segmentation%20model%20trained%20on%20mouse%20models%20be%0Aeffectively%20applied%20to%20human%20patients%3F%7D%22%20To%20answer%20this%20question%2C%20we%20introduced%0AGLAM%2C%20a%20deep%20learning%20study%20for%20fine-grained%20segmentation%20of%20human%20kidney%0Alesions%20using%20a%20mouse%20model%2C%20addressing%20mouse-to-human%20transfer%20learning%2C%20by%0Aevaluating%20different%20learning%20strategies%20for%20segmenting%20human%20pathological%0Alesions%20using%20zero-shot%20transfer%20learning%20and%20hybrid%20learning%20by%20leveraging%0Amouse%20samples.%20From%20the%20results%2C%20the%20hybrid%20learning%20model%20achieved%20superior%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18390v2&entry.124074799=Read"},
{"title": "Grounding Continuous Representations in Geometry: Equivariant Neural\n  Fields", "author": "David R Wessels and David M Knigge and Samuele Papa and Riccardo Valperga and Sharvaree Vadgama and Efstratios Gavves and Erik J Bekkers", "abstract": "  Conditional Neural Fields (CNFs) are increasingly being leveraged as\ncontinuous signal representations, by associating each data-sample with a\nlatent variable that conditions a shared backbone Neural Field (NeF) to\nreconstruct the sample. However, existing CNF architectures face limitations\nwhen using this latent downstream in tasks requiring fine-grained geometric\nreasoning, such as classification and segmentation. We posit that this results\nfrom lack of explicit modelling of geometric information (e.g., locality in the\nsignal or the orientation of a feature) in the latent space of CNFs. As such,\nwe propose Equivariant Neural Fields (ENFs), a novel CNF architecture which\nuses a geometry-informed cross-attention to condition the NeF on a geometric\nvariable--a latent point cloud of features--that enables an equivariant\ndecoding from latent to field. We show that this approach induces a\nsteerability property by which both field and latent are grounded in geometry\nand amenable to transformation laws: if the field transforms, the latent\nrepresentation transforms accordingly--and vice versa. Crucially, this\nequivariance relation ensures that the latent is capable of (1) representing\ngeometric patterns faithfully, allowing for geometric reasoning in latent\nspace, and (2) weight-sharing over similar local patterns, allowing for\nefficient learning of datasets of fields. We validate these main properties in\na range of tasks including classification, segmentation, forecasting,\nreconstruction and generative modelling, showing clear improvement over\nbaselines with a geometry-free latent space. Code attached to submission\nhttps://github.com/Dafidofff/enf-jax. Code for a clean and minimal repo\nhttps://github.com/david-knigge/enf-min-jax.\n", "link": "http://arxiv.org/abs/2406.05753v5", "date": "2025-02-07", "relevancy": 2.5858, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5372}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5078}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grounding%20Continuous%20Representations%20in%20Geometry%3A%20Equivariant%20Neural%0A%20%20Fields&body=Title%3A%20Grounding%20Continuous%20Representations%20in%20Geometry%3A%20Equivariant%20Neural%0A%20%20Fields%0AAuthor%3A%20David%20R%20Wessels%20and%20David%20M%20Knigge%20and%20Samuele%20Papa%20and%20Riccardo%20Valperga%20and%20Sharvaree%20Vadgama%20and%20Efstratios%20Gavves%20and%20Erik%20J%20Bekkers%0AAbstract%3A%20%20%20Conditional%20Neural%20Fields%20%28CNFs%29%20are%20increasingly%20being%20leveraged%20as%0Acontinuous%20signal%20representations%2C%20by%20associating%20each%20data-sample%20with%20a%0Alatent%20variable%20that%20conditions%20a%20shared%20backbone%20Neural%20Field%20%28NeF%29%20to%0Areconstruct%20the%20sample.%20However%2C%20existing%20CNF%20architectures%20face%20limitations%0Awhen%20using%20this%20latent%20downstream%20in%20tasks%20requiring%20fine-grained%20geometric%0Areasoning%2C%20such%20as%20classification%20and%20segmentation.%20We%20posit%20that%20this%20results%0Afrom%20lack%20of%20explicit%20modelling%20of%20geometric%20information%20%28e.g.%2C%20locality%20in%20the%0Asignal%20or%20the%20orientation%20of%20a%20feature%29%20in%20the%20latent%20space%20of%20CNFs.%20As%20such%2C%0Awe%20propose%20Equivariant%20Neural%20Fields%20%28ENFs%29%2C%20a%20novel%20CNF%20architecture%20which%0Auses%20a%20geometry-informed%20cross-attention%20to%20condition%20the%20NeF%20on%20a%20geometric%0Avariable--a%20latent%20point%20cloud%20of%20features--that%20enables%20an%20equivariant%0Adecoding%20from%20latent%20to%20field.%20We%20show%20that%20this%20approach%20induces%20a%0Asteerability%20property%20by%20which%20both%20field%20and%20latent%20are%20grounded%20in%20geometry%0Aand%20amenable%20to%20transformation%20laws%3A%20if%20the%20field%20transforms%2C%20the%20latent%0Arepresentation%20transforms%20accordingly--and%20vice%20versa.%20Crucially%2C%20this%0Aequivariance%20relation%20ensures%20that%20the%20latent%20is%20capable%20of%20%281%29%20representing%0Ageometric%20patterns%20faithfully%2C%20allowing%20for%20geometric%20reasoning%20in%20latent%0Aspace%2C%20and%20%282%29%20weight-sharing%20over%20similar%20local%20patterns%2C%20allowing%20for%0Aefficient%20learning%20of%20datasets%20of%20fields.%20We%20validate%20these%20main%20properties%20in%0Aa%20range%20of%20tasks%20including%20classification%2C%20segmentation%2C%20forecasting%2C%0Areconstruction%20and%20generative%20modelling%2C%20showing%20clear%20improvement%20over%0Abaselines%20with%20a%20geometry-free%20latent%20space.%20Code%20attached%20to%20submission%0Ahttps%3A//github.com/Dafidofff/enf-jax.%20Code%20for%20a%20clean%20and%20minimal%20repo%0Ahttps%3A//github.com/david-knigge/enf-min-jax.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05753v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrounding%2520Continuous%2520Representations%2520in%2520Geometry%253A%2520Equivariant%2520Neural%250A%2520%2520Fields%26entry.906535625%3DDavid%2520R%2520Wessels%2520and%2520David%2520M%2520Knigge%2520and%2520Samuele%2520Papa%2520and%2520Riccardo%2520Valperga%2520and%2520Sharvaree%2520Vadgama%2520and%2520Efstratios%2520Gavves%2520and%2520Erik%2520J%2520Bekkers%26entry.1292438233%3D%2520%2520Conditional%2520Neural%2520Fields%2520%2528CNFs%2529%2520are%2520increasingly%2520being%2520leveraged%2520as%250Acontinuous%2520signal%2520representations%252C%2520by%2520associating%2520each%2520data-sample%2520with%2520a%250Alatent%2520variable%2520that%2520conditions%2520a%2520shared%2520backbone%2520Neural%2520Field%2520%2528NeF%2529%2520to%250Areconstruct%2520the%2520sample.%2520However%252C%2520existing%2520CNF%2520architectures%2520face%2520limitations%250Awhen%2520using%2520this%2520latent%2520downstream%2520in%2520tasks%2520requiring%2520fine-grained%2520geometric%250Areasoning%252C%2520such%2520as%2520classification%2520and%2520segmentation.%2520We%2520posit%2520that%2520this%2520results%250Afrom%2520lack%2520of%2520explicit%2520modelling%2520of%2520geometric%2520information%2520%2528e.g.%252C%2520locality%2520in%2520the%250Asignal%2520or%2520the%2520orientation%2520of%2520a%2520feature%2529%2520in%2520the%2520latent%2520space%2520of%2520CNFs.%2520As%2520such%252C%250Awe%2520propose%2520Equivariant%2520Neural%2520Fields%2520%2528ENFs%2529%252C%2520a%2520novel%2520CNF%2520architecture%2520which%250Auses%2520a%2520geometry-informed%2520cross-attention%2520to%2520condition%2520the%2520NeF%2520on%2520a%2520geometric%250Avariable--a%2520latent%2520point%2520cloud%2520of%2520features--that%2520enables%2520an%2520equivariant%250Adecoding%2520from%2520latent%2520to%2520field.%2520We%2520show%2520that%2520this%2520approach%2520induces%2520a%250Asteerability%2520property%2520by%2520which%2520both%2520field%2520and%2520latent%2520are%2520grounded%2520in%2520geometry%250Aand%2520amenable%2520to%2520transformation%2520laws%253A%2520if%2520the%2520field%2520transforms%252C%2520the%2520latent%250Arepresentation%2520transforms%2520accordingly--and%2520vice%2520versa.%2520Crucially%252C%2520this%250Aequivariance%2520relation%2520ensures%2520that%2520the%2520latent%2520is%2520capable%2520of%2520%25281%2529%2520representing%250Ageometric%2520patterns%2520faithfully%252C%2520allowing%2520for%2520geometric%2520reasoning%2520in%2520latent%250Aspace%252C%2520and%2520%25282%2529%2520weight-sharing%2520over%2520similar%2520local%2520patterns%252C%2520allowing%2520for%250Aefficient%2520learning%2520of%2520datasets%2520of%2520fields.%2520We%2520validate%2520these%2520main%2520properties%2520in%250Aa%2520range%2520of%2520tasks%2520including%2520classification%252C%2520segmentation%252C%2520forecasting%252C%250Areconstruction%2520and%2520generative%2520modelling%252C%2520showing%2520clear%2520improvement%2520over%250Abaselines%2520with%2520a%2520geometry-free%2520latent%2520space.%2520Code%2520attached%2520to%2520submission%250Ahttps%253A//github.com/Dafidofff/enf-jax.%2520Code%2520for%2520a%2520clean%2520and%2520minimal%2520repo%250Ahttps%253A//github.com/david-knigge/enf-min-jax.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05753v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grounding%20Continuous%20Representations%20in%20Geometry%3A%20Equivariant%20Neural%0A%20%20Fields&entry.906535625=David%20R%20Wessels%20and%20David%20M%20Knigge%20and%20Samuele%20Papa%20and%20Riccardo%20Valperga%20and%20Sharvaree%20Vadgama%20and%20Efstratios%20Gavves%20and%20Erik%20J%20Bekkers&entry.1292438233=%20%20Conditional%20Neural%20Fields%20%28CNFs%29%20are%20increasingly%20being%20leveraged%20as%0Acontinuous%20signal%20representations%2C%20by%20associating%20each%20data-sample%20with%20a%0Alatent%20variable%20that%20conditions%20a%20shared%20backbone%20Neural%20Field%20%28NeF%29%20to%0Areconstruct%20the%20sample.%20However%2C%20existing%20CNF%20architectures%20face%20limitations%0Awhen%20using%20this%20latent%20downstream%20in%20tasks%20requiring%20fine-grained%20geometric%0Areasoning%2C%20such%20as%20classification%20and%20segmentation.%20We%20posit%20that%20this%20results%0Afrom%20lack%20of%20explicit%20modelling%20of%20geometric%20information%20%28e.g.%2C%20locality%20in%20the%0Asignal%20or%20the%20orientation%20of%20a%20feature%29%20in%20the%20latent%20space%20of%20CNFs.%20As%20such%2C%0Awe%20propose%20Equivariant%20Neural%20Fields%20%28ENFs%29%2C%20a%20novel%20CNF%20architecture%20which%0Auses%20a%20geometry-informed%20cross-attention%20to%20condition%20the%20NeF%20on%20a%20geometric%0Avariable--a%20latent%20point%20cloud%20of%20features--that%20enables%20an%20equivariant%0Adecoding%20from%20latent%20to%20field.%20We%20show%20that%20this%20approach%20induces%20a%0Asteerability%20property%20by%20which%20both%20field%20and%20latent%20are%20grounded%20in%20geometry%0Aand%20amenable%20to%20transformation%20laws%3A%20if%20the%20field%20transforms%2C%20the%20latent%0Arepresentation%20transforms%20accordingly--and%20vice%20versa.%20Crucially%2C%20this%0Aequivariance%20relation%20ensures%20that%20the%20latent%20is%20capable%20of%20%281%29%20representing%0Ageometric%20patterns%20faithfully%2C%20allowing%20for%20geometric%20reasoning%20in%20latent%0Aspace%2C%20and%20%282%29%20weight-sharing%20over%20similar%20local%20patterns%2C%20allowing%20for%0Aefficient%20learning%20of%20datasets%20of%20fields.%20We%20validate%20these%20main%20properties%20in%0Aa%20range%20of%20tasks%20including%20classification%2C%20segmentation%2C%20forecasting%2C%0Areconstruction%20and%20generative%20modelling%2C%20showing%20clear%20improvement%20over%0Abaselines%20with%20a%20geometry-free%20latent%20space.%20Code%20attached%20to%20submission%0Ahttps%3A//github.com/Dafidofff/enf-jax.%20Code%20for%20a%20clean%20and%20minimal%20repo%0Ahttps%3A//github.com/david-knigge/enf-min-jax.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05753v5&entry.124074799=Read"},
{"title": "Chest X-ray Foundation Model with Global and Local Representations\n  Integration", "author": "Zefan Yang and Xuanang Xu and Jiajin Zhang and Ge Wang and Mannudeep K. Kalra and Pingkun Yan", "abstract": "  Chest X-ray (CXR) is the most frequently ordered imaging test, supporting\ndiverse clinical tasks from thoracic disease detection to postoperative\nmonitoring. However, task-specific classification models are limited in scope,\nrequire costly labeled data, and lack generalizability to out-of-distribution\ndatasets. To address these challenges, we introduce CheXFound, a\nself-supervised vision foundation model that learns robust CXR representations\nand generalizes effectively across a wide range of downstream tasks. We\npretrain CheXFound on a curated CXR-1M dataset, comprising over one million\nunique CXRs from publicly available sources. We propose a Global and Local\nRepresentations Integration (GLoRI) module for downstream adaptations, by\nincorporating disease-specific local features with global image features for\nenhanced performance in multilabel classification. Our experimental results\nshow that CheXFound outperforms state-of-the-art models in classifying 40\ndisease findings across different prevalence levels on the CXR-LT 24 dataset\nand exhibits superior label efficiency on downstream tasks with limited\ntraining data. Additionally, CheXFound achieved significant improvements on new\ntasks with out-of-distribution datasets, including opportunistic cardiovascular\ndisease risk estimation and mortality prediction. These results highlight\nCheXFound's strong generalization capabilities, enabling diverse adaptations\nwith improved label efficiency. The project source code is publicly available\nat https://github.com/RPIDIAL/CheXFound.\n", "link": "http://arxiv.org/abs/2502.05142v1", "date": "2025-02-07", "relevancy": 2.5716, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5222}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5222}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chest%20X-ray%20Foundation%20Model%20with%20Global%20and%20Local%20Representations%0A%20%20Integration&body=Title%3A%20Chest%20X-ray%20Foundation%20Model%20with%20Global%20and%20Local%20Representations%0A%20%20Integration%0AAuthor%3A%20Zefan%20Yang%20and%20Xuanang%20Xu%20and%20Jiajin%20Zhang%20and%20Ge%20Wang%20and%20Mannudeep%20K.%20Kalra%20and%20Pingkun%20Yan%0AAbstract%3A%20%20%20Chest%20X-ray%20%28CXR%29%20is%20the%20most%20frequently%20ordered%20imaging%20test%2C%20supporting%0Adiverse%20clinical%20tasks%20from%20thoracic%20disease%20detection%20to%20postoperative%0Amonitoring.%20However%2C%20task-specific%20classification%20models%20are%20limited%20in%20scope%2C%0Arequire%20costly%20labeled%20data%2C%20and%20lack%20generalizability%20to%20out-of-distribution%0Adatasets.%20To%20address%20these%20challenges%2C%20we%20introduce%20CheXFound%2C%20a%0Aself-supervised%20vision%20foundation%20model%20that%20learns%20robust%20CXR%20representations%0Aand%20generalizes%20effectively%20across%20a%20wide%20range%20of%20downstream%20tasks.%20We%0Apretrain%20CheXFound%20on%20a%20curated%20CXR-1M%20dataset%2C%20comprising%20over%20one%20million%0Aunique%20CXRs%20from%20publicly%20available%20sources.%20We%20propose%20a%20Global%20and%20Local%0ARepresentations%20Integration%20%28GLoRI%29%20module%20for%20downstream%20adaptations%2C%20by%0Aincorporating%20disease-specific%20local%20features%20with%20global%20image%20features%20for%0Aenhanced%20performance%20in%20multilabel%20classification.%20Our%20experimental%20results%0Ashow%20that%20CheXFound%20outperforms%20state-of-the-art%20models%20in%20classifying%2040%0Adisease%20findings%20across%20different%20prevalence%20levels%20on%20the%20CXR-LT%2024%20dataset%0Aand%20exhibits%20superior%20label%20efficiency%20on%20downstream%20tasks%20with%20limited%0Atraining%20data.%20Additionally%2C%20CheXFound%20achieved%20significant%20improvements%20on%20new%0Atasks%20with%20out-of-distribution%20datasets%2C%20including%20opportunistic%20cardiovascular%0Adisease%20risk%20estimation%20and%20mortality%20prediction.%20These%20results%20highlight%0ACheXFound%27s%20strong%20generalization%20capabilities%2C%20enabling%20diverse%20adaptations%0Awith%20improved%20label%20efficiency.%20The%20project%20source%20code%20is%20publicly%20available%0Aat%20https%3A//github.com/RPIDIAL/CheXFound.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05142v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChest%2520X-ray%2520Foundation%2520Model%2520with%2520Global%2520and%2520Local%2520Representations%250A%2520%2520Integration%26entry.906535625%3DZefan%2520Yang%2520and%2520Xuanang%2520Xu%2520and%2520Jiajin%2520Zhang%2520and%2520Ge%2520Wang%2520and%2520Mannudeep%2520K.%2520Kalra%2520and%2520Pingkun%2520Yan%26entry.1292438233%3D%2520%2520Chest%2520X-ray%2520%2528CXR%2529%2520is%2520the%2520most%2520frequently%2520ordered%2520imaging%2520test%252C%2520supporting%250Adiverse%2520clinical%2520tasks%2520from%2520thoracic%2520disease%2520detection%2520to%2520postoperative%250Amonitoring.%2520However%252C%2520task-specific%2520classification%2520models%2520are%2520limited%2520in%2520scope%252C%250Arequire%2520costly%2520labeled%2520data%252C%2520and%2520lack%2520generalizability%2520to%2520out-of-distribution%250Adatasets.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520CheXFound%252C%2520a%250Aself-supervised%2520vision%2520foundation%2520model%2520that%2520learns%2520robust%2520CXR%2520representations%250Aand%2520generalizes%2520effectively%2520across%2520a%2520wide%2520range%2520of%2520downstream%2520tasks.%2520We%250Apretrain%2520CheXFound%2520on%2520a%2520curated%2520CXR-1M%2520dataset%252C%2520comprising%2520over%2520one%2520million%250Aunique%2520CXRs%2520from%2520publicly%2520available%2520sources.%2520We%2520propose%2520a%2520Global%2520and%2520Local%250ARepresentations%2520Integration%2520%2528GLoRI%2529%2520module%2520for%2520downstream%2520adaptations%252C%2520by%250Aincorporating%2520disease-specific%2520local%2520features%2520with%2520global%2520image%2520features%2520for%250Aenhanced%2520performance%2520in%2520multilabel%2520classification.%2520Our%2520experimental%2520results%250Ashow%2520that%2520CheXFound%2520outperforms%2520state-of-the-art%2520models%2520in%2520classifying%252040%250Adisease%2520findings%2520across%2520different%2520prevalence%2520levels%2520on%2520the%2520CXR-LT%252024%2520dataset%250Aand%2520exhibits%2520superior%2520label%2520efficiency%2520on%2520downstream%2520tasks%2520with%2520limited%250Atraining%2520data.%2520Additionally%252C%2520CheXFound%2520achieved%2520significant%2520improvements%2520on%2520new%250Atasks%2520with%2520out-of-distribution%2520datasets%252C%2520including%2520opportunistic%2520cardiovascular%250Adisease%2520risk%2520estimation%2520and%2520mortality%2520prediction.%2520These%2520results%2520highlight%250ACheXFound%2527s%2520strong%2520generalization%2520capabilities%252C%2520enabling%2520diverse%2520adaptations%250Awith%2520improved%2520label%2520efficiency.%2520The%2520project%2520source%2520code%2520is%2520publicly%2520available%250Aat%2520https%253A//github.com/RPIDIAL/CheXFound.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05142v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chest%20X-ray%20Foundation%20Model%20with%20Global%20and%20Local%20Representations%0A%20%20Integration&entry.906535625=Zefan%20Yang%20and%20Xuanang%20Xu%20and%20Jiajin%20Zhang%20and%20Ge%20Wang%20and%20Mannudeep%20K.%20Kalra%20and%20Pingkun%20Yan&entry.1292438233=%20%20Chest%20X-ray%20%28CXR%29%20is%20the%20most%20frequently%20ordered%20imaging%20test%2C%20supporting%0Adiverse%20clinical%20tasks%20from%20thoracic%20disease%20detection%20to%20postoperative%0Amonitoring.%20However%2C%20task-specific%20classification%20models%20are%20limited%20in%20scope%2C%0Arequire%20costly%20labeled%20data%2C%20and%20lack%20generalizability%20to%20out-of-distribution%0Adatasets.%20To%20address%20these%20challenges%2C%20we%20introduce%20CheXFound%2C%20a%0Aself-supervised%20vision%20foundation%20model%20that%20learns%20robust%20CXR%20representations%0Aand%20generalizes%20effectively%20across%20a%20wide%20range%20of%20downstream%20tasks.%20We%0Apretrain%20CheXFound%20on%20a%20curated%20CXR-1M%20dataset%2C%20comprising%20over%20one%20million%0Aunique%20CXRs%20from%20publicly%20available%20sources.%20We%20propose%20a%20Global%20and%20Local%0ARepresentations%20Integration%20%28GLoRI%29%20module%20for%20downstream%20adaptations%2C%20by%0Aincorporating%20disease-specific%20local%20features%20with%20global%20image%20features%20for%0Aenhanced%20performance%20in%20multilabel%20classification.%20Our%20experimental%20results%0Ashow%20that%20CheXFound%20outperforms%20state-of-the-art%20models%20in%20classifying%2040%0Adisease%20findings%20across%20different%20prevalence%20levels%20on%20the%20CXR-LT%2024%20dataset%0Aand%20exhibits%20superior%20label%20efficiency%20on%20downstream%20tasks%20with%20limited%0Atraining%20data.%20Additionally%2C%20CheXFound%20achieved%20significant%20improvements%20on%20new%0Atasks%20with%20out-of-distribution%20datasets%2C%20including%20opportunistic%20cardiovascular%0Adisease%20risk%20estimation%20and%20mortality%20prediction.%20These%20results%20highlight%0ACheXFound%27s%20strong%20generalization%20capabilities%2C%20enabling%20diverse%20adaptations%0Awith%20improved%20label%20efficiency.%20The%20project%20source%20code%20is%20publicly%20available%0Aat%20https%3A//github.com/RPIDIAL/CheXFound.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05142v1&entry.124074799=Read"},
{"title": "FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution\n  Video Generation", "author": "Shilong Zhang and Wenbo Li and Shoufa Chen and Chongjian Ge and Peize Sun and Yida Zhang and Yi Jiang and Zehuan Yuan and Binyue Peng and Ping Luo", "abstract": "  DiT diffusion models have achieved great success in text-to-video generation,\nleveraging their scalability in model capacity and data scale. High content and\nmotion fidelity aligned with text prompts, however, often require large model\nparameters and a substantial number of function evaluations (NFEs). Realistic\nand visually appealing details are typically reflected in high resolution\noutputs, further amplifying computational demands especially for single stage\nDiT models. To address these challenges, we propose a novel two stage\nframework, FlashVideo, which strategically allocates model capacity and NFEs\nacross stages to balance generation fidelity and quality. In the first stage,\nprompt fidelity is prioritized through a low resolution generation process\nutilizing large parameters and sufficient NFEs to enhance computational\nefficiency. The second stage establishes flow matching between low and high\nresolutions, effectively generating fine details with minimal NFEs.\nQuantitative and visual results demonstrate that FlashVideo achieves\nstate-of-the-art high resolution video generation with superior computational\nefficiency. Additionally, the two-stage design enables users to preview the\ninitial output before committing to full resolution generation, thereby\nsignificantly reducing computational costs and wait times as well as enhancing\ncommercial viability .\n", "link": "http://arxiv.org/abs/2502.05179v1", "date": "2025-02-07", "relevancy": 2.5682, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.659}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6383}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlashVideo%3AFlowing%20Fidelity%20to%20Detail%20for%20Efficient%20High-Resolution%0A%20%20Video%20Generation&body=Title%3A%20FlashVideo%3AFlowing%20Fidelity%20to%20Detail%20for%20Efficient%20High-Resolution%0A%20%20Video%20Generation%0AAuthor%3A%20Shilong%20Zhang%20and%20Wenbo%20Li%20and%20Shoufa%20Chen%20and%20Chongjian%20Ge%20and%20Peize%20Sun%20and%20Yida%20Zhang%20and%20Yi%20Jiang%20and%20Zehuan%20Yuan%20and%20Binyue%20Peng%20and%20Ping%20Luo%0AAbstract%3A%20%20%20DiT%20diffusion%20models%20have%20achieved%20great%20success%20in%20text-to-video%20generation%2C%0Aleveraging%20their%20scalability%20in%20model%20capacity%20and%20data%20scale.%20High%20content%20and%0Amotion%20fidelity%20aligned%20with%20text%20prompts%2C%20however%2C%20often%20require%20large%20model%0Aparameters%20and%20a%20substantial%20number%20of%20function%20evaluations%20%28NFEs%29.%20Realistic%0Aand%20visually%20appealing%20details%20are%20typically%20reflected%20in%20high%20resolution%0Aoutputs%2C%20further%20amplifying%20computational%20demands%20especially%20for%20single%20stage%0ADiT%20models.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20two%20stage%0Aframework%2C%20FlashVideo%2C%20which%20strategically%20allocates%20model%20capacity%20and%20NFEs%0Aacross%20stages%20to%20balance%20generation%20fidelity%20and%20quality.%20In%20the%20first%20stage%2C%0Aprompt%20fidelity%20is%20prioritized%20through%20a%20low%20resolution%20generation%20process%0Autilizing%20large%20parameters%20and%20sufficient%20NFEs%20to%20enhance%20computational%0Aefficiency.%20The%20second%20stage%20establishes%20flow%20matching%20between%20low%20and%20high%0Aresolutions%2C%20effectively%20generating%20fine%20details%20with%20minimal%20NFEs.%0AQuantitative%20and%20visual%20results%20demonstrate%20that%20FlashVideo%20achieves%0Astate-of-the-art%20high%20resolution%20video%20generation%20with%20superior%20computational%0Aefficiency.%20Additionally%2C%20the%20two-stage%20design%20enables%20users%20to%20preview%20the%0Ainitial%20output%20before%20committing%20to%20full%20resolution%20generation%2C%20thereby%0Asignificantly%20reducing%20computational%20costs%20and%20wait%20times%20as%20well%20as%20enhancing%0Acommercial%20viability%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05179v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlashVideo%253AFlowing%2520Fidelity%2520to%2520Detail%2520for%2520Efficient%2520High-Resolution%250A%2520%2520Video%2520Generation%26entry.906535625%3DShilong%2520Zhang%2520and%2520Wenbo%2520Li%2520and%2520Shoufa%2520Chen%2520and%2520Chongjian%2520Ge%2520and%2520Peize%2520Sun%2520and%2520Yida%2520Zhang%2520and%2520Yi%2520Jiang%2520and%2520Zehuan%2520Yuan%2520and%2520Binyue%2520Peng%2520and%2520Ping%2520Luo%26entry.1292438233%3D%2520%2520DiT%2520diffusion%2520models%2520have%2520achieved%2520great%2520success%2520in%2520text-to-video%2520generation%252C%250Aleveraging%2520their%2520scalability%2520in%2520model%2520capacity%2520and%2520data%2520scale.%2520High%2520content%2520and%250Amotion%2520fidelity%2520aligned%2520with%2520text%2520prompts%252C%2520however%252C%2520often%2520require%2520large%2520model%250Aparameters%2520and%2520a%2520substantial%2520number%2520of%2520function%2520evaluations%2520%2528NFEs%2529.%2520Realistic%250Aand%2520visually%2520appealing%2520details%2520are%2520typically%2520reflected%2520in%2520high%2520resolution%250Aoutputs%252C%2520further%2520amplifying%2520computational%2520demands%2520especially%2520for%2520single%2520stage%250ADiT%2520models.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520two%2520stage%250Aframework%252C%2520FlashVideo%252C%2520which%2520strategically%2520allocates%2520model%2520capacity%2520and%2520NFEs%250Aacross%2520stages%2520to%2520balance%2520generation%2520fidelity%2520and%2520quality.%2520In%2520the%2520first%2520stage%252C%250Aprompt%2520fidelity%2520is%2520prioritized%2520through%2520a%2520low%2520resolution%2520generation%2520process%250Autilizing%2520large%2520parameters%2520and%2520sufficient%2520NFEs%2520to%2520enhance%2520computational%250Aefficiency.%2520The%2520second%2520stage%2520establishes%2520flow%2520matching%2520between%2520low%2520and%2520high%250Aresolutions%252C%2520effectively%2520generating%2520fine%2520details%2520with%2520minimal%2520NFEs.%250AQuantitative%2520and%2520visual%2520results%2520demonstrate%2520that%2520FlashVideo%2520achieves%250Astate-of-the-art%2520high%2520resolution%2520video%2520generation%2520with%2520superior%2520computational%250Aefficiency.%2520Additionally%252C%2520the%2520two-stage%2520design%2520enables%2520users%2520to%2520preview%2520the%250Ainitial%2520output%2520before%2520committing%2520to%2520full%2520resolution%2520generation%252C%2520thereby%250Asignificantly%2520reducing%2520computational%2520costs%2520and%2520wait%2520times%2520as%2520well%2520as%2520enhancing%250Acommercial%2520viability%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05179v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlashVideo%3AFlowing%20Fidelity%20to%20Detail%20for%20Efficient%20High-Resolution%0A%20%20Video%20Generation&entry.906535625=Shilong%20Zhang%20and%20Wenbo%20Li%20and%20Shoufa%20Chen%20and%20Chongjian%20Ge%20and%20Peize%20Sun%20and%20Yida%20Zhang%20and%20Yi%20Jiang%20and%20Zehuan%20Yuan%20and%20Binyue%20Peng%20and%20Ping%20Luo&entry.1292438233=%20%20DiT%20diffusion%20models%20have%20achieved%20great%20success%20in%20text-to-video%20generation%2C%0Aleveraging%20their%20scalability%20in%20model%20capacity%20and%20data%20scale.%20High%20content%20and%0Amotion%20fidelity%20aligned%20with%20text%20prompts%2C%20however%2C%20often%20require%20large%20model%0Aparameters%20and%20a%20substantial%20number%20of%20function%20evaluations%20%28NFEs%29.%20Realistic%0Aand%20visually%20appealing%20details%20are%20typically%20reflected%20in%20high%20resolution%0Aoutputs%2C%20further%20amplifying%20computational%20demands%20especially%20for%20single%20stage%0ADiT%20models.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20two%20stage%0Aframework%2C%20FlashVideo%2C%20which%20strategically%20allocates%20model%20capacity%20and%20NFEs%0Aacross%20stages%20to%20balance%20generation%20fidelity%20and%20quality.%20In%20the%20first%20stage%2C%0Aprompt%20fidelity%20is%20prioritized%20through%20a%20low%20resolution%20generation%20process%0Autilizing%20large%20parameters%20and%20sufficient%20NFEs%20to%20enhance%20computational%0Aefficiency.%20The%20second%20stage%20establishes%20flow%20matching%20between%20low%20and%20high%0Aresolutions%2C%20effectively%20generating%20fine%20details%20with%20minimal%20NFEs.%0AQuantitative%20and%20visual%20results%20demonstrate%20that%20FlashVideo%20achieves%0Astate-of-the-art%20high%20resolution%20video%20generation%20with%20superior%20computational%0Aefficiency.%20Additionally%2C%20the%20two-stage%20design%20enables%20users%20to%20preview%20the%0Ainitial%20output%20before%20committing%20to%20full%20resolution%20generation%2C%20thereby%0Asignificantly%20reducing%20computational%20costs%20and%20wait%20times%20as%20well%20as%20enhancing%0Acommercial%20viability%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05179v1&entry.124074799=Read"},
{"title": "Sparse Autoencoders Do Not Find Canonical Units of Analysis", "author": "Patrick Leask and Bart Bussmann and Michael Pearce and Joseph Bloom and Curt Tigges and Noura Al Moubayed and Lee Sharkey and Neel Nanda", "abstract": "  A common goal of mechanistic interpretability is to decompose the activations\nof neural networks into features: interpretable properties of the input\ncomputed by the model. Sparse autoencoders (SAEs) are a popular method for\nfinding these features in LLMs, and it has been postulated that they can be\nused to find a \\textit{canonical} set of units: a unique and complete list of\natomic features. We cast doubt on this belief using two novel techniques: SAE\nstitching to show they are incomplete, and meta-SAEs to show they are not\natomic. SAE stitching involves inserting or swapping latents from a larger SAE\ninto a smaller one. Latents from the larger SAE can be divided into two\ncategories: \\emph{novel latents}, which improve performance when added to the\nsmaller SAE, indicating they capture novel information, and\n\\emph{reconstruction latents}, which can replace corresponding latents in the\nsmaller SAE that have similar behavior. The existence of novel features\nindicates incompleteness of smaller SAEs. Using meta-SAEs -- SAEs trained on\nthe decoder matrix of another SAE -- we find that latents in SAEs often\ndecompose into combinations of latents from a smaller SAE, showing that larger\nSAE latents are not atomic. The resulting decompositions are often\ninterpretable; e.g. a latent representing ``Einstein'' decomposes into\n``scientist'', ``Germany'', and ``famous person''. Even if SAEs do not find\ncanonical units of analysis, they may still be useful tools. We suggest that\nfuture research should either pursue different approaches for identifying such\nunits, or pragmatically choose the SAE size suited to their task. We provide an\ninteractive dashboard to explore meta-SAEs: https://metasaes.streamlit.app/\n", "link": "http://arxiv.org/abs/2502.04878v1", "date": "2025-02-07", "relevancy": 2.5243, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5076}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5076}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Autoencoders%20Do%20Not%20Find%20Canonical%20Units%20of%20Analysis&body=Title%3A%20Sparse%20Autoencoders%20Do%20Not%20Find%20Canonical%20Units%20of%20Analysis%0AAuthor%3A%20Patrick%20Leask%20and%20Bart%20Bussmann%20and%20Michael%20Pearce%20and%20Joseph%20Bloom%20and%20Curt%20Tigges%20and%20Noura%20Al%20Moubayed%20and%20Lee%20Sharkey%20and%20Neel%20Nanda%0AAbstract%3A%20%20%20A%20common%20goal%20of%20mechanistic%20interpretability%20is%20to%20decompose%20the%20activations%0Aof%20neural%20networks%20into%20features%3A%20interpretable%20properties%20of%20the%20input%0Acomputed%20by%20the%20model.%20Sparse%20autoencoders%20%28SAEs%29%20are%20a%20popular%20method%20for%0Afinding%20these%20features%20in%20LLMs%2C%20and%20it%20has%20been%20postulated%20that%20they%20can%20be%0Aused%20to%20find%20a%20%5Ctextit%7Bcanonical%7D%20set%20of%20units%3A%20a%20unique%20and%20complete%20list%20of%0Aatomic%20features.%20We%20cast%20doubt%20on%20this%20belief%20using%20two%20novel%20techniques%3A%20SAE%0Astitching%20to%20show%20they%20are%20incomplete%2C%20and%20meta-SAEs%20to%20show%20they%20are%20not%0Aatomic.%20SAE%20stitching%20involves%20inserting%20or%20swapping%20latents%20from%20a%20larger%20SAE%0Ainto%20a%20smaller%20one.%20Latents%20from%20the%20larger%20SAE%20can%20be%20divided%20into%20two%0Acategories%3A%20%5Cemph%7Bnovel%20latents%7D%2C%20which%20improve%20performance%20when%20added%20to%20the%0Asmaller%20SAE%2C%20indicating%20they%20capture%20novel%20information%2C%20and%0A%5Cemph%7Breconstruction%20latents%7D%2C%20which%20can%20replace%20corresponding%20latents%20in%20the%0Asmaller%20SAE%20that%20have%20similar%20behavior.%20The%20existence%20of%20novel%20features%0Aindicates%20incompleteness%20of%20smaller%20SAEs.%20Using%20meta-SAEs%20--%20SAEs%20trained%20on%0Athe%20decoder%20matrix%20of%20another%20SAE%20--%20we%20find%20that%20latents%20in%20SAEs%20often%0Adecompose%20into%20combinations%20of%20latents%20from%20a%20smaller%20SAE%2C%20showing%20that%20larger%0ASAE%20latents%20are%20not%20atomic.%20The%20resulting%20decompositions%20are%20often%0Ainterpretable%3B%20e.g.%20a%20latent%20representing%20%60%60Einstein%27%27%20decomposes%20into%0A%60%60scientist%27%27%2C%20%60%60Germany%27%27%2C%20and%20%60%60famous%20person%27%27.%20Even%20if%20SAEs%20do%20not%20find%0Acanonical%20units%20of%20analysis%2C%20they%20may%20still%20be%20useful%20tools.%20We%20suggest%20that%0Afuture%20research%20should%20either%20pursue%20different%20approaches%20for%20identifying%20such%0Aunits%2C%20or%20pragmatically%20choose%20the%20SAE%20size%20suited%20to%20their%20task.%20We%20provide%20an%0Ainteractive%20dashboard%20to%20explore%20meta-SAEs%3A%20https%3A//metasaes.streamlit.app/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04878v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Autoencoders%2520Do%2520Not%2520Find%2520Canonical%2520Units%2520of%2520Analysis%26entry.906535625%3DPatrick%2520Leask%2520and%2520Bart%2520Bussmann%2520and%2520Michael%2520Pearce%2520and%2520Joseph%2520Bloom%2520and%2520Curt%2520Tigges%2520and%2520Noura%2520Al%2520Moubayed%2520and%2520Lee%2520Sharkey%2520and%2520Neel%2520Nanda%26entry.1292438233%3D%2520%2520A%2520common%2520goal%2520of%2520mechanistic%2520interpretability%2520is%2520to%2520decompose%2520the%2520activations%250Aof%2520neural%2520networks%2520into%2520features%253A%2520interpretable%2520properties%2520of%2520the%2520input%250Acomputed%2520by%2520the%2520model.%2520Sparse%2520autoencoders%2520%2528SAEs%2529%2520are%2520a%2520popular%2520method%2520for%250Afinding%2520these%2520features%2520in%2520LLMs%252C%2520and%2520it%2520has%2520been%2520postulated%2520that%2520they%2520can%2520be%250Aused%2520to%2520find%2520a%2520%255Ctextit%257Bcanonical%257D%2520set%2520of%2520units%253A%2520a%2520unique%2520and%2520complete%2520list%2520of%250Aatomic%2520features.%2520We%2520cast%2520doubt%2520on%2520this%2520belief%2520using%2520two%2520novel%2520techniques%253A%2520SAE%250Astitching%2520to%2520show%2520they%2520are%2520incomplete%252C%2520and%2520meta-SAEs%2520to%2520show%2520they%2520are%2520not%250Aatomic.%2520SAE%2520stitching%2520involves%2520inserting%2520or%2520swapping%2520latents%2520from%2520a%2520larger%2520SAE%250Ainto%2520a%2520smaller%2520one.%2520Latents%2520from%2520the%2520larger%2520SAE%2520can%2520be%2520divided%2520into%2520two%250Acategories%253A%2520%255Cemph%257Bnovel%2520latents%257D%252C%2520which%2520improve%2520performance%2520when%2520added%2520to%2520the%250Asmaller%2520SAE%252C%2520indicating%2520they%2520capture%2520novel%2520information%252C%2520and%250A%255Cemph%257Breconstruction%2520latents%257D%252C%2520which%2520can%2520replace%2520corresponding%2520latents%2520in%2520the%250Asmaller%2520SAE%2520that%2520have%2520similar%2520behavior.%2520The%2520existence%2520of%2520novel%2520features%250Aindicates%2520incompleteness%2520of%2520smaller%2520SAEs.%2520Using%2520meta-SAEs%2520--%2520SAEs%2520trained%2520on%250Athe%2520decoder%2520matrix%2520of%2520another%2520SAE%2520--%2520we%2520find%2520that%2520latents%2520in%2520SAEs%2520often%250Adecompose%2520into%2520combinations%2520of%2520latents%2520from%2520a%2520smaller%2520SAE%252C%2520showing%2520that%2520larger%250ASAE%2520latents%2520are%2520not%2520atomic.%2520The%2520resulting%2520decompositions%2520are%2520often%250Ainterpretable%253B%2520e.g.%2520a%2520latent%2520representing%2520%2560%2560Einstein%2527%2527%2520decomposes%2520into%250A%2560%2560scientist%2527%2527%252C%2520%2560%2560Germany%2527%2527%252C%2520and%2520%2560%2560famous%2520person%2527%2527.%2520Even%2520if%2520SAEs%2520do%2520not%2520find%250Acanonical%2520units%2520of%2520analysis%252C%2520they%2520may%2520still%2520be%2520useful%2520tools.%2520We%2520suggest%2520that%250Afuture%2520research%2520should%2520either%2520pursue%2520different%2520approaches%2520for%2520identifying%2520such%250Aunits%252C%2520or%2520pragmatically%2520choose%2520the%2520SAE%2520size%2520suited%2520to%2520their%2520task.%2520We%2520provide%2520an%250Ainteractive%2520dashboard%2520to%2520explore%2520meta-SAEs%253A%2520https%253A//metasaes.streamlit.app/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04878v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Autoencoders%20Do%20Not%20Find%20Canonical%20Units%20of%20Analysis&entry.906535625=Patrick%20Leask%20and%20Bart%20Bussmann%20and%20Michael%20Pearce%20and%20Joseph%20Bloom%20and%20Curt%20Tigges%20and%20Noura%20Al%20Moubayed%20and%20Lee%20Sharkey%20and%20Neel%20Nanda&entry.1292438233=%20%20A%20common%20goal%20of%20mechanistic%20interpretability%20is%20to%20decompose%20the%20activations%0Aof%20neural%20networks%20into%20features%3A%20interpretable%20properties%20of%20the%20input%0Acomputed%20by%20the%20model.%20Sparse%20autoencoders%20%28SAEs%29%20are%20a%20popular%20method%20for%0Afinding%20these%20features%20in%20LLMs%2C%20and%20it%20has%20been%20postulated%20that%20they%20can%20be%0Aused%20to%20find%20a%20%5Ctextit%7Bcanonical%7D%20set%20of%20units%3A%20a%20unique%20and%20complete%20list%20of%0Aatomic%20features.%20We%20cast%20doubt%20on%20this%20belief%20using%20two%20novel%20techniques%3A%20SAE%0Astitching%20to%20show%20they%20are%20incomplete%2C%20and%20meta-SAEs%20to%20show%20they%20are%20not%0Aatomic.%20SAE%20stitching%20involves%20inserting%20or%20swapping%20latents%20from%20a%20larger%20SAE%0Ainto%20a%20smaller%20one.%20Latents%20from%20the%20larger%20SAE%20can%20be%20divided%20into%20two%0Acategories%3A%20%5Cemph%7Bnovel%20latents%7D%2C%20which%20improve%20performance%20when%20added%20to%20the%0Asmaller%20SAE%2C%20indicating%20they%20capture%20novel%20information%2C%20and%0A%5Cemph%7Breconstruction%20latents%7D%2C%20which%20can%20replace%20corresponding%20latents%20in%20the%0Asmaller%20SAE%20that%20have%20similar%20behavior.%20The%20existence%20of%20novel%20features%0Aindicates%20incompleteness%20of%20smaller%20SAEs.%20Using%20meta-SAEs%20--%20SAEs%20trained%20on%0Athe%20decoder%20matrix%20of%20another%20SAE%20--%20we%20find%20that%20latents%20in%20SAEs%20often%0Adecompose%20into%20combinations%20of%20latents%20from%20a%20smaller%20SAE%2C%20showing%20that%20larger%0ASAE%20latents%20are%20not%20atomic.%20The%20resulting%20decompositions%20are%20often%0Ainterpretable%3B%20e.g.%20a%20latent%20representing%20%60%60Einstein%27%27%20decomposes%20into%0A%60%60scientist%27%27%2C%20%60%60Germany%27%27%2C%20and%20%60%60famous%20person%27%27.%20Even%20if%20SAEs%20do%20not%20find%0Acanonical%20units%20of%20analysis%2C%20they%20may%20still%20be%20useful%20tools.%20We%20suggest%20that%0Afuture%20research%20should%20either%20pursue%20different%20approaches%20for%20identifying%20such%0Aunits%2C%20or%20pragmatically%20choose%20the%20SAE%20size%20suited%20to%20their%20task.%20We%20provide%20an%0Ainteractive%20dashboard%20to%20explore%20meta-SAEs%3A%20https%3A//metasaes.streamlit.app/%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04878v1&entry.124074799=Read"},
{"title": "Leveraging Bi-Focal Perspectives and Granular Feature Integration for\n  Accurate Reliable Early Alzheimer's Detection", "author": "Pandiyaraju V and Shravan Venkatraman and Abeshek A and Pavan Kumar S and Aravintakshan S A", "abstract": "  Being the most commonly known neurodegeneration, Alzheimer's Disease (AD) is\nannually diagnosed in millions of patients. The present medical scenario still\nfinds the exact diagnosis and classification of AD through neuroimaging data as\na challenging task. Traditional CNNs can extract a good amount of low-level\ninformation in an image while failing to extract high-level minuscule\nparticles, which is a significant challenge in detecting AD from MRI scans. To\novercome this, we propose a novel Granular Feature Integration method to\ncombine information extraction at different scales along with an efficient\ninformation flow, enabling the model to capture both broad and fine-grained\nfeatures simultaneously. We also propose a Bi-Focal Perspective mechanism to\nhighlight the subtle neurofibrillary tangles and amyloid plaques in the MRI\nscans, ensuring that critical pathological markers are accurately identified.\nOur model achieved an F1-Score of 99.31%, precision of 99.24%, and recall of\n99.51%. These scores prove that our model is significantly better than the\nstate-of-the-art (SOTA) CNNs in existence.\n", "link": "http://arxiv.org/abs/2407.10921v6", "date": "2025-02-07", "relevancy": 2.52, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5229}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4968}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Bi-Focal%20Perspectives%20and%20Granular%20Feature%20Integration%20for%0A%20%20Accurate%20Reliable%20Early%20Alzheimer%27s%20Detection&body=Title%3A%20Leveraging%20Bi-Focal%20Perspectives%20and%20Granular%20Feature%20Integration%20for%0A%20%20Accurate%20Reliable%20Early%20Alzheimer%27s%20Detection%0AAuthor%3A%20Pandiyaraju%20V%20and%20Shravan%20Venkatraman%20and%20Abeshek%20A%20and%20Pavan%20Kumar%20S%20and%20Aravintakshan%20S%20A%0AAbstract%3A%20%20%20Being%20the%20most%20commonly%20known%20neurodegeneration%2C%20Alzheimer%27s%20Disease%20%28AD%29%20is%0Aannually%20diagnosed%20in%20millions%20of%20patients.%20The%20present%20medical%20scenario%20still%0Afinds%20the%20exact%20diagnosis%20and%20classification%20of%20AD%20through%20neuroimaging%20data%20as%0Aa%20challenging%20task.%20Traditional%20CNNs%20can%20extract%20a%20good%20amount%20of%20low-level%0Ainformation%20in%20an%20image%20while%20failing%20to%20extract%20high-level%20minuscule%0Aparticles%2C%20which%20is%20a%20significant%20challenge%20in%20detecting%20AD%20from%20MRI%20scans.%20To%0Aovercome%20this%2C%20we%20propose%20a%20novel%20Granular%20Feature%20Integration%20method%20to%0Acombine%20information%20extraction%20at%20different%20scales%20along%20with%20an%20efficient%0Ainformation%20flow%2C%20enabling%20the%20model%20to%20capture%20both%20broad%20and%20fine-grained%0Afeatures%20simultaneously.%20We%20also%20propose%20a%20Bi-Focal%20Perspective%20mechanism%20to%0Ahighlight%20the%20subtle%20neurofibrillary%20tangles%20and%20amyloid%20plaques%20in%20the%20MRI%0Ascans%2C%20ensuring%20that%20critical%20pathological%20markers%20are%20accurately%20identified.%0AOur%20model%20achieved%20an%20F1-Score%20of%2099.31%25%2C%20precision%20of%2099.24%25%2C%20and%20recall%20of%0A99.51%25.%20These%20scores%20prove%20that%20our%20model%20is%20significantly%20better%20than%20the%0Astate-of-the-art%20%28SOTA%29%20CNNs%20in%20existence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10921v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Bi-Focal%2520Perspectives%2520and%2520Granular%2520Feature%2520Integration%2520for%250A%2520%2520Accurate%2520Reliable%2520Early%2520Alzheimer%2527s%2520Detection%26entry.906535625%3DPandiyaraju%2520V%2520and%2520Shravan%2520Venkatraman%2520and%2520Abeshek%2520A%2520and%2520Pavan%2520Kumar%2520S%2520and%2520Aravintakshan%2520S%2520A%26entry.1292438233%3D%2520%2520Being%2520the%2520most%2520commonly%2520known%2520neurodegeneration%252C%2520Alzheimer%2527s%2520Disease%2520%2528AD%2529%2520is%250Aannually%2520diagnosed%2520in%2520millions%2520of%2520patients.%2520The%2520present%2520medical%2520scenario%2520still%250Afinds%2520the%2520exact%2520diagnosis%2520and%2520classification%2520of%2520AD%2520through%2520neuroimaging%2520data%2520as%250Aa%2520challenging%2520task.%2520Traditional%2520CNNs%2520can%2520extract%2520a%2520good%2520amount%2520of%2520low-level%250Ainformation%2520in%2520an%2520image%2520while%2520failing%2520to%2520extract%2520high-level%2520minuscule%250Aparticles%252C%2520which%2520is%2520a%2520significant%2520challenge%2520in%2520detecting%2520AD%2520from%2520MRI%2520scans.%2520To%250Aovercome%2520this%252C%2520we%2520propose%2520a%2520novel%2520Granular%2520Feature%2520Integration%2520method%2520to%250Acombine%2520information%2520extraction%2520at%2520different%2520scales%2520along%2520with%2520an%2520efficient%250Ainformation%2520flow%252C%2520enabling%2520the%2520model%2520to%2520capture%2520both%2520broad%2520and%2520fine-grained%250Afeatures%2520simultaneously.%2520We%2520also%2520propose%2520a%2520Bi-Focal%2520Perspective%2520mechanism%2520to%250Ahighlight%2520the%2520subtle%2520neurofibrillary%2520tangles%2520and%2520amyloid%2520plaques%2520in%2520the%2520MRI%250Ascans%252C%2520ensuring%2520that%2520critical%2520pathological%2520markers%2520are%2520accurately%2520identified.%250AOur%2520model%2520achieved%2520an%2520F1-Score%2520of%252099.31%2525%252C%2520precision%2520of%252099.24%2525%252C%2520and%2520recall%2520of%250A99.51%2525.%2520These%2520scores%2520prove%2520that%2520our%2520model%2520is%2520significantly%2520better%2520than%2520the%250Astate-of-the-art%2520%2528SOTA%2529%2520CNNs%2520in%2520existence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10921v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Bi-Focal%20Perspectives%20and%20Granular%20Feature%20Integration%20for%0A%20%20Accurate%20Reliable%20Early%20Alzheimer%27s%20Detection&entry.906535625=Pandiyaraju%20V%20and%20Shravan%20Venkatraman%20and%20Abeshek%20A%20and%20Pavan%20Kumar%20S%20and%20Aravintakshan%20S%20A&entry.1292438233=%20%20Being%20the%20most%20commonly%20known%20neurodegeneration%2C%20Alzheimer%27s%20Disease%20%28AD%29%20is%0Aannually%20diagnosed%20in%20millions%20of%20patients.%20The%20present%20medical%20scenario%20still%0Afinds%20the%20exact%20diagnosis%20and%20classification%20of%20AD%20through%20neuroimaging%20data%20as%0Aa%20challenging%20task.%20Traditional%20CNNs%20can%20extract%20a%20good%20amount%20of%20low-level%0Ainformation%20in%20an%20image%20while%20failing%20to%20extract%20high-level%20minuscule%0Aparticles%2C%20which%20is%20a%20significant%20challenge%20in%20detecting%20AD%20from%20MRI%20scans.%20To%0Aovercome%20this%2C%20we%20propose%20a%20novel%20Granular%20Feature%20Integration%20method%20to%0Acombine%20information%20extraction%20at%20different%20scales%20along%20with%20an%20efficient%0Ainformation%20flow%2C%20enabling%20the%20model%20to%20capture%20both%20broad%20and%20fine-grained%0Afeatures%20simultaneously.%20We%20also%20propose%20a%20Bi-Focal%20Perspective%20mechanism%20to%0Ahighlight%20the%20subtle%20neurofibrillary%20tangles%20and%20amyloid%20plaques%20in%20the%20MRI%0Ascans%2C%20ensuring%20that%20critical%20pathological%20markers%20are%20accurately%20identified.%0AOur%20model%20achieved%20an%20F1-Score%20of%2099.31%25%2C%20precision%20of%2099.24%25%2C%20and%20recall%20of%0A99.51%25.%20These%20scores%20prove%20that%20our%20model%20is%20significantly%20better%20than%20the%0Astate-of-the-art%20%28SOTA%29%20CNNs%20in%20existence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10921v6&entry.124074799=Read"},
{"title": "Wavelet-Assisted Multi-Frequency Attention Network for Pansharpening", "author": "Jie Huang and Rui Huang and Jinghao Xu and Siran Pen and Yule Duan and Liangjian Deng", "abstract": "  Pansharpening aims to combine a high-resolution panchromatic (PAN) image with\na low-resolution multispectral (LRMS) image to produce a high-resolution\nmultispectral (HRMS) image. Although pansharpening in the frequency domain\noffers clear advantages, most existing methods either continue to operate\nsolely in the spatial domain or fail to fully exploit the benefits of the\nfrequency domain. To address this issue, we innovatively propose\nMulti-Frequency Fusion Attention (MFFA), which leverages wavelet transforms to\ncleanly separate frequencies and enable lossless reconstruction across\ndifferent frequency domains. Then, we generate Frequency-Query, Spatial-Key,\nand Fusion-Value based on the physical meanings represented by different\nfeatures, which enables a more effective capture of specific information in the\nfrequency domain. Additionally, we focus on the preservation of frequency\nfeatures across different operations. On a broader level, our network employs a\nwavelet pyramid to progressively fuse information across multiple scales.\nCompared to previous frequency domain approaches, our network better prevents\nconfusion and loss of different frequency features during the fusion process.\nQuantitative and qualitative experiments on multiple datasets demonstrate that\nour method outperforms existing approaches and shows significant generalization\ncapabilities for real-world scenarios.\n", "link": "http://arxiv.org/abs/2502.04903v1", "date": "2025-02-07", "relevancy": 2.5058, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5262}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4933}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wavelet-Assisted%20Multi-Frequency%20Attention%20Network%20for%20Pansharpening&body=Title%3A%20Wavelet-Assisted%20Multi-Frequency%20Attention%20Network%20for%20Pansharpening%0AAuthor%3A%20Jie%20Huang%20and%20Rui%20Huang%20and%20Jinghao%20Xu%20and%20Siran%20Pen%20and%20Yule%20Duan%20and%20Liangjian%20Deng%0AAbstract%3A%20%20%20Pansharpening%20aims%20to%20combine%20a%20high-resolution%20panchromatic%20%28PAN%29%20image%20with%0Aa%20low-resolution%20multispectral%20%28LRMS%29%20image%20to%20produce%20a%20high-resolution%0Amultispectral%20%28HRMS%29%20image.%20Although%20pansharpening%20in%20the%20frequency%20domain%0Aoffers%20clear%20advantages%2C%20most%20existing%20methods%20either%20continue%20to%20operate%0Asolely%20in%20the%20spatial%20domain%20or%20fail%20to%20fully%20exploit%20the%20benefits%20of%20the%0Afrequency%20domain.%20To%20address%20this%20issue%2C%20we%20innovatively%20propose%0AMulti-Frequency%20Fusion%20Attention%20%28MFFA%29%2C%20which%20leverages%20wavelet%20transforms%20to%0Acleanly%20separate%20frequencies%20and%20enable%20lossless%20reconstruction%20across%0Adifferent%20frequency%20domains.%20Then%2C%20we%20generate%20Frequency-Query%2C%20Spatial-Key%2C%0Aand%20Fusion-Value%20based%20on%20the%20physical%20meanings%20represented%20by%20different%0Afeatures%2C%20which%20enables%20a%20more%20effective%20capture%20of%20specific%20information%20in%20the%0Afrequency%20domain.%20Additionally%2C%20we%20focus%20on%20the%20preservation%20of%20frequency%0Afeatures%20across%20different%20operations.%20On%20a%20broader%20level%2C%20our%20network%20employs%20a%0Awavelet%20pyramid%20to%20progressively%20fuse%20information%20across%20multiple%20scales.%0ACompared%20to%20previous%20frequency%20domain%20approaches%2C%20our%20network%20better%20prevents%0Aconfusion%20and%20loss%20of%20different%20frequency%20features%20during%20the%20fusion%20process.%0AQuantitative%20and%20qualitative%20experiments%20on%20multiple%20datasets%20demonstrate%20that%0Aour%20method%20outperforms%20existing%20approaches%20and%20shows%20significant%20generalization%0Acapabilities%20for%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04903v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWavelet-Assisted%2520Multi-Frequency%2520Attention%2520Network%2520for%2520Pansharpening%26entry.906535625%3DJie%2520Huang%2520and%2520Rui%2520Huang%2520and%2520Jinghao%2520Xu%2520and%2520Siran%2520Pen%2520and%2520Yule%2520Duan%2520and%2520Liangjian%2520Deng%26entry.1292438233%3D%2520%2520Pansharpening%2520aims%2520to%2520combine%2520a%2520high-resolution%2520panchromatic%2520%2528PAN%2529%2520image%2520with%250Aa%2520low-resolution%2520multispectral%2520%2528LRMS%2529%2520image%2520to%2520produce%2520a%2520high-resolution%250Amultispectral%2520%2528HRMS%2529%2520image.%2520Although%2520pansharpening%2520in%2520the%2520frequency%2520domain%250Aoffers%2520clear%2520advantages%252C%2520most%2520existing%2520methods%2520either%2520continue%2520to%2520operate%250Asolely%2520in%2520the%2520spatial%2520domain%2520or%2520fail%2520to%2520fully%2520exploit%2520the%2520benefits%2520of%2520the%250Afrequency%2520domain.%2520To%2520address%2520this%2520issue%252C%2520we%2520innovatively%2520propose%250AMulti-Frequency%2520Fusion%2520Attention%2520%2528MFFA%2529%252C%2520which%2520leverages%2520wavelet%2520transforms%2520to%250Acleanly%2520separate%2520frequencies%2520and%2520enable%2520lossless%2520reconstruction%2520across%250Adifferent%2520frequency%2520domains.%2520Then%252C%2520we%2520generate%2520Frequency-Query%252C%2520Spatial-Key%252C%250Aand%2520Fusion-Value%2520based%2520on%2520the%2520physical%2520meanings%2520represented%2520by%2520different%250Afeatures%252C%2520which%2520enables%2520a%2520more%2520effective%2520capture%2520of%2520specific%2520information%2520in%2520the%250Afrequency%2520domain.%2520Additionally%252C%2520we%2520focus%2520on%2520the%2520preservation%2520of%2520frequency%250Afeatures%2520across%2520different%2520operations.%2520On%2520a%2520broader%2520level%252C%2520our%2520network%2520employs%2520a%250Awavelet%2520pyramid%2520to%2520progressively%2520fuse%2520information%2520across%2520multiple%2520scales.%250ACompared%2520to%2520previous%2520frequency%2520domain%2520approaches%252C%2520our%2520network%2520better%2520prevents%250Aconfusion%2520and%2520loss%2520of%2520different%2520frequency%2520features%2520during%2520the%2520fusion%2520process.%250AQuantitative%2520and%2520qualitative%2520experiments%2520on%2520multiple%2520datasets%2520demonstrate%2520that%250Aour%2520method%2520outperforms%2520existing%2520approaches%2520and%2520shows%2520significant%2520generalization%250Acapabilities%2520for%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04903v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wavelet-Assisted%20Multi-Frequency%20Attention%20Network%20for%20Pansharpening&entry.906535625=Jie%20Huang%20and%20Rui%20Huang%20and%20Jinghao%20Xu%20and%20Siran%20Pen%20and%20Yule%20Duan%20and%20Liangjian%20Deng&entry.1292438233=%20%20Pansharpening%20aims%20to%20combine%20a%20high-resolution%20panchromatic%20%28PAN%29%20image%20with%0Aa%20low-resolution%20multispectral%20%28LRMS%29%20image%20to%20produce%20a%20high-resolution%0Amultispectral%20%28HRMS%29%20image.%20Although%20pansharpening%20in%20the%20frequency%20domain%0Aoffers%20clear%20advantages%2C%20most%20existing%20methods%20either%20continue%20to%20operate%0Asolely%20in%20the%20spatial%20domain%20or%20fail%20to%20fully%20exploit%20the%20benefits%20of%20the%0Afrequency%20domain.%20To%20address%20this%20issue%2C%20we%20innovatively%20propose%0AMulti-Frequency%20Fusion%20Attention%20%28MFFA%29%2C%20which%20leverages%20wavelet%20transforms%20to%0Acleanly%20separate%20frequencies%20and%20enable%20lossless%20reconstruction%20across%0Adifferent%20frequency%20domains.%20Then%2C%20we%20generate%20Frequency-Query%2C%20Spatial-Key%2C%0Aand%20Fusion-Value%20based%20on%20the%20physical%20meanings%20represented%20by%20different%0Afeatures%2C%20which%20enables%20a%20more%20effective%20capture%20of%20specific%20information%20in%20the%0Afrequency%20domain.%20Additionally%2C%20we%20focus%20on%20the%20preservation%20of%20frequency%0Afeatures%20across%20different%20operations.%20On%20a%20broader%20level%2C%20our%20network%20employs%20a%0Awavelet%20pyramid%20to%20progressively%20fuse%20information%20across%20multiple%20scales.%0ACompared%20to%20previous%20frequency%20domain%20approaches%2C%20our%20network%20better%20prevents%0Aconfusion%20and%20loss%20of%20different%20frequency%20features%20during%20the%20fusion%20process.%0AQuantitative%20and%20qualitative%20experiments%20on%20multiple%20datasets%20demonstrate%20that%0Aour%20method%20outperforms%20existing%20approaches%20and%20shows%20significant%20generalization%0Acapabilities%20for%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04903v1&entry.124074799=Read"},
{"title": "Enhancing Pre-Trained Decision Transformers with Prompt-Tuning Bandits", "author": "Finn Rietz and Oleg Smirnov and Sara Karimi and Lele Cao", "abstract": "  Harnessing large offline datasets is vital for training foundation models\nthat can generalize across diverse tasks. Offline Reinforcement Learning (RL)\noffers a powerful framework for these scenarios, enabling the derivation of\noptimal policies even from suboptimal data. The Prompting Decision Transformer\n(PDT) is an offline RL multi-task model that distinguishes tasks through\nstochastic trajectory prompts, which are task-specific tokens maintained in\ncontext during rollouts. However, PDT samples these tokens uniformly at random\nfrom per-task demonstration datasets, failing to account for differences in\ntoken informativeness and potentially leading to performance degradation. To\naddress this limitation, we introduce a scalable bandit-based prompt-tuning\nmethod that dynamically learns to construct high-performance trajectory\nprompts. Our approach significantly enhances downstream task performance\nwithout modifying the pre-trained Transformer backbone. Empirical results on\nbenchmark tasks and a newly designed multi-task environment demonstrate the\neffectiveness of our method, creating a seamless bridge between general\nmulti-task offline pre-training and task-specific online adaptation.\n", "link": "http://arxiv.org/abs/2502.04979v1", "date": "2025-02-07", "relevancy": 2.5019, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5116}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4956}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Pre-Trained%20Decision%20Transformers%20with%20Prompt-Tuning%20Bandits&body=Title%3A%20Enhancing%20Pre-Trained%20Decision%20Transformers%20with%20Prompt-Tuning%20Bandits%0AAuthor%3A%20Finn%20Rietz%20and%20Oleg%20Smirnov%20and%20Sara%20Karimi%20and%20Lele%20Cao%0AAbstract%3A%20%20%20Harnessing%20large%20offline%20datasets%20is%20vital%20for%20training%20foundation%20models%0Athat%20can%20generalize%20across%20diverse%20tasks.%20Offline%20Reinforcement%20Learning%20%28RL%29%0Aoffers%20a%20powerful%20framework%20for%20these%20scenarios%2C%20enabling%20the%20derivation%20of%0Aoptimal%20policies%20even%20from%20suboptimal%20data.%20The%20Prompting%20Decision%20Transformer%0A%28PDT%29%20is%20an%20offline%20RL%20multi-task%20model%20that%20distinguishes%20tasks%20through%0Astochastic%20trajectory%20prompts%2C%20which%20are%20task-specific%20tokens%20maintained%20in%0Acontext%20during%20rollouts.%20However%2C%20PDT%20samples%20these%20tokens%20uniformly%20at%20random%0Afrom%20per-task%20demonstration%20datasets%2C%20failing%20to%20account%20for%20differences%20in%0Atoken%20informativeness%20and%20potentially%20leading%20to%20performance%20degradation.%20To%0Aaddress%20this%20limitation%2C%20we%20introduce%20a%20scalable%20bandit-based%20prompt-tuning%0Amethod%20that%20dynamically%20learns%20to%20construct%20high-performance%20trajectory%0Aprompts.%20Our%20approach%20significantly%20enhances%20downstream%20task%20performance%0Awithout%20modifying%20the%20pre-trained%20Transformer%20backbone.%20Empirical%20results%20on%0Abenchmark%20tasks%20and%20a%20newly%20designed%20multi-task%20environment%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%2C%20creating%20a%20seamless%20bridge%20between%20general%0Amulti-task%20offline%20pre-training%20and%20task-specific%20online%20adaptation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04979v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Pre-Trained%2520Decision%2520Transformers%2520with%2520Prompt-Tuning%2520Bandits%26entry.906535625%3DFinn%2520Rietz%2520and%2520Oleg%2520Smirnov%2520and%2520Sara%2520Karimi%2520and%2520Lele%2520Cao%26entry.1292438233%3D%2520%2520Harnessing%2520large%2520offline%2520datasets%2520is%2520vital%2520for%2520training%2520foundation%2520models%250Athat%2520can%2520generalize%2520across%2520diverse%2520tasks.%2520Offline%2520Reinforcement%2520Learning%2520%2528RL%2529%250Aoffers%2520a%2520powerful%2520framework%2520for%2520these%2520scenarios%252C%2520enabling%2520the%2520derivation%2520of%250Aoptimal%2520policies%2520even%2520from%2520suboptimal%2520data.%2520The%2520Prompting%2520Decision%2520Transformer%250A%2528PDT%2529%2520is%2520an%2520offline%2520RL%2520multi-task%2520model%2520that%2520distinguishes%2520tasks%2520through%250Astochastic%2520trajectory%2520prompts%252C%2520which%2520are%2520task-specific%2520tokens%2520maintained%2520in%250Acontext%2520during%2520rollouts.%2520However%252C%2520PDT%2520samples%2520these%2520tokens%2520uniformly%2520at%2520random%250Afrom%2520per-task%2520demonstration%2520datasets%252C%2520failing%2520to%2520account%2520for%2520differences%2520in%250Atoken%2520informativeness%2520and%2520potentially%2520leading%2520to%2520performance%2520degradation.%2520To%250Aaddress%2520this%2520limitation%252C%2520we%2520introduce%2520a%2520scalable%2520bandit-based%2520prompt-tuning%250Amethod%2520that%2520dynamically%2520learns%2520to%2520construct%2520high-performance%2520trajectory%250Aprompts.%2520Our%2520approach%2520significantly%2520enhances%2520downstream%2520task%2520performance%250Awithout%2520modifying%2520the%2520pre-trained%2520Transformer%2520backbone.%2520Empirical%2520results%2520on%250Abenchmark%2520tasks%2520and%2520a%2520newly%2520designed%2520multi-task%2520environment%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520method%252C%2520creating%2520a%2520seamless%2520bridge%2520between%2520general%250Amulti-task%2520offline%2520pre-training%2520and%2520task-specific%2520online%2520adaptation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04979v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Pre-Trained%20Decision%20Transformers%20with%20Prompt-Tuning%20Bandits&entry.906535625=Finn%20Rietz%20and%20Oleg%20Smirnov%20and%20Sara%20Karimi%20and%20Lele%20Cao&entry.1292438233=%20%20Harnessing%20large%20offline%20datasets%20is%20vital%20for%20training%20foundation%20models%0Athat%20can%20generalize%20across%20diverse%20tasks.%20Offline%20Reinforcement%20Learning%20%28RL%29%0Aoffers%20a%20powerful%20framework%20for%20these%20scenarios%2C%20enabling%20the%20derivation%20of%0Aoptimal%20policies%20even%20from%20suboptimal%20data.%20The%20Prompting%20Decision%20Transformer%0A%28PDT%29%20is%20an%20offline%20RL%20multi-task%20model%20that%20distinguishes%20tasks%20through%0Astochastic%20trajectory%20prompts%2C%20which%20are%20task-specific%20tokens%20maintained%20in%0Acontext%20during%20rollouts.%20However%2C%20PDT%20samples%20these%20tokens%20uniformly%20at%20random%0Afrom%20per-task%20demonstration%20datasets%2C%20failing%20to%20account%20for%20differences%20in%0Atoken%20informativeness%20and%20potentially%20leading%20to%20performance%20degradation.%20To%0Aaddress%20this%20limitation%2C%20we%20introduce%20a%20scalable%20bandit-based%20prompt-tuning%0Amethod%20that%20dynamically%20learns%20to%20construct%20high-performance%20trajectory%0Aprompts.%20Our%20approach%20significantly%20enhances%20downstream%20task%20performance%0Awithout%20modifying%20the%20pre-trained%20Transformer%20backbone.%20Empirical%20results%20on%0Abenchmark%20tasks%20and%20a%20newly%20designed%20multi-task%20environment%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%2C%20creating%20a%20seamless%20bridge%20between%20general%0Amulti-task%20offline%20pre-training%20and%20task-specific%20online%20adaptation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04979v1&entry.124074799=Read"},
{"title": "Graph Contrastive Learning for Connectome Classification", "author": "Mart\u00edn Schmidt and Sara Silva and Federico Larroca and Gonzalo Mateos and Pablo Mus\u00e9", "abstract": "  With recent advancements in non-invasive techniques for measuring brain\nactivity, such as magnetic resonance imaging (MRI), the study of structural and\nfunctional brain networks through graph signal processing (GSP) has gained\nnotable prominence. GSP stands as a key tool in unraveling the interplay\nbetween the brain's function and structure, enabling the analysis of graphs\ndefined by the connections between regions of interest -- referred to as\nconnectomes in this context. Our work represents a further step in this\ndirection by exploring supervised contrastive learning methods within the realm\nof graph representation learning. The main objective of this approach is to\ngenerate subject-level (i.e., graph-level) vector representations that bring\ntogether subjects sharing the same label while separating those with different\nlabels. These connectome embeddings are derived from a graph neural network\nEncoder-Decoder architecture, which jointly considers structural and functional\nconnectivity. By leveraging data augmentation techniques, the proposed\nframework achieves state-of-the-art performance in a gender classification task\nusing Human Connectome Project data. More broadly, our connectome-centric\nmethodological advances support the promising prospect of using GSP to discover\nmore about brain function, with potential impact to understanding heterogeneity\nin the neurodegeneration for precision medicine and diagnosis.\n", "link": "http://arxiv.org/abs/2502.05109v1", "date": "2025-02-07", "relevancy": 2.4898, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5458}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4809}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Contrastive%20Learning%20for%20Connectome%20Classification&body=Title%3A%20Graph%20Contrastive%20Learning%20for%20Connectome%20Classification%0AAuthor%3A%20Mart%C3%ADn%20Schmidt%20and%20Sara%20Silva%20and%20Federico%20Larroca%20and%20Gonzalo%20Mateos%20and%20Pablo%20Mus%C3%A9%0AAbstract%3A%20%20%20With%20recent%20advancements%20in%20non-invasive%20techniques%20for%20measuring%20brain%0Aactivity%2C%20such%20as%20magnetic%20resonance%20imaging%20%28MRI%29%2C%20the%20study%20of%20structural%20and%0Afunctional%20brain%20networks%20through%20graph%20signal%20processing%20%28GSP%29%20has%20gained%0Anotable%20prominence.%20GSP%20stands%20as%20a%20key%20tool%20in%20unraveling%20the%20interplay%0Abetween%20the%20brain%27s%20function%20and%20structure%2C%20enabling%20the%20analysis%20of%20graphs%0Adefined%20by%20the%20connections%20between%20regions%20of%20interest%20--%20referred%20to%20as%0Aconnectomes%20in%20this%20context.%20Our%20work%20represents%20a%20further%20step%20in%20this%0Adirection%20by%20exploring%20supervised%20contrastive%20learning%20methods%20within%20the%20realm%0Aof%20graph%20representation%20learning.%20The%20main%20objective%20of%20this%20approach%20is%20to%0Agenerate%20subject-level%20%28i.e.%2C%20graph-level%29%20vector%20representations%20that%20bring%0Atogether%20subjects%20sharing%20the%20same%20label%20while%20separating%20those%20with%20different%0Alabels.%20These%20connectome%20embeddings%20are%20derived%20from%20a%20graph%20neural%20network%0AEncoder-Decoder%20architecture%2C%20which%20jointly%20considers%20structural%20and%20functional%0Aconnectivity.%20By%20leveraging%20data%20augmentation%20techniques%2C%20the%20proposed%0Aframework%20achieves%20state-of-the-art%20performance%20in%20a%20gender%20classification%20task%0Ausing%20Human%20Connectome%20Project%20data.%20More%20broadly%2C%20our%20connectome-centric%0Amethodological%20advances%20support%20the%20promising%20prospect%20of%20using%20GSP%20to%20discover%0Amore%20about%20brain%20function%2C%20with%20potential%20impact%20to%20understanding%20heterogeneity%0Ain%20the%20neurodegeneration%20for%20precision%20medicine%20and%20diagnosis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05109v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Contrastive%2520Learning%2520for%2520Connectome%2520Classification%26entry.906535625%3DMart%25C3%25ADn%2520Schmidt%2520and%2520Sara%2520Silva%2520and%2520Federico%2520Larroca%2520and%2520Gonzalo%2520Mateos%2520and%2520Pablo%2520Mus%25C3%25A9%26entry.1292438233%3D%2520%2520With%2520recent%2520advancements%2520in%2520non-invasive%2520techniques%2520for%2520measuring%2520brain%250Aactivity%252C%2520such%2520as%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%252C%2520the%2520study%2520of%2520structural%2520and%250Afunctional%2520brain%2520networks%2520through%2520graph%2520signal%2520processing%2520%2528GSP%2529%2520has%2520gained%250Anotable%2520prominence.%2520GSP%2520stands%2520as%2520a%2520key%2520tool%2520in%2520unraveling%2520the%2520interplay%250Abetween%2520the%2520brain%2527s%2520function%2520and%2520structure%252C%2520enabling%2520the%2520analysis%2520of%2520graphs%250Adefined%2520by%2520the%2520connections%2520between%2520regions%2520of%2520interest%2520--%2520referred%2520to%2520as%250Aconnectomes%2520in%2520this%2520context.%2520Our%2520work%2520represents%2520a%2520further%2520step%2520in%2520this%250Adirection%2520by%2520exploring%2520supervised%2520contrastive%2520learning%2520methods%2520within%2520the%2520realm%250Aof%2520graph%2520representation%2520learning.%2520The%2520main%2520objective%2520of%2520this%2520approach%2520is%2520to%250Agenerate%2520subject-level%2520%2528i.e.%252C%2520graph-level%2529%2520vector%2520representations%2520that%2520bring%250Atogether%2520subjects%2520sharing%2520the%2520same%2520label%2520while%2520separating%2520those%2520with%2520different%250Alabels.%2520These%2520connectome%2520embeddings%2520are%2520derived%2520from%2520a%2520graph%2520neural%2520network%250AEncoder-Decoder%2520architecture%252C%2520which%2520jointly%2520considers%2520structural%2520and%2520functional%250Aconnectivity.%2520By%2520leveraging%2520data%2520augmentation%2520techniques%252C%2520the%2520proposed%250Aframework%2520achieves%2520state-of-the-art%2520performance%2520in%2520a%2520gender%2520classification%2520task%250Ausing%2520Human%2520Connectome%2520Project%2520data.%2520More%2520broadly%252C%2520our%2520connectome-centric%250Amethodological%2520advances%2520support%2520the%2520promising%2520prospect%2520of%2520using%2520GSP%2520to%2520discover%250Amore%2520about%2520brain%2520function%252C%2520with%2520potential%2520impact%2520to%2520understanding%2520heterogeneity%250Ain%2520the%2520neurodegeneration%2520for%2520precision%2520medicine%2520and%2520diagnosis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05109v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Contrastive%20Learning%20for%20Connectome%20Classification&entry.906535625=Mart%C3%ADn%20Schmidt%20and%20Sara%20Silva%20and%20Federico%20Larroca%20and%20Gonzalo%20Mateos%20and%20Pablo%20Mus%C3%A9&entry.1292438233=%20%20With%20recent%20advancements%20in%20non-invasive%20techniques%20for%20measuring%20brain%0Aactivity%2C%20such%20as%20magnetic%20resonance%20imaging%20%28MRI%29%2C%20the%20study%20of%20structural%20and%0Afunctional%20brain%20networks%20through%20graph%20signal%20processing%20%28GSP%29%20has%20gained%0Anotable%20prominence.%20GSP%20stands%20as%20a%20key%20tool%20in%20unraveling%20the%20interplay%0Abetween%20the%20brain%27s%20function%20and%20structure%2C%20enabling%20the%20analysis%20of%20graphs%0Adefined%20by%20the%20connections%20between%20regions%20of%20interest%20--%20referred%20to%20as%0Aconnectomes%20in%20this%20context.%20Our%20work%20represents%20a%20further%20step%20in%20this%0Adirection%20by%20exploring%20supervised%20contrastive%20learning%20methods%20within%20the%20realm%0Aof%20graph%20representation%20learning.%20The%20main%20objective%20of%20this%20approach%20is%20to%0Agenerate%20subject-level%20%28i.e.%2C%20graph-level%29%20vector%20representations%20that%20bring%0Atogether%20subjects%20sharing%20the%20same%20label%20while%20separating%20those%20with%20different%0Alabels.%20These%20connectome%20embeddings%20are%20derived%20from%20a%20graph%20neural%20network%0AEncoder-Decoder%20architecture%2C%20which%20jointly%20considers%20structural%20and%20functional%0Aconnectivity.%20By%20leveraging%20data%20augmentation%20techniques%2C%20the%20proposed%0Aframework%20achieves%20state-of-the-art%20performance%20in%20a%20gender%20classification%20task%0Ausing%20Human%20Connectome%20Project%20data.%20More%20broadly%2C%20our%20connectome-centric%0Amethodological%20advances%20support%20the%20promising%20prospect%20of%20using%20GSP%20to%20discover%0Amore%20about%20brain%20function%2C%20with%20potential%20impact%20to%20understanding%20heterogeneity%0Ain%20the%20neurodegeneration%20for%20precision%20medicine%20and%20diagnosis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05109v1&entry.124074799=Read"},
{"title": "Lost in Time: Clock and Calendar Understanding Challenges in Multimodal\n  LLMs", "author": "Rohit Saxena and Aryo Pradipta Gema and Pasquale Minervini", "abstract": "  Understanding time from visual representations is a fundamental cognitive\nskill, yet it remains a challenge for multimodal large language models (MLLMs).\nIn this work, we investigate the capabilities of MLLMs in interpreting time and\ndate through analogue clocks and yearly calendars. To facilitate this, we\ncurated a structured dataset comprising two subsets: 1) $\\textit{ClockQA}$,\nwhich comprises various types of clock styles$-$standard, black-dial,\nno-second-hand, Roman numeral, and arrow-hand clocks$-$paired with time related\nquestions; and 2) $\\textit{CalendarQA}$, which consists of yearly calendar\nimages with questions ranging from commonly known dates (e.g., Christmas, New\nYear's Day) to computationally derived ones (e.g., the 100th or 153rd day of\nthe year). We aim to analyse how MLLMs can perform visual recognition,\nnumerical reasoning, and temporal inference when presented with time-related\nvisual data. Our evaluations show that despite recent advancements, reliably\nunderstanding time remains a significant challenge for MLLMs.\n", "link": "http://arxiv.org/abs/2502.05092v1", "date": "2025-02-07", "relevancy": 2.4768, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5035}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4913}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lost%20in%20Time%3A%20Clock%20and%20Calendar%20Understanding%20Challenges%20in%20Multimodal%0A%20%20LLMs&body=Title%3A%20Lost%20in%20Time%3A%20Clock%20and%20Calendar%20Understanding%20Challenges%20in%20Multimodal%0A%20%20LLMs%0AAuthor%3A%20Rohit%20Saxena%20and%20Aryo%20Pradipta%20Gema%20and%20Pasquale%20Minervini%0AAbstract%3A%20%20%20Understanding%20time%20from%20visual%20representations%20is%20a%20fundamental%20cognitive%0Askill%2C%20yet%20it%20remains%20a%20challenge%20for%20multimodal%20large%20language%20models%20%28MLLMs%29.%0AIn%20this%20work%2C%20we%20investigate%20the%20capabilities%20of%20MLLMs%20in%20interpreting%20time%20and%0Adate%20through%20analogue%20clocks%20and%20yearly%20calendars.%20To%20facilitate%20this%2C%20we%0Acurated%20a%20structured%20dataset%20comprising%20two%20subsets%3A%201%29%20%24%5Ctextit%7BClockQA%7D%24%2C%0Awhich%20comprises%20various%20types%20of%20clock%20styles%24-%24standard%2C%20black-dial%2C%0Ano-second-hand%2C%20Roman%20numeral%2C%20and%20arrow-hand%20clocks%24-%24paired%20with%20time%20related%0Aquestions%3B%20and%202%29%20%24%5Ctextit%7BCalendarQA%7D%24%2C%20which%20consists%20of%20yearly%20calendar%0Aimages%20with%20questions%20ranging%20from%20commonly%20known%20dates%20%28e.g.%2C%20Christmas%2C%20New%0AYear%27s%20Day%29%20to%20computationally%20derived%20ones%20%28e.g.%2C%20the%20100th%20or%20153rd%20day%20of%0Athe%20year%29.%20We%20aim%20to%20analyse%20how%20MLLMs%20can%20perform%20visual%20recognition%2C%0Anumerical%20reasoning%2C%20and%20temporal%20inference%20when%20presented%20with%20time-related%0Avisual%20data.%20Our%20evaluations%20show%20that%20despite%20recent%20advancements%2C%20reliably%0Aunderstanding%20time%20remains%20a%20significant%20challenge%20for%20MLLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05092v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLost%2520in%2520Time%253A%2520Clock%2520and%2520Calendar%2520Understanding%2520Challenges%2520in%2520Multimodal%250A%2520%2520LLMs%26entry.906535625%3DRohit%2520Saxena%2520and%2520Aryo%2520Pradipta%2520Gema%2520and%2520Pasquale%2520Minervini%26entry.1292438233%3D%2520%2520Understanding%2520time%2520from%2520visual%2520representations%2520is%2520a%2520fundamental%2520cognitive%250Askill%252C%2520yet%2520it%2520remains%2520a%2520challenge%2520for%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529.%250AIn%2520this%2520work%252C%2520we%2520investigate%2520the%2520capabilities%2520of%2520MLLMs%2520in%2520interpreting%2520time%2520and%250Adate%2520through%2520analogue%2520clocks%2520and%2520yearly%2520calendars.%2520To%2520facilitate%2520this%252C%2520we%250Acurated%2520a%2520structured%2520dataset%2520comprising%2520two%2520subsets%253A%25201%2529%2520%2524%255Ctextit%257BClockQA%257D%2524%252C%250Awhich%2520comprises%2520various%2520types%2520of%2520clock%2520styles%2524-%2524standard%252C%2520black-dial%252C%250Ano-second-hand%252C%2520Roman%2520numeral%252C%2520and%2520arrow-hand%2520clocks%2524-%2524paired%2520with%2520time%2520related%250Aquestions%253B%2520and%25202%2529%2520%2524%255Ctextit%257BCalendarQA%257D%2524%252C%2520which%2520consists%2520of%2520yearly%2520calendar%250Aimages%2520with%2520questions%2520ranging%2520from%2520commonly%2520known%2520dates%2520%2528e.g.%252C%2520Christmas%252C%2520New%250AYear%2527s%2520Day%2529%2520to%2520computationally%2520derived%2520ones%2520%2528e.g.%252C%2520the%2520100th%2520or%2520153rd%2520day%2520of%250Athe%2520year%2529.%2520We%2520aim%2520to%2520analyse%2520how%2520MLLMs%2520can%2520perform%2520visual%2520recognition%252C%250Anumerical%2520reasoning%252C%2520and%2520temporal%2520inference%2520when%2520presented%2520with%2520time-related%250Avisual%2520data.%2520Our%2520evaluations%2520show%2520that%2520despite%2520recent%2520advancements%252C%2520reliably%250Aunderstanding%2520time%2520remains%2520a%2520significant%2520challenge%2520for%2520MLLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05092v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lost%20in%20Time%3A%20Clock%20and%20Calendar%20Understanding%20Challenges%20in%20Multimodal%0A%20%20LLMs&entry.906535625=Rohit%20Saxena%20and%20Aryo%20Pradipta%20Gema%20and%20Pasquale%20Minervini&entry.1292438233=%20%20Understanding%20time%20from%20visual%20representations%20is%20a%20fundamental%20cognitive%0Askill%2C%20yet%20it%20remains%20a%20challenge%20for%20multimodal%20large%20language%20models%20%28MLLMs%29.%0AIn%20this%20work%2C%20we%20investigate%20the%20capabilities%20of%20MLLMs%20in%20interpreting%20time%20and%0Adate%20through%20analogue%20clocks%20and%20yearly%20calendars.%20To%20facilitate%20this%2C%20we%0Acurated%20a%20structured%20dataset%20comprising%20two%20subsets%3A%201%29%20%24%5Ctextit%7BClockQA%7D%24%2C%0Awhich%20comprises%20various%20types%20of%20clock%20styles%24-%24standard%2C%20black-dial%2C%0Ano-second-hand%2C%20Roman%20numeral%2C%20and%20arrow-hand%20clocks%24-%24paired%20with%20time%20related%0Aquestions%3B%20and%202%29%20%24%5Ctextit%7BCalendarQA%7D%24%2C%20which%20consists%20of%20yearly%20calendar%0Aimages%20with%20questions%20ranging%20from%20commonly%20known%20dates%20%28e.g.%2C%20Christmas%2C%20New%0AYear%27s%20Day%29%20to%20computationally%20derived%20ones%20%28e.g.%2C%20the%20100th%20or%20153rd%20day%20of%0Athe%20year%29.%20We%20aim%20to%20analyse%20how%20MLLMs%20can%20perform%20visual%20recognition%2C%0Anumerical%20reasoning%2C%20and%20temporal%20inference%20when%20presented%20with%20time-related%0Avisual%20data.%20Our%20evaluations%20show%20that%20despite%20recent%20advancements%2C%20reliably%0Aunderstanding%20time%20remains%20a%20significant%20challenge%20for%20MLLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05092v1&entry.124074799=Read"},
{"title": "Fillerbuster: Multi-View Scene Completion for Casual Captures", "author": "Ethan Weber and Norman M\u00fcller and Yash Kant and Vasu Agrawal and Michael Zollh\u00f6fer and Angjoo Kanazawa and Christian Richardt", "abstract": "  We present Fillerbuster, a method that completes unknown regions of a 3D\nscene by utilizing a novel large-scale multi-view latent diffusion transformer.\nCasual captures are often sparse and miss surrounding content behind objects or\nabove the scene. Existing methods are not suitable for handling this challenge\nas they focus on making the known pixels look good with sparse-view priors, or\non creating the missing sides of objects from just one or two photos. In\nreality, we often have hundreds of input frames and want to complete areas that\nare missing and unobserved from the input frames. Additionally, the images\noften do not have known camera parameters. Our solution is to train a\ngenerative model that can consume a large context of input frames while\ngenerating unknown target views and recovering image poses when desired. We\nshow results where we complete partial captures on two existing datasets. We\nalso present an uncalibrated scene completion task where our unified model\npredicts both poses and creates new content. Our model is the first to predict\nmany images and poses together for scene completion.\n", "link": "http://arxiv.org/abs/2502.05175v1", "date": "2025-02-07", "relevancy": 2.4756, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6195}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6195}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fillerbuster%3A%20Multi-View%20Scene%20Completion%20for%20Casual%20Captures&body=Title%3A%20Fillerbuster%3A%20Multi-View%20Scene%20Completion%20for%20Casual%20Captures%0AAuthor%3A%20Ethan%20Weber%20and%20Norman%20M%C3%BCller%20and%20Yash%20Kant%20and%20Vasu%20Agrawal%20and%20Michael%20Zollh%C3%B6fer%20and%20Angjoo%20Kanazawa%20and%20Christian%20Richardt%0AAbstract%3A%20%20%20We%20present%20Fillerbuster%2C%20a%20method%20that%20completes%20unknown%20regions%20of%20a%203D%0Ascene%20by%20utilizing%20a%20novel%20large-scale%20multi-view%20latent%20diffusion%20transformer.%0ACasual%20captures%20are%20often%20sparse%20and%20miss%20surrounding%20content%20behind%20objects%20or%0Aabove%20the%20scene.%20Existing%20methods%20are%20not%20suitable%20for%20handling%20this%20challenge%0Aas%20they%20focus%20on%20making%20the%20known%20pixels%20look%20good%20with%20sparse-view%20priors%2C%20or%0Aon%20creating%20the%20missing%20sides%20of%20objects%20from%20just%20one%20or%20two%20photos.%20In%0Areality%2C%20we%20often%20have%20hundreds%20of%20input%20frames%20and%20want%20to%20complete%20areas%20that%0Aare%20missing%20and%20unobserved%20from%20the%20input%20frames.%20Additionally%2C%20the%20images%0Aoften%20do%20not%20have%20known%20camera%20parameters.%20Our%20solution%20is%20to%20train%20a%0Agenerative%20model%20that%20can%20consume%20a%20large%20context%20of%20input%20frames%20while%0Agenerating%20unknown%20target%20views%20and%20recovering%20image%20poses%20when%20desired.%20We%0Ashow%20results%20where%20we%20complete%20partial%20captures%20on%20two%20existing%20datasets.%20We%0Aalso%20present%20an%20uncalibrated%20scene%20completion%20task%20where%20our%20unified%20model%0Apredicts%20both%20poses%20and%20creates%20new%20content.%20Our%20model%20is%20the%20first%20to%20predict%0Amany%20images%20and%20poses%20together%20for%20scene%20completion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05175v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFillerbuster%253A%2520Multi-View%2520Scene%2520Completion%2520for%2520Casual%2520Captures%26entry.906535625%3DEthan%2520Weber%2520and%2520Norman%2520M%25C3%25BCller%2520and%2520Yash%2520Kant%2520and%2520Vasu%2520Agrawal%2520and%2520Michael%2520Zollh%25C3%25B6fer%2520and%2520Angjoo%2520Kanazawa%2520and%2520Christian%2520Richardt%26entry.1292438233%3D%2520%2520We%2520present%2520Fillerbuster%252C%2520a%2520method%2520that%2520completes%2520unknown%2520regions%2520of%2520a%25203D%250Ascene%2520by%2520utilizing%2520a%2520novel%2520large-scale%2520multi-view%2520latent%2520diffusion%2520transformer.%250ACasual%2520captures%2520are%2520often%2520sparse%2520and%2520miss%2520surrounding%2520content%2520behind%2520objects%2520or%250Aabove%2520the%2520scene.%2520Existing%2520methods%2520are%2520not%2520suitable%2520for%2520handling%2520this%2520challenge%250Aas%2520they%2520focus%2520on%2520making%2520the%2520known%2520pixels%2520look%2520good%2520with%2520sparse-view%2520priors%252C%2520or%250Aon%2520creating%2520the%2520missing%2520sides%2520of%2520objects%2520from%2520just%2520one%2520or%2520two%2520photos.%2520In%250Areality%252C%2520we%2520often%2520have%2520hundreds%2520of%2520input%2520frames%2520and%2520want%2520to%2520complete%2520areas%2520that%250Aare%2520missing%2520and%2520unobserved%2520from%2520the%2520input%2520frames.%2520Additionally%252C%2520the%2520images%250Aoften%2520do%2520not%2520have%2520known%2520camera%2520parameters.%2520Our%2520solution%2520is%2520to%2520train%2520a%250Agenerative%2520model%2520that%2520can%2520consume%2520a%2520large%2520context%2520of%2520input%2520frames%2520while%250Agenerating%2520unknown%2520target%2520views%2520and%2520recovering%2520image%2520poses%2520when%2520desired.%2520We%250Ashow%2520results%2520where%2520we%2520complete%2520partial%2520captures%2520on%2520two%2520existing%2520datasets.%2520We%250Aalso%2520present%2520an%2520uncalibrated%2520scene%2520completion%2520task%2520where%2520our%2520unified%2520model%250Apredicts%2520both%2520poses%2520and%2520creates%2520new%2520content.%2520Our%2520model%2520is%2520the%2520first%2520to%2520predict%250Amany%2520images%2520and%2520poses%2520together%2520for%2520scene%2520completion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05175v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fillerbuster%3A%20Multi-View%20Scene%20Completion%20for%20Casual%20Captures&entry.906535625=Ethan%20Weber%20and%20Norman%20M%C3%BCller%20and%20Yash%20Kant%20and%20Vasu%20Agrawal%20and%20Michael%20Zollh%C3%B6fer%20and%20Angjoo%20Kanazawa%20and%20Christian%20Richardt&entry.1292438233=%20%20We%20present%20Fillerbuster%2C%20a%20method%20that%20completes%20unknown%20regions%20of%20a%203D%0Ascene%20by%20utilizing%20a%20novel%20large-scale%20multi-view%20latent%20diffusion%20transformer.%0ACasual%20captures%20are%20often%20sparse%20and%20miss%20surrounding%20content%20behind%20objects%20or%0Aabove%20the%20scene.%20Existing%20methods%20are%20not%20suitable%20for%20handling%20this%20challenge%0Aas%20they%20focus%20on%20making%20the%20known%20pixels%20look%20good%20with%20sparse-view%20priors%2C%20or%0Aon%20creating%20the%20missing%20sides%20of%20objects%20from%20just%20one%20or%20two%20photos.%20In%0Areality%2C%20we%20often%20have%20hundreds%20of%20input%20frames%20and%20want%20to%20complete%20areas%20that%0Aare%20missing%20and%20unobserved%20from%20the%20input%20frames.%20Additionally%2C%20the%20images%0Aoften%20do%20not%20have%20known%20camera%20parameters.%20Our%20solution%20is%20to%20train%20a%0Agenerative%20model%20that%20can%20consume%20a%20large%20context%20of%20input%20frames%20while%0Agenerating%20unknown%20target%20views%20and%20recovering%20image%20poses%20when%20desired.%20We%0Ashow%20results%20where%20we%20complete%20partial%20captures%20on%20two%20existing%20datasets.%20We%0Aalso%20present%20an%20uncalibrated%20scene%20completion%20task%20where%20our%20unified%20model%0Apredicts%20both%20poses%20and%20creates%20new%20content.%20Our%20model%20is%20the%20first%20to%20predict%0Amany%20images%20and%20poses%20together%20for%20scene%20completion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05175v1&entry.124074799=Read"},
{"title": "SpecTUS: Spectral Translator for Unknown Structures annotation from\n  EI-MS spectra", "author": "Adam H\u00e1jek and Helge Hecht and Elliott J. Price and Ale\u0161 K\u0159enek", "abstract": "  Compound identification and structure annotation from mass spectra is a\nwell-established task widely applied in drug detection, criminal forensics,\nsmall molecule biomarker discovery and chemical engineering.\n  We propose SpecTUS: Spectral Translator for Unknown Structures, a deep neural\nmodel that addresses the task of structural annotation of small molecules from\nlow-resolution gas chromatography electron ionization mass spectra (GC-EI-MS).\nOur model analyzes the spectra in \\textit{de novo} manner -- a direct\ntranslation from the spectra into 2D-structural representation. Our approach is\nparticularly useful for analyzing compounds unavailable in spectral libraries.\n  In a rigorous evaluation of our model on the novel structure annotation task\nacross different libraries, we outperformed standard database search techniques\nby a wide margin. On a held-out testing set, including \\numprint{28267} spectra\nfrom the NIST database, we show that our model's single suggestion perfectly\nreconstructs 43\\% of the subset's compounds. This single suggestion is strictly\nbetter than the candidate of the database hybrid search (common method among\npractitioners)\n  in 76\\% of cases. In a~still affordable scenario of~10 suggestions, perfect\nreconstruction is achieved in 65\\%, and 84\\% are better than the hybrid search.\n", "link": "http://arxiv.org/abs/2502.05114v1", "date": "2025-02-07", "relevancy": 2.4513, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4979}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4864}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpecTUS%3A%20Spectral%20Translator%20for%20Unknown%20Structures%20annotation%20from%0A%20%20EI-MS%20spectra&body=Title%3A%20SpecTUS%3A%20Spectral%20Translator%20for%20Unknown%20Structures%20annotation%20from%0A%20%20EI-MS%20spectra%0AAuthor%3A%20Adam%20H%C3%A1jek%20and%20Helge%20Hecht%20and%20Elliott%20J.%20Price%20and%20Ale%C5%A1%20K%C5%99enek%0AAbstract%3A%20%20%20Compound%20identification%20and%20structure%20annotation%20from%20mass%20spectra%20is%20a%0Awell-established%20task%20widely%20applied%20in%20drug%20detection%2C%20criminal%20forensics%2C%0Asmall%20molecule%20biomarker%20discovery%20and%20chemical%20engineering.%0A%20%20We%20propose%20SpecTUS%3A%20Spectral%20Translator%20for%20Unknown%20Structures%2C%20a%20deep%20neural%0Amodel%20that%20addresses%20the%20task%20of%20structural%20annotation%20of%20small%20molecules%20from%0Alow-resolution%20gas%20chromatography%20electron%20ionization%20mass%20spectra%20%28GC-EI-MS%29.%0AOur%20model%20analyzes%20the%20spectra%20in%20%5Ctextit%7Bde%20novo%7D%20manner%20--%20a%20direct%0Atranslation%20from%20the%20spectra%20into%202D-structural%20representation.%20Our%20approach%20is%0Aparticularly%20useful%20for%20analyzing%20compounds%20unavailable%20in%20spectral%20libraries.%0A%20%20In%20a%20rigorous%20evaluation%20of%20our%20model%20on%20the%20novel%20structure%20annotation%20task%0Aacross%20different%20libraries%2C%20we%20outperformed%20standard%20database%20search%20techniques%0Aby%20a%20wide%20margin.%20On%20a%20held-out%20testing%20set%2C%20including%20%5Cnumprint%7B28267%7D%20spectra%0Afrom%20the%20NIST%20database%2C%20we%20show%20that%20our%20model%27s%20single%20suggestion%20perfectly%0Areconstructs%2043%5C%25%20of%20the%20subset%27s%20compounds.%20This%20single%20suggestion%20is%20strictly%0Abetter%20than%20the%20candidate%20of%20the%20database%20hybrid%20search%20%28common%20method%20among%0Apractitioners%29%0A%20%20in%2076%5C%25%20of%20cases.%20In%20a~still%20affordable%20scenario%20of~10%20suggestions%2C%20perfect%0Areconstruction%20is%20achieved%20in%2065%5C%25%2C%20and%2084%5C%25%20are%20better%20than%20the%20hybrid%20search.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05114v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpecTUS%253A%2520Spectral%2520Translator%2520for%2520Unknown%2520Structures%2520annotation%2520from%250A%2520%2520EI-MS%2520spectra%26entry.906535625%3DAdam%2520H%25C3%25A1jek%2520and%2520Helge%2520Hecht%2520and%2520Elliott%2520J.%2520Price%2520and%2520Ale%25C5%25A1%2520K%25C5%2599enek%26entry.1292438233%3D%2520%2520Compound%2520identification%2520and%2520structure%2520annotation%2520from%2520mass%2520spectra%2520is%2520a%250Awell-established%2520task%2520widely%2520applied%2520in%2520drug%2520detection%252C%2520criminal%2520forensics%252C%250Asmall%2520molecule%2520biomarker%2520discovery%2520and%2520chemical%2520engineering.%250A%2520%2520We%2520propose%2520SpecTUS%253A%2520Spectral%2520Translator%2520for%2520Unknown%2520Structures%252C%2520a%2520deep%2520neural%250Amodel%2520that%2520addresses%2520the%2520task%2520of%2520structural%2520annotation%2520of%2520small%2520molecules%2520from%250Alow-resolution%2520gas%2520chromatography%2520electron%2520ionization%2520mass%2520spectra%2520%2528GC-EI-MS%2529.%250AOur%2520model%2520analyzes%2520the%2520spectra%2520in%2520%255Ctextit%257Bde%2520novo%257D%2520manner%2520--%2520a%2520direct%250Atranslation%2520from%2520the%2520spectra%2520into%25202D-structural%2520representation.%2520Our%2520approach%2520is%250Aparticularly%2520useful%2520for%2520analyzing%2520compounds%2520unavailable%2520in%2520spectral%2520libraries.%250A%2520%2520In%2520a%2520rigorous%2520evaluation%2520of%2520our%2520model%2520on%2520the%2520novel%2520structure%2520annotation%2520task%250Aacross%2520different%2520libraries%252C%2520we%2520outperformed%2520standard%2520database%2520search%2520techniques%250Aby%2520a%2520wide%2520margin.%2520On%2520a%2520held-out%2520testing%2520set%252C%2520including%2520%255Cnumprint%257B28267%257D%2520spectra%250Afrom%2520the%2520NIST%2520database%252C%2520we%2520show%2520that%2520our%2520model%2527s%2520single%2520suggestion%2520perfectly%250Areconstructs%252043%255C%2525%2520of%2520the%2520subset%2527s%2520compounds.%2520This%2520single%2520suggestion%2520is%2520strictly%250Abetter%2520than%2520the%2520candidate%2520of%2520the%2520database%2520hybrid%2520search%2520%2528common%2520method%2520among%250Apractitioners%2529%250A%2520%2520in%252076%255C%2525%2520of%2520cases.%2520In%2520a~still%2520affordable%2520scenario%2520of~10%2520suggestions%252C%2520perfect%250Areconstruction%2520is%2520achieved%2520in%252065%255C%2525%252C%2520and%252084%255C%2525%2520are%2520better%2520than%2520the%2520hybrid%2520search.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05114v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpecTUS%3A%20Spectral%20Translator%20for%20Unknown%20Structures%20annotation%20from%0A%20%20EI-MS%20spectra&entry.906535625=Adam%20H%C3%A1jek%20and%20Helge%20Hecht%20and%20Elliott%20J.%20Price%20and%20Ale%C5%A1%20K%C5%99enek&entry.1292438233=%20%20Compound%20identification%20and%20structure%20annotation%20from%20mass%20spectra%20is%20a%0Awell-established%20task%20widely%20applied%20in%20drug%20detection%2C%20criminal%20forensics%2C%0Asmall%20molecule%20biomarker%20discovery%20and%20chemical%20engineering.%0A%20%20We%20propose%20SpecTUS%3A%20Spectral%20Translator%20for%20Unknown%20Structures%2C%20a%20deep%20neural%0Amodel%20that%20addresses%20the%20task%20of%20structural%20annotation%20of%20small%20molecules%20from%0Alow-resolution%20gas%20chromatography%20electron%20ionization%20mass%20spectra%20%28GC-EI-MS%29.%0AOur%20model%20analyzes%20the%20spectra%20in%20%5Ctextit%7Bde%20novo%7D%20manner%20--%20a%20direct%0Atranslation%20from%20the%20spectra%20into%202D-structural%20representation.%20Our%20approach%20is%0Aparticularly%20useful%20for%20analyzing%20compounds%20unavailable%20in%20spectral%20libraries.%0A%20%20In%20a%20rigorous%20evaluation%20of%20our%20model%20on%20the%20novel%20structure%20annotation%20task%0Aacross%20different%20libraries%2C%20we%20outperformed%20standard%20database%20search%20techniques%0Aby%20a%20wide%20margin.%20On%20a%20held-out%20testing%20set%2C%20including%20%5Cnumprint%7B28267%7D%20spectra%0Afrom%20the%20NIST%20database%2C%20we%20show%20that%20our%20model%27s%20single%20suggestion%20perfectly%0Areconstructs%2043%5C%25%20of%20the%20subset%27s%20compounds.%20This%20single%20suggestion%20is%20strictly%0Abetter%20than%20the%20candidate%20of%20the%20database%20hybrid%20search%20%28common%20method%20among%0Apractitioners%29%0A%20%20in%2076%5C%25%20of%20cases.%20In%20a~still%20affordable%20scenario%20of~10%20suggestions%2C%20perfect%0Areconstruction%20is%20achieved%20in%2065%5C%25%2C%20and%2084%5C%25%20are%20better%20than%20the%20hybrid%20search.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05114v1&entry.124074799=Read"},
{"title": "DE-PADA: Personalized Augmentation and Domain Adaptation for ECG\n  Biometrics Across Physiological States", "author": "Amro Abu Saleh and Elliot Sprecher and Kfir Y. Levy and Daniel H. Lange", "abstract": "  Electrocardiogram (ECG)-based biometrics offer a promising method for user\nidentification, combining intrinsic liveness detection with morphological\nuniqueness. However, elevated heart rates introduce significant physiological\nvariability, posing challenges to pattern recognition systems and leading to a\nnotable performance gap between resting and post-exercise conditions.\nAddressing this gap is critical for advancing ECG-based biometric systems for\nreal-world applications. We propose DE-PADA, a Dual Expert model with\nPersonalized Augmentation and Domain Adaptation, designed to enhance robustness\nacross diverse physiological states. The model is trained primarily on\nresting-state data from the evaluation dataset, without direct exposure to\ntheir exercise data. To address variability, DE-PADA incorporates ECG-specific\ninnovations, including heartbeat segmentation into the PQRS interval, known for\nits relative temporal consistency, and the heart rate-sensitive ST interval,\nenabling targeted feature extraction tailored to each region's unique\ncharacteristics. Personalized augmentation simulates subject-specific T-wave\nvariability across heart rates using individual T-wave peak predictions to\nadapt augmentation ranges. Domain adaptation further improves generalization by\nleveraging auxiliary data from supplementary subjects used exclusively for\ntraining, including both resting and exercise conditions. Experiments on the\nUniversity of Toronto ECG Database demonstrate the model's effectiveness.\nDE-PADA achieves relative improvements in post-exercise identification rates of\n26.75% in the initial recovery phase and 11.72% in the late recovery phase,\nwhile maintaining a 98.12% identification rate in the sitting position. These\nresults highlight DE-PADA's ability to address intra-subject variability and\nenhance the robustness of ECG-based biometric systems across diverse\nphysiological states.\n", "link": "http://arxiv.org/abs/2502.04973v1", "date": "2025-02-07", "relevancy": 2.4362, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4981}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4842}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4795}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DE-PADA%3A%20Personalized%20Augmentation%20and%20Domain%20Adaptation%20for%20ECG%0A%20%20Biometrics%20Across%20Physiological%20States&body=Title%3A%20DE-PADA%3A%20Personalized%20Augmentation%20and%20Domain%20Adaptation%20for%20ECG%0A%20%20Biometrics%20Across%20Physiological%20States%0AAuthor%3A%20Amro%20Abu%20Saleh%20and%20Elliot%20Sprecher%20and%20Kfir%20Y.%20Levy%20and%20Daniel%20H.%20Lange%0AAbstract%3A%20%20%20Electrocardiogram%20%28ECG%29-based%20biometrics%20offer%20a%20promising%20method%20for%20user%0Aidentification%2C%20combining%20intrinsic%20liveness%20detection%20with%20morphological%0Auniqueness.%20However%2C%20elevated%20heart%20rates%20introduce%20significant%20physiological%0Avariability%2C%20posing%20challenges%20to%20pattern%20recognition%20systems%20and%20leading%20to%20a%0Anotable%20performance%20gap%20between%20resting%20and%20post-exercise%20conditions.%0AAddressing%20this%20gap%20is%20critical%20for%20advancing%20ECG-based%20biometric%20systems%20for%0Areal-world%20applications.%20We%20propose%20DE-PADA%2C%20a%20Dual%20Expert%20model%20with%0APersonalized%20Augmentation%20and%20Domain%20Adaptation%2C%20designed%20to%20enhance%20robustness%0Aacross%20diverse%20physiological%20states.%20The%20model%20is%20trained%20primarily%20on%0Aresting-state%20data%20from%20the%20evaluation%20dataset%2C%20without%20direct%20exposure%20to%0Atheir%20exercise%20data.%20To%20address%20variability%2C%20DE-PADA%20incorporates%20ECG-specific%0Ainnovations%2C%20including%20heartbeat%20segmentation%20into%20the%20PQRS%20interval%2C%20known%20for%0Aits%20relative%20temporal%20consistency%2C%20and%20the%20heart%20rate-sensitive%20ST%20interval%2C%0Aenabling%20targeted%20feature%20extraction%20tailored%20to%20each%20region%27s%20unique%0Acharacteristics.%20Personalized%20augmentation%20simulates%20subject-specific%20T-wave%0Avariability%20across%20heart%20rates%20using%20individual%20T-wave%20peak%20predictions%20to%0Aadapt%20augmentation%20ranges.%20Domain%20adaptation%20further%20improves%20generalization%20by%0Aleveraging%20auxiliary%20data%20from%20supplementary%20subjects%20used%20exclusively%20for%0Atraining%2C%20including%20both%20resting%20and%20exercise%20conditions.%20Experiments%20on%20the%0AUniversity%20of%20Toronto%20ECG%20Database%20demonstrate%20the%20model%27s%20effectiveness.%0ADE-PADA%20achieves%20relative%20improvements%20in%20post-exercise%20identification%20rates%20of%0A26.75%25%20in%20the%20initial%20recovery%20phase%20and%2011.72%25%20in%20the%20late%20recovery%20phase%2C%0Awhile%20maintaining%20a%2098.12%25%20identification%20rate%20in%20the%20sitting%20position.%20These%0Aresults%20highlight%20DE-PADA%27s%20ability%20to%20address%20intra-subject%20variability%20and%0Aenhance%20the%20robustness%20of%20ECG-based%20biometric%20systems%20across%20diverse%0Aphysiological%20states.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04973v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDE-PADA%253A%2520Personalized%2520Augmentation%2520and%2520Domain%2520Adaptation%2520for%2520ECG%250A%2520%2520Biometrics%2520Across%2520Physiological%2520States%26entry.906535625%3DAmro%2520Abu%2520Saleh%2520and%2520Elliot%2520Sprecher%2520and%2520Kfir%2520Y.%2520Levy%2520and%2520Daniel%2520H.%2520Lange%26entry.1292438233%3D%2520%2520Electrocardiogram%2520%2528ECG%2529-based%2520biometrics%2520offer%2520a%2520promising%2520method%2520for%2520user%250Aidentification%252C%2520combining%2520intrinsic%2520liveness%2520detection%2520with%2520morphological%250Auniqueness.%2520However%252C%2520elevated%2520heart%2520rates%2520introduce%2520significant%2520physiological%250Avariability%252C%2520posing%2520challenges%2520to%2520pattern%2520recognition%2520systems%2520and%2520leading%2520to%2520a%250Anotable%2520performance%2520gap%2520between%2520resting%2520and%2520post-exercise%2520conditions.%250AAddressing%2520this%2520gap%2520is%2520critical%2520for%2520advancing%2520ECG-based%2520biometric%2520systems%2520for%250Areal-world%2520applications.%2520We%2520propose%2520DE-PADA%252C%2520a%2520Dual%2520Expert%2520model%2520with%250APersonalized%2520Augmentation%2520and%2520Domain%2520Adaptation%252C%2520designed%2520to%2520enhance%2520robustness%250Aacross%2520diverse%2520physiological%2520states.%2520The%2520model%2520is%2520trained%2520primarily%2520on%250Aresting-state%2520data%2520from%2520the%2520evaluation%2520dataset%252C%2520without%2520direct%2520exposure%2520to%250Atheir%2520exercise%2520data.%2520To%2520address%2520variability%252C%2520DE-PADA%2520incorporates%2520ECG-specific%250Ainnovations%252C%2520including%2520heartbeat%2520segmentation%2520into%2520the%2520PQRS%2520interval%252C%2520known%2520for%250Aits%2520relative%2520temporal%2520consistency%252C%2520and%2520the%2520heart%2520rate-sensitive%2520ST%2520interval%252C%250Aenabling%2520targeted%2520feature%2520extraction%2520tailored%2520to%2520each%2520region%2527s%2520unique%250Acharacteristics.%2520Personalized%2520augmentation%2520simulates%2520subject-specific%2520T-wave%250Avariability%2520across%2520heart%2520rates%2520using%2520individual%2520T-wave%2520peak%2520predictions%2520to%250Aadapt%2520augmentation%2520ranges.%2520Domain%2520adaptation%2520further%2520improves%2520generalization%2520by%250Aleveraging%2520auxiliary%2520data%2520from%2520supplementary%2520subjects%2520used%2520exclusively%2520for%250Atraining%252C%2520including%2520both%2520resting%2520and%2520exercise%2520conditions.%2520Experiments%2520on%2520the%250AUniversity%2520of%2520Toronto%2520ECG%2520Database%2520demonstrate%2520the%2520model%2527s%2520effectiveness.%250ADE-PADA%2520achieves%2520relative%2520improvements%2520in%2520post-exercise%2520identification%2520rates%2520of%250A26.75%2525%2520in%2520the%2520initial%2520recovery%2520phase%2520and%252011.72%2525%2520in%2520the%2520late%2520recovery%2520phase%252C%250Awhile%2520maintaining%2520a%252098.12%2525%2520identification%2520rate%2520in%2520the%2520sitting%2520position.%2520These%250Aresults%2520highlight%2520DE-PADA%2527s%2520ability%2520to%2520address%2520intra-subject%2520variability%2520and%250Aenhance%2520the%2520robustness%2520of%2520ECG-based%2520biometric%2520systems%2520across%2520diverse%250Aphysiological%2520states.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04973v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DE-PADA%3A%20Personalized%20Augmentation%20and%20Domain%20Adaptation%20for%20ECG%0A%20%20Biometrics%20Across%20Physiological%20States&entry.906535625=Amro%20Abu%20Saleh%20and%20Elliot%20Sprecher%20and%20Kfir%20Y.%20Levy%20and%20Daniel%20H.%20Lange&entry.1292438233=%20%20Electrocardiogram%20%28ECG%29-based%20biometrics%20offer%20a%20promising%20method%20for%20user%0Aidentification%2C%20combining%20intrinsic%20liveness%20detection%20with%20morphological%0Auniqueness.%20However%2C%20elevated%20heart%20rates%20introduce%20significant%20physiological%0Avariability%2C%20posing%20challenges%20to%20pattern%20recognition%20systems%20and%20leading%20to%20a%0Anotable%20performance%20gap%20between%20resting%20and%20post-exercise%20conditions.%0AAddressing%20this%20gap%20is%20critical%20for%20advancing%20ECG-based%20biometric%20systems%20for%0Areal-world%20applications.%20We%20propose%20DE-PADA%2C%20a%20Dual%20Expert%20model%20with%0APersonalized%20Augmentation%20and%20Domain%20Adaptation%2C%20designed%20to%20enhance%20robustness%0Aacross%20diverse%20physiological%20states.%20The%20model%20is%20trained%20primarily%20on%0Aresting-state%20data%20from%20the%20evaluation%20dataset%2C%20without%20direct%20exposure%20to%0Atheir%20exercise%20data.%20To%20address%20variability%2C%20DE-PADA%20incorporates%20ECG-specific%0Ainnovations%2C%20including%20heartbeat%20segmentation%20into%20the%20PQRS%20interval%2C%20known%20for%0Aits%20relative%20temporal%20consistency%2C%20and%20the%20heart%20rate-sensitive%20ST%20interval%2C%0Aenabling%20targeted%20feature%20extraction%20tailored%20to%20each%20region%27s%20unique%0Acharacteristics.%20Personalized%20augmentation%20simulates%20subject-specific%20T-wave%0Avariability%20across%20heart%20rates%20using%20individual%20T-wave%20peak%20predictions%20to%0Aadapt%20augmentation%20ranges.%20Domain%20adaptation%20further%20improves%20generalization%20by%0Aleveraging%20auxiliary%20data%20from%20supplementary%20subjects%20used%20exclusively%20for%0Atraining%2C%20including%20both%20resting%20and%20exercise%20conditions.%20Experiments%20on%20the%0AUniversity%20of%20Toronto%20ECG%20Database%20demonstrate%20the%20model%27s%20effectiveness.%0ADE-PADA%20achieves%20relative%20improvements%20in%20post-exercise%20identification%20rates%20of%0A26.75%25%20in%20the%20initial%20recovery%20phase%20and%2011.72%25%20in%20the%20late%20recovery%20phase%2C%0Awhile%20maintaining%20a%2098.12%25%20identification%20rate%20in%20the%20sitting%20position.%20These%0Aresults%20highlight%20DE-PADA%27s%20ability%20to%20address%20intra-subject%20variability%20and%0Aenhance%20the%20robustness%20of%20ECG-based%20biometric%20systems%20across%20diverse%0Aphysiological%20states.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04973v1&entry.124074799=Read"},
{"title": "On the Power of Heuristics in Temporal Graphs", "author": "Filip Cornell and Oleg Smirnov and Gabriela Zarzar Gandler and Lele Cao", "abstract": "  Dynamic graph datasets often exhibit strong temporal patterns, such as\nrecency, which prioritizes recent interactions, and popularity, which favors\nfrequently occurring nodes. We demonstrate that simple heuristics leveraging\nonly these patterns can perform on par or outperform state-of-the-art neural\nnetwork models under standard evaluation protocols. To further explore these\ndynamics, we introduce metrics that quantify the impact of recency and\npopularity across datasets. Our experiments on BenchTemp and the Temporal Graph\nBenchmark show that our approaches achieve state-of-the-art performance across\nall datasets in the latter and secure top ranks on multiple datasets in the\nformer. These results emphasize the importance of refined evaluation schemes to\nenable fair comparisons and promote the development of more robust temporal\ngraph models. Additionally, they reveal that current deep learning methods\noften struggle to capture the key patterns underlying predictions in real-world\ntemporal graphs. For reproducibility, we have made our code publicly available.\n", "link": "http://arxiv.org/abs/2502.04910v1", "date": "2025-02-07", "relevancy": 2.4166, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.498}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4793}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Power%20of%20Heuristics%20in%20Temporal%20Graphs&body=Title%3A%20On%20the%20Power%20of%20Heuristics%20in%20Temporal%20Graphs%0AAuthor%3A%20Filip%20Cornell%20and%20Oleg%20Smirnov%20and%20Gabriela%20Zarzar%20Gandler%20and%20Lele%20Cao%0AAbstract%3A%20%20%20Dynamic%20graph%20datasets%20often%20exhibit%20strong%20temporal%20patterns%2C%20such%20as%0Arecency%2C%20which%20prioritizes%20recent%20interactions%2C%20and%20popularity%2C%20which%20favors%0Afrequently%20occurring%20nodes.%20We%20demonstrate%20that%20simple%20heuristics%20leveraging%0Aonly%20these%20patterns%20can%20perform%20on%20par%20or%20outperform%20state-of-the-art%20neural%0Anetwork%20models%20under%20standard%20evaluation%20protocols.%20To%20further%20explore%20these%0Adynamics%2C%20we%20introduce%20metrics%20that%20quantify%20the%20impact%20of%20recency%20and%0Apopularity%20across%20datasets.%20Our%20experiments%20on%20BenchTemp%20and%20the%20Temporal%20Graph%0ABenchmark%20show%20that%20our%20approaches%20achieve%20state-of-the-art%20performance%20across%0Aall%20datasets%20in%20the%20latter%20and%20secure%20top%20ranks%20on%20multiple%20datasets%20in%20the%0Aformer.%20These%20results%20emphasize%20the%20importance%20of%20refined%20evaluation%20schemes%20to%0Aenable%20fair%20comparisons%20and%20promote%20the%20development%20of%20more%20robust%20temporal%0Agraph%20models.%20Additionally%2C%20they%20reveal%20that%20current%20deep%20learning%20methods%0Aoften%20struggle%20to%20capture%20the%20key%20patterns%20underlying%20predictions%20in%20real-world%0Atemporal%20graphs.%20For%20reproducibility%2C%20we%20have%20made%20our%20code%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04910v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Power%2520of%2520Heuristics%2520in%2520Temporal%2520Graphs%26entry.906535625%3DFilip%2520Cornell%2520and%2520Oleg%2520Smirnov%2520and%2520Gabriela%2520Zarzar%2520Gandler%2520and%2520Lele%2520Cao%26entry.1292438233%3D%2520%2520Dynamic%2520graph%2520datasets%2520often%2520exhibit%2520strong%2520temporal%2520patterns%252C%2520such%2520as%250Arecency%252C%2520which%2520prioritizes%2520recent%2520interactions%252C%2520and%2520popularity%252C%2520which%2520favors%250Afrequently%2520occurring%2520nodes.%2520We%2520demonstrate%2520that%2520simple%2520heuristics%2520leveraging%250Aonly%2520these%2520patterns%2520can%2520perform%2520on%2520par%2520or%2520outperform%2520state-of-the-art%2520neural%250Anetwork%2520models%2520under%2520standard%2520evaluation%2520protocols.%2520To%2520further%2520explore%2520these%250Adynamics%252C%2520we%2520introduce%2520metrics%2520that%2520quantify%2520the%2520impact%2520of%2520recency%2520and%250Apopularity%2520across%2520datasets.%2520Our%2520experiments%2520on%2520BenchTemp%2520and%2520the%2520Temporal%2520Graph%250ABenchmark%2520show%2520that%2520our%2520approaches%2520achieve%2520state-of-the-art%2520performance%2520across%250Aall%2520datasets%2520in%2520the%2520latter%2520and%2520secure%2520top%2520ranks%2520on%2520multiple%2520datasets%2520in%2520the%250Aformer.%2520These%2520results%2520emphasize%2520the%2520importance%2520of%2520refined%2520evaluation%2520schemes%2520to%250Aenable%2520fair%2520comparisons%2520and%2520promote%2520the%2520development%2520of%2520more%2520robust%2520temporal%250Agraph%2520models.%2520Additionally%252C%2520they%2520reveal%2520that%2520current%2520deep%2520learning%2520methods%250Aoften%2520struggle%2520to%2520capture%2520the%2520key%2520patterns%2520underlying%2520predictions%2520in%2520real-world%250Atemporal%2520graphs.%2520For%2520reproducibility%252C%2520we%2520have%2520made%2520our%2520code%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04910v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Power%20of%20Heuristics%20in%20Temporal%20Graphs&entry.906535625=Filip%20Cornell%20and%20Oleg%20Smirnov%20and%20Gabriela%20Zarzar%20Gandler%20and%20Lele%20Cao&entry.1292438233=%20%20Dynamic%20graph%20datasets%20often%20exhibit%20strong%20temporal%20patterns%2C%20such%20as%0Arecency%2C%20which%20prioritizes%20recent%20interactions%2C%20and%20popularity%2C%20which%20favors%0Afrequently%20occurring%20nodes.%20We%20demonstrate%20that%20simple%20heuristics%20leveraging%0Aonly%20these%20patterns%20can%20perform%20on%20par%20or%20outperform%20state-of-the-art%20neural%0Anetwork%20models%20under%20standard%20evaluation%20protocols.%20To%20further%20explore%20these%0Adynamics%2C%20we%20introduce%20metrics%20that%20quantify%20the%20impact%20of%20recency%20and%0Apopularity%20across%20datasets.%20Our%20experiments%20on%20BenchTemp%20and%20the%20Temporal%20Graph%0ABenchmark%20show%20that%20our%20approaches%20achieve%20state-of-the-art%20performance%20across%0Aall%20datasets%20in%20the%20latter%20and%20secure%20top%20ranks%20on%20multiple%20datasets%20in%20the%0Aformer.%20These%20results%20emphasize%20the%20importance%20of%20refined%20evaluation%20schemes%20to%0Aenable%20fair%20comparisons%20and%20promote%20the%20development%20of%20more%20robust%20temporal%0Agraph%20models.%20Additionally%2C%20they%20reveal%20that%20current%20deep%20learning%20methods%0Aoften%20struggle%20to%20capture%20the%20key%20patterns%20underlying%20predictions%20in%20real-world%0Atemporal%20graphs.%20For%20reproducibility%2C%20we%20have%20made%20our%20code%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04910v1&entry.124074799=Read"},
{"title": "Smirk: An Atomically Complete Tokenizer for Molecular Foundation Models", "author": "Alexius Wadell and Anoushka Bhutani and Venkatasubramanian Viswanathan", "abstract": "  Text-based foundation models have become an important part of scientific\ndiscovery, with molecular foundation models accelerating advancements in\nmolecular design and materials science. However, existing models are\nconstrained by closed-vocabulary tokenizers which capture only a fraction of\nmolecular space. In this work, we systematically evaluate thirty tokenizers,\nincluding 19 chemistry-specific ones, for their coverage of the SMILES\nmolecular representation language, revealing significant gaps. To assess the\nimpact of tokenizer choice, we introduce n-gram language models as a low-cost\nproxy and validate their effectiveness by training and fine-tuning 18\nRoBERTa-style encoders for molecular property prediction. To overcome the\nlimitations of existing tokenizers, we propose two new tokenizers -- Smirk and\nSmirk-GPE -- with full coverage of the OpenSMILES specification. Our results\nhighlight the need for open-vocabulary modeling and chemically diverse\nbenchmarks in cheminformatics. The proposed tokenizer framework systematically\nintegrates nuclear, electronic, and geometric degrees of freedom; this\nfacilitates applications in pharmacology, agriculture, biology, and energy\nstorage.\n", "link": "http://arxiv.org/abs/2409.15370v2", "date": "2025-02-07", "relevancy": 2.415, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4939}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4939}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Smirk%3A%20An%20Atomically%20Complete%20Tokenizer%20for%20Molecular%20Foundation%20Models&body=Title%3A%20Smirk%3A%20An%20Atomically%20Complete%20Tokenizer%20for%20Molecular%20Foundation%20Models%0AAuthor%3A%20Alexius%20Wadell%20and%20Anoushka%20Bhutani%20and%20Venkatasubramanian%20Viswanathan%0AAbstract%3A%20%20%20Text-based%20foundation%20models%20have%20become%20an%20important%20part%20of%20scientific%0Adiscovery%2C%20with%20molecular%20foundation%20models%20accelerating%20advancements%20in%0Amolecular%20design%20and%20materials%20science.%20However%2C%20existing%20models%20are%0Aconstrained%20by%20closed-vocabulary%20tokenizers%20which%20capture%20only%20a%20fraction%20of%0Amolecular%20space.%20In%20this%20work%2C%20we%20systematically%20evaluate%20thirty%20tokenizers%2C%0Aincluding%2019%20chemistry-specific%20ones%2C%20for%20their%20coverage%20of%20the%20SMILES%0Amolecular%20representation%20language%2C%20revealing%20significant%20gaps.%20To%20assess%20the%0Aimpact%20of%20tokenizer%20choice%2C%20we%20introduce%20n-gram%20language%20models%20as%20a%20low-cost%0Aproxy%20and%20validate%20their%20effectiveness%20by%20training%20and%20fine-tuning%2018%0ARoBERTa-style%20encoders%20for%20molecular%20property%20prediction.%20To%20overcome%20the%0Alimitations%20of%20existing%20tokenizers%2C%20we%20propose%20two%20new%20tokenizers%20--%20Smirk%20and%0ASmirk-GPE%20--%20with%20full%20coverage%20of%20the%20OpenSMILES%20specification.%20Our%20results%0Ahighlight%20the%20need%20for%20open-vocabulary%20modeling%20and%20chemically%20diverse%0Abenchmarks%20in%20cheminformatics.%20The%20proposed%20tokenizer%20framework%20systematically%0Aintegrates%20nuclear%2C%20electronic%2C%20and%20geometric%20degrees%20of%20freedom%3B%20this%0Afacilitates%20applications%20in%20pharmacology%2C%20agriculture%2C%20biology%2C%20and%20energy%0Astorage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.15370v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmirk%253A%2520An%2520Atomically%2520Complete%2520Tokenizer%2520for%2520Molecular%2520Foundation%2520Models%26entry.906535625%3DAlexius%2520Wadell%2520and%2520Anoushka%2520Bhutani%2520and%2520Venkatasubramanian%2520Viswanathan%26entry.1292438233%3D%2520%2520Text-based%2520foundation%2520models%2520have%2520become%2520an%2520important%2520part%2520of%2520scientific%250Adiscovery%252C%2520with%2520molecular%2520foundation%2520models%2520accelerating%2520advancements%2520in%250Amolecular%2520design%2520and%2520materials%2520science.%2520However%252C%2520existing%2520models%2520are%250Aconstrained%2520by%2520closed-vocabulary%2520tokenizers%2520which%2520capture%2520only%2520a%2520fraction%2520of%250Amolecular%2520space.%2520In%2520this%2520work%252C%2520we%2520systematically%2520evaluate%2520thirty%2520tokenizers%252C%250Aincluding%252019%2520chemistry-specific%2520ones%252C%2520for%2520their%2520coverage%2520of%2520the%2520SMILES%250Amolecular%2520representation%2520language%252C%2520revealing%2520significant%2520gaps.%2520To%2520assess%2520the%250Aimpact%2520of%2520tokenizer%2520choice%252C%2520we%2520introduce%2520n-gram%2520language%2520models%2520as%2520a%2520low-cost%250Aproxy%2520and%2520validate%2520their%2520effectiveness%2520by%2520training%2520and%2520fine-tuning%252018%250ARoBERTa-style%2520encoders%2520for%2520molecular%2520property%2520prediction.%2520To%2520overcome%2520the%250Alimitations%2520of%2520existing%2520tokenizers%252C%2520we%2520propose%2520two%2520new%2520tokenizers%2520--%2520Smirk%2520and%250ASmirk-GPE%2520--%2520with%2520full%2520coverage%2520of%2520the%2520OpenSMILES%2520specification.%2520Our%2520results%250Ahighlight%2520the%2520need%2520for%2520open-vocabulary%2520modeling%2520and%2520chemically%2520diverse%250Abenchmarks%2520in%2520cheminformatics.%2520The%2520proposed%2520tokenizer%2520framework%2520systematically%250Aintegrates%2520nuclear%252C%2520electronic%252C%2520and%2520geometric%2520degrees%2520of%2520freedom%253B%2520this%250Afacilitates%2520applications%2520in%2520pharmacology%252C%2520agriculture%252C%2520biology%252C%2520and%2520energy%250Astorage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.15370v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Smirk%3A%20An%20Atomically%20Complete%20Tokenizer%20for%20Molecular%20Foundation%20Models&entry.906535625=Alexius%20Wadell%20and%20Anoushka%20Bhutani%20and%20Venkatasubramanian%20Viswanathan&entry.1292438233=%20%20Text-based%20foundation%20models%20have%20become%20an%20important%20part%20of%20scientific%0Adiscovery%2C%20with%20molecular%20foundation%20models%20accelerating%20advancements%20in%0Amolecular%20design%20and%20materials%20science.%20However%2C%20existing%20models%20are%0Aconstrained%20by%20closed-vocabulary%20tokenizers%20which%20capture%20only%20a%20fraction%20of%0Amolecular%20space.%20In%20this%20work%2C%20we%20systematically%20evaluate%20thirty%20tokenizers%2C%0Aincluding%2019%20chemistry-specific%20ones%2C%20for%20their%20coverage%20of%20the%20SMILES%0Amolecular%20representation%20language%2C%20revealing%20significant%20gaps.%20To%20assess%20the%0Aimpact%20of%20tokenizer%20choice%2C%20we%20introduce%20n-gram%20language%20models%20as%20a%20low-cost%0Aproxy%20and%20validate%20their%20effectiveness%20by%20training%20and%20fine-tuning%2018%0ARoBERTa-style%20encoders%20for%20molecular%20property%20prediction.%20To%20overcome%20the%0Alimitations%20of%20existing%20tokenizers%2C%20we%20propose%20two%20new%20tokenizers%20--%20Smirk%20and%0ASmirk-GPE%20--%20with%20full%20coverage%20of%20the%20OpenSMILES%20specification.%20Our%20results%0Ahighlight%20the%20need%20for%20open-vocabulary%20modeling%20and%20chemically%20diverse%0Abenchmarks%20in%20cheminformatics.%20The%20proposed%20tokenizer%20framework%20systematically%0Aintegrates%20nuclear%2C%20electronic%2C%20and%20geometric%20degrees%20of%20freedom%3B%20this%0Afacilitates%20applications%20in%20pharmacology%2C%20agriculture%2C%20biology%2C%20and%20energy%0Astorage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.15370v2&entry.124074799=Read"},
{"title": "A Strong Baseline for Molecular Few-Shot Learning", "author": "Philippe Formont and Hugo Jeannin and Pablo Piantanida and Ismail Ben Ayed", "abstract": "  Few-shot learning has recently attracted significant interest in drug\ndiscovery, with a recent, fast-growing literature mostly involving convoluted\nmeta-learning strategies. We revisit the more straightforward fine-tuning\napproach for molecular data, and propose a regularized quadratic-probe loss\nbased on the the Mahalanobis distance. We design a dedicated block-coordinate\ndescent optimizer, which avoid the degenerate solutions of our loss.\nInterestingly, our simple fine-tuning approach achieves highly competitive\nperformances in comparison to state-of-the-art methods, while being applicable\nto black-box settings and removing the need for specific episodic pre-training\nstrategies. Furthermore, we introduce a new benchmark to assess the robustness\nof the competing methods to domain shifts. In this setting, our fine-tuning\nbaseline obtains consistently better results than meta-learning methods.\n", "link": "http://arxiv.org/abs/2404.02314v2", "date": "2025-02-07", "relevancy": 2.3955, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4891}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4781}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.47}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Strong%20Baseline%20for%20Molecular%20Few-Shot%20Learning&body=Title%3A%20A%20Strong%20Baseline%20for%20Molecular%20Few-Shot%20Learning%0AAuthor%3A%20Philippe%20Formont%20and%20Hugo%20Jeannin%20and%20Pablo%20Piantanida%20and%20Ismail%20Ben%20Ayed%0AAbstract%3A%20%20%20Few-shot%20learning%20has%20recently%20attracted%20significant%20interest%20in%20drug%0Adiscovery%2C%20with%20a%20recent%2C%20fast-growing%20literature%20mostly%20involving%20convoluted%0Ameta-learning%20strategies.%20We%20revisit%20the%20more%20straightforward%20fine-tuning%0Aapproach%20for%20molecular%20data%2C%20and%20propose%20a%20regularized%20quadratic-probe%20loss%0Abased%20on%20the%20the%20Mahalanobis%20distance.%20We%20design%20a%20dedicated%20block-coordinate%0Adescent%20optimizer%2C%20which%20avoid%20the%20degenerate%20solutions%20of%20our%20loss.%0AInterestingly%2C%20our%20simple%20fine-tuning%20approach%20achieves%20highly%20competitive%0Aperformances%20in%20comparison%20to%20state-of-the-art%20methods%2C%20while%20being%20applicable%0Ato%20black-box%20settings%20and%20removing%20the%20need%20for%20specific%20episodic%20pre-training%0Astrategies.%20Furthermore%2C%20we%20introduce%20a%20new%20benchmark%20to%20assess%20the%20robustness%0Aof%20the%20competing%20methods%20to%20domain%20shifts.%20In%20this%20setting%2C%20our%20fine-tuning%0Abaseline%20obtains%20consistently%20better%20results%20than%20meta-learning%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02314v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Strong%2520Baseline%2520for%2520Molecular%2520Few-Shot%2520Learning%26entry.906535625%3DPhilippe%2520Formont%2520and%2520Hugo%2520Jeannin%2520and%2520Pablo%2520Piantanida%2520and%2520Ismail%2520Ben%2520Ayed%26entry.1292438233%3D%2520%2520Few-shot%2520learning%2520has%2520recently%2520attracted%2520significant%2520interest%2520in%2520drug%250Adiscovery%252C%2520with%2520a%2520recent%252C%2520fast-growing%2520literature%2520mostly%2520involving%2520convoluted%250Ameta-learning%2520strategies.%2520We%2520revisit%2520the%2520more%2520straightforward%2520fine-tuning%250Aapproach%2520for%2520molecular%2520data%252C%2520and%2520propose%2520a%2520regularized%2520quadratic-probe%2520loss%250Abased%2520on%2520the%2520the%2520Mahalanobis%2520distance.%2520We%2520design%2520a%2520dedicated%2520block-coordinate%250Adescent%2520optimizer%252C%2520which%2520avoid%2520the%2520degenerate%2520solutions%2520of%2520our%2520loss.%250AInterestingly%252C%2520our%2520simple%2520fine-tuning%2520approach%2520achieves%2520highly%2520competitive%250Aperformances%2520in%2520comparison%2520to%2520state-of-the-art%2520methods%252C%2520while%2520being%2520applicable%250Ato%2520black-box%2520settings%2520and%2520removing%2520the%2520need%2520for%2520specific%2520episodic%2520pre-training%250Astrategies.%2520Furthermore%252C%2520we%2520introduce%2520a%2520new%2520benchmark%2520to%2520assess%2520the%2520robustness%250Aof%2520the%2520competing%2520methods%2520to%2520domain%2520shifts.%2520In%2520this%2520setting%252C%2520our%2520fine-tuning%250Abaseline%2520obtains%2520consistently%2520better%2520results%2520than%2520meta-learning%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.02314v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Strong%20Baseline%20for%20Molecular%20Few-Shot%20Learning&entry.906535625=Philippe%20Formont%20and%20Hugo%20Jeannin%20and%20Pablo%20Piantanida%20and%20Ismail%20Ben%20Ayed&entry.1292438233=%20%20Few-shot%20learning%20has%20recently%20attracted%20significant%20interest%20in%20drug%0Adiscovery%2C%20with%20a%20recent%2C%20fast-growing%20literature%20mostly%20involving%20convoluted%0Ameta-learning%20strategies.%20We%20revisit%20the%20more%20straightforward%20fine-tuning%0Aapproach%20for%20molecular%20data%2C%20and%20propose%20a%20regularized%20quadratic-probe%20loss%0Abased%20on%20the%20the%20Mahalanobis%20distance.%20We%20design%20a%20dedicated%20block-coordinate%0Adescent%20optimizer%2C%20which%20avoid%20the%20degenerate%20solutions%20of%20our%20loss.%0AInterestingly%2C%20our%20simple%20fine-tuning%20approach%20achieves%20highly%20competitive%0Aperformances%20in%20comparison%20to%20state-of-the-art%20methods%2C%20while%20being%20applicable%0Ato%20black-box%20settings%20and%20removing%20the%20need%20for%20specific%20episodic%20pre-training%0Astrategies.%20Furthermore%2C%20we%20introduce%20a%20new%20benchmark%20to%20assess%20the%20robustness%0Aof%20the%20competing%20methods%20to%20domain%20shifts.%20In%20this%20setting%2C%20our%20fine-tuning%0Abaseline%20obtains%20consistently%20better%20results%20than%20meta-learning%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02314v2&entry.124074799=Read"},
{"title": "PRISM-TopoMap: Online Topological Mapping with Place Recognition and\n  Scan Matching", "author": "Kirill Muravyev and Alexander Melekhin and Dmitry Yudin and Konstantin Yakovlev", "abstract": "  Mapping is one of the crucial tasks enabling autonomous navigation of a\nmobile robot. Conventional mapping methods output a dense geometric map\nrepresentation, e.g. an occupancy grid, which is not trivial to keep consistent\nfor prolonged runs covering large environments. Meanwhile, capturing the\ntopological structure of the workspace enables fast path planning, is typically\nless prone to odometry error accumulation, and does not consume much memory.\nFollowing this idea, this paper introduces PRISM-TopoMap -- a topological\nmapping method that maintains a graph of locally aligned locations not relying\non global metric coordinates. The proposed method involves original learnable\nmultimodal place recognition paired with the scan matching pipeline for\nlocalization and loop closure in the graph of locations. The latter is updated\nonline, and the robot is localized in a proper node at each time step. We\nconduct a broad experimental evaluation of the suggested approach in a range of\nphoto-realistic environments and on a real robot, and compare it to state of\nthe art. The results of the empirical evaluation confirm that PRISM-Topomap\nconsistently outperforms competitors computationally-wise, achieves high\nmapping quality and performs well on a real robot. The code of PRISM-Topomap is\nopen-sourced and is available at:\nhttps://github.com/kirillMouraviev/prism-topomap.\n", "link": "http://arxiv.org/abs/2404.01674v4", "date": "2025-02-07", "relevancy": 2.3892, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6407}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5713}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRISM-TopoMap%3A%20Online%20Topological%20Mapping%20with%20Place%20Recognition%20and%0A%20%20Scan%20Matching&body=Title%3A%20PRISM-TopoMap%3A%20Online%20Topological%20Mapping%20with%20Place%20Recognition%20and%0A%20%20Scan%20Matching%0AAuthor%3A%20Kirill%20Muravyev%20and%20Alexander%20Melekhin%20and%20Dmitry%20Yudin%20and%20Konstantin%20Yakovlev%0AAbstract%3A%20%20%20Mapping%20is%20one%20of%20the%20crucial%20tasks%20enabling%20autonomous%20navigation%20of%20a%0Amobile%20robot.%20Conventional%20mapping%20methods%20output%20a%20dense%20geometric%20map%0Arepresentation%2C%20e.g.%20an%20occupancy%20grid%2C%20which%20is%20not%20trivial%20to%20keep%20consistent%0Afor%20prolonged%20runs%20covering%20large%20environments.%20Meanwhile%2C%20capturing%20the%0Atopological%20structure%20of%20the%20workspace%20enables%20fast%20path%20planning%2C%20is%20typically%0Aless%20prone%20to%20odometry%20error%20accumulation%2C%20and%20does%20not%20consume%20much%20memory.%0AFollowing%20this%20idea%2C%20this%20paper%20introduces%20PRISM-TopoMap%20--%20a%20topological%0Amapping%20method%20that%20maintains%20a%20graph%20of%20locally%20aligned%20locations%20not%20relying%0Aon%20global%20metric%20coordinates.%20The%20proposed%20method%20involves%20original%20learnable%0Amultimodal%20place%20recognition%20paired%20with%20the%20scan%20matching%20pipeline%20for%0Alocalization%20and%20loop%20closure%20in%20the%20graph%20of%20locations.%20The%20latter%20is%20updated%0Aonline%2C%20and%20the%20robot%20is%20localized%20in%20a%20proper%20node%20at%20each%20time%20step.%20We%0Aconduct%20a%20broad%20experimental%20evaluation%20of%20the%20suggested%20approach%20in%20a%20range%20of%0Aphoto-realistic%20environments%20and%20on%20a%20real%20robot%2C%20and%20compare%20it%20to%20state%20of%0Athe%20art.%20The%20results%20of%20the%20empirical%20evaluation%20confirm%20that%20PRISM-Topomap%0Aconsistently%20outperforms%20competitors%20computationally-wise%2C%20achieves%20high%0Amapping%20quality%20and%20performs%20well%20on%20a%20real%20robot.%20The%20code%20of%20PRISM-Topomap%20is%0Aopen-sourced%20and%20is%20available%20at%3A%0Ahttps%3A//github.com/kirillMouraviev/prism-topomap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01674v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRISM-TopoMap%253A%2520Online%2520Topological%2520Mapping%2520with%2520Place%2520Recognition%2520and%250A%2520%2520Scan%2520Matching%26entry.906535625%3DKirill%2520Muravyev%2520and%2520Alexander%2520Melekhin%2520and%2520Dmitry%2520Yudin%2520and%2520Konstantin%2520Yakovlev%26entry.1292438233%3D%2520%2520Mapping%2520is%2520one%2520of%2520the%2520crucial%2520tasks%2520enabling%2520autonomous%2520navigation%2520of%2520a%250Amobile%2520robot.%2520Conventional%2520mapping%2520methods%2520output%2520a%2520dense%2520geometric%2520map%250Arepresentation%252C%2520e.g.%2520an%2520occupancy%2520grid%252C%2520which%2520is%2520not%2520trivial%2520to%2520keep%2520consistent%250Afor%2520prolonged%2520runs%2520covering%2520large%2520environments.%2520Meanwhile%252C%2520capturing%2520the%250Atopological%2520structure%2520of%2520the%2520workspace%2520enables%2520fast%2520path%2520planning%252C%2520is%2520typically%250Aless%2520prone%2520to%2520odometry%2520error%2520accumulation%252C%2520and%2520does%2520not%2520consume%2520much%2520memory.%250AFollowing%2520this%2520idea%252C%2520this%2520paper%2520introduces%2520PRISM-TopoMap%2520--%2520a%2520topological%250Amapping%2520method%2520that%2520maintains%2520a%2520graph%2520of%2520locally%2520aligned%2520locations%2520not%2520relying%250Aon%2520global%2520metric%2520coordinates.%2520The%2520proposed%2520method%2520involves%2520original%2520learnable%250Amultimodal%2520place%2520recognition%2520paired%2520with%2520the%2520scan%2520matching%2520pipeline%2520for%250Alocalization%2520and%2520loop%2520closure%2520in%2520the%2520graph%2520of%2520locations.%2520The%2520latter%2520is%2520updated%250Aonline%252C%2520and%2520the%2520robot%2520is%2520localized%2520in%2520a%2520proper%2520node%2520at%2520each%2520time%2520step.%2520We%250Aconduct%2520a%2520broad%2520experimental%2520evaluation%2520of%2520the%2520suggested%2520approach%2520in%2520a%2520range%2520of%250Aphoto-realistic%2520environments%2520and%2520on%2520a%2520real%2520robot%252C%2520and%2520compare%2520it%2520to%2520state%2520of%250Athe%2520art.%2520The%2520results%2520of%2520the%2520empirical%2520evaluation%2520confirm%2520that%2520PRISM-Topomap%250Aconsistently%2520outperforms%2520competitors%2520computationally-wise%252C%2520achieves%2520high%250Amapping%2520quality%2520and%2520performs%2520well%2520on%2520a%2520real%2520robot.%2520The%2520code%2520of%2520PRISM-Topomap%2520is%250Aopen-sourced%2520and%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/kirillMouraviev/prism-topomap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.01674v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRISM-TopoMap%3A%20Online%20Topological%20Mapping%20with%20Place%20Recognition%20and%0A%20%20Scan%20Matching&entry.906535625=Kirill%20Muravyev%20and%20Alexander%20Melekhin%20and%20Dmitry%20Yudin%20and%20Konstantin%20Yakovlev&entry.1292438233=%20%20Mapping%20is%20one%20of%20the%20crucial%20tasks%20enabling%20autonomous%20navigation%20of%20a%0Amobile%20robot.%20Conventional%20mapping%20methods%20output%20a%20dense%20geometric%20map%0Arepresentation%2C%20e.g.%20an%20occupancy%20grid%2C%20which%20is%20not%20trivial%20to%20keep%20consistent%0Afor%20prolonged%20runs%20covering%20large%20environments.%20Meanwhile%2C%20capturing%20the%0Atopological%20structure%20of%20the%20workspace%20enables%20fast%20path%20planning%2C%20is%20typically%0Aless%20prone%20to%20odometry%20error%20accumulation%2C%20and%20does%20not%20consume%20much%20memory.%0AFollowing%20this%20idea%2C%20this%20paper%20introduces%20PRISM-TopoMap%20--%20a%20topological%0Amapping%20method%20that%20maintains%20a%20graph%20of%20locally%20aligned%20locations%20not%20relying%0Aon%20global%20metric%20coordinates.%20The%20proposed%20method%20involves%20original%20learnable%0Amultimodal%20place%20recognition%20paired%20with%20the%20scan%20matching%20pipeline%20for%0Alocalization%20and%20loop%20closure%20in%20the%20graph%20of%20locations.%20The%20latter%20is%20updated%0Aonline%2C%20and%20the%20robot%20is%20localized%20in%20a%20proper%20node%20at%20each%20time%20step.%20We%0Aconduct%20a%20broad%20experimental%20evaluation%20of%20the%20suggested%20approach%20in%20a%20range%20of%0Aphoto-realistic%20environments%20and%20on%20a%20real%20robot%2C%20and%20compare%20it%20to%20state%20of%0Athe%20art.%20The%20results%20of%20the%20empirical%20evaluation%20confirm%20that%20PRISM-Topomap%0Aconsistently%20outperforms%20competitors%20computationally-wise%2C%20achieves%20high%0Amapping%20quality%20and%20performs%20well%20on%20a%20real%20robot.%20The%20code%20of%20PRISM-Topomap%20is%0Aopen-sourced%20and%20is%20available%20at%3A%0Ahttps%3A//github.com/kirillMouraviev/prism-topomap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01674v4&entry.124074799=Read"},
{"title": "Training-free Task-oriented Grasp Generation", "author": "Jiaming Wang and Jizhuo Chen and Diwen Liu", "abstract": "  This paper presents a training-free pipeline for task-oriented grasp\ngeneration that combines pre-trained grasp generation models with\nvision-language models (VLMs). Unlike traditional approaches that focus solely\non stable grasps, our method incorporates task-specific requirements by\nleveraging the semantic reasoning capabilities of VLMs. We evaluate five\nquerying strategies, each utilizing different visual representations of\ncandidate grasps, and demonstrate significant improvements over a baseline\nmethod in both grasp success and task compliance rates, with absolute gains of\nup to 36.9% in overall success rate. Our results underline the potential of\nVLMs to enhance task-oriented manipulation, providing insights for future\nresearch in robotic grasping and human-robot interaction.\n", "link": "http://arxiv.org/abs/2502.04873v1", "date": "2025-02-07", "relevancy": 2.3806, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6585}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5522}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-free%20Task-oriented%20Grasp%20Generation&body=Title%3A%20Training-free%20Task-oriented%20Grasp%20Generation%0AAuthor%3A%20Jiaming%20Wang%20and%20Jizhuo%20Chen%20and%20Diwen%20Liu%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20training-free%20pipeline%20for%20task-oriented%20grasp%0Ageneration%20that%20combines%20pre-trained%20grasp%20generation%20models%20with%0Avision-language%20models%20%28VLMs%29.%20Unlike%20traditional%20approaches%20that%20focus%20solely%0Aon%20stable%20grasps%2C%20our%20method%20incorporates%20task-specific%20requirements%20by%0Aleveraging%20the%20semantic%20reasoning%20capabilities%20of%20VLMs.%20We%20evaluate%20five%0Aquerying%20strategies%2C%20each%20utilizing%20different%20visual%20representations%20of%0Acandidate%20grasps%2C%20and%20demonstrate%20significant%20improvements%20over%20a%20baseline%0Amethod%20in%20both%20grasp%20success%20and%20task%20compliance%20rates%2C%20with%20absolute%20gains%20of%0Aup%20to%2036.9%25%20in%20overall%20success%20rate.%20Our%20results%20underline%20the%20potential%20of%0AVLMs%20to%20enhance%20task-oriented%20manipulation%2C%20providing%20insights%20for%20future%0Aresearch%20in%20robotic%20grasping%20and%20human-robot%20interaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04873v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-free%2520Task-oriented%2520Grasp%2520Generation%26entry.906535625%3DJiaming%2520Wang%2520and%2520Jizhuo%2520Chen%2520and%2520Diwen%2520Liu%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520training-free%2520pipeline%2520for%2520task-oriented%2520grasp%250Ageneration%2520that%2520combines%2520pre-trained%2520grasp%2520generation%2520models%2520with%250Avision-language%2520models%2520%2528VLMs%2529.%2520Unlike%2520traditional%2520approaches%2520that%2520focus%2520solely%250Aon%2520stable%2520grasps%252C%2520our%2520method%2520incorporates%2520task-specific%2520requirements%2520by%250Aleveraging%2520the%2520semantic%2520reasoning%2520capabilities%2520of%2520VLMs.%2520We%2520evaluate%2520five%250Aquerying%2520strategies%252C%2520each%2520utilizing%2520different%2520visual%2520representations%2520of%250Acandidate%2520grasps%252C%2520and%2520demonstrate%2520significant%2520improvements%2520over%2520a%2520baseline%250Amethod%2520in%2520both%2520grasp%2520success%2520and%2520task%2520compliance%2520rates%252C%2520with%2520absolute%2520gains%2520of%250Aup%2520to%252036.9%2525%2520in%2520overall%2520success%2520rate.%2520Our%2520results%2520underline%2520the%2520potential%2520of%250AVLMs%2520to%2520enhance%2520task-oriented%2520manipulation%252C%2520providing%2520insights%2520for%2520future%250Aresearch%2520in%2520robotic%2520grasping%2520and%2520human-robot%2520interaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04873v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-free%20Task-oriented%20Grasp%20Generation&entry.906535625=Jiaming%20Wang%20and%20Jizhuo%20Chen%20and%20Diwen%20Liu&entry.1292438233=%20%20This%20paper%20presents%20a%20training-free%20pipeline%20for%20task-oriented%20grasp%0Ageneration%20that%20combines%20pre-trained%20grasp%20generation%20models%20with%0Avision-language%20models%20%28VLMs%29.%20Unlike%20traditional%20approaches%20that%20focus%20solely%0Aon%20stable%20grasps%2C%20our%20method%20incorporates%20task-specific%20requirements%20by%0Aleveraging%20the%20semantic%20reasoning%20capabilities%20of%20VLMs.%20We%20evaluate%20five%0Aquerying%20strategies%2C%20each%20utilizing%20different%20visual%20representations%20of%0Acandidate%20grasps%2C%20and%20demonstrate%20significant%20improvements%20over%20a%20baseline%0Amethod%20in%20both%20grasp%20success%20and%20task%20compliance%20rates%2C%20with%20absolute%20gains%20of%0Aup%20to%2036.9%25%20in%20overall%20success%20rate.%20Our%20results%20underline%20the%20potential%20of%0AVLMs%20to%20enhance%20task-oriented%20manipulation%2C%20providing%20insights%20for%20future%0Aresearch%20in%20robotic%20grasping%20and%20human-robot%20interaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04873v1&entry.124074799=Read"},
{"title": "Kinematic-ICP: Enhancing LiDAR Odometry with Kinematic Constraints for\n  Wheeled Mobile Robots Moving on Planar Surfaces", "author": "Tiziano Guadagnino and Benedikt Mersch and Ignacio Vizzo and Saurabh Gupta and Meher V. R. Malladi and Luca Lobefaro and Guillaume Doisy and Cyrill Stachniss", "abstract": "  LiDAR odometry is essential for many robotics applications, including 3D\nmapping, navigation, and simultaneous localization and mapping. LiDAR odometry\nsystems are usually based on some form of point cloud registration to compute\nthe ego-motion of a mobile robot. Yet, few of today's LiDAR odometry systems\nconsider domain-specific knowledge or the kinematic model of the mobile\nplatform during the point cloud alignment. In this paper, we present\nKinematic-ICP, a LiDAR odometry system that focuses on wheeled mobile robots\nequipped with a 3D LiDAR and moving on a planar surface, which is a common\nassumption for warehouses, offices, hospitals, etc. Our approach introduces\nkinematic constraints within the optimization of a traditional point-to-point\niterative closest point scheme. In this way, the resulting motion follows the\nkinematic constraints of the platform, effectively exploiting the robot's wheel\nodometry and the 3D LiDAR observations. We dynamically adjust the influence of\nLiDAR measurements and wheel odometry in our optimization scheme, allowing the\nsystem to handle degenerate scenarios such as feature-poor corridors. We\nevaluate our approach on robots operating in large-scale warehouse\nenvironments, but also outdoors. The experiments show that our approach\nachieves top performances and is more accurate than wheel odometry and common\nLiDAR odometry systems. Kinematic-ICP has been recently deployed in the Dexory\nfleet of robots operating in warehouses worldwide at their customers' sites,\nshowing that our method can run in the real world alongside a complete\nnavigation stack.\n", "link": "http://arxiv.org/abs/2410.10277v2", "date": "2025-02-07", "relevancy": 2.3587, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6287}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.589}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kinematic-ICP%3A%20Enhancing%20LiDAR%20Odometry%20with%20Kinematic%20Constraints%20for%0A%20%20Wheeled%20Mobile%20Robots%20Moving%20on%20Planar%20Surfaces&body=Title%3A%20Kinematic-ICP%3A%20Enhancing%20LiDAR%20Odometry%20with%20Kinematic%20Constraints%20for%0A%20%20Wheeled%20Mobile%20Robots%20Moving%20on%20Planar%20Surfaces%0AAuthor%3A%20Tiziano%20Guadagnino%20and%20Benedikt%20Mersch%20and%20Ignacio%20Vizzo%20and%20Saurabh%20Gupta%20and%20Meher%20V.%20R.%20Malladi%20and%20Luca%20Lobefaro%20and%20Guillaume%20Doisy%20and%20Cyrill%20Stachniss%0AAbstract%3A%20%20%20LiDAR%20odometry%20is%20essential%20for%20many%20robotics%20applications%2C%20including%203D%0Amapping%2C%20navigation%2C%20and%20simultaneous%20localization%20and%20mapping.%20LiDAR%20odometry%0Asystems%20are%20usually%20based%20on%20some%20form%20of%20point%20cloud%20registration%20to%20compute%0Athe%20ego-motion%20of%20a%20mobile%20robot.%20Yet%2C%20few%20of%20today%27s%20LiDAR%20odometry%20systems%0Aconsider%20domain-specific%20knowledge%20or%20the%20kinematic%20model%20of%20the%20mobile%0Aplatform%20during%20the%20point%20cloud%20alignment.%20In%20this%20paper%2C%20we%20present%0AKinematic-ICP%2C%20a%20LiDAR%20odometry%20system%20that%20focuses%20on%20wheeled%20mobile%20robots%0Aequipped%20with%20a%203D%20LiDAR%20and%20moving%20on%20a%20planar%20surface%2C%20which%20is%20a%20common%0Aassumption%20for%20warehouses%2C%20offices%2C%20hospitals%2C%20etc.%20Our%20approach%20introduces%0Akinematic%20constraints%20within%20the%20optimization%20of%20a%20traditional%20point-to-point%0Aiterative%20closest%20point%20scheme.%20In%20this%20way%2C%20the%20resulting%20motion%20follows%20the%0Akinematic%20constraints%20of%20the%20platform%2C%20effectively%20exploiting%20the%20robot%27s%20wheel%0Aodometry%20and%20the%203D%20LiDAR%20observations.%20We%20dynamically%20adjust%20the%20influence%20of%0ALiDAR%20measurements%20and%20wheel%20odometry%20in%20our%20optimization%20scheme%2C%20allowing%20the%0Asystem%20to%20handle%20degenerate%20scenarios%20such%20as%20feature-poor%20corridors.%20We%0Aevaluate%20our%20approach%20on%20robots%20operating%20in%20large-scale%20warehouse%0Aenvironments%2C%20but%20also%20outdoors.%20The%20experiments%20show%20that%20our%20approach%0Aachieves%20top%20performances%20and%20is%20more%20accurate%20than%20wheel%20odometry%20and%20common%0ALiDAR%20odometry%20systems.%20Kinematic-ICP%20has%20been%20recently%20deployed%20in%20the%20Dexory%0Afleet%20of%20robots%20operating%20in%20warehouses%20worldwide%20at%20their%20customers%27%20sites%2C%0Ashowing%20that%20our%20method%20can%20run%20in%20the%20real%20world%20alongside%20a%20complete%0Anavigation%20stack.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10277v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKinematic-ICP%253A%2520Enhancing%2520LiDAR%2520Odometry%2520with%2520Kinematic%2520Constraints%2520for%250A%2520%2520Wheeled%2520Mobile%2520Robots%2520Moving%2520on%2520Planar%2520Surfaces%26entry.906535625%3DTiziano%2520Guadagnino%2520and%2520Benedikt%2520Mersch%2520and%2520Ignacio%2520Vizzo%2520and%2520Saurabh%2520Gupta%2520and%2520Meher%2520V.%2520R.%2520Malladi%2520and%2520Luca%2520Lobefaro%2520and%2520Guillaume%2520Doisy%2520and%2520Cyrill%2520Stachniss%26entry.1292438233%3D%2520%2520LiDAR%2520odometry%2520is%2520essential%2520for%2520many%2520robotics%2520applications%252C%2520including%25203D%250Amapping%252C%2520navigation%252C%2520and%2520simultaneous%2520localization%2520and%2520mapping.%2520LiDAR%2520odometry%250Asystems%2520are%2520usually%2520based%2520on%2520some%2520form%2520of%2520point%2520cloud%2520registration%2520to%2520compute%250Athe%2520ego-motion%2520of%2520a%2520mobile%2520robot.%2520Yet%252C%2520few%2520of%2520today%2527s%2520LiDAR%2520odometry%2520systems%250Aconsider%2520domain-specific%2520knowledge%2520or%2520the%2520kinematic%2520model%2520of%2520the%2520mobile%250Aplatform%2520during%2520the%2520point%2520cloud%2520alignment.%2520In%2520this%2520paper%252C%2520we%2520present%250AKinematic-ICP%252C%2520a%2520LiDAR%2520odometry%2520system%2520that%2520focuses%2520on%2520wheeled%2520mobile%2520robots%250Aequipped%2520with%2520a%25203D%2520LiDAR%2520and%2520moving%2520on%2520a%2520planar%2520surface%252C%2520which%2520is%2520a%2520common%250Aassumption%2520for%2520warehouses%252C%2520offices%252C%2520hospitals%252C%2520etc.%2520Our%2520approach%2520introduces%250Akinematic%2520constraints%2520within%2520the%2520optimization%2520of%2520a%2520traditional%2520point-to-point%250Aiterative%2520closest%2520point%2520scheme.%2520In%2520this%2520way%252C%2520the%2520resulting%2520motion%2520follows%2520the%250Akinematic%2520constraints%2520of%2520the%2520platform%252C%2520effectively%2520exploiting%2520the%2520robot%2527s%2520wheel%250Aodometry%2520and%2520the%25203D%2520LiDAR%2520observations.%2520We%2520dynamically%2520adjust%2520the%2520influence%2520of%250ALiDAR%2520measurements%2520and%2520wheel%2520odometry%2520in%2520our%2520optimization%2520scheme%252C%2520allowing%2520the%250Asystem%2520to%2520handle%2520degenerate%2520scenarios%2520such%2520as%2520feature-poor%2520corridors.%2520We%250Aevaluate%2520our%2520approach%2520on%2520robots%2520operating%2520in%2520large-scale%2520warehouse%250Aenvironments%252C%2520but%2520also%2520outdoors.%2520The%2520experiments%2520show%2520that%2520our%2520approach%250Aachieves%2520top%2520performances%2520and%2520is%2520more%2520accurate%2520than%2520wheel%2520odometry%2520and%2520common%250ALiDAR%2520odometry%2520systems.%2520Kinematic-ICP%2520has%2520been%2520recently%2520deployed%2520in%2520the%2520Dexory%250Afleet%2520of%2520robots%2520operating%2520in%2520warehouses%2520worldwide%2520at%2520their%2520customers%2527%2520sites%252C%250Ashowing%2520that%2520our%2520method%2520can%2520run%2520in%2520the%2520real%2520world%2520alongside%2520a%2520complete%250Anavigation%2520stack.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10277v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kinematic-ICP%3A%20Enhancing%20LiDAR%20Odometry%20with%20Kinematic%20Constraints%20for%0A%20%20Wheeled%20Mobile%20Robots%20Moving%20on%20Planar%20Surfaces&entry.906535625=Tiziano%20Guadagnino%20and%20Benedikt%20Mersch%20and%20Ignacio%20Vizzo%20and%20Saurabh%20Gupta%20and%20Meher%20V.%20R.%20Malladi%20and%20Luca%20Lobefaro%20and%20Guillaume%20Doisy%20and%20Cyrill%20Stachniss&entry.1292438233=%20%20LiDAR%20odometry%20is%20essential%20for%20many%20robotics%20applications%2C%20including%203D%0Amapping%2C%20navigation%2C%20and%20simultaneous%20localization%20and%20mapping.%20LiDAR%20odometry%0Asystems%20are%20usually%20based%20on%20some%20form%20of%20point%20cloud%20registration%20to%20compute%0Athe%20ego-motion%20of%20a%20mobile%20robot.%20Yet%2C%20few%20of%20today%27s%20LiDAR%20odometry%20systems%0Aconsider%20domain-specific%20knowledge%20or%20the%20kinematic%20model%20of%20the%20mobile%0Aplatform%20during%20the%20point%20cloud%20alignment.%20In%20this%20paper%2C%20we%20present%0AKinematic-ICP%2C%20a%20LiDAR%20odometry%20system%20that%20focuses%20on%20wheeled%20mobile%20robots%0Aequipped%20with%20a%203D%20LiDAR%20and%20moving%20on%20a%20planar%20surface%2C%20which%20is%20a%20common%0Aassumption%20for%20warehouses%2C%20offices%2C%20hospitals%2C%20etc.%20Our%20approach%20introduces%0Akinematic%20constraints%20within%20the%20optimization%20of%20a%20traditional%20point-to-point%0Aiterative%20closest%20point%20scheme.%20In%20this%20way%2C%20the%20resulting%20motion%20follows%20the%0Akinematic%20constraints%20of%20the%20platform%2C%20effectively%20exploiting%20the%20robot%27s%20wheel%0Aodometry%20and%20the%203D%20LiDAR%20observations.%20We%20dynamically%20adjust%20the%20influence%20of%0ALiDAR%20measurements%20and%20wheel%20odometry%20in%20our%20optimization%20scheme%2C%20allowing%20the%0Asystem%20to%20handle%20degenerate%20scenarios%20such%20as%20feature-poor%20corridors.%20We%0Aevaluate%20our%20approach%20on%20robots%20operating%20in%20large-scale%20warehouse%0Aenvironments%2C%20but%20also%20outdoors.%20The%20experiments%20show%20that%20our%20approach%0Aachieves%20top%20performances%20and%20is%20more%20accurate%20than%20wheel%20odometry%20and%20common%0ALiDAR%20odometry%20systems.%20Kinematic-ICP%20has%20been%20recently%20deployed%20in%20the%20Dexory%0Afleet%20of%20robots%20operating%20in%20warehouses%20worldwide%20at%20their%20customers%27%20sites%2C%0Ashowing%20that%20our%20method%20can%20run%20in%20the%20real%20world%20alongside%20a%20complete%0Anavigation%20stack.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10277v2&entry.124074799=Read"},
{"title": "Goku: Flow Based Video Generative Foundation Models", "author": "Shoufa Chen and Chongjian Ge and Yuqi Zhang and Yida Zhang and Fengda Zhu and Hao Yang and Hongxiang Hao and Hui Wu and Zhichao Lai and Yifei Hu and Ting-Che Lin and Shilong Zhang and Fu Li and Chuan Li and Xing Wang and Yanghua Peng and Peize Sun and Ping Luo and Yi Jiang and Zehuan Yuan and Bingyue Peng and Xiaobing Liu", "abstract": "  This paper introduces Goku, a state-of-the-art family of joint\nimage-and-video generation models leveraging rectified flow Transformers to\nachieve industry-leading performance. We detail the foundational elements\nenabling high-quality visual generation, including the data curation pipeline,\nmodel architecture design, flow formulation, and advanced infrastructure for\nefficient and robust large-scale training. The Goku models demonstrate superior\nperformance in both qualitative and quantitative evaluations, setting new\nbenchmarks across major tasks. Specifically, Goku achieves 0.76 on GenEval and\n83.65 on DPG-Bench for text-to-image generation, and 84.85 on VBench for\ntext-to-video tasks. We believe that this work provides valuable insights and\npractical advancements for the research community in developing joint\nimage-and-video generation models.\n", "link": "http://arxiv.org/abs/2502.04896v1", "date": "2025-02-07", "relevancy": 2.3537, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6432}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5812}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Goku%3A%20Flow%20Based%20Video%20Generative%20Foundation%20Models&body=Title%3A%20Goku%3A%20Flow%20Based%20Video%20Generative%20Foundation%20Models%0AAuthor%3A%20Shoufa%20Chen%20and%20Chongjian%20Ge%20and%20Yuqi%20Zhang%20and%20Yida%20Zhang%20and%20Fengda%20Zhu%20and%20Hao%20Yang%20and%20Hongxiang%20Hao%20and%20Hui%20Wu%20and%20Zhichao%20Lai%20and%20Yifei%20Hu%20and%20Ting-Che%20Lin%20and%20Shilong%20Zhang%20and%20Fu%20Li%20and%20Chuan%20Li%20and%20Xing%20Wang%20and%20Yanghua%20Peng%20and%20Peize%20Sun%20and%20Ping%20Luo%20and%20Yi%20Jiang%20and%20Zehuan%20Yuan%20and%20Bingyue%20Peng%20and%20Xiaobing%20Liu%0AAbstract%3A%20%20%20This%20paper%20introduces%20Goku%2C%20a%20state-of-the-art%20family%20of%20joint%0Aimage-and-video%20generation%20models%20leveraging%20rectified%20flow%20Transformers%20to%0Aachieve%20industry-leading%20performance.%20We%20detail%20the%20foundational%20elements%0Aenabling%20high-quality%20visual%20generation%2C%20including%20the%20data%20curation%20pipeline%2C%0Amodel%20architecture%20design%2C%20flow%20formulation%2C%20and%20advanced%20infrastructure%20for%0Aefficient%20and%20robust%20large-scale%20training.%20The%20Goku%20models%20demonstrate%20superior%0Aperformance%20in%20both%20qualitative%20and%20quantitative%20evaluations%2C%20setting%20new%0Abenchmarks%20across%20major%20tasks.%20Specifically%2C%20Goku%20achieves%200.76%20on%20GenEval%20and%0A83.65%20on%20DPG-Bench%20for%20text-to-image%20generation%2C%20and%2084.85%20on%20VBench%20for%0Atext-to-video%20tasks.%20We%20believe%20that%20this%20work%20provides%20valuable%20insights%20and%0Apractical%20advancements%20for%20the%20research%20community%20in%20developing%20joint%0Aimage-and-video%20generation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04896v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGoku%253A%2520Flow%2520Based%2520Video%2520Generative%2520Foundation%2520Models%26entry.906535625%3DShoufa%2520Chen%2520and%2520Chongjian%2520Ge%2520and%2520Yuqi%2520Zhang%2520and%2520Yida%2520Zhang%2520and%2520Fengda%2520Zhu%2520and%2520Hao%2520Yang%2520and%2520Hongxiang%2520Hao%2520and%2520Hui%2520Wu%2520and%2520Zhichao%2520Lai%2520and%2520Yifei%2520Hu%2520and%2520Ting-Che%2520Lin%2520and%2520Shilong%2520Zhang%2520and%2520Fu%2520Li%2520and%2520Chuan%2520Li%2520and%2520Xing%2520Wang%2520and%2520Yanghua%2520Peng%2520and%2520Peize%2520Sun%2520and%2520Ping%2520Luo%2520and%2520Yi%2520Jiang%2520and%2520Zehuan%2520Yuan%2520and%2520Bingyue%2520Peng%2520and%2520Xiaobing%2520Liu%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520Goku%252C%2520a%2520state-of-the-art%2520family%2520of%2520joint%250Aimage-and-video%2520generation%2520models%2520leveraging%2520rectified%2520flow%2520Transformers%2520to%250Aachieve%2520industry-leading%2520performance.%2520We%2520detail%2520the%2520foundational%2520elements%250Aenabling%2520high-quality%2520visual%2520generation%252C%2520including%2520the%2520data%2520curation%2520pipeline%252C%250Amodel%2520architecture%2520design%252C%2520flow%2520formulation%252C%2520and%2520advanced%2520infrastructure%2520for%250Aefficient%2520and%2520robust%2520large-scale%2520training.%2520The%2520Goku%2520models%2520demonstrate%2520superior%250Aperformance%2520in%2520both%2520qualitative%2520and%2520quantitative%2520evaluations%252C%2520setting%2520new%250Abenchmarks%2520across%2520major%2520tasks.%2520Specifically%252C%2520Goku%2520achieves%25200.76%2520on%2520GenEval%2520and%250A83.65%2520on%2520DPG-Bench%2520for%2520text-to-image%2520generation%252C%2520and%252084.85%2520on%2520VBench%2520for%250Atext-to-video%2520tasks.%2520We%2520believe%2520that%2520this%2520work%2520provides%2520valuable%2520insights%2520and%250Apractical%2520advancements%2520for%2520the%2520research%2520community%2520in%2520developing%2520joint%250Aimage-and-video%2520generation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04896v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Goku%3A%20Flow%20Based%20Video%20Generative%20Foundation%20Models&entry.906535625=Shoufa%20Chen%20and%20Chongjian%20Ge%20and%20Yuqi%20Zhang%20and%20Yida%20Zhang%20and%20Fengda%20Zhu%20and%20Hao%20Yang%20and%20Hongxiang%20Hao%20and%20Hui%20Wu%20and%20Zhichao%20Lai%20and%20Yifei%20Hu%20and%20Ting-Che%20Lin%20and%20Shilong%20Zhang%20and%20Fu%20Li%20and%20Chuan%20Li%20and%20Xing%20Wang%20and%20Yanghua%20Peng%20and%20Peize%20Sun%20and%20Ping%20Luo%20and%20Yi%20Jiang%20and%20Zehuan%20Yuan%20and%20Bingyue%20Peng%20and%20Xiaobing%20Liu&entry.1292438233=%20%20This%20paper%20introduces%20Goku%2C%20a%20state-of-the-art%20family%20of%20joint%0Aimage-and-video%20generation%20models%20leveraging%20rectified%20flow%20Transformers%20to%0Aachieve%20industry-leading%20performance.%20We%20detail%20the%20foundational%20elements%0Aenabling%20high-quality%20visual%20generation%2C%20including%20the%20data%20curation%20pipeline%2C%0Amodel%20architecture%20design%2C%20flow%20formulation%2C%20and%20advanced%20infrastructure%20for%0Aefficient%20and%20robust%20large-scale%20training.%20The%20Goku%20models%20demonstrate%20superior%0Aperformance%20in%20both%20qualitative%20and%20quantitative%20evaluations%2C%20setting%20new%0Abenchmarks%20across%20major%20tasks.%20Specifically%2C%20Goku%20achieves%200.76%20on%20GenEval%20and%0A83.65%20on%20DPG-Bench%20for%20text-to-image%20generation%2C%20and%2084.85%20on%20VBench%20for%0Atext-to-video%20tasks.%20We%20believe%20that%20this%20work%20provides%20valuable%20insights%20and%0Apractical%20advancements%20for%20the%20research%20community%20in%20developing%20joint%0Aimage-and-video%20generation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04896v1&entry.124074799=Read"},
{"title": "Supervised Quadratic Feature Analysis: An Information Geometry Approach\n  to Dimensionality Reduction", "author": "Daniel Herrera-Esposito and Johannes Burge", "abstract": "  Supervised dimensionality reduction aims to map labeled data to a\nlow-dimensional feature space while maximizing class discriminability. Despite\nthe availability of methods for learning complex non-linear features (e.g. Deep\nLearning), there is an enduring demand for dimensionality reduction methods\nthat learn linear features due to their interpretability, low computational\ncost, and broad applicability. However, there is a gap between methods that\noptimize linear separability (e.g. LDA), and more flexible but computationally\nexpensive methods that optimize over arbitrary class boundaries (e.g.\nmetric-learning methods). Here, we present Supervised Quadratic Feature\nAnalysis (SQFA), a dimensionality reduction method for learning linear features\nthat maximize the differences between class-conditional first- and second-order\nstatistics, which allow for quadratic discrimination. SQFA exploits the\ninformation geometry of second-order statistics in the symmetric positive\ndefinite manifold. We show that SQFA features support quadratic\ndiscriminability in real-world problems. We also provide a theoretical link,\nbased on information geometry, between SQFA and the Quadratic Discriminant\nAnalysis (QDA) classifier.\n", "link": "http://arxiv.org/abs/2502.00168v2", "date": "2025-02-07", "relevancy": 2.3505, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4888}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.467}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Supervised%20Quadratic%20Feature%20Analysis%3A%20An%20Information%20Geometry%20Approach%0A%20%20to%20Dimensionality%20Reduction&body=Title%3A%20Supervised%20Quadratic%20Feature%20Analysis%3A%20An%20Information%20Geometry%20Approach%0A%20%20to%20Dimensionality%20Reduction%0AAuthor%3A%20Daniel%20Herrera-Esposito%20and%20Johannes%20Burge%0AAbstract%3A%20%20%20Supervised%20dimensionality%20reduction%20aims%20to%20map%20labeled%20data%20to%20a%0Alow-dimensional%20feature%20space%20while%20maximizing%20class%20discriminability.%20Despite%0Athe%20availability%20of%20methods%20for%20learning%20complex%20non-linear%20features%20%28e.g.%20Deep%0ALearning%29%2C%20there%20is%20an%20enduring%20demand%20for%20dimensionality%20reduction%20methods%0Athat%20learn%20linear%20features%20due%20to%20their%20interpretability%2C%20low%20computational%0Acost%2C%20and%20broad%20applicability.%20However%2C%20there%20is%20a%20gap%20between%20methods%20that%0Aoptimize%20linear%20separability%20%28e.g.%20LDA%29%2C%20and%20more%20flexible%20but%20computationally%0Aexpensive%20methods%20that%20optimize%20over%20arbitrary%20class%20boundaries%20%28e.g.%0Ametric-learning%20methods%29.%20Here%2C%20we%20present%20Supervised%20Quadratic%20Feature%0AAnalysis%20%28SQFA%29%2C%20a%20dimensionality%20reduction%20method%20for%20learning%20linear%20features%0Athat%20maximize%20the%20differences%20between%20class-conditional%20first-%20and%20second-order%0Astatistics%2C%20which%20allow%20for%20quadratic%20discrimination.%20SQFA%20exploits%20the%0Ainformation%20geometry%20of%20second-order%20statistics%20in%20the%20symmetric%20positive%0Adefinite%20manifold.%20We%20show%20that%20SQFA%20features%20support%20quadratic%0Adiscriminability%20in%20real-world%20problems.%20We%20also%20provide%20a%20theoretical%20link%2C%0Abased%20on%20information%20geometry%2C%20between%20SQFA%20and%20the%20Quadratic%20Discriminant%0AAnalysis%20%28QDA%29%20classifier.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00168v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSupervised%2520Quadratic%2520Feature%2520Analysis%253A%2520An%2520Information%2520Geometry%2520Approach%250A%2520%2520to%2520Dimensionality%2520Reduction%26entry.906535625%3DDaniel%2520Herrera-Esposito%2520and%2520Johannes%2520Burge%26entry.1292438233%3D%2520%2520Supervised%2520dimensionality%2520reduction%2520aims%2520to%2520map%2520labeled%2520data%2520to%2520a%250Alow-dimensional%2520feature%2520space%2520while%2520maximizing%2520class%2520discriminability.%2520Despite%250Athe%2520availability%2520of%2520methods%2520for%2520learning%2520complex%2520non-linear%2520features%2520%2528e.g.%2520Deep%250ALearning%2529%252C%2520there%2520is%2520an%2520enduring%2520demand%2520for%2520dimensionality%2520reduction%2520methods%250Athat%2520learn%2520linear%2520features%2520due%2520to%2520their%2520interpretability%252C%2520low%2520computational%250Acost%252C%2520and%2520broad%2520applicability.%2520However%252C%2520there%2520is%2520a%2520gap%2520between%2520methods%2520that%250Aoptimize%2520linear%2520separability%2520%2528e.g.%2520LDA%2529%252C%2520and%2520more%2520flexible%2520but%2520computationally%250Aexpensive%2520methods%2520that%2520optimize%2520over%2520arbitrary%2520class%2520boundaries%2520%2528e.g.%250Ametric-learning%2520methods%2529.%2520Here%252C%2520we%2520present%2520Supervised%2520Quadratic%2520Feature%250AAnalysis%2520%2528SQFA%2529%252C%2520a%2520dimensionality%2520reduction%2520method%2520for%2520learning%2520linear%2520features%250Athat%2520maximize%2520the%2520differences%2520between%2520class-conditional%2520first-%2520and%2520second-order%250Astatistics%252C%2520which%2520allow%2520for%2520quadratic%2520discrimination.%2520SQFA%2520exploits%2520the%250Ainformation%2520geometry%2520of%2520second-order%2520statistics%2520in%2520the%2520symmetric%2520positive%250Adefinite%2520manifold.%2520We%2520show%2520that%2520SQFA%2520features%2520support%2520quadratic%250Adiscriminability%2520in%2520real-world%2520problems.%2520We%2520also%2520provide%2520a%2520theoretical%2520link%252C%250Abased%2520on%2520information%2520geometry%252C%2520between%2520SQFA%2520and%2520the%2520Quadratic%2520Discriminant%250AAnalysis%2520%2528QDA%2529%2520classifier.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00168v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Supervised%20Quadratic%20Feature%20Analysis%3A%20An%20Information%20Geometry%20Approach%0A%20%20to%20Dimensionality%20Reduction&entry.906535625=Daniel%20Herrera-Esposito%20and%20Johannes%20Burge&entry.1292438233=%20%20Supervised%20dimensionality%20reduction%20aims%20to%20map%20labeled%20data%20to%20a%0Alow-dimensional%20feature%20space%20while%20maximizing%20class%20discriminability.%20Despite%0Athe%20availability%20of%20methods%20for%20learning%20complex%20non-linear%20features%20%28e.g.%20Deep%0ALearning%29%2C%20there%20is%20an%20enduring%20demand%20for%20dimensionality%20reduction%20methods%0Athat%20learn%20linear%20features%20due%20to%20their%20interpretability%2C%20low%20computational%0Acost%2C%20and%20broad%20applicability.%20However%2C%20there%20is%20a%20gap%20between%20methods%20that%0Aoptimize%20linear%20separability%20%28e.g.%20LDA%29%2C%20and%20more%20flexible%20but%20computationally%0Aexpensive%20methods%20that%20optimize%20over%20arbitrary%20class%20boundaries%20%28e.g.%0Ametric-learning%20methods%29.%20Here%2C%20we%20present%20Supervised%20Quadratic%20Feature%0AAnalysis%20%28SQFA%29%2C%20a%20dimensionality%20reduction%20method%20for%20learning%20linear%20features%0Athat%20maximize%20the%20differences%20between%20class-conditional%20first-%20and%20second-order%0Astatistics%2C%20which%20allow%20for%20quadratic%20discrimination.%20SQFA%20exploits%20the%0Ainformation%20geometry%20of%20second-order%20statistics%20in%20the%20symmetric%20positive%0Adefinite%20manifold.%20We%20show%20that%20SQFA%20features%20support%20quadratic%0Adiscriminability%20in%20real-world%20problems.%20We%20also%20provide%20a%20theoretical%20link%2C%0Abased%20on%20information%20geometry%2C%20between%20SQFA%20and%20the%20Quadratic%20Discriminant%0AAnalysis%20%28QDA%29%20classifier.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00168v2&entry.124074799=Read"},
{"title": "Intelligent Go-Explore: Standing on the Shoulders of Giant Foundation\n  Models", "author": "Cong Lu and Shengran Hu and Jeff Clune", "abstract": "  Go-Explore is a powerful family of algorithms designed to solve\nhard-exploration problems built on the principle of archiving discovered\nstates, and iteratively returning to and exploring from the most promising\nstates. This approach has led to superhuman performance across a wide variety\nof challenging problems including Atari games and robotic control, but requires\nmanually designing heuristics to guide exploration (i.e., determine which\nstates to save and explore from, and what actions to consider next), which is\ntime-consuming and infeasible in general. To resolve this, we propose\nIntelligent Go-Explore (IGE) which greatly extends the scope of the original\nGo-Explore by replacing these handcrafted heuristics with the intelligence and\ninternalized human notions of interestingness captured by giant pretrained\nfoundation models (FMs). This provides IGE with a human-like ability to\ninstinctively identify how interesting or promising any new state is (e.g.,\ndiscovering new objects, locations, or behaviors), even in complex environments\nwhere heuristics are hard to define. Moreover, IGE offers the exciting\nopportunity to recognize and capitalize on serendipitous discoveries -- states\nencountered during exploration that are valuable in terms of exploration, yet\nwhere what makes them interesting was not anticipated by the human user. We\nevaluate our algorithm on a diverse range of language and vision-based tasks\nthat require search and exploration. Across these tasks, IGE strongly exceeds\nclassic reinforcement learning and graph search baselines, and also succeeds\nwhere prior state-of-the-art FM agents like Reflexion completely fail. Overall,\nIntelligent Go-Explore combines the tremendous strengths of FMs and the\npowerful Go-Explore algorithm, opening up a new frontier of research into\ncreating more generally capable agents with impressive exploration\ncapabilities.\n", "link": "http://arxiv.org/abs/2405.15143v4", "date": "2025-02-07", "relevancy": 2.3247, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6345}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.598}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intelligent%20Go-Explore%3A%20Standing%20on%20the%20Shoulders%20of%20Giant%20Foundation%0A%20%20Models&body=Title%3A%20Intelligent%20Go-Explore%3A%20Standing%20on%20the%20Shoulders%20of%20Giant%20Foundation%0A%20%20Models%0AAuthor%3A%20Cong%20Lu%20and%20Shengran%20Hu%20and%20Jeff%20Clune%0AAbstract%3A%20%20%20Go-Explore%20is%20a%20powerful%20family%20of%20algorithms%20designed%20to%20solve%0Ahard-exploration%20problems%20built%20on%20the%20principle%20of%20archiving%20discovered%0Astates%2C%20and%20iteratively%20returning%20to%20and%20exploring%20from%20the%20most%20promising%0Astates.%20This%20approach%20has%20led%20to%20superhuman%20performance%20across%20a%20wide%20variety%0Aof%20challenging%20problems%20including%20Atari%20games%20and%20robotic%20control%2C%20but%20requires%0Amanually%20designing%20heuristics%20to%20guide%20exploration%20%28i.e.%2C%20determine%20which%0Astates%20to%20save%20and%20explore%20from%2C%20and%20what%20actions%20to%20consider%20next%29%2C%20which%20is%0Atime-consuming%20and%20infeasible%20in%20general.%20To%20resolve%20this%2C%20we%20propose%0AIntelligent%20Go-Explore%20%28IGE%29%20which%20greatly%20extends%20the%20scope%20of%20the%20original%0AGo-Explore%20by%20replacing%20these%20handcrafted%20heuristics%20with%20the%20intelligence%20and%0Ainternalized%20human%20notions%20of%20interestingness%20captured%20by%20giant%20pretrained%0Afoundation%20models%20%28FMs%29.%20This%20provides%20IGE%20with%20a%20human-like%20ability%20to%0Ainstinctively%20identify%20how%20interesting%20or%20promising%20any%20new%20state%20is%20%28e.g.%2C%0Adiscovering%20new%20objects%2C%20locations%2C%20or%20behaviors%29%2C%20even%20in%20complex%20environments%0Awhere%20heuristics%20are%20hard%20to%20define.%20Moreover%2C%20IGE%20offers%20the%20exciting%0Aopportunity%20to%20recognize%20and%20capitalize%20on%20serendipitous%20discoveries%20--%20states%0Aencountered%20during%20exploration%20that%20are%20valuable%20in%20terms%20of%20exploration%2C%20yet%0Awhere%20what%20makes%20them%20interesting%20was%20not%20anticipated%20by%20the%20human%20user.%20We%0Aevaluate%20our%20algorithm%20on%20a%20diverse%20range%20of%20language%20and%20vision-based%20tasks%0Athat%20require%20search%20and%20exploration.%20Across%20these%20tasks%2C%20IGE%20strongly%20exceeds%0Aclassic%20reinforcement%20learning%20and%20graph%20search%20baselines%2C%20and%20also%20succeeds%0Awhere%20prior%20state-of-the-art%20FM%20agents%20like%20Reflexion%20completely%20fail.%20Overall%2C%0AIntelligent%20Go-Explore%20combines%20the%20tremendous%20strengths%20of%20FMs%20and%20the%0Apowerful%20Go-Explore%20algorithm%2C%20opening%20up%20a%20new%20frontier%20of%20research%20into%0Acreating%20more%20generally%20capable%20agents%20with%20impressive%20exploration%0Acapabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15143v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntelligent%2520Go-Explore%253A%2520Standing%2520on%2520the%2520Shoulders%2520of%2520Giant%2520Foundation%250A%2520%2520Models%26entry.906535625%3DCong%2520Lu%2520and%2520Shengran%2520Hu%2520and%2520Jeff%2520Clune%26entry.1292438233%3D%2520%2520Go-Explore%2520is%2520a%2520powerful%2520family%2520of%2520algorithms%2520designed%2520to%2520solve%250Ahard-exploration%2520problems%2520built%2520on%2520the%2520principle%2520of%2520archiving%2520discovered%250Astates%252C%2520and%2520iteratively%2520returning%2520to%2520and%2520exploring%2520from%2520the%2520most%2520promising%250Astates.%2520This%2520approach%2520has%2520led%2520to%2520superhuman%2520performance%2520across%2520a%2520wide%2520variety%250Aof%2520challenging%2520problems%2520including%2520Atari%2520games%2520and%2520robotic%2520control%252C%2520but%2520requires%250Amanually%2520designing%2520heuristics%2520to%2520guide%2520exploration%2520%2528i.e.%252C%2520determine%2520which%250Astates%2520to%2520save%2520and%2520explore%2520from%252C%2520and%2520what%2520actions%2520to%2520consider%2520next%2529%252C%2520which%2520is%250Atime-consuming%2520and%2520infeasible%2520in%2520general.%2520To%2520resolve%2520this%252C%2520we%2520propose%250AIntelligent%2520Go-Explore%2520%2528IGE%2529%2520which%2520greatly%2520extends%2520the%2520scope%2520of%2520the%2520original%250AGo-Explore%2520by%2520replacing%2520these%2520handcrafted%2520heuristics%2520with%2520the%2520intelligence%2520and%250Ainternalized%2520human%2520notions%2520of%2520interestingness%2520captured%2520by%2520giant%2520pretrained%250Afoundation%2520models%2520%2528FMs%2529.%2520This%2520provides%2520IGE%2520with%2520a%2520human-like%2520ability%2520to%250Ainstinctively%2520identify%2520how%2520interesting%2520or%2520promising%2520any%2520new%2520state%2520is%2520%2528e.g.%252C%250Adiscovering%2520new%2520objects%252C%2520locations%252C%2520or%2520behaviors%2529%252C%2520even%2520in%2520complex%2520environments%250Awhere%2520heuristics%2520are%2520hard%2520to%2520define.%2520Moreover%252C%2520IGE%2520offers%2520the%2520exciting%250Aopportunity%2520to%2520recognize%2520and%2520capitalize%2520on%2520serendipitous%2520discoveries%2520--%2520states%250Aencountered%2520during%2520exploration%2520that%2520are%2520valuable%2520in%2520terms%2520of%2520exploration%252C%2520yet%250Awhere%2520what%2520makes%2520them%2520interesting%2520was%2520not%2520anticipated%2520by%2520the%2520human%2520user.%2520We%250Aevaluate%2520our%2520algorithm%2520on%2520a%2520diverse%2520range%2520of%2520language%2520and%2520vision-based%2520tasks%250Athat%2520require%2520search%2520and%2520exploration.%2520Across%2520these%2520tasks%252C%2520IGE%2520strongly%2520exceeds%250Aclassic%2520reinforcement%2520learning%2520and%2520graph%2520search%2520baselines%252C%2520and%2520also%2520succeeds%250Awhere%2520prior%2520state-of-the-art%2520FM%2520agents%2520like%2520Reflexion%2520completely%2520fail.%2520Overall%252C%250AIntelligent%2520Go-Explore%2520combines%2520the%2520tremendous%2520strengths%2520of%2520FMs%2520and%2520the%250Apowerful%2520Go-Explore%2520algorithm%252C%2520opening%2520up%2520a%2520new%2520frontier%2520of%2520research%2520into%250Acreating%2520more%2520generally%2520capable%2520agents%2520with%2520impressive%2520exploration%250Acapabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15143v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intelligent%20Go-Explore%3A%20Standing%20on%20the%20Shoulders%20of%20Giant%20Foundation%0A%20%20Models&entry.906535625=Cong%20Lu%20and%20Shengran%20Hu%20and%20Jeff%20Clune&entry.1292438233=%20%20Go-Explore%20is%20a%20powerful%20family%20of%20algorithms%20designed%20to%20solve%0Ahard-exploration%20problems%20built%20on%20the%20principle%20of%20archiving%20discovered%0Astates%2C%20and%20iteratively%20returning%20to%20and%20exploring%20from%20the%20most%20promising%0Astates.%20This%20approach%20has%20led%20to%20superhuman%20performance%20across%20a%20wide%20variety%0Aof%20challenging%20problems%20including%20Atari%20games%20and%20robotic%20control%2C%20but%20requires%0Amanually%20designing%20heuristics%20to%20guide%20exploration%20%28i.e.%2C%20determine%20which%0Astates%20to%20save%20and%20explore%20from%2C%20and%20what%20actions%20to%20consider%20next%29%2C%20which%20is%0Atime-consuming%20and%20infeasible%20in%20general.%20To%20resolve%20this%2C%20we%20propose%0AIntelligent%20Go-Explore%20%28IGE%29%20which%20greatly%20extends%20the%20scope%20of%20the%20original%0AGo-Explore%20by%20replacing%20these%20handcrafted%20heuristics%20with%20the%20intelligence%20and%0Ainternalized%20human%20notions%20of%20interestingness%20captured%20by%20giant%20pretrained%0Afoundation%20models%20%28FMs%29.%20This%20provides%20IGE%20with%20a%20human-like%20ability%20to%0Ainstinctively%20identify%20how%20interesting%20or%20promising%20any%20new%20state%20is%20%28e.g.%2C%0Adiscovering%20new%20objects%2C%20locations%2C%20or%20behaviors%29%2C%20even%20in%20complex%20environments%0Awhere%20heuristics%20are%20hard%20to%20define.%20Moreover%2C%20IGE%20offers%20the%20exciting%0Aopportunity%20to%20recognize%20and%20capitalize%20on%20serendipitous%20discoveries%20--%20states%0Aencountered%20during%20exploration%20that%20are%20valuable%20in%20terms%20of%20exploration%2C%20yet%0Awhere%20what%20makes%20them%20interesting%20was%20not%20anticipated%20by%20the%20human%20user.%20We%0Aevaluate%20our%20algorithm%20on%20a%20diverse%20range%20of%20language%20and%20vision-based%20tasks%0Athat%20require%20search%20and%20exploration.%20Across%20these%20tasks%2C%20IGE%20strongly%20exceeds%0Aclassic%20reinforcement%20learning%20and%20graph%20search%20baselines%2C%20and%20also%20succeeds%0Awhere%20prior%20state-of-the-art%20FM%20agents%20like%20Reflexion%20completely%20fail.%20Overall%2C%0AIntelligent%20Go-Explore%20combines%20the%20tremendous%20strengths%20of%20FMs%20and%20the%0Apowerful%20Go-Explore%20algorithm%2C%20opening%20up%20a%20new%20frontier%20of%20research%20into%0Acreating%20more%20generally%20capable%20agents%20with%20impressive%20exploration%0Acapabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15143v4&entry.124074799=Read"},
{"title": "Representation of Molecules via Algebraic Data Types : Advancing Beyond\n  SMILES & SELFIES", "author": "Oliver Goldstein and Samuel March", "abstract": "  We introduce a novel molecular representation through Algebraic Data Types\n(ADTs) - composite data structures formed through the combination of simpler\ntypes that obey algebraic laws. By explicitly considering how the datatype of a\nrepresentation constrains the operations which may be performed, we ensure\nmeaningful inference can be performed over generative models (programs with\nsample} and score operations). This stands in contrast to string-based\nrepresentations where string-type operations may only indirectly correspond to\nchemical and physical molecular properties, and at worst produce nonsensical\noutput. The ADT presented implements the Dietz representation for molecular\nconstitution via multigraphs and bonding systems, and uses atomic coordinate\ndata to represent 3D information and stereochemical features. This creates a\ngeneral digital molecular representation which surpasses the limitations of the\nstring-based representations and the 2D-graph based models on which they are\nbased. In addition, we present novel support for quantum information through\nrepresentation of shells, subshells, and orbitals, greatly expanding the\nrepresentational scope beyond current approaches, for instance in Molecular\nOrbital theory. The framework's capabilities are demonstrated through key\napplications: Bayesian probabilistic programming is demonstrated through\nintegration with LazyPPL, a lazy probabilistic programming library; molecules\nare made instances of a group under rotation, necessary for geometric learning\ntechniques which exploit the invariance of molecular properties under different\nrepresentations; and the framework's flexibility is demonstrated through an\nextension to model chemical reactions. After critiquing previous\nrepresentations, we provide an open-source solution in Haskell - a type-safe,\npurely functional programming language.\n", "link": "http://arxiv.org/abs/2501.13633v3", "date": "2025-02-07", "relevancy": 2.2955, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4689}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4627}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Representation%20of%20Molecules%20via%20Algebraic%20Data%20Types%20%3A%20Advancing%20Beyond%0A%20%20SMILES%20%26%20SELFIES&body=Title%3A%20Representation%20of%20Molecules%20via%20Algebraic%20Data%20Types%20%3A%20Advancing%20Beyond%0A%20%20SMILES%20%26%20SELFIES%0AAuthor%3A%20Oliver%20Goldstein%20and%20Samuel%20March%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20molecular%20representation%20through%20Algebraic%20Data%20Types%0A%28ADTs%29%20-%20composite%20data%20structures%20formed%20through%20the%20combination%20of%20simpler%0Atypes%20that%20obey%20algebraic%20laws.%20By%20explicitly%20considering%20how%20the%20datatype%20of%20a%0Arepresentation%20constrains%20the%20operations%20which%20may%20be%20performed%2C%20we%20ensure%0Ameaningful%20inference%20can%20be%20performed%20over%20generative%20models%20%28programs%20with%0Asample%7D%20and%20score%20operations%29.%20This%20stands%20in%20contrast%20to%20string-based%0Arepresentations%20where%20string-type%20operations%20may%20only%20indirectly%20correspond%20to%0Achemical%20and%20physical%20molecular%20properties%2C%20and%20at%20worst%20produce%20nonsensical%0Aoutput.%20The%20ADT%20presented%20implements%20the%20Dietz%20representation%20for%20molecular%0Aconstitution%20via%20multigraphs%20and%20bonding%20systems%2C%20and%20uses%20atomic%20coordinate%0Adata%20to%20represent%203D%20information%20and%20stereochemical%20features.%20This%20creates%20a%0Ageneral%20digital%20molecular%20representation%20which%20surpasses%20the%20limitations%20of%20the%0Astring-based%20representations%20and%20the%202D-graph%20based%20models%20on%20which%20they%20are%0Abased.%20In%20addition%2C%20we%20present%20novel%20support%20for%20quantum%20information%20through%0Arepresentation%20of%20shells%2C%20subshells%2C%20and%20orbitals%2C%20greatly%20expanding%20the%0Arepresentational%20scope%20beyond%20current%20approaches%2C%20for%20instance%20in%20Molecular%0AOrbital%20theory.%20The%20framework%27s%20capabilities%20are%20demonstrated%20through%20key%0Aapplications%3A%20Bayesian%20probabilistic%20programming%20is%20demonstrated%20through%0Aintegration%20with%20LazyPPL%2C%20a%20lazy%20probabilistic%20programming%20library%3B%20molecules%0Aare%20made%20instances%20of%20a%20group%20under%20rotation%2C%20necessary%20for%20geometric%20learning%0Atechniques%20which%20exploit%20the%20invariance%20of%20molecular%20properties%20under%20different%0Arepresentations%3B%20and%20the%20framework%27s%20flexibility%20is%20demonstrated%20through%20an%0Aextension%20to%20model%20chemical%20reactions.%20After%20critiquing%20previous%0Arepresentations%2C%20we%20provide%20an%20open-source%20solution%20in%20Haskell%20-%20a%20type-safe%2C%0Apurely%20functional%20programming%20language.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13633v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepresentation%2520of%2520Molecules%2520via%2520Algebraic%2520Data%2520Types%2520%253A%2520Advancing%2520Beyond%250A%2520%2520SMILES%2520%2526%2520SELFIES%26entry.906535625%3DOliver%2520Goldstein%2520and%2520Samuel%2520March%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520molecular%2520representation%2520through%2520Algebraic%2520Data%2520Types%250A%2528ADTs%2529%2520-%2520composite%2520data%2520structures%2520formed%2520through%2520the%2520combination%2520of%2520simpler%250Atypes%2520that%2520obey%2520algebraic%2520laws.%2520By%2520explicitly%2520considering%2520how%2520the%2520datatype%2520of%2520a%250Arepresentation%2520constrains%2520the%2520operations%2520which%2520may%2520be%2520performed%252C%2520we%2520ensure%250Ameaningful%2520inference%2520can%2520be%2520performed%2520over%2520generative%2520models%2520%2528programs%2520with%250Asample%257D%2520and%2520score%2520operations%2529.%2520This%2520stands%2520in%2520contrast%2520to%2520string-based%250Arepresentations%2520where%2520string-type%2520operations%2520may%2520only%2520indirectly%2520correspond%2520to%250Achemical%2520and%2520physical%2520molecular%2520properties%252C%2520and%2520at%2520worst%2520produce%2520nonsensical%250Aoutput.%2520The%2520ADT%2520presented%2520implements%2520the%2520Dietz%2520representation%2520for%2520molecular%250Aconstitution%2520via%2520multigraphs%2520and%2520bonding%2520systems%252C%2520and%2520uses%2520atomic%2520coordinate%250Adata%2520to%2520represent%25203D%2520information%2520and%2520stereochemical%2520features.%2520This%2520creates%2520a%250Ageneral%2520digital%2520molecular%2520representation%2520which%2520surpasses%2520the%2520limitations%2520of%2520the%250Astring-based%2520representations%2520and%2520the%25202D-graph%2520based%2520models%2520on%2520which%2520they%2520are%250Abased.%2520In%2520addition%252C%2520we%2520present%2520novel%2520support%2520for%2520quantum%2520information%2520through%250Arepresentation%2520of%2520shells%252C%2520subshells%252C%2520and%2520orbitals%252C%2520greatly%2520expanding%2520the%250Arepresentational%2520scope%2520beyond%2520current%2520approaches%252C%2520for%2520instance%2520in%2520Molecular%250AOrbital%2520theory.%2520The%2520framework%2527s%2520capabilities%2520are%2520demonstrated%2520through%2520key%250Aapplications%253A%2520Bayesian%2520probabilistic%2520programming%2520is%2520demonstrated%2520through%250Aintegration%2520with%2520LazyPPL%252C%2520a%2520lazy%2520probabilistic%2520programming%2520library%253B%2520molecules%250Aare%2520made%2520instances%2520of%2520a%2520group%2520under%2520rotation%252C%2520necessary%2520for%2520geometric%2520learning%250Atechniques%2520which%2520exploit%2520the%2520invariance%2520of%2520molecular%2520properties%2520under%2520different%250Arepresentations%253B%2520and%2520the%2520framework%2527s%2520flexibility%2520is%2520demonstrated%2520through%2520an%250Aextension%2520to%2520model%2520chemical%2520reactions.%2520After%2520critiquing%2520previous%250Arepresentations%252C%2520we%2520provide%2520an%2520open-source%2520solution%2520in%2520Haskell%2520-%2520a%2520type-safe%252C%250Apurely%2520functional%2520programming%2520language.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13633v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Representation%20of%20Molecules%20via%20Algebraic%20Data%20Types%20%3A%20Advancing%20Beyond%0A%20%20SMILES%20%26%20SELFIES&entry.906535625=Oliver%20Goldstein%20and%20Samuel%20March&entry.1292438233=%20%20We%20introduce%20a%20novel%20molecular%20representation%20through%20Algebraic%20Data%20Types%0A%28ADTs%29%20-%20composite%20data%20structures%20formed%20through%20the%20combination%20of%20simpler%0Atypes%20that%20obey%20algebraic%20laws.%20By%20explicitly%20considering%20how%20the%20datatype%20of%20a%0Arepresentation%20constrains%20the%20operations%20which%20may%20be%20performed%2C%20we%20ensure%0Ameaningful%20inference%20can%20be%20performed%20over%20generative%20models%20%28programs%20with%0Asample%7D%20and%20score%20operations%29.%20This%20stands%20in%20contrast%20to%20string-based%0Arepresentations%20where%20string-type%20operations%20may%20only%20indirectly%20correspond%20to%0Achemical%20and%20physical%20molecular%20properties%2C%20and%20at%20worst%20produce%20nonsensical%0Aoutput.%20The%20ADT%20presented%20implements%20the%20Dietz%20representation%20for%20molecular%0Aconstitution%20via%20multigraphs%20and%20bonding%20systems%2C%20and%20uses%20atomic%20coordinate%0Adata%20to%20represent%203D%20information%20and%20stereochemical%20features.%20This%20creates%20a%0Ageneral%20digital%20molecular%20representation%20which%20surpasses%20the%20limitations%20of%20the%0Astring-based%20representations%20and%20the%202D-graph%20based%20models%20on%20which%20they%20are%0Abased.%20In%20addition%2C%20we%20present%20novel%20support%20for%20quantum%20information%20through%0Arepresentation%20of%20shells%2C%20subshells%2C%20and%20orbitals%2C%20greatly%20expanding%20the%0Arepresentational%20scope%20beyond%20current%20approaches%2C%20for%20instance%20in%20Molecular%0AOrbital%20theory.%20The%20framework%27s%20capabilities%20are%20demonstrated%20through%20key%0Aapplications%3A%20Bayesian%20probabilistic%20programming%20is%20demonstrated%20through%0Aintegration%20with%20LazyPPL%2C%20a%20lazy%20probabilistic%20programming%20library%3B%20molecules%0Aare%20made%20instances%20of%20a%20group%20under%20rotation%2C%20necessary%20for%20geometric%20learning%0Atechniques%20which%20exploit%20the%20invariance%20of%20molecular%20properties%20under%20different%0Arepresentations%3B%20and%20the%20framework%27s%20flexibility%20is%20demonstrated%20through%20an%0Aextension%20to%20model%20chemical%20reactions.%20After%20critiquing%20previous%0Arepresentations%2C%20we%20provide%20an%20open-source%20solution%20in%20Haskell%20-%20a%20type-safe%2C%0Apurely%20functional%20programming%20language.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13633v3&entry.124074799=Read"},
{"title": "Tethering Broken Themes: Aligning Neural Topic Models with Labels and\n  Authors", "author": "Mayank Nagda and Phil Ostheimer and Sophie Fellenz", "abstract": "  Topic models are a popular approach for extracting semantic information from\nlarge document collections. However, recent studies suggest that the topics\ngenerated by these models often do not align well with human intentions.\nAlthough metadata such as labels and authorship information are available, it\nhas not yet been effectively incorporated into neural topic models. To address\nthis gap, we introduce FANToM, a novel method to align neural topic models with\nboth labels and authorship information. FANToM allows for the inclusion of this\nmetadata when available, producing interpretable topics and author\ndistributions for each topic. Our approach demonstrates greater expressiveness\nthan conventional topic models by learning the alignment between labels,\ntopics, and authors. Experimental results show that FANToM improves existing\nmodels in terms of both topic quality and alignment. Additionally, it\nidentifies author interests and similarities.\n", "link": "http://arxiv.org/abs/2410.18140v2", "date": "2025-02-07", "relevancy": 2.2883, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.489}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.443}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tethering%20Broken%20Themes%3A%20Aligning%20Neural%20Topic%20Models%20with%20Labels%20and%0A%20%20Authors&body=Title%3A%20Tethering%20Broken%20Themes%3A%20Aligning%20Neural%20Topic%20Models%20with%20Labels%20and%0A%20%20Authors%0AAuthor%3A%20Mayank%20Nagda%20and%20Phil%20Ostheimer%20and%20Sophie%20Fellenz%0AAbstract%3A%20%20%20Topic%20models%20are%20a%20popular%20approach%20for%20extracting%20semantic%20information%20from%0Alarge%20document%20collections.%20However%2C%20recent%20studies%20suggest%20that%20the%20topics%0Agenerated%20by%20these%20models%20often%20do%20not%20align%20well%20with%20human%20intentions.%0AAlthough%20metadata%20such%20as%20labels%20and%20authorship%20information%20are%20available%2C%20it%0Ahas%20not%20yet%20been%20effectively%20incorporated%20into%20neural%20topic%20models.%20To%20address%0Athis%20gap%2C%20we%20introduce%20FANToM%2C%20a%20novel%20method%20to%20align%20neural%20topic%20models%20with%0Aboth%20labels%20and%20authorship%20information.%20FANToM%20allows%20for%20the%20inclusion%20of%20this%0Ametadata%20when%20available%2C%20producing%20interpretable%20topics%20and%20author%0Adistributions%20for%20each%20topic.%20Our%20approach%20demonstrates%20greater%20expressiveness%0Athan%20conventional%20topic%20models%20by%20learning%20the%20alignment%20between%20labels%2C%0Atopics%2C%20and%20authors.%20Experimental%20results%20show%20that%20FANToM%20improves%20existing%0Amodels%20in%20terms%20of%20both%20topic%20quality%20and%20alignment.%20Additionally%2C%20it%0Aidentifies%20author%20interests%20and%20similarities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18140v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTethering%2520Broken%2520Themes%253A%2520Aligning%2520Neural%2520Topic%2520Models%2520with%2520Labels%2520and%250A%2520%2520Authors%26entry.906535625%3DMayank%2520Nagda%2520and%2520Phil%2520Ostheimer%2520and%2520Sophie%2520Fellenz%26entry.1292438233%3D%2520%2520Topic%2520models%2520are%2520a%2520popular%2520approach%2520for%2520extracting%2520semantic%2520information%2520from%250Alarge%2520document%2520collections.%2520However%252C%2520recent%2520studies%2520suggest%2520that%2520the%2520topics%250Agenerated%2520by%2520these%2520models%2520often%2520do%2520not%2520align%2520well%2520with%2520human%2520intentions.%250AAlthough%2520metadata%2520such%2520as%2520labels%2520and%2520authorship%2520information%2520are%2520available%252C%2520it%250Ahas%2520not%2520yet%2520been%2520effectively%2520incorporated%2520into%2520neural%2520topic%2520models.%2520To%2520address%250Athis%2520gap%252C%2520we%2520introduce%2520FANToM%252C%2520a%2520novel%2520method%2520to%2520align%2520neural%2520topic%2520models%2520with%250Aboth%2520labels%2520and%2520authorship%2520information.%2520FANToM%2520allows%2520for%2520the%2520inclusion%2520of%2520this%250Ametadata%2520when%2520available%252C%2520producing%2520interpretable%2520topics%2520and%2520author%250Adistributions%2520for%2520each%2520topic.%2520Our%2520approach%2520demonstrates%2520greater%2520expressiveness%250Athan%2520conventional%2520topic%2520models%2520by%2520learning%2520the%2520alignment%2520between%2520labels%252C%250Atopics%252C%2520and%2520authors.%2520Experimental%2520results%2520show%2520that%2520FANToM%2520improves%2520existing%250Amodels%2520in%2520terms%2520of%2520both%2520topic%2520quality%2520and%2520alignment.%2520Additionally%252C%2520it%250Aidentifies%2520author%2520interests%2520and%2520similarities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18140v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tethering%20Broken%20Themes%3A%20Aligning%20Neural%20Topic%20Models%20with%20Labels%20and%0A%20%20Authors&entry.906535625=Mayank%20Nagda%20and%20Phil%20Ostheimer%20and%20Sophie%20Fellenz&entry.1292438233=%20%20Topic%20models%20are%20a%20popular%20approach%20for%20extracting%20semantic%20information%20from%0Alarge%20document%20collections.%20However%2C%20recent%20studies%20suggest%20that%20the%20topics%0Agenerated%20by%20these%20models%20often%20do%20not%20align%20well%20with%20human%20intentions.%0AAlthough%20metadata%20such%20as%20labels%20and%20authorship%20information%20are%20available%2C%20it%0Ahas%20not%20yet%20been%20effectively%20incorporated%20into%20neural%20topic%20models.%20To%20address%0Athis%20gap%2C%20we%20introduce%20FANToM%2C%20a%20novel%20method%20to%20align%20neural%20topic%20models%20with%0Aboth%20labels%20and%20authorship%20information.%20FANToM%20allows%20for%20the%20inclusion%20of%20this%0Ametadata%20when%20available%2C%20producing%20interpretable%20topics%20and%20author%0Adistributions%20for%20each%20topic.%20Our%20approach%20demonstrates%20greater%20expressiveness%0Athan%20conventional%20topic%20models%20by%20learning%20the%20alignment%20between%20labels%2C%0Atopics%2C%20and%20authors.%20Experimental%20results%20show%20that%20FANToM%20improves%20existing%0Amodels%20in%20terms%20of%20both%20topic%20quality%20and%20alignment.%20Additionally%2C%20it%0Aidentifies%20author%20interests%20and%20similarities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18140v2&entry.124074799=Read"},
{"title": "DetVPCC: RoI-based Point Cloud Sequence Compression for 3D Object\n  Detection", "author": "Mingxuan Yan and Ruijie Zhang and Xuedou Xiao and Wei Wang", "abstract": "  While MPEG-standardized video-based point cloud compression (VPCC) achieves\nhigh compression efficiency for human perception, it struggles with a poor\ntrade-off between bitrate savings and detection accuracy when supporting 3D\nobject detectors. This limitation stems from VPCC's inability to prioritize\nregions of different importance within point clouds. To address this issue, we\npropose DetVPCC, a novel method integrating region-of-interest (RoI) encoding\nwith VPCC for efficient point cloud sequence compression while preserving the\n3D object detection accuracy. Specifically, we augment VPCC to support\nRoI-based compression by assigning spatially non-uniform quality levels. Then,\nwe introduce a lightweight RoI detector to identify crucial regions that\npotentially contain objects. Experiments on the nuScenes dataset demonstrate\nthat our approach significantly improves the detection accuracy. The code and\ndemo video are available in supplementary materials.\n", "link": "http://arxiv.org/abs/2502.04804v1", "date": "2025-02-07", "relevancy": 2.2814, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5811}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5682}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DetVPCC%3A%20RoI-based%20Point%20Cloud%20Sequence%20Compression%20for%203D%20Object%0A%20%20Detection&body=Title%3A%20DetVPCC%3A%20RoI-based%20Point%20Cloud%20Sequence%20Compression%20for%203D%20Object%0A%20%20Detection%0AAuthor%3A%20Mingxuan%20Yan%20and%20Ruijie%20Zhang%20and%20Xuedou%20Xiao%20and%20Wei%20Wang%0AAbstract%3A%20%20%20While%20MPEG-standardized%20video-based%20point%20cloud%20compression%20%28VPCC%29%20achieves%0Ahigh%20compression%20efficiency%20for%20human%20perception%2C%20it%20struggles%20with%20a%20poor%0Atrade-off%20between%20bitrate%20savings%20and%20detection%20accuracy%20when%20supporting%203D%0Aobject%20detectors.%20This%20limitation%20stems%20from%20VPCC%27s%20inability%20to%20prioritize%0Aregions%20of%20different%20importance%20within%20point%20clouds.%20To%20address%20this%20issue%2C%20we%0Apropose%20DetVPCC%2C%20a%20novel%20method%20integrating%20region-of-interest%20%28RoI%29%20encoding%0Awith%20VPCC%20for%20efficient%20point%20cloud%20sequence%20compression%20while%20preserving%20the%0A3D%20object%20detection%20accuracy.%20Specifically%2C%20we%20augment%20VPCC%20to%20support%0ARoI-based%20compression%20by%20assigning%20spatially%20non-uniform%20quality%20levels.%20Then%2C%0Awe%20introduce%20a%20lightweight%20RoI%20detector%20to%20identify%20crucial%20regions%20that%0Apotentially%20contain%20objects.%20Experiments%20on%20the%20nuScenes%20dataset%20demonstrate%0Athat%20our%20approach%20significantly%20improves%20the%20detection%20accuracy.%20The%20code%20and%0Ademo%20video%20are%20available%20in%20supplementary%20materials.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04804v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetVPCC%253A%2520RoI-based%2520Point%2520Cloud%2520Sequence%2520Compression%2520for%25203D%2520Object%250A%2520%2520Detection%26entry.906535625%3DMingxuan%2520Yan%2520and%2520Ruijie%2520Zhang%2520and%2520Xuedou%2520Xiao%2520and%2520Wei%2520Wang%26entry.1292438233%3D%2520%2520While%2520MPEG-standardized%2520video-based%2520point%2520cloud%2520compression%2520%2528VPCC%2529%2520achieves%250Ahigh%2520compression%2520efficiency%2520for%2520human%2520perception%252C%2520it%2520struggles%2520with%2520a%2520poor%250Atrade-off%2520between%2520bitrate%2520savings%2520and%2520detection%2520accuracy%2520when%2520supporting%25203D%250Aobject%2520detectors.%2520This%2520limitation%2520stems%2520from%2520VPCC%2527s%2520inability%2520to%2520prioritize%250Aregions%2520of%2520different%2520importance%2520within%2520point%2520clouds.%2520To%2520address%2520this%2520issue%252C%2520we%250Apropose%2520DetVPCC%252C%2520a%2520novel%2520method%2520integrating%2520region-of-interest%2520%2528RoI%2529%2520encoding%250Awith%2520VPCC%2520for%2520efficient%2520point%2520cloud%2520sequence%2520compression%2520while%2520preserving%2520the%250A3D%2520object%2520detection%2520accuracy.%2520Specifically%252C%2520we%2520augment%2520VPCC%2520to%2520support%250ARoI-based%2520compression%2520by%2520assigning%2520spatially%2520non-uniform%2520quality%2520levels.%2520Then%252C%250Awe%2520introduce%2520a%2520lightweight%2520RoI%2520detector%2520to%2520identify%2520crucial%2520regions%2520that%250Apotentially%2520contain%2520objects.%2520Experiments%2520on%2520the%2520nuScenes%2520dataset%2520demonstrate%250Athat%2520our%2520approach%2520significantly%2520improves%2520the%2520detection%2520accuracy.%2520The%2520code%2520and%250Ademo%2520video%2520are%2520available%2520in%2520supplementary%2520materials.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04804v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DetVPCC%3A%20RoI-based%20Point%20Cloud%20Sequence%20Compression%20for%203D%20Object%0A%20%20Detection&entry.906535625=Mingxuan%20Yan%20and%20Ruijie%20Zhang%20and%20Xuedou%20Xiao%20and%20Wei%20Wang&entry.1292438233=%20%20While%20MPEG-standardized%20video-based%20point%20cloud%20compression%20%28VPCC%29%20achieves%0Ahigh%20compression%20efficiency%20for%20human%20perception%2C%20it%20struggles%20with%20a%20poor%0Atrade-off%20between%20bitrate%20savings%20and%20detection%20accuracy%20when%20supporting%203D%0Aobject%20detectors.%20This%20limitation%20stems%20from%20VPCC%27s%20inability%20to%20prioritize%0Aregions%20of%20different%20importance%20within%20point%20clouds.%20To%20address%20this%20issue%2C%20we%0Apropose%20DetVPCC%2C%20a%20novel%20method%20integrating%20region-of-interest%20%28RoI%29%20encoding%0Awith%20VPCC%20for%20efficient%20point%20cloud%20sequence%20compression%20while%20preserving%20the%0A3D%20object%20detection%20accuracy.%20Specifically%2C%20we%20augment%20VPCC%20to%20support%0ARoI-based%20compression%20by%20assigning%20spatially%20non-uniform%20quality%20levels.%20Then%2C%0Awe%20introduce%20a%20lightweight%20RoI%20detector%20to%20identify%20crucial%20regions%20that%0Apotentially%20contain%20objects.%20Experiments%20on%20the%20nuScenes%20dataset%20demonstrate%0Athat%20our%20approach%20significantly%20improves%20the%20detection%20accuracy.%20The%20code%20and%0Ademo%20video%20are%20available%20in%20supplementary%20materials.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04804v1&entry.124074799=Read"},
{"title": "Exploit Gradient Skewness to Circumvent Byzantine Defenses for Federated\n  Learning", "author": "Yuchen Liu and Chen Chen and Lingjuan Lyu and Yaochu Jin and Gang Chen", "abstract": "  Federated Learning (FL) is notorious for its vulnerability to Byzantine\nattacks. Most current Byzantine defenses share a common inductive bias: among\nall the gradients, the densely distributed ones are more likely to be honest.\nHowever, such a bias is a poison to Byzantine robustness due to a newly\ndiscovered phenomenon in this paper - gradient skew. We discover that a group\nof densely distributed honest gradients skew away from the optimal gradient\n(the average of honest gradients) due to heterogeneous data. This gradient skew\nphenomenon allows Byzantine gradients to hide within the densely distributed\nskewed gradients. As a result, Byzantine defenses are confused into believing\nthat Byzantine gradients are honest. Motivated by this observation, we propose\na novel skew-aware attack called STRIKE: first, we search for the skewed\ngradients; then, we construct Byzantine gradients within the skewed gradients.\nExperiments on three benchmark datasets validate the effectiveness of our\nattack\n", "link": "http://arxiv.org/abs/2502.04890v1", "date": "2025-02-07", "relevancy": 2.2803, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.47}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4675}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploit%20Gradient%20Skewness%20to%20Circumvent%20Byzantine%20Defenses%20for%20Federated%0A%20%20Learning&body=Title%3A%20Exploit%20Gradient%20Skewness%20to%20Circumvent%20Byzantine%20Defenses%20for%20Federated%0A%20%20Learning%0AAuthor%3A%20Yuchen%20Liu%20and%20Chen%20Chen%20and%20Lingjuan%20Lyu%20and%20Yaochu%20Jin%20and%20Gang%20Chen%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20is%20notorious%20for%20its%20vulnerability%20to%20Byzantine%0Aattacks.%20Most%20current%20Byzantine%20defenses%20share%20a%20common%20inductive%20bias%3A%20among%0Aall%20the%20gradients%2C%20the%20densely%20distributed%20ones%20are%20more%20likely%20to%20be%20honest.%0AHowever%2C%20such%20a%20bias%20is%20a%20poison%20to%20Byzantine%20robustness%20due%20to%20a%20newly%0Adiscovered%20phenomenon%20in%20this%20paper%20-%20gradient%20skew.%20We%20discover%20that%20a%20group%0Aof%20densely%20distributed%20honest%20gradients%20skew%20away%20from%20the%20optimal%20gradient%0A%28the%20average%20of%20honest%20gradients%29%20due%20to%20heterogeneous%20data.%20This%20gradient%20skew%0Aphenomenon%20allows%20Byzantine%20gradients%20to%20hide%20within%20the%20densely%20distributed%0Askewed%20gradients.%20As%20a%20result%2C%20Byzantine%20defenses%20are%20confused%20into%20believing%0Athat%20Byzantine%20gradients%20are%20honest.%20Motivated%20by%20this%20observation%2C%20we%20propose%0Aa%20novel%20skew-aware%20attack%20called%20STRIKE%3A%20first%2C%20we%20search%20for%20the%20skewed%0Agradients%3B%20then%2C%20we%20construct%20Byzantine%20gradients%20within%20the%20skewed%20gradients.%0AExperiments%20on%20three%20benchmark%20datasets%20validate%20the%20effectiveness%20of%20our%0Aattack%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04890v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploit%2520Gradient%2520Skewness%2520to%2520Circumvent%2520Byzantine%2520Defenses%2520for%2520Federated%250A%2520%2520Learning%26entry.906535625%3DYuchen%2520Liu%2520and%2520Chen%2520Chen%2520and%2520Lingjuan%2520Lyu%2520and%2520Yaochu%2520Jin%2520and%2520Gang%2520Chen%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520notorious%2520for%2520its%2520vulnerability%2520to%2520Byzantine%250Aattacks.%2520Most%2520current%2520Byzantine%2520defenses%2520share%2520a%2520common%2520inductive%2520bias%253A%2520among%250Aall%2520the%2520gradients%252C%2520the%2520densely%2520distributed%2520ones%2520are%2520more%2520likely%2520to%2520be%2520honest.%250AHowever%252C%2520such%2520a%2520bias%2520is%2520a%2520poison%2520to%2520Byzantine%2520robustness%2520due%2520to%2520a%2520newly%250Adiscovered%2520phenomenon%2520in%2520this%2520paper%2520-%2520gradient%2520skew.%2520We%2520discover%2520that%2520a%2520group%250Aof%2520densely%2520distributed%2520honest%2520gradients%2520skew%2520away%2520from%2520the%2520optimal%2520gradient%250A%2528the%2520average%2520of%2520honest%2520gradients%2529%2520due%2520to%2520heterogeneous%2520data.%2520This%2520gradient%2520skew%250Aphenomenon%2520allows%2520Byzantine%2520gradients%2520to%2520hide%2520within%2520the%2520densely%2520distributed%250Askewed%2520gradients.%2520As%2520a%2520result%252C%2520Byzantine%2520defenses%2520are%2520confused%2520into%2520believing%250Athat%2520Byzantine%2520gradients%2520are%2520honest.%2520Motivated%2520by%2520this%2520observation%252C%2520we%2520propose%250Aa%2520novel%2520skew-aware%2520attack%2520called%2520STRIKE%253A%2520first%252C%2520we%2520search%2520for%2520the%2520skewed%250Agradients%253B%2520then%252C%2520we%2520construct%2520Byzantine%2520gradients%2520within%2520the%2520skewed%2520gradients.%250AExperiments%2520on%2520three%2520benchmark%2520datasets%2520validate%2520the%2520effectiveness%2520of%2520our%250Aattack%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04890v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploit%20Gradient%20Skewness%20to%20Circumvent%20Byzantine%20Defenses%20for%20Federated%0A%20%20Learning&entry.906535625=Yuchen%20Liu%20and%20Chen%20Chen%20and%20Lingjuan%20Lyu%20and%20Yaochu%20Jin%20and%20Gang%20Chen&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20is%20notorious%20for%20its%20vulnerability%20to%20Byzantine%0Aattacks.%20Most%20current%20Byzantine%20defenses%20share%20a%20common%20inductive%20bias%3A%20among%0Aall%20the%20gradients%2C%20the%20densely%20distributed%20ones%20are%20more%20likely%20to%20be%20honest.%0AHowever%2C%20such%20a%20bias%20is%20a%20poison%20to%20Byzantine%20robustness%20due%20to%20a%20newly%0Adiscovered%20phenomenon%20in%20this%20paper%20-%20gradient%20skew.%20We%20discover%20that%20a%20group%0Aof%20densely%20distributed%20honest%20gradients%20skew%20away%20from%20the%20optimal%20gradient%0A%28the%20average%20of%20honest%20gradients%29%20due%20to%20heterogeneous%20data.%20This%20gradient%20skew%0Aphenomenon%20allows%20Byzantine%20gradients%20to%20hide%20within%20the%20densely%20distributed%0Askewed%20gradients.%20As%20a%20result%2C%20Byzantine%20defenses%20are%20confused%20into%20believing%0Athat%20Byzantine%20gradients%20are%20honest.%20Motivated%20by%20this%20observation%2C%20we%20propose%0Aa%20novel%20skew-aware%20attack%20called%20STRIKE%3A%20first%2C%20we%20search%20for%20the%20skewed%0Agradients%3B%20then%2C%20we%20construct%20Byzantine%20gradients%20within%20the%20skewed%20gradients.%0AExperiments%20on%20three%20benchmark%20datasets%20validate%20the%20effectiveness%20of%20our%0Aattack%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04890v1&entry.124074799=Read"},
{"title": "Long-tailed Medical Diagnosis with Relation-aware Representation\n  Learning and Iterative Classifier Calibration", "author": "Li Pan and Yupei Zhang and Qiushi Yang and Tan Li and Zhen Chen", "abstract": "  Recently computer-aided diagnosis has demonstrated promising performance,\neffectively alleviating the workload of clinicians. However, the inherent\nsample imbalance among different diseases leads algorithms biased to the\nmajority categories, leading to poor performance for rare categories. Existing\nworks formulated this challenge as a long-tailed problem and attempted to\ntackle it by decoupling the feature representation and classification. Yet, due\nto the imbalanced distribution and limited samples from tail classes, these\nworks are prone to biased representation learning and insufficient classifier\ncalibration. To tackle these problems, we propose a new Long-tailed Medical\nDiagnosis (LMD) framework for balanced medical image classification on\nlong-tailed datasets. In the initial stage, we develop a Relation-aware\nRepresentation Learning (RRL) scheme to boost the representation ability by\nencouraging the encoder to capture intrinsic semantic features through\ndifferent data augmentations. In the subsequent stage, we propose an Iterative\nClassifier Calibration (ICC) scheme to calibrate the classifier iteratively.\nThis is achieved by generating a large number of balanced virtual features and\nfine-tuning the encoder using an Expectation-Maximization manner. The proposed\nICC compensates for minority categories to facilitate unbiased classifier\noptimization while maintaining the diagnostic knowledge in majority classes.\nComprehensive experiments on three public long-tailed medical datasets\ndemonstrate that our LMD framework significantly surpasses state-of-the-art\napproaches. The source code can be accessed at\nhttps://github.com/peterlipan/LMD.\n", "link": "http://arxiv.org/abs/2502.03238v2", "date": "2025-02-07", "relevancy": 2.2775, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5923}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5623}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long-tailed%20Medical%20Diagnosis%20with%20Relation-aware%20Representation%0A%20%20Learning%20and%20Iterative%20Classifier%20Calibration&body=Title%3A%20Long-tailed%20Medical%20Diagnosis%20with%20Relation-aware%20Representation%0A%20%20Learning%20and%20Iterative%20Classifier%20Calibration%0AAuthor%3A%20Li%20Pan%20and%20Yupei%20Zhang%20and%20Qiushi%20Yang%20and%20Tan%20Li%20and%20Zhen%20Chen%0AAbstract%3A%20%20%20Recently%20computer-aided%20diagnosis%20has%20demonstrated%20promising%20performance%2C%0Aeffectively%20alleviating%20the%20workload%20of%20clinicians.%20However%2C%20the%20inherent%0Asample%20imbalance%20among%20different%20diseases%20leads%20algorithms%20biased%20to%20the%0Amajority%20categories%2C%20leading%20to%20poor%20performance%20for%20rare%20categories.%20Existing%0Aworks%20formulated%20this%20challenge%20as%20a%20long-tailed%20problem%20and%20attempted%20to%0Atackle%20it%20by%20decoupling%20the%20feature%20representation%20and%20classification.%20Yet%2C%20due%0Ato%20the%20imbalanced%20distribution%20and%20limited%20samples%20from%20tail%20classes%2C%20these%0Aworks%20are%20prone%20to%20biased%20representation%20learning%20and%20insufficient%20classifier%0Acalibration.%20To%20tackle%20these%20problems%2C%20we%20propose%20a%20new%20Long-tailed%20Medical%0ADiagnosis%20%28LMD%29%20framework%20for%20balanced%20medical%20image%20classification%20on%0Along-tailed%20datasets.%20In%20the%20initial%20stage%2C%20we%20develop%20a%20Relation-aware%0ARepresentation%20Learning%20%28RRL%29%20scheme%20to%20boost%20the%20representation%20ability%20by%0Aencouraging%20the%20encoder%20to%20capture%20intrinsic%20semantic%20features%20through%0Adifferent%20data%20augmentations.%20In%20the%20subsequent%20stage%2C%20we%20propose%20an%20Iterative%0AClassifier%20Calibration%20%28ICC%29%20scheme%20to%20calibrate%20the%20classifier%20iteratively.%0AThis%20is%20achieved%20by%20generating%20a%20large%20number%20of%20balanced%20virtual%20features%20and%0Afine-tuning%20the%20encoder%20using%20an%20Expectation-Maximization%20manner.%20The%20proposed%0AICC%20compensates%20for%20minority%20categories%20to%20facilitate%20unbiased%20classifier%0Aoptimization%20while%20maintaining%20the%20diagnostic%20knowledge%20in%20majority%20classes.%0AComprehensive%20experiments%20on%20three%20public%20long-tailed%20medical%20datasets%0Ademonstrate%20that%20our%20LMD%20framework%20significantly%20surpasses%20state-of-the-art%0Aapproaches.%20The%20source%20code%20can%20be%20accessed%20at%0Ahttps%3A//github.com/peterlipan/LMD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03238v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong-tailed%2520Medical%2520Diagnosis%2520with%2520Relation-aware%2520Representation%250A%2520%2520Learning%2520and%2520Iterative%2520Classifier%2520Calibration%26entry.906535625%3DLi%2520Pan%2520and%2520Yupei%2520Zhang%2520and%2520Qiushi%2520Yang%2520and%2520Tan%2520Li%2520and%2520Zhen%2520Chen%26entry.1292438233%3D%2520%2520Recently%2520computer-aided%2520diagnosis%2520has%2520demonstrated%2520promising%2520performance%252C%250Aeffectively%2520alleviating%2520the%2520workload%2520of%2520clinicians.%2520However%252C%2520the%2520inherent%250Asample%2520imbalance%2520among%2520different%2520diseases%2520leads%2520algorithms%2520biased%2520to%2520the%250Amajority%2520categories%252C%2520leading%2520to%2520poor%2520performance%2520for%2520rare%2520categories.%2520Existing%250Aworks%2520formulated%2520this%2520challenge%2520as%2520a%2520long-tailed%2520problem%2520and%2520attempted%2520to%250Atackle%2520it%2520by%2520decoupling%2520the%2520feature%2520representation%2520and%2520classification.%2520Yet%252C%2520due%250Ato%2520the%2520imbalanced%2520distribution%2520and%2520limited%2520samples%2520from%2520tail%2520classes%252C%2520these%250Aworks%2520are%2520prone%2520to%2520biased%2520representation%2520learning%2520and%2520insufficient%2520classifier%250Acalibration.%2520To%2520tackle%2520these%2520problems%252C%2520we%2520propose%2520a%2520new%2520Long-tailed%2520Medical%250ADiagnosis%2520%2528LMD%2529%2520framework%2520for%2520balanced%2520medical%2520image%2520classification%2520on%250Along-tailed%2520datasets.%2520In%2520the%2520initial%2520stage%252C%2520we%2520develop%2520a%2520Relation-aware%250ARepresentation%2520Learning%2520%2528RRL%2529%2520scheme%2520to%2520boost%2520the%2520representation%2520ability%2520by%250Aencouraging%2520the%2520encoder%2520to%2520capture%2520intrinsic%2520semantic%2520features%2520through%250Adifferent%2520data%2520augmentations.%2520In%2520the%2520subsequent%2520stage%252C%2520we%2520propose%2520an%2520Iterative%250AClassifier%2520Calibration%2520%2528ICC%2529%2520scheme%2520to%2520calibrate%2520the%2520classifier%2520iteratively.%250AThis%2520is%2520achieved%2520by%2520generating%2520a%2520large%2520number%2520of%2520balanced%2520virtual%2520features%2520and%250Afine-tuning%2520the%2520encoder%2520using%2520an%2520Expectation-Maximization%2520manner.%2520The%2520proposed%250AICC%2520compensates%2520for%2520minority%2520categories%2520to%2520facilitate%2520unbiased%2520classifier%250Aoptimization%2520while%2520maintaining%2520the%2520diagnostic%2520knowledge%2520in%2520majority%2520classes.%250AComprehensive%2520experiments%2520on%2520three%2520public%2520long-tailed%2520medical%2520datasets%250Ademonstrate%2520that%2520our%2520LMD%2520framework%2520significantly%2520surpasses%2520state-of-the-art%250Aapproaches.%2520The%2520source%2520code%2520can%2520be%2520accessed%2520at%250Ahttps%253A//github.com/peterlipan/LMD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03238v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-tailed%20Medical%20Diagnosis%20with%20Relation-aware%20Representation%0A%20%20Learning%20and%20Iterative%20Classifier%20Calibration&entry.906535625=Li%20Pan%20and%20Yupei%20Zhang%20and%20Qiushi%20Yang%20and%20Tan%20Li%20and%20Zhen%20Chen&entry.1292438233=%20%20Recently%20computer-aided%20diagnosis%20has%20demonstrated%20promising%20performance%2C%0Aeffectively%20alleviating%20the%20workload%20of%20clinicians.%20However%2C%20the%20inherent%0Asample%20imbalance%20among%20different%20diseases%20leads%20algorithms%20biased%20to%20the%0Amajority%20categories%2C%20leading%20to%20poor%20performance%20for%20rare%20categories.%20Existing%0Aworks%20formulated%20this%20challenge%20as%20a%20long-tailed%20problem%20and%20attempted%20to%0Atackle%20it%20by%20decoupling%20the%20feature%20representation%20and%20classification.%20Yet%2C%20due%0Ato%20the%20imbalanced%20distribution%20and%20limited%20samples%20from%20tail%20classes%2C%20these%0Aworks%20are%20prone%20to%20biased%20representation%20learning%20and%20insufficient%20classifier%0Acalibration.%20To%20tackle%20these%20problems%2C%20we%20propose%20a%20new%20Long-tailed%20Medical%0ADiagnosis%20%28LMD%29%20framework%20for%20balanced%20medical%20image%20classification%20on%0Along-tailed%20datasets.%20In%20the%20initial%20stage%2C%20we%20develop%20a%20Relation-aware%0ARepresentation%20Learning%20%28RRL%29%20scheme%20to%20boost%20the%20representation%20ability%20by%0Aencouraging%20the%20encoder%20to%20capture%20intrinsic%20semantic%20features%20through%0Adifferent%20data%20augmentations.%20In%20the%20subsequent%20stage%2C%20we%20propose%20an%20Iterative%0AClassifier%20Calibration%20%28ICC%29%20scheme%20to%20calibrate%20the%20classifier%20iteratively.%0AThis%20is%20achieved%20by%20generating%20a%20large%20number%20of%20balanced%20virtual%20features%20and%0Afine-tuning%20the%20encoder%20using%20an%20Expectation-Maximization%20manner.%20The%20proposed%0AICC%20compensates%20for%20minority%20categories%20to%20facilitate%20unbiased%20classifier%0Aoptimization%20while%20maintaining%20the%20diagnostic%20knowledge%20in%20majority%20classes.%0AComprehensive%20experiments%20on%20three%20public%20long-tailed%20medical%20datasets%0Ademonstrate%20that%20our%20LMD%20framework%20significantly%20surpasses%20state-of-the-art%0Aapproaches.%20The%20source%20code%20can%20be%20accessed%20at%0Ahttps%3A//github.com/peterlipan/LMD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03238v2&entry.124074799=Read"},
{"title": "Bridging Voting and Deliberation with Algorithms: Field Insights from\n  vTaiwan and Kultur Komitee", "author": "Joshua C. Yang and Fynn Bachmann", "abstract": "  Democratic processes increasingly aim to integrate large-scale voting with\nface-to-face deliberation, addressing the challenge of reconciling individual\npreferences with collective decision-making. This work introduces new methods\nthat use algorithms and computational tools to bridge online voting with\nface-to-face deliberation, tested in two real-world scenarios: Kultur Komitee\n2024 (KK24) and vTaiwan. These case studies highlight the practical\napplications and impacts of the proposed methods.\n  We present three key contributions: (1) Radial Clustering for Preference\nBased Subgroups, which enables both in-depth and broad discussions in\ndeliberative settings by computing homogeneous and heterogeneous group\ncompositions with balanced and adjustable group sizes; (2) Human-in-the-loop\nMES, a practical method that enhances the Method of Equal Shares (MES)\nalgorithm with real-time digital feedback. This builds algorithmic trust by\ngiving participants full control over how much decision-making is delegated to\nthe voting aggregation algorithm as compared to deliberation; and (3) the\nReadTheRoom deliberation method, which uses opinion space mapping to identify\nagreement and divergence, along with spectrum-based preference visualisation to\ntrack opinion shifts during deliberation. This approach enhances transparency\nby clarifying collective sentiment and fosters collaboration by encouraging\nparticipants to engage constructively with differing perspectives.\n  By introducing these actionable frameworks, this research extends in-person\ndeliberation with scalable digital methods that address the complexities of\nmodern decision-making in participatory processes.\n", "link": "http://arxiv.org/abs/2502.05017v1", "date": "2025-02-07", "relevancy": 2.2756, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4567}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4543}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Voting%20and%20Deliberation%20with%20Algorithms%3A%20Field%20Insights%20from%0A%20%20vTaiwan%20and%20Kultur%20Komitee&body=Title%3A%20Bridging%20Voting%20and%20Deliberation%20with%20Algorithms%3A%20Field%20Insights%20from%0A%20%20vTaiwan%20and%20Kultur%20Komitee%0AAuthor%3A%20Joshua%20C.%20Yang%20and%20Fynn%20Bachmann%0AAbstract%3A%20%20%20Democratic%20processes%20increasingly%20aim%20to%20integrate%20large-scale%20voting%20with%0Aface-to-face%20deliberation%2C%20addressing%20the%20challenge%20of%20reconciling%20individual%0Apreferences%20with%20collective%20decision-making.%20This%20work%20introduces%20new%20methods%0Athat%20use%20algorithms%20and%20computational%20tools%20to%20bridge%20online%20voting%20with%0Aface-to-face%20deliberation%2C%20tested%20in%20two%20real-world%20scenarios%3A%20Kultur%20Komitee%0A2024%20%28KK24%29%20and%20vTaiwan.%20These%20case%20studies%20highlight%20the%20practical%0Aapplications%20and%20impacts%20of%20the%20proposed%20methods.%0A%20%20We%20present%20three%20key%20contributions%3A%20%281%29%20Radial%20Clustering%20for%20Preference%0ABased%20Subgroups%2C%20which%20enables%20both%20in-depth%20and%20broad%20discussions%20in%0Adeliberative%20settings%20by%20computing%20homogeneous%20and%20heterogeneous%20group%0Acompositions%20with%20balanced%20and%20adjustable%20group%20sizes%3B%20%282%29%20Human-in-the-loop%0AMES%2C%20a%20practical%20method%20that%20enhances%20the%20Method%20of%20Equal%20Shares%20%28MES%29%0Aalgorithm%20with%20real-time%20digital%20feedback.%20This%20builds%20algorithmic%20trust%20by%0Agiving%20participants%20full%20control%20over%20how%20much%20decision-making%20is%20delegated%20to%0Athe%20voting%20aggregation%20algorithm%20as%20compared%20to%20deliberation%3B%20and%20%283%29%20the%0AReadTheRoom%20deliberation%20method%2C%20which%20uses%20opinion%20space%20mapping%20to%20identify%0Aagreement%20and%20divergence%2C%20along%20with%20spectrum-based%20preference%20visualisation%20to%0Atrack%20opinion%20shifts%20during%20deliberation.%20This%20approach%20enhances%20transparency%0Aby%20clarifying%20collective%20sentiment%20and%20fosters%20collaboration%20by%20encouraging%0Aparticipants%20to%20engage%20constructively%20with%20differing%20perspectives.%0A%20%20By%20introducing%20these%20actionable%20frameworks%2C%20this%20research%20extends%20in-person%0Adeliberation%20with%20scalable%20digital%20methods%20that%20address%20the%20complexities%20of%0Amodern%20decision-making%20in%20participatory%20processes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05017v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Voting%2520and%2520Deliberation%2520with%2520Algorithms%253A%2520Field%2520Insights%2520from%250A%2520%2520vTaiwan%2520and%2520Kultur%2520Komitee%26entry.906535625%3DJoshua%2520C.%2520Yang%2520and%2520Fynn%2520Bachmann%26entry.1292438233%3D%2520%2520Democratic%2520processes%2520increasingly%2520aim%2520to%2520integrate%2520large-scale%2520voting%2520with%250Aface-to-face%2520deliberation%252C%2520addressing%2520the%2520challenge%2520of%2520reconciling%2520individual%250Apreferences%2520with%2520collective%2520decision-making.%2520This%2520work%2520introduces%2520new%2520methods%250Athat%2520use%2520algorithms%2520and%2520computational%2520tools%2520to%2520bridge%2520online%2520voting%2520with%250Aface-to-face%2520deliberation%252C%2520tested%2520in%2520two%2520real-world%2520scenarios%253A%2520Kultur%2520Komitee%250A2024%2520%2528KK24%2529%2520and%2520vTaiwan.%2520These%2520case%2520studies%2520highlight%2520the%2520practical%250Aapplications%2520and%2520impacts%2520of%2520the%2520proposed%2520methods.%250A%2520%2520We%2520present%2520three%2520key%2520contributions%253A%2520%25281%2529%2520Radial%2520Clustering%2520for%2520Preference%250ABased%2520Subgroups%252C%2520which%2520enables%2520both%2520in-depth%2520and%2520broad%2520discussions%2520in%250Adeliberative%2520settings%2520by%2520computing%2520homogeneous%2520and%2520heterogeneous%2520group%250Acompositions%2520with%2520balanced%2520and%2520adjustable%2520group%2520sizes%253B%2520%25282%2529%2520Human-in-the-loop%250AMES%252C%2520a%2520practical%2520method%2520that%2520enhances%2520the%2520Method%2520of%2520Equal%2520Shares%2520%2528MES%2529%250Aalgorithm%2520with%2520real-time%2520digital%2520feedback.%2520This%2520builds%2520algorithmic%2520trust%2520by%250Agiving%2520participants%2520full%2520control%2520over%2520how%2520much%2520decision-making%2520is%2520delegated%2520to%250Athe%2520voting%2520aggregation%2520algorithm%2520as%2520compared%2520to%2520deliberation%253B%2520and%2520%25283%2529%2520the%250AReadTheRoom%2520deliberation%2520method%252C%2520which%2520uses%2520opinion%2520space%2520mapping%2520to%2520identify%250Aagreement%2520and%2520divergence%252C%2520along%2520with%2520spectrum-based%2520preference%2520visualisation%2520to%250Atrack%2520opinion%2520shifts%2520during%2520deliberation.%2520This%2520approach%2520enhances%2520transparency%250Aby%2520clarifying%2520collective%2520sentiment%2520and%2520fosters%2520collaboration%2520by%2520encouraging%250Aparticipants%2520to%2520engage%2520constructively%2520with%2520differing%2520perspectives.%250A%2520%2520By%2520introducing%2520these%2520actionable%2520frameworks%252C%2520this%2520research%2520extends%2520in-person%250Adeliberation%2520with%2520scalable%2520digital%2520methods%2520that%2520address%2520the%2520complexities%2520of%250Amodern%2520decision-making%2520in%2520participatory%2520processes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05017v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Voting%20and%20Deliberation%20with%20Algorithms%3A%20Field%20Insights%20from%0A%20%20vTaiwan%20and%20Kultur%20Komitee&entry.906535625=Joshua%20C.%20Yang%20and%20Fynn%20Bachmann&entry.1292438233=%20%20Democratic%20processes%20increasingly%20aim%20to%20integrate%20large-scale%20voting%20with%0Aface-to-face%20deliberation%2C%20addressing%20the%20challenge%20of%20reconciling%20individual%0Apreferences%20with%20collective%20decision-making.%20This%20work%20introduces%20new%20methods%0Athat%20use%20algorithms%20and%20computational%20tools%20to%20bridge%20online%20voting%20with%0Aface-to-face%20deliberation%2C%20tested%20in%20two%20real-world%20scenarios%3A%20Kultur%20Komitee%0A2024%20%28KK24%29%20and%20vTaiwan.%20These%20case%20studies%20highlight%20the%20practical%0Aapplications%20and%20impacts%20of%20the%20proposed%20methods.%0A%20%20We%20present%20three%20key%20contributions%3A%20%281%29%20Radial%20Clustering%20for%20Preference%0ABased%20Subgroups%2C%20which%20enables%20both%20in-depth%20and%20broad%20discussions%20in%0Adeliberative%20settings%20by%20computing%20homogeneous%20and%20heterogeneous%20group%0Acompositions%20with%20balanced%20and%20adjustable%20group%20sizes%3B%20%282%29%20Human-in-the-loop%0AMES%2C%20a%20practical%20method%20that%20enhances%20the%20Method%20of%20Equal%20Shares%20%28MES%29%0Aalgorithm%20with%20real-time%20digital%20feedback.%20This%20builds%20algorithmic%20trust%20by%0Agiving%20participants%20full%20control%20over%20how%20much%20decision-making%20is%20delegated%20to%0Athe%20voting%20aggregation%20algorithm%20as%20compared%20to%20deliberation%3B%20and%20%283%29%20the%0AReadTheRoom%20deliberation%20method%2C%20which%20uses%20opinion%20space%20mapping%20to%20identify%0Aagreement%20and%20divergence%2C%20along%20with%20spectrum-based%20preference%20visualisation%20to%0Atrack%20opinion%20shifts%20during%20deliberation.%20This%20approach%20enhances%20transparency%0Aby%20clarifying%20collective%20sentiment%20and%20fosters%20collaboration%20by%20encouraging%0Aparticipants%20to%20engage%20constructively%20with%20differing%20perspectives.%0A%20%20By%20introducing%20these%20actionable%20frameworks%2C%20this%20research%20extends%20in-person%0Adeliberation%20with%20scalable%20digital%20methods%20that%20address%20the%20complexities%20of%0Amodern%20decision-making%20in%20participatory%20processes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05017v1&entry.124074799=Read"},
{"title": "Any-stepsize Gradient Descent for Separable Data under Fenchel--Young\n  Losses", "author": "Han Bao and Shinsaku Sakaue and Yuki Takezawa", "abstract": "  The gradient descent (GD) has been one of the most common optimizer in\nmachine learning. In particular, the loss landscape of a neural network is\ntypically sharpened during the initial phase of training, making the training\ndynamics hover on the edge of stability. This is beyond our standard\nunderstanding of GD convergence in the stable regime where arbitrarily chosen\nstepsize is sufficiently smaller than the edge of stability. Recently, Wu et\nal. (COLT2024) have showed that GD converges with arbitrary stepsize under\nlinearly separable logistic regression. Although their analysis hinges on the\nself-bounding property of the logistic loss, which seems to be a cornerstone to\nestablish a modified descent lemma, our pilot study shows that other loss\nfunctions without the self-bounding property can make GD converge with\narbitrary stepsize. To further understand what property of a loss function\nmatters in GD, we aim to show arbitrary-stepsize GD convergence for a general\nloss function based on the framework of \\emph{Fenchel--Young losses}. We\nessentially leverage the classical perceptron argument to derive the\nconvergence rate for achieving $\\epsilon$-optimal loss, which is possible for a\nmajority of Fenchel--Young losses. Among typical loss functions, the Tsallis\nentropy achieves the GD convergence rate $T=\\Omega(\\epsilon^{-1/2})$, and the\nR{\\'e}nyi entropy achieves the far better rate $T=\\Omega(\\epsilon^{-1/3})$. We\nargue that these better rate is possible because of \\emph{separation margin} of\nloss functions, instead of the self-bounding property.\n", "link": "http://arxiv.org/abs/2502.04889v1", "date": "2025-02-07", "relevancy": 2.2663, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4626}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4526}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Any-stepsize%20Gradient%20Descent%20for%20Separable%20Data%20under%20Fenchel--Young%0A%20%20Losses&body=Title%3A%20Any-stepsize%20Gradient%20Descent%20for%20Separable%20Data%20under%20Fenchel--Young%0A%20%20Losses%0AAuthor%3A%20Han%20Bao%20and%20Shinsaku%20Sakaue%20and%20Yuki%20Takezawa%0AAbstract%3A%20%20%20The%20gradient%20descent%20%28GD%29%20has%20been%20one%20of%20the%20most%20common%20optimizer%20in%0Amachine%20learning.%20In%20particular%2C%20the%20loss%20landscape%20of%20a%20neural%20network%20is%0Atypically%20sharpened%20during%20the%20initial%20phase%20of%20training%2C%20making%20the%20training%0Adynamics%20hover%20on%20the%20edge%20of%20stability.%20This%20is%20beyond%20our%20standard%0Aunderstanding%20of%20GD%20convergence%20in%20the%20stable%20regime%20where%20arbitrarily%20chosen%0Astepsize%20is%20sufficiently%20smaller%20than%20the%20edge%20of%20stability.%20Recently%2C%20Wu%20et%0Aal.%20%28COLT2024%29%20have%20showed%20that%20GD%20converges%20with%20arbitrary%20stepsize%20under%0Alinearly%20separable%20logistic%20regression.%20Although%20their%20analysis%20hinges%20on%20the%0Aself-bounding%20property%20of%20the%20logistic%20loss%2C%20which%20seems%20to%20be%20a%20cornerstone%20to%0Aestablish%20a%20modified%20descent%20lemma%2C%20our%20pilot%20study%20shows%20that%20other%20loss%0Afunctions%20without%20the%20self-bounding%20property%20can%20make%20GD%20converge%20with%0Aarbitrary%20stepsize.%20To%20further%20understand%20what%20property%20of%20a%20loss%20function%0Amatters%20in%20GD%2C%20we%20aim%20to%20show%20arbitrary-stepsize%20GD%20convergence%20for%20a%20general%0Aloss%20function%20based%20on%20the%20framework%20of%20%5Cemph%7BFenchel--Young%20losses%7D.%20We%0Aessentially%20leverage%20the%20classical%20perceptron%20argument%20to%20derive%20the%0Aconvergence%20rate%20for%20achieving%20%24%5Cepsilon%24-optimal%20loss%2C%20which%20is%20possible%20for%20a%0Amajority%20of%20Fenchel--Young%20losses.%20Among%20typical%20loss%20functions%2C%20the%20Tsallis%0Aentropy%20achieves%20the%20GD%20convergence%20rate%20%24T%3D%5COmega%28%5Cepsilon%5E%7B-1/2%7D%29%24%2C%20and%20the%0AR%7B%5C%27e%7Dnyi%20entropy%20achieves%20the%20far%20better%20rate%20%24T%3D%5COmega%28%5Cepsilon%5E%7B-1/3%7D%29%24.%20We%0Aargue%20that%20these%20better%20rate%20is%20possible%20because%20of%20%5Cemph%7Bseparation%20margin%7D%20of%0Aloss%20functions%2C%20instead%20of%20the%20self-bounding%20property.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04889v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAny-stepsize%2520Gradient%2520Descent%2520for%2520Separable%2520Data%2520under%2520Fenchel--Young%250A%2520%2520Losses%26entry.906535625%3DHan%2520Bao%2520and%2520Shinsaku%2520Sakaue%2520and%2520Yuki%2520Takezawa%26entry.1292438233%3D%2520%2520The%2520gradient%2520descent%2520%2528GD%2529%2520has%2520been%2520one%2520of%2520the%2520most%2520common%2520optimizer%2520in%250Amachine%2520learning.%2520In%2520particular%252C%2520the%2520loss%2520landscape%2520of%2520a%2520neural%2520network%2520is%250Atypically%2520sharpened%2520during%2520the%2520initial%2520phase%2520of%2520training%252C%2520making%2520the%2520training%250Adynamics%2520hover%2520on%2520the%2520edge%2520of%2520stability.%2520This%2520is%2520beyond%2520our%2520standard%250Aunderstanding%2520of%2520GD%2520convergence%2520in%2520the%2520stable%2520regime%2520where%2520arbitrarily%2520chosen%250Astepsize%2520is%2520sufficiently%2520smaller%2520than%2520the%2520edge%2520of%2520stability.%2520Recently%252C%2520Wu%2520et%250Aal.%2520%2528COLT2024%2529%2520have%2520showed%2520that%2520GD%2520converges%2520with%2520arbitrary%2520stepsize%2520under%250Alinearly%2520separable%2520logistic%2520regression.%2520Although%2520their%2520analysis%2520hinges%2520on%2520the%250Aself-bounding%2520property%2520of%2520the%2520logistic%2520loss%252C%2520which%2520seems%2520to%2520be%2520a%2520cornerstone%2520to%250Aestablish%2520a%2520modified%2520descent%2520lemma%252C%2520our%2520pilot%2520study%2520shows%2520that%2520other%2520loss%250Afunctions%2520without%2520the%2520self-bounding%2520property%2520can%2520make%2520GD%2520converge%2520with%250Aarbitrary%2520stepsize.%2520To%2520further%2520understand%2520what%2520property%2520of%2520a%2520loss%2520function%250Amatters%2520in%2520GD%252C%2520we%2520aim%2520to%2520show%2520arbitrary-stepsize%2520GD%2520convergence%2520for%2520a%2520general%250Aloss%2520function%2520based%2520on%2520the%2520framework%2520of%2520%255Cemph%257BFenchel--Young%2520losses%257D.%2520We%250Aessentially%2520leverage%2520the%2520classical%2520perceptron%2520argument%2520to%2520derive%2520the%250Aconvergence%2520rate%2520for%2520achieving%2520%2524%255Cepsilon%2524-optimal%2520loss%252C%2520which%2520is%2520possible%2520for%2520a%250Amajority%2520of%2520Fenchel--Young%2520losses.%2520Among%2520typical%2520loss%2520functions%252C%2520the%2520Tsallis%250Aentropy%2520achieves%2520the%2520GD%2520convergence%2520rate%2520%2524T%253D%255COmega%2528%255Cepsilon%255E%257B-1/2%257D%2529%2524%252C%2520and%2520the%250AR%257B%255C%2527e%257Dnyi%2520entropy%2520achieves%2520the%2520far%2520better%2520rate%2520%2524T%253D%255COmega%2528%255Cepsilon%255E%257B-1/3%257D%2529%2524.%2520We%250Aargue%2520that%2520these%2520better%2520rate%2520is%2520possible%2520because%2520of%2520%255Cemph%257Bseparation%2520margin%257D%2520of%250Aloss%2520functions%252C%2520instead%2520of%2520the%2520self-bounding%2520property.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04889v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Any-stepsize%20Gradient%20Descent%20for%20Separable%20Data%20under%20Fenchel--Young%0A%20%20Losses&entry.906535625=Han%20Bao%20and%20Shinsaku%20Sakaue%20and%20Yuki%20Takezawa&entry.1292438233=%20%20The%20gradient%20descent%20%28GD%29%20has%20been%20one%20of%20the%20most%20common%20optimizer%20in%0Amachine%20learning.%20In%20particular%2C%20the%20loss%20landscape%20of%20a%20neural%20network%20is%0Atypically%20sharpened%20during%20the%20initial%20phase%20of%20training%2C%20making%20the%20training%0Adynamics%20hover%20on%20the%20edge%20of%20stability.%20This%20is%20beyond%20our%20standard%0Aunderstanding%20of%20GD%20convergence%20in%20the%20stable%20regime%20where%20arbitrarily%20chosen%0Astepsize%20is%20sufficiently%20smaller%20than%20the%20edge%20of%20stability.%20Recently%2C%20Wu%20et%0Aal.%20%28COLT2024%29%20have%20showed%20that%20GD%20converges%20with%20arbitrary%20stepsize%20under%0Alinearly%20separable%20logistic%20regression.%20Although%20their%20analysis%20hinges%20on%20the%0Aself-bounding%20property%20of%20the%20logistic%20loss%2C%20which%20seems%20to%20be%20a%20cornerstone%20to%0Aestablish%20a%20modified%20descent%20lemma%2C%20our%20pilot%20study%20shows%20that%20other%20loss%0Afunctions%20without%20the%20self-bounding%20property%20can%20make%20GD%20converge%20with%0Aarbitrary%20stepsize.%20To%20further%20understand%20what%20property%20of%20a%20loss%20function%0Amatters%20in%20GD%2C%20we%20aim%20to%20show%20arbitrary-stepsize%20GD%20convergence%20for%20a%20general%0Aloss%20function%20based%20on%20the%20framework%20of%20%5Cemph%7BFenchel--Young%20losses%7D.%20We%0Aessentially%20leverage%20the%20classical%20perceptron%20argument%20to%20derive%20the%0Aconvergence%20rate%20for%20achieving%20%24%5Cepsilon%24-optimal%20loss%2C%20which%20is%20possible%20for%20a%0Amajority%20of%20Fenchel--Young%20losses.%20Among%20typical%20loss%20functions%2C%20the%20Tsallis%0Aentropy%20achieves%20the%20GD%20convergence%20rate%20%24T%3D%5COmega%28%5Cepsilon%5E%7B-1/2%7D%29%24%2C%20and%20the%0AR%7B%5C%27e%7Dnyi%20entropy%20achieves%20the%20far%20better%20rate%20%24T%3D%5COmega%28%5Cepsilon%5E%7B-1/3%7D%29%24.%20We%0Aargue%20that%20these%20better%20rate%20is%20possible%20because%20of%20%5Cemph%7Bseparation%20margin%7D%20of%0Aloss%20functions%2C%20instead%20of%20the%20self-bounding%20property.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04889v1&entry.124074799=Read"},
{"title": "Efficient Optimal PAC Learning", "author": "Mikael M\u00f8ller H\u00f8gsgaard", "abstract": "  Recent advances in the binary classification setting by Hanneke [2016b] and\nLarsen [2023] have resulted in optimal PAC learners. These learners leverage,\nrespectively, a clever deterministic subsampling scheme and the classic\nheuristic of bagging Breiman [1996]. Both optimal PAC learners use, as a\nsubroutine, the natural algorithm of empirical risk minimization. Consequently,\nthe computational cost of these optimal PAC learners is tied to that of the\nempirical risk minimizer algorithm. In this work, we seek to provide an\nalternative perspective on the computational cost imposed by the link to the\nempirical risk minimizer algorithm. To this end, we show the existence of an\noptimal PAC learner, which offers a different tradeoff in terms of the\ncomputational cost induced by the empirical risk minimizer.\n", "link": "http://arxiv.org/abs/2502.03620v2", "date": "2025-02-07", "relevancy": 2.2557, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4879}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4386}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Optimal%20PAC%20Learning&body=Title%3A%20Efficient%20Optimal%20PAC%20Learning%0AAuthor%3A%20Mikael%20M%C3%B8ller%20H%C3%B8gsgaard%0AAbstract%3A%20%20%20Recent%20advances%20in%20the%20binary%20classification%20setting%20by%20Hanneke%20%5B2016b%5D%20and%0ALarsen%20%5B2023%5D%20have%20resulted%20in%20optimal%20PAC%20learners.%20These%20learners%20leverage%2C%0Arespectively%2C%20a%20clever%20deterministic%20subsampling%20scheme%20and%20the%20classic%0Aheuristic%20of%20bagging%20Breiman%20%5B1996%5D.%20Both%20optimal%20PAC%20learners%20use%2C%20as%20a%0Asubroutine%2C%20the%20natural%20algorithm%20of%20empirical%20risk%20minimization.%20Consequently%2C%0Athe%20computational%20cost%20of%20these%20optimal%20PAC%20learners%20is%20tied%20to%20that%20of%20the%0Aempirical%20risk%20minimizer%20algorithm.%20In%20this%20work%2C%20we%20seek%20to%20provide%20an%0Aalternative%20perspective%20on%20the%20computational%20cost%20imposed%20by%20the%20link%20to%20the%0Aempirical%20risk%20minimizer%20algorithm.%20To%20this%20end%2C%20we%20show%20the%20existence%20of%20an%0Aoptimal%20PAC%20learner%2C%20which%20offers%20a%20different%20tradeoff%20in%20terms%20of%20the%0Acomputational%20cost%20induced%20by%20the%20empirical%20risk%20minimizer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03620v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Optimal%2520PAC%2520Learning%26entry.906535625%3DMikael%2520M%25C3%25B8ller%2520H%25C3%25B8gsgaard%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520the%2520binary%2520classification%2520setting%2520by%2520Hanneke%2520%255B2016b%255D%2520and%250ALarsen%2520%255B2023%255D%2520have%2520resulted%2520in%2520optimal%2520PAC%2520learners.%2520These%2520learners%2520leverage%252C%250Arespectively%252C%2520a%2520clever%2520deterministic%2520subsampling%2520scheme%2520and%2520the%2520classic%250Aheuristic%2520of%2520bagging%2520Breiman%2520%255B1996%255D.%2520Both%2520optimal%2520PAC%2520learners%2520use%252C%2520as%2520a%250Asubroutine%252C%2520the%2520natural%2520algorithm%2520of%2520empirical%2520risk%2520minimization.%2520Consequently%252C%250Athe%2520computational%2520cost%2520of%2520these%2520optimal%2520PAC%2520learners%2520is%2520tied%2520to%2520that%2520of%2520the%250Aempirical%2520risk%2520minimizer%2520algorithm.%2520In%2520this%2520work%252C%2520we%2520seek%2520to%2520provide%2520an%250Aalternative%2520perspective%2520on%2520the%2520computational%2520cost%2520imposed%2520by%2520the%2520link%2520to%2520the%250Aempirical%2520risk%2520minimizer%2520algorithm.%2520To%2520this%2520end%252C%2520we%2520show%2520the%2520existence%2520of%2520an%250Aoptimal%2520PAC%2520learner%252C%2520which%2520offers%2520a%2520different%2520tradeoff%2520in%2520terms%2520of%2520the%250Acomputational%2520cost%2520induced%2520by%2520the%2520empirical%2520risk%2520minimizer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03620v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Optimal%20PAC%20Learning&entry.906535625=Mikael%20M%C3%B8ller%20H%C3%B8gsgaard&entry.1292438233=%20%20Recent%20advances%20in%20the%20binary%20classification%20setting%20by%20Hanneke%20%5B2016b%5D%20and%0ALarsen%20%5B2023%5D%20have%20resulted%20in%20optimal%20PAC%20learners.%20These%20learners%20leverage%2C%0Arespectively%2C%20a%20clever%20deterministic%20subsampling%20scheme%20and%20the%20classic%0Aheuristic%20of%20bagging%20Breiman%20%5B1996%5D.%20Both%20optimal%20PAC%20learners%20use%2C%20as%20a%0Asubroutine%2C%20the%20natural%20algorithm%20of%20empirical%20risk%20minimization.%20Consequently%2C%0Athe%20computational%20cost%20of%20these%20optimal%20PAC%20learners%20is%20tied%20to%20that%20of%20the%0Aempirical%20risk%20minimizer%20algorithm.%20In%20this%20work%2C%20we%20seek%20to%20provide%20an%0Aalternative%20perspective%20on%20the%20computational%20cost%20imposed%20by%20the%20link%20to%20the%0Aempirical%20risk%20minimizer%20algorithm.%20To%20this%20end%2C%20we%20show%20the%20existence%20of%20an%0Aoptimal%20PAC%20learner%2C%20which%20offers%20a%20different%20tradeoff%20in%20terms%20of%20the%0Acomputational%20cost%20induced%20by%20the%20empirical%20risk%20minimizer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03620v2&entry.124074799=Read"},
{"title": "Evaluating Standard and Dialectal Frisian ASR: Multilingual Fine-tuning\n  and Language Identification for Improved Low-resource Performance", "author": "Reihaneh Amooie and Wietse de Vries and Yun Hao and Jelske Dijkstra and Matt Coler and Martijn Wieling", "abstract": "  Automatic Speech Recognition (ASR) performance for low-resource languages is\nstill far behind that of higher-resource languages such as English, due to a\nlack of sufficient labeled data. State-of-the-art methods deploy\nself-supervised transfer learning where a model pre-trained on large amounts of\ndata is fine-tuned using little labeled data in a target low-resource language.\nIn this paper, we present and examine a method for fine-tuning an SSL-based\nmodel in order to improve the performance for Frisian and its regional dialects\n(Clay Frisian, Wood Frisian, and South Frisian). We show that Frisian ASR\nperformance can be improved by using multilingual (Frisian, Dutch, English and\nGerman) fine-tuning data and an auxiliary language identification task. In\naddition, our findings show that performance on dialectal speech suffers\nsubstantially, and, importantly, that this effect is moderated by the\nelicitation approach used to collect the dialectal data. Our findings also\nparticularly suggest that relying solely on standard language data for ASR\nevaluation may underestimate real-world performance, particularly in languages\nwith substantial dialectal variation.\n", "link": "http://arxiv.org/abs/2502.04883v1", "date": "2025-02-07", "relevancy": 2.2392, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4617}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4617}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Standard%20and%20Dialectal%20Frisian%20ASR%3A%20Multilingual%20Fine-tuning%0A%20%20and%20Language%20Identification%20for%20Improved%20Low-resource%20Performance&body=Title%3A%20Evaluating%20Standard%20and%20Dialectal%20Frisian%20ASR%3A%20Multilingual%20Fine-tuning%0A%20%20and%20Language%20Identification%20for%20Improved%20Low-resource%20Performance%0AAuthor%3A%20Reihaneh%20Amooie%20and%20Wietse%20de%20Vries%20and%20Yun%20Hao%20and%20Jelske%20Dijkstra%20and%20Matt%20Coler%20and%20Martijn%20Wieling%0AAbstract%3A%20%20%20Automatic%20Speech%20Recognition%20%28ASR%29%20performance%20for%20low-resource%20languages%20is%0Astill%20far%20behind%20that%20of%20higher-resource%20languages%20such%20as%20English%2C%20due%20to%20a%0Alack%20of%20sufficient%20labeled%20data.%20State-of-the-art%20methods%20deploy%0Aself-supervised%20transfer%20learning%20where%20a%20model%20pre-trained%20on%20large%20amounts%20of%0Adata%20is%20fine-tuned%20using%20little%20labeled%20data%20in%20a%20target%20low-resource%20language.%0AIn%20this%20paper%2C%20we%20present%20and%20examine%20a%20method%20for%20fine-tuning%20an%20SSL-based%0Amodel%20in%20order%20to%20improve%20the%20performance%20for%20Frisian%20and%20its%20regional%20dialects%0A%28Clay%20Frisian%2C%20Wood%20Frisian%2C%20and%20South%20Frisian%29.%20We%20show%20that%20Frisian%20ASR%0Aperformance%20can%20be%20improved%20by%20using%20multilingual%20%28Frisian%2C%20Dutch%2C%20English%20and%0AGerman%29%20fine-tuning%20data%20and%20an%20auxiliary%20language%20identification%20task.%20In%0Aaddition%2C%20our%20findings%20show%20that%20performance%20on%20dialectal%20speech%20suffers%0Asubstantially%2C%20and%2C%20importantly%2C%20that%20this%20effect%20is%20moderated%20by%20the%0Aelicitation%20approach%20used%20to%20collect%20the%20dialectal%20data.%20Our%20findings%20also%0Aparticularly%20suggest%20that%20relying%20solely%20on%20standard%20language%20data%20for%20ASR%0Aevaluation%20may%20underestimate%20real-world%20performance%2C%20particularly%20in%20languages%0Awith%20substantial%20dialectal%20variation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04883v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Standard%2520and%2520Dialectal%2520Frisian%2520ASR%253A%2520Multilingual%2520Fine-tuning%250A%2520%2520and%2520Language%2520Identification%2520for%2520Improved%2520Low-resource%2520Performance%26entry.906535625%3DReihaneh%2520Amooie%2520and%2520Wietse%2520de%2520Vries%2520and%2520Yun%2520Hao%2520and%2520Jelske%2520Dijkstra%2520and%2520Matt%2520Coler%2520and%2520Martijn%2520Wieling%26entry.1292438233%3D%2520%2520Automatic%2520Speech%2520Recognition%2520%2528ASR%2529%2520performance%2520for%2520low-resource%2520languages%2520is%250Astill%2520far%2520behind%2520that%2520of%2520higher-resource%2520languages%2520such%2520as%2520English%252C%2520due%2520to%2520a%250Alack%2520of%2520sufficient%2520labeled%2520data.%2520State-of-the-art%2520methods%2520deploy%250Aself-supervised%2520transfer%2520learning%2520where%2520a%2520model%2520pre-trained%2520on%2520large%2520amounts%2520of%250Adata%2520is%2520fine-tuned%2520using%2520little%2520labeled%2520data%2520in%2520a%2520target%2520low-resource%2520language.%250AIn%2520this%2520paper%252C%2520we%2520present%2520and%2520examine%2520a%2520method%2520for%2520fine-tuning%2520an%2520SSL-based%250Amodel%2520in%2520order%2520to%2520improve%2520the%2520performance%2520for%2520Frisian%2520and%2520its%2520regional%2520dialects%250A%2528Clay%2520Frisian%252C%2520Wood%2520Frisian%252C%2520and%2520South%2520Frisian%2529.%2520We%2520show%2520that%2520Frisian%2520ASR%250Aperformance%2520can%2520be%2520improved%2520by%2520using%2520multilingual%2520%2528Frisian%252C%2520Dutch%252C%2520English%2520and%250AGerman%2529%2520fine-tuning%2520data%2520and%2520an%2520auxiliary%2520language%2520identification%2520task.%2520In%250Aaddition%252C%2520our%2520findings%2520show%2520that%2520performance%2520on%2520dialectal%2520speech%2520suffers%250Asubstantially%252C%2520and%252C%2520importantly%252C%2520that%2520this%2520effect%2520is%2520moderated%2520by%2520the%250Aelicitation%2520approach%2520used%2520to%2520collect%2520the%2520dialectal%2520data.%2520Our%2520findings%2520also%250Aparticularly%2520suggest%2520that%2520relying%2520solely%2520on%2520standard%2520language%2520data%2520for%2520ASR%250Aevaluation%2520may%2520underestimate%2520real-world%2520performance%252C%2520particularly%2520in%2520languages%250Awith%2520substantial%2520dialectal%2520variation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04883v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Standard%20and%20Dialectal%20Frisian%20ASR%3A%20Multilingual%20Fine-tuning%0A%20%20and%20Language%20Identification%20for%20Improved%20Low-resource%20Performance&entry.906535625=Reihaneh%20Amooie%20and%20Wietse%20de%20Vries%20and%20Yun%20Hao%20and%20Jelske%20Dijkstra%20and%20Matt%20Coler%20and%20Martijn%20Wieling&entry.1292438233=%20%20Automatic%20Speech%20Recognition%20%28ASR%29%20performance%20for%20low-resource%20languages%20is%0Astill%20far%20behind%20that%20of%20higher-resource%20languages%20such%20as%20English%2C%20due%20to%20a%0Alack%20of%20sufficient%20labeled%20data.%20State-of-the-art%20methods%20deploy%0Aself-supervised%20transfer%20learning%20where%20a%20model%20pre-trained%20on%20large%20amounts%20of%0Adata%20is%20fine-tuned%20using%20little%20labeled%20data%20in%20a%20target%20low-resource%20language.%0AIn%20this%20paper%2C%20we%20present%20and%20examine%20a%20method%20for%20fine-tuning%20an%20SSL-based%0Amodel%20in%20order%20to%20improve%20the%20performance%20for%20Frisian%20and%20its%20regional%20dialects%0A%28Clay%20Frisian%2C%20Wood%20Frisian%2C%20and%20South%20Frisian%29.%20We%20show%20that%20Frisian%20ASR%0Aperformance%20can%20be%20improved%20by%20using%20multilingual%20%28Frisian%2C%20Dutch%2C%20English%20and%0AGerman%29%20fine-tuning%20data%20and%20an%20auxiliary%20language%20identification%20task.%20In%0Aaddition%2C%20our%20findings%20show%20that%20performance%20on%20dialectal%20speech%20suffers%0Asubstantially%2C%20and%2C%20importantly%2C%20that%20this%20effect%20is%20moderated%20by%20the%0Aelicitation%20approach%20used%20to%20collect%20the%20dialectal%20data.%20Our%20findings%20also%0Aparticularly%20suggest%20that%20relying%20solely%20on%20standard%20language%20data%20for%20ASR%0Aevaluation%20may%20underestimate%20real-world%20performance%2C%20particularly%20in%20languages%0Awith%20substantial%20dialectal%20variation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04883v1&entry.124074799=Read"},
{"title": "Masked Diffusion Models are Secretly Time-Agnostic Masked Models and\n  Exploit Inaccurate Categorical Sampling", "author": "Kaiwen Zheng and Yongxin Chen and Hanzi Mao and Ming-Yu Liu and Jun Zhu and Qinsheng Zhang", "abstract": "  Masked diffusion models (MDMs) have emerged as a popular research topic for\ngenerative modeling of discrete data, thanks to their superior performance over\nother discrete diffusion models, and are rivaling the auto-regressive models\n(ARMs) for language modeling tasks. The recent effort in simplifying the masked\ndiffusion framework further leads to alignment with continuous-space diffusion\nmodels and more principled training and sampling recipes. In this paper,\nhowever, we reveal that both training and sampling of MDMs are theoretically\nfree from the time variable, arguably the key signature of diffusion models,\nand are instead equivalent to masked models. The connection on the sampling\naspect is drawn by our proposed first-hitting sampler (FHS). Specifically, we\nshow that the FHS is theoretically equivalent to MDMs' original generation\nprocess while significantly alleviating the time-consuming categorical sampling\nand achieving a 20$\\times$ speedup. In addition, our investigation raises\ndoubts about whether MDMs can truly beat ARMs in text generation. We identify,\nfor the first time, an underlying numerical issue, even with the commonly used\n32-bit floating-point precision, which results in inaccurate categorical\nsampling. We show that it lowers the effective temperature both theoretically\nand empirically, and the resulting decrease in token diversity makes previous\nevaluations, which assess the generation quality solely through the incomplete\ngenerative perplexity metric, somewhat unfair.\n", "link": "http://arxiv.org/abs/2409.02908v4", "date": "2025-02-07", "relevancy": 2.2314, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5949}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5519}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Masked%20Diffusion%20Models%20are%20Secretly%20Time-Agnostic%20Masked%20Models%20and%0A%20%20Exploit%20Inaccurate%20Categorical%20Sampling&body=Title%3A%20Masked%20Diffusion%20Models%20are%20Secretly%20Time-Agnostic%20Masked%20Models%20and%0A%20%20Exploit%20Inaccurate%20Categorical%20Sampling%0AAuthor%3A%20Kaiwen%20Zheng%20and%20Yongxin%20Chen%20and%20Hanzi%20Mao%20and%20Ming-Yu%20Liu%20and%20Jun%20Zhu%20and%20Qinsheng%20Zhang%0AAbstract%3A%20%20%20Masked%20diffusion%20models%20%28MDMs%29%20have%20emerged%20as%20a%20popular%20research%20topic%20for%0Agenerative%20modeling%20of%20discrete%20data%2C%20thanks%20to%20their%20superior%20performance%20over%0Aother%20discrete%20diffusion%20models%2C%20and%20are%20rivaling%20the%20auto-regressive%20models%0A%28ARMs%29%20for%20language%20modeling%20tasks.%20The%20recent%20effort%20in%20simplifying%20the%20masked%0Adiffusion%20framework%20further%20leads%20to%20alignment%20with%20continuous-space%20diffusion%0Amodels%20and%20more%20principled%20training%20and%20sampling%20recipes.%20In%20this%20paper%2C%0Ahowever%2C%20we%20reveal%20that%20both%20training%20and%20sampling%20of%20MDMs%20are%20theoretically%0Afree%20from%20the%20time%20variable%2C%20arguably%20the%20key%20signature%20of%20diffusion%20models%2C%0Aand%20are%20instead%20equivalent%20to%20masked%20models.%20The%20connection%20on%20the%20sampling%0Aaspect%20is%20drawn%20by%20our%20proposed%20first-hitting%20sampler%20%28FHS%29.%20Specifically%2C%20we%0Ashow%20that%20the%20FHS%20is%20theoretically%20equivalent%20to%20MDMs%27%20original%20generation%0Aprocess%20while%20significantly%20alleviating%20the%20time-consuming%20categorical%20sampling%0Aand%20achieving%20a%2020%24%5Ctimes%24%20speedup.%20In%20addition%2C%20our%20investigation%20raises%0Adoubts%20about%20whether%20MDMs%20can%20truly%20beat%20ARMs%20in%20text%20generation.%20We%20identify%2C%0Afor%20the%20first%20time%2C%20an%20underlying%20numerical%20issue%2C%20even%20with%20the%20commonly%20used%0A32-bit%20floating-point%20precision%2C%20which%20results%20in%20inaccurate%20categorical%0Asampling.%20We%20show%20that%20it%20lowers%20the%20effective%20temperature%20both%20theoretically%0Aand%20empirically%2C%20and%20the%20resulting%20decrease%20in%20token%20diversity%20makes%20previous%0Aevaluations%2C%20which%20assess%20the%20generation%20quality%20solely%20through%20the%20incomplete%0Agenerative%20perplexity%20metric%2C%20somewhat%20unfair.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02908v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMasked%2520Diffusion%2520Models%2520are%2520Secretly%2520Time-Agnostic%2520Masked%2520Models%2520and%250A%2520%2520Exploit%2520Inaccurate%2520Categorical%2520Sampling%26entry.906535625%3DKaiwen%2520Zheng%2520and%2520Yongxin%2520Chen%2520and%2520Hanzi%2520Mao%2520and%2520Ming-Yu%2520Liu%2520and%2520Jun%2520Zhu%2520and%2520Qinsheng%2520Zhang%26entry.1292438233%3D%2520%2520Masked%2520diffusion%2520models%2520%2528MDMs%2529%2520have%2520emerged%2520as%2520a%2520popular%2520research%2520topic%2520for%250Agenerative%2520modeling%2520of%2520discrete%2520data%252C%2520thanks%2520to%2520their%2520superior%2520performance%2520over%250Aother%2520discrete%2520diffusion%2520models%252C%2520and%2520are%2520rivaling%2520the%2520auto-regressive%2520models%250A%2528ARMs%2529%2520for%2520language%2520modeling%2520tasks.%2520The%2520recent%2520effort%2520in%2520simplifying%2520the%2520masked%250Adiffusion%2520framework%2520further%2520leads%2520to%2520alignment%2520with%2520continuous-space%2520diffusion%250Amodels%2520and%2520more%2520principled%2520training%2520and%2520sampling%2520recipes.%2520In%2520this%2520paper%252C%250Ahowever%252C%2520we%2520reveal%2520that%2520both%2520training%2520and%2520sampling%2520of%2520MDMs%2520are%2520theoretically%250Afree%2520from%2520the%2520time%2520variable%252C%2520arguably%2520the%2520key%2520signature%2520of%2520diffusion%2520models%252C%250Aand%2520are%2520instead%2520equivalent%2520to%2520masked%2520models.%2520The%2520connection%2520on%2520the%2520sampling%250Aaspect%2520is%2520drawn%2520by%2520our%2520proposed%2520first-hitting%2520sampler%2520%2528FHS%2529.%2520Specifically%252C%2520we%250Ashow%2520that%2520the%2520FHS%2520is%2520theoretically%2520equivalent%2520to%2520MDMs%2527%2520original%2520generation%250Aprocess%2520while%2520significantly%2520alleviating%2520the%2520time-consuming%2520categorical%2520sampling%250Aand%2520achieving%2520a%252020%2524%255Ctimes%2524%2520speedup.%2520In%2520addition%252C%2520our%2520investigation%2520raises%250Adoubts%2520about%2520whether%2520MDMs%2520can%2520truly%2520beat%2520ARMs%2520in%2520text%2520generation.%2520We%2520identify%252C%250Afor%2520the%2520first%2520time%252C%2520an%2520underlying%2520numerical%2520issue%252C%2520even%2520with%2520the%2520commonly%2520used%250A32-bit%2520floating-point%2520precision%252C%2520which%2520results%2520in%2520inaccurate%2520categorical%250Asampling.%2520We%2520show%2520that%2520it%2520lowers%2520the%2520effective%2520temperature%2520both%2520theoretically%250Aand%2520empirically%252C%2520and%2520the%2520resulting%2520decrease%2520in%2520token%2520diversity%2520makes%2520previous%250Aevaluations%252C%2520which%2520assess%2520the%2520generation%2520quality%2520solely%2520through%2520the%2520incomplete%250Agenerative%2520perplexity%2520metric%252C%2520somewhat%2520unfair.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02908v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Masked%20Diffusion%20Models%20are%20Secretly%20Time-Agnostic%20Masked%20Models%20and%0A%20%20Exploit%20Inaccurate%20Categorical%20Sampling&entry.906535625=Kaiwen%20Zheng%20and%20Yongxin%20Chen%20and%20Hanzi%20Mao%20and%20Ming-Yu%20Liu%20and%20Jun%20Zhu%20and%20Qinsheng%20Zhang&entry.1292438233=%20%20Masked%20diffusion%20models%20%28MDMs%29%20have%20emerged%20as%20a%20popular%20research%20topic%20for%0Agenerative%20modeling%20of%20discrete%20data%2C%20thanks%20to%20their%20superior%20performance%20over%0Aother%20discrete%20diffusion%20models%2C%20and%20are%20rivaling%20the%20auto-regressive%20models%0A%28ARMs%29%20for%20language%20modeling%20tasks.%20The%20recent%20effort%20in%20simplifying%20the%20masked%0Adiffusion%20framework%20further%20leads%20to%20alignment%20with%20continuous-space%20diffusion%0Amodels%20and%20more%20principled%20training%20and%20sampling%20recipes.%20In%20this%20paper%2C%0Ahowever%2C%20we%20reveal%20that%20both%20training%20and%20sampling%20of%20MDMs%20are%20theoretically%0Afree%20from%20the%20time%20variable%2C%20arguably%20the%20key%20signature%20of%20diffusion%20models%2C%0Aand%20are%20instead%20equivalent%20to%20masked%20models.%20The%20connection%20on%20the%20sampling%0Aaspect%20is%20drawn%20by%20our%20proposed%20first-hitting%20sampler%20%28FHS%29.%20Specifically%2C%20we%0Ashow%20that%20the%20FHS%20is%20theoretically%20equivalent%20to%20MDMs%27%20original%20generation%0Aprocess%20while%20significantly%20alleviating%20the%20time-consuming%20categorical%20sampling%0Aand%20achieving%20a%2020%24%5Ctimes%24%20speedup.%20In%20addition%2C%20our%20investigation%20raises%0Adoubts%20about%20whether%20MDMs%20can%20truly%20beat%20ARMs%20in%20text%20generation.%20We%20identify%2C%0Afor%20the%20first%20time%2C%20an%20underlying%20numerical%20issue%2C%20even%20with%20the%20commonly%20used%0A32-bit%20floating-point%20precision%2C%20which%20results%20in%20inaccurate%20categorical%0Asampling.%20We%20show%20that%20it%20lowers%20the%20effective%20temperature%20both%20theoretically%0Aand%20empirically%2C%20and%20the%20resulting%20decrease%20in%20token%20diversity%20makes%20previous%0Aevaluations%2C%20which%20assess%20the%20generation%20quality%20solely%20through%20the%20incomplete%0Agenerative%20perplexity%20metric%2C%20somewhat%20unfair.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02908v4&entry.124074799=Read"},
{"title": "R-LLaVA: Improving Med-VQA Understanding through Visual Region of\n  Interest", "author": "Xupeng Chen and Zhixin Lai and Kangrui Ruan and Shichu Chen and Jiaxiang Liu and Zuozhu Liu", "abstract": "  Artificial intelligence has made significant strides in medical visual\nquestion answering (Med-VQA), yet prevalent studies often interpret images\nholistically, overlooking the visual regions of interest that may contain\ncrucial information, potentially aligning with a doctor's prior knowledge that\ncan be incorporated with minimal annotations (e.g., bounding boxes). To address\nthis gap, this paper introduces R-LLaVA, designed to enhance biomedical VQA\nunderstanding by integrating simple medical annotations as prior knowledge\ndirectly into the image space through CLIP. These annotated visual regions of\ninterest are then fed into the LLaVA model during training, aiming to enrich\nthe model's understanding of biomedical queries. Experimental evaluation on\nfour standard Med-VQA datasets demonstrates R-LLaVA's superiority over existing\nstate-of-the-art (SoTA) methods. Additionally, to verify the model's capability\nin visual comprehension, a novel multiple-choice medical visual understanding\ndataset is introduced, confirming the positive impact of focusing on visual\nregions of interest in advancing biomedical VQA understanding.\n", "link": "http://arxiv.org/abs/2410.20327v4", "date": "2025-02-07", "relevancy": 2.1885, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5461}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20R-LLaVA%3A%20Improving%20Med-VQA%20Understanding%20through%20Visual%20Region%20of%0A%20%20Interest&body=Title%3A%20R-LLaVA%3A%20Improving%20Med-VQA%20Understanding%20through%20Visual%20Region%20of%0A%20%20Interest%0AAuthor%3A%20Xupeng%20Chen%20and%20Zhixin%20Lai%20and%20Kangrui%20Ruan%20and%20Shichu%20Chen%20and%20Jiaxiang%20Liu%20and%20Zuozhu%20Liu%0AAbstract%3A%20%20%20Artificial%20intelligence%20has%20made%20significant%20strides%20in%20medical%20visual%0Aquestion%20answering%20%28Med-VQA%29%2C%20yet%20prevalent%20studies%20often%20interpret%20images%0Aholistically%2C%20overlooking%20the%20visual%20regions%20of%20interest%20that%20may%20contain%0Acrucial%20information%2C%20potentially%20aligning%20with%20a%20doctor%27s%20prior%20knowledge%20that%0Acan%20be%20incorporated%20with%20minimal%20annotations%20%28e.g.%2C%20bounding%20boxes%29.%20To%20address%0Athis%20gap%2C%20this%20paper%20introduces%20R-LLaVA%2C%20designed%20to%20enhance%20biomedical%20VQA%0Aunderstanding%20by%20integrating%20simple%20medical%20annotations%20as%20prior%20knowledge%0Adirectly%20into%20the%20image%20space%20through%20CLIP.%20These%20annotated%20visual%20regions%20of%0Ainterest%20are%20then%20fed%20into%20the%20LLaVA%20model%20during%20training%2C%20aiming%20to%20enrich%0Athe%20model%27s%20understanding%20of%20biomedical%20queries.%20Experimental%20evaluation%20on%0Afour%20standard%20Med-VQA%20datasets%20demonstrates%20R-LLaVA%27s%20superiority%20over%20existing%0Astate-of-the-art%20%28SoTA%29%20methods.%20Additionally%2C%20to%20verify%20the%20model%27s%20capability%0Ain%20visual%20comprehension%2C%20a%20novel%20multiple-choice%20medical%20visual%20understanding%0Adataset%20is%20introduced%2C%20confirming%20the%20positive%20impact%20of%20focusing%20on%20visual%0Aregions%20of%20interest%20in%20advancing%20biomedical%20VQA%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.20327v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DR-LLaVA%253A%2520Improving%2520Med-VQA%2520Understanding%2520through%2520Visual%2520Region%2520of%250A%2520%2520Interest%26entry.906535625%3DXupeng%2520Chen%2520and%2520Zhixin%2520Lai%2520and%2520Kangrui%2520Ruan%2520and%2520Shichu%2520Chen%2520and%2520Jiaxiang%2520Liu%2520and%2520Zuozhu%2520Liu%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520has%2520made%2520significant%2520strides%2520in%2520medical%2520visual%250Aquestion%2520answering%2520%2528Med-VQA%2529%252C%2520yet%2520prevalent%2520studies%2520often%2520interpret%2520images%250Aholistically%252C%2520overlooking%2520the%2520visual%2520regions%2520of%2520interest%2520that%2520may%2520contain%250Acrucial%2520information%252C%2520potentially%2520aligning%2520with%2520a%2520doctor%2527s%2520prior%2520knowledge%2520that%250Acan%2520be%2520incorporated%2520with%2520minimal%2520annotations%2520%2528e.g.%252C%2520bounding%2520boxes%2529.%2520To%2520address%250Athis%2520gap%252C%2520this%2520paper%2520introduces%2520R-LLaVA%252C%2520designed%2520to%2520enhance%2520biomedical%2520VQA%250Aunderstanding%2520by%2520integrating%2520simple%2520medical%2520annotations%2520as%2520prior%2520knowledge%250Adirectly%2520into%2520the%2520image%2520space%2520through%2520CLIP.%2520These%2520annotated%2520visual%2520regions%2520of%250Ainterest%2520are%2520then%2520fed%2520into%2520the%2520LLaVA%2520model%2520during%2520training%252C%2520aiming%2520to%2520enrich%250Athe%2520model%2527s%2520understanding%2520of%2520biomedical%2520queries.%2520Experimental%2520evaluation%2520on%250Afour%2520standard%2520Med-VQA%2520datasets%2520demonstrates%2520R-LLaVA%2527s%2520superiority%2520over%2520existing%250Astate-of-the-art%2520%2528SoTA%2529%2520methods.%2520Additionally%252C%2520to%2520verify%2520the%2520model%2527s%2520capability%250Ain%2520visual%2520comprehension%252C%2520a%2520novel%2520multiple-choice%2520medical%2520visual%2520understanding%250Adataset%2520is%2520introduced%252C%2520confirming%2520the%2520positive%2520impact%2520of%2520focusing%2520on%2520visual%250Aregions%2520of%2520interest%2520in%2520advancing%2520biomedical%2520VQA%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.20327v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=R-LLaVA%3A%20Improving%20Med-VQA%20Understanding%20through%20Visual%20Region%20of%0A%20%20Interest&entry.906535625=Xupeng%20Chen%20and%20Zhixin%20Lai%20and%20Kangrui%20Ruan%20and%20Shichu%20Chen%20and%20Jiaxiang%20Liu%20and%20Zuozhu%20Liu&entry.1292438233=%20%20Artificial%20intelligence%20has%20made%20significant%20strides%20in%20medical%20visual%0Aquestion%20answering%20%28Med-VQA%29%2C%20yet%20prevalent%20studies%20often%20interpret%20images%0Aholistically%2C%20overlooking%20the%20visual%20regions%20of%20interest%20that%20may%20contain%0Acrucial%20information%2C%20potentially%20aligning%20with%20a%20doctor%27s%20prior%20knowledge%20that%0Acan%20be%20incorporated%20with%20minimal%20annotations%20%28e.g.%2C%20bounding%20boxes%29.%20To%20address%0Athis%20gap%2C%20this%20paper%20introduces%20R-LLaVA%2C%20designed%20to%20enhance%20biomedical%20VQA%0Aunderstanding%20by%20integrating%20simple%20medical%20annotations%20as%20prior%20knowledge%0Adirectly%20into%20the%20image%20space%20through%20CLIP.%20These%20annotated%20visual%20regions%20of%0Ainterest%20are%20then%20fed%20into%20the%20LLaVA%20model%20during%20training%2C%20aiming%20to%20enrich%0Athe%20model%27s%20understanding%20of%20biomedical%20queries.%20Experimental%20evaluation%20on%0Afour%20standard%20Med-VQA%20datasets%20demonstrates%20R-LLaVA%27s%20superiority%20over%20existing%0Astate-of-the-art%20%28SoTA%29%20methods.%20Additionally%2C%20to%20verify%20the%20model%27s%20capability%0Ain%20visual%20comprehension%2C%20a%20novel%20multiple-choice%20medical%20visual%20understanding%0Adataset%20is%20introduced%2C%20confirming%20the%20positive%20impact%20of%20focusing%20on%20visual%0Aregions%20of%20interest%20in%20advancing%20biomedical%20VQA%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.20327v4&entry.124074799=Read"},
{"title": "VideoRoPE: What Makes for Good Video Rotary Position Embedding?", "author": "Xilin Wei and Xiaoran Liu and Yuhang Zang and Xiaoyi Dong and Pan Zhang and Yuhang Cao and Jian Tong and Haodong Duan and Qipeng Guo and Jiaqi Wang and Xipeng Qiu and Dahua Lin", "abstract": "  While Rotary Position Embedding (RoPE) and its variants are widely adopted\nfor their long-context capabilities, the extension of the 1D RoPE to video,\nwith its complex spatio-temporal structure, remains an open challenge. This\nwork first introduces a comprehensive analysis that identifies four key\ncharacteristics essential for the effective adaptation of RoPE to video, which\nhave not been fully considered in prior work. As part of our analysis, we\nintroduce a challenging V-NIAH-D (Visual Needle-In-A-Haystack with Distractors)\ntask, which adds periodic distractors into V-NIAH. The V-NIAH-D task\ndemonstrates that previous RoPE variants, lacking appropriate temporal\ndimension allocation, are easily misled by distractors. Based on our analysis,\nwe introduce \\textbf{VideoRoPE}, with a \\textit{3D structure} designed to\npreserve spatio-temporal relationships. VideoRoPE features\n\\textit{low-frequency temporal allocation} to mitigate periodic oscillations, a\n\\textit{diagonal layout} to maintain spatial symmetry, and \\textit{adjustable\ntemporal spacing} to decouple temporal and spatial indexing. VideoRoPE\nconsistently surpasses previous RoPE variants, across diverse downstream tasks\nsuch as long video retrieval, video understanding, and video hallucination. Our\ncode will be available at\n\\href{https://github.com/Wiselnn570/VideoRoPE}{https://github.com/Wiselnn570/VideoRoPE}.\n", "link": "http://arxiv.org/abs/2502.05173v1", "date": "2025-02-07", "relevancy": 2.1855, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5528}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5451}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoRoPE%3A%20What%20Makes%20for%20Good%20Video%20Rotary%20Position%20Embedding%3F&body=Title%3A%20VideoRoPE%3A%20What%20Makes%20for%20Good%20Video%20Rotary%20Position%20Embedding%3F%0AAuthor%3A%20Xilin%20Wei%20and%20Xiaoran%20Liu%20and%20Yuhang%20Zang%20and%20Xiaoyi%20Dong%20and%20Pan%20Zhang%20and%20Yuhang%20Cao%20and%20Jian%20Tong%20and%20Haodong%20Duan%20and%20Qipeng%20Guo%20and%20Jiaqi%20Wang%20and%20Xipeng%20Qiu%20and%20Dahua%20Lin%0AAbstract%3A%20%20%20While%20Rotary%20Position%20Embedding%20%28RoPE%29%20and%20its%20variants%20are%20widely%20adopted%0Afor%20their%20long-context%20capabilities%2C%20the%20extension%20of%20the%201D%20RoPE%20to%20video%2C%0Awith%20its%20complex%20spatio-temporal%20structure%2C%20remains%20an%20open%20challenge.%20This%0Awork%20first%20introduces%20a%20comprehensive%20analysis%20that%20identifies%20four%20key%0Acharacteristics%20essential%20for%20the%20effective%20adaptation%20of%20RoPE%20to%20video%2C%20which%0Ahave%20not%20been%20fully%20considered%20in%20prior%20work.%20As%20part%20of%20our%20analysis%2C%20we%0Aintroduce%20a%20challenging%20V-NIAH-D%20%28Visual%20Needle-In-A-Haystack%20with%20Distractors%29%0Atask%2C%20which%20adds%20periodic%20distractors%20into%20V-NIAH.%20The%20V-NIAH-D%20task%0Ademonstrates%20that%20previous%20RoPE%20variants%2C%20lacking%20appropriate%20temporal%0Adimension%20allocation%2C%20are%20easily%20misled%20by%20distractors.%20Based%20on%20our%20analysis%2C%0Awe%20introduce%20%5Ctextbf%7BVideoRoPE%7D%2C%20with%20a%20%5Ctextit%7B3D%20structure%7D%20designed%20to%0Apreserve%20spatio-temporal%20relationships.%20VideoRoPE%20features%0A%5Ctextit%7Blow-frequency%20temporal%20allocation%7D%20to%20mitigate%20periodic%20oscillations%2C%20a%0A%5Ctextit%7Bdiagonal%20layout%7D%20to%20maintain%20spatial%20symmetry%2C%20and%20%5Ctextit%7Badjustable%0Atemporal%20spacing%7D%20to%20decouple%20temporal%20and%20spatial%20indexing.%20VideoRoPE%0Aconsistently%20surpasses%20previous%20RoPE%20variants%2C%20across%20diverse%20downstream%20tasks%0Asuch%20as%20long%20video%20retrieval%2C%20video%20understanding%2C%20and%20video%20hallucination.%20Our%0Acode%20will%20be%20available%20at%0A%5Chref%7Bhttps%3A//github.com/Wiselnn570/VideoRoPE%7D%7Bhttps%3A//github.com/Wiselnn570/VideoRoPE%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05173v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoRoPE%253A%2520What%2520Makes%2520for%2520Good%2520Video%2520Rotary%2520Position%2520Embedding%253F%26entry.906535625%3DXilin%2520Wei%2520and%2520Xiaoran%2520Liu%2520and%2520Yuhang%2520Zang%2520and%2520Xiaoyi%2520Dong%2520and%2520Pan%2520Zhang%2520and%2520Yuhang%2520Cao%2520and%2520Jian%2520Tong%2520and%2520Haodong%2520Duan%2520and%2520Qipeng%2520Guo%2520and%2520Jiaqi%2520Wang%2520and%2520Xipeng%2520Qiu%2520and%2520Dahua%2520Lin%26entry.1292438233%3D%2520%2520While%2520Rotary%2520Position%2520Embedding%2520%2528RoPE%2529%2520and%2520its%2520variants%2520are%2520widely%2520adopted%250Afor%2520their%2520long-context%2520capabilities%252C%2520the%2520extension%2520of%2520the%25201D%2520RoPE%2520to%2520video%252C%250Awith%2520its%2520complex%2520spatio-temporal%2520structure%252C%2520remains%2520an%2520open%2520challenge.%2520This%250Awork%2520first%2520introduces%2520a%2520comprehensive%2520analysis%2520that%2520identifies%2520four%2520key%250Acharacteristics%2520essential%2520for%2520the%2520effective%2520adaptation%2520of%2520RoPE%2520to%2520video%252C%2520which%250Ahave%2520not%2520been%2520fully%2520considered%2520in%2520prior%2520work.%2520As%2520part%2520of%2520our%2520analysis%252C%2520we%250Aintroduce%2520a%2520challenging%2520V-NIAH-D%2520%2528Visual%2520Needle-In-A-Haystack%2520with%2520Distractors%2529%250Atask%252C%2520which%2520adds%2520periodic%2520distractors%2520into%2520V-NIAH.%2520The%2520V-NIAH-D%2520task%250Ademonstrates%2520that%2520previous%2520RoPE%2520variants%252C%2520lacking%2520appropriate%2520temporal%250Adimension%2520allocation%252C%2520are%2520easily%2520misled%2520by%2520distractors.%2520Based%2520on%2520our%2520analysis%252C%250Awe%2520introduce%2520%255Ctextbf%257BVideoRoPE%257D%252C%2520with%2520a%2520%255Ctextit%257B3D%2520structure%257D%2520designed%2520to%250Apreserve%2520spatio-temporal%2520relationships.%2520VideoRoPE%2520features%250A%255Ctextit%257Blow-frequency%2520temporal%2520allocation%257D%2520to%2520mitigate%2520periodic%2520oscillations%252C%2520a%250A%255Ctextit%257Bdiagonal%2520layout%257D%2520to%2520maintain%2520spatial%2520symmetry%252C%2520and%2520%255Ctextit%257Badjustable%250Atemporal%2520spacing%257D%2520to%2520decouple%2520temporal%2520and%2520spatial%2520indexing.%2520VideoRoPE%250Aconsistently%2520surpasses%2520previous%2520RoPE%2520variants%252C%2520across%2520diverse%2520downstream%2520tasks%250Asuch%2520as%2520long%2520video%2520retrieval%252C%2520video%2520understanding%252C%2520and%2520video%2520hallucination.%2520Our%250Acode%2520will%2520be%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/Wiselnn570/VideoRoPE%257D%257Bhttps%253A//github.com/Wiselnn570/VideoRoPE%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05173v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoRoPE%3A%20What%20Makes%20for%20Good%20Video%20Rotary%20Position%20Embedding%3F&entry.906535625=Xilin%20Wei%20and%20Xiaoran%20Liu%20and%20Yuhang%20Zang%20and%20Xiaoyi%20Dong%20and%20Pan%20Zhang%20and%20Yuhang%20Cao%20and%20Jian%20Tong%20and%20Haodong%20Duan%20and%20Qipeng%20Guo%20and%20Jiaqi%20Wang%20and%20Xipeng%20Qiu%20and%20Dahua%20Lin&entry.1292438233=%20%20While%20Rotary%20Position%20Embedding%20%28RoPE%29%20and%20its%20variants%20are%20widely%20adopted%0Afor%20their%20long-context%20capabilities%2C%20the%20extension%20of%20the%201D%20RoPE%20to%20video%2C%0Awith%20its%20complex%20spatio-temporal%20structure%2C%20remains%20an%20open%20challenge.%20This%0Awork%20first%20introduces%20a%20comprehensive%20analysis%20that%20identifies%20four%20key%0Acharacteristics%20essential%20for%20the%20effective%20adaptation%20of%20RoPE%20to%20video%2C%20which%0Ahave%20not%20been%20fully%20considered%20in%20prior%20work.%20As%20part%20of%20our%20analysis%2C%20we%0Aintroduce%20a%20challenging%20V-NIAH-D%20%28Visual%20Needle-In-A-Haystack%20with%20Distractors%29%0Atask%2C%20which%20adds%20periodic%20distractors%20into%20V-NIAH.%20The%20V-NIAH-D%20task%0Ademonstrates%20that%20previous%20RoPE%20variants%2C%20lacking%20appropriate%20temporal%0Adimension%20allocation%2C%20are%20easily%20misled%20by%20distractors.%20Based%20on%20our%20analysis%2C%0Awe%20introduce%20%5Ctextbf%7BVideoRoPE%7D%2C%20with%20a%20%5Ctextit%7B3D%20structure%7D%20designed%20to%0Apreserve%20spatio-temporal%20relationships.%20VideoRoPE%20features%0A%5Ctextit%7Blow-frequency%20temporal%20allocation%7D%20to%20mitigate%20periodic%20oscillations%2C%20a%0A%5Ctextit%7Bdiagonal%20layout%7D%20to%20maintain%20spatial%20symmetry%2C%20and%20%5Ctextit%7Badjustable%0Atemporal%20spacing%7D%20to%20decouple%20temporal%20and%20spatial%20indexing.%20VideoRoPE%0Aconsistently%20surpasses%20previous%20RoPE%20variants%2C%20across%20diverse%20downstream%20tasks%0Asuch%20as%20long%20video%20retrieval%2C%20video%20understanding%2C%20and%20video%20hallucination.%20Our%0Acode%20will%20be%20available%20at%0A%5Chref%7Bhttps%3A//github.com/Wiselnn570/VideoRoPE%7D%7Bhttps%3A//github.com/Wiselnn570/VideoRoPE%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05173v1&entry.124074799=Read"},
{"title": "Self-supervised Conformal Prediction for Uncertainty Quantification in\n  Imaging Problems", "author": "Jasper M. Everink and Bernardin Tamo Amougou and Marcelo Pereyra", "abstract": "  Most image restoration problems are ill-conditioned or ill-posed and hence\ninvolve significant uncertainty. Quantifying this uncertainty is crucial for\nreliably interpreting experimental results, particularly when reconstructed\nimages inform critical decisions and science. However, most existing image\nrestoration methods either fail to quantify uncertainty or provide estimates\nthat are highly inaccurate. Conformal prediction has recently emerged as a\nflexible framework to equip any estimator with uncertainty quantification\ncapabilities that, by construction, have nearly exact marginal coverage. To\nachieve this, conformal prediction relies on abundant ground truth data for\ncalibration. However, in image restoration problems, reliable ground truth data\nis often expensive or not possible to acquire. Also, reliance on ground truth\ndata can introduce large biases in situations of distribution shift between\ncalibration and deployment. This paper seeks to develop a more robust approach\nto conformal prediction for image restoration problems by proposing a\nself-supervised conformal prediction method that leverages Stein's Unbiased\nRisk Estimator (SURE) to self-calibrate itself directly from the observed noisy\nmeasurements, bypassing the need for ground truth. The method is suitable for\nany linear imaging inverse problem that is ill-conditioned, and it is\nespecially powerful when used with modern self-supervised image restoration\ntechniques that can also be trained directly from measurement data. The\nproposed approach is demonstrated through numerical experiments on image\ndenoising and deblurring, where it delivers results that are remarkably\naccurate and comparable to those obtained by supervised conformal prediction\nwith ground truth data.\n", "link": "http://arxiv.org/abs/2502.05127v1", "date": "2025-02-07", "relevancy": 2.171, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5534}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5511}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5301}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20Conformal%20Prediction%20for%20Uncertainty%20Quantification%20in%0A%20%20Imaging%20Problems&body=Title%3A%20Self-supervised%20Conformal%20Prediction%20for%20Uncertainty%20Quantification%20in%0A%20%20Imaging%20Problems%0AAuthor%3A%20Jasper%20M.%20Everink%20and%20Bernardin%20Tamo%20Amougou%20and%20Marcelo%20Pereyra%0AAbstract%3A%20%20%20Most%20image%20restoration%20problems%20are%20ill-conditioned%20or%20ill-posed%20and%20hence%0Ainvolve%20significant%20uncertainty.%20Quantifying%20this%20uncertainty%20is%20crucial%20for%0Areliably%20interpreting%20experimental%20results%2C%20particularly%20when%20reconstructed%0Aimages%20inform%20critical%20decisions%20and%20science.%20However%2C%20most%20existing%20image%0Arestoration%20methods%20either%20fail%20to%20quantify%20uncertainty%20or%20provide%20estimates%0Athat%20are%20highly%20inaccurate.%20Conformal%20prediction%20has%20recently%20emerged%20as%20a%0Aflexible%20framework%20to%20equip%20any%20estimator%20with%20uncertainty%20quantification%0Acapabilities%20that%2C%20by%20construction%2C%20have%20nearly%20exact%20marginal%20coverage.%20To%0Aachieve%20this%2C%20conformal%20prediction%20relies%20on%20abundant%20ground%20truth%20data%20for%0Acalibration.%20However%2C%20in%20image%20restoration%20problems%2C%20reliable%20ground%20truth%20data%0Ais%20often%20expensive%20or%20not%20possible%20to%20acquire.%20Also%2C%20reliance%20on%20ground%20truth%0Adata%20can%20introduce%20large%20biases%20in%20situations%20of%20distribution%20shift%20between%0Acalibration%20and%20deployment.%20This%20paper%20seeks%20to%20develop%20a%20more%20robust%20approach%0Ato%20conformal%20prediction%20for%20image%20restoration%20problems%20by%20proposing%20a%0Aself-supervised%20conformal%20prediction%20method%20that%20leverages%20Stein%27s%20Unbiased%0ARisk%20Estimator%20%28SURE%29%20to%20self-calibrate%20itself%20directly%20from%20the%20observed%20noisy%0Ameasurements%2C%20bypassing%20the%20need%20for%20ground%20truth.%20The%20method%20is%20suitable%20for%0Aany%20linear%20imaging%20inverse%20problem%20that%20is%20ill-conditioned%2C%20and%20it%20is%0Aespecially%20powerful%20when%20used%20with%20modern%20self-supervised%20image%20restoration%0Atechniques%20that%20can%20also%20be%20trained%20directly%20from%20measurement%20data.%20The%0Aproposed%20approach%20is%20demonstrated%20through%20numerical%20experiments%20on%20image%0Adenoising%20and%20deblurring%2C%20where%20it%20delivers%20results%20that%20are%20remarkably%0Aaccurate%20and%20comparable%20to%20those%20obtained%20by%20supervised%20conformal%20prediction%0Awith%20ground%20truth%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05127v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520Conformal%2520Prediction%2520for%2520Uncertainty%2520Quantification%2520in%250A%2520%2520Imaging%2520Problems%26entry.906535625%3DJasper%2520M.%2520Everink%2520and%2520Bernardin%2520Tamo%2520Amougou%2520and%2520Marcelo%2520Pereyra%26entry.1292438233%3D%2520%2520Most%2520image%2520restoration%2520problems%2520are%2520ill-conditioned%2520or%2520ill-posed%2520and%2520hence%250Ainvolve%2520significant%2520uncertainty.%2520Quantifying%2520this%2520uncertainty%2520is%2520crucial%2520for%250Areliably%2520interpreting%2520experimental%2520results%252C%2520particularly%2520when%2520reconstructed%250Aimages%2520inform%2520critical%2520decisions%2520and%2520science.%2520However%252C%2520most%2520existing%2520image%250Arestoration%2520methods%2520either%2520fail%2520to%2520quantify%2520uncertainty%2520or%2520provide%2520estimates%250Athat%2520are%2520highly%2520inaccurate.%2520Conformal%2520prediction%2520has%2520recently%2520emerged%2520as%2520a%250Aflexible%2520framework%2520to%2520equip%2520any%2520estimator%2520with%2520uncertainty%2520quantification%250Acapabilities%2520that%252C%2520by%2520construction%252C%2520have%2520nearly%2520exact%2520marginal%2520coverage.%2520To%250Aachieve%2520this%252C%2520conformal%2520prediction%2520relies%2520on%2520abundant%2520ground%2520truth%2520data%2520for%250Acalibration.%2520However%252C%2520in%2520image%2520restoration%2520problems%252C%2520reliable%2520ground%2520truth%2520data%250Ais%2520often%2520expensive%2520or%2520not%2520possible%2520to%2520acquire.%2520Also%252C%2520reliance%2520on%2520ground%2520truth%250Adata%2520can%2520introduce%2520large%2520biases%2520in%2520situations%2520of%2520distribution%2520shift%2520between%250Acalibration%2520and%2520deployment.%2520This%2520paper%2520seeks%2520to%2520develop%2520a%2520more%2520robust%2520approach%250Ato%2520conformal%2520prediction%2520for%2520image%2520restoration%2520problems%2520by%2520proposing%2520a%250Aself-supervised%2520conformal%2520prediction%2520method%2520that%2520leverages%2520Stein%2527s%2520Unbiased%250ARisk%2520Estimator%2520%2528SURE%2529%2520to%2520self-calibrate%2520itself%2520directly%2520from%2520the%2520observed%2520noisy%250Ameasurements%252C%2520bypassing%2520the%2520need%2520for%2520ground%2520truth.%2520The%2520method%2520is%2520suitable%2520for%250Aany%2520linear%2520imaging%2520inverse%2520problem%2520that%2520is%2520ill-conditioned%252C%2520and%2520it%2520is%250Aespecially%2520powerful%2520when%2520used%2520with%2520modern%2520self-supervised%2520image%2520restoration%250Atechniques%2520that%2520can%2520also%2520be%2520trained%2520directly%2520from%2520measurement%2520data.%2520The%250Aproposed%2520approach%2520is%2520demonstrated%2520through%2520numerical%2520experiments%2520on%2520image%250Adenoising%2520and%2520deblurring%252C%2520where%2520it%2520delivers%2520results%2520that%2520are%2520remarkably%250Aaccurate%2520and%2520comparable%2520to%2520those%2520obtained%2520by%2520supervised%2520conformal%2520prediction%250Awith%2520ground%2520truth%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05127v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20Conformal%20Prediction%20for%20Uncertainty%20Quantification%20in%0A%20%20Imaging%20Problems&entry.906535625=Jasper%20M.%20Everink%20and%20Bernardin%20Tamo%20Amougou%20and%20Marcelo%20Pereyra&entry.1292438233=%20%20Most%20image%20restoration%20problems%20are%20ill-conditioned%20or%20ill-posed%20and%20hence%0Ainvolve%20significant%20uncertainty.%20Quantifying%20this%20uncertainty%20is%20crucial%20for%0Areliably%20interpreting%20experimental%20results%2C%20particularly%20when%20reconstructed%0Aimages%20inform%20critical%20decisions%20and%20science.%20However%2C%20most%20existing%20image%0Arestoration%20methods%20either%20fail%20to%20quantify%20uncertainty%20or%20provide%20estimates%0Athat%20are%20highly%20inaccurate.%20Conformal%20prediction%20has%20recently%20emerged%20as%20a%0Aflexible%20framework%20to%20equip%20any%20estimator%20with%20uncertainty%20quantification%0Acapabilities%20that%2C%20by%20construction%2C%20have%20nearly%20exact%20marginal%20coverage.%20To%0Aachieve%20this%2C%20conformal%20prediction%20relies%20on%20abundant%20ground%20truth%20data%20for%0Acalibration.%20However%2C%20in%20image%20restoration%20problems%2C%20reliable%20ground%20truth%20data%0Ais%20often%20expensive%20or%20not%20possible%20to%20acquire.%20Also%2C%20reliance%20on%20ground%20truth%0Adata%20can%20introduce%20large%20biases%20in%20situations%20of%20distribution%20shift%20between%0Acalibration%20and%20deployment.%20This%20paper%20seeks%20to%20develop%20a%20more%20robust%20approach%0Ato%20conformal%20prediction%20for%20image%20restoration%20problems%20by%20proposing%20a%0Aself-supervised%20conformal%20prediction%20method%20that%20leverages%20Stein%27s%20Unbiased%0ARisk%20Estimator%20%28SURE%29%20to%20self-calibrate%20itself%20directly%20from%20the%20observed%20noisy%0Ameasurements%2C%20bypassing%20the%20need%20for%20ground%20truth.%20The%20method%20is%20suitable%20for%0Aany%20linear%20imaging%20inverse%20problem%20that%20is%20ill-conditioned%2C%20and%20it%20is%0Aespecially%20powerful%20when%20used%20with%20modern%20self-supervised%20image%20restoration%0Atechniques%20that%20can%20also%20be%20trained%20directly%20from%20measurement%20data.%20The%0Aproposed%20approach%20is%20demonstrated%20through%20numerical%20experiments%20on%20image%0Adenoising%20and%20deblurring%2C%20where%20it%20delivers%20results%20that%20are%20remarkably%0Aaccurate%20and%20comparable%20to%20those%20obtained%20by%20supervised%20conformal%20prediction%0Awith%20ground%20truth%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05127v1&entry.124074799=Read"},
{"title": "CurbNet: Curb Detection Framework Based on LiDAR Point Cloud\n  Segmentation", "author": "Guoyang Zhao and Fulong Ma and Weiqing Qi and Yuxuan Liu and Ming Liu and Jun Ma", "abstract": "  Curb detection is a crucial function in intelligent driving, essential for\ndetermining drivable areas on the road. However, the complexity of road\nenvironments makes curb detection challenging. This paper introduces CurbNet, a\nnovel framework for curb detection utilizing point cloud segmentation. To\naddress the lack of comprehensive curb datasets with 3D annotations, we have\ndeveloped the 3D-Curb dataset based on SemanticKITTI, currently the largest and\nmost diverse collection of curb point clouds. Recognizing that the primary\ncharacteristic of curbs is height variation, our approach leverages spatially\nrich 3D point clouds for training. To tackle the challenges posed by the uneven\ndistribution of curb features on the xy-plane and their dependence on\nhigh-frequency features along the z-axis, we introduce the Multi-Scale and\nChannel Attention (MSCA) module, a customized solution designed to optimize\ndetection performance. Additionally, we propose an adaptive weighted loss\nfunction group specifically formulated to counteract the imbalance in the\ndistribution of curb point clouds relative to other categories. Extensive\nexperiments conducted on 2 major datasets demonstrate that our method surpasses\nexisting benchmarks set by leading curb detection and point cloud segmentation\nmodels. Through the post-processing refinement of the detection results, we\nhave significantly reduced noise in curb detection, thereby improving precision\nby 4.5 points. Similarly, our tolerance experiments also achieve\nstate-of-the-art results. Furthermore, real-world experiments and dataset\nanalyses mutually validate each other, reinforcing CurbNet's superior detection\ncapability and robust generalizability. The project website is available at:\nhttps://github.com/guoyangzhao/CurbNet/.\n", "link": "http://arxiv.org/abs/2403.16794v3", "date": "2025-02-07", "relevancy": 2.1706, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5593}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.532}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CurbNet%3A%20Curb%20Detection%20Framework%20Based%20on%20LiDAR%20Point%20Cloud%0A%20%20Segmentation&body=Title%3A%20CurbNet%3A%20Curb%20Detection%20Framework%20Based%20on%20LiDAR%20Point%20Cloud%0A%20%20Segmentation%0AAuthor%3A%20Guoyang%20Zhao%20and%20Fulong%20Ma%20and%20Weiqing%20Qi%20and%20Yuxuan%20Liu%20and%20Ming%20Liu%20and%20Jun%20Ma%0AAbstract%3A%20%20%20Curb%20detection%20is%20a%20crucial%20function%20in%20intelligent%20driving%2C%20essential%20for%0Adetermining%20drivable%20areas%20on%20the%20road.%20However%2C%20the%20complexity%20of%20road%0Aenvironments%20makes%20curb%20detection%20challenging.%20This%20paper%20introduces%20CurbNet%2C%20a%0Anovel%20framework%20for%20curb%20detection%20utilizing%20point%20cloud%20segmentation.%20To%0Aaddress%20the%20lack%20of%20comprehensive%20curb%20datasets%20with%203D%20annotations%2C%20we%20have%0Adeveloped%20the%203D-Curb%20dataset%20based%20on%20SemanticKITTI%2C%20currently%20the%20largest%20and%0Amost%20diverse%20collection%20of%20curb%20point%20clouds.%20Recognizing%20that%20the%20primary%0Acharacteristic%20of%20curbs%20is%20height%20variation%2C%20our%20approach%20leverages%20spatially%0Arich%203D%20point%20clouds%20for%20training.%20To%20tackle%20the%20challenges%20posed%20by%20the%20uneven%0Adistribution%20of%20curb%20features%20on%20the%20xy-plane%20and%20their%20dependence%20on%0Ahigh-frequency%20features%20along%20the%20z-axis%2C%20we%20introduce%20the%20Multi-Scale%20and%0AChannel%20Attention%20%28MSCA%29%20module%2C%20a%20customized%20solution%20designed%20to%20optimize%0Adetection%20performance.%20Additionally%2C%20we%20propose%20an%20adaptive%20weighted%20loss%0Afunction%20group%20specifically%20formulated%20to%20counteract%20the%20imbalance%20in%20the%0Adistribution%20of%20curb%20point%20clouds%20relative%20to%20other%20categories.%20Extensive%0Aexperiments%20conducted%20on%202%20major%20datasets%20demonstrate%20that%20our%20method%20surpasses%0Aexisting%20benchmarks%20set%20by%20leading%20curb%20detection%20and%20point%20cloud%20segmentation%0Amodels.%20Through%20the%20post-processing%20refinement%20of%20the%20detection%20results%2C%20we%0Ahave%20significantly%20reduced%20noise%20in%20curb%20detection%2C%20thereby%20improving%20precision%0Aby%204.5%20points.%20Similarly%2C%20our%20tolerance%20experiments%20also%20achieve%0Astate-of-the-art%20results.%20Furthermore%2C%20real-world%20experiments%20and%20dataset%0Aanalyses%20mutually%20validate%20each%20other%2C%20reinforcing%20CurbNet%27s%20superior%20detection%0Acapability%20and%20robust%20generalizability.%20The%20project%20website%20is%20available%20at%3A%0Ahttps%3A//github.com/guoyangzhao/CurbNet/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16794v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCurbNet%253A%2520Curb%2520Detection%2520Framework%2520Based%2520on%2520LiDAR%2520Point%2520Cloud%250A%2520%2520Segmentation%26entry.906535625%3DGuoyang%2520Zhao%2520and%2520Fulong%2520Ma%2520and%2520Weiqing%2520Qi%2520and%2520Yuxuan%2520Liu%2520and%2520Ming%2520Liu%2520and%2520Jun%2520Ma%26entry.1292438233%3D%2520%2520Curb%2520detection%2520is%2520a%2520crucial%2520function%2520in%2520intelligent%2520driving%252C%2520essential%2520for%250Adetermining%2520drivable%2520areas%2520on%2520the%2520road.%2520However%252C%2520the%2520complexity%2520of%2520road%250Aenvironments%2520makes%2520curb%2520detection%2520challenging.%2520This%2520paper%2520introduces%2520CurbNet%252C%2520a%250Anovel%2520framework%2520for%2520curb%2520detection%2520utilizing%2520point%2520cloud%2520segmentation.%2520To%250Aaddress%2520the%2520lack%2520of%2520comprehensive%2520curb%2520datasets%2520with%25203D%2520annotations%252C%2520we%2520have%250Adeveloped%2520the%25203D-Curb%2520dataset%2520based%2520on%2520SemanticKITTI%252C%2520currently%2520the%2520largest%2520and%250Amost%2520diverse%2520collection%2520of%2520curb%2520point%2520clouds.%2520Recognizing%2520that%2520the%2520primary%250Acharacteristic%2520of%2520curbs%2520is%2520height%2520variation%252C%2520our%2520approach%2520leverages%2520spatially%250Arich%25203D%2520point%2520clouds%2520for%2520training.%2520To%2520tackle%2520the%2520challenges%2520posed%2520by%2520the%2520uneven%250Adistribution%2520of%2520curb%2520features%2520on%2520the%2520xy-plane%2520and%2520their%2520dependence%2520on%250Ahigh-frequency%2520features%2520along%2520the%2520z-axis%252C%2520we%2520introduce%2520the%2520Multi-Scale%2520and%250AChannel%2520Attention%2520%2528MSCA%2529%2520module%252C%2520a%2520customized%2520solution%2520designed%2520to%2520optimize%250Adetection%2520performance.%2520Additionally%252C%2520we%2520propose%2520an%2520adaptive%2520weighted%2520loss%250Afunction%2520group%2520specifically%2520formulated%2520to%2520counteract%2520the%2520imbalance%2520in%2520the%250Adistribution%2520of%2520curb%2520point%2520clouds%2520relative%2520to%2520other%2520categories.%2520Extensive%250Aexperiments%2520conducted%2520on%25202%2520major%2520datasets%2520demonstrate%2520that%2520our%2520method%2520surpasses%250Aexisting%2520benchmarks%2520set%2520by%2520leading%2520curb%2520detection%2520and%2520point%2520cloud%2520segmentation%250Amodels.%2520Through%2520the%2520post-processing%2520refinement%2520of%2520the%2520detection%2520results%252C%2520we%250Ahave%2520significantly%2520reduced%2520noise%2520in%2520curb%2520detection%252C%2520thereby%2520improving%2520precision%250Aby%25204.5%2520points.%2520Similarly%252C%2520our%2520tolerance%2520experiments%2520also%2520achieve%250Astate-of-the-art%2520results.%2520Furthermore%252C%2520real-world%2520experiments%2520and%2520dataset%250Aanalyses%2520mutually%2520validate%2520each%2520other%252C%2520reinforcing%2520CurbNet%2527s%2520superior%2520detection%250Acapability%2520and%2520robust%2520generalizability.%2520The%2520project%2520website%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/guoyangzhao/CurbNet/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16794v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CurbNet%3A%20Curb%20Detection%20Framework%20Based%20on%20LiDAR%20Point%20Cloud%0A%20%20Segmentation&entry.906535625=Guoyang%20Zhao%20and%20Fulong%20Ma%20and%20Weiqing%20Qi%20and%20Yuxuan%20Liu%20and%20Ming%20Liu%20and%20Jun%20Ma&entry.1292438233=%20%20Curb%20detection%20is%20a%20crucial%20function%20in%20intelligent%20driving%2C%20essential%20for%0Adetermining%20drivable%20areas%20on%20the%20road.%20However%2C%20the%20complexity%20of%20road%0Aenvironments%20makes%20curb%20detection%20challenging.%20This%20paper%20introduces%20CurbNet%2C%20a%0Anovel%20framework%20for%20curb%20detection%20utilizing%20point%20cloud%20segmentation.%20To%0Aaddress%20the%20lack%20of%20comprehensive%20curb%20datasets%20with%203D%20annotations%2C%20we%20have%0Adeveloped%20the%203D-Curb%20dataset%20based%20on%20SemanticKITTI%2C%20currently%20the%20largest%20and%0Amost%20diverse%20collection%20of%20curb%20point%20clouds.%20Recognizing%20that%20the%20primary%0Acharacteristic%20of%20curbs%20is%20height%20variation%2C%20our%20approach%20leverages%20spatially%0Arich%203D%20point%20clouds%20for%20training.%20To%20tackle%20the%20challenges%20posed%20by%20the%20uneven%0Adistribution%20of%20curb%20features%20on%20the%20xy-plane%20and%20their%20dependence%20on%0Ahigh-frequency%20features%20along%20the%20z-axis%2C%20we%20introduce%20the%20Multi-Scale%20and%0AChannel%20Attention%20%28MSCA%29%20module%2C%20a%20customized%20solution%20designed%20to%20optimize%0Adetection%20performance.%20Additionally%2C%20we%20propose%20an%20adaptive%20weighted%20loss%0Afunction%20group%20specifically%20formulated%20to%20counteract%20the%20imbalance%20in%20the%0Adistribution%20of%20curb%20point%20clouds%20relative%20to%20other%20categories.%20Extensive%0Aexperiments%20conducted%20on%202%20major%20datasets%20demonstrate%20that%20our%20method%20surpasses%0Aexisting%20benchmarks%20set%20by%20leading%20curb%20detection%20and%20point%20cloud%20segmentation%0Amodels.%20Through%20the%20post-processing%20refinement%20of%20the%20detection%20results%2C%20we%0Ahave%20significantly%20reduced%20noise%20in%20curb%20detection%2C%20thereby%20improving%20precision%0Aby%204.5%20points.%20Similarly%2C%20our%20tolerance%20experiments%20also%20achieve%0Astate-of-the-art%20results.%20Furthermore%2C%20real-world%20experiments%20and%20dataset%0Aanalyses%20mutually%20validate%20each%20other%2C%20reinforcing%20CurbNet%27s%20superior%20detection%0Acapability%20and%20robust%20generalizability.%20The%20project%20website%20is%20available%20at%3A%0Ahttps%3A//github.com/guoyangzhao/CurbNet/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16794v3&entry.124074799=Read"},
{"title": "A Label Propagation Strategy for CutMix in Multi-Label Remote Sensing\n  Image Classification", "author": "Tom Burgert and Kai Norman Clasen and Jonas Klotz and Tim Siebert and Beg\u00fcm Demir", "abstract": "  The development of supervised deep learning-based methods for multi-label\nscene classification (MLC) is one of the prominent research directions in\nremote sensing (RS). Yet, collecting annotations for large RS image archives is\ntime-consuming and costly. To address this issue, several data augmentation\nmethods have been introduced in RS. Among others, the data augmentation\ntechnique CutMix, which combines parts of two existing training images to\ngenerate an augmented image, stands out as a particularly effective approach.\nHowever, the direct application of CutMix in RS MLC can lead to the erasure or\naddition of class labels (i.e., label noise) in the augmented (i.e., combined)\ntraining image. To address this problem, we introduce a label propagation (LP)\nstrategy that allows the effective application of CutMix in the context of MLC\nproblems in RS without being affected by label noise. To this end, our proposed\nLP strategy exploits pixel-level class positional information to update the\nmulti-label of the augmented training image. We propose to access such class\npositional information from reference maps associated to each training image\n(e.g., thematic products) or from class explanation masks provided by an\nexplanation method if no reference maps are available. Similarly to pairing two\ntraining images, our LP strategy carries out a pairing operation on the\nassociated pixel-level class positional information to derive the updated\nmulti-label for the augmented image. Experimental results show the\neffectiveness of our LP strategy in general and its robustness in the case of\nvarious simulated and real scenarios with noisy class positional information in\nparticular.\n", "link": "http://arxiv.org/abs/2405.13451v2", "date": "2025-02-07", "relevancy": 2.1521, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5517}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5322}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Label%20Propagation%20Strategy%20for%20CutMix%20in%20Multi-Label%20Remote%20Sensing%0A%20%20Image%20Classification&body=Title%3A%20A%20Label%20Propagation%20Strategy%20for%20CutMix%20in%20Multi-Label%20Remote%20Sensing%0A%20%20Image%20Classification%0AAuthor%3A%20Tom%20Burgert%20and%20Kai%20Norman%20Clasen%20and%20Jonas%20Klotz%20and%20Tim%20Siebert%20and%20Beg%C3%BCm%20Demir%0AAbstract%3A%20%20%20The%20development%20of%20supervised%20deep%20learning-based%20methods%20for%20multi-label%0Ascene%20classification%20%28MLC%29%20is%20one%20of%20the%20prominent%20research%20directions%20in%0Aremote%20sensing%20%28RS%29.%20Yet%2C%20collecting%20annotations%20for%20large%20RS%20image%20archives%20is%0Atime-consuming%20and%20costly.%20To%20address%20this%20issue%2C%20several%20data%20augmentation%0Amethods%20have%20been%20introduced%20in%20RS.%20Among%20others%2C%20the%20data%20augmentation%0Atechnique%20CutMix%2C%20which%20combines%20parts%20of%20two%20existing%20training%20images%20to%0Agenerate%20an%20augmented%20image%2C%20stands%20out%20as%20a%20particularly%20effective%20approach.%0AHowever%2C%20the%20direct%20application%20of%20CutMix%20in%20RS%20MLC%20can%20lead%20to%20the%20erasure%20or%0Aaddition%20of%20class%20labels%20%28i.e.%2C%20label%20noise%29%20in%20the%20augmented%20%28i.e.%2C%20combined%29%0Atraining%20image.%20To%20address%20this%20problem%2C%20we%20introduce%20a%20label%20propagation%20%28LP%29%0Astrategy%20that%20allows%20the%20effective%20application%20of%20CutMix%20in%20the%20context%20of%20MLC%0Aproblems%20in%20RS%20without%20being%20affected%20by%20label%20noise.%20To%20this%20end%2C%20our%20proposed%0ALP%20strategy%20exploits%20pixel-level%20class%20positional%20information%20to%20update%20the%0Amulti-label%20of%20the%20augmented%20training%20image.%20We%20propose%20to%20access%20such%20class%0Apositional%20information%20from%20reference%20maps%20associated%20to%20each%20training%20image%0A%28e.g.%2C%20thematic%20products%29%20or%20from%20class%20explanation%20masks%20provided%20by%20an%0Aexplanation%20method%20if%20no%20reference%20maps%20are%20available.%20Similarly%20to%20pairing%20two%0Atraining%20images%2C%20our%20LP%20strategy%20carries%20out%20a%20pairing%20operation%20on%20the%0Aassociated%20pixel-level%20class%20positional%20information%20to%20derive%20the%20updated%0Amulti-label%20for%20the%20augmented%20image.%20Experimental%20results%20show%20the%0Aeffectiveness%20of%20our%20LP%20strategy%20in%20general%20and%20its%20robustness%20in%20the%20case%20of%0Avarious%20simulated%20and%20real%20scenarios%20with%20noisy%20class%20positional%20information%20in%0Aparticular.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13451v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Label%2520Propagation%2520Strategy%2520for%2520CutMix%2520in%2520Multi-Label%2520Remote%2520Sensing%250A%2520%2520Image%2520Classification%26entry.906535625%3DTom%2520Burgert%2520and%2520Kai%2520Norman%2520Clasen%2520and%2520Jonas%2520Klotz%2520and%2520Tim%2520Siebert%2520and%2520Beg%25C3%25BCm%2520Demir%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520supervised%2520deep%2520learning-based%2520methods%2520for%2520multi-label%250Ascene%2520classification%2520%2528MLC%2529%2520is%2520one%2520of%2520the%2520prominent%2520research%2520directions%2520in%250Aremote%2520sensing%2520%2528RS%2529.%2520Yet%252C%2520collecting%2520annotations%2520for%2520large%2520RS%2520image%2520archives%2520is%250Atime-consuming%2520and%2520costly.%2520To%2520address%2520this%2520issue%252C%2520several%2520data%2520augmentation%250Amethods%2520have%2520been%2520introduced%2520in%2520RS.%2520Among%2520others%252C%2520the%2520data%2520augmentation%250Atechnique%2520CutMix%252C%2520which%2520combines%2520parts%2520of%2520two%2520existing%2520training%2520images%2520to%250Agenerate%2520an%2520augmented%2520image%252C%2520stands%2520out%2520as%2520a%2520particularly%2520effective%2520approach.%250AHowever%252C%2520the%2520direct%2520application%2520of%2520CutMix%2520in%2520RS%2520MLC%2520can%2520lead%2520to%2520the%2520erasure%2520or%250Aaddition%2520of%2520class%2520labels%2520%2528i.e.%252C%2520label%2520noise%2529%2520in%2520the%2520augmented%2520%2528i.e.%252C%2520combined%2529%250Atraining%2520image.%2520To%2520address%2520this%2520problem%252C%2520we%2520introduce%2520a%2520label%2520propagation%2520%2528LP%2529%250Astrategy%2520that%2520allows%2520the%2520effective%2520application%2520of%2520CutMix%2520in%2520the%2520context%2520of%2520MLC%250Aproblems%2520in%2520RS%2520without%2520being%2520affected%2520by%2520label%2520noise.%2520To%2520this%2520end%252C%2520our%2520proposed%250ALP%2520strategy%2520exploits%2520pixel-level%2520class%2520positional%2520information%2520to%2520update%2520the%250Amulti-label%2520of%2520the%2520augmented%2520training%2520image.%2520We%2520propose%2520to%2520access%2520such%2520class%250Apositional%2520information%2520from%2520reference%2520maps%2520associated%2520to%2520each%2520training%2520image%250A%2528e.g.%252C%2520thematic%2520products%2529%2520or%2520from%2520class%2520explanation%2520masks%2520provided%2520by%2520an%250Aexplanation%2520method%2520if%2520no%2520reference%2520maps%2520are%2520available.%2520Similarly%2520to%2520pairing%2520two%250Atraining%2520images%252C%2520our%2520LP%2520strategy%2520carries%2520out%2520a%2520pairing%2520operation%2520on%2520the%250Aassociated%2520pixel-level%2520class%2520positional%2520information%2520to%2520derive%2520the%2520updated%250Amulti-label%2520for%2520the%2520augmented%2520image.%2520Experimental%2520results%2520show%2520the%250Aeffectiveness%2520of%2520our%2520LP%2520strategy%2520in%2520general%2520and%2520its%2520robustness%2520in%2520the%2520case%2520of%250Avarious%2520simulated%2520and%2520real%2520scenarios%2520with%2520noisy%2520class%2520positional%2520information%2520in%250Aparticular.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13451v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Label%20Propagation%20Strategy%20for%20CutMix%20in%20Multi-Label%20Remote%20Sensing%0A%20%20Image%20Classification&entry.906535625=Tom%20Burgert%20and%20Kai%20Norman%20Clasen%20and%20Jonas%20Klotz%20and%20Tim%20Siebert%20and%20Beg%C3%BCm%20Demir&entry.1292438233=%20%20The%20development%20of%20supervised%20deep%20learning-based%20methods%20for%20multi-label%0Ascene%20classification%20%28MLC%29%20is%20one%20of%20the%20prominent%20research%20directions%20in%0Aremote%20sensing%20%28RS%29.%20Yet%2C%20collecting%20annotations%20for%20large%20RS%20image%20archives%20is%0Atime-consuming%20and%20costly.%20To%20address%20this%20issue%2C%20several%20data%20augmentation%0Amethods%20have%20been%20introduced%20in%20RS.%20Among%20others%2C%20the%20data%20augmentation%0Atechnique%20CutMix%2C%20which%20combines%20parts%20of%20two%20existing%20training%20images%20to%0Agenerate%20an%20augmented%20image%2C%20stands%20out%20as%20a%20particularly%20effective%20approach.%0AHowever%2C%20the%20direct%20application%20of%20CutMix%20in%20RS%20MLC%20can%20lead%20to%20the%20erasure%20or%0Aaddition%20of%20class%20labels%20%28i.e.%2C%20label%20noise%29%20in%20the%20augmented%20%28i.e.%2C%20combined%29%0Atraining%20image.%20To%20address%20this%20problem%2C%20we%20introduce%20a%20label%20propagation%20%28LP%29%0Astrategy%20that%20allows%20the%20effective%20application%20of%20CutMix%20in%20the%20context%20of%20MLC%0Aproblems%20in%20RS%20without%20being%20affected%20by%20label%20noise.%20To%20this%20end%2C%20our%20proposed%0ALP%20strategy%20exploits%20pixel-level%20class%20positional%20information%20to%20update%20the%0Amulti-label%20of%20the%20augmented%20training%20image.%20We%20propose%20to%20access%20such%20class%0Apositional%20information%20from%20reference%20maps%20associated%20to%20each%20training%20image%0A%28e.g.%2C%20thematic%20products%29%20or%20from%20class%20explanation%20masks%20provided%20by%20an%0Aexplanation%20method%20if%20no%20reference%20maps%20are%20available.%20Similarly%20to%20pairing%20two%0Atraining%20images%2C%20our%20LP%20strategy%20carries%20out%20a%20pairing%20operation%20on%20the%0Aassociated%20pixel-level%20class%20positional%20information%20to%20derive%20the%20updated%0Amulti-label%20for%20the%20augmented%20image.%20Experimental%20results%20show%20the%0Aeffectiveness%20of%20our%20LP%20strategy%20in%20general%20and%20its%20robustness%20in%20the%20case%20of%0Avarious%20simulated%20and%20real%20scenarios%20with%20noisy%20class%20positional%20information%20in%0Aparticular.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13451v2&entry.124074799=Read"},
{"title": "Harnessing Scale and Physics: A Multi-Graph Neural Operator Framework\n  for PDEs on Arbitrary Geometries", "author": "Zhihao Li and Haoze Song and Di Xiao and Zhilu Lai and Wei Wang", "abstract": "  Partial Differential Equations (PDEs) underpin many scientific phenomena, yet\ntraditional computational approaches often struggle with complex, nonlinear\nsystems and irregular geometries. This paper introduces the AMG method, a\nMulti-Graph neural operator approach designed for efficiently solving PDEs on\nArbitrary geometries. AMG leverages advanced graph-based techniques and dynamic\nattention mechanisms within a novel GraphFormer architecture, enabling precise\nmanagement of diverse spatial domains and complex data interdependencies. By\nconstructing multi-scale graphs to handle variable feature frequencies and a\nphysics graph to encapsulate inherent physical properties, AMG significantly\noutperforms previous methods, which are typically limited to uniform grids. We\npresent a comprehensive evaluation of AMG across six benchmarks, demonstrating\nits consistent superiority over existing state-of-the-art models. Our findings\nhighlight the transformative potential of tailored graph neural operators in\nsurmounting the challenges faced by conventional PDE solvers. Our code and\ndatasets are available on https://github.com/lizhihao2022/AMG.\n", "link": "http://arxiv.org/abs/2411.15178v3", "date": "2025-02-07", "relevancy": 2.149, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5679}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5269}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harnessing%20Scale%20and%20Physics%3A%20A%20Multi-Graph%20Neural%20Operator%20Framework%0A%20%20for%20PDEs%20on%20Arbitrary%20Geometries&body=Title%3A%20Harnessing%20Scale%20and%20Physics%3A%20A%20Multi-Graph%20Neural%20Operator%20Framework%0A%20%20for%20PDEs%20on%20Arbitrary%20Geometries%0AAuthor%3A%20Zhihao%20Li%20and%20Haoze%20Song%20and%20Di%20Xiao%20and%20Zhilu%20Lai%20and%20Wei%20Wang%0AAbstract%3A%20%20%20Partial%20Differential%20Equations%20%28PDEs%29%20underpin%20many%20scientific%20phenomena%2C%20yet%0Atraditional%20computational%20approaches%20often%20struggle%20with%20complex%2C%20nonlinear%0Asystems%20and%20irregular%20geometries.%20This%20paper%20introduces%20the%20AMG%20method%2C%20a%0AMulti-Graph%20neural%20operator%20approach%20designed%20for%20efficiently%20solving%20PDEs%20on%0AArbitrary%20geometries.%20AMG%20leverages%20advanced%20graph-based%20techniques%20and%20dynamic%0Aattention%20mechanisms%20within%20a%20novel%20GraphFormer%20architecture%2C%20enabling%20precise%0Amanagement%20of%20diverse%20spatial%20domains%20and%20complex%20data%20interdependencies.%20By%0Aconstructing%20multi-scale%20graphs%20to%20handle%20variable%20feature%20frequencies%20and%20a%0Aphysics%20graph%20to%20encapsulate%20inherent%20physical%20properties%2C%20AMG%20significantly%0Aoutperforms%20previous%20methods%2C%20which%20are%20typically%20limited%20to%20uniform%20grids.%20We%0Apresent%20a%20comprehensive%20evaluation%20of%20AMG%20across%20six%20benchmarks%2C%20demonstrating%0Aits%20consistent%20superiority%20over%20existing%20state-of-the-art%20models.%20Our%20findings%0Ahighlight%20the%20transformative%20potential%20of%20tailored%20graph%20neural%20operators%20in%0Asurmounting%20the%20challenges%20faced%20by%20conventional%20PDE%20solvers.%20Our%20code%20and%0Adatasets%20are%20available%20on%20https%3A//github.com/lizhihao2022/AMG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15178v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarnessing%2520Scale%2520and%2520Physics%253A%2520A%2520Multi-Graph%2520Neural%2520Operator%2520Framework%250A%2520%2520for%2520PDEs%2520on%2520Arbitrary%2520Geometries%26entry.906535625%3DZhihao%2520Li%2520and%2520Haoze%2520Song%2520and%2520Di%2520Xiao%2520and%2520Zhilu%2520Lai%2520and%2520Wei%2520Wang%26entry.1292438233%3D%2520%2520Partial%2520Differential%2520Equations%2520%2528PDEs%2529%2520underpin%2520many%2520scientific%2520phenomena%252C%2520yet%250Atraditional%2520computational%2520approaches%2520often%2520struggle%2520with%2520complex%252C%2520nonlinear%250Asystems%2520and%2520irregular%2520geometries.%2520This%2520paper%2520introduces%2520the%2520AMG%2520method%252C%2520a%250AMulti-Graph%2520neural%2520operator%2520approach%2520designed%2520for%2520efficiently%2520solving%2520PDEs%2520on%250AArbitrary%2520geometries.%2520AMG%2520leverages%2520advanced%2520graph-based%2520techniques%2520and%2520dynamic%250Aattention%2520mechanisms%2520within%2520a%2520novel%2520GraphFormer%2520architecture%252C%2520enabling%2520precise%250Amanagement%2520of%2520diverse%2520spatial%2520domains%2520and%2520complex%2520data%2520interdependencies.%2520By%250Aconstructing%2520multi-scale%2520graphs%2520to%2520handle%2520variable%2520feature%2520frequencies%2520and%2520a%250Aphysics%2520graph%2520to%2520encapsulate%2520inherent%2520physical%2520properties%252C%2520AMG%2520significantly%250Aoutperforms%2520previous%2520methods%252C%2520which%2520are%2520typically%2520limited%2520to%2520uniform%2520grids.%2520We%250Apresent%2520a%2520comprehensive%2520evaluation%2520of%2520AMG%2520across%2520six%2520benchmarks%252C%2520demonstrating%250Aits%2520consistent%2520superiority%2520over%2520existing%2520state-of-the-art%2520models.%2520Our%2520findings%250Ahighlight%2520the%2520transformative%2520potential%2520of%2520tailored%2520graph%2520neural%2520operators%2520in%250Asurmounting%2520the%2520challenges%2520faced%2520by%2520conventional%2520PDE%2520solvers.%2520Our%2520code%2520and%250Adatasets%2520are%2520available%2520on%2520https%253A//github.com/lizhihao2022/AMG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15178v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20Scale%20and%20Physics%3A%20A%20Multi-Graph%20Neural%20Operator%20Framework%0A%20%20for%20PDEs%20on%20Arbitrary%20Geometries&entry.906535625=Zhihao%20Li%20and%20Haoze%20Song%20and%20Di%20Xiao%20and%20Zhilu%20Lai%20and%20Wei%20Wang&entry.1292438233=%20%20Partial%20Differential%20Equations%20%28PDEs%29%20underpin%20many%20scientific%20phenomena%2C%20yet%0Atraditional%20computational%20approaches%20often%20struggle%20with%20complex%2C%20nonlinear%0Asystems%20and%20irregular%20geometries.%20This%20paper%20introduces%20the%20AMG%20method%2C%20a%0AMulti-Graph%20neural%20operator%20approach%20designed%20for%20efficiently%20solving%20PDEs%20on%0AArbitrary%20geometries.%20AMG%20leverages%20advanced%20graph-based%20techniques%20and%20dynamic%0Aattention%20mechanisms%20within%20a%20novel%20GraphFormer%20architecture%2C%20enabling%20precise%0Amanagement%20of%20diverse%20spatial%20domains%20and%20complex%20data%20interdependencies.%20By%0Aconstructing%20multi-scale%20graphs%20to%20handle%20variable%20feature%20frequencies%20and%20a%0Aphysics%20graph%20to%20encapsulate%20inherent%20physical%20properties%2C%20AMG%20significantly%0Aoutperforms%20previous%20methods%2C%20which%20are%20typically%20limited%20to%20uniform%20grids.%20We%0Apresent%20a%20comprehensive%20evaluation%20of%20AMG%20across%20six%20benchmarks%2C%20demonstrating%0Aits%20consistent%20superiority%20over%20existing%20state-of-the-art%20models.%20Our%20findings%0Ahighlight%20the%20transformative%20potential%20of%20tailored%20graph%20neural%20operators%20in%0Asurmounting%20the%20challenges%20faced%20by%20conventional%20PDE%20solvers.%20Our%20code%20and%0Adatasets%20are%20available%20on%20https%3A//github.com/lizhihao2022/AMG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15178v3&entry.124074799=Read"},
{"title": "Lightweight Operations for Visual Speech Recognition", "author": "Iason Ioannis Panagos and Giorgos Sfikas and Christophoros Nikou", "abstract": "  Visual speech recognition (VSR), which decodes spoken words from video data,\noffers significant benefits, particularly when audio is unavailable. However,\nthe high dimensionality of video data leads to prohibitive computational costs\nthat demand powerful hardware, limiting VSR deployment on resource-constrained\ndevices. This work addresses this limitation by developing lightweight VSR\narchitectures. Leveraging efficient operation design paradigms, we create\ncompact yet powerful models with reduced resource requirements and minimal\naccuracy loss. We train and evaluate our models on a large-scale public dataset\nfor recognition of words from video sequences, demonstrating their\neffectiveness for practical applications. We also conduct an extensive array of\nablative experiments to thoroughly analyze the size and complexity of each\nmodel. Code and trained models will be made publicly available.\n", "link": "http://arxiv.org/abs/2502.04834v1", "date": "2025-02-07", "relevancy": 2.1469, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5413}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5413}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lightweight%20Operations%20for%20Visual%20Speech%20Recognition&body=Title%3A%20Lightweight%20Operations%20for%20Visual%20Speech%20Recognition%0AAuthor%3A%20Iason%20Ioannis%20Panagos%20and%20Giorgos%20Sfikas%20and%20Christophoros%20Nikou%0AAbstract%3A%20%20%20Visual%20speech%20recognition%20%28VSR%29%2C%20which%20decodes%20spoken%20words%20from%20video%20data%2C%0Aoffers%20significant%20benefits%2C%20particularly%20when%20audio%20is%20unavailable.%20However%2C%0Athe%20high%20dimensionality%20of%20video%20data%20leads%20to%20prohibitive%20computational%20costs%0Athat%20demand%20powerful%20hardware%2C%20limiting%20VSR%20deployment%20on%20resource-constrained%0Adevices.%20This%20work%20addresses%20this%20limitation%20by%20developing%20lightweight%20VSR%0Aarchitectures.%20Leveraging%20efficient%20operation%20design%20paradigms%2C%20we%20create%0Acompact%20yet%20powerful%20models%20with%20reduced%20resource%20requirements%20and%20minimal%0Aaccuracy%20loss.%20We%20train%20and%20evaluate%20our%20models%20on%20a%20large-scale%20public%20dataset%0Afor%20recognition%20of%20words%20from%20video%20sequences%2C%20demonstrating%20their%0Aeffectiveness%20for%20practical%20applications.%20We%20also%20conduct%20an%20extensive%20array%20of%0Aablative%20experiments%20to%20thoroughly%20analyze%20the%20size%20and%20complexity%20of%20each%0Amodel.%20Code%20and%20trained%20models%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightweight%2520Operations%2520for%2520Visual%2520Speech%2520Recognition%26entry.906535625%3DIason%2520Ioannis%2520Panagos%2520and%2520Giorgos%2520Sfikas%2520and%2520Christophoros%2520Nikou%26entry.1292438233%3D%2520%2520Visual%2520speech%2520recognition%2520%2528VSR%2529%252C%2520which%2520decodes%2520spoken%2520words%2520from%2520video%2520data%252C%250Aoffers%2520significant%2520benefits%252C%2520particularly%2520when%2520audio%2520is%2520unavailable.%2520However%252C%250Athe%2520high%2520dimensionality%2520of%2520video%2520data%2520leads%2520to%2520prohibitive%2520computational%2520costs%250Athat%2520demand%2520powerful%2520hardware%252C%2520limiting%2520VSR%2520deployment%2520on%2520resource-constrained%250Adevices.%2520This%2520work%2520addresses%2520this%2520limitation%2520by%2520developing%2520lightweight%2520VSR%250Aarchitectures.%2520Leveraging%2520efficient%2520operation%2520design%2520paradigms%252C%2520we%2520create%250Acompact%2520yet%2520powerful%2520models%2520with%2520reduced%2520resource%2520requirements%2520and%2520minimal%250Aaccuracy%2520loss.%2520We%2520train%2520and%2520evaluate%2520our%2520models%2520on%2520a%2520large-scale%2520public%2520dataset%250Afor%2520recognition%2520of%2520words%2520from%2520video%2520sequences%252C%2520demonstrating%2520their%250Aeffectiveness%2520for%2520practical%2520applications.%2520We%2520also%2520conduct%2520an%2520extensive%2520array%2520of%250Aablative%2520experiments%2520to%2520thoroughly%2520analyze%2520the%2520size%2520and%2520complexity%2520of%2520each%250Amodel.%2520Code%2520and%2520trained%2520models%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lightweight%20Operations%20for%20Visual%20Speech%20Recognition&entry.906535625=Iason%20Ioannis%20Panagos%20and%20Giorgos%20Sfikas%20and%20Christophoros%20Nikou&entry.1292438233=%20%20Visual%20speech%20recognition%20%28VSR%29%2C%20which%20decodes%20spoken%20words%20from%20video%20data%2C%0Aoffers%20significant%20benefits%2C%20particularly%20when%20audio%20is%20unavailable.%20However%2C%0Athe%20high%20dimensionality%20of%20video%20data%20leads%20to%20prohibitive%20computational%20costs%0Athat%20demand%20powerful%20hardware%2C%20limiting%20VSR%20deployment%20on%20resource-constrained%0Adevices.%20This%20work%20addresses%20this%20limitation%20by%20developing%20lightweight%20VSR%0Aarchitectures.%20Leveraging%20efficient%20operation%20design%20paradigms%2C%20we%20create%0Acompact%20yet%20powerful%20models%20with%20reduced%20resource%20requirements%20and%20minimal%0Aaccuracy%20loss.%20We%20train%20and%20evaluate%20our%20models%20on%20a%20large-scale%20public%20dataset%0Afor%20recognition%20of%20words%20from%20video%20sequences%2C%20demonstrating%20their%0Aeffectiveness%20for%20practical%20applications.%20We%20also%20conduct%20an%20extensive%20array%20of%0Aablative%20experiments%20to%20thoroughly%20analyze%20the%20size%20and%20complexity%20of%20each%0Amodel.%20Code%20and%20trained%20models%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04834v1&entry.124074799=Read"},
{"title": "A Multi-Scale Feature Fusion Framework Integrating Frequency Domain and\n  Cross-View Attention for Dual-View X-ray Security Inspections", "author": "Shilong Hong and Yanzhou Zhou and Weichao Xu", "abstract": "  With the rapid development of modern transportation systems and the\nexponential growth of logistics volumes, intelligent X-ray-based security\ninspection systems play a crucial role in public safety. Although single-view\nX-ray equipment is widely deployed, it struggles to accurately identify\ncontraband in complex stacking scenarios due to strong viewpoint dependency and\ninadequate feature representation. To address this, we propose an innovative\nmulti-scale interactive feature fusion framework tailored for dual-view X-ray\nsecurity inspection image classification. The framework comprises three core\nmodules: the Frequency Domain Interaction Module (FDIM) enhances\nfrequency-domain features through Fourier transform; the Multi-Scale Cross-View\nFeature Enhancement (MSCFE) leverages cross-view attention mechanisms to\nstrengthen feature interactions; and the Convolutional Attention Fusion Module\n(CAFM) efficiently fuses features by integrating channel attention with\ndepthwise-separable convolutions. Experimental results demonstrate that our\nmethod outperforms existing state-of-the-art approaches across multiple\nbackbone architectures, particularly excelling in complex scenarios with\nocclusions and object stacking.\n", "link": "http://arxiv.org/abs/2502.01710v2", "date": "2025-02-07", "relevancy": 2.146, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5418}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5395}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multi-Scale%20Feature%20Fusion%20Framework%20Integrating%20Frequency%20Domain%20and%0A%20%20Cross-View%20Attention%20for%20Dual-View%20X-ray%20Security%20Inspections&body=Title%3A%20A%20Multi-Scale%20Feature%20Fusion%20Framework%20Integrating%20Frequency%20Domain%20and%0A%20%20Cross-View%20Attention%20for%20Dual-View%20X-ray%20Security%20Inspections%0AAuthor%3A%20Shilong%20Hong%20and%20Yanzhou%20Zhou%20and%20Weichao%20Xu%0AAbstract%3A%20%20%20With%20the%20rapid%20development%20of%20modern%20transportation%20systems%20and%20the%0Aexponential%20growth%20of%20logistics%20volumes%2C%20intelligent%20X-ray-based%20security%0Ainspection%20systems%20play%20a%20crucial%20role%20in%20public%20safety.%20Although%20single-view%0AX-ray%20equipment%20is%20widely%20deployed%2C%20it%20struggles%20to%20accurately%20identify%0Acontraband%20in%20complex%20stacking%20scenarios%20due%20to%20strong%20viewpoint%20dependency%20and%0Ainadequate%20feature%20representation.%20To%20address%20this%2C%20we%20propose%20an%20innovative%0Amulti-scale%20interactive%20feature%20fusion%20framework%20tailored%20for%20dual-view%20X-ray%0Asecurity%20inspection%20image%20classification.%20The%20framework%20comprises%20three%20core%0Amodules%3A%20the%20Frequency%20Domain%20Interaction%20Module%20%28FDIM%29%20enhances%0Afrequency-domain%20features%20through%20Fourier%20transform%3B%20the%20Multi-Scale%20Cross-View%0AFeature%20Enhancement%20%28MSCFE%29%20leverages%20cross-view%20attention%20mechanisms%20to%0Astrengthen%20feature%20interactions%3B%20and%20the%20Convolutional%20Attention%20Fusion%20Module%0A%28CAFM%29%20efficiently%20fuses%20features%20by%20integrating%20channel%20attention%20with%0Adepthwise-separable%20convolutions.%20Experimental%20results%20demonstrate%20that%20our%0Amethod%20outperforms%20existing%20state-of-the-art%20approaches%20across%20multiple%0Abackbone%20architectures%2C%20particularly%20excelling%20in%20complex%20scenarios%20with%0Aocclusions%20and%20object%20stacking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.01710v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multi-Scale%2520Feature%2520Fusion%2520Framework%2520Integrating%2520Frequency%2520Domain%2520and%250A%2520%2520Cross-View%2520Attention%2520for%2520Dual-View%2520X-ray%2520Security%2520Inspections%26entry.906535625%3DShilong%2520Hong%2520and%2520Yanzhou%2520Zhou%2520and%2520Weichao%2520Xu%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520development%2520of%2520modern%2520transportation%2520systems%2520and%2520the%250Aexponential%2520growth%2520of%2520logistics%2520volumes%252C%2520intelligent%2520X-ray-based%2520security%250Ainspection%2520systems%2520play%2520a%2520crucial%2520role%2520in%2520public%2520safety.%2520Although%2520single-view%250AX-ray%2520equipment%2520is%2520widely%2520deployed%252C%2520it%2520struggles%2520to%2520accurately%2520identify%250Acontraband%2520in%2520complex%2520stacking%2520scenarios%2520due%2520to%2520strong%2520viewpoint%2520dependency%2520and%250Ainadequate%2520feature%2520representation.%2520To%2520address%2520this%252C%2520we%2520propose%2520an%2520innovative%250Amulti-scale%2520interactive%2520feature%2520fusion%2520framework%2520tailored%2520for%2520dual-view%2520X-ray%250Asecurity%2520inspection%2520image%2520classification.%2520The%2520framework%2520comprises%2520three%2520core%250Amodules%253A%2520the%2520Frequency%2520Domain%2520Interaction%2520Module%2520%2528FDIM%2529%2520enhances%250Afrequency-domain%2520features%2520through%2520Fourier%2520transform%253B%2520the%2520Multi-Scale%2520Cross-View%250AFeature%2520Enhancement%2520%2528MSCFE%2529%2520leverages%2520cross-view%2520attention%2520mechanisms%2520to%250Astrengthen%2520feature%2520interactions%253B%2520and%2520the%2520Convolutional%2520Attention%2520Fusion%2520Module%250A%2528CAFM%2529%2520efficiently%2520fuses%2520features%2520by%2520integrating%2520channel%2520attention%2520with%250Adepthwise-separable%2520convolutions.%2520Experimental%2520results%2520demonstrate%2520that%2520our%250Amethod%2520outperforms%2520existing%2520state-of-the-art%2520approaches%2520across%2520multiple%250Abackbone%2520architectures%252C%2520particularly%2520excelling%2520in%2520complex%2520scenarios%2520with%250Aocclusions%2520and%2520object%2520stacking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.01710v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-Scale%20Feature%20Fusion%20Framework%20Integrating%20Frequency%20Domain%20and%0A%20%20Cross-View%20Attention%20for%20Dual-View%20X-ray%20Security%20Inspections&entry.906535625=Shilong%20Hong%20and%20Yanzhou%20Zhou%20and%20Weichao%20Xu&entry.1292438233=%20%20With%20the%20rapid%20development%20of%20modern%20transportation%20systems%20and%20the%0Aexponential%20growth%20of%20logistics%20volumes%2C%20intelligent%20X-ray-based%20security%0Ainspection%20systems%20play%20a%20crucial%20role%20in%20public%20safety.%20Although%20single-view%0AX-ray%20equipment%20is%20widely%20deployed%2C%20it%20struggles%20to%20accurately%20identify%0Acontraband%20in%20complex%20stacking%20scenarios%20due%20to%20strong%20viewpoint%20dependency%20and%0Ainadequate%20feature%20representation.%20To%20address%20this%2C%20we%20propose%20an%20innovative%0Amulti-scale%20interactive%20feature%20fusion%20framework%20tailored%20for%20dual-view%20X-ray%0Asecurity%20inspection%20image%20classification.%20The%20framework%20comprises%20three%20core%0Amodules%3A%20the%20Frequency%20Domain%20Interaction%20Module%20%28FDIM%29%20enhances%0Afrequency-domain%20features%20through%20Fourier%20transform%3B%20the%20Multi-Scale%20Cross-View%0AFeature%20Enhancement%20%28MSCFE%29%20leverages%20cross-view%20attention%20mechanisms%20to%0Astrengthen%20feature%20interactions%3B%20and%20the%20Convolutional%20Attention%20Fusion%20Module%0A%28CAFM%29%20efficiently%20fuses%20features%20by%20integrating%20channel%20attention%20with%0Adepthwise-separable%20convolutions.%20Experimental%20results%20demonstrate%20that%20our%0Amethod%20outperforms%20existing%20state-of-the-art%20approaches%20across%20multiple%0Abackbone%20architectures%2C%20particularly%20excelling%20in%20complex%20scenarios%20with%0Aocclusions%20and%20object%20stacking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.01710v2&entry.124074799=Read"},
{"title": "Cached Multi-Lora Composition for Multi-Concept Image Generation", "author": "Xiandong Zou and Mingzhu Shen and Christos-Savvas Bouganis and Yiren Zhao", "abstract": "  Low-Rank Adaptation (LoRA) has emerged as a widely adopted technique in\ntext-to-image models, enabling precise rendering of multiple distinct elements,\nsuch as characters and styles, in multi-concept image generation. However,\ncurrent approaches face significant challenges when composing these LoRAs for\nmulti-concept image generation, resulting in diminished generated image\nquality. In this paper, we initially investigate the role of LoRAs in the\ndenoising process through the lens of the Fourier frequency domain. Based on\nthe hypothesis that applying multiple LoRAs could lead to \"semantic conflicts\",\nwe find that certain LoRAs amplify high-frequency features such as edges and\ntextures, whereas others mainly focus on low-frequency elements, including the\noverall structure and smooth color gradients. Building on these insights, we\ndevise a frequency domain based sequencing strategy to determine the optimal\norder in which LoRAs should be integrated during inference. This strategy\noffers a methodical and generalizable solution compared to the naive\nintegration commonly found in existing LoRA fusion techniques. To fully\nleverage our proposed LoRA order sequence determination method in multi-LoRA\ncomposition tasks, we introduce a novel, training-free framework, Cached\nMulti-LoRA (CMLoRA), designed to efficiently integrate multiple LoRAs while\nmaintaining cohesive image generation. With its flexible backbone for\nmulti-LoRA fusion and a non-uniform caching strategy tailored to individual\nLoRAs, CMLoRA has the potential to reduce semantic conflicts in LoRA\ncomposition and improve computational efficiency. Our experimental evaluations\ndemonstrate that CMLoRA outperforms state-of-the-art training-free LoRA fusion\nmethods by a significant margin -- it achieves an average improvement of\n$2.19\\%$ in CLIPScore, and $11.25\\%$ in MLLM win rate compared to LoraHub, LoRA\nComposite, and LoRA Switch.\n", "link": "http://arxiv.org/abs/2502.04923v1", "date": "2025-02-07", "relevancy": 2.1346, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5447}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5328}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5229}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cached%20Multi-Lora%20Composition%20for%20Multi-Concept%20Image%20Generation&body=Title%3A%20Cached%20Multi-Lora%20Composition%20for%20Multi-Concept%20Image%20Generation%0AAuthor%3A%20Xiandong%20Zou%20and%20Mingzhu%20Shen%20and%20Christos-Savvas%20Bouganis%20and%20Yiren%20Zhao%0AAbstract%3A%20%20%20Low-Rank%20Adaptation%20%28LoRA%29%20has%20emerged%20as%20a%20widely%20adopted%20technique%20in%0Atext-to-image%20models%2C%20enabling%20precise%20rendering%20of%20multiple%20distinct%20elements%2C%0Asuch%20as%20characters%20and%20styles%2C%20in%20multi-concept%20image%20generation.%20However%2C%0Acurrent%20approaches%20face%20significant%20challenges%20when%20composing%20these%20LoRAs%20for%0Amulti-concept%20image%20generation%2C%20resulting%20in%20diminished%20generated%20image%0Aquality.%20In%20this%20paper%2C%20we%20initially%20investigate%20the%20role%20of%20LoRAs%20in%20the%0Adenoising%20process%20through%20the%20lens%20of%20the%20Fourier%20frequency%20domain.%20Based%20on%0Athe%20hypothesis%20that%20applying%20multiple%20LoRAs%20could%20lead%20to%20%22semantic%20conflicts%22%2C%0Awe%20find%20that%20certain%20LoRAs%20amplify%20high-frequency%20features%20such%20as%20edges%20and%0Atextures%2C%20whereas%20others%20mainly%20focus%20on%20low-frequency%20elements%2C%20including%20the%0Aoverall%20structure%20and%20smooth%20color%20gradients.%20Building%20on%20these%20insights%2C%20we%0Adevise%20a%20frequency%20domain%20based%20sequencing%20strategy%20to%20determine%20the%20optimal%0Aorder%20in%20which%20LoRAs%20should%20be%20integrated%20during%20inference.%20This%20strategy%0Aoffers%20a%20methodical%20and%20generalizable%20solution%20compared%20to%20the%20naive%0Aintegration%20commonly%20found%20in%20existing%20LoRA%20fusion%20techniques.%20To%20fully%0Aleverage%20our%20proposed%20LoRA%20order%20sequence%20determination%20method%20in%20multi-LoRA%0Acomposition%20tasks%2C%20we%20introduce%20a%20novel%2C%20training-free%20framework%2C%20Cached%0AMulti-LoRA%20%28CMLoRA%29%2C%20designed%20to%20efficiently%20integrate%20multiple%20LoRAs%20while%0Amaintaining%20cohesive%20image%20generation.%20With%20its%20flexible%20backbone%20for%0Amulti-LoRA%20fusion%20and%20a%20non-uniform%20caching%20strategy%20tailored%20to%20individual%0ALoRAs%2C%20CMLoRA%20has%20the%20potential%20to%20reduce%20semantic%20conflicts%20in%20LoRA%0Acomposition%20and%20improve%20computational%20efficiency.%20Our%20experimental%20evaluations%0Ademonstrate%20that%20CMLoRA%20outperforms%20state-of-the-art%20training-free%20LoRA%20fusion%0Amethods%20by%20a%20significant%20margin%20--%20it%20achieves%20an%20average%20improvement%20of%0A%242.19%5C%25%24%20in%20CLIPScore%2C%20and%20%2411.25%5C%25%24%20in%20MLLM%20win%20rate%20compared%20to%20LoraHub%2C%20LoRA%0AComposite%2C%20and%20LoRA%20Switch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04923v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCached%2520Multi-Lora%2520Composition%2520for%2520Multi-Concept%2520Image%2520Generation%26entry.906535625%3DXiandong%2520Zou%2520and%2520Mingzhu%2520Shen%2520and%2520Christos-Savvas%2520Bouganis%2520and%2520Yiren%2520Zhao%26entry.1292438233%3D%2520%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520has%2520emerged%2520as%2520a%2520widely%2520adopted%2520technique%2520in%250Atext-to-image%2520models%252C%2520enabling%2520precise%2520rendering%2520of%2520multiple%2520distinct%2520elements%252C%250Asuch%2520as%2520characters%2520and%2520styles%252C%2520in%2520multi-concept%2520image%2520generation.%2520However%252C%250Acurrent%2520approaches%2520face%2520significant%2520challenges%2520when%2520composing%2520these%2520LoRAs%2520for%250Amulti-concept%2520image%2520generation%252C%2520resulting%2520in%2520diminished%2520generated%2520image%250Aquality.%2520In%2520this%2520paper%252C%2520we%2520initially%2520investigate%2520the%2520role%2520of%2520LoRAs%2520in%2520the%250Adenoising%2520process%2520through%2520the%2520lens%2520of%2520the%2520Fourier%2520frequency%2520domain.%2520Based%2520on%250Athe%2520hypothesis%2520that%2520applying%2520multiple%2520LoRAs%2520could%2520lead%2520to%2520%2522semantic%2520conflicts%2522%252C%250Awe%2520find%2520that%2520certain%2520LoRAs%2520amplify%2520high-frequency%2520features%2520such%2520as%2520edges%2520and%250Atextures%252C%2520whereas%2520others%2520mainly%2520focus%2520on%2520low-frequency%2520elements%252C%2520including%2520the%250Aoverall%2520structure%2520and%2520smooth%2520color%2520gradients.%2520Building%2520on%2520these%2520insights%252C%2520we%250Adevise%2520a%2520frequency%2520domain%2520based%2520sequencing%2520strategy%2520to%2520determine%2520the%2520optimal%250Aorder%2520in%2520which%2520LoRAs%2520should%2520be%2520integrated%2520during%2520inference.%2520This%2520strategy%250Aoffers%2520a%2520methodical%2520and%2520generalizable%2520solution%2520compared%2520to%2520the%2520naive%250Aintegration%2520commonly%2520found%2520in%2520existing%2520LoRA%2520fusion%2520techniques.%2520To%2520fully%250Aleverage%2520our%2520proposed%2520LoRA%2520order%2520sequence%2520determination%2520method%2520in%2520multi-LoRA%250Acomposition%2520tasks%252C%2520we%2520introduce%2520a%2520novel%252C%2520training-free%2520framework%252C%2520Cached%250AMulti-LoRA%2520%2528CMLoRA%2529%252C%2520designed%2520to%2520efficiently%2520integrate%2520multiple%2520LoRAs%2520while%250Amaintaining%2520cohesive%2520image%2520generation.%2520With%2520its%2520flexible%2520backbone%2520for%250Amulti-LoRA%2520fusion%2520and%2520a%2520non-uniform%2520caching%2520strategy%2520tailored%2520to%2520individual%250ALoRAs%252C%2520CMLoRA%2520has%2520the%2520potential%2520to%2520reduce%2520semantic%2520conflicts%2520in%2520LoRA%250Acomposition%2520and%2520improve%2520computational%2520efficiency.%2520Our%2520experimental%2520evaluations%250Ademonstrate%2520that%2520CMLoRA%2520outperforms%2520state-of-the-art%2520training-free%2520LoRA%2520fusion%250Amethods%2520by%2520a%2520significant%2520margin%2520--%2520it%2520achieves%2520an%2520average%2520improvement%2520of%250A%25242.19%255C%2525%2524%2520in%2520CLIPScore%252C%2520and%2520%252411.25%255C%2525%2524%2520in%2520MLLM%2520win%2520rate%2520compared%2520to%2520LoraHub%252C%2520LoRA%250AComposite%252C%2520and%2520LoRA%2520Switch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04923v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cached%20Multi-Lora%20Composition%20for%20Multi-Concept%20Image%20Generation&entry.906535625=Xiandong%20Zou%20and%20Mingzhu%20Shen%20and%20Christos-Savvas%20Bouganis%20and%20Yiren%20Zhao&entry.1292438233=%20%20Low-Rank%20Adaptation%20%28LoRA%29%20has%20emerged%20as%20a%20widely%20adopted%20technique%20in%0Atext-to-image%20models%2C%20enabling%20precise%20rendering%20of%20multiple%20distinct%20elements%2C%0Asuch%20as%20characters%20and%20styles%2C%20in%20multi-concept%20image%20generation.%20However%2C%0Acurrent%20approaches%20face%20significant%20challenges%20when%20composing%20these%20LoRAs%20for%0Amulti-concept%20image%20generation%2C%20resulting%20in%20diminished%20generated%20image%0Aquality.%20In%20this%20paper%2C%20we%20initially%20investigate%20the%20role%20of%20LoRAs%20in%20the%0Adenoising%20process%20through%20the%20lens%20of%20the%20Fourier%20frequency%20domain.%20Based%20on%0Athe%20hypothesis%20that%20applying%20multiple%20LoRAs%20could%20lead%20to%20%22semantic%20conflicts%22%2C%0Awe%20find%20that%20certain%20LoRAs%20amplify%20high-frequency%20features%20such%20as%20edges%20and%0Atextures%2C%20whereas%20others%20mainly%20focus%20on%20low-frequency%20elements%2C%20including%20the%0Aoverall%20structure%20and%20smooth%20color%20gradients.%20Building%20on%20these%20insights%2C%20we%0Adevise%20a%20frequency%20domain%20based%20sequencing%20strategy%20to%20determine%20the%20optimal%0Aorder%20in%20which%20LoRAs%20should%20be%20integrated%20during%20inference.%20This%20strategy%0Aoffers%20a%20methodical%20and%20generalizable%20solution%20compared%20to%20the%20naive%0Aintegration%20commonly%20found%20in%20existing%20LoRA%20fusion%20techniques.%20To%20fully%0Aleverage%20our%20proposed%20LoRA%20order%20sequence%20determination%20method%20in%20multi-LoRA%0Acomposition%20tasks%2C%20we%20introduce%20a%20novel%2C%20training-free%20framework%2C%20Cached%0AMulti-LoRA%20%28CMLoRA%29%2C%20designed%20to%20efficiently%20integrate%20multiple%20LoRAs%20while%0Amaintaining%20cohesive%20image%20generation.%20With%20its%20flexible%20backbone%20for%0Amulti-LoRA%20fusion%20and%20a%20non-uniform%20caching%20strategy%20tailored%20to%20individual%0ALoRAs%2C%20CMLoRA%20has%20the%20potential%20to%20reduce%20semantic%20conflicts%20in%20LoRA%0Acomposition%20and%20improve%20computational%20efficiency.%20Our%20experimental%20evaluations%0Ademonstrate%20that%20CMLoRA%20outperforms%20state-of-the-art%20training-free%20LoRA%20fusion%0Amethods%20by%20a%20significant%20margin%20--%20it%20achieves%20an%20average%20improvement%20of%0A%242.19%5C%25%24%20in%20CLIPScore%2C%20and%20%2411.25%5C%25%24%20in%20MLLM%20win%20rate%20compared%20to%20LoraHub%2C%20LoRA%0AComposite%2C%20and%20LoRA%20Switch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04923v1&entry.124074799=Read"},
{"title": "Beautiful Images, Toxic Words: Understanding and Addressing Offensive\n  Text in Generated Images", "author": "Aditya Kumar and Tom Blanchard and Adam Dziedzic and Franziska Boenisch", "abstract": "  State-of-the-art visual generation models, such as Diffusion Models (DMs) and\nVision Auto-Regressive Models (VARs), produce highly realistic images. While\nprior work has successfully mitigated Not Safe For Work (NSFW) content in the\nvisual domain, we identify a novel threat: the generation of NSFW text embedded\nwithin images. This includes offensive language, such as insults, racial slurs,\nand sexually explicit terms, posing significant risks to users. We show that\nall state-of-the-art DMs (e.g., SD3, Flux, DeepFloyd IF) and VARs (e.g.,\nInfinity) are vulnerable to this issue. Through extensive experiments, we\ndemonstrate that existing mitigation techniques, effective for visual content,\nfail to prevent harmful text generation while substantially degrading benign\ntext generation. As an initial step toward addressing this threat, we explore\nsafety fine-tuning of the text encoder underlying major DM architectures using\na customized dataset. Thereby, we suppress NSFW generation while preserving\noverall image and text generation quality. Finally, to advance research in this\narea, we introduce ToxicBench, an open-source benchmark for evaluating NSFW\ntext generation in images. ToxicBench provides a curated dataset of harmful\nprompts, new metrics, and an evaluation pipeline assessing both NSFW-ness and\ngeneration quality. Our benchmark aims to guide future efforts in mitigating\nNSFW text generation in text-to-image models and is available at\nhttps://github.com/sprintml/ToxicBench\n", "link": "http://arxiv.org/abs/2502.05066v1", "date": "2025-02-07", "relevancy": 2.1305, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5775}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5252}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beautiful%20Images%2C%20Toxic%20Words%3A%20Understanding%20and%20Addressing%20Offensive%0A%20%20Text%20in%20Generated%20Images&body=Title%3A%20Beautiful%20Images%2C%20Toxic%20Words%3A%20Understanding%20and%20Addressing%20Offensive%0A%20%20Text%20in%20Generated%20Images%0AAuthor%3A%20Aditya%20Kumar%20and%20Tom%20Blanchard%20and%20Adam%20Dziedzic%20and%20Franziska%20Boenisch%0AAbstract%3A%20%20%20State-of-the-art%20visual%20generation%20models%2C%20such%20as%20Diffusion%20Models%20%28DMs%29%20and%0AVision%20Auto-Regressive%20Models%20%28VARs%29%2C%20produce%20highly%20realistic%20images.%20While%0Aprior%20work%20has%20successfully%20mitigated%20Not%20Safe%20For%20Work%20%28NSFW%29%20content%20in%20the%0Avisual%20domain%2C%20we%20identify%20a%20novel%20threat%3A%20the%20generation%20of%20NSFW%20text%20embedded%0Awithin%20images.%20This%20includes%20offensive%20language%2C%20such%20as%20insults%2C%20racial%20slurs%2C%0Aand%20sexually%20explicit%20terms%2C%20posing%20significant%20risks%20to%20users.%20We%20show%20that%0Aall%20state-of-the-art%20DMs%20%28e.g.%2C%20SD3%2C%20Flux%2C%20DeepFloyd%20IF%29%20and%20VARs%20%28e.g.%2C%0AInfinity%29%20are%20vulnerable%20to%20this%20issue.%20Through%20extensive%20experiments%2C%20we%0Ademonstrate%20that%20existing%20mitigation%20techniques%2C%20effective%20for%20visual%20content%2C%0Afail%20to%20prevent%20harmful%20text%20generation%20while%20substantially%20degrading%20benign%0Atext%20generation.%20As%20an%20initial%20step%20toward%20addressing%20this%20threat%2C%20we%20explore%0Asafety%20fine-tuning%20of%20the%20text%20encoder%20underlying%20major%20DM%20architectures%20using%0Aa%20customized%20dataset.%20Thereby%2C%20we%20suppress%20NSFW%20generation%20while%20preserving%0Aoverall%20image%20and%20text%20generation%20quality.%20Finally%2C%20to%20advance%20research%20in%20this%0Aarea%2C%20we%20introduce%20ToxicBench%2C%20an%20open-source%20benchmark%20for%20evaluating%20NSFW%0Atext%20generation%20in%20images.%20ToxicBench%20provides%20a%20curated%20dataset%20of%20harmful%0Aprompts%2C%20new%20metrics%2C%20and%20an%20evaluation%20pipeline%20assessing%20both%20NSFW-ness%20and%0Ageneration%20quality.%20Our%20benchmark%20aims%20to%20guide%20future%20efforts%20in%20mitigating%0ANSFW%20text%20generation%20in%20text-to-image%20models%20and%20is%20available%20at%0Ahttps%3A//github.com/sprintml/ToxicBench%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05066v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeautiful%2520Images%252C%2520Toxic%2520Words%253A%2520Understanding%2520and%2520Addressing%2520Offensive%250A%2520%2520Text%2520in%2520Generated%2520Images%26entry.906535625%3DAditya%2520Kumar%2520and%2520Tom%2520Blanchard%2520and%2520Adam%2520Dziedzic%2520and%2520Franziska%2520Boenisch%26entry.1292438233%3D%2520%2520State-of-the-art%2520visual%2520generation%2520models%252C%2520such%2520as%2520Diffusion%2520Models%2520%2528DMs%2529%2520and%250AVision%2520Auto-Regressive%2520Models%2520%2528VARs%2529%252C%2520produce%2520highly%2520realistic%2520images.%2520While%250Aprior%2520work%2520has%2520successfully%2520mitigated%2520Not%2520Safe%2520For%2520Work%2520%2528NSFW%2529%2520content%2520in%2520the%250Avisual%2520domain%252C%2520we%2520identify%2520a%2520novel%2520threat%253A%2520the%2520generation%2520of%2520NSFW%2520text%2520embedded%250Awithin%2520images.%2520This%2520includes%2520offensive%2520language%252C%2520such%2520as%2520insults%252C%2520racial%2520slurs%252C%250Aand%2520sexually%2520explicit%2520terms%252C%2520posing%2520significant%2520risks%2520to%2520users.%2520We%2520show%2520that%250Aall%2520state-of-the-art%2520DMs%2520%2528e.g.%252C%2520SD3%252C%2520Flux%252C%2520DeepFloyd%2520IF%2529%2520and%2520VARs%2520%2528e.g.%252C%250AInfinity%2529%2520are%2520vulnerable%2520to%2520this%2520issue.%2520Through%2520extensive%2520experiments%252C%2520we%250Ademonstrate%2520that%2520existing%2520mitigation%2520techniques%252C%2520effective%2520for%2520visual%2520content%252C%250Afail%2520to%2520prevent%2520harmful%2520text%2520generation%2520while%2520substantially%2520degrading%2520benign%250Atext%2520generation.%2520As%2520an%2520initial%2520step%2520toward%2520addressing%2520this%2520threat%252C%2520we%2520explore%250Asafety%2520fine-tuning%2520of%2520the%2520text%2520encoder%2520underlying%2520major%2520DM%2520architectures%2520using%250Aa%2520customized%2520dataset.%2520Thereby%252C%2520we%2520suppress%2520NSFW%2520generation%2520while%2520preserving%250Aoverall%2520image%2520and%2520text%2520generation%2520quality.%2520Finally%252C%2520to%2520advance%2520research%2520in%2520this%250Aarea%252C%2520we%2520introduce%2520ToxicBench%252C%2520an%2520open-source%2520benchmark%2520for%2520evaluating%2520NSFW%250Atext%2520generation%2520in%2520images.%2520ToxicBench%2520provides%2520a%2520curated%2520dataset%2520of%2520harmful%250Aprompts%252C%2520new%2520metrics%252C%2520and%2520an%2520evaluation%2520pipeline%2520assessing%2520both%2520NSFW-ness%2520and%250Ageneration%2520quality.%2520Our%2520benchmark%2520aims%2520to%2520guide%2520future%2520efforts%2520in%2520mitigating%250ANSFW%2520text%2520generation%2520in%2520text-to-image%2520models%2520and%2520is%2520available%2520at%250Ahttps%253A//github.com/sprintml/ToxicBench%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05066v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beautiful%20Images%2C%20Toxic%20Words%3A%20Understanding%20and%20Addressing%20Offensive%0A%20%20Text%20in%20Generated%20Images&entry.906535625=Aditya%20Kumar%20and%20Tom%20Blanchard%20and%20Adam%20Dziedzic%20and%20Franziska%20Boenisch&entry.1292438233=%20%20State-of-the-art%20visual%20generation%20models%2C%20such%20as%20Diffusion%20Models%20%28DMs%29%20and%0AVision%20Auto-Regressive%20Models%20%28VARs%29%2C%20produce%20highly%20realistic%20images.%20While%0Aprior%20work%20has%20successfully%20mitigated%20Not%20Safe%20For%20Work%20%28NSFW%29%20content%20in%20the%0Avisual%20domain%2C%20we%20identify%20a%20novel%20threat%3A%20the%20generation%20of%20NSFW%20text%20embedded%0Awithin%20images.%20This%20includes%20offensive%20language%2C%20such%20as%20insults%2C%20racial%20slurs%2C%0Aand%20sexually%20explicit%20terms%2C%20posing%20significant%20risks%20to%20users.%20We%20show%20that%0Aall%20state-of-the-art%20DMs%20%28e.g.%2C%20SD3%2C%20Flux%2C%20DeepFloyd%20IF%29%20and%20VARs%20%28e.g.%2C%0AInfinity%29%20are%20vulnerable%20to%20this%20issue.%20Through%20extensive%20experiments%2C%20we%0Ademonstrate%20that%20existing%20mitigation%20techniques%2C%20effective%20for%20visual%20content%2C%0Afail%20to%20prevent%20harmful%20text%20generation%20while%20substantially%20degrading%20benign%0Atext%20generation.%20As%20an%20initial%20step%20toward%20addressing%20this%20threat%2C%20we%20explore%0Asafety%20fine-tuning%20of%20the%20text%20encoder%20underlying%20major%20DM%20architectures%20using%0Aa%20customized%20dataset.%20Thereby%2C%20we%20suppress%20NSFW%20generation%20while%20preserving%0Aoverall%20image%20and%20text%20generation%20quality.%20Finally%2C%20to%20advance%20research%20in%20this%0Aarea%2C%20we%20introduce%20ToxicBench%2C%20an%20open-source%20benchmark%20for%20evaluating%20NSFW%0Atext%20generation%20in%20images.%20ToxicBench%20provides%20a%20curated%20dataset%20of%20harmful%0Aprompts%2C%20new%20metrics%2C%20and%20an%20evaluation%20pipeline%20assessing%20both%20NSFW-ness%20and%0Ageneration%20quality.%20Our%20benchmark%20aims%20to%20guide%20future%20efforts%20in%20mitigating%0ANSFW%20text%20generation%20in%20text-to-image%20models%20and%20is%20available%20at%0Ahttps%3A//github.com/sprintml/ToxicBench%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05066v1&entry.124074799=Read"},
{"title": "SynCo: Synthetic Hard Negatives in Contrastive Learning for Better\n  Unsupervised Visual Representations", "author": "Nikolaos Giakoumoglou and Tania Stathaki", "abstract": "  Contrastive learning has become a dominant approach in self-supervised visual\nrepresentation learning, but efficiently leveraging hard negatives, which are\nsamples closely resembling the anchor, remains challenging. We introduce SynCo\n(Synthetic negatives in Contrastive learning), a novel approach that improves\nmodel performance by generating synthetic hard negatives on the representation\nspace. Building on the MoCo framework, SynCo introduces six strategies for\ncreating diverse synthetic hard negatives on-the-fly with minimal computational\noverhead. SynCo achieves faster training and strong representation learning,\nsurpassing MoCo-v2 by +0.4% and MoCHI by +1.0% on ImageNet ILSVRC-2012 linear\nevaluation. It also transfers more effectively to detection tasks achieving\nstrong results on PASCAL VOC detection (57.2% AP) and significantly improving\nover MoCo-v2 on COCO detection (+1.0% AP) and instance segmentation (+0.8% AP).\nOur synthetic hard negative generation approach significantly enhances visual\nrepresentations learned through self-supervised contrastive learning.\n", "link": "http://arxiv.org/abs/2410.02401v6", "date": "2025-02-07", "relevancy": 2.1222, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5542}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5463}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynCo%3A%20Synthetic%20Hard%20Negatives%20in%20Contrastive%20Learning%20for%20Better%0A%20%20Unsupervised%20Visual%20Representations&body=Title%3A%20SynCo%3A%20Synthetic%20Hard%20Negatives%20in%20Contrastive%20Learning%20for%20Better%0A%20%20Unsupervised%20Visual%20Representations%0AAuthor%3A%20Nikolaos%20Giakoumoglou%20and%20Tania%20Stathaki%0AAbstract%3A%20%20%20Contrastive%20learning%20has%20become%20a%20dominant%20approach%20in%20self-supervised%20visual%0Arepresentation%20learning%2C%20but%20efficiently%20leveraging%20hard%20negatives%2C%20which%20are%0Asamples%20closely%20resembling%20the%20anchor%2C%20remains%20challenging.%20We%20introduce%20SynCo%0A%28Synthetic%20negatives%20in%20Contrastive%20learning%29%2C%20a%20novel%20approach%20that%20improves%0Amodel%20performance%20by%20generating%20synthetic%20hard%20negatives%20on%20the%20representation%0Aspace.%20Building%20on%20the%20MoCo%20framework%2C%20SynCo%20introduces%20six%20strategies%20for%0Acreating%20diverse%20synthetic%20hard%20negatives%20on-the-fly%20with%20minimal%20computational%0Aoverhead.%20SynCo%20achieves%20faster%20training%20and%20strong%20representation%20learning%2C%0Asurpassing%20MoCo-v2%20by%20%2B0.4%25%20and%20MoCHI%20by%20%2B1.0%25%20on%20ImageNet%20ILSVRC-2012%20linear%0Aevaluation.%20It%20also%20transfers%20more%20effectively%20to%20detection%20tasks%20achieving%0Astrong%20results%20on%20PASCAL%20VOC%20detection%20%2857.2%25%20AP%29%20and%20significantly%20improving%0Aover%20MoCo-v2%20on%20COCO%20detection%20%28%2B1.0%25%20AP%29%20and%20instance%20segmentation%20%28%2B0.8%25%20AP%29.%0AOur%20synthetic%20hard%20negative%20generation%20approach%20significantly%20enhances%20visual%0Arepresentations%20learned%20through%20self-supervised%20contrastive%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02401v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynCo%253A%2520Synthetic%2520Hard%2520Negatives%2520in%2520Contrastive%2520Learning%2520for%2520Better%250A%2520%2520Unsupervised%2520Visual%2520Representations%26entry.906535625%3DNikolaos%2520Giakoumoglou%2520and%2520Tania%2520Stathaki%26entry.1292438233%3D%2520%2520Contrastive%2520learning%2520has%2520become%2520a%2520dominant%2520approach%2520in%2520self-supervised%2520visual%250Arepresentation%2520learning%252C%2520but%2520efficiently%2520leveraging%2520hard%2520negatives%252C%2520which%2520are%250Asamples%2520closely%2520resembling%2520the%2520anchor%252C%2520remains%2520challenging.%2520We%2520introduce%2520SynCo%250A%2528Synthetic%2520negatives%2520in%2520Contrastive%2520learning%2529%252C%2520a%2520novel%2520approach%2520that%2520improves%250Amodel%2520performance%2520by%2520generating%2520synthetic%2520hard%2520negatives%2520on%2520the%2520representation%250Aspace.%2520Building%2520on%2520the%2520MoCo%2520framework%252C%2520SynCo%2520introduces%2520six%2520strategies%2520for%250Acreating%2520diverse%2520synthetic%2520hard%2520negatives%2520on-the-fly%2520with%2520minimal%2520computational%250Aoverhead.%2520SynCo%2520achieves%2520faster%2520training%2520and%2520strong%2520representation%2520learning%252C%250Asurpassing%2520MoCo-v2%2520by%2520%252B0.4%2525%2520and%2520MoCHI%2520by%2520%252B1.0%2525%2520on%2520ImageNet%2520ILSVRC-2012%2520linear%250Aevaluation.%2520It%2520also%2520transfers%2520more%2520effectively%2520to%2520detection%2520tasks%2520achieving%250Astrong%2520results%2520on%2520PASCAL%2520VOC%2520detection%2520%252857.2%2525%2520AP%2529%2520and%2520significantly%2520improving%250Aover%2520MoCo-v2%2520on%2520COCO%2520detection%2520%2528%252B1.0%2525%2520AP%2529%2520and%2520instance%2520segmentation%2520%2528%252B0.8%2525%2520AP%2529.%250AOur%2520synthetic%2520hard%2520negative%2520generation%2520approach%2520significantly%2520enhances%2520visual%250Arepresentations%2520learned%2520through%2520self-supervised%2520contrastive%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02401v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynCo%3A%20Synthetic%20Hard%20Negatives%20in%20Contrastive%20Learning%20for%20Better%0A%20%20Unsupervised%20Visual%20Representations&entry.906535625=Nikolaos%20Giakoumoglou%20and%20Tania%20Stathaki&entry.1292438233=%20%20Contrastive%20learning%20has%20become%20a%20dominant%20approach%20in%20self-supervised%20visual%0Arepresentation%20learning%2C%20but%20efficiently%20leveraging%20hard%20negatives%2C%20which%20are%0Asamples%20closely%20resembling%20the%20anchor%2C%20remains%20challenging.%20We%20introduce%20SynCo%0A%28Synthetic%20negatives%20in%20Contrastive%20learning%29%2C%20a%20novel%20approach%20that%20improves%0Amodel%20performance%20by%20generating%20synthetic%20hard%20negatives%20on%20the%20representation%0Aspace.%20Building%20on%20the%20MoCo%20framework%2C%20SynCo%20introduces%20six%20strategies%20for%0Acreating%20diverse%20synthetic%20hard%20negatives%20on-the-fly%20with%20minimal%20computational%0Aoverhead.%20SynCo%20achieves%20faster%20training%20and%20strong%20representation%20learning%2C%0Asurpassing%20MoCo-v2%20by%20%2B0.4%25%20and%20MoCHI%20by%20%2B1.0%25%20on%20ImageNet%20ILSVRC-2012%20linear%0Aevaluation.%20It%20also%20transfers%20more%20effectively%20to%20detection%20tasks%20achieving%0Astrong%20results%20on%20PASCAL%20VOC%20detection%20%2857.2%25%20AP%29%20and%20significantly%20improving%0Aover%20MoCo-v2%20on%20COCO%20detection%20%28%2B1.0%25%20AP%29%20and%20instance%20segmentation%20%28%2B0.8%25%20AP%29.%0AOur%20synthetic%20hard%20negative%20generation%20approach%20significantly%20enhances%20visual%0Arepresentations%20learned%20through%20self-supervised%20contrastive%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02401v6&entry.124074799=Read"},
{"title": "ConFIG: Towards Conflict-free Training of Physics Informed Neural\n  Networks", "author": "Qiang Liu and Mengyu Chu and Nils Thuerey", "abstract": "  The loss functions of many learning problems contain multiple additive terms\nthat can disagree and yield conflicting update directions. For Physics-Informed\nNeural Networks (PINNs), loss terms on initial/boundary conditions and physics\nequations are particularly interesting as they are well-established as highly\ndifficult tasks. To improve learning the challenging multi-objective task posed\nby PINNs, we propose the ConFIG method, which provides conflict-free updates by\nensuring a positive dot product between the final update and each loss-specific\ngradient. It also maintains consistent optimization rates for all loss terms\nand dynamically adjusts gradient magnitudes based on conflict levels. We\nadditionally leverage momentum to accelerate optimizations by alternating the\nback-propagation of different loss terms. We provide a mathematical proof\nshowing the convergence of the ConFIG method, and it is evaluated across a\nrange of challenging PINN scenarios. ConFIG consistently shows superior\nperformance and runtime compared to baseline methods. We also test the proposed\nmethod in a classic multi-task benchmark, where the ConFIG method likewise\nexhibits a highly promising performance. Source code is available at\nhttps://tum-pbs.github.io/ConFIG\n", "link": "http://arxiv.org/abs/2408.11104v2", "date": "2025-02-07", "relevancy": 2.1203, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5506}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5341}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConFIG%3A%20Towards%20Conflict-free%20Training%20of%20Physics%20Informed%20Neural%0A%20%20Networks&body=Title%3A%20ConFIG%3A%20Towards%20Conflict-free%20Training%20of%20Physics%20Informed%20Neural%0A%20%20Networks%0AAuthor%3A%20Qiang%20Liu%20and%20Mengyu%20Chu%20and%20Nils%20Thuerey%0AAbstract%3A%20%20%20The%20loss%20functions%20of%20many%20learning%20problems%20contain%20multiple%20additive%20terms%0Athat%20can%20disagree%20and%20yield%20conflicting%20update%20directions.%20For%20Physics-Informed%0ANeural%20Networks%20%28PINNs%29%2C%20loss%20terms%20on%20initial/boundary%20conditions%20and%20physics%0Aequations%20are%20particularly%20interesting%20as%20they%20are%20well-established%20as%20highly%0Adifficult%20tasks.%20To%20improve%20learning%20the%20challenging%20multi-objective%20task%20posed%0Aby%20PINNs%2C%20we%20propose%20the%20ConFIG%20method%2C%20which%20provides%20conflict-free%20updates%20by%0Aensuring%20a%20positive%20dot%20product%20between%20the%20final%20update%20and%20each%20loss-specific%0Agradient.%20It%20also%20maintains%20consistent%20optimization%20rates%20for%20all%20loss%20terms%0Aand%20dynamically%20adjusts%20gradient%20magnitudes%20based%20on%20conflict%20levels.%20We%0Aadditionally%20leverage%20momentum%20to%20accelerate%20optimizations%20by%20alternating%20the%0Aback-propagation%20of%20different%20loss%20terms.%20We%20provide%20a%20mathematical%20proof%0Ashowing%20the%20convergence%20of%20the%20ConFIG%20method%2C%20and%20it%20is%20evaluated%20across%20a%0Arange%20of%20challenging%20PINN%20scenarios.%20ConFIG%20consistently%20shows%20superior%0Aperformance%20and%20runtime%20compared%20to%20baseline%20methods.%20We%20also%20test%20the%20proposed%0Amethod%20in%20a%20classic%20multi-task%20benchmark%2C%20where%20the%20ConFIG%20method%20likewise%0Aexhibits%20a%20highly%20promising%20performance.%20Source%20code%20is%20available%20at%0Ahttps%3A//tum-pbs.github.io/ConFIG%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11104v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConFIG%253A%2520Towards%2520Conflict-free%2520Training%2520of%2520Physics%2520Informed%2520Neural%250A%2520%2520Networks%26entry.906535625%3DQiang%2520Liu%2520and%2520Mengyu%2520Chu%2520and%2520Nils%2520Thuerey%26entry.1292438233%3D%2520%2520The%2520loss%2520functions%2520of%2520many%2520learning%2520problems%2520contain%2520multiple%2520additive%2520terms%250Athat%2520can%2520disagree%2520and%2520yield%2520conflicting%2520update%2520directions.%2520For%2520Physics-Informed%250ANeural%2520Networks%2520%2528PINNs%2529%252C%2520loss%2520terms%2520on%2520initial/boundary%2520conditions%2520and%2520physics%250Aequations%2520are%2520particularly%2520interesting%2520as%2520they%2520are%2520well-established%2520as%2520highly%250Adifficult%2520tasks.%2520To%2520improve%2520learning%2520the%2520challenging%2520multi-objective%2520task%2520posed%250Aby%2520PINNs%252C%2520we%2520propose%2520the%2520ConFIG%2520method%252C%2520which%2520provides%2520conflict-free%2520updates%2520by%250Aensuring%2520a%2520positive%2520dot%2520product%2520between%2520the%2520final%2520update%2520and%2520each%2520loss-specific%250Agradient.%2520It%2520also%2520maintains%2520consistent%2520optimization%2520rates%2520for%2520all%2520loss%2520terms%250Aand%2520dynamically%2520adjusts%2520gradient%2520magnitudes%2520based%2520on%2520conflict%2520levels.%2520We%250Aadditionally%2520leverage%2520momentum%2520to%2520accelerate%2520optimizations%2520by%2520alternating%2520the%250Aback-propagation%2520of%2520different%2520loss%2520terms.%2520We%2520provide%2520a%2520mathematical%2520proof%250Ashowing%2520the%2520convergence%2520of%2520the%2520ConFIG%2520method%252C%2520and%2520it%2520is%2520evaluated%2520across%2520a%250Arange%2520of%2520challenging%2520PINN%2520scenarios.%2520ConFIG%2520consistently%2520shows%2520superior%250Aperformance%2520and%2520runtime%2520compared%2520to%2520baseline%2520methods.%2520We%2520also%2520test%2520the%2520proposed%250Amethod%2520in%2520a%2520classic%2520multi-task%2520benchmark%252C%2520where%2520the%2520ConFIG%2520method%2520likewise%250Aexhibits%2520a%2520highly%2520promising%2520performance.%2520Source%2520code%2520is%2520available%2520at%250Ahttps%253A//tum-pbs.github.io/ConFIG%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11104v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConFIG%3A%20Towards%20Conflict-free%20Training%20of%20Physics%20Informed%20Neural%0A%20%20Networks&entry.906535625=Qiang%20Liu%20and%20Mengyu%20Chu%20and%20Nils%20Thuerey&entry.1292438233=%20%20The%20loss%20functions%20of%20many%20learning%20problems%20contain%20multiple%20additive%20terms%0Athat%20can%20disagree%20and%20yield%20conflicting%20update%20directions.%20For%20Physics-Informed%0ANeural%20Networks%20%28PINNs%29%2C%20loss%20terms%20on%20initial/boundary%20conditions%20and%20physics%0Aequations%20are%20particularly%20interesting%20as%20they%20are%20well-established%20as%20highly%0Adifficult%20tasks.%20To%20improve%20learning%20the%20challenging%20multi-objective%20task%20posed%0Aby%20PINNs%2C%20we%20propose%20the%20ConFIG%20method%2C%20which%20provides%20conflict-free%20updates%20by%0Aensuring%20a%20positive%20dot%20product%20between%20the%20final%20update%20and%20each%20loss-specific%0Agradient.%20It%20also%20maintains%20consistent%20optimization%20rates%20for%20all%20loss%20terms%0Aand%20dynamically%20adjusts%20gradient%20magnitudes%20based%20on%20conflict%20levels.%20We%0Aadditionally%20leverage%20momentum%20to%20accelerate%20optimizations%20by%20alternating%20the%0Aback-propagation%20of%20different%20loss%20terms.%20We%20provide%20a%20mathematical%20proof%0Ashowing%20the%20convergence%20of%20the%20ConFIG%20method%2C%20and%20it%20is%20evaluated%20across%20a%0Arange%20of%20challenging%20PINN%20scenarios.%20ConFIG%20consistently%20shows%20superior%0Aperformance%20and%20runtime%20compared%20to%20baseline%20methods.%20We%20also%20test%20the%20proposed%0Amethod%20in%20a%20classic%20multi-task%20benchmark%2C%20where%20the%20ConFIG%20method%20likewise%0Aexhibits%20a%20highly%20promising%20performance.%20Source%20code%20is%20available%20at%0Ahttps%3A//tum-pbs.github.io/ConFIG%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11104v2&entry.124074799=Read"},
{"title": "Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth\n  Approach", "author": "Jonas Geiping and Sean McLeish and Neel Jain and John Kirchenbauer and Siddharth Singh and Brian R. Bartoldson and Bhavya Kailkhura and Abhinav Bhatele and Tom Goldstein", "abstract": "  We study a novel language model architecture that is capable of scaling\ntest-time computation by implicitly reasoning in latent space. Our model works\nby iterating a recurrent block, thereby unrolling to arbitrary depth at\ntest-time. This stands in contrast to mainstream reasoning models that scale up\ncompute by producing more tokens. Unlike approaches based on chain-of-thought,\nour approach does not require any specialized training data, can work with\nsmall context windows, and can capture types of reasoning that are not easily\nrepresented in words. We scale a proof-of-concept model to 3.5 billion\nparameters and 800 billion tokens. We show that the resulting model can improve\nits performance on reasoning benchmarks, sometimes dramatically, up to a\ncomputation load equivalent to 50 billion parameters.\n", "link": "http://arxiv.org/abs/2502.05171v1", "date": "2025-02-07", "relevancy": 2.0993, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5442}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.521}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20up%20Test-Time%20Compute%20with%20Latent%20Reasoning%3A%20A%20Recurrent%20Depth%0A%20%20Approach&body=Title%3A%20Scaling%20up%20Test-Time%20Compute%20with%20Latent%20Reasoning%3A%20A%20Recurrent%20Depth%0A%20%20Approach%0AAuthor%3A%20Jonas%20Geiping%20and%20Sean%20McLeish%20and%20Neel%20Jain%20and%20John%20Kirchenbauer%20and%20Siddharth%20Singh%20and%20Brian%20R.%20Bartoldson%20and%20Bhavya%20Kailkhura%20and%20Abhinav%20Bhatele%20and%20Tom%20Goldstein%0AAbstract%3A%20%20%20We%20study%20a%20novel%20language%20model%20architecture%20that%20is%20capable%20of%20scaling%0Atest-time%20computation%20by%20implicitly%20reasoning%20in%20latent%20space.%20Our%20model%20works%0Aby%20iterating%20a%20recurrent%20block%2C%20thereby%20unrolling%20to%20arbitrary%20depth%20at%0Atest-time.%20This%20stands%20in%20contrast%20to%20mainstream%20reasoning%20models%20that%20scale%20up%0Acompute%20by%20producing%20more%20tokens.%20Unlike%20approaches%20based%20on%20chain-of-thought%2C%0Aour%20approach%20does%20not%20require%20any%20specialized%20training%20data%2C%20can%20work%20with%0Asmall%20context%20windows%2C%20and%20can%20capture%20types%20of%20reasoning%20that%20are%20not%20easily%0Arepresented%20in%20words.%20We%20scale%20a%20proof-of-concept%20model%20to%203.5%20billion%0Aparameters%20and%20800%20billion%20tokens.%20We%20show%20that%20the%20resulting%20model%20can%20improve%0Aits%20performance%20on%20reasoning%20benchmarks%2C%20sometimes%20dramatically%2C%20up%20to%20a%0Acomputation%20load%20equivalent%20to%2050%20billion%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05171v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520up%2520Test-Time%2520Compute%2520with%2520Latent%2520Reasoning%253A%2520A%2520Recurrent%2520Depth%250A%2520%2520Approach%26entry.906535625%3DJonas%2520Geiping%2520and%2520Sean%2520McLeish%2520and%2520Neel%2520Jain%2520and%2520John%2520Kirchenbauer%2520and%2520Siddharth%2520Singh%2520and%2520Brian%2520R.%2520Bartoldson%2520and%2520Bhavya%2520Kailkhura%2520and%2520Abhinav%2520Bhatele%2520and%2520Tom%2520Goldstein%26entry.1292438233%3D%2520%2520We%2520study%2520a%2520novel%2520language%2520model%2520architecture%2520that%2520is%2520capable%2520of%2520scaling%250Atest-time%2520computation%2520by%2520implicitly%2520reasoning%2520in%2520latent%2520space.%2520Our%2520model%2520works%250Aby%2520iterating%2520a%2520recurrent%2520block%252C%2520thereby%2520unrolling%2520to%2520arbitrary%2520depth%2520at%250Atest-time.%2520This%2520stands%2520in%2520contrast%2520to%2520mainstream%2520reasoning%2520models%2520that%2520scale%2520up%250Acompute%2520by%2520producing%2520more%2520tokens.%2520Unlike%2520approaches%2520based%2520on%2520chain-of-thought%252C%250Aour%2520approach%2520does%2520not%2520require%2520any%2520specialized%2520training%2520data%252C%2520can%2520work%2520with%250Asmall%2520context%2520windows%252C%2520and%2520can%2520capture%2520types%2520of%2520reasoning%2520that%2520are%2520not%2520easily%250Arepresented%2520in%2520words.%2520We%2520scale%2520a%2520proof-of-concept%2520model%2520to%25203.5%2520billion%250Aparameters%2520and%2520800%2520billion%2520tokens.%2520We%2520show%2520that%2520the%2520resulting%2520model%2520can%2520improve%250Aits%2520performance%2520on%2520reasoning%2520benchmarks%252C%2520sometimes%2520dramatically%252C%2520up%2520to%2520a%250Acomputation%2520load%2520equivalent%2520to%252050%2520billion%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05171v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20up%20Test-Time%20Compute%20with%20Latent%20Reasoning%3A%20A%20Recurrent%20Depth%0A%20%20Approach&entry.906535625=Jonas%20Geiping%20and%20Sean%20McLeish%20and%20Neel%20Jain%20and%20John%20Kirchenbauer%20and%20Siddharth%20Singh%20and%20Brian%20R.%20Bartoldson%20and%20Bhavya%20Kailkhura%20and%20Abhinav%20Bhatele%20and%20Tom%20Goldstein&entry.1292438233=%20%20We%20study%20a%20novel%20language%20model%20architecture%20that%20is%20capable%20of%20scaling%0Atest-time%20computation%20by%20implicitly%20reasoning%20in%20latent%20space.%20Our%20model%20works%0Aby%20iterating%20a%20recurrent%20block%2C%20thereby%20unrolling%20to%20arbitrary%20depth%20at%0Atest-time.%20This%20stands%20in%20contrast%20to%20mainstream%20reasoning%20models%20that%20scale%20up%0Acompute%20by%20producing%20more%20tokens.%20Unlike%20approaches%20based%20on%20chain-of-thought%2C%0Aour%20approach%20does%20not%20require%20any%20specialized%20training%20data%2C%20can%20work%20with%0Asmall%20context%20windows%2C%20and%20can%20capture%20types%20of%20reasoning%20that%20are%20not%20easily%0Arepresented%20in%20words.%20We%20scale%20a%20proof-of-concept%20model%20to%203.5%20billion%0Aparameters%20and%20800%20billion%20tokens.%20We%20show%20that%20the%20resulting%20model%20can%20improve%0Aits%20performance%20on%20reasoning%20benchmarks%2C%20sometimes%20dramatically%2C%20up%20to%20a%0Acomputation%20load%20equivalent%20to%2050%20billion%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05171v1&entry.124074799=Read"},
{"title": "CMamba: Learned Image Compression with State Space Models", "author": "Zhuojie Wu and Heming Du and Shuyun Wang and Ming Lu and Haiyang Sun and Yandong Guo and Xin Yu", "abstract": "  Learned Image Compression (LIC) has explored various architectures, such as\nConvolutional Neural Networks (CNNs) and transformers, in modeling image\ncontent distributions in order to achieve compression effectiveness. However,\nachieving high rate-distortion performance while maintaining low computational\ncomplexity (\\ie, parameters, FLOPs, and latency) remains challenging. In this\npaper, we propose a hybrid Convolution and State Space Models (SSMs) based\nimage compression framework, termed \\textit{CMamba}, to achieve superior\nrate-distortion performance with low computational complexity. Specifically,\nCMamba introduces two key components: a Content-Adaptive SSM (CA-SSM) module\nand a Context-Aware Entropy (CAE) module. First, we observed that SSMs excel in\nmodeling overall content but tend to lose high-frequency details. In contrast,\nCNNs are proficient at capturing local details. Motivated by this, we propose\nthe CA-SSM module that can dynamically fuse global content extracted by SSM\nblocks and local details captured by CNN blocks in both encoding and decoding\nstages. As a result, important image content is well preserved during\ncompression. Second, our proposed CAE module is designed to reduce spatial and\nchannel redundancies in latent representations after encoding. Specifically,\nour CAE leverages SSMs to parameterize the spatial content in latent\nrepresentations. Benefiting from SSMs, CAE significantly improves spatial\ncompression efficiency while reducing spatial content redundancies. Moreover,\nalong the channel dimension, CAE reduces inter-channel redundancies of latent\nrepresentations via an autoregressive manner, which can fully exploit prior\nknowledge from previous channels without sacrificing efficiency. Experimental\nresults demonstrate that CMamba achieves superior rate-distortion performance.\n", "link": "http://arxiv.org/abs/2502.04988v1", "date": "2025-02-07", "relevancy": 2.0945, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5407}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5238}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CMamba%3A%20Learned%20Image%20Compression%20with%20State%20Space%20Models&body=Title%3A%20CMamba%3A%20Learned%20Image%20Compression%20with%20State%20Space%20Models%0AAuthor%3A%20Zhuojie%20Wu%20and%20Heming%20Du%20and%20Shuyun%20Wang%20and%20Ming%20Lu%20and%20Haiyang%20Sun%20and%20Yandong%20Guo%20and%20Xin%20Yu%0AAbstract%3A%20%20%20Learned%20Image%20Compression%20%28LIC%29%20has%20explored%20various%20architectures%2C%20such%20as%0AConvolutional%20Neural%20Networks%20%28CNNs%29%20and%20transformers%2C%20in%20modeling%20image%0Acontent%20distributions%20in%20order%20to%20achieve%20compression%20effectiveness.%20However%2C%0Aachieving%20high%20rate-distortion%20performance%20while%20maintaining%20low%20computational%0Acomplexity%20%28%5Cie%2C%20parameters%2C%20FLOPs%2C%20and%20latency%29%20remains%20challenging.%20In%20this%0Apaper%2C%20we%20propose%20a%20hybrid%20Convolution%20and%20State%20Space%20Models%20%28SSMs%29%20based%0Aimage%20compression%20framework%2C%20termed%20%5Ctextit%7BCMamba%7D%2C%20to%20achieve%20superior%0Arate-distortion%20performance%20with%20low%20computational%20complexity.%20Specifically%2C%0ACMamba%20introduces%20two%20key%20components%3A%20a%20Content-Adaptive%20SSM%20%28CA-SSM%29%20module%0Aand%20a%20Context-Aware%20Entropy%20%28CAE%29%20module.%20First%2C%20we%20observed%20that%20SSMs%20excel%20in%0Amodeling%20overall%20content%20but%20tend%20to%20lose%20high-frequency%20details.%20In%20contrast%2C%0ACNNs%20are%20proficient%20at%20capturing%20local%20details.%20Motivated%20by%20this%2C%20we%20propose%0Athe%20CA-SSM%20module%20that%20can%20dynamically%20fuse%20global%20content%20extracted%20by%20SSM%0Ablocks%20and%20local%20details%20captured%20by%20CNN%20blocks%20in%20both%20encoding%20and%20decoding%0Astages.%20As%20a%20result%2C%20important%20image%20content%20is%20well%20preserved%20during%0Acompression.%20Second%2C%20our%20proposed%20CAE%20module%20is%20designed%20to%20reduce%20spatial%20and%0Achannel%20redundancies%20in%20latent%20representations%20after%20encoding.%20Specifically%2C%0Aour%20CAE%20leverages%20SSMs%20to%20parameterize%20the%20spatial%20content%20in%20latent%0Arepresentations.%20Benefiting%20from%20SSMs%2C%20CAE%20significantly%20improves%20spatial%0Acompression%20efficiency%20while%20reducing%20spatial%20content%20redundancies.%20Moreover%2C%0Aalong%20the%20channel%20dimension%2C%20CAE%20reduces%20inter-channel%20redundancies%20of%20latent%0Arepresentations%20via%20an%20autoregressive%20manner%2C%20which%20can%20fully%20exploit%20prior%0Aknowledge%20from%20previous%20channels%20without%20sacrificing%20efficiency.%20Experimental%0Aresults%20demonstrate%20that%20CMamba%20achieves%20superior%20rate-distortion%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04988v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCMamba%253A%2520Learned%2520Image%2520Compression%2520with%2520State%2520Space%2520Models%26entry.906535625%3DZhuojie%2520Wu%2520and%2520Heming%2520Du%2520and%2520Shuyun%2520Wang%2520and%2520Ming%2520Lu%2520and%2520Haiyang%2520Sun%2520and%2520Yandong%2520Guo%2520and%2520Xin%2520Yu%26entry.1292438233%3D%2520%2520Learned%2520Image%2520Compression%2520%2528LIC%2529%2520has%2520explored%2520various%2520architectures%252C%2520such%2520as%250AConvolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520and%2520transformers%252C%2520in%2520modeling%2520image%250Acontent%2520distributions%2520in%2520order%2520to%2520achieve%2520compression%2520effectiveness.%2520However%252C%250Aachieving%2520high%2520rate-distortion%2520performance%2520while%2520maintaining%2520low%2520computational%250Acomplexity%2520%2528%255Cie%252C%2520parameters%252C%2520FLOPs%252C%2520and%2520latency%2529%2520remains%2520challenging.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520hybrid%2520Convolution%2520and%2520State%2520Space%2520Models%2520%2528SSMs%2529%2520based%250Aimage%2520compression%2520framework%252C%2520termed%2520%255Ctextit%257BCMamba%257D%252C%2520to%2520achieve%2520superior%250Arate-distortion%2520performance%2520with%2520low%2520computational%2520complexity.%2520Specifically%252C%250ACMamba%2520introduces%2520two%2520key%2520components%253A%2520a%2520Content-Adaptive%2520SSM%2520%2528CA-SSM%2529%2520module%250Aand%2520a%2520Context-Aware%2520Entropy%2520%2528CAE%2529%2520module.%2520First%252C%2520we%2520observed%2520that%2520SSMs%2520excel%2520in%250Amodeling%2520overall%2520content%2520but%2520tend%2520to%2520lose%2520high-frequency%2520details.%2520In%2520contrast%252C%250ACNNs%2520are%2520proficient%2520at%2520capturing%2520local%2520details.%2520Motivated%2520by%2520this%252C%2520we%2520propose%250Athe%2520CA-SSM%2520module%2520that%2520can%2520dynamically%2520fuse%2520global%2520content%2520extracted%2520by%2520SSM%250Ablocks%2520and%2520local%2520details%2520captured%2520by%2520CNN%2520blocks%2520in%2520both%2520encoding%2520and%2520decoding%250Astages.%2520As%2520a%2520result%252C%2520important%2520image%2520content%2520is%2520well%2520preserved%2520during%250Acompression.%2520Second%252C%2520our%2520proposed%2520CAE%2520module%2520is%2520designed%2520to%2520reduce%2520spatial%2520and%250Achannel%2520redundancies%2520in%2520latent%2520representations%2520after%2520encoding.%2520Specifically%252C%250Aour%2520CAE%2520leverages%2520SSMs%2520to%2520parameterize%2520the%2520spatial%2520content%2520in%2520latent%250Arepresentations.%2520Benefiting%2520from%2520SSMs%252C%2520CAE%2520significantly%2520improves%2520spatial%250Acompression%2520efficiency%2520while%2520reducing%2520spatial%2520content%2520redundancies.%2520Moreover%252C%250Aalong%2520the%2520channel%2520dimension%252C%2520CAE%2520reduces%2520inter-channel%2520redundancies%2520of%2520latent%250Arepresentations%2520via%2520an%2520autoregressive%2520manner%252C%2520which%2520can%2520fully%2520exploit%2520prior%250Aknowledge%2520from%2520previous%2520channels%2520without%2520sacrificing%2520efficiency.%2520Experimental%250Aresults%2520demonstrate%2520that%2520CMamba%2520achieves%2520superior%2520rate-distortion%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04988v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CMamba%3A%20Learned%20Image%20Compression%20with%20State%20Space%20Models&entry.906535625=Zhuojie%20Wu%20and%20Heming%20Du%20and%20Shuyun%20Wang%20and%20Ming%20Lu%20and%20Haiyang%20Sun%20and%20Yandong%20Guo%20and%20Xin%20Yu&entry.1292438233=%20%20Learned%20Image%20Compression%20%28LIC%29%20has%20explored%20various%20architectures%2C%20such%20as%0AConvolutional%20Neural%20Networks%20%28CNNs%29%20and%20transformers%2C%20in%20modeling%20image%0Acontent%20distributions%20in%20order%20to%20achieve%20compression%20effectiveness.%20However%2C%0Aachieving%20high%20rate-distortion%20performance%20while%20maintaining%20low%20computational%0Acomplexity%20%28%5Cie%2C%20parameters%2C%20FLOPs%2C%20and%20latency%29%20remains%20challenging.%20In%20this%0Apaper%2C%20we%20propose%20a%20hybrid%20Convolution%20and%20State%20Space%20Models%20%28SSMs%29%20based%0Aimage%20compression%20framework%2C%20termed%20%5Ctextit%7BCMamba%7D%2C%20to%20achieve%20superior%0Arate-distortion%20performance%20with%20low%20computational%20complexity.%20Specifically%2C%0ACMamba%20introduces%20two%20key%20components%3A%20a%20Content-Adaptive%20SSM%20%28CA-SSM%29%20module%0Aand%20a%20Context-Aware%20Entropy%20%28CAE%29%20module.%20First%2C%20we%20observed%20that%20SSMs%20excel%20in%0Amodeling%20overall%20content%20but%20tend%20to%20lose%20high-frequency%20details.%20In%20contrast%2C%0ACNNs%20are%20proficient%20at%20capturing%20local%20details.%20Motivated%20by%20this%2C%20we%20propose%0Athe%20CA-SSM%20module%20that%20can%20dynamically%20fuse%20global%20content%20extracted%20by%20SSM%0Ablocks%20and%20local%20details%20captured%20by%20CNN%20blocks%20in%20both%20encoding%20and%20decoding%0Astages.%20As%20a%20result%2C%20important%20image%20content%20is%20well%20preserved%20during%0Acompression.%20Second%2C%20our%20proposed%20CAE%20module%20is%20designed%20to%20reduce%20spatial%20and%0Achannel%20redundancies%20in%20latent%20representations%20after%20encoding.%20Specifically%2C%0Aour%20CAE%20leverages%20SSMs%20to%20parameterize%20the%20spatial%20content%20in%20latent%0Arepresentations.%20Benefiting%20from%20SSMs%2C%20CAE%20significantly%20improves%20spatial%0Acompression%20efficiency%20while%20reducing%20spatial%20content%20redundancies.%20Moreover%2C%0Aalong%20the%20channel%20dimension%2C%20CAE%20reduces%20inter-channel%20redundancies%20of%20latent%0Arepresentations%20via%20an%20autoregressive%20manner%2C%20which%20can%20fully%20exploit%20prior%0Aknowledge%20from%20previous%20channels%20without%20sacrificing%20efficiency.%20Experimental%0Aresults%20demonstrate%20that%20CMamba%20achieves%20superior%20rate-distortion%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04988v1&entry.124074799=Read"},
{"title": "Optimistic Gradient Learning with Hessian Corrections for\n  High-Dimensional Black-Box Optimization", "author": "Yedidya Kfir and Elad Sarafian and Sarit Kraus and Yoram Louzoun", "abstract": "  Black-box algorithms are designed to optimize functions without relying on\ntheir underlying analytical structure or gradient information, making them\nessential when gradients are inaccessible or difficult to compute. Traditional\nmethods for solving black-box optimization (BBO) problems predominantly rely on\nnon-parametric models and struggle to scale to large input spaces. Conversely,\nparametric methods that model the function with neural estimators and obtain\ngradient signals via backpropagation may suffer from significant gradient\nerrors. A recent alternative, Explicit Gradient Learning (EGL), which directly\nlearns the gradient using a first-order Taylor approximation, has demonstrated\nsuperior performance over both parametric and non-parametric methods. In this\nwork, we propose two novel gradient learning variants to address the robustness\nchallenges posed by high-dimensional, complex, and highly non-linear problems.\nOptimistic Gradient Learning (OGL) introduces a bias toward lower regions in\nthe function landscape, while Higher-order Gradient Learning (HGL) incorporates\nsecond-order Taylor corrections to improve gradient accuracy. We combine these\napproaches into the unified OHGL algorithm, achieving state-of-the-art (SOTA)\nperformance on the synthetic COCO suite. Additionally, we demonstrate OHGLs\napplicability to high-dimensional real-world machine learning (ML) tasks such\nas adversarial training and code generation. Our results highlight OHGLs\nability to generate stronger candidates, offering a valuable tool for ML\nresearchers and practitioners tackling high-dimensional, non-linear\noptimization challenges\n", "link": "http://arxiv.org/abs/2502.04829v1", "date": "2025-02-07", "relevancy": 2.0807, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5273}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5161}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimistic%20Gradient%20Learning%20with%20Hessian%20Corrections%20for%0A%20%20High-Dimensional%20Black-Box%20Optimization&body=Title%3A%20Optimistic%20Gradient%20Learning%20with%20Hessian%20Corrections%20for%0A%20%20High-Dimensional%20Black-Box%20Optimization%0AAuthor%3A%20Yedidya%20Kfir%20and%20Elad%20Sarafian%20and%20Sarit%20Kraus%20and%20Yoram%20Louzoun%0AAbstract%3A%20%20%20Black-box%20algorithms%20are%20designed%20to%20optimize%20functions%20without%20relying%20on%0Atheir%20underlying%20analytical%20structure%20or%20gradient%20information%2C%20making%20them%0Aessential%20when%20gradients%20are%20inaccessible%20or%20difficult%20to%20compute.%20Traditional%0Amethods%20for%20solving%20black-box%20optimization%20%28BBO%29%20problems%20predominantly%20rely%20on%0Anon-parametric%20models%20and%20struggle%20to%20scale%20to%20large%20input%20spaces.%20Conversely%2C%0Aparametric%20methods%20that%20model%20the%20function%20with%20neural%20estimators%20and%20obtain%0Agradient%20signals%20via%20backpropagation%20may%20suffer%20from%20significant%20gradient%0Aerrors.%20A%20recent%20alternative%2C%20Explicit%20Gradient%20Learning%20%28EGL%29%2C%20which%20directly%0Alearns%20the%20gradient%20using%20a%20first-order%20Taylor%20approximation%2C%20has%20demonstrated%0Asuperior%20performance%20over%20both%20parametric%20and%20non-parametric%20methods.%20In%20this%0Awork%2C%20we%20propose%20two%20novel%20gradient%20learning%20variants%20to%20address%20the%20robustness%0Achallenges%20posed%20by%20high-dimensional%2C%20complex%2C%20and%20highly%20non-linear%20problems.%0AOptimistic%20Gradient%20Learning%20%28OGL%29%20introduces%20a%20bias%20toward%20lower%20regions%20in%0Athe%20function%20landscape%2C%20while%20Higher-order%20Gradient%20Learning%20%28HGL%29%20incorporates%0Asecond-order%20Taylor%20corrections%20to%20improve%20gradient%20accuracy.%20We%20combine%20these%0Aapproaches%20into%20the%20unified%20OHGL%20algorithm%2C%20achieving%20state-of-the-art%20%28SOTA%29%0Aperformance%20on%20the%20synthetic%20COCO%20suite.%20Additionally%2C%20we%20demonstrate%20OHGLs%0Aapplicability%20to%20high-dimensional%20real-world%20machine%20learning%20%28ML%29%20tasks%20such%0Aas%20adversarial%20training%20and%20code%20generation.%20Our%20results%20highlight%20OHGLs%0Aability%20to%20generate%20stronger%20candidates%2C%20offering%20a%20valuable%20tool%20for%20ML%0Aresearchers%20and%20practitioners%20tackling%20high-dimensional%2C%20non-linear%0Aoptimization%20challenges%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04829v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimistic%2520Gradient%2520Learning%2520with%2520Hessian%2520Corrections%2520for%250A%2520%2520High-Dimensional%2520Black-Box%2520Optimization%26entry.906535625%3DYedidya%2520Kfir%2520and%2520Elad%2520Sarafian%2520and%2520Sarit%2520Kraus%2520and%2520Yoram%2520Louzoun%26entry.1292438233%3D%2520%2520Black-box%2520algorithms%2520are%2520designed%2520to%2520optimize%2520functions%2520without%2520relying%2520on%250Atheir%2520underlying%2520analytical%2520structure%2520or%2520gradient%2520information%252C%2520making%2520them%250Aessential%2520when%2520gradients%2520are%2520inaccessible%2520or%2520difficult%2520to%2520compute.%2520Traditional%250Amethods%2520for%2520solving%2520black-box%2520optimization%2520%2528BBO%2529%2520problems%2520predominantly%2520rely%2520on%250Anon-parametric%2520models%2520and%2520struggle%2520to%2520scale%2520to%2520large%2520input%2520spaces.%2520Conversely%252C%250Aparametric%2520methods%2520that%2520model%2520the%2520function%2520with%2520neural%2520estimators%2520and%2520obtain%250Agradient%2520signals%2520via%2520backpropagation%2520may%2520suffer%2520from%2520significant%2520gradient%250Aerrors.%2520A%2520recent%2520alternative%252C%2520Explicit%2520Gradient%2520Learning%2520%2528EGL%2529%252C%2520which%2520directly%250Alearns%2520the%2520gradient%2520using%2520a%2520first-order%2520Taylor%2520approximation%252C%2520has%2520demonstrated%250Asuperior%2520performance%2520over%2520both%2520parametric%2520and%2520non-parametric%2520methods.%2520In%2520this%250Awork%252C%2520we%2520propose%2520two%2520novel%2520gradient%2520learning%2520variants%2520to%2520address%2520the%2520robustness%250Achallenges%2520posed%2520by%2520high-dimensional%252C%2520complex%252C%2520and%2520highly%2520non-linear%2520problems.%250AOptimistic%2520Gradient%2520Learning%2520%2528OGL%2529%2520introduces%2520a%2520bias%2520toward%2520lower%2520regions%2520in%250Athe%2520function%2520landscape%252C%2520while%2520Higher-order%2520Gradient%2520Learning%2520%2528HGL%2529%2520incorporates%250Asecond-order%2520Taylor%2520corrections%2520to%2520improve%2520gradient%2520accuracy.%2520We%2520combine%2520these%250Aapproaches%2520into%2520the%2520unified%2520OHGL%2520algorithm%252C%2520achieving%2520state-of-the-art%2520%2528SOTA%2529%250Aperformance%2520on%2520the%2520synthetic%2520COCO%2520suite.%2520Additionally%252C%2520we%2520demonstrate%2520OHGLs%250Aapplicability%2520to%2520high-dimensional%2520real-world%2520machine%2520learning%2520%2528ML%2529%2520tasks%2520such%250Aas%2520adversarial%2520training%2520and%2520code%2520generation.%2520Our%2520results%2520highlight%2520OHGLs%250Aability%2520to%2520generate%2520stronger%2520candidates%252C%2520offering%2520a%2520valuable%2520tool%2520for%2520ML%250Aresearchers%2520and%2520practitioners%2520tackling%2520high-dimensional%252C%2520non-linear%250Aoptimization%2520challenges%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04829v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimistic%20Gradient%20Learning%20with%20Hessian%20Corrections%20for%0A%20%20High-Dimensional%20Black-Box%20Optimization&entry.906535625=Yedidya%20Kfir%20and%20Elad%20Sarafian%20and%20Sarit%20Kraus%20and%20Yoram%20Louzoun&entry.1292438233=%20%20Black-box%20algorithms%20are%20designed%20to%20optimize%20functions%20without%20relying%20on%0Atheir%20underlying%20analytical%20structure%20or%20gradient%20information%2C%20making%20them%0Aessential%20when%20gradients%20are%20inaccessible%20or%20difficult%20to%20compute.%20Traditional%0Amethods%20for%20solving%20black-box%20optimization%20%28BBO%29%20problems%20predominantly%20rely%20on%0Anon-parametric%20models%20and%20struggle%20to%20scale%20to%20large%20input%20spaces.%20Conversely%2C%0Aparametric%20methods%20that%20model%20the%20function%20with%20neural%20estimators%20and%20obtain%0Agradient%20signals%20via%20backpropagation%20may%20suffer%20from%20significant%20gradient%0Aerrors.%20A%20recent%20alternative%2C%20Explicit%20Gradient%20Learning%20%28EGL%29%2C%20which%20directly%0Alearns%20the%20gradient%20using%20a%20first-order%20Taylor%20approximation%2C%20has%20demonstrated%0Asuperior%20performance%20over%20both%20parametric%20and%20non-parametric%20methods.%20In%20this%0Awork%2C%20we%20propose%20two%20novel%20gradient%20learning%20variants%20to%20address%20the%20robustness%0Achallenges%20posed%20by%20high-dimensional%2C%20complex%2C%20and%20highly%20non-linear%20problems.%0AOptimistic%20Gradient%20Learning%20%28OGL%29%20introduces%20a%20bias%20toward%20lower%20regions%20in%0Athe%20function%20landscape%2C%20while%20Higher-order%20Gradient%20Learning%20%28HGL%29%20incorporates%0Asecond-order%20Taylor%20corrections%20to%20improve%20gradient%20accuracy.%20We%20combine%20these%0Aapproaches%20into%20the%20unified%20OHGL%20algorithm%2C%20achieving%20state-of-the-art%20%28SOTA%29%0Aperformance%20on%20the%20synthetic%20COCO%20suite.%20Additionally%2C%20we%20demonstrate%20OHGLs%0Aapplicability%20to%20high-dimensional%20real-world%20machine%20learning%20%28ML%29%20tasks%20such%0Aas%20adversarial%20training%20and%20code%20generation.%20Our%20results%20highlight%20OHGLs%0Aability%20to%20generate%20stronger%20candidates%2C%20offering%20a%20valuable%20tool%20for%20ML%0Aresearchers%20and%20practitioners%20tackling%20high-dimensional%2C%20non-linear%0Aoptimization%20challenges%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04829v1&entry.124074799=Read"},
{"title": "Adversarial Training Can Provably Improve Robustness: Theoretical\n  Analysis of Feature Learning Process Under Structured Data", "author": "Binghui Li and Yuanzhi Li", "abstract": "  Adversarial training is a widely-applied approach to training deep neural\nnetworks to be robust against adversarial perturbation. However, although\nadversarial training has achieved empirical success in practice, it still\nremains unclear why adversarial examples exist and how adversarial training\nmethods improve model robustness. In this paper, we provide a theoretical\nunderstanding of adversarial examples and adversarial training algorithms from\nthe perspective of feature learning theory. Specifically, we focus on a\nmultiple classification setting, where the structured data can be composed of\ntwo types of features: the robust features, which are resistant to perturbation\nbut sparse, and the non-robust features, which are susceptible to perturbation\nbut dense. We train a two-layer smoothed ReLU convolutional neural network to\nlearn our structured data. First, we prove that by using standard training\n(gradient descent over the empirical risk), the network learner primarily\nlearns the non-robust feature rather than the robust feature, which thereby\nleads to the adversarial examples that are generated by perturbations aligned\nwith negative non-robust feature directions. Then, we consider the\ngradient-based adversarial training algorithm, which runs gradient ascent to\nfind adversarial examples and runs gradient descent over the empirical risk at\nadversarial examples to update models. We show that the adversarial training\nmethod can provably strengthen the robust feature learning and suppress the\nnon-robust feature learning to improve the network robustness. Finally, we also\nempirically validate our theoretical findings with experiments on real-image\ndatasets, including MNIST, CIFAR10 and SVHN.\n", "link": "http://arxiv.org/abs/2410.08503v2", "date": "2025-02-07", "relevancy": 2.0758, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5411}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5104}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Training%20Can%20Provably%20Improve%20Robustness%3A%20Theoretical%0A%20%20Analysis%20of%20Feature%20Learning%20Process%20Under%20Structured%20Data&body=Title%3A%20Adversarial%20Training%20Can%20Provably%20Improve%20Robustness%3A%20Theoretical%0A%20%20Analysis%20of%20Feature%20Learning%20Process%20Under%20Structured%20Data%0AAuthor%3A%20Binghui%20Li%20and%20Yuanzhi%20Li%0AAbstract%3A%20%20%20Adversarial%20training%20is%20a%20widely-applied%20approach%20to%20training%20deep%20neural%0Anetworks%20to%20be%20robust%20against%20adversarial%20perturbation.%20However%2C%20although%0Aadversarial%20training%20has%20achieved%20empirical%20success%20in%20practice%2C%20it%20still%0Aremains%20unclear%20why%20adversarial%20examples%20exist%20and%20how%20adversarial%20training%0Amethods%20improve%20model%20robustness.%20In%20this%20paper%2C%20we%20provide%20a%20theoretical%0Aunderstanding%20of%20adversarial%20examples%20and%20adversarial%20training%20algorithms%20from%0Athe%20perspective%20of%20feature%20learning%20theory.%20Specifically%2C%20we%20focus%20on%20a%0Amultiple%20classification%20setting%2C%20where%20the%20structured%20data%20can%20be%20composed%20of%0Atwo%20types%20of%20features%3A%20the%20robust%20features%2C%20which%20are%20resistant%20to%20perturbation%0Abut%20sparse%2C%20and%20the%20non-robust%20features%2C%20which%20are%20susceptible%20to%20perturbation%0Abut%20dense.%20We%20train%20a%20two-layer%20smoothed%20ReLU%20convolutional%20neural%20network%20to%0Alearn%20our%20structured%20data.%20First%2C%20we%20prove%20that%20by%20using%20standard%20training%0A%28gradient%20descent%20over%20the%20empirical%20risk%29%2C%20the%20network%20learner%20primarily%0Alearns%20the%20non-robust%20feature%20rather%20than%20the%20robust%20feature%2C%20which%20thereby%0Aleads%20to%20the%20adversarial%20examples%20that%20are%20generated%20by%20perturbations%20aligned%0Awith%20negative%20non-robust%20feature%20directions.%20Then%2C%20we%20consider%20the%0Agradient-based%20adversarial%20training%20algorithm%2C%20which%20runs%20gradient%20ascent%20to%0Afind%20adversarial%20examples%20and%20runs%20gradient%20descent%20over%20the%20empirical%20risk%20at%0Aadversarial%20examples%20to%20update%20models.%20We%20show%20that%20the%20adversarial%20training%0Amethod%20can%20provably%20strengthen%20the%20robust%20feature%20learning%20and%20suppress%20the%0Anon-robust%20feature%20learning%20to%20improve%20the%20network%20robustness.%20Finally%2C%20we%20also%0Aempirically%20validate%20our%20theoretical%20findings%20with%20experiments%20on%20real-image%0Adatasets%2C%20including%20MNIST%2C%20CIFAR10%20and%20SVHN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08503v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Training%2520Can%2520Provably%2520Improve%2520Robustness%253A%2520Theoretical%250A%2520%2520Analysis%2520of%2520Feature%2520Learning%2520Process%2520Under%2520Structured%2520Data%26entry.906535625%3DBinghui%2520Li%2520and%2520Yuanzhi%2520Li%26entry.1292438233%3D%2520%2520Adversarial%2520training%2520is%2520a%2520widely-applied%2520approach%2520to%2520training%2520deep%2520neural%250Anetworks%2520to%2520be%2520robust%2520against%2520adversarial%2520perturbation.%2520However%252C%2520although%250Aadversarial%2520training%2520has%2520achieved%2520empirical%2520success%2520in%2520practice%252C%2520it%2520still%250Aremains%2520unclear%2520why%2520adversarial%2520examples%2520exist%2520and%2520how%2520adversarial%2520training%250Amethods%2520improve%2520model%2520robustness.%2520In%2520this%2520paper%252C%2520we%2520provide%2520a%2520theoretical%250Aunderstanding%2520of%2520adversarial%2520examples%2520and%2520adversarial%2520training%2520algorithms%2520from%250Athe%2520perspective%2520of%2520feature%2520learning%2520theory.%2520Specifically%252C%2520we%2520focus%2520on%2520a%250Amultiple%2520classification%2520setting%252C%2520where%2520the%2520structured%2520data%2520can%2520be%2520composed%2520of%250Atwo%2520types%2520of%2520features%253A%2520the%2520robust%2520features%252C%2520which%2520are%2520resistant%2520to%2520perturbation%250Abut%2520sparse%252C%2520and%2520the%2520non-robust%2520features%252C%2520which%2520are%2520susceptible%2520to%2520perturbation%250Abut%2520dense.%2520We%2520train%2520a%2520two-layer%2520smoothed%2520ReLU%2520convolutional%2520neural%2520network%2520to%250Alearn%2520our%2520structured%2520data.%2520First%252C%2520we%2520prove%2520that%2520by%2520using%2520standard%2520training%250A%2528gradient%2520descent%2520over%2520the%2520empirical%2520risk%2529%252C%2520the%2520network%2520learner%2520primarily%250Alearns%2520the%2520non-robust%2520feature%2520rather%2520than%2520the%2520robust%2520feature%252C%2520which%2520thereby%250Aleads%2520to%2520the%2520adversarial%2520examples%2520that%2520are%2520generated%2520by%2520perturbations%2520aligned%250Awith%2520negative%2520non-robust%2520feature%2520directions.%2520Then%252C%2520we%2520consider%2520the%250Agradient-based%2520adversarial%2520training%2520algorithm%252C%2520which%2520runs%2520gradient%2520ascent%2520to%250Afind%2520adversarial%2520examples%2520and%2520runs%2520gradient%2520descent%2520over%2520the%2520empirical%2520risk%2520at%250Aadversarial%2520examples%2520to%2520update%2520models.%2520We%2520show%2520that%2520the%2520adversarial%2520training%250Amethod%2520can%2520provably%2520strengthen%2520the%2520robust%2520feature%2520learning%2520and%2520suppress%2520the%250Anon-robust%2520feature%2520learning%2520to%2520improve%2520the%2520network%2520robustness.%2520Finally%252C%2520we%2520also%250Aempirically%2520validate%2520our%2520theoretical%2520findings%2520with%2520experiments%2520on%2520real-image%250Adatasets%252C%2520including%2520MNIST%252C%2520CIFAR10%2520and%2520SVHN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08503v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Training%20Can%20Provably%20Improve%20Robustness%3A%20Theoretical%0A%20%20Analysis%20of%20Feature%20Learning%20Process%20Under%20Structured%20Data&entry.906535625=Binghui%20Li%20and%20Yuanzhi%20Li&entry.1292438233=%20%20Adversarial%20training%20is%20a%20widely-applied%20approach%20to%20training%20deep%20neural%0Anetworks%20to%20be%20robust%20against%20adversarial%20perturbation.%20However%2C%20although%0Aadversarial%20training%20has%20achieved%20empirical%20success%20in%20practice%2C%20it%20still%0Aremains%20unclear%20why%20adversarial%20examples%20exist%20and%20how%20adversarial%20training%0Amethods%20improve%20model%20robustness.%20In%20this%20paper%2C%20we%20provide%20a%20theoretical%0Aunderstanding%20of%20adversarial%20examples%20and%20adversarial%20training%20algorithms%20from%0Athe%20perspective%20of%20feature%20learning%20theory.%20Specifically%2C%20we%20focus%20on%20a%0Amultiple%20classification%20setting%2C%20where%20the%20structured%20data%20can%20be%20composed%20of%0Atwo%20types%20of%20features%3A%20the%20robust%20features%2C%20which%20are%20resistant%20to%20perturbation%0Abut%20sparse%2C%20and%20the%20non-robust%20features%2C%20which%20are%20susceptible%20to%20perturbation%0Abut%20dense.%20We%20train%20a%20two-layer%20smoothed%20ReLU%20convolutional%20neural%20network%20to%0Alearn%20our%20structured%20data.%20First%2C%20we%20prove%20that%20by%20using%20standard%20training%0A%28gradient%20descent%20over%20the%20empirical%20risk%29%2C%20the%20network%20learner%20primarily%0Alearns%20the%20non-robust%20feature%20rather%20than%20the%20robust%20feature%2C%20which%20thereby%0Aleads%20to%20the%20adversarial%20examples%20that%20are%20generated%20by%20perturbations%20aligned%0Awith%20negative%20non-robust%20feature%20directions.%20Then%2C%20we%20consider%20the%0Agradient-based%20adversarial%20training%20algorithm%2C%20which%20runs%20gradient%20ascent%20to%0Afind%20adversarial%20examples%20and%20runs%20gradient%20descent%20over%20the%20empirical%20risk%20at%0Aadversarial%20examples%20to%20update%20models.%20We%20show%20that%20the%20adversarial%20training%0Amethod%20can%20provably%20strengthen%20the%20robust%20feature%20learning%20and%20suppress%20the%0Anon-robust%20feature%20learning%20to%20improve%20the%20network%20robustness.%20Finally%2C%20we%20also%0Aempirically%20validate%20our%20theoretical%20findings%20with%20experiments%20on%20real-image%0Adatasets%2C%20including%20MNIST%2C%20CIFAR10%20and%20SVHN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08503v2&entry.124074799=Read"},
{"title": "A Regularized Newton Method for Nonconvex Optimization with Global and\n  Local Complexity Guarantees", "author": "Yuhao Zhou and Jintao Xu and Chenglong Bao and Chao Ding and Jun Zhu", "abstract": "  We consider the problem of finding an $\\epsilon$-stationary point of a\nnonconvex function with a Lipschitz continuous Hessian and propose a quadratic\nregularized Newton method incorporating a new class of regularizers constructed\nfrom the current and previous gradients. The method leverages a recently\ndeveloped linear conjugate gradient approach with a negative curvature monitor\nto solve the regularized Newton equation. Notably, our algorithm is adaptive,\nrequiring no prior knowledge of the Lipschitz constant of the Hessian, and\nachieves a global complexity of $O(\\epsilon^{-\\frac{3}{2}}) + \\tilde O(1)$ in\nterms of the second-order oracle calls, and $\\tilde O(\\epsilon^{-\\frac{7}{4}})$\nfor Hessian-vector products, respectively. Moreover, when the iterates converge\nto a point where the Hessian is positive definite, the method exhibits\nquadratic local convergence. Preliminary numerical results illustrate the\ncompetitiveness of our algorithm.\n", "link": "http://arxiv.org/abs/2502.04799v1", "date": "2025-02-07", "relevancy": 2.0716, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4305}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4188}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Regularized%20Newton%20Method%20for%20Nonconvex%20Optimization%20with%20Global%20and%0A%20%20Local%20Complexity%20Guarantees&body=Title%3A%20A%20Regularized%20Newton%20Method%20for%20Nonconvex%20Optimization%20with%20Global%20and%0A%20%20Local%20Complexity%20Guarantees%0AAuthor%3A%20Yuhao%20Zhou%20and%20Jintao%20Xu%20and%20Chenglong%20Bao%20and%20Chao%20Ding%20and%20Jun%20Zhu%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20finding%20an%20%24%5Cepsilon%24-stationary%20point%20of%20a%0Anonconvex%20function%20with%20a%20Lipschitz%20continuous%20Hessian%20and%20propose%20a%20quadratic%0Aregularized%20Newton%20method%20incorporating%20a%20new%20class%20of%20regularizers%20constructed%0Afrom%20the%20current%20and%20previous%20gradients.%20The%20method%20leverages%20a%20recently%0Adeveloped%20linear%20conjugate%20gradient%20approach%20with%20a%20negative%20curvature%20monitor%0Ato%20solve%20the%20regularized%20Newton%20equation.%20Notably%2C%20our%20algorithm%20is%20adaptive%2C%0Arequiring%20no%20prior%20knowledge%20of%20the%20Lipschitz%20constant%20of%20the%20Hessian%2C%20and%0Aachieves%20a%20global%20complexity%20of%20%24O%28%5Cepsilon%5E%7B-%5Cfrac%7B3%7D%7B2%7D%7D%29%20%2B%20%5Ctilde%20O%281%29%24%20in%0Aterms%20of%20the%20second-order%20oracle%20calls%2C%20and%20%24%5Ctilde%20O%28%5Cepsilon%5E%7B-%5Cfrac%7B7%7D%7B4%7D%7D%29%24%0Afor%20Hessian-vector%20products%2C%20respectively.%20Moreover%2C%20when%20the%20iterates%20converge%0Ato%20a%20point%20where%20the%20Hessian%20is%20positive%20definite%2C%20the%20method%20exhibits%0Aquadratic%20local%20convergence.%20Preliminary%20numerical%20results%20illustrate%20the%0Acompetitiveness%20of%20our%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04799v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Regularized%2520Newton%2520Method%2520for%2520Nonconvex%2520Optimization%2520with%2520Global%2520and%250A%2520%2520Local%2520Complexity%2520Guarantees%26entry.906535625%3DYuhao%2520Zhou%2520and%2520Jintao%2520Xu%2520and%2520Chenglong%2520Bao%2520and%2520Chao%2520Ding%2520and%2520Jun%2520Zhu%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520finding%2520an%2520%2524%255Cepsilon%2524-stationary%2520point%2520of%2520a%250Anonconvex%2520function%2520with%2520a%2520Lipschitz%2520continuous%2520Hessian%2520and%2520propose%2520a%2520quadratic%250Aregularized%2520Newton%2520method%2520incorporating%2520a%2520new%2520class%2520of%2520regularizers%2520constructed%250Afrom%2520the%2520current%2520and%2520previous%2520gradients.%2520The%2520method%2520leverages%2520a%2520recently%250Adeveloped%2520linear%2520conjugate%2520gradient%2520approach%2520with%2520a%2520negative%2520curvature%2520monitor%250Ato%2520solve%2520the%2520regularized%2520Newton%2520equation.%2520Notably%252C%2520our%2520algorithm%2520is%2520adaptive%252C%250Arequiring%2520no%2520prior%2520knowledge%2520of%2520the%2520Lipschitz%2520constant%2520of%2520the%2520Hessian%252C%2520and%250Aachieves%2520a%2520global%2520complexity%2520of%2520%2524O%2528%255Cepsilon%255E%257B-%255Cfrac%257B3%257D%257B2%257D%257D%2529%2520%252B%2520%255Ctilde%2520O%25281%2529%2524%2520in%250Aterms%2520of%2520the%2520second-order%2520oracle%2520calls%252C%2520and%2520%2524%255Ctilde%2520O%2528%255Cepsilon%255E%257B-%255Cfrac%257B7%257D%257B4%257D%257D%2529%2524%250Afor%2520Hessian-vector%2520products%252C%2520respectively.%2520Moreover%252C%2520when%2520the%2520iterates%2520converge%250Ato%2520a%2520point%2520where%2520the%2520Hessian%2520is%2520positive%2520definite%252C%2520the%2520method%2520exhibits%250Aquadratic%2520local%2520convergence.%2520Preliminary%2520numerical%2520results%2520illustrate%2520the%250Acompetitiveness%2520of%2520our%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04799v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Regularized%20Newton%20Method%20for%20Nonconvex%20Optimization%20with%20Global%20and%0A%20%20Local%20Complexity%20Guarantees&entry.906535625=Yuhao%20Zhou%20and%20Jintao%20Xu%20and%20Chenglong%20Bao%20and%20Chao%20Ding%20and%20Jun%20Zhu&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20finding%20an%20%24%5Cepsilon%24-stationary%20point%20of%20a%0Anonconvex%20function%20with%20a%20Lipschitz%20continuous%20Hessian%20and%20propose%20a%20quadratic%0Aregularized%20Newton%20method%20incorporating%20a%20new%20class%20of%20regularizers%20constructed%0Afrom%20the%20current%20and%20previous%20gradients.%20The%20method%20leverages%20a%20recently%0Adeveloped%20linear%20conjugate%20gradient%20approach%20with%20a%20negative%20curvature%20monitor%0Ato%20solve%20the%20regularized%20Newton%20equation.%20Notably%2C%20our%20algorithm%20is%20adaptive%2C%0Arequiring%20no%20prior%20knowledge%20of%20the%20Lipschitz%20constant%20of%20the%20Hessian%2C%20and%0Aachieves%20a%20global%20complexity%20of%20%24O%28%5Cepsilon%5E%7B-%5Cfrac%7B3%7D%7B2%7D%7D%29%20%2B%20%5Ctilde%20O%281%29%24%20in%0Aterms%20of%20the%20second-order%20oracle%20calls%2C%20and%20%24%5Ctilde%20O%28%5Cepsilon%5E%7B-%5Cfrac%7B7%7D%7B4%7D%7D%29%24%0Afor%20Hessian-vector%20products%2C%20respectively.%20Moreover%2C%20when%20the%20iterates%20converge%0Ato%20a%20point%20where%20the%20Hessian%20is%20positive%20definite%2C%20the%20method%20exhibits%0Aquadratic%20local%20convergence.%20Preliminary%20numerical%20results%20illustrate%20the%0Acompetitiveness%20of%20our%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04799v1&entry.124074799=Read"},
{"title": "Brief analysis of DeepSeek R1 and its implications for Generative AI", "author": "Sarah Mercer and Samuel Spillard and Daniel P. Martin", "abstract": "  In late January 2025, DeepSeek released their new reasoning model (DeepSeek\nR1); which was developed at a fraction of the cost yet remains competitive with\nOpenAI's models, despite the US's GPU export ban. This report discusses the\nmodel, and what its release means for the field of Generative AI more widely.\nWe briefly discuss other models released from China in recent weeks, their\nsimilarities; innovative use of Mixture of Experts (MoE), Reinforcement\nLearning (RL) and clever engineering appear to be key factors in the\ncapabilities of these models. This think piece has been written to a tight\ntimescale, providing broad coverage of the topic, and serves as introductory\nmaterial for those looking to understand the model's technical advancements, as\nwell as its place in the ecosystem. Several further areas of research are\nidentified.\n", "link": "http://arxiv.org/abs/2502.02523v3", "date": "2025-02-07", "relevancy": 2.0607, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5224}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5118}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brief%20analysis%20of%20DeepSeek%20R1%20and%20its%20implications%20for%20Generative%20AI&body=Title%3A%20Brief%20analysis%20of%20DeepSeek%20R1%20and%20its%20implications%20for%20Generative%20AI%0AAuthor%3A%20Sarah%20Mercer%20and%20Samuel%20Spillard%20and%20Daniel%20P.%20Martin%0AAbstract%3A%20%20%20In%20late%20January%202025%2C%20DeepSeek%20released%20their%20new%20reasoning%20model%20%28DeepSeek%0AR1%29%3B%20which%20was%20developed%20at%20a%20fraction%20of%20the%20cost%20yet%20remains%20competitive%20with%0AOpenAI%27s%20models%2C%20despite%20the%20US%27s%20GPU%20export%20ban.%20This%20report%20discusses%20the%0Amodel%2C%20and%20what%20its%20release%20means%20for%20the%20field%20of%20Generative%20AI%20more%20widely.%0AWe%20briefly%20discuss%20other%20models%20released%20from%20China%20in%20recent%20weeks%2C%20their%0Asimilarities%3B%20innovative%20use%20of%20Mixture%20of%20Experts%20%28MoE%29%2C%20Reinforcement%0ALearning%20%28RL%29%20and%20clever%20engineering%20appear%20to%20be%20key%20factors%20in%20the%0Acapabilities%20of%20these%20models.%20This%20think%20piece%20has%20been%20written%20to%20a%20tight%0Atimescale%2C%20providing%20broad%20coverage%20of%20the%20topic%2C%20and%20serves%20as%20introductory%0Amaterial%20for%20those%20looking%20to%20understand%20the%20model%27s%20technical%20advancements%2C%20as%0Awell%20as%20its%20place%20in%20the%20ecosystem.%20Several%20further%20areas%20of%20research%20are%0Aidentified.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02523v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrief%2520analysis%2520of%2520DeepSeek%2520R1%2520and%2520its%2520implications%2520for%2520Generative%2520AI%26entry.906535625%3DSarah%2520Mercer%2520and%2520Samuel%2520Spillard%2520and%2520Daniel%2520P.%2520Martin%26entry.1292438233%3D%2520%2520In%2520late%2520January%25202025%252C%2520DeepSeek%2520released%2520their%2520new%2520reasoning%2520model%2520%2528DeepSeek%250AR1%2529%253B%2520which%2520was%2520developed%2520at%2520a%2520fraction%2520of%2520the%2520cost%2520yet%2520remains%2520competitive%2520with%250AOpenAI%2527s%2520models%252C%2520despite%2520the%2520US%2527s%2520GPU%2520export%2520ban.%2520This%2520report%2520discusses%2520the%250Amodel%252C%2520and%2520what%2520its%2520release%2520means%2520for%2520the%2520field%2520of%2520Generative%2520AI%2520more%2520widely.%250AWe%2520briefly%2520discuss%2520other%2520models%2520released%2520from%2520China%2520in%2520recent%2520weeks%252C%2520their%250Asimilarities%253B%2520innovative%2520use%2520of%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%252C%2520Reinforcement%250ALearning%2520%2528RL%2529%2520and%2520clever%2520engineering%2520appear%2520to%2520be%2520key%2520factors%2520in%2520the%250Acapabilities%2520of%2520these%2520models.%2520This%2520think%2520piece%2520has%2520been%2520written%2520to%2520a%2520tight%250Atimescale%252C%2520providing%2520broad%2520coverage%2520of%2520the%2520topic%252C%2520and%2520serves%2520as%2520introductory%250Amaterial%2520for%2520those%2520looking%2520to%2520understand%2520the%2520model%2527s%2520technical%2520advancements%252C%2520as%250Awell%2520as%2520its%2520place%2520in%2520the%2520ecosystem.%2520Several%2520further%2520areas%2520of%2520research%2520are%250Aidentified.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02523v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brief%20analysis%20of%20DeepSeek%20R1%20and%20its%20implications%20for%20Generative%20AI&entry.906535625=Sarah%20Mercer%20and%20Samuel%20Spillard%20and%20Daniel%20P.%20Martin&entry.1292438233=%20%20In%20late%20January%202025%2C%20DeepSeek%20released%20their%20new%20reasoning%20model%20%28DeepSeek%0AR1%29%3B%20which%20was%20developed%20at%20a%20fraction%20of%20the%20cost%20yet%20remains%20competitive%20with%0AOpenAI%27s%20models%2C%20despite%20the%20US%27s%20GPU%20export%20ban.%20This%20report%20discusses%20the%0Amodel%2C%20and%20what%20its%20release%20means%20for%20the%20field%20of%20Generative%20AI%20more%20widely.%0AWe%20briefly%20discuss%20other%20models%20released%20from%20China%20in%20recent%20weeks%2C%20their%0Asimilarities%3B%20innovative%20use%20of%20Mixture%20of%20Experts%20%28MoE%29%2C%20Reinforcement%0ALearning%20%28RL%29%20and%20clever%20engineering%20appear%20to%20be%20key%20factors%20in%20the%0Acapabilities%20of%20these%20models.%20This%20think%20piece%20has%20been%20written%20to%20a%20tight%0Atimescale%2C%20providing%20broad%20coverage%20of%20the%20topic%2C%20and%20serves%20as%20introductory%0Amaterial%20for%20those%20looking%20to%20understand%20the%20model%27s%20technical%20advancements%2C%20as%0Awell%20as%20its%20place%20in%20the%20ecosystem.%20Several%20further%20areas%20of%20research%20are%0Aidentified.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02523v3&entry.124074799=Read"},
{"title": "Aligning Black-box Language Models with Human Judgments", "author": "Gerrit J. J. van den Burg and Gen Suzuki and Wei Liu and Murat Sensoy", "abstract": "  Large language models (LLMs) are increasingly used as automated judges to\nevaluate recommendation systems, search engines, and other subjective tasks,\nwhere relying on human evaluators can be costly, time-consuming, and\nunscalable. LLMs offer an efficient solution for continuous, automated\nevaluation. However, since the systems that are built and improved with these\njudgments are ultimately designed for human use, it is crucial that LLM\njudgments align closely with human evaluators to ensure such systems remain\nhuman-centered. On the other hand, aligning LLM judgments with human evaluators\nis challenging due to individual variability and biases in human judgments. We\npropose a simple yet effective framework to align LLM judgments with individual\nhuman evaluators or their aggregated judgments, without retraining or\nfine-tuning the LLM. Our approach learns a linear mapping between the LLM's\noutputs and human judgments, achieving over 142% average improvement in\nagreement across 29 tasks with only a small number of calibration examples used\nfor training. Notably, our method works in zero-shot and few-shot settings,\nexceeds inter-human agreement on four out of six tasks, and enables smaller\nLLMs to achieve performance comparable to that of larger models.\n", "link": "http://arxiv.org/abs/2502.04997v1", "date": "2025-02-07", "relevancy": 2.0453, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5435}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5186}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligning%20Black-box%20Language%20Models%20with%20Human%20Judgments&body=Title%3A%20Aligning%20Black-box%20Language%20Models%20with%20Human%20Judgments%0AAuthor%3A%20Gerrit%20J.%20J.%20van%20den%20Burg%20and%20Gen%20Suzuki%20and%20Wei%20Liu%20and%20Murat%20Sensoy%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20used%20as%20automated%20judges%20to%0Aevaluate%20recommendation%20systems%2C%20search%20engines%2C%20and%20other%20subjective%20tasks%2C%0Awhere%20relying%20on%20human%20evaluators%20can%20be%20costly%2C%20time-consuming%2C%20and%0Aunscalable.%20LLMs%20offer%20an%20efficient%20solution%20for%20continuous%2C%20automated%0Aevaluation.%20However%2C%20since%20the%20systems%20that%20are%20built%20and%20improved%20with%20these%0Ajudgments%20are%20ultimately%20designed%20for%20human%20use%2C%20it%20is%20crucial%20that%20LLM%0Ajudgments%20align%20closely%20with%20human%20evaluators%20to%20ensure%20such%20systems%20remain%0Ahuman-centered.%20On%20the%20other%20hand%2C%20aligning%20LLM%20judgments%20with%20human%20evaluators%0Ais%20challenging%20due%20to%20individual%20variability%20and%20biases%20in%20human%20judgments.%20We%0Apropose%20a%20simple%20yet%20effective%20framework%20to%20align%20LLM%20judgments%20with%20individual%0Ahuman%20evaluators%20or%20their%20aggregated%20judgments%2C%20without%20retraining%20or%0Afine-tuning%20the%20LLM.%20Our%20approach%20learns%20a%20linear%20mapping%20between%20the%20LLM%27s%0Aoutputs%20and%20human%20judgments%2C%20achieving%20over%20142%25%20average%20improvement%20in%0Aagreement%20across%2029%20tasks%20with%20only%20a%20small%20number%20of%20calibration%20examples%20used%0Afor%20training.%20Notably%2C%20our%20method%20works%20in%20zero-shot%20and%20few-shot%20settings%2C%0Aexceeds%20inter-human%20agreement%20on%20four%20out%20of%20six%20tasks%2C%20and%20enables%20smaller%0ALLMs%20to%20achieve%20performance%20comparable%20to%20that%20of%20larger%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligning%2520Black-box%2520Language%2520Models%2520with%2520Human%2520Judgments%26entry.906535625%3DGerrit%2520J.%2520J.%2520van%2520den%2520Burg%2520and%2520Gen%2520Suzuki%2520and%2520Wei%2520Liu%2520and%2520Murat%2520Sensoy%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520used%2520as%2520automated%2520judges%2520to%250Aevaluate%2520recommendation%2520systems%252C%2520search%2520engines%252C%2520and%2520other%2520subjective%2520tasks%252C%250Awhere%2520relying%2520on%2520human%2520evaluators%2520can%2520be%2520costly%252C%2520time-consuming%252C%2520and%250Aunscalable.%2520LLMs%2520offer%2520an%2520efficient%2520solution%2520for%2520continuous%252C%2520automated%250Aevaluation.%2520However%252C%2520since%2520the%2520systems%2520that%2520are%2520built%2520and%2520improved%2520with%2520these%250Ajudgments%2520are%2520ultimately%2520designed%2520for%2520human%2520use%252C%2520it%2520is%2520crucial%2520that%2520LLM%250Ajudgments%2520align%2520closely%2520with%2520human%2520evaluators%2520to%2520ensure%2520such%2520systems%2520remain%250Ahuman-centered.%2520On%2520the%2520other%2520hand%252C%2520aligning%2520LLM%2520judgments%2520with%2520human%2520evaluators%250Ais%2520challenging%2520due%2520to%2520individual%2520variability%2520and%2520biases%2520in%2520human%2520judgments.%2520We%250Apropose%2520a%2520simple%2520yet%2520effective%2520framework%2520to%2520align%2520LLM%2520judgments%2520with%2520individual%250Ahuman%2520evaluators%2520or%2520their%2520aggregated%2520judgments%252C%2520without%2520retraining%2520or%250Afine-tuning%2520the%2520LLM.%2520Our%2520approach%2520learns%2520a%2520linear%2520mapping%2520between%2520the%2520LLM%2527s%250Aoutputs%2520and%2520human%2520judgments%252C%2520achieving%2520over%2520142%2525%2520average%2520improvement%2520in%250Aagreement%2520across%252029%2520tasks%2520with%2520only%2520a%2520small%2520number%2520of%2520calibration%2520examples%2520used%250Afor%2520training.%2520Notably%252C%2520our%2520method%2520works%2520in%2520zero-shot%2520and%2520few-shot%2520settings%252C%250Aexceeds%2520inter-human%2520agreement%2520on%2520four%2520out%2520of%2520six%2520tasks%252C%2520and%2520enables%2520smaller%250ALLMs%2520to%2520achieve%2520performance%2520comparable%2520to%2520that%2520of%2520larger%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligning%20Black-box%20Language%20Models%20with%20Human%20Judgments&entry.906535625=Gerrit%20J.%20J.%20van%20den%20Burg%20and%20Gen%20Suzuki%20and%20Wei%20Liu%20and%20Murat%20Sensoy&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20used%20as%20automated%20judges%20to%0Aevaluate%20recommendation%20systems%2C%20search%20engines%2C%20and%20other%20subjective%20tasks%2C%0Awhere%20relying%20on%20human%20evaluators%20can%20be%20costly%2C%20time-consuming%2C%20and%0Aunscalable.%20LLMs%20offer%20an%20efficient%20solution%20for%20continuous%2C%20automated%0Aevaluation.%20However%2C%20since%20the%20systems%20that%20are%20built%20and%20improved%20with%20these%0Ajudgments%20are%20ultimately%20designed%20for%20human%20use%2C%20it%20is%20crucial%20that%20LLM%0Ajudgments%20align%20closely%20with%20human%20evaluators%20to%20ensure%20such%20systems%20remain%0Ahuman-centered.%20On%20the%20other%20hand%2C%20aligning%20LLM%20judgments%20with%20human%20evaluators%0Ais%20challenging%20due%20to%20individual%20variability%20and%20biases%20in%20human%20judgments.%20We%0Apropose%20a%20simple%20yet%20effective%20framework%20to%20align%20LLM%20judgments%20with%20individual%0Ahuman%20evaluators%20or%20their%20aggregated%20judgments%2C%20without%20retraining%20or%0Afine-tuning%20the%20LLM.%20Our%20approach%20learns%20a%20linear%20mapping%20between%20the%20LLM%27s%0Aoutputs%20and%20human%20judgments%2C%20achieving%20over%20142%25%20average%20improvement%20in%0Aagreement%20across%2029%20tasks%20with%20only%20a%20small%20number%20of%20calibration%20examples%20used%0Afor%20training.%20Notably%2C%20our%20method%20works%20in%20zero-shot%20and%20few-shot%20settings%2C%0Aexceeds%20inter-human%20agreement%20on%20four%20out%20of%20six%20tasks%2C%20and%20enables%20smaller%0ALLMs%20to%20achieve%20performance%20comparable%20to%20that%20of%20larger%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04997v1&entry.124074799=Read"},
{"title": "Training-free Neural Architecture Search through Variance of Knowledge\n  of Deep Network Weights", "author": "Ond\u0159ej T\u00fdbl and Luk\u00e1\u0161 Neumann", "abstract": "  Deep learning has revolutionized computer vision, but it achieved its\ntremendous success using deep network architectures which are mostly\nhand-crafted and therefore likely suboptimal. Neural Architecture Search (NAS)\naims to bridge this gap by following a well-defined optimization paradigm which\nsystematically looks for the best architecture, given objective criterion such\nas maximal classification accuracy. The main limitation of NAS is however its\nastronomical computational cost, as it typically requires training each\ncandidate network architecture from scratch.\n  In this paper, we aim to alleviate this limitation by proposing a novel\ntraining-free proxy for image classification accuracy based on Fisher\nInformation. The proposed proxy has a strong theoretical background in\nstatistics and it allows estimating expected image classification accuracy of a\ngiven deep network without training the network, thus significantly reducing\ncomputational cost of standard NAS algorithms.\n  Our training-free proxy achieves state-of-the-art results on three public\ndatasets and in two search spaces, both when evaluated using previously\nproposed metrics, as well as using a new metric that we propose which we\ndemonstrate is more informative for practical NAS applications. The source code\nis publicly available at http://www.github.com/ondratybl/VKDNW\n", "link": "http://arxiv.org/abs/2502.04975v1", "date": "2025-02-07", "relevancy": 2.0416, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5157}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5088}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-free%20Neural%20Architecture%20Search%20through%20Variance%20of%20Knowledge%0A%20%20of%20Deep%20Network%20Weights&body=Title%3A%20Training-free%20Neural%20Architecture%20Search%20through%20Variance%20of%20Knowledge%0A%20%20of%20Deep%20Network%20Weights%0AAuthor%3A%20Ond%C5%99ej%20T%C3%BDbl%20and%20Luk%C3%A1%C5%A1%20Neumann%0AAbstract%3A%20%20%20Deep%20learning%20has%20revolutionized%20computer%20vision%2C%20but%20it%20achieved%20its%0Atremendous%20success%20using%20deep%20network%20architectures%20which%20are%20mostly%0Ahand-crafted%20and%20therefore%20likely%20suboptimal.%20Neural%20Architecture%20Search%20%28NAS%29%0Aaims%20to%20bridge%20this%20gap%20by%20following%20a%20well-defined%20optimization%20paradigm%20which%0Asystematically%20looks%20for%20the%20best%20architecture%2C%20given%20objective%20criterion%20such%0Aas%20maximal%20classification%20accuracy.%20The%20main%20limitation%20of%20NAS%20is%20however%20its%0Aastronomical%20computational%20cost%2C%20as%20it%20typically%20requires%20training%20each%0Acandidate%20network%20architecture%20from%20scratch.%0A%20%20In%20this%20paper%2C%20we%20aim%20to%20alleviate%20this%20limitation%20by%20proposing%20a%20novel%0Atraining-free%20proxy%20for%20image%20classification%20accuracy%20based%20on%20Fisher%0AInformation.%20The%20proposed%20proxy%20has%20a%20strong%20theoretical%20background%20in%0Astatistics%20and%20it%20allows%20estimating%20expected%20image%20classification%20accuracy%20of%20a%0Agiven%20deep%20network%20without%20training%20the%20network%2C%20thus%20significantly%20reducing%0Acomputational%20cost%20of%20standard%20NAS%20algorithms.%0A%20%20Our%20training-free%20proxy%20achieves%20state-of-the-art%20results%20on%20three%20public%0Adatasets%20and%20in%20two%20search%20spaces%2C%20both%20when%20evaluated%20using%20previously%0Aproposed%20metrics%2C%20as%20well%20as%20using%20a%20new%20metric%20that%20we%20propose%20which%20we%0Ademonstrate%20is%20more%20informative%20for%20practical%20NAS%20applications.%20The%20source%20code%0Ais%20publicly%20available%20at%20http%3A//www.github.com/ondratybl/VKDNW%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04975v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-free%2520Neural%2520Architecture%2520Search%2520through%2520Variance%2520of%2520Knowledge%250A%2520%2520of%2520Deep%2520Network%2520Weights%26entry.906535625%3DOnd%25C5%2599ej%2520T%25C3%25BDbl%2520and%2520Luk%25C3%25A1%25C5%25A1%2520Neumann%26entry.1292438233%3D%2520%2520Deep%2520learning%2520has%2520revolutionized%2520computer%2520vision%252C%2520but%2520it%2520achieved%2520its%250Atremendous%2520success%2520using%2520deep%2520network%2520architectures%2520which%2520are%2520mostly%250Ahand-crafted%2520and%2520therefore%2520likely%2520suboptimal.%2520Neural%2520Architecture%2520Search%2520%2528NAS%2529%250Aaims%2520to%2520bridge%2520this%2520gap%2520by%2520following%2520a%2520well-defined%2520optimization%2520paradigm%2520which%250Asystematically%2520looks%2520for%2520the%2520best%2520architecture%252C%2520given%2520objective%2520criterion%2520such%250Aas%2520maximal%2520classification%2520accuracy.%2520The%2520main%2520limitation%2520of%2520NAS%2520is%2520however%2520its%250Aastronomical%2520computational%2520cost%252C%2520as%2520it%2520typically%2520requires%2520training%2520each%250Acandidate%2520network%2520architecture%2520from%2520scratch.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520alleviate%2520this%2520limitation%2520by%2520proposing%2520a%2520novel%250Atraining-free%2520proxy%2520for%2520image%2520classification%2520accuracy%2520based%2520on%2520Fisher%250AInformation.%2520The%2520proposed%2520proxy%2520has%2520a%2520strong%2520theoretical%2520background%2520in%250Astatistics%2520and%2520it%2520allows%2520estimating%2520expected%2520image%2520classification%2520accuracy%2520of%2520a%250Agiven%2520deep%2520network%2520without%2520training%2520the%2520network%252C%2520thus%2520significantly%2520reducing%250Acomputational%2520cost%2520of%2520standard%2520NAS%2520algorithms.%250A%2520%2520Our%2520training-free%2520proxy%2520achieves%2520state-of-the-art%2520results%2520on%2520three%2520public%250Adatasets%2520and%2520in%2520two%2520search%2520spaces%252C%2520both%2520when%2520evaluated%2520using%2520previously%250Aproposed%2520metrics%252C%2520as%2520well%2520as%2520using%2520a%2520new%2520metric%2520that%2520we%2520propose%2520which%2520we%250Ademonstrate%2520is%2520more%2520informative%2520for%2520practical%2520NAS%2520applications.%2520The%2520source%2520code%250Ais%2520publicly%2520available%2520at%2520http%253A//www.github.com/ondratybl/VKDNW%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04975v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-free%20Neural%20Architecture%20Search%20through%20Variance%20of%20Knowledge%0A%20%20of%20Deep%20Network%20Weights&entry.906535625=Ond%C5%99ej%20T%C3%BDbl%20and%20Luk%C3%A1%C5%A1%20Neumann&entry.1292438233=%20%20Deep%20learning%20has%20revolutionized%20computer%20vision%2C%20but%20it%20achieved%20its%0Atremendous%20success%20using%20deep%20network%20architectures%20which%20are%20mostly%0Ahand-crafted%20and%20therefore%20likely%20suboptimal.%20Neural%20Architecture%20Search%20%28NAS%29%0Aaims%20to%20bridge%20this%20gap%20by%20following%20a%20well-defined%20optimization%20paradigm%20which%0Asystematically%20looks%20for%20the%20best%20architecture%2C%20given%20objective%20criterion%20such%0Aas%20maximal%20classification%20accuracy.%20The%20main%20limitation%20of%20NAS%20is%20however%20its%0Aastronomical%20computational%20cost%2C%20as%20it%20typically%20requires%20training%20each%0Acandidate%20network%20architecture%20from%20scratch.%0A%20%20In%20this%20paper%2C%20we%20aim%20to%20alleviate%20this%20limitation%20by%20proposing%20a%20novel%0Atraining-free%20proxy%20for%20image%20classification%20accuracy%20based%20on%20Fisher%0AInformation.%20The%20proposed%20proxy%20has%20a%20strong%20theoretical%20background%20in%0Astatistics%20and%20it%20allows%20estimating%20expected%20image%20classification%20accuracy%20of%20a%0Agiven%20deep%20network%20without%20training%20the%20network%2C%20thus%20significantly%20reducing%0Acomputational%20cost%20of%20standard%20NAS%20algorithms.%0A%20%20Our%20training-free%20proxy%20achieves%20state-of-the-art%20results%20on%20three%20public%0Adatasets%20and%20in%20two%20search%20spaces%2C%20both%20when%20evaluated%20using%20previously%0Aproposed%20metrics%2C%20as%20well%20as%20using%20a%20new%20metric%20that%20we%20propose%20which%20we%0Ademonstrate%20is%20more%20informative%20for%20practical%20NAS%20applications.%20The%20source%20code%0Ais%20publicly%20available%20at%20http%3A//www.github.com/ondratybl/VKDNW%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04975v1&entry.124074799=Read"},
{"title": "Relative Age Estimation Using Face Images", "author": "Ran Sandhaus and Yosi Keller", "abstract": "  This work introduces a novel deep-learning approach for estimating age from a\nsingle facial image by refining an initial age estimate. The refinement\nleverages a reference face database of individuals with similar ages and\nappearances. We employ a network that estimates age differences between an\ninput image and reference images with known ages, thus refining the initial\nestimate. Our method explicitly models age-dependent facial variations using\ndifferential regression, yielding improved accuracy compared to conventional\nabsolute age estimation. Additionally, we introduce an age augmentation scheme\nthat iteratively refines initial age estimates by modeling their error\ndistribution during training. This iterative approach further enhances the\ninitial estimates. Our approach surpasses existing methods, achieving\nstate-of-the-art accuracy on the MORPH II and CACD datasets. Furthermore, we\nexamine the biases inherent in contemporary state-of-the-art age estimation\ntechniques.\n", "link": "http://arxiv.org/abs/2502.04852v1", "date": "2025-02-07", "relevancy": 2.0416, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5505}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5156}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relative%20Age%20Estimation%20Using%20Face%20Images&body=Title%3A%20Relative%20Age%20Estimation%20Using%20Face%20Images%0AAuthor%3A%20Ran%20Sandhaus%20and%20Yosi%20Keller%0AAbstract%3A%20%20%20This%20work%20introduces%20a%20novel%20deep-learning%20approach%20for%20estimating%20age%20from%20a%0Asingle%20facial%20image%20by%20refining%20an%20initial%20age%20estimate.%20The%20refinement%0Aleverages%20a%20reference%20face%20database%20of%20individuals%20with%20similar%20ages%20and%0Aappearances.%20We%20employ%20a%20network%20that%20estimates%20age%20differences%20between%20an%0Ainput%20image%20and%20reference%20images%20with%20known%20ages%2C%20thus%20refining%20the%20initial%0Aestimate.%20Our%20method%20explicitly%20models%20age-dependent%20facial%20variations%20using%0Adifferential%20regression%2C%20yielding%20improved%20accuracy%20compared%20to%20conventional%0Aabsolute%20age%20estimation.%20Additionally%2C%20we%20introduce%20an%20age%20augmentation%20scheme%0Athat%20iteratively%20refines%20initial%20age%20estimates%20by%20modeling%20their%20error%0Adistribution%20during%20training.%20This%20iterative%20approach%20further%20enhances%20the%0Ainitial%20estimates.%20Our%20approach%20surpasses%20existing%20methods%2C%20achieving%0Astate-of-the-art%20accuracy%20on%20the%20MORPH%20II%20and%20CACD%20datasets.%20Furthermore%2C%20we%0Aexamine%20the%20biases%20inherent%20in%20contemporary%20state-of-the-art%20age%20estimation%0Atechniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04852v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelative%2520Age%2520Estimation%2520Using%2520Face%2520Images%26entry.906535625%3DRan%2520Sandhaus%2520and%2520Yosi%2520Keller%26entry.1292438233%3D%2520%2520This%2520work%2520introduces%2520a%2520novel%2520deep-learning%2520approach%2520for%2520estimating%2520age%2520from%2520a%250Asingle%2520facial%2520image%2520by%2520refining%2520an%2520initial%2520age%2520estimate.%2520The%2520refinement%250Aleverages%2520a%2520reference%2520face%2520database%2520of%2520individuals%2520with%2520similar%2520ages%2520and%250Aappearances.%2520We%2520employ%2520a%2520network%2520that%2520estimates%2520age%2520differences%2520between%2520an%250Ainput%2520image%2520and%2520reference%2520images%2520with%2520known%2520ages%252C%2520thus%2520refining%2520the%2520initial%250Aestimate.%2520Our%2520method%2520explicitly%2520models%2520age-dependent%2520facial%2520variations%2520using%250Adifferential%2520regression%252C%2520yielding%2520improved%2520accuracy%2520compared%2520to%2520conventional%250Aabsolute%2520age%2520estimation.%2520Additionally%252C%2520we%2520introduce%2520an%2520age%2520augmentation%2520scheme%250Athat%2520iteratively%2520refines%2520initial%2520age%2520estimates%2520by%2520modeling%2520their%2520error%250Adistribution%2520during%2520training.%2520This%2520iterative%2520approach%2520further%2520enhances%2520the%250Ainitial%2520estimates.%2520Our%2520approach%2520surpasses%2520existing%2520methods%252C%2520achieving%250Astate-of-the-art%2520accuracy%2520on%2520the%2520MORPH%2520II%2520and%2520CACD%2520datasets.%2520Furthermore%252C%2520we%250Aexamine%2520the%2520biases%2520inherent%2520in%2520contemporary%2520state-of-the-art%2520age%2520estimation%250Atechniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04852v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relative%20Age%20Estimation%20Using%20Face%20Images&entry.906535625=Ran%20Sandhaus%20and%20Yosi%20Keller&entry.1292438233=%20%20This%20work%20introduces%20a%20novel%20deep-learning%20approach%20for%20estimating%20age%20from%20a%0Asingle%20facial%20image%20by%20refining%20an%20initial%20age%20estimate.%20The%20refinement%0Aleverages%20a%20reference%20face%20database%20of%20individuals%20with%20similar%20ages%20and%0Aappearances.%20We%20employ%20a%20network%20that%20estimates%20age%20differences%20between%20an%0Ainput%20image%20and%20reference%20images%20with%20known%20ages%2C%20thus%20refining%20the%20initial%0Aestimate.%20Our%20method%20explicitly%20models%20age-dependent%20facial%20variations%20using%0Adifferential%20regression%2C%20yielding%20improved%20accuracy%20compared%20to%20conventional%0Aabsolute%20age%20estimation.%20Additionally%2C%20we%20introduce%20an%20age%20augmentation%20scheme%0Athat%20iteratively%20refines%20initial%20age%20estimates%20by%20modeling%20their%20error%0Adistribution%20during%20training.%20This%20iterative%20approach%20further%20enhances%20the%0Ainitial%20estimates.%20Our%20approach%20surpasses%20existing%20methods%2C%20achieving%0Astate-of-the-art%20accuracy%20on%20the%20MORPH%20II%20and%20CACD%20datasets.%20Furthermore%2C%20we%0Aexamine%20the%20biases%20inherent%20in%20contemporary%20state-of-the-art%20age%20estimation%0Atechniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04852v1&entry.124074799=Read"},
{"title": "Flopping for FLOPs: Leveraging equivariance for computational efficiency", "author": "Georg B\u00f6kman and David Nordstr\u00f6m and Fredrik Kahl", "abstract": "  Incorporating geometric invariance into neural networks enhances parameter\nefficiency but typically increases computational costs. This paper introduces\nnew equivariant neural networks that preserve symmetry while maintaining a\ncomparable number of floating-point operations (FLOPs) per parameter to\nstandard non-equivariant networks. We focus on horizontal mirroring (flopping)\ninvariance, common in many computer vision tasks. The main idea is to\nparametrize the feature spaces in terms of mirror-symmetric and\nmirror-antisymmetric features, i.e., irreps of the flopping group. This\ndecomposes the linear layers to be block-diagonal, requiring half the number of\nFLOPs. Our approach reduces both FLOPs and wall-clock time, providing a\npractical solution for efficient, scalable symmetry-aware architectures.\n", "link": "http://arxiv.org/abs/2502.05169v1", "date": "2025-02-07", "relevancy": 2.0385, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.541}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5037}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flopping%20for%20FLOPs%3A%20Leveraging%20equivariance%20for%20computational%20efficiency&body=Title%3A%20Flopping%20for%20FLOPs%3A%20Leveraging%20equivariance%20for%20computational%20efficiency%0AAuthor%3A%20Georg%20B%C3%B6kman%20and%20David%20Nordstr%C3%B6m%20and%20Fredrik%20Kahl%0AAbstract%3A%20%20%20Incorporating%20geometric%20invariance%20into%20neural%20networks%20enhances%20parameter%0Aefficiency%20but%20typically%20increases%20computational%20costs.%20This%20paper%20introduces%0Anew%20equivariant%20neural%20networks%20that%20preserve%20symmetry%20while%20maintaining%20a%0Acomparable%20number%20of%20floating-point%20operations%20%28FLOPs%29%20per%20parameter%20to%0Astandard%20non-equivariant%20networks.%20We%20focus%20on%20horizontal%20mirroring%20%28flopping%29%0Ainvariance%2C%20common%20in%20many%20computer%20vision%20tasks.%20The%20main%20idea%20is%20to%0Aparametrize%20the%20feature%20spaces%20in%20terms%20of%20mirror-symmetric%20and%0Amirror-antisymmetric%20features%2C%20i.e.%2C%20irreps%20of%20the%20flopping%20group.%20This%0Adecomposes%20the%20linear%20layers%20to%20be%20block-diagonal%2C%20requiring%20half%20the%20number%20of%0AFLOPs.%20Our%20approach%20reduces%20both%20FLOPs%20and%20wall-clock%20time%2C%20providing%20a%0Apractical%20solution%20for%20efficient%2C%20scalable%20symmetry-aware%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05169v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlopping%2520for%2520FLOPs%253A%2520Leveraging%2520equivariance%2520for%2520computational%2520efficiency%26entry.906535625%3DGeorg%2520B%25C3%25B6kman%2520and%2520David%2520Nordstr%25C3%25B6m%2520and%2520Fredrik%2520Kahl%26entry.1292438233%3D%2520%2520Incorporating%2520geometric%2520invariance%2520into%2520neural%2520networks%2520enhances%2520parameter%250Aefficiency%2520but%2520typically%2520increases%2520computational%2520costs.%2520This%2520paper%2520introduces%250Anew%2520equivariant%2520neural%2520networks%2520that%2520preserve%2520symmetry%2520while%2520maintaining%2520a%250Acomparable%2520number%2520of%2520floating-point%2520operations%2520%2528FLOPs%2529%2520per%2520parameter%2520to%250Astandard%2520non-equivariant%2520networks.%2520We%2520focus%2520on%2520horizontal%2520mirroring%2520%2528flopping%2529%250Ainvariance%252C%2520common%2520in%2520many%2520computer%2520vision%2520tasks.%2520The%2520main%2520idea%2520is%2520to%250Aparametrize%2520the%2520feature%2520spaces%2520in%2520terms%2520of%2520mirror-symmetric%2520and%250Amirror-antisymmetric%2520features%252C%2520i.e.%252C%2520irreps%2520of%2520the%2520flopping%2520group.%2520This%250Adecomposes%2520the%2520linear%2520layers%2520to%2520be%2520block-diagonal%252C%2520requiring%2520half%2520the%2520number%2520of%250AFLOPs.%2520Our%2520approach%2520reduces%2520both%2520FLOPs%2520and%2520wall-clock%2520time%252C%2520providing%2520a%250Apractical%2520solution%2520for%2520efficient%252C%2520scalable%2520symmetry-aware%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05169v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flopping%20for%20FLOPs%3A%20Leveraging%20equivariance%20for%20computational%20efficiency&entry.906535625=Georg%20B%C3%B6kman%20and%20David%20Nordstr%C3%B6m%20and%20Fredrik%20Kahl&entry.1292438233=%20%20Incorporating%20geometric%20invariance%20into%20neural%20networks%20enhances%20parameter%0Aefficiency%20but%20typically%20increases%20computational%20costs.%20This%20paper%20introduces%0Anew%20equivariant%20neural%20networks%20that%20preserve%20symmetry%20while%20maintaining%20a%0Acomparable%20number%20of%20floating-point%20operations%20%28FLOPs%29%20per%20parameter%20to%0Astandard%20non-equivariant%20networks.%20We%20focus%20on%20horizontal%20mirroring%20%28flopping%29%0Ainvariance%2C%20common%20in%20many%20computer%20vision%20tasks.%20The%20main%20idea%20is%20to%0Aparametrize%20the%20feature%20spaces%20in%20terms%20of%20mirror-symmetric%20and%0Amirror-antisymmetric%20features%2C%20i.e.%2C%20irreps%20of%20the%20flopping%20group.%20This%0Adecomposes%20the%20linear%20layers%20to%20be%20block-diagonal%2C%20requiring%20half%20the%20number%20of%0AFLOPs.%20Our%20approach%20reduces%20both%20FLOPs%20and%20wall-clock%20time%2C%20providing%20a%0Apractical%20solution%20for%20efficient%2C%20scalable%20symmetry-aware%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05169v1&entry.124074799=Read"},
{"title": "SurGen: 1020 H&E-stained Whole Slide Images With Survival and Genetic\n  Markers", "author": "Craig Myles and In Hwa Um and Craig Marshall and David Harris-Birtill and David J. Harrison", "abstract": "  $\\textbf{Background}$: Cancer remains one of the leading causes of morbidity\nand mortality worldwide. Comprehensive datasets that combine histopathological\nimages with genetic and survival data across various tumour sites are essential\nfor advancing computational pathology and personalised medicine.\n$\\textbf{Results}$: We present SurGen, a dataset comprising 1,020 H&E-stained\nwhole slide images (WSIs) from 843 colorectal cancer cases. The dataset\nincludes detailed annotations for key genetic mutations (KRAS, NRAS, BRAF) and\nmismatch repair status, as well as survival data for 426 cases. To demonstrate\nSurGen's practical utility, we conducted a proof-of-concept machine learning\nexperiment predicting mismatch repair status from the WSIs, achieving a test\nAUROC of 0.8316. These preliminary results underscore the dataset's potential\nto facilitate research in biomarker discovery, prognostic modelling, and\nadvanced machine learning applications in colorectal cancer.\n$\\textbf{Conclusions}$: SurGen offers a valuable resource for the scientific\ncommunity, enabling studies that require high-quality WSIs linked with\ncomprehensive clinical and genetic information on colorectal cancer. Our\ninitial findings affirm the dataset's capacity to advance diagnostic precision\nand foster the development of personalised treatment strategies in colorectal\noncology. Data available online at https://doi.org/10.6019/S-BIAD1285.\n", "link": "http://arxiv.org/abs/2502.04946v1", "date": "2025-02-07", "relevancy": 2.0331, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4261}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.3972}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.3966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SurGen%3A%201020%20H%26E-stained%20Whole%20Slide%20Images%20With%20Survival%20and%20Genetic%0A%20%20Markers&body=Title%3A%20SurGen%3A%201020%20H%26E-stained%20Whole%20Slide%20Images%20With%20Survival%20and%20Genetic%0A%20%20Markers%0AAuthor%3A%20Craig%20Myles%20and%20In%20Hwa%20Um%20and%20Craig%20Marshall%20and%20David%20Harris-Birtill%20and%20David%20J.%20Harrison%0AAbstract%3A%20%20%20%24%5Ctextbf%7BBackground%7D%24%3A%20Cancer%20remains%20one%20of%20the%20leading%20causes%20of%20morbidity%0Aand%20mortality%20worldwide.%20Comprehensive%20datasets%20that%20combine%20histopathological%0Aimages%20with%20genetic%20and%20survival%20data%20across%20various%20tumour%20sites%20are%20essential%0Afor%20advancing%20computational%20pathology%20and%20personalised%20medicine.%0A%24%5Ctextbf%7BResults%7D%24%3A%20We%20present%20SurGen%2C%20a%20dataset%20comprising%201%2C020%20H%26E-stained%0Awhole%20slide%20images%20%28WSIs%29%20from%20843%20colorectal%20cancer%20cases.%20The%20dataset%0Aincludes%20detailed%20annotations%20for%20key%20genetic%20mutations%20%28KRAS%2C%20NRAS%2C%20BRAF%29%20and%0Amismatch%20repair%20status%2C%20as%20well%20as%20survival%20data%20for%20426%20cases.%20To%20demonstrate%0ASurGen%27s%20practical%20utility%2C%20we%20conducted%20a%20proof-of-concept%20machine%20learning%0Aexperiment%20predicting%20mismatch%20repair%20status%20from%20the%20WSIs%2C%20achieving%20a%20test%0AAUROC%20of%200.8316.%20These%20preliminary%20results%20underscore%20the%20dataset%27s%20potential%0Ato%20facilitate%20research%20in%20biomarker%20discovery%2C%20prognostic%20modelling%2C%20and%0Aadvanced%20machine%20learning%20applications%20in%20colorectal%20cancer.%0A%24%5Ctextbf%7BConclusions%7D%24%3A%20SurGen%20offers%20a%20valuable%20resource%20for%20the%20scientific%0Acommunity%2C%20enabling%20studies%20that%20require%20high-quality%20WSIs%20linked%20with%0Acomprehensive%20clinical%20and%20genetic%20information%20on%20colorectal%20cancer.%20Our%0Ainitial%20findings%20affirm%20the%20dataset%27s%20capacity%20to%20advance%20diagnostic%20precision%0Aand%20foster%20the%20development%20of%20personalised%20treatment%20strategies%20in%20colorectal%0Aoncology.%20Data%20available%20online%20at%20https%3A//doi.org/10.6019/S-BIAD1285.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04946v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurGen%253A%25201020%2520H%2526E-stained%2520Whole%2520Slide%2520Images%2520With%2520Survival%2520and%2520Genetic%250A%2520%2520Markers%26entry.906535625%3DCraig%2520Myles%2520and%2520In%2520Hwa%2520Um%2520and%2520Craig%2520Marshall%2520and%2520David%2520Harris-Birtill%2520and%2520David%2520J.%2520Harrison%26entry.1292438233%3D%2520%2520%2524%255Ctextbf%257BBackground%257D%2524%253A%2520Cancer%2520remains%2520one%2520of%2520the%2520leading%2520causes%2520of%2520morbidity%250Aand%2520mortality%2520worldwide.%2520Comprehensive%2520datasets%2520that%2520combine%2520histopathological%250Aimages%2520with%2520genetic%2520and%2520survival%2520data%2520across%2520various%2520tumour%2520sites%2520are%2520essential%250Afor%2520advancing%2520computational%2520pathology%2520and%2520personalised%2520medicine.%250A%2524%255Ctextbf%257BResults%257D%2524%253A%2520We%2520present%2520SurGen%252C%2520a%2520dataset%2520comprising%25201%252C020%2520H%2526E-stained%250Awhole%2520slide%2520images%2520%2528WSIs%2529%2520from%2520843%2520colorectal%2520cancer%2520cases.%2520The%2520dataset%250Aincludes%2520detailed%2520annotations%2520for%2520key%2520genetic%2520mutations%2520%2528KRAS%252C%2520NRAS%252C%2520BRAF%2529%2520and%250Amismatch%2520repair%2520status%252C%2520as%2520well%2520as%2520survival%2520data%2520for%2520426%2520cases.%2520To%2520demonstrate%250ASurGen%2527s%2520practical%2520utility%252C%2520we%2520conducted%2520a%2520proof-of-concept%2520machine%2520learning%250Aexperiment%2520predicting%2520mismatch%2520repair%2520status%2520from%2520the%2520WSIs%252C%2520achieving%2520a%2520test%250AAUROC%2520of%25200.8316.%2520These%2520preliminary%2520results%2520underscore%2520the%2520dataset%2527s%2520potential%250Ato%2520facilitate%2520research%2520in%2520biomarker%2520discovery%252C%2520prognostic%2520modelling%252C%2520and%250Aadvanced%2520machine%2520learning%2520applications%2520in%2520colorectal%2520cancer.%250A%2524%255Ctextbf%257BConclusions%257D%2524%253A%2520SurGen%2520offers%2520a%2520valuable%2520resource%2520for%2520the%2520scientific%250Acommunity%252C%2520enabling%2520studies%2520that%2520require%2520high-quality%2520WSIs%2520linked%2520with%250Acomprehensive%2520clinical%2520and%2520genetic%2520information%2520on%2520colorectal%2520cancer.%2520Our%250Ainitial%2520findings%2520affirm%2520the%2520dataset%2527s%2520capacity%2520to%2520advance%2520diagnostic%2520precision%250Aand%2520foster%2520the%2520development%2520of%2520personalised%2520treatment%2520strategies%2520in%2520colorectal%250Aoncology.%2520Data%2520available%2520online%2520at%2520https%253A//doi.org/10.6019/S-BIAD1285.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04946v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SurGen%3A%201020%20H%26E-stained%20Whole%20Slide%20Images%20With%20Survival%20and%20Genetic%0A%20%20Markers&entry.906535625=Craig%20Myles%20and%20In%20Hwa%20Um%20and%20Craig%20Marshall%20and%20David%20Harris-Birtill%20and%20David%20J.%20Harrison&entry.1292438233=%20%20%24%5Ctextbf%7BBackground%7D%24%3A%20Cancer%20remains%20one%20of%20the%20leading%20causes%20of%20morbidity%0Aand%20mortality%20worldwide.%20Comprehensive%20datasets%20that%20combine%20histopathological%0Aimages%20with%20genetic%20and%20survival%20data%20across%20various%20tumour%20sites%20are%20essential%0Afor%20advancing%20computational%20pathology%20and%20personalised%20medicine.%0A%24%5Ctextbf%7BResults%7D%24%3A%20We%20present%20SurGen%2C%20a%20dataset%20comprising%201%2C020%20H%26E-stained%0Awhole%20slide%20images%20%28WSIs%29%20from%20843%20colorectal%20cancer%20cases.%20The%20dataset%0Aincludes%20detailed%20annotations%20for%20key%20genetic%20mutations%20%28KRAS%2C%20NRAS%2C%20BRAF%29%20and%0Amismatch%20repair%20status%2C%20as%20well%20as%20survival%20data%20for%20426%20cases.%20To%20demonstrate%0ASurGen%27s%20practical%20utility%2C%20we%20conducted%20a%20proof-of-concept%20machine%20learning%0Aexperiment%20predicting%20mismatch%20repair%20status%20from%20the%20WSIs%2C%20achieving%20a%20test%0AAUROC%20of%200.8316.%20These%20preliminary%20results%20underscore%20the%20dataset%27s%20potential%0Ato%20facilitate%20research%20in%20biomarker%20discovery%2C%20prognostic%20modelling%2C%20and%0Aadvanced%20machine%20learning%20applications%20in%20colorectal%20cancer.%0A%24%5Ctextbf%7BConclusions%7D%24%3A%20SurGen%20offers%20a%20valuable%20resource%20for%20the%20scientific%0Acommunity%2C%20enabling%20studies%20that%20require%20high-quality%20WSIs%20linked%20with%0Acomprehensive%20clinical%20and%20genetic%20information%20on%20colorectal%20cancer.%20Our%0Ainitial%20findings%20affirm%20the%20dataset%27s%20capacity%20to%20advance%20diagnostic%20precision%0Aand%20foster%20the%20development%20of%20personalised%20treatment%20strategies%20in%20colorectal%0Aoncology.%20Data%20available%20online%20at%20https%3A//doi.org/10.6019/S-BIAD1285.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04946v1&entry.124074799=Read"},
{"title": "TLXML: Task-Level Explanation of Meta-Learning via Influence Functions", "author": "Yoshihiro Mitsuka and Shadan Golestan and Zahin Sufiyan and Sheila Schoepp and Shotaro Miwa and Osmar R. Zaiane", "abstract": "  The scheme of adaptation via meta-learning is seen as an ingredient for\nsolving the problem of data shortage or distribution shift in real-world\napplications, but it also brings the new risk of inappropriate updates of the\nmodel in the user environment, which increases the demand for explainability.\nAmong the various types of XAI methods, establishing a method of explanation\nbased on past experience in meta-learning requires special consideration due to\nits bi-level structure of training, which has been left unexplored. In this\nwork, we propose influence functions for explaining meta-learning that measure\nthe sensitivities of training tasks to adaptation and inference. We also argue\nthat the approximation of the Hessian using the Gauss-Newton matrix resolves\ncomputational barriers peculiar to meta-learning. We demonstrate the adequacy\nof the method through experiments on task distinction and task distribution\ndistinction using image classification tasks with MAML and Prototypical\nNetwork.\n", "link": "http://arxiv.org/abs/2501.14271v2", "date": "2025-02-07", "relevancy": 2.0311, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5386}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4919}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TLXML%3A%20Task-Level%20Explanation%20of%20Meta-Learning%20via%20Influence%20Functions&body=Title%3A%20TLXML%3A%20Task-Level%20Explanation%20of%20Meta-Learning%20via%20Influence%20Functions%0AAuthor%3A%20Yoshihiro%20Mitsuka%20and%20Shadan%20Golestan%20and%20Zahin%20Sufiyan%20and%20Sheila%20Schoepp%20and%20Shotaro%20Miwa%20and%20Osmar%20R.%20Zaiane%0AAbstract%3A%20%20%20The%20scheme%20of%20adaptation%20via%20meta-learning%20is%20seen%20as%20an%20ingredient%20for%0Asolving%20the%20problem%20of%20data%20shortage%20or%20distribution%20shift%20in%20real-world%0Aapplications%2C%20but%20it%20also%20brings%20the%20new%20risk%20of%20inappropriate%20updates%20of%20the%0Amodel%20in%20the%20user%20environment%2C%20which%20increases%20the%20demand%20for%20explainability.%0AAmong%20the%20various%20types%20of%20XAI%20methods%2C%20establishing%20a%20method%20of%20explanation%0Abased%20on%20past%20experience%20in%20meta-learning%20requires%20special%20consideration%20due%20to%0Aits%20bi-level%20structure%20of%20training%2C%20which%20has%20been%20left%20unexplored.%20In%20this%0Awork%2C%20we%20propose%20influence%20functions%20for%20explaining%20meta-learning%20that%20measure%0Athe%20sensitivities%20of%20training%20tasks%20to%20adaptation%20and%20inference.%20We%20also%20argue%0Athat%20the%20approximation%20of%20the%20Hessian%20using%20the%20Gauss-Newton%20matrix%20resolves%0Acomputational%20barriers%20peculiar%20to%20meta-learning.%20We%20demonstrate%20the%20adequacy%0Aof%20the%20method%20through%20experiments%20on%20task%20distinction%20and%20task%20distribution%0Adistinction%20using%20image%20classification%20tasks%20with%20MAML%20and%20Prototypical%0ANetwork.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.14271v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTLXML%253A%2520Task-Level%2520Explanation%2520of%2520Meta-Learning%2520via%2520Influence%2520Functions%26entry.906535625%3DYoshihiro%2520Mitsuka%2520and%2520Shadan%2520Golestan%2520and%2520Zahin%2520Sufiyan%2520and%2520Sheila%2520Schoepp%2520and%2520Shotaro%2520Miwa%2520and%2520Osmar%2520R.%2520Zaiane%26entry.1292438233%3D%2520%2520The%2520scheme%2520of%2520adaptation%2520via%2520meta-learning%2520is%2520seen%2520as%2520an%2520ingredient%2520for%250Asolving%2520the%2520problem%2520of%2520data%2520shortage%2520or%2520distribution%2520shift%2520in%2520real-world%250Aapplications%252C%2520but%2520it%2520also%2520brings%2520the%2520new%2520risk%2520of%2520inappropriate%2520updates%2520of%2520the%250Amodel%2520in%2520the%2520user%2520environment%252C%2520which%2520increases%2520the%2520demand%2520for%2520explainability.%250AAmong%2520the%2520various%2520types%2520of%2520XAI%2520methods%252C%2520establishing%2520a%2520method%2520of%2520explanation%250Abased%2520on%2520past%2520experience%2520in%2520meta-learning%2520requires%2520special%2520consideration%2520due%2520to%250Aits%2520bi-level%2520structure%2520of%2520training%252C%2520which%2520has%2520been%2520left%2520unexplored.%2520In%2520this%250Awork%252C%2520we%2520propose%2520influence%2520functions%2520for%2520explaining%2520meta-learning%2520that%2520measure%250Athe%2520sensitivities%2520of%2520training%2520tasks%2520to%2520adaptation%2520and%2520inference.%2520We%2520also%2520argue%250Athat%2520the%2520approximation%2520of%2520the%2520Hessian%2520using%2520the%2520Gauss-Newton%2520matrix%2520resolves%250Acomputational%2520barriers%2520peculiar%2520to%2520meta-learning.%2520We%2520demonstrate%2520the%2520adequacy%250Aof%2520the%2520method%2520through%2520experiments%2520on%2520task%2520distinction%2520and%2520task%2520distribution%250Adistinction%2520using%2520image%2520classification%2520tasks%2520with%2520MAML%2520and%2520Prototypical%250ANetwork.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.14271v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TLXML%3A%20Task-Level%20Explanation%20of%20Meta-Learning%20via%20Influence%20Functions&entry.906535625=Yoshihiro%20Mitsuka%20and%20Shadan%20Golestan%20and%20Zahin%20Sufiyan%20and%20Sheila%20Schoepp%20and%20Shotaro%20Miwa%20and%20Osmar%20R.%20Zaiane&entry.1292438233=%20%20The%20scheme%20of%20adaptation%20via%20meta-learning%20is%20seen%20as%20an%20ingredient%20for%0Asolving%20the%20problem%20of%20data%20shortage%20or%20distribution%20shift%20in%20real-world%0Aapplications%2C%20but%20it%20also%20brings%20the%20new%20risk%20of%20inappropriate%20updates%20of%20the%0Amodel%20in%20the%20user%20environment%2C%20which%20increases%20the%20demand%20for%20explainability.%0AAmong%20the%20various%20types%20of%20XAI%20methods%2C%20establishing%20a%20method%20of%20explanation%0Abased%20on%20past%20experience%20in%20meta-learning%20requires%20special%20consideration%20due%20to%0Aits%20bi-level%20structure%20of%20training%2C%20which%20has%20been%20left%20unexplored.%20In%20this%0Awork%2C%20we%20propose%20influence%20functions%20for%20explaining%20meta-learning%20that%20measure%0Athe%20sensitivities%20of%20training%20tasks%20to%20adaptation%20and%20inference.%20We%20also%20argue%0Athat%20the%20approximation%20of%20the%20Hessian%20using%20the%20Gauss-Newton%20matrix%20resolves%0Acomputational%20barriers%20peculiar%20to%20meta-learning.%20We%20demonstrate%20the%20adequacy%0Aof%20the%20method%20through%20experiments%20on%20task%20distinction%20and%20task%20distribution%0Adistinction%20using%20image%20classification%20tasks%20with%20MAML%20and%20Prototypical%0ANetwork.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.14271v2&entry.124074799=Read"},
{"title": "Unified Approaches in Self-Supervised Event Stream Modeling: Progress\n  and Prospects", "author": "Levente Z\u00f3lyomi and Tianze Wang and Sofiane Ennadir and Oleg Smirnov and Lele Cao", "abstract": "  The proliferation of digital interactions across diverse domains, such as\nhealthcare, e-commerce, gaming, and finance, has resulted in the generation of\nvast volumes of event stream (ES) data. ES data comprises continuous sequences\nof timestamped events that encapsulate detailed contextual information relevant\nto each domain. While ES data holds significant potential for extracting\nactionable insights and enhancing decision-making, its effective utilization is\nhindered by challenges such as the scarcity of labeled data and the fragmented\nnature of existing research efforts. Self-Supervised Learning (SSL) has emerged\nas a promising paradigm to address these challenges by enabling the extraction\nof meaningful representations from unlabeled ES data. In this survey, we\nsystematically review and synthesize SSL methodologies tailored for ES modeling\nacross multiple domains, bridging the gaps between domain-specific approaches\nthat have traditionally operated in isolation. We present a comprehensive\ntaxonomy of SSL techniques, encompassing both predictive and contrastive\nparadigms, and analyze their applicability and effectiveness within different\napplication contexts. Furthermore, we identify critical gaps in current\nresearch and propose a future research agenda aimed at developing scalable,\ndomain-agnostic SSL frameworks for ES modeling. By unifying disparate research\nefforts and highlighting cross-domain synergies, this survey aims to accelerate\ninnovation, improve reproducibility, and expand the applicability of SSL to\ndiverse real-world ES challenges.\n", "link": "http://arxiv.org/abs/2502.04899v1", "date": "2025-02-07", "relevancy": 2.0194, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5121}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5121}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Approaches%20in%20Self-Supervised%20Event%20Stream%20Modeling%3A%20Progress%0A%20%20and%20Prospects&body=Title%3A%20Unified%20Approaches%20in%20Self-Supervised%20Event%20Stream%20Modeling%3A%20Progress%0A%20%20and%20Prospects%0AAuthor%3A%20Levente%20Z%C3%B3lyomi%20and%20Tianze%20Wang%20and%20Sofiane%20Ennadir%20and%20Oleg%20Smirnov%20and%20Lele%20Cao%0AAbstract%3A%20%20%20The%20proliferation%20of%20digital%20interactions%20across%20diverse%20domains%2C%20such%20as%0Ahealthcare%2C%20e-commerce%2C%20gaming%2C%20and%20finance%2C%20has%20resulted%20in%20the%20generation%20of%0Avast%20volumes%20of%20event%20stream%20%28ES%29%20data.%20ES%20data%20comprises%20continuous%20sequences%0Aof%20timestamped%20events%20that%20encapsulate%20detailed%20contextual%20information%20relevant%0Ato%20each%20domain.%20While%20ES%20data%20holds%20significant%20potential%20for%20extracting%0Aactionable%20insights%20and%20enhancing%20decision-making%2C%20its%20effective%20utilization%20is%0Ahindered%20by%20challenges%20such%20as%20the%20scarcity%20of%20labeled%20data%20and%20the%20fragmented%0Anature%20of%20existing%20research%20efforts.%20Self-Supervised%20Learning%20%28SSL%29%20has%20emerged%0Aas%20a%20promising%20paradigm%20to%20address%20these%20challenges%20by%20enabling%20the%20extraction%0Aof%20meaningful%20representations%20from%20unlabeled%20ES%20data.%20In%20this%20survey%2C%20we%0Asystematically%20review%20and%20synthesize%20SSL%20methodologies%20tailored%20for%20ES%20modeling%0Aacross%20multiple%20domains%2C%20bridging%20the%20gaps%20between%20domain-specific%20approaches%0Athat%20have%20traditionally%20operated%20in%20isolation.%20We%20present%20a%20comprehensive%0Ataxonomy%20of%20SSL%20techniques%2C%20encompassing%20both%20predictive%20and%20contrastive%0Aparadigms%2C%20and%20analyze%20their%20applicability%20and%20effectiveness%20within%20different%0Aapplication%20contexts.%20Furthermore%2C%20we%20identify%20critical%20gaps%20in%20current%0Aresearch%20and%20propose%20a%20future%20research%20agenda%20aimed%20at%20developing%20scalable%2C%0Adomain-agnostic%20SSL%20frameworks%20for%20ES%20modeling.%20By%20unifying%20disparate%20research%0Aefforts%20and%20highlighting%20cross-domain%20synergies%2C%20this%20survey%20aims%20to%20accelerate%0Ainnovation%2C%20improve%20reproducibility%2C%20and%20expand%20the%20applicability%20of%20SSL%20to%0Adiverse%20real-world%20ES%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04899v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Approaches%2520in%2520Self-Supervised%2520Event%2520Stream%2520Modeling%253A%2520Progress%250A%2520%2520and%2520Prospects%26entry.906535625%3DLevente%2520Z%25C3%25B3lyomi%2520and%2520Tianze%2520Wang%2520and%2520Sofiane%2520Ennadir%2520and%2520Oleg%2520Smirnov%2520and%2520Lele%2520Cao%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520digital%2520interactions%2520across%2520diverse%2520domains%252C%2520such%2520as%250Ahealthcare%252C%2520e-commerce%252C%2520gaming%252C%2520and%2520finance%252C%2520has%2520resulted%2520in%2520the%2520generation%2520of%250Avast%2520volumes%2520of%2520event%2520stream%2520%2528ES%2529%2520data.%2520ES%2520data%2520comprises%2520continuous%2520sequences%250Aof%2520timestamped%2520events%2520that%2520encapsulate%2520detailed%2520contextual%2520information%2520relevant%250Ato%2520each%2520domain.%2520While%2520ES%2520data%2520holds%2520significant%2520potential%2520for%2520extracting%250Aactionable%2520insights%2520and%2520enhancing%2520decision-making%252C%2520its%2520effective%2520utilization%2520is%250Ahindered%2520by%2520challenges%2520such%2520as%2520the%2520scarcity%2520of%2520labeled%2520data%2520and%2520the%2520fragmented%250Anature%2520of%2520existing%2520research%2520efforts.%2520Self-Supervised%2520Learning%2520%2528SSL%2529%2520has%2520emerged%250Aas%2520a%2520promising%2520paradigm%2520to%2520address%2520these%2520challenges%2520by%2520enabling%2520the%2520extraction%250Aof%2520meaningful%2520representations%2520from%2520unlabeled%2520ES%2520data.%2520In%2520this%2520survey%252C%2520we%250Asystematically%2520review%2520and%2520synthesize%2520SSL%2520methodologies%2520tailored%2520for%2520ES%2520modeling%250Aacross%2520multiple%2520domains%252C%2520bridging%2520the%2520gaps%2520between%2520domain-specific%2520approaches%250Athat%2520have%2520traditionally%2520operated%2520in%2520isolation.%2520We%2520present%2520a%2520comprehensive%250Ataxonomy%2520of%2520SSL%2520techniques%252C%2520encompassing%2520both%2520predictive%2520and%2520contrastive%250Aparadigms%252C%2520and%2520analyze%2520their%2520applicability%2520and%2520effectiveness%2520within%2520different%250Aapplication%2520contexts.%2520Furthermore%252C%2520we%2520identify%2520critical%2520gaps%2520in%2520current%250Aresearch%2520and%2520propose%2520a%2520future%2520research%2520agenda%2520aimed%2520at%2520developing%2520scalable%252C%250Adomain-agnostic%2520SSL%2520frameworks%2520for%2520ES%2520modeling.%2520By%2520unifying%2520disparate%2520research%250Aefforts%2520and%2520highlighting%2520cross-domain%2520synergies%252C%2520this%2520survey%2520aims%2520to%2520accelerate%250Ainnovation%252C%2520improve%2520reproducibility%252C%2520and%2520expand%2520the%2520applicability%2520of%2520SSL%2520to%250Adiverse%2520real-world%2520ES%2520challenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04899v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Approaches%20in%20Self-Supervised%20Event%20Stream%20Modeling%3A%20Progress%0A%20%20and%20Prospects&entry.906535625=Levente%20Z%C3%B3lyomi%20and%20Tianze%20Wang%20and%20Sofiane%20Ennadir%20and%20Oleg%20Smirnov%20and%20Lele%20Cao&entry.1292438233=%20%20The%20proliferation%20of%20digital%20interactions%20across%20diverse%20domains%2C%20such%20as%0Ahealthcare%2C%20e-commerce%2C%20gaming%2C%20and%20finance%2C%20has%20resulted%20in%20the%20generation%20of%0Avast%20volumes%20of%20event%20stream%20%28ES%29%20data.%20ES%20data%20comprises%20continuous%20sequences%0Aof%20timestamped%20events%20that%20encapsulate%20detailed%20contextual%20information%20relevant%0Ato%20each%20domain.%20While%20ES%20data%20holds%20significant%20potential%20for%20extracting%0Aactionable%20insights%20and%20enhancing%20decision-making%2C%20its%20effective%20utilization%20is%0Ahindered%20by%20challenges%20such%20as%20the%20scarcity%20of%20labeled%20data%20and%20the%20fragmented%0Anature%20of%20existing%20research%20efforts.%20Self-Supervised%20Learning%20%28SSL%29%20has%20emerged%0Aas%20a%20promising%20paradigm%20to%20address%20these%20challenges%20by%20enabling%20the%20extraction%0Aof%20meaningful%20representations%20from%20unlabeled%20ES%20data.%20In%20this%20survey%2C%20we%0Asystematically%20review%20and%20synthesize%20SSL%20methodologies%20tailored%20for%20ES%20modeling%0Aacross%20multiple%20domains%2C%20bridging%20the%20gaps%20between%20domain-specific%20approaches%0Athat%20have%20traditionally%20operated%20in%20isolation.%20We%20present%20a%20comprehensive%0Ataxonomy%20of%20SSL%20techniques%2C%20encompassing%20both%20predictive%20and%20contrastive%0Aparadigms%2C%20and%20analyze%20their%20applicability%20and%20effectiveness%20within%20different%0Aapplication%20contexts.%20Furthermore%2C%20we%20identify%20critical%20gaps%20in%20current%0Aresearch%20and%20propose%20a%20future%20research%20agenda%20aimed%20at%20developing%20scalable%2C%0Adomain-agnostic%20SSL%20frameworks%20for%20ES%20modeling.%20By%20unifying%20disparate%20research%0Aefforts%20and%20highlighting%20cross-domain%20synergies%2C%20this%20survey%20aims%20to%20accelerate%0Ainnovation%2C%20improve%20reproducibility%2C%20and%20expand%20the%20applicability%20of%20SSL%20to%0Adiverse%20real-world%20ES%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04899v1&entry.124074799=Read"},
{"title": "Multiple Instance Learning with Coarse-to-Fine Self-Distillation", "author": "Shuyang Wu and Yifu Qiu and Ines P. Nearchou and Sandrine Prost and Jonathan A. Fallowfield and Hakan Bilen and Timothy J. Kendall", "abstract": "  Multiple Instance Learning (MIL) for whole slide image (WSI) analysis in\ncomputational pathology often neglects instance-level learning as supervision\nis typically provided only at the bag level. In this work, we present PathMIL,\na framework designed to improve MIL through two perspectives: (1) employing\ninstance-level supervision and (2) learning inter-instance contextual\ninformation on bag level. Firstly, we propose a novel Coarse-to-Fine\nSelf-Distillation (CFSD) paradigm, to probe and distil a classifier trained\nwith bag-level information to obtain instance-level labels which could\neffectively provide the supervision for the same classifier in a finer way.\nSecondly, to capture inter-instance contextual information in WSI, we propose\nTwo-Dimensional Positional Encoding (2DPE), which encodes the spatial\nappearance of instances within a bag. We also theoretically and empirically\nprove the instance-level learnability of CFSD. PathMIL is evaluated on multiple\nbenchmarking tasks, including subtype classification (TCGA-NSCLC), tumour\nclassification (CAMELYON16), and an internal benchmark for breast cancer\nreceptor status classification. Our method achieves state-of-the-art\nperformance, with AUC scores of 0.9152 and 0.8524 for estrogen and progesterone\nreceptor status classification, respectively, an AUC of 0.9618 for subtype\nclassification, and 0.8634 for tumour classification, surpassing existing\nmethods.\n", "link": "http://arxiv.org/abs/2502.02707v2", "date": "2025-02-07", "relevancy": 2.0109, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5144}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4956}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multiple%20Instance%20Learning%20with%20Coarse-to-Fine%20Self-Distillation&body=Title%3A%20Multiple%20Instance%20Learning%20with%20Coarse-to-Fine%20Self-Distillation%0AAuthor%3A%20Shuyang%20Wu%20and%20Yifu%20Qiu%20and%20Ines%20P.%20Nearchou%20and%20Sandrine%20Prost%20and%20Jonathan%20A.%20Fallowfield%20and%20Hakan%20Bilen%20and%20Timothy%20J.%20Kendall%0AAbstract%3A%20%20%20Multiple%20Instance%20Learning%20%28MIL%29%20for%20whole%20slide%20image%20%28WSI%29%20analysis%20in%0Acomputational%20pathology%20often%20neglects%20instance-level%20learning%20as%20supervision%0Ais%20typically%20provided%20only%20at%20the%20bag%20level.%20In%20this%20work%2C%20we%20present%20PathMIL%2C%0Aa%20framework%20designed%20to%20improve%20MIL%20through%20two%20perspectives%3A%20%281%29%20employing%0Ainstance-level%20supervision%20and%20%282%29%20learning%20inter-instance%20contextual%0Ainformation%20on%20bag%20level.%20Firstly%2C%20we%20propose%20a%20novel%20Coarse-to-Fine%0ASelf-Distillation%20%28CFSD%29%20paradigm%2C%20to%20probe%20and%20distil%20a%20classifier%20trained%0Awith%20bag-level%20information%20to%20obtain%20instance-level%20labels%20which%20could%0Aeffectively%20provide%20the%20supervision%20for%20the%20same%20classifier%20in%20a%20finer%20way.%0ASecondly%2C%20to%20capture%20inter-instance%20contextual%20information%20in%20WSI%2C%20we%20propose%0ATwo-Dimensional%20Positional%20Encoding%20%282DPE%29%2C%20which%20encodes%20the%20spatial%0Aappearance%20of%20instances%20within%20a%20bag.%20We%20also%20theoretically%20and%20empirically%0Aprove%20the%20instance-level%20learnability%20of%20CFSD.%20PathMIL%20is%20evaluated%20on%20multiple%0Abenchmarking%20tasks%2C%20including%20subtype%20classification%20%28TCGA-NSCLC%29%2C%20tumour%0Aclassification%20%28CAMELYON16%29%2C%20and%20an%20internal%20benchmark%20for%20breast%20cancer%0Areceptor%20status%20classification.%20Our%20method%20achieves%20state-of-the-art%0Aperformance%2C%20with%20AUC%20scores%20of%200.9152%20and%200.8524%20for%20estrogen%20and%20progesterone%0Areceptor%20status%20classification%2C%20respectively%2C%20an%20AUC%20of%200.9618%20for%20subtype%0Aclassification%2C%20and%200.8634%20for%20tumour%20classification%2C%20surpassing%20existing%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02707v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiple%2520Instance%2520Learning%2520with%2520Coarse-to-Fine%2520Self-Distillation%26entry.906535625%3DShuyang%2520Wu%2520and%2520Yifu%2520Qiu%2520and%2520Ines%2520P.%2520Nearchou%2520and%2520Sandrine%2520Prost%2520and%2520Jonathan%2520A.%2520Fallowfield%2520and%2520Hakan%2520Bilen%2520and%2520Timothy%2520J.%2520Kendall%26entry.1292438233%3D%2520%2520Multiple%2520Instance%2520Learning%2520%2528MIL%2529%2520for%2520whole%2520slide%2520image%2520%2528WSI%2529%2520analysis%2520in%250Acomputational%2520pathology%2520often%2520neglects%2520instance-level%2520learning%2520as%2520supervision%250Ais%2520typically%2520provided%2520only%2520at%2520the%2520bag%2520level.%2520In%2520this%2520work%252C%2520we%2520present%2520PathMIL%252C%250Aa%2520framework%2520designed%2520to%2520improve%2520MIL%2520through%2520two%2520perspectives%253A%2520%25281%2529%2520employing%250Ainstance-level%2520supervision%2520and%2520%25282%2529%2520learning%2520inter-instance%2520contextual%250Ainformation%2520on%2520bag%2520level.%2520Firstly%252C%2520we%2520propose%2520a%2520novel%2520Coarse-to-Fine%250ASelf-Distillation%2520%2528CFSD%2529%2520paradigm%252C%2520to%2520probe%2520and%2520distil%2520a%2520classifier%2520trained%250Awith%2520bag-level%2520information%2520to%2520obtain%2520instance-level%2520labels%2520which%2520could%250Aeffectively%2520provide%2520the%2520supervision%2520for%2520the%2520same%2520classifier%2520in%2520a%2520finer%2520way.%250ASecondly%252C%2520to%2520capture%2520inter-instance%2520contextual%2520information%2520in%2520WSI%252C%2520we%2520propose%250ATwo-Dimensional%2520Positional%2520Encoding%2520%25282DPE%2529%252C%2520which%2520encodes%2520the%2520spatial%250Aappearance%2520of%2520instances%2520within%2520a%2520bag.%2520We%2520also%2520theoretically%2520and%2520empirically%250Aprove%2520the%2520instance-level%2520learnability%2520of%2520CFSD.%2520PathMIL%2520is%2520evaluated%2520on%2520multiple%250Abenchmarking%2520tasks%252C%2520including%2520subtype%2520classification%2520%2528TCGA-NSCLC%2529%252C%2520tumour%250Aclassification%2520%2528CAMELYON16%2529%252C%2520and%2520an%2520internal%2520benchmark%2520for%2520breast%2520cancer%250Areceptor%2520status%2520classification.%2520Our%2520method%2520achieves%2520state-of-the-art%250Aperformance%252C%2520with%2520AUC%2520scores%2520of%25200.9152%2520and%25200.8524%2520for%2520estrogen%2520and%2520progesterone%250Areceptor%2520status%2520classification%252C%2520respectively%252C%2520an%2520AUC%2520of%25200.9618%2520for%2520subtype%250Aclassification%252C%2520and%25200.8634%2520for%2520tumour%2520classification%252C%2520surpassing%2520existing%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02707v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multiple%20Instance%20Learning%20with%20Coarse-to-Fine%20Self-Distillation&entry.906535625=Shuyang%20Wu%20and%20Yifu%20Qiu%20and%20Ines%20P.%20Nearchou%20and%20Sandrine%20Prost%20and%20Jonathan%20A.%20Fallowfield%20and%20Hakan%20Bilen%20and%20Timothy%20J.%20Kendall&entry.1292438233=%20%20Multiple%20Instance%20Learning%20%28MIL%29%20for%20whole%20slide%20image%20%28WSI%29%20analysis%20in%0Acomputational%20pathology%20often%20neglects%20instance-level%20learning%20as%20supervision%0Ais%20typically%20provided%20only%20at%20the%20bag%20level.%20In%20this%20work%2C%20we%20present%20PathMIL%2C%0Aa%20framework%20designed%20to%20improve%20MIL%20through%20two%20perspectives%3A%20%281%29%20employing%0Ainstance-level%20supervision%20and%20%282%29%20learning%20inter-instance%20contextual%0Ainformation%20on%20bag%20level.%20Firstly%2C%20we%20propose%20a%20novel%20Coarse-to-Fine%0ASelf-Distillation%20%28CFSD%29%20paradigm%2C%20to%20probe%20and%20distil%20a%20classifier%20trained%0Awith%20bag-level%20information%20to%20obtain%20instance-level%20labels%20which%20could%0Aeffectively%20provide%20the%20supervision%20for%20the%20same%20classifier%20in%20a%20finer%20way.%0ASecondly%2C%20to%20capture%20inter-instance%20contextual%20information%20in%20WSI%2C%20we%20propose%0ATwo-Dimensional%20Positional%20Encoding%20%282DPE%29%2C%20which%20encodes%20the%20spatial%0Aappearance%20of%20instances%20within%20a%20bag.%20We%20also%20theoretically%20and%20empirically%0Aprove%20the%20instance-level%20learnability%20of%20CFSD.%20PathMIL%20is%20evaluated%20on%20multiple%0Abenchmarking%20tasks%2C%20including%20subtype%20classification%20%28TCGA-NSCLC%29%2C%20tumour%0Aclassification%20%28CAMELYON16%29%2C%20and%20an%20internal%20benchmark%20for%20breast%20cancer%0Areceptor%20status%20classification.%20Our%20method%20achieves%20state-of-the-art%0Aperformance%2C%20with%20AUC%20scores%20of%200.9152%20and%200.8524%20for%20estrogen%20and%20progesterone%0Areceptor%20status%20classification%2C%20respectively%2C%20an%20AUC%20of%200.9618%20for%20subtype%0Aclassification%2C%20and%200.8634%20for%20tumour%20classification%2C%20surpassing%20existing%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02707v2&entry.124074799=Read"},
{"title": "Data-Parallel Neural Network Training via Nonlinearly Preconditioned\n  Trust-Region Method", "author": "Samuel A. Cruz Alegr\u00eda and Ken Trotti and Alena Kopani\u010d\u00e1kov\u00e1 and Rolf Krause", "abstract": "  Parallel training methods are increasingly relevant in machine learning (ML)\ndue to the continuing growth in model and dataset sizes. We propose a variant\nof the Additively Preconditioned Trust-Region Strategy (APTS) for training deep\nneural networks (DNNs). The proposed APTS method utilizes a data-parallel\napproach to construct a nonlinear preconditioner employed in the nonlinear\noptimization strategy. In contrast to the common employment of Stochastic\nGradient Descent (SGD) and Adaptive Moment Estimation (Adam), which are both\nvariants of gradient descent (GD) algorithms, the APTS method implicitly\nadjusts the step sizes in each iteration, thereby removing the need for costly\nhyperparameter tuning. We demonstrate the performance of the proposed APTS\nvariant using the MNIST and CIFAR-10 datasets. The results obtained indicate\nthat the APTS variant proposed here achieves comparable validation accuracy to\nSGD and Adam, all while allowing for parallel training and obviating the need\nfor expensive hyperparameter tuning.\n", "link": "http://arxiv.org/abs/2502.05133v1", "date": "2025-02-07", "relevancy": 2.0065, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5319}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4884}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Parallel%20Neural%20Network%20Training%20via%20Nonlinearly%20Preconditioned%0A%20%20Trust-Region%20Method&body=Title%3A%20Data-Parallel%20Neural%20Network%20Training%20via%20Nonlinearly%20Preconditioned%0A%20%20Trust-Region%20Method%0AAuthor%3A%20Samuel%20A.%20Cruz%20Alegr%C3%ADa%20and%20Ken%20Trotti%20and%20Alena%20Kopani%C4%8D%C3%A1kov%C3%A1%20and%20Rolf%20Krause%0AAbstract%3A%20%20%20Parallel%20training%20methods%20are%20increasingly%20relevant%20in%20machine%20learning%20%28ML%29%0Adue%20to%20the%20continuing%20growth%20in%20model%20and%20dataset%20sizes.%20We%20propose%20a%20variant%0Aof%20the%20Additively%20Preconditioned%20Trust-Region%20Strategy%20%28APTS%29%20for%20training%20deep%0Aneural%20networks%20%28DNNs%29.%20The%20proposed%20APTS%20method%20utilizes%20a%20data-parallel%0Aapproach%20to%20construct%20a%20nonlinear%20preconditioner%20employed%20in%20the%20nonlinear%0Aoptimization%20strategy.%20In%20contrast%20to%20the%20common%20employment%20of%20Stochastic%0AGradient%20Descent%20%28SGD%29%20and%20Adaptive%20Moment%20Estimation%20%28Adam%29%2C%20which%20are%20both%0Avariants%20of%20gradient%20descent%20%28GD%29%20algorithms%2C%20the%20APTS%20method%20implicitly%0Aadjusts%20the%20step%20sizes%20in%20each%20iteration%2C%20thereby%20removing%20the%20need%20for%20costly%0Ahyperparameter%20tuning.%20We%20demonstrate%20the%20performance%20of%20the%20proposed%20APTS%0Avariant%20using%20the%20MNIST%20and%20CIFAR-10%20datasets.%20The%20results%20obtained%20indicate%0Athat%20the%20APTS%20variant%20proposed%20here%20achieves%20comparable%20validation%20accuracy%20to%0ASGD%20and%20Adam%2C%20all%20while%20allowing%20for%20parallel%20training%20and%20obviating%20the%20need%0Afor%20expensive%20hyperparameter%20tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05133v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Parallel%2520Neural%2520Network%2520Training%2520via%2520Nonlinearly%2520Preconditioned%250A%2520%2520Trust-Region%2520Method%26entry.906535625%3DSamuel%2520A.%2520Cruz%2520Alegr%25C3%25ADa%2520and%2520Ken%2520Trotti%2520and%2520Alena%2520Kopani%25C4%258D%25C3%25A1kov%25C3%25A1%2520and%2520Rolf%2520Krause%26entry.1292438233%3D%2520%2520Parallel%2520training%2520methods%2520are%2520increasingly%2520relevant%2520in%2520machine%2520learning%2520%2528ML%2529%250Adue%2520to%2520the%2520continuing%2520growth%2520in%2520model%2520and%2520dataset%2520sizes.%2520We%2520propose%2520a%2520variant%250Aof%2520the%2520Additively%2520Preconditioned%2520Trust-Region%2520Strategy%2520%2528APTS%2529%2520for%2520training%2520deep%250Aneural%2520networks%2520%2528DNNs%2529.%2520The%2520proposed%2520APTS%2520method%2520utilizes%2520a%2520data-parallel%250Aapproach%2520to%2520construct%2520a%2520nonlinear%2520preconditioner%2520employed%2520in%2520the%2520nonlinear%250Aoptimization%2520strategy.%2520In%2520contrast%2520to%2520the%2520common%2520employment%2520of%2520Stochastic%250AGradient%2520Descent%2520%2528SGD%2529%2520and%2520Adaptive%2520Moment%2520Estimation%2520%2528Adam%2529%252C%2520which%2520are%2520both%250Avariants%2520of%2520gradient%2520descent%2520%2528GD%2529%2520algorithms%252C%2520the%2520APTS%2520method%2520implicitly%250Aadjusts%2520the%2520step%2520sizes%2520in%2520each%2520iteration%252C%2520thereby%2520removing%2520the%2520need%2520for%2520costly%250Ahyperparameter%2520tuning.%2520We%2520demonstrate%2520the%2520performance%2520of%2520the%2520proposed%2520APTS%250Avariant%2520using%2520the%2520MNIST%2520and%2520CIFAR-10%2520datasets.%2520The%2520results%2520obtained%2520indicate%250Athat%2520the%2520APTS%2520variant%2520proposed%2520here%2520achieves%2520comparable%2520validation%2520accuracy%2520to%250ASGD%2520and%2520Adam%252C%2520all%2520while%2520allowing%2520for%2520parallel%2520training%2520and%2520obviating%2520the%2520need%250Afor%2520expensive%2520hyperparameter%2520tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05133v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Parallel%20Neural%20Network%20Training%20via%20Nonlinearly%20Preconditioned%0A%20%20Trust-Region%20Method&entry.906535625=Samuel%20A.%20Cruz%20Alegr%C3%ADa%20and%20Ken%20Trotti%20and%20Alena%20Kopani%C4%8D%C3%A1kov%C3%A1%20and%20Rolf%20Krause&entry.1292438233=%20%20Parallel%20training%20methods%20are%20increasingly%20relevant%20in%20machine%20learning%20%28ML%29%0Adue%20to%20the%20continuing%20growth%20in%20model%20and%20dataset%20sizes.%20We%20propose%20a%20variant%0Aof%20the%20Additively%20Preconditioned%20Trust-Region%20Strategy%20%28APTS%29%20for%20training%20deep%0Aneural%20networks%20%28DNNs%29.%20The%20proposed%20APTS%20method%20utilizes%20a%20data-parallel%0Aapproach%20to%20construct%20a%20nonlinear%20preconditioner%20employed%20in%20the%20nonlinear%0Aoptimization%20strategy.%20In%20contrast%20to%20the%20common%20employment%20of%20Stochastic%0AGradient%20Descent%20%28SGD%29%20and%20Adaptive%20Moment%20Estimation%20%28Adam%29%2C%20which%20are%20both%0Avariants%20of%20gradient%20descent%20%28GD%29%20algorithms%2C%20the%20APTS%20method%20implicitly%0Aadjusts%20the%20step%20sizes%20in%20each%20iteration%2C%20thereby%20removing%20the%20need%20for%20costly%0Ahyperparameter%20tuning.%20We%20demonstrate%20the%20performance%20of%20the%20proposed%20APTS%0Avariant%20using%20the%20MNIST%20and%20CIFAR-10%20datasets.%20The%20results%20obtained%20indicate%0Athat%20the%20APTS%20variant%20proposed%20here%20achieves%20comparable%20validation%20accuracy%20to%0ASGD%20and%20Adam%2C%20all%20while%20allowing%20for%20parallel%20training%20and%20obviating%20the%20need%0Afor%20expensive%20hyperparameter%20tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05133v1&entry.124074799=Read"},
{"title": "$TAR^2$: Temporal-Agent Reward Redistribution for Optimal Policy\n  Preservation in Multi-Agent Reinforcement Learning", "author": "Aditya Kapoor and Kale-ab Tessera and Mayank Baranwal and Harshad Khadilkar and Stefano Albrecht and Mingfei Sun", "abstract": "  In cooperative multi-agent reinforcement learning (MARL), learning effective\npolicies is challenging when global rewards are sparse and delayed. This\ndifficulty arises from the need to assign credit across both agents and time\nsteps, a problem that existing methods often fail to address in episodic,\nlong-horizon tasks. We propose Temporal-Agent Reward Redistribution $TAR^2$, a\nnovel approach that decomposes sparse global rewards into agent-specific,\ntime-step-specific components, thereby providing more frequent and accurate\nfeedback for policy learning. Theoretically, we show that $TAR^2$ (i) aligns\nwith potential-based reward shaping, preserving the same optimal policies as\nthe original environment, and (ii) maintains policy gradient update directions\nidentical to those under the original sparse reward, ensuring unbiased credit\nsignals. Empirical results on two challenging benchmarks, SMACLite and Google\nResearch Football, demonstrate that $TAR^2$ significantly stabilizes and\naccelerates convergence, outperforming strong baselines like AREL and STAS in\nboth learning speed and final performance. These findings establish $TAR^2$ as\na principled and practical solution for agent-temporal credit assignment in\nsparse-reward multi-agent systems.\n", "link": "http://arxiv.org/abs/2502.04864v1", "date": "2025-02-07", "relevancy": 2.0029, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5237}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4885}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24TAR%5E2%24%3A%20Temporal-Agent%20Reward%20Redistribution%20for%20Optimal%20Policy%0A%20%20Preservation%20in%20Multi-Agent%20Reinforcement%20Learning&body=Title%3A%20%24TAR%5E2%24%3A%20Temporal-Agent%20Reward%20Redistribution%20for%20Optimal%20Policy%0A%20%20Preservation%20in%20Multi-Agent%20Reinforcement%20Learning%0AAuthor%3A%20Aditya%20Kapoor%20and%20Kale-ab%20Tessera%20and%20Mayank%20Baranwal%20and%20Harshad%20Khadilkar%20and%20Stefano%20Albrecht%20and%20Mingfei%20Sun%0AAbstract%3A%20%20%20In%20cooperative%20multi-agent%20reinforcement%20learning%20%28MARL%29%2C%20learning%20effective%0Apolicies%20is%20challenging%20when%20global%20rewards%20are%20sparse%20and%20delayed.%20This%0Adifficulty%20arises%20from%20the%20need%20to%20assign%20credit%20across%20both%20agents%20and%20time%0Asteps%2C%20a%20problem%20that%20existing%20methods%20often%20fail%20to%20address%20in%20episodic%2C%0Along-horizon%20tasks.%20We%20propose%20Temporal-Agent%20Reward%20Redistribution%20%24TAR%5E2%24%2C%20a%0Anovel%20approach%20that%20decomposes%20sparse%20global%20rewards%20into%20agent-specific%2C%0Atime-step-specific%20components%2C%20thereby%20providing%20more%20frequent%20and%20accurate%0Afeedback%20for%20policy%20learning.%20Theoretically%2C%20we%20show%20that%20%24TAR%5E2%24%20%28i%29%20aligns%0Awith%20potential-based%20reward%20shaping%2C%20preserving%20the%20same%20optimal%20policies%20as%0Athe%20original%20environment%2C%20and%20%28ii%29%20maintains%20policy%20gradient%20update%20directions%0Aidentical%20to%20those%20under%20the%20original%20sparse%20reward%2C%20ensuring%20unbiased%20credit%0Asignals.%20Empirical%20results%20on%20two%20challenging%20benchmarks%2C%20SMACLite%20and%20Google%0AResearch%20Football%2C%20demonstrate%20that%20%24TAR%5E2%24%20significantly%20stabilizes%20and%0Aaccelerates%20convergence%2C%20outperforming%20strong%20baselines%20like%20AREL%20and%20STAS%20in%0Aboth%20learning%20speed%20and%20final%20performance.%20These%20findings%20establish%20%24TAR%5E2%24%20as%0Aa%20principled%20and%20practical%20solution%20for%20agent-temporal%20credit%20assignment%20in%0Asparse-reward%20multi-agent%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04864v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524TAR%255E2%2524%253A%2520Temporal-Agent%2520Reward%2520Redistribution%2520for%2520Optimal%2520Policy%250A%2520%2520Preservation%2520in%2520Multi-Agent%2520Reinforcement%2520Learning%26entry.906535625%3DAditya%2520Kapoor%2520and%2520Kale-ab%2520Tessera%2520and%2520Mayank%2520Baranwal%2520and%2520Harshad%2520Khadilkar%2520and%2520Stefano%2520Albrecht%2520and%2520Mingfei%2520Sun%26entry.1292438233%3D%2520%2520In%2520cooperative%2520multi-agent%2520reinforcement%2520learning%2520%2528MARL%2529%252C%2520learning%2520effective%250Apolicies%2520is%2520challenging%2520when%2520global%2520rewards%2520are%2520sparse%2520and%2520delayed.%2520This%250Adifficulty%2520arises%2520from%2520the%2520need%2520to%2520assign%2520credit%2520across%2520both%2520agents%2520and%2520time%250Asteps%252C%2520a%2520problem%2520that%2520existing%2520methods%2520often%2520fail%2520to%2520address%2520in%2520episodic%252C%250Along-horizon%2520tasks.%2520We%2520propose%2520Temporal-Agent%2520Reward%2520Redistribution%2520%2524TAR%255E2%2524%252C%2520a%250Anovel%2520approach%2520that%2520decomposes%2520sparse%2520global%2520rewards%2520into%2520agent-specific%252C%250Atime-step-specific%2520components%252C%2520thereby%2520providing%2520more%2520frequent%2520and%2520accurate%250Afeedback%2520for%2520policy%2520learning.%2520Theoretically%252C%2520we%2520show%2520that%2520%2524TAR%255E2%2524%2520%2528i%2529%2520aligns%250Awith%2520potential-based%2520reward%2520shaping%252C%2520preserving%2520the%2520same%2520optimal%2520policies%2520as%250Athe%2520original%2520environment%252C%2520and%2520%2528ii%2529%2520maintains%2520policy%2520gradient%2520update%2520directions%250Aidentical%2520to%2520those%2520under%2520the%2520original%2520sparse%2520reward%252C%2520ensuring%2520unbiased%2520credit%250Asignals.%2520Empirical%2520results%2520on%2520two%2520challenging%2520benchmarks%252C%2520SMACLite%2520and%2520Google%250AResearch%2520Football%252C%2520demonstrate%2520that%2520%2524TAR%255E2%2524%2520significantly%2520stabilizes%2520and%250Aaccelerates%2520convergence%252C%2520outperforming%2520strong%2520baselines%2520like%2520AREL%2520and%2520STAS%2520in%250Aboth%2520learning%2520speed%2520and%2520final%2520performance.%2520These%2520findings%2520establish%2520%2524TAR%255E2%2524%2520as%250Aa%2520principled%2520and%2520practical%2520solution%2520for%2520agent-temporal%2520credit%2520assignment%2520in%250Asparse-reward%2520multi-agent%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04864v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24TAR%5E2%24%3A%20Temporal-Agent%20Reward%20Redistribution%20for%20Optimal%20Policy%0A%20%20Preservation%20in%20Multi-Agent%20Reinforcement%20Learning&entry.906535625=Aditya%20Kapoor%20and%20Kale-ab%20Tessera%20and%20Mayank%20Baranwal%20and%20Harshad%20Khadilkar%20and%20Stefano%20Albrecht%20and%20Mingfei%20Sun&entry.1292438233=%20%20In%20cooperative%20multi-agent%20reinforcement%20learning%20%28MARL%29%2C%20learning%20effective%0Apolicies%20is%20challenging%20when%20global%20rewards%20are%20sparse%20and%20delayed.%20This%0Adifficulty%20arises%20from%20the%20need%20to%20assign%20credit%20across%20both%20agents%20and%20time%0Asteps%2C%20a%20problem%20that%20existing%20methods%20often%20fail%20to%20address%20in%20episodic%2C%0Along-horizon%20tasks.%20We%20propose%20Temporal-Agent%20Reward%20Redistribution%20%24TAR%5E2%24%2C%20a%0Anovel%20approach%20that%20decomposes%20sparse%20global%20rewards%20into%20agent-specific%2C%0Atime-step-specific%20components%2C%20thereby%20providing%20more%20frequent%20and%20accurate%0Afeedback%20for%20policy%20learning.%20Theoretically%2C%20we%20show%20that%20%24TAR%5E2%24%20%28i%29%20aligns%0Awith%20potential-based%20reward%20shaping%2C%20preserving%20the%20same%20optimal%20policies%20as%0Athe%20original%20environment%2C%20and%20%28ii%29%20maintains%20policy%20gradient%20update%20directions%0Aidentical%20to%20those%20under%20the%20original%20sparse%20reward%2C%20ensuring%20unbiased%20credit%0Asignals.%20Empirical%20results%20on%20two%20challenging%20benchmarks%2C%20SMACLite%20and%20Google%0AResearch%20Football%2C%20demonstrate%20that%20%24TAR%5E2%24%20significantly%20stabilizes%20and%0Aaccelerates%20convergence%2C%20outperforming%20strong%20baselines%20like%20AREL%20and%20STAS%20in%0Aboth%20learning%20speed%20and%20final%20performance.%20These%20findings%20establish%20%24TAR%5E2%24%20as%0Aa%20principled%20and%20practical%20solution%20for%20agent-temporal%20credit%20assignment%20in%0Asparse-reward%20multi-agent%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04864v1&entry.124074799=Read"},
{"title": "CASE-Bench: Context-Aware SafEty Benchmark for Large Language Models", "author": "Guangzhi Sun and Xiao Zhan and Shutong Feng and Philip C. Woodland and Jose Such", "abstract": "  Aligning large language models (LLMs) with human values is essential for\ntheir safe deployment and widespread adoption. Current LLM safety benchmarks\noften focus solely on the refusal of individual problematic queries, which\noverlooks the importance of the context where the query occurs and may cause\nundesired refusal of queries under safe contexts that diminish user experience.\nAddressing this gap, we introduce CASE-Bench, a Context-Aware SafEty Benchmark\nthat integrates context into safety assessments of LLMs. CASE-Bench assigns\ndistinct, formally described contexts to categorized queries based on\nContextual Integrity theory. Additionally, in contrast to previous studies\nwhich mainly rely on majority voting from just a few annotators, we recruited a\nsufficient number of annotators necessary to ensure the detection of\nstatistically significant differences among the experimental conditions based\non power analysis. Our extensive analysis using CASE-Bench on various\nopen-source and commercial LLMs reveals a substantial and significant influence\nof context on human judgments (p<0.0001 from a z-test), underscoring the\nnecessity of context in safety evaluations. We also identify notable mismatches\nbetween human judgments and LLM responses, particularly in commercial models\nwithin safe contexts.\n", "link": "http://arxiv.org/abs/2501.14940v3", "date": "2025-02-07", "relevancy": 1.9991, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5095}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4978}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CASE-Bench%3A%20Context-Aware%20SafEty%20Benchmark%20for%20Large%20Language%20Models&body=Title%3A%20CASE-Bench%3A%20Context-Aware%20SafEty%20Benchmark%20for%20Large%20Language%20Models%0AAuthor%3A%20Guangzhi%20Sun%20and%20Xiao%20Zhan%20and%20Shutong%20Feng%20and%20Philip%20C.%20Woodland%20and%20Jose%20Such%0AAbstract%3A%20%20%20Aligning%20large%20language%20models%20%28LLMs%29%20with%20human%20values%20is%20essential%20for%0Atheir%20safe%20deployment%20and%20widespread%20adoption.%20Current%20LLM%20safety%20benchmarks%0Aoften%20focus%20solely%20on%20the%20refusal%20of%20individual%20problematic%20queries%2C%20which%0Aoverlooks%20the%20importance%20of%20the%20context%20where%20the%20query%20occurs%20and%20may%20cause%0Aundesired%20refusal%20of%20queries%20under%20safe%20contexts%20that%20diminish%20user%20experience.%0AAddressing%20this%20gap%2C%20we%20introduce%20CASE-Bench%2C%20a%20Context-Aware%20SafEty%20Benchmark%0Athat%20integrates%20context%20into%20safety%20assessments%20of%20LLMs.%20CASE-Bench%20assigns%0Adistinct%2C%20formally%20described%20contexts%20to%20categorized%20queries%20based%20on%0AContextual%20Integrity%20theory.%20Additionally%2C%20in%20contrast%20to%20previous%20studies%0Awhich%20mainly%20rely%20on%20majority%20voting%20from%20just%20a%20few%20annotators%2C%20we%20recruited%20a%0Asufficient%20number%20of%20annotators%20necessary%20to%20ensure%20the%20detection%20of%0Astatistically%20significant%20differences%20among%20the%20experimental%20conditions%20based%0Aon%20power%20analysis.%20Our%20extensive%20analysis%20using%20CASE-Bench%20on%20various%0Aopen-source%20and%20commercial%20LLMs%20reveals%20a%20substantial%20and%20significant%20influence%0Aof%20context%20on%20human%20judgments%20%28p%3C0.0001%20from%20a%20z-test%29%2C%20underscoring%20the%0Anecessity%20of%20context%20in%20safety%20evaluations.%20We%20also%20identify%20notable%20mismatches%0Abetween%20human%20judgments%20and%20LLM%20responses%2C%20particularly%20in%20commercial%20models%0Awithin%20safe%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.14940v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCASE-Bench%253A%2520Context-Aware%2520SafEty%2520Benchmark%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DGuangzhi%2520Sun%2520and%2520Xiao%2520Zhan%2520and%2520Shutong%2520Feng%2520and%2520Philip%2520C.%2520Woodland%2520and%2520Jose%2520Such%26entry.1292438233%3D%2520%2520Aligning%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520human%2520values%2520is%2520essential%2520for%250Atheir%2520safe%2520deployment%2520and%2520widespread%2520adoption.%2520Current%2520LLM%2520safety%2520benchmarks%250Aoften%2520focus%2520solely%2520on%2520the%2520refusal%2520of%2520individual%2520problematic%2520queries%252C%2520which%250Aoverlooks%2520the%2520importance%2520of%2520the%2520context%2520where%2520the%2520query%2520occurs%2520and%2520may%2520cause%250Aundesired%2520refusal%2520of%2520queries%2520under%2520safe%2520contexts%2520that%2520diminish%2520user%2520experience.%250AAddressing%2520this%2520gap%252C%2520we%2520introduce%2520CASE-Bench%252C%2520a%2520Context-Aware%2520SafEty%2520Benchmark%250Athat%2520integrates%2520context%2520into%2520safety%2520assessments%2520of%2520LLMs.%2520CASE-Bench%2520assigns%250Adistinct%252C%2520formally%2520described%2520contexts%2520to%2520categorized%2520queries%2520based%2520on%250AContextual%2520Integrity%2520theory.%2520Additionally%252C%2520in%2520contrast%2520to%2520previous%2520studies%250Awhich%2520mainly%2520rely%2520on%2520majority%2520voting%2520from%2520just%2520a%2520few%2520annotators%252C%2520we%2520recruited%2520a%250Asufficient%2520number%2520of%2520annotators%2520necessary%2520to%2520ensure%2520the%2520detection%2520of%250Astatistically%2520significant%2520differences%2520among%2520the%2520experimental%2520conditions%2520based%250Aon%2520power%2520analysis.%2520Our%2520extensive%2520analysis%2520using%2520CASE-Bench%2520on%2520various%250Aopen-source%2520and%2520commercial%2520LLMs%2520reveals%2520a%2520substantial%2520and%2520significant%2520influence%250Aof%2520context%2520on%2520human%2520judgments%2520%2528p%253C0.0001%2520from%2520a%2520z-test%2529%252C%2520underscoring%2520the%250Anecessity%2520of%2520context%2520in%2520safety%2520evaluations.%2520We%2520also%2520identify%2520notable%2520mismatches%250Abetween%2520human%2520judgments%2520and%2520LLM%2520responses%252C%2520particularly%2520in%2520commercial%2520models%250Awithin%2520safe%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.14940v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CASE-Bench%3A%20Context-Aware%20SafEty%20Benchmark%20for%20Large%20Language%20Models&entry.906535625=Guangzhi%20Sun%20and%20Xiao%20Zhan%20and%20Shutong%20Feng%20and%20Philip%20C.%20Woodland%20and%20Jose%20Such&entry.1292438233=%20%20Aligning%20large%20language%20models%20%28LLMs%29%20with%20human%20values%20is%20essential%20for%0Atheir%20safe%20deployment%20and%20widespread%20adoption.%20Current%20LLM%20safety%20benchmarks%0Aoften%20focus%20solely%20on%20the%20refusal%20of%20individual%20problematic%20queries%2C%20which%0Aoverlooks%20the%20importance%20of%20the%20context%20where%20the%20query%20occurs%20and%20may%20cause%0Aundesired%20refusal%20of%20queries%20under%20safe%20contexts%20that%20diminish%20user%20experience.%0AAddressing%20this%20gap%2C%20we%20introduce%20CASE-Bench%2C%20a%20Context-Aware%20SafEty%20Benchmark%0Athat%20integrates%20context%20into%20safety%20assessments%20of%20LLMs.%20CASE-Bench%20assigns%0Adistinct%2C%20formally%20described%20contexts%20to%20categorized%20queries%20based%20on%0AContextual%20Integrity%20theory.%20Additionally%2C%20in%20contrast%20to%20previous%20studies%0Awhich%20mainly%20rely%20on%20majority%20voting%20from%20just%20a%20few%20annotators%2C%20we%20recruited%20a%0Asufficient%20number%20of%20annotators%20necessary%20to%20ensure%20the%20detection%20of%0Astatistically%20significant%20differences%20among%20the%20experimental%20conditions%20based%0Aon%20power%20analysis.%20Our%20extensive%20analysis%20using%20CASE-Bench%20on%20various%0Aopen-source%20and%20commercial%20LLMs%20reveals%20a%20substantial%20and%20significant%20influence%0Aof%20context%20on%20human%20judgments%20%28p%3C0.0001%20from%20a%20z-test%29%2C%20underscoring%20the%0Anecessity%20of%20context%20in%20safety%20evaluations.%20We%20also%20identify%20notable%20mismatches%0Abetween%20human%20judgments%20and%20LLM%20responses%2C%20particularly%20in%20commercial%20models%0Awithin%20safe%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.14940v3&entry.124074799=Read"},
{"title": "No Task Left Behind: Isotropic Model Merging with Common and\n  Task-Specific Subspaces", "author": "Daniel Marczak and Simone Magistri and Sebastian Cygert and Bart\u0142omiej Twardowski and Andrew D. Bagdanov and Joost van de Weijer", "abstract": "  Model merging integrates the weights of multiple task-specific models into a\nsingle multi-task model. Despite recent interest in the problem, a significant\nperformance gap between the combined and single-task models remains. In this\npaper, we investigate the key characteristics of task matrices -- weight update\nmatrices applied to a pre-trained model -- that enable effective merging. We\nshow that alignment between singular components of task-specific and merged\nmatrices strongly correlates with performance improvement over the pre-trained\nmodel. Based on this, we propose an isotropic merging framework that flattens\nthe singular value spectrum of task matrices, enhances alignment, and reduces\nthe performance gap. Additionally, we incorporate both common and task-specific\nsubspaces to further improve alignment and performance. Our proposed approach\nachieves state-of-the-art performance across multiple scenarios, including\nvarious sets of tasks and model scales. This work advances the understanding of\nmodel merging dynamics, offering an effective methodology to merge models\nwithout requiring additional training. Code is available at\nhttps://github.com/danielm1405/iso-merging .\n", "link": "http://arxiv.org/abs/2502.04959v1", "date": "2025-02-07", "relevancy": 1.9953, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5184}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4913}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20No%20Task%20Left%20Behind%3A%20Isotropic%20Model%20Merging%20with%20Common%20and%0A%20%20Task-Specific%20Subspaces&body=Title%3A%20No%20Task%20Left%20Behind%3A%20Isotropic%20Model%20Merging%20with%20Common%20and%0A%20%20Task-Specific%20Subspaces%0AAuthor%3A%20Daniel%20Marczak%20and%20Simone%20Magistri%20and%20Sebastian%20Cygert%20and%20Bart%C5%82omiej%20Twardowski%20and%20Andrew%20D.%20Bagdanov%20and%20Joost%20van%20de%20Weijer%0AAbstract%3A%20%20%20Model%20merging%20integrates%20the%20weights%20of%20multiple%20task-specific%20models%20into%20a%0Asingle%20multi-task%20model.%20Despite%20recent%20interest%20in%20the%20problem%2C%20a%20significant%0Aperformance%20gap%20between%20the%20combined%20and%20single-task%20models%20remains.%20In%20this%0Apaper%2C%20we%20investigate%20the%20key%20characteristics%20of%20task%20matrices%20--%20weight%20update%0Amatrices%20applied%20to%20a%20pre-trained%20model%20--%20that%20enable%20effective%20merging.%20We%0Ashow%20that%20alignment%20between%20singular%20components%20of%20task-specific%20and%20merged%0Amatrices%20strongly%20correlates%20with%20performance%20improvement%20over%20the%20pre-trained%0Amodel.%20Based%20on%20this%2C%20we%20propose%20an%20isotropic%20merging%20framework%20that%20flattens%0Athe%20singular%20value%20spectrum%20of%20task%20matrices%2C%20enhances%20alignment%2C%20and%20reduces%0Athe%20performance%20gap.%20Additionally%2C%20we%20incorporate%20both%20common%20and%20task-specific%0Asubspaces%20to%20further%20improve%20alignment%20and%20performance.%20Our%20proposed%20approach%0Aachieves%20state-of-the-art%20performance%20across%20multiple%20scenarios%2C%20including%0Avarious%20sets%20of%20tasks%20and%20model%20scales.%20This%20work%20advances%20the%20understanding%20of%0Amodel%20merging%20dynamics%2C%20offering%20an%20effective%20methodology%20to%20merge%20models%0Awithout%20requiring%20additional%20training.%20Code%20is%20available%20at%0Ahttps%3A//github.com/danielm1405/iso-merging%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04959v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNo%2520Task%2520Left%2520Behind%253A%2520Isotropic%2520Model%2520Merging%2520with%2520Common%2520and%250A%2520%2520Task-Specific%2520Subspaces%26entry.906535625%3DDaniel%2520Marczak%2520and%2520Simone%2520Magistri%2520and%2520Sebastian%2520Cygert%2520and%2520Bart%25C5%2582omiej%2520Twardowski%2520and%2520Andrew%2520D.%2520Bagdanov%2520and%2520Joost%2520van%2520de%2520Weijer%26entry.1292438233%3D%2520%2520Model%2520merging%2520integrates%2520the%2520weights%2520of%2520multiple%2520task-specific%2520models%2520into%2520a%250Asingle%2520multi-task%2520model.%2520Despite%2520recent%2520interest%2520in%2520the%2520problem%252C%2520a%2520significant%250Aperformance%2520gap%2520between%2520the%2520combined%2520and%2520single-task%2520models%2520remains.%2520In%2520this%250Apaper%252C%2520we%2520investigate%2520the%2520key%2520characteristics%2520of%2520task%2520matrices%2520--%2520weight%2520update%250Amatrices%2520applied%2520to%2520a%2520pre-trained%2520model%2520--%2520that%2520enable%2520effective%2520merging.%2520We%250Ashow%2520that%2520alignment%2520between%2520singular%2520components%2520of%2520task-specific%2520and%2520merged%250Amatrices%2520strongly%2520correlates%2520with%2520performance%2520improvement%2520over%2520the%2520pre-trained%250Amodel.%2520Based%2520on%2520this%252C%2520we%2520propose%2520an%2520isotropic%2520merging%2520framework%2520that%2520flattens%250Athe%2520singular%2520value%2520spectrum%2520of%2520task%2520matrices%252C%2520enhances%2520alignment%252C%2520and%2520reduces%250Athe%2520performance%2520gap.%2520Additionally%252C%2520we%2520incorporate%2520both%2520common%2520and%2520task-specific%250Asubspaces%2520to%2520further%2520improve%2520alignment%2520and%2520performance.%2520Our%2520proposed%2520approach%250Aachieves%2520state-of-the-art%2520performance%2520across%2520multiple%2520scenarios%252C%2520including%250Avarious%2520sets%2520of%2520tasks%2520and%2520model%2520scales.%2520This%2520work%2520advances%2520the%2520understanding%2520of%250Amodel%2520merging%2520dynamics%252C%2520offering%2520an%2520effective%2520methodology%2520to%2520merge%2520models%250Awithout%2520requiring%2520additional%2520training.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/danielm1405/iso-merging%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04959v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No%20Task%20Left%20Behind%3A%20Isotropic%20Model%20Merging%20with%20Common%20and%0A%20%20Task-Specific%20Subspaces&entry.906535625=Daniel%20Marczak%20and%20Simone%20Magistri%20and%20Sebastian%20Cygert%20and%20Bart%C5%82omiej%20Twardowski%20and%20Andrew%20D.%20Bagdanov%20and%20Joost%20van%20de%20Weijer&entry.1292438233=%20%20Model%20merging%20integrates%20the%20weights%20of%20multiple%20task-specific%20models%20into%20a%0Asingle%20multi-task%20model.%20Despite%20recent%20interest%20in%20the%20problem%2C%20a%20significant%0Aperformance%20gap%20between%20the%20combined%20and%20single-task%20models%20remains.%20In%20this%0Apaper%2C%20we%20investigate%20the%20key%20characteristics%20of%20task%20matrices%20--%20weight%20update%0Amatrices%20applied%20to%20a%20pre-trained%20model%20--%20that%20enable%20effective%20merging.%20We%0Ashow%20that%20alignment%20between%20singular%20components%20of%20task-specific%20and%20merged%0Amatrices%20strongly%20correlates%20with%20performance%20improvement%20over%20the%20pre-trained%0Amodel.%20Based%20on%20this%2C%20we%20propose%20an%20isotropic%20merging%20framework%20that%20flattens%0Athe%20singular%20value%20spectrum%20of%20task%20matrices%2C%20enhances%20alignment%2C%20and%20reduces%0Athe%20performance%20gap.%20Additionally%2C%20we%20incorporate%20both%20common%20and%20task-specific%0Asubspaces%20to%20further%20improve%20alignment%20and%20performance.%20Our%20proposed%20approach%0Aachieves%20state-of-the-art%20performance%20across%20multiple%20scenarios%2C%20including%0Avarious%20sets%20of%20tasks%20and%20model%20scales.%20This%20work%20advances%20the%20understanding%20of%0Amodel%20merging%20dynamics%2C%20offering%20an%20effective%20methodology%20to%20merge%20models%0Awithout%20requiring%20additional%20training.%20Code%20is%20available%20at%0Ahttps%3A//github.com/danielm1405/iso-merging%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04959v1&entry.124074799=Read"},
{"title": "Assigning Credit with Partial Reward Decoupling in Multi-Agent Proximal\n  Policy Optimization", "author": "Aditya Kapoor and Benjamin Freed and Howie Choset and Jeff Schneider", "abstract": "  Multi-agent proximal policy optimization (MAPPO) has recently demonstrated\nstate-of-the-art performance on challenging multi-agent reinforcement learning\ntasks. However, MAPPO still struggles with the credit assignment problem,\nwherein the sheer difficulty in ascribing credit to individual agents' actions\nscales poorly with team size. In this paper, we propose a multi-agent\nreinforcement learning algorithm that adapts recent developments in credit\nassignment to improve upon MAPPO. Our approach leverages partial reward\ndecoupling (PRD), which uses a learned attention mechanism to estimate which of\na particular agent's teammates are relevant to its learning updates. We use\nthis estimate to dynamically decompose large groups of agents into smaller,\nmore manageable subgroups. We empirically demonstrate that our approach,\nPRD-MAPPO, decouples agents from teammates that do not influence their expected\nfuture reward, thereby streamlining credit assignment. We additionally show\nthat PRD-MAPPO yields significantly higher data efficiency and asymptotic\nperformance compared to both MAPPO and other state-of-the-art methods across\nseveral multi-agent tasks, including StarCraft II. Finally, we propose a\nversion of PRD-MAPPO that is applicable to \\textit{shared} reward settings,\nwhere PRD was previously not applicable, and empirically show that this also\nleads to performance improvements over MAPPO.\n", "link": "http://arxiv.org/abs/2408.04295v3", "date": "2025-02-07", "relevancy": 1.9821, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5427}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4993}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assigning%20Credit%20with%20Partial%20Reward%20Decoupling%20in%20Multi-Agent%20Proximal%0A%20%20Policy%20Optimization&body=Title%3A%20Assigning%20Credit%20with%20Partial%20Reward%20Decoupling%20in%20Multi-Agent%20Proximal%0A%20%20Policy%20Optimization%0AAuthor%3A%20Aditya%20Kapoor%20and%20Benjamin%20Freed%20and%20Howie%20Choset%20and%20Jeff%20Schneider%0AAbstract%3A%20%20%20Multi-agent%20proximal%20policy%20optimization%20%28MAPPO%29%20has%20recently%20demonstrated%0Astate-of-the-art%20performance%20on%20challenging%20multi-agent%20reinforcement%20learning%0Atasks.%20However%2C%20MAPPO%20still%20struggles%20with%20the%20credit%20assignment%20problem%2C%0Awherein%20the%20sheer%20difficulty%20in%20ascribing%20credit%20to%20individual%20agents%27%20actions%0Ascales%20poorly%20with%20team%20size.%20In%20this%20paper%2C%20we%20propose%20a%20multi-agent%0Areinforcement%20learning%20algorithm%20that%20adapts%20recent%20developments%20in%20credit%0Aassignment%20to%20improve%20upon%20MAPPO.%20Our%20approach%20leverages%20partial%20reward%0Adecoupling%20%28PRD%29%2C%20which%20uses%20a%20learned%20attention%20mechanism%20to%20estimate%20which%20of%0Aa%20particular%20agent%27s%20teammates%20are%20relevant%20to%20its%20learning%20updates.%20We%20use%0Athis%20estimate%20to%20dynamically%20decompose%20large%20groups%20of%20agents%20into%20smaller%2C%0Amore%20manageable%20subgroups.%20We%20empirically%20demonstrate%20that%20our%20approach%2C%0APRD-MAPPO%2C%20decouples%20agents%20from%20teammates%20that%20do%20not%20influence%20their%20expected%0Afuture%20reward%2C%20thereby%20streamlining%20credit%20assignment.%20We%20additionally%20show%0Athat%20PRD-MAPPO%20yields%20significantly%20higher%20data%20efficiency%20and%20asymptotic%0Aperformance%20compared%20to%20both%20MAPPO%20and%20other%20state-of-the-art%20methods%20across%0Aseveral%20multi-agent%20tasks%2C%20including%20StarCraft%20II.%20Finally%2C%20we%20propose%20a%0Aversion%20of%20PRD-MAPPO%20that%20is%20applicable%20to%20%5Ctextit%7Bshared%7D%20reward%20settings%2C%0Awhere%20PRD%20was%20previously%20not%20applicable%2C%20and%20empirically%20show%20that%20this%20also%0Aleads%20to%20performance%20improvements%20over%20MAPPO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04295v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssigning%2520Credit%2520with%2520Partial%2520Reward%2520Decoupling%2520in%2520Multi-Agent%2520Proximal%250A%2520%2520Policy%2520Optimization%26entry.906535625%3DAditya%2520Kapoor%2520and%2520Benjamin%2520Freed%2520and%2520Howie%2520Choset%2520and%2520Jeff%2520Schneider%26entry.1292438233%3D%2520%2520Multi-agent%2520proximal%2520policy%2520optimization%2520%2528MAPPO%2529%2520has%2520recently%2520demonstrated%250Astate-of-the-art%2520performance%2520on%2520challenging%2520multi-agent%2520reinforcement%2520learning%250Atasks.%2520However%252C%2520MAPPO%2520still%2520struggles%2520with%2520the%2520credit%2520assignment%2520problem%252C%250Awherein%2520the%2520sheer%2520difficulty%2520in%2520ascribing%2520credit%2520to%2520individual%2520agents%2527%2520actions%250Ascales%2520poorly%2520with%2520team%2520size.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520multi-agent%250Areinforcement%2520learning%2520algorithm%2520that%2520adapts%2520recent%2520developments%2520in%2520credit%250Aassignment%2520to%2520improve%2520upon%2520MAPPO.%2520Our%2520approach%2520leverages%2520partial%2520reward%250Adecoupling%2520%2528PRD%2529%252C%2520which%2520uses%2520a%2520learned%2520attention%2520mechanism%2520to%2520estimate%2520which%2520of%250Aa%2520particular%2520agent%2527s%2520teammates%2520are%2520relevant%2520to%2520its%2520learning%2520updates.%2520We%2520use%250Athis%2520estimate%2520to%2520dynamically%2520decompose%2520large%2520groups%2520of%2520agents%2520into%2520smaller%252C%250Amore%2520manageable%2520subgroups.%2520We%2520empirically%2520demonstrate%2520that%2520our%2520approach%252C%250APRD-MAPPO%252C%2520decouples%2520agents%2520from%2520teammates%2520that%2520do%2520not%2520influence%2520their%2520expected%250Afuture%2520reward%252C%2520thereby%2520streamlining%2520credit%2520assignment.%2520We%2520additionally%2520show%250Athat%2520PRD-MAPPO%2520yields%2520significantly%2520higher%2520data%2520efficiency%2520and%2520asymptotic%250Aperformance%2520compared%2520to%2520both%2520MAPPO%2520and%2520other%2520state-of-the-art%2520methods%2520across%250Aseveral%2520multi-agent%2520tasks%252C%2520including%2520StarCraft%2520II.%2520Finally%252C%2520we%2520propose%2520a%250Aversion%2520of%2520PRD-MAPPO%2520that%2520is%2520applicable%2520to%2520%255Ctextit%257Bshared%257D%2520reward%2520settings%252C%250Awhere%2520PRD%2520was%2520previously%2520not%2520applicable%252C%2520and%2520empirically%2520show%2520that%2520this%2520also%250Aleads%2520to%2520performance%2520improvements%2520over%2520MAPPO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04295v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assigning%20Credit%20with%20Partial%20Reward%20Decoupling%20in%20Multi-Agent%20Proximal%0A%20%20Policy%20Optimization&entry.906535625=Aditya%20Kapoor%20and%20Benjamin%20Freed%20and%20Howie%20Choset%20and%20Jeff%20Schneider&entry.1292438233=%20%20Multi-agent%20proximal%20policy%20optimization%20%28MAPPO%29%20has%20recently%20demonstrated%0Astate-of-the-art%20performance%20on%20challenging%20multi-agent%20reinforcement%20learning%0Atasks.%20However%2C%20MAPPO%20still%20struggles%20with%20the%20credit%20assignment%20problem%2C%0Awherein%20the%20sheer%20difficulty%20in%20ascribing%20credit%20to%20individual%20agents%27%20actions%0Ascales%20poorly%20with%20team%20size.%20In%20this%20paper%2C%20we%20propose%20a%20multi-agent%0Areinforcement%20learning%20algorithm%20that%20adapts%20recent%20developments%20in%20credit%0Aassignment%20to%20improve%20upon%20MAPPO.%20Our%20approach%20leverages%20partial%20reward%0Adecoupling%20%28PRD%29%2C%20which%20uses%20a%20learned%20attention%20mechanism%20to%20estimate%20which%20of%0Aa%20particular%20agent%27s%20teammates%20are%20relevant%20to%20its%20learning%20updates.%20We%20use%0Athis%20estimate%20to%20dynamically%20decompose%20large%20groups%20of%20agents%20into%20smaller%2C%0Amore%20manageable%20subgroups.%20We%20empirically%20demonstrate%20that%20our%20approach%2C%0APRD-MAPPO%2C%20decouples%20agents%20from%20teammates%20that%20do%20not%20influence%20their%20expected%0Afuture%20reward%2C%20thereby%20streamlining%20credit%20assignment.%20We%20additionally%20show%0Athat%20PRD-MAPPO%20yields%20significantly%20higher%20data%20efficiency%20and%20asymptotic%0Aperformance%20compared%20to%20both%20MAPPO%20and%20other%20state-of-the-art%20methods%20across%0Aseveral%20multi-agent%20tasks%2C%20including%20StarCraft%20II.%20Finally%2C%20we%20propose%20a%0Aversion%20of%20PRD-MAPPO%20that%20is%20applicable%20to%20%5Ctextit%7Bshared%7D%20reward%20settings%2C%0Awhere%20PRD%20was%20previously%20not%20applicable%2C%20and%20empirically%20show%20that%20this%20also%0Aleads%20to%20performance%20improvements%20over%20MAPPO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04295v3&entry.124074799=Read"},
{"title": "Towards Smarter Sensing: 2D Clutter Mitigation in RL-Driven Cognitive\n  MIMO Radar", "author": "Adam Umra and Aya Mostafa Ahmed and Aydin Sezgin", "abstract": "  Motivated by the growing interest in integrated sensing and communication for\n6th generation (6G) networks, this paper presents a cognitive Multiple-Input\nMultiple-Output (MIMO) radar system enhanced by reinforcement learning (RL) for\nrobust multitarget detection in dynamic environments. The system employs a\nplanar array configuration and adapts its transmitted waveforms and beamforming\npatterns to optimize detection performance in the presence of unknown\ntwo-dimensional (2D) disturbances. A robust Wald-type detector is integrated\nwith a SARSA-based RL algorithm, enabling the radar to learn and adapt to\ncomplex clutter environments modeled by a 2D autoregressive process. Simulation\nresults demonstrate significant improvements in detection probability compared\nto omnidirectional methods, particularly for low Signal-to-Noise Ratio (SNR)\ntargets masked by clutter.\n", "link": "http://arxiv.org/abs/2502.04967v1", "date": "2025-02-07", "relevancy": 1.9818, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5077}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4938}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Smarter%20Sensing%3A%202D%20Clutter%20Mitigation%20in%20RL-Driven%20Cognitive%0A%20%20MIMO%20Radar&body=Title%3A%20Towards%20Smarter%20Sensing%3A%202D%20Clutter%20Mitigation%20in%20RL-Driven%20Cognitive%0A%20%20MIMO%20Radar%0AAuthor%3A%20Adam%20Umra%20and%20Aya%20Mostafa%20Ahmed%20and%20Aydin%20Sezgin%0AAbstract%3A%20%20%20Motivated%20by%20the%20growing%20interest%20in%20integrated%20sensing%20and%20communication%20for%0A6th%20generation%20%286G%29%20networks%2C%20this%20paper%20presents%20a%20cognitive%20Multiple-Input%0AMultiple-Output%20%28MIMO%29%20radar%20system%20enhanced%20by%20reinforcement%20learning%20%28RL%29%20for%0Arobust%20multitarget%20detection%20in%20dynamic%20environments.%20The%20system%20employs%20a%0Aplanar%20array%20configuration%20and%20adapts%20its%20transmitted%20waveforms%20and%20beamforming%0Apatterns%20to%20optimize%20detection%20performance%20in%20the%20presence%20of%20unknown%0Atwo-dimensional%20%282D%29%20disturbances.%20A%20robust%20Wald-type%20detector%20is%20integrated%0Awith%20a%20SARSA-based%20RL%20algorithm%2C%20enabling%20the%20radar%20to%20learn%20and%20adapt%20to%0Acomplex%20clutter%20environments%20modeled%20by%20a%202D%20autoregressive%20process.%20Simulation%0Aresults%20demonstrate%20significant%20improvements%20in%20detection%20probability%20compared%0Ato%20omnidirectional%20methods%2C%20particularly%20for%20low%20Signal-to-Noise%20Ratio%20%28SNR%29%0Atargets%20masked%20by%20clutter.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Smarter%2520Sensing%253A%25202D%2520Clutter%2520Mitigation%2520in%2520RL-Driven%2520Cognitive%250A%2520%2520MIMO%2520Radar%26entry.906535625%3DAdam%2520Umra%2520and%2520Aya%2520Mostafa%2520Ahmed%2520and%2520Aydin%2520Sezgin%26entry.1292438233%3D%2520%2520Motivated%2520by%2520the%2520growing%2520interest%2520in%2520integrated%2520sensing%2520and%2520communication%2520for%250A6th%2520generation%2520%25286G%2529%2520networks%252C%2520this%2520paper%2520presents%2520a%2520cognitive%2520Multiple-Input%250AMultiple-Output%2520%2528MIMO%2529%2520radar%2520system%2520enhanced%2520by%2520reinforcement%2520learning%2520%2528RL%2529%2520for%250Arobust%2520multitarget%2520detection%2520in%2520dynamic%2520environments.%2520The%2520system%2520employs%2520a%250Aplanar%2520array%2520configuration%2520and%2520adapts%2520its%2520transmitted%2520waveforms%2520and%2520beamforming%250Apatterns%2520to%2520optimize%2520detection%2520performance%2520in%2520the%2520presence%2520of%2520unknown%250Atwo-dimensional%2520%25282D%2529%2520disturbances.%2520A%2520robust%2520Wald-type%2520detector%2520is%2520integrated%250Awith%2520a%2520SARSA-based%2520RL%2520algorithm%252C%2520enabling%2520the%2520radar%2520to%2520learn%2520and%2520adapt%2520to%250Acomplex%2520clutter%2520environments%2520modeled%2520by%2520a%25202D%2520autoregressive%2520process.%2520Simulation%250Aresults%2520demonstrate%2520significant%2520improvements%2520in%2520detection%2520probability%2520compared%250Ato%2520omnidirectional%2520methods%252C%2520particularly%2520for%2520low%2520Signal-to-Noise%2520Ratio%2520%2528SNR%2529%250Atargets%2520masked%2520by%2520clutter.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Smarter%20Sensing%3A%202D%20Clutter%20Mitigation%20in%20RL-Driven%20Cognitive%0A%20%20MIMO%20Radar&entry.906535625=Adam%20Umra%20and%20Aya%20Mostafa%20Ahmed%20and%20Aydin%20Sezgin&entry.1292438233=%20%20Motivated%20by%20the%20growing%20interest%20in%20integrated%20sensing%20and%20communication%20for%0A6th%20generation%20%286G%29%20networks%2C%20this%20paper%20presents%20a%20cognitive%20Multiple-Input%0AMultiple-Output%20%28MIMO%29%20radar%20system%20enhanced%20by%20reinforcement%20learning%20%28RL%29%20for%0Arobust%20multitarget%20detection%20in%20dynamic%20environments.%20The%20system%20employs%20a%0Aplanar%20array%20configuration%20and%20adapts%20its%20transmitted%20waveforms%20and%20beamforming%0Apatterns%20to%20optimize%20detection%20performance%20in%20the%20presence%20of%20unknown%0Atwo-dimensional%20%282D%29%20disturbances.%20A%20robust%20Wald-type%20detector%20is%20integrated%0Awith%20a%20SARSA-based%20RL%20algorithm%2C%20enabling%20the%20radar%20to%20learn%20and%20adapt%20to%0Acomplex%20clutter%20environments%20modeled%20by%20a%202D%20autoregressive%20process.%20Simulation%0Aresults%20demonstrate%20significant%20improvements%20in%20detection%20probability%20compared%0Ato%20omnidirectional%20methods%2C%20particularly%20for%20low%20Signal-to-Noise%20Ratio%20%28SNR%29%0Atargets%20masked%20by%20clutter.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04967v1&entry.124074799=Read"},
{"title": "Meta Audiobox Aesthetics: Unified Automatic Quality Assessment for\n  Speech, Music, and Sound", "author": "Andros Tjandra and Yi-Chiao Wu and Baishan Guo and John Hoffman and Brian Ellis and Apoorv Vyas and Bowen Shi and Sanyuan Chen and Matt Le and Nick Zacharov and Carleigh Wood and Ann Lee and Wei-Ning Hsu", "abstract": "  The quantification of audio aesthetics remains a complex challenge in audio\nprocessing, primarily due to its subjective nature, which is influenced by\nhuman perception and cultural context. Traditional methods often depend on\nhuman listeners for evaluation, leading to inconsistencies and high resource\ndemands. This paper addresses the growing need for automated systems capable of\npredicting audio aesthetics without human intervention. Such systems are\ncrucial for applications like data filtering, pseudo-labeling large datasets,\nand evaluating generative audio models, especially as these models become more\nsophisticated. In this work, we introduce a novel approach to audio aesthetic\nevaluation by proposing new annotation guidelines that decompose human\nlistening perspectives into four distinct axes. We develop and train\nno-reference, per-item prediction models that offer a more nuanced assessment\nof audio quality. Our models are evaluated against human mean opinion scores\n(MOS) and existing methods, demonstrating comparable or superior performance.\nThis research not only advances the field of audio aesthetics but also provides\nopen-source models and datasets to facilitate future work and benchmarking. We\nrelease our code and pre-trained model at:\nhttps://github.com/facebookresearch/audiobox-aesthetics\n", "link": "http://arxiv.org/abs/2502.05139v1", "date": "2025-02-07", "relevancy": 1.4838, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5014}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.493}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meta%20Audiobox%20Aesthetics%3A%20Unified%20Automatic%20Quality%20Assessment%20for%0A%20%20Speech%2C%20Music%2C%20and%20Sound&body=Title%3A%20Meta%20Audiobox%20Aesthetics%3A%20Unified%20Automatic%20Quality%20Assessment%20for%0A%20%20Speech%2C%20Music%2C%20and%20Sound%0AAuthor%3A%20Andros%20Tjandra%20and%20Yi-Chiao%20Wu%20and%20Baishan%20Guo%20and%20John%20Hoffman%20and%20Brian%20Ellis%20and%20Apoorv%20Vyas%20and%20Bowen%20Shi%20and%20Sanyuan%20Chen%20and%20Matt%20Le%20and%20Nick%20Zacharov%20and%20Carleigh%20Wood%20and%20Ann%20Lee%20and%20Wei-Ning%20Hsu%0AAbstract%3A%20%20%20The%20quantification%20of%20audio%20aesthetics%20remains%20a%20complex%20challenge%20in%20audio%0Aprocessing%2C%20primarily%20due%20to%20its%20subjective%20nature%2C%20which%20is%20influenced%20by%0Ahuman%20perception%20and%20cultural%20context.%20Traditional%20methods%20often%20depend%20on%0Ahuman%20listeners%20for%20evaluation%2C%20leading%20to%20inconsistencies%20and%20high%20resource%0Ademands.%20This%20paper%20addresses%20the%20growing%20need%20for%20automated%20systems%20capable%20of%0Apredicting%20audio%20aesthetics%20without%20human%20intervention.%20Such%20systems%20are%0Acrucial%20for%20applications%20like%20data%20filtering%2C%20pseudo-labeling%20large%20datasets%2C%0Aand%20evaluating%20generative%20audio%20models%2C%20especially%20as%20these%20models%20become%20more%0Asophisticated.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20approach%20to%20audio%20aesthetic%0Aevaluation%20by%20proposing%20new%20annotation%20guidelines%20that%20decompose%20human%0Alistening%20perspectives%20into%20four%20distinct%20axes.%20We%20develop%20and%20train%0Ano-reference%2C%20per-item%20prediction%20models%20that%20offer%20a%20more%20nuanced%20assessment%0Aof%20audio%20quality.%20Our%20models%20are%20evaluated%20against%20human%20mean%20opinion%20scores%0A%28MOS%29%20and%20existing%20methods%2C%20demonstrating%20comparable%20or%20superior%20performance.%0AThis%20research%20not%20only%20advances%20the%20field%20of%20audio%20aesthetics%20but%20also%20provides%0Aopen-source%20models%20and%20datasets%20to%20facilitate%20future%20work%20and%20benchmarking.%20We%0Arelease%20our%20code%20and%20pre-trained%20model%20at%3A%0Ahttps%3A//github.com/facebookresearch/audiobox-aesthetics%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05139v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeta%2520Audiobox%2520Aesthetics%253A%2520Unified%2520Automatic%2520Quality%2520Assessment%2520for%250A%2520%2520Speech%252C%2520Music%252C%2520and%2520Sound%26entry.906535625%3DAndros%2520Tjandra%2520and%2520Yi-Chiao%2520Wu%2520and%2520Baishan%2520Guo%2520and%2520John%2520Hoffman%2520and%2520Brian%2520Ellis%2520and%2520Apoorv%2520Vyas%2520and%2520Bowen%2520Shi%2520and%2520Sanyuan%2520Chen%2520and%2520Matt%2520Le%2520and%2520Nick%2520Zacharov%2520and%2520Carleigh%2520Wood%2520and%2520Ann%2520Lee%2520and%2520Wei-Ning%2520Hsu%26entry.1292438233%3D%2520%2520The%2520quantification%2520of%2520audio%2520aesthetics%2520remains%2520a%2520complex%2520challenge%2520in%2520audio%250Aprocessing%252C%2520primarily%2520due%2520to%2520its%2520subjective%2520nature%252C%2520which%2520is%2520influenced%2520by%250Ahuman%2520perception%2520and%2520cultural%2520context.%2520Traditional%2520methods%2520often%2520depend%2520on%250Ahuman%2520listeners%2520for%2520evaluation%252C%2520leading%2520to%2520inconsistencies%2520and%2520high%2520resource%250Ademands.%2520This%2520paper%2520addresses%2520the%2520growing%2520need%2520for%2520automated%2520systems%2520capable%2520of%250Apredicting%2520audio%2520aesthetics%2520without%2520human%2520intervention.%2520Such%2520systems%2520are%250Acrucial%2520for%2520applications%2520like%2520data%2520filtering%252C%2520pseudo-labeling%2520large%2520datasets%252C%250Aand%2520evaluating%2520generative%2520audio%2520models%252C%2520especially%2520as%2520these%2520models%2520become%2520more%250Asophisticated.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520to%2520audio%2520aesthetic%250Aevaluation%2520by%2520proposing%2520new%2520annotation%2520guidelines%2520that%2520decompose%2520human%250Alistening%2520perspectives%2520into%2520four%2520distinct%2520axes.%2520We%2520develop%2520and%2520train%250Ano-reference%252C%2520per-item%2520prediction%2520models%2520that%2520offer%2520a%2520more%2520nuanced%2520assessment%250Aof%2520audio%2520quality.%2520Our%2520models%2520are%2520evaluated%2520against%2520human%2520mean%2520opinion%2520scores%250A%2528MOS%2529%2520and%2520existing%2520methods%252C%2520demonstrating%2520comparable%2520or%2520superior%2520performance.%250AThis%2520research%2520not%2520only%2520advances%2520the%2520field%2520of%2520audio%2520aesthetics%2520but%2520also%2520provides%250Aopen-source%2520models%2520and%2520datasets%2520to%2520facilitate%2520future%2520work%2520and%2520benchmarking.%2520We%250Arelease%2520our%2520code%2520and%2520pre-trained%2520model%2520at%253A%250Ahttps%253A//github.com/facebookresearch/audiobox-aesthetics%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05139v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta%20Audiobox%20Aesthetics%3A%20Unified%20Automatic%20Quality%20Assessment%20for%0A%20%20Speech%2C%20Music%2C%20and%20Sound&entry.906535625=Andros%20Tjandra%20and%20Yi-Chiao%20Wu%20and%20Baishan%20Guo%20and%20John%20Hoffman%20and%20Brian%20Ellis%20and%20Apoorv%20Vyas%20and%20Bowen%20Shi%20and%20Sanyuan%20Chen%20and%20Matt%20Le%20and%20Nick%20Zacharov%20and%20Carleigh%20Wood%20and%20Ann%20Lee%20and%20Wei-Ning%20Hsu&entry.1292438233=%20%20The%20quantification%20of%20audio%20aesthetics%20remains%20a%20complex%20challenge%20in%20audio%0Aprocessing%2C%20primarily%20due%20to%20its%20subjective%20nature%2C%20which%20is%20influenced%20by%0Ahuman%20perception%20and%20cultural%20context.%20Traditional%20methods%20often%20depend%20on%0Ahuman%20listeners%20for%20evaluation%2C%20leading%20to%20inconsistencies%20and%20high%20resource%0Ademands.%20This%20paper%20addresses%20the%20growing%20need%20for%20automated%20systems%20capable%20of%0Apredicting%20audio%20aesthetics%20without%20human%20intervention.%20Such%20systems%20are%0Acrucial%20for%20applications%20like%20data%20filtering%2C%20pseudo-labeling%20large%20datasets%2C%0Aand%20evaluating%20generative%20audio%20models%2C%20especially%20as%20these%20models%20become%20more%0Asophisticated.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20approach%20to%20audio%20aesthetic%0Aevaluation%20by%20proposing%20new%20annotation%20guidelines%20that%20decompose%20human%0Alistening%20perspectives%20into%20four%20distinct%20axes.%20We%20develop%20and%20train%0Ano-reference%2C%20per-item%20prediction%20models%20that%20offer%20a%20more%20nuanced%20assessment%0Aof%20audio%20quality.%20Our%20models%20are%20evaluated%20against%20human%20mean%20opinion%20scores%0A%28MOS%29%20and%20existing%20methods%2C%20demonstrating%20comparable%20or%20superior%20performance.%0AThis%20research%20not%20only%20advances%20the%20field%20of%20audio%20aesthetics%20but%20also%20provides%0Aopen-source%20models%20and%20datasets%20to%20facilitate%20future%20work%20and%20benchmarking.%20We%0Arelease%20our%20code%20and%20pre-trained%20model%20at%3A%0Ahttps%3A//github.com/facebookresearch/audiobox-aesthetics%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05139v1&entry.124074799=Read"},
{"title": "Explainable and externally validated machine learning for\n  neuropsychiatric diagnosis via electrocardiograms", "author": "Juan Miguel Lopez Alcaraz and Ebenezer Oloyede and David Taylor and Wilhelm Haverkamp and Nils Strodthoff", "abstract": "  Electrocardiogram (ECG) analysis has emerged as a promising tool for\nidentifying physiological changes associated with neuropsychiatric conditions.\nThe relationship between cardiovascular health and neuropsychiatric disorders\nsuggests that ECG abnormalities could serve as valuable biomarkers for more\nefficient detection, therapy monitoring, and risk stratification. However, the\npotential of the ECG to accurately distinguish neuropsychiatric conditions,\nparticularly among diverse patient populations, remains underexplored. This\nstudy utilized ECG markers and basic demographic data to predict\nneuropsychiatric conditions using machine learning models, with targets defined\nthrough ICD-10 codes. Both internal and external validation were performed\nusing the MIMIC-IV and ECG-View datasets respectively. Performance was assessed\nusing AUROC scores. To enhance model interpretability, Shapley values were\napplied to provide insights into the contributions of individual ECG features\nto the predictions. Significant predictive performance was observed for\nconditions within the neurological and psychiatric groups. For the neurological\ngroup, Alzheimer's disease (G30) achieved an internal AUROC of 0.813\n(0.812-0.814) and an external AUROC of 0.868 (0.867-0.868). In the psychiatric\ngroup, unspecified dementia (F03) showed an internal AUROC of 0.849\n(0.848-0.849) and an external AUROC of 0.862 (0.861-0.863). Discriminative\nfeatures align with known ECG markers but also provide hints on potentially new\nmarkers. ECG offers significant promise for diagnosing and monitoring\nneuropsychiatric conditions, with robust predictive performance across internal\nand external cohorts. Future work should focus on addressing potential\nconfounders, such as therapy-related cardiotoxicity, and expanding the scope of\nECG applications, including personalized care and early intervention\nstrategies.\n", "link": "http://arxiv.org/abs/2502.04918v1", "date": "2025-02-07", "relevancy": 1.2446, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4293}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4119}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explainable%20and%20externally%20validated%20machine%20learning%20for%0A%20%20neuropsychiatric%20diagnosis%20via%20electrocardiograms&body=Title%3A%20Explainable%20and%20externally%20validated%20machine%20learning%20for%0A%20%20neuropsychiatric%20diagnosis%20via%20electrocardiograms%0AAuthor%3A%20Juan%20Miguel%20Lopez%20Alcaraz%20and%20Ebenezer%20Oloyede%20and%20David%20Taylor%20and%20Wilhelm%20Haverkamp%20and%20Nils%20Strodthoff%0AAbstract%3A%20%20%20Electrocardiogram%20%28ECG%29%20analysis%20has%20emerged%20as%20a%20promising%20tool%20for%0Aidentifying%20physiological%20changes%20associated%20with%20neuropsychiatric%20conditions.%0AThe%20relationship%20between%20cardiovascular%20health%20and%20neuropsychiatric%20disorders%0Asuggests%20that%20ECG%20abnormalities%20could%20serve%20as%20valuable%20biomarkers%20for%20more%0Aefficient%20detection%2C%20therapy%20monitoring%2C%20and%20risk%20stratification.%20However%2C%20the%0Apotential%20of%20the%20ECG%20to%20accurately%20distinguish%20neuropsychiatric%20conditions%2C%0Aparticularly%20among%20diverse%20patient%20populations%2C%20remains%20underexplored.%20This%0Astudy%20utilized%20ECG%20markers%20and%20basic%20demographic%20data%20to%20predict%0Aneuropsychiatric%20conditions%20using%20machine%20learning%20models%2C%20with%20targets%20defined%0Athrough%20ICD-10%20codes.%20Both%20internal%20and%20external%20validation%20were%20performed%0Ausing%20the%20MIMIC-IV%20and%20ECG-View%20datasets%20respectively.%20Performance%20was%20assessed%0Ausing%20AUROC%20scores.%20To%20enhance%20model%20interpretability%2C%20Shapley%20values%20were%0Aapplied%20to%20provide%20insights%20into%20the%20contributions%20of%20individual%20ECG%20features%0Ato%20the%20predictions.%20Significant%20predictive%20performance%20was%20observed%20for%0Aconditions%20within%20the%20neurological%20and%20psychiatric%20groups.%20For%20the%20neurological%0Agroup%2C%20Alzheimer%27s%20disease%20%28G30%29%20achieved%20an%20internal%20AUROC%20of%200.813%0A%280.812-0.814%29%20and%20an%20external%20AUROC%20of%200.868%20%280.867-0.868%29.%20In%20the%20psychiatric%0Agroup%2C%20unspecified%20dementia%20%28F03%29%20showed%20an%20internal%20AUROC%20of%200.849%0A%280.848-0.849%29%20and%20an%20external%20AUROC%20of%200.862%20%280.861-0.863%29.%20Discriminative%0Afeatures%20align%20with%20known%20ECG%20markers%20but%20also%20provide%20hints%20on%20potentially%20new%0Amarkers.%20ECG%20offers%20significant%20promise%20for%20diagnosing%20and%20monitoring%0Aneuropsychiatric%20conditions%2C%20with%20robust%20predictive%20performance%20across%20internal%0Aand%20external%20cohorts.%20Future%20work%20should%20focus%20on%20addressing%20potential%0Aconfounders%2C%20such%20as%20therapy-related%20cardiotoxicity%2C%20and%20expanding%20the%20scope%20of%0AECG%20applications%2C%20including%20personalized%20care%20and%20early%20intervention%0Astrategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04918v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplainable%2520and%2520externally%2520validated%2520machine%2520learning%2520for%250A%2520%2520neuropsychiatric%2520diagnosis%2520via%2520electrocardiograms%26entry.906535625%3DJuan%2520Miguel%2520Lopez%2520Alcaraz%2520and%2520Ebenezer%2520Oloyede%2520and%2520David%2520Taylor%2520and%2520Wilhelm%2520Haverkamp%2520and%2520Nils%2520Strodthoff%26entry.1292438233%3D%2520%2520Electrocardiogram%2520%2528ECG%2529%2520analysis%2520has%2520emerged%2520as%2520a%2520promising%2520tool%2520for%250Aidentifying%2520physiological%2520changes%2520associated%2520with%2520neuropsychiatric%2520conditions.%250AThe%2520relationship%2520between%2520cardiovascular%2520health%2520and%2520neuropsychiatric%2520disorders%250Asuggests%2520that%2520ECG%2520abnormalities%2520could%2520serve%2520as%2520valuable%2520biomarkers%2520for%2520more%250Aefficient%2520detection%252C%2520therapy%2520monitoring%252C%2520and%2520risk%2520stratification.%2520However%252C%2520the%250Apotential%2520of%2520the%2520ECG%2520to%2520accurately%2520distinguish%2520neuropsychiatric%2520conditions%252C%250Aparticularly%2520among%2520diverse%2520patient%2520populations%252C%2520remains%2520underexplored.%2520This%250Astudy%2520utilized%2520ECG%2520markers%2520and%2520basic%2520demographic%2520data%2520to%2520predict%250Aneuropsychiatric%2520conditions%2520using%2520machine%2520learning%2520models%252C%2520with%2520targets%2520defined%250Athrough%2520ICD-10%2520codes.%2520Both%2520internal%2520and%2520external%2520validation%2520were%2520performed%250Ausing%2520the%2520MIMIC-IV%2520and%2520ECG-View%2520datasets%2520respectively.%2520Performance%2520was%2520assessed%250Ausing%2520AUROC%2520scores.%2520To%2520enhance%2520model%2520interpretability%252C%2520Shapley%2520values%2520were%250Aapplied%2520to%2520provide%2520insights%2520into%2520the%2520contributions%2520of%2520individual%2520ECG%2520features%250Ato%2520the%2520predictions.%2520Significant%2520predictive%2520performance%2520was%2520observed%2520for%250Aconditions%2520within%2520the%2520neurological%2520and%2520psychiatric%2520groups.%2520For%2520the%2520neurological%250Agroup%252C%2520Alzheimer%2527s%2520disease%2520%2528G30%2529%2520achieved%2520an%2520internal%2520AUROC%2520of%25200.813%250A%25280.812-0.814%2529%2520and%2520an%2520external%2520AUROC%2520of%25200.868%2520%25280.867-0.868%2529.%2520In%2520the%2520psychiatric%250Agroup%252C%2520unspecified%2520dementia%2520%2528F03%2529%2520showed%2520an%2520internal%2520AUROC%2520of%25200.849%250A%25280.848-0.849%2529%2520and%2520an%2520external%2520AUROC%2520of%25200.862%2520%25280.861-0.863%2529.%2520Discriminative%250Afeatures%2520align%2520with%2520known%2520ECG%2520markers%2520but%2520also%2520provide%2520hints%2520on%2520potentially%2520new%250Amarkers.%2520ECG%2520offers%2520significant%2520promise%2520for%2520diagnosing%2520and%2520monitoring%250Aneuropsychiatric%2520conditions%252C%2520with%2520robust%2520predictive%2520performance%2520across%2520internal%250Aand%2520external%2520cohorts.%2520Future%2520work%2520should%2520focus%2520on%2520addressing%2520potential%250Aconfounders%252C%2520such%2520as%2520therapy-related%2520cardiotoxicity%252C%2520and%2520expanding%2520the%2520scope%2520of%250AECG%2520applications%252C%2520including%2520personalized%2520care%2520and%2520early%2520intervention%250Astrategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04918v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainable%20and%20externally%20validated%20machine%20learning%20for%0A%20%20neuropsychiatric%20diagnosis%20via%20electrocardiograms&entry.906535625=Juan%20Miguel%20Lopez%20Alcaraz%20and%20Ebenezer%20Oloyede%20and%20David%20Taylor%20and%20Wilhelm%20Haverkamp%20and%20Nils%20Strodthoff&entry.1292438233=%20%20Electrocardiogram%20%28ECG%29%20analysis%20has%20emerged%20as%20a%20promising%20tool%20for%0Aidentifying%20physiological%20changes%20associated%20with%20neuropsychiatric%20conditions.%0AThe%20relationship%20between%20cardiovascular%20health%20and%20neuropsychiatric%20disorders%0Asuggests%20that%20ECG%20abnormalities%20could%20serve%20as%20valuable%20biomarkers%20for%20more%0Aefficient%20detection%2C%20therapy%20monitoring%2C%20and%20risk%20stratification.%20However%2C%20the%0Apotential%20of%20the%20ECG%20to%20accurately%20distinguish%20neuropsychiatric%20conditions%2C%0Aparticularly%20among%20diverse%20patient%20populations%2C%20remains%20underexplored.%20This%0Astudy%20utilized%20ECG%20markers%20and%20basic%20demographic%20data%20to%20predict%0Aneuropsychiatric%20conditions%20using%20machine%20learning%20models%2C%20with%20targets%20defined%0Athrough%20ICD-10%20codes.%20Both%20internal%20and%20external%20validation%20were%20performed%0Ausing%20the%20MIMIC-IV%20and%20ECG-View%20datasets%20respectively.%20Performance%20was%20assessed%0Ausing%20AUROC%20scores.%20To%20enhance%20model%20interpretability%2C%20Shapley%20values%20were%0Aapplied%20to%20provide%20insights%20into%20the%20contributions%20of%20individual%20ECG%20features%0Ato%20the%20predictions.%20Significant%20predictive%20performance%20was%20observed%20for%0Aconditions%20within%20the%20neurological%20and%20psychiatric%20groups.%20For%20the%20neurological%0Agroup%2C%20Alzheimer%27s%20disease%20%28G30%29%20achieved%20an%20internal%20AUROC%20of%200.813%0A%280.812-0.814%29%20and%20an%20external%20AUROC%20of%200.868%20%280.867-0.868%29.%20In%20the%20psychiatric%0Agroup%2C%20unspecified%20dementia%20%28F03%29%20showed%20an%20internal%20AUROC%20of%200.849%0A%280.848-0.849%29%20and%20an%20external%20AUROC%20of%200.862%20%280.861-0.863%29.%20Discriminative%0Afeatures%20align%20with%20known%20ECG%20markers%20but%20also%20provide%20hints%20on%20potentially%20new%0Amarkers.%20ECG%20offers%20significant%20promise%20for%20diagnosing%20and%20monitoring%0Aneuropsychiatric%20conditions%2C%20with%20robust%20predictive%20performance%20across%20internal%0Aand%20external%20cohorts.%20Future%20work%20should%20focus%20on%20addressing%20potential%0Aconfounders%2C%20such%20as%20therapy-related%20cardiotoxicity%2C%20and%20expanding%20the%20scope%20of%0AECG%20applications%2C%20including%20personalized%20care%20and%20early%20intervention%0Astrategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04918v1&entry.124074799=Read"},
{"title": "Measuring Variable Importance in Heterogeneous Treatment Effects with\n  Confidence", "author": "Joseph Paillard and Angel Reyero Lobo and Vitaliy Kolodyazhniy and Bertrand Thirion and Denis A. Engemann", "abstract": "  Causal machine learning (ML) holds promise for estimating individual\ntreatment effects from complex data. For successful real-world applications\nusing machine learning methods, it is of paramount importance to obtain\nreliable insights into which variables drive heterogeneity in the response to\ntreatment. We propose PermuCATE, an algorithm based on the Conditional\nPermutation Importance (CPI) method, for statistically rigorous global variable\nimportance assessment in the estimation of the Conditional Average Treatment\nEffect (CATE). Theoretical analysis of the finite sample regime and empirical\nstudies show that PermuCATE has lower variance than the Leave-One-Covariate-Out\n(LOCO) reference method and provides a reliable measure of variable importance.\nThis property increases statistical power, which is crucial for causal\ninference in the limited-data regime common to biomedical applications. We\nempirically demonstrate the benefits of PermuCATE in simulated and real-world\nhealth datasets, including settings with up to hundreds of correlated\nvariables.\n", "link": "http://arxiv.org/abs/2408.13002v2", "date": "2025-02-07", "relevancy": 1.3878, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4877}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4636}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Measuring%20Variable%20Importance%20in%20Heterogeneous%20Treatment%20Effects%20with%0A%20%20Confidence&body=Title%3A%20Measuring%20Variable%20Importance%20in%20Heterogeneous%20Treatment%20Effects%20with%0A%20%20Confidence%0AAuthor%3A%20Joseph%20Paillard%20and%20Angel%20Reyero%20Lobo%20and%20Vitaliy%20Kolodyazhniy%20and%20Bertrand%20Thirion%20and%20Denis%20A.%20Engemann%0AAbstract%3A%20%20%20Causal%20machine%20learning%20%28ML%29%20holds%20promise%20for%20estimating%20individual%0Atreatment%20effects%20from%20complex%20data.%20For%20successful%20real-world%20applications%0Ausing%20machine%20learning%20methods%2C%20it%20is%20of%20paramount%20importance%20to%20obtain%0Areliable%20insights%20into%20which%20variables%20drive%20heterogeneity%20in%20the%20response%20to%0Atreatment.%20We%20propose%20PermuCATE%2C%20an%20algorithm%20based%20on%20the%20Conditional%0APermutation%20Importance%20%28CPI%29%20method%2C%20for%20statistically%20rigorous%20global%20variable%0Aimportance%20assessment%20in%20the%20estimation%20of%20the%20Conditional%20Average%20Treatment%0AEffect%20%28CATE%29.%20Theoretical%20analysis%20of%20the%20finite%20sample%20regime%20and%20empirical%0Astudies%20show%20that%20PermuCATE%20has%20lower%20variance%20than%20the%20Leave-One-Covariate-Out%0A%28LOCO%29%20reference%20method%20and%20provides%20a%20reliable%20measure%20of%20variable%20importance.%0AThis%20property%20increases%20statistical%20power%2C%20which%20is%20crucial%20for%20causal%0Ainference%20in%20the%20limited-data%20regime%20common%20to%20biomedical%20applications.%20We%0Aempirically%20demonstrate%20the%20benefits%20of%20PermuCATE%20in%20simulated%20and%20real-world%0Ahealth%20datasets%2C%20including%20settings%20with%20up%20to%20hundreds%20of%20correlated%0Avariables.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13002v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeasuring%2520Variable%2520Importance%2520in%2520Heterogeneous%2520Treatment%2520Effects%2520with%250A%2520%2520Confidence%26entry.906535625%3DJoseph%2520Paillard%2520and%2520Angel%2520Reyero%2520Lobo%2520and%2520Vitaliy%2520Kolodyazhniy%2520and%2520Bertrand%2520Thirion%2520and%2520Denis%2520A.%2520Engemann%26entry.1292438233%3D%2520%2520Causal%2520machine%2520learning%2520%2528ML%2529%2520holds%2520promise%2520for%2520estimating%2520individual%250Atreatment%2520effects%2520from%2520complex%2520data.%2520For%2520successful%2520real-world%2520applications%250Ausing%2520machine%2520learning%2520methods%252C%2520it%2520is%2520of%2520paramount%2520importance%2520to%2520obtain%250Areliable%2520insights%2520into%2520which%2520variables%2520drive%2520heterogeneity%2520in%2520the%2520response%2520to%250Atreatment.%2520We%2520propose%2520PermuCATE%252C%2520an%2520algorithm%2520based%2520on%2520the%2520Conditional%250APermutation%2520Importance%2520%2528CPI%2529%2520method%252C%2520for%2520statistically%2520rigorous%2520global%2520variable%250Aimportance%2520assessment%2520in%2520the%2520estimation%2520of%2520the%2520Conditional%2520Average%2520Treatment%250AEffect%2520%2528CATE%2529.%2520Theoretical%2520analysis%2520of%2520the%2520finite%2520sample%2520regime%2520and%2520empirical%250Astudies%2520show%2520that%2520PermuCATE%2520has%2520lower%2520variance%2520than%2520the%2520Leave-One-Covariate-Out%250A%2528LOCO%2529%2520reference%2520method%2520and%2520provides%2520a%2520reliable%2520measure%2520of%2520variable%2520importance.%250AThis%2520property%2520increases%2520statistical%2520power%252C%2520which%2520is%2520crucial%2520for%2520causal%250Ainference%2520in%2520the%2520limited-data%2520regime%2520common%2520to%2520biomedical%2520applications.%2520We%250Aempirically%2520demonstrate%2520the%2520benefits%2520of%2520PermuCATE%2520in%2520simulated%2520and%2520real-world%250Ahealth%2520datasets%252C%2520including%2520settings%2520with%2520up%2520to%2520hundreds%2520of%2520correlated%250Avariables.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13002v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Measuring%20Variable%20Importance%20in%20Heterogeneous%20Treatment%20Effects%20with%0A%20%20Confidence&entry.906535625=Joseph%20Paillard%20and%20Angel%20Reyero%20Lobo%20and%20Vitaliy%20Kolodyazhniy%20and%20Bertrand%20Thirion%20and%20Denis%20A.%20Engemann&entry.1292438233=%20%20Causal%20machine%20learning%20%28ML%29%20holds%20promise%20for%20estimating%20individual%0Atreatment%20effects%20from%20complex%20data.%20For%20successful%20real-world%20applications%0Ausing%20machine%20learning%20methods%2C%20it%20is%20of%20paramount%20importance%20to%20obtain%0Areliable%20insights%20into%20which%20variables%20drive%20heterogeneity%20in%20the%20response%20to%0Atreatment.%20We%20propose%20PermuCATE%2C%20an%20algorithm%20based%20on%20the%20Conditional%0APermutation%20Importance%20%28CPI%29%20method%2C%20for%20statistically%20rigorous%20global%20variable%0Aimportance%20assessment%20in%20the%20estimation%20of%20the%20Conditional%20Average%20Treatment%0AEffect%20%28CATE%29.%20Theoretical%20analysis%20of%20the%20finite%20sample%20regime%20and%20empirical%0Astudies%20show%20that%20PermuCATE%20has%20lower%20variance%20than%20the%20Leave-One-Covariate-Out%0A%28LOCO%29%20reference%20method%20and%20provides%20a%20reliable%20measure%20of%20variable%20importance.%0AThis%20property%20increases%20statistical%20power%2C%20which%20is%20crucial%20for%20causal%0Ainference%20in%20the%20limited-data%20regime%20common%20to%20biomedical%20applications.%20We%0Aempirically%20demonstrate%20the%20benefits%20of%20PermuCATE%20in%20simulated%20and%20real-world%0Ahealth%20datasets%2C%20including%20settings%20with%20up%20to%20hundreds%20of%20correlated%0Avariables.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13002v2&entry.124074799=Read"},
{"title": "Leveraging Hypernetworks and Learnable Kernels for Consumer Energy\n  Forecasting Across Diverse Consumer Types", "author": "Muhammad Umair Danish and Katarina Grolinger", "abstract": "  Consumer energy forecasting is essential for managing energy consumption and\nplanning, directly influencing operational efficiency, cost reduction,\npersonalized energy management, and sustainability efforts. In recent years,\ndeep learning techniques, especially LSTMs and transformers, have been greatly\nsuccessful in the field of energy consumption forecasting. Nevertheless, these\ntechniques have difficulties in capturing complex and sudden variations, and,\nmoreover, they are commonly examined only on a specific type of consumer (e.g.,\nonly offices, only schools). Consequently, this paper proposes HyperEnergy, a\nconsumer energy forecasting strategy that leverages hypernetworks for improved\nmodeling of complex patterns applicable across a diversity of consumers.\nHypernetwork is responsible for predicting the parameters of the primary\nprediction network, in our case LSTM. A learnable adaptable kernel, comprised\nof polynomial and radial basis function kernels, is incorporated to enhance\nperformance. The proposed HyperEnergy was evaluated on diverse consumers\nincluding, student residences, detached homes, a home with electric vehicle\ncharging, and a townhouse. Across all consumer types, HyperEnergy consistently\noutperformed 10 other techniques, including state-of-the-art models such as\nLSTM, AttentionLSTM, and transformer.\n", "link": "http://arxiv.org/abs/2502.05104v1", "date": "2025-02-07", "relevancy": 1.8896, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.498}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4614}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Hypernetworks%20and%20Learnable%20Kernels%20for%20Consumer%20Energy%0A%20%20Forecasting%20Across%20Diverse%20Consumer%20Types&body=Title%3A%20Leveraging%20Hypernetworks%20and%20Learnable%20Kernels%20for%20Consumer%20Energy%0A%20%20Forecasting%20Across%20Diverse%20Consumer%20Types%0AAuthor%3A%20Muhammad%20Umair%20Danish%20and%20Katarina%20Grolinger%0AAbstract%3A%20%20%20Consumer%20energy%20forecasting%20is%20essential%20for%20managing%20energy%20consumption%20and%0Aplanning%2C%20directly%20influencing%20operational%20efficiency%2C%20cost%20reduction%2C%0Apersonalized%20energy%20management%2C%20and%20sustainability%20efforts.%20In%20recent%20years%2C%0Adeep%20learning%20techniques%2C%20especially%20LSTMs%20and%20transformers%2C%20have%20been%20greatly%0Asuccessful%20in%20the%20field%20of%20energy%20consumption%20forecasting.%20Nevertheless%2C%20these%0Atechniques%20have%20difficulties%20in%20capturing%20complex%20and%20sudden%20variations%2C%20and%2C%0Amoreover%2C%20they%20are%20commonly%20examined%20only%20on%20a%20specific%20type%20of%20consumer%20%28e.g.%2C%0Aonly%20offices%2C%20only%20schools%29.%20Consequently%2C%20this%20paper%20proposes%20HyperEnergy%2C%20a%0Aconsumer%20energy%20forecasting%20strategy%20that%20leverages%20hypernetworks%20for%20improved%0Amodeling%20of%20complex%20patterns%20applicable%20across%20a%20diversity%20of%20consumers.%0AHypernetwork%20is%20responsible%20for%20predicting%20the%20parameters%20of%20the%20primary%0Aprediction%20network%2C%20in%20our%20case%20LSTM.%20A%20learnable%20adaptable%20kernel%2C%20comprised%0Aof%20polynomial%20and%20radial%20basis%20function%20kernels%2C%20is%20incorporated%20to%20enhance%0Aperformance.%20The%20proposed%20HyperEnergy%20was%20evaluated%20on%20diverse%20consumers%0Aincluding%2C%20student%20residences%2C%20detached%20homes%2C%20a%20home%20with%20electric%20vehicle%0Acharging%2C%20and%20a%20townhouse.%20Across%20all%20consumer%20types%2C%20HyperEnergy%20consistently%0Aoutperformed%2010%20other%20techniques%2C%20including%20state-of-the-art%20models%20such%20as%0ALSTM%2C%20AttentionLSTM%2C%20and%20transformer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05104v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Hypernetworks%2520and%2520Learnable%2520Kernels%2520for%2520Consumer%2520Energy%250A%2520%2520Forecasting%2520Across%2520Diverse%2520Consumer%2520Types%26entry.906535625%3DMuhammad%2520Umair%2520Danish%2520and%2520Katarina%2520Grolinger%26entry.1292438233%3D%2520%2520Consumer%2520energy%2520forecasting%2520is%2520essential%2520for%2520managing%2520energy%2520consumption%2520and%250Aplanning%252C%2520directly%2520influencing%2520operational%2520efficiency%252C%2520cost%2520reduction%252C%250Apersonalized%2520energy%2520management%252C%2520and%2520sustainability%2520efforts.%2520In%2520recent%2520years%252C%250Adeep%2520learning%2520techniques%252C%2520especially%2520LSTMs%2520and%2520transformers%252C%2520have%2520been%2520greatly%250Asuccessful%2520in%2520the%2520field%2520of%2520energy%2520consumption%2520forecasting.%2520Nevertheless%252C%2520these%250Atechniques%2520have%2520difficulties%2520in%2520capturing%2520complex%2520and%2520sudden%2520variations%252C%2520and%252C%250Amoreover%252C%2520they%2520are%2520commonly%2520examined%2520only%2520on%2520a%2520specific%2520type%2520of%2520consumer%2520%2528e.g.%252C%250Aonly%2520offices%252C%2520only%2520schools%2529.%2520Consequently%252C%2520this%2520paper%2520proposes%2520HyperEnergy%252C%2520a%250Aconsumer%2520energy%2520forecasting%2520strategy%2520that%2520leverages%2520hypernetworks%2520for%2520improved%250Amodeling%2520of%2520complex%2520patterns%2520applicable%2520across%2520a%2520diversity%2520of%2520consumers.%250AHypernetwork%2520is%2520responsible%2520for%2520predicting%2520the%2520parameters%2520of%2520the%2520primary%250Aprediction%2520network%252C%2520in%2520our%2520case%2520LSTM.%2520A%2520learnable%2520adaptable%2520kernel%252C%2520comprised%250Aof%2520polynomial%2520and%2520radial%2520basis%2520function%2520kernels%252C%2520is%2520incorporated%2520to%2520enhance%250Aperformance.%2520The%2520proposed%2520HyperEnergy%2520was%2520evaluated%2520on%2520diverse%2520consumers%250Aincluding%252C%2520student%2520residences%252C%2520detached%2520homes%252C%2520a%2520home%2520with%2520electric%2520vehicle%250Acharging%252C%2520and%2520a%2520townhouse.%2520Across%2520all%2520consumer%2520types%252C%2520HyperEnergy%2520consistently%250Aoutperformed%252010%2520other%2520techniques%252C%2520including%2520state-of-the-art%2520models%2520such%2520as%250ALSTM%252C%2520AttentionLSTM%252C%2520and%2520transformer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05104v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Hypernetworks%20and%20Learnable%20Kernels%20for%20Consumer%20Energy%0A%20%20Forecasting%20Across%20Diverse%20Consumer%20Types&entry.906535625=Muhammad%20Umair%20Danish%20and%20Katarina%20Grolinger&entry.1292438233=%20%20Consumer%20energy%20forecasting%20is%20essential%20for%20managing%20energy%20consumption%20and%0Aplanning%2C%20directly%20influencing%20operational%20efficiency%2C%20cost%20reduction%2C%0Apersonalized%20energy%20management%2C%20and%20sustainability%20efforts.%20In%20recent%20years%2C%0Adeep%20learning%20techniques%2C%20especially%20LSTMs%20and%20transformers%2C%20have%20been%20greatly%0Asuccessful%20in%20the%20field%20of%20energy%20consumption%20forecasting.%20Nevertheless%2C%20these%0Atechniques%20have%20difficulties%20in%20capturing%20complex%20and%20sudden%20variations%2C%20and%2C%0Amoreover%2C%20they%20are%20commonly%20examined%20only%20on%20a%20specific%20type%20of%20consumer%20%28e.g.%2C%0Aonly%20offices%2C%20only%20schools%29.%20Consequently%2C%20this%20paper%20proposes%20HyperEnergy%2C%20a%0Aconsumer%20energy%20forecasting%20strategy%20that%20leverages%20hypernetworks%20for%20improved%0Amodeling%20of%20complex%20patterns%20applicable%20across%20a%20diversity%20of%20consumers.%0AHypernetwork%20is%20responsible%20for%20predicting%20the%20parameters%20of%20the%20primary%0Aprediction%20network%2C%20in%20our%20case%20LSTM.%20A%20learnable%20adaptable%20kernel%2C%20comprised%0Aof%20polynomial%20and%20radial%20basis%20function%20kernels%2C%20is%20incorporated%20to%20enhance%0Aperformance.%20The%20proposed%20HyperEnergy%20was%20evaluated%20on%20diverse%20consumers%0Aincluding%2C%20student%20residences%2C%20detached%20homes%2C%20a%20home%20with%20electric%20vehicle%0Acharging%2C%20and%20a%20townhouse.%20Across%20all%20consumer%20types%2C%20HyperEnergy%20consistently%0Aoutperformed%2010%20other%20techniques%2C%20including%20state-of-the-art%20models%20such%20as%0ALSTM%2C%20AttentionLSTM%2C%20and%20transformer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05104v1&entry.124074799=Read"},
{"title": "PoI: Pixel of Interest for Novel View Synthesis Assisted Scene\n  Coordinate Regression", "author": "Feifei Li and Qi Song and Chi Zhang and Hui Shuai and Rui Huang", "abstract": "  The task of estimating camera poses can be enhanced through novel view\nsynthesis techniques such as NeRF and Gaussian Splatting to increase the\ndiversity and extension of training data. However, these techniques often\nproduce rendered images with issues like blurring and ghosting, which\ncompromise their reliability. These issues become particularly pronounced for\nScene Coordinate Regression (SCR) methods, which estimate 3D coordinates at the\npixel level. To mitigate the problems associated with unreliable rendered\nimages, we introduce a novel filtering approach, which selectively extracts\nwell-rendered pixels while discarding the inferior ones. This filter\nsimultaneously measures the SCR model's real-time reprojection loss and\ngradient during training. Building on this filtering technique, we also develop\na new strategy to improve scene coordinate regression using sparse inputs,\ndrawing on successful applications of sparse input techniques in novel view\nsynthesis. Our experimental results validate the effectiveness of our method,\ndemonstrating state-of-the-art performance on indoor and outdoor datasets.\n", "link": "http://arxiv.org/abs/2502.04843v1", "date": "2025-02-07", "relevancy": 1.8477, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.625}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6164}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PoI%3A%20Pixel%20of%20Interest%20for%20Novel%20View%20Synthesis%20Assisted%20Scene%0A%20%20Coordinate%20Regression&body=Title%3A%20PoI%3A%20Pixel%20of%20Interest%20for%20Novel%20View%20Synthesis%20Assisted%20Scene%0A%20%20Coordinate%20Regression%0AAuthor%3A%20Feifei%20Li%20and%20Qi%20Song%20and%20Chi%20Zhang%20and%20Hui%20Shuai%20and%20Rui%20Huang%0AAbstract%3A%20%20%20The%20task%20of%20estimating%20camera%20poses%20can%20be%20enhanced%20through%20novel%20view%0Asynthesis%20techniques%20such%20as%20NeRF%20and%20Gaussian%20Splatting%20to%20increase%20the%0Adiversity%20and%20extension%20of%20training%20data.%20However%2C%20these%20techniques%20often%0Aproduce%20rendered%20images%20with%20issues%20like%20blurring%20and%20ghosting%2C%20which%0Acompromise%20their%20reliability.%20These%20issues%20become%20particularly%20pronounced%20for%0AScene%20Coordinate%20Regression%20%28SCR%29%20methods%2C%20which%20estimate%203D%20coordinates%20at%20the%0Apixel%20level.%20To%20mitigate%20the%20problems%20associated%20with%20unreliable%20rendered%0Aimages%2C%20we%20introduce%20a%20novel%20filtering%20approach%2C%20which%20selectively%20extracts%0Awell-rendered%20pixels%20while%20discarding%20the%20inferior%20ones.%20This%20filter%0Asimultaneously%20measures%20the%20SCR%20model%27s%20real-time%20reprojection%20loss%20and%0Agradient%20during%20training.%20Building%20on%20this%20filtering%20technique%2C%20we%20also%20develop%0Aa%20new%20strategy%20to%20improve%20scene%20coordinate%20regression%20using%20sparse%20inputs%2C%0Adrawing%20on%20successful%20applications%20of%20sparse%20input%20techniques%20in%20novel%20view%0Asynthesis.%20Our%20experimental%20results%20validate%20the%20effectiveness%20of%20our%20method%2C%0Ademonstrating%20state-of-the-art%20performance%20on%20indoor%20and%20outdoor%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04843v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoI%253A%2520Pixel%2520of%2520Interest%2520for%2520Novel%2520View%2520Synthesis%2520Assisted%2520Scene%250A%2520%2520Coordinate%2520Regression%26entry.906535625%3DFeifei%2520Li%2520and%2520Qi%2520Song%2520and%2520Chi%2520Zhang%2520and%2520Hui%2520Shuai%2520and%2520Rui%2520Huang%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520estimating%2520camera%2520poses%2520can%2520be%2520enhanced%2520through%2520novel%2520view%250Asynthesis%2520techniques%2520such%2520as%2520NeRF%2520and%2520Gaussian%2520Splatting%2520to%2520increase%2520the%250Adiversity%2520and%2520extension%2520of%2520training%2520data.%2520However%252C%2520these%2520techniques%2520often%250Aproduce%2520rendered%2520images%2520with%2520issues%2520like%2520blurring%2520and%2520ghosting%252C%2520which%250Acompromise%2520their%2520reliability.%2520These%2520issues%2520become%2520particularly%2520pronounced%2520for%250AScene%2520Coordinate%2520Regression%2520%2528SCR%2529%2520methods%252C%2520which%2520estimate%25203D%2520coordinates%2520at%2520the%250Apixel%2520level.%2520To%2520mitigate%2520the%2520problems%2520associated%2520with%2520unreliable%2520rendered%250Aimages%252C%2520we%2520introduce%2520a%2520novel%2520filtering%2520approach%252C%2520which%2520selectively%2520extracts%250Awell-rendered%2520pixels%2520while%2520discarding%2520the%2520inferior%2520ones.%2520This%2520filter%250Asimultaneously%2520measures%2520the%2520SCR%2520model%2527s%2520real-time%2520reprojection%2520loss%2520and%250Agradient%2520during%2520training.%2520Building%2520on%2520this%2520filtering%2520technique%252C%2520we%2520also%2520develop%250Aa%2520new%2520strategy%2520to%2520improve%2520scene%2520coordinate%2520regression%2520using%2520sparse%2520inputs%252C%250Adrawing%2520on%2520successful%2520applications%2520of%2520sparse%2520input%2520techniques%2520in%2520novel%2520view%250Asynthesis.%2520Our%2520experimental%2520results%2520validate%2520the%2520effectiveness%2520of%2520our%2520method%252C%250Ademonstrating%2520state-of-the-art%2520performance%2520on%2520indoor%2520and%2520outdoor%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04843v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PoI%3A%20Pixel%20of%20Interest%20for%20Novel%20View%20Synthesis%20Assisted%20Scene%0A%20%20Coordinate%20Regression&entry.906535625=Feifei%20Li%20and%20Qi%20Song%20and%20Chi%20Zhang%20and%20Hui%20Shuai%20and%20Rui%20Huang&entry.1292438233=%20%20The%20task%20of%20estimating%20camera%20poses%20can%20be%20enhanced%20through%20novel%20view%0Asynthesis%20techniques%20such%20as%20NeRF%20and%20Gaussian%20Splatting%20to%20increase%20the%0Adiversity%20and%20extension%20of%20training%20data.%20However%2C%20these%20techniques%20often%0Aproduce%20rendered%20images%20with%20issues%20like%20blurring%20and%20ghosting%2C%20which%0Acompromise%20their%20reliability.%20These%20issues%20become%20particularly%20pronounced%20for%0AScene%20Coordinate%20Regression%20%28SCR%29%20methods%2C%20which%20estimate%203D%20coordinates%20at%20the%0Apixel%20level.%20To%20mitigate%20the%20problems%20associated%20with%20unreliable%20rendered%0Aimages%2C%20we%20introduce%20a%20novel%20filtering%20approach%2C%20which%20selectively%20extracts%0Awell-rendered%20pixels%20while%20discarding%20the%20inferior%20ones.%20This%20filter%0Asimultaneously%20measures%20the%20SCR%20model%27s%20real-time%20reprojection%20loss%20and%0Agradient%20during%20training.%20Building%20on%20this%20filtering%20technique%2C%20we%20also%20develop%0Aa%20new%20strategy%20to%20improve%20scene%20coordinate%20regression%20using%20sparse%20inputs%2C%0Adrawing%20on%20successful%20applications%20of%20sparse%20input%20techniques%20in%20novel%20view%0Asynthesis.%20Our%20experimental%20results%20validate%20the%20effectiveness%20of%20our%20method%2C%0Ademonstrating%20state-of-the-art%20performance%20on%20indoor%20and%20outdoor%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04843v1&entry.124074799=Read"},
{"title": "Federated Learning for Anomaly Detection in Energy Consumption Data:\n  Assessing the Vulnerability to Adversarial Attacks", "author": "Yohannis Kifle Telila and Damitha Senevirathne and Dumindu Tissera and Apurva Narayan and Miriam A. M. Capretz and Katarina Grolinger", "abstract": "  Anomaly detection is crucial in the energy sector to identify irregular\npatterns indicating equipment failures, energy theft, or other issues. Machine\nlearning techniques for anomaly detection have achieved great success, but are\ntypically centralized, involving sharing local data with a central server which\nraises privacy and security concerns. Federated Learning (FL) has been gaining\npopularity as it enables distributed learning without sharing local data.\nHowever, FL depends on neural networks, which are vulnerable to adversarial\nattacks that manipulate data, leading models to make erroneous predictions.\nWhile adversarial attacks have been explored in the image domain, they remain\nlargely unexplored in time series problems, especially in the energy domain.\nMoreover, the effect of adversarial attacks in the FL setting is also mostly\nunknown. This paper assesses the vulnerability of FL-based anomaly detection in\nenergy data to adversarial attacks. Specifically, two state-of-the-art models,\nLong Short Term Memory (LSTM) and Transformers, are used to detect anomalies in\nan FL setting, and two white-box attack methods, Fast Gradient Sign Method\n(FGSM) and Projected Gradient Descent (PGD), are employed to perturb the data.\nThe results show that FL is more sensitive to PGD attacks than to FGSM attacks,\nattributed to PGD's iterative nature, resulting in an accuracy drop of over 10%\neven with naive, weaker attacks. Moreover, FL is more affected by these attacks\nthan centralized learning, highlighting the need for defense mechanisms in FL.\n", "link": "http://arxiv.org/abs/2502.05041v1", "date": "2025-02-07", "relevancy": 1.8624, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4832}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4586}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Learning%20for%20Anomaly%20Detection%20in%20Energy%20Consumption%20Data%3A%0A%20%20Assessing%20the%20Vulnerability%20to%20Adversarial%20Attacks&body=Title%3A%20Federated%20Learning%20for%20Anomaly%20Detection%20in%20Energy%20Consumption%20Data%3A%0A%20%20Assessing%20the%20Vulnerability%20to%20Adversarial%20Attacks%0AAuthor%3A%20Yohannis%20Kifle%20Telila%20and%20Damitha%20Senevirathne%20and%20Dumindu%20Tissera%20and%20Apurva%20Narayan%20and%20Miriam%20A.%20M.%20Capretz%20and%20Katarina%20Grolinger%0AAbstract%3A%20%20%20Anomaly%20detection%20is%20crucial%20in%20the%20energy%20sector%20to%20identify%20irregular%0Apatterns%20indicating%20equipment%20failures%2C%20energy%20theft%2C%20or%20other%20issues.%20Machine%0Alearning%20techniques%20for%20anomaly%20detection%20have%20achieved%20great%20success%2C%20but%20are%0Atypically%20centralized%2C%20involving%20sharing%20local%20data%20with%20a%20central%20server%20which%0Araises%20privacy%20and%20security%20concerns.%20Federated%20Learning%20%28FL%29%20has%20been%20gaining%0Apopularity%20as%20it%20enables%20distributed%20learning%20without%20sharing%20local%20data.%0AHowever%2C%20FL%20depends%20on%20neural%20networks%2C%20which%20are%20vulnerable%20to%20adversarial%0Aattacks%20that%20manipulate%20data%2C%20leading%20models%20to%20make%20erroneous%20predictions.%0AWhile%20adversarial%20attacks%20have%20been%20explored%20in%20the%20image%20domain%2C%20they%20remain%0Alargely%20unexplored%20in%20time%20series%20problems%2C%20especially%20in%20the%20energy%20domain.%0AMoreover%2C%20the%20effect%20of%20adversarial%20attacks%20in%20the%20FL%20setting%20is%20also%20mostly%0Aunknown.%20This%20paper%20assesses%20the%20vulnerability%20of%20FL-based%20anomaly%20detection%20in%0Aenergy%20data%20to%20adversarial%20attacks.%20Specifically%2C%20two%20state-of-the-art%20models%2C%0ALong%20Short%20Term%20Memory%20%28LSTM%29%20and%20Transformers%2C%20are%20used%20to%20detect%20anomalies%20in%0Aan%20FL%20setting%2C%20and%20two%20white-box%20attack%20methods%2C%20Fast%20Gradient%20Sign%20Method%0A%28FGSM%29%20and%20Projected%20Gradient%20Descent%20%28PGD%29%2C%20are%20employed%20to%20perturb%20the%20data.%0AThe%20results%20show%20that%20FL%20is%20more%20sensitive%20to%20PGD%20attacks%20than%20to%20FGSM%20attacks%2C%0Aattributed%20to%20PGD%27s%20iterative%20nature%2C%20resulting%20in%20an%20accuracy%20drop%20of%20over%2010%25%0Aeven%20with%20naive%2C%20weaker%20attacks.%20Moreover%2C%20FL%20is%20more%20affected%20by%20these%20attacks%0Athan%20centralized%20learning%2C%20highlighting%20the%20need%20for%20defense%20mechanisms%20in%20FL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05041v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Learning%2520for%2520Anomaly%2520Detection%2520in%2520Energy%2520Consumption%2520Data%253A%250A%2520%2520Assessing%2520the%2520Vulnerability%2520to%2520Adversarial%2520Attacks%26entry.906535625%3DYohannis%2520Kifle%2520Telila%2520and%2520Damitha%2520Senevirathne%2520and%2520Dumindu%2520Tissera%2520and%2520Apurva%2520Narayan%2520and%2520Miriam%2520A.%2520M.%2520Capretz%2520and%2520Katarina%2520Grolinger%26entry.1292438233%3D%2520%2520Anomaly%2520detection%2520is%2520crucial%2520in%2520the%2520energy%2520sector%2520to%2520identify%2520irregular%250Apatterns%2520indicating%2520equipment%2520failures%252C%2520energy%2520theft%252C%2520or%2520other%2520issues.%2520Machine%250Alearning%2520techniques%2520for%2520anomaly%2520detection%2520have%2520achieved%2520great%2520success%252C%2520but%2520are%250Atypically%2520centralized%252C%2520involving%2520sharing%2520local%2520data%2520with%2520a%2520central%2520server%2520which%250Araises%2520privacy%2520and%2520security%2520concerns.%2520Federated%2520Learning%2520%2528FL%2529%2520has%2520been%2520gaining%250Apopularity%2520as%2520it%2520enables%2520distributed%2520learning%2520without%2520sharing%2520local%2520data.%250AHowever%252C%2520FL%2520depends%2520on%2520neural%2520networks%252C%2520which%2520are%2520vulnerable%2520to%2520adversarial%250Aattacks%2520that%2520manipulate%2520data%252C%2520leading%2520models%2520to%2520make%2520erroneous%2520predictions.%250AWhile%2520adversarial%2520attacks%2520have%2520been%2520explored%2520in%2520the%2520image%2520domain%252C%2520they%2520remain%250Alargely%2520unexplored%2520in%2520time%2520series%2520problems%252C%2520especially%2520in%2520the%2520energy%2520domain.%250AMoreover%252C%2520the%2520effect%2520of%2520adversarial%2520attacks%2520in%2520the%2520FL%2520setting%2520is%2520also%2520mostly%250Aunknown.%2520This%2520paper%2520assesses%2520the%2520vulnerability%2520of%2520FL-based%2520anomaly%2520detection%2520in%250Aenergy%2520data%2520to%2520adversarial%2520attacks.%2520Specifically%252C%2520two%2520state-of-the-art%2520models%252C%250ALong%2520Short%2520Term%2520Memory%2520%2528LSTM%2529%2520and%2520Transformers%252C%2520are%2520used%2520to%2520detect%2520anomalies%2520in%250Aan%2520FL%2520setting%252C%2520and%2520two%2520white-box%2520attack%2520methods%252C%2520Fast%2520Gradient%2520Sign%2520Method%250A%2528FGSM%2529%2520and%2520Projected%2520Gradient%2520Descent%2520%2528PGD%2529%252C%2520are%2520employed%2520to%2520perturb%2520the%2520data.%250AThe%2520results%2520show%2520that%2520FL%2520is%2520more%2520sensitive%2520to%2520PGD%2520attacks%2520than%2520to%2520FGSM%2520attacks%252C%250Aattributed%2520to%2520PGD%2527s%2520iterative%2520nature%252C%2520resulting%2520in%2520an%2520accuracy%2520drop%2520of%2520over%252010%2525%250Aeven%2520with%2520naive%252C%2520weaker%2520attacks.%2520Moreover%252C%2520FL%2520is%2520more%2520affected%2520by%2520these%2520attacks%250Athan%2520centralized%2520learning%252C%2520highlighting%2520the%2520need%2520for%2520defense%2520mechanisms%2520in%2520FL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05041v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Learning%20for%20Anomaly%20Detection%20in%20Energy%20Consumption%20Data%3A%0A%20%20Assessing%20the%20Vulnerability%20to%20Adversarial%20Attacks&entry.906535625=Yohannis%20Kifle%20Telila%20and%20Damitha%20Senevirathne%20and%20Dumindu%20Tissera%20and%20Apurva%20Narayan%20and%20Miriam%20A.%20M.%20Capretz%20and%20Katarina%20Grolinger&entry.1292438233=%20%20Anomaly%20detection%20is%20crucial%20in%20the%20energy%20sector%20to%20identify%20irregular%0Apatterns%20indicating%20equipment%20failures%2C%20energy%20theft%2C%20or%20other%20issues.%20Machine%0Alearning%20techniques%20for%20anomaly%20detection%20have%20achieved%20great%20success%2C%20but%20are%0Atypically%20centralized%2C%20involving%20sharing%20local%20data%20with%20a%20central%20server%20which%0Araises%20privacy%20and%20security%20concerns.%20Federated%20Learning%20%28FL%29%20has%20been%20gaining%0Apopularity%20as%20it%20enables%20distributed%20learning%20without%20sharing%20local%20data.%0AHowever%2C%20FL%20depends%20on%20neural%20networks%2C%20which%20are%20vulnerable%20to%20adversarial%0Aattacks%20that%20manipulate%20data%2C%20leading%20models%20to%20make%20erroneous%20predictions.%0AWhile%20adversarial%20attacks%20have%20been%20explored%20in%20the%20image%20domain%2C%20they%20remain%0Alargely%20unexplored%20in%20time%20series%20problems%2C%20especially%20in%20the%20energy%20domain.%0AMoreover%2C%20the%20effect%20of%20adversarial%20attacks%20in%20the%20FL%20setting%20is%20also%20mostly%0Aunknown.%20This%20paper%20assesses%20the%20vulnerability%20of%20FL-based%20anomaly%20detection%20in%0Aenergy%20data%20to%20adversarial%20attacks.%20Specifically%2C%20two%20state-of-the-art%20models%2C%0ALong%20Short%20Term%20Memory%20%28LSTM%29%20and%20Transformers%2C%20are%20used%20to%20detect%20anomalies%20in%0Aan%20FL%20setting%2C%20and%20two%20white-box%20attack%20methods%2C%20Fast%20Gradient%20Sign%20Method%0A%28FGSM%29%20and%20Projected%20Gradient%20Descent%20%28PGD%29%2C%20are%20employed%20to%20perturb%20the%20data.%0AThe%20results%20show%20that%20FL%20is%20more%20sensitive%20to%20PGD%20attacks%20than%20to%20FGSM%20attacks%2C%0Aattributed%20to%20PGD%27s%20iterative%20nature%2C%20resulting%20in%20an%20accuracy%20drop%20of%20over%2010%25%0Aeven%20with%20naive%2C%20weaker%20attacks.%20Moreover%2C%20FL%20is%20more%20affected%20by%20these%20attacks%0Athan%20centralized%20learning%2C%20highlighting%20the%20need%20for%20defense%20mechanisms%20in%20FL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05041v1&entry.124074799=Read"},
{"title": "Online Robot Motion Planning Methodology Guided by Group Social\n  Proxemics Feature", "author": "Xuan Mu and Xiaorui Liu and Shuai Guo and Wenzheng Chi and Wei Wang and Shuzhi Sam Ge", "abstract": "  Nowadays robot is supposed to demonstrate human-like perception, reasoning\nand behavior pattern in social or service application. However, most of the\nexisting motion planning methods are incompatible with above requirement. A\npotential reason is that the existing navigation algorithms usually intend to\ntreat people as another kind of obstacle, and hardly take the social principle\nor awareness into consideration. In this paper, we attempt to model the\nproxemics of group and blend it into the scenario perception and navigation of\nrobot. For this purpose, a group clustering method considering both social\nrelevance and spatial confidence is introduced. It can enable robot to identify\nindividuals and divide them into groups. Next, we propose defining the\nindividual proxemics within magnetic dipole model, and further established the\ngroup proxemics and scenario map through vector-field superposition. On the\nbasis of the group clustering and proxemics modeling, we present the method to\nobtain the optimal observation positions (OOPs) of group. Once the OOPs grid\nand scenario map are established, a heuristic path is employed to generate path\nthat guide robot cruising among the groups for interactive purpose. A series of\nexperiments are conducted to validate the proposed methodology on the practical\nrobot, the results have demonstrated that our methodology has achieved\npromising performance on group recognition accuracy and path-generation\nefficiency. This concludes that the group awareness evolved as an important\nmodule to make robot socially behave in the practical scenario.\n", "link": "http://arxiv.org/abs/2502.04837v1", "date": "2025-02-07", "relevancy": 1.7902, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6112}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5968}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Robot%20Motion%20Planning%20Methodology%20Guided%20by%20Group%20Social%0A%20%20Proxemics%20Feature&body=Title%3A%20Online%20Robot%20Motion%20Planning%20Methodology%20Guided%20by%20Group%20Social%0A%20%20Proxemics%20Feature%0AAuthor%3A%20Xuan%20Mu%20and%20Xiaorui%20Liu%20and%20Shuai%20Guo%20and%20Wenzheng%20Chi%20and%20Wei%20Wang%20and%20Shuzhi%20Sam%20Ge%0AAbstract%3A%20%20%20Nowadays%20robot%20is%20supposed%20to%20demonstrate%20human-like%20perception%2C%20reasoning%0Aand%20behavior%20pattern%20in%20social%20or%20service%20application.%20However%2C%20most%20of%20the%0Aexisting%20motion%20planning%20methods%20are%20incompatible%20with%20above%20requirement.%20A%0Apotential%20reason%20is%20that%20the%20existing%20navigation%20algorithms%20usually%20intend%20to%0Atreat%20people%20as%20another%20kind%20of%20obstacle%2C%20and%20hardly%20take%20the%20social%20principle%0Aor%20awareness%20into%20consideration.%20In%20this%20paper%2C%20we%20attempt%20to%20model%20the%0Aproxemics%20of%20group%20and%20blend%20it%20into%20the%20scenario%20perception%20and%20navigation%20of%0Arobot.%20For%20this%20purpose%2C%20a%20group%20clustering%20method%20considering%20both%20social%0Arelevance%20and%20spatial%20confidence%20is%20introduced.%20It%20can%20enable%20robot%20to%20identify%0Aindividuals%20and%20divide%20them%20into%20groups.%20Next%2C%20we%20propose%20defining%20the%0Aindividual%20proxemics%20within%20magnetic%20dipole%20model%2C%20and%20further%20established%20the%0Agroup%20proxemics%20and%20scenario%20map%20through%20vector-field%20superposition.%20On%20the%0Abasis%20of%20the%20group%20clustering%20and%20proxemics%20modeling%2C%20we%20present%20the%20method%20to%0Aobtain%20the%20optimal%20observation%20positions%20%28OOPs%29%20of%20group.%20Once%20the%20OOPs%20grid%0Aand%20scenario%20map%20are%20established%2C%20a%20heuristic%20path%20is%20employed%20to%20generate%20path%0Athat%20guide%20robot%20cruising%20among%20the%20groups%20for%20interactive%20purpose.%20A%20series%20of%0Aexperiments%20are%20conducted%20to%20validate%20the%20proposed%20methodology%20on%20the%20practical%0Arobot%2C%20the%20results%20have%20demonstrated%20that%20our%20methodology%20has%20achieved%0Apromising%20performance%20on%20group%20recognition%20accuracy%20and%20path-generation%0Aefficiency.%20This%20concludes%20that%20the%20group%20awareness%20evolved%20as%20an%20important%0Amodule%20to%20make%20robot%20socially%20behave%20in%20the%20practical%20scenario.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04837v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Robot%2520Motion%2520Planning%2520Methodology%2520Guided%2520by%2520Group%2520Social%250A%2520%2520Proxemics%2520Feature%26entry.906535625%3DXuan%2520Mu%2520and%2520Xiaorui%2520Liu%2520and%2520Shuai%2520Guo%2520and%2520Wenzheng%2520Chi%2520and%2520Wei%2520Wang%2520and%2520Shuzhi%2520Sam%2520Ge%26entry.1292438233%3D%2520%2520Nowadays%2520robot%2520is%2520supposed%2520to%2520demonstrate%2520human-like%2520perception%252C%2520reasoning%250Aand%2520behavior%2520pattern%2520in%2520social%2520or%2520service%2520application.%2520However%252C%2520most%2520of%2520the%250Aexisting%2520motion%2520planning%2520methods%2520are%2520incompatible%2520with%2520above%2520requirement.%2520A%250Apotential%2520reason%2520is%2520that%2520the%2520existing%2520navigation%2520algorithms%2520usually%2520intend%2520to%250Atreat%2520people%2520as%2520another%2520kind%2520of%2520obstacle%252C%2520and%2520hardly%2520take%2520the%2520social%2520principle%250Aor%2520awareness%2520into%2520consideration.%2520In%2520this%2520paper%252C%2520we%2520attempt%2520to%2520model%2520the%250Aproxemics%2520of%2520group%2520and%2520blend%2520it%2520into%2520the%2520scenario%2520perception%2520and%2520navigation%2520of%250Arobot.%2520For%2520this%2520purpose%252C%2520a%2520group%2520clustering%2520method%2520considering%2520both%2520social%250Arelevance%2520and%2520spatial%2520confidence%2520is%2520introduced.%2520It%2520can%2520enable%2520robot%2520to%2520identify%250Aindividuals%2520and%2520divide%2520them%2520into%2520groups.%2520Next%252C%2520we%2520propose%2520defining%2520the%250Aindividual%2520proxemics%2520within%2520magnetic%2520dipole%2520model%252C%2520and%2520further%2520established%2520the%250Agroup%2520proxemics%2520and%2520scenario%2520map%2520through%2520vector-field%2520superposition.%2520On%2520the%250Abasis%2520of%2520the%2520group%2520clustering%2520and%2520proxemics%2520modeling%252C%2520we%2520present%2520the%2520method%2520to%250Aobtain%2520the%2520optimal%2520observation%2520positions%2520%2528OOPs%2529%2520of%2520group.%2520Once%2520the%2520OOPs%2520grid%250Aand%2520scenario%2520map%2520are%2520established%252C%2520a%2520heuristic%2520path%2520is%2520employed%2520to%2520generate%2520path%250Athat%2520guide%2520robot%2520cruising%2520among%2520the%2520groups%2520for%2520interactive%2520purpose.%2520A%2520series%2520of%250Aexperiments%2520are%2520conducted%2520to%2520validate%2520the%2520proposed%2520methodology%2520on%2520the%2520practical%250Arobot%252C%2520the%2520results%2520have%2520demonstrated%2520that%2520our%2520methodology%2520has%2520achieved%250Apromising%2520performance%2520on%2520group%2520recognition%2520accuracy%2520and%2520path-generation%250Aefficiency.%2520This%2520concludes%2520that%2520the%2520group%2520awareness%2520evolved%2520as%2520an%2520important%250Amodule%2520to%2520make%2520robot%2520socially%2520behave%2520in%2520the%2520practical%2520scenario.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04837v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Robot%20Motion%20Planning%20Methodology%20Guided%20by%20Group%20Social%0A%20%20Proxemics%20Feature&entry.906535625=Xuan%20Mu%20and%20Xiaorui%20Liu%20and%20Shuai%20Guo%20and%20Wenzheng%20Chi%20and%20Wei%20Wang%20and%20Shuzhi%20Sam%20Ge&entry.1292438233=%20%20Nowadays%20robot%20is%20supposed%20to%20demonstrate%20human-like%20perception%2C%20reasoning%0Aand%20behavior%20pattern%20in%20social%20or%20service%20application.%20However%2C%20most%20of%20the%0Aexisting%20motion%20planning%20methods%20are%20incompatible%20with%20above%20requirement.%20A%0Apotential%20reason%20is%20that%20the%20existing%20navigation%20algorithms%20usually%20intend%20to%0Atreat%20people%20as%20another%20kind%20of%20obstacle%2C%20and%20hardly%20take%20the%20social%20principle%0Aor%20awareness%20into%20consideration.%20In%20this%20paper%2C%20we%20attempt%20to%20model%20the%0Aproxemics%20of%20group%20and%20blend%20it%20into%20the%20scenario%20perception%20and%20navigation%20of%0Arobot.%20For%20this%20purpose%2C%20a%20group%20clustering%20method%20considering%20both%20social%0Arelevance%20and%20spatial%20confidence%20is%20introduced.%20It%20can%20enable%20robot%20to%20identify%0Aindividuals%20and%20divide%20them%20into%20groups.%20Next%2C%20we%20propose%20defining%20the%0Aindividual%20proxemics%20within%20magnetic%20dipole%20model%2C%20and%20further%20established%20the%0Agroup%20proxemics%20and%20scenario%20map%20through%20vector-field%20superposition.%20On%20the%0Abasis%20of%20the%20group%20clustering%20and%20proxemics%20modeling%2C%20we%20present%20the%20method%20to%0Aobtain%20the%20optimal%20observation%20positions%20%28OOPs%29%20of%20group.%20Once%20the%20OOPs%20grid%0Aand%20scenario%20map%20are%20established%2C%20a%20heuristic%20path%20is%20employed%20to%20generate%20path%0Athat%20guide%20robot%20cruising%20among%20the%20groups%20for%20interactive%20purpose.%20A%20series%20of%0Aexperiments%20are%20conducted%20to%20validate%20the%20proposed%20methodology%20on%20the%20practical%0Arobot%2C%20the%20results%20have%20demonstrated%20that%20our%20methodology%20has%20achieved%0Apromising%20performance%20on%20group%20recognition%20accuracy%20and%20path-generation%0Aefficiency.%20This%20concludes%20that%20the%20group%20awareness%20evolved%20as%20an%20important%0Amodule%20to%20make%20robot%20socially%20behave%20in%20the%20practical%20scenario.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04837v1&entry.124074799=Read"},
{"title": "SACNet: A Spatially Adaptive Convolution Network for 2D Multi-organ\n  Medical Segmentation", "author": "Lin Zhang and Wenbo Gao and Jie Yi and Yunyun Yang", "abstract": "  Multi-organ segmentation in medical image analysis is crucial for diagnosis\nand treatment planning. However, many factors complicate the task, including\nvariability in different target categories and interference from complex\nbackgrounds. In this paper, we utilize the knowledge of Deformable Convolution\nV3 (DCNv3) and multi-object segmentation to optimize our Spatially Adaptive\nConvolution Network (SACNet) in three aspects: feature extraction, model\narchitecture, and loss constraint, simultaneously enhancing the perception of\ndifferent segmentation targets. Firstly, we propose the Adaptive Receptive\nField Module (ARFM), which combines DCNv3 with a series of customized\nblock-level and architecture-level designs similar to transformers. This module\ncan capture the unique features of different organs by adaptively adjusting the\nreceptive field according to various targets. Secondly, we utilize ARFM as\nbuilding blocks to construct the encoder-decoder of SACNet and partially share\nparameters between the encoder and decoder, making the network wider rather\nthan deeper. This design achieves a shared lightweight decoder and a more\nparameter-efficient and effective framework. Lastly, we propose a novel\ncontinuity dynamic adjustment loss function, based on t-vMF dice loss and\ncross-entropy loss, to better balance easy and complex classes in segmentation.\nExperiments on 3D slice datasets from ACDC and Synapse demonstrate that SACNet\ndelivers superior segmentation performance in multi-organ segmentation tasks\ncompared to several existing methods.\n", "link": "http://arxiv.org/abs/2407.10157v2", "date": "2025-02-07", "relevancy": 1.6739, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5726}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5426}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SACNet%3A%20A%20Spatially%20Adaptive%20Convolution%20Network%20for%202D%20Multi-organ%0A%20%20Medical%20Segmentation&body=Title%3A%20SACNet%3A%20A%20Spatially%20Adaptive%20Convolution%20Network%20for%202D%20Multi-organ%0A%20%20Medical%20Segmentation%0AAuthor%3A%20Lin%20Zhang%20and%20Wenbo%20Gao%20and%20Jie%20Yi%20and%20Yunyun%20Yang%0AAbstract%3A%20%20%20Multi-organ%20segmentation%20in%20medical%20image%20analysis%20is%20crucial%20for%20diagnosis%0Aand%20treatment%20planning.%20However%2C%20many%20factors%20complicate%20the%20task%2C%20including%0Avariability%20in%20different%20target%20categories%20and%20interference%20from%20complex%0Abackgrounds.%20In%20this%20paper%2C%20we%20utilize%20the%20knowledge%20of%20Deformable%20Convolution%0AV3%20%28DCNv3%29%20and%20multi-object%20segmentation%20to%20optimize%20our%20Spatially%20Adaptive%0AConvolution%20Network%20%28SACNet%29%20in%20three%20aspects%3A%20feature%20extraction%2C%20model%0Aarchitecture%2C%20and%20loss%20constraint%2C%20simultaneously%20enhancing%20the%20perception%20of%0Adifferent%20segmentation%20targets.%20Firstly%2C%20we%20propose%20the%20Adaptive%20Receptive%0AField%20Module%20%28ARFM%29%2C%20which%20combines%20DCNv3%20with%20a%20series%20of%20customized%0Ablock-level%20and%20architecture-level%20designs%20similar%20to%20transformers.%20This%20module%0Acan%20capture%20the%20unique%20features%20of%20different%20organs%20by%20adaptively%20adjusting%20the%0Areceptive%20field%20according%20to%20various%20targets.%20Secondly%2C%20we%20utilize%20ARFM%20as%0Abuilding%20blocks%20to%20construct%20the%20encoder-decoder%20of%20SACNet%20and%20partially%20share%0Aparameters%20between%20the%20encoder%20and%20decoder%2C%20making%20the%20network%20wider%20rather%0Athan%20deeper.%20This%20design%20achieves%20a%20shared%20lightweight%20decoder%20and%20a%20more%0Aparameter-efficient%20and%20effective%20framework.%20Lastly%2C%20we%20propose%20a%20novel%0Acontinuity%20dynamic%20adjustment%20loss%20function%2C%20based%20on%20t-vMF%20dice%20loss%20and%0Across-entropy%20loss%2C%20to%20better%20balance%20easy%20and%20complex%20classes%20in%20segmentation.%0AExperiments%20on%203D%20slice%20datasets%20from%20ACDC%20and%20Synapse%20demonstrate%20that%20SACNet%0Adelivers%20superior%20segmentation%20performance%20in%20multi-organ%20segmentation%20tasks%0Acompared%20to%20several%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10157v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSACNet%253A%2520A%2520Spatially%2520Adaptive%2520Convolution%2520Network%2520for%25202D%2520Multi-organ%250A%2520%2520Medical%2520Segmentation%26entry.906535625%3DLin%2520Zhang%2520and%2520Wenbo%2520Gao%2520and%2520Jie%2520Yi%2520and%2520Yunyun%2520Yang%26entry.1292438233%3D%2520%2520Multi-organ%2520segmentation%2520in%2520medical%2520image%2520analysis%2520is%2520crucial%2520for%2520diagnosis%250Aand%2520treatment%2520planning.%2520However%252C%2520many%2520factors%2520complicate%2520the%2520task%252C%2520including%250Avariability%2520in%2520different%2520target%2520categories%2520and%2520interference%2520from%2520complex%250Abackgrounds.%2520In%2520this%2520paper%252C%2520we%2520utilize%2520the%2520knowledge%2520of%2520Deformable%2520Convolution%250AV3%2520%2528DCNv3%2529%2520and%2520multi-object%2520segmentation%2520to%2520optimize%2520our%2520Spatially%2520Adaptive%250AConvolution%2520Network%2520%2528SACNet%2529%2520in%2520three%2520aspects%253A%2520feature%2520extraction%252C%2520model%250Aarchitecture%252C%2520and%2520loss%2520constraint%252C%2520simultaneously%2520enhancing%2520the%2520perception%2520of%250Adifferent%2520segmentation%2520targets.%2520Firstly%252C%2520we%2520propose%2520the%2520Adaptive%2520Receptive%250AField%2520Module%2520%2528ARFM%2529%252C%2520which%2520combines%2520DCNv3%2520with%2520a%2520series%2520of%2520customized%250Ablock-level%2520and%2520architecture-level%2520designs%2520similar%2520to%2520transformers.%2520This%2520module%250Acan%2520capture%2520the%2520unique%2520features%2520of%2520different%2520organs%2520by%2520adaptively%2520adjusting%2520the%250Areceptive%2520field%2520according%2520to%2520various%2520targets.%2520Secondly%252C%2520we%2520utilize%2520ARFM%2520as%250Abuilding%2520blocks%2520to%2520construct%2520the%2520encoder-decoder%2520of%2520SACNet%2520and%2520partially%2520share%250Aparameters%2520between%2520the%2520encoder%2520and%2520decoder%252C%2520making%2520the%2520network%2520wider%2520rather%250Athan%2520deeper.%2520This%2520design%2520achieves%2520a%2520shared%2520lightweight%2520decoder%2520and%2520a%2520more%250Aparameter-efficient%2520and%2520effective%2520framework.%2520Lastly%252C%2520we%2520propose%2520a%2520novel%250Acontinuity%2520dynamic%2520adjustment%2520loss%2520function%252C%2520based%2520on%2520t-vMF%2520dice%2520loss%2520and%250Across-entropy%2520loss%252C%2520to%2520better%2520balance%2520easy%2520and%2520complex%2520classes%2520in%2520segmentation.%250AExperiments%2520on%25203D%2520slice%2520datasets%2520from%2520ACDC%2520and%2520Synapse%2520demonstrate%2520that%2520SACNet%250Adelivers%2520superior%2520segmentation%2520performance%2520in%2520multi-organ%2520segmentation%2520tasks%250Acompared%2520to%2520several%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10157v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SACNet%3A%20A%20Spatially%20Adaptive%20Convolution%20Network%20for%202D%20Multi-organ%0A%20%20Medical%20Segmentation&entry.906535625=Lin%20Zhang%20and%20Wenbo%20Gao%20and%20Jie%20Yi%20and%20Yunyun%20Yang&entry.1292438233=%20%20Multi-organ%20segmentation%20in%20medical%20image%20analysis%20is%20crucial%20for%20diagnosis%0Aand%20treatment%20planning.%20However%2C%20many%20factors%20complicate%20the%20task%2C%20including%0Avariability%20in%20different%20target%20categories%20and%20interference%20from%20complex%0Abackgrounds.%20In%20this%20paper%2C%20we%20utilize%20the%20knowledge%20of%20Deformable%20Convolution%0AV3%20%28DCNv3%29%20and%20multi-object%20segmentation%20to%20optimize%20our%20Spatially%20Adaptive%0AConvolution%20Network%20%28SACNet%29%20in%20three%20aspects%3A%20feature%20extraction%2C%20model%0Aarchitecture%2C%20and%20loss%20constraint%2C%20simultaneously%20enhancing%20the%20perception%20of%0Adifferent%20segmentation%20targets.%20Firstly%2C%20we%20propose%20the%20Adaptive%20Receptive%0AField%20Module%20%28ARFM%29%2C%20which%20combines%20DCNv3%20with%20a%20series%20of%20customized%0Ablock-level%20and%20architecture-level%20designs%20similar%20to%20transformers.%20This%20module%0Acan%20capture%20the%20unique%20features%20of%20different%20organs%20by%20adaptively%20adjusting%20the%0Areceptive%20field%20according%20to%20various%20targets.%20Secondly%2C%20we%20utilize%20ARFM%20as%0Abuilding%20blocks%20to%20construct%20the%20encoder-decoder%20of%20SACNet%20and%20partially%20share%0Aparameters%20between%20the%20encoder%20and%20decoder%2C%20making%20the%20network%20wider%20rather%0Athan%20deeper.%20This%20design%20achieves%20a%20shared%20lightweight%20decoder%20and%20a%20more%0Aparameter-efficient%20and%20effective%20framework.%20Lastly%2C%20we%20propose%20a%20novel%0Acontinuity%20dynamic%20adjustment%20loss%20function%2C%20based%20on%20t-vMF%20dice%20loss%20and%0Across-entropy%20loss%2C%20to%20better%20balance%20easy%20and%20complex%20classes%20in%20segmentation.%0AExperiments%20on%203D%20slice%20datasets%20from%20ACDC%20and%20Synapse%20demonstrate%20that%20SACNet%0Adelivers%20superior%20segmentation%20performance%20in%20multi-organ%20segmentation%20tasks%0Acompared%20to%20several%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10157v2&entry.124074799=Read"},
{"title": "Humans Co-exist, So Must Embodied Artificial Agents", "author": "Hannah Kuehn and Joseph La Delfa and Miguel Vasco and Danica Kragic and Iolanda Leite", "abstract": "  Modern embodied artificial agents excel in static, predefined tasks but fall\nshort in dynamic and long-term interactions with humans. On the other hand,\nhumans can adapt and evolve continuously, exploiting the situated knowledge\nembedded in their environment and other agents, thus contributing to meaningful\ninteractions. We introduce the concept of co-existence for embodied artificial\nagents and argues that it is a prerequisite for meaningful, long-term\ninteraction with humans. We take inspiration from biology and design theory to\nunderstand how human and non-human organisms foster entities that co-exist\nwithin their specific niches. Finally, we propose key research directions for\nthe machine learning community to foster co-existing embodied agents, focusing\non the principles, hardware and learning methods responsible for shaping them.\n", "link": "http://arxiv.org/abs/2502.04809v1", "date": "2025-02-07", "relevancy": 1.5066, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5276}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.512}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Humans%20Co-exist%2C%20So%20Must%20Embodied%20Artificial%20Agents&body=Title%3A%20Humans%20Co-exist%2C%20So%20Must%20Embodied%20Artificial%20Agents%0AAuthor%3A%20Hannah%20Kuehn%20and%20Joseph%20La%20Delfa%20and%20Miguel%20Vasco%20and%20Danica%20Kragic%20and%20Iolanda%20Leite%0AAbstract%3A%20%20%20Modern%20embodied%20artificial%20agents%20excel%20in%20static%2C%20predefined%20tasks%20but%20fall%0Ashort%20in%20dynamic%20and%20long-term%20interactions%20with%20humans.%20On%20the%20other%20hand%2C%0Ahumans%20can%20adapt%20and%20evolve%20continuously%2C%20exploiting%20the%20situated%20knowledge%0Aembedded%20in%20their%20environment%20and%20other%20agents%2C%20thus%20contributing%20to%20meaningful%0Ainteractions.%20We%20introduce%20the%20concept%20of%20co-existence%20for%20embodied%20artificial%0Aagents%20and%20argues%20that%20it%20is%20a%20prerequisite%20for%20meaningful%2C%20long-term%0Ainteraction%20with%20humans.%20We%20take%20inspiration%20from%20biology%20and%20design%20theory%20to%0Aunderstand%20how%20human%20and%20non-human%20organisms%20foster%20entities%20that%20co-exist%0Awithin%20their%20specific%20niches.%20Finally%2C%20we%20propose%20key%20research%20directions%20for%0Athe%20machine%20learning%20community%20to%20foster%20co-existing%20embodied%20agents%2C%20focusing%0Aon%20the%20principles%2C%20hardware%20and%20learning%20methods%20responsible%20for%20shaping%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04809v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumans%2520Co-exist%252C%2520So%2520Must%2520Embodied%2520Artificial%2520Agents%26entry.906535625%3DHannah%2520Kuehn%2520and%2520Joseph%2520La%2520Delfa%2520and%2520Miguel%2520Vasco%2520and%2520Danica%2520Kragic%2520and%2520Iolanda%2520Leite%26entry.1292438233%3D%2520%2520Modern%2520embodied%2520artificial%2520agents%2520excel%2520in%2520static%252C%2520predefined%2520tasks%2520but%2520fall%250Ashort%2520in%2520dynamic%2520and%2520long-term%2520interactions%2520with%2520humans.%2520On%2520the%2520other%2520hand%252C%250Ahumans%2520can%2520adapt%2520and%2520evolve%2520continuously%252C%2520exploiting%2520the%2520situated%2520knowledge%250Aembedded%2520in%2520their%2520environment%2520and%2520other%2520agents%252C%2520thus%2520contributing%2520to%2520meaningful%250Ainteractions.%2520We%2520introduce%2520the%2520concept%2520of%2520co-existence%2520for%2520embodied%2520artificial%250Aagents%2520and%2520argues%2520that%2520it%2520is%2520a%2520prerequisite%2520for%2520meaningful%252C%2520long-term%250Ainteraction%2520with%2520humans.%2520We%2520take%2520inspiration%2520from%2520biology%2520and%2520design%2520theory%2520to%250Aunderstand%2520how%2520human%2520and%2520non-human%2520organisms%2520foster%2520entities%2520that%2520co-exist%250Awithin%2520their%2520specific%2520niches.%2520Finally%252C%2520we%2520propose%2520key%2520research%2520directions%2520for%250Athe%2520machine%2520learning%2520community%2520to%2520foster%2520co-existing%2520embodied%2520agents%252C%2520focusing%250Aon%2520the%2520principles%252C%2520hardware%2520and%2520learning%2520methods%2520responsible%2520for%2520shaping%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04809v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Humans%20Co-exist%2C%20So%20Must%20Embodied%20Artificial%20Agents&entry.906535625=Hannah%20Kuehn%20and%20Joseph%20La%20Delfa%20and%20Miguel%20Vasco%20and%20Danica%20Kragic%20and%20Iolanda%20Leite&entry.1292438233=%20%20Modern%20embodied%20artificial%20agents%20excel%20in%20static%2C%20predefined%20tasks%20but%20fall%0Ashort%20in%20dynamic%20and%20long-term%20interactions%20with%20humans.%20On%20the%20other%20hand%2C%0Ahumans%20can%20adapt%20and%20evolve%20continuously%2C%20exploiting%20the%20situated%20knowledge%0Aembedded%20in%20their%20environment%20and%20other%20agents%2C%20thus%20contributing%20to%20meaningful%0Ainteractions.%20We%20introduce%20the%20concept%20of%20co-existence%20for%20embodied%20artificial%0Aagents%20and%20argues%20that%20it%20is%20a%20prerequisite%20for%20meaningful%2C%20long-term%0Ainteraction%20with%20humans.%20We%20take%20inspiration%20from%20biology%20and%20design%20theory%20to%0Aunderstand%20how%20human%20and%20non-human%20organisms%20foster%20entities%20that%20co-exist%0Awithin%20their%20specific%20niches.%20Finally%2C%20we%20propose%20key%20research%20directions%20for%0Athe%20machine%20learning%20community%20to%20foster%20co-existing%20embodied%20agents%2C%20focusing%0Aon%20the%20principles%2C%20hardware%20and%20learning%20methods%20responsible%20for%20shaping%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04809v1&entry.124074799=Read"},
{"title": "FlightForge: Advancing UAV Research with Procedural Generation of\n  High-Fidelity Simulation and Integrated Autonomy", "author": "David \u010capek and Jan Hrn\u010d\u00ed\u0159 and Tom\u00e1\u0161 B\u00e1\u010da and Jakub Jirkal and Vojt\u011bch Von\u00e1sek and Robert P\u011bni\u010dka and Martin Saska", "abstract": "  Robotic simulators play a crucial role in the development and testing of\nautonomous systems, particularly in the realm of Uncrewed Aerial Vehicles\n(UAV). However, existing simulators often lack high-level autonomy, hindering\ntheir immediate applicability to complex tasks such as autonomous navigation in\nunknown environments. This limitation stems from the challenge of integrating\nrealistic physics, photorealistic rendering, and diverse sensor modalities into\na single simulation environment. At the same time, the existing photorealistic\nUAV simulators use mostly hand-crafted environments with limited environment\nsizes, which prevents the testing of long-range missions. This restricts the\nusage of existing simulators to only low-level tasks such as control and\ncollision avoidance. To this end, we propose the novel FlightForge UAV\nopen-source simulator. FlightForge offers advanced rendering capabilities,\ndiverse control modalities, and, foremost, procedural generation of\nenvironments. Moreover, the simulator is already integrated with a fully\nautonomous UAV system capable of long-range flights in cluttered unknown\nenvironments. The key innovation lies in novel procedural environment\ngeneration and seamless integration of high-level autonomy into the simulation\nenvironment. Experimental results demonstrate superior sensor rendering\ncapability compared to existing simulators, and also the ability of autonomous\nnavigation in almost infinite environments.\n", "link": "http://arxiv.org/abs/2502.05038v1", "date": "2025-02-07", "relevancy": 1.5875, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.534}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5235}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlightForge%3A%20Advancing%20UAV%20Research%20with%20Procedural%20Generation%20of%0A%20%20High-Fidelity%20Simulation%20and%20Integrated%20Autonomy&body=Title%3A%20FlightForge%3A%20Advancing%20UAV%20Research%20with%20Procedural%20Generation%20of%0A%20%20High-Fidelity%20Simulation%20and%20Integrated%20Autonomy%0AAuthor%3A%20David%20%C4%8Capek%20and%20Jan%20Hrn%C4%8D%C3%AD%C5%99%20and%20Tom%C3%A1%C5%A1%20B%C3%A1%C4%8Da%20and%20Jakub%20Jirkal%20and%20Vojt%C4%9Bch%20Von%C3%A1sek%20and%20Robert%20P%C4%9Bni%C4%8Dka%20and%20Martin%20Saska%0AAbstract%3A%20%20%20Robotic%20simulators%20play%20a%20crucial%20role%20in%20the%20development%20and%20testing%20of%0Aautonomous%20systems%2C%20particularly%20in%20the%20realm%20of%20Uncrewed%20Aerial%20Vehicles%0A%28UAV%29.%20However%2C%20existing%20simulators%20often%20lack%20high-level%20autonomy%2C%20hindering%0Atheir%20immediate%20applicability%20to%20complex%20tasks%20such%20as%20autonomous%20navigation%20in%0Aunknown%20environments.%20This%20limitation%20stems%20from%20the%20challenge%20of%20integrating%0Arealistic%20physics%2C%20photorealistic%20rendering%2C%20and%20diverse%20sensor%20modalities%20into%0Aa%20single%20simulation%20environment.%20At%20the%20same%20time%2C%20the%20existing%20photorealistic%0AUAV%20simulators%20use%20mostly%20hand-crafted%20environments%20with%20limited%20environment%0Asizes%2C%20which%20prevents%20the%20testing%20of%20long-range%20missions.%20This%20restricts%20the%0Ausage%20of%20existing%20simulators%20to%20only%20low-level%20tasks%20such%20as%20control%20and%0Acollision%20avoidance.%20To%20this%20end%2C%20we%20propose%20the%20novel%20FlightForge%20UAV%0Aopen-source%20simulator.%20FlightForge%20offers%20advanced%20rendering%20capabilities%2C%0Adiverse%20control%20modalities%2C%20and%2C%20foremost%2C%20procedural%20generation%20of%0Aenvironments.%20Moreover%2C%20the%20simulator%20is%20already%20integrated%20with%20a%20fully%0Aautonomous%20UAV%20system%20capable%20of%20long-range%20flights%20in%20cluttered%20unknown%0Aenvironments.%20The%20key%20innovation%20lies%20in%20novel%20procedural%20environment%0Ageneration%20and%20seamless%20integration%20of%20high-level%20autonomy%20into%20the%20simulation%0Aenvironment.%20Experimental%20results%20demonstrate%20superior%20sensor%20rendering%0Acapability%20compared%20to%20existing%20simulators%2C%20and%20also%20the%20ability%20of%20autonomous%0Anavigation%20in%20almost%20infinite%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlightForge%253A%2520Advancing%2520UAV%2520Research%2520with%2520Procedural%2520Generation%2520of%250A%2520%2520High-Fidelity%2520Simulation%2520and%2520Integrated%2520Autonomy%26entry.906535625%3DDavid%2520%25C4%258Capek%2520and%2520Jan%2520Hrn%25C4%258D%25C3%25AD%25C5%2599%2520and%2520Tom%25C3%25A1%25C5%25A1%2520B%25C3%25A1%25C4%258Da%2520and%2520Jakub%2520Jirkal%2520and%2520Vojt%25C4%259Bch%2520Von%25C3%25A1sek%2520and%2520Robert%2520P%25C4%259Bni%25C4%258Dka%2520and%2520Martin%2520Saska%26entry.1292438233%3D%2520%2520Robotic%2520simulators%2520play%2520a%2520crucial%2520role%2520in%2520the%2520development%2520and%2520testing%2520of%250Aautonomous%2520systems%252C%2520particularly%2520in%2520the%2520realm%2520of%2520Uncrewed%2520Aerial%2520Vehicles%250A%2528UAV%2529.%2520However%252C%2520existing%2520simulators%2520often%2520lack%2520high-level%2520autonomy%252C%2520hindering%250Atheir%2520immediate%2520applicability%2520to%2520complex%2520tasks%2520such%2520as%2520autonomous%2520navigation%2520in%250Aunknown%2520environments.%2520This%2520limitation%2520stems%2520from%2520the%2520challenge%2520of%2520integrating%250Arealistic%2520physics%252C%2520photorealistic%2520rendering%252C%2520and%2520diverse%2520sensor%2520modalities%2520into%250Aa%2520single%2520simulation%2520environment.%2520At%2520the%2520same%2520time%252C%2520the%2520existing%2520photorealistic%250AUAV%2520simulators%2520use%2520mostly%2520hand-crafted%2520environments%2520with%2520limited%2520environment%250Asizes%252C%2520which%2520prevents%2520the%2520testing%2520of%2520long-range%2520missions.%2520This%2520restricts%2520the%250Ausage%2520of%2520existing%2520simulators%2520to%2520only%2520low-level%2520tasks%2520such%2520as%2520control%2520and%250Acollision%2520avoidance.%2520To%2520this%2520end%252C%2520we%2520propose%2520the%2520novel%2520FlightForge%2520UAV%250Aopen-source%2520simulator.%2520FlightForge%2520offers%2520advanced%2520rendering%2520capabilities%252C%250Adiverse%2520control%2520modalities%252C%2520and%252C%2520foremost%252C%2520procedural%2520generation%2520of%250Aenvironments.%2520Moreover%252C%2520the%2520simulator%2520is%2520already%2520integrated%2520with%2520a%2520fully%250Aautonomous%2520UAV%2520system%2520capable%2520of%2520long-range%2520flights%2520in%2520cluttered%2520unknown%250Aenvironments.%2520The%2520key%2520innovation%2520lies%2520in%2520novel%2520procedural%2520environment%250Ageneration%2520and%2520seamless%2520integration%2520of%2520high-level%2520autonomy%2520into%2520the%2520simulation%250Aenvironment.%2520Experimental%2520results%2520demonstrate%2520superior%2520sensor%2520rendering%250Acapability%2520compared%2520to%2520existing%2520simulators%252C%2520and%2520also%2520the%2520ability%2520of%2520autonomous%250Anavigation%2520in%2520almost%2520infinite%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlightForge%3A%20Advancing%20UAV%20Research%20with%20Procedural%20Generation%20of%0A%20%20High-Fidelity%20Simulation%20and%20Integrated%20Autonomy&entry.906535625=David%20%C4%8Capek%20and%20Jan%20Hrn%C4%8D%C3%AD%C5%99%20and%20Tom%C3%A1%C5%A1%20B%C3%A1%C4%8Da%20and%20Jakub%20Jirkal%20and%20Vojt%C4%9Bch%20Von%C3%A1sek%20and%20Robert%20P%C4%9Bni%C4%8Dka%20and%20Martin%20Saska&entry.1292438233=%20%20Robotic%20simulators%20play%20a%20crucial%20role%20in%20the%20development%20and%20testing%20of%0Aautonomous%20systems%2C%20particularly%20in%20the%20realm%20of%20Uncrewed%20Aerial%20Vehicles%0A%28UAV%29.%20However%2C%20existing%20simulators%20often%20lack%20high-level%20autonomy%2C%20hindering%0Atheir%20immediate%20applicability%20to%20complex%20tasks%20such%20as%20autonomous%20navigation%20in%0Aunknown%20environments.%20This%20limitation%20stems%20from%20the%20challenge%20of%20integrating%0Arealistic%20physics%2C%20photorealistic%20rendering%2C%20and%20diverse%20sensor%20modalities%20into%0Aa%20single%20simulation%20environment.%20At%20the%20same%20time%2C%20the%20existing%20photorealistic%0AUAV%20simulators%20use%20mostly%20hand-crafted%20environments%20with%20limited%20environment%0Asizes%2C%20which%20prevents%20the%20testing%20of%20long-range%20missions.%20This%20restricts%20the%0Ausage%20of%20existing%20simulators%20to%20only%20low-level%20tasks%20such%20as%20control%20and%0Acollision%20avoidance.%20To%20this%20end%2C%20we%20propose%20the%20novel%20FlightForge%20UAV%0Aopen-source%20simulator.%20FlightForge%20offers%20advanced%20rendering%20capabilities%2C%0Adiverse%20control%20modalities%2C%20and%2C%20foremost%2C%20procedural%20generation%20of%0Aenvironments.%20Moreover%2C%20the%20simulator%20is%20already%20integrated%20with%20a%20fully%0Aautonomous%20UAV%20system%20capable%20of%20long-range%20flights%20in%20cluttered%20unknown%0Aenvironments.%20The%20key%20innovation%20lies%20in%20novel%20procedural%20environment%0Ageneration%20and%20seamless%20integration%20of%20high-level%20autonomy%20into%20the%20simulation%0Aenvironment.%20Experimental%20results%20demonstrate%20superior%20sensor%20rendering%0Acapability%20compared%20to%20existing%20simulators%2C%20and%20also%20the%20ability%20of%20autonomous%0Anavigation%20in%20almost%20infinite%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05038v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


