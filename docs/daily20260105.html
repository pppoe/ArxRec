<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20260101.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Matrix-free Second-order Optimization of Gaussian Splats with Residual Sampling", "author": "Hamza Pehlivan and Andrea Boscolo Camiletto and Lin Geng Foo and Marc Habermann and Christian Theobalt", "abstract": "3D Gaussian Splatting (3DGS) is widely used for novel view synthesis due to its high rendering quality and fast inference time. However, 3DGS predominantly relies on first-order optimizers such as Adam, which leads to long training times. To address this limitation, we propose a novel second-order optimization strategy based on Levenberg-Marquardt (LM) and Conjugate Gradient (CG), which we specifically tailor towards Gaussian Splatting. Our key insight is that the Jacobian in 3DGS exhibits significant sparsity since each Gaussian affects only a limited number of pixels. We exploit this sparsity by proposing a matrix-free and GPU-parallelized LM optimization. To further improve its efficiency, we propose sampling strategies for both the camera views and loss function and, consequently, the normal equation, significantly reducing the computational complexity. In addition, we increase the convergence rate of the second-order approximation by introducing an effective heuristic to determine the learning rate that avoids the expensive computation cost of line search methods. As a result, our method achieves a $3\\times$ speedup over standard LM and outperforms Adam by $~6\\times$ when the Gaussian count is low while remaining competitive for moderate counts. Project Page: https://vcai.mpi-inf.mpg.de/projects/LM-IS", "link": "http://arxiv.org/abs/2504.12905v2", "date": "2026-01-02", "relevancy": 3.3104, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6891}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6553}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Matrix-free%20Second-order%20Optimization%20of%20Gaussian%20Splats%20with%20Residual%20Sampling&body=Title%3A%20Matrix-free%20Second-order%20Optimization%20of%20Gaussian%20Splats%20with%20Residual%20Sampling%0AAuthor%3A%20Hamza%20Pehlivan%20and%20Andrea%20Boscolo%20Camiletto%20and%20Lin%20Geng%20Foo%20and%20Marc%20Habermann%20and%20Christian%20Theobalt%0AAbstract%3A%203D%20Gaussian%20Splatting%20%283DGS%29%20is%20widely%20used%20for%20novel%20view%20synthesis%20due%20to%20its%20high%20rendering%20quality%20and%20fast%20inference%20time.%20However%2C%203DGS%20predominantly%20relies%20on%20first-order%20optimizers%20such%20as%20Adam%2C%20which%20leads%20to%20long%20training%20times.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20novel%20second-order%20optimization%20strategy%20based%20on%20Levenberg-Marquardt%20%28LM%29%20and%20Conjugate%20Gradient%20%28CG%29%2C%20which%20we%20specifically%20tailor%20towards%20Gaussian%20Splatting.%20Our%20key%20insight%20is%20that%20the%20Jacobian%20in%203DGS%20exhibits%20significant%20sparsity%20since%20each%20Gaussian%20affects%20only%20a%20limited%20number%20of%20pixels.%20We%20exploit%20this%20sparsity%20by%20proposing%20a%20matrix-free%20and%20GPU-parallelized%20LM%20optimization.%20To%20further%20improve%20its%20efficiency%2C%20we%20propose%20sampling%20strategies%20for%20both%20the%20camera%20views%20and%20loss%20function%20and%2C%20consequently%2C%20the%20normal%20equation%2C%20significantly%20reducing%20the%20computational%20complexity.%20In%20addition%2C%20we%20increase%20the%20convergence%20rate%20of%20the%20second-order%20approximation%20by%20introducing%20an%20effective%20heuristic%20to%20determine%20the%20learning%20rate%20that%20avoids%20the%20expensive%20computation%20cost%20of%20line%20search%20methods.%20As%20a%20result%2C%20our%20method%20achieves%20a%20%243%5Ctimes%24%20speedup%20over%20standard%20LM%20and%20outperforms%20Adam%20by%20%24~6%5Ctimes%24%20when%20the%20Gaussian%20count%20is%20low%20while%20remaining%20competitive%20for%20moderate%20counts.%20Project%20Page%3A%20https%3A//vcai.mpi-inf.mpg.de/projects/LM-IS%0ALink%3A%20http%3A//arxiv.org/abs/2504.12905v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatrix-free%2520Second-order%2520Optimization%2520of%2520Gaussian%2520Splats%2520with%2520Residual%2520Sampling%26entry.906535625%3DHamza%2520Pehlivan%2520and%2520Andrea%2520Boscolo%2520Camiletto%2520and%2520Lin%2520Geng%2520Foo%2520and%2520Marc%2520Habermann%2520and%2520Christian%2520Theobalt%26entry.1292438233%3D3D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520is%2520widely%2520used%2520for%2520novel%2520view%2520synthesis%2520due%2520to%2520its%2520high%2520rendering%2520quality%2520and%2520fast%2520inference%2520time.%2520However%252C%25203DGS%2520predominantly%2520relies%2520on%2520first-order%2520optimizers%2520such%2520as%2520Adam%252C%2520which%2520leads%2520to%2520long%2520training%2520times.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%2520novel%2520second-order%2520optimization%2520strategy%2520based%2520on%2520Levenberg-Marquardt%2520%2528LM%2529%2520and%2520Conjugate%2520Gradient%2520%2528CG%2529%252C%2520which%2520we%2520specifically%2520tailor%2520towards%2520Gaussian%2520Splatting.%2520Our%2520key%2520insight%2520is%2520that%2520the%2520Jacobian%2520in%25203DGS%2520exhibits%2520significant%2520sparsity%2520since%2520each%2520Gaussian%2520affects%2520only%2520a%2520limited%2520number%2520of%2520pixels.%2520We%2520exploit%2520this%2520sparsity%2520by%2520proposing%2520a%2520matrix-free%2520and%2520GPU-parallelized%2520LM%2520optimization.%2520To%2520further%2520improve%2520its%2520efficiency%252C%2520we%2520propose%2520sampling%2520strategies%2520for%2520both%2520the%2520camera%2520views%2520and%2520loss%2520function%2520and%252C%2520consequently%252C%2520the%2520normal%2520equation%252C%2520significantly%2520reducing%2520the%2520computational%2520complexity.%2520In%2520addition%252C%2520we%2520increase%2520the%2520convergence%2520rate%2520of%2520the%2520second-order%2520approximation%2520by%2520introducing%2520an%2520effective%2520heuristic%2520to%2520determine%2520the%2520learning%2520rate%2520that%2520avoids%2520the%2520expensive%2520computation%2520cost%2520of%2520line%2520search%2520methods.%2520As%2520a%2520result%252C%2520our%2520method%2520achieves%2520a%2520%25243%255Ctimes%2524%2520speedup%2520over%2520standard%2520LM%2520and%2520outperforms%2520Adam%2520by%2520%2524~6%255Ctimes%2524%2520when%2520the%2520Gaussian%2520count%2520is%2520low%2520while%2520remaining%2520competitive%2520for%2520moderate%2520counts.%2520Project%2520Page%253A%2520https%253A//vcai.mpi-inf.mpg.de/projects/LM-IS%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12905v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Matrix-free%20Second-order%20Optimization%20of%20Gaussian%20Splats%20with%20Residual%20Sampling&entry.906535625=Hamza%20Pehlivan%20and%20Andrea%20Boscolo%20Camiletto%20and%20Lin%20Geng%20Foo%20and%20Marc%20Habermann%20and%20Christian%20Theobalt&entry.1292438233=3D%20Gaussian%20Splatting%20%283DGS%29%20is%20widely%20used%20for%20novel%20view%20synthesis%20due%20to%20its%20high%20rendering%20quality%20and%20fast%20inference%20time.%20However%2C%203DGS%20predominantly%20relies%20on%20first-order%20optimizers%20such%20as%20Adam%2C%20which%20leads%20to%20long%20training%20times.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20novel%20second-order%20optimization%20strategy%20based%20on%20Levenberg-Marquardt%20%28LM%29%20and%20Conjugate%20Gradient%20%28CG%29%2C%20which%20we%20specifically%20tailor%20towards%20Gaussian%20Splatting.%20Our%20key%20insight%20is%20that%20the%20Jacobian%20in%203DGS%20exhibits%20significant%20sparsity%20since%20each%20Gaussian%20affects%20only%20a%20limited%20number%20of%20pixels.%20We%20exploit%20this%20sparsity%20by%20proposing%20a%20matrix-free%20and%20GPU-parallelized%20LM%20optimization.%20To%20further%20improve%20its%20efficiency%2C%20we%20propose%20sampling%20strategies%20for%20both%20the%20camera%20views%20and%20loss%20function%20and%2C%20consequently%2C%20the%20normal%20equation%2C%20significantly%20reducing%20the%20computational%20complexity.%20In%20addition%2C%20we%20increase%20the%20convergence%20rate%20of%20the%20second-order%20approximation%20by%20introducing%20an%20effective%20heuristic%20to%20determine%20the%20learning%20rate%20that%20avoids%20the%20expensive%20computation%20cost%20of%20line%20search%20methods.%20As%20a%20result%2C%20our%20method%20achieves%20a%20%243%5Ctimes%24%20speedup%20over%20standard%20LM%20and%20outperforms%20Adam%20by%20%24~6%5Ctimes%24%20when%20the%20Gaussian%20count%20is%20low%20while%20remaining%20competitive%20for%20moderate%20counts.%20Project%20Page%3A%20https%3A//vcai.mpi-inf.mpg.de/projects/LM-IS&entry.1838667208=http%3A//arxiv.org/abs/2504.12905v2&entry.124074799=Read"},
{"title": "AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction", "author": "Jiewen Chan and Zhenjun Zhao and Yu-Lun Liu", "abstract": "Reconstructing dynamic 3D scenes from monocular videos requires simultaneously capturing high-frequency appearance details and temporally continuous motion. Existing methods using single Gaussian primitives are limited by their low-pass filtering nature, while standard Gabor functions introduce energy instability. Moreover, lack of temporal continuity constraints often leads to motion artifacts during interpolation. We propose AdaGaR, a unified framework addressing both frequency adaptivity and temporal continuity in explicit dynamic scene modeling. We introduce Adaptive Gabor Representation, extending Gaussians through learnable frequency weights and adaptive energy compensation to balance detail capture and stability. For temporal continuity, we employ Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution. An Adaptive Initialization mechanism combining depth estimation, point tracking, and foreground masks establishes stable point cloud distributions in early training. Experiments on Tap-Vid DAVIS demonstrate state-of-the-art performance (PSNR 35.49, SSIM 0.9433, LPIPS 0.0723) and strong generalization across frame interpolation, depth consistency, video editing, and stereo view synthesis. Project page: https://jiewenchan.github.io/AdaGaR/", "link": "http://arxiv.org/abs/2601.00796v1", "date": "2026-01-02", "relevancy": 3.2866, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7135}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6675}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaGaR%3A%20Adaptive%20Gabor%20Representation%20for%20Dynamic%20Scene%20Reconstruction&body=Title%3A%20AdaGaR%3A%20Adaptive%20Gabor%20Representation%20for%20Dynamic%20Scene%20Reconstruction%0AAuthor%3A%20Jiewen%20Chan%20and%20Zhenjun%20Zhao%20and%20Yu-Lun%20Liu%0AAbstract%3A%20Reconstructing%20dynamic%203D%20scenes%20from%20monocular%20videos%20requires%20simultaneously%20capturing%20high-frequency%20appearance%20details%20and%20temporally%20continuous%20motion.%20Existing%20methods%20using%20single%20Gaussian%20primitives%20are%20limited%20by%20their%20low-pass%20filtering%20nature%2C%20while%20standard%20Gabor%20functions%20introduce%20energy%20instability.%20Moreover%2C%20lack%20of%20temporal%20continuity%20constraints%20often%20leads%20to%20motion%20artifacts%20during%20interpolation.%20We%20propose%20AdaGaR%2C%20a%20unified%20framework%20addressing%20both%20frequency%20adaptivity%20and%20temporal%20continuity%20in%20explicit%20dynamic%20scene%20modeling.%20We%20introduce%20Adaptive%20Gabor%20Representation%2C%20extending%20Gaussians%20through%20learnable%20frequency%20weights%20and%20adaptive%20energy%20compensation%20to%20balance%20detail%20capture%20and%20stability.%20For%20temporal%20continuity%2C%20we%20employ%20Cubic%20Hermite%20Splines%20with%20Temporal%20Curvature%20Regularization%20to%20ensure%20smooth%20motion%20evolution.%20An%20Adaptive%20Initialization%20mechanism%20combining%20depth%20estimation%2C%20point%20tracking%2C%20and%20foreground%20masks%20establishes%20stable%20point%20cloud%20distributions%20in%20early%20training.%20Experiments%20on%20Tap-Vid%20DAVIS%20demonstrate%20state-of-the-art%20performance%20%28PSNR%2035.49%2C%20SSIM%200.9433%2C%20LPIPS%200.0723%29%20and%20strong%20generalization%20across%20frame%20interpolation%2C%20depth%20consistency%2C%20video%20editing%2C%20and%20stereo%20view%20synthesis.%20Project%20page%3A%20https%3A//jiewenchan.github.io/AdaGaR/%0ALink%3A%20http%3A//arxiv.org/abs/2601.00796v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaGaR%253A%2520Adaptive%2520Gabor%2520Representation%2520for%2520Dynamic%2520Scene%2520Reconstruction%26entry.906535625%3DJiewen%2520Chan%2520and%2520Zhenjun%2520Zhao%2520and%2520Yu-Lun%2520Liu%26entry.1292438233%3DReconstructing%2520dynamic%25203D%2520scenes%2520from%2520monocular%2520videos%2520requires%2520simultaneously%2520capturing%2520high-frequency%2520appearance%2520details%2520and%2520temporally%2520continuous%2520motion.%2520Existing%2520methods%2520using%2520single%2520Gaussian%2520primitives%2520are%2520limited%2520by%2520their%2520low-pass%2520filtering%2520nature%252C%2520while%2520standard%2520Gabor%2520functions%2520introduce%2520energy%2520instability.%2520Moreover%252C%2520lack%2520of%2520temporal%2520continuity%2520constraints%2520often%2520leads%2520to%2520motion%2520artifacts%2520during%2520interpolation.%2520We%2520propose%2520AdaGaR%252C%2520a%2520unified%2520framework%2520addressing%2520both%2520frequency%2520adaptivity%2520and%2520temporal%2520continuity%2520in%2520explicit%2520dynamic%2520scene%2520modeling.%2520We%2520introduce%2520Adaptive%2520Gabor%2520Representation%252C%2520extending%2520Gaussians%2520through%2520learnable%2520frequency%2520weights%2520and%2520adaptive%2520energy%2520compensation%2520to%2520balance%2520detail%2520capture%2520and%2520stability.%2520For%2520temporal%2520continuity%252C%2520we%2520employ%2520Cubic%2520Hermite%2520Splines%2520with%2520Temporal%2520Curvature%2520Regularization%2520to%2520ensure%2520smooth%2520motion%2520evolution.%2520An%2520Adaptive%2520Initialization%2520mechanism%2520combining%2520depth%2520estimation%252C%2520point%2520tracking%252C%2520and%2520foreground%2520masks%2520establishes%2520stable%2520point%2520cloud%2520distributions%2520in%2520early%2520training.%2520Experiments%2520on%2520Tap-Vid%2520DAVIS%2520demonstrate%2520state-of-the-art%2520performance%2520%2528PSNR%252035.49%252C%2520SSIM%25200.9433%252C%2520LPIPS%25200.0723%2529%2520and%2520strong%2520generalization%2520across%2520frame%2520interpolation%252C%2520depth%2520consistency%252C%2520video%2520editing%252C%2520and%2520stereo%2520view%2520synthesis.%2520Project%2520page%253A%2520https%253A//jiewenchan.github.io/AdaGaR/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00796v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaGaR%3A%20Adaptive%20Gabor%20Representation%20for%20Dynamic%20Scene%20Reconstruction&entry.906535625=Jiewen%20Chan%20and%20Zhenjun%20Zhao%20and%20Yu-Lun%20Liu&entry.1292438233=Reconstructing%20dynamic%203D%20scenes%20from%20monocular%20videos%20requires%20simultaneously%20capturing%20high-frequency%20appearance%20details%20and%20temporally%20continuous%20motion.%20Existing%20methods%20using%20single%20Gaussian%20primitives%20are%20limited%20by%20their%20low-pass%20filtering%20nature%2C%20while%20standard%20Gabor%20functions%20introduce%20energy%20instability.%20Moreover%2C%20lack%20of%20temporal%20continuity%20constraints%20often%20leads%20to%20motion%20artifacts%20during%20interpolation.%20We%20propose%20AdaGaR%2C%20a%20unified%20framework%20addressing%20both%20frequency%20adaptivity%20and%20temporal%20continuity%20in%20explicit%20dynamic%20scene%20modeling.%20We%20introduce%20Adaptive%20Gabor%20Representation%2C%20extending%20Gaussians%20through%20learnable%20frequency%20weights%20and%20adaptive%20energy%20compensation%20to%20balance%20detail%20capture%20and%20stability.%20For%20temporal%20continuity%2C%20we%20employ%20Cubic%20Hermite%20Splines%20with%20Temporal%20Curvature%20Regularization%20to%20ensure%20smooth%20motion%20evolution.%20An%20Adaptive%20Initialization%20mechanism%20combining%20depth%20estimation%2C%20point%20tracking%2C%20and%20foreground%20masks%20establishes%20stable%20point%20cloud%20distributions%20in%20early%20training.%20Experiments%20on%20Tap-Vid%20DAVIS%20demonstrate%20state-of-the-art%20performance%20%28PSNR%2035.49%2C%20SSIM%200.9433%2C%20LPIPS%200.0723%29%20and%20strong%20generalization%20across%20frame%20interpolation%2C%20depth%20consistency%2C%20video%20editing%2C%20and%20stereo%20view%20synthesis.%20Project%20page%3A%20https%3A//jiewenchan.github.io/AdaGaR/&entry.1838667208=http%3A//arxiv.org/abs/2601.00796v1&entry.124074799=Read"},
{"title": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation", "author": "Taekyung Ki and Sangwon Jang and Jaehyeong Jo and Jaehong Yoon and Sung Ju Hwang", "abstract": "Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.", "link": "http://arxiv.org/abs/2601.00664v1", "date": "2026-01-02", "relevancy": 3.0949, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6329}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6329}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Avatar%20Forcing%3A%20Real-Time%20Interactive%20Head%20Avatar%20Generation%20for%20Natural%20Conversation&body=Title%3A%20Avatar%20Forcing%3A%20Real-Time%20Interactive%20Head%20Avatar%20Generation%20for%20Natural%20Conversation%0AAuthor%3A%20Taekyung%20Ki%20and%20Sangwon%20Jang%20and%20Jaehyeong%20Jo%20and%20Jaehong%20Yoon%20and%20Sung%20Ju%20Hwang%0AAbstract%3A%20Talking%20head%20generation%20creates%20lifelike%20avatars%20from%20static%20portraits%20for%20virtual%20communication%20and%20content%20creation.%20However%2C%20current%20models%20do%20not%20yet%20convey%20the%20feeling%20of%20truly%20interactive%20communication%2C%20often%20generating%20one-way%20responses%20that%20lack%20emotional%20engagement.%20We%20identify%20two%20key%20challenges%20toward%20truly%20interactive%20avatars%3A%20generating%20motion%20in%20real-time%20under%20causal%20constraints%20and%20learning%20expressive%2C%20vibrant%20reactions%20without%20additional%20labeled%20data.%20To%20address%20these%20challenges%2C%20we%20propose%20Avatar%20Forcing%2C%20a%20new%20framework%20for%20interactive%20head%20avatar%20generation%20that%20models%20real-time%20user-avatar%20interactions%20through%20diffusion%20forcing.%20This%20design%20allows%20the%20avatar%20to%20process%20real-time%20multimodal%20inputs%2C%20including%20the%20user%27s%20audio%20and%20motion%2C%20with%20low%20latency%20for%20instant%20reactions%20to%20both%20verbal%20and%20non-verbal%20cues%20such%20as%20speech%2C%20nods%2C%20and%20laughter.%20Furthermore%2C%20we%20introduce%20a%20direct%20preference%20optimization%20method%20that%20leverages%20synthetic%20losing%20samples%20constructed%20by%20dropping%20user%20conditions%2C%20enabling%20label-free%20learning%20of%20expressive%20interaction.%20Experimental%20results%20demonstrate%20that%20our%20framework%20enables%20real-time%20interaction%20with%20low%20latency%20%28approximately%20500ms%29%2C%20achieving%206.8X%20speedup%20compared%20to%20the%20baseline%2C%20and%20produces%20reactive%20and%20expressive%20avatar%20motion%2C%20which%20is%20preferred%20over%2080%25%20against%20the%20baseline.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00664v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAvatar%2520Forcing%253A%2520Real-Time%2520Interactive%2520Head%2520Avatar%2520Generation%2520for%2520Natural%2520Conversation%26entry.906535625%3DTaekyung%2520Ki%2520and%2520Sangwon%2520Jang%2520and%2520Jaehyeong%2520Jo%2520and%2520Jaehong%2520Yoon%2520and%2520Sung%2520Ju%2520Hwang%26entry.1292438233%3DTalking%2520head%2520generation%2520creates%2520lifelike%2520avatars%2520from%2520static%2520portraits%2520for%2520virtual%2520communication%2520and%2520content%2520creation.%2520However%252C%2520current%2520models%2520do%2520not%2520yet%2520convey%2520the%2520feeling%2520of%2520truly%2520interactive%2520communication%252C%2520often%2520generating%2520one-way%2520responses%2520that%2520lack%2520emotional%2520engagement.%2520We%2520identify%2520two%2520key%2520challenges%2520toward%2520truly%2520interactive%2520avatars%253A%2520generating%2520motion%2520in%2520real-time%2520under%2520causal%2520constraints%2520and%2520learning%2520expressive%252C%2520vibrant%2520reactions%2520without%2520additional%2520labeled%2520data.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520Avatar%2520Forcing%252C%2520a%2520new%2520framework%2520for%2520interactive%2520head%2520avatar%2520generation%2520that%2520models%2520real-time%2520user-avatar%2520interactions%2520through%2520diffusion%2520forcing.%2520This%2520design%2520allows%2520the%2520avatar%2520to%2520process%2520real-time%2520multimodal%2520inputs%252C%2520including%2520the%2520user%2527s%2520audio%2520and%2520motion%252C%2520with%2520low%2520latency%2520for%2520instant%2520reactions%2520to%2520both%2520verbal%2520and%2520non-verbal%2520cues%2520such%2520as%2520speech%252C%2520nods%252C%2520and%2520laughter.%2520Furthermore%252C%2520we%2520introduce%2520a%2520direct%2520preference%2520optimization%2520method%2520that%2520leverages%2520synthetic%2520losing%2520samples%2520constructed%2520by%2520dropping%2520user%2520conditions%252C%2520enabling%2520label-free%2520learning%2520of%2520expressive%2520interaction.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520framework%2520enables%2520real-time%2520interaction%2520with%2520low%2520latency%2520%2528approximately%2520500ms%2529%252C%2520achieving%25206.8X%2520speedup%2520compared%2520to%2520the%2520baseline%252C%2520and%2520produces%2520reactive%2520and%2520expressive%2520avatar%2520motion%252C%2520which%2520is%2520preferred%2520over%252080%2525%2520against%2520the%2520baseline.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00664v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Avatar%20Forcing%3A%20Real-Time%20Interactive%20Head%20Avatar%20Generation%20for%20Natural%20Conversation&entry.906535625=Taekyung%20Ki%20and%20Sangwon%20Jang%20and%20Jaehyeong%20Jo%20and%20Jaehong%20Yoon%20and%20Sung%20Ju%20Hwang&entry.1292438233=Talking%20head%20generation%20creates%20lifelike%20avatars%20from%20static%20portraits%20for%20virtual%20communication%20and%20content%20creation.%20However%2C%20current%20models%20do%20not%20yet%20convey%20the%20feeling%20of%20truly%20interactive%20communication%2C%20often%20generating%20one-way%20responses%20that%20lack%20emotional%20engagement.%20We%20identify%20two%20key%20challenges%20toward%20truly%20interactive%20avatars%3A%20generating%20motion%20in%20real-time%20under%20causal%20constraints%20and%20learning%20expressive%2C%20vibrant%20reactions%20without%20additional%20labeled%20data.%20To%20address%20these%20challenges%2C%20we%20propose%20Avatar%20Forcing%2C%20a%20new%20framework%20for%20interactive%20head%20avatar%20generation%20that%20models%20real-time%20user-avatar%20interactions%20through%20diffusion%20forcing.%20This%20design%20allows%20the%20avatar%20to%20process%20real-time%20multimodal%20inputs%2C%20including%20the%20user%27s%20audio%20and%20motion%2C%20with%20low%20latency%20for%20instant%20reactions%20to%20both%20verbal%20and%20non-verbal%20cues%20such%20as%20speech%2C%20nods%2C%20and%20laughter.%20Furthermore%2C%20we%20introduce%20a%20direct%20preference%20optimization%20method%20that%20leverages%20synthetic%20losing%20samples%20constructed%20by%20dropping%20user%20conditions%2C%20enabling%20label-free%20learning%20of%20expressive%20interaction.%20Experimental%20results%20demonstrate%20that%20our%20framework%20enables%20real-time%20interaction%20with%20low%20latency%20%28approximately%20500ms%29%2C%20achieving%206.8X%20speedup%20compared%20to%20the%20baseline%2C%20and%20produces%20reactive%20and%20expressive%20avatar%20motion%2C%20which%20is%20preferred%20over%2080%25%20against%20the%20baseline.&entry.1838667208=http%3A//arxiv.org/abs/2601.00664v1&entry.124074799=Read"},
{"title": "Lamps: Learning Anatomy from Multiple Perspectives via Self-supervision in Chest Radiographs", "author": "Ziyu Zhou and Haozhe Luo and Mohammad Reza Hosseinzadeh Taher and Jiaxuan Pang and Xiaowei Ding and Michael B. Gotway and Jianming Liang", "abstract": "Foundation models have been successful in natural language processing and computer vision because they are capable of capturing the underlying structures (foundation) of natural languages. However, in medical imaging, the key foundation lies in human anatomy, as these images directly represent the internal structures of the body, reflecting the consistency, coherence, and hierarchy of human anatomy. Yet, existing self-supervised learning (SSL) methods often overlook these perspectives, limiting their ability to effectively learn anatomical features. To overcome the limitation, we built Lamps (learning anatomy from multiple perspectives via self-supervision) pre-trained on large-scale chest radiographs by harmoniously utilizing the consistency, coherence, and hierarchy of human anatomy as the supervision signal. Extensive experiments across 10 datasets evaluated through fine-tuning and emergent property analysis demonstrate Lamps' superior robustness, transferability, and clinical potential when compared to 10 baseline models. By learning from multiple perspectives, Lamps presents a unique opportunity for foundation models to develop meaningful, robust representations that are aligned with the structure of human anatomy.", "link": "http://arxiv.org/abs/2512.22872v2", "date": "2026-01-02", "relevancy": 2.8411, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5799}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5661}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lamps%3A%20Learning%20Anatomy%20from%20Multiple%20Perspectives%20via%20Self-supervision%20in%20Chest%20Radiographs&body=Title%3A%20Lamps%3A%20Learning%20Anatomy%20from%20Multiple%20Perspectives%20via%20Self-supervision%20in%20Chest%20Radiographs%0AAuthor%3A%20Ziyu%20Zhou%20and%20Haozhe%20Luo%20and%20Mohammad%20Reza%20Hosseinzadeh%20Taher%20and%20Jiaxuan%20Pang%20and%20Xiaowei%20Ding%20and%20Michael%20B.%20Gotway%20and%20Jianming%20Liang%0AAbstract%3A%20Foundation%20models%20have%20been%20successful%20in%20natural%20language%20processing%20and%20computer%20vision%20because%20they%20are%20capable%20of%20capturing%20the%20underlying%20structures%20%28foundation%29%20of%20natural%20languages.%20However%2C%20in%20medical%20imaging%2C%20the%20key%20foundation%20lies%20in%20human%20anatomy%2C%20as%20these%20images%20directly%20represent%20the%20internal%20structures%20of%20the%20body%2C%20reflecting%20the%20consistency%2C%20coherence%2C%20and%20hierarchy%20of%20human%20anatomy.%20Yet%2C%20existing%20self-supervised%20learning%20%28SSL%29%20methods%20often%20overlook%20these%20perspectives%2C%20limiting%20their%20ability%20to%20effectively%20learn%20anatomical%20features.%20To%20overcome%20the%20limitation%2C%20we%20built%20Lamps%20%28learning%20anatomy%20from%20multiple%20perspectives%20via%20self-supervision%29%20pre-trained%20on%20large-scale%20chest%20radiographs%20by%20harmoniously%20utilizing%20the%20consistency%2C%20coherence%2C%20and%20hierarchy%20of%20human%20anatomy%20as%20the%20supervision%20signal.%20Extensive%20experiments%20across%2010%20datasets%20evaluated%20through%20fine-tuning%20and%20emergent%20property%20analysis%20demonstrate%20Lamps%27%20superior%20robustness%2C%20transferability%2C%20and%20clinical%20potential%20when%20compared%20to%2010%20baseline%20models.%20By%20learning%20from%20multiple%20perspectives%2C%20Lamps%20presents%20a%20unique%20opportunity%20for%20foundation%20models%20to%20develop%20meaningful%2C%20robust%20representations%20that%20are%20aligned%20with%20the%20structure%20of%20human%20anatomy.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22872v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLamps%253A%2520Learning%2520Anatomy%2520from%2520Multiple%2520Perspectives%2520via%2520Self-supervision%2520in%2520Chest%2520Radiographs%26entry.906535625%3DZiyu%2520Zhou%2520and%2520Haozhe%2520Luo%2520and%2520Mohammad%2520Reza%2520Hosseinzadeh%2520Taher%2520and%2520Jiaxuan%2520Pang%2520and%2520Xiaowei%2520Ding%2520and%2520Michael%2520B.%2520Gotway%2520and%2520Jianming%2520Liang%26entry.1292438233%3DFoundation%2520models%2520have%2520been%2520successful%2520in%2520natural%2520language%2520processing%2520and%2520computer%2520vision%2520because%2520they%2520are%2520capable%2520of%2520capturing%2520the%2520underlying%2520structures%2520%2528foundation%2529%2520of%2520natural%2520languages.%2520However%252C%2520in%2520medical%2520imaging%252C%2520the%2520key%2520foundation%2520lies%2520in%2520human%2520anatomy%252C%2520as%2520these%2520images%2520directly%2520represent%2520the%2520internal%2520structures%2520of%2520the%2520body%252C%2520reflecting%2520the%2520consistency%252C%2520coherence%252C%2520and%2520hierarchy%2520of%2520human%2520anatomy.%2520Yet%252C%2520existing%2520self-supervised%2520learning%2520%2528SSL%2529%2520methods%2520often%2520overlook%2520these%2520perspectives%252C%2520limiting%2520their%2520ability%2520to%2520effectively%2520learn%2520anatomical%2520features.%2520To%2520overcome%2520the%2520limitation%252C%2520we%2520built%2520Lamps%2520%2528learning%2520anatomy%2520from%2520multiple%2520perspectives%2520via%2520self-supervision%2529%2520pre-trained%2520on%2520large-scale%2520chest%2520radiographs%2520by%2520harmoniously%2520utilizing%2520the%2520consistency%252C%2520coherence%252C%2520and%2520hierarchy%2520of%2520human%2520anatomy%2520as%2520the%2520supervision%2520signal.%2520Extensive%2520experiments%2520across%252010%2520datasets%2520evaluated%2520through%2520fine-tuning%2520and%2520emergent%2520property%2520analysis%2520demonstrate%2520Lamps%2527%2520superior%2520robustness%252C%2520transferability%252C%2520and%2520clinical%2520potential%2520when%2520compared%2520to%252010%2520baseline%2520models.%2520By%2520learning%2520from%2520multiple%2520perspectives%252C%2520Lamps%2520presents%2520a%2520unique%2520opportunity%2520for%2520foundation%2520models%2520to%2520develop%2520meaningful%252C%2520robust%2520representations%2520that%2520are%2520aligned%2520with%2520the%2520structure%2520of%2520human%2520anatomy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22872v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lamps%3A%20Learning%20Anatomy%20from%20Multiple%20Perspectives%20via%20Self-supervision%20in%20Chest%20Radiographs&entry.906535625=Ziyu%20Zhou%20and%20Haozhe%20Luo%20and%20Mohammad%20Reza%20Hosseinzadeh%20Taher%20and%20Jiaxuan%20Pang%20and%20Xiaowei%20Ding%20and%20Michael%20B.%20Gotway%20and%20Jianming%20Liang&entry.1292438233=Foundation%20models%20have%20been%20successful%20in%20natural%20language%20processing%20and%20computer%20vision%20because%20they%20are%20capable%20of%20capturing%20the%20underlying%20structures%20%28foundation%29%20of%20natural%20languages.%20However%2C%20in%20medical%20imaging%2C%20the%20key%20foundation%20lies%20in%20human%20anatomy%2C%20as%20these%20images%20directly%20represent%20the%20internal%20structures%20of%20the%20body%2C%20reflecting%20the%20consistency%2C%20coherence%2C%20and%20hierarchy%20of%20human%20anatomy.%20Yet%2C%20existing%20self-supervised%20learning%20%28SSL%29%20methods%20often%20overlook%20these%20perspectives%2C%20limiting%20their%20ability%20to%20effectively%20learn%20anatomical%20features.%20To%20overcome%20the%20limitation%2C%20we%20built%20Lamps%20%28learning%20anatomy%20from%20multiple%20perspectives%20via%20self-supervision%29%20pre-trained%20on%20large-scale%20chest%20radiographs%20by%20harmoniously%20utilizing%20the%20consistency%2C%20coherence%2C%20and%20hierarchy%20of%20human%20anatomy%20as%20the%20supervision%20signal.%20Extensive%20experiments%20across%2010%20datasets%20evaluated%20through%20fine-tuning%20and%20emergent%20property%20analysis%20demonstrate%20Lamps%27%20superior%20robustness%2C%20transferability%2C%20and%20clinical%20potential%20when%20compared%20to%2010%20baseline%20models.%20By%20learning%20from%20multiple%20perspectives%2C%20Lamps%20presents%20a%20unique%20opportunity%20for%20foundation%20models%20to%20develop%20meaningful%2C%20robust%20representations%20that%20are%20aligned%20with%20the%20structure%20of%20human%20anatomy.&entry.1838667208=http%3A//arxiv.org/abs/2512.22872v2&entry.124074799=Read"},
{"title": "Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians", "author": "Melonie de Almeida and Daniela Ivanova and Tong Shi and John H. Williamson and Paul Henderson", "abstract": "Humans excel at forecasting the future dynamics of a scene given just a single image. Video generation models that can mimic this ability are an essential component for intelligent systems. Recent approaches have improved temporal coherence and 3D consistency in single-image-conditioned video generation. However, these methods often lack robust user controllability, such as modifying the camera path, limiting their applicability in real-world applications. Most existing camera-controlled image-to-video models struggle with accurately modeling camera motion, maintaining temporal consistency, and preserving geometric integrity. Leveraging explicit intermediate 3D representations offers a promising solution by enabling coherent video generation aligned with a given camera trajectory. Although these methods often use 3D point clouds to render scenes and introduce object motion in a later stage, this two-step process still falls short in achieving full temporal consistency, despite allowing precise control over camera movement. We propose a novel framework that constructs a 3D Gaussian scene representation and samples plausible object motion, given a single image in a single forward pass. This enables fast, camera-guided video generation without the need for iterative denoising to inject object motion into render frames. Extensive experiments on the KITTI, Waymo, RealEstate10K and DL3DV-10K datasets demonstrate that our method achieves state-of-the-art video quality and inference efficiency. The project page is available at https://melonienimasha.github.io/Pixel-to-4D-Website.", "link": "http://arxiv.org/abs/2601.00678v1", "date": "2026-01-02", "relevancy": 2.7675, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.7324}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6847}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pixel-to-4D%3A%20Camera-Controlled%20Image-to-Video%20Generation%20with%20Dynamic%203D%20Gaussians&body=Title%3A%20Pixel-to-4D%3A%20Camera-Controlled%20Image-to-Video%20Generation%20with%20Dynamic%203D%20Gaussians%0AAuthor%3A%20Melonie%20de%20Almeida%20and%20Daniela%20Ivanova%20and%20Tong%20Shi%20and%20John%20H.%20Williamson%20and%20Paul%20Henderson%0AAbstract%3A%20Humans%20excel%20at%20forecasting%20the%20future%20dynamics%20of%20a%20scene%20given%20just%20a%20single%20image.%20Video%20generation%20models%20that%20can%20mimic%20this%20ability%20are%20an%20essential%20component%20for%20intelligent%20systems.%20Recent%20approaches%20have%20improved%20temporal%20coherence%20and%203D%20consistency%20in%20single-image-conditioned%20video%20generation.%20However%2C%20these%20methods%20often%20lack%20robust%20user%20controllability%2C%20such%20as%20modifying%20the%20camera%20path%2C%20limiting%20their%20applicability%20in%20real-world%20applications.%20Most%20existing%20camera-controlled%20image-to-video%20models%20struggle%20with%20accurately%20modeling%20camera%20motion%2C%20maintaining%20temporal%20consistency%2C%20and%20preserving%20geometric%20integrity.%20Leveraging%20explicit%20intermediate%203D%20representations%20offers%20a%20promising%20solution%20by%20enabling%20coherent%20video%20generation%20aligned%20with%20a%20given%20camera%20trajectory.%20Although%20these%20methods%20often%20use%203D%20point%20clouds%20to%20render%20scenes%20and%20introduce%20object%20motion%20in%20a%20later%20stage%2C%20this%20two-step%20process%20still%20falls%20short%20in%20achieving%20full%20temporal%20consistency%2C%20despite%20allowing%20precise%20control%20over%20camera%20movement.%20We%20propose%20a%20novel%20framework%20that%20constructs%20a%203D%20Gaussian%20scene%20representation%20and%20samples%20plausible%20object%20motion%2C%20given%20a%20single%20image%20in%20a%20single%20forward%20pass.%20This%20enables%20fast%2C%20camera-guided%20video%20generation%20without%20the%20need%20for%20iterative%20denoising%20to%20inject%20object%20motion%20into%20render%20frames.%20Extensive%20experiments%20on%20the%20KITTI%2C%20Waymo%2C%20RealEstate10K%20and%20DL3DV-10K%20datasets%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20video%20quality%20and%20inference%20efficiency.%20The%20project%20page%20is%20available%20at%20https%3A//melonienimasha.github.io/Pixel-to-4D-Website.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00678v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPixel-to-4D%253A%2520Camera-Controlled%2520Image-to-Video%2520Generation%2520with%2520Dynamic%25203D%2520Gaussians%26entry.906535625%3DMelonie%2520de%2520Almeida%2520and%2520Daniela%2520Ivanova%2520and%2520Tong%2520Shi%2520and%2520John%2520H.%2520Williamson%2520and%2520Paul%2520Henderson%26entry.1292438233%3DHumans%2520excel%2520at%2520forecasting%2520the%2520future%2520dynamics%2520of%2520a%2520scene%2520given%2520just%2520a%2520single%2520image.%2520Video%2520generation%2520models%2520that%2520can%2520mimic%2520this%2520ability%2520are%2520an%2520essential%2520component%2520for%2520intelligent%2520systems.%2520Recent%2520approaches%2520have%2520improved%2520temporal%2520coherence%2520and%25203D%2520consistency%2520in%2520single-image-conditioned%2520video%2520generation.%2520However%252C%2520these%2520methods%2520often%2520lack%2520robust%2520user%2520controllability%252C%2520such%2520as%2520modifying%2520the%2520camera%2520path%252C%2520limiting%2520their%2520applicability%2520in%2520real-world%2520applications.%2520Most%2520existing%2520camera-controlled%2520image-to-video%2520models%2520struggle%2520with%2520accurately%2520modeling%2520camera%2520motion%252C%2520maintaining%2520temporal%2520consistency%252C%2520and%2520preserving%2520geometric%2520integrity.%2520Leveraging%2520explicit%2520intermediate%25203D%2520representations%2520offers%2520a%2520promising%2520solution%2520by%2520enabling%2520coherent%2520video%2520generation%2520aligned%2520with%2520a%2520given%2520camera%2520trajectory.%2520Although%2520these%2520methods%2520often%2520use%25203D%2520point%2520clouds%2520to%2520render%2520scenes%2520and%2520introduce%2520object%2520motion%2520in%2520a%2520later%2520stage%252C%2520this%2520two-step%2520process%2520still%2520falls%2520short%2520in%2520achieving%2520full%2520temporal%2520consistency%252C%2520despite%2520allowing%2520precise%2520control%2520over%2520camera%2520movement.%2520We%2520propose%2520a%2520novel%2520framework%2520that%2520constructs%2520a%25203D%2520Gaussian%2520scene%2520representation%2520and%2520samples%2520plausible%2520object%2520motion%252C%2520given%2520a%2520single%2520image%2520in%2520a%2520single%2520forward%2520pass.%2520This%2520enables%2520fast%252C%2520camera-guided%2520video%2520generation%2520without%2520the%2520need%2520for%2520iterative%2520denoising%2520to%2520inject%2520object%2520motion%2520into%2520render%2520frames.%2520Extensive%2520experiments%2520on%2520the%2520KITTI%252C%2520Waymo%252C%2520RealEstate10K%2520and%2520DL3DV-10K%2520datasets%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520video%2520quality%2520and%2520inference%2520efficiency.%2520The%2520project%2520page%2520is%2520available%2520at%2520https%253A//melonienimasha.github.io/Pixel-to-4D-Website.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00678v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pixel-to-4D%3A%20Camera-Controlled%20Image-to-Video%20Generation%20with%20Dynamic%203D%20Gaussians&entry.906535625=Melonie%20de%20Almeida%20and%20Daniela%20Ivanova%20and%20Tong%20Shi%20and%20John%20H.%20Williamson%20and%20Paul%20Henderson&entry.1292438233=Humans%20excel%20at%20forecasting%20the%20future%20dynamics%20of%20a%20scene%20given%20just%20a%20single%20image.%20Video%20generation%20models%20that%20can%20mimic%20this%20ability%20are%20an%20essential%20component%20for%20intelligent%20systems.%20Recent%20approaches%20have%20improved%20temporal%20coherence%20and%203D%20consistency%20in%20single-image-conditioned%20video%20generation.%20However%2C%20these%20methods%20often%20lack%20robust%20user%20controllability%2C%20such%20as%20modifying%20the%20camera%20path%2C%20limiting%20their%20applicability%20in%20real-world%20applications.%20Most%20existing%20camera-controlled%20image-to-video%20models%20struggle%20with%20accurately%20modeling%20camera%20motion%2C%20maintaining%20temporal%20consistency%2C%20and%20preserving%20geometric%20integrity.%20Leveraging%20explicit%20intermediate%203D%20representations%20offers%20a%20promising%20solution%20by%20enabling%20coherent%20video%20generation%20aligned%20with%20a%20given%20camera%20trajectory.%20Although%20these%20methods%20often%20use%203D%20point%20clouds%20to%20render%20scenes%20and%20introduce%20object%20motion%20in%20a%20later%20stage%2C%20this%20two-step%20process%20still%20falls%20short%20in%20achieving%20full%20temporal%20consistency%2C%20despite%20allowing%20precise%20control%20over%20camera%20movement.%20We%20propose%20a%20novel%20framework%20that%20constructs%20a%203D%20Gaussian%20scene%20representation%20and%20samples%20plausible%20object%20motion%2C%20given%20a%20single%20image%20in%20a%20single%20forward%20pass.%20This%20enables%20fast%2C%20camera-guided%20video%20generation%20without%20the%20need%20for%20iterative%20denoising%20to%20inject%20object%20motion%20into%20render%20frames.%20Extensive%20experiments%20on%20the%20KITTI%2C%20Waymo%2C%20RealEstate10K%20and%20DL3DV-10K%20datasets%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20video%20quality%20and%20inference%20efficiency.%20The%20project%20page%20is%20available%20at%20https%3A//melonienimasha.github.io/Pixel-to-4D-Website.&entry.1838667208=http%3A//arxiv.org/abs/2601.00678v1&entry.124074799=Read"},
{"title": "Reconstructing Building Height from Spaceborne TomoSAR Point Clouds Using a Dual-Topology Network", "author": "Zhaiyu Chen and Yuanyuan Wang and Yilei Shi and Xiao Xiang Zhu", "abstract": "Reliable building height estimation is essential for various urban applications. Spaceborne SAR tomography (TomoSAR) provides weather-independent, side-looking observations that capture facade-level structure, offering a promising alternative to conventional optical methods. However, TomoSAR point clouds often suffer from noise, anisotropic point distributions, and data voids on incoherent surfaces, all of which hinder accurate height reconstruction. To address these challenges, we introduce a learning-based framework for converting raw TomoSAR points into high-resolution building height maps. Our dual-topology network alternates between a point branch that models irregular scatterer features and a grid branch that enforces spatial consistency. By jointly processing these representations, the network denoises the input points and inpaints missing regions to produce continuous height estimates. To our knowledge, this is the first proof of concept for large-scale urban height mapping directly from TomoSAR point clouds. Extensive experiments on data from Munich and Berlin validate the effectiveness of our approach. Moreover, we demonstrate that our framework can be extended to incorporate optical satellite imagery, further enhancing reconstruction quality. The source code is available at https://github.com/zhu-xlab/tomosar2height.", "link": "http://arxiv.org/abs/2601.00658v1", "date": "2026-01-02", "relevancy": 2.6508, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5472}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5274}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reconstructing%20Building%20Height%20from%20Spaceborne%20TomoSAR%20Point%20Clouds%20Using%20a%20Dual-Topology%20Network&body=Title%3A%20Reconstructing%20Building%20Height%20from%20Spaceborne%20TomoSAR%20Point%20Clouds%20Using%20a%20Dual-Topology%20Network%0AAuthor%3A%20Zhaiyu%20Chen%20and%20Yuanyuan%20Wang%20and%20Yilei%20Shi%20and%20Xiao%20Xiang%20Zhu%0AAbstract%3A%20Reliable%20building%20height%20estimation%20is%20essential%20for%20various%20urban%20applications.%20Spaceborne%20SAR%20tomography%20%28TomoSAR%29%20provides%20weather-independent%2C%20side-looking%20observations%20that%20capture%20facade-level%20structure%2C%20offering%20a%20promising%20alternative%20to%20conventional%20optical%20methods.%20However%2C%20TomoSAR%20point%20clouds%20often%20suffer%20from%20noise%2C%20anisotropic%20point%20distributions%2C%20and%20data%20voids%20on%20incoherent%20surfaces%2C%20all%20of%20which%20hinder%20accurate%20height%20reconstruction.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20learning-based%20framework%20for%20converting%20raw%20TomoSAR%20points%20into%20high-resolution%20building%20height%20maps.%20Our%20dual-topology%20network%20alternates%20between%20a%20point%20branch%20that%20models%20irregular%20scatterer%20features%20and%20a%20grid%20branch%20that%20enforces%20spatial%20consistency.%20By%20jointly%20processing%20these%20representations%2C%20the%20network%20denoises%20the%20input%20points%20and%20inpaints%20missing%20regions%20to%20produce%20continuous%20height%20estimates.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20proof%20of%20concept%20for%20large-scale%20urban%20height%20mapping%20directly%20from%20TomoSAR%20point%20clouds.%20Extensive%20experiments%20on%20data%20from%20Munich%20and%20Berlin%20validate%20the%20effectiveness%20of%20our%20approach.%20Moreover%2C%20we%20demonstrate%20that%20our%20framework%20can%20be%20extended%20to%20incorporate%20optical%20satellite%20imagery%2C%20further%20enhancing%20reconstruction%20quality.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/zhu-xlab/tomosar2height.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00658v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReconstructing%2520Building%2520Height%2520from%2520Spaceborne%2520TomoSAR%2520Point%2520Clouds%2520Using%2520a%2520Dual-Topology%2520Network%26entry.906535625%3DZhaiyu%2520Chen%2520and%2520Yuanyuan%2520Wang%2520and%2520Yilei%2520Shi%2520and%2520Xiao%2520Xiang%2520Zhu%26entry.1292438233%3DReliable%2520building%2520height%2520estimation%2520is%2520essential%2520for%2520various%2520urban%2520applications.%2520Spaceborne%2520SAR%2520tomography%2520%2528TomoSAR%2529%2520provides%2520weather-independent%252C%2520side-looking%2520observations%2520that%2520capture%2520facade-level%2520structure%252C%2520offering%2520a%2520promising%2520alternative%2520to%2520conventional%2520optical%2520methods.%2520However%252C%2520TomoSAR%2520point%2520clouds%2520often%2520suffer%2520from%2520noise%252C%2520anisotropic%2520point%2520distributions%252C%2520and%2520data%2520voids%2520on%2520incoherent%2520surfaces%252C%2520all%2520of%2520which%2520hinder%2520accurate%2520height%2520reconstruction.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520learning-based%2520framework%2520for%2520converting%2520raw%2520TomoSAR%2520points%2520into%2520high-resolution%2520building%2520height%2520maps.%2520Our%2520dual-topology%2520network%2520alternates%2520between%2520a%2520point%2520branch%2520that%2520models%2520irregular%2520scatterer%2520features%2520and%2520a%2520grid%2520branch%2520that%2520enforces%2520spatial%2520consistency.%2520By%2520jointly%2520processing%2520these%2520representations%252C%2520the%2520network%2520denoises%2520the%2520input%2520points%2520and%2520inpaints%2520missing%2520regions%2520to%2520produce%2520continuous%2520height%2520estimates.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520proof%2520of%2520concept%2520for%2520large-scale%2520urban%2520height%2520mapping%2520directly%2520from%2520TomoSAR%2520point%2520clouds.%2520Extensive%2520experiments%2520on%2520data%2520from%2520Munich%2520and%2520Berlin%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach.%2520Moreover%252C%2520we%2520demonstrate%2520that%2520our%2520framework%2520can%2520be%2520extended%2520to%2520incorporate%2520optical%2520satellite%2520imagery%252C%2520further%2520enhancing%2520reconstruction%2520quality.%2520The%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/zhu-xlab/tomosar2height.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00658v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reconstructing%20Building%20Height%20from%20Spaceborne%20TomoSAR%20Point%20Clouds%20Using%20a%20Dual-Topology%20Network&entry.906535625=Zhaiyu%20Chen%20and%20Yuanyuan%20Wang%20and%20Yilei%20Shi%20and%20Xiao%20Xiang%20Zhu&entry.1292438233=Reliable%20building%20height%20estimation%20is%20essential%20for%20various%20urban%20applications.%20Spaceborne%20SAR%20tomography%20%28TomoSAR%29%20provides%20weather-independent%2C%20side-looking%20observations%20that%20capture%20facade-level%20structure%2C%20offering%20a%20promising%20alternative%20to%20conventional%20optical%20methods.%20However%2C%20TomoSAR%20point%20clouds%20often%20suffer%20from%20noise%2C%20anisotropic%20point%20distributions%2C%20and%20data%20voids%20on%20incoherent%20surfaces%2C%20all%20of%20which%20hinder%20accurate%20height%20reconstruction.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20learning-based%20framework%20for%20converting%20raw%20TomoSAR%20points%20into%20high-resolution%20building%20height%20maps.%20Our%20dual-topology%20network%20alternates%20between%20a%20point%20branch%20that%20models%20irregular%20scatterer%20features%20and%20a%20grid%20branch%20that%20enforces%20spatial%20consistency.%20By%20jointly%20processing%20these%20representations%2C%20the%20network%20denoises%20the%20input%20points%20and%20inpaints%20missing%20regions%20to%20produce%20continuous%20height%20estimates.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20proof%20of%20concept%20for%20large-scale%20urban%20height%20mapping%20directly%20from%20TomoSAR%20point%20clouds.%20Extensive%20experiments%20on%20data%20from%20Munich%20and%20Berlin%20validate%20the%20effectiveness%20of%20our%20approach.%20Moreover%2C%20we%20demonstrate%20that%20our%20framework%20can%20be%20extended%20to%20incorporate%20optical%20satellite%20imagery%2C%20further%20enhancing%20reconstruction%20quality.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/zhu-xlab/tomosar2height.&entry.1838667208=http%3A//arxiv.org/abs/2601.00658v1&entry.124074799=Read"},
{"title": "Multi-Level Feature Fusion for Continual Learning in Visual Quality Inspection", "author": "Johannes C. Bauer and Paul Geng and Stephan Trattnig and Petr Dokl\u00e1dal and R\u00fcdiger Daub", "abstract": "Deep neural networks show great potential for automating various visual quality inspection tasks in manufacturing. However, their applicability is limited in more volatile scenarios, such as remanufacturing, where the inspected products and defect patterns often change. In such settings, deployed models require frequent adaptation to novel conditions, effectively posing a continual learning problem. To enable quick adaptation, the necessary training processes must be computationally efficient while still avoiding effects like catastrophic forgetting. This work presents a multi-level feature fusion (MLFF) approach that aims to improve both aspects simultaneously by utilizing representations from different depths of a pretrained network. We show that our approach is able to match the performance of end-to-end training for different quality inspection problems while using significantly less trainable parameters. Furthermore, it reduces catastrophic forgetting and improves generalization robustness to new product types or defects.", "link": "http://arxiv.org/abs/2601.00725v1", "date": "2026-01-02", "relevancy": 2.6341, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.534}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5238}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Level%20Feature%20Fusion%20for%20Continual%20Learning%20in%20Visual%20Quality%20Inspection&body=Title%3A%20Multi-Level%20Feature%20Fusion%20for%20Continual%20Learning%20in%20Visual%20Quality%20Inspection%0AAuthor%3A%20Johannes%20C.%20Bauer%20and%20Paul%20Geng%20and%20Stephan%20Trattnig%20and%20Petr%20Dokl%C3%A1dal%20and%20R%C3%BCdiger%20Daub%0AAbstract%3A%20Deep%20neural%20networks%20show%20great%20potential%20for%20automating%20various%20visual%20quality%20inspection%20tasks%20in%20manufacturing.%20However%2C%20their%20applicability%20is%20limited%20in%20more%20volatile%20scenarios%2C%20such%20as%20remanufacturing%2C%20where%20the%20inspected%20products%20and%20defect%20patterns%20often%20change.%20In%20such%20settings%2C%20deployed%20models%20require%20frequent%20adaptation%20to%20novel%20conditions%2C%20effectively%20posing%20a%20continual%20learning%20problem.%20To%20enable%20quick%20adaptation%2C%20the%20necessary%20training%20processes%20must%20be%20computationally%20efficient%20while%20still%20avoiding%20effects%20like%20catastrophic%20forgetting.%20This%20work%20presents%20a%20multi-level%20feature%20fusion%20%28MLFF%29%20approach%20that%20aims%20to%20improve%20both%20aspects%20simultaneously%20by%20utilizing%20representations%20from%20different%20depths%20of%20a%20pretrained%20network.%20We%20show%20that%20our%20approach%20is%20able%20to%20match%20the%20performance%20of%20end-to-end%20training%20for%20different%20quality%20inspection%20problems%20while%20using%20significantly%20less%20trainable%20parameters.%20Furthermore%2C%20it%20reduces%20catastrophic%20forgetting%20and%20improves%20generalization%20robustness%20to%20new%20product%20types%20or%20defects.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00725v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Level%2520Feature%2520Fusion%2520for%2520Continual%2520Learning%2520in%2520Visual%2520Quality%2520Inspection%26entry.906535625%3DJohannes%2520C.%2520Bauer%2520and%2520Paul%2520Geng%2520and%2520Stephan%2520Trattnig%2520and%2520Petr%2520Dokl%25C3%25A1dal%2520and%2520R%25C3%25BCdiger%2520Daub%26entry.1292438233%3DDeep%2520neural%2520networks%2520show%2520great%2520potential%2520for%2520automating%2520various%2520visual%2520quality%2520inspection%2520tasks%2520in%2520manufacturing.%2520However%252C%2520their%2520applicability%2520is%2520limited%2520in%2520more%2520volatile%2520scenarios%252C%2520such%2520as%2520remanufacturing%252C%2520where%2520the%2520inspected%2520products%2520and%2520defect%2520patterns%2520often%2520change.%2520In%2520such%2520settings%252C%2520deployed%2520models%2520require%2520frequent%2520adaptation%2520to%2520novel%2520conditions%252C%2520effectively%2520posing%2520a%2520continual%2520learning%2520problem.%2520To%2520enable%2520quick%2520adaptation%252C%2520the%2520necessary%2520training%2520processes%2520must%2520be%2520computationally%2520efficient%2520while%2520still%2520avoiding%2520effects%2520like%2520catastrophic%2520forgetting.%2520This%2520work%2520presents%2520a%2520multi-level%2520feature%2520fusion%2520%2528MLFF%2529%2520approach%2520that%2520aims%2520to%2520improve%2520both%2520aspects%2520simultaneously%2520by%2520utilizing%2520representations%2520from%2520different%2520depths%2520of%2520a%2520pretrained%2520network.%2520We%2520show%2520that%2520our%2520approach%2520is%2520able%2520to%2520match%2520the%2520performance%2520of%2520end-to-end%2520training%2520for%2520different%2520quality%2520inspection%2520problems%2520while%2520using%2520significantly%2520less%2520trainable%2520parameters.%2520Furthermore%252C%2520it%2520reduces%2520catastrophic%2520forgetting%2520and%2520improves%2520generalization%2520robustness%2520to%2520new%2520product%2520types%2520or%2520defects.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00725v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Level%20Feature%20Fusion%20for%20Continual%20Learning%20in%20Visual%20Quality%20Inspection&entry.906535625=Johannes%20C.%20Bauer%20and%20Paul%20Geng%20and%20Stephan%20Trattnig%20and%20Petr%20Dokl%C3%A1dal%20and%20R%C3%BCdiger%20Daub&entry.1292438233=Deep%20neural%20networks%20show%20great%20potential%20for%20automating%20various%20visual%20quality%20inspection%20tasks%20in%20manufacturing.%20However%2C%20their%20applicability%20is%20limited%20in%20more%20volatile%20scenarios%2C%20such%20as%20remanufacturing%2C%20where%20the%20inspected%20products%20and%20defect%20patterns%20often%20change.%20In%20such%20settings%2C%20deployed%20models%20require%20frequent%20adaptation%20to%20novel%20conditions%2C%20effectively%20posing%20a%20continual%20learning%20problem.%20To%20enable%20quick%20adaptation%2C%20the%20necessary%20training%20processes%20must%20be%20computationally%20efficient%20while%20still%20avoiding%20effects%20like%20catastrophic%20forgetting.%20This%20work%20presents%20a%20multi-level%20feature%20fusion%20%28MLFF%29%20approach%20that%20aims%20to%20improve%20both%20aspects%20simultaneously%20by%20utilizing%20representations%20from%20different%20depths%20of%20a%20pretrained%20network.%20We%20show%20that%20our%20approach%20is%20able%20to%20match%20the%20performance%20of%20end-to-end%20training%20for%20different%20quality%20inspection%20problems%20while%20using%20significantly%20less%20trainable%20parameters.%20Furthermore%2C%20it%20reduces%20catastrophic%20forgetting%20and%20improves%20generalization%20robustness%20to%20new%20product%20types%20or%20defects.&entry.1838667208=http%3A//arxiv.org/abs/2601.00725v1&entry.124074799=Read"},
{"title": "JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation", "author": "Kai Liu and Jungang Li and Yuchong Sun and Shengqiong Wu and Jianzhang Gao and Daoan Zhang and Wei Zhang and Sheng Jin and Sicheng Yu and Geng Zhan and Jiayi Ji and Fan Zhou and Liang Zheng and Shuicheng Yan and Hao Fei and Tat-Seng Chua", "abstract": "This paper presents JavisGPT, the first unified multimodal large language model (MLLM) for joint audio-video (JAV) comprehension and generation. JavisGPT has a concise encoder-LLM-decoder architecture, which has a SyncFusion module for spatio-temporal audio-video fusion and synchrony-aware learnable queries to bridge a pretrained JAV-DiT generator. This design enables temporally coherent video-audio understanding and generation from multimodal instructions. We design an effective three-stage training pipeline consisting of multimodal pretraining, audio-video fine-tuning, and large-scale instruction-tuning, to progressively build multimodal comprehension and generation from existing vision-language models. For instruction tuning, we construct JavisInst-Omni, a high-quality instruction dataset with over 200K GPT-4o-curated audio-video-text dialogues that cover diverse and multi-level comprehension and generation scenarios. On JAV comprehension and generation benchmarks, our experiments show that JavisGPT outperforms existing MLLMs, particularly in complex and temporally synchronized settings.", "link": "http://arxiv.org/abs/2512.22905v2", "date": "2026-01-02", "relevancy": 2.62, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5437}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5229}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5054}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JavisGPT%3A%20A%20Unified%20Multi-modal%20LLM%20for%20Sounding-Video%20Comprehension%20and%20Generation&body=Title%3A%20JavisGPT%3A%20A%20Unified%20Multi-modal%20LLM%20for%20Sounding-Video%20Comprehension%20and%20Generation%0AAuthor%3A%20Kai%20Liu%20and%20Jungang%20Li%20and%20Yuchong%20Sun%20and%20Shengqiong%20Wu%20and%20Jianzhang%20Gao%20and%20Daoan%20Zhang%20and%20Wei%20Zhang%20and%20Sheng%20Jin%20and%20Sicheng%20Yu%20and%20Geng%20Zhan%20and%20Jiayi%20Ji%20and%20Fan%20Zhou%20and%20Liang%20Zheng%20and%20Shuicheng%20Yan%20and%20Hao%20Fei%20and%20Tat-Seng%20Chua%0AAbstract%3A%20This%20paper%20presents%20JavisGPT%2C%20the%20first%20unified%20multimodal%20large%20language%20model%20%28MLLM%29%20for%20joint%20audio-video%20%28JAV%29%20comprehension%20and%20generation.%20JavisGPT%20has%20a%20concise%20encoder-LLM-decoder%20architecture%2C%20which%20has%20a%20SyncFusion%20module%20for%20spatio-temporal%20audio-video%20fusion%20and%20synchrony-aware%20learnable%20queries%20to%20bridge%20a%20pretrained%20JAV-DiT%20generator.%20This%20design%20enables%20temporally%20coherent%20video-audio%20understanding%20and%20generation%20from%20multimodal%20instructions.%20We%20design%20an%20effective%20three-stage%20training%20pipeline%20consisting%20of%20multimodal%20pretraining%2C%20audio-video%20fine-tuning%2C%20and%20large-scale%20instruction-tuning%2C%20to%20progressively%20build%20multimodal%20comprehension%20and%20generation%20from%20existing%20vision-language%20models.%20For%20instruction%20tuning%2C%20we%20construct%20JavisInst-Omni%2C%20a%20high-quality%20instruction%20dataset%20with%20over%20200K%20GPT-4o-curated%20audio-video-text%20dialogues%20that%20cover%20diverse%20and%20multi-level%20comprehension%20and%20generation%20scenarios.%20On%20JAV%20comprehension%20and%20generation%20benchmarks%2C%20our%20experiments%20show%20that%20JavisGPT%20outperforms%20existing%20MLLMs%2C%20particularly%20in%20complex%20and%20temporally%20synchronized%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22905v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJavisGPT%253A%2520A%2520Unified%2520Multi-modal%2520LLM%2520for%2520Sounding-Video%2520Comprehension%2520and%2520Generation%26entry.906535625%3DKai%2520Liu%2520and%2520Jungang%2520Li%2520and%2520Yuchong%2520Sun%2520and%2520Shengqiong%2520Wu%2520and%2520Jianzhang%2520Gao%2520and%2520Daoan%2520Zhang%2520and%2520Wei%2520Zhang%2520and%2520Sheng%2520Jin%2520and%2520Sicheng%2520Yu%2520and%2520Geng%2520Zhan%2520and%2520Jiayi%2520Ji%2520and%2520Fan%2520Zhou%2520and%2520Liang%2520Zheng%2520and%2520Shuicheng%2520Yan%2520and%2520Hao%2520Fei%2520and%2520Tat-Seng%2520Chua%26entry.1292438233%3DThis%2520paper%2520presents%2520JavisGPT%252C%2520the%2520first%2520unified%2520multimodal%2520large%2520language%2520model%2520%2528MLLM%2529%2520for%2520joint%2520audio-video%2520%2528JAV%2529%2520comprehension%2520and%2520generation.%2520JavisGPT%2520has%2520a%2520concise%2520encoder-LLM-decoder%2520architecture%252C%2520which%2520has%2520a%2520SyncFusion%2520module%2520for%2520spatio-temporal%2520audio-video%2520fusion%2520and%2520synchrony-aware%2520learnable%2520queries%2520to%2520bridge%2520a%2520pretrained%2520JAV-DiT%2520generator.%2520This%2520design%2520enables%2520temporally%2520coherent%2520video-audio%2520understanding%2520and%2520generation%2520from%2520multimodal%2520instructions.%2520We%2520design%2520an%2520effective%2520three-stage%2520training%2520pipeline%2520consisting%2520of%2520multimodal%2520pretraining%252C%2520audio-video%2520fine-tuning%252C%2520and%2520large-scale%2520instruction-tuning%252C%2520to%2520progressively%2520build%2520multimodal%2520comprehension%2520and%2520generation%2520from%2520existing%2520vision-language%2520models.%2520For%2520instruction%2520tuning%252C%2520we%2520construct%2520JavisInst-Omni%252C%2520a%2520high-quality%2520instruction%2520dataset%2520with%2520over%2520200K%2520GPT-4o-curated%2520audio-video-text%2520dialogues%2520that%2520cover%2520diverse%2520and%2520multi-level%2520comprehension%2520and%2520generation%2520scenarios.%2520On%2520JAV%2520comprehension%2520and%2520generation%2520benchmarks%252C%2520our%2520experiments%2520show%2520that%2520JavisGPT%2520outperforms%2520existing%2520MLLMs%252C%2520particularly%2520in%2520complex%2520and%2520temporally%2520synchronized%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22905v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JavisGPT%3A%20A%20Unified%20Multi-modal%20LLM%20for%20Sounding-Video%20Comprehension%20and%20Generation&entry.906535625=Kai%20Liu%20and%20Jungang%20Li%20and%20Yuchong%20Sun%20and%20Shengqiong%20Wu%20and%20Jianzhang%20Gao%20and%20Daoan%20Zhang%20and%20Wei%20Zhang%20and%20Sheng%20Jin%20and%20Sicheng%20Yu%20and%20Geng%20Zhan%20and%20Jiayi%20Ji%20and%20Fan%20Zhou%20and%20Liang%20Zheng%20and%20Shuicheng%20Yan%20and%20Hao%20Fei%20and%20Tat-Seng%20Chua&entry.1292438233=This%20paper%20presents%20JavisGPT%2C%20the%20first%20unified%20multimodal%20large%20language%20model%20%28MLLM%29%20for%20joint%20audio-video%20%28JAV%29%20comprehension%20and%20generation.%20JavisGPT%20has%20a%20concise%20encoder-LLM-decoder%20architecture%2C%20which%20has%20a%20SyncFusion%20module%20for%20spatio-temporal%20audio-video%20fusion%20and%20synchrony-aware%20learnable%20queries%20to%20bridge%20a%20pretrained%20JAV-DiT%20generator.%20This%20design%20enables%20temporally%20coherent%20video-audio%20understanding%20and%20generation%20from%20multimodal%20instructions.%20We%20design%20an%20effective%20three-stage%20training%20pipeline%20consisting%20of%20multimodal%20pretraining%2C%20audio-video%20fine-tuning%2C%20and%20large-scale%20instruction-tuning%2C%20to%20progressively%20build%20multimodal%20comprehension%20and%20generation%20from%20existing%20vision-language%20models.%20For%20instruction%20tuning%2C%20we%20construct%20JavisInst-Omni%2C%20a%20high-quality%20instruction%20dataset%20with%20over%20200K%20GPT-4o-curated%20audio-video-text%20dialogues%20that%20cover%20diverse%20and%20multi-level%20comprehension%20and%20generation%20scenarios.%20On%20JAV%20comprehension%20and%20generation%20benchmarks%2C%20our%20experiments%20show%20that%20JavisGPT%20outperforms%20existing%20MLLMs%2C%20particularly%20in%20complex%20and%20temporally%20synchronized%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2512.22905v2&entry.124074799=Read"},
{"title": "uGMM-NN: Univariate Gaussian Mixture Model Neural Network", "author": "Zakeria Sharif Ali", "abstract": "This paper introduces the Univariate Gaussian Mixture Model Neural Network (uGMM-NN), a novel neural architecture that embeds probabilistic reasoning directly into the computational units of deep networks. Unlike traditional neurons, which apply weighted sums followed by fixed non-linearities, each uGMM-NN node parameterizes its activations as a univariate Gaussian mixture, with learnable means, variances, and mixing coefficients. This design enables richer representations by capturing multimodality and uncertainty at the level of individual neurons, while retaining the scalability of standard feed-forward networks. We demonstrate that uGMM-NN can achieve competitive discriminative performance compared to conventional multilayer perceptrons, while additionally offering a probabilistic interpretation of activations. The proposed framework provides a foundation for integrating uncertainty-aware components into modern neural architectures, opening new directions for both discriminative and generative modeling.", "link": "http://arxiv.org/abs/2509.07569v2", "date": "2026-01-02", "relevancy": 2.6002, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5448}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.522}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20uGMM-NN%3A%20Univariate%20Gaussian%20Mixture%20Model%20Neural%20Network&body=Title%3A%20uGMM-NN%3A%20Univariate%20Gaussian%20Mixture%20Model%20Neural%20Network%0AAuthor%3A%20Zakeria%20Sharif%20Ali%0AAbstract%3A%20This%20paper%20introduces%20the%20Univariate%20Gaussian%20Mixture%20Model%20Neural%20Network%20%28uGMM-NN%29%2C%20a%20novel%20neural%20architecture%20that%20embeds%20probabilistic%20reasoning%20directly%20into%20the%20computational%20units%20of%20deep%20networks.%20Unlike%20traditional%20neurons%2C%20which%20apply%20weighted%20sums%20followed%20by%20fixed%20non-linearities%2C%20each%20uGMM-NN%20node%20parameterizes%20its%20activations%20as%20a%20univariate%20Gaussian%20mixture%2C%20with%20learnable%20means%2C%20variances%2C%20and%20mixing%20coefficients.%20This%20design%20enables%20richer%20representations%20by%20capturing%20multimodality%20and%20uncertainty%20at%20the%20level%20of%20individual%20neurons%2C%20while%20retaining%20the%20scalability%20of%20standard%20feed-forward%20networks.%20We%20demonstrate%20that%20uGMM-NN%20can%20achieve%20competitive%20discriminative%20performance%20compared%20to%20conventional%20multilayer%20perceptrons%2C%20while%20additionally%20offering%20a%20probabilistic%20interpretation%20of%20activations.%20The%20proposed%20framework%20provides%20a%20foundation%20for%20integrating%20uncertainty-aware%20components%20into%20modern%20neural%20architectures%2C%20opening%20new%20directions%20for%20both%20discriminative%20and%20generative%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2509.07569v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DuGMM-NN%253A%2520Univariate%2520Gaussian%2520Mixture%2520Model%2520Neural%2520Network%26entry.906535625%3DZakeria%2520Sharif%2520Ali%26entry.1292438233%3DThis%2520paper%2520introduces%2520the%2520Univariate%2520Gaussian%2520Mixture%2520Model%2520Neural%2520Network%2520%2528uGMM-NN%2529%252C%2520a%2520novel%2520neural%2520architecture%2520that%2520embeds%2520probabilistic%2520reasoning%2520directly%2520into%2520the%2520computational%2520units%2520of%2520deep%2520networks.%2520Unlike%2520traditional%2520neurons%252C%2520which%2520apply%2520weighted%2520sums%2520followed%2520by%2520fixed%2520non-linearities%252C%2520each%2520uGMM-NN%2520node%2520parameterizes%2520its%2520activations%2520as%2520a%2520univariate%2520Gaussian%2520mixture%252C%2520with%2520learnable%2520means%252C%2520variances%252C%2520and%2520mixing%2520coefficients.%2520This%2520design%2520enables%2520richer%2520representations%2520by%2520capturing%2520multimodality%2520and%2520uncertainty%2520at%2520the%2520level%2520of%2520individual%2520neurons%252C%2520while%2520retaining%2520the%2520scalability%2520of%2520standard%2520feed-forward%2520networks.%2520We%2520demonstrate%2520that%2520uGMM-NN%2520can%2520achieve%2520competitive%2520discriminative%2520performance%2520compared%2520to%2520conventional%2520multilayer%2520perceptrons%252C%2520while%2520additionally%2520offering%2520a%2520probabilistic%2520interpretation%2520of%2520activations.%2520The%2520proposed%2520framework%2520provides%2520a%2520foundation%2520for%2520integrating%2520uncertainty-aware%2520components%2520into%2520modern%2520neural%2520architectures%252C%2520opening%2520new%2520directions%2520for%2520both%2520discriminative%2520and%2520generative%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07569v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=uGMM-NN%3A%20Univariate%20Gaussian%20Mixture%20Model%20Neural%20Network&entry.906535625=Zakeria%20Sharif%20Ali&entry.1292438233=This%20paper%20introduces%20the%20Univariate%20Gaussian%20Mixture%20Model%20Neural%20Network%20%28uGMM-NN%29%2C%20a%20novel%20neural%20architecture%20that%20embeds%20probabilistic%20reasoning%20directly%20into%20the%20computational%20units%20of%20deep%20networks.%20Unlike%20traditional%20neurons%2C%20which%20apply%20weighted%20sums%20followed%20by%20fixed%20non-linearities%2C%20each%20uGMM-NN%20node%20parameterizes%20its%20activations%20as%20a%20univariate%20Gaussian%20mixture%2C%20with%20learnable%20means%2C%20variances%2C%20and%20mixing%20coefficients.%20This%20design%20enables%20richer%20representations%20by%20capturing%20multimodality%20and%20uncertainty%20at%20the%20level%20of%20individual%20neurons%2C%20while%20retaining%20the%20scalability%20of%20standard%20feed-forward%20networks.%20We%20demonstrate%20that%20uGMM-NN%20can%20achieve%20competitive%20discriminative%20performance%20compared%20to%20conventional%20multilayer%20perceptrons%2C%20while%20additionally%20offering%20a%20probabilistic%20interpretation%20of%20activations.%20The%20proposed%20framework%20provides%20a%20foundation%20for%20integrating%20uncertainty-aware%20components%20into%20modern%20neural%20architectures%2C%20opening%20new%20directions%20for%20both%20discriminative%20and%20generative%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2509.07569v2&entry.124074799=Read"},
{"title": "FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing", "author": "Sunny Gupta and Amit Sethi", "abstract": "Federated data sharing promises utility without centralizing raw data, yet existing embedding-level generators struggle under non-IID client heterogeneity and provide limited formal protection against gradient leakage. We propose FedHypeVAE, a differentially private, hypernetwork-driven framework for synthesizing embedding-level data across decentralized clients. Building on a conditional VAE backbone, we replace the single global decoder and fixed latent prior with client-aware decoders and class-conditional priors generated by a shared hypernetwork from private, trainable client codes. This bi-level design personalizes the generative layerrather than the downstream modelwhile decoupling local data from communicated parameters. The shared hypernetwork is optimized under differential privacy, ensuring that only noise-perturbed, clipped gradients are aggregated across clients. A local MMD alignment between real and synthetic embeddings and a Lipschitz regularizer on hypernetwork outputs further enhance stability and distributional coherence under non-IID conditions. After training, a neutral meta-code enables domain agnostic synthesis, while mixtures of meta-codes provide controllable multi-domain coverage. FedHypeVAE unifies personalization, privacy, and distribution alignment at the generator level, establishing a principled foundation for privacy-preserving data synthesis in federated settings. Code: github.com/sunnyinAI/FedHypeVAE", "link": "http://arxiv.org/abs/2601.00785v1", "date": "2026-01-02", "relevancy": 2.5986, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.539}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5134}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedHypeVAE%3A%20Federated%20Learning%20with%20Hypernetwork%20Generated%20Conditional%20VAEs%20for%20Differentially%20Private%20Embedding%20Sharing&body=Title%3A%20FedHypeVAE%3A%20Federated%20Learning%20with%20Hypernetwork%20Generated%20Conditional%20VAEs%20for%20Differentially%20Private%20Embedding%20Sharing%0AAuthor%3A%20Sunny%20Gupta%20and%20Amit%20Sethi%0AAbstract%3A%20Federated%20data%20sharing%20promises%20utility%20without%20centralizing%20raw%20data%2C%20yet%20existing%20embedding-level%20generators%20struggle%20under%20non-IID%20client%20heterogeneity%20and%20provide%20limited%20formal%20protection%20against%20gradient%20leakage.%20We%20propose%20FedHypeVAE%2C%20a%20differentially%20private%2C%20hypernetwork-driven%20framework%20for%20synthesizing%20embedding-level%20data%20across%20decentralized%20clients.%20Building%20on%20a%20conditional%20VAE%20backbone%2C%20we%20replace%20the%20single%20global%20decoder%20and%20fixed%20latent%20prior%20with%20client-aware%20decoders%20and%20class-conditional%20priors%20generated%20by%20a%20shared%20hypernetwork%20from%20private%2C%20trainable%20client%20codes.%20This%20bi-level%20design%20personalizes%20the%20generative%20layerrather%20than%20the%20downstream%20modelwhile%20decoupling%20local%20data%20from%20communicated%20parameters.%20The%20shared%20hypernetwork%20is%20optimized%20under%20differential%20privacy%2C%20ensuring%20that%20only%20noise-perturbed%2C%20clipped%20gradients%20are%20aggregated%20across%20clients.%20A%20local%20MMD%20alignment%20between%20real%20and%20synthetic%20embeddings%20and%20a%20Lipschitz%20regularizer%20on%20hypernetwork%20outputs%20further%20enhance%20stability%20and%20distributional%20coherence%20under%20non-IID%20conditions.%20After%20training%2C%20a%20neutral%20meta-code%20enables%20domain%20agnostic%20synthesis%2C%20while%20mixtures%20of%20meta-codes%20provide%20controllable%20multi-domain%20coverage.%20FedHypeVAE%20unifies%20personalization%2C%20privacy%2C%20and%20distribution%20alignment%20at%20the%20generator%20level%2C%20establishing%20a%20principled%20foundation%20for%20privacy-preserving%20data%20synthesis%20in%20federated%20settings.%20Code%3A%20github.com/sunnyinAI/FedHypeVAE%0ALink%3A%20http%3A//arxiv.org/abs/2601.00785v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedHypeVAE%253A%2520Federated%2520Learning%2520with%2520Hypernetwork%2520Generated%2520Conditional%2520VAEs%2520for%2520Differentially%2520Private%2520Embedding%2520Sharing%26entry.906535625%3DSunny%2520Gupta%2520and%2520Amit%2520Sethi%26entry.1292438233%3DFederated%2520data%2520sharing%2520promises%2520utility%2520without%2520centralizing%2520raw%2520data%252C%2520yet%2520existing%2520embedding-level%2520generators%2520struggle%2520under%2520non-IID%2520client%2520heterogeneity%2520and%2520provide%2520limited%2520formal%2520protection%2520against%2520gradient%2520leakage.%2520We%2520propose%2520FedHypeVAE%252C%2520a%2520differentially%2520private%252C%2520hypernetwork-driven%2520framework%2520for%2520synthesizing%2520embedding-level%2520data%2520across%2520decentralized%2520clients.%2520Building%2520on%2520a%2520conditional%2520VAE%2520backbone%252C%2520we%2520replace%2520the%2520single%2520global%2520decoder%2520and%2520fixed%2520latent%2520prior%2520with%2520client-aware%2520decoders%2520and%2520class-conditional%2520priors%2520generated%2520by%2520a%2520shared%2520hypernetwork%2520from%2520private%252C%2520trainable%2520client%2520codes.%2520This%2520bi-level%2520design%2520personalizes%2520the%2520generative%2520layerrather%2520than%2520the%2520downstream%2520modelwhile%2520decoupling%2520local%2520data%2520from%2520communicated%2520parameters.%2520The%2520shared%2520hypernetwork%2520is%2520optimized%2520under%2520differential%2520privacy%252C%2520ensuring%2520that%2520only%2520noise-perturbed%252C%2520clipped%2520gradients%2520are%2520aggregated%2520across%2520clients.%2520A%2520local%2520MMD%2520alignment%2520between%2520real%2520and%2520synthetic%2520embeddings%2520and%2520a%2520Lipschitz%2520regularizer%2520on%2520hypernetwork%2520outputs%2520further%2520enhance%2520stability%2520and%2520distributional%2520coherence%2520under%2520non-IID%2520conditions.%2520After%2520training%252C%2520a%2520neutral%2520meta-code%2520enables%2520domain%2520agnostic%2520synthesis%252C%2520while%2520mixtures%2520of%2520meta-codes%2520provide%2520controllable%2520multi-domain%2520coverage.%2520FedHypeVAE%2520unifies%2520personalization%252C%2520privacy%252C%2520and%2520distribution%2520alignment%2520at%2520the%2520generator%2520level%252C%2520establishing%2520a%2520principled%2520foundation%2520for%2520privacy-preserving%2520data%2520synthesis%2520in%2520federated%2520settings.%2520Code%253A%2520github.com/sunnyinAI/FedHypeVAE%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00785v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedHypeVAE%3A%20Federated%20Learning%20with%20Hypernetwork%20Generated%20Conditional%20VAEs%20for%20Differentially%20Private%20Embedding%20Sharing&entry.906535625=Sunny%20Gupta%20and%20Amit%20Sethi&entry.1292438233=Federated%20data%20sharing%20promises%20utility%20without%20centralizing%20raw%20data%2C%20yet%20existing%20embedding-level%20generators%20struggle%20under%20non-IID%20client%20heterogeneity%20and%20provide%20limited%20formal%20protection%20against%20gradient%20leakage.%20We%20propose%20FedHypeVAE%2C%20a%20differentially%20private%2C%20hypernetwork-driven%20framework%20for%20synthesizing%20embedding-level%20data%20across%20decentralized%20clients.%20Building%20on%20a%20conditional%20VAE%20backbone%2C%20we%20replace%20the%20single%20global%20decoder%20and%20fixed%20latent%20prior%20with%20client-aware%20decoders%20and%20class-conditional%20priors%20generated%20by%20a%20shared%20hypernetwork%20from%20private%2C%20trainable%20client%20codes.%20This%20bi-level%20design%20personalizes%20the%20generative%20layerrather%20than%20the%20downstream%20modelwhile%20decoupling%20local%20data%20from%20communicated%20parameters.%20The%20shared%20hypernetwork%20is%20optimized%20under%20differential%20privacy%2C%20ensuring%20that%20only%20noise-perturbed%2C%20clipped%20gradients%20are%20aggregated%20across%20clients.%20A%20local%20MMD%20alignment%20between%20real%20and%20synthetic%20embeddings%20and%20a%20Lipschitz%20regularizer%20on%20hypernetwork%20outputs%20further%20enhance%20stability%20and%20distributional%20coherence%20under%20non-IID%20conditions.%20After%20training%2C%20a%20neutral%20meta-code%20enables%20domain%20agnostic%20synthesis%2C%20while%20mixtures%20of%20meta-codes%20provide%20controllable%20multi-domain%20coverage.%20FedHypeVAE%20unifies%20personalization%2C%20privacy%2C%20and%20distribution%20alignment%20at%20the%20generator%20level%2C%20establishing%20a%20principled%20foundation%20for%20privacy-preserving%20data%20synthesis%20in%20federated%20settings.%20Code%3A%20github.com/sunnyinAI/FedHypeVAE&entry.1838667208=http%3A//arxiv.org/abs/2601.00785v1&entry.124074799=Read"},
{"title": "Unified Primitive Proxies for Structured Shape Completion", "author": "Zhaiyu Chen and Yuqing Wang and Xiao Xiang Zhu", "abstract": "Structured shape completion recovers missing geometry as primitives rather than as unstructured points, which enables primitive-based surface reconstruction. Instead of following the prevailing cascade, we rethink how primitives and points should interact, and find it more effective to decode primitives in a dedicated pathway that attends to shared shape features. Following this principle, we present UniCo, which in a single feed-forward pass predicts a set of primitives with complete geometry, semantics, and inlier membership. To drive this unified representation, we introduce primitive proxies, learnable queries that are contextualized to produce assembly-ready outputs. To ensure consistent optimization, our training strategy couples primitives and points with online target updates. Across synthetic and real-world benchmarks with four independent assembly solvers, UniCo consistently outperforms recent baselines, lowering Chamfer distance by up to 50% and improving normal consistency by up to 7%. These results establish an attractive recipe for structured 3D understanding from incomplete data. Project page: https://unico-completion.github.io.", "link": "http://arxiv.org/abs/2601.00759v1", "date": "2026-01-02", "relevancy": 2.5767, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5175}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5175}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Primitive%20Proxies%20for%20Structured%20Shape%20Completion&body=Title%3A%20Unified%20Primitive%20Proxies%20for%20Structured%20Shape%20Completion%0AAuthor%3A%20Zhaiyu%20Chen%20and%20Yuqing%20Wang%20and%20Xiao%20Xiang%20Zhu%0AAbstract%3A%20Structured%20shape%20completion%20recovers%20missing%20geometry%20as%20primitives%20rather%20than%20as%20unstructured%20points%2C%20which%20enables%20primitive-based%20surface%20reconstruction.%20Instead%20of%20following%20the%20prevailing%20cascade%2C%20we%20rethink%20how%20primitives%20and%20points%20should%20interact%2C%20and%20find%20it%20more%20effective%20to%20decode%20primitives%20in%20a%20dedicated%20pathway%20that%20attends%20to%20shared%20shape%20features.%20Following%20this%20principle%2C%20we%20present%20UniCo%2C%20which%20in%20a%20single%20feed-forward%20pass%20predicts%20a%20set%20of%20primitives%20with%20complete%20geometry%2C%20semantics%2C%20and%20inlier%20membership.%20To%20drive%20this%20unified%20representation%2C%20we%20introduce%20primitive%20proxies%2C%20learnable%20queries%20that%20are%20contextualized%20to%20produce%20assembly-ready%20outputs.%20To%20ensure%20consistent%20optimization%2C%20our%20training%20strategy%20couples%20primitives%20and%20points%20with%20online%20target%20updates.%20Across%20synthetic%20and%20real-world%20benchmarks%20with%20four%20independent%20assembly%20solvers%2C%20UniCo%20consistently%20outperforms%20recent%20baselines%2C%20lowering%20Chamfer%20distance%20by%20up%20to%2050%25%20and%20improving%20normal%20consistency%20by%20up%20to%207%25.%20These%20results%20establish%20an%20attractive%20recipe%20for%20structured%203D%20understanding%20from%20incomplete%20data.%20Project%20page%3A%20https%3A//unico-completion.github.io.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00759v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Primitive%2520Proxies%2520for%2520Structured%2520Shape%2520Completion%26entry.906535625%3DZhaiyu%2520Chen%2520and%2520Yuqing%2520Wang%2520and%2520Xiao%2520Xiang%2520Zhu%26entry.1292438233%3DStructured%2520shape%2520completion%2520recovers%2520missing%2520geometry%2520as%2520primitives%2520rather%2520than%2520as%2520unstructured%2520points%252C%2520which%2520enables%2520primitive-based%2520surface%2520reconstruction.%2520Instead%2520of%2520following%2520the%2520prevailing%2520cascade%252C%2520we%2520rethink%2520how%2520primitives%2520and%2520points%2520should%2520interact%252C%2520and%2520find%2520it%2520more%2520effective%2520to%2520decode%2520primitives%2520in%2520a%2520dedicated%2520pathway%2520that%2520attends%2520to%2520shared%2520shape%2520features.%2520Following%2520this%2520principle%252C%2520we%2520present%2520UniCo%252C%2520which%2520in%2520a%2520single%2520feed-forward%2520pass%2520predicts%2520a%2520set%2520of%2520primitives%2520with%2520complete%2520geometry%252C%2520semantics%252C%2520and%2520inlier%2520membership.%2520To%2520drive%2520this%2520unified%2520representation%252C%2520we%2520introduce%2520primitive%2520proxies%252C%2520learnable%2520queries%2520that%2520are%2520contextualized%2520to%2520produce%2520assembly-ready%2520outputs.%2520To%2520ensure%2520consistent%2520optimization%252C%2520our%2520training%2520strategy%2520couples%2520primitives%2520and%2520points%2520with%2520online%2520target%2520updates.%2520Across%2520synthetic%2520and%2520real-world%2520benchmarks%2520with%2520four%2520independent%2520assembly%2520solvers%252C%2520UniCo%2520consistently%2520outperforms%2520recent%2520baselines%252C%2520lowering%2520Chamfer%2520distance%2520by%2520up%2520to%252050%2525%2520and%2520improving%2520normal%2520consistency%2520by%2520up%2520to%25207%2525.%2520These%2520results%2520establish%2520an%2520attractive%2520recipe%2520for%2520structured%25203D%2520understanding%2520from%2520incomplete%2520data.%2520Project%2520page%253A%2520https%253A//unico-completion.github.io.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00759v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Primitive%20Proxies%20for%20Structured%20Shape%20Completion&entry.906535625=Zhaiyu%20Chen%20and%20Yuqing%20Wang%20and%20Xiao%20Xiang%20Zhu&entry.1292438233=Structured%20shape%20completion%20recovers%20missing%20geometry%20as%20primitives%20rather%20than%20as%20unstructured%20points%2C%20which%20enables%20primitive-based%20surface%20reconstruction.%20Instead%20of%20following%20the%20prevailing%20cascade%2C%20we%20rethink%20how%20primitives%20and%20points%20should%20interact%2C%20and%20find%20it%20more%20effective%20to%20decode%20primitives%20in%20a%20dedicated%20pathway%20that%20attends%20to%20shared%20shape%20features.%20Following%20this%20principle%2C%20we%20present%20UniCo%2C%20which%20in%20a%20single%20feed-forward%20pass%20predicts%20a%20set%20of%20primitives%20with%20complete%20geometry%2C%20semantics%2C%20and%20inlier%20membership.%20To%20drive%20this%20unified%20representation%2C%20we%20introduce%20primitive%20proxies%2C%20learnable%20queries%20that%20are%20contextualized%20to%20produce%20assembly-ready%20outputs.%20To%20ensure%20consistent%20optimization%2C%20our%20training%20strategy%20couples%20primitives%20and%20points%20with%20online%20target%20updates.%20Across%20synthetic%20and%20real-world%20benchmarks%20with%20four%20independent%20assembly%20solvers%2C%20UniCo%20consistently%20outperforms%20recent%20baselines%2C%20lowering%20Chamfer%20distance%20by%20up%20to%2050%25%20and%20improving%20normal%20consistency%20by%20up%20to%207%25.%20These%20results%20establish%20an%20attractive%20recipe%20for%20structured%203D%20understanding%20from%20incomplete%20data.%20Project%20page%3A%20https%3A//unico-completion.github.io.&entry.1838667208=http%3A//arxiv.org/abs/2601.00759v1&entry.124074799=Read"},
{"title": "EXAONE Deep: Reasoning Enhanced Language Models", "author": "Kyunghoon Bae and Eunbi Choi and Kibong Choi and Stanley Jungkyu Choi and Yemuk Choi and Seokhee Hong and Junwon Hwang and Hyojin Jeon and Kijeong Jeon and Gerrard Jeongwon Jo and Hyunjik Jo and Jiyeon Jung and Hyosang Kim and Joonkee Kim and Seonghwan Kim and Soyeon Kim and Sunkyoung Kim and Yireun Kim and Yongil Kim and Youchul Kim and Edward Hwayoung Lee and Haeju Lee and Honglak Lee and Jinsik Lee and Kyungmin Lee and Sangha Park and Yongmin Park and Sihoon Yang and Heuiyeen Yeen and Sihyuk Yi and Hyeongu Yun", "abstract": "We present EXAONE Deep series, which exhibits superior capabilities in various reasoning tasks, including math and coding benchmarks. We train our models mainly on the reasoning-specialized dataset that incorporates long streams of thought processes. Evaluation results show that our smaller models, EXAONE Deep 2.4B and 7.8B, outperform other models of comparable size, while the largest model, EXAONE Deep 32B, demonstrates competitive performance against leading open-weight models. All EXAONE Deep models are openly available for research purposes and can be downloaded from https://huggingface.co/LGAI-EXAONE.", "link": "http://arxiv.org/abs/2503.12524v3", "date": "2026-01-02", "relevancy": 2.5702, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5342}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5342}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EXAONE%20Deep%3A%20Reasoning%20Enhanced%20Language%20Models&body=Title%3A%20EXAONE%20Deep%3A%20Reasoning%20Enhanced%20Language%20Models%0AAuthor%3A%20Kyunghoon%20Bae%20and%20Eunbi%20Choi%20and%20Kibong%20Choi%20and%20Stanley%20Jungkyu%20Choi%20and%20Yemuk%20Choi%20and%20Seokhee%20Hong%20and%20Junwon%20Hwang%20and%20Hyojin%20Jeon%20and%20Kijeong%20Jeon%20and%20Gerrard%20Jeongwon%20Jo%20and%20Hyunjik%20Jo%20and%20Jiyeon%20Jung%20and%20Hyosang%20Kim%20and%20Joonkee%20Kim%20and%20Seonghwan%20Kim%20and%20Soyeon%20Kim%20and%20Sunkyoung%20Kim%20and%20Yireun%20Kim%20and%20Yongil%20Kim%20and%20Youchul%20Kim%20and%20Edward%20Hwayoung%20Lee%20and%20Haeju%20Lee%20and%20Honglak%20Lee%20and%20Jinsik%20Lee%20and%20Kyungmin%20Lee%20and%20Sangha%20Park%20and%20Yongmin%20Park%20and%20Sihoon%20Yang%20and%20Heuiyeen%20Yeen%20and%20Sihyuk%20Yi%20and%20Hyeongu%20Yun%0AAbstract%3A%20We%20present%20EXAONE%20Deep%20series%2C%20which%20exhibits%20superior%20capabilities%20in%20various%20reasoning%20tasks%2C%20including%20math%20and%20coding%20benchmarks.%20We%20train%20our%20models%20mainly%20on%20the%20reasoning-specialized%20dataset%20that%20incorporates%20long%20streams%20of%20thought%20processes.%20Evaluation%20results%20show%20that%20our%20smaller%20models%2C%20EXAONE%20Deep%202.4B%20and%207.8B%2C%20outperform%20other%20models%20of%20comparable%20size%2C%20while%20the%20largest%20model%2C%20EXAONE%20Deep%2032B%2C%20demonstrates%20competitive%20performance%20against%20leading%20open-weight%20models.%20All%20EXAONE%20Deep%20models%20are%20openly%20available%20for%20research%20purposes%20and%20can%20be%20downloaded%20from%20https%3A//huggingface.co/LGAI-EXAONE.%0ALink%3A%20http%3A//arxiv.org/abs/2503.12524v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEXAONE%2520Deep%253A%2520Reasoning%2520Enhanced%2520Language%2520Models%26entry.906535625%3DKyunghoon%2520Bae%2520and%2520Eunbi%2520Choi%2520and%2520Kibong%2520Choi%2520and%2520Stanley%2520Jungkyu%2520Choi%2520and%2520Yemuk%2520Choi%2520and%2520Seokhee%2520Hong%2520and%2520Junwon%2520Hwang%2520and%2520Hyojin%2520Jeon%2520and%2520Kijeong%2520Jeon%2520and%2520Gerrard%2520Jeongwon%2520Jo%2520and%2520Hyunjik%2520Jo%2520and%2520Jiyeon%2520Jung%2520and%2520Hyosang%2520Kim%2520and%2520Joonkee%2520Kim%2520and%2520Seonghwan%2520Kim%2520and%2520Soyeon%2520Kim%2520and%2520Sunkyoung%2520Kim%2520and%2520Yireun%2520Kim%2520and%2520Yongil%2520Kim%2520and%2520Youchul%2520Kim%2520and%2520Edward%2520Hwayoung%2520Lee%2520and%2520Haeju%2520Lee%2520and%2520Honglak%2520Lee%2520and%2520Jinsik%2520Lee%2520and%2520Kyungmin%2520Lee%2520and%2520Sangha%2520Park%2520and%2520Yongmin%2520Park%2520and%2520Sihoon%2520Yang%2520and%2520Heuiyeen%2520Yeen%2520and%2520Sihyuk%2520Yi%2520and%2520Hyeongu%2520Yun%26entry.1292438233%3DWe%2520present%2520EXAONE%2520Deep%2520series%252C%2520which%2520exhibits%2520superior%2520capabilities%2520in%2520various%2520reasoning%2520tasks%252C%2520including%2520math%2520and%2520coding%2520benchmarks.%2520We%2520train%2520our%2520models%2520mainly%2520on%2520the%2520reasoning-specialized%2520dataset%2520that%2520incorporates%2520long%2520streams%2520of%2520thought%2520processes.%2520Evaluation%2520results%2520show%2520that%2520our%2520smaller%2520models%252C%2520EXAONE%2520Deep%25202.4B%2520and%25207.8B%252C%2520outperform%2520other%2520models%2520of%2520comparable%2520size%252C%2520while%2520the%2520largest%2520model%252C%2520EXAONE%2520Deep%252032B%252C%2520demonstrates%2520competitive%2520performance%2520against%2520leading%2520open-weight%2520models.%2520All%2520EXAONE%2520Deep%2520models%2520are%2520openly%2520available%2520for%2520research%2520purposes%2520and%2520can%2520be%2520downloaded%2520from%2520https%253A//huggingface.co/LGAI-EXAONE.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.12524v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EXAONE%20Deep%3A%20Reasoning%20Enhanced%20Language%20Models&entry.906535625=Kyunghoon%20Bae%20and%20Eunbi%20Choi%20and%20Kibong%20Choi%20and%20Stanley%20Jungkyu%20Choi%20and%20Yemuk%20Choi%20and%20Seokhee%20Hong%20and%20Junwon%20Hwang%20and%20Hyojin%20Jeon%20and%20Kijeong%20Jeon%20and%20Gerrard%20Jeongwon%20Jo%20and%20Hyunjik%20Jo%20and%20Jiyeon%20Jung%20and%20Hyosang%20Kim%20and%20Joonkee%20Kim%20and%20Seonghwan%20Kim%20and%20Soyeon%20Kim%20and%20Sunkyoung%20Kim%20and%20Yireun%20Kim%20and%20Yongil%20Kim%20and%20Youchul%20Kim%20and%20Edward%20Hwayoung%20Lee%20and%20Haeju%20Lee%20and%20Honglak%20Lee%20and%20Jinsik%20Lee%20and%20Kyungmin%20Lee%20and%20Sangha%20Park%20and%20Yongmin%20Park%20and%20Sihoon%20Yang%20and%20Heuiyeen%20Yeen%20and%20Sihyuk%20Yi%20and%20Hyeongu%20Yun&entry.1292438233=We%20present%20EXAONE%20Deep%20series%2C%20which%20exhibits%20superior%20capabilities%20in%20various%20reasoning%20tasks%2C%20including%20math%20and%20coding%20benchmarks.%20We%20train%20our%20models%20mainly%20on%20the%20reasoning-specialized%20dataset%20that%20incorporates%20long%20streams%20of%20thought%20processes.%20Evaluation%20results%20show%20that%20our%20smaller%20models%2C%20EXAONE%20Deep%202.4B%20and%207.8B%2C%20outperform%20other%20models%20of%20comparable%20size%2C%20while%20the%20largest%20model%2C%20EXAONE%20Deep%2032B%2C%20demonstrates%20competitive%20performance%20against%20leading%20open-weight%20models.%20All%20EXAONE%20Deep%20models%20are%20openly%20available%20for%20research%20purposes%20and%20can%20be%20downloaded%20from%20https%3A//huggingface.co/LGAI-EXAONE.&entry.1838667208=http%3A//arxiv.org/abs/2503.12524v3&entry.124074799=Read"},
{"title": "Fusion-SSAT: Unleashing the Potential of Self-supervised Auxiliary Task by Feature Fusion for Generalized Deepfake Detection", "author": "Shukesh Reddy and Srijan Das and Abhijit Das", "abstract": "In this work, we attempted to unleash the potential of self-supervised learning as an auxiliary task that can optimise the primary task of generalised deepfake detection. To explore this, we examined different combinations of the training schemes for these tasks that can be most effective. Our findings reveal that fusing the feature representation from self-supervised auxiliary tasks is a powerful feature representation for the problem at hand. Such a representation can leverage the ultimate potential and bring in a unique representation of both the self-supervised and primary tasks, achieving better performance for the primary task. We experimented on a large set of datasets, which includes DF40, FaceForensics++, Celeb-DF, DFD, FaceShifter, UADFV, and our results showed better generalizability on cross-dataset evaluation when compared with current state-of-the-art detectors.", "link": "http://arxiv.org/abs/2601.00789v1", "date": "2026-01-02", "relevancy": 2.5453, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5612}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.493}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fusion-SSAT%3A%20Unleashing%20the%20Potential%20of%20Self-supervised%20Auxiliary%20Task%20by%20Feature%20Fusion%20for%20Generalized%20Deepfake%20Detection&body=Title%3A%20Fusion-SSAT%3A%20Unleashing%20the%20Potential%20of%20Self-supervised%20Auxiliary%20Task%20by%20Feature%20Fusion%20for%20Generalized%20Deepfake%20Detection%0AAuthor%3A%20Shukesh%20Reddy%20and%20Srijan%20Das%20and%20Abhijit%20Das%0AAbstract%3A%20In%20this%20work%2C%20we%20attempted%20to%20unleash%20the%20potential%20of%20self-supervised%20learning%20as%20an%20auxiliary%20task%20that%20can%20optimise%20the%20primary%20task%20of%20generalised%20deepfake%20detection.%20To%20explore%20this%2C%20we%20examined%20different%20combinations%20of%20the%20training%20schemes%20for%20these%20tasks%20that%20can%20be%20most%20effective.%20Our%20findings%20reveal%20that%20fusing%20the%20feature%20representation%20from%20self-supervised%20auxiliary%20tasks%20is%20a%20powerful%20feature%20representation%20for%20the%20problem%20at%20hand.%20Such%20a%20representation%20can%20leverage%20the%20ultimate%20potential%20and%20bring%20in%20a%20unique%20representation%20of%20both%20the%20self-supervised%20and%20primary%20tasks%2C%20achieving%20better%20performance%20for%20the%20primary%20task.%20We%20experimented%20on%20a%20large%20set%20of%20datasets%2C%20which%20includes%20DF40%2C%20FaceForensics%2B%2B%2C%20Celeb-DF%2C%20DFD%2C%20FaceShifter%2C%20UADFV%2C%20and%20our%20results%20showed%20better%20generalizability%20on%20cross-dataset%20evaluation%20when%20compared%20with%20current%20state-of-the-art%20detectors.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00789v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFusion-SSAT%253A%2520Unleashing%2520the%2520Potential%2520of%2520Self-supervised%2520Auxiliary%2520Task%2520by%2520Feature%2520Fusion%2520for%2520Generalized%2520Deepfake%2520Detection%26entry.906535625%3DShukesh%2520Reddy%2520and%2520Srijan%2520Das%2520and%2520Abhijit%2520Das%26entry.1292438233%3DIn%2520this%2520work%252C%2520we%2520attempted%2520to%2520unleash%2520the%2520potential%2520of%2520self-supervised%2520learning%2520as%2520an%2520auxiliary%2520task%2520that%2520can%2520optimise%2520the%2520primary%2520task%2520of%2520generalised%2520deepfake%2520detection.%2520To%2520explore%2520this%252C%2520we%2520examined%2520different%2520combinations%2520of%2520the%2520training%2520schemes%2520for%2520these%2520tasks%2520that%2520can%2520be%2520most%2520effective.%2520Our%2520findings%2520reveal%2520that%2520fusing%2520the%2520feature%2520representation%2520from%2520self-supervised%2520auxiliary%2520tasks%2520is%2520a%2520powerful%2520feature%2520representation%2520for%2520the%2520problem%2520at%2520hand.%2520Such%2520a%2520representation%2520can%2520leverage%2520the%2520ultimate%2520potential%2520and%2520bring%2520in%2520a%2520unique%2520representation%2520of%2520both%2520the%2520self-supervised%2520and%2520primary%2520tasks%252C%2520achieving%2520better%2520performance%2520for%2520the%2520primary%2520task.%2520We%2520experimented%2520on%2520a%2520large%2520set%2520of%2520datasets%252C%2520which%2520includes%2520DF40%252C%2520FaceForensics%252B%252B%252C%2520Celeb-DF%252C%2520DFD%252C%2520FaceShifter%252C%2520UADFV%252C%2520and%2520our%2520results%2520showed%2520better%2520generalizability%2520on%2520cross-dataset%2520evaluation%2520when%2520compared%2520with%2520current%2520state-of-the-art%2520detectors.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00789v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fusion-SSAT%3A%20Unleashing%20the%20Potential%20of%20Self-supervised%20Auxiliary%20Task%20by%20Feature%20Fusion%20for%20Generalized%20Deepfake%20Detection&entry.906535625=Shukesh%20Reddy%20and%20Srijan%20Das%20and%20Abhijit%20Das&entry.1292438233=In%20this%20work%2C%20we%20attempted%20to%20unleash%20the%20potential%20of%20self-supervised%20learning%20as%20an%20auxiliary%20task%20that%20can%20optimise%20the%20primary%20task%20of%20generalised%20deepfake%20detection.%20To%20explore%20this%2C%20we%20examined%20different%20combinations%20of%20the%20training%20schemes%20for%20these%20tasks%20that%20can%20be%20most%20effective.%20Our%20findings%20reveal%20that%20fusing%20the%20feature%20representation%20from%20self-supervised%20auxiliary%20tasks%20is%20a%20powerful%20feature%20representation%20for%20the%20problem%20at%20hand.%20Such%20a%20representation%20can%20leverage%20the%20ultimate%20potential%20and%20bring%20in%20a%20unique%20representation%20of%20both%20the%20self-supervised%20and%20primary%20tasks%2C%20achieving%20better%20performance%20for%20the%20primary%20task.%20We%20experimented%20on%20a%20large%20set%20of%20datasets%2C%20which%20includes%20DF40%2C%20FaceForensics%2B%2B%2C%20Celeb-DF%2C%20DFD%2C%20FaceShifter%2C%20UADFV%2C%20and%20our%20results%20showed%20better%20generalizability%20on%20cross-dataset%20evaluation%20when%20compared%20with%20current%20state-of-the-art%20detectors.&entry.1838667208=http%3A//arxiv.org/abs/2601.00789v1&entry.124074799=Read"},
{"title": "Modeling the One-to-Many Property in Open-Domain Dialogue with LLMs", "author": "Jing Yang Lee and Kong-Aik Lee and Woon-Seng Gan", "abstract": "Open-domain Dialogue (OD) exhibits a one-to-many (o2m) property, whereby multiple appropriate responses exist for a single dialogue context. Despite prior research showing that modeling this property boosts response diversity, most modern LLM-based dialogue agents do not explicitly do so. In this work, we model the o2m property of OD in LLMs by decomposing OD generation into two key tasks: Multi-Response Generation (MRG) and Preference-based Selection (PS), which entail generating a set of n semantically and lexically diverse high-quality responses for a given dialogue context, followed by selecting a single response based on human preference, respectively. To facilitate MRG and PS, we introduce o2mDial, a dialogue corpus explicitly designed to capture the o2m property by featuring multiple plausible responses for each context. Leveraging o2mDial, we propose new in-context learning and instruction-tuning strategies, as well as novel evaluation metrics for MRG, alongside a model-based approach for PS. Empirical results demonstrate that applying the proposed two-stage framework to smaller LLMs for OD generation enhances overall response diversity while maintaining contextual coherence, improving response quality by up to 90%, bringing them closer to the performance of larger models.", "link": "http://arxiv.org/abs/2506.15131v2", "date": "2026-01-02", "relevancy": 2.5156, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5103}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5103}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20the%20One-to-Many%20Property%20in%20Open-Domain%20Dialogue%20with%20LLMs&body=Title%3A%20Modeling%20the%20One-to-Many%20Property%20in%20Open-Domain%20Dialogue%20with%20LLMs%0AAuthor%3A%20Jing%20Yang%20Lee%20and%20Kong-Aik%20Lee%20and%20Woon-Seng%20Gan%0AAbstract%3A%20Open-domain%20Dialogue%20%28OD%29%20exhibits%20a%20one-to-many%20%28o2m%29%20property%2C%20whereby%20multiple%20appropriate%20responses%20exist%20for%20a%20single%20dialogue%20context.%20Despite%20prior%20research%20showing%20that%20modeling%20this%20property%20boosts%20response%20diversity%2C%20most%20modern%20LLM-based%20dialogue%20agents%20do%20not%20explicitly%20do%20so.%20In%20this%20work%2C%20we%20model%20the%20o2m%20property%20of%20OD%20in%20LLMs%20by%20decomposing%20OD%20generation%20into%20two%20key%20tasks%3A%20Multi-Response%20Generation%20%28MRG%29%20and%20Preference-based%20Selection%20%28PS%29%2C%20which%20entail%20generating%20a%20set%20of%20n%20semantically%20and%20lexically%20diverse%20high-quality%20responses%20for%20a%20given%20dialogue%20context%2C%20followed%20by%20selecting%20a%20single%20response%20based%20on%20human%20preference%2C%20respectively.%20To%20facilitate%20MRG%20and%20PS%2C%20we%20introduce%20o2mDial%2C%20a%20dialogue%20corpus%20explicitly%20designed%20to%20capture%20the%20o2m%20property%20by%20featuring%20multiple%20plausible%20responses%20for%20each%20context.%20Leveraging%20o2mDial%2C%20we%20propose%20new%20in-context%20learning%20and%20instruction-tuning%20strategies%2C%20as%20well%20as%20novel%20evaluation%20metrics%20for%20MRG%2C%20alongside%20a%20model-based%20approach%20for%20PS.%20Empirical%20results%20demonstrate%20that%20applying%20the%20proposed%20two-stage%20framework%20to%20smaller%20LLMs%20for%20OD%20generation%20enhances%20overall%20response%20diversity%20while%20maintaining%20contextual%20coherence%2C%20improving%20response%20quality%20by%20up%20to%2090%25%2C%20bringing%20them%20closer%20to%20the%20performance%20of%20larger%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2506.15131v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520the%2520One-to-Many%2520Property%2520in%2520Open-Domain%2520Dialogue%2520with%2520LLMs%26entry.906535625%3DJing%2520Yang%2520Lee%2520and%2520Kong-Aik%2520Lee%2520and%2520Woon-Seng%2520Gan%26entry.1292438233%3DOpen-domain%2520Dialogue%2520%2528OD%2529%2520exhibits%2520a%2520one-to-many%2520%2528o2m%2529%2520property%252C%2520whereby%2520multiple%2520appropriate%2520responses%2520exist%2520for%2520a%2520single%2520dialogue%2520context.%2520Despite%2520prior%2520research%2520showing%2520that%2520modeling%2520this%2520property%2520boosts%2520response%2520diversity%252C%2520most%2520modern%2520LLM-based%2520dialogue%2520agents%2520do%2520not%2520explicitly%2520do%2520so.%2520In%2520this%2520work%252C%2520we%2520model%2520the%2520o2m%2520property%2520of%2520OD%2520in%2520LLMs%2520by%2520decomposing%2520OD%2520generation%2520into%2520two%2520key%2520tasks%253A%2520Multi-Response%2520Generation%2520%2528MRG%2529%2520and%2520Preference-based%2520Selection%2520%2528PS%2529%252C%2520which%2520entail%2520generating%2520a%2520set%2520of%2520n%2520semantically%2520and%2520lexically%2520diverse%2520high-quality%2520responses%2520for%2520a%2520given%2520dialogue%2520context%252C%2520followed%2520by%2520selecting%2520a%2520single%2520response%2520based%2520on%2520human%2520preference%252C%2520respectively.%2520To%2520facilitate%2520MRG%2520and%2520PS%252C%2520we%2520introduce%2520o2mDial%252C%2520a%2520dialogue%2520corpus%2520explicitly%2520designed%2520to%2520capture%2520the%2520o2m%2520property%2520by%2520featuring%2520multiple%2520plausible%2520responses%2520for%2520each%2520context.%2520Leveraging%2520o2mDial%252C%2520we%2520propose%2520new%2520in-context%2520learning%2520and%2520instruction-tuning%2520strategies%252C%2520as%2520well%2520as%2520novel%2520evaluation%2520metrics%2520for%2520MRG%252C%2520alongside%2520a%2520model-based%2520approach%2520for%2520PS.%2520Empirical%2520results%2520demonstrate%2520that%2520applying%2520the%2520proposed%2520two-stage%2520framework%2520to%2520smaller%2520LLMs%2520for%2520OD%2520generation%2520enhances%2520overall%2520response%2520diversity%2520while%2520maintaining%2520contextual%2520coherence%252C%2520improving%2520response%2520quality%2520by%2520up%2520to%252090%2525%252C%2520bringing%2520them%2520closer%2520to%2520the%2520performance%2520of%2520larger%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15131v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20the%20One-to-Many%20Property%20in%20Open-Domain%20Dialogue%20with%20LLMs&entry.906535625=Jing%20Yang%20Lee%20and%20Kong-Aik%20Lee%20and%20Woon-Seng%20Gan&entry.1292438233=Open-domain%20Dialogue%20%28OD%29%20exhibits%20a%20one-to-many%20%28o2m%29%20property%2C%20whereby%20multiple%20appropriate%20responses%20exist%20for%20a%20single%20dialogue%20context.%20Despite%20prior%20research%20showing%20that%20modeling%20this%20property%20boosts%20response%20diversity%2C%20most%20modern%20LLM-based%20dialogue%20agents%20do%20not%20explicitly%20do%20so.%20In%20this%20work%2C%20we%20model%20the%20o2m%20property%20of%20OD%20in%20LLMs%20by%20decomposing%20OD%20generation%20into%20two%20key%20tasks%3A%20Multi-Response%20Generation%20%28MRG%29%20and%20Preference-based%20Selection%20%28PS%29%2C%20which%20entail%20generating%20a%20set%20of%20n%20semantically%20and%20lexically%20diverse%20high-quality%20responses%20for%20a%20given%20dialogue%20context%2C%20followed%20by%20selecting%20a%20single%20response%20based%20on%20human%20preference%2C%20respectively.%20To%20facilitate%20MRG%20and%20PS%2C%20we%20introduce%20o2mDial%2C%20a%20dialogue%20corpus%20explicitly%20designed%20to%20capture%20the%20o2m%20property%20by%20featuring%20multiple%20plausible%20responses%20for%20each%20context.%20Leveraging%20o2mDial%2C%20we%20propose%20new%20in-context%20learning%20and%20instruction-tuning%20strategies%2C%20as%20well%20as%20novel%20evaluation%20metrics%20for%20MRG%2C%20alongside%20a%20model-based%20approach%20for%20PS.%20Empirical%20results%20demonstrate%20that%20applying%20the%20proposed%20two-stage%20framework%20to%20smaller%20LLMs%20for%20OD%20generation%20enhances%20overall%20response%20diversity%20while%20maintaining%20contextual%20coherence%2C%20improving%20response%20quality%20by%20up%20to%2090%25%2C%20bringing%20them%20closer%20to%20the%20performance%20of%20larger%20models.&entry.1838667208=http%3A//arxiv.org/abs/2506.15131v2&entry.124074799=Read"},
{"title": "Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients", "author": "Armin Berger and Manuela Bergau and Helen Schneider and Saad Ahmad and Tom Anglim Lagones and Gianluca Brugnara and Martha Foltyn-Dumitru and Kai Schlamp and Philipp Vollmuth and Rafet Sifa", "abstract": "Recent Reinforcement Learning (RL) advances for Large Language Models (LLMs) have improved reasoning tasks, yet their resource-constrained application to medical imaging remains underexplored. We introduce ChexReason, a vision-language model trained via R1-style methodology (SFT followed by GRPO) using only 2,000 SFT samples, 1,000 RL samples, and a single A100 GPU. Evaluations on CheXpert and NIH benchmarks reveal a fundamental tension: GRPO recovers in-distribution performance (23% improvement on CheXpert, macro-F1 = 0.346) but degrades cross-dataset transferability (19% drop on NIH). This mirrors high-resource models like NV-Reason-CXR-3B, suggesting the issue stems from the RL paradigm rather than scale. We identify a generalization paradox where the SFT checkpoint uniquely improves on NIH before optimization, indicating teacher-guided reasoning captures more institution-agnostic features. Furthermore, cross-model comparisons show structured reasoning scaffolds benefit general-purpose VLMs but offer minimal gain for medically pre-trained models. Consequently, curated supervised fine-tuning may outperform aggressive RL for clinical deployment requiring robustness across diverse populations.", "link": "http://arxiv.org/abs/2512.23090v2", "date": "2026-01-02", "relevancy": 2.5097, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5262}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5262}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmark%20Success%2C%20Clinical%20Failure%3A%20When%20Reinforcement%20Learning%20Optimizes%20for%20Benchmarks%2C%20Not%20Patients&body=Title%3A%20Benchmark%20Success%2C%20Clinical%20Failure%3A%20When%20Reinforcement%20Learning%20Optimizes%20for%20Benchmarks%2C%20Not%20Patients%0AAuthor%3A%20Armin%20Berger%20and%20Manuela%20Bergau%20and%20Helen%20Schneider%20and%20Saad%20Ahmad%20and%20Tom%20Anglim%20Lagones%20and%20Gianluca%20Brugnara%20and%20Martha%20Foltyn-Dumitru%20and%20Kai%20Schlamp%20and%20Philipp%20Vollmuth%20and%20Rafet%20Sifa%0AAbstract%3A%20Recent%20Reinforcement%20Learning%20%28RL%29%20advances%20for%20Large%20Language%20Models%20%28LLMs%29%20have%20improved%20reasoning%20tasks%2C%20yet%20their%20resource-constrained%20application%20to%20medical%20imaging%20remains%20underexplored.%20We%20introduce%20ChexReason%2C%20a%20vision-language%20model%20trained%20via%20R1-style%20methodology%20%28SFT%20followed%20by%20GRPO%29%20using%20only%202%2C000%20SFT%20samples%2C%201%2C000%20RL%20samples%2C%20and%20a%20single%20A100%20GPU.%20Evaluations%20on%20CheXpert%20and%20NIH%20benchmarks%20reveal%20a%20fundamental%20tension%3A%20GRPO%20recovers%20in-distribution%20performance%20%2823%25%20improvement%20on%20CheXpert%2C%20macro-F1%20%3D%200.346%29%20but%20degrades%20cross-dataset%20transferability%20%2819%25%20drop%20on%20NIH%29.%20This%20mirrors%20high-resource%20models%20like%20NV-Reason-CXR-3B%2C%20suggesting%20the%20issue%20stems%20from%20the%20RL%20paradigm%20rather%20than%20scale.%20We%20identify%20a%20generalization%20paradox%20where%20the%20SFT%20checkpoint%20uniquely%20improves%20on%20NIH%20before%20optimization%2C%20indicating%20teacher-guided%20reasoning%20captures%20more%20institution-agnostic%20features.%20Furthermore%2C%20cross-model%20comparisons%20show%20structured%20reasoning%20scaffolds%20benefit%20general-purpose%20VLMs%20but%20offer%20minimal%20gain%20for%20medically%20pre-trained%20models.%20Consequently%2C%20curated%20supervised%20fine-tuning%20may%20outperform%20aggressive%20RL%20for%20clinical%20deployment%20requiring%20robustness%20across%20diverse%20populations.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23090v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmark%2520Success%252C%2520Clinical%2520Failure%253A%2520When%2520Reinforcement%2520Learning%2520Optimizes%2520for%2520Benchmarks%252C%2520Not%2520Patients%26entry.906535625%3DArmin%2520Berger%2520and%2520Manuela%2520Bergau%2520and%2520Helen%2520Schneider%2520and%2520Saad%2520Ahmad%2520and%2520Tom%2520Anglim%2520Lagones%2520and%2520Gianluca%2520Brugnara%2520and%2520Martha%2520Foltyn-Dumitru%2520and%2520Kai%2520Schlamp%2520and%2520Philipp%2520Vollmuth%2520and%2520Rafet%2520Sifa%26entry.1292438233%3DRecent%2520Reinforcement%2520Learning%2520%2528RL%2529%2520advances%2520for%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520improved%2520reasoning%2520tasks%252C%2520yet%2520their%2520resource-constrained%2520application%2520to%2520medical%2520imaging%2520remains%2520underexplored.%2520We%2520introduce%2520ChexReason%252C%2520a%2520vision-language%2520model%2520trained%2520via%2520R1-style%2520methodology%2520%2528SFT%2520followed%2520by%2520GRPO%2529%2520using%2520only%25202%252C000%2520SFT%2520samples%252C%25201%252C000%2520RL%2520samples%252C%2520and%2520a%2520single%2520A100%2520GPU.%2520Evaluations%2520on%2520CheXpert%2520and%2520NIH%2520benchmarks%2520reveal%2520a%2520fundamental%2520tension%253A%2520GRPO%2520recovers%2520in-distribution%2520performance%2520%252823%2525%2520improvement%2520on%2520CheXpert%252C%2520macro-F1%2520%253D%25200.346%2529%2520but%2520degrades%2520cross-dataset%2520transferability%2520%252819%2525%2520drop%2520on%2520NIH%2529.%2520This%2520mirrors%2520high-resource%2520models%2520like%2520NV-Reason-CXR-3B%252C%2520suggesting%2520the%2520issue%2520stems%2520from%2520the%2520RL%2520paradigm%2520rather%2520than%2520scale.%2520We%2520identify%2520a%2520generalization%2520paradox%2520where%2520the%2520SFT%2520checkpoint%2520uniquely%2520improves%2520on%2520NIH%2520before%2520optimization%252C%2520indicating%2520teacher-guided%2520reasoning%2520captures%2520more%2520institution-agnostic%2520features.%2520Furthermore%252C%2520cross-model%2520comparisons%2520show%2520structured%2520reasoning%2520scaffolds%2520benefit%2520general-purpose%2520VLMs%2520but%2520offer%2520minimal%2520gain%2520for%2520medically%2520pre-trained%2520models.%2520Consequently%252C%2520curated%2520supervised%2520fine-tuning%2520may%2520outperform%2520aggressive%2520RL%2520for%2520clinical%2520deployment%2520requiring%2520robustness%2520across%2520diverse%2520populations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23090v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmark%20Success%2C%20Clinical%20Failure%3A%20When%20Reinforcement%20Learning%20Optimizes%20for%20Benchmarks%2C%20Not%20Patients&entry.906535625=Armin%20Berger%20and%20Manuela%20Bergau%20and%20Helen%20Schneider%20and%20Saad%20Ahmad%20and%20Tom%20Anglim%20Lagones%20and%20Gianluca%20Brugnara%20and%20Martha%20Foltyn-Dumitru%20and%20Kai%20Schlamp%20and%20Philipp%20Vollmuth%20and%20Rafet%20Sifa&entry.1292438233=Recent%20Reinforcement%20Learning%20%28RL%29%20advances%20for%20Large%20Language%20Models%20%28LLMs%29%20have%20improved%20reasoning%20tasks%2C%20yet%20their%20resource-constrained%20application%20to%20medical%20imaging%20remains%20underexplored.%20We%20introduce%20ChexReason%2C%20a%20vision-language%20model%20trained%20via%20R1-style%20methodology%20%28SFT%20followed%20by%20GRPO%29%20using%20only%202%2C000%20SFT%20samples%2C%201%2C000%20RL%20samples%2C%20and%20a%20single%20A100%20GPU.%20Evaluations%20on%20CheXpert%20and%20NIH%20benchmarks%20reveal%20a%20fundamental%20tension%3A%20GRPO%20recovers%20in-distribution%20performance%20%2823%25%20improvement%20on%20CheXpert%2C%20macro-F1%20%3D%200.346%29%20but%20degrades%20cross-dataset%20transferability%20%2819%25%20drop%20on%20NIH%29.%20This%20mirrors%20high-resource%20models%20like%20NV-Reason-CXR-3B%2C%20suggesting%20the%20issue%20stems%20from%20the%20RL%20paradigm%20rather%20than%20scale.%20We%20identify%20a%20generalization%20paradox%20where%20the%20SFT%20checkpoint%20uniquely%20improves%20on%20NIH%20before%20optimization%2C%20indicating%20teacher-guided%20reasoning%20captures%20more%20institution-agnostic%20features.%20Furthermore%2C%20cross-model%20comparisons%20show%20structured%20reasoning%20scaffolds%20benefit%20general-purpose%20VLMs%20but%20offer%20minimal%20gain%20for%20medically%20pre-trained%20models.%20Consequently%2C%20curated%20supervised%20fine-tuning%20may%20outperform%20aggressive%20RL%20for%20clinical%20deployment%20requiring%20robustness%20across%20diverse%20populations.&entry.1838667208=http%3A//arxiv.org/abs/2512.23090v2&entry.124074799=Read"},
{"title": "PoseStreamer: A Multi-modal Framework for 3D Tracking of Unseen Moving Objects", "author": "Huiming Yang and Linglin Liao and Fei Ding and Sibo Wang and Zijian Zeng", "abstract": "Six degree of freedom (6DoF) pose estimation for novel objects is a critical task in computer vision, yet it faces significant challenges in high-speed and low-light scenarios where standard RGB cameras suffer from motion blur. While event cameras offer a promising solution due to their high temporal resolution, current 6DoF pose estimation methods typically yield suboptimal performance in high-speed object moving scenarios. To address this gap, we propose PoseStreamer, a robust multi-modal 6DoF pose estimation framework designed specifically on high-speed moving scenarios. Our approach integrates three core components: an Adaptive Pose Memory Queue that utilizes historical orientation cues for temporal consistency, an Object-centric 2D Tracker that provides strong 2D priors to boost 3D center recall, and a Ray Pose Filter for geometric refinement along camera rays. Furthermore, we introduce MoCapCube6D, a novel multi-modal dataset constructed to benchmark performance under rapid motion. Extensive experiments demonstrate that PoseStreamer not only achieves superior accuracy in high-speed moving scenarios, but also exhibits strong generalizability as a template-free framework for unseen moving objects.", "link": "http://arxiv.org/abs/2512.22979v3", "date": "2026-01-02", "relevancy": 2.4379, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6298}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5999}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PoseStreamer%3A%20A%20Multi-modal%20Framework%20for%203D%20Tracking%20of%20Unseen%20Moving%20Objects&body=Title%3A%20PoseStreamer%3A%20A%20Multi-modal%20Framework%20for%203D%20Tracking%20of%20Unseen%20Moving%20Objects%0AAuthor%3A%20Huiming%20Yang%20and%20Linglin%20Liao%20and%20Fei%20Ding%20and%20Sibo%20Wang%20and%20Zijian%20Zeng%0AAbstract%3A%20Six%20degree%20of%20freedom%20%286DoF%29%20pose%20estimation%20for%20novel%20objects%20is%20a%20critical%20task%20in%20computer%20vision%2C%20yet%20it%20faces%20significant%20challenges%20in%20high-speed%20and%20low-light%20scenarios%20where%20standard%20RGB%20cameras%20suffer%20from%20motion%20blur.%20While%20event%20cameras%20offer%20a%20promising%20solution%20due%20to%20their%20high%20temporal%20resolution%2C%20current%206DoF%20pose%20estimation%20methods%20typically%20yield%20suboptimal%20performance%20in%20high-speed%20object%20moving%20scenarios.%20To%20address%20this%20gap%2C%20we%20propose%20PoseStreamer%2C%20a%20robust%20multi-modal%206DoF%20pose%20estimation%20framework%20designed%20specifically%20on%20high-speed%20moving%20scenarios.%20Our%20approach%20integrates%20three%20core%20components%3A%20an%20Adaptive%20Pose%20Memory%20Queue%20that%20utilizes%20historical%20orientation%20cues%20for%20temporal%20consistency%2C%20an%20Object-centric%202D%20Tracker%20that%20provides%20strong%202D%20priors%20to%20boost%203D%20center%20recall%2C%20and%20a%20Ray%20Pose%20Filter%20for%20geometric%20refinement%20along%20camera%20rays.%20Furthermore%2C%20we%20introduce%20MoCapCube6D%2C%20a%20novel%20multi-modal%20dataset%20constructed%20to%20benchmark%20performance%20under%20rapid%20motion.%20Extensive%20experiments%20demonstrate%20that%20PoseStreamer%20not%20only%20achieves%20superior%20accuracy%20in%20high-speed%20moving%20scenarios%2C%20but%20also%20exhibits%20strong%20generalizability%20as%20a%20template-free%20framework%20for%20unseen%20moving%20objects.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22979v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoseStreamer%253A%2520A%2520Multi-modal%2520Framework%2520for%25203D%2520Tracking%2520of%2520Unseen%2520Moving%2520Objects%26entry.906535625%3DHuiming%2520Yang%2520and%2520Linglin%2520Liao%2520and%2520Fei%2520Ding%2520and%2520Sibo%2520Wang%2520and%2520Zijian%2520Zeng%26entry.1292438233%3DSix%2520degree%2520of%2520freedom%2520%25286DoF%2529%2520pose%2520estimation%2520for%2520novel%2520objects%2520is%2520a%2520critical%2520task%2520in%2520computer%2520vision%252C%2520yet%2520it%2520faces%2520significant%2520challenges%2520in%2520high-speed%2520and%2520low-light%2520scenarios%2520where%2520standard%2520RGB%2520cameras%2520suffer%2520from%2520motion%2520blur.%2520While%2520event%2520cameras%2520offer%2520a%2520promising%2520solution%2520due%2520to%2520their%2520high%2520temporal%2520resolution%252C%2520current%25206DoF%2520pose%2520estimation%2520methods%2520typically%2520yield%2520suboptimal%2520performance%2520in%2520high-speed%2520object%2520moving%2520scenarios.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520PoseStreamer%252C%2520a%2520robust%2520multi-modal%25206DoF%2520pose%2520estimation%2520framework%2520designed%2520specifically%2520on%2520high-speed%2520moving%2520scenarios.%2520Our%2520approach%2520integrates%2520three%2520core%2520components%253A%2520an%2520Adaptive%2520Pose%2520Memory%2520Queue%2520that%2520utilizes%2520historical%2520orientation%2520cues%2520for%2520temporal%2520consistency%252C%2520an%2520Object-centric%25202D%2520Tracker%2520that%2520provides%2520strong%25202D%2520priors%2520to%2520boost%25203D%2520center%2520recall%252C%2520and%2520a%2520Ray%2520Pose%2520Filter%2520for%2520geometric%2520refinement%2520along%2520camera%2520rays.%2520Furthermore%252C%2520we%2520introduce%2520MoCapCube6D%252C%2520a%2520novel%2520multi-modal%2520dataset%2520constructed%2520to%2520benchmark%2520performance%2520under%2520rapid%2520motion.%2520Extensive%2520experiments%2520demonstrate%2520that%2520PoseStreamer%2520not%2520only%2520achieves%2520superior%2520accuracy%2520in%2520high-speed%2520moving%2520scenarios%252C%2520but%2520also%2520exhibits%2520strong%2520generalizability%2520as%2520a%2520template-free%2520framework%2520for%2520unseen%2520moving%2520objects.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22979v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PoseStreamer%3A%20A%20Multi-modal%20Framework%20for%203D%20Tracking%20of%20Unseen%20Moving%20Objects&entry.906535625=Huiming%20Yang%20and%20Linglin%20Liao%20and%20Fei%20Ding%20and%20Sibo%20Wang%20and%20Zijian%20Zeng&entry.1292438233=Six%20degree%20of%20freedom%20%286DoF%29%20pose%20estimation%20for%20novel%20objects%20is%20a%20critical%20task%20in%20computer%20vision%2C%20yet%20it%20faces%20significant%20challenges%20in%20high-speed%20and%20low-light%20scenarios%20where%20standard%20RGB%20cameras%20suffer%20from%20motion%20blur.%20While%20event%20cameras%20offer%20a%20promising%20solution%20due%20to%20their%20high%20temporal%20resolution%2C%20current%206DoF%20pose%20estimation%20methods%20typically%20yield%20suboptimal%20performance%20in%20high-speed%20object%20moving%20scenarios.%20To%20address%20this%20gap%2C%20we%20propose%20PoseStreamer%2C%20a%20robust%20multi-modal%206DoF%20pose%20estimation%20framework%20designed%20specifically%20on%20high-speed%20moving%20scenarios.%20Our%20approach%20integrates%20three%20core%20components%3A%20an%20Adaptive%20Pose%20Memory%20Queue%20that%20utilizes%20historical%20orientation%20cues%20for%20temporal%20consistency%2C%20an%20Object-centric%202D%20Tracker%20that%20provides%20strong%202D%20priors%20to%20boost%203D%20center%20recall%2C%20and%20a%20Ray%20Pose%20Filter%20for%20geometric%20refinement%20along%20camera%20rays.%20Furthermore%2C%20we%20introduce%20MoCapCube6D%2C%20a%20novel%20multi-modal%20dataset%20constructed%20to%20benchmark%20performance%20under%20rapid%20motion.%20Extensive%20experiments%20demonstrate%20that%20PoseStreamer%20not%20only%20achieves%20superior%20accuracy%20in%20high-speed%20moving%20scenarios%2C%20but%20also%20exhibits%20strong%20generalizability%20as%20a%20template-free%20framework%20for%20unseen%20moving%20objects.&entry.1838667208=http%3A//arxiv.org/abs/2512.22979v3&entry.124074799=Read"},
{"title": "Memory Bank Compression for Continual Adaptation of Large Language Models", "author": "Thomas Katraouras and Dimitrios Rafailidis", "abstract": "Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC.", "link": "http://arxiv.org/abs/2601.00756v1", "date": "2026-01-02", "relevancy": 2.4083, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.503}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4794}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4627}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memory%20Bank%20Compression%20for%20Continual%20Adaptation%20of%20Large%20Language%20Models&body=Title%3A%20Memory%20Bank%20Compression%20for%20Continual%20Adaptation%20of%20Large%20Language%20Models%0AAuthor%3A%20Thomas%20Katraouras%20and%20Dimitrios%20Rafailidis%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20a%20mainstay%20for%20many%20everyday%20applications.%20However%2C%20as%20data%20evolve%20their%20knowledge%20quickly%20becomes%20outdated.%20Continual%20learning%20aims%20to%20update%20LLMs%20with%20new%20information%20without%20erasing%20previously%20acquired%20knowledge.%20Although%20methods%20such%20as%20full%20fine-tuning%20can%20incorporate%20new%20data%2C%20they%20are%20computationally%20expensive%20and%20prone%20to%20catastrophic%20forgetting%2C%20where%20prior%20knowledge%20is%20overwritten.%20Memory-augmented%20approaches%20address%20this%20by%20equipping%20LLMs%20with%20a%20memory%20bank%2C%20that%20is%20an%20external%20memory%20module%20which%20stores%20information%20for%20future%20use.%20However%2C%20these%20methods%20face%20a%20critical%20limitation%2C%20in%20particular%2C%20the%20memory%20bank%20constantly%20grows%20in%20the%20real-world%20scenario%20when%20large-scale%20data%20streams%20arrive.%20In%20this%20paper%2C%20we%20propose%20MBC%2C%20a%20model%20that%20compresses%20the%20memory%20bank%20through%20a%20codebook%20optimization%20strategy%20during%20online%20adaptation%20learning.%20To%20ensure%20stable%20learning%2C%20we%20also%20introduce%20an%20online%20resetting%20mechanism%20that%20prevents%20codebook%20collapse.%20In%20addition%2C%20we%20employ%20Key-Value%20Low-Rank%20Adaptation%20in%20the%20attention%20layers%20of%20the%20LLM%2C%20enabling%20efficient%20utilization%20of%20the%20compressed%20memory%20representations.%20Experiments%20with%20benchmark%20question-answering%20datasets%20demonstrate%20that%20MBC%20reduces%20the%20memory%20bank%20size%20to%200.3%25%20when%20compared%20against%20the%20most%20competitive%20baseline%2C%20while%20maintaining%20high%20retention%20accuracy%20during%20online%20adaptation%20learning.%20Our%20code%20is%20publicly%20available%20at%20https%3A//github.com/Thomkat/MBC.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00756v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemory%2520Bank%2520Compression%2520for%2520Continual%2520Adaptation%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DThomas%2520Katraouras%2520and%2520Dimitrios%2520Rafailidis%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520become%2520a%2520mainstay%2520for%2520many%2520everyday%2520applications.%2520However%252C%2520as%2520data%2520evolve%2520their%2520knowledge%2520quickly%2520becomes%2520outdated.%2520Continual%2520learning%2520aims%2520to%2520update%2520LLMs%2520with%2520new%2520information%2520without%2520erasing%2520previously%2520acquired%2520knowledge.%2520Although%2520methods%2520such%2520as%2520full%2520fine-tuning%2520can%2520incorporate%2520new%2520data%252C%2520they%2520are%2520computationally%2520expensive%2520and%2520prone%2520to%2520catastrophic%2520forgetting%252C%2520where%2520prior%2520knowledge%2520is%2520overwritten.%2520Memory-augmented%2520approaches%2520address%2520this%2520by%2520equipping%2520LLMs%2520with%2520a%2520memory%2520bank%252C%2520that%2520is%2520an%2520external%2520memory%2520module%2520which%2520stores%2520information%2520for%2520future%2520use.%2520However%252C%2520these%2520methods%2520face%2520a%2520critical%2520limitation%252C%2520in%2520particular%252C%2520the%2520memory%2520bank%2520constantly%2520grows%2520in%2520the%2520real-world%2520scenario%2520when%2520large-scale%2520data%2520streams%2520arrive.%2520In%2520this%2520paper%252C%2520we%2520propose%2520MBC%252C%2520a%2520model%2520that%2520compresses%2520the%2520memory%2520bank%2520through%2520a%2520codebook%2520optimization%2520strategy%2520during%2520online%2520adaptation%2520learning.%2520To%2520ensure%2520stable%2520learning%252C%2520we%2520also%2520introduce%2520an%2520online%2520resetting%2520mechanism%2520that%2520prevents%2520codebook%2520collapse.%2520In%2520addition%252C%2520we%2520employ%2520Key-Value%2520Low-Rank%2520Adaptation%2520in%2520the%2520attention%2520layers%2520of%2520the%2520LLM%252C%2520enabling%2520efficient%2520utilization%2520of%2520the%2520compressed%2520memory%2520representations.%2520Experiments%2520with%2520benchmark%2520question-answering%2520datasets%2520demonstrate%2520that%2520MBC%2520reduces%2520the%2520memory%2520bank%2520size%2520to%25200.3%2525%2520when%2520compared%2520against%2520the%2520most%2520competitive%2520baseline%252C%2520while%2520maintaining%2520high%2520retention%2520accuracy%2520during%2520online%2520adaptation%2520learning.%2520Our%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/Thomkat/MBC.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00756v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memory%20Bank%20Compression%20for%20Continual%20Adaptation%20of%20Large%20Language%20Models&entry.906535625=Thomas%20Katraouras%20and%20Dimitrios%20Rafailidis&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20have%20become%20a%20mainstay%20for%20many%20everyday%20applications.%20However%2C%20as%20data%20evolve%20their%20knowledge%20quickly%20becomes%20outdated.%20Continual%20learning%20aims%20to%20update%20LLMs%20with%20new%20information%20without%20erasing%20previously%20acquired%20knowledge.%20Although%20methods%20such%20as%20full%20fine-tuning%20can%20incorporate%20new%20data%2C%20they%20are%20computationally%20expensive%20and%20prone%20to%20catastrophic%20forgetting%2C%20where%20prior%20knowledge%20is%20overwritten.%20Memory-augmented%20approaches%20address%20this%20by%20equipping%20LLMs%20with%20a%20memory%20bank%2C%20that%20is%20an%20external%20memory%20module%20which%20stores%20information%20for%20future%20use.%20However%2C%20these%20methods%20face%20a%20critical%20limitation%2C%20in%20particular%2C%20the%20memory%20bank%20constantly%20grows%20in%20the%20real-world%20scenario%20when%20large-scale%20data%20streams%20arrive.%20In%20this%20paper%2C%20we%20propose%20MBC%2C%20a%20model%20that%20compresses%20the%20memory%20bank%20through%20a%20codebook%20optimization%20strategy%20during%20online%20adaptation%20learning.%20To%20ensure%20stable%20learning%2C%20we%20also%20introduce%20an%20online%20resetting%20mechanism%20that%20prevents%20codebook%20collapse.%20In%20addition%2C%20we%20employ%20Key-Value%20Low-Rank%20Adaptation%20in%20the%20attention%20layers%20of%20the%20LLM%2C%20enabling%20efficient%20utilization%20of%20the%20compressed%20memory%20representations.%20Experiments%20with%20benchmark%20question-answering%20datasets%20demonstrate%20that%20MBC%20reduces%20the%20memory%20bank%20size%20to%200.3%25%20when%20compared%20against%20the%20most%20competitive%20baseline%2C%20while%20maintaining%20high%20retention%20accuracy%20during%20online%20adaptation%20learning.%20Our%20code%20is%20publicly%20available%20at%20https%3A//github.com/Thomkat/MBC.&entry.1838667208=http%3A//arxiv.org/abs/2601.00756v1&entry.124074799=Read"},
{"title": "Exploring the Performance of Large Language Models on Subjective Span Identification Tasks", "author": "Alphaeus Dmonte and Roland Oruche and Tharindu Ranasinghe and Marcos Zampieri and Prasad Calyam", "abstract": "Identifying relevant text spans is important for several downstream tasks in NLP, as it contributes to model explainability. While most span identification approaches rely on relatively smaller pre-trained language models like BERT, a few recent approaches have leveraged the latest generation of Large Language Models (LLMs) for the task. Current work has focused on explicit span identification like Named Entity Recognition (NER), while more subjective span identification with LLMs in tasks like Aspect-based Sentiment Analysis (ABSA) has been underexplored. In this paper, we fill this important gap by presenting an evaluation of the performance of various LLMs on text span identification in three popular tasks, namely sentiment analysis, offensive language identification, and claim verification. We explore several LLM strategies like instruction tuning, in-context learning, and chain of thought. Our results indicate underlying relationships within text aid LLMs in identifying precise text spans.", "link": "http://arxiv.org/abs/2601.00736v1", "date": "2026-01-02", "relevancy": 2.3946, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4986}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4986}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4396}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Performance%20of%20Large%20Language%20Models%20on%20Subjective%20Span%20Identification%20Tasks&body=Title%3A%20Exploring%20the%20Performance%20of%20Large%20Language%20Models%20on%20Subjective%20Span%20Identification%20Tasks%0AAuthor%3A%20Alphaeus%20Dmonte%20and%20Roland%20Oruche%20and%20Tharindu%20Ranasinghe%20and%20Marcos%20Zampieri%20and%20Prasad%20Calyam%0AAbstract%3A%20Identifying%20relevant%20text%20spans%20is%20important%20for%20several%20downstream%20tasks%20in%20NLP%2C%20as%20it%20contributes%20to%20model%20explainability.%20While%20most%20span%20identification%20approaches%20rely%20on%20relatively%20smaller%20pre-trained%20language%20models%20like%20BERT%2C%20a%20few%20recent%20approaches%20have%20leveraged%20the%20latest%20generation%20of%20Large%20Language%20Models%20%28LLMs%29%20for%20the%20task.%20Current%20work%20has%20focused%20on%20explicit%20span%20identification%20like%20Named%20Entity%20Recognition%20%28NER%29%2C%20while%20more%20subjective%20span%20identification%20with%20LLMs%20in%20tasks%20like%20Aspect-based%20Sentiment%20Analysis%20%28ABSA%29%20has%20been%20underexplored.%20In%20this%20paper%2C%20we%20fill%20this%20important%20gap%20by%20presenting%20an%20evaluation%20of%20the%20performance%20of%20various%20LLMs%20on%20text%20span%20identification%20in%20three%20popular%20tasks%2C%20namely%20sentiment%20analysis%2C%20offensive%20language%20identification%2C%20and%20claim%20verification.%20We%20explore%20several%20LLM%20strategies%20like%20instruction%20tuning%2C%20in-context%20learning%2C%20and%20chain%20of%20thought.%20Our%20results%20indicate%20underlying%20relationships%20within%20text%20aid%20LLMs%20in%20identifying%20precise%20text%20spans.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00736v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Performance%2520of%2520Large%2520Language%2520Models%2520on%2520Subjective%2520Span%2520Identification%2520Tasks%26entry.906535625%3DAlphaeus%2520Dmonte%2520and%2520Roland%2520Oruche%2520and%2520Tharindu%2520Ranasinghe%2520and%2520Marcos%2520Zampieri%2520and%2520Prasad%2520Calyam%26entry.1292438233%3DIdentifying%2520relevant%2520text%2520spans%2520is%2520important%2520for%2520several%2520downstream%2520tasks%2520in%2520NLP%252C%2520as%2520it%2520contributes%2520to%2520model%2520explainability.%2520While%2520most%2520span%2520identification%2520approaches%2520rely%2520on%2520relatively%2520smaller%2520pre-trained%2520language%2520models%2520like%2520BERT%252C%2520a%2520few%2520recent%2520approaches%2520have%2520leveraged%2520the%2520latest%2520generation%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520for%2520the%2520task.%2520Current%2520work%2520has%2520focused%2520on%2520explicit%2520span%2520identification%2520like%2520Named%2520Entity%2520Recognition%2520%2528NER%2529%252C%2520while%2520more%2520subjective%2520span%2520identification%2520with%2520LLMs%2520in%2520tasks%2520like%2520Aspect-based%2520Sentiment%2520Analysis%2520%2528ABSA%2529%2520has%2520been%2520underexplored.%2520In%2520this%2520paper%252C%2520we%2520fill%2520this%2520important%2520gap%2520by%2520presenting%2520an%2520evaluation%2520of%2520the%2520performance%2520of%2520various%2520LLMs%2520on%2520text%2520span%2520identification%2520in%2520three%2520popular%2520tasks%252C%2520namely%2520sentiment%2520analysis%252C%2520offensive%2520language%2520identification%252C%2520and%2520claim%2520verification.%2520We%2520explore%2520several%2520LLM%2520strategies%2520like%2520instruction%2520tuning%252C%2520in-context%2520learning%252C%2520and%2520chain%2520of%2520thought.%2520Our%2520results%2520indicate%2520underlying%2520relationships%2520within%2520text%2520aid%2520LLMs%2520in%2520identifying%2520precise%2520text%2520spans.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00736v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Performance%20of%20Large%20Language%20Models%20on%20Subjective%20Span%20Identification%20Tasks&entry.906535625=Alphaeus%20Dmonte%20and%20Roland%20Oruche%20and%20Tharindu%20Ranasinghe%20and%20Marcos%20Zampieri%20and%20Prasad%20Calyam&entry.1292438233=Identifying%20relevant%20text%20spans%20is%20important%20for%20several%20downstream%20tasks%20in%20NLP%2C%20as%20it%20contributes%20to%20model%20explainability.%20While%20most%20span%20identification%20approaches%20rely%20on%20relatively%20smaller%20pre-trained%20language%20models%20like%20BERT%2C%20a%20few%20recent%20approaches%20have%20leveraged%20the%20latest%20generation%20of%20Large%20Language%20Models%20%28LLMs%29%20for%20the%20task.%20Current%20work%20has%20focused%20on%20explicit%20span%20identification%20like%20Named%20Entity%20Recognition%20%28NER%29%2C%20while%20more%20subjective%20span%20identification%20with%20LLMs%20in%20tasks%20like%20Aspect-based%20Sentiment%20Analysis%20%28ABSA%29%20has%20been%20underexplored.%20In%20this%20paper%2C%20we%20fill%20this%20important%20gap%20by%20presenting%20an%20evaluation%20of%20the%20performance%20of%20various%20LLMs%20on%20text%20span%20identification%20in%20three%20popular%20tasks%2C%20namely%20sentiment%20analysis%2C%20offensive%20language%20identification%2C%20and%20claim%20verification.%20We%20explore%20several%20LLM%20strategies%20like%20instruction%20tuning%2C%20in-context%20learning%2C%20and%20chain%20of%20thought.%20Our%20results%20indicate%20underlying%20relationships%20within%20text%20aid%20LLMs%20in%20identifying%20precise%20text%20spans.&entry.1838667208=http%3A//arxiv.org/abs/2601.00736v1&entry.124074799=Read"},
{"title": "Efficient Deep Demosaicing with Spatially Downsampled Isotropic Networks", "author": "Cory Fan and Wenchao Zhang", "abstract": "In digital imaging, image demosaicing is a crucial first step which recovers the RGB information from a color filter array (CFA). Oftentimes, deep learning is utilized to perform image demosaicing. Given that most modern digital imaging applications occur on mobile platforms, applying deep learning to demosaicing requires lightweight and efficient networks. Isotropic networks, also known as residual-in-residual networks, have been often employed for image demosaicing and joint-demosaicing-and-denoising (JDD). Most demosaicing isotropic networks avoid spatial downsampling entirely, and thus are often prohibitively expensive computationally for mobile applications. Contrary to previous isotropic network designs, this paper claims that spatial downsampling to a signficant degree can improve the efficiency and performance of isotropic networks. To validate this claim, we design simple fully convolutional networks with and without downsampling using a mathematical architecture design technique adapted from DeepMAD, and find that downsampling improves empirical performance. Additionally, empirical testing of the downsampled variant, JD3Net, of our fully convolutional networks reveals strong empirical performance on a variety of image demosaicing and JDD tasks.", "link": "http://arxiv.org/abs/2601.00703v1", "date": "2026-01-02", "relevancy": 2.2551, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6013}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5887}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Deep%20Demosaicing%20with%20Spatially%20Downsampled%20Isotropic%20Networks&body=Title%3A%20Efficient%20Deep%20Demosaicing%20with%20Spatially%20Downsampled%20Isotropic%20Networks%0AAuthor%3A%20Cory%20Fan%20and%20Wenchao%20Zhang%0AAbstract%3A%20In%20digital%20imaging%2C%20image%20demosaicing%20is%20a%20crucial%20first%20step%20which%20recovers%20the%20RGB%20information%20from%20a%20color%20filter%20array%20%28CFA%29.%20Oftentimes%2C%20deep%20learning%20is%20utilized%20to%20perform%20image%20demosaicing.%20Given%20that%20most%20modern%20digital%20imaging%20applications%20occur%20on%20mobile%20platforms%2C%20applying%20deep%20learning%20to%20demosaicing%20requires%20lightweight%20and%20efficient%20networks.%20Isotropic%20networks%2C%20also%20known%20as%20residual-in-residual%20networks%2C%20have%20been%20often%20employed%20for%20image%20demosaicing%20and%20joint-demosaicing-and-denoising%20%28JDD%29.%20Most%20demosaicing%20isotropic%20networks%20avoid%20spatial%20downsampling%20entirely%2C%20and%20thus%20are%20often%20prohibitively%20expensive%20computationally%20for%20mobile%20applications.%20Contrary%20to%20previous%20isotropic%20network%20designs%2C%20this%20paper%20claims%20that%20spatial%20downsampling%20to%20a%20signficant%20degree%20can%20improve%20the%20efficiency%20and%20performance%20of%20isotropic%20networks.%20To%20validate%20this%20claim%2C%20we%20design%20simple%20fully%20convolutional%20networks%20with%20and%20without%20downsampling%20using%20a%20mathematical%20architecture%20design%20technique%20adapted%20from%20DeepMAD%2C%20and%20find%20that%20downsampling%20improves%20empirical%20performance.%20Additionally%2C%20empirical%20testing%20of%20the%20downsampled%20variant%2C%20JD3Net%2C%20of%20our%20fully%20convolutional%20networks%20reveals%20strong%20empirical%20performance%20on%20a%20variety%20of%20image%20demosaicing%20and%20JDD%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00703v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Deep%2520Demosaicing%2520with%2520Spatially%2520Downsampled%2520Isotropic%2520Networks%26entry.906535625%3DCory%2520Fan%2520and%2520Wenchao%2520Zhang%26entry.1292438233%3DIn%2520digital%2520imaging%252C%2520image%2520demosaicing%2520is%2520a%2520crucial%2520first%2520step%2520which%2520recovers%2520the%2520RGB%2520information%2520from%2520a%2520color%2520filter%2520array%2520%2528CFA%2529.%2520Oftentimes%252C%2520deep%2520learning%2520is%2520utilized%2520to%2520perform%2520image%2520demosaicing.%2520Given%2520that%2520most%2520modern%2520digital%2520imaging%2520applications%2520occur%2520on%2520mobile%2520platforms%252C%2520applying%2520deep%2520learning%2520to%2520demosaicing%2520requires%2520lightweight%2520and%2520efficient%2520networks.%2520Isotropic%2520networks%252C%2520also%2520known%2520as%2520residual-in-residual%2520networks%252C%2520have%2520been%2520often%2520employed%2520for%2520image%2520demosaicing%2520and%2520joint-demosaicing-and-denoising%2520%2528JDD%2529.%2520Most%2520demosaicing%2520isotropic%2520networks%2520avoid%2520spatial%2520downsampling%2520entirely%252C%2520and%2520thus%2520are%2520often%2520prohibitively%2520expensive%2520computationally%2520for%2520mobile%2520applications.%2520Contrary%2520to%2520previous%2520isotropic%2520network%2520designs%252C%2520this%2520paper%2520claims%2520that%2520spatial%2520downsampling%2520to%2520a%2520signficant%2520degree%2520can%2520improve%2520the%2520efficiency%2520and%2520performance%2520of%2520isotropic%2520networks.%2520To%2520validate%2520this%2520claim%252C%2520we%2520design%2520simple%2520fully%2520convolutional%2520networks%2520with%2520and%2520without%2520downsampling%2520using%2520a%2520mathematical%2520architecture%2520design%2520technique%2520adapted%2520from%2520DeepMAD%252C%2520and%2520find%2520that%2520downsampling%2520improves%2520empirical%2520performance.%2520Additionally%252C%2520empirical%2520testing%2520of%2520the%2520downsampled%2520variant%252C%2520JD3Net%252C%2520of%2520our%2520fully%2520convolutional%2520networks%2520reveals%2520strong%2520empirical%2520performance%2520on%2520a%2520variety%2520of%2520image%2520demosaicing%2520and%2520JDD%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00703v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Deep%20Demosaicing%20with%20Spatially%20Downsampled%20Isotropic%20Networks&entry.906535625=Cory%20Fan%20and%20Wenchao%20Zhang&entry.1292438233=In%20digital%20imaging%2C%20image%20demosaicing%20is%20a%20crucial%20first%20step%20which%20recovers%20the%20RGB%20information%20from%20a%20color%20filter%20array%20%28CFA%29.%20Oftentimes%2C%20deep%20learning%20is%20utilized%20to%20perform%20image%20demosaicing.%20Given%20that%20most%20modern%20digital%20imaging%20applications%20occur%20on%20mobile%20platforms%2C%20applying%20deep%20learning%20to%20demosaicing%20requires%20lightweight%20and%20efficient%20networks.%20Isotropic%20networks%2C%20also%20known%20as%20residual-in-residual%20networks%2C%20have%20been%20often%20employed%20for%20image%20demosaicing%20and%20joint-demosaicing-and-denoising%20%28JDD%29.%20Most%20demosaicing%20isotropic%20networks%20avoid%20spatial%20downsampling%20entirely%2C%20and%20thus%20are%20often%20prohibitively%20expensive%20computationally%20for%20mobile%20applications.%20Contrary%20to%20previous%20isotropic%20network%20designs%2C%20this%20paper%20claims%20that%20spatial%20downsampling%20to%20a%20signficant%20degree%20can%20improve%20the%20efficiency%20and%20performance%20of%20isotropic%20networks.%20To%20validate%20this%20claim%2C%20we%20design%20simple%20fully%20convolutional%20networks%20with%20and%20without%20downsampling%20using%20a%20mathematical%20architecture%20design%20technique%20adapted%20from%20DeepMAD%2C%20and%20find%20that%20downsampling%20improves%20empirical%20performance.%20Additionally%2C%20empirical%20testing%20of%20the%20downsampled%20variant%2C%20JD3Net%2C%20of%20our%20fully%20convolutional%20networks%20reveals%20strong%20empirical%20performance%20on%20a%20variety%20of%20image%20demosaicing%20and%20JDD%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2601.00703v1&entry.124074799=Read"},
{"title": "Frequent subgraph-based persistent homology for graph classification", "author": "Xinyang Chen and Ama\u00ebl Broustet and Guanyuan Zeng and Cheng He and Guoting Chen", "abstract": "Persistent homology (PH) has recently emerged as a powerful tool for extracting topological features. Integrating PH into machine learning and deep learning models enhances topology awareness and interpretability. However, most PH methods on graphs rely on a limited set of filtrations, such as degree-based or weight-based filtrations, which overlook richer features like recurring information across the dataset and thus restrict expressive power. In this work, we propose a novel graph filtration called Frequent Subgraph Filtration (FSF), which is derived from frequent subgraphs and produces stable and information-rich frequency-based persistent homology (FPH) features. We study the theoretical properties of FSF and provide both proofs and experimental validation. Beyond persistent homology itself, we introduce two approaches for graph classification: an FPH-based machine learning model (FPH-ML) and a hybrid framework that integrates FPH with graph neural networks (FPH-GNNs) to enhance topology-aware graph representation learning. Our frameworks bridge frequent subgraph mining and topological data analysis, offering a new perspective on topology-aware feature extraction. Experimental results show that FPH-ML achieves competitive or superior accuracy compared with kernel-based and degree-based filtration methods. When integrated into graph neural networks, FPH yields relative performance gains ranging from 0.4 to 21 percent, with improvements of up to 8.2 percentage points over GCN and GIN backbones across benchmarks.", "link": "http://arxiv.org/abs/2512.24917v2", "date": "2026-01-02", "relevancy": 2.2287, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4581}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4458}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frequent%20subgraph-based%20persistent%20homology%20for%20graph%20classification&body=Title%3A%20Frequent%20subgraph-based%20persistent%20homology%20for%20graph%20classification%0AAuthor%3A%20Xinyang%20Chen%20and%20Ama%C3%ABl%20Broustet%20and%20Guanyuan%20Zeng%20and%20Cheng%20He%20and%20Guoting%20Chen%0AAbstract%3A%20Persistent%20homology%20%28PH%29%20has%20recently%20emerged%20as%20a%20powerful%20tool%20for%20extracting%20topological%20features.%20Integrating%20PH%20into%20machine%20learning%20and%20deep%20learning%20models%20enhances%20topology%20awareness%20and%20interpretability.%20However%2C%20most%20PH%20methods%20on%20graphs%20rely%20on%20a%20limited%20set%20of%20filtrations%2C%20such%20as%20degree-based%20or%20weight-based%20filtrations%2C%20which%20overlook%20richer%20features%20like%20recurring%20information%20across%20the%20dataset%20and%20thus%20restrict%20expressive%20power.%20In%20this%20work%2C%20we%20propose%20a%20novel%20graph%20filtration%20called%20Frequent%20Subgraph%20Filtration%20%28FSF%29%2C%20which%20is%20derived%20from%20frequent%20subgraphs%20and%20produces%20stable%20and%20information-rich%20frequency-based%20persistent%20homology%20%28FPH%29%20features.%20We%20study%20the%20theoretical%20properties%20of%20FSF%20and%20provide%20both%20proofs%20and%20experimental%20validation.%20Beyond%20persistent%20homology%20itself%2C%20we%20introduce%20two%20approaches%20for%20graph%20classification%3A%20an%20FPH-based%20machine%20learning%20model%20%28FPH-ML%29%20and%20a%20hybrid%20framework%20that%20integrates%20FPH%20with%20graph%20neural%20networks%20%28FPH-GNNs%29%20to%20enhance%20topology-aware%20graph%20representation%20learning.%20Our%20frameworks%20bridge%20frequent%20subgraph%20mining%20and%20topological%20data%20analysis%2C%20offering%20a%20new%20perspective%20on%20topology-aware%20feature%20extraction.%20Experimental%20results%20show%20that%20FPH-ML%20achieves%20competitive%20or%20superior%20accuracy%20compared%20with%20kernel-based%20and%20degree-based%20filtration%20methods.%20When%20integrated%20into%20graph%20neural%20networks%2C%20FPH%20yields%20relative%20performance%20gains%20ranging%20from%200.4%20to%2021%20percent%2C%20with%20improvements%20of%20up%20to%208.2%20percentage%20points%20over%20GCN%20and%20GIN%20backbones%20across%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.24917v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrequent%2520subgraph-based%2520persistent%2520homology%2520for%2520graph%2520classification%26entry.906535625%3DXinyang%2520Chen%2520and%2520Ama%25C3%25ABl%2520Broustet%2520and%2520Guanyuan%2520Zeng%2520and%2520Cheng%2520He%2520and%2520Guoting%2520Chen%26entry.1292438233%3DPersistent%2520homology%2520%2528PH%2529%2520has%2520recently%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%2520extracting%2520topological%2520features.%2520Integrating%2520PH%2520into%2520machine%2520learning%2520and%2520deep%2520learning%2520models%2520enhances%2520topology%2520awareness%2520and%2520interpretability.%2520However%252C%2520most%2520PH%2520methods%2520on%2520graphs%2520rely%2520on%2520a%2520limited%2520set%2520of%2520filtrations%252C%2520such%2520as%2520degree-based%2520or%2520weight-based%2520filtrations%252C%2520which%2520overlook%2520richer%2520features%2520like%2520recurring%2520information%2520across%2520the%2520dataset%2520and%2520thus%2520restrict%2520expressive%2520power.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520graph%2520filtration%2520called%2520Frequent%2520Subgraph%2520Filtration%2520%2528FSF%2529%252C%2520which%2520is%2520derived%2520from%2520frequent%2520subgraphs%2520and%2520produces%2520stable%2520and%2520information-rich%2520frequency-based%2520persistent%2520homology%2520%2528FPH%2529%2520features.%2520We%2520study%2520the%2520theoretical%2520properties%2520of%2520FSF%2520and%2520provide%2520both%2520proofs%2520and%2520experimental%2520validation.%2520Beyond%2520persistent%2520homology%2520itself%252C%2520we%2520introduce%2520two%2520approaches%2520for%2520graph%2520classification%253A%2520an%2520FPH-based%2520machine%2520learning%2520model%2520%2528FPH-ML%2529%2520and%2520a%2520hybrid%2520framework%2520that%2520integrates%2520FPH%2520with%2520graph%2520neural%2520networks%2520%2528FPH-GNNs%2529%2520to%2520enhance%2520topology-aware%2520graph%2520representation%2520learning.%2520Our%2520frameworks%2520bridge%2520frequent%2520subgraph%2520mining%2520and%2520topological%2520data%2520analysis%252C%2520offering%2520a%2520new%2520perspective%2520on%2520topology-aware%2520feature%2520extraction.%2520Experimental%2520results%2520show%2520that%2520FPH-ML%2520achieves%2520competitive%2520or%2520superior%2520accuracy%2520compared%2520with%2520kernel-based%2520and%2520degree-based%2520filtration%2520methods.%2520When%2520integrated%2520into%2520graph%2520neural%2520networks%252C%2520FPH%2520yields%2520relative%2520performance%2520gains%2520ranging%2520from%25200.4%2520to%252021%2520percent%252C%2520with%2520improvements%2520of%2520up%2520to%25208.2%2520percentage%2520points%2520over%2520GCN%2520and%2520GIN%2520backbones%2520across%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.24917v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frequent%20subgraph-based%20persistent%20homology%20for%20graph%20classification&entry.906535625=Xinyang%20Chen%20and%20Ama%C3%ABl%20Broustet%20and%20Guanyuan%20Zeng%20and%20Cheng%20He%20and%20Guoting%20Chen&entry.1292438233=Persistent%20homology%20%28PH%29%20has%20recently%20emerged%20as%20a%20powerful%20tool%20for%20extracting%20topological%20features.%20Integrating%20PH%20into%20machine%20learning%20and%20deep%20learning%20models%20enhances%20topology%20awareness%20and%20interpretability.%20However%2C%20most%20PH%20methods%20on%20graphs%20rely%20on%20a%20limited%20set%20of%20filtrations%2C%20such%20as%20degree-based%20or%20weight-based%20filtrations%2C%20which%20overlook%20richer%20features%20like%20recurring%20information%20across%20the%20dataset%20and%20thus%20restrict%20expressive%20power.%20In%20this%20work%2C%20we%20propose%20a%20novel%20graph%20filtration%20called%20Frequent%20Subgraph%20Filtration%20%28FSF%29%2C%20which%20is%20derived%20from%20frequent%20subgraphs%20and%20produces%20stable%20and%20information-rich%20frequency-based%20persistent%20homology%20%28FPH%29%20features.%20We%20study%20the%20theoretical%20properties%20of%20FSF%20and%20provide%20both%20proofs%20and%20experimental%20validation.%20Beyond%20persistent%20homology%20itself%2C%20we%20introduce%20two%20approaches%20for%20graph%20classification%3A%20an%20FPH-based%20machine%20learning%20model%20%28FPH-ML%29%20and%20a%20hybrid%20framework%20that%20integrates%20FPH%20with%20graph%20neural%20networks%20%28FPH-GNNs%29%20to%20enhance%20topology-aware%20graph%20representation%20learning.%20Our%20frameworks%20bridge%20frequent%20subgraph%20mining%20and%20topological%20data%20analysis%2C%20offering%20a%20new%20perspective%20on%20topology-aware%20feature%20extraction.%20Experimental%20results%20show%20that%20FPH-ML%20achieves%20competitive%20or%20superior%20accuracy%20compared%20with%20kernel-based%20and%20degree-based%20filtration%20methods.%20When%20integrated%20into%20graph%20neural%20networks%2C%20FPH%20yields%20relative%20performance%20gains%20ranging%20from%200.4%20to%2021%20percent%2C%20with%20improvements%20of%20up%20to%208.2%20percentage%20points%20over%20GCN%20and%20GIN%20backbones%20across%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2512.24917v2&entry.124074799=Read"},
{"title": "Training a Huggingface Model on AWS Sagemaker (Without Tears)", "author": "Liling Tan", "abstract": "The development of Large Language Models (LLMs) has primarily been driven by resource-rich research groups and industry partners. Due to the lack of on-premise computing resources required for increasingly complex models, many researchers are turning to cloud services like AWS SageMaker to train Hugging Face models. However, the steep learning curve of cloud platforms often presents a barrier for researchers accustomed to local environments. Existing documentation frequently leaves knowledge gaps, forcing users to seek fragmented information across the web. This demo paper aims to democratize cloud adoption by centralizing the essential information required for researchers to successfully train their first Hugging Face model on AWS SageMaker from scratch.", "link": "http://arxiv.org/abs/2512.24098v2", "date": "2026-01-02", "relevancy": 2.2281, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4739}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4391}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20a%20Huggingface%20Model%20on%20AWS%20Sagemaker%20%28Without%20Tears%29&body=Title%3A%20Training%20a%20Huggingface%20Model%20on%20AWS%20Sagemaker%20%28Without%20Tears%29%0AAuthor%3A%20Liling%20Tan%0AAbstract%3A%20The%20development%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20primarily%20been%20driven%20by%20resource-rich%20research%20groups%20and%20industry%20partners.%20Due%20to%20the%20lack%20of%20on-premise%20computing%20resources%20required%20for%20increasingly%20complex%20models%2C%20many%20researchers%20are%20turning%20to%20cloud%20services%20like%20AWS%20SageMaker%20to%20train%20Hugging%20Face%20models.%20However%2C%20the%20steep%20learning%20curve%20of%20cloud%20platforms%20often%20presents%20a%20barrier%20for%20researchers%20accustomed%20to%20local%20environments.%20Existing%20documentation%20frequently%20leaves%20knowledge%20gaps%2C%20forcing%20users%20to%20seek%20fragmented%20information%20across%20the%20web.%20This%20demo%20paper%20aims%20to%20democratize%20cloud%20adoption%20by%20centralizing%20the%20essential%20information%20required%20for%20researchers%20to%20successfully%20train%20their%20first%20Hugging%20Face%20model%20on%20AWS%20SageMaker%20from%20scratch.%0ALink%3A%20http%3A//arxiv.org/abs/2512.24098v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520a%2520Huggingface%2520Model%2520on%2520AWS%2520Sagemaker%2520%2528Without%2520Tears%2529%26entry.906535625%3DLiling%2520Tan%26entry.1292438233%3DThe%2520development%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520primarily%2520been%2520driven%2520by%2520resource-rich%2520research%2520groups%2520and%2520industry%2520partners.%2520Due%2520to%2520the%2520lack%2520of%2520on-premise%2520computing%2520resources%2520required%2520for%2520increasingly%2520complex%2520models%252C%2520many%2520researchers%2520are%2520turning%2520to%2520cloud%2520services%2520like%2520AWS%2520SageMaker%2520to%2520train%2520Hugging%2520Face%2520models.%2520However%252C%2520the%2520steep%2520learning%2520curve%2520of%2520cloud%2520platforms%2520often%2520presents%2520a%2520barrier%2520for%2520researchers%2520accustomed%2520to%2520local%2520environments.%2520Existing%2520documentation%2520frequently%2520leaves%2520knowledge%2520gaps%252C%2520forcing%2520users%2520to%2520seek%2520fragmented%2520information%2520across%2520the%2520web.%2520This%2520demo%2520paper%2520aims%2520to%2520democratize%2520cloud%2520adoption%2520by%2520centralizing%2520the%2520essential%2520information%2520required%2520for%2520researchers%2520to%2520successfully%2520train%2520their%2520first%2520Hugging%2520Face%2520model%2520on%2520AWS%2520SageMaker%2520from%2520scratch.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.24098v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20a%20Huggingface%20Model%20on%20AWS%20Sagemaker%20%28Without%20Tears%29&entry.906535625=Liling%20Tan&entry.1292438233=The%20development%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20primarily%20been%20driven%20by%20resource-rich%20research%20groups%20and%20industry%20partners.%20Due%20to%20the%20lack%20of%20on-premise%20computing%20resources%20required%20for%20increasingly%20complex%20models%2C%20many%20researchers%20are%20turning%20to%20cloud%20services%20like%20AWS%20SageMaker%20to%20train%20Hugging%20Face%20models.%20However%2C%20the%20steep%20learning%20curve%20of%20cloud%20platforms%20often%20presents%20a%20barrier%20for%20researchers%20accustomed%20to%20local%20environments.%20Existing%20documentation%20frequently%20leaves%20knowledge%20gaps%2C%20forcing%20users%20to%20seek%20fragmented%20information%20across%20the%20web.%20This%20demo%20paper%20aims%20to%20democratize%20cloud%20adoption%20by%20centralizing%20the%20essential%20information%20required%20for%20researchers%20to%20successfully%20train%20their%20first%20Hugging%20Face%20model%20on%20AWS%20SageMaker%20from%20scratch.&entry.1838667208=http%3A//arxiv.org/abs/2512.24098v2&entry.124074799=Read"},
{"title": "Sparse FEONet: A Low-Cost, Memory-Efficient Operator Network via Finite-Element Local Sparsity for Parametric PDEs", "author": "Seungchan Ko and Jiyeon Kim and Dongwook Shin", "abstract": "In this paper, we study the finite element operator network (FEONet), an operator-learning method for parametric problems, originally introduced in J. Y. Lee, S. Ko, and Y. Hong, Finite Element Operator Network for Solving Elliptic-Type Parametric PDEs, SIAM J. Sci. Comput., 47(2), C501-C528, 2025. FEONet realizes the parameter-to-solution map on a finite element space and admits a training procedure that does not require training data, while exhibiting high accuracy and robustness across a broad class of problems. However, its computational cost increases and accuracy may deteriorate as the number of elements grows, posing notable challenges for large-scale problems. In this paper, we propose a new sparse network architecture motivated by the structure of the finite elements to address this issue. Throughout extensive numerical experiments, we show that the proposed sparse network achieves substantial improvements in computational cost and efficiency while maintaining comparable accuracy. We also establish theoretical results demonstrating that the sparse architecture can approximate the target operator effectively and provide a stability analysis ensuring reliable training and prediction.", "link": "http://arxiv.org/abs/2601.00672v1", "date": "2026-01-02", "relevancy": 2.2256, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4771}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4367}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20FEONet%3A%20A%20Low-Cost%2C%20Memory-Efficient%20Operator%20Network%20via%20Finite-Element%20Local%20Sparsity%20for%20Parametric%20PDEs&body=Title%3A%20Sparse%20FEONet%3A%20A%20Low-Cost%2C%20Memory-Efficient%20Operator%20Network%20via%20Finite-Element%20Local%20Sparsity%20for%20Parametric%20PDEs%0AAuthor%3A%20Seungchan%20Ko%20and%20Jiyeon%20Kim%20and%20Dongwook%20Shin%0AAbstract%3A%20In%20this%20paper%2C%20we%20study%20the%20finite%20element%20operator%20network%20%28FEONet%29%2C%20an%20operator-learning%20method%20for%20parametric%20problems%2C%20originally%20introduced%20in%20J.%20Y.%20Lee%2C%20S.%20Ko%2C%20and%20Y.%20Hong%2C%20Finite%20Element%20Operator%20Network%20for%20Solving%20Elliptic-Type%20Parametric%20PDEs%2C%20SIAM%20J.%20Sci.%20Comput.%2C%2047%282%29%2C%20C501-C528%2C%202025.%20FEONet%20realizes%20the%20parameter-to-solution%20map%20on%20a%20finite%20element%20space%20and%20admits%20a%20training%20procedure%20that%20does%20not%20require%20training%20data%2C%20while%20exhibiting%20high%20accuracy%20and%20robustness%20across%20a%20broad%20class%20of%20problems.%20However%2C%20its%20computational%20cost%20increases%20and%20accuracy%20may%20deteriorate%20as%20the%20number%20of%20elements%20grows%2C%20posing%20notable%20challenges%20for%20large-scale%20problems.%20In%20this%20paper%2C%20we%20propose%20a%20new%20sparse%20network%20architecture%20motivated%20by%20the%20structure%20of%20the%20finite%20elements%20to%20address%20this%20issue.%20Throughout%20extensive%20numerical%20experiments%2C%20we%20show%20that%20the%20proposed%20sparse%20network%20achieves%20substantial%20improvements%20in%20computational%20cost%20and%20efficiency%20while%20maintaining%20comparable%20accuracy.%20We%20also%20establish%20theoretical%20results%20demonstrating%20that%20the%20sparse%20architecture%20can%20approximate%20the%20target%20operator%20effectively%20and%20provide%20a%20stability%20analysis%20ensuring%20reliable%20training%20and%20prediction.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520FEONet%253A%2520A%2520Low-Cost%252C%2520Memory-Efficient%2520Operator%2520Network%2520via%2520Finite-Element%2520Local%2520Sparsity%2520for%2520Parametric%2520PDEs%26entry.906535625%3DSeungchan%2520Ko%2520and%2520Jiyeon%2520Kim%2520and%2520Dongwook%2520Shin%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520study%2520the%2520finite%2520element%2520operator%2520network%2520%2528FEONet%2529%252C%2520an%2520operator-learning%2520method%2520for%2520parametric%2520problems%252C%2520originally%2520introduced%2520in%2520J.%2520Y.%2520Lee%252C%2520S.%2520Ko%252C%2520and%2520Y.%2520Hong%252C%2520Finite%2520Element%2520Operator%2520Network%2520for%2520Solving%2520Elliptic-Type%2520Parametric%2520PDEs%252C%2520SIAM%2520J.%2520Sci.%2520Comput.%252C%252047%25282%2529%252C%2520C501-C528%252C%25202025.%2520FEONet%2520realizes%2520the%2520parameter-to-solution%2520map%2520on%2520a%2520finite%2520element%2520space%2520and%2520admits%2520a%2520training%2520procedure%2520that%2520does%2520not%2520require%2520training%2520data%252C%2520while%2520exhibiting%2520high%2520accuracy%2520and%2520robustness%2520across%2520a%2520broad%2520class%2520of%2520problems.%2520However%252C%2520its%2520computational%2520cost%2520increases%2520and%2520accuracy%2520may%2520deteriorate%2520as%2520the%2520number%2520of%2520elements%2520grows%252C%2520posing%2520notable%2520challenges%2520for%2520large-scale%2520problems.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520sparse%2520network%2520architecture%2520motivated%2520by%2520the%2520structure%2520of%2520the%2520finite%2520elements%2520to%2520address%2520this%2520issue.%2520Throughout%2520extensive%2520numerical%2520experiments%252C%2520we%2520show%2520that%2520the%2520proposed%2520sparse%2520network%2520achieves%2520substantial%2520improvements%2520in%2520computational%2520cost%2520and%2520efficiency%2520while%2520maintaining%2520comparable%2520accuracy.%2520We%2520also%2520establish%2520theoretical%2520results%2520demonstrating%2520that%2520the%2520sparse%2520architecture%2520can%2520approximate%2520the%2520target%2520operator%2520effectively%2520and%2520provide%2520a%2520stability%2520analysis%2520ensuring%2520reliable%2520training%2520and%2520prediction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20FEONet%3A%20A%20Low-Cost%2C%20Memory-Efficient%20Operator%20Network%20via%20Finite-Element%20Local%20Sparsity%20for%20Parametric%20PDEs&entry.906535625=Seungchan%20Ko%20and%20Jiyeon%20Kim%20and%20Dongwook%20Shin&entry.1292438233=In%20this%20paper%2C%20we%20study%20the%20finite%20element%20operator%20network%20%28FEONet%29%2C%20an%20operator-learning%20method%20for%20parametric%20problems%2C%20originally%20introduced%20in%20J.%20Y.%20Lee%2C%20S.%20Ko%2C%20and%20Y.%20Hong%2C%20Finite%20Element%20Operator%20Network%20for%20Solving%20Elliptic-Type%20Parametric%20PDEs%2C%20SIAM%20J.%20Sci.%20Comput.%2C%2047%282%29%2C%20C501-C528%2C%202025.%20FEONet%20realizes%20the%20parameter-to-solution%20map%20on%20a%20finite%20element%20space%20and%20admits%20a%20training%20procedure%20that%20does%20not%20require%20training%20data%2C%20while%20exhibiting%20high%20accuracy%20and%20robustness%20across%20a%20broad%20class%20of%20problems.%20However%2C%20its%20computational%20cost%20increases%20and%20accuracy%20may%20deteriorate%20as%20the%20number%20of%20elements%20grows%2C%20posing%20notable%20challenges%20for%20large-scale%20problems.%20In%20this%20paper%2C%20we%20propose%20a%20new%20sparse%20network%20architecture%20motivated%20by%20the%20structure%20of%20the%20finite%20elements%20to%20address%20this%20issue.%20Throughout%20extensive%20numerical%20experiments%2C%20we%20show%20that%20the%20proposed%20sparse%20network%20achieves%20substantial%20improvements%20in%20computational%20cost%20and%20efficiency%20while%20maintaining%20comparable%20accuracy.%20We%20also%20establish%20theoretical%20results%20demonstrating%20that%20the%20sparse%20architecture%20can%20approximate%20the%20target%20operator%20effectively%20and%20provide%20a%20stability%20analysis%20ensuring%20reliable%20training%20and%20prediction.&entry.1838667208=http%3A//arxiv.org/abs/2601.00672v1&entry.124074799=Read"},
{"title": "A Vision-and-Knowledge Enhanced Large Language Model for Generalizable Pedestrian Crossing Behavior Inference", "author": "Qingwen Pu and Kun Xie and Hong Yang and Guocong Zhai", "abstract": "Existing paradigms for inferring pedestrian crossing behavior, ranging from statistical models to supervised learning methods, demonstrate limited generalizability and perform inadequately on new sites. Recent advances in Large Language Models (LLMs) offer a shift from numerical pattern fitting to semantic, context-aware behavioral reasoning, yet existing LLM applications lack domain-specific adaptation and visual context. This study introduces Pedestrian Crossing LLM (PedX-LLM), a vision-and-knowledge enhanced framework designed to transform pedestrian crossing inference from site-specific pattern recognition to generalizable behavioral reasoning. By integrating LLaVA-extracted visual features with textual data and transportation domain knowledge, PedX-LLM fine-tunes a LLaMA-2-7B foundation model via Low-Rank Adaptation (LoRA) to infer crossing decisions. PedX-LLM achieves 82.0% balanced accuracy, outperforming the best statistical and supervised learning methods. Results demonstrate that the vision-augmented module contributes a 2.9% performance gain by capturing the built environment and integrating domain knowledge yields an additional 4.1% improvement. To evaluate generalizability across unseen environments, cross-site validation was conducted using site-based partitioning. The zero-shot PedX-LLM configuration achieves 66.9% balanced accuracy on five unseen test sites, outperforming the baseline data-driven methods by at least 18 percentage points. Incorporating just five validation examples via few-shot learning to PedX-LLM further elevates the balanced accuracy to 72.2%. PedX-LLM demonstrates strong generalizability to unseen scenarios, confirming that vision-and-knowledge-enhanced reasoning enables the model to mimic human-like decision logic and overcome the limitations of purely data-driven methods.", "link": "http://arxiv.org/abs/2601.00694v1", "date": "2026-01-02", "relevancy": 2.1804, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5477}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5477}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5322}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Vision-and-Knowledge%20Enhanced%20Large%20Language%20Model%20for%20Generalizable%20Pedestrian%20Crossing%20Behavior%20Inference&body=Title%3A%20A%20Vision-and-Knowledge%20Enhanced%20Large%20Language%20Model%20for%20Generalizable%20Pedestrian%20Crossing%20Behavior%20Inference%0AAuthor%3A%20Qingwen%20Pu%20and%20Kun%20Xie%20and%20Hong%20Yang%20and%20Guocong%20Zhai%0AAbstract%3A%20Existing%20paradigms%20for%20inferring%20pedestrian%20crossing%20behavior%2C%20ranging%20from%20statistical%20models%20to%20supervised%20learning%20methods%2C%20demonstrate%20limited%20generalizability%20and%20perform%20inadequately%20on%20new%20sites.%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20offer%20a%20shift%20from%20numerical%20pattern%20fitting%20to%20semantic%2C%20context-aware%20behavioral%20reasoning%2C%20yet%20existing%20LLM%20applications%20lack%20domain-specific%20adaptation%20and%20visual%20context.%20This%20study%20introduces%20Pedestrian%20Crossing%20LLM%20%28PedX-LLM%29%2C%20a%20vision-and-knowledge%20enhanced%20framework%20designed%20to%20transform%20pedestrian%20crossing%20inference%20from%20site-specific%20pattern%20recognition%20to%20generalizable%20behavioral%20reasoning.%20By%20integrating%20LLaVA-extracted%20visual%20features%20with%20textual%20data%20and%20transportation%20domain%20knowledge%2C%20PedX-LLM%20fine-tunes%20a%20LLaMA-2-7B%20foundation%20model%20via%20Low-Rank%20Adaptation%20%28LoRA%29%20to%20infer%20crossing%20decisions.%20PedX-LLM%20achieves%2082.0%25%20balanced%20accuracy%2C%20outperforming%20the%20best%20statistical%20and%20supervised%20learning%20methods.%20Results%20demonstrate%20that%20the%20vision-augmented%20module%20contributes%20a%202.9%25%20performance%20gain%20by%20capturing%20the%20built%20environment%20and%20integrating%20domain%20knowledge%20yields%20an%20additional%204.1%25%20improvement.%20To%20evaluate%20generalizability%20across%20unseen%20environments%2C%20cross-site%20validation%20was%20conducted%20using%20site-based%20partitioning.%20The%20zero-shot%20PedX-LLM%20configuration%20achieves%2066.9%25%20balanced%20accuracy%20on%20five%20unseen%20test%20sites%2C%20outperforming%20the%20baseline%20data-driven%20methods%20by%20at%20least%2018%20percentage%20points.%20Incorporating%20just%20five%20validation%20examples%20via%20few-shot%20learning%20to%20PedX-LLM%20further%20elevates%20the%20balanced%20accuracy%20to%2072.2%25.%20PedX-LLM%20demonstrates%20strong%20generalizability%20to%20unseen%20scenarios%2C%20confirming%20that%20vision-and-knowledge-enhanced%20reasoning%20enables%20the%20model%20to%20mimic%20human-like%20decision%20logic%20and%20overcome%20the%20limitations%20of%20purely%20data-driven%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00694v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Vision-and-Knowledge%2520Enhanced%2520Large%2520Language%2520Model%2520for%2520Generalizable%2520Pedestrian%2520Crossing%2520Behavior%2520Inference%26entry.906535625%3DQingwen%2520Pu%2520and%2520Kun%2520Xie%2520and%2520Hong%2520Yang%2520and%2520Guocong%2520Zhai%26entry.1292438233%3DExisting%2520paradigms%2520for%2520inferring%2520pedestrian%2520crossing%2520behavior%252C%2520ranging%2520from%2520statistical%2520models%2520to%2520supervised%2520learning%2520methods%252C%2520demonstrate%2520limited%2520generalizability%2520and%2520perform%2520inadequately%2520on%2520new%2520sites.%2520Recent%2520advances%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520offer%2520a%2520shift%2520from%2520numerical%2520pattern%2520fitting%2520to%2520semantic%252C%2520context-aware%2520behavioral%2520reasoning%252C%2520yet%2520existing%2520LLM%2520applications%2520lack%2520domain-specific%2520adaptation%2520and%2520visual%2520context.%2520This%2520study%2520introduces%2520Pedestrian%2520Crossing%2520LLM%2520%2528PedX-LLM%2529%252C%2520a%2520vision-and-knowledge%2520enhanced%2520framework%2520designed%2520to%2520transform%2520pedestrian%2520crossing%2520inference%2520from%2520site-specific%2520pattern%2520recognition%2520to%2520generalizable%2520behavioral%2520reasoning.%2520By%2520integrating%2520LLaVA-extracted%2520visual%2520features%2520with%2520textual%2520data%2520and%2520transportation%2520domain%2520knowledge%252C%2520PedX-LLM%2520fine-tunes%2520a%2520LLaMA-2-7B%2520foundation%2520model%2520via%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520to%2520infer%2520crossing%2520decisions.%2520PedX-LLM%2520achieves%252082.0%2525%2520balanced%2520accuracy%252C%2520outperforming%2520the%2520best%2520statistical%2520and%2520supervised%2520learning%2520methods.%2520Results%2520demonstrate%2520that%2520the%2520vision-augmented%2520module%2520contributes%2520a%25202.9%2525%2520performance%2520gain%2520by%2520capturing%2520the%2520built%2520environment%2520and%2520integrating%2520domain%2520knowledge%2520yields%2520an%2520additional%25204.1%2525%2520improvement.%2520To%2520evaluate%2520generalizability%2520across%2520unseen%2520environments%252C%2520cross-site%2520validation%2520was%2520conducted%2520using%2520site-based%2520partitioning.%2520The%2520zero-shot%2520PedX-LLM%2520configuration%2520achieves%252066.9%2525%2520balanced%2520accuracy%2520on%2520five%2520unseen%2520test%2520sites%252C%2520outperforming%2520the%2520baseline%2520data-driven%2520methods%2520by%2520at%2520least%252018%2520percentage%2520points.%2520Incorporating%2520just%2520five%2520validation%2520examples%2520via%2520few-shot%2520learning%2520to%2520PedX-LLM%2520further%2520elevates%2520the%2520balanced%2520accuracy%2520to%252072.2%2525.%2520PedX-LLM%2520demonstrates%2520strong%2520generalizability%2520to%2520unseen%2520scenarios%252C%2520confirming%2520that%2520vision-and-knowledge-enhanced%2520reasoning%2520enables%2520the%2520model%2520to%2520mimic%2520human-like%2520decision%2520logic%2520and%2520overcome%2520the%2520limitations%2520of%2520purely%2520data-driven%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00694v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Vision-and-Knowledge%20Enhanced%20Large%20Language%20Model%20for%20Generalizable%20Pedestrian%20Crossing%20Behavior%20Inference&entry.906535625=Qingwen%20Pu%20and%20Kun%20Xie%20and%20Hong%20Yang%20and%20Guocong%20Zhai&entry.1292438233=Existing%20paradigms%20for%20inferring%20pedestrian%20crossing%20behavior%2C%20ranging%20from%20statistical%20models%20to%20supervised%20learning%20methods%2C%20demonstrate%20limited%20generalizability%20and%20perform%20inadequately%20on%20new%20sites.%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20offer%20a%20shift%20from%20numerical%20pattern%20fitting%20to%20semantic%2C%20context-aware%20behavioral%20reasoning%2C%20yet%20existing%20LLM%20applications%20lack%20domain-specific%20adaptation%20and%20visual%20context.%20This%20study%20introduces%20Pedestrian%20Crossing%20LLM%20%28PedX-LLM%29%2C%20a%20vision-and-knowledge%20enhanced%20framework%20designed%20to%20transform%20pedestrian%20crossing%20inference%20from%20site-specific%20pattern%20recognition%20to%20generalizable%20behavioral%20reasoning.%20By%20integrating%20LLaVA-extracted%20visual%20features%20with%20textual%20data%20and%20transportation%20domain%20knowledge%2C%20PedX-LLM%20fine-tunes%20a%20LLaMA-2-7B%20foundation%20model%20via%20Low-Rank%20Adaptation%20%28LoRA%29%20to%20infer%20crossing%20decisions.%20PedX-LLM%20achieves%2082.0%25%20balanced%20accuracy%2C%20outperforming%20the%20best%20statistical%20and%20supervised%20learning%20methods.%20Results%20demonstrate%20that%20the%20vision-augmented%20module%20contributes%20a%202.9%25%20performance%20gain%20by%20capturing%20the%20built%20environment%20and%20integrating%20domain%20knowledge%20yields%20an%20additional%204.1%25%20improvement.%20To%20evaluate%20generalizability%20across%20unseen%20environments%2C%20cross-site%20validation%20was%20conducted%20using%20site-based%20partitioning.%20The%20zero-shot%20PedX-LLM%20configuration%20achieves%2066.9%25%20balanced%20accuracy%20on%20five%20unseen%20test%20sites%2C%20outperforming%20the%20baseline%20data-driven%20methods%20by%20at%20least%2018%20percentage%20points.%20Incorporating%20just%20five%20validation%20examples%20via%20few-shot%20learning%20to%20PedX-LLM%20further%20elevates%20the%20balanced%20accuracy%20to%2072.2%25.%20PedX-LLM%20demonstrates%20strong%20generalizability%20to%20unseen%20scenarios%2C%20confirming%20that%20vision-and-knowledge-enhanced%20reasoning%20enables%20the%20model%20to%20mimic%20human-like%20decision%20logic%20and%20overcome%20the%20limitations%20of%20purely%20data-driven%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2601.00694v1&entry.124074799=Read"},
{"title": "Brain network science modelling of sparse neural networks enables Transformers and LLMs to perform as fully connected", "author": "Yingtao Zhang and Diego Cerretti and Jialin Zhao and Wenjing Wu and Ziheng Liao and Umberto Michieli and Carlo Vittorio Cannistraci", "abstract": "Dynamic sparse training (DST) can reduce the computational demands in ANNs, but faces difficulties in keeping peak performance at high sparsity levels. The Cannistraci-Hebb training (CHT) is a brain-inspired method for growing connectivity in DST. CHT leverages a gradient-free, topology-driven link regrowth, which has shown ultra-sparse (less than 1% connectivity) advantage across various tasks compared to fully connected networks. Yet, CHT suffers two main drawbacks: (i) its time complexity is $O(Nd^3)$ - N node network size, d node degree - restricting it to ultra-sparse regimes. (ii) it selects top link prediction scores, which is inappropriate for the early training epochs, when the network presents unreliable connections. Here, we design the first brain-inspired network model - termed bipartite receptive field (BRF) - to initialize the connectivity of sparse artificial neural networks. We further introduce a GPU-friendly matrix-based approximation of CH link prediction, reducing complexity to $O(N^3)$. We introduce the Cannistraci-Hebb training soft rule (CHTs), which adopts a flexible strategy for sampling connections in both link removal and regrowth, balancing the exploration and exploitation of network topology. Additionally, we integrate CHTs with a sigmoid gradual density decay (CHTss). Empirical results show that BRF offers performance advantages over previous network science models. Using 1% of connections, CHTs outperforms fully connected networks in MLP architectures on image classification tasks, compressing some networks to less than 30% of the nodes. Using 5% of the connections, CHTss outperforms fully connected networks in two Transformer-based machine translation tasks. Finally, at 30% connectivity, both CHTs and CHTss outperform other DST methods in language modeling task.", "link": "http://arxiv.org/abs/2501.19107v3", "date": "2026-01-02", "relevancy": 2.1363, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5567}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5279}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brain%20network%20science%20modelling%20of%20sparse%20neural%20networks%20enables%20Transformers%20and%20LLMs%20to%20perform%20as%20fully%20connected&body=Title%3A%20Brain%20network%20science%20modelling%20of%20sparse%20neural%20networks%20enables%20Transformers%20and%20LLMs%20to%20perform%20as%20fully%20connected%0AAuthor%3A%20Yingtao%20Zhang%20and%20Diego%20Cerretti%20and%20Jialin%20Zhao%20and%20Wenjing%20Wu%20and%20Ziheng%20Liao%20and%20Umberto%20Michieli%20and%20Carlo%20Vittorio%20Cannistraci%0AAbstract%3A%20Dynamic%20sparse%20training%20%28DST%29%20can%20reduce%20the%20computational%20demands%20in%20ANNs%2C%20but%20faces%20difficulties%20in%20keeping%20peak%20performance%20at%20high%20sparsity%20levels.%20The%20Cannistraci-Hebb%20training%20%28CHT%29%20is%20a%20brain-inspired%20method%20for%20growing%20connectivity%20in%20DST.%20CHT%20leverages%20a%20gradient-free%2C%20topology-driven%20link%20regrowth%2C%20which%20has%20shown%20ultra-sparse%20%28less%20than%201%25%20connectivity%29%20advantage%20across%20various%20tasks%20compared%20to%20fully%20connected%20networks.%20Yet%2C%20CHT%20suffers%20two%20main%20drawbacks%3A%20%28i%29%20its%20time%20complexity%20is%20%24O%28Nd%5E3%29%24%20-%20N%20node%20network%20size%2C%20d%20node%20degree%20-%20restricting%20it%20to%20ultra-sparse%20regimes.%20%28ii%29%20it%20selects%20top%20link%20prediction%20scores%2C%20which%20is%20inappropriate%20for%20the%20early%20training%20epochs%2C%20when%20the%20network%20presents%20unreliable%20connections.%20Here%2C%20we%20design%20the%20first%20brain-inspired%20network%20model%20-%20termed%20bipartite%20receptive%20field%20%28BRF%29%20-%20to%20initialize%20the%20connectivity%20of%20sparse%20artificial%20neural%20networks.%20We%20further%20introduce%20a%20GPU-friendly%20matrix-based%20approximation%20of%20CH%20link%20prediction%2C%20reducing%20complexity%20to%20%24O%28N%5E3%29%24.%20We%20introduce%20the%20Cannistraci-Hebb%20training%20soft%20rule%20%28CHTs%29%2C%20which%20adopts%20a%20flexible%20strategy%20for%20sampling%20connections%20in%20both%20link%20removal%20and%20regrowth%2C%20balancing%20the%20exploration%20and%20exploitation%20of%20network%20topology.%20Additionally%2C%20we%20integrate%20CHTs%20with%20a%20sigmoid%20gradual%20density%20decay%20%28CHTss%29.%20Empirical%20results%20show%20that%20BRF%20offers%20performance%20advantages%20over%20previous%20network%20science%20models.%20Using%201%25%20of%20connections%2C%20CHTs%20outperforms%20fully%20connected%20networks%20in%20MLP%20architectures%20on%20image%20classification%20tasks%2C%20compressing%20some%20networks%20to%20less%20than%2030%25%20of%20the%20nodes.%20Using%205%25%20of%20the%20connections%2C%20CHTss%20outperforms%20fully%20connected%20networks%20in%20two%20Transformer-based%20machine%20translation%20tasks.%20Finally%2C%20at%2030%25%20connectivity%2C%20both%20CHTs%20and%20CHTss%20outperform%20other%20DST%20methods%20in%20language%20modeling%20task.%0ALink%3A%20http%3A//arxiv.org/abs/2501.19107v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrain%2520network%2520science%2520modelling%2520of%2520sparse%2520neural%2520networks%2520enables%2520Transformers%2520and%2520LLMs%2520to%2520perform%2520as%2520fully%2520connected%26entry.906535625%3DYingtao%2520Zhang%2520and%2520Diego%2520Cerretti%2520and%2520Jialin%2520Zhao%2520and%2520Wenjing%2520Wu%2520and%2520Ziheng%2520Liao%2520and%2520Umberto%2520Michieli%2520and%2520Carlo%2520Vittorio%2520Cannistraci%26entry.1292438233%3DDynamic%2520sparse%2520training%2520%2528DST%2529%2520can%2520reduce%2520the%2520computational%2520demands%2520in%2520ANNs%252C%2520but%2520faces%2520difficulties%2520in%2520keeping%2520peak%2520performance%2520at%2520high%2520sparsity%2520levels.%2520The%2520Cannistraci-Hebb%2520training%2520%2528CHT%2529%2520is%2520a%2520brain-inspired%2520method%2520for%2520growing%2520connectivity%2520in%2520DST.%2520CHT%2520leverages%2520a%2520gradient-free%252C%2520topology-driven%2520link%2520regrowth%252C%2520which%2520has%2520shown%2520ultra-sparse%2520%2528less%2520than%25201%2525%2520connectivity%2529%2520advantage%2520across%2520various%2520tasks%2520compared%2520to%2520fully%2520connected%2520networks.%2520Yet%252C%2520CHT%2520suffers%2520two%2520main%2520drawbacks%253A%2520%2528i%2529%2520its%2520time%2520complexity%2520is%2520%2524O%2528Nd%255E3%2529%2524%2520-%2520N%2520node%2520network%2520size%252C%2520d%2520node%2520degree%2520-%2520restricting%2520it%2520to%2520ultra-sparse%2520regimes.%2520%2528ii%2529%2520it%2520selects%2520top%2520link%2520prediction%2520scores%252C%2520which%2520is%2520inappropriate%2520for%2520the%2520early%2520training%2520epochs%252C%2520when%2520the%2520network%2520presents%2520unreliable%2520connections.%2520Here%252C%2520we%2520design%2520the%2520first%2520brain-inspired%2520network%2520model%2520-%2520termed%2520bipartite%2520receptive%2520field%2520%2528BRF%2529%2520-%2520to%2520initialize%2520the%2520connectivity%2520of%2520sparse%2520artificial%2520neural%2520networks.%2520We%2520further%2520introduce%2520a%2520GPU-friendly%2520matrix-based%2520approximation%2520of%2520CH%2520link%2520prediction%252C%2520reducing%2520complexity%2520to%2520%2524O%2528N%255E3%2529%2524.%2520We%2520introduce%2520the%2520Cannistraci-Hebb%2520training%2520soft%2520rule%2520%2528CHTs%2529%252C%2520which%2520adopts%2520a%2520flexible%2520strategy%2520for%2520sampling%2520connections%2520in%2520both%2520link%2520removal%2520and%2520regrowth%252C%2520balancing%2520the%2520exploration%2520and%2520exploitation%2520of%2520network%2520topology.%2520Additionally%252C%2520we%2520integrate%2520CHTs%2520with%2520a%2520sigmoid%2520gradual%2520density%2520decay%2520%2528CHTss%2529.%2520Empirical%2520results%2520show%2520that%2520BRF%2520offers%2520performance%2520advantages%2520over%2520previous%2520network%2520science%2520models.%2520Using%25201%2525%2520of%2520connections%252C%2520CHTs%2520outperforms%2520fully%2520connected%2520networks%2520in%2520MLP%2520architectures%2520on%2520image%2520classification%2520tasks%252C%2520compressing%2520some%2520networks%2520to%2520less%2520than%252030%2525%2520of%2520the%2520nodes.%2520Using%25205%2525%2520of%2520the%2520connections%252C%2520CHTss%2520outperforms%2520fully%2520connected%2520networks%2520in%2520two%2520Transformer-based%2520machine%2520translation%2520tasks.%2520Finally%252C%2520at%252030%2525%2520connectivity%252C%2520both%2520CHTs%2520and%2520CHTss%2520outperform%2520other%2520DST%2520methods%2520in%2520language%2520modeling%2520task.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19107v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brain%20network%20science%20modelling%20of%20sparse%20neural%20networks%20enables%20Transformers%20and%20LLMs%20to%20perform%20as%20fully%20connected&entry.906535625=Yingtao%20Zhang%20and%20Diego%20Cerretti%20and%20Jialin%20Zhao%20and%20Wenjing%20Wu%20and%20Ziheng%20Liao%20and%20Umberto%20Michieli%20and%20Carlo%20Vittorio%20Cannistraci&entry.1292438233=Dynamic%20sparse%20training%20%28DST%29%20can%20reduce%20the%20computational%20demands%20in%20ANNs%2C%20but%20faces%20difficulties%20in%20keeping%20peak%20performance%20at%20high%20sparsity%20levels.%20The%20Cannistraci-Hebb%20training%20%28CHT%29%20is%20a%20brain-inspired%20method%20for%20growing%20connectivity%20in%20DST.%20CHT%20leverages%20a%20gradient-free%2C%20topology-driven%20link%20regrowth%2C%20which%20has%20shown%20ultra-sparse%20%28less%20than%201%25%20connectivity%29%20advantage%20across%20various%20tasks%20compared%20to%20fully%20connected%20networks.%20Yet%2C%20CHT%20suffers%20two%20main%20drawbacks%3A%20%28i%29%20its%20time%20complexity%20is%20%24O%28Nd%5E3%29%24%20-%20N%20node%20network%20size%2C%20d%20node%20degree%20-%20restricting%20it%20to%20ultra-sparse%20regimes.%20%28ii%29%20it%20selects%20top%20link%20prediction%20scores%2C%20which%20is%20inappropriate%20for%20the%20early%20training%20epochs%2C%20when%20the%20network%20presents%20unreliable%20connections.%20Here%2C%20we%20design%20the%20first%20brain-inspired%20network%20model%20-%20termed%20bipartite%20receptive%20field%20%28BRF%29%20-%20to%20initialize%20the%20connectivity%20of%20sparse%20artificial%20neural%20networks.%20We%20further%20introduce%20a%20GPU-friendly%20matrix-based%20approximation%20of%20CH%20link%20prediction%2C%20reducing%20complexity%20to%20%24O%28N%5E3%29%24.%20We%20introduce%20the%20Cannistraci-Hebb%20training%20soft%20rule%20%28CHTs%29%2C%20which%20adopts%20a%20flexible%20strategy%20for%20sampling%20connections%20in%20both%20link%20removal%20and%20regrowth%2C%20balancing%20the%20exploration%20and%20exploitation%20of%20network%20topology.%20Additionally%2C%20we%20integrate%20CHTs%20with%20a%20sigmoid%20gradual%20density%20decay%20%28CHTss%29.%20Empirical%20results%20show%20that%20BRF%20offers%20performance%20advantages%20over%20previous%20network%20science%20models.%20Using%201%25%20of%20connections%2C%20CHTs%20outperforms%20fully%20connected%20networks%20in%20MLP%20architectures%20on%20image%20classification%20tasks%2C%20compressing%20some%20networks%20to%20less%20than%2030%25%20of%20the%20nodes.%20Using%205%25%20of%20the%20connections%2C%20CHTss%20outperforms%20fully%20connected%20networks%20in%20two%20Transformer-based%20machine%20translation%20tasks.%20Finally%2C%20at%2030%25%20connectivity%2C%20both%20CHTs%20and%20CHTss%20outperform%20other%20DST%20methods%20in%20language%20modeling%20task.&entry.1838667208=http%3A//arxiv.org/abs/2501.19107v3&entry.124074799=Read"},
{"title": "Simulation as Supervision: Mechanistic Pretraining for Scientific Discovery", "author": "Carson Dudley and Reiden Magdaleno and Christopher Harding and Marisa Eisenberg", "abstract": "Scientific modeling faces a tradeoff between the interpretability of mechanistic theory and the predictive power of machine learning. While hybrid approaches like Physics-Informed Neural Networks (PINNs) embed domain knowledge as functional constraints, they can be brittle under model misspecification. We introduce Simulation-Grounded Neural Networks (SGNNs), a framework that instead embeds domain knowledge into the training data to establish a structural prior.\n  By pretraining on synthetic corpora spanning diverse model structures and observational artifacts, SGNNs learn the broad patterns of physical possibility. This allows the model to internalize the underlying dynamics of a system without being forced to satisfy a single, potentially incorrect equation.\n  We evaluated SGNNs across scientific disciplines and found that this approach confers significant robustness. In prediction tasks, SGNNs nearly tripled COVID-19 forecasting skill versus CDC baselines. In tests on dengue outbreaks, SGNNs outperformed physics-constrained models even when both were restricted to incorrect human-to-human transmission equations, demonstrating that SGNNs are potentially more robust to model misspecification. For inference, SGNNs extend the logic of simulation-based inference to enable supervised learning for unobservable targets, estimating early COVID-19 transmissibility more accurately than traditional methods. Finally, SGNNs enable back-to-simulation attribution, a form of mechanistic interpretability that maps real-world data back to the simulated manifold to identify underlying processes. By unifying these disparate simulation-based techniques into a single framework, we demonstrate that mechanistic simulations can serve as effective training data for robust scientific inference that generalizes beyond the limitations of fixed functional forms.", "link": "http://arxiv.org/abs/2507.08977v3", "date": "2026-01-02", "relevancy": 2.1116, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5349}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5291}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5204}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simulation%20as%20Supervision%3A%20Mechanistic%20Pretraining%20for%20Scientific%20Discovery&body=Title%3A%20Simulation%20as%20Supervision%3A%20Mechanistic%20Pretraining%20for%20Scientific%20Discovery%0AAuthor%3A%20Carson%20Dudley%20and%20Reiden%20Magdaleno%20and%20Christopher%20Harding%20and%20Marisa%20Eisenberg%0AAbstract%3A%20Scientific%20modeling%20faces%20a%20tradeoff%20between%20the%20interpretability%20of%20mechanistic%20theory%20and%20the%20predictive%20power%20of%20machine%20learning.%20While%20hybrid%20approaches%20like%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20embed%20domain%20knowledge%20as%20functional%20constraints%2C%20they%20can%20be%20brittle%20under%20model%20misspecification.%20We%20introduce%20Simulation-Grounded%20Neural%20Networks%20%28SGNNs%29%2C%20a%20framework%20that%20instead%20embeds%20domain%20knowledge%20into%20the%20training%20data%20to%20establish%20a%20structural%20prior.%0A%20%20By%20pretraining%20on%20synthetic%20corpora%20spanning%20diverse%20model%20structures%20and%20observational%20artifacts%2C%20SGNNs%20learn%20the%20broad%20patterns%20of%20physical%20possibility.%20This%20allows%20the%20model%20to%20internalize%20the%20underlying%20dynamics%20of%20a%20system%20without%20being%20forced%20to%20satisfy%20a%20single%2C%20potentially%20incorrect%20equation.%0A%20%20We%20evaluated%20SGNNs%20across%20scientific%20disciplines%20and%20found%20that%20this%20approach%20confers%20significant%20robustness.%20In%20prediction%20tasks%2C%20SGNNs%20nearly%20tripled%20COVID-19%20forecasting%20skill%20versus%20CDC%20baselines.%20In%20tests%20on%20dengue%20outbreaks%2C%20SGNNs%20outperformed%20physics-constrained%20models%20even%20when%20both%20were%20restricted%20to%20incorrect%20human-to-human%20transmission%20equations%2C%20demonstrating%20that%20SGNNs%20are%20potentially%20more%20robust%20to%20model%20misspecification.%20For%20inference%2C%20SGNNs%20extend%20the%20logic%20of%20simulation-based%20inference%20to%20enable%20supervised%20learning%20for%20unobservable%20targets%2C%20estimating%20early%20COVID-19%20transmissibility%20more%20accurately%20than%20traditional%20methods.%20Finally%2C%20SGNNs%20enable%20back-to-simulation%20attribution%2C%20a%20form%20of%20mechanistic%20interpretability%20that%20maps%20real-world%20data%20back%20to%20the%20simulated%20manifold%20to%20identify%20underlying%20processes.%20By%20unifying%20these%20disparate%20simulation-based%20techniques%20into%20a%20single%20framework%2C%20we%20demonstrate%20that%20mechanistic%20simulations%20can%20serve%20as%20effective%20training%20data%20for%20robust%20scientific%20inference%20that%20generalizes%20beyond%20the%20limitations%20of%20fixed%20functional%20forms.%0ALink%3A%20http%3A//arxiv.org/abs/2507.08977v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimulation%2520as%2520Supervision%253A%2520Mechanistic%2520Pretraining%2520for%2520Scientific%2520Discovery%26entry.906535625%3DCarson%2520Dudley%2520and%2520Reiden%2520Magdaleno%2520and%2520Christopher%2520Harding%2520and%2520Marisa%2520Eisenberg%26entry.1292438233%3DScientific%2520modeling%2520faces%2520a%2520tradeoff%2520between%2520the%2520interpretability%2520of%2520mechanistic%2520theory%2520and%2520the%2520predictive%2520power%2520of%2520machine%2520learning.%2520While%2520hybrid%2520approaches%2520like%2520Physics-Informed%2520Neural%2520Networks%2520%2528PINNs%2529%2520embed%2520domain%2520knowledge%2520as%2520functional%2520constraints%252C%2520they%2520can%2520be%2520brittle%2520under%2520model%2520misspecification.%2520We%2520introduce%2520Simulation-Grounded%2520Neural%2520Networks%2520%2528SGNNs%2529%252C%2520a%2520framework%2520that%2520instead%2520embeds%2520domain%2520knowledge%2520into%2520the%2520training%2520data%2520to%2520establish%2520a%2520structural%2520prior.%250A%2520%2520By%2520pretraining%2520on%2520synthetic%2520corpora%2520spanning%2520diverse%2520model%2520structures%2520and%2520observational%2520artifacts%252C%2520SGNNs%2520learn%2520the%2520broad%2520patterns%2520of%2520physical%2520possibility.%2520This%2520allows%2520the%2520model%2520to%2520internalize%2520the%2520underlying%2520dynamics%2520of%2520a%2520system%2520without%2520being%2520forced%2520to%2520satisfy%2520a%2520single%252C%2520potentially%2520incorrect%2520equation.%250A%2520%2520We%2520evaluated%2520SGNNs%2520across%2520scientific%2520disciplines%2520and%2520found%2520that%2520this%2520approach%2520confers%2520significant%2520robustness.%2520In%2520prediction%2520tasks%252C%2520SGNNs%2520nearly%2520tripled%2520COVID-19%2520forecasting%2520skill%2520versus%2520CDC%2520baselines.%2520In%2520tests%2520on%2520dengue%2520outbreaks%252C%2520SGNNs%2520outperformed%2520physics-constrained%2520models%2520even%2520when%2520both%2520were%2520restricted%2520to%2520incorrect%2520human-to-human%2520transmission%2520equations%252C%2520demonstrating%2520that%2520SGNNs%2520are%2520potentially%2520more%2520robust%2520to%2520model%2520misspecification.%2520For%2520inference%252C%2520SGNNs%2520extend%2520the%2520logic%2520of%2520simulation-based%2520inference%2520to%2520enable%2520supervised%2520learning%2520for%2520unobservable%2520targets%252C%2520estimating%2520early%2520COVID-19%2520transmissibility%2520more%2520accurately%2520than%2520traditional%2520methods.%2520Finally%252C%2520SGNNs%2520enable%2520back-to-simulation%2520attribution%252C%2520a%2520form%2520of%2520mechanistic%2520interpretability%2520that%2520maps%2520real-world%2520data%2520back%2520to%2520the%2520simulated%2520manifold%2520to%2520identify%2520underlying%2520processes.%2520By%2520unifying%2520these%2520disparate%2520simulation-based%2520techniques%2520into%2520a%2520single%2520framework%252C%2520we%2520demonstrate%2520that%2520mechanistic%2520simulations%2520can%2520serve%2520as%2520effective%2520training%2520data%2520for%2520robust%2520scientific%2520inference%2520that%2520generalizes%2520beyond%2520the%2520limitations%2520of%2520fixed%2520functional%2520forms.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08977v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simulation%20as%20Supervision%3A%20Mechanistic%20Pretraining%20for%20Scientific%20Discovery&entry.906535625=Carson%20Dudley%20and%20Reiden%20Magdaleno%20and%20Christopher%20Harding%20and%20Marisa%20Eisenberg&entry.1292438233=Scientific%20modeling%20faces%20a%20tradeoff%20between%20the%20interpretability%20of%20mechanistic%20theory%20and%20the%20predictive%20power%20of%20machine%20learning.%20While%20hybrid%20approaches%20like%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20embed%20domain%20knowledge%20as%20functional%20constraints%2C%20they%20can%20be%20brittle%20under%20model%20misspecification.%20We%20introduce%20Simulation-Grounded%20Neural%20Networks%20%28SGNNs%29%2C%20a%20framework%20that%20instead%20embeds%20domain%20knowledge%20into%20the%20training%20data%20to%20establish%20a%20structural%20prior.%0A%20%20By%20pretraining%20on%20synthetic%20corpora%20spanning%20diverse%20model%20structures%20and%20observational%20artifacts%2C%20SGNNs%20learn%20the%20broad%20patterns%20of%20physical%20possibility.%20This%20allows%20the%20model%20to%20internalize%20the%20underlying%20dynamics%20of%20a%20system%20without%20being%20forced%20to%20satisfy%20a%20single%2C%20potentially%20incorrect%20equation.%0A%20%20We%20evaluated%20SGNNs%20across%20scientific%20disciplines%20and%20found%20that%20this%20approach%20confers%20significant%20robustness.%20In%20prediction%20tasks%2C%20SGNNs%20nearly%20tripled%20COVID-19%20forecasting%20skill%20versus%20CDC%20baselines.%20In%20tests%20on%20dengue%20outbreaks%2C%20SGNNs%20outperformed%20physics-constrained%20models%20even%20when%20both%20were%20restricted%20to%20incorrect%20human-to-human%20transmission%20equations%2C%20demonstrating%20that%20SGNNs%20are%20potentially%20more%20robust%20to%20model%20misspecification.%20For%20inference%2C%20SGNNs%20extend%20the%20logic%20of%20simulation-based%20inference%20to%20enable%20supervised%20learning%20for%20unobservable%20targets%2C%20estimating%20early%20COVID-19%20transmissibility%20more%20accurately%20than%20traditional%20methods.%20Finally%2C%20SGNNs%20enable%20back-to-simulation%20attribution%2C%20a%20form%20of%20mechanistic%20interpretability%20that%20maps%20real-world%20data%20back%20to%20the%20simulated%20manifold%20to%20identify%20underlying%20processes.%20By%20unifying%20these%20disparate%20simulation-based%20techniques%20into%20a%20single%20framework%2C%20we%20demonstrate%20that%20mechanistic%20simulations%20can%20serve%20as%20effective%20training%20data%20for%20robust%20scientific%20inference%20that%20generalizes%20beyond%20the%20limitations%20of%20fixed%20functional%20forms.&entry.1838667208=http%3A//arxiv.org/abs/2507.08977v3&entry.124074799=Read"},
{"title": "CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models", "author": "Neeraj Anand and Samyak Jha and Udbhav Bamba and Rahul Rahaman", "abstract": "Despite the rapid success of Large Vision-Language Models (LVLMs), a persistent challenge is their tendency to generate hallucinated content, undermining reliability in real-world use. Existing training-free methods address hallucinations but face two limitations: (i) they rely on narrow assumptions about hallucination sources, and (ii) their effectiveness declines toward the end of generation, where hallucinations are most likely to occur. A common strategy is to build hallucinated models by completely or partially removing visual tokens and contrasting them with the original model. Yet, this alone proves insufficient, since visual information still propagates into generated text. Building on this insight, we propose a novel hallucinated model that captures hallucination effects by selectively removing key text tokens. We further introduce Generalized Contrastive Decoding, which integrates multiple hallucinated models to represent diverse hallucination sources. Together, these ideas form CRoPS, a training-free hallucination mitigation framework that improves CHAIR scores by 20% and achieves consistent gains across six benchmarks and three LVLM families, outperforming state-of-the-art training-free methods.", "link": "http://arxiv.org/abs/2601.00659v1", "date": "2026-01-02", "relevancy": 2.1053, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5285}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5285}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CRoPS%3A%20A%20Training-Free%20Hallucination%20Mitigation%20Framework%20for%20Vision-Language%20Models&body=Title%3A%20CRoPS%3A%20A%20Training-Free%20Hallucination%20Mitigation%20Framework%20for%20Vision-Language%20Models%0AAuthor%3A%20Neeraj%20Anand%20and%20Samyak%20Jha%20and%20Udbhav%20Bamba%20and%20Rahul%20Rahaman%0AAbstract%3A%20Despite%20the%20rapid%20success%20of%20Large%20Vision-Language%20Models%20%28LVLMs%29%2C%20a%20persistent%20challenge%20is%20their%20tendency%20to%20generate%20hallucinated%20content%2C%20undermining%20reliability%20in%20real-world%20use.%20Existing%20training-free%20methods%20address%20hallucinations%20but%20face%20two%20limitations%3A%20%28i%29%20they%20rely%20on%20narrow%20assumptions%20about%20hallucination%20sources%2C%20and%20%28ii%29%20their%20effectiveness%20declines%20toward%20the%20end%20of%20generation%2C%20where%20hallucinations%20are%20most%20likely%20to%20occur.%20A%20common%20strategy%20is%20to%20build%20hallucinated%20models%20by%20completely%20or%20partially%20removing%20visual%20tokens%20and%20contrasting%20them%20with%20the%20original%20model.%20Yet%2C%20this%20alone%20proves%20insufficient%2C%20since%20visual%20information%20still%20propagates%20into%20generated%20text.%20Building%20on%20this%20insight%2C%20we%20propose%20a%20novel%20hallucinated%20model%20that%20captures%20hallucination%20effects%20by%20selectively%20removing%20key%20text%20tokens.%20We%20further%20introduce%20Generalized%20Contrastive%20Decoding%2C%20which%20integrates%20multiple%20hallucinated%20models%20to%20represent%20diverse%20hallucination%20sources.%20Together%2C%20these%20ideas%20form%20CRoPS%2C%20a%20training-free%20hallucination%20mitigation%20framework%20that%20improves%20CHAIR%20scores%20by%2020%25%20and%20achieves%20consistent%20gains%20across%20six%20benchmarks%20and%20three%20LVLM%20families%2C%20outperforming%20state-of-the-art%20training-free%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00659v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCRoPS%253A%2520A%2520Training-Free%2520Hallucination%2520Mitigation%2520Framework%2520for%2520Vision-Language%2520Models%26entry.906535625%3DNeeraj%2520Anand%2520and%2520Samyak%2520Jha%2520and%2520Udbhav%2520Bamba%2520and%2520Rahul%2520Rahaman%26entry.1292438233%3DDespite%2520the%2520rapid%2520success%2520of%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%252C%2520a%2520persistent%2520challenge%2520is%2520their%2520tendency%2520to%2520generate%2520hallucinated%2520content%252C%2520undermining%2520reliability%2520in%2520real-world%2520use.%2520Existing%2520training-free%2520methods%2520address%2520hallucinations%2520but%2520face%2520two%2520limitations%253A%2520%2528i%2529%2520they%2520rely%2520on%2520narrow%2520assumptions%2520about%2520hallucination%2520sources%252C%2520and%2520%2528ii%2529%2520their%2520effectiveness%2520declines%2520toward%2520the%2520end%2520of%2520generation%252C%2520where%2520hallucinations%2520are%2520most%2520likely%2520to%2520occur.%2520A%2520common%2520strategy%2520is%2520to%2520build%2520hallucinated%2520models%2520by%2520completely%2520or%2520partially%2520removing%2520visual%2520tokens%2520and%2520contrasting%2520them%2520with%2520the%2520original%2520model.%2520Yet%252C%2520this%2520alone%2520proves%2520insufficient%252C%2520since%2520visual%2520information%2520still%2520propagates%2520into%2520generated%2520text.%2520Building%2520on%2520this%2520insight%252C%2520we%2520propose%2520a%2520novel%2520hallucinated%2520model%2520that%2520captures%2520hallucination%2520effects%2520by%2520selectively%2520removing%2520key%2520text%2520tokens.%2520We%2520further%2520introduce%2520Generalized%2520Contrastive%2520Decoding%252C%2520which%2520integrates%2520multiple%2520hallucinated%2520models%2520to%2520represent%2520diverse%2520hallucination%2520sources.%2520Together%252C%2520these%2520ideas%2520form%2520CRoPS%252C%2520a%2520training-free%2520hallucination%2520mitigation%2520framework%2520that%2520improves%2520CHAIR%2520scores%2520by%252020%2525%2520and%2520achieves%2520consistent%2520gains%2520across%2520six%2520benchmarks%2520and%2520three%2520LVLM%2520families%252C%2520outperforming%2520state-of-the-art%2520training-free%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00659v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CRoPS%3A%20A%20Training-Free%20Hallucination%20Mitigation%20Framework%20for%20Vision-Language%20Models&entry.906535625=Neeraj%20Anand%20and%20Samyak%20Jha%20and%20Udbhav%20Bamba%20and%20Rahul%20Rahaman&entry.1292438233=Despite%20the%20rapid%20success%20of%20Large%20Vision-Language%20Models%20%28LVLMs%29%2C%20a%20persistent%20challenge%20is%20their%20tendency%20to%20generate%20hallucinated%20content%2C%20undermining%20reliability%20in%20real-world%20use.%20Existing%20training-free%20methods%20address%20hallucinations%20but%20face%20two%20limitations%3A%20%28i%29%20they%20rely%20on%20narrow%20assumptions%20about%20hallucination%20sources%2C%20and%20%28ii%29%20their%20effectiveness%20declines%20toward%20the%20end%20of%20generation%2C%20where%20hallucinations%20are%20most%20likely%20to%20occur.%20A%20common%20strategy%20is%20to%20build%20hallucinated%20models%20by%20completely%20or%20partially%20removing%20visual%20tokens%20and%20contrasting%20them%20with%20the%20original%20model.%20Yet%2C%20this%20alone%20proves%20insufficient%2C%20since%20visual%20information%20still%20propagates%20into%20generated%20text.%20Building%20on%20this%20insight%2C%20we%20propose%20a%20novel%20hallucinated%20model%20that%20captures%20hallucination%20effects%20by%20selectively%20removing%20key%20text%20tokens.%20We%20further%20introduce%20Generalized%20Contrastive%20Decoding%2C%20which%20integrates%20multiple%20hallucinated%20models%20to%20represent%20diverse%20hallucination%20sources.%20Together%2C%20these%20ideas%20form%20CRoPS%2C%20a%20training-free%20hallucination%20mitigation%20framework%20that%20improves%20CHAIR%20scores%20by%2020%25%20and%20achieves%20consistent%20gains%20across%20six%20benchmarks%20and%20three%20LVLM%20families%2C%20outperforming%20state-of-the-art%20training-free%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2601.00659v1&entry.124074799=Read"},
{"title": "Evaluating the Performance of Open-Vocabulary Object Detection in Low-quality Image", "author": "Po-Chih Wu", "abstract": "Open-vocabulary object detection enables models to localize and recognize objects beyond a predefined set of categories and is expected to achieve recognition capabilities comparable to human performance. In this study, we aim to evaluate the performance of existing models on open-vocabulary object detection tasks under low-quality image conditions. For this purpose, we introduce a new dataset that simulates low-quality images in the real world. In our evaluation experiment, we find that although open-vocabulary object detection models exhibited no significant decrease in mAP scores under low-level image degradation, the performance of all models dropped sharply under high-level image degradation. OWLv2 models consistently performed better across different types of degradation, while OWL-ViT, GroundingDINO, and Detic showed significant performance declines. We will release our dataset and codes to facilitate future studies.", "link": "http://arxiv.org/abs/2512.22801v2", "date": "2026-01-02", "relevancy": 2.1036, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5298}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5298}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20the%20Performance%20of%20Open-Vocabulary%20Object%20Detection%20in%20Low-quality%20Image&body=Title%3A%20Evaluating%20the%20Performance%20of%20Open-Vocabulary%20Object%20Detection%20in%20Low-quality%20Image%0AAuthor%3A%20Po-Chih%20Wu%0AAbstract%3A%20Open-vocabulary%20object%20detection%20enables%20models%20to%20localize%20and%20recognize%20objects%20beyond%20a%20predefined%20set%20of%20categories%20and%20is%20expected%20to%20achieve%20recognition%20capabilities%20comparable%20to%20human%20performance.%20In%20this%20study%2C%20we%20aim%20to%20evaluate%20the%20performance%20of%20existing%20models%20on%20open-vocabulary%20object%20detection%20tasks%20under%20low-quality%20image%20conditions.%20For%20this%20purpose%2C%20we%20introduce%20a%20new%20dataset%20that%20simulates%20low-quality%20images%20in%20the%20real%20world.%20In%20our%20evaluation%20experiment%2C%20we%20find%20that%20although%20open-vocabulary%20object%20detection%20models%20exhibited%20no%20significant%20decrease%20in%20mAP%20scores%20under%20low-level%20image%20degradation%2C%20the%20performance%20of%20all%20models%20dropped%20sharply%20under%20high-level%20image%20degradation.%20OWLv2%20models%20consistently%20performed%20better%20across%20different%20types%20of%20degradation%2C%20while%20OWL-ViT%2C%20GroundingDINO%2C%20and%20Detic%20showed%20significant%20performance%20declines.%20We%20will%20release%20our%20dataset%20and%20codes%20to%20facilitate%20future%20studies.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22801v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520the%2520Performance%2520of%2520Open-Vocabulary%2520Object%2520Detection%2520in%2520Low-quality%2520Image%26entry.906535625%3DPo-Chih%2520Wu%26entry.1292438233%3DOpen-vocabulary%2520object%2520detection%2520enables%2520models%2520to%2520localize%2520and%2520recognize%2520objects%2520beyond%2520a%2520predefined%2520set%2520of%2520categories%2520and%2520is%2520expected%2520to%2520achieve%2520recognition%2520capabilities%2520comparable%2520to%2520human%2520performance.%2520In%2520this%2520study%252C%2520we%2520aim%2520to%2520evaluate%2520the%2520performance%2520of%2520existing%2520models%2520on%2520open-vocabulary%2520object%2520detection%2520tasks%2520under%2520low-quality%2520image%2520conditions.%2520For%2520this%2520purpose%252C%2520we%2520introduce%2520a%2520new%2520dataset%2520that%2520simulates%2520low-quality%2520images%2520in%2520the%2520real%2520world.%2520In%2520our%2520evaluation%2520experiment%252C%2520we%2520find%2520that%2520although%2520open-vocabulary%2520object%2520detection%2520models%2520exhibited%2520no%2520significant%2520decrease%2520in%2520mAP%2520scores%2520under%2520low-level%2520image%2520degradation%252C%2520the%2520performance%2520of%2520all%2520models%2520dropped%2520sharply%2520under%2520high-level%2520image%2520degradation.%2520OWLv2%2520models%2520consistently%2520performed%2520better%2520across%2520different%2520types%2520of%2520degradation%252C%2520while%2520OWL-ViT%252C%2520GroundingDINO%252C%2520and%2520Detic%2520showed%2520significant%2520performance%2520declines.%2520We%2520will%2520release%2520our%2520dataset%2520and%2520codes%2520to%2520facilitate%2520future%2520studies.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22801v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20the%20Performance%20of%20Open-Vocabulary%20Object%20Detection%20in%20Low-quality%20Image&entry.906535625=Po-Chih%20Wu&entry.1292438233=Open-vocabulary%20object%20detection%20enables%20models%20to%20localize%20and%20recognize%20objects%20beyond%20a%20predefined%20set%20of%20categories%20and%20is%20expected%20to%20achieve%20recognition%20capabilities%20comparable%20to%20human%20performance.%20In%20this%20study%2C%20we%20aim%20to%20evaluate%20the%20performance%20of%20existing%20models%20on%20open-vocabulary%20object%20detection%20tasks%20under%20low-quality%20image%20conditions.%20For%20this%20purpose%2C%20we%20introduce%20a%20new%20dataset%20that%20simulates%20low-quality%20images%20in%20the%20real%20world.%20In%20our%20evaluation%20experiment%2C%20we%20find%20that%20although%20open-vocabulary%20object%20detection%20models%20exhibited%20no%20significant%20decrease%20in%20mAP%20scores%20under%20low-level%20image%20degradation%2C%20the%20performance%20of%20all%20models%20dropped%20sharply%20under%20high-level%20image%20degradation.%20OWLv2%20models%20consistently%20performed%20better%20across%20different%20types%20of%20degradation%2C%20while%20OWL-ViT%2C%20GroundingDINO%2C%20and%20Detic%20showed%20significant%20performance%20declines.%20We%20will%20release%20our%20dataset%20and%20codes%20to%20facilitate%20future%20studies.&entry.1838667208=http%3A//arxiv.org/abs/2512.22801v2&entry.124074799=Read"},
{"title": "Adaptive Learning Guided by Bias-Noise-Alignment Diagnostics", "author": "Akash Samanta and Sheldon Williamson", "abstract": "Learning systems deployed in nonstationary and safety-critical environments often suffer from instability, slow convergence, or brittle adaptation when learning dynamics evolve over time. While modern optimization, reinforcement learning, and meta-learning methods adapt to gradient statistics, they largely ignore the temporal structure of the error signal itself. This paper proposes a diagnostic-driven adaptive learning framework that explicitly models error evolution through a principled decomposition into bias, capturing persistent drift; noise, capturing stochastic variability; and alignment, capturing repeated directional excitation leading to overshoot. These diagnostics are computed online from lightweight statistics of loss or temporal-difference (TD) error trajectories and are independent of model architecture or task domain. We show that the proposed bias-noise-alignment decomposition provides a unifying control backbone for supervised optimization, actor-critic reinforcement learning, and learned optimizers. Within this framework, we introduce three diagnostic-driven instantiations: the Human-inspired Supervised Adaptive Optimizer (HSAO), Hybrid Error-Diagnostic Reinforcement Learning (HED-RL) for actor-critic methods, and the Meta-Learned Learning Policy (MLLP). Under standard smoothness assumptions, we establish bounded effective updates and stability properties for all cases. Representative diagnostic illustrations in actor-critic learning highlight how the proposed signals modulate adaptation in response to TD error structure. Overall, this work elevates error evolution to a first-class object in adaptive learning and provides an interpretable, lightweight foundation for reliable learning in dynamic environments.", "link": "http://arxiv.org/abs/2512.24445v2", "date": "2026-01-02", "relevancy": 2.0812, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5415}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5065}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5016}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Learning%20Guided%20by%20Bias-Noise-Alignment%20Diagnostics&body=Title%3A%20Adaptive%20Learning%20Guided%20by%20Bias-Noise-Alignment%20Diagnostics%0AAuthor%3A%20Akash%20Samanta%20and%20Sheldon%20Williamson%0AAbstract%3A%20Learning%20systems%20deployed%20in%20nonstationary%20and%20safety-critical%20environments%20often%20suffer%20from%20instability%2C%20slow%20convergence%2C%20or%20brittle%20adaptation%20when%20learning%20dynamics%20evolve%20over%20time.%20While%20modern%20optimization%2C%20reinforcement%20learning%2C%20and%20meta-learning%20methods%20adapt%20to%20gradient%20statistics%2C%20they%20largely%20ignore%20the%20temporal%20structure%20of%20the%20error%20signal%20itself.%20This%20paper%20proposes%20a%20diagnostic-driven%20adaptive%20learning%20framework%20that%20explicitly%20models%20error%20evolution%20through%20a%20principled%20decomposition%20into%20bias%2C%20capturing%20persistent%20drift%3B%20noise%2C%20capturing%20stochastic%20variability%3B%20and%20alignment%2C%20capturing%20repeated%20directional%20excitation%20leading%20to%20overshoot.%20These%20diagnostics%20are%20computed%20online%20from%20lightweight%20statistics%20of%20loss%20or%20temporal-difference%20%28TD%29%20error%20trajectories%20and%20are%20independent%20of%20model%20architecture%20or%20task%20domain.%20We%20show%20that%20the%20proposed%20bias-noise-alignment%20decomposition%20provides%20a%20unifying%20control%20backbone%20for%20supervised%20optimization%2C%20actor-critic%20reinforcement%20learning%2C%20and%20learned%20optimizers.%20Within%20this%20framework%2C%20we%20introduce%20three%20diagnostic-driven%20instantiations%3A%20the%20Human-inspired%20Supervised%20Adaptive%20Optimizer%20%28HSAO%29%2C%20Hybrid%20Error-Diagnostic%20Reinforcement%20Learning%20%28HED-RL%29%20for%20actor-critic%20methods%2C%20and%20the%20Meta-Learned%20Learning%20Policy%20%28MLLP%29.%20Under%20standard%20smoothness%20assumptions%2C%20we%20establish%20bounded%20effective%20updates%20and%20stability%20properties%20for%20all%20cases.%20Representative%20diagnostic%20illustrations%20in%20actor-critic%20learning%20highlight%20how%20the%20proposed%20signals%20modulate%20adaptation%20in%20response%20to%20TD%20error%20structure.%20Overall%2C%20this%20work%20elevates%20error%20evolution%20to%20a%20first-class%20object%20in%20adaptive%20learning%20and%20provides%20an%20interpretable%2C%20lightweight%20foundation%20for%20reliable%20learning%20in%20dynamic%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2512.24445v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Learning%2520Guided%2520by%2520Bias-Noise-Alignment%2520Diagnostics%26entry.906535625%3DAkash%2520Samanta%2520and%2520Sheldon%2520Williamson%26entry.1292438233%3DLearning%2520systems%2520deployed%2520in%2520nonstationary%2520and%2520safety-critical%2520environments%2520often%2520suffer%2520from%2520instability%252C%2520slow%2520convergence%252C%2520or%2520brittle%2520adaptation%2520when%2520learning%2520dynamics%2520evolve%2520over%2520time.%2520While%2520modern%2520optimization%252C%2520reinforcement%2520learning%252C%2520and%2520meta-learning%2520methods%2520adapt%2520to%2520gradient%2520statistics%252C%2520they%2520largely%2520ignore%2520the%2520temporal%2520structure%2520of%2520the%2520error%2520signal%2520itself.%2520This%2520paper%2520proposes%2520a%2520diagnostic-driven%2520adaptive%2520learning%2520framework%2520that%2520explicitly%2520models%2520error%2520evolution%2520through%2520a%2520principled%2520decomposition%2520into%2520bias%252C%2520capturing%2520persistent%2520drift%253B%2520noise%252C%2520capturing%2520stochastic%2520variability%253B%2520and%2520alignment%252C%2520capturing%2520repeated%2520directional%2520excitation%2520leading%2520to%2520overshoot.%2520These%2520diagnostics%2520are%2520computed%2520online%2520from%2520lightweight%2520statistics%2520of%2520loss%2520or%2520temporal-difference%2520%2528TD%2529%2520error%2520trajectories%2520and%2520are%2520independent%2520of%2520model%2520architecture%2520or%2520task%2520domain.%2520We%2520show%2520that%2520the%2520proposed%2520bias-noise-alignment%2520decomposition%2520provides%2520a%2520unifying%2520control%2520backbone%2520for%2520supervised%2520optimization%252C%2520actor-critic%2520reinforcement%2520learning%252C%2520and%2520learned%2520optimizers.%2520Within%2520this%2520framework%252C%2520we%2520introduce%2520three%2520diagnostic-driven%2520instantiations%253A%2520the%2520Human-inspired%2520Supervised%2520Adaptive%2520Optimizer%2520%2528HSAO%2529%252C%2520Hybrid%2520Error-Diagnostic%2520Reinforcement%2520Learning%2520%2528HED-RL%2529%2520for%2520actor-critic%2520methods%252C%2520and%2520the%2520Meta-Learned%2520Learning%2520Policy%2520%2528MLLP%2529.%2520Under%2520standard%2520smoothness%2520assumptions%252C%2520we%2520establish%2520bounded%2520effective%2520updates%2520and%2520stability%2520properties%2520for%2520all%2520cases.%2520Representative%2520diagnostic%2520illustrations%2520in%2520actor-critic%2520learning%2520highlight%2520how%2520the%2520proposed%2520signals%2520modulate%2520adaptation%2520in%2520response%2520to%2520TD%2520error%2520structure.%2520Overall%252C%2520this%2520work%2520elevates%2520error%2520evolution%2520to%2520a%2520first-class%2520object%2520in%2520adaptive%2520learning%2520and%2520provides%2520an%2520interpretable%252C%2520lightweight%2520foundation%2520for%2520reliable%2520learning%2520in%2520dynamic%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.24445v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Learning%20Guided%20by%20Bias-Noise-Alignment%20Diagnostics&entry.906535625=Akash%20Samanta%20and%20Sheldon%20Williamson&entry.1292438233=Learning%20systems%20deployed%20in%20nonstationary%20and%20safety-critical%20environments%20often%20suffer%20from%20instability%2C%20slow%20convergence%2C%20or%20brittle%20adaptation%20when%20learning%20dynamics%20evolve%20over%20time.%20While%20modern%20optimization%2C%20reinforcement%20learning%2C%20and%20meta-learning%20methods%20adapt%20to%20gradient%20statistics%2C%20they%20largely%20ignore%20the%20temporal%20structure%20of%20the%20error%20signal%20itself.%20This%20paper%20proposes%20a%20diagnostic-driven%20adaptive%20learning%20framework%20that%20explicitly%20models%20error%20evolution%20through%20a%20principled%20decomposition%20into%20bias%2C%20capturing%20persistent%20drift%3B%20noise%2C%20capturing%20stochastic%20variability%3B%20and%20alignment%2C%20capturing%20repeated%20directional%20excitation%20leading%20to%20overshoot.%20These%20diagnostics%20are%20computed%20online%20from%20lightweight%20statistics%20of%20loss%20or%20temporal-difference%20%28TD%29%20error%20trajectories%20and%20are%20independent%20of%20model%20architecture%20or%20task%20domain.%20We%20show%20that%20the%20proposed%20bias-noise-alignment%20decomposition%20provides%20a%20unifying%20control%20backbone%20for%20supervised%20optimization%2C%20actor-critic%20reinforcement%20learning%2C%20and%20learned%20optimizers.%20Within%20this%20framework%2C%20we%20introduce%20three%20diagnostic-driven%20instantiations%3A%20the%20Human-inspired%20Supervised%20Adaptive%20Optimizer%20%28HSAO%29%2C%20Hybrid%20Error-Diagnostic%20Reinforcement%20Learning%20%28HED-RL%29%20for%20actor-critic%20methods%2C%20and%20the%20Meta-Learned%20Learning%20Policy%20%28MLLP%29.%20Under%20standard%20smoothness%20assumptions%2C%20we%20establish%20bounded%20effective%20updates%20and%20stability%20properties%20for%20all%20cases.%20Representative%20diagnostic%20illustrations%20in%20actor-critic%20learning%20highlight%20how%20the%20proposed%20signals%20modulate%20adaptation%20in%20response%20to%20TD%20error%20structure.%20Overall%2C%20this%20work%20elevates%20error%20evolution%20to%20a%20first-class%20object%20in%20adaptive%20learning%20and%20provides%20an%20interpretable%2C%20lightweight%20foundation%20for%20reliable%20learning%20in%20dynamic%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2512.24445v2&entry.124074799=Read"},
{"title": "Iterative Tuning of Nonlinear Model Predictive Control for Robotic Manufacturing Tasks", "author": "Deepak Ingole and Valentin Bhend and Shiva Ganesh Murali and Oliver Dobrich and Alisa Rupenyan", "abstract": "Manufacturing processes are often perturbed by drifts in the environment and wear in the system, requiring control re-tuning even in the presence of repetitive operations. This paper presents an iterative learning framework for automatic tuning of Nonlinear Model Predictive Control (NMPC) weighting matrices based on task-level performance feedback. Inspired by norm-optimal Iterative Learning Control (ILC), the proposed method adaptively adjusts NMPC weights Q and R across task repetitions to minimize key performance indicators (KPIs) related to tracking accuracy, control effort, and saturation. Unlike gradient-based approaches that require differentiating through the NMPC solver, we construct an empirical sensitivity matrix, enabling structured weight updates without analytic derivatives. The framework is validated through simulation on a UR10e robot performing carbon fiber winding on a tetrahedral core. Results demonstrate that the proposed approach converges to near-optimal tracking performance (RMSE within 0.3% of offline Bayesian Optimization (BO)) in just 4 online repetitions, compared to 100 offline evaluations required by BO algorithm. The method offers a practical solution for adaptive NMPC tuning in repetitive robotic tasks, combining the precision of carefully optimized controllers with the flexibility of online adaptation.", "link": "http://arxiv.org/abs/2512.13170v2", "date": "2026-01-02", "relevancy": 2.0544, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5259}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5113}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Iterative%20Tuning%20of%20Nonlinear%20Model%20Predictive%20Control%20for%20Robotic%20Manufacturing%20Tasks&body=Title%3A%20Iterative%20Tuning%20of%20Nonlinear%20Model%20Predictive%20Control%20for%20Robotic%20Manufacturing%20Tasks%0AAuthor%3A%20Deepak%20Ingole%20and%20Valentin%20Bhend%20and%20Shiva%20Ganesh%20Murali%20and%20Oliver%20Dobrich%20and%20Alisa%20Rupenyan%0AAbstract%3A%20Manufacturing%20processes%20are%20often%20perturbed%20by%20drifts%20in%20the%20environment%20and%20wear%20in%20the%20system%2C%20requiring%20control%20re-tuning%20even%20in%20the%20presence%20of%20repetitive%20operations.%20This%20paper%20presents%20an%20iterative%20learning%20framework%20for%20automatic%20tuning%20of%20Nonlinear%20Model%20Predictive%20Control%20%28NMPC%29%20weighting%20matrices%20based%20on%20task-level%20performance%20feedback.%20Inspired%20by%20norm-optimal%20Iterative%20Learning%20Control%20%28ILC%29%2C%20the%20proposed%20method%20adaptively%20adjusts%20NMPC%20weights%20Q%20and%20R%20across%20task%20repetitions%20to%20minimize%20key%20performance%20indicators%20%28KPIs%29%20related%20to%20tracking%20accuracy%2C%20control%20effort%2C%20and%20saturation.%20Unlike%20gradient-based%20approaches%20that%20require%20differentiating%20through%20the%20NMPC%20solver%2C%20we%20construct%20an%20empirical%20sensitivity%20matrix%2C%20enabling%20structured%20weight%20updates%20without%20analytic%20derivatives.%20The%20framework%20is%20validated%20through%20simulation%20on%20a%20UR10e%20robot%20performing%20carbon%20fiber%20winding%20on%20a%20tetrahedral%20core.%20Results%20demonstrate%20that%20the%20proposed%20approach%20converges%20to%20near-optimal%20tracking%20performance%20%28RMSE%20within%200.3%25%20of%20offline%20Bayesian%20Optimization%20%28BO%29%29%20in%20just%204%20online%20repetitions%2C%20compared%20to%20100%20offline%20evaluations%20required%20by%20BO%20algorithm.%20The%20method%20offers%20a%20practical%20solution%20for%20adaptive%20NMPC%20tuning%20in%20repetitive%20robotic%20tasks%2C%20combining%20the%20precision%20of%20carefully%20optimized%20controllers%20with%20the%20flexibility%20of%20online%20adaptation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13170v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIterative%2520Tuning%2520of%2520Nonlinear%2520Model%2520Predictive%2520Control%2520for%2520Robotic%2520Manufacturing%2520Tasks%26entry.906535625%3DDeepak%2520Ingole%2520and%2520Valentin%2520Bhend%2520and%2520Shiva%2520Ganesh%2520Murali%2520and%2520Oliver%2520Dobrich%2520and%2520Alisa%2520Rupenyan%26entry.1292438233%3DManufacturing%2520processes%2520are%2520often%2520perturbed%2520by%2520drifts%2520in%2520the%2520environment%2520and%2520wear%2520in%2520the%2520system%252C%2520requiring%2520control%2520re-tuning%2520even%2520in%2520the%2520presence%2520of%2520repetitive%2520operations.%2520This%2520paper%2520presents%2520an%2520iterative%2520learning%2520framework%2520for%2520automatic%2520tuning%2520of%2520Nonlinear%2520Model%2520Predictive%2520Control%2520%2528NMPC%2529%2520weighting%2520matrices%2520based%2520on%2520task-level%2520performance%2520feedback.%2520Inspired%2520by%2520norm-optimal%2520Iterative%2520Learning%2520Control%2520%2528ILC%2529%252C%2520the%2520proposed%2520method%2520adaptively%2520adjusts%2520NMPC%2520weights%2520Q%2520and%2520R%2520across%2520task%2520repetitions%2520to%2520minimize%2520key%2520performance%2520indicators%2520%2528KPIs%2529%2520related%2520to%2520tracking%2520accuracy%252C%2520control%2520effort%252C%2520and%2520saturation.%2520Unlike%2520gradient-based%2520approaches%2520that%2520require%2520differentiating%2520through%2520the%2520NMPC%2520solver%252C%2520we%2520construct%2520an%2520empirical%2520sensitivity%2520matrix%252C%2520enabling%2520structured%2520weight%2520updates%2520without%2520analytic%2520derivatives.%2520The%2520framework%2520is%2520validated%2520through%2520simulation%2520on%2520a%2520UR10e%2520robot%2520performing%2520carbon%2520fiber%2520winding%2520on%2520a%2520tetrahedral%2520core.%2520Results%2520demonstrate%2520that%2520the%2520proposed%2520approach%2520converges%2520to%2520near-optimal%2520tracking%2520performance%2520%2528RMSE%2520within%25200.3%2525%2520of%2520offline%2520Bayesian%2520Optimization%2520%2528BO%2529%2529%2520in%2520just%25204%2520online%2520repetitions%252C%2520compared%2520to%2520100%2520offline%2520evaluations%2520required%2520by%2520BO%2520algorithm.%2520The%2520method%2520offers%2520a%2520practical%2520solution%2520for%2520adaptive%2520NMPC%2520tuning%2520in%2520repetitive%2520robotic%2520tasks%252C%2520combining%2520the%2520precision%2520of%2520carefully%2520optimized%2520controllers%2520with%2520the%2520flexibility%2520of%2520online%2520adaptation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13170v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Iterative%20Tuning%20of%20Nonlinear%20Model%20Predictive%20Control%20for%20Robotic%20Manufacturing%20Tasks&entry.906535625=Deepak%20Ingole%20and%20Valentin%20Bhend%20and%20Shiva%20Ganesh%20Murali%20and%20Oliver%20Dobrich%20and%20Alisa%20Rupenyan&entry.1292438233=Manufacturing%20processes%20are%20often%20perturbed%20by%20drifts%20in%20the%20environment%20and%20wear%20in%20the%20system%2C%20requiring%20control%20re-tuning%20even%20in%20the%20presence%20of%20repetitive%20operations.%20This%20paper%20presents%20an%20iterative%20learning%20framework%20for%20automatic%20tuning%20of%20Nonlinear%20Model%20Predictive%20Control%20%28NMPC%29%20weighting%20matrices%20based%20on%20task-level%20performance%20feedback.%20Inspired%20by%20norm-optimal%20Iterative%20Learning%20Control%20%28ILC%29%2C%20the%20proposed%20method%20adaptively%20adjusts%20NMPC%20weights%20Q%20and%20R%20across%20task%20repetitions%20to%20minimize%20key%20performance%20indicators%20%28KPIs%29%20related%20to%20tracking%20accuracy%2C%20control%20effort%2C%20and%20saturation.%20Unlike%20gradient-based%20approaches%20that%20require%20differentiating%20through%20the%20NMPC%20solver%2C%20we%20construct%20an%20empirical%20sensitivity%20matrix%2C%20enabling%20structured%20weight%20updates%20without%20analytic%20derivatives.%20The%20framework%20is%20validated%20through%20simulation%20on%20a%20UR10e%20robot%20performing%20carbon%20fiber%20winding%20on%20a%20tetrahedral%20core.%20Results%20demonstrate%20that%20the%20proposed%20approach%20converges%20to%20near-optimal%20tracking%20performance%20%28RMSE%20within%200.3%25%20of%20offline%20Bayesian%20Optimization%20%28BO%29%29%20in%20just%204%20online%20repetitions%2C%20compared%20to%20100%20offline%20evaluations%20required%20by%20BO%20algorithm.%20The%20method%20offers%20a%20practical%20solution%20for%20adaptive%20NMPC%20tuning%20in%20repetitive%20robotic%20tasks%2C%20combining%20the%20precision%20of%20carefully%20optimized%20controllers%20with%20the%20flexibility%20of%20online%20adaptation.&entry.1838667208=http%3A//arxiv.org/abs/2512.13170v2&entry.124074799=Read"},
{"title": "Investigating the Viability of Employing Multi-modal Large Language Models in the Context of Audio Deepfake Detection", "author": "Akanksha Chuchra and Shukesh Reddy and Sudeepta Mishra and Abhijit Das and Abhinav Dhall", "abstract": "While Vision-Language Models (VLMs) and Multimodal Large Language Models (MLLMs) have shown strong generalisation in detecting image and video deepfakes, their use for audio deepfake detection remains largely unexplored. In this work, we aim to explore the potential of MLLMs for audio deepfake detection. Combining audio inputs with a range of text prompts as queries to find out the viability of MLLMs to learn robust representations across modalities for audio deepfake detection. Therefore, we attempt to explore text-aware and context-rich, question-answer based prompts with binary decisions. We hypothesise that such a feature-guided reasoning will help in facilitating deeper multimodal understanding and enable robust feature learning for audio deepfake detection. We evaluate the performance of two MLLMs, Qwen2-Audio-7B-Instruct and SALMONN, in two evaluation modes: (a) zero-shot and (b) fine-tuned. Our experiments demonstrate that combining audio with a multi-prompt approach could be a viable way forward for audio deepfake detection. Our experiments show that the models perform poorly without task-specific training and struggle to generalise to out-of-domain data. However, they achieve good performance on in-domain data with minimal supervision, indicating promising potential for audio deepfake detection.", "link": "http://arxiv.org/abs/2601.00777v1", "date": "2026-01-02", "relevancy": 2.0535, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5207}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5119}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20the%20Viability%20of%20Employing%20Multi-modal%20Large%20Language%20Models%20in%20the%20Context%20of%20Audio%20Deepfake%20Detection&body=Title%3A%20Investigating%20the%20Viability%20of%20Employing%20Multi-modal%20Large%20Language%20Models%20in%20the%20Context%20of%20Audio%20Deepfake%20Detection%0AAuthor%3A%20Akanksha%20Chuchra%20and%20Shukesh%20Reddy%20and%20Sudeepta%20Mishra%20and%20Abhijit%20Das%20and%20Abhinav%20Dhall%0AAbstract%3A%20While%20Vision-Language%20Models%20%28VLMs%29%20and%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20shown%20strong%20generalisation%20in%20detecting%20image%20and%20video%20deepfakes%2C%20their%20use%20for%20audio%20deepfake%20detection%20remains%20largely%20unexplored.%20In%20this%20work%2C%20we%20aim%20to%20explore%20the%20potential%20of%20MLLMs%20for%20audio%20deepfake%20detection.%20Combining%20audio%20inputs%20with%20a%20range%20of%20text%20prompts%20as%20queries%20to%20find%20out%20the%20viability%20of%20MLLMs%20to%20learn%20robust%20representations%20across%20modalities%20for%20audio%20deepfake%20detection.%20Therefore%2C%20we%20attempt%20to%20explore%20text-aware%20and%20context-rich%2C%20question-answer%20based%20prompts%20with%20binary%20decisions.%20We%20hypothesise%20that%20such%20a%20feature-guided%20reasoning%20will%20help%20in%20facilitating%20deeper%20multimodal%20understanding%20and%20enable%20robust%20feature%20learning%20for%20audio%20deepfake%20detection.%20We%20evaluate%20the%20performance%20of%20two%20MLLMs%2C%20Qwen2-Audio-7B-Instruct%20and%20SALMONN%2C%20in%20two%20evaluation%20modes%3A%20%28a%29%20zero-shot%20and%20%28b%29%20fine-tuned.%20Our%20experiments%20demonstrate%20that%20combining%20audio%20with%20a%20multi-prompt%20approach%20could%20be%20a%20viable%20way%20forward%20for%20audio%20deepfake%20detection.%20Our%20experiments%20show%20that%20the%20models%20perform%20poorly%20without%20task-specific%20training%20and%20struggle%20to%20generalise%20to%20out-of-domain%20data.%20However%2C%20they%20achieve%20good%20performance%20on%20in-domain%20data%20with%20minimal%20supervision%2C%20indicating%20promising%20potential%20for%20audio%20deepfake%20detection.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520the%2520Viability%2520of%2520Employing%2520Multi-modal%2520Large%2520Language%2520Models%2520in%2520the%2520Context%2520of%2520Audio%2520Deepfake%2520Detection%26entry.906535625%3DAkanksha%2520Chuchra%2520and%2520Shukesh%2520Reddy%2520and%2520Sudeepta%2520Mishra%2520and%2520Abhijit%2520Das%2520and%2520Abhinav%2520Dhall%26entry.1292438233%3DWhile%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520and%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520shown%2520strong%2520generalisation%2520in%2520detecting%2520image%2520and%2520video%2520deepfakes%252C%2520their%2520use%2520for%2520audio%2520deepfake%2520detection%2520remains%2520largely%2520unexplored.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520explore%2520the%2520potential%2520of%2520MLLMs%2520for%2520audio%2520deepfake%2520detection.%2520Combining%2520audio%2520inputs%2520with%2520a%2520range%2520of%2520text%2520prompts%2520as%2520queries%2520to%2520find%2520out%2520the%2520viability%2520of%2520MLLMs%2520to%2520learn%2520robust%2520representations%2520across%2520modalities%2520for%2520audio%2520deepfake%2520detection.%2520Therefore%252C%2520we%2520attempt%2520to%2520explore%2520text-aware%2520and%2520context-rich%252C%2520question-answer%2520based%2520prompts%2520with%2520binary%2520decisions.%2520We%2520hypothesise%2520that%2520such%2520a%2520feature-guided%2520reasoning%2520will%2520help%2520in%2520facilitating%2520deeper%2520multimodal%2520understanding%2520and%2520enable%2520robust%2520feature%2520learning%2520for%2520audio%2520deepfake%2520detection.%2520We%2520evaluate%2520the%2520performance%2520of%2520two%2520MLLMs%252C%2520Qwen2-Audio-7B-Instruct%2520and%2520SALMONN%252C%2520in%2520two%2520evaluation%2520modes%253A%2520%2528a%2529%2520zero-shot%2520and%2520%2528b%2529%2520fine-tuned.%2520Our%2520experiments%2520demonstrate%2520that%2520combining%2520audio%2520with%2520a%2520multi-prompt%2520approach%2520could%2520be%2520a%2520viable%2520way%2520forward%2520for%2520audio%2520deepfake%2520detection.%2520Our%2520experiments%2520show%2520that%2520the%2520models%2520perform%2520poorly%2520without%2520task-specific%2520training%2520and%2520struggle%2520to%2520generalise%2520to%2520out-of-domain%2520data.%2520However%252C%2520they%2520achieve%2520good%2520performance%2520on%2520in-domain%2520data%2520with%2520minimal%2520supervision%252C%2520indicating%2520promising%2520potential%2520for%2520audio%2520deepfake%2520detection.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20the%20Viability%20of%20Employing%20Multi-modal%20Large%20Language%20Models%20in%20the%20Context%20of%20Audio%20Deepfake%20Detection&entry.906535625=Akanksha%20Chuchra%20and%20Shukesh%20Reddy%20and%20Sudeepta%20Mishra%20and%20Abhijit%20Das%20and%20Abhinav%20Dhall&entry.1292438233=While%20Vision-Language%20Models%20%28VLMs%29%20and%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20shown%20strong%20generalisation%20in%20detecting%20image%20and%20video%20deepfakes%2C%20their%20use%20for%20audio%20deepfake%20detection%20remains%20largely%20unexplored.%20In%20this%20work%2C%20we%20aim%20to%20explore%20the%20potential%20of%20MLLMs%20for%20audio%20deepfake%20detection.%20Combining%20audio%20inputs%20with%20a%20range%20of%20text%20prompts%20as%20queries%20to%20find%20out%20the%20viability%20of%20MLLMs%20to%20learn%20robust%20representations%20across%20modalities%20for%20audio%20deepfake%20detection.%20Therefore%2C%20we%20attempt%20to%20explore%20text-aware%20and%20context-rich%2C%20question-answer%20based%20prompts%20with%20binary%20decisions.%20We%20hypothesise%20that%20such%20a%20feature-guided%20reasoning%20will%20help%20in%20facilitating%20deeper%20multimodal%20understanding%20and%20enable%20robust%20feature%20learning%20for%20audio%20deepfake%20detection.%20We%20evaluate%20the%20performance%20of%20two%20MLLMs%2C%20Qwen2-Audio-7B-Instruct%20and%20SALMONN%2C%20in%20two%20evaluation%20modes%3A%20%28a%29%20zero-shot%20and%20%28b%29%20fine-tuned.%20Our%20experiments%20demonstrate%20that%20combining%20audio%20with%20a%20multi-prompt%20approach%20could%20be%20a%20viable%20way%20forward%20for%20audio%20deepfake%20detection.%20Our%20experiments%20show%20that%20the%20models%20perform%20poorly%20without%20task-specific%20training%20and%20struggle%20to%20generalise%20to%20out-of-domain%20data.%20However%2C%20they%20achieve%20good%20performance%20on%20in-domain%20data%20with%20minimal%20supervision%2C%20indicating%20promising%20potential%20for%20audio%20deepfake%20detection.&entry.1838667208=http%3A//arxiv.org/abs/2601.00777v1&entry.124074799=Read"},
{"title": "BSAT: B-Spline Adaptive Tokenizer for Long-Term Time Series Forecasting", "author": "Maximilian Reinwardt and Michael Eichelbeck and Matthias Althoff", "abstract": "Long-term time series forecasting using transformers is hampered by the quadratic complexity of self-attention and the rigidity of uniform patching, which may be misaligned with the data's semantic structure. In this paper, we introduce the \\textit{B-Spline Adaptive Tokenizer (BSAT)}, a novel, parameter-free method that adaptively segments a time series by fitting it with B-splines. BSAT algorithmically places tokens in high-curvature regions and represents each variable-length basis function as a fixed-size token, composed of its coefficient and position. Further, we propose a hybrid positional encoding that combines a additive learnable positional encoding with Rotary Positional Embedding featuring a layer-wise learnable base: L-RoPE. This allows each layer to attend to different temporal dependencies. Our experiments on several public benchmarks show that our model is competitive with strong performance at high compression rates. This makes it particularly well-suited for use cases with strong memory constraints.", "link": "http://arxiv.org/abs/2601.00698v1", "date": "2026-01-02", "relevancy": 2.0464, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5183}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5144}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BSAT%3A%20B-Spline%20Adaptive%20Tokenizer%20for%20Long-Term%20Time%20Series%20Forecasting&body=Title%3A%20BSAT%3A%20B-Spline%20Adaptive%20Tokenizer%20for%20Long-Term%20Time%20Series%20Forecasting%0AAuthor%3A%20Maximilian%20Reinwardt%20and%20Michael%20Eichelbeck%20and%20Matthias%20Althoff%0AAbstract%3A%20Long-term%20time%20series%20forecasting%20using%20transformers%20is%20hampered%20by%20the%20quadratic%20complexity%20of%20self-attention%20and%20the%20rigidity%20of%20uniform%20patching%2C%20which%20may%20be%20misaligned%20with%20the%20data%27s%20semantic%20structure.%20In%20this%20paper%2C%20we%20introduce%20the%20%5Ctextit%7BB-Spline%20Adaptive%20Tokenizer%20%28BSAT%29%7D%2C%20a%20novel%2C%20parameter-free%20method%20that%20adaptively%20segments%20a%20time%20series%20by%20fitting%20it%20with%20B-splines.%20BSAT%20algorithmically%20places%20tokens%20in%20high-curvature%20regions%20and%20represents%20each%20variable-length%20basis%20function%20as%20a%20fixed-size%20token%2C%20composed%20of%20its%20coefficient%20and%20position.%20Further%2C%20we%20propose%20a%20hybrid%20positional%20encoding%20that%20combines%20a%20additive%20learnable%20positional%20encoding%20with%20Rotary%20Positional%20Embedding%20featuring%20a%20layer-wise%20learnable%20base%3A%20L-RoPE.%20This%20allows%20each%20layer%20to%20attend%20to%20different%20temporal%20dependencies.%20Our%20experiments%20on%20several%20public%20benchmarks%20show%20that%20our%20model%20is%20competitive%20with%20strong%20performance%20at%20high%20compression%20rates.%20This%20makes%20it%20particularly%20well-suited%20for%20use%20cases%20with%20strong%20memory%20constraints.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00698v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBSAT%253A%2520B-Spline%2520Adaptive%2520Tokenizer%2520for%2520Long-Term%2520Time%2520Series%2520Forecasting%26entry.906535625%3DMaximilian%2520Reinwardt%2520and%2520Michael%2520Eichelbeck%2520and%2520Matthias%2520Althoff%26entry.1292438233%3DLong-term%2520time%2520series%2520forecasting%2520using%2520transformers%2520is%2520hampered%2520by%2520the%2520quadratic%2520complexity%2520of%2520self-attention%2520and%2520the%2520rigidity%2520of%2520uniform%2520patching%252C%2520which%2520may%2520be%2520misaligned%2520with%2520the%2520data%2527s%2520semantic%2520structure.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520%255Ctextit%257BB-Spline%2520Adaptive%2520Tokenizer%2520%2528BSAT%2529%257D%252C%2520a%2520novel%252C%2520parameter-free%2520method%2520that%2520adaptively%2520segments%2520a%2520time%2520series%2520by%2520fitting%2520it%2520with%2520B-splines.%2520BSAT%2520algorithmically%2520places%2520tokens%2520in%2520high-curvature%2520regions%2520and%2520represents%2520each%2520variable-length%2520basis%2520function%2520as%2520a%2520fixed-size%2520token%252C%2520composed%2520of%2520its%2520coefficient%2520and%2520position.%2520Further%252C%2520we%2520propose%2520a%2520hybrid%2520positional%2520encoding%2520that%2520combines%2520a%2520additive%2520learnable%2520positional%2520encoding%2520with%2520Rotary%2520Positional%2520Embedding%2520featuring%2520a%2520layer-wise%2520learnable%2520base%253A%2520L-RoPE.%2520This%2520allows%2520each%2520layer%2520to%2520attend%2520to%2520different%2520temporal%2520dependencies.%2520Our%2520experiments%2520on%2520several%2520public%2520benchmarks%2520show%2520that%2520our%2520model%2520is%2520competitive%2520with%2520strong%2520performance%2520at%2520high%2520compression%2520rates.%2520This%2520makes%2520it%2520particularly%2520well-suited%2520for%2520use%2520cases%2520with%2520strong%2520memory%2520constraints.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00698v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BSAT%3A%20B-Spline%20Adaptive%20Tokenizer%20for%20Long-Term%20Time%20Series%20Forecasting&entry.906535625=Maximilian%20Reinwardt%20and%20Michael%20Eichelbeck%20and%20Matthias%20Althoff&entry.1292438233=Long-term%20time%20series%20forecasting%20using%20transformers%20is%20hampered%20by%20the%20quadratic%20complexity%20of%20self-attention%20and%20the%20rigidity%20of%20uniform%20patching%2C%20which%20may%20be%20misaligned%20with%20the%20data%27s%20semantic%20structure.%20In%20this%20paper%2C%20we%20introduce%20the%20%5Ctextit%7BB-Spline%20Adaptive%20Tokenizer%20%28BSAT%29%7D%2C%20a%20novel%2C%20parameter-free%20method%20that%20adaptively%20segments%20a%20time%20series%20by%20fitting%20it%20with%20B-splines.%20BSAT%20algorithmically%20places%20tokens%20in%20high-curvature%20regions%20and%20represents%20each%20variable-length%20basis%20function%20as%20a%20fixed-size%20token%2C%20composed%20of%20its%20coefficient%20and%20position.%20Further%2C%20we%20propose%20a%20hybrid%20positional%20encoding%20that%20combines%20a%20additive%20learnable%20positional%20encoding%20with%20Rotary%20Positional%20Embedding%20featuring%20a%20layer-wise%20learnable%20base%3A%20L-RoPE.%20This%20allows%20each%20layer%20to%20attend%20to%20different%20temporal%20dependencies.%20Our%20experiments%20on%20several%20public%20benchmarks%20show%20that%20our%20model%20is%20competitive%20with%20strong%20performance%20at%20high%20compression%20rates.%20This%20makes%20it%20particularly%20well-suited%20for%20use%20cases%20with%20strong%20memory%20constraints.&entry.1838667208=http%3A//arxiv.org/abs/2601.00698v1&entry.124074799=Read"},
{"title": "PrivTune: Efficient and Privacy-Preserving Fine-Tuning of Large Language Models via Device-Cloud Collaboration", "author": "Yi Liu and Weixiang Han and Chengjun Cai and Xingliang Yuan and Cong Wang", "abstract": "With the rise of large language models, service providers offer language models as a service, enabling users to fine-tune customized models via uploaded private datasets. However, this raises concerns about sensitive data leakage. Prior methods, relying on differential privacy within device-cloud collaboration frameworks, struggle to balance privacy and utility, exposing users to inference attacks or degrading fine-tuning performance. To address this, we propose PrivTune, an efficient and privacy-preserving fine-tuning framework via Split Learning (SL). The key idea of PrivTune is to inject crafted noise into token representations from the SL bottom model, making each token resemble the $n$-hop indirect neighbors. PrivTune formulates this as an optimization problem to compute the optimal noise vector, aligning with defense-utility goals. On this basis, it then adjusts the parameters (i.e., mean) of the $d_\u03c7$-Privacy noise distribution to align with the optimization direction and scales the noise according to token importance to minimize distortion. Experiments on five datasets (covering both classification and generation tasks) against three embedding inversion and three attribute inference attacks show that, using RoBERTa on the Stanford Sentiment Treebank dataset, PrivTune reduces the attack success rate to 10% with only a 3.33% drop in utility performance, outperforming state-of-the-art baselines.", "link": "http://arxiv.org/abs/2512.08809v2", "date": "2026-01-02", "relevancy": 2.0417, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5223}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5187}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PrivTune%3A%20Efficient%20and%20Privacy-Preserving%20Fine-Tuning%20of%20Large%20Language%20Models%20via%20Device-Cloud%20Collaboration&body=Title%3A%20PrivTune%3A%20Efficient%20and%20Privacy-Preserving%20Fine-Tuning%20of%20Large%20Language%20Models%20via%20Device-Cloud%20Collaboration%0AAuthor%3A%20Yi%20Liu%20and%20Weixiang%20Han%20and%20Chengjun%20Cai%20and%20Xingliang%20Yuan%20and%20Cong%20Wang%0AAbstract%3A%20With%20the%20rise%20of%20large%20language%20models%2C%20service%20providers%20offer%20language%20models%20as%20a%20service%2C%20enabling%20users%20to%20fine-tune%20customized%20models%20via%20uploaded%20private%20datasets.%20However%2C%20this%20raises%20concerns%20about%20sensitive%20data%20leakage.%20Prior%20methods%2C%20relying%20on%20differential%20privacy%20within%20device-cloud%20collaboration%20frameworks%2C%20struggle%20to%20balance%20privacy%20and%20utility%2C%20exposing%20users%20to%20inference%20attacks%20or%20degrading%20fine-tuning%20performance.%20To%20address%20this%2C%20we%20propose%20PrivTune%2C%20an%20efficient%20and%20privacy-preserving%20fine-tuning%20framework%20via%20Split%20Learning%20%28SL%29.%20The%20key%20idea%20of%20PrivTune%20is%20to%20inject%20crafted%20noise%20into%20token%20representations%20from%20the%20SL%20bottom%20model%2C%20making%20each%20token%20resemble%20the%20%24n%24-hop%20indirect%20neighbors.%20PrivTune%20formulates%20this%20as%20an%20optimization%20problem%20to%20compute%20the%20optimal%20noise%20vector%2C%20aligning%20with%20defense-utility%20goals.%20On%20this%20basis%2C%20it%20then%20adjusts%20the%20parameters%20%28i.e.%2C%20mean%29%20of%20the%20%24d_%CF%87%24-Privacy%20noise%20distribution%20to%20align%20with%20the%20optimization%20direction%20and%20scales%20the%20noise%20according%20to%20token%20importance%20to%20minimize%20distortion.%20Experiments%20on%20five%20datasets%20%28covering%20both%20classification%20and%20generation%20tasks%29%20against%20three%20embedding%20inversion%20and%20three%20attribute%20inference%20attacks%20show%20that%2C%20using%20RoBERTa%20on%20the%20Stanford%20Sentiment%20Treebank%20dataset%2C%20PrivTune%20reduces%20the%20attack%20success%20rate%20to%2010%25%20with%20only%20a%203.33%25%20drop%20in%20utility%20performance%2C%20outperforming%20state-of-the-art%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08809v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrivTune%253A%2520Efficient%2520and%2520Privacy-Preserving%2520Fine-Tuning%2520of%2520Large%2520Language%2520Models%2520via%2520Device-Cloud%2520Collaboration%26entry.906535625%3DYi%2520Liu%2520and%2520Weixiang%2520Han%2520and%2520Chengjun%2520Cai%2520and%2520Xingliang%2520Yuan%2520and%2520Cong%2520Wang%26entry.1292438233%3DWith%2520the%2520rise%2520of%2520large%2520language%2520models%252C%2520service%2520providers%2520offer%2520language%2520models%2520as%2520a%2520service%252C%2520enabling%2520users%2520to%2520fine-tune%2520customized%2520models%2520via%2520uploaded%2520private%2520datasets.%2520However%252C%2520this%2520raises%2520concerns%2520about%2520sensitive%2520data%2520leakage.%2520Prior%2520methods%252C%2520relying%2520on%2520differential%2520privacy%2520within%2520device-cloud%2520collaboration%2520frameworks%252C%2520struggle%2520to%2520balance%2520privacy%2520and%2520utility%252C%2520exposing%2520users%2520to%2520inference%2520attacks%2520or%2520degrading%2520fine-tuning%2520performance.%2520To%2520address%2520this%252C%2520we%2520propose%2520PrivTune%252C%2520an%2520efficient%2520and%2520privacy-preserving%2520fine-tuning%2520framework%2520via%2520Split%2520Learning%2520%2528SL%2529.%2520The%2520key%2520idea%2520of%2520PrivTune%2520is%2520to%2520inject%2520crafted%2520noise%2520into%2520token%2520representations%2520from%2520the%2520SL%2520bottom%2520model%252C%2520making%2520each%2520token%2520resemble%2520the%2520%2524n%2524-hop%2520indirect%2520neighbors.%2520PrivTune%2520formulates%2520this%2520as%2520an%2520optimization%2520problem%2520to%2520compute%2520the%2520optimal%2520noise%2520vector%252C%2520aligning%2520with%2520defense-utility%2520goals.%2520On%2520this%2520basis%252C%2520it%2520then%2520adjusts%2520the%2520parameters%2520%2528i.e.%252C%2520mean%2529%2520of%2520the%2520%2524d_%25CF%2587%2524-Privacy%2520noise%2520distribution%2520to%2520align%2520with%2520the%2520optimization%2520direction%2520and%2520scales%2520the%2520noise%2520according%2520to%2520token%2520importance%2520to%2520minimize%2520distortion.%2520Experiments%2520on%2520five%2520datasets%2520%2528covering%2520both%2520classification%2520and%2520generation%2520tasks%2529%2520against%2520three%2520embedding%2520inversion%2520and%2520three%2520attribute%2520inference%2520attacks%2520show%2520that%252C%2520using%2520RoBERTa%2520on%2520the%2520Stanford%2520Sentiment%2520Treebank%2520dataset%252C%2520PrivTune%2520reduces%2520the%2520attack%2520success%2520rate%2520to%252010%2525%2520with%2520only%2520a%25203.33%2525%2520drop%2520in%2520utility%2520performance%252C%2520outperforming%2520state-of-the-art%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08809v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PrivTune%3A%20Efficient%20and%20Privacy-Preserving%20Fine-Tuning%20of%20Large%20Language%20Models%20via%20Device-Cloud%20Collaboration&entry.906535625=Yi%20Liu%20and%20Weixiang%20Han%20and%20Chengjun%20Cai%20and%20Xingliang%20Yuan%20and%20Cong%20Wang&entry.1292438233=With%20the%20rise%20of%20large%20language%20models%2C%20service%20providers%20offer%20language%20models%20as%20a%20service%2C%20enabling%20users%20to%20fine-tune%20customized%20models%20via%20uploaded%20private%20datasets.%20However%2C%20this%20raises%20concerns%20about%20sensitive%20data%20leakage.%20Prior%20methods%2C%20relying%20on%20differential%20privacy%20within%20device-cloud%20collaboration%20frameworks%2C%20struggle%20to%20balance%20privacy%20and%20utility%2C%20exposing%20users%20to%20inference%20attacks%20or%20degrading%20fine-tuning%20performance.%20To%20address%20this%2C%20we%20propose%20PrivTune%2C%20an%20efficient%20and%20privacy-preserving%20fine-tuning%20framework%20via%20Split%20Learning%20%28SL%29.%20The%20key%20idea%20of%20PrivTune%20is%20to%20inject%20crafted%20noise%20into%20token%20representations%20from%20the%20SL%20bottom%20model%2C%20making%20each%20token%20resemble%20the%20%24n%24-hop%20indirect%20neighbors.%20PrivTune%20formulates%20this%20as%20an%20optimization%20problem%20to%20compute%20the%20optimal%20noise%20vector%2C%20aligning%20with%20defense-utility%20goals.%20On%20this%20basis%2C%20it%20then%20adjusts%20the%20parameters%20%28i.e.%2C%20mean%29%20of%20the%20%24d_%CF%87%24-Privacy%20noise%20distribution%20to%20align%20with%20the%20optimization%20direction%20and%20scales%20the%20noise%20according%20to%20token%20importance%20to%20minimize%20distortion.%20Experiments%20on%20five%20datasets%20%28covering%20both%20classification%20and%20generation%20tasks%29%20against%20three%20embedding%20inversion%20and%20three%20attribute%20inference%20attacks%20show%20that%2C%20using%20RoBERTa%20on%20the%20Stanford%20Sentiment%20Treebank%20dataset%2C%20PrivTune%20reduces%20the%20attack%20success%20rate%20to%2010%25%20with%20only%20a%203.33%25%20drop%20in%20utility%20performance%2C%20outperforming%20state-of-the-art%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2512.08809v2&entry.124074799=Read"},
{"title": "EXAONE 3.0 7.8B Instruction Tuned Language Model", "author": "Soyoung An and Kyunghoon Bae and Eunbi Choi and Stanley Jungkyu Choi and Yemuk Choi and Seokhee Hong and Yeonjung Hong and Junwon Hwang and Hyojin Jeon and Gerrard Jeongwon Jo and Hyunjik Jo and Jiyeon Jung and Yountae Jung and Euisoon Kim and Hyosang Kim and Joonkee Kim and Seonghwan Kim and Soyeon Kim and Sunkyoung Kim and Yireun Kim and Youchul Kim and Edward Hwayoung Lee and Haeju Lee and Honglak Lee and Jinsik Lee and Kyungmin Lee and Moontae Lee and Seungjun Lee and Woohyung Lim and Sangha Park and Sooyoun Park and Yongmin Park and Boseong Seo and Sihoon Yang and Heuiyeen Yeen and Kyungjae Yoo and Hyeongu Yun", "abstract": "We introduce EXAONE 3.0 instruction-tuned language model, the first open model in the family of Large Language Models (LLMs) developed by LG AI Research. Among different model sizes, we publicly release the 7.8B instruction-tuned model to promote open research and innovations. Through extensive evaluations across a wide range of public and in-house benchmarks, EXAONE 3.0 demonstrates highly competitive real-world performance with instruction-following capability against other state-of-the-art open models of similar size. Our comparative analysis shows that EXAONE 3.0 excels particularly in Korean, while achieving compelling performance across general tasks and complex reasoning. With its strong real-world effectiveness and bilingual proficiency, we hope that EXAONE keeps contributing to advancements in Expert AI. Our EXAONE 3.0 instruction-tuned model is available at https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct.", "link": "http://arxiv.org/abs/2408.03541v4", "date": "2026-01-02", "relevancy": 2.0289, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5173}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5173}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EXAONE%203.0%207.8B%20Instruction%20Tuned%20Language%20Model&body=Title%3A%20EXAONE%203.0%207.8B%20Instruction%20Tuned%20Language%20Model%0AAuthor%3A%20Soyoung%20An%20and%20Kyunghoon%20Bae%20and%20Eunbi%20Choi%20and%20Stanley%20Jungkyu%20Choi%20and%20Yemuk%20Choi%20and%20Seokhee%20Hong%20and%20Yeonjung%20Hong%20and%20Junwon%20Hwang%20and%20Hyojin%20Jeon%20and%20Gerrard%20Jeongwon%20Jo%20and%20Hyunjik%20Jo%20and%20Jiyeon%20Jung%20and%20Yountae%20Jung%20and%20Euisoon%20Kim%20and%20Hyosang%20Kim%20and%20Joonkee%20Kim%20and%20Seonghwan%20Kim%20and%20Soyeon%20Kim%20and%20Sunkyoung%20Kim%20and%20Yireun%20Kim%20and%20Youchul%20Kim%20and%20Edward%20Hwayoung%20Lee%20and%20Haeju%20Lee%20and%20Honglak%20Lee%20and%20Jinsik%20Lee%20and%20Kyungmin%20Lee%20and%20Moontae%20Lee%20and%20Seungjun%20Lee%20and%20Woohyung%20Lim%20and%20Sangha%20Park%20and%20Sooyoun%20Park%20and%20Yongmin%20Park%20and%20Boseong%20Seo%20and%20Sihoon%20Yang%20and%20Heuiyeen%20Yeen%20and%20Kyungjae%20Yoo%20and%20Hyeongu%20Yun%0AAbstract%3A%20We%20introduce%20EXAONE%203.0%20instruction-tuned%20language%20model%2C%20the%20first%20open%20model%20in%20the%20family%20of%20Large%20Language%20Models%20%28LLMs%29%20developed%20by%20LG%20AI%20Research.%20Among%20different%20model%20sizes%2C%20we%20publicly%20release%20the%207.8B%20instruction-tuned%20model%20to%20promote%20open%20research%20and%20innovations.%20Through%20extensive%20evaluations%20across%20a%20wide%20range%20of%20public%20and%20in-house%20benchmarks%2C%20EXAONE%203.0%20demonstrates%20highly%20competitive%20real-world%20performance%20with%20instruction-following%20capability%20against%20other%20state-of-the-art%20open%20models%20of%20similar%20size.%20Our%20comparative%20analysis%20shows%20that%20EXAONE%203.0%20excels%20particularly%20in%20Korean%2C%20while%20achieving%20compelling%20performance%20across%20general%20tasks%20and%20complex%20reasoning.%20With%20its%20strong%20real-world%20effectiveness%20and%20bilingual%20proficiency%2C%20we%20hope%20that%20EXAONE%20keeps%20contributing%20to%20advancements%20in%20Expert%20AI.%20Our%20EXAONE%203.0%20instruction-tuned%20model%20is%20available%20at%20https%3A//huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct.%0ALink%3A%20http%3A//arxiv.org/abs/2408.03541v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEXAONE%25203.0%25207.8B%2520Instruction%2520Tuned%2520Language%2520Model%26entry.906535625%3DSoyoung%2520An%2520and%2520Kyunghoon%2520Bae%2520and%2520Eunbi%2520Choi%2520and%2520Stanley%2520Jungkyu%2520Choi%2520and%2520Yemuk%2520Choi%2520and%2520Seokhee%2520Hong%2520and%2520Yeonjung%2520Hong%2520and%2520Junwon%2520Hwang%2520and%2520Hyojin%2520Jeon%2520and%2520Gerrard%2520Jeongwon%2520Jo%2520and%2520Hyunjik%2520Jo%2520and%2520Jiyeon%2520Jung%2520and%2520Yountae%2520Jung%2520and%2520Euisoon%2520Kim%2520and%2520Hyosang%2520Kim%2520and%2520Joonkee%2520Kim%2520and%2520Seonghwan%2520Kim%2520and%2520Soyeon%2520Kim%2520and%2520Sunkyoung%2520Kim%2520and%2520Yireun%2520Kim%2520and%2520Youchul%2520Kim%2520and%2520Edward%2520Hwayoung%2520Lee%2520and%2520Haeju%2520Lee%2520and%2520Honglak%2520Lee%2520and%2520Jinsik%2520Lee%2520and%2520Kyungmin%2520Lee%2520and%2520Moontae%2520Lee%2520and%2520Seungjun%2520Lee%2520and%2520Woohyung%2520Lim%2520and%2520Sangha%2520Park%2520and%2520Sooyoun%2520Park%2520and%2520Yongmin%2520Park%2520and%2520Boseong%2520Seo%2520and%2520Sihoon%2520Yang%2520and%2520Heuiyeen%2520Yeen%2520and%2520Kyungjae%2520Yoo%2520and%2520Hyeongu%2520Yun%26entry.1292438233%3DWe%2520introduce%2520EXAONE%25203.0%2520instruction-tuned%2520language%2520model%252C%2520the%2520first%2520open%2520model%2520in%2520the%2520family%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520developed%2520by%2520LG%2520AI%2520Research.%2520Among%2520different%2520model%2520sizes%252C%2520we%2520publicly%2520release%2520the%25207.8B%2520instruction-tuned%2520model%2520to%2520promote%2520open%2520research%2520and%2520innovations.%2520Through%2520extensive%2520evaluations%2520across%2520a%2520wide%2520range%2520of%2520public%2520and%2520in-house%2520benchmarks%252C%2520EXAONE%25203.0%2520demonstrates%2520highly%2520competitive%2520real-world%2520performance%2520with%2520instruction-following%2520capability%2520against%2520other%2520state-of-the-art%2520open%2520models%2520of%2520similar%2520size.%2520Our%2520comparative%2520analysis%2520shows%2520that%2520EXAONE%25203.0%2520excels%2520particularly%2520in%2520Korean%252C%2520while%2520achieving%2520compelling%2520performance%2520across%2520general%2520tasks%2520and%2520complex%2520reasoning.%2520With%2520its%2520strong%2520real-world%2520effectiveness%2520and%2520bilingual%2520proficiency%252C%2520we%2520hope%2520that%2520EXAONE%2520keeps%2520contributing%2520to%2520advancements%2520in%2520Expert%2520AI.%2520Our%2520EXAONE%25203.0%2520instruction-tuned%2520model%2520is%2520available%2520at%2520https%253A//huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03541v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EXAONE%203.0%207.8B%20Instruction%20Tuned%20Language%20Model&entry.906535625=Soyoung%20An%20and%20Kyunghoon%20Bae%20and%20Eunbi%20Choi%20and%20Stanley%20Jungkyu%20Choi%20and%20Yemuk%20Choi%20and%20Seokhee%20Hong%20and%20Yeonjung%20Hong%20and%20Junwon%20Hwang%20and%20Hyojin%20Jeon%20and%20Gerrard%20Jeongwon%20Jo%20and%20Hyunjik%20Jo%20and%20Jiyeon%20Jung%20and%20Yountae%20Jung%20and%20Euisoon%20Kim%20and%20Hyosang%20Kim%20and%20Joonkee%20Kim%20and%20Seonghwan%20Kim%20and%20Soyeon%20Kim%20and%20Sunkyoung%20Kim%20and%20Yireun%20Kim%20and%20Youchul%20Kim%20and%20Edward%20Hwayoung%20Lee%20and%20Haeju%20Lee%20and%20Honglak%20Lee%20and%20Jinsik%20Lee%20and%20Kyungmin%20Lee%20and%20Moontae%20Lee%20and%20Seungjun%20Lee%20and%20Woohyung%20Lim%20and%20Sangha%20Park%20and%20Sooyoun%20Park%20and%20Yongmin%20Park%20and%20Boseong%20Seo%20and%20Sihoon%20Yang%20and%20Heuiyeen%20Yeen%20and%20Kyungjae%20Yoo%20and%20Hyeongu%20Yun&entry.1292438233=We%20introduce%20EXAONE%203.0%20instruction-tuned%20language%20model%2C%20the%20first%20open%20model%20in%20the%20family%20of%20Large%20Language%20Models%20%28LLMs%29%20developed%20by%20LG%20AI%20Research.%20Among%20different%20model%20sizes%2C%20we%20publicly%20release%20the%207.8B%20instruction-tuned%20model%20to%20promote%20open%20research%20and%20innovations.%20Through%20extensive%20evaluations%20across%20a%20wide%20range%20of%20public%20and%20in-house%20benchmarks%2C%20EXAONE%203.0%20demonstrates%20highly%20competitive%20real-world%20performance%20with%20instruction-following%20capability%20against%20other%20state-of-the-art%20open%20models%20of%20similar%20size.%20Our%20comparative%20analysis%20shows%20that%20EXAONE%203.0%20excels%20particularly%20in%20Korean%2C%20while%20achieving%20compelling%20performance%20across%20general%20tasks%20and%20complex%20reasoning.%20With%20its%20strong%20real-world%20effectiveness%20and%20bilingual%20proficiency%2C%20we%20hope%20that%20EXAONE%20keeps%20contributing%20to%20advancements%20in%20Expert%20AI.%20Our%20EXAONE%203.0%20instruction-tuned%20model%20is%20available%20at%20https%3A//huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct.&entry.1838667208=http%3A//arxiv.org/abs/2408.03541v4&entry.124074799=Read"},
{"title": "Two Deep Learning Approaches for Automated Segmentation of Left Ventricle in Cine Cardiac MRI", "author": "Wenhui Chu and Nikolaos V. Tsekos", "abstract": "Left ventricle (LV) segmentation is critical for clinical quantification and diagnosis of cardiac images. In this work, we propose two novel deep learning architectures called LNU-Net and IBU-Net for left ventricle segmentation from short-axis cine MRI images. LNU-Net is derived from layer normalization (LN) U-Net architecture, while IBU-Net is derived from the instance-batch normalized (IB) U-Net for medical image segmentation. The architectures of LNU-Net and IBU-Net have a down-sampling path for feature extraction and an up-sampling path for precise localization. We use the original U-Net as the basic segmentation approach and compared it with our proposed architectures. Both LNU-Net and IBU-Net have left ventricle segmentation methods: LNU-Net applies layer normalization in each convolutional block, while IBU-Net incorporates instance and batch normalization together in the first convolutional block and passes its result to the next layer. Our method incorporates affine transformations and elastic deformations for image data processing. Our dataset that contains 805 MRI images regarding the left ventricle from 45 patients is used for evaluation. We experimentally evaluate the results of the proposed approaches outperforming the dice coefficient and the average perpendicular distance than other state-of-the-art approaches.", "link": "http://arxiv.org/abs/2601.00794v1", "date": "2026-01-02", "relevancy": 2.0192, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5144}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.509}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Two%20Deep%20Learning%20Approaches%20for%20Automated%20Segmentation%20of%20Left%20Ventricle%20in%20Cine%20Cardiac%20MRI&body=Title%3A%20Two%20Deep%20Learning%20Approaches%20for%20Automated%20Segmentation%20of%20Left%20Ventricle%20in%20Cine%20Cardiac%20MRI%0AAuthor%3A%20Wenhui%20Chu%20and%20Nikolaos%20V.%20Tsekos%0AAbstract%3A%20Left%20ventricle%20%28LV%29%20segmentation%20is%20critical%20for%20clinical%20quantification%20and%20diagnosis%20of%20cardiac%20images.%20In%20this%20work%2C%20we%20propose%20two%20novel%20deep%20learning%20architectures%20called%20LNU-Net%20and%20IBU-Net%20for%20left%20ventricle%20segmentation%20from%20short-axis%20cine%20MRI%20images.%20LNU-Net%20is%20derived%20from%20layer%20normalization%20%28LN%29%20U-Net%20architecture%2C%20while%20IBU-Net%20is%20derived%20from%20the%20instance-batch%20normalized%20%28IB%29%20U-Net%20for%20medical%20image%20segmentation.%20The%20architectures%20of%20LNU-Net%20and%20IBU-Net%20have%20a%20down-sampling%20path%20for%20feature%20extraction%20and%20an%20up-sampling%20path%20for%20precise%20localization.%20We%20use%20the%20original%20U-Net%20as%20the%20basic%20segmentation%20approach%20and%20compared%20it%20with%20our%20proposed%20architectures.%20Both%20LNU-Net%20and%20IBU-Net%20have%20left%20ventricle%20segmentation%20methods%3A%20LNU-Net%20applies%20layer%20normalization%20in%20each%20convolutional%20block%2C%20while%20IBU-Net%20incorporates%20instance%20and%20batch%20normalization%20together%20in%20the%20first%20convolutional%20block%20and%20passes%20its%20result%20to%20the%20next%20layer.%20Our%20method%20incorporates%20affine%20transformations%20and%20elastic%20deformations%20for%20image%20data%20processing.%20Our%20dataset%20that%20contains%20805%20MRI%20images%20regarding%20the%20left%20ventricle%20from%2045%20patients%20is%20used%20for%20evaluation.%20We%20experimentally%20evaluate%20the%20results%20of%20the%20proposed%20approaches%20outperforming%20the%20dice%20coefficient%20and%20the%20average%20perpendicular%20distance%20than%20other%20state-of-the-art%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00794v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwo%2520Deep%2520Learning%2520Approaches%2520for%2520Automated%2520Segmentation%2520of%2520Left%2520Ventricle%2520in%2520Cine%2520Cardiac%2520MRI%26entry.906535625%3DWenhui%2520Chu%2520and%2520Nikolaos%2520V.%2520Tsekos%26entry.1292438233%3DLeft%2520ventricle%2520%2528LV%2529%2520segmentation%2520is%2520critical%2520for%2520clinical%2520quantification%2520and%2520diagnosis%2520of%2520cardiac%2520images.%2520In%2520this%2520work%252C%2520we%2520propose%2520two%2520novel%2520deep%2520learning%2520architectures%2520called%2520LNU-Net%2520and%2520IBU-Net%2520for%2520left%2520ventricle%2520segmentation%2520from%2520short-axis%2520cine%2520MRI%2520images.%2520LNU-Net%2520is%2520derived%2520from%2520layer%2520normalization%2520%2528LN%2529%2520U-Net%2520architecture%252C%2520while%2520IBU-Net%2520is%2520derived%2520from%2520the%2520instance-batch%2520normalized%2520%2528IB%2529%2520U-Net%2520for%2520medical%2520image%2520segmentation.%2520The%2520architectures%2520of%2520LNU-Net%2520and%2520IBU-Net%2520have%2520a%2520down-sampling%2520path%2520for%2520feature%2520extraction%2520and%2520an%2520up-sampling%2520path%2520for%2520precise%2520localization.%2520We%2520use%2520the%2520original%2520U-Net%2520as%2520the%2520basic%2520segmentation%2520approach%2520and%2520compared%2520it%2520with%2520our%2520proposed%2520architectures.%2520Both%2520LNU-Net%2520and%2520IBU-Net%2520have%2520left%2520ventricle%2520segmentation%2520methods%253A%2520LNU-Net%2520applies%2520layer%2520normalization%2520in%2520each%2520convolutional%2520block%252C%2520while%2520IBU-Net%2520incorporates%2520instance%2520and%2520batch%2520normalization%2520together%2520in%2520the%2520first%2520convolutional%2520block%2520and%2520passes%2520its%2520result%2520to%2520the%2520next%2520layer.%2520Our%2520method%2520incorporates%2520affine%2520transformations%2520and%2520elastic%2520deformations%2520for%2520image%2520data%2520processing.%2520Our%2520dataset%2520that%2520contains%2520805%2520MRI%2520images%2520regarding%2520the%2520left%2520ventricle%2520from%252045%2520patients%2520is%2520used%2520for%2520evaluation.%2520We%2520experimentally%2520evaluate%2520the%2520results%2520of%2520the%2520proposed%2520approaches%2520outperforming%2520the%2520dice%2520coefficient%2520and%2520the%2520average%2520perpendicular%2520distance%2520than%2520other%2520state-of-the-art%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00794v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two%20Deep%20Learning%20Approaches%20for%20Automated%20Segmentation%20of%20Left%20Ventricle%20in%20Cine%20Cardiac%20MRI&entry.906535625=Wenhui%20Chu%20and%20Nikolaos%20V.%20Tsekos&entry.1292438233=Left%20ventricle%20%28LV%29%20segmentation%20is%20critical%20for%20clinical%20quantification%20and%20diagnosis%20of%20cardiac%20images.%20In%20this%20work%2C%20we%20propose%20two%20novel%20deep%20learning%20architectures%20called%20LNU-Net%20and%20IBU-Net%20for%20left%20ventricle%20segmentation%20from%20short-axis%20cine%20MRI%20images.%20LNU-Net%20is%20derived%20from%20layer%20normalization%20%28LN%29%20U-Net%20architecture%2C%20while%20IBU-Net%20is%20derived%20from%20the%20instance-batch%20normalized%20%28IB%29%20U-Net%20for%20medical%20image%20segmentation.%20The%20architectures%20of%20LNU-Net%20and%20IBU-Net%20have%20a%20down-sampling%20path%20for%20feature%20extraction%20and%20an%20up-sampling%20path%20for%20precise%20localization.%20We%20use%20the%20original%20U-Net%20as%20the%20basic%20segmentation%20approach%20and%20compared%20it%20with%20our%20proposed%20architectures.%20Both%20LNU-Net%20and%20IBU-Net%20have%20left%20ventricle%20segmentation%20methods%3A%20LNU-Net%20applies%20layer%20normalization%20in%20each%20convolutional%20block%2C%20while%20IBU-Net%20incorporates%20instance%20and%20batch%20normalization%20together%20in%20the%20first%20convolutional%20block%20and%20passes%20its%20result%20to%20the%20next%20layer.%20Our%20method%20incorporates%20affine%20transformations%20and%20elastic%20deformations%20for%20image%20data%20processing.%20Our%20dataset%20that%20contains%20805%20MRI%20images%20regarding%20the%20left%20ventricle%20from%2045%20patients%20is%20used%20for%20evaluation.%20We%20experimentally%20evaluate%20the%20results%20of%20the%20proposed%20approaches%20outperforming%20the%20dice%20coefficient%20and%20the%20average%20perpendicular%20distance%20than%20other%20state-of-the-art%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2601.00794v1&entry.124074799=Read"},
{"title": "EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes", "author": "Kyunghoon Bae and Eunbi Choi and Kibong Choi and Stanley Jungkyu Choi and Yemuk Choi and Kyubeen Han and Seokhee Hong and Junwon Hwang and Taewan Hwang and Joonwon Jang and Hyojin Jeon and Kijeong Jeon and Gerrard Jeongwon Jo and Hyunjik Jo and Jiyeon Jung and Euisoon Kim and Hyosang Kim and Jihoon Kim and Joonkee Kim and Seonghwan Kim and Soyeon Kim and Sunkyoung Kim and Yireun Kim and Yongil Kim and Youchul Kim and Edward Hwayoung Lee and Gwangho Lee and Haeju Lee and Honglak Lee and Jinsik Lee and Kyungmin Lee and Sangha Park and Young Min Paik and Yongmin Park and Youngyong Park and Sanghyun Seo and Sihoon Yang and Heuiyeen Yeen and Sihyuk Yi and Hyeongu Yun", "abstract": "This technical report introduces EXAONE 4.0, which integrates a Non-reasoning mode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5 and the advanced reasoning abilities of EXAONE Deep. To pave the way for the agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool use, and its multilingual capabilities are extended to support Spanish in addition to English and Korean. The EXAONE 4.0 model series consists of two sizes: a mid-size 32B model optimized for high performance, and a small-size 1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates superior performance compared to open-weight models in its class and remains competitive even against frontier-class models. The models are publicly available for research purposes and can be easily downloaded via https://huggingface.co/LGAI-EXAONE.", "link": "http://arxiv.org/abs/2507.11407v2", "date": "2026-01-02", "relevancy": 2.018, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5102}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5102}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EXAONE%204.0%3A%20Unified%20Large%20Language%20Models%20Integrating%20Non-reasoning%20and%20Reasoning%20Modes&body=Title%3A%20EXAONE%204.0%3A%20Unified%20Large%20Language%20Models%20Integrating%20Non-reasoning%20and%20Reasoning%20Modes%0AAuthor%3A%20Kyunghoon%20Bae%20and%20Eunbi%20Choi%20and%20Kibong%20Choi%20and%20Stanley%20Jungkyu%20Choi%20and%20Yemuk%20Choi%20and%20Kyubeen%20Han%20and%20Seokhee%20Hong%20and%20Junwon%20Hwang%20and%20Taewan%20Hwang%20and%20Joonwon%20Jang%20and%20Hyojin%20Jeon%20and%20Kijeong%20Jeon%20and%20Gerrard%20Jeongwon%20Jo%20and%20Hyunjik%20Jo%20and%20Jiyeon%20Jung%20and%20Euisoon%20Kim%20and%20Hyosang%20Kim%20and%20Jihoon%20Kim%20and%20Joonkee%20Kim%20and%20Seonghwan%20Kim%20and%20Soyeon%20Kim%20and%20Sunkyoung%20Kim%20and%20Yireun%20Kim%20and%20Yongil%20Kim%20and%20Youchul%20Kim%20and%20Edward%20Hwayoung%20Lee%20and%20Gwangho%20Lee%20and%20Haeju%20Lee%20and%20Honglak%20Lee%20and%20Jinsik%20Lee%20and%20Kyungmin%20Lee%20and%20Sangha%20Park%20and%20Young%20Min%20Paik%20and%20Yongmin%20Park%20and%20Youngyong%20Park%20and%20Sanghyun%20Seo%20and%20Sihoon%20Yang%20and%20Heuiyeen%20Yeen%20and%20Sihyuk%20Yi%20and%20Hyeongu%20Yun%0AAbstract%3A%20This%20technical%20report%20introduces%20EXAONE%204.0%2C%20which%20integrates%20a%20Non-reasoning%20mode%20and%20a%20Reasoning%20mode%20to%20achieve%20both%20the%20excellent%20usability%20of%20EXAONE%203.5%20and%20the%20advanced%20reasoning%20abilities%20of%20EXAONE%20Deep.%20To%20pave%20the%20way%20for%20the%20agentic%20AI%20era%2C%20EXAONE%204.0%20incorporates%20essential%20features%20such%20as%20agentic%20tool%20use%2C%20and%20its%20multilingual%20capabilities%20are%20extended%20to%20support%20Spanish%20in%20addition%20to%20English%20and%20Korean.%20The%20EXAONE%204.0%20model%20series%20consists%20of%20two%20sizes%3A%20a%20mid-size%2032B%20model%20optimized%20for%20high%20performance%2C%20and%20a%20small-size%201.2B%20model%20designed%20for%20on-device%20applications.%20The%20EXAONE%204.0%20demonstrates%20superior%20performance%20compared%20to%20open-weight%20models%20in%20its%20class%20and%20remains%20competitive%20even%20against%20frontier-class%20models.%20The%20models%20are%20publicly%20available%20for%20research%20purposes%20and%20can%20be%20easily%20downloaded%20via%20https%3A//huggingface.co/LGAI-EXAONE.%0ALink%3A%20http%3A//arxiv.org/abs/2507.11407v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEXAONE%25204.0%253A%2520Unified%2520Large%2520Language%2520Models%2520Integrating%2520Non-reasoning%2520and%2520Reasoning%2520Modes%26entry.906535625%3DKyunghoon%2520Bae%2520and%2520Eunbi%2520Choi%2520and%2520Kibong%2520Choi%2520and%2520Stanley%2520Jungkyu%2520Choi%2520and%2520Yemuk%2520Choi%2520and%2520Kyubeen%2520Han%2520and%2520Seokhee%2520Hong%2520and%2520Junwon%2520Hwang%2520and%2520Taewan%2520Hwang%2520and%2520Joonwon%2520Jang%2520and%2520Hyojin%2520Jeon%2520and%2520Kijeong%2520Jeon%2520and%2520Gerrard%2520Jeongwon%2520Jo%2520and%2520Hyunjik%2520Jo%2520and%2520Jiyeon%2520Jung%2520and%2520Euisoon%2520Kim%2520and%2520Hyosang%2520Kim%2520and%2520Jihoon%2520Kim%2520and%2520Joonkee%2520Kim%2520and%2520Seonghwan%2520Kim%2520and%2520Soyeon%2520Kim%2520and%2520Sunkyoung%2520Kim%2520and%2520Yireun%2520Kim%2520and%2520Yongil%2520Kim%2520and%2520Youchul%2520Kim%2520and%2520Edward%2520Hwayoung%2520Lee%2520and%2520Gwangho%2520Lee%2520and%2520Haeju%2520Lee%2520and%2520Honglak%2520Lee%2520and%2520Jinsik%2520Lee%2520and%2520Kyungmin%2520Lee%2520and%2520Sangha%2520Park%2520and%2520Young%2520Min%2520Paik%2520and%2520Yongmin%2520Park%2520and%2520Youngyong%2520Park%2520and%2520Sanghyun%2520Seo%2520and%2520Sihoon%2520Yang%2520and%2520Heuiyeen%2520Yeen%2520and%2520Sihyuk%2520Yi%2520and%2520Hyeongu%2520Yun%26entry.1292438233%3DThis%2520technical%2520report%2520introduces%2520EXAONE%25204.0%252C%2520which%2520integrates%2520a%2520Non-reasoning%2520mode%2520and%2520a%2520Reasoning%2520mode%2520to%2520achieve%2520both%2520the%2520excellent%2520usability%2520of%2520EXAONE%25203.5%2520and%2520the%2520advanced%2520reasoning%2520abilities%2520of%2520EXAONE%2520Deep.%2520To%2520pave%2520the%2520way%2520for%2520the%2520agentic%2520AI%2520era%252C%2520EXAONE%25204.0%2520incorporates%2520essential%2520features%2520such%2520as%2520agentic%2520tool%2520use%252C%2520and%2520its%2520multilingual%2520capabilities%2520are%2520extended%2520to%2520support%2520Spanish%2520in%2520addition%2520to%2520English%2520and%2520Korean.%2520The%2520EXAONE%25204.0%2520model%2520series%2520consists%2520of%2520two%2520sizes%253A%2520a%2520mid-size%252032B%2520model%2520optimized%2520for%2520high%2520performance%252C%2520and%2520a%2520small-size%25201.2B%2520model%2520designed%2520for%2520on-device%2520applications.%2520The%2520EXAONE%25204.0%2520demonstrates%2520superior%2520performance%2520compared%2520to%2520open-weight%2520models%2520in%2520its%2520class%2520and%2520remains%2520competitive%2520even%2520against%2520frontier-class%2520models.%2520The%2520models%2520are%2520publicly%2520available%2520for%2520research%2520purposes%2520and%2520can%2520be%2520easily%2520downloaded%2520via%2520https%253A//huggingface.co/LGAI-EXAONE.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11407v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EXAONE%204.0%3A%20Unified%20Large%20Language%20Models%20Integrating%20Non-reasoning%20and%20Reasoning%20Modes&entry.906535625=Kyunghoon%20Bae%20and%20Eunbi%20Choi%20and%20Kibong%20Choi%20and%20Stanley%20Jungkyu%20Choi%20and%20Yemuk%20Choi%20and%20Kyubeen%20Han%20and%20Seokhee%20Hong%20and%20Junwon%20Hwang%20and%20Taewan%20Hwang%20and%20Joonwon%20Jang%20and%20Hyojin%20Jeon%20and%20Kijeong%20Jeon%20and%20Gerrard%20Jeongwon%20Jo%20and%20Hyunjik%20Jo%20and%20Jiyeon%20Jung%20and%20Euisoon%20Kim%20and%20Hyosang%20Kim%20and%20Jihoon%20Kim%20and%20Joonkee%20Kim%20and%20Seonghwan%20Kim%20and%20Soyeon%20Kim%20and%20Sunkyoung%20Kim%20and%20Yireun%20Kim%20and%20Yongil%20Kim%20and%20Youchul%20Kim%20and%20Edward%20Hwayoung%20Lee%20and%20Gwangho%20Lee%20and%20Haeju%20Lee%20and%20Honglak%20Lee%20and%20Jinsik%20Lee%20and%20Kyungmin%20Lee%20and%20Sangha%20Park%20and%20Young%20Min%20Paik%20and%20Yongmin%20Park%20and%20Youngyong%20Park%20and%20Sanghyun%20Seo%20and%20Sihoon%20Yang%20and%20Heuiyeen%20Yeen%20and%20Sihyuk%20Yi%20and%20Hyeongu%20Yun&entry.1292438233=This%20technical%20report%20introduces%20EXAONE%204.0%2C%20which%20integrates%20a%20Non-reasoning%20mode%20and%20a%20Reasoning%20mode%20to%20achieve%20both%20the%20excellent%20usability%20of%20EXAONE%203.5%20and%20the%20advanced%20reasoning%20abilities%20of%20EXAONE%20Deep.%20To%20pave%20the%20way%20for%20the%20agentic%20AI%20era%2C%20EXAONE%204.0%20incorporates%20essential%20features%20such%20as%20agentic%20tool%20use%2C%20and%20its%20multilingual%20capabilities%20are%20extended%20to%20support%20Spanish%20in%20addition%20to%20English%20and%20Korean.%20The%20EXAONE%204.0%20model%20series%20consists%20of%20two%20sizes%3A%20a%20mid-size%2032B%20model%20optimized%20for%20high%20performance%2C%20and%20a%20small-size%201.2B%20model%20designed%20for%20on-device%20applications.%20The%20EXAONE%204.0%20demonstrates%20superior%20performance%20compared%20to%20open-weight%20models%20in%20its%20class%20and%20remains%20competitive%20even%20against%20frontier-class%20models.%20The%20models%20are%20publicly%20available%20for%20research%20purposes%20and%20can%20be%20easily%20downloaded%20via%20https%3A//huggingface.co/LGAI-EXAONE.&entry.1838667208=http%3A//arxiv.org/abs/2507.11407v2&entry.124074799=Read"},
{"title": "Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model", "author": "Hao Guan and Li Zhou", "abstract": "Vision-Language Models have demonstrated strong potential in medical image analysis and disease diagnosis. However, after deployment, their performance may deteriorate when the input data distribution shifts from that observed during development. Detecting such performance degradation is essential for clinical reliability, yet remains challenging for large pre-trained VLMs operating without labeled data. In this study, we investigate performance degradation detection under data shift in a state-of-the-art pathology VLM. We examine both input-level data shift and output-level prediction behavior to understand their respective roles in monitoring model reliability. To facilitate systematic analysis of input data shift, we develop DomainSAT, a lightweight toolbox with a graphical interface that integrates representative shift detection algorithms and enables intuitive exploration of data shift. Our analysis shows that while input data shift detection is effective at identifying distributional changes and providing early diagnostic signals, it does not always correspond to actual performance degradation. Motivated by this observation, we further study output-based monitoring and introduce a label-free, confidence-based degradation indicator that directly captures changes in model prediction confidence. We find that this indicator exhibits a close relationship with performance degradation and serves as an effective complement to input shift detection. Experiments on a large-scale pathology dataset for tumor classification demonstrate that combining input data shift detection and output confidence-based indicators enables more reliable detection and interpretation of performance degradation in VLMs under data shift. These findings provide a practical and complementary framework for monitoring the reliability of foundation models in digital pathology.", "link": "http://arxiv.org/abs/2601.00716v1", "date": "2026-01-02", "relevancy": 2.0115, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5048}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5048}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20Performance%20Degradation%20under%20Data%20Shift%20in%20Pathology%20Vision-Language%20Model&body=Title%3A%20Detecting%20Performance%20Degradation%20under%20Data%20Shift%20in%20Pathology%20Vision-Language%20Model%0AAuthor%3A%20Hao%20Guan%20and%20Li%20Zhou%0AAbstract%3A%20Vision-Language%20Models%20have%20demonstrated%20strong%20potential%20in%20medical%20image%20analysis%20and%20disease%20diagnosis.%20However%2C%20after%20deployment%2C%20their%20performance%20may%20deteriorate%20when%20the%20input%20data%20distribution%20shifts%20from%20that%20observed%20during%20development.%20Detecting%20such%20performance%20degradation%20is%20essential%20for%20clinical%20reliability%2C%20yet%20remains%20challenging%20for%20large%20pre-trained%20VLMs%20operating%20without%20labeled%20data.%20In%20this%20study%2C%20we%20investigate%20performance%20degradation%20detection%20under%20data%20shift%20in%20a%20state-of-the-art%20pathology%20VLM.%20We%20examine%20both%20input-level%20data%20shift%20and%20output-level%20prediction%20behavior%20to%20understand%20their%20respective%20roles%20in%20monitoring%20model%20reliability.%20To%20facilitate%20systematic%20analysis%20of%20input%20data%20shift%2C%20we%20develop%20DomainSAT%2C%20a%20lightweight%20toolbox%20with%20a%20graphical%20interface%20that%20integrates%20representative%20shift%20detection%20algorithms%20and%20enables%20intuitive%20exploration%20of%20data%20shift.%20Our%20analysis%20shows%20that%20while%20input%20data%20shift%20detection%20is%20effective%20at%20identifying%20distributional%20changes%20and%20providing%20early%20diagnostic%20signals%2C%20it%20does%20not%20always%20correspond%20to%20actual%20performance%20degradation.%20Motivated%20by%20this%20observation%2C%20we%20further%20study%20output-based%20monitoring%20and%20introduce%20a%20label-free%2C%20confidence-based%20degradation%20indicator%20that%20directly%20captures%20changes%20in%20model%20prediction%20confidence.%20We%20find%20that%20this%20indicator%20exhibits%20a%20close%20relationship%20with%20performance%20degradation%20and%20serves%20as%20an%20effective%20complement%20to%20input%20shift%20detection.%20Experiments%20on%20a%20large-scale%20pathology%20dataset%20for%20tumor%20classification%20demonstrate%20that%20combining%20input%20data%20shift%20detection%20and%20output%20confidence-based%20indicators%20enables%20more%20reliable%20detection%20and%20interpretation%20of%20performance%20degradation%20in%20VLMs%20under%20data%20shift.%20These%20findings%20provide%20a%20practical%20and%20complementary%20framework%20for%20monitoring%20the%20reliability%20of%20foundation%20models%20in%20digital%20pathology.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00716v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520Performance%2520Degradation%2520under%2520Data%2520Shift%2520in%2520Pathology%2520Vision-Language%2520Model%26entry.906535625%3DHao%2520Guan%2520and%2520Li%2520Zhou%26entry.1292438233%3DVision-Language%2520Models%2520have%2520demonstrated%2520strong%2520potential%2520in%2520medical%2520image%2520analysis%2520and%2520disease%2520diagnosis.%2520However%252C%2520after%2520deployment%252C%2520their%2520performance%2520may%2520deteriorate%2520when%2520the%2520input%2520data%2520distribution%2520shifts%2520from%2520that%2520observed%2520during%2520development.%2520Detecting%2520such%2520performance%2520degradation%2520is%2520essential%2520for%2520clinical%2520reliability%252C%2520yet%2520remains%2520challenging%2520for%2520large%2520pre-trained%2520VLMs%2520operating%2520without%2520labeled%2520data.%2520In%2520this%2520study%252C%2520we%2520investigate%2520performance%2520degradation%2520detection%2520under%2520data%2520shift%2520in%2520a%2520state-of-the-art%2520pathology%2520VLM.%2520We%2520examine%2520both%2520input-level%2520data%2520shift%2520and%2520output-level%2520prediction%2520behavior%2520to%2520understand%2520their%2520respective%2520roles%2520in%2520monitoring%2520model%2520reliability.%2520To%2520facilitate%2520systematic%2520analysis%2520of%2520input%2520data%2520shift%252C%2520we%2520develop%2520DomainSAT%252C%2520a%2520lightweight%2520toolbox%2520with%2520a%2520graphical%2520interface%2520that%2520integrates%2520representative%2520shift%2520detection%2520algorithms%2520and%2520enables%2520intuitive%2520exploration%2520of%2520data%2520shift.%2520Our%2520analysis%2520shows%2520that%2520while%2520input%2520data%2520shift%2520detection%2520is%2520effective%2520at%2520identifying%2520distributional%2520changes%2520and%2520providing%2520early%2520diagnostic%2520signals%252C%2520it%2520does%2520not%2520always%2520correspond%2520to%2520actual%2520performance%2520degradation.%2520Motivated%2520by%2520this%2520observation%252C%2520we%2520further%2520study%2520output-based%2520monitoring%2520and%2520introduce%2520a%2520label-free%252C%2520confidence-based%2520degradation%2520indicator%2520that%2520directly%2520captures%2520changes%2520in%2520model%2520prediction%2520confidence.%2520We%2520find%2520that%2520this%2520indicator%2520exhibits%2520a%2520close%2520relationship%2520with%2520performance%2520degradation%2520and%2520serves%2520as%2520an%2520effective%2520complement%2520to%2520input%2520shift%2520detection.%2520Experiments%2520on%2520a%2520large-scale%2520pathology%2520dataset%2520for%2520tumor%2520classification%2520demonstrate%2520that%2520combining%2520input%2520data%2520shift%2520detection%2520and%2520output%2520confidence-based%2520indicators%2520enables%2520more%2520reliable%2520detection%2520and%2520interpretation%2520of%2520performance%2520degradation%2520in%2520VLMs%2520under%2520data%2520shift.%2520These%2520findings%2520provide%2520a%2520practical%2520and%2520complementary%2520framework%2520for%2520monitoring%2520the%2520reliability%2520of%2520foundation%2520models%2520in%2520digital%2520pathology.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00716v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20Performance%20Degradation%20under%20Data%20Shift%20in%20Pathology%20Vision-Language%20Model&entry.906535625=Hao%20Guan%20and%20Li%20Zhou&entry.1292438233=Vision-Language%20Models%20have%20demonstrated%20strong%20potential%20in%20medical%20image%20analysis%20and%20disease%20diagnosis.%20However%2C%20after%20deployment%2C%20their%20performance%20may%20deteriorate%20when%20the%20input%20data%20distribution%20shifts%20from%20that%20observed%20during%20development.%20Detecting%20such%20performance%20degradation%20is%20essential%20for%20clinical%20reliability%2C%20yet%20remains%20challenging%20for%20large%20pre-trained%20VLMs%20operating%20without%20labeled%20data.%20In%20this%20study%2C%20we%20investigate%20performance%20degradation%20detection%20under%20data%20shift%20in%20a%20state-of-the-art%20pathology%20VLM.%20We%20examine%20both%20input-level%20data%20shift%20and%20output-level%20prediction%20behavior%20to%20understand%20their%20respective%20roles%20in%20monitoring%20model%20reliability.%20To%20facilitate%20systematic%20analysis%20of%20input%20data%20shift%2C%20we%20develop%20DomainSAT%2C%20a%20lightweight%20toolbox%20with%20a%20graphical%20interface%20that%20integrates%20representative%20shift%20detection%20algorithms%20and%20enables%20intuitive%20exploration%20of%20data%20shift.%20Our%20analysis%20shows%20that%20while%20input%20data%20shift%20detection%20is%20effective%20at%20identifying%20distributional%20changes%20and%20providing%20early%20diagnostic%20signals%2C%20it%20does%20not%20always%20correspond%20to%20actual%20performance%20degradation.%20Motivated%20by%20this%20observation%2C%20we%20further%20study%20output-based%20monitoring%20and%20introduce%20a%20label-free%2C%20confidence-based%20degradation%20indicator%20that%20directly%20captures%20changes%20in%20model%20prediction%20confidence.%20We%20find%20that%20this%20indicator%20exhibits%20a%20close%20relationship%20with%20performance%20degradation%20and%20serves%20as%20an%20effective%20complement%20to%20input%20shift%20detection.%20Experiments%20on%20a%20large-scale%20pathology%20dataset%20for%20tumor%20classification%20demonstrate%20that%20combining%20input%20data%20shift%20detection%20and%20output%20confidence-based%20indicators%20enables%20more%20reliable%20detection%20and%20interpretation%20of%20performance%20degradation%20in%20VLMs%20under%20data%20shift.%20These%20findings%20provide%20a%20practical%20and%20complementary%20framework%20for%20monitoring%20the%20reliability%20of%20foundation%20models%20in%20digital%20pathology.&entry.1838667208=http%3A//arxiv.org/abs/2601.00716v1&entry.124074799=Read"},
{"title": "Bayesian Inverse Games with High-Dimensional Multi-Modal Observations", "author": "Yash Jain and Xinjie Liu and Lasse Peters and David Fridovich-Keil and Ufuk Topcu", "abstract": "Many multi-agent interaction scenarios can be naturally modeled as noncooperative games, where each agent's decisions depend on others' future actions. However, deploying game-theoretic planners for autonomous decision-making requires a specification of all agents' objectives. To circumvent this practical difficulty, recent work develops maximum likelihood techniques for solving inverse games that can identify unknown agent objectives from interaction data. Unfortunately, these methods only infer point estimates and do not quantify estimator uncertainty; correspondingly, downstream planning decisions can overconfidently commit to unsafe actions. We present an approximate Bayesian inference approach for solving the inverse game problem, which can incorporate observation data from multiple modalities and be used to generate samples from the Bayesian posterior over the hidden agent objectives given limited sensor observations in real time. Concretely, the proposed Bayesian inverse game framework trains a structured variational autoencoder with an embedded differentiable Nash game solver on interaction datasets and does not require labels of agents' true objectives. Extensive experiments show that our framework successfully learns prior and posterior distributions, improves inference quality over maximum likelihood estimation-based inverse game approaches, and enables safer downstream decision-making without sacrificing efficiency. When trajectory information is uninformative or unavailable, multimodal inference further reduces uncertainty by exploiting additional observation modalities.", "link": "http://arxiv.org/abs/2601.00696v1", "date": "2026-01-02", "relevancy": 1.9887, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6167}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4733}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Inverse%20Games%20with%20High-Dimensional%20Multi-Modal%20Observations&body=Title%3A%20Bayesian%20Inverse%20Games%20with%20High-Dimensional%20Multi-Modal%20Observations%0AAuthor%3A%20Yash%20Jain%20and%20Xinjie%20Liu%20and%20Lasse%20Peters%20and%20David%20Fridovich-Keil%20and%20Ufuk%20Topcu%0AAbstract%3A%20Many%20multi-agent%20interaction%20scenarios%20can%20be%20naturally%20modeled%20as%20noncooperative%20games%2C%20where%20each%20agent%27s%20decisions%20depend%20on%20others%27%20future%20actions.%20However%2C%20deploying%20game-theoretic%20planners%20for%20autonomous%20decision-making%20requires%20a%20specification%20of%20all%20agents%27%20objectives.%20To%20circumvent%20this%20practical%20difficulty%2C%20recent%20work%20develops%20maximum%20likelihood%20techniques%20for%20solving%20inverse%20games%20that%20can%20identify%20unknown%20agent%20objectives%20from%20interaction%20data.%20Unfortunately%2C%20these%20methods%20only%20infer%20point%20estimates%20and%20do%20not%20quantify%20estimator%20uncertainty%3B%20correspondingly%2C%20downstream%20planning%20decisions%20can%20overconfidently%20commit%20to%20unsafe%20actions.%20We%20present%20an%20approximate%20Bayesian%20inference%20approach%20for%20solving%20the%20inverse%20game%20problem%2C%20which%20can%20incorporate%20observation%20data%20from%20multiple%20modalities%20and%20be%20used%20to%20generate%20samples%20from%20the%20Bayesian%20posterior%20over%20the%20hidden%20agent%20objectives%20given%20limited%20sensor%20observations%20in%20real%20time.%20Concretely%2C%20the%20proposed%20Bayesian%20inverse%20game%20framework%20trains%20a%20structured%20variational%20autoencoder%20with%20an%20embedded%20differentiable%20Nash%20game%20solver%20on%20interaction%20datasets%20and%20does%20not%20require%20labels%20of%20agents%27%20true%20objectives.%20Extensive%20experiments%20show%20that%20our%20framework%20successfully%20learns%20prior%20and%20posterior%20distributions%2C%20improves%20inference%20quality%20over%20maximum%20likelihood%20estimation-based%20inverse%20game%20approaches%2C%20and%20enables%20safer%20downstream%20decision-making%20without%20sacrificing%20efficiency.%20When%20trajectory%20information%20is%20uninformative%20or%20unavailable%2C%20multimodal%20inference%20further%20reduces%20uncertainty%20by%20exploiting%20additional%20observation%20modalities.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00696v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Inverse%2520Games%2520with%2520High-Dimensional%2520Multi-Modal%2520Observations%26entry.906535625%3DYash%2520Jain%2520and%2520Xinjie%2520Liu%2520and%2520Lasse%2520Peters%2520and%2520David%2520Fridovich-Keil%2520and%2520Ufuk%2520Topcu%26entry.1292438233%3DMany%2520multi-agent%2520interaction%2520scenarios%2520can%2520be%2520naturally%2520modeled%2520as%2520noncooperative%2520games%252C%2520where%2520each%2520agent%2527s%2520decisions%2520depend%2520on%2520others%2527%2520future%2520actions.%2520However%252C%2520deploying%2520game-theoretic%2520planners%2520for%2520autonomous%2520decision-making%2520requires%2520a%2520specification%2520of%2520all%2520agents%2527%2520objectives.%2520To%2520circumvent%2520this%2520practical%2520difficulty%252C%2520recent%2520work%2520develops%2520maximum%2520likelihood%2520techniques%2520for%2520solving%2520inverse%2520games%2520that%2520can%2520identify%2520unknown%2520agent%2520objectives%2520from%2520interaction%2520data.%2520Unfortunately%252C%2520these%2520methods%2520only%2520infer%2520point%2520estimates%2520and%2520do%2520not%2520quantify%2520estimator%2520uncertainty%253B%2520correspondingly%252C%2520downstream%2520planning%2520decisions%2520can%2520overconfidently%2520commit%2520to%2520unsafe%2520actions.%2520We%2520present%2520an%2520approximate%2520Bayesian%2520inference%2520approach%2520for%2520solving%2520the%2520inverse%2520game%2520problem%252C%2520which%2520can%2520incorporate%2520observation%2520data%2520from%2520multiple%2520modalities%2520and%2520be%2520used%2520to%2520generate%2520samples%2520from%2520the%2520Bayesian%2520posterior%2520over%2520the%2520hidden%2520agent%2520objectives%2520given%2520limited%2520sensor%2520observations%2520in%2520real%2520time.%2520Concretely%252C%2520the%2520proposed%2520Bayesian%2520inverse%2520game%2520framework%2520trains%2520a%2520structured%2520variational%2520autoencoder%2520with%2520an%2520embedded%2520differentiable%2520Nash%2520game%2520solver%2520on%2520interaction%2520datasets%2520and%2520does%2520not%2520require%2520labels%2520of%2520agents%2527%2520true%2520objectives.%2520Extensive%2520experiments%2520show%2520that%2520our%2520framework%2520successfully%2520learns%2520prior%2520and%2520posterior%2520distributions%252C%2520improves%2520inference%2520quality%2520over%2520maximum%2520likelihood%2520estimation-based%2520inverse%2520game%2520approaches%252C%2520and%2520enables%2520safer%2520downstream%2520decision-making%2520without%2520sacrificing%2520efficiency.%2520When%2520trajectory%2520information%2520is%2520uninformative%2520or%2520unavailable%252C%2520multimodal%2520inference%2520further%2520reduces%2520uncertainty%2520by%2520exploiting%2520additional%2520observation%2520modalities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00696v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Inverse%20Games%20with%20High-Dimensional%20Multi-Modal%20Observations&entry.906535625=Yash%20Jain%20and%20Xinjie%20Liu%20and%20Lasse%20Peters%20and%20David%20Fridovich-Keil%20and%20Ufuk%20Topcu&entry.1292438233=Many%20multi-agent%20interaction%20scenarios%20can%20be%20naturally%20modeled%20as%20noncooperative%20games%2C%20where%20each%20agent%27s%20decisions%20depend%20on%20others%27%20future%20actions.%20However%2C%20deploying%20game-theoretic%20planners%20for%20autonomous%20decision-making%20requires%20a%20specification%20of%20all%20agents%27%20objectives.%20To%20circumvent%20this%20practical%20difficulty%2C%20recent%20work%20develops%20maximum%20likelihood%20techniques%20for%20solving%20inverse%20games%20that%20can%20identify%20unknown%20agent%20objectives%20from%20interaction%20data.%20Unfortunately%2C%20these%20methods%20only%20infer%20point%20estimates%20and%20do%20not%20quantify%20estimator%20uncertainty%3B%20correspondingly%2C%20downstream%20planning%20decisions%20can%20overconfidently%20commit%20to%20unsafe%20actions.%20We%20present%20an%20approximate%20Bayesian%20inference%20approach%20for%20solving%20the%20inverse%20game%20problem%2C%20which%20can%20incorporate%20observation%20data%20from%20multiple%20modalities%20and%20be%20used%20to%20generate%20samples%20from%20the%20Bayesian%20posterior%20over%20the%20hidden%20agent%20objectives%20given%20limited%20sensor%20observations%20in%20real%20time.%20Concretely%2C%20the%20proposed%20Bayesian%20inverse%20game%20framework%20trains%20a%20structured%20variational%20autoencoder%20with%20an%20embedded%20differentiable%20Nash%20game%20solver%20on%20interaction%20datasets%20and%20does%20not%20require%20labels%20of%20agents%27%20true%20objectives.%20Extensive%20experiments%20show%20that%20our%20framework%20successfully%20learns%20prior%20and%20posterior%20distributions%2C%20improves%20inference%20quality%20over%20maximum%20likelihood%20estimation-based%20inverse%20game%20approaches%2C%20and%20enables%20safer%20downstream%20decision-making%20without%20sacrificing%20efficiency.%20When%20trajectory%20information%20is%20uninformative%20or%20unavailable%2C%20multimodal%20inference%20further%20reduces%20uncertainty%20by%20exploiting%20additional%20observation%20modalities.&entry.1838667208=http%3A//arxiv.org/abs/2601.00696v1&entry.124074799=Read"},
{"title": "The Curse of Depth in Large Language Models", "author": "Wenfang Sun and Xinyuan Song and Pengxiang Li and Lu Yin and Yefeng Zheng and Shiwei Liu", "abstract": "In this paper, we introduce the Curse of Depth, a concept that highlights, explains, and addresses the recent observation in modern Large Language Models (LLMs) where nearly half of the layers are less effective than expected. We first confirm the wide existence of this phenomenon across the most popular families of LLMs such as Llama, Mistral, DeepSeek, and Qwen. Our analysis, theoretically and empirically, identifies that the underlying reason for the ineffectiveness of deep layers in LLMs is the widespread usage of Pre-Layer Normalization (Pre-LN). While Pre-LN stabilizes the training of Transformer LLMs, its output variance exponentially grows with the model depth, which undesirably causes the derivative of the deep Transformer blocks to be an identity matrix, and therefore barely contributes to the training. To resolve this training pitfall, we propose LayerNorm Scaling (LNS), which scales the variance of output of the layer normalization inversely by the square root of its depth. This simple modification mitigates the output variance explosion of deeper Transformer layers, improving their contribution. Across a wide range of model sizes (130M to 7B), our experiments show that LNS consistently outperforms previous normalization and scaling techniques in enhancing LLM pre-training performance. Moreover, this improvement seamlessly carries over to supervised fine-tuning. All these gains can be attributed to the fact that LayerNorm Scaling enables deeper layers to contribute more effectively during training. Our code is available at \\href{https://github.com/lmsdss/LayerNorm-Scaling}{LayerNorm-Scaling}.", "link": "http://arxiv.org/abs/2502.05795v3", "date": "2026-01-02", "relevancy": 1.975, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5078}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.491}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Curse%20of%20Depth%20in%20Large%20Language%20Models&body=Title%3A%20The%20Curse%20of%20Depth%20in%20Large%20Language%20Models%0AAuthor%3A%20Wenfang%20Sun%20and%20Xinyuan%20Song%20and%20Pengxiang%20Li%20and%20Lu%20Yin%20and%20Yefeng%20Zheng%20and%20Shiwei%20Liu%0AAbstract%3A%20In%20this%20paper%2C%20we%20introduce%20the%20Curse%20of%20Depth%2C%20a%20concept%20that%20highlights%2C%20explains%2C%20and%20addresses%20the%20recent%20observation%20in%20modern%20Large%20Language%20Models%20%28LLMs%29%20where%20nearly%20half%20of%20the%20layers%20are%20less%20effective%20than%20expected.%20We%20first%20confirm%20the%20wide%20existence%20of%20this%20phenomenon%20across%20the%20most%20popular%20families%20of%20LLMs%20such%20as%20Llama%2C%20Mistral%2C%20DeepSeek%2C%20and%20Qwen.%20Our%20analysis%2C%20theoretically%20and%20empirically%2C%20identifies%20that%20the%20underlying%20reason%20for%20the%20ineffectiveness%20of%20deep%20layers%20in%20LLMs%20is%20the%20widespread%20usage%20of%20Pre-Layer%20Normalization%20%28Pre-LN%29.%20While%20Pre-LN%20stabilizes%20the%20training%20of%20Transformer%20LLMs%2C%20its%20output%20variance%20exponentially%20grows%20with%20the%20model%20depth%2C%20which%20undesirably%20causes%20the%20derivative%20of%20the%20deep%20Transformer%20blocks%20to%20be%20an%20identity%20matrix%2C%20and%20therefore%20barely%20contributes%20to%20the%20training.%20To%20resolve%20this%20training%20pitfall%2C%20we%20propose%20LayerNorm%20Scaling%20%28LNS%29%2C%20which%20scales%20the%20variance%20of%20output%20of%20the%20layer%20normalization%20inversely%20by%20the%20square%20root%20of%20its%20depth.%20This%20simple%20modification%20mitigates%20the%20output%20variance%20explosion%20of%20deeper%20Transformer%20layers%2C%20improving%20their%20contribution.%20Across%20a%20wide%20range%20of%20model%20sizes%20%28130M%20to%207B%29%2C%20our%20experiments%20show%20that%20LNS%20consistently%20outperforms%20previous%20normalization%20and%20scaling%20techniques%20in%20enhancing%20LLM%20pre-training%20performance.%20Moreover%2C%20this%20improvement%20seamlessly%20carries%20over%20to%20supervised%20fine-tuning.%20All%20these%20gains%20can%20be%20attributed%20to%20the%20fact%20that%20LayerNorm%20Scaling%20enables%20deeper%20layers%20to%20contribute%20more%20effectively%20during%20training.%20Our%20code%20is%20available%20at%20%5Chref%7Bhttps%3A//github.com/lmsdss/LayerNorm-Scaling%7D%7BLayerNorm-Scaling%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2502.05795v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Curse%2520of%2520Depth%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DWenfang%2520Sun%2520and%2520Xinyuan%2520Song%2520and%2520Pengxiang%2520Li%2520and%2520Lu%2520Yin%2520and%2520Yefeng%2520Zheng%2520and%2520Shiwei%2520Liu%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520introduce%2520the%2520Curse%2520of%2520Depth%252C%2520a%2520concept%2520that%2520highlights%252C%2520explains%252C%2520and%2520addresses%2520the%2520recent%2520observation%2520in%2520modern%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520where%2520nearly%2520half%2520of%2520the%2520layers%2520are%2520less%2520effective%2520than%2520expected.%2520We%2520first%2520confirm%2520the%2520wide%2520existence%2520of%2520this%2520phenomenon%2520across%2520the%2520most%2520popular%2520families%2520of%2520LLMs%2520such%2520as%2520Llama%252C%2520Mistral%252C%2520DeepSeek%252C%2520and%2520Qwen.%2520Our%2520analysis%252C%2520theoretically%2520and%2520empirically%252C%2520identifies%2520that%2520the%2520underlying%2520reason%2520for%2520the%2520ineffectiveness%2520of%2520deep%2520layers%2520in%2520LLMs%2520is%2520the%2520widespread%2520usage%2520of%2520Pre-Layer%2520Normalization%2520%2528Pre-LN%2529.%2520While%2520Pre-LN%2520stabilizes%2520the%2520training%2520of%2520Transformer%2520LLMs%252C%2520its%2520output%2520variance%2520exponentially%2520grows%2520with%2520the%2520model%2520depth%252C%2520which%2520undesirably%2520causes%2520the%2520derivative%2520of%2520the%2520deep%2520Transformer%2520blocks%2520to%2520be%2520an%2520identity%2520matrix%252C%2520and%2520therefore%2520barely%2520contributes%2520to%2520the%2520training.%2520To%2520resolve%2520this%2520training%2520pitfall%252C%2520we%2520propose%2520LayerNorm%2520Scaling%2520%2528LNS%2529%252C%2520which%2520scales%2520the%2520variance%2520of%2520output%2520of%2520the%2520layer%2520normalization%2520inversely%2520by%2520the%2520square%2520root%2520of%2520its%2520depth.%2520This%2520simple%2520modification%2520mitigates%2520the%2520output%2520variance%2520explosion%2520of%2520deeper%2520Transformer%2520layers%252C%2520improving%2520their%2520contribution.%2520Across%2520a%2520wide%2520range%2520of%2520model%2520sizes%2520%2528130M%2520to%25207B%2529%252C%2520our%2520experiments%2520show%2520that%2520LNS%2520consistently%2520outperforms%2520previous%2520normalization%2520and%2520scaling%2520techniques%2520in%2520enhancing%2520LLM%2520pre-training%2520performance.%2520Moreover%252C%2520this%2520improvement%2520seamlessly%2520carries%2520over%2520to%2520supervised%2520fine-tuning.%2520All%2520these%2520gains%2520can%2520be%2520attributed%2520to%2520the%2520fact%2520that%2520LayerNorm%2520Scaling%2520enables%2520deeper%2520layers%2520to%2520contribute%2520more%2520effectively%2520during%2520training.%2520Our%2520code%2520is%2520available%2520at%2520%255Chref%257Bhttps%253A//github.com/lmsdss/LayerNorm-Scaling%257D%257BLayerNorm-Scaling%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05795v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Curse%20of%20Depth%20in%20Large%20Language%20Models&entry.906535625=Wenfang%20Sun%20and%20Xinyuan%20Song%20and%20Pengxiang%20Li%20and%20Lu%20Yin%20and%20Yefeng%20Zheng%20and%20Shiwei%20Liu&entry.1292438233=In%20this%20paper%2C%20we%20introduce%20the%20Curse%20of%20Depth%2C%20a%20concept%20that%20highlights%2C%20explains%2C%20and%20addresses%20the%20recent%20observation%20in%20modern%20Large%20Language%20Models%20%28LLMs%29%20where%20nearly%20half%20of%20the%20layers%20are%20less%20effective%20than%20expected.%20We%20first%20confirm%20the%20wide%20existence%20of%20this%20phenomenon%20across%20the%20most%20popular%20families%20of%20LLMs%20such%20as%20Llama%2C%20Mistral%2C%20DeepSeek%2C%20and%20Qwen.%20Our%20analysis%2C%20theoretically%20and%20empirically%2C%20identifies%20that%20the%20underlying%20reason%20for%20the%20ineffectiveness%20of%20deep%20layers%20in%20LLMs%20is%20the%20widespread%20usage%20of%20Pre-Layer%20Normalization%20%28Pre-LN%29.%20While%20Pre-LN%20stabilizes%20the%20training%20of%20Transformer%20LLMs%2C%20its%20output%20variance%20exponentially%20grows%20with%20the%20model%20depth%2C%20which%20undesirably%20causes%20the%20derivative%20of%20the%20deep%20Transformer%20blocks%20to%20be%20an%20identity%20matrix%2C%20and%20therefore%20barely%20contributes%20to%20the%20training.%20To%20resolve%20this%20training%20pitfall%2C%20we%20propose%20LayerNorm%20Scaling%20%28LNS%29%2C%20which%20scales%20the%20variance%20of%20output%20of%20the%20layer%20normalization%20inversely%20by%20the%20square%20root%20of%20its%20depth.%20This%20simple%20modification%20mitigates%20the%20output%20variance%20explosion%20of%20deeper%20Transformer%20layers%2C%20improving%20their%20contribution.%20Across%20a%20wide%20range%20of%20model%20sizes%20%28130M%20to%207B%29%2C%20our%20experiments%20show%20that%20LNS%20consistently%20outperforms%20previous%20normalization%20and%20scaling%20techniques%20in%20enhancing%20LLM%20pre-training%20performance.%20Moreover%2C%20this%20improvement%20seamlessly%20carries%20over%20to%20supervised%20fine-tuning.%20All%20these%20gains%20can%20be%20attributed%20to%20the%20fact%20that%20LayerNorm%20Scaling%20enables%20deeper%20layers%20to%20contribute%20more%20effectively%20during%20training.%20Our%20code%20is%20available%20at%20%5Chref%7Bhttps%3A//github.com/lmsdss/LayerNorm-Scaling%7D%7BLayerNorm-Scaling%7D.&entry.1838667208=http%3A//arxiv.org/abs/2502.05795v3&entry.124074799=Read"},
{"title": "Digital implementations of deep feature extractors are intrinsically informative", "author": "Max Getter", "abstract": "Rapid information (energy) propagation in deep feature extractors is crucial to balance computational complexity versus expressiveness as a representation of the input. We prove an upper bound for the speed of energy propagation in a unified framework that covers different neural network models, both over Euclidean and non-Euclidean domains. Additional structural information about the signal domain can be used to explicitly determine or improve the rate of decay. To illustrate this, we show global exponential energy decay for a range of 1) feature extractors with discrete-domain input signals, and 2) convolutional neural networks (CNNs) via scattering over locally compact abelian (LCA) groups.", "link": "http://arxiv.org/abs/2502.15004v3", "date": "2026-01-02", "relevancy": 1.9743, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5103}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.504}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Digital%20implementations%20of%20deep%20feature%20extractors%20are%20intrinsically%20informative&body=Title%3A%20Digital%20implementations%20of%20deep%20feature%20extractors%20are%20intrinsically%20informative%0AAuthor%3A%20Max%20Getter%0AAbstract%3A%20Rapid%20information%20%28energy%29%20propagation%20in%20deep%20feature%20extractors%20is%20crucial%20to%20balance%20computational%20complexity%20versus%20expressiveness%20as%20a%20representation%20of%20the%20input.%20We%20prove%20an%20upper%20bound%20for%20the%20speed%20of%20energy%20propagation%20in%20a%20unified%20framework%20that%20covers%20different%20neural%20network%20models%2C%20both%20over%20Euclidean%20and%20non-Euclidean%20domains.%20Additional%20structural%20information%20about%20the%20signal%20domain%20can%20be%20used%20to%20explicitly%20determine%20or%20improve%20the%20rate%20of%20decay.%20To%20illustrate%20this%2C%20we%20show%20global%20exponential%20energy%20decay%20for%20a%20range%20of%201%29%20feature%20extractors%20with%20discrete-domain%20input%20signals%2C%20and%202%29%20convolutional%20neural%20networks%20%28CNNs%29%20via%20scattering%20over%20locally%20compact%20abelian%20%28LCA%29%20groups.%0ALink%3A%20http%3A//arxiv.org/abs/2502.15004v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDigital%2520implementations%2520of%2520deep%2520feature%2520extractors%2520are%2520intrinsically%2520informative%26entry.906535625%3DMax%2520Getter%26entry.1292438233%3DRapid%2520information%2520%2528energy%2529%2520propagation%2520in%2520deep%2520feature%2520extractors%2520is%2520crucial%2520to%2520balance%2520computational%2520complexity%2520versus%2520expressiveness%2520as%2520a%2520representation%2520of%2520the%2520input.%2520We%2520prove%2520an%2520upper%2520bound%2520for%2520the%2520speed%2520of%2520energy%2520propagation%2520in%2520a%2520unified%2520framework%2520that%2520covers%2520different%2520neural%2520network%2520models%252C%2520both%2520over%2520Euclidean%2520and%2520non-Euclidean%2520domains.%2520Additional%2520structural%2520information%2520about%2520the%2520signal%2520domain%2520can%2520be%2520used%2520to%2520explicitly%2520determine%2520or%2520improve%2520the%2520rate%2520of%2520decay.%2520To%2520illustrate%2520this%252C%2520we%2520show%2520global%2520exponential%2520energy%2520decay%2520for%2520a%2520range%2520of%25201%2529%2520feature%2520extractors%2520with%2520discrete-domain%2520input%2520signals%252C%2520and%25202%2529%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520via%2520scattering%2520over%2520locally%2520compact%2520abelian%2520%2528LCA%2529%2520groups.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15004v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Digital%20implementations%20of%20deep%20feature%20extractors%20are%20intrinsically%20informative&entry.906535625=Max%20Getter&entry.1292438233=Rapid%20information%20%28energy%29%20propagation%20in%20deep%20feature%20extractors%20is%20crucial%20to%20balance%20computational%20complexity%20versus%20expressiveness%20as%20a%20representation%20of%20the%20input.%20We%20prove%20an%20upper%20bound%20for%20the%20speed%20of%20energy%20propagation%20in%20a%20unified%20framework%20that%20covers%20different%20neural%20network%20models%2C%20both%20over%20Euclidean%20and%20non-Euclidean%20domains.%20Additional%20structural%20information%20about%20the%20signal%20domain%20can%20be%20used%20to%20explicitly%20determine%20or%20improve%20the%20rate%20of%20decay.%20To%20illustrate%20this%2C%20we%20show%20global%20exponential%20energy%20decay%20for%20a%20range%20of%201%29%20feature%20extractors%20with%20discrete-domain%20input%20signals%2C%20and%202%29%20convolutional%20neural%20networks%20%28CNNs%29%20via%20scattering%20over%20locally%20compact%20abelian%20%28LCA%29%20groups.&entry.1838667208=http%3A//arxiv.org/abs/2502.15004v3&entry.124074799=Read"},
{"title": "PolarGrad: A Class of Matrix-Gradient Optimizers from a Unifying Preconditioning Perspective", "author": "Tim Tsz-Kit Lau and Qi Long and Weijie Su", "abstract": "The ever-growing scale of deep learning models and training data underscores the critical importance of efficient optimization methods. While preconditioned gradient methods such as Adam and AdamW are the de facto optimizers for training neural networks and large language models, structure-aware preconditioned optimizers like Shampoo and Muon, which utilize the matrix structure of gradients, have demonstrated promising evidence of faster convergence. In this paper, we introduce a unifying framework for analyzing \"matrix-aware\" preconditioned methods, which not only sheds light on the effectiveness of Muon and related optimizers but also leads to a class of new structure-aware preconditioned methods. A key contribution of this framework is its precise distinction between preconditioning strategies that treat neural network weights as vectors (addressing curvature anisotropy) versus those that consider their matrix structure (addressing gradient anisotropy). This perspective provides new insights into several empirical phenomena in language model pre-training, including Adam's training instabilities, Muon's accelerated convergence, and the necessity of learning rate warmup for Adam. Building upon this framework, we introduce PolarGrad, a new class of preconditioned optimization methods based on the polar decomposition of matrix-valued gradients. As a special instance, PolarGrad includes Muon with updates scaled by the nuclear norm of the gradients. We provide numerical implementations of these methods, leveraging efficient numerical polar decomposition algorithms for enhanced convergence. Our extensive evaluations across diverse matrix optimization problems and language model pre-training tasks demonstrate that PolarGrad outperforms both Adam and Muon.", "link": "http://arxiv.org/abs/2505.21799v3", "date": "2026-01-02", "relevancy": 1.9701, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4999}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4876}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PolarGrad%3A%20A%20Class%20of%20Matrix-Gradient%20Optimizers%20from%20a%20Unifying%20Preconditioning%20Perspective&body=Title%3A%20PolarGrad%3A%20A%20Class%20of%20Matrix-Gradient%20Optimizers%20from%20a%20Unifying%20Preconditioning%20Perspective%0AAuthor%3A%20Tim%20Tsz-Kit%20Lau%20and%20Qi%20Long%20and%20Weijie%20Su%0AAbstract%3A%20The%20ever-growing%20scale%20of%20deep%20learning%20models%20and%20training%20data%20underscores%20the%20critical%20importance%20of%20efficient%20optimization%20methods.%20While%20preconditioned%20gradient%20methods%20such%20as%20Adam%20and%20AdamW%20are%20the%20de%20facto%20optimizers%20for%20training%20neural%20networks%20and%20large%20language%20models%2C%20structure-aware%20preconditioned%20optimizers%20like%20Shampoo%20and%20Muon%2C%20which%20utilize%20the%20matrix%20structure%20of%20gradients%2C%20have%20demonstrated%20promising%20evidence%20of%20faster%20convergence.%20In%20this%20paper%2C%20we%20introduce%20a%20unifying%20framework%20for%20analyzing%20%22matrix-aware%22%20preconditioned%20methods%2C%20which%20not%20only%20sheds%20light%20on%20the%20effectiveness%20of%20Muon%20and%20related%20optimizers%20but%20also%20leads%20to%20a%20class%20of%20new%20structure-aware%20preconditioned%20methods.%20A%20key%20contribution%20of%20this%20framework%20is%20its%20precise%20distinction%20between%20preconditioning%20strategies%20that%20treat%20neural%20network%20weights%20as%20vectors%20%28addressing%20curvature%20anisotropy%29%20versus%20those%20that%20consider%20their%20matrix%20structure%20%28addressing%20gradient%20anisotropy%29.%20This%20perspective%20provides%20new%20insights%20into%20several%20empirical%20phenomena%20in%20language%20model%20pre-training%2C%20including%20Adam%27s%20training%20instabilities%2C%20Muon%27s%20accelerated%20convergence%2C%20and%20the%20necessity%20of%20learning%20rate%20warmup%20for%20Adam.%20Building%20upon%20this%20framework%2C%20we%20introduce%20PolarGrad%2C%20a%20new%20class%20of%20preconditioned%20optimization%20methods%20based%20on%20the%20polar%20decomposition%20of%20matrix-valued%20gradients.%20As%20a%20special%20instance%2C%20PolarGrad%20includes%20Muon%20with%20updates%20scaled%20by%20the%20nuclear%20norm%20of%20the%20gradients.%20We%20provide%20numerical%20implementations%20of%20these%20methods%2C%20leveraging%20efficient%20numerical%20polar%20decomposition%20algorithms%20for%20enhanced%20convergence.%20Our%20extensive%20evaluations%20across%20diverse%20matrix%20optimization%20problems%20and%20language%20model%20pre-training%20tasks%20demonstrate%20that%20PolarGrad%20outperforms%20both%20Adam%20and%20Muon.%0ALink%3A%20http%3A//arxiv.org/abs/2505.21799v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPolarGrad%253A%2520A%2520Class%2520of%2520Matrix-Gradient%2520Optimizers%2520from%2520a%2520Unifying%2520Preconditioning%2520Perspective%26entry.906535625%3DTim%2520Tsz-Kit%2520Lau%2520and%2520Qi%2520Long%2520and%2520Weijie%2520Su%26entry.1292438233%3DThe%2520ever-growing%2520scale%2520of%2520deep%2520learning%2520models%2520and%2520training%2520data%2520underscores%2520the%2520critical%2520importance%2520of%2520efficient%2520optimization%2520methods.%2520While%2520preconditioned%2520gradient%2520methods%2520such%2520as%2520Adam%2520and%2520AdamW%2520are%2520the%2520de%2520facto%2520optimizers%2520for%2520training%2520neural%2520networks%2520and%2520large%2520language%2520models%252C%2520structure-aware%2520preconditioned%2520optimizers%2520like%2520Shampoo%2520and%2520Muon%252C%2520which%2520utilize%2520the%2520matrix%2520structure%2520of%2520gradients%252C%2520have%2520demonstrated%2520promising%2520evidence%2520of%2520faster%2520convergence.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520unifying%2520framework%2520for%2520analyzing%2520%2522matrix-aware%2522%2520preconditioned%2520methods%252C%2520which%2520not%2520only%2520sheds%2520light%2520on%2520the%2520effectiveness%2520of%2520Muon%2520and%2520related%2520optimizers%2520but%2520also%2520leads%2520to%2520a%2520class%2520of%2520new%2520structure-aware%2520preconditioned%2520methods.%2520A%2520key%2520contribution%2520of%2520this%2520framework%2520is%2520its%2520precise%2520distinction%2520between%2520preconditioning%2520strategies%2520that%2520treat%2520neural%2520network%2520weights%2520as%2520vectors%2520%2528addressing%2520curvature%2520anisotropy%2529%2520versus%2520those%2520that%2520consider%2520their%2520matrix%2520structure%2520%2528addressing%2520gradient%2520anisotropy%2529.%2520This%2520perspective%2520provides%2520new%2520insights%2520into%2520several%2520empirical%2520phenomena%2520in%2520language%2520model%2520pre-training%252C%2520including%2520Adam%2527s%2520training%2520instabilities%252C%2520Muon%2527s%2520accelerated%2520convergence%252C%2520and%2520the%2520necessity%2520of%2520learning%2520rate%2520warmup%2520for%2520Adam.%2520Building%2520upon%2520this%2520framework%252C%2520we%2520introduce%2520PolarGrad%252C%2520a%2520new%2520class%2520of%2520preconditioned%2520optimization%2520methods%2520based%2520on%2520the%2520polar%2520decomposition%2520of%2520matrix-valued%2520gradients.%2520As%2520a%2520special%2520instance%252C%2520PolarGrad%2520includes%2520Muon%2520with%2520updates%2520scaled%2520by%2520the%2520nuclear%2520norm%2520of%2520the%2520gradients.%2520We%2520provide%2520numerical%2520implementations%2520of%2520these%2520methods%252C%2520leveraging%2520efficient%2520numerical%2520polar%2520decomposition%2520algorithms%2520for%2520enhanced%2520convergence.%2520Our%2520extensive%2520evaluations%2520across%2520diverse%2520matrix%2520optimization%2520problems%2520and%2520language%2520model%2520pre-training%2520tasks%2520demonstrate%2520that%2520PolarGrad%2520outperforms%2520both%2520Adam%2520and%2520Muon.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21799v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PolarGrad%3A%20A%20Class%20of%20Matrix-Gradient%20Optimizers%20from%20a%20Unifying%20Preconditioning%20Perspective&entry.906535625=Tim%20Tsz-Kit%20Lau%20and%20Qi%20Long%20and%20Weijie%20Su&entry.1292438233=The%20ever-growing%20scale%20of%20deep%20learning%20models%20and%20training%20data%20underscores%20the%20critical%20importance%20of%20efficient%20optimization%20methods.%20While%20preconditioned%20gradient%20methods%20such%20as%20Adam%20and%20AdamW%20are%20the%20de%20facto%20optimizers%20for%20training%20neural%20networks%20and%20large%20language%20models%2C%20structure-aware%20preconditioned%20optimizers%20like%20Shampoo%20and%20Muon%2C%20which%20utilize%20the%20matrix%20structure%20of%20gradients%2C%20have%20demonstrated%20promising%20evidence%20of%20faster%20convergence.%20In%20this%20paper%2C%20we%20introduce%20a%20unifying%20framework%20for%20analyzing%20%22matrix-aware%22%20preconditioned%20methods%2C%20which%20not%20only%20sheds%20light%20on%20the%20effectiveness%20of%20Muon%20and%20related%20optimizers%20but%20also%20leads%20to%20a%20class%20of%20new%20structure-aware%20preconditioned%20methods.%20A%20key%20contribution%20of%20this%20framework%20is%20its%20precise%20distinction%20between%20preconditioning%20strategies%20that%20treat%20neural%20network%20weights%20as%20vectors%20%28addressing%20curvature%20anisotropy%29%20versus%20those%20that%20consider%20their%20matrix%20structure%20%28addressing%20gradient%20anisotropy%29.%20This%20perspective%20provides%20new%20insights%20into%20several%20empirical%20phenomena%20in%20language%20model%20pre-training%2C%20including%20Adam%27s%20training%20instabilities%2C%20Muon%27s%20accelerated%20convergence%2C%20and%20the%20necessity%20of%20learning%20rate%20warmup%20for%20Adam.%20Building%20upon%20this%20framework%2C%20we%20introduce%20PolarGrad%2C%20a%20new%20class%20of%20preconditioned%20optimization%20methods%20based%20on%20the%20polar%20decomposition%20of%20matrix-valued%20gradients.%20As%20a%20special%20instance%2C%20PolarGrad%20includes%20Muon%20with%20updates%20scaled%20by%20the%20nuclear%20norm%20of%20the%20gradients.%20We%20provide%20numerical%20implementations%20of%20these%20methods%2C%20leveraging%20efficient%20numerical%20polar%20decomposition%20algorithms%20for%20enhanced%20convergence.%20Our%20extensive%20evaluations%20across%20diverse%20matrix%20optimization%20problems%20and%20language%20model%20pre-training%20tasks%20demonstrate%20that%20PolarGrad%20outperforms%20both%20Adam%20and%20Muon.&entry.1838667208=http%3A//arxiv.org/abs/2505.21799v3&entry.124074799=Read"},
{"title": "IRPO: Scaling the Bradley-Terry Model via Reinforcement Learning", "author": "Haonan Song and Qingchen Xie and Huan Zhu and Feng Xiao and Luxi Xing and Fuzhen Li and Liu Kang and Feng Jiang and Zhiyong Zheng and Fan Yang", "abstract": "Generative Reward Models (GRMs) have attracted considerable research interest in reward modeling due to their interpretability, inference-time scalability, and potential for refinement through reinforcement learning (RL). However, widely used pairwise GRMs create a computational bottleneck when integrated with RL algorithms such as Group Relative Policy Optimization (GRPO). This bottleneck arises from two factors: (i) the O(n^2) time complexity of pairwise comparisons required to obtain relative scores, and (ii) the computational overhead of repeated sampling or additional chain-of-thought (CoT) reasoning to improve performance. To address the first factor, we propose Intergroup Relative Preference Optimization (IRPO), a novel RL framework that incorporates the well-established Bradley-Terry model into GRPO. By generating a pointwise score for each response, IRPO enables efficient evaluation of arbitrarily many candidates during RL training while preserving interpretability and fine-grained reward signals. Experimental results demonstrate that IRPO achieves state-of-the-art (SOTA) performance among pointwise GRMs across multiple benchmarks, with performance comparable to that of current leading pairwise GRMs. Furthermore, we show that IRPO significantly outperforms pairwise GRMs in post-training evaluations.", "link": "http://arxiv.org/abs/2601.00677v1", "date": "2026-01-02", "relevancy": 1.9666, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5004}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4854}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IRPO%3A%20Scaling%20the%20Bradley-Terry%20Model%20via%20Reinforcement%20Learning&body=Title%3A%20IRPO%3A%20Scaling%20the%20Bradley-Terry%20Model%20via%20Reinforcement%20Learning%0AAuthor%3A%20Haonan%20Song%20and%20Qingchen%20Xie%20and%20Huan%20Zhu%20and%20Feng%20Xiao%20and%20Luxi%20Xing%20and%20Fuzhen%20Li%20and%20Liu%20Kang%20and%20Feng%20Jiang%20and%20Zhiyong%20Zheng%20and%20Fan%20Yang%0AAbstract%3A%20Generative%20Reward%20Models%20%28GRMs%29%20have%20attracted%20considerable%20research%20interest%20in%20reward%20modeling%20due%20to%20their%20interpretability%2C%20inference-time%20scalability%2C%20and%20potential%20for%20refinement%20through%20reinforcement%20learning%20%28RL%29.%20However%2C%20widely%20used%20pairwise%20GRMs%20create%20a%20computational%20bottleneck%20when%20integrated%20with%20RL%20algorithms%20such%20as%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29.%20This%20bottleneck%20arises%20from%20two%20factors%3A%20%28i%29%20the%20O%28n%5E2%29%20time%20complexity%20of%20pairwise%20comparisons%20required%20to%20obtain%20relative%20scores%2C%20and%20%28ii%29%20the%20computational%20overhead%20of%20repeated%20sampling%20or%20additional%20chain-of-thought%20%28CoT%29%20reasoning%20to%20improve%20performance.%20To%20address%20the%20first%20factor%2C%20we%20propose%20Intergroup%20Relative%20Preference%20Optimization%20%28IRPO%29%2C%20a%20novel%20RL%20framework%20that%20incorporates%20the%20well-established%20Bradley-Terry%20model%20into%20GRPO.%20By%20generating%20a%20pointwise%20score%20for%20each%20response%2C%20IRPO%20enables%20efficient%20evaluation%20of%20arbitrarily%20many%20candidates%20during%20RL%20training%20while%20preserving%20interpretability%20and%20fine-grained%20reward%20signals.%20Experimental%20results%20demonstrate%20that%20IRPO%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20among%20pointwise%20GRMs%20across%20multiple%20benchmarks%2C%20with%20performance%20comparable%20to%20that%20of%20current%20leading%20pairwise%20GRMs.%20Furthermore%2C%20we%20show%20that%20IRPO%20significantly%20outperforms%20pairwise%20GRMs%20in%20post-training%20evaluations.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00677v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIRPO%253A%2520Scaling%2520the%2520Bradley-Terry%2520Model%2520via%2520Reinforcement%2520Learning%26entry.906535625%3DHaonan%2520Song%2520and%2520Qingchen%2520Xie%2520and%2520Huan%2520Zhu%2520and%2520Feng%2520Xiao%2520and%2520Luxi%2520Xing%2520and%2520Fuzhen%2520Li%2520and%2520Liu%2520Kang%2520and%2520Feng%2520Jiang%2520and%2520Zhiyong%2520Zheng%2520and%2520Fan%2520Yang%26entry.1292438233%3DGenerative%2520Reward%2520Models%2520%2528GRMs%2529%2520have%2520attracted%2520considerable%2520research%2520interest%2520in%2520reward%2520modeling%2520due%2520to%2520their%2520interpretability%252C%2520inference-time%2520scalability%252C%2520and%2520potential%2520for%2520refinement%2520through%2520reinforcement%2520learning%2520%2528RL%2529.%2520However%252C%2520widely%2520used%2520pairwise%2520GRMs%2520create%2520a%2520computational%2520bottleneck%2520when%2520integrated%2520with%2520RL%2520algorithms%2520such%2520as%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529.%2520This%2520bottleneck%2520arises%2520from%2520two%2520factors%253A%2520%2528i%2529%2520the%2520O%2528n%255E2%2529%2520time%2520complexity%2520of%2520pairwise%2520comparisons%2520required%2520to%2520obtain%2520relative%2520scores%252C%2520and%2520%2528ii%2529%2520the%2520computational%2520overhead%2520of%2520repeated%2520sampling%2520or%2520additional%2520chain-of-thought%2520%2528CoT%2529%2520reasoning%2520to%2520improve%2520performance.%2520To%2520address%2520the%2520first%2520factor%252C%2520we%2520propose%2520Intergroup%2520Relative%2520Preference%2520Optimization%2520%2528IRPO%2529%252C%2520a%2520novel%2520RL%2520framework%2520that%2520incorporates%2520the%2520well-established%2520Bradley-Terry%2520model%2520into%2520GRPO.%2520By%2520generating%2520a%2520pointwise%2520score%2520for%2520each%2520response%252C%2520IRPO%2520enables%2520efficient%2520evaluation%2520of%2520arbitrarily%2520many%2520candidates%2520during%2520RL%2520training%2520while%2520preserving%2520interpretability%2520and%2520fine-grained%2520reward%2520signals.%2520Experimental%2520results%2520demonstrate%2520that%2520IRPO%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520among%2520pointwise%2520GRMs%2520across%2520multiple%2520benchmarks%252C%2520with%2520performance%2520comparable%2520to%2520that%2520of%2520current%2520leading%2520pairwise%2520GRMs.%2520Furthermore%252C%2520we%2520show%2520that%2520IRPO%2520significantly%2520outperforms%2520pairwise%2520GRMs%2520in%2520post-training%2520evaluations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00677v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IRPO%3A%20Scaling%20the%20Bradley-Terry%20Model%20via%20Reinforcement%20Learning&entry.906535625=Haonan%20Song%20and%20Qingchen%20Xie%20and%20Huan%20Zhu%20and%20Feng%20Xiao%20and%20Luxi%20Xing%20and%20Fuzhen%20Li%20and%20Liu%20Kang%20and%20Feng%20Jiang%20and%20Zhiyong%20Zheng%20and%20Fan%20Yang&entry.1292438233=Generative%20Reward%20Models%20%28GRMs%29%20have%20attracted%20considerable%20research%20interest%20in%20reward%20modeling%20due%20to%20their%20interpretability%2C%20inference-time%20scalability%2C%20and%20potential%20for%20refinement%20through%20reinforcement%20learning%20%28RL%29.%20However%2C%20widely%20used%20pairwise%20GRMs%20create%20a%20computational%20bottleneck%20when%20integrated%20with%20RL%20algorithms%20such%20as%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29.%20This%20bottleneck%20arises%20from%20two%20factors%3A%20%28i%29%20the%20O%28n%5E2%29%20time%20complexity%20of%20pairwise%20comparisons%20required%20to%20obtain%20relative%20scores%2C%20and%20%28ii%29%20the%20computational%20overhead%20of%20repeated%20sampling%20or%20additional%20chain-of-thought%20%28CoT%29%20reasoning%20to%20improve%20performance.%20To%20address%20the%20first%20factor%2C%20we%20propose%20Intergroup%20Relative%20Preference%20Optimization%20%28IRPO%29%2C%20a%20novel%20RL%20framework%20that%20incorporates%20the%20well-established%20Bradley-Terry%20model%20into%20GRPO.%20By%20generating%20a%20pointwise%20score%20for%20each%20response%2C%20IRPO%20enables%20efficient%20evaluation%20of%20arbitrarily%20many%20candidates%20during%20RL%20training%20while%20preserving%20interpretability%20and%20fine-grained%20reward%20signals.%20Experimental%20results%20demonstrate%20that%20IRPO%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20among%20pointwise%20GRMs%20across%20multiple%20benchmarks%2C%20with%20performance%20comparable%20to%20that%20of%20current%20leading%20pairwise%20GRMs.%20Furthermore%2C%20we%20show%20that%20IRPO%20significantly%20outperforms%20pairwise%20GRMs%20in%20post-training%20evaluations.&entry.1838667208=http%3A//arxiv.org/abs/2601.00677v1&entry.124074799=Read"},
{"title": "An Agentic Framework for Neuro-Symbolic Programming", "author": "Aliakbar Nafar and Chetan Chigurupati and Danial Kamali and Hamid Karimian and Parisa Kordjamshidi", "abstract": "Integrating symbolic constraints into deep learning models could make them more robust, interpretable, and data-efficient. Still, it remains a time-consuming and challenging task. Existing frameworks like DomiKnowS help this integration by providing a high-level declarative programming interface, but they still assume the user is proficient with the library's specific syntax. We propose AgenticDomiKnowS (ADS) to eliminate this dependency. ADS translates free-form task descriptions into a complete DomiKnowS program using an agentic workflow that creates and tests each DomiKnowS component separately. The workflow supports optional human-in-the-loop intervention, enabling users familiar with DomiKnowS to refine intermediate outputs. We show how ADS enables experienced DomiKnowS users and non-users to rapidly construct neuro-symbolic programs, reducing development time from hours to 10-15 minutes.", "link": "http://arxiv.org/abs/2601.00743v1", "date": "2026-01-02", "relevancy": 1.9435, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5218}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4885}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Agentic%20Framework%20for%20Neuro-Symbolic%20Programming&body=Title%3A%20An%20Agentic%20Framework%20for%20Neuro-Symbolic%20Programming%0AAuthor%3A%20Aliakbar%20Nafar%20and%20Chetan%20Chigurupati%20and%20Danial%20Kamali%20and%20Hamid%20Karimian%20and%20Parisa%20Kordjamshidi%0AAbstract%3A%20Integrating%20symbolic%20constraints%20into%20deep%20learning%20models%20could%20make%20them%20more%20robust%2C%20interpretable%2C%20and%20data-efficient.%20Still%2C%20it%20remains%20a%20time-consuming%20and%20challenging%20task.%20Existing%20frameworks%20like%20DomiKnowS%20help%20this%20integration%20by%20providing%20a%20high-level%20declarative%20programming%20interface%2C%20but%20they%20still%20assume%20the%20user%20is%20proficient%20with%20the%20library%27s%20specific%20syntax.%20We%20propose%20AgenticDomiKnowS%20%28ADS%29%20to%20eliminate%20this%20dependency.%20ADS%20translates%20free-form%20task%20descriptions%20into%20a%20complete%20DomiKnowS%20program%20using%20an%20agentic%20workflow%20that%20creates%20and%20tests%20each%20DomiKnowS%20component%20separately.%20The%20workflow%20supports%20optional%20human-in-the-loop%20intervention%2C%20enabling%20users%20familiar%20with%20DomiKnowS%20to%20refine%20intermediate%20outputs.%20We%20show%20how%20ADS%20enables%20experienced%20DomiKnowS%20users%20and%20non-users%20to%20rapidly%20construct%20neuro-symbolic%20programs%2C%20reducing%20development%20time%20from%20hours%20to%2010-15%20minutes.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00743v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Agentic%2520Framework%2520for%2520Neuro-Symbolic%2520Programming%26entry.906535625%3DAliakbar%2520Nafar%2520and%2520Chetan%2520Chigurupati%2520and%2520Danial%2520Kamali%2520and%2520Hamid%2520Karimian%2520and%2520Parisa%2520Kordjamshidi%26entry.1292438233%3DIntegrating%2520symbolic%2520constraints%2520into%2520deep%2520learning%2520models%2520could%2520make%2520them%2520more%2520robust%252C%2520interpretable%252C%2520and%2520data-efficient.%2520Still%252C%2520it%2520remains%2520a%2520time-consuming%2520and%2520challenging%2520task.%2520Existing%2520frameworks%2520like%2520DomiKnowS%2520help%2520this%2520integration%2520by%2520providing%2520a%2520high-level%2520declarative%2520programming%2520interface%252C%2520but%2520they%2520still%2520assume%2520the%2520user%2520is%2520proficient%2520with%2520the%2520library%2527s%2520specific%2520syntax.%2520We%2520propose%2520AgenticDomiKnowS%2520%2528ADS%2529%2520to%2520eliminate%2520this%2520dependency.%2520ADS%2520translates%2520free-form%2520task%2520descriptions%2520into%2520a%2520complete%2520DomiKnowS%2520program%2520using%2520an%2520agentic%2520workflow%2520that%2520creates%2520and%2520tests%2520each%2520DomiKnowS%2520component%2520separately.%2520The%2520workflow%2520supports%2520optional%2520human-in-the-loop%2520intervention%252C%2520enabling%2520users%2520familiar%2520with%2520DomiKnowS%2520to%2520refine%2520intermediate%2520outputs.%2520We%2520show%2520how%2520ADS%2520enables%2520experienced%2520DomiKnowS%2520users%2520and%2520non-users%2520to%2520rapidly%2520construct%2520neuro-symbolic%2520programs%252C%2520reducing%2520development%2520time%2520from%2520hours%2520to%252010-15%2520minutes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00743v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Agentic%20Framework%20for%20Neuro-Symbolic%20Programming&entry.906535625=Aliakbar%20Nafar%20and%20Chetan%20Chigurupati%20and%20Danial%20Kamali%20and%20Hamid%20Karimian%20and%20Parisa%20Kordjamshidi&entry.1292438233=Integrating%20symbolic%20constraints%20into%20deep%20learning%20models%20could%20make%20them%20more%20robust%2C%20interpretable%2C%20and%20data-efficient.%20Still%2C%20it%20remains%20a%20time-consuming%20and%20challenging%20task.%20Existing%20frameworks%20like%20DomiKnowS%20help%20this%20integration%20by%20providing%20a%20high-level%20declarative%20programming%20interface%2C%20but%20they%20still%20assume%20the%20user%20is%20proficient%20with%20the%20library%27s%20specific%20syntax.%20We%20propose%20AgenticDomiKnowS%20%28ADS%29%20to%20eliminate%20this%20dependency.%20ADS%20translates%20free-form%20task%20descriptions%20into%20a%20complete%20DomiKnowS%20program%20using%20an%20agentic%20workflow%20that%20creates%20and%20tests%20each%20DomiKnowS%20component%20separately.%20The%20workflow%20supports%20optional%20human-in-the-loop%20intervention%2C%20enabling%20users%20familiar%20with%20DomiKnowS%20to%20refine%20intermediate%20outputs.%20We%20show%20how%20ADS%20enables%20experienced%20DomiKnowS%20users%20and%20non-users%20to%20rapidly%20construct%20neuro-symbolic%20programs%2C%20reducing%20development%20time%20from%20hours%20to%2010-15%20minutes.&entry.1838667208=http%3A//arxiv.org/abs/2601.00743v1&entry.124074799=Read"},
{"title": "HyperPriv-EPN: Hypergraph Learning with Privileged Knowledge for Ependymoma Prognosis", "author": "Shuren Gabriel Yu and Sikang Ren and Yongji Tian", "abstract": "Preoperative prognosis of Ependymoma is critical for treatment planning but challenging due to the lack of semantic insights in MRI compared to post-operative surgical reports. Existing multimodal methods fail to leverage this privileged text data when it is unavailable during inference. To bridge this gap, we propose HyperPriv-EPN, a hypergraph-based Learning Using Privileged Information (LUPI) framework. We introduce a Severed Graph Strategy, utilizing a shared encoder to process both a Teacher graph (enriched with privileged post-surgery information) and a Student graph (restricted to pre-operation data). Through dual-stream distillation, the Student learns to hallucinate semantic community structures from visual features alone. Validated on a multi-center cohort of 311 patients, HyperPriv-EPN achieves state-of-the-art diagnostic accuracy and survival stratification. This effectively transfers expert knowledge to the preoperative setting, unlocking the value of historical post-operative data to guide the diagnosis of new patients without requiring text at inference.", "link": "http://arxiv.org/abs/2601.00626v1", "date": "2026-01-02", "relevancy": 1.9215, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4899}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.487}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.47}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyperPriv-EPN%3A%20Hypergraph%20Learning%20with%20Privileged%20Knowledge%20for%20Ependymoma%20Prognosis&body=Title%3A%20HyperPriv-EPN%3A%20Hypergraph%20Learning%20with%20Privileged%20Knowledge%20for%20Ependymoma%20Prognosis%0AAuthor%3A%20Shuren%20Gabriel%20Yu%20and%20Sikang%20Ren%20and%20Yongji%20Tian%0AAbstract%3A%20Preoperative%20prognosis%20of%20Ependymoma%20is%20critical%20for%20treatment%20planning%20but%20challenging%20due%20to%20the%20lack%20of%20semantic%20insights%20in%20MRI%20compared%20to%20post-operative%20surgical%20reports.%20Existing%20multimodal%20methods%20fail%20to%20leverage%20this%20privileged%20text%20data%20when%20it%20is%20unavailable%20during%20inference.%20To%20bridge%20this%20gap%2C%20we%20propose%20HyperPriv-EPN%2C%20a%20hypergraph-based%20Learning%20Using%20Privileged%20Information%20%28LUPI%29%20framework.%20We%20introduce%20a%20Severed%20Graph%20Strategy%2C%20utilizing%20a%20shared%20encoder%20to%20process%20both%20a%20Teacher%20graph%20%28enriched%20with%20privileged%20post-surgery%20information%29%20and%20a%20Student%20graph%20%28restricted%20to%20pre-operation%20data%29.%20Through%20dual-stream%20distillation%2C%20the%20Student%20learns%20to%20hallucinate%20semantic%20community%20structures%20from%20visual%20features%20alone.%20Validated%20on%20a%20multi-center%20cohort%20of%20311%20patients%2C%20HyperPriv-EPN%20achieves%20state-of-the-art%20diagnostic%20accuracy%20and%20survival%20stratification.%20This%20effectively%20transfers%20expert%20knowledge%20to%20the%20preoperative%20setting%2C%20unlocking%20the%20value%20of%20historical%20post-operative%20data%20to%20guide%20the%20diagnosis%20of%20new%20patients%20without%20requiring%20text%20at%20inference.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00626v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperPriv-EPN%253A%2520Hypergraph%2520Learning%2520with%2520Privileged%2520Knowledge%2520for%2520Ependymoma%2520Prognosis%26entry.906535625%3DShuren%2520Gabriel%2520Yu%2520and%2520Sikang%2520Ren%2520and%2520Yongji%2520Tian%26entry.1292438233%3DPreoperative%2520prognosis%2520of%2520Ependymoma%2520is%2520critical%2520for%2520treatment%2520planning%2520but%2520challenging%2520due%2520to%2520the%2520lack%2520of%2520semantic%2520insights%2520in%2520MRI%2520compared%2520to%2520post-operative%2520surgical%2520reports.%2520Existing%2520multimodal%2520methods%2520fail%2520to%2520leverage%2520this%2520privileged%2520text%2520data%2520when%2520it%2520is%2520unavailable%2520during%2520inference.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520HyperPriv-EPN%252C%2520a%2520hypergraph-based%2520Learning%2520Using%2520Privileged%2520Information%2520%2528LUPI%2529%2520framework.%2520We%2520introduce%2520a%2520Severed%2520Graph%2520Strategy%252C%2520utilizing%2520a%2520shared%2520encoder%2520to%2520process%2520both%2520a%2520Teacher%2520graph%2520%2528enriched%2520with%2520privileged%2520post-surgery%2520information%2529%2520and%2520a%2520Student%2520graph%2520%2528restricted%2520to%2520pre-operation%2520data%2529.%2520Through%2520dual-stream%2520distillation%252C%2520the%2520Student%2520learns%2520to%2520hallucinate%2520semantic%2520community%2520structures%2520from%2520visual%2520features%2520alone.%2520Validated%2520on%2520a%2520multi-center%2520cohort%2520of%2520311%2520patients%252C%2520HyperPriv-EPN%2520achieves%2520state-of-the-art%2520diagnostic%2520accuracy%2520and%2520survival%2520stratification.%2520This%2520effectively%2520transfers%2520expert%2520knowledge%2520to%2520the%2520preoperative%2520setting%252C%2520unlocking%2520the%2520value%2520of%2520historical%2520post-operative%2520data%2520to%2520guide%2520the%2520diagnosis%2520of%2520new%2520patients%2520without%2520requiring%2520text%2520at%2520inference.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00626v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyperPriv-EPN%3A%20Hypergraph%20Learning%20with%20Privileged%20Knowledge%20for%20Ependymoma%20Prognosis&entry.906535625=Shuren%20Gabriel%20Yu%20and%20Sikang%20Ren%20and%20Yongji%20Tian&entry.1292438233=Preoperative%20prognosis%20of%20Ependymoma%20is%20critical%20for%20treatment%20planning%20but%20challenging%20due%20to%20the%20lack%20of%20semantic%20insights%20in%20MRI%20compared%20to%20post-operative%20surgical%20reports.%20Existing%20multimodal%20methods%20fail%20to%20leverage%20this%20privileged%20text%20data%20when%20it%20is%20unavailable%20during%20inference.%20To%20bridge%20this%20gap%2C%20we%20propose%20HyperPriv-EPN%2C%20a%20hypergraph-based%20Learning%20Using%20Privileged%20Information%20%28LUPI%29%20framework.%20We%20introduce%20a%20Severed%20Graph%20Strategy%2C%20utilizing%20a%20shared%20encoder%20to%20process%20both%20a%20Teacher%20graph%20%28enriched%20with%20privileged%20post-surgery%20information%29%20and%20a%20Student%20graph%20%28restricted%20to%20pre-operation%20data%29.%20Through%20dual-stream%20distillation%2C%20the%20Student%20learns%20to%20hallucinate%20semantic%20community%20structures%20from%20visual%20features%20alone.%20Validated%20on%20a%20multi-center%20cohort%20of%20311%20patients%2C%20HyperPriv-EPN%20achieves%20state-of-the-art%20diagnostic%20accuracy%20and%20survival%20stratification.%20This%20effectively%20transfers%20expert%20knowledge%20to%20the%20preoperative%20setting%2C%20unlocking%20the%20value%20of%20historical%20post-operative%20data%20to%20guide%20the%20diagnosis%20of%20new%20patients%20without%20requiring%20text%20at%20inference.&entry.1838667208=http%3A//arxiv.org/abs/2601.00626v1&entry.124074799=Read"},
{"title": "Interpretability-Guided Bi-objective Optimization: Aligning Accuracy and Explainability", "author": "Kasra Fouladi and Hamta Rahmani", "abstract": "This paper introduces Interpretability-Guided Bi-objective Optimization (IGBO), a framework that trains interpretable models by incorporating structured domain knowledge via a bi-objective formulation. IGBO encodes feature importance hierarchies as a Directed Acyclic Graph (DAG) and uses Temporal Integrated Gradients (TIG) to measure feature importance. To address the Out-of-Distribution (OOD) problem in TIG computation, we propose an Optimal Path Oracle that learns data-manifold-aware integration paths. Theoretical analysis proves convergence properties and robustness to mini-batch noise, while empirical results on time-series data demonstrate IGBO's effectiveness in enforcing DAG constraints with minimal accuracy loss, outperforming standard regularization baselines.", "link": "http://arxiv.org/abs/2601.00655v1", "date": "2026-01-02", "relevancy": 1.8859, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4753}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4749}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretability-Guided%20Bi-objective%20Optimization%3A%20Aligning%20Accuracy%20and%20Explainability&body=Title%3A%20Interpretability-Guided%20Bi-objective%20Optimization%3A%20Aligning%20Accuracy%20and%20Explainability%0AAuthor%3A%20Kasra%20Fouladi%20and%20Hamta%20Rahmani%0AAbstract%3A%20This%20paper%20introduces%20Interpretability-Guided%20Bi-objective%20Optimization%20%28IGBO%29%2C%20a%20framework%20that%20trains%20interpretable%20models%20by%20incorporating%20structured%20domain%20knowledge%20via%20a%20bi-objective%20formulation.%20IGBO%20encodes%20feature%20importance%20hierarchies%20as%20a%20Directed%20Acyclic%20Graph%20%28DAG%29%20and%20uses%20Temporal%20Integrated%20Gradients%20%28TIG%29%20to%20measure%20feature%20importance.%20To%20address%20the%20Out-of-Distribution%20%28OOD%29%20problem%20in%20TIG%20computation%2C%20we%20propose%20an%20Optimal%20Path%20Oracle%20that%20learns%20data-manifold-aware%20integration%20paths.%20Theoretical%20analysis%20proves%20convergence%20properties%20and%20robustness%20to%20mini-batch%20noise%2C%20while%20empirical%20results%20on%20time-series%20data%20demonstrate%20IGBO%27s%20effectiveness%20in%20enforcing%20DAG%20constraints%20with%20minimal%20accuracy%20loss%2C%20outperforming%20standard%20regularization%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00655v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretability-Guided%2520Bi-objective%2520Optimization%253A%2520Aligning%2520Accuracy%2520and%2520Explainability%26entry.906535625%3DKasra%2520Fouladi%2520and%2520Hamta%2520Rahmani%26entry.1292438233%3DThis%2520paper%2520introduces%2520Interpretability-Guided%2520Bi-objective%2520Optimization%2520%2528IGBO%2529%252C%2520a%2520framework%2520that%2520trains%2520interpretable%2520models%2520by%2520incorporating%2520structured%2520domain%2520knowledge%2520via%2520a%2520bi-objective%2520formulation.%2520IGBO%2520encodes%2520feature%2520importance%2520hierarchies%2520as%2520a%2520Directed%2520Acyclic%2520Graph%2520%2528DAG%2529%2520and%2520uses%2520Temporal%2520Integrated%2520Gradients%2520%2528TIG%2529%2520to%2520measure%2520feature%2520importance.%2520To%2520address%2520the%2520Out-of-Distribution%2520%2528OOD%2529%2520problem%2520in%2520TIG%2520computation%252C%2520we%2520propose%2520an%2520Optimal%2520Path%2520Oracle%2520that%2520learns%2520data-manifold-aware%2520integration%2520paths.%2520Theoretical%2520analysis%2520proves%2520convergence%2520properties%2520and%2520robustness%2520to%2520mini-batch%2520noise%252C%2520while%2520empirical%2520results%2520on%2520time-series%2520data%2520demonstrate%2520IGBO%2527s%2520effectiveness%2520in%2520enforcing%2520DAG%2520constraints%2520with%2520minimal%2520accuracy%2520loss%252C%2520outperforming%2520standard%2520regularization%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00655v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretability-Guided%20Bi-objective%20Optimization%3A%20Aligning%20Accuracy%20and%20Explainability&entry.906535625=Kasra%20Fouladi%20and%20Hamta%20Rahmani&entry.1292438233=This%20paper%20introduces%20Interpretability-Guided%20Bi-objective%20Optimization%20%28IGBO%29%2C%20a%20framework%20that%20trains%20interpretable%20models%20by%20incorporating%20structured%20domain%20knowledge%20via%20a%20bi-objective%20formulation.%20IGBO%20encodes%20feature%20importance%20hierarchies%20as%20a%20Directed%20Acyclic%20Graph%20%28DAG%29%20and%20uses%20Temporal%20Integrated%20Gradients%20%28TIG%29%20to%20measure%20feature%20importance.%20To%20address%20the%20Out-of-Distribution%20%28OOD%29%20problem%20in%20TIG%20computation%2C%20we%20propose%20an%20Optimal%20Path%20Oracle%20that%20learns%20data-manifold-aware%20integration%20paths.%20Theoretical%20analysis%20proves%20convergence%20properties%20and%20robustness%20to%20mini-batch%20noise%2C%20while%20empirical%20results%20on%20time-series%20data%20demonstrate%20IGBO%27s%20effectiveness%20in%20enforcing%20DAG%20constraints%20with%20minimal%20accuracy%20loss%2C%20outperforming%20standard%20regularization%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2601.00655v1&entry.124074799=Read"},
{"title": "Designing an Optimal Sensor Network via Minimizing Information Loss", "author": "Daniel Waxman and Fernando Llorente and Katia Lamer and Petar M. Djuri\u0107", "abstract": "Optimal experimental design is a classic topic in statistics, with many well-studied problems, applications, and solutions. The design problem we study is the placement of sensors to monitor spatiotemporal processes, explicitly accounting for the temporal dimension in our modeling and optimization. We observe that recent advancements in computational sciences often yield large datasets based on physics-based simulations, which are rarely leveraged in experimental design. We introduce a novel model-based sensor placement criterion, along with a highly-efficient optimization algorithm, which integrates physics-based simulations and Bayesian experimental design principles to identify sensor networks that \"minimize information loss\" from simulated data. Our technique relies on sparse variational inference and (separable) Gauss-Markov priors, and thus may adapt many techniques from Bayesian experimental design. We validate our method through a case study monitoring air temperature in Phoenix, Arizona, using state-of-the-art physics-based simulations. Our results show our framework to be superior to random or quasi-random sampling, particularly with a limited number of sensors. We conclude by discussing practical considerations and implications of our framework, including more complex modeling tools and real-world deployments.", "link": "http://arxiv.org/abs/2512.05940v2", "date": "2026-01-02", "relevancy": 1.8844, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5104}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4634}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Designing%20an%20Optimal%20Sensor%20Network%20via%20Minimizing%20Information%20Loss&body=Title%3A%20Designing%20an%20Optimal%20Sensor%20Network%20via%20Minimizing%20Information%20Loss%0AAuthor%3A%20Daniel%20Waxman%20and%20Fernando%20Llorente%20and%20Katia%20Lamer%20and%20Petar%20M.%20Djuri%C4%87%0AAbstract%3A%20Optimal%20experimental%20design%20is%20a%20classic%20topic%20in%20statistics%2C%20with%20many%20well-studied%20problems%2C%20applications%2C%20and%20solutions.%20The%20design%20problem%20we%20study%20is%20the%20placement%20of%20sensors%20to%20monitor%20spatiotemporal%20processes%2C%20explicitly%20accounting%20for%20the%20temporal%20dimension%20in%20our%20modeling%20and%20optimization.%20We%20observe%20that%20recent%20advancements%20in%20computational%20sciences%20often%20yield%20large%20datasets%20based%20on%20physics-based%20simulations%2C%20which%20are%20rarely%20leveraged%20in%20experimental%20design.%20We%20introduce%20a%20novel%20model-based%20sensor%20placement%20criterion%2C%20along%20with%20a%20highly-efficient%20optimization%20algorithm%2C%20which%20integrates%20physics-based%20simulations%20and%20Bayesian%20experimental%20design%20principles%20to%20identify%20sensor%20networks%20that%20%22minimize%20information%20loss%22%20from%20simulated%20data.%20Our%20technique%20relies%20on%20sparse%20variational%20inference%20and%20%28separable%29%20Gauss-Markov%20priors%2C%20and%20thus%20may%20adapt%20many%20techniques%20from%20Bayesian%20experimental%20design.%20We%20validate%20our%20method%20through%20a%20case%20study%20monitoring%20air%20temperature%20in%20Phoenix%2C%20Arizona%2C%20using%20state-of-the-art%20physics-based%20simulations.%20Our%20results%20show%20our%20framework%20to%20be%20superior%20to%20random%20or%20quasi-random%20sampling%2C%20particularly%20with%20a%20limited%20number%20of%20sensors.%20We%20conclude%20by%20discussing%20practical%20considerations%20and%20implications%20of%20our%20framework%2C%20including%20more%20complex%20modeling%20tools%20and%20real-world%20deployments.%0ALink%3A%20http%3A//arxiv.org/abs/2512.05940v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesigning%2520an%2520Optimal%2520Sensor%2520Network%2520via%2520Minimizing%2520Information%2520Loss%26entry.906535625%3DDaniel%2520Waxman%2520and%2520Fernando%2520Llorente%2520and%2520Katia%2520Lamer%2520and%2520Petar%2520M.%2520Djuri%25C4%2587%26entry.1292438233%3DOptimal%2520experimental%2520design%2520is%2520a%2520classic%2520topic%2520in%2520statistics%252C%2520with%2520many%2520well-studied%2520problems%252C%2520applications%252C%2520and%2520solutions.%2520The%2520design%2520problem%2520we%2520study%2520is%2520the%2520placement%2520of%2520sensors%2520to%2520monitor%2520spatiotemporal%2520processes%252C%2520explicitly%2520accounting%2520for%2520the%2520temporal%2520dimension%2520in%2520our%2520modeling%2520and%2520optimization.%2520We%2520observe%2520that%2520recent%2520advancements%2520in%2520computational%2520sciences%2520often%2520yield%2520large%2520datasets%2520based%2520on%2520physics-based%2520simulations%252C%2520which%2520are%2520rarely%2520leveraged%2520in%2520experimental%2520design.%2520We%2520introduce%2520a%2520novel%2520model-based%2520sensor%2520placement%2520criterion%252C%2520along%2520with%2520a%2520highly-efficient%2520optimization%2520algorithm%252C%2520which%2520integrates%2520physics-based%2520simulations%2520and%2520Bayesian%2520experimental%2520design%2520principles%2520to%2520identify%2520sensor%2520networks%2520that%2520%2522minimize%2520information%2520loss%2522%2520from%2520simulated%2520data.%2520Our%2520technique%2520relies%2520on%2520sparse%2520variational%2520inference%2520and%2520%2528separable%2529%2520Gauss-Markov%2520priors%252C%2520and%2520thus%2520may%2520adapt%2520many%2520techniques%2520from%2520Bayesian%2520experimental%2520design.%2520We%2520validate%2520our%2520method%2520through%2520a%2520case%2520study%2520monitoring%2520air%2520temperature%2520in%2520Phoenix%252C%2520Arizona%252C%2520using%2520state-of-the-art%2520physics-based%2520simulations.%2520Our%2520results%2520show%2520our%2520framework%2520to%2520be%2520superior%2520to%2520random%2520or%2520quasi-random%2520sampling%252C%2520particularly%2520with%2520a%2520limited%2520number%2520of%2520sensors.%2520We%2520conclude%2520by%2520discussing%2520practical%2520considerations%2520and%2520implications%2520of%2520our%2520framework%252C%2520including%2520more%2520complex%2520modeling%2520tools%2520and%2520real-world%2520deployments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05940v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Designing%20an%20Optimal%20Sensor%20Network%20via%20Minimizing%20Information%20Loss&entry.906535625=Daniel%20Waxman%20and%20Fernando%20Llorente%20and%20Katia%20Lamer%20and%20Petar%20M.%20Djuri%C4%87&entry.1292438233=Optimal%20experimental%20design%20is%20a%20classic%20topic%20in%20statistics%2C%20with%20many%20well-studied%20problems%2C%20applications%2C%20and%20solutions.%20The%20design%20problem%20we%20study%20is%20the%20placement%20of%20sensors%20to%20monitor%20spatiotemporal%20processes%2C%20explicitly%20accounting%20for%20the%20temporal%20dimension%20in%20our%20modeling%20and%20optimization.%20We%20observe%20that%20recent%20advancements%20in%20computational%20sciences%20often%20yield%20large%20datasets%20based%20on%20physics-based%20simulations%2C%20which%20are%20rarely%20leveraged%20in%20experimental%20design.%20We%20introduce%20a%20novel%20model-based%20sensor%20placement%20criterion%2C%20along%20with%20a%20highly-efficient%20optimization%20algorithm%2C%20which%20integrates%20physics-based%20simulations%20and%20Bayesian%20experimental%20design%20principles%20to%20identify%20sensor%20networks%20that%20%22minimize%20information%20loss%22%20from%20simulated%20data.%20Our%20technique%20relies%20on%20sparse%20variational%20inference%20and%20%28separable%29%20Gauss-Markov%20priors%2C%20and%20thus%20may%20adapt%20many%20techniques%20from%20Bayesian%20experimental%20design.%20We%20validate%20our%20method%20through%20a%20case%20study%20monitoring%20air%20temperature%20in%20Phoenix%2C%20Arizona%2C%20using%20state-of-the-art%20physics-based%20simulations.%20Our%20results%20show%20our%20framework%20to%20be%20superior%20to%20random%20or%20quasi-random%20sampling%2C%20particularly%20with%20a%20limited%20number%20of%20sensors.%20We%20conclude%20by%20discussing%20practical%20considerations%20and%20implications%20of%20our%20framework%2C%20including%20more%20complex%20modeling%20tools%20and%20real-world%20deployments.&entry.1838667208=http%3A//arxiv.org/abs/2512.05940v2&entry.124074799=Read"},
{"title": "Three factor delay learning rules for spiking neural networks", "author": "Luke Vassallo and Nima Taherinejad", "abstract": "Spiking Neural Networks (SNNs) are dynamical systems that operate on spatiotemporal data, yet their learnable parameters are often limited to synaptic weights, contributing little to temporal pattern recognition. Learnable parameters that delay spike times can improve classification performance in temporal tasks, but existing methods rely on large networks and offline learning, making them unsuitable for real-time operation in resource-constrained environments. In this paper, we introduce synaptic and axonal delays to leaky integrate and fire (LIF)-based feedforward and recurrent SNNs, and propose three-factor learning rules to simultaneously learn delay parameters online. We employ a smooth Gaussian surrogate to approximate spike derivatives exclusively for the eligibility trace calculation, and together with a top-down error signal determine parameter updates. Our experiments show that incorporating delays improves accuracy by up to 20% over a weights-only baseline, and for networks with similar parameter counts, jointly learning weights and delays yields up to 14% higher accuracy. On the SHD speech recognition dataset, our method achieves similar accuracy to offline backpropagation-based approaches. Compared to state-of-the-art methods, it reduces model size by 6.6x and inference latency by 67%, with only a 2.4% drop in classification accuracy. Our findings benefit the design of power and area-constrained neuromorphic processors by enabling on-device learning and lowering memory requirements.", "link": "http://arxiv.org/abs/2601.00668v1", "date": "2026-01-02", "relevancy": 1.8752, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.493}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4885}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Three%20factor%20delay%20learning%20rules%20for%20spiking%20neural%20networks&body=Title%3A%20Three%20factor%20delay%20learning%20rules%20for%20spiking%20neural%20networks%0AAuthor%3A%20Luke%20Vassallo%20and%20Nima%20Taherinejad%0AAbstract%3A%20Spiking%20Neural%20Networks%20%28SNNs%29%20are%20dynamical%20systems%20that%20operate%20on%20spatiotemporal%20data%2C%20yet%20their%20learnable%20parameters%20are%20often%20limited%20to%20synaptic%20weights%2C%20contributing%20little%20to%20temporal%20pattern%20recognition.%20Learnable%20parameters%20that%20delay%20spike%20times%20can%20improve%20classification%20performance%20in%20temporal%20tasks%2C%20but%20existing%20methods%20rely%20on%20large%20networks%20and%20offline%20learning%2C%20making%20them%20unsuitable%20for%20real-time%20operation%20in%20resource-constrained%20environments.%20In%20this%20paper%2C%20we%20introduce%20synaptic%20and%20axonal%20delays%20to%20leaky%20integrate%20and%20fire%20%28LIF%29-based%20feedforward%20and%20recurrent%20SNNs%2C%20and%20propose%20three-factor%20learning%20rules%20to%20simultaneously%20learn%20delay%20parameters%20online.%20We%20employ%20a%20smooth%20Gaussian%20surrogate%20to%20approximate%20spike%20derivatives%20exclusively%20for%20the%20eligibility%20trace%20calculation%2C%20and%20together%20with%20a%20top-down%20error%20signal%20determine%20parameter%20updates.%20Our%20experiments%20show%20that%20incorporating%20delays%20improves%20accuracy%20by%20up%20to%2020%25%20over%20a%20weights-only%20baseline%2C%20and%20for%20networks%20with%20similar%20parameter%20counts%2C%20jointly%20learning%20weights%20and%20delays%20yields%20up%20to%2014%25%20higher%20accuracy.%20On%20the%20SHD%20speech%20recognition%20dataset%2C%20our%20method%20achieves%20similar%20accuracy%20to%20offline%20backpropagation-based%20approaches.%20Compared%20to%20state-of-the-art%20methods%2C%20it%20reduces%20model%20size%20by%206.6x%20and%20inference%20latency%20by%2067%25%2C%20with%20only%20a%202.4%25%20drop%20in%20classification%20accuracy.%20Our%20findings%20benefit%20the%20design%20of%20power%20and%20area-constrained%20neuromorphic%20processors%20by%20enabling%20on-device%20learning%20and%20lowering%20memory%20requirements.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00668v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThree%2520factor%2520delay%2520learning%2520rules%2520for%2520spiking%2520neural%2520networks%26entry.906535625%3DLuke%2520Vassallo%2520and%2520Nima%2520Taherinejad%26entry.1292438233%3DSpiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520are%2520dynamical%2520systems%2520that%2520operate%2520on%2520spatiotemporal%2520data%252C%2520yet%2520their%2520learnable%2520parameters%2520are%2520often%2520limited%2520to%2520synaptic%2520weights%252C%2520contributing%2520little%2520to%2520temporal%2520pattern%2520recognition.%2520Learnable%2520parameters%2520that%2520delay%2520spike%2520times%2520can%2520improve%2520classification%2520performance%2520in%2520temporal%2520tasks%252C%2520but%2520existing%2520methods%2520rely%2520on%2520large%2520networks%2520and%2520offline%2520learning%252C%2520making%2520them%2520unsuitable%2520for%2520real-time%2520operation%2520in%2520resource-constrained%2520environments.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520synaptic%2520and%2520axonal%2520delays%2520to%2520leaky%2520integrate%2520and%2520fire%2520%2528LIF%2529-based%2520feedforward%2520and%2520recurrent%2520SNNs%252C%2520and%2520propose%2520three-factor%2520learning%2520rules%2520to%2520simultaneously%2520learn%2520delay%2520parameters%2520online.%2520We%2520employ%2520a%2520smooth%2520Gaussian%2520surrogate%2520to%2520approximate%2520spike%2520derivatives%2520exclusively%2520for%2520the%2520eligibility%2520trace%2520calculation%252C%2520and%2520together%2520with%2520a%2520top-down%2520error%2520signal%2520determine%2520parameter%2520updates.%2520Our%2520experiments%2520show%2520that%2520incorporating%2520delays%2520improves%2520accuracy%2520by%2520up%2520to%252020%2525%2520over%2520a%2520weights-only%2520baseline%252C%2520and%2520for%2520networks%2520with%2520similar%2520parameter%2520counts%252C%2520jointly%2520learning%2520weights%2520and%2520delays%2520yields%2520up%2520to%252014%2525%2520higher%2520accuracy.%2520On%2520the%2520SHD%2520speech%2520recognition%2520dataset%252C%2520our%2520method%2520achieves%2520similar%2520accuracy%2520to%2520offline%2520backpropagation-based%2520approaches.%2520Compared%2520to%2520state-of-the-art%2520methods%252C%2520it%2520reduces%2520model%2520size%2520by%25206.6x%2520and%2520inference%2520latency%2520by%252067%2525%252C%2520with%2520only%2520a%25202.4%2525%2520drop%2520in%2520classification%2520accuracy.%2520Our%2520findings%2520benefit%2520the%2520design%2520of%2520power%2520and%2520area-constrained%2520neuromorphic%2520processors%2520by%2520enabling%2520on-device%2520learning%2520and%2520lowering%2520memory%2520requirements.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00668v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Three%20factor%20delay%20learning%20rules%20for%20spiking%20neural%20networks&entry.906535625=Luke%20Vassallo%20and%20Nima%20Taherinejad&entry.1292438233=Spiking%20Neural%20Networks%20%28SNNs%29%20are%20dynamical%20systems%20that%20operate%20on%20spatiotemporal%20data%2C%20yet%20their%20learnable%20parameters%20are%20often%20limited%20to%20synaptic%20weights%2C%20contributing%20little%20to%20temporal%20pattern%20recognition.%20Learnable%20parameters%20that%20delay%20spike%20times%20can%20improve%20classification%20performance%20in%20temporal%20tasks%2C%20but%20existing%20methods%20rely%20on%20large%20networks%20and%20offline%20learning%2C%20making%20them%20unsuitable%20for%20real-time%20operation%20in%20resource-constrained%20environments.%20In%20this%20paper%2C%20we%20introduce%20synaptic%20and%20axonal%20delays%20to%20leaky%20integrate%20and%20fire%20%28LIF%29-based%20feedforward%20and%20recurrent%20SNNs%2C%20and%20propose%20three-factor%20learning%20rules%20to%20simultaneously%20learn%20delay%20parameters%20online.%20We%20employ%20a%20smooth%20Gaussian%20surrogate%20to%20approximate%20spike%20derivatives%20exclusively%20for%20the%20eligibility%20trace%20calculation%2C%20and%20together%20with%20a%20top-down%20error%20signal%20determine%20parameter%20updates.%20Our%20experiments%20show%20that%20incorporating%20delays%20improves%20accuracy%20by%20up%20to%2020%25%20over%20a%20weights-only%20baseline%2C%20and%20for%20networks%20with%20similar%20parameter%20counts%2C%20jointly%20learning%20weights%20and%20delays%20yields%20up%20to%2014%25%20higher%20accuracy.%20On%20the%20SHD%20speech%20recognition%20dataset%2C%20our%20method%20achieves%20similar%20accuracy%20to%20offline%20backpropagation-based%20approaches.%20Compared%20to%20state-of-the-art%20methods%2C%20it%20reduces%20model%20size%20by%206.6x%20and%20inference%20latency%20by%2067%25%2C%20with%20only%20a%202.4%25%20drop%20in%20classification%20accuracy.%20Our%20findings%20benefit%20the%20design%20of%20power%20and%20area-constrained%20neuromorphic%20processors%20by%20enabling%20on-device%20learning%20and%20lowering%20memory%20requirements.&entry.1838667208=http%3A//arxiv.org/abs/2601.00668v1&entry.124074799=Read"},
{"title": "Stochastic Actor-Critic: Mitigating Overestimation via Temporal Aleatoric Uncertainty", "author": "U\u011furcan \u00d6zalp", "abstract": "Off-policy actor-critic methods in reinforcement learning train a critic with temporal-difference updates and use it as a learning signal for the policy (actor). This design typically achieves higher sample efficiency than purely on-policy methods. However, critic networks tend to overestimate value estimates systematically. This is often addressed by introducing a pessimistic bias based on uncertainty estimates. Current methods employ ensembling to quantify the critic's epistemic uncertainty-uncertainty due to limited data and model ambiguity-to scale pessimistic updates. In this work, we propose a new algorithm called Stochastic Actor-Critic (STAC) that incorporates temporal (one-step) aleatoric uncertainty-uncertainty arising from stochastic transitions, rewards, and policy-induced variability in Bellman targets-to scale pessimistic bias in temporal-difference updates, rather than relying on epistemic uncertainty. STAC uses a single distributional critic network to model the temporal return uncertainty, and applies dropout to both the critic and actor networks for regularization. Our results show that pessimism based on a distributional critic alone suffices to mitigate overestimation, and naturally leads to risk-averse behavior in stochastic environments. Introducing dropout further improves training stability and performance by means of regularization. With this design, STAC achieves improved computational efficiency using a single distributional critic network.", "link": "http://arxiv.org/abs/2601.00737v1", "date": "2026-01-02", "relevancy": 1.8675, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4714}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4637}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stochastic%20Actor-Critic%3A%20Mitigating%20Overestimation%20via%20Temporal%20Aleatoric%20Uncertainty&body=Title%3A%20Stochastic%20Actor-Critic%3A%20Mitigating%20Overestimation%20via%20Temporal%20Aleatoric%20Uncertainty%0AAuthor%3A%20U%C4%9Furcan%20%C3%96zalp%0AAbstract%3A%20Off-policy%20actor-critic%20methods%20in%20reinforcement%20learning%20train%20a%20critic%20with%20temporal-difference%20updates%20and%20use%20it%20as%20a%20learning%20signal%20for%20the%20policy%20%28actor%29.%20This%20design%20typically%20achieves%20higher%20sample%20efficiency%20than%20purely%20on-policy%20methods.%20However%2C%20critic%20networks%20tend%20to%20overestimate%20value%20estimates%20systematically.%20This%20is%20often%20addressed%20by%20introducing%20a%20pessimistic%20bias%20based%20on%20uncertainty%20estimates.%20Current%20methods%20employ%20ensembling%20to%20quantify%20the%20critic%27s%20epistemic%20uncertainty-uncertainty%20due%20to%20limited%20data%20and%20model%20ambiguity-to%20scale%20pessimistic%20updates.%20In%20this%20work%2C%20we%20propose%20a%20new%20algorithm%20called%20Stochastic%20Actor-Critic%20%28STAC%29%20that%20incorporates%20temporal%20%28one-step%29%20aleatoric%20uncertainty-uncertainty%20arising%20from%20stochastic%20transitions%2C%20rewards%2C%20and%20policy-induced%20variability%20in%20Bellman%20targets-to%20scale%20pessimistic%20bias%20in%20temporal-difference%20updates%2C%20rather%20than%20relying%20on%20epistemic%20uncertainty.%20STAC%20uses%20a%20single%20distributional%20critic%20network%20to%20model%20the%20temporal%20return%20uncertainty%2C%20and%20applies%20dropout%20to%20both%20the%20critic%20and%20actor%20networks%20for%20regularization.%20Our%20results%20show%20that%20pessimism%20based%20on%20a%20distributional%20critic%20alone%20suffices%20to%20mitigate%20overestimation%2C%20and%20naturally%20leads%20to%20risk-averse%20behavior%20in%20stochastic%20environments.%20Introducing%20dropout%20further%20improves%20training%20stability%20and%20performance%20by%20means%20of%20regularization.%20With%20this%20design%2C%20STAC%20achieves%20improved%20computational%20efficiency%20using%20a%20single%20distributional%20critic%20network.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00737v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStochastic%2520Actor-Critic%253A%2520Mitigating%2520Overestimation%2520via%2520Temporal%2520Aleatoric%2520Uncertainty%26entry.906535625%3DU%25C4%259Furcan%2520%25C3%2596zalp%26entry.1292438233%3DOff-policy%2520actor-critic%2520methods%2520in%2520reinforcement%2520learning%2520train%2520a%2520critic%2520with%2520temporal-difference%2520updates%2520and%2520use%2520it%2520as%2520a%2520learning%2520signal%2520for%2520the%2520policy%2520%2528actor%2529.%2520This%2520design%2520typically%2520achieves%2520higher%2520sample%2520efficiency%2520than%2520purely%2520on-policy%2520methods.%2520However%252C%2520critic%2520networks%2520tend%2520to%2520overestimate%2520value%2520estimates%2520systematically.%2520This%2520is%2520often%2520addressed%2520by%2520introducing%2520a%2520pessimistic%2520bias%2520based%2520on%2520uncertainty%2520estimates.%2520Current%2520methods%2520employ%2520ensembling%2520to%2520quantify%2520the%2520critic%2527s%2520epistemic%2520uncertainty-uncertainty%2520due%2520to%2520limited%2520data%2520and%2520model%2520ambiguity-to%2520scale%2520pessimistic%2520updates.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520new%2520algorithm%2520called%2520Stochastic%2520Actor-Critic%2520%2528STAC%2529%2520that%2520incorporates%2520temporal%2520%2528one-step%2529%2520aleatoric%2520uncertainty-uncertainty%2520arising%2520from%2520stochastic%2520transitions%252C%2520rewards%252C%2520and%2520policy-induced%2520variability%2520in%2520Bellman%2520targets-to%2520scale%2520pessimistic%2520bias%2520in%2520temporal-difference%2520updates%252C%2520rather%2520than%2520relying%2520on%2520epistemic%2520uncertainty.%2520STAC%2520uses%2520a%2520single%2520distributional%2520critic%2520network%2520to%2520model%2520the%2520temporal%2520return%2520uncertainty%252C%2520and%2520applies%2520dropout%2520to%2520both%2520the%2520critic%2520and%2520actor%2520networks%2520for%2520regularization.%2520Our%2520results%2520show%2520that%2520pessimism%2520based%2520on%2520a%2520distributional%2520critic%2520alone%2520suffices%2520to%2520mitigate%2520overestimation%252C%2520and%2520naturally%2520leads%2520to%2520risk-averse%2520behavior%2520in%2520stochastic%2520environments.%2520Introducing%2520dropout%2520further%2520improves%2520training%2520stability%2520and%2520performance%2520by%2520means%2520of%2520regularization.%2520With%2520this%2520design%252C%2520STAC%2520achieves%2520improved%2520computational%2520efficiency%2520using%2520a%2520single%2520distributional%2520critic%2520network.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00737v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stochastic%20Actor-Critic%3A%20Mitigating%20Overestimation%20via%20Temporal%20Aleatoric%20Uncertainty&entry.906535625=U%C4%9Furcan%20%C3%96zalp&entry.1292438233=Off-policy%20actor-critic%20methods%20in%20reinforcement%20learning%20train%20a%20critic%20with%20temporal-difference%20updates%20and%20use%20it%20as%20a%20learning%20signal%20for%20the%20policy%20%28actor%29.%20This%20design%20typically%20achieves%20higher%20sample%20efficiency%20than%20purely%20on-policy%20methods.%20However%2C%20critic%20networks%20tend%20to%20overestimate%20value%20estimates%20systematically.%20This%20is%20often%20addressed%20by%20introducing%20a%20pessimistic%20bias%20based%20on%20uncertainty%20estimates.%20Current%20methods%20employ%20ensembling%20to%20quantify%20the%20critic%27s%20epistemic%20uncertainty-uncertainty%20due%20to%20limited%20data%20and%20model%20ambiguity-to%20scale%20pessimistic%20updates.%20In%20this%20work%2C%20we%20propose%20a%20new%20algorithm%20called%20Stochastic%20Actor-Critic%20%28STAC%29%20that%20incorporates%20temporal%20%28one-step%29%20aleatoric%20uncertainty-uncertainty%20arising%20from%20stochastic%20transitions%2C%20rewards%2C%20and%20policy-induced%20variability%20in%20Bellman%20targets-to%20scale%20pessimistic%20bias%20in%20temporal-difference%20updates%2C%20rather%20than%20relying%20on%20epistemic%20uncertainty.%20STAC%20uses%20a%20single%20distributional%20critic%20network%20to%20model%20the%20temporal%20return%20uncertainty%2C%20and%20applies%20dropout%20to%20both%20the%20critic%20and%20actor%20networks%20for%20regularization.%20Our%20results%20show%20that%20pessimism%20based%20on%20a%20distributional%20critic%20alone%20suffices%20to%20mitigate%20overestimation%2C%20and%20naturally%20leads%20to%20risk-averse%20behavior%20in%20stochastic%20environments.%20Introducing%20dropout%20further%20improves%20training%20stability%20and%20performance%20by%20means%20of%20regularization.%20With%20this%20design%2C%20STAC%20achieves%20improved%20computational%20efficiency%20using%20a%20single%20distributional%20critic%20network.&entry.1838667208=http%3A//arxiv.org/abs/2601.00737v1&entry.124074799=Read"},
{"title": "Precision Autotuning for Linear Solvers via Contextual Bandit-Based RL", "author": "Erin Carson and Xinye Chen", "abstract": "We propose a reinforcement learning (RL) framework for adaptive precision tuning of linear solvers, and can be extended to general algorithms. The framework is formulated as a contextual bandit problem and solved using incremental action-value estimation with a discretized state space to select optimal precision configurations for computational steps, balancing precision and computational efficiency. To verify its effectiveness, we apply the framework to iterative refinement for solving linear systems $Ax = b$. In this application, our approach dynamically chooses precisions based on calculated features from the system. In detail, a Q-table maps discretized features (e.g., approximate condition number and matrix norm)to actions (chosen precision configurations for specific steps), optimized via an epsilon-greedy strategy to maximize a multi-objective reward balancing accuracy and computational cost. Empirical results demonstrate effective precision selection, reducing computational cost while maintaining accuracy comparable to double-precision baselines. The framework generalizes to diverse out-of-sample data and offers insight into utilizing RL precision selection for other numerical algorithms, advancing mixed-precision numerical methods in scientific computing. To the best of our knowledge, this is the first work on precision autotuning with RL and verified on unseen datasets.", "link": "http://arxiv.org/abs/2601.00728v1", "date": "2026-01-02", "relevancy": 1.8578, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.481}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4656}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Precision%20Autotuning%20for%20Linear%20Solvers%20via%20Contextual%20Bandit-Based%20RL&body=Title%3A%20Precision%20Autotuning%20for%20Linear%20Solvers%20via%20Contextual%20Bandit-Based%20RL%0AAuthor%3A%20Erin%20Carson%20and%20Xinye%20Chen%0AAbstract%3A%20We%20propose%20a%20reinforcement%20learning%20%28RL%29%20framework%20for%20adaptive%20precision%20tuning%20of%20linear%20solvers%2C%20and%20can%20be%20extended%20to%20general%20algorithms.%20The%20framework%20is%20formulated%20as%20a%20contextual%20bandit%20problem%20and%20solved%20using%20incremental%20action-value%20estimation%20with%20a%20discretized%20state%20space%20to%20select%20optimal%20precision%20configurations%20for%20computational%20steps%2C%20balancing%20precision%20and%20computational%20efficiency.%20To%20verify%20its%20effectiveness%2C%20we%20apply%20the%20framework%20to%20iterative%20refinement%20for%20solving%20linear%20systems%20%24Ax%20%3D%20b%24.%20In%20this%20application%2C%20our%20approach%20dynamically%20chooses%20precisions%20based%20on%20calculated%20features%20from%20the%20system.%20In%20detail%2C%20a%20Q-table%20maps%20discretized%20features%20%28e.g.%2C%20approximate%20condition%20number%20and%20matrix%20norm%29to%20actions%20%28chosen%20precision%20configurations%20for%20specific%20steps%29%2C%20optimized%20via%20an%20epsilon-greedy%20strategy%20to%20maximize%20a%20multi-objective%20reward%20balancing%20accuracy%20and%20computational%20cost.%20Empirical%20results%20demonstrate%20effective%20precision%20selection%2C%20reducing%20computational%20cost%20while%20maintaining%20accuracy%20comparable%20to%20double-precision%20baselines.%20The%20framework%20generalizes%20to%20diverse%20out-of-sample%20data%20and%20offers%20insight%20into%20utilizing%20RL%20precision%20selection%20for%20other%20numerical%20algorithms%2C%20advancing%20mixed-precision%20numerical%20methods%20in%20scientific%20computing.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20work%20on%20precision%20autotuning%20with%20RL%20and%20verified%20on%20unseen%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00728v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrecision%2520Autotuning%2520for%2520Linear%2520Solvers%2520via%2520Contextual%2520Bandit-Based%2520RL%26entry.906535625%3DErin%2520Carson%2520and%2520Xinye%2520Chen%26entry.1292438233%3DWe%2520propose%2520a%2520reinforcement%2520learning%2520%2528RL%2529%2520framework%2520for%2520adaptive%2520precision%2520tuning%2520of%2520linear%2520solvers%252C%2520and%2520can%2520be%2520extended%2520to%2520general%2520algorithms.%2520The%2520framework%2520is%2520formulated%2520as%2520a%2520contextual%2520bandit%2520problem%2520and%2520solved%2520using%2520incremental%2520action-value%2520estimation%2520with%2520a%2520discretized%2520state%2520space%2520to%2520select%2520optimal%2520precision%2520configurations%2520for%2520computational%2520steps%252C%2520balancing%2520precision%2520and%2520computational%2520efficiency.%2520To%2520verify%2520its%2520effectiveness%252C%2520we%2520apply%2520the%2520framework%2520to%2520iterative%2520refinement%2520for%2520solving%2520linear%2520systems%2520%2524Ax%2520%253D%2520b%2524.%2520In%2520this%2520application%252C%2520our%2520approach%2520dynamically%2520chooses%2520precisions%2520based%2520on%2520calculated%2520features%2520from%2520the%2520system.%2520In%2520detail%252C%2520a%2520Q-table%2520maps%2520discretized%2520features%2520%2528e.g.%252C%2520approximate%2520condition%2520number%2520and%2520matrix%2520norm%2529to%2520actions%2520%2528chosen%2520precision%2520configurations%2520for%2520specific%2520steps%2529%252C%2520optimized%2520via%2520an%2520epsilon-greedy%2520strategy%2520to%2520maximize%2520a%2520multi-objective%2520reward%2520balancing%2520accuracy%2520and%2520computational%2520cost.%2520Empirical%2520results%2520demonstrate%2520effective%2520precision%2520selection%252C%2520reducing%2520computational%2520cost%2520while%2520maintaining%2520accuracy%2520comparable%2520to%2520double-precision%2520baselines.%2520The%2520framework%2520generalizes%2520to%2520diverse%2520out-of-sample%2520data%2520and%2520offers%2520insight%2520into%2520utilizing%2520RL%2520precision%2520selection%2520for%2520other%2520numerical%2520algorithms%252C%2520advancing%2520mixed-precision%2520numerical%2520methods%2520in%2520scientific%2520computing.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520work%2520on%2520precision%2520autotuning%2520with%2520RL%2520and%2520verified%2520on%2520unseen%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00728v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Precision%20Autotuning%20for%20Linear%20Solvers%20via%20Contextual%20Bandit-Based%20RL&entry.906535625=Erin%20Carson%20and%20Xinye%20Chen&entry.1292438233=We%20propose%20a%20reinforcement%20learning%20%28RL%29%20framework%20for%20adaptive%20precision%20tuning%20of%20linear%20solvers%2C%20and%20can%20be%20extended%20to%20general%20algorithms.%20The%20framework%20is%20formulated%20as%20a%20contextual%20bandit%20problem%20and%20solved%20using%20incremental%20action-value%20estimation%20with%20a%20discretized%20state%20space%20to%20select%20optimal%20precision%20configurations%20for%20computational%20steps%2C%20balancing%20precision%20and%20computational%20efficiency.%20To%20verify%20its%20effectiveness%2C%20we%20apply%20the%20framework%20to%20iterative%20refinement%20for%20solving%20linear%20systems%20%24Ax%20%3D%20b%24.%20In%20this%20application%2C%20our%20approach%20dynamically%20chooses%20precisions%20based%20on%20calculated%20features%20from%20the%20system.%20In%20detail%2C%20a%20Q-table%20maps%20discretized%20features%20%28e.g.%2C%20approximate%20condition%20number%20and%20matrix%20norm%29to%20actions%20%28chosen%20precision%20configurations%20for%20specific%20steps%29%2C%20optimized%20via%20an%20epsilon-greedy%20strategy%20to%20maximize%20a%20multi-objective%20reward%20balancing%20accuracy%20and%20computational%20cost.%20Empirical%20results%20demonstrate%20effective%20precision%20selection%2C%20reducing%20computational%20cost%20while%20maintaining%20accuracy%20comparable%20to%20double-precision%20baselines.%20The%20framework%20generalizes%20to%20diverse%20out-of-sample%20data%20and%20offers%20insight%20into%20utilizing%20RL%20precision%20selection%20for%20other%20numerical%20algorithms%2C%20advancing%20mixed-precision%20numerical%20methods%20in%20scientific%20computing.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20work%20on%20precision%20autotuning%20with%20RL%20and%20verified%20on%20unseen%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2601.00728v1&entry.124074799=Read"},
{"title": "Grading Handwritten Engineering Exams with Multimodal Large Language Models", "author": "Janez Per\u0161 and Jon Muhovi\u010d and Andrej Ko\u0161ir and Bo\u0161tjan Murovec", "abstract": "Handwritten STEM exams capture open-ended reasoning and diagrams, but manual grading is slow and difficult to scale. We present an end-to-end workflow for grading scanned handwritten engineering quizzes with multimodal large language models (LLMs) that preserves the standard exam process (A4 paper, unconstrained student handwriting). The lecturer provides only a handwritten reference solution (100%) and a short set of grading rules; the reference is converted into a text-only summary that conditions grading without exposing the reference scan. Reliability is achieved through a multi-stage design with a format/presence check to prevent grading blank answers, an ensemble of independent graders, supervisor aggregation, and rigid templates with deterministic validation to produce auditable, machine-parseable reports. We evaluate the frozen pipeline in a clean-room protocol on a held-out real course quiz in Slovenian, including hand-drawn circuit schematics. With state-of-the-art backends (GPT-5.2 and Gemini-3 Pro), the full pipeline achieves $\\approx$8-point mean absolute difference to lecturer grades with low bias and an estimated manual-review trigger rate of $\\approx$17% at $D_{\\max}=40$. Ablations show that trivial prompting and removing the reference solution substantially degrade accuracy and introduce systematic over-grading, confirming that structured prompting and reference grounding are essential.", "link": "http://arxiv.org/abs/2601.00730v1", "date": "2026-01-02", "relevancy": 1.8379, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4678}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4653}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grading%20Handwritten%20Engineering%20Exams%20with%20Multimodal%20Large%20Language%20Models&body=Title%3A%20Grading%20Handwritten%20Engineering%20Exams%20with%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Janez%20Per%C5%A1%20and%20Jon%20Muhovi%C4%8D%20and%20Andrej%20Ko%C5%A1ir%20and%20Bo%C5%A1tjan%20Murovec%0AAbstract%3A%20Handwritten%20STEM%20exams%20capture%20open-ended%20reasoning%20and%20diagrams%2C%20but%20manual%20grading%20is%20slow%20and%20difficult%20to%20scale.%20We%20present%20an%20end-to-end%20workflow%20for%20grading%20scanned%20handwritten%20engineering%20quizzes%20with%20multimodal%20large%20language%20models%20%28LLMs%29%20that%20preserves%20the%20standard%20exam%20process%20%28A4%20paper%2C%20unconstrained%20student%20handwriting%29.%20The%20lecturer%20provides%20only%20a%20handwritten%20reference%20solution%20%28100%25%29%20and%20a%20short%20set%20of%20grading%20rules%3B%20the%20reference%20is%20converted%20into%20a%20text-only%20summary%20that%20conditions%20grading%20without%20exposing%20the%20reference%20scan.%20Reliability%20is%20achieved%20through%20a%20multi-stage%20design%20with%20a%20format/presence%20check%20to%20prevent%20grading%20blank%20answers%2C%20an%20ensemble%20of%20independent%20graders%2C%20supervisor%20aggregation%2C%20and%20rigid%20templates%20with%20deterministic%20validation%20to%20produce%20auditable%2C%20machine-parseable%20reports.%20We%20evaluate%20the%20frozen%20pipeline%20in%20a%20clean-room%20protocol%20on%20a%20held-out%20real%20course%20quiz%20in%20Slovenian%2C%20including%20hand-drawn%20circuit%20schematics.%20With%20state-of-the-art%20backends%20%28GPT-5.2%20and%20Gemini-3%20Pro%29%2C%20the%20full%20pipeline%20achieves%20%24%5Capprox%248-point%20mean%20absolute%20difference%20to%20lecturer%20grades%20with%20low%20bias%20and%20an%20estimated%20manual-review%20trigger%20rate%20of%20%24%5Capprox%2417%25%20at%20%24D_%7B%5Cmax%7D%3D40%24.%20Ablations%20show%20that%20trivial%20prompting%20and%20removing%20the%20reference%20solution%20substantially%20degrade%20accuracy%20and%20introduce%20systematic%20over-grading%2C%20confirming%20that%20structured%20prompting%20and%20reference%20grounding%20are%20essential.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00730v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrading%2520Handwritten%2520Engineering%2520Exams%2520with%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DJanez%2520Per%25C5%25A1%2520and%2520Jon%2520Muhovi%25C4%258D%2520and%2520Andrej%2520Ko%25C5%25A1ir%2520and%2520Bo%25C5%25A1tjan%2520Murovec%26entry.1292438233%3DHandwritten%2520STEM%2520exams%2520capture%2520open-ended%2520reasoning%2520and%2520diagrams%252C%2520but%2520manual%2520grading%2520is%2520slow%2520and%2520difficult%2520to%2520scale.%2520We%2520present%2520an%2520end-to-end%2520workflow%2520for%2520grading%2520scanned%2520handwritten%2520engineering%2520quizzes%2520with%2520multimodal%2520large%2520language%2520models%2520%2528LLMs%2529%2520that%2520preserves%2520the%2520standard%2520exam%2520process%2520%2528A4%2520paper%252C%2520unconstrained%2520student%2520handwriting%2529.%2520The%2520lecturer%2520provides%2520only%2520a%2520handwritten%2520reference%2520solution%2520%2528100%2525%2529%2520and%2520a%2520short%2520set%2520of%2520grading%2520rules%253B%2520the%2520reference%2520is%2520converted%2520into%2520a%2520text-only%2520summary%2520that%2520conditions%2520grading%2520without%2520exposing%2520the%2520reference%2520scan.%2520Reliability%2520is%2520achieved%2520through%2520a%2520multi-stage%2520design%2520with%2520a%2520format/presence%2520check%2520to%2520prevent%2520grading%2520blank%2520answers%252C%2520an%2520ensemble%2520of%2520independent%2520graders%252C%2520supervisor%2520aggregation%252C%2520and%2520rigid%2520templates%2520with%2520deterministic%2520validation%2520to%2520produce%2520auditable%252C%2520machine-parseable%2520reports.%2520We%2520evaluate%2520the%2520frozen%2520pipeline%2520in%2520a%2520clean-room%2520protocol%2520on%2520a%2520held-out%2520real%2520course%2520quiz%2520in%2520Slovenian%252C%2520including%2520hand-drawn%2520circuit%2520schematics.%2520With%2520state-of-the-art%2520backends%2520%2528GPT-5.2%2520and%2520Gemini-3%2520Pro%2529%252C%2520the%2520full%2520pipeline%2520achieves%2520%2524%255Capprox%25248-point%2520mean%2520absolute%2520difference%2520to%2520lecturer%2520grades%2520with%2520low%2520bias%2520and%2520an%2520estimated%2520manual-review%2520trigger%2520rate%2520of%2520%2524%255Capprox%252417%2525%2520at%2520%2524D_%257B%255Cmax%257D%253D40%2524.%2520Ablations%2520show%2520that%2520trivial%2520prompting%2520and%2520removing%2520the%2520reference%2520solution%2520substantially%2520degrade%2520accuracy%2520and%2520introduce%2520systematic%2520over-grading%252C%2520confirming%2520that%2520structured%2520prompting%2520and%2520reference%2520grounding%2520are%2520essential.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00730v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grading%20Handwritten%20Engineering%20Exams%20with%20Multimodal%20Large%20Language%20Models&entry.906535625=Janez%20Per%C5%A1%20and%20Jon%20Muhovi%C4%8D%20and%20Andrej%20Ko%C5%A1ir%20and%20Bo%C5%A1tjan%20Murovec&entry.1292438233=Handwritten%20STEM%20exams%20capture%20open-ended%20reasoning%20and%20diagrams%2C%20but%20manual%20grading%20is%20slow%20and%20difficult%20to%20scale.%20We%20present%20an%20end-to-end%20workflow%20for%20grading%20scanned%20handwritten%20engineering%20quizzes%20with%20multimodal%20large%20language%20models%20%28LLMs%29%20that%20preserves%20the%20standard%20exam%20process%20%28A4%20paper%2C%20unconstrained%20student%20handwriting%29.%20The%20lecturer%20provides%20only%20a%20handwritten%20reference%20solution%20%28100%25%29%20and%20a%20short%20set%20of%20grading%20rules%3B%20the%20reference%20is%20converted%20into%20a%20text-only%20summary%20that%20conditions%20grading%20without%20exposing%20the%20reference%20scan.%20Reliability%20is%20achieved%20through%20a%20multi-stage%20design%20with%20a%20format/presence%20check%20to%20prevent%20grading%20blank%20answers%2C%20an%20ensemble%20of%20independent%20graders%2C%20supervisor%20aggregation%2C%20and%20rigid%20templates%20with%20deterministic%20validation%20to%20produce%20auditable%2C%20machine-parseable%20reports.%20We%20evaluate%20the%20frozen%20pipeline%20in%20a%20clean-room%20protocol%20on%20a%20held-out%20real%20course%20quiz%20in%20Slovenian%2C%20including%20hand-drawn%20circuit%20schematics.%20With%20state-of-the-art%20backends%20%28GPT-5.2%20and%20Gemini-3%20Pro%29%2C%20the%20full%20pipeline%20achieves%20%24%5Capprox%248-point%20mean%20absolute%20difference%20to%20lecturer%20grades%20with%20low%20bias%20and%20an%20estimated%20manual-review%20trigger%20rate%20of%20%24%5Capprox%2417%25%20at%20%24D_%7B%5Cmax%7D%3D40%24.%20Ablations%20show%20that%20trivial%20prompting%20and%20removing%20the%20reference%20solution%20substantially%20degrade%20accuracy%20and%20introduce%20systematic%20over-grading%2C%20confirming%20that%20structured%20prompting%20and%20reference%20grounding%20are%20essential.&entry.1838667208=http%3A//arxiv.org/abs/2601.00730v1&entry.124074799=Read"},
{"title": "TeleDoCTR: Domain-Specific and Contextual Troubleshooting for Telecommunications", "author": "Mohamed Trabelsi and Huseyin Uzunalioglu", "abstract": "Ticket troubleshooting refers to the process of analyzing and resolving problems that are reported through a ticketing system. In large organizations offering a wide range of services, this task is highly complex due to the diversity of submitted tickets and the need for specialized domain knowledge. In particular, troubleshooting in telecommunications (telecom) is a very time-consuming task as it requires experts to interpret ticket content, consult documentation, and search historical records to identify appropriate resolutions. This human-intensive approach not only delays issue resolution but also hinders overall operational efficiency. To enhance the effectiveness and efficiency of ticket troubleshooting in telecom, we propose TeleDoCTR, a novel telecom-related, domain-specific, and contextual troubleshooting system tailored for end-to-end ticket resolution in telecom. TeleDoCTR integrates both domain-specific ranking and generative models to automate key steps of the troubleshooting workflow which are: routing tickets to the appropriate expert team responsible for resolving the ticket (classification task), retrieving contextually and semantically similar historical tickets (retrieval task), and generating a detailed fault analysis report outlining the issue, root cause, and potential solutions (generation task). We evaluate TeleDoCTR on a real-world dataset from a telecom infrastructure and demonstrate that it achieves superior performance over existing state-of-the-art methods, significantly enhancing the accuracy and efficiency of the troubleshooting process.", "link": "http://arxiv.org/abs/2601.00691v1", "date": "2026-01-02", "relevancy": 1.8278, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3859}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3559}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.3548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TeleDoCTR%3A%20Domain-Specific%20and%20Contextual%20Troubleshooting%20for%20Telecommunications&body=Title%3A%20TeleDoCTR%3A%20Domain-Specific%20and%20Contextual%20Troubleshooting%20for%20Telecommunications%0AAuthor%3A%20Mohamed%20Trabelsi%20and%20Huseyin%20Uzunalioglu%0AAbstract%3A%20Ticket%20troubleshooting%20refers%20to%20the%20process%20of%20analyzing%20and%20resolving%20problems%20that%20are%20reported%20through%20a%20ticketing%20system.%20In%20large%20organizations%20offering%20a%20wide%20range%20of%20services%2C%20this%20task%20is%20highly%20complex%20due%20to%20the%20diversity%20of%20submitted%20tickets%20and%20the%20need%20for%20specialized%20domain%20knowledge.%20In%20particular%2C%20troubleshooting%20in%20telecommunications%20%28telecom%29%20is%20a%20very%20time-consuming%20task%20as%20it%20requires%20experts%20to%20interpret%20ticket%20content%2C%20consult%20documentation%2C%20and%20search%20historical%20records%20to%20identify%20appropriate%20resolutions.%20This%20human-intensive%20approach%20not%20only%20delays%20issue%20resolution%20but%20also%20hinders%20overall%20operational%20efficiency.%20To%20enhance%20the%20effectiveness%20and%20efficiency%20of%20ticket%20troubleshooting%20in%20telecom%2C%20we%20propose%20TeleDoCTR%2C%20a%20novel%20telecom-related%2C%20domain-specific%2C%20and%20contextual%20troubleshooting%20system%20tailored%20for%20end-to-end%20ticket%20resolution%20in%20telecom.%20TeleDoCTR%20integrates%20both%20domain-specific%20ranking%20and%20generative%20models%20to%20automate%20key%20steps%20of%20the%20troubleshooting%20workflow%20which%20are%3A%20routing%20tickets%20to%20the%20appropriate%20expert%20team%20responsible%20for%20resolving%20the%20ticket%20%28classification%20task%29%2C%20retrieving%20contextually%20and%20semantically%20similar%20historical%20tickets%20%28retrieval%20task%29%2C%20and%20generating%20a%20detailed%20fault%20analysis%20report%20outlining%20the%20issue%2C%20root%20cause%2C%20and%20potential%20solutions%20%28generation%20task%29.%20We%20evaluate%20TeleDoCTR%20on%20a%20real-world%20dataset%20from%20a%20telecom%20infrastructure%20and%20demonstrate%20that%20it%20achieves%20superior%20performance%20over%20existing%20state-of-the-art%20methods%2C%20significantly%20enhancing%20the%20accuracy%20and%20efficiency%20of%20the%20troubleshooting%20process.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeleDoCTR%253A%2520Domain-Specific%2520and%2520Contextual%2520Troubleshooting%2520for%2520Telecommunications%26entry.906535625%3DMohamed%2520Trabelsi%2520and%2520Huseyin%2520Uzunalioglu%26entry.1292438233%3DTicket%2520troubleshooting%2520refers%2520to%2520the%2520process%2520of%2520analyzing%2520and%2520resolving%2520problems%2520that%2520are%2520reported%2520through%2520a%2520ticketing%2520system.%2520In%2520large%2520organizations%2520offering%2520a%2520wide%2520range%2520of%2520services%252C%2520this%2520task%2520is%2520highly%2520complex%2520due%2520to%2520the%2520diversity%2520of%2520submitted%2520tickets%2520and%2520the%2520need%2520for%2520specialized%2520domain%2520knowledge.%2520In%2520particular%252C%2520troubleshooting%2520in%2520telecommunications%2520%2528telecom%2529%2520is%2520a%2520very%2520time-consuming%2520task%2520as%2520it%2520requires%2520experts%2520to%2520interpret%2520ticket%2520content%252C%2520consult%2520documentation%252C%2520and%2520search%2520historical%2520records%2520to%2520identify%2520appropriate%2520resolutions.%2520This%2520human-intensive%2520approach%2520not%2520only%2520delays%2520issue%2520resolution%2520but%2520also%2520hinders%2520overall%2520operational%2520efficiency.%2520To%2520enhance%2520the%2520effectiveness%2520and%2520efficiency%2520of%2520ticket%2520troubleshooting%2520in%2520telecom%252C%2520we%2520propose%2520TeleDoCTR%252C%2520a%2520novel%2520telecom-related%252C%2520domain-specific%252C%2520and%2520contextual%2520troubleshooting%2520system%2520tailored%2520for%2520end-to-end%2520ticket%2520resolution%2520in%2520telecom.%2520TeleDoCTR%2520integrates%2520both%2520domain-specific%2520ranking%2520and%2520generative%2520models%2520to%2520automate%2520key%2520steps%2520of%2520the%2520troubleshooting%2520workflow%2520which%2520are%253A%2520routing%2520tickets%2520to%2520the%2520appropriate%2520expert%2520team%2520responsible%2520for%2520resolving%2520the%2520ticket%2520%2528classification%2520task%2529%252C%2520retrieving%2520contextually%2520and%2520semantically%2520similar%2520historical%2520tickets%2520%2528retrieval%2520task%2529%252C%2520and%2520generating%2520a%2520detailed%2520fault%2520analysis%2520report%2520outlining%2520the%2520issue%252C%2520root%2520cause%252C%2520and%2520potential%2520solutions%2520%2528generation%2520task%2529.%2520We%2520evaluate%2520TeleDoCTR%2520on%2520a%2520real-world%2520dataset%2520from%2520a%2520telecom%2520infrastructure%2520and%2520demonstrate%2520that%2520it%2520achieves%2520superior%2520performance%2520over%2520existing%2520state-of-the-art%2520methods%252C%2520significantly%2520enhancing%2520the%2520accuracy%2520and%2520efficiency%2520of%2520the%2520troubleshooting%2520process.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TeleDoCTR%3A%20Domain-Specific%20and%20Contextual%20Troubleshooting%20for%20Telecommunications&entry.906535625=Mohamed%20Trabelsi%20and%20Huseyin%20Uzunalioglu&entry.1292438233=Ticket%20troubleshooting%20refers%20to%20the%20process%20of%20analyzing%20and%20resolving%20problems%20that%20are%20reported%20through%20a%20ticketing%20system.%20In%20large%20organizations%20offering%20a%20wide%20range%20of%20services%2C%20this%20task%20is%20highly%20complex%20due%20to%20the%20diversity%20of%20submitted%20tickets%20and%20the%20need%20for%20specialized%20domain%20knowledge.%20In%20particular%2C%20troubleshooting%20in%20telecommunications%20%28telecom%29%20is%20a%20very%20time-consuming%20task%20as%20it%20requires%20experts%20to%20interpret%20ticket%20content%2C%20consult%20documentation%2C%20and%20search%20historical%20records%20to%20identify%20appropriate%20resolutions.%20This%20human-intensive%20approach%20not%20only%20delays%20issue%20resolution%20but%20also%20hinders%20overall%20operational%20efficiency.%20To%20enhance%20the%20effectiveness%20and%20efficiency%20of%20ticket%20troubleshooting%20in%20telecom%2C%20we%20propose%20TeleDoCTR%2C%20a%20novel%20telecom-related%2C%20domain-specific%2C%20and%20contextual%20troubleshooting%20system%20tailored%20for%20end-to-end%20ticket%20resolution%20in%20telecom.%20TeleDoCTR%20integrates%20both%20domain-specific%20ranking%20and%20generative%20models%20to%20automate%20key%20steps%20of%20the%20troubleshooting%20workflow%20which%20are%3A%20routing%20tickets%20to%20the%20appropriate%20expert%20team%20responsible%20for%20resolving%20the%20ticket%20%28classification%20task%29%2C%20retrieving%20contextually%20and%20semantically%20similar%20historical%20tickets%20%28retrieval%20task%29%2C%20and%20generating%20a%20detailed%20fault%20analysis%20report%20outlining%20the%20issue%2C%20root%20cause%2C%20and%20potential%20solutions%20%28generation%20task%29.%20We%20evaluate%20TeleDoCTR%20on%20a%20real-world%20dataset%20from%20a%20telecom%20infrastructure%20and%20demonstrate%20that%20it%20achieves%20superior%20performance%20over%20existing%20state-of-the-art%20methods%2C%20significantly%20enhancing%20the%20accuracy%20and%20efficiency%20of%20the%20troubleshooting%20process.&entry.1838667208=http%3A//arxiv.org/abs/2601.00691v1&entry.124074799=Read"},
{"title": "A Near-optimal, Scalable and Parallelizable Framework for Stochastic Bandits Robust to Adversarial Corruptions and Beyond", "author": "Zicheng Hu and Cheng Chen", "abstract": "We investigate various stochastic bandit problems in the presence of adversarial corruptions. A seminal work for this problem is the BARBAR~\\cite{gupta2019better} algorithm, which achieves both robustness and efficiency. However, it suffers from a regret of $O(KC)$, which does not match the lower bound of $\u03a9(C)$, where $K$ denotes the number of arms and $C$ denotes the corruption level. In this paper, we first improve the BARBAR algorithm by proposing a novel framework called BARBAT, which eliminates the factor of $K$ to achieve an optimal regret bound up to a logarithmic factor. We also extend BARBAT to various settings, including multi-agent bandits, graph bandits, combinatorial semi-bandits and batched bandits. Compared with the Follow-the-Regularized-Leader framework, our methods are more amenable to parallelization, making them suitable for multi-agent and batched bandit settings, and they incur lower computational costs, particularly in semi-bandit problems. Numerical experiments verify the efficiency of the proposed methods.", "link": "http://arxiv.org/abs/2502.07514v2", "date": "2026-01-02", "relevancy": 1.7969, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4722}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4589}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Near-optimal%2C%20Scalable%20and%20Parallelizable%20Framework%20for%20Stochastic%20Bandits%20Robust%20to%20Adversarial%20Corruptions%20and%20Beyond&body=Title%3A%20A%20Near-optimal%2C%20Scalable%20and%20Parallelizable%20Framework%20for%20Stochastic%20Bandits%20Robust%20to%20Adversarial%20Corruptions%20and%20Beyond%0AAuthor%3A%20Zicheng%20Hu%20and%20Cheng%20Chen%0AAbstract%3A%20We%20investigate%20various%20stochastic%20bandit%20problems%20in%20the%20presence%20of%20adversarial%20corruptions.%20A%20seminal%20work%20for%20this%20problem%20is%20the%20BARBAR~%5Ccite%7Bgupta2019better%7D%20algorithm%2C%20which%20achieves%20both%20robustness%20and%20efficiency.%20However%2C%20it%20suffers%20from%20a%20regret%20of%20%24O%28KC%29%24%2C%20which%20does%20not%20match%20the%20lower%20bound%20of%20%24%CE%A9%28C%29%24%2C%20where%20%24K%24%20denotes%20the%20number%20of%20arms%20and%20%24C%24%20denotes%20the%20corruption%20level.%20In%20this%20paper%2C%20we%20first%20improve%20the%20BARBAR%20algorithm%20by%20proposing%20a%20novel%20framework%20called%20BARBAT%2C%20which%20eliminates%20the%20factor%20of%20%24K%24%20to%20achieve%20an%20optimal%20regret%20bound%20up%20to%20a%20logarithmic%20factor.%20We%20also%20extend%20BARBAT%20to%20various%20settings%2C%20including%20multi-agent%20bandits%2C%20graph%20bandits%2C%20combinatorial%20semi-bandits%20and%20batched%20bandits.%20Compared%20with%20the%20Follow-the-Regularized-Leader%20framework%2C%20our%20methods%20are%20more%20amenable%20to%20parallelization%2C%20making%20them%20suitable%20for%20multi-agent%20and%20batched%20bandit%20settings%2C%20and%20they%20incur%20lower%20computational%20costs%2C%20particularly%20in%20semi-bandit%20problems.%20Numerical%20experiments%20verify%20the%20efficiency%20of%20the%20proposed%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2502.07514v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Near-optimal%252C%2520Scalable%2520and%2520Parallelizable%2520Framework%2520for%2520Stochastic%2520Bandits%2520Robust%2520to%2520Adversarial%2520Corruptions%2520and%2520Beyond%26entry.906535625%3DZicheng%2520Hu%2520and%2520Cheng%2520Chen%26entry.1292438233%3DWe%2520investigate%2520various%2520stochastic%2520bandit%2520problems%2520in%2520the%2520presence%2520of%2520adversarial%2520corruptions.%2520A%2520seminal%2520work%2520for%2520this%2520problem%2520is%2520the%2520BARBAR~%255Ccite%257Bgupta2019better%257D%2520algorithm%252C%2520which%2520achieves%2520both%2520robustness%2520and%2520efficiency.%2520However%252C%2520it%2520suffers%2520from%2520a%2520regret%2520of%2520%2524O%2528KC%2529%2524%252C%2520which%2520does%2520not%2520match%2520the%2520lower%2520bound%2520of%2520%2524%25CE%25A9%2528C%2529%2524%252C%2520where%2520%2524K%2524%2520denotes%2520the%2520number%2520of%2520arms%2520and%2520%2524C%2524%2520denotes%2520the%2520corruption%2520level.%2520In%2520this%2520paper%252C%2520we%2520first%2520improve%2520the%2520BARBAR%2520algorithm%2520by%2520proposing%2520a%2520novel%2520framework%2520called%2520BARBAT%252C%2520which%2520eliminates%2520the%2520factor%2520of%2520%2524K%2524%2520to%2520achieve%2520an%2520optimal%2520regret%2520bound%2520up%2520to%2520a%2520logarithmic%2520factor.%2520We%2520also%2520extend%2520BARBAT%2520to%2520various%2520settings%252C%2520including%2520multi-agent%2520bandits%252C%2520graph%2520bandits%252C%2520combinatorial%2520semi-bandits%2520and%2520batched%2520bandits.%2520Compared%2520with%2520the%2520Follow-the-Regularized-Leader%2520framework%252C%2520our%2520methods%2520are%2520more%2520amenable%2520to%2520parallelization%252C%2520making%2520them%2520suitable%2520for%2520multi-agent%2520and%2520batched%2520bandit%2520settings%252C%2520and%2520they%2520incur%2520lower%2520computational%2520costs%252C%2520particularly%2520in%2520semi-bandit%2520problems.%2520Numerical%2520experiments%2520verify%2520the%2520efficiency%2520of%2520the%2520proposed%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07514v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Near-optimal%2C%20Scalable%20and%20Parallelizable%20Framework%20for%20Stochastic%20Bandits%20Robust%20to%20Adversarial%20Corruptions%20and%20Beyond&entry.906535625=Zicheng%20Hu%20and%20Cheng%20Chen&entry.1292438233=We%20investigate%20various%20stochastic%20bandit%20problems%20in%20the%20presence%20of%20adversarial%20corruptions.%20A%20seminal%20work%20for%20this%20problem%20is%20the%20BARBAR~%5Ccite%7Bgupta2019better%7D%20algorithm%2C%20which%20achieves%20both%20robustness%20and%20efficiency.%20However%2C%20it%20suffers%20from%20a%20regret%20of%20%24O%28KC%29%24%2C%20which%20does%20not%20match%20the%20lower%20bound%20of%20%24%CE%A9%28C%29%24%2C%20where%20%24K%24%20denotes%20the%20number%20of%20arms%20and%20%24C%24%20denotes%20the%20corruption%20level.%20In%20this%20paper%2C%20we%20first%20improve%20the%20BARBAR%20algorithm%20by%20proposing%20a%20novel%20framework%20called%20BARBAT%2C%20which%20eliminates%20the%20factor%20of%20%24K%24%20to%20achieve%20an%20optimal%20regret%20bound%20up%20to%20a%20logarithmic%20factor.%20We%20also%20extend%20BARBAT%20to%20various%20settings%2C%20including%20multi-agent%20bandits%2C%20graph%20bandits%2C%20combinatorial%20semi-bandits%20and%20batched%20bandits.%20Compared%20with%20the%20Follow-the-Regularized-Leader%20framework%2C%20our%20methods%20are%20more%20amenable%20to%20parallelization%2C%20making%20them%20suitable%20for%20multi-agent%20and%20batched%20bandit%20settings%2C%20and%20they%20incur%20lower%20computational%20costs%2C%20particularly%20in%20semi-bandit%20problems.%20Numerical%20experiments%20verify%20the%20efficiency%20of%20the%20proposed%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2502.07514v2&entry.124074799=Read"},
{"title": "Calling for Backup: How Children Navigate Successive Robot Communication Failures", "author": "Maria Teresa Parreira and Isabel Neto and Filipa Rocha and Wendy Ju", "abstract": "How do children respond to repeated robot errors? While prior research has examined adult reactions to successive robot errors, children's responses remain largely unexplored. In this study, we explore children's reactions to robot social errors and performance errors. For the latter, this study reproduces the successive robot failure paradigm of Liu et al. with child participants (N=59, ages 8-10) to examine how young users respond to repeated robot conversational errors. Participants interacted with a robot that failed to understand their prompts three times in succession, with their behavioral responses video-recorded and analyzed. We found both similarities and differences compared to adult responses from the original study. Like adults, children adjusted their prompts, modified their verbal tone, and exhibited increasingly emotional non-verbal responses throughout successive errors. However, children demonstrated more disengagement behaviors, including temporarily ignoring the robot or actively seeking an adult. Errors did not affect participants' perception of the robot, suggesting more flexible conversational expectations in children. These findings inform the design of more effective and developmentally appropriate human-robot interaction systems for young users.", "link": "http://arxiv.org/abs/2601.00754v1", "date": "2026-01-02", "relevancy": 1.7704, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4726}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4406}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Calling%20for%20Backup%3A%20How%20Children%20Navigate%20Successive%20Robot%20Communication%20Failures&body=Title%3A%20Calling%20for%20Backup%3A%20How%20Children%20Navigate%20Successive%20Robot%20Communication%20Failures%0AAuthor%3A%20Maria%20Teresa%20Parreira%20and%20Isabel%20Neto%20and%20Filipa%20Rocha%20and%20Wendy%20Ju%0AAbstract%3A%20How%20do%20children%20respond%20to%20repeated%20robot%20errors%3F%20While%20prior%20research%20has%20examined%20adult%20reactions%20to%20successive%20robot%20errors%2C%20children%27s%20responses%20remain%20largely%20unexplored.%20In%20this%20study%2C%20we%20explore%20children%27s%20reactions%20to%20robot%20social%20errors%20and%20performance%20errors.%20For%20the%20latter%2C%20this%20study%20reproduces%20the%20successive%20robot%20failure%20paradigm%20of%20Liu%20et%20al.%20with%20child%20participants%20%28N%3D59%2C%20ages%208-10%29%20to%20examine%20how%20young%20users%20respond%20to%20repeated%20robot%20conversational%20errors.%20Participants%20interacted%20with%20a%20robot%20that%20failed%20to%20understand%20their%20prompts%20three%20times%20in%20succession%2C%20with%20their%20behavioral%20responses%20video-recorded%20and%20analyzed.%20We%20found%20both%20similarities%20and%20differences%20compared%20to%20adult%20responses%20from%20the%20original%20study.%20Like%20adults%2C%20children%20adjusted%20their%20prompts%2C%20modified%20their%20verbal%20tone%2C%20and%20exhibited%20increasingly%20emotional%20non-verbal%20responses%20throughout%20successive%20errors.%20However%2C%20children%20demonstrated%20more%20disengagement%20behaviors%2C%20including%20temporarily%20ignoring%20the%20robot%20or%20actively%20seeking%20an%20adult.%20Errors%20did%20not%20affect%20participants%27%20perception%20of%20the%20robot%2C%20suggesting%20more%20flexible%20conversational%20expectations%20in%20children.%20These%20findings%20inform%20the%20design%20of%20more%20effective%20and%20developmentally%20appropriate%20human-robot%20interaction%20systems%20for%20young%20users.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00754v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCalling%2520for%2520Backup%253A%2520How%2520Children%2520Navigate%2520Successive%2520Robot%2520Communication%2520Failures%26entry.906535625%3DMaria%2520Teresa%2520Parreira%2520and%2520Isabel%2520Neto%2520and%2520Filipa%2520Rocha%2520and%2520Wendy%2520Ju%26entry.1292438233%3DHow%2520do%2520children%2520respond%2520to%2520repeated%2520robot%2520errors%253F%2520While%2520prior%2520research%2520has%2520examined%2520adult%2520reactions%2520to%2520successive%2520robot%2520errors%252C%2520children%2527s%2520responses%2520remain%2520largely%2520unexplored.%2520In%2520this%2520study%252C%2520we%2520explore%2520children%2527s%2520reactions%2520to%2520robot%2520social%2520errors%2520and%2520performance%2520errors.%2520For%2520the%2520latter%252C%2520this%2520study%2520reproduces%2520the%2520successive%2520robot%2520failure%2520paradigm%2520of%2520Liu%2520et%2520al.%2520with%2520child%2520participants%2520%2528N%253D59%252C%2520ages%25208-10%2529%2520to%2520examine%2520how%2520young%2520users%2520respond%2520to%2520repeated%2520robot%2520conversational%2520errors.%2520Participants%2520interacted%2520with%2520a%2520robot%2520that%2520failed%2520to%2520understand%2520their%2520prompts%2520three%2520times%2520in%2520succession%252C%2520with%2520their%2520behavioral%2520responses%2520video-recorded%2520and%2520analyzed.%2520We%2520found%2520both%2520similarities%2520and%2520differences%2520compared%2520to%2520adult%2520responses%2520from%2520the%2520original%2520study.%2520Like%2520adults%252C%2520children%2520adjusted%2520their%2520prompts%252C%2520modified%2520their%2520verbal%2520tone%252C%2520and%2520exhibited%2520increasingly%2520emotional%2520non-verbal%2520responses%2520throughout%2520successive%2520errors.%2520However%252C%2520children%2520demonstrated%2520more%2520disengagement%2520behaviors%252C%2520including%2520temporarily%2520ignoring%2520the%2520robot%2520or%2520actively%2520seeking%2520an%2520adult.%2520Errors%2520did%2520not%2520affect%2520participants%2527%2520perception%2520of%2520the%2520robot%252C%2520suggesting%2520more%2520flexible%2520conversational%2520expectations%2520in%2520children.%2520These%2520findings%2520inform%2520the%2520design%2520of%2520more%2520effective%2520and%2520developmentally%2520appropriate%2520human-robot%2520interaction%2520systems%2520for%2520young%2520users.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00754v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Calling%20for%20Backup%3A%20How%20Children%20Navigate%20Successive%20Robot%20Communication%20Failures&entry.906535625=Maria%20Teresa%20Parreira%20and%20Isabel%20Neto%20and%20Filipa%20Rocha%20and%20Wendy%20Ju&entry.1292438233=How%20do%20children%20respond%20to%20repeated%20robot%20errors%3F%20While%20prior%20research%20has%20examined%20adult%20reactions%20to%20successive%20robot%20errors%2C%20children%27s%20responses%20remain%20largely%20unexplored.%20In%20this%20study%2C%20we%20explore%20children%27s%20reactions%20to%20robot%20social%20errors%20and%20performance%20errors.%20For%20the%20latter%2C%20this%20study%20reproduces%20the%20successive%20robot%20failure%20paradigm%20of%20Liu%20et%20al.%20with%20child%20participants%20%28N%3D59%2C%20ages%208-10%29%20to%20examine%20how%20young%20users%20respond%20to%20repeated%20robot%20conversational%20errors.%20Participants%20interacted%20with%20a%20robot%20that%20failed%20to%20understand%20their%20prompts%20three%20times%20in%20succession%2C%20with%20their%20behavioral%20responses%20video-recorded%20and%20analyzed.%20We%20found%20both%20similarities%20and%20differences%20compared%20to%20adult%20responses%20from%20the%20original%20study.%20Like%20adults%2C%20children%20adjusted%20their%20prompts%2C%20modified%20their%20verbal%20tone%2C%20and%20exhibited%20increasingly%20emotional%20non-verbal%20responses%20throughout%20successive%20errors.%20However%2C%20children%20demonstrated%20more%20disengagement%20behaviors%2C%20including%20temporarily%20ignoring%20the%20robot%20or%20actively%20seeking%20an%20adult.%20Errors%20did%20not%20affect%20participants%27%20perception%20of%20the%20robot%2C%20suggesting%20more%20flexible%20conversational%20expectations%20in%20children.%20These%20findings%20inform%20the%20design%20of%20more%20effective%20and%20developmentally%20appropriate%20human-robot%20interaction%20systems%20for%20young%20users.&entry.1838667208=http%3A//arxiv.org/abs/2601.00754v1&entry.124074799=Read"},
{"title": "Data-Driven Analysis of Crash Patterns in SAE Level 2 and Level 4 Automated Vehicles Using K-means Clustering and Association Rule Mining", "author": "Jewel Rana Palit and Vijayalakshmi K Kumarasamy and Osama A. Osman", "abstract": "Automated Vehicles (AV) hold potential to reduce or eliminate human driving errors, enhance traffic safety, and support sustainable mobility. Recently, crash data has increasingly revealed that AV behavior can deviate from expected safety outcomes, raising concerns about the technology's safety and operational reliability in mixed traffic environments. While past research has investigated AV crash, most studies rely on small-size California-centered datasets, with a limited focus on understanding crash trends across various SAE Levels of automation. This study analyzes over 2,500 AV crash records from the United States National Highway Traffic Safety Administration (NHTSA), covering SAE Levels 2 and 4, to uncover underlying crash dynamics. A two-stage data mining framework is developed. K-means clustering is first applied to segment crash records into 4 distinct behavioral clusters based on temporal, spatial, and environmental factors. Then, Association Rule Mining (ARM) is used to extract interpretable multivariate relationships between crash patterns and crash contributors including lighting conditions, surface condition, vehicle dynamics, and environmental conditions within each cluster. These insights provide actionable guidance for AV developers, safety regulators, and policymakers in formulating AV deployment strategies and minimizing crash risks.", "link": "http://arxiv.org/abs/2512.22589v2", "date": "2026-01-02", "relevancy": 1.7584, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4601}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4477}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Driven%20Analysis%20of%20Crash%20Patterns%20in%20SAE%20Level%202%20and%20Level%204%20Automated%20Vehicles%20Using%20K-means%20Clustering%20and%20Association%20Rule%20Mining&body=Title%3A%20Data-Driven%20Analysis%20of%20Crash%20Patterns%20in%20SAE%20Level%202%20and%20Level%204%20Automated%20Vehicles%20Using%20K-means%20Clustering%20and%20Association%20Rule%20Mining%0AAuthor%3A%20Jewel%20Rana%20Palit%20and%20Vijayalakshmi%20K%20Kumarasamy%20and%20Osama%20A.%20Osman%0AAbstract%3A%20Automated%20Vehicles%20%28AV%29%20hold%20potential%20to%20reduce%20or%20eliminate%20human%20driving%20errors%2C%20enhance%20traffic%20safety%2C%20and%20support%20sustainable%20mobility.%20Recently%2C%20crash%20data%20has%20increasingly%20revealed%20that%20AV%20behavior%20can%20deviate%20from%20expected%20safety%20outcomes%2C%20raising%20concerns%20about%20the%20technology%27s%20safety%20and%20operational%20reliability%20in%20mixed%20traffic%20environments.%20While%20past%20research%20has%20investigated%20AV%20crash%2C%20most%20studies%20rely%20on%20small-size%20California-centered%20datasets%2C%20with%20a%20limited%20focus%20on%20understanding%20crash%20trends%20across%20various%20SAE%20Levels%20of%20automation.%20This%20study%20analyzes%20over%202%2C500%20AV%20crash%20records%20from%20the%20United%20States%20National%20Highway%20Traffic%20Safety%20Administration%20%28NHTSA%29%2C%20covering%20SAE%20Levels%202%20and%204%2C%20to%20uncover%20underlying%20crash%20dynamics.%20A%20two-stage%20data%20mining%20framework%20is%20developed.%20K-means%20clustering%20is%20first%20applied%20to%20segment%20crash%20records%20into%204%20distinct%20behavioral%20clusters%20based%20on%20temporal%2C%20spatial%2C%20and%20environmental%20factors.%20Then%2C%20Association%20Rule%20Mining%20%28ARM%29%20is%20used%20to%20extract%20interpretable%20multivariate%20relationships%20between%20crash%20patterns%20and%20crash%20contributors%20including%20lighting%20conditions%2C%20surface%20condition%2C%20vehicle%20dynamics%2C%20and%20environmental%20conditions%20within%20each%20cluster.%20These%20insights%20provide%20actionable%20guidance%20for%20AV%20developers%2C%20safety%20regulators%2C%20and%20policymakers%20in%20formulating%20AV%20deployment%20strategies%20and%20minimizing%20crash%20risks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22589v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Driven%2520Analysis%2520of%2520Crash%2520Patterns%2520in%2520SAE%2520Level%25202%2520and%2520Level%25204%2520Automated%2520Vehicles%2520Using%2520K-means%2520Clustering%2520and%2520Association%2520Rule%2520Mining%26entry.906535625%3DJewel%2520Rana%2520Palit%2520and%2520Vijayalakshmi%2520K%2520Kumarasamy%2520and%2520Osama%2520A.%2520Osman%26entry.1292438233%3DAutomated%2520Vehicles%2520%2528AV%2529%2520hold%2520potential%2520to%2520reduce%2520or%2520eliminate%2520human%2520driving%2520errors%252C%2520enhance%2520traffic%2520safety%252C%2520and%2520support%2520sustainable%2520mobility.%2520Recently%252C%2520crash%2520data%2520has%2520increasingly%2520revealed%2520that%2520AV%2520behavior%2520can%2520deviate%2520from%2520expected%2520safety%2520outcomes%252C%2520raising%2520concerns%2520about%2520the%2520technology%2527s%2520safety%2520and%2520operational%2520reliability%2520in%2520mixed%2520traffic%2520environments.%2520While%2520past%2520research%2520has%2520investigated%2520AV%2520crash%252C%2520most%2520studies%2520rely%2520on%2520small-size%2520California-centered%2520datasets%252C%2520with%2520a%2520limited%2520focus%2520on%2520understanding%2520crash%2520trends%2520across%2520various%2520SAE%2520Levels%2520of%2520automation.%2520This%2520study%2520analyzes%2520over%25202%252C500%2520AV%2520crash%2520records%2520from%2520the%2520United%2520States%2520National%2520Highway%2520Traffic%2520Safety%2520Administration%2520%2528NHTSA%2529%252C%2520covering%2520SAE%2520Levels%25202%2520and%25204%252C%2520to%2520uncover%2520underlying%2520crash%2520dynamics.%2520A%2520two-stage%2520data%2520mining%2520framework%2520is%2520developed.%2520K-means%2520clustering%2520is%2520first%2520applied%2520to%2520segment%2520crash%2520records%2520into%25204%2520distinct%2520behavioral%2520clusters%2520based%2520on%2520temporal%252C%2520spatial%252C%2520and%2520environmental%2520factors.%2520Then%252C%2520Association%2520Rule%2520Mining%2520%2528ARM%2529%2520is%2520used%2520to%2520extract%2520interpretable%2520multivariate%2520relationships%2520between%2520crash%2520patterns%2520and%2520crash%2520contributors%2520including%2520lighting%2520conditions%252C%2520surface%2520condition%252C%2520vehicle%2520dynamics%252C%2520and%2520environmental%2520conditions%2520within%2520each%2520cluster.%2520These%2520insights%2520provide%2520actionable%2520guidance%2520for%2520AV%2520developers%252C%2520safety%2520regulators%252C%2520and%2520policymakers%2520in%2520formulating%2520AV%2520deployment%2520strategies%2520and%2520minimizing%2520crash%2520risks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22589v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Driven%20Analysis%20of%20Crash%20Patterns%20in%20SAE%20Level%202%20and%20Level%204%20Automated%20Vehicles%20Using%20K-means%20Clustering%20and%20Association%20Rule%20Mining&entry.906535625=Jewel%20Rana%20Palit%20and%20Vijayalakshmi%20K%20Kumarasamy%20and%20Osama%20A.%20Osman&entry.1292438233=Automated%20Vehicles%20%28AV%29%20hold%20potential%20to%20reduce%20or%20eliminate%20human%20driving%20errors%2C%20enhance%20traffic%20safety%2C%20and%20support%20sustainable%20mobility.%20Recently%2C%20crash%20data%20has%20increasingly%20revealed%20that%20AV%20behavior%20can%20deviate%20from%20expected%20safety%20outcomes%2C%20raising%20concerns%20about%20the%20technology%27s%20safety%20and%20operational%20reliability%20in%20mixed%20traffic%20environments.%20While%20past%20research%20has%20investigated%20AV%20crash%2C%20most%20studies%20rely%20on%20small-size%20California-centered%20datasets%2C%20with%20a%20limited%20focus%20on%20understanding%20crash%20trends%20across%20various%20SAE%20Levels%20of%20automation.%20This%20study%20analyzes%20over%202%2C500%20AV%20crash%20records%20from%20the%20United%20States%20National%20Highway%20Traffic%20Safety%20Administration%20%28NHTSA%29%2C%20covering%20SAE%20Levels%202%20and%204%2C%20to%20uncover%20underlying%20crash%20dynamics.%20A%20two-stage%20data%20mining%20framework%20is%20developed.%20K-means%20clustering%20is%20first%20applied%20to%20segment%20crash%20records%20into%204%20distinct%20behavioral%20clusters%20based%20on%20temporal%2C%20spatial%2C%20and%20environmental%20factors.%20Then%2C%20Association%20Rule%20Mining%20%28ARM%29%20is%20used%20to%20extract%20interpretable%20multivariate%20relationships%20between%20crash%20patterns%20and%20crash%20contributors%20including%20lighting%20conditions%2C%20surface%20condition%2C%20vehicle%20dynamics%2C%20and%20environmental%20conditions%20within%20each%20cluster.%20These%20insights%20provide%20actionable%20guidance%20for%20AV%20developers%2C%20safety%20regulators%2C%20and%20policymakers%20in%20formulating%20AV%20deployment%20strategies%20and%20minimizing%20crash%20risks.&entry.1838667208=http%3A//arxiv.org/abs/2512.22589v2&entry.124074799=Read"},
{"title": "Distributed Sparse Linear Regression under Communication Constraints", "author": "Rodney Fonseca and Boaz Nadler", "abstract": "In multiple domains, statistical tasks are performed in distributed settings, with data split among several end machines that are connected to a fusion center. In various applications, the end machines have limited bandwidth and power, and thus a tight communication budget. In this work we focus on distributed learning of a sparse linear regression model, under severe communication constraints. We propose several two round distributed schemes, whose communication per machine is sublinear in the data dimension. In our schemes, individual machines compute debiased lasso estimators, but send to the fusion center only very few values. On the theoretical front, we analyze one of these schemes and prove that with high probability it achieves exact support recovery at low signal to noise ratios, where individual machines fail to recover the support. We show in simulations that our scheme works as well as, and in some cases better, than more communication intensive approaches.", "link": "http://arxiv.org/abs/2301.04022v2", "date": "2026-01-02", "relevancy": 1.7546, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4593}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4398}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distributed%20Sparse%20Linear%20Regression%20under%20Communication%20Constraints&body=Title%3A%20Distributed%20Sparse%20Linear%20Regression%20under%20Communication%20Constraints%0AAuthor%3A%20Rodney%20Fonseca%20and%20Boaz%20Nadler%0AAbstract%3A%20In%20multiple%20domains%2C%20statistical%20tasks%20are%20performed%20in%20distributed%20settings%2C%20with%20data%20split%20among%20several%20end%20machines%20that%20are%20connected%20to%20a%20fusion%20center.%20In%20various%20applications%2C%20the%20end%20machines%20have%20limited%20bandwidth%20and%20power%2C%20and%20thus%20a%20tight%20communication%20budget.%20In%20this%20work%20we%20focus%20on%20distributed%20learning%20of%20a%20sparse%20linear%20regression%20model%2C%20under%20severe%20communication%20constraints.%20We%20propose%20several%20two%20round%20distributed%20schemes%2C%20whose%20communication%20per%20machine%20is%20sublinear%20in%20the%20data%20dimension.%20In%20our%20schemes%2C%20individual%20machines%20compute%20debiased%20lasso%20estimators%2C%20but%20send%20to%20the%20fusion%20center%20only%20very%20few%20values.%20On%20the%20theoretical%20front%2C%20we%20analyze%20one%20of%20these%20schemes%20and%20prove%20that%20with%20high%20probability%20it%20achieves%20exact%20support%20recovery%20at%20low%20signal%20to%20noise%20ratios%2C%20where%20individual%20machines%20fail%20to%20recover%20the%20support.%20We%20show%20in%20simulations%20that%20our%20scheme%20works%20as%20well%20as%2C%20and%20in%20some%20cases%20better%2C%20than%20more%20communication%20intensive%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2301.04022v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistributed%2520Sparse%2520Linear%2520Regression%2520under%2520Communication%2520Constraints%26entry.906535625%3DRodney%2520Fonseca%2520and%2520Boaz%2520Nadler%26entry.1292438233%3DIn%2520multiple%2520domains%252C%2520statistical%2520tasks%2520are%2520performed%2520in%2520distributed%2520settings%252C%2520with%2520data%2520split%2520among%2520several%2520end%2520machines%2520that%2520are%2520connected%2520to%2520a%2520fusion%2520center.%2520In%2520various%2520applications%252C%2520the%2520end%2520machines%2520have%2520limited%2520bandwidth%2520and%2520power%252C%2520and%2520thus%2520a%2520tight%2520communication%2520budget.%2520In%2520this%2520work%2520we%2520focus%2520on%2520distributed%2520learning%2520of%2520a%2520sparse%2520linear%2520regression%2520model%252C%2520under%2520severe%2520communication%2520constraints.%2520We%2520propose%2520several%2520two%2520round%2520distributed%2520schemes%252C%2520whose%2520communication%2520per%2520machine%2520is%2520sublinear%2520in%2520the%2520data%2520dimension.%2520In%2520our%2520schemes%252C%2520individual%2520machines%2520compute%2520debiased%2520lasso%2520estimators%252C%2520but%2520send%2520to%2520the%2520fusion%2520center%2520only%2520very%2520few%2520values.%2520On%2520the%2520theoretical%2520front%252C%2520we%2520analyze%2520one%2520of%2520these%2520schemes%2520and%2520prove%2520that%2520with%2520high%2520probability%2520it%2520achieves%2520exact%2520support%2520recovery%2520at%2520low%2520signal%2520to%2520noise%2520ratios%252C%2520where%2520individual%2520machines%2520fail%2520to%2520recover%2520the%2520support.%2520We%2520show%2520in%2520simulations%2520that%2520our%2520scheme%2520works%2520as%2520well%2520as%252C%2520and%2520in%2520some%2520cases%2520better%252C%2520than%2520more%2520communication%2520intensive%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.04022v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributed%20Sparse%20Linear%20Regression%20under%20Communication%20Constraints&entry.906535625=Rodney%20Fonseca%20and%20Boaz%20Nadler&entry.1292438233=In%20multiple%20domains%2C%20statistical%20tasks%20are%20performed%20in%20distributed%20settings%2C%20with%20data%20split%20among%20several%20end%20machines%20that%20are%20connected%20to%20a%20fusion%20center.%20In%20various%20applications%2C%20the%20end%20machines%20have%20limited%20bandwidth%20and%20power%2C%20and%20thus%20a%20tight%20communication%20budget.%20In%20this%20work%20we%20focus%20on%20distributed%20learning%20of%20a%20sparse%20linear%20regression%20model%2C%20under%20severe%20communication%20constraints.%20We%20propose%20several%20two%20round%20distributed%20schemes%2C%20whose%20communication%20per%20machine%20is%20sublinear%20in%20the%20data%20dimension.%20In%20our%20schemes%2C%20individual%20machines%20compute%20debiased%20lasso%20estimators%2C%20but%20send%20to%20the%20fusion%20center%20only%20very%20few%20values.%20On%20the%20theoretical%20front%2C%20we%20analyze%20one%20of%20these%20schemes%20and%20prove%20that%20with%20high%20probability%20it%20achieves%20exact%20support%20recovery%20at%20low%20signal%20to%20noise%20ratios%2C%20where%20individual%20machines%20fail%20to%20recover%20the%20support.%20We%20show%20in%20simulations%20that%20our%20scheme%20works%20as%20well%20as%2C%20and%20in%20some%20cases%20better%2C%20than%20more%20communication%20intensive%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2301.04022v2&entry.124074799=Read"},
{"title": "Semantic Anchor Transport: Robust Test-Time Adaptation for Vision-Language Models", "author": "Shambhavi Mishra and Julio Silva-Rodriguez and Ismail Ben Ayed and Marco Pedersoli and Jose Dolz", "abstract": "Large pre-trained vision-language models (VLMs), such as CLIP, have shown unprecedented zero-shot performance across a wide range of tasks. Nevertheless, these models may be unreliable under distributional shifts, as their performance is significantly degraded. In this work, we investigate how to efficiently utilize class text information to mitigate distribution drifts encountered by VLMs during inference. In particular, we propose generating pseudo-labels for the noisy test-time samples by aligning visual embeddings with reliable, text-based semantic anchors. Specifically, to maintain the regular structure of the dataset properly, we formulate the problem as a batch-wise label assignment, which is efficiently solved using Optimal Transport. Our method, Semantic Anchor Transport (SAT), utilizes such pseudo-labels as supervisory signals for test-time adaptation, yielding a principled cross-modal alignment solution. Moreover, SAT further leverages heterogeneous textual clues, with a multi-template distillation approach that replicates multi-view contrastive learning strategies in unsupervised representation learning without incurring additional computational complexity. Extensive experiments on multiple popular test-time adaptation benchmarks presenting diverse complexity empirically show the superiority of SAT, achieving consistent performance gains over recent state-of-the-art methods, yet being computationally efficient.", "link": "http://arxiv.org/abs/2411.17002v3", "date": "2026-01-02", "relevancy": 1.7515, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6159}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5471}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20Anchor%20Transport%3A%20Robust%20Test-Time%20Adaptation%20for%20Vision-Language%20Models&body=Title%3A%20Semantic%20Anchor%20Transport%3A%20Robust%20Test-Time%20Adaptation%20for%20Vision-Language%20Models%0AAuthor%3A%20Shambhavi%20Mishra%20and%20Julio%20Silva-Rodriguez%20and%20Ismail%20Ben%20Ayed%20and%20Marco%20Pedersoli%20and%20Jose%20Dolz%0AAbstract%3A%20Large%20pre-trained%20vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20have%20shown%20unprecedented%20zero-shot%20performance%20across%20a%20wide%20range%20of%20tasks.%20Nevertheless%2C%20these%20models%20may%20be%20unreliable%20under%20distributional%20shifts%2C%20as%20their%20performance%20is%20significantly%20degraded.%20In%20this%20work%2C%20we%20investigate%20how%20to%20efficiently%20utilize%20class%20text%20information%20to%20mitigate%20distribution%20drifts%20encountered%20by%20VLMs%20during%20inference.%20In%20particular%2C%20we%20propose%20generating%20pseudo-labels%20for%20the%20noisy%20test-time%20samples%20by%20aligning%20visual%20embeddings%20with%20reliable%2C%20text-based%20semantic%20anchors.%20Specifically%2C%20to%20maintain%20the%20regular%20structure%20of%20the%20dataset%20properly%2C%20we%20formulate%20the%20problem%20as%20a%20batch-wise%20label%20assignment%2C%20which%20is%20efficiently%20solved%20using%20Optimal%20Transport.%20Our%20method%2C%20Semantic%20Anchor%20Transport%20%28SAT%29%2C%20utilizes%20such%20pseudo-labels%20as%20supervisory%20signals%20for%20test-time%20adaptation%2C%20yielding%20a%20principled%20cross-modal%20alignment%20solution.%20Moreover%2C%20SAT%20further%20leverages%20heterogeneous%20textual%20clues%2C%20with%20a%20multi-template%20distillation%20approach%20that%20replicates%20multi-view%20contrastive%20learning%20strategies%20in%20unsupervised%20representation%20learning%20without%20incurring%20additional%20computational%20complexity.%20Extensive%20experiments%20on%20multiple%20popular%20test-time%20adaptation%20benchmarks%20presenting%20diverse%20complexity%20empirically%20show%20the%20superiority%20of%20SAT%2C%20achieving%20consistent%20performance%20gains%20over%20recent%20state-of-the-art%20methods%2C%20yet%20being%20computationally%20efficient.%0ALink%3A%20http%3A//arxiv.org/abs/2411.17002v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520Anchor%2520Transport%253A%2520Robust%2520Test-Time%2520Adaptation%2520for%2520Vision-Language%2520Models%26entry.906535625%3DShambhavi%2520Mishra%2520and%2520Julio%2520Silva-Rodriguez%2520and%2520Ismail%2520Ben%2520Ayed%2520and%2520Marco%2520Pedersoli%2520and%2520Jose%2520Dolz%26entry.1292438233%3DLarge%2520pre-trained%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520such%2520as%2520CLIP%252C%2520have%2520shown%2520unprecedented%2520zero-shot%2520performance%2520across%2520a%2520wide%2520range%2520of%2520tasks.%2520Nevertheless%252C%2520these%2520models%2520may%2520be%2520unreliable%2520under%2520distributional%2520shifts%252C%2520as%2520their%2520performance%2520is%2520significantly%2520degraded.%2520In%2520this%2520work%252C%2520we%2520investigate%2520how%2520to%2520efficiently%2520utilize%2520class%2520text%2520information%2520to%2520mitigate%2520distribution%2520drifts%2520encountered%2520by%2520VLMs%2520during%2520inference.%2520In%2520particular%252C%2520we%2520propose%2520generating%2520pseudo-labels%2520for%2520the%2520noisy%2520test-time%2520samples%2520by%2520aligning%2520visual%2520embeddings%2520with%2520reliable%252C%2520text-based%2520semantic%2520anchors.%2520Specifically%252C%2520to%2520maintain%2520the%2520regular%2520structure%2520of%2520the%2520dataset%2520properly%252C%2520we%2520formulate%2520the%2520problem%2520as%2520a%2520batch-wise%2520label%2520assignment%252C%2520which%2520is%2520efficiently%2520solved%2520using%2520Optimal%2520Transport.%2520Our%2520method%252C%2520Semantic%2520Anchor%2520Transport%2520%2528SAT%2529%252C%2520utilizes%2520such%2520pseudo-labels%2520as%2520supervisory%2520signals%2520for%2520test-time%2520adaptation%252C%2520yielding%2520a%2520principled%2520cross-modal%2520alignment%2520solution.%2520Moreover%252C%2520SAT%2520further%2520leverages%2520heterogeneous%2520textual%2520clues%252C%2520with%2520a%2520multi-template%2520distillation%2520approach%2520that%2520replicates%2520multi-view%2520contrastive%2520learning%2520strategies%2520in%2520unsupervised%2520representation%2520learning%2520without%2520incurring%2520additional%2520computational%2520complexity.%2520Extensive%2520experiments%2520on%2520multiple%2520popular%2520test-time%2520adaptation%2520benchmarks%2520presenting%2520diverse%2520complexity%2520empirically%2520show%2520the%2520superiority%2520of%2520SAT%252C%2520achieving%2520consistent%2520performance%2520gains%2520over%2520recent%2520state-of-the-art%2520methods%252C%2520yet%2520being%2520computationally%2520efficient.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17002v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Anchor%20Transport%3A%20Robust%20Test-Time%20Adaptation%20for%20Vision-Language%20Models&entry.906535625=Shambhavi%20Mishra%20and%20Julio%20Silva-Rodriguez%20and%20Ismail%20Ben%20Ayed%20and%20Marco%20Pedersoli%20and%20Jose%20Dolz&entry.1292438233=Large%20pre-trained%20vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20have%20shown%20unprecedented%20zero-shot%20performance%20across%20a%20wide%20range%20of%20tasks.%20Nevertheless%2C%20these%20models%20may%20be%20unreliable%20under%20distributional%20shifts%2C%20as%20their%20performance%20is%20significantly%20degraded.%20In%20this%20work%2C%20we%20investigate%20how%20to%20efficiently%20utilize%20class%20text%20information%20to%20mitigate%20distribution%20drifts%20encountered%20by%20VLMs%20during%20inference.%20In%20particular%2C%20we%20propose%20generating%20pseudo-labels%20for%20the%20noisy%20test-time%20samples%20by%20aligning%20visual%20embeddings%20with%20reliable%2C%20text-based%20semantic%20anchors.%20Specifically%2C%20to%20maintain%20the%20regular%20structure%20of%20the%20dataset%20properly%2C%20we%20formulate%20the%20problem%20as%20a%20batch-wise%20label%20assignment%2C%20which%20is%20efficiently%20solved%20using%20Optimal%20Transport.%20Our%20method%2C%20Semantic%20Anchor%20Transport%20%28SAT%29%2C%20utilizes%20such%20pseudo-labels%20as%20supervisory%20signals%20for%20test-time%20adaptation%2C%20yielding%20a%20principled%20cross-modal%20alignment%20solution.%20Moreover%2C%20SAT%20further%20leverages%20heterogeneous%20textual%20clues%2C%20with%20a%20multi-template%20distillation%20approach%20that%20replicates%20multi-view%20contrastive%20learning%20strategies%20in%20unsupervised%20representation%20learning%20without%20incurring%20additional%20computational%20complexity.%20Extensive%20experiments%20on%20multiple%20popular%20test-time%20adaptation%20benchmarks%20presenting%20diverse%20complexity%20empirically%20show%20the%20superiority%20of%20SAT%2C%20achieving%20consistent%20performance%20gains%20over%20recent%20state-of-the-art%20methods%2C%20yet%20being%20computationally%20efficient.&entry.1838667208=http%3A//arxiv.org/abs/2411.17002v3&entry.124074799=Read"},
{"title": "Fast-weight Product Key Memory", "author": "Tianyu Zhao and Llion Jones", "abstract": "Sequence modeling layers in modern language models typically face a trade-off between storage capacity and computational efficiency. While Softmax attention offers unbounded storage at prohibitive quadratic costs, linear variants provide efficiency but suffer from limited, fixed-size storage. We propose Fast-weight Product Key Memory (FwPKM), a novel architecture that resolves this tension by transforming the sparse Product Key Memory (PKM) from a static module into a dynamic, \"fast-weight\" episodic memory. Unlike PKM, FwPKM updates its parameters dynamically at both training and inference time via local chunk-level gradient descent, allowing the model to rapidly memorize and retrieve new key-value pairs from input sequences. Experiments reveal that FwPKM functions as an effective episodic memory that complements the semantic memory of standard modules, yielding significant perplexity reductions on long-context datasets. Notably, in Needle in a Haystack evaluations, FwPKM generalizes to 128K-token contexts despite being trained on only 4K-token sequences.", "link": "http://arxiv.org/abs/2601.00671v1", "date": "2026-01-02", "relevancy": 1.7129, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4426}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4371}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast-weight%20Product%20Key%20Memory&body=Title%3A%20Fast-weight%20Product%20Key%20Memory%0AAuthor%3A%20Tianyu%20Zhao%20and%20Llion%20Jones%0AAbstract%3A%20Sequence%20modeling%20layers%20in%20modern%20language%20models%20typically%20face%20a%20trade-off%20between%20storage%20capacity%20and%20computational%20efficiency.%20While%20Softmax%20attention%20offers%20unbounded%20storage%20at%20prohibitive%20quadratic%20costs%2C%20linear%20variants%20provide%20efficiency%20but%20suffer%20from%20limited%2C%20fixed-size%20storage.%20We%20propose%20Fast-weight%20Product%20Key%20Memory%20%28FwPKM%29%2C%20a%20novel%20architecture%20that%20resolves%20this%20tension%20by%20transforming%20the%20sparse%20Product%20Key%20Memory%20%28PKM%29%20from%20a%20static%20module%20into%20a%20dynamic%2C%20%22fast-weight%22%20episodic%20memory.%20Unlike%20PKM%2C%20FwPKM%20updates%20its%20parameters%20dynamically%20at%20both%20training%20and%20inference%20time%20via%20local%20chunk-level%20gradient%20descent%2C%20allowing%20the%20model%20to%20rapidly%20memorize%20and%20retrieve%20new%20key-value%20pairs%20from%20input%20sequences.%20Experiments%20reveal%20that%20FwPKM%20functions%20as%20an%20effective%20episodic%20memory%20that%20complements%20the%20semantic%20memory%20of%20standard%20modules%2C%20yielding%20significant%20perplexity%20reductions%20on%20long-context%20datasets.%20Notably%2C%20in%20Needle%20in%20a%20Haystack%20evaluations%2C%20FwPKM%20generalizes%20to%20128K-token%20contexts%20despite%20being%20trained%20on%20only%204K-token%20sequences.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00671v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast-weight%2520Product%2520Key%2520Memory%26entry.906535625%3DTianyu%2520Zhao%2520and%2520Llion%2520Jones%26entry.1292438233%3DSequence%2520modeling%2520layers%2520in%2520modern%2520language%2520models%2520typically%2520face%2520a%2520trade-off%2520between%2520storage%2520capacity%2520and%2520computational%2520efficiency.%2520While%2520Softmax%2520attention%2520offers%2520unbounded%2520storage%2520at%2520prohibitive%2520quadratic%2520costs%252C%2520linear%2520variants%2520provide%2520efficiency%2520but%2520suffer%2520from%2520limited%252C%2520fixed-size%2520storage.%2520We%2520propose%2520Fast-weight%2520Product%2520Key%2520Memory%2520%2528FwPKM%2529%252C%2520a%2520novel%2520architecture%2520that%2520resolves%2520this%2520tension%2520by%2520transforming%2520the%2520sparse%2520Product%2520Key%2520Memory%2520%2528PKM%2529%2520from%2520a%2520static%2520module%2520into%2520a%2520dynamic%252C%2520%2522fast-weight%2522%2520episodic%2520memory.%2520Unlike%2520PKM%252C%2520FwPKM%2520updates%2520its%2520parameters%2520dynamically%2520at%2520both%2520training%2520and%2520inference%2520time%2520via%2520local%2520chunk-level%2520gradient%2520descent%252C%2520allowing%2520the%2520model%2520to%2520rapidly%2520memorize%2520and%2520retrieve%2520new%2520key-value%2520pairs%2520from%2520input%2520sequences.%2520Experiments%2520reveal%2520that%2520FwPKM%2520functions%2520as%2520an%2520effective%2520episodic%2520memory%2520that%2520complements%2520the%2520semantic%2520memory%2520of%2520standard%2520modules%252C%2520yielding%2520significant%2520perplexity%2520reductions%2520on%2520long-context%2520datasets.%2520Notably%252C%2520in%2520Needle%2520in%2520a%2520Haystack%2520evaluations%252C%2520FwPKM%2520generalizes%2520to%2520128K-token%2520contexts%2520despite%2520being%2520trained%2520on%2520only%25204K-token%2520sequences.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00671v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast-weight%20Product%20Key%20Memory&entry.906535625=Tianyu%20Zhao%20and%20Llion%20Jones&entry.1292438233=Sequence%20modeling%20layers%20in%20modern%20language%20models%20typically%20face%20a%20trade-off%20between%20storage%20capacity%20and%20computational%20efficiency.%20While%20Softmax%20attention%20offers%20unbounded%20storage%20at%20prohibitive%20quadratic%20costs%2C%20linear%20variants%20provide%20efficiency%20but%20suffer%20from%20limited%2C%20fixed-size%20storage.%20We%20propose%20Fast-weight%20Product%20Key%20Memory%20%28FwPKM%29%2C%20a%20novel%20architecture%20that%20resolves%20this%20tension%20by%20transforming%20the%20sparse%20Product%20Key%20Memory%20%28PKM%29%20from%20a%20static%20module%20into%20a%20dynamic%2C%20%22fast-weight%22%20episodic%20memory.%20Unlike%20PKM%2C%20FwPKM%20updates%20its%20parameters%20dynamically%20at%20both%20training%20and%20inference%20time%20via%20local%20chunk-level%20gradient%20descent%2C%20allowing%20the%20model%20to%20rapidly%20memorize%20and%20retrieve%20new%20key-value%20pairs%20from%20input%20sequences.%20Experiments%20reveal%20that%20FwPKM%20functions%20as%20an%20effective%20episodic%20memory%20that%20complements%20the%20semantic%20memory%20of%20standard%20modules%2C%20yielding%20significant%20perplexity%20reductions%20on%20long-context%20datasets.%20Notably%2C%20in%20Needle%20in%20a%20Haystack%20evaluations%2C%20FwPKM%20generalizes%20to%20128K-token%20contexts%20despite%20being%20trained%20on%20only%204K-token%20sequences.&entry.1838667208=http%3A//arxiv.org/abs/2601.00671v1&entry.124074799=Read"},
{"title": "It's complicated. The relationship of algorithmic fairness and non-discrimination provisions for high-risk systems in the EU AI Act", "author": "Kristof Meding", "abstract": "What constitutes a fair decision? This question is not only difficult for humans but becomes more challenging when Artificial Intelligence (AI) models are used. In light of discriminatory algorithmic behaviors, the EU has recently passed the AI Act, which mandates specific rules for high-risk systems, incorporating both traditional legal non-discrimination regulations and machine learning based algorithmic fairness concepts. This paper aims to bridge these two different concepts in the AI Act through: First, a necessary high-level introduction of both concepts targeting legal and computer science-oriented scholars, and second, an in-depth analysis of the AI Act's relationship between legal non-discrimination regulations and algorithmic fairness. Our analysis reveals three key findings: (1.) Most non-discrimination regulations target only high-risk AI systems. (2.) The regulation of high-risk systems encompasses both data input requirements and output monitoring, though these regulations are partly inconsistent and raise questions of computational feasibility. (3.) Finally, we consider the possible (future) interaction of classical EU non-discrimination law and the AI Act regulations. We recommend developing more specific auditing and testing methodologies for AI systems. This paper aims to serve as a foundation for future interdisciplinary collaboration between legal scholars and computer science-oriented machine learning researchers studying discrimination in AI systems.", "link": "http://arxiv.org/abs/2501.12962v5", "date": "2026-01-02", "relevancy": 1.6828, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4539}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4198}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20It%27s%20complicated.%20The%20relationship%20of%20algorithmic%20fairness%20and%20non-discrimination%20provisions%20for%20high-risk%20systems%20in%20the%20EU%20AI%20Act&body=Title%3A%20It%27s%20complicated.%20The%20relationship%20of%20algorithmic%20fairness%20and%20non-discrimination%20provisions%20for%20high-risk%20systems%20in%20the%20EU%20AI%20Act%0AAuthor%3A%20Kristof%20Meding%0AAbstract%3A%20What%20constitutes%20a%20fair%20decision%3F%20This%20question%20is%20not%20only%20difficult%20for%20humans%20but%20becomes%20more%20challenging%20when%20Artificial%20Intelligence%20%28AI%29%20models%20are%20used.%20In%20light%20of%20discriminatory%20algorithmic%20behaviors%2C%20the%20EU%20has%20recently%20passed%20the%20AI%20Act%2C%20which%20mandates%20specific%20rules%20for%20high-risk%20systems%2C%20incorporating%20both%20traditional%20legal%20non-discrimination%20regulations%20and%20machine%20learning%20based%20algorithmic%20fairness%20concepts.%20This%20paper%20aims%20to%20bridge%20these%20two%20different%20concepts%20in%20the%20AI%20Act%20through%3A%20First%2C%20a%20necessary%20high-level%20introduction%20of%20both%20concepts%20targeting%20legal%20and%20computer%20science-oriented%20scholars%2C%20and%20second%2C%20an%20in-depth%20analysis%20of%20the%20AI%20Act%27s%20relationship%20between%20legal%20non-discrimination%20regulations%20and%20algorithmic%20fairness.%20Our%20analysis%20reveals%20three%20key%20findings%3A%20%281.%29%20Most%20non-discrimination%20regulations%20target%20only%20high-risk%20AI%20systems.%20%282.%29%20The%20regulation%20of%20high-risk%20systems%20encompasses%20both%20data%20input%20requirements%20and%20output%20monitoring%2C%20though%20these%20regulations%20are%20partly%20inconsistent%20and%20raise%20questions%20of%20computational%20feasibility.%20%283.%29%20Finally%2C%20we%20consider%20the%20possible%20%28future%29%20interaction%20of%20classical%20EU%20non-discrimination%20law%20and%20the%20AI%20Act%20regulations.%20We%20recommend%20developing%20more%20specific%20auditing%20and%20testing%20methodologies%20for%20AI%20systems.%20This%20paper%20aims%20to%20serve%20as%20a%20foundation%20for%20future%20interdisciplinary%20collaboration%20between%20legal%20scholars%20and%20computer%20science-oriented%20machine%20learning%20researchers%20studying%20discrimination%20in%20AI%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2501.12962v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIt%2527s%2520complicated.%2520The%2520relationship%2520of%2520algorithmic%2520fairness%2520and%2520non-discrimination%2520provisions%2520for%2520high-risk%2520systems%2520in%2520the%2520EU%2520AI%2520Act%26entry.906535625%3DKristof%2520Meding%26entry.1292438233%3DWhat%2520constitutes%2520a%2520fair%2520decision%253F%2520This%2520question%2520is%2520not%2520only%2520difficult%2520for%2520humans%2520but%2520becomes%2520more%2520challenging%2520when%2520Artificial%2520Intelligence%2520%2528AI%2529%2520models%2520are%2520used.%2520In%2520light%2520of%2520discriminatory%2520algorithmic%2520behaviors%252C%2520the%2520EU%2520has%2520recently%2520passed%2520the%2520AI%2520Act%252C%2520which%2520mandates%2520specific%2520rules%2520for%2520high-risk%2520systems%252C%2520incorporating%2520both%2520traditional%2520legal%2520non-discrimination%2520regulations%2520and%2520machine%2520learning%2520based%2520algorithmic%2520fairness%2520concepts.%2520This%2520paper%2520aims%2520to%2520bridge%2520these%2520two%2520different%2520concepts%2520in%2520the%2520AI%2520Act%2520through%253A%2520First%252C%2520a%2520necessary%2520high-level%2520introduction%2520of%2520both%2520concepts%2520targeting%2520legal%2520and%2520computer%2520science-oriented%2520scholars%252C%2520and%2520second%252C%2520an%2520in-depth%2520analysis%2520of%2520the%2520AI%2520Act%2527s%2520relationship%2520between%2520legal%2520non-discrimination%2520regulations%2520and%2520algorithmic%2520fairness.%2520Our%2520analysis%2520reveals%2520three%2520key%2520findings%253A%2520%25281.%2529%2520Most%2520non-discrimination%2520regulations%2520target%2520only%2520high-risk%2520AI%2520systems.%2520%25282.%2529%2520The%2520regulation%2520of%2520high-risk%2520systems%2520encompasses%2520both%2520data%2520input%2520requirements%2520and%2520output%2520monitoring%252C%2520though%2520these%2520regulations%2520are%2520partly%2520inconsistent%2520and%2520raise%2520questions%2520of%2520computational%2520feasibility.%2520%25283.%2529%2520Finally%252C%2520we%2520consider%2520the%2520possible%2520%2528future%2529%2520interaction%2520of%2520classical%2520EU%2520non-discrimination%2520law%2520and%2520the%2520AI%2520Act%2520regulations.%2520We%2520recommend%2520developing%2520more%2520specific%2520auditing%2520and%2520testing%2520methodologies%2520for%2520AI%2520systems.%2520This%2520paper%2520aims%2520to%2520serve%2520as%2520a%2520foundation%2520for%2520future%2520interdisciplinary%2520collaboration%2520between%2520legal%2520scholars%2520and%2520computer%2520science-oriented%2520machine%2520learning%2520researchers%2520studying%2520discrimination%2520in%2520AI%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12962v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=It%27s%20complicated.%20The%20relationship%20of%20algorithmic%20fairness%20and%20non-discrimination%20provisions%20for%20high-risk%20systems%20in%20the%20EU%20AI%20Act&entry.906535625=Kristof%20Meding&entry.1292438233=What%20constitutes%20a%20fair%20decision%3F%20This%20question%20is%20not%20only%20difficult%20for%20humans%20but%20becomes%20more%20challenging%20when%20Artificial%20Intelligence%20%28AI%29%20models%20are%20used.%20In%20light%20of%20discriminatory%20algorithmic%20behaviors%2C%20the%20EU%20has%20recently%20passed%20the%20AI%20Act%2C%20which%20mandates%20specific%20rules%20for%20high-risk%20systems%2C%20incorporating%20both%20traditional%20legal%20non-discrimination%20regulations%20and%20machine%20learning%20based%20algorithmic%20fairness%20concepts.%20This%20paper%20aims%20to%20bridge%20these%20two%20different%20concepts%20in%20the%20AI%20Act%20through%3A%20First%2C%20a%20necessary%20high-level%20introduction%20of%20both%20concepts%20targeting%20legal%20and%20computer%20science-oriented%20scholars%2C%20and%20second%2C%20an%20in-depth%20analysis%20of%20the%20AI%20Act%27s%20relationship%20between%20legal%20non-discrimination%20regulations%20and%20algorithmic%20fairness.%20Our%20analysis%20reveals%20three%20key%20findings%3A%20%281.%29%20Most%20non-discrimination%20regulations%20target%20only%20high-risk%20AI%20systems.%20%282.%29%20The%20regulation%20of%20high-risk%20systems%20encompasses%20both%20data%20input%20requirements%20and%20output%20monitoring%2C%20though%20these%20regulations%20are%20partly%20inconsistent%20and%20raise%20questions%20of%20computational%20feasibility.%20%283.%29%20Finally%2C%20we%20consider%20the%20possible%20%28future%29%20interaction%20of%20classical%20EU%20non-discrimination%20law%20and%20the%20AI%20Act%20regulations.%20We%20recommend%20developing%20more%20specific%20auditing%20and%20testing%20methodologies%20for%20AI%20systems.%20This%20paper%20aims%20to%20serve%20as%20a%20foundation%20for%20future%20interdisciplinary%20collaboration%20between%20legal%20scholars%20and%20computer%20science-oriented%20machine%20learning%20researchers%20studying%20discrimination%20in%20AI%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2501.12962v5&entry.124074799=Read"},
{"title": "Cost Optimization in Production Line Using Genetic Algorithm", "author": "Alireza Rezaee", "abstract": "This paper presents a genetic algorithm (GA) approach to cost-optimal task scheduling in a production line. The system consists of a set of serial processing tasks, each with a given duration, unit execution cost, and precedence constraints, which must be assigned to an unlimited number of stations subject to a per-station duration bound. The objective is to minimize the total production cost, modeled as a station-wise function of task costs and the duration bound, while strictly satisfying all prerequisite and capacity constraints. Two chromosome encoding strategies are investigated: a station-based representation implemented using the JGAP library with SuperGene validity checks, and a task-based representation in which genes encode station assignments directly. For each encoding, standard GA operators (crossover, mutation, selection, and replacement) are adapted to preserve feasibility and drive the population toward lower-cost schedules. Experimental results on three classes of precedence structures-tightly coupled, loosely coupled, and uncoupled-demonstrate that the task-based encoding yields smoother convergence and more reliable cost minimization than the station-based encoding, particularly when the number of valid schedules is large. The study highlights the advantages of GA over gradient-based and analytical methods for combinatorial scheduling problems, especially in the presence of complex constraints and non-differentiable cost landscapes.", "link": "http://arxiv.org/abs/2601.00689v1", "date": "2026-01-02", "relevancy": 1.6301, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4172}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4044}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.3913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cost%20Optimization%20in%20Production%20Line%20Using%20Genetic%20Algorithm&body=Title%3A%20Cost%20Optimization%20in%20Production%20Line%20Using%20Genetic%20Algorithm%0AAuthor%3A%20Alireza%20Rezaee%0AAbstract%3A%20This%20paper%20presents%20a%20genetic%20algorithm%20%28GA%29%20approach%20to%20cost-optimal%20task%20scheduling%20in%20a%20production%20line.%20The%20system%20consists%20of%20a%20set%20of%20serial%20processing%20tasks%2C%20each%20with%20a%20given%20duration%2C%20unit%20execution%20cost%2C%20and%20precedence%20constraints%2C%20which%20must%20be%20assigned%20to%20an%20unlimited%20number%20of%20stations%20subject%20to%20a%20per-station%20duration%20bound.%20The%20objective%20is%20to%20minimize%20the%20total%20production%20cost%2C%20modeled%20as%20a%20station-wise%20function%20of%20task%20costs%20and%20the%20duration%20bound%2C%20while%20strictly%20satisfying%20all%20prerequisite%20and%20capacity%20constraints.%20Two%20chromosome%20encoding%20strategies%20are%20investigated%3A%20a%20station-based%20representation%20implemented%20using%20the%20JGAP%20library%20with%20SuperGene%20validity%20checks%2C%20and%20a%20task-based%20representation%20in%20which%20genes%20encode%20station%20assignments%20directly.%20For%20each%20encoding%2C%20standard%20GA%20operators%20%28crossover%2C%20mutation%2C%20selection%2C%20and%20replacement%29%20are%20adapted%20to%20preserve%20feasibility%20and%20drive%20the%20population%20toward%20lower-cost%20schedules.%20Experimental%20results%20on%20three%20classes%20of%20precedence%20structures-tightly%20coupled%2C%20loosely%20coupled%2C%20and%20uncoupled-demonstrate%20that%20the%20task-based%20encoding%20yields%20smoother%20convergence%20and%20more%20reliable%20cost%20minimization%20than%20the%20station-based%20encoding%2C%20particularly%20when%20the%20number%20of%20valid%20schedules%20is%20large.%20The%20study%20highlights%20the%20advantages%20of%20GA%20over%20gradient-based%20and%20analytical%20methods%20for%20combinatorial%20scheduling%20problems%2C%20especially%20in%20the%20presence%20of%20complex%20constraints%20and%20non-differentiable%20cost%20landscapes.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00689v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCost%2520Optimization%2520in%2520Production%2520Line%2520Using%2520Genetic%2520Algorithm%26entry.906535625%3DAlireza%2520Rezaee%26entry.1292438233%3DThis%2520paper%2520presents%2520a%2520genetic%2520algorithm%2520%2528GA%2529%2520approach%2520to%2520cost-optimal%2520task%2520scheduling%2520in%2520a%2520production%2520line.%2520The%2520system%2520consists%2520of%2520a%2520set%2520of%2520serial%2520processing%2520tasks%252C%2520each%2520with%2520a%2520given%2520duration%252C%2520unit%2520execution%2520cost%252C%2520and%2520precedence%2520constraints%252C%2520which%2520must%2520be%2520assigned%2520to%2520an%2520unlimited%2520number%2520of%2520stations%2520subject%2520to%2520a%2520per-station%2520duration%2520bound.%2520The%2520objective%2520is%2520to%2520minimize%2520the%2520total%2520production%2520cost%252C%2520modeled%2520as%2520a%2520station-wise%2520function%2520of%2520task%2520costs%2520and%2520the%2520duration%2520bound%252C%2520while%2520strictly%2520satisfying%2520all%2520prerequisite%2520and%2520capacity%2520constraints.%2520Two%2520chromosome%2520encoding%2520strategies%2520are%2520investigated%253A%2520a%2520station-based%2520representation%2520implemented%2520using%2520the%2520JGAP%2520library%2520with%2520SuperGene%2520validity%2520checks%252C%2520and%2520a%2520task-based%2520representation%2520in%2520which%2520genes%2520encode%2520station%2520assignments%2520directly.%2520For%2520each%2520encoding%252C%2520standard%2520GA%2520operators%2520%2528crossover%252C%2520mutation%252C%2520selection%252C%2520and%2520replacement%2529%2520are%2520adapted%2520to%2520preserve%2520feasibility%2520and%2520drive%2520the%2520population%2520toward%2520lower-cost%2520schedules.%2520Experimental%2520results%2520on%2520three%2520classes%2520of%2520precedence%2520structures-tightly%2520coupled%252C%2520loosely%2520coupled%252C%2520and%2520uncoupled-demonstrate%2520that%2520the%2520task-based%2520encoding%2520yields%2520smoother%2520convergence%2520and%2520more%2520reliable%2520cost%2520minimization%2520than%2520the%2520station-based%2520encoding%252C%2520particularly%2520when%2520the%2520number%2520of%2520valid%2520schedules%2520is%2520large.%2520The%2520study%2520highlights%2520the%2520advantages%2520of%2520GA%2520over%2520gradient-based%2520and%2520analytical%2520methods%2520for%2520combinatorial%2520scheduling%2520problems%252C%2520especially%2520in%2520the%2520presence%2520of%2520complex%2520constraints%2520and%2520non-differentiable%2520cost%2520landscapes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00689v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cost%20Optimization%20in%20Production%20Line%20Using%20Genetic%20Algorithm&entry.906535625=Alireza%20Rezaee&entry.1292438233=This%20paper%20presents%20a%20genetic%20algorithm%20%28GA%29%20approach%20to%20cost-optimal%20task%20scheduling%20in%20a%20production%20line.%20The%20system%20consists%20of%20a%20set%20of%20serial%20processing%20tasks%2C%20each%20with%20a%20given%20duration%2C%20unit%20execution%20cost%2C%20and%20precedence%20constraints%2C%20which%20must%20be%20assigned%20to%20an%20unlimited%20number%20of%20stations%20subject%20to%20a%20per-station%20duration%20bound.%20The%20objective%20is%20to%20minimize%20the%20total%20production%20cost%2C%20modeled%20as%20a%20station-wise%20function%20of%20task%20costs%20and%20the%20duration%20bound%2C%20while%20strictly%20satisfying%20all%20prerequisite%20and%20capacity%20constraints.%20Two%20chromosome%20encoding%20strategies%20are%20investigated%3A%20a%20station-based%20representation%20implemented%20using%20the%20JGAP%20library%20with%20SuperGene%20validity%20checks%2C%20and%20a%20task-based%20representation%20in%20which%20genes%20encode%20station%20assignments%20directly.%20For%20each%20encoding%2C%20standard%20GA%20operators%20%28crossover%2C%20mutation%2C%20selection%2C%20and%20replacement%29%20are%20adapted%20to%20preserve%20feasibility%20and%20drive%20the%20population%20toward%20lower-cost%20schedules.%20Experimental%20results%20on%20three%20classes%20of%20precedence%20structures-tightly%20coupled%2C%20loosely%20coupled%2C%20and%20uncoupled-demonstrate%20that%20the%20task-based%20encoding%20yields%20smoother%20convergence%20and%20more%20reliable%20cost%20minimization%20than%20the%20station-based%20encoding%2C%20particularly%20when%20the%20number%20of%20valid%20schedules%20is%20large.%20The%20study%20highlights%20the%20advantages%20of%20GA%20over%20gradient-based%20and%20analytical%20methods%20for%20combinatorial%20scheduling%20problems%2C%20especially%20in%20the%20presence%20of%20complex%20constraints%20and%20non-differentiable%20cost%20landscapes.&entry.1838667208=http%3A//arxiv.org/abs/2601.00689v1&entry.124074799=Read"},
{"title": "RoboReward: General-Purpose Vision-Language Reward Models for Robotics", "author": "Tony Lee and Andrew Wagenmaker and Karl Pertsch and Percy Liang and Sergey Levine and Chelsea Finn", "abstract": "A well-designed reward is critical for effective reinforcement learning-based policy improvement. In real-world robotic domains, obtaining such rewards typically requires either labor-intensive human labeling or brittle, handcrafted objectives. Vision-language models (VLMs) have shown promise as automatic reward models, yet their effectiveness on real robot tasks is poorly understood. In this work, we aim to close this gap by introducing (1) \\textbf{RoboReward}, a robotics reward dataset and benchmark built on large-scale real-robot corpora from Open X-Embodiment (OXE) and RoboArena, and (2) vision-language reward models trained on this dataset (RoboReward 4B/8B). Because OXE is success-heavy and lacks failure examples, we propose a \\emph{negative examples data augmentation} pipeline that generates calibrated \\emph{negatives} and \\emph{near-misses} via counterfactual relabeling of successful episodes and temporal clipping to create partial-progress outcomes from the same videos. Using this framework, we produce an extensive training and evaluation dataset that spans diverse tasks and embodiments and enables systematic evaluation of whether state-of-the-art VLMs can reliably provide rewards for robotics. Our evaluation of leading open-weight and proprietary VLMs reveals that no model excels across all tasks, underscoring substantial room for improvement. We then train general-purpose 4B- and 8B-parameter models that outperform much larger VLMs in assigning rewards for short-horizon robotic tasks. Finally, we deploy the 8B-parameter reward VLM in real-robot reinforcement learning and find that it improves policy learning over Gemini Robotics-ER 1.5, a frontier physical reasoning VLM trained on robotics data, by a large margin, while substantially narrowing the gap to RL training with human-provided rewards.", "link": "http://arxiv.org/abs/2601.00675v1", "date": "2026-01-02", "relevancy": 1.6141, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5504}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.549}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoboReward%3A%20General-Purpose%20Vision-Language%20Reward%20Models%20for%20Robotics&body=Title%3A%20RoboReward%3A%20General-Purpose%20Vision-Language%20Reward%20Models%20for%20Robotics%0AAuthor%3A%20Tony%20Lee%20and%20Andrew%20Wagenmaker%20and%20Karl%20Pertsch%20and%20Percy%20Liang%20and%20Sergey%20Levine%20and%20Chelsea%20Finn%0AAbstract%3A%20A%20well-designed%20reward%20is%20critical%20for%20effective%20reinforcement%20learning-based%20policy%20improvement.%20In%20real-world%20robotic%20domains%2C%20obtaining%20such%20rewards%20typically%20requires%20either%20labor-intensive%20human%20labeling%20or%20brittle%2C%20handcrafted%20objectives.%20Vision-language%20models%20%28VLMs%29%20have%20shown%20promise%20as%20automatic%20reward%20models%2C%20yet%20their%20effectiveness%20on%20real%20robot%20tasks%20is%20poorly%20understood.%20In%20this%20work%2C%20we%20aim%20to%20close%20this%20gap%20by%20introducing%20%281%29%20%5Ctextbf%7BRoboReward%7D%2C%20a%20robotics%20reward%20dataset%20and%20benchmark%20built%20on%20large-scale%20real-robot%20corpora%20from%20Open%20X-Embodiment%20%28OXE%29%20and%20RoboArena%2C%20and%20%282%29%20vision-language%20reward%20models%20trained%20on%20this%20dataset%20%28RoboReward%204B/8B%29.%20Because%20OXE%20is%20success-heavy%20and%20lacks%20failure%20examples%2C%20we%20propose%20a%20%5Cemph%7Bnegative%20examples%20data%20augmentation%7D%20pipeline%20that%20generates%20calibrated%20%5Cemph%7Bnegatives%7D%20and%20%5Cemph%7Bnear-misses%7D%20via%20counterfactual%20relabeling%20of%20successful%20episodes%20and%20temporal%20clipping%20to%20create%20partial-progress%20outcomes%20from%20the%20same%20videos.%20Using%20this%20framework%2C%20we%20produce%20an%20extensive%20training%20and%20evaluation%20dataset%20that%20spans%20diverse%20tasks%20and%20embodiments%20and%20enables%20systematic%20evaluation%20of%20whether%20state-of-the-art%20VLMs%20can%20reliably%20provide%20rewards%20for%20robotics.%20Our%20evaluation%20of%20leading%20open-weight%20and%20proprietary%20VLMs%20reveals%20that%20no%20model%20excels%20across%20all%20tasks%2C%20underscoring%20substantial%20room%20for%20improvement.%20We%20then%20train%20general-purpose%204B-%20and%208B-parameter%20models%20that%20outperform%20much%20larger%20VLMs%20in%20assigning%20rewards%20for%20short-horizon%20robotic%20tasks.%20Finally%2C%20we%20deploy%20the%208B-parameter%20reward%20VLM%20in%20real-robot%20reinforcement%20learning%20and%20find%20that%20it%20improves%20policy%20learning%20over%20Gemini%20Robotics-ER%201.5%2C%20a%20frontier%20physical%20reasoning%20VLM%20trained%20on%20robotics%20data%2C%20by%20a%20large%20margin%2C%20while%20substantially%20narrowing%20the%20gap%20to%20RL%20training%20with%20human-provided%20rewards.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00675v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoboReward%253A%2520General-Purpose%2520Vision-Language%2520Reward%2520Models%2520for%2520Robotics%26entry.906535625%3DTony%2520Lee%2520and%2520Andrew%2520Wagenmaker%2520and%2520Karl%2520Pertsch%2520and%2520Percy%2520Liang%2520and%2520Sergey%2520Levine%2520and%2520Chelsea%2520Finn%26entry.1292438233%3DA%2520well-designed%2520reward%2520is%2520critical%2520for%2520effective%2520reinforcement%2520learning-based%2520policy%2520improvement.%2520In%2520real-world%2520robotic%2520domains%252C%2520obtaining%2520such%2520rewards%2520typically%2520requires%2520either%2520labor-intensive%2520human%2520labeling%2520or%2520brittle%252C%2520handcrafted%2520objectives.%2520Vision-language%2520models%2520%2528VLMs%2529%2520have%2520shown%2520promise%2520as%2520automatic%2520reward%2520models%252C%2520yet%2520their%2520effectiveness%2520on%2520real%2520robot%2520tasks%2520is%2520poorly%2520understood.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520close%2520this%2520gap%2520by%2520introducing%2520%25281%2529%2520%255Ctextbf%257BRoboReward%257D%252C%2520a%2520robotics%2520reward%2520dataset%2520and%2520benchmark%2520built%2520on%2520large-scale%2520real-robot%2520corpora%2520from%2520Open%2520X-Embodiment%2520%2528OXE%2529%2520and%2520RoboArena%252C%2520and%2520%25282%2529%2520vision-language%2520reward%2520models%2520trained%2520on%2520this%2520dataset%2520%2528RoboReward%25204B/8B%2529.%2520Because%2520OXE%2520is%2520success-heavy%2520and%2520lacks%2520failure%2520examples%252C%2520we%2520propose%2520a%2520%255Cemph%257Bnegative%2520examples%2520data%2520augmentation%257D%2520pipeline%2520that%2520generates%2520calibrated%2520%255Cemph%257Bnegatives%257D%2520and%2520%255Cemph%257Bnear-misses%257D%2520via%2520counterfactual%2520relabeling%2520of%2520successful%2520episodes%2520and%2520temporal%2520clipping%2520to%2520create%2520partial-progress%2520outcomes%2520from%2520the%2520same%2520videos.%2520Using%2520this%2520framework%252C%2520we%2520produce%2520an%2520extensive%2520training%2520and%2520evaluation%2520dataset%2520that%2520spans%2520diverse%2520tasks%2520and%2520embodiments%2520and%2520enables%2520systematic%2520evaluation%2520of%2520whether%2520state-of-the-art%2520VLMs%2520can%2520reliably%2520provide%2520rewards%2520for%2520robotics.%2520Our%2520evaluation%2520of%2520leading%2520open-weight%2520and%2520proprietary%2520VLMs%2520reveals%2520that%2520no%2520model%2520excels%2520across%2520all%2520tasks%252C%2520underscoring%2520substantial%2520room%2520for%2520improvement.%2520We%2520then%2520train%2520general-purpose%25204B-%2520and%25208B-parameter%2520models%2520that%2520outperform%2520much%2520larger%2520VLMs%2520in%2520assigning%2520rewards%2520for%2520short-horizon%2520robotic%2520tasks.%2520Finally%252C%2520we%2520deploy%2520the%25208B-parameter%2520reward%2520VLM%2520in%2520real-robot%2520reinforcement%2520learning%2520and%2520find%2520that%2520it%2520improves%2520policy%2520learning%2520over%2520Gemini%2520Robotics-ER%25201.5%252C%2520a%2520frontier%2520physical%2520reasoning%2520VLM%2520trained%2520on%2520robotics%2520data%252C%2520by%2520a%2520large%2520margin%252C%2520while%2520substantially%2520narrowing%2520the%2520gap%2520to%2520RL%2520training%2520with%2520human-provided%2520rewards.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00675v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoboReward%3A%20General-Purpose%20Vision-Language%20Reward%20Models%20for%20Robotics&entry.906535625=Tony%20Lee%20and%20Andrew%20Wagenmaker%20and%20Karl%20Pertsch%20and%20Percy%20Liang%20and%20Sergey%20Levine%20and%20Chelsea%20Finn&entry.1292438233=A%20well-designed%20reward%20is%20critical%20for%20effective%20reinforcement%20learning-based%20policy%20improvement.%20In%20real-world%20robotic%20domains%2C%20obtaining%20such%20rewards%20typically%20requires%20either%20labor-intensive%20human%20labeling%20or%20brittle%2C%20handcrafted%20objectives.%20Vision-language%20models%20%28VLMs%29%20have%20shown%20promise%20as%20automatic%20reward%20models%2C%20yet%20their%20effectiveness%20on%20real%20robot%20tasks%20is%20poorly%20understood.%20In%20this%20work%2C%20we%20aim%20to%20close%20this%20gap%20by%20introducing%20%281%29%20%5Ctextbf%7BRoboReward%7D%2C%20a%20robotics%20reward%20dataset%20and%20benchmark%20built%20on%20large-scale%20real-robot%20corpora%20from%20Open%20X-Embodiment%20%28OXE%29%20and%20RoboArena%2C%20and%20%282%29%20vision-language%20reward%20models%20trained%20on%20this%20dataset%20%28RoboReward%204B/8B%29.%20Because%20OXE%20is%20success-heavy%20and%20lacks%20failure%20examples%2C%20we%20propose%20a%20%5Cemph%7Bnegative%20examples%20data%20augmentation%7D%20pipeline%20that%20generates%20calibrated%20%5Cemph%7Bnegatives%7D%20and%20%5Cemph%7Bnear-misses%7D%20via%20counterfactual%20relabeling%20of%20successful%20episodes%20and%20temporal%20clipping%20to%20create%20partial-progress%20outcomes%20from%20the%20same%20videos.%20Using%20this%20framework%2C%20we%20produce%20an%20extensive%20training%20and%20evaluation%20dataset%20that%20spans%20diverse%20tasks%20and%20embodiments%20and%20enables%20systematic%20evaluation%20of%20whether%20state-of-the-art%20VLMs%20can%20reliably%20provide%20rewards%20for%20robotics.%20Our%20evaluation%20of%20leading%20open-weight%20and%20proprietary%20VLMs%20reveals%20that%20no%20model%20excels%20across%20all%20tasks%2C%20underscoring%20substantial%20room%20for%20improvement.%20We%20then%20train%20general-purpose%204B-%20and%208B-parameter%20models%20that%20outperform%20much%20larger%20VLMs%20in%20assigning%20rewards%20for%20short-horizon%20robotic%20tasks.%20Finally%2C%20we%20deploy%20the%208B-parameter%20reward%20VLM%20in%20real-robot%20reinforcement%20learning%20and%20find%20that%20it%20improves%20policy%20learning%20over%20Gemini%20Robotics-ER%201.5%2C%20a%20frontier%20physical%20reasoning%20VLM%20trained%20on%20robotics%20data%2C%20by%20a%20large%20margin%2C%20while%20substantially%20narrowing%20the%20gap%20to%20RL%20training%20with%20human-provided%20rewards.&entry.1838667208=http%3A//arxiv.org/abs/2601.00675v1&entry.124074799=Read"},
{"title": "Flattening Hierarchies with Policy Bootstrapping", "author": "John L. Zhou and Jonathan C. Kao", "abstract": "Offline goal-conditioned reinforcement learning (GCRL) is a promising approach for pretraining generalist policies on large datasets of reward-free trajectories, akin to the self-supervised objectives used to train foundation models for computer vision and natural language processing. However, scaling GCRL to longer horizons remains challenging due to the combination of sparse rewards and discounting, which obscures the comparative advantages of primitive actions with respect to distant goals. Hierarchical RL methods achieve strong empirical results on long-horizon goal-reaching tasks, but their reliance on modular, timescale-specific policies and subgoal generation introduces significant additional complexity and hinders scaling to high-dimensional goal spaces. In this work, we introduce an algorithm to train a flat (non-hierarchical) goal-conditioned policy by bootstrapping on subgoal-conditioned policies with advantage-weighted importance sampling. Our approach eliminates the need for a generative model over the (sub)goal space, which we find is key for scaling to high-dimensional control in large state spaces. We further show that existing hierarchical and bootstrapping-based approaches correspond to specific design choices within our derivation. Across a comprehensive suite of state- and pixel-based locomotion and manipulation benchmarks, our method matches or surpasses state-of-the-art offline GCRL algorithms and scales to complex, long-horizon tasks where prior approaches fail. Project page: https://johnlyzhou.github.io/saw/", "link": "http://arxiv.org/abs/2505.14975v3", "date": "2026-01-02", "relevancy": 1.5962, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5442}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5203}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5135}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flattening%20Hierarchies%20with%20Policy%20Bootstrapping&body=Title%3A%20Flattening%20Hierarchies%20with%20Policy%20Bootstrapping%0AAuthor%3A%20John%20L.%20Zhou%20and%20Jonathan%20C.%20Kao%0AAbstract%3A%20Offline%20goal-conditioned%20reinforcement%20learning%20%28GCRL%29%20is%20a%20promising%20approach%20for%20pretraining%20generalist%20policies%20on%20large%20datasets%20of%20reward-free%20trajectories%2C%20akin%20to%20the%20self-supervised%20objectives%20used%20to%20train%20foundation%20models%20for%20computer%20vision%20and%20natural%20language%20processing.%20However%2C%20scaling%20GCRL%20to%20longer%20horizons%20remains%20challenging%20due%20to%20the%20combination%20of%20sparse%20rewards%20and%20discounting%2C%20which%20obscures%20the%20comparative%20advantages%20of%20primitive%20actions%20with%20respect%20to%20distant%20goals.%20Hierarchical%20RL%20methods%20achieve%20strong%20empirical%20results%20on%20long-horizon%20goal-reaching%20tasks%2C%20but%20their%20reliance%20on%20modular%2C%20timescale-specific%20policies%20and%20subgoal%20generation%20introduces%20significant%20additional%20complexity%20and%20hinders%20scaling%20to%20high-dimensional%20goal%20spaces.%20In%20this%20work%2C%20we%20introduce%20an%20algorithm%20to%20train%20a%20flat%20%28non-hierarchical%29%20goal-conditioned%20policy%20by%20bootstrapping%20on%20subgoal-conditioned%20policies%20with%20advantage-weighted%20importance%20sampling.%20Our%20approach%20eliminates%20the%20need%20for%20a%20generative%20model%20over%20the%20%28sub%29goal%20space%2C%20which%20we%20find%20is%20key%20for%20scaling%20to%20high-dimensional%20control%20in%20large%20state%20spaces.%20We%20further%20show%20that%20existing%20hierarchical%20and%20bootstrapping-based%20approaches%20correspond%20to%20specific%20design%20choices%20within%20our%20derivation.%20Across%20a%20comprehensive%20suite%20of%20state-%20and%20pixel-based%20locomotion%20and%20manipulation%20benchmarks%2C%20our%20method%20matches%20or%20surpasses%20state-of-the-art%20offline%20GCRL%20algorithms%20and%20scales%20to%20complex%2C%20long-horizon%20tasks%20where%20prior%20approaches%20fail.%20Project%20page%3A%20https%3A//johnlyzhou.github.io/saw/%0ALink%3A%20http%3A//arxiv.org/abs/2505.14975v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlattening%2520Hierarchies%2520with%2520Policy%2520Bootstrapping%26entry.906535625%3DJohn%2520L.%2520Zhou%2520and%2520Jonathan%2520C.%2520Kao%26entry.1292438233%3DOffline%2520goal-conditioned%2520reinforcement%2520learning%2520%2528GCRL%2529%2520is%2520a%2520promising%2520approach%2520for%2520pretraining%2520generalist%2520policies%2520on%2520large%2520datasets%2520of%2520reward-free%2520trajectories%252C%2520akin%2520to%2520the%2520self-supervised%2520objectives%2520used%2520to%2520train%2520foundation%2520models%2520for%2520computer%2520vision%2520and%2520natural%2520language%2520processing.%2520However%252C%2520scaling%2520GCRL%2520to%2520longer%2520horizons%2520remains%2520challenging%2520due%2520to%2520the%2520combination%2520of%2520sparse%2520rewards%2520and%2520discounting%252C%2520which%2520obscures%2520the%2520comparative%2520advantages%2520of%2520primitive%2520actions%2520with%2520respect%2520to%2520distant%2520goals.%2520Hierarchical%2520RL%2520methods%2520achieve%2520strong%2520empirical%2520results%2520on%2520long-horizon%2520goal-reaching%2520tasks%252C%2520but%2520their%2520reliance%2520on%2520modular%252C%2520timescale-specific%2520policies%2520and%2520subgoal%2520generation%2520introduces%2520significant%2520additional%2520complexity%2520and%2520hinders%2520scaling%2520to%2520high-dimensional%2520goal%2520spaces.%2520In%2520this%2520work%252C%2520we%2520introduce%2520an%2520algorithm%2520to%2520train%2520a%2520flat%2520%2528non-hierarchical%2529%2520goal-conditioned%2520policy%2520by%2520bootstrapping%2520on%2520subgoal-conditioned%2520policies%2520with%2520advantage-weighted%2520importance%2520sampling.%2520Our%2520approach%2520eliminates%2520the%2520need%2520for%2520a%2520generative%2520model%2520over%2520the%2520%2528sub%2529goal%2520space%252C%2520which%2520we%2520find%2520is%2520key%2520for%2520scaling%2520to%2520high-dimensional%2520control%2520in%2520large%2520state%2520spaces.%2520We%2520further%2520show%2520that%2520existing%2520hierarchical%2520and%2520bootstrapping-based%2520approaches%2520correspond%2520to%2520specific%2520design%2520choices%2520within%2520our%2520derivation.%2520Across%2520a%2520comprehensive%2520suite%2520of%2520state-%2520and%2520pixel-based%2520locomotion%2520and%2520manipulation%2520benchmarks%252C%2520our%2520method%2520matches%2520or%2520surpasses%2520state-of-the-art%2520offline%2520GCRL%2520algorithms%2520and%2520scales%2520to%2520complex%252C%2520long-horizon%2520tasks%2520where%2520prior%2520approaches%2520fail.%2520Project%2520page%253A%2520https%253A//johnlyzhou.github.io/saw/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14975v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flattening%20Hierarchies%20with%20Policy%20Bootstrapping&entry.906535625=John%20L.%20Zhou%20and%20Jonathan%20C.%20Kao&entry.1292438233=Offline%20goal-conditioned%20reinforcement%20learning%20%28GCRL%29%20is%20a%20promising%20approach%20for%20pretraining%20generalist%20policies%20on%20large%20datasets%20of%20reward-free%20trajectories%2C%20akin%20to%20the%20self-supervised%20objectives%20used%20to%20train%20foundation%20models%20for%20computer%20vision%20and%20natural%20language%20processing.%20However%2C%20scaling%20GCRL%20to%20longer%20horizons%20remains%20challenging%20due%20to%20the%20combination%20of%20sparse%20rewards%20and%20discounting%2C%20which%20obscures%20the%20comparative%20advantages%20of%20primitive%20actions%20with%20respect%20to%20distant%20goals.%20Hierarchical%20RL%20methods%20achieve%20strong%20empirical%20results%20on%20long-horizon%20goal-reaching%20tasks%2C%20but%20their%20reliance%20on%20modular%2C%20timescale-specific%20policies%20and%20subgoal%20generation%20introduces%20significant%20additional%20complexity%20and%20hinders%20scaling%20to%20high-dimensional%20goal%20spaces.%20In%20this%20work%2C%20we%20introduce%20an%20algorithm%20to%20train%20a%20flat%20%28non-hierarchical%29%20goal-conditioned%20policy%20by%20bootstrapping%20on%20subgoal-conditioned%20policies%20with%20advantage-weighted%20importance%20sampling.%20Our%20approach%20eliminates%20the%20need%20for%20a%20generative%20model%20over%20the%20%28sub%29goal%20space%2C%20which%20we%20find%20is%20key%20for%20scaling%20to%20high-dimensional%20control%20in%20large%20state%20spaces.%20We%20further%20show%20that%20existing%20hierarchical%20and%20bootstrapping-based%20approaches%20correspond%20to%20specific%20design%20choices%20within%20our%20derivation.%20Across%20a%20comprehensive%20suite%20of%20state-%20and%20pixel-based%20locomotion%20and%20manipulation%20benchmarks%2C%20our%20method%20matches%20or%20surpasses%20state-of-the-art%20offline%20GCRL%20algorithms%20and%20scales%20to%20complex%2C%20long-horizon%20tasks%20where%20prior%20approaches%20fail.%20Project%20page%3A%20https%3A//johnlyzhou.github.io/saw/&entry.1838667208=http%3A//arxiv.org/abs/2505.14975v3&entry.124074799=Read"},
{"title": "Med-2D SegNet: A Light Weight Deep Neural Network for Medical 2D Image Segmentation", "author": "Lameya Sabrin and Md. Sanaullah Chowdhury and Salauddin Tapu and Noyon Kumar Sarkar and Ferdous Bin Ali", "abstract": "Accurate and efficient medical image segmentation is crucial for advancing clinical diagnostics and surgical planning, yet remains a complex challenge due to the variability in anatomical structures and the demand for low-complexity models. In this paper, we introduced Med-2D SegNet, a novel and highly efficient segmentation architecture that delivers outstanding accuracy while maintaining a minimal computational footprint. Med-2D SegNet achieves state-of-the-art performance across multiple benchmark datasets, including KVASIR-SEG, PH2, EndoVis, and GLAS, with an average Dice similarity coefficient (DSC) of 89.77% across 20 diverse datasets. Central to its success is the compact Med Block, a specialized encoder design that incorporates dimension expansion and parameter reduction, enabling precise feature extraction while keeping model parameters to a low count of just 2.07 million. Med-2D SegNet excels in cross-dataset generalization, particularly in polyp segmentation, where it was trained on KVASIR-SEG and showed strong performance on unseen datasets, demonstrating its robustness in zero-shot learning scenarios, even though we acknowledge that further improvements are possible. With top-tier performance in both binary and multi-class segmentation, Med-2D SegNet redefines the balance between accuracy and efficiency, setting a new benchmark for medical image analysis. This work paves the way for developing accessible, high-performance diagnostic tools suitable for clinical environments and resource-constrained settings, making it a step forward in the democratization of advanced medical technology.", "link": "http://arxiv.org/abs/2504.14715v2", "date": "2026-01-02", "relevancy": 1.5705, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.541}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5203}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Med-2D%20SegNet%3A%20A%20Light%20Weight%20Deep%20Neural%20Network%20for%20Medical%202D%20Image%20Segmentation&body=Title%3A%20Med-2D%20SegNet%3A%20A%20Light%20Weight%20Deep%20Neural%20Network%20for%20Medical%202D%20Image%20Segmentation%0AAuthor%3A%20Lameya%20Sabrin%20and%20Md.%20Sanaullah%20Chowdhury%20and%20Salauddin%20Tapu%20and%20Noyon%20Kumar%20Sarkar%20and%20Ferdous%20Bin%20Ali%0AAbstract%3A%20Accurate%20and%20efficient%20medical%20image%20segmentation%20is%20crucial%20for%20advancing%20clinical%20diagnostics%20and%20surgical%20planning%2C%20yet%20remains%20a%20complex%20challenge%20due%20to%20the%20variability%20in%20anatomical%20structures%20and%20the%20demand%20for%20low-complexity%20models.%20In%20this%20paper%2C%20we%20introduced%20Med-2D%20SegNet%2C%20a%20novel%20and%20highly%20efficient%20segmentation%20architecture%20that%20delivers%20outstanding%20accuracy%20while%20maintaining%20a%20minimal%20computational%20footprint.%20Med-2D%20SegNet%20achieves%20state-of-the-art%20performance%20across%20multiple%20benchmark%20datasets%2C%20including%20KVASIR-SEG%2C%20PH2%2C%20EndoVis%2C%20and%20GLAS%2C%20with%20an%20average%20Dice%20similarity%20coefficient%20%28DSC%29%20of%2089.77%25%20across%2020%20diverse%20datasets.%20Central%20to%20its%20success%20is%20the%20compact%20Med%20Block%2C%20a%20specialized%20encoder%20design%20that%20incorporates%20dimension%20expansion%20and%20parameter%20reduction%2C%20enabling%20precise%20feature%20extraction%20while%20keeping%20model%20parameters%20to%20a%20low%20count%20of%20just%202.07%20million.%20Med-2D%20SegNet%20excels%20in%20cross-dataset%20generalization%2C%20particularly%20in%20polyp%20segmentation%2C%20where%20it%20was%20trained%20on%20KVASIR-SEG%20and%20showed%20strong%20performance%20on%20unseen%20datasets%2C%20demonstrating%20its%20robustness%20in%20zero-shot%20learning%20scenarios%2C%20even%20though%20we%20acknowledge%20that%20further%20improvements%20are%20possible.%20With%20top-tier%20performance%20in%20both%20binary%20and%20multi-class%20segmentation%2C%20Med-2D%20SegNet%20redefines%20the%20balance%20between%20accuracy%20and%20efficiency%2C%20setting%20a%20new%20benchmark%20for%20medical%20image%20analysis.%20This%20work%20paves%20the%20way%20for%20developing%20accessible%2C%20high-performance%20diagnostic%20tools%20suitable%20for%20clinical%20environments%20and%20resource-constrained%20settings%2C%20making%20it%20a%20step%20forward%20in%20the%20democratization%20of%20advanced%20medical%20technology.%0ALink%3A%20http%3A//arxiv.org/abs/2504.14715v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMed-2D%2520SegNet%253A%2520A%2520Light%2520Weight%2520Deep%2520Neural%2520Network%2520for%2520Medical%25202D%2520Image%2520Segmentation%26entry.906535625%3DLameya%2520Sabrin%2520and%2520Md.%2520Sanaullah%2520Chowdhury%2520and%2520Salauddin%2520Tapu%2520and%2520Noyon%2520Kumar%2520Sarkar%2520and%2520Ferdous%2520Bin%2520Ali%26entry.1292438233%3DAccurate%2520and%2520efficient%2520medical%2520image%2520segmentation%2520is%2520crucial%2520for%2520advancing%2520clinical%2520diagnostics%2520and%2520surgical%2520planning%252C%2520yet%2520remains%2520a%2520complex%2520challenge%2520due%2520to%2520the%2520variability%2520in%2520anatomical%2520structures%2520and%2520the%2520demand%2520for%2520low-complexity%2520models.%2520In%2520this%2520paper%252C%2520we%2520introduced%2520Med-2D%2520SegNet%252C%2520a%2520novel%2520and%2520highly%2520efficient%2520segmentation%2520architecture%2520that%2520delivers%2520outstanding%2520accuracy%2520while%2520maintaining%2520a%2520minimal%2520computational%2520footprint.%2520Med-2D%2520SegNet%2520achieves%2520state-of-the-art%2520performance%2520across%2520multiple%2520benchmark%2520datasets%252C%2520including%2520KVASIR-SEG%252C%2520PH2%252C%2520EndoVis%252C%2520and%2520GLAS%252C%2520with%2520an%2520average%2520Dice%2520similarity%2520coefficient%2520%2528DSC%2529%2520of%252089.77%2525%2520across%252020%2520diverse%2520datasets.%2520Central%2520to%2520its%2520success%2520is%2520the%2520compact%2520Med%2520Block%252C%2520a%2520specialized%2520encoder%2520design%2520that%2520incorporates%2520dimension%2520expansion%2520and%2520parameter%2520reduction%252C%2520enabling%2520precise%2520feature%2520extraction%2520while%2520keeping%2520model%2520parameters%2520to%2520a%2520low%2520count%2520of%2520just%25202.07%2520million.%2520Med-2D%2520SegNet%2520excels%2520in%2520cross-dataset%2520generalization%252C%2520particularly%2520in%2520polyp%2520segmentation%252C%2520where%2520it%2520was%2520trained%2520on%2520KVASIR-SEG%2520and%2520showed%2520strong%2520performance%2520on%2520unseen%2520datasets%252C%2520demonstrating%2520its%2520robustness%2520in%2520zero-shot%2520learning%2520scenarios%252C%2520even%2520though%2520we%2520acknowledge%2520that%2520further%2520improvements%2520are%2520possible.%2520With%2520top-tier%2520performance%2520in%2520both%2520binary%2520and%2520multi-class%2520segmentation%252C%2520Med-2D%2520SegNet%2520redefines%2520the%2520balance%2520between%2520accuracy%2520and%2520efficiency%252C%2520setting%2520a%2520new%2520benchmark%2520for%2520medical%2520image%2520analysis.%2520This%2520work%2520paves%2520the%2520way%2520for%2520developing%2520accessible%252C%2520high-performance%2520diagnostic%2520tools%2520suitable%2520for%2520clinical%2520environments%2520and%2520resource-constrained%2520settings%252C%2520making%2520it%2520a%2520step%2520forward%2520in%2520the%2520democratization%2520of%2520advanced%2520medical%2520technology.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.14715v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Med-2D%20SegNet%3A%20A%20Light%20Weight%20Deep%20Neural%20Network%20for%20Medical%202D%20Image%20Segmentation&entry.906535625=Lameya%20Sabrin%20and%20Md.%20Sanaullah%20Chowdhury%20and%20Salauddin%20Tapu%20and%20Noyon%20Kumar%20Sarkar%20and%20Ferdous%20Bin%20Ali&entry.1292438233=Accurate%20and%20efficient%20medical%20image%20segmentation%20is%20crucial%20for%20advancing%20clinical%20diagnostics%20and%20surgical%20planning%2C%20yet%20remains%20a%20complex%20challenge%20due%20to%20the%20variability%20in%20anatomical%20structures%20and%20the%20demand%20for%20low-complexity%20models.%20In%20this%20paper%2C%20we%20introduced%20Med-2D%20SegNet%2C%20a%20novel%20and%20highly%20efficient%20segmentation%20architecture%20that%20delivers%20outstanding%20accuracy%20while%20maintaining%20a%20minimal%20computational%20footprint.%20Med-2D%20SegNet%20achieves%20state-of-the-art%20performance%20across%20multiple%20benchmark%20datasets%2C%20including%20KVASIR-SEG%2C%20PH2%2C%20EndoVis%2C%20and%20GLAS%2C%20with%20an%20average%20Dice%20similarity%20coefficient%20%28DSC%29%20of%2089.77%25%20across%2020%20diverse%20datasets.%20Central%20to%20its%20success%20is%20the%20compact%20Med%20Block%2C%20a%20specialized%20encoder%20design%20that%20incorporates%20dimension%20expansion%20and%20parameter%20reduction%2C%20enabling%20precise%20feature%20extraction%20while%20keeping%20model%20parameters%20to%20a%20low%20count%20of%20just%202.07%20million.%20Med-2D%20SegNet%20excels%20in%20cross-dataset%20generalization%2C%20particularly%20in%20polyp%20segmentation%2C%20where%20it%20was%20trained%20on%20KVASIR-SEG%20and%20showed%20strong%20performance%20on%20unseen%20datasets%2C%20demonstrating%20its%20robustness%20in%20zero-shot%20learning%20scenarios%2C%20even%20though%20we%20acknowledge%20that%20further%20improvements%20are%20possible.%20With%20top-tier%20performance%20in%20both%20binary%20and%20multi-class%20segmentation%2C%20Med-2D%20SegNet%20redefines%20the%20balance%20between%20accuracy%20and%20efficiency%2C%20setting%20a%20new%20benchmark%20for%20medical%20image%20analysis.%20This%20work%20paves%20the%20way%20for%20developing%20accessible%2C%20high-performance%20diagnostic%20tools%20suitable%20for%20clinical%20environments%20and%20resource-constrained%20settings%2C%20making%20it%20a%20step%20forward%20in%20the%20democratization%20of%20advanced%20medical%20technology.&entry.1838667208=http%3A//arxiv.org/abs/2504.14715v2&entry.124074799=Read"},
{"title": "DefVINS: Visual-Inertial Odometry for Deformable Scenes", "author": "Samuel Cerezo and Javier Civera", "abstract": "Deformable scenes violate the rigidity assumptions underpinning classical visual-inertial odometry (VIO), often leading to over-fitting to local non-rigid motion or severe drift when deformation dominates visual parallax. We introduce DefVINS, a visual-inertial odometry framework that explicitly separates a rigid, IMU-anchored state from a non--rigid warp represented by an embedded deformation graph. The system is initialized using a standard VIO procedure that fixes gravity, velocity, and IMU biases, after which non-rigid degrees of freedom are activated progressively as the estimation becomes well conditioned. An observability analysis is included to characterize how inertial measurements constrain the rigid motion and render otherwise unobservable modes identifiable in the presence of deformation. This analysis motivates the use of IMU anchoring and informs a conditioning-based activation strategy that prevents ill-posed updates under poor excitation. Ablation studies demonstrate the benefits of combining inertial constraints with observability-aware deformation activation, resulting in improved robustness under non-rigid environments.", "link": "http://arxiv.org/abs/2601.00702v1", "date": "2026-01-02", "relevancy": 1.5647, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5279}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5224}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DefVINS%3A%20Visual-Inertial%20Odometry%20for%20Deformable%20Scenes&body=Title%3A%20DefVINS%3A%20Visual-Inertial%20Odometry%20for%20Deformable%20Scenes%0AAuthor%3A%20Samuel%20Cerezo%20and%20Javier%20Civera%0AAbstract%3A%20Deformable%20scenes%20violate%20the%20rigidity%20assumptions%20underpinning%20classical%20visual-inertial%20odometry%20%28VIO%29%2C%20often%20leading%20to%20over-fitting%20to%20local%20non-rigid%20motion%20or%20severe%20drift%20when%20deformation%20dominates%20visual%20parallax.%20We%20introduce%20DefVINS%2C%20a%20visual-inertial%20odometry%20framework%20that%20explicitly%20separates%20a%20rigid%2C%20IMU-anchored%20state%20from%20a%20non--rigid%20warp%20represented%20by%20an%20embedded%20deformation%20graph.%20The%20system%20is%20initialized%20using%20a%20standard%20VIO%20procedure%20that%20fixes%20gravity%2C%20velocity%2C%20and%20IMU%20biases%2C%20after%20which%20non-rigid%20degrees%20of%20freedom%20are%20activated%20progressively%20as%20the%20estimation%20becomes%20well%20conditioned.%20An%20observability%20analysis%20is%20included%20to%20characterize%20how%20inertial%20measurements%20constrain%20the%20rigid%20motion%20and%20render%20otherwise%20unobservable%20modes%20identifiable%20in%20the%20presence%20of%20deformation.%20This%20analysis%20motivates%20the%20use%20of%20IMU%20anchoring%20and%20informs%20a%20conditioning-based%20activation%20strategy%20that%20prevents%20ill-posed%20updates%20under%20poor%20excitation.%20Ablation%20studies%20demonstrate%20the%20benefits%20of%20combining%20inertial%20constraints%20with%20observability-aware%20deformation%20activation%2C%20resulting%20in%20improved%20robustness%20under%20non-rigid%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00702v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDefVINS%253A%2520Visual-Inertial%2520Odometry%2520for%2520Deformable%2520Scenes%26entry.906535625%3DSamuel%2520Cerezo%2520and%2520Javier%2520Civera%26entry.1292438233%3DDeformable%2520scenes%2520violate%2520the%2520rigidity%2520assumptions%2520underpinning%2520classical%2520visual-inertial%2520odometry%2520%2528VIO%2529%252C%2520often%2520leading%2520to%2520over-fitting%2520to%2520local%2520non-rigid%2520motion%2520or%2520severe%2520drift%2520when%2520deformation%2520dominates%2520visual%2520parallax.%2520We%2520introduce%2520DefVINS%252C%2520a%2520visual-inertial%2520odometry%2520framework%2520that%2520explicitly%2520separates%2520a%2520rigid%252C%2520IMU-anchored%2520state%2520from%2520a%2520non--rigid%2520warp%2520represented%2520by%2520an%2520embedded%2520deformation%2520graph.%2520The%2520system%2520is%2520initialized%2520using%2520a%2520standard%2520VIO%2520procedure%2520that%2520fixes%2520gravity%252C%2520velocity%252C%2520and%2520IMU%2520biases%252C%2520after%2520which%2520non-rigid%2520degrees%2520of%2520freedom%2520are%2520activated%2520progressively%2520as%2520the%2520estimation%2520becomes%2520well%2520conditioned.%2520An%2520observability%2520analysis%2520is%2520included%2520to%2520characterize%2520how%2520inertial%2520measurements%2520constrain%2520the%2520rigid%2520motion%2520and%2520render%2520otherwise%2520unobservable%2520modes%2520identifiable%2520in%2520the%2520presence%2520of%2520deformation.%2520This%2520analysis%2520motivates%2520the%2520use%2520of%2520IMU%2520anchoring%2520and%2520informs%2520a%2520conditioning-based%2520activation%2520strategy%2520that%2520prevents%2520ill-posed%2520updates%2520under%2520poor%2520excitation.%2520Ablation%2520studies%2520demonstrate%2520the%2520benefits%2520of%2520combining%2520inertial%2520constraints%2520with%2520observability-aware%2520deformation%2520activation%252C%2520resulting%2520in%2520improved%2520robustness%2520under%2520non-rigid%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00702v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DefVINS%3A%20Visual-Inertial%20Odometry%20for%20Deformable%20Scenes&entry.906535625=Samuel%20Cerezo%20and%20Javier%20Civera&entry.1292438233=Deformable%20scenes%20violate%20the%20rigidity%20assumptions%20underpinning%20classical%20visual-inertial%20odometry%20%28VIO%29%2C%20often%20leading%20to%20over-fitting%20to%20local%20non-rigid%20motion%20or%20severe%20drift%20when%20deformation%20dominates%20visual%20parallax.%20We%20introduce%20DefVINS%2C%20a%20visual-inertial%20odometry%20framework%20that%20explicitly%20separates%20a%20rigid%2C%20IMU-anchored%20state%20from%20a%20non--rigid%20warp%20represented%20by%20an%20embedded%20deformation%20graph.%20The%20system%20is%20initialized%20using%20a%20standard%20VIO%20procedure%20that%20fixes%20gravity%2C%20velocity%2C%20and%20IMU%20biases%2C%20after%20which%20non-rigid%20degrees%20of%20freedom%20are%20activated%20progressively%20as%20the%20estimation%20becomes%20well%20conditioned.%20An%20observability%20analysis%20is%20included%20to%20characterize%20how%20inertial%20measurements%20constrain%20the%20rigid%20motion%20and%20render%20otherwise%20unobservable%20modes%20identifiable%20in%20the%20presence%20of%20deformation.%20This%20analysis%20motivates%20the%20use%20of%20IMU%20anchoring%20and%20informs%20a%20conditioning-based%20activation%20strategy%20that%20prevents%20ill-posed%20updates%20under%20poor%20excitation.%20Ablation%20studies%20demonstrate%20the%20benefits%20of%20combining%20inertial%20constraints%20with%20observability-aware%20deformation%20activation%2C%20resulting%20in%20improved%20robustness%20under%20non-rigid%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2601.00702v1&entry.124074799=Read"},
{"title": "Clustering by Denoising: Latent plug-and-play diffusion for single-cell data", "author": "Dominik Meier and Shixing Yu and Sagnik Nandy and Promit Ghosal and Kyra Gan", "abstract": "Single-cell RNA sequencing (scRNA-seq) enables the study of cellular heterogeneity. Yet, clustering accuracy, and with it downstream analyses based on cell labels, remain challenging due to measurement noise and biological variability. In standard latent spaces (e.g., obtained through PCA), data from different cell types can be projected close together, making accurate clustering difficult. We introduce a latent plug-and-play diffusion framework that separates the observation and denoising space. This separation is operationalized through a novel Gibbs sampling procedure: the learned diffusion prior is applied in a low-dimensional latent space to perform denoising, while to steer this process, noise is reintroduced into the original high-dimensional observation space. This unique \"input-space steering\" ensures the denoising trajectory remains faithful to the original data structure. Our approach offers three key advantages: (1) adaptive noise handling via a tunable balance between prior and observed data; (2) uncertainty quantification through principled uncertainty estimates for downstream analysis; and (3) generalizable denoising by leveraging clean reference data to denoise noisier datasets, and via averaging, improve quality beyond the training set. We evaluate robustness on both synthetic and real single-cell genomics data. Our method improves clustering accuracy on synthetic data across varied noise levels and dataset shifts. On real-world single-cell data, our method demonstrates improved biological coherence in the resulting cell clusters, with cluster boundaries that better align with known cell type markers and developmental trajectories.", "link": "http://arxiv.org/abs/2510.22835v2", "date": "2026-01-02", "relevancy": 1.5195, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5458}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4972}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clustering%20by%20Denoising%3A%20Latent%20plug-and-play%20diffusion%20for%20single-cell%20data&body=Title%3A%20Clustering%20by%20Denoising%3A%20Latent%20plug-and-play%20diffusion%20for%20single-cell%20data%0AAuthor%3A%20Dominik%20Meier%20and%20Shixing%20Yu%20and%20Sagnik%20Nandy%20and%20Promit%20Ghosal%20and%20Kyra%20Gan%0AAbstract%3A%20Single-cell%20RNA%20sequencing%20%28scRNA-seq%29%20enables%20the%20study%20of%20cellular%20heterogeneity.%20Yet%2C%20clustering%20accuracy%2C%20and%20with%20it%20downstream%20analyses%20based%20on%20cell%20labels%2C%20remain%20challenging%20due%20to%20measurement%20noise%20and%20biological%20variability.%20In%20standard%20latent%20spaces%20%28e.g.%2C%20obtained%20through%20PCA%29%2C%20data%20from%20different%20cell%20types%20can%20be%20projected%20close%20together%2C%20making%20accurate%20clustering%20difficult.%20We%20introduce%20a%20latent%20plug-and-play%20diffusion%20framework%20that%20separates%20the%20observation%20and%20denoising%20space.%20This%20separation%20is%20operationalized%20through%20a%20novel%20Gibbs%20sampling%20procedure%3A%20the%20learned%20diffusion%20prior%20is%20applied%20in%20a%20low-dimensional%20latent%20space%20to%20perform%20denoising%2C%20while%20to%20steer%20this%20process%2C%20noise%20is%20reintroduced%20into%20the%20original%20high-dimensional%20observation%20space.%20This%20unique%20%22input-space%20steering%22%20ensures%20the%20denoising%20trajectory%20remains%20faithful%20to%20the%20original%20data%20structure.%20Our%20approach%20offers%20three%20key%20advantages%3A%20%281%29%20adaptive%20noise%20handling%20via%20a%20tunable%20balance%20between%20prior%20and%20observed%20data%3B%20%282%29%20uncertainty%20quantification%20through%20principled%20uncertainty%20estimates%20for%20downstream%20analysis%3B%20and%20%283%29%20generalizable%20denoising%20by%20leveraging%20clean%20reference%20data%20to%20denoise%20noisier%20datasets%2C%20and%20via%20averaging%2C%20improve%20quality%20beyond%20the%20training%20set.%20We%20evaluate%20robustness%20on%20both%20synthetic%20and%20real%20single-cell%20genomics%20data.%20Our%20method%20improves%20clustering%20accuracy%20on%20synthetic%20data%20across%20varied%20noise%20levels%20and%20dataset%20shifts.%20On%20real-world%20single-cell%20data%2C%20our%20method%20demonstrates%20improved%20biological%20coherence%20in%20the%20resulting%20cell%20clusters%2C%20with%20cluster%20boundaries%20that%20better%20align%20with%20known%20cell%20type%20markers%20and%20developmental%20trajectories.%0ALink%3A%20http%3A//arxiv.org/abs/2510.22835v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClustering%2520by%2520Denoising%253A%2520Latent%2520plug-and-play%2520diffusion%2520for%2520single-cell%2520data%26entry.906535625%3DDominik%2520Meier%2520and%2520Shixing%2520Yu%2520and%2520Sagnik%2520Nandy%2520and%2520Promit%2520Ghosal%2520and%2520Kyra%2520Gan%26entry.1292438233%3DSingle-cell%2520RNA%2520sequencing%2520%2528scRNA-seq%2529%2520enables%2520the%2520study%2520of%2520cellular%2520heterogeneity.%2520Yet%252C%2520clustering%2520accuracy%252C%2520and%2520with%2520it%2520downstream%2520analyses%2520based%2520on%2520cell%2520labels%252C%2520remain%2520challenging%2520due%2520to%2520measurement%2520noise%2520and%2520biological%2520variability.%2520In%2520standard%2520latent%2520spaces%2520%2528e.g.%252C%2520obtained%2520through%2520PCA%2529%252C%2520data%2520from%2520different%2520cell%2520types%2520can%2520be%2520projected%2520close%2520together%252C%2520making%2520accurate%2520clustering%2520difficult.%2520We%2520introduce%2520a%2520latent%2520plug-and-play%2520diffusion%2520framework%2520that%2520separates%2520the%2520observation%2520and%2520denoising%2520space.%2520This%2520separation%2520is%2520operationalized%2520through%2520a%2520novel%2520Gibbs%2520sampling%2520procedure%253A%2520the%2520learned%2520diffusion%2520prior%2520is%2520applied%2520in%2520a%2520low-dimensional%2520latent%2520space%2520to%2520perform%2520denoising%252C%2520while%2520to%2520steer%2520this%2520process%252C%2520noise%2520is%2520reintroduced%2520into%2520the%2520original%2520high-dimensional%2520observation%2520space.%2520This%2520unique%2520%2522input-space%2520steering%2522%2520ensures%2520the%2520denoising%2520trajectory%2520remains%2520faithful%2520to%2520the%2520original%2520data%2520structure.%2520Our%2520approach%2520offers%2520three%2520key%2520advantages%253A%2520%25281%2529%2520adaptive%2520noise%2520handling%2520via%2520a%2520tunable%2520balance%2520between%2520prior%2520and%2520observed%2520data%253B%2520%25282%2529%2520uncertainty%2520quantification%2520through%2520principled%2520uncertainty%2520estimates%2520for%2520downstream%2520analysis%253B%2520and%2520%25283%2529%2520generalizable%2520denoising%2520by%2520leveraging%2520clean%2520reference%2520data%2520to%2520denoise%2520noisier%2520datasets%252C%2520and%2520via%2520averaging%252C%2520improve%2520quality%2520beyond%2520the%2520training%2520set.%2520We%2520evaluate%2520robustness%2520on%2520both%2520synthetic%2520and%2520real%2520single-cell%2520genomics%2520data.%2520Our%2520method%2520improves%2520clustering%2520accuracy%2520on%2520synthetic%2520data%2520across%2520varied%2520noise%2520levels%2520and%2520dataset%2520shifts.%2520On%2520real-world%2520single-cell%2520data%252C%2520our%2520method%2520demonstrates%2520improved%2520biological%2520coherence%2520in%2520the%2520resulting%2520cell%2520clusters%252C%2520with%2520cluster%2520boundaries%2520that%2520better%2520align%2520with%2520known%2520cell%2520type%2520markers%2520and%2520developmental%2520trajectories.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.22835v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clustering%20by%20Denoising%3A%20Latent%20plug-and-play%20diffusion%20for%20single-cell%20data&entry.906535625=Dominik%20Meier%20and%20Shixing%20Yu%20and%20Sagnik%20Nandy%20and%20Promit%20Ghosal%20and%20Kyra%20Gan&entry.1292438233=Single-cell%20RNA%20sequencing%20%28scRNA-seq%29%20enables%20the%20study%20of%20cellular%20heterogeneity.%20Yet%2C%20clustering%20accuracy%2C%20and%20with%20it%20downstream%20analyses%20based%20on%20cell%20labels%2C%20remain%20challenging%20due%20to%20measurement%20noise%20and%20biological%20variability.%20In%20standard%20latent%20spaces%20%28e.g.%2C%20obtained%20through%20PCA%29%2C%20data%20from%20different%20cell%20types%20can%20be%20projected%20close%20together%2C%20making%20accurate%20clustering%20difficult.%20We%20introduce%20a%20latent%20plug-and-play%20diffusion%20framework%20that%20separates%20the%20observation%20and%20denoising%20space.%20This%20separation%20is%20operationalized%20through%20a%20novel%20Gibbs%20sampling%20procedure%3A%20the%20learned%20diffusion%20prior%20is%20applied%20in%20a%20low-dimensional%20latent%20space%20to%20perform%20denoising%2C%20while%20to%20steer%20this%20process%2C%20noise%20is%20reintroduced%20into%20the%20original%20high-dimensional%20observation%20space.%20This%20unique%20%22input-space%20steering%22%20ensures%20the%20denoising%20trajectory%20remains%20faithful%20to%20the%20original%20data%20structure.%20Our%20approach%20offers%20three%20key%20advantages%3A%20%281%29%20adaptive%20noise%20handling%20via%20a%20tunable%20balance%20between%20prior%20and%20observed%20data%3B%20%282%29%20uncertainty%20quantification%20through%20principled%20uncertainty%20estimates%20for%20downstream%20analysis%3B%20and%20%283%29%20generalizable%20denoising%20by%20leveraging%20clean%20reference%20data%20to%20denoise%20noisier%20datasets%2C%20and%20via%20averaging%2C%20improve%20quality%20beyond%20the%20training%20set.%20We%20evaluate%20robustness%20on%20both%20synthetic%20and%20real%20single-cell%20genomics%20data.%20Our%20method%20improves%20clustering%20accuracy%20on%20synthetic%20data%20across%20varied%20noise%20levels%20and%20dataset%20shifts.%20On%20real-world%20single-cell%20data%2C%20our%20method%20demonstrates%20improved%20biological%20coherence%20in%20the%20resulting%20cell%20clusters%2C%20with%20cluster%20boundaries%20that%20better%20align%20with%20known%20cell%20type%20markers%20and%20developmental%20trajectories.&entry.1838667208=http%3A//arxiv.org/abs/2510.22835v2&entry.124074799=Read"},
{"title": "Beyond Accuracy: What Matters in Designing Well-Behaved Image Classification Models?", "author": "Robin Hesse and Do\u011fukan Ba\u011fc\u0131 and Bernt Schiele and Simone Schaub-Meyer and Stefan Roth", "abstract": "Deep learning has become an essential part of computer vision, with deep neural networks (DNNs) excelling in predictive performance. However, they often fall short in other critical quality dimensions, such as robustness, calibration, or fairness. While existing studies have focused on a subset of these quality dimensions, none have explored a more general form of \"well-behavedness\" of DNNs. With this work, we address this gap by simultaneously studying nine different quality dimensions for image classification. Through a large-scale study, we provide a bird's-eye view by analyzing 326 backbone models and how different training paradigms and model architectures affect these quality dimensions. We reveal various new insights such that (i) vision-language models exhibit high class balance on ImageNet-1k classification and strong robustness against domain changes; (ii) training models initialized with weights obtained through self-supervised learning is an effective strategy to improve most considered quality dimensions; and (iii) the training dataset size is a major driver for most of the quality dimensions. We conclude our study by introducing the QUBA score (Quality Understanding Beyond Accuracy), a novel metric that ranks models across multiple dimensions of quality, enabling tailored recommendations based on specific user needs.", "link": "http://arxiv.org/abs/2503.17110v2", "date": "2026-01-02", "relevancy": 1.514, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5093}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5048}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Accuracy%3A%20What%20Matters%20in%20Designing%20Well-Behaved%20Image%20Classification%20Models%3F&body=Title%3A%20Beyond%20Accuracy%3A%20What%20Matters%20in%20Designing%20Well-Behaved%20Image%20Classification%20Models%3F%0AAuthor%3A%20Robin%20Hesse%20and%20Do%C4%9Fukan%20Ba%C4%9Fc%C4%B1%20and%20Bernt%20Schiele%20and%20Simone%20Schaub-Meyer%20and%20Stefan%20Roth%0AAbstract%3A%20Deep%20learning%20has%20become%20an%20essential%20part%20of%20computer%20vision%2C%20with%20deep%20neural%20networks%20%28DNNs%29%20excelling%20in%20predictive%20performance.%20However%2C%20they%20often%20fall%20short%20in%20other%20critical%20quality%20dimensions%2C%20such%20as%20robustness%2C%20calibration%2C%20or%20fairness.%20While%20existing%20studies%20have%20focused%20on%20a%20subset%20of%20these%20quality%20dimensions%2C%20none%20have%20explored%20a%20more%20general%20form%20of%20%22well-behavedness%22%20of%20DNNs.%20With%20this%20work%2C%20we%20address%20this%20gap%20by%20simultaneously%20studying%20nine%20different%20quality%20dimensions%20for%20image%20classification.%20Through%20a%20large-scale%20study%2C%20we%20provide%20a%20bird%27s-eye%20view%20by%20analyzing%20326%20backbone%20models%20and%20how%20different%20training%20paradigms%20and%20model%20architectures%20affect%20these%20quality%20dimensions.%20We%20reveal%20various%20new%20insights%20such%20that%20%28i%29%20vision-language%20models%20exhibit%20high%20class%20balance%20on%20ImageNet-1k%20classification%20and%20strong%20robustness%20against%20domain%20changes%3B%20%28ii%29%20training%20models%20initialized%20with%20weights%20obtained%20through%20self-supervised%20learning%20is%20an%20effective%20strategy%20to%20improve%20most%20considered%20quality%20dimensions%3B%20and%20%28iii%29%20the%20training%20dataset%20size%20is%20a%20major%20driver%20for%20most%20of%20the%20quality%20dimensions.%20We%20conclude%20our%20study%20by%20introducing%20the%20QUBA%20score%20%28Quality%20Understanding%20Beyond%20Accuracy%29%2C%20a%20novel%20metric%20that%20ranks%20models%20across%20multiple%20dimensions%20of%20quality%2C%20enabling%20tailored%20recommendations%20based%20on%20specific%20user%20needs.%0ALink%3A%20http%3A//arxiv.org/abs/2503.17110v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Accuracy%253A%2520What%2520Matters%2520in%2520Designing%2520Well-Behaved%2520Image%2520Classification%2520Models%253F%26entry.906535625%3DRobin%2520Hesse%2520and%2520Do%25C4%259Fukan%2520Ba%25C4%259Fc%25C4%25B1%2520and%2520Bernt%2520Schiele%2520and%2520Simone%2520Schaub-Meyer%2520and%2520Stefan%2520Roth%26entry.1292438233%3DDeep%2520learning%2520has%2520become%2520an%2520essential%2520part%2520of%2520computer%2520vision%252C%2520with%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520excelling%2520in%2520predictive%2520performance.%2520However%252C%2520they%2520often%2520fall%2520short%2520in%2520other%2520critical%2520quality%2520dimensions%252C%2520such%2520as%2520robustness%252C%2520calibration%252C%2520or%2520fairness.%2520While%2520existing%2520studies%2520have%2520focused%2520on%2520a%2520subset%2520of%2520these%2520quality%2520dimensions%252C%2520none%2520have%2520explored%2520a%2520more%2520general%2520form%2520of%2520%2522well-behavedness%2522%2520of%2520DNNs.%2520With%2520this%2520work%252C%2520we%2520address%2520this%2520gap%2520by%2520simultaneously%2520studying%2520nine%2520different%2520quality%2520dimensions%2520for%2520image%2520classification.%2520Through%2520a%2520large-scale%2520study%252C%2520we%2520provide%2520a%2520bird%2527s-eye%2520view%2520by%2520analyzing%2520326%2520backbone%2520models%2520and%2520how%2520different%2520training%2520paradigms%2520and%2520model%2520architectures%2520affect%2520these%2520quality%2520dimensions.%2520We%2520reveal%2520various%2520new%2520insights%2520such%2520that%2520%2528i%2529%2520vision-language%2520models%2520exhibit%2520high%2520class%2520balance%2520on%2520ImageNet-1k%2520classification%2520and%2520strong%2520robustness%2520against%2520domain%2520changes%253B%2520%2528ii%2529%2520training%2520models%2520initialized%2520with%2520weights%2520obtained%2520through%2520self-supervised%2520learning%2520is%2520an%2520effective%2520strategy%2520to%2520improve%2520most%2520considered%2520quality%2520dimensions%253B%2520and%2520%2528iii%2529%2520the%2520training%2520dataset%2520size%2520is%2520a%2520major%2520driver%2520for%2520most%2520of%2520the%2520quality%2520dimensions.%2520We%2520conclude%2520our%2520study%2520by%2520introducing%2520the%2520QUBA%2520score%2520%2528Quality%2520Understanding%2520Beyond%2520Accuracy%2529%252C%2520a%2520novel%2520metric%2520that%2520ranks%2520models%2520across%2520multiple%2520dimensions%2520of%2520quality%252C%2520enabling%2520tailored%2520recommendations%2520based%2520on%2520specific%2520user%2520needs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.17110v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Accuracy%3A%20What%20Matters%20in%20Designing%20Well-Behaved%20Image%20Classification%20Models%3F&entry.906535625=Robin%20Hesse%20and%20Do%C4%9Fukan%20Ba%C4%9Fc%C4%B1%20and%20Bernt%20Schiele%20and%20Simone%20Schaub-Meyer%20and%20Stefan%20Roth&entry.1292438233=Deep%20learning%20has%20become%20an%20essential%20part%20of%20computer%20vision%2C%20with%20deep%20neural%20networks%20%28DNNs%29%20excelling%20in%20predictive%20performance.%20However%2C%20they%20often%20fall%20short%20in%20other%20critical%20quality%20dimensions%2C%20such%20as%20robustness%2C%20calibration%2C%20or%20fairness.%20While%20existing%20studies%20have%20focused%20on%20a%20subset%20of%20these%20quality%20dimensions%2C%20none%20have%20explored%20a%20more%20general%20form%20of%20%22well-behavedness%22%20of%20DNNs.%20With%20this%20work%2C%20we%20address%20this%20gap%20by%20simultaneously%20studying%20nine%20different%20quality%20dimensions%20for%20image%20classification.%20Through%20a%20large-scale%20study%2C%20we%20provide%20a%20bird%27s-eye%20view%20by%20analyzing%20326%20backbone%20models%20and%20how%20different%20training%20paradigms%20and%20model%20architectures%20affect%20these%20quality%20dimensions.%20We%20reveal%20various%20new%20insights%20such%20that%20%28i%29%20vision-language%20models%20exhibit%20high%20class%20balance%20on%20ImageNet-1k%20classification%20and%20strong%20robustness%20against%20domain%20changes%3B%20%28ii%29%20training%20models%20initialized%20with%20weights%20obtained%20through%20self-supervised%20learning%20is%20an%20effective%20strategy%20to%20improve%20most%20considered%20quality%20dimensions%3B%20and%20%28iii%29%20the%20training%20dataset%20size%20is%20a%20major%20driver%20for%20most%20of%20the%20quality%20dimensions.%20We%20conclude%20our%20study%20by%20introducing%20the%20QUBA%20score%20%28Quality%20Understanding%20Beyond%20Accuracy%29%2C%20a%20novel%20metric%20that%20ranks%20models%20across%20multiple%20dimensions%20of%20quality%2C%20enabling%20tailored%20recommendations%20based%20on%20specific%20user%20needs.&entry.1838667208=http%3A//arxiv.org/abs/2503.17110v2&entry.124074799=Read"},
{"title": "A Machine Learning Framework for Off Ball Defensive Role and Performance Evaluation in Football", "author": "Sean Groom and Shuo Wang and Francisco Belo and Axl Rice and Liam Anderson", "abstract": "Evaluating off-ball defensive performance in football is challenging, as traditional metrics do not capture the nuanced coordinated movements that limit opponent action selection and success probabilities. Although widely used possession value models excel at appraising on-ball actions, their application to defense remains limited. Existing counterfactual methods, such as ghosting models, help extend these analyses but often rely on simulating \"average\" behavior that lacks tactical context. To address this, we introduce a covariate-dependent Hidden Markov Model (CDHMM) tailored to corner kicks, a highly structured aspect of football games. Our label-free model infers time-resolved man-marking and zonal assignments directly from player tracking data. We leverage these assignments to propose a novel framework for defensive credit attribution and a role-conditioned ghosting method for counterfactual analysis of off-ball defensive performance. We show how these contributions provide a interpretable evaluation of defensive contributions against context-aware baselines.", "link": "http://arxiv.org/abs/2601.00748v1", "date": "2026-01-02", "relevancy": 1.5001, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5322}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4923}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Machine%20Learning%20Framework%20for%20Off%20Ball%20Defensive%20Role%20and%20Performance%20Evaluation%20in%20Football&body=Title%3A%20A%20Machine%20Learning%20Framework%20for%20Off%20Ball%20Defensive%20Role%20and%20Performance%20Evaluation%20in%20Football%0AAuthor%3A%20Sean%20Groom%20and%20Shuo%20Wang%20and%20Francisco%20Belo%20and%20Axl%20Rice%20and%20Liam%20Anderson%0AAbstract%3A%20Evaluating%20off-ball%20defensive%20performance%20in%20football%20is%20challenging%2C%20as%20traditional%20metrics%20do%20not%20capture%20the%20nuanced%20coordinated%20movements%20that%20limit%20opponent%20action%20selection%20and%20success%20probabilities.%20Although%20widely%20used%20possession%20value%20models%20excel%20at%20appraising%20on-ball%20actions%2C%20their%20application%20to%20defense%20remains%20limited.%20Existing%20counterfactual%20methods%2C%20such%20as%20ghosting%20models%2C%20help%20extend%20these%20analyses%20but%20often%20rely%20on%20simulating%20%22average%22%20behavior%20that%20lacks%20tactical%20context.%20To%20address%20this%2C%20we%20introduce%20a%20covariate-dependent%20Hidden%20Markov%20Model%20%28CDHMM%29%20tailored%20to%20corner%20kicks%2C%20a%20highly%20structured%20aspect%20of%20football%20games.%20Our%20label-free%20model%20infers%20time-resolved%20man-marking%20and%20zonal%20assignments%20directly%20from%20player%20tracking%20data.%20We%20leverage%20these%20assignments%20to%20propose%20a%20novel%20framework%20for%20defensive%20credit%20attribution%20and%20a%20role-conditioned%20ghosting%20method%20for%20counterfactual%20analysis%20of%20off-ball%20defensive%20performance.%20We%20show%20how%20these%20contributions%20provide%20a%20interpretable%20evaluation%20of%20defensive%20contributions%20against%20context-aware%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00748v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Machine%2520Learning%2520Framework%2520for%2520Off%2520Ball%2520Defensive%2520Role%2520and%2520Performance%2520Evaluation%2520in%2520Football%26entry.906535625%3DSean%2520Groom%2520and%2520Shuo%2520Wang%2520and%2520Francisco%2520Belo%2520and%2520Axl%2520Rice%2520and%2520Liam%2520Anderson%26entry.1292438233%3DEvaluating%2520off-ball%2520defensive%2520performance%2520in%2520football%2520is%2520challenging%252C%2520as%2520traditional%2520metrics%2520do%2520not%2520capture%2520the%2520nuanced%2520coordinated%2520movements%2520that%2520limit%2520opponent%2520action%2520selection%2520and%2520success%2520probabilities.%2520Although%2520widely%2520used%2520possession%2520value%2520models%2520excel%2520at%2520appraising%2520on-ball%2520actions%252C%2520their%2520application%2520to%2520defense%2520remains%2520limited.%2520Existing%2520counterfactual%2520methods%252C%2520such%2520as%2520ghosting%2520models%252C%2520help%2520extend%2520these%2520analyses%2520but%2520often%2520rely%2520on%2520simulating%2520%2522average%2522%2520behavior%2520that%2520lacks%2520tactical%2520context.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520covariate-dependent%2520Hidden%2520Markov%2520Model%2520%2528CDHMM%2529%2520tailored%2520to%2520corner%2520kicks%252C%2520a%2520highly%2520structured%2520aspect%2520of%2520football%2520games.%2520Our%2520label-free%2520model%2520infers%2520time-resolved%2520man-marking%2520and%2520zonal%2520assignments%2520directly%2520from%2520player%2520tracking%2520data.%2520We%2520leverage%2520these%2520assignments%2520to%2520propose%2520a%2520novel%2520framework%2520for%2520defensive%2520credit%2520attribution%2520and%2520a%2520role-conditioned%2520ghosting%2520method%2520for%2520counterfactual%2520analysis%2520of%2520off-ball%2520defensive%2520performance.%2520We%2520show%2520how%2520these%2520contributions%2520provide%2520a%2520interpretable%2520evaluation%2520of%2520defensive%2520contributions%2520against%2520context-aware%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00748v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Machine%20Learning%20Framework%20for%20Off%20Ball%20Defensive%20Role%20and%20Performance%20Evaluation%20in%20Football&entry.906535625=Sean%20Groom%20and%20Shuo%20Wang%20and%20Francisco%20Belo%20and%20Axl%20Rice%20and%20Liam%20Anderson&entry.1292438233=Evaluating%20off-ball%20defensive%20performance%20in%20football%20is%20challenging%2C%20as%20traditional%20metrics%20do%20not%20capture%20the%20nuanced%20coordinated%20movements%20that%20limit%20opponent%20action%20selection%20and%20success%20probabilities.%20Although%20widely%20used%20possession%20value%20models%20excel%20at%20appraising%20on-ball%20actions%2C%20their%20application%20to%20defense%20remains%20limited.%20Existing%20counterfactual%20methods%2C%20such%20as%20ghosting%20models%2C%20help%20extend%20these%20analyses%20but%20often%20rely%20on%20simulating%20%22average%22%20behavior%20that%20lacks%20tactical%20context.%20To%20address%20this%2C%20we%20introduce%20a%20covariate-dependent%20Hidden%20Markov%20Model%20%28CDHMM%29%20tailored%20to%20corner%20kicks%2C%20a%20highly%20structured%20aspect%20of%20football%20games.%20Our%20label-free%20model%20infers%20time-resolved%20man-marking%20and%20zonal%20assignments%20directly%20from%20player%20tracking%20data.%20We%20leverage%20these%20assignments%20to%20propose%20a%20novel%20framework%20for%20defensive%20credit%20attribution%20and%20a%20role-conditioned%20ghosting%20method%20for%20counterfactual%20analysis%20of%20off-ball%20defensive%20performance.%20We%20show%20how%20these%20contributions%20provide%20a%20interpretable%20evaluation%20of%20defensive%20contributions%20against%20context-aware%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2601.00748v1&entry.124074799=Read"},
{"title": "The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving", "author": "Max Ruiz Luyten and Mihaela van der Schaar", "abstract": "State-of-the-art large language model (LLM) pipelines rely on bootstrapped reasoning loops: sampling diverse chains of thought and reinforcing the highest-scoring ones, mainly optimizing correctness. We analyze how this design choice is sensitive to the collapse of the model's distribution over reasoning paths, slashing semantic entropy and undermining creative problem-solving. To analyze this failure, we introduce Distributional Creative Reasoning (DCR), a unified variational objective that casts training as gradient flow through probability measures on solution traces. STaR, GRPO, and DPO, as well as entropy bonuses, and other methods, all constitute special cases of the same loss. The framework delivers three core results: (i) the diversity decay theorem, describing how correctness-based objectives lead to distinct modes of diversity decay for STaR, GRPO, and DPO; (ii) designs that ensure convergence to a stable and diverse policy, effectively preventing collapse; and (iii) simple, actionable recipes to achieve this in practice. DCR thus offers the first principled recipe for LLMs that remain both correct and creative.", "link": "http://arxiv.org/abs/2601.00747v1", "date": "2026-01-02", "relevancy": 1.4987, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5155}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4954}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Reasoning-Creativity%20Trade-off%3A%20Toward%20Creativity-Driven%20Problem%20Solving&body=Title%3A%20The%20Reasoning-Creativity%20Trade-off%3A%20Toward%20Creativity-Driven%20Problem%20Solving%0AAuthor%3A%20Max%20Ruiz%20Luyten%20and%20Mihaela%20van%20der%20Schaar%0AAbstract%3A%20State-of-the-art%20large%20language%20model%20%28LLM%29%20pipelines%20rely%20on%20bootstrapped%20reasoning%20loops%3A%20sampling%20diverse%20chains%20of%20thought%20and%20reinforcing%20the%20highest-scoring%20ones%2C%20mainly%20optimizing%20correctness.%20We%20analyze%20how%20this%20design%20choice%20is%20sensitive%20to%20the%20collapse%20of%20the%20model%27s%20distribution%20over%20reasoning%20paths%2C%20slashing%20semantic%20entropy%20and%20undermining%20creative%20problem-solving.%20To%20analyze%20this%20failure%2C%20we%20introduce%20Distributional%20Creative%20Reasoning%20%28DCR%29%2C%20a%20unified%20variational%20objective%20that%20casts%20training%20as%20gradient%20flow%20through%20probability%20measures%20on%20solution%20traces.%20STaR%2C%20GRPO%2C%20and%20DPO%2C%20as%20well%20as%20entropy%20bonuses%2C%20and%20other%20methods%2C%20all%20constitute%20special%20cases%20of%20the%20same%20loss.%20The%20framework%20delivers%20three%20core%20results%3A%20%28i%29%20the%20diversity%20decay%20theorem%2C%20describing%20how%20correctness-based%20objectives%20lead%20to%20distinct%20modes%20of%20diversity%20decay%20for%20STaR%2C%20GRPO%2C%20and%20DPO%3B%20%28ii%29%20designs%20that%20ensure%20convergence%20to%20a%20stable%20and%20diverse%20policy%2C%20effectively%20preventing%20collapse%3B%20and%20%28iii%29%20simple%2C%20actionable%20recipes%20to%20achieve%20this%20in%20practice.%20DCR%20thus%20offers%20the%20first%20principled%20recipe%20for%20LLMs%20that%20remain%20both%20correct%20and%20creative.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00747v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Reasoning-Creativity%2520Trade-off%253A%2520Toward%2520Creativity-Driven%2520Problem%2520Solving%26entry.906535625%3DMax%2520Ruiz%2520Luyten%2520and%2520Mihaela%2520van%2520der%2520Schaar%26entry.1292438233%3DState-of-the-art%2520large%2520language%2520model%2520%2528LLM%2529%2520pipelines%2520rely%2520on%2520bootstrapped%2520reasoning%2520loops%253A%2520sampling%2520diverse%2520chains%2520of%2520thought%2520and%2520reinforcing%2520the%2520highest-scoring%2520ones%252C%2520mainly%2520optimizing%2520correctness.%2520We%2520analyze%2520how%2520this%2520design%2520choice%2520is%2520sensitive%2520to%2520the%2520collapse%2520of%2520the%2520model%2527s%2520distribution%2520over%2520reasoning%2520paths%252C%2520slashing%2520semantic%2520entropy%2520and%2520undermining%2520creative%2520problem-solving.%2520To%2520analyze%2520this%2520failure%252C%2520we%2520introduce%2520Distributional%2520Creative%2520Reasoning%2520%2528DCR%2529%252C%2520a%2520unified%2520variational%2520objective%2520that%2520casts%2520training%2520as%2520gradient%2520flow%2520through%2520probability%2520measures%2520on%2520solution%2520traces.%2520STaR%252C%2520GRPO%252C%2520and%2520DPO%252C%2520as%2520well%2520as%2520entropy%2520bonuses%252C%2520and%2520other%2520methods%252C%2520all%2520constitute%2520special%2520cases%2520of%2520the%2520same%2520loss.%2520The%2520framework%2520delivers%2520three%2520core%2520results%253A%2520%2528i%2529%2520the%2520diversity%2520decay%2520theorem%252C%2520describing%2520how%2520correctness-based%2520objectives%2520lead%2520to%2520distinct%2520modes%2520of%2520diversity%2520decay%2520for%2520STaR%252C%2520GRPO%252C%2520and%2520DPO%253B%2520%2528ii%2529%2520designs%2520that%2520ensure%2520convergence%2520to%2520a%2520stable%2520and%2520diverse%2520policy%252C%2520effectively%2520preventing%2520collapse%253B%2520and%2520%2528iii%2529%2520simple%252C%2520actionable%2520recipes%2520to%2520achieve%2520this%2520in%2520practice.%2520DCR%2520thus%2520offers%2520the%2520first%2520principled%2520recipe%2520for%2520LLMs%2520that%2520remain%2520both%2520correct%2520and%2520creative.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00747v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Reasoning-Creativity%20Trade-off%3A%20Toward%20Creativity-Driven%20Problem%20Solving&entry.906535625=Max%20Ruiz%20Luyten%20and%20Mihaela%20van%20der%20Schaar&entry.1292438233=State-of-the-art%20large%20language%20model%20%28LLM%29%20pipelines%20rely%20on%20bootstrapped%20reasoning%20loops%3A%20sampling%20diverse%20chains%20of%20thought%20and%20reinforcing%20the%20highest-scoring%20ones%2C%20mainly%20optimizing%20correctness.%20We%20analyze%20how%20this%20design%20choice%20is%20sensitive%20to%20the%20collapse%20of%20the%20model%27s%20distribution%20over%20reasoning%20paths%2C%20slashing%20semantic%20entropy%20and%20undermining%20creative%20problem-solving.%20To%20analyze%20this%20failure%2C%20we%20introduce%20Distributional%20Creative%20Reasoning%20%28DCR%29%2C%20a%20unified%20variational%20objective%20that%20casts%20training%20as%20gradient%20flow%20through%20probability%20measures%20on%20solution%20traces.%20STaR%2C%20GRPO%2C%20and%20DPO%2C%20as%20well%20as%20entropy%20bonuses%2C%20and%20other%20methods%2C%20all%20constitute%20special%20cases%20of%20the%20same%20loss.%20The%20framework%20delivers%20three%20core%20results%3A%20%28i%29%20the%20diversity%20decay%20theorem%2C%20describing%20how%20correctness-based%20objectives%20lead%20to%20distinct%20modes%20of%20diversity%20decay%20for%20STaR%2C%20GRPO%2C%20and%20DPO%3B%20%28ii%29%20designs%20that%20ensure%20convergence%20to%20a%20stable%20and%20diverse%20policy%2C%20effectively%20preventing%20collapse%3B%20and%20%28iii%29%20simple%2C%20actionable%20recipes%20to%20achieve%20this%20in%20practice.%20DCR%20thus%20offers%20the%20first%20principled%20recipe%20for%20LLMs%20that%20remain%20both%20correct%20and%20creative.&entry.1838667208=http%3A//arxiv.org/abs/2601.00747v1&entry.124074799=Read"},
{"title": "ARISE: Adaptive Reinforcement Integrated with Swarm Exploration", "author": "Rajiv Chaitanya M and D R Ramesh Babu", "abstract": "Effective exploration remains a key challenge in RL, especially with non-stationary rewards or high-dimensional policies. We introduce ARISE, a lightweight framework that enhances reinforcement learning by augmenting standard policy-gradient methods with a compact swarm-based exploration layer. ARISE blends policy actions with particle-driven proposals, where each particle represents a candidate policy trajectory sampled in the action space, and modulates exploration adaptively using reward-variance cues. While easy benchmarks exhibit only slight improvements (e.g., +0.7% on CartPole-v1), ARISE yields substantial gains on more challenging tasks, including +46% on LunarLander-v3 and +22% on Hopper-v4, while preserving stability on Walker2d and Ant. Under non-stationary reward shifts, ARISE provides marked robustness advantages, outperforming PPO by +75 points on CartPole and improving LunarLander accordingly. Ablation studies confirm that both the swarm component and the adaptive mechanism contribute to the performance. Overall, ARISE offers a simple, architecture-agnostic route to more exploratory and resilient RL agents without altering core algorithmic structures.", "link": "http://arxiv.org/abs/2601.00693v1", "date": "2026-01-02", "relevancy": 1.4828, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5392}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5006}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ARISE%3A%20Adaptive%20Reinforcement%20Integrated%20with%20Swarm%20Exploration&body=Title%3A%20ARISE%3A%20Adaptive%20Reinforcement%20Integrated%20with%20Swarm%20Exploration%0AAuthor%3A%20Rajiv%20Chaitanya%20M%20and%20D%20R%20Ramesh%20Babu%0AAbstract%3A%20Effective%20exploration%20remains%20a%20key%20challenge%20in%20RL%2C%20especially%20with%20non-stationary%20rewards%20or%20high-dimensional%20policies.%20We%20introduce%20ARISE%2C%20a%20lightweight%20framework%20that%20enhances%20reinforcement%20learning%20by%20augmenting%20standard%20policy-gradient%20methods%20with%20a%20compact%20swarm-based%20exploration%20layer.%20ARISE%20blends%20policy%20actions%20with%20particle-driven%20proposals%2C%20where%20each%20particle%20represents%20a%20candidate%20policy%20trajectory%20sampled%20in%20the%20action%20space%2C%20and%20modulates%20exploration%20adaptively%20using%20reward-variance%20cues.%20While%20easy%20benchmarks%20exhibit%20only%20slight%20improvements%20%28e.g.%2C%20%2B0.7%25%20on%20CartPole-v1%29%2C%20ARISE%20yields%20substantial%20gains%20on%20more%20challenging%20tasks%2C%20including%20%2B46%25%20on%20LunarLander-v3%20and%20%2B22%25%20on%20Hopper-v4%2C%20while%20preserving%20stability%20on%20Walker2d%20and%20Ant.%20Under%20non-stationary%20reward%20shifts%2C%20ARISE%20provides%20marked%20robustness%20advantages%2C%20outperforming%20PPO%20by%20%2B75%20points%20on%20CartPole%20and%20improving%20LunarLander%20accordingly.%20Ablation%20studies%20confirm%20that%20both%20the%20swarm%20component%20and%20the%20adaptive%20mechanism%20contribute%20to%20the%20performance.%20Overall%2C%20ARISE%20offers%20a%20simple%2C%20architecture-agnostic%20route%20to%20more%20exploratory%20and%20resilient%20RL%20agents%20without%20altering%20core%20algorithmic%20structures.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00693v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DARISE%253A%2520Adaptive%2520Reinforcement%2520Integrated%2520with%2520Swarm%2520Exploration%26entry.906535625%3DRajiv%2520Chaitanya%2520M%2520and%2520D%2520R%2520Ramesh%2520Babu%26entry.1292438233%3DEffective%2520exploration%2520remains%2520a%2520key%2520challenge%2520in%2520RL%252C%2520especially%2520with%2520non-stationary%2520rewards%2520or%2520high-dimensional%2520policies.%2520We%2520introduce%2520ARISE%252C%2520a%2520lightweight%2520framework%2520that%2520enhances%2520reinforcement%2520learning%2520by%2520augmenting%2520standard%2520policy-gradient%2520methods%2520with%2520a%2520compact%2520swarm-based%2520exploration%2520layer.%2520ARISE%2520blends%2520policy%2520actions%2520with%2520particle-driven%2520proposals%252C%2520where%2520each%2520particle%2520represents%2520a%2520candidate%2520policy%2520trajectory%2520sampled%2520in%2520the%2520action%2520space%252C%2520and%2520modulates%2520exploration%2520adaptively%2520using%2520reward-variance%2520cues.%2520While%2520easy%2520benchmarks%2520exhibit%2520only%2520slight%2520improvements%2520%2528e.g.%252C%2520%252B0.7%2525%2520on%2520CartPole-v1%2529%252C%2520ARISE%2520yields%2520substantial%2520gains%2520on%2520more%2520challenging%2520tasks%252C%2520including%2520%252B46%2525%2520on%2520LunarLander-v3%2520and%2520%252B22%2525%2520on%2520Hopper-v4%252C%2520while%2520preserving%2520stability%2520on%2520Walker2d%2520and%2520Ant.%2520Under%2520non-stationary%2520reward%2520shifts%252C%2520ARISE%2520provides%2520marked%2520robustness%2520advantages%252C%2520outperforming%2520PPO%2520by%2520%252B75%2520points%2520on%2520CartPole%2520and%2520improving%2520LunarLander%2520accordingly.%2520Ablation%2520studies%2520confirm%2520that%2520both%2520the%2520swarm%2520component%2520and%2520the%2520adaptive%2520mechanism%2520contribute%2520to%2520the%2520performance.%2520Overall%252C%2520ARISE%2520offers%2520a%2520simple%252C%2520architecture-agnostic%2520route%2520to%2520more%2520exploratory%2520and%2520resilient%2520RL%2520agents%2520without%2520altering%2520core%2520algorithmic%2520structures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00693v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ARISE%3A%20Adaptive%20Reinforcement%20Integrated%20with%20Swarm%20Exploration&entry.906535625=Rajiv%20Chaitanya%20M%20and%20D%20R%20Ramesh%20Babu&entry.1292438233=Effective%20exploration%20remains%20a%20key%20challenge%20in%20RL%2C%20especially%20with%20non-stationary%20rewards%20or%20high-dimensional%20policies.%20We%20introduce%20ARISE%2C%20a%20lightweight%20framework%20that%20enhances%20reinforcement%20learning%20by%20augmenting%20standard%20policy-gradient%20methods%20with%20a%20compact%20swarm-based%20exploration%20layer.%20ARISE%20blends%20policy%20actions%20with%20particle-driven%20proposals%2C%20where%20each%20particle%20represents%20a%20candidate%20policy%20trajectory%20sampled%20in%20the%20action%20space%2C%20and%20modulates%20exploration%20adaptively%20using%20reward-variance%20cues.%20While%20easy%20benchmarks%20exhibit%20only%20slight%20improvements%20%28e.g.%2C%20%2B0.7%25%20on%20CartPole-v1%29%2C%20ARISE%20yields%20substantial%20gains%20on%20more%20challenging%20tasks%2C%20including%20%2B46%25%20on%20LunarLander-v3%20and%20%2B22%25%20on%20Hopper-v4%2C%20while%20preserving%20stability%20on%20Walker2d%20and%20Ant.%20Under%20non-stationary%20reward%20shifts%2C%20ARISE%20provides%20marked%20robustness%20advantages%2C%20outperforming%20PPO%20by%20%2B75%20points%20on%20CartPole%20and%20improving%20LunarLander%20accordingly.%20Ablation%20studies%20confirm%20that%20both%20the%20swarm%20component%20and%20the%20adaptive%20mechanism%20contribute%20to%20the%20performance.%20Overall%2C%20ARISE%20offers%20a%20simple%2C%20architecture-agnostic%20route%20to%20more%20exploratory%20and%20resilient%20RL%20agents%20without%20altering%20core%20algorithmic%20structures.&entry.1838667208=http%3A//arxiv.org/abs/2601.00693v1&entry.124074799=Read"},
{"title": "MCD: Marginal Contrastive Discrimination for conditional density estimation", "author": "Katia Meziani and Aminata Ndiaye and Benjamin Riu", "abstract": "We consider the problem of conditional density estimation, which is a major topic of interest in the fields of statistical and machine learning. Our method, called Marginal Contrastive Discrimination, MCD, reformulates the conditional density function into two factors, the marginal density function of the target variable and a ratio of density functions which can be estimated through binary classification. Like noise-contrastive methods, MCD can leverage state-of-the-art supervised learning techniques to perform conditional density estimation, including neural networks. Our benchmark reveals that our method significantly outperforms in practice existing methods on most density models and regression datasets.", "link": "http://arxiv.org/abs/2206.01592v2", "date": "2026-01-02", "relevancy": 1.4602, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4958}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4804}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MCD%3A%20Marginal%20Contrastive%20Discrimination%20for%20conditional%20density%20estimation&body=Title%3A%20MCD%3A%20Marginal%20Contrastive%20Discrimination%20for%20conditional%20density%20estimation%0AAuthor%3A%20Katia%20Meziani%20and%20Aminata%20Ndiaye%20and%20Benjamin%20Riu%0AAbstract%3A%20We%20consider%20the%20problem%20of%20conditional%20density%20estimation%2C%20which%20is%20a%20major%20topic%20of%20interest%20in%20the%20fields%20of%20statistical%20and%20machine%20learning.%20Our%20method%2C%20called%20Marginal%20Contrastive%20Discrimination%2C%20MCD%2C%20reformulates%20the%20conditional%20density%20function%20into%20two%20factors%2C%20the%20marginal%20density%20function%20of%20the%20target%20variable%20and%20a%20ratio%20of%20density%20functions%20which%20can%20be%20estimated%20through%20binary%20classification.%20Like%20noise-contrastive%20methods%2C%20MCD%20can%20leverage%20state-of-the-art%20supervised%20learning%20techniques%20to%20perform%20conditional%20density%20estimation%2C%20including%20neural%20networks.%20Our%20benchmark%20reveals%20that%20our%20method%20significantly%20outperforms%20in%20practice%20existing%20methods%20on%20most%20density%20models%20and%20regression%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2206.01592v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMCD%253A%2520Marginal%2520Contrastive%2520Discrimination%2520for%2520conditional%2520density%2520estimation%26entry.906535625%3DKatia%2520Meziani%2520and%2520Aminata%2520Ndiaye%2520and%2520Benjamin%2520Riu%26entry.1292438233%3DWe%2520consider%2520the%2520problem%2520of%2520conditional%2520density%2520estimation%252C%2520which%2520is%2520a%2520major%2520topic%2520of%2520interest%2520in%2520the%2520fields%2520of%2520statistical%2520and%2520machine%2520learning.%2520Our%2520method%252C%2520called%2520Marginal%2520Contrastive%2520Discrimination%252C%2520MCD%252C%2520reformulates%2520the%2520conditional%2520density%2520function%2520into%2520two%2520factors%252C%2520the%2520marginal%2520density%2520function%2520of%2520the%2520target%2520variable%2520and%2520a%2520ratio%2520of%2520density%2520functions%2520which%2520can%2520be%2520estimated%2520through%2520binary%2520classification.%2520Like%2520noise-contrastive%2520methods%252C%2520MCD%2520can%2520leverage%2520state-of-the-art%2520supervised%2520learning%2520techniques%2520to%2520perform%2520conditional%2520density%2520estimation%252C%2520including%2520neural%2520networks.%2520Our%2520benchmark%2520reveals%2520that%2520our%2520method%2520significantly%2520outperforms%2520in%2520practice%2520existing%2520methods%2520on%2520most%2520density%2520models%2520and%2520regression%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2206.01592v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCD%3A%20Marginal%20Contrastive%20Discrimination%20for%20conditional%20density%20estimation&entry.906535625=Katia%20Meziani%20and%20Aminata%20Ndiaye%20and%20Benjamin%20Riu&entry.1292438233=We%20consider%20the%20problem%20of%20conditional%20density%20estimation%2C%20which%20is%20a%20major%20topic%20of%20interest%20in%20the%20fields%20of%20statistical%20and%20machine%20learning.%20Our%20method%2C%20called%20Marginal%20Contrastive%20Discrimination%2C%20MCD%2C%20reformulates%20the%20conditional%20density%20function%20into%20two%20factors%2C%20the%20marginal%20density%20function%20of%20the%20target%20variable%20and%20a%20ratio%20of%20density%20functions%20which%20can%20be%20estimated%20through%20binary%20classification.%20Like%20noise-contrastive%20methods%2C%20MCD%20can%20leverage%20state-of-the-art%20supervised%20learning%20techniques%20to%20perform%20conditional%20density%20estimation%2C%20including%20neural%20networks.%20Our%20benchmark%20reveals%20that%20our%20method%20significantly%20outperforms%20in%20practice%20existing%20methods%20on%20most%20density%20models%20and%20regression%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2206.01592v2&entry.124074799=Read"},
{"title": "Categorical Reparameterization with Denoising Diffusion models", "author": "Samson Gourevitch and Alain Durmus and Eric Moulines and Jimmy Olsson and Yazid Janati", "abstract": "Gradient-based optimization with categorical variables typically relies on score-function estimators, which are unbiased but noisy, or on continuous relaxations that replace the discrete distribution with a smooth surrogate admitting a pathwise (reparameterized) gradient, at the cost of optimizing a biased, temperature-dependent objective. In this paper, we extend this family of relaxations by introducing a diffusion-based soft reparameterization for categorical distributions. For these distributions, the denoiser under a Gaussian noising process admits a closed form and can be computed efficiently, yielding a training-free diffusion sampler through which we can backpropagate. Our experiments show that the proposed reparameterization trick yields competitive or improved optimization performance on various benchmarks.", "link": "http://arxiv.org/abs/2601.00781v1", "date": "2026-01-02", "relevancy": 1.4411, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4978}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4767}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Categorical%20Reparameterization%20with%20Denoising%20Diffusion%20models&body=Title%3A%20Categorical%20Reparameterization%20with%20Denoising%20Diffusion%20models%0AAuthor%3A%20Samson%20Gourevitch%20and%20Alain%20Durmus%20and%20Eric%20Moulines%20and%20Jimmy%20Olsson%20and%20Yazid%20Janati%0AAbstract%3A%20Gradient-based%20optimization%20with%20categorical%20variables%20typically%20relies%20on%20score-function%20estimators%2C%20which%20are%20unbiased%20but%20noisy%2C%20or%20on%20continuous%20relaxations%20that%20replace%20the%20discrete%20distribution%20with%20a%20smooth%20surrogate%20admitting%20a%20pathwise%20%28reparameterized%29%20gradient%2C%20at%20the%20cost%20of%20optimizing%20a%20biased%2C%20temperature-dependent%20objective.%20In%20this%20paper%2C%20we%20extend%20this%20family%20of%20relaxations%20by%20introducing%20a%20diffusion-based%20soft%20reparameterization%20for%20categorical%20distributions.%20For%20these%20distributions%2C%20the%20denoiser%20under%20a%20Gaussian%20noising%20process%20admits%20a%20closed%20form%20and%20can%20be%20computed%20efficiently%2C%20yielding%20a%20training-free%20diffusion%20sampler%20through%20which%20we%20can%20backpropagate.%20Our%20experiments%20show%20that%20the%20proposed%20reparameterization%20trick%20yields%20competitive%20or%20improved%20optimization%20performance%20on%20various%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00781v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCategorical%2520Reparameterization%2520with%2520Denoising%2520Diffusion%2520models%26entry.906535625%3DSamson%2520Gourevitch%2520and%2520Alain%2520Durmus%2520and%2520Eric%2520Moulines%2520and%2520Jimmy%2520Olsson%2520and%2520Yazid%2520Janati%26entry.1292438233%3DGradient-based%2520optimization%2520with%2520categorical%2520variables%2520typically%2520relies%2520on%2520score-function%2520estimators%252C%2520which%2520are%2520unbiased%2520but%2520noisy%252C%2520or%2520on%2520continuous%2520relaxations%2520that%2520replace%2520the%2520discrete%2520distribution%2520with%2520a%2520smooth%2520surrogate%2520admitting%2520a%2520pathwise%2520%2528reparameterized%2529%2520gradient%252C%2520at%2520the%2520cost%2520of%2520optimizing%2520a%2520biased%252C%2520temperature-dependent%2520objective.%2520In%2520this%2520paper%252C%2520we%2520extend%2520this%2520family%2520of%2520relaxations%2520by%2520introducing%2520a%2520diffusion-based%2520soft%2520reparameterization%2520for%2520categorical%2520distributions.%2520For%2520these%2520distributions%252C%2520the%2520denoiser%2520under%2520a%2520Gaussian%2520noising%2520process%2520admits%2520a%2520closed%2520form%2520and%2520can%2520be%2520computed%2520efficiently%252C%2520yielding%2520a%2520training-free%2520diffusion%2520sampler%2520through%2520which%2520we%2520can%2520backpropagate.%2520Our%2520experiments%2520show%2520that%2520the%2520proposed%2520reparameterization%2520trick%2520yields%2520competitive%2520or%2520improved%2520optimization%2520performance%2520on%2520various%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00781v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Categorical%20Reparameterization%20with%20Denoising%20Diffusion%20models&entry.906535625=Samson%20Gourevitch%20and%20Alain%20Durmus%20and%20Eric%20Moulines%20and%20Jimmy%20Olsson%20and%20Yazid%20Janati&entry.1292438233=Gradient-based%20optimization%20with%20categorical%20variables%20typically%20relies%20on%20score-function%20estimators%2C%20which%20are%20unbiased%20but%20noisy%2C%20or%20on%20continuous%20relaxations%20that%20replace%20the%20discrete%20distribution%20with%20a%20smooth%20surrogate%20admitting%20a%20pathwise%20%28reparameterized%29%20gradient%2C%20at%20the%20cost%20of%20optimizing%20a%20biased%2C%20temperature-dependent%20objective.%20In%20this%20paper%2C%20we%20extend%20this%20family%20of%20relaxations%20by%20introducing%20a%20diffusion-based%20soft%20reparameterization%20for%20categorical%20distributions.%20For%20these%20distributions%2C%20the%20denoiser%20under%20a%20Gaussian%20noising%20process%20admits%20a%20closed%20form%20and%20can%20be%20computed%20efficiently%2C%20yielding%20a%20training-free%20diffusion%20sampler%20through%20which%20we%20can%20backpropagate.%20Our%20experiments%20show%20that%20the%20proposed%20reparameterization%20trick%20yields%20competitive%20or%20improved%20optimization%20performance%20on%20various%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2601.00781v1&entry.124074799=Read"},
{"title": "Quality Detection of Stored Potatoes via Transfer Learning: A CNN and Vision Transformer Approach", "author": "Shrikant Kapse and Priyankkumar Dhrangdhariya and Priya Kedia and Manasi Patwardhan and Shankar Kausley and Soumyadipta Maiti and Beena Rai and Shirish Karande", "abstract": "Image-based deep learning provides a non-invasive, scalable solution for monitoring potato quality during storage, addressing key challenges such as sprout detection, weight loss estimation, and shelf-life prediction. In this study, images and corresponding weight data were collected over a 200-day period under controlled temperature and humidity conditions. Leveraging powerful pre-trained architectures of ResNet, VGG, DenseNet, and Vision Transformer (ViT), we designed two specialized models: (1) a high-precision binary classifier for sprout detection, and (2) an advanced multi-class predictor to estimate weight loss and forecast remaining shelf-life with remarkable accuracy. DenseNet achieved exceptional performance, with 98.03% accuracy in sprout detection. Shelf-life prediction models performed best with coarse class divisions (2-5 classes), achieving over 89.83% accuracy, while accuracy declined for finer divisions (6-8 classes) due to subtle visual differences and limited data per class. These findings demonstrate the feasibility of integrating image-based models into automated sorting and inventory systems, enabling early identification of sprouted potatoes and dynamic categorization based on storage stage. Practical implications include improved inventory management, differential pricing strategies, and reduced food waste across supply chains. While predicting exact shelf-life intervals remains challenging, focusing on broader class divisions ensures robust performance. Future research should aim to develop generalized models trained on diverse potato varieties and storage conditions to enhance adaptability and scalability. Overall, this approach offers a cost-effective, non-destructive method for quality assessment, supporting efficiency and sustainability in potato storage and distribution.", "link": "http://arxiv.org/abs/2601.00645v1", "date": "2026-01-02", "relevancy": 1.4383, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4955}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4794}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quality%20Detection%20of%20Stored%20Potatoes%20via%20Transfer%20Learning%3A%20A%20CNN%20and%20Vision%20Transformer%20Approach&body=Title%3A%20Quality%20Detection%20of%20Stored%20Potatoes%20via%20Transfer%20Learning%3A%20A%20CNN%20and%20Vision%20Transformer%20Approach%0AAuthor%3A%20Shrikant%20Kapse%20and%20Priyankkumar%20Dhrangdhariya%20and%20Priya%20Kedia%20and%20Manasi%20Patwardhan%20and%20Shankar%20Kausley%20and%20Soumyadipta%20Maiti%20and%20Beena%20Rai%20and%20Shirish%20Karande%0AAbstract%3A%20Image-based%20deep%20learning%20provides%20a%20non-invasive%2C%20scalable%20solution%20for%20monitoring%20potato%20quality%20during%20storage%2C%20addressing%20key%20challenges%20such%20as%20sprout%20detection%2C%20weight%20loss%20estimation%2C%20and%20shelf-life%20prediction.%20In%20this%20study%2C%20images%20and%20corresponding%20weight%20data%20were%20collected%20over%20a%20200-day%20period%20under%20controlled%20temperature%20and%20humidity%20conditions.%20Leveraging%20powerful%20pre-trained%20architectures%20of%20ResNet%2C%20VGG%2C%20DenseNet%2C%20and%20Vision%20Transformer%20%28ViT%29%2C%20we%20designed%20two%20specialized%20models%3A%20%281%29%20a%20high-precision%20binary%20classifier%20for%20sprout%20detection%2C%20and%20%282%29%20an%20advanced%20multi-class%20predictor%20to%20estimate%20weight%20loss%20and%20forecast%20remaining%20shelf-life%20with%20remarkable%20accuracy.%20DenseNet%20achieved%20exceptional%20performance%2C%20with%2098.03%25%20accuracy%20in%20sprout%20detection.%20Shelf-life%20prediction%20models%20performed%20best%20with%20coarse%20class%20divisions%20%282-5%20classes%29%2C%20achieving%20over%2089.83%25%20accuracy%2C%20while%20accuracy%20declined%20for%20finer%20divisions%20%286-8%20classes%29%20due%20to%20subtle%20visual%20differences%20and%20limited%20data%20per%20class.%20These%20findings%20demonstrate%20the%20feasibility%20of%20integrating%20image-based%20models%20into%20automated%20sorting%20and%20inventory%20systems%2C%20enabling%20early%20identification%20of%20sprouted%20potatoes%20and%20dynamic%20categorization%20based%20on%20storage%20stage.%20Practical%20implications%20include%20improved%20inventory%20management%2C%20differential%20pricing%20strategies%2C%20and%20reduced%20food%20waste%20across%20supply%20chains.%20While%20predicting%20exact%20shelf-life%20intervals%20remains%20challenging%2C%20focusing%20on%20broader%20class%20divisions%20ensures%20robust%20performance.%20Future%20research%20should%20aim%20to%20develop%20generalized%20models%20trained%20on%20diverse%20potato%20varieties%20and%20storage%20conditions%20to%20enhance%20adaptability%20and%20scalability.%20Overall%2C%20this%20approach%20offers%20a%20cost-effective%2C%20non-destructive%20method%20for%20quality%20assessment%2C%20supporting%20efficiency%20and%20sustainability%20in%20potato%20storage%20and%20distribution.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00645v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuality%2520Detection%2520of%2520Stored%2520Potatoes%2520via%2520Transfer%2520Learning%253A%2520A%2520CNN%2520and%2520Vision%2520Transformer%2520Approach%26entry.906535625%3DShrikant%2520Kapse%2520and%2520Priyankkumar%2520Dhrangdhariya%2520and%2520Priya%2520Kedia%2520and%2520Manasi%2520Patwardhan%2520and%2520Shankar%2520Kausley%2520and%2520Soumyadipta%2520Maiti%2520and%2520Beena%2520Rai%2520and%2520Shirish%2520Karande%26entry.1292438233%3DImage-based%2520deep%2520learning%2520provides%2520a%2520non-invasive%252C%2520scalable%2520solution%2520for%2520monitoring%2520potato%2520quality%2520during%2520storage%252C%2520addressing%2520key%2520challenges%2520such%2520as%2520sprout%2520detection%252C%2520weight%2520loss%2520estimation%252C%2520and%2520shelf-life%2520prediction.%2520In%2520this%2520study%252C%2520images%2520and%2520corresponding%2520weight%2520data%2520were%2520collected%2520over%2520a%2520200-day%2520period%2520under%2520controlled%2520temperature%2520and%2520humidity%2520conditions.%2520Leveraging%2520powerful%2520pre-trained%2520architectures%2520of%2520ResNet%252C%2520VGG%252C%2520DenseNet%252C%2520and%2520Vision%2520Transformer%2520%2528ViT%2529%252C%2520we%2520designed%2520two%2520specialized%2520models%253A%2520%25281%2529%2520a%2520high-precision%2520binary%2520classifier%2520for%2520sprout%2520detection%252C%2520and%2520%25282%2529%2520an%2520advanced%2520multi-class%2520predictor%2520to%2520estimate%2520weight%2520loss%2520and%2520forecast%2520remaining%2520shelf-life%2520with%2520remarkable%2520accuracy.%2520DenseNet%2520achieved%2520exceptional%2520performance%252C%2520with%252098.03%2525%2520accuracy%2520in%2520sprout%2520detection.%2520Shelf-life%2520prediction%2520models%2520performed%2520best%2520with%2520coarse%2520class%2520divisions%2520%25282-5%2520classes%2529%252C%2520achieving%2520over%252089.83%2525%2520accuracy%252C%2520while%2520accuracy%2520declined%2520for%2520finer%2520divisions%2520%25286-8%2520classes%2529%2520due%2520to%2520subtle%2520visual%2520differences%2520and%2520limited%2520data%2520per%2520class.%2520These%2520findings%2520demonstrate%2520the%2520feasibility%2520of%2520integrating%2520image-based%2520models%2520into%2520automated%2520sorting%2520and%2520inventory%2520systems%252C%2520enabling%2520early%2520identification%2520of%2520sprouted%2520potatoes%2520and%2520dynamic%2520categorization%2520based%2520on%2520storage%2520stage.%2520Practical%2520implications%2520include%2520improved%2520inventory%2520management%252C%2520differential%2520pricing%2520strategies%252C%2520and%2520reduced%2520food%2520waste%2520across%2520supply%2520chains.%2520While%2520predicting%2520exact%2520shelf-life%2520intervals%2520remains%2520challenging%252C%2520focusing%2520on%2520broader%2520class%2520divisions%2520ensures%2520robust%2520performance.%2520Future%2520research%2520should%2520aim%2520to%2520develop%2520generalized%2520models%2520trained%2520on%2520diverse%2520potato%2520varieties%2520and%2520storage%2520conditions%2520to%2520enhance%2520adaptability%2520and%2520scalability.%2520Overall%252C%2520this%2520approach%2520offers%2520a%2520cost-effective%252C%2520non-destructive%2520method%2520for%2520quality%2520assessment%252C%2520supporting%2520efficiency%2520and%2520sustainability%2520in%2520potato%2520storage%2520and%2520distribution.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00645v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quality%20Detection%20of%20Stored%20Potatoes%20via%20Transfer%20Learning%3A%20A%20CNN%20and%20Vision%20Transformer%20Approach&entry.906535625=Shrikant%20Kapse%20and%20Priyankkumar%20Dhrangdhariya%20and%20Priya%20Kedia%20and%20Manasi%20Patwardhan%20and%20Shankar%20Kausley%20and%20Soumyadipta%20Maiti%20and%20Beena%20Rai%20and%20Shirish%20Karande&entry.1292438233=Image-based%20deep%20learning%20provides%20a%20non-invasive%2C%20scalable%20solution%20for%20monitoring%20potato%20quality%20during%20storage%2C%20addressing%20key%20challenges%20such%20as%20sprout%20detection%2C%20weight%20loss%20estimation%2C%20and%20shelf-life%20prediction.%20In%20this%20study%2C%20images%20and%20corresponding%20weight%20data%20were%20collected%20over%20a%20200-day%20period%20under%20controlled%20temperature%20and%20humidity%20conditions.%20Leveraging%20powerful%20pre-trained%20architectures%20of%20ResNet%2C%20VGG%2C%20DenseNet%2C%20and%20Vision%20Transformer%20%28ViT%29%2C%20we%20designed%20two%20specialized%20models%3A%20%281%29%20a%20high-precision%20binary%20classifier%20for%20sprout%20detection%2C%20and%20%282%29%20an%20advanced%20multi-class%20predictor%20to%20estimate%20weight%20loss%20and%20forecast%20remaining%20shelf-life%20with%20remarkable%20accuracy.%20DenseNet%20achieved%20exceptional%20performance%2C%20with%2098.03%25%20accuracy%20in%20sprout%20detection.%20Shelf-life%20prediction%20models%20performed%20best%20with%20coarse%20class%20divisions%20%282-5%20classes%29%2C%20achieving%20over%2089.83%25%20accuracy%2C%20while%20accuracy%20declined%20for%20finer%20divisions%20%286-8%20classes%29%20due%20to%20subtle%20visual%20differences%20and%20limited%20data%20per%20class.%20These%20findings%20demonstrate%20the%20feasibility%20of%20integrating%20image-based%20models%20into%20automated%20sorting%20and%20inventory%20systems%2C%20enabling%20early%20identification%20of%20sprouted%20potatoes%20and%20dynamic%20categorization%20based%20on%20storage%20stage.%20Practical%20implications%20include%20improved%20inventory%20management%2C%20differential%20pricing%20strategies%2C%20and%20reduced%20food%20waste%20across%20supply%20chains.%20While%20predicting%20exact%20shelf-life%20intervals%20remains%20challenging%2C%20focusing%20on%20broader%20class%20divisions%20ensures%20robust%20performance.%20Future%20research%20should%20aim%20to%20develop%20generalized%20models%20trained%20on%20diverse%20potato%20varieties%20and%20storage%20conditions%20to%20enhance%20adaptability%20and%20scalability.%20Overall%2C%20this%20approach%20offers%20a%20cost-effective%2C%20non-destructive%20method%20for%20quality%20assessment%2C%20supporting%20efficiency%20and%20sustainability%20in%20potato%20storage%20and%20distribution.&entry.1838667208=http%3A//arxiv.org/abs/2601.00645v1&entry.124074799=Read"},
{"title": "Effects of Structural Allocation of Geometric Task Diversity in Linear Meta-Learning Models", "author": "Saptati Datta and Nicolas W. Hengartner and Yulia Pimonova and Natalie E. Klein and Nicholas Lubbers", "abstract": "Meta-learning aims to leverage information across related tasks to improve prediction on unlabeled data for new tasks when only a small number of labeled observations are available (\"few-shot\" learning). Increased task diversity is often believed to enhance meta-learning by providing richer information across tasks. However, recent work by Kumar et al. (2022) shows that increasing task diversity, quantified through the overall geometric spread of task representations, can in fact degrade meta-learning prediction performance across a range of models and datasets. In this work, we build on this observation by showing that meta-learning performance is affected not only by the overall geometric variability of task parameters, but also by how this variability is allocated relative to an underlying low-dimensional structure. Similar to Pimonova et al. (2025), we decompose task-specific regression effects into a structurally informative component and an orthogonal, non-informative component. We show theoretically and through simulation that meta-learning prediction degrades when a larger fraction of between-task variability lies in orthogonal, non-informative directions, even when the overall geometric variability of tasks is held fixed.", "link": "http://arxiv.org/abs/2509.18349v2", "date": "2026-01-02", "relevancy": 1.4246, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4816}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4692}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Effects%20of%20Structural%20Allocation%20of%20Geometric%20Task%20Diversity%20in%20Linear%20Meta-Learning%20Models&body=Title%3A%20Effects%20of%20Structural%20Allocation%20of%20Geometric%20Task%20Diversity%20in%20Linear%20Meta-Learning%20Models%0AAuthor%3A%20Saptati%20Datta%20and%20Nicolas%20W.%20Hengartner%20and%20Yulia%20Pimonova%20and%20Natalie%20E.%20Klein%20and%20Nicholas%20Lubbers%0AAbstract%3A%20Meta-learning%20aims%20to%20leverage%20information%20across%20related%20tasks%20to%20improve%20prediction%20on%20unlabeled%20data%20for%20new%20tasks%20when%20only%20a%20small%20number%20of%20labeled%20observations%20are%20available%20%28%22few-shot%22%20learning%29.%20Increased%20task%20diversity%20is%20often%20believed%20to%20enhance%20meta-learning%20by%20providing%20richer%20information%20across%20tasks.%20However%2C%20recent%20work%20by%20Kumar%20et%20al.%20%282022%29%20shows%20that%20increasing%20task%20diversity%2C%20quantified%20through%20the%20overall%20geometric%20spread%20of%20task%20representations%2C%20can%20in%20fact%20degrade%20meta-learning%20prediction%20performance%20across%20a%20range%20of%20models%20and%20datasets.%20In%20this%20work%2C%20we%20build%20on%20this%20observation%20by%20showing%20that%20meta-learning%20performance%20is%20affected%20not%20only%20by%20the%20overall%20geometric%20variability%20of%20task%20parameters%2C%20but%20also%20by%20how%20this%20variability%20is%20allocated%20relative%20to%20an%20underlying%20low-dimensional%20structure.%20Similar%20to%20Pimonova%20et%20al.%20%282025%29%2C%20we%20decompose%20task-specific%20regression%20effects%20into%20a%20structurally%20informative%20component%20and%20an%20orthogonal%2C%20non-informative%20component.%20We%20show%20theoretically%20and%20through%20simulation%20that%20meta-learning%20prediction%20degrades%20when%20a%20larger%20fraction%20of%20between-task%20variability%20lies%20in%20orthogonal%2C%20non-informative%20directions%2C%20even%20when%20the%20overall%20geometric%20variability%20of%20tasks%20is%20held%20fixed.%0ALink%3A%20http%3A//arxiv.org/abs/2509.18349v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEffects%2520of%2520Structural%2520Allocation%2520of%2520Geometric%2520Task%2520Diversity%2520in%2520Linear%2520Meta-Learning%2520Models%26entry.906535625%3DSaptati%2520Datta%2520and%2520Nicolas%2520W.%2520Hengartner%2520and%2520Yulia%2520Pimonova%2520and%2520Natalie%2520E.%2520Klein%2520and%2520Nicholas%2520Lubbers%26entry.1292438233%3DMeta-learning%2520aims%2520to%2520leverage%2520information%2520across%2520related%2520tasks%2520to%2520improve%2520prediction%2520on%2520unlabeled%2520data%2520for%2520new%2520tasks%2520when%2520only%2520a%2520small%2520number%2520of%2520labeled%2520observations%2520are%2520available%2520%2528%2522few-shot%2522%2520learning%2529.%2520Increased%2520task%2520diversity%2520is%2520often%2520believed%2520to%2520enhance%2520meta-learning%2520by%2520providing%2520richer%2520information%2520across%2520tasks.%2520However%252C%2520recent%2520work%2520by%2520Kumar%2520et%2520al.%2520%25282022%2529%2520shows%2520that%2520increasing%2520task%2520diversity%252C%2520quantified%2520through%2520the%2520overall%2520geometric%2520spread%2520of%2520task%2520representations%252C%2520can%2520in%2520fact%2520degrade%2520meta-learning%2520prediction%2520performance%2520across%2520a%2520range%2520of%2520models%2520and%2520datasets.%2520In%2520this%2520work%252C%2520we%2520build%2520on%2520this%2520observation%2520by%2520showing%2520that%2520meta-learning%2520performance%2520is%2520affected%2520not%2520only%2520by%2520the%2520overall%2520geometric%2520variability%2520of%2520task%2520parameters%252C%2520but%2520also%2520by%2520how%2520this%2520variability%2520is%2520allocated%2520relative%2520to%2520an%2520underlying%2520low-dimensional%2520structure.%2520Similar%2520to%2520Pimonova%2520et%2520al.%2520%25282025%2529%252C%2520we%2520decompose%2520task-specific%2520regression%2520effects%2520into%2520a%2520structurally%2520informative%2520component%2520and%2520an%2520orthogonal%252C%2520non-informative%2520component.%2520We%2520show%2520theoretically%2520and%2520through%2520simulation%2520that%2520meta-learning%2520prediction%2520degrades%2520when%2520a%2520larger%2520fraction%2520of%2520between-task%2520variability%2520lies%2520in%2520orthogonal%252C%2520non-informative%2520directions%252C%2520even%2520when%2520the%2520overall%2520geometric%2520variability%2520of%2520tasks%2520is%2520held%2520fixed.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18349v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Effects%20of%20Structural%20Allocation%20of%20Geometric%20Task%20Diversity%20in%20Linear%20Meta-Learning%20Models&entry.906535625=Saptati%20Datta%20and%20Nicolas%20W.%20Hengartner%20and%20Yulia%20Pimonova%20and%20Natalie%20E.%20Klein%20and%20Nicholas%20Lubbers&entry.1292438233=Meta-learning%20aims%20to%20leverage%20information%20across%20related%20tasks%20to%20improve%20prediction%20on%20unlabeled%20data%20for%20new%20tasks%20when%20only%20a%20small%20number%20of%20labeled%20observations%20are%20available%20%28%22few-shot%22%20learning%29.%20Increased%20task%20diversity%20is%20often%20believed%20to%20enhance%20meta-learning%20by%20providing%20richer%20information%20across%20tasks.%20However%2C%20recent%20work%20by%20Kumar%20et%20al.%20%282022%29%20shows%20that%20increasing%20task%20diversity%2C%20quantified%20through%20the%20overall%20geometric%20spread%20of%20task%20representations%2C%20can%20in%20fact%20degrade%20meta-learning%20prediction%20performance%20across%20a%20range%20of%20models%20and%20datasets.%20In%20this%20work%2C%20we%20build%20on%20this%20observation%20by%20showing%20that%20meta-learning%20performance%20is%20affected%20not%20only%20by%20the%20overall%20geometric%20variability%20of%20task%20parameters%2C%20but%20also%20by%20how%20this%20variability%20is%20allocated%20relative%20to%20an%20underlying%20low-dimensional%20structure.%20Similar%20to%20Pimonova%20et%20al.%20%282025%29%2C%20we%20decompose%20task-specific%20regression%20effects%20into%20a%20structurally%20informative%20component%20and%20an%20orthogonal%2C%20non-informative%20component.%20We%20show%20theoretically%20and%20through%20simulation%20that%20meta-learning%20prediction%20degrades%20when%20a%20larger%20fraction%20of%20between-task%20variability%20lies%20in%20orthogonal%2C%20non-informative%20directions%2C%20even%20when%20the%20overall%20geometric%20variability%20of%20tasks%20is%20held%20fixed.&entry.1838667208=http%3A//arxiv.org/abs/2509.18349v2&entry.124074799=Read"},
{"title": "Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning", "author": "Valentin No\u00ebl", "abstract": "We present a training-free method for detecting valid mathematical reasoning in large language models through spectral analysis of attention patterns. By treating attention matrices as adjacency matrices of dynamic graphs over tokens, we extract four interpretable spectral diagnostics, the Fiedler value (algebraic connectivity), high-frequency energy ratio (HFER), graph signal smoothness, and spectral entropy, that exhibit statistically significant differences between valid and invalid mathematical proofs. Experiments across seven transformer models from four independent architectural families (Meta Llama, Alibaba Qwen, Microsoft Phi, and Mistral AI) demonstrate that this spectral signature produces effect sizes up to Cohen's $d = 3.30$ ($p < 10^{-116}$), enabling 85.0--95.6\\% classification accuracy under rigorous evaluation, with calibrated thresholds reaching 93--95\\% on the full dataset. The method requires no training data, fine-tuning, or learned classifiers: a single threshold on a spectral metric suffices for high accuracy. Through systematic label correction, we discover that the spectral method detects logical coherence rather than compiler acceptance, identifying mathematically valid proofs that formal verifiers reject due to technical failures. We further identify an architectural dependency: Mistral-7B's Sliding Window Attention shifts the discriminative signal from HFER to late-layer Smoothness ($d = 2.09$, $p_{\\text{MW}} = 1.16 \\times 10^{-48}$), revealing that attention mechanism design affects which spectral features capture reasoning validity. These findings establish spectral graph analysis as a principled framework for reasoning verification with immediate applications to hallucination detection and AI safety monitoring.", "link": "http://arxiv.org/abs/2601.00791v1", "date": "2026-01-02", "relevancy": 1.4179, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.48}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.464}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4627}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry%20of%20Reason%3A%20Spectral%20Signatures%20of%20Valid%20Mathematical%20Reasoning&body=Title%3A%20Geometry%20of%20Reason%3A%20Spectral%20Signatures%20of%20Valid%20Mathematical%20Reasoning%0AAuthor%3A%20Valentin%20No%C3%ABl%0AAbstract%3A%20We%20present%20a%20training-free%20method%20for%20detecting%20valid%20mathematical%20reasoning%20in%20large%20language%20models%20through%20spectral%20analysis%20of%20attention%20patterns.%20By%20treating%20attention%20matrices%20as%20adjacency%20matrices%20of%20dynamic%20graphs%20over%20tokens%2C%20we%20extract%20four%20interpretable%20spectral%20diagnostics%2C%20the%20Fiedler%20value%20%28algebraic%20connectivity%29%2C%20high-frequency%20energy%20ratio%20%28HFER%29%2C%20graph%20signal%20smoothness%2C%20and%20spectral%20entropy%2C%20that%20exhibit%20statistically%20significant%20differences%20between%20valid%20and%20invalid%20mathematical%20proofs.%20Experiments%20across%20seven%20transformer%20models%20from%20four%20independent%20architectural%20families%20%28Meta%20Llama%2C%20Alibaba%20Qwen%2C%20Microsoft%20Phi%2C%20and%20Mistral%20AI%29%20demonstrate%20that%20this%20spectral%20signature%20produces%20effect%20sizes%20up%20to%20Cohen%27s%20%24d%20%3D%203.30%24%20%28%24p%20%3C%2010%5E%7B-116%7D%24%29%2C%20enabling%2085.0--95.6%5C%25%20classification%20accuracy%20under%20rigorous%20evaluation%2C%20with%20calibrated%20thresholds%20reaching%2093--95%5C%25%20on%20the%20full%20dataset.%20The%20method%20requires%20no%20training%20data%2C%20fine-tuning%2C%20or%20learned%20classifiers%3A%20a%20single%20threshold%20on%20a%20spectral%20metric%20suffices%20for%20high%20accuracy.%20Through%20systematic%20label%20correction%2C%20we%20discover%20that%20the%20spectral%20method%20detects%20logical%20coherence%20rather%20than%20compiler%20acceptance%2C%20identifying%20mathematically%20valid%20proofs%20that%20formal%20verifiers%20reject%20due%20to%20technical%20failures.%20We%20further%20identify%20an%20architectural%20dependency%3A%20Mistral-7B%27s%20Sliding%20Window%20Attention%20shifts%20the%20discriminative%20signal%20from%20HFER%20to%20late-layer%20Smoothness%20%28%24d%20%3D%202.09%24%2C%20%24p_%7B%5Ctext%7BMW%7D%7D%20%3D%201.16%20%5Ctimes%2010%5E%7B-48%7D%24%29%2C%20revealing%20that%20attention%20mechanism%20design%20affects%20which%20spectral%20features%20capture%20reasoning%20validity.%20These%20findings%20establish%20spectral%20graph%20analysis%20as%20a%20principled%20framework%20for%20reasoning%20verification%20with%20immediate%20applications%20to%20hallucination%20detection%20and%20AI%20safety%20monitoring.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00791v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry%2520of%2520Reason%253A%2520Spectral%2520Signatures%2520of%2520Valid%2520Mathematical%2520Reasoning%26entry.906535625%3DValentin%2520No%25C3%25ABl%26entry.1292438233%3DWe%2520present%2520a%2520training-free%2520method%2520for%2520detecting%2520valid%2520mathematical%2520reasoning%2520in%2520large%2520language%2520models%2520through%2520spectral%2520analysis%2520of%2520attention%2520patterns.%2520By%2520treating%2520attention%2520matrices%2520as%2520adjacency%2520matrices%2520of%2520dynamic%2520graphs%2520over%2520tokens%252C%2520we%2520extract%2520four%2520interpretable%2520spectral%2520diagnostics%252C%2520the%2520Fiedler%2520value%2520%2528algebraic%2520connectivity%2529%252C%2520high-frequency%2520energy%2520ratio%2520%2528HFER%2529%252C%2520graph%2520signal%2520smoothness%252C%2520and%2520spectral%2520entropy%252C%2520that%2520exhibit%2520statistically%2520significant%2520differences%2520between%2520valid%2520and%2520invalid%2520mathematical%2520proofs.%2520Experiments%2520across%2520seven%2520transformer%2520models%2520from%2520four%2520independent%2520architectural%2520families%2520%2528Meta%2520Llama%252C%2520Alibaba%2520Qwen%252C%2520Microsoft%2520Phi%252C%2520and%2520Mistral%2520AI%2529%2520demonstrate%2520that%2520this%2520spectral%2520signature%2520produces%2520effect%2520sizes%2520up%2520to%2520Cohen%2527s%2520%2524d%2520%253D%25203.30%2524%2520%2528%2524p%2520%253C%252010%255E%257B-116%257D%2524%2529%252C%2520enabling%252085.0--95.6%255C%2525%2520classification%2520accuracy%2520under%2520rigorous%2520evaluation%252C%2520with%2520calibrated%2520thresholds%2520reaching%252093--95%255C%2525%2520on%2520the%2520full%2520dataset.%2520The%2520method%2520requires%2520no%2520training%2520data%252C%2520fine-tuning%252C%2520or%2520learned%2520classifiers%253A%2520a%2520single%2520threshold%2520on%2520a%2520spectral%2520metric%2520suffices%2520for%2520high%2520accuracy.%2520Through%2520systematic%2520label%2520correction%252C%2520we%2520discover%2520that%2520the%2520spectral%2520method%2520detects%2520logical%2520coherence%2520rather%2520than%2520compiler%2520acceptance%252C%2520identifying%2520mathematically%2520valid%2520proofs%2520that%2520formal%2520verifiers%2520reject%2520due%2520to%2520technical%2520failures.%2520We%2520further%2520identify%2520an%2520architectural%2520dependency%253A%2520Mistral-7B%2527s%2520Sliding%2520Window%2520Attention%2520shifts%2520the%2520discriminative%2520signal%2520from%2520HFER%2520to%2520late-layer%2520Smoothness%2520%2528%2524d%2520%253D%25202.09%2524%252C%2520%2524p_%257B%255Ctext%257BMW%257D%257D%2520%253D%25201.16%2520%255Ctimes%252010%255E%257B-48%257D%2524%2529%252C%2520revealing%2520that%2520attention%2520mechanism%2520design%2520affects%2520which%2520spectral%2520features%2520capture%2520reasoning%2520validity.%2520These%2520findings%2520establish%2520spectral%2520graph%2520analysis%2520as%2520a%2520principled%2520framework%2520for%2520reasoning%2520verification%2520with%2520immediate%2520applications%2520to%2520hallucination%2520detection%2520and%2520AI%2520safety%2520monitoring.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00791v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry%20of%20Reason%3A%20Spectral%20Signatures%20of%20Valid%20Mathematical%20Reasoning&entry.906535625=Valentin%20No%C3%ABl&entry.1292438233=We%20present%20a%20training-free%20method%20for%20detecting%20valid%20mathematical%20reasoning%20in%20large%20language%20models%20through%20spectral%20analysis%20of%20attention%20patterns.%20By%20treating%20attention%20matrices%20as%20adjacency%20matrices%20of%20dynamic%20graphs%20over%20tokens%2C%20we%20extract%20four%20interpretable%20spectral%20diagnostics%2C%20the%20Fiedler%20value%20%28algebraic%20connectivity%29%2C%20high-frequency%20energy%20ratio%20%28HFER%29%2C%20graph%20signal%20smoothness%2C%20and%20spectral%20entropy%2C%20that%20exhibit%20statistically%20significant%20differences%20between%20valid%20and%20invalid%20mathematical%20proofs.%20Experiments%20across%20seven%20transformer%20models%20from%20four%20independent%20architectural%20families%20%28Meta%20Llama%2C%20Alibaba%20Qwen%2C%20Microsoft%20Phi%2C%20and%20Mistral%20AI%29%20demonstrate%20that%20this%20spectral%20signature%20produces%20effect%20sizes%20up%20to%20Cohen%27s%20%24d%20%3D%203.30%24%20%28%24p%20%3C%2010%5E%7B-116%7D%24%29%2C%20enabling%2085.0--95.6%5C%25%20classification%20accuracy%20under%20rigorous%20evaluation%2C%20with%20calibrated%20thresholds%20reaching%2093--95%5C%25%20on%20the%20full%20dataset.%20The%20method%20requires%20no%20training%20data%2C%20fine-tuning%2C%20or%20learned%20classifiers%3A%20a%20single%20threshold%20on%20a%20spectral%20metric%20suffices%20for%20high%20accuracy.%20Through%20systematic%20label%20correction%2C%20we%20discover%20that%20the%20spectral%20method%20detects%20logical%20coherence%20rather%20than%20compiler%20acceptance%2C%20identifying%20mathematically%20valid%20proofs%20that%20formal%20verifiers%20reject%20due%20to%20technical%20failures.%20We%20further%20identify%20an%20architectural%20dependency%3A%20Mistral-7B%27s%20Sliding%20Window%20Attention%20shifts%20the%20discriminative%20signal%20from%20HFER%20to%20late-layer%20Smoothness%20%28%24d%20%3D%202.09%24%2C%20%24p_%7B%5Ctext%7BMW%7D%7D%20%3D%201.16%20%5Ctimes%2010%5E%7B-48%7D%24%29%2C%20revealing%20that%20attention%20mechanism%20design%20affects%20which%20spectral%20features%20capture%20reasoning%20validity.%20These%20findings%20establish%20spectral%20graph%20analysis%20as%20a%20principled%20framework%20for%20reasoning%20verification%20with%20immediate%20applications%20to%20hallucination%20detection%20and%20AI%20safety%20monitoring.&entry.1838667208=http%3A//arxiv.org/abs/2601.00791v1&entry.124074799=Read"},
{"title": "NormCode: A Semi-Formal Language for Auditable AI Planning", "author": "Xin Guan and Yunshan Li and Zekun Wu and Ruibo Zhang", "abstract": "As AI systems move into high stakes domains such as legal reasoning, medical diagnosis, and financial decision making, regulators and practitioners increasingly demand auditability. Auditability means the ability to trace exactly what each step in a multi step workflow saw and did. Current large language model based workflows are fundamentally opaque. Context pollution, defined as the accumulation of information across reasoning steps, causes models to hallucinate and lose track of constraints. At the same time, implicit data flow makes it impossible to reconstruct what any given step actually received as input. We present NormCode, a semi formal language that makes AI workflows auditable by construction. Each inference step operates in enforced data isolation and can access only explicitly passed inputs. This eliminates cross step contamination and ensures that every intermediate state can be inspected. A strict separation between semantic operations, meaning probabilistic language model reasoning, and syntactic operations, meaning deterministic data flow, allows auditors to clearly distinguish inference from mechanical restructuring. The multi format ecosystem, consisting of NCDS, NCD, NCN, and NCDN files, allows developers, domain experts, and auditors to inspect the same plan in formats suited to their individual needs. A four phase compilation pipeline transforms natural language intent into executable JSON repositories. A visual Canvas application provides real time graph visualization and breakpoint debugging. We validate the approach by achieving full accuracy on base X addition and by self hosted execution of the NormCode compiler itself. These results demonstrate that structured intermediate representations can bridge human intuition and machine rigor while maintaining full transparency.", "link": "http://arxiv.org/abs/2512.10563v3", "date": "2026-01-02", "relevancy": 1.4112, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.498}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4842}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NormCode%3A%20A%20Semi-Formal%20Language%20for%20Auditable%20AI%20Planning&body=Title%3A%20NormCode%3A%20A%20Semi-Formal%20Language%20for%20Auditable%20AI%20Planning%0AAuthor%3A%20Xin%20Guan%20and%20Yunshan%20Li%20and%20Zekun%20Wu%20and%20Ruibo%20Zhang%0AAbstract%3A%20As%20AI%20systems%20move%20into%20high%20stakes%20domains%20such%20as%20legal%20reasoning%2C%20medical%20diagnosis%2C%20and%20financial%20decision%20making%2C%20regulators%20and%20practitioners%20increasingly%20demand%20auditability.%20Auditability%20means%20the%20ability%20to%20trace%20exactly%20what%20each%20step%20in%20a%20multi%20step%20workflow%20saw%20and%20did.%20Current%20large%20language%20model%20based%20workflows%20are%20fundamentally%20opaque.%20Context%20pollution%2C%20defined%20as%20the%20accumulation%20of%20information%20across%20reasoning%20steps%2C%20causes%20models%20to%20hallucinate%20and%20lose%20track%20of%20constraints.%20At%20the%20same%20time%2C%20implicit%20data%20flow%20makes%20it%20impossible%20to%20reconstruct%20what%20any%20given%20step%20actually%20received%20as%20input.%20We%20present%20NormCode%2C%20a%20semi%20formal%20language%20that%20makes%20AI%20workflows%20auditable%20by%20construction.%20Each%20inference%20step%20operates%20in%20enforced%20data%20isolation%20and%20can%20access%20only%20explicitly%20passed%20inputs.%20This%20eliminates%20cross%20step%20contamination%20and%20ensures%20that%20every%20intermediate%20state%20can%20be%20inspected.%20A%20strict%20separation%20between%20semantic%20operations%2C%20meaning%20probabilistic%20language%20model%20reasoning%2C%20and%20syntactic%20operations%2C%20meaning%20deterministic%20data%20flow%2C%20allows%20auditors%20to%20clearly%20distinguish%20inference%20from%20mechanical%20restructuring.%20The%20multi%20format%20ecosystem%2C%20consisting%20of%20NCDS%2C%20NCD%2C%20NCN%2C%20and%20NCDN%20files%2C%20allows%20developers%2C%20domain%20experts%2C%20and%20auditors%20to%20inspect%20the%20same%20plan%20in%20formats%20suited%20to%20their%20individual%20needs.%20A%20four%20phase%20compilation%20pipeline%20transforms%20natural%20language%20intent%20into%20executable%20JSON%20repositories.%20A%20visual%20Canvas%20application%20provides%20real%20time%20graph%20visualization%20and%20breakpoint%20debugging.%20We%20validate%20the%20approach%20by%20achieving%20full%20accuracy%20on%20base%20X%20addition%20and%20by%20self%20hosted%20execution%20of%20the%20NormCode%20compiler%20itself.%20These%20results%20demonstrate%20that%20structured%20intermediate%20representations%20can%20bridge%20human%20intuition%20and%20machine%20rigor%20while%20maintaining%20full%20transparency.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10563v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNormCode%253A%2520A%2520Semi-Formal%2520Language%2520for%2520Auditable%2520AI%2520Planning%26entry.906535625%3DXin%2520Guan%2520and%2520Yunshan%2520Li%2520and%2520Zekun%2520Wu%2520and%2520Ruibo%2520Zhang%26entry.1292438233%3DAs%2520AI%2520systems%2520move%2520into%2520high%2520stakes%2520domains%2520such%2520as%2520legal%2520reasoning%252C%2520medical%2520diagnosis%252C%2520and%2520financial%2520decision%2520making%252C%2520regulators%2520and%2520practitioners%2520increasingly%2520demand%2520auditability.%2520Auditability%2520means%2520the%2520ability%2520to%2520trace%2520exactly%2520what%2520each%2520step%2520in%2520a%2520multi%2520step%2520workflow%2520saw%2520and%2520did.%2520Current%2520large%2520language%2520model%2520based%2520workflows%2520are%2520fundamentally%2520opaque.%2520Context%2520pollution%252C%2520defined%2520as%2520the%2520accumulation%2520of%2520information%2520across%2520reasoning%2520steps%252C%2520causes%2520models%2520to%2520hallucinate%2520and%2520lose%2520track%2520of%2520constraints.%2520At%2520the%2520same%2520time%252C%2520implicit%2520data%2520flow%2520makes%2520it%2520impossible%2520to%2520reconstruct%2520what%2520any%2520given%2520step%2520actually%2520received%2520as%2520input.%2520We%2520present%2520NormCode%252C%2520a%2520semi%2520formal%2520language%2520that%2520makes%2520AI%2520workflows%2520auditable%2520by%2520construction.%2520Each%2520inference%2520step%2520operates%2520in%2520enforced%2520data%2520isolation%2520and%2520can%2520access%2520only%2520explicitly%2520passed%2520inputs.%2520This%2520eliminates%2520cross%2520step%2520contamination%2520and%2520ensures%2520that%2520every%2520intermediate%2520state%2520can%2520be%2520inspected.%2520A%2520strict%2520separation%2520between%2520semantic%2520operations%252C%2520meaning%2520probabilistic%2520language%2520model%2520reasoning%252C%2520and%2520syntactic%2520operations%252C%2520meaning%2520deterministic%2520data%2520flow%252C%2520allows%2520auditors%2520to%2520clearly%2520distinguish%2520inference%2520from%2520mechanical%2520restructuring.%2520The%2520multi%2520format%2520ecosystem%252C%2520consisting%2520of%2520NCDS%252C%2520NCD%252C%2520NCN%252C%2520and%2520NCDN%2520files%252C%2520allows%2520developers%252C%2520domain%2520experts%252C%2520and%2520auditors%2520to%2520inspect%2520the%2520same%2520plan%2520in%2520formats%2520suited%2520to%2520their%2520individual%2520needs.%2520A%2520four%2520phase%2520compilation%2520pipeline%2520transforms%2520natural%2520language%2520intent%2520into%2520executable%2520JSON%2520repositories.%2520A%2520visual%2520Canvas%2520application%2520provides%2520real%2520time%2520graph%2520visualization%2520and%2520breakpoint%2520debugging.%2520We%2520validate%2520the%2520approach%2520by%2520achieving%2520full%2520accuracy%2520on%2520base%2520X%2520addition%2520and%2520by%2520self%2520hosted%2520execution%2520of%2520the%2520NormCode%2520compiler%2520itself.%2520These%2520results%2520demonstrate%2520that%2520structured%2520intermediate%2520representations%2520can%2520bridge%2520human%2520intuition%2520and%2520machine%2520rigor%2520while%2520maintaining%2520full%2520transparency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10563v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NormCode%3A%20A%20Semi-Formal%20Language%20for%20Auditable%20AI%20Planning&entry.906535625=Xin%20Guan%20and%20Yunshan%20Li%20and%20Zekun%20Wu%20and%20Ruibo%20Zhang&entry.1292438233=As%20AI%20systems%20move%20into%20high%20stakes%20domains%20such%20as%20legal%20reasoning%2C%20medical%20diagnosis%2C%20and%20financial%20decision%20making%2C%20regulators%20and%20practitioners%20increasingly%20demand%20auditability.%20Auditability%20means%20the%20ability%20to%20trace%20exactly%20what%20each%20step%20in%20a%20multi%20step%20workflow%20saw%20and%20did.%20Current%20large%20language%20model%20based%20workflows%20are%20fundamentally%20opaque.%20Context%20pollution%2C%20defined%20as%20the%20accumulation%20of%20information%20across%20reasoning%20steps%2C%20causes%20models%20to%20hallucinate%20and%20lose%20track%20of%20constraints.%20At%20the%20same%20time%2C%20implicit%20data%20flow%20makes%20it%20impossible%20to%20reconstruct%20what%20any%20given%20step%20actually%20received%20as%20input.%20We%20present%20NormCode%2C%20a%20semi%20formal%20language%20that%20makes%20AI%20workflows%20auditable%20by%20construction.%20Each%20inference%20step%20operates%20in%20enforced%20data%20isolation%20and%20can%20access%20only%20explicitly%20passed%20inputs.%20This%20eliminates%20cross%20step%20contamination%20and%20ensures%20that%20every%20intermediate%20state%20can%20be%20inspected.%20A%20strict%20separation%20between%20semantic%20operations%2C%20meaning%20probabilistic%20language%20model%20reasoning%2C%20and%20syntactic%20operations%2C%20meaning%20deterministic%20data%20flow%2C%20allows%20auditors%20to%20clearly%20distinguish%20inference%20from%20mechanical%20restructuring.%20The%20multi%20format%20ecosystem%2C%20consisting%20of%20NCDS%2C%20NCD%2C%20NCN%2C%20and%20NCDN%20files%2C%20allows%20developers%2C%20domain%20experts%2C%20and%20auditors%20to%20inspect%20the%20same%20plan%20in%20formats%20suited%20to%20their%20individual%20needs.%20A%20four%20phase%20compilation%20pipeline%20transforms%20natural%20language%20intent%20into%20executable%20JSON%20repositories.%20A%20visual%20Canvas%20application%20provides%20real%20time%20graph%20visualization%20and%20breakpoint%20debugging.%20We%20validate%20the%20approach%20by%20achieving%20full%20accuracy%20on%20base%20X%20addition%20and%20by%20self%20hosted%20execution%20of%20the%20NormCode%20compiler%20itself.%20These%20results%20demonstrate%20that%20structured%20intermediate%20representations%20can%20bridge%20human%20intuition%20and%20machine%20rigor%20while%20maintaining%20full%20transparency.&entry.1838667208=http%3A//arxiv.org/abs/2512.10563v3&entry.124074799=Read"},
{"title": "QSLM: A Performance- and Memory-aware Quantization Framework with Tiered Search Strategy for Spike-driven Language Models", "author": "Rachmad Vidya Wicaksana Putra and Pasindu Wickramasinghe and Muhammad Shafique", "abstract": "Large Language Models (LLMs) have been emerging as prominent AI models for solving many natural language tasks due to their high performance (e.g., accuracy) and capabilities in generating high-quality responses to the given inputs. However, their large computational cost, huge memory footprints, and high processing power/energy make it challenging for their embedded deployments. Amid several tinyLLMs, recent works have proposed spike-driven language models (SLMs) for significantly reducing the processing power/energy of LLMs. However, their memory footprints still remain too large for low-cost and resource-constrained embedded devices. Manual quantization approach may effectively compress SLM memory footprints, but it requires a huge design time and compute power to find the quantization setting for each network, hence making this approach not-scalable for handling different networks, performance requirements, and memory budgets. To bridge this gap, we propose QSLM, a novel framework that performs automated quantization for compressing pre-trained SLMs, while meeting the performance and memory constraints. To achieve this, QSLM first identifies the hierarchy of the given network architecture and the sensitivity of network layers under quantization, then employs a tiered quantization strategy (e.g., global-, block-, and module-level quantization) while leveraging a multi-objective performance-and-memory trade-off function to select the final quantization setting. Experimental results indicate that our QSLM reduces memory footprint by up to 86.5%, reduces power consumption by up to 20%, maintains high performance across different tasks (i.e., by up to 84.4% accuracy of sentiment classification on the SST-2 dataset and perplexity score of 23.2 for text generation on the WikiText-2 dataset) close to the original non-quantized model while meeting the performance and memory constraints.", "link": "http://arxiv.org/abs/2601.00679v1", "date": "2026-01-02", "relevancy": 1.4028, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4823}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4641}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QSLM%3A%20A%20Performance-%20and%20Memory-aware%20Quantization%20Framework%20with%20Tiered%20Search%20Strategy%20for%20Spike-driven%20Language%20Models&body=Title%3A%20QSLM%3A%20A%20Performance-%20and%20Memory-aware%20Quantization%20Framework%20with%20Tiered%20Search%20Strategy%20for%20Spike-driven%20Language%20Models%0AAuthor%3A%20Rachmad%20Vidya%20Wicaksana%20Putra%20and%20Pasindu%20Wickramasinghe%20and%20Muhammad%20Shafique%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20emerging%20as%20prominent%20AI%20models%20for%20solving%20many%20natural%20language%20tasks%20due%20to%20their%20high%20performance%20%28e.g.%2C%20accuracy%29%20and%20capabilities%20in%20generating%20high-quality%20responses%20to%20the%20given%20inputs.%20However%2C%20their%20large%20computational%20cost%2C%20huge%20memory%20footprints%2C%20and%20high%20processing%20power/energy%20make%20it%20challenging%20for%20their%20embedded%20deployments.%20Amid%20several%20tinyLLMs%2C%20recent%20works%20have%20proposed%20spike-driven%20language%20models%20%28SLMs%29%20for%20significantly%20reducing%20the%20processing%20power/energy%20of%20LLMs.%20However%2C%20their%20memory%20footprints%20still%20remain%20too%20large%20for%20low-cost%20and%20resource-constrained%20embedded%20devices.%20Manual%20quantization%20approach%20may%20effectively%20compress%20SLM%20memory%20footprints%2C%20but%20it%20requires%20a%20huge%20design%20time%20and%20compute%20power%20to%20find%20the%20quantization%20setting%20for%20each%20network%2C%20hence%20making%20this%20approach%20not-scalable%20for%20handling%20different%20networks%2C%20performance%20requirements%2C%20and%20memory%20budgets.%20To%20bridge%20this%20gap%2C%20we%20propose%20QSLM%2C%20a%20novel%20framework%20that%20performs%20automated%20quantization%20for%20compressing%20pre-trained%20SLMs%2C%20while%20meeting%20the%20performance%20and%20memory%20constraints.%20To%20achieve%20this%2C%20QSLM%20first%20identifies%20the%20hierarchy%20of%20the%20given%20network%20architecture%20and%20the%20sensitivity%20of%20network%20layers%20under%20quantization%2C%20then%20employs%20a%20tiered%20quantization%20strategy%20%28e.g.%2C%20global-%2C%20block-%2C%20and%20module-level%20quantization%29%20while%20leveraging%20a%20multi-objective%20performance-and-memory%20trade-off%20function%20to%20select%20the%20final%20quantization%20setting.%20Experimental%20results%20indicate%20that%20our%20QSLM%20reduces%20memory%20footprint%20by%20up%20to%2086.5%25%2C%20reduces%20power%20consumption%20by%20up%20to%2020%25%2C%20maintains%20high%20performance%20across%20different%20tasks%20%28i.e.%2C%20by%20up%20to%2084.4%25%20accuracy%20of%20sentiment%20classification%20on%20the%20SST-2%20dataset%20and%20perplexity%20score%20of%2023.2%20for%20text%20generation%20on%20the%20WikiText-2%20dataset%29%20close%20to%20the%20original%20non-quantized%20model%20while%20meeting%20the%20performance%20and%20memory%20constraints.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQSLM%253A%2520A%2520Performance-%2520and%2520Memory-aware%2520Quantization%2520Framework%2520with%2520Tiered%2520Search%2520Strategy%2520for%2520Spike-driven%2520Language%2520Models%26entry.906535625%3DRachmad%2520Vidya%2520Wicaksana%2520Putra%2520and%2520Pasindu%2520Wickramasinghe%2520and%2520Muhammad%2520Shafique%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520been%2520emerging%2520as%2520prominent%2520AI%2520models%2520for%2520solving%2520many%2520natural%2520language%2520tasks%2520due%2520to%2520their%2520high%2520performance%2520%2528e.g.%252C%2520accuracy%2529%2520and%2520capabilities%2520in%2520generating%2520high-quality%2520responses%2520to%2520the%2520given%2520inputs.%2520However%252C%2520their%2520large%2520computational%2520cost%252C%2520huge%2520memory%2520footprints%252C%2520and%2520high%2520processing%2520power/energy%2520make%2520it%2520challenging%2520for%2520their%2520embedded%2520deployments.%2520Amid%2520several%2520tinyLLMs%252C%2520recent%2520works%2520have%2520proposed%2520spike-driven%2520language%2520models%2520%2528SLMs%2529%2520for%2520significantly%2520reducing%2520the%2520processing%2520power/energy%2520of%2520LLMs.%2520However%252C%2520their%2520memory%2520footprints%2520still%2520remain%2520too%2520large%2520for%2520low-cost%2520and%2520resource-constrained%2520embedded%2520devices.%2520Manual%2520quantization%2520approach%2520may%2520effectively%2520compress%2520SLM%2520memory%2520footprints%252C%2520but%2520it%2520requires%2520a%2520huge%2520design%2520time%2520and%2520compute%2520power%2520to%2520find%2520the%2520quantization%2520setting%2520for%2520each%2520network%252C%2520hence%2520making%2520this%2520approach%2520not-scalable%2520for%2520handling%2520different%2520networks%252C%2520performance%2520requirements%252C%2520and%2520memory%2520budgets.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520QSLM%252C%2520a%2520novel%2520framework%2520that%2520performs%2520automated%2520quantization%2520for%2520compressing%2520pre-trained%2520SLMs%252C%2520while%2520meeting%2520the%2520performance%2520and%2520memory%2520constraints.%2520To%2520achieve%2520this%252C%2520QSLM%2520first%2520identifies%2520the%2520hierarchy%2520of%2520the%2520given%2520network%2520architecture%2520and%2520the%2520sensitivity%2520of%2520network%2520layers%2520under%2520quantization%252C%2520then%2520employs%2520a%2520tiered%2520quantization%2520strategy%2520%2528e.g.%252C%2520global-%252C%2520block-%252C%2520and%2520module-level%2520quantization%2529%2520while%2520leveraging%2520a%2520multi-objective%2520performance-and-memory%2520trade-off%2520function%2520to%2520select%2520the%2520final%2520quantization%2520setting.%2520Experimental%2520results%2520indicate%2520that%2520our%2520QSLM%2520reduces%2520memory%2520footprint%2520by%2520up%2520to%252086.5%2525%252C%2520reduces%2520power%2520consumption%2520by%2520up%2520to%252020%2525%252C%2520maintains%2520high%2520performance%2520across%2520different%2520tasks%2520%2528i.e.%252C%2520by%2520up%2520to%252084.4%2525%2520accuracy%2520of%2520sentiment%2520classification%2520on%2520the%2520SST-2%2520dataset%2520and%2520perplexity%2520score%2520of%252023.2%2520for%2520text%2520generation%2520on%2520the%2520WikiText-2%2520dataset%2529%2520close%2520to%2520the%2520original%2520non-quantized%2520model%2520while%2520meeting%2520the%2520performance%2520and%2520memory%2520constraints.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QSLM%3A%20A%20Performance-%20and%20Memory-aware%20Quantization%20Framework%20with%20Tiered%20Search%20Strategy%20for%20Spike-driven%20Language%20Models&entry.906535625=Rachmad%20Vidya%20Wicaksana%20Putra%20and%20Pasindu%20Wickramasinghe%20and%20Muhammad%20Shafique&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20have%20been%20emerging%20as%20prominent%20AI%20models%20for%20solving%20many%20natural%20language%20tasks%20due%20to%20their%20high%20performance%20%28e.g.%2C%20accuracy%29%20and%20capabilities%20in%20generating%20high-quality%20responses%20to%20the%20given%20inputs.%20However%2C%20their%20large%20computational%20cost%2C%20huge%20memory%20footprints%2C%20and%20high%20processing%20power/energy%20make%20it%20challenging%20for%20their%20embedded%20deployments.%20Amid%20several%20tinyLLMs%2C%20recent%20works%20have%20proposed%20spike-driven%20language%20models%20%28SLMs%29%20for%20significantly%20reducing%20the%20processing%20power/energy%20of%20LLMs.%20However%2C%20their%20memory%20footprints%20still%20remain%20too%20large%20for%20low-cost%20and%20resource-constrained%20embedded%20devices.%20Manual%20quantization%20approach%20may%20effectively%20compress%20SLM%20memory%20footprints%2C%20but%20it%20requires%20a%20huge%20design%20time%20and%20compute%20power%20to%20find%20the%20quantization%20setting%20for%20each%20network%2C%20hence%20making%20this%20approach%20not-scalable%20for%20handling%20different%20networks%2C%20performance%20requirements%2C%20and%20memory%20budgets.%20To%20bridge%20this%20gap%2C%20we%20propose%20QSLM%2C%20a%20novel%20framework%20that%20performs%20automated%20quantization%20for%20compressing%20pre-trained%20SLMs%2C%20while%20meeting%20the%20performance%20and%20memory%20constraints.%20To%20achieve%20this%2C%20QSLM%20first%20identifies%20the%20hierarchy%20of%20the%20given%20network%20architecture%20and%20the%20sensitivity%20of%20network%20layers%20under%20quantization%2C%20then%20employs%20a%20tiered%20quantization%20strategy%20%28e.g.%2C%20global-%2C%20block-%2C%20and%20module-level%20quantization%29%20while%20leveraging%20a%20multi-objective%20performance-and-memory%20trade-off%20function%20to%20select%20the%20final%20quantization%20setting.%20Experimental%20results%20indicate%20that%20our%20QSLM%20reduces%20memory%20footprint%20by%20up%20to%2086.5%25%2C%20reduces%20power%20consumption%20by%20up%20to%2020%25%2C%20maintains%20high%20performance%20across%20different%20tasks%20%28i.e.%2C%20by%20up%20to%2084.4%25%20accuracy%20of%20sentiment%20classification%20on%20the%20SST-2%20dataset%20and%20perplexity%20score%20of%2023.2%20for%20text%20generation%20on%20the%20WikiText-2%20dataset%29%20close%20to%20the%20original%20non-quantized%20model%20while%20meeting%20the%20performance%20and%20memory%20constraints.&entry.1838667208=http%3A//arxiv.org/abs/2601.00679v1&entry.124074799=Read"},
{"title": "LLM Agents for Combinatorial Efficient Frontiers: Investment Portfolio Optimization", "author": "Simon Paquette-Greenbaum and Jiangbo Yu", "abstract": "Investment portfolio optimization is a task conducted in all major financial institutions. The Cardinality Constrained Mean-Variance Portfolio Optimization (CCPO) problem formulation is ubiquitous for portfolio optimization. The challenge of this type of portfolio optimization, a mixed-integer quadratic programming (MIQP) problem, arises from the intractability of solutions from exact solvers, where heuristic algorithms are used to find approximate portfolio solutions. CCPO entails many laborious and complex workflows and also requires extensive effort pertaining to heuristic algorithm development, where the combination of pooled heuristic solutions results in improved efficient frontiers. Hence, common approaches are to develop many heuristic algorithms. Agentic frameworks emerge as a promising candidate for many problems within combinatorial optimization, as they have been shown to be equally efficient with regard to automating large workflows and have been shown to be excellent in terms of algorithm development, sometimes surpassing human-level performance. This study implements a novel agentic framework for the CCPO and explores several concrete architectures. In benchmark problems, the implemented agentic framework matches state-of-the-art algorithms. Furthermore, complex workflows and algorithm development efforts are alleviated, while in the worst case, lower but acceptable error is reported.", "link": "http://arxiv.org/abs/2601.00770v1", "date": "2026-01-02", "relevancy": 1.3779, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4851}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4522}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM%20Agents%20for%20Combinatorial%20Efficient%20Frontiers%3A%20Investment%20Portfolio%20Optimization&body=Title%3A%20LLM%20Agents%20for%20Combinatorial%20Efficient%20Frontiers%3A%20Investment%20Portfolio%20Optimization%0AAuthor%3A%20Simon%20Paquette-Greenbaum%20and%20Jiangbo%20Yu%0AAbstract%3A%20Investment%20portfolio%20optimization%20is%20a%20task%20conducted%20in%20all%20major%20financial%20institutions.%20The%20Cardinality%20Constrained%20Mean-Variance%20Portfolio%20Optimization%20%28CCPO%29%20problem%20formulation%20is%20ubiquitous%20for%20portfolio%20optimization.%20The%20challenge%20of%20this%20type%20of%20portfolio%20optimization%2C%20a%20mixed-integer%20quadratic%20programming%20%28MIQP%29%20problem%2C%20arises%20from%20the%20intractability%20of%20solutions%20from%20exact%20solvers%2C%20where%20heuristic%20algorithms%20are%20used%20to%20find%20approximate%20portfolio%20solutions.%20CCPO%20entails%20many%20laborious%20and%20complex%20workflows%20and%20also%20requires%20extensive%20effort%20pertaining%20to%20heuristic%20algorithm%20development%2C%20where%20the%20combination%20of%20pooled%20heuristic%20solutions%20results%20in%20improved%20efficient%20frontiers.%20Hence%2C%20common%20approaches%20are%20to%20develop%20many%20heuristic%20algorithms.%20Agentic%20frameworks%20emerge%20as%20a%20promising%20candidate%20for%20many%20problems%20within%20combinatorial%20optimization%2C%20as%20they%20have%20been%20shown%20to%20be%20equally%20efficient%20with%20regard%20to%20automating%20large%20workflows%20and%20have%20been%20shown%20to%20be%20excellent%20in%20terms%20of%20algorithm%20development%2C%20sometimes%20surpassing%20human-level%20performance.%20This%20study%20implements%20a%20novel%20agentic%20framework%20for%20the%20CCPO%20and%20explores%20several%20concrete%20architectures.%20In%20benchmark%20problems%2C%20the%20implemented%20agentic%20framework%20matches%20state-of-the-art%20algorithms.%20Furthermore%2C%20complex%20workflows%20and%20algorithm%20development%20efforts%20are%20alleviated%2C%20while%20in%20the%20worst%20case%2C%20lower%20but%20acceptable%20error%20is%20reported.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00770v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM%2520Agents%2520for%2520Combinatorial%2520Efficient%2520Frontiers%253A%2520Investment%2520Portfolio%2520Optimization%26entry.906535625%3DSimon%2520Paquette-Greenbaum%2520and%2520Jiangbo%2520Yu%26entry.1292438233%3DInvestment%2520portfolio%2520optimization%2520is%2520a%2520task%2520conducted%2520in%2520all%2520major%2520financial%2520institutions.%2520The%2520Cardinality%2520Constrained%2520Mean-Variance%2520Portfolio%2520Optimization%2520%2528CCPO%2529%2520problem%2520formulation%2520is%2520ubiquitous%2520for%2520portfolio%2520optimization.%2520The%2520challenge%2520of%2520this%2520type%2520of%2520portfolio%2520optimization%252C%2520a%2520mixed-integer%2520quadratic%2520programming%2520%2528MIQP%2529%2520problem%252C%2520arises%2520from%2520the%2520intractability%2520of%2520solutions%2520from%2520exact%2520solvers%252C%2520where%2520heuristic%2520algorithms%2520are%2520used%2520to%2520find%2520approximate%2520portfolio%2520solutions.%2520CCPO%2520entails%2520many%2520laborious%2520and%2520complex%2520workflows%2520and%2520also%2520requires%2520extensive%2520effort%2520pertaining%2520to%2520heuristic%2520algorithm%2520development%252C%2520where%2520the%2520combination%2520of%2520pooled%2520heuristic%2520solutions%2520results%2520in%2520improved%2520efficient%2520frontiers.%2520Hence%252C%2520common%2520approaches%2520are%2520to%2520develop%2520many%2520heuristic%2520algorithms.%2520Agentic%2520frameworks%2520emerge%2520as%2520a%2520promising%2520candidate%2520for%2520many%2520problems%2520within%2520combinatorial%2520optimization%252C%2520as%2520they%2520have%2520been%2520shown%2520to%2520be%2520equally%2520efficient%2520with%2520regard%2520to%2520automating%2520large%2520workflows%2520and%2520have%2520been%2520shown%2520to%2520be%2520excellent%2520in%2520terms%2520of%2520algorithm%2520development%252C%2520sometimes%2520surpassing%2520human-level%2520performance.%2520This%2520study%2520implements%2520a%2520novel%2520agentic%2520framework%2520for%2520the%2520CCPO%2520and%2520explores%2520several%2520concrete%2520architectures.%2520In%2520benchmark%2520problems%252C%2520the%2520implemented%2520agentic%2520framework%2520matches%2520state-of-the-art%2520algorithms.%2520Furthermore%252C%2520complex%2520workflows%2520and%2520algorithm%2520development%2520efforts%2520are%2520alleviated%252C%2520while%2520in%2520the%2520worst%2520case%252C%2520lower%2520but%2520acceptable%2520error%2520is%2520reported.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00770v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM%20Agents%20for%20Combinatorial%20Efficient%20Frontiers%3A%20Investment%20Portfolio%20Optimization&entry.906535625=Simon%20Paquette-Greenbaum%20and%20Jiangbo%20Yu&entry.1292438233=Investment%20portfolio%20optimization%20is%20a%20task%20conducted%20in%20all%20major%20financial%20institutions.%20The%20Cardinality%20Constrained%20Mean-Variance%20Portfolio%20Optimization%20%28CCPO%29%20problem%20formulation%20is%20ubiquitous%20for%20portfolio%20optimization.%20The%20challenge%20of%20this%20type%20of%20portfolio%20optimization%2C%20a%20mixed-integer%20quadratic%20programming%20%28MIQP%29%20problem%2C%20arises%20from%20the%20intractability%20of%20solutions%20from%20exact%20solvers%2C%20where%20heuristic%20algorithms%20are%20used%20to%20find%20approximate%20portfolio%20solutions.%20CCPO%20entails%20many%20laborious%20and%20complex%20workflows%20and%20also%20requires%20extensive%20effort%20pertaining%20to%20heuristic%20algorithm%20development%2C%20where%20the%20combination%20of%20pooled%20heuristic%20solutions%20results%20in%20improved%20efficient%20frontiers.%20Hence%2C%20common%20approaches%20are%20to%20develop%20many%20heuristic%20algorithms.%20Agentic%20frameworks%20emerge%20as%20a%20promising%20candidate%20for%20many%20problems%20within%20combinatorial%20optimization%2C%20as%20they%20have%20been%20shown%20to%20be%20equally%20efficient%20with%20regard%20to%20automating%20large%20workflows%20and%20have%20been%20shown%20to%20be%20excellent%20in%20terms%20of%20algorithm%20development%2C%20sometimes%20surpassing%20human-level%20performance.%20This%20study%20implements%20a%20novel%20agentic%20framework%20for%20the%20CCPO%20and%20explores%20several%20concrete%20architectures.%20In%20benchmark%20problems%2C%20the%20implemented%20agentic%20framework%20matches%20state-of-the-art%20algorithms.%20Furthermore%2C%20complex%20workflows%20and%20algorithm%20development%20efforts%20are%20alleviated%2C%20while%20in%20the%20worst%20case%2C%20lower%20but%20acceptable%20error%20is%20reported.&entry.1838667208=http%3A//arxiv.org/abs/2601.00770v1&entry.124074799=Read"},
{"title": "Navigating the safe harbor paradox in human-machine systems", "author": "Riccardo Zanardelli", "abstract": "When deploying artificial skills, decision-makers often assume that layering human oversight is a safe harbor that mitigates the risks of full automation in high-complexity tasks. This paper formally challenges the economic validity of this widespread assumption, arguing that the true bottom-line economic utility of a human-machine skill policy is highly contingent on situational and design factors. To investigate this gap, we develop an in-silico exploratory framework for policy analysis based on Monte Carlo simulations to quantify the economic impact of skill policies in the execution of tasks presenting varying levels of complexity across diverse setups. Our results show that in complex scenarios, a human-machine strategy can yield the highest economic utility, but only if genuine augmentation is achieved. In contrast, when failing to realize this synergy, the human-machine approach can perform worse than either the machine-exclusive or the human-exclusive policy, actively destroying value under the pressure of costs that are not sufficiently compensated by performance gains. This finding points to a key implication for decision-makers: when the context is complex and critical, simply allocating human and machine skills to a task may be insufficient, and far from being a silver-bullet solution or a low-risk compromise. Rather, it is a critical opportunity to boost competitiveness that demands a strong organizational commitment to enabling augmentation. Also, our findings show that improving the cost-effectiveness of machine skills over time, while useful, does not replace the fundamental need to focus on achieving augmentation when surprise is the norm, even when machines become more effective than humans in handling uncertainty.", "link": "http://arxiv.org/abs/2509.14057v7", "date": "2026-01-02", "relevancy": 1.345, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4622}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4454}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Navigating%20the%20safe%20harbor%20paradox%20in%20human-machine%20systems&body=Title%3A%20Navigating%20the%20safe%20harbor%20paradox%20in%20human-machine%20systems%0AAuthor%3A%20Riccardo%20Zanardelli%0AAbstract%3A%20When%20deploying%20artificial%20skills%2C%20decision-makers%20often%20assume%20that%20layering%20human%20oversight%20is%20a%20safe%20harbor%20that%20mitigates%20the%20risks%20of%20full%20automation%20in%20high-complexity%20tasks.%20This%20paper%20formally%20challenges%20the%20economic%20validity%20of%20this%20widespread%20assumption%2C%20arguing%20that%20the%20true%20bottom-line%20economic%20utility%20of%20a%20human-machine%20skill%20policy%20is%20highly%20contingent%20on%20situational%20and%20design%20factors.%20To%20investigate%20this%20gap%2C%20we%20develop%20an%20in-silico%20exploratory%20framework%20for%20policy%20analysis%20based%20on%20Monte%20Carlo%20simulations%20to%20quantify%20the%20economic%20impact%20of%20skill%20policies%20in%20the%20execution%20of%20tasks%20presenting%20varying%20levels%20of%20complexity%20across%20diverse%20setups.%20Our%20results%20show%20that%20in%20complex%20scenarios%2C%20a%20human-machine%20strategy%20can%20yield%20the%20highest%20economic%20utility%2C%20but%20only%20if%20genuine%20augmentation%20is%20achieved.%20In%20contrast%2C%20when%20failing%20to%20realize%20this%20synergy%2C%20the%20human-machine%20approach%20can%20perform%20worse%20than%20either%20the%20machine-exclusive%20or%20the%20human-exclusive%20policy%2C%20actively%20destroying%20value%20under%20the%20pressure%20of%20costs%20that%20are%20not%20sufficiently%20compensated%20by%20performance%20gains.%20This%20finding%20points%20to%20a%20key%20implication%20for%20decision-makers%3A%20when%20the%20context%20is%20complex%20and%20critical%2C%20simply%20allocating%20human%20and%20machine%20skills%20to%20a%20task%20may%20be%20insufficient%2C%20and%20far%20from%20being%20a%20silver-bullet%20solution%20or%20a%20low-risk%20compromise.%20Rather%2C%20it%20is%20a%20critical%20opportunity%20to%20boost%20competitiveness%20that%20demands%20a%20strong%20organizational%20commitment%20to%20enabling%20augmentation.%20Also%2C%20our%20findings%20show%20that%20improving%20the%20cost-effectiveness%20of%20machine%20skills%20over%20time%2C%20while%20useful%2C%20does%20not%20replace%20the%20fundamental%20need%20to%20focus%20on%20achieving%20augmentation%20when%20surprise%20is%20the%20norm%2C%20even%20when%20machines%20become%20more%20effective%20than%20humans%20in%20handling%20uncertainty.%0ALink%3A%20http%3A//arxiv.org/abs/2509.14057v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNavigating%2520the%2520safe%2520harbor%2520paradox%2520in%2520human-machine%2520systems%26entry.906535625%3DRiccardo%2520Zanardelli%26entry.1292438233%3DWhen%2520deploying%2520artificial%2520skills%252C%2520decision-makers%2520often%2520assume%2520that%2520layering%2520human%2520oversight%2520is%2520a%2520safe%2520harbor%2520that%2520mitigates%2520the%2520risks%2520of%2520full%2520automation%2520in%2520high-complexity%2520tasks.%2520This%2520paper%2520formally%2520challenges%2520the%2520economic%2520validity%2520of%2520this%2520widespread%2520assumption%252C%2520arguing%2520that%2520the%2520true%2520bottom-line%2520economic%2520utility%2520of%2520a%2520human-machine%2520skill%2520policy%2520is%2520highly%2520contingent%2520on%2520situational%2520and%2520design%2520factors.%2520To%2520investigate%2520this%2520gap%252C%2520we%2520develop%2520an%2520in-silico%2520exploratory%2520framework%2520for%2520policy%2520analysis%2520based%2520on%2520Monte%2520Carlo%2520simulations%2520to%2520quantify%2520the%2520economic%2520impact%2520of%2520skill%2520policies%2520in%2520the%2520execution%2520of%2520tasks%2520presenting%2520varying%2520levels%2520of%2520complexity%2520across%2520diverse%2520setups.%2520Our%2520results%2520show%2520that%2520in%2520complex%2520scenarios%252C%2520a%2520human-machine%2520strategy%2520can%2520yield%2520the%2520highest%2520economic%2520utility%252C%2520but%2520only%2520if%2520genuine%2520augmentation%2520is%2520achieved.%2520In%2520contrast%252C%2520when%2520failing%2520to%2520realize%2520this%2520synergy%252C%2520the%2520human-machine%2520approach%2520can%2520perform%2520worse%2520than%2520either%2520the%2520machine-exclusive%2520or%2520the%2520human-exclusive%2520policy%252C%2520actively%2520destroying%2520value%2520under%2520the%2520pressure%2520of%2520costs%2520that%2520are%2520not%2520sufficiently%2520compensated%2520by%2520performance%2520gains.%2520This%2520finding%2520points%2520to%2520a%2520key%2520implication%2520for%2520decision-makers%253A%2520when%2520the%2520context%2520is%2520complex%2520and%2520critical%252C%2520simply%2520allocating%2520human%2520and%2520machine%2520skills%2520to%2520a%2520task%2520may%2520be%2520insufficient%252C%2520and%2520far%2520from%2520being%2520a%2520silver-bullet%2520solution%2520or%2520a%2520low-risk%2520compromise.%2520Rather%252C%2520it%2520is%2520a%2520critical%2520opportunity%2520to%2520boost%2520competitiveness%2520that%2520demands%2520a%2520strong%2520organizational%2520commitment%2520to%2520enabling%2520augmentation.%2520Also%252C%2520our%2520findings%2520show%2520that%2520improving%2520the%2520cost-effectiveness%2520of%2520machine%2520skills%2520over%2520time%252C%2520while%2520useful%252C%2520does%2520not%2520replace%2520the%2520fundamental%2520need%2520to%2520focus%2520on%2520achieving%2520augmentation%2520when%2520surprise%2520is%2520the%2520norm%252C%2520even%2520when%2520machines%2520become%2520more%2520effective%2520than%2520humans%2520in%2520handling%2520uncertainty.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14057v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Navigating%20the%20safe%20harbor%20paradox%20in%20human-machine%20systems&entry.906535625=Riccardo%20Zanardelli&entry.1292438233=When%20deploying%20artificial%20skills%2C%20decision-makers%20often%20assume%20that%20layering%20human%20oversight%20is%20a%20safe%20harbor%20that%20mitigates%20the%20risks%20of%20full%20automation%20in%20high-complexity%20tasks.%20This%20paper%20formally%20challenges%20the%20economic%20validity%20of%20this%20widespread%20assumption%2C%20arguing%20that%20the%20true%20bottom-line%20economic%20utility%20of%20a%20human-machine%20skill%20policy%20is%20highly%20contingent%20on%20situational%20and%20design%20factors.%20To%20investigate%20this%20gap%2C%20we%20develop%20an%20in-silico%20exploratory%20framework%20for%20policy%20analysis%20based%20on%20Monte%20Carlo%20simulations%20to%20quantify%20the%20economic%20impact%20of%20skill%20policies%20in%20the%20execution%20of%20tasks%20presenting%20varying%20levels%20of%20complexity%20across%20diverse%20setups.%20Our%20results%20show%20that%20in%20complex%20scenarios%2C%20a%20human-machine%20strategy%20can%20yield%20the%20highest%20economic%20utility%2C%20but%20only%20if%20genuine%20augmentation%20is%20achieved.%20In%20contrast%2C%20when%20failing%20to%20realize%20this%20synergy%2C%20the%20human-machine%20approach%20can%20perform%20worse%20than%20either%20the%20machine-exclusive%20or%20the%20human-exclusive%20policy%2C%20actively%20destroying%20value%20under%20the%20pressure%20of%20costs%20that%20are%20not%20sufficiently%20compensated%20by%20performance%20gains.%20This%20finding%20points%20to%20a%20key%20implication%20for%20decision-makers%3A%20when%20the%20context%20is%20complex%20and%20critical%2C%20simply%20allocating%20human%20and%20machine%20skills%20to%20a%20task%20may%20be%20insufficient%2C%20and%20far%20from%20being%20a%20silver-bullet%20solution%20or%20a%20low-risk%20compromise.%20Rather%2C%20it%20is%20a%20critical%20opportunity%20to%20boost%20competitiveness%20that%20demands%20a%20strong%20organizational%20commitment%20to%20enabling%20augmentation.%20Also%2C%20our%20findings%20show%20that%20improving%20the%20cost-effectiveness%20of%20machine%20skills%20over%20time%2C%20while%20useful%2C%20does%20not%20replace%20the%20fundamental%20need%20to%20focus%20on%20achieving%20augmentation%20when%20surprise%20is%20the%20norm%2C%20even%20when%20machines%20become%20more%20effective%20than%20humans%20in%20handling%20uncertainty.&entry.1838667208=http%3A//arxiv.org/abs/2509.14057v7&entry.124074799=Read"},
{"title": "Episodic Contextual Bandits with Knapsacks under Conversion Models", "author": "Wang Chi Cheung and Zitian Li", "abstract": "We study an online setting, where a decision maker (DM) interacts with contextual bandit-with-knapsack (BwK) instances in repeated episodes. These episodes start with different resource amounts, and the contexts' probability distributions are non-stationary in an episode. All episodes share the same latent conversion model, which governs the random outcome contingent upon a request's context and an allocation decision. Our model captures applications such as dynamic pricing on perishable resources with episodic replenishment, and first price auctions in repeated episodes with different starting budgets. We design an online algorithm that achieves a regret sub-linear in $T$, the number of episodes, assuming access to a \\emph{confidence bound oracle} that achieves an $o(T)$-regret. Such an oracle is readily available from existing contextual bandit literature. We overcome the technical challenge with arbitrarily many possible contexts, which leads to a reinforcement learning problem with an unbounded state space. Our framework provides improved regret bounds in certain settings when the DM is provided with unlabeled feature data, which is novel to the contextual BwK literature.", "link": "http://arxiv.org/abs/2507.06859v2", "date": "2026-01-02", "relevancy": 1.2802, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4471}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4212}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Episodic%20Contextual%20Bandits%20with%20Knapsacks%20under%20Conversion%20Models&body=Title%3A%20Episodic%20Contextual%20Bandits%20with%20Knapsacks%20under%20Conversion%20Models%0AAuthor%3A%20Wang%20Chi%20Cheung%20and%20Zitian%20Li%0AAbstract%3A%20We%20study%20an%20online%20setting%2C%20where%20a%20decision%20maker%20%28DM%29%20interacts%20with%20contextual%20bandit-with-knapsack%20%28BwK%29%20instances%20in%20repeated%20episodes.%20These%20episodes%20start%20with%20different%20resource%20amounts%2C%20and%20the%20contexts%27%20probability%20distributions%20are%20non-stationary%20in%20an%20episode.%20All%20episodes%20share%20the%20same%20latent%20conversion%20model%2C%20which%20governs%20the%20random%20outcome%20contingent%20upon%20a%20request%27s%20context%20and%20an%20allocation%20decision.%20Our%20model%20captures%20applications%20such%20as%20dynamic%20pricing%20on%20perishable%20resources%20with%20episodic%20replenishment%2C%20and%20first%20price%20auctions%20in%20repeated%20episodes%20with%20different%20starting%20budgets.%20We%20design%20an%20online%20algorithm%20that%20achieves%20a%20regret%20sub-linear%20in%20%24T%24%2C%20the%20number%20of%20episodes%2C%20assuming%20access%20to%20a%20%5Cemph%7Bconfidence%20bound%20oracle%7D%20that%20achieves%20an%20%24o%28T%29%24-regret.%20Such%20an%20oracle%20is%20readily%20available%20from%20existing%20contextual%20bandit%20literature.%20We%20overcome%20the%20technical%20challenge%20with%20arbitrarily%20many%20possible%20contexts%2C%20which%20leads%20to%20a%20reinforcement%20learning%20problem%20with%20an%20unbounded%20state%20space.%20Our%20framework%20provides%20improved%20regret%20bounds%20in%20certain%20settings%20when%20the%20DM%20is%20provided%20with%20unlabeled%20feature%20data%2C%20which%20is%20novel%20to%20the%20contextual%20BwK%20literature.%0ALink%3A%20http%3A//arxiv.org/abs/2507.06859v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEpisodic%2520Contextual%2520Bandits%2520with%2520Knapsacks%2520under%2520Conversion%2520Models%26entry.906535625%3DWang%2520Chi%2520Cheung%2520and%2520Zitian%2520Li%26entry.1292438233%3DWe%2520study%2520an%2520online%2520setting%252C%2520where%2520a%2520decision%2520maker%2520%2528DM%2529%2520interacts%2520with%2520contextual%2520bandit-with-knapsack%2520%2528BwK%2529%2520instances%2520in%2520repeated%2520episodes.%2520These%2520episodes%2520start%2520with%2520different%2520resource%2520amounts%252C%2520and%2520the%2520contexts%2527%2520probability%2520distributions%2520are%2520non-stationary%2520in%2520an%2520episode.%2520All%2520episodes%2520share%2520the%2520same%2520latent%2520conversion%2520model%252C%2520which%2520governs%2520the%2520random%2520outcome%2520contingent%2520upon%2520a%2520request%2527s%2520context%2520and%2520an%2520allocation%2520decision.%2520Our%2520model%2520captures%2520applications%2520such%2520as%2520dynamic%2520pricing%2520on%2520perishable%2520resources%2520with%2520episodic%2520replenishment%252C%2520and%2520first%2520price%2520auctions%2520in%2520repeated%2520episodes%2520with%2520different%2520starting%2520budgets.%2520We%2520design%2520an%2520online%2520algorithm%2520that%2520achieves%2520a%2520regret%2520sub-linear%2520in%2520%2524T%2524%252C%2520the%2520number%2520of%2520episodes%252C%2520assuming%2520access%2520to%2520a%2520%255Cemph%257Bconfidence%2520bound%2520oracle%257D%2520that%2520achieves%2520an%2520%2524o%2528T%2529%2524-regret.%2520Such%2520an%2520oracle%2520is%2520readily%2520available%2520from%2520existing%2520contextual%2520bandit%2520literature.%2520We%2520overcome%2520the%2520technical%2520challenge%2520with%2520arbitrarily%2520many%2520possible%2520contexts%252C%2520which%2520leads%2520to%2520a%2520reinforcement%2520learning%2520problem%2520with%2520an%2520unbounded%2520state%2520space.%2520Our%2520framework%2520provides%2520improved%2520regret%2520bounds%2520in%2520certain%2520settings%2520when%2520the%2520DM%2520is%2520provided%2520with%2520unlabeled%2520feature%2520data%252C%2520which%2520is%2520novel%2520to%2520the%2520contextual%2520BwK%2520literature.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06859v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Episodic%20Contextual%20Bandits%20with%20Knapsacks%20under%20Conversion%20Models&entry.906535625=Wang%20Chi%20Cheung%20and%20Zitian%20Li&entry.1292438233=We%20study%20an%20online%20setting%2C%20where%20a%20decision%20maker%20%28DM%29%20interacts%20with%20contextual%20bandit-with-knapsack%20%28BwK%29%20instances%20in%20repeated%20episodes.%20These%20episodes%20start%20with%20different%20resource%20amounts%2C%20and%20the%20contexts%27%20probability%20distributions%20are%20non-stationary%20in%20an%20episode.%20All%20episodes%20share%20the%20same%20latent%20conversion%20model%2C%20which%20governs%20the%20random%20outcome%20contingent%20upon%20a%20request%27s%20context%20and%20an%20allocation%20decision.%20Our%20model%20captures%20applications%20such%20as%20dynamic%20pricing%20on%20perishable%20resources%20with%20episodic%20replenishment%2C%20and%20first%20price%20auctions%20in%20repeated%20episodes%20with%20different%20starting%20budgets.%20We%20design%20an%20online%20algorithm%20that%20achieves%20a%20regret%20sub-linear%20in%20%24T%24%2C%20the%20number%20of%20episodes%2C%20assuming%20access%20to%20a%20%5Cemph%7Bconfidence%20bound%20oracle%7D%20that%20achieves%20an%20%24o%28T%29%24-regret.%20Such%20an%20oracle%20is%20readily%20available%20from%20existing%20contextual%20bandit%20literature.%20We%20overcome%20the%20technical%20challenge%20with%20arbitrarily%20many%20possible%20contexts%2C%20which%20leads%20to%20a%20reinforcement%20learning%20problem%20with%20an%20unbounded%20state%20space.%20Our%20framework%20provides%20improved%20regret%20bounds%20in%20certain%20settings%20when%20the%20DM%20is%20provided%20with%20unlabeled%20feature%20data%2C%20which%20is%20novel%20to%20the%20contextual%20BwK%20literature.&entry.1838667208=http%3A//arxiv.org/abs/2507.06859v2&entry.124074799=Read"},
{"title": "QUITE: A Query Rewrite System Beyond Rules with LLM Agents", "author": "Yuyang Song and Hanxu Yan and Jiale Lao and Yibo Wang and Yufei Li and Yuanchun Zhou and Jianguo Wang and Mingjie Tang", "abstract": "Query rewrite transforms SQL queries into semantically equivalent forms that run more efficiently. Existing approaches mainly rely on predefined rewrite rules, but they handle a limited subset of queries and can cause performance regressions. This limitation stems from three challenges of rule-based query rewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite rules do not generalize to new query patterns, and (3) some rewrite techniques cannot be expressed as fixed rules. Motivated by the fact that human experts exhibit significantly better rewrite ability but suffer from scalability, and Large Language Models (LLMs) have demonstrated nearly human-level semantic and reasoning abilities, we propose a new approach of using LLMs to rewrite SQL queries beyond rules. Due to the hallucination problems in LLMs, directly applying LLMs often leads to nonequivalent and suboptimal queries. To address this issue, we propose QUITE (query rewrite), a training-free and feedback-aware system based on LLM agents that rewrites SQL queries into semantically equivalent forms with significantly better performance, covering a broader range of query patterns and rewrite strategies compared to rule-based methods. Firstly, we design a multi-agent framework controlled by a finite state machine (FSM) to equip LLMs with the ability to use external tools and enhance the rewrite process with real-time database feedback. Secondly, we develop a rewrite middleware to enhance the ability of LLMs to generate optimized query equivalents. Finally, we employ a novel hint injection technique to improve execution plans for rewritten queries. Extensive experiments show that QUITE reduces query execution time by up to 35.8% over state-of-the-art approaches and produces 24.1% more rewrites than prior methods, covering query cases that earlier systems did not handle.", "link": "http://arxiv.org/abs/2506.07675v3", "date": "2026-01-02", "relevancy": 1.2533, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4371}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4127}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QUITE%3A%20A%20Query%20Rewrite%20System%20Beyond%20Rules%20with%20LLM%20Agents&body=Title%3A%20QUITE%3A%20A%20Query%20Rewrite%20System%20Beyond%20Rules%20with%20LLM%20Agents%0AAuthor%3A%20Yuyang%20Song%20and%20Hanxu%20Yan%20and%20Jiale%20Lao%20and%20Yibo%20Wang%20and%20Yufei%20Li%20and%20Yuanchun%20Zhou%20and%20Jianguo%20Wang%20and%20Mingjie%20Tang%0AAbstract%3A%20Query%20rewrite%20transforms%20SQL%20queries%20into%20semantically%20equivalent%20forms%20that%20run%20more%20efficiently.%20Existing%20approaches%20mainly%20rely%20on%20predefined%20rewrite%20rules%2C%20but%20they%20handle%20a%20limited%20subset%20of%20queries%20and%20can%20cause%20performance%20regressions.%20This%20limitation%20stems%20from%20three%20challenges%20of%20rule-based%20query%20rewrite%3A%20%281%29%20it%20is%20hard%20to%20discover%20and%20verify%20new%20rules%2C%20%282%29%20fixed%20rewrite%20rules%20do%20not%20generalize%20to%20new%20query%20patterns%2C%20and%20%283%29%20some%20rewrite%20techniques%20cannot%20be%20expressed%20as%20fixed%20rules.%20Motivated%20by%20the%20fact%20that%20human%20experts%20exhibit%20significantly%20better%20rewrite%20ability%20but%20suffer%20from%20scalability%2C%20and%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20nearly%20human-level%20semantic%20and%20reasoning%20abilities%2C%20we%20propose%20a%20new%20approach%20of%20using%20LLMs%20to%20rewrite%20SQL%20queries%20beyond%20rules.%20Due%20to%20the%20hallucination%20problems%20in%20LLMs%2C%20directly%20applying%20LLMs%20often%20leads%20to%20nonequivalent%20and%20suboptimal%20queries.%20To%20address%20this%20issue%2C%20we%20propose%20QUITE%20%28query%20rewrite%29%2C%20a%20training-free%20and%20feedback-aware%20system%20based%20on%20LLM%20agents%20that%20rewrites%20SQL%20queries%20into%20semantically%20equivalent%20forms%20with%20significantly%20better%20performance%2C%20covering%20a%20broader%20range%20of%20query%20patterns%20and%20rewrite%20strategies%20compared%20to%20rule-based%20methods.%20Firstly%2C%20we%20design%20a%20multi-agent%20framework%20controlled%20by%20a%20finite%20state%20machine%20%28FSM%29%20to%20equip%20LLMs%20with%20the%20ability%20to%20use%20external%20tools%20and%20enhance%20the%20rewrite%20process%20with%20real-time%20database%20feedback.%20Secondly%2C%20we%20develop%20a%20rewrite%20middleware%20to%20enhance%20the%20ability%20of%20LLMs%20to%20generate%20optimized%20query%20equivalents.%20Finally%2C%20we%20employ%20a%20novel%20hint%20injection%20technique%20to%20improve%20execution%20plans%20for%20rewritten%20queries.%20Extensive%20experiments%20show%20that%20QUITE%20reduces%20query%20execution%20time%20by%20up%20to%2035.8%25%20over%20state-of-the-art%20approaches%20and%20produces%2024.1%25%20more%20rewrites%20than%20prior%20methods%2C%20covering%20query%20cases%20that%20earlier%20systems%20did%20not%20handle.%0ALink%3A%20http%3A//arxiv.org/abs/2506.07675v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQUITE%253A%2520A%2520Query%2520Rewrite%2520System%2520Beyond%2520Rules%2520with%2520LLM%2520Agents%26entry.906535625%3DYuyang%2520Song%2520and%2520Hanxu%2520Yan%2520and%2520Jiale%2520Lao%2520and%2520Yibo%2520Wang%2520and%2520Yufei%2520Li%2520and%2520Yuanchun%2520Zhou%2520and%2520Jianguo%2520Wang%2520and%2520Mingjie%2520Tang%26entry.1292438233%3DQuery%2520rewrite%2520transforms%2520SQL%2520queries%2520into%2520semantically%2520equivalent%2520forms%2520that%2520run%2520more%2520efficiently.%2520Existing%2520approaches%2520mainly%2520rely%2520on%2520predefined%2520rewrite%2520rules%252C%2520but%2520they%2520handle%2520a%2520limited%2520subset%2520of%2520queries%2520and%2520can%2520cause%2520performance%2520regressions.%2520This%2520limitation%2520stems%2520from%2520three%2520challenges%2520of%2520rule-based%2520query%2520rewrite%253A%2520%25281%2529%2520it%2520is%2520hard%2520to%2520discover%2520and%2520verify%2520new%2520rules%252C%2520%25282%2529%2520fixed%2520rewrite%2520rules%2520do%2520not%2520generalize%2520to%2520new%2520query%2520patterns%252C%2520and%2520%25283%2529%2520some%2520rewrite%2520techniques%2520cannot%2520be%2520expressed%2520as%2520fixed%2520rules.%2520Motivated%2520by%2520the%2520fact%2520that%2520human%2520experts%2520exhibit%2520significantly%2520better%2520rewrite%2520ability%2520but%2520suffer%2520from%2520scalability%252C%2520and%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520nearly%2520human-level%2520semantic%2520and%2520reasoning%2520abilities%252C%2520we%2520propose%2520a%2520new%2520approach%2520of%2520using%2520LLMs%2520to%2520rewrite%2520SQL%2520queries%2520beyond%2520rules.%2520Due%2520to%2520the%2520hallucination%2520problems%2520in%2520LLMs%252C%2520directly%2520applying%2520LLMs%2520often%2520leads%2520to%2520nonequivalent%2520and%2520suboptimal%2520queries.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520QUITE%2520%2528query%2520rewrite%2529%252C%2520a%2520training-free%2520and%2520feedback-aware%2520system%2520based%2520on%2520LLM%2520agents%2520that%2520rewrites%2520SQL%2520queries%2520into%2520semantically%2520equivalent%2520forms%2520with%2520significantly%2520better%2520performance%252C%2520covering%2520a%2520broader%2520range%2520of%2520query%2520patterns%2520and%2520rewrite%2520strategies%2520compared%2520to%2520rule-based%2520methods.%2520Firstly%252C%2520we%2520design%2520a%2520multi-agent%2520framework%2520controlled%2520by%2520a%2520finite%2520state%2520machine%2520%2528FSM%2529%2520to%2520equip%2520LLMs%2520with%2520the%2520ability%2520to%2520use%2520external%2520tools%2520and%2520enhance%2520the%2520rewrite%2520process%2520with%2520real-time%2520database%2520feedback.%2520Secondly%252C%2520we%2520develop%2520a%2520rewrite%2520middleware%2520to%2520enhance%2520the%2520ability%2520of%2520LLMs%2520to%2520generate%2520optimized%2520query%2520equivalents.%2520Finally%252C%2520we%2520employ%2520a%2520novel%2520hint%2520injection%2520technique%2520to%2520improve%2520execution%2520plans%2520for%2520rewritten%2520queries.%2520Extensive%2520experiments%2520show%2520that%2520QUITE%2520reduces%2520query%2520execution%2520time%2520by%2520up%2520to%252035.8%2525%2520over%2520state-of-the-art%2520approaches%2520and%2520produces%252024.1%2525%2520more%2520rewrites%2520than%2520prior%2520methods%252C%2520covering%2520query%2520cases%2520that%2520earlier%2520systems%2520did%2520not%2520handle.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07675v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QUITE%3A%20A%20Query%20Rewrite%20System%20Beyond%20Rules%20with%20LLM%20Agents&entry.906535625=Yuyang%20Song%20and%20Hanxu%20Yan%20and%20Jiale%20Lao%20and%20Yibo%20Wang%20and%20Yufei%20Li%20and%20Yuanchun%20Zhou%20and%20Jianguo%20Wang%20and%20Mingjie%20Tang&entry.1292438233=Query%20rewrite%20transforms%20SQL%20queries%20into%20semantically%20equivalent%20forms%20that%20run%20more%20efficiently.%20Existing%20approaches%20mainly%20rely%20on%20predefined%20rewrite%20rules%2C%20but%20they%20handle%20a%20limited%20subset%20of%20queries%20and%20can%20cause%20performance%20regressions.%20This%20limitation%20stems%20from%20three%20challenges%20of%20rule-based%20query%20rewrite%3A%20%281%29%20it%20is%20hard%20to%20discover%20and%20verify%20new%20rules%2C%20%282%29%20fixed%20rewrite%20rules%20do%20not%20generalize%20to%20new%20query%20patterns%2C%20and%20%283%29%20some%20rewrite%20techniques%20cannot%20be%20expressed%20as%20fixed%20rules.%20Motivated%20by%20the%20fact%20that%20human%20experts%20exhibit%20significantly%20better%20rewrite%20ability%20but%20suffer%20from%20scalability%2C%20and%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20nearly%20human-level%20semantic%20and%20reasoning%20abilities%2C%20we%20propose%20a%20new%20approach%20of%20using%20LLMs%20to%20rewrite%20SQL%20queries%20beyond%20rules.%20Due%20to%20the%20hallucination%20problems%20in%20LLMs%2C%20directly%20applying%20LLMs%20often%20leads%20to%20nonequivalent%20and%20suboptimal%20queries.%20To%20address%20this%20issue%2C%20we%20propose%20QUITE%20%28query%20rewrite%29%2C%20a%20training-free%20and%20feedback-aware%20system%20based%20on%20LLM%20agents%20that%20rewrites%20SQL%20queries%20into%20semantically%20equivalent%20forms%20with%20significantly%20better%20performance%2C%20covering%20a%20broader%20range%20of%20query%20patterns%20and%20rewrite%20strategies%20compared%20to%20rule-based%20methods.%20Firstly%2C%20we%20design%20a%20multi-agent%20framework%20controlled%20by%20a%20finite%20state%20machine%20%28FSM%29%20to%20equip%20LLMs%20with%20the%20ability%20to%20use%20external%20tools%20and%20enhance%20the%20rewrite%20process%20with%20real-time%20database%20feedback.%20Secondly%2C%20we%20develop%20a%20rewrite%20middleware%20to%20enhance%20the%20ability%20of%20LLMs%20to%20generate%20optimized%20query%20equivalents.%20Finally%2C%20we%20employ%20a%20novel%20hint%20injection%20technique%20to%20improve%20execution%20plans%20for%20rewritten%20queries.%20Extensive%20experiments%20show%20that%20QUITE%20reduces%20query%20execution%20time%20by%20up%20to%2035.8%25%20over%20state-of-the-art%20approaches%20and%20produces%2024.1%25%20more%20rewrites%20than%20prior%20methods%2C%20covering%20query%20cases%20that%20earlier%20systems%20did%20not%20handle.&entry.1838667208=http%3A//arxiv.org/abs/2506.07675v3&entry.124074799=Read"},
{"title": "Sorbet: A Neuromorphic Hardware-Compatible Transformer-Based Spiking Language Model", "author": "Kaiwen Tang and Zhanglu Yan and Weng-Fai Wong", "abstract": "For reasons such as privacy, there are use cases for language models at the edge. This has given rise to small language models targeted for deployment in resource-constrained devices where energy efficiency is critical. Spiking neural networks (SNNs) offer a promising solution due to their energy efficiency, and there are already works on realizing transformer-based models on SNNs. However, key operations like softmax and layer normalization (LN) are difficult to implement on neuromorphic hardware, and many of these early works sidestepped them. To address these challenges, we introduce Sorbet, a transformer-based spiking language model that is more neuromorphic hardware-compatible. Sorbet incorporates a novel shifting-based softmax called PTsoftmax and a Bit Shifting PowerNorm (BSPN), both designed to replace the respective energy-intensive operations. By leveraging knowledge distillation and model quantization, Sorbet achieved a highly compressed binary weight model that maintains competitive performance while achieving $27.16\\times$ energy savings compared to BERT. We validate Sorbet through extensive testing on the GLUE benchmark and a series of ablation studies, demonstrating its potential as an energy-efficient solution for language model inference. Our code is publicly available at \\href{https://github.com/Kaiwen-Tang/Sorbet}{https://github.com/Kaiwen-Tang/Sorbet}", "link": "http://arxiv.org/abs/2409.15298v2", "date": "2026-01-02", "relevancy": 1.005, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5152}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5017}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sorbet%3A%20A%20Neuromorphic%20Hardware-Compatible%20Transformer-Based%20Spiking%20Language%20Model&body=Title%3A%20Sorbet%3A%20A%20Neuromorphic%20Hardware-Compatible%20Transformer-Based%20Spiking%20Language%20Model%0AAuthor%3A%20Kaiwen%20Tang%20and%20Zhanglu%20Yan%20and%20Weng-Fai%20Wong%0AAbstract%3A%20For%20reasons%20such%20as%20privacy%2C%20there%20are%20use%20cases%20for%20language%20models%20at%20the%20edge.%20This%20has%20given%20rise%20to%20small%20language%20models%20targeted%20for%20deployment%20in%20resource-constrained%20devices%20where%20energy%20efficiency%20is%20critical.%20Spiking%20neural%20networks%20%28SNNs%29%20offer%20a%20promising%20solution%20due%20to%20their%20energy%20efficiency%2C%20and%20there%20are%20already%20works%20on%20realizing%20transformer-based%20models%20on%20SNNs.%20However%2C%20key%20operations%20like%20softmax%20and%20layer%20normalization%20%28LN%29%20are%20difficult%20to%20implement%20on%20neuromorphic%20hardware%2C%20and%20many%20of%20these%20early%20works%20sidestepped%20them.%20To%20address%20these%20challenges%2C%20we%20introduce%20Sorbet%2C%20a%20transformer-based%20spiking%20language%20model%20that%20is%20more%20neuromorphic%20hardware-compatible.%20Sorbet%20incorporates%20a%20novel%20shifting-based%20softmax%20called%20PTsoftmax%20and%20a%20Bit%20Shifting%20PowerNorm%20%28BSPN%29%2C%20both%20designed%20to%20replace%20the%20respective%20energy-intensive%20operations.%20By%20leveraging%20knowledge%20distillation%20and%20model%20quantization%2C%20Sorbet%20achieved%20a%20highly%20compressed%20binary%20weight%20model%20that%20maintains%20competitive%20performance%20while%20achieving%20%2427.16%5Ctimes%24%20energy%20savings%20compared%20to%20BERT.%20We%20validate%20Sorbet%20through%20extensive%20testing%20on%20the%20GLUE%20benchmark%20and%20a%20series%20of%20ablation%20studies%2C%20demonstrating%20its%20potential%20as%20an%20energy-efficient%20solution%20for%20language%20model%20inference.%20Our%20code%20is%20publicly%20available%20at%20%5Chref%7Bhttps%3A//github.com/Kaiwen-Tang/Sorbet%7D%7Bhttps%3A//github.com/Kaiwen-Tang/Sorbet%7D%0ALink%3A%20http%3A//arxiv.org/abs/2409.15298v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSorbet%253A%2520A%2520Neuromorphic%2520Hardware-Compatible%2520Transformer-Based%2520Spiking%2520Language%2520Model%26entry.906535625%3DKaiwen%2520Tang%2520and%2520Zhanglu%2520Yan%2520and%2520Weng-Fai%2520Wong%26entry.1292438233%3DFor%2520reasons%2520such%2520as%2520privacy%252C%2520there%2520are%2520use%2520cases%2520for%2520language%2520models%2520at%2520the%2520edge.%2520This%2520has%2520given%2520rise%2520to%2520small%2520language%2520models%2520targeted%2520for%2520deployment%2520in%2520resource-constrained%2520devices%2520where%2520energy%2520efficiency%2520is%2520critical.%2520Spiking%2520neural%2520networks%2520%2528SNNs%2529%2520offer%2520a%2520promising%2520solution%2520due%2520to%2520their%2520energy%2520efficiency%252C%2520and%2520there%2520are%2520already%2520works%2520on%2520realizing%2520transformer-based%2520models%2520on%2520SNNs.%2520However%252C%2520key%2520operations%2520like%2520softmax%2520and%2520layer%2520normalization%2520%2528LN%2529%2520are%2520difficult%2520to%2520implement%2520on%2520neuromorphic%2520hardware%252C%2520and%2520many%2520of%2520these%2520early%2520works%2520sidestepped%2520them.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520Sorbet%252C%2520a%2520transformer-based%2520spiking%2520language%2520model%2520that%2520is%2520more%2520neuromorphic%2520hardware-compatible.%2520Sorbet%2520incorporates%2520a%2520novel%2520shifting-based%2520softmax%2520called%2520PTsoftmax%2520and%2520a%2520Bit%2520Shifting%2520PowerNorm%2520%2528BSPN%2529%252C%2520both%2520designed%2520to%2520replace%2520the%2520respective%2520energy-intensive%2520operations.%2520By%2520leveraging%2520knowledge%2520distillation%2520and%2520model%2520quantization%252C%2520Sorbet%2520achieved%2520a%2520highly%2520compressed%2520binary%2520weight%2520model%2520that%2520maintains%2520competitive%2520performance%2520while%2520achieving%2520%252427.16%255Ctimes%2524%2520energy%2520savings%2520compared%2520to%2520BERT.%2520We%2520validate%2520Sorbet%2520through%2520extensive%2520testing%2520on%2520the%2520GLUE%2520benchmark%2520and%2520a%2520series%2520of%2520ablation%2520studies%252C%2520demonstrating%2520its%2520potential%2520as%2520an%2520energy-efficient%2520solution%2520for%2520language%2520model%2520inference.%2520Our%2520code%2520is%2520publicly%2520available%2520at%2520%255Chref%257Bhttps%253A//github.com/Kaiwen-Tang/Sorbet%257D%257Bhttps%253A//github.com/Kaiwen-Tang/Sorbet%257D%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.15298v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sorbet%3A%20A%20Neuromorphic%20Hardware-Compatible%20Transformer-Based%20Spiking%20Language%20Model&entry.906535625=Kaiwen%20Tang%20and%20Zhanglu%20Yan%20and%20Weng-Fai%20Wong&entry.1292438233=For%20reasons%20such%20as%20privacy%2C%20there%20are%20use%20cases%20for%20language%20models%20at%20the%20edge.%20This%20has%20given%20rise%20to%20small%20language%20models%20targeted%20for%20deployment%20in%20resource-constrained%20devices%20where%20energy%20efficiency%20is%20critical.%20Spiking%20neural%20networks%20%28SNNs%29%20offer%20a%20promising%20solution%20due%20to%20their%20energy%20efficiency%2C%20and%20there%20are%20already%20works%20on%20realizing%20transformer-based%20models%20on%20SNNs.%20However%2C%20key%20operations%20like%20softmax%20and%20layer%20normalization%20%28LN%29%20are%20difficult%20to%20implement%20on%20neuromorphic%20hardware%2C%20and%20many%20of%20these%20early%20works%20sidestepped%20them.%20To%20address%20these%20challenges%2C%20we%20introduce%20Sorbet%2C%20a%20transformer-based%20spiking%20language%20model%20that%20is%20more%20neuromorphic%20hardware-compatible.%20Sorbet%20incorporates%20a%20novel%20shifting-based%20softmax%20called%20PTsoftmax%20and%20a%20Bit%20Shifting%20PowerNorm%20%28BSPN%29%2C%20both%20designed%20to%20replace%20the%20respective%20energy-intensive%20operations.%20By%20leveraging%20knowledge%20distillation%20and%20model%20quantization%2C%20Sorbet%20achieved%20a%20highly%20compressed%20binary%20weight%20model%20that%20maintains%20competitive%20performance%20while%20achieving%20%2427.16%5Ctimes%24%20energy%20savings%20compared%20to%20BERT.%20We%20validate%20Sorbet%20through%20extensive%20testing%20on%20the%20GLUE%20benchmark%20and%20a%20series%20of%20ablation%20studies%2C%20demonstrating%20its%20potential%20as%20an%20energy-efficient%20solution%20for%20language%20model%20inference.%20Our%20code%20is%20publicly%20available%20at%20%5Chref%7Bhttps%3A//github.com/Kaiwen-Tang/Sorbet%7D%7Bhttps%3A//github.com/Kaiwen-Tang/Sorbet%7D&entry.1838667208=http%3A//arxiv.org/abs/2409.15298v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


