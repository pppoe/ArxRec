<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240911.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Hi3D: Pursuing High-Resolution Image-to-3D Generation with Video\n  Diffusion Models", "author": "Haibo Yang and Yang Chen and Yingwei Pan and Ting Yao and Zhineng Chen and Chong-Wah Ngo and Tao Mei", "abstract": "  Despite having tremendous progress in image-to-3D generation, existing\nmethods still struggle to produce multi-view consistent images with\nhigh-resolution textures in detail, especially in the paradigm of 2D diffusion\nthat lacks 3D awareness. In this work, we present High-resolution Image-to-3D\nmodel (Hi3D), a new video diffusion based paradigm that redefines a single\nimage to multi-view images as 3D-aware sequential image generation (i.e.,\norbital video generation). This methodology delves into the underlying temporal\nconsistency knowledge in video diffusion model that generalizes well to\ngeometry consistency across multiple views in 3D generation. Technically, Hi3D\nfirst empowers the pre-trained video diffusion model with 3D-aware prior\n(camera pose condition), yielding multi-view images with low-resolution texture\ndetails. A 3D-aware video-to-video refiner is learnt to further scale up the\nmulti-view images with high-resolution texture details. Such high-resolution\nmulti-view images are further augmented with novel views through 3D Gaussian\nSplatting, which are finally leveraged to obtain high-fidelity meshes via 3D\nreconstruction. Extensive experiments on both novel view synthesis and single\nview reconstruction demonstrate that our Hi3D manages to produce superior\nmulti-view consistency images with highly-detailed textures. Source code and\ndata are available at \\url{https://github.com/yanghb22-fdu/Hi3D-Official}.\n", "link": "http://arxiv.org/abs/2409.07452v1", "date": "2024-09-11", "relevancy": 3.6203, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7591}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7591}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hi3D%3A%20Pursuing%20High-Resolution%20Image-to-3D%20Generation%20with%20Video%0A%20%20Diffusion%20Models&body=Title%3A%20Hi3D%3A%20Pursuing%20High-Resolution%20Image-to-3D%20Generation%20with%20Video%0A%20%20Diffusion%20Models%0AAuthor%3A%20Haibo%20Yang%20and%20Yang%20Chen%20and%20Yingwei%20Pan%20and%20Ting%20Yao%20and%20Zhineng%20Chen%20and%20Chong-Wah%20Ngo%20and%20Tao%20Mei%0AAbstract%3A%20%20%20Despite%20having%20tremendous%20progress%20in%20image-to-3D%20generation%2C%20existing%0Amethods%20still%20struggle%20to%20produce%20multi-view%20consistent%20images%20with%0Ahigh-resolution%20textures%20in%20detail%2C%20especially%20in%20the%20paradigm%20of%202D%20diffusion%0Athat%20lacks%203D%20awareness.%20In%20this%20work%2C%20we%20present%20High-resolution%20Image-to-3D%0Amodel%20%28Hi3D%29%2C%20a%20new%20video%20diffusion%20based%20paradigm%20that%20redefines%20a%20single%0Aimage%20to%20multi-view%20images%20as%203D-aware%20sequential%20image%20generation%20%28i.e.%2C%0Aorbital%20video%20generation%29.%20This%20methodology%20delves%20into%20the%20underlying%20temporal%0Aconsistency%20knowledge%20in%20video%20diffusion%20model%20that%20generalizes%20well%20to%0Ageometry%20consistency%20across%20multiple%20views%20in%203D%20generation.%20Technically%2C%20Hi3D%0Afirst%20empowers%20the%20pre-trained%20video%20diffusion%20model%20with%203D-aware%20prior%0A%28camera%20pose%20condition%29%2C%20yielding%20multi-view%20images%20with%20low-resolution%20texture%0Adetails.%20A%203D-aware%20video-to-video%20refiner%20is%20learnt%20to%20further%20scale%20up%20the%0Amulti-view%20images%20with%20high-resolution%20texture%20details.%20Such%20high-resolution%0Amulti-view%20images%20are%20further%20augmented%20with%20novel%20views%20through%203D%20Gaussian%0ASplatting%2C%20which%20are%20finally%20leveraged%20to%20obtain%20high-fidelity%20meshes%20via%203D%0Areconstruction.%20Extensive%20experiments%20on%20both%20novel%20view%20synthesis%20and%20single%0Aview%20reconstruction%20demonstrate%20that%20our%20Hi3D%20manages%20to%20produce%20superior%0Amulti-view%20consistency%20images%20with%20highly-detailed%20textures.%20Source%20code%20and%0Adata%20are%20available%20at%20%5Curl%7Bhttps%3A//github.com/yanghb22-fdu/Hi3D-Official%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07452v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHi3D%253A%2520Pursuing%2520High-Resolution%2520Image-to-3D%2520Generation%2520with%2520Video%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DHaibo%2520Yang%2520and%2520Yang%2520Chen%2520and%2520Yingwei%2520Pan%2520and%2520Ting%2520Yao%2520and%2520Zhineng%2520Chen%2520and%2520Chong-Wah%2520Ngo%2520and%2520Tao%2520Mei%26entry.1292438233%3D%2520%2520Despite%2520having%2520tremendous%2520progress%2520in%2520image-to-3D%2520generation%252C%2520existing%250Amethods%2520still%2520struggle%2520to%2520produce%2520multi-view%2520consistent%2520images%2520with%250Ahigh-resolution%2520textures%2520in%2520detail%252C%2520especially%2520in%2520the%2520paradigm%2520of%25202D%2520diffusion%250Athat%2520lacks%25203D%2520awareness.%2520In%2520this%2520work%252C%2520we%2520present%2520High-resolution%2520Image-to-3D%250Amodel%2520%2528Hi3D%2529%252C%2520a%2520new%2520video%2520diffusion%2520based%2520paradigm%2520that%2520redefines%2520a%2520single%250Aimage%2520to%2520multi-view%2520images%2520as%25203D-aware%2520sequential%2520image%2520generation%2520%2528i.e.%252C%250Aorbital%2520video%2520generation%2529.%2520This%2520methodology%2520delves%2520into%2520the%2520underlying%2520temporal%250Aconsistency%2520knowledge%2520in%2520video%2520diffusion%2520model%2520that%2520generalizes%2520well%2520to%250Ageometry%2520consistency%2520across%2520multiple%2520views%2520in%25203D%2520generation.%2520Technically%252C%2520Hi3D%250Afirst%2520empowers%2520the%2520pre-trained%2520video%2520diffusion%2520model%2520with%25203D-aware%2520prior%250A%2528camera%2520pose%2520condition%2529%252C%2520yielding%2520multi-view%2520images%2520with%2520low-resolution%2520texture%250Adetails.%2520A%25203D-aware%2520video-to-video%2520refiner%2520is%2520learnt%2520to%2520further%2520scale%2520up%2520the%250Amulti-view%2520images%2520with%2520high-resolution%2520texture%2520details.%2520Such%2520high-resolution%250Amulti-view%2520images%2520are%2520further%2520augmented%2520with%2520novel%2520views%2520through%25203D%2520Gaussian%250ASplatting%252C%2520which%2520are%2520finally%2520leveraged%2520to%2520obtain%2520high-fidelity%2520meshes%2520via%25203D%250Areconstruction.%2520Extensive%2520experiments%2520on%2520both%2520novel%2520view%2520synthesis%2520and%2520single%250Aview%2520reconstruction%2520demonstrate%2520that%2520our%2520Hi3D%2520manages%2520to%2520produce%2520superior%250Amulti-view%2520consistency%2520images%2520with%2520highly-detailed%2520textures.%2520Source%2520code%2520and%250Adata%2520are%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/yanghb22-fdu/Hi3D-Official%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07452v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hi3D%3A%20Pursuing%20High-Resolution%20Image-to-3D%20Generation%20with%20Video%0A%20%20Diffusion%20Models&entry.906535625=Haibo%20Yang%20and%20Yang%20Chen%20and%20Yingwei%20Pan%20and%20Ting%20Yao%20and%20Zhineng%20Chen%20and%20Chong-Wah%20Ngo%20and%20Tao%20Mei&entry.1292438233=%20%20Despite%20having%20tremendous%20progress%20in%20image-to-3D%20generation%2C%20existing%0Amethods%20still%20struggle%20to%20produce%20multi-view%20consistent%20images%20with%0Ahigh-resolution%20textures%20in%20detail%2C%20especially%20in%20the%20paradigm%20of%202D%20diffusion%0Athat%20lacks%203D%20awareness.%20In%20this%20work%2C%20we%20present%20High-resolution%20Image-to-3D%0Amodel%20%28Hi3D%29%2C%20a%20new%20video%20diffusion%20based%20paradigm%20that%20redefines%20a%20single%0Aimage%20to%20multi-view%20images%20as%203D-aware%20sequential%20image%20generation%20%28i.e.%2C%0Aorbital%20video%20generation%29.%20This%20methodology%20delves%20into%20the%20underlying%20temporal%0Aconsistency%20knowledge%20in%20video%20diffusion%20model%20that%20generalizes%20well%20to%0Ageometry%20consistency%20across%20multiple%20views%20in%203D%20generation.%20Technically%2C%20Hi3D%0Afirst%20empowers%20the%20pre-trained%20video%20diffusion%20model%20with%203D-aware%20prior%0A%28camera%20pose%20condition%29%2C%20yielding%20multi-view%20images%20with%20low-resolution%20texture%0Adetails.%20A%203D-aware%20video-to-video%20refiner%20is%20learnt%20to%20further%20scale%20up%20the%0Amulti-view%20images%20with%20high-resolution%20texture%20details.%20Such%20high-resolution%0Amulti-view%20images%20are%20further%20augmented%20with%20novel%20views%20through%203D%20Gaussian%0ASplatting%2C%20which%20are%20finally%20leveraged%20to%20obtain%20high-fidelity%20meshes%20via%203D%0Areconstruction.%20Extensive%20experiments%20on%20both%20novel%20view%20synthesis%20and%20single%0Aview%20reconstruction%20demonstrate%20that%20our%20Hi3D%20manages%20to%20produce%20superior%0Amulti-view%20consistency%20images%20with%20highly-detailed%20textures.%20Source%20code%20and%0Adata%20are%20available%20at%20%5Curl%7Bhttps%3A//github.com/yanghb22-fdu/Hi3D-Official%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07452v1&entry.124074799=Read"},
{"title": "Single-View 3D Reconstruction via SO(2)-Equivariant Gaussian Sculpting\n  Networks", "author": "Ruihan Xu and Anthony Opipari and Joshua Mah and Stanley Lewis and Haoran Zhang and Hanzhe Guo and Odest Chadwicke Jenkins", "abstract": "  This paper introduces SO(2)-Equivariant Gaussian Sculpting Networks (GSNs) as\nan approach for SO(2)-Equivariant 3D object reconstruction from single-view\nimage observations.\n  GSNs take a single observation as input to generate a Gaussian splat\nrepresentation describing the observed object's geometry and texture. By using\na shared feature extractor before decoding Gaussian colors, covariances,\npositions, and opacities, GSNs achieve extremely high throughput (>150FPS).\nExperiments demonstrate that GSNs can be trained efficiently using a multi-view\nrendering loss and are competitive, in quality, with expensive diffusion-based\nreconstruction algorithms. The GSN model is validated on multiple benchmark\nexperiments. Moreover, we demonstrate the potential for GSNs to be used within\na robotic manipulation pipeline for object-centric grasping.\n", "link": "http://arxiv.org/abs/2409.07245v1", "date": "2024-09-11", "relevancy": 3.2774, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6942}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6923}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single-View%203D%20Reconstruction%20via%20SO%282%29-Equivariant%20Gaussian%20Sculpting%0A%20%20Networks&body=Title%3A%20Single-View%203D%20Reconstruction%20via%20SO%282%29-Equivariant%20Gaussian%20Sculpting%0A%20%20Networks%0AAuthor%3A%20Ruihan%20Xu%20and%20Anthony%20Opipari%20and%20Joshua%20Mah%20and%20Stanley%20Lewis%20and%20Haoran%20Zhang%20and%20Hanzhe%20Guo%20and%20Odest%20Chadwicke%20Jenkins%0AAbstract%3A%20%20%20This%20paper%20introduces%20SO%282%29-Equivariant%20Gaussian%20Sculpting%20Networks%20%28GSNs%29%20as%0Aan%20approach%20for%20SO%282%29-Equivariant%203D%20object%20reconstruction%20from%20single-view%0Aimage%20observations.%0A%20%20GSNs%20take%20a%20single%20observation%20as%20input%20to%20generate%20a%20Gaussian%20splat%0Arepresentation%20describing%20the%20observed%20object%27s%20geometry%20and%20texture.%20By%20using%0Aa%20shared%20feature%20extractor%20before%20decoding%20Gaussian%20colors%2C%20covariances%2C%0Apositions%2C%20and%20opacities%2C%20GSNs%20achieve%20extremely%20high%20throughput%20%28%3E150FPS%29.%0AExperiments%20demonstrate%20that%20GSNs%20can%20be%20trained%20efficiently%20using%20a%20multi-view%0Arendering%20loss%20and%20are%20competitive%2C%20in%20quality%2C%20with%20expensive%20diffusion-based%0Areconstruction%20algorithms.%20The%20GSN%20model%20is%20validated%20on%20multiple%20benchmark%0Aexperiments.%20Moreover%2C%20we%20demonstrate%20the%20potential%20for%20GSNs%20to%20be%20used%20within%0Aa%20robotic%20manipulation%20pipeline%20for%20object-centric%20grasping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07245v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle-View%25203D%2520Reconstruction%2520via%2520SO%25282%2529-Equivariant%2520Gaussian%2520Sculpting%250A%2520%2520Networks%26entry.906535625%3DRuihan%2520Xu%2520and%2520Anthony%2520Opipari%2520and%2520Joshua%2520Mah%2520and%2520Stanley%2520Lewis%2520and%2520Haoran%2520Zhang%2520and%2520Hanzhe%2520Guo%2520and%2520Odest%2520Chadwicke%2520Jenkins%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520SO%25282%2529-Equivariant%2520Gaussian%2520Sculpting%2520Networks%2520%2528GSNs%2529%2520as%250Aan%2520approach%2520for%2520SO%25282%2529-Equivariant%25203D%2520object%2520reconstruction%2520from%2520single-view%250Aimage%2520observations.%250A%2520%2520GSNs%2520take%2520a%2520single%2520observation%2520as%2520input%2520to%2520generate%2520a%2520Gaussian%2520splat%250Arepresentation%2520describing%2520the%2520observed%2520object%2527s%2520geometry%2520and%2520texture.%2520By%2520using%250Aa%2520shared%2520feature%2520extractor%2520before%2520decoding%2520Gaussian%2520colors%252C%2520covariances%252C%250Apositions%252C%2520and%2520opacities%252C%2520GSNs%2520achieve%2520extremely%2520high%2520throughput%2520%2528%253E150FPS%2529.%250AExperiments%2520demonstrate%2520that%2520GSNs%2520can%2520be%2520trained%2520efficiently%2520using%2520a%2520multi-view%250Arendering%2520loss%2520and%2520are%2520competitive%252C%2520in%2520quality%252C%2520with%2520expensive%2520diffusion-based%250Areconstruction%2520algorithms.%2520The%2520GSN%2520model%2520is%2520validated%2520on%2520multiple%2520benchmark%250Aexperiments.%2520Moreover%252C%2520we%2520demonstrate%2520the%2520potential%2520for%2520GSNs%2520to%2520be%2520used%2520within%250Aa%2520robotic%2520manipulation%2520pipeline%2520for%2520object-centric%2520grasping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07245v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single-View%203D%20Reconstruction%20via%20SO%282%29-Equivariant%20Gaussian%20Sculpting%0A%20%20Networks&entry.906535625=Ruihan%20Xu%20and%20Anthony%20Opipari%20and%20Joshua%20Mah%20and%20Stanley%20Lewis%20and%20Haoran%20Zhang%20and%20Hanzhe%20Guo%20and%20Odest%20Chadwicke%20Jenkins&entry.1292438233=%20%20This%20paper%20introduces%20SO%282%29-Equivariant%20Gaussian%20Sculpting%20Networks%20%28GSNs%29%20as%0Aan%20approach%20for%20SO%282%29-Equivariant%203D%20object%20reconstruction%20from%20single-view%0Aimage%20observations.%0A%20%20GSNs%20take%20a%20single%20observation%20as%20input%20to%20generate%20a%20Gaussian%20splat%0Arepresentation%20describing%20the%20observed%20object%27s%20geometry%20and%20texture.%20By%20using%0Aa%20shared%20feature%20extractor%20before%20decoding%20Gaussian%20colors%2C%20covariances%2C%0Apositions%2C%20and%20opacities%2C%20GSNs%20achieve%20extremely%20high%20throughput%20%28%3E150FPS%29.%0AExperiments%20demonstrate%20that%20GSNs%20can%20be%20trained%20efficiently%20using%20a%20multi-view%0Arendering%20loss%20and%20are%20competitive%2C%20in%20quality%2C%20with%20expensive%20diffusion-based%0Areconstruction%20algorithms.%20The%20GSN%20model%20is%20validated%20on%20multiple%20benchmark%0Aexperiments.%20Moreover%2C%20we%20demonstrate%20the%20potential%20for%20GSNs%20to%20be%20used%20within%0Aa%20robotic%20manipulation%20pipeline%20for%20object-centric%20grasping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07245v1&entry.124074799=Read"},
{"title": "MV-CLIP: Multi-View CLIP for Zero-shot 3D Shape Recognition", "author": "Dan Song and Xinwei Fu and Ning Liu and Weizhi Nie and Wenhui Li and Lanjun Wang and You Yang and Anan Liu", "abstract": "  Large-scale pre-trained models have demonstrated impressive performance in\nvision and language tasks within open-world scenarios. Due to the lack of\ncomparable pre-trained models for 3D shapes, recent methods utilize\nlanguage-image pre-training to realize zero-shot 3D shape recognition. However,\ndue to the modality gap, pretrained language-image models are not confident\nenough in the generalization to 3D shape recognition. Consequently, this paper\naims to improve the confidence with view selection and hierarchical prompts.\nLeveraging the CLIP model as an example, we employ view selection on the vision\nside by identifying views with high prediction confidence from multiple\nrendered views of a 3D shape. On the textual side, the strategy of hierarchical\nprompts is proposed for the first time. The first layer prompts several\nclassification candidates with traditional class-level descriptions, while the\nsecond layer refines the prediction based on function-level descriptions or\nfurther distinctions between the candidates. Remarkably, without the need for\nadditional training, our proposed method achieves impressive zero-shot 3D\nclassification accuracies of 84.44%, 91.51%, and 66.17% on ModelNet40,\nModelNet10, and ShapeNet Core55, respectively. Furthermore, we will make the\ncode publicly available to facilitate reproducibility and further research in\nthis area.\n", "link": "http://arxiv.org/abs/2311.18402v3", "date": "2024-09-11", "relevancy": 3.2521, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.7136}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6188}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MV-CLIP%3A%20Multi-View%20CLIP%20for%20Zero-shot%203D%20Shape%20Recognition&body=Title%3A%20MV-CLIP%3A%20Multi-View%20CLIP%20for%20Zero-shot%203D%20Shape%20Recognition%0AAuthor%3A%20Dan%20Song%20and%20Xinwei%20Fu%20and%20Ning%20Liu%20and%20Weizhi%20Nie%20and%20Wenhui%20Li%20and%20Lanjun%20Wang%20and%20You%20Yang%20and%20Anan%20Liu%0AAbstract%3A%20%20%20Large-scale%20pre-trained%20models%20have%20demonstrated%20impressive%20performance%20in%0Avision%20and%20language%20tasks%20within%20open-world%20scenarios.%20Due%20to%20the%20lack%20of%0Acomparable%20pre-trained%20models%20for%203D%20shapes%2C%20recent%20methods%20utilize%0Alanguage-image%20pre-training%20to%20realize%20zero-shot%203D%20shape%20recognition.%20However%2C%0Adue%20to%20the%20modality%20gap%2C%20pretrained%20language-image%20models%20are%20not%20confident%0Aenough%20in%20the%20generalization%20to%203D%20shape%20recognition.%20Consequently%2C%20this%20paper%0Aaims%20to%20improve%20the%20confidence%20with%20view%20selection%20and%20hierarchical%20prompts.%0ALeveraging%20the%20CLIP%20model%20as%20an%20example%2C%20we%20employ%20view%20selection%20on%20the%20vision%0Aside%20by%20identifying%20views%20with%20high%20prediction%20confidence%20from%20multiple%0Arendered%20views%20of%20a%203D%20shape.%20On%20the%20textual%20side%2C%20the%20strategy%20of%20hierarchical%0Aprompts%20is%20proposed%20for%20the%20first%20time.%20The%20first%20layer%20prompts%20several%0Aclassification%20candidates%20with%20traditional%20class-level%20descriptions%2C%20while%20the%0Asecond%20layer%20refines%20the%20prediction%20based%20on%20function-level%20descriptions%20or%0Afurther%20distinctions%20between%20the%20candidates.%20Remarkably%2C%20without%20the%20need%20for%0Aadditional%20training%2C%20our%20proposed%20method%20achieves%20impressive%20zero-shot%203D%0Aclassification%20accuracies%20of%2084.44%25%2C%2091.51%25%2C%20and%2066.17%25%20on%20ModelNet40%2C%0AModelNet10%2C%20and%20ShapeNet%20Core55%2C%20respectively.%20Furthermore%2C%20we%20will%20make%20the%0Acode%20publicly%20available%20to%20facilitate%20reproducibility%20and%20further%20research%20in%0Athis%20area.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.18402v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMV-CLIP%253A%2520Multi-View%2520CLIP%2520for%2520Zero-shot%25203D%2520Shape%2520Recognition%26entry.906535625%3DDan%2520Song%2520and%2520Xinwei%2520Fu%2520and%2520Ning%2520Liu%2520and%2520Weizhi%2520Nie%2520and%2520Wenhui%2520Li%2520and%2520Lanjun%2520Wang%2520and%2520You%2520Yang%2520and%2520Anan%2520Liu%26entry.1292438233%3D%2520%2520Large-scale%2520pre-trained%2520models%2520have%2520demonstrated%2520impressive%2520performance%2520in%250Avision%2520and%2520language%2520tasks%2520within%2520open-world%2520scenarios.%2520Due%2520to%2520the%2520lack%2520of%250Acomparable%2520pre-trained%2520models%2520for%25203D%2520shapes%252C%2520recent%2520methods%2520utilize%250Alanguage-image%2520pre-training%2520to%2520realize%2520zero-shot%25203D%2520shape%2520recognition.%2520However%252C%250Adue%2520to%2520the%2520modality%2520gap%252C%2520pretrained%2520language-image%2520models%2520are%2520not%2520confident%250Aenough%2520in%2520the%2520generalization%2520to%25203D%2520shape%2520recognition.%2520Consequently%252C%2520this%2520paper%250Aaims%2520to%2520improve%2520the%2520confidence%2520with%2520view%2520selection%2520and%2520hierarchical%2520prompts.%250ALeveraging%2520the%2520CLIP%2520model%2520as%2520an%2520example%252C%2520we%2520employ%2520view%2520selection%2520on%2520the%2520vision%250Aside%2520by%2520identifying%2520views%2520with%2520high%2520prediction%2520confidence%2520from%2520multiple%250Arendered%2520views%2520of%2520a%25203D%2520shape.%2520On%2520the%2520textual%2520side%252C%2520the%2520strategy%2520of%2520hierarchical%250Aprompts%2520is%2520proposed%2520for%2520the%2520first%2520time.%2520The%2520first%2520layer%2520prompts%2520several%250Aclassification%2520candidates%2520with%2520traditional%2520class-level%2520descriptions%252C%2520while%2520the%250Asecond%2520layer%2520refines%2520the%2520prediction%2520based%2520on%2520function-level%2520descriptions%2520or%250Afurther%2520distinctions%2520between%2520the%2520candidates.%2520Remarkably%252C%2520without%2520the%2520need%2520for%250Aadditional%2520training%252C%2520our%2520proposed%2520method%2520achieves%2520impressive%2520zero-shot%25203D%250Aclassification%2520accuracies%2520of%252084.44%2525%252C%252091.51%2525%252C%2520and%252066.17%2525%2520on%2520ModelNet40%252C%250AModelNet10%252C%2520and%2520ShapeNet%2520Core55%252C%2520respectively.%2520Furthermore%252C%2520we%2520will%2520make%2520the%250Acode%2520publicly%2520available%2520to%2520facilitate%2520reproducibility%2520and%2520further%2520research%2520in%250Athis%2520area.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.18402v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MV-CLIP%3A%20Multi-View%20CLIP%20for%20Zero-shot%203D%20Shape%20Recognition&entry.906535625=Dan%20Song%20and%20Xinwei%20Fu%20and%20Ning%20Liu%20and%20Weizhi%20Nie%20and%20Wenhui%20Li%20and%20Lanjun%20Wang%20and%20You%20Yang%20and%20Anan%20Liu&entry.1292438233=%20%20Large-scale%20pre-trained%20models%20have%20demonstrated%20impressive%20performance%20in%0Avision%20and%20language%20tasks%20within%20open-world%20scenarios.%20Due%20to%20the%20lack%20of%0Acomparable%20pre-trained%20models%20for%203D%20shapes%2C%20recent%20methods%20utilize%0Alanguage-image%20pre-training%20to%20realize%20zero-shot%203D%20shape%20recognition.%20However%2C%0Adue%20to%20the%20modality%20gap%2C%20pretrained%20language-image%20models%20are%20not%20confident%0Aenough%20in%20the%20generalization%20to%203D%20shape%20recognition.%20Consequently%2C%20this%20paper%0Aaims%20to%20improve%20the%20confidence%20with%20view%20selection%20and%20hierarchical%20prompts.%0ALeveraging%20the%20CLIP%20model%20as%20an%20example%2C%20we%20employ%20view%20selection%20on%20the%20vision%0Aside%20by%20identifying%20views%20with%20high%20prediction%20confidence%20from%20multiple%0Arendered%20views%20of%20a%203D%20shape.%20On%20the%20textual%20side%2C%20the%20strategy%20of%20hierarchical%0Aprompts%20is%20proposed%20for%20the%20first%20time.%20The%20first%20layer%20prompts%20several%0Aclassification%20candidates%20with%20traditional%20class-level%20descriptions%2C%20while%20the%0Asecond%20layer%20refines%20the%20prediction%20based%20on%20function-level%20descriptions%20or%0Afurther%20distinctions%20between%20the%20candidates.%20Remarkably%2C%20without%20the%20need%20for%0Aadditional%20training%2C%20our%20proposed%20method%20achieves%20impressive%20zero-shot%203D%0Aclassification%20accuracies%20of%2084.44%25%2C%2091.51%25%2C%20and%2066.17%25%20on%20ModelNet40%2C%0AModelNet10%2C%20and%20ShapeNet%20Core55%2C%20respectively.%20Furthermore%2C%20we%20will%20make%20the%0Acode%20publicly%20available%20to%20facilitate%20reproducibility%20and%20further%20research%20in%0Athis%20area.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.18402v3&entry.124074799=Read"},
{"title": "StereoCrafter: Diffusion-based Generation of Long and High-fidelity\n  Stereoscopic 3D from Monocular Videos", "author": "Sijie Zhao and Wenbo Hu and Xiaodong Cun and Yong Zhang and Xiaoyu Li and Zhe Kong and Xiangjun Gao and Muyao Niu and Ying Shan", "abstract": "  This paper presents a novel framework for converting 2D videos to immersive\nstereoscopic 3D, addressing the growing demand for 3D content in immersive\nexperience. Leveraging foundation models as priors, our approach overcomes the\nlimitations of traditional methods and boosts the performance to ensure the\nhigh-fidelity generation required by the display devices. The proposed system\nconsists of two main steps: depth-based video splatting for warping and\nextracting occlusion mask, and stereo video inpainting. We utilize pre-trained\nstable video diffusion as the backbone and introduce a fine-tuning protocol for\nthe stereo video inpainting task. To handle input video with varying lengths\nand resolutions, we explore auto-regressive strategies and tiled processing.\nFinally, a sophisticated data processing pipeline has been developed to\nreconstruct a large-scale and high-quality dataset to support our training. Our\nframework demonstrates significant improvements in 2D-to-3D video conversion,\noffering a practical solution for creating immersive content for 3D devices\nlike Apple Vision Pro and 3D displays. In summary, this work contributes to the\nfield by presenting an effective method for generating high-quality\nstereoscopic videos from monocular input, potentially transforming how we\nexperience digital media.\n", "link": "http://arxiv.org/abs/2409.07447v1", "date": "2024-09-11", "relevancy": 3.2366, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6582}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6582}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StereoCrafter%3A%20Diffusion-based%20Generation%20of%20Long%20and%20High-fidelity%0A%20%20Stereoscopic%203D%20from%20Monocular%20Videos&body=Title%3A%20StereoCrafter%3A%20Diffusion-based%20Generation%20of%20Long%20and%20High-fidelity%0A%20%20Stereoscopic%203D%20from%20Monocular%20Videos%0AAuthor%3A%20Sijie%20Zhao%20and%20Wenbo%20Hu%20and%20Xiaodong%20Cun%20and%20Yong%20Zhang%20and%20Xiaoyu%20Li%20and%20Zhe%20Kong%20and%20Xiangjun%20Gao%20and%20Muyao%20Niu%20and%20Ying%20Shan%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20framework%20for%20converting%202D%20videos%20to%20immersive%0Astereoscopic%203D%2C%20addressing%20the%20growing%20demand%20for%203D%20content%20in%20immersive%0Aexperience.%20Leveraging%20foundation%20models%20as%20priors%2C%20our%20approach%20overcomes%20the%0Alimitations%20of%20traditional%20methods%20and%20boosts%20the%20performance%20to%20ensure%20the%0Ahigh-fidelity%20generation%20required%20by%20the%20display%20devices.%20The%20proposed%20system%0Aconsists%20of%20two%20main%20steps%3A%20depth-based%20video%20splatting%20for%20warping%20and%0Aextracting%20occlusion%20mask%2C%20and%20stereo%20video%20inpainting.%20We%20utilize%20pre-trained%0Astable%20video%20diffusion%20as%20the%20backbone%20and%20introduce%20a%20fine-tuning%20protocol%20for%0Athe%20stereo%20video%20inpainting%20task.%20To%20handle%20input%20video%20with%20varying%20lengths%0Aand%20resolutions%2C%20we%20explore%20auto-regressive%20strategies%20and%20tiled%20processing.%0AFinally%2C%20a%20sophisticated%20data%20processing%20pipeline%20has%20been%20developed%20to%0Areconstruct%20a%20large-scale%20and%20high-quality%20dataset%20to%20support%20our%20training.%20Our%0Aframework%20demonstrates%20significant%20improvements%20in%202D-to-3D%20video%20conversion%2C%0Aoffering%20a%20practical%20solution%20for%20creating%20immersive%20content%20for%203D%20devices%0Alike%20Apple%20Vision%20Pro%20and%203D%20displays.%20In%20summary%2C%20this%20work%20contributes%20to%20the%0Afield%20by%20presenting%20an%20effective%20method%20for%20generating%20high-quality%0Astereoscopic%20videos%20from%20monocular%20input%2C%20potentially%20transforming%20how%20we%0Aexperience%20digital%20media.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07447v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStereoCrafter%253A%2520Diffusion-based%2520Generation%2520of%2520Long%2520and%2520High-fidelity%250A%2520%2520Stereoscopic%25203D%2520from%2520Monocular%2520Videos%26entry.906535625%3DSijie%2520Zhao%2520and%2520Wenbo%2520Hu%2520and%2520Xiaodong%2520Cun%2520and%2520Yong%2520Zhang%2520and%2520Xiaoyu%2520Li%2520and%2520Zhe%2520Kong%2520and%2520Xiangjun%2520Gao%2520and%2520Muyao%2520Niu%2520and%2520Ying%2520Shan%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520framework%2520for%2520converting%25202D%2520videos%2520to%2520immersive%250Astereoscopic%25203D%252C%2520addressing%2520the%2520growing%2520demand%2520for%25203D%2520content%2520in%2520immersive%250Aexperience.%2520Leveraging%2520foundation%2520models%2520as%2520priors%252C%2520our%2520approach%2520overcomes%2520the%250Alimitations%2520of%2520traditional%2520methods%2520and%2520boosts%2520the%2520performance%2520to%2520ensure%2520the%250Ahigh-fidelity%2520generation%2520required%2520by%2520the%2520display%2520devices.%2520The%2520proposed%2520system%250Aconsists%2520of%2520two%2520main%2520steps%253A%2520depth-based%2520video%2520splatting%2520for%2520warping%2520and%250Aextracting%2520occlusion%2520mask%252C%2520and%2520stereo%2520video%2520inpainting.%2520We%2520utilize%2520pre-trained%250Astable%2520video%2520diffusion%2520as%2520the%2520backbone%2520and%2520introduce%2520a%2520fine-tuning%2520protocol%2520for%250Athe%2520stereo%2520video%2520inpainting%2520task.%2520To%2520handle%2520input%2520video%2520with%2520varying%2520lengths%250Aand%2520resolutions%252C%2520we%2520explore%2520auto-regressive%2520strategies%2520and%2520tiled%2520processing.%250AFinally%252C%2520a%2520sophisticated%2520data%2520processing%2520pipeline%2520has%2520been%2520developed%2520to%250Areconstruct%2520a%2520large-scale%2520and%2520high-quality%2520dataset%2520to%2520support%2520our%2520training.%2520Our%250Aframework%2520demonstrates%2520significant%2520improvements%2520in%25202D-to-3D%2520video%2520conversion%252C%250Aoffering%2520a%2520practical%2520solution%2520for%2520creating%2520immersive%2520content%2520for%25203D%2520devices%250Alike%2520Apple%2520Vision%2520Pro%2520and%25203D%2520displays.%2520In%2520summary%252C%2520this%2520work%2520contributes%2520to%2520the%250Afield%2520by%2520presenting%2520an%2520effective%2520method%2520for%2520generating%2520high-quality%250Astereoscopic%2520videos%2520from%2520monocular%2520input%252C%2520potentially%2520transforming%2520how%2520we%250Aexperience%2520digital%2520media.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07447v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StereoCrafter%3A%20Diffusion-based%20Generation%20of%20Long%20and%20High-fidelity%0A%20%20Stereoscopic%203D%20from%20Monocular%20Videos&entry.906535625=Sijie%20Zhao%20and%20Wenbo%20Hu%20and%20Xiaodong%20Cun%20and%20Yong%20Zhang%20and%20Xiaoyu%20Li%20and%20Zhe%20Kong%20and%20Xiangjun%20Gao%20and%20Muyao%20Niu%20and%20Ying%20Shan&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20framework%20for%20converting%202D%20videos%20to%20immersive%0Astereoscopic%203D%2C%20addressing%20the%20growing%20demand%20for%203D%20content%20in%20immersive%0Aexperience.%20Leveraging%20foundation%20models%20as%20priors%2C%20our%20approach%20overcomes%20the%0Alimitations%20of%20traditional%20methods%20and%20boosts%20the%20performance%20to%20ensure%20the%0Ahigh-fidelity%20generation%20required%20by%20the%20display%20devices.%20The%20proposed%20system%0Aconsists%20of%20two%20main%20steps%3A%20depth-based%20video%20splatting%20for%20warping%20and%0Aextracting%20occlusion%20mask%2C%20and%20stereo%20video%20inpainting.%20We%20utilize%20pre-trained%0Astable%20video%20diffusion%20as%20the%20backbone%20and%20introduce%20a%20fine-tuning%20protocol%20for%0Athe%20stereo%20video%20inpainting%20task.%20To%20handle%20input%20video%20with%20varying%20lengths%0Aand%20resolutions%2C%20we%20explore%20auto-regressive%20strategies%20and%20tiled%20processing.%0AFinally%2C%20a%20sophisticated%20data%20processing%20pipeline%20has%20been%20developed%20to%0Areconstruct%20a%20large-scale%20and%20high-quality%20dataset%20to%20support%20our%20training.%20Our%0Aframework%20demonstrates%20significant%20improvements%20in%202D-to-3D%20video%20conversion%2C%0Aoffering%20a%20practical%20solution%20for%20creating%20immersive%20content%20for%203D%20devices%0Alike%20Apple%20Vision%20Pro%20and%203D%20displays.%20In%20summary%2C%20this%20work%20contributes%20to%20the%0Afield%20by%20presenting%20an%20effective%20method%20for%20generating%20high-quality%0Astereoscopic%20videos%20from%20monocular%20input%2C%20potentially%20transforming%20how%20we%0Aexperience%20digital%20media.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07447v1&entry.124074799=Read"},
{"title": "Self-Evolving Depth-Supervised 3D Gaussian Splatting from Rendered\n  Stereo Pairs", "author": "Sadra Safadoust and Fabio Tosi and Fatma G\u00fcney and Matteo Poggi", "abstract": "  3D Gaussian Splatting (GS) significantly struggles to accurately represent\nthe underlying 3D scene geometry, resulting in inaccuracies and floating\nartifacts when rendering depth maps. In this paper, we address this limitation,\nundertaking a comprehensive analysis of the integration of depth priors\nthroughout the optimization process of Gaussian primitives, and present a novel\nstrategy for this purpose. This latter dynamically exploits depth cues from a\nreadily available stereo network, processing virtual stereo pairs rendered by\nthe GS model itself during training and achieving consistent self-improvement\nof the scene representation. Experimental results on three popular datasets,\nbreaking ground as the first to assess depth accuracy for these models,\nvalidate our findings.\n", "link": "http://arxiv.org/abs/2409.07456v1", "date": "2024-09-11", "relevancy": 3.1566, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6912}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6341}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Evolving%20Depth-Supervised%203D%20Gaussian%20Splatting%20from%20Rendered%0A%20%20Stereo%20Pairs&body=Title%3A%20Self-Evolving%20Depth-Supervised%203D%20Gaussian%20Splatting%20from%20Rendered%0A%20%20Stereo%20Pairs%0AAuthor%3A%20Sadra%20Safadoust%20and%20Fabio%20Tosi%20and%20Fatma%20G%C3%BCney%20and%20Matteo%20Poggi%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%28GS%29%20significantly%20struggles%20to%20accurately%20represent%0Athe%20underlying%203D%20scene%20geometry%2C%20resulting%20in%20inaccuracies%20and%20floating%0Aartifacts%20when%20rendering%20depth%20maps.%20In%20this%20paper%2C%20we%20address%20this%20limitation%2C%0Aundertaking%20a%20comprehensive%20analysis%20of%20the%20integration%20of%20depth%20priors%0Athroughout%20the%20optimization%20process%20of%20Gaussian%20primitives%2C%20and%20present%20a%20novel%0Astrategy%20for%20this%20purpose.%20This%20latter%20dynamically%20exploits%20depth%20cues%20from%20a%0Areadily%20available%20stereo%20network%2C%20processing%20virtual%20stereo%20pairs%20rendered%20by%0Athe%20GS%20model%20itself%20during%20training%20and%20achieving%20consistent%20self-improvement%0Aof%20the%20scene%20representation.%20Experimental%20results%20on%20three%20popular%20datasets%2C%0Abreaking%20ground%20as%20the%20first%20to%20assess%20depth%20accuracy%20for%20these%20models%2C%0Avalidate%20our%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07456v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Evolving%2520Depth-Supervised%25203D%2520Gaussian%2520Splatting%2520from%2520Rendered%250A%2520%2520Stereo%2520Pairs%26entry.906535625%3DSadra%2520Safadoust%2520and%2520Fabio%2520Tosi%2520and%2520Fatma%2520G%25C3%25BCney%2520and%2520Matteo%2520Poggi%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%2528GS%2529%2520significantly%2520struggles%2520to%2520accurately%2520represent%250Athe%2520underlying%25203D%2520scene%2520geometry%252C%2520resulting%2520in%2520inaccuracies%2520and%2520floating%250Aartifacts%2520when%2520rendering%2520depth%2520maps.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%2520limitation%252C%250Aundertaking%2520a%2520comprehensive%2520analysis%2520of%2520the%2520integration%2520of%2520depth%2520priors%250Athroughout%2520the%2520optimization%2520process%2520of%2520Gaussian%2520primitives%252C%2520and%2520present%2520a%2520novel%250Astrategy%2520for%2520this%2520purpose.%2520This%2520latter%2520dynamically%2520exploits%2520depth%2520cues%2520from%2520a%250Areadily%2520available%2520stereo%2520network%252C%2520processing%2520virtual%2520stereo%2520pairs%2520rendered%2520by%250Athe%2520GS%2520model%2520itself%2520during%2520training%2520and%2520achieving%2520consistent%2520self-improvement%250Aof%2520the%2520scene%2520representation.%2520Experimental%2520results%2520on%2520three%2520popular%2520datasets%252C%250Abreaking%2520ground%2520as%2520the%2520first%2520to%2520assess%2520depth%2520accuracy%2520for%2520these%2520models%252C%250Avalidate%2520our%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07456v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Evolving%20Depth-Supervised%203D%20Gaussian%20Splatting%20from%20Rendered%0A%20%20Stereo%20Pairs&entry.906535625=Sadra%20Safadoust%20and%20Fabio%20Tosi%20and%20Fatma%20G%C3%BCney%20and%20Matteo%20Poggi&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%28GS%29%20significantly%20struggles%20to%20accurately%20represent%0Athe%20underlying%203D%20scene%20geometry%2C%20resulting%20in%20inaccuracies%20and%20floating%0Aartifacts%20when%20rendering%20depth%20maps.%20In%20this%20paper%2C%20we%20address%20this%20limitation%2C%0Aundertaking%20a%20comprehensive%20analysis%20of%20the%20integration%20of%20depth%20priors%0Athroughout%20the%20optimization%20process%20of%20Gaussian%20primitives%2C%20and%20present%20a%20novel%0Astrategy%20for%20this%20purpose.%20This%20latter%20dynamically%20exploits%20depth%20cues%20from%20a%0Areadily%20available%20stereo%20network%2C%20processing%20virtual%20stereo%20pairs%20rendered%20by%0Athe%20GS%20model%20itself%20during%20training%20and%20achieving%20consistent%20self-improvement%0Aof%20the%20scene%20representation.%20Experimental%20results%20on%20three%20popular%20datasets%2C%0Abreaking%20ground%20as%20the%20first%20to%20assess%20depth%20accuracy%20for%20these%20models%2C%0Avalidate%20our%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07456v1&entry.124074799=Read"},
{"title": "ThermalGaussian: Thermal 3D Gaussian Splatting", "author": "Rongfeng Lu and Hangyu Chen and Zunjie Zhu and Yuhang Qin and Ming Lu and Le Zhang and Chenggang Yan and Anke Xue", "abstract": "  Thermography is especially valuable for the military and other users of\nsurveillance cameras. Some recent methods based on Neural Radiance Fields\n(NeRF) are proposed to reconstruct the thermal scenes in 3D from a set of\nthermal and RGB images. However, unlike NeRF, 3D Gaussian splatting (3DGS)\nprevails due to its rapid training and real-time rendering. In this work, we\npropose ThermalGaussian, the first thermal 3DGS approach capable of rendering\nhigh-quality images in RGB and thermal modalities. We first calibrate the RGB\ncamera and the thermal camera to ensure that both modalities are accurately\naligned. Subsequently, we use the registered images to learn the multimodal 3D\nGaussians. To prevent the overfitting of any single modality, we introduce\nseveral multimodal regularization constraints. We also develop smoothing\nconstraints tailored to the physical characteristics of the thermal modality.\nBesides, we contribute a real-world dataset named RGBT-Scenes, captured by a\nhand-hold thermal-infrared camera, facilitating future research on thermal\nscene reconstruction. We conduct comprehensive experiments to show that\nThermalGaussian achieves photorealistic rendering of thermal images and\nimproves the rendering quality of RGB images. With the proposed multimodal\nregularization constraints, we also reduced the model's storage cost by 90\\%.\nThe code and dataset will be released.\n", "link": "http://arxiv.org/abs/2409.07200v1", "date": "2024-09-11", "relevancy": 3.1432, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7164}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6193}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ThermalGaussian%3A%20Thermal%203D%20Gaussian%20Splatting&body=Title%3A%20ThermalGaussian%3A%20Thermal%203D%20Gaussian%20Splatting%0AAuthor%3A%20Rongfeng%20Lu%20and%20Hangyu%20Chen%20and%20Zunjie%20Zhu%20and%20Yuhang%20Qin%20and%20Ming%20Lu%20and%20Le%20Zhang%20and%20Chenggang%20Yan%20and%20Anke%20Xue%0AAbstract%3A%20%20%20Thermography%20is%20especially%20valuable%20for%20the%20military%20and%20other%20users%20of%0Asurveillance%20cameras.%20Some%20recent%20methods%20based%20on%20Neural%20Radiance%20Fields%0A%28NeRF%29%20are%20proposed%20to%20reconstruct%20the%20thermal%20scenes%20in%203D%20from%20a%20set%20of%0Athermal%20and%20RGB%20images.%20However%2C%20unlike%20NeRF%2C%203D%20Gaussian%20splatting%20%283DGS%29%0Aprevails%20due%20to%20its%20rapid%20training%20and%20real-time%20rendering.%20In%20this%20work%2C%20we%0Apropose%20ThermalGaussian%2C%20the%20first%20thermal%203DGS%20approach%20capable%20of%20rendering%0Ahigh-quality%20images%20in%20RGB%20and%20thermal%20modalities.%20We%20first%20calibrate%20the%20RGB%0Acamera%20and%20the%20thermal%20camera%20to%20ensure%20that%20both%20modalities%20are%20accurately%0Aaligned.%20Subsequently%2C%20we%20use%20the%20registered%20images%20to%20learn%20the%20multimodal%203D%0AGaussians.%20To%20prevent%20the%20overfitting%20of%20any%20single%20modality%2C%20we%20introduce%0Aseveral%20multimodal%20regularization%20constraints.%20We%20also%20develop%20smoothing%0Aconstraints%20tailored%20to%20the%20physical%20characteristics%20of%20the%20thermal%20modality.%0ABesides%2C%20we%20contribute%20a%20real-world%20dataset%20named%20RGBT-Scenes%2C%20captured%20by%20a%0Ahand-hold%20thermal-infrared%20camera%2C%20facilitating%20future%20research%20on%20thermal%0Ascene%20reconstruction.%20We%20conduct%20comprehensive%20experiments%20to%20show%20that%0AThermalGaussian%20achieves%20photorealistic%20rendering%20of%20thermal%20images%20and%0Aimproves%20the%20rendering%20quality%20of%20RGB%20images.%20With%20the%20proposed%20multimodal%0Aregularization%20constraints%2C%20we%20also%20reduced%20the%20model%27s%20storage%20cost%20by%2090%5C%25.%0AThe%20code%20and%20dataset%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07200v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThermalGaussian%253A%2520Thermal%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DRongfeng%2520Lu%2520and%2520Hangyu%2520Chen%2520and%2520Zunjie%2520Zhu%2520and%2520Yuhang%2520Qin%2520and%2520Ming%2520Lu%2520and%2520Le%2520Zhang%2520and%2520Chenggang%2520Yan%2520and%2520Anke%2520Xue%26entry.1292438233%3D%2520%2520Thermography%2520is%2520especially%2520valuable%2520for%2520the%2520military%2520and%2520other%2520users%2520of%250Asurveillance%2520cameras.%2520Some%2520recent%2520methods%2520based%2520on%2520Neural%2520Radiance%2520Fields%250A%2528NeRF%2529%2520are%2520proposed%2520to%2520reconstruct%2520the%2520thermal%2520scenes%2520in%25203D%2520from%2520a%2520set%2520of%250Athermal%2520and%2520RGB%2520images.%2520However%252C%2520unlike%2520NeRF%252C%25203D%2520Gaussian%2520splatting%2520%25283DGS%2529%250Aprevails%2520due%2520to%2520its%2520rapid%2520training%2520and%2520real-time%2520rendering.%2520In%2520this%2520work%252C%2520we%250Apropose%2520ThermalGaussian%252C%2520the%2520first%2520thermal%25203DGS%2520approach%2520capable%2520of%2520rendering%250Ahigh-quality%2520images%2520in%2520RGB%2520and%2520thermal%2520modalities.%2520We%2520first%2520calibrate%2520the%2520RGB%250Acamera%2520and%2520the%2520thermal%2520camera%2520to%2520ensure%2520that%2520both%2520modalities%2520are%2520accurately%250Aaligned.%2520Subsequently%252C%2520we%2520use%2520the%2520registered%2520images%2520to%2520learn%2520the%2520multimodal%25203D%250AGaussians.%2520To%2520prevent%2520the%2520overfitting%2520of%2520any%2520single%2520modality%252C%2520we%2520introduce%250Aseveral%2520multimodal%2520regularization%2520constraints.%2520We%2520also%2520develop%2520smoothing%250Aconstraints%2520tailored%2520to%2520the%2520physical%2520characteristics%2520of%2520the%2520thermal%2520modality.%250ABesides%252C%2520we%2520contribute%2520a%2520real-world%2520dataset%2520named%2520RGBT-Scenes%252C%2520captured%2520by%2520a%250Ahand-hold%2520thermal-infrared%2520camera%252C%2520facilitating%2520future%2520research%2520on%2520thermal%250Ascene%2520reconstruction.%2520We%2520conduct%2520comprehensive%2520experiments%2520to%2520show%2520that%250AThermalGaussian%2520achieves%2520photorealistic%2520rendering%2520of%2520thermal%2520images%2520and%250Aimproves%2520the%2520rendering%2520quality%2520of%2520RGB%2520images.%2520With%2520the%2520proposed%2520multimodal%250Aregularization%2520constraints%252C%2520we%2520also%2520reduced%2520the%2520model%2527s%2520storage%2520cost%2520by%252090%255C%2525.%250AThe%2520code%2520and%2520dataset%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07200v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ThermalGaussian%3A%20Thermal%203D%20Gaussian%20Splatting&entry.906535625=Rongfeng%20Lu%20and%20Hangyu%20Chen%20and%20Zunjie%20Zhu%20and%20Yuhang%20Qin%20and%20Ming%20Lu%20and%20Le%20Zhang%20and%20Chenggang%20Yan%20and%20Anke%20Xue&entry.1292438233=%20%20Thermography%20is%20especially%20valuable%20for%20the%20military%20and%20other%20users%20of%0Asurveillance%20cameras.%20Some%20recent%20methods%20based%20on%20Neural%20Radiance%20Fields%0A%28NeRF%29%20are%20proposed%20to%20reconstruct%20the%20thermal%20scenes%20in%203D%20from%20a%20set%20of%0Athermal%20and%20RGB%20images.%20However%2C%20unlike%20NeRF%2C%203D%20Gaussian%20splatting%20%283DGS%29%0Aprevails%20due%20to%20its%20rapid%20training%20and%20real-time%20rendering.%20In%20this%20work%2C%20we%0Apropose%20ThermalGaussian%2C%20the%20first%20thermal%203DGS%20approach%20capable%20of%20rendering%0Ahigh-quality%20images%20in%20RGB%20and%20thermal%20modalities.%20We%20first%20calibrate%20the%20RGB%0Acamera%20and%20the%20thermal%20camera%20to%20ensure%20that%20both%20modalities%20are%20accurately%0Aaligned.%20Subsequently%2C%20we%20use%20the%20registered%20images%20to%20learn%20the%20multimodal%203D%0AGaussians.%20To%20prevent%20the%20overfitting%20of%20any%20single%20modality%2C%20we%20introduce%0Aseveral%20multimodal%20regularization%20constraints.%20We%20also%20develop%20smoothing%0Aconstraints%20tailored%20to%20the%20physical%20characteristics%20of%20the%20thermal%20modality.%0ABesides%2C%20we%20contribute%20a%20real-world%20dataset%20named%20RGBT-Scenes%2C%20captured%20by%20a%0Ahand-hold%20thermal-infrared%20camera%2C%20facilitating%20future%20research%20on%20thermal%0Ascene%20reconstruction.%20We%20conduct%20comprehensive%20experiments%20to%20show%20that%0AThermalGaussian%20achieves%20photorealistic%20rendering%20of%20thermal%20images%20and%0Aimproves%20the%20rendering%20quality%20of%20RGB%20images.%20With%20the%20proposed%20multimodal%0Aregularization%20constraints%2C%20we%20also%20reduced%20the%20model%27s%20storage%20cost%20by%2090%5C%25.%0AThe%20code%20and%20dataset%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07200v1&entry.124074799=Read"},
{"title": "DriveScape: Towards High-Resolution Controllable Multi-View Driving\n  Video Generation", "author": "Wei Wu and Xi Guo and Weixuan Tang and Tingxuan Huang and Chiyu Wang and Dongyue Chen and Chenjing Ding", "abstract": "  Recent advancements in generative models have provided promising solutions\nfor synthesizing realistic driving videos, which are crucial for training\nautonomous driving perception models. However, existing approaches often\nstruggle with multi-view video generation due to the challenges of integrating\n3D information while maintaining spatial-temporal consistency and effectively\nlearning from a unified model. In this paper, we propose an end-to-end\nframework named DriveScape for multi-view, 3D condition-guided video\ngeneration. DriveScape not only streamlines the process by integrating camera\ndata to ensure comprehensive spatial-temporal coverage, but also introduces a\nBi-Directional Modulated Transformer module to effectively align 3D road\nstructural information. As a result, our approach enables precise control over\nvideo generation, significantly enhancing realism and providing a robust\nsolution for generating multi-view driving videos. Our framework achieves\nstate-of-the-art results on the nuScenes dataset, demonstrating impressive\ngenerative quality metrics with an FID score of 8.34 and an FVD score of 76.39,\nas well as superior performance across various perception tasks. This paves the\nway for more accurate environmental simulations in autonomous driving. Our\nproject homepage:\nhttps://metadrivescape.github.io/papers_project/drivescapev1/index.html\n", "link": "http://arxiv.org/abs/2409.05463v3", "date": "2024-09-11", "relevancy": 3.1011, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6342}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6342}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DriveScape%3A%20Towards%20High-Resolution%20Controllable%20Multi-View%20Driving%0A%20%20Video%20Generation&body=Title%3A%20DriveScape%3A%20Towards%20High-Resolution%20Controllable%20Multi-View%20Driving%0A%20%20Video%20Generation%0AAuthor%3A%20Wei%20Wu%20and%20Xi%20Guo%20and%20Weixuan%20Tang%20and%20Tingxuan%20Huang%20and%20Chiyu%20Wang%20and%20Dongyue%20Chen%20and%20Chenjing%20Ding%0AAbstract%3A%20%20%20Recent%20advancements%20in%20generative%20models%20have%20provided%20promising%20solutions%0Afor%20synthesizing%20realistic%20driving%20videos%2C%20which%20are%20crucial%20for%20training%0Aautonomous%20driving%20perception%20models.%20However%2C%20existing%20approaches%20often%0Astruggle%20with%20multi-view%20video%20generation%20due%20to%20the%20challenges%20of%20integrating%0A3D%20information%20while%20maintaining%20spatial-temporal%20consistency%20and%20effectively%0Alearning%20from%20a%20unified%20model.%20In%20this%20paper%2C%20we%20propose%20an%20end-to-end%0Aframework%20named%20DriveScape%20for%20multi-view%2C%203D%20condition-guided%20video%0Ageneration.%20DriveScape%20not%20only%20streamlines%20the%20process%20by%20integrating%20camera%0Adata%20to%20ensure%20comprehensive%20spatial-temporal%20coverage%2C%20but%20also%20introduces%20a%0ABi-Directional%20Modulated%20Transformer%20module%20to%20effectively%20align%203D%20road%0Astructural%20information.%20As%20a%20result%2C%20our%20approach%20enables%20precise%20control%20over%0Avideo%20generation%2C%20significantly%20enhancing%20realism%20and%20providing%20a%20robust%0Asolution%20for%20generating%20multi-view%20driving%20videos.%20Our%20framework%20achieves%0Astate-of-the-art%20results%20on%20the%20nuScenes%20dataset%2C%20demonstrating%20impressive%0Agenerative%20quality%20metrics%20with%20an%20FID%20score%20of%208.34%20and%20an%20FVD%20score%20of%2076.39%2C%0Aas%20well%20as%20superior%20performance%20across%20various%20perception%20tasks.%20This%20paves%20the%0Away%20for%20more%20accurate%20environmental%20simulations%20in%20autonomous%20driving.%20Our%0Aproject%20homepage%3A%0Ahttps%3A//metadrivescape.github.io/papers_project/drivescapev1/index.html%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05463v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDriveScape%253A%2520Towards%2520High-Resolution%2520Controllable%2520Multi-View%2520Driving%250A%2520%2520Video%2520Generation%26entry.906535625%3DWei%2520Wu%2520and%2520Xi%2520Guo%2520and%2520Weixuan%2520Tang%2520and%2520Tingxuan%2520Huang%2520and%2520Chiyu%2520Wang%2520and%2520Dongyue%2520Chen%2520and%2520Chenjing%2520Ding%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520generative%2520models%2520have%2520provided%2520promising%2520solutions%250Afor%2520synthesizing%2520realistic%2520driving%2520videos%252C%2520which%2520are%2520crucial%2520for%2520training%250Aautonomous%2520driving%2520perception%2520models.%2520However%252C%2520existing%2520approaches%2520often%250Astruggle%2520with%2520multi-view%2520video%2520generation%2520due%2520to%2520the%2520challenges%2520of%2520integrating%250A3D%2520information%2520while%2520maintaining%2520spatial-temporal%2520consistency%2520and%2520effectively%250Alearning%2520from%2520a%2520unified%2520model.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520end-to-end%250Aframework%2520named%2520DriveScape%2520for%2520multi-view%252C%25203D%2520condition-guided%2520video%250Ageneration.%2520DriveScape%2520not%2520only%2520streamlines%2520the%2520process%2520by%2520integrating%2520camera%250Adata%2520to%2520ensure%2520comprehensive%2520spatial-temporal%2520coverage%252C%2520but%2520also%2520introduces%2520a%250ABi-Directional%2520Modulated%2520Transformer%2520module%2520to%2520effectively%2520align%25203D%2520road%250Astructural%2520information.%2520As%2520a%2520result%252C%2520our%2520approach%2520enables%2520precise%2520control%2520over%250Avideo%2520generation%252C%2520significantly%2520enhancing%2520realism%2520and%2520providing%2520a%2520robust%250Asolution%2520for%2520generating%2520multi-view%2520driving%2520videos.%2520Our%2520framework%2520achieves%250Astate-of-the-art%2520results%2520on%2520the%2520nuScenes%2520dataset%252C%2520demonstrating%2520impressive%250Agenerative%2520quality%2520metrics%2520with%2520an%2520FID%2520score%2520of%25208.34%2520and%2520an%2520FVD%2520score%2520of%252076.39%252C%250Aas%2520well%2520as%2520superior%2520performance%2520across%2520various%2520perception%2520tasks.%2520This%2520paves%2520the%250Away%2520for%2520more%2520accurate%2520environmental%2520simulations%2520in%2520autonomous%2520driving.%2520Our%250Aproject%2520homepage%253A%250Ahttps%253A//metadrivescape.github.io/papers_project/drivescapev1/index.html%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05463v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DriveScape%3A%20Towards%20High-Resolution%20Controllable%20Multi-View%20Driving%0A%20%20Video%20Generation&entry.906535625=Wei%20Wu%20and%20Xi%20Guo%20and%20Weixuan%20Tang%20and%20Tingxuan%20Huang%20and%20Chiyu%20Wang%20and%20Dongyue%20Chen%20and%20Chenjing%20Ding&entry.1292438233=%20%20Recent%20advancements%20in%20generative%20models%20have%20provided%20promising%20solutions%0Afor%20synthesizing%20realistic%20driving%20videos%2C%20which%20are%20crucial%20for%20training%0Aautonomous%20driving%20perception%20models.%20However%2C%20existing%20approaches%20often%0Astruggle%20with%20multi-view%20video%20generation%20due%20to%20the%20challenges%20of%20integrating%0A3D%20information%20while%20maintaining%20spatial-temporal%20consistency%20and%20effectively%0Alearning%20from%20a%20unified%20model.%20In%20this%20paper%2C%20we%20propose%20an%20end-to-end%0Aframework%20named%20DriveScape%20for%20multi-view%2C%203D%20condition-guided%20video%0Ageneration.%20DriveScape%20not%20only%20streamlines%20the%20process%20by%20integrating%20camera%0Adata%20to%20ensure%20comprehensive%20spatial-temporal%20coverage%2C%20but%20also%20introduces%20a%0ABi-Directional%20Modulated%20Transformer%20module%20to%20effectively%20align%203D%20road%0Astructural%20information.%20As%20a%20result%2C%20our%20approach%20enables%20precise%20control%20over%0Avideo%20generation%2C%20significantly%20enhancing%20realism%20and%20providing%20a%20robust%0Asolution%20for%20generating%20multi-view%20driving%20videos.%20Our%20framework%20achieves%0Astate-of-the-art%20results%20on%20the%20nuScenes%20dataset%2C%20demonstrating%20impressive%0Agenerative%20quality%20metrics%20with%20an%20FID%20score%20of%208.34%20and%20an%20FVD%20score%20of%2076.39%2C%0Aas%20well%20as%20superior%20performance%20across%20various%20perception%20tasks.%20This%20paves%20the%0Away%20for%20more%20accurate%20environmental%20simulations%20in%20autonomous%20driving.%20Our%0Aproject%20homepage%3A%0Ahttps%3A//metadrivescape.github.io/papers_project/drivescapev1/index.html%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05463v3&entry.124074799=Read"},
{"title": "EmoVOCA: Speech-Driven Emotional 3D Talking Heads", "author": "Federico Nocentini and Claudio Ferrari and Stefano Berretti", "abstract": "  The domain of 3D talking head generation has witnessed significant progress\nin recent years. A notable challenge in this field consists in blending\nspeech-related motions with expression dynamics, which is primarily caused by\nthe lack of comprehensive 3D datasets that combine diversity in spoken\nsentences with a variety of facial expressions. Whereas literature works\nattempted to exploit 2D video data and parametric 3D models as a workaround,\nthese still show limitations when jointly modeling the two motions. In this\nwork, we address this problem from a different perspective, and propose an\ninnovative data-driven technique that we used for creating a synthetic dataset,\ncalled EmoVOCA, obtained by combining a collection of inexpressive 3D talking\nheads and a set of 3D expressive sequences. To demonstrate the advantages of\nthis approach, and the quality of the dataset, we then designed and trained an\nemotional 3D talking head generator that accepts a 3D face, an audio file, an\nemotion label, and an intensity value as inputs, and learns to animate the\naudio-synchronized lip movements with expressive traits of the face.\nComprehensive experiments, both quantitative and qualitative, using our data\nand generator evidence superior ability in synthesizing convincing animations,\nwhen compared with the best performing methods in the literature. Our code and\npre-trained model will be made available.\n", "link": "http://arxiv.org/abs/2403.12886v2", "date": "2024-09-11", "relevancy": 3.0351, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6314}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6314}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EmoVOCA%3A%20Speech-Driven%20Emotional%203D%20Talking%20Heads&body=Title%3A%20EmoVOCA%3A%20Speech-Driven%20Emotional%203D%20Talking%20Heads%0AAuthor%3A%20Federico%20Nocentini%20and%20Claudio%20Ferrari%20and%20Stefano%20Berretti%0AAbstract%3A%20%20%20The%20domain%20of%203D%20talking%20head%20generation%20has%20witnessed%20significant%20progress%0Ain%20recent%20years.%20A%20notable%20challenge%20in%20this%20field%20consists%20in%20blending%0Aspeech-related%20motions%20with%20expression%20dynamics%2C%20which%20is%20primarily%20caused%20by%0Athe%20lack%20of%20comprehensive%203D%20datasets%20that%20combine%20diversity%20in%20spoken%0Asentences%20with%20a%20variety%20of%20facial%20expressions.%20Whereas%20literature%20works%0Aattempted%20to%20exploit%202D%20video%20data%20and%20parametric%203D%20models%20as%20a%20workaround%2C%0Athese%20still%20show%20limitations%20when%20jointly%20modeling%20the%20two%20motions.%20In%20this%0Awork%2C%20we%20address%20this%20problem%20from%20a%20different%20perspective%2C%20and%20propose%20an%0Ainnovative%20data-driven%20technique%20that%20we%20used%20for%20creating%20a%20synthetic%20dataset%2C%0Acalled%20EmoVOCA%2C%20obtained%20by%20combining%20a%20collection%20of%20inexpressive%203D%20talking%0Aheads%20and%20a%20set%20of%203D%20expressive%20sequences.%20To%20demonstrate%20the%20advantages%20of%0Athis%20approach%2C%20and%20the%20quality%20of%20the%20dataset%2C%20we%20then%20designed%20and%20trained%20an%0Aemotional%203D%20talking%20head%20generator%20that%20accepts%20a%203D%20face%2C%20an%20audio%20file%2C%20an%0Aemotion%20label%2C%20and%20an%20intensity%20value%20as%20inputs%2C%20and%20learns%20to%20animate%20the%0Aaudio-synchronized%20lip%20movements%20with%20expressive%20traits%20of%20the%20face.%0AComprehensive%20experiments%2C%20both%20quantitative%20and%20qualitative%2C%20using%20our%20data%0Aand%20generator%20evidence%20superior%20ability%20in%20synthesizing%20convincing%20animations%2C%0Awhen%20compared%20with%20the%20best%20performing%20methods%20in%20the%20literature.%20Our%20code%20and%0Apre-trained%20model%20will%20be%20made%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12886v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmoVOCA%253A%2520Speech-Driven%2520Emotional%25203D%2520Talking%2520Heads%26entry.906535625%3DFederico%2520Nocentini%2520and%2520Claudio%2520Ferrari%2520and%2520Stefano%2520Berretti%26entry.1292438233%3D%2520%2520The%2520domain%2520of%25203D%2520talking%2520head%2520generation%2520has%2520witnessed%2520significant%2520progress%250Ain%2520recent%2520years.%2520A%2520notable%2520challenge%2520in%2520this%2520field%2520consists%2520in%2520blending%250Aspeech-related%2520motions%2520with%2520expression%2520dynamics%252C%2520which%2520is%2520primarily%2520caused%2520by%250Athe%2520lack%2520of%2520comprehensive%25203D%2520datasets%2520that%2520combine%2520diversity%2520in%2520spoken%250Asentences%2520with%2520a%2520variety%2520of%2520facial%2520expressions.%2520Whereas%2520literature%2520works%250Aattempted%2520to%2520exploit%25202D%2520video%2520data%2520and%2520parametric%25203D%2520models%2520as%2520a%2520workaround%252C%250Athese%2520still%2520show%2520limitations%2520when%2520jointly%2520modeling%2520the%2520two%2520motions.%2520In%2520this%250Awork%252C%2520we%2520address%2520this%2520problem%2520from%2520a%2520different%2520perspective%252C%2520and%2520propose%2520an%250Ainnovative%2520data-driven%2520technique%2520that%2520we%2520used%2520for%2520creating%2520a%2520synthetic%2520dataset%252C%250Acalled%2520EmoVOCA%252C%2520obtained%2520by%2520combining%2520a%2520collection%2520of%2520inexpressive%25203D%2520talking%250Aheads%2520and%2520a%2520set%2520of%25203D%2520expressive%2520sequences.%2520To%2520demonstrate%2520the%2520advantages%2520of%250Athis%2520approach%252C%2520and%2520the%2520quality%2520of%2520the%2520dataset%252C%2520we%2520then%2520designed%2520and%2520trained%2520an%250Aemotional%25203D%2520talking%2520head%2520generator%2520that%2520accepts%2520a%25203D%2520face%252C%2520an%2520audio%2520file%252C%2520an%250Aemotion%2520label%252C%2520and%2520an%2520intensity%2520value%2520as%2520inputs%252C%2520and%2520learns%2520to%2520animate%2520the%250Aaudio-synchronized%2520lip%2520movements%2520with%2520expressive%2520traits%2520of%2520the%2520face.%250AComprehensive%2520experiments%252C%2520both%2520quantitative%2520and%2520qualitative%252C%2520using%2520our%2520data%250Aand%2520generator%2520evidence%2520superior%2520ability%2520in%2520synthesizing%2520convincing%2520animations%252C%250Awhen%2520compared%2520with%2520the%2520best%2520performing%2520methods%2520in%2520the%2520literature.%2520Our%2520code%2520and%250Apre-trained%2520model%2520will%2520be%2520made%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12886v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EmoVOCA%3A%20Speech-Driven%20Emotional%203D%20Talking%20Heads&entry.906535625=Federico%20Nocentini%20and%20Claudio%20Ferrari%20and%20Stefano%20Berretti&entry.1292438233=%20%20The%20domain%20of%203D%20talking%20head%20generation%20has%20witnessed%20significant%20progress%0Ain%20recent%20years.%20A%20notable%20challenge%20in%20this%20field%20consists%20in%20blending%0Aspeech-related%20motions%20with%20expression%20dynamics%2C%20which%20is%20primarily%20caused%20by%0Athe%20lack%20of%20comprehensive%203D%20datasets%20that%20combine%20diversity%20in%20spoken%0Asentences%20with%20a%20variety%20of%20facial%20expressions.%20Whereas%20literature%20works%0Aattempted%20to%20exploit%202D%20video%20data%20and%20parametric%203D%20models%20as%20a%20workaround%2C%0Athese%20still%20show%20limitations%20when%20jointly%20modeling%20the%20two%20motions.%20In%20this%0Awork%2C%20we%20address%20this%20problem%20from%20a%20different%20perspective%2C%20and%20propose%20an%0Ainnovative%20data-driven%20technique%20that%20we%20used%20for%20creating%20a%20synthetic%20dataset%2C%0Acalled%20EmoVOCA%2C%20obtained%20by%20combining%20a%20collection%20of%20inexpressive%203D%20talking%0Aheads%20and%20a%20set%20of%203D%20expressive%20sequences.%20To%20demonstrate%20the%20advantages%20of%0Athis%20approach%2C%20and%20the%20quality%20of%20the%20dataset%2C%20we%20then%20designed%20and%20trained%20an%0Aemotional%203D%20talking%20head%20generator%20that%20accepts%20a%203D%20face%2C%20an%20audio%20file%2C%20an%0Aemotion%20label%2C%20and%20an%20intensity%20value%20as%20inputs%2C%20and%20learns%20to%20animate%20the%0Aaudio-synchronized%20lip%20movements%20with%20expressive%20traits%20of%20the%20face.%0AComprehensive%20experiments%2C%20both%20quantitative%20and%20qualitative%2C%20using%20our%20data%0Aand%20generator%20evidence%20superior%20ability%20in%20synthesizing%20convincing%20animations%2C%0Awhen%20compared%20with%20the%20best%20performing%20methods%20in%20the%20literature.%20Our%20code%20and%0Apre-trained%20model%20will%20be%20made%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12886v2&entry.124074799=Read"},
{"title": "DreamMesh: Jointly Manipulating and Texturing Triangle Meshes for\n  Text-to-3D Generation", "author": "Haibo Yang and Yang Chen and Yingwei Pan and Ting Yao and Zhineng Chen and Zuxuan Wu and Yu-Gang Jiang and Tao Mei", "abstract": "  Learning radiance fields (NeRF) with powerful 2D diffusion models has\ngarnered popularity for text-to-3D generation. Nevertheless, the implicit 3D\nrepresentations of NeRF lack explicit modeling of meshes and textures over\nsurfaces, and such surface-undefined way may suffer from the issues, e.g.,\nnoisy surfaces with ambiguous texture details or cross-view inconsistency. To\nalleviate this, we present DreamMesh, a novel text-to-3D architecture that\npivots on well-defined surfaces (triangle meshes) to generate high-fidelity\nexplicit 3D model. Technically, DreamMesh capitalizes on a distinctive\ncoarse-to-fine scheme. In the coarse stage, the mesh is first deformed by\ntext-guided Jacobians and then DreamMesh textures the mesh with an interlaced\nuse of 2D diffusion models in a tuning free manner from multiple viewpoints. In\nthe fine stage, DreamMesh jointly manipulates the mesh and refines the texture\nmap, leading to high-quality triangle meshes with high-fidelity textured\nmaterials. Extensive experiments demonstrate that DreamMesh significantly\noutperforms state-of-the-art text-to-3D methods in faithfully generating 3D\ncontent with richer textual details and enhanced geometry. Our project page is\navailable at https://dreammesh.github.io.\n", "link": "http://arxiv.org/abs/2409.07454v1", "date": "2024-09-11", "relevancy": 3.0255, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6111}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6111}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamMesh%3A%20Jointly%20Manipulating%20and%20Texturing%20Triangle%20Meshes%20for%0A%20%20Text-to-3D%20Generation&body=Title%3A%20DreamMesh%3A%20Jointly%20Manipulating%20and%20Texturing%20Triangle%20Meshes%20for%0A%20%20Text-to-3D%20Generation%0AAuthor%3A%20Haibo%20Yang%20and%20Yang%20Chen%20and%20Yingwei%20Pan%20and%20Ting%20Yao%20and%20Zhineng%20Chen%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang%20and%20Tao%20Mei%0AAbstract%3A%20%20%20Learning%20radiance%20fields%20%28NeRF%29%20with%20powerful%202D%20diffusion%20models%20has%0Agarnered%20popularity%20for%20text-to-3D%20generation.%20Nevertheless%2C%20the%20implicit%203D%0Arepresentations%20of%20NeRF%20lack%20explicit%20modeling%20of%20meshes%20and%20textures%20over%0Asurfaces%2C%20and%20such%20surface-undefined%20way%20may%20suffer%20from%20the%20issues%2C%20e.g.%2C%0Anoisy%20surfaces%20with%20ambiguous%20texture%20details%20or%20cross-view%20inconsistency.%20To%0Aalleviate%20this%2C%20we%20present%20DreamMesh%2C%20a%20novel%20text-to-3D%20architecture%20that%0Apivots%20on%20well-defined%20surfaces%20%28triangle%20meshes%29%20to%20generate%20high-fidelity%0Aexplicit%203D%20model.%20Technically%2C%20DreamMesh%20capitalizes%20on%20a%20distinctive%0Acoarse-to-fine%20scheme.%20In%20the%20coarse%20stage%2C%20the%20mesh%20is%20first%20deformed%20by%0Atext-guided%20Jacobians%20and%20then%20DreamMesh%20textures%20the%20mesh%20with%20an%20interlaced%0Ause%20of%202D%20diffusion%20models%20in%20a%20tuning%20free%20manner%20from%20multiple%20viewpoints.%20In%0Athe%20fine%20stage%2C%20DreamMesh%20jointly%20manipulates%20the%20mesh%20and%20refines%20the%20texture%0Amap%2C%20leading%20to%20high-quality%20triangle%20meshes%20with%20high-fidelity%20textured%0Amaterials.%20Extensive%20experiments%20demonstrate%20that%20DreamMesh%20significantly%0Aoutperforms%20state-of-the-art%20text-to-3D%20methods%20in%20faithfully%20generating%203D%0Acontent%20with%20richer%20textual%20details%20and%20enhanced%20geometry.%20Our%20project%20page%20is%0Aavailable%20at%20https%3A//dreammesh.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07454v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamMesh%253A%2520Jointly%2520Manipulating%2520and%2520Texturing%2520Triangle%2520Meshes%2520for%250A%2520%2520Text-to-3D%2520Generation%26entry.906535625%3DHaibo%2520Yang%2520and%2520Yang%2520Chen%2520and%2520Yingwei%2520Pan%2520and%2520Ting%2520Yao%2520and%2520Zhineng%2520Chen%2520and%2520Zuxuan%2520Wu%2520and%2520Yu-Gang%2520Jiang%2520and%2520Tao%2520Mei%26entry.1292438233%3D%2520%2520Learning%2520radiance%2520fields%2520%2528NeRF%2529%2520with%2520powerful%25202D%2520diffusion%2520models%2520has%250Agarnered%2520popularity%2520for%2520text-to-3D%2520generation.%2520Nevertheless%252C%2520the%2520implicit%25203D%250Arepresentations%2520of%2520NeRF%2520lack%2520explicit%2520modeling%2520of%2520meshes%2520and%2520textures%2520over%250Asurfaces%252C%2520and%2520such%2520surface-undefined%2520way%2520may%2520suffer%2520from%2520the%2520issues%252C%2520e.g.%252C%250Anoisy%2520surfaces%2520with%2520ambiguous%2520texture%2520details%2520or%2520cross-view%2520inconsistency.%2520To%250Aalleviate%2520this%252C%2520we%2520present%2520DreamMesh%252C%2520a%2520novel%2520text-to-3D%2520architecture%2520that%250Apivots%2520on%2520well-defined%2520surfaces%2520%2528triangle%2520meshes%2529%2520to%2520generate%2520high-fidelity%250Aexplicit%25203D%2520model.%2520Technically%252C%2520DreamMesh%2520capitalizes%2520on%2520a%2520distinctive%250Acoarse-to-fine%2520scheme.%2520In%2520the%2520coarse%2520stage%252C%2520the%2520mesh%2520is%2520first%2520deformed%2520by%250Atext-guided%2520Jacobians%2520and%2520then%2520DreamMesh%2520textures%2520the%2520mesh%2520with%2520an%2520interlaced%250Ause%2520of%25202D%2520diffusion%2520models%2520in%2520a%2520tuning%2520free%2520manner%2520from%2520multiple%2520viewpoints.%2520In%250Athe%2520fine%2520stage%252C%2520DreamMesh%2520jointly%2520manipulates%2520the%2520mesh%2520and%2520refines%2520the%2520texture%250Amap%252C%2520leading%2520to%2520high-quality%2520triangle%2520meshes%2520with%2520high-fidelity%2520textured%250Amaterials.%2520Extensive%2520experiments%2520demonstrate%2520that%2520DreamMesh%2520significantly%250Aoutperforms%2520state-of-the-art%2520text-to-3D%2520methods%2520in%2520faithfully%2520generating%25203D%250Acontent%2520with%2520richer%2520textual%2520details%2520and%2520enhanced%2520geometry.%2520Our%2520project%2520page%2520is%250Aavailable%2520at%2520https%253A//dreammesh.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07454v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamMesh%3A%20Jointly%20Manipulating%20and%20Texturing%20Triangle%20Meshes%20for%0A%20%20Text-to-3D%20Generation&entry.906535625=Haibo%20Yang%20and%20Yang%20Chen%20and%20Yingwei%20Pan%20and%20Ting%20Yao%20and%20Zhineng%20Chen%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang%20and%20Tao%20Mei&entry.1292438233=%20%20Learning%20radiance%20fields%20%28NeRF%29%20with%20powerful%202D%20diffusion%20models%20has%0Agarnered%20popularity%20for%20text-to-3D%20generation.%20Nevertheless%2C%20the%20implicit%203D%0Arepresentations%20of%20NeRF%20lack%20explicit%20modeling%20of%20meshes%20and%20textures%20over%0Asurfaces%2C%20and%20such%20surface-undefined%20way%20may%20suffer%20from%20the%20issues%2C%20e.g.%2C%0Anoisy%20surfaces%20with%20ambiguous%20texture%20details%20or%20cross-view%20inconsistency.%20To%0Aalleviate%20this%2C%20we%20present%20DreamMesh%2C%20a%20novel%20text-to-3D%20architecture%20that%0Apivots%20on%20well-defined%20surfaces%20%28triangle%20meshes%29%20to%20generate%20high-fidelity%0Aexplicit%203D%20model.%20Technically%2C%20DreamMesh%20capitalizes%20on%20a%20distinctive%0Acoarse-to-fine%20scheme.%20In%20the%20coarse%20stage%2C%20the%20mesh%20is%20first%20deformed%20by%0Atext-guided%20Jacobians%20and%20then%20DreamMesh%20textures%20the%20mesh%20with%20an%20interlaced%0Ause%20of%202D%20diffusion%20models%20in%20a%20tuning%20free%20manner%20from%20multiple%20viewpoints.%20In%0Athe%20fine%20stage%2C%20DreamMesh%20jointly%20manipulates%20the%20mesh%20and%20refines%20the%20texture%0Amap%2C%20leading%20to%20high-quality%20triangle%20meshes%20with%20high-fidelity%20textured%0Amaterials.%20Extensive%20experiments%20demonstrate%20that%20DreamMesh%20significantly%0Aoutperforms%20state-of-the-art%20text-to-3D%20methods%20in%20faithfully%20generating%203D%0Acontent%20with%20richer%20textual%20details%20and%20enhanced%20geometry.%20Our%20project%20page%20is%0Aavailable%20at%20https%3A//dreammesh.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07454v1&entry.124074799=Read"},
{"title": "Phy124: Fast Physics-Driven 4D Content Generation from a Single Image", "author": "Jiajing Lin and Zhenzhong Wang and Yongjie Hou and Yuzhou Tang and Min Jiang", "abstract": "  4D content generation focuses on creating dynamic 3D objects that change over\ntime. Existing methods primarily rely on pre-trained video diffusion models,\nutilizing sampling processes or reference videos. However, these approaches\nface significant challenges. Firstly, the generated 4D content often fails to\nadhere to real-world physics since video diffusion models do not incorporate\nphysical priors. Secondly, the extensive sampling process and the large number\nof parameters in diffusion models result in exceedingly time-consuming\ngeneration processes. To address these issues, we introduce Phy124, a novel,\nfast, and physics-driven method for controllable 4D content generation from a\nsingle image. Phy124 integrates physical simulation directly into the 4D\ngeneration process, ensuring that the resulting 4D content adheres to natural\nphysical laws. Phy124 also eliminates the use of diffusion models during the 4D\ndynamics generation phase, significantly speeding up the process. Phy124 allows\nfor the control of 4D dynamics, including movement speed and direction, by\nmanipulating external forces. Extensive experiments demonstrate that Phy124\ngenerates high-fidelity 4D content with significantly reduced inference times,\nachieving stateof-the-art performance. The code and generated 4D content are\navailable at the provided link: https://anonymous.4open.science/r/BBF2/.\n", "link": "http://arxiv.org/abs/2409.07179v1", "date": "2024-09-11", "relevancy": 3.0076, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6253}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6253}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Phy124%3A%20Fast%20Physics-Driven%204D%20Content%20Generation%20from%20a%20Single%20Image&body=Title%3A%20Phy124%3A%20Fast%20Physics-Driven%204D%20Content%20Generation%20from%20a%20Single%20Image%0AAuthor%3A%20Jiajing%20Lin%20and%20Zhenzhong%20Wang%20and%20Yongjie%20Hou%20and%20Yuzhou%20Tang%20and%20Min%20Jiang%0AAbstract%3A%20%20%204D%20content%20generation%20focuses%20on%20creating%20dynamic%203D%20objects%20that%20change%20over%0Atime.%20Existing%20methods%20primarily%20rely%20on%20pre-trained%20video%20diffusion%20models%2C%0Autilizing%20sampling%20processes%20or%20reference%20videos.%20However%2C%20these%20approaches%0Aface%20significant%20challenges.%20Firstly%2C%20the%20generated%204D%20content%20often%20fails%20to%0Aadhere%20to%20real-world%20physics%20since%20video%20diffusion%20models%20do%20not%20incorporate%0Aphysical%20priors.%20Secondly%2C%20the%20extensive%20sampling%20process%20and%20the%20large%20number%0Aof%20parameters%20in%20diffusion%20models%20result%20in%20exceedingly%20time-consuming%0Ageneration%20processes.%20To%20address%20these%20issues%2C%20we%20introduce%20Phy124%2C%20a%20novel%2C%0Afast%2C%20and%20physics-driven%20method%20for%20controllable%204D%20content%20generation%20from%20a%0Asingle%20image.%20Phy124%20integrates%20physical%20simulation%20directly%20into%20the%204D%0Ageneration%20process%2C%20ensuring%20that%20the%20resulting%204D%20content%20adheres%20to%20natural%0Aphysical%20laws.%20Phy124%20also%20eliminates%20the%20use%20of%20diffusion%20models%20during%20the%204D%0Adynamics%20generation%20phase%2C%20significantly%20speeding%20up%20the%20process.%20Phy124%20allows%0Afor%20the%20control%20of%204D%20dynamics%2C%20including%20movement%20speed%20and%20direction%2C%20by%0Amanipulating%20external%20forces.%20Extensive%20experiments%20demonstrate%20that%20Phy124%0Agenerates%20high-fidelity%204D%20content%20with%20significantly%20reduced%20inference%20times%2C%0Aachieving%20stateof-the-art%20performance.%20The%20code%20and%20generated%204D%20content%20are%0Aavailable%20at%20the%20provided%20link%3A%20https%3A//anonymous.4open.science/r/BBF2/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07179v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhy124%253A%2520Fast%2520Physics-Driven%25204D%2520Content%2520Generation%2520from%2520a%2520Single%2520Image%26entry.906535625%3DJiajing%2520Lin%2520and%2520Zhenzhong%2520Wang%2520and%2520Yongjie%2520Hou%2520and%2520Yuzhou%2520Tang%2520and%2520Min%2520Jiang%26entry.1292438233%3D%2520%25204D%2520content%2520generation%2520focuses%2520on%2520creating%2520dynamic%25203D%2520objects%2520that%2520change%2520over%250Atime.%2520Existing%2520methods%2520primarily%2520rely%2520on%2520pre-trained%2520video%2520diffusion%2520models%252C%250Autilizing%2520sampling%2520processes%2520or%2520reference%2520videos.%2520However%252C%2520these%2520approaches%250Aface%2520significant%2520challenges.%2520Firstly%252C%2520the%2520generated%25204D%2520content%2520often%2520fails%2520to%250Aadhere%2520to%2520real-world%2520physics%2520since%2520video%2520diffusion%2520models%2520do%2520not%2520incorporate%250Aphysical%2520priors.%2520Secondly%252C%2520the%2520extensive%2520sampling%2520process%2520and%2520the%2520large%2520number%250Aof%2520parameters%2520in%2520diffusion%2520models%2520result%2520in%2520exceedingly%2520time-consuming%250Ageneration%2520processes.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520Phy124%252C%2520a%2520novel%252C%250Afast%252C%2520and%2520physics-driven%2520method%2520for%2520controllable%25204D%2520content%2520generation%2520from%2520a%250Asingle%2520image.%2520Phy124%2520integrates%2520physical%2520simulation%2520directly%2520into%2520the%25204D%250Ageneration%2520process%252C%2520ensuring%2520that%2520the%2520resulting%25204D%2520content%2520adheres%2520to%2520natural%250Aphysical%2520laws.%2520Phy124%2520also%2520eliminates%2520the%2520use%2520of%2520diffusion%2520models%2520during%2520the%25204D%250Adynamics%2520generation%2520phase%252C%2520significantly%2520speeding%2520up%2520the%2520process.%2520Phy124%2520allows%250Afor%2520the%2520control%2520of%25204D%2520dynamics%252C%2520including%2520movement%2520speed%2520and%2520direction%252C%2520by%250Amanipulating%2520external%2520forces.%2520Extensive%2520experiments%2520demonstrate%2520that%2520Phy124%250Agenerates%2520high-fidelity%25204D%2520content%2520with%2520significantly%2520reduced%2520inference%2520times%252C%250Aachieving%2520stateof-the-art%2520performance.%2520The%2520code%2520and%2520generated%25204D%2520content%2520are%250Aavailable%2520at%2520the%2520provided%2520link%253A%2520https%253A//anonymous.4open.science/r/BBF2/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07179v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Phy124%3A%20Fast%20Physics-Driven%204D%20Content%20Generation%20from%20a%20Single%20Image&entry.906535625=Jiajing%20Lin%20and%20Zhenzhong%20Wang%20and%20Yongjie%20Hou%20and%20Yuzhou%20Tang%20and%20Min%20Jiang&entry.1292438233=%20%204D%20content%20generation%20focuses%20on%20creating%20dynamic%203D%20objects%20that%20change%20over%0Atime.%20Existing%20methods%20primarily%20rely%20on%20pre-trained%20video%20diffusion%20models%2C%0Autilizing%20sampling%20processes%20or%20reference%20videos.%20However%2C%20these%20approaches%0Aface%20significant%20challenges.%20Firstly%2C%20the%20generated%204D%20content%20often%20fails%20to%0Aadhere%20to%20real-world%20physics%20since%20video%20diffusion%20models%20do%20not%20incorporate%0Aphysical%20priors.%20Secondly%2C%20the%20extensive%20sampling%20process%20and%20the%20large%20number%0Aof%20parameters%20in%20diffusion%20models%20result%20in%20exceedingly%20time-consuming%0Ageneration%20processes.%20To%20address%20these%20issues%2C%20we%20introduce%20Phy124%2C%20a%20novel%2C%0Afast%2C%20and%20physics-driven%20method%20for%20controllable%204D%20content%20generation%20from%20a%0Asingle%20image.%20Phy124%20integrates%20physical%20simulation%20directly%20into%20the%204D%0Ageneration%20process%2C%20ensuring%20that%20the%20resulting%204D%20content%20adheres%20to%20natural%0Aphysical%20laws.%20Phy124%20also%20eliminates%20the%20use%20of%20diffusion%20models%20during%20the%204D%0Adynamics%20generation%20phase%2C%20significantly%20speeding%20up%20the%20process.%20Phy124%20allows%0Afor%20the%20control%20of%204D%20dynamics%2C%20including%20movement%20speed%20and%20direction%2C%20by%0Amanipulating%20external%20forces.%20Extensive%20experiments%20demonstrate%20that%20Phy124%0Agenerates%20high-fidelity%204D%20content%20with%20significantly%20reduced%20inference%20times%2C%0Aachieving%20stateof-the-art%20performance.%20The%20code%20and%20generated%204D%20content%20are%0Aavailable%20at%20the%20provided%20link%3A%20https%3A//anonymous.4open.science/r/BBF2/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07179v1&entry.124074799=Read"},
{"title": "EyeCLIP: A visual-language foundation model for multi-modal ophthalmic\n  image analysis", "author": "Danli Shi and Weiyi Zhang and Jiancheng Yang and Siyu Huang and Xiaolan Chen and Mayinuer Yusufu and Kai Jin and Shan Lin and Shunming Liu and Qing Zhang and Mingguang He", "abstract": "  Early detection of eye diseases like glaucoma, macular degeneration, and\ndiabetic retinopathy is crucial for preventing vision loss. While artificial\nintelligence (AI) foundation models hold significant promise for addressing\nthese challenges, existing ophthalmic foundation models primarily focus on a\nsingle modality, whereas diagnosing eye diseases requires multiple modalities.\nA critical yet often overlooked aspect is harnessing the multi-view information\nacross various modalities for the same patient. Additionally, due to the\nlong-tail nature of ophthalmic diseases, standard fully supervised or\nunsupervised learning approaches often struggle. Therefore, it is essential to\nintegrate clinical text to capture a broader spectrum of diseases. We propose\nEyeCLIP, a visual-language foundation model developed using over 2.77 million\nmulti-modal ophthalmology images with partial text data. To fully leverage the\nlarge multi-modal unlabeled and labeled data, we introduced a pretraining\nstrategy that combines self-supervised reconstructions, multi-modal image\ncontrastive learning, and image-text contrastive learning to learn a shared\nrepresentation of multiple modalities. Through evaluation using 14 benchmark\ndatasets, EyeCLIP can be transferred to a wide range of downstream tasks\ninvolving ocular and systemic diseases, achieving state-of-the-art performance\nin disease classification, visual question answering, and cross-modal\nretrieval. EyeCLIP represents a significant advancement over previous methods,\nespecially showcasing few-shot, even zero-shot capabilities in real-world\nlong-tail scenarios.\n", "link": "http://arxiv.org/abs/2409.06644v2", "date": "2024-09-11", "relevancy": 2.993, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6206}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5876}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EyeCLIP%3A%20A%20visual-language%20foundation%20model%20for%20multi-modal%20ophthalmic%0A%20%20image%20analysis&body=Title%3A%20EyeCLIP%3A%20A%20visual-language%20foundation%20model%20for%20multi-modal%20ophthalmic%0A%20%20image%20analysis%0AAuthor%3A%20Danli%20Shi%20and%20Weiyi%20Zhang%20and%20Jiancheng%20Yang%20and%20Siyu%20Huang%20and%20Xiaolan%20Chen%20and%20Mayinuer%20Yusufu%20and%20Kai%20Jin%20and%20Shan%20Lin%20and%20Shunming%20Liu%20and%20Qing%20Zhang%20and%20Mingguang%20He%0AAbstract%3A%20%20%20Early%20detection%20of%20eye%20diseases%20like%20glaucoma%2C%20macular%20degeneration%2C%20and%0Adiabetic%20retinopathy%20is%20crucial%20for%20preventing%20vision%20loss.%20While%20artificial%0Aintelligence%20%28AI%29%20foundation%20models%20hold%20significant%20promise%20for%20addressing%0Athese%20challenges%2C%20existing%20ophthalmic%20foundation%20models%20primarily%20focus%20on%20a%0Asingle%20modality%2C%20whereas%20diagnosing%20eye%20diseases%20requires%20multiple%20modalities.%0AA%20critical%20yet%20often%20overlooked%20aspect%20is%20harnessing%20the%20multi-view%20information%0Aacross%20various%20modalities%20for%20the%20same%20patient.%20Additionally%2C%20due%20to%20the%0Along-tail%20nature%20of%20ophthalmic%20diseases%2C%20standard%20fully%20supervised%20or%0Aunsupervised%20learning%20approaches%20often%20struggle.%20Therefore%2C%20it%20is%20essential%20to%0Aintegrate%20clinical%20text%20to%20capture%20a%20broader%20spectrum%20of%20diseases.%20We%20propose%0AEyeCLIP%2C%20a%20visual-language%20foundation%20model%20developed%20using%20over%202.77%20million%0Amulti-modal%20ophthalmology%20images%20with%20partial%20text%20data.%20To%20fully%20leverage%20the%0Alarge%20multi-modal%20unlabeled%20and%20labeled%20data%2C%20we%20introduced%20a%20pretraining%0Astrategy%20that%20combines%20self-supervised%20reconstructions%2C%20multi-modal%20image%0Acontrastive%20learning%2C%20and%20image-text%20contrastive%20learning%20to%20learn%20a%20shared%0Arepresentation%20of%20multiple%20modalities.%20Through%20evaluation%20using%2014%20benchmark%0Adatasets%2C%20EyeCLIP%20can%20be%20transferred%20to%20a%20wide%20range%20of%20downstream%20tasks%0Ainvolving%20ocular%20and%20systemic%20diseases%2C%20achieving%20state-of-the-art%20performance%0Ain%20disease%20classification%2C%20visual%20question%20answering%2C%20and%20cross-modal%0Aretrieval.%20EyeCLIP%20represents%20a%20significant%20advancement%20over%20previous%20methods%2C%0Aespecially%20showcasing%20few-shot%2C%20even%20zero-shot%20capabilities%20in%20real-world%0Along-tail%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06644v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEyeCLIP%253A%2520A%2520visual-language%2520foundation%2520model%2520for%2520multi-modal%2520ophthalmic%250A%2520%2520image%2520analysis%26entry.906535625%3DDanli%2520Shi%2520and%2520Weiyi%2520Zhang%2520and%2520Jiancheng%2520Yang%2520and%2520Siyu%2520Huang%2520and%2520Xiaolan%2520Chen%2520and%2520Mayinuer%2520Yusufu%2520and%2520Kai%2520Jin%2520and%2520Shan%2520Lin%2520and%2520Shunming%2520Liu%2520and%2520Qing%2520Zhang%2520and%2520Mingguang%2520He%26entry.1292438233%3D%2520%2520Early%2520detection%2520of%2520eye%2520diseases%2520like%2520glaucoma%252C%2520macular%2520degeneration%252C%2520and%250Adiabetic%2520retinopathy%2520is%2520crucial%2520for%2520preventing%2520vision%2520loss.%2520While%2520artificial%250Aintelligence%2520%2528AI%2529%2520foundation%2520models%2520hold%2520significant%2520promise%2520for%2520addressing%250Athese%2520challenges%252C%2520existing%2520ophthalmic%2520foundation%2520models%2520primarily%2520focus%2520on%2520a%250Asingle%2520modality%252C%2520whereas%2520diagnosing%2520eye%2520diseases%2520requires%2520multiple%2520modalities.%250AA%2520critical%2520yet%2520often%2520overlooked%2520aspect%2520is%2520harnessing%2520the%2520multi-view%2520information%250Aacross%2520various%2520modalities%2520for%2520the%2520same%2520patient.%2520Additionally%252C%2520due%2520to%2520the%250Along-tail%2520nature%2520of%2520ophthalmic%2520diseases%252C%2520standard%2520fully%2520supervised%2520or%250Aunsupervised%2520learning%2520approaches%2520often%2520struggle.%2520Therefore%252C%2520it%2520is%2520essential%2520to%250Aintegrate%2520clinical%2520text%2520to%2520capture%2520a%2520broader%2520spectrum%2520of%2520diseases.%2520We%2520propose%250AEyeCLIP%252C%2520a%2520visual-language%2520foundation%2520model%2520developed%2520using%2520over%25202.77%2520million%250Amulti-modal%2520ophthalmology%2520images%2520with%2520partial%2520text%2520data.%2520To%2520fully%2520leverage%2520the%250Alarge%2520multi-modal%2520unlabeled%2520and%2520labeled%2520data%252C%2520we%2520introduced%2520a%2520pretraining%250Astrategy%2520that%2520combines%2520self-supervised%2520reconstructions%252C%2520multi-modal%2520image%250Acontrastive%2520learning%252C%2520and%2520image-text%2520contrastive%2520learning%2520to%2520learn%2520a%2520shared%250Arepresentation%2520of%2520multiple%2520modalities.%2520Through%2520evaluation%2520using%252014%2520benchmark%250Adatasets%252C%2520EyeCLIP%2520can%2520be%2520transferred%2520to%2520a%2520wide%2520range%2520of%2520downstream%2520tasks%250Ainvolving%2520ocular%2520and%2520systemic%2520diseases%252C%2520achieving%2520state-of-the-art%2520performance%250Ain%2520disease%2520classification%252C%2520visual%2520question%2520answering%252C%2520and%2520cross-modal%250Aretrieval.%2520EyeCLIP%2520represents%2520a%2520significant%2520advancement%2520over%2520previous%2520methods%252C%250Aespecially%2520showcasing%2520few-shot%252C%2520even%2520zero-shot%2520capabilities%2520in%2520real-world%250Along-tail%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06644v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EyeCLIP%3A%20A%20visual-language%20foundation%20model%20for%20multi-modal%20ophthalmic%0A%20%20image%20analysis&entry.906535625=Danli%20Shi%20and%20Weiyi%20Zhang%20and%20Jiancheng%20Yang%20and%20Siyu%20Huang%20and%20Xiaolan%20Chen%20and%20Mayinuer%20Yusufu%20and%20Kai%20Jin%20and%20Shan%20Lin%20and%20Shunming%20Liu%20and%20Qing%20Zhang%20and%20Mingguang%20He&entry.1292438233=%20%20Early%20detection%20of%20eye%20diseases%20like%20glaucoma%2C%20macular%20degeneration%2C%20and%0Adiabetic%20retinopathy%20is%20crucial%20for%20preventing%20vision%20loss.%20While%20artificial%0Aintelligence%20%28AI%29%20foundation%20models%20hold%20significant%20promise%20for%20addressing%0Athese%20challenges%2C%20existing%20ophthalmic%20foundation%20models%20primarily%20focus%20on%20a%0Asingle%20modality%2C%20whereas%20diagnosing%20eye%20diseases%20requires%20multiple%20modalities.%0AA%20critical%20yet%20often%20overlooked%20aspect%20is%20harnessing%20the%20multi-view%20information%0Aacross%20various%20modalities%20for%20the%20same%20patient.%20Additionally%2C%20due%20to%20the%0Along-tail%20nature%20of%20ophthalmic%20diseases%2C%20standard%20fully%20supervised%20or%0Aunsupervised%20learning%20approaches%20often%20struggle.%20Therefore%2C%20it%20is%20essential%20to%0Aintegrate%20clinical%20text%20to%20capture%20a%20broader%20spectrum%20of%20diseases.%20We%20propose%0AEyeCLIP%2C%20a%20visual-language%20foundation%20model%20developed%20using%20over%202.77%20million%0Amulti-modal%20ophthalmology%20images%20with%20partial%20text%20data.%20To%20fully%20leverage%20the%0Alarge%20multi-modal%20unlabeled%20and%20labeled%20data%2C%20we%20introduced%20a%20pretraining%0Astrategy%20that%20combines%20self-supervised%20reconstructions%2C%20multi-modal%20image%0Acontrastive%20learning%2C%20and%20image-text%20contrastive%20learning%20to%20learn%20a%20shared%0Arepresentation%20of%20multiple%20modalities.%20Through%20evaluation%20using%2014%20benchmark%0Adatasets%2C%20EyeCLIP%20can%20be%20transferred%20to%20a%20wide%20range%20of%20downstream%20tasks%0Ainvolving%20ocular%20and%20systemic%20diseases%2C%20achieving%20state-of-the-art%20performance%0Ain%20disease%20classification%2C%20visual%20question%20answering%2C%20and%20cross-modal%0Aretrieval.%20EyeCLIP%20represents%20a%20significant%20advancement%20over%20previous%20methods%2C%0Aespecially%20showcasing%20few-shot%2C%20even%20zero-shot%20capabilities%20in%20real-world%0Along-tail%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06644v2&entry.124074799=Read"},
{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "author": "Gemma Canet Tarr\u00e9s and Zhe Lin and Zhifei Zhang and Jianming Zhang and Yizhi Song and Dan Ruta and Andrew Gilbert and John Collomosse and Soo Ye Kim", "abstract": "  Compositing an object into an image involves multiple non-trivial sub-tasks\nsuch as object placement and scaling, color/lighting harmonization,\nviewpoint/geometry adjustment, and shadow/reflection generation. Recent\ngenerative image compositing methods leverage diffusion models to handle\nmultiple sub-tasks at once. However, existing models face limitations due to\ntheir reliance on masking the original object during training, which constrains\ntheir generation to the input mask. Furthermore, obtaining an accurate input\nmask specifying the location and scale of the object in a new image can be\nhighly challenging. To overcome such limitations, we define a novel problem of\nunconstrained generative object compositing, i.e., the generation is not\nbounded by the mask, and train a diffusion-based model on a synthesized paired\ndataset. Our first-of-its-kind model is able to generate object effects such as\nshadows and reflections that go beyond the mask, enhancing image realism.\nAdditionally, if an empty mask is provided, our model automatically places the\nobject in diverse natural locations and scales, accelerating the compositing\nworkflow. Our model outperforms existing object placement and compositing\nmodels in various quality metrics and user studies.\n", "link": "http://arxiv.org/abs/2409.04559v2", "date": "2024-09-11", "relevancy": 2.9216, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5906}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5822}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thinking%20Outside%20the%20BBox%3A%20Unconstrained%20Generative%20Object%20Compositing&body=Title%3A%20Thinking%20Outside%20the%20BBox%3A%20Unconstrained%20Generative%20Object%20Compositing%0AAuthor%3A%20Gemma%20Canet%20Tarr%C3%A9s%20and%20Zhe%20Lin%20and%20Zhifei%20Zhang%20and%20Jianming%20Zhang%20and%20Yizhi%20Song%20and%20Dan%20Ruta%20and%20Andrew%20Gilbert%20and%20John%20Collomosse%20and%20Soo%20Ye%20Kim%0AAbstract%3A%20%20%20Compositing%20an%20object%20into%20an%20image%20involves%20multiple%20non-trivial%20sub-tasks%0Asuch%20as%20object%20placement%20and%20scaling%2C%20color/lighting%20harmonization%2C%0Aviewpoint/geometry%20adjustment%2C%20and%20shadow/reflection%20generation.%20Recent%0Agenerative%20image%20compositing%20methods%20leverage%20diffusion%20models%20to%20handle%0Amultiple%20sub-tasks%20at%20once.%20However%2C%20existing%20models%20face%20limitations%20due%20to%0Atheir%20reliance%20on%20masking%20the%20original%20object%20during%20training%2C%20which%20constrains%0Atheir%20generation%20to%20the%20input%20mask.%20Furthermore%2C%20obtaining%20an%20accurate%20input%0Amask%20specifying%20the%20location%20and%20scale%20of%20the%20object%20in%20a%20new%20image%20can%20be%0Ahighly%20challenging.%20To%20overcome%20such%20limitations%2C%20we%20define%20a%20novel%20problem%20of%0Aunconstrained%20generative%20object%20compositing%2C%20i.e.%2C%20the%20generation%20is%20not%0Abounded%20by%20the%20mask%2C%20and%20train%20a%20diffusion-based%20model%20on%20a%20synthesized%20paired%0Adataset.%20Our%20first-of-its-kind%20model%20is%20able%20to%20generate%20object%20effects%20such%20as%0Ashadows%20and%20reflections%20that%20go%20beyond%20the%20mask%2C%20enhancing%20image%20realism.%0AAdditionally%2C%20if%20an%20empty%20mask%20is%20provided%2C%20our%20model%20automatically%20places%20the%0Aobject%20in%20diverse%20natural%20locations%20and%20scales%2C%20accelerating%20the%20compositing%0Aworkflow.%20Our%20model%20outperforms%20existing%20object%20placement%20and%20compositing%0Amodels%20in%20various%20quality%20metrics%20and%20user%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04559v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThinking%2520Outside%2520the%2520BBox%253A%2520Unconstrained%2520Generative%2520Object%2520Compositing%26entry.906535625%3DGemma%2520Canet%2520Tarr%25C3%25A9s%2520and%2520Zhe%2520Lin%2520and%2520Zhifei%2520Zhang%2520and%2520Jianming%2520Zhang%2520and%2520Yizhi%2520Song%2520and%2520Dan%2520Ruta%2520and%2520Andrew%2520Gilbert%2520and%2520John%2520Collomosse%2520and%2520Soo%2520Ye%2520Kim%26entry.1292438233%3D%2520%2520Compositing%2520an%2520object%2520into%2520an%2520image%2520involves%2520multiple%2520non-trivial%2520sub-tasks%250Asuch%2520as%2520object%2520placement%2520and%2520scaling%252C%2520color/lighting%2520harmonization%252C%250Aviewpoint/geometry%2520adjustment%252C%2520and%2520shadow/reflection%2520generation.%2520Recent%250Agenerative%2520image%2520compositing%2520methods%2520leverage%2520diffusion%2520models%2520to%2520handle%250Amultiple%2520sub-tasks%2520at%2520once.%2520However%252C%2520existing%2520models%2520face%2520limitations%2520due%2520to%250Atheir%2520reliance%2520on%2520masking%2520the%2520original%2520object%2520during%2520training%252C%2520which%2520constrains%250Atheir%2520generation%2520to%2520the%2520input%2520mask.%2520Furthermore%252C%2520obtaining%2520an%2520accurate%2520input%250Amask%2520specifying%2520the%2520location%2520and%2520scale%2520of%2520the%2520object%2520in%2520a%2520new%2520image%2520can%2520be%250Ahighly%2520challenging.%2520To%2520overcome%2520such%2520limitations%252C%2520we%2520define%2520a%2520novel%2520problem%2520of%250Aunconstrained%2520generative%2520object%2520compositing%252C%2520i.e.%252C%2520the%2520generation%2520is%2520not%250Abounded%2520by%2520the%2520mask%252C%2520and%2520train%2520a%2520diffusion-based%2520model%2520on%2520a%2520synthesized%2520paired%250Adataset.%2520Our%2520first-of-its-kind%2520model%2520is%2520able%2520to%2520generate%2520object%2520effects%2520such%2520as%250Ashadows%2520and%2520reflections%2520that%2520go%2520beyond%2520the%2520mask%252C%2520enhancing%2520image%2520realism.%250AAdditionally%252C%2520if%2520an%2520empty%2520mask%2520is%2520provided%252C%2520our%2520model%2520automatically%2520places%2520the%250Aobject%2520in%2520diverse%2520natural%2520locations%2520and%2520scales%252C%2520accelerating%2520the%2520compositing%250Aworkflow.%2520Our%2520model%2520outperforms%2520existing%2520object%2520placement%2520and%2520compositing%250Amodels%2520in%2520various%2520quality%2520metrics%2520and%2520user%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04559v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thinking%20Outside%20the%20BBox%3A%20Unconstrained%20Generative%20Object%20Compositing&entry.906535625=Gemma%20Canet%20Tarr%C3%A9s%20and%20Zhe%20Lin%20and%20Zhifei%20Zhang%20and%20Jianming%20Zhang%20and%20Yizhi%20Song%20and%20Dan%20Ruta%20and%20Andrew%20Gilbert%20and%20John%20Collomosse%20and%20Soo%20Ye%20Kim&entry.1292438233=%20%20Compositing%20an%20object%20into%20an%20image%20involves%20multiple%20non-trivial%20sub-tasks%0Asuch%20as%20object%20placement%20and%20scaling%2C%20color/lighting%20harmonization%2C%0Aviewpoint/geometry%20adjustment%2C%20and%20shadow/reflection%20generation.%20Recent%0Agenerative%20image%20compositing%20methods%20leverage%20diffusion%20models%20to%20handle%0Amultiple%20sub-tasks%20at%20once.%20However%2C%20existing%20models%20face%20limitations%20due%20to%0Atheir%20reliance%20on%20masking%20the%20original%20object%20during%20training%2C%20which%20constrains%0Atheir%20generation%20to%20the%20input%20mask.%20Furthermore%2C%20obtaining%20an%20accurate%20input%0Amask%20specifying%20the%20location%20and%20scale%20of%20the%20object%20in%20a%20new%20image%20can%20be%0Ahighly%20challenging.%20To%20overcome%20such%20limitations%2C%20we%20define%20a%20novel%20problem%20of%0Aunconstrained%20generative%20object%20compositing%2C%20i.e.%2C%20the%20generation%20is%20not%0Abounded%20by%20the%20mask%2C%20and%20train%20a%20diffusion-based%20model%20on%20a%20synthesized%20paired%0Adataset.%20Our%20first-of-its-kind%20model%20is%20able%20to%20generate%20object%20effects%20such%20as%0Ashadows%20and%20reflections%20that%20go%20beyond%20the%20mask%2C%20enhancing%20image%20realism.%0AAdditionally%2C%20if%20an%20empty%20mask%20is%20provided%2C%20our%20model%20automatically%20places%20the%0Aobject%20in%20diverse%20natural%20locations%20and%20scales%2C%20accelerating%20the%20compositing%0Aworkflow.%20Our%20model%20outperforms%20existing%20object%20placement%20and%20compositing%0Amodels%20in%20various%20quality%20metrics%20and%20user%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04559v2&entry.124074799=Read"},
{"title": "PiTe: Pixel-Temporal Alignment for Large Video-Language Model", "author": "Yang Liu and Pengxiang Ding and Siteng Huang and Min Zhang and Han Zhao and Donglin Wang", "abstract": "  Fueled by the Large Language Models (LLMs) wave, Large Visual-Language Models\n(LVLMs) have emerged as a pivotal advancement, bridging the gap between image\nand text. However, video making it challenging for LVLMs to perform adequately\ndue to the complexity of the relationship between language and spatial-temporal\ndata structure. Recent Large Video-Language Models (LVidLMs) align feature of\nstatic visual data like image into latent space of language feature, by general\nmulti-modal tasks to leverage abilities of LLMs sufficiently. In this paper, we\nexplore fine-grained alignment approach via object trajectory for different\nmodalities across both spatial and temporal dimensions simultaneously. Thus, we\npropose a novel LVidLM by trajectory-guided Pixel-Temporal Alignment, dubbed\nPiTe, that exhibits promising applicable model property. To achieve\nfine-grained video-language alignment, we curate a multi-modal pre-training\ndataset PiTe-143k, the dataset provision of moving trajectories in pixel level\nfor all individual objects, that appear and mention in the video and caption\nboth, by our automatic annotation pipeline. Meanwhile, PiTe demonstrates\nastounding capabilities on myriad video-related multi-modal tasks through beat\nthe state-of-the-art methods by a large margin.\n", "link": "http://arxiv.org/abs/2409.07239v1", "date": "2024-09-11", "relevancy": 2.8939, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5994}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5684}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PiTe%3A%20Pixel-Temporal%20Alignment%20for%20Large%20Video-Language%20Model&body=Title%3A%20PiTe%3A%20Pixel-Temporal%20Alignment%20for%20Large%20Video-Language%20Model%0AAuthor%3A%20Yang%20Liu%20and%20Pengxiang%20Ding%20and%20Siteng%20Huang%20and%20Min%20Zhang%20and%20Han%20Zhao%20and%20Donglin%20Wang%0AAbstract%3A%20%20%20Fueled%20by%20the%20Large%20Language%20Models%20%28LLMs%29%20wave%2C%20Large%20Visual-Language%20Models%0A%28LVLMs%29%20have%20emerged%20as%20a%20pivotal%20advancement%2C%20bridging%20the%20gap%20between%20image%0Aand%20text.%20However%2C%20video%20making%20it%20challenging%20for%20LVLMs%20to%20perform%20adequately%0Adue%20to%20the%20complexity%20of%20the%20relationship%20between%20language%20and%20spatial-temporal%0Adata%20structure.%20Recent%20Large%20Video-Language%20Models%20%28LVidLMs%29%20align%20feature%20of%0Astatic%20visual%20data%20like%20image%20into%20latent%20space%20of%20language%20feature%2C%20by%20general%0Amulti-modal%20tasks%20to%20leverage%20abilities%20of%20LLMs%20sufficiently.%20In%20this%20paper%2C%20we%0Aexplore%20fine-grained%20alignment%20approach%20via%20object%20trajectory%20for%20different%0Amodalities%20across%20both%20spatial%20and%20temporal%20dimensions%20simultaneously.%20Thus%2C%20we%0Apropose%20a%20novel%20LVidLM%20by%20trajectory-guided%20Pixel-Temporal%20Alignment%2C%20dubbed%0APiTe%2C%20that%20exhibits%20promising%20applicable%20model%20property.%20To%20achieve%0Afine-grained%20video-language%20alignment%2C%20we%20curate%20a%20multi-modal%20pre-training%0Adataset%20PiTe-143k%2C%20the%20dataset%20provision%20of%20moving%20trajectories%20in%20pixel%20level%0Afor%20all%20individual%20objects%2C%20that%20appear%20and%20mention%20in%20the%20video%20and%20caption%0Aboth%2C%20by%20our%20automatic%20annotation%20pipeline.%20Meanwhile%2C%20PiTe%20demonstrates%0Aastounding%20capabilities%20on%20myriad%20video-related%20multi-modal%20tasks%20through%20beat%0Athe%20state-of-the-art%20methods%20by%20a%20large%20margin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPiTe%253A%2520Pixel-Temporal%2520Alignment%2520for%2520Large%2520Video-Language%2520Model%26entry.906535625%3DYang%2520Liu%2520and%2520Pengxiang%2520Ding%2520and%2520Siteng%2520Huang%2520and%2520Min%2520Zhang%2520and%2520Han%2520Zhao%2520and%2520Donglin%2520Wang%26entry.1292438233%3D%2520%2520Fueled%2520by%2520the%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520wave%252C%2520Large%2520Visual-Language%2520Models%250A%2528LVLMs%2529%2520have%2520emerged%2520as%2520a%2520pivotal%2520advancement%252C%2520bridging%2520the%2520gap%2520between%2520image%250Aand%2520text.%2520However%252C%2520video%2520making%2520it%2520challenging%2520for%2520LVLMs%2520to%2520perform%2520adequately%250Adue%2520to%2520the%2520complexity%2520of%2520the%2520relationship%2520between%2520language%2520and%2520spatial-temporal%250Adata%2520structure.%2520Recent%2520Large%2520Video-Language%2520Models%2520%2528LVidLMs%2529%2520align%2520feature%2520of%250Astatic%2520visual%2520data%2520like%2520image%2520into%2520latent%2520space%2520of%2520language%2520feature%252C%2520by%2520general%250Amulti-modal%2520tasks%2520to%2520leverage%2520abilities%2520of%2520LLMs%2520sufficiently.%2520In%2520this%2520paper%252C%2520we%250Aexplore%2520fine-grained%2520alignment%2520approach%2520via%2520object%2520trajectory%2520for%2520different%250Amodalities%2520across%2520both%2520spatial%2520and%2520temporal%2520dimensions%2520simultaneously.%2520Thus%252C%2520we%250Apropose%2520a%2520novel%2520LVidLM%2520by%2520trajectory-guided%2520Pixel-Temporal%2520Alignment%252C%2520dubbed%250APiTe%252C%2520that%2520exhibits%2520promising%2520applicable%2520model%2520property.%2520To%2520achieve%250Afine-grained%2520video-language%2520alignment%252C%2520we%2520curate%2520a%2520multi-modal%2520pre-training%250Adataset%2520PiTe-143k%252C%2520the%2520dataset%2520provision%2520of%2520moving%2520trajectories%2520in%2520pixel%2520level%250Afor%2520all%2520individual%2520objects%252C%2520that%2520appear%2520and%2520mention%2520in%2520the%2520video%2520and%2520caption%250Aboth%252C%2520by%2520our%2520automatic%2520annotation%2520pipeline.%2520Meanwhile%252C%2520PiTe%2520demonstrates%250Aastounding%2520capabilities%2520on%2520myriad%2520video-related%2520multi-modal%2520tasks%2520through%2520beat%250Athe%2520state-of-the-art%2520methods%2520by%2520a%2520large%2520margin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PiTe%3A%20Pixel-Temporal%20Alignment%20for%20Large%20Video-Language%20Model&entry.906535625=Yang%20Liu%20and%20Pengxiang%20Ding%20and%20Siteng%20Huang%20and%20Min%20Zhang%20and%20Han%20Zhao%20and%20Donglin%20Wang&entry.1292438233=%20%20Fueled%20by%20the%20Large%20Language%20Models%20%28LLMs%29%20wave%2C%20Large%20Visual-Language%20Models%0A%28LVLMs%29%20have%20emerged%20as%20a%20pivotal%20advancement%2C%20bridging%20the%20gap%20between%20image%0Aand%20text.%20However%2C%20video%20making%20it%20challenging%20for%20LVLMs%20to%20perform%20adequately%0Adue%20to%20the%20complexity%20of%20the%20relationship%20between%20language%20and%20spatial-temporal%0Adata%20structure.%20Recent%20Large%20Video-Language%20Models%20%28LVidLMs%29%20align%20feature%20of%0Astatic%20visual%20data%20like%20image%20into%20latent%20space%20of%20language%20feature%2C%20by%20general%0Amulti-modal%20tasks%20to%20leverage%20abilities%20of%20LLMs%20sufficiently.%20In%20this%20paper%2C%20we%0Aexplore%20fine-grained%20alignment%20approach%20via%20object%20trajectory%20for%20different%0Amodalities%20across%20both%20spatial%20and%20temporal%20dimensions%20simultaneously.%20Thus%2C%20we%0Apropose%20a%20novel%20LVidLM%20by%20trajectory-guided%20Pixel-Temporal%20Alignment%2C%20dubbed%0APiTe%2C%20that%20exhibits%20promising%20applicable%20model%20property.%20To%20achieve%0Afine-grained%20video-language%20alignment%2C%20we%20curate%20a%20multi-modal%20pre-training%0Adataset%20PiTe-143k%2C%20the%20dataset%20provision%20of%20moving%20trajectories%20in%20pixel%20level%0Afor%20all%20individual%20objects%2C%20that%20appear%20and%20mention%20in%20the%20video%20and%20caption%0Aboth%2C%20by%20our%20automatic%20annotation%20pipeline.%20Meanwhile%2C%20PiTe%20demonstrates%0Aastounding%20capabilities%20on%20myriad%20video-related%20multi-modal%20tasks%20through%20beat%0Athe%20state-of-the-art%20methods%20by%20a%20large%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07239v1&entry.124074799=Read"},
{"title": "3DGCQA: A Quality Assessment Database for 3D AI-Generated Contents", "author": "Yingjie Zhou and Zicheng Zhang and Farong Wen and Jun Jia and Yanwei Jiang and Xiaohong Liu and Xiongkuo Min and Guangtao Zhai", "abstract": "  Although 3D generated content (3DGC) offers advantages in reducing production\ncosts and accelerating design timelines, its quality often falls short when\ncompared to 3D professionally generated content. Common quality issues\nfrequently affect 3DGC, highlighting the importance of timely and effective\nquality assessment. Such evaluations not only ensure a higher standard of 3DGCs\nfor end-users but also provide critical insights for advancing generative\ntechnologies. To address existing gaps in this domain, this paper introduces a\nnovel 3DGC quality assessment dataset, 3DGCQA, built using 7 representative\nText-to-3D generation methods. During the dataset's construction, 50 fixed\nprompts are utilized to generate contents across all methods, resulting in the\ncreation of 313 textured meshes that constitute the 3DGCQA dataset. The\nvisualization intuitively reveals the presence of 6 common distortion\ncategories in the generated 3DGCs. To further explore the quality of the 3DGCs,\nsubjective quality assessment is conducted by evaluators, whose ratings reveal\nsignificant variation in quality across different generation methods.\nAdditionally, several objective quality assessment algorithms are tested on the\n3DGCQA dataset. The results expose limitations in the performance of existing\nalgorithms and underscore the need for developing more specialized quality\nassessment methods. To provide a valuable resource for future research and\ndevelopment in 3D content generation and quality assessment, the dataset has\nbeen open-sourced in https://github.com/zyj-2000/3DGCQA.\n", "link": "http://arxiv.org/abs/2409.07236v1", "date": "2024-09-11", "relevancy": 2.8831, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.589}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.589}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DGCQA%3A%20A%20Quality%20Assessment%20Database%20for%203D%20AI-Generated%20Contents&body=Title%3A%203DGCQA%3A%20A%20Quality%20Assessment%20Database%20for%203D%20AI-Generated%20Contents%0AAuthor%3A%20Yingjie%20Zhou%20and%20Zicheng%20Zhang%20and%20Farong%20Wen%20and%20Jun%20Jia%20and%20Yanwei%20Jiang%20and%20Xiaohong%20Liu%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%0AAbstract%3A%20%20%20Although%203D%20generated%20content%20%283DGC%29%20offers%20advantages%20in%20reducing%20production%0Acosts%20and%20accelerating%20design%20timelines%2C%20its%20quality%20often%20falls%20short%20when%0Acompared%20to%203D%20professionally%20generated%20content.%20Common%20quality%20issues%0Afrequently%20affect%203DGC%2C%20highlighting%20the%20importance%20of%20timely%20and%20effective%0Aquality%20assessment.%20Such%20evaluations%20not%20only%20ensure%20a%20higher%20standard%20of%203DGCs%0Afor%20end-users%20but%20also%20provide%20critical%20insights%20for%20advancing%20generative%0Atechnologies.%20To%20address%20existing%20gaps%20in%20this%20domain%2C%20this%20paper%20introduces%20a%0Anovel%203DGC%20quality%20assessment%20dataset%2C%203DGCQA%2C%20built%20using%207%20representative%0AText-to-3D%20generation%20methods.%20During%20the%20dataset%27s%20construction%2C%2050%20fixed%0Aprompts%20are%20utilized%20to%20generate%20contents%20across%20all%20methods%2C%20resulting%20in%20the%0Acreation%20of%20313%20textured%20meshes%20that%20constitute%20the%203DGCQA%20dataset.%20The%0Avisualization%20intuitively%20reveals%20the%20presence%20of%206%20common%20distortion%0Acategories%20in%20the%20generated%203DGCs.%20To%20further%20explore%20the%20quality%20of%20the%203DGCs%2C%0Asubjective%20quality%20assessment%20is%20conducted%20by%20evaluators%2C%20whose%20ratings%20reveal%0Asignificant%20variation%20in%20quality%20across%20different%20generation%20methods.%0AAdditionally%2C%20several%20objective%20quality%20assessment%20algorithms%20are%20tested%20on%20the%0A3DGCQA%20dataset.%20The%20results%20expose%20limitations%20in%20the%20performance%20of%20existing%0Aalgorithms%20and%20underscore%20the%20need%20for%20developing%20more%20specialized%20quality%0Aassessment%20methods.%20To%20provide%20a%20valuable%20resource%20for%20future%20research%20and%0Adevelopment%20in%203D%20content%20generation%20and%20quality%20assessment%2C%20the%20dataset%20has%0Abeen%20open-sourced%20in%20https%3A//github.com/zyj-2000/3DGCQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07236v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DGCQA%253A%2520A%2520Quality%2520Assessment%2520Database%2520for%25203D%2520AI-Generated%2520Contents%26entry.906535625%3DYingjie%2520Zhou%2520and%2520Zicheng%2520Zhang%2520and%2520Farong%2520Wen%2520and%2520Jun%2520Jia%2520and%2520Yanwei%2520Jiang%2520and%2520Xiaohong%2520Liu%2520and%2520Xiongkuo%2520Min%2520and%2520Guangtao%2520Zhai%26entry.1292438233%3D%2520%2520Although%25203D%2520generated%2520content%2520%25283DGC%2529%2520offers%2520advantages%2520in%2520reducing%2520production%250Acosts%2520and%2520accelerating%2520design%2520timelines%252C%2520its%2520quality%2520often%2520falls%2520short%2520when%250Acompared%2520to%25203D%2520professionally%2520generated%2520content.%2520Common%2520quality%2520issues%250Afrequently%2520affect%25203DGC%252C%2520highlighting%2520the%2520importance%2520of%2520timely%2520and%2520effective%250Aquality%2520assessment.%2520Such%2520evaluations%2520not%2520only%2520ensure%2520a%2520higher%2520standard%2520of%25203DGCs%250Afor%2520end-users%2520but%2520also%2520provide%2520critical%2520insights%2520for%2520advancing%2520generative%250Atechnologies.%2520To%2520address%2520existing%2520gaps%2520in%2520this%2520domain%252C%2520this%2520paper%2520introduces%2520a%250Anovel%25203DGC%2520quality%2520assessment%2520dataset%252C%25203DGCQA%252C%2520built%2520using%25207%2520representative%250AText-to-3D%2520generation%2520methods.%2520During%2520the%2520dataset%2527s%2520construction%252C%252050%2520fixed%250Aprompts%2520are%2520utilized%2520to%2520generate%2520contents%2520across%2520all%2520methods%252C%2520resulting%2520in%2520the%250Acreation%2520of%2520313%2520textured%2520meshes%2520that%2520constitute%2520the%25203DGCQA%2520dataset.%2520The%250Avisualization%2520intuitively%2520reveals%2520the%2520presence%2520of%25206%2520common%2520distortion%250Acategories%2520in%2520the%2520generated%25203DGCs.%2520To%2520further%2520explore%2520the%2520quality%2520of%2520the%25203DGCs%252C%250Asubjective%2520quality%2520assessment%2520is%2520conducted%2520by%2520evaluators%252C%2520whose%2520ratings%2520reveal%250Asignificant%2520variation%2520in%2520quality%2520across%2520different%2520generation%2520methods.%250AAdditionally%252C%2520several%2520objective%2520quality%2520assessment%2520algorithms%2520are%2520tested%2520on%2520the%250A3DGCQA%2520dataset.%2520The%2520results%2520expose%2520limitations%2520in%2520the%2520performance%2520of%2520existing%250Aalgorithms%2520and%2520underscore%2520the%2520need%2520for%2520developing%2520more%2520specialized%2520quality%250Aassessment%2520methods.%2520To%2520provide%2520a%2520valuable%2520resource%2520for%2520future%2520research%2520and%250Adevelopment%2520in%25203D%2520content%2520generation%2520and%2520quality%2520assessment%252C%2520the%2520dataset%2520has%250Abeen%2520open-sourced%2520in%2520https%253A//github.com/zyj-2000/3DGCQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07236v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DGCQA%3A%20A%20Quality%20Assessment%20Database%20for%203D%20AI-Generated%20Contents&entry.906535625=Yingjie%20Zhou%20and%20Zicheng%20Zhang%20and%20Farong%20Wen%20and%20Jun%20Jia%20and%20Yanwei%20Jiang%20and%20Xiaohong%20Liu%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai&entry.1292438233=%20%20Although%203D%20generated%20content%20%283DGC%29%20offers%20advantages%20in%20reducing%20production%0Acosts%20and%20accelerating%20design%20timelines%2C%20its%20quality%20often%20falls%20short%20when%0Acompared%20to%203D%20professionally%20generated%20content.%20Common%20quality%20issues%0Afrequently%20affect%203DGC%2C%20highlighting%20the%20importance%20of%20timely%20and%20effective%0Aquality%20assessment.%20Such%20evaluations%20not%20only%20ensure%20a%20higher%20standard%20of%203DGCs%0Afor%20end-users%20but%20also%20provide%20critical%20insights%20for%20advancing%20generative%0Atechnologies.%20To%20address%20existing%20gaps%20in%20this%20domain%2C%20this%20paper%20introduces%20a%0Anovel%203DGC%20quality%20assessment%20dataset%2C%203DGCQA%2C%20built%20using%207%20representative%0AText-to-3D%20generation%20methods.%20During%20the%20dataset%27s%20construction%2C%2050%20fixed%0Aprompts%20are%20utilized%20to%20generate%20contents%20across%20all%20methods%2C%20resulting%20in%20the%0Acreation%20of%20313%20textured%20meshes%20that%20constitute%20the%203DGCQA%20dataset.%20The%0Avisualization%20intuitively%20reveals%20the%20presence%20of%206%20common%20distortion%0Acategories%20in%20the%20generated%203DGCs.%20To%20further%20explore%20the%20quality%20of%20the%203DGCs%2C%0Asubjective%20quality%20assessment%20is%20conducted%20by%20evaluators%2C%20whose%20ratings%20reveal%0Asignificant%20variation%20in%20quality%20across%20different%20generation%20methods.%0AAdditionally%2C%20several%20objective%20quality%20assessment%20algorithms%20are%20tested%20on%20the%0A3DGCQA%20dataset.%20The%20results%20expose%20limitations%20in%20the%20performance%20of%20existing%0Aalgorithms%20and%20underscore%20the%20need%20for%20developing%20more%20specialized%20quality%0Aassessment%20methods.%20To%20provide%20a%20valuable%20resource%20for%20future%20research%20and%0Adevelopment%20in%203D%20content%20generation%20and%20quality%20assessment%2C%20the%20dataset%20has%0Abeen%20open-sourced%20in%20https%3A//github.com/zyj-2000/3DGCQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07236v1&entry.124074799=Read"},
{"title": "Three-Dimensional, Multimodal Synchrotron Data for Machine Learning\n  Applications", "author": "Calum Green and Sharif Ahmed and Shashidhara Marathe and Liam Perera and Alberto Leonardi and Killian Gmyrek and Daniele Dini and James Le Houx", "abstract": "  Machine learning techniques are being increasingly applied in medical and\nphysical sciences across a variety of imaging modalities; however, an important\nissue when developing these tools is the availability of good quality training\ndata. Here we present a unique, multimodal synchrotron dataset of a bespoke\nzinc-doped Zeolite 13X sample that can be used to develop advanced deep\nlearning and data fusion pipelines. Multi-resolution micro X-ray computed\ntomography was performed on a zinc-doped Zeolite 13X fragment to characterise\nits pores and features, before spatially resolved X-ray diffraction computed\ntomography was carried out to characterise the homogeneous distribution of\nsodium and zinc phases. Zinc absorption was controlled to create a simple,\nspatially isolated, two-phase material. Both raw and processed data is\navailable as a series of Zenodo entries. Altogether we present a spatially\nresolved, three-dimensional, multimodal, multi-resolution dataset that can be\nused for the development of machine learning techniques. Such techniques\ninclude development of super-resolution, multimodal data fusion, and 3D\nreconstruction algorithm development.\n", "link": "http://arxiv.org/abs/2409.07322v1", "date": "2024-09-11", "relevancy": 2.8791, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5928}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5928}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Three-Dimensional%2C%20Multimodal%20Synchrotron%20Data%20for%20Machine%20Learning%0A%20%20Applications&body=Title%3A%20Three-Dimensional%2C%20Multimodal%20Synchrotron%20Data%20for%20Machine%20Learning%0A%20%20Applications%0AAuthor%3A%20Calum%20Green%20and%20Sharif%20Ahmed%20and%20Shashidhara%20Marathe%20and%20Liam%20Perera%20and%20Alberto%20Leonardi%20and%20Killian%20Gmyrek%20and%20Daniele%20Dini%20and%20James%20Le%20Houx%0AAbstract%3A%20%20%20Machine%20learning%20techniques%20are%20being%20increasingly%20applied%20in%20medical%20and%0Aphysical%20sciences%20across%20a%20variety%20of%20imaging%20modalities%3B%20however%2C%20an%20important%0Aissue%20when%20developing%20these%20tools%20is%20the%20availability%20of%20good%20quality%20training%0Adata.%20Here%20we%20present%20a%20unique%2C%20multimodal%20synchrotron%20dataset%20of%20a%20bespoke%0Azinc-doped%20Zeolite%2013X%20sample%20that%20can%20be%20used%20to%20develop%20advanced%20deep%0Alearning%20and%20data%20fusion%20pipelines.%20Multi-resolution%20micro%20X-ray%20computed%0Atomography%20was%20performed%20on%20a%20zinc-doped%20Zeolite%2013X%20fragment%20to%20characterise%0Aits%20pores%20and%20features%2C%20before%20spatially%20resolved%20X-ray%20diffraction%20computed%0Atomography%20was%20carried%20out%20to%20characterise%20the%20homogeneous%20distribution%20of%0Asodium%20and%20zinc%20phases.%20Zinc%20absorption%20was%20controlled%20to%20create%20a%20simple%2C%0Aspatially%20isolated%2C%20two-phase%20material.%20Both%20raw%20and%20processed%20data%20is%0Aavailable%20as%20a%20series%20of%20Zenodo%20entries.%20Altogether%20we%20present%20a%20spatially%0Aresolved%2C%20three-dimensional%2C%20multimodal%2C%20multi-resolution%20dataset%20that%20can%20be%0Aused%20for%20the%20development%20of%20machine%20learning%20techniques.%20Such%20techniques%0Ainclude%20development%20of%20super-resolution%2C%20multimodal%20data%20fusion%2C%20and%203D%0Areconstruction%20algorithm%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07322v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThree-Dimensional%252C%2520Multimodal%2520Synchrotron%2520Data%2520for%2520Machine%2520Learning%250A%2520%2520Applications%26entry.906535625%3DCalum%2520Green%2520and%2520Sharif%2520Ahmed%2520and%2520Shashidhara%2520Marathe%2520and%2520Liam%2520Perera%2520and%2520Alberto%2520Leonardi%2520and%2520Killian%2520Gmyrek%2520and%2520Daniele%2520Dini%2520and%2520James%2520Le%2520Houx%26entry.1292438233%3D%2520%2520Machine%2520learning%2520techniques%2520are%2520being%2520increasingly%2520applied%2520in%2520medical%2520and%250Aphysical%2520sciences%2520across%2520a%2520variety%2520of%2520imaging%2520modalities%253B%2520however%252C%2520an%2520important%250Aissue%2520when%2520developing%2520these%2520tools%2520is%2520the%2520availability%2520of%2520good%2520quality%2520training%250Adata.%2520Here%2520we%2520present%2520a%2520unique%252C%2520multimodal%2520synchrotron%2520dataset%2520of%2520a%2520bespoke%250Azinc-doped%2520Zeolite%252013X%2520sample%2520that%2520can%2520be%2520used%2520to%2520develop%2520advanced%2520deep%250Alearning%2520and%2520data%2520fusion%2520pipelines.%2520Multi-resolution%2520micro%2520X-ray%2520computed%250Atomography%2520was%2520performed%2520on%2520a%2520zinc-doped%2520Zeolite%252013X%2520fragment%2520to%2520characterise%250Aits%2520pores%2520and%2520features%252C%2520before%2520spatially%2520resolved%2520X-ray%2520diffraction%2520computed%250Atomography%2520was%2520carried%2520out%2520to%2520characterise%2520the%2520homogeneous%2520distribution%2520of%250Asodium%2520and%2520zinc%2520phases.%2520Zinc%2520absorption%2520was%2520controlled%2520to%2520create%2520a%2520simple%252C%250Aspatially%2520isolated%252C%2520two-phase%2520material.%2520Both%2520raw%2520and%2520processed%2520data%2520is%250Aavailable%2520as%2520a%2520series%2520of%2520Zenodo%2520entries.%2520Altogether%2520we%2520present%2520a%2520spatially%250Aresolved%252C%2520three-dimensional%252C%2520multimodal%252C%2520multi-resolution%2520dataset%2520that%2520can%2520be%250Aused%2520for%2520the%2520development%2520of%2520machine%2520learning%2520techniques.%2520Such%2520techniques%250Ainclude%2520development%2520of%2520super-resolution%252C%2520multimodal%2520data%2520fusion%252C%2520and%25203D%250Areconstruction%2520algorithm%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07322v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Three-Dimensional%2C%20Multimodal%20Synchrotron%20Data%20for%20Machine%20Learning%0A%20%20Applications&entry.906535625=Calum%20Green%20and%20Sharif%20Ahmed%20and%20Shashidhara%20Marathe%20and%20Liam%20Perera%20and%20Alberto%20Leonardi%20and%20Killian%20Gmyrek%20and%20Daniele%20Dini%20and%20James%20Le%20Houx&entry.1292438233=%20%20Machine%20learning%20techniques%20are%20being%20increasingly%20applied%20in%20medical%20and%0Aphysical%20sciences%20across%20a%20variety%20of%20imaging%20modalities%3B%20however%2C%20an%20important%0Aissue%20when%20developing%20these%20tools%20is%20the%20availability%20of%20good%20quality%20training%0Adata.%20Here%20we%20present%20a%20unique%2C%20multimodal%20synchrotron%20dataset%20of%20a%20bespoke%0Azinc-doped%20Zeolite%2013X%20sample%20that%20can%20be%20used%20to%20develop%20advanced%20deep%0Alearning%20and%20data%20fusion%20pipelines.%20Multi-resolution%20micro%20X-ray%20computed%0Atomography%20was%20performed%20on%20a%20zinc-doped%20Zeolite%2013X%20fragment%20to%20characterise%0Aits%20pores%20and%20features%2C%20before%20spatially%20resolved%20X-ray%20diffraction%20computed%0Atomography%20was%20carried%20out%20to%20characterise%20the%20homogeneous%20distribution%20of%0Asodium%20and%20zinc%20phases.%20Zinc%20absorption%20was%20controlled%20to%20create%20a%20simple%2C%0Aspatially%20isolated%2C%20two-phase%20material.%20Both%20raw%20and%20processed%20data%20is%0Aavailable%20as%20a%20series%20of%20Zenodo%20entries.%20Altogether%20we%20present%20a%20spatially%0Aresolved%2C%20three-dimensional%2C%20multimodal%2C%20multi-resolution%20dataset%20that%20can%20be%0Aused%20for%20the%20development%20of%20machine%20learning%20techniques.%20Such%20techniques%0Ainclude%20development%20of%20super-resolution%2C%20multimodal%20data%20fusion%2C%20and%203D%0Areconstruction%20algorithm%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07322v1&entry.124074799=Read"},
{"title": "MyGo: Consistent and Controllable Multi-View Driving Video Generation\n  with Camera Control", "author": "Yining Yao and Xi Guo and Chenjing Ding and Wei Wu", "abstract": "  High-quality driving video generation is crucial for providing training data\nfor autonomous driving models. However, current generative models rarely focus\non enhancing camera motion control under multi-view tasks, which is essential\nfor driving video generation. Therefore, we propose MyGo, an end-to-end\nframework for video generation, introducing motion of onboard cameras as\nconditions to make progress in camera controllability and multi-view\nconsistency. MyGo employs additional plug-in modules to inject camera\nparameters into the pre-trained video diffusion model, which retains the\nextensive knowledge of the pre-trained model as much as possible. Furthermore,\nwe use epipolar constraints and neighbor view information during the generation\nprocess of each view to enhance spatial-temporal consistency. Experimental\nresults show that MyGo has achieved state-of-the-art results in both general\ncamera-controlled video generation and multi-view driving video generation\ntasks, which lays the foundation for more accurate environment simulation in\nautonomous driving. Project page:\nhttps://metadrivescape.github.io/papers_project/MyGo/page.html\n", "link": "http://arxiv.org/abs/2409.06189v2", "date": "2024-09-11", "relevancy": 2.7836, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5634}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5534}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MyGo%3A%20Consistent%20and%20Controllable%20Multi-View%20Driving%20Video%20Generation%0A%20%20with%20Camera%20Control&body=Title%3A%20MyGo%3A%20Consistent%20and%20Controllable%20Multi-View%20Driving%20Video%20Generation%0A%20%20with%20Camera%20Control%0AAuthor%3A%20Yining%20Yao%20and%20Xi%20Guo%20and%20Chenjing%20Ding%20and%20Wei%20Wu%0AAbstract%3A%20%20%20High-quality%20driving%20video%20generation%20is%20crucial%20for%20providing%20training%20data%0Afor%20autonomous%20driving%20models.%20However%2C%20current%20generative%20models%20rarely%20focus%0Aon%20enhancing%20camera%20motion%20control%20under%20multi-view%20tasks%2C%20which%20is%20essential%0Afor%20driving%20video%20generation.%20Therefore%2C%20we%20propose%20MyGo%2C%20an%20end-to-end%0Aframework%20for%20video%20generation%2C%20introducing%20motion%20of%20onboard%20cameras%20as%0Aconditions%20to%20make%20progress%20in%20camera%20controllability%20and%20multi-view%0Aconsistency.%20MyGo%20employs%20additional%20plug-in%20modules%20to%20inject%20camera%0Aparameters%20into%20the%20pre-trained%20video%20diffusion%20model%2C%20which%20retains%20the%0Aextensive%20knowledge%20of%20the%20pre-trained%20model%20as%20much%20as%20possible.%20Furthermore%2C%0Awe%20use%20epipolar%20constraints%20and%20neighbor%20view%20information%20during%20the%20generation%0Aprocess%20of%20each%20view%20to%20enhance%20spatial-temporal%20consistency.%20Experimental%0Aresults%20show%20that%20MyGo%20has%20achieved%20state-of-the-art%20results%20in%20both%20general%0Acamera-controlled%20video%20generation%20and%20multi-view%20driving%20video%20generation%0Atasks%2C%20which%20lays%20the%20foundation%20for%20more%20accurate%20environment%20simulation%20in%0Aautonomous%20driving.%20Project%20page%3A%0Ahttps%3A//metadrivescape.github.io/papers_project/MyGo/page.html%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06189v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMyGo%253A%2520Consistent%2520and%2520Controllable%2520Multi-View%2520Driving%2520Video%2520Generation%250A%2520%2520with%2520Camera%2520Control%26entry.906535625%3DYining%2520Yao%2520and%2520Xi%2520Guo%2520and%2520Chenjing%2520Ding%2520and%2520Wei%2520Wu%26entry.1292438233%3D%2520%2520High-quality%2520driving%2520video%2520generation%2520is%2520crucial%2520for%2520providing%2520training%2520data%250Afor%2520autonomous%2520driving%2520models.%2520However%252C%2520current%2520generative%2520models%2520rarely%2520focus%250Aon%2520enhancing%2520camera%2520motion%2520control%2520under%2520multi-view%2520tasks%252C%2520which%2520is%2520essential%250Afor%2520driving%2520video%2520generation.%2520Therefore%252C%2520we%2520propose%2520MyGo%252C%2520an%2520end-to-end%250Aframework%2520for%2520video%2520generation%252C%2520introducing%2520motion%2520of%2520onboard%2520cameras%2520as%250Aconditions%2520to%2520make%2520progress%2520in%2520camera%2520controllability%2520and%2520multi-view%250Aconsistency.%2520MyGo%2520employs%2520additional%2520plug-in%2520modules%2520to%2520inject%2520camera%250Aparameters%2520into%2520the%2520pre-trained%2520video%2520diffusion%2520model%252C%2520which%2520retains%2520the%250Aextensive%2520knowledge%2520of%2520the%2520pre-trained%2520model%2520as%2520much%2520as%2520possible.%2520Furthermore%252C%250Awe%2520use%2520epipolar%2520constraints%2520and%2520neighbor%2520view%2520information%2520during%2520the%2520generation%250Aprocess%2520of%2520each%2520view%2520to%2520enhance%2520spatial-temporal%2520consistency.%2520Experimental%250Aresults%2520show%2520that%2520MyGo%2520has%2520achieved%2520state-of-the-art%2520results%2520in%2520both%2520general%250Acamera-controlled%2520video%2520generation%2520and%2520multi-view%2520driving%2520video%2520generation%250Atasks%252C%2520which%2520lays%2520the%2520foundation%2520for%2520more%2520accurate%2520environment%2520simulation%2520in%250Aautonomous%2520driving.%2520Project%2520page%253A%250Ahttps%253A//metadrivescape.github.io/papers_project/MyGo/page.html%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06189v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MyGo%3A%20Consistent%20and%20Controllable%20Multi-View%20Driving%20Video%20Generation%0A%20%20with%20Camera%20Control&entry.906535625=Yining%20Yao%20and%20Xi%20Guo%20and%20Chenjing%20Ding%20and%20Wei%20Wu&entry.1292438233=%20%20High-quality%20driving%20video%20generation%20is%20crucial%20for%20providing%20training%20data%0Afor%20autonomous%20driving%20models.%20However%2C%20current%20generative%20models%20rarely%20focus%0Aon%20enhancing%20camera%20motion%20control%20under%20multi-view%20tasks%2C%20which%20is%20essential%0Afor%20driving%20video%20generation.%20Therefore%2C%20we%20propose%20MyGo%2C%20an%20end-to-end%0Aframework%20for%20video%20generation%2C%20introducing%20motion%20of%20onboard%20cameras%20as%0Aconditions%20to%20make%20progress%20in%20camera%20controllability%20and%20multi-view%0Aconsistency.%20MyGo%20employs%20additional%20plug-in%20modules%20to%20inject%20camera%0Aparameters%20into%20the%20pre-trained%20video%20diffusion%20model%2C%20which%20retains%20the%0Aextensive%20knowledge%20of%20the%20pre-trained%20model%20as%20much%20as%20possible.%20Furthermore%2C%0Awe%20use%20epipolar%20constraints%20and%20neighbor%20view%20information%20during%20the%20generation%0Aprocess%20of%20each%20view%20to%20enhance%20spatial-temporal%20consistency.%20Experimental%0Aresults%20show%20that%20MyGo%20has%20achieved%20state-of-the-art%20results%20in%20both%20general%0Acamera-controlled%20video%20generation%20and%20multi-view%20driving%20video%20generation%0Atasks%2C%20which%20lays%20the%20foundation%20for%20more%20accurate%20environment%20simulation%20in%0Aautonomous%20driving.%20Project%20page%3A%0Ahttps%3A//metadrivescape.github.io/papers_project/MyGo/page.html%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06189v2&entry.124074799=Read"},
{"title": "Alignist: CAD-Informed Orientation Distribution Estimation by Fusing\n  Shape and Correspondences", "author": "Shishir Reddy Vutukur and Rasmus Laurvig Haugaard and Junwen Huang and Benjamin Busam and Tolga Birdal", "abstract": "  Object pose distribution estimation is crucial in robotics for better path\nplanning and handling of symmetric objects. Recent distribution estimation\napproaches employ contrastive learning-based approaches by maximizing the\nlikelihood of a single pose estimate in the absence of a CAD model. We propose\na pose distribution estimation method leveraging symmetry respecting\ncorrespondence distributions and shape information obtained using a CAD model.\nContrastive learning-based approaches require an exhaustive amount of training\nimages from different viewpoints to learn the distribution properly, which is\nnot possible in realistic scenarios. Instead, we propose a pipeline that can\nleverage correspondence distributions and shape information from the CAD model,\nwhich are later used to learn pose distributions. Besides, having access to\npose distribution based on correspondences before learning pose distributions\nconditioned on images, can help formulate the loss between distributions. The\nprior knowledge of distribution also helps the network to focus on getting\nsharper modes instead. With the CAD prior, our approach converges much faster\nand learns distribution better by focusing on learning sharper distribution\nnear all the valid modes, unlike contrastive approaches, which focus on a\nsingle mode at a time. We achieve benchmark results on SYMSOL-I and T-Less\ndatasets.\n", "link": "http://arxiv.org/abs/2409.06683v2", "date": "2024-09-11", "relevancy": 2.7677, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5571}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5556}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Alignist%3A%20CAD-Informed%20Orientation%20Distribution%20Estimation%20by%20Fusing%0A%20%20Shape%20and%20Correspondences&body=Title%3A%20Alignist%3A%20CAD-Informed%20Orientation%20Distribution%20Estimation%20by%20Fusing%0A%20%20Shape%20and%20Correspondences%0AAuthor%3A%20Shishir%20Reddy%20Vutukur%20and%20Rasmus%20Laurvig%20Haugaard%20and%20Junwen%20Huang%20and%20Benjamin%20Busam%20and%20Tolga%20Birdal%0AAbstract%3A%20%20%20Object%20pose%20distribution%20estimation%20is%20crucial%20in%20robotics%20for%20better%20path%0Aplanning%20and%20handling%20of%20symmetric%20objects.%20Recent%20distribution%20estimation%0Aapproaches%20employ%20contrastive%20learning-based%20approaches%20by%20maximizing%20the%0Alikelihood%20of%20a%20single%20pose%20estimate%20in%20the%20absence%20of%20a%20CAD%20model.%20We%20propose%0Aa%20pose%20distribution%20estimation%20method%20leveraging%20symmetry%20respecting%0Acorrespondence%20distributions%20and%20shape%20information%20obtained%20using%20a%20CAD%20model.%0AContrastive%20learning-based%20approaches%20require%20an%20exhaustive%20amount%20of%20training%0Aimages%20from%20different%20viewpoints%20to%20learn%20the%20distribution%20properly%2C%20which%20is%0Anot%20possible%20in%20realistic%20scenarios.%20Instead%2C%20we%20propose%20a%20pipeline%20that%20can%0Aleverage%20correspondence%20distributions%20and%20shape%20information%20from%20the%20CAD%20model%2C%0Awhich%20are%20later%20used%20to%20learn%20pose%20distributions.%20Besides%2C%20having%20access%20to%0Apose%20distribution%20based%20on%20correspondences%20before%20learning%20pose%20distributions%0Aconditioned%20on%20images%2C%20can%20help%20formulate%20the%20loss%20between%20distributions.%20The%0Aprior%20knowledge%20of%20distribution%20also%20helps%20the%20network%20to%20focus%20on%20getting%0Asharper%20modes%20instead.%20With%20the%20CAD%20prior%2C%20our%20approach%20converges%20much%20faster%0Aand%20learns%20distribution%20better%20by%20focusing%20on%20learning%20sharper%20distribution%0Anear%20all%20the%20valid%20modes%2C%20unlike%20contrastive%20approaches%2C%20which%20focus%20on%20a%0Asingle%20mode%20at%20a%20time.%20We%20achieve%20benchmark%20results%20on%20SYMSOL-I%20and%20T-Less%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06683v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlignist%253A%2520CAD-Informed%2520Orientation%2520Distribution%2520Estimation%2520by%2520Fusing%250A%2520%2520Shape%2520and%2520Correspondences%26entry.906535625%3DShishir%2520Reddy%2520Vutukur%2520and%2520Rasmus%2520Laurvig%2520Haugaard%2520and%2520Junwen%2520Huang%2520and%2520Benjamin%2520Busam%2520and%2520Tolga%2520Birdal%26entry.1292438233%3D%2520%2520Object%2520pose%2520distribution%2520estimation%2520is%2520crucial%2520in%2520robotics%2520for%2520better%2520path%250Aplanning%2520and%2520handling%2520of%2520symmetric%2520objects.%2520Recent%2520distribution%2520estimation%250Aapproaches%2520employ%2520contrastive%2520learning-based%2520approaches%2520by%2520maximizing%2520the%250Alikelihood%2520of%2520a%2520single%2520pose%2520estimate%2520in%2520the%2520absence%2520of%2520a%2520CAD%2520model.%2520We%2520propose%250Aa%2520pose%2520distribution%2520estimation%2520method%2520leveraging%2520symmetry%2520respecting%250Acorrespondence%2520distributions%2520and%2520shape%2520information%2520obtained%2520using%2520a%2520CAD%2520model.%250AContrastive%2520learning-based%2520approaches%2520require%2520an%2520exhaustive%2520amount%2520of%2520training%250Aimages%2520from%2520different%2520viewpoints%2520to%2520learn%2520the%2520distribution%2520properly%252C%2520which%2520is%250Anot%2520possible%2520in%2520realistic%2520scenarios.%2520Instead%252C%2520we%2520propose%2520a%2520pipeline%2520that%2520can%250Aleverage%2520correspondence%2520distributions%2520and%2520shape%2520information%2520from%2520the%2520CAD%2520model%252C%250Awhich%2520are%2520later%2520used%2520to%2520learn%2520pose%2520distributions.%2520Besides%252C%2520having%2520access%2520to%250Apose%2520distribution%2520based%2520on%2520correspondences%2520before%2520learning%2520pose%2520distributions%250Aconditioned%2520on%2520images%252C%2520can%2520help%2520formulate%2520the%2520loss%2520between%2520distributions.%2520The%250Aprior%2520knowledge%2520of%2520distribution%2520also%2520helps%2520the%2520network%2520to%2520focus%2520on%2520getting%250Asharper%2520modes%2520instead.%2520With%2520the%2520CAD%2520prior%252C%2520our%2520approach%2520converges%2520much%2520faster%250Aand%2520learns%2520distribution%2520better%2520by%2520focusing%2520on%2520learning%2520sharper%2520distribution%250Anear%2520all%2520the%2520valid%2520modes%252C%2520unlike%2520contrastive%2520approaches%252C%2520which%2520focus%2520on%2520a%250Asingle%2520mode%2520at%2520a%2520time.%2520We%2520achieve%2520benchmark%2520results%2520on%2520SYMSOL-I%2520and%2520T-Less%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06683v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alignist%3A%20CAD-Informed%20Orientation%20Distribution%20Estimation%20by%20Fusing%0A%20%20Shape%20and%20Correspondences&entry.906535625=Shishir%20Reddy%20Vutukur%20and%20Rasmus%20Laurvig%20Haugaard%20and%20Junwen%20Huang%20and%20Benjamin%20Busam%20and%20Tolga%20Birdal&entry.1292438233=%20%20Object%20pose%20distribution%20estimation%20is%20crucial%20in%20robotics%20for%20better%20path%0Aplanning%20and%20handling%20of%20symmetric%20objects.%20Recent%20distribution%20estimation%0Aapproaches%20employ%20contrastive%20learning-based%20approaches%20by%20maximizing%20the%0Alikelihood%20of%20a%20single%20pose%20estimate%20in%20the%20absence%20of%20a%20CAD%20model.%20We%20propose%0Aa%20pose%20distribution%20estimation%20method%20leveraging%20symmetry%20respecting%0Acorrespondence%20distributions%20and%20shape%20information%20obtained%20using%20a%20CAD%20model.%0AContrastive%20learning-based%20approaches%20require%20an%20exhaustive%20amount%20of%20training%0Aimages%20from%20different%20viewpoints%20to%20learn%20the%20distribution%20properly%2C%20which%20is%0Anot%20possible%20in%20realistic%20scenarios.%20Instead%2C%20we%20propose%20a%20pipeline%20that%20can%0Aleverage%20correspondence%20distributions%20and%20shape%20information%20from%20the%20CAD%20model%2C%0Awhich%20are%20later%20used%20to%20learn%20pose%20distributions.%20Besides%2C%20having%20access%20to%0Apose%20distribution%20based%20on%20correspondences%20before%20learning%20pose%20distributions%0Aconditioned%20on%20images%2C%20can%20help%20formulate%20the%20loss%20between%20distributions.%20The%0Aprior%20knowledge%20of%20distribution%20also%20helps%20the%20network%20to%20focus%20on%20getting%0Asharper%20modes%20instead.%20With%20the%20CAD%20prior%2C%20our%20approach%20converges%20much%20faster%0Aand%20learns%20distribution%20better%20by%20focusing%20on%20learning%20sharper%20distribution%0Anear%20all%20the%20valid%20modes%2C%20unlike%20contrastive%20approaches%2C%20which%20focus%20on%20a%0Asingle%20mode%20at%20a%20time.%20We%20achieve%20benchmark%20results%20on%20SYMSOL-I%20and%20T-Less%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06683v2&entry.124074799=Read"},
{"title": "Securing Vision-Language Models with a Robust Encoder Against Jailbreak\n  and Adversarial Attacks", "author": "Md Zarif Hossain and Ahmed Imteaj", "abstract": "  Large Vision-Language Models (LVLMs), trained on multimodal big datasets,\nhave significantly advanced AI by excelling in vision-language tasks. However,\nthese models remain vulnerable to adversarial attacks, particularly jailbreak\nattacks, which bypass safety protocols and cause the model to generate\nmisleading or harmful responses. This vulnerability stems from both the\ninherent susceptibilities of LLMs and the expanded attack surface introduced by\nthe visual modality. We propose Sim-CLIP+, a novel defense mechanism that\nadversarially fine-tunes the CLIP vision encoder by leveraging a Siamese\narchitecture. This approach maximizes cosine similarity between perturbed and\nclean samples, facilitating resilience against adversarial manipulations.\nSim-CLIP+ offers a plug-and-play solution, allowing seamless integration into\nexisting LVLM architectures as a robust vision encoder. Unlike previous\ndefenses, our method requires no structural modifications to the LVLM and\nincurs minimal computational overhead. Sim-CLIP+ demonstrates effectiveness\nagainst both gradient-based adversarial attacks and various jailbreak\ntechniques. We evaluate Sim-CLIP+ against three distinct jailbreak attack\nstrategies and perform clean evaluations using standard downstream datasets,\nincluding COCO for image captioning and OKVQA for visual question answering.\nExtensive experiments demonstrate that Sim-CLIP+ maintains high clean accuracy\nwhile substantially improving robustness against both gradient-based\nadversarial attacks and jailbreak techniques. Our code and robust vision\nencoders are available at\nhttps://github.com/speedlab-git/Robust-Encoder-against-Jailbreak-attack.git.\n", "link": "http://arxiv.org/abs/2409.07353v1", "date": "2024-09-11", "relevancy": 2.7656, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5604}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5604}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5385}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Securing%20Vision-Language%20Models%20with%20a%20Robust%20Encoder%20Against%20Jailbreak%0A%20%20and%20Adversarial%20Attacks&body=Title%3A%20Securing%20Vision-Language%20Models%20with%20a%20Robust%20Encoder%20Against%20Jailbreak%0A%20%20and%20Adversarial%20Attacks%0AAuthor%3A%20Md%20Zarif%20Hossain%20and%20Ahmed%20Imteaj%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%2C%20trained%20on%20multimodal%20big%20datasets%2C%0Ahave%20significantly%20advanced%20AI%20by%20excelling%20in%20vision-language%20tasks.%20However%2C%0Athese%20models%20remain%20vulnerable%20to%20adversarial%20attacks%2C%20particularly%20jailbreak%0Aattacks%2C%20which%20bypass%20safety%20protocols%20and%20cause%20the%20model%20to%20generate%0Amisleading%20or%20harmful%20responses.%20This%20vulnerability%20stems%20from%20both%20the%0Ainherent%20susceptibilities%20of%20LLMs%20and%20the%20expanded%20attack%20surface%20introduced%20by%0Athe%20visual%20modality.%20We%20propose%20Sim-CLIP%2B%2C%20a%20novel%20defense%20mechanism%20that%0Aadversarially%20fine-tunes%20the%20CLIP%20vision%20encoder%20by%20leveraging%20a%20Siamese%0Aarchitecture.%20This%20approach%20maximizes%20cosine%20similarity%20between%20perturbed%20and%0Aclean%20samples%2C%20facilitating%20resilience%20against%20adversarial%20manipulations.%0ASim-CLIP%2B%20offers%20a%20plug-and-play%20solution%2C%20allowing%20seamless%20integration%20into%0Aexisting%20LVLM%20architectures%20as%20a%20robust%20vision%20encoder.%20Unlike%20previous%0Adefenses%2C%20our%20method%20requires%20no%20structural%20modifications%20to%20the%20LVLM%20and%0Aincurs%20minimal%20computational%20overhead.%20Sim-CLIP%2B%20demonstrates%20effectiveness%0Aagainst%20both%20gradient-based%20adversarial%20attacks%20and%20various%20jailbreak%0Atechniques.%20We%20evaluate%20Sim-CLIP%2B%20against%20three%20distinct%20jailbreak%20attack%0Astrategies%20and%20perform%20clean%20evaluations%20using%20standard%20downstream%20datasets%2C%0Aincluding%20COCO%20for%20image%20captioning%20and%20OKVQA%20for%20visual%20question%20answering.%0AExtensive%20experiments%20demonstrate%20that%20Sim-CLIP%2B%20maintains%20high%20clean%20accuracy%0Awhile%20substantially%20improving%20robustness%20against%20both%20gradient-based%0Aadversarial%20attacks%20and%20jailbreak%20techniques.%20Our%20code%20and%20robust%20vision%0Aencoders%20are%20available%20at%0Ahttps%3A//github.com/speedlab-git/Robust-Encoder-against-Jailbreak-attack.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07353v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSecuring%2520Vision-Language%2520Models%2520with%2520a%2520Robust%2520Encoder%2520Against%2520Jailbreak%250A%2520%2520and%2520Adversarial%2520Attacks%26entry.906535625%3DMd%2520Zarif%2520Hossain%2520and%2520Ahmed%2520Imteaj%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%252C%2520trained%2520on%2520multimodal%2520big%2520datasets%252C%250Ahave%2520significantly%2520advanced%2520AI%2520by%2520excelling%2520in%2520vision-language%2520tasks.%2520However%252C%250Athese%2520models%2520remain%2520vulnerable%2520to%2520adversarial%2520attacks%252C%2520particularly%2520jailbreak%250Aattacks%252C%2520which%2520bypass%2520safety%2520protocols%2520and%2520cause%2520the%2520model%2520to%2520generate%250Amisleading%2520or%2520harmful%2520responses.%2520This%2520vulnerability%2520stems%2520from%2520both%2520the%250Ainherent%2520susceptibilities%2520of%2520LLMs%2520and%2520the%2520expanded%2520attack%2520surface%2520introduced%2520by%250Athe%2520visual%2520modality.%2520We%2520propose%2520Sim-CLIP%252B%252C%2520a%2520novel%2520defense%2520mechanism%2520that%250Aadversarially%2520fine-tunes%2520the%2520CLIP%2520vision%2520encoder%2520by%2520leveraging%2520a%2520Siamese%250Aarchitecture.%2520This%2520approach%2520maximizes%2520cosine%2520similarity%2520between%2520perturbed%2520and%250Aclean%2520samples%252C%2520facilitating%2520resilience%2520against%2520adversarial%2520manipulations.%250ASim-CLIP%252B%2520offers%2520a%2520plug-and-play%2520solution%252C%2520allowing%2520seamless%2520integration%2520into%250Aexisting%2520LVLM%2520architectures%2520as%2520a%2520robust%2520vision%2520encoder.%2520Unlike%2520previous%250Adefenses%252C%2520our%2520method%2520requires%2520no%2520structural%2520modifications%2520to%2520the%2520LVLM%2520and%250Aincurs%2520minimal%2520computational%2520overhead.%2520Sim-CLIP%252B%2520demonstrates%2520effectiveness%250Aagainst%2520both%2520gradient-based%2520adversarial%2520attacks%2520and%2520various%2520jailbreak%250Atechniques.%2520We%2520evaluate%2520Sim-CLIP%252B%2520against%2520three%2520distinct%2520jailbreak%2520attack%250Astrategies%2520and%2520perform%2520clean%2520evaluations%2520using%2520standard%2520downstream%2520datasets%252C%250Aincluding%2520COCO%2520for%2520image%2520captioning%2520and%2520OKVQA%2520for%2520visual%2520question%2520answering.%250AExtensive%2520experiments%2520demonstrate%2520that%2520Sim-CLIP%252B%2520maintains%2520high%2520clean%2520accuracy%250Awhile%2520substantially%2520improving%2520robustness%2520against%2520both%2520gradient-based%250Aadversarial%2520attacks%2520and%2520jailbreak%2520techniques.%2520Our%2520code%2520and%2520robust%2520vision%250Aencoders%2520are%2520available%2520at%250Ahttps%253A//github.com/speedlab-git/Robust-Encoder-against-Jailbreak-attack.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07353v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Securing%20Vision-Language%20Models%20with%20a%20Robust%20Encoder%20Against%20Jailbreak%0A%20%20and%20Adversarial%20Attacks&entry.906535625=Md%20Zarif%20Hossain%20and%20Ahmed%20Imteaj&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%2C%20trained%20on%20multimodal%20big%20datasets%2C%0Ahave%20significantly%20advanced%20AI%20by%20excelling%20in%20vision-language%20tasks.%20However%2C%0Athese%20models%20remain%20vulnerable%20to%20adversarial%20attacks%2C%20particularly%20jailbreak%0Aattacks%2C%20which%20bypass%20safety%20protocols%20and%20cause%20the%20model%20to%20generate%0Amisleading%20or%20harmful%20responses.%20This%20vulnerability%20stems%20from%20both%20the%0Ainherent%20susceptibilities%20of%20LLMs%20and%20the%20expanded%20attack%20surface%20introduced%20by%0Athe%20visual%20modality.%20We%20propose%20Sim-CLIP%2B%2C%20a%20novel%20defense%20mechanism%20that%0Aadversarially%20fine-tunes%20the%20CLIP%20vision%20encoder%20by%20leveraging%20a%20Siamese%0Aarchitecture.%20This%20approach%20maximizes%20cosine%20similarity%20between%20perturbed%20and%0Aclean%20samples%2C%20facilitating%20resilience%20against%20adversarial%20manipulations.%0ASim-CLIP%2B%20offers%20a%20plug-and-play%20solution%2C%20allowing%20seamless%20integration%20into%0Aexisting%20LVLM%20architectures%20as%20a%20robust%20vision%20encoder.%20Unlike%20previous%0Adefenses%2C%20our%20method%20requires%20no%20structural%20modifications%20to%20the%20LVLM%20and%0Aincurs%20minimal%20computational%20overhead.%20Sim-CLIP%2B%20demonstrates%20effectiveness%0Aagainst%20both%20gradient-based%20adversarial%20attacks%20and%20various%20jailbreak%0Atechniques.%20We%20evaluate%20Sim-CLIP%2B%20against%20three%20distinct%20jailbreak%20attack%0Astrategies%20and%20perform%20clean%20evaluations%20using%20standard%20downstream%20datasets%2C%0Aincluding%20COCO%20for%20image%20captioning%20and%20OKVQA%20for%20visual%20question%20answering.%0AExtensive%20experiments%20demonstrate%20that%20Sim-CLIP%2B%20maintains%20high%20clean%20accuracy%0Awhile%20substantially%20improving%20robustness%20against%20both%20gradient-based%0Aadversarial%20attacks%20and%20jailbreak%20techniques.%20Our%20code%20and%20robust%20vision%0Aencoders%20are%20available%20at%0Ahttps%3A//github.com/speedlab-git/Robust-Encoder-against-Jailbreak-attack.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07353v1&entry.124074799=Read"},
{"title": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering", "author": "Weixi Weng and Jieming Zhu and Hao Zhang and Xiaojun Meng and Rui Zhang and Chun Yuan", "abstract": "  Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot\nperformance on visual question answering (VQA). However, when it comes to\nknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized\ndomain knowledge to answer such questions and require obtaining necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts\n(RACC). RACC learns to compress and aggregate retrieved contexts, from which it\ngenerates a compact modulation in the form of Key-Value (KV) cache. This\nmodulation is then used to adapt the downstream frozen MLLM, thereby achieving\neffective and efficient inference. RACC achieves a state-of-the-art (SOTA)\nperformance of 62.9% on OK-VQA. Moreover, it significantly reduces inference\nlatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments\nshow RACC's broad applicability. It is compatible with various off-the-shelf\nMLLMs and can also handle different knowledge sources including textual and\nmultimodal documents.\n", "link": "http://arxiv.org/abs/2409.07331v1", "date": "2024-09-11", "relevancy": 2.7585, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5657}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5657}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Compress%20Contexts%20for%20Efficient%20Knowledge-based%20Visual%0A%20%20Question%20Answering&body=Title%3A%20Learning%20to%20Compress%20Contexts%20for%20Efficient%20Knowledge-based%20Visual%0A%20%20Question%20Answering%0AAuthor%3A%20Weixi%20Weng%20and%20Jieming%20Zhu%20and%20Hao%20Zhang%20and%20Xiaojun%20Meng%20and%20Rui%20Zhang%20and%20Chun%20Yuan%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20great%20zero-shot%0Aperformance%20on%20visual%20question%20answering%20%28VQA%29.%20However%2C%20when%20it%20comes%20to%0Aknowledge-based%20VQA%20%28KB-VQA%29%2C%20MLLMs%20may%20lack%20human%20commonsense%20or%20specialized%0Adomain%20knowledge%20to%20answer%20such%20questions%20and%20require%20obtaining%20necessary%0Ainformation%20from%20external%20knowledge%20sources.%20Previous%20works%20like%0ARetrival-Augmented%20VQA-v2%20%28RAVQA-v2%29%20focus%20on%20utilizing%20as%20much%20input%0Ainformation%2C%20such%20as%20image-based%20textual%20descriptions%20and%20retrieved%20knowledge%2C%0Aas%20possible%20to%20improve%20performance%2C%20but%20they%20all%20overlook%20the%20issue%20that%20with%0Athe%20number%20of%20input%20tokens%20increasing%2C%20inference%20efficiency%20significantly%0Adecreases%2C%20which%20contradicts%20the%20demands%20of%20practical%20applications.%20To%20address%0Athis%20issue%2C%20we%20propose%20Retrieval-Augmented%20MLLM%20with%20Compressed%20Contexts%0A%28RACC%29.%20RACC%20learns%20to%20compress%20and%20aggregate%20retrieved%20contexts%2C%20from%20which%20it%0Agenerates%20a%20compact%20modulation%20in%20the%20form%20of%20Key-Value%20%28KV%29%20cache.%20This%0Amodulation%20is%20then%20used%20to%20adapt%20the%20downstream%20frozen%20MLLM%2C%20thereby%20achieving%0Aeffective%20and%20efficient%20inference.%20RACC%20achieves%20a%20state-of-the-art%20%28SOTA%29%0Aperformance%20of%2062.9%25%20on%20OK-VQA.%20Moreover%2C%20it%20significantly%20reduces%20inference%0Alatency%20by%2022.0%25-59.7%25%20compared%20to%20the%20prominent%20RAVQA-v2.%20Abundant%20experiments%0Ashow%20RACC%27s%20broad%20applicability.%20It%20is%20compatible%20with%20various%20off-the-shelf%0AMLLMs%20and%20can%20also%20handle%20different%20knowledge%20sources%20including%20textual%20and%0Amultimodal%20documents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07331v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Compress%2520Contexts%2520for%2520Efficient%2520Knowledge-based%2520Visual%250A%2520%2520Question%2520Answering%26entry.906535625%3DWeixi%2520Weng%2520and%2520Jieming%2520Zhu%2520and%2520Hao%2520Zhang%2520and%2520Xiaojun%2520Meng%2520and%2520Rui%2520Zhang%2520and%2520Chun%2520Yuan%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520great%2520zero-shot%250Aperformance%2520on%2520visual%2520question%2520answering%2520%2528VQA%2529.%2520However%252C%2520when%2520it%2520comes%2520to%250Aknowledge-based%2520VQA%2520%2528KB-VQA%2529%252C%2520MLLMs%2520may%2520lack%2520human%2520commonsense%2520or%2520specialized%250Adomain%2520knowledge%2520to%2520answer%2520such%2520questions%2520and%2520require%2520obtaining%2520necessary%250Ainformation%2520from%2520external%2520knowledge%2520sources.%2520Previous%2520works%2520like%250ARetrival-Augmented%2520VQA-v2%2520%2528RAVQA-v2%2529%2520focus%2520on%2520utilizing%2520as%2520much%2520input%250Ainformation%252C%2520such%2520as%2520image-based%2520textual%2520descriptions%2520and%2520retrieved%2520knowledge%252C%250Aas%2520possible%2520to%2520improve%2520performance%252C%2520but%2520they%2520all%2520overlook%2520the%2520issue%2520that%2520with%250Athe%2520number%2520of%2520input%2520tokens%2520increasing%252C%2520inference%2520efficiency%2520significantly%250Adecreases%252C%2520which%2520contradicts%2520the%2520demands%2520of%2520practical%2520applications.%2520To%2520address%250Athis%2520issue%252C%2520we%2520propose%2520Retrieval-Augmented%2520MLLM%2520with%2520Compressed%2520Contexts%250A%2528RACC%2529.%2520RACC%2520learns%2520to%2520compress%2520and%2520aggregate%2520retrieved%2520contexts%252C%2520from%2520which%2520it%250Agenerates%2520a%2520compact%2520modulation%2520in%2520the%2520form%2520of%2520Key-Value%2520%2528KV%2529%2520cache.%2520This%250Amodulation%2520is%2520then%2520used%2520to%2520adapt%2520the%2520downstream%2520frozen%2520MLLM%252C%2520thereby%2520achieving%250Aeffective%2520and%2520efficient%2520inference.%2520RACC%2520achieves%2520a%2520state-of-the-art%2520%2528SOTA%2529%250Aperformance%2520of%252062.9%2525%2520on%2520OK-VQA.%2520Moreover%252C%2520it%2520significantly%2520reduces%2520inference%250Alatency%2520by%252022.0%2525-59.7%2525%2520compared%2520to%2520the%2520prominent%2520RAVQA-v2.%2520Abundant%2520experiments%250Ashow%2520RACC%2527s%2520broad%2520applicability.%2520It%2520is%2520compatible%2520with%2520various%2520off-the-shelf%250AMLLMs%2520and%2520can%2520also%2520handle%2520different%2520knowledge%2520sources%2520including%2520textual%2520and%250Amultimodal%2520documents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07331v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Compress%20Contexts%20for%20Efficient%20Knowledge-based%20Visual%0A%20%20Question%20Answering&entry.906535625=Weixi%20Weng%20and%20Jieming%20Zhu%20and%20Hao%20Zhang%20and%20Xiaojun%20Meng%20and%20Rui%20Zhang%20and%20Chun%20Yuan&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20great%20zero-shot%0Aperformance%20on%20visual%20question%20answering%20%28VQA%29.%20However%2C%20when%20it%20comes%20to%0Aknowledge-based%20VQA%20%28KB-VQA%29%2C%20MLLMs%20may%20lack%20human%20commonsense%20or%20specialized%0Adomain%20knowledge%20to%20answer%20such%20questions%20and%20require%20obtaining%20necessary%0Ainformation%20from%20external%20knowledge%20sources.%20Previous%20works%20like%0ARetrival-Augmented%20VQA-v2%20%28RAVQA-v2%29%20focus%20on%20utilizing%20as%20much%20input%0Ainformation%2C%20such%20as%20image-based%20textual%20descriptions%20and%20retrieved%20knowledge%2C%0Aas%20possible%20to%20improve%20performance%2C%20but%20they%20all%20overlook%20the%20issue%20that%20with%0Athe%20number%20of%20input%20tokens%20increasing%2C%20inference%20efficiency%20significantly%0Adecreases%2C%20which%20contradicts%20the%20demands%20of%20practical%20applications.%20To%20address%0Athis%20issue%2C%20we%20propose%20Retrieval-Augmented%20MLLM%20with%20Compressed%20Contexts%0A%28RACC%29.%20RACC%20learns%20to%20compress%20and%20aggregate%20retrieved%20contexts%2C%20from%20which%20it%0Agenerates%20a%20compact%20modulation%20in%20the%20form%20of%20Key-Value%20%28KV%29%20cache.%20This%0Amodulation%20is%20then%20used%20to%20adapt%20the%20downstream%20frozen%20MLLM%2C%20thereby%20achieving%0Aeffective%20and%20efficient%20inference.%20RACC%20achieves%20a%20state-of-the-art%20%28SOTA%29%0Aperformance%20of%2062.9%25%20on%20OK-VQA.%20Moreover%2C%20it%20significantly%20reduces%20inference%0Alatency%20by%2022.0%25-59.7%25%20compared%20to%20the%20prominent%20RAVQA-v2.%20Abundant%20experiments%0Ashow%20RACC%27s%20broad%20applicability.%20It%20is%20compatible%20with%20various%20off-the-shelf%0AMLLMs%20and%20can%20also%20handle%20different%20knowledge%20sources%20including%20textual%20and%0Amultimodal%20documents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07331v1&entry.124074799=Read"},
{"title": "Event-based Mosaicing Bundle Adjustment", "author": "Shuang Guo and Guillermo Gallego", "abstract": "  We tackle the problem of mosaicing bundle adjustment (i.e., simultaneous\nrefinement of camera orientations and scene map) for a purely rotating event\ncamera. We formulate the problem as a regularized non-linear least squares\noptimization. The objective function is defined using the linearized event\ngeneration model in the camera orientations and the panoramic gradient map of\nthe scene. We show that this BA optimization has an exploitable block-diagonal\nsparsity structure, so that the problem can be solved efficiently. To the best\nof our knowledge, this is the first work to leverage such sparsity to speed up\nthe optimization in the context of event-based cameras, without the need to\nconvert events into image-like representations. We evaluate our method, called\nEMBA, on both synthetic and real-world datasets to show its effectiveness (50%\nphotometric error decrease), yielding results of unprecedented quality. In\naddition, we demonstrate EMBA using high spatial resolution event cameras,\nyielding delicate panoramas in the wild, even without an initial map. Project\npage: https://github.com/tub-rip/emba\n", "link": "http://arxiv.org/abs/2409.07365v1", "date": "2024-09-11", "relevancy": 2.7548, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5679}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5581}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Event-based%20Mosaicing%20Bundle%20Adjustment&body=Title%3A%20Event-based%20Mosaicing%20Bundle%20Adjustment%0AAuthor%3A%20Shuang%20Guo%20and%20Guillermo%20Gallego%0AAbstract%3A%20%20%20We%20tackle%20the%20problem%20of%20mosaicing%20bundle%20adjustment%20%28i.e.%2C%20simultaneous%0Arefinement%20of%20camera%20orientations%20and%20scene%20map%29%20for%20a%20purely%20rotating%20event%0Acamera.%20We%20formulate%20the%20problem%20as%20a%20regularized%20non-linear%20least%20squares%0Aoptimization.%20The%20objective%20function%20is%20defined%20using%20the%20linearized%20event%0Ageneration%20model%20in%20the%20camera%20orientations%20and%20the%20panoramic%20gradient%20map%20of%0Athe%20scene.%20We%20show%20that%20this%20BA%20optimization%20has%20an%20exploitable%20block-diagonal%0Asparsity%20structure%2C%20so%20that%20the%20problem%20can%20be%20solved%20efficiently.%20To%20the%20best%0Aof%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%20leverage%20such%20sparsity%20to%20speed%20up%0Athe%20optimization%20in%20the%20context%20of%20event-based%20cameras%2C%20without%20the%20need%20to%0Aconvert%20events%20into%20image-like%20representations.%20We%20evaluate%20our%20method%2C%20called%0AEMBA%2C%20on%20both%20synthetic%20and%20real-world%20datasets%20to%20show%20its%20effectiveness%20%2850%25%0Aphotometric%20error%20decrease%29%2C%20yielding%20results%20of%20unprecedented%20quality.%20In%0Aaddition%2C%20we%20demonstrate%20EMBA%20using%20high%20spatial%20resolution%20event%20cameras%2C%0Ayielding%20delicate%20panoramas%20in%20the%20wild%2C%20even%20without%20an%20initial%20map.%20Project%0Apage%3A%20https%3A//github.com/tub-rip/emba%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07365v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvent-based%2520Mosaicing%2520Bundle%2520Adjustment%26entry.906535625%3DShuang%2520Guo%2520and%2520Guillermo%2520Gallego%26entry.1292438233%3D%2520%2520We%2520tackle%2520the%2520problem%2520of%2520mosaicing%2520bundle%2520adjustment%2520%2528i.e.%252C%2520simultaneous%250Arefinement%2520of%2520camera%2520orientations%2520and%2520scene%2520map%2529%2520for%2520a%2520purely%2520rotating%2520event%250Acamera.%2520We%2520formulate%2520the%2520problem%2520as%2520a%2520regularized%2520non-linear%2520least%2520squares%250Aoptimization.%2520The%2520objective%2520function%2520is%2520defined%2520using%2520the%2520linearized%2520event%250Ageneration%2520model%2520in%2520the%2520camera%2520orientations%2520and%2520the%2520panoramic%2520gradient%2520map%2520of%250Athe%2520scene.%2520We%2520show%2520that%2520this%2520BA%2520optimization%2520has%2520an%2520exploitable%2520block-diagonal%250Asparsity%2520structure%252C%2520so%2520that%2520the%2520problem%2520can%2520be%2520solved%2520efficiently.%2520To%2520the%2520best%250Aof%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520work%2520to%2520leverage%2520such%2520sparsity%2520to%2520speed%2520up%250Athe%2520optimization%2520in%2520the%2520context%2520of%2520event-based%2520cameras%252C%2520without%2520the%2520need%2520to%250Aconvert%2520events%2520into%2520image-like%2520representations.%2520We%2520evaluate%2520our%2520method%252C%2520called%250AEMBA%252C%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%2520to%2520show%2520its%2520effectiveness%2520%252850%2525%250Aphotometric%2520error%2520decrease%2529%252C%2520yielding%2520results%2520of%2520unprecedented%2520quality.%2520In%250Aaddition%252C%2520we%2520demonstrate%2520EMBA%2520using%2520high%2520spatial%2520resolution%2520event%2520cameras%252C%250Ayielding%2520delicate%2520panoramas%2520in%2520the%2520wild%252C%2520even%2520without%2520an%2520initial%2520map.%2520Project%250Apage%253A%2520https%253A//github.com/tub-rip/emba%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07365v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Event-based%20Mosaicing%20Bundle%20Adjustment&entry.906535625=Shuang%20Guo%20and%20Guillermo%20Gallego&entry.1292438233=%20%20We%20tackle%20the%20problem%20of%20mosaicing%20bundle%20adjustment%20%28i.e.%2C%20simultaneous%0Arefinement%20of%20camera%20orientations%20and%20scene%20map%29%20for%20a%20purely%20rotating%20event%0Acamera.%20We%20formulate%20the%20problem%20as%20a%20regularized%20non-linear%20least%20squares%0Aoptimization.%20The%20objective%20function%20is%20defined%20using%20the%20linearized%20event%0Ageneration%20model%20in%20the%20camera%20orientations%20and%20the%20panoramic%20gradient%20map%20of%0Athe%20scene.%20We%20show%20that%20this%20BA%20optimization%20has%20an%20exploitable%20block-diagonal%0Asparsity%20structure%2C%20so%20that%20the%20problem%20can%20be%20solved%20efficiently.%20To%20the%20best%0Aof%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%20leverage%20such%20sparsity%20to%20speed%20up%0Athe%20optimization%20in%20the%20context%20of%20event-based%20cameras%2C%20without%20the%20need%20to%0Aconvert%20events%20into%20image-like%20representations.%20We%20evaluate%20our%20method%2C%20called%0AEMBA%2C%20on%20both%20synthetic%20and%20real-world%20datasets%20to%20show%20its%20effectiveness%20%2850%25%0Aphotometric%20error%20decrease%29%2C%20yielding%20results%20of%20unprecedented%20quality.%20In%0Aaddition%2C%20we%20demonstrate%20EMBA%20using%20high%20spatial%20resolution%20event%20cameras%2C%0Ayielding%20delicate%20panoramas%20in%20the%20wild%2C%20even%20without%20an%20initial%20map.%20Project%0Apage%3A%20https%3A//github.com/tub-rip/emba%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07365v1&entry.124074799=Read"},
{"title": "Adaptive Adapter Routing for Long-Tailed Class-Incremental Learning", "author": "Zhi-Hong Qi and Da-Wei Zhou and Yiran Yao and Han-Jia Ye and De-Chuan Zhan", "abstract": "  In our ever-evolving world, new data exhibits a long-tailed distribution,\nsuch as e-commerce platform reviews. This necessitates continuous model\nlearning imbalanced data without forgetting, addressing the challenge of\nlong-tailed class-incremental learning (LTCIL). Existing methods often rely on\nretraining linear classifiers with former data, which is impractical in\nreal-world settings. In this paper, we harness the potent representation\ncapabilities of pre-trained models and introduce AdaPtive Adapter RouTing\n(APART) as an exemplar-free solution for LTCIL. To counteract forgetting, we\ntrain inserted adapters with frozen pre-trained weights for deeper adaptation\nand maintain a pool of adapters for selection during sequential model updates.\nAdditionally, we present an auxiliary adapter pool designed for effective\ngeneralization, especially on minority classes. Adaptive instance routing\nacross these pools captures crucial correlations, facilitating a comprehensive\nrepresentation of all classes. Consequently, APART tackles the imbalance\nproblem as well as catastrophic forgetting in a unified framework. Extensive\nbenchmark experiments validate the effectiveness of APART. Code is available\nat: https://github.com/vita-qzh/APART\n", "link": "http://arxiv.org/abs/2409.07446v1", "date": "2024-09-11", "relevancy": 2.7351, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5768}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5717}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Adapter%20Routing%20for%20Long-Tailed%20Class-Incremental%20Learning&body=Title%3A%20Adaptive%20Adapter%20Routing%20for%20Long-Tailed%20Class-Incremental%20Learning%0AAuthor%3A%20Zhi-Hong%20Qi%20and%20Da-Wei%20Zhou%20and%20Yiran%20Yao%20and%20Han-Jia%20Ye%20and%20De-Chuan%20Zhan%0AAbstract%3A%20%20%20In%20our%20ever-evolving%20world%2C%20new%20data%20exhibits%20a%20long-tailed%20distribution%2C%0Asuch%20as%20e-commerce%20platform%20reviews.%20This%20necessitates%20continuous%20model%0Alearning%20imbalanced%20data%20without%20forgetting%2C%20addressing%20the%20challenge%20of%0Along-tailed%20class-incremental%20learning%20%28LTCIL%29.%20Existing%20methods%20often%20rely%20on%0Aretraining%20linear%20classifiers%20with%20former%20data%2C%20which%20is%20impractical%20in%0Areal-world%20settings.%20In%20this%20paper%2C%20we%20harness%20the%20potent%20representation%0Acapabilities%20of%20pre-trained%20models%20and%20introduce%20AdaPtive%20Adapter%20RouTing%0A%28APART%29%20as%20an%20exemplar-free%20solution%20for%20LTCIL.%20To%20counteract%20forgetting%2C%20we%0Atrain%20inserted%20adapters%20with%20frozen%20pre-trained%20weights%20for%20deeper%20adaptation%0Aand%20maintain%20a%20pool%20of%20adapters%20for%20selection%20during%20sequential%20model%20updates.%0AAdditionally%2C%20we%20present%20an%20auxiliary%20adapter%20pool%20designed%20for%20effective%0Ageneralization%2C%20especially%20on%20minority%20classes.%20Adaptive%20instance%20routing%0Aacross%20these%20pools%20captures%20crucial%20correlations%2C%20facilitating%20a%20comprehensive%0Arepresentation%20of%20all%20classes.%20Consequently%2C%20APART%20tackles%20the%20imbalance%0Aproblem%20as%20well%20as%20catastrophic%20forgetting%20in%20a%20unified%20framework.%20Extensive%0Abenchmark%20experiments%20validate%20the%20effectiveness%20of%20APART.%20Code%20is%20available%0Aat%3A%20https%3A//github.com/vita-qzh/APART%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07446v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Adapter%2520Routing%2520for%2520Long-Tailed%2520Class-Incremental%2520Learning%26entry.906535625%3DZhi-Hong%2520Qi%2520and%2520Da-Wei%2520Zhou%2520and%2520Yiran%2520Yao%2520and%2520Han-Jia%2520Ye%2520and%2520De-Chuan%2520Zhan%26entry.1292438233%3D%2520%2520In%2520our%2520ever-evolving%2520world%252C%2520new%2520data%2520exhibits%2520a%2520long-tailed%2520distribution%252C%250Asuch%2520as%2520e-commerce%2520platform%2520reviews.%2520This%2520necessitates%2520continuous%2520model%250Alearning%2520imbalanced%2520data%2520without%2520forgetting%252C%2520addressing%2520the%2520challenge%2520of%250Along-tailed%2520class-incremental%2520learning%2520%2528LTCIL%2529.%2520Existing%2520methods%2520often%2520rely%2520on%250Aretraining%2520linear%2520classifiers%2520with%2520former%2520data%252C%2520which%2520is%2520impractical%2520in%250Areal-world%2520settings.%2520In%2520this%2520paper%252C%2520we%2520harness%2520the%2520potent%2520representation%250Acapabilities%2520of%2520pre-trained%2520models%2520and%2520introduce%2520AdaPtive%2520Adapter%2520RouTing%250A%2528APART%2529%2520as%2520an%2520exemplar-free%2520solution%2520for%2520LTCIL.%2520To%2520counteract%2520forgetting%252C%2520we%250Atrain%2520inserted%2520adapters%2520with%2520frozen%2520pre-trained%2520weights%2520for%2520deeper%2520adaptation%250Aand%2520maintain%2520a%2520pool%2520of%2520adapters%2520for%2520selection%2520during%2520sequential%2520model%2520updates.%250AAdditionally%252C%2520we%2520present%2520an%2520auxiliary%2520adapter%2520pool%2520designed%2520for%2520effective%250Ageneralization%252C%2520especially%2520on%2520minority%2520classes.%2520Adaptive%2520instance%2520routing%250Aacross%2520these%2520pools%2520captures%2520crucial%2520correlations%252C%2520facilitating%2520a%2520comprehensive%250Arepresentation%2520of%2520all%2520classes.%2520Consequently%252C%2520APART%2520tackles%2520the%2520imbalance%250Aproblem%2520as%2520well%2520as%2520catastrophic%2520forgetting%2520in%2520a%2520unified%2520framework.%2520Extensive%250Abenchmark%2520experiments%2520validate%2520the%2520effectiveness%2520of%2520APART.%2520Code%2520is%2520available%250Aat%253A%2520https%253A//github.com/vita-qzh/APART%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07446v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Adapter%20Routing%20for%20Long-Tailed%20Class-Incremental%20Learning&entry.906535625=Zhi-Hong%20Qi%20and%20Da-Wei%20Zhou%20and%20Yiran%20Yao%20and%20Han-Jia%20Ye%20and%20De-Chuan%20Zhan&entry.1292438233=%20%20In%20our%20ever-evolving%20world%2C%20new%20data%20exhibits%20a%20long-tailed%20distribution%2C%0Asuch%20as%20e-commerce%20platform%20reviews.%20This%20necessitates%20continuous%20model%0Alearning%20imbalanced%20data%20without%20forgetting%2C%20addressing%20the%20challenge%20of%0Along-tailed%20class-incremental%20learning%20%28LTCIL%29.%20Existing%20methods%20often%20rely%20on%0Aretraining%20linear%20classifiers%20with%20former%20data%2C%20which%20is%20impractical%20in%0Areal-world%20settings.%20In%20this%20paper%2C%20we%20harness%20the%20potent%20representation%0Acapabilities%20of%20pre-trained%20models%20and%20introduce%20AdaPtive%20Adapter%20RouTing%0A%28APART%29%20as%20an%20exemplar-free%20solution%20for%20LTCIL.%20To%20counteract%20forgetting%2C%20we%0Atrain%20inserted%20adapters%20with%20frozen%20pre-trained%20weights%20for%20deeper%20adaptation%0Aand%20maintain%20a%20pool%20of%20adapters%20for%20selection%20during%20sequential%20model%20updates.%0AAdditionally%2C%20we%20present%20an%20auxiliary%20adapter%20pool%20designed%20for%20effective%0Ageneralization%2C%20especially%20on%20minority%20classes.%20Adaptive%20instance%20routing%0Aacross%20these%20pools%20captures%20crucial%20correlations%2C%20facilitating%20a%20comprehensive%0Arepresentation%20of%20all%20classes.%20Consequently%2C%20APART%20tackles%20the%20imbalance%0Aproblem%20as%20well%20as%20catastrophic%20forgetting%20in%20a%20unified%20framework.%20Extensive%0Abenchmark%20experiments%20validate%20the%20effectiveness%20of%20APART.%20Code%20is%20available%0Aat%3A%20https%3A//github.com/vita-qzh/APART%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07446v1&entry.124074799=Read"},
{"title": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality\n  Models", "author": "Haodong Duan and Junming Yang and Yuxuan Qiao and Xinyu Fang and Lin Chen and Yuan Liu and Amit Agarwal and Zhe Chen and Mo Li and Yubo Ma and Hailong Sun and Xiangyu Zhao and Junbo Cui and Xiaoyi Dong and Yuhang Zang and Pan Zhang and Jiaqi Wang and Dahua Lin and Kai Chen", "abstract": "  We present VLMEvalKit: an open-source toolkit for evaluating large\nmulti-modality models based on PyTorch. The toolkit aims to provide a\nuser-friendly and comprehensive framework for researchers and developers to\nevaluate existing multi-modality models and publish reproducible evaluation\nresults. In VLMEvalKit, we implement over 70 different large multi-modality\nmodels, including both proprietary APIs and open-source models, as well as more\nthan 20 different multi-modal benchmarks. By implementing a single interface,\nnew models can be easily added to the toolkit, while the toolkit automatically\nhandles the remaining workloads, including data preparation, distributed\ninference, prediction post-processing, and metric calculation. Although the\ntoolkit is currently mainly used for evaluating large vision-language models,\nits design is compatible with future updates that incorporate additional\nmodalities, such as audio and video. Based on the evaluation results obtained\nwith the toolkit, we host OpenVLM Leaderboard, a comprehensive leaderboard to\ntrack the progress of multi-modality learning research. The toolkit is released\nat https://github.com/open-compass/VLMEvalKit and is actively maintained.\n", "link": "http://arxiv.org/abs/2407.11691v2", "date": "2024-09-11", "relevancy": 2.7299, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5486}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5447}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLMEvalKit%3A%20An%20Open-Source%20Toolkit%20for%20Evaluating%20Large%20Multi-Modality%0A%20%20Models&body=Title%3A%20VLMEvalKit%3A%20An%20Open-Source%20Toolkit%20for%20Evaluating%20Large%20Multi-Modality%0A%20%20Models%0AAuthor%3A%20Haodong%20Duan%20and%20Junming%20Yang%20and%20Yuxuan%20Qiao%20and%20Xinyu%20Fang%20and%20Lin%20Chen%20and%20Yuan%20Liu%20and%20Amit%20Agarwal%20and%20Zhe%20Chen%20and%20Mo%20Li%20and%20Yubo%20Ma%20and%20Hailong%20Sun%20and%20Xiangyu%20Zhao%20and%20Junbo%20Cui%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Zang%20and%20Pan%20Zhang%20and%20Jiaqi%20Wang%20and%20Dahua%20Lin%20and%20Kai%20Chen%0AAbstract%3A%20%20%20We%20present%20VLMEvalKit%3A%20an%20open-source%20toolkit%20for%20evaluating%20large%0Amulti-modality%20models%20based%20on%20PyTorch.%20The%20toolkit%20aims%20to%20provide%20a%0Auser-friendly%20and%20comprehensive%20framework%20for%20researchers%20and%20developers%20to%0Aevaluate%20existing%20multi-modality%20models%20and%20publish%20reproducible%20evaluation%0Aresults.%20In%20VLMEvalKit%2C%20we%20implement%20over%2070%20different%20large%20multi-modality%0Amodels%2C%20including%20both%20proprietary%20APIs%20and%20open-source%20models%2C%20as%20well%20as%20more%0Athan%2020%20different%20multi-modal%20benchmarks.%20By%20implementing%20a%20single%20interface%2C%0Anew%20models%20can%20be%20easily%20added%20to%20the%20toolkit%2C%20while%20the%20toolkit%20automatically%0Ahandles%20the%20remaining%20workloads%2C%20including%20data%20preparation%2C%20distributed%0Ainference%2C%20prediction%20post-processing%2C%20and%20metric%20calculation.%20Although%20the%0Atoolkit%20is%20currently%20mainly%20used%20for%20evaluating%20large%20vision-language%20models%2C%0Aits%20design%20is%20compatible%20with%20future%20updates%20that%20incorporate%20additional%0Amodalities%2C%20such%20as%20audio%20and%20video.%20Based%20on%20the%20evaluation%20results%20obtained%0Awith%20the%20toolkit%2C%20we%20host%20OpenVLM%20Leaderboard%2C%20a%20comprehensive%20leaderboard%20to%0Atrack%20the%20progress%20of%20multi-modality%20learning%20research.%20The%20toolkit%20is%20released%0Aat%20https%3A//github.com/open-compass/VLMEvalKit%20and%20is%20actively%20maintained.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11691v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLMEvalKit%253A%2520An%2520Open-Source%2520Toolkit%2520for%2520Evaluating%2520Large%2520Multi-Modality%250A%2520%2520Models%26entry.906535625%3DHaodong%2520Duan%2520and%2520Junming%2520Yang%2520and%2520Yuxuan%2520Qiao%2520and%2520Xinyu%2520Fang%2520and%2520Lin%2520Chen%2520and%2520Yuan%2520Liu%2520and%2520Amit%2520Agarwal%2520and%2520Zhe%2520Chen%2520and%2520Mo%2520Li%2520and%2520Yubo%2520Ma%2520and%2520Hailong%2520Sun%2520and%2520Xiangyu%2520Zhao%2520and%2520Junbo%2520Cui%2520and%2520Xiaoyi%2520Dong%2520and%2520Yuhang%2520Zang%2520and%2520Pan%2520Zhang%2520and%2520Jiaqi%2520Wang%2520and%2520Dahua%2520Lin%2520and%2520Kai%2520Chen%26entry.1292438233%3D%2520%2520We%2520present%2520VLMEvalKit%253A%2520an%2520open-source%2520toolkit%2520for%2520evaluating%2520large%250Amulti-modality%2520models%2520based%2520on%2520PyTorch.%2520The%2520toolkit%2520aims%2520to%2520provide%2520a%250Auser-friendly%2520and%2520comprehensive%2520framework%2520for%2520researchers%2520and%2520developers%2520to%250Aevaluate%2520existing%2520multi-modality%2520models%2520and%2520publish%2520reproducible%2520evaluation%250Aresults.%2520In%2520VLMEvalKit%252C%2520we%2520implement%2520over%252070%2520different%2520large%2520multi-modality%250Amodels%252C%2520including%2520both%2520proprietary%2520APIs%2520and%2520open-source%2520models%252C%2520as%2520well%2520as%2520more%250Athan%252020%2520different%2520multi-modal%2520benchmarks.%2520By%2520implementing%2520a%2520single%2520interface%252C%250Anew%2520models%2520can%2520be%2520easily%2520added%2520to%2520the%2520toolkit%252C%2520while%2520the%2520toolkit%2520automatically%250Ahandles%2520the%2520remaining%2520workloads%252C%2520including%2520data%2520preparation%252C%2520distributed%250Ainference%252C%2520prediction%2520post-processing%252C%2520and%2520metric%2520calculation.%2520Although%2520the%250Atoolkit%2520is%2520currently%2520mainly%2520used%2520for%2520evaluating%2520large%2520vision-language%2520models%252C%250Aits%2520design%2520is%2520compatible%2520with%2520future%2520updates%2520that%2520incorporate%2520additional%250Amodalities%252C%2520such%2520as%2520audio%2520and%2520video.%2520Based%2520on%2520the%2520evaluation%2520results%2520obtained%250Awith%2520the%2520toolkit%252C%2520we%2520host%2520OpenVLM%2520Leaderboard%252C%2520a%2520comprehensive%2520leaderboard%2520to%250Atrack%2520the%2520progress%2520of%2520multi-modality%2520learning%2520research.%2520The%2520toolkit%2520is%2520released%250Aat%2520https%253A//github.com/open-compass/VLMEvalKit%2520and%2520is%2520actively%2520maintained.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11691v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLMEvalKit%3A%20An%20Open-Source%20Toolkit%20for%20Evaluating%20Large%20Multi-Modality%0A%20%20Models&entry.906535625=Haodong%20Duan%20and%20Junming%20Yang%20and%20Yuxuan%20Qiao%20and%20Xinyu%20Fang%20and%20Lin%20Chen%20and%20Yuan%20Liu%20and%20Amit%20Agarwal%20and%20Zhe%20Chen%20and%20Mo%20Li%20and%20Yubo%20Ma%20and%20Hailong%20Sun%20and%20Xiangyu%20Zhao%20and%20Junbo%20Cui%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Zang%20and%20Pan%20Zhang%20and%20Jiaqi%20Wang%20and%20Dahua%20Lin%20and%20Kai%20Chen&entry.1292438233=%20%20We%20present%20VLMEvalKit%3A%20an%20open-source%20toolkit%20for%20evaluating%20large%0Amulti-modality%20models%20based%20on%20PyTorch.%20The%20toolkit%20aims%20to%20provide%20a%0Auser-friendly%20and%20comprehensive%20framework%20for%20researchers%20and%20developers%20to%0Aevaluate%20existing%20multi-modality%20models%20and%20publish%20reproducible%20evaluation%0Aresults.%20In%20VLMEvalKit%2C%20we%20implement%20over%2070%20different%20large%20multi-modality%0Amodels%2C%20including%20both%20proprietary%20APIs%20and%20open-source%20models%2C%20as%20well%20as%20more%0Athan%2020%20different%20multi-modal%20benchmarks.%20By%20implementing%20a%20single%20interface%2C%0Anew%20models%20can%20be%20easily%20added%20to%20the%20toolkit%2C%20while%20the%20toolkit%20automatically%0Ahandles%20the%20remaining%20workloads%2C%20including%20data%20preparation%2C%20distributed%0Ainference%2C%20prediction%20post-processing%2C%20and%20metric%20calculation.%20Although%20the%0Atoolkit%20is%20currently%20mainly%20used%20for%20evaluating%20large%20vision-language%20models%2C%0Aits%20design%20is%20compatible%20with%20future%20updates%20that%20incorporate%20additional%0Amodalities%2C%20such%20as%20audio%20and%20video.%20Based%20on%20the%20evaluation%20results%20obtained%0Awith%20the%20toolkit%2C%20we%20host%20OpenVLM%20Leaderboard%2C%20a%20comprehensive%20leaderboard%20to%0Atrack%20the%20progress%20of%20multi-modality%20learning%20research.%20The%20toolkit%20is%20released%0Aat%20https%3A//github.com/open-compass/VLMEvalKit%20and%20is%20actively%20maintained.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11691v2&entry.124074799=Read"},
{"title": "AC-IND: Sparse CT reconstruction based on attenuation coefficient\n  estimation and implicit neural distribution", "author": "Wangduo Xie and Richard Schoonhoven and Tristan van Leeuwen and Matthew B. Blaschko", "abstract": "  Computed tomography (CT) reconstruction plays a crucial role in industrial\nnondestructive testing and medical diagnosis. Sparse view CT reconstruction\naims to reconstruct high-quality CT images while only using a small number of\nprojections, which helps to improve the detection speed of industrial assembly\nlines and is also meaningful for reducing radiation in medical scenarios.\nSparse CT reconstruction methods based on implicit neural representations\n(INRs) have recently shown promising performance, but still produce artifacts\nbecause of the difficulty of obtaining useful prior information. In this work,\nwe incorporate a powerful prior: the total number of material categories of\nobjects. To utilize the prior, we design AC-IND, a self-supervised method based\non Attenuation Coefficient Estimation and Implicit Neural Distribution.\nSpecifically, our method first transforms the traditional INR from scalar\nmapping to probability distribution mapping. Then we design a compact\nattenuation coefficient estimator initialized with values from a rough\nreconstruction and fast segmentation. Finally, our algorithm finishes the CT\nreconstruction by jointly optimizing the estimator and the generated\ndistribution. Through experiments, we find that our method not only outperforms\nthe comparative methods in sparse CT reconstruction but also can automatically\ngenerate semantic segmentation maps.\n", "link": "http://arxiv.org/abs/2409.07171v1", "date": "2024-09-11", "relevancy": 2.698, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5548}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.532}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AC-IND%3A%20Sparse%20CT%20reconstruction%20based%20on%20attenuation%20coefficient%0A%20%20estimation%20and%20implicit%20neural%20distribution&body=Title%3A%20AC-IND%3A%20Sparse%20CT%20reconstruction%20based%20on%20attenuation%20coefficient%0A%20%20estimation%20and%20implicit%20neural%20distribution%0AAuthor%3A%20Wangduo%20Xie%20and%20Richard%20Schoonhoven%20and%20Tristan%20van%20Leeuwen%20and%20Matthew%20B.%20Blaschko%0AAbstract%3A%20%20%20Computed%20tomography%20%28CT%29%20reconstruction%20plays%20a%20crucial%20role%20in%20industrial%0Anondestructive%20testing%20and%20medical%20diagnosis.%20Sparse%20view%20CT%20reconstruction%0Aaims%20to%20reconstruct%20high-quality%20CT%20images%20while%20only%20using%20a%20small%20number%20of%0Aprojections%2C%20which%20helps%20to%20improve%20the%20detection%20speed%20of%20industrial%20assembly%0Alines%20and%20is%20also%20meaningful%20for%20reducing%20radiation%20in%20medical%20scenarios.%0ASparse%20CT%20reconstruction%20methods%20based%20on%20implicit%20neural%20representations%0A%28INRs%29%20have%20recently%20shown%20promising%20performance%2C%20but%20still%20produce%20artifacts%0Abecause%20of%20the%20difficulty%20of%20obtaining%20useful%20prior%20information.%20In%20this%20work%2C%0Awe%20incorporate%20a%20powerful%20prior%3A%20the%20total%20number%20of%20material%20categories%20of%0Aobjects.%20To%20utilize%20the%20prior%2C%20we%20design%20AC-IND%2C%20a%20self-supervised%20method%20based%0Aon%20Attenuation%20Coefficient%20Estimation%20and%20Implicit%20Neural%20Distribution.%0ASpecifically%2C%20our%20method%20first%20transforms%20the%20traditional%20INR%20from%20scalar%0Amapping%20to%20probability%20distribution%20mapping.%20Then%20we%20design%20a%20compact%0Aattenuation%20coefficient%20estimator%20initialized%20with%20values%20from%20a%20rough%0Areconstruction%20and%20fast%20segmentation.%20Finally%2C%20our%20algorithm%20finishes%20the%20CT%0Areconstruction%20by%20jointly%20optimizing%20the%20estimator%20and%20the%20generated%0Adistribution.%20Through%20experiments%2C%20we%20find%20that%20our%20method%20not%20only%20outperforms%0Athe%20comparative%20methods%20in%20sparse%20CT%20reconstruction%20but%20also%20can%20automatically%0Agenerate%20semantic%20segmentation%20maps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07171v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAC-IND%253A%2520Sparse%2520CT%2520reconstruction%2520based%2520on%2520attenuation%2520coefficient%250A%2520%2520estimation%2520and%2520implicit%2520neural%2520distribution%26entry.906535625%3DWangduo%2520Xie%2520and%2520Richard%2520Schoonhoven%2520and%2520Tristan%2520van%2520Leeuwen%2520and%2520Matthew%2520B.%2520Blaschko%26entry.1292438233%3D%2520%2520Computed%2520tomography%2520%2528CT%2529%2520reconstruction%2520plays%2520a%2520crucial%2520role%2520in%2520industrial%250Anondestructive%2520testing%2520and%2520medical%2520diagnosis.%2520Sparse%2520view%2520CT%2520reconstruction%250Aaims%2520to%2520reconstruct%2520high-quality%2520CT%2520images%2520while%2520only%2520using%2520a%2520small%2520number%2520of%250Aprojections%252C%2520which%2520helps%2520to%2520improve%2520the%2520detection%2520speed%2520of%2520industrial%2520assembly%250Alines%2520and%2520is%2520also%2520meaningful%2520for%2520reducing%2520radiation%2520in%2520medical%2520scenarios.%250ASparse%2520CT%2520reconstruction%2520methods%2520based%2520on%2520implicit%2520neural%2520representations%250A%2528INRs%2529%2520have%2520recently%2520shown%2520promising%2520performance%252C%2520but%2520still%2520produce%2520artifacts%250Abecause%2520of%2520the%2520difficulty%2520of%2520obtaining%2520useful%2520prior%2520information.%2520In%2520this%2520work%252C%250Awe%2520incorporate%2520a%2520powerful%2520prior%253A%2520the%2520total%2520number%2520of%2520material%2520categories%2520of%250Aobjects.%2520To%2520utilize%2520the%2520prior%252C%2520we%2520design%2520AC-IND%252C%2520a%2520self-supervised%2520method%2520based%250Aon%2520Attenuation%2520Coefficient%2520Estimation%2520and%2520Implicit%2520Neural%2520Distribution.%250ASpecifically%252C%2520our%2520method%2520first%2520transforms%2520the%2520traditional%2520INR%2520from%2520scalar%250Amapping%2520to%2520probability%2520distribution%2520mapping.%2520Then%2520we%2520design%2520a%2520compact%250Aattenuation%2520coefficient%2520estimator%2520initialized%2520with%2520values%2520from%2520a%2520rough%250Areconstruction%2520and%2520fast%2520segmentation.%2520Finally%252C%2520our%2520algorithm%2520finishes%2520the%2520CT%250Areconstruction%2520by%2520jointly%2520optimizing%2520the%2520estimator%2520and%2520the%2520generated%250Adistribution.%2520Through%2520experiments%252C%2520we%2520find%2520that%2520our%2520method%2520not%2520only%2520outperforms%250Athe%2520comparative%2520methods%2520in%2520sparse%2520CT%2520reconstruction%2520but%2520also%2520can%2520automatically%250Agenerate%2520semantic%2520segmentation%2520maps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07171v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AC-IND%3A%20Sparse%20CT%20reconstruction%20based%20on%20attenuation%20coefficient%0A%20%20estimation%20and%20implicit%20neural%20distribution&entry.906535625=Wangduo%20Xie%20and%20Richard%20Schoonhoven%20and%20Tristan%20van%20Leeuwen%20and%20Matthew%20B.%20Blaschko&entry.1292438233=%20%20Computed%20tomography%20%28CT%29%20reconstruction%20plays%20a%20crucial%20role%20in%20industrial%0Anondestructive%20testing%20and%20medical%20diagnosis.%20Sparse%20view%20CT%20reconstruction%0Aaims%20to%20reconstruct%20high-quality%20CT%20images%20while%20only%20using%20a%20small%20number%20of%0Aprojections%2C%20which%20helps%20to%20improve%20the%20detection%20speed%20of%20industrial%20assembly%0Alines%20and%20is%20also%20meaningful%20for%20reducing%20radiation%20in%20medical%20scenarios.%0ASparse%20CT%20reconstruction%20methods%20based%20on%20implicit%20neural%20representations%0A%28INRs%29%20have%20recently%20shown%20promising%20performance%2C%20but%20still%20produce%20artifacts%0Abecause%20of%20the%20difficulty%20of%20obtaining%20useful%20prior%20information.%20In%20this%20work%2C%0Awe%20incorporate%20a%20powerful%20prior%3A%20the%20total%20number%20of%20material%20categories%20of%0Aobjects.%20To%20utilize%20the%20prior%2C%20we%20design%20AC-IND%2C%20a%20self-supervised%20method%20based%0Aon%20Attenuation%20Coefficient%20Estimation%20and%20Implicit%20Neural%20Distribution.%0ASpecifically%2C%20our%20method%20first%20transforms%20the%20traditional%20INR%20from%20scalar%0Amapping%20to%20probability%20distribution%20mapping.%20Then%20we%20design%20a%20compact%0Aattenuation%20coefficient%20estimator%20initialized%20with%20values%20from%20a%20rough%0Areconstruction%20and%20fast%20segmentation.%20Finally%2C%20our%20algorithm%20finishes%20the%20CT%0Areconstruction%20by%20jointly%20optimizing%20the%20estimator%20and%20the%20generated%0Adistribution.%20Through%20experiments%2C%20we%20find%20that%20our%20method%20not%20only%20outperforms%0Athe%20comparative%20methods%20in%20sparse%20CT%20reconstruction%20but%20also%20can%20automatically%0Agenerate%20semantic%20segmentation%20maps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07171v1&entry.124074799=Read"},
{"title": "NVRC: Neural Video Representation Compression", "author": "Ho Man Kwan and Ge Gao and Fan Zhang and Andrew Gower and David Bull", "abstract": "  Recent advances in implicit neural representation (INR)-based video coding\nhave demonstrated its potential to compete with both conventional and other\nlearning-based approaches. With INR methods, a neural network is trained to\noverfit a video sequence, with its parameters compressed to obtain a compact\nrepresentation of the video content. However, although promising results have\nbeen achieved, the best INR-based methods are still out-performed by the latest\nstandard codecs, such as VVC VTM, partially due to the simple model compression\ntechniques employed. In this paper, rather than focusing on representation\narchitectures as in many existing works, we propose a novel INR-based video\ncompression framework, Neural Video Representation Compression (NVRC),\ntargeting compression of the representation. Based on the novel entropy coding\nand quantization models proposed, NVRC, for the first time, is able to optimize\nan INR-based video codec in a fully end-to-end manner. To further minimize the\nadditional bitrate overhead introduced by the entropy models, we have also\nproposed a new model compression framework for coding all the network,\nquantization and entropy model parameters hierarchically. Our experiments show\nthat NVRC outperforms many conventional and learning-based benchmark codecs,\nwith a 24% average coding gain over VVC VTM (Random Access) on the UVG dataset,\nmeasured in PSNR. As far as we are aware, this is the first time an INR-based\nvideo codec achieving such performance. The implementation of NVRC will be\nreleased at www.github.com.\n", "link": "http://arxiv.org/abs/2409.07414v1", "date": "2024-09-11", "relevancy": 2.6247, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5608}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.514}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NVRC%3A%20Neural%20Video%20Representation%20Compression&body=Title%3A%20NVRC%3A%20Neural%20Video%20Representation%20Compression%0AAuthor%3A%20Ho%20Man%20Kwan%20and%20Ge%20Gao%20and%20Fan%20Zhang%20and%20Andrew%20Gower%20and%20David%20Bull%0AAbstract%3A%20%20%20Recent%20advances%20in%20implicit%20neural%20representation%20%28INR%29-based%20video%20coding%0Ahave%20demonstrated%20its%20potential%20to%20compete%20with%20both%20conventional%20and%20other%0Alearning-based%20approaches.%20With%20INR%20methods%2C%20a%20neural%20network%20is%20trained%20to%0Aoverfit%20a%20video%20sequence%2C%20with%20its%20parameters%20compressed%20to%20obtain%20a%20compact%0Arepresentation%20of%20the%20video%20content.%20However%2C%20although%20promising%20results%20have%0Abeen%20achieved%2C%20the%20best%20INR-based%20methods%20are%20still%20out-performed%20by%20the%20latest%0Astandard%20codecs%2C%20such%20as%20VVC%20VTM%2C%20partially%20due%20to%20the%20simple%20model%20compression%0Atechniques%20employed.%20In%20this%20paper%2C%20rather%20than%20focusing%20on%20representation%0Aarchitectures%20as%20in%20many%20existing%20works%2C%20we%20propose%20a%20novel%20INR-based%20video%0Acompression%20framework%2C%20Neural%20Video%20Representation%20Compression%20%28NVRC%29%2C%0Atargeting%20compression%20of%20the%20representation.%20Based%20on%20the%20novel%20entropy%20coding%0Aand%20quantization%20models%20proposed%2C%20NVRC%2C%20for%20the%20first%20time%2C%20is%20able%20to%20optimize%0Aan%20INR-based%20video%20codec%20in%20a%20fully%20end-to-end%20manner.%20To%20further%20minimize%20the%0Aadditional%20bitrate%20overhead%20introduced%20by%20the%20entropy%20models%2C%20we%20have%20also%0Aproposed%20a%20new%20model%20compression%20framework%20for%20coding%20all%20the%20network%2C%0Aquantization%20and%20entropy%20model%20parameters%20hierarchically.%20Our%20experiments%20show%0Athat%20NVRC%20outperforms%20many%20conventional%20and%20learning-based%20benchmark%20codecs%2C%0Awith%20a%2024%25%20average%20coding%20gain%20over%20VVC%20VTM%20%28Random%20Access%29%20on%20the%20UVG%20dataset%2C%0Ameasured%20in%20PSNR.%20As%20far%20as%20we%20are%20aware%2C%20this%20is%20the%20first%20time%20an%20INR-based%0Avideo%20codec%20achieving%20such%20performance.%20The%20implementation%20of%20NVRC%20will%20be%0Areleased%20at%20www.github.com.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07414v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNVRC%253A%2520Neural%2520Video%2520Representation%2520Compression%26entry.906535625%3DHo%2520Man%2520Kwan%2520and%2520Ge%2520Gao%2520and%2520Fan%2520Zhang%2520and%2520Andrew%2520Gower%2520and%2520David%2520Bull%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520implicit%2520neural%2520representation%2520%2528INR%2529-based%2520video%2520coding%250Ahave%2520demonstrated%2520its%2520potential%2520to%2520compete%2520with%2520both%2520conventional%2520and%2520other%250Alearning-based%2520approaches.%2520With%2520INR%2520methods%252C%2520a%2520neural%2520network%2520is%2520trained%2520to%250Aoverfit%2520a%2520video%2520sequence%252C%2520with%2520its%2520parameters%2520compressed%2520to%2520obtain%2520a%2520compact%250Arepresentation%2520of%2520the%2520video%2520content.%2520However%252C%2520although%2520promising%2520results%2520have%250Abeen%2520achieved%252C%2520the%2520best%2520INR-based%2520methods%2520are%2520still%2520out-performed%2520by%2520the%2520latest%250Astandard%2520codecs%252C%2520such%2520as%2520VVC%2520VTM%252C%2520partially%2520due%2520to%2520the%2520simple%2520model%2520compression%250Atechniques%2520employed.%2520In%2520this%2520paper%252C%2520rather%2520than%2520focusing%2520on%2520representation%250Aarchitectures%2520as%2520in%2520many%2520existing%2520works%252C%2520we%2520propose%2520a%2520novel%2520INR-based%2520video%250Acompression%2520framework%252C%2520Neural%2520Video%2520Representation%2520Compression%2520%2528NVRC%2529%252C%250Atargeting%2520compression%2520of%2520the%2520representation.%2520Based%2520on%2520the%2520novel%2520entropy%2520coding%250Aand%2520quantization%2520models%2520proposed%252C%2520NVRC%252C%2520for%2520the%2520first%2520time%252C%2520is%2520able%2520to%2520optimize%250Aan%2520INR-based%2520video%2520codec%2520in%2520a%2520fully%2520end-to-end%2520manner.%2520To%2520further%2520minimize%2520the%250Aadditional%2520bitrate%2520overhead%2520introduced%2520by%2520the%2520entropy%2520models%252C%2520we%2520have%2520also%250Aproposed%2520a%2520new%2520model%2520compression%2520framework%2520for%2520coding%2520all%2520the%2520network%252C%250Aquantization%2520and%2520entropy%2520model%2520parameters%2520hierarchically.%2520Our%2520experiments%2520show%250Athat%2520NVRC%2520outperforms%2520many%2520conventional%2520and%2520learning-based%2520benchmark%2520codecs%252C%250Awith%2520a%252024%2525%2520average%2520coding%2520gain%2520over%2520VVC%2520VTM%2520%2528Random%2520Access%2529%2520on%2520the%2520UVG%2520dataset%252C%250Ameasured%2520in%2520PSNR.%2520As%2520far%2520as%2520we%2520are%2520aware%252C%2520this%2520is%2520the%2520first%2520time%2520an%2520INR-based%250Avideo%2520codec%2520achieving%2520such%2520performance.%2520The%2520implementation%2520of%2520NVRC%2520will%2520be%250Areleased%2520at%2520www.github.com.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07414v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NVRC%3A%20Neural%20Video%20Representation%20Compression&entry.906535625=Ho%20Man%20Kwan%20and%20Ge%20Gao%20and%20Fan%20Zhang%20and%20Andrew%20Gower%20and%20David%20Bull&entry.1292438233=%20%20Recent%20advances%20in%20implicit%20neural%20representation%20%28INR%29-based%20video%20coding%0Ahave%20demonstrated%20its%20potential%20to%20compete%20with%20both%20conventional%20and%20other%0Alearning-based%20approaches.%20With%20INR%20methods%2C%20a%20neural%20network%20is%20trained%20to%0Aoverfit%20a%20video%20sequence%2C%20with%20its%20parameters%20compressed%20to%20obtain%20a%20compact%0Arepresentation%20of%20the%20video%20content.%20However%2C%20although%20promising%20results%20have%0Abeen%20achieved%2C%20the%20best%20INR-based%20methods%20are%20still%20out-performed%20by%20the%20latest%0Astandard%20codecs%2C%20such%20as%20VVC%20VTM%2C%20partially%20due%20to%20the%20simple%20model%20compression%0Atechniques%20employed.%20In%20this%20paper%2C%20rather%20than%20focusing%20on%20representation%0Aarchitectures%20as%20in%20many%20existing%20works%2C%20we%20propose%20a%20novel%20INR-based%20video%0Acompression%20framework%2C%20Neural%20Video%20Representation%20Compression%20%28NVRC%29%2C%0Atargeting%20compression%20of%20the%20representation.%20Based%20on%20the%20novel%20entropy%20coding%0Aand%20quantization%20models%20proposed%2C%20NVRC%2C%20for%20the%20first%20time%2C%20is%20able%20to%20optimize%0Aan%20INR-based%20video%20codec%20in%20a%20fully%20end-to-end%20manner.%20To%20further%20minimize%20the%0Aadditional%20bitrate%20overhead%20introduced%20by%20the%20entropy%20models%2C%20we%20have%20also%0Aproposed%20a%20new%20model%20compression%20framework%20for%20coding%20all%20the%20network%2C%0Aquantization%20and%20entropy%20model%20parameters%20hierarchically.%20Our%20experiments%20show%0Athat%20NVRC%20outperforms%20many%20conventional%20and%20learning-based%20benchmark%20codecs%2C%0Awith%20a%2024%25%20average%20coding%20gain%20over%20VVC%20VTM%20%28Random%20Access%29%20on%20the%20UVG%20dataset%2C%0Ameasured%20in%20PSNR.%20As%20far%20as%20we%20are%20aware%2C%20this%20is%20the%20first%20time%20an%20INR-based%0Avideo%20codec%20achieving%20such%20performance.%20The%20implementation%20of%20NVRC%20will%20be%0Areleased%20at%20www.github.com.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07414v1&entry.124074799=Read"},
{"title": "DocGenome: An Open Large-scale Scientific Document Benchmark for\n  Training and Testing Multi-modal Large Language Models", "author": "Renqiu Xia and Song Mao and Xiangchao Yan and Hongbin Zhou and Bo Zhang and Haoyang Peng and Jiahao Pi and Daocheng Fu and Wenjie Wu and Hancheng Ye and Shiyang Feng and Bin Wang and Chao Xu and Conghui He and Pinlong Cai and Min Dou and Botian Shi and Sheng Zhou and Yongwei Wang and Bin Wang and Junchi Yan and Fei Wu and Yu Qiao", "abstract": "  Scientific documents record research findings and valuable human knowledge,\ncomprising a vast corpus of high-quality data. Leveraging multi-modality data\nextracted from these documents and assessing large models' abilities to handle\nscientific document-oriented tasks is therefore meaningful. Despite promising\nadvancements, large models still perform poorly on multi-page scientific\ndocument extraction and understanding tasks, and their capacity to process\nwithin-document data formats such as charts and equations remains\nunder-explored. To address these issues, we present DocGenome, a structured\ndocument benchmark constructed by annotating 500K scientific documents from 153\ndisciplines in the arXiv open-access community, using our custom auto-labeling\npipeline. DocGenome features four key characteristics: 1) Completeness: It is\nthe first dataset to structure data from all modalities including 13 layout\nattributes along with their LaTeX source codes. 2) Logicality: It provides 6\nlogical relationships between different entities within each scientific\ndocument. 3) Diversity: It covers various document-oriented tasks, including\ndocument classification, visual grounding, document layout detection, document\ntransformation, open-ended single-page QA and multi-page QA. 4) Correctness: It\nundergoes rigorous quality control checks conducted by a specialized team. We\nconduct extensive experiments to demonstrate the advantages of DocGenome and\nobjectively evaluate the performance of large models on our benchmark.\n", "link": "http://arxiv.org/abs/2406.11633v2", "date": "2024-09-11", "relevancy": 2.6086, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5333}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5333}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DocGenome%3A%20An%20Open%20Large-scale%20Scientific%20Document%20Benchmark%20for%0A%20%20Training%20and%20Testing%20Multi-modal%20Large%20Language%20Models&body=Title%3A%20DocGenome%3A%20An%20Open%20Large-scale%20Scientific%20Document%20Benchmark%20for%0A%20%20Training%20and%20Testing%20Multi-modal%20Large%20Language%20Models%0AAuthor%3A%20Renqiu%20Xia%20and%20Song%20Mao%20and%20Xiangchao%20Yan%20and%20Hongbin%20Zhou%20and%20Bo%20Zhang%20and%20Haoyang%20Peng%20and%20Jiahao%20Pi%20and%20Daocheng%20Fu%20and%20Wenjie%20Wu%20and%20Hancheng%20Ye%20and%20Shiyang%20Feng%20and%20Bin%20Wang%20and%20Chao%20Xu%20and%20Conghui%20He%20and%20Pinlong%20Cai%20and%20Min%20Dou%20and%20Botian%20Shi%20and%20Sheng%20Zhou%20and%20Yongwei%20Wang%20and%20Bin%20Wang%20and%20Junchi%20Yan%20and%20Fei%20Wu%20and%20Yu%20Qiao%0AAbstract%3A%20%20%20Scientific%20documents%20record%20research%20findings%20and%20valuable%20human%20knowledge%2C%0Acomprising%20a%20vast%20corpus%20of%20high-quality%20data.%20Leveraging%20multi-modality%20data%0Aextracted%20from%20these%20documents%20and%20assessing%20large%20models%27%20abilities%20to%20handle%0Ascientific%20document-oriented%20tasks%20is%20therefore%20meaningful.%20Despite%20promising%0Aadvancements%2C%20large%20models%20still%20perform%20poorly%20on%20multi-page%20scientific%0Adocument%20extraction%20and%20understanding%20tasks%2C%20and%20their%20capacity%20to%20process%0Awithin-document%20data%20formats%20such%20as%20charts%20and%20equations%20remains%0Aunder-explored.%20To%20address%20these%20issues%2C%20we%20present%20DocGenome%2C%20a%20structured%0Adocument%20benchmark%20constructed%20by%20annotating%20500K%20scientific%20documents%20from%20153%0Adisciplines%20in%20the%20arXiv%20open-access%20community%2C%20using%20our%20custom%20auto-labeling%0Apipeline.%20DocGenome%20features%20four%20key%20characteristics%3A%201%29%20Completeness%3A%20It%20is%0Athe%20first%20dataset%20to%20structure%20data%20from%20all%20modalities%20including%2013%20layout%0Aattributes%20along%20with%20their%20LaTeX%20source%20codes.%202%29%20Logicality%3A%20It%20provides%206%0Alogical%20relationships%20between%20different%20entities%20within%20each%20scientific%0Adocument.%203%29%20Diversity%3A%20It%20covers%20various%20document-oriented%20tasks%2C%20including%0Adocument%20classification%2C%20visual%20grounding%2C%20document%20layout%20detection%2C%20document%0Atransformation%2C%20open-ended%20single-page%20QA%20and%20multi-page%20QA.%204%29%20Correctness%3A%20It%0Aundergoes%20rigorous%20quality%20control%20checks%20conducted%20by%20a%20specialized%20team.%20We%0Aconduct%20extensive%20experiments%20to%20demonstrate%20the%20advantages%20of%20DocGenome%20and%0Aobjectively%20evaluate%20the%20performance%20of%20large%20models%20on%20our%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11633v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDocGenome%253A%2520An%2520Open%2520Large-scale%2520Scientific%2520Document%2520Benchmark%2520for%250A%2520%2520Training%2520and%2520Testing%2520Multi-modal%2520Large%2520Language%2520Models%26entry.906535625%3DRenqiu%2520Xia%2520and%2520Song%2520Mao%2520and%2520Xiangchao%2520Yan%2520and%2520Hongbin%2520Zhou%2520and%2520Bo%2520Zhang%2520and%2520Haoyang%2520Peng%2520and%2520Jiahao%2520Pi%2520and%2520Daocheng%2520Fu%2520and%2520Wenjie%2520Wu%2520and%2520Hancheng%2520Ye%2520and%2520Shiyang%2520Feng%2520and%2520Bin%2520Wang%2520and%2520Chao%2520Xu%2520and%2520Conghui%2520He%2520and%2520Pinlong%2520Cai%2520and%2520Min%2520Dou%2520and%2520Botian%2520Shi%2520and%2520Sheng%2520Zhou%2520and%2520Yongwei%2520Wang%2520and%2520Bin%2520Wang%2520and%2520Junchi%2520Yan%2520and%2520Fei%2520Wu%2520and%2520Yu%2520Qiao%26entry.1292438233%3D%2520%2520Scientific%2520documents%2520record%2520research%2520findings%2520and%2520valuable%2520human%2520knowledge%252C%250Acomprising%2520a%2520vast%2520corpus%2520of%2520high-quality%2520data.%2520Leveraging%2520multi-modality%2520data%250Aextracted%2520from%2520these%2520documents%2520and%2520assessing%2520large%2520models%2527%2520abilities%2520to%2520handle%250Ascientific%2520document-oriented%2520tasks%2520is%2520therefore%2520meaningful.%2520Despite%2520promising%250Aadvancements%252C%2520large%2520models%2520still%2520perform%2520poorly%2520on%2520multi-page%2520scientific%250Adocument%2520extraction%2520and%2520understanding%2520tasks%252C%2520and%2520their%2520capacity%2520to%2520process%250Awithin-document%2520data%2520formats%2520such%2520as%2520charts%2520and%2520equations%2520remains%250Aunder-explored.%2520To%2520address%2520these%2520issues%252C%2520we%2520present%2520DocGenome%252C%2520a%2520structured%250Adocument%2520benchmark%2520constructed%2520by%2520annotating%2520500K%2520scientific%2520documents%2520from%2520153%250Adisciplines%2520in%2520the%2520arXiv%2520open-access%2520community%252C%2520using%2520our%2520custom%2520auto-labeling%250Apipeline.%2520DocGenome%2520features%2520four%2520key%2520characteristics%253A%25201%2529%2520Completeness%253A%2520It%2520is%250Athe%2520first%2520dataset%2520to%2520structure%2520data%2520from%2520all%2520modalities%2520including%252013%2520layout%250Aattributes%2520along%2520with%2520their%2520LaTeX%2520source%2520codes.%25202%2529%2520Logicality%253A%2520It%2520provides%25206%250Alogical%2520relationships%2520between%2520different%2520entities%2520within%2520each%2520scientific%250Adocument.%25203%2529%2520Diversity%253A%2520It%2520covers%2520various%2520document-oriented%2520tasks%252C%2520including%250Adocument%2520classification%252C%2520visual%2520grounding%252C%2520document%2520layout%2520detection%252C%2520document%250Atransformation%252C%2520open-ended%2520single-page%2520QA%2520and%2520multi-page%2520QA.%25204%2529%2520Correctness%253A%2520It%250Aundergoes%2520rigorous%2520quality%2520control%2520checks%2520conducted%2520by%2520a%2520specialized%2520team.%2520We%250Aconduct%2520extensive%2520experiments%2520to%2520demonstrate%2520the%2520advantages%2520of%2520DocGenome%2520and%250Aobjectively%2520evaluate%2520the%2520performance%2520of%2520large%2520models%2520on%2520our%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11633v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DocGenome%3A%20An%20Open%20Large-scale%20Scientific%20Document%20Benchmark%20for%0A%20%20Training%20and%20Testing%20Multi-modal%20Large%20Language%20Models&entry.906535625=Renqiu%20Xia%20and%20Song%20Mao%20and%20Xiangchao%20Yan%20and%20Hongbin%20Zhou%20and%20Bo%20Zhang%20and%20Haoyang%20Peng%20and%20Jiahao%20Pi%20and%20Daocheng%20Fu%20and%20Wenjie%20Wu%20and%20Hancheng%20Ye%20and%20Shiyang%20Feng%20and%20Bin%20Wang%20and%20Chao%20Xu%20and%20Conghui%20He%20and%20Pinlong%20Cai%20and%20Min%20Dou%20and%20Botian%20Shi%20and%20Sheng%20Zhou%20and%20Yongwei%20Wang%20and%20Bin%20Wang%20and%20Junchi%20Yan%20and%20Fei%20Wu%20and%20Yu%20Qiao&entry.1292438233=%20%20Scientific%20documents%20record%20research%20findings%20and%20valuable%20human%20knowledge%2C%0Acomprising%20a%20vast%20corpus%20of%20high-quality%20data.%20Leveraging%20multi-modality%20data%0Aextracted%20from%20these%20documents%20and%20assessing%20large%20models%27%20abilities%20to%20handle%0Ascientific%20document-oriented%20tasks%20is%20therefore%20meaningful.%20Despite%20promising%0Aadvancements%2C%20large%20models%20still%20perform%20poorly%20on%20multi-page%20scientific%0Adocument%20extraction%20and%20understanding%20tasks%2C%20and%20their%20capacity%20to%20process%0Awithin-document%20data%20formats%20such%20as%20charts%20and%20equations%20remains%0Aunder-explored.%20To%20address%20these%20issues%2C%20we%20present%20DocGenome%2C%20a%20structured%0Adocument%20benchmark%20constructed%20by%20annotating%20500K%20scientific%20documents%20from%20153%0Adisciplines%20in%20the%20arXiv%20open-access%20community%2C%20using%20our%20custom%20auto-labeling%0Apipeline.%20DocGenome%20features%20four%20key%20characteristics%3A%201%29%20Completeness%3A%20It%20is%0Athe%20first%20dataset%20to%20structure%20data%20from%20all%20modalities%20including%2013%20layout%0Aattributes%20along%20with%20their%20LaTeX%20source%20codes.%202%29%20Logicality%3A%20It%20provides%206%0Alogical%20relationships%20between%20different%20entities%20within%20each%20scientific%0Adocument.%203%29%20Diversity%3A%20It%20covers%20various%20document-oriented%20tasks%2C%20including%0Adocument%20classification%2C%20visual%20grounding%2C%20document%20layout%20detection%2C%20document%0Atransformation%2C%20open-ended%20single-page%20QA%20and%20multi-page%20QA.%204%29%20Correctness%3A%20It%0Aundergoes%20rigorous%20quality%20control%20checks%20conducted%20by%20a%20specialized%20team.%20We%0Aconduct%20extensive%20experiments%20to%20demonstrate%20the%20advantages%20of%20DocGenome%20and%0Aobjectively%20evaluate%20the%20performance%20of%20large%20models%20on%20our%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11633v2&entry.124074799=Read"},
{"title": "Watchlist Challenge: 3rd Open-set Face Detection and Identification", "author": "Furkan Kas\u0131m and Terrance E. Boult and Rensso Mora and Bernardo Biesseck and Rafael Ribeiro and Jan Schlueter and Tom\u00e1\u0161 Rep\u00e1k and Rafael Henrique Vareto and David Menotti and William Robson Schwartz and Manuel G\u00fcnther", "abstract": "  In the current landscape of biometrics and surveillance, the ability to\naccurately recognize faces in uncontrolled settings is paramount. The Watchlist\nChallenge addresses this critical need by focusing on face detection and\nopen-set identification in real-world surveillance scenarios. This paper\npresents a comprehensive evaluation of participating algorithms, using the\nenhanced UnConstrained College Students (UCCS) dataset with new evaluation\nprotocols. In total, four participants submitted four face detection and nine\nopen-set face recognition systems. The evaluation demonstrates that while\ndetection capabilities are generally robust, closed-set identification\nperformance varies significantly, with models pre-trained on large-scale\ndatasets showing superior performance. However, open-set scenarios require\nfurther improvement, especially at higher true positive identification rates,\ni.e., lower thresholds.\n", "link": "http://arxiv.org/abs/2409.07220v1", "date": "2024-09-11", "relevancy": 2.5889, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5179}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5179}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Watchlist%20Challenge%3A%203rd%20Open-set%20Face%20Detection%20and%20Identification&body=Title%3A%20Watchlist%20Challenge%3A%203rd%20Open-set%20Face%20Detection%20and%20Identification%0AAuthor%3A%20Furkan%20Kas%C4%B1m%20and%20Terrance%20E.%20Boult%20and%20Rensso%20Mora%20and%20Bernardo%20Biesseck%20and%20Rafael%20Ribeiro%20and%20Jan%20Schlueter%20and%20Tom%C3%A1%C5%A1%20Rep%C3%A1k%20and%20Rafael%20Henrique%20Vareto%20and%20David%20Menotti%20and%20William%20Robson%20Schwartz%20and%20Manuel%20G%C3%BCnther%0AAbstract%3A%20%20%20In%20the%20current%20landscape%20of%20biometrics%20and%20surveillance%2C%20the%20ability%20to%0Aaccurately%20recognize%20faces%20in%20uncontrolled%20settings%20is%20paramount.%20The%20Watchlist%0AChallenge%20addresses%20this%20critical%20need%20by%20focusing%20on%20face%20detection%20and%0Aopen-set%20identification%20in%20real-world%20surveillance%20scenarios.%20This%20paper%0Apresents%20a%20comprehensive%20evaluation%20of%20participating%20algorithms%2C%20using%20the%0Aenhanced%20UnConstrained%20College%20Students%20%28UCCS%29%20dataset%20with%20new%20evaluation%0Aprotocols.%20In%20total%2C%20four%20participants%20submitted%20four%20face%20detection%20and%20nine%0Aopen-set%20face%20recognition%20systems.%20The%20evaluation%20demonstrates%20that%20while%0Adetection%20capabilities%20are%20generally%20robust%2C%20closed-set%20identification%0Aperformance%20varies%20significantly%2C%20with%20models%20pre-trained%20on%20large-scale%0Adatasets%20showing%20superior%20performance.%20However%2C%20open-set%20scenarios%20require%0Afurther%20improvement%2C%20especially%20at%20higher%20true%20positive%20identification%20rates%2C%0Ai.e.%2C%20lower%20thresholds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07220v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWatchlist%2520Challenge%253A%25203rd%2520Open-set%2520Face%2520Detection%2520and%2520Identification%26entry.906535625%3DFurkan%2520Kas%25C4%25B1m%2520and%2520Terrance%2520E.%2520Boult%2520and%2520Rensso%2520Mora%2520and%2520Bernardo%2520Biesseck%2520and%2520Rafael%2520Ribeiro%2520and%2520Jan%2520Schlueter%2520and%2520Tom%25C3%25A1%25C5%25A1%2520Rep%25C3%25A1k%2520and%2520Rafael%2520Henrique%2520Vareto%2520and%2520David%2520Menotti%2520and%2520William%2520Robson%2520Schwartz%2520and%2520Manuel%2520G%25C3%25BCnther%26entry.1292438233%3D%2520%2520In%2520the%2520current%2520landscape%2520of%2520biometrics%2520and%2520surveillance%252C%2520the%2520ability%2520to%250Aaccurately%2520recognize%2520faces%2520in%2520uncontrolled%2520settings%2520is%2520paramount.%2520The%2520Watchlist%250AChallenge%2520addresses%2520this%2520critical%2520need%2520by%2520focusing%2520on%2520face%2520detection%2520and%250Aopen-set%2520identification%2520in%2520real-world%2520surveillance%2520scenarios.%2520This%2520paper%250Apresents%2520a%2520comprehensive%2520evaluation%2520of%2520participating%2520algorithms%252C%2520using%2520the%250Aenhanced%2520UnConstrained%2520College%2520Students%2520%2528UCCS%2529%2520dataset%2520with%2520new%2520evaluation%250Aprotocols.%2520In%2520total%252C%2520four%2520participants%2520submitted%2520four%2520face%2520detection%2520and%2520nine%250Aopen-set%2520face%2520recognition%2520systems.%2520The%2520evaluation%2520demonstrates%2520that%2520while%250Adetection%2520capabilities%2520are%2520generally%2520robust%252C%2520closed-set%2520identification%250Aperformance%2520varies%2520significantly%252C%2520with%2520models%2520pre-trained%2520on%2520large-scale%250Adatasets%2520showing%2520superior%2520performance.%2520However%252C%2520open-set%2520scenarios%2520require%250Afurther%2520improvement%252C%2520especially%2520at%2520higher%2520true%2520positive%2520identification%2520rates%252C%250Ai.e.%252C%2520lower%2520thresholds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07220v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Watchlist%20Challenge%3A%203rd%20Open-set%20Face%20Detection%20and%20Identification&entry.906535625=Furkan%20Kas%C4%B1m%20and%20Terrance%20E.%20Boult%20and%20Rensso%20Mora%20and%20Bernardo%20Biesseck%20and%20Rafael%20Ribeiro%20and%20Jan%20Schlueter%20and%20Tom%C3%A1%C5%A1%20Rep%C3%A1k%20and%20Rafael%20Henrique%20Vareto%20and%20David%20Menotti%20and%20William%20Robson%20Schwartz%20and%20Manuel%20G%C3%BCnther&entry.1292438233=%20%20In%20the%20current%20landscape%20of%20biometrics%20and%20surveillance%2C%20the%20ability%20to%0Aaccurately%20recognize%20faces%20in%20uncontrolled%20settings%20is%20paramount.%20The%20Watchlist%0AChallenge%20addresses%20this%20critical%20need%20by%20focusing%20on%20face%20detection%20and%0Aopen-set%20identification%20in%20real-world%20surveillance%20scenarios.%20This%20paper%0Apresents%20a%20comprehensive%20evaluation%20of%20participating%20algorithms%2C%20using%20the%0Aenhanced%20UnConstrained%20College%20Students%20%28UCCS%29%20dataset%20with%20new%20evaluation%0Aprotocols.%20In%20total%2C%20four%20participants%20submitted%20four%20face%20detection%20and%20nine%0Aopen-set%20face%20recognition%20systems.%20The%20evaluation%20demonstrates%20that%20while%0Adetection%20capabilities%20are%20generally%20robust%2C%20closed-set%20identification%0Aperformance%20varies%20significantly%2C%20with%20models%20pre-trained%20on%20large-scale%0Adatasets%20showing%20superior%20performance.%20However%2C%20open-set%20scenarios%20require%0Afurther%20improvement%2C%20especially%20at%20higher%20true%20positive%20identification%20rates%2C%0Ai.e.%2C%20lower%20thresholds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07220v1&entry.124074799=Read"},
{"title": "Deep Neural Network-Based Sign Language Recognition: A Comprehensive\n  Approach Using Transfer Learning with Explainability", "author": "A. E. M Ridwan and Mushfiqul Islam Chowdhury and Mekhala Mariam Mary and Md Tahmid Chowdhury Abir", "abstract": "  To promote inclusion and ensuring effective communication for those who rely\non sign language as their main form of communication, sign language recognition\n(SLR) is crucial. Sign language recognition (SLR) seamlessly incorporates with\ndiverse technology, enhancing accessibility for the deaf community by\nfacilitating their use of digital platforms, video calls, and communication\ndevices. To effectively solve this problem, we suggest a novel solution that\nuses a deep neural network to fully automate sign language recognition. This\nmethodology integrates sophisticated preprocessing methodologies to optimise\nthe overall performance. The architectures resnet, inception, xception, and vgg\nare utilised to selectively categorise images of sign language. We prepared a\nDNN architecture and merged it with the pre-processing architectures. In the\npost-processing phase, we utilised the SHAP deep explainer, which is based on\ncooperative game theory, to quantify the influence of specific features on the\noutput of a machine learning model. Bhutanese-Sign-Language (BSL) dataset was\nused for training and testing the suggested technique. While training on\nBhutanese-Sign-Language (BSL) dataset, overall ResNet50 with the DNN model\nperformed better accuracy which is 98.90%. Our model's ability to provide\ninformational clarity was assessed using the SHAP (SHapley Additive\nexPlanations) method. In part to its considerable robustness and reliability,\nthe proposed methodological approach can be used to develop a fully automated\nsystem for sign language recognition.\n", "link": "http://arxiv.org/abs/2409.07426v1", "date": "2024-09-11", "relevancy": 2.5882, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.518}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5174}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Neural%20Network-Based%20Sign%20Language%20Recognition%3A%20A%20Comprehensive%0A%20%20Approach%20Using%20Transfer%20Learning%20with%20Explainability&body=Title%3A%20Deep%20Neural%20Network-Based%20Sign%20Language%20Recognition%3A%20A%20Comprehensive%0A%20%20Approach%20Using%20Transfer%20Learning%20with%20Explainability%0AAuthor%3A%20A.%20E.%20M%20Ridwan%20and%20Mushfiqul%20Islam%20Chowdhury%20and%20Mekhala%20Mariam%20Mary%20and%20Md%20Tahmid%20Chowdhury%20Abir%0AAbstract%3A%20%20%20To%20promote%20inclusion%20and%20ensuring%20effective%20communication%20for%20those%20who%20rely%0Aon%20sign%20language%20as%20their%20main%20form%20of%20communication%2C%20sign%20language%20recognition%0A%28SLR%29%20is%20crucial.%20Sign%20language%20recognition%20%28SLR%29%20seamlessly%20incorporates%20with%0Adiverse%20technology%2C%20enhancing%20accessibility%20for%20the%20deaf%20community%20by%0Afacilitating%20their%20use%20of%20digital%20platforms%2C%20video%20calls%2C%20and%20communication%0Adevices.%20To%20effectively%20solve%20this%20problem%2C%20we%20suggest%20a%20novel%20solution%20that%0Auses%20a%20deep%20neural%20network%20to%20fully%20automate%20sign%20language%20recognition.%20This%0Amethodology%20integrates%20sophisticated%20preprocessing%20methodologies%20to%20optimise%0Athe%20overall%20performance.%20The%20architectures%20resnet%2C%20inception%2C%20xception%2C%20and%20vgg%0Aare%20utilised%20to%20selectively%20categorise%20images%20of%20sign%20language.%20We%20prepared%20a%0ADNN%20architecture%20and%20merged%20it%20with%20the%20pre-processing%20architectures.%20In%20the%0Apost-processing%20phase%2C%20we%20utilised%20the%20SHAP%20deep%20explainer%2C%20which%20is%20based%20on%0Acooperative%20game%20theory%2C%20to%20quantify%20the%20influence%20of%20specific%20features%20on%20the%0Aoutput%20of%20a%20machine%20learning%20model.%20Bhutanese-Sign-Language%20%28BSL%29%20dataset%20was%0Aused%20for%20training%20and%20testing%20the%20suggested%20technique.%20While%20training%20on%0ABhutanese-Sign-Language%20%28BSL%29%20dataset%2C%20overall%20ResNet50%20with%20the%20DNN%20model%0Aperformed%20better%20accuracy%20which%20is%2098.90%25.%20Our%20model%27s%20ability%20to%20provide%0Ainformational%20clarity%20was%20assessed%20using%20the%20SHAP%20%28SHapley%20Additive%0AexPlanations%29%20method.%20In%20part%20to%20its%20considerable%20robustness%20and%20reliability%2C%0Athe%20proposed%20methodological%20approach%20can%20be%20used%20to%20develop%20a%20fully%20automated%0Asystem%20for%20sign%20language%20recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07426v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Neural%2520Network-Based%2520Sign%2520Language%2520Recognition%253A%2520A%2520Comprehensive%250A%2520%2520Approach%2520Using%2520Transfer%2520Learning%2520with%2520Explainability%26entry.906535625%3DA.%2520E.%2520M%2520Ridwan%2520and%2520Mushfiqul%2520Islam%2520Chowdhury%2520and%2520Mekhala%2520Mariam%2520Mary%2520and%2520Md%2520Tahmid%2520Chowdhury%2520Abir%26entry.1292438233%3D%2520%2520To%2520promote%2520inclusion%2520and%2520ensuring%2520effective%2520communication%2520for%2520those%2520who%2520rely%250Aon%2520sign%2520language%2520as%2520their%2520main%2520form%2520of%2520communication%252C%2520sign%2520language%2520recognition%250A%2528SLR%2529%2520is%2520crucial.%2520Sign%2520language%2520recognition%2520%2528SLR%2529%2520seamlessly%2520incorporates%2520with%250Adiverse%2520technology%252C%2520enhancing%2520accessibility%2520for%2520the%2520deaf%2520community%2520by%250Afacilitating%2520their%2520use%2520of%2520digital%2520platforms%252C%2520video%2520calls%252C%2520and%2520communication%250Adevices.%2520To%2520effectively%2520solve%2520this%2520problem%252C%2520we%2520suggest%2520a%2520novel%2520solution%2520that%250Auses%2520a%2520deep%2520neural%2520network%2520to%2520fully%2520automate%2520sign%2520language%2520recognition.%2520This%250Amethodology%2520integrates%2520sophisticated%2520preprocessing%2520methodologies%2520to%2520optimise%250Athe%2520overall%2520performance.%2520The%2520architectures%2520resnet%252C%2520inception%252C%2520xception%252C%2520and%2520vgg%250Aare%2520utilised%2520to%2520selectively%2520categorise%2520images%2520of%2520sign%2520language.%2520We%2520prepared%2520a%250ADNN%2520architecture%2520and%2520merged%2520it%2520with%2520the%2520pre-processing%2520architectures.%2520In%2520the%250Apost-processing%2520phase%252C%2520we%2520utilised%2520the%2520SHAP%2520deep%2520explainer%252C%2520which%2520is%2520based%2520on%250Acooperative%2520game%2520theory%252C%2520to%2520quantify%2520the%2520influence%2520of%2520specific%2520features%2520on%2520the%250Aoutput%2520of%2520a%2520machine%2520learning%2520model.%2520Bhutanese-Sign-Language%2520%2528BSL%2529%2520dataset%2520was%250Aused%2520for%2520training%2520and%2520testing%2520the%2520suggested%2520technique.%2520While%2520training%2520on%250ABhutanese-Sign-Language%2520%2528BSL%2529%2520dataset%252C%2520overall%2520ResNet50%2520with%2520the%2520DNN%2520model%250Aperformed%2520better%2520accuracy%2520which%2520is%252098.90%2525.%2520Our%2520model%2527s%2520ability%2520to%2520provide%250Ainformational%2520clarity%2520was%2520assessed%2520using%2520the%2520SHAP%2520%2528SHapley%2520Additive%250AexPlanations%2529%2520method.%2520In%2520part%2520to%2520its%2520considerable%2520robustness%2520and%2520reliability%252C%250Athe%2520proposed%2520methodological%2520approach%2520can%2520be%2520used%2520to%2520develop%2520a%2520fully%2520automated%250Asystem%2520for%2520sign%2520language%2520recognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07426v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Neural%20Network-Based%20Sign%20Language%20Recognition%3A%20A%20Comprehensive%0A%20%20Approach%20Using%20Transfer%20Learning%20with%20Explainability&entry.906535625=A.%20E.%20M%20Ridwan%20and%20Mushfiqul%20Islam%20Chowdhury%20and%20Mekhala%20Mariam%20Mary%20and%20Md%20Tahmid%20Chowdhury%20Abir&entry.1292438233=%20%20To%20promote%20inclusion%20and%20ensuring%20effective%20communication%20for%20those%20who%20rely%0Aon%20sign%20language%20as%20their%20main%20form%20of%20communication%2C%20sign%20language%20recognition%0A%28SLR%29%20is%20crucial.%20Sign%20language%20recognition%20%28SLR%29%20seamlessly%20incorporates%20with%0Adiverse%20technology%2C%20enhancing%20accessibility%20for%20the%20deaf%20community%20by%0Afacilitating%20their%20use%20of%20digital%20platforms%2C%20video%20calls%2C%20and%20communication%0Adevices.%20To%20effectively%20solve%20this%20problem%2C%20we%20suggest%20a%20novel%20solution%20that%0Auses%20a%20deep%20neural%20network%20to%20fully%20automate%20sign%20language%20recognition.%20This%0Amethodology%20integrates%20sophisticated%20preprocessing%20methodologies%20to%20optimise%0Athe%20overall%20performance.%20The%20architectures%20resnet%2C%20inception%2C%20xception%2C%20and%20vgg%0Aare%20utilised%20to%20selectively%20categorise%20images%20of%20sign%20language.%20We%20prepared%20a%0ADNN%20architecture%20and%20merged%20it%20with%20the%20pre-processing%20architectures.%20In%20the%0Apost-processing%20phase%2C%20we%20utilised%20the%20SHAP%20deep%20explainer%2C%20which%20is%20based%20on%0Acooperative%20game%20theory%2C%20to%20quantify%20the%20influence%20of%20specific%20features%20on%20the%0Aoutput%20of%20a%20machine%20learning%20model.%20Bhutanese-Sign-Language%20%28BSL%29%20dataset%20was%0Aused%20for%20training%20and%20testing%20the%20suggested%20technique.%20While%20training%20on%0ABhutanese-Sign-Language%20%28BSL%29%20dataset%2C%20overall%20ResNet50%20with%20the%20DNN%20model%0Aperformed%20better%20accuracy%20which%20is%2098.90%25.%20Our%20model%27s%20ability%20to%20provide%0Ainformational%20clarity%20was%20assessed%20using%20the%20SHAP%20%28SHapley%20Additive%0AexPlanations%29%20method.%20In%20part%20to%20its%20considerable%20robustness%20and%20reliability%2C%0Athe%20proposed%20methodological%20approach%20can%20be%20used%20to%20develop%20a%20fully%20automated%0Asystem%20for%20sign%20language%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07426v1&entry.124074799=Read"},
{"title": "Learning to Generate Instruction Tuning Datasets for Zero-Shot Task\n  Adaptation", "author": "Nihal V. Nayak and Yiyang Nan and Avi Trost and Stephen H. Bach", "abstract": "  We introduce Bonito, an open-source model for conditional task generation\nthat converts unannotated text into task-specific training datasets for\ninstruction tuning. We aim to enable zero-shot task adaptation of large\nlanguage models on users' specialized, private data. We train Bonito by\nfine-tuning a pretrained large language model on a new large-scale dataset with\n1.65M examples created by remixing existing instruction tuning datasets into\nmeta-templates. The meta-templates for a dataset produce training examples\nwhere the input is the unannotated text and the task attribute and the output\nconsists of the instruction and the response. We use Bonito to generate\nsynthetic tasks for seven datasets from specialized domains with unannotated\ntext across three task types -- yes-no question answering, extractive question\nanswering, and natural language inference -- and adapt language models. We show\nthat Bonito significantly improves the average performance of pretrained and\ninstruction tuned models over the de facto self supervised baseline. For\nexample, adapting Mistral-Instruct-v2 and instruction tuned variants of Mistral\nand Llama2 with Bonito improves the strong zero-shot performance by 22.1 F1\npoints whereas the next word prediction objective undoes some of the benefits\nof instruction tuning and reduces the average performance by 0.8 F1 points. We\nconduct additional experiments with Bonito to understand the effects of the\ndomain, the size of the training set, and the choice of alternative synthetic\ntask generators. Overall, we show that learning with synthetic instruction\ntuning datasets is an effective way to adapt language models to new domains.\nThe model, dataset, and code are available at\nhttps://github.com/BatsResearch/bonito.\n", "link": "http://arxiv.org/abs/2402.18334v3", "date": "2024-09-11", "relevancy": 2.5509, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5152}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.508}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Generate%20Instruction%20Tuning%20Datasets%20for%20Zero-Shot%20Task%0A%20%20Adaptation&body=Title%3A%20Learning%20to%20Generate%20Instruction%20Tuning%20Datasets%20for%20Zero-Shot%20Task%0A%20%20Adaptation%0AAuthor%3A%20Nihal%20V.%20Nayak%20and%20Yiyang%20Nan%20and%20Avi%20Trost%20and%20Stephen%20H.%20Bach%0AAbstract%3A%20%20%20We%20introduce%20Bonito%2C%20an%20open-source%20model%20for%20conditional%20task%20generation%0Athat%20converts%20unannotated%20text%20into%20task-specific%20training%20datasets%20for%0Ainstruction%20tuning.%20We%20aim%20to%20enable%20zero-shot%20task%20adaptation%20of%20large%0Alanguage%20models%20on%20users%27%20specialized%2C%20private%20data.%20We%20train%20Bonito%20by%0Afine-tuning%20a%20pretrained%20large%20language%20model%20on%20a%20new%20large-scale%20dataset%20with%0A1.65M%20examples%20created%20by%20remixing%20existing%20instruction%20tuning%20datasets%20into%0Ameta-templates.%20The%20meta-templates%20for%20a%20dataset%20produce%20training%20examples%0Awhere%20the%20input%20is%20the%20unannotated%20text%20and%20the%20task%20attribute%20and%20the%20output%0Aconsists%20of%20the%20instruction%20and%20the%20response.%20We%20use%20Bonito%20to%20generate%0Asynthetic%20tasks%20for%20seven%20datasets%20from%20specialized%20domains%20with%20unannotated%0Atext%20across%20three%20task%20types%20--%20yes-no%20question%20answering%2C%20extractive%20question%0Aanswering%2C%20and%20natural%20language%20inference%20--%20and%20adapt%20language%20models.%20We%20show%0Athat%20Bonito%20significantly%20improves%20the%20average%20performance%20of%20pretrained%20and%0Ainstruction%20tuned%20models%20over%20the%20de%20facto%20self%20supervised%20baseline.%20For%0Aexample%2C%20adapting%20Mistral-Instruct-v2%20and%20instruction%20tuned%20variants%20of%20Mistral%0Aand%20Llama2%20with%20Bonito%20improves%20the%20strong%20zero-shot%20performance%20by%2022.1%20F1%0Apoints%20whereas%20the%20next%20word%20prediction%20objective%20undoes%20some%20of%20the%20benefits%0Aof%20instruction%20tuning%20and%20reduces%20the%20average%20performance%20by%200.8%20F1%20points.%20We%0Aconduct%20additional%20experiments%20with%20Bonito%20to%20understand%20the%20effects%20of%20the%0Adomain%2C%20the%20size%20of%20the%20training%20set%2C%20and%20the%20choice%20of%20alternative%20synthetic%0Atask%20generators.%20Overall%2C%20we%20show%20that%20learning%20with%20synthetic%20instruction%0Atuning%20datasets%20is%20an%20effective%20way%20to%20adapt%20language%20models%20to%20new%20domains.%0AThe%20model%2C%20dataset%2C%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/BatsResearch/bonito.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18334v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Generate%2520Instruction%2520Tuning%2520Datasets%2520for%2520Zero-Shot%2520Task%250A%2520%2520Adaptation%26entry.906535625%3DNihal%2520V.%2520Nayak%2520and%2520Yiyang%2520Nan%2520and%2520Avi%2520Trost%2520and%2520Stephen%2520H.%2520Bach%26entry.1292438233%3D%2520%2520We%2520introduce%2520Bonito%252C%2520an%2520open-source%2520model%2520for%2520conditional%2520task%2520generation%250Athat%2520converts%2520unannotated%2520text%2520into%2520task-specific%2520training%2520datasets%2520for%250Ainstruction%2520tuning.%2520We%2520aim%2520to%2520enable%2520zero-shot%2520task%2520adaptation%2520of%2520large%250Alanguage%2520models%2520on%2520users%2527%2520specialized%252C%2520private%2520data.%2520We%2520train%2520Bonito%2520by%250Afine-tuning%2520a%2520pretrained%2520large%2520language%2520model%2520on%2520a%2520new%2520large-scale%2520dataset%2520with%250A1.65M%2520examples%2520created%2520by%2520remixing%2520existing%2520instruction%2520tuning%2520datasets%2520into%250Ameta-templates.%2520The%2520meta-templates%2520for%2520a%2520dataset%2520produce%2520training%2520examples%250Awhere%2520the%2520input%2520is%2520the%2520unannotated%2520text%2520and%2520the%2520task%2520attribute%2520and%2520the%2520output%250Aconsists%2520of%2520the%2520instruction%2520and%2520the%2520response.%2520We%2520use%2520Bonito%2520to%2520generate%250Asynthetic%2520tasks%2520for%2520seven%2520datasets%2520from%2520specialized%2520domains%2520with%2520unannotated%250Atext%2520across%2520three%2520task%2520types%2520--%2520yes-no%2520question%2520answering%252C%2520extractive%2520question%250Aanswering%252C%2520and%2520natural%2520language%2520inference%2520--%2520and%2520adapt%2520language%2520models.%2520We%2520show%250Athat%2520Bonito%2520significantly%2520improves%2520the%2520average%2520performance%2520of%2520pretrained%2520and%250Ainstruction%2520tuned%2520models%2520over%2520the%2520de%2520facto%2520self%2520supervised%2520baseline.%2520For%250Aexample%252C%2520adapting%2520Mistral-Instruct-v2%2520and%2520instruction%2520tuned%2520variants%2520of%2520Mistral%250Aand%2520Llama2%2520with%2520Bonito%2520improves%2520the%2520strong%2520zero-shot%2520performance%2520by%252022.1%2520F1%250Apoints%2520whereas%2520the%2520next%2520word%2520prediction%2520objective%2520undoes%2520some%2520of%2520the%2520benefits%250Aof%2520instruction%2520tuning%2520and%2520reduces%2520the%2520average%2520performance%2520by%25200.8%2520F1%2520points.%2520We%250Aconduct%2520additional%2520experiments%2520with%2520Bonito%2520to%2520understand%2520the%2520effects%2520of%2520the%250Adomain%252C%2520the%2520size%2520of%2520the%2520training%2520set%252C%2520and%2520the%2520choice%2520of%2520alternative%2520synthetic%250Atask%2520generators.%2520Overall%252C%2520we%2520show%2520that%2520learning%2520with%2520synthetic%2520instruction%250Atuning%2520datasets%2520is%2520an%2520effective%2520way%2520to%2520adapt%2520language%2520models%2520to%2520new%2520domains.%250AThe%2520model%252C%2520dataset%252C%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/BatsResearch/bonito.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.18334v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Generate%20Instruction%20Tuning%20Datasets%20for%20Zero-Shot%20Task%0A%20%20Adaptation&entry.906535625=Nihal%20V.%20Nayak%20and%20Yiyang%20Nan%20and%20Avi%20Trost%20and%20Stephen%20H.%20Bach&entry.1292438233=%20%20We%20introduce%20Bonito%2C%20an%20open-source%20model%20for%20conditional%20task%20generation%0Athat%20converts%20unannotated%20text%20into%20task-specific%20training%20datasets%20for%0Ainstruction%20tuning.%20We%20aim%20to%20enable%20zero-shot%20task%20adaptation%20of%20large%0Alanguage%20models%20on%20users%27%20specialized%2C%20private%20data.%20We%20train%20Bonito%20by%0Afine-tuning%20a%20pretrained%20large%20language%20model%20on%20a%20new%20large-scale%20dataset%20with%0A1.65M%20examples%20created%20by%20remixing%20existing%20instruction%20tuning%20datasets%20into%0Ameta-templates.%20The%20meta-templates%20for%20a%20dataset%20produce%20training%20examples%0Awhere%20the%20input%20is%20the%20unannotated%20text%20and%20the%20task%20attribute%20and%20the%20output%0Aconsists%20of%20the%20instruction%20and%20the%20response.%20We%20use%20Bonito%20to%20generate%0Asynthetic%20tasks%20for%20seven%20datasets%20from%20specialized%20domains%20with%20unannotated%0Atext%20across%20three%20task%20types%20--%20yes-no%20question%20answering%2C%20extractive%20question%0Aanswering%2C%20and%20natural%20language%20inference%20--%20and%20adapt%20language%20models.%20We%20show%0Athat%20Bonito%20significantly%20improves%20the%20average%20performance%20of%20pretrained%20and%0Ainstruction%20tuned%20models%20over%20the%20de%20facto%20self%20supervised%20baseline.%20For%0Aexample%2C%20adapting%20Mistral-Instruct-v2%20and%20instruction%20tuned%20variants%20of%20Mistral%0Aand%20Llama2%20with%20Bonito%20improves%20the%20strong%20zero-shot%20performance%20by%2022.1%20F1%0Apoints%20whereas%20the%20next%20word%20prediction%20objective%20undoes%20some%20of%20the%20benefits%0Aof%20instruction%20tuning%20and%20reduces%20the%20average%20performance%20by%200.8%20F1%20points.%20We%0Aconduct%20additional%20experiments%20with%20Bonito%20to%20understand%20the%20effects%20of%20the%0Adomain%2C%20the%20size%20of%20the%20training%20set%2C%20and%20the%20choice%20of%20alternative%20synthetic%0Atask%20generators.%20Overall%2C%20we%20show%20that%20learning%20with%20synthetic%20instruction%0Atuning%20datasets%20is%20an%20effective%20way%20to%20adapt%20language%20models%20to%20new%20domains.%0AThe%20model%2C%20dataset%2C%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/BatsResearch/bonito.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18334v3&entry.124074799=Read"},
{"title": "Benchmarking 2D Egocentric Hand Pose Datasets", "author": "Olga Taran and Damian M. Manzone and Jose Zariffa", "abstract": "  Hand pose estimation from egocentric video has broad implications across\nvarious domains, including human-computer interaction, assistive technologies,\nactivity recognition, and robotics, making it a topic of significant research\ninterest. The efficacy of modern machine learning models depends on the quality\nof data used for their training. Thus, this work is devoted to the analysis of\nstate-of-the-art egocentric datasets suitable for 2D hand pose estimation. We\npropose a novel protocol for dataset evaluation, which encompasses not only the\nanalysis of stated dataset characteristics and assessment of data quality, but\nalso the identification of dataset shortcomings through the evaluation of\nstate-of-the-art hand pose estimation models. Our study reveals that despite\nthe availability of numerous egocentric databases intended for 2D hand pose\nestimation, the majority are tailored for specific use cases. There is no ideal\nbenchmark dataset yet; however, H2O and GANerated Hands datasets emerge as the\nmost promising real and synthetic datasets, respectively.\n", "link": "http://arxiv.org/abs/2409.07337v1", "date": "2024-09-11", "relevancy": 2.538, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5142}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5101}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%202D%20Egocentric%20Hand%20Pose%20Datasets&body=Title%3A%20Benchmarking%202D%20Egocentric%20Hand%20Pose%20Datasets%0AAuthor%3A%20Olga%20Taran%20and%20Damian%20M.%20Manzone%20and%20Jose%20Zariffa%0AAbstract%3A%20%20%20Hand%20pose%20estimation%20from%20egocentric%20video%20has%20broad%20implications%20across%0Avarious%20domains%2C%20including%20human-computer%20interaction%2C%20assistive%20technologies%2C%0Aactivity%20recognition%2C%20and%20robotics%2C%20making%20it%20a%20topic%20of%20significant%20research%0Ainterest.%20The%20efficacy%20of%20modern%20machine%20learning%20models%20depends%20on%20the%20quality%0Aof%20data%20used%20for%20their%20training.%20Thus%2C%20this%20work%20is%20devoted%20to%20the%20analysis%20of%0Astate-of-the-art%20egocentric%20datasets%20suitable%20for%202D%20hand%20pose%20estimation.%20We%0Apropose%20a%20novel%20protocol%20for%20dataset%20evaluation%2C%20which%20encompasses%20not%20only%20the%0Aanalysis%20of%20stated%20dataset%20characteristics%20and%20assessment%20of%20data%20quality%2C%20but%0Aalso%20the%20identification%20of%20dataset%20shortcomings%20through%20the%20evaluation%20of%0Astate-of-the-art%20hand%20pose%20estimation%20models.%20Our%20study%20reveals%20that%20despite%0Athe%20availability%20of%20numerous%20egocentric%20databases%20intended%20for%202D%20hand%20pose%0Aestimation%2C%20the%20majority%20are%20tailored%20for%20specific%20use%20cases.%20There%20is%20no%20ideal%0Abenchmark%20dataset%20yet%3B%20however%2C%20H2O%20and%20GANerated%20Hands%20datasets%20emerge%20as%20the%0Amost%20promising%20real%20and%20synthetic%20datasets%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07337v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%25202D%2520Egocentric%2520Hand%2520Pose%2520Datasets%26entry.906535625%3DOlga%2520Taran%2520and%2520Damian%2520M.%2520Manzone%2520and%2520Jose%2520Zariffa%26entry.1292438233%3D%2520%2520Hand%2520pose%2520estimation%2520from%2520egocentric%2520video%2520has%2520broad%2520implications%2520across%250Avarious%2520domains%252C%2520including%2520human-computer%2520interaction%252C%2520assistive%2520technologies%252C%250Aactivity%2520recognition%252C%2520and%2520robotics%252C%2520making%2520it%2520a%2520topic%2520of%2520significant%2520research%250Ainterest.%2520The%2520efficacy%2520of%2520modern%2520machine%2520learning%2520models%2520depends%2520on%2520the%2520quality%250Aof%2520data%2520used%2520for%2520their%2520training.%2520Thus%252C%2520this%2520work%2520is%2520devoted%2520to%2520the%2520analysis%2520of%250Astate-of-the-art%2520egocentric%2520datasets%2520suitable%2520for%25202D%2520hand%2520pose%2520estimation.%2520We%250Apropose%2520a%2520novel%2520protocol%2520for%2520dataset%2520evaluation%252C%2520which%2520encompasses%2520not%2520only%2520the%250Aanalysis%2520of%2520stated%2520dataset%2520characteristics%2520and%2520assessment%2520of%2520data%2520quality%252C%2520but%250Aalso%2520the%2520identification%2520of%2520dataset%2520shortcomings%2520through%2520the%2520evaluation%2520of%250Astate-of-the-art%2520hand%2520pose%2520estimation%2520models.%2520Our%2520study%2520reveals%2520that%2520despite%250Athe%2520availability%2520of%2520numerous%2520egocentric%2520databases%2520intended%2520for%25202D%2520hand%2520pose%250Aestimation%252C%2520the%2520majority%2520are%2520tailored%2520for%2520specific%2520use%2520cases.%2520There%2520is%2520no%2520ideal%250Abenchmark%2520dataset%2520yet%253B%2520however%252C%2520H2O%2520and%2520GANerated%2520Hands%2520datasets%2520emerge%2520as%2520the%250Amost%2520promising%2520real%2520and%2520synthetic%2520datasets%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07337v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%202D%20Egocentric%20Hand%20Pose%20Datasets&entry.906535625=Olga%20Taran%20and%20Damian%20M.%20Manzone%20and%20Jose%20Zariffa&entry.1292438233=%20%20Hand%20pose%20estimation%20from%20egocentric%20video%20has%20broad%20implications%20across%0Avarious%20domains%2C%20including%20human-computer%20interaction%2C%20assistive%20technologies%2C%0Aactivity%20recognition%2C%20and%20robotics%2C%20making%20it%20a%20topic%20of%20significant%20research%0Ainterest.%20The%20efficacy%20of%20modern%20machine%20learning%20models%20depends%20on%20the%20quality%0Aof%20data%20used%20for%20their%20training.%20Thus%2C%20this%20work%20is%20devoted%20to%20the%20analysis%20of%0Astate-of-the-art%20egocentric%20datasets%20suitable%20for%202D%20hand%20pose%20estimation.%20We%0Apropose%20a%20novel%20protocol%20for%20dataset%20evaluation%2C%20which%20encompasses%20not%20only%20the%0Aanalysis%20of%20stated%20dataset%20characteristics%20and%20assessment%20of%20data%20quality%2C%20but%0Aalso%20the%20identification%20of%20dataset%20shortcomings%20through%20the%20evaluation%20of%0Astate-of-the-art%20hand%20pose%20estimation%20models.%20Our%20study%20reveals%20that%20despite%0Athe%20availability%20of%20numerous%20egocentric%20databases%20intended%20for%202D%20hand%20pose%0Aestimation%2C%20the%20majority%20are%20tailored%20for%20specific%20use%20cases.%20There%20is%20no%20ideal%0Abenchmark%20dataset%20yet%3B%20however%2C%20H2O%20and%20GANerated%20Hands%20datasets%20emerge%20as%20the%0Amost%20promising%20real%20and%20synthetic%20datasets%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07337v1&entry.124074799=Read"},
{"title": "A Unified Contrastive Loss for Self-Training", "author": "Aurelien Gauffre and Julien Horvat and Massih-Reza Amini", "abstract": "  Self-training methods have proven to be effective in exploiting abundant\nunlabeled data in semi-supervised learning, particularly when labeled data is\nscarce. While many of these approaches rely on a cross-entropy loss function\n(CE), recent advances have shown that the supervised contrastive loss function\n(SupCon) can be more effective. Additionally, unsupervised contrastive learning\napproaches have also been shown to capture high quality data representations in\nthe unsupervised setting. To benefit from these advantages in a semi-supervised\nsetting, we propose a general framework to enhance self-training methods, which\nreplaces all instances of CE losses with a unique contrastive loss. By using\nclass prototypes, which are a set of class-wise trainable parameters, we\nrecover the probability distributions of the CE setting and show a theoretical\nequivalence with it. Our framework, when applied to popular self-training\nmethods, results in significant performance improvements across three different\ndatasets with a limited number of labeled data. Additionally, we demonstrate\nfurther improvements in convergence speed, transfer ability, and hyperparameter\nstability. The code is available at\n\\url{https://github.com/AurelienGauffre/semisupcon/}.\n", "link": "http://arxiv.org/abs/2409.07292v1", "date": "2024-09-11", "relevancy": 2.5249, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5286}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4943}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Contrastive%20Loss%20for%20Self-Training&body=Title%3A%20A%20Unified%20Contrastive%20Loss%20for%20Self-Training%0AAuthor%3A%20Aurelien%20Gauffre%20and%20Julien%20Horvat%20and%20Massih-Reza%20Amini%0AAbstract%3A%20%20%20Self-training%20methods%20have%20proven%20to%20be%20effective%20in%20exploiting%20abundant%0Aunlabeled%20data%20in%20semi-supervised%20learning%2C%20particularly%20when%20labeled%20data%20is%0Ascarce.%20While%20many%20of%20these%20approaches%20rely%20on%20a%20cross-entropy%20loss%20function%0A%28CE%29%2C%20recent%20advances%20have%20shown%20that%20the%20supervised%20contrastive%20loss%20function%0A%28SupCon%29%20can%20be%20more%20effective.%20Additionally%2C%20unsupervised%20contrastive%20learning%0Aapproaches%20have%20also%20been%20shown%20to%20capture%20high%20quality%20data%20representations%20in%0Athe%20unsupervised%20setting.%20To%20benefit%20from%20these%20advantages%20in%20a%20semi-supervised%0Asetting%2C%20we%20propose%20a%20general%20framework%20to%20enhance%20self-training%20methods%2C%20which%0Areplaces%20all%20instances%20of%20CE%20losses%20with%20a%20unique%20contrastive%20loss.%20By%20using%0Aclass%20prototypes%2C%20which%20are%20a%20set%20of%20class-wise%20trainable%20parameters%2C%20we%0Arecover%20the%20probability%20distributions%20of%20the%20CE%20setting%20and%20show%20a%20theoretical%0Aequivalence%20with%20it.%20Our%20framework%2C%20when%20applied%20to%20popular%20self-training%0Amethods%2C%20results%20in%20significant%20performance%20improvements%20across%20three%20different%0Adatasets%20with%20a%20limited%20number%20of%20labeled%20data.%20Additionally%2C%20we%20demonstrate%0Afurther%20improvements%20in%20convergence%20speed%2C%20transfer%20ability%2C%20and%20hyperparameter%0Astability.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/AurelienGauffre/semisupcon/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Contrastive%2520Loss%2520for%2520Self-Training%26entry.906535625%3DAurelien%2520Gauffre%2520and%2520Julien%2520Horvat%2520and%2520Massih-Reza%2520Amini%26entry.1292438233%3D%2520%2520Self-training%2520methods%2520have%2520proven%2520to%2520be%2520effective%2520in%2520exploiting%2520abundant%250Aunlabeled%2520data%2520in%2520semi-supervised%2520learning%252C%2520particularly%2520when%2520labeled%2520data%2520is%250Ascarce.%2520While%2520many%2520of%2520these%2520approaches%2520rely%2520on%2520a%2520cross-entropy%2520loss%2520function%250A%2528CE%2529%252C%2520recent%2520advances%2520have%2520shown%2520that%2520the%2520supervised%2520contrastive%2520loss%2520function%250A%2528SupCon%2529%2520can%2520be%2520more%2520effective.%2520Additionally%252C%2520unsupervised%2520contrastive%2520learning%250Aapproaches%2520have%2520also%2520been%2520shown%2520to%2520capture%2520high%2520quality%2520data%2520representations%2520in%250Athe%2520unsupervised%2520setting.%2520To%2520benefit%2520from%2520these%2520advantages%2520in%2520a%2520semi-supervised%250Asetting%252C%2520we%2520propose%2520a%2520general%2520framework%2520to%2520enhance%2520self-training%2520methods%252C%2520which%250Areplaces%2520all%2520instances%2520of%2520CE%2520losses%2520with%2520a%2520unique%2520contrastive%2520loss.%2520By%2520using%250Aclass%2520prototypes%252C%2520which%2520are%2520a%2520set%2520of%2520class-wise%2520trainable%2520parameters%252C%2520we%250Arecover%2520the%2520probability%2520distributions%2520of%2520the%2520CE%2520setting%2520and%2520show%2520a%2520theoretical%250Aequivalence%2520with%2520it.%2520Our%2520framework%252C%2520when%2520applied%2520to%2520popular%2520self-training%250Amethods%252C%2520results%2520in%2520significant%2520performance%2520improvements%2520across%2520three%2520different%250Adatasets%2520with%2520a%2520limited%2520number%2520of%2520labeled%2520data.%2520Additionally%252C%2520we%2520demonstrate%250Afurther%2520improvements%2520in%2520convergence%2520speed%252C%2520transfer%2520ability%252C%2520and%2520hyperparameter%250Astability.%2520The%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/AurelienGauffre/semisupcon/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Contrastive%20Loss%20for%20Self-Training&entry.906535625=Aurelien%20Gauffre%20and%20Julien%20Horvat%20and%20Massih-Reza%20Amini&entry.1292438233=%20%20Self-training%20methods%20have%20proven%20to%20be%20effective%20in%20exploiting%20abundant%0Aunlabeled%20data%20in%20semi-supervised%20learning%2C%20particularly%20when%20labeled%20data%20is%0Ascarce.%20While%20many%20of%20these%20approaches%20rely%20on%20a%20cross-entropy%20loss%20function%0A%28CE%29%2C%20recent%20advances%20have%20shown%20that%20the%20supervised%20contrastive%20loss%20function%0A%28SupCon%29%20can%20be%20more%20effective.%20Additionally%2C%20unsupervised%20contrastive%20learning%0Aapproaches%20have%20also%20been%20shown%20to%20capture%20high%20quality%20data%20representations%20in%0Athe%20unsupervised%20setting.%20To%20benefit%20from%20these%20advantages%20in%20a%20semi-supervised%0Asetting%2C%20we%20propose%20a%20general%20framework%20to%20enhance%20self-training%20methods%2C%20which%0Areplaces%20all%20instances%20of%20CE%20losses%20with%20a%20unique%20contrastive%20loss.%20By%20using%0Aclass%20prototypes%2C%20which%20are%20a%20set%20of%20class-wise%20trainable%20parameters%2C%20we%0Arecover%20the%20probability%20distributions%20of%20the%20CE%20setting%20and%20show%20a%20theoretical%0Aequivalence%20with%20it.%20Our%20framework%2C%20when%20applied%20to%20popular%20self-training%0Amethods%2C%20results%20in%20significant%20performance%20improvements%20across%20three%20different%0Adatasets%20with%20a%20limited%20number%20of%20labeled%20data.%20Additionally%2C%20we%20demonstrate%0Afurther%20improvements%20in%20convergence%20speed%2C%20transfer%20ability%2C%20and%20hyperparameter%0Astability.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/AurelienGauffre/semisupcon/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07292v1&entry.124074799=Read"},
{"title": "Enhancing CTC-Based Visual Speech Recognition", "author": "Hendrik Laux and Anke Schmeink", "abstract": "  This paper presents LiteVSR2, an enhanced version of our previously\nintroduced efficient approach to Visual Speech Recognition (VSR). Building upon\nour knowledge distillation framework from a pre-trained Automatic Speech\nRecognition (ASR) model, we introduce two key improvements: a stabilized video\npreprocessing technique and feature normalization in the distillation process.\nThese improvements yield substantial performance gains on the LRS2 and LRS3\nbenchmarks, positioning LiteVSR2 as the current best CTC-based VSR model\nwithout increasing the volume of training data or computational resources\nutilized. Furthermore, we explore the scalability of our approach by examining\nperformance metrics across varying model complexities and training data\nvolumes. LiteVSR2 maintains the efficiency of its predecessor while\nsignificantly enhancing accuracy, thereby demonstrating the potential for\nresource-efficient advancements in VSR technology.\n", "link": "http://arxiv.org/abs/2409.07210v1", "date": "2024-09-11", "relevancy": 2.4922, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5005}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5005}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20CTC-Based%20Visual%20Speech%20Recognition&body=Title%3A%20Enhancing%20CTC-Based%20Visual%20Speech%20Recognition%0AAuthor%3A%20Hendrik%20Laux%20and%20Anke%20Schmeink%0AAbstract%3A%20%20%20This%20paper%20presents%20LiteVSR2%2C%20an%20enhanced%20version%20of%20our%20previously%0Aintroduced%20efficient%20approach%20to%20Visual%20Speech%20Recognition%20%28VSR%29.%20Building%20upon%0Aour%20knowledge%20distillation%20framework%20from%20a%20pre-trained%20Automatic%20Speech%0ARecognition%20%28ASR%29%20model%2C%20we%20introduce%20two%20key%20improvements%3A%20a%20stabilized%20video%0Apreprocessing%20technique%20and%20feature%20normalization%20in%20the%20distillation%20process.%0AThese%20improvements%20yield%20substantial%20performance%20gains%20on%20the%20LRS2%20and%20LRS3%0Abenchmarks%2C%20positioning%20LiteVSR2%20as%20the%20current%20best%20CTC-based%20VSR%20model%0Awithout%20increasing%20the%20volume%20of%20training%20data%20or%20computational%20resources%0Autilized.%20Furthermore%2C%20we%20explore%20the%20scalability%20of%20our%20approach%20by%20examining%0Aperformance%20metrics%20across%20varying%20model%20complexities%20and%20training%20data%0Avolumes.%20LiteVSR2%20maintains%20the%20efficiency%20of%20its%20predecessor%20while%0Asignificantly%20enhancing%20accuracy%2C%20thereby%20demonstrating%20the%20potential%20for%0Aresource-efficient%20advancements%20in%20VSR%20technology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07210v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520CTC-Based%2520Visual%2520Speech%2520Recognition%26entry.906535625%3DHendrik%2520Laux%2520and%2520Anke%2520Schmeink%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520LiteVSR2%252C%2520an%2520enhanced%2520version%2520of%2520our%2520previously%250Aintroduced%2520efficient%2520approach%2520to%2520Visual%2520Speech%2520Recognition%2520%2528VSR%2529.%2520Building%2520upon%250Aour%2520knowledge%2520distillation%2520framework%2520from%2520a%2520pre-trained%2520Automatic%2520Speech%250ARecognition%2520%2528ASR%2529%2520model%252C%2520we%2520introduce%2520two%2520key%2520improvements%253A%2520a%2520stabilized%2520video%250Apreprocessing%2520technique%2520and%2520feature%2520normalization%2520in%2520the%2520distillation%2520process.%250AThese%2520improvements%2520yield%2520substantial%2520performance%2520gains%2520on%2520the%2520LRS2%2520and%2520LRS3%250Abenchmarks%252C%2520positioning%2520LiteVSR2%2520as%2520the%2520current%2520best%2520CTC-based%2520VSR%2520model%250Awithout%2520increasing%2520the%2520volume%2520of%2520training%2520data%2520or%2520computational%2520resources%250Autilized.%2520Furthermore%252C%2520we%2520explore%2520the%2520scalability%2520of%2520our%2520approach%2520by%2520examining%250Aperformance%2520metrics%2520across%2520varying%2520model%2520complexities%2520and%2520training%2520data%250Avolumes.%2520LiteVSR2%2520maintains%2520the%2520efficiency%2520of%2520its%2520predecessor%2520while%250Asignificantly%2520enhancing%2520accuracy%252C%2520thereby%2520demonstrating%2520the%2520potential%2520for%250Aresource-efficient%2520advancements%2520in%2520VSR%2520technology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07210v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20CTC-Based%20Visual%20Speech%20Recognition&entry.906535625=Hendrik%20Laux%20and%20Anke%20Schmeink&entry.1292438233=%20%20This%20paper%20presents%20LiteVSR2%2C%20an%20enhanced%20version%20of%20our%20previously%0Aintroduced%20efficient%20approach%20to%20Visual%20Speech%20Recognition%20%28VSR%29.%20Building%20upon%0Aour%20knowledge%20distillation%20framework%20from%20a%20pre-trained%20Automatic%20Speech%0ARecognition%20%28ASR%29%20model%2C%20we%20introduce%20two%20key%20improvements%3A%20a%20stabilized%20video%0Apreprocessing%20technique%20and%20feature%20normalization%20in%20the%20distillation%20process.%0AThese%20improvements%20yield%20substantial%20performance%20gains%20on%20the%20LRS2%20and%20LRS3%0Abenchmarks%2C%20positioning%20LiteVSR2%20as%20the%20current%20best%20CTC-based%20VSR%20model%0Awithout%20increasing%20the%20volume%20of%20training%20data%20or%20computational%20resources%0Autilized.%20Furthermore%2C%20we%20explore%20the%20scalability%20of%20our%20approach%20by%20examining%0Aperformance%20metrics%20across%20varying%20model%20complexities%20and%20training%20data%0Avolumes.%20LiteVSR2%20maintains%20the%20efficiency%20of%20its%20predecessor%20while%0Asignificantly%20enhancing%20accuracy%2C%20thereby%20demonstrating%20the%20potential%20for%0Aresource-efficient%20advancements%20in%20VSR%20technology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07210v1&entry.124074799=Read"},
{"title": "TopoMap++: A faster and more space efficient technique to compute\n  projections with topological guarantees", "author": "Vitoria Guardieiro and Felipe Inagaki de Oliveira and Harish Doraiswamy and Luis Gustavo Nonato and Claudio Silva", "abstract": "  High-dimensional data, characterized by many features, can be difficult to\nvisualize effectively. Dimensionality reduction techniques, such as PCA, UMAP,\nand t-SNE, address this challenge by projecting the data into a\nlower-dimensional space while preserving important relationships. TopoMap is\nanother technique that excels at preserving the underlying structure of the\ndata, leading to interpretable visualizations. In particular, TopoMap maps the\nhigh-dimensional data into a visual space, guaranteeing that the 0-dimensional\npersistence diagram of the Rips filtration of the visual space matches the one\nfrom the high-dimensional data. However, the original TopoMap algorithm can be\nslow and its layout can be too sparse for large and complex datasets. In this\npaper, we propose three improvements to TopoMap: 1) a more space-efficient\nlayout, 2) a significantly faster implementation, and 3) a novel TreeMap-based\nrepresentation that makes use of the topological hierarchy to aid the\nexploration of the projections. These advancements make TopoMap, now referred\nto as TopoMap++, a more powerful tool for visualizing high-dimensional data\nwhich we demonstrate through different use case scenarios.\n", "link": "http://arxiv.org/abs/2409.07257v1", "date": "2024-09-11", "relevancy": 2.4898, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5067}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5039}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TopoMap%2B%2B%3A%20A%20faster%20and%20more%20space%20efficient%20technique%20to%20compute%0A%20%20projections%20with%20topological%20guarantees&body=Title%3A%20TopoMap%2B%2B%3A%20A%20faster%20and%20more%20space%20efficient%20technique%20to%20compute%0A%20%20projections%20with%20topological%20guarantees%0AAuthor%3A%20Vitoria%20Guardieiro%20and%20Felipe%20Inagaki%20de%20Oliveira%20and%20Harish%20Doraiswamy%20and%20Luis%20Gustavo%20Nonato%20and%20Claudio%20Silva%0AAbstract%3A%20%20%20High-dimensional%20data%2C%20characterized%20by%20many%20features%2C%20can%20be%20difficult%20to%0Avisualize%20effectively.%20Dimensionality%20reduction%20techniques%2C%20such%20as%20PCA%2C%20UMAP%2C%0Aand%20t-SNE%2C%20address%20this%20challenge%20by%20projecting%20the%20data%20into%20a%0Alower-dimensional%20space%20while%20preserving%20important%20relationships.%20TopoMap%20is%0Aanother%20technique%20that%20excels%20at%20preserving%20the%20underlying%20structure%20of%20the%0Adata%2C%20leading%20to%20interpretable%20visualizations.%20In%20particular%2C%20TopoMap%20maps%20the%0Ahigh-dimensional%20data%20into%20a%20visual%20space%2C%20guaranteeing%20that%20the%200-dimensional%0Apersistence%20diagram%20of%20the%20Rips%20filtration%20of%20the%20visual%20space%20matches%20the%20one%0Afrom%20the%20high-dimensional%20data.%20However%2C%20the%20original%20TopoMap%20algorithm%20can%20be%0Aslow%20and%20its%20layout%20can%20be%20too%20sparse%20for%20large%20and%20complex%20datasets.%20In%20this%0Apaper%2C%20we%20propose%20three%20improvements%20to%20TopoMap%3A%201%29%20a%20more%20space-efficient%0Alayout%2C%202%29%20a%20significantly%20faster%20implementation%2C%20and%203%29%20a%20novel%20TreeMap-based%0Arepresentation%20that%20makes%20use%20of%20the%20topological%20hierarchy%20to%20aid%20the%0Aexploration%20of%20the%20projections.%20These%20advancements%20make%20TopoMap%2C%20now%20referred%0Ato%20as%20TopoMap%2B%2B%2C%20a%20more%20powerful%20tool%20for%20visualizing%20high-dimensional%20data%0Awhich%20we%20demonstrate%20through%20different%20use%20case%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07257v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopoMap%252B%252B%253A%2520A%2520faster%2520and%2520more%2520space%2520efficient%2520technique%2520to%2520compute%250A%2520%2520projections%2520with%2520topological%2520guarantees%26entry.906535625%3DVitoria%2520Guardieiro%2520and%2520Felipe%2520Inagaki%2520de%2520Oliveira%2520and%2520Harish%2520Doraiswamy%2520and%2520Luis%2520Gustavo%2520Nonato%2520and%2520Claudio%2520Silva%26entry.1292438233%3D%2520%2520High-dimensional%2520data%252C%2520characterized%2520by%2520many%2520features%252C%2520can%2520be%2520difficult%2520to%250Avisualize%2520effectively.%2520Dimensionality%2520reduction%2520techniques%252C%2520such%2520as%2520PCA%252C%2520UMAP%252C%250Aand%2520t-SNE%252C%2520address%2520this%2520challenge%2520by%2520projecting%2520the%2520data%2520into%2520a%250Alower-dimensional%2520space%2520while%2520preserving%2520important%2520relationships.%2520TopoMap%2520is%250Aanother%2520technique%2520that%2520excels%2520at%2520preserving%2520the%2520underlying%2520structure%2520of%2520the%250Adata%252C%2520leading%2520to%2520interpretable%2520visualizations.%2520In%2520particular%252C%2520TopoMap%2520maps%2520the%250Ahigh-dimensional%2520data%2520into%2520a%2520visual%2520space%252C%2520guaranteeing%2520that%2520the%25200-dimensional%250Apersistence%2520diagram%2520of%2520the%2520Rips%2520filtration%2520of%2520the%2520visual%2520space%2520matches%2520the%2520one%250Afrom%2520the%2520high-dimensional%2520data.%2520However%252C%2520the%2520original%2520TopoMap%2520algorithm%2520can%2520be%250Aslow%2520and%2520its%2520layout%2520can%2520be%2520too%2520sparse%2520for%2520large%2520and%2520complex%2520datasets.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520three%2520improvements%2520to%2520TopoMap%253A%25201%2529%2520a%2520more%2520space-efficient%250Alayout%252C%25202%2529%2520a%2520significantly%2520faster%2520implementation%252C%2520and%25203%2529%2520a%2520novel%2520TreeMap-based%250Arepresentation%2520that%2520makes%2520use%2520of%2520the%2520topological%2520hierarchy%2520to%2520aid%2520the%250Aexploration%2520of%2520the%2520projections.%2520These%2520advancements%2520make%2520TopoMap%252C%2520now%2520referred%250Ato%2520as%2520TopoMap%252B%252B%252C%2520a%2520more%2520powerful%2520tool%2520for%2520visualizing%2520high-dimensional%2520data%250Awhich%2520we%2520demonstrate%2520through%2520different%2520use%2520case%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07257v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TopoMap%2B%2B%3A%20A%20faster%20and%20more%20space%20efficient%20technique%20to%20compute%0A%20%20projections%20with%20topological%20guarantees&entry.906535625=Vitoria%20Guardieiro%20and%20Felipe%20Inagaki%20de%20Oliveira%20and%20Harish%20Doraiswamy%20and%20Luis%20Gustavo%20Nonato%20and%20Claudio%20Silva&entry.1292438233=%20%20High-dimensional%20data%2C%20characterized%20by%20many%20features%2C%20can%20be%20difficult%20to%0Avisualize%20effectively.%20Dimensionality%20reduction%20techniques%2C%20such%20as%20PCA%2C%20UMAP%2C%0Aand%20t-SNE%2C%20address%20this%20challenge%20by%20projecting%20the%20data%20into%20a%0Alower-dimensional%20space%20while%20preserving%20important%20relationships.%20TopoMap%20is%0Aanother%20technique%20that%20excels%20at%20preserving%20the%20underlying%20structure%20of%20the%0Adata%2C%20leading%20to%20interpretable%20visualizations.%20In%20particular%2C%20TopoMap%20maps%20the%0Ahigh-dimensional%20data%20into%20a%20visual%20space%2C%20guaranteeing%20that%20the%200-dimensional%0Apersistence%20diagram%20of%20the%20Rips%20filtration%20of%20the%20visual%20space%20matches%20the%20one%0Afrom%20the%20high-dimensional%20data.%20However%2C%20the%20original%20TopoMap%20algorithm%20can%20be%0Aslow%20and%20its%20layout%20can%20be%20too%20sparse%20for%20large%20and%20complex%20datasets.%20In%20this%0Apaper%2C%20we%20propose%20three%20improvements%20to%20TopoMap%3A%201%29%20a%20more%20space-efficient%0Alayout%2C%202%29%20a%20significantly%20faster%20implementation%2C%20and%203%29%20a%20novel%20TreeMap-based%0Arepresentation%20that%20makes%20use%20of%20the%20topological%20hierarchy%20to%20aid%20the%0Aexploration%20of%20the%20projections.%20These%20advancements%20make%20TopoMap%2C%20now%20referred%0Ato%20as%20TopoMap%2B%2B%2C%20a%20more%20powerful%20tool%20for%20visualizing%20high-dimensional%20data%0Awhich%20we%20demonstrate%20through%20different%20use%20case%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07257v1&entry.124074799=Read"},
{"title": "BLS-GAN: A Deep Layer Separation Framework for Eliminating Bone Overlap\n  in Conventional Radiographs", "author": "Haolin Wang and Yafei Ou and Prasoon Ambalathankandy and Gen Ota and Pengyu Dai and Masayuki Ikebe and Kenji Suzuki and Tamotsu Kamishima", "abstract": "  Conventional radiography is the widely used imaging technology in diagnosing,\nmonitoring, and prognosticating musculoskeletal (MSK) diseases because of its\neasy availability, versatility, and cost-effectiveness. In conventional\nradiographs, bone overlaps are prevalent, and can impede the accurate\nassessment of bone characteristics by radiologists or algorithms, posing\nsignificant challenges to conventional and computer-aided diagnoses. This work\ninitiated the study of a challenging scenario - bone layer separation in\nconventional radiographs, in which separate overlapped bone regions enable the\nindependent assessment of the bone characteristics of each bone layer and lay\nthe groundwork for MSK disease diagnosis and its automation. This work proposed\na Bone Layer Separation GAN (BLS-GAN) framework that can produce high-quality\nbone layer images with reasonable bone characteristics and texture. This\nframework introduced a reconstructor based on conventional radiography imaging\nprinciples, which achieved efficient reconstruction and mitigates the recurrent\ncalculations and training instability issues caused by soft tissue in the\noverlapped regions. Additionally, pre-training with synthetic images was\nimplemented to enhance the stability of both the training process and the\nresults. The generated images passed the visual Turing test, and improved\nperformance in downstream tasks. This work affirms the feasibility of\nextracting bone layer images from conventional radiographs, which holds promise\nfor leveraging bone layer separation technology to facilitate more\ncomprehensive analytical research in MSK diagnosis, monitoring, and prognosis.\nCode and dataset will be made available.\n", "link": "http://arxiv.org/abs/2409.07304v1", "date": "2024-09-11", "relevancy": 2.4819, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5088}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4934}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BLS-GAN%3A%20A%20Deep%20Layer%20Separation%20Framework%20for%20Eliminating%20Bone%20Overlap%0A%20%20in%20Conventional%20Radiographs&body=Title%3A%20BLS-GAN%3A%20A%20Deep%20Layer%20Separation%20Framework%20for%20Eliminating%20Bone%20Overlap%0A%20%20in%20Conventional%20Radiographs%0AAuthor%3A%20Haolin%20Wang%20and%20Yafei%20Ou%20and%20Prasoon%20Ambalathankandy%20and%20Gen%20Ota%20and%20Pengyu%20Dai%20and%20Masayuki%20Ikebe%20and%20Kenji%20Suzuki%20and%20Tamotsu%20Kamishima%0AAbstract%3A%20%20%20Conventional%20radiography%20is%20the%20widely%20used%20imaging%20technology%20in%20diagnosing%2C%0Amonitoring%2C%20and%20prognosticating%20musculoskeletal%20%28MSK%29%20diseases%20because%20of%20its%0Aeasy%20availability%2C%20versatility%2C%20and%20cost-effectiveness.%20In%20conventional%0Aradiographs%2C%20bone%20overlaps%20are%20prevalent%2C%20and%20can%20impede%20the%20accurate%0Aassessment%20of%20bone%20characteristics%20by%20radiologists%20or%20algorithms%2C%20posing%0Asignificant%20challenges%20to%20conventional%20and%20computer-aided%20diagnoses.%20This%20work%0Ainitiated%20the%20study%20of%20a%20challenging%20scenario%20-%20bone%20layer%20separation%20in%0Aconventional%20radiographs%2C%20in%20which%20separate%20overlapped%20bone%20regions%20enable%20the%0Aindependent%20assessment%20of%20the%20bone%20characteristics%20of%20each%20bone%20layer%20and%20lay%0Athe%20groundwork%20for%20MSK%20disease%20diagnosis%20and%20its%20automation.%20This%20work%20proposed%0Aa%20Bone%20Layer%20Separation%20GAN%20%28BLS-GAN%29%20framework%20that%20can%20produce%20high-quality%0Abone%20layer%20images%20with%20reasonable%20bone%20characteristics%20and%20texture.%20This%0Aframework%20introduced%20a%20reconstructor%20based%20on%20conventional%20radiography%20imaging%0Aprinciples%2C%20which%20achieved%20efficient%20reconstruction%20and%20mitigates%20the%20recurrent%0Acalculations%20and%20training%20instability%20issues%20caused%20by%20soft%20tissue%20in%20the%0Aoverlapped%20regions.%20Additionally%2C%20pre-training%20with%20synthetic%20images%20was%0Aimplemented%20to%20enhance%20the%20stability%20of%20both%20the%20training%20process%20and%20the%0Aresults.%20The%20generated%20images%20passed%20the%20visual%20Turing%20test%2C%20and%20improved%0Aperformance%20in%20downstream%20tasks.%20This%20work%20affirms%20the%20feasibility%20of%0Aextracting%20bone%20layer%20images%20from%20conventional%20radiographs%2C%20which%20holds%20promise%0Afor%20leveraging%20bone%20layer%20separation%20technology%20to%20facilitate%20more%0Acomprehensive%20analytical%20research%20in%20MSK%20diagnosis%2C%20monitoring%2C%20and%20prognosis.%0ACode%20and%20dataset%20will%20be%20made%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07304v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBLS-GAN%253A%2520A%2520Deep%2520Layer%2520Separation%2520Framework%2520for%2520Eliminating%2520Bone%2520Overlap%250A%2520%2520in%2520Conventional%2520Radiographs%26entry.906535625%3DHaolin%2520Wang%2520and%2520Yafei%2520Ou%2520and%2520Prasoon%2520Ambalathankandy%2520and%2520Gen%2520Ota%2520and%2520Pengyu%2520Dai%2520and%2520Masayuki%2520Ikebe%2520and%2520Kenji%2520Suzuki%2520and%2520Tamotsu%2520Kamishima%26entry.1292438233%3D%2520%2520Conventional%2520radiography%2520is%2520the%2520widely%2520used%2520imaging%2520technology%2520in%2520diagnosing%252C%250Amonitoring%252C%2520and%2520prognosticating%2520musculoskeletal%2520%2528MSK%2529%2520diseases%2520because%2520of%2520its%250Aeasy%2520availability%252C%2520versatility%252C%2520and%2520cost-effectiveness.%2520In%2520conventional%250Aradiographs%252C%2520bone%2520overlaps%2520are%2520prevalent%252C%2520and%2520can%2520impede%2520the%2520accurate%250Aassessment%2520of%2520bone%2520characteristics%2520by%2520radiologists%2520or%2520algorithms%252C%2520posing%250Asignificant%2520challenges%2520to%2520conventional%2520and%2520computer-aided%2520diagnoses.%2520This%2520work%250Ainitiated%2520the%2520study%2520of%2520a%2520challenging%2520scenario%2520-%2520bone%2520layer%2520separation%2520in%250Aconventional%2520radiographs%252C%2520in%2520which%2520separate%2520overlapped%2520bone%2520regions%2520enable%2520the%250Aindependent%2520assessment%2520of%2520the%2520bone%2520characteristics%2520of%2520each%2520bone%2520layer%2520and%2520lay%250Athe%2520groundwork%2520for%2520MSK%2520disease%2520diagnosis%2520and%2520its%2520automation.%2520This%2520work%2520proposed%250Aa%2520Bone%2520Layer%2520Separation%2520GAN%2520%2528BLS-GAN%2529%2520framework%2520that%2520can%2520produce%2520high-quality%250Abone%2520layer%2520images%2520with%2520reasonable%2520bone%2520characteristics%2520and%2520texture.%2520This%250Aframework%2520introduced%2520a%2520reconstructor%2520based%2520on%2520conventional%2520radiography%2520imaging%250Aprinciples%252C%2520which%2520achieved%2520efficient%2520reconstruction%2520and%2520mitigates%2520the%2520recurrent%250Acalculations%2520and%2520training%2520instability%2520issues%2520caused%2520by%2520soft%2520tissue%2520in%2520the%250Aoverlapped%2520regions.%2520Additionally%252C%2520pre-training%2520with%2520synthetic%2520images%2520was%250Aimplemented%2520to%2520enhance%2520the%2520stability%2520of%2520both%2520the%2520training%2520process%2520and%2520the%250Aresults.%2520The%2520generated%2520images%2520passed%2520the%2520visual%2520Turing%2520test%252C%2520and%2520improved%250Aperformance%2520in%2520downstream%2520tasks.%2520This%2520work%2520affirms%2520the%2520feasibility%2520of%250Aextracting%2520bone%2520layer%2520images%2520from%2520conventional%2520radiographs%252C%2520which%2520holds%2520promise%250Afor%2520leveraging%2520bone%2520layer%2520separation%2520technology%2520to%2520facilitate%2520more%250Acomprehensive%2520analytical%2520research%2520in%2520MSK%2520diagnosis%252C%2520monitoring%252C%2520and%2520prognosis.%250ACode%2520and%2520dataset%2520will%2520be%2520made%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07304v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BLS-GAN%3A%20A%20Deep%20Layer%20Separation%20Framework%20for%20Eliminating%20Bone%20Overlap%0A%20%20in%20Conventional%20Radiographs&entry.906535625=Haolin%20Wang%20and%20Yafei%20Ou%20and%20Prasoon%20Ambalathankandy%20and%20Gen%20Ota%20and%20Pengyu%20Dai%20and%20Masayuki%20Ikebe%20and%20Kenji%20Suzuki%20and%20Tamotsu%20Kamishima&entry.1292438233=%20%20Conventional%20radiography%20is%20the%20widely%20used%20imaging%20technology%20in%20diagnosing%2C%0Amonitoring%2C%20and%20prognosticating%20musculoskeletal%20%28MSK%29%20diseases%20because%20of%20its%0Aeasy%20availability%2C%20versatility%2C%20and%20cost-effectiveness.%20In%20conventional%0Aradiographs%2C%20bone%20overlaps%20are%20prevalent%2C%20and%20can%20impede%20the%20accurate%0Aassessment%20of%20bone%20characteristics%20by%20radiologists%20or%20algorithms%2C%20posing%0Asignificant%20challenges%20to%20conventional%20and%20computer-aided%20diagnoses.%20This%20work%0Ainitiated%20the%20study%20of%20a%20challenging%20scenario%20-%20bone%20layer%20separation%20in%0Aconventional%20radiographs%2C%20in%20which%20separate%20overlapped%20bone%20regions%20enable%20the%0Aindependent%20assessment%20of%20the%20bone%20characteristics%20of%20each%20bone%20layer%20and%20lay%0Athe%20groundwork%20for%20MSK%20disease%20diagnosis%20and%20its%20automation.%20This%20work%20proposed%0Aa%20Bone%20Layer%20Separation%20GAN%20%28BLS-GAN%29%20framework%20that%20can%20produce%20high-quality%0Abone%20layer%20images%20with%20reasonable%20bone%20characteristics%20and%20texture.%20This%0Aframework%20introduced%20a%20reconstructor%20based%20on%20conventional%20radiography%20imaging%0Aprinciples%2C%20which%20achieved%20efficient%20reconstruction%20and%20mitigates%20the%20recurrent%0Acalculations%20and%20training%20instability%20issues%20caused%20by%20soft%20tissue%20in%20the%0Aoverlapped%20regions.%20Additionally%2C%20pre-training%20with%20synthetic%20images%20was%0Aimplemented%20to%20enhance%20the%20stability%20of%20both%20the%20training%20process%20and%20the%0Aresults.%20The%20generated%20images%20passed%20the%20visual%20Turing%20test%2C%20and%20improved%0Aperformance%20in%20downstream%20tasks.%20This%20work%20affirms%20the%20feasibility%20of%0Aextracting%20bone%20layer%20images%20from%20conventional%20radiographs%2C%20which%20holds%20promise%0Afor%20leveraging%20bone%20layer%20separation%20technology%20to%20facilitate%20more%0Acomprehensive%20analytical%20research%20in%20MSK%20diagnosis%2C%20monitoring%2C%20and%20prognosis.%0ACode%20and%20dataset%20will%20be%20made%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07304v1&entry.124074799=Read"},
{"title": "DreamMapping: High-Fidelity Text-to-3D Generation via Variational\n  Distribution Mapping", "author": "Zeyu Cai and Duotun Wang and Yixun Liang and Zhijing Shao and Ying-Cong Chen and Xiaohang Zhan and Zeyu Wang", "abstract": "  Score Distillation Sampling (SDS) has emerged as a prevalent technique for\ntext-to-3D generation, enabling 3D content creation by distilling\nview-dependent information from text-to-2D guidance. However, they frequently\nexhibit shortcomings such as over-saturated color and excess smoothness. In\nthis paper, we conduct a thorough analysis of SDS and refine its formulation,\nfinding that the core design is to model the distribution of rendered images.\nFollowing this insight, we introduce a novel strategy called Variational\nDistribution Mapping (VDM), which expedites the distribution modeling process\nby regarding the rendered images as instances of degradation from\ndiffusion-based generation. This special design enables the efficient training\nof variational distribution by skipping the calculations of the Jacobians in\nthe diffusion U-Net. We also introduce timestep-dependent Distribution\nCoefficient Annealing (DCA) to further improve distilling precision. Leveraging\nVDM and DCA, we use Gaussian Splatting as the 3D representation and build a\ntext-to-3D generation framework. Extensive experiments and evaluations\ndemonstrate the capability of VDM and DCA to generate high-fidelity and\nrealistic assets with optimization efficiency.\n", "link": "http://arxiv.org/abs/2409.05099v2", "date": "2024-09-11", "relevancy": 2.4386, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6122}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6122}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamMapping%3A%20High-Fidelity%20Text-to-3D%20Generation%20via%20Variational%0A%20%20Distribution%20Mapping&body=Title%3A%20DreamMapping%3A%20High-Fidelity%20Text-to-3D%20Generation%20via%20Variational%0A%20%20Distribution%20Mapping%0AAuthor%3A%20Zeyu%20Cai%20and%20Duotun%20Wang%20and%20Yixun%20Liang%20and%20Zhijing%20Shao%20and%20Ying-Cong%20Chen%20and%20Xiaohang%20Zhan%20and%20Zeyu%20Wang%0AAbstract%3A%20%20%20Score%20Distillation%20Sampling%20%28SDS%29%20has%20emerged%20as%20a%20prevalent%20technique%20for%0Atext-to-3D%20generation%2C%20enabling%203D%20content%20creation%20by%20distilling%0Aview-dependent%20information%20from%20text-to-2D%20guidance.%20However%2C%20they%20frequently%0Aexhibit%20shortcomings%20such%20as%20over-saturated%20color%20and%20excess%20smoothness.%20In%0Athis%20paper%2C%20we%20conduct%20a%20thorough%20analysis%20of%20SDS%20and%20refine%20its%20formulation%2C%0Afinding%20that%20the%20core%20design%20is%20to%20model%20the%20distribution%20of%20rendered%20images.%0AFollowing%20this%20insight%2C%20we%20introduce%20a%20novel%20strategy%20called%20Variational%0ADistribution%20Mapping%20%28VDM%29%2C%20which%20expedites%20the%20distribution%20modeling%20process%0Aby%20regarding%20the%20rendered%20images%20as%20instances%20of%20degradation%20from%0Adiffusion-based%20generation.%20This%20special%20design%20enables%20the%20efficient%20training%0Aof%20variational%20distribution%20by%20skipping%20the%20calculations%20of%20the%20Jacobians%20in%0Athe%20diffusion%20U-Net.%20We%20also%20introduce%20timestep-dependent%20Distribution%0ACoefficient%20Annealing%20%28DCA%29%20to%20further%20improve%20distilling%20precision.%20Leveraging%0AVDM%20and%20DCA%2C%20we%20use%20Gaussian%20Splatting%20as%20the%203D%20representation%20and%20build%20a%0Atext-to-3D%20generation%20framework.%20Extensive%20experiments%20and%20evaluations%0Ademonstrate%20the%20capability%20of%20VDM%20and%20DCA%20to%20generate%20high-fidelity%20and%0Arealistic%20assets%20with%20optimization%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05099v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamMapping%253A%2520High-Fidelity%2520Text-to-3D%2520Generation%2520via%2520Variational%250A%2520%2520Distribution%2520Mapping%26entry.906535625%3DZeyu%2520Cai%2520and%2520Duotun%2520Wang%2520and%2520Yixun%2520Liang%2520and%2520Zhijing%2520Shao%2520and%2520Ying-Cong%2520Chen%2520and%2520Xiaohang%2520Zhan%2520and%2520Zeyu%2520Wang%26entry.1292438233%3D%2520%2520Score%2520Distillation%2520Sampling%2520%2528SDS%2529%2520has%2520emerged%2520as%2520a%2520prevalent%2520technique%2520for%250Atext-to-3D%2520generation%252C%2520enabling%25203D%2520content%2520creation%2520by%2520distilling%250Aview-dependent%2520information%2520from%2520text-to-2D%2520guidance.%2520However%252C%2520they%2520frequently%250Aexhibit%2520shortcomings%2520such%2520as%2520over-saturated%2520color%2520and%2520excess%2520smoothness.%2520In%250Athis%2520paper%252C%2520we%2520conduct%2520a%2520thorough%2520analysis%2520of%2520SDS%2520and%2520refine%2520its%2520formulation%252C%250Afinding%2520that%2520the%2520core%2520design%2520is%2520to%2520model%2520the%2520distribution%2520of%2520rendered%2520images.%250AFollowing%2520this%2520insight%252C%2520we%2520introduce%2520a%2520novel%2520strategy%2520called%2520Variational%250ADistribution%2520Mapping%2520%2528VDM%2529%252C%2520which%2520expedites%2520the%2520distribution%2520modeling%2520process%250Aby%2520regarding%2520the%2520rendered%2520images%2520as%2520instances%2520of%2520degradation%2520from%250Adiffusion-based%2520generation.%2520This%2520special%2520design%2520enables%2520the%2520efficient%2520training%250Aof%2520variational%2520distribution%2520by%2520skipping%2520the%2520calculations%2520of%2520the%2520Jacobians%2520in%250Athe%2520diffusion%2520U-Net.%2520We%2520also%2520introduce%2520timestep-dependent%2520Distribution%250ACoefficient%2520Annealing%2520%2528DCA%2529%2520to%2520further%2520improve%2520distilling%2520precision.%2520Leveraging%250AVDM%2520and%2520DCA%252C%2520we%2520use%2520Gaussian%2520Splatting%2520as%2520the%25203D%2520representation%2520and%2520build%2520a%250Atext-to-3D%2520generation%2520framework.%2520Extensive%2520experiments%2520and%2520evaluations%250Ademonstrate%2520the%2520capability%2520of%2520VDM%2520and%2520DCA%2520to%2520generate%2520high-fidelity%2520and%250Arealistic%2520assets%2520with%2520optimization%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05099v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamMapping%3A%20High-Fidelity%20Text-to-3D%20Generation%20via%20Variational%0A%20%20Distribution%20Mapping&entry.906535625=Zeyu%20Cai%20and%20Duotun%20Wang%20and%20Yixun%20Liang%20and%20Zhijing%20Shao%20and%20Ying-Cong%20Chen%20and%20Xiaohang%20Zhan%20and%20Zeyu%20Wang&entry.1292438233=%20%20Score%20Distillation%20Sampling%20%28SDS%29%20has%20emerged%20as%20a%20prevalent%20technique%20for%0Atext-to-3D%20generation%2C%20enabling%203D%20content%20creation%20by%20distilling%0Aview-dependent%20information%20from%20text-to-2D%20guidance.%20However%2C%20they%20frequently%0Aexhibit%20shortcomings%20such%20as%20over-saturated%20color%20and%20excess%20smoothness.%20In%0Athis%20paper%2C%20we%20conduct%20a%20thorough%20analysis%20of%20SDS%20and%20refine%20its%20formulation%2C%0Afinding%20that%20the%20core%20design%20is%20to%20model%20the%20distribution%20of%20rendered%20images.%0AFollowing%20this%20insight%2C%20we%20introduce%20a%20novel%20strategy%20called%20Variational%0ADistribution%20Mapping%20%28VDM%29%2C%20which%20expedites%20the%20distribution%20modeling%20process%0Aby%20regarding%20the%20rendered%20images%20as%20instances%20of%20degradation%20from%0Adiffusion-based%20generation.%20This%20special%20design%20enables%20the%20efficient%20training%0Aof%20variational%20distribution%20by%20skipping%20the%20calculations%20of%20the%20Jacobians%20in%0Athe%20diffusion%20U-Net.%20We%20also%20introduce%20timestep-dependent%20Distribution%0ACoefficient%20Annealing%20%28DCA%29%20to%20further%20improve%20distilling%20precision.%20Leveraging%0AVDM%20and%20DCA%2C%20we%20use%20Gaussian%20Splatting%20as%20the%203D%20representation%20and%20build%20a%0Atext-to-3D%20generation%20framework.%20Extensive%20experiments%20and%20evaluations%0Ademonstrate%20the%20capability%20of%20VDM%20and%20DCA%20to%20generate%20high-fidelity%20and%0Arealistic%20assets%20with%20optimization%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05099v2&entry.124074799=Read"},
{"title": "Adversarial Doodles: Interpretable and Human-drawable Attacks Provide\n  Describable Insights", "author": "Ryoya Nara and Yusuke Matsui", "abstract": "  DNN-based image classifiers are susceptible to adversarial attacks. Most\nprevious adversarial attacks do not have clear patterns, making it difficult to\ninterpret attacks' results and gain insights into classifiers' mechanisms.\nTherefore, we propose Adversarial Doodles, which have interpretable shapes. We\noptimize black bezier curves to fool the classifier by overlaying them onto the\ninput image. By introducing random affine transformation and regularizing the\ndoodled area, we obtain small-sized attacks that cause misclassification even\nwhen humans replicate them by hand. Adversarial doodles provide describable\ninsights into the relationship between the human-drawn doodle's shape and the\nclassifier's output, such as \"When we add three small circles on a helicopter\nimage, the ResNet-50 classifier mistakenly classifies it as an airplane.\"\n", "link": "http://arxiv.org/abs/2311.15994v3", "date": "2024-09-11", "relevancy": 2.426, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4901}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4854}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Doodles%3A%20Interpretable%20and%20Human-drawable%20Attacks%20Provide%0A%20%20Describable%20Insights&body=Title%3A%20Adversarial%20Doodles%3A%20Interpretable%20and%20Human-drawable%20Attacks%20Provide%0A%20%20Describable%20Insights%0AAuthor%3A%20Ryoya%20Nara%20and%20Yusuke%20Matsui%0AAbstract%3A%20%20%20DNN-based%20image%20classifiers%20are%20susceptible%20to%20adversarial%20attacks.%20Most%0Aprevious%20adversarial%20attacks%20do%20not%20have%20clear%20patterns%2C%20making%20it%20difficult%20to%0Ainterpret%20attacks%27%20results%20and%20gain%20insights%20into%20classifiers%27%20mechanisms.%0ATherefore%2C%20we%20propose%20Adversarial%20Doodles%2C%20which%20have%20interpretable%20shapes.%20We%0Aoptimize%20black%20bezier%20curves%20to%20fool%20the%20classifier%20by%20overlaying%20them%20onto%20the%0Ainput%20image.%20By%20introducing%20random%20affine%20transformation%20and%20regularizing%20the%0Adoodled%20area%2C%20we%20obtain%20small-sized%20attacks%20that%20cause%20misclassification%20even%0Awhen%20humans%20replicate%20them%20by%20hand.%20Adversarial%20doodles%20provide%20describable%0Ainsights%20into%20the%20relationship%20between%20the%20human-drawn%20doodle%27s%20shape%20and%20the%0Aclassifier%27s%20output%2C%20such%20as%20%22When%20we%20add%20three%20small%20circles%20on%20a%20helicopter%0Aimage%2C%20the%20ResNet-50%20classifier%20mistakenly%20classifies%20it%20as%20an%20airplane.%22%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.15994v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Doodles%253A%2520Interpretable%2520and%2520Human-drawable%2520Attacks%2520Provide%250A%2520%2520Describable%2520Insights%26entry.906535625%3DRyoya%2520Nara%2520and%2520Yusuke%2520Matsui%26entry.1292438233%3D%2520%2520DNN-based%2520image%2520classifiers%2520are%2520susceptible%2520to%2520adversarial%2520attacks.%2520Most%250Aprevious%2520adversarial%2520attacks%2520do%2520not%2520have%2520clear%2520patterns%252C%2520making%2520it%2520difficult%2520to%250Ainterpret%2520attacks%2527%2520results%2520and%2520gain%2520insights%2520into%2520classifiers%2527%2520mechanisms.%250ATherefore%252C%2520we%2520propose%2520Adversarial%2520Doodles%252C%2520which%2520have%2520interpretable%2520shapes.%2520We%250Aoptimize%2520black%2520bezier%2520curves%2520to%2520fool%2520the%2520classifier%2520by%2520overlaying%2520them%2520onto%2520the%250Ainput%2520image.%2520By%2520introducing%2520random%2520affine%2520transformation%2520and%2520regularizing%2520the%250Adoodled%2520area%252C%2520we%2520obtain%2520small-sized%2520attacks%2520that%2520cause%2520misclassification%2520even%250Awhen%2520humans%2520replicate%2520them%2520by%2520hand.%2520Adversarial%2520doodles%2520provide%2520describable%250Ainsights%2520into%2520the%2520relationship%2520between%2520the%2520human-drawn%2520doodle%2527s%2520shape%2520and%2520the%250Aclassifier%2527s%2520output%252C%2520such%2520as%2520%2522When%2520we%2520add%2520three%2520small%2520circles%2520on%2520a%2520helicopter%250Aimage%252C%2520the%2520ResNet-50%2520classifier%2520mistakenly%2520classifies%2520it%2520as%2520an%2520airplane.%2522%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.15994v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Doodles%3A%20Interpretable%20and%20Human-drawable%20Attacks%20Provide%0A%20%20Describable%20Insights&entry.906535625=Ryoya%20Nara%20and%20Yusuke%20Matsui&entry.1292438233=%20%20DNN-based%20image%20classifiers%20are%20susceptible%20to%20adversarial%20attacks.%20Most%0Aprevious%20adversarial%20attacks%20do%20not%20have%20clear%20patterns%2C%20making%20it%20difficult%20to%0Ainterpret%20attacks%27%20results%20and%20gain%20insights%20into%20classifiers%27%20mechanisms.%0ATherefore%2C%20we%20propose%20Adversarial%20Doodles%2C%20which%20have%20interpretable%20shapes.%20We%0Aoptimize%20black%20bezier%20curves%20to%20fool%20the%20classifier%20by%20overlaying%20them%20onto%20the%0Ainput%20image.%20By%20introducing%20random%20affine%20transformation%20and%20regularizing%20the%0Adoodled%20area%2C%20we%20obtain%20small-sized%20attacks%20that%20cause%20misclassification%20even%0Awhen%20humans%20replicate%20them%20by%20hand.%20Adversarial%20doodles%20provide%20describable%0Ainsights%20into%20the%20relationship%20between%20the%20human-drawn%20doodle%27s%20shape%20and%20the%0Aclassifier%27s%20output%2C%20such%20as%20%22When%20we%20add%20three%20small%20circles%20on%20a%20helicopter%0Aimage%2C%20the%20ResNet-50%20classifier%20mistakenly%20classifies%20it%20as%20an%20airplane.%22%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.15994v3&entry.124074799=Read"},
{"title": "Manifold Learning via Foliations and Knowledge Transfer", "author": "E. Tron and E. Fioresi", "abstract": "  Understanding how real data is distributed in high dimensional spaces is the\nkey to many tasks in machine learning. We want to provide a natural geometric\nstructure on the space of data employing a deep ReLU neural network trained as\na classifier. Through the data information matrix (DIM), a variation of the\nFisher information matrix, the model will discern a singular foliation\nstructure on the space of data. We show that the singular points of such\nfoliation are contained in a measure zero set, and that a local regular\nfoliation exists almost everywhere. Experiments show that the data is\ncorrelated with leaves of such foliation. Moreover we show the potential of our\napproach for knowledge transfer by analyzing the spectrum of the DIM to measure\ndistances between datasets.\n", "link": "http://arxiv.org/abs/2409.07412v1", "date": "2024-09-11", "relevancy": 2.4142, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4857}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4814}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4814}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Manifold%20Learning%20via%20Foliations%20and%20Knowledge%20Transfer&body=Title%3A%20Manifold%20Learning%20via%20Foliations%20and%20Knowledge%20Transfer%0AAuthor%3A%20E.%20Tron%20and%20E.%20Fioresi%0AAbstract%3A%20%20%20Understanding%20how%20real%20data%20is%20distributed%20in%20high%20dimensional%20spaces%20is%20the%0Akey%20to%20many%20tasks%20in%20machine%20learning.%20We%20want%20to%20provide%20a%20natural%20geometric%0Astructure%20on%20the%20space%20of%20data%20employing%20a%20deep%20ReLU%20neural%20network%20trained%20as%0Aa%20classifier.%20Through%20the%20data%20information%20matrix%20%28DIM%29%2C%20a%20variation%20of%20the%0AFisher%20information%20matrix%2C%20the%20model%20will%20discern%20a%20singular%20foliation%0Astructure%20on%20the%20space%20of%20data.%20We%20show%20that%20the%20singular%20points%20of%20such%0Afoliation%20are%20contained%20in%20a%20measure%20zero%20set%2C%20and%20that%20a%20local%20regular%0Afoliation%20exists%20almost%20everywhere.%20Experiments%20show%20that%20the%20data%20is%0Acorrelated%20with%20leaves%20of%20such%20foliation.%20Moreover%20we%20show%20the%20potential%20of%20our%0Aapproach%20for%20knowledge%20transfer%20by%20analyzing%20the%20spectrum%20of%20the%20DIM%20to%20measure%0Adistances%20between%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07412v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DManifold%2520Learning%2520via%2520Foliations%2520and%2520Knowledge%2520Transfer%26entry.906535625%3DE.%2520Tron%2520and%2520E.%2520Fioresi%26entry.1292438233%3D%2520%2520Understanding%2520how%2520real%2520data%2520is%2520distributed%2520in%2520high%2520dimensional%2520spaces%2520is%2520the%250Akey%2520to%2520many%2520tasks%2520in%2520machine%2520learning.%2520We%2520want%2520to%2520provide%2520a%2520natural%2520geometric%250Astructure%2520on%2520the%2520space%2520of%2520data%2520employing%2520a%2520deep%2520ReLU%2520neural%2520network%2520trained%2520as%250Aa%2520classifier.%2520Through%2520the%2520data%2520information%2520matrix%2520%2528DIM%2529%252C%2520a%2520variation%2520of%2520the%250AFisher%2520information%2520matrix%252C%2520the%2520model%2520will%2520discern%2520a%2520singular%2520foliation%250Astructure%2520on%2520the%2520space%2520of%2520data.%2520We%2520show%2520that%2520the%2520singular%2520points%2520of%2520such%250Afoliation%2520are%2520contained%2520in%2520a%2520measure%2520zero%2520set%252C%2520and%2520that%2520a%2520local%2520regular%250Afoliation%2520exists%2520almost%2520everywhere.%2520Experiments%2520show%2520that%2520the%2520data%2520is%250Acorrelated%2520with%2520leaves%2520of%2520such%2520foliation.%2520Moreover%2520we%2520show%2520the%2520potential%2520of%2520our%250Aapproach%2520for%2520knowledge%2520transfer%2520by%2520analyzing%2520the%2520spectrum%2520of%2520the%2520DIM%2520to%2520measure%250Adistances%2520between%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07412v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Manifold%20Learning%20via%20Foliations%20and%20Knowledge%20Transfer&entry.906535625=E.%20Tron%20and%20E.%20Fioresi&entry.1292438233=%20%20Understanding%20how%20real%20data%20is%20distributed%20in%20high%20dimensional%20spaces%20is%20the%0Akey%20to%20many%20tasks%20in%20machine%20learning.%20We%20want%20to%20provide%20a%20natural%20geometric%0Astructure%20on%20the%20space%20of%20data%20employing%20a%20deep%20ReLU%20neural%20network%20trained%20as%0Aa%20classifier.%20Through%20the%20data%20information%20matrix%20%28DIM%29%2C%20a%20variation%20of%20the%0AFisher%20information%20matrix%2C%20the%20model%20will%20discern%20a%20singular%20foliation%0Astructure%20on%20the%20space%20of%20data.%20We%20show%20that%20the%20singular%20points%20of%20such%0Afoliation%20are%20contained%20in%20a%20measure%20zero%20set%2C%20and%20that%20a%20local%20regular%0Afoliation%20exists%20almost%20everywhere.%20Experiments%20show%20that%20the%20data%20is%0Acorrelated%20with%20leaves%20of%20such%20foliation.%20Moreover%20we%20show%20the%20potential%20of%20our%0Aapproach%20for%20knowledge%20transfer%20by%20analyzing%20the%20spectrum%20of%20the%20DIM%20to%20measure%0Adistances%20between%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07412v1&entry.124074799=Read"},
{"title": "A Scalable Algorithm for Active Learning", "author": "Youguang Chen and Zheyu Wen and George Biros", "abstract": "  FIRAL is a recently proposed deterministic active learning algorithm for\nmulticlass classification using logistic regression. It was shown to outperform\nthe state-of-the-art in terms of accuracy and robustness and comes with\ntheoretical performance guarantees. However, its scalability suffers when\ndealing with datasets featuring a large number of points $n$, dimensions $d$,\nand classes $c$, due to its $\\mathcal{O}(c^2d^2+nc^2d)$ storage and\n$\\mathcal{O}(c^3(nd^2 + bd^3 + bn))$ computational complexity where $b$ is the\nnumber of points to select in active learning. To address these challenges, we\npropose an approximate algorithm with storage requirements reduced to\n$\\mathcal{O}(n(d+c) + cd^2)$ and a computational complexity of\n$\\mathcal{O}(bncd^2)$. Additionally, we present a parallel implementation on\nGPUs. We demonstrate the accuracy and scalability of our approach using MNIST,\nCIFAR-10, Caltech101, and ImageNet. The accuracy tests reveal no deterioration\nin accuracy compared to FIRAL. We report strong and weak scaling tests on up to\n12 GPUs, for three million point synthetic dataset.\n", "link": "http://arxiv.org/abs/2409.07392v1", "date": "2024-09-11", "relevancy": 2.4135, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5034}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4805}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Scalable%20Algorithm%20for%20Active%20Learning&body=Title%3A%20A%20Scalable%20Algorithm%20for%20Active%20Learning%0AAuthor%3A%20Youguang%20Chen%20and%20Zheyu%20Wen%20and%20George%20Biros%0AAbstract%3A%20%20%20FIRAL%20is%20a%20recently%20proposed%20deterministic%20active%20learning%20algorithm%20for%0Amulticlass%20classification%20using%20logistic%20regression.%20It%20was%20shown%20to%20outperform%0Athe%20state-of-the-art%20in%20terms%20of%20accuracy%20and%20robustness%20and%20comes%20with%0Atheoretical%20performance%20guarantees.%20However%2C%20its%20scalability%20suffers%20when%0Adealing%20with%20datasets%20featuring%20a%20large%20number%20of%20points%20%24n%24%2C%20dimensions%20%24d%24%2C%0Aand%20classes%20%24c%24%2C%20due%20to%20its%20%24%5Cmathcal%7BO%7D%28c%5E2d%5E2%2Bnc%5E2d%29%24%20storage%20and%0A%24%5Cmathcal%7BO%7D%28c%5E3%28nd%5E2%20%2B%20bd%5E3%20%2B%20bn%29%29%24%20computational%20complexity%20where%20%24b%24%20is%20the%0Anumber%20of%20points%20to%20select%20in%20active%20learning.%20To%20address%20these%20challenges%2C%20we%0Apropose%20an%20approximate%20algorithm%20with%20storage%20requirements%20reduced%20to%0A%24%5Cmathcal%7BO%7D%28n%28d%2Bc%29%20%2B%20cd%5E2%29%24%20and%20a%20computational%20complexity%20of%0A%24%5Cmathcal%7BO%7D%28bncd%5E2%29%24.%20Additionally%2C%20we%20present%20a%20parallel%20implementation%20on%0AGPUs.%20We%20demonstrate%20the%20accuracy%20and%20scalability%20of%20our%20approach%20using%20MNIST%2C%0ACIFAR-10%2C%20Caltech101%2C%20and%20ImageNet.%20The%20accuracy%20tests%20reveal%20no%20deterioration%0Ain%20accuracy%20compared%20to%20FIRAL.%20We%20report%20strong%20and%20weak%20scaling%20tests%20on%20up%20to%0A12%20GPUs%2C%20for%20three%20million%20point%20synthetic%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07392v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Scalable%2520Algorithm%2520for%2520Active%2520Learning%26entry.906535625%3DYouguang%2520Chen%2520and%2520Zheyu%2520Wen%2520and%2520George%2520Biros%26entry.1292438233%3D%2520%2520FIRAL%2520is%2520a%2520recently%2520proposed%2520deterministic%2520active%2520learning%2520algorithm%2520for%250Amulticlass%2520classification%2520using%2520logistic%2520regression.%2520It%2520was%2520shown%2520to%2520outperform%250Athe%2520state-of-the-art%2520in%2520terms%2520of%2520accuracy%2520and%2520robustness%2520and%2520comes%2520with%250Atheoretical%2520performance%2520guarantees.%2520However%252C%2520its%2520scalability%2520suffers%2520when%250Adealing%2520with%2520datasets%2520featuring%2520a%2520large%2520number%2520of%2520points%2520%2524n%2524%252C%2520dimensions%2520%2524d%2524%252C%250Aand%2520classes%2520%2524c%2524%252C%2520due%2520to%2520its%2520%2524%255Cmathcal%257BO%257D%2528c%255E2d%255E2%252Bnc%255E2d%2529%2524%2520storage%2520and%250A%2524%255Cmathcal%257BO%257D%2528c%255E3%2528nd%255E2%2520%252B%2520bd%255E3%2520%252B%2520bn%2529%2529%2524%2520computational%2520complexity%2520where%2520%2524b%2524%2520is%2520the%250Anumber%2520of%2520points%2520to%2520select%2520in%2520active%2520learning.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520an%2520approximate%2520algorithm%2520with%2520storage%2520requirements%2520reduced%2520to%250A%2524%255Cmathcal%257BO%257D%2528n%2528d%252Bc%2529%2520%252B%2520cd%255E2%2529%2524%2520and%2520a%2520computational%2520complexity%2520of%250A%2524%255Cmathcal%257BO%257D%2528bncd%255E2%2529%2524.%2520Additionally%252C%2520we%2520present%2520a%2520parallel%2520implementation%2520on%250AGPUs.%2520We%2520demonstrate%2520the%2520accuracy%2520and%2520scalability%2520of%2520our%2520approach%2520using%2520MNIST%252C%250ACIFAR-10%252C%2520Caltech101%252C%2520and%2520ImageNet.%2520The%2520accuracy%2520tests%2520reveal%2520no%2520deterioration%250Ain%2520accuracy%2520compared%2520to%2520FIRAL.%2520We%2520report%2520strong%2520and%2520weak%2520scaling%2520tests%2520on%2520up%2520to%250A12%2520GPUs%252C%2520for%2520three%2520million%2520point%2520synthetic%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07392v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Scalable%20Algorithm%20for%20Active%20Learning&entry.906535625=Youguang%20Chen%20and%20Zheyu%20Wen%20and%20George%20Biros&entry.1292438233=%20%20FIRAL%20is%20a%20recently%20proposed%20deterministic%20active%20learning%20algorithm%20for%0Amulticlass%20classification%20using%20logistic%20regression.%20It%20was%20shown%20to%20outperform%0Athe%20state-of-the-art%20in%20terms%20of%20accuracy%20and%20robustness%20and%20comes%20with%0Atheoretical%20performance%20guarantees.%20However%2C%20its%20scalability%20suffers%20when%0Adealing%20with%20datasets%20featuring%20a%20large%20number%20of%20points%20%24n%24%2C%20dimensions%20%24d%24%2C%0Aand%20classes%20%24c%24%2C%20due%20to%20its%20%24%5Cmathcal%7BO%7D%28c%5E2d%5E2%2Bnc%5E2d%29%24%20storage%20and%0A%24%5Cmathcal%7BO%7D%28c%5E3%28nd%5E2%20%2B%20bd%5E3%20%2B%20bn%29%29%24%20computational%20complexity%20where%20%24b%24%20is%20the%0Anumber%20of%20points%20to%20select%20in%20active%20learning.%20To%20address%20these%20challenges%2C%20we%0Apropose%20an%20approximate%20algorithm%20with%20storage%20requirements%20reduced%20to%0A%24%5Cmathcal%7BO%7D%28n%28d%2Bc%29%20%2B%20cd%5E2%29%24%20and%20a%20computational%20complexity%20of%0A%24%5Cmathcal%7BO%7D%28bncd%5E2%29%24.%20Additionally%2C%20we%20present%20a%20parallel%20implementation%20on%0AGPUs.%20We%20demonstrate%20the%20accuracy%20and%20scalability%20of%20our%20approach%20using%20MNIST%2C%0ACIFAR-10%2C%20Caltech101%2C%20and%20ImageNet.%20The%20accuracy%20tests%20reveal%20no%20deterioration%0Ain%20accuracy%20compared%20to%20FIRAL.%20We%20report%20strong%20and%20weak%20scaling%20tests%20on%20up%20to%0A12%20GPUs%2C%20for%20three%20million%20point%20synthetic%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07392v1&entry.124074799=Read"},
{"title": "Contrastive Learning and Abstract Concepts: The Case of Natural Numbers", "author": "Daniel N. Nissani", "abstract": "  Contrastive Learning (CL) has been successfully applied to classification and\nother downstream tasks related to concrete concepts, such as objects contained\nin the ImageNet dataset. No attempts seem to have been made so far in applying\nthis promising scheme to more abstract entities. A prominent example of these\ncould be the concept of (discrete) Quantity. CL can be frequently interpreted\nas a self-supervised scheme guided by some profound and ubiquitous conservation\nprinciple (e.g. conservation of identity in object classification tasks). In\nthis introductory work we apply a suitable conservation principle to the\nsemi-abstract concept of natural numbers by which discrete quantities can be\nestimated or predicted. We experimentally show, by means of a toy problem, that\ncontrastive learning can be trained to count at a glance with high accuracy\nboth at human as well as at super-human ranges.. We compare this with the\nresults of a trained-to-count at a glance supervised learning (SL) neural\nnetwork scheme of similar architecture. We show that both schemes exhibit\nsimilar good performance on baseline experiments, where the distributions of\nthe training and testing stages are equal. Importantly, we demonstrate that in\nsome generalization scenarios, where training and testing distributions differ,\nCL boasts more robust and much better error performance.\n", "link": "http://arxiv.org/abs/2408.02247v5", "date": "2024-09-11", "relevancy": 2.398, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4882}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4753}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Learning%20and%20Abstract%20Concepts%3A%20The%20Case%20of%20Natural%20Numbers&body=Title%3A%20Contrastive%20Learning%20and%20Abstract%20Concepts%3A%20The%20Case%20of%20Natural%20Numbers%0AAuthor%3A%20Daniel%20N.%20Nissani%0AAbstract%3A%20%20%20Contrastive%20Learning%20%28CL%29%20has%20been%20successfully%20applied%20to%20classification%20and%0Aother%20downstream%20tasks%20related%20to%20concrete%20concepts%2C%20such%20as%20objects%20contained%0Ain%20the%20ImageNet%20dataset.%20No%20attempts%20seem%20to%20have%20been%20made%20so%20far%20in%20applying%0Athis%20promising%20scheme%20to%20more%20abstract%20entities.%20A%20prominent%20example%20of%20these%0Acould%20be%20the%20concept%20of%20%28discrete%29%20Quantity.%20CL%20can%20be%20frequently%20interpreted%0Aas%20a%20self-supervised%20scheme%20guided%20by%20some%20profound%20and%20ubiquitous%20conservation%0Aprinciple%20%28e.g.%20conservation%20of%20identity%20in%20object%20classification%20tasks%29.%20In%0Athis%20introductory%20work%20we%20apply%20a%20suitable%20conservation%20principle%20to%20the%0Asemi-abstract%20concept%20of%20natural%20numbers%20by%20which%20discrete%20quantities%20can%20be%0Aestimated%20or%20predicted.%20We%20experimentally%20show%2C%20by%20means%20of%20a%20toy%20problem%2C%20that%0Acontrastive%20learning%20can%20be%20trained%20to%20count%20at%20a%20glance%20with%20high%20accuracy%0Aboth%20at%20human%20as%20well%20as%20at%20super-human%20ranges..%20We%20compare%20this%20with%20the%0Aresults%20of%20a%20trained-to-count%20at%20a%20glance%20supervised%20learning%20%28SL%29%20neural%0Anetwork%20scheme%20of%20similar%20architecture.%20We%20show%20that%20both%20schemes%20exhibit%0Asimilar%20good%20performance%20on%20baseline%20experiments%2C%20where%20the%20distributions%20of%0Athe%20training%20and%20testing%20stages%20are%20equal.%20Importantly%2C%20we%20demonstrate%20that%20in%0Asome%20generalization%20scenarios%2C%20where%20training%20and%20testing%20distributions%20differ%2C%0ACL%20boasts%20more%20robust%20and%20much%20better%20error%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02247v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520Learning%2520and%2520Abstract%2520Concepts%253A%2520The%2520Case%2520of%2520Natural%2520Numbers%26entry.906535625%3DDaniel%2520N.%2520Nissani%26entry.1292438233%3D%2520%2520Contrastive%2520Learning%2520%2528CL%2529%2520has%2520been%2520successfully%2520applied%2520to%2520classification%2520and%250Aother%2520downstream%2520tasks%2520related%2520to%2520concrete%2520concepts%252C%2520such%2520as%2520objects%2520contained%250Ain%2520the%2520ImageNet%2520dataset.%2520No%2520attempts%2520seem%2520to%2520have%2520been%2520made%2520so%2520far%2520in%2520applying%250Athis%2520promising%2520scheme%2520to%2520more%2520abstract%2520entities.%2520A%2520prominent%2520example%2520of%2520these%250Acould%2520be%2520the%2520concept%2520of%2520%2528discrete%2529%2520Quantity.%2520CL%2520can%2520be%2520frequently%2520interpreted%250Aas%2520a%2520self-supervised%2520scheme%2520guided%2520by%2520some%2520profound%2520and%2520ubiquitous%2520conservation%250Aprinciple%2520%2528e.g.%2520conservation%2520of%2520identity%2520in%2520object%2520classification%2520tasks%2529.%2520In%250Athis%2520introductory%2520work%2520we%2520apply%2520a%2520suitable%2520conservation%2520principle%2520to%2520the%250Asemi-abstract%2520concept%2520of%2520natural%2520numbers%2520by%2520which%2520discrete%2520quantities%2520can%2520be%250Aestimated%2520or%2520predicted.%2520We%2520experimentally%2520show%252C%2520by%2520means%2520of%2520a%2520toy%2520problem%252C%2520that%250Acontrastive%2520learning%2520can%2520be%2520trained%2520to%2520count%2520at%2520a%2520glance%2520with%2520high%2520accuracy%250Aboth%2520at%2520human%2520as%2520well%2520as%2520at%2520super-human%2520ranges..%2520We%2520compare%2520this%2520with%2520the%250Aresults%2520of%2520a%2520trained-to-count%2520at%2520a%2520glance%2520supervised%2520learning%2520%2528SL%2529%2520neural%250Anetwork%2520scheme%2520of%2520similar%2520architecture.%2520We%2520show%2520that%2520both%2520schemes%2520exhibit%250Asimilar%2520good%2520performance%2520on%2520baseline%2520experiments%252C%2520where%2520the%2520distributions%2520of%250Athe%2520training%2520and%2520testing%2520stages%2520are%2520equal.%2520Importantly%252C%2520we%2520demonstrate%2520that%2520in%250Asome%2520generalization%2520scenarios%252C%2520where%2520training%2520and%2520testing%2520distributions%2520differ%252C%250ACL%2520boasts%2520more%2520robust%2520and%2520much%2520better%2520error%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02247v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Learning%20and%20Abstract%20Concepts%3A%20The%20Case%20of%20Natural%20Numbers&entry.906535625=Daniel%20N.%20Nissani&entry.1292438233=%20%20Contrastive%20Learning%20%28CL%29%20has%20been%20successfully%20applied%20to%20classification%20and%0Aother%20downstream%20tasks%20related%20to%20concrete%20concepts%2C%20such%20as%20objects%20contained%0Ain%20the%20ImageNet%20dataset.%20No%20attempts%20seem%20to%20have%20been%20made%20so%20far%20in%20applying%0Athis%20promising%20scheme%20to%20more%20abstract%20entities.%20A%20prominent%20example%20of%20these%0Acould%20be%20the%20concept%20of%20%28discrete%29%20Quantity.%20CL%20can%20be%20frequently%20interpreted%0Aas%20a%20self-supervised%20scheme%20guided%20by%20some%20profound%20and%20ubiquitous%20conservation%0Aprinciple%20%28e.g.%20conservation%20of%20identity%20in%20object%20classification%20tasks%29.%20In%0Athis%20introductory%20work%20we%20apply%20a%20suitable%20conservation%20principle%20to%20the%0Asemi-abstract%20concept%20of%20natural%20numbers%20by%20which%20discrete%20quantities%20can%20be%0Aestimated%20or%20predicted.%20We%20experimentally%20show%2C%20by%20means%20of%20a%20toy%20problem%2C%20that%0Acontrastive%20learning%20can%20be%20trained%20to%20count%20at%20a%20glance%20with%20high%20accuracy%0Aboth%20at%20human%20as%20well%20as%20at%20super-human%20ranges..%20We%20compare%20this%20with%20the%0Aresults%20of%20a%20trained-to-count%20at%20a%20glance%20supervised%20learning%20%28SL%29%20neural%0Anetwork%20scheme%20of%20similar%20architecture.%20We%20show%20that%20both%20schemes%20exhibit%0Asimilar%20good%20performance%20on%20baseline%20experiments%2C%20where%20the%20distributions%20of%0Athe%20training%20and%20testing%20stages%20are%20equal.%20Importantly%2C%20we%20demonstrate%20that%20in%0Asome%20generalization%20scenarios%2C%20where%20training%20and%20testing%20distributions%20differ%2C%0ACL%20boasts%20more%20robust%20and%20much%20better%20error%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02247v5&entry.124074799=Read"},
{"title": "D-CAPTCHA++: A Study of Resilience of Deepfake CAPTCHA under\n  Transferable Imperceptible Adversarial Attack", "author": "Hong-Hanh Nguyen-Le and Van-Tuan Tran and Dinh-Thuc Nguyen and Nhien-An Le-Khac", "abstract": "  The advancements in generative AI have enabled the improvement of audio\nsynthesis models, including text-to-speech and voice conversion. This raises\nconcerns about its potential misuse in social manipulation and political\ninterference, as synthetic speech has become indistinguishable from natural\nhuman speech. Several speech-generation programs are utilized for malicious\npurposes, especially impersonating individuals through phone calls. Therefore,\ndetecting fake audio is crucial to maintain social security and safeguard the\nintegrity of information. Recent research has proposed a D-CAPTCHA system based\non the challenge-response protocol to differentiate fake phone calls from real\nones. In this work, we study the resilience of this system and introduce a more\nrobust version, D-CAPTCHA++, to defend against fake calls. Specifically, we\nfirst expose the vulnerability of the D-CAPTCHA system under transferable\nimperceptible adversarial attack. Secondly, we mitigate such vulnerability by\nimproving the robustness of the system by using adversarial training in\nD-CAPTCHA deepfake detectors and task classifiers.\n", "link": "http://arxiv.org/abs/2409.07390v1", "date": "2024-09-11", "relevancy": 2.3928, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5066}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4686}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20D-CAPTCHA%2B%2B%3A%20A%20Study%20of%20Resilience%20of%20Deepfake%20CAPTCHA%20under%0A%20%20Transferable%20Imperceptible%20Adversarial%20Attack&body=Title%3A%20D-CAPTCHA%2B%2B%3A%20A%20Study%20of%20Resilience%20of%20Deepfake%20CAPTCHA%20under%0A%20%20Transferable%20Imperceptible%20Adversarial%20Attack%0AAuthor%3A%20Hong-Hanh%20Nguyen-Le%20and%20Van-Tuan%20Tran%20and%20Dinh-Thuc%20Nguyen%20and%20Nhien-An%20Le-Khac%0AAbstract%3A%20%20%20The%20advancements%20in%20generative%20AI%20have%20enabled%20the%20improvement%20of%20audio%0Asynthesis%20models%2C%20including%20text-to-speech%20and%20voice%20conversion.%20This%20raises%0Aconcerns%20about%20its%20potential%20misuse%20in%20social%20manipulation%20and%20political%0Ainterference%2C%20as%20synthetic%20speech%20has%20become%20indistinguishable%20from%20natural%0Ahuman%20speech.%20Several%20speech-generation%20programs%20are%20utilized%20for%20malicious%0Apurposes%2C%20especially%20impersonating%20individuals%20through%20phone%20calls.%20Therefore%2C%0Adetecting%20fake%20audio%20is%20crucial%20to%20maintain%20social%20security%20and%20safeguard%20the%0Aintegrity%20of%20information.%20Recent%20research%20has%20proposed%20a%20D-CAPTCHA%20system%20based%0Aon%20the%20challenge-response%20protocol%20to%20differentiate%20fake%20phone%20calls%20from%20real%0Aones.%20In%20this%20work%2C%20we%20study%20the%20resilience%20of%20this%20system%20and%20introduce%20a%20more%0Arobust%20version%2C%20D-CAPTCHA%2B%2B%2C%20to%20defend%20against%20fake%20calls.%20Specifically%2C%20we%0Afirst%20expose%20the%20vulnerability%20of%20the%20D-CAPTCHA%20system%20under%20transferable%0Aimperceptible%20adversarial%20attack.%20Secondly%2C%20we%20mitigate%20such%20vulnerability%20by%0Aimproving%20the%20robustness%20of%20the%20system%20by%20using%20adversarial%20training%20in%0AD-CAPTCHA%20deepfake%20detectors%20and%20task%20classifiers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07390v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DD-CAPTCHA%252B%252B%253A%2520A%2520Study%2520of%2520Resilience%2520of%2520Deepfake%2520CAPTCHA%2520under%250A%2520%2520Transferable%2520Imperceptible%2520Adversarial%2520Attack%26entry.906535625%3DHong-Hanh%2520Nguyen-Le%2520and%2520Van-Tuan%2520Tran%2520and%2520Dinh-Thuc%2520Nguyen%2520and%2520Nhien-An%2520Le-Khac%26entry.1292438233%3D%2520%2520The%2520advancements%2520in%2520generative%2520AI%2520have%2520enabled%2520the%2520improvement%2520of%2520audio%250Asynthesis%2520models%252C%2520including%2520text-to-speech%2520and%2520voice%2520conversion.%2520This%2520raises%250Aconcerns%2520about%2520its%2520potential%2520misuse%2520in%2520social%2520manipulation%2520and%2520political%250Ainterference%252C%2520as%2520synthetic%2520speech%2520has%2520become%2520indistinguishable%2520from%2520natural%250Ahuman%2520speech.%2520Several%2520speech-generation%2520programs%2520are%2520utilized%2520for%2520malicious%250Apurposes%252C%2520especially%2520impersonating%2520individuals%2520through%2520phone%2520calls.%2520Therefore%252C%250Adetecting%2520fake%2520audio%2520is%2520crucial%2520to%2520maintain%2520social%2520security%2520and%2520safeguard%2520the%250Aintegrity%2520of%2520information.%2520Recent%2520research%2520has%2520proposed%2520a%2520D-CAPTCHA%2520system%2520based%250Aon%2520the%2520challenge-response%2520protocol%2520to%2520differentiate%2520fake%2520phone%2520calls%2520from%2520real%250Aones.%2520In%2520this%2520work%252C%2520we%2520study%2520the%2520resilience%2520of%2520this%2520system%2520and%2520introduce%2520a%2520more%250Arobust%2520version%252C%2520D-CAPTCHA%252B%252B%252C%2520to%2520defend%2520against%2520fake%2520calls.%2520Specifically%252C%2520we%250Afirst%2520expose%2520the%2520vulnerability%2520of%2520the%2520D-CAPTCHA%2520system%2520under%2520transferable%250Aimperceptible%2520adversarial%2520attack.%2520Secondly%252C%2520we%2520mitigate%2520such%2520vulnerability%2520by%250Aimproving%2520the%2520robustness%2520of%2520the%2520system%2520by%2520using%2520adversarial%2520training%2520in%250AD-CAPTCHA%2520deepfake%2520detectors%2520and%2520task%2520classifiers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07390v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D-CAPTCHA%2B%2B%3A%20A%20Study%20of%20Resilience%20of%20Deepfake%20CAPTCHA%20under%0A%20%20Transferable%20Imperceptible%20Adversarial%20Attack&entry.906535625=Hong-Hanh%20Nguyen-Le%20and%20Van-Tuan%20Tran%20and%20Dinh-Thuc%20Nguyen%20and%20Nhien-An%20Le-Khac&entry.1292438233=%20%20The%20advancements%20in%20generative%20AI%20have%20enabled%20the%20improvement%20of%20audio%0Asynthesis%20models%2C%20including%20text-to-speech%20and%20voice%20conversion.%20This%20raises%0Aconcerns%20about%20its%20potential%20misuse%20in%20social%20manipulation%20and%20political%0Ainterference%2C%20as%20synthetic%20speech%20has%20become%20indistinguishable%20from%20natural%0Ahuman%20speech.%20Several%20speech-generation%20programs%20are%20utilized%20for%20malicious%0Apurposes%2C%20especially%20impersonating%20individuals%20through%20phone%20calls.%20Therefore%2C%0Adetecting%20fake%20audio%20is%20crucial%20to%20maintain%20social%20security%20and%20safeguard%20the%0Aintegrity%20of%20information.%20Recent%20research%20has%20proposed%20a%20D-CAPTCHA%20system%20based%0Aon%20the%20challenge-response%20protocol%20to%20differentiate%20fake%20phone%20calls%20from%20real%0Aones.%20In%20this%20work%2C%20we%20study%20the%20resilience%20of%20this%20system%20and%20introduce%20a%20more%0Arobust%20version%2C%20D-CAPTCHA%2B%2B%2C%20to%20defend%20against%20fake%20calls.%20Specifically%2C%20we%0Afirst%20expose%20the%20vulnerability%20of%20the%20D-CAPTCHA%20system%20under%20transferable%0Aimperceptible%20adversarial%20attack.%20Secondly%2C%20we%20mitigate%20such%20vulnerability%20by%0Aimproving%20the%20robustness%20of%20the%20system%20by%20using%20adversarial%20training%20in%0AD-CAPTCHA%20deepfake%20detectors%20and%20task%20classifiers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07390v1&entry.124074799=Read"},
{"title": "MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D\n  Features as Text Tokens for Autonomous Driving", "author": "Enming Zhang and Xingyuan Dai and Yisheng Lv and Qianghai Miao", "abstract": "  Vision-language models (VLMs) serve as general-purpose end-to-end models in\nautonomous driving, performing subtasks such as prediction, planning, and\nperception through question-and-answer interactions. However, most existing\nmethods rely on computationally expensive visual encoders and large language\nmodels (LLMs), making them difficult to deploy in real-world scenarios and\nreal-time applications. Meanwhile, most existing VLMs lack the ability to\nprocess multiple images, making it difficult to adapt to multi-camera\nperception in autonomous driving. To address these issues, we propose a novel\nframework called MiniDrive, which incorporates our proposed Feature Engineering\nMixture of Experts (FE-MoE) module and Dynamic Instruction Adapter\n(DI-Adapter). The FE-MoE effectively maps 2D features into visual token\nembeddings before being input into the language model. The DI-Adapter enables\nthe visual token embeddings to dynamically change with the instruction text\nembeddings, resolving the issue of static visual token embeddings for the same\nimage in previous approaches. Compared to previous works, MiniDrive achieves\nstate-of-the-art performance in terms of parameter size, floating point\noperations, and response efficiency, with the smallest version containing only\n83M parameters.\n", "link": "http://arxiv.org/abs/2409.07267v1", "date": "2024-09-11", "relevancy": 2.3908, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6024}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6024}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MiniDrive%3A%20More%20Efficient%20Vision-Language%20Models%20with%20Multi-Level%202D%0A%20%20Features%20as%20Text%20Tokens%20for%20Autonomous%20Driving&body=Title%3A%20MiniDrive%3A%20More%20Efficient%20Vision-Language%20Models%20with%20Multi-Level%202D%0A%20%20Features%20as%20Text%20Tokens%20for%20Autonomous%20Driving%0AAuthor%3A%20Enming%20Zhang%20and%20Xingyuan%20Dai%20and%20Yisheng%20Lv%20and%20Qianghai%20Miao%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20serve%20as%20general-purpose%20end-to-end%20models%20in%0Aautonomous%20driving%2C%20performing%20subtasks%20such%20as%20prediction%2C%20planning%2C%20and%0Aperception%20through%20question-and-answer%20interactions.%20However%2C%20most%20existing%0Amethods%20rely%20on%20computationally%20expensive%20visual%20encoders%20and%20large%20language%0Amodels%20%28LLMs%29%2C%20making%20them%20difficult%20to%20deploy%20in%20real-world%20scenarios%20and%0Areal-time%20applications.%20Meanwhile%2C%20most%20existing%20VLMs%20lack%20the%20ability%20to%0Aprocess%20multiple%20images%2C%20making%20it%20difficult%20to%20adapt%20to%20multi-camera%0Aperception%20in%20autonomous%20driving.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%0Aframework%20called%20MiniDrive%2C%20which%20incorporates%20our%20proposed%20Feature%20Engineering%0AMixture%20of%20Experts%20%28FE-MoE%29%20module%20and%20Dynamic%20Instruction%20Adapter%0A%28DI-Adapter%29.%20The%20FE-MoE%20effectively%20maps%202D%20features%20into%20visual%20token%0Aembeddings%20before%20being%20input%20into%20the%20language%20model.%20The%20DI-Adapter%20enables%0Athe%20visual%20token%20embeddings%20to%20dynamically%20change%20with%20the%20instruction%20text%0Aembeddings%2C%20resolving%20the%20issue%20of%20static%20visual%20token%20embeddings%20for%20the%20same%0Aimage%20in%20previous%20approaches.%20Compared%20to%20previous%20works%2C%20MiniDrive%20achieves%0Astate-of-the-art%20performance%20in%20terms%20of%20parameter%20size%2C%20floating%20point%0Aoperations%2C%20and%20response%20efficiency%2C%20with%20the%20smallest%20version%20containing%20only%0A83M%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07267v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMiniDrive%253A%2520More%2520Efficient%2520Vision-Language%2520Models%2520with%2520Multi-Level%25202D%250A%2520%2520Features%2520as%2520Text%2520Tokens%2520for%2520Autonomous%2520Driving%26entry.906535625%3DEnming%2520Zhang%2520and%2520Xingyuan%2520Dai%2520and%2520Yisheng%2520Lv%2520and%2520Qianghai%2520Miao%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520serve%2520as%2520general-purpose%2520end-to-end%2520models%2520in%250Aautonomous%2520driving%252C%2520performing%2520subtasks%2520such%2520as%2520prediction%252C%2520planning%252C%2520and%250Aperception%2520through%2520question-and-answer%2520interactions.%2520However%252C%2520most%2520existing%250Amethods%2520rely%2520on%2520computationally%2520expensive%2520visual%2520encoders%2520and%2520large%2520language%250Amodels%2520%2528LLMs%2529%252C%2520making%2520them%2520difficult%2520to%2520deploy%2520in%2520real-world%2520scenarios%2520and%250Areal-time%2520applications.%2520Meanwhile%252C%2520most%2520existing%2520VLMs%2520lack%2520the%2520ability%2520to%250Aprocess%2520multiple%2520images%252C%2520making%2520it%2520difficult%2520to%2520adapt%2520to%2520multi-camera%250Aperception%2520in%2520autonomous%2520driving.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%250Aframework%2520called%2520MiniDrive%252C%2520which%2520incorporates%2520our%2520proposed%2520Feature%2520Engineering%250AMixture%2520of%2520Experts%2520%2528FE-MoE%2529%2520module%2520and%2520Dynamic%2520Instruction%2520Adapter%250A%2528DI-Adapter%2529.%2520The%2520FE-MoE%2520effectively%2520maps%25202D%2520features%2520into%2520visual%2520token%250Aembeddings%2520before%2520being%2520input%2520into%2520the%2520language%2520model.%2520The%2520DI-Adapter%2520enables%250Athe%2520visual%2520token%2520embeddings%2520to%2520dynamically%2520change%2520with%2520the%2520instruction%2520text%250Aembeddings%252C%2520resolving%2520the%2520issue%2520of%2520static%2520visual%2520token%2520embeddings%2520for%2520the%2520same%250Aimage%2520in%2520previous%2520approaches.%2520Compared%2520to%2520previous%2520works%252C%2520MiniDrive%2520achieves%250Astate-of-the-art%2520performance%2520in%2520terms%2520of%2520parameter%2520size%252C%2520floating%2520point%250Aoperations%252C%2520and%2520response%2520efficiency%252C%2520with%2520the%2520smallest%2520version%2520containing%2520only%250A83M%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07267v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MiniDrive%3A%20More%20Efficient%20Vision-Language%20Models%20with%20Multi-Level%202D%0A%20%20Features%20as%20Text%20Tokens%20for%20Autonomous%20Driving&entry.906535625=Enming%20Zhang%20and%20Xingyuan%20Dai%20and%20Yisheng%20Lv%20and%20Qianghai%20Miao&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20serve%20as%20general-purpose%20end-to-end%20models%20in%0Aautonomous%20driving%2C%20performing%20subtasks%20such%20as%20prediction%2C%20planning%2C%20and%0Aperception%20through%20question-and-answer%20interactions.%20However%2C%20most%20existing%0Amethods%20rely%20on%20computationally%20expensive%20visual%20encoders%20and%20large%20language%0Amodels%20%28LLMs%29%2C%20making%20them%20difficult%20to%20deploy%20in%20real-world%20scenarios%20and%0Areal-time%20applications.%20Meanwhile%2C%20most%20existing%20VLMs%20lack%20the%20ability%20to%0Aprocess%20multiple%20images%2C%20making%20it%20difficult%20to%20adapt%20to%20multi-camera%0Aperception%20in%20autonomous%20driving.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%0Aframework%20called%20MiniDrive%2C%20which%20incorporates%20our%20proposed%20Feature%20Engineering%0AMixture%20of%20Experts%20%28FE-MoE%29%20module%20and%20Dynamic%20Instruction%20Adapter%0A%28DI-Adapter%29.%20The%20FE-MoE%20effectively%20maps%202D%20features%20into%20visual%20token%0Aembeddings%20before%20being%20input%20into%20the%20language%20model.%20The%20DI-Adapter%20enables%0Athe%20visual%20token%20embeddings%20to%20dynamically%20change%20with%20the%20instruction%20text%0Aembeddings%2C%20resolving%20the%20issue%20of%20static%20visual%20token%20embeddings%20for%20the%20same%0Aimage%20in%20previous%20approaches.%20Compared%20to%20previous%20works%2C%20MiniDrive%20achieves%0Astate-of-the-art%20performance%20in%20terms%20of%20parameter%20size%2C%20floating%20point%0Aoperations%2C%20and%20response%20efficiency%2C%20with%20the%20smallest%20version%20containing%20only%0A83M%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07267v1&entry.124074799=Read"},
{"title": "EMOdiffhead: Continuously Emotional Control in Talking Head Generation\n  via Diffusion", "author": "Jian Zhang and Weijian Mai and Zhijun Zhang", "abstract": "  The task of audio-driven portrait animation involves generating a talking\nhead video using an identity image and an audio track of speech. While many\nexisting approaches focus on lip synchronization and video quality, few tackle\nthe challenge of generating emotion-driven talking head videos. The ability to\ncontrol and edit emotions is essential for producing expressive and realistic\nanimations. In response to this challenge, we propose EMOdiffhead, a novel\nmethod for emotional talking head video generation that not only enables\nfine-grained control of emotion categories and intensities but also enables\none-shot generation. Given the FLAME 3D model's linearity in expression\nmodeling, we utilize the DECA method to extract expression vectors, that are\ncombined with audio to guide a diffusion model in generating videos with\nprecise lip synchronization and rich emotional expressiveness. This approach\nnot only enables the learning of rich facial information from\nemotion-irrelevant data but also facilitates the generation of emotional\nvideos. It effectively overcomes the limitations of emotional data, such as the\nlack of diversity in facial and background information, and addresses the\nabsence of emotional details in emotion-irrelevant data. Extensive experiments\nand user studies demonstrate that our approach achieves state-of-the-art\nperformance compared to other emotion portrait animation methods.\n", "link": "http://arxiv.org/abs/2409.07255v1", "date": "2024-09-11", "relevancy": 2.3867, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6005}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5988}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EMOdiffhead%3A%20Continuously%20Emotional%20Control%20in%20Talking%20Head%20Generation%0A%20%20via%20Diffusion&body=Title%3A%20EMOdiffhead%3A%20Continuously%20Emotional%20Control%20in%20Talking%20Head%20Generation%0A%20%20via%20Diffusion%0AAuthor%3A%20Jian%20Zhang%20and%20Weijian%20Mai%20and%20Zhijun%20Zhang%0AAbstract%3A%20%20%20The%20task%20of%20audio-driven%20portrait%20animation%20involves%20generating%20a%20talking%0Ahead%20video%20using%20an%20identity%20image%20and%20an%20audio%20track%20of%20speech.%20While%20many%0Aexisting%20approaches%20focus%20on%20lip%20synchronization%20and%20video%20quality%2C%20few%20tackle%0Athe%20challenge%20of%20generating%20emotion-driven%20talking%20head%20videos.%20The%20ability%20to%0Acontrol%20and%20edit%20emotions%20is%20essential%20for%20producing%20expressive%20and%20realistic%0Aanimations.%20In%20response%20to%20this%20challenge%2C%20we%20propose%20EMOdiffhead%2C%20a%20novel%0Amethod%20for%20emotional%20talking%20head%20video%20generation%20that%20not%20only%20enables%0Afine-grained%20control%20of%20emotion%20categories%20and%20intensities%20but%20also%20enables%0Aone-shot%20generation.%20Given%20the%20FLAME%203D%20model%27s%20linearity%20in%20expression%0Amodeling%2C%20we%20utilize%20the%20DECA%20method%20to%20extract%20expression%20vectors%2C%20that%20are%0Acombined%20with%20audio%20to%20guide%20a%20diffusion%20model%20in%20generating%20videos%20with%0Aprecise%20lip%20synchronization%20and%20rich%20emotional%20expressiveness.%20This%20approach%0Anot%20only%20enables%20the%20learning%20of%20rich%20facial%20information%20from%0Aemotion-irrelevant%20data%20but%20also%20facilitates%20the%20generation%20of%20emotional%0Avideos.%20It%20effectively%20overcomes%20the%20limitations%20of%20emotional%20data%2C%20such%20as%20the%0Alack%20of%20diversity%20in%20facial%20and%20background%20information%2C%20and%20addresses%20the%0Aabsence%20of%20emotional%20details%20in%20emotion-irrelevant%20data.%20Extensive%20experiments%0Aand%20user%20studies%20demonstrate%20that%20our%20approach%20achieves%20state-of-the-art%0Aperformance%20compared%20to%20other%20emotion%20portrait%20animation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07255v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEMOdiffhead%253A%2520Continuously%2520Emotional%2520Control%2520in%2520Talking%2520Head%2520Generation%250A%2520%2520via%2520Diffusion%26entry.906535625%3DJian%2520Zhang%2520and%2520Weijian%2520Mai%2520and%2520Zhijun%2520Zhang%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520audio-driven%2520portrait%2520animation%2520involves%2520generating%2520a%2520talking%250Ahead%2520video%2520using%2520an%2520identity%2520image%2520and%2520an%2520audio%2520track%2520of%2520speech.%2520While%2520many%250Aexisting%2520approaches%2520focus%2520on%2520lip%2520synchronization%2520and%2520video%2520quality%252C%2520few%2520tackle%250Athe%2520challenge%2520of%2520generating%2520emotion-driven%2520talking%2520head%2520videos.%2520The%2520ability%2520to%250Acontrol%2520and%2520edit%2520emotions%2520is%2520essential%2520for%2520producing%2520expressive%2520and%2520realistic%250Aanimations.%2520In%2520response%2520to%2520this%2520challenge%252C%2520we%2520propose%2520EMOdiffhead%252C%2520a%2520novel%250Amethod%2520for%2520emotional%2520talking%2520head%2520video%2520generation%2520that%2520not%2520only%2520enables%250Afine-grained%2520control%2520of%2520emotion%2520categories%2520and%2520intensities%2520but%2520also%2520enables%250Aone-shot%2520generation.%2520Given%2520the%2520FLAME%25203D%2520model%2527s%2520linearity%2520in%2520expression%250Amodeling%252C%2520we%2520utilize%2520the%2520DECA%2520method%2520to%2520extract%2520expression%2520vectors%252C%2520that%2520are%250Acombined%2520with%2520audio%2520to%2520guide%2520a%2520diffusion%2520model%2520in%2520generating%2520videos%2520with%250Aprecise%2520lip%2520synchronization%2520and%2520rich%2520emotional%2520expressiveness.%2520This%2520approach%250Anot%2520only%2520enables%2520the%2520learning%2520of%2520rich%2520facial%2520information%2520from%250Aemotion-irrelevant%2520data%2520but%2520also%2520facilitates%2520the%2520generation%2520of%2520emotional%250Avideos.%2520It%2520effectively%2520overcomes%2520the%2520limitations%2520of%2520emotional%2520data%252C%2520such%2520as%2520the%250Alack%2520of%2520diversity%2520in%2520facial%2520and%2520background%2520information%252C%2520and%2520addresses%2520the%250Aabsence%2520of%2520emotional%2520details%2520in%2520emotion-irrelevant%2520data.%2520Extensive%2520experiments%250Aand%2520user%2520studies%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520state-of-the-art%250Aperformance%2520compared%2520to%2520other%2520emotion%2520portrait%2520animation%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07255v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EMOdiffhead%3A%20Continuously%20Emotional%20Control%20in%20Talking%20Head%20Generation%0A%20%20via%20Diffusion&entry.906535625=Jian%20Zhang%20and%20Weijian%20Mai%20and%20Zhijun%20Zhang&entry.1292438233=%20%20The%20task%20of%20audio-driven%20portrait%20animation%20involves%20generating%20a%20talking%0Ahead%20video%20using%20an%20identity%20image%20and%20an%20audio%20track%20of%20speech.%20While%20many%0Aexisting%20approaches%20focus%20on%20lip%20synchronization%20and%20video%20quality%2C%20few%20tackle%0Athe%20challenge%20of%20generating%20emotion-driven%20talking%20head%20videos.%20The%20ability%20to%0Acontrol%20and%20edit%20emotions%20is%20essential%20for%20producing%20expressive%20and%20realistic%0Aanimations.%20In%20response%20to%20this%20challenge%2C%20we%20propose%20EMOdiffhead%2C%20a%20novel%0Amethod%20for%20emotional%20talking%20head%20video%20generation%20that%20not%20only%20enables%0Afine-grained%20control%20of%20emotion%20categories%20and%20intensities%20but%20also%20enables%0Aone-shot%20generation.%20Given%20the%20FLAME%203D%20model%27s%20linearity%20in%20expression%0Amodeling%2C%20we%20utilize%20the%20DECA%20method%20to%20extract%20expression%20vectors%2C%20that%20are%0Acombined%20with%20audio%20to%20guide%20a%20diffusion%20model%20in%20generating%20videos%20with%0Aprecise%20lip%20synchronization%20and%20rich%20emotional%20expressiveness.%20This%20approach%0Anot%20only%20enables%20the%20learning%20of%20rich%20facial%20information%20from%0Aemotion-irrelevant%20data%20but%20also%20facilitates%20the%20generation%20of%20emotional%0Avideos.%20It%20effectively%20overcomes%20the%20limitations%20of%20emotional%20data%2C%20such%20as%20the%0Alack%20of%20diversity%20in%20facial%20and%20background%20information%2C%20and%20addresses%20the%0Aabsence%20of%20emotional%20details%20in%20emotion-irrelevant%20data.%20Extensive%20experiments%0Aand%20user%20studies%20demonstrate%20that%20our%20approach%20achieves%20state-of-the-art%0Aperformance%20compared%20to%20other%20emotion%20portrait%20animation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07255v1&entry.124074799=Read"},
{"title": "Tuning-Free Online Robust Principal Component Analysis through Implicit\n  Regularization", "author": "Lakshmi Jayalal and Gokularam Muthukrishnan and Sheetal Kalyani", "abstract": "  The performance of the standard Online Robust Principal Component Analysis\n(OR-PCA) technique depends on the optimum tuning of the explicit regularizers\nand this tuning is dataset sensitive. We aim to remove the dependency on these\ntuning parameters by using implicit regularization. We propose to use the\nimplicit regularization effect of various modified gradient descents to make\nOR-PCA tuning free. Our method incorporates three different versions of\nmodified gradient descent that separately but naturally encourage sparsity and\nlow-rank structures in the data. The proposed method performs comparable or\nbetter than the tuned OR-PCA for both simulated and real-world datasets.\nTuning-free ORPCA makes it more scalable for large datasets since we do not\nrequire dataset-dependent parameter tuning.\n", "link": "http://arxiv.org/abs/2409.07275v1", "date": "2024-09-11", "relevancy": 2.3782, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5071}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4632}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tuning-Free%20Online%20Robust%20Principal%20Component%20Analysis%20through%20Implicit%0A%20%20Regularization&body=Title%3A%20Tuning-Free%20Online%20Robust%20Principal%20Component%20Analysis%20through%20Implicit%0A%20%20Regularization%0AAuthor%3A%20Lakshmi%20Jayalal%20and%20Gokularam%20Muthukrishnan%20and%20Sheetal%20Kalyani%0AAbstract%3A%20%20%20The%20performance%20of%20the%20standard%20Online%20Robust%20Principal%20Component%20Analysis%0A%28OR-PCA%29%20technique%20depends%20on%20the%20optimum%20tuning%20of%20the%20explicit%20regularizers%0Aand%20this%20tuning%20is%20dataset%20sensitive.%20We%20aim%20to%20remove%20the%20dependency%20on%20these%0Atuning%20parameters%20by%20using%20implicit%20regularization.%20We%20propose%20to%20use%20the%0Aimplicit%20regularization%20effect%20of%20various%20modified%20gradient%20descents%20to%20make%0AOR-PCA%20tuning%20free.%20Our%20method%20incorporates%20three%20different%20versions%20of%0Amodified%20gradient%20descent%20that%20separately%20but%20naturally%20encourage%20sparsity%20and%0Alow-rank%20structures%20in%20the%20data.%20The%20proposed%20method%20performs%20comparable%20or%0Abetter%20than%20the%20tuned%20OR-PCA%20for%20both%20simulated%20and%20real-world%20datasets.%0ATuning-free%20ORPCA%20makes%20it%20more%20scalable%20for%20large%20datasets%20since%20we%20do%20not%0Arequire%20dataset-dependent%20parameter%20tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07275v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTuning-Free%2520Online%2520Robust%2520Principal%2520Component%2520Analysis%2520through%2520Implicit%250A%2520%2520Regularization%26entry.906535625%3DLakshmi%2520Jayalal%2520and%2520Gokularam%2520Muthukrishnan%2520and%2520Sheetal%2520Kalyani%26entry.1292438233%3D%2520%2520The%2520performance%2520of%2520the%2520standard%2520Online%2520Robust%2520Principal%2520Component%2520Analysis%250A%2528OR-PCA%2529%2520technique%2520depends%2520on%2520the%2520optimum%2520tuning%2520of%2520the%2520explicit%2520regularizers%250Aand%2520this%2520tuning%2520is%2520dataset%2520sensitive.%2520We%2520aim%2520to%2520remove%2520the%2520dependency%2520on%2520these%250Atuning%2520parameters%2520by%2520using%2520implicit%2520regularization.%2520We%2520propose%2520to%2520use%2520the%250Aimplicit%2520regularization%2520effect%2520of%2520various%2520modified%2520gradient%2520descents%2520to%2520make%250AOR-PCA%2520tuning%2520free.%2520Our%2520method%2520incorporates%2520three%2520different%2520versions%2520of%250Amodified%2520gradient%2520descent%2520that%2520separately%2520but%2520naturally%2520encourage%2520sparsity%2520and%250Alow-rank%2520structures%2520in%2520the%2520data.%2520The%2520proposed%2520method%2520performs%2520comparable%2520or%250Abetter%2520than%2520the%2520tuned%2520OR-PCA%2520for%2520both%2520simulated%2520and%2520real-world%2520datasets.%250ATuning-free%2520ORPCA%2520makes%2520it%2520more%2520scalable%2520for%2520large%2520datasets%2520since%2520we%2520do%2520not%250Arequire%2520dataset-dependent%2520parameter%2520tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07275v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tuning-Free%20Online%20Robust%20Principal%20Component%20Analysis%20through%20Implicit%0A%20%20Regularization&entry.906535625=Lakshmi%20Jayalal%20and%20Gokularam%20Muthukrishnan%20and%20Sheetal%20Kalyani&entry.1292438233=%20%20The%20performance%20of%20the%20standard%20Online%20Robust%20Principal%20Component%20Analysis%0A%28OR-PCA%29%20technique%20depends%20on%20the%20optimum%20tuning%20of%20the%20explicit%20regularizers%0Aand%20this%20tuning%20is%20dataset%20sensitive.%20We%20aim%20to%20remove%20the%20dependency%20on%20these%0Atuning%20parameters%20by%20using%20implicit%20regularization.%20We%20propose%20to%20use%20the%0Aimplicit%20regularization%20effect%20of%20various%20modified%20gradient%20descents%20to%20make%0AOR-PCA%20tuning%20free.%20Our%20method%20incorporates%20three%20different%20versions%20of%0Amodified%20gradient%20descent%20that%20separately%20but%20naturally%20encourage%20sparsity%20and%0Alow-rank%20structures%20in%20the%20data.%20The%20proposed%20method%20performs%20comparable%20or%0Abetter%20than%20the%20tuned%20OR-PCA%20for%20both%20simulated%20and%20real-world%20datasets.%0ATuning-free%20ORPCA%20makes%20it%20more%20scalable%20for%20large%20datasets%20since%20we%20do%20not%0Arequire%20dataset-dependent%20parameter%20tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07275v1&entry.124074799=Read"},
{"title": "Heterogeneity-Aware Coordination for Federated Learning via Stitching\n  Pre-trained blocks", "author": "Shichen Zhan and Yebo Wu and Chunlin Tian and Yan Zhao and Li Li", "abstract": "  Federated learning (FL) coordinates multiple devices to collaboratively train\na shared model while preserving data privacy. However, large memory footprint\nand high energy consumption during the training process excludes the low-end\ndevices from contributing to the global model with their own data, which\nseverely deteriorates the model performance in real-world scenarios. In this\npaper, we propose FedStitch, a hierarchical coordination framework for\nheterogeneous federated learning with pre-trained blocks. Unlike the\ntraditional approaches that train the global model from scratch, for a new\ntask, FedStitch composes the global model via stitching pre-trained blocks.\nSpecifically, each participating client selects the most suitable block based\non their local data from the candidate pool composed of blocks from pre-trained\nmodels. The server then aggregates the optimal block for stitching. This\nprocess iterates until a new stitched network is generated. Except for the new\ntraining paradigm, FedStitch consists of the following three core components:\n1) an RL-weighted aggregator, 2) a search space optimizer deployed on the\nserver side, and 3) a local energy optimizer deployed on each participating\nclient. The RL-weighted aggregator helps to select the right block in the\nnon-IID scenario, while the search space optimizer continuously reduces the\nsize of the candidate block pool during stitching. Meanwhile, the local energy\noptimizer is designed to minimize energy consumption of each client while\nguaranteeing the overall training progress. The results demonstrate that\ncompared to existing approaches, FedStitch improves the model accuracy up to\n20.93%. At the same time, it achieves up to 8.12% speedup, reduces the memory\nfootprint up to 79.5%, and achieves 89.41% energy saving at most during the\nlearning procedure.\n", "link": "http://arxiv.org/abs/2409.07202v1", "date": "2024-09-11", "relevancy": 2.3403, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4755}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.465}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4637}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heterogeneity-Aware%20Coordination%20for%20Federated%20Learning%20via%20Stitching%0A%20%20Pre-trained%20blocks&body=Title%3A%20Heterogeneity-Aware%20Coordination%20for%20Federated%20Learning%20via%20Stitching%0A%20%20Pre-trained%20blocks%0AAuthor%3A%20Shichen%20Zhan%20and%20Yebo%20Wu%20and%20Chunlin%20Tian%20and%20Yan%20Zhao%20and%20Li%20Li%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20coordinates%20multiple%20devices%20to%20collaboratively%20train%0Aa%20shared%20model%20while%20preserving%20data%20privacy.%20However%2C%20large%20memory%20footprint%0Aand%20high%20energy%20consumption%20during%20the%20training%20process%20excludes%20the%20low-end%0Adevices%20from%20contributing%20to%20the%20global%20model%20with%20their%20own%20data%2C%20which%0Aseverely%20deteriorates%20the%20model%20performance%20in%20real-world%20scenarios.%20In%20this%0Apaper%2C%20we%20propose%20FedStitch%2C%20a%20hierarchical%20coordination%20framework%20for%0Aheterogeneous%20federated%20learning%20with%20pre-trained%20blocks.%20Unlike%20the%0Atraditional%20approaches%20that%20train%20the%20global%20model%20from%20scratch%2C%20for%20a%20new%0Atask%2C%20FedStitch%20composes%20the%20global%20model%20via%20stitching%20pre-trained%20blocks.%0ASpecifically%2C%20each%20participating%20client%20selects%20the%20most%20suitable%20block%20based%0Aon%20their%20local%20data%20from%20the%20candidate%20pool%20composed%20of%20blocks%20from%20pre-trained%0Amodels.%20The%20server%20then%20aggregates%20the%20optimal%20block%20for%20stitching.%20This%0Aprocess%20iterates%20until%20a%20new%20stitched%20network%20is%20generated.%20Except%20for%20the%20new%0Atraining%20paradigm%2C%20FedStitch%20consists%20of%20the%20following%20three%20core%20components%3A%0A1%29%20an%20RL-weighted%20aggregator%2C%202%29%20a%20search%20space%20optimizer%20deployed%20on%20the%0Aserver%20side%2C%20and%203%29%20a%20local%20energy%20optimizer%20deployed%20on%20each%20participating%0Aclient.%20The%20RL-weighted%20aggregator%20helps%20to%20select%20the%20right%20block%20in%20the%0Anon-IID%20scenario%2C%20while%20the%20search%20space%20optimizer%20continuously%20reduces%20the%0Asize%20of%20the%20candidate%20block%20pool%20during%20stitching.%20Meanwhile%2C%20the%20local%20energy%0Aoptimizer%20is%20designed%20to%20minimize%20energy%20consumption%20of%20each%20client%20while%0Aguaranteeing%20the%20overall%20training%20progress.%20The%20results%20demonstrate%20that%0Acompared%20to%20existing%20approaches%2C%20FedStitch%20improves%20the%20model%20accuracy%20up%20to%0A20.93%25.%20At%20the%20same%20time%2C%20it%20achieves%20up%20to%208.12%25%20speedup%2C%20reduces%20the%20memory%0Afootprint%20up%20to%2079.5%25%2C%20and%20achieves%2089.41%25%20energy%20saving%20at%20most%20during%20the%0Alearning%20procedure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07202v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeterogeneity-Aware%2520Coordination%2520for%2520Federated%2520Learning%2520via%2520Stitching%250A%2520%2520Pre-trained%2520blocks%26entry.906535625%3DShichen%2520Zhan%2520and%2520Yebo%2520Wu%2520and%2520Chunlin%2520Tian%2520and%2520Yan%2520Zhao%2520and%2520Li%2520Li%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520coordinates%2520multiple%2520devices%2520to%2520collaboratively%2520train%250Aa%2520shared%2520model%2520while%2520preserving%2520data%2520privacy.%2520However%252C%2520large%2520memory%2520footprint%250Aand%2520high%2520energy%2520consumption%2520during%2520the%2520training%2520process%2520excludes%2520the%2520low-end%250Adevices%2520from%2520contributing%2520to%2520the%2520global%2520model%2520with%2520their%2520own%2520data%252C%2520which%250Aseverely%2520deteriorates%2520the%2520model%2520performance%2520in%2520real-world%2520scenarios.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520FedStitch%252C%2520a%2520hierarchical%2520coordination%2520framework%2520for%250Aheterogeneous%2520federated%2520learning%2520with%2520pre-trained%2520blocks.%2520Unlike%2520the%250Atraditional%2520approaches%2520that%2520train%2520the%2520global%2520model%2520from%2520scratch%252C%2520for%2520a%2520new%250Atask%252C%2520FedStitch%2520composes%2520the%2520global%2520model%2520via%2520stitching%2520pre-trained%2520blocks.%250ASpecifically%252C%2520each%2520participating%2520client%2520selects%2520the%2520most%2520suitable%2520block%2520based%250Aon%2520their%2520local%2520data%2520from%2520the%2520candidate%2520pool%2520composed%2520of%2520blocks%2520from%2520pre-trained%250Amodels.%2520The%2520server%2520then%2520aggregates%2520the%2520optimal%2520block%2520for%2520stitching.%2520This%250Aprocess%2520iterates%2520until%2520a%2520new%2520stitched%2520network%2520is%2520generated.%2520Except%2520for%2520the%2520new%250Atraining%2520paradigm%252C%2520FedStitch%2520consists%2520of%2520the%2520following%2520three%2520core%2520components%253A%250A1%2529%2520an%2520RL-weighted%2520aggregator%252C%25202%2529%2520a%2520search%2520space%2520optimizer%2520deployed%2520on%2520the%250Aserver%2520side%252C%2520and%25203%2529%2520a%2520local%2520energy%2520optimizer%2520deployed%2520on%2520each%2520participating%250Aclient.%2520The%2520RL-weighted%2520aggregator%2520helps%2520to%2520select%2520the%2520right%2520block%2520in%2520the%250Anon-IID%2520scenario%252C%2520while%2520the%2520search%2520space%2520optimizer%2520continuously%2520reduces%2520the%250Asize%2520of%2520the%2520candidate%2520block%2520pool%2520during%2520stitching.%2520Meanwhile%252C%2520the%2520local%2520energy%250Aoptimizer%2520is%2520designed%2520to%2520minimize%2520energy%2520consumption%2520of%2520each%2520client%2520while%250Aguaranteeing%2520the%2520overall%2520training%2520progress.%2520The%2520results%2520demonstrate%2520that%250Acompared%2520to%2520existing%2520approaches%252C%2520FedStitch%2520improves%2520the%2520model%2520accuracy%2520up%2520to%250A20.93%2525.%2520At%2520the%2520same%2520time%252C%2520it%2520achieves%2520up%2520to%25208.12%2525%2520speedup%252C%2520reduces%2520the%2520memory%250Afootprint%2520up%2520to%252079.5%2525%252C%2520and%2520achieves%252089.41%2525%2520energy%2520saving%2520at%2520most%2520during%2520the%250Alearning%2520procedure.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07202v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heterogeneity-Aware%20Coordination%20for%20Federated%20Learning%20via%20Stitching%0A%20%20Pre-trained%20blocks&entry.906535625=Shichen%20Zhan%20and%20Yebo%20Wu%20and%20Chunlin%20Tian%20and%20Yan%20Zhao%20and%20Li%20Li&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20coordinates%20multiple%20devices%20to%20collaboratively%20train%0Aa%20shared%20model%20while%20preserving%20data%20privacy.%20However%2C%20large%20memory%20footprint%0Aand%20high%20energy%20consumption%20during%20the%20training%20process%20excludes%20the%20low-end%0Adevices%20from%20contributing%20to%20the%20global%20model%20with%20their%20own%20data%2C%20which%0Aseverely%20deteriorates%20the%20model%20performance%20in%20real-world%20scenarios.%20In%20this%0Apaper%2C%20we%20propose%20FedStitch%2C%20a%20hierarchical%20coordination%20framework%20for%0Aheterogeneous%20federated%20learning%20with%20pre-trained%20blocks.%20Unlike%20the%0Atraditional%20approaches%20that%20train%20the%20global%20model%20from%20scratch%2C%20for%20a%20new%0Atask%2C%20FedStitch%20composes%20the%20global%20model%20via%20stitching%20pre-trained%20blocks.%0ASpecifically%2C%20each%20participating%20client%20selects%20the%20most%20suitable%20block%20based%0Aon%20their%20local%20data%20from%20the%20candidate%20pool%20composed%20of%20blocks%20from%20pre-trained%0Amodels.%20The%20server%20then%20aggregates%20the%20optimal%20block%20for%20stitching.%20This%0Aprocess%20iterates%20until%20a%20new%20stitched%20network%20is%20generated.%20Except%20for%20the%20new%0Atraining%20paradigm%2C%20FedStitch%20consists%20of%20the%20following%20three%20core%20components%3A%0A1%29%20an%20RL-weighted%20aggregator%2C%202%29%20a%20search%20space%20optimizer%20deployed%20on%20the%0Aserver%20side%2C%20and%203%29%20a%20local%20energy%20optimizer%20deployed%20on%20each%20participating%0Aclient.%20The%20RL-weighted%20aggregator%20helps%20to%20select%20the%20right%20block%20in%20the%0Anon-IID%20scenario%2C%20while%20the%20search%20space%20optimizer%20continuously%20reduces%20the%0Asize%20of%20the%20candidate%20block%20pool%20during%20stitching.%20Meanwhile%2C%20the%20local%20energy%0Aoptimizer%20is%20designed%20to%20minimize%20energy%20consumption%20of%20each%20client%20while%0Aguaranteeing%20the%20overall%20training%20progress.%20The%20results%20demonstrate%20that%0Acompared%20to%20existing%20approaches%2C%20FedStitch%20improves%20the%20model%20accuracy%20up%20to%0A20.93%25.%20At%20the%20same%20time%2C%20it%20achieves%20up%20to%208.12%25%20speedup%2C%20reduces%20the%20memory%0Afootprint%20up%20to%2079.5%25%2C%20and%20achieves%2089.41%25%20energy%20saving%20at%20most%20during%20the%0Alearning%20procedure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07202v1&entry.124074799=Read"},
{"title": "HIMO: A New Benchmark for Full-Body Human Interacting with Multiple\n  Objects", "author": "Xintao Lv and Liang Xu and Yichao Yan and Xin Jin and Congsheng Xu and Shuwen Wu and Yifan Liu and Lincheng Li and Mengxiao Bi and Wenjun Zeng and Xiaokang Yang", "abstract": "  Generating human-object interactions (HOIs) is critical with the tremendous\nadvances of digital avatars. Existing datasets are typically limited to humans\ninteracting with a single object while neglecting the ubiquitous manipulation\nof multiple objects. Thus, we propose HIMO, a large-scale MoCap dataset of\nfull-body human interacting with multiple objects, containing 3.3K 4D HOI\nsequences and 4.08M 3D HOI frames. We also annotate HIMO with detailed textual\ndescriptions and temporal segments, benchmarking two novel tasks of HOI\nsynthesis conditioned on either the whole text prompt or the segmented text\nprompts as fine-grained timeline control. To address these novel tasks, we\npropose a dual-branch conditional diffusion model with a mutual interaction\nmodule for HOI synthesis. Besides, an auto-regressive generation pipeline is\nalso designed to obtain smooth transitions between HOI segments. Experimental\nresults demonstrate the generalization ability to unseen object geometries and\ntemporal compositions.\n", "link": "http://arxiv.org/abs/2407.12371v2", "date": "2024-09-11", "relevancy": 2.3382, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6244}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5599}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HIMO%3A%20A%20New%20Benchmark%20for%20Full-Body%20Human%20Interacting%20with%20Multiple%0A%20%20Objects&body=Title%3A%20HIMO%3A%20A%20New%20Benchmark%20for%20Full-Body%20Human%20Interacting%20with%20Multiple%0A%20%20Objects%0AAuthor%3A%20Xintao%20Lv%20and%20Liang%20Xu%20and%20Yichao%20Yan%20and%20Xin%20Jin%20and%20Congsheng%20Xu%20and%20Shuwen%20Wu%20and%20Yifan%20Liu%20and%20Lincheng%20Li%20and%20Mengxiao%20Bi%20and%20Wenjun%20Zeng%20and%20Xiaokang%20Yang%0AAbstract%3A%20%20%20Generating%20human-object%20interactions%20%28HOIs%29%20is%20critical%20with%20the%20tremendous%0Aadvances%20of%20digital%20avatars.%20Existing%20datasets%20are%20typically%20limited%20to%20humans%0Ainteracting%20with%20a%20single%20object%20while%20neglecting%20the%20ubiquitous%20manipulation%0Aof%20multiple%20objects.%20Thus%2C%20we%20propose%20HIMO%2C%20a%20large-scale%20MoCap%20dataset%20of%0Afull-body%20human%20interacting%20with%20multiple%20objects%2C%20containing%203.3K%204D%20HOI%0Asequences%20and%204.08M%203D%20HOI%20frames.%20We%20also%20annotate%20HIMO%20with%20detailed%20textual%0Adescriptions%20and%20temporal%20segments%2C%20benchmarking%20two%20novel%20tasks%20of%20HOI%0Asynthesis%20conditioned%20on%20either%20the%20whole%20text%20prompt%20or%20the%20segmented%20text%0Aprompts%20as%20fine-grained%20timeline%20control.%20To%20address%20these%20novel%20tasks%2C%20we%0Apropose%20a%20dual-branch%20conditional%20diffusion%20model%20with%20a%20mutual%20interaction%0Amodule%20for%20HOI%20synthesis.%20Besides%2C%20an%20auto-regressive%20generation%20pipeline%20is%0Aalso%20designed%20to%20obtain%20smooth%20transitions%20between%20HOI%20segments.%20Experimental%0Aresults%20demonstrate%20the%20generalization%20ability%20to%20unseen%20object%20geometries%20and%0Atemporal%20compositions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12371v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHIMO%253A%2520A%2520New%2520Benchmark%2520for%2520Full-Body%2520Human%2520Interacting%2520with%2520Multiple%250A%2520%2520Objects%26entry.906535625%3DXintao%2520Lv%2520and%2520Liang%2520Xu%2520and%2520Yichao%2520Yan%2520and%2520Xin%2520Jin%2520and%2520Congsheng%2520Xu%2520and%2520Shuwen%2520Wu%2520and%2520Yifan%2520Liu%2520and%2520Lincheng%2520Li%2520and%2520Mengxiao%2520Bi%2520and%2520Wenjun%2520Zeng%2520and%2520Xiaokang%2520Yang%26entry.1292438233%3D%2520%2520Generating%2520human-object%2520interactions%2520%2528HOIs%2529%2520is%2520critical%2520with%2520the%2520tremendous%250Aadvances%2520of%2520digital%2520avatars.%2520Existing%2520datasets%2520are%2520typically%2520limited%2520to%2520humans%250Ainteracting%2520with%2520a%2520single%2520object%2520while%2520neglecting%2520the%2520ubiquitous%2520manipulation%250Aof%2520multiple%2520objects.%2520Thus%252C%2520we%2520propose%2520HIMO%252C%2520a%2520large-scale%2520MoCap%2520dataset%2520of%250Afull-body%2520human%2520interacting%2520with%2520multiple%2520objects%252C%2520containing%25203.3K%25204D%2520HOI%250Asequences%2520and%25204.08M%25203D%2520HOI%2520frames.%2520We%2520also%2520annotate%2520HIMO%2520with%2520detailed%2520textual%250Adescriptions%2520and%2520temporal%2520segments%252C%2520benchmarking%2520two%2520novel%2520tasks%2520of%2520HOI%250Asynthesis%2520conditioned%2520on%2520either%2520the%2520whole%2520text%2520prompt%2520or%2520the%2520segmented%2520text%250Aprompts%2520as%2520fine-grained%2520timeline%2520control.%2520To%2520address%2520these%2520novel%2520tasks%252C%2520we%250Apropose%2520a%2520dual-branch%2520conditional%2520diffusion%2520model%2520with%2520a%2520mutual%2520interaction%250Amodule%2520for%2520HOI%2520synthesis.%2520Besides%252C%2520an%2520auto-regressive%2520generation%2520pipeline%2520is%250Aalso%2520designed%2520to%2520obtain%2520smooth%2520transitions%2520between%2520HOI%2520segments.%2520Experimental%250Aresults%2520demonstrate%2520the%2520generalization%2520ability%2520to%2520unseen%2520object%2520geometries%2520and%250Atemporal%2520compositions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12371v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HIMO%3A%20A%20New%20Benchmark%20for%20Full-Body%20Human%20Interacting%20with%20Multiple%0A%20%20Objects&entry.906535625=Xintao%20Lv%20and%20Liang%20Xu%20and%20Yichao%20Yan%20and%20Xin%20Jin%20and%20Congsheng%20Xu%20and%20Shuwen%20Wu%20and%20Yifan%20Liu%20and%20Lincheng%20Li%20and%20Mengxiao%20Bi%20and%20Wenjun%20Zeng%20and%20Xiaokang%20Yang&entry.1292438233=%20%20Generating%20human-object%20interactions%20%28HOIs%29%20is%20critical%20with%20the%20tremendous%0Aadvances%20of%20digital%20avatars.%20Existing%20datasets%20are%20typically%20limited%20to%20humans%0Ainteracting%20with%20a%20single%20object%20while%20neglecting%20the%20ubiquitous%20manipulation%0Aof%20multiple%20objects.%20Thus%2C%20we%20propose%20HIMO%2C%20a%20large-scale%20MoCap%20dataset%20of%0Afull-body%20human%20interacting%20with%20multiple%20objects%2C%20containing%203.3K%204D%20HOI%0Asequences%20and%204.08M%203D%20HOI%20frames.%20We%20also%20annotate%20HIMO%20with%20detailed%20textual%0Adescriptions%20and%20temporal%20segments%2C%20benchmarking%20two%20novel%20tasks%20of%20HOI%0Asynthesis%20conditioned%20on%20either%20the%20whole%20text%20prompt%20or%20the%20segmented%20text%0Aprompts%20as%20fine-grained%20timeline%20control.%20To%20address%20these%20novel%20tasks%2C%20we%0Apropose%20a%20dual-branch%20conditional%20diffusion%20model%20with%20a%20mutual%20interaction%0Amodule%20for%20HOI%20synthesis.%20Besides%2C%20an%20auto-regressive%20generation%20pipeline%20is%0Aalso%20designed%20to%20obtain%20smooth%20transitions%20between%20HOI%20segments.%20Experimental%0Aresults%20demonstrate%20the%20generalization%20ability%20to%20unseen%20object%20geometries%20and%0Atemporal%20compositions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12371v2&entry.124074799=Read"},
{"title": "Efficient One-Step Diffusion Refinement for Snapshot Compressive Imaging", "author": "Yunzhen Wang and Haijin Zeng and Shaoguang Huang and Hongyu Chen and Hongyan Zhang", "abstract": "  Coded Aperture Snapshot Spectral Imaging (CASSI) is a crucial technique for\ncapturing three-dimensional multispectral images (MSIs) through the complex\ninverse task of reconstructing these images from coded two-dimensional\nmeasurements. Current state-of-the-art methods, predominantly end-to-end, face\nlimitations in reconstructing high-frequency details and often rely on\nconstrained datasets like KAIST and CAVE, resulting in models with poor\ngeneralizability. In response to these challenges, this paper introduces a\nnovel one-step Diffusion Probabilistic Model within a self-supervised\nadaptation framework for Snapshot Compressive Imaging (SCI). Our approach\nleverages a pretrained SCI reconstruction network to generate initial\npredictions from two-dimensional measurements. Subsequently, a one-step\ndiffusion model produces high-frequency residuals to enhance these initial\npredictions. Additionally, acknowledging the high costs associated with\ncollecting MSIs, we develop a self-supervised paradigm based on the Equivariant\nImaging (EI) framework. Experimental results validate the superiority of our\nmodel compared to previous methods, showcasing its simplicity and adaptability\nto various end-to-end or unfolding techniques.\n", "link": "http://arxiv.org/abs/2409.07417v1", "date": "2024-09-11", "relevancy": 2.3233, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6271}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5716}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20One-Step%20Diffusion%20Refinement%20for%20Snapshot%20Compressive%20Imaging&body=Title%3A%20Efficient%20One-Step%20Diffusion%20Refinement%20for%20Snapshot%20Compressive%20Imaging%0AAuthor%3A%20Yunzhen%20Wang%20and%20Haijin%20Zeng%20and%20Shaoguang%20Huang%20and%20Hongyu%20Chen%20and%20Hongyan%20Zhang%0AAbstract%3A%20%20%20Coded%20Aperture%20Snapshot%20Spectral%20Imaging%20%28CASSI%29%20is%20a%20crucial%20technique%20for%0Acapturing%20three-dimensional%20multispectral%20images%20%28MSIs%29%20through%20the%20complex%0Ainverse%20task%20of%20reconstructing%20these%20images%20from%20coded%20two-dimensional%0Ameasurements.%20Current%20state-of-the-art%20methods%2C%20predominantly%20end-to-end%2C%20face%0Alimitations%20in%20reconstructing%20high-frequency%20details%20and%20often%20rely%20on%0Aconstrained%20datasets%20like%20KAIST%20and%20CAVE%2C%20resulting%20in%20models%20with%20poor%0Ageneralizability.%20In%20response%20to%20these%20challenges%2C%20this%20paper%20introduces%20a%0Anovel%20one-step%20Diffusion%20Probabilistic%20Model%20within%20a%20self-supervised%0Aadaptation%20framework%20for%20Snapshot%20Compressive%20Imaging%20%28SCI%29.%20Our%20approach%0Aleverages%20a%20pretrained%20SCI%20reconstruction%20network%20to%20generate%20initial%0Apredictions%20from%20two-dimensional%20measurements.%20Subsequently%2C%20a%20one-step%0Adiffusion%20model%20produces%20high-frequency%20residuals%20to%20enhance%20these%20initial%0Apredictions.%20Additionally%2C%20acknowledging%20the%20high%20costs%20associated%20with%0Acollecting%20MSIs%2C%20we%20develop%20a%20self-supervised%20paradigm%20based%20on%20the%20Equivariant%0AImaging%20%28EI%29%20framework.%20Experimental%20results%20validate%20the%20superiority%20of%20our%0Amodel%20compared%20to%20previous%20methods%2C%20showcasing%20its%20simplicity%20and%20adaptability%0Ato%20various%20end-to-end%20or%20unfolding%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07417v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520One-Step%2520Diffusion%2520Refinement%2520for%2520Snapshot%2520Compressive%2520Imaging%26entry.906535625%3DYunzhen%2520Wang%2520and%2520Haijin%2520Zeng%2520and%2520Shaoguang%2520Huang%2520and%2520Hongyu%2520Chen%2520and%2520Hongyan%2520Zhang%26entry.1292438233%3D%2520%2520Coded%2520Aperture%2520Snapshot%2520Spectral%2520Imaging%2520%2528CASSI%2529%2520is%2520a%2520crucial%2520technique%2520for%250Acapturing%2520three-dimensional%2520multispectral%2520images%2520%2528MSIs%2529%2520through%2520the%2520complex%250Ainverse%2520task%2520of%2520reconstructing%2520these%2520images%2520from%2520coded%2520two-dimensional%250Ameasurements.%2520Current%2520state-of-the-art%2520methods%252C%2520predominantly%2520end-to-end%252C%2520face%250Alimitations%2520in%2520reconstructing%2520high-frequency%2520details%2520and%2520often%2520rely%2520on%250Aconstrained%2520datasets%2520like%2520KAIST%2520and%2520CAVE%252C%2520resulting%2520in%2520models%2520with%2520poor%250Ageneralizability.%2520In%2520response%2520to%2520these%2520challenges%252C%2520this%2520paper%2520introduces%2520a%250Anovel%2520one-step%2520Diffusion%2520Probabilistic%2520Model%2520within%2520a%2520self-supervised%250Aadaptation%2520framework%2520for%2520Snapshot%2520Compressive%2520Imaging%2520%2528SCI%2529.%2520Our%2520approach%250Aleverages%2520a%2520pretrained%2520SCI%2520reconstruction%2520network%2520to%2520generate%2520initial%250Apredictions%2520from%2520two-dimensional%2520measurements.%2520Subsequently%252C%2520a%2520one-step%250Adiffusion%2520model%2520produces%2520high-frequency%2520residuals%2520to%2520enhance%2520these%2520initial%250Apredictions.%2520Additionally%252C%2520acknowledging%2520the%2520high%2520costs%2520associated%2520with%250Acollecting%2520MSIs%252C%2520we%2520develop%2520a%2520self-supervised%2520paradigm%2520based%2520on%2520the%2520Equivariant%250AImaging%2520%2528EI%2529%2520framework.%2520Experimental%2520results%2520validate%2520the%2520superiority%2520of%2520our%250Amodel%2520compared%2520to%2520previous%2520methods%252C%2520showcasing%2520its%2520simplicity%2520and%2520adaptability%250Ato%2520various%2520end-to-end%2520or%2520unfolding%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07417v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20One-Step%20Diffusion%20Refinement%20for%20Snapshot%20Compressive%20Imaging&entry.906535625=Yunzhen%20Wang%20and%20Haijin%20Zeng%20and%20Shaoguang%20Huang%20and%20Hongyu%20Chen%20and%20Hongyan%20Zhang&entry.1292438233=%20%20Coded%20Aperture%20Snapshot%20Spectral%20Imaging%20%28CASSI%29%20is%20a%20crucial%20technique%20for%0Acapturing%20three-dimensional%20multispectral%20images%20%28MSIs%29%20through%20the%20complex%0Ainverse%20task%20of%20reconstructing%20these%20images%20from%20coded%20two-dimensional%0Ameasurements.%20Current%20state-of-the-art%20methods%2C%20predominantly%20end-to-end%2C%20face%0Alimitations%20in%20reconstructing%20high-frequency%20details%20and%20often%20rely%20on%0Aconstrained%20datasets%20like%20KAIST%20and%20CAVE%2C%20resulting%20in%20models%20with%20poor%0Ageneralizability.%20In%20response%20to%20these%20challenges%2C%20this%20paper%20introduces%20a%0Anovel%20one-step%20Diffusion%20Probabilistic%20Model%20within%20a%20self-supervised%0Aadaptation%20framework%20for%20Snapshot%20Compressive%20Imaging%20%28SCI%29.%20Our%20approach%0Aleverages%20a%20pretrained%20SCI%20reconstruction%20network%20to%20generate%20initial%0Apredictions%20from%20two-dimensional%20measurements.%20Subsequently%2C%20a%20one-step%0Adiffusion%20model%20produces%20high-frequency%20residuals%20to%20enhance%20these%20initial%0Apredictions.%20Additionally%2C%20acknowledging%20the%20high%20costs%20associated%20with%0Acollecting%20MSIs%2C%20we%20develop%20a%20self-supervised%20paradigm%20based%20on%20the%20Equivariant%0AImaging%20%28EI%29%20framework.%20Experimental%20results%20validate%20the%20superiority%20of%20our%0Amodel%20compared%20to%20previous%20methods%2C%20showcasing%20its%20simplicity%20and%20adaptability%0Ato%20various%20end-to-end%20or%20unfolding%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07417v1&entry.124074799=Read"},
{"title": "BeNeRF: Neural Radiance Fields from a Single Blurry Image and Event\n  Stream", "author": "Wenpu Li and Pian Wan and Peng Wang and Jinghang Li and Yi Zhou and Peidong Liu", "abstract": "  Neural implicit representation of visual scenes has attracted a lot of\nattention in recent research of computer vision and graphics. Most prior\nmethods focus on how to reconstruct 3D scene representation from a set of\nimages. In this work, we demonstrate the possibility to recover the neural\nradiance fields (NeRF) from a single blurry image and its corresponding event\nstream. We model the camera motion with a cubic B-Spline in SE(3) space. Both\nthe blurry image and the brightness change within a time interval, can then be\nsynthesized from the 3D scene representation given the 6-DoF poses interpolated\nfrom the cubic B-Spline. Our method can jointly learn both the implicit neural\nscene representation and recover the camera motion by minimizing the\ndifferences between the synthesized data and the real measurements without\npre-computed camera poses from COLMAP. We evaluate the proposed method with\nboth synthetic and real datasets. The experimental results demonstrate that we\nare able to render view-consistent latent sharp images from the learned NeRF\nand bring a blurry image alive in high quality. Code and data are available at\nhttps://github.com/wu-cvgl/BeNeRF.\n", "link": "http://arxiv.org/abs/2407.02174v3", "date": "2024-09-11", "relevancy": 2.3166, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5818}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5784}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BeNeRF%3A%20Neural%20Radiance%20Fields%20from%20a%20Single%20Blurry%20Image%20and%20Event%0A%20%20Stream&body=Title%3A%20BeNeRF%3A%20Neural%20Radiance%20Fields%20from%20a%20Single%20Blurry%20Image%20and%20Event%0A%20%20Stream%0AAuthor%3A%20Wenpu%20Li%20and%20Pian%20Wan%20and%20Peng%20Wang%20and%20Jinghang%20Li%20and%20Yi%20Zhou%20and%20Peidong%20Liu%0AAbstract%3A%20%20%20Neural%20implicit%20representation%20of%20visual%20scenes%20has%20attracted%20a%20lot%20of%0Aattention%20in%20recent%20research%20of%20computer%20vision%20and%20graphics.%20Most%20prior%0Amethods%20focus%20on%20how%20to%20reconstruct%203D%20scene%20representation%20from%20a%20set%20of%0Aimages.%20In%20this%20work%2C%20we%20demonstrate%20the%20possibility%20to%20recover%20the%20neural%0Aradiance%20fields%20%28NeRF%29%20from%20a%20single%20blurry%20image%20and%20its%20corresponding%20event%0Astream.%20We%20model%20the%20camera%20motion%20with%20a%20cubic%20B-Spline%20in%20SE%283%29%20space.%20Both%0Athe%20blurry%20image%20and%20the%20brightness%20change%20within%20a%20time%20interval%2C%20can%20then%20be%0Asynthesized%20from%20the%203D%20scene%20representation%20given%20the%206-DoF%20poses%20interpolated%0Afrom%20the%20cubic%20B-Spline.%20Our%20method%20can%20jointly%20learn%20both%20the%20implicit%20neural%0Ascene%20representation%20and%20recover%20the%20camera%20motion%20by%20minimizing%20the%0Adifferences%20between%20the%20synthesized%20data%20and%20the%20real%20measurements%20without%0Apre-computed%20camera%20poses%20from%20COLMAP.%20We%20evaluate%20the%20proposed%20method%20with%0Aboth%20synthetic%20and%20real%20datasets.%20The%20experimental%20results%20demonstrate%20that%20we%0Aare%20able%20to%20render%20view-consistent%20latent%20sharp%20images%20from%20the%20learned%20NeRF%0Aand%20bring%20a%20blurry%20image%20alive%20in%20high%20quality.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/wu-cvgl/BeNeRF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02174v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeNeRF%253A%2520Neural%2520Radiance%2520Fields%2520from%2520a%2520Single%2520Blurry%2520Image%2520and%2520Event%250A%2520%2520Stream%26entry.906535625%3DWenpu%2520Li%2520and%2520Pian%2520Wan%2520and%2520Peng%2520Wang%2520and%2520Jinghang%2520Li%2520and%2520Yi%2520Zhou%2520and%2520Peidong%2520Liu%26entry.1292438233%3D%2520%2520Neural%2520implicit%2520representation%2520of%2520visual%2520scenes%2520has%2520attracted%2520a%2520lot%2520of%250Aattention%2520in%2520recent%2520research%2520of%2520computer%2520vision%2520and%2520graphics.%2520Most%2520prior%250Amethods%2520focus%2520on%2520how%2520to%2520reconstruct%25203D%2520scene%2520representation%2520from%2520a%2520set%2520of%250Aimages.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520the%2520possibility%2520to%2520recover%2520the%2520neural%250Aradiance%2520fields%2520%2528NeRF%2529%2520from%2520a%2520single%2520blurry%2520image%2520and%2520its%2520corresponding%2520event%250Astream.%2520We%2520model%2520the%2520camera%2520motion%2520with%2520a%2520cubic%2520B-Spline%2520in%2520SE%25283%2529%2520space.%2520Both%250Athe%2520blurry%2520image%2520and%2520the%2520brightness%2520change%2520within%2520a%2520time%2520interval%252C%2520can%2520then%2520be%250Asynthesized%2520from%2520the%25203D%2520scene%2520representation%2520given%2520the%25206-DoF%2520poses%2520interpolated%250Afrom%2520the%2520cubic%2520B-Spline.%2520Our%2520method%2520can%2520jointly%2520learn%2520both%2520the%2520implicit%2520neural%250Ascene%2520representation%2520and%2520recover%2520the%2520camera%2520motion%2520by%2520minimizing%2520the%250Adifferences%2520between%2520the%2520synthesized%2520data%2520and%2520the%2520real%2520measurements%2520without%250Apre-computed%2520camera%2520poses%2520from%2520COLMAP.%2520We%2520evaluate%2520the%2520proposed%2520method%2520with%250Aboth%2520synthetic%2520and%2520real%2520datasets.%2520The%2520experimental%2520results%2520demonstrate%2520that%2520we%250Aare%2520able%2520to%2520render%2520view-consistent%2520latent%2520sharp%2520images%2520from%2520the%2520learned%2520NeRF%250Aand%2520bring%2520a%2520blurry%2520image%2520alive%2520in%2520high%2520quality.%2520Code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/wu-cvgl/BeNeRF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02174v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BeNeRF%3A%20Neural%20Radiance%20Fields%20from%20a%20Single%20Blurry%20Image%20and%20Event%0A%20%20Stream&entry.906535625=Wenpu%20Li%20and%20Pian%20Wan%20and%20Peng%20Wang%20and%20Jinghang%20Li%20and%20Yi%20Zhou%20and%20Peidong%20Liu&entry.1292438233=%20%20Neural%20implicit%20representation%20of%20visual%20scenes%20has%20attracted%20a%20lot%20of%0Aattention%20in%20recent%20research%20of%20computer%20vision%20and%20graphics.%20Most%20prior%0Amethods%20focus%20on%20how%20to%20reconstruct%203D%20scene%20representation%20from%20a%20set%20of%0Aimages.%20In%20this%20work%2C%20we%20demonstrate%20the%20possibility%20to%20recover%20the%20neural%0Aradiance%20fields%20%28NeRF%29%20from%20a%20single%20blurry%20image%20and%20its%20corresponding%20event%0Astream.%20We%20model%20the%20camera%20motion%20with%20a%20cubic%20B-Spline%20in%20SE%283%29%20space.%20Both%0Athe%20blurry%20image%20and%20the%20brightness%20change%20within%20a%20time%20interval%2C%20can%20then%20be%0Asynthesized%20from%20the%203D%20scene%20representation%20given%20the%206-DoF%20poses%20interpolated%0Afrom%20the%20cubic%20B-Spline.%20Our%20method%20can%20jointly%20learn%20both%20the%20implicit%20neural%0Ascene%20representation%20and%20recover%20the%20camera%20motion%20by%20minimizing%20the%0Adifferences%20between%20the%20synthesized%20data%20and%20the%20real%20measurements%20without%0Apre-computed%20camera%20poses%20from%20COLMAP.%20We%20evaluate%20the%20proposed%20method%20with%0Aboth%20synthetic%20and%20real%20datasets.%20The%20experimental%20results%20demonstrate%20that%20we%0Aare%20able%20to%20render%20view-consistent%20latent%20sharp%20images%20from%20the%20learned%20NeRF%0Aand%20bring%20a%20blurry%20image%20alive%20in%20high%20quality.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/wu-cvgl/BeNeRF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02174v3&entry.124074799=Read"},
{"title": "CLNX: Bridging Code and Natural Language for C/C++\n  Vulnerability-Contributing Commits Identification", "author": "Zeqing Qin and Yiwei Wu and Lansheng Han", "abstract": "  Large Language Models (LLMs) have shown great promise in vulnerability\nidentification. As C/C++ comprises half of the Open-Source Software (OSS)\nvulnerabilities over the past decade and updates in OSS mainly occur through\ncommits, enhancing LLMs' ability to identify C/C++ Vulnerability-Contributing\nCommits (VCCs) is essential. However, current studies primarily focus on\nfurther pre-training LLMs on massive code datasets, which is resource-intensive\nand poses efficiency challenges. In this paper, we enhance the ability of\nBERT-based LLMs to identify C/C++ VCCs in a lightweight manner. We propose\nCodeLinguaNexus (CLNX) as a bridge facilitating communication between C/C++\nprograms and LLMs. Based on commits, CLNX efficiently converts the source code\ninto a more natural representation while preserving key details. Specifically,\nCLNX first applies structure-level naturalization to decompose complex\nprograms, followed by token-level naturalization to interpret complex symbols.\nWe evaluate CLNX on public datasets of 25,872 C/C++ functions with their\ncommits. The results show that CLNX significantly enhances the performance of\nLLMs on identifying C/C++ VCCs. Moreover, CLNX-equipped CodeBERT achieves new\nstate-of-the-art and identifies 38 OSS vulnerabilities in the real world.\n", "link": "http://arxiv.org/abs/2409.07407v1", "date": "2024-09-11", "relevancy": 2.2999, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4815}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4815}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLNX%3A%20Bridging%20Code%20and%20Natural%20Language%20for%20C/C%2B%2B%0A%20%20Vulnerability-Contributing%20Commits%20Identification&body=Title%3A%20CLNX%3A%20Bridging%20Code%20and%20Natural%20Language%20for%20C/C%2B%2B%0A%20%20Vulnerability-Contributing%20Commits%20Identification%0AAuthor%3A%20Zeqing%20Qin%20and%20Yiwei%20Wu%20and%20Lansheng%20Han%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20great%20promise%20in%20vulnerability%0Aidentification.%20As%20C/C%2B%2B%20comprises%20half%20of%20the%20Open-Source%20Software%20%28OSS%29%0Avulnerabilities%20over%20the%20past%20decade%20and%20updates%20in%20OSS%20mainly%20occur%20through%0Acommits%2C%20enhancing%20LLMs%27%20ability%20to%20identify%20C/C%2B%2B%20Vulnerability-Contributing%0ACommits%20%28VCCs%29%20is%20essential.%20However%2C%20current%20studies%20primarily%20focus%20on%0Afurther%20pre-training%20LLMs%20on%20massive%20code%20datasets%2C%20which%20is%20resource-intensive%0Aand%20poses%20efficiency%20challenges.%20In%20this%20paper%2C%20we%20enhance%20the%20ability%20of%0ABERT-based%20LLMs%20to%20identify%20C/C%2B%2B%20VCCs%20in%20a%20lightweight%20manner.%20We%20propose%0ACodeLinguaNexus%20%28CLNX%29%20as%20a%20bridge%20facilitating%20communication%20between%20C/C%2B%2B%0Aprograms%20and%20LLMs.%20Based%20on%20commits%2C%20CLNX%20efficiently%20converts%20the%20source%20code%0Ainto%20a%20more%20natural%20representation%20while%20preserving%20key%20details.%20Specifically%2C%0ACLNX%20first%20applies%20structure-level%20naturalization%20to%20decompose%20complex%0Aprograms%2C%20followed%20by%20token-level%20naturalization%20to%20interpret%20complex%20symbols.%0AWe%20evaluate%20CLNX%20on%20public%20datasets%20of%2025%2C872%20C/C%2B%2B%20functions%20with%20their%0Acommits.%20The%20results%20show%20that%20CLNX%20significantly%20enhances%20the%20performance%20of%0ALLMs%20on%20identifying%20C/C%2B%2B%20VCCs.%20Moreover%2C%20CLNX-equipped%20CodeBERT%20achieves%20new%0Astate-of-the-art%20and%20identifies%2038%20OSS%20vulnerabilities%20in%20the%20real%20world.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07407v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLNX%253A%2520Bridging%2520Code%2520and%2520Natural%2520Language%2520for%2520C/C%252B%252B%250A%2520%2520Vulnerability-Contributing%2520Commits%2520Identification%26entry.906535625%3DZeqing%2520Qin%2520and%2520Yiwei%2520Wu%2520and%2520Lansheng%2520Han%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520great%2520promise%2520in%2520vulnerability%250Aidentification.%2520As%2520C/C%252B%252B%2520comprises%2520half%2520of%2520the%2520Open-Source%2520Software%2520%2528OSS%2529%250Avulnerabilities%2520over%2520the%2520past%2520decade%2520and%2520updates%2520in%2520OSS%2520mainly%2520occur%2520through%250Acommits%252C%2520enhancing%2520LLMs%2527%2520ability%2520to%2520identify%2520C/C%252B%252B%2520Vulnerability-Contributing%250ACommits%2520%2528VCCs%2529%2520is%2520essential.%2520However%252C%2520current%2520studies%2520primarily%2520focus%2520on%250Afurther%2520pre-training%2520LLMs%2520on%2520massive%2520code%2520datasets%252C%2520which%2520is%2520resource-intensive%250Aand%2520poses%2520efficiency%2520challenges.%2520In%2520this%2520paper%252C%2520we%2520enhance%2520the%2520ability%2520of%250ABERT-based%2520LLMs%2520to%2520identify%2520C/C%252B%252B%2520VCCs%2520in%2520a%2520lightweight%2520manner.%2520We%2520propose%250ACodeLinguaNexus%2520%2528CLNX%2529%2520as%2520a%2520bridge%2520facilitating%2520communication%2520between%2520C/C%252B%252B%250Aprograms%2520and%2520LLMs.%2520Based%2520on%2520commits%252C%2520CLNX%2520efficiently%2520converts%2520the%2520source%2520code%250Ainto%2520a%2520more%2520natural%2520representation%2520while%2520preserving%2520key%2520details.%2520Specifically%252C%250ACLNX%2520first%2520applies%2520structure-level%2520naturalization%2520to%2520decompose%2520complex%250Aprograms%252C%2520followed%2520by%2520token-level%2520naturalization%2520to%2520interpret%2520complex%2520symbols.%250AWe%2520evaluate%2520CLNX%2520on%2520public%2520datasets%2520of%252025%252C872%2520C/C%252B%252B%2520functions%2520with%2520their%250Acommits.%2520The%2520results%2520show%2520that%2520CLNX%2520significantly%2520enhances%2520the%2520performance%2520of%250ALLMs%2520on%2520identifying%2520C/C%252B%252B%2520VCCs.%2520Moreover%252C%2520CLNX-equipped%2520CodeBERT%2520achieves%2520new%250Astate-of-the-art%2520and%2520identifies%252038%2520OSS%2520vulnerabilities%2520in%2520the%2520real%2520world.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07407v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLNX%3A%20Bridging%20Code%20and%20Natural%20Language%20for%20C/C%2B%2B%0A%20%20Vulnerability-Contributing%20Commits%20Identification&entry.906535625=Zeqing%20Qin%20and%20Yiwei%20Wu%20and%20Lansheng%20Han&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20great%20promise%20in%20vulnerability%0Aidentification.%20As%20C/C%2B%2B%20comprises%20half%20of%20the%20Open-Source%20Software%20%28OSS%29%0Avulnerabilities%20over%20the%20past%20decade%20and%20updates%20in%20OSS%20mainly%20occur%20through%0Acommits%2C%20enhancing%20LLMs%27%20ability%20to%20identify%20C/C%2B%2B%20Vulnerability-Contributing%0ACommits%20%28VCCs%29%20is%20essential.%20However%2C%20current%20studies%20primarily%20focus%20on%0Afurther%20pre-training%20LLMs%20on%20massive%20code%20datasets%2C%20which%20is%20resource-intensive%0Aand%20poses%20efficiency%20challenges.%20In%20this%20paper%2C%20we%20enhance%20the%20ability%20of%0ABERT-based%20LLMs%20to%20identify%20C/C%2B%2B%20VCCs%20in%20a%20lightweight%20manner.%20We%20propose%0ACodeLinguaNexus%20%28CLNX%29%20as%20a%20bridge%20facilitating%20communication%20between%20C/C%2B%2B%0Aprograms%20and%20LLMs.%20Based%20on%20commits%2C%20CLNX%20efficiently%20converts%20the%20source%20code%0Ainto%20a%20more%20natural%20representation%20while%20preserving%20key%20details.%20Specifically%2C%0ACLNX%20first%20applies%20structure-level%20naturalization%20to%20decompose%20complex%0Aprograms%2C%20followed%20by%20token-level%20naturalization%20to%20interpret%20complex%20symbols.%0AWe%20evaluate%20CLNX%20on%20public%20datasets%20of%2025%2C872%20C/C%2B%2B%20functions%20with%20their%0Acommits.%20The%20results%20show%20that%20CLNX%20significantly%20enhances%20the%20performance%20of%0ALLMs%20on%20identifying%20C/C%2B%2B%20VCCs.%20Moreover%2C%20CLNX-equipped%20CodeBERT%20achieves%20new%0Astate-of-the-art%20and%20identifies%2038%20OSS%20vulnerabilities%20in%20the%20real%20world.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07407v1&entry.124074799=Read"},
{"title": "Explicit Mutual Information Maximization for Self-Supervised Learning", "author": "Lele Chang and Peilin Liu and Qinghai Guo and Fei Wen", "abstract": "  Recently, self-supervised learning (SSL) has been extensively studied.\nTheoretically, mutual information maximization (MIM) is an optimal criterion\nfor SSL, with a strong theoretical foundation in information theory. However,\nit is difficult to directly apply MIM in SSL since the data distribution is not\nanalytically available in applications. In practice, many existing methods can\nbe viewed as approximate implementations of the MIM criterion. This work shows\nthat, based on the invariance property of MI, explicit MI maximization can be\napplied to SSL under a generic distribution assumption, i.e., a relaxed\ncondition of the data distribution. We further illustrate this by analyzing the\ngeneralized Gaussian distribution. Based on this result, we derive a loss\nfunction based on the MIM criterion using only second-order statistics. We\nimplement the new loss for SSL and demonstrate its effectiveness via extensive\nexperiments.\n", "link": "http://arxiv.org/abs/2409.04747v2", "date": "2024-09-11", "relevancy": 2.2999, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4753}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4546}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.45}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explicit%20Mutual%20Information%20Maximization%20for%20Self-Supervised%20Learning&body=Title%3A%20Explicit%20Mutual%20Information%20Maximization%20for%20Self-Supervised%20Learning%0AAuthor%3A%20Lele%20Chang%20and%20Peilin%20Liu%20and%20Qinghai%20Guo%20and%20Fei%20Wen%0AAbstract%3A%20%20%20Recently%2C%20self-supervised%20learning%20%28SSL%29%20has%20been%20extensively%20studied.%0ATheoretically%2C%20mutual%20information%20maximization%20%28MIM%29%20is%20an%20optimal%20criterion%0Afor%20SSL%2C%20with%20a%20strong%20theoretical%20foundation%20in%20information%20theory.%20However%2C%0Ait%20is%20difficult%20to%20directly%20apply%20MIM%20in%20SSL%20since%20the%20data%20distribution%20is%20not%0Aanalytically%20available%20in%20applications.%20In%20practice%2C%20many%20existing%20methods%20can%0Abe%20viewed%20as%20approximate%20implementations%20of%20the%20MIM%20criterion.%20This%20work%20shows%0Athat%2C%20based%20on%20the%20invariance%20property%20of%20MI%2C%20explicit%20MI%20maximization%20can%20be%0Aapplied%20to%20SSL%20under%20a%20generic%20distribution%20assumption%2C%20i.e.%2C%20a%20relaxed%0Acondition%20of%20the%20data%20distribution.%20We%20further%20illustrate%20this%20by%20analyzing%20the%0Ageneralized%20Gaussian%20distribution.%20Based%20on%20this%20result%2C%20we%20derive%20a%20loss%0Afunction%20based%20on%20the%20MIM%20criterion%20using%20only%20second-order%20statistics.%20We%0Aimplement%20the%20new%20loss%20for%20SSL%20and%20demonstrate%20its%20effectiveness%20via%20extensive%0Aexperiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04747v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplicit%2520Mutual%2520Information%2520Maximization%2520for%2520Self-Supervised%2520Learning%26entry.906535625%3DLele%2520Chang%2520and%2520Peilin%2520Liu%2520and%2520Qinghai%2520Guo%2520and%2520Fei%2520Wen%26entry.1292438233%3D%2520%2520Recently%252C%2520self-supervised%2520learning%2520%2528SSL%2529%2520has%2520been%2520extensively%2520studied.%250ATheoretically%252C%2520mutual%2520information%2520maximization%2520%2528MIM%2529%2520is%2520an%2520optimal%2520criterion%250Afor%2520SSL%252C%2520with%2520a%2520strong%2520theoretical%2520foundation%2520in%2520information%2520theory.%2520However%252C%250Ait%2520is%2520difficult%2520to%2520directly%2520apply%2520MIM%2520in%2520SSL%2520since%2520the%2520data%2520distribution%2520is%2520not%250Aanalytically%2520available%2520in%2520applications.%2520In%2520practice%252C%2520many%2520existing%2520methods%2520can%250Abe%2520viewed%2520as%2520approximate%2520implementations%2520of%2520the%2520MIM%2520criterion.%2520This%2520work%2520shows%250Athat%252C%2520based%2520on%2520the%2520invariance%2520property%2520of%2520MI%252C%2520explicit%2520MI%2520maximization%2520can%2520be%250Aapplied%2520to%2520SSL%2520under%2520a%2520generic%2520distribution%2520assumption%252C%2520i.e.%252C%2520a%2520relaxed%250Acondition%2520of%2520the%2520data%2520distribution.%2520We%2520further%2520illustrate%2520this%2520by%2520analyzing%2520the%250Ageneralized%2520Gaussian%2520distribution.%2520Based%2520on%2520this%2520result%252C%2520we%2520derive%2520a%2520loss%250Afunction%2520based%2520on%2520the%2520MIM%2520criterion%2520using%2520only%2520second-order%2520statistics.%2520We%250Aimplement%2520the%2520new%2520loss%2520for%2520SSL%2520and%2520demonstrate%2520its%2520effectiveness%2520via%2520extensive%250Aexperiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04747v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explicit%20Mutual%20Information%20Maximization%20for%20Self-Supervised%20Learning&entry.906535625=Lele%20Chang%20and%20Peilin%20Liu%20and%20Qinghai%20Guo%20and%20Fei%20Wen&entry.1292438233=%20%20Recently%2C%20self-supervised%20learning%20%28SSL%29%20has%20been%20extensively%20studied.%0ATheoretically%2C%20mutual%20information%20maximization%20%28MIM%29%20is%20an%20optimal%20criterion%0Afor%20SSL%2C%20with%20a%20strong%20theoretical%20foundation%20in%20information%20theory.%20However%2C%0Ait%20is%20difficult%20to%20directly%20apply%20MIM%20in%20SSL%20since%20the%20data%20distribution%20is%20not%0Aanalytically%20available%20in%20applications.%20In%20practice%2C%20many%20existing%20methods%20can%0Abe%20viewed%20as%20approximate%20implementations%20of%20the%20MIM%20criterion.%20This%20work%20shows%0Athat%2C%20based%20on%20the%20invariance%20property%20of%20MI%2C%20explicit%20MI%20maximization%20can%20be%0Aapplied%20to%20SSL%20under%20a%20generic%20distribution%20assumption%2C%20i.e.%2C%20a%20relaxed%0Acondition%20of%20the%20data%20distribution.%20We%20further%20illustrate%20this%20by%20analyzing%20the%0Ageneralized%20Gaussian%20distribution.%20Based%20on%20this%20result%2C%20we%20derive%20a%20loss%0Afunction%20based%20on%20the%20MIM%20criterion%20using%20only%20second-order%20statistics.%20We%0Aimplement%20the%20new%20loss%20for%20SSL%20and%20demonstrate%20its%20effectiveness%20via%20extensive%0Aexperiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04747v2&entry.124074799=Read"},
{"title": "VMAS: Video-to-Music Generation via Semantic Alignment in Web Music\n  Videos", "author": "Yan-Bo Lin and Yu Tian and Linjie Yang and Gedas Bertasius and Heng Wang", "abstract": "  We present a framework for learning to generate background music from video\ninputs. Unlike existing works that rely on symbolic musical annotations, which\nare limited in quantity and diversity, our method leverages large-scale web\nvideos accompanied by background music. This enables our model to learn to\ngenerate realistic and diverse music. To accomplish this goal, we develop a\ngenerative video-music Transformer with a novel semantic video-music alignment\nscheme. Our model uses a joint autoregressive and contrastive learning\nobjective, which encourages the generation of music aligned with high-level\nvideo content. We also introduce a novel video-beat alignment scheme to match\nthe generated music beats with the low-level motions in the video. Lastly, to\ncapture fine-grained visual cues in a video needed for realistic background\nmusic generation, we introduce a new temporal video encoder architecture,\nallowing us to efficiently process videos consisting of many densely sampled\nframes. We train our framework on our newly curated DISCO-MV dataset,\nconsisting of 2.2M video-music samples, which is orders of magnitude larger\nthan any prior datasets used for video music generation. Our method outperforms\nexisting approaches on the DISCO-MV and MusicCaps datasets according to various\nmusic generation evaluation metrics, including human evaluation. Results are\navailable at https://genjib.github.io/project_page/VMAs/index.html\n", "link": "http://arxiv.org/abs/2409.07450v1", "date": "2024-09-11", "relevancy": 2.2814, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6085}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5495}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VMAS%3A%20Video-to-Music%20Generation%20via%20Semantic%20Alignment%20in%20Web%20Music%0A%20%20Videos&body=Title%3A%20VMAS%3A%20Video-to-Music%20Generation%20via%20Semantic%20Alignment%20in%20Web%20Music%0A%20%20Videos%0AAuthor%3A%20Yan-Bo%20Lin%20and%20Yu%20Tian%20and%20Linjie%20Yang%20and%20Gedas%20Bertasius%20and%20Heng%20Wang%0AAbstract%3A%20%20%20We%20present%20a%20framework%20for%20learning%20to%20generate%20background%20music%20from%20video%0Ainputs.%20Unlike%20existing%20works%20that%20rely%20on%20symbolic%20musical%20annotations%2C%20which%0Aare%20limited%20in%20quantity%20and%20diversity%2C%20our%20method%20leverages%20large-scale%20web%0Avideos%20accompanied%20by%20background%20music.%20This%20enables%20our%20model%20to%20learn%20to%0Agenerate%20realistic%20and%20diverse%20music.%20To%20accomplish%20this%20goal%2C%20we%20develop%20a%0Agenerative%20video-music%20Transformer%20with%20a%20novel%20semantic%20video-music%20alignment%0Ascheme.%20Our%20model%20uses%20a%20joint%20autoregressive%20and%20contrastive%20learning%0Aobjective%2C%20which%20encourages%20the%20generation%20of%20music%20aligned%20with%20high-level%0Avideo%20content.%20We%20also%20introduce%20a%20novel%20video-beat%20alignment%20scheme%20to%20match%0Athe%20generated%20music%20beats%20with%20the%20low-level%20motions%20in%20the%20video.%20Lastly%2C%20to%0Acapture%20fine-grained%20visual%20cues%20in%20a%20video%20needed%20for%20realistic%20background%0Amusic%20generation%2C%20we%20introduce%20a%20new%20temporal%20video%20encoder%20architecture%2C%0Aallowing%20us%20to%20efficiently%20process%20videos%20consisting%20of%20many%20densely%20sampled%0Aframes.%20We%20train%20our%20framework%20on%20our%20newly%20curated%20DISCO-MV%20dataset%2C%0Aconsisting%20of%202.2M%20video-music%20samples%2C%20which%20is%20orders%20of%20magnitude%20larger%0Athan%20any%20prior%20datasets%20used%20for%20video%20music%20generation.%20Our%20method%20outperforms%0Aexisting%20approaches%20on%20the%20DISCO-MV%20and%20MusicCaps%20datasets%20according%20to%20various%0Amusic%20generation%20evaluation%20metrics%2C%20including%20human%20evaluation.%20Results%20are%0Aavailable%20at%20https%3A//genjib.github.io/project_page/VMAs/index.html%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07450v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVMAS%253A%2520Video-to-Music%2520Generation%2520via%2520Semantic%2520Alignment%2520in%2520Web%2520Music%250A%2520%2520Videos%26entry.906535625%3DYan-Bo%2520Lin%2520and%2520Yu%2520Tian%2520and%2520Linjie%2520Yang%2520and%2520Gedas%2520Bertasius%2520and%2520Heng%2520Wang%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520framework%2520for%2520learning%2520to%2520generate%2520background%2520music%2520from%2520video%250Ainputs.%2520Unlike%2520existing%2520works%2520that%2520rely%2520on%2520symbolic%2520musical%2520annotations%252C%2520which%250Aare%2520limited%2520in%2520quantity%2520and%2520diversity%252C%2520our%2520method%2520leverages%2520large-scale%2520web%250Avideos%2520accompanied%2520by%2520background%2520music.%2520This%2520enables%2520our%2520model%2520to%2520learn%2520to%250Agenerate%2520realistic%2520and%2520diverse%2520music.%2520To%2520accomplish%2520this%2520goal%252C%2520we%2520develop%2520a%250Agenerative%2520video-music%2520Transformer%2520with%2520a%2520novel%2520semantic%2520video-music%2520alignment%250Ascheme.%2520Our%2520model%2520uses%2520a%2520joint%2520autoregressive%2520and%2520contrastive%2520learning%250Aobjective%252C%2520which%2520encourages%2520the%2520generation%2520of%2520music%2520aligned%2520with%2520high-level%250Avideo%2520content.%2520We%2520also%2520introduce%2520a%2520novel%2520video-beat%2520alignment%2520scheme%2520to%2520match%250Athe%2520generated%2520music%2520beats%2520with%2520the%2520low-level%2520motions%2520in%2520the%2520video.%2520Lastly%252C%2520to%250Acapture%2520fine-grained%2520visual%2520cues%2520in%2520a%2520video%2520needed%2520for%2520realistic%2520background%250Amusic%2520generation%252C%2520we%2520introduce%2520a%2520new%2520temporal%2520video%2520encoder%2520architecture%252C%250Aallowing%2520us%2520to%2520efficiently%2520process%2520videos%2520consisting%2520of%2520many%2520densely%2520sampled%250Aframes.%2520We%2520train%2520our%2520framework%2520on%2520our%2520newly%2520curated%2520DISCO-MV%2520dataset%252C%250Aconsisting%2520of%25202.2M%2520video-music%2520samples%252C%2520which%2520is%2520orders%2520of%2520magnitude%2520larger%250Athan%2520any%2520prior%2520datasets%2520used%2520for%2520video%2520music%2520generation.%2520Our%2520method%2520outperforms%250Aexisting%2520approaches%2520on%2520the%2520DISCO-MV%2520and%2520MusicCaps%2520datasets%2520according%2520to%2520various%250Amusic%2520generation%2520evaluation%2520metrics%252C%2520including%2520human%2520evaluation.%2520Results%2520are%250Aavailable%2520at%2520https%253A//genjib.github.io/project_page/VMAs/index.html%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07450v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VMAS%3A%20Video-to-Music%20Generation%20via%20Semantic%20Alignment%20in%20Web%20Music%0A%20%20Videos&entry.906535625=Yan-Bo%20Lin%20and%20Yu%20Tian%20and%20Linjie%20Yang%20and%20Gedas%20Bertasius%20and%20Heng%20Wang&entry.1292438233=%20%20We%20present%20a%20framework%20for%20learning%20to%20generate%20background%20music%20from%20video%0Ainputs.%20Unlike%20existing%20works%20that%20rely%20on%20symbolic%20musical%20annotations%2C%20which%0Aare%20limited%20in%20quantity%20and%20diversity%2C%20our%20method%20leverages%20large-scale%20web%0Avideos%20accompanied%20by%20background%20music.%20This%20enables%20our%20model%20to%20learn%20to%0Agenerate%20realistic%20and%20diverse%20music.%20To%20accomplish%20this%20goal%2C%20we%20develop%20a%0Agenerative%20video-music%20Transformer%20with%20a%20novel%20semantic%20video-music%20alignment%0Ascheme.%20Our%20model%20uses%20a%20joint%20autoregressive%20and%20contrastive%20learning%0Aobjective%2C%20which%20encourages%20the%20generation%20of%20music%20aligned%20with%20high-level%0Avideo%20content.%20We%20also%20introduce%20a%20novel%20video-beat%20alignment%20scheme%20to%20match%0Athe%20generated%20music%20beats%20with%20the%20low-level%20motions%20in%20the%20video.%20Lastly%2C%20to%0Acapture%20fine-grained%20visual%20cues%20in%20a%20video%20needed%20for%20realistic%20background%0Amusic%20generation%2C%20we%20introduce%20a%20new%20temporal%20video%20encoder%20architecture%2C%0Aallowing%20us%20to%20efficiently%20process%20videos%20consisting%20of%20many%20densely%20sampled%0Aframes.%20We%20train%20our%20framework%20on%20our%20newly%20curated%20DISCO-MV%20dataset%2C%0Aconsisting%20of%202.2M%20video-music%20samples%2C%20which%20is%20orders%20of%20magnitude%20larger%0Athan%20any%20prior%20datasets%20used%20for%20video%20music%20generation.%20Our%20method%20outperforms%0Aexisting%20approaches%20on%20the%20DISCO-MV%20and%20MusicCaps%20datasets%20according%20to%20various%0Amusic%20generation%20evaluation%20metrics%2C%20including%20human%20evaluation.%20Results%20are%0Aavailable%20at%20https%3A//genjib.github.io/project_page/VMAs/index.html%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07450v1&entry.124074799=Read"},
{"title": "Error-Driven Uncertainty Aware Training", "author": "Pedro Mendes and Paolo Romano and David Garlan", "abstract": "  Neural networks are often overconfident about their predictions, which\nundermines their reliability and trustworthiness. In this work, we present a\nnovel technique, named Error-Driven Uncertainty Aware Training (EUAT), which\naims to enhance the ability of neural classifiers to estimate their uncertainty\ncorrectly, namely to be highly uncertain when they output inaccurate\npredictions and low uncertain when their output is accurate. The EUAT approach\noperates during the model's training phase by selectively employing two loss\nfunctions depending on whether the training examples are correctly or\nincorrectly predicted by the model. This allows for pursuing the twofold goal\nof i) minimizing model uncertainty for correctly predicted inputs and ii)\nmaximizing uncertainty for mispredicted inputs, while preserving the model's\nmisprediction rate. We evaluate EUAT using diverse neural models and datasets\nin the image recognition domains considering both non-adversarial and\nadversarial settings. The results show that EUAT outperforms existing\napproaches for uncertainty estimation (including other uncertainty-aware\ntraining techniques, calibration, ensembles, and DEUP) by providing uncertainty\nestimates that not only have higher quality when evaluated via statistical\nmetrics (e.g., correlation with residuals) but also when employed to build\nbinary classifiers that decide whether the model's output can be trusted or not\nand under distributional data shifts.\n", "link": "http://arxiv.org/abs/2405.01205v2", "date": "2024-09-11", "relevancy": 2.2706, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6364}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5762}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Error-Driven%20Uncertainty%20Aware%20Training&body=Title%3A%20Error-Driven%20Uncertainty%20Aware%20Training%0AAuthor%3A%20Pedro%20Mendes%20and%20Paolo%20Romano%20and%20David%20Garlan%0AAbstract%3A%20%20%20Neural%20networks%20are%20often%20overconfident%20about%20their%20predictions%2C%20which%0Aundermines%20their%20reliability%20and%20trustworthiness.%20In%20this%20work%2C%20we%20present%20a%0Anovel%20technique%2C%20named%20Error-Driven%20Uncertainty%20Aware%20Training%20%28EUAT%29%2C%20which%0Aaims%20to%20enhance%20the%20ability%20of%20neural%20classifiers%20to%20estimate%20their%20uncertainty%0Acorrectly%2C%20namely%20to%20be%20highly%20uncertain%20when%20they%20output%20inaccurate%0Apredictions%20and%20low%20uncertain%20when%20their%20output%20is%20accurate.%20The%20EUAT%20approach%0Aoperates%20during%20the%20model%27s%20training%20phase%20by%20selectively%20employing%20two%20loss%0Afunctions%20depending%20on%20whether%20the%20training%20examples%20are%20correctly%20or%0Aincorrectly%20predicted%20by%20the%20model.%20This%20allows%20for%20pursuing%20the%20twofold%20goal%0Aof%20i%29%20minimizing%20model%20uncertainty%20for%20correctly%20predicted%20inputs%20and%20ii%29%0Amaximizing%20uncertainty%20for%20mispredicted%20inputs%2C%20while%20preserving%20the%20model%27s%0Amisprediction%20rate.%20We%20evaluate%20EUAT%20using%20diverse%20neural%20models%20and%20datasets%0Ain%20the%20image%20recognition%20domains%20considering%20both%20non-adversarial%20and%0Aadversarial%20settings.%20The%20results%20show%20that%20EUAT%20outperforms%20existing%0Aapproaches%20for%20uncertainty%20estimation%20%28including%20other%20uncertainty-aware%0Atraining%20techniques%2C%20calibration%2C%20ensembles%2C%20and%20DEUP%29%20by%20providing%20uncertainty%0Aestimates%20that%20not%20only%20have%20higher%20quality%20when%20evaluated%20via%20statistical%0Ametrics%20%28e.g.%2C%20correlation%20with%20residuals%29%20but%20also%20when%20employed%20to%20build%0Abinary%20classifiers%20that%20decide%20whether%20the%20model%27s%20output%20can%20be%20trusted%20or%20not%0Aand%20under%20distributional%20data%20shifts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01205v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DError-Driven%2520Uncertainty%2520Aware%2520Training%26entry.906535625%3DPedro%2520Mendes%2520and%2520Paolo%2520Romano%2520and%2520David%2520Garlan%26entry.1292438233%3D%2520%2520Neural%2520networks%2520are%2520often%2520overconfident%2520about%2520their%2520predictions%252C%2520which%250Aundermines%2520their%2520reliability%2520and%2520trustworthiness.%2520In%2520this%2520work%252C%2520we%2520present%2520a%250Anovel%2520technique%252C%2520named%2520Error-Driven%2520Uncertainty%2520Aware%2520Training%2520%2528EUAT%2529%252C%2520which%250Aaims%2520to%2520enhance%2520the%2520ability%2520of%2520neural%2520classifiers%2520to%2520estimate%2520their%2520uncertainty%250Acorrectly%252C%2520namely%2520to%2520be%2520highly%2520uncertain%2520when%2520they%2520output%2520inaccurate%250Apredictions%2520and%2520low%2520uncertain%2520when%2520their%2520output%2520is%2520accurate.%2520The%2520EUAT%2520approach%250Aoperates%2520during%2520the%2520model%2527s%2520training%2520phase%2520by%2520selectively%2520employing%2520two%2520loss%250Afunctions%2520depending%2520on%2520whether%2520the%2520training%2520examples%2520are%2520correctly%2520or%250Aincorrectly%2520predicted%2520by%2520the%2520model.%2520This%2520allows%2520for%2520pursuing%2520the%2520twofold%2520goal%250Aof%2520i%2529%2520minimizing%2520model%2520uncertainty%2520for%2520correctly%2520predicted%2520inputs%2520and%2520ii%2529%250Amaximizing%2520uncertainty%2520for%2520mispredicted%2520inputs%252C%2520while%2520preserving%2520the%2520model%2527s%250Amisprediction%2520rate.%2520We%2520evaluate%2520EUAT%2520using%2520diverse%2520neural%2520models%2520and%2520datasets%250Ain%2520the%2520image%2520recognition%2520domains%2520considering%2520both%2520non-adversarial%2520and%250Aadversarial%2520settings.%2520The%2520results%2520show%2520that%2520EUAT%2520outperforms%2520existing%250Aapproaches%2520for%2520uncertainty%2520estimation%2520%2528including%2520other%2520uncertainty-aware%250Atraining%2520techniques%252C%2520calibration%252C%2520ensembles%252C%2520and%2520DEUP%2529%2520by%2520providing%2520uncertainty%250Aestimates%2520that%2520not%2520only%2520have%2520higher%2520quality%2520when%2520evaluated%2520via%2520statistical%250Ametrics%2520%2528e.g.%252C%2520correlation%2520with%2520residuals%2529%2520but%2520also%2520when%2520employed%2520to%2520build%250Abinary%2520classifiers%2520that%2520decide%2520whether%2520the%2520model%2527s%2520output%2520can%2520be%2520trusted%2520or%2520not%250Aand%2520under%2520distributional%2520data%2520shifts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01205v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Error-Driven%20Uncertainty%20Aware%20Training&entry.906535625=Pedro%20Mendes%20and%20Paolo%20Romano%20and%20David%20Garlan&entry.1292438233=%20%20Neural%20networks%20are%20often%20overconfident%20about%20their%20predictions%2C%20which%0Aundermines%20their%20reliability%20and%20trustworthiness.%20In%20this%20work%2C%20we%20present%20a%0Anovel%20technique%2C%20named%20Error-Driven%20Uncertainty%20Aware%20Training%20%28EUAT%29%2C%20which%0Aaims%20to%20enhance%20the%20ability%20of%20neural%20classifiers%20to%20estimate%20their%20uncertainty%0Acorrectly%2C%20namely%20to%20be%20highly%20uncertain%20when%20they%20output%20inaccurate%0Apredictions%20and%20low%20uncertain%20when%20their%20output%20is%20accurate.%20The%20EUAT%20approach%0Aoperates%20during%20the%20model%27s%20training%20phase%20by%20selectively%20employing%20two%20loss%0Afunctions%20depending%20on%20whether%20the%20training%20examples%20are%20correctly%20or%0Aincorrectly%20predicted%20by%20the%20model.%20This%20allows%20for%20pursuing%20the%20twofold%20goal%0Aof%20i%29%20minimizing%20model%20uncertainty%20for%20correctly%20predicted%20inputs%20and%20ii%29%0Amaximizing%20uncertainty%20for%20mispredicted%20inputs%2C%20while%20preserving%20the%20model%27s%0Amisprediction%20rate.%20We%20evaluate%20EUAT%20using%20diverse%20neural%20models%20and%20datasets%0Ain%20the%20image%20recognition%20domains%20considering%20both%20non-adversarial%20and%0Aadversarial%20settings.%20The%20results%20show%20that%20EUAT%20outperforms%20existing%0Aapproaches%20for%20uncertainty%20estimation%20%28including%20other%20uncertainty-aware%0Atraining%20techniques%2C%20calibration%2C%20ensembles%2C%20and%20DEUP%29%20by%20providing%20uncertainty%0Aestimates%20that%20not%20only%20have%20higher%20quality%20when%20evaluated%20via%20statistical%0Ametrics%20%28e.g.%2C%20correlation%20with%20residuals%29%20but%20also%20when%20employed%20to%20build%0Abinary%20classifiers%20that%20decide%20whether%20the%20model%27s%20output%20can%20be%20trusted%20or%20not%0Aand%20under%20distributional%20data%20shifts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01205v2&entry.124074799=Read"},
{"title": "Using Generative Agents to Create Tip Sheets for Investigative Data\n  Reporting", "author": "Joris Veerbeek and Nicholas Diakopoulos", "abstract": "  This paper introduces a system using generative AI agents to create tip\nsheets for investigative data reporting. Our system employs three specialized\nagents--an analyst, a reporter, and an editor--to collaboratively generate and\nrefine tips from datasets. We validate this approach using real-world\ninvestigative stories, demonstrating that our agent-based system generally\ngenerates more newsworthy and accurate insights compared to a baseline model\nwithout agents, although some variability was noted between different stories.\nOur findings highlight the potential of generative AI to provide leads for\ninvestigative data reporting.\n", "link": "http://arxiv.org/abs/2409.07286v1", "date": "2024-09-11", "relevancy": 2.2512, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5027}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4262}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20Generative%20Agents%20to%20Create%20Tip%20Sheets%20for%20Investigative%20Data%0A%20%20Reporting&body=Title%3A%20Using%20Generative%20Agents%20to%20Create%20Tip%20Sheets%20for%20Investigative%20Data%0A%20%20Reporting%0AAuthor%3A%20Joris%20Veerbeek%20and%20Nicholas%20Diakopoulos%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20system%20using%20generative%20AI%20agents%20to%20create%20tip%0Asheets%20for%20investigative%20data%20reporting.%20Our%20system%20employs%20three%20specialized%0Aagents--an%20analyst%2C%20a%20reporter%2C%20and%20an%20editor--to%20collaboratively%20generate%20and%0Arefine%20tips%20from%20datasets.%20We%20validate%20this%20approach%20using%20real-world%0Ainvestigative%20stories%2C%20demonstrating%20that%20our%20agent-based%20system%20generally%0Agenerates%20more%20newsworthy%20and%20accurate%20insights%20compared%20to%20a%20baseline%20model%0Awithout%20agents%2C%20although%20some%20variability%20was%20noted%20between%20different%20stories.%0AOur%20findings%20highlight%20the%20potential%20of%20generative%20AI%20to%20provide%20leads%20for%0Ainvestigative%20data%20reporting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07286v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520Generative%2520Agents%2520to%2520Create%2520Tip%2520Sheets%2520for%2520Investigative%2520Data%250A%2520%2520Reporting%26entry.906535625%3DJoris%2520Veerbeek%2520and%2520Nicholas%2520Diakopoulos%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520system%2520using%2520generative%2520AI%2520agents%2520to%2520create%2520tip%250Asheets%2520for%2520investigative%2520data%2520reporting.%2520Our%2520system%2520employs%2520three%2520specialized%250Aagents--an%2520analyst%252C%2520a%2520reporter%252C%2520and%2520an%2520editor--to%2520collaboratively%2520generate%2520and%250Arefine%2520tips%2520from%2520datasets.%2520We%2520validate%2520this%2520approach%2520using%2520real-world%250Ainvestigative%2520stories%252C%2520demonstrating%2520that%2520our%2520agent-based%2520system%2520generally%250Agenerates%2520more%2520newsworthy%2520and%2520accurate%2520insights%2520compared%2520to%2520a%2520baseline%2520model%250Awithout%2520agents%252C%2520although%2520some%2520variability%2520was%2520noted%2520between%2520different%2520stories.%250AOur%2520findings%2520highlight%2520the%2520potential%2520of%2520generative%2520AI%2520to%2520provide%2520leads%2520for%250Ainvestigative%2520data%2520reporting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07286v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Generative%20Agents%20to%20Create%20Tip%20Sheets%20for%20Investigative%20Data%0A%20%20Reporting&entry.906535625=Joris%20Veerbeek%20and%20Nicholas%20Diakopoulos&entry.1292438233=%20%20This%20paper%20introduces%20a%20system%20using%20generative%20AI%20agents%20to%20create%20tip%0Asheets%20for%20investigative%20data%20reporting.%20Our%20system%20employs%20three%20specialized%0Aagents--an%20analyst%2C%20a%20reporter%2C%20and%20an%20editor--to%20collaboratively%20generate%20and%0Arefine%20tips%20from%20datasets.%20We%20validate%20this%20approach%20using%20real-world%0Ainvestigative%20stories%2C%20demonstrating%20that%20our%20agent-based%20system%20generally%0Agenerates%20more%20newsworthy%20and%20accurate%20insights%20compared%20to%20a%20baseline%20model%0Awithout%20agents%2C%20although%20some%20variability%20was%20noted%20between%20different%20stories.%0AOur%20findings%20highlight%20the%20potential%20of%20generative%20AI%20to%20provide%20leads%20for%0Ainvestigative%20data%20reporting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07286v1&entry.124074799=Read"},
{"title": "Segment Anything Model for Brain Tumor Segmentation", "author": "Peng Zhang and Yaping Wang", "abstract": "  Glioma is a prevalent brain tumor that poses a significant health risk to\nindividuals. Accurate segmentation of brain tumor is essential for clinical\ndiagnosis and treatment. The Segment Anything Model(SAM), released by Meta AI,\nis a fundamental model in image segmentation and has excellent zero-sample\ngeneralization capabilities. Thus, it is interesting to apply SAM to the task\nof brain tumor segmentation. In this study, we evaluated the performance of SAM\non brain tumor segmentation and found that without any model fine-tuning, there\nis still a gap between SAM and the current state-of-the-art(SOTA) model.\n", "link": "http://arxiv.org/abs/2309.08434v2", "date": "2024-09-11", "relevancy": 2.2426, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4671}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4392}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segment%20Anything%20Model%20for%20Brain%20Tumor%20Segmentation&body=Title%3A%20Segment%20Anything%20Model%20for%20Brain%20Tumor%20Segmentation%0AAuthor%3A%20Peng%20Zhang%20and%20Yaping%20Wang%0AAbstract%3A%20%20%20Glioma%20is%20a%20prevalent%20brain%20tumor%20that%20poses%20a%20significant%20health%20risk%20to%0Aindividuals.%20Accurate%20segmentation%20of%20brain%20tumor%20is%20essential%20for%20clinical%0Adiagnosis%20and%20treatment.%20The%20Segment%20Anything%20Model%28SAM%29%2C%20released%20by%20Meta%20AI%2C%0Ais%20a%20fundamental%20model%20in%20image%20segmentation%20and%20has%20excellent%20zero-sample%0Ageneralization%20capabilities.%20Thus%2C%20it%20is%20interesting%20to%20apply%20SAM%20to%20the%20task%0Aof%20brain%20tumor%20segmentation.%20In%20this%20study%2C%20we%20evaluated%20the%20performance%20of%20SAM%0Aon%20brain%20tumor%20segmentation%20and%20found%20that%20without%20any%20model%20fine-tuning%2C%20there%0Ais%20still%20a%20gap%20between%20SAM%20and%20the%20current%20state-of-the-art%28SOTA%29%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.08434v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegment%2520Anything%2520Model%2520for%2520Brain%2520Tumor%2520Segmentation%26entry.906535625%3DPeng%2520Zhang%2520and%2520Yaping%2520Wang%26entry.1292438233%3D%2520%2520Glioma%2520is%2520a%2520prevalent%2520brain%2520tumor%2520that%2520poses%2520a%2520significant%2520health%2520risk%2520to%250Aindividuals.%2520Accurate%2520segmentation%2520of%2520brain%2520tumor%2520is%2520essential%2520for%2520clinical%250Adiagnosis%2520and%2520treatment.%2520The%2520Segment%2520Anything%2520Model%2528SAM%2529%252C%2520released%2520by%2520Meta%2520AI%252C%250Ais%2520a%2520fundamental%2520model%2520in%2520image%2520segmentation%2520and%2520has%2520excellent%2520zero-sample%250Ageneralization%2520capabilities.%2520Thus%252C%2520it%2520is%2520interesting%2520to%2520apply%2520SAM%2520to%2520the%2520task%250Aof%2520brain%2520tumor%2520segmentation.%2520In%2520this%2520study%252C%2520we%2520evaluated%2520the%2520performance%2520of%2520SAM%250Aon%2520brain%2520tumor%2520segmentation%2520and%2520found%2520that%2520without%2520any%2520model%2520fine-tuning%252C%2520there%250Ais%2520still%2520a%2520gap%2520between%2520SAM%2520and%2520the%2520current%2520state-of-the-art%2528SOTA%2529%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.08434v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segment%20Anything%20Model%20for%20Brain%20Tumor%20Segmentation&entry.906535625=Peng%20Zhang%20and%20Yaping%20Wang&entry.1292438233=%20%20Glioma%20is%20a%20prevalent%20brain%20tumor%20that%20poses%20a%20significant%20health%20risk%20to%0Aindividuals.%20Accurate%20segmentation%20of%20brain%20tumor%20is%20essential%20for%20clinical%0Adiagnosis%20and%20treatment.%20The%20Segment%20Anything%20Model%28SAM%29%2C%20released%20by%20Meta%20AI%2C%0Ais%20a%20fundamental%20model%20in%20image%20segmentation%20and%20has%20excellent%20zero-sample%0Ageneralization%20capabilities.%20Thus%2C%20it%20is%20interesting%20to%20apply%20SAM%20to%20the%20task%0Aof%20brain%20tumor%20segmentation.%20In%20this%20study%2C%20we%20evaluated%20the%20performance%20of%20SAM%0Aon%20brain%20tumor%20segmentation%20and%20found%20that%20without%20any%20model%20fine-tuning%2C%20there%0Ais%20still%20a%20gap%20between%20SAM%20and%20the%20current%20state-of-the-art%28SOTA%29%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.08434v2&entry.124074799=Read"},
{"title": "MCTR: Multi Camera Tracking Transformer", "author": "Alexandru Niculescu-Mizil and Deep Patel and Iain Melvin", "abstract": "  Multi-camera tracking plays a pivotal role in various real-world\napplications. While end-to-end methods have gained significant interest in\nsingle-camera tracking, multi-camera tracking remains predominantly reliant on\nheuristic techniques. In response to this gap, this paper introduces\nMulti-Camera Tracking tRansformer (MCTR), a novel end-to-end approach tailored\nfor multi-object detection and tracking across multiple cameras with\noverlapping fields of view. MCTR leverages end-to-end detectors like DEtector\nTRansformer (DETR) to produce detections and detection embeddings independently\nfor each camera view. The framework maintains set of track embeddings that\nencaplusate global information about the tracked objects, and updates them at\nevery frame by integrating the local information from the view-specific\ndetection embeddings. The track embeddings are probabilistically associated\nwith detections in every camera view and frame to generate consistent object\ntracks. The soft probabilistic association facilitates the design of\ndifferentiable losses that enable end-to-end training of the entire system. To\nvalidate our approach, we conduct experiments on MMPTrack and AI City\nChallenge, two recently introduced large-scale multi-camera multi-object\ntracking datasets.\n", "link": "http://arxiv.org/abs/2408.13243v2", "date": "2024-09-11", "relevancy": 2.2285, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.565}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.559}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MCTR%3A%20Multi%20Camera%20Tracking%20Transformer&body=Title%3A%20MCTR%3A%20Multi%20Camera%20Tracking%20Transformer%0AAuthor%3A%20Alexandru%20Niculescu-Mizil%20and%20Deep%20Patel%20and%20Iain%20Melvin%0AAbstract%3A%20%20%20Multi-camera%20tracking%20plays%20a%20pivotal%20role%20in%20various%20real-world%0Aapplications.%20While%20end-to-end%20methods%20have%20gained%20significant%20interest%20in%0Asingle-camera%20tracking%2C%20multi-camera%20tracking%20remains%20predominantly%20reliant%20on%0Aheuristic%20techniques.%20In%20response%20to%20this%20gap%2C%20this%20paper%20introduces%0AMulti-Camera%20Tracking%20tRansformer%20%28MCTR%29%2C%20a%20novel%20end-to-end%20approach%20tailored%0Afor%20multi-object%20detection%20and%20tracking%20across%20multiple%20cameras%20with%0Aoverlapping%20fields%20of%20view.%20MCTR%20leverages%20end-to-end%20detectors%20like%20DEtector%0ATRansformer%20%28DETR%29%20to%20produce%20detections%20and%20detection%20embeddings%20independently%0Afor%20each%20camera%20view.%20The%20framework%20maintains%20set%20of%20track%20embeddings%20that%0Aencaplusate%20global%20information%20about%20the%20tracked%20objects%2C%20and%20updates%20them%20at%0Aevery%20frame%20by%20integrating%20the%20local%20information%20from%20the%20view-specific%0Adetection%20embeddings.%20The%20track%20embeddings%20are%20probabilistically%20associated%0Awith%20detections%20in%20every%20camera%20view%20and%20frame%20to%20generate%20consistent%20object%0Atracks.%20The%20soft%20probabilistic%20association%20facilitates%20the%20design%20of%0Adifferentiable%20losses%20that%20enable%20end-to-end%20training%20of%20the%20entire%20system.%20To%0Avalidate%20our%20approach%2C%20we%20conduct%20experiments%20on%20MMPTrack%20and%20AI%20City%0AChallenge%2C%20two%20recently%20introduced%20large-scale%20multi-camera%20multi-object%0Atracking%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13243v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMCTR%253A%2520Multi%2520Camera%2520Tracking%2520Transformer%26entry.906535625%3DAlexandru%2520Niculescu-Mizil%2520and%2520Deep%2520Patel%2520and%2520Iain%2520Melvin%26entry.1292438233%3D%2520%2520Multi-camera%2520tracking%2520plays%2520a%2520pivotal%2520role%2520in%2520various%2520real-world%250Aapplications.%2520While%2520end-to-end%2520methods%2520have%2520gained%2520significant%2520interest%2520in%250Asingle-camera%2520tracking%252C%2520multi-camera%2520tracking%2520remains%2520predominantly%2520reliant%2520on%250Aheuristic%2520techniques.%2520In%2520response%2520to%2520this%2520gap%252C%2520this%2520paper%2520introduces%250AMulti-Camera%2520Tracking%2520tRansformer%2520%2528MCTR%2529%252C%2520a%2520novel%2520end-to-end%2520approach%2520tailored%250Afor%2520multi-object%2520detection%2520and%2520tracking%2520across%2520multiple%2520cameras%2520with%250Aoverlapping%2520fields%2520of%2520view.%2520MCTR%2520leverages%2520end-to-end%2520detectors%2520like%2520DEtector%250ATRansformer%2520%2528DETR%2529%2520to%2520produce%2520detections%2520and%2520detection%2520embeddings%2520independently%250Afor%2520each%2520camera%2520view.%2520The%2520framework%2520maintains%2520set%2520of%2520track%2520embeddings%2520that%250Aencaplusate%2520global%2520information%2520about%2520the%2520tracked%2520objects%252C%2520and%2520updates%2520them%2520at%250Aevery%2520frame%2520by%2520integrating%2520the%2520local%2520information%2520from%2520the%2520view-specific%250Adetection%2520embeddings.%2520The%2520track%2520embeddings%2520are%2520probabilistically%2520associated%250Awith%2520detections%2520in%2520every%2520camera%2520view%2520and%2520frame%2520to%2520generate%2520consistent%2520object%250Atracks.%2520The%2520soft%2520probabilistic%2520association%2520facilitates%2520the%2520design%2520of%250Adifferentiable%2520losses%2520that%2520enable%2520end-to-end%2520training%2520of%2520the%2520entire%2520system.%2520To%250Avalidate%2520our%2520approach%252C%2520we%2520conduct%2520experiments%2520on%2520MMPTrack%2520and%2520AI%2520City%250AChallenge%252C%2520two%2520recently%2520introduced%2520large-scale%2520multi-camera%2520multi-object%250Atracking%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13243v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCTR%3A%20Multi%20Camera%20Tracking%20Transformer&entry.906535625=Alexandru%20Niculescu-Mizil%20and%20Deep%20Patel%20and%20Iain%20Melvin&entry.1292438233=%20%20Multi-camera%20tracking%20plays%20a%20pivotal%20role%20in%20various%20real-world%0Aapplications.%20While%20end-to-end%20methods%20have%20gained%20significant%20interest%20in%0Asingle-camera%20tracking%2C%20multi-camera%20tracking%20remains%20predominantly%20reliant%20on%0Aheuristic%20techniques.%20In%20response%20to%20this%20gap%2C%20this%20paper%20introduces%0AMulti-Camera%20Tracking%20tRansformer%20%28MCTR%29%2C%20a%20novel%20end-to-end%20approach%20tailored%0Afor%20multi-object%20detection%20and%20tracking%20across%20multiple%20cameras%20with%0Aoverlapping%20fields%20of%20view.%20MCTR%20leverages%20end-to-end%20detectors%20like%20DEtector%0ATRansformer%20%28DETR%29%20to%20produce%20detections%20and%20detection%20embeddings%20independently%0Afor%20each%20camera%20view.%20The%20framework%20maintains%20set%20of%20track%20embeddings%20that%0Aencaplusate%20global%20information%20about%20the%20tracked%20objects%2C%20and%20updates%20them%20at%0Aevery%20frame%20by%20integrating%20the%20local%20information%20from%20the%20view-specific%0Adetection%20embeddings.%20The%20track%20embeddings%20are%20probabilistically%20associated%0Awith%20detections%20in%20every%20camera%20view%20and%20frame%20to%20generate%20consistent%20object%0Atracks.%20The%20soft%20probabilistic%20association%20facilitates%20the%20design%20of%0Adifferentiable%20losses%20that%20enable%20end-to-end%20training%20of%20the%20entire%20system.%20To%0Avalidate%20our%20approach%2C%20we%20conduct%20experiments%20on%20MMPTrack%20and%20AI%20City%0AChallenge%2C%20two%20recently%20introduced%20large-scale%20multi-camera%20multi-object%0Atracking%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13243v2&entry.124074799=Read"},
{"title": "Auto-Multilift: Distributed Learning and Control for Cooperative Load\n  Transportation With Quadrotors", "author": "Bingheng Wang and Rui Huang and Lin Zhao", "abstract": "  Designing motion control and planning algorithms for multilift systems\nremains challenging due to the complexities of dynamics, collision avoidance,\nactuator limits, and scalability. Existing methods that use optimization and\ndistributed techniques effectively address these constraints and scalability\nissues. However, they often require substantial manual tuning, leading to\nsuboptimal performance. This paper proposes Auto-Multilift, a novel framework\nthat automates the tuning of model predictive controllers (MPCs) for multilift\nsystems. We model the MPC cost functions with deep neural networks (DNNs),\nenabling fast online adaptation to various scenarios. We develop a distributed\npolicy gradient algorithm to train these DNNs efficiently in a closed-loop\nmanner. Central to our algorithm is distributed sensitivity propagation, which\nis built on fully exploiting the unique dynamic couplings within the multilift\nsystem. It parallelizes gradient computation across quadrotors and focuses on\nactual system state sensitivities relative to key MPC parameters. Extensive\nsimulations demonstrate favorable scalability to a large number of quadrotors.\nOur method outperforms a state-of-the-art open-loop MPC tuning approach by\neffectively learning adaptive MPCs from trajectory tracking errors. It also\nexcels in learning an adaptive reference for reconfiguring the system when\ntraversing multiple narrow slots.\n", "link": "http://arxiv.org/abs/2406.04858v3", "date": "2024-09-11", "relevancy": 2.2282, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5791}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5556}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Auto-Multilift%3A%20Distributed%20Learning%20and%20Control%20for%20Cooperative%20Load%0A%20%20Transportation%20With%20Quadrotors&body=Title%3A%20Auto-Multilift%3A%20Distributed%20Learning%20and%20Control%20for%20Cooperative%20Load%0A%20%20Transportation%20With%20Quadrotors%0AAuthor%3A%20Bingheng%20Wang%20and%20Rui%20Huang%20and%20Lin%20Zhao%0AAbstract%3A%20%20%20Designing%20motion%20control%20and%20planning%20algorithms%20for%20multilift%20systems%0Aremains%20challenging%20due%20to%20the%20complexities%20of%20dynamics%2C%20collision%20avoidance%2C%0Aactuator%20limits%2C%20and%20scalability.%20Existing%20methods%20that%20use%20optimization%20and%0Adistributed%20techniques%20effectively%20address%20these%20constraints%20and%20scalability%0Aissues.%20However%2C%20they%20often%20require%20substantial%20manual%20tuning%2C%20leading%20to%0Asuboptimal%20performance.%20This%20paper%20proposes%20Auto-Multilift%2C%20a%20novel%20framework%0Athat%20automates%20the%20tuning%20of%20model%20predictive%20controllers%20%28MPCs%29%20for%20multilift%0Asystems.%20We%20model%20the%20MPC%20cost%20functions%20with%20deep%20neural%20networks%20%28DNNs%29%2C%0Aenabling%20fast%20online%20adaptation%20to%20various%20scenarios.%20We%20develop%20a%20distributed%0Apolicy%20gradient%20algorithm%20to%20train%20these%20DNNs%20efficiently%20in%20a%20closed-loop%0Amanner.%20Central%20to%20our%20algorithm%20is%20distributed%20sensitivity%20propagation%2C%20which%0Ais%20built%20on%20fully%20exploiting%20the%20unique%20dynamic%20couplings%20within%20the%20multilift%0Asystem.%20It%20parallelizes%20gradient%20computation%20across%20quadrotors%20and%20focuses%20on%0Aactual%20system%20state%20sensitivities%20relative%20to%20key%20MPC%20parameters.%20Extensive%0Asimulations%20demonstrate%20favorable%20scalability%20to%20a%20large%20number%20of%20quadrotors.%0AOur%20method%20outperforms%20a%20state-of-the-art%20open-loop%20MPC%20tuning%20approach%20by%0Aeffectively%20learning%20adaptive%20MPCs%20from%20trajectory%20tracking%20errors.%20It%20also%0Aexcels%20in%20learning%20an%20adaptive%20reference%20for%20reconfiguring%20the%20system%20when%0Atraversing%20multiple%20narrow%20slots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04858v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuto-Multilift%253A%2520Distributed%2520Learning%2520and%2520Control%2520for%2520Cooperative%2520Load%250A%2520%2520Transportation%2520With%2520Quadrotors%26entry.906535625%3DBingheng%2520Wang%2520and%2520Rui%2520Huang%2520and%2520Lin%2520Zhao%26entry.1292438233%3D%2520%2520Designing%2520motion%2520control%2520and%2520planning%2520algorithms%2520for%2520multilift%2520systems%250Aremains%2520challenging%2520due%2520to%2520the%2520complexities%2520of%2520dynamics%252C%2520collision%2520avoidance%252C%250Aactuator%2520limits%252C%2520and%2520scalability.%2520Existing%2520methods%2520that%2520use%2520optimization%2520and%250Adistributed%2520techniques%2520effectively%2520address%2520these%2520constraints%2520and%2520scalability%250Aissues.%2520However%252C%2520they%2520often%2520require%2520substantial%2520manual%2520tuning%252C%2520leading%2520to%250Asuboptimal%2520performance.%2520This%2520paper%2520proposes%2520Auto-Multilift%252C%2520a%2520novel%2520framework%250Athat%2520automates%2520the%2520tuning%2520of%2520model%2520predictive%2520controllers%2520%2528MPCs%2529%2520for%2520multilift%250Asystems.%2520We%2520model%2520the%2520MPC%2520cost%2520functions%2520with%2520deep%2520neural%2520networks%2520%2528DNNs%2529%252C%250Aenabling%2520fast%2520online%2520adaptation%2520to%2520various%2520scenarios.%2520We%2520develop%2520a%2520distributed%250Apolicy%2520gradient%2520algorithm%2520to%2520train%2520these%2520DNNs%2520efficiently%2520in%2520a%2520closed-loop%250Amanner.%2520Central%2520to%2520our%2520algorithm%2520is%2520distributed%2520sensitivity%2520propagation%252C%2520which%250Ais%2520built%2520on%2520fully%2520exploiting%2520the%2520unique%2520dynamic%2520couplings%2520within%2520the%2520multilift%250Asystem.%2520It%2520parallelizes%2520gradient%2520computation%2520across%2520quadrotors%2520and%2520focuses%2520on%250Aactual%2520system%2520state%2520sensitivities%2520relative%2520to%2520key%2520MPC%2520parameters.%2520Extensive%250Asimulations%2520demonstrate%2520favorable%2520scalability%2520to%2520a%2520large%2520number%2520of%2520quadrotors.%250AOur%2520method%2520outperforms%2520a%2520state-of-the-art%2520open-loop%2520MPC%2520tuning%2520approach%2520by%250Aeffectively%2520learning%2520adaptive%2520MPCs%2520from%2520trajectory%2520tracking%2520errors.%2520It%2520also%250Aexcels%2520in%2520learning%2520an%2520adaptive%2520reference%2520for%2520reconfiguring%2520the%2520system%2520when%250Atraversing%2520multiple%2520narrow%2520slots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04858v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Auto-Multilift%3A%20Distributed%20Learning%20and%20Control%20for%20Cooperative%20Load%0A%20%20Transportation%20With%20Quadrotors&entry.906535625=Bingheng%20Wang%20and%20Rui%20Huang%20and%20Lin%20Zhao&entry.1292438233=%20%20Designing%20motion%20control%20and%20planning%20algorithms%20for%20multilift%20systems%0Aremains%20challenging%20due%20to%20the%20complexities%20of%20dynamics%2C%20collision%20avoidance%2C%0Aactuator%20limits%2C%20and%20scalability.%20Existing%20methods%20that%20use%20optimization%20and%0Adistributed%20techniques%20effectively%20address%20these%20constraints%20and%20scalability%0Aissues.%20However%2C%20they%20often%20require%20substantial%20manual%20tuning%2C%20leading%20to%0Asuboptimal%20performance.%20This%20paper%20proposes%20Auto-Multilift%2C%20a%20novel%20framework%0Athat%20automates%20the%20tuning%20of%20model%20predictive%20controllers%20%28MPCs%29%20for%20multilift%0Asystems.%20We%20model%20the%20MPC%20cost%20functions%20with%20deep%20neural%20networks%20%28DNNs%29%2C%0Aenabling%20fast%20online%20adaptation%20to%20various%20scenarios.%20We%20develop%20a%20distributed%0Apolicy%20gradient%20algorithm%20to%20train%20these%20DNNs%20efficiently%20in%20a%20closed-loop%0Amanner.%20Central%20to%20our%20algorithm%20is%20distributed%20sensitivity%20propagation%2C%20which%0Ais%20built%20on%20fully%20exploiting%20the%20unique%20dynamic%20couplings%20within%20the%20multilift%0Asystem.%20It%20parallelizes%20gradient%20computation%20across%20quadrotors%20and%20focuses%20on%0Aactual%20system%20state%20sensitivities%20relative%20to%20key%20MPC%20parameters.%20Extensive%0Asimulations%20demonstrate%20favorable%20scalability%20to%20a%20large%20number%20of%20quadrotors.%0AOur%20method%20outperforms%20a%20state-of-the-art%20open-loop%20MPC%20tuning%20approach%20by%0Aeffectively%20learning%20adaptive%20MPCs%20from%20trajectory%20tracking%20errors.%20It%20also%0Aexcels%20in%20learning%20an%20adaptive%20reference%20for%20reconfiguring%20the%20system%20when%0Atraversing%20multiple%20narrow%20slots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04858v3&entry.124074799=Read"},
{"title": "CCFExp: Facial Image Synthesis with Cycle Cross-Fusion Diffusion Model\n  for Facial Paralysis Individuals", "author": "Weixiang Gao and Yifan Xia", "abstract": "  Facial paralysis is a debilitating condition that affects the movement of\nfacial muscles, leading to a significant loss of facial expressions. Currently,\nthe diagnosis of facial paralysis remains a challenging task, often relying\nheavily on the subjective judgment and experience of clinicians, which can\nintroduce variability and uncertainty in the assessment process. One promising\napplication in real-life situations is the automatic estimation of facial\nparalysis. However, the scarcity of facial paralysis datasets limits the\ndevelopment of robust machine learning models for automated diagnosis and\ntherapeutic interventions. To this end, this study aims to synthesize a\nhigh-quality facial paralysis dataset to address this gap, enabling more\naccurate and efficient algorithm training. Specifically, a novel Cycle\nCross-Fusion Expression Generative Model (CCFExp) based on the diffusion model\nis proposed to combine different features of facial information and enhance the\nvisual details of facial appearance and texture in facial regions, thus\ncreating synthetic facial images that accurately represent various degrees and\ntypes of facial paralysis. We have qualitatively and quantitatively evaluated\nthe proposed method on the commonly used public clinical datasets of facial\nparalysis to demonstrate its effectiveness. Experimental results indicate that\nthe proposed method surpasses state-of-the-art methods, generating more\nrealistic facial images and maintaining identity consistency.\n", "link": "http://arxiv.org/abs/2409.07271v1", "date": "2024-09-11", "relevancy": 2.2259, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5611}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5556}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CCFExp%3A%20Facial%20Image%20Synthesis%20with%20Cycle%20Cross-Fusion%20Diffusion%20Model%0A%20%20for%20Facial%20Paralysis%20Individuals&body=Title%3A%20CCFExp%3A%20Facial%20Image%20Synthesis%20with%20Cycle%20Cross-Fusion%20Diffusion%20Model%0A%20%20for%20Facial%20Paralysis%20Individuals%0AAuthor%3A%20Weixiang%20Gao%20and%20Yifan%20Xia%0AAbstract%3A%20%20%20Facial%20paralysis%20is%20a%20debilitating%20condition%20that%20affects%20the%20movement%20of%0Afacial%20muscles%2C%20leading%20to%20a%20significant%20loss%20of%20facial%20expressions.%20Currently%2C%0Athe%20diagnosis%20of%20facial%20paralysis%20remains%20a%20challenging%20task%2C%20often%20relying%0Aheavily%20on%20the%20subjective%20judgment%20and%20experience%20of%20clinicians%2C%20which%20can%0Aintroduce%20variability%20and%20uncertainty%20in%20the%20assessment%20process.%20One%20promising%0Aapplication%20in%20real-life%20situations%20is%20the%20automatic%20estimation%20of%20facial%0Aparalysis.%20However%2C%20the%20scarcity%20of%20facial%20paralysis%20datasets%20limits%20the%0Adevelopment%20of%20robust%20machine%20learning%20models%20for%20automated%20diagnosis%20and%0Atherapeutic%20interventions.%20To%20this%20end%2C%20this%20study%20aims%20to%20synthesize%20a%0Ahigh-quality%20facial%20paralysis%20dataset%20to%20address%20this%20gap%2C%20enabling%20more%0Aaccurate%20and%20efficient%20algorithm%20training.%20Specifically%2C%20a%20novel%20Cycle%0ACross-Fusion%20Expression%20Generative%20Model%20%28CCFExp%29%20based%20on%20the%20diffusion%20model%0Ais%20proposed%20to%20combine%20different%20features%20of%20facial%20information%20and%20enhance%20the%0Avisual%20details%20of%20facial%20appearance%20and%20texture%20in%20facial%20regions%2C%20thus%0Acreating%20synthetic%20facial%20images%20that%20accurately%20represent%20various%20degrees%20and%0Atypes%20of%20facial%20paralysis.%20We%20have%20qualitatively%20and%20quantitatively%20evaluated%0Athe%20proposed%20method%20on%20the%20commonly%20used%20public%20clinical%20datasets%20of%20facial%0Aparalysis%20to%20demonstrate%20its%20effectiveness.%20Experimental%20results%20indicate%20that%0Athe%20proposed%20method%20surpasses%20state-of-the-art%20methods%2C%20generating%20more%0Arealistic%20facial%20images%20and%20maintaining%20identity%20consistency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07271v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCCFExp%253A%2520Facial%2520Image%2520Synthesis%2520with%2520Cycle%2520Cross-Fusion%2520Diffusion%2520Model%250A%2520%2520for%2520Facial%2520Paralysis%2520Individuals%26entry.906535625%3DWeixiang%2520Gao%2520and%2520Yifan%2520Xia%26entry.1292438233%3D%2520%2520Facial%2520paralysis%2520is%2520a%2520debilitating%2520condition%2520that%2520affects%2520the%2520movement%2520of%250Afacial%2520muscles%252C%2520leading%2520to%2520a%2520significant%2520loss%2520of%2520facial%2520expressions.%2520Currently%252C%250Athe%2520diagnosis%2520of%2520facial%2520paralysis%2520remains%2520a%2520challenging%2520task%252C%2520often%2520relying%250Aheavily%2520on%2520the%2520subjective%2520judgment%2520and%2520experience%2520of%2520clinicians%252C%2520which%2520can%250Aintroduce%2520variability%2520and%2520uncertainty%2520in%2520the%2520assessment%2520process.%2520One%2520promising%250Aapplication%2520in%2520real-life%2520situations%2520is%2520the%2520automatic%2520estimation%2520of%2520facial%250Aparalysis.%2520However%252C%2520the%2520scarcity%2520of%2520facial%2520paralysis%2520datasets%2520limits%2520the%250Adevelopment%2520of%2520robust%2520machine%2520learning%2520models%2520for%2520automated%2520diagnosis%2520and%250Atherapeutic%2520interventions.%2520To%2520this%2520end%252C%2520this%2520study%2520aims%2520to%2520synthesize%2520a%250Ahigh-quality%2520facial%2520paralysis%2520dataset%2520to%2520address%2520this%2520gap%252C%2520enabling%2520more%250Aaccurate%2520and%2520efficient%2520algorithm%2520training.%2520Specifically%252C%2520a%2520novel%2520Cycle%250ACross-Fusion%2520Expression%2520Generative%2520Model%2520%2528CCFExp%2529%2520based%2520on%2520the%2520diffusion%2520model%250Ais%2520proposed%2520to%2520combine%2520different%2520features%2520of%2520facial%2520information%2520and%2520enhance%2520the%250Avisual%2520details%2520of%2520facial%2520appearance%2520and%2520texture%2520in%2520facial%2520regions%252C%2520thus%250Acreating%2520synthetic%2520facial%2520images%2520that%2520accurately%2520represent%2520various%2520degrees%2520and%250Atypes%2520of%2520facial%2520paralysis.%2520We%2520have%2520qualitatively%2520and%2520quantitatively%2520evaluated%250Athe%2520proposed%2520method%2520on%2520the%2520commonly%2520used%2520public%2520clinical%2520datasets%2520of%2520facial%250Aparalysis%2520to%2520demonstrate%2520its%2520effectiveness.%2520Experimental%2520results%2520indicate%2520that%250Athe%2520proposed%2520method%2520surpasses%2520state-of-the-art%2520methods%252C%2520generating%2520more%250Arealistic%2520facial%2520images%2520and%2520maintaining%2520identity%2520consistency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07271v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CCFExp%3A%20Facial%20Image%20Synthesis%20with%20Cycle%20Cross-Fusion%20Diffusion%20Model%0A%20%20for%20Facial%20Paralysis%20Individuals&entry.906535625=Weixiang%20Gao%20and%20Yifan%20Xia&entry.1292438233=%20%20Facial%20paralysis%20is%20a%20debilitating%20condition%20that%20affects%20the%20movement%20of%0Afacial%20muscles%2C%20leading%20to%20a%20significant%20loss%20of%20facial%20expressions.%20Currently%2C%0Athe%20diagnosis%20of%20facial%20paralysis%20remains%20a%20challenging%20task%2C%20often%20relying%0Aheavily%20on%20the%20subjective%20judgment%20and%20experience%20of%20clinicians%2C%20which%20can%0Aintroduce%20variability%20and%20uncertainty%20in%20the%20assessment%20process.%20One%20promising%0Aapplication%20in%20real-life%20situations%20is%20the%20automatic%20estimation%20of%20facial%0Aparalysis.%20However%2C%20the%20scarcity%20of%20facial%20paralysis%20datasets%20limits%20the%0Adevelopment%20of%20robust%20machine%20learning%20models%20for%20automated%20diagnosis%20and%0Atherapeutic%20interventions.%20To%20this%20end%2C%20this%20study%20aims%20to%20synthesize%20a%0Ahigh-quality%20facial%20paralysis%20dataset%20to%20address%20this%20gap%2C%20enabling%20more%0Aaccurate%20and%20efficient%20algorithm%20training.%20Specifically%2C%20a%20novel%20Cycle%0ACross-Fusion%20Expression%20Generative%20Model%20%28CCFExp%29%20based%20on%20the%20diffusion%20model%0Ais%20proposed%20to%20combine%20different%20features%20of%20facial%20information%20and%20enhance%20the%0Avisual%20details%20of%20facial%20appearance%20and%20texture%20in%20facial%20regions%2C%20thus%0Acreating%20synthetic%20facial%20images%20that%20accurately%20represent%20various%20degrees%20and%0Atypes%20of%20facial%20paralysis.%20We%20have%20qualitatively%20and%20quantitatively%20evaluated%0Athe%20proposed%20method%20on%20the%20commonly%20used%20public%20clinical%20datasets%20of%20facial%0Aparalysis%20to%20demonstrate%20its%20effectiveness.%20Experimental%20results%20indicate%20that%0Athe%20proposed%20method%20surpasses%20state-of-the-art%20methods%2C%20generating%20more%0Arealistic%20facial%20images%20and%20maintaining%20identity%20consistency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07271v1&entry.124074799=Read"},
{"title": "COCOLA: Coherence-Oriented Contrastive Learning of Musical Audio\n  Representations", "author": "Ruben Ciranni and Giorgio Mariani and Michele Mancusi and Emilian Postolache and Giorgio Fabbro and Emanuele Rodol\u00e0 and Luca Cosmo", "abstract": "  We present COCOLA (Coherence-Oriented Contrastive Learning for Audio), a\ncontrastive learning method for musical audio representations that captures the\nharmonic and rhythmic coherence between samples. Our method operates at the\nlevel of the stems composing music tracks and can input features obtained via\nHarmonic-Percussive Separation (HPS). COCOLA allows the objective evaluation of\ngenerative models for music accompaniment generation, which are difficult to\nbenchmark with established metrics. In this regard, we evaluate recent music\naccompaniment generation models, demonstrating the effectiveness of the\nproposed method. We release the model checkpoints trained on public datasets\ncontaining separate stems (MUSDB18-HQ, MoisesDB, Slakh2100, and CocoChorales).\n", "link": "http://arxiv.org/abs/2404.16969v3", "date": "2024-09-11", "relevancy": 2.2201, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4524}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4477}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COCOLA%3A%20Coherence-Oriented%20Contrastive%20Learning%20of%20Musical%20Audio%0A%20%20Representations&body=Title%3A%20COCOLA%3A%20Coherence-Oriented%20Contrastive%20Learning%20of%20Musical%20Audio%0A%20%20Representations%0AAuthor%3A%20Ruben%20Ciranni%20and%20Giorgio%20Mariani%20and%20Michele%20Mancusi%20and%20Emilian%20Postolache%20and%20Giorgio%20Fabbro%20and%20Emanuele%20Rodol%C3%A0%20and%20Luca%20Cosmo%0AAbstract%3A%20%20%20We%20present%20COCOLA%20%28Coherence-Oriented%20Contrastive%20Learning%20for%20Audio%29%2C%20a%0Acontrastive%20learning%20method%20for%20musical%20audio%20representations%20that%20captures%20the%0Aharmonic%20and%20rhythmic%20coherence%20between%20samples.%20Our%20method%20operates%20at%20the%0Alevel%20of%20the%20stems%20composing%20music%20tracks%20and%20can%20input%20features%20obtained%20via%0AHarmonic-Percussive%20Separation%20%28HPS%29.%20COCOLA%20allows%20the%20objective%20evaluation%20of%0Agenerative%20models%20for%20music%20accompaniment%20generation%2C%20which%20are%20difficult%20to%0Abenchmark%20with%20established%20metrics.%20In%20this%20regard%2C%20we%20evaluate%20recent%20music%0Aaccompaniment%20generation%20models%2C%20demonstrating%20the%20effectiveness%20of%20the%0Aproposed%20method.%20We%20release%20the%20model%20checkpoints%20trained%20on%20public%20datasets%0Acontaining%20separate%20stems%20%28MUSDB18-HQ%2C%20MoisesDB%2C%20Slakh2100%2C%20and%20CocoChorales%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16969v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOCOLA%253A%2520Coherence-Oriented%2520Contrastive%2520Learning%2520of%2520Musical%2520Audio%250A%2520%2520Representations%26entry.906535625%3DRuben%2520Ciranni%2520and%2520Giorgio%2520Mariani%2520and%2520Michele%2520Mancusi%2520and%2520Emilian%2520Postolache%2520and%2520Giorgio%2520Fabbro%2520and%2520Emanuele%2520Rodol%25C3%25A0%2520and%2520Luca%2520Cosmo%26entry.1292438233%3D%2520%2520We%2520present%2520COCOLA%2520%2528Coherence-Oriented%2520Contrastive%2520Learning%2520for%2520Audio%2529%252C%2520a%250Acontrastive%2520learning%2520method%2520for%2520musical%2520audio%2520representations%2520that%2520captures%2520the%250Aharmonic%2520and%2520rhythmic%2520coherence%2520between%2520samples.%2520Our%2520method%2520operates%2520at%2520the%250Alevel%2520of%2520the%2520stems%2520composing%2520music%2520tracks%2520and%2520can%2520input%2520features%2520obtained%2520via%250AHarmonic-Percussive%2520Separation%2520%2528HPS%2529.%2520COCOLA%2520allows%2520the%2520objective%2520evaluation%2520of%250Agenerative%2520models%2520for%2520music%2520accompaniment%2520generation%252C%2520which%2520are%2520difficult%2520to%250Abenchmark%2520with%2520established%2520metrics.%2520In%2520this%2520regard%252C%2520we%2520evaluate%2520recent%2520music%250Aaccompaniment%2520generation%2520models%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520method.%2520We%2520release%2520the%2520model%2520checkpoints%2520trained%2520on%2520public%2520datasets%250Acontaining%2520separate%2520stems%2520%2528MUSDB18-HQ%252C%2520MoisesDB%252C%2520Slakh2100%252C%2520and%2520CocoChorales%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.16969v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COCOLA%3A%20Coherence-Oriented%20Contrastive%20Learning%20of%20Musical%20Audio%0A%20%20Representations&entry.906535625=Ruben%20Ciranni%20and%20Giorgio%20Mariani%20and%20Michele%20Mancusi%20and%20Emilian%20Postolache%20and%20Giorgio%20Fabbro%20and%20Emanuele%20Rodol%C3%A0%20and%20Luca%20Cosmo&entry.1292438233=%20%20We%20present%20COCOLA%20%28Coherence-Oriented%20Contrastive%20Learning%20for%20Audio%29%2C%20a%0Acontrastive%20learning%20method%20for%20musical%20audio%20representations%20that%20captures%20the%0Aharmonic%20and%20rhythmic%20coherence%20between%20samples.%20Our%20method%20operates%20at%20the%0Alevel%20of%20the%20stems%20composing%20music%20tracks%20and%20can%20input%20features%20obtained%20via%0AHarmonic-Percussive%20Separation%20%28HPS%29.%20COCOLA%20allows%20the%20objective%20evaluation%20of%0Agenerative%20models%20for%20music%20accompaniment%20generation%2C%20which%20are%20difficult%20to%0Abenchmark%20with%20established%20metrics.%20In%20this%20regard%2C%20we%20evaluate%20recent%20music%0Aaccompaniment%20generation%20models%2C%20demonstrating%20the%20effectiveness%20of%20the%0Aproposed%20method.%20We%20release%20the%20model%20checkpoints%20trained%20on%20public%20datasets%0Acontaining%20separate%20stems%20%28MUSDB18-HQ%2C%20MoisesDB%2C%20Slakh2100%2C%20and%20CocoChorales%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16969v3&entry.124074799=Read"},
{"title": "One-Shot Diffusion Mimicker for Handwritten Text Generation", "author": "Gang Dai and Yifan Zhang and Quhui Ke and Qiangya Guo and Shuangping Huang", "abstract": "  Existing handwritten text generation methods often require more than ten\nhandwriting samples as style references. However, in practical applications,\nusers tend to prefer a handwriting generation model that operates with just a\nsingle reference sample for its convenience and efficiency. This approach,\nknown as \"one-shot generation\", significantly simplifies the process but poses\na significant challenge due to the difficulty of accurately capturing a\nwriter's style from a single sample, especially when extracting fine details\nfrom the characters' edges amidst sparse foreground and undesired background\nnoise. To address this problem, we propose a One-shot Diffusion Mimicker\n(One-DM) to generate handwritten text that can mimic any calligraphic style\nwith only one reference sample. Inspired by the fact that high-frequency\ninformation of the individual sample often contains distinct style patterns\n(e.g., character slant and letter joining), we develop a novel style-enhanced\nmodule to improve the style extraction by incorporating high-frequency\ncomponents from a single sample. We then fuse the style features with the text\ncontent as a merged condition for guiding the diffusion model to produce\nhigh-quality handwritten text images. Extensive experiments demonstrate that\nour method can successfully generate handwriting scripts with just one sample\nreference in multiple languages, even outperforming previous methods using over\nten samples. Our source code is available at\nhttps://github.com/dailenson/One-DM.\n", "link": "http://arxiv.org/abs/2409.04004v2", "date": "2024-09-11", "relevancy": 2.2147, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5568}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5561}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.55}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One-Shot%20Diffusion%20Mimicker%20for%20Handwritten%20Text%20Generation&body=Title%3A%20One-Shot%20Diffusion%20Mimicker%20for%20Handwritten%20Text%20Generation%0AAuthor%3A%20Gang%20Dai%20and%20Yifan%20Zhang%20and%20Quhui%20Ke%20and%20Qiangya%20Guo%20and%20Shuangping%20Huang%0AAbstract%3A%20%20%20Existing%20handwritten%20text%20generation%20methods%20often%20require%20more%20than%20ten%0Ahandwriting%20samples%20as%20style%20references.%20However%2C%20in%20practical%20applications%2C%0Ausers%20tend%20to%20prefer%20a%20handwriting%20generation%20model%20that%20operates%20with%20just%20a%0Asingle%20reference%20sample%20for%20its%20convenience%20and%20efficiency.%20This%20approach%2C%0Aknown%20as%20%22one-shot%20generation%22%2C%20significantly%20simplifies%20the%20process%20but%20poses%0Aa%20significant%20challenge%20due%20to%20the%20difficulty%20of%20accurately%20capturing%20a%0Awriter%27s%20style%20from%20a%20single%20sample%2C%20especially%20when%20extracting%20fine%20details%0Afrom%20the%20characters%27%20edges%20amidst%20sparse%20foreground%20and%20undesired%20background%0Anoise.%20To%20address%20this%20problem%2C%20we%20propose%20a%20One-shot%20Diffusion%20Mimicker%0A%28One-DM%29%20to%20generate%20handwritten%20text%20that%20can%20mimic%20any%20calligraphic%20style%0Awith%20only%20one%20reference%20sample.%20Inspired%20by%20the%20fact%20that%20high-frequency%0Ainformation%20of%20the%20individual%20sample%20often%20contains%20distinct%20style%20patterns%0A%28e.g.%2C%20character%20slant%20and%20letter%20joining%29%2C%20we%20develop%20a%20novel%20style-enhanced%0Amodule%20to%20improve%20the%20style%20extraction%20by%20incorporating%20high-frequency%0Acomponents%20from%20a%20single%20sample.%20We%20then%20fuse%20the%20style%20features%20with%20the%20text%0Acontent%20as%20a%20merged%20condition%20for%20guiding%20the%20diffusion%20model%20to%20produce%0Ahigh-quality%20handwritten%20text%20images.%20Extensive%20experiments%20demonstrate%20that%0Aour%20method%20can%20successfully%20generate%20handwriting%20scripts%20with%20just%20one%20sample%0Areference%20in%20multiple%20languages%2C%20even%20outperforming%20previous%20methods%20using%20over%0Aten%20samples.%20Our%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/dailenson/One-DM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04004v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne-Shot%2520Diffusion%2520Mimicker%2520for%2520Handwritten%2520Text%2520Generation%26entry.906535625%3DGang%2520Dai%2520and%2520Yifan%2520Zhang%2520and%2520Quhui%2520Ke%2520and%2520Qiangya%2520Guo%2520and%2520Shuangping%2520Huang%26entry.1292438233%3D%2520%2520Existing%2520handwritten%2520text%2520generation%2520methods%2520often%2520require%2520more%2520than%2520ten%250Ahandwriting%2520samples%2520as%2520style%2520references.%2520However%252C%2520in%2520practical%2520applications%252C%250Ausers%2520tend%2520to%2520prefer%2520a%2520handwriting%2520generation%2520model%2520that%2520operates%2520with%2520just%2520a%250Asingle%2520reference%2520sample%2520for%2520its%2520convenience%2520and%2520efficiency.%2520This%2520approach%252C%250Aknown%2520as%2520%2522one-shot%2520generation%2522%252C%2520significantly%2520simplifies%2520the%2520process%2520but%2520poses%250Aa%2520significant%2520challenge%2520due%2520to%2520the%2520difficulty%2520of%2520accurately%2520capturing%2520a%250Awriter%2527s%2520style%2520from%2520a%2520single%2520sample%252C%2520especially%2520when%2520extracting%2520fine%2520details%250Afrom%2520the%2520characters%2527%2520edges%2520amidst%2520sparse%2520foreground%2520and%2520undesired%2520background%250Anoise.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520a%2520One-shot%2520Diffusion%2520Mimicker%250A%2528One-DM%2529%2520to%2520generate%2520handwritten%2520text%2520that%2520can%2520mimic%2520any%2520calligraphic%2520style%250Awith%2520only%2520one%2520reference%2520sample.%2520Inspired%2520by%2520the%2520fact%2520that%2520high-frequency%250Ainformation%2520of%2520the%2520individual%2520sample%2520often%2520contains%2520distinct%2520style%2520patterns%250A%2528e.g.%252C%2520character%2520slant%2520and%2520letter%2520joining%2529%252C%2520we%2520develop%2520a%2520novel%2520style-enhanced%250Amodule%2520to%2520improve%2520the%2520style%2520extraction%2520by%2520incorporating%2520high-frequency%250Acomponents%2520from%2520a%2520single%2520sample.%2520We%2520then%2520fuse%2520the%2520style%2520features%2520with%2520the%2520text%250Acontent%2520as%2520a%2520merged%2520condition%2520for%2520guiding%2520the%2520diffusion%2520model%2520to%2520produce%250Ahigh-quality%2520handwritten%2520text%2520images.%2520Extensive%2520experiments%2520demonstrate%2520that%250Aour%2520method%2520can%2520successfully%2520generate%2520handwriting%2520scripts%2520with%2520just%2520one%2520sample%250Areference%2520in%2520multiple%2520languages%252C%2520even%2520outperforming%2520previous%2520methods%2520using%2520over%250Aten%2520samples.%2520Our%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/dailenson/One-DM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04004v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One-Shot%20Diffusion%20Mimicker%20for%20Handwritten%20Text%20Generation&entry.906535625=Gang%20Dai%20and%20Yifan%20Zhang%20and%20Quhui%20Ke%20and%20Qiangya%20Guo%20and%20Shuangping%20Huang&entry.1292438233=%20%20Existing%20handwritten%20text%20generation%20methods%20often%20require%20more%20than%20ten%0Ahandwriting%20samples%20as%20style%20references.%20However%2C%20in%20practical%20applications%2C%0Ausers%20tend%20to%20prefer%20a%20handwriting%20generation%20model%20that%20operates%20with%20just%20a%0Asingle%20reference%20sample%20for%20its%20convenience%20and%20efficiency.%20This%20approach%2C%0Aknown%20as%20%22one-shot%20generation%22%2C%20significantly%20simplifies%20the%20process%20but%20poses%0Aa%20significant%20challenge%20due%20to%20the%20difficulty%20of%20accurately%20capturing%20a%0Awriter%27s%20style%20from%20a%20single%20sample%2C%20especially%20when%20extracting%20fine%20details%0Afrom%20the%20characters%27%20edges%20amidst%20sparse%20foreground%20and%20undesired%20background%0Anoise.%20To%20address%20this%20problem%2C%20we%20propose%20a%20One-shot%20Diffusion%20Mimicker%0A%28One-DM%29%20to%20generate%20handwritten%20text%20that%20can%20mimic%20any%20calligraphic%20style%0Awith%20only%20one%20reference%20sample.%20Inspired%20by%20the%20fact%20that%20high-frequency%0Ainformation%20of%20the%20individual%20sample%20often%20contains%20distinct%20style%20patterns%0A%28e.g.%2C%20character%20slant%20and%20letter%20joining%29%2C%20we%20develop%20a%20novel%20style-enhanced%0Amodule%20to%20improve%20the%20style%20extraction%20by%20incorporating%20high-frequency%0Acomponents%20from%20a%20single%20sample.%20We%20then%20fuse%20the%20style%20features%20with%20the%20text%0Acontent%20as%20a%20merged%20condition%20for%20guiding%20the%20diffusion%20model%20to%20produce%0Ahigh-quality%20handwritten%20text%20images.%20Extensive%20experiments%20demonstrate%20that%0Aour%20method%20can%20successfully%20generate%20handwriting%20scripts%20with%20just%20one%20sample%0Areference%20in%20multiple%20languages%2C%20even%20outperforming%20previous%20methods%20using%20over%0Aten%20samples.%20Our%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/dailenson/One-DM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04004v2&entry.124074799=Read"},
{"title": "Application of Langevin Dynamics to Advance the Quantum Natural Gradient\n  Optimization Algorithm", "author": "Oleksandr Borysenko and Mykhailo Bratchenko and Ilya Lukin and Mykola Luhanko and Ihor Omelchenko and Andrii Sotnikov and Alessandro Lomi", "abstract": "  A Quantum Natural Gradient (QNG) algorithm for optimization of variational\nquantum circuits has been proposed recently. In this study, we employ the\nLangevin equation with a QNG stochastic force to demonstrate that its\ndiscrete-time solution gives a generalized form of the above-specified\nalgorithm, which we call Momentum-QNG. Similar to other optimization algorithms\nwith the momentum term, such as the Stochastic Gradient Descent with momentum,\nRMSProp with momentum and Adam, Momentum-QNG is more effective to escape local\nminima and plateaus in the variational parameter space and, therefore, achieves\na better convergence behavior compared to the basic QNG. Our open-source code\nis available at https://github.com/borbysh/Momentum-QNG\n", "link": "http://arxiv.org/abs/2409.01978v2", "date": "2024-09-11", "relevancy": 2.2069, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4533}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4378}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Application%20of%20Langevin%20Dynamics%20to%20Advance%20the%20Quantum%20Natural%20Gradient%0A%20%20Optimization%20Algorithm&body=Title%3A%20Application%20of%20Langevin%20Dynamics%20to%20Advance%20the%20Quantum%20Natural%20Gradient%0A%20%20Optimization%20Algorithm%0AAuthor%3A%20Oleksandr%20Borysenko%20and%20Mykhailo%20Bratchenko%20and%20Ilya%20Lukin%20and%20Mykola%20Luhanko%20and%20Ihor%20Omelchenko%20and%20Andrii%20Sotnikov%20and%20Alessandro%20Lomi%0AAbstract%3A%20%20%20A%20Quantum%20Natural%20Gradient%20%28QNG%29%20algorithm%20for%20optimization%20of%20variational%0Aquantum%20circuits%20has%20been%20proposed%20recently.%20In%20this%20study%2C%20we%20employ%20the%0ALangevin%20equation%20with%20a%20QNG%20stochastic%20force%20to%20demonstrate%20that%20its%0Adiscrete-time%20solution%20gives%20a%20generalized%20form%20of%20the%20above-specified%0Aalgorithm%2C%20which%20we%20call%20Momentum-QNG.%20Similar%20to%20other%20optimization%20algorithms%0Awith%20the%20momentum%20term%2C%20such%20as%20the%20Stochastic%20Gradient%20Descent%20with%20momentum%2C%0ARMSProp%20with%20momentum%20and%20Adam%2C%20Momentum-QNG%20is%20more%20effective%20to%20escape%20local%0Aminima%20and%20plateaus%20in%20the%20variational%20parameter%20space%20and%2C%20therefore%2C%20achieves%0Aa%20better%20convergence%20behavior%20compared%20to%20the%20basic%20QNG.%20Our%20open-source%20code%0Ais%20available%20at%20https%3A//github.com/borbysh/Momentum-QNG%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.01978v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApplication%2520of%2520Langevin%2520Dynamics%2520to%2520Advance%2520the%2520Quantum%2520Natural%2520Gradient%250A%2520%2520Optimization%2520Algorithm%26entry.906535625%3DOleksandr%2520Borysenko%2520and%2520Mykhailo%2520Bratchenko%2520and%2520Ilya%2520Lukin%2520and%2520Mykola%2520Luhanko%2520and%2520Ihor%2520Omelchenko%2520and%2520Andrii%2520Sotnikov%2520and%2520Alessandro%2520Lomi%26entry.1292438233%3D%2520%2520A%2520Quantum%2520Natural%2520Gradient%2520%2528QNG%2529%2520algorithm%2520for%2520optimization%2520of%2520variational%250Aquantum%2520circuits%2520has%2520been%2520proposed%2520recently.%2520In%2520this%2520study%252C%2520we%2520employ%2520the%250ALangevin%2520equation%2520with%2520a%2520QNG%2520stochastic%2520force%2520to%2520demonstrate%2520that%2520its%250Adiscrete-time%2520solution%2520gives%2520a%2520generalized%2520form%2520of%2520the%2520above-specified%250Aalgorithm%252C%2520which%2520we%2520call%2520Momentum-QNG.%2520Similar%2520to%2520other%2520optimization%2520algorithms%250Awith%2520the%2520momentum%2520term%252C%2520such%2520as%2520the%2520Stochastic%2520Gradient%2520Descent%2520with%2520momentum%252C%250ARMSProp%2520with%2520momentum%2520and%2520Adam%252C%2520Momentum-QNG%2520is%2520more%2520effective%2520to%2520escape%2520local%250Aminima%2520and%2520plateaus%2520in%2520the%2520variational%2520parameter%2520space%2520and%252C%2520therefore%252C%2520achieves%250Aa%2520better%2520convergence%2520behavior%2520compared%2520to%2520the%2520basic%2520QNG.%2520Our%2520open-source%2520code%250Ais%2520available%2520at%2520https%253A//github.com/borbysh/Momentum-QNG%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.01978v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Application%20of%20Langevin%20Dynamics%20to%20Advance%20the%20Quantum%20Natural%20Gradient%0A%20%20Optimization%20Algorithm&entry.906535625=Oleksandr%20Borysenko%20and%20Mykhailo%20Bratchenko%20and%20Ilya%20Lukin%20and%20Mykola%20Luhanko%20and%20Ihor%20Omelchenko%20and%20Andrii%20Sotnikov%20and%20Alessandro%20Lomi&entry.1292438233=%20%20A%20Quantum%20Natural%20Gradient%20%28QNG%29%20algorithm%20for%20optimization%20of%20variational%0Aquantum%20circuits%20has%20been%20proposed%20recently.%20In%20this%20study%2C%20we%20employ%20the%0ALangevin%20equation%20with%20a%20QNG%20stochastic%20force%20to%20demonstrate%20that%20its%0Adiscrete-time%20solution%20gives%20a%20generalized%20form%20of%20the%20above-specified%0Aalgorithm%2C%20which%20we%20call%20Momentum-QNG.%20Similar%20to%20other%20optimization%20algorithms%0Awith%20the%20momentum%20term%2C%20such%20as%20the%20Stochastic%20Gradient%20Descent%20with%20momentum%2C%0ARMSProp%20with%20momentum%20and%20Adam%2C%20Momentum-QNG%20is%20more%20effective%20to%20escape%20local%0Aminima%20and%20plateaus%20in%20the%20variational%20parameter%20space%20and%2C%20therefore%2C%20achieves%0Aa%20better%20convergence%20behavior%20compared%20to%20the%20basic%20QNG.%20Our%20open-source%20code%0Ais%20available%20at%20https%3A//github.com/borbysh/Momentum-QNG%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.01978v2&entry.124074799=Read"},
{"title": "MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical\n  Applications", "author": "Praveen K Kanithi and Cl\u00e9ment Christophe and Marco AF Pimentel and Tathagata Raha and Nada Saadi and Hamza Javed and Svetlana Maslenkova and Nasir Hayat and Ronnie Rajan and Shadab Khan", "abstract": "  The rapid development of Large Language Models (LLMs) for healthcare\napplications has spurred calls for holistic evaluation beyond frequently-cited\nbenchmarks like USMLE, to better reflect real-world performance. While\nreal-world assessments are valuable indicators of utility, they often lag\nbehind the pace of LLM evolution, likely rendering findings obsolete upon\ndeployment. This temporal disconnect necessitates a comprehensive upfront\nevaluation that can guide model selection for specific clinical applications.\nWe introduce MEDIC, a framework assessing LLMs across five critical dimensions\nof clinical competence: medical reasoning, ethics and bias, data and language\nunderstanding, in-context learning, and clinical safety. MEDIC features a novel\ncross-examination framework quantifying LLM performance across areas like\ncoverage and hallucination detection, without requiring reference outputs. We\napply MEDIC to evaluate LLMs on medical question-answering, safety,\nsummarization, note generation, and other tasks. Our results show performance\ndisparities across model sizes, baseline vs medically finetuned models, and\nhave implications on model selection for applications requiring specific model\nstrengths, such as low hallucination or lower cost of inference. MEDIC's\nmultifaceted evaluation reveals these performance trade-offs, bridging the gap\nbetween theoretical capabilities and practical implementation in healthcare\nsettings, ensuring that the most promising models are identified and adapted\nfor diverse healthcare applications.\n", "link": "http://arxiv.org/abs/2409.07314v1", "date": "2024-09-11", "relevancy": 2.1595, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5875}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5304}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MEDIC%3A%20Towards%20a%20Comprehensive%20Framework%20for%20Evaluating%20LLMs%20in%20Clinical%0A%20%20Applications&body=Title%3A%20MEDIC%3A%20Towards%20a%20Comprehensive%20Framework%20for%20Evaluating%20LLMs%20in%20Clinical%0A%20%20Applications%0AAuthor%3A%20Praveen%20K%20Kanithi%20and%20Cl%C3%A9ment%20Christophe%20and%20Marco%20AF%20Pimentel%20and%20Tathagata%20Raha%20and%20Nada%20Saadi%20and%20Hamza%20Javed%20and%20Svetlana%20Maslenkova%20and%20Nasir%20Hayat%20and%20Ronnie%20Rajan%20and%20Shadab%20Khan%0AAbstract%3A%20%20%20The%20rapid%20development%20of%20Large%20Language%20Models%20%28LLMs%29%20for%20healthcare%0Aapplications%20has%20spurred%20calls%20for%20holistic%20evaluation%20beyond%20frequently-cited%0Abenchmarks%20like%20USMLE%2C%20to%20better%20reflect%20real-world%20performance.%20While%0Areal-world%20assessments%20are%20valuable%20indicators%20of%20utility%2C%20they%20often%20lag%0Abehind%20the%20pace%20of%20LLM%20evolution%2C%20likely%20rendering%20findings%20obsolete%20upon%0Adeployment.%20This%20temporal%20disconnect%20necessitates%20a%20comprehensive%20upfront%0Aevaluation%20that%20can%20guide%20model%20selection%20for%20specific%20clinical%20applications.%0AWe%20introduce%20MEDIC%2C%20a%20framework%20assessing%20LLMs%20across%20five%20critical%20dimensions%0Aof%20clinical%20competence%3A%20medical%20reasoning%2C%20ethics%20and%20bias%2C%20data%20and%20language%0Aunderstanding%2C%20in-context%20learning%2C%20and%20clinical%20safety.%20MEDIC%20features%20a%20novel%0Across-examination%20framework%20quantifying%20LLM%20performance%20across%20areas%20like%0Acoverage%20and%20hallucination%20detection%2C%20without%20requiring%20reference%20outputs.%20We%0Aapply%20MEDIC%20to%20evaluate%20LLMs%20on%20medical%20question-answering%2C%20safety%2C%0Asummarization%2C%20note%20generation%2C%20and%20other%20tasks.%20Our%20results%20show%20performance%0Adisparities%20across%20model%20sizes%2C%20baseline%20vs%20medically%20finetuned%20models%2C%20and%0Ahave%20implications%20on%20model%20selection%20for%20applications%20requiring%20specific%20model%0Astrengths%2C%20such%20as%20low%20hallucination%20or%20lower%20cost%20of%20inference.%20MEDIC%27s%0Amultifaceted%20evaluation%20reveals%20these%20performance%20trade-offs%2C%20bridging%20the%20gap%0Abetween%20theoretical%20capabilities%20and%20practical%20implementation%20in%20healthcare%0Asettings%2C%20ensuring%20that%20the%20most%20promising%20models%20are%20identified%20and%20adapted%0Afor%20diverse%20healthcare%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07314v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMEDIC%253A%2520Towards%2520a%2520Comprehensive%2520Framework%2520for%2520Evaluating%2520LLMs%2520in%2520Clinical%250A%2520%2520Applications%26entry.906535625%3DPraveen%2520K%2520Kanithi%2520and%2520Cl%25C3%25A9ment%2520Christophe%2520and%2520Marco%2520AF%2520Pimentel%2520and%2520Tathagata%2520Raha%2520and%2520Nada%2520Saadi%2520and%2520Hamza%2520Javed%2520and%2520Svetlana%2520Maslenkova%2520and%2520Nasir%2520Hayat%2520and%2520Ronnie%2520Rajan%2520and%2520Shadab%2520Khan%26entry.1292438233%3D%2520%2520The%2520rapid%2520development%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520for%2520healthcare%250Aapplications%2520has%2520spurred%2520calls%2520for%2520holistic%2520evaluation%2520beyond%2520frequently-cited%250Abenchmarks%2520like%2520USMLE%252C%2520to%2520better%2520reflect%2520real-world%2520performance.%2520While%250Areal-world%2520assessments%2520are%2520valuable%2520indicators%2520of%2520utility%252C%2520they%2520often%2520lag%250Abehind%2520the%2520pace%2520of%2520LLM%2520evolution%252C%2520likely%2520rendering%2520findings%2520obsolete%2520upon%250Adeployment.%2520This%2520temporal%2520disconnect%2520necessitates%2520a%2520comprehensive%2520upfront%250Aevaluation%2520that%2520can%2520guide%2520model%2520selection%2520for%2520specific%2520clinical%2520applications.%250AWe%2520introduce%2520MEDIC%252C%2520a%2520framework%2520assessing%2520LLMs%2520across%2520five%2520critical%2520dimensions%250Aof%2520clinical%2520competence%253A%2520medical%2520reasoning%252C%2520ethics%2520and%2520bias%252C%2520data%2520and%2520language%250Aunderstanding%252C%2520in-context%2520learning%252C%2520and%2520clinical%2520safety.%2520MEDIC%2520features%2520a%2520novel%250Across-examination%2520framework%2520quantifying%2520LLM%2520performance%2520across%2520areas%2520like%250Acoverage%2520and%2520hallucination%2520detection%252C%2520without%2520requiring%2520reference%2520outputs.%2520We%250Aapply%2520MEDIC%2520to%2520evaluate%2520LLMs%2520on%2520medical%2520question-answering%252C%2520safety%252C%250Asummarization%252C%2520note%2520generation%252C%2520and%2520other%2520tasks.%2520Our%2520results%2520show%2520performance%250Adisparities%2520across%2520model%2520sizes%252C%2520baseline%2520vs%2520medically%2520finetuned%2520models%252C%2520and%250Ahave%2520implications%2520on%2520model%2520selection%2520for%2520applications%2520requiring%2520specific%2520model%250Astrengths%252C%2520such%2520as%2520low%2520hallucination%2520or%2520lower%2520cost%2520of%2520inference.%2520MEDIC%2527s%250Amultifaceted%2520evaluation%2520reveals%2520these%2520performance%2520trade-offs%252C%2520bridging%2520the%2520gap%250Abetween%2520theoretical%2520capabilities%2520and%2520practical%2520implementation%2520in%2520healthcare%250Asettings%252C%2520ensuring%2520that%2520the%2520most%2520promising%2520models%2520are%2520identified%2520and%2520adapted%250Afor%2520diverse%2520healthcare%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07314v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEDIC%3A%20Towards%20a%20Comprehensive%20Framework%20for%20Evaluating%20LLMs%20in%20Clinical%0A%20%20Applications&entry.906535625=Praveen%20K%20Kanithi%20and%20Cl%C3%A9ment%20Christophe%20and%20Marco%20AF%20Pimentel%20and%20Tathagata%20Raha%20and%20Nada%20Saadi%20and%20Hamza%20Javed%20and%20Svetlana%20Maslenkova%20and%20Nasir%20Hayat%20and%20Ronnie%20Rajan%20and%20Shadab%20Khan&entry.1292438233=%20%20The%20rapid%20development%20of%20Large%20Language%20Models%20%28LLMs%29%20for%20healthcare%0Aapplications%20has%20spurred%20calls%20for%20holistic%20evaluation%20beyond%20frequently-cited%0Abenchmarks%20like%20USMLE%2C%20to%20better%20reflect%20real-world%20performance.%20While%0Areal-world%20assessments%20are%20valuable%20indicators%20of%20utility%2C%20they%20often%20lag%0Abehind%20the%20pace%20of%20LLM%20evolution%2C%20likely%20rendering%20findings%20obsolete%20upon%0Adeployment.%20This%20temporal%20disconnect%20necessitates%20a%20comprehensive%20upfront%0Aevaluation%20that%20can%20guide%20model%20selection%20for%20specific%20clinical%20applications.%0AWe%20introduce%20MEDIC%2C%20a%20framework%20assessing%20LLMs%20across%20five%20critical%20dimensions%0Aof%20clinical%20competence%3A%20medical%20reasoning%2C%20ethics%20and%20bias%2C%20data%20and%20language%0Aunderstanding%2C%20in-context%20learning%2C%20and%20clinical%20safety.%20MEDIC%20features%20a%20novel%0Across-examination%20framework%20quantifying%20LLM%20performance%20across%20areas%20like%0Acoverage%20and%20hallucination%20detection%2C%20without%20requiring%20reference%20outputs.%20We%0Aapply%20MEDIC%20to%20evaluate%20LLMs%20on%20medical%20question-answering%2C%20safety%2C%0Asummarization%2C%20note%20generation%2C%20and%20other%20tasks.%20Our%20results%20show%20performance%0Adisparities%20across%20model%20sizes%2C%20baseline%20vs%20medically%20finetuned%20models%2C%20and%0Ahave%20implications%20on%20model%20selection%20for%20applications%20requiring%20specific%20model%0Astrengths%2C%20such%20as%20low%20hallucination%20or%20lower%20cost%20of%20inference.%20MEDIC%27s%0Amultifaceted%20evaluation%20reveals%20these%20performance%20trade-offs%2C%20bridging%20the%20gap%0Abetween%20theoretical%20capabilities%20and%20practical%20implementation%20in%20healthcare%0Asettings%2C%20ensuring%20that%20the%20most%20promising%20models%20are%20identified%20and%20adapted%0Afor%20diverse%20healthcare%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07314v1&entry.124074799=Read"},
{"title": "Module-wise Adaptive Adversarial Training for End-to-end Autonomous\n  Driving", "author": "Tianyuan Zhang and Lu Wang and Jiaqi Kang and Xinwei Zhang and Siyuan Liang and Yuwei Chen and Aishan Liu and Xianglong Liu", "abstract": "  Recent advances in deep learning have markedly improved autonomous driving\n(AD) models, particularly end-to-end systems that integrate perception,\nprediction, and planning stages, achieving state-of-the-art performance.\nHowever, these models remain vulnerable to adversarial attacks, where\nhuman-imperceptible perturbations can disrupt decision-making processes. While\nadversarial training is an effective method for enhancing model robustness\nagainst such attacks, no prior studies have focused on its application to\nend-to-end AD models. In this paper, we take the first step in adversarial\ntraining for end-to-end AD models and present a novel Module-wise Adaptive\nAdversarial Training (MA2T). However, extending conventional adversarial\ntraining to this context is highly non-trivial, as different stages within the\nmodel have distinct objectives and are strongly interconnected. To address\nthese challenges, MA2T first introduces Module-wise Noise Injection, which\ninjects noise before the input of different modules, targeting training models\nwith the guidance of overall objectives rather than each independent module\nloss. Additionally, we introduce Dynamic Weight Accumulation Adaptation, which\nincorporates accumulated weight changes to adaptively learn and adjust the loss\nweights of each module based on their contributions (accumulated reduction\nrates) for better balance and robust training. To demonstrate the efficacy of\nour defense, we conduct extensive experiments on the widely-used nuScenes\ndataset across several end-to-end AD models under both white-box and black-box\nattacks, where our method outperforms other baselines by large margins\n(+5-10%). Moreover, we validate the robustness of our defense through\nclosed-loop evaluation in the CARLA simulation environment, showing improved\nresilience even against natural corruption.\n", "link": "http://arxiv.org/abs/2409.07321v1", "date": "2024-09-11", "relevancy": 2.1209, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5401}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5251}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Module-wise%20Adaptive%20Adversarial%20Training%20for%20End-to-end%20Autonomous%0A%20%20Driving&body=Title%3A%20Module-wise%20Adaptive%20Adversarial%20Training%20for%20End-to-end%20Autonomous%0A%20%20Driving%0AAuthor%3A%20Tianyuan%20Zhang%20and%20Lu%20Wang%20and%20Jiaqi%20Kang%20and%20Xinwei%20Zhang%20and%20Siyuan%20Liang%20and%20Yuwei%20Chen%20and%20Aishan%20Liu%20and%20Xianglong%20Liu%0AAbstract%3A%20%20%20Recent%20advances%20in%20deep%20learning%20have%20markedly%20improved%20autonomous%20driving%0A%28AD%29%20models%2C%20particularly%20end-to-end%20systems%20that%20integrate%20perception%2C%0Aprediction%2C%20and%20planning%20stages%2C%20achieving%20state-of-the-art%20performance.%0AHowever%2C%20these%20models%20remain%20vulnerable%20to%20adversarial%20attacks%2C%20where%0Ahuman-imperceptible%20perturbations%20can%20disrupt%20decision-making%20processes.%20While%0Aadversarial%20training%20is%20an%20effective%20method%20for%20enhancing%20model%20robustness%0Aagainst%20such%20attacks%2C%20no%20prior%20studies%20have%20focused%20on%20its%20application%20to%0Aend-to-end%20AD%20models.%20In%20this%20paper%2C%20we%20take%20the%20first%20step%20in%20adversarial%0Atraining%20for%20end-to-end%20AD%20models%20and%20present%20a%20novel%20Module-wise%20Adaptive%0AAdversarial%20Training%20%28MA2T%29.%20However%2C%20extending%20conventional%20adversarial%0Atraining%20to%20this%20context%20is%20highly%20non-trivial%2C%20as%20different%20stages%20within%20the%0Amodel%20have%20distinct%20objectives%20and%20are%20strongly%20interconnected.%20To%20address%0Athese%20challenges%2C%20MA2T%20first%20introduces%20Module-wise%20Noise%20Injection%2C%20which%0Ainjects%20noise%20before%20the%20input%20of%20different%20modules%2C%20targeting%20training%20models%0Awith%20the%20guidance%20of%20overall%20objectives%20rather%20than%20each%20independent%20module%0Aloss.%20Additionally%2C%20we%20introduce%20Dynamic%20Weight%20Accumulation%20Adaptation%2C%20which%0Aincorporates%20accumulated%20weight%20changes%20to%20adaptively%20learn%20and%20adjust%20the%20loss%0Aweights%20of%20each%20module%20based%20on%20their%20contributions%20%28accumulated%20reduction%0Arates%29%20for%20better%20balance%20and%20robust%20training.%20To%20demonstrate%20the%20efficacy%20of%0Aour%20defense%2C%20we%20conduct%20extensive%20experiments%20on%20the%20widely-used%20nuScenes%0Adataset%20across%20several%20end-to-end%20AD%20models%20under%20both%20white-box%20and%20black-box%0Aattacks%2C%20where%20our%20method%20outperforms%20other%20baselines%20by%20large%20margins%0A%28%2B5-10%25%29.%20Moreover%2C%20we%20validate%20the%20robustness%20of%20our%20defense%20through%0Aclosed-loop%20evaluation%20in%20the%20CARLA%20simulation%20environment%2C%20showing%20improved%0Aresilience%20even%20against%20natural%20corruption.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07321v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModule-wise%2520Adaptive%2520Adversarial%2520Training%2520for%2520End-to-end%2520Autonomous%250A%2520%2520Driving%26entry.906535625%3DTianyuan%2520Zhang%2520and%2520Lu%2520Wang%2520and%2520Jiaqi%2520Kang%2520and%2520Xinwei%2520Zhang%2520and%2520Siyuan%2520Liang%2520and%2520Yuwei%2520Chen%2520and%2520Aishan%2520Liu%2520and%2520Xianglong%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520deep%2520learning%2520have%2520markedly%2520improved%2520autonomous%2520driving%250A%2528AD%2529%2520models%252C%2520particularly%2520end-to-end%2520systems%2520that%2520integrate%2520perception%252C%250Aprediction%252C%2520and%2520planning%2520stages%252C%2520achieving%2520state-of-the-art%2520performance.%250AHowever%252C%2520these%2520models%2520remain%2520vulnerable%2520to%2520adversarial%2520attacks%252C%2520where%250Ahuman-imperceptible%2520perturbations%2520can%2520disrupt%2520decision-making%2520processes.%2520While%250Aadversarial%2520training%2520is%2520an%2520effective%2520method%2520for%2520enhancing%2520model%2520robustness%250Aagainst%2520such%2520attacks%252C%2520no%2520prior%2520studies%2520have%2520focused%2520on%2520its%2520application%2520to%250Aend-to-end%2520AD%2520models.%2520In%2520this%2520paper%252C%2520we%2520take%2520the%2520first%2520step%2520in%2520adversarial%250Atraining%2520for%2520end-to-end%2520AD%2520models%2520and%2520present%2520a%2520novel%2520Module-wise%2520Adaptive%250AAdversarial%2520Training%2520%2528MA2T%2529.%2520However%252C%2520extending%2520conventional%2520adversarial%250Atraining%2520to%2520this%2520context%2520is%2520highly%2520non-trivial%252C%2520as%2520different%2520stages%2520within%2520the%250Amodel%2520have%2520distinct%2520objectives%2520and%2520are%2520strongly%2520interconnected.%2520To%2520address%250Athese%2520challenges%252C%2520MA2T%2520first%2520introduces%2520Module-wise%2520Noise%2520Injection%252C%2520which%250Ainjects%2520noise%2520before%2520the%2520input%2520of%2520different%2520modules%252C%2520targeting%2520training%2520models%250Awith%2520the%2520guidance%2520of%2520overall%2520objectives%2520rather%2520than%2520each%2520independent%2520module%250Aloss.%2520Additionally%252C%2520we%2520introduce%2520Dynamic%2520Weight%2520Accumulation%2520Adaptation%252C%2520which%250Aincorporates%2520accumulated%2520weight%2520changes%2520to%2520adaptively%2520learn%2520and%2520adjust%2520the%2520loss%250Aweights%2520of%2520each%2520module%2520based%2520on%2520their%2520contributions%2520%2528accumulated%2520reduction%250Arates%2529%2520for%2520better%2520balance%2520and%2520robust%2520training.%2520To%2520demonstrate%2520the%2520efficacy%2520of%250Aour%2520defense%252C%2520we%2520conduct%2520extensive%2520experiments%2520on%2520the%2520widely-used%2520nuScenes%250Adataset%2520across%2520several%2520end-to-end%2520AD%2520models%2520under%2520both%2520white-box%2520and%2520black-box%250Aattacks%252C%2520where%2520our%2520method%2520outperforms%2520other%2520baselines%2520by%2520large%2520margins%250A%2528%252B5-10%2525%2529.%2520Moreover%252C%2520we%2520validate%2520the%2520robustness%2520of%2520our%2520defense%2520through%250Aclosed-loop%2520evaluation%2520in%2520the%2520CARLA%2520simulation%2520environment%252C%2520showing%2520improved%250Aresilience%2520even%2520against%2520natural%2520corruption.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07321v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Module-wise%20Adaptive%20Adversarial%20Training%20for%20End-to-end%20Autonomous%0A%20%20Driving&entry.906535625=Tianyuan%20Zhang%20and%20Lu%20Wang%20and%20Jiaqi%20Kang%20and%20Xinwei%20Zhang%20and%20Siyuan%20Liang%20and%20Yuwei%20Chen%20and%20Aishan%20Liu%20and%20Xianglong%20Liu&entry.1292438233=%20%20Recent%20advances%20in%20deep%20learning%20have%20markedly%20improved%20autonomous%20driving%0A%28AD%29%20models%2C%20particularly%20end-to-end%20systems%20that%20integrate%20perception%2C%0Aprediction%2C%20and%20planning%20stages%2C%20achieving%20state-of-the-art%20performance.%0AHowever%2C%20these%20models%20remain%20vulnerable%20to%20adversarial%20attacks%2C%20where%0Ahuman-imperceptible%20perturbations%20can%20disrupt%20decision-making%20processes.%20While%0Aadversarial%20training%20is%20an%20effective%20method%20for%20enhancing%20model%20robustness%0Aagainst%20such%20attacks%2C%20no%20prior%20studies%20have%20focused%20on%20its%20application%20to%0Aend-to-end%20AD%20models.%20In%20this%20paper%2C%20we%20take%20the%20first%20step%20in%20adversarial%0Atraining%20for%20end-to-end%20AD%20models%20and%20present%20a%20novel%20Module-wise%20Adaptive%0AAdversarial%20Training%20%28MA2T%29.%20However%2C%20extending%20conventional%20adversarial%0Atraining%20to%20this%20context%20is%20highly%20non-trivial%2C%20as%20different%20stages%20within%20the%0Amodel%20have%20distinct%20objectives%20and%20are%20strongly%20interconnected.%20To%20address%0Athese%20challenges%2C%20MA2T%20first%20introduces%20Module-wise%20Noise%20Injection%2C%20which%0Ainjects%20noise%20before%20the%20input%20of%20different%20modules%2C%20targeting%20training%20models%0Awith%20the%20guidance%20of%20overall%20objectives%20rather%20than%20each%20independent%20module%0Aloss.%20Additionally%2C%20we%20introduce%20Dynamic%20Weight%20Accumulation%20Adaptation%2C%20which%0Aincorporates%20accumulated%20weight%20changes%20to%20adaptively%20learn%20and%20adjust%20the%20loss%0Aweights%20of%20each%20module%20based%20on%20their%20contributions%20%28accumulated%20reduction%0Arates%29%20for%20better%20balance%20and%20robust%20training.%20To%20demonstrate%20the%20efficacy%20of%0Aour%20defense%2C%20we%20conduct%20extensive%20experiments%20on%20the%20widely-used%20nuScenes%0Adataset%20across%20several%20end-to-end%20AD%20models%20under%20both%20white-box%20and%20black-box%0Aattacks%2C%20where%20our%20method%20outperforms%20other%20baselines%20by%20large%20margins%0A%28%2B5-10%25%29.%20Moreover%2C%20we%20validate%20the%20robustness%20of%20our%20defense%20through%0Aclosed-loop%20evaluation%20in%20the%20CARLA%20simulation%20environment%2C%20showing%20improved%0Aresilience%20even%20against%20natural%20corruption.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07321v1&entry.124074799=Read"},
{"title": "Lossless Image Compression Using Multi-level Dictionaries: Binary Images", "author": "Samar Agnihotri and Renu Rameshan and Ritwik Ghosal", "abstract": "  Lossless image compression is required in various applications to reduce\nstorage or transmission costs of images, while requiring the reconstructed\nimages to have zero information loss compared to the original. Existing\nlossless image compression methods either have simple design but poor\ncompression performance, or complex design, better performance, but with no\nperformance guarantees. In our endeavor to develop a lossless image compression\nmethod with low complexity and guaranteed performance, we argue that\ncompressibility of a color image is essentially derived from the patterns in\nits spatial structure, intensity variations, and color variations. Thus, we\ndivide the overall design of a lossless image compression scheme into three\nparts that exploit corresponding redundancies. We further argue that the\nbinarized version of an image captures its fundamental spatial structure. In\nthis first part of our work, we propose a scheme for lossless compression of\nbinary images.\n  The proposed scheme first learns dictionaries of $16\\times16$, $8\\times8$,\n$4\\times4$, and $2\\times 2$ square pixel patterns from various datasets of\nbinary images. It then uses these dictionaries to encode binary images. These\ndictionaries have various interesting properties that are further exploited to\nconstruct an efficient and scalable scheme. Our preliminary results show that\nthe proposed scheme consistently outperforms existing conventional and learning\nbased lossless compression approaches, and provides, on average, as much as\n$1.5\\times$ better performance than a common general purpose lossless\ncompression scheme (WebP), more than $3\\times$ better performance than a state\nof the art learning based scheme, and better performance than a specialized\nscheme for binary image compression (JBIG2).\n", "link": "http://arxiv.org/abs/2406.03087v3", "date": "2024-09-11", "relevancy": 2.1142, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.556}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5282}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lossless%20Image%20Compression%20Using%20Multi-level%20Dictionaries%3A%20Binary%20Images&body=Title%3A%20Lossless%20Image%20Compression%20Using%20Multi-level%20Dictionaries%3A%20Binary%20Images%0AAuthor%3A%20Samar%20Agnihotri%20and%20Renu%20Rameshan%20and%20Ritwik%20Ghosal%0AAbstract%3A%20%20%20Lossless%20image%20compression%20is%20required%20in%20various%20applications%20to%20reduce%0Astorage%20or%20transmission%20costs%20of%20images%2C%20while%20requiring%20the%20reconstructed%0Aimages%20to%20have%20zero%20information%20loss%20compared%20to%20the%20original.%20Existing%0Alossless%20image%20compression%20methods%20either%20have%20simple%20design%20but%20poor%0Acompression%20performance%2C%20or%20complex%20design%2C%20better%20performance%2C%20but%20with%20no%0Aperformance%20guarantees.%20In%20our%20endeavor%20to%20develop%20a%20lossless%20image%20compression%0Amethod%20with%20low%20complexity%20and%20guaranteed%20performance%2C%20we%20argue%20that%0Acompressibility%20of%20a%20color%20image%20is%20essentially%20derived%20from%20the%20patterns%20in%0Aits%20spatial%20structure%2C%20intensity%20variations%2C%20and%20color%20variations.%20Thus%2C%20we%0Adivide%20the%20overall%20design%20of%20a%20lossless%20image%20compression%20scheme%20into%20three%0Aparts%20that%20exploit%20corresponding%20redundancies.%20We%20further%20argue%20that%20the%0Abinarized%20version%20of%20an%20image%20captures%20its%20fundamental%20spatial%20structure.%20In%0Athis%20first%20part%20of%20our%20work%2C%20we%20propose%20a%20scheme%20for%20lossless%20compression%20of%0Abinary%20images.%0A%20%20The%20proposed%20scheme%20first%20learns%20dictionaries%20of%20%2416%5Ctimes16%24%2C%20%248%5Ctimes8%24%2C%0A%244%5Ctimes4%24%2C%20and%20%242%5Ctimes%202%24%20square%20pixel%20patterns%20from%20various%20datasets%20of%0Abinary%20images.%20It%20then%20uses%20these%20dictionaries%20to%20encode%20binary%20images.%20These%0Adictionaries%20have%20various%20interesting%20properties%20that%20are%20further%20exploited%20to%0Aconstruct%20an%20efficient%20and%20scalable%20scheme.%20Our%20preliminary%20results%20show%20that%0Athe%20proposed%20scheme%20consistently%20outperforms%20existing%20conventional%20and%20learning%0Abased%20lossless%20compression%20approaches%2C%20and%20provides%2C%20on%20average%2C%20as%20much%20as%0A%241.5%5Ctimes%24%20better%20performance%20than%20a%20common%20general%20purpose%20lossless%0Acompression%20scheme%20%28WebP%29%2C%20more%20than%20%243%5Ctimes%24%20better%20performance%20than%20a%20state%0Aof%20the%20art%20learning%20based%20scheme%2C%20and%20better%20performance%20than%20a%20specialized%0Ascheme%20for%20binary%20image%20compression%20%28JBIG2%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03087v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLossless%2520Image%2520Compression%2520Using%2520Multi-level%2520Dictionaries%253A%2520Binary%2520Images%26entry.906535625%3DSamar%2520Agnihotri%2520and%2520Renu%2520Rameshan%2520and%2520Ritwik%2520Ghosal%26entry.1292438233%3D%2520%2520Lossless%2520image%2520compression%2520is%2520required%2520in%2520various%2520applications%2520to%2520reduce%250Astorage%2520or%2520transmission%2520costs%2520of%2520images%252C%2520while%2520requiring%2520the%2520reconstructed%250Aimages%2520to%2520have%2520zero%2520information%2520loss%2520compared%2520to%2520the%2520original.%2520Existing%250Alossless%2520image%2520compression%2520methods%2520either%2520have%2520simple%2520design%2520but%2520poor%250Acompression%2520performance%252C%2520or%2520complex%2520design%252C%2520better%2520performance%252C%2520but%2520with%2520no%250Aperformance%2520guarantees.%2520In%2520our%2520endeavor%2520to%2520develop%2520a%2520lossless%2520image%2520compression%250Amethod%2520with%2520low%2520complexity%2520and%2520guaranteed%2520performance%252C%2520we%2520argue%2520that%250Acompressibility%2520of%2520a%2520color%2520image%2520is%2520essentially%2520derived%2520from%2520the%2520patterns%2520in%250Aits%2520spatial%2520structure%252C%2520intensity%2520variations%252C%2520and%2520color%2520variations.%2520Thus%252C%2520we%250Adivide%2520the%2520overall%2520design%2520of%2520a%2520lossless%2520image%2520compression%2520scheme%2520into%2520three%250Aparts%2520that%2520exploit%2520corresponding%2520redundancies.%2520We%2520further%2520argue%2520that%2520the%250Abinarized%2520version%2520of%2520an%2520image%2520captures%2520its%2520fundamental%2520spatial%2520structure.%2520In%250Athis%2520first%2520part%2520of%2520our%2520work%252C%2520we%2520propose%2520a%2520scheme%2520for%2520lossless%2520compression%2520of%250Abinary%2520images.%250A%2520%2520The%2520proposed%2520scheme%2520first%2520learns%2520dictionaries%2520of%2520%252416%255Ctimes16%2524%252C%2520%25248%255Ctimes8%2524%252C%250A%25244%255Ctimes4%2524%252C%2520and%2520%25242%255Ctimes%25202%2524%2520square%2520pixel%2520patterns%2520from%2520various%2520datasets%2520of%250Abinary%2520images.%2520It%2520then%2520uses%2520these%2520dictionaries%2520to%2520encode%2520binary%2520images.%2520These%250Adictionaries%2520have%2520various%2520interesting%2520properties%2520that%2520are%2520further%2520exploited%2520to%250Aconstruct%2520an%2520efficient%2520and%2520scalable%2520scheme.%2520Our%2520preliminary%2520results%2520show%2520that%250Athe%2520proposed%2520scheme%2520consistently%2520outperforms%2520existing%2520conventional%2520and%2520learning%250Abased%2520lossless%2520compression%2520approaches%252C%2520and%2520provides%252C%2520on%2520average%252C%2520as%2520much%2520as%250A%25241.5%255Ctimes%2524%2520better%2520performance%2520than%2520a%2520common%2520general%2520purpose%2520lossless%250Acompression%2520scheme%2520%2528WebP%2529%252C%2520more%2520than%2520%25243%255Ctimes%2524%2520better%2520performance%2520than%2520a%2520state%250Aof%2520the%2520art%2520learning%2520based%2520scheme%252C%2520and%2520better%2520performance%2520than%2520a%2520specialized%250Ascheme%2520for%2520binary%2520image%2520compression%2520%2528JBIG2%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03087v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lossless%20Image%20Compression%20Using%20Multi-level%20Dictionaries%3A%20Binary%20Images&entry.906535625=Samar%20Agnihotri%20and%20Renu%20Rameshan%20and%20Ritwik%20Ghosal&entry.1292438233=%20%20Lossless%20image%20compression%20is%20required%20in%20various%20applications%20to%20reduce%0Astorage%20or%20transmission%20costs%20of%20images%2C%20while%20requiring%20the%20reconstructed%0Aimages%20to%20have%20zero%20information%20loss%20compared%20to%20the%20original.%20Existing%0Alossless%20image%20compression%20methods%20either%20have%20simple%20design%20but%20poor%0Acompression%20performance%2C%20or%20complex%20design%2C%20better%20performance%2C%20but%20with%20no%0Aperformance%20guarantees.%20In%20our%20endeavor%20to%20develop%20a%20lossless%20image%20compression%0Amethod%20with%20low%20complexity%20and%20guaranteed%20performance%2C%20we%20argue%20that%0Acompressibility%20of%20a%20color%20image%20is%20essentially%20derived%20from%20the%20patterns%20in%0Aits%20spatial%20structure%2C%20intensity%20variations%2C%20and%20color%20variations.%20Thus%2C%20we%0Adivide%20the%20overall%20design%20of%20a%20lossless%20image%20compression%20scheme%20into%20three%0Aparts%20that%20exploit%20corresponding%20redundancies.%20We%20further%20argue%20that%20the%0Abinarized%20version%20of%20an%20image%20captures%20its%20fundamental%20spatial%20structure.%20In%0Athis%20first%20part%20of%20our%20work%2C%20we%20propose%20a%20scheme%20for%20lossless%20compression%20of%0Abinary%20images.%0A%20%20The%20proposed%20scheme%20first%20learns%20dictionaries%20of%20%2416%5Ctimes16%24%2C%20%248%5Ctimes8%24%2C%0A%244%5Ctimes4%24%2C%20and%20%242%5Ctimes%202%24%20square%20pixel%20patterns%20from%20various%20datasets%20of%0Abinary%20images.%20It%20then%20uses%20these%20dictionaries%20to%20encode%20binary%20images.%20These%0Adictionaries%20have%20various%20interesting%20properties%20that%20are%20further%20exploited%20to%0Aconstruct%20an%20efficient%20and%20scalable%20scheme.%20Our%20preliminary%20results%20show%20that%0Athe%20proposed%20scheme%20consistently%20outperforms%20existing%20conventional%20and%20learning%0Abased%20lossless%20compression%20approaches%2C%20and%20provides%2C%20on%20average%2C%20as%20much%20as%0A%241.5%5Ctimes%24%20better%20performance%20than%20a%20common%20general%20purpose%20lossless%0Acompression%20scheme%20%28WebP%29%2C%20more%20than%20%243%5Ctimes%24%20better%20performance%20than%20a%20state%0Aof%20the%20art%20learning%20based%20scheme%2C%20and%20better%20performance%20than%20a%20specialized%0Ascheme%20for%20binary%20image%20compression%20%28JBIG2%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03087v3&entry.124074799=Read"},
{"title": "Synthetic continued pretraining", "author": "Zitong Yang and Neil Band and Shuangping Li and Emmanuel Cand\u00e8s and Tatsunori Hashimoto", "abstract": "  Pretraining on large-scale, unstructured internet text has enabled language\nmodels to acquire a significant amount of world knowledge. However, this\nknowledge acquisition is data-inefficient -- to learn a given fact, models must\nbe trained on hundreds to thousands of diverse representations of it. This\nposes a challenge when adapting a pretrained model to a small corpus of\ndomain-specific documents, where each fact may appear rarely or only once. We\npropose to bridge this gap with synthetic continued pretraining: using the\nsmall domain-specific corpus to synthesize a large corpus more amenable to\nlearning, and then performing continued pretraining on the synthesized corpus.\nWe instantiate this proposal with EntiGraph, a synthetic data augmentation\nalgorithm that extracts salient entities from the source documents and then\ngenerates diverse text by drawing connections between the sampled entities.\nSynthetic continued pretraining using EntiGraph enables a language model to\nanswer questions and follow generic instructions related to the source\ndocuments without access to them. If instead, the source documents are\navailable at inference time, we show that the knowledge acquired through our\napproach compounds with retrieval-augmented generation. To better understand\nthese results, we build a simple mathematical model of EntiGraph, and show how\nsynthetic data augmentation can \"rearrange\" knowledge to enable more\ndata-efficient learning.\n", "link": "http://arxiv.org/abs/2409.07431v1", "date": "2024-09-11", "relevancy": 2.1093, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5689}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5265}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthetic%20continued%20pretraining&body=Title%3A%20Synthetic%20continued%20pretraining%0AAuthor%3A%20Zitong%20Yang%20and%20Neil%20Band%20and%20Shuangping%20Li%20and%20Emmanuel%20Cand%C3%A8s%20and%20Tatsunori%20Hashimoto%0AAbstract%3A%20%20%20Pretraining%20on%20large-scale%2C%20unstructured%20internet%20text%20has%20enabled%20language%0Amodels%20to%20acquire%20a%20significant%20amount%20of%20world%20knowledge.%20However%2C%20this%0Aknowledge%20acquisition%20is%20data-inefficient%20--%20to%20learn%20a%20given%20fact%2C%20models%20must%0Abe%20trained%20on%20hundreds%20to%20thousands%20of%20diverse%20representations%20of%20it.%20This%0Aposes%20a%20challenge%20when%20adapting%20a%20pretrained%20model%20to%20a%20small%20corpus%20of%0Adomain-specific%20documents%2C%20where%20each%20fact%20may%20appear%20rarely%20or%20only%20once.%20We%0Apropose%20to%20bridge%20this%20gap%20with%20synthetic%20continued%20pretraining%3A%20using%20the%0Asmall%20domain-specific%20corpus%20to%20synthesize%20a%20large%20corpus%20more%20amenable%20to%0Alearning%2C%20and%20then%20performing%20continued%20pretraining%20on%20the%20synthesized%20corpus.%0AWe%20instantiate%20this%20proposal%20with%20EntiGraph%2C%20a%20synthetic%20data%20augmentation%0Aalgorithm%20that%20extracts%20salient%20entities%20from%20the%20source%20documents%20and%20then%0Agenerates%20diverse%20text%20by%20drawing%20connections%20between%20the%20sampled%20entities.%0ASynthetic%20continued%20pretraining%20using%20EntiGraph%20enables%20a%20language%20model%20to%0Aanswer%20questions%20and%20follow%20generic%20instructions%20related%20to%20the%20source%0Adocuments%20without%20access%20to%20them.%20If%20instead%2C%20the%20source%20documents%20are%0Aavailable%20at%20inference%20time%2C%20we%20show%20that%20the%20knowledge%20acquired%20through%20our%0Aapproach%20compounds%20with%20retrieval-augmented%20generation.%20To%20better%20understand%0Athese%20results%2C%20we%20build%20a%20simple%20mathematical%20model%20of%20EntiGraph%2C%20and%20show%20how%0Asynthetic%20data%20augmentation%20can%20%22rearrange%22%20knowledge%20to%20enable%20more%0Adata-efficient%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07431v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthetic%2520continued%2520pretraining%26entry.906535625%3DZitong%2520Yang%2520and%2520Neil%2520Band%2520and%2520Shuangping%2520Li%2520and%2520Emmanuel%2520Cand%25C3%25A8s%2520and%2520Tatsunori%2520Hashimoto%26entry.1292438233%3D%2520%2520Pretraining%2520on%2520large-scale%252C%2520unstructured%2520internet%2520text%2520has%2520enabled%2520language%250Amodels%2520to%2520acquire%2520a%2520significant%2520amount%2520of%2520world%2520knowledge.%2520However%252C%2520this%250Aknowledge%2520acquisition%2520is%2520data-inefficient%2520--%2520to%2520learn%2520a%2520given%2520fact%252C%2520models%2520must%250Abe%2520trained%2520on%2520hundreds%2520to%2520thousands%2520of%2520diverse%2520representations%2520of%2520it.%2520This%250Aposes%2520a%2520challenge%2520when%2520adapting%2520a%2520pretrained%2520model%2520to%2520a%2520small%2520corpus%2520of%250Adomain-specific%2520documents%252C%2520where%2520each%2520fact%2520may%2520appear%2520rarely%2520or%2520only%2520once.%2520We%250Apropose%2520to%2520bridge%2520this%2520gap%2520with%2520synthetic%2520continued%2520pretraining%253A%2520using%2520the%250Asmall%2520domain-specific%2520corpus%2520to%2520synthesize%2520a%2520large%2520corpus%2520more%2520amenable%2520to%250Alearning%252C%2520and%2520then%2520performing%2520continued%2520pretraining%2520on%2520the%2520synthesized%2520corpus.%250AWe%2520instantiate%2520this%2520proposal%2520with%2520EntiGraph%252C%2520a%2520synthetic%2520data%2520augmentation%250Aalgorithm%2520that%2520extracts%2520salient%2520entities%2520from%2520the%2520source%2520documents%2520and%2520then%250Agenerates%2520diverse%2520text%2520by%2520drawing%2520connections%2520between%2520the%2520sampled%2520entities.%250ASynthetic%2520continued%2520pretraining%2520using%2520EntiGraph%2520enables%2520a%2520language%2520model%2520to%250Aanswer%2520questions%2520and%2520follow%2520generic%2520instructions%2520related%2520to%2520the%2520source%250Adocuments%2520without%2520access%2520to%2520them.%2520If%2520instead%252C%2520the%2520source%2520documents%2520are%250Aavailable%2520at%2520inference%2520time%252C%2520we%2520show%2520that%2520the%2520knowledge%2520acquired%2520through%2520our%250Aapproach%2520compounds%2520with%2520retrieval-augmented%2520generation.%2520To%2520better%2520understand%250Athese%2520results%252C%2520we%2520build%2520a%2520simple%2520mathematical%2520model%2520of%2520EntiGraph%252C%2520and%2520show%2520how%250Asynthetic%2520data%2520augmentation%2520can%2520%2522rearrange%2522%2520knowledge%2520to%2520enable%2520more%250Adata-efficient%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07431v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthetic%20continued%20pretraining&entry.906535625=Zitong%20Yang%20and%20Neil%20Band%20and%20Shuangping%20Li%20and%20Emmanuel%20Cand%C3%A8s%20and%20Tatsunori%20Hashimoto&entry.1292438233=%20%20Pretraining%20on%20large-scale%2C%20unstructured%20internet%20text%20has%20enabled%20language%0Amodels%20to%20acquire%20a%20significant%20amount%20of%20world%20knowledge.%20However%2C%20this%0Aknowledge%20acquisition%20is%20data-inefficient%20--%20to%20learn%20a%20given%20fact%2C%20models%20must%0Abe%20trained%20on%20hundreds%20to%20thousands%20of%20diverse%20representations%20of%20it.%20This%0Aposes%20a%20challenge%20when%20adapting%20a%20pretrained%20model%20to%20a%20small%20corpus%20of%0Adomain-specific%20documents%2C%20where%20each%20fact%20may%20appear%20rarely%20or%20only%20once.%20We%0Apropose%20to%20bridge%20this%20gap%20with%20synthetic%20continued%20pretraining%3A%20using%20the%0Asmall%20domain-specific%20corpus%20to%20synthesize%20a%20large%20corpus%20more%20amenable%20to%0Alearning%2C%20and%20then%20performing%20continued%20pretraining%20on%20the%20synthesized%20corpus.%0AWe%20instantiate%20this%20proposal%20with%20EntiGraph%2C%20a%20synthetic%20data%20augmentation%0Aalgorithm%20that%20extracts%20salient%20entities%20from%20the%20source%20documents%20and%20then%0Agenerates%20diverse%20text%20by%20drawing%20connections%20between%20the%20sampled%20entities.%0ASynthetic%20continued%20pretraining%20using%20EntiGraph%20enables%20a%20language%20model%20to%0Aanswer%20questions%20and%20follow%20generic%20instructions%20related%20to%20the%20source%0Adocuments%20without%20access%20to%20them.%20If%20instead%2C%20the%20source%20documents%20are%0Aavailable%20at%20inference%20time%2C%20we%20show%20that%20the%20knowledge%20acquired%20through%20our%0Aapproach%20compounds%20with%20retrieval-augmented%20generation.%20To%20better%20understand%0Athese%20results%2C%20we%20build%20a%20simple%20mathematical%20model%20of%20EntiGraph%2C%20and%20show%20how%0Asynthetic%20data%20augmentation%20can%20%22rearrange%22%20knowledge%20to%20enable%20more%0Adata-efficient%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07431v1&entry.124074799=Read"},
{"title": "Cyber Deception: State of the art, Trends and Open challenges", "author": "Pedro Beltr\u00e1n L\u00f3pez and Manuel Gil P\u00e9rez and Pantaleone Nespoli", "abstract": "  The growing interest in cybersecurity has significantly increased articles\ndesigning and implementing various Cyber Deception (CYDEC) mechanisms. This\ntrend reflects the urgent need for new strategies to address cyber threats\neffectively. Since its emergence, CYDEC has established itself as an innovative\ndefense against attackers, thanks to its proactive and reactive capabilities,\nfinding applications in numerous real-life scenarios. Despite the considerable\nwork devoted to CYDEC, the literature still presents significant gaps. In\nparticular, there has not been (i) a comprehensive analysis of the main\ncomponents characterizing CYDEC, (ii) a generic classification covering all\ntypes of solutions, nor (iii) a survey of the current state of the literature\nin various contexts. This article aims to fill these gaps through a detailed\nreview of the main features that comprise CYDEC, developing a comprehensive\nclassification taxonomy. In addition, the different frameworks used to generate\nCYDEC are reviewed, presenting a more comprehensive one. Existing solutions in\nthe literature using CYDEC, both without Artificial Intelligence (AI) and with\nAI, are studied and compared. Finally, the most salient trends of the current\nstate of the art are discussed, offering a list of pending challenges for\nfuture research.\n", "link": "http://arxiv.org/abs/2409.07194v1", "date": "2024-09-11", "relevancy": 2.1071, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4307}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4229}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cyber%20Deception%3A%20State%20of%20the%20art%2C%20Trends%20and%20Open%20challenges&body=Title%3A%20Cyber%20Deception%3A%20State%20of%20the%20art%2C%20Trends%20and%20Open%20challenges%0AAuthor%3A%20Pedro%20Beltr%C3%A1n%20L%C3%B3pez%20and%20Manuel%20Gil%20P%C3%A9rez%20and%20Pantaleone%20Nespoli%0AAbstract%3A%20%20%20The%20growing%20interest%20in%20cybersecurity%20has%20significantly%20increased%20articles%0Adesigning%20and%20implementing%20various%20Cyber%20Deception%20%28CYDEC%29%20mechanisms.%20This%0Atrend%20reflects%20the%20urgent%20need%20for%20new%20strategies%20to%20address%20cyber%20threats%0Aeffectively.%20Since%20its%20emergence%2C%20CYDEC%20has%20established%20itself%20as%20an%20innovative%0Adefense%20against%20attackers%2C%20thanks%20to%20its%20proactive%20and%20reactive%20capabilities%2C%0Afinding%20applications%20in%20numerous%20real-life%20scenarios.%20Despite%20the%20considerable%0Awork%20devoted%20to%20CYDEC%2C%20the%20literature%20still%20presents%20significant%20gaps.%20In%0Aparticular%2C%20there%20has%20not%20been%20%28i%29%20a%20comprehensive%20analysis%20of%20the%20main%0Acomponents%20characterizing%20CYDEC%2C%20%28ii%29%20a%20generic%20classification%20covering%20all%0Atypes%20of%20solutions%2C%20nor%20%28iii%29%20a%20survey%20of%20the%20current%20state%20of%20the%20literature%0Ain%20various%20contexts.%20This%20article%20aims%20to%20fill%20these%20gaps%20through%20a%20detailed%0Areview%20of%20the%20main%20features%20that%20comprise%20CYDEC%2C%20developing%20a%20comprehensive%0Aclassification%20taxonomy.%20In%20addition%2C%20the%20different%20frameworks%20used%20to%20generate%0ACYDEC%20are%20reviewed%2C%20presenting%20a%20more%20comprehensive%20one.%20Existing%20solutions%20in%0Athe%20literature%20using%20CYDEC%2C%20both%20without%20Artificial%20Intelligence%20%28AI%29%20and%20with%0AAI%2C%20are%20studied%20and%20compared.%20Finally%2C%20the%20most%20salient%20trends%20of%20the%20current%0Astate%20of%20the%20art%20are%20discussed%2C%20offering%20a%20list%20of%20pending%20challenges%20for%0Afuture%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07194v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCyber%2520Deception%253A%2520State%2520of%2520the%2520art%252C%2520Trends%2520and%2520Open%2520challenges%26entry.906535625%3DPedro%2520Beltr%25C3%25A1n%2520L%25C3%25B3pez%2520and%2520Manuel%2520Gil%2520P%25C3%25A9rez%2520and%2520Pantaleone%2520Nespoli%26entry.1292438233%3D%2520%2520The%2520growing%2520interest%2520in%2520cybersecurity%2520has%2520significantly%2520increased%2520articles%250Adesigning%2520and%2520implementing%2520various%2520Cyber%2520Deception%2520%2528CYDEC%2529%2520mechanisms.%2520This%250Atrend%2520reflects%2520the%2520urgent%2520need%2520for%2520new%2520strategies%2520to%2520address%2520cyber%2520threats%250Aeffectively.%2520Since%2520its%2520emergence%252C%2520CYDEC%2520has%2520established%2520itself%2520as%2520an%2520innovative%250Adefense%2520against%2520attackers%252C%2520thanks%2520to%2520its%2520proactive%2520and%2520reactive%2520capabilities%252C%250Afinding%2520applications%2520in%2520numerous%2520real-life%2520scenarios.%2520Despite%2520the%2520considerable%250Awork%2520devoted%2520to%2520CYDEC%252C%2520the%2520literature%2520still%2520presents%2520significant%2520gaps.%2520In%250Aparticular%252C%2520there%2520has%2520not%2520been%2520%2528i%2529%2520a%2520comprehensive%2520analysis%2520of%2520the%2520main%250Acomponents%2520characterizing%2520CYDEC%252C%2520%2528ii%2529%2520a%2520generic%2520classification%2520covering%2520all%250Atypes%2520of%2520solutions%252C%2520nor%2520%2528iii%2529%2520a%2520survey%2520of%2520the%2520current%2520state%2520of%2520the%2520literature%250Ain%2520various%2520contexts.%2520This%2520article%2520aims%2520to%2520fill%2520these%2520gaps%2520through%2520a%2520detailed%250Areview%2520of%2520the%2520main%2520features%2520that%2520comprise%2520CYDEC%252C%2520developing%2520a%2520comprehensive%250Aclassification%2520taxonomy.%2520In%2520addition%252C%2520the%2520different%2520frameworks%2520used%2520to%2520generate%250ACYDEC%2520are%2520reviewed%252C%2520presenting%2520a%2520more%2520comprehensive%2520one.%2520Existing%2520solutions%2520in%250Athe%2520literature%2520using%2520CYDEC%252C%2520both%2520without%2520Artificial%2520Intelligence%2520%2528AI%2529%2520and%2520with%250AAI%252C%2520are%2520studied%2520and%2520compared.%2520Finally%252C%2520the%2520most%2520salient%2520trends%2520of%2520the%2520current%250Astate%2520of%2520the%2520art%2520are%2520discussed%252C%2520offering%2520a%2520list%2520of%2520pending%2520challenges%2520for%250Afuture%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07194v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cyber%20Deception%3A%20State%20of%20the%20art%2C%20Trends%20and%20Open%20challenges&entry.906535625=Pedro%20Beltr%C3%A1n%20L%C3%B3pez%20and%20Manuel%20Gil%20P%C3%A9rez%20and%20Pantaleone%20Nespoli&entry.1292438233=%20%20The%20growing%20interest%20in%20cybersecurity%20has%20significantly%20increased%20articles%0Adesigning%20and%20implementing%20various%20Cyber%20Deception%20%28CYDEC%29%20mechanisms.%20This%0Atrend%20reflects%20the%20urgent%20need%20for%20new%20strategies%20to%20address%20cyber%20threats%0Aeffectively.%20Since%20its%20emergence%2C%20CYDEC%20has%20established%20itself%20as%20an%20innovative%0Adefense%20against%20attackers%2C%20thanks%20to%20its%20proactive%20and%20reactive%20capabilities%2C%0Afinding%20applications%20in%20numerous%20real-life%20scenarios.%20Despite%20the%20considerable%0Awork%20devoted%20to%20CYDEC%2C%20the%20literature%20still%20presents%20significant%20gaps.%20In%0Aparticular%2C%20there%20has%20not%20been%20%28i%29%20a%20comprehensive%20analysis%20of%20the%20main%0Acomponents%20characterizing%20CYDEC%2C%20%28ii%29%20a%20generic%20classification%20covering%20all%0Atypes%20of%20solutions%2C%20nor%20%28iii%29%20a%20survey%20of%20the%20current%20state%20of%20the%20literature%0Ain%20various%20contexts.%20This%20article%20aims%20to%20fill%20these%20gaps%20through%20a%20detailed%0Areview%20of%20the%20main%20features%20that%20comprise%20CYDEC%2C%20developing%20a%20comprehensive%0Aclassification%20taxonomy.%20In%20addition%2C%20the%20different%20frameworks%20used%20to%20generate%0ACYDEC%20are%20reviewed%2C%20presenting%20a%20more%20comprehensive%20one.%20Existing%20solutions%20in%0Athe%20literature%20using%20CYDEC%2C%20both%20without%20Artificial%20Intelligence%20%28AI%29%20and%20with%0AAI%2C%20are%20studied%20and%20compared.%20Finally%2C%20the%20most%20salient%20trends%20of%20the%20current%0Astate%20of%20the%20art%20are%20discussed%2C%20offering%20a%20list%20of%20pending%20challenges%20for%0Afuture%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07194v1&entry.124074799=Read"},
{"title": "Physically Feasible Semantic Segmentation", "author": "Shamik Basu and Luc Van Gool and Christos Sakaridis", "abstract": "  State-of-the-art semantic segmentation models are typically optimized in a\ndata-driven fashion, minimizing solely per-pixel classification objectives on\ntheir training data. This purely data-driven paradigm often leads to absurd\nsegmentations, especially when the domain of input images is shifted from the\none encountered during training. For instance, state-of-the-art models may\nassign the label ``road'' to a segment which is located above a segment that is\nrespectively labeled as ``sky'', although our knowledge of the physical world\ndictates that such a configuration is not feasible for images captured by\nforward-facing upright cameras. Our method, Physically Feasible Semantic\nSegmentation (PhyFea), extracts explicit physical constraints that govern\nspatial class relations from the training sets of semantic segmentation\ndatasets and enforces a differentiable loss function that penalizes violations\nof these constraints to promote prediction feasibility. PhyFea yields\nsignificant performance improvements in mIoU over each state-of-the-art network\nwe use as baseline across ADE20K, Cityscapes and ACDC, notably a $1.5\\%$\nimprovement on ADE20K and a $2.1\\%$ improvement on ACDC.\n", "link": "http://arxiv.org/abs/2408.14672v2", "date": "2024-09-11", "relevancy": 2.0983, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5905}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5114}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physically%20Feasible%20Semantic%20Segmentation&body=Title%3A%20Physically%20Feasible%20Semantic%20Segmentation%0AAuthor%3A%20Shamik%20Basu%20and%20Luc%20Van%20Gool%20and%20Christos%20Sakaridis%0AAbstract%3A%20%20%20State-of-the-art%20semantic%20segmentation%20models%20are%20typically%20optimized%20in%20a%0Adata-driven%20fashion%2C%20minimizing%20solely%20per-pixel%20classification%20objectives%20on%0Atheir%20training%20data.%20This%20purely%20data-driven%20paradigm%20often%20leads%20to%20absurd%0Asegmentations%2C%20especially%20when%20the%20domain%20of%20input%20images%20is%20shifted%20from%20the%0Aone%20encountered%20during%20training.%20For%20instance%2C%20state-of-the-art%20models%20may%0Aassign%20the%20label%20%60%60road%27%27%20to%20a%20segment%20which%20is%20located%20above%20a%20segment%20that%20is%0Arespectively%20labeled%20as%20%60%60sky%27%27%2C%20although%20our%20knowledge%20of%20the%20physical%20world%0Adictates%20that%20such%20a%20configuration%20is%20not%20feasible%20for%20images%20captured%20by%0Aforward-facing%20upright%20cameras.%20Our%20method%2C%20Physically%20Feasible%20Semantic%0ASegmentation%20%28PhyFea%29%2C%20extracts%20explicit%20physical%20constraints%20that%20govern%0Aspatial%20class%20relations%20from%20the%20training%20sets%20of%20semantic%20segmentation%0Adatasets%20and%20enforces%20a%20differentiable%20loss%20function%20that%20penalizes%20violations%0Aof%20these%20constraints%20to%20promote%20prediction%20feasibility.%20PhyFea%20yields%0Asignificant%20performance%20improvements%20in%20mIoU%20over%20each%20state-of-the-art%20network%0Awe%20use%20as%20baseline%20across%20ADE20K%2C%20Cityscapes%20and%20ACDC%2C%20notably%20a%20%241.5%5C%25%24%0Aimprovement%20on%20ADE20K%20and%20a%20%242.1%5C%25%24%20improvement%20on%20ACDC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14672v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysically%2520Feasible%2520Semantic%2520Segmentation%26entry.906535625%3DShamik%2520Basu%2520and%2520Luc%2520Van%2520Gool%2520and%2520Christos%2520Sakaridis%26entry.1292438233%3D%2520%2520State-of-the-art%2520semantic%2520segmentation%2520models%2520are%2520typically%2520optimized%2520in%2520a%250Adata-driven%2520fashion%252C%2520minimizing%2520solely%2520per-pixel%2520classification%2520objectives%2520on%250Atheir%2520training%2520data.%2520This%2520purely%2520data-driven%2520paradigm%2520often%2520leads%2520to%2520absurd%250Asegmentations%252C%2520especially%2520when%2520the%2520domain%2520of%2520input%2520images%2520is%2520shifted%2520from%2520the%250Aone%2520encountered%2520during%2520training.%2520For%2520instance%252C%2520state-of-the-art%2520models%2520may%250Aassign%2520the%2520label%2520%2560%2560road%2527%2527%2520to%2520a%2520segment%2520which%2520is%2520located%2520above%2520a%2520segment%2520that%2520is%250Arespectively%2520labeled%2520as%2520%2560%2560sky%2527%2527%252C%2520although%2520our%2520knowledge%2520of%2520the%2520physical%2520world%250Adictates%2520that%2520such%2520a%2520configuration%2520is%2520not%2520feasible%2520for%2520images%2520captured%2520by%250Aforward-facing%2520upright%2520cameras.%2520Our%2520method%252C%2520Physically%2520Feasible%2520Semantic%250ASegmentation%2520%2528PhyFea%2529%252C%2520extracts%2520explicit%2520physical%2520constraints%2520that%2520govern%250Aspatial%2520class%2520relations%2520from%2520the%2520training%2520sets%2520of%2520semantic%2520segmentation%250Adatasets%2520and%2520enforces%2520a%2520differentiable%2520loss%2520function%2520that%2520penalizes%2520violations%250Aof%2520these%2520constraints%2520to%2520promote%2520prediction%2520feasibility.%2520PhyFea%2520yields%250Asignificant%2520performance%2520improvements%2520in%2520mIoU%2520over%2520each%2520state-of-the-art%2520network%250Awe%2520use%2520as%2520baseline%2520across%2520ADE20K%252C%2520Cityscapes%2520and%2520ACDC%252C%2520notably%2520a%2520%25241.5%255C%2525%2524%250Aimprovement%2520on%2520ADE20K%2520and%2520a%2520%25242.1%255C%2525%2524%2520improvement%2520on%2520ACDC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14672v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physically%20Feasible%20Semantic%20Segmentation&entry.906535625=Shamik%20Basu%20and%20Luc%20Van%20Gool%20and%20Christos%20Sakaridis&entry.1292438233=%20%20State-of-the-art%20semantic%20segmentation%20models%20are%20typically%20optimized%20in%20a%0Adata-driven%20fashion%2C%20minimizing%20solely%20per-pixel%20classification%20objectives%20on%0Atheir%20training%20data.%20This%20purely%20data-driven%20paradigm%20often%20leads%20to%20absurd%0Asegmentations%2C%20especially%20when%20the%20domain%20of%20input%20images%20is%20shifted%20from%20the%0Aone%20encountered%20during%20training.%20For%20instance%2C%20state-of-the-art%20models%20may%0Aassign%20the%20label%20%60%60road%27%27%20to%20a%20segment%20which%20is%20located%20above%20a%20segment%20that%20is%0Arespectively%20labeled%20as%20%60%60sky%27%27%2C%20although%20our%20knowledge%20of%20the%20physical%20world%0Adictates%20that%20such%20a%20configuration%20is%20not%20feasible%20for%20images%20captured%20by%0Aforward-facing%20upright%20cameras.%20Our%20method%2C%20Physically%20Feasible%20Semantic%0ASegmentation%20%28PhyFea%29%2C%20extracts%20explicit%20physical%20constraints%20that%20govern%0Aspatial%20class%20relations%20from%20the%20training%20sets%20of%20semantic%20segmentation%0Adatasets%20and%20enforces%20a%20differentiable%20loss%20function%20that%20penalizes%20violations%0Aof%20these%20constraints%20to%20promote%20prediction%20feasibility.%20PhyFea%20yields%0Asignificant%20performance%20improvements%20in%20mIoU%20over%20each%20state-of-the-art%20network%0Awe%20use%20as%20baseline%20across%20ADE20K%2C%20Cityscapes%20and%20ACDC%2C%20notably%20a%20%241.5%5C%25%24%0Aimprovement%20on%20ADE20K%20and%20a%20%242.1%5C%25%24%20improvement%20on%20ACDC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14672v2&entry.124074799=Read"},
{"title": "Compliant Blind Handover Control for Human-Robot Collaboration", "author": "Davide Ferrari and Andrea Pupa and Cristian Secchi", "abstract": "  This paper presents a Human-Robot Blind Handover architecture within the\ncontext of Human-Robot Collaboration (HRC). The focus lies on a blind handover\nscenario where the operator is intentionally faced away, focused in a task, and\nrequires an object from the robot. In this context, it is imperative for the\nrobot to autonomously manage the entire handover process. Key considerations\ninclude ensuring safety while handing the object to the operator's hand, and\ndetect the proper timing to release the object. The article explores strategies\nto navigate these challenges, emphasizing the need for a robot to operate\nsafely and independently in facilitating blind handovers, thereby contributing\nto the advancement of HRC protocols and fostering a natural and efficient\ncollaboration between humans and robots.\n", "link": "http://arxiv.org/abs/2409.07155v1", "date": "2024-09-11", "relevancy": 2.0976, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5883}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4836}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compliant%20Blind%20Handover%20Control%20for%20Human-Robot%20Collaboration&body=Title%3A%20Compliant%20Blind%20Handover%20Control%20for%20Human-Robot%20Collaboration%0AAuthor%3A%20Davide%20Ferrari%20and%20Andrea%20Pupa%20and%20Cristian%20Secchi%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20Human-Robot%20Blind%20Handover%20architecture%20within%20the%0Acontext%20of%20Human-Robot%20Collaboration%20%28HRC%29.%20The%20focus%20lies%20on%20a%20blind%20handover%0Ascenario%20where%20the%20operator%20is%20intentionally%20faced%20away%2C%20focused%20in%20a%20task%2C%20and%0Arequires%20an%20object%20from%20the%20robot.%20In%20this%20context%2C%20it%20is%20imperative%20for%20the%0Arobot%20to%20autonomously%20manage%20the%20entire%20handover%20process.%20Key%20considerations%0Ainclude%20ensuring%20safety%20while%20handing%20the%20object%20to%20the%20operator%27s%20hand%2C%20and%0Adetect%20the%20proper%20timing%20to%20release%20the%20object.%20The%20article%20explores%20strategies%0Ato%20navigate%20these%20challenges%2C%20emphasizing%20the%20need%20for%20a%20robot%20to%20operate%0Asafely%20and%20independently%20in%20facilitating%20blind%20handovers%2C%20thereby%20contributing%0Ato%20the%20advancement%20of%20HRC%20protocols%20and%20fostering%20a%20natural%20and%20efficient%0Acollaboration%20between%20humans%20and%20robots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07155v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompliant%2520Blind%2520Handover%2520Control%2520for%2520Human-Robot%2520Collaboration%26entry.906535625%3DDavide%2520Ferrari%2520and%2520Andrea%2520Pupa%2520and%2520Cristian%2520Secchi%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520Human-Robot%2520Blind%2520Handover%2520architecture%2520within%2520the%250Acontext%2520of%2520Human-Robot%2520Collaboration%2520%2528HRC%2529.%2520The%2520focus%2520lies%2520on%2520a%2520blind%2520handover%250Ascenario%2520where%2520the%2520operator%2520is%2520intentionally%2520faced%2520away%252C%2520focused%2520in%2520a%2520task%252C%2520and%250Arequires%2520an%2520object%2520from%2520the%2520robot.%2520In%2520this%2520context%252C%2520it%2520is%2520imperative%2520for%2520the%250Arobot%2520to%2520autonomously%2520manage%2520the%2520entire%2520handover%2520process.%2520Key%2520considerations%250Ainclude%2520ensuring%2520safety%2520while%2520handing%2520the%2520object%2520to%2520the%2520operator%2527s%2520hand%252C%2520and%250Adetect%2520the%2520proper%2520timing%2520to%2520release%2520the%2520object.%2520The%2520article%2520explores%2520strategies%250Ato%2520navigate%2520these%2520challenges%252C%2520emphasizing%2520the%2520need%2520for%2520a%2520robot%2520to%2520operate%250Asafely%2520and%2520independently%2520in%2520facilitating%2520blind%2520handovers%252C%2520thereby%2520contributing%250Ato%2520the%2520advancement%2520of%2520HRC%2520protocols%2520and%2520fostering%2520a%2520natural%2520and%2520efficient%250Acollaboration%2520between%2520humans%2520and%2520robots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07155v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compliant%20Blind%20Handover%20Control%20for%20Human-Robot%20Collaboration&entry.906535625=Davide%20Ferrari%20and%20Andrea%20Pupa%20and%20Cristian%20Secchi&entry.1292438233=%20%20This%20paper%20presents%20a%20Human-Robot%20Blind%20Handover%20architecture%20within%20the%0Acontext%20of%20Human-Robot%20Collaboration%20%28HRC%29.%20The%20focus%20lies%20on%20a%20blind%20handover%0Ascenario%20where%20the%20operator%20is%20intentionally%20faced%20away%2C%20focused%20in%20a%20task%2C%20and%0Arequires%20an%20object%20from%20the%20robot.%20In%20this%20context%2C%20it%20is%20imperative%20for%20the%0Arobot%20to%20autonomously%20manage%20the%20entire%20handover%20process.%20Key%20considerations%0Ainclude%20ensuring%20safety%20while%20handing%20the%20object%20to%20the%20operator%27s%20hand%2C%20and%0Adetect%20the%20proper%20timing%20to%20release%20the%20object.%20The%20article%20explores%20strategies%0Ato%20navigate%20these%20challenges%2C%20emphasizing%20the%20need%20for%20a%20robot%20to%20operate%0Asafely%20and%20independently%20in%20facilitating%20blind%20handovers%2C%20thereby%20contributing%0Ato%20the%20advancement%20of%20HRC%20protocols%20and%20fostering%20a%20natural%20and%20efficient%0Acollaboration%20between%20humans%20and%20robots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07155v1&entry.124074799=Read"},
{"title": "Label Alignment Regularization for Distribution Shift", "author": "Ehsan Imani and Guojun Zhang and Runjia Li and Jun Luo and Pascal Poupart and Philip H. S. Torr and Yangchen Pan", "abstract": "  Recent work has highlighted the label alignment property (LAP) in supervised\nlearning, where the vector of all labels in the dataset is mostly in the span\nof the top few singular vectors of the data matrix. Drawing inspiration from\nthis observation, we propose a regularization method for unsupervised domain\nadaptation that encourages alignment between the predictions in the target\ndomain and its top singular vectors. Unlike conventional domain adaptation\napproaches that focus on regularizing representations, we instead regularize\nthe classifier to align with the unsupervised target data, guided by the LAP in\nboth the source and target domains. Theoretical analysis demonstrates that,\nunder certain assumptions, our solution resides within the span of the top\nright singular vectors of the target domain data and aligns with the optimal\nsolution. By removing the reliance on the commonly used optimal joint risk\nassumption found in classic domain adaptation theory, we showcase the\neffectiveness of our method on addressing problems where traditional domain\nadaptation methods often fall short due to high joint error. Additionally, we\nreport improved performance over domain adaptation baselines in well-known\ntasks such as MNIST-USPS domain adaptation and cross-lingual sentiment\nanalysis.\n", "link": "http://arxiv.org/abs/2211.14960v5", "date": "2024-09-11", "relevancy": 2.0945, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5413}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5125}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Label%20Alignment%20Regularization%20for%20Distribution%20Shift&body=Title%3A%20Label%20Alignment%20Regularization%20for%20Distribution%20Shift%0AAuthor%3A%20Ehsan%20Imani%20and%20Guojun%20Zhang%20and%20Runjia%20Li%20and%20Jun%20Luo%20and%20Pascal%20Poupart%20and%20Philip%20H.%20S.%20Torr%20and%20Yangchen%20Pan%0AAbstract%3A%20%20%20Recent%20work%20has%20highlighted%20the%20label%20alignment%20property%20%28LAP%29%20in%20supervised%0Alearning%2C%20where%20the%20vector%20of%20all%20labels%20in%20the%20dataset%20is%20mostly%20in%20the%20span%0Aof%20the%20top%20few%20singular%20vectors%20of%20the%20data%20matrix.%20Drawing%20inspiration%20from%0Athis%20observation%2C%20we%20propose%20a%20regularization%20method%20for%20unsupervised%20domain%0Aadaptation%20that%20encourages%20alignment%20between%20the%20predictions%20in%20the%20target%0Adomain%20and%20its%20top%20singular%20vectors.%20Unlike%20conventional%20domain%20adaptation%0Aapproaches%20that%20focus%20on%20regularizing%20representations%2C%20we%20instead%20regularize%0Athe%20classifier%20to%20align%20with%20the%20unsupervised%20target%20data%2C%20guided%20by%20the%20LAP%20in%0Aboth%20the%20source%20and%20target%20domains.%20Theoretical%20analysis%20demonstrates%20that%2C%0Aunder%20certain%20assumptions%2C%20our%20solution%20resides%20within%20the%20span%20of%20the%20top%0Aright%20singular%20vectors%20of%20the%20target%20domain%20data%20and%20aligns%20with%20the%20optimal%0Asolution.%20By%20removing%20the%20reliance%20on%20the%20commonly%20used%20optimal%20joint%20risk%0Aassumption%20found%20in%20classic%20domain%20adaptation%20theory%2C%20we%20showcase%20the%0Aeffectiveness%20of%20our%20method%20on%20addressing%20problems%20where%20traditional%20domain%0Aadaptation%20methods%20often%20fall%20short%20due%20to%20high%20joint%20error.%20Additionally%2C%20we%0Areport%20improved%20performance%20over%20domain%20adaptation%20baselines%20in%20well-known%0Atasks%20such%20as%20MNIST-USPS%20domain%20adaptation%20and%20cross-lingual%20sentiment%0Aanalysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.14960v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLabel%2520Alignment%2520Regularization%2520for%2520Distribution%2520Shift%26entry.906535625%3DEhsan%2520Imani%2520and%2520Guojun%2520Zhang%2520and%2520Runjia%2520Li%2520and%2520Jun%2520Luo%2520and%2520Pascal%2520Poupart%2520and%2520Philip%2520H.%2520S.%2520Torr%2520and%2520Yangchen%2520Pan%26entry.1292438233%3D%2520%2520Recent%2520work%2520has%2520highlighted%2520the%2520label%2520alignment%2520property%2520%2528LAP%2529%2520in%2520supervised%250Alearning%252C%2520where%2520the%2520vector%2520of%2520all%2520labels%2520in%2520the%2520dataset%2520is%2520mostly%2520in%2520the%2520span%250Aof%2520the%2520top%2520few%2520singular%2520vectors%2520of%2520the%2520data%2520matrix.%2520Drawing%2520inspiration%2520from%250Athis%2520observation%252C%2520we%2520propose%2520a%2520regularization%2520method%2520for%2520unsupervised%2520domain%250Aadaptation%2520that%2520encourages%2520alignment%2520between%2520the%2520predictions%2520in%2520the%2520target%250Adomain%2520and%2520its%2520top%2520singular%2520vectors.%2520Unlike%2520conventional%2520domain%2520adaptation%250Aapproaches%2520that%2520focus%2520on%2520regularizing%2520representations%252C%2520we%2520instead%2520regularize%250Athe%2520classifier%2520to%2520align%2520with%2520the%2520unsupervised%2520target%2520data%252C%2520guided%2520by%2520the%2520LAP%2520in%250Aboth%2520the%2520source%2520and%2520target%2520domains.%2520Theoretical%2520analysis%2520demonstrates%2520that%252C%250Aunder%2520certain%2520assumptions%252C%2520our%2520solution%2520resides%2520within%2520the%2520span%2520of%2520the%2520top%250Aright%2520singular%2520vectors%2520of%2520the%2520target%2520domain%2520data%2520and%2520aligns%2520with%2520the%2520optimal%250Asolution.%2520By%2520removing%2520the%2520reliance%2520on%2520the%2520commonly%2520used%2520optimal%2520joint%2520risk%250Aassumption%2520found%2520in%2520classic%2520domain%2520adaptation%2520theory%252C%2520we%2520showcase%2520the%250Aeffectiveness%2520of%2520our%2520method%2520on%2520addressing%2520problems%2520where%2520traditional%2520domain%250Aadaptation%2520methods%2520often%2520fall%2520short%2520due%2520to%2520high%2520joint%2520error.%2520Additionally%252C%2520we%250Areport%2520improved%2520performance%2520over%2520domain%2520adaptation%2520baselines%2520in%2520well-known%250Atasks%2520such%2520as%2520MNIST-USPS%2520domain%2520adaptation%2520and%2520cross-lingual%2520sentiment%250Aanalysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.14960v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Label%20Alignment%20Regularization%20for%20Distribution%20Shift&entry.906535625=Ehsan%20Imani%20and%20Guojun%20Zhang%20and%20Runjia%20Li%20and%20Jun%20Luo%20and%20Pascal%20Poupart%20and%20Philip%20H.%20S.%20Torr%20and%20Yangchen%20Pan&entry.1292438233=%20%20Recent%20work%20has%20highlighted%20the%20label%20alignment%20property%20%28LAP%29%20in%20supervised%0Alearning%2C%20where%20the%20vector%20of%20all%20labels%20in%20the%20dataset%20is%20mostly%20in%20the%20span%0Aof%20the%20top%20few%20singular%20vectors%20of%20the%20data%20matrix.%20Drawing%20inspiration%20from%0Athis%20observation%2C%20we%20propose%20a%20regularization%20method%20for%20unsupervised%20domain%0Aadaptation%20that%20encourages%20alignment%20between%20the%20predictions%20in%20the%20target%0Adomain%20and%20its%20top%20singular%20vectors.%20Unlike%20conventional%20domain%20adaptation%0Aapproaches%20that%20focus%20on%20regularizing%20representations%2C%20we%20instead%20regularize%0Athe%20classifier%20to%20align%20with%20the%20unsupervised%20target%20data%2C%20guided%20by%20the%20LAP%20in%0Aboth%20the%20source%20and%20target%20domains.%20Theoretical%20analysis%20demonstrates%20that%2C%0Aunder%20certain%20assumptions%2C%20our%20solution%20resides%20within%20the%20span%20of%20the%20top%0Aright%20singular%20vectors%20of%20the%20target%20domain%20data%20and%20aligns%20with%20the%20optimal%0Asolution.%20By%20removing%20the%20reliance%20on%20the%20commonly%20used%20optimal%20joint%20risk%0Aassumption%20found%20in%20classic%20domain%20adaptation%20theory%2C%20we%20showcase%20the%0Aeffectiveness%20of%20our%20method%20on%20addressing%20problems%20where%20traditional%20domain%0Aadaptation%20methods%20often%20fall%20short%20due%20to%20high%20joint%20error.%20Additionally%2C%20we%0Areport%20improved%20performance%20over%20domain%20adaptation%20baselines%20in%20well-known%0Atasks%20such%20as%20MNIST-USPS%20domain%20adaptation%20and%20cross-lingual%20sentiment%0Aanalysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.14960v5&entry.124074799=Read"},
{"title": "The Integration of Prediction and Planning in Deep Learning Automated\n  Driving Systems: A Review", "author": "Steffen Hagedorn and Marcel Hallgarten and Martin Stoll and Alexandru Condurache", "abstract": "  Automated driving has the potential to revolutionize personal, public, and\nfreight mobility. Beside accurately perceiving the environment, automated\nvehicles must plan a safe, comfortable, and efficient motion trajectory. To\npromote safety and progress, many works rely on modules that predict the future\nmotion of surrounding traffic. Modular automated driving systems commonly\nhandle prediction and planning as sequential, separate tasks. While this\naccounts for the influence of surrounding traffic on the ego vehicle, it fails\nto anticipate the reactions of traffic participants to the ego vehicle's\nbehavior. Recent methods increasingly integrate prediction and planning in a\njoint or interdependent step to model bidirectional interactions. To date, a\ncomprehensive overview of different integration principles is lacking. We\nsystematically review state-of-the-art deep learning-based planning systems,\nand focus on how they integrate prediction. Different facets of the integration\nranging from system architecture to high-level behavioral aspects are\nconsidered and related to each other. Moreover, we discuss the implications,\nstrengths, and limitations of different integration principles. By pointing out\nresearch gaps, describing relevant future challenges, and highlighting trends\nin the research field, we identify promising directions for future research.\n", "link": "http://arxiv.org/abs/2308.05731v3", "date": "2024-09-11", "relevancy": 2.0907, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5496}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5228}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Integration%20of%20Prediction%20and%20Planning%20in%20Deep%20Learning%20Automated%0A%20%20Driving%20Systems%3A%20A%20Review&body=Title%3A%20The%20Integration%20of%20Prediction%20and%20Planning%20in%20Deep%20Learning%20Automated%0A%20%20Driving%20Systems%3A%20A%20Review%0AAuthor%3A%20Steffen%20Hagedorn%20and%20Marcel%20Hallgarten%20and%20Martin%20Stoll%20and%20Alexandru%20Condurache%0AAbstract%3A%20%20%20Automated%20driving%20has%20the%20potential%20to%20revolutionize%20personal%2C%20public%2C%20and%0Afreight%20mobility.%20Beside%20accurately%20perceiving%20the%20environment%2C%20automated%0Avehicles%20must%20plan%20a%20safe%2C%20comfortable%2C%20and%20efficient%20motion%20trajectory.%20To%0Apromote%20safety%20and%20progress%2C%20many%20works%20rely%20on%20modules%20that%20predict%20the%20future%0Amotion%20of%20surrounding%20traffic.%20Modular%20automated%20driving%20systems%20commonly%0Ahandle%20prediction%20and%20planning%20as%20sequential%2C%20separate%20tasks.%20While%20this%0Aaccounts%20for%20the%20influence%20of%20surrounding%20traffic%20on%20the%20ego%20vehicle%2C%20it%20fails%0Ato%20anticipate%20the%20reactions%20of%20traffic%20participants%20to%20the%20ego%20vehicle%27s%0Abehavior.%20Recent%20methods%20increasingly%20integrate%20prediction%20and%20planning%20in%20a%0Ajoint%20or%20interdependent%20step%20to%20model%20bidirectional%20interactions.%20To%20date%2C%20a%0Acomprehensive%20overview%20of%20different%20integration%20principles%20is%20lacking.%20We%0Asystematically%20review%20state-of-the-art%20deep%20learning-based%20planning%20systems%2C%0Aand%20focus%20on%20how%20they%20integrate%20prediction.%20Different%20facets%20of%20the%20integration%0Aranging%20from%20system%20architecture%20to%20high-level%20behavioral%20aspects%20are%0Aconsidered%20and%20related%20to%20each%20other.%20Moreover%2C%20we%20discuss%20the%20implications%2C%0Astrengths%2C%20and%20limitations%20of%20different%20integration%20principles.%20By%20pointing%20out%0Aresearch%20gaps%2C%20describing%20relevant%20future%20challenges%2C%20and%20highlighting%20trends%0Ain%20the%20research%20field%2C%20we%20identify%20promising%20directions%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.05731v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Integration%2520of%2520Prediction%2520and%2520Planning%2520in%2520Deep%2520Learning%2520Automated%250A%2520%2520Driving%2520Systems%253A%2520A%2520Review%26entry.906535625%3DSteffen%2520Hagedorn%2520and%2520Marcel%2520Hallgarten%2520and%2520Martin%2520Stoll%2520and%2520Alexandru%2520Condurache%26entry.1292438233%3D%2520%2520Automated%2520driving%2520has%2520the%2520potential%2520to%2520revolutionize%2520personal%252C%2520public%252C%2520and%250Afreight%2520mobility.%2520Beside%2520accurately%2520perceiving%2520the%2520environment%252C%2520automated%250Avehicles%2520must%2520plan%2520a%2520safe%252C%2520comfortable%252C%2520and%2520efficient%2520motion%2520trajectory.%2520To%250Apromote%2520safety%2520and%2520progress%252C%2520many%2520works%2520rely%2520on%2520modules%2520that%2520predict%2520the%2520future%250Amotion%2520of%2520surrounding%2520traffic.%2520Modular%2520automated%2520driving%2520systems%2520commonly%250Ahandle%2520prediction%2520and%2520planning%2520as%2520sequential%252C%2520separate%2520tasks.%2520While%2520this%250Aaccounts%2520for%2520the%2520influence%2520of%2520surrounding%2520traffic%2520on%2520the%2520ego%2520vehicle%252C%2520it%2520fails%250Ato%2520anticipate%2520the%2520reactions%2520of%2520traffic%2520participants%2520to%2520the%2520ego%2520vehicle%2527s%250Abehavior.%2520Recent%2520methods%2520increasingly%2520integrate%2520prediction%2520and%2520planning%2520in%2520a%250Ajoint%2520or%2520interdependent%2520step%2520to%2520model%2520bidirectional%2520interactions.%2520To%2520date%252C%2520a%250Acomprehensive%2520overview%2520of%2520different%2520integration%2520principles%2520is%2520lacking.%2520We%250Asystematically%2520review%2520state-of-the-art%2520deep%2520learning-based%2520planning%2520systems%252C%250Aand%2520focus%2520on%2520how%2520they%2520integrate%2520prediction.%2520Different%2520facets%2520of%2520the%2520integration%250Aranging%2520from%2520system%2520architecture%2520to%2520high-level%2520behavioral%2520aspects%2520are%250Aconsidered%2520and%2520related%2520to%2520each%2520other.%2520Moreover%252C%2520we%2520discuss%2520the%2520implications%252C%250Astrengths%252C%2520and%2520limitations%2520of%2520different%2520integration%2520principles.%2520By%2520pointing%2520out%250Aresearch%2520gaps%252C%2520describing%2520relevant%2520future%2520challenges%252C%2520and%2520highlighting%2520trends%250Ain%2520the%2520research%2520field%252C%2520we%2520identify%2520promising%2520directions%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.05731v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Integration%20of%20Prediction%20and%20Planning%20in%20Deep%20Learning%20Automated%0A%20%20Driving%20Systems%3A%20A%20Review&entry.906535625=Steffen%20Hagedorn%20and%20Marcel%20Hallgarten%20and%20Martin%20Stoll%20and%20Alexandru%20Condurache&entry.1292438233=%20%20Automated%20driving%20has%20the%20potential%20to%20revolutionize%20personal%2C%20public%2C%20and%0Afreight%20mobility.%20Beside%20accurately%20perceiving%20the%20environment%2C%20automated%0Avehicles%20must%20plan%20a%20safe%2C%20comfortable%2C%20and%20efficient%20motion%20trajectory.%20To%0Apromote%20safety%20and%20progress%2C%20many%20works%20rely%20on%20modules%20that%20predict%20the%20future%0Amotion%20of%20surrounding%20traffic.%20Modular%20automated%20driving%20systems%20commonly%0Ahandle%20prediction%20and%20planning%20as%20sequential%2C%20separate%20tasks.%20While%20this%0Aaccounts%20for%20the%20influence%20of%20surrounding%20traffic%20on%20the%20ego%20vehicle%2C%20it%20fails%0Ato%20anticipate%20the%20reactions%20of%20traffic%20participants%20to%20the%20ego%20vehicle%27s%0Abehavior.%20Recent%20methods%20increasingly%20integrate%20prediction%20and%20planning%20in%20a%0Ajoint%20or%20interdependent%20step%20to%20model%20bidirectional%20interactions.%20To%20date%2C%20a%0Acomprehensive%20overview%20of%20different%20integration%20principles%20is%20lacking.%20We%0Asystematically%20review%20state-of-the-art%20deep%20learning-based%20planning%20systems%2C%0Aand%20focus%20on%20how%20they%20integrate%20prediction.%20Different%20facets%20of%20the%20integration%0Aranging%20from%20system%20architecture%20to%20high-level%20behavioral%20aspects%20are%0Aconsidered%20and%20related%20to%20each%20other.%20Moreover%2C%20we%20discuss%20the%20implications%2C%0Astrengths%2C%20and%20limitations%20of%20different%20integration%20principles.%20By%20pointing%20out%0Aresearch%20gaps%2C%20describing%20relevant%20future%20challenges%2C%20and%20highlighting%20trends%0Ain%20the%20research%20field%2C%20we%20identify%20promising%20directions%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.05731v3&entry.124074799=Read"},
{"title": "Behavioral Cloning Models Reality Check for Autonomous Driving", "author": "Mustafa Yildirim and Barkin Dagda and Vinal Asodia and Saber Fallah", "abstract": "  How effective are recent advancements in autonomous vehicle perception\nsystems when applied to real-world autonomous vehicle control? While numerous\nvision-based autonomous vehicle systems have been trained and evaluated in\nsimulated environments, there is a notable lack of real-world validation for\nthese systems. This paper addresses this gap by presenting the real-world\nvalidation of state-of-the-art perception systems that utilize Behavior Cloning\n(BC) for lateral control, processing raw image data to predict steering\ncommands. The dataset was collected using a scaled research vehicle and tested\non various track setups. Experimental results demonstrate that these methods\npredict steering angles with low error margins in real-time, indicating\npromising potential for real-world applications.\n", "link": "http://arxiv.org/abs/2409.07218v1", "date": "2024-09-11", "relevancy": 2.0871, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5479}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5415}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Behavioral%20Cloning%20Models%20Reality%20Check%20for%20Autonomous%20Driving&body=Title%3A%20Behavioral%20Cloning%20Models%20Reality%20Check%20for%20Autonomous%20Driving%0AAuthor%3A%20Mustafa%20Yildirim%20and%20Barkin%20Dagda%20and%20Vinal%20Asodia%20and%20Saber%20Fallah%0AAbstract%3A%20%20%20How%20effective%20are%20recent%20advancements%20in%20autonomous%20vehicle%20perception%0Asystems%20when%20applied%20to%20real-world%20autonomous%20vehicle%20control%3F%20While%20numerous%0Avision-based%20autonomous%20vehicle%20systems%20have%20been%20trained%20and%20evaluated%20in%0Asimulated%20environments%2C%20there%20is%20a%20notable%20lack%20of%20real-world%20validation%20for%0Athese%20systems.%20This%20paper%20addresses%20this%20gap%20by%20presenting%20the%20real-world%0Avalidation%20of%20state-of-the-art%20perception%20systems%20that%20utilize%20Behavior%20Cloning%0A%28BC%29%20for%20lateral%20control%2C%20processing%20raw%20image%20data%20to%20predict%20steering%0Acommands.%20The%20dataset%20was%20collected%20using%20a%20scaled%20research%20vehicle%20and%20tested%0Aon%20various%20track%20setups.%20Experimental%20results%20demonstrate%20that%20these%20methods%0Apredict%20steering%20angles%20with%20low%20error%20margins%20in%20real-time%2C%20indicating%0Apromising%20potential%20for%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07218v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBehavioral%2520Cloning%2520Models%2520Reality%2520Check%2520for%2520Autonomous%2520Driving%26entry.906535625%3DMustafa%2520Yildirim%2520and%2520Barkin%2520Dagda%2520and%2520Vinal%2520Asodia%2520and%2520Saber%2520Fallah%26entry.1292438233%3D%2520%2520How%2520effective%2520are%2520recent%2520advancements%2520in%2520autonomous%2520vehicle%2520perception%250Asystems%2520when%2520applied%2520to%2520real-world%2520autonomous%2520vehicle%2520control%253F%2520While%2520numerous%250Avision-based%2520autonomous%2520vehicle%2520systems%2520have%2520been%2520trained%2520and%2520evaluated%2520in%250Asimulated%2520environments%252C%2520there%2520is%2520a%2520notable%2520lack%2520of%2520real-world%2520validation%2520for%250Athese%2520systems.%2520This%2520paper%2520addresses%2520this%2520gap%2520by%2520presenting%2520the%2520real-world%250Avalidation%2520of%2520state-of-the-art%2520perception%2520systems%2520that%2520utilize%2520Behavior%2520Cloning%250A%2528BC%2529%2520for%2520lateral%2520control%252C%2520processing%2520raw%2520image%2520data%2520to%2520predict%2520steering%250Acommands.%2520The%2520dataset%2520was%2520collected%2520using%2520a%2520scaled%2520research%2520vehicle%2520and%2520tested%250Aon%2520various%2520track%2520setups.%2520Experimental%2520results%2520demonstrate%2520that%2520these%2520methods%250Apredict%2520steering%2520angles%2520with%2520low%2520error%2520margins%2520in%2520real-time%252C%2520indicating%250Apromising%2520potential%2520for%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07218v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Behavioral%20Cloning%20Models%20Reality%20Check%20for%20Autonomous%20Driving&entry.906535625=Mustafa%20Yildirim%20and%20Barkin%20Dagda%20and%20Vinal%20Asodia%20and%20Saber%20Fallah&entry.1292438233=%20%20How%20effective%20are%20recent%20advancements%20in%20autonomous%20vehicle%20perception%0Asystems%20when%20applied%20to%20real-world%20autonomous%20vehicle%20control%3F%20While%20numerous%0Avision-based%20autonomous%20vehicle%20systems%20have%20been%20trained%20and%20evaluated%20in%0Asimulated%20environments%2C%20there%20is%20a%20notable%20lack%20of%20real-world%20validation%20for%0Athese%20systems.%20This%20paper%20addresses%20this%20gap%20by%20presenting%20the%20real-world%0Avalidation%20of%20state-of-the-art%20perception%20systems%20that%20utilize%20Behavior%20Cloning%0A%28BC%29%20for%20lateral%20control%2C%20processing%20raw%20image%20data%20to%20predict%20steering%0Acommands.%20The%20dataset%20was%20collected%20using%20a%20scaled%20research%20vehicle%20and%20tested%0Aon%20various%20track%20setups.%20Experimental%20results%20demonstrate%20that%20these%20methods%0Apredict%20steering%20angles%20with%20low%20error%20margins%20in%20real-time%2C%20indicating%0Apromising%20potential%20for%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07218v1&entry.124074799=Read"},
{"title": "A Contrastive Symmetric Forward-Forward Algorithm (SFFA) for Continual\n  Learning Tasks", "author": "Erik B. Terres-Escudero and Javier Del Ser and Pablo Garcia Bringas", "abstract": "  The so-called Forward-Forward Algorithm (FFA) has recently gained momentum as\nan alternative to the conventional back-propagation algorithm for neural\nnetwork learning, yielding competitive performance across various modeling\ntasks. By replacing the backward pass of gradient back-propagation with two\ncontrastive forward passes, the FFA avoids several shortcomings undergone by\nits predecessor (e.g., vanishing/exploding gradient) by enabling layer-wise\ntraining heuristics. In classification tasks, this contrastive method has been\nproven to effectively create a latent sparse representation of the input data,\nultimately favoring discriminability. However, FFA exhibits an inherent\nasymmetric gradient behavior due to an imbalanced loss function between\npositive and negative data, adversely impacting on the model's generalization\ncapabilities and leading to an accuracy degradation. To address this issue,\nthis work proposes the Symmetric Forward-Forward Algorithm (SFFA), a novel\nmodification of the original FFA which partitions each layer into positive and\nnegative neurons. This allows the local fitness function to be defined as the\nratio between the activation of positive neurons and the overall layer\nactivity, resulting in a symmetric loss landscape during the training phase. To\nevaluate the enhanced convergence of our method, we conduct several experiments\nusing multiple image classification benchmarks, comparing the accuracy of\nmodels trained with SFFA to those trained with its FFA counterpart. As a\nbyproduct of this reformulation, we explore the advantages of using a\nlayer-wise training algorithm for Continual Learning (CL) tasks. The\nspecialization of neurons and the sparsity of their activations induced by\nlayer-wise training algorithms enable efficient CL strategies that incorporate\nnew knowledge (classes) into the neural network, while preventing catastrophic\nforgetting of previously...\n", "link": "http://arxiv.org/abs/2409.07387v1", "date": "2024-09-11", "relevancy": 2.0742, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5317}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5138}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Contrastive%20Symmetric%20Forward-Forward%20Algorithm%20%28SFFA%29%20for%20Continual%0A%20%20Learning%20Tasks&body=Title%3A%20A%20Contrastive%20Symmetric%20Forward-Forward%20Algorithm%20%28SFFA%29%20for%20Continual%0A%20%20Learning%20Tasks%0AAuthor%3A%20Erik%20B.%20Terres-Escudero%20and%20Javier%20Del%20Ser%20and%20Pablo%20Garcia%20Bringas%0AAbstract%3A%20%20%20The%20so-called%20Forward-Forward%20Algorithm%20%28FFA%29%20has%20recently%20gained%20momentum%20as%0Aan%20alternative%20to%20the%20conventional%20back-propagation%20algorithm%20for%20neural%0Anetwork%20learning%2C%20yielding%20competitive%20performance%20across%20various%20modeling%0Atasks.%20By%20replacing%20the%20backward%20pass%20of%20gradient%20back-propagation%20with%20two%0Acontrastive%20forward%20passes%2C%20the%20FFA%20avoids%20several%20shortcomings%20undergone%20by%0Aits%20predecessor%20%28e.g.%2C%20vanishing/exploding%20gradient%29%20by%20enabling%20layer-wise%0Atraining%20heuristics.%20In%20classification%20tasks%2C%20this%20contrastive%20method%20has%20been%0Aproven%20to%20effectively%20create%20a%20latent%20sparse%20representation%20of%20the%20input%20data%2C%0Aultimately%20favoring%20discriminability.%20However%2C%20FFA%20exhibits%20an%20inherent%0Aasymmetric%20gradient%20behavior%20due%20to%20an%20imbalanced%20loss%20function%20between%0Apositive%20and%20negative%20data%2C%20adversely%20impacting%20on%20the%20model%27s%20generalization%0Acapabilities%20and%20leading%20to%20an%20accuracy%20degradation.%20To%20address%20this%20issue%2C%0Athis%20work%20proposes%20the%20Symmetric%20Forward-Forward%20Algorithm%20%28SFFA%29%2C%20a%20novel%0Amodification%20of%20the%20original%20FFA%20which%20partitions%20each%20layer%20into%20positive%20and%0Anegative%20neurons.%20This%20allows%20the%20local%20fitness%20function%20to%20be%20defined%20as%20the%0Aratio%20between%20the%20activation%20of%20positive%20neurons%20and%20the%20overall%20layer%0Aactivity%2C%20resulting%20in%20a%20symmetric%20loss%20landscape%20during%20the%20training%20phase.%20To%0Aevaluate%20the%20enhanced%20convergence%20of%20our%20method%2C%20we%20conduct%20several%20experiments%0Ausing%20multiple%20image%20classification%20benchmarks%2C%20comparing%20the%20accuracy%20of%0Amodels%20trained%20with%20SFFA%20to%20those%20trained%20with%20its%20FFA%20counterpart.%20As%20a%0Abyproduct%20of%20this%20reformulation%2C%20we%20explore%20the%20advantages%20of%20using%20a%0Alayer-wise%20training%20algorithm%20for%20Continual%20Learning%20%28CL%29%20tasks.%20The%0Aspecialization%20of%20neurons%20and%20the%20sparsity%20of%20their%20activations%20induced%20by%0Alayer-wise%20training%20algorithms%20enable%20efficient%20CL%20strategies%20that%20incorporate%0Anew%20knowledge%20%28classes%29%20into%20the%20neural%20network%2C%20while%20preventing%20catastrophic%0Aforgetting%20of%20previously...%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07387v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Contrastive%2520Symmetric%2520Forward-Forward%2520Algorithm%2520%2528SFFA%2529%2520for%2520Continual%250A%2520%2520Learning%2520Tasks%26entry.906535625%3DErik%2520B.%2520Terres-Escudero%2520and%2520Javier%2520Del%2520Ser%2520and%2520Pablo%2520Garcia%2520Bringas%26entry.1292438233%3D%2520%2520The%2520so-called%2520Forward-Forward%2520Algorithm%2520%2528FFA%2529%2520has%2520recently%2520gained%2520momentum%2520as%250Aan%2520alternative%2520to%2520the%2520conventional%2520back-propagation%2520algorithm%2520for%2520neural%250Anetwork%2520learning%252C%2520yielding%2520competitive%2520performance%2520across%2520various%2520modeling%250Atasks.%2520By%2520replacing%2520the%2520backward%2520pass%2520of%2520gradient%2520back-propagation%2520with%2520two%250Acontrastive%2520forward%2520passes%252C%2520the%2520FFA%2520avoids%2520several%2520shortcomings%2520undergone%2520by%250Aits%2520predecessor%2520%2528e.g.%252C%2520vanishing/exploding%2520gradient%2529%2520by%2520enabling%2520layer-wise%250Atraining%2520heuristics.%2520In%2520classification%2520tasks%252C%2520this%2520contrastive%2520method%2520has%2520been%250Aproven%2520to%2520effectively%2520create%2520a%2520latent%2520sparse%2520representation%2520of%2520the%2520input%2520data%252C%250Aultimately%2520favoring%2520discriminability.%2520However%252C%2520FFA%2520exhibits%2520an%2520inherent%250Aasymmetric%2520gradient%2520behavior%2520due%2520to%2520an%2520imbalanced%2520loss%2520function%2520between%250Apositive%2520and%2520negative%2520data%252C%2520adversely%2520impacting%2520on%2520the%2520model%2527s%2520generalization%250Acapabilities%2520and%2520leading%2520to%2520an%2520accuracy%2520degradation.%2520To%2520address%2520this%2520issue%252C%250Athis%2520work%2520proposes%2520the%2520Symmetric%2520Forward-Forward%2520Algorithm%2520%2528SFFA%2529%252C%2520a%2520novel%250Amodification%2520of%2520the%2520original%2520FFA%2520which%2520partitions%2520each%2520layer%2520into%2520positive%2520and%250Anegative%2520neurons.%2520This%2520allows%2520the%2520local%2520fitness%2520function%2520to%2520be%2520defined%2520as%2520the%250Aratio%2520between%2520the%2520activation%2520of%2520positive%2520neurons%2520and%2520the%2520overall%2520layer%250Aactivity%252C%2520resulting%2520in%2520a%2520symmetric%2520loss%2520landscape%2520during%2520the%2520training%2520phase.%2520To%250Aevaluate%2520the%2520enhanced%2520convergence%2520of%2520our%2520method%252C%2520we%2520conduct%2520several%2520experiments%250Ausing%2520multiple%2520image%2520classification%2520benchmarks%252C%2520comparing%2520the%2520accuracy%2520of%250Amodels%2520trained%2520with%2520SFFA%2520to%2520those%2520trained%2520with%2520its%2520FFA%2520counterpart.%2520As%2520a%250Abyproduct%2520of%2520this%2520reformulation%252C%2520we%2520explore%2520the%2520advantages%2520of%2520using%2520a%250Alayer-wise%2520training%2520algorithm%2520for%2520Continual%2520Learning%2520%2528CL%2529%2520tasks.%2520The%250Aspecialization%2520of%2520neurons%2520and%2520the%2520sparsity%2520of%2520their%2520activations%2520induced%2520by%250Alayer-wise%2520training%2520algorithms%2520enable%2520efficient%2520CL%2520strategies%2520that%2520incorporate%250Anew%2520knowledge%2520%2528classes%2529%2520into%2520the%2520neural%2520network%252C%2520while%2520preventing%2520catastrophic%250Aforgetting%2520of%2520previously...%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07387v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Contrastive%20Symmetric%20Forward-Forward%20Algorithm%20%28SFFA%29%20for%20Continual%0A%20%20Learning%20Tasks&entry.906535625=Erik%20B.%20Terres-Escudero%20and%20Javier%20Del%20Ser%20and%20Pablo%20Garcia%20Bringas&entry.1292438233=%20%20The%20so-called%20Forward-Forward%20Algorithm%20%28FFA%29%20has%20recently%20gained%20momentum%20as%0Aan%20alternative%20to%20the%20conventional%20back-propagation%20algorithm%20for%20neural%0Anetwork%20learning%2C%20yielding%20competitive%20performance%20across%20various%20modeling%0Atasks.%20By%20replacing%20the%20backward%20pass%20of%20gradient%20back-propagation%20with%20two%0Acontrastive%20forward%20passes%2C%20the%20FFA%20avoids%20several%20shortcomings%20undergone%20by%0Aits%20predecessor%20%28e.g.%2C%20vanishing/exploding%20gradient%29%20by%20enabling%20layer-wise%0Atraining%20heuristics.%20In%20classification%20tasks%2C%20this%20contrastive%20method%20has%20been%0Aproven%20to%20effectively%20create%20a%20latent%20sparse%20representation%20of%20the%20input%20data%2C%0Aultimately%20favoring%20discriminability.%20However%2C%20FFA%20exhibits%20an%20inherent%0Aasymmetric%20gradient%20behavior%20due%20to%20an%20imbalanced%20loss%20function%20between%0Apositive%20and%20negative%20data%2C%20adversely%20impacting%20on%20the%20model%27s%20generalization%0Acapabilities%20and%20leading%20to%20an%20accuracy%20degradation.%20To%20address%20this%20issue%2C%0Athis%20work%20proposes%20the%20Symmetric%20Forward-Forward%20Algorithm%20%28SFFA%29%2C%20a%20novel%0Amodification%20of%20the%20original%20FFA%20which%20partitions%20each%20layer%20into%20positive%20and%0Anegative%20neurons.%20This%20allows%20the%20local%20fitness%20function%20to%20be%20defined%20as%20the%0Aratio%20between%20the%20activation%20of%20positive%20neurons%20and%20the%20overall%20layer%0Aactivity%2C%20resulting%20in%20a%20symmetric%20loss%20landscape%20during%20the%20training%20phase.%20To%0Aevaluate%20the%20enhanced%20convergence%20of%20our%20method%2C%20we%20conduct%20several%20experiments%0Ausing%20multiple%20image%20classification%20benchmarks%2C%20comparing%20the%20accuracy%20of%0Amodels%20trained%20with%20SFFA%20to%20those%20trained%20with%20its%20FFA%20counterpart.%20As%20a%0Abyproduct%20of%20this%20reformulation%2C%20we%20explore%20the%20advantages%20of%20using%20a%0Alayer-wise%20training%20algorithm%20for%20Continual%20Learning%20%28CL%29%20tasks.%20The%0Aspecialization%20of%20neurons%20and%20the%20sparsity%20of%20their%20activations%20induced%20by%0Alayer-wise%20training%20algorithms%20enable%20efficient%20CL%20strategies%20that%20incorporate%0Anew%20knowledge%20%28classes%29%20into%20the%20neural%20network%2C%20while%20preventing%20catastrophic%0Aforgetting%20of%20previously...%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07387v1&entry.124074799=Read"},
{"title": "FullCert: Deterministic End-to-End Certification for Training and\n  Inference of Neural Networks", "author": "Tobias Lorenz and Marta Kwiatkowska and Mario Fritz", "abstract": "  Modern machine learning models are sensitive to the manipulation of both the\ntraining data (poisoning attacks) and inference data (adversarial examples).\nRecognizing this issue, the community has developed many empirical defenses\nagainst both attacks and, more recently, certification methods with provable\nguarantees against inference-time attacks. However, such guarantees are still\nlargely lacking for training-time attacks. In this work, we present FullCert,\nthe first end-to-end certifier with sound, deterministic bounds, which proves\nrobustness against both training-time and inference-time attacks. We first\nbound all possible perturbations an adversary can make to the training data\nunder the considered threat model. Using these constraints, we bound the\nperturbations' influence on the model's parameters. Finally, we bound the\nimpact of these parameter changes on the model's prediction, resulting in joint\nrobustness guarantees against poisoning and adversarial examples. To facilitate\nthis novel certification paradigm, we combine our theoretical work with a new\nopen-source library BoundFlow, which enables model training on bounded\ndatasets. We experimentally demonstrate FullCert's feasibility on two datasets.\n", "link": "http://arxiv.org/abs/2406.11522v2", "date": "2024-09-11", "relevancy": 2.0532, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5341}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5285}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FullCert%3A%20Deterministic%20End-to-End%20Certification%20for%20Training%20and%0A%20%20Inference%20of%20Neural%20Networks&body=Title%3A%20FullCert%3A%20Deterministic%20End-to-End%20Certification%20for%20Training%20and%0A%20%20Inference%20of%20Neural%20Networks%0AAuthor%3A%20Tobias%20Lorenz%20and%20Marta%20Kwiatkowska%20and%20Mario%20Fritz%0AAbstract%3A%20%20%20Modern%20machine%20learning%20models%20are%20sensitive%20to%20the%20manipulation%20of%20both%20the%0Atraining%20data%20%28poisoning%20attacks%29%20and%20inference%20data%20%28adversarial%20examples%29.%0ARecognizing%20this%20issue%2C%20the%20community%20has%20developed%20many%20empirical%20defenses%0Aagainst%20both%20attacks%20and%2C%20more%20recently%2C%20certification%20methods%20with%20provable%0Aguarantees%20against%20inference-time%20attacks.%20However%2C%20such%20guarantees%20are%20still%0Alargely%20lacking%20for%20training-time%20attacks.%20In%20this%20work%2C%20we%20present%20FullCert%2C%0Athe%20first%20end-to-end%20certifier%20with%20sound%2C%20deterministic%20bounds%2C%20which%20proves%0Arobustness%20against%20both%20training-time%20and%20inference-time%20attacks.%20We%20first%0Abound%20all%20possible%20perturbations%20an%20adversary%20can%20make%20to%20the%20training%20data%0Aunder%20the%20considered%20threat%20model.%20Using%20these%20constraints%2C%20we%20bound%20the%0Aperturbations%27%20influence%20on%20the%20model%27s%20parameters.%20Finally%2C%20we%20bound%20the%0Aimpact%20of%20these%20parameter%20changes%20on%20the%20model%27s%20prediction%2C%20resulting%20in%20joint%0Arobustness%20guarantees%20against%20poisoning%20and%20adversarial%20examples.%20To%20facilitate%0Athis%20novel%20certification%20paradigm%2C%20we%20combine%20our%20theoretical%20work%20with%20a%20new%0Aopen-source%20library%20BoundFlow%2C%20which%20enables%20model%20training%20on%20bounded%0Adatasets.%20We%20experimentally%20demonstrate%20FullCert%27s%20feasibility%20on%20two%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11522v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFullCert%253A%2520Deterministic%2520End-to-End%2520Certification%2520for%2520Training%2520and%250A%2520%2520Inference%2520of%2520Neural%2520Networks%26entry.906535625%3DTobias%2520Lorenz%2520and%2520Marta%2520Kwiatkowska%2520and%2520Mario%2520Fritz%26entry.1292438233%3D%2520%2520Modern%2520machine%2520learning%2520models%2520are%2520sensitive%2520to%2520the%2520manipulation%2520of%2520both%2520the%250Atraining%2520data%2520%2528poisoning%2520attacks%2529%2520and%2520inference%2520data%2520%2528adversarial%2520examples%2529.%250ARecognizing%2520this%2520issue%252C%2520the%2520community%2520has%2520developed%2520many%2520empirical%2520defenses%250Aagainst%2520both%2520attacks%2520and%252C%2520more%2520recently%252C%2520certification%2520methods%2520with%2520provable%250Aguarantees%2520against%2520inference-time%2520attacks.%2520However%252C%2520such%2520guarantees%2520are%2520still%250Alargely%2520lacking%2520for%2520training-time%2520attacks.%2520In%2520this%2520work%252C%2520we%2520present%2520FullCert%252C%250Athe%2520first%2520end-to-end%2520certifier%2520with%2520sound%252C%2520deterministic%2520bounds%252C%2520which%2520proves%250Arobustness%2520against%2520both%2520training-time%2520and%2520inference-time%2520attacks.%2520We%2520first%250Abound%2520all%2520possible%2520perturbations%2520an%2520adversary%2520can%2520make%2520to%2520the%2520training%2520data%250Aunder%2520the%2520considered%2520threat%2520model.%2520Using%2520these%2520constraints%252C%2520we%2520bound%2520the%250Aperturbations%2527%2520influence%2520on%2520the%2520model%2527s%2520parameters.%2520Finally%252C%2520we%2520bound%2520the%250Aimpact%2520of%2520these%2520parameter%2520changes%2520on%2520the%2520model%2527s%2520prediction%252C%2520resulting%2520in%2520joint%250Arobustness%2520guarantees%2520against%2520poisoning%2520and%2520adversarial%2520examples.%2520To%2520facilitate%250Athis%2520novel%2520certification%2520paradigm%252C%2520we%2520combine%2520our%2520theoretical%2520work%2520with%2520a%2520new%250Aopen-source%2520library%2520BoundFlow%252C%2520which%2520enables%2520model%2520training%2520on%2520bounded%250Adatasets.%2520We%2520experimentally%2520demonstrate%2520FullCert%2527s%2520feasibility%2520on%2520two%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11522v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FullCert%3A%20Deterministic%20End-to-End%20Certification%20for%20Training%20and%0A%20%20Inference%20of%20Neural%20Networks&entry.906535625=Tobias%20Lorenz%20and%20Marta%20Kwiatkowska%20and%20Mario%20Fritz&entry.1292438233=%20%20Modern%20machine%20learning%20models%20are%20sensitive%20to%20the%20manipulation%20of%20both%20the%0Atraining%20data%20%28poisoning%20attacks%29%20and%20inference%20data%20%28adversarial%20examples%29.%0ARecognizing%20this%20issue%2C%20the%20community%20has%20developed%20many%20empirical%20defenses%0Aagainst%20both%20attacks%20and%2C%20more%20recently%2C%20certification%20methods%20with%20provable%0Aguarantees%20against%20inference-time%20attacks.%20However%2C%20such%20guarantees%20are%20still%0Alargely%20lacking%20for%20training-time%20attacks.%20In%20this%20work%2C%20we%20present%20FullCert%2C%0Athe%20first%20end-to-end%20certifier%20with%20sound%2C%20deterministic%20bounds%2C%20which%20proves%0Arobustness%20against%20both%20training-time%20and%20inference-time%20attacks.%20We%20first%0Abound%20all%20possible%20perturbations%20an%20adversary%20can%20make%20to%20the%20training%20data%0Aunder%20the%20considered%20threat%20model.%20Using%20these%20constraints%2C%20we%20bound%20the%0Aperturbations%27%20influence%20on%20the%20model%27s%20parameters.%20Finally%2C%20we%20bound%20the%0Aimpact%20of%20these%20parameter%20changes%20on%20the%20model%27s%20prediction%2C%20resulting%20in%20joint%0Arobustness%20guarantees%20against%20poisoning%20and%20adversarial%20examples.%20To%20facilitate%0Athis%20novel%20certification%20paradigm%2C%20we%20combine%20our%20theoretical%20work%20with%20a%20new%0Aopen-source%20library%20BoundFlow%2C%20which%20enables%20model%20training%20on%20bounded%0Adatasets.%20We%20experimentally%20demonstrate%20FullCert%27s%20feasibility%20on%20two%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11522v2&entry.124074799=Read"},
{"title": "SECURE: Benchmarking Large Language Models for Cybersecurity Advisory", "author": "Dipkamal Bhusal and Md Tanvirul Alam and Le Nguyen and Ashim Mahara and Zachary Lightcap and Rodney Frazier and Romy Fieblinger and Grace Long Torales and Nidhi Rastogi", "abstract": "  Large Language Models (LLMs) have demonstrated potential in cybersecurity\napplications but have also caused lower confidence due to problems like\nhallucinations and a lack of truthfulness. Existing benchmarks provide general\nevaluations but do not sufficiently address the practical and applied aspects\nof LLM performance in cybersecurity-specific tasks. To address this gap, we\nintroduce the SECURE (Security Extraction, Understanding \\& Reasoning\nEvaluation), a benchmark designed to assess LLMs performance in realistic\ncybersecurity scenarios. SECURE includes six datasets focussed on the\nIndustrial Control System sector to evaluate knowledge extraction,\nunderstanding, and reasoning based on industry-standard sources. Our study\nevaluates seven state-of-the-art models on these tasks, providing insights into\ntheir strengths and weaknesses in cybersecurity contexts, and offer\nrecommendations for improving LLMs reliability as cyber advisory tools.\n", "link": "http://arxiv.org/abs/2405.20441v2", "date": "2024-09-11", "relevancy": 2.0529, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.515}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5129}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SECURE%3A%20Benchmarking%20Large%20Language%20Models%20for%20Cybersecurity%20Advisory&body=Title%3A%20SECURE%3A%20Benchmarking%20Large%20Language%20Models%20for%20Cybersecurity%20Advisory%0AAuthor%3A%20Dipkamal%20Bhusal%20and%20Md%20Tanvirul%20Alam%20and%20Le%20Nguyen%20and%20Ashim%20Mahara%20and%20Zachary%20Lightcap%20and%20Rodney%20Frazier%20and%20Romy%20Fieblinger%20and%20Grace%20Long%20Torales%20and%20Nidhi%20Rastogi%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20potential%20in%20cybersecurity%0Aapplications%20but%20have%20also%20caused%20lower%20confidence%20due%20to%20problems%20like%0Ahallucinations%20and%20a%20lack%20of%20truthfulness.%20Existing%20benchmarks%20provide%20general%0Aevaluations%20but%20do%20not%20sufficiently%20address%20the%20practical%20and%20applied%20aspects%0Aof%20LLM%20performance%20in%20cybersecurity-specific%20tasks.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20the%20SECURE%20%28Security%20Extraction%2C%20Understanding%20%5C%26%20Reasoning%0AEvaluation%29%2C%20a%20benchmark%20designed%20to%20assess%20LLMs%20performance%20in%20realistic%0Acybersecurity%20scenarios.%20SECURE%20includes%20six%20datasets%20focussed%20on%20the%0AIndustrial%20Control%20System%20sector%20to%20evaluate%20knowledge%20extraction%2C%0Aunderstanding%2C%20and%20reasoning%20based%20on%20industry-standard%20sources.%20Our%20study%0Aevaluates%20seven%20state-of-the-art%20models%20on%20these%20tasks%2C%20providing%20insights%20into%0Atheir%20strengths%20and%20weaknesses%20in%20cybersecurity%20contexts%2C%20and%20offer%0Arecommendations%20for%20improving%20LLMs%20reliability%20as%20cyber%20advisory%20tools.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20441v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSECURE%253A%2520Benchmarking%2520Large%2520Language%2520Models%2520for%2520Cybersecurity%2520Advisory%26entry.906535625%3DDipkamal%2520Bhusal%2520and%2520Md%2520Tanvirul%2520Alam%2520and%2520Le%2520Nguyen%2520and%2520Ashim%2520Mahara%2520and%2520Zachary%2520Lightcap%2520and%2520Rodney%2520Frazier%2520and%2520Romy%2520Fieblinger%2520and%2520Grace%2520Long%2520Torales%2520and%2520Nidhi%2520Rastogi%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520potential%2520in%2520cybersecurity%250Aapplications%2520but%2520have%2520also%2520caused%2520lower%2520confidence%2520due%2520to%2520problems%2520like%250Ahallucinations%2520and%2520a%2520lack%2520of%2520truthfulness.%2520Existing%2520benchmarks%2520provide%2520general%250Aevaluations%2520but%2520do%2520not%2520sufficiently%2520address%2520the%2520practical%2520and%2520applied%2520aspects%250Aof%2520LLM%2520performance%2520in%2520cybersecurity-specific%2520tasks.%2520To%2520address%2520this%2520gap%252C%2520we%250Aintroduce%2520the%2520SECURE%2520%2528Security%2520Extraction%252C%2520Understanding%2520%255C%2526%2520Reasoning%250AEvaluation%2529%252C%2520a%2520benchmark%2520designed%2520to%2520assess%2520LLMs%2520performance%2520in%2520realistic%250Acybersecurity%2520scenarios.%2520SECURE%2520includes%2520six%2520datasets%2520focussed%2520on%2520the%250AIndustrial%2520Control%2520System%2520sector%2520to%2520evaluate%2520knowledge%2520extraction%252C%250Aunderstanding%252C%2520and%2520reasoning%2520based%2520on%2520industry-standard%2520sources.%2520Our%2520study%250Aevaluates%2520seven%2520state-of-the-art%2520models%2520on%2520these%2520tasks%252C%2520providing%2520insights%2520into%250Atheir%2520strengths%2520and%2520weaknesses%2520in%2520cybersecurity%2520contexts%252C%2520and%2520offer%250Arecommendations%2520for%2520improving%2520LLMs%2520reliability%2520as%2520cyber%2520advisory%2520tools.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20441v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SECURE%3A%20Benchmarking%20Large%20Language%20Models%20for%20Cybersecurity%20Advisory&entry.906535625=Dipkamal%20Bhusal%20and%20Md%20Tanvirul%20Alam%20and%20Le%20Nguyen%20and%20Ashim%20Mahara%20and%20Zachary%20Lightcap%20and%20Rodney%20Frazier%20and%20Romy%20Fieblinger%20and%20Grace%20Long%20Torales%20and%20Nidhi%20Rastogi&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20potential%20in%20cybersecurity%0Aapplications%20but%20have%20also%20caused%20lower%20confidence%20due%20to%20problems%20like%0Ahallucinations%20and%20a%20lack%20of%20truthfulness.%20Existing%20benchmarks%20provide%20general%0Aevaluations%20but%20do%20not%20sufficiently%20address%20the%20practical%20and%20applied%20aspects%0Aof%20LLM%20performance%20in%20cybersecurity-specific%20tasks.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20the%20SECURE%20%28Security%20Extraction%2C%20Understanding%20%5C%26%20Reasoning%0AEvaluation%29%2C%20a%20benchmark%20designed%20to%20assess%20LLMs%20performance%20in%20realistic%0Acybersecurity%20scenarios.%20SECURE%20includes%20six%20datasets%20focussed%20on%20the%0AIndustrial%20Control%20System%20sector%20to%20evaluate%20knowledge%20extraction%2C%0Aunderstanding%2C%20and%20reasoning%20based%20on%20industry-standard%20sources.%20Our%20study%0Aevaluates%20seven%20state-of-the-art%20models%20on%20these%20tasks%2C%20providing%20insights%20into%0Atheir%20strengths%20and%20weaknesses%20in%20cybersecurity%20contexts%2C%20and%20offer%0Arecommendations%20for%20improving%20LLMs%20reliability%20as%20cyber%20advisory%20tools.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20441v2&entry.124074799=Read"},
{"title": "Distance Measurement for UAVs in Deep Hazardous Tunnels", "author": "Vishal Choudhary and Shashi Kant Gupta and Shaohui Foong and Hock Beng Lim", "abstract": "  The localization of Unmanned aerial vehicles (UAVs) in deep tunnels is\nextremely challenging due to their inaccessibility and hazardous environment.\nConventional outdoor localization techniques (such as using GPS) and indoor\nlocalization techniques (such as those based on WiFi, Infrared (IR),\nUltra-Wideband, etc.) do not work in deep tunnels. We are developing a\nUAV-based system for the inspection of defects in the Deep Tunnel Sewerage\nSystem (DTSS) in Singapore. To enable the UAV localization in the DTSS, we have\ndeveloped a distance measurement module based on the optical flow technique.\nHowever, the standard optical flow technique does not work well in tunnels with\npoor lighting and a lack of features. Thus, we have developed an enhanced\noptical flow algorithm with prediction, to improve the distance measurement for\nUAVs in deep hazardous tunnels.\n", "link": "http://arxiv.org/abs/2409.07160v1", "date": "2024-09-11", "relevancy": 2.0527, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5497}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4896}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distance%20Measurement%20for%20UAVs%20in%20Deep%20Hazardous%20Tunnels&body=Title%3A%20Distance%20Measurement%20for%20UAVs%20in%20Deep%20Hazardous%20Tunnels%0AAuthor%3A%20Vishal%20Choudhary%20and%20Shashi%20Kant%20Gupta%20and%20Shaohui%20Foong%20and%20Hock%20Beng%20Lim%0AAbstract%3A%20%20%20The%20localization%20of%20Unmanned%20aerial%20vehicles%20%28UAVs%29%20in%20deep%20tunnels%20is%0Aextremely%20challenging%20due%20to%20their%20inaccessibility%20and%20hazardous%20environment.%0AConventional%20outdoor%20localization%20techniques%20%28such%20as%20using%20GPS%29%20and%20indoor%0Alocalization%20techniques%20%28such%20as%20those%20based%20on%20WiFi%2C%20Infrared%20%28IR%29%2C%0AUltra-Wideband%2C%20etc.%29%20do%20not%20work%20in%20deep%20tunnels.%20We%20are%20developing%20a%0AUAV-based%20system%20for%20the%20inspection%20of%20defects%20in%20the%20Deep%20Tunnel%20Sewerage%0ASystem%20%28DTSS%29%20in%20Singapore.%20To%20enable%20the%20UAV%20localization%20in%20the%20DTSS%2C%20we%20have%0Adeveloped%20a%20distance%20measurement%20module%20based%20on%20the%20optical%20flow%20technique.%0AHowever%2C%20the%20standard%20optical%20flow%20technique%20does%20not%20work%20well%20in%20tunnels%20with%0Apoor%20lighting%20and%20a%20lack%20of%20features.%20Thus%2C%20we%20have%20developed%20an%20enhanced%0Aoptical%20flow%20algorithm%20with%20prediction%2C%20to%20improve%20the%20distance%20measurement%20for%0AUAVs%20in%20deep%20hazardous%20tunnels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07160v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistance%2520Measurement%2520for%2520UAVs%2520in%2520Deep%2520Hazardous%2520Tunnels%26entry.906535625%3DVishal%2520Choudhary%2520and%2520Shashi%2520Kant%2520Gupta%2520and%2520Shaohui%2520Foong%2520and%2520Hock%2520Beng%2520Lim%26entry.1292438233%3D%2520%2520The%2520localization%2520of%2520Unmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%2520in%2520deep%2520tunnels%2520is%250Aextremely%2520challenging%2520due%2520to%2520their%2520inaccessibility%2520and%2520hazardous%2520environment.%250AConventional%2520outdoor%2520localization%2520techniques%2520%2528such%2520as%2520using%2520GPS%2529%2520and%2520indoor%250Alocalization%2520techniques%2520%2528such%2520as%2520those%2520based%2520on%2520WiFi%252C%2520Infrared%2520%2528IR%2529%252C%250AUltra-Wideband%252C%2520etc.%2529%2520do%2520not%2520work%2520in%2520deep%2520tunnels.%2520We%2520are%2520developing%2520a%250AUAV-based%2520system%2520for%2520the%2520inspection%2520of%2520defects%2520in%2520the%2520Deep%2520Tunnel%2520Sewerage%250ASystem%2520%2528DTSS%2529%2520in%2520Singapore.%2520To%2520enable%2520the%2520UAV%2520localization%2520in%2520the%2520DTSS%252C%2520we%2520have%250Adeveloped%2520a%2520distance%2520measurement%2520module%2520based%2520on%2520the%2520optical%2520flow%2520technique.%250AHowever%252C%2520the%2520standard%2520optical%2520flow%2520technique%2520does%2520not%2520work%2520well%2520in%2520tunnels%2520with%250Apoor%2520lighting%2520and%2520a%2520lack%2520of%2520features.%2520Thus%252C%2520we%2520have%2520developed%2520an%2520enhanced%250Aoptical%2520flow%2520algorithm%2520with%2520prediction%252C%2520to%2520improve%2520the%2520distance%2520measurement%2520for%250AUAVs%2520in%2520deep%2520hazardous%2520tunnels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07160v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distance%20Measurement%20for%20UAVs%20in%20Deep%20Hazardous%20Tunnels&entry.906535625=Vishal%20Choudhary%20and%20Shashi%20Kant%20Gupta%20and%20Shaohui%20Foong%20and%20Hock%20Beng%20Lim&entry.1292438233=%20%20The%20localization%20of%20Unmanned%20aerial%20vehicles%20%28UAVs%29%20in%20deep%20tunnels%20is%0Aextremely%20challenging%20due%20to%20their%20inaccessibility%20and%20hazardous%20environment.%0AConventional%20outdoor%20localization%20techniques%20%28such%20as%20using%20GPS%29%20and%20indoor%0Alocalization%20techniques%20%28such%20as%20those%20based%20on%20WiFi%2C%20Infrared%20%28IR%29%2C%0AUltra-Wideband%2C%20etc.%29%20do%20not%20work%20in%20deep%20tunnels.%20We%20are%20developing%20a%0AUAV-based%20system%20for%20the%20inspection%20of%20defects%20in%20the%20Deep%20Tunnel%20Sewerage%0ASystem%20%28DTSS%29%20in%20Singapore.%20To%20enable%20the%20UAV%20localization%20in%20the%20DTSS%2C%20we%20have%0Adeveloped%20a%20distance%20measurement%20module%20based%20on%20the%20optical%20flow%20technique.%0AHowever%2C%20the%20standard%20optical%20flow%20technique%20does%20not%20work%20well%20in%20tunnels%20with%0Apoor%20lighting%20and%20a%20lack%20of%20features.%20Thus%2C%20we%20have%20developed%20an%20enhanced%0Aoptical%20flow%20algorithm%20with%20prediction%2C%20to%20improve%20the%20distance%20measurement%20for%0AUAVs%20in%20deep%20hazardous%20tunnels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07160v1&entry.124074799=Read"},
{"title": "Training-Free Guidance for Discrete Diffusion Models for Molecular\n  Generation", "author": "Thomas J. Kerby and Kevin R. Moon", "abstract": "  Training-free guidance methods for continuous data have seen an explosion of\ninterest due to the fact that they enable foundation diffusion models to be\npaired with interchangable guidance models. Currently, equivalent guidance\nmethods for discrete diffusion models are unknown. We present a framework for\napplying training-free guidance to discrete data and demonstrate its utility on\nmolecular graph generation tasks using the discrete diffusion model\narchitecture of DiGress. We pair this model with guidance functions that return\nthe proportion of heavy atoms that are a specific atom type and the molecular\nweight of the heavy atoms and demonstrate our method's ability to guide the\ndata generation.\n", "link": "http://arxiv.org/abs/2409.07359v1", "date": "2024-09-11", "relevancy": 2.0512, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.542}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5135}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-Free%20Guidance%20for%20Discrete%20Diffusion%20Models%20for%20Molecular%0A%20%20Generation&body=Title%3A%20Training-Free%20Guidance%20for%20Discrete%20Diffusion%20Models%20for%20Molecular%0A%20%20Generation%0AAuthor%3A%20Thomas%20J.%20Kerby%20and%20Kevin%20R.%20Moon%0AAbstract%3A%20%20%20Training-free%20guidance%20methods%20for%20continuous%20data%20have%20seen%20an%20explosion%20of%0Ainterest%20due%20to%20the%20fact%20that%20they%20enable%20foundation%20diffusion%20models%20to%20be%0Apaired%20with%20interchangable%20guidance%20models.%20Currently%2C%20equivalent%20guidance%0Amethods%20for%20discrete%20diffusion%20models%20are%20unknown.%20We%20present%20a%20framework%20for%0Aapplying%20training-free%20guidance%20to%20discrete%20data%20and%20demonstrate%20its%20utility%20on%0Amolecular%20graph%20generation%20tasks%20using%20the%20discrete%20diffusion%20model%0Aarchitecture%20of%20DiGress.%20We%20pair%20this%20model%20with%20guidance%20functions%20that%20return%0Athe%20proportion%20of%20heavy%20atoms%20that%20are%20a%20specific%20atom%20type%20and%20the%20molecular%0Aweight%20of%20the%20heavy%20atoms%20and%20demonstrate%20our%20method%27s%20ability%20to%20guide%20the%0Adata%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07359v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-Free%2520Guidance%2520for%2520Discrete%2520Diffusion%2520Models%2520for%2520Molecular%250A%2520%2520Generation%26entry.906535625%3DThomas%2520J.%2520Kerby%2520and%2520Kevin%2520R.%2520Moon%26entry.1292438233%3D%2520%2520Training-free%2520guidance%2520methods%2520for%2520continuous%2520data%2520have%2520seen%2520an%2520explosion%2520of%250Ainterest%2520due%2520to%2520the%2520fact%2520that%2520they%2520enable%2520foundation%2520diffusion%2520models%2520to%2520be%250Apaired%2520with%2520interchangable%2520guidance%2520models.%2520Currently%252C%2520equivalent%2520guidance%250Amethods%2520for%2520discrete%2520diffusion%2520models%2520are%2520unknown.%2520We%2520present%2520a%2520framework%2520for%250Aapplying%2520training-free%2520guidance%2520to%2520discrete%2520data%2520and%2520demonstrate%2520its%2520utility%2520on%250Amolecular%2520graph%2520generation%2520tasks%2520using%2520the%2520discrete%2520diffusion%2520model%250Aarchitecture%2520of%2520DiGress.%2520We%2520pair%2520this%2520model%2520with%2520guidance%2520functions%2520that%2520return%250Athe%2520proportion%2520of%2520heavy%2520atoms%2520that%2520are%2520a%2520specific%2520atom%2520type%2520and%2520the%2520molecular%250Aweight%2520of%2520the%2520heavy%2520atoms%2520and%2520demonstrate%2520our%2520method%2527s%2520ability%2520to%2520guide%2520the%250Adata%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07359v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-Free%20Guidance%20for%20Discrete%20Diffusion%20Models%20for%20Molecular%0A%20%20Generation&entry.906535625=Thomas%20J.%20Kerby%20and%20Kevin%20R.%20Moon&entry.1292438233=%20%20Training-free%20guidance%20methods%20for%20continuous%20data%20have%20seen%20an%20explosion%20of%0Ainterest%20due%20to%20the%20fact%20that%20they%20enable%20foundation%20diffusion%20models%20to%20be%0Apaired%20with%20interchangable%20guidance%20models.%20Currently%2C%20equivalent%20guidance%0Amethods%20for%20discrete%20diffusion%20models%20are%20unknown.%20We%20present%20a%20framework%20for%0Aapplying%20training-free%20guidance%20to%20discrete%20data%20and%20demonstrate%20its%20utility%20on%0Amolecular%20graph%20generation%20tasks%20using%20the%20discrete%20diffusion%20model%0Aarchitecture%20of%20DiGress.%20We%20pair%20this%20model%20with%20guidance%20functions%20that%20return%0Athe%20proportion%20of%20heavy%20atoms%20that%20are%20a%20specific%20atom%20type%20and%20the%20molecular%0Aweight%20of%20the%20heavy%20atoms%20and%20demonstrate%20our%20method%27s%20ability%20to%20guide%20the%0Adata%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07359v1&entry.124074799=Read"},
{"title": "Real-Time Human Action Recognition on Embedded Platforms", "author": "Ruiqi Wang and Zichen Wang and Peiqi Gao and Mingzhen Li and Jaehwan Jeong and Yihang Xu and Yejin Lee and Carolyn M. Baum and Lisa Tabor Connor and Chenyang Lu", "abstract": "  With advancements in computer vision and deep learning, video-based human\naction recognition (HAR) has become practical. However, due to the complexity\nof the computation pipeline, running HAR on live video streams incurs excessive\ndelays on embedded platforms. This work tackles the real-time performance\nchallenges of HAR with four contributions: 1) an experimental study identifying\na standard Optical Flow (OF) extraction technique as the latency bottleneck in\na state-of-the-art HAR pipeline, 2) an exploration of the latency-accuracy\ntradeoff between the standard and deep learning approaches to OF extraction,\nwhich highlights the need for a novel, efficient motion feature extractor, 3)\nthe design of Integrated Motion Feature Extractor (IMFE), a novel single-shot\nneural network architecture for motion feature extraction with drastic\nimprovement in latency, 4) the development of RT-HARE, a real-time HAR system\ntailored for embedded platforms. Experimental results on an Nvidia Jetson\nXavier NX platform demonstrated that RT-HARE realizes real-time HAR at a video\nframe rate of 30 frames per second while delivering high levels of recognition\naccuracy.\n", "link": "http://arxiv.org/abs/2409.05662v2", "date": "2024-09-11", "relevancy": 2.0455, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5436}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5075}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Human%20Action%20Recognition%20on%20Embedded%20Platforms&body=Title%3A%20Real-Time%20Human%20Action%20Recognition%20on%20Embedded%20Platforms%0AAuthor%3A%20Ruiqi%20Wang%20and%20Zichen%20Wang%20and%20Peiqi%20Gao%20and%20Mingzhen%20Li%20and%20Jaehwan%20Jeong%20and%20Yihang%20Xu%20and%20Yejin%20Lee%20and%20Carolyn%20M.%20Baum%20and%20Lisa%20Tabor%20Connor%20and%20Chenyang%20Lu%0AAbstract%3A%20%20%20With%20advancements%20in%20computer%20vision%20and%20deep%20learning%2C%20video-based%20human%0Aaction%20recognition%20%28HAR%29%20has%20become%20practical.%20However%2C%20due%20to%20the%20complexity%0Aof%20the%20computation%20pipeline%2C%20running%20HAR%20on%20live%20video%20streams%20incurs%20excessive%0Adelays%20on%20embedded%20platforms.%20This%20work%20tackles%20the%20real-time%20performance%0Achallenges%20of%20HAR%20with%20four%20contributions%3A%201%29%20an%20experimental%20study%20identifying%0Aa%20standard%20Optical%20Flow%20%28OF%29%20extraction%20technique%20as%20the%20latency%20bottleneck%20in%0Aa%20state-of-the-art%20HAR%20pipeline%2C%202%29%20an%20exploration%20of%20the%20latency-accuracy%0Atradeoff%20between%20the%20standard%20and%20deep%20learning%20approaches%20to%20OF%20extraction%2C%0Awhich%20highlights%20the%20need%20for%20a%20novel%2C%20efficient%20motion%20feature%20extractor%2C%203%29%0Athe%20design%20of%20Integrated%20Motion%20Feature%20Extractor%20%28IMFE%29%2C%20a%20novel%20single-shot%0Aneural%20network%20architecture%20for%20motion%20feature%20extraction%20with%20drastic%0Aimprovement%20in%20latency%2C%204%29%20the%20development%20of%20RT-HARE%2C%20a%20real-time%20HAR%20system%0Atailored%20for%20embedded%20platforms.%20Experimental%20results%20on%20an%20Nvidia%20Jetson%0AXavier%20NX%20platform%20demonstrated%20that%20RT-HARE%20realizes%20real-time%20HAR%20at%20a%20video%0Aframe%20rate%20of%2030%20frames%20per%20second%20while%20delivering%20high%20levels%20of%20recognition%0Aaccuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05662v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Human%2520Action%2520Recognition%2520on%2520Embedded%2520Platforms%26entry.906535625%3DRuiqi%2520Wang%2520and%2520Zichen%2520Wang%2520and%2520Peiqi%2520Gao%2520and%2520Mingzhen%2520Li%2520and%2520Jaehwan%2520Jeong%2520and%2520Yihang%2520Xu%2520and%2520Yejin%2520Lee%2520and%2520Carolyn%2520M.%2520Baum%2520and%2520Lisa%2520Tabor%2520Connor%2520and%2520Chenyang%2520Lu%26entry.1292438233%3D%2520%2520With%2520advancements%2520in%2520computer%2520vision%2520and%2520deep%2520learning%252C%2520video-based%2520human%250Aaction%2520recognition%2520%2528HAR%2529%2520has%2520become%2520practical.%2520However%252C%2520due%2520to%2520the%2520complexity%250Aof%2520the%2520computation%2520pipeline%252C%2520running%2520HAR%2520on%2520live%2520video%2520streams%2520incurs%2520excessive%250Adelays%2520on%2520embedded%2520platforms.%2520This%2520work%2520tackles%2520the%2520real-time%2520performance%250Achallenges%2520of%2520HAR%2520with%2520four%2520contributions%253A%25201%2529%2520an%2520experimental%2520study%2520identifying%250Aa%2520standard%2520Optical%2520Flow%2520%2528OF%2529%2520extraction%2520technique%2520as%2520the%2520latency%2520bottleneck%2520in%250Aa%2520state-of-the-art%2520HAR%2520pipeline%252C%25202%2529%2520an%2520exploration%2520of%2520the%2520latency-accuracy%250Atradeoff%2520between%2520the%2520standard%2520and%2520deep%2520learning%2520approaches%2520to%2520OF%2520extraction%252C%250Awhich%2520highlights%2520the%2520need%2520for%2520a%2520novel%252C%2520efficient%2520motion%2520feature%2520extractor%252C%25203%2529%250Athe%2520design%2520of%2520Integrated%2520Motion%2520Feature%2520Extractor%2520%2528IMFE%2529%252C%2520a%2520novel%2520single-shot%250Aneural%2520network%2520architecture%2520for%2520motion%2520feature%2520extraction%2520with%2520drastic%250Aimprovement%2520in%2520latency%252C%25204%2529%2520the%2520development%2520of%2520RT-HARE%252C%2520a%2520real-time%2520HAR%2520system%250Atailored%2520for%2520embedded%2520platforms.%2520Experimental%2520results%2520on%2520an%2520Nvidia%2520Jetson%250AXavier%2520NX%2520platform%2520demonstrated%2520that%2520RT-HARE%2520realizes%2520real-time%2520HAR%2520at%2520a%2520video%250Aframe%2520rate%2520of%252030%2520frames%2520per%2520second%2520while%2520delivering%2520high%2520levels%2520of%2520recognition%250Aaccuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05662v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Human%20Action%20Recognition%20on%20Embedded%20Platforms&entry.906535625=Ruiqi%20Wang%20and%20Zichen%20Wang%20and%20Peiqi%20Gao%20and%20Mingzhen%20Li%20and%20Jaehwan%20Jeong%20and%20Yihang%20Xu%20and%20Yejin%20Lee%20and%20Carolyn%20M.%20Baum%20and%20Lisa%20Tabor%20Connor%20and%20Chenyang%20Lu&entry.1292438233=%20%20With%20advancements%20in%20computer%20vision%20and%20deep%20learning%2C%20video-based%20human%0Aaction%20recognition%20%28HAR%29%20has%20become%20practical.%20However%2C%20due%20to%20the%20complexity%0Aof%20the%20computation%20pipeline%2C%20running%20HAR%20on%20live%20video%20streams%20incurs%20excessive%0Adelays%20on%20embedded%20platforms.%20This%20work%20tackles%20the%20real-time%20performance%0Achallenges%20of%20HAR%20with%20four%20contributions%3A%201%29%20an%20experimental%20study%20identifying%0Aa%20standard%20Optical%20Flow%20%28OF%29%20extraction%20technique%20as%20the%20latency%20bottleneck%20in%0Aa%20state-of-the-art%20HAR%20pipeline%2C%202%29%20an%20exploration%20of%20the%20latency-accuracy%0Atradeoff%20between%20the%20standard%20and%20deep%20learning%20approaches%20to%20OF%20extraction%2C%0Awhich%20highlights%20the%20need%20for%20a%20novel%2C%20efficient%20motion%20feature%20extractor%2C%203%29%0Athe%20design%20of%20Integrated%20Motion%20Feature%20Extractor%20%28IMFE%29%2C%20a%20novel%20single-shot%0Aneural%20network%20architecture%20for%20motion%20feature%20extraction%20with%20drastic%0Aimprovement%20in%20latency%2C%204%29%20the%20development%20of%20RT-HARE%2C%20a%20real-time%20HAR%20system%0Atailored%20for%20embedded%20platforms.%20Experimental%20results%20on%20an%20Nvidia%20Jetson%0AXavier%20NX%20platform%20demonstrated%20that%20RT-HARE%20realizes%20real-time%20HAR%20at%20a%20video%0Aframe%20rate%20of%2030%20frames%20per%20second%20while%20delivering%20high%20levels%20of%20recognition%0Aaccuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05662v2&entry.124074799=Read"},
{"title": "On the Improvement of Generalization and Stability of Forward-Only\n  Learning via Neural Polarization", "author": "Erik B. Terres-Escudero and Javier Del Ser and Pablo Garcia-Bringas", "abstract": "  Forward-only learning algorithms have recently gained attention as\nalternatives to gradient backpropagation, replacing the backward step of this\nlatter solver with an additional contrastive forward pass. Among these\napproaches, the so-called Forward-Forward Algorithm (FFA) has been shown to\nachieve competitive levels of performance in terms of generalization and\ncomplexity. Networks trained using FFA learn to contrastively maximize a\nlayer-wise defined goodness score when presented with real data (denoted as\npositive samples) and to minimize it when processing synthetic data (corr.\nnegative samples). However, this algorithm still faces weaknesses that\nnegatively affect the model accuracy and training stability, primarily due to a\ngradient imbalance between positive and negative samples. To overcome this\nissue, in this work we propose a novel implementation of the FFA algorithm,\ndenoted as Polar-FFA, which extends the original formulation by introducing a\nneural division (\\emph{polarization}) between positive and negative instances.\nNeurons in each of these groups aim to maximize their goodness when presented\nwith their respective data type, thereby creating a symmetric gradient\nbehavior. To empirically gauge the improved learning capabilities of our\nproposed Polar-FFA, we perform several systematic experiments using different\nactivation and goodness functions over image classification datasets. Our\nresults demonstrate that Polar-FFA outperforms FFA in terms of accuracy and\nconvergence speed. Furthermore, its lower reliance on hyperparameters reduces\nthe need for hyperparameter tuning to guarantee optimal generalization\ncapabilities, thereby allowing for a broader range of neural network\nconfigurations.\n", "link": "http://arxiv.org/abs/2408.09210v2", "date": "2024-09-11", "relevancy": 2.0435, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5232}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5047}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4957}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Improvement%20of%20Generalization%20and%20Stability%20of%20Forward-Only%0A%20%20Learning%20via%20Neural%20Polarization&body=Title%3A%20On%20the%20Improvement%20of%20Generalization%20and%20Stability%20of%20Forward-Only%0A%20%20Learning%20via%20Neural%20Polarization%0AAuthor%3A%20Erik%20B.%20Terres-Escudero%20and%20Javier%20Del%20Ser%20and%20Pablo%20Garcia-Bringas%0AAbstract%3A%20%20%20Forward-only%20learning%20algorithms%20have%20recently%20gained%20attention%20as%0Aalternatives%20to%20gradient%20backpropagation%2C%20replacing%20the%20backward%20step%20of%20this%0Alatter%20solver%20with%20an%20additional%20contrastive%20forward%20pass.%20Among%20these%0Aapproaches%2C%20the%20so-called%20Forward-Forward%20Algorithm%20%28FFA%29%20has%20been%20shown%20to%0Aachieve%20competitive%20levels%20of%20performance%20in%20terms%20of%20generalization%20and%0Acomplexity.%20Networks%20trained%20using%20FFA%20learn%20to%20contrastively%20maximize%20a%0Alayer-wise%20defined%20goodness%20score%20when%20presented%20with%20real%20data%20%28denoted%20as%0Apositive%20samples%29%20and%20to%20minimize%20it%20when%20processing%20synthetic%20data%20%28corr.%0Anegative%20samples%29.%20However%2C%20this%20algorithm%20still%20faces%20weaknesses%20that%0Anegatively%20affect%20the%20model%20accuracy%20and%20training%20stability%2C%20primarily%20due%20to%20a%0Agradient%20imbalance%20between%20positive%20and%20negative%20samples.%20To%20overcome%20this%0Aissue%2C%20in%20this%20work%20we%20propose%20a%20novel%20implementation%20of%20the%20FFA%20algorithm%2C%0Adenoted%20as%20Polar-FFA%2C%20which%20extends%20the%20original%20formulation%20by%20introducing%20a%0Aneural%20division%20%28%5Cemph%7Bpolarization%7D%29%20between%20positive%20and%20negative%20instances.%0ANeurons%20in%20each%20of%20these%20groups%20aim%20to%20maximize%20their%20goodness%20when%20presented%0Awith%20their%20respective%20data%20type%2C%20thereby%20creating%20a%20symmetric%20gradient%0Abehavior.%20To%20empirically%20gauge%20the%20improved%20learning%20capabilities%20of%20our%0Aproposed%20Polar-FFA%2C%20we%20perform%20several%20systematic%20experiments%20using%20different%0Aactivation%20and%20goodness%20functions%20over%20image%20classification%20datasets.%20Our%0Aresults%20demonstrate%20that%20Polar-FFA%20outperforms%20FFA%20in%20terms%20of%20accuracy%20and%0Aconvergence%20speed.%20Furthermore%2C%20its%20lower%20reliance%20on%20hyperparameters%20reduces%0Athe%20need%20for%20hyperparameter%20tuning%20to%20guarantee%20optimal%20generalization%0Acapabilities%2C%20thereby%20allowing%20for%20a%20broader%20range%20of%20neural%20network%0Aconfigurations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09210v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Improvement%2520of%2520Generalization%2520and%2520Stability%2520of%2520Forward-Only%250A%2520%2520Learning%2520via%2520Neural%2520Polarization%26entry.906535625%3DErik%2520B.%2520Terres-Escudero%2520and%2520Javier%2520Del%2520Ser%2520and%2520Pablo%2520Garcia-Bringas%26entry.1292438233%3D%2520%2520Forward-only%2520learning%2520algorithms%2520have%2520recently%2520gained%2520attention%2520as%250Aalternatives%2520to%2520gradient%2520backpropagation%252C%2520replacing%2520the%2520backward%2520step%2520of%2520this%250Alatter%2520solver%2520with%2520an%2520additional%2520contrastive%2520forward%2520pass.%2520Among%2520these%250Aapproaches%252C%2520the%2520so-called%2520Forward-Forward%2520Algorithm%2520%2528FFA%2529%2520has%2520been%2520shown%2520to%250Aachieve%2520competitive%2520levels%2520of%2520performance%2520in%2520terms%2520of%2520generalization%2520and%250Acomplexity.%2520Networks%2520trained%2520using%2520FFA%2520learn%2520to%2520contrastively%2520maximize%2520a%250Alayer-wise%2520defined%2520goodness%2520score%2520when%2520presented%2520with%2520real%2520data%2520%2528denoted%2520as%250Apositive%2520samples%2529%2520and%2520to%2520minimize%2520it%2520when%2520processing%2520synthetic%2520data%2520%2528corr.%250Anegative%2520samples%2529.%2520However%252C%2520this%2520algorithm%2520still%2520faces%2520weaknesses%2520that%250Anegatively%2520affect%2520the%2520model%2520accuracy%2520and%2520training%2520stability%252C%2520primarily%2520due%2520to%2520a%250Agradient%2520imbalance%2520between%2520positive%2520and%2520negative%2520samples.%2520To%2520overcome%2520this%250Aissue%252C%2520in%2520this%2520work%2520we%2520propose%2520a%2520novel%2520implementation%2520of%2520the%2520FFA%2520algorithm%252C%250Adenoted%2520as%2520Polar-FFA%252C%2520which%2520extends%2520the%2520original%2520formulation%2520by%2520introducing%2520a%250Aneural%2520division%2520%2528%255Cemph%257Bpolarization%257D%2529%2520between%2520positive%2520and%2520negative%2520instances.%250ANeurons%2520in%2520each%2520of%2520these%2520groups%2520aim%2520to%2520maximize%2520their%2520goodness%2520when%2520presented%250Awith%2520their%2520respective%2520data%2520type%252C%2520thereby%2520creating%2520a%2520symmetric%2520gradient%250Abehavior.%2520To%2520empirically%2520gauge%2520the%2520improved%2520learning%2520capabilities%2520of%2520our%250Aproposed%2520Polar-FFA%252C%2520we%2520perform%2520several%2520systematic%2520experiments%2520using%2520different%250Aactivation%2520and%2520goodness%2520functions%2520over%2520image%2520classification%2520datasets.%2520Our%250Aresults%2520demonstrate%2520that%2520Polar-FFA%2520outperforms%2520FFA%2520in%2520terms%2520of%2520accuracy%2520and%250Aconvergence%2520speed.%2520Furthermore%252C%2520its%2520lower%2520reliance%2520on%2520hyperparameters%2520reduces%250Athe%2520need%2520for%2520hyperparameter%2520tuning%2520to%2520guarantee%2520optimal%2520generalization%250Acapabilities%252C%2520thereby%2520allowing%2520for%2520a%2520broader%2520range%2520of%2520neural%2520network%250Aconfigurations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09210v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Improvement%20of%20Generalization%20and%20Stability%20of%20Forward-Only%0A%20%20Learning%20via%20Neural%20Polarization&entry.906535625=Erik%20B.%20Terres-Escudero%20and%20Javier%20Del%20Ser%20and%20Pablo%20Garcia-Bringas&entry.1292438233=%20%20Forward-only%20learning%20algorithms%20have%20recently%20gained%20attention%20as%0Aalternatives%20to%20gradient%20backpropagation%2C%20replacing%20the%20backward%20step%20of%20this%0Alatter%20solver%20with%20an%20additional%20contrastive%20forward%20pass.%20Among%20these%0Aapproaches%2C%20the%20so-called%20Forward-Forward%20Algorithm%20%28FFA%29%20has%20been%20shown%20to%0Aachieve%20competitive%20levels%20of%20performance%20in%20terms%20of%20generalization%20and%0Acomplexity.%20Networks%20trained%20using%20FFA%20learn%20to%20contrastively%20maximize%20a%0Alayer-wise%20defined%20goodness%20score%20when%20presented%20with%20real%20data%20%28denoted%20as%0Apositive%20samples%29%20and%20to%20minimize%20it%20when%20processing%20synthetic%20data%20%28corr.%0Anegative%20samples%29.%20However%2C%20this%20algorithm%20still%20faces%20weaknesses%20that%0Anegatively%20affect%20the%20model%20accuracy%20and%20training%20stability%2C%20primarily%20due%20to%20a%0Agradient%20imbalance%20between%20positive%20and%20negative%20samples.%20To%20overcome%20this%0Aissue%2C%20in%20this%20work%20we%20propose%20a%20novel%20implementation%20of%20the%20FFA%20algorithm%2C%0Adenoted%20as%20Polar-FFA%2C%20which%20extends%20the%20original%20formulation%20by%20introducing%20a%0Aneural%20division%20%28%5Cemph%7Bpolarization%7D%29%20between%20positive%20and%20negative%20instances.%0ANeurons%20in%20each%20of%20these%20groups%20aim%20to%20maximize%20their%20goodness%20when%20presented%0Awith%20their%20respective%20data%20type%2C%20thereby%20creating%20a%20symmetric%20gradient%0Abehavior.%20To%20empirically%20gauge%20the%20improved%20learning%20capabilities%20of%20our%0Aproposed%20Polar-FFA%2C%20we%20perform%20several%20systematic%20experiments%20using%20different%0Aactivation%20and%20goodness%20functions%20over%20image%20classification%20datasets.%20Our%0Aresults%20demonstrate%20that%20Polar-FFA%20outperforms%20FFA%20in%20terms%20of%20accuracy%20and%0Aconvergence%20speed.%20Furthermore%2C%20its%20lower%20reliance%20on%20hyperparameters%20reduces%0Athe%20need%20for%20hyperparameter%20tuning%20to%20guarantee%20optimal%20generalization%0Acapabilities%2C%20thereby%20allowing%20for%20a%20broader%20range%20of%20neural%20network%0Aconfigurations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09210v2&entry.124074799=Read"},
{"title": "Explanation, Debate, Align: A Weak-to-Strong Framework for Language\n  Model Generalization", "author": "Mehrdad Zakershahrak and Samira Ghodratnama", "abstract": "  The rapid advancement of artificial intelligence systems has brought the\nchallenge of AI alignment to the forefront of research, particularly in complex\ndecision-making and task execution. As these systems surpass human-level\nperformance in sophisticated problems, ensuring their alignment with human\nvalues, intentions, and ethical guidelines becomes crucial. Building on\nprevious work in explanation generation for human-agent alignment, we address\nthe more complex dynamics of multi-agent systems and human-AI teams. This paper\nintroduces a novel approach to model alignment through weak-to-strong\ngeneralization in the context of language models. We present a framework where\na strong model facilitates the improvement of a weaker model, bridging the gap\nbetween explanation generation and model alignment. Our method, formalized as a\nfacilitation function, allows for the transfer of capabilities from advanced\nmodels to less capable ones without direct access to extensive training data.\nOur results suggest that this facilitation-based approach not only enhances\nmodel performance but also provides insights into the nature of model alignment\nand the potential for scalable oversight of AI systems.\n", "link": "http://arxiv.org/abs/2409.07335v1", "date": "2024-09-11", "relevancy": 2.0427, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5177}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5177}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explanation%2C%20Debate%2C%20Align%3A%20A%20Weak-to-Strong%20Framework%20for%20Language%0A%20%20Model%20Generalization&body=Title%3A%20Explanation%2C%20Debate%2C%20Align%3A%20A%20Weak-to-Strong%20Framework%20for%20Language%0A%20%20Model%20Generalization%0AAuthor%3A%20Mehrdad%20Zakershahrak%20and%20Samira%20Ghodratnama%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20artificial%20intelligence%20systems%20has%20brought%20the%0Achallenge%20of%20AI%20alignment%20to%20the%20forefront%20of%20research%2C%20particularly%20in%20complex%0Adecision-making%20and%20task%20execution.%20As%20these%20systems%20surpass%20human-level%0Aperformance%20in%20sophisticated%20problems%2C%20ensuring%20their%20alignment%20with%20human%0Avalues%2C%20intentions%2C%20and%20ethical%20guidelines%20becomes%20crucial.%20Building%20on%0Aprevious%20work%20in%20explanation%20generation%20for%20human-agent%20alignment%2C%20we%20address%0Athe%20more%20complex%20dynamics%20of%20multi-agent%20systems%20and%20human-AI%20teams.%20This%20paper%0Aintroduces%20a%20novel%20approach%20to%20model%20alignment%20through%20weak-to-strong%0Ageneralization%20in%20the%20context%20of%20language%20models.%20We%20present%20a%20framework%20where%0Aa%20strong%20model%20facilitates%20the%20improvement%20of%20a%20weaker%20model%2C%20bridging%20the%20gap%0Abetween%20explanation%20generation%20and%20model%20alignment.%20Our%20method%2C%20formalized%20as%20a%0Afacilitation%20function%2C%20allows%20for%20the%20transfer%20of%20capabilities%20from%20advanced%0Amodels%20to%20less%20capable%20ones%20without%20direct%20access%20to%20extensive%20training%20data.%0AOur%20results%20suggest%20that%20this%20facilitation-based%20approach%20not%20only%20enhances%0Amodel%20performance%20but%20also%20provides%20insights%20into%20the%20nature%20of%20model%20alignment%0Aand%20the%20potential%20for%20scalable%20oversight%20of%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07335v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplanation%252C%2520Debate%252C%2520Align%253A%2520A%2520Weak-to-Strong%2520Framework%2520for%2520Language%250A%2520%2520Model%2520Generalization%26entry.906535625%3DMehrdad%2520Zakershahrak%2520and%2520Samira%2520Ghodratnama%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520artificial%2520intelligence%2520systems%2520has%2520brought%2520the%250Achallenge%2520of%2520AI%2520alignment%2520to%2520the%2520forefront%2520of%2520research%252C%2520particularly%2520in%2520complex%250Adecision-making%2520and%2520task%2520execution.%2520As%2520these%2520systems%2520surpass%2520human-level%250Aperformance%2520in%2520sophisticated%2520problems%252C%2520ensuring%2520their%2520alignment%2520with%2520human%250Avalues%252C%2520intentions%252C%2520and%2520ethical%2520guidelines%2520becomes%2520crucial.%2520Building%2520on%250Aprevious%2520work%2520in%2520explanation%2520generation%2520for%2520human-agent%2520alignment%252C%2520we%2520address%250Athe%2520more%2520complex%2520dynamics%2520of%2520multi-agent%2520systems%2520and%2520human-AI%2520teams.%2520This%2520paper%250Aintroduces%2520a%2520novel%2520approach%2520to%2520model%2520alignment%2520through%2520weak-to-strong%250Ageneralization%2520in%2520the%2520context%2520of%2520language%2520models.%2520We%2520present%2520a%2520framework%2520where%250Aa%2520strong%2520model%2520facilitates%2520the%2520improvement%2520of%2520a%2520weaker%2520model%252C%2520bridging%2520the%2520gap%250Abetween%2520explanation%2520generation%2520and%2520model%2520alignment.%2520Our%2520method%252C%2520formalized%2520as%2520a%250Afacilitation%2520function%252C%2520allows%2520for%2520the%2520transfer%2520of%2520capabilities%2520from%2520advanced%250Amodels%2520to%2520less%2520capable%2520ones%2520without%2520direct%2520access%2520to%2520extensive%2520training%2520data.%250AOur%2520results%2520suggest%2520that%2520this%2520facilitation-based%2520approach%2520not%2520only%2520enhances%250Amodel%2520performance%2520but%2520also%2520provides%2520insights%2520into%2520the%2520nature%2520of%2520model%2520alignment%250Aand%2520the%2520potential%2520for%2520scalable%2520oversight%2520of%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07335v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explanation%2C%20Debate%2C%20Align%3A%20A%20Weak-to-Strong%20Framework%20for%20Language%0A%20%20Model%20Generalization&entry.906535625=Mehrdad%20Zakershahrak%20and%20Samira%20Ghodratnama&entry.1292438233=%20%20The%20rapid%20advancement%20of%20artificial%20intelligence%20systems%20has%20brought%20the%0Achallenge%20of%20AI%20alignment%20to%20the%20forefront%20of%20research%2C%20particularly%20in%20complex%0Adecision-making%20and%20task%20execution.%20As%20these%20systems%20surpass%20human-level%0Aperformance%20in%20sophisticated%20problems%2C%20ensuring%20their%20alignment%20with%20human%0Avalues%2C%20intentions%2C%20and%20ethical%20guidelines%20becomes%20crucial.%20Building%20on%0Aprevious%20work%20in%20explanation%20generation%20for%20human-agent%20alignment%2C%20we%20address%0Athe%20more%20complex%20dynamics%20of%20multi-agent%20systems%20and%20human-AI%20teams.%20This%20paper%0Aintroduces%20a%20novel%20approach%20to%20model%20alignment%20through%20weak-to-strong%0Ageneralization%20in%20the%20context%20of%20language%20models.%20We%20present%20a%20framework%20where%0Aa%20strong%20model%20facilitates%20the%20improvement%20of%20a%20weaker%20model%2C%20bridging%20the%20gap%0Abetween%20explanation%20generation%20and%20model%20alignment.%20Our%20method%2C%20formalized%20as%20a%0Afacilitation%20function%2C%20allows%20for%20the%20transfer%20of%20capabilities%20from%20advanced%0Amodels%20to%20less%20capable%20ones%20without%20direct%20access%20to%20extensive%20training%20data.%0AOur%20results%20suggest%20that%20this%20facilitation-based%20approach%20not%20only%20enhances%0Amodel%20performance%20but%20also%20provides%20insights%20into%20the%20nature%20of%20model%20alignment%0Aand%20the%20potential%20for%20scalable%20oversight%20of%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07335v1&entry.124074799=Read"},
{"title": "ART: Artifact Removal Transformer for Reconstructing Noise-Free\n  Multichannel Electroencephalographic Signals", "author": "Chun-Hsiang Chuang and Kong-Yi Chang and Chih-Sheng Huang and Anne-Mei Bessas", "abstract": "  Artifact removal in electroencephalography (EEG) is a longstanding challenge\nthat significantly impacts neuroscientific analysis and brain-computer\ninterface (BCI) performance. Tackling this problem demands advanced algorithms,\nextensive noisy-clean training data, and thorough evaluation strategies. This\nstudy presents the Artifact Removal Transformer (ART), an innovative EEG\ndenoising model employing transformer architecture to adeptly capture the\ntransient millisecond-scale dynamics characteristic of EEG signals. Our\napproach offers a holistic, end-to-end denoising solution for diverse artifact\ntypes in multichannel EEG data. We enhanced the generation of noisy-clean EEG\ndata pairs using an independent component analysis, thus fortifying the\ntraining scenarios critical for effective supervised learning. We performed\ncomprehensive validations using a wide range of open datasets from various BCI\napplications, employing metrics like mean squared error and signal-to-noise\nratio, as well as sophisticated techniques such as source localization and EEG\ncomponent classification. Our evaluations confirm that ART surpasses other\ndeep-learning-based artifact removal methods, setting a new benchmark in EEG\nsignal processing. This advancement not only boosts the accuracy and\nreliability of artifact removal but also promises to catalyze further\ninnovations in the field, facilitating the study of brain dynamics in\nnaturalistic environments.\n", "link": "http://arxiv.org/abs/2409.07326v1", "date": "2024-09-11", "relevancy": 2.0192, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5449}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5223}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ART%3A%20Artifact%20Removal%20Transformer%20for%20Reconstructing%20Noise-Free%0A%20%20Multichannel%20Electroencephalographic%20Signals&body=Title%3A%20ART%3A%20Artifact%20Removal%20Transformer%20for%20Reconstructing%20Noise-Free%0A%20%20Multichannel%20Electroencephalographic%20Signals%0AAuthor%3A%20Chun-Hsiang%20Chuang%20and%20Kong-Yi%20Chang%20and%20Chih-Sheng%20Huang%20and%20Anne-Mei%20Bessas%0AAbstract%3A%20%20%20Artifact%20removal%20in%20electroencephalography%20%28EEG%29%20is%20a%20longstanding%20challenge%0Athat%20significantly%20impacts%20neuroscientific%20analysis%20and%20brain-computer%0Ainterface%20%28BCI%29%20performance.%20Tackling%20this%20problem%20demands%20advanced%20algorithms%2C%0Aextensive%20noisy-clean%20training%20data%2C%20and%20thorough%20evaluation%20strategies.%20This%0Astudy%20presents%20the%20Artifact%20Removal%20Transformer%20%28ART%29%2C%20an%20innovative%20EEG%0Adenoising%20model%20employing%20transformer%20architecture%20to%20adeptly%20capture%20the%0Atransient%20millisecond-scale%20dynamics%20characteristic%20of%20EEG%20signals.%20Our%0Aapproach%20offers%20a%20holistic%2C%20end-to-end%20denoising%20solution%20for%20diverse%20artifact%0Atypes%20in%20multichannel%20EEG%20data.%20We%20enhanced%20the%20generation%20of%20noisy-clean%20EEG%0Adata%20pairs%20using%20an%20independent%20component%20analysis%2C%20thus%20fortifying%20the%0Atraining%20scenarios%20critical%20for%20effective%20supervised%20learning.%20We%20performed%0Acomprehensive%20validations%20using%20a%20wide%20range%20of%20open%20datasets%20from%20various%20BCI%0Aapplications%2C%20employing%20metrics%20like%20mean%20squared%20error%20and%20signal-to-noise%0Aratio%2C%20as%20well%20as%20sophisticated%20techniques%20such%20as%20source%20localization%20and%20EEG%0Acomponent%20classification.%20Our%20evaluations%20confirm%20that%20ART%20surpasses%20other%0Adeep-learning-based%20artifact%20removal%20methods%2C%20setting%20a%20new%20benchmark%20in%20EEG%0Asignal%20processing.%20This%20advancement%20not%20only%20boosts%20the%20accuracy%20and%0Areliability%20of%20artifact%20removal%20but%20also%20promises%20to%20catalyze%20further%0Ainnovations%20in%20the%20field%2C%20facilitating%20the%20study%20of%20brain%20dynamics%20in%0Anaturalistic%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07326v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DART%253A%2520Artifact%2520Removal%2520Transformer%2520for%2520Reconstructing%2520Noise-Free%250A%2520%2520Multichannel%2520Electroencephalographic%2520Signals%26entry.906535625%3DChun-Hsiang%2520Chuang%2520and%2520Kong-Yi%2520Chang%2520and%2520Chih-Sheng%2520Huang%2520and%2520Anne-Mei%2520Bessas%26entry.1292438233%3D%2520%2520Artifact%2520removal%2520in%2520electroencephalography%2520%2528EEG%2529%2520is%2520a%2520longstanding%2520challenge%250Athat%2520significantly%2520impacts%2520neuroscientific%2520analysis%2520and%2520brain-computer%250Ainterface%2520%2528BCI%2529%2520performance.%2520Tackling%2520this%2520problem%2520demands%2520advanced%2520algorithms%252C%250Aextensive%2520noisy-clean%2520training%2520data%252C%2520and%2520thorough%2520evaluation%2520strategies.%2520This%250Astudy%2520presents%2520the%2520Artifact%2520Removal%2520Transformer%2520%2528ART%2529%252C%2520an%2520innovative%2520EEG%250Adenoising%2520model%2520employing%2520transformer%2520architecture%2520to%2520adeptly%2520capture%2520the%250Atransient%2520millisecond-scale%2520dynamics%2520characteristic%2520of%2520EEG%2520signals.%2520Our%250Aapproach%2520offers%2520a%2520holistic%252C%2520end-to-end%2520denoising%2520solution%2520for%2520diverse%2520artifact%250Atypes%2520in%2520multichannel%2520EEG%2520data.%2520We%2520enhanced%2520the%2520generation%2520of%2520noisy-clean%2520EEG%250Adata%2520pairs%2520using%2520an%2520independent%2520component%2520analysis%252C%2520thus%2520fortifying%2520the%250Atraining%2520scenarios%2520critical%2520for%2520effective%2520supervised%2520learning.%2520We%2520performed%250Acomprehensive%2520validations%2520using%2520a%2520wide%2520range%2520of%2520open%2520datasets%2520from%2520various%2520BCI%250Aapplications%252C%2520employing%2520metrics%2520like%2520mean%2520squared%2520error%2520and%2520signal-to-noise%250Aratio%252C%2520as%2520well%2520as%2520sophisticated%2520techniques%2520such%2520as%2520source%2520localization%2520and%2520EEG%250Acomponent%2520classification.%2520Our%2520evaluations%2520confirm%2520that%2520ART%2520surpasses%2520other%250Adeep-learning-based%2520artifact%2520removal%2520methods%252C%2520setting%2520a%2520new%2520benchmark%2520in%2520EEG%250Asignal%2520processing.%2520This%2520advancement%2520not%2520only%2520boosts%2520the%2520accuracy%2520and%250Areliability%2520of%2520artifact%2520removal%2520but%2520also%2520promises%2520to%2520catalyze%2520further%250Ainnovations%2520in%2520the%2520field%252C%2520facilitating%2520the%2520study%2520of%2520brain%2520dynamics%2520in%250Anaturalistic%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07326v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ART%3A%20Artifact%20Removal%20Transformer%20for%20Reconstructing%20Noise-Free%0A%20%20Multichannel%20Electroencephalographic%20Signals&entry.906535625=Chun-Hsiang%20Chuang%20and%20Kong-Yi%20Chang%20and%20Chih-Sheng%20Huang%20and%20Anne-Mei%20Bessas&entry.1292438233=%20%20Artifact%20removal%20in%20electroencephalography%20%28EEG%29%20is%20a%20longstanding%20challenge%0Athat%20significantly%20impacts%20neuroscientific%20analysis%20and%20brain-computer%0Ainterface%20%28BCI%29%20performance.%20Tackling%20this%20problem%20demands%20advanced%20algorithms%2C%0Aextensive%20noisy-clean%20training%20data%2C%20and%20thorough%20evaluation%20strategies.%20This%0Astudy%20presents%20the%20Artifact%20Removal%20Transformer%20%28ART%29%2C%20an%20innovative%20EEG%0Adenoising%20model%20employing%20transformer%20architecture%20to%20adeptly%20capture%20the%0Atransient%20millisecond-scale%20dynamics%20characteristic%20of%20EEG%20signals.%20Our%0Aapproach%20offers%20a%20holistic%2C%20end-to-end%20denoising%20solution%20for%20diverse%20artifact%0Atypes%20in%20multichannel%20EEG%20data.%20We%20enhanced%20the%20generation%20of%20noisy-clean%20EEG%0Adata%20pairs%20using%20an%20independent%20component%20analysis%2C%20thus%20fortifying%20the%0Atraining%20scenarios%20critical%20for%20effective%20supervised%20learning.%20We%20performed%0Acomprehensive%20validations%20using%20a%20wide%20range%20of%20open%20datasets%20from%20various%20BCI%0Aapplications%2C%20employing%20metrics%20like%20mean%20squared%20error%20and%20signal-to-noise%0Aratio%2C%20as%20well%20as%20sophisticated%20techniques%20such%20as%20source%20localization%20and%20EEG%0Acomponent%20classification.%20Our%20evaluations%20confirm%20that%20ART%20surpasses%20other%0Adeep-learning-based%20artifact%20removal%20methods%2C%20setting%20a%20new%20benchmark%20in%20EEG%0Asignal%20processing.%20This%20advancement%20not%20only%20boosts%20the%20accuracy%20and%0Areliability%20of%20artifact%20removal%20but%20also%20promises%20to%20catalyze%20further%0Ainnovations%20in%20the%20field%2C%20facilitating%20the%20study%20of%20brain%20dynamics%20in%0Anaturalistic%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07326v1&entry.124074799=Read"},
{"title": "SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research\n  Repositories", "author": "Ben Bogin and Kejuan Yang and Shashank Gupta and Kyle Richardson and Erin Bransom and Peter Clark and Ashish Sabharwal and Tushar Khot", "abstract": "  Given that Large Language Models (LLMs) have made significant progress in\nwriting code, can they now be used to autonomously reproduce results from\nresearch repositories? Such a capability would be a boon to the research\ncommunity, helping researchers validate, understand, and extend prior work. To\nadvance towards this goal, we introduce SUPER, the first benchmark designed to\nevaluate the capability of LLMs in setting up and executing tasks from research\nrepositories. SUPERaims to capture the realistic challenges faced by\nresearchers working with Machine Learning (ML) and Natural Language Processing\n(NLP) research repositories. Our benchmark comprises three distinct problem\nsets: 45 end-to-end problems with annotated expert solutions, 152 sub problems\nderived from the expert set that focus on specific challenges (e.g.,\nconfiguring a trainer), and 602 automatically generated problems for\nlarger-scale development. We introduce various evaluation measures to assess\nboth task success and progress, utilizing gold solutions when available or\napproximations otherwise. We show that state-of-the-art approaches struggle to\nsolve these problems with the best model (GPT-4o) solving only 16.3% of the\nend-to-end set, and 46.1% of the scenarios. This illustrates the challenge of\nthis task, and suggests that SUPER can serve as a valuable resource for the\ncommunity to make and measure progress.\n", "link": "http://arxiv.org/abs/2409.07440v1", "date": "2024-09-11", "relevancy": 2.0155, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5046}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5046}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SUPER%3A%20Evaluating%20Agents%20on%20Setting%20Up%20and%20Executing%20Tasks%20from%20Research%0A%20%20Repositories&body=Title%3A%20SUPER%3A%20Evaluating%20Agents%20on%20Setting%20Up%20and%20Executing%20Tasks%20from%20Research%0A%20%20Repositories%0AAuthor%3A%20Ben%20Bogin%20and%20Kejuan%20Yang%20and%20Shashank%20Gupta%20and%20Kyle%20Richardson%20and%20Erin%20Bransom%20and%20Peter%20Clark%20and%20Ashish%20Sabharwal%20and%20Tushar%20Khot%0AAbstract%3A%20%20%20Given%20that%20Large%20Language%20Models%20%28LLMs%29%20have%20made%20significant%20progress%20in%0Awriting%20code%2C%20can%20they%20now%20be%20used%20to%20autonomously%20reproduce%20results%20from%0Aresearch%20repositories%3F%20Such%20a%20capability%20would%20be%20a%20boon%20to%20the%20research%0Acommunity%2C%20helping%20researchers%20validate%2C%20understand%2C%20and%20extend%20prior%20work.%20To%0Aadvance%20towards%20this%20goal%2C%20we%20introduce%20SUPER%2C%20the%20first%20benchmark%20designed%20to%0Aevaluate%20the%20capability%20of%20LLMs%20in%20setting%20up%20and%20executing%20tasks%20from%20research%0Arepositories.%20SUPERaims%20to%20capture%20the%20realistic%20challenges%20faced%20by%0Aresearchers%20working%20with%20Machine%20Learning%20%28ML%29%20and%20Natural%20Language%20Processing%0A%28NLP%29%20research%20repositories.%20Our%20benchmark%20comprises%20three%20distinct%20problem%0Asets%3A%2045%20end-to-end%20problems%20with%20annotated%20expert%20solutions%2C%20152%20sub%20problems%0Aderived%20from%20the%20expert%20set%20that%20focus%20on%20specific%20challenges%20%28e.g.%2C%0Aconfiguring%20a%20trainer%29%2C%20and%20602%20automatically%20generated%20problems%20for%0Alarger-scale%20development.%20We%20introduce%20various%20evaluation%20measures%20to%20assess%0Aboth%20task%20success%20and%20progress%2C%20utilizing%20gold%20solutions%20when%20available%20or%0Aapproximations%20otherwise.%20We%20show%20that%20state-of-the-art%20approaches%20struggle%20to%0Asolve%20these%20problems%20with%20the%20best%20model%20%28GPT-4o%29%20solving%20only%2016.3%25%20of%20the%0Aend-to-end%20set%2C%20and%2046.1%25%20of%20the%20scenarios.%20This%20illustrates%20the%20challenge%20of%0Athis%20task%2C%20and%20suggests%20that%20SUPER%20can%20serve%20as%20a%20valuable%20resource%20for%20the%0Acommunity%20to%20make%20and%20measure%20progress.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07440v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSUPER%253A%2520Evaluating%2520Agents%2520on%2520Setting%2520Up%2520and%2520Executing%2520Tasks%2520from%2520Research%250A%2520%2520Repositories%26entry.906535625%3DBen%2520Bogin%2520and%2520Kejuan%2520Yang%2520and%2520Shashank%2520Gupta%2520and%2520Kyle%2520Richardson%2520and%2520Erin%2520Bransom%2520and%2520Peter%2520Clark%2520and%2520Ashish%2520Sabharwal%2520and%2520Tushar%2520Khot%26entry.1292438233%3D%2520%2520Given%2520that%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520made%2520significant%2520progress%2520in%250Awriting%2520code%252C%2520can%2520they%2520now%2520be%2520used%2520to%2520autonomously%2520reproduce%2520results%2520from%250Aresearch%2520repositories%253F%2520Such%2520a%2520capability%2520would%2520be%2520a%2520boon%2520to%2520the%2520research%250Acommunity%252C%2520helping%2520researchers%2520validate%252C%2520understand%252C%2520and%2520extend%2520prior%2520work.%2520To%250Aadvance%2520towards%2520this%2520goal%252C%2520we%2520introduce%2520SUPER%252C%2520the%2520first%2520benchmark%2520designed%2520to%250Aevaluate%2520the%2520capability%2520of%2520LLMs%2520in%2520setting%2520up%2520and%2520executing%2520tasks%2520from%2520research%250Arepositories.%2520SUPERaims%2520to%2520capture%2520the%2520realistic%2520challenges%2520faced%2520by%250Aresearchers%2520working%2520with%2520Machine%2520Learning%2520%2528ML%2529%2520and%2520Natural%2520Language%2520Processing%250A%2528NLP%2529%2520research%2520repositories.%2520Our%2520benchmark%2520comprises%2520three%2520distinct%2520problem%250Asets%253A%252045%2520end-to-end%2520problems%2520with%2520annotated%2520expert%2520solutions%252C%2520152%2520sub%2520problems%250Aderived%2520from%2520the%2520expert%2520set%2520that%2520focus%2520on%2520specific%2520challenges%2520%2528e.g.%252C%250Aconfiguring%2520a%2520trainer%2529%252C%2520and%2520602%2520automatically%2520generated%2520problems%2520for%250Alarger-scale%2520development.%2520We%2520introduce%2520various%2520evaluation%2520measures%2520to%2520assess%250Aboth%2520task%2520success%2520and%2520progress%252C%2520utilizing%2520gold%2520solutions%2520when%2520available%2520or%250Aapproximations%2520otherwise.%2520We%2520show%2520that%2520state-of-the-art%2520approaches%2520struggle%2520to%250Asolve%2520these%2520problems%2520with%2520the%2520best%2520model%2520%2528GPT-4o%2529%2520solving%2520only%252016.3%2525%2520of%2520the%250Aend-to-end%2520set%252C%2520and%252046.1%2525%2520of%2520the%2520scenarios.%2520This%2520illustrates%2520the%2520challenge%2520of%250Athis%2520task%252C%2520and%2520suggests%2520that%2520SUPER%2520can%2520serve%2520as%2520a%2520valuable%2520resource%2520for%2520the%250Acommunity%2520to%2520make%2520and%2520measure%2520progress.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07440v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SUPER%3A%20Evaluating%20Agents%20on%20Setting%20Up%20and%20Executing%20Tasks%20from%20Research%0A%20%20Repositories&entry.906535625=Ben%20Bogin%20and%20Kejuan%20Yang%20and%20Shashank%20Gupta%20and%20Kyle%20Richardson%20and%20Erin%20Bransom%20and%20Peter%20Clark%20and%20Ashish%20Sabharwal%20and%20Tushar%20Khot&entry.1292438233=%20%20Given%20that%20Large%20Language%20Models%20%28LLMs%29%20have%20made%20significant%20progress%20in%0Awriting%20code%2C%20can%20they%20now%20be%20used%20to%20autonomously%20reproduce%20results%20from%0Aresearch%20repositories%3F%20Such%20a%20capability%20would%20be%20a%20boon%20to%20the%20research%0Acommunity%2C%20helping%20researchers%20validate%2C%20understand%2C%20and%20extend%20prior%20work.%20To%0Aadvance%20towards%20this%20goal%2C%20we%20introduce%20SUPER%2C%20the%20first%20benchmark%20designed%20to%0Aevaluate%20the%20capability%20of%20LLMs%20in%20setting%20up%20and%20executing%20tasks%20from%20research%0Arepositories.%20SUPERaims%20to%20capture%20the%20realistic%20challenges%20faced%20by%0Aresearchers%20working%20with%20Machine%20Learning%20%28ML%29%20and%20Natural%20Language%20Processing%0A%28NLP%29%20research%20repositories.%20Our%20benchmark%20comprises%20three%20distinct%20problem%0Asets%3A%2045%20end-to-end%20problems%20with%20annotated%20expert%20solutions%2C%20152%20sub%20problems%0Aderived%20from%20the%20expert%20set%20that%20focus%20on%20specific%20challenges%20%28e.g.%2C%0Aconfiguring%20a%20trainer%29%2C%20and%20602%20automatically%20generated%20problems%20for%0Alarger-scale%20development.%20We%20introduce%20various%20evaluation%20measures%20to%20assess%0Aboth%20task%20success%20and%20progress%2C%20utilizing%20gold%20solutions%20when%20available%20or%0Aapproximations%20otherwise.%20We%20show%20that%20state-of-the-art%20approaches%20struggle%20to%0Asolve%20these%20problems%20with%20the%20best%20model%20%28GPT-4o%29%20solving%20only%2016.3%25%20of%20the%0Aend-to-end%20set%2C%20and%2046.1%25%20of%20the%20scenarios.%20This%20illustrates%20the%20challenge%20of%0Athis%20task%2C%20and%20suggests%20that%20SUPER%20can%20serve%20as%20a%20valuable%20resource%20for%20the%0Acommunity%20to%20make%20and%20measure%20progress.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07440v1&entry.124074799=Read"},
{"title": "Awaking the Slides: A Tuning-free and Knowledge-regulated AI Tutoring\n  System via Language Model Coordination", "author": "Daniel Zhang-Li and Zheyuan Zhang and Jifan Yu and Joy Lim Jia Yin and Shangqing Tu and Linlu Gong and Haohua Wang and Zhiyuan Liu and Huiqin Liu and Lei Hou and Juanzi Li", "abstract": "  The vast pre-existing slides serve as rich and important materials to carry\nlecture knowledge. However, effectively leveraging lecture slides to serve\nstudents is difficult due to the multi-modal nature of slide content and the\nheterogeneous teaching actions. We study the problem of discovering effective\ndesigns that convert a slide into an interactive lecture. We develop\nSlide2Lecture, a tuning-free and knowledge-regulated intelligent tutoring\nsystem that can (1) effectively convert an input lecture slide into a\nstructured teaching agenda consisting of a set of heterogeneous teaching\nactions; (2) create and manage an interactive lecture that generates responsive\ninteractions catering to student learning demands while regulating the\ninteractions to follow teaching actions. Slide2Lecture contains a complete\npipeline for learners to obtain an interactive classroom experience to learn\nthe slide. For teachers and developers, Slide2Lecture enables customization to\ncater to personalized demands. The evaluation rated by annotators and students\nshows that Slide2Lecture is effective in outperforming the remaining\nimplementation. Slide2Lecture's online deployment has made more than 200K\ninteraction with students in the 3K lecture sessions. We open source\nSlide2Lecture's implementation in\nhttps://anonymous.4open.science/r/slide2lecture-4210/.\n", "link": "http://arxiv.org/abs/2409.07372v1", "date": "2024-09-11", "relevancy": 2.0088, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5235}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4879}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Awaking%20the%20Slides%3A%20A%20Tuning-free%20and%20Knowledge-regulated%20AI%20Tutoring%0A%20%20System%20via%20Language%20Model%20Coordination&body=Title%3A%20Awaking%20the%20Slides%3A%20A%20Tuning-free%20and%20Knowledge-regulated%20AI%20Tutoring%0A%20%20System%20via%20Language%20Model%20Coordination%0AAuthor%3A%20Daniel%20Zhang-Li%20and%20Zheyuan%20Zhang%20and%20Jifan%20Yu%20and%20Joy%20Lim%20Jia%20Yin%20and%20Shangqing%20Tu%20and%20Linlu%20Gong%20and%20Haohua%20Wang%20and%20Zhiyuan%20Liu%20and%20Huiqin%20Liu%20and%20Lei%20Hou%20and%20Juanzi%20Li%0AAbstract%3A%20%20%20The%20vast%20pre-existing%20slides%20serve%20as%20rich%20and%20important%20materials%20to%20carry%0Alecture%20knowledge.%20However%2C%20effectively%20leveraging%20lecture%20slides%20to%20serve%0Astudents%20is%20difficult%20due%20to%20the%20multi-modal%20nature%20of%20slide%20content%20and%20the%0Aheterogeneous%20teaching%20actions.%20We%20study%20the%20problem%20of%20discovering%20effective%0Adesigns%20that%20convert%20a%20slide%20into%20an%20interactive%20lecture.%20We%20develop%0ASlide2Lecture%2C%20a%20tuning-free%20and%20knowledge-regulated%20intelligent%20tutoring%0Asystem%20that%20can%20%281%29%20effectively%20convert%20an%20input%20lecture%20slide%20into%20a%0Astructured%20teaching%20agenda%20consisting%20of%20a%20set%20of%20heterogeneous%20teaching%0Aactions%3B%20%282%29%20create%20and%20manage%20an%20interactive%20lecture%20that%20generates%20responsive%0Ainteractions%20catering%20to%20student%20learning%20demands%20while%20regulating%20the%0Ainteractions%20to%20follow%20teaching%20actions.%20Slide2Lecture%20contains%20a%20complete%0Apipeline%20for%20learners%20to%20obtain%20an%20interactive%20classroom%20experience%20to%20learn%0Athe%20slide.%20For%20teachers%20and%20developers%2C%20Slide2Lecture%20enables%20customization%20to%0Acater%20to%20personalized%20demands.%20The%20evaluation%20rated%20by%20annotators%20and%20students%0Ashows%20that%20Slide2Lecture%20is%20effective%20in%20outperforming%20the%20remaining%0Aimplementation.%20Slide2Lecture%27s%20online%20deployment%20has%20made%20more%20than%20200K%0Ainteraction%20with%20students%20in%20the%203K%20lecture%20sessions.%20We%20open%20source%0ASlide2Lecture%27s%20implementation%20in%0Ahttps%3A//anonymous.4open.science/r/slide2lecture-4210/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07372v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAwaking%2520the%2520Slides%253A%2520A%2520Tuning-free%2520and%2520Knowledge-regulated%2520AI%2520Tutoring%250A%2520%2520System%2520via%2520Language%2520Model%2520Coordination%26entry.906535625%3DDaniel%2520Zhang-Li%2520and%2520Zheyuan%2520Zhang%2520and%2520Jifan%2520Yu%2520and%2520Joy%2520Lim%2520Jia%2520Yin%2520and%2520Shangqing%2520Tu%2520and%2520Linlu%2520Gong%2520and%2520Haohua%2520Wang%2520and%2520Zhiyuan%2520Liu%2520and%2520Huiqin%2520Liu%2520and%2520Lei%2520Hou%2520and%2520Juanzi%2520Li%26entry.1292438233%3D%2520%2520The%2520vast%2520pre-existing%2520slides%2520serve%2520as%2520rich%2520and%2520important%2520materials%2520to%2520carry%250Alecture%2520knowledge.%2520However%252C%2520effectively%2520leveraging%2520lecture%2520slides%2520to%2520serve%250Astudents%2520is%2520difficult%2520due%2520to%2520the%2520multi-modal%2520nature%2520of%2520slide%2520content%2520and%2520the%250Aheterogeneous%2520teaching%2520actions.%2520We%2520study%2520the%2520problem%2520of%2520discovering%2520effective%250Adesigns%2520that%2520convert%2520a%2520slide%2520into%2520an%2520interactive%2520lecture.%2520We%2520develop%250ASlide2Lecture%252C%2520a%2520tuning-free%2520and%2520knowledge-regulated%2520intelligent%2520tutoring%250Asystem%2520that%2520can%2520%25281%2529%2520effectively%2520convert%2520an%2520input%2520lecture%2520slide%2520into%2520a%250Astructured%2520teaching%2520agenda%2520consisting%2520of%2520a%2520set%2520of%2520heterogeneous%2520teaching%250Aactions%253B%2520%25282%2529%2520create%2520and%2520manage%2520an%2520interactive%2520lecture%2520that%2520generates%2520responsive%250Ainteractions%2520catering%2520to%2520student%2520learning%2520demands%2520while%2520regulating%2520the%250Ainteractions%2520to%2520follow%2520teaching%2520actions.%2520Slide2Lecture%2520contains%2520a%2520complete%250Apipeline%2520for%2520learners%2520to%2520obtain%2520an%2520interactive%2520classroom%2520experience%2520to%2520learn%250Athe%2520slide.%2520For%2520teachers%2520and%2520developers%252C%2520Slide2Lecture%2520enables%2520customization%2520to%250Acater%2520to%2520personalized%2520demands.%2520The%2520evaluation%2520rated%2520by%2520annotators%2520and%2520students%250Ashows%2520that%2520Slide2Lecture%2520is%2520effective%2520in%2520outperforming%2520the%2520remaining%250Aimplementation.%2520Slide2Lecture%2527s%2520online%2520deployment%2520has%2520made%2520more%2520than%2520200K%250Ainteraction%2520with%2520students%2520in%2520the%25203K%2520lecture%2520sessions.%2520We%2520open%2520source%250ASlide2Lecture%2527s%2520implementation%2520in%250Ahttps%253A//anonymous.4open.science/r/slide2lecture-4210/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07372v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Awaking%20the%20Slides%3A%20A%20Tuning-free%20and%20Knowledge-regulated%20AI%20Tutoring%0A%20%20System%20via%20Language%20Model%20Coordination&entry.906535625=Daniel%20Zhang-Li%20and%20Zheyuan%20Zhang%20and%20Jifan%20Yu%20and%20Joy%20Lim%20Jia%20Yin%20and%20Shangqing%20Tu%20and%20Linlu%20Gong%20and%20Haohua%20Wang%20and%20Zhiyuan%20Liu%20and%20Huiqin%20Liu%20and%20Lei%20Hou%20and%20Juanzi%20Li&entry.1292438233=%20%20The%20vast%20pre-existing%20slides%20serve%20as%20rich%20and%20important%20materials%20to%20carry%0Alecture%20knowledge.%20However%2C%20effectively%20leveraging%20lecture%20slides%20to%20serve%0Astudents%20is%20difficult%20due%20to%20the%20multi-modal%20nature%20of%20slide%20content%20and%20the%0Aheterogeneous%20teaching%20actions.%20We%20study%20the%20problem%20of%20discovering%20effective%0Adesigns%20that%20convert%20a%20slide%20into%20an%20interactive%20lecture.%20We%20develop%0ASlide2Lecture%2C%20a%20tuning-free%20and%20knowledge-regulated%20intelligent%20tutoring%0Asystem%20that%20can%20%281%29%20effectively%20convert%20an%20input%20lecture%20slide%20into%20a%0Astructured%20teaching%20agenda%20consisting%20of%20a%20set%20of%20heterogeneous%20teaching%0Aactions%3B%20%282%29%20create%20and%20manage%20an%20interactive%20lecture%20that%20generates%20responsive%0Ainteractions%20catering%20to%20student%20learning%20demands%20while%20regulating%20the%0Ainteractions%20to%20follow%20teaching%20actions.%20Slide2Lecture%20contains%20a%20complete%0Apipeline%20for%20learners%20to%20obtain%20an%20interactive%20classroom%20experience%20to%20learn%0Athe%20slide.%20For%20teachers%20and%20developers%2C%20Slide2Lecture%20enables%20customization%20to%0Acater%20to%20personalized%20demands.%20The%20evaluation%20rated%20by%20annotators%20and%20students%0Ashows%20that%20Slide2Lecture%20is%20effective%20in%20outperforming%20the%20remaining%0Aimplementation.%20Slide2Lecture%27s%20online%20deployment%20has%20made%20more%20than%20200K%0Ainteraction%20with%20students%20in%20the%203K%20lecture%20sessions.%20We%20open%20source%0ASlide2Lecture%27s%20implementation%20in%0Ahttps%3A//anonymous.4open.science/r/slide2lecture-4210/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07372v1&entry.124074799=Read"},
{"title": "FreeEnhance: Tuning-Free Image Enhancement via Content-Consistent\n  Noising-and-Denoising Process", "author": "Yang Luo and Yiheng Zhang and Zhaofan Qiu and Ting Yao and Zhineng Chen and Yu-Gang Jiang and Tao Mei", "abstract": "  The emergence of text-to-image generation models has led to the recognition\nthat image enhancement, performed as post-processing, would significantly\nimprove the visual quality of the generated images. Exploring diffusion models\nto enhance the generated images nevertheless is not trivial and necessitates to\ndelicately enrich plentiful details while preserving the visual appearance of\nkey content in the original image. In this paper, we propose a novel framework,\nnamely FreeEnhance, for content-consistent image enhancement using the\noff-the-shelf image diffusion models. Technically, FreeEnhance is a two-stage\nprocess that firstly adds random noise to the input image and then capitalizes\non a pre-trained image diffusion model (i.e., Latent Diffusion Models) to\ndenoise and enhance the image details. In the noising stage, FreeEnhance is\ndevised to add lighter noise to the region with higher frequency to preserve\nthe high-frequent patterns (e.g., edge, corner) in the original image. In the\ndenoising stage, we present three target properties as constraints to\nregularize the predicted noise, enhancing images with high acutance and high\nvisual quality. Extensive experiments conducted on the HPDv2 dataset\ndemonstrate that our FreeEnhance outperforms the state-of-the-art image\nenhancement models in terms of quantitative metrics and human preference. More\nremarkably, FreeEnhance also shows higher human preference compared to the\ncommercial image enhancement solution of Magnific AI.\n", "link": "http://arxiv.org/abs/2409.07451v1", "date": "2024-09-11", "relevancy": 1.9994, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.7122}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6116}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreeEnhance%3A%20Tuning-Free%20Image%20Enhancement%20via%20Content-Consistent%0A%20%20Noising-and-Denoising%20Process&body=Title%3A%20FreeEnhance%3A%20Tuning-Free%20Image%20Enhancement%20via%20Content-Consistent%0A%20%20Noising-and-Denoising%20Process%0AAuthor%3A%20Yang%20Luo%20and%20Yiheng%20Zhang%20and%20Zhaofan%20Qiu%20and%20Ting%20Yao%20and%20Zhineng%20Chen%20and%20Yu-Gang%20Jiang%20and%20Tao%20Mei%0AAbstract%3A%20%20%20The%20emergence%20of%20text-to-image%20generation%20models%20has%20led%20to%20the%20recognition%0Athat%20image%20enhancement%2C%20performed%20as%20post-processing%2C%20would%20significantly%0Aimprove%20the%20visual%20quality%20of%20the%20generated%20images.%20Exploring%20diffusion%20models%0Ato%20enhance%20the%20generated%20images%20nevertheless%20is%20not%20trivial%20and%20necessitates%20to%0Adelicately%20enrich%20plentiful%20details%20while%20preserving%20the%20visual%20appearance%20of%0Akey%20content%20in%20the%20original%20image.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20framework%2C%0Anamely%20FreeEnhance%2C%20for%20content-consistent%20image%20enhancement%20using%20the%0Aoff-the-shelf%20image%20diffusion%20models.%20Technically%2C%20FreeEnhance%20is%20a%20two-stage%0Aprocess%20that%20firstly%20adds%20random%20noise%20to%20the%20input%20image%20and%20then%20capitalizes%0Aon%20a%20pre-trained%20image%20diffusion%20model%20%28i.e.%2C%20Latent%20Diffusion%20Models%29%20to%0Adenoise%20and%20enhance%20the%20image%20details.%20In%20the%20noising%20stage%2C%20FreeEnhance%20is%0Adevised%20to%20add%20lighter%20noise%20to%20the%20region%20with%20higher%20frequency%20to%20preserve%0Athe%20high-frequent%20patterns%20%28e.g.%2C%20edge%2C%20corner%29%20in%20the%20original%20image.%20In%20the%0Adenoising%20stage%2C%20we%20present%20three%20target%20properties%20as%20constraints%20to%0Aregularize%20the%20predicted%20noise%2C%20enhancing%20images%20with%20high%20acutance%20and%20high%0Avisual%20quality.%20Extensive%20experiments%20conducted%20on%20the%20HPDv2%20dataset%0Ademonstrate%20that%20our%20FreeEnhance%20outperforms%20the%20state-of-the-art%20image%0Aenhancement%20models%20in%20terms%20of%20quantitative%20metrics%20and%20human%20preference.%20More%0Aremarkably%2C%20FreeEnhance%20also%20shows%20higher%20human%20preference%20compared%20to%20the%0Acommercial%20image%20enhancement%20solution%20of%20Magnific%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07451v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreeEnhance%253A%2520Tuning-Free%2520Image%2520Enhancement%2520via%2520Content-Consistent%250A%2520%2520Noising-and-Denoising%2520Process%26entry.906535625%3DYang%2520Luo%2520and%2520Yiheng%2520Zhang%2520and%2520Zhaofan%2520Qiu%2520and%2520Ting%2520Yao%2520and%2520Zhineng%2520Chen%2520and%2520Yu-Gang%2520Jiang%2520and%2520Tao%2520Mei%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520text-to-image%2520generation%2520models%2520has%2520led%2520to%2520the%2520recognition%250Athat%2520image%2520enhancement%252C%2520performed%2520as%2520post-processing%252C%2520would%2520significantly%250Aimprove%2520the%2520visual%2520quality%2520of%2520the%2520generated%2520images.%2520Exploring%2520diffusion%2520models%250Ato%2520enhance%2520the%2520generated%2520images%2520nevertheless%2520is%2520not%2520trivial%2520and%2520necessitates%2520to%250Adelicately%2520enrich%2520plentiful%2520details%2520while%2520preserving%2520the%2520visual%2520appearance%2520of%250Akey%2520content%2520in%2520the%2520original%2520image.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520framework%252C%250Anamely%2520FreeEnhance%252C%2520for%2520content-consistent%2520image%2520enhancement%2520using%2520the%250Aoff-the-shelf%2520image%2520diffusion%2520models.%2520Technically%252C%2520FreeEnhance%2520is%2520a%2520two-stage%250Aprocess%2520that%2520firstly%2520adds%2520random%2520noise%2520to%2520the%2520input%2520image%2520and%2520then%2520capitalizes%250Aon%2520a%2520pre-trained%2520image%2520diffusion%2520model%2520%2528i.e.%252C%2520Latent%2520Diffusion%2520Models%2529%2520to%250Adenoise%2520and%2520enhance%2520the%2520image%2520details.%2520In%2520the%2520noising%2520stage%252C%2520FreeEnhance%2520is%250Adevised%2520to%2520add%2520lighter%2520noise%2520to%2520the%2520region%2520with%2520higher%2520frequency%2520to%2520preserve%250Athe%2520high-frequent%2520patterns%2520%2528e.g.%252C%2520edge%252C%2520corner%2529%2520in%2520the%2520original%2520image.%2520In%2520the%250Adenoising%2520stage%252C%2520we%2520present%2520three%2520target%2520properties%2520as%2520constraints%2520to%250Aregularize%2520the%2520predicted%2520noise%252C%2520enhancing%2520images%2520with%2520high%2520acutance%2520and%2520high%250Avisual%2520quality.%2520Extensive%2520experiments%2520conducted%2520on%2520the%2520HPDv2%2520dataset%250Ademonstrate%2520that%2520our%2520FreeEnhance%2520outperforms%2520the%2520state-of-the-art%2520image%250Aenhancement%2520models%2520in%2520terms%2520of%2520quantitative%2520metrics%2520and%2520human%2520preference.%2520More%250Aremarkably%252C%2520FreeEnhance%2520also%2520shows%2520higher%2520human%2520preference%2520compared%2520to%2520the%250Acommercial%2520image%2520enhancement%2520solution%2520of%2520Magnific%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07451v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeEnhance%3A%20Tuning-Free%20Image%20Enhancement%20via%20Content-Consistent%0A%20%20Noising-and-Denoising%20Process&entry.906535625=Yang%20Luo%20and%20Yiheng%20Zhang%20and%20Zhaofan%20Qiu%20and%20Ting%20Yao%20and%20Zhineng%20Chen%20and%20Yu-Gang%20Jiang%20and%20Tao%20Mei&entry.1292438233=%20%20The%20emergence%20of%20text-to-image%20generation%20models%20has%20led%20to%20the%20recognition%0Athat%20image%20enhancement%2C%20performed%20as%20post-processing%2C%20would%20significantly%0Aimprove%20the%20visual%20quality%20of%20the%20generated%20images.%20Exploring%20diffusion%20models%0Ato%20enhance%20the%20generated%20images%20nevertheless%20is%20not%20trivial%20and%20necessitates%20to%0Adelicately%20enrich%20plentiful%20details%20while%20preserving%20the%20visual%20appearance%20of%0Akey%20content%20in%20the%20original%20image.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20framework%2C%0Anamely%20FreeEnhance%2C%20for%20content-consistent%20image%20enhancement%20using%20the%0Aoff-the-shelf%20image%20diffusion%20models.%20Technically%2C%20FreeEnhance%20is%20a%20two-stage%0Aprocess%20that%20firstly%20adds%20random%20noise%20to%20the%20input%20image%20and%20then%20capitalizes%0Aon%20a%20pre-trained%20image%20diffusion%20model%20%28i.e.%2C%20Latent%20Diffusion%20Models%29%20to%0Adenoise%20and%20enhance%20the%20image%20details.%20In%20the%20noising%20stage%2C%20FreeEnhance%20is%0Adevised%20to%20add%20lighter%20noise%20to%20the%20region%20with%20higher%20frequency%20to%20preserve%0Athe%20high-frequent%20patterns%20%28e.g.%2C%20edge%2C%20corner%29%20in%20the%20original%20image.%20In%20the%0Adenoising%20stage%2C%20we%20present%20three%20target%20properties%20as%20constraints%20to%0Aregularize%20the%20predicted%20noise%2C%20enhancing%20images%20with%20high%20acutance%20and%20high%0Avisual%20quality.%20Extensive%20experiments%20conducted%20on%20the%20HPDv2%20dataset%0Ademonstrate%20that%20our%20FreeEnhance%20outperforms%20the%20state-of-the-art%20image%0Aenhancement%20models%20in%20terms%20of%20quantitative%20metrics%20and%20human%20preference.%20More%0Aremarkably%2C%20FreeEnhance%20also%20shows%20higher%20human%20preference%20compared%20to%20the%0Acommercial%20image%20enhancement%20solution%20of%20Magnific%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07451v1&entry.124074799=Read"},
{"title": "Gesture-Controlled Aerial Robot Formation for Human-Swarm Interaction in\n  Safety Monitoring Applications", "author": "V\u00edt Kr\u00e1tk\u00fd and Giuseppe Silano and Matou\u0161 Vrba and Christos Papaioannidis and Ioannis Mademlis and Robert P\u011bni\u010dka and Ioannis Pitas and Martin Saska", "abstract": "  This paper presents a formation control approach for contactless\ngesture-based Human-Swarm Interaction (HSI) between a team of multi-rotor\nUnmanned Aerial Vehicles (UAVs) and a human worker. The approach is designed to\nmonitor the safety of human workers, particularly those operating at heights.\nIn the proposed dynamic formation scheme, one UAV acts as the formation leader,\nequipped with sensors for detecting human workers and recognizing gestures. The\nfollower UAVs maintain a predetermined formation relative to the worker's\nposition, providing additional perspectives of the monitored scene. Hand\ngestures enable the human worker to specify movement and action commands for\nthe UAV team and to initiate other mission-related tasks without requiring\nadditional communication channels or specific markers. Combined with a novel\nunified human detection and tracking algorithm, a human position estimation\nmethod, and a gesture detection pipeline, the proposed approach represents the\nfirst instance of an HSI system incorporating all these modules onboard\nreal-world UAVs. Simulations and field experiments involving three UAVs and a\nhuman worker in a mock-up scenario demonstrate the effectiveness and\nresponsiveness of the proposed approach.\n", "link": "http://arxiv.org/abs/2403.15333v2", "date": "2024-09-11", "relevancy": 1.9973, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5354}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4803}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gesture-Controlled%20Aerial%20Robot%20Formation%20for%20Human-Swarm%20Interaction%20in%0A%20%20Safety%20Monitoring%20Applications&body=Title%3A%20Gesture-Controlled%20Aerial%20Robot%20Formation%20for%20Human-Swarm%20Interaction%20in%0A%20%20Safety%20Monitoring%20Applications%0AAuthor%3A%20V%C3%ADt%20Kr%C3%A1tk%C3%BD%20and%20Giuseppe%20Silano%20and%20Matou%C5%A1%20Vrba%20and%20Christos%20Papaioannidis%20and%20Ioannis%20Mademlis%20and%20Robert%20P%C4%9Bni%C4%8Dka%20and%20Ioannis%20Pitas%20and%20Martin%20Saska%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20formation%20control%20approach%20for%20contactless%0Agesture-based%20Human-Swarm%20Interaction%20%28HSI%29%20between%20a%20team%20of%20multi-rotor%0AUnmanned%20Aerial%20Vehicles%20%28UAVs%29%20and%20a%20human%20worker.%20The%20approach%20is%20designed%20to%0Amonitor%20the%20safety%20of%20human%20workers%2C%20particularly%20those%20operating%20at%20heights.%0AIn%20the%20proposed%20dynamic%20formation%20scheme%2C%20one%20UAV%20acts%20as%20the%20formation%20leader%2C%0Aequipped%20with%20sensors%20for%20detecting%20human%20workers%20and%20recognizing%20gestures.%20The%0Afollower%20UAVs%20maintain%20a%20predetermined%20formation%20relative%20to%20the%20worker%27s%0Aposition%2C%20providing%20additional%20perspectives%20of%20the%20monitored%20scene.%20Hand%0Agestures%20enable%20the%20human%20worker%20to%20specify%20movement%20and%20action%20commands%20for%0Athe%20UAV%20team%20and%20to%20initiate%20other%20mission-related%20tasks%20without%20requiring%0Aadditional%20communication%20channels%20or%20specific%20markers.%20Combined%20with%20a%20novel%0Aunified%20human%20detection%20and%20tracking%20algorithm%2C%20a%20human%20position%20estimation%0Amethod%2C%20and%20a%20gesture%20detection%20pipeline%2C%20the%20proposed%20approach%20represents%20the%0Afirst%20instance%20of%20an%20HSI%20system%20incorporating%20all%20these%20modules%20onboard%0Areal-world%20UAVs.%20Simulations%20and%20field%20experiments%20involving%20three%20UAVs%20and%20a%0Ahuman%20worker%20in%20a%20mock-up%20scenario%20demonstrate%20the%20effectiveness%20and%0Aresponsiveness%20of%20the%20proposed%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15333v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGesture-Controlled%2520Aerial%2520Robot%2520Formation%2520for%2520Human-Swarm%2520Interaction%2520in%250A%2520%2520Safety%2520Monitoring%2520Applications%26entry.906535625%3DV%25C3%25ADt%2520Kr%25C3%25A1tk%25C3%25BD%2520and%2520Giuseppe%2520Silano%2520and%2520Matou%25C5%25A1%2520Vrba%2520and%2520Christos%2520Papaioannidis%2520and%2520Ioannis%2520Mademlis%2520and%2520Robert%2520P%25C4%259Bni%25C4%258Dka%2520and%2520Ioannis%2520Pitas%2520and%2520Martin%2520Saska%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520formation%2520control%2520approach%2520for%2520contactless%250Agesture-based%2520Human-Swarm%2520Interaction%2520%2528HSI%2529%2520between%2520a%2520team%2520of%2520multi-rotor%250AUnmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520and%2520a%2520human%2520worker.%2520The%2520approach%2520is%2520designed%2520to%250Amonitor%2520the%2520safety%2520of%2520human%2520workers%252C%2520particularly%2520those%2520operating%2520at%2520heights.%250AIn%2520the%2520proposed%2520dynamic%2520formation%2520scheme%252C%2520one%2520UAV%2520acts%2520as%2520the%2520formation%2520leader%252C%250Aequipped%2520with%2520sensors%2520for%2520detecting%2520human%2520workers%2520and%2520recognizing%2520gestures.%2520The%250Afollower%2520UAVs%2520maintain%2520a%2520predetermined%2520formation%2520relative%2520to%2520the%2520worker%2527s%250Aposition%252C%2520providing%2520additional%2520perspectives%2520of%2520the%2520monitored%2520scene.%2520Hand%250Agestures%2520enable%2520the%2520human%2520worker%2520to%2520specify%2520movement%2520and%2520action%2520commands%2520for%250Athe%2520UAV%2520team%2520and%2520to%2520initiate%2520other%2520mission-related%2520tasks%2520without%2520requiring%250Aadditional%2520communication%2520channels%2520or%2520specific%2520markers.%2520Combined%2520with%2520a%2520novel%250Aunified%2520human%2520detection%2520and%2520tracking%2520algorithm%252C%2520a%2520human%2520position%2520estimation%250Amethod%252C%2520and%2520a%2520gesture%2520detection%2520pipeline%252C%2520the%2520proposed%2520approach%2520represents%2520the%250Afirst%2520instance%2520of%2520an%2520HSI%2520system%2520incorporating%2520all%2520these%2520modules%2520onboard%250Areal-world%2520UAVs.%2520Simulations%2520and%2520field%2520experiments%2520involving%2520three%2520UAVs%2520and%2520a%250Ahuman%2520worker%2520in%2520a%2520mock-up%2520scenario%2520demonstrate%2520the%2520effectiveness%2520and%250Aresponsiveness%2520of%2520the%2520proposed%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15333v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gesture-Controlled%20Aerial%20Robot%20Formation%20for%20Human-Swarm%20Interaction%20in%0A%20%20Safety%20Monitoring%20Applications&entry.906535625=V%C3%ADt%20Kr%C3%A1tk%C3%BD%20and%20Giuseppe%20Silano%20and%20Matou%C5%A1%20Vrba%20and%20Christos%20Papaioannidis%20and%20Ioannis%20Mademlis%20and%20Robert%20P%C4%9Bni%C4%8Dka%20and%20Ioannis%20Pitas%20and%20Martin%20Saska&entry.1292438233=%20%20This%20paper%20presents%20a%20formation%20control%20approach%20for%20contactless%0Agesture-based%20Human-Swarm%20Interaction%20%28HSI%29%20between%20a%20team%20of%20multi-rotor%0AUnmanned%20Aerial%20Vehicles%20%28UAVs%29%20and%20a%20human%20worker.%20The%20approach%20is%20designed%20to%0Amonitor%20the%20safety%20of%20human%20workers%2C%20particularly%20those%20operating%20at%20heights.%0AIn%20the%20proposed%20dynamic%20formation%20scheme%2C%20one%20UAV%20acts%20as%20the%20formation%20leader%2C%0Aequipped%20with%20sensors%20for%20detecting%20human%20workers%20and%20recognizing%20gestures.%20The%0Afollower%20UAVs%20maintain%20a%20predetermined%20formation%20relative%20to%20the%20worker%27s%0Aposition%2C%20providing%20additional%20perspectives%20of%20the%20monitored%20scene.%20Hand%0Agestures%20enable%20the%20human%20worker%20to%20specify%20movement%20and%20action%20commands%20for%0Athe%20UAV%20team%20and%20to%20initiate%20other%20mission-related%20tasks%20without%20requiring%0Aadditional%20communication%20channels%20or%20specific%20markers.%20Combined%20with%20a%20novel%0Aunified%20human%20detection%20and%20tracking%20algorithm%2C%20a%20human%20position%20estimation%0Amethod%2C%20and%20a%20gesture%20detection%20pipeline%2C%20the%20proposed%20approach%20represents%20the%0Afirst%20instance%20of%20an%20HSI%20system%20incorporating%20all%20these%20modules%20onboard%0Areal-world%20UAVs.%20Simulations%20and%20field%20experiments%20involving%20three%20UAVs%20and%20a%0Ahuman%20worker%20in%20a%20mock-up%20scenario%20demonstrate%20the%20effectiveness%20and%0Aresponsiveness%20of%20the%20proposed%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15333v2&entry.124074799=Read"},
{"title": "GMT: Guided Mask Transformer for Leaf Instance Segmentation", "author": "Feng Chen and Sotirios A. Tsaftaris and Mario Valerio Giuffrida", "abstract": "  Leaf instance segmentation is a challenging multi-instance segmentation task,\naiming to separate and delineate each leaf in an image of a plant. Accurate\nsegmentation of each leaf is crucial for plant-related applications such as the\nfine-grained monitoring of plant growth and crop yield estimation. This task is\nchallenging because of the high similarity (in shape and colour), great size\nvariation, and heavy occlusions among leaf instances. Furthermore, the\ntypically small size of annotated leaf datasets makes it more difficult to\nlearn the distinctive features needed for precise segmentation. We hypothesise\nthat the key to overcoming the these challenges lies in the specific spatial\npatterns of leaf distribution. In this paper, we propose the Guided Mask\nTransformer (GMT), which leverages and integrates leaf spatial distribution\npriors into a Transformer-based segmentor. These spatial priors are embedded in\na set of guide functions that map leaves at different positions into a more\nseparable embedding space. Our GMT consistently outperforms the\nstate-of-the-art on three public plant datasets.\n", "link": "http://arxiv.org/abs/2406.17109v2", "date": "2024-09-11", "relevancy": 1.988, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5122}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5041}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GMT%3A%20Guided%20Mask%20Transformer%20for%20Leaf%20Instance%20Segmentation&body=Title%3A%20GMT%3A%20Guided%20Mask%20Transformer%20for%20Leaf%20Instance%20Segmentation%0AAuthor%3A%20Feng%20Chen%20and%20Sotirios%20A.%20Tsaftaris%20and%20Mario%20Valerio%20Giuffrida%0AAbstract%3A%20%20%20Leaf%20instance%20segmentation%20is%20a%20challenging%20multi-instance%20segmentation%20task%2C%0Aaiming%20to%20separate%20and%20delineate%20each%20leaf%20in%20an%20image%20of%20a%20plant.%20Accurate%0Asegmentation%20of%20each%20leaf%20is%20crucial%20for%20plant-related%20applications%20such%20as%20the%0Afine-grained%20monitoring%20of%20plant%20growth%20and%20crop%20yield%20estimation.%20This%20task%20is%0Achallenging%20because%20of%20the%20high%20similarity%20%28in%20shape%20and%20colour%29%2C%20great%20size%0Avariation%2C%20and%20heavy%20occlusions%20among%20leaf%20instances.%20Furthermore%2C%20the%0Atypically%20small%20size%20of%20annotated%20leaf%20datasets%20makes%20it%20more%20difficult%20to%0Alearn%20the%20distinctive%20features%20needed%20for%20precise%20segmentation.%20We%20hypothesise%0Athat%20the%20key%20to%20overcoming%20the%20these%20challenges%20lies%20in%20the%20specific%20spatial%0Apatterns%20of%20leaf%20distribution.%20In%20this%20paper%2C%20we%20propose%20the%20Guided%20Mask%0ATransformer%20%28GMT%29%2C%20which%20leverages%20and%20integrates%20leaf%20spatial%20distribution%0Apriors%20into%20a%20Transformer-based%20segmentor.%20These%20spatial%20priors%20are%20embedded%20in%0Aa%20set%20of%20guide%20functions%20that%20map%20leaves%20at%20different%20positions%20into%20a%20more%0Aseparable%20embedding%20space.%20Our%20GMT%20consistently%20outperforms%20the%0Astate-of-the-art%20on%20three%20public%20plant%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17109v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGMT%253A%2520Guided%2520Mask%2520Transformer%2520for%2520Leaf%2520Instance%2520Segmentation%26entry.906535625%3DFeng%2520Chen%2520and%2520Sotirios%2520A.%2520Tsaftaris%2520and%2520Mario%2520Valerio%2520Giuffrida%26entry.1292438233%3D%2520%2520Leaf%2520instance%2520segmentation%2520is%2520a%2520challenging%2520multi-instance%2520segmentation%2520task%252C%250Aaiming%2520to%2520separate%2520and%2520delineate%2520each%2520leaf%2520in%2520an%2520image%2520of%2520a%2520plant.%2520Accurate%250Asegmentation%2520of%2520each%2520leaf%2520is%2520crucial%2520for%2520plant-related%2520applications%2520such%2520as%2520the%250Afine-grained%2520monitoring%2520of%2520plant%2520growth%2520and%2520crop%2520yield%2520estimation.%2520This%2520task%2520is%250Achallenging%2520because%2520of%2520the%2520high%2520similarity%2520%2528in%2520shape%2520and%2520colour%2529%252C%2520great%2520size%250Avariation%252C%2520and%2520heavy%2520occlusions%2520among%2520leaf%2520instances.%2520Furthermore%252C%2520the%250Atypically%2520small%2520size%2520of%2520annotated%2520leaf%2520datasets%2520makes%2520it%2520more%2520difficult%2520to%250Alearn%2520the%2520distinctive%2520features%2520needed%2520for%2520precise%2520segmentation.%2520We%2520hypothesise%250Athat%2520the%2520key%2520to%2520overcoming%2520the%2520these%2520challenges%2520lies%2520in%2520the%2520specific%2520spatial%250Apatterns%2520of%2520leaf%2520distribution.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520Guided%2520Mask%250ATransformer%2520%2528GMT%2529%252C%2520which%2520leverages%2520and%2520integrates%2520leaf%2520spatial%2520distribution%250Apriors%2520into%2520a%2520Transformer-based%2520segmentor.%2520These%2520spatial%2520priors%2520are%2520embedded%2520in%250Aa%2520set%2520of%2520guide%2520functions%2520that%2520map%2520leaves%2520at%2520different%2520positions%2520into%2520a%2520more%250Aseparable%2520embedding%2520space.%2520Our%2520GMT%2520consistently%2520outperforms%2520the%250Astate-of-the-art%2520on%2520three%2520public%2520plant%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17109v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GMT%3A%20Guided%20Mask%20Transformer%20for%20Leaf%20Instance%20Segmentation&entry.906535625=Feng%20Chen%20and%20Sotirios%20A.%20Tsaftaris%20and%20Mario%20Valerio%20Giuffrida&entry.1292438233=%20%20Leaf%20instance%20segmentation%20is%20a%20challenging%20multi-instance%20segmentation%20task%2C%0Aaiming%20to%20separate%20and%20delineate%20each%20leaf%20in%20an%20image%20of%20a%20plant.%20Accurate%0Asegmentation%20of%20each%20leaf%20is%20crucial%20for%20plant-related%20applications%20such%20as%20the%0Afine-grained%20monitoring%20of%20plant%20growth%20and%20crop%20yield%20estimation.%20This%20task%20is%0Achallenging%20because%20of%20the%20high%20similarity%20%28in%20shape%20and%20colour%29%2C%20great%20size%0Avariation%2C%20and%20heavy%20occlusions%20among%20leaf%20instances.%20Furthermore%2C%20the%0Atypically%20small%20size%20of%20annotated%20leaf%20datasets%20makes%20it%20more%20difficult%20to%0Alearn%20the%20distinctive%20features%20needed%20for%20precise%20segmentation.%20We%20hypothesise%0Athat%20the%20key%20to%20overcoming%20the%20these%20challenges%20lies%20in%20the%20specific%20spatial%0Apatterns%20of%20leaf%20distribution.%20In%20this%20paper%2C%20we%20propose%20the%20Guided%20Mask%0ATransformer%20%28GMT%29%2C%20which%20leverages%20and%20integrates%20leaf%20spatial%20distribution%0Apriors%20into%20a%20Transformer-based%20segmentor.%20These%20spatial%20priors%20are%20embedded%20in%0Aa%20set%20of%20guide%20functions%20that%20map%20leaves%20at%20different%20positions%20into%20a%20more%0Aseparable%20embedding%20space.%20Our%20GMT%20consistently%20outperforms%20the%0Astate-of-the-art%20on%20three%20public%20plant%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17109v2&entry.124074799=Read"},
{"title": "Current Symmetry Group Equivariant Convolution Frameworks for\n  Representation Learning", "author": "Ramzan Basheer and Deepak Mishra", "abstract": "  Euclidean deep learning is often inadequate for addressing real-world signals\nwhere the representation space is irregular and curved with complex topologies.\nInterpreting the geometric properties of such feature spaces has become\nparamount in obtaining robust and compact feature representations that remain\nunaffected by nontrivial geometric transformations, which vanilla CNNs cannot\neffectively handle. Recognizing rotation, translation, permutation, or scale\nsymmetries can lead to equivariance properties in the learned representations.\nThis has led to notable advancements in computer vision and machine learning\ntasks under the framework of geometric deep learning, as compared to their\ninvariant counterparts. In this report, we emphasize the importance of symmetry\ngroup equivariant deep learning models and their realization of\nconvolution-like operations on graphs, 3D shapes, and non-Euclidean spaces by\nleveraging group theory and symmetry. We categorize them as regular, steerable,\nand PDE-based convolutions and thoroughly examine the inherent symmetries of\ntheir input spaces and ensuing representations. We also outline the\nmathematical link between group convolutions or message aggregation operations\nand the concept of equivariance. The report also highlights various datasets,\ntheir application scopes, limitations, and insightful observations on future\ndirections to serve as a valuable reference and stimulate further research in\nthis emerging discipline.\n", "link": "http://arxiv.org/abs/2409.07327v1", "date": "2024-09-11", "relevancy": 1.957, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4962}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4843}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Current%20Symmetry%20Group%20Equivariant%20Convolution%20Frameworks%20for%0A%20%20Representation%20Learning&body=Title%3A%20Current%20Symmetry%20Group%20Equivariant%20Convolution%20Frameworks%20for%0A%20%20Representation%20Learning%0AAuthor%3A%20Ramzan%20Basheer%20and%20Deepak%20Mishra%0AAbstract%3A%20%20%20Euclidean%20deep%20learning%20is%20often%20inadequate%20for%20addressing%20real-world%20signals%0Awhere%20the%20representation%20space%20is%20irregular%20and%20curved%20with%20complex%20topologies.%0AInterpreting%20the%20geometric%20properties%20of%20such%20feature%20spaces%20has%20become%0Aparamount%20in%20obtaining%20robust%20and%20compact%20feature%20representations%20that%20remain%0Aunaffected%20by%20nontrivial%20geometric%20transformations%2C%20which%20vanilla%20CNNs%20cannot%0Aeffectively%20handle.%20Recognizing%20rotation%2C%20translation%2C%20permutation%2C%20or%20scale%0Asymmetries%20can%20lead%20to%20equivariance%20properties%20in%20the%20learned%20representations.%0AThis%20has%20led%20to%20notable%20advancements%20in%20computer%20vision%20and%20machine%20learning%0Atasks%20under%20the%20framework%20of%20geometric%20deep%20learning%2C%20as%20compared%20to%20their%0Ainvariant%20counterparts.%20In%20this%20report%2C%20we%20emphasize%20the%20importance%20of%20symmetry%0Agroup%20equivariant%20deep%20learning%20models%20and%20their%20realization%20of%0Aconvolution-like%20operations%20on%20graphs%2C%203D%20shapes%2C%20and%20non-Euclidean%20spaces%20by%0Aleveraging%20group%20theory%20and%20symmetry.%20We%20categorize%20them%20as%20regular%2C%20steerable%2C%0Aand%20PDE-based%20convolutions%20and%20thoroughly%20examine%20the%20inherent%20symmetries%20of%0Atheir%20input%20spaces%20and%20ensuing%20representations.%20We%20also%20outline%20the%0Amathematical%20link%20between%20group%20convolutions%20or%20message%20aggregation%20operations%0Aand%20the%20concept%20of%20equivariance.%20The%20report%20also%20highlights%20various%20datasets%2C%0Atheir%20application%20scopes%2C%20limitations%2C%20and%20insightful%20observations%20on%20future%0Adirections%20to%20serve%20as%20a%20valuable%20reference%20and%20stimulate%20further%20research%20in%0Athis%20emerging%20discipline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07327v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCurrent%2520Symmetry%2520Group%2520Equivariant%2520Convolution%2520Frameworks%2520for%250A%2520%2520Representation%2520Learning%26entry.906535625%3DRamzan%2520Basheer%2520and%2520Deepak%2520Mishra%26entry.1292438233%3D%2520%2520Euclidean%2520deep%2520learning%2520is%2520often%2520inadequate%2520for%2520addressing%2520real-world%2520signals%250Awhere%2520the%2520representation%2520space%2520is%2520irregular%2520and%2520curved%2520with%2520complex%2520topologies.%250AInterpreting%2520the%2520geometric%2520properties%2520of%2520such%2520feature%2520spaces%2520has%2520become%250Aparamount%2520in%2520obtaining%2520robust%2520and%2520compact%2520feature%2520representations%2520that%2520remain%250Aunaffected%2520by%2520nontrivial%2520geometric%2520transformations%252C%2520which%2520vanilla%2520CNNs%2520cannot%250Aeffectively%2520handle.%2520Recognizing%2520rotation%252C%2520translation%252C%2520permutation%252C%2520or%2520scale%250Asymmetries%2520can%2520lead%2520to%2520equivariance%2520properties%2520in%2520the%2520learned%2520representations.%250AThis%2520has%2520led%2520to%2520notable%2520advancements%2520in%2520computer%2520vision%2520and%2520machine%2520learning%250Atasks%2520under%2520the%2520framework%2520of%2520geometric%2520deep%2520learning%252C%2520as%2520compared%2520to%2520their%250Ainvariant%2520counterparts.%2520In%2520this%2520report%252C%2520we%2520emphasize%2520the%2520importance%2520of%2520symmetry%250Agroup%2520equivariant%2520deep%2520learning%2520models%2520and%2520their%2520realization%2520of%250Aconvolution-like%2520operations%2520on%2520graphs%252C%25203D%2520shapes%252C%2520and%2520non-Euclidean%2520spaces%2520by%250Aleveraging%2520group%2520theory%2520and%2520symmetry.%2520We%2520categorize%2520them%2520as%2520regular%252C%2520steerable%252C%250Aand%2520PDE-based%2520convolutions%2520and%2520thoroughly%2520examine%2520the%2520inherent%2520symmetries%2520of%250Atheir%2520input%2520spaces%2520and%2520ensuing%2520representations.%2520We%2520also%2520outline%2520the%250Amathematical%2520link%2520between%2520group%2520convolutions%2520or%2520message%2520aggregation%2520operations%250Aand%2520the%2520concept%2520of%2520equivariance.%2520The%2520report%2520also%2520highlights%2520various%2520datasets%252C%250Atheir%2520application%2520scopes%252C%2520limitations%252C%2520and%2520insightful%2520observations%2520on%2520future%250Adirections%2520to%2520serve%2520as%2520a%2520valuable%2520reference%2520and%2520stimulate%2520further%2520research%2520in%250Athis%2520emerging%2520discipline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07327v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Current%20Symmetry%20Group%20Equivariant%20Convolution%20Frameworks%20for%0A%20%20Representation%20Learning&entry.906535625=Ramzan%20Basheer%20and%20Deepak%20Mishra&entry.1292438233=%20%20Euclidean%20deep%20learning%20is%20often%20inadequate%20for%20addressing%20real-world%20signals%0Awhere%20the%20representation%20space%20is%20irregular%20and%20curved%20with%20complex%20topologies.%0AInterpreting%20the%20geometric%20properties%20of%20such%20feature%20spaces%20has%20become%0Aparamount%20in%20obtaining%20robust%20and%20compact%20feature%20representations%20that%20remain%0Aunaffected%20by%20nontrivial%20geometric%20transformations%2C%20which%20vanilla%20CNNs%20cannot%0Aeffectively%20handle.%20Recognizing%20rotation%2C%20translation%2C%20permutation%2C%20or%20scale%0Asymmetries%20can%20lead%20to%20equivariance%20properties%20in%20the%20learned%20representations.%0AThis%20has%20led%20to%20notable%20advancements%20in%20computer%20vision%20and%20machine%20learning%0Atasks%20under%20the%20framework%20of%20geometric%20deep%20learning%2C%20as%20compared%20to%20their%0Ainvariant%20counterparts.%20In%20this%20report%2C%20we%20emphasize%20the%20importance%20of%20symmetry%0Agroup%20equivariant%20deep%20learning%20models%20and%20their%20realization%20of%0Aconvolution-like%20operations%20on%20graphs%2C%203D%20shapes%2C%20and%20non-Euclidean%20spaces%20by%0Aleveraging%20group%20theory%20and%20symmetry.%20We%20categorize%20them%20as%20regular%2C%20steerable%2C%0Aand%20PDE-based%20convolutions%20and%20thoroughly%20examine%20the%20inherent%20symmetries%20of%0Atheir%20input%20spaces%20and%20ensuing%20representations.%20We%20also%20outline%20the%0Amathematical%20link%20between%20group%20convolutions%20or%20message%20aggregation%20operations%0Aand%20the%20concept%20of%20equivariance.%20The%20report%20also%20highlights%20various%20datasets%2C%0Atheir%20application%20scopes%2C%20limitations%2C%20and%20insightful%20observations%20on%20future%0Adirections%20to%20serve%20as%20a%20valuable%20reference%20and%20stimulate%20further%20research%20in%0Athis%20emerging%20discipline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07327v1&entry.124074799=Read"},
{"title": "CLIBE: Detecting Dynamic Backdoors in Transformer-based NLP Models", "author": "Rui Zeng and Xi Chen and Yuwen Pu and Xuhong Zhang and Tianyu Du and Shouling Ji", "abstract": "  Backdoors can be injected into NLP models to induce misbehavior when the\ninput text contains a specific feature, known as a trigger, which the attacker\nsecretly selects. Unlike fixed words, phrases, or sentences used in the static\ntext trigger, NLP dynamic backdoor attacks design triggers associated with\nabstract and latent text features, making them considerably stealthier than\ntraditional static backdoor attacks. However, existing research on NLP backdoor\ndetection primarily focuses on defending against static backdoor attacks, while\ndetecting dynamic backdoors in NLP models remains largely unexplored. This\npaper presents CLIBE, the first framework to detect dynamic backdoors in\nTransformer-based NLP models. CLIBE injects a \"few-shot perturbation\" into the\nsuspect Transformer model by crafting optimized weight perturbation in the\nattention layers to make the perturbed model classify a limited number of\nreference samples as a target label. Subsequently, CLIBE leverages the\ngeneralization ability of this few-shot perturbation to determine whether the\noriginal model contains a dynamic backdoor. Extensive evaluation on three\nadvanced NLP dynamic backdoor attacks, two widely-used Transformer frameworks,\nand four real-world classification tasks strongly validates the effectiveness\nof CLIBE. We also demonstrate the robustness of CLIBE against various adaptive\nattacks. Furthermore, we employ CLIBE to scrutinize 49 popular Transformer\nmodels on Hugging Face and discover one exhibiting a high probability of\ncontaining a dynamic backdoor. We have contacted Hugging Face and provided\ndetailed evidence of this model's backdoor behavior. Moreover, we extend CLIBE\nto detect backdoor text generation models modified to exhibit toxic behavior.\nTo the best of our knowledge, CLIBE is the first framework capable of detecting\nbackdoors in text generation models without access to trigger input test\nsamples.\n", "link": "http://arxiv.org/abs/2409.01193v2", "date": "2024-09-11", "relevancy": 1.9523, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4986}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4831}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4795}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIBE%3A%20Detecting%20Dynamic%20Backdoors%20in%20Transformer-based%20NLP%20Models&body=Title%3A%20CLIBE%3A%20Detecting%20Dynamic%20Backdoors%20in%20Transformer-based%20NLP%20Models%0AAuthor%3A%20Rui%20Zeng%20and%20Xi%20Chen%20and%20Yuwen%20Pu%20and%20Xuhong%20Zhang%20and%20Tianyu%20Du%20and%20Shouling%20Ji%0AAbstract%3A%20%20%20Backdoors%20can%20be%20injected%20into%20NLP%20models%20to%20induce%20misbehavior%20when%20the%0Ainput%20text%20contains%20a%20specific%20feature%2C%20known%20as%20a%20trigger%2C%20which%20the%20attacker%0Asecretly%20selects.%20Unlike%20fixed%20words%2C%20phrases%2C%20or%20sentences%20used%20in%20the%20static%0Atext%20trigger%2C%20NLP%20dynamic%20backdoor%20attacks%20design%20triggers%20associated%20with%0Aabstract%20and%20latent%20text%20features%2C%20making%20them%20considerably%20stealthier%20than%0Atraditional%20static%20backdoor%20attacks.%20However%2C%20existing%20research%20on%20NLP%20backdoor%0Adetection%20primarily%20focuses%20on%20defending%20against%20static%20backdoor%20attacks%2C%20while%0Adetecting%20dynamic%20backdoors%20in%20NLP%20models%20remains%20largely%20unexplored.%20This%0Apaper%20presents%20CLIBE%2C%20the%20first%20framework%20to%20detect%20dynamic%20backdoors%20in%0ATransformer-based%20NLP%20models.%20CLIBE%20injects%20a%20%22few-shot%20perturbation%22%20into%20the%0Asuspect%20Transformer%20model%20by%20crafting%20optimized%20weight%20perturbation%20in%20the%0Aattention%20layers%20to%20make%20the%20perturbed%20model%20classify%20a%20limited%20number%20of%0Areference%20samples%20as%20a%20target%20label.%20Subsequently%2C%20CLIBE%20leverages%20the%0Ageneralization%20ability%20of%20this%20few-shot%20perturbation%20to%20determine%20whether%20the%0Aoriginal%20model%20contains%20a%20dynamic%20backdoor.%20Extensive%20evaluation%20on%20three%0Aadvanced%20NLP%20dynamic%20backdoor%20attacks%2C%20two%20widely-used%20Transformer%20frameworks%2C%0Aand%20four%20real-world%20classification%20tasks%20strongly%20validates%20the%20effectiveness%0Aof%20CLIBE.%20We%20also%20demonstrate%20the%20robustness%20of%20CLIBE%20against%20various%20adaptive%0Aattacks.%20Furthermore%2C%20we%20employ%20CLIBE%20to%20scrutinize%2049%20popular%20Transformer%0Amodels%20on%20Hugging%20Face%20and%20discover%20one%20exhibiting%20a%20high%20probability%20of%0Acontaining%20a%20dynamic%20backdoor.%20We%20have%20contacted%20Hugging%20Face%20and%20provided%0Adetailed%20evidence%20of%20this%20model%27s%20backdoor%20behavior.%20Moreover%2C%20we%20extend%20CLIBE%0Ato%20detect%20backdoor%20text%20generation%20models%20modified%20to%20exhibit%20toxic%20behavior.%0ATo%20the%20best%20of%20our%20knowledge%2C%20CLIBE%20is%20the%20first%20framework%20capable%20of%20detecting%0Abackdoors%20in%20text%20generation%20models%20without%20access%20to%20trigger%20input%20test%0Asamples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.01193v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIBE%253A%2520Detecting%2520Dynamic%2520Backdoors%2520in%2520Transformer-based%2520NLP%2520Models%26entry.906535625%3DRui%2520Zeng%2520and%2520Xi%2520Chen%2520and%2520Yuwen%2520Pu%2520and%2520Xuhong%2520Zhang%2520and%2520Tianyu%2520Du%2520and%2520Shouling%2520Ji%26entry.1292438233%3D%2520%2520Backdoors%2520can%2520be%2520injected%2520into%2520NLP%2520models%2520to%2520induce%2520misbehavior%2520when%2520the%250Ainput%2520text%2520contains%2520a%2520specific%2520feature%252C%2520known%2520as%2520a%2520trigger%252C%2520which%2520the%2520attacker%250Asecretly%2520selects.%2520Unlike%2520fixed%2520words%252C%2520phrases%252C%2520or%2520sentences%2520used%2520in%2520the%2520static%250Atext%2520trigger%252C%2520NLP%2520dynamic%2520backdoor%2520attacks%2520design%2520triggers%2520associated%2520with%250Aabstract%2520and%2520latent%2520text%2520features%252C%2520making%2520them%2520considerably%2520stealthier%2520than%250Atraditional%2520static%2520backdoor%2520attacks.%2520However%252C%2520existing%2520research%2520on%2520NLP%2520backdoor%250Adetection%2520primarily%2520focuses%2520on%2520defending%2520against%2520static%2520backdoor%2520attacks%252C%2520while%250Adetecting%2520dynamic%2520backdoors%2520in%2520NLP%2520models%2520remains%2520largely%2520unexplored.%2520This%250Apaper%2520presents%2520CLIBE%252C%2520the%2520first%2520framework%2520to%2520detect%2520dynamic%2520backdoors%2520in%250ATransformer-based%2520NLP%2520models.%2520CLIBE%2520injects%2520a%2520%2522few-shot%2520perturbation%2522%2520into%2520the%250Asuspect%2520Transformer%2520model%2520by%2520crafting%2520optimized%2520weight%2520perturbation%2520in%2520the%250Aattention%2520layers%2520to%2520make%2520the%2520perturbed%2520model%2520classify%2520a%2520limited%2520number%2520of%250Areference%2520samples%2520as%2520a%2520target%2520label.%2520Subsequently%252C%2520CLIBE%2520leverages%2520the%250Ageneralization%2520ability%2520of%2520this%2520few-shot%2520perturbation%2520to%2520determine%2520whether%2520the%250Aoriginal%2520model%2520contains%2520a%2520dynamic%2520backdoor.%2520Extensive%2520evaluation%2520on%2520three%250Aadvanced%2520NLP%2520dynamic%2520backdoor%2520attacks%252C%2520two%2520widely-used%2520Transformer%2520frameworks%252C%250Aand%2520four%2520real-world%2520classification%2520tasks%2520strongly%2520validates%2520the%2520effectiveness%250Aof%2520CLIBE.%2520We%2520also%2520demonstrate%2520the%2520robustness%2520of%2520CLIBE%2520against%2520various%2520adaptive%250Aattacks.%2520Furthermore%252C%2520we%2520employ%2520CLIBE%2520to%2520scrutinize%252049%2520popular%2520Transformer%250Amodels%2520on%2520Hugging%2520Face%2520and%2520discover%2520one%2520exhibiting%2520a%2520high%2520probability%2520of%250Acontaining%2520a%2520dynamic%2520backdoor.%2520We%2520have%2520contacted%2520Hugging%2520Face%2520and%2520provided%250Adetailed%2520evidence%2520of%2520this%2520model%2527s%2520backdoor%2520behavior.%2520Moreover%252C%2520we%2520extend%2520CLIBE%250Ato%2520detect%2520backdoor%2520text%2520generation%2520models%2520modified%2520to%2520exhibit%2520toxic%2520behavior.%250ATo%2520the%2520best%2520of%2520our%2520knowledge%252C%2520CLIBE%2520is%2520the%2520first%2520framework%2520capable%2520of%2520detecting%250Abackdoors%2520in%2520text%2520generation%2520models%2520without%2520access%2520to%2520trigger%2520input%2520test%250Asamples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.01193v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIBE%3A%20Detecting%20Dynamic%20Backdoors%20in%20Transformer-based%20NLP%20Models&entry.906535625=Rui%20Zeng%20and%20Xi%20Chen%20and%20Yuwen%20Pu%20and%20Xuhong%20Zhang%20and%20Tianyu%20Du%20and%20Shouling%20Ji&entry.1292438233=%20%20Backdoors%20can%20be%20injected%20into%20NLP%20models%20to%20induce%20misbehavior%20when%20the%0Ainput%20text%20contains%20a%20specific%20feature%2C%20known%20as%20a%20trigger%2C%20which%20the%20attacker%0Asecretly%20selects.%20Unlike%20fixed%20words%2C%20phrases%2C%20or%20sentences%20used%20in%20the%20static%0Atext%20trigger%2C%20NLP%20dynamic%20backdoor%20attacks%20design%20triggers%20associated%20with%0Aabstract%20and%20latent%20text%20features%2C%20making%20them%20considerably%20stealthier%20than%0Atraditional%20static%20backdoor%20attacks.%20However%2C%20existing%20research%20on%20NLP%20backdoor%0Adetection%20primarily%20focuses%20on%20defending%20against%20static%20backdoor%20attacks%2C%20while%0Adetecting%20dynamic%20backdoors%20in%20NLP%20models%20remains%20largely%20unexplored.%20This%0Apaper%20presents%20CLIBE%2C%20the%20first%20framework%20to%20detect%20dynamic%20backdoors%20in%0ATransformer-based%20NLP%20models.%20CLIBE%20injects%20a%20%22few-shot%20perturbation%22%20into%20the%0Asuspect%20Transformer%20model%20by%20crafting%20optimized%20weight%20perturbation%20in%20the%0Aattention%20layers%20to%20make%20the%20perturbed%20model%20classify%20a%20limited%20number%20of%0Areference%20samples%20as%20a%20target%20label.%20Subsequently%2C%20CLIBE%20leverages%20the%0Ageneralization%20ability%20of%20this%20few-shot%20perturbation%20to%20determine%20whether%20the%0Aoriginal%20model%20contains%20a%20dynamic%20backdoor.%20Extensive%20evaluation%20on%20three%0Aadvanced%20NLP%20dynamic%20backdoor%20attacks%2C%20two%20widely-used%20Transformer%20frameworks%2C%0Aand%20four%20real-world%20classification%20tasks%20strongly%20validates%20the%20effectiveness%0Aof%20CLIBE.%20We%20also%20demonstrate%20the%20robustness%20of%20CLIBE%20against%20various%20adaptive%0Aattacks.%20Furthermore%2C%20we%20employ%20CLIBE%20to%20scrutinize%2049%20popular%20Transformer%0Amodels%20on%20Hugging%20Face%20and%20discover%20one%20exhibiting%20a%20high%20probability%20of%0Acontaining%20a%20dynamic%20backdoor.%20We%20have%20contacted%20Hugging%20Face%20and%20provided%0Adetailed%20evidence%20of%20this%20model%27s%20backdoor%20behavior.%20Moreover%2C%20we%20extend%20CLIBE%0Ato%20detect%20backdoor%20text%20generation%20models%20modified%20to%20exhibit%20toxic%20behavior.%0ATo%20the%20best%20of%20our%20knowledge%2C%20CLIBE%20is%20the%20first%20framework%20capable%20of%20detecting%0Abackdoors%20in%20text%20generation%20models%20without%20access%20to%20trigger%20input%20test%0Asamples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.01193v2&entry.124074799=Read"},
{"title": "CriticEval: Evaluating Large Language Model as Critic", "author": "Tian Lan and Wenwei Zhang and Chen Xu and Heyan Huang and Dahua Lin and Kai Chen and Xian-ling Mao", "abstract": "  Critique ability, i.e., the capability of Large Language Models (LLMs) to\nidentify and rectify flaws in responses, is crucial for their applications in\nself-improvement and scalable oversight. While numerous studies have been\nproposed to evaluate critique ability of LLMs, their comprehensiveness and\nreliability are still limited. To overcome this problem, we introduce\nCriticEval, a novel benchmark designed to comprehensively and reliably evaluate\ncritique ability of LLMs. Specifically, to ensure the comprehensiveness,\nCriticEval evaluates critique ability from four dimensions across nine diverse\ntask scenarios. It evaluates both scalar-valued and textual critiques,\ntargeting responses of varying quality. To ensure the reliability, a large\nnumber of critiques are annotated to serve as references, enabling GPT-4 to\nevaluate textual critiques reliably. Extensive evaluations of open-source and\nclosed-source LLMs first validate the reliability of evaluation in CriticEval.\nThen, experimental results demonstrate the promising potential of open-source\nLLMs, the effectiveness of critique datasets and several intriguing\nrelationships between the critique ability and some critical factors, including\ntask types, response qualities and critique dimensions. Datasets and evaluation\ntoolkit for CriticEval will be publicly released.\n", "link": "http://arxiv.org/abs/2402.13764v4", "date": "2024-09-11", "relevancy": 1.9412, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4884}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4884}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CriticEval%3A%20Evaluating%20Large%20Language%20Model%20as%20Critic&body=Title%3A%20CriticEval%3A%20Evaluating%20Large%20Language%20Model%20as%20Critic%0AAuthor%3A%20Tian%20Lan%20and%20Wenwei%20Zhang%20and%20Chen%20Xu%20and%20Heyan%20Huang%20and%20Dahua%20Lin%20and%20Kai%20Chen%20and%20Xian-ling%20Mao%0AAbstract%3A%20%20%20Critique%20ability%2C%20i.e.%2C%20the%20capability%20of%20Large%20Language%20Models%20%28LLMs%29%20to%0Aidentify%20and%20rectify%20flaws%20in%20responses%2C%20is%20crucial%20for%20their%20applications%20in%0Aself-improvement%20and%20scalable%20oversight.%20While%20numerous%20studies%20have%20been%0Aproposed%20to%20evaluate%20critique%20ability%20of%20LLMs%2C%20their%20comprehensiveness%20and%0Areliability%20are%20still%20limited.%20To%20overcome%20this%20problem%2C%20we%20introduce%0ACriticEval%2C%20a%20novel%20benchmark%20designed%20to%20comprehensively%20and%20reliably%20evaluate%0Acritique%20ability%20of%20LLMs.%20Specifically%2C%20to%20ensure%20the%20comprehensiveness%2C%0ACriticEval%20evaluates%20critique%20ability%20from%20four%20dimensions%20across%20nine%20diverse%0Atask%20scenarios.%20It%20evaluates%20both%20scalar-valued%20and%20textual%20critiques%2C%0Atargeting%20responses%20of%20varying%20quality.%20To%20ensure%20the%20reliability%2C%20a%20large%0Anumber%20of%20critiques%20are%20annotated%20to%20serve%20as%20references%2C%20enabling%20GPT-4%20to%0Aevaluate%20textual%20critiques%20reliably.%20Extensive%20evaluations%20of%20open-source%20and%0Aclosed-source%20LLMs%20first%20validate%20the%20reliability%20of%20evaluation%20in%20CriticEval.%0AThen%2C%20experimental%20results%20demonstrate%20the%20promising%20potential%20of%20open-source%0ALLMs%2C%20the%20effectiveness%20of%20critique%20datasets%20and%20several%20intriguing%0Arelationships%20between%20the%20critique%20ability%20and%20some%20critical%20factors%2C%20including%0Atask%20types%2C%20response%20qualities%20and%20critique%20dimensions.%20Datasets%20and%20evaluation%0Atoolkit%20for%20CriticEval%20will%20be%20publicly%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13764v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCriticEval%253A%2520Evaluating%2520Large%2520Language%2520Model%2520as%2520Critic%26entry.906535625%3DTian%2520Lan%2520and%2520Wenwei%2520Zhang%2520and%2520Chen%2520Xu%2520and%2520Heyan%2520Huang%2520and%2520Dahua%2520Lin%2520and%2520Kai%2520Chen%2520and%2520Xian-ling%2520Mao%26entry.1292438233%3D%2520%2520Critique%2520ability%252C%2520i.e.%252C%2520the%2520capability%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%250Aidentify%2520and%2520rectify%2520flaws%2520in%2520responses%252C%2520is%2520crucial%2520for%2520their%2520applications%2520in%250Aself-improvement%2520and%2520scalable%2520oversight.%2520While%2520numerous%2520studies%2520have%2520been%250Aproposed%2520to%2520evaluate%2520critique%2520ability%2520of%2520LLMs%252C%2520their%2520comprehensiveness%2520and%250Areliability%2520are%2520still%2520limited.%2520To%2520overcome%2520this%2520problem%252C%2520we%2520introduce%250ACriticEval%252C%2520a%2520novel%2520benchmark%2520designed%2520to%2520comprehensively%2520and%2520reliably%2520evaluate%250Acritique%2520ability%2520of%2520LLMs.%2520Specifically%252C%2520to%2520ensure%2520the%2520comprehensiveness%252C%250ACriticEval%2520evaluates%2520critique%2520ability%2520from%2520four%2520dimensions%2520across%2520nine%2520diverse%250Atask%2520scenarios.%2520It%2520evaluates%2520both%2520scalar-valued%2520and%2520textual%2520critiques%252C%250Atargeting%2520responses%2520of%2520varying%2520quality.%2520To%2520ensure%2520the%2520reliability%252C%2520a%2520large%250Anumber%2520of%2520critiques%2520are%2520annotated%2520to%2520serve%2520as%2520references%252C%2520enabling%2520GPT-4%2520to%250Aevaluate%2520textual%2520critiques%2520reliably.%2520Extensive%2520evaluations%2520of%2520open-source%2520and%250Aclosed-source%2520LLMs%2520first%2520validate%2520the%2520reliability%2520of%2520evaluation%2520in%2520CriticEval.%250AThen%252C%2520experimental%2520results%2520demonstrate%2520the%2520promising%2520potential%2520of%2520open-source%250ALLMs%252C%2520the%2520effectiveness%2520of%2520critique%2520datasets%2520and%2520several%2520intriguing%250Arelationships%2520between%2520the%2520critique%2520ability%2520and%2520some%2520critical%2520factors%252C%2520including%250Atask%2520types%252C%2520response%2520qualities%2520and%2520critique%2520dimensions.%2520Datasets%2520and%2520evaluation%250Atoolkit%2520for%2520CriticEval%2520will%2520be%2520publicly%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.13764v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CriticEval%3A%20Evaluating%20Large%20Language%20Model%20as%20Critic&entry.906535625=Tian%20Lan%20and%20Wenwei%20Zhang%20and%20Chen%20Xu%20and%20Heyan%20Huang%20and%20Dahua%20Lin%20and%20Kai%20Chen%20and%20Xian-ling%20Mao&entry.1292438233=%20%20Critique%20ability%2C%20i.e.%2C%20the%20capability%20of%20Large%20Language%20Models%20%28LLMs%29%20to%0Aidentify%20and%20rectify%20flaws%20in%20responses%2C%20is%20crucial%20for%20their%20applications%20in%0Aself-improvement%20and%20scalable%20oversight.%20While%20numerous%20studies%20have%20been%0Aproposed%20to%20evaluate%20critique%20ability%20of%20LLMs%2C%20their%20comprehensiveness%20and%0Areliability%20are%20still%20limited.%20To%20overcome%20this%20problem%2C%20we%20introduce%0ACriticEval%2C%20a%20novel%20benchmark%20designed%20to%20comprehensively%20and%20reliably%20evaluate%0Acritique%20ability%20of%20LLMs.%20Specifically%2C%20to%20ensure%20the%20comprehensiveness%2C%0ACriticEval%20evaluates%20critique%20ability%20from%20four%20dimensions%20across%20nine%20diverse%0Atask%20scenarios.%20It%20evaluates%20both%20scalar-valued%20and%20textual%20critiques%2C%0Atargeting%20responses%20of%20varying%20quality.%20To%20ensure%20the%20reliability%2C%20a%20large%0Anumber%20of%20critiques%20are%20annotated%20to%20serve%20as%20references%2C%20enabling%20GPT-4%20to%0Aevaluate%20textual%20critiques%20reliably.%20Extensive%20evaluations%20of%20open-source%20and%0Aclosed-source%20LLMs%20first%20validate%20the%20reliability%20of%20evaluation%20in%20CriticEval.%0AThen%2C%20experimental%20results%20demonstrate%20the%20promising%20potential%20of%20open-source%0ALLMs%2C%20the%20effectiveness%20of%20critique%20datasets%20and%20several%20intriguing%0Arelationships%20between%20the%20critique%20ability%20and%20some%20critical%20factors%2C%20including%0Atask%20types%2C%20response%20qualities%20and%20critique%20dimensions.%20Datasets%20and%20evaluation%0Atoolkit%20for%20CriticEval%20will%20be%20publicly%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13764v4&entry.124074799=Read"},
{"title": "Gradient descent provably escapes saddle points in the training of\n  shallow ReLU networks", "author": "Patrick Cheridito and Arnulf Jentzen and Florian Rossmannek", "abstract": "  Dynamical systems theory has recently been applied in optimization to prove\nthat gradient descent algorithms bypass so-called strict saddle points of the\nloss function. However, in many modern machine learning applications, the\nrequired regularity conditions are not satisfied. In this paper, we prove a\nvariant of the relevant dynamical systems result, a center-stable manifold\ntheorem, in which we relax some of the regularity requirements. We explore its\nrelevance for various machine learning tasks, with a particular focus on\nshallow rectified linear unit (ReLU) and leaky ReLU networks with scalar input.\nBuilding on a detailed examination of critical points of the square integral\nloss function for shallow ReLU and leaky ReLU networks relative to an affine\ntarget function, we show that gradient descent circumvents most saddle points.\nFurthermore, we prove convergence to global minima under favourable\ninitialization conditions, quantified by an explicit threshold on the limiting\nloss.\n", "link": "http://arxiv.org/abs/2208.02083v2", "date": "2024-09-11", "relevancy": 1.9411, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5094}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4722}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient%20descent%20provably%20escapes%20saddle%20points%20in%20the%20training%20of%0A%20%20shallow%20ReLU%20networks&body=Title%3A%20Gradient%20descent%20provably%20escapes%20saddle%20points%20in%20the%20training%20of%0A%20%20shallow%20ReLU%20networks%0AAuthor%3A%20Patrick%20Cheridito%20and%20Arnulf%20Jentzen%20and%20Florian%20Rossmannek%0AAbstract%3A%20%20%20Dynamical%20systems%20theory%20has%20recently%20been%20applied%20in%20optimization%20to%20prove%0Athat%20gradient%20descent%20algorithms%20bypass%20so-called%20strict%20saddle%20points%20of%20the%0Aloss%20function.%20However%2C%20in%20many%20modern%20machine%20learning%20applications%2C%20the%0Arequired%20regularity%20conditions%20are%20not%20satisfied.%20In%20this%20paper%2C%20we%20prove%20a%0Avariant%20of%20the%20relevant%20dynamical%20systems%20result%2C%20a%20center-stable%20manifold%0Atheorem%2C%20in%20which%20we%20relax%20some%20of%20the%20regularity%20requirements.%20We%20explore%20its%0Arelevance%20for%20various%20machine%20learning%20tasks%2C%20with%20a%20particular%20focus%20on%0Ashallow%20rectified%20linear%20unit%20%28ReLU%29%20and%20leaky%20ReLU%20networks%20with%20scalar%20input.%0ABuilding%20on%20a%20detailed%20examination%20of%20critical%20points%20of%20the%20square%20integral%0Aloss%20function%20for%20shallow%20ReLU%20and%20leaky%20ReLU%20networks%20relative%20to%20an%20affine%0Atarget%20function%2C%20we%20show%20that%20gradient%20descent%20circumvents%20most%20saddle%20points.%0AFurthermore%2C%20we%20prove%20convergence%20to%20global%20minima%20under%20favourable%0Ainitialization%20conditions%2C%20quantified%20by%20an%20explicit%20threshold%20on%20the%20limiting%0Aloss.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2208.02083v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient%2520descent%2520provably%2520escapes%2520saddle%2520points%2520in%2520the%2520training%2520of%250A%2520%2520shallow%2520ReLU%2520networks%26entry.906535625%3DPatrick%2520Cheridito%2520and%2520Arnulf%2520Jentzen%2520and%2520Florian%2520Rossmannek%26entry.1292438233%3D%2520%2520Dynamical%2520systems%2520theory%2520has%2520recently%2520been%2520applied%2520in%2520optimization%2520to%2520prove%250Athat%2520gradient%2520descent%2520algorithms%2520bypass%2520so-called%2520strict%2520saddle%2520points%2520of%2520the%250Aloss%2520function.%2520However%252C%2520in%2520many%2520modern%2520machine%2520learning%2520applications%252C%2520the%250Arequired%2520regularity%2520conditions%2520are%2520not%2520satisfied.%2520In%2520this%2520paper%252C%2520we%2520prove%2520a%250Avariant%2520of%2520the%2520relevant%2520dynamical%2520systems%2520result%252C%2520a%2520center-stable%2520manifold%250Atheorem%252C%2520in%2520which%2520we%2520relax%2520some%2520of%2520the%2520regularity%2520requirements.%2520We%2520explore%2520its%250Arelevance%2520for%2520various%2520machine%2520learning%2520tasks%252C%2520with%2520a%2520particular%2520focus%2520on%250Ashallow%2520rectified%2520linear%2520unit%2520%2528ReLU%2529%2520and%2520leaky%2520ReLU%2520networks%2520with%2520scalar%2520input.%250ABuilding%2520on%2520a%2520detailed%2520examination%2520of%2520critical%2520points%2520of%2520the%2520square%2520integral%250Aloss%2520function%2520for%2520shallow%2520ReLU%2520and%2520leaky%2520ReLU%2520networks%2520relative%2520to%2520an%2520affine%250Atarget%2520function%252C%2520we%2520show%2520that%2520gradient%2520descent%2520circumvents%2520most%2520saddle%2520points.%250AFurthermore%252C%2520we%2520prove%2520convergence%2520to%2520global%2520minima%2520under%2520favourable%250Ainitialization%2520conditions%252C%2520quantified%2520by%2520an%2520explicit%2520threshold%2520on%2520the%2520limiting%250Aloss.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2208.02083v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient%20descent%20provably%20escapes%20saddle%20points%20in%20the%20training%20of%0A%20%20shallow%20ReLU%20networks&entry.906535625=Patrick%20Cheridito%20and%20Arnulf%20Jentzen%20and%20Florian%20Rossmannek&entry.1292438233=%20%20Dynamical%20systems%20theory%20has%20recently%20been%20applied%20in%20optimization%20to%20prove%0Athat%20gradient%20descent%20algorithms%20bypass%20so-called%20strict%20saddle%20points%20of%20the%0Aloss%20function.%20However%2C%20in%20many%20modern%20machine%20learning%20applications%2C%20the%0Arequired%20regularity%20conditions%20are%20not%20satisfied.%20In%20this%20paper%2C%20we%20prove%20a%0Avariant%20of%20the%20relevant%20dynamical%20systems%20result%2C%20a%20center-stable%20manifold%0Atheorem%2C%20in%20which%20we%20relax%20some%20of%20the%20regularity%20requirements.%20We%20explore%20its%0Arelevance%20for%20various%20machine%20learning%20tasks%2C%20with%20a%20particular%20focus%20on%0Ashallow%20rectified%20linear%20unit%20%28ReLU%29%20and%20leaky%20ReLU%20networks%20with%20scalar%20input.%0ABuilding%20on%20a%20detailed%20examination%20of%20critical%20points%20of%20the%20square%20integral%0Aloss%20function%20for%20shallow%20ReLU%20and%20leaky%20ReLU%20networks%20relative%20to%20an%20affine%0Atarget%20function%2C%20we%20show%20that%20gradient%20descent%20circumvents%20most%20saddle%20points.%0AFurthermore%2C%20we%20prove%20convergence%20to%20global%20minima%20under%20favourable%0Ainitialization%20conditions%2C%20quantified%20by%20an%20explicit%20threshold%20on%20the%20limiting%0Aloss.%0A&entry.1838667208=http%3A//arxiv.org/abs/2208.02083v2&entry.124074799=Read"},
{"title": "Multi-Type Preference Learning: Empowering Preference-Based\n  Reinforcement Learning with Equal Preferences", "author": "Ziang Liu and Junjie Xu and Xingjiao Wu and Jing Yang and Liang He", "abstract": "  Preference-Based reinforcement learning (PBRL) learns directly from the\npreferences of human teachers regarding agent behaviors without needing\nmeticulously designed reward functions. However, existing PBRL methods often\nlearn primarily from explicit preferences, neglecting the possibility that\nteachers may choose equal preferences. This neglect may hinder the\nunderstanding of the agent regarding the task perspective of the teacher,\nleading to the loss of important information. To address this issue, we\nintroduce the Equal Preference Learning Task, which optimizes the neural\nnetwork by promoting similar reward predictions when the behaviors of two\nagents are labeled as equal preferences. Building on this task, we propose a\nnovel PBRL method, Multi-Type Preference Learning (MTPL), which allows\nsimultaneous learning from equal preferences while leveraging existing methods\nfor learning from explicit preferences. To validate our approach, we design\nexperiments applying MTPL to four existing state-of-the-art baselines across\nten locomotion and robotic manipulation tasks in the DeepMind Control Suite.\nThe experimental results indicate that simultaneous learning from both equal\nand explicit preferences enables the PBRL method to more comprehensively\nunderstand the feedback from teachers, thereby enhancing feedback efficiency.\n", "link": "http://arxiv.org/abs/2409.07268v1", "date": "2024-09-11", "relevancy": 1.9318, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5364}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4779}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Type%20Preference%20Learning%3A%20Empowering%20Preference-Based%0A%20%20Reinforcement%20Learning%20with%20Equal%20Preferences&body=Title%3A%20Multi-Type%20Preference%20Learning%3A%20Empowering%20Preference-Based%0A%20%20Reinforcement%20Learning%20with%20Equal%20Preferences%0AAuthor%3A%20Ziang%20Liu%20and%20Junjie%20Xu%20and%20Xingjiao%20Wu%20and%20Jing%20Yang%20and%20Liang%20He%0AAbstract%3A%20%20%20Preference-Based%20reinforcement%20learning%20%28PBRL%29%20learns%20directly%20from%20the%0Apreferences%20of%20human%20teachers%20regarding%20agent%20behaviors%20without%20needing%0Ameticulously%20designed%20reward%20functions.%20However%2C%20existing%20PBRL%20methods%20often%0Alearn%20primarily%20from%20explicit%20preferences%2C%20neglecting%20the%20possibility%20that%0Ateachers%20may%20choose%20equal%20preferences.%20This%20neglect%20may%20hinder%20the%0Aunderstanding%20of%20the%20agent%20regarding%20the%20task%20perspective%20of%20the%20teacher%2C%0Aleading%20to%20the%20loss%20of%20important%20information.%20To%20address%20this%20issue%2C%20we%0Aintroduce%20the%20Equal%20Preference%20Learning%20Task%2C%20which%20optimizes%20the%20neural%0Anetwork%20by%20promoting%20similar%20reward%20predictions%20when%20the%20behaviors%20of%20two%0Aagents%20are%20labeled%20as%20equal%20preferences.%20Building%20on%20this%20task%2C%20we%20propose%20a%0Anovel%20PBRL%20method%2C%20Multi-Type%20Preference%20Learning%20%28MTPL%29%2C%20which%20allows%0Asimultaneous%20learning%20from%20equal%20preferences%20while%20leveraging%20existing%20methods%0Afor%20learning%20from%20explicit%20preferences.%20To%20validate%20our%20approach%2C%20we%20design%0Aexperiments%20applying%20MTPL%20to%20four%20existing%20state-of-the-art%20baselines%20across%0Aten%20locomotion%20and%20robotic%20manipulation%20tasks%20in%20the%20DeepMind%20Control%20Suite.%0AThe%20experimental%20results%20indicate%20that%20simultaneous%20learning%20from%20both%20equal%0Aand%20explicit%20preferences%20enables%20the%20PBRL%20method%20to%20more%20comprehensively%0Aunderstand%20the%20feedback%20from%20teachers%2C%20thereby%20enhancing%20feedback%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07268v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Type%2520Preference%2520Learning%253A%2520Empowering%2520Preference-Based%250A%2520%2520Reinforcement%2520Learning%2520with%2520Equal%2520Preferences%26entry.906535625%3DZiang%2520Liu%2520and%2520Junjie%2520Xu%2520and%2520Xingjiao%2520Wu%2520and%2520Jing%2520Yang%2520and%2520Liang%2520He%26entry.1292438233%3D%2520%2520Preference-Based%2520reinforcement%2520learning%2520%2528PBRL%2529%2520learns%2520directly%2520from%2520the%250Apreferences%2520of%2520human%2520teachers%2520regarding%2520agent%2520behaviors%2520without%2520needing%250Ameticulously%2520designed%2520reward%2520functions.%2520However%252C%2520existing%2520PBRL%2520methods%2520often%250Alearn%2520primarily%2520from%2520explicit%2520preferences%252C%2520neglecting%2520the%2520possibility%2520that%250Ateachers%2520may%2520choose%2520equal%2520preferences.%2520This%2520neglect%2520may%2520hinder%2520the%250Aunderstanding%2520of%2520the%2520agent%2520regarding%2520the%2520task%2520perspective%2520of%2520the%2520teacher%252C%250Aleading%2520to%2520the%2520loss%2520of%2520important%2520information.%2520To%2520address%2520this%2520issue%252C%2520we%250Aintroduce%2520the%2520Equal%2520Preference%2520Learning%2520Task%252C%2520which%2520optimizes%2520the%2520neural%250Anetwork%2520by%2520promoting%2520similar%2520reward%2520predictions%2520when%2520the%2520behaviors%2520of%2520two%250Aagents%2520are%2520labeled%2520as%2520equal%2520preferences.%2520Building%2520on%2520this%2520task%252C%2520we%2520propose%2520a%250Anovel%2520PBRL%2520method%252C%2520Multi-Type%2520Preference%2520Learning%2520%2528MTPL%2529%252C%2520which%2520allows%250Asimultaneous%2520learning%2520from%2520equal%2520preferences%2520while%2520leveraging%2520existing%2520methods%250Afor%2520learning%2520from%2520explicit%2520preferences.%2520To%2520validate%2520our%2520approach%252C%2520we%2520design%250Aexperiments%2520applying%2520MTPL%2520to%2520four%2520existing%2520state-of-the-art%2520baselines%2520across%250Aten%2520locomotion%2520and%2520robotic%2520manipulation%2520tasks%2520in%2520the%2520DeepMind%2520Control%2520Suite.%250AThe%2520experimental%2520results%2520indicate%2520that%2520simultaneous%2520learning%2520from%2520both%2520equal%250Aand%2520explicit%2520preferences%2520enables%2520the%2520PBRL%2520method%2520to%2520more%2520comprehensively%250Aunderstand%2520the%2520feedback%2520from%2520teachers%252C%2520thereby%2520enhancing%2520feedback%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07268v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Type%20Preference%20Learning%3A%20Empowering%20Preference-Based%0A%20%20Reinforcement%20Learning%20with%20Equal%20Preferences&entry.906535625=Ziang%20Liu%20and%20Junjie%20Xu%20and%20Xingjiao%20Wu%20and%20Jing%20Yang%20and%20Liang%20He&entry.1292438233=%20%20Preference-Based%20reinforcement%20learning%20%28PBRL%29%20learns%20directly%20from%20the%0Apreferences%20of%20human%20teachers%20regarding%20agent%20behaviors%20without%20needing%0Ameticulously%20designed%20reward%20functions.%20However%2C%20existing%20PBRL%20methods%20often%0Alearn%20primarily%20from%20explicit%20preferences%2C%20neglecting%20the%20possibility%20that%0Ateachers%20may%20choose%20equal%20preferences.%20This%20neglect%20may%20hinder%20the%0Aunderstanding%20of%20the%20agent%20regarding%20the%20task%20perspective%20of%20the%20teacher%2C%0Aleading%20to%20the%20loss%20of%20important%20information.%20To%20address%20this%20issue%2C%20we%0Aintroduce%20the%20Equal%20Preference%20Learning%20Task%2C%20which%20optimizes%20the%20neural%0Anetwork%20by%20promoting%20similar%20reward%20predictions%20when%20the%20behaviors%20of%20two%0Aagents%20are%20labeled%20as%20equal%20preferences.%20Building%20on%20this%20task%2C%20we%20propose%20a%0Anovel%20PBRL%20method%2C%20Multi-Type%20Preference%20Learning%20%28MTPL%29%2C%20which%20allows%0Asimultaneous%20learning%20from%20equal%20preferences%20while%20leveraging%20existing%20methods%0Afor%20learning%20from%20explicit%20preferences.%20To%20validate%20our%20approach%2C%20we%20design%0Aexperiments%20applying%20MTPL%20to%20four%20existing%20state-of-the-art%20baselines%20across%0Aten%20locomotion%20and%20robotic%20manipulation%20tasks%20in%20the%20DeepMind%20Control%20Suite.%0AThe%20experimental%20results%20indicate%20that%20simultaneous%20learning%20from%20both%20equal%0Aand%20explicit%20preferences%20enables%20the%20PBRL%20method%20to%20more%20comprehensively%0Aunderstand%20the%20feedback%20from%20teachers%2C%20thereby%20enhancing%20feedback%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07268v1&entry.124074799=Read"},
{"title": "Federated $\\mathcal{X}$-armed Bandit with Flexible Personalisation", "author": "Ali Arabzadeh and James A. Grant and David S. Leslie", "abstract": "  This paper introduces a novel approach to personalised federated learning\nwithin the $\\mathcal{X}$-armed bandit framework, addressing the challenge of\noptimising both local and global objectives in a highly heterogeneous\nenvironment. Our method employs a surrogate objective function that combines\nindividual client preferences with aggregated global knowledge, allowing for a\nflexible trade-off between personalisation and collective learning. We propose\na phase-based elimination algorithm that achieves sublinear regret with\nlogarithmic communication overhead, making it well-suited for federated\nsettings. Theoretical analysis and empirical evaluations demonstrate the\neffectiveness of our approach compared to existing methods. Potential\napplications of this work span various domains, including healthcare, smart\nhome devices, and e-commerce, where balancing personalisation with global\ninsights is crucial.\n", "link": "http://arxiv.org/abs/2409.07251v1", "date": "2024-09-11", "relevancy": 1.9024, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4804}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4797}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20%24%5Cmathcal%7BX%7D%24-armed%20Bandit%20with%20Flexible%20Personalisation&body=Title%3A%20Federated%20%24%5Cmathcal%7BX%7D%24-armed%20Bandit%20with%20Flexible%20Personalisation%0AAuthor%3A%20Ali%20Arabzadeh%20and%20James%20A.%20Grant%20and%20David%20S.%20Leslie%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20approach%20to%20personalised%20federated%20learning%0Awithin%20the%20%24%5Cmathcal%7BX%7D%24-armed%20bandit%20framework%2C%20addressing%20the%20challenge%20of%0Aoptimising%20both%20local%20and%20global%20objectives%20in%20a%20highly%20heterogeneous%0Aenvironment.%20Our%20method%20employs%20a%20surrogate%20objective%20function%20that%20combines%0Aindividual%20client%20preferences%20with%20aggregated%20global%20knowledge%2C%20allowing%20for%20a%0Aflexible%20trade-off%20between%20personalisation%20and%20collective%20learning.%20We%20propose%0Aa%20phase-based%20elimination%20algorithm%20that%20achieves%20sublinear%20regret%20with%0Alogarithmic%20communication%20overhead%2C%20making%20it%20well-suited%20for%20federated%0Asettings.%20Theoretical%20analysis%20and%20empirical%20evaluations%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20compared%20to%20existing%20methods.%20Potential%0Aapplications%20of%20this%20work%20span%20various%20domains%2C%20including%20healthcare%2C%20smart%0Ahome%20devices%2C%20and%20e-commerce%2C%20where%20balancing%20personalisation%20with%20global%0Ainsights%20is%20crucial.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07251v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520%2524%255Cmathcal%257BX%257D%2524-armed%2520Bandit%2520with%2520Flexible%2520Personalisation%26entry.906535625%3DAli%2520Arabzadeh%2520and%2520James%2520A.%2520Grant%2520and%2520David%2520S.%2520Leslie%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520approach%2520to%2520personalised%2520federated%2520learning%250Awithin%2520the%2520%2524%255Cmathcal%257BX%257D%2524-armed%2520bandit%2520framework%252C%2520addressing%2520the%2520challenge%2520of%250Aoptimising%2520both%2520local%2520and%2520global%2520objectives%2520in%2520a%2520highly%2520heterogeneous%250Aenvironment.%2520Our%2520method%2520employs%2520a%2520surrogate%2520objective%2520function%2520that%2520combines%250Aindividual%2520client%2520preferences%2520with%2520aggregated%2520global%2520knowledge%252C%2520allowing%2520for%2520a%250Aflexible%2520trade-off%2520between%2520personalisation%2520and%2520collective%2520learning.%2520We%2520propose%250Aa%2520phase-based%2520elimination%2520algorithm%2520that%2520achieves%2520sublinear%2520regret%2520with%250Alogarithmic%2520communication%2520overhead%252C%2520making%2520it%2520well-suited%2520for%2520federated%250Asettings.%2520Theoretical%2520analysis%2520and%2520empirical%2520evaluations%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520approach%2520compared%2520to%2520existing%2520methods.%2520Potential%250Aapplications%2520of%2520this%2520work%2520span%2520various%2520domains%252C%2520including%2520healthcare%252C%2520smart%250Ahome%2520devices%252C%2520and%2520e-commerce%252C%2520where%2520balancing%2520personalisation%2520with%2520global%250Ainsights%2520is%2520crucial.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07251v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20%24%5Cmathcal%7BX%7D%24-armed%20Bandit%20with%20Flexible%20Personalisation&entry.906535625=Ali%20Arabzadeh%20and%20James%20A.%20Grant%20and%20David%20S.%20Leslie&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20approach%20to%20personalised%20federated%20learning%0Awithin%20the%20%24%5Cmathcal%7BX%7D%24-armed%20bandit%20framework%2C%20addressing%20the%20challenge%20of%0Aoptimising%20both%20local%20and%20global%20objectives%20in%20a%20highly%20heterogeneous%0Aenvironment.%20Our%20method%20employs%20a%20surrogate%20objective%20function%20that%20combines%0Aindividual%20client%20preferences%20with%20aggregated%20global%20knowledge%2C%20allowing%20for%20a%0Aflexible%20trade-off%20between%20personalisation%20and%20collective%20learning.%20We%20propose%0Aa%20phase-based%20elimination%20algorithm%20that%20achieves%20sublinear%20regret%20with%0Alogarithmic%20communication%20overhead%2C%20making%20it%20well-suited%20for%20federated%0Asettings.%20Theoretical%20analysis%20and%20empirical%20evaluations%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20compared%20to%20existing%20methods.%20Potential%0Aapplications%20of%20this%20work%20span%20various%20domains%2C%20including%20healthcare%2C%20smart%0Ahome%20devices%2C%20and%20e-commerce%2C%20where%20balancing%20personalisation%20with%20global%0Ainsights%20is%20crucial.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07251v1&entry.124074799=Read"},
{"title": "Exclusive Style Removal for Cross Domain Novel Class Discovery", "author": "Yicheng Wang and Feng Liu and Junmin Liu and Zhen Fang and Kai Sun", "abstract": "  As a promising field in open-world learning, \\textit{Novel Class Discovery}\n(NCD) is usually a task to cluster unseen novel classes in an unlabeled set\nbased on the prior knowledge of labeled data within the same domain. However,\nthe performance of existing NCD methods could be severely compromised when\nnovel classes are sampled from a different distribution with the labeled ones.\nIn this paper, we explore and establish the solvability of NCD in cross domain\nsetting with the necessary condition that style information must be removed.\nBased on the theoretical analysis, we introduce an exclusive style removal\nmodule for extracting style information that is distinctive from the baseline\nfeatures, thereby facilitating inference. Moreover, this module is easy to\nintegrate with other NCD methods, acting as a plug-in to improve performance on\nnovel classes with different distributions compared to the seen labeled set.\nAdditionally, recognizing the non-negligible influence of different backbones\nand pre-training strategies on the performance of the NCD methods, we build a\nfair benchmark for future NCD research. Extensive experiments on three common\ndatasets demonstrate the effectiveness of our proposed module.\n", "link": "http://arxiv.org/abs/2406.18140v2", "date": "2024-09-11", "relevancy": 1.8924, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5223}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4797}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exclusive%20Style%20Removal%20for%20Cross%20Domain%20Novel%20Class%20Discovery&body=Title%3A%20Exclusive%20Style%20Removal%20for%20Cross%20Domain%20Novel%20Class%20Discovery%0AAuthor%3A%20Yicheng%20Wang%20and%20Feng%20Liu%20and%20Junmin%20Liu%20and%20Zhen%20Fang%20and%20Kai%20Sun%0AAbstract%3A%20%20%20As%20a%20promising%20field%20in%20open-world%20learning%2C%20%5Ctextit%7BNovel%20Class%20Discovery%7D%0A%28NCD%29%20is%20usually%20a%20task%20to%20cluster%20unseen%20novel%20classes%20in%20an%20unlabeled%20set%0Abased%20on%20the%20prior%20knowledge%20of%20labeled%20data%20within%20the%20same%20domain.%20However%2C%0Athe%20performance%20of%20existing%20NCD%20methods%20could%20be%20severely%20compromised%20when%0Anovel%20classes%20are%20sampled%20from%20a%20different%20distribution%20with%20the%20labeled%20ones.%0AIn%20this%20paper%2C%20we%20explore%20and%20establish%20the%20solvability%20of%20NCD%20in%20cross%20domain%0Asetting%20with%20the%20necessary%20condition%20that%20style%20information%20must%20be%20removed.%0ABased%20on%20the%20theoretical%20analysis%2C%20we%20introduce%20an%20exclusive%20style%20removal%0Amodule%20for%20extracting%20style%20information%20that%20is%20distinctive%20from%20the%20baseline%0Afeatures%2C%20thereby%20facilitating%20inference.%20Moreover%2C%20this%20module%20is%20easy%20to%0Aintegrate%20with%20other%20NCD%20methods%2C%20acting%20as%20a%20plug-in%20to%20improve%20performance%20on%0Anovel%20classes%20with%20different%20distributions%20compared%20to%20the%20seen%20labeled%20set.%0AAdditionally%2C%20recognizing%20the%20non-negligible%20influence%20of%20different%20backbones%0Aand%20pre-training%20strategies%20on%20the%20performance%20of%20the%20NCD%20methods%2C%20we%20build%20a%0Afair%20benchmark%20for%20future%20NCD%20research.%20Extensive%20experiments%20on%20three%20common%0Adatasets%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20module.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18140v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExclusive%2520Style%2520Removal%2520for%2520Cross%2520Domain%2520Novel%2520Class%2520Discovery%26entry.906535625%3DYicheng%2520Wang%2520and%2520Feng%2520Liu%2520and%2520Junmin%2520Liu%2520and%2520Zhen%2520Fang%2520and%2520Kai%2520Sun%26entry.1292438233%3D%2520%2520As%2520a%2520promising%2520field%2520in%2520open-world%2520learning%252C%2520%255Ctextit%257BNovel%2520Class%2520Discovery%257D%250A%2528NCD%2529%2520is%2520usually%2520a%2520task%2520to%2520cluster%2520unseen%2520novel%2520classes%2520in%2520an%2520unlabeled%2520set%250Abased%2520on%2520the%2520prior%2520knowledge%2520of%2520labeled%2520data%2520within%2520the%2520same%2520domain.%2520However%252C%250Athe%2520performance%2520of%2520existing%2520NCD%2520methods%2520could%2520be%2520severely%2520compromised%2520when%250Anovel%2520classes%2520are%2520sampled%2520from%2520a%2520different%2520distribution%2520with%2520the%2520labeled%2520ones.%250AIn%2520this%2520paper%252C%2520we%2520explore%2520and%2520establish%2520the%2520solvability%2520of%2520NCD%2520in%2520cross%2520domain%250Asetting%2520with%2520the%2520necessary%2520condition%2520that%2520style%2520information%2520must%2520be%2520removed.%250ABased%2520on%2520the%2520theoretical%2520analysis%252C%2520we%2520introduce%2520an%2520exclusive%2520style%2520removal%250Amodule%2520for%2520extracting%2520style%2520information%2520that%2520is%2520distinctive%2520from%2520the%2520baseline%250Afeatures%252C%2520thereby%2520facilitating%2520inference.%2520Moreover%252C%2520this%2520module%2520is%2520easy%2520to%250Aintegrate%2520with%2520other%2520NCD%2520methods%252C%2520acting%2520as%2520a%2520plug-in%2520to%2520improve%2520performance%2520on%250Anovel%2520classes%2520with%2520different%2520distributions%2520compared%2520to%2520the%2520seen%2520labeled%2520set.%250AAdditionally%252C%2520recognizing%2520the%2520non-negligible%2520influence%2520of%2520different%2520backbones%250Aand%2520pre-training%2520strategies%2520on%2520the%2520performance%2520of%2520the%2520NCD%2520methods%252C%2520we%2520build%2520a%250Afair%2520benchmark%2520for%2520future%2520NCD%2520research.%2520Extensive%2520experiments%2520on%2520three%2520common%250Adatasets%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520module.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18140v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exclusive%20Style%20Removal%20for%20Cross%20Domain%20Novel%20Class%20Discovery&entry.906535625=Yicheng%20Wang%20and%20Feng%20Liu%20and%20Junmin%20Liu%20and%20Zhen%20Fang%20and%20Kai%20Sun&entry.1292438233=%20%20As%20a%20promising%20field%20in%20open-world%20learning%2C%20%5Ctextit%7BNovel%20Class%20Discovery%7D%0A%28NCD%29%20is%20usually%20a%20task%20to%20cluster%20unseen%20novel%20classes%20in%20an%20unlabeled%20set%0Abased%20on%20the%20prior%20knowledge%20of%20labeled%20data%20within%20the%20same%20domain.%20However%2C%0Athe%20performance%20of%20existing%20NCD%20methods%20could%20be%20severely%20compromised%20when%0Anovel%20classes%20are%20sampled%20from%20a%20different%20distribution%20with%20the%20labeled%20ones.%0AIn%20this%20paper%2C%20we%20explore%20and%20establish%20the%20solvability%20of%20NCD%20in%20cross%20domain%0Asetting%20with%20the%20necessary%20condition%20that%20style%20information%20must%20be%20removed.%0ABased%20on%20the%20theoretical%20analysis%2C%20we%20introduce%20an%20exclusive%20style%20removal%0Amodule%20for%20extracting%20style%20information%20that%20is%20distinctive%20from%20the%20baseline%0Afeatures%2C%20thereby%20facilitating%20inference.%20Moreover%2C%20this%20module%20is%20easy%20to%0Aintegrate%20with%20other%20NCD%20methods%2C%20acting%20as%20a%20plug-in%20to%20improve%20performance%20on%0Anovel%20classes%20with%20different%20distributions%20compared%20to%20the%20seen%20labeled%20set.%0AAdditionally%2C%20recognizing%20the%20non-negligible%20influence%20of%20different%20backbones%0Aand%20pre-training%20strategies%20on%20the%20performance%20of%20the%20NCD%20methods%2C%20we%20build%20a%0Afair%20benchmark%20for%20future%20NCD%20research.%20Extensive%20experiments%20on%20three%20common%0Adatasets%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20module.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18140v2&entry.124074799=Read"},
{"title": "Riemannian Federated Learning via Averaging Gradient Stream", "author": "Zhenwei Huang and Wen Huang and Pratik Jawanpuria and Bamdev Mishra", "abstract": "  In recent years, federated learning has garnered significant attention as an\nefficient and privacy-preserving distributed learning paradigm. In the\nEuclidean setting, Federated Averaging (FedAvg) and its variants are a class of\nefficient algorithms for expected (empirical) risk minimization. This paper\ndevelops and analyzes a Riemannian Federated Averaging Gradient Stream\n(RFedAGS) algorithm, which is a generalization of FedAvg, to problems defined\non a Riemannian manifold. Under standard assumptions, the convergence rate of\nRFedAGS with fixed step sizes is proven to be sublinear for an approximate\nstationary solution. If decaying step sizes are used, the global convergence is\nestablished. Furthermore, assuming that the objective obeys the Riemannian\nPolyak-{\\L}ojasiewicz property, the optimal gaps generated by RFedAGS with\nfixed step size are linearly decreasing up to a tiny upper bound, meanwhile, if\ndecaying step sizes are used, then the gaps sublinearly vanish.\n  Numerical simulations conducted on synthetic and real-world data demonstrate\nthe performance of the proposed RFedAGS.\n", "link": "http://arxiv.org/abs/2409.07223v1", "date": "2024-09-11", "relevancy": 1.8859, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4806}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.47}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Riemannian%20Federated%20Learning%20via%20Averaging%20Gradient%20Stream&body=Title%3A%20Riemannian%20Federated%20Learning%20via%20Averaging%20Gradient%20Stream%0AAuthor%3A%20Zhenwei%20Huang%20and%20Wen%20Huang%20and%20Pratik%20Jawanpuria%20and%20Bamdev%20Mishra%0AAbstract%3A%20%20%20In%20recent%20years%2C%20federated%20learning%20has%20garnered%20significant%20attention%20as%20an%0Aefficient%20and%20privacy-preserving%20distributed%20learning%20paradigm.%20In%20the%0AEuclidean%20setting%2C%20Federated%20Averaging%20%28FedAvg%29%20and%20its%20variants%20are%20a%20class%20of%0Aefficient%20algorithms%20for%20expected%20%28empirical%29%20risk%20minimization.%20This%20paper%0Adevelops%20and%20analyzes%20a%20Riemannian%20Federated%20Averaging%20Gradient%20Stream%0A%28RFedAGS%29%20algorithm%2C%20which%20is%20a%20generalization%20of%20FedAvg%2C%20to%20problems%20defined%0Aon%20a%20Riemannian%20manifold.%20Under%20standard%20assumptions%2C%20the%20convergence%20rate%20of%0ARFedAGS%20with%20fixed%20step%20sizes%20is%20proven%20to%20be%20sublinear%20for%20an%20approximate%0Astationary%20solution.%20If%20decaying%20step%20sizes%20are%20used%2C%20the%20global%20convergence%20is%0Aestablished.%20Furthermore%2C%20assuming%20that%20the%20objective%20obeys%20the%20Riemannian%0APolyak-%7B%5CL%7Dojasiewicz%20property%2C%20the%20optimal%20gaps%20generated%20by%20RFedAGS%20with%0Afixed%20step%20size%20are%20linearly%20decreasing%20up%20to%20a%20tiny%20upper%20bound%2C%20meanwhile%2C%20if%0Adecaying%20step%20sizes%20are%20used%2C%20then%20the%20gaps%20sublinearly%20vanish.%0A%20%20Numerical%20simulations%20conducted%20on%20synthetic%20and%20real-world%20data%20demonstrate%0Athe%20performance%20of%20the%20proposed%20RFedAGS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07223v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRiemannian%2520Federated%2520Learning%2520via%2520Averaging%2520Gradient%2520Stream%26entry.906535625%3DZhenwei%2520Huang%2520and%2520Wen%2520Huang%2520and%2520Pratik%2520Jawanpuria%2520and%2520Bamdev%2520Mishra%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520federated%2520learning%2520has%2520garnered%2520significant%2520attention%2520as%2520an%250Aefficient%2520and%2520privacy-preserving%2520distributed%2520learning%2520paradigm.%2520In%2520the%250AEuclidean%2520setting%252C%2520Federated%2520Averaging%2520%2528FedAvg%2529%2520and%2520its%2520variants%2520are%2520a%2520class%2520of%250Aefficient%2520algorithms%2520for%2520expected%2520%2528empirical%2529%2520risk%2520minimization.%2520This%2520paper%250Adevelops%2520and%2520analyzes%2520a%2520Riemannian%2520Federated%2520Averaging%2520Gradient%2520Stream%250A%2528RFedAGS%2529%2520algorithm%252C%2520which%2520is%2520a%2520generalization%2520of%2520FedAvg%252C%2520to%2520problems%2520defined%250Aon%2520a%2520Riemannian%2520manifold.%2520Under%2520standard%2520assumptions%252C%2520the%2520convergence%2520rate%2520of%250ARFedAGS%2520with%2520fixed%2520step%2520sizes%2520is%2520proven%2520to%2520be%2520sublinear%2520for%2520an%2520approximate%250Astationary%2520solution.%2520If%2520decaying%2520step%2520sizes%2520are%2520used%252C%2520the%2520global%2520convergence%2520is%250Aestablished.%2520Furthermore%252C%2520assuming%2520that%2520the%2520objective%2520obeys%2520the%2520Riemannian%250APolyak-%257B%255CL%257Dojasiewicz%2520property%252C%2520the%2520optimal%2520gaps%2520generated%2520by%2520RFedAGS%2520with%250Afixed%2520step%2520size%2520are%2520linearly%2520decreasing%2520up%2520to%2520a%2520tiny%2520upper%2520bound%252C%2520meanwhile%252C%2520if%250Adecaying%2520step%2520sizes%2520are%2520used%252C%2520then%2520the%2520gaps%2520sublinearly%2520vanish.%250A%2520%2520Numerical%2520simulations%2520conducted%2520on%2520synthetic%2520and%2520real-world%2520data%2520demonstrate%250Athe%2520performance%2520of%2520the%2520proposed%2520RFedAGS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07223v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Riemannian%20Federated%20Learning%20via%20Averaging%20Gradient%20Stream&entry.906535625=Zhenwei%20Huang%20and%20Wen%20Huang%20and%20Pratik%20Jawanpuria%20and%20Bamdev%20Mishra&entry.1292438233=%20%20In%20recent%20years%2C%20federated%20learning%20has%20garnered%20significant%20attention%20as%20an%0Aefficient%20and%20privacy-preserving%20distributed%20learning%20paradigm.%20In%20the%0AEuclidean%20setting%2C%20Federated%20Averaging%20%28FedAvg%29%20and%20its%20variants%20are%20a%20class%20of%0Aefficient%20algorithms%20for%20expected%20%28empirical%29%20risk%20minimization.%20This%20paper%0Adevelops%20and%20analyzes%20a%20Riemannian%20Federated%20Averaging%20Gradient%20Stream%0A%28RFedAGS%29%20algorithm%2C%20which%20is%20a%20generalization%20of%20FedAvg%2C%20to%20problems%20defined%0Aon%20a%20Riemannian%20manifold.%20Under%20standard%20assumptions%2C%20the%20convergence%20rate%20of%0ARFedAGS%20with%20fixed%20step%20sizes%20is%20proven%20to%20be%20sublinear%20for%20an%20approximate%0Astationary%20solution.%20If%20decaying%20step%20sizes%20are%20used%2C%20the%20global%20convergence%20is%0Aestablished.%20Furthermore%2C%20assuming%20that%20the%20objective%20obeys%20the%20Riemannian%0APolyak-%7B%5CL%7Dojasiewicz%20property%2C%20the%20optimal%20gaps%20generated%20by%20RFedAGS%20with%0Afixed%20step%20size%20are%20linearly%20decreasing%20up%20to%20a%20tiny%20upper%20bound%2C%20meanwhile%2C%20if%0Adecaying%20step%20sizes%20are%20used%2C%20then%20the%20gaps%20sublinearly%20vanish.%0A%20%20Numerical%20simulations%20conducted%20on%20synthetic%20and%20real-world%20data%20demonstrate%0Athe%20performance%20of%20the%20proposed%20RFedAGS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07223v1&entry.124074799=Read"},
{"title": "The Human Factor in AI Red Teaming: Perspectives from Social and\n  Collaborative Computing", "author": "Alice Qian Zhang and Ryland Shaw and Jacy Reese Anthis and Ashlee Milton and Emily Tseng and Jina Suh and Lama Ahmad and Ram Shankar Siva Kumar and Julian Posada and Benjamin Shestakofsky and Sarah T. Roberts and Mary L. Gray", "abstract": "  Rapid progress in general-purpose AI has sparked significant interest in \"red\nteaming,\" a practice of adversarial testing originating in military and\ncybersecurity applications. AI red teaming raises many questions about the\nhuman factor, such as how red teamers are selected, biases and blindspots in\nhow tests are conducted, and harmful content's psychological effects on red\nteamers. A growing body of HCI and CSCW literature examines related\npractices-including data labeling, content moderation, and algorithmic\nauditing. However, few, if any have investigated red teaming itself. Future\nstudies may explore topics ranging from fairness to mental health and other\nareas of potential harm. We aim to facilitate a community of researchers and\npractitioners who can begin to meet these challenges with creativity,\ninnovation, and thoughtful reflection.\n", "link": "http://arxiv.org/abs/2407.07786v2", "date": "2024-09-11", "relevancy": 1.2603, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4435}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4188}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Human%20Factor%20in%20AI%20Red%20Teaming%3A%20Perspectives%20from%20Social%20and%0A%20%20Collaborative%20Computing&body=Title%3A%20The%20Human%20Factor%20in%20AI%20Red%20Teaming%3A%20Perspectives%20from%20Social%20and%0A%20%20Collaborative%20Computing%0AAuthor%3A%20Alice%20Qian%20Zhang%20and%20Ryland%20Shaw%20and%20Jacy%20Reese%20Anthis%20and%20Ashlee%20Milton%20and%20Emily%20Tseng%20and%20Jina%20Suh%20and%20Lama%20Ahmad%20and%20Ram%20Shankar%20Siva%20Kumar%20and%20Julian%20Posada%20and%20Benjamin%20Shestakofsky%20and%20Sarah%20T.%20Roberts%20and%20Mary%20L.%20Gray%0AAbstract%3A%20%20%20Rapid%20progress%20in%20general-purpose%20AI%20has%20sparked%20significant%20interest%20in%20%22red%0Ateaming%2C%22%20a%20practice%20of%20adversarial%20testing%20originating%20in%20military%20and%0Acybersecurity%20applications.%20AI%20red%20teaming%20raises%20many%20questions%20about%20the%0Ahuman%20factor%2C%20such%20as%20how%20red%20teamers%20are%20selected%2C%20biases%20and%20blindspots%20in%0Ahow%20tests%20are%20conducted%2C%20and%20harmful%20content%27s%20psychological%20effects%20on%20red%0Ateamers.%20A%20growing%20body%20of%20HCI%20and%20CSCW%20literature%20examines%20related%0Apractices-including%20data%20labeling%2C%20content%20moderation%2C%20and%20algorithmic%0Aauditing.%20However%2C%20few%2C%20if%20any%20have%20investigated%20red%20teaming%20itself.%20Future%0Astudies%20may%20explore%20topics%20ranging%20from%20fairness%20to%20mental%20health%20and%20other%0Aareas%20of%20potential%20harm.%20We%20aim%20to%20facilitate%20a%20community%20of%20researchers%20and%0Apractitioners%20who%20can%20begin%20to%20meet%20these%20challenges%20with%20creativity%2C%0Ainnovation%2C%20and%20thoughtful%20reflection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07786v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Human%2520Factor%2520in%2520AI%2520Red%2520Teaming%253A%2520Perspectives%2520from%2520Social%2520and%250A%2520%2520Collaborative%2520Computing%26entry.906535625%3DAlice%2520Qian%2520Zhang%2520and%2520Ryland%2520Shaw%2520and%2520Jacy%2520Reese%2520Anthis%2520and%2520Ashlee%2520Milton%2520and%2520Emily%2520Tseng%2520and%2520Jina%2520Suh%2520and%2520Lama%2520Ahmad%2520and%2520Ram%2520Shankar%2520Siva%2520Kumar%2520and%2520Julian%2520Posada%2520and%2520Benjamin%2520Shestakofsky%2520and%2520Sarah%2520T.%2520Roberts%2520and%2520Mary%2520L.%2520Gray%26entry.1292438233%3D%2520%2520Rapid%2520progress%2520in%2520general-purpose%2520AI%2520has%2520sparked%2520significant%2520interest%2520in%2520%2522red%250Ateaming%252C%2522%2520a%2520practice%2520of%2520adversarial%2520testing%2520originating%2520in%2520military%2520and%250Acybersecurity%2520applications.%2520AI%2520red%2520teaming%2520raises%2520many%2520questions%2520about%2520the%250Ahuman%2520factor%252C%2520such%2520as%2520how%2520red%2520teamers%2520are%2520selected%252C%2520biases%2520and%2520blindspots%2520in%250Ahow%2520tests%2520are%2520conducted%252C%2520and%2520harmful%2520content%2527s%2520psychological%2520effects%2520on%2520red%250Ateamers.%2520A%2520growing%2520body%2520of%2520HCI%2520and%2520CSCW%2520literature%2520examines%2520related%250Apractices-including%2520data%2520labeling%252C%2520content%2520moderation%252C%2520and%2520algorithmic%250Aauditing.%2520However%252C%2520few%252C%2520if%2520any%2520have%2520investigated%2520red%2520teaming%2520itself.%2520Future%250Astudies%2520may%2520explore%2520topics%2520ranging%2520from%2520fairness%2520to%2520mental%2520health%2520and%2520other%250Aareas%2520of%2520potential%2520harm.%2520We%2520aim%2520to%2520facilitate%2520a%2520community%2520of%2520researchers%2520and%250Apractitioners%2520who%2520can%2520begin%2520to%2520meet%2520these%2520challenges%2520with%2520creativity%252C%250Ainnovation%252C%2520and%2520thoughtful%2520reflection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07786v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Human%20Factor%20in%20AI%20Red%20Teaming%3A%20Perspectives%20from%20Social%20and%0A%20%20Collaborative%20Computing&entry.906535625=Alice%20Qian%20Zhang%20and%20Ryland%20Shaw%20and%20Jacy%20Reese%20Anthis%20and%20Ashlee%20Milton%20and%20Emily%20Tseng%20and%20Jina%20Suh%20and%20Lama%20Ahmad%20and%20Ram%20Shankar%20Siva%20Kumar%20and%20Julian%20Posada%20and%20Benjamin%20Shestakofsky%20and%20Sarah%20T.%20Roberts%20and%20Mary%20L.%20Gray&entry.1292438233=%20%20Rapid%20progress%20in%20general-purpose%20AI%20has%20sparked%20significant%20interest%20in%20%22red%0Ateaming%2C%22%20a%20practice%20of%20adversarial%20testing%20originating%20in%20military%20and%0Acybersecurity%20applications.%20AI%20red%20teaming%20raises%20many%20questions%20about%20the%0Ahuman%20factor%2C%20such%20as%20how%20red%20teamers%20are%20selected%2C%20biases%20and%20blindspots%20in%0Ahow%20tests%20are%20conducted%2C%20and%20harmful%20content%27s%20psychological%20effects%20on%20red%0Ateamers.%20A%20growing%20body%20of%20HCI%20and%20CSCW%20literature%20examines%20related%0Apractices-including%20data%20labeling%2C%20content%20moderation%2C%20and%20algorithmic%0Aauditing.%20However%2C%20few%2C%20if%20any%20have%20investigated%20red%20teaming%20itself.%20Future%0Astudies%20may%20explore%20topics%20ranging%20from%20fairness%20to%20mental%20health%20and%20other%0Aareas%20of%20potential%20harm.%20We%20aim%20to%20facilitate%20a%20community%20of%20researchers%20and%0Apractitioners%20who%20can%20begin%20to%20meet%20these%20challenges%20with%20creativity%2C%0Ainnovation%2C%20and%20thoughtful%20reflection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07786v2&entry.124074799=Read"},
{"title": "Enhancing Angular Resolution via Directionality Encoding and Geometric\n  Constraints in Brain Diffusion Tensor Imaging", "author": "Sheng Chen and Zihao Tang and Mariano Cabezas and Xinyi Wang and Arkiev D'Souza and Michael Barnett and Fernando Calamante and Weidong Cai and Chenyu Wang", "abstract": "  Diffusion-weighted imaging (DWI) is a type of Magnetic Resonance Imaging\n(MRI) technique sensitised to the diffusivity of water molecules, offering the\ncapability to inspect tissue microstructures and is the only in-vivo method to\nreconstruct white matter fiber tracts non-invasively. The DWI signal can be\nanalysed with the diffusion tensor imaging (DTI) model to estimate the\ndirectionality of water diffusion within voxels. Several scalar metrics,\nincluding axial diffusivity (AD), mean diffusivity (MD), radial diffusivity\n(RD), and fractional anisotropy (FA), can be further derived from DTI to\nquantitatively summarise the microstructural integrity of brain tissue. These\nscalar metrics have played an important role in understanding the organisation\nand health of brain tissue at a microscopic level in clinical studies. However,\nreliable DTI metrics rely on DWI acquisitions with high gradient directions,\nwhich often go beyond the commonly used clinical protocols. To enhance the\nutility of clinically acquired DWI and save scanning time for robust DTI\nanalysis, this work proposes DirGeo-DTI, a deep learning-based method to\nestimate reliable DTI metrics even from a set of DWIs acquired with the minimum\ntheoretical number (6) of gradient directions. DirGeo-DTI leverages directional\nencoding and geometric constraints to facilitate the training process. Two\npublic DWI datasets were used for evaluation, demonstrating the effectiveness\nof the proposed method. Extensive experimental results show that the proposed\nmethod achieves the best performance compared to existing DTI enhancement\nmethods and potentially reveals further clinical insights with routine clinical\nDWI scans.\n", "link": "http://arxiv.org/abs/2409.07186v1", "date": "2024-09-11", "relevancy": 1.5659, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5466}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5156}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Angular%20Resolution%20via%20Directionality%20Encoding%20and%20Geometric%0A%20%20Constraints%20in%20Brain%20Diffusion%20Tensor%20Imaging&body=Title%3A%20Enhancing%20Angular%20Resolution%20via%20Directionality%20Encoding%20and%20Geometric%0A%20%20Constraints%20in%20Brain%20Diffusion%20Tensor%20Imaging%0AAuthor%3A%20Sheng%20Chen%20and%20Zihao%20Tang%20and%20Mariano%20Cabezas%20and%20Xinyi%20Wang%20and%20Arkiev%20D%27Souza%20and%20Michael%20Barnett%20and%20Fernando%20Calamante%20and%20Weidong%20Cai%20and%20Chenyu%20Wang%0AAbstract%3A%20%20%20Diffusion-weighted%20imaging%20%28DWI%29%20is%20a%20type%20of%20Magnetic%20Resonance%20Imaging%0A%28MRI%29%20technique%20sensitised%20to%20the%20diffusivity%20of%20water%20molecules%2C%20offering%20the%0Acapability%20to%20inspect%20tissue%20microstructures%20and%20is%20the%20only%20in-vivo%20method%20to%0Areconstruct%20white%20matter%20fiber%20tracts%20non-invasively.%20The%20DWI%20signal%20can%20be%0Aanalysed%20with%20the%20diffusion%20tensor%20imaging%20%28DTI%29%20model%20to%20estimate%20the%0Adirectionality%20of%20water%20diffusion%20within%20voxels.%20Several%20scalar%20metrics%2C%0Aincluding%20axial%20diffusivity%20%28AD%29%2C%20mean%20diffusivity%20%28MD%29%2C%20radial%20diffusivity%0A%28RD%29%2C%20and%20fractional%20anisotropy%20%28FA%29%2C%20can%20be%20further%20derived%20from%20DTI%20to%0Aquantitatively%20summarise%20the%20microstructural%20integrity%20of%20brain%20tissue.%20These%0Ascalar%20metrics%20have%20played%20an%20important%20role%20in%20understanding%20the%20organisation%0Aand%20health%20of%20brain%20tissue%20at%20a%20microscopic%20level%20in%20clinical%20studies.%20However%2C%0Areliable%20DTI%20metrics%20rely%20on%20DWI%20acquisitions%20with%20high%20gradient%20directions%2C%0Awhich%20often%20go%20beyond%20the%20commonly%20used%20clinical%20protocols.%20To%20enhance%20the%0Autility%20of%20clinically%20acquired%20DWI%20and%20save%20scanning%20time%20for%20robust%20DTI%0Aanalysis%2C%20this%20work%20proposes%20DirGeo-DTI%2C%20a%20deep%20learning-based%20method%20to%0Aestimate%20reliable%20DTI%20metrics%20even%20from%20a%20set%20of%20DWIs%20acquired%20with%20the%20minimum%0Atheoretical%20number%20%286%29%20of%20gradient%20directions.%20DirGeo-DTI%20leverages%20directional%0Aencoding%20and%20geometric%20constraints%20to%20facilitate%20the%20training%20process.%20Two%0Apublic%20DWI%20datasets%20were%20used%20for%20evaluation%2C%20demonstrating%20the%20effectiveness%0Aof%20the%20proposed%20method.%20Extensive%20experimental%20results%20show%20that%20the%20proposed%0Amethod%20achieves%20the%20best%20performance%20compared%20to%20existing%20DTI%20enhancement%0Amethods%20and%20potentially%20reveals%20further%20clinical%20insights%20with%20routine%20clinical%0ADWI%20scans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07186v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Angular%2520Resolution%2520via%2520Directionality%2520Encoding%2520and%2520Geometric%250A%2520%2520Constraints%2520in%2520Brain%2520Diffusion%2520Tensor%2520Imaging%26entry.906535625%3DSheng%2520Chen%2520and%2520Zihao%2520Tang%2520and%2520Mariano%2520Cabezas%2520and%2520Xinyi%2520Wang%2520and%2520Arkiev%2520D%2527Souza%2520and%2520Michael%2520Barnett%2520and%2520Fernando%2520Calamante%2520and%2520Weidong%2520Cai%2520and%2520Chenyu%2520Wang%26entry.1292438233%3D%2520%2520Diffusion-weighted%2520imaging%2520%2528DWI%2529%2520is%2520a%2520type%2520of%2520Magnetic%2520Resonance%2520Imaging%250A%2528MRI%2529%2520technique%2520sensitised%2520to%2520the%2520diffusivity%2520of%2520water%2520molecules%252C%2520offering%2520the%250Acapability%2520to%2520inspect%2520tissue%2520microstructures%2520and%2520is%2520the%2520only%2520in-vivo%2520method%2520to%250Areconstruct%2520white%2520matter%2520fiber%2520tracts%2520non-invasively.%2520The%2520DWI%2520signal%2520can%2520be%250Aanalysed%2520with%2520the%2520diffusion%2520tensor%2520imaging%2520%2528DTI%2529%2520model%2520to%2520estimate%2520the%250Adirectionality%2520of%2520water%2520diffusion%2520within%2520voxels.%2520Several%2520scalar%2520metrics%252C%250Aincluding%2520axial%2520diffusivity%2520%2528AD%2529%252C%2520mean%2520diffusivity%2520%2528MD%2529%252C%2520radial%2520diffusivity%250A%2528RD%2529%252C%2520and%2520fractional%2520anisotropy%2520%2528FA%2529%252C%2520can%2520be%2520further%2520derived%2520from%2520DTI%2520to%250Aquantitatively%2520summarise%2520the%2520microstructural%2520integrity%2520of%2520brain%2520tissue.%2520These%250Ascalar%2520metrics%2520have%2520played%2520an%2520important%2520role%2520in%2520understanding%2520the%2520organisation%250Aand%2520health%2520of%2520brain%2520tissue%2520at%2520a%2520microscopic%2520level%2520in%2520clinical%2520studies.%2520However%252C%250Areliable%2520DTI%2520metrics%2520rely%2520on%2520DWI%2520acquisitions%2520with%2520high%2520gradient%2520directions%252C%250Awhich%2520often%2520go%2520beyond%2520the%2520commonly%2520used%2520clinical%2520protocols.%2520To%2520enhance%2520the%250Autility%2520of%2520clinically%2520acquired%2520DWI%2520and%2520save%2520scanning%2520time%2520for%2520robust%2520DTI%250Aanalysis%252C%2520this%2520work%2520proposes%2520DirGeo-DTI%252C%2520a%2520deep%2520learning-based%2520method%2520to%250Aestimate%2520reliable%2520DTI%2520metrics%2520even%2520from%2520a%2520set%2520of%2520DWIs%2520acquired%2520with%2520the%2520minimum%250Atheoretical%2520number%2520%25286%2529%2520of%2520gradient%2520directions.%2520DirGeo-DTI%2520leverages%2520directional%250Aencoding%2520and%2520geometric%2520constraints%2520to%2520facilitate%2520the%2520training%2520process.%2520Two%250Apublic%2520DWI%2520datasets%2520were%2520used%2520for%2520evaluation%252C%2520demonstrating%2520the%2520effectiveness%250Aof%2520the%2520proposed%2520method.%2520Extensive%2520experimental%2520results%2520show%2520that%2520the%2520proposed%250Amethod%2520achieves%2520the%2520best%2520performance%2520compared%2520to%2520existing%2520DTI%2520enhancement%250Amethods%2520and%2520potentially%2520reveals%2520further%2520clinical%2520insights%2520with%2520routine%2520clinical%250ADWI%2520scans.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07186v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Angular%20Resolution%20via%20Directionality%20Encoding%20and%20Geometric%0A%20%20Constraints%20in%20Brain%20Diffusion%20Tensor%20Imaging&entry.906535625=Sheng%20Chen%20and%20Zihao%20Tang%20and%20Mariano%20Cabezas%20and%20Xinyi%20Wang%20and%20Arkiev%20D%27Souza%20and%20Michael%20Barnett%20and%20Fernando%20Calamante%20and%20Weidong%20Cai%20and%20Chenyu%20Wang&entry.1292438233=%20%20Diffusion-weighted%20imaging%20%28DWI%29%20is%20a%20type%20of%20Magnetic%20Resonance%20Imaging%0A%28MRI%29%20technique%20sensitised%20to%20the%20diffusivity%20of%20water%20molecules%2C%20offering%20the%0Acapability%20to%20inspect%20tissue%20microstructures%20and%20is%20the%20only%20in-vivo%20method%20to%0Areconstruct%20white%20matter%20fiber%20tracts%20non-invasively.%20The%20DWI%20signal%20can%20be%0Aanalysed%20with%20the%20diffusion%20tensor%20imaging%20%28DTI%29%20model%20to%20estimate%20the%0Adirectionality%20of%20water%20diffusion%20within%20voxels.%20Several%20scalar%20metrics%2C%0Aincluding%20axial%20diffusivity%20%28AD%29%2C%20mean%20diffusivity%20%28MD%29%2C%20radial%20diffusivity%0A%28RD%29%2C%20and%20fractional%20anisotropy%20%28FA%29%2C%20can%20be%20further%20derived%20from%20DTI%20to%0Aquantitatively%20summarise%20the%20microstructural%20integrity%20of%20brain%20tissue.%20These%0Ascalar%20metrics%20have%20played%20an%20important%20role%20in%20understanding%20the%20organisation%0Aand%20health%20of%20brain%20tissue%20at%20a%20microscopic%20level%20in%20clinical%20studies.%20However%2C%0Areliable%20DTI%20metrics%20rely%20on%20DWI%20acquisitions%20with%20high%20gradient%20directions%2C%0Awhich%20often%20go%20beyond%20the%20commonly%20used%20clinical%20protocols.%20To%20enhance%20the%0Autility%20of%20clinically%20acquired%20DWI%20and%20save%20scanning%20time%20for%20robust%20DTI%0Aanalysis%2C%20this%20work%20proposes%20DirGeo-DTI%2C%20a%20deep%20learning-based%20method%20to%0Aestimate%20reliable%20DTI%20metrics%20even%20from%20a%20set%20of%20DWIs%20acquired%20with%20the%20minimum%0Atheoretical%20number%20%286%29%20of%20gradient%20directions.%20DirGeo-DTI%20leverages%20directional%0Aencoding%20and%20geometric%20constraints%20to%20facilitate%20the%20training%20process.%20Two%0Apublic%20DWI%20datasets%20were%20used%20for%20evaluation%2C%20demonstrating%20the%20effectiveness%0Aof%20the%20proposed%20method.%20Extensive%20experimental%20results%20show%20that%20the%20proposed%0Amethod%20achieves%20the%20best%20performance%20compared%20to%20existing%20DTI%20enhancement%0Amethods%20and%20potentially%20reveals%20further%20clinical%20insights%20with%20routine%20clinical%0ADWI%20scans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07186v1&entry.124074799=Read"},
{"title": "Realistic and Efficient Face Swapping: A Unified Approach with Diffusion\n  Models", "author": "Sanoojan Baliah and Qinliang Lin and Shengcai Liao and Xiaodan Liang and Muhammad Haris Khan", "abstract": "  Despite promising progress in face swapping task, realistic swapped images\nremain elusive, often marred by artifacts, particularly in scenarios involving\nhigh pose variation, color differences, and occlusion. To address these issues,\nwe propose a novel approach that better harnesses diffusion models for\nface-swapping by making following core contributions. (a) We propose to\nre-frame the face-swapping task as a self-supervised, train-time inpainting\nproblem, enhancing the identity transfer while blending with the target image.\n(b) We introduce a multi-step Denoising Diffusion Implicit Model (DDIM)\nsampling during training, reinforcing identity and perceptual similarities. (c)\nThird, we introduce CLIP feature disentanglement to extract pose, expression,\nand lighting information from the target image, improving fidelity. (d)\nFurther, we introduce a mask shuffling technique during inpainting training,\nwhich allows us to create a so-called universal model for swapping, with an\nadditional feature of head swapping. Ours can swap hair and even accessories,\nbeyond traditional face swapping. Unlike prior works reliant on multiple\noff-the-shelf models, ours is a relatively unified approach and so it is\nresilient to errors in other off-the-shelf models. Extensive experiments on\nFFHQ and CelebA datasets validate the efficacy and robustness of our approach,\nshowcasing high-fidelity, realistic face-swapping with minimal inference time.\nOur code is available at https://github.com/Sanoojan/REFace.\n", "link": "http://arxiv.org/abs/2409.07269v1", "date": "2024-09-11", "relevancy": 1.7669, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5924}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5892}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Realistic%20and%20Efficient%20Face%20Swapping%3A%20A%20Unified%20Approach%20with%20Diffusion%0A%20%20Models&body=Title%3A%20Realistic%20and%20Efficient%20Face%20Swapping%3A%20A%20Unified%20Approach%20with%20Diffusion%0A%20%20Models%0AAuthor%3A%20Sanoojan%20Baliah%20and%20Qinliang%20Lin%20and%20Shengcai%20Liao%20and%20Xiaodan%20Liang%20and%20Muhammad%20Haris%20Khan%0AAbstract%3A%20%20%20Despite%20promising%20progress%20in%20face%20swapping%20task%2C%20realistic%20swapped%20images%0Aremain%20elusive%2C%20often%20marred%20by%20artifacts%2C%20particularly%20in%20scenarios%20involving%0Ahigh%20pose%20variation%2C%20color%20differences%2C%20and%20occlusion.%20To%20address%20these%20issues%2C%0Awe%20propose%20a%20novel%20approach%20that%20better%20harnesses%20diffusion%20models%20for%0Aface-swapping%20by%20making%20following%20core%20contributions.%20%28a%29%20We%20propose%20to%0Are-frame%20the%20face-swapping%20task%20as%20a%20self-supervised%2C%20train-time%20inpainting%0Aproblem%2C%20enhancing%20the%20identity%20transfer%20while%20blending%20with%20the%20target%20image.%0A%28b%29%20We%20introduce%20a%20multi-step%20Denoising%20Diffusion%20Implicit%20Model%20%28DDIM%29%0Asampling%20during%20training%2C%20reinforcing%20identity%20and%20perceptual%20similarities.%20%28c%29%0AThird%2C%20we%20introduce%20CLIP%20feature%20disentanglement%20to%20extract%20pose%2C%20expression%2C%0Aand%20lighting%20information%20from%20the%20target%20image%2C%20improving%20fidelity.%20%28d%29%0AFurther%2C%20we%20introduce%20a%20mask%20shuffling%20technique%20during%20inpainting%20training%2C%0Awhich%20allows%20us%20to%20create%20a%20so-called%20universal%20model%20for%20swapping%2C%20with%20an%0Aadditional%20feature%20of%20head%20swapping.%20Ours%20can%20swap%20hair%20and%20even%20accessories%2C%0Abeyond%20traditional%20face%20swapping.%20Unlike%20prior%20works%20reliant%20on%20multiple%0Aoff-the-shelf%20models%2C%20ours%20is%20a%20relatively%20unified%20approach%20and%20so%20it%20is%0Aresilient%20to%20errors%20in%20other%20off-the-shelf%20models.%20Extensive%20experiments%20on%0AFFHQ%20and%20CelebA%20datasets%20validate%20the%20efficacy%20and%20robustness%20of%20our%20approach%2C%0Ashowcasing%20high-fidelity%2C%20realistic%20face-swapping%20with%20minimal%20inference%20time.%0AOur%20code%20is%20available%20at%20https%3A//github.com/Sanoojan/REFace.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07269v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealistic%2520and%2520Efficient%2520Face%2520Swapping%253A%2520A%2520Unified%2520Approach%2520with%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DSanoojan%2520Baliah%2520and%2520Qinliang%2520Lin%2520and%2520Shengcai%2520Liao%2520and%2520Xiaodan%2520Liang%2520and%2520Muhammad%2520Haris%2520Khan%26entry.1292438233%3D%2520%2520Despite%2520promising%2520progress%2520in%2520face%2520swapping%2520task%252C%2520realistic%2520swapped%2520images%250Aremain%2520elusive%252C%2520often%2520marred%2520by%2520artifacts%252C%2520particularly%2520in%2520scenarios%2520involving%250Ahigh%2520pose%2520variation%252C%2520color%2520differences%252C%2520and%2520occlusion.%2520To%2520address%2520these%2520issues%252C%250Awe%2520propose%2520a%2520novel%2520approach%2520that%2520better%2520harnesses%2520diffusion%2520models%2520for%250Aface-swapping%2520by%2520making%2520following%2520core%2520contributions.%2520%2528a%2529%2520We%2520propose%2520to%250Are-frame%2520the%2520face-swapping%2520task%2520as%2520a%2520self-supervised%252C%2520train-time%2520inpainting%250Aproblem%252C%2520enhancing%2520the%2520identity%2520transfer%2520while%2520blending%2520with%2520the%2520target%2520image.%250A%2528b%2529%2520We%2520introduce%2520a%2520multi-step%2520Denoising%2520Diffusion%2520Implicit%2520Model%2520%2528DDIM%2529%250Asampling%2520during%2520training%252C%2520reinforcing%2520identity%2520and%2520perceptual%2520similarities.%2520%2528c%2529%250AThird%252C%2520we%2520introduce%2520CLIP%2520feature%2520disentanglement%2520to%2520extract%2520pose%252C%2520expression%252C%250Aand%2520lighting%2520information%2520from%2520the%2520target%2520image%252C%2520improving%2520fidelity.%2520%2528d%2529%250AFurther%252C%2520we%2520introduce%2520a%2520mask%2520shuffling%2520technique%2520during%2520inpainting%2520training%252C%250Awhich%2520allows%2520us%2520to%2520create%2520a%2520so-called%2520universal%2520model%2520for%2520swapping%252C%2520with%2520an%250Aadditional%2520feature%2520of%2520head%2520swapping.%2520Ours%2520can%2520swap%2520hair%2520and%2520even%2520accessories%252C%250Abeyond%2520traditional%2520face%2520swapping.%2520Unlike%2520prior%2520works%2520reliant%2520on%2520multiple%250Aoff-the-shelf%2520models%252C%2520ours%2520is%2520a%2520relatively%2520unified%2520approach%2520and%2520so%2520it%2520is%250Aresilient%2520to%2520errors%2520in%2520other%2520off-the-shelf%2520models.%2520Extensive%2520experiments%2520on%250AFFHQ%2520and%2520CelebA%2520datasets%2520validate%2520the%2520efficacy%2520and%2520robustness%2520of%2520our%2520approach%252C%250Ashowcasing%2520high-fidelity%252C%2520realistic%2520face-swapping%2520with%2520minimal%2520inference%2520time.%250AOur%2520code%2520is%2520available%2520at%2520https%253A//github.com/Sanoojan/REFace.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07269v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Realistic%20and%20Efficient%20Face%20Swapping%3A%20A%20Unified%20Approach%20with%20Diffusion%0A%20%20Models&entry.906535625=Sanoojan%20Baliah%20and%20Qinliang%20Lin%20and%20Shengcai%20Liao%20and%20Xiaodan%20Liang%20and%20Muhammad%20Haris%20Khan&entry.1292438233=%20%20Despite%20promising%20progress%20in%20face%20swapping%20task%2C%20realistic%20swapped%20images%0Aremain%20elusive%2C%20often%20marred%20by%20artifacts%2C%20particularly%20in%20scenarios%20involving%0Ahigh%20pose%20variation%2C%20color%20differences%2C%20and%20occlusion.%20To%20address%20these%20issues%2C%0Awe%20propose%20a%20novel%20approach%20that%20better%20harnesses%20diffusion%20models%20for%0Aface-swapping%20by%20making%20following%20core%20contributions.%20%28a%29%20We%20propose%20to%0Are-frame%20the%20face-swapping%20task%20as%20a%20self-supervised%2C%20train-time%20inpainting%0Aproblem%2C%20enhancing%20the%20identity%20transfer%20while%20blending%20with%20the%20target%20image.%0A%28b%29%20We%20introduce%20a%20multi-step%20Denoising%20Diffusion%20Implicit%20Model%20%28DDIM%29%0Asampling%20during%20training%2C%20reinforcing%20identity%20and%20perceptual%20similarities.%20%28c%29%0AThird%2C%20we%20introduce%20CLIP%20feature%20disentanglement%20to%20extract%20pose%2C%20expression%2C%0Aand%20lighting%20information%20from%20the%20target%20image%2C%20improving%20fidelity.%20%28d%29%0AFurther%2C%20we%20introduce%20a%20mask%20shuffling%20technique%20during%20inpainting%20training%2C%0Awhich%20allows%20us%20to%20create%20a%20so-called%20universal%20model%20for%20swapping%2C%20with%20an%0Aadditional%20feature%20of%20head%20swapping.%20Ours%20can%20swap%20hair%20and%20even%20accessories%2C%0Abeyond%20traditional%20face%20swapping.%20Unlike%20prior%20works%20reliant%20on%20multiple%0Aoff-the-shelf%20models%2C%20ours%20is%20a%20relatively%20unified%20approach%20and%20so%20it%20is%0Aresilient%20to%20errors%20in%20other%20off-the-shelf%20models.%20Extensive%20experiments%20on%0AFFHQ%20and%20CelebA%20datasets%20validate%20the%20efficacy%20and%20robustness%20of%20our%20approach%2C%0Ashowcasing%20high-fidelity%2C%20realistic%20face-swapping%20with%20minimal%20inference%20time.%0AOur%20code%20is%20available%20at%20https%3A//github.com/Sanoojan/REFace.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07269v1&entry.124074799=Read"},
{"title": "Learning Robotic Manipulation Policies from Point Clouds with\n  Conditional Flow Matching", "author": "Eugenio Chisari and Nick Heppert and Max Argus and Tim Welschehold and Thomas Brox and Abhinav Valada", "abstract": "  Learning from expert demonstrations is a promising approach for training\nrobotic manipulation policies from limited data. However, imitation learning\nalgorithms require a number of design choices ranging from the input modality,\ntraining objective, and 6-DoF end-effector pose representation. Diffusion-based\nmethods have gained popularity as they enable predicting long-horizon\ntrajectories and handle multimodal action distributions. Recently, Conditional\nFlow Matching (CFM) (or Rectified Flow) has been proposed as a more flexible\ngeneralization of diffusion models. In this paper, we investigate the\napplication of CFM in the context of robotic policy learning and specifically\nstudy the interplay with the other design choices required to build an\nimitation learning algorithm. We show that CFM gives the best performance when\ncombined with point cloud input observations. Additionally, we study the\nfeasibility of a CFM formulation on the SO(3) manifold and evaluate its\nsuitability with a simplified example. We perform extensive experiments on\nRLBench which demonstrate that our proposed PointFlowMatch approach achieves a\nstate-of-the-art average success rate of 67.8% over eight tasks, double the\nperformance of the next best method.\n", "link": "http://arxiv.org/abs/2409.07343v1", "date": "2024-09-11", "relevancy": 1.1428, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5966}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5826}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Robotic%20Manipulation%20Policies%20from%20Point%20Clouds%20with%0A%20%20Conditional%20Flow%20Matching&body=Title%3A%20Learning%20Robotic%20Manipulation%20Policies%20from%20Point%20Clouds%20with%0A%20%20Conditional%20Flow%20Matching%0AAuthor%3A%20Eugenio%20Chisari%20and%20Nick%20Heppert%20and%20Max%20Argus%20and%20Tim%20Welschehold%20and%20Thomas%20Brox%20and%20Abhinav%20Valada%0AAbstract%3A%20%20%20Learning%20from%20expert%20demonstrations%20is%20a%20promising%20approach%20for%20training%0Arobotic%20manipulation%20policies%20from%20limited%20data.%20However%2C%20imitation%20learning%0Aalgorithms%20require%20a%20number%20of%20design%20choices%20ranging%20from%20the%20input%20modality%2C%0Atraining%20objective%2C%20and%206-DoF%20end-effector%20pose%20representation.%20Diffusion-based%0Amethods%20have%20gained%20popularity%20as%20they%20enable%20predicting%20long-horizon%0Atrajectories%20and%20handle%20multimodal%20action%20distributions.%20Recently%2C%20Conditional%0AFlow%20Matching%20%28CFM%29%20%28or%20Rectified%20Flow%29%20has%20been%20proposed%20as%20a%20more%20flexible%0Ageneralization%20of%20diffusion%20models.%20In%20this%20paper%2C%20we%20investigate%20the%0Aapplication%20of%20CFM%20in%20the%20context%20of%20robotic%20policy%20learning%20and%20specifically%0Astudy%20the%20interplay%20with%20the%20other%20design%20choices%20required%20to%20build%20an%0Aimitation%20learning%20algorithm.%20We%20show%20that%20CFM%20gives%20the%20best%20performance%20when%0Acombined%20with%20point%20cloud%20input%20observations.%20Additionally%2C%20we%20study%20the%0Afeasibility%20of%20a%20CFM%20formulation%20on%20the%20SO%283%29%20manifold%20and%20evaluate%20its%0Asuitability%20with%20a%20simplified%20example.%20We%20perform%20extensive%20experiments%20on%0ARLBench%20which%20demonstrate%20that%20our%20proposed%20PointFlowMatch%20approach%20achieves%20a%0Astate-of-the-art%20average%20success%20rate%20of%2067.8%25%20over%20eight%20tasks%2C%20double%20the%0Aperformance%20of%20the%20next%20best%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07343v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Robotic%2520Manipulation%2520Policies%2520from%2520Point%2520Clouds%2520with%250A%2520%2520Conditional%2520Flow%2520Matching%26entry.906535625%3DEugenio%2520Chisari%2520and%2520Nick%2520Heppert%2520and%2520Max%2520Argus%2520and%2520Tim%2520Welschehold%2520and%2520Thomas%2520Brox%2520and%2520Abhinav%2520Valada%26entry.1292438233%3D%2520%2520Learning%2520from%2520expert%2520demonstrations%2520is%2520a%2520promising%2520approach%2520for%2520training%250Arobotic%2520manipulation%2520policies%2520from%2520limited%2520data.%2520However%252C%2520imitation%2520learning%250Aalgorithms%2520require%2520a%2520number%2520of%2520design%2520choices%2520ranging%2520from%2520the%2520input%2520modality%252C%250Atraining%2520objective%252C%2520and%25206-DoF%2520end-effector%2520pose%2520representation.%2520Diffusion-based%250Amethods%2520have%2520gained%2520popularity%2520as%2520they%2520enable%2520predicting%2520long-horizon%250Atrajectories%2520and%2520handle%2520multimodal%2520action%2520distributions.%2520Recently%252C%2520Conditional%250AFlow%2520Matching%2520%2528CFM%2529%2520%2528or%2520Rectified%2520Flow%2529%2520has%2520been%2520proposed%2520as%2520a%2520more%2520flexible%250Ageneralization%2520of%2520diffusion%2520models.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%250Aapplication%2520of%2520CFM%2520in%2520the%2520context%2520of%2520robotic%2520policy%2520learning%2520and%2520specifically%250Astudy%2520the%2520interplay%2520with%2520the%2520other%2520design%2520choices%2520required%2520to%2520build%2520an%250Aimitation%2520learning%2520algorithm.%2520We%2520show%2520that%2520CFM%2520gives%2520the%2520best%2520performance%2520when%250Acombined%2520with%2520point%2520cloud%2520input%2520observations.%2520Additionally%252C%2520we%2520study%2520the%250Afeasibility%2520of%2520a%2520CFM%2520formulation%2520on%2520the%2520SO%25283%2529%2520manifold%2520and%2520evaluate%2520its%250Asuitability%2520with%2520a%2520simplified%2520example.%2520We%2520perform%2520extensive%2520experiments%2520on%250ARLBench%2520which%2520demonstrate%2520that%2520our%2520proposed%2520PointFlowMatch%2520approach%2520achieves%2520a%250Astate-of-the-art%2520average%2520success%2520rate%2520of%252067.8%2525%2520over%2520eight%2520tasks%252C%2520double%2520the%250Aperformance%2520of%2520the%2520next%2520best%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07343v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Robotic%20Manipulation%20Policies%20from%20Point%20Clouds%20with%0A%20%20Conditional%20Flow%20Matching&entry.906535625=Eugenio%20Chisari%20and%20Nick%20Heppert%20and%20Max%20Argus%20and%20Tim%20Welschehold%20and%20Thomas%20Brox%20and%20Abhinav%20Valada&entry.1292438233=%20%20Learning%20from%20expert%20demonstrations%20is%20a%20promising%20approach%20for%20training%0Arobotic%20manipulation%20policies%20from%20limited%20data.%20However%2C%20imitation%20learning%0Aalgorithms%20require%20a%20number%20of%20design%20choices%20ranging%20from%20the%20input%20modality%2C%0Atraining%20objective%2C%20and%206-DoF%20end-effector%20pose%20representation.%20Diffusion-based%0Amethods%20have%20gained%20popularity%20as%20they%20enable%20predicting%20long-horizon%0Atrajectories%20and%20handle%20multimodal%20action%20distributions.%20Recently%2C%20Conditional%0AFlow%20Matching%20%28CFM%29%20%28or%20Rectified%20Flow%29%20has%20been%20proposed%20as%20a%20more%20flexible%0Ageneralization%20of%20diffusion%20models.%20In%20this%20paper%2C%20we%20investigate%20the%0Aapplication%20of%20CFM%20in%20the%20context%20of%20robotic%20policy%20learning%20and%20specifically%0Astudy%20the%20interplay%20with%20the%20other%20design%20choices%20required%20to%20build%20an%0Aimitation%20learning%20algorithm.%20We%20show%20that%20CFM%20gives%20the%20best%20performance%20when%0Acombined%20with%20point%20cloud%20input%20observations.%20Additionally%2C%20we%20study%20the%0Afeasibility%20of%20a%20CFM%20formulation%20on%20the%20SO%283%29%20manifold%20and%20evaluate%20its%0Asuitability%20with%20a%20simplified%20example.%20We%20perform%20extensive%20experiments%20on%0ARLBench%20which%20demonstrate%20that%20our%20proposed%20PointFlowMatch%20approach%20achieves%20a%0Astate-of-the-art%20average%20success%20rate%20of%2067.8%25%20over%20eight%20tasks%2C%20double%20the%0Aperformance%20of%20the%20next%20best%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07343v1&entry.124074799=Read"},
{"title": "Alignment of Diffusion Models: Fundamentals, Challenges, and Future", "author": "Buhua Liu and Shitong Shao and Bao Li and Lichen Bai and Haoyi Xiong and James Kwok and Sumi Helal and Zeke Xie", "abstract": "  Diffusion models have emerged as the leading paradigm in generative modeling,\nexcelling in various applications. Despite their success, these models often\nmisalign with human intentions, generating outputs that may not match text\nprompts or possess desired properties. Inspired by the success of alignment in\ntuning large language models, recent studies have investigated aligning\ndiffusion models with human expectations and preferences. This work mainly\nreviews alignment of diffusion models, covering advancements in fundamentals of\nalignment, alignment techniques of diffusion models, preference benchmarks, and\nevaluation for diffusion models. Moreover, we discuss key perspectives on\ncurrent challenges and promising future directions on solving the remaining\nchallenges in alignment of diffusion models. To the best of our knowledge, our\nwork is the first comprehensive review paper for researchers and engineers to\ncomprehend, practice, and research alignment of diffusion models.\n", "link": "http://arxiv.org/abs/2409.07253v1", "date": "2024-09-11", "relevancy": 1.0816, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5632}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5339}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Alignment%20of%20Diffusion%20Models%3A%20Fundamentals%2C%20Challenges%2C%20and%20Future&body=Title%3A%20Alignment%20of%20Diffusion%20Models%3A%20Fundamentals%2C%20Challenges%2C%20and%20Future%0AAuthor%3A%20Buhua%20Liu%20and%20Shitong%20Shao%20and%20Bao%20Li%20and%20Lichen%20Bai%20and%20Haoyi%20Xiong%20and%20James%20Kwok%20and%20Sumi%20Helal%20and%20Zeke%20Xie%0AAbstract%3A%20%20%20Diffusion%20models%20have%20emerged%20as%20the%20leading%20paradigm%20in%20generative%20modeling%2C%0Aexcelling%20in%20various%20applications.%20Despite%20their%20success%2C%20these%20models%20often%0Amisalign%20with%20human%20intentions%2C%20generating%20outputs%20that%20may%20not%20match%20text%0Aprompts%20or%20possess%20desired%20properties.%20Inspired%20by%20the%20success%20of%20alignment%20in%0Atuning%20large%20language%20models%2C%20recent%20studies%20have%20investigated%20aligning%0Adiffusion%20models%20with%20human%20expectations%20and%20preferences.%20This%20work%20mainly%0Areviews%20alignment%20of%20diffusion%20models%2C%20covering%20advancements%20in%20fundamentals%20of%0Aalignment%2C%20alignment%20techniques%20of%20diffusion%20models%2C%20preference%20benchmarks%2C%20and%0Aevaluation%20for%20diffusion%20models.%20Moreover%2C%20we%20discuss%20key%20perspectives%20on%0Acurrent%20challenges%20and%20promising%20future%20directions%20on%20solving%20the%20remaining%0Achallenges%20in%20alignment%20of%20diffusion%20models.%20To%20the%20best%20of%20our%20knowledge%2C%20our%0Awork%20is%20the%20first%20comprehensive%20review%20paper%20for%20researchers%20and%20engineers%20to%0Acomprehend%2C%20practice%2C%20and%20research%20alignment%20of%20diffusion%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07253v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlignment%2520of%2520Diffusion%2520Models%253A%2520Fundamentals%252C%2520Challenges%252C%2520and%2520Future%26entry.906535625%3DBuhua%2520Liu%2520and%2520Shitong%2520Shao%2520and%2520Bao%2520Li%2520and%2520Lichen%2520Bai%2520and%2520Haoyi%2520Xiong%2520and%2520James%2520Kwok%2520and%2520Sumi%2520Helal%2520and%2520Zeke%2520Xie%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520emerged%2520as%2520the%2520leading%2520paradigm%2520in%2520generative%2520modeling%252C%250Aexcelling%2520in%2520various%2520applications.%2520Despite%2520their%2520success%252C%2520these%2520models%2520often%250Amisalign%2520with%2520human%2520intentions%252C%2520generating%2520outputs%2520that%2520may%2520not%2520match%2520text%250Aprompts%2520or%2520possess%2520desired%2520properties.%2520Inspired%2520by%2520the%2520success%2520of%2520alignment%2520in%250Atuning%2520large%2520language%2520models%252C%2520recent%2520studies%2520have%2520investigated%2520aligning%250Adiffusion%2520models%2520with%2520human%2520expectations%2520and%2520preferences.%2520This%2520work%2520mainly%250Areviews%2520alignment%2520of%2520diffusion%2520models%252C%2520covering%2520advancements%2520in%2520fundamentals%2520of%250Aalignment%252C%2520alignment%2520techniques%2520of%2520diffusion%2520models%252C%2520preference%2520benchmarks%252C%2520and%250Aevaluation%2520for%2520diffusion%2520models.%2520Moreover%252C%2520we%2520discuss%2520key%2520perspectives%2520on%250Acurrent%2520challenges%2520and%2520promising%2520future%2520directions%2520on%2520solving%2520the%2520remaining%250Achallenges%2520in%2520alignment%2520of%2520diffusion%2520models.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520our%250Awork%2520is%2520the%2520first%2520comprehensive%2520review%2520paper%2520for%2520researchers%2520and%2520engineers%2520to%250Acomprehend%252C%2520practice%252C%2520and%2520research%2520alignment%2520of%2520diffusion%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07253v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alignment%20of%20Diffusion%20Models%3A%20Fundamentals%2C%20Challenges%2C%20and%20Future&entry.906535625=Buhua%20Liu%20and%20Shitong%20Shao%20and%20Bao%20Li%20and%20Lichen%20Bai%20and%20Haoyi%20Xiong%20and%20James%20Kwok%20and%20Sumi%20Helal%20and%20Zeke%20Xie&entry.1292438233=%20%20Diffusion%20models%20have%20emerged%20as%20the%20leading%20paradigm%20in%20generative%20modeling%2C%0Aexcelling%20in%20various%20applications.%20Despite%20their%20success%2C%20these%20models%20often%0Amisalign%20with%20human%20intentions%2C%20generating%20outputs%20that%20may%20not%20match%20text%0Aprompts%20or%20possess%20desired%20properties.%20Inspired%20by%20the%20success%20of%20alignment%20in%0Atuning%20large%20language%20models%2C%20recent%20studies%20have%20investigated%20aligning%0Adiffusion%20models%20with%20human%20expectations%20and%20preferences.%20This%20work%20mainly%0Areviews%20alignment%20of%20diffusion%20models%2C%20covering%20advancements%20in%20fundamentals%20of%0Aalignment%2C%20alignment%20techniques%20of%20diffusion%20models%2C%20preference%20benchmarks%2C%20and%0Aevaluation%20for%20diffusion%20models.%20Moreover%2C%20we%20discuss%20key%20perspectives%20on%0Acurrent%20challenges%20and%20promising%20future%20directions%20on%20solving%20the%20remaining%0Achallenges%20in%20alignment%20of%20diffusion%20models.%20To%20the%20best%20of%20our%20knowledge%2C%20our%0Awork%20is%20the%20first%20comprehensive%20review%20paper%20for%20researchers%20and%20engineers%20to%0Acomprehend%2C%20practice%2C%20and%20research%20alignment%20of%20diffusion%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07253v1&entry.124074799=Read"},
{"title": "Introducing Perturb-ability Score (PS) to Enhance Robustness Against\n  Evasion Adversarial Attacks on ML-NIDS", "author": "Mohamed elShehaby and Ashraf Matrawy", "abstract": "  This paper proposes a novel Perturb-ability Score (PS) that can be used to\nidentify Network Intrusion Detection Systems (NIDS) features that can be easily\nmanipulated by attackers in the problem-space. We demonstrate that using PS to\nselect only non-perturb-able features for ML-based NIDS maintains detection\nperformance while enhancing robustness against adversarial attacks.\n", "link": "http://arxiv.org/abs/2409.07448v1", "date": "2024-09-11", "relevancy": 1.2523, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4184}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4176}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Introducing%20Perturb-ability%20Score%20%28PS%29%20to%20Enhance%20Robustness%20Against%0A%20%20Evasion%20Adversarial%20Attacks%20on%20ML-NIDS&body=Title%3A%20Introducing%20Perturb-ability%20Score%20%28PS%29%20to%20Enhance%20Robustness%20Against%0A%20%20Evasion%20Adversarial%20Attacks%20on%20ML-NIDS%0AAuthor%3A%20Mohamed%20elShehaby%20and%20Ashraf%20Matrawy%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20novel%20Perturb-ability%20Score%20%28PS%29%20that%20can%20be%20used%20to%0Aidentify%20Network%20Intrusion%20Detection%20Systems%20%28NIDS%29%20features%20that%20can%20be%20easily%0Amanipulated%20by%20attackers%20in%20the%20problem-space.%20We%20demonstrate%20that%20using%20PS%20to%0Aselect%20only%20non-perturb-able%20features%20for%20ML-based%20NIDS%20maintains%20detection%0Aperformance%20while%20enhancing%20robustness%20against%20adversarial%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07448v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntroducing%2520Perturb-ability%2520Score%2520%2528PS%2529%2520to%2520Enhance%2520Robustness%2520Against%250A%2520%2520Evasion%2520Adversarial%2520Attacks%2520on%2520ML-NIDS%26entry.906535625%3DMohamed%2520elShehaby%2520and%2520Ashraf%2520Matrawy%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520novel%2520Perturb-ability%2520Score%2520%2528PS%2529%2520that%2520can%2520be%2520used%2520to%250Aidentify%2520Network%2520Intrusion%2520Detection%2520Systems%2520%2528NIDS%2529%2520features%2520that%2520can%2520be%2520easily%250Amanipulated%2520by%2520attackers%2520in%2520the%2520problem-space.%2520We%2520demonstrate%2520that%2520using%2520PS%2520to%250Aselect%2520only%2520non-perturb-able%2520features%2520for%2520ML-based%2520NIDS%2520maintains%2520detection%250Aperformance%2520while%2520enhancing%2520robustness%2520against%2520adversarial%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07448v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Introducing%20Perturb-ability%20Score%20%28PS%29%20to%20Enhance%20Robustness%20Against%0A%20%20Evasion%20Adversarial%20Attacks%20on%20ML-NIDS&entry.906535625=Mohamed%20elShehaby%20and%20Ashraf%20Matrawy&entry.1292438233=%20%20This%20paper%20proposes%20a%20novel%20Perturb-ability%20Score%20%28PS%29%20that%20can%20be%20used%20to%0Aidentify%20Network%20Intrusion%20Detection%20Systems%20%28NIDS%29%20features%20that%20can%20be%20easily%0Amanipulated%20by%20attackers%20in%20the%20problem-space.%20We%20demonstrate%20that%20using%20PS%20to%0Aselect%20only%20non-perturb-able%20features%20for%20ML-based%20NIDS%20maintains%20detection%0Aperformance%20while%20enhancing%20robustness%20against%20adversarial%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07448v1&entry.124074799=Read"},
{"title": "Towards Fairer Health Recommendations: finding informative unbiased\n  samples via Word Sense Disambiguation", "author": "Gavin Butts and Pegah Emdad and Jethro Lee and Shannon Song and Chiman Salavati and Willmar Sosa Diaz and Shiri Dori-Hacohen and Fabricio Murai", "abstract": "  There have been growing concerns around high-stake applications that rely on\nmodels trained with biased data, which consequently produce biased predictions,\noften harming the most vulnerable. In particular, biased medical data could\ncause health-related applications and recommender systems to create outputs\nthat jeopardize patient care and widen disparities in health outcomes. A recent\nframework titled Fairness via AI posits that, instead of attempting to correct\nmodel biases, researchers must focus on their root causes by using AI to debias\ndata. Inspired by this framework, we tackle bias detection in medical curricula\nusing NLP models, including LLMs, and evaluate them on a gold standard dataset\ncontaining 4,105 excerpts annotated by medical experts for bias from a large\ncorpus. We build on previous work by coauthors which augments the set of\nnegative samples with non-annotated text containing social identifier terms.\nHowever, some of these terms, especially those related to race and ethnicity,\ncan carry different meanings (e.g., \"white matter of spinal cord\"). To address\nthis issue, we propose the use of Word Sense Disambiguation models to refine\ndataset quality by removing irrelevant sentences. We then evaluate fine-tuned\nvariations of BERT models as well as GPT models with zero- and few-shot\nprompting. We found LLMs, considered SOTA on many NLP tasks, unsuitable for\nbias detection, while fine-tuned BERT models generally perform well across all\nevaluated metrics.\n", "link": "http://arxiv.org/abs/2409.07424v1", "date": "2024-09-11", "relevancy": 1.4349, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5096}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4707}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Fairer%20Health%20Recommendations%3A%20finding%20informative%20unbiased%0A%20%20samples%20via%20Word%20Sense%20Disambiguation&body=Title%3A%20Towards%20Fairer%20Health%20Recommendations%3A%20finding%20informative%20unbiased%0A%20%20samples%20via%20Word%20Sense%20Disambiguation%0AAuthor%3A%20Gavin%20Butts%20and%20Pegah%20Emdad%20and%20Jethro%20Lee%20and%20Shannon%20Song%20and%20Chiman%20Salavati%20and%20Willmar%20Sosa%20Diaz%20and%20Shiri%20Dori-Hacohen%20and%20Fabricio%20Murai%0AAbstract%3A%20%20%20There%20have%20been%20growing%20concerns%20around%20high-stake%20applications%20that%20rely%20on%0Amodels%20trained%20with%20biased%20data%2C%20which%20consequently%20produce%20biased%20predictions%2C%0Aoften%20harming%20the%20most%20vulnerable.%20In%20particular%2C%20biased%20medical%20data%20could%0Acause%20health-related%20applications%20and%20recommender%20systems%20to%20create%20outputs%0Athat%20jeopardize%20patient%20care%20and%20widen%20disparities%20in%20health%20outcomes.%20A%20recent%0Aframework%20titled%20Fairness%20via%20AI%20posits%20that%2C%20instead%20of%20attempting%20to%20correct%0Amodel%20biases%2C%20researchers%20must%20focus%20on%20their%20root%20causes%20by%20using%20AI%20to%20debias%0Adata.%20Inspired%20by%20this%20framework%2C%20we%20tackle%20bias%20detection%20in%20medical%20curricula%0Ausing%20NLP%20models%2C%20including%20LLMs%2C%20and%20evaluate%20them%20on%20a%20gold%20standard%20dataset%0Acontaining%204%2C105%20excerpts%20annotated%20by%20medical%20experts%20for%20bias%20from%20a%20large%0Acorpus.%20We%20build%20on%20previous%20work%20by%20coauthors%20which%20augments%20the%20set%20of%0Anegative%20samples%20with%20non-annotated%20text%20containing%20social%20identifier%20terms.%0AHowever%2C%20some%20of%20these%20terms%2C%20especially%20those%20related%20to%20race%20and%20ethnicity%2C%0Acan%20carry%20different%20meanings%20%28e.g.%2C%20%22white%20matter%20of%20spinal%20cord%22%29.%20To%20address%0Athis%20issue%2C%20we%20propose%20the%20use%20of%20Word%20Sense%20Disambiguation%20models%20to%20refine%0Adataset%20quality%20by%20removing%20irrelevant%20sentences.%20We%20then%20evaluate%20fine-tuned%0Avariations%20of%20BERT%20models%20as%20well%20as%20GPT%20models%20with%20zero-%20and%20few-shot%0Aprompting.%20We%20found%20LLMs%2C%20considered%20SOTA%20on%20many%20NLP%20tasks%2C%20unsuitable%20for%0Abias%20detection%2C%20while%20fine-tuned%20BERT%20models%20generally%20perform%20well%20across%20all%0Aevaluated%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07424v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Fairer%2520Health%2520Recommendations%253A%2520finding%2520informative%2520unbiased%250A%2520%2520samples%2520via%2520Word%2520Sense%2520Disambiguation%26entry.906535625%3DGavin%2520Butts%2520and%2520Pegah%2520Emdad%2520and%2520Jethro%2520Lee%2520and%2520Shannon%2520Song%2520and%2520Chiman%2520Salavati%2520and%2520Willmar%2520Sosa%2520Diaz%2520and%2520Shiri%2520Dori-Hacohen%2520and%2520Fabricio%2520Murai%26entry.1292438233%3D%2520%2520There%2520have%2520been%2520growing%2520concerns%2520around%2520high-stake%2520applications%2520that%2520rely%2520on%250Amodels%2520trained%2520with%2520biased%2520data%252C%2520which%2520consequently%2520produce%2520biased%2520predictions%252C%250Aoften%2520harming%2520the%2520most%2520vulnerable.%2520In%2520particular%252C%2520biased%2520medical%2520data%2520could%250Acause%2520health-related%2520applications%2520and%2520recommender%2520systems%2520to%2520create%2520outputs%250Athat%2520jeopardize%2520patient%2520care%2520and%2520widen%2520disparities%2520in%2520health%2520outcomes.%2520A%2520recent%250Aframework%2520titled%2520Fairness%2520via%2520AI%2520posits%2520that%252C%2520instead%2520of%2520attempting%2520to%2520correct%250Amodel%2520biases%252C%2520researchers%2520must%2520focus%2520on%2520their%2520root%2520causes%2520by%2520using%2520AI%2520to%2520debias%250Adata.%2520Inspired%2520by%2520this%2520framework%252C%2520we%2520tackle%2520bias%2520detection%2520in%2520medical%2520curricula%250Ausing%2520NLP%2520models%252C%2520including%2520LLMs%252C%2520and%2520evaluate%2520them%2520on%2520a%2520gold%2520standard%2520dataset%250Acontaining%25204%252C105%2520excerpts%2520annotated%2520by%2520medical%2520experts%2520for%2520bias%2520from%2520a%2520large%250Acorpus.%2520We%2520build%2520on%2520previous%2520work%2520by%2520coauthors%2520which%2520augments%2520the%2520set%2520of%250Anegative%2520samples%2520with%2520non-annotated%2520text%2520containing%2520social%2520identifier%2520terms.%250AHowever%252C%2520some%2520of%2520these%2520terms%252C%2520especially%2520those%2520related%2520to%2520race%2520and%2520ethnicity%252C%250Acan%2520carry%2520different%2520meanings%2520%2528e.g.%252C%2520%2522white%2520matter%2520of%2520spinal%2520cord%2522%2529.%2520To%2520address%250Athis%2520issue%252C%2520we%2520propose%2520the%2520use%2520of%2520Word%2520Sense%2520Disambiguation%2520models%2520to%2520refine%250Adataset%2520quality%2520by%2520removing%2520irrelevant%2520sentences.%2520We%2520then%2520evaluate%2520fine-tuned%250Avariations%2520of%2520BERT%2520models%2520as%2520well%2520as%2520GPT%2520models%2520with%2520zero-%2520and%2520few-shot%250Aprompting.%2520We%2520found%2520LLMs%252C%2520considered%2520SOTA%2520on%2520many%2520NLP%2520tasks%252C%2520unsuitable%2520for%250Abias%2520detection%252C%2520while%2520fine-tuned%2520BERT%2520models%2520generally%2520perform%2520well%2520across%2520all%250Aevaluated%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07424v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Fairer%20Health%20Recommendations%3A%20finding%20informative%20unbiased%0A%20%20samples%20via%20Word%20Sense%20Disambiguation&entry.906535625=Gavin%20Butts%20and%20Pegah%20Emdad%20and%20Jethro%20Lee%20and%20Shannon%20Song%20and%20Chiman%20Salavati%20and%20Willmar%20Sosa%20Diaz%20and%20Shiri%20Dori-Hacohen%20and%20Fabricio%20Murai&entry.1292438233=%20%20There%20have%20been%20growing%20concerns%20around%20high-stake%20applications%20that%20rely%20on%0Amodels%20trained%20with%20biased%20data%2C%20which%20consequently%20produce%20biased%20predictions%2C%0Aoften%20harming%20the%20most%20vulnerable.%20In%20particular%2C%20biased%20medical%20data%20could%0Acause%20health-related%20applications%20and%20recommender%20systems%20to%20create%20outputs%0Athat%20jeopardize%20patient%20care%20and%20widen%20disparities%20in%20health%20outcomes.%20A%20recent%0Aframework%20titled%20Fairness%20via%20AI%20posits%20that%2C%20instead%20of%20attempting%20to%20correct%0Amodel%20biases%2C%20researchers%20must%20focus%20on%20their%20root%20causes%20by%20using%20AI%20to%20debias%0Adata.%20Inspired%20by%20this%20framework%2C%20we%20tackle%20bias%20detection%20in%20medical%20curricula%0Ausing%20NLP%20models%2C%20including%20LLMs%2C%20and%20evaluate%20them%20on%20a%20gold%20standard%20dataset%0Acontaining%204%2C105%20excerpts%20annotated%20by%20medical%20experts%20for%20bias%20from%20a%20large%0Acorpus.%20We%20build%20on%20previous%20work%20by%20coauthors%20which%20augments%20the%20set%20of%0Anegative%20samples%20with%20non-annotated%20text%20containing%20social%20identifier%20terms.%0AHowever%2C%20some%20of%20these%20terms%2C%20especially%20those%20related%20to%20race%20and%20ethnicity%2C%0Acan%20carry%20different%20meanings%20%28e.g.%2C%20%22white%20matter%20of%20spinal%20cord%22%29.%20To%20address%0Athis%20issue%2C%20we%20propose%20the%20use%20of%20Word%20Sense%20Disambiguation%20models%20to%20refine%0Adataset%20quality%20by%20removing%20irrelevant%20sentences.%20We%20then%20evaluate%20fine-tuned%0Avariations%20of%20BERT%20models%20as%20well%20as%20GPT%20models%20with%20zero-%20and%20few-shot%0Aprompting.%20We%20found%20LLMs%2C%20considered%20SOTA%20on%20many%20NLP%20tasks%2C%20unsuitable%20for%0Abias%20detection%2C%20while%20fine-tuned%20BERT%20models%20generally%20perform%20well%20across%20all%0Aevaluated%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07424v1&entry.124074799=Read"},
{"title": "Approximation and generalization properties of the random projection\n  classification method", "author": "Mireille Boutin and Evzenie Coupkova", "abstract": "  The generalization gap of a classifier is related to the complexity of the\nset of functions among which the classifier is chosen. We study a family of\nlow-complexity classifiers consisting of thresholding a random one-dimensional\nfeature. The feature is obtained by projecting the data on a random line after\nembedding it into a higher-dimensional space parametrized by monomials of order\nup to k. More specifically, the extended data is projected n-times and the best\nclassifier among those n, based on its performance on training data, is chosen.\nWe show that this type of classifier is extremely flexible as, given full\nknowledge of the class conditional densities, under mild conditions, the error\nof these classifiers would converge to the optimal (Bayes) error as k and n go\nto infinity. We also bound the generalization gap of the random classifiers. In\ngeneral, these bounds are better than those for any classifier with VC\ndimension greater than O(ln n). In particular, the bounds imply that, unless\nthe number of projections n is extremely large, the generalization gap of the\nrandom projection approach is significantly smaller than that of a linear\nclassifier in the extended space. Thus, for certain classification problems\n(e.g., those with a large Rashomon ratio), there is a potntially large gain in\ngeneralization properties by selecting parameters at random, rather than\nselecting the best one amongst the class.\n", "link": "http://arxiv.org/abs/2108.06339v4", "date": "2024-09-11", "relevancy": 1.8129, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4846}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4512}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Approximation%20and%20generalization%20properties%20of%20the%20random%20projection%0A%20%20classification%20method&body=Title%3A%20Approximation%20and%20generalization%20properties%20of%20the%20random%20projection%0A%20%20classification%20method%0AAuthor%3A%20Mireille%20Boutin%20and%20Evzenie%20Coupkova%0AAbstract%3A%20%20%20The%20generalization%20gap%20of%20a%20classifier%20is%20related%20to%20the%20complexity%20of%20the%0Aset%20of%20functions%20among%20which%20the%20classifier%20is%20chosen.%20We%20study%20a%20family%20of%0Alow-complexity%20classifiers%20consisting%20of%20thresholding%20a%20random%20one-dimensional%0Afeature.%20The%20feature%20is%20obtained%20by%20projecting%20the%20data%20on%20a%20random%20line%20after%0Aembedding%20it%20into%20a%20higher-dimensional%20space%20parametrized%20by%20monomials%20of%20order%0Aup%20to%20k.%20More%20specifically%2C%20the%20extended%20data%20is%20projected%20n-times%20and%20the%20best%0Aclassifier%20among%20those%20n%2C%20based%20on%20its%20performance%20on%20training%20data%2C%20is%20chosen.%0AWe%20show%20that%20this%20type%20of%20classifier%20is%20extremely%20flexible%20as%2C%20given%20full%0Aknowledge%20of%20the%20class%20conditional%20densities%2C%20under%20mild%20conditions%2C%20the%20error%0Aof%20these%20classifiers%20would%20converge%20to%20the%20optimal%20%28Bayes%29%20error%20as%20k%20and%20n%20go%0Ato%20infinity.%20We%20also%20bound%20the%20generalization%20gap%20of%20the%20random%20classifiers.%20In%0Ageneral%2C%20these%20bounds%20are%20better%20than%20those%20for%20any%20classifier%20with%20VC%0Adimension%20greater%20than%20O%28ln%20n%29.%20In%20particular%2C%20the%20bounds%20imply%20that%2C%20unless%0Athe%20number%20of%20projections%20n%20is%20extremely%20large%2C%20the%20generalization%20gap%20of%20the%0Arandom%20projection%20approach%20is%20significantly%20smaller%20than%20that%20of%20a%20linear%0Aclassifier%20in%20the%20extended%20space.%20Thus%2C%20for%20certain%20classification%20problems%0A%28e.g.%2C%20those%20with%20a%20large%20Rashomon%20ratio%29%2C%20there%20is%20a%20potntially%20large%20gain%20in%0Ageneralization%20properties%20by%20selecting%20parameters%20at%20random%2C%20rather%20than%0Aselecting%20the%20best%20one%20amongst%20the%20class.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2108.06339v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApproximation%2520and%2520generalization%2520properties%2520of%2520the%2520random%2520projection%250A%2520%2520classification%2520method%26entry.906535625%3DMireille%2520Boutin%2520and%2520Evzenie%2520Coupkova%26entry.1292438233%3D%2520%2520The%2520generalization%2520gap%2520of%2520a%2520classifier%2520is%2520related%2520to%2520the%2520complexity%2520of%2520the%250Aset%2520of%2520functions%2520among%2520which%2520the%2520classifier%2520is%2520chosen.%2520We%2520study%2520a%2520family%2520of%250Alow-complexity%2520classifiers%2520consisting%2520of%2520thresholding%2520a%2520random%2520one-dimensional%250Afeature.%2520The%2520feature%2520is%2520obtained%2520by%2520projecting%2520the%2520data%2520on%2520a%2520random%2520line%2520after%250Aembedding%2520it%2520into%2520a%2520higher-dimensional%2520space%2520parametrized%2520by%2520monomials%2520of%2520order%250Aup%2520to%2520k.%2520More%2520specifically%252C%2520the%2520extended%2520data%2520is%2520projected%2520n-times%2520and%2520the%2520best%250Aclassifier%2520among%2520those%2520n%252C%2520based%2520on%2520its%2520performance%2520on%2520training%2520data%252C%2520is%2520chosen.%250AWe%2520show%2520that%2520this%2520type%2520of%2520classifier%2520is%2520extremely%2520flexible%2520as%252C%2520given%2520full%250Aknowledge%2520of%2520the%2520class%2520conditional%2520densities%252C%2520under%2520mild%2520conditions%252C%2520the%2520error%250Aof%2520these%2520classifiers%2520would%2520converge%2520to%2520the%2520optimal%2520%2528Bayes%2529%2520error%2520as%2520k%2520and%2520n%2520go%250Ato%2520infinity.%2520We%2520also%2520bound%2520the%2520generalization%2520gap%2520of%2520the%2520random%2520classifiers.%2520In%250Ageneral%252C%2520these%2520bounds%2520are%2520better%2520than%2520those%2520for%2520any%2520classifier%2520with%2520VC%250Adimension%2520greater%2520than%2520O%2528ln%2520n%2529.%2520In%2520particular%252C%2520the%2520bounds%2520imply%2520that%252C%2520unless%250Athe%2520number%2520of%2520projections%2520n%2520is%2520extremely%2520large%252C%2520the%2520generalization%2520gap%2520of%2520the%250Arandom%2520projection%2520approach%2520is%2520significantly%2520smaller%2520than%2520that%2520of%2520a%2520linear%250Aclassifier%2520in%2520the%2520extended%2520space.%2520Thus%252C%2520for%2520certain%2520classification%2520problems%250A%2528e.g.%252C%2520those%2520with%2520a%2520large%2520Rashomon%2520ratio%2529%252C%2520there%2520is%2520a%2520potntially%2520large%2520gain%2520in%250Ageneralization%2520properties%2520by%2520selecting%2520parameters%2520at%2520random%252C%2520rather%2520than%250Aselecting%2520the%2520best%2520one%2520amongst%2520the%2520class.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2108.06339v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Approximation%20and%20generalization%20properties%20of%20the%20random%20projection%0A%20%20classification%20method&entry.906535625=Mireille%20Boutin%20and%20Evzenie%20Coupkova&entry.1292438233=%20%20The%20generalization%20gap%20of%20a%20classifier%20is%20related%20to%20the%20complexity%20of%20the%0Aset%20of%20functions%20among%20which%20the%20classifier%20is%20chosen.%20We%20study%20a%20family%20of%0Alow-complexity%20classifiers%20consisting%20of%20thresholding%20a%20random%20one-dimensional%0Afeature.%20The%20feature%20is%20obtained%20by%20projecting%20the%20data%20on%20a%20random%20line%20after%0Aembedding%20it%20into%20a%20higher-dimensional%20space%20parametrized%20by%20monomials%20of%20order%0Aup%20to%20k.%20More%20specifically%2C%20the%20extended%20data%20is%20projected%20n-times%20and%20the%20best%0Aclassifier%20among%20those%20n%2C%20based%20on%20its%20performance%20on%20training%20data%2C%20is%20chosen.%0AWe%20show%20that%20this%20type%20of%20classifier%20is%20extremely%20flexible%20as%2C%20given%20full%0Aknowledge%20of%20the%20class%20conditional%20densities%2C%20under%20mild%20conditions%2C%20the%20error%0Aof%20these%20classifiers%20would%20converge%20to%20the%20optimal%20%28Bayes%29%20error%20as%20k%20and%20n%20go%0Ato%20infinity.%20We%20also%20bound%20the%20generalization%20gap%20of%20the%20random%20classifiers.%20In%0Ageneral%2C%20these%20bounds%20are%20better%20than%20those%20for%20any%20classifier%20with%20VC%0Adimension%20greater%20than%20O%28ln%20n%29.%20In%20particular%2C%20the%20bounds%20imply%20that%2C%20unless%0Athe%20number%20of%20projections%20n%20is%20extremely%20large%2C%20the%20generalization%20gap%20of%20the%0Arandom%20projection%20approach%20is%20significantly%20smaller%20than%20that%20of%20a%20linear%0Aclassifier%20in%20the%20extended%20space.%20Thus%2C%20for%20certain%20classification%20problems%0A%28e.g.%2C%20those%20with%20a%20large%20Rashomon%20ratio%29%2C%20there%20is%20a%20potntially%20large%20gain%20in%0Ageneralization%20properties%20by%20selecting%20parameters%20at%20random%2C%20rather%20than%0Aselecting%20the%20best%20one%20amongst%20the%20class.%0A&entry.1838667208=http%3A//arxiv.org/abs/2108.06339v4&entry.124074799=Read"},
{"title": "A Machine Learning Based Approach for Statistical Analysis of Detonation\n  Cells from Soot Foils", "author": "Vansh Sharma and Michael Ullman and Venkat Raman", "abstract": "  This study presents a novel algorithm based on machine learning (ML) for the\nprecise segmentation and measurement of detonation cells from soot foil images,\naddressing the limitations of manual and primitive edge detection methods\nprevalent in the field. Using advances in cellular biology segmentation models,\nthe proposed algorithm is designed to accurately extract cellular patterns\nwithout a training procedure or dataset, which is a significant challenge in\ndetonation research. The algorithm's performance was validated using a series\nof test cases that mimic experimental and numerical detonation studies. The\nresults demonstrated consistent accuracy, with errors remaining within 10%,\neven in complex cases. The algorithm effectively captured key cell metrics such\nas cell area and span, revealing trends across different soot foil samples with\nuniform to highly irregular cellular structures. Although the model proved\nrobust, challenges remain in segmenting and analyzing highly complex or\nirregular cellular patterns. This work highlights the broad applicability and\npotential of the algorithm to advance the understanding of detonation wave\ndynamics.\n", "link": "http://arxiv.org/abs/2409.06466v2", "date": "2024-09-11", "relevancy": 1.3425, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4577}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4452}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Machine%20Learning%20Based%20Approach%20for%20Statistical%20Analysis%20of%20Detonation%0A%20%20Cells%20from%20Soot%20Foils&body=Title%3A%20A%20Machine%20Learning%20Based%20Approach%20for%20Statistical%20Analysis%20of%20Detonation%0A%20%20Cells%20from%20Soot%20Foils%0AAuthor%3A%20Vansh%20Sharma%20and%20Michael%20Ullman%20and%20Venkat%20Raman%0AAbstract%3A%20%20%20This%20study%20presents%20a%20novel%20algorithm%20based%20on%20machine%20learning%20%28ML%29%20for%20the%0Aprecise%20segmentation%20and%20measurement%20of%20detonation%20cells%20from%20soot%20foil%20images%2C%0Aaddressing%20the%20limitations%20of%20manual%20and%20primitive%20edge%20detection%20methods%0Aprevalent%20in%20the%20field.%20Using%20advances%20in%20cellular%20biology%20segmentation%20models%2C%0Athe%20proposed%20algorithm%20is%20designed%20to%20accurately%20extract%20cellular%20patterns%0Awithout%20a%20training%20procedure%20or%20dataset%2C%20which%20is%20a%20significant%20challenge%20in%0Adetonation%20research.%20The%20algorithm%27s%20performance%20was%20validated%20using%20a%20series%0Aof%20test%20cases%20that%20mimic%20experimental%20and%20numerical%20detonation%20studies.%20The%0Aresults%20demonstrated%20consistent%20accuracy%2C%20with%20errors%20remaining%20within%2010%25%2C%0Aeven%20in%20complex%20cases.%20The%20algorithm%20effectively%20captured%20key%20cell%20metrics%20such%0Aas%20cell%20area%20and%20span%2C%20revealing%20trends%20across%20different%20soot%20foil%20samples%20with%0Auniform%20to%20highly%20irregular%20cellular%20structures.%20Although%20the%20model%20proved%0Arobust%2C%20challenges%20remain%20in%20segmenting%20and%20analyzing%20highly%20complex%20or%0Airregular%20cellular%20patterns.%20This%20work%20highlights%20the%20broad%20applicability%20and%0Apotential%20of%20the%20algorithm%20to%20advance%20the%20understanding%20of%20detonation%20wave%0Adynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06466v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Machine%2520Learning%2520Based%2520Approach%2520for%2520Statistical%2520Analysis%2520of%2520Detonation%250A%2520%2520Cells%2520from%2520Soot%2520Foils%26entry.906535625%3DVansh%2520Sharma%2520and%2520Michael%2520Ullman%2520and%2520Venkat%2520Raman%26entry.1292438233%3D%2520%2520This%2520study%2520presents%2520a%2520novel%2520algorithm%2520based%2520on%2520machine%2520learning%2520%2528ML%2529%2520for%2520the%250Aprecise%2520segmentation%2520and%2520measurement%2520of%2520detonation%2520cells%2520from%2520soot%2520foil%2520images%252C%250Aaddressing%2520the%2520limitations%2520of%2520manual%2520and%2520primitive%2520edge%2520detection%2520methods%250Aprevalent%2520in%2520the%2520field.%2520Using%2520advances%2520in%2520cellular%2520biology%2520segmentation%2520models%252C%250Athe%2520proposed%2520algorithm%2520is%2520designed%2520to%2520accurately%2520extract%2520cellular%2520patterns%250Awithout%2520a%2520training%2520procedure%2520or%2520dataset%252C%2520which%2520is%2520a%2520significant%2520challenge%2520in%250Adetonation%2520research.%2520The%2520algorithm%2527s%2520performance%2520was%2520validated%2520using%2520a%2520series%250Aof%2520test%2520cases%2520that%2520mimic%2520experimental%2520and%2520numerical%2520detonation%2520studies.%2520The%250Aresults%2520demonstrated%2520consistent%2520accuracy%252C%2520with%2520errors%2520remaining%2520within%252010%2525%252C%250Aeven%2520in%2520complex%2520cases.%2520The%2520algorithm%2520effectively%2520captured%2520key%2520cell%2520metrics%2520such%250Aas%2520cell%2520area%2520and%2520span%252C%2520revealing%2520trends%2520across%2520different%2520soot%2520foil%2520samples%2520with%250Auniform%2520to%2520highly%2520irregular%2520cellular%2520structures.%2520Although%2520the%2520model%2520proved%250Arobust%252C%2520challenges%2520remain%2520in%2520segmenting%2520and%2520analyzing%2520highly%2520complex%2520or%250Airregular%2520cellular%2520patterns.%2520This%2520work%2520highlights%2520the%2520broad%2520applicability%2520and%250Apotential%2520of%2520the%2520algorithm%2520to%2520advance%2520the%2520understanding%2520of%2520detonation%2520wave%250Adynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06466v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Machine%20Learning%20Based%20Approach%20for%20Statistical%20Analysis%20of%20Detonation%0A%20%20Cells%20from%20Soot%20Foils&entry.906535625=Vansh%20Sharma%20and%20Michael%20Ullman%20and%20Venkat%20Raman&entry.1292438233=%20%20This%20study%20presents%20a%20novel%20algorithm%20based%20on%20machine%20learning%20%28ML%29%20for%20the%0Aprecise%20segmentation%20and%20measurement%20of%20detonation%20cells%20from%20soot%20foil%20images%2C%0Aaddressing%20the%20limitations%20of%20manual%20and%20primitive%20edge%20detection%20methods%0Aprevalent%20in%20the%20field.%20Using%20advances%20in%20cellular%20biology%20segmentation%20models%2C%0Athe%20proposed%20algorithm%20is%20designed%20to%20accurately%20extract%20cellular%20patterns%0Awithout%20a%20training%20procedure%20or%20dataset%2C%20which%20is%20a%20significant%20challenge%20in%0Adetonation%20research.%20The%20algorithm%27s%20performance%20was%20validated%20using%20a%20series%0Aof%20test%20cases%20that%20mimic%20experimental%20and%20numerical%20detonation%20studies.%20The%0Aresults%20demonstrated%20consistent%20accuracy%2C%20with%20errors%20remaining%20within%2010%25%2C%0Aeven%20in%20complex%20cases.%20The%20algorithm%20effectively%20captured%20key%20cell%20metrics%20such%0Aas%20cell%20area%20and%20span%2C%20revealing%20trends%20across%20different%20soot%20foil%20samples%20with%0Auniform%20to%20highly%20irregular%20cellular%20structures.%20Although%20the%20model%20proved%0Arobust%2C%20challenges%20remain%20in%20segmenting%20and%20analyzing%20highly%20complex%20or%0Airregular%20cellular%20patterns.%20This%20work%20highlights%20the%20broad%20applicability%20and%0Apotential%20of%20the%20algorithm%20to%20advance%20the%20understanding%20of%20detonation%20wave%0Adynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06466v2&entry.124074799=Read"},
{"title": "Revisiting Static Feature-Based Android Malware Detection", "author": "Md Tanvirul Alam and Dipkamal Bhusal and Nidhi Rastogi", "abstract": "  The increasing reliance on machine learning (ML) in computer security,\nparticularly for malware classification, has driven significant advancements.\nHowever, the replicability and reproducibility of these results are often\noverlooked, leading to challenges in verifying research findings. This paper\nhighlights critical pitfalls that undermine the validity of ML research in\nAndroid malware detection, focusing on dataset and methodological issues. We\ncomprehensively analyze Android malware detection using two datasets and assess\noffline and continual learning settings with six widely used ML models. Our\nstudy reveals that when properly tuned, simpler baseline methods can often\noutperform more complex models. To address reproducibility challenges, we\npropose solutions for improving datasets and methodological practices, enabling\nfairer model comparisons. Additionally, we open-source our code to facilitate\nmalware analysis, making it extensible for new models and datasets. Our paper\naims to support future research in Android malware detection and other security\ndomains, enhancing the reliability and reproducibility of published results.\n", "link": "http://arxiv.org/abs/2409.07397v1", "date": "2024-09-11", "relevancy": 1.3065, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4433}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4361}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Static%20Feature-Based%20Android%20Malware%20Detection&body=Title%3A%20Revisiting%20Static%20Feature-Based%20Android%20Malware%20Detection%0AAuthor%3A%20Md%20Tanvirul%20Alam%20and%20Dipkamal%20Bhusal%20and%20Nidhi%20Rastogi%0AAbstract%3A%20%20%20The%20increasing%20reliance%20on%20machine%20learning%20%28ML%29%20in%20computer%20security%2C%0Aparticularly%20for%20malware%20classification%2C%20has%20driven%20significant%20advancements.%0AHowever%2C%20the%20replicability%20and%20reproducibility%20of%20these%20results%20are%20often%0Aoverlooked%2C%20leading%20to%20challenges%20in%20verifying%20research%20findings.%20This%20paper%0Ahighlights%20critical%20pitfalls%20that%20undermine%20the%20validity%20of%20ML%20research%20in%0AAndroid%20malware%20detection%2C%20focusing%20on%20dataset%20and%20methodological%20issues.%20We%0Acomprehensively%20analyze%20Android%20malware%20detection%20using%20two%20datasets%20and%20assess%0Aoffline%20and%20continual%20learning%20settings%20with%20six%20widely%20used%20ML%20models.%20Our%0Astudy%20reveals%20that%20when%20properly%20tuned%2C%20simpler%20baseline%20methods%20can%20often%0Aoutperform%20more%20complex%20models.%20To%20address%20reproducibility%20challenges%2C%20we%0Apropose%20solutions%20for%20improving%20datasets%20and%20methodological%20practices%2C%20enabling%0Afairer%20model%20comparisons.%20Additionally%2C%20we%20open-source%20our%20code%20to%20facilitate%0Amalware%20analysis%2C%20making%20it%20extensible%20for%20new%20models%20and%20datasets.%20Our%20paper%0Aaims%20to%20support%20future%20research%20in%20Android%20malware%20detection%20and%20other%20security%0Adomains%2C%20enhancing%20the%20reliability%20and%20reproducibility%20of%20published%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07397v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Static%2520Feature-Based%2520Android%2520Malware%2520Detection%26entry.906535625%3DMd%2520Tanvirul%2520Alam%2520and%2520Dipkamal%2520Bhusal%2520and%2520Nidhi%2520Rastogi%26entry.1292438233%3D%2520%2520The%2520increasing%2520reliance%2520on%2520machine%2520learning%2520%2528ML%2529%2520in%2520computer%2520security%252C%250Aparticularly%2520for%2520malware%2520classification%252C%2520has%2520driven%2520significant%2520advancements.%250AHowever%252C%2520the%2520replicability%2520and%2520reproducibility%2520of%2520these%2520results%2520are%2520often%250Aoverlooked%252C%2520leading%2520to%2520challenges%2520in%2520verifying%2520research%2520findings.%2520This%2520paper%250Ahighlights%2520critical%2520pitfalls%2520that%2520undermine%2520the%2520validity%2520of%2520ML%2520research%2520in%250AAndroid%2520malware%2520detection%252C%2520focusing%2520on%2520dataset%2520and%2520methodological%2520issues.%2520We%250Acomprehensively%2520analyze%2520Android%2520malware%2520detection%2520using%2520two%2520datasets%2520and%2520assess%250Aoffline%2520and%2520continual%2520learning%2520settings%2520with%2520six%2520widely%2520used%2520ML%2520models.%2520Our%250Astudy%2520reveals%2520that%2520when%2520properly%2520tuned%252C%2520simpler%2520baseline%2520methods%2520can%2520often%250Aoutperform%2520more%2520complex%2520models.%2520To%2520address%2520reproducibility%2520challenges%252C%2520we%250Apropose%2520solutions%2520for%2520improving%2520datasets%2520and%2520methodological%2520practices%252C%2520enabling%250Afairer%2520model%2520comparisons.%2520Additionally%252C%2520we%2520open-source%2520our%2520code%2520to%2520facilitate%250Amalware%2520analysis%252C%2520making%2520it%2520extensible%2520for%2520new%2520models%2520and%2520datasets.%2520Our%2520paper%250Aaims%2520to%2520support%2520future%2520research%2520in%2520Android%2520malware%2520detection%2520and%2520other%2520security%250Adomains%252C%2520enhancing%2520the%2520reliability%2520and%2520reproducibility%2520of%2520published%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07397v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Static%20Feature-Based%20Android%20Malware%20Detection&entry.906535625=Md%20Tanvirul%20Alam%20and%20Dipkamal%20Bhusal%20and%20Nidhi%20Rastogi&entry.1292438233=%20%20The%20increasing%20reliance%20on%20machine%20learning%20%28ML%29%20in%20computer%20security%2C%0Aparticularly%20for%20malware%20classification%2C%20has%20driven%20significant%20advancements.%0AHowever%2C%20the%20replicability%20and%20reproducibility%20of%20these%20results%20are%20often%0Aoverlooked%2C%20leading%20to%20challenges%20in%20verifying%20research%20findings.%20This%20paper%0Ahighlights%20critical%20pitfalls%20that%20undermine%20the%20validity%20of%20ML%20research%20in%0AAndroid%20malware%20detection%2C%20focusing%20on%20dataset%20and%20methodological%20issues.%20We%0Acomprehensively%20analyze%20Android%20malware%20detection%20using%20two%20datasets%20and%20assess%0Aoffline%20and%20continual%20learning%20settings%20with%20six%20widely%20used%20ML%20models.%20Our%0Astudy%20reveals%20that%20when%20properly%20tuned%2C%20simpler%20baseline%20methods%20can%20often%0Aoutperform%20more%20complex%20models.%20To%20address%20reproducibility%20challenges%2C%20we%0Apropose%20solutions%20for%20improving%20datasets%20and%20methodological%20practices%2C%20enabling%0Afairer%20model%20comparisons.%20Additionally%2C%20we%20open-source%20our%20code%20to%20facilitate%0Amalware%20analysis%2C%20making%20it%20extensible%20for%20new%20models%20and%20datasets.%20Our%20paper%0Aaims%20to%20support%20future%20research%20in%20Android%20malware%20detection%20and%20other%20security%0Adomains%2C%20enhancing%20the%20reliability%20and%20reproducibility%20of%20published%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07397v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


