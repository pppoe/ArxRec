<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250904.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D\n  Visual Grounding based on CLIP", "author": "Fan Li and Zanyi Wang and Zeyi Huang and Guang Dai and Jingdong Wang and Mengmeng Wang", "abstract": "  3D visual grounding allows an embodied agent to understand visual information\nin real-world 3D environments based on human instructions, which is crucial for\nembodied intelligence. Existing 3D visual grounding methods typically rely on\nseparate encoders for different modalities (e.g., RGB images, text, and 3D\npoint clouds), resulting in large and complex models that are inefficient to\ntrain. While some approaches use pre-trained 2D multi-modal models like CLIP\nfor 3D tasks, they still struggle with aligning point cloud data to 2D\nencoders. As a result, these methods continue to depend on 3D encoders for\nfeature extraction, further increasing model complexity and training\ninefficiency. In this paper, we propose a unified 2D pre-trained multi-modal\nnetwork to process all three modalities (RGB images, text, and point clouds),\nsignificantly simplifying the architecture. By leveraging a 2D CLIP bi-modal\nmodel with adapter-based fine-tuning, this framework effectively adapts to the\ntri-modal setting, improving both adaptability and performance across\nmodalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module\nis designed to fuse geometric multi-scale features from point clouds and\nimages. We then integrate textual features for final modality fusion and\nintroduce a multi-modal decoder to facilitate deep cross-modal understanding.\nTogether, our method achieves unified feature extraction and fusion across the\nthree modalities, enabling an end-to-end 3D visual grounding model. Compared to\nthe baseline, our method reduces the number of trainable parameters by\napproximately 58\\%, while achieving a 6.52\\% improvement in the 3D detection\ntask and a 6.25\\% improvement in the 3D visual grounding task.\n", "link": "http://arxiv.org/abs/2507.14904v2", "date": "2025-09-04", "relevancy": 3.2831, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6939}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.638}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TriCLIP-3D%3A%20A%20Unified%20Parameter-Efficient%20Framework%20for%20Tri-Modal%203D%0A%20%20Visual%20Grounding%20based%20on%20CLIP&body=Title%3A%20TriCLIP-3D%3A%20A%20Unified%20Parameter-Efficient%20Framework%20for%20Tri-Modal%203D%0A%20%20Visual%20Grounding%20based%20on%20CLIP%0AAuthor%3A%20Fan%20Li%20and%20Zanyi%20Wang%20and%20Zeyi%20Huang%20and%20Guang%20Dai%20and%20Jingdong%20Wang%20and%20Mengmeng%20Wang%0AAbstract%3A%20%20%203D%20visual%20grounding%20allows%20an%20embodied%20agent%20to%20understand%20visual%20information%0Ain%20real-world%203D%20environments%20based%20on%20human%20instructions%2C%20which%20is%20crucial%20for%0Aembodied%20intelligence.%20Existing%203D%20visual%20grounding%20methods%20typically%20rely%20on%0Aseparate%20encoders%20for%20different%20modalities%20%28e.g.%2C%20RGB%20images%2C%20text%2C%20and%203D%0Apoint%20clouds%29%2C%20resulting%20in%20large%20and%20complex%20models%20that%20are%20inefficient%20to%0Atrain.%20While%20some%20approaches%20use%20pre-trained%202D%20multi-modal%20models%20like%20CLIP%0Afor%203D%20tasks%2C%20they%20still%20struggle%20with%20aligning%20point%20cloud%20data%20to%202D%0Aencoders.%20As%20a%20result%2C%20these%20methods%20continue%20to%20depend%20on%203D%20encoders%20for%0Afeature%20extraction%2C%20further%20increasing%20model%20complexity%20and%20training%0Ainefficiency.%20In%20this%20paper%2C%20we%20propose%20a%20unified%202D%20pre-trained%20multi-modal%0Anetwork%20to%20process%20all%20three%20modalities%20%28RGB%20images%2C%20text%2C%20and%20point%20clouds%29%2C%0Asignificantly%20simplifying%20the%20architecture.%20By%20leveraging%20a%202D%20CLIP%20bi-modal%0Amodel%20with%20adapter-based%20fine-tuning%2C%20this%20framework%20effectively%20adapts%20to%20the%0Atri-modal%20setting%2C%20improving%20both%20adaptability%20and%20performance%20across%0Amodalities.%20Our%20Geometric-Aware%202D-3D%20Feature%20Recovery%20and%20Fusion%20%28GARF%29%20module%0Ais%20designed%20to%20fuse%20geometric%20multi-scale%20features%20from%20point%20clouds%20and%0Aimages.%20We%20then%20integrate%20textual%20features%20for%20final%20modality%20fusion%20and%0Aintroduce%20a%20multi-modal%20decoder%20to%20facilitate%20deep%20cross-modal%20understanding.%0ATogether%2C%20our%20method%20achieves%20unified%20feature%20extraction%20and%20fusion%20across%20the%0Athree%20modalities%2C%20enabling%20an%20end-to-end%203D%20visual%20grounding%20model.%20Compared%20to%0Athe%20baseline%2C%20our%20method%20reduces%20the%20number%20of%20trainable%20parameters%20by%0Aapproximately%2058%5C%25%2C%20while%20achieving%20a%206.52%5C%25%20improvement%20in%20the%203D%20detection%0Atask%20and%20a%206.25%5C%25%20improvement%20in%20the%203D%20visual%20grounding%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14904v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTriCLIP-3D%253A%2520A%2520Unified%2520Parameter-Efficient%2520Framework%2520for%2520Tri-Modal%25203D%250A%2520%2520Visual%2520Grounding%2520based%2520on%2520CLIP%26entry.906535625%3DFan%2520Li%2520and%2520Zanyi%2520Wang%2520and%2520Zeyi%2520Huang%2520and%2520Guang%2520Dai%2520and%2520Jingdong%2520Wang%2520and%2520Mengmeng%2520Wang%26entry.1292438233%3D%2520%25203D%2520visual%2520grounding%2520allows%2520an%2520embodied%2520agent%2520to%2520understand%2520visual%2520information%250Ain%2520real-world%25203D%2520environments%2520based%2520on%2520human%2520instructions%252C%2520which%2520is%2520crucial%2520for%250Aembodied%2520intelligence.%2520Existing%25203D%2520visual%2520grounding%2520methods%2520typically%2520rely%2520on%250Aseparate%2520encoders%2520for%2520different%2520modalities%2520%2528e.g.%252C%2520RGB%2520images%252C%2520text%252C%2520and%25203D%250Apoint%2520clouds%2529%252C%2520resulting%2520in%2520large%2520and%2520complex%2520models%2520that%2520are%2520inefficient%2520to%250Atrain.%2520While%2520some%2520approaches%2520use%2520pre-trained%25202D%2520multi-modal%2520models%2520like%2520CLIP%250Afor%25203D%2520tasks%252C%2520they%2520still%2520struggle%2520with%2520aligning%2520point%2520cloud%2520data%2520to%25202D%250Aencoders.%2520As%2520a%2520result%252C%2520these%2520methods%2520continue%2520to%2520depend%2520on%25203D%2520encoders%2520for%250Afeature%2520extraction%252C%2520further%2520increasing%2520model%2520complexity%2520and%2520training%250Ainefficiency.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520unified%25202D%2520pre-trained%2520multi-modal%250Anetwork%2520to%2520process%2520all%2520three%2520modalities%2520%2528RGB%2520images%252C%2520text%252C%2520and%2520point%2520clouds%2529%252C%250Asignificantly%2520simplifying%2520the%2520architecture.%2520By%2520leveraging%2520a%25202D%2520CLIP%2520bi-modal%250Amodel%2520with%2520adapter-based%2520fine-tuning%252C%2520this%2520framework%2520effectively%2520adapts%2520to%2520the%250Atri-modal%2520setting%252C%2520improving%2520both%2520adaptability%2520and%2520performance%2520across%250Amodalities.%2520Our%2520Geometric-Aware%25202D-3D%2520Feature%2520Recovery%2520and%2520Fusion%2520%2528GARF%2529%2520module%250Ais%2520designed%2520to%2520fuse%2520geometric%2520multi-scale%2520features%2520from%2520point%2520clouds%2520and%250Aimages.%2520We%2520then%2520integrate%2520textual%2520features%2520for%2520final%2520modality%2520fusion%2520and%250Aintroduce%2520a%2520multi-modal%2520decoder%2520to%2520facilitate%2520deep%2520cross-modal%2520understanding.%250ATogether%252C%2520our%2520method%2520achieves%2520unified%2520feature%2520extraction%2520and%2520fusion%2520across%2520the%250Athree%2520modalities%252C%2520enabling%2520an%2520end-to-end%25203D%2520visual%2520grounding%2520model.%2520Compared%2520to%250Athe%2520baseline%252C%2520our%2520method%2520reduces%2520the%2520number%2520of%2520trainable%2520parameters%2520by%250Aapproximately%252058%255C%2525%252C%2520while%2520achieving%2520a%25206.52%255C%2525%2520improvement%2520in%2520the%25203D%2520detection%250Atask%2520and%2520a%25206.25%255C%2525%2520improvement%2520in%2520the%25203D%2520visual%2520grounding%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14904v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TriCLIP-3D%3A%20A%20Unified%20Parameter-Efficient%20Framework%20for%20Tri-Modal%203D%0A%20%20Visual%20Grounding%20based%20on%20CLIP&entry.906535625=Fan%20Li%20and%20Zanyi%20Wang%20and%20Zeyi%20Huang%20and%20Guang%20Dai%20and%20Jingdong%20Wang%20and%20Mengmeng%20Wang&entry.1292438233=%20%203D%20visual%20grounding%20allows%20an%20embodied%20agent%20to%20understand%20visual%20information%0Ain%20real-world%203D%20environments%20based%20on%20human%20instructions%2C%20which%20is%20crucial%20for%0Aembodied%20intelligence.%20Existing%203D%20visual%20grounding%20methods%20typically%20rely%20on%0Aseparate%20encoders%20for%20different%20modalities%20%28e.g.%2C%20RGB%20images%2C%20text%2C%20and%203D%0Apoint%20clouds%29%2C%20resulting%20in%20large%20and%20complex%20models%20that%20are%20inefficient%20to%0Atrain.%20While%20some%20approaches%20use%20pre-trained%202D%20multi-modal%20models%20like%20CLIP%0Afor%203D%20tasks%2C%20they%20still%20struggle%20with%20aligning%20point%20cloud%20data%20to%202D%0Aencoders.%20As%20a%20result%2C%20these%20methods%20continue%20to%20depend%20on%203D%20encoders%20for%0Afeature%20extraction%2C%20further%20increasing%20model%20complexity%20and%20training%0Ainefficiency.%20In%20this%20paper%2C%20we%20propose%20a%20unified%202D%20pre-trained%20multi-modal%0Anetwork%20to%20process%20all%20three%20modalities%20%28RGB%20images%2C%20text%2C%20and%20point%20clouds%29%2C%0Asignificantly%20simplifying%20the%20architecture.%20By%20leveraging%20a%202D%20CLIP%20bi-modal%0Amodel%20with%20adapter-based%20fine-tuning%2C%20this%20framework%20effectively%20adapts%20to%20the%0Atri-modal%20setting%2C%20improving%20both%20adaptability%20and%20performance%20across%0Amodalities.%20Our%20Geometric-Aware%202D-3D%20Feature%20Recovery%20and%20Fusion%20%28GARF%29%20module%0Ais%20designed%20to%20fuse%20geometric%20multi-scale%20features%20from%20point%20clouds%20and%0Aimages.%20We%20then%20integrate%20textual%20features%20for%20final%20modality%20fusion%20and%0Aintroduce%20a%20multi-modal%20decoder%20to%20facilitate%20deep%20cross-modal%20understanding.%0ATogether%2C%20our%20method%20achieves%20unified%20feature%20extraction%20and%20fusion%20across%20the%0Athree%20modalities%2C%20enabling%20an%20end-to-end%203D%20visual%20grounding%20model.%20Compared%20to%0Athe%20baseline%2C%20our%20method%20reduces%20the%20number%20of%20trainable%20parameters%20by%0Aapproximately%2058%5C%25%2C%20while%20achieving%20a%206.52%5C%25%20improvement%20in%20the%203D%20detection%0Atask%20and%20a%206.25%5C%25%20improvement%20in%20the%203D%20visual%20grounding%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14904v2&entry.124074799=Read"},
{"title": "Hyper Diffusion Avatars: Dynamic Human Avatar Generation using Network\n  Weight Space Diffusion", "author": "Dongliang Cao and Guoxing Sun and Marc Habermann and Florian Bernard", "abstract": "  Creating human avatars is a highly desirable yet challenging task. Recent\nadvancements in radiance field rendering have achieved unprecedented\nphotorealism and real-time performance for personalized dynamic human avatars.\nHowever, these approaches are typically limited to person-specific rendering\nmodels trained on multi-view video data for a single individual, limiting their\nability to generalize across different identities. On the other hand,\ngenerative approaches leveraging prior knowledge from pre-trained 2D diffusion\nmodels can produce cartoonish, static human avatars, which are animated through\nsimple skeleton-based articulation. Therefore, the avatars generated by these\nmethods suffer from lower rendering quality compared to person-specific\nrendering methods and fail to capture pose-dependent deformations such as cloth\nwrinkles. In this paper, we propose a novel approach that unites the strengths\nof person-specific rendering and diffusion-based generative modeling to enable\ndynamic human avatar generation with both high photorealism and realistic\npose-dependent deformations. Our method follows a two-stage pipeline: first, we\noptimize a set of person-specific UNets, with each network representing a\ndynamic human avatar that captures intricate pose-dependent deformations. In\nthe second stage, we train a hyper diffusion model over the optimized network\nweights. During inference, our method generates network weights for real-time,\ncontrollable rendering of dynamic human avatars. Using a large-scale,\ncross-identity, multi-view video dataset, we demonstrate that our approach\noutperforms state-of-the-art human avatar generation methods.\n", "link": "http://arxiv.org/abs/2509.04145v1", "date": "2025-09-04", "relevancy": 3.2146, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6855}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6216}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hyper%20Diffusion%20Avatars%3A%20Dynamic%20Human%20Avatar%20Generation%20using%20Network%0A%20%20Weight%20Space%20Diffusion&body=Title%3A%20Hyper%20Diffusion%20Avatars%3A%20Dynamic%20Human%20Avatar%20Generation%20using%20Network%0A%20%20Weight%20Space%20Diffusion%0AAuthor%3A%20Dongliang%20Cao%20and%20Guoxing%20Sun%20and%20Marc%20Habermann%20and%20Florian%20Bernard%0AAbstract%3A%20%20%20Creating%20human%20avatars%20is%20a%20highly%20desirable%20yet%20challenging%20task.%20Recent%0Aadvancements%20in%20radiance%20field%20rendering%20have%20achieved%20unprecedented%0Aphotorealism%20and%20real-time%20performance%20for%20personalized%20dynamic%20human%20avatars.%0AHowever%2C%20these%20approaches%20are%20typically%20limited%20to%20person-specific%20rendering%0Amodels%20trained%20on%20multi-view%20video%20data%20for%20a%20single%20individual%2C%20limiting%20their%0Aability%20to%20generalize%20across%20different%20identities.%20On%20the%20other%20hand%2C%0Agenerative%20approaches%20leveraging%20prior%20knowledge%20from%20pre-trained%202D%20diffusion%0Amodels%20can%20produce%20cartoonish%2C%20static%20human%20avatars%2C%20which%20are%20animated%20through%0Asimple%20skeleton-based%20articulation.%20Therefore%2C%20the%20avatars%20generated%20by%20these%0Amethods%20suffer%20from%20lower%20rendering%20quality%20compared%20to%20person-specific%0Arendering%20methods%20and%20fail%20to%20capture%20pose-dependent%20deformations%20such%20as%20cloth%0Awrinkles.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20that%20unites%20the%20strengths%0Aof%20person-specific%20rendering%20and%20diffusion-based%20generative%20modeling%20to%20enable%0Adynamic%20human%20avatar%20generation%20with%20both%20high%20photorealism%20and%20realistic%0Apose-dependent%20deformations.%20Our%20method%20follows%20a%20two-stage%20pipeline%3A%20first%2C%20we%0Aoptimize%20a%20set%20of%20person-specific%20UNets%2C%20with%20each%20network%20representing%20a%0Adynamic%20human%20avatar%20that%20captures%20intricate%20pose-dependent%20deformations.%20In%0Athe%20second%20stage%2C%20we%20train%20a%20hyper%20diffusion%20model%20over%20the%20optimized%20network%0Aweights.%20During%20inference%2C%20our%20method%20generates%20network%20weights%20for%20real-time%2C%0Acontrollable%20rendering%20of%20dynamic%20human%20avatars.%20Using%20a%20large-scale%2C%0Across-identity%2C%20multi-view%20video%20dataset%2C%20we%20demonstrate%20that%20our%20approach%0Aoutperforms%20state-of-the-art%20human%20avatar%20generation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04145v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyper%2520Diffusion%2520Avatars%253A%2520Dynamic%2520Human%2520Avatar%2520Generation%2520using%2520Network%250A%2520%2520Weight%2520Space%2520Diffusion%26entry.906535625%3DDongliang%2520Cao%2520and%2520Guoxing%2520Sun%2520and%2520Marc%2520Habermann%2520and%2520Florian%2520Bernard%26entry.1292438233%3D%2520%2520Creating%2520human%2520avatars%2520is%2520a%2520highly%2520desirable%2520yet%2520challenging%2520task.%2520Recent%250Aadvancements%2520in%2520radiance%2520field%2520rendering%2520have%2520achieved%2520unprecedented%250Aphotorealism%2520and%2520real-time%2520performance%2520for%2520personalized%2520dynamic%2520human%2520avatars.%250AHowever%252C%2520these%2520approaches%2520are%2520typically%2520limited%2520to%2520person-specific%2520rendering%250Amodels%2520trained%2520on%2520multi-view%2520video%2520data%2520for%2520a%2520single%2520individual%252C%2520limiting%2520their%250Aability%2520to%2520generalize%2520across%2520different%2520identities.%2520On%2520the%2520other%2520hand%252C%250Agenerative%2520approaches%2520leveraging%2520prior%2520knowledge%2520from%2520pre-trained%25202D%2520diffusion%250Amodels%2520can%2520produce%2520cartoonish%252C%2520static%2520human%2520avatars%252C%2520which%2520are%2520animated%2520through%250Asimple%2520skeleton-based%2520articulation.%2520Therefore%252C%2520the%2520avatars%2520generated%2520by%2520these%250Amethods%2520suffer%2520from%2520lower%2520rendering%2520quality%2520compared%2520to%2520person-specific%250Arendering%2520methods%2520and%2520fail%2520to%2520capture%2520pose-dependent%2520deformations%2520such%2520as%2520cloth%250Awrinkles.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520approach%2520that%2520unites%2520the%2520strengths%250Aof%2520person-specific%2520rendering%2520and%2520diffusion-based%2520generative%2520modeling%2520to%2520enable%250Adynamic%2520human%2520avatar%2520generation%2520with%2520both%2520high%2520photorealism%2520and%2520realistic%250Apose-dependent%2520deformations.%2520Our%2520method%2520follows%2520a%2520two-stage%2520pipeline%253A%2520first%252C%2520we%250Aoptimize%2520a%2520set%2520of%2520person-specific%2520UNets%252C%2520with%2520each%2520network%2520representing%2520a%250Adynamic%2520human%2520avatar%2520that%2520captures%2520intricate%2520pose-dependent%2520deformations.%2520In%250Athe%2520second%2520stage%252C%2520we%2520train%2520a%2520hyper%2520diffusion%2520model%2520over%2520the%2520optimized%2520network%250Aweights.%2520During%2520inference%252C%2520our%2520method%2520generates%2520network%2520weights%2520for%2520real-time%252C%250Acontrollable%2520rendering%2520of%2520dynamic%2520human%2520avatars.%2520Using%2520a%2520large-scale%252C%250Across-identity%252C%2520multi-view%2520video%2520dataset%252C%2520we%2520demonstrate%2520that%2520our%2520approach%250Aoutperforms%2520state-of-the-art%2520human%2520avatar%2520generation%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04145v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyper%20Diffusion%20Avatars%3A%20Dynamic%20Human%20Avatar%20Generation%20using%20Network%0A%20%20Weight%20Space%20Diffusion&entry.906535625=Dongliang%20Cao%20and%20Guoxing%20Sun%20and%20Marc%20Habermann%20and%20Florian%20Bernard&entry.1292438233=%20%20Creating%20human%20avatars%20is%20a%20highly%20desirable%20yet%20challenging%20task.%20Recent%0Aadvancements%20in%20radiance%20field%20rendering%20have%20achieved%20unprecedented%0Aphotorealism%20and%20real-time%20performance%20for%20personalized%20dynamic%20human%20avatars.%0AHowever%2C%20these%20approaches%20are%20typically%20limited%20to%20person-specific%20rendering%0Amodels%20trained%20on%20multi-view%20video%20data%20for%20a%20single%20individual%2C%20limiting%20their%0Aability%20to%20generalize%20across%20different%20identities.%20On%20the%20other%20hand%2C%0Agenerative%20approaches%20leveraging%20prior%20knowledge%20from%20pre-trained%202D%20diffusion%0Amodels%20can%20produce%20cartoonish%2C%20static%20human%20avatars%2C%20which%20are%20animated%20through%0Asimple%20skeleton-based%20articulation.%20Therefore%2C%20the%20avatars%20generated%20by%20these%0Amethods%20suffer%20from%20lower%20rendering%20quality%20compared%20to%20person-specific%0Arendering%20methods%20and%20fail%20to%20capture%20pose-dependent%20deformations%20such%20as%20cloth%0Awrinkles.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20that%20unites%20the%20strengths%0Aof%20person-specific%20rendering%20and%20diffusion-based%20generative%20modeling%20to%20enable%0Adynamic%20human%20avatar%20generation%20with%20both%20high%20photorealism%20and%20realistic%0Apose-dependent%20deformations.%20Our%20method%20follows%20a%20two-stage%20pipeline%3A%20first%2C%20we%0Aoptimize%20a%20set%20of%20person-specific%20UNets%2C%20with%20each%20network%20representing%20a%0Adynamic%20human%20avatar%20that%20captures%20intricate%20pose-dependent%20deformations.%20In%0Athe%20second%20stage%2C%20we%20train%20a%20hyper%20diffusion%20model%20over%20the%20optimized%20network%0Aweights.%20During%20inference%2C%20our%20method%20generates%20network%20weights%20for%20real-time%2C%0Acontrollable%20rendering%20of%20dynamic%20human%20avatars.%20Using%20a%20large-scale%2C%0Across-identity%2C%20multi-view%20video%20dataset%2C%20we%20demonstrate%20that%20our%20approach%0Aoutperforms%20state-of-the-art%20human%20avatar%20generation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04145v1&entry.124074799=Read"},
{"title": "SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer", "author": "Jimin Xu and Bosheng Qin and Tao Jin and Zhou Zhao and Zhenhui Ye and Jun Yu and Fei Wu", "abstract": "  Recent advancements in neural representations, such as Neural Radiance Fields\nand 3D Gaussian Splatting, have increased interest in applying style transfer\nto 3D scenes. While existing methods can transfer style patterns onto\n3D-consistent neural representations, they struggle to effectively extract and\ntransfer high-level style semantics from the reference style image.\nAdditionally, the stylized results often lack structural clarity and\nseparation, making it difficult to distinguish between different instances or\nobjects within the 3D scene. To address these limitations, we propose a novel\n3D style transfer pipeline that effectively integrates prior knowledge from\npretrained 2D diffusion models. Our pipeline consists of two key stages: First,\nwe leverage diffusion priors to generate stylized renderings of key viewpoints.\nThen, we transfer the stylized key views onto the 3D representation. This\nprocess incorporates two innovative designs. The first is cross-view style\nalignment, which inserts cross-view attention into the last upsampling block of\nthe UNet, allowing feature interactions across multiple key views. This ensures\nthat the diffusion model generates stylized key views that maintain both style\nfidelity and instance-level consistency. The second is instance-level style\ntransfer, which effectively leverages instance-level consistency across\nstylized key views and transfers it onto the 3D representation. This results in\na more structured, visually coherent, and artistically enriched stylization.\nExtensive qualitative and quantitative experiments demonstrate that our 3D\nstyle transfer pipeline significantly outperforms state-of-the-art methods\nacross a wide range of scenes, from forward-facing to challenging 360-degree\nenvironments. Visit our project page https://jm-xu.github.io/SSGaussian for\nimmersive visualization.\n", "link": "http://arxiv.org/abs/2509.04379v1", "date": "2025-09-04", "relevancy": 3.1531, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6559}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6257}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSGaussian%3A%20Semantic-Aware%20and%20Structure-Preserving%203D%20Style%20Transfer&body=Title%3A%20SSGaussian%3A%20Semantic-Aware%20and%20Structure-Preserving%203D%20Style%20Transfer%0AAuthor%3A%20Jimin%20Xu%20and%20Bosheng%20Qin%20and%20Tao%20Jin%20and%20Zhou%20Zhao%20and%20Zhenhui%20Ye%20and%20Jun%20Yu%20and%20Fei%20Wu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20neural%20representations%2C%20such%20as%20Neural%20Radiance%20Fields%0Aand%203D%20Gaussian%20Splatting%2C%20have%20increased%20interest%20in%20applying%20style%20transfer%0Ato%203D%20scenes.%20While%20existing%20methods%20can%20transfer%20style%20patterns%20onto%0A3D-consistent%20neural%20representations%2C%20they%20struggle%20to%20effectively%20extract%20and%0Atransfer%20high-level%20style%20semantics%20from%20the%20reference%20style%20image.%0AAdditionally%2C%20the%20stylized%20results%20often%20lack%20structural%20clarity%20and%0Aseparation%2C%20making%20it%20difficult%20to%20distinguish%20between%20different%20instances%20or%0Aobjects%20within%20the%203D%20scene.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%0A3D%20style%20transfer%20pipeline%20that%20effectively%20integrates%20prior%20knowledge%20from%0Apretrained%202D%20diffusion%20models.%20Our%20pipeline%20consists%20of%20two%20key%20stages%3A%20First%2C%0Awe%20leverage%20diffusion%20priors%20to%20generate%20stylized%20renderings%20of%20key%20viewpoints.%0AThen%2C%20we%20transfer%20the%20stylized%20key%20views%20onto%20the%203D%20representation.%20This%0Aprocess%20incorporates%20two%20innovative%20designs.%20The%20first%20is%20cross-view%20style%0Aalignment%2C%20which%20inserts%20cross-view%20attention%20into%20the%20last%20upsampling%20block%20of%0Athe%20UNet%2C%20allowing%20feature%20interactions%20across%20multiple%20key%20views.%20This%20ensures%0Athat%20the%20diffusion%20model%20generates%20stylized%20key%20views%20that%20maintain%20both%20style%0Afidelity%20and%20instance-level%20consistency.%20The%20second%20is%20instance-level%20style%0Atransfer%2C%20which%20effectively%20leverages%20instance-level%20consistency%20across%0Astylized%20key%20views%20and%20transfers%20it%20onto%20the%203D%20representation.%20This%20results%20in%0Aa%20more%20structured%2C%20visually%20coherent%2C%20and%20artistically%20enriched%20stylization.%0AExtensive%20qualitative%20and%20quantitative%20experiments%20demonstrate%20that%20our%203D%0Astyle%20transfer%20pipeline%20significantly%20outperforms%20state-of-the-art%20methods%0Aacross%20a%20wide%20range%20of%20scenes%2C%20from%20forward-facing%20to%20challenging%20360-degree%0Aenvironments.%20Visit%20our%20project%20page%20https%3A//jm-xu.github.io/SSGaussian%20for%0Aimmersive%20visualization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04379v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSGaussian%253A%2520Semantic-Aware%2520and%2520Structure-Preserving%25203D%2520Style%2520Transfer%26entry.906535625%3DJimin%2520Xu%2520and%2520Bosheng%2520Qin%2520and%2520Tao%2520Jin%2520and%2520Zhou%2520Zhao%2520and%2520Zhenhui%2520Ye%2520and%2520Jun%2520Yu%2520and%2520Fei%2520Wu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520neural%2520representations%252C%2520such%2520as%2520Neural%2520Radiance%2520Fields%250Aand%25203D%2520Gaussian%2520Splatting%252C%2520have%2520increased%2520interest%2520in%2520applying%2520style%2520transfer%250Ato%25203D%2520scenes.%2520While%2520existing%2520methods%2520can%2520transfer%2520style%2520patterns%2520onto%250A3D-consistent%2520neural%2520representations%252C%2520they%2520struggle%2520to%2520effectively%2520extract%2520and%250Atransfer%2520high-level%2520style%2520semantics%2520from%2520the%2520reference%2520style%2520image.%250AAdditionally%252C%2520the%2520stylized%2520results%2520often%2520lack%2520structural%2520clarity%2520and%250Aseparation%252C%2520making%2520it%2520difficult%2520to%2520distinguish%2520between%2520different%2520instances%2520or%250Aobjects%2520within%2520the%25203D%2520scene.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%250A3D%2520style%2520transfer%2520pipeline%2520that%2520effectively%2520integrates%2520prior%2520knowledge%2520from%250Apretrained%25202D%2520diffusion%2520models.%2520Our%2520pipeline%2520consists%2520of%2520two%2520key%2520stages%253A%2520First%252C%250Awe%2520leverage%2520diffusion%2520priors%2520to%2520generate%2520stylized%2520renderings%2520of%2520key%2520viewpoints.%250AThen%252C%2520we%2520transfer%2520the%2520stylized%2520key%2520views%2520onto%2520the%25203D%2520representation.%2520This%250Aprocess%2520incorporates%2520two%2520innovative%2520designs.%2520The%2520first%2520is%2520cross-view%2520style%250Aalignment%252C%2520which%2520inserts%2520cross-view%2520attention%2520into%2520the%2520last%2520upsampling%2520block%2520of%250Athe%2520UNet%252C%2520allowing%2520feature%2520interactions%2520across%2520multiple%2520key%2520views.%2520This%2520ensures%250Athat%2520the%2520diffusion%2520model%2520generates%2520stylized%2520key%2520views%2520that%2520maintain%2520both%2520style%250Afidelity%2520and%2520instance-level%2520consistency.%2520The%2520second%2520is%2520instance-level%2520style%250Atransfer%252C%2520which%2520effectively%2520leverages%2520instance-level%2520consistency%2520across%250Astylized%2520key%2520views%2520and%2520transfers%2520it%2520onto%2520the%25203D%2520representation.%2520This%2520results%2520in%250Aa%2520more%2520structured%252C%2520visually%2520coherent%252C%2520and%2520artistically%2520enriched%2520stylization.%250AExtensive%2520qualitative%2520and%2520quantitative%2520experiments%2520demonstrate%2520that%2520our%25203D%250Astyle%2520transfer%2520pipeline%2520significantly%2520outperforms%2520state-of-the-art%2520methods%250Aacross%2520a%2520wide%2520range%2520of%2520scenes%252C%2520from%2520forward-facing%2520to%2520challenging%2520360-degree%250Aenvironments.%2520Visit%2520our%2520project%2520page%2520https%253A//jm-xu.github.io/SSGaussian%2520for%250Aimmersive%2520visualization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04379v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSGaussian%3A%20Semantic-Aware%20and%20Structure-Preserving%203D%20Style%20Transfer&entry.906535625=Jimin%20Xu%20and%20Bosheng%20Qin%20and%20Tao%20Jin%20and%20Zhou%20Zhao%20and%20Zhenhui%20Ye%20and%20Jun%20Yu%20and%20Fei%20Wu&entry.1292438233=%20%20Recent%20advancements%20in%20neural%20representations%2C%20such%20as%20Neural%20Radiance%20Fields%0Aand%203D%20Gaussian%20Splatting%2C%20have%20increased%20interest%20in%20applying%20style%20transfer%0Ato%203D%20scenes.%20While%20existing%20methods%20can%20transfer%20style%20patterns%20onto%0A3D-consistent%20neural%20representations%2C%20they%20struggle%20to%20effectively%20extract%20and%0Atransfer%20high-level%20style%20semantics%20from%20the%20reference%20style%20image.%0AAdditionally%2C%20the%20stylized%20results%20often%20lack%20structural%20clarity%20and%0Aseparation%2C%20making%20it%20difficult%20to%20distinguish%20between%20different%20instances%20or%0Aobjects%20within%20the%203D%20scene.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%0A3D%20style%20transfer%20pipeline%20that%20effectively%20integrates%20prior%20knowledge%20from%0Apretrained%202D%20diffusion%20models.%20Our%20pipeline%20consists%20of%20two%20key%20stages%3A%20First%2C%0Awe%20leverage%20diffusion%20priors%20to%20generate%20stylized%20renderings%20of%20key%20viewpoints.%0AThen%2C%20we%20transfer%20the%20stylized%20key%20views%20onto%20the%203D%20representation.%20This%0Aprocess%20incorporates%20two%20innovative%20designs.%20The%20first%20is%20cross-view%20style%0Aalignment%2C%20which%20inserts%20cross-view%20attention%20into%20the%20last%20upsampling%20block%20of%0Athe%20UNet%2C%20allowing%20feature%20interactions%20across%20multiple%20key%20views.%20This%20ensures%0Athat%20the%20diffusion%20model%20generates%20stylized%20key%20views%20that%20maintain%20both%20style%0Afidelity%20and%20instance-level%20consistency.%20The%20second%20is%20instance-level%20style%0Atransfer%2C%20which%20effectively%20leverages%20instance-level%20consistency%20across%0Astylized%20key%20views%20and%20transfers%20it%20onto%20the%203D%20representation.%20This%20results%20in%0Aa%20more%20structured%2C%20visually%20coherent%2C%20and%20artistically%20enriched%20stylization.%0AExtensive%20qualitative%20and%20quantitative%20experiments%20demonstrate%20that%20our%203D%0Astyle%20transfer%20pipeline%20significantly%20outperforms%20state-of-the-art%20methods%0Aacross%20a%20wide%20range%20of%20scenes%2C%20from%20forward-facing%20to%20challenging%20360-degree%0Aenvironments.%20Visit%20our%20project%20page%20https%3A//jm-xu.github.io/SSGaussian%20for%0Aimmersive%20visualization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04379v1&entry.124074799=Read"},
{"title": "Sat-DN: Implicit Surface Reconstruction from Multi-View Satellite Images\n  with Depth and Normal Supervision", "author": "Tianle Liu and Shuangming Zhao and Wanshou Jiang and Bingxuan Guo", "abstract": "  With advancements in satellite imaging technology, acquiring high-resolution\nmulti-view satellite imagery has become increasingly accessible, enabling rapid\nand location-independent ground model reconstruction. However, traditional\nstereo matching methods struggle to capture fine details, and while neural\nradiance fields (NeRFs) achieve high-quality reconstructions, their training\ntime is prohibitively long. Moreover, challenges such as low visibility of\nbuilding facades, illumination and style differences between pixels, and weakly\ntextured regions in satellite imagery further make it hard to reconstruct\nreasonable terrain geometry and detailed building facades. To address these\nissues, we propose Sat-DN, a novel framework leveraging a progressively trained\nmulti-resolution hash grid reconstruction architecture with explicit depth\nguidance and surface normal consistency constraints to enhance reconstruction\nquality. The multi-resolution hash grid accelerates training, while the\nprogressive strategy incrementally increases the learning frequency, using\ncoarse low-frequency geometry to guide the reconstruction of fine\nhigh-frequency details. The depth and normal constraints ensure a clear\nbuilding outline and correct planar distribution. Extensive experiments on the\nDFC2019 dataset demonstrate that Sat-DN outperforms existing methods, achieving\nstate-of-the-art results in both qualitative and quantitative evaluations. The\ncode is available at https://github.com/costune/SatDN.\n", "link": "http://arxiv.org/abs/2502.08352v2", "date": "2025-09-04", "relevancy": 2.9863, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6048}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5973}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sat-DN%3A%20Implicit%20Surface%20Reconstruction%20from%20Multi-View%20Satellite%20Images%0A%20%20with%20Depth%20and%20Normal%20Supervision&body=Title%3A%20Sat-DN%3A%20Implicit%20Surface%20Reconstruction%20from%20Multi-View%20Satellite%20Images%0A%20%20with%20Depth%20and%20Normal%20Supervision%0AAuthor%3A%20Tianle%20Liu%20and%20Shuangming%20Zhao%20and%20Wanshou%20Jiang%20and%20Bingxuan%20Guo%0AAbstract%3A%20%20%20With%20advancements%20in%20satellite%20imaging%20technology%2C%20acquiring%20high-resolution%0Amulti-view%20satellite%20imagery%20has%20become%20increasingly%20accessible%2C%20enabling%20rapid%0Aand%20location-independent%20ground%20model%20reconstruction.%20However%2C%20traditional%0Astereo%20matching%20methods%20struggle%20to%20capture%20fine%20details%2C%20and%20while%20neural%0Aradiance%20fields%20%28NeRFs%29%20achieve%20high-quality%20reconstructions%2C%20their%20training%0Atime%20is%20prohibitively%20long.%20Moreover%2C%20challenges%20such%20as%20low%20visibility%20of%0Abuilding%20facades%2C%20illumination%20and%20style%20differences%20between%20pixels%2C%20and%20weakly%0Atextured%20regions%20in%20satellite%20imagery%20further%20make%20it%20hard%20to%20reconstruct%0Areasonable%20terrain%20geometry%20and%20detailed%20building%20facades.%20To%20address%20these%0Aissues%2C%20we%20propose%20Sat-DN%2C%20a%20novel%20framework%20leveraging%20a%20progressively%20trained%0Amulti-resolution%20hash%20grid%20reconstruction%20architecture%20with%20explicit%20depth%0Aguidance%20and%20surface%20normal%20consistency%20constraints%20to%20enhance%20reconstruction%0Aquality.%20The%20multi-resolution%20hash%20grid%20accelerates%20training%2C%20while%20the%0Aprogressive%20strategy%20incrementally%20increases%20the%20learning%20frequency%2C%20using%0Acoarse%20low-frequency%20geometry%20to%20guide%20the%20reconstruction%20of%20fine%0Ahigh-frequency%20details.%20The%20depth%20and%20normal%20constraints%20ensure%20a%20clear%0Abuilding%20outline%20and%20correct%20planar%20distribution.%20Extensive%20experiments%20on%20the%0ADFC2019%20dataset%20demonstrate%20that%20Sat-DN%20outperforms%20existing%20methods%2C%20achieving%0Astate-of-the-art%20results%20in%20both%20qualitative%20and%20quantitative%20evaluations.%20The%0Acode%20is%20available%20at%20https%3A//github.com/costune/SatDN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08352v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSat-DN%253A%2520Implicit%2520Surface%2520Reconstruction%2520from%2520Multi-View%2520Satellite%2520Images%250A%2520%2520with%2520Depth%2520and%2520Normal%2520Supervision%26entry.906535625%3DTianle%2520Liu%2520and%2520Shuangming%2520Zhao%2520and%2520Wanshou%2520Jiang%2520and%2520Bingxuan%2520Guo%26entry.1292438233%3D%2520%2520With%2520advancements%2520in%2520satellite%2520imaging%2520technology%252C%2520acquiring%2520high-resolution%250Amulti-view%2520satellite%2520imagery%2520has%2520become%2520increasingly%2520accessible%252C%2520enabling%2520rapid%250Aand%2520location-independent%2520ground%2520model%2520reconstruction.%2520However%252C%2520traditional%250Astereo%2520matching%2520methods%2520struggle%2520to%2520capture%2520fine%2520details%252C%2520and%2520while%2520neural%250Aradiance%2520fields%2520%2528NeRFs%2529%2520achieve%2520high-quality%2520reconstructions%252C%2520their%2520training%250Atime%2520is%2520prohibitively%2520long.%2520Moreover%252C%2520challenges%2520such%2520as%2520low%2520visibility%2520of%250Abuilding%2520facades%252C%2520illumination%2520and%2520style%2520differences%2520between%2520pixels%252C%2520and%2520weakly%250Atextured%2520regions%2520in%2520satellite%2520imagery%2520further%2520make%2520it%2520hard%2520to%2520reconstruct%250Areasonable%2520terrain%2520geometry%2520and%2520detailed%2520building%2520facades.%2520To%2520address%2520these%250Aissues%252C%2520we%2520propose%2520Sat-DN%252C%2520a%2520novel%2520framework%2520leveraging%2520a%2520progressively%2520trained%250Amulti-resolution%2520hash%2520grid%2520reconstruction%2520architecture%2520with%2520explicit%2520depth%250Aguidance%2520and%2520surface%2520normal%2520consistency%2520constraints%2520to%2520enhance%2520reconstruction%250Aquality.%2520The%2520multi-resolution%2520hash%2520grid%2520accelerates%2520training%252C%2520while%2520the%250Aprogressive%2520strategy%2520incrementally%2520increases%2520the%2520learning%2520frequency%252C%2520using%250Acoarse%2520low-frequency%2520geometry%2520to%2520guide%2520the%2520reconstruction%2520of%2520fine%250Ahigh-frequency%2520details.%2520The%2520depth%2520and%2520normal%2520constraints%2520ensure%2520a%2520clear%250Abuilding%2520outline%2520and%2520correct%2520planar%2520distribution.%2520Extensive%2520experiments%2520on%2520the%250ADFC2019%2520dataset%2520demonstrate%2520that%2520Sat-DN%2520outperforms%2520existing%2520methods%252C%2520achieving%250Astate-of-the-art%2520results%2520in%2520both%2520qualitative%2520and%2520quantitative%2520evaluations.%2520The%250Acode%2520is%2520available%2520at%2520https%253A//github.com/costune/SatDN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08352v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sat-DN%3A%20Implicit%20Surface%20Reconstruction%20from%20Multi-View%20Satellite%20Images%0A%20%20with%20Depth%20and%20Normal%20Supervision&entry.906535625=Tianle%20Liu%20and%20Shuangming%20Zhao%20and%20Wanshou%20Jiang%20and%20Bingxuan%20Guo&entry.1292438233=%20%20With%20advancements%20in%20satellite%20imaging%20technology%2C%20acquiring%20high-resolution%0Amulti-view%20satellite%20imagery%20has%20become%20increasingly%20accessible%2C%20enabling%20rapid%0Aand%20location-independent%20ground%20model%20reconstruction.%20However%2C%20traditional%0Astereo%20matching%20methods%20struggle%20to%20capture%20fine%20details%2C%20and%20while%20neural%0Aradiance%20fields%20%28NeRFs%29%20achieve%20high-quality%20reconstructions%2C%20their%20training%0Atime%20is%20prohibitively%20long.%20Moreover%2C%20challenges%20such%20as%20low%20visibility%20of%0Abuilding%20facades%2C%20illumination%20and%20style%20differences%20between%20pixels%2C%20and%20weakly%0Atextured%20regions%20in%20satellite%20imagery%20further%20make%20it%20hard%20to%20reconstruct%0Areasonable%20terrain%20geometry%20and%20detailed%20building%20facades.%20To%20address%20these%0Aissues%2C%20we%20propose%20Sat-DN%2C%20a%20novel%20framework%20leveraging%20a%20progressively%20trained%0Amulti-resolution%20hash%20grid%20reconstruction%20architecture%20with%20explicit%20depth%0Aguidance%20and%20surface%20normal%20consistency%20constraints%20to%20enhance%20reconstruction%0Aquality.%20The%20multi-resolution%20hash%20grid%20accelerates%20training%2C%20while%20the%0Aprogressive%20strategy%20incrementally%20increases%20the%20learning%20frequency%2C%20using%0Acoarse%20low-frequency%20geometry%20to%20guide%20the%20reconstruction%20of%20fine%0Ahigh-frequency%20details.%20The%20depth%20and%20normal%20constraints%20ensure%20a%20clear%0Abuilding%20outline%20and%20correct%20planar%20distribution.%20Extensive%20experiments%20on%20the%0ADFC2019%20dataset%20demonstrate%20that%20Sat-DN%20outperforms%20existing%20methods%2C%20achieving%0Astate-of-the-art%20results%20in%20both%20qualitative%20and%20quantitative%20evaluations.%20The%0Acode%20is%20available%20at%20https%3A//github.com/costune/SatDN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08352v2&entry.124074799=Read"},
{"title": "Learning Active Perception via Self-Evolving Preference Optimization for\n  GUI Grounding", "author": "Wanfu Wang and Qipeng Huang and Guangquan Xue and Xiaobo Liang and Juntao Li", "abstract": "  Vision Language Models (VLMs) have recently achieved significant progress in\nbridging visual perception and linguistic reasoning. Recently, OpenAI o3 model\nintroduced a zoom-in search strategy that effectively elicits active perception\ncapabilities in VLMs, improving downstream task performance. However, enabling\nVLMs to reason effectively over appropriate image regions remains a core\nchallenge in GUI grounding, particularly under high-resolution inputs and\ncomplex multi-element visual interactions. In this work, we propose LASER, a\nself-evolving framework that progressively endows VLMs with multi-step\nperception capabilities, enabling precise coordinate prediction. Specifically,\nour approach integrate Monte Carlo quality estimation with\nIntersection-over-Union (IoU)-based region quality evaluation to jointly\nencourage both accuracy and diversity in constructing high-quality preference\ndata. This combination explicitly guides the model to focus on\ninstruction-relevant key regions while adaptively allocating reasoning steps\nbased on task complexity. Comprehensive experiments on the ScreenSpot Pro and\nScreenSpot-v2 benchmarks demonstrate consistent performance gains, validating\nthe effectiveness of our method. Furthermore, when fine-tuned on GTA1-7B, LASER\nachieves a score of 55.7 on the ScreenSpot-Pro benchmark, establishing a new\nstate-of-the-art (SoTA) among 7B-scale models.\n", "link": "http://arxiv.org/abs/2509.04243v1", "date": "2025-09-04", "relevancy": 2.9417, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5961}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5961}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Active%20Perception%20via%20Self-Evolving%20Preference%20Optimization%20for%0A%20%20GUI%20Grounding&body=Title%3A%20Learning%20Active%20Perception%20via%20Self-Evolving%20Preference%20Optimization%20for%0A%20%20GUI%20Grounding%0AAuthor%3A%20Wanfu%20Wang%20and%20Qipeng%20Huang%20and%20Guangquan%20Xue%20and%20Xiaobo%20Liang%20and%20Juntao%20Li%0AAbstract%3A%20%20%20Vision%20Language%20Models%20%28VLMs%29%20have%20recently%20achieved%20significant%20progress%20in%0Abridging%20visual%20perception%20and%20linguistic%20reasoning.%20Recently%2C%20OpenAI%20o3%20model%0Aintroduced%20a%20zoom-in%20search%20strategy%20that%20effectively%20elicits%20active%20perception%0Acapabilities%20in%20VLMs%2C%20improving%20downstream%20task%20performance.%20However%2C%20enabling%0AVLMs%20to%20reason%20effectively%20over%20appropriate%20image%20regions%20remains%20a%20core%0Achallenge%20in%20GUI%20grounding%2C%20particularly%20under%20high-resolution%20inputs%20and%0Acomplex%20multi-element%20visual%20interactions.%20In%20this%20work%2C%20we%20propose%20LASER%2C%20a%0Aself-evolving%20framework%20that%20progressively%20endows%20VLMs%20with%20multi-step%0Aperception%20capabilities%2C%20enabling%20precise%20coordinate%20prediction.%20Specifically%2C%0Aour%20approach%20integrate%20Monte%20Carlo%20quality%20estimation%20with%0AIntersection-over-Union%20%28IoU%29-based%20region%20quality%20evaluation%20to%20jointly%0Aencourage%20both%20accuracy%20and%20diversity%20in%20constructing%20high-quality%20preference%0Adata.%20This%20combination%20explicitly%20guides%20the%20model%20to%20focus%20on%0Ainstruction-relevant%20key%20regions%20while%20adaptively%20allocating%20reasoning%20steps%0Abased%20on%20task%20complexity.%20Comprehensive%20experiments%20on%20the%20ScreenSpot%20Pro%20and%0AScreenSpot-v2%20benchmarks%20demonstrate%20consistent%20performance%20gains%2C%20validating%0Athe%20effectiveness%20of%20our%20method.%20Furthermore%2C%20when%20fine-tuned%20on%20GTA1-7B%2C%20LASER%0Aachieves%20a%20score%20of%2055.7%20on%20the%20ScreenSpot-Pro%20benchmark%2C%20establishing%20a%20new%0Astate-of-the-art%20%28SoTA%29%20among%207B-scale%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04243v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Active%2520Perception%2520via%2520Self-Evolving%2520Preference%2520Optimization%2520for%250A%2520%2520GUI%2520Grounding%26entry.906535625%3DWanfu%2520Wang%2520and%2520Qipeng%2520Huang%2520and%2520Guangquan%2520Xue%2520and%2520Xiaobo%2520Liang%2520and%2520Juntao%2520Li%26entry.1292438233%3D%2520%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520have%2520recently%2520achieved%2520significant%2520progress%2520in%250Abridging%2520visual%2520perception%2520and%2520linguistic%2520reasoning.%2520Recently%252C%2520OpenAI%2520o3%2520model%250Aintroduced%2520a%2520zoom-in%2520search%2520strategy%2520that%2520effectively%2520elicits%2520active%2520perception%250Acapabilities%2520in%2520VLMs%252C%2520improving%2520downstream%2520task%2520performance.%2520However%252C%2520enabling%250AVLMs%2520to%2520reason%2520effectively%2520over%2520appropriate%2520image%2520regions%2520remains%2520a%2520core%250Achallenge%2520in%2520GUI%2520grounding%252C%2520particularly%2520under%2520high-resolution%2520inputs%2520and%250Acomplex%2520multi-element%2520visual%2520interactions.%2520In%2520this%2520work%252C%2520we%2520propose%2520LASER%252C%2520a%250Aself-evolving%2520framework%2520that%2520progressively%2520endows%2520VLMs%2520with%2520multi-step%250Aperception%2520capabilities%252C%2520enabling%2520precise%2520coordinate%2520prediction.%2520Specifically%252C%250Aour%2520approach%2520integrate%2520Monte%2520Carlo%2520quality%2520estimation%2520with%250AIntersection-over-Union%2520%2528IoU%2529-based%2520region%2520quality%2520evaluation%2520to%2520jointly%250Aencourage%2520both%2520accuracy%2520and%2520diversity%2520in%2520constructing%2520high-quality%2520preference%250Adata.%2520This%2520combination%2520explicitly%2520guides%2520the%2520model%2520to%2520focus%2520on%250Ainstruction-relevant%2520key%2520regions%2520while%2520adaptively%2520allocating%2520reasoning%2520steps%250Abased%2520on%2520task%2520complexity.%2520Comprehensive%2520experiments%2520on%2520the%2520ScreenSpot%2520Pro%2520and%250AScreenSpot-v2%2520benchmarks%2520demonstrate%2520consistent%2520performance%2520gains%252C%2520validating%250Athe%2520effectiveness%2520of%2520our%2520method.%2520Furthermore%252C%2520when%2520fine-tuned%2520on%2520GTA1-7B%252C%2520LASER%250Aachieves%2520a%2520score%2520of%252055.7%2520on%2520the%2520ScreenSpot-Pro%2520benchmark%252C%2520establishing%2520a%2520new%250Astate-of-the-art%2520%2528SoTA%2529%2520among%25207B-scale%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04243v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Active%20Perception%20via%20Self-Evolving%20Preference%20Optimization%20for%0A%20%20GUI%20Grounding&entry.906535625=Wanfu%20Wang%20and%20Qipeng%20Huang%20and%20Guangquan%20Xue%20and%20Xiaobo%20Liang%20and%20Juntao%20Li&entry.1292438233=%20%20Vision%20Language%20Models%20%28VLMs%29%20have%20recently%20achieved%20significant%20progress%20in%0Abridging%20visual%20perception%20and%20linguistic%20reasoning.%20Recently%2C%20OpenAI%20o3%20model%0Aintroduced%20a%20zoom-in%20search%20strategy%20that%20effectively%20elicits%20active%20perception%0Acapabilities%20in%20VLMs%2C%20improving%20downstream%20task%20performance.%20However%2C%20enabling%0AVLMs%20to%20reason%20effectively%20over%20appropriate%20image%20regions%20remains%20a%20core%0Achallenge%20in%20GUI%20grounding%2C%20particularly%20under%20high-resolution%20inputs%20and%0Acomplex%20multi-element%20visual%20interactions.%20In%20this%20work%2C%20we%20propose%20LASER%2C%20a%0Aself-evolving%20framework%20that%20progressively%20endows%20VLMs%20with%20multi-step%0Aperception%20capabilities%2C%20enabling%20precise%20coordinate%20prediction.%20Specifically%2C%0Aour%20approach%20integrate%20Monte%20Carlo%20quality%20estimation%20with%0AIntersection-over-Union%20%28IoU%29-based%20region%20quality%20evaluation%20to%20jointly%0Aencourage%20both%20accuracy%20and%20diversity%20in%20constructing%20high-quality%20preference%0Adata.%20This%20combination%20explicitly%20guides%20the%20model%20to%20focus%20on%0Ainstruction-relevant%20key%20regions%20while%20adaptively%20allocating%20reasoning%20steps%0Abased%20on%20task%20complexity.%20Comprehensive%20experiments%20on%20the%20ScreenSpot%20Pro%20and%0AScreenSpot-v2%20benchmarks%20demonstrate%20consistent%20performance%20gains%2C%20validating%0Athe%20effectiveness%20of%20our%20method.%20Furthermore%2C%20when%20fine-tuned%20on%20GTA1-7B%2C%20LASER%0Aachieves%20a%20score%20of%2055.7%20on%20the%20ScreenSpot-Pro%20benchmark%2C%20establishing%20a%20new%0Astate-of-the-art%20%28SoTA%29%20among%207B-scale%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04243v1&entry.124074799=Read"},
{"title": "PAOLI: Pose-free Articulated Object Learning from Sparse-view Images", "author": "Jianning Deng and Kartic Subr and Hakan Bilen", "abstract": "  We present a novel self-supervised framework for learning articulated object\nrepresentations from sparse-view, unposed images. Unlike prior methods that\nrequire dense multi-view observations and ground-truth camera poses, our\napproach operates with as few as four views per articulation and no camera\nsupervision. To address the inherent challenges, we first reconstruct each\narticulation independently using recent advances in sparse-view 3D\nreconstruction, then learn a deformation field that establishes dense\ncorrespondences across poses. A progressive disentanglement strategy further\nseparates static from moving parts, enabling robust separation of camera and\nobject motion. Finally, we jointly optimize geometry, appearance, and\nkinematics with a self-supervised loss that enforces cross-view and cross-pose\nconsistency. Experiments on the standard benchmark and real-world examples\ndemonstrate that our method produces accurate and detailed articulated object\nrepresentations under significantly weaker input assumptions than existing\napproaches.\n", "link": "http://arxiv.org/abs/2509.04276v1", "date": "2025-09-04", "relevancy": 2.9116, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5976}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.58}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAOLI%3A%20Pose-free%20Articulated%20Object%20Learning%20from%20Sparse-view%20Images&body=Title%3A%20PAOLI%3A%20Pose-free%20Articulated%20Object%20Learning%20from%20Sparse-view%20Images%0AAuthor%3A%20Jianning%20Deng%20and%20Kartic%20Subr%20and%20Hakan%20Bilen%0AAbstract%3A%20%20%20We%20present%20a%20novel%20self-supervised%20framework%20for%20learning%20articulated%20object%0Arepresentations%20from%20sparse-view%2C%20unposed%20images.%20Unlike%20prior%20methods%20that%0Arequire%20dense%20multi-view%20observations%20and%20ground-truth%20camera%20poses%2C%20our%0Aapproach%20operates%20with%20as%20few%20as%20four%20views%20per%20articulation%20and%20no%20camera%0Asupervision.%20To%20address%20the%20inherent%20challenges%2C%20we%20first%20reconstruct%20each%0Aarticulation%20independently%20using%20recent%20advances%20in%20sparse-view%203D%0Areconstruction%2C%20then%20learn%20a%20deformation%20field%20that%20establishes%20dense%0Acorrespondences%20across%20poses.%20A%20progressive%20disentanglement%20strategy%20further%0Aseparates%20static%20from%20moving%20parts%2C%20enabling%20robust%20separation%20of%20camera%20and%0Aobject%20motion.%20Finally%2C%20we%20jointly%20optimize%20geometry%2C%20appearance%2C%20and%0Akinematics%20with%20a%20self-supervised%20loss%20that%20enforces%20cross-view%20and%20cross-pose%0Aconsistency.%20Experiments%20on%20the%20standard%20benchmark%20and%20real-world%20examples%0Ademonstrate%20that%20our%20method%20produces%20accurate%20and%20detailed%20articulated%20object%0Arepresentations%20under%20significantly%20weaker%20input%20assumptions%20than%20existing%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04276v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAOLI%253A%2520Pose-free%2520Articulated%2520Object%2520Learning%2520from%2520Sparse-view%2520Images%26entry.906535625%3DJianning%2520Deng%2520and%2520Kartic%2520Subr%2520and%2520Hakan%2520Bilen%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520self-supervised%2520framework%2520for%2520learning%2520articulated%2520object%250Arepresentations%2520from%2520sparse-view%252C%2520unposed%2520images.%2520Unlike%2520prior%2520methods%2520that%250Arequire%2520dense%2520multi-view%2520observations%2520and%2520ground-truth%2520camera%2520poses%252C%2520our%250Aapproach%2520operates%2520with%2520as%2520few%2520as%2520four%2520views%2520per%2520articulation%2520and%2520no%2520camera%250Asupervision.%2520To%2520address%2520the%2520inherent%2520challenges%252C%2520we%2520first%2520reconstruct%2520each%250Aarticulation%2520independently%2520using%2520recent%2520advances%2520in%2520sparse-view%25203D%250Areconstruction%252C%2520then%2520learn%2520a%2520deformation%2520field%2520that%2520establishes%2520dense%250Acorrespondences%2520across%2520poses.%2520A%2520progressive%2520disentanglement%2520strategy%2520further%250Aseparates%2520static%2520from%2520moving%2520parts%252C%2520enabling%2520robust%2520separation%2520of%2520camera%2520and%250Aobject%2520motion.%2520Finally%252C%2520we%2520jointly%2520optimize%2520geometry%252C%2520appearance%252C%2520and%250Akinematics%2520with%2520a%2520self-supervised%2520loss%2520that%2520enforces%2520cross-view%2520and%2520cross-pose%250Aconsistency.%2520Experiments%2520on%2520the%2520standard%2520benchmark%2520and%2520real-world%2520examples%250Ademonstrate%2520that%2520our%2520method%2520produces%2520accurate%2520and%2520detailed%2520articulated%2520object%250Arepresentations%2520under%2520significantly%2520weaker%2520input%2520assumptions%2520than%2520existing%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04276v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAOLI%3A%20Pose-free%20Articulated%20Object%20Learning%20from%20Sparse-view%20Images&entry.906535625=Jianning%20Deng%20and%20Kartic%20Subr%20and%20Hakan%20Bilen&entry.1292438233=%20%20We%20present%20a%20novel%20self-supervised%20framework%20for%20learning%20articulated%20object%0Arepresentations%20from%20sparse-view%2C%20unposed%20images.%20Unlike%20prior%20methods%20that%0Arequire%20dense%20multi-view%20observations%20and%20ground-truth%20camera%20poses%2C%20our%0Aapproach%20operates%20with%20as%20few%20as%20four%20views%20per%20articulation%20and%20no%20camera%0Asupervision.%20To%20address%20the%20inherent%20challenges%2C%20we%20first%20reconstruct%20each%0Aarticulation%20independently%20using%20recent%20advances%20in%20sparse-view%203D%0Areconstruction%2C%20then%20learn%20a%20deformation%20field%20that%20establishes%20dense%0Acorrespondences%20across%20poses.%20A%20progressive%20disentanglement%20strategy%20further%0Aseparates%20static%20from%20moving%20parts%2C%20enabling%20robust%20separation%20of%20camera%20and%0Aobject%20motion.%20Finally%2C%20we%20jointly%20optimize%20geometry%2C%20appearance%2C%20and%0Akinematics%20with%20a%20self-supervised%20loss%20that%20enforces%20cross-view%20and%20cross-pose%0Aconsistency.%20Experiments%20on%20the%20standard%20benchmark%20and%20real-world%20examples%0Ademonstrate%20that%20our%20method%20produces%20accurate%20and%20detailed%20articulated%20object%0Arepresentations%20under%20significantly%20weaker%20input%20assumptions%20than%20existing%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04276v1&entry.124074799=Read"},
{"title": "Imitating Radiological Scrolling: A Global-Local Attention Model for 3D\n  Chest CT Volumes Multi-Label Anomaly Classification", "author": "Theo Di Piazza and Carole Lazarus and Olivier Nempont and Loic Boussel", "abstract": "  The rapid increase in the number of Computed Tomography (CT) scan\nexaminations has created an urgent need for automated tools, such as organ\nsegmentation, anomaly classification, and report generation, to assist\nradiologists with their growing workload. Multi-label classification of\nThree-Dimensional (3D) CT scans is a challenging task due to the volumetric\nnature of the data and the variety of anomalies to be detected. Existing deep\nlearning methods based on Convolutional Neural Networks (CNNs) struggle to\ncapture long-range dependencies effectively, while Vision Transformers require\nextensive pre-training, posing challenges for practical use. Additionally,\nthese existing methods do not explicitly model the radiologist's navigational\nbehavior while scrolling through CT scan slices, which requires both global\ncontext understanding and local detail awareness. In this study, we present\nCT-Scroll, a novel global-local attention model specifically designed to\nemulate the scrolling behavior of radiologists during the analysis of 3D CT\nscans. Our approach is evaluated on two public datasets, demonstrating its\nefficacy through comprehensive experiments and an ablation study that\nhighlights the contribution of each model component.\n", "link": "http://arxiv.org/abs/2503.20652v5", "date": "2025-09-04", "relevancy": 2.8925, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5878}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5739}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imitating%20Radiological%20Scrolling%3A%20A%20Global-Local%20Attention%20Model%20for%203D%0A%20%20Chest%20CT%20Volumes%20Multi-Label%20Anomaly%20Classification&body=Title%3A%20Imitating%20Radiological%20Scrolling%3A%20A%20Global-Local%20Attention%20Model%20for%203D%0A%20%20Chest%20CT%20Volumes%20Multi-Label%20Anomaly%20Classification%0AAuthor%3A%20Theo%20Di%20Piazza%20and%20Carole%20Lazarus%20and%20Olivier%20Nempont%20and%20Loic%20Boussel%0AAbstract%3A%20%20%20The%20rapid%20increase%20in%20the%20number%20of%20Computed%20Tomography%20%28CT%29%20scan%0Aexaminations%20has%20created%20an%20urgent%20need%20for%20automated%20tools%2C%20such%20as%20organ%0Asegmentation%2C%20anomaly%20classification%2C%20and%20report%20generation%2C%20to%20assist%0Aradiologists%20with%20their%20growing%20workload.%20Multi-label%20classification%20of%0AThree-Dimensional%20%283D%29%20CT%20scans%20is%20a%20challenging%20task%20due%20to%20the%20volumetric%0Anature%20of%20the%20data%20and%20the%20variety%20of%20anomalies%20to%20be%20detected.%20Existing%20deep%0Alearning%20methods%20based%20on%20Convolutional%20Neural%20Networks%20%28CNNs%29%20struggle%20to%0Acapture%20long-range%20dependencies%20effectively%2C%20while%20Vision%20Transformers%20require%0Aextensive%20pre-training%2C%20posing%20challenges%20for%20practical%20use.%20Additionally%2C%0Athese%20existing%20methods%20do%20not%20explicitly%20model%20the%20radiologist%27s%20navigational%0Abehavior%20while%20scrolling%20through%20CT%20scan%20slices%2C%20which%20requires%20both%20global%0Acontext%20understanding%20and%20local%20detail%20awareness.%20In%20this%20study%2C%20we%20present%0ACT-Scroll%2C%20a%20novel%20global-local%20attention%20model%20specifically%20designed%20to%0Aemulate%20the%20scrolling%20behavior%20of%20radiologists%20during%20the%20analysis%20of%203D%20CT%0Ascans.%20Our%20approach%20is%20evaluated%20on%20two%20public%20datasets%2C%20demonstrating%20its%0Aefficacy%20through%20comprehensive%20experiments%20and%20an%20ablation%20study%20that%0Ahighlights%20the%20contribution%20of%20each%20model%20component.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.20652v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImitating%2520Radiological%2520Scrolling%253A%2520A%2520Global-Local%2520Attention%2520Model%2520for%25203D%250A%2520%2520Chest%2520CT%2520Volumes%2520Multi-Label%2520Anomaly%2520Classification%26entry.906535625%3DTheo%2520Di%2520Piazza%2520and%2520Carole%2520Lazarus%2520and%2520Olivier%2520Nempont%2520and%2520Loic%2520Boussel%26entry.1292438233%3D%2520%2520The%2520rapid%2520increase%2520in%2520the%2520number%2520of%2520Computed%2520Tomography%2520%2528CT%2529%2520scan%250Aexaminations%2520has%2520created%2520an%2520urgent%2520need%2520for%2520automated%2520tools%252C%2520such%2520as%2520organ%250Asegmentation%252C%2520anomaly%2520classification%252C%2520and%2520report%2520generation%252C%2520to%2520assist%250Aradiologists%2520with%2520their%2520growing%2520workload.%2520Multi-label%2520classification%2520of%250AThree-Dimensional%2520%25283D%2529%2520CT%2520scans%2520is%2520a%2520challenging%2520task%2520due%2520to%2520the%2520volumetric%250Anature%2520of%2520the%2520data%2520and%2520the%2520variety%2520of%2520anomalies%2520to%2520be%2520detected.%2520Existing%2520deep%250Alearning%2520methods%2520based%2520on%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520struggle%2520to%250Acapture%2520long-range%2520dependencies%2520effectively%252C%2520while%2520Vision%2520Transformers%2520require%250Aextensive%2520pre-training%252C%2520posing%2520challenges%2520for%2520practical%2520use.%2520Additionally%252C%250Athese%2520existing%2520methods%2520do%2520not%2520explicitly%2520model%2520the%2520radiologist%2527s%2520navigational%250Abehavior%2520while%2520scrolling%2520through%2520CT%2520scan%2520slices%252C%2520which%2520requires%2520both%2520global%250Acontext%2520understanding%2520and%2520local%2520detail%2520awareness.%2520In%2520this%2520study%252C%2520we%2520present%250ACT-Scroll%252C%2520a%2520novel%2520global-local%2520attention%2520model%2520specifically%2520designed%2520to%250Aemulate%2520the%2520scrolling%2520behavior%2520of%2520radiologists%2520during%2520the%2520analysis%2520of%25203D%2520CT%250Ascans.%2520Our%2520approach%2520is%2520evaluated%2520on%2520two%2520public%2520datasets%252C%2520demonstrating%2520its%250Aefficacy%2520through%2520comprehensive%2520experiments%2520and%2520an%2520ablation%2520study%2520that%250Ahighlights%2520the%2520contribution%2520of%2520each%2520model%2520component.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.20652v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imitating%20Radiological%20Scrolling%3A%20A%20Global-Local%20Attention%20Model%20for%203D%0A%20%20Chest%20CT%20Volumes%20Multi-Label%20Anomaly%20Classification&entry.906535625=Theo%20Di%20Piazza%20and%20Carole%20Lazarus%20and%20Olivier%20Nempont%20and%20Loic%20Boussel&entry.1292438233=%20%20The%20rapid%20increase%20in%20the%20number%20of%20Computed%20Tomography%20%28CT%29%20scan%0Aexaminations%20has%20created%20an%20urgent%20need%20for%20automated%20tools%2C%20such%20as%20organ%0Asegmentation%2C%20anomaly%20classification%2C%20and%20report%20generation%2C%20to%20assist%0Aradiologists%20with%20their%20growing%20workload.%20Multi-label%20classification%20of%0AThree-Dimensional%20%283D%29%20CT%20scans%20is%20a%20challenging%20task%20due%20to%20the%20volumetric%0Anature%20of%20the%20data%20and%20the%20variety%20of%20anomalies%20to%20be%20detected.%20Existing%20deep%0Alearning%20methods%20based%20on%20Convolutional%20Neural%20Networks%20%28CNNs%29%20struggle%20to%0Acapture%20long-range%20dependencies%20effectively%2C%20while%20Vision%20Transformers%20require%0Aextensive%20pre-training%2C%20posing%20challenges%20for%20practical%20use.%20Additionally%2C%0Athese%20existing%20methods%20do%20not%20explicitly%20model%20the%20radiologist%27s%20navigational%0Abehavior%20while%20scrolling%20through%20CT%20scan%20slices%2C%20which%20requires%20both%20global%0Acontext%20understanding%20and%20local%20detail%20awareness.%20In%20this%20study%2C%20we%20present%0ACT-Scroll%2C%20a%20novel%20global-local%20attention%20model%20specifically%20designed%20to%0Aemulate%20the%20scrolling%20behavior%20of%20radiologists%20during%20the%20analysis%20of%203D%20CT%0Ascans.%20Our%20approach%20is%20evaluated%20on%20two%20public%20datasets%2C%20demonstrating%20its%0Aefficacy%20through%20comprehensive%20experiments%20and%20an%20ablation%20study%20that%0Ahighlights%20the%20contribution%20of%20each%20model%20component.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.20652v5&entry.124074799=Read"},
{"title": "One Flight Over the Gap: A Survey from Perspective to Panoramic Vision", "author": "Xin Lin and Xian Ge and Dizhe Zhang and Zhaoliang Wan and Xianshun Wang and Xiangtai Li and Wenjie Jiang and Bo Du and Dacheng Tao and Ming-Hsuan Yang and Lu Qi", "abstract": "  Driven by the demand for spatial intelligence and holistic scene perception,\nomnidirectional images (ODIs), which provide a complete 360\\textdegree{} field\nof view, are receiving growing attention across diverse applications such as\nvirtual reality, autonomous driving, and embodied robotics. Despite their\nunique characteristics, ODIs exhibit remarkable differences from perspective\nimages in geometric projection, spatial distribution, and boundary continuity,\nmaking it challenging for direct domain adaption from perspective methods. This\nsurvey reviews recent panoramic vision techniques with a particular emphasis on\nthe perspective-to-panorama adaptation. We first revisit the panoramic imaging\npipeline and projection methods to build the prior knowledge required for\nanalyzing the structural disparities. Then, we summarize three challenges of\ndomain adaptation: severe geometric distortions near the poles, non-uniform\nsampling in Equirectangular Projection (ERP), and periodic boundary continuity.\nBuilding on this, we cover 20+ representative tasks drawn from more than 300\nresearch papers in two dimensions. On one hand, we present a cross-method\nanalysis of representative strategies for addressing panoramic specific\nchallenges across different tasks. On the other hand, we conduct a cross-task\ncomparison and classify panoramic vision into four major categories: visual\nquality enhancement and assessment, visual understanding, multimodal\nunderstanding, and visual generation. In addition, we discuss open challenges\nand future directions in data, models, and applications that will drive the\nadvancement of panoramic vision research. We hope that our work can provide new\ninsight and forward looking perspectives to advance the development of\npanoramic vision technologies. Our project page is\nhttps://insta360-research-team.github.io/Survey-of-Panorama\n", "link": "http://arxiv.org/abs/2509.04444v1", "date": "2025-09-04", "relevancy": 2.8762, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5785}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5736}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20Flight%20Over%20the%20Gap%3A%20A%20Survey%20from%20Perspective%20to%20Panoramic%20Vision&body=Title%3A%20One%20Flight%20Over%20the%20Gap%3A%20A%20Survey%20from%20Perspective%20to%20Panoramic%20Vision%0AAuthor%3A%20Xin%20Lin%20and%20Xian%20Ge%20and%20Dizhe%20Zhang%20and%20Zhaoliang%20Wan%20and%20Xianshun%20Wang%20and%20Xiangtai%20Li%20and%20Wenjie%20Jiang%20and%20Bo%20Du%20and%20Dacheng%20Tao%20and%20Ming-Hsuan%20Yang%20and%20Lu%20Qi%0AAbstract%3A%20%20%20Driven%20by%20the%20demand%20for%20spatial%20intelligence%20and%20holistic%20scene%20perception%2C%0Aomnidirectional%20images%20%28ODIs%29%2C%20which%20provide%20a%20complete%20360%5Ctextdegree%7B%7D%20field%0Aof%20view%2C%20are%20receiving%20growing%20attention%20across%20diverse%20applications%20such%20as%0Avirtual%20reality%2C%20autonomous%20driving%2C%20and%20embodied%20robotics.%20Despite%20their%0Aunique%20characteristics%2C%20ODIs%20exhibit%20remarkable%20differences%20from%20perspective%0Aimages%20in%20geometric%20projection%2C%20spatial%20distribution%2C%20and%20boundary%20continuity%2C%0Amaking%20it%20challenging%20for%20direct%20domain%20adaption%20from%20perspective%20methods.%20This%0Asurvey%20reviews%20recent%20panoramic%20vision%20techniques%20with%20a%20particular%20emphasis%20on%0Athe%20perspective-to-panorama%20adaptation.%20We%20first%20revisit%20the%20panoramic%20imaging%0Apipeline%20and%20projection%20methods%20to%20build%20the%20prior%20knowledge%20required%20for%0Aanalyzing%20the%20structural%20disparities.%20Then%2C%20we%20summarize%20three%20challenges%20of%0Adomain%20adaptation%3A%20severe%20geometric%20distortions%20near%20the%20poles%2C%20non-uniform%0Asampling%20in%20Equirectangular%20Projection%20%28ERP%29%2C%20and%20periodic%20boundary%20continuity.%0ABuilding%20on%20this%2C%20we%20cover%2020%2B%20representative%20tasks%20drawn%20from%20more%20than%20300%0Aresearch%20papers%20in%20two%20dimensions.%20On%20one%20hand%2C%20we%20present%20a%20cross-method%0Aanalysis%20of%20representative%20strategies%20for%20addressing%20panoramic%20specific%0Achallenges%20across%20different%20tasks.%20On%20the%20other%20hand%2C%20we%20conduct%20a%20cross-task%0Acomparison%20and%20classify%20panoramic%20vision%20into%20four%20major%20categories%3A%20visual%0Aquality%20enhancement%20and%20assessment%2C%20visual%20understanding%2C%20multimodal%0Aunderstanding%2C%20and%20visual%20generation.%20In%20addition%2C%20we%20discuss%20open%20challenges%0Aand%20future%20directions%20in%20data%2C%20models%2C%20and%20applications%20that%20will%20drive%20the%0Aadvancement%20of%20panoramic%20vision%20research.%20We%20hope%20that%20our%20work%20can%20provide%20new%0Ainsight%20and%20forward%20looking%20perspectives%20to%20advance%20the%20development%20of%0Apanoramic%20vision%20technologies.%20Our%20project%20page%20is%0Ahttps%3A//insta360-research-team.github.io/Survey-of-Panorama%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04444v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520Flight%2520Over%2520the%2520Gap%253A%2520A%2520Survey%2520from%2520Perspective%2520to%2520Panoramic%2520Vision%26entry.906535625%3DXin%2520Lin%2520and%2520Xian%2520Ge%2520and%2520Dizhe%2520Zhang%2520and%2520Zhaoliang%2520Wan%2520and%2520Xianshun%2520Wang%2520and%2520Xiangtai%2520Li%2520and%2520Wenjie%2520Jiang%2520and%2520Bo%2520Du%2520and%2520Dacheng%2520Tao%2520and%2520Ming-Hsuan%2520Yang%2520and%2520Lu%2520Qi%26entry.1292438233%3D%2520%2520Driven%2520by%2520the%2520demand%2520for%2520spatial%2520intelligence%2520and%2520holistic%2520scene%2520perception%252C%250Aomnidirectional%2520images%2520%2528ODIs%2529%252C%2520which%2520provide%2520a%2520complete%2520360%255Ctextdegree%257B%257D%2520field%250Aof%2520view%252C%2520are%2520receiving%2520growing%2520attention%2520across%2520diverse%2520applications%2520such%2520as%250Avirtual%2520reality%252C%2520autonomous%2520driving%252C%2520and%2520embodied%2520robotics.%2520Despite%2520their%250Aunique%2520characteristics%252C%2520ODIs%2520exhibit%2520remarkable%2520differences%2520from%2520perspective%250Aimages%2520in%2520geometric%2520projection%252C%2520spatial%2520distribution%252C%2520and%2520boundary%2520continuity%252C%250Amaking%2520it%2520challenging%2520for%2520direct%2520domain%2520adaption%2520from%2520perspective%2520methods.%2520This%250Asurvey%2520reviews%2520recent%2520panoramic%2520vision%2520techniques%2520with%2520a%2520particular%2520emphasis%2520on%250Athe%2520perspective-to-panorama%2520adaptation.%2520We%2520first%2520revisit%2520the%2520panoramic%2520imaging%250Apipeline%2520and%2520projection%2520methods%2520to%2520build%2520the%2520prior%2520knowledge%2520required%2520for%250Aanalyzing%2520the%2520structural%2520disparities.%2520Then%252C%2520we%2520summarize%2520three%2520challenges%2520of%250Adomain%2520adaptation%253A%2520severe%2520geometric%2520distortions%2520near%2520the%2520poles%252C%2520non-uniform%250Asampling%2520in%2520Equirectangular%2520Projection%2520%2528ERP%2529%252C%2520and%2520periodic%2520boundary%2520continuity.%250ABuilding%2520on%2520this%252C%2520we%2520cover%252020%252B%2520representative%2520tasks%2520drawn%2520from%2520more%2520than%2520300%250Aresearch%2520papers%2520in%2520two%2520dimensions.%2520On%2520one%2520hand%252C%2520we%2520present%2520a%2520cross-method%250Aanalysis%2520of%2520representative%2520strategies%2520for%2520addressing%2520panoramic%2520specific%250Achallenges%2520across%2520different%2520tasks.%2520On%2520the%2520other%2520hand%252C%2520we%2520conduct%2520a%2520cross-task%250Acomparison%2520and%2520classify%2520panoramic%2520vision%2520into%2520four%2520major%2520categories%253A%2520visual%250Aquality%2520enhancement%2520and%2520assessment%252C%2520visual%2520understanding%252C%2520multimodal%250Aunderstanding%252C%2520and%2520visual%2520generation.%2520In%2520addition%252C%2520we%2520discuss%2520open%2520challenges%250Aand%2520future%2520directions%2520in%2520data%252C%2520models%252C%2520and%2520applications%2520that%2520will%2520drive%2520the%250Aadvancement%2520of%2520panoramic%2520vision%2520research.%2520We%2520hope%2520that%2520our%2520work%2520can%2520provide%2520new%250Ainsight%2520and%2520forward%2520looking%2520perspectives%2520to%2520advance%2520the%2520development%2520of%250Apanoramic%2520vision%2520technologies.%2520Our%2520project%2520page%2520is%250Ahttps%253A//insta360-research-team.github.io/Survey-of-Panorama%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04444v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20Flight%20Over%20the%20Gap%3A%20A%20Survey%20from%20Perspective%20to%20Panoramic%20Vision&entry.906535625=Xin%20Lin%20and%20Xian%20Ge%20and%20Dizhe%20Zhang%20and%20Zhaoliang%20Wan%20and%20Xianshun%20Wang%20and%20Xiangtai%20Li%20and%20Wenjie%20Jiang%20and%20Bo%20Du%20and%20Dacheng%20Tao%20and%20Ming-Hsuan%20Yang%20and%20Lu%20Qi&entry.1292438233=%20%20Driven%20by%20the%20demand%20for%20spatial%20intelligence%20and%20holistic%20scene%20perception%2C%0Aomnidirectional%20images%20%28ODIs%29%2C%20which%20provide%20a%20complete%20360%5Ctextdegree%7B%7D%20field%0Aof%20view%2C%20are%20receiving%20growing%20attention%20across%20diverse%20applications%20such%20as%0Avirtual%20reality%2C%20autonomous%20driving%2C%20and%20embodied%20robotics.%20Despite%20their%0Aunique%20characteristics%2C%20ODIs%20exhibit%20remarkable%20differences%20from%20perspective%0Aimages%20in%20geometric%20projection%2C%20spatial%20distribution%2C%20and%20boundary%20continuity%2C%0Amaking%20it%20challenging%20for%20direct%20domain%20adaption%20from%20perspective%20methods.%20This%0Asurvey%20reviews%20recent%20panoramic%20vision%20techniques%20with%20a%20particular%20emphasis%20on%0Athe%20perspective-to-panorama%20adaptation.%20We%20first%20revisit%20the%20panoramic%20imaging%0Apipeline%20and%20projection%20methods%20to%20build%20the%20prior%20knowledge%20required%20for%0Aanalyzing%20the%20structural%20disparities.%20Then%2C%20we%20summarize%20three%20challenges%20of%0Adomain%20adaptation%3A%20severe%20geometric%20distortions%20near%20the%20poles%2C%20non-uniform%0Asampling%20in%20Equirectangular%20Projection%20%28ERP%29%2C%20and%20periodic%20boundary%20continuity.%0ABuilding%20on%20this%2C%20we%20cover%2020%2B%20representative%20tasks%20drawn%20from%20more%20than%20300%0Aresearch%20papers%20in%20two%20dimensions.%20On%20one%20hand%2C%20we%20present%20a%20cross-method%0Aanalysis%20of%20representative%20strategies%20for%20addressing%20panoramic%20specific%0Achallenges%20across%20different%20tasks.%20On%20the%20other%20hand%2C%20we%20conduct%20a%20cross-task%0Acomparison%20and%20classify%20panoramic%20vision%20into%20four%20major%20categories%3A%20visual%0Aquality%20enhancement%20and%20assessment%2C%20visual%20understanding%2C%20multimodal%0Aunderstanding%2C%20and%20visual%20generation.%20In%20addition%2C%20we%20discuss%20open%20challenges%0Aand%20future%20directions%20in%20data%2C%20models%2C%20and%20applications%20that%20will%20drive%20the%0Aadvancement%20of%20panoramic%20vision%20research.%20We%20hope%20that%20our%20work%20can%20provide%20new%0Ainsight%20and%20forward%20looking%20perspectives%20to%20advance%20the%20development%20of%0Apanoramic%20vision%20technologies.%20Our%20project%20page%20is%0Ahttps%3A//insta360-research-team.github.io/Survey-of-Panorama%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04444v1&entry.124074799=Read"},
{"title": "TriLiteNet: Lightweight Model for Multi-Task Visual Perception", "author": "Quang-Huy Che and Duc-Khai Lam", "abstract": "  Efficient perception models are essential for Advanced Driver Assistance\nSystems (ADAS), as these applications require rapid processing and response to\nensure safety and effectiveness in real-world environments. To address the\nreal-time execution needs of such perception models, this study introduces the\nTriLiteNet model. This model can simultaneously manage multiple tasks related\nto panoramic driving perception. TriLiteNet is designed to optimize performance\nwhile maintaining low computational costs. Experimental results on the BDD100k\ndataset demonstrate that the model achieves competitive performance across\nthree key tasks: vehicle detection, drivable area segmentation, and lane line\nsegmentation. Specifically, the TriLiteNet_{base} demonstrated a recall of\n85.6% for vehicle detection, a mean Intersection over Union (mIoU) of 92.4% for\ndrivable area segmentation, and an Acc of 82.3% for lane line segmentation with\nonly 2.35M parameters and a computational cost of 7.72 GFLOPs. Our proposed\nmodel includes a tiny configuration with just 0.14M parameters, which provides\na multi-task solution with minimal computational demand. Evaluated for latency\nand power consumption on embedded devices, TriLiteNet in both configurations\nshows low latency and reasonable power during inference. By balancing\nperformance, computational efficiency, and scalability, TriLiteNet offers a\npractical and deployable solution for real-world autonomous driving\napplications. Code is available at https://github.com/chequanghuy/TriLiteNet.\n", "link": "http://arxiv.org/abs/2509.04092v1", "date": "2025-09-04", "relevancy": 2.8678, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5762}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5751}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TriLiteNet%3A%20Lightweight%20Model%20for%20Multi-Task%20Visual%20Perception&body=Title%3A%20TriLiteNet%3A%20Lightweight%20Model%20for%20Multi-Task%20Visual%20Perception%0AAuthor%3A%20Quang-Huy%20Che%20and%20Duc-Khai%20Lam%0AAbstract%3A%20%20%20Efficient%20perception%20models%20are%20essential%20for%20Advanced%20Driver%20Assistance%0ASystems%20%28ADAS%29%2C%20as%20these%20applications%20require%20rapid%20processing%20and%20response%20to%0Aensure%20safety%20and%20effectiveness%20in%20real-world%20environments.%20To%20address%20the%0Areal-time%20execution%20needs%20of%20such%20perception%20models%2C%20this%20study%20introduces%20the%0ATriLiteNet%20model.%20This%20model%20can%20simultaneously%20manage%20multiple%20tasks%20related%0Ato%20panoramic%20driving%20perception.%20TriLiteNet%20is%20designed%20to%20optimize%20performance%0Awhile%20maintaining%20low%20computational%20costs.%20Experimental%20results%20on%20the%20BDD100k%0Adataset%20demonstrate%20that%20the%20model%20achieves%20competitive%20performance%20across%0Athree%20key%20tasks%3A%20vehicle%20detection%2C%20drivable%20area%20segmentation%2C%20and%20lane%20line%0Asegmentation.%20Specifically%2C%20the%20TriLiteNet_%7Bbase%7D%20demonstrated%20a%20recall%20of%0A85.6%25%20for%20vehicle%20detection%2C%20a%20mean%20Intersection%20over%20Union%20%28mIoU%29%20of%2092.4%25%20for%0Adrivable%20area%20segmentation%2C%20and%20an%20Acc%20of%2082.3%25%20for%20lane%20line%20segmentation%20with%0Aonly%202.35M%20parameters%20and%20a%20computational%20cost%20of%207.72%20GFLOPs.%20Our%20proposed%0Amodel%20includes%20a%20tiny%20configuration%20with%20just%200.14M%20parameters%2C%20which%20provides%0Aa%20multi-task%20solution%20with%20minimal%20computational%20demand.%20Evaluated%20for%20latency%0Aand%20power%20consumption%20on%20embedded%20devices%2C%20TriLiteNet%20in%20both%20configurations%0Ashows%20low%20latency%20and%20reasonable%20power%20during%20inference.%20By%20balancing%0Aperformance%2C%20computational%20efficiency%2C%20and%20scalability%2C%20TriLiteNet%20offers%20a%0Apractical%20and%20deployable%20solution%20for%20real-world%20autonomous%20driving%0Aapplications.%20Code%20is%20available%20at%20https%3A//github.com/chequanghuy/TriLiteNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04092v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTriLiteNet%253A%2520Lightweight%2520Model%2520for%2520Multi-Task%2520Visual%2520Perception%26entry.906535625%3DQuang-Huy%2520Che%2520and%2520Duc-Khai%2520Lam%26entry.1292438233%3D%2520%2520Efficient%2520perception%2520models%2520are%2520essential%2520for%2520Advanced%2520Driver%2520Assistance%250ASystems%2520%2528ADAS%2529%252C%2520as%2520these%2520applications%2520require%2520rapid%2520processing%2520and%2520response%2520to%250Aensure%2520safety%2520and%2520effectiveness%2520in%2520real-world%2520environments.%2520To%2520address%2520the%250Areal-time%2520execution%2520needs%2520of%2520such%2520perception%2520models%252C%2520this%2520study%2520introduces%2520the%250ATriLiteNet%2520model.%2520This%2520model%2520can%2520simultaneously%2520manage%2520multiple%2520tasks%2520related%250Ato%2520panoramic%2520driving%2520perception.%2520TriLiteNet%2520is%2520designed%2520to%2520optimize%2520performance%250Awhile%2520maintaining%2520low%2520computational%2520costs.%2520Experimental%2520results%2520on%2520the%2520BDD100k%250Adataset%2520demonstrate%2520that%2520the%2520model%2520achieves%2520competitive%2520performance%2520across%250Athree%2520key%2520tasks%253A%2520vehicle%2520detection%252C%2520drivable%2520area%2520segmentation%252C%2520and%2520lane%2520line%250Asegmentation.%2520Specifically%252C%2520the%2520TriLiteNet_%257Bbase%257D%2520demonstrated%2520a%2520recall%2520of%250A85.6%2525%2520for%2520vehicle%2520detection%252C%2520a%2520mean%2520Intersection%2520over%2520Union%2520%2528mIoU%2529%2520of%252092.4%2525%2520for%250Adrivable%2520area%2520segmentation%252C%2520and%2520an%2520Acc%2520of%252082.3%2525%2520for%2520lane%2520line%2520segmentation%2520with%250Aonly%25202.35M%2520parameters%2520and%2520a%2520computational%2520cost%2520of%25207.72%2520GFLOPs.%2520Our%2520proposed%250Amodel%2520includes%2520a%2520tiny%2520configuration%2520with%2520just%25200.14M%2520parameters%252C%2520which%2520provides%250Aa%2520multi-task%2520solution%2520with%2520minimal%2520computational%2520demand.%2520Evaluated%2520for%2520latency%250Aand%2520power%2520consumption%2520on%2520embedded%2520devices%252C%2520TriLiteNet%2520in%2520both%2520configurations%250Ashows%2520low%2520latency%2520and%2520reasonable%2520power%2520during%2520inference.%2520By%2520balancing%250Aperformance%252C%2520computational%2520efficiency%252C%2520and%2520scalability%252C%2520TriLiteNet%2520offers%2520a%250Apractical%2520and%2520deployable%2520solution%2520for%2520real-world%2520autonomous%2520driving%250Aapplications.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/chequanghuy/TriLiteNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04092v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TriLiteNet%3A%20Lightweight%20Model%20for%20Multi-Task%20Visual%20Perception&entry.906535625=Quang-Huy%20Che%20and%20Duc-Khai%20Lam&entry.1292438233=%20%20Efficient%20perception%20models%20are%20essential%20for%20Advanced%20Driver%20Assistance%0ASystems%20%28ADAS%29%2C%20as%20these%20applications%20require%20rapid%20processing%20and%20response%20to%0Aensure%20safety%20and%20effectiveness%20in%20real-world%20environments.%20To%20address%20the%0Areal-time%20execution%20needs%20of%20such%20perception%20models%2C%20this%20study%20introduces%20the%0ATriLiteNet%20model.%20This%20model%20can%20simultaneously%20manage%20multiple%20tasks%20related%0Ato%20panoramic%20driving%20perception.%20TriLiteNet%20is%20designed%20to%20optimize%20performance%0Awhile%20maintaining%20low%20computational%20costs.%20Experimental%20results%20on%20the%20BDD100k%0Adataset%20demonstrate%20that%20the%20model%20achieves%20competitive%20performance%20across%0Athree%20key%20tasks%3A%20vehicle%20detection%2C%20drivable%20area%20segmentation%2C%20and%20lane%20line%0Asegmentation.%20Specifically%2C%20the%20TriLiteNet_%7Bbase%7D%20demonstrated%20a%20recall%20of%0A85.6%25%20for%20vehicle%20detection%2C%20a%20mean%20Intersection%20over%20Union%20%28mIoU%29%20of%2092.4%25%20for%0Adrivable%20area%20segmentation%2C%20and%20an%20Acc%20of%2082.3%25%20for%20lane%20line%20segmentation%20with%0Aonly%202.35M%20parameters%20and%20a%20computational%20cost%20of%207.72%20GFLOPs.%20Our%20proposed%0Amodel%20includes%20a%20tiny%20configuration%20with%20just%200.14M%20parameters%2C%20which%20provides%0Aa%20multi-task%20solution%20with%20minimal%20computational%20demand.%20Evaluated%20for%20latency%0Aand%20power%20consumption%20on%20embedded%20devices%2C%20TriLiteNet%20in%20both%20configurations%0Ashows%20low%20latency%20and%20reasonable%20power%20during%20inference.%20By%20balancing%0Aperformance%2C%20computational%20efficiency%2C%20and%20scalability%2C%20TriLiteNet%20offers%20a%0Apractical%20and%20deployable%20solution%20for%20real-world%20autonomous%20driving%0Aapplications.%20Code%20is%20available%20at%20https%3A//github.com/chequanghuy/TriLiteNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04092v1&entry.124074799=Read"},
{"title": "GeoArena: An Open Platform for Benchmarking Large Vision-language Models\n  on WorldWide Image Geolocalization", "author": "Pengyue Jia and Yingyi Zhang and Xiangyu Zhao and Yixuan Li", "abstract": "  Image geolocalization aims to predict the geographic location of images\ncaptured anywhere on Earth, but its global nature presents significant\nchallenges. Current evaluation methodologies suffer from two major limitations.\nFirst, data leakage: advanced approaches often rely on large vision-language\nmodels (LVLMs) to predict image locations, yet these models are frequently\npretrained on the test datasets, compromising the accuracy of evaluating a\nmodel's actual geolocalization capability. Second, existing metrics primarily\nrely on exact geographic coordinates to assess predictions, which not only\nneglects the reasoning process but also raises privacy concerns when user-level\nlocation data is required. To address these issues, we propose GeoArena, a\nfirst open platform for evaluating LVLMs on worldwide image geolocalization\ntasks, offering true in-the-wild and human-centered benchmarking. GeoArena\nenables users to upload in-the-wild images for a more diverse evaluation\ncorpus, and it leverages pairwise human judgments to determine which model\noutput better aligns with human expectations. Our platform has been deployed\nonline for two months, during which we collected over thousands voting records.\nBased on this data, we conduct a detailed analysis and establish a leaderboard\nof different LVLMs on the image geolocalization task.\n", "link": "http://arxiv.org/abs/2509.04334v1", "date": "2025-09-04", "relevancy": 2.8579, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5737}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5705}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoArena%3A%20An%20Open%20Platform%20for%20Benchmarking%20Large%20Vision-language%20Models%0A%20%20on%20WorldWide%20Image%20Geolocalization&body=Title%3A%20GeoArena%3A%20An%20Open%20Platform%20for%20Benchmarking%20Large%20Vision-language%20Models%0A%20%20on%20WorldWide%20Image%20Geolocalization%0AAuthor%3A%20Pengyue%20Jia%20and%20Yingyi%20Zhang%20and%20Xiangyu%20Zhao%20and%20Yixuan%20Li%0AAbstract%3A%20%20%20Image%20geolocalization%20aims%20to%20predict%20the%20geographic%20location%20of%20images%0Acaptured%20anywhere%20on%20Earth%2C%20but%20its%20global%20nature%20presents%20significant%0Achallenges.%20Current%20evaluation%20methodologies%20suffer%20from%20two%20major%20limitations.%0AFirst%2C%20data%20leakage%3A%20advanced%20approaches%20often%20rely%20on%20large%20vision-language%0Amodels%20%28LVLMs%29%20to%20predict%20image%20locations%2C%20yet%20these%20models%20are%20frequently%0Apretrained%20on%20the%20test%20datasets%2C%20compromising%20the%20accuracy%20of%20evaluating%20a%0Amodel%27s%20actual%20geolocalization%20capability.%20Second%2C%20existing%20metrics%20primarily%0Arely%20on%20exact%20geographic%20coordinates%20to%20assess%20predictions%2C%20which%20not%20only%0Aneglects%20the%20reasoning%20process%20but%20also%20raises%20privacy%20concerns%20when%20user-level%0Alocation%20data%20is%20required.%20To%20address%20these%20issues%2C%20we%20propose%20GeoArena%2C%20a%0Afirst%20open%20platform%20for%20evaluating%20LVLMs%20on%20worldwide%20image%20geolocalization%0Atasks%2C%20offering%20true%20in-the-wild%20and%20human-centered%20benchmarking.%20GeoArena%0Aenables%20users%20to%20upload%20in-the-wild%20images%20for%20a%20more%20diverse%20evaluation%0Acorpus%2C%20and%20it%20leverages%20pairwise%20human%20judgments%20to%20determine%20which%20model%0Aoutput%20better%20aligns%20with%20human%20expectations.%20Our%20platform%20has%20been%20deployed%0Aonline%20for%20two%20months%2C%20during%20which%20we%20collected%20over%20thousands%20voting%20records.%0ABased%20on%20this%20data%2C%20we%20conduct%20a%20detailed%20analysis%20and%20establish%20a%20leaderboard%0Aof%20different%20LVLMs%20on%20the%20image%20geolocalization%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04334v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoArena%253A%2520An%2520Open%2520Platform%2520for%2520Benchmarking%2520Large%2520Vision-language%2520Models%250A%2520%2520on%2520WorldWide%2520Image%2520Geolocalization%26entry.906535625%3DPengyue%2520Jia%2520and%2520Yingyi%2520Zhang%2520and%2520Xiangyu%2520Zhao%2520and%2520Yixuan%2520Li%26entry.1292438233%3D%2520%2520Image%2520geolocalization%2520aims%2520to%2520predict%2520the%2520geographic%2520location%2520of%2520images%250Acaptured%2520anywhere%2520on%2520Earth%252C%2520but%2520its%2520global%2520nature%2520presents%2520significant%250Achallenges.%2520Current%2520evaluation%2520methodologies%2520suffer%2520from%2520two%2520major%2520limitations.%250AFirst%252C%2520data%2520leakage%253A%2520advanced%2520approaches%2520often%2520rely%2520on%2520large%2520vision-language%250Amodels%2520%2528LVLMs%2529%2520to%2520predict%2520image%2520locations%252C%2520yet%2520these%2520models%2520are%2520frequently%250Apretrained%2520on%2520the%2520test%2520datasets%252C%2520compromising%2520the%2520accuracy%2520of%2520evaluating%2520a%250Amodel%2527s%2520actual%2520geolocalization%2520capability.%2520Second%252C%2520existing%2520metrics%2520primarily%250Arely%2520on%2520exact%2520geographic%2520coordinates%2520to%2520assess%2520predictions%252C%2520which%2520not%2520only%250Aneglects%2520the%2520reasoning%2520process%2520but%2520also%2520raises%2520privacy%2520concerns%2520when%2520user-level%250Alocation%2520data%2520is%2520required.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520GeoArena%252C%2520a%250Afirst%2520open%2520platform%2520for%2520evaluating%2520LVLMs%2520on%2520worldwide%2520image%2520geolocalization%250Atasks%252C%2520offering%2520true%2520in-the-wild%2520and%2520human-centered%2520benchmarking.%2520GeoArena%250Aenables%2520users%2520to%2520upload%2520in-the-wild%2520images%2520for%2520a%2520more%2520diverse%2520evaluation%250Acorpus%252C%2520and%2520it%2520leverages%2520pairwise%2520human%2520judgments%2520to%2520determine%2520which%2520model%250Aoutput%2520better%2520aligns%2520with%2520human%2520expectations.%2520Our%2520platform%2520has%2520been%2520deployed%250Aonline%2520for%2520two%2520months%252C%2520during%2520which%2520we%2520collected%2520over%2520thousands%2520voting%2520records.%250ABased%2520on%2520this%2520data%252C%2520we%2520conduct%2520a%2520detailed%2520analysis%2520and%2520establish%2520a%2520leaderboard%250Aof%2520different%2520LVLMs%2520on%2520the%2520image%2520geolocalization%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04334v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoArena%3A%20An%20Open%20Platform%20for%20Benchmarking%20Large%20Vision-language%20Models%0A%20%20on%20WorldWide%20Image%20Geolocalization&entry.906535625=Pengyue%20Jia%20and%20Yingyi%20Zhang%20and%20Xiangyu%20Zhao%20and%20Yixuan%20Li&entry.1292438233=%20%20Image%20geolocalization%20aims%20to%20predict%20the%20geographic%20location%20of%20images%0Acaptured%20anywhere%20on%20Earth%2C%20but%20its%20global%20nature%20presents%20significant%0Achallenges.%20Current%20evaluation%20methodologies%20suffer%20from%20two%20major%20limitations.%0AFirst%2C%20data%20leakage%3A%20advanced%20approaches%20often%20rely%20on%20large%20vision-language%0Amodels%20%28LVLMs%29%20to%20predict%20image%20locations%2C%20yet%20these%20models%20are%20frequently%0Apretrained%20on%20the%20test%20datasets%2C%20compromising%20the%20accuracy%20of%20evaluating%20a%0Amodel%27s%20actual%20geolocalization%20capability.%20Second%2C%20existing%20metrics%20primarily%0Arely%20on%20exact%20geographic%20coordinates%20to%20assess%20predictions%2C%20which%20not%20only%0Aneglects%20the%20reasoning%20process%20but%20also%20raises%20privacy%20concerns%20when%20user-level%0Alocation%20data%20is%20required.%20To%20address%20these%20issues%2C%20we%20propose%20GeoArena%2C%20a%0Afirst%20open%20platform%20for%20evaluating%20LVLMs%20on%20worldwide%20image%20geolocalization%0Atasks%2C%20offering%20true%20in-the-wild%20and%20human-centered%20benchmarking.%20GeoArena%0Aenables%20users%20to%20upload%20in-the-wild%20images%20for%20a%20more%20diverse%20evaluation%0Acorpus%2C%20and%20it%20leverages%20pairwise%20human%20judgments%20to%20determine%20which%20model%0Aoutput%20better%20aligns%20with%20human%20expectations.%20Our%20platform%20has%20been%20deployed%0Aonline%20for%20two%20months%2C%20during%20which%20we%20collected%20over%20thousands%20voting%20records.%0ABased%20on%20this%20data%2C%20we%20conduct%20a%20detailed%20analysis%20and%20establish%20a%20leaderboard%0Aof%20different%20LVLMs%20on%20the%20image%20geolocalization%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04334v1&entry.124074799=Read"},
{"title": "The Telephone Game: Evaluating Semantic Drift in Unified Models", "author": "Sabbir Mollah and Rohit Gupta and Sirnam Swetha and Qingyang Liu and Ahnaf Munir and Mubarak Shah", "abstract": "  Employing a single, unified model (UM) for both visual understanding\n(image-to-text: I2T) and and visual generation (text-to-image: T2I) has opened\na new direction in Visual Language Model (VLM) research. While UMs can also\nsupport broader unimodal tasks (e.g., text-to-text, image-to-image), we focus\non the core cross-modal pair T2I and I2T, as consistency between understanding\nand generation is critical for downstream use. Existing evaluations consider\nthese capabilities in isolation: FID and GenEval for T2I, and benchmarks such\nas MME, MMBench for I2T. These single-pass metrics do not reveal whether a\nmodel that understands a concept can also render it, nor whether meaning is\npreserved when cycling between image and text modalities. To address this, we\nintroduce the Unified Consistency Framework for Unified Models (UCF-UM), a\ncyclic evaluation protocol that alternates I2T and T2I over multiple\ngenerations to quantify semantic drift. UCF formulates 3 metrics: (i) Mean\nCumulative Drift (MCD), an embedding-based measure of overall semantic loss;\n(ii) Semantic Drift Rate (SDR), that summarizes semantic decay rate; and (iii)\nMulti-Generation GenEval (MGG), an object-level compliance score extending\nGenEval. To assess generalization beyond COCO, which is widely used in\ntraining; we create a new benchmark ND400, sampled from NoCaps and DOCCI and\nevaluate on seven recent models. UCF-UM reveals substantial variation in\ncross-modal stability: some models like BAGEL maintain semantics over many\nalternations, whereas others like Vila-u drift quickly despite strong\nsingle-pass scores. Our results highlight cyclic consistency as a necessary\ncomplement to standard I2T and T2I evaluations, and provide practical metrics\nto consistently assess unified model's cross-modal stability and strength of\ntheir shared representations. Code:\nhttps://github.com/mollahsabbir/Semantic-Drift-in-Unified-Models\n", "link": "http://arxiv.org/abs/2509.04438v1", "date": "2025-09-04", "relevancy": 2.8345, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5678}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5664}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Telephone%20Game%3A%20Evaluating%20Semantic%20Drift%20in%20Unified%20Models&body=Title%3A%20The%20Telephone%20Game%3A%20Evaluating%20Semantic%20Drift%20in%20Unified%20Models%0AAuthor%3A%20Sabbir%20Mollah%20and%20Rohit%20Gupta%20and%20Sirnam%20Swetha%20and%20Qingyang%20Liu%20and%20Ahnaf%20Munir%20and%20Mubarak%20Shah%0AAbstract%3A%20%20%20Employing%20a%20single%2C%20unified%20model%20%28UM%29%20for%20both%20visual%20understanding%0A%28image-to-text%3A%20I2T%29%20and%20and%20visual%20generation%20%28text-to-image%3A%20T2I%29%20has%20opened%0Aa%20new%20direction%20in%20Visual%20Language%20Model%20%28VLM%29%20research.%20While%20UMs%20can%20also%0Asupport%20broader%20unimodal%20tasks%20%28e.g.%2C%20text-to-text%2C%20image-to-image%29%2C%20we%20focus%0Aon%20the%20core%20cross-modal%20pair%20T2I%20and%20I2T%2C%20as%20consistency%20between%20understanding%0Aand%20generation%20is%20critical%20for%20downstream%20use.%20Existing%20evaluations%20consider%0Athese%20capabilities%20in%20isolation%3A%20FID%20and%20GenEval%20for%20T2I%2C%20and%20benchmarks%20such%0Aas%20MME%2C%20MMBench%20for%20I2T.%20These%20single-pass%20metrics%20do%20not%20reveal%20whether%20a%0Amodel%20that%20understands%20a%20concept%20can%20also%20render%20it%2C%20nor%20whether%20meaning%20is%0Apreserved%20when%20cycling%20between%20image%20and%20text%20modalities.%20To%20address%20this%2C%20we%0Aintroduce%20the%20Unified%20Consistency%20Framework%20for%20Unified%20Models%20%28UCF-UM%29%2C%20a%0Acyclic%20evaluation%20protocol%20that%20alternates%20I2T%20and%20T2I%20over%20multiple%0Agenerations%20to%20quantify%20semantic%20drift.%20UCF%20formulates%203%20metrics%3A%20%28i%29%20Mean%0ACumulative%20Drift%20%28MCD%29%2C%20an%20embedding-based%20measure%20of%20overall%20semantic%20loss%3B%0A%28ii%29%20Semantic%20Drift%20Rate%20%28SDR%29%2C%20that%20summarizes%20semantic%20decay%20rate%3B%20and%20%28iii%29%0AMulti-Generation%20GenEval%20%28MGG%29%2C%20an%20object-level%20compliance%20score%20extending%0AGenEval.%20To%20assess%20generalization%20beyond%20COCO%2C%20which%20is%20widely%20used%20in%0Atraining%3B%20we%20create%20a%20new%20benchmark%20ND400%2C%20sampled%20from%20NoCaps%20and%20DOCCI%20and%0Aevaluate%20on%20seven%20recent%20models.%20UCF-UM%20reveals%20substantial%20variation%20in%0Across-modal%20stability%3A%20some%20models%20like%20BAGEL%20maintain%20semantics%20over%20many%0Aalternations%2C%20whereas%20others%20like%20Vila-u%20drift%20quickly%20despite%20strong%0Asingle-pass%20scores.%20Our%20results%20highlight%20cyclic%20consistency%20as%20a%20necessary%0Acomplement%20to%20standard%20I2T%20and%20T2I%20evaluations%2C%20and%20provide%20practical%20metrics%0Ato%20consistently%20assess%20unified%20model%27s%20cross-modal%20stability%20and%20strength%20of%0Atheir%20shared%20representations.%20Code%3A%0Ahttps%3A//github.com/mollahsabbir/Semantic-Drift-in-Unified-Models%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04438v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Telephone%2520Game%253A%2520Evaluating%2520Semantic%2520Drift%2520in%2520Unified%2520Models%26entry.906535625%3DSabbir%2520Mollah%2520and%2520Rohit%2520Gupta%2520and%2520Sirnam%2520Swetha%2520and%2520Qingyang%2520Liu%2520and%2520Ahnaf%2520Munir%2520and%2520Mubarak%2520Shah%26entry.1292438233%3D%2520%2520Employing%2520a%2520single%252C%2520unified%2520model%2520%2528UM%2529%2520for%2520both%2520visual%2520understanding%250A%2528image-to-text%253A%2520I2T%2529%2520and%2520and%2520visual%2520generation%2520%2528text-to-image%253A%2520T2I%2529%2520has%2520opened%250Aa%2520new%2520direction%2520in%2520Visual%2520Language%2520Model%2520%2528VLM%2529%2520research.%2520While%2520UMs%2520can%2520also%250Asupport%2520broader%2520unimodal%2520tasks%2520%2528e.g.%252C%2520text-to-text%252C%2520image-to-image%2529%252C%2520we%2520focus%250Aon%2520the%2520core%2520cross-modal%2520pair%2520T2I%2520and%2520I2T%252C%2520as%2520consistency%2520between%2520understanding%250Aand%2520generation%2520is%2520critical%2520for%2520downstream%2520use.%2520Existing%2520evaluations%2520consider%250Athese%2520capabilities%2520in%2520isolation%253A%2520FID%2520and%2520GenEval%2520for%2520T2I%252C%2520and%2520benchmarks%2520such%250Aas%2520MME%252C%2520MMBench%2520for%2520I2T.%2520These%2520single-pass%2520metrics%2520do%2520not%2520reveal%2520whether%2520a%250Amodel%2520that%2520understands%2520a%2520concept%2520can%2520also%2520render%2520it%252C%2520nor%2520whether%2520meaning%2520is%250Apreserved%2520when%2520cycling%2520between%2520image%2520and%2520text%2520modalities.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520the%2520Unified%2520Consistency%2520Framework%2520for%2520Unified%2520Models%2520%2528UCF-UM%2529%252C%2520a%250Acyclic%2520evaluation%2520protocol%2520that%2520alternates%2520I2T%2520and%2520T2I%2520over%2520multiple%250Agenerations%2520to%2520quantify%2520semantic%2520drift.%2520UCF%2520formulates%25203%2520metrics%253A%2520%2528i%2529%2520Mean%250ACumulative%2520Drift%2520%2528MCD%2529%252C%2520an%2520embedding-based%2520measure%2520of%2520overall%2520semantic%2520loss%253B%250A%2528ii%2529%2520Semantic%2520Drift%2520Rate%2520%2528SDR%2529%252C%2520that%2520summarizes%2520semantic%2520decay%2520rate%253B%2520and%2520%2528iii%2529%250AMulti-Generation%2520GenEval%2520%2528MGG%2529%252C%2520an%2520object-level%2520compliance%2520score%2520extending%250AGenEval.%2520To%2520assess%2520generalization%2520beyond%2520COCO%252C%2520which%2520is%2520widely%2520used%2520in%250Atraining%253B%2520we%2520create%2520a%2520new%2520benchmark%2520ND400%252C%2520sampled%2520from%2520NoCaps%2520and%2520DOCCI%2520and%250Aevaluate%2520on%2520seven%2520recent%2520models.%2520UCF-UM%2520reveals%2520substantial%2520variation%2520in%250Across-modal%2520stability%253A%2520some%2520models%2520like%2520BAGEL%2520maintain%2520semantics%2520over%2520many%250Aalternations%252C%2520whereas%2520others%2520like%2520Vila-u%2520drift%2520quickly%2520despite%2520strong%250Asingle-pass%2520scores.%2520Our%2520results%2520highlight%2520cyclic%2520consistency%2520as%2520a%2520necessary%250Acomplement%2520to%2520standard%2520I2T%2520and%2520T2I%2520evaluations%252C%2520and%2520provide%2520practical%2520metrics%250Ato%2520consistently%2520assess%2520unified%2520model%2527s%2520cross-modal%2520stability%2520and%2520strength%2520of%250Atheir%2520shared%2520representations.%2520Code%253A%250Ahttps%253A//github.com/mollahsabbir/Semantic-Drift-in-Unified-Models%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04438v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Telephone%20Game%3A%20Evaluating%20Semantic%20Drift%20in%20Unified%20Models&entry.906535625=Sabbir%20Mollah%20and%20Rohit%20Gupta%20and%20Sirnam%20Swetha%20and%20Qingyang%20Liu%20and%20Ahnaf%20Munir%20and%20Mubarak%20Shah&entry.1292438233=%20%20Employing%20a%20single%2C%20unified%20model%20%28UM%29%20for%20both%20visual%20understanding%0A%28image-to-text%3A%20I2T%29%20and%20and%20visual%20generation%20%28text-to-image%3A%20T2I%29%20has%20opened%0Aa%20new%20direction%20in%20Visual%20Language%20Model%20%28VLM%29%20research.%20While%20UMs%20can%20also%0Asupport%20broader%20unimodal%20tasks%20%28e.g.%2C%20text-to-text%2C%20image-to-image%29%2C%20we%20focus%0Aon%20the%20core%20cross-modal%20pair%20T2I%20and%20I2T%2C%20as%20consistency%20between%20understanding%0Aand%20generation%20is%20critical%20for%20downstream%20use.%20Existing%20evaluations%20consider%0Athese%20capabilities%20in%20isolation%3A%20FID%20and%20GenEval%20for%20T2I%2C%20and%20benchmarks%20such%0Aas%20MME%2C%20MMBench%20for%20I2T.%20These%20single-pass%20metrics%20do%20not%20reveal%20whether%20a%0Amodel%20that%20understands%20a%20concept%20can%20also%20render%20it%2C%20nor%20whether%20meaning%20is%0Apreserved%20when%20cycling%20between%20image%20and%20text%20modalities.%20To%20address%20this%2C%20we%0Aintroduce%20the%20Unified%20Consistency%20Framework%20for%20Unified%20Models%20%28UCF-UM%29%2C%20a%0Acyclic%20evaluation%20protocol%20that%20alternates%20I2T%20and%20T2I%20over%20multiple%0Agenerations%20to%20quantify%20semantic%20drift.%20UCF%20formulates%203%20metrics%3A%20%28i%29%20Mean%0ACumulative%20Drift%20%28MCD%29%2C%20an%20embedding-based%20measure%20of%20overall%20semantic%20loss%3B%0A%28ii%29%20Semantic%20Drift%20Rate%20%28SDR%29%2C%20that%20summarizes%20semantic%20decay%20rate%3B%20and%20%28iii%29%0AMulti-Generation%20GenEval%20%28MGG%29%2C%20an%20object-level%20compliance%20score%20extending%0AGenEval.%20To%20assess%20generalization%20beyond%20COCO%2C%20which%20is%20widely%20used%20in%0Atraining%3B%20we%20create%20a%20new%20benchmark%20ND400%2C%20sampled%20from%20NoCaps%20and%20DOCCI%20and%0Aevaluate%20on%20seven%20recent%20models.%20UCF-UM%20reveals%20substantial%20variation%20in%0Across-modal%20stability%3A%20some%20models%20like%20BAGEL%20maintain%20semantics%20over%20many%0Aalternations%2C%20whereas%20others%20like%20Vila-u%20drift%20quickly%20despite%20strong%0Asingle-pass%20scores.%20Our%20results%20highlight%20cyclic%20consistency%20as%20a%20necessary%0Acomplement%20to%20standard%20I2T%20and%20T2I%20evaluations%2C%20and%20provide%20practical%20metrics%0Ato%20consistently%20assess%20unified%20model%27s%20cross-modal%20stability%20and%20strength%20of%0Atheir%20shared%20representations.%20Code%3A%0Ahttps%3A//github.com/mollahsabbir/Semantic-Drift-in-Unified-Models%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04438v1&entry.124074799=Read"},
{"title": "Image Embedding Sampling Method for Diverse Captioning", "author": "Sania Waheed and Na Min An", "abstract": "  Image Captioning for state-of-the-art VLMs has significantly improved over\ntime; however, this comes at the cost of increased computational complexity,\nmaking them less accessible for resource-constrained applications such as\nmobile devices and assistive technologies. Alternatively, comparably smaller\nVLMs prioritize high-level scene descriptions, overlooking finer details that\ncontribute to a richer understanding of an image. In this paper, we introduce a\ntraining-free framework that enhances caption diversity and informativeness by\nexplicitly attending to distinct image regions using a comparably small VLM,\nBLIP, as the backbone. Our approach leverages structured segmentation to\nproduce hierarchical representations that capture both global and localized\nsemantics. Without requiring additional model training, we demonstrate that our\nmethod allows smaller VLMs to achieve performance comparable to larger models\nin terms of image-caption alignment, semantic integrity, and diversity. We\nevaluate our framework on MSCOCO, Flickr30k, and Nocaps test datasets,\nachieving a Div-2 score of 0.735, 0.750, and 0.748 for each dataset,\nrespectively, while maintaining strong image-caption relevancy and semantic\nintegrity with the human-annotated captions.\n", "link": "http://arxiv.org/abs/2502.10118v2", "date": "2025-09-04", "relevancy": 2.8168, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5643}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5629}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20Embedding%20Sampling%20Method%20for%20Diverse%20Captioning&body=Title%3A%20Image%20Embedding%20Sampling%20Method%20for%20Diverse%20Captioning%0AAuthor%3A%20Sania%20Waheed%20and%20Na%20Min%20An%0AAbstract%3A%20%20%20Image%20Captioning%20for%20state-of-the-art%20VLMs%20has%20significantly%20improved%20over%0Atime%3B%20however%2C%20this%20comes%20at%20the%20cost%20of%20increased%20computational%20complexity%2C%0Amaking%20them%20less%20accessible%20for%20resource-constrained%20applications%20such%20as%0Amobile%20devices%20and%20assistive%20technologies.%20Alternatively%2C%20comparably%20smaller%0AVLMs%20prioritize%20high-level%20scene%20descriptions%2C%20overlooking%20finer%20details%20that%0Acontribute%20to%20a%20richer%20understanding%20of%20an%20image.%20In%20this%20paper%2C%20we%20introduce%20a%0Atraining-free%20framework%20that%20enhances%20caption%20diversity%20and%20informativeness%20by%0Aexplicitly%20attending%20to%20distinct%20image%20regions%20using%20a%20comparably%20small%20VLM%2C%0ABLIP%2C%20as%20the%20backbone.%20Our%20approach%20leverages%20structured%20segmentation%20to%0Aproduce%20hierarchical%20representations%20that%20capture%20both%20global%20and%20localized%0Asemantics.%20Without%20requiring%20additional%20model%20training%2C%20we%20demonstrate%20that%20our%0Amethod%20allows%20smaller%20VLMs%20to%20achieve%20performance%20comparable%20to%20larger%20models%0Ain%20terms%20of%20image-caption%20alignment%2C%20semantic%20integrity%2C%20and%20diversity.%20We%0Aevaluate%20our%20framework%20on%20MSCOCO%2C%20Flickr30k%2C%20and%20Nocaps%20test%20datasets%2C%0Aachieving%20a%20Div-2%20score%20of%200.735%2C%200.750%2C%20and%200.748%20for%20each%20dataset%2C%0Arespectively%2C%20while%20maintaining%20strong%20image-caption%20relevancy%20and%20semantic%0Aintegrity%20with%20the%20human-annotated%20captions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10118v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520Embedding%2520Sampling%2520Method%2520for%2520Diverse%2520Captioning%26entry.906535625%3DSania%2520Waheed%2520and%2520Na%2520Min%2520An%26entry.1292438233%3D%2520%2520Image%2520Captioning%2520for%2520state-of-the-art%2520VLMs%2520has%2520significantly%2520improved%2520over%250Atime%253B%2520however%252C%2520this%2520comes%2520at%2520the%2520cost%2520of%2520increased%2520computational%2520complexity%252C%250Amaking%2520them%2520less%2520accessible%2520for%2520resource-constrained%2520applications%2520such%2520as%250Amobile%2520devices%2520and%2520assistive%2520technologies.%2520Alternatively%252C%2520comparably%2520smaller%250AVLMs%2520prioritize%2520high-level%2520scene%2520descriptions%252C%2520overlooking%2520finer%2520details%2520that%250Acontribute%2520to%2520a%2520richer%2520understanding%2520of%2520an%2520image.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%250Atraining-free%2520framework%2520that%2520enhances%2520caption%2520diversity%2520and%2520informativeness%2520by%250Aexplicitly%2520attending%2520to%2520distinct%2520image%2520regions%2520using%2520a%2520comparably%2520small%2520VLM%252C%250ABLIP%252C%2520as%2520the%2520backbone.%2520Our%2520approach%2520leverages%2520structured%2520segmentation%2520to%250Aproduce%2520hierarchical%2520representations%2520that%2520capture%2520both%2520global%2520and%2520localized%250Asemantics.%2520Without%2520requiring%2520additional%2520model%2520training%252C%2520we%2520demonstrate%2520that%2520our%250Amethod%2520allows%2520smaller%2520VLMs%2520to%2520achieve%2520performance%2520comparable%2520to%2520larger%2520models%250Ain%2520terms%2520of%2520image-caption%2520alignment%252C%2520semantic%2520integrity%252C%2520and%2520diversity.%2520We%250Aevaluate%2520our%2520framework%2520on%2520MSCOCO%252C%2520Flickr30k%252C%2520and%2520Nocaps%2520test%2520datasets%252C%250Aachieving%2520a%2520Div-2%2520score%2520of%25200.735%252C%25200.750%252C%2520and%25200.748%2520for%2520each%2520dataset%252C%250Arespectively%252C%2520while%2520maintaining%2520strong%2520image-caption%2520relevancy%2520and%2520semantic%250Aintegrity%2520with%2520the%2520human-annotated%2520captions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10118v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Embedding%20Sampling%20Method%20for%20Diverse%20Captioning&entry.906535625=Sania%20Waheed%20and%20Na%20Min%20An&entry.1292438233=%20%20Image%20Captioning%20for%20state-of-the-art%20VLMs%20has%20significantly%20improved%20over%0Atime%3B%20however%2C%20this%20comes%20at%20the%20cost%20of%20increased%20computational%20complexity%2C%0Amaking%20them%20less%20accessible%20for%20resource-constrained%20applications%20such%20as%0Amobile%20devices%20and%20assistive%20technologies.%20Alternatively%2C%20comparably%20smaller%0AVLMs%20prioritize%20high-level%20scene%20descriptions%2C%20overlooking%20finer%20details%20that%0Acontribute%20to%20a%20richer%20understanding%20of%20an%20image.%20In%20this%20paper%2C%20we%20introduce%20a%0Atraining-free%20framework%20that%20enhances%20caption%20diversity%20and%20informativeness%20by%0Aexplicitly%20attending%20to%20distinct%20image%20regions%20using%20a%20comparably%20small%20VLM%2C%0ABLIP%2C%20as%20the%20backbone.%20Our%20approach%20leverages%20structured%20segmentation%20to%0Aproduce%20hierarchical%20representations%20that%20capture%20both%20global%20and%20localized%0Asemantics.%20Without%20requiring%20additional%20model%20training%2C%20we%20demonstrate%20that%20our%0Amethod%20allows%20smaller%20VLMs%20to%20achieve%20performance%20comparable%20to%20larger%20models%0Ain%20terms%20of%20image-caption%20alignment%2C%20semantic%20integrity%2C%20and%20diversity.%20We%0Aevaluate%20our%20framework%20on%20MSCOCO%2C%20Flickr30k%2C%20and%20Nocaps%20test%20datasets%2C%0Aachieving%20a%20Div-2%20score%20of%200.735%2C%200.750%2C%20and%200.748%20for%20each%20dataset%2C%0Arespectively%2C%20while%20maintaining%20strong%20image-caption%20relevancy%20and%20semantic%0Aintegrity%20with%20the%20human-annotated%20captions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10118v2&entry.124074799=Read"},
{"title": "StreetViewAI: Making Street View Accessible Using Context-Aware\n  Multimodal AI", "author": "Jon E. Froehlich and Alexander Fiannaca and Nimer Jaber and Victor Tsaran and Shaun Kane", "abstract": "  Interactive streetscape mapping tools such as Google Street View (GSV) and\nMeta Mapillary enable users to virtually navigate and experience real-world\nenvironments via immersive 360{\\deg} imagery but remain fundamentally\ninaccessible to blind users. We introduce StreetViewAI, the first-ever\naccessible street view tool, which combines context-aware, multimodal AI,\naccessible navigation controls, and conversational speech. With StreetViewAI,\nblind users can virtually examine destinations, engage in open-world\nexploration, or virtually tour any of the over 220 billion images and 100+\ncountries where GSV is deployed. We iteratively designed StreetViewAI with a\nmixed-visual ability team and performed an evaluation with eleven blind users.\nOur findings demonstrate the value of an accessible street view in supporting\nPOI investigations and remote route planning. We close by enumerating key\nguidelines for future work.\n", "link": "http://arxiv.org/abs/2508.08524v3", "date": "2025-09-04", "relevancy": 2.7439, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5927}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5268}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StreetViewAI%3A%20Making%20Street%20View%20Accessible%20Using%20Context-Aware%0A%20%20Multimodal%20AI&body=Title%3A%20StreetViewAI%3A%20Making%20Street%20View%20Accessible%20Using%20Context-Aware%0A%20%20Multimodal%20AI%0AAuthor%3A%20Jon%20E.%20Froehlich%20and%20Alexander%20Fiannaca%20and%20Nimer%20Jaber%20and%20Victor%20Tsaran%20and%20Shaun%20Kane%0AAbstract%3A%20%20%20Interactive%20streetscape%20mapping%20tools%20such%20as%20Google%20Street%20View%20%28GSV%29%20and%0AMeta%20Mapillary%20enable%20users%20to%20virtually%20navigate%20and%20experience%20real-world%0Aenvironments%20via%20immersive%20360%7B%5Cdeg%7D%20imagery%20but%20remain%20fundamentally%0Ainaccessible%20to%20blind%20users.%20We%20introduce%20StreetViewAI%2C%20the%20first-ever%0Aaccessible%20street%20view%20tool%2C%20which%20combines%20context-aware%2C%20multimodal%20AI%2C%0Aaccessible%20navigation%20controls%2C%20and%20conversational%20speech.%20With%20StreetViewAI%2C%0Ablind%20users%20can%20virtually%20examine%20destinations%2C%20engage%20in%20open-world%0Aexploration%2C%20or%20virtually%20tour%20any%20of%20the%20over%20220%20billion%20images%20and%20100%2B%0Acountries%20where%20GSV%20is%20deployed.%20We%20iteratively%20designed%20StreetViewAI%20with%20a%0Amixed-visual%20ability%20team%20and%20performed%20an%20evaluation%20with%20eleven%20blind%20users.%0AOur%20findings%20demonstrate%20the%20value%20of%20an%20accessible%20street%20view%20in%20supporting%0APOI%20investigations%20and%20remote%20route%20planning.%20We%20close%20by%20enumerating%20key%0Aguidelines%20for%20future%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08524v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStreetViewAI%253A%2520Making%2520Street%2520View%2520Accessible%2520Using%2520Context-Aware%250A%2520%2520Multimodal%2520AI%26entry.906535625%3DJon%2520E.%2520Froehlich%2520and%2520Alexander%2520Fiannaca%2520and%2520Nimer%2520Jaber%2520and%2520Victor%2520Tsaran%2520and%2520Shaun%2520Kane%26entry.1292438233%3D%2520%2520Interactive%2520streetscape%2520mapping%2520tools%2520such%2520as%2520Google%2520Street%2520View%2520%2528GSV%2529%2520and%250AMeta%2520Mapillary%2520enable%2520users%2520to%2520virtually%2520navigate%2520and%2520experience%2520real-world%250Aenvironments%2520via%2520immersive%2520360%257B%255Cdeg%257D%2520imagery%2520but%2520remain%2520fundamentally%250Ainaccessible%2520to%2520blind%2520users.%2520We%2520introduce%2520StreetViewAI%252C%2520the%2520first-ever%250Aaccessible%2520street%2520view%2520tool%252C%2520which%2520combines%2520context-aware%252C%2520multimodal%2520AI%252C%250Aaccessible%2520navigation%2520controls%252C%2520and%2520conversational%2520speech.%2520With%2520StreetViewAI%252C%250Ablind%2520users%2520can%2520virtually%2520examine%2520destinations%252C%2520engage%2520in%2520open-world%250Aexploration%252C%2520or%2520virtually%2520tour%2520any%2520of%2520the%2520over%2520220%2520billion%2520images%2520and%2520100%252B%250Acountries%2520where%2520GSV%2520is%2520deployed.%2520We%2520iteratively%2520designed%2520StreetViewAI%2520with%2520a%250Amixed-visual%2520ability%2520team%2520and%2520performed%2520an%2520evaluation%2520with%2520eleven%2520blind%2520users.%250AOur%2520findings%2520demonstrate%2520the%2520value%2520of%2520an%2520accessible%2520street%2520view%2520in%2520supporting%250APOI%2520investigations%2520and%2520remote%2520route%2520planning.%2520We%2520close%2520by%2520enumerating%2520key%250Aguidelines%2520for%2520future%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08524v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StreetViewAI%3A%20Making%20Street%20View%20Accessible%20Using%20Context-Aware%0A%20%20Multimodal%20AI&entry.906535625=Jon%20E.%20Froehlich%20and%20Alexander%20Fiannaca%20and%20Nimer%20Jaber%20and%20Victor%20Tsaran%20and%20Shaun%20Kane&entry.1292438233=%20%20Interactive%20streetscape%20mapping%20tools%20such%20as%20Google%20Street%20View%20%28GSV%29%20and%0AMeta%20Mapillary%20enable%20users%20to%20virtually%20navigate%20and%20experience%20real-world%0Aenvironments%20via%20immersive%20360%7B%5Cdeg%7D%20imagery%20but%20remain%20fundamentally%0Ainaccessible%20to%20blind%20users.%20We%20introduce%20StreetViewAI%2C%20the%20first-ever%0Aaccessible%20street%20view%20tool%2C%20which%20combines%20context-aware%2C%20multimodal%20AI%2C%0Aaccessible%20navigation%20controls%2C%20and%20conversational%20speech.%20With%20StreetViewAI%2C%0Ablind%20users%20can%20virtually%20examine%20destinations%2C%20engage%20in%20open-world%0Aexploration%2C%20or%20virtually%20tour%20any%20of%20the%20over%20220%20billion%20images%20and%20100%2B%0Acountries%20where%20GSV%20is%20deployed.%20We%20iteratively%20designed%20StreetViewAI%20with%20a%0Amixed-visual%20ability%20team%20and%20performed%20an%20evaluation%20with%20eleven%20blind%20users.%0AOur%20findings%20demonstrate%20the%20value%20of%20an%20accessible%20street%20view%20in%20supporting%0APOI%20investigations%20and%20remote%20route%20planning.%20We%20close%20by%20enumerating%20key%0Aguidelines%20for%20future%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08524v3&entry.124074799=Read"},
{"title": "Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from\n  Vector Drawings", "author": "Feiwei Qin and Shichao Lu and Junhao Hou and Changmiao Wang and Meie Fang and Ligang Liu", "abstract": "  Computer-Aided Design (CAD) generative modeling is driving significant\ninnovations across industrial applications. Recent works have shown remarkable\nprogress in creating solid models from various inputs such as point clouds,\nmeshes, and text descriptions. However, these methods fundamentally diverge\nfrom traditional industrial workflows that begin with 2D engineering drawings.\nThe automatic generation of parametric CAD models from these 2D vector drawings\nremains underexplored despite being a critical step in engineering design. To\naddress this gap, our key insight is to reframe CAD generation as a\nsequence-to-sequence learning problem where vector drawing primitives directly\ninform the generation of parametric CAD operations, preserving geometric\nprecision and design intent throughout the transformation process. We propose\nDrawing2CAD, a framework with three key technical components: a\nnetwork-friendly vector primitive representation that preserves precise\ngeometric information, a dual-decoder transformer architecture that decouples\ncommand type and parameter generation while maintaining precise correspondence,\nand a soft target distribution loss function accommodating inherent flexibility\nin CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing,\na dataset of paired engineering drawings and parametric CAD models, and conduct\nthorough experiments to demonstrate the effectiveness of our method. Code and\ndataset are available at https://github.com/lllssc/Drawing2CAD.\n", "link": "http://arxiv.org/abs/2508.18733v3", "date": "2025-09-04", "relevancy": 2.712, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5645}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5399}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Drawing2CAD%3A%20Sequence-to-Sequence%20Learning%20for%20CAD%20Generation%20from%0A%20%20Vector%20Drawings&body=Title%3A%20Drawing2CAD%3A%20Sequence-to-Sequence%20Learning%20for%20CAD%20Generation%20from%0A%20%20Vector%20Drawings%0AAuthor%3A%20Feiwei%20Qin%20and%20Shichao%20Lu%20and%20Junhao%20Hou%20and%20Changmiao%20Wang%20and%20Meie%20Fang%20and%20Ligang%20Liu%0AAbstract%3A%20%20%20Computer-Aided%20Design%20%28CAD%29%20generative%20modeling%20is%20driving%20significant%0Ainnovations%20across%20industrial%20applications.%20Recent%20works%20have%20shown%20remarkable%0Aprogress%20in%20creating%20solid%20models%20from%20various%20inputs%20such%20as%20point%20clouds%2C%0Ameshes%2C%20and%20text%20descriptions.%20However%2C%20these%20methods%20fundamentally%20diverge%0Afrom%20traditional%20industrial%20workflows%20that%20begin%20with%202D%20engineering%20drawings.%0AThe%20automatic%20generation%20of%20parametric%20CAD%20models%20from%20these%202D%20vector%20drawings%0Aremains%20underexplored%20despite%20being%20a%20critical%20step%20in%20engineering%20design.%20To%0Aaddress%20this%20gap%2C%20our%20key%20insight%20is%20to%20reframe%20CAD%20generation%20as%20a%0Asequence-to-sequence%20learning%20problem%20where%20vector%20drawing%20primitives%20directly%0Ainform%20the%20generation%20of%20parametric%20CAD%20operations%2C%20preserving%20geometric%0Aprecision%20and%20design%20intent%20throughout%20the%20transformation%20process.%20We%20propose%0ADrawing2CAD%2C%20a%20framework%20with%20three%20key%20technical%20components%3A%20a%0Anetwork-friendly%20vector%20primitive%20representation%20that%20preserves%20precise%0Ageometric%20information%2C%20a%20dual-decoder%20transformer%20architecture%20that%20decouples%0Acommand%20type%20and%20parameter%20generation%20while%20maintaining%20precise%20correspondence%2C%0Aand%20a%20soft%20target%20distribution%20loss%20function%20accommodating%20inherent%20flexibility%0Ain%20CAD%20parameters.%20To%20train%20and%20evaluate%20Drawing2CAD%2C%20we%20create%20CAD-VGDrawing%2C%0Aa%20dataset%20of%20paired%20engineering%20drawings%20and%20parametric%20CAD%20models%2C%20and%20conduct%0Athorough%20experiments%20to%20demonstrate%20the%20effectiveness%20of%20our%20method.%20Code%20and%0Adataset%20are%20available%20at%20https%3A//github.com/lllssc/Drawing2CAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18733v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDrawing2CAD%253A%2520Sequence-to-Sequence%2520Learning%2520for%2520CAD%2520Generation%2520from%250A%2520%2520Vector%2520Drawings%26entry.906535625%3DFeiwei%2520Qin%2520and%2520Shichao%2520Lu%2520and%2520Junhao%2520Hou%2520and%2520Changmiao%2520Wang%2520and%2520Meie%2520Fang%2520and%2520Ligang%2520Liu%26entry.1292438233%3D%2520%2520Computer-Aided%2520Design%2520%2528CAD%2529%2520generative%2520modeling%2520is%2520driving%2520significant%250Ainnovations%2520across%2520industrial%2520applications.%2520Recent%2520works%2520have%2520shown%2520remarkable%250Aprogress%2520in%2520creating%2520solid%2520models%2520from%2520various%2520inputs%2520such%2520as%2520point%2520clouds%252C%250Ameshes%252C%2520and%2520text%2520descriptions.%2520However%252C%2520these%2520methods%2520fundamentally%2520diverge%250Afrom%2520traditional%2520industrial%2520workflows%2520that%2520begin%2520with%25202D%2520engineering%2520drawings.%250AThe%2520automatic%2520generation%2520of%2520parametric%2520CAD%2520models%2520from%2520these%25202D%2520vector%2520drawings%250Aremains%2520underexplored%2520despite%2520being%2520a%2520critical%2520step%2520in%2520engineering%2520design.%2520To%250Aaddress%2520this%2520gap%252C%2520our%2520key%2520insight%2520is%2520to%2520reframe%2520CAD%2520generation%2520as%2520a%250Asequence-to-sequence%2520learning%2520problem%2520where%2520vector%2520drawing%2520primitives%2520directly%250Ainform%2520the%2520generation%2520of%2520parametric%2520CAD%2520operations%252C%2520preserving%2520geometric%250Aprecision%2520and%2520design%2520intent%2520throughout%2520the%2520transformation%2520process.%2520We%2520propose%250ADrawing2CAD%252C%2520a%2520framework%2520with%2520three%2520key%2520technical%2520components%253A%2520a%250Anetwork-friendly%2520vector%2520primitive%2520representation%2520that%2520preserves%2520precise%250Ageometric%2520information%252C%2520a%2520dual-decoder%2520transformer%2520architecture%2520that%2520decouples%250Acommand%2520type%2520and%2520parameter%2520generation%2520while%2520maintaining%2520precise%2520correspondence%252C%250Aand%2520a%2520soft%2520target%2520distribution%2520loss%2520function%2520accommodating%2520inherent%2520flexibility%250Ain%2520CAD%2520parameters.%2520To%2520train%2520and%2520evaluate%2520Drawing2CAD%252C%2520we%2520create%2520CAD-VGDrawing%252C%250Aa%2520dataset%2520of%2520paired%2520engineering%2520drawings%2520and%2520parametric%2520CAD%2520models%252C%2520and%2520conduct%250Athorough%2520experiments%2520to%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method.%2520Code%2520and%250Adataset%2520are%2520available%2520at%2520https%253A//github.com/lllssc/Drawing2CAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18733v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Drawing2CAD%3A%20Sequence-to-Sequence%20Learning%20for%20CAD%20Generation%20from%0A%20%20Vector%20Drawings&entry.906535625=Feiwei%20Qin%20and%20Shichao%20Lu%20and%20Junhao%20Hou%20and%20Changmiao%20Wang%20and%20Meie%20Fang%20and%20Ligang%20Liu&entry.1292438233=%20%20Computer-Aided%20Design%20%28CAD%29%20generative%20modeling%20is%20driving%20significant%0Ainnovations%20across%20industrial%20applications.%20Recent%20works%20have%20shown%20remarkable%0Aprogress%20in%20creating%20solid%20models%20from%20various%20inputs%20such%20as%20point%20clouds%2C%0Ameshes%2C%20and%20text%20descriptions.%20However%2C%20these%20methods%20fundamentally%20diverge%0Afrom%20traditional%20industrial%20workflows%20that%20begin%20with%202D%20engineering%20drawings.%0AThe%20automatic%20generation%20of%20parametric%20CAD%20models%20from%20these%202D%20vector%20drawings%0Aremains%20underexplored%20despite%20being%20a%20critical%20step%20in%20engineering%20design.%20To%0Aaddress%20this%20gap%2C%20our%20key%20insight%20is%20to%20reframe%20CAD%20generation%20as%20a%0Asequence-to-sequence%20learning%20problem%20where%20vector%20drawing%20primitives%20directly%0Ainform%20the%20generation%20of%20parametric%20CAD%20operations%2C%20preserving%20geometric%0Aprecision%20and%20design%20intent%20throughout%20the%20transformation%20process.%20We%20propose%0ADrawing2CAD%2C%20a%20framework%20with%20three%20key%20technical%20components%3A%20a%0Anetwork-friendly%20vector%20primitive%20representation%20that%20preserves%20precise%0Ageometric%20information%2C%20a%20dual-decoder%20transformer%20architecture%20that%20decouples%0Acommand%20type%20and%20parameter%20generation%20while%20maintaining%20precise%20correspondence%2C%0Aand%20a%20soft%20target%20distribution%20loss%20function%20accommodating%20inherent%20flexibility%0Ain%20CAD%20parameters.%20To%20train%20and%20evaluate%20Drawing2CAD%2C%20we%20create%20CAD-VGDrawing%2C%0Aa%20dataset%20of%20paired%20engineering%20drawings%20and%20parametric%20CAD%20models%2C%20and%20conduct%0Athorough%20experiments%20to%20demonstrate%20the%20effectiveness%20of%20our%20method.%20Code%20and%0Adataset%20are%20available%20at%20https%3A//github.com/lllssc/Drawing2CAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18733v3&entry.124074799=Read"},
{"title": "EHVC: Efficient Hierarchical Reference and Quality Structure for Neural\n  Video Coding", "author": "Junqi Liao and Yaojun Wu and Chaoyi Lin and Zhipin Deng and Li Li and Dong Liu and Xiaoyan Sun", "abstract": "  Neural video codecs (NVCs), leveraging the power of end-to-end learning, have\ndemonstrated remarkable coding efficiency improvements over traditional video\ncodecs. Recent research has begun to pay attention to the quality structures in\nNVCs, optimizing them by introducing explicit hierarchical designs. However,\nless attention has been paid to the reference structure design, which\nfundamentally should be aligned with the hierarchical quality structure. In\naddition, there is still significant room for further optimization of the\nhierarchical quality structure. To address these challenges in NVCs, we propose\nEHVC, an efficient hierarchical neural video codec featuring three key\ninnovations: (1) a hierarchical multi-reference scheme that draws on\ntraditional video codec design to align reference and quality structures,\nthereby addressing the reference-quality mismatch; (2) a lookahead strategy to\nutilize an encoder-side context from future frames to enhance the quality\nstructure; (3) a layer-wise quality scale with random quality training strategy\nto stabilize quality structures during inference. With these improvements, EHVC\nachieves significantly superior performance to the state-of-the-art NVCs. Code\nwill be released in: https://github.com/bytedance/NEVC.\n", "link": "http://arxiv.org/abs/2509.04118v1", "date": "2025-09-04", "relevancy": 2.6921, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5492}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5377}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EHVC%3A%20Efficient%20Hierarchical%20Reference%20and%20Quality%20Structure%20for%20Neural%0A%20%20Video%20Coding&body=Title%3A%20EHVC%3A%20Efficient%20Hierarchical%20Reference%20and%20Quality%20Structure%20for%20Neural%0A%20%20Video%20Coding%0AAuthor%3A%20Junqi%20Liao%20and%20Yaojun%20Wu%20and%20Chaoyi%20Lin%20and%20Zhipin%20Deng%20and%20Li%20Li%20and%20Dong%20Liu%20and%20Xiaoyan%20Sun%0AAbstract%3A%20%20%20Neural%20video%20codecs%20%28NVCs%29%2C%20leveraging%20the%20power%20of%20end-to-end%20learning%2C%20have%0Ademonstrated%20remarkable%20coding%20efficiency%20improvements%20over%20traditional%20video%0Acodecs.%20Recent%20research%20has%20begun%20to%20pay%20attention%20to%20the%20quality%20structures%20in%0ANVCs%2C%20optimizing%20them%20by%20introducing%20explicit%20hierarchical%20designs.%20However%2C%0Aless%20attention%20has%20been%20paid%20to%20the%20reference%20structure%20design%2C%20which%0Afundamentally%20should%20be%20aligned%20with%20the%20hierarchical%20quality%20structure.%20In%0Aaddition%2C%20there%20is%20still%20significant%20room%20for%20further%20optimization%20of%20the%0Ahierarchical%20quality%20structure.%20To%20address%20these%20challenges%20in%20NVCs%2C%20we%20propose%0AEHVC%2C%20an%20efficient%20hierarchical%20neural%20video%20codec%20featuring%20three%20key%0Ainnovations%3A%20%281%29%20a%20hierarchical%20multi-reference%20scheme%20that%20draws%20on%0Atraditional%20video%20codec%20design%20to%20align%20reference%20and%20quality%20structures%2C%0Athereby%20addressing%20the%20reference-quality%20mismatch%3B%20%282%29%20a%20lookahead%20strategy%20to%0Autilize%20an%20encoder-side%20context%20from%20future%20frames%20to%20enhance%20the%20quality%0Astructure%3B%20%283%29%20a%20layer-wise%20quality%20scale%20with%20random%20quality%20training%20strategy%0Ato%20stabilize%20quality%20structures%20during%20inference.%20With%20these%20improvements%2C%20EHVC%0Aachieves%20significantly%20superior%20performance%20to%20the%20state-of-the-art%20NVCs.%20Code%0Awill%20be%20released%20in%3A%20https%3A//github.com/bytedance/NEVC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04118v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEHVC%253A%2520Efficient%2520Hierarchical%2520Reference%2520and%2520Quality%2520Structure%2520for%2520Neural%250A%2520%2520Video%2520Coding%26entry.906535625%3DJunqi%2520Liao%2520and%2520Yaojun%2520Wu%2520and%2520Chaoyi%2520Lin%2520and%2520Zhipin%2520Deng%2520and%2520Li%2520Li%2520and%2520Dong%2520Liu%2520and%2520Xiaoyan%2520Sun%26entry.1292438233%3D%2520%2520Neural%2520video%2520codecs%2520%2528NVCs%2529%252C%2520leveraging%2520the%2520power%2520of%2520end-to-end%2520learning%252C%2520have%250Ademonstrated%2520remarkable%2520coding%2520efficiency%2520improvements%2520over%2520traditional%2520video%250Acodecs.%2520Recent%2520research%2520has%2520begun%2520to%2520pay%2520attention%2520to%2520the%2520quality%2520structures%2520in%250ANVCs%252C%2520optimizing%2520them%2520by%2520introducing%2520explicit%2520hierarchical%2520designs.%2520However%252C%250Aless%2520attention%2520has%2520been%2520paid%2520to%2520the%2520reference%2520structure%2520design%252C%2520which%250Afundamentally%2520should%2520be%2520aligned%2520with%2520the%2520hierarchical%2520quality%2520structure.%2520In%250Aaddition%252C%2520there%2520is%2520still%2520significant%2520room%2520for%2520further%2520optimization%2520of%2520the%250Ahierarchical%2520quality%2520structure.%2520To%2520address%2520these%2520challenges%2520in%2520NVCs%252C%2520we%2520propose%250AEHVC%252C%2520an%2520efficient%2520hierarchical%2520neural%2520video%2520codec%2520featuring%2520three%2520key%250Ainnovations%253A%2520%25281%2529%2520a%2520hierarchical%2520multi-reference%2520scheme%2520that%2520draws%2520on%250Atraditional%2520video%2520codec%2520design%2520to%2520align%2520reference%2520and%2520quality%2520structures%252C%250Athereby%2520addressing%2520the%2520reference-quality%2520mismatch%253B%2520%25282%2529%2520a%2520lookahead%2520strategy%2520to%250Autilize%2520an%2520encoder-side%2520context%2520from%2520future%2520frames%2520to%2520enhance%2520the%2520quality%250Astructure%253B%2520%25283%2529%2520a%2520layer-wise%2520quality%2520scale%2520with%2520random%2520quality%2520training%2520strategy%250Ato%2520stabilize%2520quality%2520structures%2520during%2520inference.%2520With%2520these%2520improvements%252C%2520EHVC%250Aachieves%2520significantly%2520superior%2520performance%2520to%2520the%2520state-of-the-art%2520NVCs.%2520Code%250Awill%2520be%2520released%2520in%253A%2520https%253A//github.com/bytedance/NEVC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04118v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EHVC%3A%20Efficient%20Hierarchical%20Reference%20and%20Quality%20Structure%20for%20Neural%0A%20%20Video%20Coding&entry.906535625=Junqi%20Liao%20and%20Yaojun%20Wu%20and%20Chaoyi%20Lin%20and%20Zhipin%20Deng%20and%20Li%20Li%20and%20Dong%20Liu%20and%20Xiaoyan%20Sun&entry.1292438233=%20%20Neural%20video%20codecs%20%28NVCs%29%2C%20leveraging%20the%20power%20of%20end-to-end%20learning%2C%20have%0Ademonstrated%20remarkable%20coding%20efficiency%20improvements%20over%20traditional%20video%0Acodecs.%20Recent%20research%20has%20begun%20to%20pay%20attention%20to%20the%20quality%20structures%20in%0ANVCs%2C%20optimizing%20them%20by%20introducing%20explicit%20hierarchical%20designs.%20However%2C%0Aless%20attention%20has%20been%20paid%20to%20the%20reference%20structure%20design%2C%20which%0Afundamentally%20should%20be%20aligned%20with%20the%20hierarchical%20quality%20structure.%20In%0Aaddition%2C%20there%20is%20still%20significant%20room%20for%20further%20optimization%20of%20the%0Ahierarchical%20quality%20structure.%20To%20address%20these%20challenges%20in%20NVCs%2C%20we%20propose%0AEHVC%2C%20an%20efficient%20hierarchical%20neural%20video%20codec%20featuring%20three%20key%0Ainnovations%3A%20%281%29%20a%20hierarchical%20multi-reference%20scheme%20that%20draws%20on%0Atraditional%20video%20codec%20design%20to%20align%20reference%20and%20quality%20structures%2C%0Athereby%20addressing%20the%20reference-quality%20mismatch%3B%20%282%29%20a%20lookahead%20strategy%20to%0Autilize%20an%20encoder-side%20context%20from%20future%20frames%20to%20enhance%20the%20quality%0Astructure%3B%20%283%29%20a%20layer-wise%20quality%20scale%20with%20random%20quality%20training%20strategy%0Ato%20stabilize%20quality%20structures%20during%20inference.%20With%20these%20improvements%2C%20EHVC%0Aachieves%20significantly%20superior%20performance%20to%20the%20state-of-the-art%20NVCs.%20Code%0Awill%20be%20released%20in%3A%20https%3A//github.com/bytedance/NEVC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04118v1&entry.124074799=Read"},
{"title": "DeepVIS: Bridging Natural Language and Data Visualization Through\n  Step-wise Reasoning", "author": "Zhihao Shuai and Boyan Li and Siyu Yan and Yuyu Luo and Weikai Yang", "abstract": "  Although data visualization is powerful for revealing patterns and\ncommunicating insights, creating effective visualizations requires familiarity\nwith authoring tools and often disrupts the analysis flow. While large language\nmodels show promise for automatically converting analysis intent into\nvisualizations, existing methods function as black boxes without transparent\nreasoning processes, which prevents users from understanding design rationales\nand refining suboptimal outputs. To bridge this gap, we propose integrating\nChain-of-Thought (CoT) reasoning into the Natural Language to Visualization\n(NL2VIS) pipeline. First, we design a comprehensive CoT reasoning process for\nNL2VIS and develop an automatic pipeline to equip existing datasets with\nstructured reasoning steps. Second, we introduce nvBench-CoT, a specialized\ndataset capturing detailed step-by-step reasoning from ambiguous natural\nlanguage descriptions to finalized visualizations, which enables\nstate-of-the-art performance when used for model fine-tuning. Third, we develop\nDeepVIS, an interactive visual interface that tightly integrates with the CoT\nreasoning process, allowing users to inspect reasoning steps, identify errors,\nand make targeted adjustments to improve visualization outcomes. Quantitative\nbenchmark evaluations, two use cases, and a user study collectively demonstrate\nthat our CoT framework effectively enhances NL2VIS quality while providing\ninsightful reasoning steps to users.\n", "link": "http://arxiv.org/abs/2508.01700v2", "date": "2025-09-04", "relevancy": 2.6915, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5563}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5563}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5024}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepVIS%3A%20Bridging%20Natural%20Language%20and%20Data%20Visualization%20Through%0A%20%20Step-wise%20Reasoning&body=Title%3A%20DeepVIS%3A%20Bridging%20Natural%20Language%20and%20Data%20Visualization%20Through%0A%20%20Step-wise%20Reasoning%0AAuthor%3A%20Zhihao%20Shuai%20and%20Boyan%20Li%20and%20Siyu%20Yan%20and%20Yuyu%20Luo%20and%20Weikai%20Yang%0AAbstract%3A%20%20%20Although%20data%20visualization%20is%20powerful%20for%20revealing%20patterns%20and%0Acommunicating%20insights%2C%20creating%20effective%20visualizations%20requires%20familiarity%0Awith%20authoring%20tools%20and%20often%20disrupts%20the%20analysis%20flow.%20While%20large%20language%0Amodels%20show%20promise%20for%20automatically%20converting%20analysis%20intent%20into%0Avisualizations%2C%20existing%20methods%20function%20as%20black%20boxes%20without%20transparent%0Areasoning%20processes%2C%20which%20prevents%20users%20from%20understanding%20design%20rationales%0Aand%20refining%20suboptimal%20outputs.%20To%20bridge%20this%20gap%2C%20we%20propose%20integrating%0AChain-of-Thought%20%28CoT%29%20reasoning%20into%20the%20Natural%20Language%20to%20Visualization%0A%28NL2VIS%29%20pipeline.%20First%2C%20we%20design%20a%20comprehensive%20CoT%20reasoning%20process%20for%0ANL2VIS%20and%20develop%20an%20automatic%20pipeline%20to%20equip%20existing%20datasets%20with%0Astructured%20reasoning%20steps.%20Second%2C%20we%20introduce%20nvBench-CoT%2C%20a%20specialized%0Adataset%20capturing%20detailed%20step-by-step%20reasoning%20from%20ambiguous%20natural%0Alanguage%20descriptions%20to%20finalized%20visualizations%2C%20which%20enables%0Astate-of-the-art%20performance%20when%20used%20for%20model%20fine-tuning.%20Third%2C%20we%20develop%0ADeepVIS%2C%20an%20interactive%20visual%20interface%20that%20tightly%20integrates%20with%20the%20CoT%0Areasoning%20process%2C%20allowing%20users%20to%20inspect%20reasoning%20steps%2C%20identify%20errors%2C%0Aand%20make%20targeted%20adjustments%20to%20improve%20visualization%20outcomes.%20Quantitative%0Abenchmark%20evaluations%2C%20two%20use%20cases%2C%20and%20a%20user%20study%20collectively%20demonstrate%0Athat%20our%20CoT%20framework%20effectively%20enhances%20NL2VIS%20quality%20while%20providing%0Ainsightful%20reasoning%20steps%20to%20users.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.01700v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepVIS%253A%2520Bridging%2520Natural%2520Language%2520and%2520Data%2520Visualization%2520Through%250A%2520%2520Step-wise%2520Reasoning%26entry.906535625%3DZhihao%2520Shuai%2520and%2520Boyan%2520Li%2520and%2520Siyu%2520Yan%2520and%2520Yuyu%2520Luo%2520and%2520Weikai%2520Yang%26entry.1292438233%3D%2520%2520Although%2520data%2520visualization%2520is%2520powerful%2520for%2520revealing%2520patterns%2520and%250Acommunicating%2520insights%252C%2520creating%2520effective%2520visualizations%2520requires%2520familiarity%250Awith%2520authoring%2520tools%2520and%2520often%2520disrupts%2520the%2520analysis%2520flow.%2520While%2520large%2520language%250Amodels%2520show%2520promise%2520for%2520automatically%2520converting%2520analysis%2520intent%2520into%250Avisualizations%252C%2520existing%2520methods%2520function%2520as%2520black%2520boxes%2520without%2520transparent%250Areasoning%2520processes%252C%2520which%2520prevents%2520users%2520from%2520understanding%2520design%2520rationales%250Aand%2520refining%2520suboptimal%2520outputs.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520integrating%250AChain-of-Thought%2520%2528CoT%2529%2520reasoning%2520into%2520the%2520Natural%2520Language%2520to%2520Visualization%250A%2528NL2VIS%2529%2520pipeline.%2520First%252C%2520we%2520design%2520a%2520comprehensive%2520CoT%2520reasoning%2520process%2520for%250ANL2VIS%2520and%2520develop%2520an%2520automatic%2520pipeline%2520to%2520equip%2520existing%2520datasets%2520with%250Astructured%2520reasoning%2520steps.%2520Second%252C%2520we%2520introduce%2520nvBench-CoT%252C%2520a%2520specialized%250Adataset%2520capturing%2520detailed%2520step-by-step%2520reasoning%2520from%2520ambiguous%2520natural%250Alanguage%2520descriptions%2520to%2520finalized%2520visualizations%252C%2520which%2520enables%250Astate-of-the-art%2520performance%2520when%2520used%2520for%2520model%2520fine-tuning.%2520Third%252C%2520we%2520develop%250ADeepVIS%252C%2520an%2520interactive%2520visual%2520interface%2520that%2520tightly%2520integrates%2520with%2520the%2520CoT%250Areasoning%2520process%252C%2520allowing%2520users%2520to%2520inspect%2520reasoning%2520steps%252C%2520identify%2520errors%252C%250Aand%2520make%2520targeted%2520adjustments%2520to%2520improve%2520visualization%2520outcomes.%2520Quantitative%250Abenchmark%2520evaluations%252C%2520two%2520use%2520cases%252C%2520and%2520a%2520user%2520study%2520collectively%2520demonstrate%250Athat%2520our%2520CoT%2520framework%2520effectively%2520enhances%2520NL2VIS%2520quality%2520while%2520providing%250Ainsightful%2520reasoning%2520steps%2520to%2520users.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.01700v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepVIS%3A%20Bridging%20Natural%20Language%20and%20Data%20Visualization%20Through%0A%20%20Step-wise%20Reasoning&entry.906535625=Zhihao%20Shuai%20and%20Boyan%20Li%20and%20Siyu%20Yan%20and%20Yuyu%20Luo%20and%20Weikai%20Yang&entry.1292438233=%20%20Although%20data%20visualization%20is%20powerful%20for%20revealing%20patterns%20and%0Acommunicating%20insights%2C%20creating%20effective%20visualizations%20requires%20familiarity%0Awith%20authoring%20tools%20and%20often%20disrupts%20the%20analysis%20flow.%20While%20large%20language%0Amodels%20show%20promise%20for%20automatically%20converting%20analysis%20intent%20into%0Avisualizations%2C%20existing%20methods%20function%20as%20black%20boxes%20without%20transparent%0Areasoning%20processes%2C%20which%20prevents%20users%20from%20understanding%20design%20rationales%0Aand%20refining%20suboptimal%20outputs.%20To%20bridge%20this%20gap%2C%20we%20propose%20integrating%0AChain-of-Thought%20%28CoT%29%20reasoning%20into%20the%20Natural%20Language%20to%20Visualization%0A%28NL2VIS%29%20pipeline.%20First%2C%20we%20design%20a%20comprehensive%20CoT%20reasoning%20process%20for%0ANL2VIS%20and%20develop%20an%20automatic%20pipeline%20to%20equip%20existing%20datasets%20with%0Astructured%20reasoning%20steps.%20Second%2C%20we%20introduce%20nvBench-CoT%2C%20a%20specialized%0Adataset%20capturing%20detailed%20step-by-step%20reasoning%20from%20ambiguous%20natural%0Alanguage%20descriptions%20to%20finalized%20visualizations%2C%20which%20enables%0Astate-of-the-art%20performance%20when%20used%20for%20model%20fine-tuning.%20Third%2C%20we%20develop%0ADeepVIS%2C%20an%20interactive%20visual%20interface%20that%20tightly%20integrates%20with%20the%20CoT%0Areasoning%20process%2C%20allowing%20users%20to%20inspect%20reasoning%20steps%2C%20identify%20errors%2C%0Aand%20make%20targeted%20adjustments%20to%20improve%20visualization%20outcomes.%20Quantitative%0Abenchmark%20evaluations%2C%20two%20use%20cases%2C%20and%20a%20user%20study%20collectively%20demonstrate%0Athat%20our%20CoT%20framework%20effectively%20enhances%20NL2VIS%20quality%20while%20providing%0Ainsightful%20reasoning%20steps%20to%20users.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.01700v2&entry.124074799=Read"},
{"title": "LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text\n  Pairing", "author": "Federico Girella and Davide Talon and Ziyue Liu and Zanxi Ruan and Yiming Wang and Marco Cristani", "abstract": "  Fashion design is a complex creative process that blends visual and textual\nexpressions. Designers convey ideas through sketches, which define spatial\nstructure and design elements, and textual descriptions, capturing material,\ntexture, and stylistic details. In this paper, we present LOcalized Text and\nSketch for fashion image generation (LOTS), an approach for compositional\nsketch-text based generation of complete fashion outlooks. LOTS leverages a\nglobal description with paired localized sketch + text information for\nconditioning and introduces a novel step-based merging strategy for diffusion\nadaptation. First, a Modularized Pair-Centric representation encodes sketches\nand text into a shared latent space while preserving independent localized\nfeatures; then, a Diffusion Pair Guidance phase integrates both local and\nglobal conditioning via attention-based guidance within the diffusion model's\nmulti-step denoising process. To validate our method, we build on Fashionpedia\nto release Sketchy, the first fashion dataset where multiple text-sketch pairs\nare provided per image. Quantitative results show LOTS achieves\nstate-of-the-art image generation performance on both global and localized\nmetrics, while qualitative examples and a human evaluation study highlight its\nunprecedented level of design customization.\n", "link": "http://arxiv.org/abs/2507.22627v2", "date": "2025-09-04", "relevancy": 2.664, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.7659}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6603}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LOTS%20of%20Fashion%21%20Multi-Conditioning%20for%20Image%20Generation%20via%20Sketch-Text%0A%20%20Pairing&body=Title%3A%20LOTS%20of%20Fashion%21%20Multi-Conditioning%20for%20Image%20Generation%20via%20Sketch-Text%0A%20%20Pairing%0AAuthor%3A%20Federico%20Girella%20and%20Davide%20Talon%20and%20Ziyue%20Liu%20and%20Zanxi%20Ruan%20and%20Yiming%20Wang%20and%20Marco%20Cristani%0AAbstract%3A%20%20%20Fashion%20design%20is%20a%20complex%20creative%20process%20that%20blends%20visual%20and%20textual%0Aexpressions.%20Designers%20convey%20ideas%20through%20sketches%2C%20which%20define%20spatial%0Astructure%20and%20design%20elements%2C%20and%20textual%20descriptions%2C%20capturing%20material%2C%0Atexture%2C%20and%20stylistic%20details.%20In%20this%20paper%2C%20we%20present%20LOcalized%20Text%20and%0ASketch%20for%20fashion%20image%20generation%20%28LOTS%29%2C%20an%20approach%20for%20compositional%0Asketch-text%20based%20generation%20of%20complete%20fashion%20outlooks.%20LOTS%20leverages%20a%0Aglobal%20description%20with%20paired%20localized%20sketch%20%2B%20text%20information%20for%0Aconditioning%20and%20introduces%20a%20novel%20step-based%20merging%20strategy%20for%20diffusion%0Aadaptation.%20First%2C%20a%20Modularized%20Pair-Centric%20representation%20encodes%20sketches%0Aand%20text%20into%20a%20shared%20latent%20space%20while%20preserving%20independent%20localized%0Afeatures%3B%20then%2C%20a%20Diffusion%20Pair%20Guidance%20phase%20integrates%20both%20local%20and%0Aglobal%20conditioning%20via%20attention-based%20guidance%20within%20the%20diffusion%20model%27s%0Amulti-step%20denoising%20process.%20To%20validate%20our%20method%2C%20we%20build%20on%20Fashionpedia%0Ato%20release%20Sketchy%2C%20the%20first%20fashion%20dataset%20where%20multiple%20text-sketch%20pairs%0Aare%20provided%20per%20image.%20Quantitative%20results%20show%20LOTS%20achieves%0Astate-of-the-art%20image%20generation%20performance%20on%20both%20global%20and%20localized%0Ametrics%2C%20while%20qualitative%20examples%20and%20a%20human%20evaluation%20study%20highlight%20its%0Aunprecedented%20level%20of%20design%20customization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22627v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLOTS%2520of%2520Fashion%2521%2520Multi-Conditioning%2520for%2520Image%2520Generation%2520via%2520Sketch-Text%250A%2520%2520Pairing%26entry.906535625%3DFederico%2520Girella%2520and%2520Davide%2520Talon%2520and%2520Ziyue%2520Liu%2520and%2520Zanxi%2520Ruan%2520and%2520Yiming%2520Wang%2520and%2520Marco%2520Cristani%26entry.1292438233%3D%2520%2520Fashion%2520design%2520is%2520a%2520complex%2520creative%2520process%2520that%2520blends%2520visual%2520and%2520textual%250Aexpressions.%2520Designers%2520convey%2520ideas%2520through%2520sketches%252C%2520which%2520define%2520spatial%250Astructure%2520and%2520design%2520elements%252C%2520and%2520textual%2520descriptions%252C%2520capturing%2520material%252C%250Atexture%252C%2520and%2520stylistic%2520details.%2520In%2520this%2520paper%252C%2520we%2520present%2520LOcalized%2520Text%2520and%250ASketch%2520for%2520fashion%2520image%2520generation%2520%2528LOTS%2529%252C%2520an%2520approach%2520for%2520compositional%250Asketch-text%2520based%2520generation%2520of%2520complete%2520fashion%2520outlooks.%2520LOTS%2520leverages%2520a%250Aglobal%2520description%2520with%2520paired%2520localized%2520sketch%2520%252B%2520text%2520information%2520for%250Aconditioning%2520and%2520introduces%2520a%2520novel%2520step-based%2520merging%2520strategy%2520for%2520diffusion%250Aadaptation.%2520First%252C%2520a%2520Modularized%2520Pair-Centric%2520representation%2520encodes%2520sketches%250Aand%2520text%2520into%2520a%2520shared%2520latent%2520space%2520while%2520preserving%2520independent%2520localized%250Afeatures%253B%2520then%252C%2520a%2520Diffusion%2520Pair%2520Guidance%2520phase%2520integrates%2520both%2520local%2520and%250Aglobal%2520conditioning%2520via%2520attention-based%2520guidance%2520within%2520the%2520diffusion%2520model%2527s%250Amulti-step%2520denoising%2520process.%2520To%2520validate%2520our%2520method%252C%2520we%2520build%2520on%2520Fashionpedia%250Ato%2520release%2520Sketchy%252C%2520the%2520first%2520fashion%2520dataset%2520where%2520multiple%2520text-sketch%2520pairs%250Aare%2520provided%2520per%2520image.%2520Quantitative%2520results%2520show%2520LOTS%2520achieves%250Astate-of-the-art%2520image%2520generation%2520performance%2520on%2520both%2520global%2520and%2520localized%250Ametrics%252C%2520while%2520qualitative%2520examples%2520and%2520a%2520human%2520evaluation%2520study%2520highlight%2520its%250Aunprecedented%2520level%2520of%2520design%2520customization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22627v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LOTS%20of%20Fashion%21%20Multi-Conditioning%20for%20Image%20Generation%20via%20Sketch-Text%0A%20%20Pairing&entry.906535625=Federico%20Girella%20and%20Davide%20Talon%20and%20Ziyue%20Liu%20and%20Zanxi%20Ruan%20and%20Yiming%20Wang%20and%20Marco%20Cristani&entry.1292438233=%20%20Fashion%20design%20is%20a%20complex%20creative%20process%20that%20blends%20visual%20and%20textual%0Aexpressions.%20Designers%20convey%20ideas%20through%20sketches%2C%20which%20define%20spatial%0Astructure%20and%20design%20elements%2C%20and%20textual%20descriptions%2C%20capturing%20material%2C%0Atexture%2C%20and%20stylistic%20details.%20In%20this%20paper%2C%20we%20present%20LOcalized%20Text%20and%0ASketch%20for%20fashion%20image%20generation%20%28LOTS%29%2C%20an%20approach%20for%20compositional%0Asketch-text%20based%20generation%20of%20complete%20fashion%20outlooks.%20LOTS%20leverages%20a%0Aglobal%20description%20with%20paired%20localized%20sketch%20%2B%20text%20information%20for%0Aconditioning%20and%20introduces%20a%20novel%20step-based%20merging%20strategy%20for%20diffusion%0Aadaptation.%20First%2C%20a%20Modularized%20Pair-Centric%20representation%20encodes%20sketches%0Aand%20text%20into%20a%20shared%20latent%20space%20while%20preserving%20independent%20localized%0Afeatures%3B%20then%2C%20a%20Diffusion%20Pair%20Guidance%20phase%20integrates%20both%20local%20and%0Aglobal%20conditioning%20via%20attention-based%20guidance%20within%20the%20diffusion%20model%27s%0Amulti-step%20denoising%20process.%20To%20validate%20our%20method%2C%20we%20build%20on%20Fashionpedia%0Ato%20release%20Sketchy%2C%20the%20first%20fashion%20dataset%20where%20multiple%20text-sketch%20pairs%0Aare%20provided%20per%20image.%20Quantitative%20results%20show%20LOTS%20achieves%0Astate-of-the-art%20image%20generation%20performance%20on%20both%20global%20and%20localized%0Ametrics%2C%20while%20qualitative%20examples%20and%20a%20human%20evaluation%20study%20highlight%20its%0Aunprecedented%20level%20of%20design%20customization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22627v2&entry.124074799=Read"},
{"title": "Delta Activations: A Representation for Finetuned Large Language Models", "author": "Zhiqiu Xu and Amish Sethi and Mayur Naik and Ser-Nam Lim", "abstract": "  The success of powerful open source Large Language Models (LLMs) has enabled\nthe community to create a vast collection of post-trained models adapted to\nspecific tasks and domains. However, navigating and understanding these models\nremains challenging due to inconsistent metadata and unstructured repositories.\nWe introduce Delta Activations, a method to represent finetuned models as\nvector embeddings by measuring shifts in their internal activations relative to\na base model. This representation allows for effective clustering by domain and\ntask, revealing structure in the model landscape. Delta Activations also\ndemonstrate desirable properties: it is robust across finetuning settings and\nexhibits an additive property when finetuning datasets are mixed. In addition,\nwe show that Delta Activations can embed tasks via few-shot finetuning, and\nfurther explore its use for model selection and merging. We hope Delta\nActivations can facilitate the practice of reusing publicly available models.\nCode is available at https://github.com/OscarXZQ/delta_activations.\n", "link": "http://arxiv.org/abs/2509.04442v1", "date": "2025-09-04", "relevancy": 2.6489, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5316}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5289}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Delta%20Activations%3A%20A%20Representation%20for%20Finetuned%20Large%20Language%20Models&body=Title%3A%20Delta%20Activations%3A%20A%20Representation%20for%20Finetuned%20Large%20Language%20Models%0AAuthor%3A%20Zhiqiu%20Xu%20and%20Amish%20Sethi%20and%20Mayur%20Naik%20and%20Ser-Nam%20Lim%0AAbstract%3A%20%20%20The%20success%20of%20powerful%20open%20source%20Large%20Language%20Models%20%28LLMs%29%20has%20enabled%0Athe%20community%20to%20create%20a%20vast%20collection%20of%20post-trained%20models%20adapted%20to%0Aspecific%20tasks%20and%20domains.%20However%2C%20navigating%20and%20understanding%20these%20models%0Aremains%20challenging%20due%20to%20inconsistent%20metadata%20and%20unstructured%20repositories.%0AWe%20introduce%20Delta%20Activations%2C%20a%20method%20to%20represent%20finetuned%20models%20as%0Avector%20embeddings%20by%20measuring%20shifts%20in%20their%20internal%20activations%20relative%20to%0Aa%20base%20model.%20This%20representation%20allows%20for%20effective%20clustering%20by%20domain%20and%0Atask%2C%20revealing%20structure%20in%20the%20model%20landscape.%20Delta%20Activations%20also%0Ademonstrate%20desirable%20properties%3A%20it%20is%20robust%20across%20finetuning%20settings%20and%0Aexhibits%20an%20additive%20property%20when%20finetuning%20datasets%20are%20mixed.%20In%20addition%2C%0Awe%20show%20that%20Delta%20Activations%20can%20embed%20tasks%20via%20few-shot%20finetuning%2C%20and%0Afurther%20explore%20its%20use%20for%20model%20selection%20and%20merging.%20We%20hope%20Delta%0AActivations%20can%20facilitate%20the%20practice%20of%20reusing%20publicly%20available%20models.%0ACode%20is%20available%20at%20https%3A//github.com/OscarXZQ/delta_activations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04442v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDelta%2520Activations%253A%2520A%2520Representation%2520for%2520Finetuned%2520Large%2520Language%2520Models%26entry.906535625%3DZhiqiu%2520Xu%2520and%2520Amish%2520Sethi%2520and%2520Mayur%2520Naik%2520and%2520Ser-Nam%2520Lim%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520powerful%2520open%2520source%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520enabled%250Athe%2520community%2520to%2520create%2520a%2520vast%2520collection%2520of%2520post-trained%2520models%2520adapted%2520to%250Aspecific%2520tasks%2520and%2520domains.%2520However%252C%2520navigating%2520and%2520understanding%2520these%2520models%250Aremains%2520challenging%2520due%2520to%2520inconsistent%2520metadata%2520and%2520unstructured%2520repositories.%250AWe%2520introduce%2520Delta%2520Activations%252C%2520a%2520method%2520to%2520represent%2520finetuned%2520models%2520as%250Avector%2520embeddings%2520by%2520measuring%2520shifts%2520in%2520their%2520internal%2520activations%2520relative%2520to%250Aa%2520base%2520model.%2520This%2520representation%2520allows%2520for%2520effective%2520clustering%2520by%2520domain%2520and%250Atask%252C%2520revealing%2520structure%2520in%2520the%2520model%2520landscape.%2520Delta%2520Activations%2520also%250Ademonstrate%2520desirable%2520properties%253A%2520it%2520is%2520robust%2520across%2520finetuning%2520settings%2520and%250Aexhibits%2520an%2520additive%2520property%2520when%2520finetuning%2520datasets%2520are%2520mixed.%2520In%2520addition%252C%250Awe%2520show%2520that%2520Delta%2520Activations%2520can%2520embed%2520tasks%2520via%2520few-shot%2520finetuning%252C%2520and%250Afurther%2520explore%2520its%2520use%2520for%2520model%2520selection%2520and%2520merging.%2520We%2520hope%2520Delta%250AActivations%2520can%2520facilitate%2520the%2520practice%2520of%2520reusing%2520publicly%2520available%2520models.%250ACode%2520is%2520available%2520at%2520https%253A//github.com/OscarXZQ/delta_activations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04442v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Delta%20Activations%3A%20A%20Representation%20for%20Finetuned%20Large%20Language%20Models&entry.906535625=Zhiqiu%20Xu%20and%20Amish%20Sethi%20and%20Mayur%20Naik%20and%20Ser-Nam%20Lim&entry.1292438233=%20%20The%20success%20of%20powerful%20open%20source%20Large%20Language%20Models%20%28LLMs%29%20has%20enabled%0Athe%20community%20to%20create%20a%20vast%20collection%20of%20post-trained%20models%20adapted%20to%0Aspecific%20tasks%20and%20domains.%20However%2C%20navigating%20and%20understanding%20these%20models%0Aremains%20challenging%20due%20to%20inconsistent%20metadata%20and%20unstructured%20repositories.%0AWe%20introduce%20Delta%20Activations%2C%20a%20method%20to%20represent%20finetuned%20models%20as%0Avector%20embeddings%20by%20measuring%20shifts%20in%20their%20internal%20activations%20relative%20to%0Aa%20base%20model.%20This%20representation%20allows%20for%20effective%20clustering%20by%20domain%20and%0Atask%2C%20revealing%20structure%20in%20the%20model%20landscape.%20Delta%20Activations%20also%0Ademonstrate%20desirable%20properties%3A%20it%20is%20robust%20across%20finetuning%20settings%20and%0Aexhibits%20an%20additive%20property%20when%20finetuning%20datasets%20are%20mixed.%20In%20addition%2C%0Awe%20show%20that%20Delta%20Activations%20can%20embed%20tasks%20via%20few-shot%20finetuning%2C%20and%0Afurther%20explore%20its%20use%20for%20model%20selection%20and%20merging.%20We%20hope%20Delta%0AActivations%20can%20facilitate%20the%20practice%20of%20reusing%20publicly%20available%20models.%0ACode%20is%20available%20at%20https%3A//github.com/OscarXZQ/delta_activations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04442v1&entry.124074799=Read"},
{"title": "Global-to-Local or Local-to-Global? Enhancing Image Retrieval with\n  Efficient Local Search and Effective Global Re-ranking", "author": "Dror Aiger and Bingyi Cao and Kaifeng Chen and Andre Araujo", "abstract": "  The dominant paradigm in image retrieval systems today is to search large\ndatabases using global image features, and re-rank those initial results with\nlocal image feature matching techniques. This design, dubbed global-to-local,\nstems from the computational cost of local matching approaches, which can only\nbe afforded for a small number of retrieved images. However, emerging efficient\nlocal feature search approaches have opened up new possibilities, in particular\nenabling detailed retrieval at large scale, to find partial matches which are\noften missed by global feature search. In parallel, global feature-based\nre-ranking has shown promising results with high computational efficiency. In\nthis work, we leverage these building blocks to introduce a local-to-global\nretrieval paradigm, where efficient local feature search meets effective global\nfeature re-ranking. Critically, we propose a re-ranking method where global\nfeatures are computed on-the-fly, based on the local feature retrieval\nsimilarities. Such re-ranking-only global features leverage multidimensional\nscaling techniques to create embeddings which respect the local similarities\nobtained during search, enabling a significant re-ranking boost.\nExperimentally, we demonstrate solid retrieval performance, setting new\nstate-of-the-art results on the Revisited Oxford and Paris datasets.\n", "link": "http://arxiv.org/abs/2509.04351v1", "date": "2025-09-04", "relevancy": 2.6252, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.555}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5118}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Global-to-Local%20or%20Local-to-Global%3F%20Enhancing%20Image%20Retrieval%20with%0A%20%20Efficient%20Local%20Search%20and%20Effective%20Global%20Re-ranking&body=Title%3A%20Global-to-Local%20or%20Local-to-Global%3F%20Enhancing%20Image%20Retrieval%20with%0A%20%20Efficient%20Local%20Search%20and%20Effective%20Global%20Re-ranking%0AAuthor%3A%20Dror%20Aiger%20and%20Bingyi%20Cao%20and%20Kaifeng%20Chen%20and%20Andre%20Araujo%0AAbstract%3A%20%20%20The%20dominant%20paradigm%20in%20image%20retrieval%20systems%20today%20is%20to%20search%20large%0Adatabases%20using%20global%20image%20features%2C%20and%20re-rank%20those%20initial%20results%20with%0Alocal%20image%20feature%20matching%20techniques.%20This%20design%2C%20dubbed%20global-to-local%2C%0Astems%20from%20the%20computational%20cost%20of%20local%20matching%20approaches%2C%20which%20can%20only%0Abe%20afforded%20for%20a%20small%20number%20of%20retrieved%20images.%20However%2C%20emerging%20efficient%0Alocal%20feature%20search%20approaches%20have%20opened%20up%20new%20possibilities%2C%20in%20particular%0Aenabling%20detailed%20retrieval%20at%20large%20scale%2C%20to%20find%20partial%20matches%20which%20are%0Aoften%20missed%20by%20global%20feature%20search.%20In%20parallel%2C%20global%20feature-based%0Are-ranking%20has%20shown%20promising%20results%20with%20high%20computational%20efficiency.%20In%0Athis%20work%2C%20we%20leverage%20these%20building%20blocks%20to%20introduce%20a%20local-to-global%0Aretrieval%20paradigm%2C%20where%20efficient%20local%20feature%20search%20meets%20effective%20global%0Afeature%20re-ranking.%20Critically%2C%20we%20propose%20a%20re-ranking%20method%20where%20global%0Afeatures%20are%20computed%20on-the-fly%2C%20based%20on%20the%20local%20feature%20retrieval%0Asimilarities.%20Such%20re-ranking-only%20global%20features%20leverage%20multidimensional%0Ascaling%20techniques%20to%20create%20embeddings%20which%20respect%20the%20local%20similarities%0Aobtained%20during%20search%2C%20enabling%20a%20significant%20re-ranking%20boost.%0AExperimentally%2C%20we%20demonstrate%20solid%20retrieval%20performance%2C%20setting%20new%0Astate-of-the-art%20results%20on%20the%20Revisited%20Oxford%20and%20Paris%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04351v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobal-to-Local%2520or%2520Local-to-Global%253F%2520Enhancing%2520Image%2520Retrieval%2520with%250A%2520%2520Efficient%2520Local%2520Search%2520and%2520Effective%2520Global%2520Re-ranking%26entry.906535625%3DDror%2520Aiger%2520and%2520Bingyi%2520Cao%2520and%2520Kaifeng%2520Chen%2520and%2520Andre%2520Araujo%26entry.1292438233%3D%2520%2520The%2520dominant%2520paradigm%2520in%2520image%2520retrieval%2520systems%2520today%2520is%2520to%2520search%2520large%250Adatabases%2520using%2520global%2520image%2520features%252C%2520and%2520re-rank%2520those%2520initial%2520results%2520with%250Alocal%2520image%2520feature%2520matching%2520techniques.%2520This%2520design%252C%2520dubbed%2520global-to-local%252C%250Astems%2520from%2520the%2520computational%2520cost%2520of%2520local%2520matching%2520approaches%252C%2520which%2520can%2520only%250Abe%2520afforded%2520for%2520a%2520small%2520number%2520of%2520retrieved%2520images.%2520However%252C%2520emerging%2520efficient%250Alocal%2520feature%2520search%2520approaches%2520have%2520opened%2520up%2520new%2520possibilities%252C%2520in%2520particular%250Aenabling%2520detailed%2520retrieval%2520at%2520large%2520scale%252C%2520to%2520find%2520partial%2520matches%2520which%2520are%250Aoften%2520missed%2520by%2520global%2520feature%2520search.%2520In%2520parallel%252C%2520global%2520feature-based%250Are-ranking%2520has%2520shown%2520promising%2520results%2520with%2520high%2520computational%2520efficiency.%2520In%250Athis%2520work%252C%2520we%2520leverage%2520these%2520building%2520blocks%2520to%2520introduce%2520a%2520local-to-global%250Aretrieval%2520paradigm%252C%2520where%2520efficient%2520local%2520feature%2520search%2520meets%2520effective%2520global%250Afeature%2520re-ranking.%2520Critically%252C%2520we%2520propose%2520a%2520re-ranking%2520method%2520where%2520global%250Afeatures%2520are%2520computed%2520on-the-fly%252C%2520based%2520on%2520the%2520local%2520feature%2520retrieval%250Asimilarities.%2520Such%2520re-ranking-only%2520global%2520features%2520leverage%2520multidimensional%250Ascaling%2520techniques%2520to%2520create%2520embeddings%2520which%2520respect%2520the%2520local%2520similarities%250Aobtained%2520during%2520search%252C%2520enabling%2520a%2520significant%2520re-ranking%2520boost.%250AExperimentally%252C%2520we%2520demonstrate%2520solid%2520retrieval%2520performance%252C%2520setting%2520new%250Astate-of-the-art%2520results%2520on%2520the%2520Revisited%2520Oxford%2520and%2520Paris%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04351v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global-to-Local%20or%20Local-to-Global%3F%20Enhancing%20Image%20Retrieval%20with%0A%20%20Efficient%20Local%20Search%20and%20Effective%20Global%20Re-ranking&entry.906535625=Dror%20Aiger%20and%20Bingyi%20Cao%20and%20Kaifeng%20Chen%20and%20Andre%20Araujo&entry.1292438233=%20%20The%20dominant%20paradigm%20in%20image%20retrieval%20systems%20today%20is%20to%20search%20large%0Adatabases%20using%20global%20image%20features%2C%20and%20re-rank%20those%20initial%20results%20with%0Alocal%20image%20feature%20matching%20techniques.%20This%20design%2C%20dubbed%20global-to-local%2C%0Astems%20from%20the%20computational%20cost%20of%20local%20matching%20approaches%2C%20which%20can%20only%0Abe%20afforded%20for%20a%20small%20number%20of%20retrieved%20images.%20However%2C%20emerging%20efficient%0Alocal%20feature%20search%20approaches%20have%20opened%20up%20new%20possibilities%2C%20in%20particular%0Aenabling%20detailed%20retrieval%20at%20large%20scale%2C%20to%20find%20partial%20matches%20which%20are%0Aoften%20missed%20by%20global%20feature%20search.%20In%20parallel%2C%20global%20feature-based%0Are-ranking%20has%20shown%20promising%20results%20with%20high%20computational%20efficiency.%20In%0Athis%20work%2C%20we%20leverage%20these%20building%20blocks%20to%20introduce%20a%20local-to-global%0Aretrieval%20paradigm%2C%20where%20efficient%20local%20feature%20search%20meets%20effective%20global%0Afeature%20re-ranking.%20Critically%2C%20we%20propose%20a%20re-ranking%20method%20where%20global%0Afeatures%20are%20computed%20on-the-fly%2C%20based%20on%20the%20local%20feature%20retrieval%0Asimilarities.%20Such%20re-ranking-only%20global%20features%20leverage%20multidimensional%0Ascaling%20techniques%20to%20create%20embeddings%20which%20respect%20the%20local%20similarities%0Aobtained%20during%20search%2C%20enabling%20a%20significant%20re-ranking%20boost.%0AExperimentally%2C%20we%20demonstrate%20solid%20retrieval%20performance%2C%20setting%20new%0Astate-of-the-art%20results%20on%20the%20Revisited%20Oxford%20and%20Paris%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04351v1&entry.124074799=Read"},
{"title": "AudioCodecBench: A Comprehensive Benchmark for Audio Codec Evaluation", "author": "Lu Wang and Hao Chen and Siyu Wu and Zhiyue Wu and Hao Zhou and Chengfeng Zhang and Ting Wang and Haodi Zhang", "abstract": "  Multimodal Large Language Models (MLLMs) have been widely applied in speech\nand music. This tendency has led to a focus on audio tokenization for Large\nModels (LMs). Unlike semantic-only text tokens, audio tokens must both capture\nglobal semantic content and preserve fine-grained acoustic details. Moreover,\nthey provide a discrete method for speech and music that can be effectively\nintegrated into MLLMs. However, existing research is unsuitable in the\ndefinitions of semantic tokens and acoustic tokens. In addition, the evaluation\nof different codecs typically concentrates on specific domains or tasks, such\nas reconstruction or Automatic Speech Recognition (ASR) task, which prevents\nfair and comprehensive comparisons. To address these problems, this paper\nprovides suitable definitions for semantic and acoustic tokens and introduces a\nsystematic evaluation framework. This framework allows for a comprehensive\nassessment of codecs' capabilities which evaluate across four dimensions: audio\nreconstruction metric, codebook index (ID) stability, decoder-only transformer\nperplexity, and performance on downstream probe tasks. Our results show the\ncorrectness of the provided suitable definitions and the correlation among\nreconstruction metrics, codebook ID stability, downstream probe tasks and\nperplexity.\n", "link": "http://arxiv.org/abs/2509.02349v2", "date": "2025-09-04", "relevancy": 2.5809, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5463}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5463}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AudioCodecBench%3A%20A%20Comprehensive%20Benchmark%20for%20Audio%20Codec%20Evaluation&body=Title%3A%20AudioCodecBench%3A%20A%20Comprehensive%20Benchmark%20for%20Audio%20Codec%20Evaluation%0AAuthor%3A%20Lu%20Wang%20and%20Hao%20Chen%20and%20Siyu%20Wu%20and%20Zhiyue%20Wu%20and%20Hao%20Zhou%20and%20Chengfeng%20Zhang%20and%20Ting%20Wang%20and%20Haodi%20Zhang%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20been%20widely%20applied%20in%20speech%0Aand%20music.%20This%20tendency%20has%20led%20to%20a%20focus%20on%20audio%20tokenization%20for%20Large%0AModels%20%28LMs%29.%20Unlike%20semantic-only%20text%20tokens%2C%20audio%20tokens%20must%20both%20capture%0Aglobal%20semantic%20content%20and%20preserve%20fine-grained%20acoustic%20details.%20Moreover%2C%0Athey%20provide%20a%20discrete%20method%20for%20speech%20and%20music%20that%20can%20be%20effectively%0Aintegrated%20into%20MLLMs.%20However%2C%20existing%20research%20is%20unsuitable%20in%20the%0Adefinitions%20of%20semantic%20tokens%20and%20acoustic%20tokens.%20In%20addition%2C%20the%20evaluation%0Aof%20different%20codecs%20typically%20concentrates%20on%20specific%20domains%20or%20tasks%2C%20such%0Aas%20reconstruction%20or%20Automatic%20Speech%20Recognition%20%28ASR%29%20task%2C%20which%20prevents%0Afair%20and%20comprehensive%20comparisons.%20To%20address%20these%20problems%2C%20this%20paper%0Aprovides%20suitable%20definitions%20for%20semantic%20and%20acoustic%20tokens%20and%20introduces%20a%0Asystematic%20evaluation%20framework.%20This%20framework%20allows%20for%20a%20comprehensive%0Aassessment%20of%20codecs%27%20capabilities%20which%20evaluate%20across%20four%20dimensions%3A%20audio%0Areconstruction%20metric%2C%20codebook%20index%20%28ID%29%20stability%2C%20decoder-only%20transformer%0Aperplexity%2C%20and%20performance%20on%20downstream%20probe%20tasks.%20Our%20results%20show%20the%0Acorrectness%20of%20the%20provided%20suitable%20definitions%20and%20the%20correlation%20among%0Areconstruction%20metrics%2C%20codebook%20ID%20stability%2C%20downstream%20probe%20tasks%20and%0Aperplexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.02349v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudioCodecBench%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Audio%2520Codec%2520Evaluation%26entry.906535625%3DLu%2520Wang%2520and%2520Hao%2520Chen%2520and%2520Siyu%2520Wu%2520and%2520Zhiyue%2520Wu%2520and%2520Hao%2520Zhou%2520and%2520Chengfeng%2520Zhang%2520and%2520Ting%2520Wang%2520and%2520Haodi%2520Zhang%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520been%2520widely%2520applied%2520in%2520speech%250Aand%2520music.%2520This%2520tendency%2520has%2520led%2520to%2520a%2520focus%2520on%2520audio%2520tokenization%2520for%2520Large%250AModels%2520%2528LMs%2529.%2520Unlike%2520semantic-only%2520text%2520tokens%252C%2520audio%2520tokens%2520must%2520both%2520capture%250Aglobal%2520semantic%2520content%2520and%2520preserve%2520fine-grained%2520acoustic%2520details.%2520Moreover%252C%250Athey%2520provide%2520a%2520discrete%2520method%2520for%2520speech%2520and%2520music%2520that%2520can%2520be%2520effectively%250Aintegrated%2520into%2520MLLMs.%2520However%252C%2520existing%2520research%2520is%2520unsuitable%2520in%2520the%250Adefinitions%2520of%2520semantic%2520tokens%2520and%2520acoustic%2520tokens.%2520In%2520addition%252C%2520the%2520evaluation%250Aof%2520different%2520codecs%2520typically%2520concentrates%2520on%2520specific%2520domains%2520or%2520tasks%252C%2520such%250Aas%2520reconstruction%2520or%2520Automatic%2520Speech%2520Recognition%2520%2528ASR%2529%2520task%252C%2520which%2520prevents%250Afair%2520and%2520comprehensive%2520comparisons.%2520To%2520address%2520these%2520problems%252C%2520this%2520paper%250Aprovides%2520suitable%2520definitions%2520for%2520semantic%2520and%2520acoustic%2520tokens%2520and%2520introduces%2520a%250Asystematic%2520evaluation%2520framework.%2520This%2520framework%2520allows%2520for%2520a%2520comprehensive%250Aassessment%2520of%2520codecs%2527%2520capabilities%2520which%2520evaluate%2520across%2520four%2520dimensions%253A%2520audio%250Areconstruction%2520metric%252C%2520codebook%2520index%2520%2528ID%2529%2520stability%252C%2520decoder-only%2520transformer%250Aperplexity%252C%2520and%2520performance%2520on%2520downstream%2520probe%2520tasks.%2520Our%2520results%2520show%2520the%250Acorrectness%2520of%2520the%2520provided%2520suitable%2520definitions%2520and%2520the%2520correlation%2520among%250Areconstruction%2520metrics%252C%2520codebook%2520ID%2520stability%252C%2520downstream%2520probe%2520tasks%2520and%250Aperplexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.02349v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AudioCodecBench%3A%20A%20Comprehensive%20Benchmark%20for%20Audio%20Codec%20Evaluation&entry.906535625=Lu%20Wang%20and%20Hao%20Chen%20and%20Siyu%20Wu%20and%20Zhiyue%20Wu%20and%20Hao%20Zhou%20and%20Chengfeng%20Zhang%20and%20Ting%20Wang%20and%20Haodi%20Zhang&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20been%20widely%20applied%20in%20speech%0Aand%20music.%20This%20tendency%20has%20led%20to%20a%20focus%20on%20audio%20tokenization%20for%20Large%0AModels%20%28LMs%29.%20Unlike%20semantic-only%20text%20tokens%2C%20audio%20tokens%20must%20both%20capture%0Aglobal%20semantic%20content%20and%20preserve%20fine-grained%20acoustic%20details.%20Moreover%2C%0Athey%20provide%20a%20discrete%20method%20for%20speech%20and%20music%20that%20can%20be%20effectively%0Aintegrated%20into%20MLLMs.%20However%2C%20existing%20research%20is%20unsuitable%20in%20the%0Adefinitions%20of%20semantic%20tokens%20and%20acoustic%20tokens.%20In%20addition%2C%20the%20evaluation%0Aof%20different%20codecs%20typically%20concentrates%20on%20specific%20domains%20or%20tasks%2C%20such%0Aas%20reconstruction%20or%20Automatic%20Speech%20Recognition%20%28ASR%29%20task%2C%20which%20prevents%0Afair%20and%20comprehensive%20comparisons.%20To%20address%20these%20problems%2C%20this%20paper%0Aprovides%20suitable%20definitions%20for%20semantic%20and%20acoustic%20tokens%20and%20introduces%20a%0Asystematic%20evaluation%20framework.%20This%20framework%20allows%20for%20a%20comprehensive%0Aassessment%20of%20codecs%27%20capabilities%20which%20evaluate%20across%20four%20dimensions%3A%20audio%0Areconstruction%20metric%2C%20codebook%20index%20%28ID%29%20stability%2C%20decoder-only%20transformer%0Aperplexity%2C%20and%20performance%20on%20downstream%20probe%20tasks.%20Our%20results%20show%20the%0Acorrectness%20of%20the%20provided%20suitable%20definitions%20and%20the%20correlation%20among%0Areconstruction%20metrics%2C%20codebook%20ID%20stability%2C%20downstream%20probe%20tasks%20and%0Aperplexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.02349v2&entry.124074799=Read"},
{"title": "Completing Spatial Transcriptomics Data for Gene Expression Prediction\n  Benchmarking", "author": "Daniela Ruiz and Paula C\u00e1rdenas and Leonardo Manrique and Daniela Vega and Gabriel M. Mejia and Pablo Arbel\u00e1ez", "abstract": "  Spatial Transcriptomics is a groundbreaking technology that integrates\nhistology images with spatially resolved gene expression profiles. Among the\nvarious Spatial Transcriptomics techniques available, Visium has emerged as the\nmost widely adopted. However, its accessibility is limited by high costs, the\nneed for specialized expertise, and slow clinical integration. Additionally,\ngene capture inefficiencies lead to significant dropout, corrupting acquired\ndata. To address these challenges, the deep learning community has explored the\ngene expression prediction task directly from histology images. Yet,\ninconsistencies in datasets, preprocessing, and training protocols hinder fair\ncomparisons between models. To bridge this gap, we introduce SpaRED, a\nsystematically curated database comprising 26 public datasets, providing a\nstandardized resource for model evaluation. We further propose SpaCKLE, a\nstate-of-the-art transformer-based gene expression completion model that\nreduces mean squared error by over 82.5% compared to existing approaches.\nFinally, we establish the SpaRED benchmark, evaluating eight state-of-the-art\nprediction models on both raw and SpaCKLE-completed data, demonstrating SpaCKLE\nsubstantially improves the results across all the gene expression prediction\nmodels. Altogether, our contributions constitute the most comprehensive\nbenchmark of gene expression prediction from histology images to date and a\nstepping stone for future research on Spatial Transcriptomics.\n", "link": "http://arxiv.org/abs/2505.02980v2", "date": "2025-09-04", "relevancy": 2.5745, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5222}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5112}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5112}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Completing%20Spatial%20Transcriptomics%20Data%20for%20Gene%20Expression%20Prediction%0A%20%20Benchmarking&body=Title%3A%20Completing%20Spatial%20Transcriptomics%20Data%20for%20Gene%20Expression%20Prediction%0A%20%20Benchmarking%0AAuthor%3A%20Daniela%20Ruiz%20and%20Paula%20C%C3%A1rdenas%20and%20Leonardo%20Manrique%20and%20Daniela%20Vega%20and%20Gabriel%20M.%20Mejia%20and%20Pablo%20Arbel%C3%A1ez%0AAbstract%3A%20%20%20Spatial%20Transcriptomics%20is%20a%20groundbreaking%20technology%20that%20integrates%0Ahistology%20images%20with%20spatially%20resolved%20gene%20expression%20profiles.%20Among%20the%0Avarious%20Spatial%20Transcriptomics%20techniques%20available%2C%20Visium%20has%20emerged%20as%20the%0Amost%20widely%20adopted.%20However%2C%20its%20accessibility%20is%20limited%20by%20high%20costs%2C%20the%0Aneed%20for%20specialized%20expertise%2C%20and%20slow%20clinical%20integration.%20Additionally%2C%0Agene%20capture%20inefficiencies%20lead%20to%20significant%20dropout%2C%20corrupting%20acquired%0Adata.%20To%20address%20these%20challenges%2C%20the%20deep%20learning%20community%20has%20explored%20the%0Agene%20expression%20prediction%20task%20directly%20from%20histology%20images.%20Yet%2C%0Ainconsistencies%20in%20datasets%2C%20preprocessing%2C%20and%20training%20protocols%20hinder%20fair%0Acomparisons%20between%20models.%20To%20bridge%20this%20gap%2C%20we%20introduce%20SpaRED%2C%20a%0Asystematically%20curated%20database%20comprising%2026%20public%20datasets%2C%20providing%20a%0Astandardized%20resource%20for%20model%20evaluation.%20We%20further%20propose%20SpaCKLE%2C%20a%0Astate-of-the-art%20transformer-based%20gene%20expression%20completion%20model%20that%0Areduces%20mean%20squared%20error%20by%20over%2082.5%25%20compared%20to%20existing%20approaches.%0AFinally%2C%20we%20establish%20the%20SpaRED%20benchmark%2C%20evaluating%20eight%20state-of-the-art%0Aprediction%20models%20on%20both%20raw%20and%20SpaCKLE-completed%20data%2C%20demonstrating%20SpaCKLE%0Asubstantially%20improves%20the%20results%20across%20all%20the%20gene%20expression%20prediction%0Amodels.%20Altogether%2C%20our%20contributions%20constitute%20the%20most%20comprehensive%0Abenchmark%20of%20gene%20expression%20prediction%20from%20histology%20images%20to%20date%20and%20a%0Astepping%20stone%20for%20future%20research%20on%20Spatial%20Transcriptomics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02980v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompleting%2520Spatial%2520Transcriptomics%2520Data%2520for%2520Gene%2520Expression%2520Prediction%250A%2520%2520Benchmarking%26entry.906535625%3DDaniela%2520Ruiz%2520and%2520Paula%2520C%25C3%25A1rdenas%2520and%2520Leonardo%2520Manrique%2520and%2520Daniela%2520Vega%2520and%2520Gabriel%2520M.%2520Mejia%2520and%2520Pablo%2520Arbel%25C3%25A1ez%26entry.1292438233%3D%2520%2520Spatial%2520Transcriptomics%2520is%2520a%2520groundbreaking%2520technology%2520that%2520integrates%250Ahistology%2520images%2520with%2520spatially%2520resolved%2520gene%2520expression%2520profiles.%2520Among%2520the%250Avarious%2520Spatial%2520Transcriptomics%2520techniques%2520available%252C%2520Visium%2520has%2520emerged%2520as%2520the%250Amost%2520widely%2520adopted.%2520However%252C%2520its%2520accessibility%2520is%2520limited%2520by%2520high%2520costs%252C%2520the%250Aneed%2520for%2520specialized%2520expertise%252C%2520and%2520slow%2520clinical%2520integration.%2520Additionally%252C%250Agene%2520capture%2520inefficiencies%2520lead%2520to%2520significant%2520dropout%252C%2520corrupting%2520acquired%250Adata.%2520To%2520address%2520these%2520challenges%252C%2520the%2520deep%2520learning%2520community%2520has%2520explored%2520the%250Agene%2520expression%2520prediction%2520task%2520directly%2520from%2520histology%2520images.%2520Yet%252C%250Ainconsistencies%2520in%2520datasets%252C%2520preprocessing%252C%2520and%2520training%2520protocols%2520hinder%2520fair%250Acomparisons%2520between%2520models.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520SpaRED%252C%2520a%250Asystematically%2520curated%2520database%2520comprising%252026%2520public%2520datasets%252C%2520providing%2520a%250Astandardized%2520resource%2520for%2520model%2520evaluation.%2520We%2520further%2520propose%2520SpaCKLE%252C%2520a%250Astate-of-the-art%2520transformer-based%2520gene%2520expression%2520completion%2520model%2520that%250Areduces%2520mean%2520squared%2520error%2520by%2520over%252082.5%2525%2520compared%2520to%2520existing%2520approaches.%250AFinally%252C%2520we%2520establish%2520the%2520SpaRED%2520benchmark%252C%2520evaluating%2520eight%2520state-of-the-art%250Aprediction%2520models%2520on%2520both%2520raw%2520and%2520SpaCKLE-completed%2520data%252C%2520demonstrating%2520SpaCKLE%250Asubstantially%2520improves%2520the%2520results%2520across%2520all%2520the%2520gene%2520expression%2520prediction%250Amodels.%2520Altogether%252C%2520our%2520contributions%2520constitute%2520the%2520most%2520comprehensive%250Abenchmark%2520of%2520gene%2520expression%2520prediction%2520from%2520histology%2520images%2520to%2520date%2520and%2520a%250Astepping%2520stone%2520for%2520future%2520research%2520on%2520Spatial%2520Transcriptomics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02980v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Completing%20Spatial%20Transcriptomics%20Data%20for%20Gene%20Expression%20Prediction%0A%20%20Benchmarking&entry.906535625=Daniela%20Ruiz%20and%20Paula%20C%C3%A1rdenas%20and%20Leonardo%20Manrique%20and%20Daniela%20Vega%20and%20Gabriel%20M.%20Mejia%20and%20Pablo%20Arbel%C3%A1ez&entry.1292438233=%20%20Spatial%20Transcriptomics%20is%20a%20groundbreaking%20technology%20that%20integrates%0Ahistology%20images%20with%20spatially%20resolved%20gene%20expression%20profiles.%20Among%20the%0Avarious%20Spatial%20Transcriptomics%20techniques%20available%2C%20Visium%20has%20emerged%20as%20the%0Amost%20widely%20adopted.%20However%2C%20its%20accessibility%20is%20limited%20by%20high%20costs%2C%20the%0Aneed%20for%20specialized%20expertise%2C%20and%20slow%20clinical%20integration.%20Additionally%2C%0Agene%20capture%20inefficiencies%20lead%20to%20significant%20dropout%2C%20corrupting%20acquired%0Adata.%20To%20address%20these%20challenges%2C%20the%20deep%20learning%20community%20has%20explored%20the%0Agene%20expression%20prediction%20task%20directly%20from%20histology%20images.%20Yet%2C%0Ainconsistencies%20in%20datasets%2C%20preprocessing%2C%20and%20training%20protocols%20hinder%20fair%0Acomparisons%20between%20models.%20To%20bridge%20this%20gap%2C%20we%20introduce%20SpaRED%2C%20a%0Asystematically%20curated%20database%20comprising%2026%20public%20datasets%2C%20providing%20a%0Astandardized%20resource%20for%20model%20evaluation.%20We%20further%20propose%20SpaCKLE%2C%20a%0Astate-of-the-art%20transformer-based%20gene%20expression%20completion%20model%20that%0Areduces%20mean%20squared%20error%20by%20over%2082.5%25%20compared%20to%20existing%20approaches.%0AFinally%2C%20we%20establish%20the%20SpaRED%20benchmark%2C%20evaluating%20eight%20state-of-the-art%0Aprediction%20models%20on%20both%20raw%20and%20SpaCKLE-completed%20data%2C%20demonstrating%20SpaCKLE%0Asubstantially%20improves%20the%20results%20across%20all%20the%20gene%20expression%20prediction%0Amodels.%20Altogether%2C%20our%20contributions%20constitute%20the%20most%20comprehensive%0Abenchmark%20of%20gene%20expression%20prediction%20from%20histology%20images%20to%20date%20and%20a%0Astepping%20stone%20for%20future%20research%20on%20Spatial%20Transcriptomics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02980v2&entry.124074799=Read"},
{"title": "Learning neural representations for X-ray ptychography reconstruction\n  with unknown probes", "author": "Tingyou Li and Zixin Xu and Zirui Gao and Hanfei Yan and Xiaojing Huang and Jizhou Li", "abstract": "  X-ray ptychography provides exceptional nanoscale resolution and is widely\napplied in materials science, biology, and nanotechnology. However, its full\npotential is constrained by the critical challenge of accurately reconstructing\nimages when the illuminating probe is unknown. Conventional iterative methods\nand deep learning approaches are often suboptimal, particularly under the\nlow-signal conditions inherent to low-dose and high-speed experiments. These\nlimitations compromise reconstruction fidelity and restrict the broader\nadoption of the technique. In this work, we introduce the Ptychographic\nImplicit Neural Representation (PtyINR), a self-supervised framework that\nsimultaneously addresses the object and probe recovery problem. By\nparameterizing both as continuous neural representations, PtyINR performs\nend-to-end reconstruction directly from raw diffraction patterns without\nrequiring any pre-characterization of the probe. Extensive evaluations\ndemonstrate that PtyINR achieves superior reconstruction quality on both\nsimulated and experimental data, with remarkable robustness under challenging\nlow-signal conditions. Furthermore, PtyINR offers a generalizable,\nphysics-informed framework for addressing probe-dependent inverse problems,\nmaking it applicable to a wide range of computational microscopy problems.\n", "link": "http://arxiv.org/abs/2509.04402v1", "date": "2025-09-04", "relevancy": 2.536, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5097}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5059}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20neural%20representations%20for%20X-ray%20ptychography%20reconstruction%0A%20%20with%20unknown%20probes&body=Title%3A%20Learning%20neural%20representations%20for%20X-ray%20ptychography%20reconstruction%0A%20%20with%20unknown%20probes%0AAuthor%3A%20Tingyou%20Li%20and%20Zixin%20Xu%20and%20Zirui%20Gao%20and%20Hanfei%20Yan%20and%20Xiaojing%20Huang%20and%20Jizhou%20Li%0AAbstract%3A%20%20%20X-ray%20ptychography%20provides%20exceptional%20nanoscale%20resolution%20and%20is%20widely%0Aapplied%20in%20materials%20science%2C%20biology%2C%20and%20nanotechnology.%20However%2C%20its%20full%0Apotential%20is%20constrained%20by%20the%20critical%20challenge%20of%20accurately%20reconstructing%0Aimages%20when%20the%20illuminating%20probe%20is%20unknown.%20Conventional%20iterative%20methods%0Aand%20deep%20learning%20approaches%20are%20often%20suboptimal%2C%20particularly%20under%20the%0Alow-signal%20conditions%20inherent%20to%20low-dose%20and%20high-speed%20experiments.%20These%0Alimitations%20compromise%20reconstruction%20fidelity%20and%20restrict%20the%20broader%0Aadoption%20of%20the%20technique.%20In%20this%20work%2C%20we%20introduce%20the%20Ptychographic%0AImplicit%20Neural%20Representation%20%28PtyINR%29%2C%20a%20self-supervised%20framework%20that%0Asimultaneously%20addresses%20the%20object%20and%20probe%20recovery%20problem.%20By%0Aparameterizing%20both%20as%20continuous%20neural%20representations%2C%20PtyINR%20performs%0Aend-to-end%20reconstruction%20directly%20from%20raw%20diffraction%20patterns%20without%0Arequiring%20any%20pre-characterization%20of%20the%20probe.%20Extensive%20evaluations%0Ademonstrate%20that%20PtyINR%20achieves%20superior%20reconstruction%20quality%20on%20both%0Asimulated%20and%20experimental%20data%2C%20with%20remarkable%20robustness%20under%20challenging%0Alow-signal%20conditions.%20Furthermore%2C%20PtyINR%20offers%20a%20generalizable%2C%0Aphysics-informed%20framework%20for%20addressing%20probe-dependent%20inverse%20problems%2C%0Amaking%20it%20applicable%20to%20a%20wide%20range%20of%20computational%20microscopy%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04402v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520neural%2520representations%2520for%2520X-ray%2520ptychography%2520reconstruction%250A%2520%2520with%2520unknown%2520probes%26entry.906535625%3DTingyou%2520Li%2520and%2520Zixin%2520Xu%2520and%2520Zirui%2520Gao%2520and%2520Hanfei%2520Yan%2520and%2520Xiaojing%2520Huang%2520and%2520Jizhou%2520Li%26entry.1292438233%3D%2520%2520X-ray%2520ptychography%2520provides%2520exceptional%2520nanoscale%2520resolution%2520and%2520is%2520widely%250Aapplied%2520in%2520materials%2520science%252C%2520biology%252C%2520and%2520nanotechnology.%2520However%252C%2520its%2520full%250Apotential%2520is%2520constrained%2520by%2520the%2520critical%2520challenge%2520of%2520accurately%2520reconstructing%250Aimages%2520when%2520the%2520illuminating%2520probe%2520is%2520unknown.%2520Conventional%2520iterative%2520methods%250Aand%2520deep%2520learning%2520approaches%2520are%2520often%2520suboptimal%252C%2520particularly%2520under%2520the%250Alow-signal%2520conditions%2520inherent%2520to%2520low-dose%2520and%2520high-speed%2520experiments.%2520These%250Alimitations%2520compromise%2520reconstruction%2520fidelity%2520and%2520restrict%2520the%2520broader%250Aadoption%2520of%2520the%2520technique.%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520Ptychographic%250AImplicit%2520Neural%2520Representation%2520%2528PtyINR%2529%252C%2520a%2520self-supervised%2520framework%2520that%250Asimultaneously%2520addresses%2520the%2520object%2520and%2520probe%2520recovery%2520problem.%2520By%250Aparameterizing%2520both%2520as%2520continuous%2520neural%2520representations%252C%2520PtyINR%2520performs%250Aend-to-end%2520reconstruction%2520directly%2520from%2520raw%2520diffraction%2520patterns%2520without%250Arequiring%2520any%2520pre-characterization%2520of%2520the%2520probe.%2520Extensive%2520evaluations%250Ademonstrate%2520that%2520PtyINR%2520achieves%2520superior%2520reconstruction%2520quality%2520on%2520both%250Asimulated%2520and%2520experimental%2520data%252C%2520with%2520remarkable%2520robustness%2520under%2520challenging%250Alow-signal%2520conditions.%2520Furthermore%252C%2520PtyINR%2520offers%2520a%2520generalizable%252C%250Aphysics-informed%2520framework%2520for%2520addressing%2520probe-dependent%2520inverse%2520problems%252C%250Amaking%2520it%2520applicable%2520to%2520a%2520wide%2520range%2520of%2520computational%2520microscopy%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04402v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20neural%20representations%20for%20X-ray%20ptychography%20reconstruction%0A%20%20with%20unknown%20probes&entry.906535625=Tingyou%20Li%20and%20Zixin%20Xu%20and%20Zirui%20Gao%20and%20Hanfei%20Yan%20and%20Xiaojing%20Huang%20and%20Jizhou%20Li&entry.1292438233=%20%20X-ray%20ptychography%20provides%20exceptional%20nanoscale%20resolution%20and%20is%20widely%0Aapplied%20in%20materials%20science%2C%20biology%2C%20and%20nanotechnology.%20However%2C%20its%20full%0Apotential%20is%20constrained%20by%20the%20critical%20challenge%20of%20accurately%20reconstructing%0Aimages%20when%20the%20illuminating%20probe%20is%20unknown.%20Conventional%20iterative%20methods%0Aand%20deep%20learning%20approaches%20are%20often%20suboptimal%2C%20particularly%20under%20the%0Alow-signal%20conditions%20inherent%20to%20low-dose%20and%20high-speed%20experiments.%20These%0Alimitations%20compromise%20reconstruction%20fidelity%20and%20restrict%20the%20broader%0Aadoption%20of%20the%20technique.%20In%20this%20work%2C%20we%20introduce%20the%20Ptychographic%0AImplicit%20Neural%20Representation%20%28PtyINR%29%2C%20a%20self-supervised%20framework%20that%0Asimultaneously%20addresses%20the%20object%20and%20probe%20recovery%20problem.%20By%0Aparameterizing%20both%20as%20continuous%20neural%20representations%2C%20PtyINR%20performs%0Aend-to-end%20reconstruction%20directly%20from%20raw%20diffraction%20patterns%20without%0Arequiring%20any%20pre-characterization%20of%20the%20probe.%20Extensive%20evaluations%0Ademonstrate%20that%20PtyINR%20achieves%20superior%20reconstruction%20quality%20on%20both%0Asimulated%20and%20experimental%20data%2C%20with%20remarkable%20robustness%20under%20challenging%0Alow-signal%20conditions.%20Furthermore%2C%20PtyINR%20offers%20a%20generalizable%2C%0Aphysics-informed%20framework%20for%20addressing%20probe-dependent%20inverse%20problems%2C%0Amaking%20it%20applicable%20to%20a%20wide%20range%20of%20computational%20microscopy%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04402v1&entry.124074799=Read"},
{"title": "OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent\n  Detection", "author": "Chen Hu and Shan Luo and Letizia Gionfrida", "abstract": "  Grasping assistance is essential for restoring autonomy in individuals with\nmotor impairments, particularly in unstructured environments where object\ncategories and user intentions are diverse and unpredictable. We present\nOVGrasp, a hierarchical control framework for soft exoskeleton-based grasp\nassistance that integrates RGB-D vision, open-vocabulary prompts, and voice\ncommands to enable robust multimodal interaction. To enhance generalization in\nopen environments, OVGrasp incorporates a vision-language foundation model with\nan open-vocabulary mechanism, allowing zero-shot detection of previously unseen\nobjects without retraining. A multimodal decision-maker further fuses spatial\nand linguistic cues to infer user intent, such as grasp or release, in\nmulti-object scenarios. We deploy the complete framework on a custom\negocentric-view wearable exoskeleton and conduct systematic evaluations on 15\nobjects across three grasp types. Experimental results with ten participants\ndemonstrate that OVGrasp achieves a grasping ability score (GAS) of 87.00%,\noutperforming state-of-the-art baselines and achieving improved kinematic\nalignment with natural hand motion.\n", "link": "http://arxiv.org/abs/2509.04324v1", "date": "2025-09-04", "relevancy": 2.5356, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6565}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6288}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OVGrasp%3A%20Open-Vocabulary%20Grasping%20Assistance%20via%20Multimodal%20Intent%0A%20%20Detection&body=Title%3A%20OVGrasp%3A%20Open-Vocabulary%20Grasping%20Assistance%20via%20Multimodal%20Intent%0A%20%20Detection%0AAuthor%3A%20Chen%20Hu%20and%20Shan%20Luo%20and%20Letizia%20Gionfrida%0AAbstract%3A%20%20%20Grasping%20assistance%20is%20essential%20for%20restoring%20autonomy%20in%20individuals%20with%0Amotor%20impairments%2C%20particularly%20in%20unstructured%20environments%20where%20object%0Acategories%20and%20user%20intentions%20are%20diverse%20and%20unpredictable.%20We%20present%0AOVGrasp%2C%20a%20hierarchical%20control%20framework%20for%20soft%20exoskeleton-based%20grasp%0Aassistance%20that%20integrates%20RGB-D%20vision%2C%20open-vocabulary%20prompts%2C%20and%20voice%0Acommands%20to%20enable%20robust%20multimodal%20interaction.%20To%20enhance%20generalization%20in%0Aopen%20environments%2C%20OVGrasp%20incorporates%20a%20vision-language%20foundation%20model%20with%0Aan%20open-vocabulary%20mechanism%2C%20allowing%20zero-shot%20detection%20of%20previously%20unseen%0Aobjects%20without%20retraining.%20A%20multimodal%20decision-maker%20further%20fuses%20spatial%0Aand%20linguistic%20cues%20to%20infer%20user%20intent%2C%20such%20as%20grasp%20or%20release%2C%20in%0Amulti-object%20scenarios.%20We%20deploy%20the%20complete%20framework%20on%20a%20custom%0Aegocentric-view%20wearable%20exoskeleton%20and%20conduct%20systematic%20evaluations%20on%2015%0Aobjects%20across%20three%20grasp%20types.%20Experimental%20results%20with%20ten%20participants%0Ademonstrate%20that%20OVGrasp%20achieves%20a%20grasping%20ability%20score%20%28GAS%29%20of%2087.00%25%2C%0Aoutperforming%20state-of-the-art%20baselines%20and%20achieving%20improved%20kinematic%0Aalignment%20with%20natural%20hand%20motion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04324v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOVGrasp%253A%2520Open-Vocabulary%2520Grasping%2520Assistance%2520via%2520Multimodal%2520Intent%250A%2520%2520Detection%26entry.906535625%3DChen%2520Hu%2520and%2520Shan%2520Luo%2520and%2520Letizia%2520Gionfrida%26entry.1292438233%3D%2520%2520Grasping%2520assistance%2520is%2520essential%2520for%2520restoring%2520autonomy%2520in%2520individuals%2520with%250Amotor%2520impairments%252C%2520particularly%2520in%2520unstructured%2520environments%2520where%2520object%250Acategories%2520and%2520user%2520intentions%2520are%2520diverse%2520and%2520unpredictable.%2520We%2520present%250AOVGrasp%252C%2520a%2520hierarchical%2520control%2520framework%2520for%2520soft%2520exoskeleton-based%2520grasp%250Aassistance%2520that%2520integrates%2520RGB-D%2520vision%252C%2520open-vocabulary%2520prompts%252C%2520and%2520voice%250Acommands%2520to%2520enable%2520robust%2520multimodal%2520interaction.%2520To%2520enhance%2520generalization%2520in%250Aopen%2520environments%252C%2520OVGrasp%2520incorporates%2520a%2520vision-language%2520foundation%2520model%2520with%250Aan%2520open-vocabulary%2520mechanism%252C%2520allowing%2520zero-shot%2520detection%2520of%2520previously%2520unseen%250Aobjects%2520without%2520retraining.%2520A%2520multimodal%2520decision-maker%2520further%2520fuses%2520spatial%250Aand%2520linguistic%2520cues%2520to%2520infer%2520user%2520intent%252C%2520such%2520as%2520grasp%2520or%2520release%252C%2520in%250Amulti-object%2520scenarios.%2520We%2520deploy%2520the%2520complete%2520framework%2520on%2520a%2520custom%250Aegocentric-view%2520wearable%2520exoskeleton%2520and%2520conduct%2520systematic%2520evaluations%2520on%252015%250Aobjects%2520across%2520three%2520grasp%2520types.%2520Experimental%2520results%2520with%2520ten%2520participants%250Ademonstrate%2520that%2520OVGrasp%2520achieves%2520a%2520grasping%2520ability%2520score%2520%2528GAS%2529%2520of%252087.00%2525%252C%250Aoutperforming%2520state-of-the-art%2520baselines%2520and%2520achieving%2520improved%2520kinematic%250Aalignment%2520with%2520natural%2520hand%2520motion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04324v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OVGrasp%3A%20Open-Vocabulary%20Grasping%20Assistance%20via%20Multimodal%20Intent%0A%20%20Detection&entry.906535625=Chen%20Hu%20and%20Shan%20Luo%20and%20Letizia%20Gionfrida&entry.1292438233=%20%20Grasping%20assistance%20is%20essential%20for%20restoring%20autonomy%20in%20individuals%20with%0Amotor%20impairments%2C%20particularly%20in%20unstructured%20environments%20where%20object%0Acategories%20and%20user%20intentions%20are%20diverse%20and%20unpredictable.%20We%20present%0AOVGrasp%2C%20a%20hierarchical%20control%20framework%20for%20soft%20exoskeleton-based%20grasp%0Aassistance%20that%20integrates%20RGB-D%20vision%2C%20open-vocabulary%20prompts%2C%20and%20voice%0Acommands%20to%20enable%20robust%20multimodal%20interaction.%20To%20enhance%20generalization%20in%0Aopen%20environments%2C%20OVGrasp%20incorporates%20a%20vision-language%20foundation%20model%20with%0Aan%20open-vocabulary%20mechanism%2C%20allowing%20zero-shot%20detection%20of%20previously%20unseen%0Aobjects%20without%20retraining.%20A%20multimodal%20decision-maker%20further%20fuses%20spatial%0Aand%20linguistic%20cues%20to%20infer%20user%20intent%2C%20such%20as%20grasp%20or%20release%2C%20in%0Amulti-object%20scenarios.%20We%20deploy%20the%20complete%20framework%20on%20a%20custom%0Aegocentric-view%20wearable%20exoskeleton%20and%20conduct%20systematic%20evaluations%20on%2015%0Aobjects%20across%20three%20grasp%20types.%20Experimental%20results%20with%20ten%20participants%0Ademonstrate%20that%20OVGrasp%20achieves%20a%20grasping%20ability%20score%20%28GAS%29%20of%2087.00%25%2C%0Aoutperforming%20state-of-the-art%20baselines%20and%20achieving%20improved%20kinematic%0Aalignment%20with%20natural%20hand%20motion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04324v1&entry.124074799=Read"},
{"title": "IPA: An Information-Preserving Input Projection Framework for Efficient\n  Foundation Model Adaptation", "author": "Yuan Yin and Shashanka Venkataramanan and Tuan-Hung Vu and Andrei Bursuc and Matthieu Cord", "abstract": "  Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, reduce\nadaptation cost by injecting low-rank updates into pretrained weights. However,\nLoRA's down-projection is randomly initialized and data-agnostic, discarding\npotentially useful information. Prior analyses show that this projection\nchanges little during training, while the up-projection carries most of the\nadaptation, making the random input compression a performance bottleneck. We\npropose IPA, a feature-aware projection framework that explicitly preserves\ninformation in the reduced hidden space. In the linear case, we instantiate IPA\nwith algorithms approximating top principal components, enabling efficient\nprojector pretraining with negligible inference overhead. Across language and\nvision benchmarks, IPA consistently improves over LoRA and DoRA, achieving on\naverage 1.5 points higher accuracy on commonsense reasoning and 2.3 points on\nVTAB-1k, while matching full LoRA performance with roughly half the trainable\nparameters when the projection is frozen.\n", "link": "http://arxiv.org/abs/2509.04398v1", "date": "2025-09-04", "relevancy": 2.5296, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5092}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5092}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IPA%3A%20An%20Information-Preserving%20Input%20Projection%20Framework%20for%20Efficient%0A%20%20Foundation%20Model%20Adaptation&body=Title%3A%20IPA%3A%20An%20Information-Preserving%20Input%20Projection%20Framework%20for%20Efficient%0A%20%20Foundation%20Model%20Adaptation%0AAuthor%3A%20Yuan%20Yin%20and%20Shashanka%20Venkataramanan%20and%20Tuan-Hung%20Vu%20and%20Andrei%20Bursuc%20and%20Matthieu%20Cord%0AAbstract%3A%20%20%20Parameter-efficient%20fine-tuning%20%28PEFT%29%20methods%2C%20such%20as%20LoRA%2C%20reduce%0Aadaptation%20cost%20by%20injecting%20low-rank%20updates%20into%20pretrained%20weights.%20However%2C%0ALoRA%27s%20down-projection%20is%20randomly%20initialized%20and%20data-agnostic%2C%20discarding%0Apotentially%20useful%20information.%20Prior%20analyses%20show%20that%20this%20projection%0Achanges%20little%20during%20training%2C%20while%20the%20up-projection%20carries%20most%20of%20the%0Aadaptation%2C%20making%20the%20random%20input%20compression%20a%20performance%20bottleneck.%20We%0Apropose%20IPA%2C%20a%20feature-aware%20projection%20framework%20that%20explicitly%20preserves%0Ainformation%20in%20the%20reduced%20hidden%20space.%20In%20the%20linear%20case%2C%20we%20instantiate%20IPA%0Awith%20algorithms%20approximating%20top%20principal%20components%2C%20enabling%20efficient%0Aprojector%20pretraining%20with%20negligible%20inference%20overhead.%20Across%20language%20and%0Avision%20benchmarks%2C%20IPA%20consistently%20improves%20over%20LoRA%20and%20DoRA%2C%20achieving%20on%0Aaverage%201.5%20points%20higher%20accuracy%20on%20commonsense%20reasoning%20and%202.3%20points%20on%0AVTAB-1k%2C%20while%20matching%20full%20LoRA%20performance%20with%20roughly%20half%20the%20trainable%0Aparameters%20when%20the%20projection%20is%20frozen.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04398v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIPA%253A%2520An%2520Information-Preserving%2520Input%2520Projection%2520Framework%2520for%2520Efficient%250A%2520%2520Foundation%2520Model%2520Adaptation%26entry.906535625%3DYuan%2520Yin%2520and%2520Shashanka%2520Venkataramanan%2520and%2520Tuan-Hung%2520Vu%2520and%2520Andrei%2520Bursuc%2520and%2520Matthieu%2520Cord%26entry.1292438233%3D%2520%2520Parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520methods%252C%2520such%2520as%2520LoRA%252C%2520reduce%250Aadaptation%2520cost%2520by%2520injecting%2520low-rank%2520updates%2520into%2520pretrained%2520weights.%2520However%252C%250ALoRA%2527s%2520down-projection%2520is%2520randomly%2520initialized%2520and%2520data-agnostic%252C%2520discarding%250Apotentially%2520useful%2520information.%2520Prior%2520analyses%2520show%2520that%2520this%2520projection%250Achanges%2520little%2520during%2520training%252C%2520while%2520the%2520up-projection%2520carries%2520most%2520of%2520the%250Aadaptation%252C%2520making%2520the%2520random%2520input%2520compression%2520a%2520performance%2520bottleneck.%2520We%250Apropose%2520IPA%252C%2520a%2520feature-aware%2520projection%2520framework%2520that%2520explicitly%2520preserves%250Ainformation%2520in%2520the%2520reduced%2520hidden%2520space.%2520In%2520the%2520linear%2520case%252C%2520we%2520instantiate%2520IPA%250Awith%2520algorithms%2520approximating%2520top%2520principal%2520components%252C%2520enabling%2520efficient%250Aprojector%2520pretraining%2520with%2520negligible%2520inference%2520overhead.%2520Across%2520language%2520and%250Avision%2520benchmarks%252C%2520IPA%2520consistently%2520improves%2520over%2520LoRA%2520and%2520DoRA%252C%2520achieving%2520on%250Aaverage%25201.5%2520points%2520higher%2520accuracy%2520on%2520commonsense%2520reasoning%2520and%25202.3%2520points%2520on%250AVTAB-1k%252C%2520while%2520matching%2520full%2520LoRA%2520performance%2520with%2520roughly%2520half%2520the%2520trainable%250Aparameters%2520when%2520the%2520projection%2520is%2520frozen.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04398v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IPA%3A%20An%20Information-Preserving%20Input%20Projection%20Framework%20for%20Efficient%0A%20%20Foundation%20Model%20Adaptation&entry.906535625=Yuan%20Yin%20and%20Shashanka%20Venkataramanan%20and%20Tuan-Hung%20Vu%20and%20Andrei%20Bursuc%20and%20Matthieu%20Cord&entry.1292438233=%20%20Parameter-efficient%20fine-tuning%20%28PEFT%29%20methods%2C%20such%20as%20LoRA%2C%20reduce%0Aadaptation%20cost%20by%20injecting%20low-rank%20updates%20into%20pretrained%20weights.%20However%2C%0ALoRA%27s%20down-projection%20is%20randomly%20initialized%20and%20data-agnostic%2C%20discarding%0Apotentially%20useful%20information.%20Prior%20analyses%20show%20that%20this%20projection%0Achanges%20little%20during%20training%2C%20while%20the%20up-projection%20carries%20most%20of%20the%0Aadaptation%2C%20making%20the%20random%20input%20compression%20a%20performance%20bottleneck.%20We%0Apropose%20IPA%2C%20a%20feature-aware%20projection%20framework%20that%20explicitly%20preserves%0Ainformation%20in%20the%20reduced%20hidden%20space.%20In%20the%20linear%20case%2C%20we%20instantiate%20IPA%0Awith%20algorithms%20approximating%20top%20principal%20components%2C%20enabling%20efficient%0Aprojector%20pretraining%20with%20negligible%20inference%20overhead.%20Across%20language%20and%0Avision%20benchmarks%2C%20IPA%20consistently%20improves%20over%20LoRA%20and%20DoRA%2C%20achieving%20on%0Aaverage%201.5%20points%20higher%20accuracy%20on%20commonsense%20reasoning%20and%202.3%20points%20on%0AVTAB-1k%2C%20while%20matching%20full%20LoRA%20performance%20with%20roughly%20half%20the%20trainable%0Aparameters%20when%20the%20projection%20is%20frozen.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04398v1&entry.124074799=Read"},
{"title": "SPARE: Symmetrized Point-to-Plane Distance for Robust Non-Rigid\n  Registration", "author": "Yuxin Yao and Bailin Deng and Junhui Hou and Juyong Zhang", "abstract": "  Existing optimization-based methods for non-rigid registration typically\nminimize an alignment error metric based on the point-to-point or\npoint-to-plane distance between corresponding point pairs on the source surface\nand target surface. However, these metrics can result in slow convergence or a\nloss of detail. In this paper, we propose SPARE, a novel formulation that\nutilizes a symmetrized point-to-plane distance for robust non-rigid\nregistration. The symmetrized point-to-plane distance relies on both the\npositions and normals of the corresponding points, resulting in a more accurate\napproximation of the underlying geometry and can achieve higher accuracy than\nexisting methods. To solve this optimization problem efficiently, we introduce\nan as-rigid-as-possible regulation term to estimate the deformed normals and\npropose an alternating minimization solver using a majorization-minimization\nstrategy. Moreover, for effective initialization of the solver, we incorporate\na deformation graph-based coarse alignment that improves registration quality\nand efficiency. Extensive experiments show that the proposed method greatly\nimproves the accuracy of non-rigid registration problems and maintains\nrelatively high solution efficiency. The code is publicly available at\nhttps://github.com/yaoyx689/spare.\n", "link": "http://arxiv.org/abs/2405.20188v2", "date": "2025-09-04", "relevancy": 2.5192, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.512}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5093}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPARE%3A%20Symmetrized%20Point-to-Plane%20Distance%20for%20Robust%20Non-Rigid%0A%20%20Registration&body=Title%3A%20SPARE%3A%20Symmetrized%20Point-to-Plane%20Distance%20for%20Robust%20Non-Rigid%0A%20%20Registration%0AAuthor%3A%20Yuxin%20Yao%20and%20Bailin%20Deng%20and%20Junhui%20Hou%20and%20Juyong%20Zhang%0AAbstract%3A%20%20%20Existing%20optimization-based%20methods%20for%20non-rigid%20registration%20typically%0Aminimize%20an%20alignment%20error%20metric%20based%20on%20the%20point-to-point%20or%0Apoint-to-plane%20distance%20between%20corresponding%20point%20pairs%20on%20the%20source%20surface%0Aand%20target%20surface.%20However%2C%20these%20metrics%20can%20result%20in%20slow%20convergence%20or%20a%0Aloss%20of%20detail.%20In%20this%20paper%2C%20we%20propose%20SPARE%2C%20a%20novel%20formulation%20that%0Autilizes%20a%20symmetrized%20point-to-plane%20distance%20for%20robust%20non-rigid%0Aregistration.%20The%20symmetrized%20point-to-plane%20distance%20relies%20on%20both%20the%0Apositions%20and%20normals%20of%20the%20corresponding%20points%2C%20resulting%20in%20a%20more%20accurate%0Aapproximation%20of%20the%20underlying%20geometry%20and%20can%20achieve%20higher%20accuracy%20than%0Aexisting%20methods.%20To%20solve%20this%20optimization%20problem%20efficiently%2C%20we%20introduce%0Aan%20as-rigid-as-possible%20regulation%20term%20to%20estimate%20the%20deformed%20normals%20and%0Apropose%20an%20alternating%20minimization%20solver%20using%20a%20majorization-minimization%0Astrategy.%20Moreover%2C%20for%20effective%20initialization%20of%20the%20solver%2C%20we%20incorporate%0Aa%20deformation%20graph-based%20coarse%20alignment%20that%20improves%20registration%20quality%0Aand%20efficiency.%20Extensive%20experiments%20show%20that%20the%20proposed%20method%20greatly%0Aimproves%20the%20accuracy%20of%20non-rigid%20registration%20problems%20and%20maintains%0Arelatively%20high%20solution%20efficiency.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/yaoyx689/spare.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20188v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPARE%253A%2520Symmetrized%2520Point-to-Plane%2520Distance%2520for%2520Robust%2520Non-Rigid%250A%2520%2520Registration%26entry.906535625%3DYuxin%2520Yao%2520and%2520Bailin%2520Deng%2520and%2520Junhui%2520Hou%2520and%2520Juyong%2520Zhang%26entry.1292438233%3D%2520%2520Existing%2520optimization-based%2520methods%2520for%2520non-rigid%2520registration%2520typically%250Aminimize%2520an%2520alignment%2520error%2520metric%2520based%2520on%2520the%2520point-to-point%2520or%250Apoint-to-plane%2520distance%2520between%2520corresponding%2520point%2520pairs%2520on%2520the%2520source%2520surface%250Aand%2520target%2520surface.%2520However%252C%2520these%2520metrics%2520can%2520result%2520in%2520slow%2520convergence%2520or%2520a%250Aloss%2520of%2520detail.%2520In%2520this%2520paper%252C%2520we%2520propose%2520SPARE%252C%2520a%2520novel%2520formulation%2520that%250Autilizes%2520a%2520symmetrized%2520point-to-plane%2520distance%2520for%2520robust%2520non-rigid%250Aregistration.%2520The%2520symmetrized%2520point-to-plane%2520distance%2520relies%2520on%2520both%2520the%250Apositions%2520and%2520normals%2520of%2520the%2520corresponding%2520points%252C%2520resulting%2520in%2520a%2520more%2520accurate%250Aapproximation%2520of%2520the%2520underlying%2520geometry%2520and%2520can%2520achieve%2520higher%2520accuracy%2520than%250Aexisting%2520methods.%2520To%2520solve%2520this%2520optimization%2520problem%2520efficiently%252C%2520we%2520introduce%250Aan%2520as-rigid-as-possible%2520regulation%2520term%2520to%2520estimate%2520the%2520deformed%2520normals%2520and%250Apropose%2520an%2520alternating%2520minimization%2520solver%2520using%2520a%2520majorization-minimization%250Astrategy.%2520Moreover%252C%2520for%2520effective%2520initialization%2520of%2520the%2520solver%252C%2520we%2520incorporate%250Aa%2520deformation%2520graph-based%2520coarse%2520alignment%2520that%2520improves%2520registration%2520quality%250Aand%2520efficiency.%2520Extensive%2520experiments%2520show%2520that%2520the%2520proposed%2520method%2520greatly%250Aimproves%2520the%2520accuracy%2520of%2520non-rigid%2520registration%2520problems%2520and%2520maintains%250Arelatively%2520high%2520solution%2520efficiency.%2520The%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/yaoyx689/spare.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20188v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPARE%3A%20Symmetrized%20Point-to-Plane%20Distance%20for%20Robust%20Non-Rigid%0A%20%20Registration&entry.906535625=Yuxin%20Yao%20and%20Bailin%20Deng%20and%20Junhui%20Hou%20and%20Juyong%20Zhang&entry.1292438233=%20%20Existing%20optimization-based%20methods%20for%20non-rigid%20registration%20typically%0Aminimize%20an%20alignment%20error%20metric%20based%20on%20the%20point-to-point%20or%0Apoint-to-plane%20distance%20between%20corresponding%20point%20pairs%20on%20the%20source%20surface%0Aand%20target%20surface.%20However%2C%20these%20metrics%20can%20result%20in%20slow%20convergence%20or%20a%0Aloss%20of%20detail.%20In%20this%20paper%2C%20we%20propose%20SPARE%2C%20a%20novel%20formulation%20that%0Autilizes%20a%20symmetrized%20point-to-plane%20distance%20for%20robust%20non-rigid%0Aregistration.%20The%20symmetrized%20point-to-plane%20distance%20relies%20on%20both%20the%0Apositions%20and%20normals%20of%20the%20corresponding%20points%2C%20resulting%20in%20a%20more%20accurate%0Aapproximation%20of%20the%20underlying%20geometry%20and%20can%20achieve%20higher%20accuracy%20than%0Aexisting%20methods.%20To%20solve%20this%20optimization%20problem%20efficiently%2C%20we%20introduce%0Aan%20as-rigid-as-possible%20regulation%20term%20to%20estimate%20the%20deformed%20normals%20and%0Apropose%20an%20alternating%20minimization%20solver%20using%20a%20majorization-minimization%0Astrategy.%20Moreover%2C%20for%20effective%20initialization%20of%20the%20solver%2C%20we%20incorporate%0Aa%20deformation%20graph-based%20coarse%20alignment%20that%20improves%20registration%20quality%0Aand%20efficiency.%20Extensive%20experiments%20show%20that%20the%20proposed%20method%20greatly%0Aimproves%20the%20accuracy%20of%20non-rigid%20registration%20problems%20and%20maintains%0Arelatively%20high%20solution%20efficiency.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/yaoyx689/spare.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20188v2&entry.124074799=Read"},
{"title": "Rapid Word Learning Through Meta In-Context Learning", "author": "Wentao Wang and Guangyuan Jiang and Tal Linzen and Brenden M. Lake", "abstract": "  Humans can quickly learn a new word from a few illustrative examples, and\nthen systematically and flexibly use it in novel contexts. Yet the abilities of\ncurrent language models for few-shot word learning, and methods for improving\nthese abilities, are underexplored. In this study, we introduce a novel method,\nMeta-training for IN-context learNing Of Words (Minnow). This method trains\nlanguage models to generate new examples of a word's usage given a few\nin-context examples, using a special placeholder token to represent the new\nword. This training is repeated on many new words to develop a general\nword-learning ability. We find that training models from scratch with Minnow on\nhuman-scale child-directed language enables strong few-shot word learning,\ncomparable to a large language model (LLM) pre-trained on orders of magnitude\nmore data. Furthermore, through discriminative and generative evaluations, we\ndemonstrate that finetuning pre-trained LLMs with Minnow improves their ability\nto discriminate between new words, identify syntactic categories of new words,\nand generate reasonable new usages and definitions for new words, based on one\nor a few in-context examples. These findings highlight the data efficiency of\nMinnow and its potential to improve language model performance in word learning\ntasks.\n", "link": "http://arxiv.org/abs/2502.14791v4", "date": "2025-09-04", "relevancy": 2.5005, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5063}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.497}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rapid%20Word%20Learning%20Through%20Meta%20In-Context%20Learning&body=Title%3A%20Rapid%20Word%20Learning%20Through%20Meta%20In-Context%20Learning%0AAuthor%3A%20Wentao%20Wang%20and%20Guangyuan%20Jiang%20and%20Tal%20Linzen%20and%20Brenden%20M.%20Lake%0AAbstract%3A%20%20%20Humans%20can%20quickly%20learn%20a%20new%20word%20from%20a%20few%20illustrative%20examples%2C%20and%0Athen%20systematically%20and%20flexibly%20use%20it%20in%20novel%20contexts.%20Yet%20the%20abilities%20of%0Acurrent%20language%20models%20for%20few-shot%20word%20learning%2C%20and%20methods%20for%20improving%0Athese%20abilities%2C%20are%20underexplored.%20In%20this%20study%2C%20we%20introduce%20a%20novel%20method%2C%0AMeta-training%20for%20IN-context%20learNing%20Of%20Words%20%28Minnow%29.%20This%20method%20trains%0Alanguage%20models%20to%20generate%20new%20examples%20of%20a%20word%27s%20usage%20given%20a%20few%0Ain-context%20examples%2C%20using%20a%20special%20placeholder%20token%20to%20represent%20the%20new%0Aword.%20This%20training%20is%20repeated%20on%20many%20new%20words%20to%20develop%20a%20general%0Aword-learning%20ability.%20We%20find%20that%20training%20models%20from%20scratch%20with%20Minnow%20on%0Ahuman-scale%20child-directed%20language%20enables%20strong%20few-shot%20word%20learning%2C%0Acomparable%20to%20a%20large%20language%20model%20%28LLM%29%20pre-trained%20on%20orders%20of%20magnitude%0Amore%20data.%20Furthermore%2C%20through%20discriminative%20and%20generative%20evaluations%2C%20we%0Ademonstrate%20that%20finetuning%20pre-trained%20LLMs%20with%20Minnow%20improves%20their%20ability%0Ato%20discriminate%20between%20new%20words%2C%20identify%20syntactic%20categories%20of%20new%20words%2C%0Aand%20generate%20reasonable%20new%20usages%20and%20definitions%20for%20new%20words%2C%20based%20on%20one%0Aor%20a%20few%20in-context%20examples.%20These%20findings%20highlight%20the%20data%20efficiency%20of%0AMinnow%20and%20its%20potential%20to%20improve%20language%20model%20performance%20in%20word%20learning%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14791v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRapid%2520Word%2520Learning%2520Through%2520Meta%2520In-Context%2520Learning%26entry.906535625%3DWentao%2520Wang%2520and%2520Guangyuan%2520Jiang%2520and%2520Tal%2520Linzen%2520and%2520Brenden%2520M.%2520Lake%26entry.1292438233%3D%2520%2520Humans%2520can%2520quickly%2520learn%2520a%2520new%2520word%2520from%2520a%2520few%2520illustrative%2520examples%252C%2520and%250Athen%2520systematically%2520and%2520flexibly%2520use%2520it%2520in%2520novel%2520contexts.%2520Yet%2520the%2520abilities%2520of%250Acurrent%2520language%2520models%2520for%2520few-shot%2520word%2520learning%252C%2520and%2520methods%2520for%2520improving%250Athese%2520abilities%252C%2520are%2520underexplored.%2520In%2520this%2520study%252C%2520we%2520introduce%2520a%2520novel%2520method%252C%250AMeta-training%2520for%2520IN-context%2520learNing%2520Of%2520Words%2520%2528Minnow%2529.%2520This%2520method%2520trains%250Alanguage%2520models%2520to%2520generate%2520new%2520examples%2520of%2520a%2520word%2527s%2520usage%2520given%2520a%2520few%250Ain-context%2520examples%252C%2520using%2520a%2520special%2520placeholder%2520token%2520to%2520represent%2520the%2520new%250Aword.%2520This%2520training%2520is%2520repeated%2520on%2520many%2520new%2520words%2520to%2520develop%2520a%2520general%250Aword-learning%2520ability.%2520We%2520find%2520that%2520training%2520models%2520from%2520scratch%2520with%2520Minnow%2520on%250Ahuman-scale%2520child-directed%2520language%2520enables%2520strong%2520few-shot%2520word%2520learning%252C%250Acomparable%2520to%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520pre-trained%2520on%2520orders%2520of%2520magnitude%250Amore%2520data.%2520Furthermore%252C%2520through%2520discriminative%2520and%2520generative%2520evaluations%252C%2520we%250Ademonstrate%2520that%2520finetuning%2520pre-trained%2520LLMs%2520with%2520Minnow%2520improves%2520their%2520ability%250Ato%2520discriminate%2520between%2520new%2520words%252C%2520identify%2520syntactic%2520categories%2520of%2520new%2520words%252C%250Aand%2520generate%2520reasonable%2520new%2520usages%2520and%2520definitions%2520for%2520new%2520words%252C%2520based%2520on%2520one%250Aor%2520a%2520few%2520in-context%2520examples.%2520These%2520findings%2520highlight%2520the%2520data%2520efficiency%2520of%250AMinnow%2520and%2520its%2520potential%2520to%2520improve%2520language%2520model%2520performance%2520in%2520word%2520learning%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14791v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rapid%20Word%20Learning%20Through%20Meta%20In-Context%20Learning&entry.906535625=Wentao%20Wang%20and%20Guangyuan%20Jiang%20and%20Tal%20Linzen%20and%20Brenden%20M.%20Lake&entry.1292438233=%20%20Humans%20can%20quickly%20learn%20a%20new%20word%20from%20a%20few%20illustrative%20examples%2C%20and%0Athen%20systematically%20and%20flexibly%20use%20it%20in%20novel%20contexts.%20Yet%20the%20abilities%20of%0Acurrent%20language%20models%20for%20few-shot%20word%20learning%2C%20and%20methods%20for%20improving%0Athese%20abilities%2C%20are%20underexplored.%20In%20this%20study%2C%20we%20introduce%20a%20novel%20method%2C%0AMeta-training%20for%20IN-context%20learNing%20Of%20Words%20%28Minnow%29.%20This%20method%20trains%0Alanguage%20models%20to%20generate%20new%20examples%20of%20a%20word%27s%20usage%20given%20a%20few%0Ain-context%20examples%2C%20using%20a%20special%20placeholder%20token%20to%20represent%20the%20new%0Aword.%20This%20training%20is%20repeated%20on%20many%20new%20words%20to%20develop%20a%20general%0Aword-learning%20ability.%20We%20find%20that%20training%20models%20from%20scratch%20with%20Minnow%20on%0Ahuman-scale%20child-directed%20language%20enables%20strong%20few-shot%20word%20learning%2C%0Acomparable%20to%20a%20large%20language%20model%20%28LLM%29%20pre-trained%20on%20orders%20of%20magnitude%0Amore%20data.%20Furthermore%2C%20through%20discriminative%20and%20generative%20evaluations%2C%20we%0Ademonstrate%20that%20finetuning%20pre-trained%20LLMs%20with%20Minnow%20improves%20their%20ability%0Ato%20discriminate%20between%20new%20words%2C%20identify%20syntactic%20categories%20of%20new%20words%2C%0Aand%20generate%20reasonable%20new%20usages%20and%20definitions%20for%20new%20words%2C%20based%20on%20one%0Aor%20a%20few%20in-context%20examples.%20These%20findings%20highlight%20the%20data%20efficiency%20of%0AMinnow%20and%20its%20potential%20to%20improve%20language%20model%20performance%20in%20word%20learning%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14791v4&entry.124074799=Read"},
{"title": "EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning", "author": "Huanyu Liu and Jia Li and Chang Yu and Taozhi Chen and Yihong Dong and Lecheng Wang and XiaoLong Hu and Ge Li", "abstract": "  Reinforcement learning with verifiable reward (RLVR) has become a promising\nparadigm for post-training large language models (LLMs) to improve their\nreasoning capability. However, when the rollout accuracy is low on hard\nproblems, the reward becomes sparse, limiting learning efficiency and causing\nexploration bottlenecks. Existing approaches either rely on stronger LLMs for\ndistillation or filter out difficult problems, which limits scalability or\nrestricts reasoning improvement through exploration.\n  We propose EvoCoT, a self-evolving curriculum learning framework based on\ntwo-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the\nexploration space by self-generating and verifying CoT trajectories, then\ngradually shortens them to expand the space in a controlled way. This enables\nLLMs to stably learn from initially unsolved hard problems under sparse\nrewards. We apply EvoCoT to multiple LLM families, including Qwen, DeepSeek,\nand Llama. Experiments show that EvoCoT enables LLMs to solve previously\nunsolved problems, improves reasoning capability without external CoT\nsupervision, and is compatible with various RL fine-tuning methods. We release\nthe source code to support future research.\n", "link": "http://arxiv.org/abs/2508.07809v2", "date": "2025-09-04", "relevancy": 2.4854, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5044}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4934}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EvoCoT%3A%20Overcoming%20the%20Exploration%20Bottleneck%20in%20Reinforcement%20Learning&body=Title%3A%20EvoCoT%3A%20Overcoming%20the%20Exploration%20Bottleneck%20in%20Reinforcement%20Learning%0AAuthor%3A%20Huanyu%20Liu%20and%20Jia%20Li%20and%20Chang%20Yu%20and%20Taozhi%20Chen%20and%20Yihong%20Dong%20and%20Lecheng%20Wang%20and%20XiaoLong%20Hu%20and%20Ge%20Li%0AAbstract%3A%20%20%20Reinforcement%20learning%20with%20verifiable%20reward%20%28RLVR%29%20has%20become%20a%20promising%0Aparadigm%20for%20post-training%20large%20language%20models%20%28LLMs%29%20to%20improve%20their%0Areasoning%20capability.%20However%2C%20when%20the%20rollout%20accuracy%20is%20low%20on%20hard%0Aproblems%2C%20the%20reward%20becomes%20sparse%2C%20limiting%20learning%20efficiency%20and%20causing%0Aexploration%20bottlenecks.%20Existing%20approaches%20either%20rely%20on%20stronger%20LLMs%20for%0Adistillation%20or%20filter%20out%20difficult%20problems%2C%20which%20limits%20scalability%20or%0Arestricts%20reasoning%20improvement%20through%20exploration.%0A%20%20We%20propose%20EvoCoT%2C%20a%20self-evolving%20curriculum%20learning%20framework%20based%20on%0Atwo-stage%20chain-of-thought%20%28CoT%29%20reasoning%20optimization.%20EvoCoT%20constrains%20the%0Aexploration%20space%20by%20self-generating%20and%20verifying%20CoT%20trajectories%2C%20then%0Agradually%20shortens%20them%20to%20expand%20the%20space%20in%20a%20controlled%20way.%20This%20enables%0ALLMs%20to%20stably%20learn%20from%20initially%20unsolved%20hard%20problems%20under%20sparse%0Arewards.%20We%20apply%20EvoCoT%20to%20multiple%20LLM%20families%2C%20including%20Qwen%2C%20DeepSeek%2C%0Aand%20Llama.%20Experiments%20show%20that%20EvoCoT%20enables%20LLMs%20to%20solve%20previously%0Aunsolved%20problems%2C%20improves%20reasoning%20capability%20without%20external%20CoT%0Asupervision%2C%20and%20is%20compatible%20with%20various%20RL%20fine-tuning%20methods.%20We%20release%0Athe%20source%20code%20to%20support%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07809v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvoCoT%253A%2520Overcoming%2520the%2520Exploration%2520Bottleneck%2520in%2520Reinforcement%2520Learning%26entry.906535625%3DHuanyu%2520Liu%2520and%2520Jia%2520Li%2520and%2520Chang%2520Yu%2520and%2520Taozhi%2520Chen%2520and%2520Yihong%2520Dong%2520and%2520Lecheng%2520Wang%2520and%2520XiaoLong%2520Hu%2520and%2520Ge%2520Li%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520with%2520verifiable%2520reward%2520%2528RLVR%2529%2520has%2520become%2520a%2520promising%250Aparadigm%2520for%2520post-training%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520improve%2520their%250Areasoning%2520capability.%2520However%252C%2520when%2520the%2520rollout%2520accuracy%2520is%2520low%2520on%2520hard%250Aproblems%252C%2520the%2520reward%2520becomes%2520sparse%252C%2520limiting%2520learning%2520efficiency%2520and%2520causing%250Aexploration%2520bottlenecks.%2520Existing%2520approaches%2520either%2520rely%2520on%2520stronger%2520LLMs%2520for%250Adistillation%2520or%2520filter%2520out%2520difficult%2520problems%252C%2520which%2520limits%2520scalability%2520or%250Arestricts%2520reasoning%2520improvement%2520through%2520exploration.%250A%2520%2520We%2520propose%2520EvoCoT%252C%2520a%2520self-evolving%2520curriculum%2520learning%2520framework%2520based%2520on%250Atwo-stage%2520chain-of-thought%2520%2528CoT%2529%2520reasoning%2520optimization.%2520EvoCoT%2520constrains%2520the%250Aexploration%2520space%2520by%2520self-generating%2520and%2520verifying%2520CoT%2520trajectories%252C%2520then%250Agradually%2520shortens%2520them%2520to%2520expand%2520the%2520space%2520in%2520a%2520controlled%2520way.%2520This%2520enables%250ALLMs%2520to%2520stably%2520learn%2520from%2520initially%2520unsolved%2520hard%2520problems%2520under%2520sparse%250Arewards.%2520We%2520apply%2520EvoCoT%2520to%2520multiple%2520LLM%2520families%252C%2520including%2520Qwen%252C%2520DeepSeek%252C%250Aand%2520Llama.%2520Experiments%2520show%2520that%2520EvoCoT%2520enables%2520LLMs%2520to%2520solve%2520previously%250Aunsolved%2520problems%252C%2520improves%2520reasoning%2520capability%2520without%2520external%2520CoT%250Asupervision%252C%2520and%2520is%2520compatible%2520with%2520various%2520RL%2520fine-tuning%2520methods.%2520We%2520release%250Athe%2520source%2520code%2520to%2520support%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07809v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EvoCoT%3A%20Overcoming%20the%20Exploration%20Bottleneck%20in%20Reinforcement%20Learning&entry.906535625=Huanyu%20Liu%20and%20Jia%20Li%20and%20Chang%20Yu%20and%20Taozhi%20Chen%20and%20Yihong%20Dong%20and%20Lecheng%20Wang%20and%20XiaoLong%20Hu%20and%20Ge%20Li&entry.1292438233=%20%20Reinforcement%20learning%20with%20verifiable%20reward%20%28RLVR%29%20has%20become%20a%20promising%0Aparadigm%20for%20post-training%20large%20language%20models%20%28LLMs%29%20to%20improve%20their%0Areasoning%20capability.%20However%2C%20when%20the%20rollout%20accuracy%20is%20low%20on%20hard%0Aproblems%2C%20the%20reward%20becomes%20sparse%2C%20limiting%20learning%20efficiency%20and%20causing%0Aexploration%20bottlenecks.%20Existing%20approaches%20either%20rely%20on%20stronger%20LLMs%20for%0Adistillation%20or%20filter%20out%20difficult%20problems%2C%20which%20limits%20scalability%20or%0Arestricts%20reasoning%20improvement%20through%20exploration.%0A%20%20We%20propose%20EvoCoT%2C%20a%20self-evolving%20curriculum%20learning%20framework%20based%20on%0Atwo-stage%20chain-of-thought%20%28CoT%29%20reasoning%20optimization.%20EvoCoT%20constrains%20the%0Aexploration%20space%20by%20self-generating%20and%20verifying%20CoT%20trajectories%2C%20then%0Agradually%20shortens%20them%20to%20expand%20the%20space%20in%20a%20controlled%20way.%20This%20enables%0ALLMs%20to%20stably%20learn%20from%20initially%20unsolved%20hard%20problems%20under%20sparse%0Arewards.%20We%20apply%20EvoCoT%20to%20multiple%20LLM%20families%2C%20including%20Qwen%2C%20DeepSeek%2C%0Aand%20Llama.%20Experiments%20show%20that%20EvoCoT%20enables%20LLMs%20to%20solve%20previously%0Aunsolved%20problems%2C%20improves%20reasoning%20capability%20without%20external%20CoT%0Asupervision%2C%20and%20is%20compatible%20with%20various%20RL%20fine-tuning%20methods.%20We%20release%0Athe%20source%20code%20to%20support%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07809v2&entry.124074799=Read"},
{"title": "Virtual Fitting Room: Generating Arbitrarily Long Videos of Virtual\n  Try-On from a Single Image -- Technical Preview", "author": "Jun-Kun Chen and Aayush Bansal and Minh Phuoc Vo and Yu-Xiong Wang", "abstract": "  We introduce the Virtual Fitting Room (VFR), a novel video generative model\nthat produces arbitrarily long virtual try-on videos. Our VFR models long video\ngeneration tasks as an auto-regressive, segment-by-segment generation process,\neliminating the need for resource-intensive generation and lengthy video data,\nwhile providing the flexibility to generate videos of arbitrary length. The key\nchallenges of this task are twofold: ensuring local smoothness between adjacent\nsegments and maintaining global temporal consistency across different segments.\nTo address these challenges, we propose our VFR framework, which ensures\nsmoothness through a prefix video condition and enforces consistency with the\nanchor video -- a 360-degree video that comprehensively captures the human's\nwholebody appearance. Our VFR generates minute-scale virtual try-on videos with\nboth local smoothness and global temporal consistency under various motions,\nmaking it a pioneering work in long virtual try-on video generation.\n", "link": "http://arxiv.org/abs/2509.04450v1", "date": "2025-09-04", "relevancy": 2.4643, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6347}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6093}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Virtual%20Fitting%20Room%3A%20Generating%20Arbitrarily%20Long%20Videos%20of%20Virtual%0A%20%20Try-On%20from%20a%20Single%20Image%20--%20Technical%20Preview&body=Title%3A%20Virtual%20Fitting%20Room%3A%20Generating%20Arbitrarily%20Long%20Videos%20of%20Virtual%0A%20%20Try-On%20from%20a%20Single%20Image%20--%20Technical%20Preview%0AAuthor%3A%20Jun-Kun%20Chen%20and%20Aayush%20Bansal%20and%20Minh%20Phuoc%20Vo%20and%20Yu-Xiong%20Wang%0AAbstract%3A%20%20%20We%20introduce%20the%20Virtual%20Fitting%20Room%20%28VFR%29%2C%20a%20novel%20video%20generative%20model%0Athat%20produces%20arbitrarily%20long%20virtual%20try-on%20videos.%20Our%20VFR%20models%20long%20video%0Ageneration%20tasks%20as%20an%20auto-regressive%2C%20segment-by-segment%20generation%20process%2C%0Aeliminating%20the%20need%20for%20resource-intensive%20generation%20and%20lengthy%20video%20data%2C%0Awhile%20providing%20the%20flexibility%20to%20generate%20videos%20of%20arbitrary%20length.%20The%20key%0Achallenges%20of%20this%20task%20are%20twofold%3A%20ensuring%20local%20smoothness%20between%20adjacent%0Asegments%20and%20maintaining%20global%20temporal%20consistency%20across%20different%20segments.%0ATo%20address%20these%20challenges%2C%20we%20propose%20our%20VFR%20framework%2C%20which%20ensures%0Asmoothness%20through%20a%20prefix%20video%20condition%20and%20enforces%20consistency%20with%20the%0Aanchor%20video%20--%20a%20360-degree%20video%20that%20comprehensively%20captures%20the%20human%27s%0Awholebody%20appearance.%20Our%20VFR%20generates%20minute-scale%20virtual%20try-on%20videos%20with%0Aboth%20local%20smoothness%20and%20global%20temporal%20consistency%20under%20various%20motions%2C%0Amaking%20it%20a%20pioneering%20work%20in%20long%20virtual%20try-on%20video%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04450v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVirtual%2520Fitting%2520Room%253A%2520Generating%2520Arbitrarily%2520Long%2520Videos%2520of%2520Virtual%250A%2520%2520Try-On%2520from%2520a%2520Single%2520Image%2520--%2520Technical%2520Preview%26entry.906535625%3DJun-Kun%2520Chen%2520and%2520Aayush%2520Bansal%2520and%2520Minh%2520Phuoc%2520Vo%2520and%2520Yu-Xiong%2520Wang%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520Virtual%2520Fitting%2520Room%2520%2528VFR%2529%252C%2520a%2520novel%2520video%2520generative%2520model%250Athat%2520produces%2520arbitrarily%2520long%2520virtual%2520try-on%2520videos.%2520Our%2520VFR%2520models%2520long%2520video%250Ageneration%2520tasks%2520as%2520an%2520auto-regressive%252C%2520segment-by-segment%2520generation%2520process%252C%250Aeliminating%2520the%2520need%2520for%2520resource-intensive%2520generation%2520and%2520lengthy%2520video%2520data%252C%250Awhile%2520providing%2520the%2520flexibility%2520to%2520generate%2520videos%2520of%2520arbitrary%2520length.%2520The%2520key%250Achallenges%2520of%2520this%2520task%2520are%2520twofold%253A%2520ensuring%2520local%2520smoothness%2520between%2520adjacent%250Asegments%2520and%2520maintaining%2520global%2520temporal%2520consistency%2520across%2520different%2520segments.%250ATo%2520address%2520these%2520challenges%252C%2520we%2520propose%2520our%2520VFR%2520framework%252C%2520which%2520ensures%250Asmoothness%2520through%2520a%2520prefix%2520video%2520condition%2520and%2520enforces%2520consistency%2520with%2520the%250Aanchor%2520video%2520--%2520a%2520360-degree%2520video%2520that%2520comprehensively%2520captures%2520the%2520human%2527s%250Awholebody%2520appearance.%2520Our%2520VFR%2520generates%2520minute-scale%2520virtual%2520try-on%2520videos%2520with%250Aboth%2520local%2520smoothness%2520and%2520global%2520temporal%2520consistency%2520under%2520various%2520motions%252C%250Amaking%2520it%2520a%2520pioneering%2520work%2520in%2520long%2520virtual%2520try-on%2520video%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04450v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Virtual%20Fitting%20Room%3A%20Generating%20Arbitrarily%20Long%20Videos%20of%20Virtual%0A%20%20Try-On%20from%20a%20Single%20Image%20--%20Technical%20Preview&entry.906535625=Jun-Kun%20Chen%20and%20Aayush%20Bansal%20and%20Minh%20Phuoc%20Vo%20and%20Yu-Xiong%20Wang&entry.1292438233=%20%20We%20introduce%20the%20Virtual%20Fitting%20Room%20%28VFR%29%2C%20a%20novel%20video%20generative%20model%0Athat%20produces%20arbitrarily%20long%20virtual%20try-on%20videos.%20Our%20VFR%20models%20long%20video%0Ageneration%20tasks%20as%20an%20auto-regressive%2C%20segment-by-segment%20generation%20process%2C%0Aeliminating%20the%20need%20for%20resource-intensive%20generation%20and%20lengthy%20video%20data%2C%0Awhile%20providing%20the%20flexibility%20to%20generate%20videos%20of%20arbitrary%20length.%20The%20key%0Achallenges%20of%20this%20task%20are%20twofold%3A%20ensuring%20local%20smoothness%20between%20adjacent%0Asegments%20and%20maintaining%20global%20temporal%20consistency%20across%20different%20segments.%0ATo%20address%20these%20challenges%2C%20we%20propose%20our%20VFR%20framework%2C%20which%20ensures%0Asmoothness%20through%20a%20prefix%20video%20condition%20and%20enforces%20consistency%20with%20the%0Aanchor%20video%20--%20a%20360-degree%20video%20that%20comprehensively%20captures%20the%20human%27s%0Awholebody%20appearance.%20Our%20VFR%20generates%20minute-scale%20virtual%20try-on%20videos%20with%0Aboth%20local%20smoothness%20and%20global%20temporal%20consistency%20under%20various%20motions%2C%0Amaking%20it%20a%20pioneering%20work%20in%20long%20virtual%20try-on%20video%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04450v1&entry.124074799=Read"},
{"title": "Intermediate Languages Matter: Formal Languages and LLMs affect\n  Neurosymbolic Reasoning", "author": "Alexander Beiser and David Penz and Nysret Musliu", "abstract": "  Large language models (LLMs) achieve astonishing results on a wide range of\ntasks. However, their formal reasoning ability still lags behind. A promising\napproach is Neurosymbolic LLM reasoning. It works by using LLMs as translators\nfrom natural to formal languages and symbolic solvers for deriving correct\nresults. Still, the contributing factors to the success of Neurosymbolic LLM\nreasoning remain unclear. This paper demonstrates that one previously\noverlooked factor is the choice of the formal language. We introduce the\nintermediate language challenge: selecting a suitable formal language for\nneurosymbolic reasoning. By comparing four formal languages across three\ndatasets and seven LLMs, we show that the choice of formal language affects\nboth syntactic and semantic reasoning capabilities. We also discuss the varying\neffects across different LLMs.\n", "link": "http://arxiv.org/abs/2509.04083v1", "date": "2025-09-04", "relevancy": 2.4397, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5159}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5159}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intermediate%20Languages%20Matter%3A%20Formal%20Languages%20and%20LLMs%20affect%0A%20%20Neurosymbolic%20Reasoning&body=Title%3A%20Intermediate%20Languages%20Matter%3A%20Formal%20Languages%20and%20LLMs%20affect%0A%20%20Neurosymbolic%20Reasoning%0AAuthor%3A%20Alexander%20Beiser%20and%20David%20Penz%20and%20Nysret%20Musliu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20achieve%20astonishing%20results%20on%20a%20wide%20range%20of%0Atasks.%20However%2C%20their%20formal%20reasoning%20ability%20still%20lags%20behind.%20A%20promising%0Aapproach%20is%20Neurosymbolic%20LLM%20reasoning.%20It%20works%20by%20using%20LLMs%20as%20translators%0Afrom%20natural%20to%20formal%20languages%20and%20symbolic%20solvers%20for%20deriving%20correct%0Aresults.%20Still%2C%20the%20contributing%20factors%20to%20the%20success%20of%20Neurosymbolic%20LLM%0Areasoning%20remain%20unclear.%20This%20paper%20demonstrates%20that%20one%20previously%0Aoverlooked%20factor%20is%20the%20choice%20of%20the%20formal%20language.%20We%20introduce%20the%0Aintermediate%20language%20challenge%3A%20selecting%20a%20suitable%20formal%20language%20for%0Aneurosymbolic%20reasoning.%20By%20comparing%20four%20formal%20languages%20across%20three%0Adatasets%20and%20seven%20LLMs%2C%20we%20show%20that%20the%20choice%20of%20formal%20language%20affects%0Aboth%20syntactic%20and%20semantic%20reasoning%20capabilities.%20We%20also%20discuss%20the%20varying%0Aeffects%20across%20different%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04083v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntermediate%2520Languages%2520Matter%253A%2520Formal%2520Languages%2520and%2520LLMs%2520affect%250A%2520%2520Neurosymbolic%2520Reasoning%26entry.906535625%3DAlexander%2520Beiser%2520and%2520David%2520Penz%2520and%2520Nysret%2520Musliu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520achieve%2520astonishing%2520results%2520on%2520a%2520wide%2520range%2520of%250Atasks.%2520However%252C%2520their%2520formal%2520reasoning%2520ability%2520still%2520lags%2520behind.%2520A%2520promising%250Aapproach%2520is%2520Neurosymbolic%2520LLM%2520reasoning.%2520It%2520works%2520by%2520using%2520LLMs%2520as%2520translators%250Afrom%2520natural%2520to%2520formal%2520languages%2520and%2520symbolic%2520solvers%2520for%2520deriving%2520correct%250Aresults.%2520Still%252C%2520the%2520contributing%2520factors%2520to%2520the%2520success%2520of%2520Neurosymbolic%2520LLM%250Areasoning%2520remain%2520unclear.%2520This%2520paper%2520demonstrates%2520that%2520one%2520previously%250Aoverlooked%2520factor%2520is%2520the%2520choice%2520of%2520the%2520formal%2520language.%2520We%2520introduce%2520the%250Aintermediate%2520language%2520challenge%253A%2520selecting%2520a%2520suitable%2520formal%2520language%2520for%250Aneurosymbolic%2520reasoning.%2520By%2520comparing%2520four%2520formal%2520languages%2520across%2520three%250Adatasets%2520and%2520seven%2520LLMs%252C%2520we%2520show%2520that%2520the%2520choice%2520of%2520formal%2520language%2520affects%250Aboth%2520syntactic%2520and%2520semantic%2520reasoning%2520capabilities.%2520We%2520also%2520discuss%2520the%2520varying%250Aeffects%2520across%2520different%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04083v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intermediate%20Languages%20Matter%3A%20Formal%20Languages%20and%20LLMs%20affect%0A%20%20Neurosymbolic%20Reasoning&entry.906535625=Alexander%20Beiser%20and%20David%20Penz%20and%20Nysret%20Musliu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20achieve%20astonishing%20results%20on%20a%20wide%20range%20of%0Atasks.%20However%2C%20their%20formal%20reasoning%20ability%20still%20lags%20behind.%20A%20promising%0Aapproach%20is%20Neurosymbolic%20LLM%20reasoning.%20It%20works%20by%20using%20LLMs%20as%20translators%0Afrom%20natural%20to%20formal%20languages%20and%20symbolic%20solvers%20for%20deriving%20correct%0Aresults.%20Still%2C%20the%20contributing%20factors%20to%20the%20success%20of%20Neurosymbolic%20LLM%0Areasoning%20remain%20unclear.%20This%20paper%20demonstrates%20that%20one%20previously%0Aoverlooked%20factor%20is%20the%20choice%20of%20the%20formal%20language.%20We%20introduce%20the%0Aintermediate%20language%20challenge%3A%20selecting%20a%20suitable%20formal%20language%20for%0Aneurosymbolic%20reasoning.%20By%20comparing%20four%20formal%20languages%20across%20three%0Adatasets%20and%20seven%20LLMs%2C%20we%20show%20that%20the%20choice%20of%20formal%20language%20affects%0Aboth%20syntactic%20and%20semantic%20reasoning%20capabilities.%20We%20also%20discuss%20the%20varying%0Aeffects%20across%20different%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04083v1&entry.124074799=Read"},
{"title": "TAGAL: Tabular Data Generation using Agentic LLM Methods", "author": "Beno\u00eet Ronval and Pierre Dupont and Siegfried Nijssen", "abstract": "  The generation of data is a common approach to improve the performance of\nmachine learning tasks, among which is the training of models for\nclassification. In this paper, we present TAGAL, a collection of methods able\nto generate synthetic tabular data using an agentic workflow. The methods\nleverage Large Language Models (LLMs) for an automatic and iterative process\nthat uses feedback to improve the generated data without any further LLM\ntraining. The use of LLMs also allows for the addition of external knowledge in\nthe generation process. We evaluate TAGAL across diverse datasets and different\naspects of quality for the generated data. We look at the utility of downstream\nML models, both by training classifiers on synthetic data only and by combining\nreal and synthetic data. Moreover, we compare the similarities between the real\nand the generated data. We show that TAGAL is able to perform on par with\nstate-of-the-art approaches that require LLM training and generally outperforms\nother training-free approaches. These findings highlight the potential of\nagentic workflow and open new directions for LLM-based data generation methods.\n", "link": "http://arxiv.org/abs/2509.04152v1", "date": "2025-09-04", "relevancy": 2.4272, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5142}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4765}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TAGAL%3A%20Tabular%20Data%20Generation%20using%20Agentic%20LLM%20Methods&body=Title%3A%20TAGAL%3A%20Tabular%20Data%20Generation%20using%20Agentic%20LLM%20Methods%0AAuthor%3A%20Beno%C3%AEt%20Ronval%20and%20Pierre%20Dupont%20and%20Siegfried%20Nijssen%0AAbstract%3A%20%20%20The%20generation%20of%20data%20is%20a%20common%20approach%20to%20improve%20the%20performance%20of%0Amachine%20learning%20tasks%2C%20among%20which%20is%20the%20training%20of%20models%20for%0Aclassification.%20In%20this%20paper%2C%20we%20present%20TAGAL%2C%20a%20collection%20of%20methods%20able%0Ato%20generate%20synthetic%20tabular%20data%20using%20an%20agentic%20workflow.%20The%20methods%0Aleverage%20Large%20Language%20Models%20%28LLMs%29%20for%20an%20automatic%20and%20iterative%20process%0Athat%20uses%20feedback%20to%20improve%20the%20generated%20data%20without%20any%20further%20LLM%0Atraining.%20The%20use%20of%20LLMs%20also%20allows%20for%20the%20addition%20of%20external%20knowledge%20in%0Athe%20generation%20process.%20We%20evaluate%20TAGAL%20across%20diverse%20datasets%20and%20different%0Aaspects%20of%20quality%20for%20the%20generated%20data.%20We%20look%20at%20the%20utility%20of%20downstream%0AML%20models%2C%20both%20by%20training%20classifiers%20on%20synthetic%20data%20only%20and%20by%20combining%0Areal%20and%20synthetic%20data.%20Moreover%2C%20we%20compare%20the%20similarities%20between%20the%20real%0Aand%20the%20generated%20data.%20We%20show%20that%20TAGAL%20is%20able%20to%20perform%20on%20par%20with%0Astate-of-the-art%20approaches%20that%20require%20LLM%20training%20and%20generally%20outperforms%0Aother%20training-free%20approaches.%20These%20findings%20highlight%20the%20potential%20of%0Aagentic%20workflow%20and%20open%20new%20directions%20for%20LLM-based%20data%20generation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTAGAL%253A%2520Tabular%2520Data%2520Generation%2520using%2520Agentic%2520LLM%2520Methods%26entry.906535625%3DBeno%25C3%25AEt%2520Ronval%2520and%2520Pierre%2520Dupont%2520and%2520Siegfried%2520Nijssen%26entry.1292438233%3D%2520%2520The%2520generation%2520of%2520data%2520is%2520a%2520common%2520approach%2520to%2520improve%2520the%2520performance%2520of%250Amachine%2520learning%2520tasks%252C%2520among%2520which%2520is%2520the%2520training%2520of%2520models%2520for%250Aclassification.%2520In%2520this%2520paper%252C%2520we%2520present%2520TAGAL%252C%2520a%2520collection%2520of%2520methods%2520able%250Ato%2520generate%2520synthetic%2520tabular%2520data%2520using%2520an%2520agentic%2520workflow.%2520The%2520methods%250Aleverage%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520for%2520an%2520automatic%2520and%2520iterative%2520process%250Athat%2520uses%2520feedback%2520to%2520improve%2520the%2520generated%2520data%2520without%2520any%2520further%2520LLM%250Atraining.%2520The%2520use%2520of%2520LLMs%2520also%2520allows%2520for%2520the%2520addition%2520of%2520external%2520knowledge%2520in%250Athe%2520generation%2520process.%2520We%2520evaluate%2520TAGAL%2520across%2520diverse%2520datasets%2520and%2520different%250Aaspects%2520of%2520quality%2520for%2520the%2520generated%2520data.%2520We%2520look%2520at%2520the%2520utility%2520of%2520downstream%250AML%2520models%252C%2520both%2520by%2520training%2520classifiers%2520on%2520synthetic%2520data%2520only%2520and%2520by%2520combining%250Areal%2520and%2520synthetic%2520data.%2520Moreover%252C%2520we%2520compare%2520the%2520similarities%2520between%2520the%2520real%250Aand%2520the%2520generated%2520data.%2520We%2520show%2520that%2520TAGAL%2520is%2520able%2520to%2520perform%2520on%2520par%2520with%250Astate-of-the-art%2520approaches%2520that%2520require%2520LLM%2520training%2520and%2520generally%2520outperforms%250Aother%2520training-free%2520approaches.%2520These%2520findings%2520highlight%2520the%2520potential%2520of%250Aagentic%2520workflow%2520and%2520open%2520new%2520directions%2520for%2520LLM-based%2520data%2520generation%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TAGAL%3A%20Tabular%20Data%20Generation%20using%20Agentic%20LLM%20Methods&entry.906535625=Beno%C3%AEt%20Ronval%20and%20Pierre%20Dupont%20and%20Siegfried%20Nijssen&entry.1292438233=%20%20The%20generation%20of%20data%20is%20a%20common%20approach%20to%20improve%20the%20performance%20of%0Amachine%20learning%20tasks%2C%20among%20which%20is%20the%20training%20of%20models%20for%0Aclassification.%20In%20this%20paper%2C%20we%20present%20TAGAL%2C%20a%20collection%20of%20methods%20able%0Ato%20generate%20synthetic%20tabular%20data%20using%20an%20agentic%20workflow.%20The%20methods%0Aleverage%20Large%20Language%20Models%20%28LLMs%29%20for%20an%20automatic%20and%20iterative%20process%0Athat%20uses%20feedback%20to%20improve%20the%20generated%20data%20without%20any%20further%20LLM%0Atraining.%20The%20use%20of%20LLMs%20also%20allows%20for%20the%20addition%20of%20external%20knowledge%20in%0Athe%20generation%20process.%20We%20evaluate%20TAGAL%20across%20diverse%20datasets%20and%20different%0Aaspects%20of%20quality%20for%20the%20generated%20data.%20We%20look%20at%20the%20utility%20of%20downstream%0AML%20models%2C%20both%20by%20training%20classifiers%20on%20synthetic%20data%20only%20and%20by%20combining%0Areal%20and%20synthetic%20data.%20Moreover%2C%20we%20compare%20the%20similarities%20between%20the%20real%0Aand%20the%20generated%20data.%20We%20show%20that%20TAGAL%20is%20able%20to%20perform%20on%20par%20with%0Astate-of-the-art%20approaches%20that%20require%20LLM%20training%20and%20generally%20outperforms%0Aother%20training-free%20approaches.%20These%20findings%20highlight%20the%20potential%20of%0Aagentic%20workflow%20and%20open%20new%20directions%20for%20LLM-based%20data%20generation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04152v1&entry.124074799=Read"},
{"title": "Why Can't I See My Clusters? A Precision-Recall Approach to\n  Dimensionality Reduction Validation", "author": "Diede P. M. van der Hoorn and Alessio Arleo and Fernando V. Paulovich", "abstract": "  Dimensionality Reduction (DR) is widely used for visualizing high-dimensional\ndata, often with the goal of revealing expected cluster structure. However,\nsuch a structure may not always appear in the projections. Existing DR quality\nmetrics assess projection reliability (to some extent) or cluster structure\nquality, but do not explain why expected structures are missing. Visual\nAnalytics solutions can help, but are often time-consuming due to the large\nhyperparameter space. This paper addresses this problem by leveraging a recent\nframework that divides the DR process into two phases: a relationship phase,\nwhere similarity relationships are modeled, and a mapping phase, where the data\nis projected accordingly. We introduce two supervised metrics, precision and\nrecall, to evaluate the relationship phase. These metrics quantify how well the\nmodeled relationships align with an expected cluster structure based on some\nset of labels representing this structure. We illustrate their application\nusing t-SNE and UMAP, and validate the approach through various usage\nscenarios. Our approach can guide hyperparameter tuning, uncover projection\nartifacts, and determine if the expected structure is captured in the\nrelationships, making the DR process faster and more reliable.\n", "link": "http://arxiv.org/abs/2509.04222v1", "date": "2025-09-04", "relevancy": 2.4261, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5034}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4761}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Why%20Can%27t%20I%20See%20My%20Clusters%3F%20A%20Precision-Recall%20Approach%20to%0A%20%20Dimensionality%20Reduction%20Validation&body=Title%3A%20Why%20Can%27t%20I%20See%20My%20Clusters%3F%20A%20Precision-Recall%20Approach%20to%0A%20%20Dimensionality%20Reduction%20Validation%0AAuthor%3A%20Diede%20P.%20M.%20van%20der%20Hoorn%20and%20Alessio%20Arleo%20and%20Fernando%20V.%20Paulovich%0AAbstract%3A%20%20%20Dimensionality%20Reduction%20%28DR%29%20is%20widely%20used%20for%20visualizing%20high-dimensional%0Adata%2C%20often%20with%20the%20goal%20of%20revealing%20expected%20cluster%20structure.%20However%2C%0Asuch%20a%20structure%20may%20not%20always%20appear%20in%20the%20projections.%20Existing%20DR%20quality%0Ametrics%20assess%20projection%20reliability%20%28to%20some%20extent%29%20or%20cluster%20structure%0Aquality%2C%20but%20do%20not%20explain%20why%20expected%20structures%20are%20missing.%20Visual%0AAnalytics%20solutions%20can%20help%2C%20but%20are%20often%20time-consuming%20due%20to%20the%20large%0Ahyperparameter%20space.%20This%20paper%20addresses%20this%20problem%20by%20leveraging%20a%20recent%0Aframework%20that%20divides%20the%20DR%20process%20into%20two%20phases%3A%20a%20relationship%20phase%2C%0Awhere%20similarity%20relationships%20are%20modeled%2C%20and%20a%20mapping%20phase%2C%20where%20the%20data%0Ais%20projected%20accordingly.%20We%20introduce%20two%20supervised%20metrics%2C%20precision%20and%0Arecall%2C%20to%20evaluate%20the%20relationship%20phase.%20These%20metrics%20quantify%20how%20well%20the%0Amodeled%20relationships%20align%20with%20an%20expected%20cluster%20structure%20based%20on%20some%0Aset%20of%20labels%20representing%20this%20structure.%20We%20illustrate%20their%20application%0Ausing%20t-SNE%20and%20UMAP%2C%20and%20validate%20the%20approach%20through%20various%20usage%0Ascenarios.%20Our%20approach%20can%20guide%20hyperparameter%20tuning%2C%20uncover%20projection%0Aartifacts%2C%20and%20determine%20if%20the%20expected%20structure%20is%20captured%20in%20the%0Arelationships%2C%20making%20the%20DR%20process%20faster%20and%20more%20reliable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04222v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhy%2520Can%2527t%2520I%2520See%2520My%2520Clusters%253F%2520A%2520Precision-Recall%2520Approach%2520to%250A%2520%2520Dimensionality%2520Reduction%2520Validation%26entry.906535625%3DDiede%2520P.%2520M.%2520van%2520der%2520Hoorn%2520and%2520Alessio%2520Arleo%2520and%2520Fernando%2520V.%2520Paulovich%26entry.1292438233%3D%2520%2520Dimensionality%2520Reduction%2520%2528DR%2529%2520is%2520widely%2520used%2520for%2520visualizing%2520high-dimensional%250Adata%252C%2520often%2520with%2520the%2520goal%2520of%2520revealing%2520expected%2520cluster%2520structure.%2520However%252C%250Asuch%2520a%2520structure%2520may%2520not%2520always%2520appear%2520in%2520the%2520projections.%2520Existing%2520DR%2520quality%250Ametrics%2520assess%2520projection%2520reliability%2520%2528to%2520some%2520extent%2529%2520or%2520cluster%2520structure%250Aquality%252C%2520but%2520do%2520not%2520explain%2520why%2520expected%2520structures%2520are%2520missing.%2520Visual%250AAnalytics%2520solutions%2520can%2520help%252C%2520but%2520are%2520often%2520time-consuming%2520due%2520to%2520the%2520large%250Ahyperparameter%2520space.%2520This%2520paper%2520addresses%2520this%2520problem%2520by%2520leveraging%2520a%2520recent%250Aframework%2520that%2520divides%2520the%2520DR%2520process%2520into%2520two%2520phases%253A%2520a%2520relationship%2520phase%252C%250Awhere%2520similarity%2520relationships%2520are%2520modeled%252C%2520and%2520a%2520mapping%2520phase%252C%2520where%2520the%2520data%250Ais%2520projected%2520accordingly.%2520We%2520introduce%2520two%2520supervised%2520metrics%252C%2520precision%2520and%250Arecall%252C%2520to%2520evaluate%2520the%2520relationship%2520phase.%2520These%2520metrics%2520quantify%2520how%2520well%2520the%250Amodeled%2520relationships%2520align%2520with%2520an%2520expected%2520cluster%2520structure%2520based%2520on%2520some%250Aset%2520of%2520labels%2520representing%2520this%2520structure.%2520We%2520illustrate%2520their%2520application%250Ausing%2520t-SNE%2520and%2520UMAP%252C%2520and%2520validate%2520the%2520approach%2520through%2520various%2520usage%250Ascenarios.%2520Our%2520approach%2520can%2520guide%2520hyperparameter%2520tuning%252C%2520uncover%2520projection%250Aartifacts%252C%2520and%2520determine%2520if%2520the%2520expected%2520structure%2520is%2520captured%2520in%2520the%250Arelationships%252C%2520making%2520the%2520DR%2520process%2520faster%2520and%2520more%2520reliable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04222v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20Can%27t%20I%20See%20My%20Clusters%3F%20A%20Precision-Recall%20Approach%20to%0A%20%20Dimensionality%20Reduction%20Validation&entry.906535625=Diede%20P.%20M.%20van%20der%20Hoorn%20and%20Alessio%20Arleo%20and%20Fernando%20V.%20Paulovich&entry.1292438233=%20%20Dimensionality%20Reduction%20%28DR%29%20is%20widely%20used%20for%20visualizing%20high-dimensional%0Adata%2C%20often%20with%20the%20goal%20of%20revealing%20expected%20cluster%20structure.%20However%2C%0Asuch%20a%20structure%20may%20not%20always%20appear%20in%20the%20projections.%20Existing%20DR%20quality%0Ametrics%20assess%20projection%20reliability%20%28to%20some%20extent%29%20or%20cluster%20structure%0Aquality%2C%20but%20do%20not%20explain%20why%20expected%20structures%20are%20missing.%20Visual%0AAnalytics%20solutions%20can%20help%2C%20but%20are%20often%20time-consuming%20due%20to%20the%20large%0Ahyperparameter%20space.%20This%20paper%20addresses%20this%20problem%20by%20leveraging%20a%20recent%0Aframework%20that%20divides%20the%20DR%20process%20into%20two%20phases%3A%20a%20relationship%20phase%2C%0Awhere%20similarity%20relationships%20are%20modeled%2C%20and%20a%20mapping%20phase%2C%20where%20the%20data%0Ais%20projected%20accordingly.%20We%20introduce%20two%20supervised%20metrics%2C%20precision%20and%0Arecall%2C%20to%20evaluate%20the%20relationship%20phase.%20These%20metrics%20quantify%20how%20well%20the%0Amodeled%20relationships%20align%20with%20an%20expected%20cluster%20structure%20based%20on%20some%0Aset%20of%20labels%20representing%20this%20structure.%20We%20illustrate%20their%20application%0Ausing%20t-SNE%20and%20UMAP%2C%20and%20validate%20the%20approach%20through%20various%20usage%0Ascenarios.%20Our%20approach%20can%20guide%20hyperparameter%20tuning%2C%20uncover%20projection%0Aartifacts%2C%20and%20determine%20if%20the%20expected%20structure%20is%20captured%20in%20the%0Arelationships%2C%20making%20the%20DR%20process%20faster%20and%20more%20reliable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04222v1&entry.124074799=Read"},
{"title": "Crossing the Species Divide: Transfer Learning from Speech to Animal\n  Sounds", "author": "Jules Cauzinille and Marius Miron and Olivier Pietquin and Masato Hagiwara and Ricard Marxer and Arnaud Rey and Benoit Favre", "abstract": "  Self-supervised speech models have demonstrated impressive performance in\nspeech processing, but their effectiveness on non-speech data remains\nunderexplored. We study the transfer learning capabilities of such models on\nbioacoustic detection and classification tasks. We show that models such as\nHuBERT, WavLM, and XEUS can generate rich latent representations of animal\nsounds across taxa. We analyze the models properties with linear probing on\ntime-averaged representations. We then extend the approach to account for the\neffect of time-wise information with other downstream architectures. Finally,\nwe study the implication of frequency range and noise on performance. Notably,\nour results are competitive with fine-tuned bioacoustic pre-trained models and\nshow the impact of noise-robust pre-training setups. These findings highlight\nthe potential of speech-based self-supervised learning as an efficient\nframework for advancing bioacoustic research.\n", "link": "http://arxiv.org/abs/2509.04166v1", "date": "2025-09-04", "relevancy": 2.4258, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4914}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4851}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Crossing%20the%20Species%20Divide%3A%20Transfer%20Learning%20from%20Speech%20to%20Animal%0A%20%20Sounds&body=Title%3A%20Crossing%20the%20Species%20Divide%3A%20Transfer%20Learning%20from%20Speech%20to%20Animal%0A%20%20Sounds%0AAuthor%3A%20Jules%20Cauzinille%20and%20Marius%20Miron%20and%20Olivier%20Pietquin%20and%20Masato%20Hagiwara%20and%20Ricard%20Marxer%20and%20Arnaud%20Rey%20and%20Benoit%20Favre%0AAbstract%3A%20%20%20Self-supervised%20speech%20models%20have%20demonstrated%20impressive%20performance%20in%0Aspeech%20processing%2C%20but%20their%20effectiveness%20on%20non-speech%20data%20remains%0Aunderexplored.%20We%20study%20the%20transfer%20learning%20capabilities%20of%20such%20models%20on%0Abioacoustic%20detection%20and%20classification%20tasks.%20We%20show%20that%20models%20such%20as%0AHuBERT%2C%20WavLM%2C%20and%20XEUS%20can%20generate%20rich%20latent%20representations%20of%20animal%0Asounds%20across%20taxa.%20We%20analyze%20the%20models%20properties%20with%20linear%20probing%20on%0Atime-averaged%20representations.%20We%20then%20extend%20the%20approach%20to%20account%20for%20the%0Aeffect%20of%20time-wise%20information%20with%20other%20downstream%20architectures.%20Finally%2C%0Awe%20study%20the%20implication%20of%20frequency%20range%20and%20noise%20on%20performance.%20Notably%2C%0Aour%20results%20are%20competitive%20with%20fine-tuned%20bioacoustic%20pre-trained%20models%20and%0Ashow%20the%20impact%20of%20noise-robust%20pre-training%20setups.%20These%20findings%20highlight%0Athe%20potential%20of%20speech-based%20self-supervised%20learning%20as%20an%20efficient%0Aframework%20for%20advancing%20bioacoustic%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04166v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCrossing%2520the%2520Species%2520Divide%253A%2520Transfer%2520Learning%2520from%2520Speech%2520to%2520Animal%250A%2520%2520Sounds%26entry.906535625%3DJules%2520Cauzinille%2520and%2520Marius%2520Miron%2520and%2520Olivier%2520Pietquin%2520and%2520Masato%2520Hagiwara%2520and%2520Ricard%2520Marxer%2520and%2520Arnaud%2520Rey%2520and%2520Benoit%2520Favre%26entry.1292438233%3D%2520%2520Self-supervised%2520speech%2520models%2520have%2520demonstrated%2520impressive%2520performance%2520in%250Aspeech%2520processing%252C%2520but%2520their%2520effectiveness%2520on%2520non-speech%2520data%2520remains%250Aunderexplored.%2520We%2520study%2520the%2520transfer%2520learning%2520capabilities%2520of%2520such%2520models%2520on%250Abioacoustic%2520detection%2520and%2520classification%2520tasks.%2520We%2520show%2520that%2520models%2520such%2520as%250AHuBERT%252C%2520WavLM%252C%2520and%2520XEUS%2520can%2520generate%2520rich%2520latent%2520representations%2520of%2520animal%250Asounds%2520across%2520taxa.%2520We%2520analyze%2520the%2520models%2520properties%2520with%2520linear%2520probing%2520on%250Atime-averaged%2520representations.%2520We%2520then%2520extend%2520the%2520approach%2520to%2520account%2520for%2520the%250Aeffect%2520of%2520time-wise%2520information%2520with%2520other%2520downstream%2520architectures.%2520Finally%252C%250Awe%2520study%2520the%2520implication%2520of%2520frequency%2520range%2520and%2520noise%2520on%2520performance.%2520Notably%252C%250Aour%2520results%2520are%2520competitive%2520with%2520fine-tuned%2520bioacoustic%2520pre-trained%2520models%2520and%250Ashow%2520the%2520impact%2520of%2520noise-robust%2520pre-training%2520setups.%2520These%2520findings%2520highlight%250Athe%2520potential%2520of%2520speech-based%2520self-supervised%2520learning%2520as%2520an%2520efficient%250Aframework%2520for%2520advancing%2520bioacoustic%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04166v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Crossing%20the%20Species%20Divide%3A%20Transfer%20Learning%20from%20Speech%20to%20Animal%0A%20%20Sounds&entry.906535625=Jules%20Cauzinille%20and%20Marius%20Miron%20and%20Olivier%20Pietquin%20and%20Masato%20Hagiwara%20and%20Ricard%20Marxer%20and%20Arnaud%20Rey%20and%20Benoit%20Favre&entry.1292438233=%20%20Self-supervised%20speech%20models%20have%20demonstrated%20impressive%20performance%20in%0Aspeech%20processing%2C%20but%20their%20effectiveness%20on%20non-speech%20data%20remains%0Aunderexplored.%20We%20study%20the%20transfer%20learning%20capabilities%20of%20such%20models%20on%0Abioacoustic%20detection%20and%20classification%20tasks.%20We%20show%20that%20models%20such%20as%0AHuBERT%2C%20WavLM%2C%20and%20XEUS%20can%20generate%20rich%20latent%20representations%20of%20animal%0Asounds%20across%20taxa.%20We%20analyze%20the%20models%20properties%20with%20linear%20probing%20on%0Atime-averaged%20representations.%20We%20then%20extend%20the%20approach%20to%20account%20for%20the%0Aeffect%20of%20time-wise%20information%20with%20other%20downstream%20architectures.%20Finally%2C%0Awe%20study%20the%20implication%20of%20frequency%20range%20and%20noise%20on%20performance.%20Notably%2C%0Aour%20results%20are%20competitive%20with%20fine-tuned%20bioacoustic%20pre-trained%20models%20and%0Ashow%20the%20impact%20of%20noise-robust%20pre-training%20setups.%20These%20findings%20highlight%0Athe%20potential%20of%20speech-based%20self-supervised%20learning%20as%20an%20efficient%0Aframework%20for%20advancing%20bioacoustic%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04166v1&entry.124074799=Read"},
{"title": "Mitigating Message Imbalance in Fraud Detection with Dual-View Graph\n  Representation Learning", "author": "Yudan Song and Yuecen Wei and Yuhang Lu and Qingyun Sun and Minglai Shao and Li-e Wang and Chunming Hu and Xianxian Li and Xingcheng Fu", "abstract": "  Graph representation learning has become a mainstream method for fraud\ndetection due to its strong expressive power, which focuses on enhancing node\nrepresentations through improved neighborhood knowledge capture. However, the\nfocus on local interactions leads to imbalanced transmission of global\ntopological information and increased risk of node-specific information being\noverwhelmed during aggregation due to the imbalance between fraud and benign\nnodes. In this paper, we first summarize the impact of topology and class\nimbalance on downstream tasks in GNN-based fraud detection, as the problem of\nimbalanced supervisory messages is caused by fraudsters' topological behavior\nobfuscation and identity feature concealment. Based on statistical validation,\nwe propose a novel dual-view graph representation learning method to mitigate\nMessage imbalance in Fraud Detection (MimbFD). Specifically, we design a\ntopological message reachability module for high-quality node representation\nlearning to penetrate fraudsters' camouflage and alleviate insufficient\npropagation. Then, we introduce a local confounding debiasing module to adjust\nnode representations, enhancing the stable association between node\nrepresentations and labels to balance the influence of different classes.\nFinally, we conducted experiments on three public fraud datasets, and the\nresults demonstrate that MimbFD exhibits outstanding performance in fraud\ndetection.\n", "link": "http://arxiv.org/abs/2507.06469v3", "date": "2025-09-04", "relevancy": 2.4175, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5011}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.479}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Message%20Imbalance%20in%20Fraud%20Detection%20with%20Dual-View%20Graph%0A%20%20Representation%20Learning&body=Title%3A%20Mitigating%20Message%20Imbalance%20in%20Fraud%20Detection%20with%20Dual-View%20Graph%0A%20%20Representation%20Learning%0AAuthor%3A%20Yudan%20Song%20and%20Yuecen%20Wei%20and%20Yuhang%20Lu%20and%20Qingyun%20Sun%20and%20Minglai%20Shao%20and%20Li-e%20Wang%20and%20Chunming%20Hu%20and%20Xianxian%20Li%20and%20Xingcheng%20Fu%0AAbstract%3A%20%20%20Graph%20representation%20learning%20has%20become%20a%20mainstream%20method%20for%20fraud%0Adetection%20due%20to%20its%20strong%20expressive%20power%2C%20which%20focuses%20on%20enhancing%20node%0Arepresentations%20through%20improved%20neighborhood%20knowledge%20capture.%20However%2C%20the%0Afocus%20on%20local%20interactions%20leads%20to%20imbalanced%20transmission%20of%20global%0Atopological%20information%20and%20increased%20risk%20of%20node-specific%20information%20being%0Aoverwhelmed%20during%20aggregation%20due%20to%20the%20imbalance%20between%20fraud%20and%20benign%0Anodes.%20In%20this%20paper%2C%20we%20first%20summarize%20the%20impact%20of%20topology%20and%20class%0Aimbalance%20on%20downstream%20tasks%20in%20GNN-based%20fraud%20detection%2C%20as%20the%20problem%20of%0Aimbalanced%20supervisory%20messages%20is%20caused%20by%20fraudsters%27%20topological%20behavior%0Aobfuscation%20and%20identity%20feature%20concealment.%20Based%20on%20statistical%20validation%2C%0Awe%20propose%20a%20novel%20dual-view%20graph%20representation%20learning%20method%20to%20mitigate%0AMessage%20imbalance%20in%20Fraud%20Detection%20%28MimbFD%29.%20Specifically%2C%20we%20design%20a%0Atopological%20message%20reachability%20module%20for%20high-quality%20node%20representation%0Alearning%20to%20penetrate%20fraudsters%27%20camouflage%20and%20alleviate%20insufficient%0Apropagation.%20Then%2C%20we%20introduce%20a%20local%20confounding%20debiasing%20module%20to%20adjust%0Anode%20representations%2C%20enhancing%20the%20stable%20association%20between%20node%0Arepresentations%20and%20labels%20to%20balance%20the%20influence%20of%20different%20classes.%0AFinally%2C%20we%20conducted%20experiments%20on%20three%20public%20fraud%20datasets%2C%20and%20the%0Aresults%20demonstrate%20that%20MimbFD%20exhibits%20outstanding%20performance%20in%20fraud%0Adetection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06469v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Message%2520Imbalance%2520in%2520Fraud%2520Detection%2520with%2520Dual-View%2520Graph%250A%2520%2520Representation%2520Learning%26entry.906535625%3DYudan%2520Song%2520and%2520Yuecen%2520Wei%2520and%2520Yuhang%2520Lu%2520and%2520Qingyun%2520Sun%2520and%2520Minglai%2520Shao%2520and%2520Li-e%2520Wang%2520and%2520Chunming%2520Hu%2520and%2520Xianxian%2520Li%2520and%2520Xingcheng%2520Fu%26entry.1292438233%3D%2520%2520Graph%2520representation%2520learning%2520has%2520become%2520a%2520mainstream%2520method%2520for%2520fraud%250Adetection%2520due%2520to%2520its%2520strong%2520expressive%2520power%252C%2520which%2520focuses%2520on%2520enhancing%2520node%250Arepresentations%2520through%2520improved%2520neighborhood%2520knowledge%2520capture.%2520However%252C%2520the%250Afocus%2520on%2520local%2520interactions%2520leads%2520to%2520imbalanced%2520transmission%2520of%2520global%250Atopological%2520information%2520and%2520increased%2520risk%2520of%2520node-specific%2520information%2520being%250Aoverwhelmed%2520during%2520aggregation%2520due%2520to%2520the%2520imbalance%2520between%2520fraud%2520and%2520benign%250Anodes.%2520In%2520this%2520paper%252C%2520we%2520first%2520summarize%2520the%2520impact%2520of%2520topology%2520and%2520class%250Aimbalance%2520on%2520downstream%2520tasks%2520in%2520GNN-based%2520fraud%2520detection%252C%2520as%2520the%2520problem%2520of%250Aimbalanced%2520supervisory%2520messages%2520is%2520caused%2520by%2520fraudsters%2527%2520topological%2520behavior%250Aobfuscation%2520and%2520identity%2520feature%2520concealment.%2520Based%2520on%2520statistical%2520validation%252C%250Awe%2520propose%2520a%2520novel%2520dual-view%2520graph%2520representation%2520learning%2520method%2520to%2520mitigate%250AMessage%2520imbalance%2520in%2520Fraud%2520Detection%2520%2528MimbFD%2529.%2520Specifically%252C%2520we%2520design%2520a%250Atopological%2520message%2520reachability%2520module%2520for%2520high-quality%2520node%2520representation%250Alearning%2520to%2520penetrate%2520fraudsters%2527%2520camouflage%2520and%2520alleviate%2520insufficient%250Apropagation.%2520Then%252C%2520we%2520introduce%2520a%2520local%2520confounding%2520debiasing%2520module%2520to%2520adjust%250Anode%2520representations%252C%2520enhancing%2520the%2520stable%2520association%2520between%2520node%250Arepresentations%2520and%2520labels%2520to%2520balance%2520the%2520influence%2520of%2520different%2520classes.%250AFinally%252C%2520we%2520conducted%2520experiments%2520on%2520three%2520public%2520fraud%2520datasets%252C%2520and%2520the%250Aresults%2520demonstrate%2520that%2520MimbFD%2520exhibits%2520outstanding%2520performance%2520in%2520fraud%250Adetection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06469v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Message%20Imbalance%20in%20Fraud%20Detection%20with%20Dual-View%20Graph%0A%20%20Representation%20Learning&entry.906535625=Yudan%20Song%20and%20Yuecen%20Wei%20and%20Yuhang%20Lu%20and%20Qingyun%20Sun%20and%20Minglai%20Shao%20and%20Li-e%20Wang%20and%20Chunming%20Hu%20and%20Xianxian%20Li%20and%20Xingcheng%20Fu&entry.1292438233=%20%20Graph%20representation%20learning%20has%20become%20a%20mainstream%20method%20for%20fraud%0Adetection%20due%20to%20its%20strong%20expressive%20power%2C%20which%20focuses%20on%20enhancing%20node%0Arepresentations%20through%20improved%20neighborhood%20knowledge%20capture.%20However%2C%20the%0Afocus%20on%20local%20interactions%20leads%20to%20imbalanced%20transmission%20of%20global%0Atopological%20information%20and%20increased%20risk%20of%20node-specific%20information%20being%0Aoverwhelmed%20during%20aggregation%20due%20to%20the%20imbalance%20between%20fraud%20and%20benign%0Anodes.%20In%20this%20paper%2C%20we%20first%20summarize%20the%20impact%20of%20topology%20and%20class%0Aimbalance%20on%20downstream%20tasks%20in%20GNN-based%20fraud%20detection%2C%20as%20the%20problem%20of%0Aimbalanced%20supervisory%20messages%20is%20caused%20by%20fraudsters%27%20topological%20behavior%0Aobfuscation%20and%20identity%20feature%20concealment.%20Based%20on%20statistical%20validation%2C%0Awe%20propose%20a%20novel%20dual-view%20graph%20representation%20learning%20method%20to%20mitigate%0AMessage%20imbalance%20in%20Fraud%20Detection%20%28MimbFD%29.%20Specifically%2C%20we%20design%20a%0Atopological%20message%20reachability%20module%20for%20high-quality%20node%20representation%0Alearning%20to%20penetrate%20fraudsters%27%20camouflage%20and%20alleviate%20insufficient%0Apropagation.%20Then%2C%20we%20introduce%20a%20local%20confounding%20debiasing%20module%20to%20adjust%0Anode%20representations%2C%20enhancing%20the%20stable%20association%20between%20node%0Arepresentations%20and%20labels%20to%20balance%20the%20influence%20of%20different%20classes.%0AFinally%2C%20we%20conducted%20experiments%20on%20three%20public%20fraud%20datasets%2C%20and%20the%0Aresults%20demonstrate%20that%20MimbFD%20exhibits%20outstanding%20performance%20in%20fraud%0Adetection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06469v3&entry.124074799=Read"},
{"title": "Transferable Mask Transformer: Cross-domain Semantic Segmentation with\n  Region-adaptive Transferability Estimation", "author": "Jianhua Liu and Zhengyu Li and Yanru Wu and Jingge Wang and Yang Tan and Ruizhe Zhao and Guan Wang and Yang Li", "abstract": "  Recent advances in Vision Transformers (ViTs) have set new benchmarks in\nsemantic segmentation. However, when adapting pretrained ViTs to new target\ndomains, significant performance degradation often occurs due to distribution\nshifts, resulting in suboptimal global attention. Since self-attention\nmechanisms are inherently data-driven, they may fail to effectively attend to\nkey objects when source and target domains exhibit differences in texture,\nscale, or object co-occurrence patterns. While global and patch-level domain\nadaptation methods provide partial solutions, region-level adaptation with\ndynamically shaped regions is crucial due to spatial heterogeneity in\ntransferability across different image areas. We present Transferable Mask\nTransformer (TMT), a novel region-level adaptation framework for semantic\nsegmentation that aligns cross-domain representations through spatial\ntransferability analysis. TMT consists of two key components: (1) An Adaptive\nCluster-based Transferability Estimator (ACTE) that dynamically segments images\ninto structurally and semantically coherent regions for localized\ntransferability assessment, and (2) A Transferable Masked Attention (TMA)\nmodule that integrates region-specific transferability maps into ViTs'\nattention mechanisms, prioritizing adaptation in regions with low\ntransferability and high semantic uncertainty. Comprehensive evaluations across\n20 cross-domain pairs demonstrate TMT's superiority, achieving an average 2%\nMIoU improvement over vanilla fine-tuning and a 1.28% increase compared to\nstate-of-the-art baselines. The source code will be publicly available.\n", "link": "http://arxiv.org/abs/2504.05774v2", "date": "2025-09-04", "relevancy": 2.4093, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6346}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5813}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transferable%20Mask%20Transformer%3A%20Cross-domain%20Semantic%20Segmentation%20with%0A%20%20Region-adaptive%20Transferability%20Estimation&body=Title%3A%20Transferable%20Mask%20Transformer%3A%20Cross-domain%20Semantic%20Segmentation%20with%0A%20%20Region-adaptive%20Transferability%20Estimation%0AAuthor%3A%20Jianhua%20Liu%20and%20Zhengyu%20Li%20and%20Yanru%20Wu%20and%20Jingge%20Wang%20and%20Yang%20Tan%20and%20Ruizhe%20Zhao%20and%20Guan%20Wang%20and%20Yang%20Li%0AAbstract%3A%20%20%20Recent%20advances%20in%20Vision%20Transformers%20%28ViTs%29%20have%20set%20new%20benchmarks%20in%0Asemantic%20segmentation.%20However%2C%20when%20adapting%20pretrained%20ViTs%20to%20new%20target%0Adomains%2C%20significant%20performance%20degradation%20often%20occurs%20due%20to%20distribution%0Ashifts%2C%20resulting%20in%20suboptimal%20global%20attention.%20Since%20self-attention%0Amechanisms%20are%20inherently%20data-driven%2C%20they%20may%20fail%20to%20effectively%20attend%20to%0Akey%20objects%20when%20source%20and%20target%20domains%20exhibit%20differences%20in%20texture%2C%0Ascale%2C%20or%20object%20co-occurrence%20patterns.%20While%20global%20and%20patch-level%20domain%0Aadaptation%20methods%20provide%20partial%20solutions%2C%20region-level%20adaptation%20with%0Adynamically%20shaped%20regions%20is%20crucial%20due%20to%20spatial%20heterogeneity%20in%0Atransferability%20across%20different%20image%20areas.%20We%20present%20Transferable%20Mask%0ATransformer%20%28TMT%29%2C%20a%20novel%20region-level%20adaptation%20framework%20for%20semantic%0Asegmentation%20that%20aligns%20cross-domain%20representations%20through%20spatial%0Atransferability%20analysis.%20TMT%20consists%20of%20two%20key%20components%3A%20%281%29%20An%20Adaptive%0ACluster-based%20Transferability%20Estimator%20%28ACTE%29%20that%20dynamically%20segments%20images%0Ainto%20structurally%20and%20semantically%20coherent%20regions%20for%20localized%0Atransferability%20assessment%2C%20and%20%282%29%20A%20Transferable%20Masked%20Attention%20%28TMA%29%0Amodule%20that%20integrates%20region-specific%20transferability%20maps%20into%20ViTs%27%0Aattention%20mechanisms%2C%20prioritizing%20adaptation%20in%20regions%20with%20low%0Atransferability%20and%20high%20semantic%20uncertainty.%20Comprehensive%20evaluations%20across%0A20%20cross-domain%20pairs%20demonstrate%20TMT%27s%20superiority%2C%20achieving%20an%20average%202%25%0AMIoU%20improvement%20over%20vanilla%20fine-tuning%20and%20a%201.28%25%20increase%20compared%20to%0Astate-of-the-art%20baselines.%20The%20source%20code%20will%20be%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.05774v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransferable%2520Mask%2520Transformer%253A%2520Cross-domain%2520Semantic%2520Segmentation%2520with%250A%2520%2520Region-adaptive%2520Transferability%2520Estimation%26entry.906535625%3DJianhua%2520Liu%2520and%2520Zhengyu%2520Li%2520and%2520Yanru%2520Wu%2520and%2520Jingge%2520Wang%2520and%2520Yang%2520Tan%2520and%2520Ruizhe%2520Zhao%2520and%2520Guan%2520Wang%2520and%2520Yang%2520Li%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520set%2520new%2520benchmarks%2520in%250Asemantic%2520segmentation.%2520However%252C%2520when%2520adapting%2520pretrained%2520ViTs%2520to%2520new%2520target%250Adomains%252C%2520significant%2520performance%2520degradation%2520often%2520occurs%2520due%2520to%2520distribution%250Ashifts%252C%2520resulting%2520in%2520suboptimal%2520global%2520attention.%2520Since%2520self-attention%250Amechanisms%2520are%2520inherently%2520data-driven%252C%2520they%2520may%2520fail%2520to%2520effectively%2520attend%2520to%250Akey%2520objects%2520when%2520source%2520and%2520target%2520domains%2520exhibit%2520differences%2520in%2520texture%252C%250Ascale%252C%2520or%2520object%2520co-occurrence%2520patterns.%2520While%2520global%2520and%2520patch-level%2520domain%250Aadaptation%2520methods%2520provide%2520partial%2520solutions%252C%2520region-level%2520adaptation%2520with%250Adynamically%2520shaped%2520regions%2520is%2520crucial%2520due%2520to%2520spatial%2520heterogeneity%2520in%250Atransferability%2520across%2520different%2520image%2520areas.%2520We%2520present%2520Transferable%2520Mask%250ATransformer%2520%2528TMT%2529%252C%2520a%2520novel%2520region-level%2520adaptation%2520framework%2520for%2520semantic%250Asegmentation%2520that%2520aligns%2520cross-domain%2520representations%2520through%2520spatial%250Atransferability%2520analysis.%2520TMT%2520consists%2520of%2520two%2520key%2520components%253A%2520%25281%2529%2520An%2520Adaptive%250ACluster-based%2520Transferability%2520Estimator%2520%2528ACTE%2529%2520that%2520dynamically%2520segments%2520images%250Ainto%2520structurally%2520and%2520semantically%2520coherent%2520regions%2520for%2520localized%250Atransferability%2520assessment%252C%2520and%2520%25282%2529%2520A%2520Transferable%2520Masked%2520Attention%2520%2528TMA%2529%250Amodule%2520that%2520integrates%2520region-specific%2520transferability%2520maps%2520into%2520ViTs%2527%250Aattention%2520mechanisms%252C%2520prioritizing%2520adaptation%2520in%2520regions%2520with%2520low%250Atransferability%2520and%2520high%2520semantic%2520uncertainty.%2520Comprehensive%2520evaluations%2520across%250A20%2520cross-domain%2520pairs%2520demonstrate%2520TMT%2527s%2520superiority%252C%2520achieving%2520an%2520average%25202%2525%250AMIoU%2520improvement%2520over%2520vanilla%2520fine-tuning%2520and%2520a%25201.28%2525%2520increase%2520compared%2520to%250Astate-of-the-art%2520baselines.%2520The%2520source%2520code%2520will%2520be%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.05774v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transferable%20Mask%20Transformer%3A%20Cross-domain%20Semantic%20Segmentation%20with%0A%20%20Region-adaptive%20Transferability%20Estimation&entry.906535625=Jianhua%20Liu%20and%20Zhengyu%20Li%20and%20Yanru%20Wu%20and%20Jingge%20Wang%20and%20Yang%20Tan%20and%20Ruizhe%20Zhao%20and%20Guan%20Wang%20and%20Yang%20Li&entry.1292438233=%20%20Recent%20advances%20in%20Vision%20Transformers%20%28ViTs%29%20have%20set%20new%20benchmarks%20in%0Asemantic%20segmentation.%20However%2C%20when%20adapting%20pretrained%20ViTs%20to%20new%20target%0Adomains%2C%20significant%20performance%20degradation%20often%20occurs%20due%20to%20distribution%0Ashifts%2C%20resulting%20in%20suboptimal%20global%20attention.%20Since%20self-attention%0Amechanisms%20are%20inherently%20data-driven%2C%20they%20may%20fail%20to%20effectively%20attend%20to%0Akey%20objects%20when%20source%20and%20target%20domains%20exhibit%20differences%20in%20texture%2C%0Ascale%2C%20or%20object%20co-occurrence%20patterns.%20While%20global%20and%20patch-level%20domain%0Aadaptation%20methods%20provide%20partial%20solutions%2C%20region-level%20adaptation%20with%0Adynamically%20shaped%20regions%20is%20crucial%20due%20to%20spatial%20heterogeneity%20in%0Atransferability%20across%20different%20image%20areas.%20We%20present%20Transferable%20Mask%0ATransformer%20%28TMT%29%2C%20a%20novel%20region-level%20adaptation%20framework%20for%20semantic%0Asegmentation%20that%20aligns%20cross-domain%20representations%20through%20spatial%0Atransferability%20analysis.%20TMT%20consists%20of%20two%20key%20components%3A%20%281%29%20An%20Adaptive%0ACluster-based%20Transferability%20Estimator%20%28ACTE%29%20that%20dynamically%20segments%20images%0Ainto%20structurally%20and%20semantically%20coherent%20regions%20for%20localized%0Atransferability%20assessment%2C%20and%20%282%29%20A%20Transferable%20Masked%20Attention%20%28TMA%29%0Amodule%20that%20integrates%20region-specific%20transferability%20maps%20into%20ViTs%27%0Aattention%20mechanisms%2C%20prioritizing%20adaptation%20in%20regions%20with%20low%0Atransferability%20and%20high%20semantic%20uncertainty.%20Comprehensive%20evaluations%20across%0A20%20cross-domain%20pairs%20demonstrate%20TMT%27s%20superiority%2C%20achieving%20an%20average%202%25%0AMIoU%20improvement%20over%20vanilla%20fine-tuning%20and%20a%201.28%25%20increase%20compared%20to%0Astate-of-the-art%20baselines.%20The%20source%20code%20will%20be%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.05774v2&entry.124074799=Read"},
{"title": "Few-step Flow for 3D Generation via Marginal-Data Transport Distillation", "author": "Zanwei Zhou and Taoran Yi and Jiemin Fang and Chen Yang and Lingxi Xie and Xinggang Wang and Wei Shen and Qi Tian", "abstract": "  Flow-based 3D generation models typically require dozens of sampling steps\nduring inference. Though few-step distillation methods, particularly\nConsistency Models (CMs), have achieved substantial advancements in\naccelerating 2D diffusion models, they remain under-explored for more complex\n3D generation tasks. In this study, we propose a novel framework, MDT-dist, for\nfew-step 3D flow distillation. Our approach is built upon a primary objective:\ndistilling the pretrained model to learn the Marginal-Data Transport. Directly\nlearning this objective needs to integrate the velocity fields, while this\nintegral is intractable to be implemented. Therefore, we propose two\noptimizable objectives, Velocity Matching (VM) and Velocity Distillation (VD),\nto equivalently convert the optimization target from the transport level to the\nvelocity and the distribution level respectively. Velocity Matching (VM) learns\nto stably match the velocity fields between the student and the teacher, but\ninevitably provides biased gradient estimates. Velocity Distillation (VD)\nfurther enhances the optimization process by leveraging the learned velocity\nfields to perform probability density distillation. When evaluated on the\npioneer 3D generation framework TRELLIS, our method reduces sampling steps of\neach flow transformer from 25 to 1 or 2, achieving 0.68s (1 step x 2) and 0.94s\n(2 steps x 2) latency with 9.0x and 6.5x speedup on A800, while preserving high\nvisual and geometric fidelity. Extensive experiments demonstrate that our\nmethod significantly outperforms existing CM distillation methods, and enables\nTRELLIS to achieve superior performance in few-step 3D generation.\n", "link": "http://arxiv.org/abs/2509.04406v1", "date": "2025-09-04", "relevancy": 2.3943, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6461}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5922}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-step%20Flow%20for%203D%20Generation%20via%20Marginal-Data%20Transport%20Distillation&body=Title%3A%20Few-step%20Flow%20for%203D%20Generation%20via%20Marginal-Data%20Transport%20Distillation%0AAuthor%3A%20Zanwei%20Zhou%20and%20Taoran%20Yi%20and%20Jiemin%20Fang%20and%20Chen%20Yang%20and%20Lingxi%20Xie%20and%20Xinggang%20Wang%20and%20Wei%20Shen%20and%20Qi%20Tian%0AAbstract%3A%20%20%20Flow-based%203D%20generation%20models%20typically%20require%20dozens%20of%20sampling%20steps%0Aduring%20inference.%20Though%20few-step%20distillation%20methods%2C%20particularly%0AConsistency%20Models%20%28CMs%29%2C%20have%20achieved%20substantial%20advancements%20in%0Aaccelerating%202D%20diffusion%20models%2C%20they%20remain%20under-explored%20for%20more%20complex%0A3D%20generation%20tasks.%20In%20this%20study%2C%20we%20propose%20a%20novel%20framework%2C%20MDT-dist%2C%20for%0Afew-step%203D%20flow%20distillation.%20Our%20approach%20is%20built%20upon%20a%20primary%20objective%3A%0Adistilling%20the%20pretrained%20model%20to%20learn%20the%20Marginal-Data%20Transport.%20Directly%0Alearning%20this%20objective%20needs%20to%20integrate%20the%20velocity%20fields%2C%20while%20this%0Aintegral%20is%20intractable%20to%20be%20implemented.%20Therefore%2C%20we%20propose%20two%0Aoptimizable%20objectives%2C%20Velocity%20Matching%20%28VM%29%20and%20Velocity%20Distillation%20%28VD%29%2C%0Ato%20equivalently%20convert%20the%20optimization%20target%20from%20the%20transport%20level%20to%20the%0Avelocity%20and%20the%20distribution%20level%20respectively.%20Velocity%20Matching%20%28VM%29%20learns%0Ato%20stably%20match%20the%20velocity%20fields%20between%20the%20student%20and%20the%20teacher%2C%20but%0Ainevitably%20provides%20biased%20gradient%20estimates.%20Velocity%20Distillation%20%28VD%29%0Afurther%20enhances%20the%20optimization%20process%20by%20leveraging%20the%20learned%20velocity%0Afields%20to%20perform%20probability%20density%20distillation.%20When%20evaluated%20on%20the%0Apioneer%203D%20generation%20framework%20TRELLIS%2C%20our%20method%20reduces%20sampling%20steps%20of%0Aeach%20flow%20transformer%20from%2025%20to%201%20or%202%2C%20achieving%200.68s%20%281%20step%20x%202%29%20and%200.94s%0A%282%20steps%20x%202%29%20latency%20with%209.0x%20and%206.5x%20speedup%20on%20A800%2C%20while%20preserving%20high%0Avisual%20and%20geometric%20fidelity.%20Extensive%20experiments%20demonstrate%20that%20our%0Amethod%20significantly%20outperforms%20existing%20CM%20distillation%20methods%2C%20and%20enables%0ATRELLIS%20to%20achieve%20superior%20performance%20in%20few-step%203D%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04406v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-step%2520Flow%2520for%25203D%2520Generation%2520via%2520Marginal-Data%2520Transport%2520Distillation%26entry.906535625%3DZanwei%2520Zhou%2520and%2520Taoran%2520Yi%2520and%2520Jiemin%2520Fang%2520and%2520Chen%2520Yang%2520and%2520Lingxi%2520Xie%2520and%2520Xinggang%2520Wang%2520and%2520Wei%2520Shen%2520and%2520Qi%2520Tian%26entry.1292438233%3D%2520%2520Flow-based%25203D%2520generation%2520models%2520typically%2520require%2520dozens%2520of%2520sampling%2520steps%250Aduring%2520inference.%2520Though%2520few-step%2520distillation%2520methods%252C%2520particularly%250AConsistency%2520Models%2520%2528CMs%2529%252C%2520have%2520achieved%2520substantial%2520advancements%2520in%250Aaccelerating%25202D%2520diffusion%2520models%252C%2520they%2520remain%2520under-explored%2520for%2520more%2520complex%250A3D%2520generation%2520tasks.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520novel%2520framework%252C%2520MDT-dist%252C%2520for%250Afew-step%25203D%2520flow%2520distillation.%2520Our%2520approach%2520is%2520built%2520upon%2520a%2520primary%2520objective%253A%250Adistilling%2520the%2520pretrained%2520model%2520to%2520learn%2520the%2520Marginal-Data%2520Transport.%2520Directly%250Alearning%2520this%2520objective%2520needs%2520to%2520integrate%2520the%2520velocity%2520fields%252C%2520while%2520this%250Aintegral%2520is%2520intractable%2520to%2520be%2520implemented.%2520Therefore%252C%2520we%2520propose%2520two%250Aoptimizable%2520objectives%252C%2520Velocity%2520Matching%2520%2528VM%2529%2520and%2520Velocity%2520Distillation%2520%2528VD%2529%252C%250Ato%2520equivalently%2520convert%2520the%2520optimization%2520target%2520from%2520the%2520transport%2520level%2520to%2520the%250Avelocity%2520and%2520the%2520distribution%2520level%2520respectively.%2520Velocity%2520Matching%2520%2528VM%2529%2520learns%250Ato%2520stably%2520match%2520the%2520velocity%2520fields%2520between%2520the%2520student%2520and%2520the%2520teacher%252C%2520but%250Ainevitably%2520provides%2520biased%2520gradient%2520estimates.%2520Velocity%2520Distillation%2520%2528VD%2529%250Afurther%2520enhances%2520the%2520optimization%2520process%2520by%2520leveraging%2520the%2520learned%2520velocity%250Afields%2520to%2520perform%2520probability%2520density%2520distillation.%2520When%2520evaluated%2520on%2520the%250Apioneer%25203D%2520generation%2520framework%2520TRELLIS%252C%2520our%2520method%2520reduces%2520sampling%2520steps%2520of%250Aeach%2520flow%2520transformer%2520from%252025%2520to%25201%2520or%25202%252C%2520achieving%25200.68s%2520%25281%2520step%2520x%25202%2529%2520and%25200.94s%250A%25282%2520steps%2520x%25202%2529%2520latency%2520with%25209.0x%2520and%25206.5x%2520speedup%2520on%2520A800%252C%2520while%2520preserving%2520high%250Avisual%2520and%2520geometric%2520fidelity.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250Amethod%2520significantly%2520outperforms%2520existing%2520CM%2520distillation%2520methods%252C%2520and%2520enables%250ATRELLIS%2520to%2520achieve%2520superior%2520performance%2520in%2520few-step%25203D%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04406v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-step%20Flow%20for%203D%20Generation%20via%20Marginal-Data%20Transport%20Distillation&entry.906535625=Zanwei%20Zhou%20and%20Taoran%20Yi%20and%20Jiemin%20Fang%20and%20Chen%20Yang%20and%20Lingxi%20Xie%20and%20Xinggang%20Wang%20and%20Wei%20Shen%20and%20Qi%20Tian&entry.1292438233=%20%20Flow-based%203D%20generation%20models%20typically%20require%20dozens%20of%20sampling%20steps%0Aduring%20inference.%20Though%20few-step%20distillation%20methods%2C%20particularly%0AConsistency%20Models%20%28CMs%29%2C%20have%20achieved%20substantial%20advancements%20in%0Aaccelerating%202D%20diffusion%20models%2C%20they%20remain%20under-explored%20for%20more%20complex%0A3D%20generation%20tasks.%20In%20this%20study%2C%20we%20propose%20a%20novel%20framework%2C%20MDT-dist%2C%20for%0Afew-step%203D%20flow%20distillation.%20Our%20approach%20is%20built%20upon%20a%20primary%20objective%3A%0Adistilling%20the%20pretrained%20model%20to%20learn%20the%20Marginal-Data%20Transport.%20Directly%0Alearning%20this%20objective%20needs%20to%20integrate%20the%20velocity%20fields%2C%20while%20this%0Aintegral%20is%20intractable%20to%20be%20implemented.%20Therefore%2C%20we%20propose%20two%0Aoptimizable%20objectives%2C%20Velocity%20Matching%20%28VM%29%20and%20Velocity%20Distillation%20%28VD%29%2C%0Ato%20equivalently%20convert%20the%20optimization%20target%20from%20the%20transport%20level%20to%20the%0Avelocity%20and%20the%20distribution%20level%20respectively.%20Velocity%20Matching%20%28VM%29%20learns%0Ato%20stably%20match%20the%20velocity%20fields%20between%20the%20student%20and%20the%20teacher%2C%20but%0Ainevitably%20provides%20biased%20gradient%20estimates.%20Velocity%20Distillation%20%28VD%29%0Afurther%20enhances%20the%20optimization%20process%20by%20leveraging%20the%20learned%20velocity%0Afields%20to%20perform%20probability%20density%20distillation.%20When%20evaluated%20on%20the%0Apioneer%203D%20generation%20framework%20TRELLIS%2C%20our%20method%20reduces%20sampling%20steps%20of%0Aeach%20flow%20transformer%20from%2025%20to%201%20or%202%2C%20achieving%200.68s%20%281%20step%20x%202%29%20and%200.94s%0A%282%20steps%20x%202%29%20latency%20with%209.0x%20and%206.5x%20speedup%20on%20A800%2C%20while%20preserving%20high%0Avisual%20and%20geometric%20fidelity.%20Extensive%20experiments%20demonstrate%20that%20our%0Amethod%20significantly%20outperforms%20existing%20CM%20distillation%20methods%2C%20and%20enables%0ATRELLIS%20to%20achieve%20superior%20performance%20in%20few-step%203D%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04406v1&entry.124074799=Read"},
{"title": "Decoupled Entity Representation Learning for Pinterest Ads Ranking", "author": "Jie Liu and Yinrui Li and Jiankai Sun and Kungang Li and Han Sun and Sihan Wang and Huasen Wu and Siyuan Gao and Paulo Soares and Nan Li and Zhifang Liu and Haoyang Li and Siping Ji and Ling Leng and Prathibha Deshikachar", "abstract": "  In this paper, we introduce a novel framework following an\nupstream-downstream paradigm to construct user and item (Pin) embeddings from\ndiverse data sources, which are essential for Pinterest to deliver personalized\nPins and ads effectively. Our upstream models are trained on extensive data\nsources featuring varied signals, utilizing complex architectures to capture\nintricate relationships between users and Pins on Pinterest. To ensure\nscalability of the upstream models, entity embeddings are learned, and\nregularly refreshed, rather than real-time computation, allowing for\nasynchronous interaction between the upstream and downstream models. These\nembeddings are then integrated as input features in numerous downstream tasks,\nincluding ad retrieval and ranking models for CTR and CVR predictions. We\ndemonstrate that our framework achieves notable performance improvements in\nboth offline and online settings across various downstream tasks. This\nframework has been deployed in Pinterest's production ad ranking systems,\nresulting in significant gains in online metrics.\n", "link": "http://arxiv.org/abs/2509.04337v1", "date": "2025-09-04", "relevancy": 2.3763, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4975}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4718}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoupled%20Entity%20Representation%20Learning%20for%20Pinterest%20Ads%20Ranking&body=Title%3A%20Decoupled%20Entity%20Representation%20Learning%20for%20Pinterest%20Ads%20Ranking%0AAuthor%3A%20Jie%20Liu%20and%20Yinrui%20Li%20and%20Jiankai%20Sun%20and%20Kungang%20Li%20and%20Han%20Sun%20and%20Sihan%20Wang%20and%20Huasen%20Wu%20and%20Siyuan%20Gao%20and%20Paulo%20Soares%20and%20Nan%20Li%20and%20Zhifang%20Liu%20and%20Haoyang%20Li%20and%20Siping%20Ji%20and%20Ling%20Leng%20and%20Prathibha%20Deshikachar%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20framework%20following%20an%0Aupstream-downstream%20paradigm%20to%20construct%20user%20and%20item%20%28Pin%29%20embeddings%20from%0Adiverse%20data%20sources%2C%20which%20are%20essential%20for%20Pinterest%20to%20deliver%20personalized%0APins%20and%20ads%20effectively.%20Our%20upstream%20models%20are%20trained%20on%20extensive%20data%0Asources%20featuring%20varied%20signals%2C%20utilizing%20complex%20architectures%20to%20capture%0Aintricate%20relationships%20between%20users%20and%20Pins%20on%20Pinterest.%20To%20ensure%0Ascalability%20of%20the%20upstream%20models%2C%20entity%20embeddings%20are%20learned%2C%20and%0Aregularly%20refreshed%2C%20rather%20than%20real-time%20computation%2C%20allowing%20for%0Aasynchronous%20interaction%20between%20the%20upstream%20and%20downstream%20models.%20These%0Aembeddings%20are%20then%20integrated%20as%20input%20features%20in%20numerous%20downstream%20tasks%2C%0Aincluding%20ad%20retrieval%20and%20ranking%20models%20for%20CTR%20and%20CVR%20predictions.%20We%0Ademonstrate%20that%20our%20framework%20achieves%20notable%20performance%20improvements%20in%0Aboth%20offline%20and%20online%20settings%20across%20various%20downstream%20tasks.%20This%0Aframework%20has%20been%20deployed%20in%20Pinterest%27s%20production%20ad%20ranking%20systems%2C%0Aresulting%20in%20significant%20gains%20in%20online%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04337v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoupled%2520Entity%2520Representation%2520Learning%2520for%2520Pinterest%2520Ads%2520Ranking%26entry.906535625%3DJie%2520Liu%2520and%2520Yinrui%2520Li%2520and%2520Jiankai%2520Sun%2520and%2520Kungang%2520Li%2520and%2520Han%2520Sun%2520and%2520Sihan%2520Wang%2520and%2520Huasen%2520Wu%2520and%2520Siyuan%2520Gao%2520and%2520Paulo%2520Soares%2520and%2520Nan%2520Li%2520and%2520Zhifang%2520Liu%2520and%2520Haoyang%2520Li%2520and%2520Siping%2520Ji%2520and%2520Ling%2520Leng%2520and%2520Prathibha%2520Deshikachar%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520framework%2520following%2520an%250Aupstream-downstream%2520paradigm%2520to%2520construct%2520user%2520and%2520item%2520%2528Pin%2529%2520embeddings%2520from%250Adiverse%2520data%2520sources%252C%2520which%2520are%2520essential%2520for%2520Pinterest%2520to%2520deliver%2520personalized%250APins%2520and%2520ads%2520effectively.%2520Our%2520upstream%2520models%2520are%2520trained%2520on%2520extensive%2520data%250Asources%2520featuring%2520varied%2520signals%252C%2520utilizing%2520complex%2520architectures%2520to%2520capture%250Aintricate%2520relationships%2520between%2520users%2520and%2520Pins%2520on%2520Pinterest.%2520To%2520ensure%250Ascalability%2520of%2520the%2520upstream%2520models%252C%2520entity%2520embeddings%2520are%2520learned%252C%2520and%250Aregularly%2520refreshed%252C%2520rather%2520than%2520real-time%2520computation%252C%2520allowing%2520for%250Aasynchronous%2520interaction%2520between%2520the%2520upstream%2520and%2520downstream%2520models.%2520These%250Aembeddings%2520are%2520then%2520integrated%2520as%2520input%2520features%2520in%2520numerous%2520downstream%2520tasks%252C%250Aincluding%2520ad%2520retrieval%2520and%2520ranking%2520models%2520for%2520CTR%2520and%2520CVR%2520predictions.%2520We%250Ademonstrate%2520that%2520our%2520framework%2520achieves%2520notable%2520performance%2520improvements%2520in%250Aboth%2520offline%2520and%2520online%2520settings%2520across%2520various%2520downstream%2520tasks.%2520This%250Aframework%2520has%2520been%2520deployed%2520in%2520Pinterest%2527s%2520production%2520ad%2520ranking%2520systems%252C%250Aresulting%2520in%2520significant%2520gains%2520in%2520online%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04337v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoupled%20Entity%20Representation%20Learning%20for%20Pinterest%20Ads%20Ranking&entry.906535625=Jie%20Liu%20and%20Yinrui%20Li%20and%20Jiankai%20Sun%20and%20Kungang%20Li%20and%20Han%20Sun%20and%20Sihan%20Wang%20and%20Huasen%20Wu%20and%20Siyuan%20Gao%20and%20Paulo%20Soares%20and%20Nan%20Li%20and%20Zhifang%20Liu%20and%20Haoyang%20Li%20and%20Siping%20Ji%20and%20Ling%20Leng%20and%20Prathibha%20Deshikachar&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20framework%20following%20an%0Aupstream-downstream%20paradigm%20to%20construct%20user%20and%20item%20%28Pin%29%20embeddings%20from%0Adiverse%20data%20sources%2C%20which%20are%20essential%20for%20Pinterest%20to%20deliver%20personalized%0APins%20and%20ads%20effectively.%20Our%20upstream%20models%20are%20trained%20on%20extensive%20data%0Asources%20featuring%20varied%20signals%2C%20utilizing%20complex%20architectures%20to%20capture%0Aintricate%20relationships%20between%20users%20and%20Pins%20on%20Pinterest.%20To%20ensure%0Ascalability%20of%20the%20upstream%20models%2C%20entity%20embeddings%20are%20learned%2C%20and%0Aregularly%20refreshed%2C%20rather%20than%20real-time%20computation%2C%20allowing%20for%0Aasynchronous%20interaction%20between%20the%20upstream%20and%20downstream%20models.%20These%0Aembeddings%20are%20then%20integrated%20as%20input%20features%20in%20numerous%20downstream%20tasks%2C%0Aincluding%20ad%20retrieval%20and%20ranking%20models%20for%20CTR%20and%20CVR%20predictions.%20We%0Ademonstrate%20that%20our%20framework%20achieves%20notable%20performance%20improvements%20in%0Aboth%20offline%20and%20online%20settings%20across%20various%20downstream%20tasks.%20This%0Aframework%20has%20been%20deployed%20in%20Pinterest%27s%20production%20ad%20ranking%20systems%2C%0Aresulting%20in%20significant%20gains%20in%20online%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04337v1&entry.124074799=Read"},
{"title": "RepoDebug: Repository-Level Multi-Task and Multi-Language Debugging\n  Evaluation of Large Language Models", "author": "Jingjing Liu and Zeming Liu and Zihao Cheng and Mengliang He and Xiaoming Shi and Yuhang Guo and Xiangrong Zhu and Yuanfang Guo and Yunhong Wang and Haifeng Wang", "abstract": "  Large Language Models (LLMs) have exhibited significant proficiency in code\ndebugging, especially in automatic program repair, which may substantially\nreduce the time consumption of developers and enhance their efficiency.\nSignificant advancements in debugging datasets have been made to promote the\ndevelopment of code debugging. However, these datasets primarily focus on\nassessing the LLM's function-level code repair capabilities, neglecting the\nmore complex and realistic repository-level scenarios, which leads to an\nincomplete understanding of the LLM's challenges in repository-level debugging.\nWhile several repository-level datasets have been proposed, they often suffer\nfrom limitations such as limited diversity of tasks, languages, and error\ntypes. To mitigate this challenge, this paper introduces RepoDebug, a\nmulti-task and multi-language repository-level code debugging dataset with 22\nsubtypes of errors that supports 8 commonly used programming languages and 3\ndebugging tasks. Furthermore, we conduct evaluation experiments on 10 LLMs,\nwhere Claude 3.5 Sonnect, the best-performing model, still cannot perform well\nin repository-level debugging.\n", "link": "http://arxiv.org/abs/2509.04078v1", "date": "2025-09-04", "relevancy": 2.3703, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4912}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4912}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RepoDebug%3A%20Repository-Level%20Multi-Task%20and%20Multi-Language%20Debugging%0A%20%20Evaluation%20of%20Large%20Language%20Models&body=Title%3A%20RepoDebug%3A%20Repository-Level%20Multi-Task%20and%20Multi-Language%20Debugging%0A%20%20Evaluation%20of%20Large%20Language%20Models%0AAuthor%3A%20Jingjing%20Liu%20and%20Zeming%20Liu%20and%20Zihao%20Cheng%20and%20Mengliang%20He%20and%20Xiaoming%20Shi%20and%20Yuhang%20Guo%20and%20Xiangrong%20Zhu%20and%20Yuanfang%20Guo%20and%20Yunhong%20Wang%20and%20Haifeng%20Wang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20exhibited%20significant%20proficiency%20in%20code%0Adebugging%2C%20especially%20in%20automatic%20program%20repair%2C%20which%20may%20substantially%0Areduce%20the%20time%20consumption%20of%20developers%20and%20enhance%20their%20efficiency.%0ASignificant%20advancements%20in%20debugging%20datasets%20have%20been%20made%20to%20promote%20the%0Adevelopment%20of%20code%20debugging.%20However%2C%20these%20datasets%20primarily%20focus%20on%0Aassessing%20the%20LLM%27s%20function-level%20code%20repair%20capabilities%2C%20neglecting%20the%0Amore%20complex%20and%20realistic%20repository-level%20scenarios%2C%20which%20leads%20to%20an%0Aincomplete%20understanding%20of%20the%20LLM%27s%20challenges%20in%20repository-level%20debugging.%0AWhile%20several%20repository-level%20datasets%20have%20been%20proposed%2C%20they%20often%20suffer%0Afrom%20limitations%20such%20as%20limited%20diversity%20of%20tasks%2C%20languages%2C%20and%20error%0Atypes.%20To%20mitigate%20this%20challenge%2C%20this%20paper%20introduces%20RepoDebug%2C%20a%0Amulti-task%20and%20multi-language%20repository-level%20code%20debugging%20dataset%20with%2022%0Asubtypes%20of%20errors%20that%20supports%208%20commonly%20used%20programming%20languages%20and%203%0Adebugging%20tasks.%20Furthermore%2C%20we%20conduct%20evaluation%20experiments%20on%2010%20LLMs%2C%0Awhere%20Claude%203.5%20Sonnect%2C%20the%20best-performing%20model%2C%20still%20cannot%20perform%20well%0Ain%20repository-level%20debugging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04078v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepoDebug%253A%2520Repository-Level%2520Multi-Task%2520and%2520Multi-Language%2520Debugging%250A%2520%2520Evaluation%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DJingjing%2520Liu%2520and%2520Zeming%2520Liu%2520and%2520Zihao%2520Cheng%2520and%2520Mengliang%2520He%2520and%2520Xiaoming%2520Shi%2520and%2520Yuhang%2520Guo%2520and%2520Xiangrong%2520Zhu%2520and%2520Yuanfang%2520Guo%2520and%2520Yunhong%2520Wang%2520and%2520Haifeng%2520Wang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520exhibited%2520significant%2520proficiency%2520in%2520code%250Adebugging%252C%2520especially%2520in%2520automatic%2520program%2520repair%252C%2520which%2520may%2520substantially%250Areduce%2520the%2520time%2520consumption%2520of%2520developers%2520and%2520enhance%2520their%2520efficiency.%250ASignificant%2520advancements%2520in%2520debugging%2520datasets%2520have%2520been%2520made%2520to%2520promote%2520the%250Adevelopment%2520of%2520code%2520debugging.%2520However%252C%2520these%2520datasets%2520primarily%2520focus%2520on%250Aassessing%2520the%2520LLM%2527s%2520function-level%2520code%2520repair%2520capabilities%252C%2520neglecting%2520the%250Amore%2520complex%2520and%2520realistic%2520repository-level%2520scenarios%252C%2520which%2520leads%2520to%2520an%250Aincomplete%2520understanding%2520of%2520the%2520LLM%2527s%2520challenges%2520in%2520repository-level%2520debugging.%250AWhile%2520several%2520repository-level%2520datasets%2520have%2520been%2520proposed%252C%2520they%2520often%2520suffer%250Afrom%2520limitations%2520such%2520as%2520limited%2520diversity%2520of%2520tasks%252C%2520languages%252C%2520and%2520error%250Atypes.%2520To%2520mitigate%2520this%2520challenge%252C%2520this%2520paper%2520introduces%2520RepoDebug%252C%2520a%250Amulti-task%2520and%2520multi-language%2520repository-level%2520code%2520debugging%2520dataset%2520with%252022%250Asubtypes%2520of%2520errors%2520that%2520supports%25208%2520commonly%2520used%2520programming%2520languages%2520and%25203%250Adebugging%2520tasks.%2520Furthermore%252C%2520we%2520conduct%2520evaluation%2520experiments%2520on%252010%2520LLMs%252C%250Awhere%2520Claude%25203.5%2520Sonnect%252C%2520the%2520best-performing%2520model%252C%2520still%2520cannot%2520perform%2520well%250Ain%2520repository-level%2520debugging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04078v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RepoDebug%3A%20Repository-Level%20Multi-Task%20and%20Multi-Language%20Debugging%0A%20%20Evaluation%20of%20Large%20Language%20Models&entry.906535625=Jingjing%20Liu%20and%20Zeming%20Liu%20and%20Zihao%20Cheng%20and%20Mengliang%20He%20and%20Xiaoming%20Shi%20and%20Yuhang%20Guo%20and%20Xiangrong%20Zhu%20and%20Yuanfang%20Guo%20and%20Yunhong%20Wang%20and%20Haifeng%20Wang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20exhibited%20significant%20proficiency%20in%20code%0Adebugging%2C%20especially%20in%20automatic%20program%20repair%2C%20which%20may%20substantially%0Areduce%20the%20time%20consumption%20of%20developers%20and%20enhance%20their%20efficiency.%0ASignificant%20advancements%20in%20debugging%20datasets%20have%20been%20made%20to%20promote%20the%0Adevelopment%20of%20code%20debugging.%20However%2C%20these%20datasets%20primarily%20focus%20on%0Aassessing%20the%20LLM%27s%20function-level%20code%20repair%20capabilities%2C%20neglecting%20the%0Amore%20complex%20and%20realistic%20repository-level%20scenarios%2C%20which%20leads%20to%20an%0Aincomplete%20understanding%20of%20the%20LLM%27s%20challenges%20in%20repository-level%20debugging.%0AWhile%20several%20repository-level%20datasets%20have%20been%20proposed%2C%20they%20often%20suffer%0Afrom%20limitations%20such%20as%20limited%20diversity%20of%20tasks%2C%20languages%2C%20and%20error%0Atypes.%20To%20mitigate%20this%20challenge%2C%20this%20paper%20introduces%20RepoDebug%2C%20a%0Amulti-task%20and%20multi-language%20repository-level%20code%20debugging%20dataset%20with%2022%0Asubtypes%20of%20errors%20that%20supports%208%20commonly%20used%20programming%20languages%20and%203%0Adebugging%20tasks.%20Furthermore%2C%20we%20conduct%20evaluation%20experiments%20on%2010%20LLMs%2C%0Awhere%20Claude%203.5%20Sonnect%2C%20the%20best-performing%20model%2C%20still%20cannot%20perform%20well%0Ain%20repository-level%20debugging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04078v1&entry.124074799=Read"},
{"title": "TauGenNet: Plasma-Driven Tau PET Image Synthesis via Text-Guided 3D\n  Diffusion Models", "author": "Yuxin Gong and Se-in Jang and Wei Shao and Yi Su and Kuang Gong", "abstract": "  Accurate quantification of tau pathology via tau positron emission tomography\n(PET) scan is crucial for diagnosing and monitoring Alzheimer's disease (AD).\nHowever, the high cost and limited availability of tau PET restrict its\nwidespread use. In contrast, structural magnetic resonance imaging (MRI) and\nplasma-based biomarkers provide non-invasive and widely available complementary\ninformation related to brain anatomy and disease progression. In this work, we\npropose a text-guided 3D diffusion model for 3D tau PET image synthesis,\nleveraging multimodal conditions from both structural MRI and plasma\nmeasurement. Specifically, the textual prompt is from the plasma p-tau217\nmeasurement, which is a key indicator of AD progression, while MRI provides\nanatomical structure constraints. The proposed framework is trained and\nevaluated using clinical AV1451 tau PET data from the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) database. Experimental results demonstrate that\nour approach can generate realistic, clinically meaningful 3D tau PET across a\nrange of disease stages. The proposed framework can help perform tau PET data\naugmentation under different settings, provide a non-invasive, cost-effective\nalternative for visualizing tau pathology, and support the simulation of\ndisease progression under varying plasma biomarker levels and cognitive\nconditions.\n", "link": "http://arxiv.org/abs/2509.04269v1", "date": "2025-09-04", "relevancy": 2.3596, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5989}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5989}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TauGenNet%3A%20Plasma-Driven%20Tau%20PET%20Image%20Synthesis%20via%20Text-Guided%203D%0A%20%20Diffusion%20Models&body=Title%3A%20TauGenNet%3A%20Plasma-Driven%20Tau%20PET%20Image%20Synthesis%20via%20Text-Guided%203D%0A%20%20Diffusion%20Models%0AAuthor%3A%20Yuxin%20Gong%20and%20Se-in%20Jang%20and%20Wei%20Shao%20and%20Yi%20Su%20and%20Kuang%20Gong%0AAbstract%3A%20%20%20Accurate%20quantification%20of%20tau%20pathology%20via%20tau%20positron%20emission%20tomography%0A%28PET%29%20scan%20is%20crucial%20for%20diagnosing%20and%20monitoring%20Alzheimer%27s%20disease%20%28AD%29.%0AHowever%2C%20the%20high%20cost%20and%20limited%20availability%20of%20tau%20PET%20restrict%20its%0Awidespread%20use.%20In%20contrast%2C%20structural%20magnetic%20resonance%20imaging%20%28MRI%29%20and%0Aplasma-based%20biomarkers%20provide%20non-invasive%20and%20widely%20available%20complementary%0Ainformation%20related%20to%20brain%20anatomy%20and%20disease%20progression.%20In%20this%20work%2C%20we%0Apropose%20a%20text-guided%203D%20diffusion%20model%20for%203D%20tau%20PET%20image%20synthesis%2C%0Aleveraging%20multimodal%20conditions%20from%20both%20structural%20MRI%20and%20plasma%0Ameasurement.%20Specifically%2C%20the%20textual%20prompt%20is%20from%20the%20plasma%20p-tau217%0Ameasurement%2C%20which%20is%20a%20key%20indicator%20of%20AD%20progression%2C%20while%20MRI%20provides%0Aanatomical%20structure%20constraints.%20The%20proposed%20framework%20is%20trained%20and%0Aevaluated%20using%20clinical%20AV1451%20tau%20PET%20data%20from%20the%20Alzheimer%27s%20Disease%0ANeuroimaging%20Initiative%20%28ADNI%29%20database.%20Experimental%20results%20demonstrate%20that%0Aour%20approach%20can%20generate%20realistic%2C%20clinically%20meaningful%203D%20tau%20PET%20across%20a%0Arange%20of%20disease%20stages.%20The%20proposed%20framework%20can%20help%20perform%20tau%20PET%20data%0Aaugmentation%20under%20different%20settings%2C%20provide%20a%20non-invasive%2C%20cost-effective%0Aalternative%20for%20visualizing%20tau%20pathology%2C%20and%20support%20the%20simulation%20of%0Adisease%20progression%20under%20varying%20plasma%20biomarker%20levels%20and%20cognitive%0Aconditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04269v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTauGenNet%253A%2520Plasma-Driven%2520Tau%2520PET%2520Image%2520Synthesis%2520via%2520Text-Guided%25203D%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DYuxin%2520Gong%2520and%2520Se-in%2520Jang%2520and%2520Wei%2520Shao%2520and%2520Yi%2520Su%2520and%2520Kuang%2520Gong%26entry.1292438233%3D%2520%2520Accurate%2520quantification%2520of%2520tau%2520pathology%2520via%2520tau%2520positron%2520emission%2520tomography%250A%2528PET%2529%2520scan%2520is%2520crucial%2520for%2520diagnosing%2520and%2520monitoring%2520Alzheimer%2527s%2520disease%2520%2528AD%2529.%250AHowever%252C%2520the%2520high%2520cost%2520and%2520limited%2520availability%2520of%2520tau%2520PET%2520restrict%2520its%250Awidespread%2520use.%2520In%2520contrast%252C%2520structural%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520and%250Aplasma-based%2520biomarkers%2520provide%2520non-invasive%2520and%2520widely%2520available%2520complementary%250Ainformation%2520related%2520to%2520brain%2520anatomy%2520and%2520disease%2520progression.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520text-guided%25203D%2520diffusion%2520model%2520for%25203D%2520tau%2520PET%2520image%2520synthesis%252C%250Aleveraging%2520multimodal%2520conditions%2520from%2520both%2520structural%2520MRI%2520and%2520plasma%250Ameasurement.%2520Specifically%252C%2520the%2520textual%2520prompt%2520is%2520from%2520the%2520plasma%2520p-tau217%250Ameasurement%252C%2520which%2520is%2520a%2520key%2520indicator%2520of%2520AD%2520progression%252C%2520while%2520MRI%2520provides%250Aanatomical%2520structure%2520constraints.%2520The%2520proposed%2520framework%2520is%2520trained%2520and%250Aevaluated%2520using%2520clinical%2520AV1451%2520tau%2520PET%2520data%2520from%2520the%2520Alzheimer%2527s%2520Disease%250ANeuroimaging%2520Initiative%2520%2528ADNI%2529%2520database.%2520Experimental%2520results%2520demonstrate%2520that%250Aour%2520approach%2520can%2520generate%2520realistic%252C%2520clinically%2520meaningful%25203D%2520tau%2520PET%2520across%2520a%250Arange%2520of%2520disease%2520stages.%2520The%2520proposed%2520framework%2520can%2520help%2520perform%2520tau%2520PET%2520data%250Aaugmentation%2520under%2520different%2520settings%252C%2520provide%2520a%2520non-invasive%252C%2520cost-effective%250Aalternative%2520for%2520visualizing%2520tau%2520pathology%252C%2520and%2520support%2520the%2520simulation%2520of%250Adisease%2520progression%2520under%2520varying%2520plasma%2520biomarker%2520levels%2520and%2520cognitive%250Aconditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04269v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TauGenNet%3A%20Plasma-Driven%20Tau%20PET%20Image%20Synthesis%20via%20Text-Guided%203D%0A%20%20Diffusion%20Models&entry.906535625=Yuxin%20Gong%20and%20Se-in%20Jang%20and%20Wei%20Shao%20and%20Yi%20Su%20and%20Kuang%20Gong&entry.1292438233=%20%20Accurate%20quantification%20of%20tau%20pathology%20via%20tau%20positron%20emission%20tomography%0A%28PET%29%20scan%20is%20crucial%20for%20diagnosing%20and%20monitoring%20Alzheimer%27s%20disease%20%28AD%29.%0AHowever%2C%20the%20high%20cost%20and%20limited%20availability%20of%20tau%20PET%20restrict%20its%0Awidespread%20use.%20In%20contrast%2C%20structural%20magnetic%20resonance%20imaging%20%28MRI%29%20and%0Aplasma-based%20biomarkers%20provide%20non-invasive%20and%20widely%20available%20complementary%0Ainformation%20related%20to%20brain%20anatomy%20and%20disease%20progression.%20In%20this%20work%2C%20we%0Apropose%20a%20text-guided%203D%20diffusion%20model%20for%203D%20tau%20PET%20image%20synthesis%2C%0Aleveraging%20multimodal%20conditions%20from%20both%20structural%20MRI%20and%20plasma%0Ameasurement.%20Specifically%2C%20the%20textual%20prompt%20is%20from%20the%20plasma%20p-tau217%0Ameasurement%2C%20which%20is%20a%20key%20indicator%20of%20AD%20progression%2C%20while%20MRI%20provides%0Aanatomical%20structure%20constraints.%20The%20proposed%20framework%20is%20trained%20and%0Aevaluated%20using%20clinical%20AV1451%20tau%20PET%20data%20from%20the%20Alzheimer%27s%20Disease%0ANeuroimaging%20Initiative%20%28ADNI%29%20database.%20Experimental%20results%20demonstrate%20that%0Aour%20approach%20can%20generate%20realistic%2C%20clinically%20meaningful%203D%20tau%20PET%20across%20a%0Arange%20of%20disease%20stages.%20The%20proposed%20framework%20can%20help%20perform%20tau%20PET%20data%0Aaugmentation%20under%20different%20settings%2C%20provide%20a%20non-invasive%2C%20cost-effective%0Aalternative%20for%20visualizing%20tau%20pathology%2C%20and%20support%20the%20simulation%20of%0Adisease%20progression%20under%20varying%20plasma%20biomarker%20levels%20and%20cognitive%0Aconditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04269v1&entry.124074799=Read"},
{"title": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference", "author": "Krishna Teja Chitty-Venkata and Jie Ye and Xian-He Sun and Anthony Kougkas and Murali Emani and Venkatram Vishwanath and Bogdan Nicolae", "abstract": "  KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks.\n", "link": "http://arxiv.org/abs/2509.04377v1", "date": "2025-09-04", "relevancy": 2.3491, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4846}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4624}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PagedEviction%3A%20Structured%20Block-wise%20KV%20Cache%20Pruning%20for%20Efficient%0A%20%20Large%20Language%20Model%20Inference&body=Title%3A%20PagedEviction%3A%20Structured%20Block-wise%20KV%20Cache%20Pruning%20for%20Efficient%0A%20%20Large%20Language%20Model%20Inference%0AAuthor%3A%20Krishna%20Teja%20Chitty-Venkata%20and%20Jie%20Ye%20and%20Xian-He%20Sun%20and%20Anthony%20Kougkas%20and%20Murali%20Emani%20and%20Venkatram%20Vishwanath%20and%20Bogdan%20Nicolae%0AAbstract%3A%20%20%20KV%20caching%20significantly%20improves%20the%20efficiency%20of%20Large%20Language%20Model%0A%28LLM%29%20inference%20by%20storing%20attention%20states%20from%20previously%20processed%20tokens%2C%0Aenabling%20faster%20generation%20of%20subsequent%20tokens.%20However%2C%20as%20sequence%20length%0Aincreases%2C%20the%20KV%20cache%20quickly%20becomes%20a%20major%20memory%20bottleneck.%20To%20address%0Athis%2C%20we%20propose%20PagedEviction%2C%20a%20novel%20fine-grained%2C%20structured%20KV%20cache%0Apruning%20strategy%20that%20enhances%20the%20memory%20efficiency%20of%20vLLM%27s%20PagedAttention.%0AUnlike%20existing%20approaches%20that%20rely%20on%20attention-based%20token%20importance%20or%0Aevict%20tokens%20across%20different%20vLLM%20pages%2C%20PagedEviction%20introduces%20an%20efficient%0Ablock-wise%20eviction%20algorithm%20tailored%20for%20paged%20memory%20layouts.%20Our%20method%0Aintegrates%20seamlessly%20with%20PagedAttention%20without%20requiring%20any%20modifications%0Ato%20its%20CUDA%20attention%20kernels.%20We%20evaluate%20PagedEviction%20across%0ALlama-3.1-8B-Instruct%2C%20Llama-3.2-1B-Instruct%2C%20and%20Llama-3.2-3B-Instruct%20models%0Aon%20the%20LongBench%20benchmark%20suite%2C%20demonstrating%20improved%20memory%20usage%20with%0Abetter%20accuracy%20than%20baselines%20on%20long%20context%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04377v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPagedEviction%253A%2520Structured%2520Block-wise%2520KV%2520Cache%2520Pruning%2520for%2520Efficient%250A%2520%2520Large%2520Language%2520Model%2520Inference%26entry.906535625%3DKrishna%2520Teja%2520Chitty-Venkata%2520and%2520Jie%2520Ye%2520and%2520Xian-He%2520Sun%2520and%2520Anthony%2520Kougkas%2520and%2520Murali%2520Emani%2520and%2520Venkatram%2520Vishwanath%2520and%2520Bogdan%2520Nicolae%26entry.1292438233%3D%2520%2520KV%2520caching%2520significantly%2520improves%2520the%2520efficiency%2520of%2520Large%2520Language%2520Model%250A%2528LLM%2529%2520inference%2520by%2520storing%2520attention%2520states%2520from%2520previously%2520processed%2520tokens%252C%250Aenabling%2520faster%2520generation%2520of%2520subsequent%2520tokens.%2520However%252C%2520as%2520sequence%2520length%250Aincreases%252C%2520the%2520KV%2520cache%2520quickly%2520becomes%2520a%2520major%2520memory%2520bottleneck.%2520To%2520address%250Athis%252C%2520we%2520propose%2520PagedEviction%252C%2520a%2520novel%2520fine-grained%252C%2520structured%2520KV%2520cache%250Apruning%2520strategy%2520that%2520enhances%2520the%2520memory%2520efficiency%2520of%2520vLLM%2527s%2520PagedAttention.%250AUnlike%2520existing%2520approaches%2520that%2520rely%2520on%2520attention-based%2520token%2520importance%2520or%250Aevict%2520tokens%2520across%2520different%2520vLLM%2520pages%252C%2520PagedEviction%2520introduces%2520an%2520efficient%250Ablock-wise%2520eviction%2520algorithm%2520tailored%2520for%2520paged%2520memory%2520layouts.%2520Our%2520method%250Aintegrates%2520seamlessly%2520with%2520PagedAttention%2520without%2520requiring%2520any%2520modifications%250Ato%2520its%2520CUDA%2520attention%2520kernels.%2520We%2520evaluate%2520PagedEviction%2520across%250ALlama-3.1-8B-Instruct%252C%2520Llama-3.2-1B-Instruct%252C%2520and%2520Llama-3.2-3B-Instruct%2520models%250Aon%2520the%2520LongBench%2520benchmark%2520suite%252C%2520demonstrating%2520improved%2520memory%2520usage%2520with%250Abetter%2520accuracy%2520than%2520baselines%2520on%2520long%2520context%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04377v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PagedEviction%3A%20Structured%20Block-wise%20KV%20Cache%20Pruning%20for%20Efficient%0A%20%20Large%20Language%20Model%20Inference&entry.906535625=Krishna%20Teja%20Chitty-Venkata%20and%20Jie%20Ye%20and%20Xian-He%20Sun%20and%20Anthony%20Kougkas%20and%20Murali%20Emani%20and%20Venkatram%20Vishwanath%20and%20Bogdan%20Nicolae&entry.1292438233=%20%20KV%20caching%20significantly%20improves%20the%20efficiency%20of%20Large%20Language%20Model%0A%28LLM%29%20inference%20by%20storing%20attention%20states%20from%20previously%20processed%20tokens%2C%0Aenabling%20faster%20generation%20of%20subsequent%20tokens.%20However%2C%20as%20sequence%20length%0Aincreases%2C%20the%20KV%20cache%20quickly%20becomes%20a%20major%20memory%20bottleneck.%20To%20address%0Athis%2C%20we%20propose%20PagedEviction%2C%20a%20novel%20fine-grained%2C%20structured%20KV%20cache%0Apruning%20strategy%20that%20enhances%20the%20memory%20efficiency%20of%20vLLM%27s%20PagedAttention.%0AUnlike%20existing%20approaches%20that%20rely%20on%20attention-based%20token%20importance%20or%0Aevict%20tokens%20across%20different%20vLLM%20pages%2C%20PagedEviction%20introduces%20an%20efficient%0Ablock-wise%20eviction%20algorithm%20tailored%20for%20paged%20memory%20layouts.%20Our%20method%0Aintegrates%20seamlessly%20with%20PagedAttention%20without%20requiring%20any%20modifications%0Ato%20its%20CUDA%20attention%20kernels.%20We%20evaluate%20PagedEviction%20across%0ALlama-3.1-8B-Instruct%2C%20Llama-3.2-1B-Instruct%2C%20and%20Llama-3.2-3B-Instruct%20models%0Aon%20the%20LongBench%20benchmark%20suite%2C%20demonstrating%20improved%20memory%20usage%20with%0Abetter%20accuracy%20than%20baselines%20on%20long%20context%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04377v1&entry.124074799=Read"},
{"title": "Durian: Dual Reference-guided Portrait Animation with Attribute Transfer", "author": "Hyunsoo Cha and Byungjun Kim and Hanbyul Joo", "abstract": "  We present Durian, the first method for generating portrait animation videos\nwith facial attribute transfer from a given reference image to a target\nportrait in a zero-shot manner. To enable high-fidelity and spatially\nconsistent attribute transfer across frames, we introduce dual reference\nnetworks that inject spatial features from both the portrait and attribute\nimages into the denoising process of a diffusion model. We train the model\nusing a self-reconstruction formulation, where two frames are sampled from the\nsame portrait video: one is treated as the attribute reference and the other as\nthe target portrait, and the remaining frames are reconstructed conditioned on\nthese inputs and their corresponding masks. To support the transfer of\nattributes with varying spatial extent, we propose a mask expansion strategy\nusing keypoint-conditioned image generation for training. In addition, we\nfurther augment the attribute and portrait images with spatial and\nappearance-level transformations to improve robustness to positional\nmisalignment between them. These strategies allow the model to effectively\ngeneralize across diverse attributes and in-the-wild reference combinations,\ndespite being trained without explicit triplet supervision. Durian achieves\nstate-of-the-art performance on portrait animation with attribute transfer, and\nnotably, its dual reference design enables multi-attribute composition in a\nsingle generation pass without additional training.\n", "link": "http://arxiv.org/abs/2509.04434v1", "date": "2025-09-04", "relevancy": 2.3411, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5896}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5893}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Durian%3A%20Dual%20Reference-guided%20Portrait%20Animation%20with%20Attribute%20Transfer&body=Title%3A%20Durian%3A%20Dual%20Reference-guided%20Portrait%20Animation%20with%20Attribute%20Transfer%0AAuthor%3A%20Hyunsoo%20Cha%20and%20Byungjun%20Kim%20and%20Hanbyul%20Joo%0AAbstract%3A%20%20%20We%20present%20Durian%2C%20the%20first%20method%20for%20generating%20portrait%20animation%20videos%0Awith%20facial%20attribute%20transfer%20from%20a%20given%20reference%20image%20to%20a%20target%0Aportrait%20in%20a%20zero-shot%20manner.%20To%20enable%20high-fidelity%20and%20spatially%0Aconsistent%20attribute%20transfer%20across%20frames%2C%20we%20introduce%20dual%20reference%0Anetworks%20that%20inject%20spatial%20features%20from%20both%20the%20portrait%20and%20attribute%0Aimages%20into%20the%20denoising%20process%20of%20a%20diffusion%20model.%20We%20train%20the%20model%0Ausing%20a%20self-reconstruction%20formulation%2C%20where%20two%20frames%20are%20sampled%20from%20the%0Asame%20portrait%20video%3A%20one%20is%20treated%20as%20the%20attribute%20reference%20and%20the%20other%20as%0Athe%20target%20portrait%2C%20and%20the%20remaining%20frames%20are%20reconstructed%20conditioned%20on%0Athese%20inputs%20and%20their%20corresponding%20masks.%20To%20support%20the%20transfer%20of%0Aattributes%20with%20varying%20spatial%20extent%2C%20we%20propose%20a%20mask%20expansion%20strategy%0Ausing%20keypoint-conditioned%20image%20generation%20for%20training.%20In%20addition%2C%20we%0Afurther%20augment%20the%20attribute%20and%20portrait%20images%20with%20spatial%20and%0Aappearance-level%20transformations%20to%20improve%20robustness%20to%20positional%0Amisalignment%20between%20them.%20These%20strategies%20allow%20the%20model%20to%20effectively%0Ageneralize%20across%20diverse%20attributes%20and%20in-the-wild%20reference%20combinations%2C%0Adespite%20being%20trained%20without%20explicit%20triplet%20supervision.%20Durian%20achieves%0Astate-of-the-art%20performance%20on%20portrait%20animation%20with%20attribute%20transfer%2C%20and%0Anotably%2C%20its%20dual%20reference%20design%20enables%20multi-attribute%20composition%20in%20a%0Asingle%20generation%20pass%20without%20additional%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04434v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDurian%253A%2520Dual%2520Reference-guided%2520Portrait%2520Animation%2520with%2520Attribute%2520Transfer%26entry.906535625%3DHyunsoo%2520Cha%2520and%2520Byungjun%2520Kim%2520and%2520Hanbyul%2520Joo%26entry.1292438233%3D%2520%2520We%2520present%2520Durian%252C%2520the%2520first%2520method%2520for%2520generating%2520portrait%2520animation%2520videos%250Awith%2520facial%2520attribute%2520transfer%2520from%2520a%2520given%2520reference%2520image%2520to%2520a%2520target%250Aportrait%2520in%2520a%2520zero-shot%2520manner.%2520To%2520enable%2520high-fidelity%2520and%2520spatially%250Aconsistent%2520attribute%2520transfer%2520across%2520frames%252C%2520we%2520introduce%2520dual%2520reference%250Anetworks%2520that%2520inject%2520spatial%2520features%2520from%2520both%2520the%2520portrait%2520and%2520attribute%250Aimages%2520into%2520the%2520denoising%2520process%2520of%2520a%2520diffusion%2520model.%2520We%2520train%2520the%2520model%250Ausing%2520a%2520self-reconstruction%2520formulation%252C%2520where%2520two%2520frames%2520are%2520sampled%2520from%2520the%250Asame%2520portrait%2520video%253A%2520one%2520is%2520treated%2520as%2520the%2520attribute%2520reference%2520and%2520the%2520other%2520as%250Athe%2520target%2520portrait%252C%2520and%2520the%2520remaining%2520frames%2520are%2520reconstructed%2520conditioned%2520on%250Athese%2520inputs%2520and%2520their%2520corresponding%2520masks.%2520To%2520support%2520the%2520transfer%2520of%250Aattributes%2520with%2520varying%2520spatial%2520extent%252C%2520we%2520propose%2520a%2520mask%2520expansion%2520strategy%250Ausing%2520keypoint-conditioned%2520image%2520generation%2520for%2520training.%2520In%2520addition%252C%2520we%250Afurther%2520augment%2520the%2520attribute%2520and%2520portrait%2520images%2520with%2520spatial%2520and%250Aappearance-level%2520transformations%2520to%2520improve%2520robustness%2520to%2520positional%250Amisalignment%2520between%2520them.%2520These%2520strategies%2520allow%2520the%2520model%2520to%2520effectively%250Ageneralize%2520across%2520diverse%2520attributes%2520and%2520in-the-wild%2520reference%2520combinations%252C%250Adespite%2520being%2520trained%2520without%2520explicit%2520triplet%2520supervision.%2520Durian%2520achieves%250Astate-of-the-art%2520performance%2520on%2520portrait%2520animation%2520with%2520attribute%2520transfer%252C%2520and%250Anotably%252C%2520its%2520dual%2520reference%2520design%2520enables%2520multi-attribute%2520composition%2520in%2520a%250Asingle%2520generation%2520pass%2520without%2520additional%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04434v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Durian%3A%20Dual%20Reference-guided%20Portrait%20Animation%20with%20Attribute%20Transfer&entry.906535625=Hyunsoo%20Cha%20and%20Byungjun%20Kim%20and%20Hanbyul%20Joo&entry.1292438233=%20%20We%20present%20Durian%2C%20the%20first%20method%20for%20generating%20portrait%20animation%20videos%0Awith%20facial%20attribute%20transfer%20from%20a%20given%20reference%20image%20to%20a%20target%0Aportrait%20in%20a%20zero-shot%20manner.%20To%20enable%20high-fidelity%20and%20spatially%0Aconsistent%20attribute%20transfer%20across%20frames%2C%20we%20introduce%20dual%20reference%0Anetworks%20that%20inject%20spatial%20features%20from%20both%20the%20portrait%20and%20attribute%0Aimages%20into%20the%20denoising%20process%20of%20a%20diffusion%20model.%20We%20train%20the%20model%0Ausing%20a%20self-reconstruction%20formulation%2C%20where%20two%20frames%20are%20sampled%20from%20the%0Asame%20portrait%20video%3A%20one%20is%20treated%20as%20the%20attribute%20reference%20and%20the%20other%20as%0Athe%20target%20portrait%2C%20and%20the%20remaining%20frames%20are%20reconstructed%20conditioned%20on%0Athese%20inputs%20and%20their%20corresponding%20masks.%20To%20support%20the%20transfer%20of%0Aattributes%20with%20varying%20spatial%20extent%2C%20we%20propose%20a%20mask%20expansion%20strategy%0Ausing%20keypoint-conditioned%20image%20generation%20for%20training.%20In%20addition%2C%20we%0Afurther%20augment%20the%20attribute%20and%20portrait%20images%20with%20spatial%20and%0Aappearance-level%20transformations%20to%20improve%20robustness%20to%20positional%0Amisalignment%20between%20them.%20These%20strategies%20allow%20the%20model%20to%20effectively%0Ageneralize%20across%20diverse%20attributes%20and%20in-the-wild%20reference%20combinations%2C%0Adespite%20being%20trained%20without%20explicit%20triplet%20supervision.%20Durian%20achieves%0Astate-of-the-art%20performance%20on%20portrait%20animation%20with%20attribute%20transfer%2C%20and%0Anotably%2C%20its%20dual%20reference%20design%20enables%20multi-attribute%20composition%20in%20a%0Asingle%20generation%20pass%20without%20additional%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04434v1&entry.124074799=Read"},
{"title": "Spatially-Enhanced Recurrent Memory for Long-Range Mapless Navigation\n  via End-to-End Reinforcement Learning", "author": "Fan Yang and Per Frivik and David Hoeller and Chen Wang and Cesar Cadena and Marco Hutter", "abstract": "  Recent advancements in robot navigation, particularly with end-to-end\nlearning approaches such as reinforcement learning (RL), have demonstrated\nstrong performance. However, successful navigation still depends on two key\ncapabilities: mapping and planning (explicitly or implicitly). Classical\napproaches rely on explicit mapping pipelines to register egocentric\nobservations into a coherent map. In contrast, end-to-end learning often\nachieves this implicitly -- through recurrent neural networks (RNNs) that fuse\ncurrent and historical observations into a latent space for planning. While\nexisting architectures, such as LSTM and GRU, can capture temporal\ndependencies, our findings reveal a critical limitation: their inability to\neffectively perform spatial memorization. This capability is essential for\nintegrating sequential observations from varying perspectives to build spatial\nrepresentations that support planning. To address this, we propose\nSpatially-Enhanced Recurrent Units (SRUs) -- a simple yet effective\nmodification to existing RNNs -- that enhance spatial memorization. We further\nintroduce an attention-based network architecture integrated with SRUs,\nenabling long-range mapless navigation using a single forward-facing stereo\ncamera. We also employ regularization techniques to facilitate robust\nend-to-end recurrent training via RL. Experimental results show 23.5% overall\nimprovement in long-range navigation compared to existing RNNs. With SRU\nmemory, our method outperforms RL baselines -- one relying on explicit mapping\nand the other on stacked historical observations -- by 29.6% and 105.0%,\nrespectively, across diverse environments requiring long-horizon mapping and\nmemorization. Finally, we address the sim-to-real gap by leveraging large-scale\npretraining on synthetic depth data, enabling zero-shot transfer for deployment\nacross diverse and complex real-world environments.\n", "link": "http://arxiv.org/abs/2506.05997v2", "date": "2025-09-04", "relevancy": 2.3362, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6072}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5723}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatially-Enhanced%20Recurrent%20Memory%20for%20Long-Range%20Mapless%20Navigation%0A%20%20via%20End-to-End%20Reinforcement%20Learning&body=Title%3A%20Spatially-Enhanced%20Recurrent%20Memory%20for%20Long-Range%20Mapless%20Navigation%0A%20%20via%20End-to-End%20Reinforcement%20Learning%0AAuthor%3A%20Fan%20Yang%20and%20Per%20Frivik%20and%20David%20Hoeller%20and%20Chen%20Wang%20and%20Cesar%20Cadena%20and%20Marco%20Hutter%0AAbstract%3A%20%20%20Recent%20advancements%20in%20robot%20navigation%2C%20particularly%20with%20end-to-end%0Alearning%20approaches%20such%20as%20reinforcement%20learning%20%28RL%29%2C%20have%20demonstrated%0Astrong%20performance.%20However%2C%20successful%20navigation%20still%20depends%20on%20two%20key%0Acapabilities%3A%20mapping%20and%20planning%20%28explicitly%20or%20implicitly%29.%20Classical%0Aapproaches%20rely%20on%20explicit%20mapping%20pipelines%20to%20register%20egocentric%0Aobservations%20into%20a%20coherent%20map.%20In%20contrast%2C%20end-to-end%20learning%20often%0Aachieves%20this%20implicitly%20--%20through%20recurrent%20neural%20networks%20%28RNNs%29%20that%20fuse%0Acurrent%20and%20historical%20observations%20into%20a%20latent%20space%20for%20planning.%20While%0Aexisting%20architectures%2C%20such%20as%20LSTM%20and%20GRU%2C%20can%20capture%20temporal%0Adependencies%2C%20our%20findings%20reveal%20a%20critical%20limitation%3A%20their%20inability%20to%0Aeffectively%20perform%20spatial%20memorization.%20This%20capability%20is%20essential%20for%0Aintegrating%20sequential%20observations%20from%20varying%20perspectives%20to%20build%20spatial%0Arepresentations%20that%20support%20planning.%20To%20address%20this%2C%20we%20propose%0ASpatially-Enhanced%20Recurrent%20Units%20%28SRUs%29%20--%20a%20simple%20yet%20effective%0Amodification%20to%20existing%20RNNs%20--%20that%20enhance%20spatial%20memorization.%20We%20further%0Aintroduce%20an%20attention-based%20network%20architecture%20integrated%20with%20SRUs%2C%0Aenabling%20long-range%20mapless%20navigation%20using%20a%20single%20forward-facing%20stereo%0Acamera.%20We%20also%20employ%20regularization%20techniques%20to%20facilitate%20robust%0Aend-to-end%20recurrent%20training%20via%20RL.%20Experimental%20results%20show%2023.5%25%20overall%0Aimprovement%20in%20long-range%20navigation%20compared%20to%20existing%20RNNs.%20With%20SRU%0Amemory%2C%20our%20method%20outperforms%20RL%20baselines%20--%20one%20relying%20on%20explicit%20mapping%0Aand%20the%20other%20on%20stacked%20historical%20observations%20--%20by%2029.6%25%20and%20105.0%25%2C%0Arespectively%2C%20across%20diverse%20environments%20requiring%20long-horizon%20mapping%20and%0Amemorization.%20Finally%2C%20we%20address%20the%20sim-to-real%20gap%20by%20leveraging%20large-scale%0Apretraining%20on%20synthetic%20depth%20data%2C%20enabling%20zero-shot%20transfer%20for%20deployment%0Aacross%20diverse%20and%20complex%20real-world%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05997v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatially-Enhanced%2520Recurrent%2520Memory%2520for%2520Long-Range%2520Mapless%2520Navigation%250A%2520%2520via%2520End-to-End%2520Reinforcement%2520Learning%26entry.906535625%3DFan%2520Yang%2520and%2520Per%2520Frivik%2520and%2520David%2520Hoeller%2520and%2520Chen%2520Wang%2520and%2520Cesar%2520Cadena%2520and%2520Marco%2520Hutter%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520robot%2520navigation%252C%2520particularly%2520with%2520end-to-end%250Alearning%2520approaches%2520such%2520as%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520have%2520demonstrated%250Astrong%2520performance.%2520However%252C%2520successful%2520navigation%2520still%2520depends%2520on%2520two%2520key%250Acapabilities%253A%2520mapping%2520and%2520planning%2520%2528explicitly%2520or%2520implicitly%2529.%2520Classical%250Aapproaches%2520rely%2520on%2520explicit%2520mapping%2520pipelines%2520to%2520register%2520egocentric%250Aobservations%2520into%2520a%2520coherent%2520map.%2520In%2520contrast%252C%2520end-to-end%2520learning%2520often%250Aachieves%2520this%2520implicitly%2520--%2520through%2520recurrent%2520neural%2520networks%2520%2528RNNs%2529%2520that%2520fuse%250Acurrent%2520and%2520historical%2520observations%2520into%2520a%2520latent%2520space%2520for%2520planning.%2520While%250Aexisting%2520architectures%252C%2520such%2520as%2520LSTM%2520and%2520GRU%252C%2520can%2520capture%2520temporal%250Adependencies%252C%2520our%2520findings%2520reveal%2520a%2520critical%2520limitation%253A%2520their%2520inability%2520to%250Aeffectively%2520perform%2520spatial%2520memorization.%2520This%2520capability%2520is%2520essential%2520for%250Aintegrating%2520sequential%2520observations%2520from%2520varying%2520perspectives%2520to%2520build%2520spatial%250Arepresentations%2520that%2520support%2520planning.%2520To%2520address%2520this%252C%2520we%2520propose%250ASpatially-Enhanced%2520Recurrent%2520Units%2520%2528SRUs%2529%2520--%2520a%2520simple%2520yet%2520effective%250Amodification%2520to%2520existing%2520RNNs%2520--%2520that%2520enhance%2520spatial%2520memorization.%2520We%2520further%250Aintroduce%2520an%2520attention-based%2520network%2520architecture%2520integrated%2520with%2520SRUs%252C%250Aenabling%2520long-range%2520mapless%2520navigation%2520using%2520a%2520single%2520forward-facing%2520stereo%250Acamera.%2520We%2520also%2520employ%2520regularization%2520techniques%2520to%2520facilitate%2520robust%250Aend-to-end%2520recurrent%2520training%2520via%2520RL.%2520Experimental%2520results%2520show%252023.5%2525%2520overall%250Aimprovement%2520in%2520long-range%2520navigation%2520compared%2520to%2520existing%2520RNNs.%2520With%2520SRU%250Amemory%252C%2520our%2520method%2520outperforms%2520RL%2520baselines%2520--%2520one%2520relying%2520on%2520explicit%2520mapping%250Aand%2520the%2520other%2520on%2520stacked%2520historical%2520observations%2520--%2520by%252029.6%2525%2520and%2520105.0%2525%252C%250Arespectively%252C%2520across%2520diverse%2520environments%2520requiring%2520long-horizon%2520mapping%2520and%250Amemorization.%2520Finally%252C%2520we%2520address%2520the%2520sim-to-real%2520gap%2520by%2520leveraging%2520large-scale%250Apretraining%2520on%2520synthetic%2520depth%2520data%252C%2520enabling%2520zero-shot%2520transfer%2520for%2520deployment%250Aacross%2520diverse%2520and%2520complex%2520real-world%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05997v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatially-Enhanced%20Recurrent%20Memory%20for%20Long-Range%20Mapless%20Navigation%0A%20%20via%20End-to-End%20Reinforcement%20Learning&entry.906535625=Fan%20Yang%20and%20Per%20Frivik%20and%20David%20Hoeller%20and%20Chen%20Wang%20and%20Cesar%20Cadena%20and%20Marco%20Hutter&entry.1292438233=%20%20Recent%20advancements%20in%20robot%20navigation%2C%20particularly%20with%20end-to-end%0Alearning%20approaches%20such%20as%20reinforcement%20learning%20%28RL%29%2C%20have%20demonstrated%0Astrong%20performance.%20However%2C%20successful%20navigation%20still%20depends%20on%20two%20key%0Acapabilities%3A%20mapping%20and%20planning%20%28explicitly%20or%20implicitly%29.%20Classical%0Aapproaches%20rely%20on%20explicit%20mapping%20pipelines%20to%20register%20egocentric%0Aobservations%20into%20a%20coherent%20map.%20In%20contrast%2C%20end-to-end%20learning%20often%0Aachieves%20this%20implicitly%20--%20through%20recurrent%20neural%20networks%20%28RNNs%29%20that%20fuse%0Acurrent%20and%20historical%20observations%20into%20a%20latent%20space%20for%20planning.%20While%0Aexisting%20architectures%2C%20such%20as%20LSTM%20and%20GRU%2C%20can%20capture%20temporal%0Adependencies%2C%20our%20findings%20reveal%20a%20critical%20limitation%3A%20their%20inability%20to%0Aeffectively%20perform%20spatial%20memorization.%20This%20capability%20is%20essential%20for%0Aintegrating%20sequential%20observations%20from%20varying%20perspectives%20to%20build%20spatial%0Arepresentations%20that%20support%20planning.%20To%20address%20this%2C%20we%20propose%0ASpatially-Enhanced%20Recurrent%20Units%20%28SRUs%29%20--%20a%20simple%20yet%20effective%0Amodification%20to%20existing%20RNNs%20--%20that%20enhance%20spatial%20memorization.%20We%20further%0Aintroduce%20an%20attention-based%20network%20architecture%20integrated%20with%20SRUs%2C%0Aenabling%20long-range%20mapless%20navigation%20using%20a%20single%20forward-facing%20stereo%0Acamera.%20We%20also%20employ%20regularization%20techniques%20to%20facilitate%20robust%0Aend-to-end%20recurrent%20training%20via%20RL.%20Experimental%20results%20show%2023.5%25%20overall%0Aimprovement%20in%20long-range%20navigation%20compared%20to%20existing%20RNNs.%20With%20SRU%0Amemory%2C%20our%20method%20outperforms%20RL%20baselines%20--%20one%20relying%20on%20explicit%20mapping%0Aand%20the%20other%20on%20stacked%20historical%20observations%20--%20by%2029.6%25%20and%20105.0%25%2C%0Arespectively%2C%20across%20diverse%20environments%20requiring%20long-horizon%20mapping%20and%0Amemorization.%20Finally%2C%20we%20address%20the%20sim-to-real%20gap%20by%20leveraging%20large-scale%0Apretraining%20on%20synthetic%20depth%20data%2C%20enabling%20zero-shot%20transfer%20for%20deployment%0Aacross%20diverse%20and%20complex%20real-world%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05997v2&entry.124074799=Read"},
{"title": "Understanding Space Is Rocket Science -- Only Top Reasoning Models Can\n  Solve Spatial Understanding Tasks", "author": "Nils Hoehing and Mayug Maniparambil and Ellen Rushe and Noel E. O'Connor and Anthony Ventresque", "abstract": "  We propose RocketScience, an open-source contrastive VLM benchmark that tests\nfor spatial relation understanding. It is comprised of entirely new real-world\nimage-text pairs covering mostly relative spatial understanding and the order\nof objects. The benchmark is designed to be very easy for humans and hard for\nthe current generation of VLMs, and this is empirically verified. Our results\nshow a striking lack of spatial relation understanding in open source and\nfrontier commercial VLMs and a surprisingly high performance of reasoning\nmodels. Additionally, we perform a disentanglement analysis to separate the\ncontributions of object localization and spatial reasoning in\nchain-of-thought-based models and find that the performance on the benchmark is\nbottlenecked by spatial reasoning and not object localization capabilities. We\nrelease the dataset with a CC-BY-4.0 license and make the evaluation code\navailable at: https://github.com/nilshoehing/rocketscience\n", "link": "http://arxiv.org/abs/2509.02175v2", "date": "2025-09-04", "relevancy": 2.3079, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5877}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5877}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5236}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Space%20Is%20Rocket%20Science%20--%20Only%20Top%20Reasoning%20Models%20Can%0A%20%20Solve%20Spatial%20Understanding%20Tasks&body=Title%3A%20Understanding%20Space%20Is%20Rocket%20Science%20--%20Only%20Top%20Reasoning%20Models%20Can%0A%20%20Solve%20Spatial%20Understanding%20Tasks%0AAuthor%3A%20Nils%20Hoehing%20and%20Mayug%20Maniparambil%20and%20Ellen%20Rushe%20and%20Noel%20E.%20O%27Connor%20and%20Anthony%20Ventresque%0AAbstract%3A%20%20%20We%20propose%20RocketScience%2C%20an%20open-source%20contrastive%20VLM%20benchmark%20that%20tests%0Afor%20spatial%20relation%20understanding.%20It%20is%20comprised%20of%20entirely%20new%20real-world%0Aimage-text%20pairs%20covering%20mostly%20relative%20spatial%20understanding%20and%20the%20order%0Aof%20objects.%20The%20benchmark%20is%20designed%20to%20be%20very%20easy%20for%20humans%20and%20hard%20for%0Athe%20current%20generation%20of%20VLMs%2C%20and%20this%20is%20empirically%20verified.%20Our%20results%0Ashow%20a%20striking%20lack%20of%20spatial%20relation%20understanding%20in%20open%20source%20and%0Afrontier%20commercial%20VLMs%20and%20a%20surprisingly%20high%20performance%20of%20reasoning%0Amodels.%20Additionally%2C%20we%20perform%20a%20disentanglement%20analysis%20to%20separate%20the%0Acontributions%20of%20object%20localization%20and%20spatial%20reasoning%20in%0Achain-of-thought-based%20models%20and%20find%20that%20the%20performance%20on%20the%20benchmark%20is%0Abottlenecked%20by%20spatial%20reasoning%20and%20not%20object%20localization%20capabilities.%20We%0Arelease%20the%20dataset%20with%20a%20CC-BY-4.0%20license%20and%20make%20the%20evaluation%20code%0Aavailable%20at%3A%20https%3A//github.com/nilshoehing/rocketscience%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.02175v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Space%2520Is%2520Rocket%2520Science%2520--%2520Only%2520Top%2520Reasoning%2520Models%2520Can%250A%2520%2520Solve%2520Spatial%2520Understanding%2520Tasks%26entry.906535625%3DNils%2520Hoehing%2520and%2520Mayug%2520Maniparambil%2520and%2520Ellen%2520Rushe%2520and%2520Noel%2520E.%2520O%2527Connor%2520and%2520Anthony%2520Ventresque%26entry.1292438233%3D%2520%2520We%2520propose%2520RocketScience%252C%2520an%2520open-source%2520contrastive%2520VLM%2520benchmark%2520that%2520tests%250Afor%2520spatial%2520relation%2520understanding.%2520It%2520is%2520comprised%2520of%2520entirely%2520new%2520real-world%250Aimage-text%2520pairs%2520covering%2520mostly%2520relative%2520spatial%2520understanding%2520and%2520the%2520order%250Aof%2520objects.%2520The%2520benchmark%2520is%2520designed%2520to%2520be%2520very%2520easy%2520for%2520humans%2520and%2520hard%2520for%250Athe%2520current%2520generation%2520of%2520VLMs%252C%2520and%2520this%2520is%2520empirically%2520verified.%2520Our%2520results%250Ashow%2520a%2520striking%2520lack%2520of%2520spatial%2520relation%2520understanding%2520in%2520open%2520source%2520and%250Afrontier%2520commercial%2520VLMs%2520and%2520a%2520surprisingly%2520high%2520performance%2520of%2520reasoning%250Amodels.%2520Additionally%252C%2520we%2520perform%2520a%2520disentanglement%2520analysis%2520to%2520separate%2520the%250Acontributions%2520of%2520object%2520localization%2520and%2520spatial%2520reasoning%2520in%250Achain-of-thought-based%2520models%2520and%2520find%2520that%2520the%2520performance%2520on%2520the%2520benchmark%2520is%250Abottlenecked%2520by%2520spatial%2520reasoning%2520and%2520not%2520object%2520localization%2520capabilities.%2520We%250Arelease%2520the%2520dataset%2520with%2520a%2520CC-BY-4.0%2520license%2520and%2520make%2520the%2520evaluation%2520code%250Aavailable%2520at%253A%2520https%253A//github.com/nilshoehing/rocketscience%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.02175v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Space%20Is%20Rocket%20Science%20--%20Only%20Top%20Reasoning%20Models%20Can%0A%20%20Solve%20Spatial%20Understanding%20Tasks&entry.906535625=Nils%20Hoehing%20and%20Mayug%20Maniparambil%20and%20Ellen%20Rushe%20and%20Noel%20E.%20O%27Connor%20and%20Anthony%20Ventresque&entry.1292438233=%20%20We%20propose%20RocketScience%2C%20an%20open-source%20contrastive%20VLM%20benchmark%20that%20tests%0Afor%20spatial%20relation%20understanding.%20It%20is%20comprised%20of%20entirely%20new%20real-world%0Aimage-text%20pairs%20covering%20mostly%20relative%20spatial%20understanding%20and%20the%20order%0Aof%20objects.%20The%20benchmark%20is%20designed%20to%20be%20very%20easy%20for%20humans%20and%20hard%20for%0Athe%20current%20generation%20of%20VLMs%2C%20and%20this%20is%20empirically%20verified.%20Our%20results%0Ashow%20a%20striking%20lack%20of%20spatial%20relation%20understanding%20in%20open%20source%20and%0Afrontier%20commercial%20VLMs%20and%20a%20surprisingly%20high%20performance%20of%20reasoning%0Amodels.%20Additionally%2C%20we%20perform%20a%20disentanglement%20analysis%20to%20separate%20the%0Acontributions%20of%20object%20localization%20and%20spatial%20reasoning%20in%0Achain-of-thought-based%20models%20and%20find%20that%20the%20performance%20on%20the%20benchmark%20is%0Abottlenecked%20by%20spatial%20reasoning%20and%20not%20object%20localization%20capabilities.%20We%0Arelease%20the%20dataset%20with%20a%20CC-BY-4.0%20license%20and%20make%20the%20evaluation%20code%0Aavailable%20at%3A%20https%3A//github.com/nilshoehing/rocketscience%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.02175v2&entry.124074799=Read"},
{"title": "PARCO: Phoneme-Augmented Robust Contextual ASR via Contrastive Entity\n  Disambiguation", "author": "Jiajun He and Naoki Sawada and Koichi Miyazaki and Tomoki Toda", "abstract": "  Automatic speech recognition (ASR) systems struggle with domain-specific\nnamed entities, especially homophones. Contextual ASR improves recognition but\noften fails to capture fine-grained phoneme variations due to limited entity\ndiversity. Moreover, prior methods treat entities as independent tokens,\nleading to incomplete multi-token biasing. To address these issues, we propose\nPhoneme-Augmented Robust Contextual ASR via COntrastive entity disambiguation\n(PARCO), which integrates phoneme-aware encoding, contrastive entity\ndisambiguation, entity-level supervision, and hierarchical entity filtering.\nThese components enhance phonetic discrimination, ensure complete entity\nretrieval, and reduce false positives under uncertainty. Experiments show that\nPARCO achieves CER of 4.22% on Chinese AISHELL-1 and WER of 11.14% on English\nDATA2 under 1,000 distractors, significantly outperforming baselines. PARCO\nalso demonstrates robust gains on out-of-domain datasets like THCHS-30 and\nLibriSpeech.\n", "link": "http://arxiv.org/abs/2509.04357v1", "date": "2025-09-04", "relevancy": 2.264, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4684}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4684}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PARCO%3A%20Phoneme-Augmented%20Robust%20Contextual%20ASR%20via%20Contrastive%20Entity%0A%20%20Disambiguation&body=Title%3A%20PARCO%3A%20Phoneme-Augmented%20Robust%20Contextual%20ASR%20via%20Contrastive%20Entity%0A%20%20Disambiguation%0AAuthor%3A%20Jiajun%20He%20and%20Naoki%20Sawada%20and%20Koichi%20Miyazaki%20and%20Tomoki%20Toda%0AAbstract%3A%20%20%20Automatic%20speech%20recognition%20%28ASR%29%20systems%20struggle%20with%20domain-specific%0Anamed%20entities%2C%20especially%20homophones.%20Contextual%20ASR%20improves%20recognition%20but%0Aoften%20fails%20to%20capture%20fine-grained%20phoneme%20variations%20due%20to%20limited%20entity%0Adiversity.%20Moreover%2C%20prior%20methods%20treat%20entities%20as%20independent%20tokens%2C%0Aleading%20to%20incomplete%20multi-token%20biasing.%20To%20address%20these%20issues%2C%20we%20propose%0APhoneme-Augmented%20Robust%20Contextual%20ASR%20via%20COntrastive%20entity%20disambiguation%0A%28PARCO%29%2C%20which%20integrates%20phoneme-aware%20encoding%2C%20contrastive%20entity%0Adisambiguation%2C%20entity-level%20supervision%2C%20and%20hierarchical%20entity%20filtering.%0AThese%20components%20enhance%20phonetic%20discrimination%2C%20ensure%20complete%20entity%0Aretrieval%2C%20and%20reduce%20false%20positives%20under%20uncertainty.%20Experiments%20show%20that%0APARCO%20achieves%20CER%20of%204.22%25%20on%20Chinese%20AISHELL-1%20and%20WER%20of%2011.14%25%20on%20English%0ADATA2%20under%201%2C000%20distractors%2C%20significantly%20outperforming%20baselines.%20PARCO%0Aalso%20demonstrates%20robust%20gains%20on%20out-of-domain%20datasets%20like%20THCHS-30%20and%0ALibriSpeech.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04357v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPARCO%253A%2520Phoneme-Augmented%2520Robust%2520Contextual%2520ASR%2520via%2520Contrastive%2520Entity%250A%2520%2520Disambiguation%26entry.906535625%3DJiajun%2520He%2520and%2520Naoki%2520Sawada%2520and%2520Koichi%2520Miyazaki%2520and%2520Tomoki%2520Toda%26entry.1292438233%3D%2520%2520Automatic%2520speech%2520recognition%2520%2528ASR%2529%2520systems%2520struggle%2520with%2520domain-specific%250Anamed%2520entities%252C%2520especially%2520homophones.%2520Contextual%2520ASR%2520improves%2520recognition%2520but%250Aoften%2520fails%2520to%2520capture%2520fine-grained%2520phoneme%2520variations%2520due%2520to%2520limited%2520entity%250Adiversity.%2520Moreover%252C%2520prior%2520methods%2520treat%2520entities%2520as%2520independent%2520tokens%252C%250Aleading%2520to%2520incomplete%2520multi-token%2520biasing.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%250APhoneme-Augmented%2520Robust%2520Contextual%2520ASR%2520via%2520COntrastive%2520entity%2520disambiguation%250A%2528PARCO%2529%252C%2520which%2520integrates%2520phoneme-aware%2520encoding%252C%2520contrastive%2520entity%250Adisambiguation%252C%2520entity-level%2520supervision%252C%2520and%2520hierarchical%2520entity%2520filtering.%250AThese%2520components%2520enhance%2520phonetic%2520discrimination%252C%2520ensure%2520complete%2520entity%250Aretrieval%252C%2520and%2520reduce%2520false%2520positives%2520under%2520uncertainty.%2520Experiments%2520show%2520that%250APARCO%2520achieves%2520CER%2520of%25204.22%2525%2520on%2520Chinese%2520AISHELL-1%2520and%2520WER%2520of%252011.14%2525%2520on%2520English%250ADATA2%2520under%25201%252C000%2520distractors%252C%2520significantly%2520outperforming%2520baselines.%2520PARCO%250Aalso%2520demonstrates%2520robust%2520gains%2520on%2520out-of-domain%2520datasets%2520like%2520THCHS-30%2520and%250ALibriSpeech.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04357v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PARCO%3A%20Phoneme-Augmented%20Robust%20Contextual%20ASR%20via%20Contrastive%20Entity%0A%20%20Disambiguation&entry.906535625=Jiajun%20He%20and%20Naoki%20Sawada%20and%20Koichi%20Miyazaki%20and%20Tomoki%20Toda&entry.1292438233=%20%20Automatic%20speech%20recognition%20%28ASR%29%20systems%20struggle%20with%20domain-specific%0Anamed%20entities%2C%20especially%20homophones.%20Contextual%20ASR%20improves%20recognition%20but%0Aoften%20fails%20to%20capture%20fine-grained%20phoneme%20variations%20due%20to%20limited%20entity%0Adiversity.%20Moreover%2C%20prior%20methods%20treat%20entities%20as%20independent%20tokens%2C%0Aleading%20to%20incomplete%20multi-token%20biasing.%20To%20address%20these%20issues%2C%20we%20propose%0APhoneme-Augmented%20Robust%20Contextual%20ASR%20via%20COntrastive%20entity%20disambiguation%0A%28PARCO%29%2C%20which%20integrates%20phoneme-aware%20encoding%2C%20contrastive%20entity%0Adisambiguation%2C%20entity-level%20supervision%2C%20and%20hierarchical%20entity%20filtering.%0AThese%20components%20enhance%20phonetic%20discrimination%2C%20ensure%20complete%20entity%0Aretrieval%2C%20and%20reduce%20false%20positives%20under%20uncertainty.%20Experiments%20show%20that%0APARCO%20achieves%20CER%20of%204.22%25%20on%20Chinese%20AISHELL-1%20and%20WER%20of%2011.14%25%20on%20English%0ADATA2%20under%201%2C000%20distractors%2C%20significantly%20outperforming%20baselines.%20PARCO%0Aalso%20demonstrates%20robust%20gains%20on%20out-of-domain%20datasets%20like%20THCHS-30%20and%0ALibriSpeech.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04357v1&entry.124074799=Read"},
{"title": "Noisy Label Refinement with Semantically Reliable Synthetic Images", "author": "Yingxuan Li and Jiafeng Mao and Yusuke Matsui", "abstract": "  Semantic noise in image classification datasets, where visually similar\ncategories are frequently mislabeled, poses a significant challenge to\nconventional supervised learning approaches. In this paper, we explore the\npotential of using synthetic images generated by advanced text-to-image models\nto address this issue. Although these high-quality synthetic images come with\nreliable labels, their direct application in training is limited by domain gaps\nand diversity constraints. Unlike conventional approaches, we propose a novel\nmethod that leverages synthetic images as reliable reference points to identify\nand correct mislabeled samples in noisy datasets. Extensive experiments across\nmultiple benchmark datasets show that our approach significantly improves\nclassification accuracy under various noise conditions, especially in\nchallenging scenarios with semantic label noise. Additionally, since our method\nis orthogonal to existing noise-robust learning techniques, when combined with\nstate-of-the-art noise-robust training methods, it achieves superior\nperformance, improving accuracy by 30% on CIFAR-10 and by 11% on CIFAR-100\nunder 70% semantic noise, and by 24% on ImageNet-100 under real-world noise\nconditions.\n", "link": "http://arxiv.org/abs/2509.04298v1", "date": "2025-09-04", "relevancy": 2.2637, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6061}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5661}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Noisy%20Label%20Refinement%20with%20Semantically%20Reliable%20Synthetic%20Images&body=Title%3A%20Noisy%20Label%20Refinement%20with%20Semantically%20Reliable%20Synthetic%20Images%0AAuthor%3A%20Yingxuan%20Li%20and%20Jiafeng%20Mao%20and%20Yusuke%20Matsui%0AAbstract%3A%20%20%20Semantic%20noise%20in%20image%20classification%20datasets%2C%20where%20visually%20similar%0Acategories%20are%20frequently%20mislabeled%2C%20poses%20a%20significant%20challenge%20to%0Aconventional%20supervised%20learning%20approaches.%20In%20this%20paper%2C%20we%20explore%20the%0Apotential%20of%20using%20synthetic%20images%20generated%20by%20advanced%20text-to-image%20models%0Ato%20address%20this%20issue.%20Although%20these%20high-quality%20synthetic%20images%20come%20with%0Areliable%20labels%2C%20their%20direct%20application%20in%20training%20is%20limited%20by%20domain%20gaps%0Aand%20diversity%20constraints.%20Unlike%20conventional%20approaches%2C%20we%20propose%20a%20novel%0Amethod%20that%20leverages%20synthetic%20images%20as%20reliable%20reference%20points%20to%20identify%0Aand%20correct%20mislabeled%20samples%20in%20noisy%20datasets.%20Extensive%20experiments%20across%0Amultiple%20benchmark%20datasets%20show%20that%20our%20approach%20significantly%20improves%0Aclassification%20accuracy%20under%20various%20noise%20conditions%2C%20especially%20in%0Achallenging%20scenarios%20with%20semantic%20label%20noise.%20Additionally%2C%20since%20our%20method%0Ais%20orthogonal%20to%20existing%20noise-robust%20learning%20techniques%2C%20when%20combined%20with%0Astate-of-the-art%20noise-robust%20training%20methods%2C%20it%20achieves%20superior%0Aperformance%2C%20improving%20accuracy%20by%2030%25%20on%20CIFAR-10%20and%20by%2011%25%20on%20CIFAR-100%0Aunder%2070%25%20semantic%20noise%2C%20and%20by%2024%25%20on%20ImageNet-100%20under%20real-world%20noise%0Aconditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04298v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoisy%2520Label%2520Refinement%2520with%2520Semantically%2520Reliable%2520Synthetic%2520Images%26entry.906535625%3DYingxuan%2520Li%2520and%2520Jiafeng%2520Mao%2520and%2520Yusuke%2520Matsui%26entry.1292438233%3D%2520%2520Semantic%2520noise%2520in%2520image%2520classification%2520datasets%252C%2520where%2520visually%2520similar%250Acategories%2520are%2520frequently%2520mislabeled%252C%2520poses%2520a%2520significant%2520challenge%2520to%250Aconventional%2520supervised%2520learning%2520approaches.%2520In%2520this%2520paper%252C%2520we%2520explore%2520the%250Apotential%2520of%2520using%2520synthetic%2520images%2520generated%2520by%2520advanced%2520text-to-image%2520models%250Ato%2520address%2520this%2520issue.%2520Although%2520these%2520high-quality%2520synthetic%2520images%2520come%2520with%250Areliable%2520labels%252C%2520their%2520direct%2520application%2520in%2520training%2520is%2520limited%2520by%2520domain%2520gaps%250Aand%2520diversity%2520constraints.%2520Unlike%2520conventional%2520approaches%252C%2520we%2520propose%2520a%2520novel%250Amethod%2520that%2520leverages%2520synthetic%2520images%2520as%2520reliable%2520reference%2520points%2520to%2520identify%250Aand%2520correct%2520mislabeled%2520samples%2520in%2520noisy%2520datasets.%2520Extensive%2520experiments%2520across%250Amultiple%2520benchmark%2520datasets%2520show%2520that%2520our%2520approach%2520significantly%2520improves%250Aclassification%2520accuracy%2520under%2520various%2520noise%2520conditions%252C%2520especially%2520in%250Achallenging%2520scenarios%2520with%2520semantic%2520label%2520noise.%2520Additionally%252C%2520since%2520our%2520method%250Ais%2520orthogonal%2520to%2520existing%2520noise-robust%2520learning%2520techniques%252C%2520when%2520combined%2520with%250Astate-of-the-art%2520noise-robust%2520training%2520methods%252C%2520it%2520achieves%2520superior%250Aperformance%252C%2520improving%2520accuracy%2520by%252030%2525%2520on%2520CIFAR-10%2520and%2520by%252011%2525%2520on%2520CIFAR-100%250Aunder%252070%2525%2520semantic%2520noise%252C%2520and%2520by%252024%2525%2520on%2520ImageNet-100%2520under%2520real-world%2520noise%250Aconditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04298v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Noisy%20Label%20Refinement%20with%20Semantically%20Reliable%20Synthetic%20Images&entry.906535625=Yingxuan%20Li%20and%20Jiafeng%20Mao%20and%20Yusuke%20Matsui&entry.1292438233=%20%20Semantic%20noise%20in%20image%20classification%20datasets%2C%20where%20visually%20similar%0Acategories%20are%20frequently%20mislabeled%2C%20poses%20a%20significant%20challenge%20to%0Aconventional%20supervised%20learning%20approaches.%20In%20this%20paper%2C%20we%20explore%20the%0Apotential%20of%20using%20synthetic%20images%20generated%20by%20advanced%20text-to-image%20models%0Ato%20address%20this%20issue.%20Although%20these%20high-quality%20synthetic%20images%20come%20with%0Areliable%20labels%2C%20their%20direct%20application%20in%20training%20is%20limited%20by%20domain%20gaps%0Aand%20diversity%20constraints.%20Unlike%20conventional%20approaches%2C%20we%20propose%20a%20novel%0Amethod%20that%20leverages%20synthetic%20images%20as%20reliable%20reference%20points%20to%20identify%0Aand%20correct%20mislabeled%20samples%20in%20noisy%20datasets.%20Extensive%20experiments%20across%0Amultiple%20benchmark%20datasets%20show%20that%20our%20approach%20significantly%20improves%0Aclassification%20accuracy%20under%20various%20noise%20conditions%2C%20especially%20in%0Achallenging%20scenarios%20with%20semantic%20label%20noise.%20Additionally%2C%20since%20our%20method%0Ais%20orthogonal%20to%20existing%20noise-robust%20learning%20techniques%2C%20when%20combined%20with%0Astate-of-the-art%20noise-robust%20training%20methods%2C%20it%20achieves%20superior%0Aperformance%2C%20improving%20accuracy%20by%2030%25%20on%20CIFAR-10%20and%20by%2011%25%20on%20CIFAR-100%0Aunder%2070%25%20semantic%20noise%2C%20and%20by%2024%25%20on%20ImageNet-100%20under%20real-world%20noise%0Aconditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04298v1&entry.124074799=Read"},
{"title": "TEn-CATS: Text-Enriched Audio-Visual Video Parsing with Multi-Scale\n  Category-Aware Temporal Graph", "author": "Yaru Chen and Faegheh Sardari and Peiliang Zhang and Ruohao Guo and Yang Xiang and Zhenbo Li and Wenwu Wang", "abstract": "  Audio-Visual Video Parsing (AVVP) task aims to identify event categories and\ntheir occurrence times in a given video with weakly supervised labels. Existing\nmethods typically fall into two categories: (i) designing enhanced\narchitectures based on attention mechanism for better temporal modeling, and\n(ii) generating richer pseudo-labels to compensate for the absence of\nframe-level annotations. However, the first type methods treat noisy\nsegment-level pseudo labels as reliable supervision and the second type methods\nlet indiscriminate attention spread them across all frames, the initial errors\nare repeatedly amplified during training. To address this issue, we propose a\nmethod that combines the Bi-Directional Text Fusion (BiT) module and\nCategory-Aware Temporal Graph (CATS) module. Specifically, we integrate the\nstrengths and complementarity of the two previous research directions. We first\nperform semantic injection and dynamic calibration on audio and visual modality\nfeatures through the BiT module, to locate and purify cleaner and richer\nsemantic cues. Then, we leverage the CATS module for semantic propagation and\nconnection to enable precise semantic information dissemination across time.\nExperimental results demonstrate that our proposed method achieves\nstate-of-the-art (SOTA) performance in multiple key indicators on two benchmark\ndatasets, LLP and UnAV-100.\n", "link": "http://arxiv.org/abs/2509.04086v1", "date": "2025-09-04", "relevancy": 2.2616, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5856}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.554}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TEn-CATS%3A%20Text-Enriched%20Audio-Visual%20Video%20Parsing%20with%20Multi-Scale%0A%20%20Category-Aware%20Temporal%20Graph&body=Title%3A%20TEn-CATS%3A%20Text-Enriched%20Audio-Visual%20Video%20Parsing%20with%20Multi-Scale%0A%20%20Category-Aware%20Temporal%20Graph%0AAuthor%3A%20Yaru%20Chen%20and%20Faegheh%20Sardari%20and%20Peiliang%20Zhang%20and%20Ruohao%20Guo%20and%20Yang%20Xiang%20and%20Zhenbo%20Li%20and%20Wenwu%20Wang%0AAbstract%3A%20%20%20Audio-Visual%20Video%20Parsing%20%28AVVP%29%20task%20aims%20to%20identify%20event%20categories%20and%0Atheir%20occurrence%20times%20in%20a%20given%20video%20with%20weakly%20supervised%20labels.%20Existing%0Amethods%20typically%20fall%20into%20two%20categories%3A%20%28i%29%20designing%20enhanced%0Aarchitectures%20based%20on%20attention%20mechanism%20for%20better%20temporal%20modeling%2C%20and%0A%28ii%29%20generating%20richer%20pseudo-labels%20to%20compensate%20for%20the%20absence%20of%0Aframe-level%20annotations.%20However%2C%20the%20first%20type%20methods%20treat%20noisy%0Asegment-level%20pseudo%20labels%20as%20reliable%20supervision%20and%20the%20second%20type%20methods%0Alet%20indiscriminate%20attention%20spread%20them%20across%20all%20frames%2C%20the%20initial%20errors%0Aare%20repeatedly%20amplified%20during%20training.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Amethod%20that%20combines%20the%20Bi-Directional%20Text%20Fusion%20%28BiT%29%20module%20and%0ACategory-Aware%20Temporal%20Graph%20%28CATS%29%20module.%20Specifically%2C%20we%20integrate%20the%0Astrengths%20and%20complementarity%20of%20the%20two%20previous%20research%20directions.%20We%20first%0Aperform%20semantic%20injection%20and%20dynamic%20calibration%20on%20audio%20and%20visual%20modality%0Afeatures%20through%20the%20BiT%20module%2C%20to%20locate%20and%20purify%20cleaner%20and%20richer%0Asemantic%20cues.%20Then%2C%20we%20leverage%20the%20CATS%20module%20for%20semantic%20propagation%20and%0Aconnection%20to%20enable%20precise%20semantic%20information%20dissemination%20across%20time.%0AExperimental%20results%20demonstrate%20that%20our%20proposed%20method%20achieves%0Astate-of-the-art%20%28SOTA%29%20performance%20in%20multiple%20key%20indicators%20on%20two%20benchmark%0Adatasets%2C%20LLP%20and%20UnAV-100.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04086v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTEn-CATS%253A%2520Text-Enriched%2520Audio-Visual%2520Video%2520Parsing%2520with%2520Multi-Scale%250A%2520%2520Category-Aware%2520Temporal%2520Graph%26entry.906535625%3DYaru%2520Chen%2520and%2520Faegheh%2520Sardari%2520and%2520Peiliang%2520Zhang%2520and%2520Ruohao%2520Guo%2520and%2520Yang%2520Xiang%2520and%2520Zhenbo%2520Li%2520and%2520Wenwu%2520Wang%26entry.1292438233%3D%2520%2520Audio-Visual%2520Video%2520Parsing%2520%2528AVVP%2529%2520task%2520aims%2520to%2520identify%2520event%2520categories%2520and%250Atheir%2520occurrence%2520times%2520in%2520a%2520given%2520video%2520with%2520weakly%2520supervised%2520labels.%2520Existing%250Amethods%2520typically%2520fall%2520into%2520two%2520categories%253A%2520%2528i%2529%2520designing%2520enhanced%250Aarchitectures%2520based%2520on%2520attention%2520mechanism%2520for%2520better%2520temporal%2520modeling%252C%2520and%250A%2528ii%2529%2520generating%2520richer%2520pseudo-labels%2520to%2520compensate%2520for%2520the%2520absence%2520of%250Aframe-level%2520annotations.%2520However%252C%2520the%2520first%2520type%2520methods%2520treat%2520noisy%250Asegment-level%2520pseudo%2520labels%2520as%2520reliable%2520supervision%2520and%2520the%2520second%2520type%2520methods%250Alet%2520indiscriminate%2520attention%2520spread%2520them%2520across%2520all%2520frames%252C%2520the%2520initial%2520errors%250Aare%2520repeatedly%2520amplified%2520during%2520training.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%250Amethod%2520that%2520combines%2520the%2520Bi-Directional%2520Text%2520Fusion%2520%2528BiT%2529%2520module%2520and%250ACategory-Aware%2520Temporal%2520Graph%2520%2528CATS%2529%2520module.%2520Specifically%252C%2520we%2520integrate%2520the%250Astrengths%2520and%2520complementarity%2520of%2520the%2520two%2520previous%2520research%2520directions.%2520We%2520first%250Aperform%2520semantic%2520injection%2520and%2520dynamic%2520calibration%2520on%2520audio%2520and%2520visual%2520modality%250Afeatures%2520through%2520the%2520BiT%2520module%252C%2520to%2520locate%2520and%2520purify%2520cleaner%2520and%2520richer%250Asemantic%2520cues.%2520Then%252C%2520we%2520leverage%2520the%2520CATS%2520module%2520for%2520semantic%2520propagation%2520and%250Aconnection%2520to%2520enable%2520precise%2520semantic%2520information%2520dissemination%2520across%2520time.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520proposed%2520method%2520achieves%250Astate-of-the-art%2520%2528SOTA%2529%2520performance%2520in%2520multiple%2520key%2520indicators%2520on%2520two%2520benchmark%250Adatasets%252C%2520LLP%2520and%2520UnAV-100.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04086v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TEn-CATS%3A%20Text-Enriched%20Audio-Visual%20Video%20Parsing%20with%20Multi-Scale%0A%20%20Category-Aware%20Temporal%20Graph&entry.906535625=Yaru%20Chen%20and%20Faegheh%20Sardari%20and%20Peiliang%20Zhang%20and%20Ruohao%20Guo%20and%20Yang%20Xiang%20and%20Zhenbo%20Li%20and%20Wenwu%20Wang&entry.1292438233=%20%20Audio-Visual%20Video%20Parsing%20%28AVVP%29%20task%20aims%20to%20identify%20event%20categories%20and%0Atheir%20occurrence%20times%20in%20a%20given%20video%20with%20weakly%20supervised%20labels.%20Existing%0Amethods%20typically%20fall%20into%20two%20categories%3A%20%28i%29%20designing%20enhanced%0Aarchitectures%20based%20on%20attention%20mechanism%20for%20better%20temporal%20modeling%2C%20and%0A%28ii%29%20generating%20richer%20pseudo-labels%20to%20compensate%20for%20the%20absence%20of%0Aframe-level%20annotations.%20However%2C%20the%20first%20type%20methods%20treat%20noisy%0Asegment-level%20pseudo%20labels%20as%20reliable%20supervision%20and%20the%20second%20type%20methods%0Alet%20indiscriminate%20attention%20spread%20them%20across%20all%20frames%2C%20the%20initial%20errors%0Aare%20repeatedly%20amplified%20during%20training.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Amethod%20that%20combines%20the%20Bi-Directional%20Text%20Fusion%20%28BiT%29%20module%20and%0ACategory-Aware%20Temporal%20Graph%20%28CATS%29%20module.%20Specifically%2C%20we%20integrate%20the%0Astrengths%20and%20complementarity%20of%20the%20two%20previous%20research%20directions.%20We%20first%0Aperform%20semantic%20injection%20and%20dynamic%20calibration%20on%20audio%20and%20visual%20modality%0Afeatures%20through%20the%20BiT%20module%2C%20to%20locate%20and%20purify%20cleaner%20and%20richer%0Asemantic%20cues.%20Then%2C%20we%20leverage%20the%20CATS%20module%20for%20semantic%20propagation%20and%0Aconnection%20to%20enable%20precise%20semantic%20information%20dissemination%20across%20time.%0AExperimental%20results%20demonstrate%20that%20our%20proposed%20method%20achieves%0Astate-of-the-art%20%28SOTA%29%20performance%20in%20multiple%20key%20indicators%20on%20two%20benchmark%0Adatasets%2C%20LLP%20and%20UnAV-100.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04086v1&entry.124074799=Read"},
{"title": "PAK-UCB Contextual Bandit: An Online Learning Approach to Prompt-Aware\n  Selection of Generative Models and LLMs", "author": "Xiaoyan Hu and Ho-fung Leung and Farzan Farnia", "abstract": "  Selecting a sample generation scheme from multiple prompt-based generative\nmodels, including large language models (LLMs) and prompt-guided image and\nvideo generation models, is typically addressed by choosing the model that\nmaximizes an averaged evaluation score. However, this score-based selection\noverlooks the possibility that different models achieve the best generation\nperformance for different types of text prompts. An online identification of\nthe best generation model for various input prompts can reduce the costs\nassociated with querying sub-optimal models. In this work, we explore the\npossibility of varying rankings of text-based generative models for different\ntext prompts and propose an online learning framework to predict the best data\ngeneration model for a given input prompt. The proposed PAK-UCB algorithm\naddresses a contextual bandit (CB) setting with shared context variables across\nthe arms, utilizing the generated data to update kernel-based functions that\npredict the score of each model available for unseen text prompts.\nAdditionally, we leverage random Fourier features (RFF) to accelerate the\nonline learning process of PAK-UCB. Our numerical experiments on real and\nsimulated text-to-image and image-to-text generative models show that RFF-UCB\nperforms successfully in identifying the best generation model across different\nsample types. The code is available at:\ngithub.com/yannxiaoyanhu/dgm-online-select.\n", "link": "http://arxiv.org/abs/2410.13287v6", "date": "2025-09-04", "relevancy": 2.2612, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5771}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5582}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAK-UCB%20Contextual%20Bandit%3A%20An%20Online%20Learning%20Approach%20to%20Prompt-Aware%0A%20%20Selection%20of%20Generative%20Models%20and%20LLMs&body=Title%3A%20PAK-UCB%20Contextual%20Bandit%3A%20An%20Online%20Learning%20Approach%20to%20Prompt-Aware%0A%20%20Selection%20of%20Generative%20Models%20and%20LLMs%0AAuthor%3A%20Xiaoyan%20Hu%20and%20Ho-fung%20Leung%20and%20Farzan%20Farnia%0AAbstract%3A%20%20%20Selecting%20a%20sample%20generation%20scheme%20from%20multiple%20prompt-based%20generative%0Amodels%2C%20including%20large%20language%20models%20%28LLMs%29%20and%20prompt-guided%20image%20and%0Avideo%20generation%20models%2C%20is%20typically%20addressed%20by%20choosing%20the%20model%20that%0Amaximizes%20an%20averaged%20evaluation%20score.%20However%2C%20this%20score-based%20selection%0Aoverlooks%20the%20possibility%20that%20different%20models%20achieve%20the%20best%20generation%0Aperformance%20for%20different%20types%20of%20text%20prompts.%20An%20online%20identification%20of%0Athe%20best%20generation%20model%20for%20various%20input%20prompts%20can%20reduce%20the%20costs%0Aassociated%20with%20querying%20sub-optimal%20models.%20In%20this%20work%2C%20we%20explore%20the%0Apossibility%20of%20varying%20rankings%20of%20text-based%20generative%20models%20for%20different%0Atext%20prompts%20and%20propose%20an%20online%20learning%20framework%20to%20predict%20the%20best%20data%0Ageneration%20model%20for%20a%20given%20input%20prompt.%20The%20proposed%20PAK-UCB%20algorithm%0Aaddresses%20a%20contextual%20bandit%20%28CB%29%20setting%20with%20shared%20context%20variables%20across%0Athe%20arms%2C%20utilizing%20the%20generated%20data%20to%20update%20kernel-based%20functions%20that%0Apredict%20the%20score%20of%20each%20model%20available%20for%20unseen%20text%20prompts.%0AAdditionally%2C%20we%20leverage%20random%20Fourier%20features%20%28RFF%29%20to%20accelerate%20the%0Aonline%20learning%20process%20of%20PAK-UCB.%20Our%20numerical%20experiments%20on%20real%20and%0Asimulated%20text-to-image%20and%20image-to-text%20generative%20models%20show%20that%20RFF-UCB%0Aperforms%20successfully%20in%20identifying%20the%20best%20generation%20model%20across%20different%0Asample%20types.%20The%20code%20is%20available%20at%3A%0Agithub.com/yannxiaoyanhu/dgm-online-select.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13287v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAK-UCB%2520Contextual%2520Bandit%253A%2520An%2520Online%2520Learning%2520Approach%2520to%2520Prompt-Aware%250A%2520%2520Selection%2520of%2520Generative%2520Models%2520and%2520LLMs%26entry.906535625%3DXiaoyan%2520Hu%2520and%2520Ho-fung%2520Leung%2520and%2520Farzan%2520Farnia%26entry.1292438233%3D%2520%2520Selecting%2520a%2520sample%2520generation%2520scheme%2520from%2520multiple%2520prompt-based%2520generative%250Amodels%252C%2520including%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520prompt-guided%2520image%2520and%250Avideo%2520generation%2520models%252C%2520is%2520typically%2520addressed%2520by%2520choosing%2520the%2520model%2520that%250Amaximizes%2520an%2520averaged%2520evaluation%2520score.%2520However%252C%2520this%2520score-based%2520selection%250Aoverlooks%2520the%2520possibility%2520that%2520different%2520models%2520achieve%2520the%2520best%2520generation%250Aperformance%2520for%2520different%2520types%2520of%2520text%2520prompts.%2520An%2520online%2520identification%2520of%250Athe%2520best%2520generation%2520model%2520for%2520various%2520input%2520prompts%2520can%2520reduce%2520the%2520costs%250Aassociated%2520with%2520querying%2520sub-optimal%2520models.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%250Apossibility%2520of%2520varying%2520rankings%2520of%2520text-based%2520generative%2520models%2520for%2520different%250Atext%2520prompts%2520and%2520propose%2520an%2520online%2520learning%2520framework%2520to%2520predict%2520the%2520best%2520data%250Ageneration%2520model%2520for%2520a%2520given%2520input%2520prompt.%2520The%2520proposed%2520PAK-UCB%2520algorithm%250Aaddresses%2520a%2520contextual%2520bandit%2520%2528CB%2529%2520setting%2520with%2520shared%2520context%2520variables%2520across%250Athe%2520arms%252C%2520utilizing%2520the%2520generated%2520data%2520to%2520update%2520kernel-based%2520functions%2520that%250Apredict%2520the%2520score%2520of%2520each%2520model%2520available%2520for%2520unseen%2520text%2520prompts.%250AAdditionally%252C%2520we%2520leverage%2520random%2520Fourier%2520features%2520%2528RFF%2529%2520to%2520accelerate%2520the%250Aonline%2520learning%2520process%2520of%2520PAK-UCB.%2520Our%2520numerical%2520experiments%2520on%2520real%2520and%250Asimulated%2520text-to-image%2520and%2520image-to-text%2520generative%2520models%2520show%2520that%2520RFF-UCB%250Aperforms%2520successfully%2520in%2520identifying%2520the%2520best%2520generation%2520model%2520across%2520different%250Asample%2520types.%2520The%2520code%2520is%2520available%2520at%253A%250Agithub.com/yannxiaoyanhu/dgm-online-select.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13287v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAK-UCB%20Contextual%20Bandit%3A%20An%20Online%20Learning%20Approach%20to%20Prompt-Aware%0A%20%20Selection%20of%20Generative%20Models%20and%20LLMs&entry.906535625=Xiaoyan%20Hu%20and%20Ho-fung%20Leung%20and%20Farzan%20Farnia&entry.1292438233=%20%20Selecting%20a%20sample%20generation%20scheme%20from%20multiple%20prompt-based%20generative%0Amodels%2C%20including%20large%20language%20models%20%28LLMs%29%20and%20prompt-guided%20image%20and%0Avideo%20generation%20models%2C%20is%20typically%20addressed%20by%20choosing%20the%20model%20that%0Amaximizes%20an%20averaged%20evaluation%20score.%20However%2C%20this%20score-based%20selection%0Aoverlooks%20the%20possibility%20that%20different%20models%20achieve%20the%20best%20generation%0Aperformance%20for%20different%20types%20of%20text%20prompts.%20An%20online%20identification%20of%0Athe%20best%20generation%20model%20for%20various%20input%20prompts%20can%20reduce%20the%20costs%0Aassociated%20with%20querying%20sub-optimal%20models.%20In%20this%20work%2C%20we%20explore%20the%0Apossibility%20of%20varying%20rankings%20of%20text-based%20generative%20models%20for%20different%0Atext%20prompts%20and%20propose%20an%20online%20learning%20framework%20to%20predict%20the%20best%20data%0Ageneration%20model%20for%20a%20given%20input%20prompt.%20The%20proposed%20PAK-UCB%20algorithm%0Aaddresses%20a%20contextual%20bandit%20%28CB%29%20setting%20with%20shared%20context%20variables%20across%0Athe%20arms%2C%20utilizing%20the%20generated%20data%20to%20update%20kernel-based%20functions%20that%0Apredict%20the%20score%20of%20each%20model%20available%20for%20unseen%20text%20prompts.%0AAdditionally%2C%20we%20leverage%20random%20Fourier%20features%20%28RFF%29%20to%20accelerate%20the%0Aonline%20learning%20process%20of%20PAK-UCB.%20Our%20numerical%20experiments%20on%20real%20and%0Asimulated%20text-to-image%20and%20image-to-text%20generative%20models%20show%20that%20RFF-UCB%0Aperforms%20successfully%20in%20identifying%20the%20best%20generation%20model%20across%20different%0Asample%20types.%20The%20code%20is%20available%20at%3A%0Agithub.com/yannxiaoyanhu/dgm-online-select.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13287v6&entry.124074799=Read"},
{"title": "Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with\n  Adaptive Exploration", "author": "Zhicheng Yang and Zhijiang Guo and Yinya Huang and Yongxin Wang and Dongchun Xie and Yiwei Wang and Xiaodan Liang and Jing Tang", "abstract": "  Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a\npowerful paradigm for unlocking reasoning capabilities in large language\nmodels, yet its full potential is hindered by two under-explored dimensions:\nDepth-the hardest problem a model can sample; Breadth-the number of instances\nconsumed in a single iteration. We dissect the popular GRPO algorithm and\nreveal a systematic bias: the cumulative-advantage disproportionately weights\nsamples with medium accuracy, while down-weighting the low-accuracy instances\nthat are crucial for pushing reasoning boundaries. To rectify the depth\nneglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which\nre-weights hard problems through targeted multi-stage rollouts, thereby\nincreasing the number of positive rollouts for hard problems. Empirically,\nnaively enlarging rollout size only accelerates convergence and even hurts\nPass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra\ninference cost at convergence. Just as we adaptively expanded the depth of\nexploration, we now ask whether aggressively scaling the breadth of training\ndata can further amplify reasoning gains. To this end, we intensely scale batch\nsize and replace PPO's mini-batch iterations with full-batch updates over\nmultiple epochs. Increasing breadth significantly enhances Pass@1 performance.\nLarge-breadth training sustains high token-level entropy, indicating continued\nexploration and reduced gradient noise. We further present DARS-B, which\naugments DARS with large breadth, and demonstrate simultaneous gains in Pass@K\nand Pass@1. The results confirm that breadth and adaptive exploration across\ndepth operate as orthogonal dimensions in RLVR, which are key to unleashing the\nreasoning power of RLVR.\n", "link": "http://arxiv.org/abs/2508.13755v2", "date": "2025-09-04", "relevancy": 2.2387, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5605}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5595}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth-Breadth%20Synergy%20in%20RLVR%3A%20Unlocking%20LLM%20Reasoning%20Gains%20with%0A%20%20Adaptive%20Exploration&body=Title%3A%20Depth-Breadth%20Synergy%20in%20RLVR%3A%20Unlocking%20LLM%20Reasoning%20Gains%20with%0A%20%20Adaptive%20Exploration%0AAuthor%3A%20Zhicheng%20Yang%20and%20Zhijiang%20Guo%20and%20Yinya%20Huang%20and%20Yongxin%20Wang%20and%20Dongchun%20Xie%20and%20Yiwei%20Wang%20and%20Xiaodan%20Liang%20and%20Jing%20Tang%0AAbstract%3A%20%20%20Reinforcement%20Learning%20with%20Verifiable%20Reward%20%28RLVR%29%20has%20emerged%20as%20a%0Apowerful%20paradigm%20for%20unlocking%20reasoning%20capabilities%20in%20large%20language%0Amodels%2C%20yet%20its%20full%20potential%20is%20hindered%20by%20two%20under-explored%20dimensions%3A%0ADepth-the%20hardest%20problem%20a%20model%20can%20sample%3B%20Breadth-the%20number%20of%20instances%0Aconsumed%20in%20a%20single%20iteration.%20We%20dissect%20the%20popular%20GRPO%20algorithm%20and%0Areveal%20a%20systematic%20bias%3A%20the%20cumulative-advantage%20disproportionately%20weights%0Asamples%20with%20medium%20accuracy%2C%20while%20down-weighting%20the%20low-accuracy%20instances%0Athat%20are%20crucial%20for%20pushing%20reasoning%20boundaries.%20To%20rectify%20the%20depth%0Aneglect%2C%20we%20introduce%20Difficulty%20Adaptive%20Rollout%20Sampling%20%28DARS%29%2C%20which%0Are-weights%20hard%20problems%20through%20targeted%20multi-stage%20rollouts%2C%20thereby%0Aincreasing%20the%20number%20of%20positive%20rollouts%20for%20hard%20problems.%20Empirically%2C%0Anaively%20enlarging%20rollout%20size%20only%20accelerates%20convergence%20and%20even%20hurts%0APass%40K.%20Our%20DARS%2C%20in%20contrast%2C%20delivers%20consistent%20Pass%40K%20gains%20without%20extra%0Ainference%20cost%20at%20convergence.%20Just%20as%20we%20adaptively%20expanded%20the%20depth%20of%0Aexploration%2C%20we%20now%20ask%20whether%20aggressively%20scaling%20the%20breadth%20of%20training%0Adata%20can%20further%20amplify%20reasoning%20gains.%20To%20this%20end%2C%20we%20intensely%20scale%20batch%0Asize%20and%20replace%20PPO%27s%20mini-batch%20iterations%20with%20full-batch%20updates%20over%0Amultiple%20epochs.%20Increasing%20breadth%20significantly%20enhances%20Pass%401%20performance.%0ALarge-breadth%20training%20sustains%20high%20token-level%20entropy%2C%20indicating%20continued%0Aexploration%20and%20reduced%20gradient%20noise.%20We%20further%20present%20DARS-B%2C%20which%0Aaugments%20DARS%20with%20large%20breadth%2C%20and%20demonstrate%20simultaneous%20gains%20in%20Pass%40K%0Aand%20Pass%401.%20The%20results%20confirm%20that%20breadth%20and%20adaptive%20exploration%20across%0Adepth%20operate%20as%20orthogonal%20dimensions%20in%20RLVR%2C%20which%20are%20key%20to%20unleashing%20the%0Areasoning%20power%20of%20RLVR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13755v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth-Breadth%2520Synergy%2520in%2520RLVR%253A%2520Unlocking%2520LLM%2520Reasoning%2520Gains%2520with%250A%2520%2520Adaptive%2520Exploration%26entry.906535625%3DZhicheng%2520Yang%2520and%2520Zhijiang%2520Guo%2520and%2520Yinya%2520Huang%2520and%2520Yongxin%2520Wang%2520and%2520Dongchun%2520Xie%2520and%2520Yiwei%2520Wang%2520and%2520Xiaodan%2520Liang%2520and%2520Jing%2520Tang%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520with%2520Verifiable%2520Reward%2520%2528RLVR%2529%2520has%2520emerged%2520as%2520a%250Apowerful%2520paradigm%2520for%2520unlocking%2520reasoning%2520capabilities%2520in%2520large%2520language%250Amodels%252C%2520yet%2520its%2520full%2520potential%2520is%2520hindered%2520by%2520two%2520under-explored%2520dimensions%253A%250ADepth-the%2520hardest%2520problem%2520a%2520model%2520can%2520sample%253B%2520Breadth-the%2520number%2520of%2520instances%250Aconsumed%2520in%2520a%2520single%2520iteration.%2520We%2520dissect%2520the%2520popular%2520GRPO%2520algorithm%2520and%250Areveal%2520a%2520systematic%2520bias%253A%2520the%2520cumulative-advantage%2520disproportionately%2520weights%250Asamples%2520with%2520medium%2520accuracy%252C%2520while%2520down-weighting%2520the%2520low-accuracy%2520instances%250Athat%2520are%2520crucial%2520for%2520pushing%2520reasoning%2520boundaries.%2520To%2520rectify%2520the%2520depth%250Aneglect%252C%2520we%2520introduce%2520Difficulty%2520Adaptive%2520Rollout%2520Sampling%2520%2528DARS%2529%252C%2520which%250Are-weights%2520hard%2520problems%2520through%2520targeted%2520multi-stage%2520rollouts%252C%2520thereby%250Aincreasing%2520the%2520number%2520of%2520positive%2520rollouts%2520for%2520hard%2520problems.%2520Empirically%252C%250Anaively%2520enlarging%2520rollout%2520size%2520only%2520accelerates%2520convergence%2520and%2520even%2520hurts%250APass%2540K.%2520Our%2520DARS%252C%2520in%2520contrast%252C%2520delivers%2520consistent%2520Pass%2540K%2520gains%2520without%2520extra%250Ainference%2520cost%2520at%2520convergence.%2520Just%2520as%2520we%2520adaptively%2520expanded%2520the%2520depth%2520of%250Aexploration%252C%2520we%2520now%2520ask%2520whether%2520aggressively%2520scaling%2520the%2520breadth%2520of%2520training%250Adata%2520can%2520further%2520amplify%2520reasoning%2520gains.%2520To%2520this%2520end%252C%2520we%2520intensely%2520scale%2520batch%250Asize%2520and%2520replace%2520PPO%2527s%2520mini-batch%2520iterations%2520with%2520full-batch%2520updates%2520over%250Amultiple%2520epochs.%2520Increasing%2520breadth%2520significantly%2520enhances%2520Pass%25401%2520performance.%250ALarge-breadth%2520training%2520sustains%2520high%2520token-level%2520entropy%252C%2520indicating%2520continued%250Aexploration%2520and%2520reduced%2520gradient%2520noise.%2520We%2520further%2520present%2520DARS-B%252C%2520which%250Aaugments%2520DARS%2520with%2520large%2520breadth%252C%2520and%2520demonstrate%2520simultaneous%2520gains%2520in%2520Pass%2540K%250Aand%2520Pass%25401.%2520The%2520results%2520confirm%2520that%2520breadth%2520and%2520adaptive%2520exploration%2520across%250Adepth%2520operate%2520as%2520orthogonal%2520dimensions%2520in%2520RLVR%252C%2520which%2520are%2520key%2520to%2520unleashing%2520the%250Areasoning%2520power%2520of%2520RLVR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13755v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth-Breadth%20Synergy%20in%20RLVR%3A%20Unlocking%20LLM%20Reasoning%20Gains%20with%0A%20%20Adaptive%20Exploration&entry.906535625=Zhicheng%20Yang%20and%20Zhijiang%20Guo%20and%20Yinya%20Huang%20and%20Yongxin%20Wang%20and%20Dongchun%20Xie%20and%20Yiwei%20Wang%20and%20Xiaodan%20Liang%20and%20Jing%20Tang&entry.1292438233=%20%20Reinforcement%20Learning%20with%20Verifiable%20Reward%20%28RLVR%29%20has%20emerged%20as%20a%0Apowerful%20paradigm%20for%20unlocking%20reasoning%20capabilities%20in%20large%20language%0Amodels%2C%20yet%20its%20full%20potential%20is%20hindered%20by%20two%20under-explored%20dimensions%3A%0ADepth-the%20hardest%20problem%20a%20model%20can%20sample%3B%20Breadth-the%20number%20of%20instances%0Aconsumed%20in%20a%20single%20iteration.%20We%20dissect%20the%20popular%20GRPO%20algorithm%20and%0Areveal%20a%20systematic%20bias%3A%20the%20cumulative-advantage%20disproportionately%20weights%0Asamples%20with%20medium%20accuracy%2C%20while%20down-weighting%20the%20low-accuracy%20instances%0Athat%20are%20crucial%20for%20pushing%20reasoning%20boundaries.%20To%20rectify%20the%20depth%0Aneglect%2C%20we%20introduce%20Difficulty%20Adaptive%20Rollout%20Sampling%20%28DARS%29%2C%20which%0Are-weights%20hard%20problems%20through%20targeted%20multi-stage%20rollouts%2C%20thereby%0Aincreasing%20the%20number%20of%20positive%20rollouts%20for%20hard%20problems.%20Empirically%2C%0Anaively%20enlarging%20rollout%20size%20only%20accelerates%20convergence%20and%20even%20hurts%0APass%40K.%20Our%20DARS%2C%20in%20contrast%2C%20delivers%20consistent%20Pass%40K%20gains%20without%20extra%0Ainference%20cost%20at%20convergence.%20Just%20as%20we%20adaptively%20expanded%20the%20depth%20of%0Aexploration%2C%20we%20now%20ask%20whether%20aggressively%20scaling%20the%20breadth%20of%20training%0Adata%20can%20further%20amplify%20reasoning%20gains.%20To%20this%20end%2C%20we%20intensely%20scale%20batch%0Asize%20and%20replace%20PPO%27s%20mini-batch%20iterations%20with%20full-batch%20updates%20over%0Amultiple%20epochs.%20Increasing%20breadth%20significantly%20enhances%20Pass%401%20performance.%0ALarge-breadth%20training%20sustains%20high%20token-level%20entropy%2C%20indicating%20continued%0Aexploration%20and%20reduced%20gradient%20noise.%20We%20further%20present%20DARS-B%2C%20which%0Aaugments%20DARS%20with%20large%20breadth%2C%20and%20demonstrate%20simultaneous%20gains%20in%20Pass%40K%0Aand%20Pass%401.%20The%20results%20confirm%20that%20breadth%20and%20adaptive%20exploration%20across%0Adepth%20operate%20as%20orthogonal%20dimensions%20in%20RLVR%2C%20which%20are%20key%20to%20unleashing%20the%0Areasoning%20power%20of%20RLVR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13755v2&entry.124074799=Read"},
{"title": "MICACL: Multi-Instance Category-Aware Contrastive Learning for\n  Long-Tailed Dynamic Facial Expression Recognition", "author": "Feng-Qi Cui and Zhen Lin and Xinlong Rao and Anyang Tong and Shiyao Li and Fei Wang and Changlin Chen and Bin Liu", "abstract": "  Dynamic facial expression recognition (DFER) faces significant challenges due\nto long-tailed category distributions and complexity of spatio-temporal feature\nmodeling. While existing deep learning-based methods have improved DFER\nperformance, they often fail to address these issues, resulting in severe model\ninduction bias. To overcome these limitations, we propose a novel\nmulti-instance learning framework called MICACL, which integrates\nspatio-temporal dependency modeling and long-tailed contrastive learning\noptimization. Specifically, we design the Graph-Enhanced Instance Interaction\nModule (GEIIM) to capture intricate spatio-temporal between adjacent instances\nrelationships through adaptive adjacency matrices and multiscale convolutions.\nTo enhance instance-level feature aggregation, we develop the Weighted Instance\nAggregation Network (WIAN), which dynamically assigns weights based on instance\nimportance. Furthermore, we introduce a Multiscale Category-aware Contrastive\nLearning (MCCL) strategy to balance training between major and minor\ncategories. Extensive experiments on in-the-wild datasets (i.e., DFEW and\nFERV39k) demonstrate that MICACL achieves state-of-the-art performance with\nsuperior robustness and generalization.\n", "link": "http://arxiv.org/abs/2509.04344v1", "date": "2025-09-04", "relevancy": 2.2264, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5675}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5516}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MICACL%3A%20Multi-Instance%20Category-Aware%20Contrastive%20Learning%20for%0A%20%20Long-Tailed%20Dynamic%20Facial%20Expression%20Recognition&body=Title%3A%20MICACL%3A%20Multi-Instance%20Category-Aware%20Contrastive%20Learning%20for%0A%20%20Long-Tailed%20Dynamic%20Facial%20Expression%20Recognition%0AAuthor%3A%20Feng-Qi%20Cui%20and%20Zhen%20Lin%20and%20Xinlong%20Rao%20and%20Anyang%20Tong%20and%20Shiyao%20Li%20and%20Fei%20Wang%20and%20Changlin%20Chen%20and%20Bin%20Liu%0AAbstract%3A%20%20%20Dynamic%20facial%20expression%20recognition%20%28DFER%29%20faces%20significant%20challenges%20due%0Ato%20long-tailed%20category%20distributions%20and%20complexity%20of%20spatio-temporal%20feature%0Amodeling.%20While%20existing%20deep%20learning-based%20methods%20have%20improved%20DFER%0Aperformance%2C%20they%20often%20fail%20to%20address%20these%20issues%2C%20resulting%20in%20severe%20model%0Ainduction%20bias.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20novel%0Amulti-instance%20learning%20framework%20called%20MICACL%2C%20which%20integrates%0Aspatio-temporal%20dependency%20modeling%20and%20long-tailed%20contrastive%20learning%0Aoptimization.%20Specifically%2C%20we%20design%20the%20Graph-Enhanced%20Instance%20Interaction%0AModule%20%28GEIIM%29%20to%20capture%20intricate%20spatio-temporal%20between%20adjacent%20instances%0Arelationships%20through%20adaptive%20adjacency%20matrices%20and%20multiscale%20convolutions.%0ATo%20enhance%20instance-level%20feature%20aggregation%2C%20we%20develop%20the%20Weighted%20Instance%0AAggregation%20Network%20%28WIAN%29%2C%20which%20dynamically%20assigns%20weights%20based%20on%20instance%0Aimportance.%20Furthermore%2C%20we%20introduce%20a%20Multiscale%20Category-aware%20Contrastive%0ALearning%20%28MCCL%29%20strategy%20to%20balance%20training%20between%20major%20and%20minor%0Acategories.%20Extensive%20experiments%20on%20in-the-wild%20datasets%20%28i.e.%2C%20DFEW%20and%0AFERV39k%29%20demonstrate%20that%20MICACL%20achieves%20state-of-the-art%20performance%20with%0Asuperior%20robustness%20and%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04344v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMICACL%253A%2520Multi-Instance%2520Category-Aware%2520Contrastive%2520Learning%2520for%250A%2520%2520Long-Tailed%2520Dynamic%2520Facial%2520Expression%2520Recognition%26entry.906535625%3DFeng-Qi%2520Cui%2520and%2520Zhen%2520Lin%2520and%2520Xinlong%2520Rao%2520and%2520Anyang%2520Tong%2520and%2520Shiyao%2520Li%2520and%2520Fei%2520Wang%2520and%2520Changlin%2520Chen%2520and%2520Bin%2520Liu%26entry.1292438233%3D%2520%2520Dynamic%2520facial%2520expression%2520recognition%2520%2528DFER%2529%2520faces%2520significant%2520challenges%2520due%250Ato%2520long-tailed%2520category%2520distributions%2520and%2520complexity%2520of%2520spatio-temporal%2520feature%250Amodeling.%2520While%2520existing%2520deep%2520learning-based%2520methods%2520have%2520improved%2520DFER%250Aperformance%252C%2520they%2520often%2520fail%2520to%2520address%2520these%2520issues%252C%2520resulting%2520in%2520severe%2520model%250Ainduction%2520bias.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%250Amulti-instance%2520learning%2520framework%2520called%2520MICACL%252C%2520which%2520integrates%250Aspatio-temporal%2520dependency%2520modeling%2520and%2520long-tailed%2520contrastive%2520learning%250Aoptimization.%2520Specifically%252C%2520we%2520design%2520the%2520Graph-Enhanced%2520Instance%2520Interaction%250AModule%2520%2528GEIIM%2529%2520to%2520capture%2520intricate%2520spatio-temporal%2520between%2520adjacent%2520instances%250Arelationships%2520through%2520adaptive%2520adjacency%2520matrices%2520and%2520multiscale%2520convolutions.%250ATo%2520enhance%2520instance-level%2520feature%2520aggregation%252C%2520we%2520develop%2520the%2520Weighted%2520Instance%250AAggregation%2520Network%2520%2528WIAN%2529%252C%2520which%2520dynamically%2520assigns%2520weights%2520based%2520on%2520instance%250Aimportance.%2520Furthermore%252C%2520we%2520introduce%2520a%2520Multiscale%2520Category-aware%2520Contrastive%250ALearning%2520%2528MCCL%2529%2520strategy%2520to%2520balance%2520training%2520between%2520major%2520and%2520minor%250Acategories.%2520Extensive%2520experiments%2520on%2520in-the-wild%2520datasets%2520%2528i.e.%252C%2520DFEW%2520and%250AFERV39k%2529%2520demonstrate%2520that%2520MICACL%2520achieves%2520state-of-the-art%2520performance%2520with%250Asuperior%2520robustness%2520and%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04344v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MICACL%3A%20Multi-Instance%20Category-Aware%20Contrastive%20Learning%20for%0A%20%20Long-Tailed%20Dynamic%20Facial%20Expression%20Recognition&entry.906535625=Feng-Qi%20Cui%20and%20Zhen%20Lin%20and%20Xinlong%20Rao%20and%20Anyang%20Tong%20and%20Shiyao%20Li%20and%20Fei%20Wang%20and%20Changlin%20Chen%20and%20Bin%20Liu&entry.1292438233=%20%20Dynamic%20facial%20expression%20recognition%20%28DFER%29%20faces%20significant%20challenges%20due%0Ato%20long-tailed%20category%20distributions%20and%20complexity%20of%20spatio-temporal%20feature%0Amodeling.%20While%20existing%20deep%20learning-based%20methods%20have%20improved%20DFER%0Aperformance%2C%20they%20often%20fail%20to%20address%20these%20issues%2C%20resulting%20in%20severe%20model%0Ainduction%20bias.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20novel%0Amulti-instance%20learning%20framework%20called%20MICACL%2C%20which%20integrates%0Aspatio-temporal%20dependency%20modeling%20and%20long-tailed%20contrastive%20learning%0Aoptimization.%20Specifically%2C%20we%20design%20the%20Graph-Enhanced%20Instance%20Interaction%0AModule%20%28GEIIM%29%20to%20capture%20intricate%20spatio-temporal%20between%20adjacent%20instances%0Arelationships%20through%20adaptive%20adjacency%20matrices%20and%20multiscale%20convolutions.%0ATo%20enhance%20instance-level%20feature%20aggregation%2C%20we%20develop%20the%20Weighted%20Instance%0AAggregation%20Network%20%28WIAN%29%2C%20which%20dynamically%20assigns%20weights%20based%20on%20instance%0Aimportance.%20Furthermore%2C%20we%20introduce%20a%20Multiscale%20Category-aware%20Contrastive%0ALearning%20%28MCCL%29%20strategy%20to%20balance%20training%20between%20major%20and%20minor%0Acategories.%20Extensive%20experiments%20on%20in-the-wild%20datasets%20%28i.e.%2C%20DFEW%20and%0AFERV39k%29%20demonstrate%20that%20MICACL%20achieves%20state-of-the-art%20performance%20with%0Asuperior%20robustness%20and%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04344v1&entry.124074799=Read"},
{"title": "TRUST-VL: An Explainable News Assistant for General Multimodal\n  Misinformation Detection", "author": "Zehong Yan and Peng Qi and Wynne Hsu and Mong Li Lee", "abstract": "  Multimodal misinformation, encompassing textual, visual, and cross-modal\ndistortions, poses an increasing societal threat that is amplified by\ngenerative AI. Existing methods typically focus on a single type of distortion\nand struggle to generalize to unseen scenarios. In this work, we observe that\ndifferent distortion types share common reasoning capabilities while also\nrequiring task-specific skills. We hypothesize that joint training across\ndistortion types facilitates knowledge sharing and enhances the model's ability\nto generalize. To this end, we introduce TRUST-VL, a unified and explainable\nvision-language model for general multimodal misinformation detection. TRUST-VL\nincorporates a novel Question-Aware Visual Amplifier module, designed to\nextract task-specific visual features. To support training, we also construct\nTRUST-Instruct, a large-scale instruction dataset containing 198K samples\nfeaturing structured reasoning chains aligned with human fact-checking\nworkflows. Extensive experiments on both in-domain and zero-shot benchmarks\ndemonstrate that TRUST-VL achieves state-of-the-art performance, while also\noffering strong generalization and interpretability.\n", "link": "http://arxiv.org/abs/2509.04448v1", "date": "2025-09-04", "relevancy": 2.2189, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5813}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5517}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRUST-VL%3A%20An%20Explainable%20News%20Assistant%20for%20General%20Multimodal%0A%20%20Misinformation%20Detection&body=Title%3A%20TRUST-VL%3A%20An%20Explainable%20News%20Assistant%20for%20General%20Multimodal%0A%20%20Misinformation%20Detection%0AAuthor%3A%20Zehong%20Yan%20and%20Peng%20Qi%20and%20Wynne%20Hsu%20and%20Mong%20Li%20Lee%0AAbstract%3A%20%20%20Multimodal%20misinformation%2C%20encompassing%20textual%2C%20visual%2C%20and%20cross-modal%0Adistortions%2C%20poses%20an%20increasing%20societal%20threat%20that%20is%20amplified%20by%0Agenerative%20AI.%20Existing%20methods%20typically%20focus%20on%20a%20single%20type%20of%20distortion%0Aand%20struggle%20to%20generalize%20to%20unseen%20scenarios.%20In%20this%20work%2C%20we%20observe%20that%0Adifferent%20distortion%20types%20share%20common%20reasoning%20capabilities%20while%20also%0Arequiring%20task-specific%20skills.%20We%20hypothesize%20that%20joint%20training%20across%0Adistortion%20types%20facilitates%20knowledge%20sharing%20and%20enhances%20the%20model%27s%20ability%0Ato%20generalize.%20To%20this%20end%2C%20we%20introduce%20TRUST-VL%2C%20a%20unified%20and%20explainable%0Avision-language%20model%20for%20general%20multimodal%20misinformation%20detection.%20TRUST-VL%0Aincorporates%20a%20novel%20Question-Aware%20Visual%20Amplifier%20module%2C%20designed%20to%0Aextract%20task-specific%20visual%20features.%20To%20support%20training%2C%20we%20also%20construct%0ATRUST-Instruct%2C%20a%20large-scale%20instruction%20dataset%20containing%20198K%20samples%0Afeaturing%20structured%20reasoning%20chains%20aligned%20with%20human%20fact-checking%0Aworkflows.%20Extensive%20experiments%20on%20both%20in-domain%20and%20zero-shot%20benchmarks%0Ademonstrate%20that%20TRUST-VL%20achieves%20state-of-the-art%20performance%2C%20while%20also%0Aoffering%20strong%20generalization%20and%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04448v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRUST-VL%253A%2520An%2520Explainable%2520News%2520Assistant%2520for%2520General%2520Multimodal%250A%2520%2520Misinformation%2520Detection%26entry.906535625%3DZehong%2520Yan%2520and%2520Peng%2520Qi%2520and%2520Wynne%2520Hsu%2520and%2520Mong%2520Li%2520Lee%26entry.1292438233%3D%2520%2520Multimodal%2520misinformation%252C%2520encompassing%2520textual%252C%2520visual%252C%2520and%2520cross-modal%250Adistortions%252C%2520poses%2520an%2520increasing%2520societal%2520threat%2520that%2520is%2520amplified%2520by%250Agenerative%2520AI.%2520Existing%2520methods%2520typically%2520focus%2520on%2520a%2520single%2520type%2520of%2520distortion%250Aand%2520struggle%2520to%2520generalize%2520to%2520unseen%2520scenarios.%2520In%2520this%2520work%252C%2520we%2520observe%2520that%250Adifferent%2520distortion%2520types%2520share%2520common%2520reasoning%2520capabilities%2520while%2520also%250Arequiring%2520task-specific%2520skills.%2520We%2520hypothesize%2520that%2520joint%2520training%2520across%250Adistortion%2520types%2520facilitates%2520knowledge%2520sharing%2520and%2520enhances%2520the%2520model%2527s%2520ability%250Ato%2520generalize.%2520To%2520this%2520end%252C%2520we%2520introduce%2520TRUST-VL%252C%2520a%2520unified%2520and%2520explainable%250Avision-language%2520model%2520for%2520general%2520multimodal%2520misinformation%2520detection.%2520TRUST-VL%250Aincorporates%2520a%2520novel%2520Question-Aware%2520Visual%2520Amplifier%2520module%252C%2520designed%2520to%250Aextract%2520task-specific%2520visual%2520features.%2520To%2520support%2520training%252C%2520we%2520also%2520construct%250ATRUST-Instruct%252C%2520a%2520large-scale%2520instruction%2520dataset%2520containing%2520198K%2520samples%250Afeaturing%2520structured%2520reasoning%2520chains%2520aligned%2520with%2520human%2520fact-checking%250Aworkflows.%2520Extensive%2520experiments%2520on%2520both%2520in-domain%2520and%2520zero-shot%2520benchmarks%250Ademonstrate%2520that%2520TRUST-VL%2520achieves%2520state-of-the-art%2520performance%252C%2520while%2520also%250Aoffering%2520strong%2520generalization%2520and%2520interpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04448v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRUST-VL%3A%20An%20Explainable%20News%20Assistant%20for%20General%20Multimodal%0A%20%20Misinformation%20Detection&entry.906535625=Zehong%20Yan%20and%20Peng%20Qi%20and%20Wynne%20Hsu%20and%20Mong%20Li%20Lee&entry.1292438233=%20%20Multimodal%20misinformation%2C%20encompassing%20textual%2C%20visual%2C%20and%20cross-modal%0Adistortions%2C%20poses%20an%20increasing%20societal%20threat%20that%20is%20amplified%20by%0Agenerative%20AI.%20Existing%20methods%20typically%20focus%20on%20a%20single%20type%20of%20distortion%0Aand%20struggle%20to%20generalize%20to%20unseen%20scenarios.%20In%20this%20work%2C%20we%20observe%20that%0Adifferent%20distortion%20types%20share%20common%20reasoning%20capabilities%20while%20also%0Arequiring%20task-specific%20skills.%20We%20hypothesize%20that%20joint%20training%20across%0Adistortion%20types%20facilitates%20knowledge%20sharing%20and%20enhances%20the%20model%27s%20ability%0Ato%20generalize.%20To%20this%20end%2C%20we%20introduce%20TRUST-VL%2C%20a%20unified%20and%20explainable%0Avision-language%20model%20for%20general%20multimodal%20misinformation%20detection.%20TRUST-VL%0Aincorporates%20a%20novel%20Question-Aware%20Visual%20Amplifier%20module%2C%20designed%20to%0Aextract%20task-specific%20visual%20features.%20To%20support%20training%2C%20we%20also%20construct%0ATRUST-Instruct%2C%20a%20large-scale%20instruction%20dataset%20containing%20198K%20samples%0Afeaturing%20structured%20reasoning%20chains%20aligned%20with%20human%20fact-checking%0Aworkflows.%20Extensive%20experiments%20on%20both%20in-domain%20and%20zero-shot%20benchmarks%0Ademonstrate%20that%20TRUST-VL%20achieves%20state-of-the-art%20performance%2C%20while%20also%0Aoffering%20strong%20generalization%20and%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04448v1&entry.124074799=Read"},
{"title": "Quantifying Calibration Error in Neural Networks Through Evidence-Based\n  Theory", "author": "Koffi Ismael Ouattara and Ioannis Krontiris and Theo Dimitrakos and Frank Kargl", "abstract": "  Trustworthiness in neural networks is crucial for their deployment in\ncritical applications, where reliability, confidence, and uncertainty play\npivotal roles in decision-making. Traditional performance metrics such as\naccuracy and precision fail to capture these aspects, particularly in cases\nwhere models exhibit overconfidence. To address these limitations, this paper\nintroduces a novel framework for quantifying the trustworthiness of neural\nnetworks by incorporating subjective logic into the evaluation of Expected\nCalibration Error (ECE). This method provides a comprehensive measure of trust,\ndisbelief, and uncertainty by clustering predicted probabilities and fusing\nopinions using appropriate fusion operators. We demonstrate the effectiveness\nof this approach through experiments on MNIST and CIFAR-10 datasets, where\npost-calibration results indicate improved trustworthiness. The proposed\nframework offers a more interpretable and nuanced assessment of AI models, with\npotential applications in sensitive domains such as healthcare and autonomous\nsystems.\n", "link": "http://arxiv.org/abs/2411.00265v3", "date": "2025-09-04", "relevancy": 2.217, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5914}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5735}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantifying%20Calibration%20Error%20in%20Neural%20Networks%20Through%20Evidence-Based%0A%20%20Theory&body=Title%3A%20Quantifying%20Calibration%20Error%20in%20Neural%20Networks%20Through%20Evidence-Based%0A%20%20Theory%0AAuthor%3A%20Koffi%20Ismael%20Ouattara%20and%20Ioannis%20Krontiris%20and%20Theo%20Dimitrakos%20and%20Frank%20Kargl%0AAbstract%3A%20%20%20Trustworthiness%20in%20neural%20networks%20is%20crucial%20for%20their%20deployment%20in%0Acritical%20applications%2C%20where%20reliability%2C%20confidence%2C%20and%20uncertainty%20play%0Apivotal%20roles%20in%20decision-making.%20Traditional%20performance%20metrics%20such%20as%0Aaccuracy%20and%20precision%20fail%20to%20capture%20these%20aspects%2C%20particularly%20in%20cases%0Awhere%20models%20exhibit%20overconfidence.%20To%20address%20these%20limitations%2C%20this%20paper%0Aintroduces%20a%20novel%20framework%20for%20quantifying%20the%20trustworthiness%20of%20neural%0Anetworks%20by%20incorporating%20subjective%20logic%20into%20the%20evaluation%20of%20Expected%0ACalibration%20Error%20%28ECE%29.%20This%20method%20provides%20a%20comprehensive%20measure%20of%20trust%2C%0Adisbelief%2C%20and%20uncertainty%20by%20clustering%20predicted%20probabilities%20and%20fusing%0Aopinions%20using%20appropriate%20fusion%20operators.%20We%20demonstrate%20the%20effectiveness%0Aof%20this%20approach%20through%20experiments%20on%20MNIST%20and%20CIFAR-10%20datasets%2C%20where%0Apost-calibration%20results%20indicate%20improved%20trustworthiness.%20The%20proposed%0Aframework%20offers%20a%20more%20interpretable%20and%20nuanced%20assessment%20of%20AI%20models%2C%20with%0Apotential%20applications%20in%20sensitive%20domains%20such%20as%20healthcare%20and%20autonomous%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.00265v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantifying%2520Calibration%2520Error%2520in%2520Neural%2520Networks%2520Through%2520Evidence-Based%250A%2520%2520Theory%26entry.906535625%3DKoffi%2520Ismael%2520Ouattara%2520and%2520Ioannis%2520Krontiris%2520and%2520Theo%2520Dimitrakos%2520and%2520Frank%2520Kargl%26entry.1292438233%3D%2520%2520Trustworthiness%2520in%2520neural%2520networks%2520is%2520crucial%2520for%2520their%2520deployment%2520in%250Acritical%2520applications%252C%2520where%2520reliability%252C%2520confidence%252C%2520and%2520uncertainty%2520play%250Apivotal%2520roles%2520in%2520decision-making.%2520Traditional%2520performance%2520metrics%2520such%2520as%250Aaccuracy%2520and%2520precision%2520fail%2520to%2520capture%2520these%2520aspects%252C%2520particularly%2520in%2520cases%250Awhere%2520models%2520exhibit%2520overconfidence.%2520To%2520address%2520these%2520limitations%252C%2520this%2520paper%250Aintroduces%2520a%2520novel%2520framework%2520for%2520quantifying%2520the%2520trustworthiness%2520of%2520neural%250Anetworks%2520by%2520incorporating%2520subjective%2520logic%2520into%2520the%2520evaluation%2520of%2520Expected%250ACalibration%2520Error%2520%2528ECE%2529.%2520This%2520method%2520provides%2520a%2520comprehensive%2520measure%2520of%2520trust%252C%250Adisbelief%252C%2520and%2520uncertainty%2520by%2520clustering%2520predicted%2520probabilities%2520and%2520fusing%250Aopinions%2520using%2520appropriate%2520fusion%2520operators.%2520We%2520demonstrate%2520the%2520effectiveness%250Aof%2520this%2520approach%2520through%2520experiments%2520on%2520MNIST%2520and%2520CIFAR-10%2520datasets%252C%2520where%250Apost-calibration%2520results%2520indicate%2520improved%2520trustworthiness.%2520The%2520proposed%250Aframework%2520offers%2520a%2520more%2520interpretable%2520and%2520nuanced%2520assessment%2520of%2520AI%2520models%252C%2520with%250Apotential%2520applications%2520in%2520sensitive%2520domains%2520such%2520as%2520healthcare%2520and%2520autonomous%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00265v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20Calibration%20Error%20in%20Neural%20Networks%20Through%20Evidence-Based%0A%20%20Theory&entry.906535625=Koffi%20Ismael%20Ouattara%20and%20Ioannis%20Krontiris%20and%20Theo%20Dimitrakos%20and%20Frank%20Kargl&entry.1292438233=%20%20Trustworthiness%20in%20neural%20networks%20is%20crucial%20for%20their%20deployment%20in%0Acritical%20applications%2C%20where%20reliability%2C%20confidence%2C%20and%20uncertainty%20play%0Apivotal%20roles%20in%20decision-making.%20Traditional%20performance%20metrics%20such%20as%0Aaccuracy%20and%20precision%20fail%20to%20capture%20these%20aspects%2C%20particularly%20in%20cases%0Awhere%20models%20exhibit%20overconfidence.%20To%20address%20these%20limitations%2C%20this%20paper%0Aintroduces%20a%20novel%20framework%20for%20quantifying%20the%20trustworthiness%20of%20neural%0Anetworks%20by%20incorporating%20subjective%20logic%20into%20the%20evaluation%20of%20Expected%0ACalibration%20Error%20%28ECE%29.%20This%20method%20provides%20a%20comprehensive%20measure%20of%20trust%2C%0Adisbelief%2C%20and%20uncertainty%20by%20clustering%20predicted%20probabilities%20and%20fusing%0Aopinions%20using%20appropriate%20fusion%20operators.%20We%20demonstrate%20the%20effectiveness%0Aof%20this%20approach%20through%20experiments%20on%20MNIST%20and%20CIFAR-10%20datasets%2C%20where%0Apost-calibration%20results%20indicate%20improved%20trustworthiness.%20The%20proposed%0Aframework%20offers%20a%20more%20interpretable%20and%20nuanced%20assessment%20of%20AI%20models%2C%20with%0Apotential%20applications%20in%20sensitive%20domains%20such%20as%20healthcare%20and%20autonomous%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.00265v3&entry.124074799=Read"},
{"title": "DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation", "author": "Hao-Shu Fang and Branden Romero and Yichen Xie and Arthur Hu and Bo-Ruei Huang and Juan Alvarez and Matthew Kim and Gabriel Margolis and Kavya Anbarasu and Masayoshi Tomizuka and Edward Adelson and Pulkit Agrawal", "abstract": "  We introduce perioperation, a paradigm for robotic data collection that\nsensorizes and records human manipulation while maximizing the transferability\nof the data to real robots. We implement this paradigm in DEXOP, a passive hand\nexoskeleton designed to maximize human ability to collect rich sensory (vision\n+ tactile) data for diverse dexterous manipulation tasks in natural\nenvironments. DEXOP mechanically connects human fingers to robot fingers,\nproviding users with direct contact feedback (via proprioception) and mirrors\nthe human hand pose to the passive robot hand to maximize the transfer of\ndemonstrated skills to the robot. The force feedback and pose mirroring make\ntask demonstrations more natural for humans compared to teleoperation,\nincreasing both speed and accuracy. We evaluate DEXOP across a range of\ndexterous, contact-rich tasks, demonstrating its ability to collect\nhigh-quality demonstration data at scale. Policies learned with DEXOP data\nsignificantly improve task performance per unit time of data collection\ncompared to teleoperation, making DEXOP a powerful tool for advancing robot\ndexterity. Our project page is at https://dex-op.github.io.\n", "link": "http://arxiv.org/abs/2509.04441v1", "date": "2025-09-04", "relevancy": 2.2108, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5788}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5739}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DEXOP%3A%20A%20Device%20for%20Robotic%20Transfer%20of%20Dexterous%20Human%20Manipulation&body=Title%3A%20DEXOP%3A%20A%20Device%20for%20Robotic%20Transfer%20of%20Dexterous%20Human%20Manipulation%0AAuthor%3A%20Hao-Shu%20Fang%20and%20Branden%20Romero%20and%20Yichen%20Xie%20and%20Arthur%20Hu%20and%20Bo-Ruei%20Huang%20and%20Juan%20Alvarez%20and%20Matthew%20Kim%20and%20Gabriel%20Margolis%20and%20Kavya%20Anbarasu%20and%20Masayoshi%20Tomizuka%20and%20Edward%20Adelson%20and%20Pulkit%20Agrawal%0AAbstract%3A%20%20%20We%20introduce%20perioperation%2C%20a%20paradigm%20for%20robotic%20data%20collection%20that%0Asensorizes%20and%20records%20human%20manipulation%20while%20maximizing%20the%20transferability%0Aof%20the%20data%20to%20real%20robots.%20We%20implement%20this%20paradigm%20in%20DEXOP%2C%20a%20passive%20hand%0Aexoskeleton%20designed%20to%20maximize%20human%20ability%20to%20collect%20rich%20sensory%20%28vision%0A%2B%20tactile%29%20data%20for%20diverse%20dexterous%20manipulation%20tasks%20in%20natural%0Aenvironments.%20DEXOP%20mechanically%20connects%20human%20fingers%20to%20robot%20fingers%2C%0Aproviding%20users%20with%20direct%20contact%20feedback%20%28via%20proprioception%29%20and%20mirrors%0Athe%20human%20hand%20pose%20to%20the%20passive%20robot%20hand%20to%20maximize%20the%20transfer%20of%0Ademonstrated%20skills%20to%20the%20robot.%20The%20force%20feedback%20and%20pose%20mirroring%20make%0Atask%20demonstrations%20more%20natural%20for%20humans%20compared%20to%20teleoperation%2C%0Aincreasing%20both%20speed%20and%20accuracy.%20We%20evaluate%20DEXOP%20across%20a%20range%20of%0Adexterous%2C%20contact-rich%20tasks%2C%20demonstrating%20its%20ability%20to%20collect%0Ahigh-quality%20demonstration%20data%20at%20scale.%20Policies%20learned%20with%20DEXOP%20data%0Asignificantly%20improve%20task%20performance%20per%20unit%20time%20of%20data%20collection%0Acompared%20to%20teleoperation%2C%20making%20DEXOP%20a%20powerful%20tool%20for%20advancing%20robot%0Adexterity.%20Our%20project%20page%20is%20at%20https%3A//dex-op.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04441v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDEXOP%253A%2520A%2520Device%2520for%2520Robotic%2520Transfer%2520of%2520Dexterous%2520Human%2520Manipulation%26entry.906535625%3DHao-Shu%2520Fang%2520and%2520Branden%2520Romero%2520and%2520Yichen%2520Xie%2520and%2520Arthur%2520Hu%2520and%2520Bo-Ruei%2520Huang%2520and%2520Juan%2520Alvarez%2520and%2520Matthew%2520Kim%2520and%2520Gabriel%2520Margolis%2520and%2520Kavya%2520Anbarasu%2520and%2520Masayoshi%2520Tomizuka%2520and%2520Edward%2520Adelson%2520and%2520Pulkit%2520Agrawal%26entry.1292438233%3D%2520%2520We%2520introduce%2520perioperation%252C%2520a%2520paradigm%2520for%2520robotic%2520data%2520collection%2520that%250Asensorizes%2520and%2520records%2520human%2520manipulation%2520while%2520maximizing%2520the%2520transferability%250Aof%2520the%2520data%2520to%2520real%2520robots.%2520We%2520implement%2520this%2520paradigm%2520in%2520DEXOP%252C%2520a%2520passive%2520hand%250Aexoskeleton%2520designed%2520to%2520maximize%2520human%2520ability%2520to%2520collect%2520rich%2520sensory%2520%2528vision%250A%252B%2520tactile%2529%2520data%2520for%2520diverse%2520dexterous%2520manipulation%2520tasks%2520in%2520natural%250Aenvironments.%2520DEXOP%2520mechanically%2520connects%2520human%2520fingers%2520to%2520robot%2520fingers%252C%250Aproviding%2520users%2520with%2520direct%2520contact%2520feedback%2520%2528via%2520proprioception%2529%2520and%2520mirrors%250Athe%2520human%2520hand%2520pose%2520to%2520the%2520passive%2520robot%2520hand%2520to%2520maximize%2520the%2520transfer%2520of%250Ademonstrated%2520skills%2520to%2520the%2520robot.%2520The%2520force%2520feedback%2520and%2520pose%2520mirroring%2520make%250Atask%2520demonstrations%2520more%2520natural%2520for%2520humans%2520compared%2520to%2520teleoperation%252C%250Aincreasing%2520both%2520speed%2520and%2520accuracy.%2520We%2520evaluate%2520DEXOP%2520across%2520a%2520range%2520of%250Adexterous%252C%2520contact-rich%2520tasks%252C%2520demonstrating%2520its%2520ability%2520to%2520collect%250Ahigh-quality%2520demonstration%2520data%2520at%2520scale.%2520Policies%2520learned%2520with%2520DEXOP%2520data%250Asignificantly%2520improve%2520task%2520performance%2520per%2520unit%2520time%2520of%2520data%2520collection%250Acompared%2520to%2520teleoperation%252C%2520making%2520DEXOP%2520a%2520powerful%2520tool%2520for%2520advancing%2520robot%250Adexterity.%2520Our%2520project%2520page%2520is%2520at%2520https%253A//dex-op.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04441v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DEXOP%3A%20A%20Device%20for%20Robotic%20Transfer%20of%20Dexterous%20Human%20Manipulation&entry.906535625=Hao-Shu%20Fang%20and%20Branden%20Romero%20and%20Yichen%20Xie%20and%20Arthur%20Hu%20and%20Bo-Ruei%20Huang%20and%20Juan%20Alvarez%20and%20Matthew%20Kim%20and%20Gabriel%20Margolis%20and%20Kavya%20Anbarasu%20and%20Masayoshi%20Tomizuka%20and%20Edward%20Adelson%20and%20Pulkit%20Agrawal&entry.1292438233=%20%20We%20introduce%20perioperation%2C%20a%20paradigm%20for%20robotic%20data%20collection%20that%0Asensorizes%20and%20records%20human%20manipulation%20while%20maximizing%20the%20transferability%0Aof%20the%20data%20to%20real%20robots.%20We%20implement%20this%20paradigm%20in%20DEXOP%2C%20a%20passive%20hand%0Aexoskeleton%20designed%20to%20maximize%20human%20ability%20to%20collect%20rich%20sensory%20%28vision%0A%2B%20tactile%29%20data%20for%20diverse%20dexterous%20manipulation%20tasks%20in%20natural%0Aenvironments.%20DEXOP%20mechanically%20connects%20human%20fingers%20to%20robot%20fingers%2C%0Aproviding%20users%20with%20direct%20contact%20feedback%20%28via%20proprioception%29%20and%20mirrors%0Athe%20human%20hand%20pose%20to%20the%20passive%20robot%20hand%20to%20maximize%20the%20transfer%20of%0Ademonstrated%20skills%20to%20the%20robot.%20The%20force%20feedback%20and%20pose%20mirroring%20make%0Atask%20demonstrations%20more%20natural%20for%20humans%20compared%20to%20teleoperation%2C%0Aincreasing%20both%20speed%20and%20accuracy.%20We%20evaluate%20DEXOP%20across%20a%20range%20of%0Adexterous%2C%20contact-rich%20tasks%2C%20demonstrating%20its%20ability%20to%20collect%0Ahigh-quality%20demonstration%20data%20at%20scale.%20Policies%20learned%20with%20DEXOP%20data%0Asignificantly%20improve%20task%20performance%20per%20unit%20time%20of%20data%20collection%0Acompared%20to%20teleoperation%2C%20making%20DEXOP%20a%20powerful%20tool%20for%20advancing%20robot%0Adexterity.%20Our%20project%20page%20is%20at%20https%3A//dex-op.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04441v1&entry.124074799=Read"},
{"title": "TaleDiffusion: Multi-Character Story Generation with Dialogue Rendering", "author": "Ayan Banerjee and Josep Llad\u00f3s and Umapada Pal and Anjan Dutta", "abstract": "  Text-to-story visualization is challenging due to the need for consistent\ninteraction among multiple characters across frames. Existing methods struggle\nwith character consistency, leading to artifact generation and inaccurate\ndialogue rendering, which results in disjointed storytelling. In response, we\nintroduce TaleDiffusion, a novel framework for generating multi-character\nstories with an iterative process, maintaining character consistency, and\naccurate dialogue assignment via postprocessing. Given a story, we use a\npre-trained LLM to generate per-frame descriptions, character details, and\ndialogues via in-context learning, followed by a bounded attention-based\nper-box mask technique to control character interactions and minimize\nartifacts. We then apply an identity-consistent self-attention mechanism to\nensure character consistency across frames and region-aware cross-attention for\nprecise object placement. Dialogues are also rendered as bubbles and assigned\nto characters via CLIPSeg. Experimental results demonstrate that TaleDiffusion\noutperforms existing methods in consistency, noise reduction, and dialogue\nrendering.\n", "link": "http://arxiv.org/abs/2509.04123v1", "date": "2025-09-04", "relevancy": 2.208, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5667}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5617}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TaleDiffusion%3A%20Multi-Character%20Story%20Generation%20with%20Dialogue%20Rendering&body=Title%3A%20TaleDiffusion%3A%20Multi-Character%20Story%20Generation%20with%20Dialogue%20Rendering%0AAuthor%3A%20Ayan%20Banerjee%20and%20Josep%20Llad%C3%B3s%20and%20Umapada%20Pal%20and%20Anjan%20Dutta%0AAbstract%3A%20%20%20Text-to-story%20visualization%20is%20challenging%20due%20to%20the%20need%20for%20consistent%0Ainteraction%20among%20multiple%20characters%20across%20frames.%20Existing%20methods%20struggle%0Awith%20character%20consistency%2C%20leading%20to%20artifact%20generation%20and%20inaccurate%0Adialogue%20rendering%2C%20which%20results%20in%20disjointed%20storytelling.%20In%20response%2C%20we%0Aintroduce%20TaleDiffusion%2C%20a%20novel%20framework%20for%20generating%20multi-character%0Astories%20with%20an%20iterative%20process%2C%20maintaining%20character%20consistency%2C%20and%0Aaccurate%20dialogue%20assignment%20via%20postprocessing.%20Given%20a%20story%2C%20we%20use%20a%0Apre-trained%20LLM%20to%20generate%20per-frame%20descriptions%2C%20character%20details%2C%20and%0Adialogues%20via%20in-context%20learning%2C%20followed%20by%20a%20bounded%20attention-based%0Aper-box%20mask%20technique%20to%20control%20character%20interactions%20and%20minimize%0Aartifacts.%20We%20then%20apply%20an%20identity-consistent%20self-attention%20mechanism%20to%0Aensure%20character%20consistency%20across%20frames%20and%20region-aware%20cross-attention%20for%0Aprecise%20object%20placement.%20Dialogues%20are%20also%20rendered%20as%20bubbles%20and%20assigned%0Ato%20characters%20via%20CLIPSeg.%20Experimental%20results%20demonstrate%20that%20TaleDiffusion%0Aoutperforms%20existing%20methods%20in%20consistency%2C%20noise%20reduction%2C%20and%20dialogue%0Arendering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04123v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaleDiffusion%253A%2520Multi-Character%2520Story%2520Generation%2520with%2520Dialogue%2520Rendering%26entry.906535625%3DAyan%2520Banerjee%2520and%2520Josep%2520Llad%25C3%25B3s%2520and%2520Umapada%2520Pal%2520and%2520Anjan%2520Dutta%26entry.1292438233%3D%2520%2520Text-to-story%2520visualization%2520is%2520challenging%2520due%2520to%2520the%2520need%2520for%2520consistent%250Ainteraction%2520among%2520multiple%2520characters%2520across%2520frames.%2520Existing%2520methods%2520struggle%250Awith%2520character%2520consistency%252C%2520leading%2520to%2520artifact%2520generation%2520and%2520inaccurate%250Adialogue%2520rendering%252C%2520which%2520results%2520in%2520disjointed%2520storytelling.%2520In%2520response%252C%2520we%250Aintroduce%2520TaleDiffusion%252C%2520a%2520novel%2520framework%2520for%2520generating%2520multi-character%250Astories%2520with%2520an%2520iterative%2520process%252C%2520maintaining%2520character%2520consistency%252C%2520and%250Aaccurate%2520dialogue%2520assignment%2520via%2520postprocessing.%2520Given%2520a%2520story%252C%2520we%2520use%2520a%250Apre-trained%2520LLM%2520to%2520generate%2520per-frame%2520descriptions%252C%2520character%2520details%252C%2520and%250Adialogues%2520via%2520in-context%2520learning%252C%2520followed%2520by%2520a%2520bounded%2520attention-based%250Aper-box%2520mask%2520technique%2520to%2520control%2520character%2520interactions%2520and%2520minimize%250Aartifacts.%2520We%2520then%2520apply%2520an%2520identity-consistent%2520self-attention%2520mechanism%2520to%250Aensure%2520character%2520consistency%2520across%2520frames%2520and%2520region-aware%2520cross-attention%2520for%250Aprecise%2520object%2520placement.%2520Dialogues%2520are%2520also%2520rendered%2520as%2520bubbles%2520and%2520assigned%250Ato%2520characters%2520via%2520CLIPSeg.%2520Experimental%2520results%2520demonstrate%2520that%2520TaleDiffusion%250Aoutperforms%2520existing%2520methods%2520in%2520consistency%252C%2520noise%2520reduction%252C%2520and%2520dialogue%250Arendering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04123v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TaleDiffusion%3A%20Multi-Character%20Story%20Generation%20with%20Dialogue%20Rendering&entry.906535625=Ayan%20Banerjee%20and%20Josep%20Llad%C3%B3s%20and%20Umapada%20Pal%20and%20Anjan%20Dutta&entry.1292438233=%20%20Text-to-story%20visualization%20is%20challenging%20due%20to%20the%20need%20for%20consistent%0Ainteraction%20among%20multiple%20characters%20across%20frames.%20Existing%20methods%20struggle%0Awith%20character%20consistency%2C%20leading%20to%20artifact%20generation%20and%20inaccurate%0Adialogue%20rendering%2C%20which%20results%20in%20disjointed%20storytelling.%20In%20response%2C%20we%0Aintroduce%20TaleDiffusion%2C%20a%20novel%20framework%20for%20generating%20multi-character%0Astories%20with%20an%20iterative%20process%2C%20maintaining%20character%20consistency%2C%20and%0Aaccurate%20dialogue%20assignment%20via%20postprocessing.%20Given%20a%20story%2C%20we%20use%20a%0Apre-trained%20LLM%20to%20generate%20per-frame%20descriptions%2C%20character%20details%2C%20and%0Adialogues%20via%20in-context%20learning%2C%20followed%20by%20a%20bounded%20attention-based%0Aper-box%20mask%20technique%20to%20control%20character%20interactions%20and%20minimize%0Aartifacts.%20We%20then%20apply%20an%20identity-consistent%20self-attention%20mechanism%20to%0Aensure%20character%20consistency%20across%20frames%20and%20region-aware%20cross-attention%20for%0Aprecise%20object%20placement.%20Dialogues%20are%20also%20rendered%20as%20bubbles%20and%20assigned%0Ato%20characters%20via%20CLIPSeg.%20Experimental%20results%20demonstrate%20that%20TaleDiffusion%0Aoutperforms%20existing%20methods%20in%20consistency%2C%20noise%20reduction%2C%20and%20dialogue%0Arendering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04123v1&entry.124074799=Read"},
{"title": "AnomalyLMM: Bridging Generative Knowledge and Discriminative Retrieval\n  for Text-Based Person Anomaly Search", "author": "Hao Ju and Hu Zhang and Zhedong Zheng", "abstract": "  With growing public safety demands, text-based person anomaly search has\nemerged as a critical task, aiming to retrieve individuals with abnormal\nbehaviors via natural language descriptions. Unlike conventional person search,\nthis task presents two unique challenges: (1) fine-grained cross-modal\nalignment between textual anomalies and visual behaviors, and (2) anomaly\nrecognition under sparse real-world samples. While Large Multi-modal Models\n(LMMs) excel in multi-modal understanding, their potential for fine-grained\nanomaly retrieval remains underexplored, hindered by: (1) a domain gap between\ngenerative knowledge and discriminative retrieval, and (2) the absence of\nefficient adaptation strategies for deployment. In this work, we propose\nAnomalyLMM, the first framework that harnesses LMMs for text-based person\nanomaly search. Our key contributions are: (1) A novel coarse-to-fine pipeline\nintegrating LMMs to bridge generative world knowledge with retrieval-centric\nanomaly detection; (2) A training-free adaptation cookbook featuring masked\ncross-modal prompting, behavioral saliency prediction, and knowledge-aware\nre-ranking, enabling zero-shot focus on subtle anomaly cues. As the first study\nto explore LMMs for this task, we conduct a rigorous evaluation on the PAB\ndataset, the only publicly available benchmark for text-based person anomaly\nsearch, with its curated real-world anomalies covering diverse scenarios (e.g.,\nfalling, collision, and being hit). Experiments show the effectiveness of the\nproposed method, surpassing the competitive baseline by +0.96% Recall@1\naccuracy. Notably, our method reveals interpretable alignment between textual\nanomalies and visual behaviors, validated via qualitative analysis. Our code\nand models will be released for future research.\n", "link": "http://arxiv.org/abs/2509.04376v1", "date": "2025-09-04", "relevancy": 2.2021, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5738}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5482}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnomalyLMM%3A%20Bridging%20Generative%20Knowledge%20and%20Discriminative%20Retrieval%0A%20%20for%20Text-Based%20Person%20Anomaly%20Search&body=Title%3A%20AnomalyLMM%3A%20Bridging%20Generative%20Knowledge%20and%20Discriminative%20Retrieval%0A%20%20for%20Text-Based%20Person%20Anomaly%20Search%0AAuthor%3A%20Hao%20Ju%20and%20Hu%20Zhang%20and%20Zhedong%20Zheng%0AAbstract%3A%20%20%20With%20growing%20public%20safety%20demands%2C%20text-based%20person%20anomaly%20search%20has%0Aemerged%20as%20a%20critical%20task%2C%20aiming%20to%20retrieve%20individuals%20with%20abnormal%0Abehaviors%20via%20natural%20language%20descriptions.%20Unlike%20conventional%20person%20search%2C%0Athis%20task%20presents%20two%20unique%20challenges%3A%20%281%29%20fine-grained%20cross-modal%0Aalignment%20between%20textual%20anomalies%20and%20visual%20behaviors%2C%20and%20%282%29%20anomaly%0Arecognition%20under%20sparse%20real-world%20samples.%20While%20Large%20Multi-modal%20Models%0A%28LMMs%29%20excel%20in%20multi-modal%20understanding%2C%20their%20potential%20for%20fine-grained%0Aanomaly%20retrieval%20remains%20underexplored%2C%20hindered%20by%3A%20%281%29%20a%20domain%20gap%20between%0Agenerative%20knowledge%20and%20discriminative%20retrieval%2C%20and%20%282%29%20the%20absence%20of%0Aefficient%20adaptation%20strategies%20for%20deployment.%20In%20this%20work%2C%20we%20propose%0AAnomalyLMM%2C%20the%20first%20framework%20that%20harnesses%20LMMs%20for%20text-based%20person%0Aanomaly%20search.%20Our%20key%20contributions%20are%3A%20%281%29%20A%20novel%20coarse-to-fine%20pipeline%0Aintegrating%20LMMs%20to%20bridge%20generative%20world%20knowledge%20with%20retrieval-centric%0Aanomaly%20detection%3B%20%282%29%20A%20training-free%20adaptation%20cookbook%20featuring%20masked%0Across-modal%20prompting%2C%20behavioral%20saliency%20prediction%2C%20and%20knowledge-aware%0Are-ranking%2C%20enabling%20zero-shot%20focus%20on%20subtle%20anomaly%20cues.%20As%20the%20first%20study%0Ato%20explore%20LMMs%20for%20this%20task%2C%20we%20conduct%20a%20rigorous%20evaluation%20on%20the%20PAB%0Adataset%2C%20the%20only%20publicly%20available%20benchmark%20for%20text-based%20person%20anomaly%0Asearch%2C%20with%20its%20curated%20real-world%20anomalies%20covering%20diverse%20scenarios%20%28e.g.%2C%0Afalling%2C%20collision%2C%20and%20being%20hit%29.%20Experiments%20show%20the%20effectiveness%20of%20the%0Aproposed%20method%2C%20surpassing%20the%20competitive%20baseline%20by%20%2B0.96%25%20Recall%401%0Aaccuracy.%20Notably%2C%20our%20method%20reveals%20interpretable%20alignment%20between%20textual%0Aanomalies%20and%20visual%20behaviors%2C%20validated%20via%20qualitative%20analysis.%20Our%20code%0Aand%20models%20will%20be%20released%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04376v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnomalyLMM%253A%2520Bridging%2520Generative%2520Knowledge%2520and%2520Discriminative%2520Retrieval%250A%2520%2520for%2520Text-Based%2520Person%2520Anomaly%2520Search%26entry.906535625%3DHao%2520Ju%2520and%2520Hu%2520Zhang%2520and%2520Zhedong%2520Zheng%26entry.1292438233%3D%2520%2520With%2520growing%2520public%2520safety%2520demands%252C%2520text-based%2520person%2520anomaly%2520search%2520has%250Aemerged%2520as%2520a%2520critical%2520task%252C%2520aiming%2520to%2520retrieve%2520individuals%2520with%2520abnormal%250Abehaviors%2520via%2520natural%2520language%2520descriptions.%2520Unlike%2520conventional%2520person%2520search%252C%250Athis%2520task%2520presents%2520two%2520unique%2520challenges%253A%2520%25281%2529%2520fine-grained%2520cross-modal%250Aalignment%2520between%2520textual%2520anomalies%2520and%2520visual%2520behaviors%252C%2520and%2520%25282%2529%2520anomaly%250Arecognition%2520under%2520sparse%2520real-world%2520samples.%2520While%2520Large%2520Multi-modal%2520Models%250A%2528LMMs%2529%2520excel%2520in%2520multi-modal%2520understanding%252C%2520their%2520potential%2520for%2520fine-grained%250Aanomaly%2520retrieval%2520remains%2520underexplored%252C%2520hindered%2520by%253A%2520%25281%2529%2520a%2520domain%2520gap%2520between%250Agenerative%2520knowledge%2520and%2520discriminative%2520retrieval%252C%2520and%2520%25282%2529%2520the%2520absence%2520of%250Aefficient%2520adaptation%2520strategies%2520for%2520deployment.%2520In%2520this%2520work%252C%2520we%2520propose%250AAnomalyLMM%252C%2520the%2520first%2520framework%2520that%2520harnesses%2520LMMs%2520for%2520text-based%2520person%250Aanomaly%2520search.%2520Our%2520key%2520contributions%2520are%253A%2520%25281%2529%2520A%2520novel%2520coarse-to-fine%2520pipeline%250Aintegrating%2520LMMs%2520to%2520bridge%2520generative%2520world%2520knowledge%2520with%2520retrieval-centric%250Aanomaly%2520detection%253B%2520%25282%2529%2520A%2520training-free%2520adaptation%2520cookbook%2520featuring%2520masked%250Across-modal%2520prompting%252C%2520behavioral%2520saliency%2520prediction%252C%2520and%2520knowledge-aware%250Are-ranking%252C%2520enabling%2520zero-shot%2520focus%2520on%2520subtle%2520anomaly%2520cues.%2520As%2520the%2520first%2520study%250Ato%2520explore%2520LMMs%2520for%2520this%2520task%252C%2520we%2520conduct%2520a%2520rigorous%2520evaluation%2520on%2520the%2520PAB%250Adataset%252C%2520the%2520only%2520publicly%2520available%2520benchmark%2520for%2520text-based%2520person%2520anomaly%250Asearch%252C%2520with%2520its%2520curated%2520real-world%2520anomalies%2520covering%2520diverse%2520scenarios%2520%2528e.g.%252C%250Afalling%252C%2520collision%252C%2520and%2520being%2520hit%2529.%2520Experiments%2520show%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520method%252C%2520surpassing%2520the%2520competitive%2520baseline%2520by%2520%252B0.96%2525%2520Recall%25401%250Aaccuracy.%2520Notably%252C%2520our%2520method%2520reveals%2520interpretable%2520alignment%2520between%2520textual%250Aanomalies%2520and%2520visual%2520behaviors%252C%2520validated%2520via%2520qualitative%2520analysis.%2520Our%2520code%250Aand%2520models%2520will%2520be%2520released%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04376v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnomalyLMM%3A%20Bridging%20Generative%20Knowledge%20and%20Discriminative%20Retrieval%0A%20%20for%20Text-Based%20Person%20Anomaly%20Search&entry.906535625=Hao%20Ju%20and%20Hu%20Zhang%20and%20Zhedong%20Zheng&entry.1292438233=%20%20With%20growing%20public%20safety%20demands%2C%20text-based%20person%20anomaly%20search%20has%0Aemerged%20as%20a%20critical%20task%2C%20aiming%20to%20retrieve%20individuals%20with%20abnormal%0Abehaviors%20via%20natural%20language%20descriptions.%20Unlike%20conventional%20person%20search%2C%0Athis%20task%20presents%20two%20unique%20challenges%3A%20%281%29%20fine-grained%20cross-modal%0Aalignment%20between%20textual%20anomalies%20and%20visual%20behaviors%2C%20and%20%282%29%20anomaly%0Arecognition%20under%20sparse%20real-world%20samples.%20While%20Large%20Multi-modal%20Models%0A%28LMMs%29%20excel%20in%20multi-modal%20understanding%2C%20their%20potential%20for%20fine-grained%0Aanomaly%20retrieval%20remains%20underexplored%2C%20hindered%20by%3A%20%281%29%20a%20domain%20gap%20between%0Agenerative%20knowledge%20and%20discriminative%20retrieval%2C%20and%20%282%29%20the%20absence%20of%0Aefficient%20adaptation%20strategies%20for%20deployment.%20In%20this%20work%2C%20we%20propose%0AAnomalyLMM%2C%20the%20first%20framework%20that%20harnesses%20LMMs%20for%20text-based%20person%0Aanomaly%20search.%20Our%20key%20contributions%20are%3A%20%281%29%20A%20novel%20coarse-to-fine%20pipeline%0Aintegrating%20LMMs%20to%20bridge%20generative%20world%20knowledge%20with%20retrieval-centric%0Aanomaly%20detection%3B%20%282%29%20A%20training-free%20adaptation%20cookbook%20featuring%20masked%0Across-modal%20prompting%2C%20behavioral%20saliency%20prediction%2C%20and%20knowledge-aware%0Are-ranking%2C%20enabling%20zero-shot%20focus%20on%20subtle%20anomaly%20cues.%20As%20the%20first%20study%0Ato%20explore%20LMMs%20for%20this%20task%2C%20we%20conduct%20a%20rigorous%20evaluation%20on%20the%20PAB%0Adataset%2C%20the%20only%20publicly%20available%20benchmark%20for%20text-based%20person%20anomaly%0Asearch%2C%20with%20its%20curated%20real-world%20anomalies%20covering%20diverse%20scenarios%20%28e.g.%2C%0Afalling%2C%20collision%2C%20and%20being%20hit%29.%20Experiments%20show%20the%20effectiveness%20of%20the%0Aproposed%20method%2C%20surpassing%20the%20competitive%20baseline%20by%20%2B0.96%25%20Recall%401%0Aaccuracy.%20Notably%2C%20our%20method%20reveals%20interpretable%20alignment%20between%20textual%0Aanomalies%20and%20visual%20behaviors%2C%20validated%20via%20qualitative%20analysis.%20Our%20code%0Aand%20models%20will%20be%20released%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04376v1&entry.124074799=Read"},
{"title": "EMMA: Scaling Mobile Manipulation via Egocentric Human Data", "author": "Lawrence Y. Zhu and Pranav Kuppili and Ryan Punamiya and Patcharapong Aphiwetsa and Dhruv Patel and Simar Kareer and Sehoon Ha and Danfei Xu", "abstract": "  Scaling mobile manipulation imitation learning is bottlenecked by expensive\nmobile robot teleoperation. We present Egocentric Mobile MAnipulation (EMMA),\nan end-to-end framework training mobile manipulation policies from human mobile\nmanipulation data with static robot data, sidestepping mobile teleoperation. To\naccomplish this, we co-train human full-body motion data with static robot\ndata. In our experiments across three real-world tasks, EMMA demonstrates\ncomparable performance to baselines trained on teleoperated mobile robot data\n(Mobile ALOHA), achieving higher or equivalent task performance in full task\nsuccess. We find that EMMA is able to generalize to new spatial configurations\nand scenes, and we observe positive performance scaling as we increase the\nhours of human data, opening new avenues for scalable robotic learning in\nreal-world environments. Details of this project can be found at\nhttps://ego-moma.github.io/.\n", "link": "http://arxiv.org/abs/2509.04443v1", "date": "2025-09-04", "relevancy": 2.1998, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5575}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5552}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EMMA%3A%20Scaling%20Mobile%20Manipulation%20via%20Egocentric%20Human%20Data&body=Title%3A%20EMMA%3A%20Scaling%20Mobile%20Manipulation%20via%20Egocentric%20Human%20Data%0AAuthor%3A%20Lawrence%20Y.%20Zhu%20and%20Pranav%20Kuppili%20and%20Ryan%20Punamiya%20and%20Patcharapong%20Aphiwetsa%20and%20Dhruv%20Patel%20and%20Simar%20Kareer%20and%20Sehoon%20Ha%20and%20Danfei%20Xu%0AAbstract%3A%20%20%20Scaling%20mobile%20manipulation%20imitation%20learning%20is%20bottlenecked%20by%20expensive%0Amobile%20robot%20teleoperation.%20We%20present%20Egocentric%20Mobile%20MAnipulation%20%28EMMA%29%2C%0Aan%20end-to-end%20framework%20training%20mobile%20manipulation%20policies%20from%20human%20mobile%0Amanipulation%20data%20with%20static%20robot%20data%2C%20sidestepping%20mobile%20teleoperation.%20To%0Aaccomplish%20this%2C%20we%20co-train%20human%20full-body%20motion%20data%20with%20static%20robot%0Adata.%20In%20our%20experiments%20across%20three%20real-world%20tasks%2C%20EMMA%20demonstrates%0Acomparable%20performance%20to%20baselines%20trained%20on%20teleoperated%20mobile%20robot%20data%0A%28Mobile%20ALOHA%29%2C%20achieving%20higher%20or%20equivalent%20task%20performance%20in%20full%20task%0Asuccess.%20We%20find%20that%20EMMA%20is%20able%20to%20generalize%20to%20new%20spatial%20configurations%0Aand%20scenes%2C%20and%20we%20observe%20positive%20performance%20scaling%20as%20we%20increase%20the%0Ahours%20of%20human%20data%2C%20opening%20new%20avenues%20for%20scalable%20robotic%20learning%20in%0Areal-world%20environments.%20Details%20of%20this%20project%20can%20be%20found%20at%0Ahttps%3A//ego-moma.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04443v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEMMA%253A%2520Scaling%2520Mobile%2520Manipulation%2520via%2520Egocentric%2520Human%2520Data%26entry.906535625%3DLawrence%2520Y.%2520Zhu%2520and%2520Pranav%2520Kuppili%2520and%2520Ryan%2520Punamiya%2520and%2520Patcharapong%2520Aphiwetsa%2520and%2520Dhruv%2520Patel%2520and%2520Simar%2520Kareer%2520and%2520Sehoon%2520Ha%2520and%2520Danfei%2520Xu%26entry.1292438233%3D%2520%2520Scaling%2520mobile%2520manipulation%2520imitation%2520learning%2520is%2520bottlenecked%2520by%2520expensive%250Amobile%2520robot%2520teleoperation.%2520We%2520present%2520Egocentric%2520Mobile%2520MAnipulation%2520%2528EMMA%2529%252C%250Aan%2520end-to-end%2520framework%2520training%2520mobile%2520manipulation%2520policies%2520from%2520human%2520mobile%250Amanipulation%2520data%2520with%2520static%2520robot%2520data%252C%2520sidestepping%2520mobile%2520teleoperation.%2520To%250Aaccomplish%2520this%252C%2520we%2520co-train%2520human%2520full-body%2520motion%2520data%2520with%2520static%2520robot%250Adata.%2520In%2520our%2520experiments%2520across%2520three%2520real-world%2520tasks%252C%2520EMMA%2520demonstrates%250Acomparable%2520performance%2520to%2520baselines%2520trained%2520on%2520teleoperated%2520mobile%2520robot%2520data%250A%2528Mobile%2520ALOHA%2529%252C%2520achieving%2520higher%2520or%2520equivalent%2520task%2520performance%2520in%2520full%2520task%250Asuccess.%2520We%2520find%2520that%2520EMMA%2520is%2520able%2520to%2520generalize%2520to%2520new%2520spatial%2520configurations%250Aand%2520scenes%252C%2520and%2520we%2520observe%2520positive%2520performance%2520scaling%2520as%2520we%2520increase%2520the%250Ahours%2520of%2520human%2520data%252C%2520opening%2520new%2520avenues%2520for%2520scalable%2520robotic%2520learning%2520in%250Areal-world%2520environments.%2520Details%2520of%2520this%2520project%2520can%2520be%2520found%2520at%250Ahttps%253A//ego-moma.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04443v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EMMA%3A%20Scaling%20Mobile%20Manipulation%20via%20Egocentric%20Human%20Data&entry.906535625=Lawrence%20Y.%20Zhu%20and%20Pranav%20Kuppili%20and%20Ryan%20Punamiya%20and%20Patcharapong%20Aphiwetsa%20and%20Dhruv%20Patel%20and%20Simar%20Kareer%20and%20Sehoon%20Ha%20and%20Danfei%20Xu&entry.1292438233=%20%20Scaling%20mobile%20manipulation%20imitation%20learning%20is%20bottlenecked%20by%20expensive%0Amobile%20robot%20teleoperation.%20We%20present%20Egocentric%20Mobile%20MAnipulation%20%28EMMA%29%2C%0Aan%20end-to-end%20framework%20training%20mobile%20manipulation%20policies%20from%20human%20mobile%0Amanipulation%20data%20with%20static%20robot%20data%2C%20sidestepping%20mobile%20teleoperation.%20To%0Aaccomplish%20this%2C%20we%20co-train%20human%20full-body%20motion%20data%20with%20static%20robot%0Adata.%20In%20our%20experiments%20across%20three%20real-world%20tasks%2C%20EMMA%20demonstrates%0Acomparable%20performance%20to%20baselines%20trained%20on%20teleoperated%20mobile%20robot%20data%0A%28Mobile%20ALOHA%29%2C%20achieving%20higher%20or%20equivalent%20task%20performance%20in%20full%20task%0Asuccess.%20We%20find%20that%20EMMA%20is%20able%20to%20generalize%20to%20new%20spatial%20configurations%0Aand%20scenes%2C%20and%20we%20observe%20positive%20performance%20scaling%20as%20we%20increase%20the%0Ahours%20of%20human%20data%2C%20opening%20new%20avenues%20for%20scalable%20robotic%20learning%20in%0Areal-world%20environments.%20Details%20of%20this%20project%20can%20be%20found%20at%0Ahttps%3A//ego-moma.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04443v1&entry.124074799=Read"},
{"title": "Taming High-Dimensional Dynamics: Learning Optimal Projections onto\n  Spectral Submanifolds", "author": "Hugo Buurmeijer and Luis A. Pabon and John Irvin Alora and Roshan S. Kaundinya and George Haller and Marco Pavone", "abstract": "  High-dimensional nonlinear systems pose considerable challenges for modeling\nand control across many domains, from fluid mechanics to advanced robotics.\nSuch systems are typically approximated with reduced-order models, which often\nrely on orthogonal projections, a simplification that may lead to large\nprediction errors. In this work, we derive optimality of fiber-aligned\nprojections onto spectral submanifolds, preserving the nonlinear geometric\nstructure and minimizing long-term prediction error. We propose a data-driven\nprocedure to learn these projections from trajectories and demonstrate its\neffectiveness through a 180-dimensional robotic system. Our reduced-order\nmodels achieve up to fivefold improvement in trajectory tracking accuracy under\nmodel predictive control compared to the state of the art.\n", "link": "http://arxiv.org/abs/2504.03157v2", "date": "2025-09-04", "relevancy": 2.1988, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5692}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5648}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Taming%20High-Dimensional%20Dynamics%3A%20Learning%20Optimal%20Projections%20onto%0A%20%20Spectral%20Submanifolds&body=Title%3A%20Taming%20High-Dimensional%20Dynamics%3A%20Learning%20Optimal%20Projections%20onto%0A%20%20Spectral%20Submanifolds%0AAuthor%3A%20Hugo%20Buurmeijer%20and%20Luis%20A.%20Pabon%20and%20John%20Irvin%20Alora%20and%20Roshan%20S.%20Kaundinya%20and%20George%20Haller%20and%20Marco%20Pavone%0AAbstract%3A%20%20%20High-dimensional%20nonlinear%20systems%20pose%20considerable%20challenges%20for%20modeling%0Aand%20control%20across%20many%20domains%2C%20from%20fluid%20mechanics%20to%20advanced%20robotics.%0ASuch%20systems%20are%20typically%20approximated%20with%20reduced-order%20models%2C%20which%20often%0Arely%20on%20orthogonal%20projections%2C%20a%20simplification%20that%20may%20lead%20to%20large%0Aprediction%20errors.%20In%20this%20work%2C%20we%20derive%20optimality%20of%20fiber-aligned%0Aprojections%20onto%20spectral%20submanifolds%2C%20preserving%20the%20nonlinear%20geometric%0Astructure%20and%20minimizing%20long-term%20prediction%20error.%20We%20propose%20a%20data-driven%0Aprocedure%20to%20learn%20these%20projections%20from%20trajectories%20and%20demonstrate%20its%0Aeffectiveness%20through%20a%20180-dimensional%20robotic%20system.%20Our%20reduced-order%0Amodels%20achieve%20up%20to%20fivefold%20improvement%20in%20trajectory%20tracking%20accuracy%20under%0Amodel%20predictive%20control%20compared%20to%20the%20state%20of%20the%20art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.03157v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaming%2520High-Dimensional%2520Dynamics%253A%2520Learning%2520Optimal%2520Projections%2520onto%250A%2520%2520Spectral%2520Submanifolds%26entry.906535625%3DHugo%2520Buurmeijer%2520and%2520Luis%2520A.%2520Pabon%2520and%2520John%2520Irvin%2520Alora%2520and%2520Roshan%2520S.%2520Kaundinya%2520and%2520George%2520Haller%2520and%2520Marco%2520Pavone%26entry.1292438233%3D%2520%2520High-dimensional%2520nonlinear%2520systems%2520pose%2520considerable%2520challenges%2520for%2520modeling%250Aand%2520control%2520across%2520many%2520domains%252C%2520from%2520fluid%2520mechanics%2520to%2520advanced%2520robotics.%250ASuch%2520systems%2520are%2520typically%2520approximated%2520with%2520reduced-order%2520models%252C%2520which%2520often%250Arely%2520on%2520orthogonal%2520projections%252C%2520a%2520simplification%2520that%2520may%2520lead%2520to%2520large%250Aprediction%2520errors.%2520In%2520this%2520work%252C%2520we%2520derive%2520optimality%2520of%2520fiber-aligned%250Aprojections%2520onto%2520spectral%2520submanifolds%252C%2520preserving%2520the%2520nonlinear%2520geometric%250Astructure%2520and%2520minimizing%2520long-term%2520prediction%2520error.%2520We%2520propose%2520a%2520data-driven%250Aprocedure%2520to%2520learn%2520these%2520projections%2520from%2520trajectories%2520and%2520demonstrate%2520its%250Aeffectiveness%2520through%2520a%2520180-dimensional%2520robotic%2520system.%2520Our%2520reduced-order%250Amodels%2520achieve%2520up%2520to%2520fivefold%2520improvement%2520in%2520trajectory%2520tracking%2520accuracy%2520under%250Amodel%2520predictive%2520control%2520compared%2520to%2520the%2520state%2520of%2520the%2520art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.03157v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Taming%20High-Dimensional%20Dynamics%3A%20Learning%20Optimal%20Projections%20onto%0A%20%20Spectral%20Submanifolds&entry.906535625=Hugo%20Buurmeijer%20and%20Luis%20A.%20Pabon%20and%20John%20Irvin%20Alora%20and%20Roshan%20S.%20Kaundinya%20and%20George%20Haller%20and%20Marco%20Pavone&entry.1292438233=%20%20High-dimensional%20nonlinear%20systems%20pose%20considerable%20challenges%20for%20modeling%0Aand%20control%20across%20many%20domains%2C%20from%20fluid%20mechanics%20to%20advanced%20robotics.%0ASuch%20systems%20are%20typically%20approximated%20with%20reduced-order%20models%2C%20which%20often%0Arely%20on%20orthogonal%20projections%2C%20a%20simplification%20that%20may%20lead%20to%20large%0Aprediction%20errors.%20In%20this%20work%2C%20we%20derive%20optimality%20of%20fiber-aligned%0Aprojections%20onto%20spectral%20submanifolds%2C%20preserving%20the%20nonlinear%20geometric%0Astructure%20and%20minimizing%20long-term%20prediction%20error.%20We%20propose%20a%20data-driven%0Aprocedure%20to%20learn%20these%20projections%20from%20trajectories%20and%20demonstrate%20its%0Aeffectiveness%20through%20a%20180-dimensional%20robotic%20system.%20Our%20reduced-order%0Amodels%20achieve%20up%20to%20fivefold%20improvement%20in%20trajectory%20tracking%20accuracy%20under%0Amodel%20predictive%20control%20compared%20to%20the%20state%20of%20the%20art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.03157v2&entry.124074799=Read"},
{"title": "Unobtrusive In-Situ Measurement of Behavior Change by Deep Metric\n  Similarity Learning of Motion Patterns", "author": "Christian Merz and Lukas Schach and Marie Luisa Fiedler and Jean-Luc Lugrin and Carolin Wienrich and Marc Erich Latoschik", "abstract": "  This paper introduces an unobtrusive in-situ measurement method to detect\nuser behavior changes during arbitrary exposures in XR systems. Here, such\nbehavior changes are typically associated with the Proteus effect or bodily\naffordances elicited by different avatars that the users embody in XR. We\npresent a biometric user model based on deep metric similarity learning, which\nuses high-dimensional embeddings as reference vectors to identify behavior\nchanges of individual users. We evaluate our model against two alternative\napproaches: a (non-learned) motion analysis based on central tendencies of\nmovement patterns and subjective post-exposure embodiment questionnaires\nfrequently used in various XR exposures. In a within-subject study,\nparticipants performed a fruit collection task while embodying avatars of\ndifferent body heights (short, actual-height, and tall). Subjective assessments\nconfirmed the effective manipulation of perceived body schema, while the\n(non-learned) objective analyses of head and hand movements revealed\nsignificant differences across conditions. Our similarity learning model\ntrained on the motion data successfully identified the elicited behavior change\nfor various query and reference data pairings of the avatar conditions. The\napproach has several advantages in comparison to existing methods: 1) In-situ\nmeasurement without additional user input, 2) generalizable and scalable motion\nanalysis for various use cases, 3) user-specific analysis on the individual\nlevel, and 4) with a trained model, users can be added and evaluated in real\ntime to study how avatar changes affect behavior.\n", "link": "http://arxiv.org/abs/2509.04174v1", "date": "2025-09-04", "relevancy": 2.1975, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5725}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.56}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unobtrusive%20In-Situ%20Measurement%20of%20Behavior%20Change%20by%20Deep%20Metric%0A%20%20Similarity%20Learning%20of%20Motion%20Patterns&body=Title%3A%20Unobtrusive%20In-Situ%20Measurement%20of%20Behavior%20Change%20by%20Deep%20Metric%0A%20%20Similarity%20Learning%20of%20Motion%20Patterns%0AAuthor%3A%20Christian%20Merz%20and%20Lukas%20Schach%20and%20Marie%20Luisa%20Fiedler%20and%20Jean-Luc%20Lugrin%20and%20Carolin%20Wienrich%20and%20Marc%20Erich%20Latoschik%0AAbstract%3A%20%20%20This%20paper%20introduces%20an%20unobtrusive%20in-situ%20measurement%20method%20to%20detect%0Auser%20behavior%20changes%20during%20arbitrary%20exposures%20in%20XR%20systems.%20Here%2C%20such%0Abehavior%20changes%20are%20typically%20associated%20with%20the%20Proteus%20effect%20or%20bodily%0Aaffordances%20elicited%20by%20different%20avatars%20that%20the%20users%20embody%20in%20XR.%20We%0Apresent%20a%20biometric%20user%20model%20based%20on%20deep%20metric%20similarity%20learning%2C%20which%0Auses%20high-dimensional%20embeddings%20as%20reference%20vectors%20to%20identify%20behavior%0Achanges%20of%20individual%20users.%20We%20evaluate%20our%20model%20against%20two%20alternative%0Aapproaches%3A%20a%20%28non-learned%29%20motion%20analysis%20based%20on%20central%20tendencies%20of%0Amovement%20patterns%20and%20subjective%20post-exposure%20embodiment%20questionnaires%0Afrequently%20used%20in%20various%20XR%20exposures.%20In%20a%20within-subject%20study%2C%0Aparticipants%20performed%20a%20fruit%20collection%20task%20while%20embodying%20avatars%20of%0Adifferent%20body%20heights%20%28short%2C%20actual-height%2C%20and%20tall%29.%20Subjective%20assessments%0Aconfirmed%20the%20effective%20manipulation%20of%20perceived%20body%20schema%2C%20while%20the%0A%28non-learned%29%20objective%20analyses%20of%20head%20and%20hand%20movements%20revealed%0Asignificant%20differences%20across%20conditions.%20Our%20similarity%20learning%20model%0Atrained%20on%20the%20motion%20data%20successfully%20identified%20the%20elicited%20behavior%20change%0Afor%20various%20query%20and%20reference%20data%20pairings%20of%20the%20avatar%20conditions.%20The%0Aapproach%20has%20several%20advantages%20in%20comparison%20to%20existing%20methods%3A%201%29%20In-situ%0Ameasurement%20without%20additional%20user%20input%2C%202%29%20generalizable%20and%20scalable%20motion%0Aanalysis%20for%20various%20use%20cases%2C%203%29%20user-specific%20analysis%20on%20the%20individual%0Alevel%2C%20and%204%29%20with%20a%20trained%20model%2C%20users%20can%20be%20added%20and%20evaluated%20in%20real%0Atime%20to%20study%20how%20avatar%20changes%20affect%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04174v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnobtrusive%2520In-Situ%2520Measurement%2520of%2520Behavior%2520Change%2520by%2520Deep%2520Metric%250A%2520%2520Similarity%2520Learning%2520of%2520Motion%2520Patterns%26entry.906535625%3DChristian%2520Merz%2520and%2520Lukas%2520Schach%2520and%2520Marie%2520Luisa%2520Fiedler%2520and%2520Jean-Luc%2520Lugrin%2520and%2520Carolin%2520Wienrich%2520and%2520Marc%2520Erich%2520Latoschik%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520an%2520unobtrusive%2520in-situ%2520measurement%2520method%2520to%2520detect%250Auser%2520behavior%2520changes%2520during%2520arbitrary%2520exposures%2520in%2520XR%2520systems.%2520Here%252C%2520such%250Abehavior%2520changes%2520are%2520typically%2520associated%2520with%2520the%2520Proteus%2520effect%2520or%2520bodily%250Aaffordances%2520elicited%2520by%2520different%2520avatars%2520that%2520the%2520users%2520embody%2520in%2520XR.%2520We%250Apresent%2520a%2520biometric%2520user%2520model%2520based%2520on%2520deep%2520metric%2520similarity%2520learning%252C%2520which%250Auses%2520high-dimensional%2520embeddings%2520as%2520reference%2520vectors%2520to%2520identify%2520behavior%250Achanges%2520of%2520individual%2520users.%2520We%2520evaluate%2520our%2520model%2520against%2520two%2520alternative%250Aapproaches%253A%2520a%2520%2528non-learned%2529%2520motion%2520analysis%2520based%2520on%2520central%2520tendencies%2520of%250Amovement%2520patterns%2520and%2520subjective%2520post-exposure%2520embodiment%2520questionnaires%250Afrequently%2520used%2520in%2520various%2520XR%2520exposures.%2520In%2520a%2520within-subject%2520study%252C%250Aparticipants%2520performed%2520a%2520fruit%2520collection%2520task%2520while%2520embodying%2520avatars%2520of%250Adifferent%2520body%2520heights%2520%2528short%252C%2520actual-height%252C%2520and%2520tall%2529.%2520Subjective%2520assessments%250Aconfirmed%2520the%2520effective%2520manipulation%2520of%2520perceived%2520body%2520schema%252C%2520while%2520the%250A%2528non-learned%2529%2520objective%2520analyses%2520of%2520head%2520and%2520hand%2520movements%2520revealed%250Asignificant%2520differences%2520across%2520conditions.%2520Our%2520similarity%2520learning%2520model%250Atrained%2520on%2520the%2520motion%2520data%2520successfully%2520identified%2520the%2520elicited%2520behavior%2520change%250Afor%2520various%2520query%2520and%2520reference%2520data%2520pairings%2520of%2520the%2520avatar%2520conditions.%2520The%250Aapproach%2520has%2520several%2520advantages%2520in%2520comparison%2520to%2520existing%2520methods%253A%25201%2529%2520In-situ%250Ameasurement%2520without%2520additional%2520user%2520input%252C%25202%2529%2520generalizable%2520and%2520scalable%2520motion%250Aanalysis%2520for%2520various%2520use%2520cases%252C%25203%2529%2520user-specific%2520analysis%2520on%2520the%2520individual%250Alevel%252C%2520and%25204%2529%2520with%2520a%2520trained%2520model%252C%2520users%2520can%2520be%2520added%2520and%2520evaluated%2520in%2520real%250Atime%2520to%2520study%2520how%2520avatar%2520changes%2520affect%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04174v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unobtrusive%20In-Situ%20Measurement%20of%20Behavior%20Change%20by%20Deep%20Metric%0A%20%20Similarity%20Learning%20of%20Motion%20Patterns&entry.906535625=Christian%20Merz%20and%20Lukas%20Schach%20and%20Marie%20Luisa%20Fiedler%20and%20Jean-Luc%20Lugrin%20and%20Carolin%20Wienrich%20and%20Marc%20Erich%20Latoschik&entry.1292438233=%20%20This%20paper%20introduces%20an%20unobtrusive%20in-situ%20measurement%20method%20to%20detect%0Auser%20behavior%20changes%20during%20arbitrary%20exposures%20in%20XR%20systems.%20Here%2C%20such%0Abehavior%20changes%20are%20typically%20associated%20with%20the%20Proteus%20effect%20or%20bodily%0Aaffordances%20elicited%20by%20different%20avatars%20that%20the%20users%20embody%20in%20XR.%20We%0Apresent%20a%20biometric%20user%20model%20based%20on%20deep%20metric%20similarity%20learning%2C%20which%0Auses%20high-dimensional%20embeddings%20as%20reference%20vectors%20to%20identify%20behavior%0Achanges%20of%20individual%20users.%20We%20evaluate%20our%20model%20against%20two%20alternative%0Aapproaches%3A%20a%20%28non-learned%29%20motion%20analysis%20based%20on%20central%20tendencies%20of%0Amovement%20patterns%20and%20subjective%20post-exposure%20embodiment%20questionnaires%0Afrequently%20used%20in%20various%20XR%20exposures.%20In%20a%20within-subject%20study%2C%0Aparticipants%20performed%20a%20fruit%20collection%20task%20while%20embodying%20avatars%20of%0Adifferent%20body%20heights%20%28short%2C%20actual-height%2C%20and%20tall%29.%20Subjective%20assessments%0Aconfirmed%20the%20effective%20manipulation%20of%20perceived%20body%20schema%2C%20while%20the%0A%28non-learned%29%20objective%20analyses%20of%20head%20and%20hand%20movements%20revealed%0Asignificant%20differences%20across%20conditions.%20Our%20similarity%20learning%20model%0Atrained%20on%20the%20motion%20data%20successfully%20identified%20the%20elicited%20behavior%20change%0Afor%20various%20query%20and%20reference%20data%20pairings%20of%20the%20avatar%20conditions.%20The%0Aapproach%20has%20several%20advantages%20in%20comparison%20to%20existing%20methods%3A%201%29%20In-situ%0Ameasurement%20without%20additional%20user%20input%2C%202%29%20generalizable%20and%20scalable%20motion%0Aanalysis%20for%20various%20use%20cases%2C%203%29%20user-specific%20analysis%20on%20the%20individual%0Alevel%2C%20and%204%29%20with%20a%20trained%20model%2C%20users%20can%20be%20added%20and%20evaluated%20in%20real%0Atime%20to%20study%20how%20avatar%20changes%20affect%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04174v1&entry.124074799=Read"},
{"title": "Differential Morphological Profile Neural Networks for Semantic\n  Segmentation", "author": "David Huangal and J. Alex Hurt", "abstract": "  Semantic segmentation of overhead remote sensing imagery enables applications\nin mapping, urban planning, and disaster response. State-of-the-art\nsegmentation networks are typically developed and tuned on ground-perspective\nphotographs and do not directly address remote sensing challenges such as\nextreme scale variation, foreground-background imbalance, and large image\nsizes. We explore the incorporation of the differential morphological profile\n(DMP), a multi-scale shape extraction method based on grayscale morphology,\ninto modern segmentation networks. Prior studies have shown that the DMP can\nprovide critical shape information to Deep Neural Networks to enable superior\ndetection and classification performance in overhead imagery. In this work, we\nextend prior DMPNet work beyond classification and object detection by\nintegrating DMP features into three state-of-the-art convolutional and\ntransformer semantic segmentation architectures. We utilize both direct input,\nwhich adapts the input stem of feature extraction architectures to accept DMP\nchannels, and hybrid architectures, a dual-stream design that fuses RGB and DMP\nencoders. Using the iSAID benchmark dataset, we evaluate a variety of DMP\ndifferentials and structuring element shapes to more effectively provide shape\ninformation to the model. Our results show that while non-DMP models generally\noutperform the direct-input variants, hybrid DMP consistently outperforms\ndirect-input and is capable of surpassing a non-DMP model on mIoU, F1, and\nRecall.\n", "link": "http://arxiv.org/abs/2509.04268v1", "date": "2025-09-04", "relevancy": 2.1941, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.549}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5484}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differential%20Morphological%20Profile%20Neural%20Networks%20for%20Semantic%0A%20%20Segmentation&body=Title%3A%20Differential%20Morphological%20Profile%20Neural%20Networks%20for%20Semantic%0A%20%20Segmentation%0AAuthor%3A%20David%20Huangal%20and%20J.%20Alex%20Hurt%0AAbstract%3A%20%20%20Semantic%20segmentation%20of%20overhead%20remote%20sensing%20imagery%20enables%20applications%0Ain%20mapping%2C%20urban%20planning%2C%20and%20disaster%20response.%20State-of-the-art%0Asegmentation%20networks%20are%20typically%20developed%20and%20tuned%20on%20ground-perspective%0Aphotographs%20and%20do%20not%20directly%20address%20remote%20sensing%20challenges%20such%20as%0Aextreme%20scale%20variation%2C%20foreground-background%20imbalance%2C%20and%20large%20image%0Asizes.%20We%20explore%20the%20incorporation%20of%20the%20differential%20morphological%20profile%0A%28DMP%29%2C%20a%20multi-scale%20shape%20extraction%20method%20based%20on%20grayscale%20morphology%2C%0Ainto%20modern%20segmentation%20networks.%20Prior%20studies%20have%20shown%20that%20the%20DMP%20can%0Aprovide%20critical%20shape%20information%20to%20Deep%20Neural%20Networks%20to%20enable%20superior%0Adetection%20and%20classification%20performance%20in%20overhead%20imagery.%20In%20this%20work%2C%20we%0Aextend%20prior%20DMPNet%20work%20beyond%20classification%20and%20object%20detection%20by%0Aintegrating%20DMP%20features%20into%20three%20state-of-the-art%20convolutional%20and%0Atransformer%20semantic%20segmentation%20architectures.%20We%20utilize%20both%20direct%20input%2C%0Awhich%20adapts%20the%20input%20stem%20of%20feature%20extraction%20architectures%20to%20accept%20DMP%0Achannels%2C%20and%20hybrid%20architectures%2C%20a%20dual-stream%20design%20that%20fuses%20RGB%20and%20DMP%0Aencoders.%20Using%20the%20iSAID%20benchmark%20dataset%2C%20we%20evaluate%20a%20variety%20of%20DMP%0Adifferentials%20and%20structuring%20element%20shapes%20to%20more%20effectively%20provide%20shape%0Ainformation%20to%20the%20model.%20Our%20results%20show%20that%20while%20non-DMP%20models%20generally%0Aoutperform%20the%20direct-input%20variants%2C%20hybrid%20DMP%20consistently%20outperforms%0Adirect-input%20and%20is%20capable%20of%20surpassing%20a%20non-DMP%20model%20on%20mIoU%2C%20F1%2C%20and%0ARecall.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04268v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferential%2520Morphological%2520Profile%2520Neural%2520Networks%2520for%2520Semantic%250A%2520%2520Segmentation%26entry.906535625%3DDavid%2520Huangal%2520and%2520J.%2520Alex%2520Hurt%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520of%2520overhead%2520remote%2520sensing%2520imagery%2520enables%2520applications%250Ain%2520mapping%252C%2520urban%2520planning%252C%2520and%2520disaster%2520response.%2520State-of-the-art%250Asegmentation%2520networks%2520are%2520typically%2520developed%2520and%2520tuned%2520on%2520ground-perspective%250Aphotographs%2520and%2520do%2520not%2520directly%2520address%2520remote%2520sensing%2520challenges%2520such%2520as%250Aextreme%2520scale%2520variation%252C%2520foreground-background%2520imbalance%252C%2520and%2520large%2520image%250Asizes.%2520We%2520explore%2520the%2520incorporation%2520of%2520the%2520differential%2520morphological%2520profile%250A%2528DMP%2529%252C%2520a%2520multi-scale%2520shape%2520extraction%2520method%2520based%2520on%2520grayscale%2520morphology%252C%250Ainto%2520modern%2520segmentation%2520networks.%2520Prior%2520studies%2520have%2520shown%2520that%2520the%2520DMP%2520can%250Aprovide%2520critical%2520shape%2520information%2520to%2520Deep%2520Neural%2520Networks%2520to%2520enable%2520superior%250Adetection%2520and%2520classification%2520performance%2520in%2520overhead%2520imagery.%2520In%2520this%2520work%252C%2520we%250Aextend%2520prior%2520DMPNet%2520work%2520beyond%2520classification%2520and%2520object%2520detection%2520by%250Aintegrating%2520DMP%2520features%2520into%2520three%2520state-of-the-art%2520convolutional%2520and%250Atransformer%2520semantic%2520segmentation%2520architectures.%2520We%2520utilize%2520both%2520direct%2520input%252C%250Awhich%2520adapts%2520the%2520input%2520stem%2520of%2520feature%2520extraction%2520architectures%2520to%2520accept%2520DMP%250Achannels%252C%2520and%2520hybrid%2520architectures%252C%2520a%2520dual-stream%2520design%2520that%2520fuses%2520RGB%2520and%2520DMP%250Aencoders.%2520Using%2520the%2520iSAID%2520benchmark%2520dataset%252C%2520we%2520evaluate%2520a%2520variety%2520of%2520DMP%250Adifferentials%2520and%2520structuring%2520element%2520shapes%2520to%2520more%2520effectively%2520provide%2520shape%250Ainformation%2520to%2520the%2520model.%2520Our%2520results%2520show%2520that%2520while%2520non-DMP%2520models%2520generally%250Aoutperform%2520the%2520direct-input%2520variants%252C%2520hybrid%2520DMP%2520consistently%2520outperforms%250Adirect-input%2520and%2520is%2520capable%2520of%2520surpassing%2520a%2520non-DMP%2520model%2520on%2520mIoU%252C%2520F1%252C%2520and%250ARecall.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04268v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differential%20Morphological%20Profile%20Neural%20Networks%20for%20Semantic%0A%20%20Segmentation&entry.906535625=David%20Huangal%20and%20J.%20Alex%20Hurt&entry.1292438233=%20%20Semantic%20segmentation%20of%20overhead%20remote%20sensing%20imagery%20enables%20applications%0Ain%20mapping%2C%20urban%20planning%2C%20and%20disaster%20response.%20State-of-the-art%0Asegmentation%20networks%20are%20typically%20developed%20and%20tuned%20on%20ground-perspective%0Aphotographs%20and%20do%20not%20directly%20address%20remote%20sensing%20challenges%20such%20as%0Aextreme%20scale%20variation%2C%20foreground-background%20imbalance%2C%20and%20large%20image%0Asizes.%20We%20explore%20the%20incorporation%20of%20the%20differential%20morphological%20profile%0A%28DMP%29%2C%20a%20multi-scale%20shape%20extraction%20method%20based%20on%20grayscale%20morphology%2C%0Ainto%20modern%20segmentation%20networks.%20Prior%20studies%20have%20shown%20that%20the%20DMP%20can%0Aprovide%20critical%20shape%20information%20to%20Deep%20Neural%20Networks%20to%20enable%20superior%0Adetection%20and%20classification%20performance%20in%20overhead%20imagery.%20In%20this%20work%2C%20we%0Aextend%20prior%20DMPNet%20work%20beyond%20classification%20and%20object%20detection%20by%0Aintegrating%20DMP%20features%20into%20three%20state-of-the-art%20convolutional%20and%0Atransformer%20semantic%20segmentation%20architectures.%20We%20utilize%20both%20direct%20input%2C%0Awhich%20adapts%20the%20input%20stem%20of%20feature%20extraction%20architectures%20to%20accept%20DMP%0Achannels%2C%20and%20hybrid%20architectures%2C%20a%20dual-stream%20design%20that%20fuses%20RGB%20and%20DMP%0Aencoders.%20Using%20the%20iSAID%20benchmark%20dataset%2C%20we%20evaluate%20a%20variety%20of%20DMP%0Adifferentials%20and%20structuring%20element%20shapes%20to%20more%20effectively%20provide%20shape%0Ainformation%20to%20the%20model.%20Our%20results%20show%20that%20while%20non-DMP%20models%20generally%0Aoutperform%20the%20direct-input%20variants%2C%20hybrid%20DMP%20consistently%20outperforms%0Adirect-input%20and%20is%20capable%20of%20surpassing%20a%20non-DMP%20model%20on%20mIoU%2C%20F1%2C%20and%0ARecall.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04268v1&entry.124074799=Read"},
{"title": "Kolb-Based Experiential Learning for Generalist Agents with Human-Level\n  Kaggle Data Science Performance", "author": "Antoine Grosnit and Alexandre Maraval and Refinath S N and Zichao Zhao and James Dora and Giuseppe Paolo and Albert Thomas and Jonas Gonzalez and Abhineet Kumar and Khyati Khandelwal and Abdelhakim Benechehab and Hamza Cherkaoui and Youssef Attia El-Hili and Kun Shao and Jianye Hao and Jun Yao and Bal\u00e1zs K\u00e9gl and Jun Wang", "abstract": "  Human expertise emerges through iterative cycles of interaction, reflection,\nand internal model updating, which are central to cognitive theories such as\nKolb's experiential learning and Vygotsky's zone of proximal development. In\ncontrast, current AI systems, particularly LLM agents, rely on static\npre-training or rigid workflows, lacking mechanisms for continual adaptation.\nRecent studies identified early cognitive traits in LLM agents (reflection,\nrevision, and self-correction) suggesting foundational elements of human-like\nexperiential learning. Thus the key question: Can we design LLM agents capable\nof structured, cognitively grounded learning similar to human processes? In\nresponse, we propose a computational framework of Kolb's learning cycle with\nVygotsky's ZPD for autonomous agents. Our architecture separates extrinsic\n(environment interaction) and intrinsic (internal reflection/abstraction)\nfunctions, enabling cognitively grounded scaffolded learning, where the agent\ninitially learns within structured environments, followed by open-ended\ngeneralisation. This approach empowers agents to master complex tasks ; domains\nthat traditional fine-tuning or simple reflective methods could not tackle\neffectively. Its potential is powerfully demonstrated via direct comparison\nwith humans in real-world Kaggle data science competitions. Learning fully\nautomated data science code generation across 81 tasks, our system, Agent K,\ndemonstrated the ability to perform the entire workflow autonomously, achieving\nan Elo-MMR score of 1694, beyond median score of the Kaggle Masters (the top 2%\namong 200,000 users) of our study. With 9 gold, 8 silver, and 12 bronze medals\nlevel performance - including 4 gold and 4 silver on prize-awarding\ncompetitions - Agent K is the 1st AI system to successfully integrate Kolb- and\nVygotsky-inspired human cognitive learning, marking a major step toward\ngeneralist AI.\n", "link": "http://arxiv.org/abs/2411.03562v2", "date": "2025-09-04", "relevancy": 2.1844, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5667}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5456}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kolb-Based%20Experiential%20Learning%20for%20Generalist%20Agents%20with%20Human-Level%0A%20%20Kaggle%20Data%20Science%20Performance&body=Title%3A%20Kolb-Based%20Experiential%20Learning%20for%20Generalist%20Agents%20with%20Human-Level%0A%20%20Kaggle%20Data%20Science%20Performance%0AAuthor%3A%20Antoine%20Grosnit%20and%20Alexandre%20Maraval%20and%20Refinath%20S%20N%20and%20Zichao%20Zhao%20and%20James%20Dora%20and%20Giuseppe%20Paolo%20and%20Albert%20Thomas%20and%20Jonas%20Gonzalez%20and%20Abhineet%20Kumar%20and%20Khyati%20Khandelwal%20and%20Abdelhakim%20Benechehab%20and%20Hamza%20Cherkaoui%20and%20Youssef%20Attia%20El-Hili%20and%20Kun%20Shao%20and%20Jianye%20Hao%20and%20Jun%20Yao%20and%20Bal%C3%A1zs%20K%C3%A9gl%20and%20Jun%20Wang%0AAbstract%3A%20%20%20Human%20expertise%20emerges%20through%20iterative%20cycles%20of%20interaction%2C%20reflection%2C%0Aand%20internal%20model%20updating%2C%20which%20are%20central%20to%20cognitive%20theories%20such%20as%0AKolb%27s%20experiential%20learning%20and%20Vygotsky%27s%20zone%20of%20proximal%20development.%20In%0Acontrast%2C%20current%20AI%20systems%2C%20particularly%20LLM%20agents%2C%20rely%20on%20static%0Apre-training%20or%20rigid%20workflows%2C%20lacking%20mechanisms%20for%20continual%20adaptation.%0ARecent%20studies%20identified%20early%20cognitive%20traits%20in%20LLM%20agents%20%28reflection%2C%0Arevision%2C%20and%20self-correction%29%20suggesting%20foundational%20elements%20of%20human-like%0Aexperiential%20learning.%20Thus%20the%20key%20question%3A%20Can%20we%20design%20LLM%20agents%20capable%0Aof%20structured%2C%20cognitively%20grounded%20learning%20similar%20to%20human%20processes%3F%20In%0Aresponse%2C%20we%20propose%20a%20computational%20framework%20of%20Kolb%27s%20learning%20cycle%20with%0AVygotsky%27s%20ZPD%20for%20autonomous%20agents.%20Our%20architecture%20separates%20extrinsic%0A%28environment%20interaction%29%20and%20intrinsic%20%28internal%20reflection/abstraction%29%0Afunctions%2C%20enabling%20cognitively%20grounded%20scaffolded%20learning%2C%20where%20the%20agent%0Ainitially%20learns%20within%20structured%20environments%2C%20followed%20by%20open-ended%0Ageneralisation.%20This%20approach%20empowers%20agents%20to%20master%20complex%20tasks%20%3B%20domains%0Athat%20traditional%20fine-tuning%20or%20simple%20reflective%20methods%20could%20not%20tackle%0Aeffectively.%20Its%20potential%20is%20powerfully%20demonstrated%20via%20direct%20comparison%0Awith%20humans%20in%20real-world%20Kaggle%20data%20science%20competitions.%20Learning%20fully%0Aautomated%20data%20science%20code%20generation%20across%2081%20tasks%2C%20our%20system%2C%20Agent%20K%2C%0Ademonstrated%20the%20ability%20to%20perform%20the%20entire%20workflow%20autonomously%2C%20achieving%0Aan%20Elo-MMR%20score%20of%201694%2C%20beyond%20median%20score%20of%20the%20Kaggle%20Masters%20%28the%20top%202%25%0Aamong%20200%2C000%20users%29%20of%20our%20study.%20With%209%20gold%2C%208%20silver%2C%20and%2012%20bronze%20medals%0Alevel%20performance%20-%20including%204%20gold%20and%204%20silver%20on%20prize-awarding%0Acompetitions%20-%20Agent%20K%20is%20the%201st%20AI%20system%20to%20successfully%20integrate%20Kolb-%20and%0AVygotsky-inspired%20human%20cognitive%20learning%2C%20marking%20a%20major%20step%20toward%0Ageneralist%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03562v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKolb-Based%2520Experiential%2520Learning%2520for%2520Generalist%2520Agents%2520with%2520Human-Level%250A%2520%2520Kaggle%2520Data%2520Science%2520Performance%26entry.906535625%3DAntoine%2520Grosnit%2520and%2520Alexandre%2520Maraval%2520and%2520Refinath%2520S%2520N%2520and%2520Zichao%2520Zhao%2520and%2520James%2520Dora%2520and%2520Giuseppe%2520Paolo%2520and%2520Albert%2520Thomas%2520and%2520Jonas%2520Gonzalez%2520and%2520Abhineet%2520Kumar%2520and%2520Khyati%2520Khandelwal%2520and%2520Abdelhakim%2520Benechehab%2520and%2520Hamza%2520Cherkaoui%2520and%2520Youssef%2520Attia%2520El-Hili%2520and%2520Kun%2520Shao%2520and%2520Jianye%2520Hao%2520and%2520Jun%2520Yao%2520and%2520Bal%25C3%25A1zs%2520K%25C3%25A9gl%2520and%2520Jun%2520Wang%26entry.1292438233%3D%2520%2520Human%2520expertise%2520emerges%2520through%2520iterative%2520cycles%2520of%2520interaction%252C%2520reflection%252C%250Aand%2520internal%2520model%2520updating%252C%2520which%2520are%2520central%2520to%2520cognitive%2520theories%2520such%2520as%250AKolb%2527s%2520experiential%2520learning%2520and%2520Vygotsky%2527s%2520zone%2520of%2520proximal%2520development.%2520In%250Acontrast%252C%2520current%2520AI%2520systems%252C%2520particularly%2520LLM%2520agents%252C%2520rely%2520on%2520static%250Apre-training%2520or%2520rigid%2520workflows%252C%2520lacking%2520mechanisms%2520for%2520continual%2520adaptation.%250ARecent%2520studies%2520identified%2520early%2520cognitive%2520traits%2520in%2520LLM%2520agents%2520%2528reflection%252C%250Arevision%252C%2520and%2520self-correction%2529%2520suggesting%2520foundational%2520elements%2520of%2520human-like%250Aexperiential%2520learning.%2520Thus%2520the%2520key%2520question%253A%2520Can%2520we%2520design%2520LLM%2520agents%2520capable%250Aof%2520structured%252C%2520cognitively%2520grounded%2520learning%2520similar%2520to%2520human%2520processes%253F%2520In%250Aresponse%252C%2520we%2520propose%2520a%2520computational%2520framework%2520of%2520Kolb%2527s%2520learning%2520cycle%2520with%250AVygotsky%2527s%2520ZPD%2520for%2520autonomous%2520agents.%2520Our%2520architecture%2520separates%2520extrinsic%250A%2528environment%2520interaction%2529%2520and%2520intrinsic%2520%2528internal%2520reflection/abstraction%2529%250Afunctions%252C%2520enabling%2520cognitively%2520grounded%2520scaffolded%2520learning%252C%2520where%2520the%2520agent%250Ainitially%2520learns%2520within%2520structured%2520environments%252C%2520followed%2520by%2520open-ended%250Ageneralisation.%2520This%2520approach%2520empowers%2520agents%2520to%2520master%2520complex%2520tasks%2520%253B%2520domains%250Athat%2520traditional%2520fine-tuning%2520or%2520simple%2520reflective%2520methods%2520could%2520not%2520tackle%250Aeffectively.%2520Its%2520potential%2520is%2520powerfully%2520demonstrated%2520via%2520direct%2520comparison%250Awith%2520humans%2520in%2520real-world%2520Kaggle%2520data%2520science%2520competitions.%2520Learning%2520fully%250Aautomated%2520data%2520science%2520code%2520generation%2520across%252081%2520tasks%252C%2520our%2520system%252C%2520Agent%2520K%252C%250Ademonstrated%2520the%2520ability%2520to%2520perform%2520the%2520entire%2520workflow%2520autonomously%252C%2520achieving%250Aan%2520Elo-MMR%2520score%2520of%25201694%252C%2520beyond%2520median%2520score%2520of%2520the%2520Kaggle%2520Masters%2520%2528the%2520top%25202%2525%250Aamong%2520200%252C000%2520users%2529%2520of%2520our%2520study.%2520With%25209%2520gold%252C%25208%2520silver%252C%2520and%252012%2520bronze%2520medals%250Alevel%2520performance%2520-%2520including%25204%2520gold%2520and%25204%2520silver%2520on%2520prize-awarding%250Acompetitions%2520-%2520Agent%2520K%2520is%2520the%25201st%2520AI%2520system%2520to%2520successfully%2520integrate%2520Kolb-%2520and%250AVygotsky-inspired%2520human%2520cognitive%2520learning%252C%2520marking%2520a%2520major%2520step%2520toward%250Ageneralist%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03562v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kolb-Based%20Experiential%20Learning%20for%20Generalist%20Agents%20with%20Human-Level%0A%20%20Kaggle%20Data%20Science%20Performance&entry.906535625=Antoine%20Grosnit%20and%20Alexandre%20Maraval%20and%20Refinath%20S%20N%20and%20Zichao%20Zhao%20and%20James%20Dora%20and%20Giuseppe%20Paolo%20and%20Albert%20Thomas%20and%20Jonas%20Gonzalez%20and%20Abhineet%20Kumar%20and%20Khyati%20Khandelwal%20and%20Abdelhakim%20Benechehab%20and%20Hamza%20Cherkaoui%20and%20Youssef%20Attia%20El-Hili%20and%20Kun%20Shao%20and%20Jianye%20Hao%20and%20Jun%20Yao%20and%20Bal%C3%A1zs%20K%C3%A9gl%20and%20Jun%20Wang&entry.1292438233=%20%20Human%20expertise%20emerges%20through%20iterative%20cycles%20of%20interaction%2C%20reflection%2C%0Aand%20internal%20model%20updating%2C%20which%20are%20central%20to%20cognitive%20theories%20such%20as%0AKolb%27s%20experiential%20learning%20and%20Vygotsky%27s%20zone%20of%20proximal%20development.%20In%0Acontrast%2C%20current%20AI%20systems%2C%20particularly%20LLM%20agents%2C%20rely%20on%20static%0Apre-training%20or%20rigid%20workflows%2C%20lacking%20mechanisms%20for%20continual%20adaptation.%0ARecent%20studies%20identified%20early%20cognitive%20traits%20in%20LLM%20agents%20%28reflection%2C%0Arevision%2C%20and%20self-correction%29%20suggesting%20foundational%20elements%20of%20human-like%0Aexperiential%20learning.%20Thus%20the%20key%20question%3A%20Can%20we%20design%20LLM%20agents%20capable%0Aof%20structured%2C%20cognitively%20grounded%20learning%20similar%20to%20human%20processes%3F%20In%0Aresponse%2C%20we%20propose%20a%20computational%20framework%20of%20Kolb%27s%20learning%20cycle%20with%0AVygotsky%27s%20ZPD%20for%20autonomous%20agents.%20Our%20architecture%20separates%20extrinsic%0A%28environment%20interaction%29%20and%20intrinsic%20%28internal%20reflection/abstraction%29%0Afunctions%2C%20enabling%20cognitively%20grounded%20scaffolded%20learning%2C%20where%20the%20agent%0Ainitially%20learns%20within%20structured%20environments%2C%20followed%20by%20open-ended%0Ageneralisation.%20This%20approach%20empowers%20agents%20to%20master%20complex%20tasks%20%3B%20domains%0Athat%20traditional%20fine-tuning%20or%20simple%20reflective%20methods%20could%20not%20tackle%0Aeffectively.%20Its%20potential%20is%20powerfully%20demonstrated%20via%20direct%20comparison%0Awith%20humans%20in%20real-world%20Kaggle%20data%20science%20competitions.%20Learning%20fully%0Aautomated%20data%20science%20code%20generation%20across%2081%20tasks%2C%20our%20system%2C%20Agent%20K%2C%0Ademonstrated%20the%20ability%20to%20perform%20the%20entire%20workflow%20autonomously%2C%20achieving%0Aan%20Elo-MMR%20score%20of%201694%2C%20beyond%20median%20score%20of%20the%20Kaggle%20Masters%20%28the%20top%202%25%0Aamong%20200%2C000%20users%29%20of%20our%20study.%20With%209%20gold%2C%208%20silver%2C%20and%2012%20bronze%20medals%0Alevel%20performance%20-%20including%204%20gold%20and%204%20silver%20on%20prize-awarding%0Acompetitions%20-%20Agent%20K%20is%20the%201st%20AI%20system%20to%20successfully%20integrate%20Kolb-%20and%0AVygotsky-inspired%20human%20cognitive%20learning%2C%20marking%20a%20major%20step%20toward%0Ageneralist%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03562v2&entry.124074799=Read"},
{"title": "Modular Techniques for Synthetic Long-Context Data Generation in\n  Language Model Training and Evaluation", "author": "Seganrasan Subramanian and Abhigya Verma", "abstract": "  The ability of large language models (LLMs) to process and reason over long\ntextual inputs is critical for a wide range of real-world applications.\nHowever, progress in this area is significantly constrained by the absence of\nhigh-quality, diverse, and verifiable long-context datasets suitable for both\ntraining and evaluation. This work introduces a modular, extensible framework\nfor synthetic long-context data generation via prompt-based interaction with\nLLMs. The framework supports multiple training and alignment objectives,\nincluding Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO),\nand Group Relative Policy Optimization (GRPO). It encompasses four core\ngeneration paradigms: multi-turn conversational dialogues, document-grounded\ninput-output pairs, verifiable instruction-response tasks, and long-context\nreasoning examples. Through templated prompting, a model-agnostic architecture,\nand metadata-enriched outputs, the proposed approach facilitates scalable,\ncontrollable, and purpose-aligned dataset creation for advancing long-context\ncapabilities in LLMs.\n", "link": "http://arxiv.org/abs/2509.01185v2", "date": "2025-09-04", "relevancy": 2.1819, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5495}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5495}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modular%20Techniques%20for%20Synthetic%20Long-Context%20Data%20Generation%20in%0A%20%20Language%20Model%20Training%20and%20Evaluation&body=Title%3A%20Modular%20Techniques%20for%20Synthetic%20Long-Context%20Data%20Generation%20in%0A%20%20Language%20Model%20Training%20and%20Evaluation%0AAuthor%3A%20Seganrasan%20Subramanian%20and%20Abhigya%20Verma%0AAbstract%3A%20%20%20The%20ability%20of%20large%20language%20models%20%28LLMs%29%20to%20process%20and%20reason%20over%20long%0Atextual%20inputs%20is%20critical%20for%20a%20wide%20range%20of%20real-world%20applications.%0AHowever%2C%20progress%20in%20this%20area%20is%20significantly%20constrained%20by%20the%20absence%20of%0Ahigh-quality%2C%20diverse%2C%20and%20verifiable%20long-context%20datasets%20suitable%20for%20both%0Atraining%20and%20evaluation.%20This%20work%20introduces%20a%20modular%2C%20extensible%20framework%0Afor%20synthetic%20long-context%20data%20generation%20via%20prompt-based%20interaction%20with%0ALLMs.%20The%20framework%20supports%20multiple%20training%20and%20alignment%20objectives%2C%0Aincluding%20Supervised%20Fine-Tuning%20%28SFT%29%2C%20Direct%20Preference%20Optimization%20%28DPO%29%2C%0Aand%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29.%20It%20encompasses%20four%20core%0Ageneration%20paradigms%3A%20multi-turn%20conversational%20dialogues%2C%20document-grounded%0Ainput-output%20pairs%2C%20verifiable%20instruction-response%20tasks%2C%20and%20long-context%0Areasoning%20examples.%20Through%20templated%20prompting%2C%20a%20model-agnostic%20architecture%2C%0Aand%20metadata-enriched%20outputs%2C%20the%20proposed%20approach%20facilitates%20scalable%2C%0Acontrollable%2C%20and%20purpose-aligned%20dataset%20creation%20for%20advancing%20long-context%0Acapabilities%20in%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.01185v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModular%2520Techniques%2520for%2520Synthetic%2520Long-Context%2520Data%2520Generation%2520in%250A%2520%2520Language%2520Model%2520Training%2520and%2520Evaluation%26entry.906535625%3DSeganrasan%2520Subramanian%2520and%2520Abhigya%2520Verma%26entry.1292438233%3D%2520%2520The%2520ability%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520process%2520and%2520reason%2520over%2520long%250Atextual%2520inputs%2520is%2520critical%2520for%2520a%2520wide%2520range%2520of%2520real-world%2520applications.%250AHowever%252C%2520progress%2520in%2520this%2520area%2520is%2520significantly%2520constrained%2520by%2520the%2520absence%2520of%250Ahigh-quality%252C%2520diverse%252C%2520and%2520verifiable%2520long-context%2520datasets%2520suitable%2520for%2520both%250Atraining%2520and%2520evaluation.%2520This%2520work%2520introduces%2520a%2520modular%252C%2520extensible%2520framework%250Afor%2520synthetic%2520long-context%2520data%2520generation%2520via%2520prompt-based%2520interaction%2520with%250ALLMs.%2520The%2520framework%2520supports%2520multiple%2520training%2520and%2520alignment%2520objectives%252C%250Aincluding%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%252C%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%252C%250Aand%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529.%2520It%2520encompasses%2520four%2520core%250Ageneration%2520paradigms%253A%2520multi-turn%2520conversational%2520dialogues%252C%2520document-grounded%250Ainput-output%2520pairs%252C%2520verifiable%2520instruction-response%2520tasks%252C%2520and%2520long-context%250Areasoning%2520examples.%2520Through%2520templated%2520prompting%252C%2520a%2520model-agnostic%2520architecture%252C%250Aand%2520metadata-enriched%2520outputs%252C%2520the%2520proposed%2520approach%2520facilitates%2520scalable%252C%250Acontrollable%252C%2520and%2520purpose-aligned%2520dataset%2520creation%2520for%2520advancing%2520long-context%250Acapabilities%2520in%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.01185v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modular%20Techniques%20for%20Synthetic%20Long-Context%20Data%20Generation%20in%0A%20%20Language%20Model%20Training%20and%20Evaluation&entry.906535625=Seganrasan%20Subramanian%20and%20Abhigya%20Verma&entry.1292438233=%20%20The%20ability%20of%20large%20language%20models%20%28LLMs%29%20to%20process%20and%20reason%20over%20long%0Atextual%20inputs%20is%20critical%20for%20a%20wide%20range%20of%20real-world%20applications.%0AHowever%2C%20progress%20in%20this%20area%20is%20significantly%20constrained%20by%20the%20absence%20of%0Ahigh-quality%2C%20diverse%2C%20and%20verifiable%20long-context%20datasets%20suitable%20for%20both%0Atraining%20and%20evaluation.%20This%20work%20introduces%20a%20modular%2C%20extensible%20framework%0Afor%20synthetic%20long-context%20data%20generation%20via%20prompt-based%20interaction%20with%0ALLMs.%20The%20framework%20supports%20multiple%20training%20and%20alignment%20objectives%2C%0Aincluding%20Supervised%20Fine-Tuning%20%28SFT%29%2C%20Direct%20Preference%20Optimization%20%28DPO%29%2C%0Aand%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29.%20It%20encompasses%20four%20core%0Ageneration%20paradigms%3A%20multi-turn%20conversational%20dialogues%2C%20document-grounded%0Ainput-output%20pairs%2C%20verifiable%20instruction-response%20tasks%2C%20and%20long-context%0Areasoning%20examples.%20Through%20templated%20prompting%2C%20a%20model-agnostic%20architecture%2C%0Aand%20metadata-enriched%20outputs%2C%20the%20proposed%20approach%20facilitates%20scalable%2C%0Acontrollable%2C%20and%20purpose-aligned%20dataset%20creation%20for%20advancing%20long-context%0Acapabilities%20in%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.01185v2&entry.124074799=Read"},
{"title": "Efficient Odd-One-Out Anomaly Detection", "author": "Silvio Chito and Paolo Rabino and Tatiana Tommasi", "abstract": "  The recently introduced odd-one-out anomaly detection task involves\nidentifying the odd-looking instances within a multi-object scene. This problem\npresents several challenges for modern deep learning models, demanding spatial\nreasoning across multiple views and relational reasoning to understand context\nand generalize across varying object categories and layouts. We argue that\nthese challenges must be addressed with efficiency in mind. To this end, we\npropose a DINO-based model that reduces the number of parameters by one third\nand shortens training time by a factor of three compared to the current\nstate-of-the-art, while maintaining competitive performance. Our experimental\nevaluation also introduces a Multimodal Large Language Model baseline,\nproviding insights into its current limitations in structured visual reasoning\ntasks. The project page can be found at\nhttps://silviochito.github.io/EfficientOddOneOut/\n", "link": "http://arxiv.org/abs/2509.04326v1", "date": "2025-09-04", "relevancy": 2.1715, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5457}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5457}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Odd-One-Out%20Anomaly%20Detection&body=Title%3A%20Efficient%20Odd-One-Out%20Anomaly%20Detection%0AAuthor%3A%20Silvio%20Chito%20and%20Paolo%20Rabino%20and%20Tatiana%20Tommasi%0AAbstract%3A%20%20%20The%20recently%20introduced%20odd-one-out%20anomaly%20detection%20task%20involves%0Aidentifying%20the%20odd-looking%20instances%20within%20a%20multi-object%20scene.%20This%20problem%0Apresents%20several%20challenges%20for%20modern%20deep%20learning%20models%2C%20demanding%20spatial%0Areasoning%20across%20multiple%20views%20and%20relational%20reasoning%20to%20understand%20context%0Aand%20generalize%20across%20varying%20object%20categories%20and%20layouts.%20We%20argue%20that%0Athese%20challenges%20must%20be%20addressed%20with%20efficiency%20in%20mind.%20To%20this%20end%2C%20we%0Apropose%20a%20DINO-based%20model%20that%20reduces%20the%20number%20of%20parameters%20by%20one%20third%0Aand%20shortens%20training%20time%20by%20a%20factor%20of%20three%20compared%20to%20the%20current%0Astate-of-the-art%2C%20while%20maintaining%20competitive%20performance.%20Our%20experimental%0Aevaluation%20also%20introduces%20a%20Multimodal%20Large%20Language%20Model%20baseline%2C%0Aproviding%20insights%20into%20its%20current%20limitations%20in%20structured%20visual%20reasoning%0Atasks.%20The%20project%20page%20can%20be%20found%20at%0Ahttps%3A//silviochito.github.io/EfficientOddOneOut/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04326v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Odd-One-Out%2520Anomaly%2520Detection%26entry.906535625%3DSilvio%2520Chito%2520and%2520Paolo%2520Rabino%2520and%2520Tatiana%2520Tommasi%26entry.1292438233%3D%2520%2520The%2520recently%2520introduced%2520odd-one-out%2520anomaly%2520detection%2520task%2520involves%250Aidentifying%2520the%2520odd-looking%2520instances%2520within%2520a%2520multi-object%2520scene.%2520This%2520problem%250Apresents%2520several%2520challenges%2520for%2520modern%2520deep%2520learning%2520models%252C%2520demanding%2520spatial%250Areasoning%2520across%2520multiple%2520views%2520and%2520relational%2520reasoning%2520to%2520understand%2520context%250Aand%2520generalize%2520across%2520varying%2520object%2520categories%2520and%2520layouts.%2520We%2520argue%2520that%250Athese%2520challenges%2520must%2520be%2520addressed%2520with%2520efficiency%2520in%2520mind.%2520To%2520this%2520end%252C%2520we%250Apropose%2520a%2520DINO-based%2520model%2520that%2520reduces%2520the%2520number%2520of%2520parameters%2520by%2520one%2520third%250Aand%2520shortens%2520training%2520time%2520by%2520a%2520factor%2520of%2520three%2520compared%2520to%2520the%2520current%250Astate-of-the-art%252C%2520while%2520maintaining%2520competitive%2520performance.%2520Our%2520experimental%250Aevaluation%2520also%2520introduces%2520a%2520Multimodal%2520Large%2520Language%2520Model%2520baseline%252C%250Aproviding%2520insights%2520into%2520its%2520current%2520limitations%2520in%2520structured%2520visual%2520reasoning%250Atasks.%2520The%2520project%2520page%2520can%2520be%2520found%2520at%250Ahttps%253A//silviochito.github.io/EfficientOddOneOut/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04326v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Odd-One-Out%20Anomaly%20Detection&entry.906535625=Silvio%20Chito%20and%20Paolo%20Rabino%20and%20Tatiana%20Tommasi&entry.1292438233=%20%20The%20recently%20introduced%20odd-one-out%20anomaly%20detection%20task%20involves%0Aidentifying%20the%20odd-looking%20instances%20within%20a%20multi-object%20scene.%20This%20problem%0Apresents%20several%20challenges%20for%20modern%20deep%20learning%20models%2C%20demanding%20spatial%0Areasoning%20across%20multiple%20views%20and%20relational%20reasoning%20to%20understand%20context%0Aand%20generalize%20across%20varying%20object%20categories%20and%20layouts.%20We%20argue%20that%0Athese%20challenges%20must%20be%20addressed%20with%20efficiency%20in%20mind.%20To%20this%20end%2C%20we%0Apropose%20a%20DINO-based%20model%20that%20reduces%20the%20number%20of%20parameters%20by%20one%20third%0Aand%20shortens%20training%20time%20by%20a%20factor%20of%20three%20compared%20to%20the%20current%0Astate-of-the-art%2C%20while%20maintaining%20competitive%20performance.%20Our%20experimental%0Aevaluation%20also%20introduces%20a%20Multimodal%20Large%20Language%20Model%20baseline%2C%0Aproviding%20insights%20into%20its%20current%20limitations%20in%20structured%20visual%20reasoning%0Atasks.%20The%20project%20page%20can%20be%20found%20at%0Ahttps%3A//silviochito.github.io/EfficientOddOneOut/%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04326v1&entry.124074799=Read"},
{"title": "PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal\n  Documents", "author": "Junjie Wang and Yuxiang Zhang and Minghao Liu and Yin Zhang and Yatai Ji and Weihao Xuan and Nie Lin and Kang Zhu and Zhiqiang Lin and Yiming Ren and Chunyang Jiang and Yiyao Yu and Zekun Wang and Tiezhen Wang and Wenhao Huang and Jie Fu and Qunshu Liu and Yujiu Yang and Ge Zhang and Ruibin Yuan and Bei Chen and Wenhu Chen", "abstract": "  Recent advancements in large multimodal models (LMMs) have leveraged\nextensive multimodal datasets to enhance capabilities in complex\nknowledge-driven tasks. However, persistent challenges in perceptual and\nreasoning errors limit their efficacy, particularly in interpreting intricate\nvisual data and deducing multimodal relationships. To address these issues, we\nintroduce PIN (Paired and INterleaved multimodal documents), a novel data\nformat designed to foster a deeper integration of visual and textual knowledge.\nThe PIN format uniquely combines semantically rich Markdown files, which\npreserve fine-grained textual structures, with holistic overall images that\ncapture the complete document layout. Following this format, we construct and\nrelease two large-scale, open-source datasets: PIN-200M (~200 million\ndocuments) and PIN-14M (~14 million), compiled from diverse web and scientific\nsources in both English and Chinese. To maximize usability, we provide detailed\nstatistical analyses and equip the datasets with quality signals, enabling\nresearchers to easily filter and select data for specific tasks. Our work\nprovides the community with a versatile data format and substantial resources,\noffering a foundation for new research in pre-training strategies and the\ndevelopment of more powerful knowledge-intensive LMMs.\n", "link": "http://arxiv.org/abs/2406.13923v2", "date": "2025-09-04", "relevancy": 2.1682, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5509}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5453}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PIN%3A%20A%20Knowledge-Intensive%20Dataset%20for%20Paired%20and%20Interleaved%20Multimodal%0A%20%20Documents&body=Title%3A%20PIN%3A%20A%20Knowledge-Intensive%20Dataset%20for%20Paired%20and%20Interleaved%20Multimodal%0A%20%20Documents%0AAuthor%3A%20Junjie%20Wang%20and%20Yuxiang%20Zhang%20and%20Minghao%20Liu%20and%20Yin%20Zhang%20and%20Yatai%20Ji%20and%20Weihao%20Xuan%20and%20Nie%20Lin%20and%20Kang%20Zhu%20and%20Zhiqiang%20Lin%20and%20Yiming%20Ren%20and%20Chunyang%20Jiang%20and%20Yiyao%20Yu%20and%20Zekun%20Wang%20and%20Tiezhen%20Wang%20and%20Wenhao%20Huang%20and%20Jie%20Fu%20and%20Qunshu%20Liu%20and%20Yujiu%20Yang%20and%20Ge%20Zhang%20and%20Ruibin%20Yuan%20and%20Bei%20Chen%20and%20Wenhu%20Chen%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20multimodal%20models%20%28LMMs%29%20have%20leveraged%0Aextensive%20multimodal%20datasets%20to%20enhance%20capabilities%20in%20complex%0Aknowledge-driven%20tasks.%20However%2C%20persistent%20challenges%20in%20perceptual%20and%0Areasoning%20errors%20limit%20their%20efficacy%2C%20particularly%20in%20interpreting%20intricate%0Avisual%20data%20and%20deducing%20multimodal%20relationships.%20To%20address%20these%20issues%2C%20we%0Aintroduce%20PIN%20%28Paired%20and%20INterleaved%20multimodal%20documents%29%2C%20a%20novel%20data%0Aformat%20designed%20to%20foster%20a%20deeper%20integration%20of%20visual%20and%20textual%20knowledge.%0AThe%20PIN%20format%20uniquely%20combines%20semantically%20rich%20Markdown%20files%2C%20which%0Apreserve%20fine-grained%20textual%20structures%2C%20with%20holistic%20overall%20images%20that%0Acapture%20the%20complete%20document%20layout.%20Following%20this%20format%2C%20we%20construct%20and%0Arelease%20two%20large-scale%2C%20open-source%20datasets%3A%20PIN-200M%20%28~200%20million%0Adocuments%29%20and%20PIN-14M%20%28~14%20million%29%2C%20compiled%20from%20diverse%20web%20and%20scientific%0Asources%20in%20both%20English%20and%20Chinese.%20To%20maximize%20usability%2C%20we%20provide%20detailed%0Astatistical%20analyses%20and%20equip%20the%20datasets%20with%20quality%20signals%2C%20enabling%0Aresearchers%20to%20easily%20filter%20and%20select%20data%20for%20specific%20tasks.%20Our%20work%0Aprovides%20the%20community%20with%20a%20versatile%20data%20format%20and%20substantial%20resources%2C%0Aoffering%20a%20foundation%20for%20new%20research%20in%20pre-training%20strategies%20and%20the%0Adevelopment%20of%20more%20powerful%20knowledge-intensive%20LMMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13923v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPIN%253A%2520A%2520Knowledge-Intensive%2520Dataset%2520for%2520Paired%2520and%2520Interleaved%2520Multimodal%250A%2520%2520Documents%26entry.906535625%3DJunjie%2520Wang%2520and%2520Yuxiang%2520Zhang%2520and%2520Minghao%2520Liu%2520and%2520Yin%2520Zhang%2520and%2520Yatai%2520Ji%2520and%2520Weihao%2520Xuan%2520and%2520Nie%2520Lin%2520and%2520Kang%2520Zhu%2520and%2520Zhiqiang%2520Lin%2520and%2520Yiming%2520Ren%2520and%2520Chunyang%2520Jiang%2520and%2520Yiyao%2520Yu%2520and%2520Zekun%2520Wang%2520and%2520Tiezhen%2520Wang%2520and%2520Wenhao%2520Huang%2520and%2520Jie%2520Fu%2520and%2520Qunshu%2520Liu%2520and%2520Yujiu%2520Yang%2520and%2520Ge%2520Zhang%2520and%2520Ruibin%2520Yuan%2520and%2520Bei%2520Chen%2520and%2520Wenhu%2520Chen%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520large%2520multimodal%2520models%2520%2528LMMs%2529%2520have%2520leveraged%250Aextensive%2520multimodal%2520datasets%2520to%2520enhance%2520capabilities%2520in%2520complex%250Aknowledge-driven%2520tasks.%2520However%252C%2520persistent%2520challenges%2520in%2520perceptual%2520and%250Areasoning%2520errors%2520limit%2520their%2520efficacy%252C%2520particularly%2520in%2520interpreting%2520intricate%250Avisual%2520data%2520and%2520deducing%2520multimodal%2520relationships.%2520To%2520address%2520these%2520issues%252C%2520we%250Aintroduce%2520PIN%2520%2528Paired%2520and%2520INterleaved%2520multimodal%2520documents%2529%252C%2520a%2520novel%2520data%250Aformat%2520designed%2520to%2520foster%2520a%2520deeper%2520integration%2520of%2520visual%2520and%2520textual%2520knowledge.%250AThe%2520PIN%2520format%2520uniquely%2520combines%2520semantically%2520rich%2520Markdown%2520files%252C%2520which%250Apreserve%2520fine-grained%2520textual%2520structures%252C%2520with%2520holistic%2520overall%2520images%2520that%250Acapture%2520the%2520complete%2520document%2520layout.%2520Following%2520this%2520format%252C%2520we%2520construct%2520and%250Arelease%2520two%2520large-scale%252C%2520open-source%2520datasets%253A%2520PIN-200M%2520%2528~200%2520million%250Adocuments%2529%2520and%2520PIN-14M%2520%2528~14%2520million%2529%252C%2520compiled%2520from%2520diverse%2520web%2520and%2520scientific%250Asources%2520in%2520both%2520English%2520and%2520Chinese.%2520To%2520maximize%2520usability%252C%2520we%2520provide%2520detailed%250Astatistical%2520analyses%2520and%2520equip%2520the%2520datasets%2520with%2520quality%2520signals%252C%2520enabling%250Aresearchers%2520to%2520easily%2520filter%2520and%2520select%2520data%2520for%2520specific%2520tasks.%2520Our%2520work%250Aprovides%2520the%2520community%2520with%2520a%2520versatile%2520data%2520format%2520and%2520substantial%2520resources%252C%250Aoffering%2520a%2520foundation%2520for%2520new%2520research%2520in%2520pre-training%2520strategies%2520and%2520the%250Adevelopment%2520of%2520more%2520powerful%2520knowledge-intensive%2520LMMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13923v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PIN%3A%20A%20Knowledge-Intensive%20Dataset%20for%20Paired%20and%20Interleaved%20Multimodal%0A%20%20Documents&entry.906535625=Junjie%20Wang%20and%20Yuxiang%20Zhang%20and%20Minghao%20Liu%20and%20Yin%20Zhang%20and%20Yatai%20Ji%20and%20Weihao%20Xuan%20and%20Nie%20Lin%20and%20Kang%20Zhu%20and%20Zhiqiang%20Lin%20and%20Yiming%20Ren%20and%20Chunyang%20Jiang%20and%20Yiyao%20Yu%20and%20Zekun%20Wang%20and%20Tiezhen%20Wang%20and%20Wenhao%20Huang%20and%20Jie%20Fu%20and%20Qunshu%20Liu%20and%20Yujiu%20Yang%20and%20Ge%20Zhang%20and%20Ruibin%20Yuan%20and%20Bei%20Chen%20and%20Wenhu%20Chen&entry.1292438233=%20%20Recent%20advancements%20in%20large%20multimodal%20models%20%28LMMs%29%20have%20leveraged%0Aextensive%20multimodal%20datasets%20to%20enhance%20capabilities%20in%20complex%0Aknowledge-driven%20tasks.%20However%2C%20persistent%20challenges%20in%20perceptual%20and%0Areasoning%20errors%20limit%20their%20efficacy%2C%20particularly%20in%20interpreting%20intricate%0Avisual%20data%20and%20deducing%20multimodal%20relationships.%20To%20address%20these%20issues%2C%20we%0Aintroduce%20PIN%20%28Paired%20and%20INterleaved%20multimodal%20documents%29%2C%20a%20novel%20data%0Aformat%20designed%20to%20foster%20a%20deeper%20integration%20of%20visual%20and%20textual%20knowledge.%0AThe%20PIN%20format%20uniquely%20combines%20semantically%20rich%20Markdown%20files%2C%20which%0Apreserve%20fine-grained%20textual%20structures%2C%20with%20holistic%20overall%20images%20that%0Acapture%20the%20complete%20document%20layout.%20Following%20this%20format%2C%20we%20construct%20and%0Arelease%20two%20large-scale%2C%20open-source%20datasets%3A%20PIN-200M%20%28~200%20million%0Adocuments%29%20and%20PIN-14M%20%28~14%20million%29%2C%20compiled%20from%20diverse%20web%20and%20scientific%0Asources%20in%20both%20English%20and%20Chinese.%20To%20maximize%20usability%2C%20we%20provide%20detailed%0Astatistical%20analyses%20and%20equip%20the%20datasets%20with%20quality%20signals%2C%20enabling%0Aresearchers%20to%20easily%20filter%20and%20select%20data%20for%20specific%20tasks.%20Our%20work%0Aprovides%20the%20community%20with%20a%20versatile%20data%20format%20and%20substantial%20resources%2C%0Aoffering%20a%20foundation%20for%20new%20research%20in%20pre-training%20strategies%20and%20the%0Adevelopment%20of%20more%20powerful%20knowledge-intensive%20LMMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13923v2&entry.124074799=Read"},
{"title": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision", "author": "Safouane El Ghazouali and Umberto Michelucci", "abstract": "  AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.\n", "link": "http://arxiv.org/abs/2509.04180v1", "date": "2025-09-04", "relevancy": 2.1561, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5421}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5391}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisioFirm%3A%20Cross-Platform%20AI-assisted%20Annotation%20Tool%20for%20Computer%0A%20%20Vision&body=Title%3A%20VisioFirm%3A%20Cross-Platform%20AI-assisted%20Annotation%20Tool%20for%20Computer%0A%20%20Vision%0AAuthor%3A%20Safouane%20El%20Ghazouali%20and%20Umberto%20Michelucci%0AAbstract%3A%20%20%20AI%20models%20rely%20on%20annotated%20data%20to%20learn%20pattern%20and%20perform%20prediction.%0AAnnotation%20is%20usually%20a%20labor-intensive%20step%20that%20require%20associating%20labels%0Aranging%20from%20a%20simple%20classification%20label%20to%20more%20complex%20tasks%20such%20as%20object%0Adetection%2C%20oriented%20bounding%20box%20estimation%2C%20and%20instance%20segmentation.%0ATraditional%20tools%20often%20require%20extensive%20manual%20input%2C%20limiting%20scalability%0Afor%20large%20datasets.%20To%20address%20this%2C%20we%20introduce%20VisioFirm%2C%20an%20open-source%20web%0Aapplication%20designed%20to%20streamline%20image%20labeling%20through%20AI-assisted%0Aautomation.%20VisioFirm%20integrates%20state-of-the-art%20foundation%20models%20into%20an%0Ainterface%20with%20a%20filtering%20pipeline%20to%20reduce%20human-in-the-loop%20efforts.%20This%0Ahybrid%20approach%20employs%20CLIP%20combined%20with%20pre-trained%20detectors%20like%0AUltralytics%20models%20for%20common%20classes%20and%20zero-shot%20models%20such%20as%20Grounding%0ADINO%20for%20custom%20labels%2C%20generating%20initial%20annotations%20with%20low-confidence%0Athresholding%20to%20maximize%20recall.%20Through%20this%20framework%2C%20when%20tested%20on%0ACOCO-type%20of%20classes%2C%20initial%20prediction%20have%20been%20proven%20to%20be%20mostly%20correct%0Athough%20the%20users%20can%20refine%20these%20via%20interactive%20tools%20supporting%20bounding%0Aboxes%2C%20oriented%20bounding%20boxes%2C%20and%20polygons.%20Additionally%2C%20VisioFirm%20has%0Aon-the-fly%20segmentation%20powered%20by%20Segment%20Anything%20accelerated%20through%20WebGPU%0Afor%20browser-side%20efficiency.%20The%20tool%20supports%20multiple%20export%20formats%20%28YOLO%2C%0ACOCO%2C%20Pascal%20VOC%2C%20CSV%29%20and%20operates%20offline%20after%20model%20caching%2C%20enhancing%0Aaccessibility.%20VisioFirm%20demonstrates%20up%20to%2090%5C%25%20reduction%20in%20manual%20effort%0Athrough%20benchmarks%20on%20diverse%20datasets%2C%20while%20maintaining%20high%20annotation%0Aaccuracy%20via%20clustering%20of%20connected%20CLIP-based%20disambiguate%20components%20and%0AIoU-graph%20for%20redundant%20detection%20suppression.%20VisioFirm%20can%20be%20accessed%20from%0A%5Chref%7Bhttps%3A//github.com/OschAI/VisioFirm%7D%7Bhttps%3A//github.com/OschAI/VisioFirm%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04180v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisioFirm%253A%2520Cross-Platform%2520AI-assisted%2520Annotation%2520Tool%2520for%2520Computer%250A%2520%2520Vision%26entry.906535625%3DSafouane%2520El%2520Ghazouali%2520and%2520Umberto%2520Michelucci%26entry.1292438233%3D%2520%2520AI%2520models%2520rely%2520on%2520annotated%2520data%2520to%2520learn%2520pattern%2520and%2520perform%2520prediction.%250AAnnotation%2520is%2520usually%2520a%2520labor-intensive%2520step%2520that%2520require%2520associating%2520labels%250Aranging%2520from%2520a%2520simple%2520classification%2520label%2520to%2520more%2520complex%2520tasks%2520such%2520as%2520object%250Adetection%252C%2520oriented%2520bounding%2520box%2520estimation%252C%2520and%2520instance%2520segmentation.%250ATraditional%2520tools%2520often%2520require%2520extensive%2520manual%2520input%252C%2520limiting%2520scalability%250Afor%2520large%2520datasets.%2520To%2520address%2520this%252C%2520we%2520introduce%2520VisioFirm%252C%2520an%2520open-source%2520web%250Aapplication%2520designed%2520to%2520streamline%2520image%2520labeling%2520through%2520AI-assisted%250Aautomation.%2520VisioFirm%2520integrates%2520state-of-the-art%2520foundation%2520models%2520into%2520an%250Ainterface%2520with%2520a%2520filtering%2520pipeline%2520to%2520reduce%2520human-in-the-loop%2520efforts.%2520This%250Ahybrid%2520approach%2520employs%2520CLIP%2520combined%2520with%2520pre-trained%2520detectors%2520like%250AUltralytics%2520models%2520for%2520common%2520classes%2520and%2520zero-shot%2520models%2520such%2520as%2520Grounding%250ADINO%2520for%2520custom%2520labels%252C%2520generating%2520initial%2520annotations%2520with%2520low-confidence%250Athresholding%2520to%2520maximize%2520recall.%2520Through%2520this%2520framework%252C%2520when%2520tested%2520on%250ACOCO-type%2520of%2520classes%252C%2520initial%2520prediction%2520have%2520been%2520proven%2520to%2520be%2520mostly%2520correct%250Athough%2520the%2520users%2520can%2520refine%2520these%2520via%2520interactive%2520tools%2520supporting%2520bounding%250Aboxes%252C%2520oriented%2520bounding%2520boxes%252C%2520and%2520polygons.%2520Additionally%252C%2520VisioFirm%2520has%250Aon-the-fly%2520segmentation%2520powered%2520by%2520Segment%2520Anything%2520accelerated%2520through%2520WebGPU%250Afor%2520browser-side%2520efficiency.%2520The%2520tool%2520supports%2520multiple%2520export%2520formats%2520%2528YOLO%252C%250ACOCO%252C%2520Pascal%2520VOC%252C%2520CSV%2529%2520and%2520operates%2520offline%2520after%2520model%2520caching%252C%2520enhancing%250Aaccessibility.%2520VisioFirm%2520demonstrates%2520up%2520to%252090%255C%2525%2520reduction%2520in%2520manual%2520effort%250Athrough%2520benchmarks%2520on%2520diverse%2520datasets%252C%2520while%2520maintaining%2520high%2520annotation%250Aaccuracy%2520via%2520clustering%2520of%2520connected%2520CLIP-based%2520disambiguate%2520components%2520and%250AIoU-graph%2520for%2520redundant%2520detection%2520suppression.%2520VisioFirm%2520can%2520be%2520accessed%2520from%250A%255Chref%257Bhttps%253A//github.com/OschAI/VisioFirm%257D%257Bhttps%253A//github.com/OschAI/VisioFirm%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04180v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisioFirm%3A%20Cross-Platform%20AI-assisted%20Annotation%20Tool%20for%20Computer%0A%20%20Vision&entry.906535625=Safouane%20El%20Ghazouali%20and%20Umberto%20Michelucci&entry.1292438233=%20%20AI%20models%20rely%20on%20annotated%20data%20to%20learn%20pattern%20and%20perform%20prediction.%0AAnnotation%20is%20usually%20a%20labor-intensive%20step%20that%20require%20associating%20labels%0Aranging%20from%20a%20simple%20classification%20label%20to%20more%20complex%20tasks%20such%20as%20object%0Adetection%2C%20oriented%20bounding%20box%20estimation%2C%20and%20instance%20segmentation.%0ATraditional%20tools%20often%20require%20extensive%20manual%20input%2C%20limiting%20scalability%0Afor%20large%20datasets.%20To%20address%20this%2C%20we%20introduce%20VisioFirm%2C%20an%20open-source%20web%0Aapplication%20designed%20to%20streamline%20image%20labeling%20through%20AI-assisted%0Aautomation.%20VisioFirm%20integrates%20state-of-the-art%20foundation%20models%20into%20an%0Ainterface%20with%20a%20filtering%20pipeline%20to%20reduce%20human-in-the-loop%20efforts.%20This%0Ahybrid%20approach%20employs%20CLIP%20combined%20with%20pre-trained%20detectors%20like%0AUltralytics%20models%20for%20common%20classes%20and%20zero-shot%20models%20such%20as%20Grounding%0ADINO%20for%20custom%20labels%2C%20generating%20initial%20annotations%20with%20low-confidence%0Athresholding%20to%20maximize%20recall.%20Through%20this%20framework%2C%20when%20tested%20on%0ACOCO-type%20of%20classes%2C%20initial%20prediction%20have%20been%20proven%20to%20be%20mostly%20correct%0Athough%20the%20users%20can%20refine%20these%20via%20interactive%20tools%20supporting%20bounding%0Aboxes%2C%20oriented%20bounding%20boxes%2C%20and%20polygons.%20Additionally%2C%20VisioFirm%20has%0Aon-the-fly%20segmentation%20powered%20by%20Segment%20Anything%20accelerated%20through%20WebGPU%0Afor%20browser-side%20efficiency.%20The%20tool%20supports%20multiple%20export%20formats%20%28YOLO%2C%0ACOCO%2C%20Pascal%20VOC%2C%20CSV%29%20and%20operates%20offline%20after%20model%20caching%2C%20enhancing%0Aaccessibility.%20VisioFirm%20demonstrates%20up%20to%2090%5C%25%20reduction%20in%20manual%20effort%0Athrough%20benchmarks%20on%20diverse%20datasets%2C%20while%20maintaining%20high%20annotation%0Aaccuracy%20via%20clustering%20of%20connected%20CLIP-based%20disambiguate%20components%20and%0AIoU-graph%20for%20redundant%20detection%20suppression.%20VisioFirm%20can%20be%20accessed%20from%0A%5Chref%7Bhttps%3A//github.com/OschAI/VisioFirm%7D%7Bhttps%3A//github.com/OschAI/VisioFirm%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04180v1&entry.124074799=Read"},
{"title": "Towards a Unified View of Large Language Model Post-Training", "author": "Xingtai Lv and Yuxin Zuo and Youbang Sun and Hongyi Liu and Yuntian Wei and Zhekai Chen and Lixuan He and Xuekai Zhu and Kaiyan Zhang and Bingning Wang and Ning Ding and Bowen Zhou", "abstract": "  Two major sources of training data exist for post-training modern language\nmodels: online (model-generated rollouts) data, and offline (human or\nother-model demonstrations) data. These two types of data are typically used by\napproaches like Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT),\nrespectively. In this paper, we show that these approaches are not in\ncontradiction, but are instances of a single optimization process. We derive a\nUnified Policy Gradient Estimator, and present the calculations of a wide\nspectrum of post-training approaches as the gradient of a common objective\nunder different data distribution assumptions and various bias-variance\ntradeoffs. The gradient estimator is constructed with four interchangeable\nparts: stabilization mask, reference policy denominator, advantage estimate,\nand likelihood gradient. Motivated by our theoretical findings, we propose\nHybrid Post-Training (HPT), an algorithm that dynamically selects different\ntraining signals. HPT is designed to yield both effective exploitation of\ndemonstration and stable exploration without sacrificing learned reasoning\npatterns. We provide extensive experiments and ablation studies to verify the\neffectiveness of our unified theoretical framework and HPT. Across six\nmathematical reasoning benchmarks and two out-of-distribution suites, HPT\nconsistently surpasses strong baselines across models of varying scales and\nfamilies.\n", "link": "http://arxiv.org/abs/2509.04419v1", "date": "2025-09-04", "relevancy": 2.1334, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5456}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5309}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5309}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20Unified%20View%20of%20Large%20Language%20Model%20Post-Training&body=Title%3A%20Towards%20a%20Unified%20View%20of%20Large%20Language%20Model%20Post-Training%0AAuthor%3A%20Xingtai%20Lv%20and%20Yuxin%20Zuo%20and%20Youbang%20Sun%20and%20Hongyi%20Liu%20and%20Yuntian%20Wei%20and%20Zhekai%20Chen%20and%20Lixuan%20He%20and%20Xuekai%20Zhu%20and%20Kaiyan%20Zhang%20and%20Bingning%20Wang%20and%20Ning%20Ding%20and%20Bowen%20Zhou%0AAbstract%3A%20%20%20Two%20major%20sources%20of%20training%20data%20exist%20for%20post-training%20modern%20language%0Amodels%3A%20online%20%28model-generated%20rollouts%29%20data%2C%20and%20offline%20%28human%20or%0Aother-model%20demonstrations%29%20data.%20These%20two%20types%20of%20data%20are%20typically%20used%20by%0Aapproaches%20like%20Reinforcement%20Learning%20%28RL%29%20and%20Supervised%20Fine-Tuning%20%28SFT%29%2C%0Arespectively.%20In%20this%20paper%2C%20we%20show%20that%20these%20approaches%20are%20not%20in%0Acontradiction%2C%20but%20are%20instances%20of%20a%20single%20optimization%20process.%20We%20derive%20a%0AUnified%20Policy%20Gradient%20Estimator%2C%20and%20present%20the%20calculations%20of%20a%20wide%0Aspectrum%20of%20post-training%20approaches%20as%20the%20gradient%20of%20a%20common%20objective%0Aunder%20different%20data%20distribution%20assumptions%20and%20various%20bias-variance%0Atradeoffs.%20The%20gradient%20estimator%20is%20constructed%20with%20four%20interchangeable%0Aparts%3A%20stabilization%20mask%2C%20reference%20policy%20denominator%2C%20advantage%20estimate%2C%0Aand%20likelihood%20gradient.%20Motivated%20by%20our%20theoretical%20findings%2C%20we%20propose%0AHybrid%20Post-Training%20%28HPT%29%2C%20an%20algorithm%20that%20dynamically%20selects%20different%0Atraining%20signals.%20HPT%20is%20designed%20to%20yield%20both%20effective%20exploitation%20of%0Ademonstration%20and%20stable%20exploration%20without%20sacrificing%20learned%20reasoning%0Apatterns.%20We%20provide%20extensive%20experiments%20and%20ablation%20studies%20to%20verify%20the%0Aeffectiveness%20of%20our%20unified%20theoretical%20framework%20and%20HPT.%20Across%20six%0Amathematical%20reasoning%20benchmarks%20and%20two%20out-of-distribution%20suites%2C%20HPT%0Aconsistently%20surpasses%20strong%20baselines%20across%20models%20of%20varying%20scales%20and%0Afamilies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04419v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520a%2520Unified%2520View%2520of%2520Large%2520Language%2520Model%2520Post-Training%26entry.906535625%3DXingtai%2520Lv%2520and%2520Yuxin%2520Zuo%2520and%2520Youbang%2520Sun%2520and%2520Hongyi%2520Liu%2520and%2520Yuntian%2520Wei%2520and%2520Zhekai%2520Chen%2520and%2520Lixuan%2520He%2520and%2520Xuekai%2520Zhu%2520and%2520Kaiyan%2520Zhang%2520and%2520Bingning%2520Wang%2520and%2520Ning%2520Ding%2520and%2520Bowen%2520Zhou%26entry.1292438233%3D%2520%2520Two%2520major%2520sources%2520of%2520training%2520data%2520exist%2520for%2520post-training%2520modern%2520language%250Amodels%253A%2520online%2520%2528model-generated%2520rollouts%2529%2520data%252C%2520and%2520offline%2520%2528human%2520or%250Aother-model%2520demonstrations%2529%2520data.%2520These%2520two%2520types%2520of%2520data%2520are%2520typically%2520used%2520by%250Aapproaches%2520like%2520Reinforcement%2520Learning%2520%2528RL%2529%2520and%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%252C%250Arespectively.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520these%2520approaches%2520are%2520not%2520in%250Acontradiction%252C%2520but%2520are%2520instances%2520of%2520a%2520single%2520optimization%2520process.%2520We%2520derive%2520a%250AUnified%2520Policy%2520Gradient%2520Estimator%252C%2520and%2520present%2520the%2520calculations%2520of%2520a%2520wide%250Aspectrum%2520of%2520post-training%2520approaches%2520as%2520the%2520gradient%2520of%2520a%2520common%2520objective%250Aunder%2520different%2520data%2520distribution%2520assumptions%2520and%2520various%2520bias-variance%250Atradeoffs.%2520The%2520gradient%2520estimator%2520is%2520constructed%2520with%2520four%2520interchangeable%250Aparts%253A%2520stabilization%2520mask%252C%2520reference%2520policy%2520denominator%252C%2520advantage%2520estimate%252C%250Aand%2520likelihood%2520gradient.%2520Motivated%2520by%2520our%2520theoretical%2520findings%252C%2520we%2520propose%250AHybrid%2520Post-Training%2520%2528HPT%2529%252C%2520an%2520algorithm%2520that%2520dynamically%2520selects%2520different%250Atraining%2520signals.%2520HPT%2520is%2520designed%2520to%2520yield%2520both%2520effective%2520exploitation%2520of%250Ademonstration%2520and%2520stable%2520exploration%2520without%2520sacrificing%2520learned%2520reasoning%250Apatterns.%2520We%2520provide%2520extensive%2520experiments%2520and%2520ablation%2520studies%2520to%2520verify%2520the%250Aeffectiveness%2520of%2520our%2520unified%2520theoretical%2520framework%2520and%2520HPT.%2520Across%2520six%250Amathematical%2520reasoning%2520benchmarks%2520and%2520two%2520out-of-distribution%2520suites%252C%2520HPT%250Aconsistently%2520surpasses%2520strong%2520baselines%2520across%2520models%2520of%2520varying%2520scales%2520and%250Afamilies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04419v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20Unified%20View%20of%20Large%20Language%20Model%20Post-Training&entry.906535625=Xingtai%20Lv%20and%20Yuxin%20Zuo%20and%20Youbang%20Sun%20and%20Hongyi%20Liu%20and%20Yuntian%20Wei%20and%20Zhekai%20Chen%20and%20Lixuan%20He%20and%20Xuekai%20Zhu%20and%20Kaiyan%20Zhang%20and%20Bingning%20Wang%20and%20Ning%20Ding%20and%20Bowen%20Zhou&entry.1292438233=%20%20Two%20major%20sources%20of%20training%20data%20exist%20for%20post-training%20modern%20language%0Amodels%3A%20online%20%28model-generated%20rollouts%29%20data%2C%20and%20offline%20%28human%20or%0Aother-model%20demonstrations%29%20data.%20These%20two%20types%20of%20data%20are%20typically%20used%20by%0Aapproaches%20like%20Reinforcement%20Learning%20%28RL%29%20and%20Supervised%20Fine-Tuning%20%28SFT%29%2C%0Arespectively.%20In%20this%20paper%2C%20we%20show%20that%20these%20approaches%20are%20not%20in%0Acontradiction%2C%20but%20are%20instances%20of%20a%20single%20optimization%20process.%20We%20derive%20a%0AUnified%20Policy%20Gradient%20Estimator%2C%20and%20present%20the%20calculations%20of%20a%20wide%0Aspectrum%20of%20post-training%20approaches%20as%20the%20gradient%20of%20a%20common%20objective%0Aunder%20different%20data%20distribution%20assumptions%20and%20various%20bias-variance%0Atradeoffs.%20The%20gradient%20estimator%20is%20constructed%20with%20four%20interchangeable%0Aparts%3A%20stabilization%20mask%2C%20reference%20policy%20denominator%2C%20advantage%20estimate%2C%0Aand%20likelihood%20gradient.%20Motivated%20by%20our%20theoretical%20findings%2C%20we%20propose%0AHybrid%20Post-Training%20%28HPT%29%2C%20an%20algorithm%20that%20dynamically%20selects%20different%0Atraining%20signals.%20HPT%20is%20designed%20to%20yield%20both%20effective%20exploitation%20of%0Ademonstration%20and%20stable%20exploration%20without%20sacrificing%20learned%20reasoning%0Apatterns.%20We%20provide%20extensive%20experiments%20and%20ablation%20studies%20to%20verify%20the%0Aeffectiveness%20of%20our%20unified%20theoretical%20framework%20and%20HPT.%20Across%20six%0Amathematical%20reasoning%20benchmarks%20and%20two%20out-of-distribution%20suites%2C%20HPT%0Aconsistently%20surpasses%20strong%20baselines%20across%20models%20of%20varying%20scales%20and%0Afamilies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04419v1&entry.124074799=Read"},
{"title": "DIO: Refining Mutual Information and Causal Chain to Enhance Machine\n  Abstract Reasoning Ability", "author": "Ruizhuo Song and Beiming Yuan", "abstract": "  Despite the outstanding performance of current deep learning models across\nvarious domains, their fundamental bottleneck in abstract reasoning remains\nunresolved. To address this challenge, the academic community has introduced\nRaven's Progressive Matrices (RPM) problems as an authoritative benchmark for\nevaluating the abstract reasoning capabilities of deep learning algorithms,\nwith a focus on core intelligence dimensions such as abstract reasoning,\npattern recognition, and complex problem-solving. Therefore, this paper centers\non solving RPM problems, aiming to contribute to enhancing the abstract\nreasoning abilities of machine intelligence. Firstly, this paper adopts a\n``causal chain modeling'' perspective to systematically analyze the complete\ncausal chain in RPM tasks: image $\\rightarrow$ abstract attributes\n$\\rightarrow$ progressive attribute patterns $\\rightarrow$ pattern consistency\n$\\rightarrow$ correct answer. Based on this analysis, the network architecture\nof the baseline model DIO is designed. However, experiments reveal that the\noptimization objective formulated for DIO, namely maximizing the variational\nlower bound of mutual information between the context and the correct option,\nfails to enable the model to genuinely acquire the predefined human reasoning\nlogic. This is attributed to two main reasons: the tightness of the lower bound\nsignificantly impacts the effectiveness of mutual information maximization, and\nmutual information, as a statistical measure, does not capture the causal\nrelationship between subjects and objects. To overcome these limitations, this\npaper progressively proposes three improvement methods:\n", "link": "http://arxiv.org/abs/2508.15387v4", "date": "2025-09-04", "relevancy": 2.1239, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5362}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5362}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DIO%3A%20Refining%20Mutual%20Information%20and%20Causal%20Chain%20to%20Enhance%20Machine%0A%20%20Abstract%20Reasoning%20Ability&body=Title%3A%20DIO%3A%20Refining%20Mutual%20Information%20and%20Causal%20Chain%20to%20Enhance%20Machine%0A%20%20Abstract%20Reasoning%20Ability%0AAuthor%3A%20Ruizhuo%20Song%20and%20Beiming%20Yuan%0AAbstract%3A%20%20%20Despite%20the%20outstanding%20performance%20of%20current%20deep%20learning%20models%20across%0Avarious%20domains%2C%20their%20fundamental%20bottleneck%20in%20abstract%20reasoning%20remains%0Aunresolved.%20To%20address%20this%20challenge%2C%20the%20academic%20community%20has%20introduced%0ARaven%27s%20Progressive%20Matrices%20%28RPM%29%20problems%20as%20an%20authoritative%20benchmark%20for%0Aevaluating%20the%20abstract%20reasoning%20capabilities%20of%20deep%20learning%20algorithms%2C%0Awith%20a%20focus%20on%20core%20intelligence%20dimensions%20such%20as%20abstract%20reasoning%2C%0Apattern%20recognition%2C%20and%20complex%20problem-solving.%20Therefore%2C%20this%20paper%20centers%0Aon%20solving%20RPM%20problems%2C%20aiming%20to%20contribute%20to%20enhancing%20the%20abstract%0Areasoning%20abilities%20of%20machine%20intelligence.%20Firstly%2C%20this%20paper%20adopts%20a%0A%60%60causal%20chain%20modeling%27%27%20perspective%20to%20systematically%20analyze%20the%20complete%0Acausal%20chain%20in%20RPM%20tasks%3A%20image%20%24%5Crightarrow%24%20abstract%20attributes%0A%24%5Crightarrow%24%20progressive%20attribute%20patterns%20%24%5Crightarrow%24%20pattern%20consistency%0A%24%5Crightarrow%24%20correct%20answer.%20Based%20on%20this%20analysis%2C%20the%20network%20architecture%0Aof%20the%20baseline%20model%20DIO%20is%20designed.%20However%2C%20experiments%20reveal%20that%20the%0Aoptimization%20objective%20formulated%20for%20DIO%2C%20namely%20maximizing%20the%20variational%0Alower%20bound%20of%20mutual%20information%20between%20the%20context%20and%20the%20correct%20option%2C%0Afails%20to%20enable%20the%20model%20to%20genuinely%20acquire%20the%20predefined%20human%20reasoning%0Alogic.%20This%20is%20attributed%20to%20two%20main%20reasons%3A%20the%20tightness%20of%20the%20lower%20bound%0Asignificantly%20impacts%20the%20effectiveness%20of%20mutual%20information%20maximization%2C%20and%0Amutual%20information%2C%20as%20a%20statistical%20measure%2C%20does%20not%20capture%20the%20causal%0Arelationship%20between%20subjects%20and%20objects.%20To%20overcome%20these%20limitations%2C%20this%0Apaper%20progressively%20proposes%20three%20improvement%20methods%3A%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15387v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDIO%253A%2520Refining%2520Mutual%2520Information%2520and%2520Causal%2520Chain%2520to%2520Enhance%2520Machine%250A%2520%2520Abstract%2520Reasoning%2520Ability%26entry.906535625%3DRuizhuo%2520Song%2520and%2520Beiming%2520Yuan%26entry.1292438233%3D%2520%2520Despite%2520the%2520outstanding%2520performance%2520of%2520current%2520deep%2520learning%2520models%2520across%250Avarious%2520domains%252C%2520their%2520fundamental%2520bottleneck%2520in%2520abstract%2520reasoning%2520remains%250Aunresolved.%2520To%2520address%2520this%2520challenge%252C%2520the%2520academic%2520community%2520has%2520introduced%250ARaven%2527s%2520Progressive%2520Matrices%2520%2528RPM%2529%2520problems%2520as%2520an%2520authoritative%2520benchmark%2520for%250Aevaluating%2520the%2520abstract%2520reasoning%2520capabilities%2520of%2520deep%2520learning%2520algorithms%252C%250Awith%2520a%2520focus%2520on%2520core%2520intelligence%2520dimensions%2520such%2520as%2520abstract%2520reasoning%252C%250Apattern%2520recognition%252C%2520and%2520complex%2520problem-solving.%2520Therefore%252C%2520this%2520paper%2520centers%250Aon%2520solving%2520RPM%2520problems%252C%2520aiming%2520to%2520contribute%2520to%2520enhancing%2520the%2520abstract%250Areasoning%2520abilities%2520of%2520machine%2520intelligence.%2520Firstly%252C%2520this%2520paper%2520adopts%2520a%250A%2560%2560causal%2520chain%2520modeling%2527%2527%2520perspective%2520to%2520systematically%2520analyze%2520the%2520complete%250Acausal%2520chain%2520in%2520RPM%2520tasks%253A%2520image%2520%2524%255Crightarrow%2524%2520abstract%2520attributes%250A%2524%255Crightarrow%2524%2520progressive%2520attribute%2520patterns%2520%2524%255Crightarrow%2524%2520pattern%2520consistency%250A%2524%255Crightarrow%2524%2520correct%2520answer.%2520Based%2520on%2520this%2520analysis%252C%2520the%2520network%2520architecture%250Aof%2520the%2520baseline%2520model%2520DIO%2520is%2520designed.%2520However%252C%2520experiments%2520reveal%2520that%2520the%250Aoptimization%2520objective%2520formulated%2520for%2520DIO%252C%2520namely%2520maximizing%2520the%2520variational%250Alower%2520bound%2520of%2520mutual%2520information%2520between%2520the%2520context%2520and%2520the%2520correct%2520option%252C%250Afails%2520to%2520enable%2520the%2520model%2520to%2520genuinely%2520acquire%2520the%2520predefined%2520human%2520reasoning%250Alogic.%2520This%2520is%2520attributed%2520to%2520two%2520main%2520reasons%253A%2520the%2520tightness%2520of%2520the%2520lower%2520bound%250Asignificantly%2520impacts%2520the%2520effectiveness%2520of%2520mutual%2520information%2520maximization%252C%2520and%250Amutual%2520information%252C%2520as%2520a%2520statistical%2520measure%252C%2520does%2520not%2520capture%2520the%2520causal%250Arelationship%2520between%2520subjects%2520and%2520objects.%2520To%2520overcome%2520these%2520limitations%252C%2520this%250Apaper%2520progressively%2520proposes%2520three%2520improvement%2520methods%253A%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15387v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DIO%3A%20Refining%20Mutual%20Information%20and%20Causal%20Chain%20to%20Enhance%20Machine%0A%20%20Abstract%20Reasoning%20Ability&entry.906535625=Ruizhuo%20Song%20and%20Beiming%20Yuan&entry.1292438233=%20%20Despite%20the%20outstanding%20performance%20of%20current%20deep%20learning%20models%20across%0Avarious%20domains%2C%20their%20fundamental%20bottleneck%20in%20abstract%20reasoning%20remains%0Aunresolved.%20To%20address%20this%20challenge%2C%20the%20academic%20community%20has%20introduced%0ARaven%27s%20Progressive%20Matrices%20%28RPM%29%20problems%20as%20an%20authoritative%20benchmark%20for%0Aevaluating%20the%20abstract%20reasoning%20capabilities%20of%20deep%20learning%20algorithms%2C%0Awith%20a%20focus%20on%20core%20intelligence%20dimensions%20such%20as%20abstract%20reasoning%2C%0Apattern%20recognition%2C%20and%20complex%20problem-solving.%20Therefore%2C%20this%20paper%20centers%0Aon%20solving%20RPM%20problems%2C%20aiming%20to%20contribute%20to%20enhancing%20the%20abstract%0Areasoning%20abilities%20of%20machine%20intelligence.%20Firstly%2C%20this%20paper%20adopts%20a%0A%60%60causal%20chain%20modeling%27%27%20perspective%20to%20systematically%20analyze%20the%20complete%0Acausal%20chain%20in%20RPM%20tasks%3A%20image%20%24%5Crightarrow%24%20abstract%20attributes%0A%24%5Crightarrow%24%20progressive%20attribute%20patterns%20%24%5Crightarrow%24%20pattern%20consistency%0A%24%5Crightarrow%24%20correct%20answer.%20Based%20on%20this%20analysis%2C%20the%20network%20architecture%0Aof%20the%20baseline%20model%20DIO%20is%20designed.%20However%2C%20experiments%20reveal%20that%20the%0Aoptimization%20objective%20formulated%20for%20DIO%2C%20namely%20maximizing%20the%20variational%0Alower%20bound%20of%20mutual%20information%20between%20the%20context%20and%20the%20correct%20option%2C%0Afails%20to%20enable%20the%20model%20to%20genuinely%20acquire%20the%20predefined%20human%20reasoning%0Alogic.%20This%20is%20attributed%20to%20two%20main%20reasons%3A%20the%20tightness%20of%20the%20lower%20bound%0Asignificantly%20impacts%20the%20effectiveness%20of%20mutual%20information%20maximization%2C%20and%0Amutual%20information%2C%20as%20a%20statistical%20measure%2C%20does%20not%20capture%20the%20causal%0Arelationship%20between%20subjects%20and%20objects.%20To%20overcome%20these%20limitations%2C%20this%0Apaper%20progressively%20proposes%20three%20improvement%20methods%3A%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15387v4&entry.124074799=Read"},
{"title": "Text2Cypher: Data Pruning using Hard Example Selection", "author": "Makbule Gulcin Ozsoy", "abstract": "  Database query languages such as SQL for relational databases and Cypher for\ngraph databases have been widely adopted. Recent advancements in large language\nmodels (LLMs) enable natural language interactions with databases through\nmodels like Text2SQL and Text2Cypher. Fine-tuning these models typically\nrequires large, diverse datasets containing non-trivial examples. However, as\ndataset size increases, the cost of fine-tuning also rises. This makes smaller,\nhigh-quality datasets essential for reducing costs for the same or better\nperformance. In this paper, we propose five hard-example selection techniques\nfor pruning the Text2Cypher dataset, aiming to preserve or improve performance\nwhile reducing resource usage. Our results show that these hard-example\nselection approaches can halve training time and costs with minimal impact on\nperformance, and demonstrates that hard-example selection provides a\ncost-effective solution.\n", "link": "http://arxiv.org/abs/2505.05122v2", "date": "2025-09-04", "relevancy": 2.1122, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4293}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.419}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text2Cypher%3A%20Data%20Pruning%20using%20Hard%20Example%20Selection&body=Title%3A%20Text2Cypher%3A%20Data%20Pruning%20using%20Hard%20Example%20Selection%0AAuthor%3A%20Makbule%20Gulcin%20Ozsoy%0AAbstract%3A%20%20%20Database%20query%20languages%20such%20as%20SQL%20for%20relational%20databases%20and%20Cypher%20for%0Agraph%20databases%20have%20been%20widely%20adopted.%20Recent%20advancements%20in%20large%20language%0Amodels%20%28LLMs%29%20enable%20natural%20language%20interactions%20with%20databases%20through%0Amodels%20like%20Text2SQL%20and%20Text2Cypher.%20Fine-tuning%20these%20models%20typically%0Arequires%20large%2C%20diverse%20datasets%20containing%20non-trivial%20examples.%20However%2C%20as%0Adataset%20size%20increases%2C%20the%20cost%20of%20fine-tuning%20also%20rises.%20This%20makes%20smaller%2C%0Ahigh-quality%20datasets%20essential%20for%20reducing%20costs%20for%20the%20same%20or%20better%0Aperformance.%20In%20this%20paper%2C%20we%20propose%20five%20hard-example%20selection%20techniques%0Afor%20pruning%20the%20Text2Cypher%20dataset%2C%20aiming%20to%20preserve%20or%20improve%20performance%0Awhile%20reducing%20resource%20usage.%20Our%20results%20show%20that%20these%20hard-example%0Aselection%20approaches%20can%20halve%20training%20time%20and%20costs%20with%20minimal%20impact%20on%0Aperformance%2C%20and%20demonstrates%20that%20hard-example%20selection%20provides%20a%0Acost-effective%20solution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05122v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText2Cypher%253A%2520Data%2520Pruning%2520using%2520Hard%2520Example%2520Selection%26entry.906535625%3DMakbule%2520Gulcin%2520Ozsoy%26entry.1292438233%3D%2520%2520Database%2520query%2520languages%2520such%2520as%2520SQL%2520for%2520relational%2520databases%2520and%2520Cypher%2520for%250Agraph%2520databases%2520have%2520been%2520widely%2520adopted.%2520Recent%2520advancements%2520in%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520enable%2520natural%2520language%2520interactions%2520with%2520databases%2520through%250Amodels%2520like%2520Text2SQL%2520and%2520Text2Cypher.%2520Fine-tuning%2520these%2520models%2520typically%250Arequires%2520large%252C%2520diverse%2520datasets%2520containing%2520non-trivial%2520examples.%2520However%252C%2520as%250Adataset%2520size%2520increases%252C%2520the%2520cost%2520of%2520fine-tuning%2520also%2520rises.%2520This%2520makes%2520smaller%252C%250Ahigh-quality%2520datasets%2520essential%2520for%2520reducing%2520costs%2520for%2520the%2520same%2520or%2520better%250Aperformance.%2520In%2520this%2520paper%252C%2520we%2520propose%2520five%2520hard-example%2520selection%2520techniques%250Afor%2520pruning%2520the%2520Text2Cypher%2520dataset%252C%2520aiming%2520to%2520preserve%2520or%2520improve%2520performance%250Awhile%2520reducing%2520resource%2520usage.%2520Our%2520results%2520show%2520that%2520these%2520hard-example%250Aselection%2520approaches%2520can%2520halve%2520training%2520time%2520and%2520costs%2520with%2520minimal%2520impact%2520on%250Aperformance%252C%2520and%2520demonstrates%2520that%2520hard-example%2520selection%2520provides%2520a%250Acost-effective%2520solution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05122v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text2Cypher%3A%20Data%20Pruning%20using%20Hard%20Example%20Selection&entry.906535625=Makbule%20Gulcin%20Ozsoy&entry.1292438233=%20%20Database%20query%20languages%20such%20as%20SQL%20for%20relational%20databases%20and%20Cypher%20for%0Agraph%20databases%20have%20been%20widely%20adopted.%20Recent%20advancements%20in%20large%20language%0Amodels%20%28LLMs%29%20enable%20natural%20language%20interactions%20with%20databases%20through%0Amodels%20like%20Text2SQL%20and%20Text2Cypher.%20Fine-tuning%20these%20models%20typically%0Arequires%20large%2C%20diverse%20datasets%20containing%20non-trivial%20examples.%20However%2C%20as%0Adataset%20size%20increases%2C%20the%20cost%20of%20fine-tuning%20also%20rises.%20This%20makes%20smaller%2C%0Ahigh-quality%20datasets%20essential%20for%20reducing%20costs%20for%20the%20same%20or%20better%0Aperformance.%20In%20this%20paper%2C%20we%20propose%20five%20hard-example%20selection%20techniques%0Afor%20pruning%20the%20Text2Cypher%20dataset%2C%20aiming%20to%20preserve%20or%20improve%20performance%0Awhile%20reducing%20resource%20usage.%20Our%20results%20show%20that%20these%20hard-example%0Aselection%20approaches%20can%20halve%20training%20time%20and%20costs%20with%20minimal%20impact%20on%0Aperformance%2C%20and%20demonstrates%20that%20hard-example%20selection%20provides%20a%0Acost-effective%20solution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05122v2&entry.124074799=Read"},
{"title": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation", "author": "Guangzhan Wang and Hongyu Zhang and Beijun Shen and Xiaodong Gu", "abstract": "  Data augmentation is a critical technique in deep learning. Traditional\nmethods like Back-translation typically focus on lexical-level rephrasing,\nwhich primarily produces variations with the same semantics. While large\nlanguage models (LLMs) have enhanced text augmentation by their \"knowledge\nemergence\" capability, controlling the style and structure of these outputs\nremains challenging and requires meticulous prompt engineering. In this paper,\nwe propose LMTransplant, a novel text augmentation paradigm leveraging LLMs.\nThe core idea of LMTransplant is transplant-then-regenerate: incorporating seed\ntext into a context expanded by LLM, and asking the LLM to regenerate a variant\nbased on the expanded context. This strategy allows the model to create more\ndiverse and creative content-level variants by fully leveraging the knowledge\nembedded in LLMs, while preserving the core attributes of the original text. We\nevaluate LMTransplant across various text-related tasks, demonstrating its\nsuperior performance over existing text augmentation methods. Moreover,\nLMTransplant demonstrates exceptional scalability as the size of augmented data\ngrows.\n", "link": "http://arxiv.org/abs/2508.14723v2", "date": "2025-09-04", "relevancy": 2.1115, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5655}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5326}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transplant%20Then%20Regenerate%3A%20A%20New%20Paradigm%20for%20Text%20Data%20Augmentation&body=Title%3A%20Transplant%20Then%20Regenerate%3A%20A%20New%20Paradigm%20for%20Text%20Data%20Augmentation%0AAuthor%3A%20Guangzhan%20Wang%20and%20Hongyu%20Zhang%20and%20Beijun%20Shen%20and%20Xiaodong%20Gu%0AAbstract%3A%20%20%20Data%20augmentation%20is%20a%20critical%20technique%20in%20deep%20learning.%20Traditional%0Amethods%20like%20Back-translation%20typically%20focus%20on%20lexical-level%20rephrasing%2C%0Awhich%20primarily%20produces%20variations%20with%20the%20same%20semantics.%20While%20large%0Alanguage%20models%20%28LLMs%29%20have%20enhanced%20text%20augmentation%20by%20their%20%22knowledge%0Aemergence%22%20capability%2C%20controlling%20the%20style%20and%20structure%20of%20these%20outputs%0Aremains%20challenging%20and%20requires%20meticulous%20prompt%20engineering.%20In%20this%20paper%2C%0Awe%20propose%20LMTransplant%2C%20a%20novel%20text%20augmentation%20paradigm%20leveraging%20LLMs.%0AThe%20core%20idea%20of%20LMTransplant%20is%20transplant-then-regenerate%3A%20incorporating%20seed%0Atext%20into%20a%20context%20expanded%20by%20LLM%2C%20and%20asking%20the%20LLM%20to%20regenerate%20a%20variant%0Abased%20on%20the%20expanded%20context.%20This%20strategy%20allows%20the%20model%20to%20create%20more%0Adiverse%20and%20creative%20content-level%20variants%20by%20fully%20leveraging%20the%20knowledge%0Aembedded%20in%20LLMs%2C%20while%20preserving%20the%20core%20attributes%20of%20the%20original%20text.%20We%0Aevaluate%20LMTransplant%20across%20various%20text-related%20tasks%2C%20demonstrating%20its%0Asuperior%20performance%20over%20existing%20text%20augmentation%20methods.%20Moreover%2C%0ALMTransplant%20demonstrates%20exceptional%20scalability%20as%20the%20size%20of%20augmented%20data%0Agrows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14723v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransplant%2520Then%2520Regenerate%253A%2520A%2520New%2520Paradigm%2520for%2520Text%2520Data%2520Augmentation%26entry.906535625%3DGuangzhan%2520Wang%2520and%2520Hongyu%2520Zhang%2520and%2520Beijun%2520Shen%2520and%2520Xiaodong%2520Gu%26entry.1292438233%3D%2520%2520Data%2520augmentation%2520is%2520a%2520critical%2520technique%2520in%2520deep%2520learning.%2520Traditional%250Amethods%2520like%2520Back-translation%2520typically%2520focus%2520on%2520lexical-level%2520rephrasing%252C%250Awhich%2520primarily%2520produces%2520variations%2520with%2520the%2520same%2520semantics.%2520While%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520have%2520enhanced%2520text%2520augmentation%2520by%2520their%2520%2522knowledge%250Aemergence%2522%2520capability%252C%2520controlling%2520the%2520style%2520and%2520structure%2520of%2520these%2520outputs%250Aremains%2520challenging%2520and%2520requires%2520meticulous%2520prompt%2520engineering.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520LMTransplant%252C%2520a%2520novel%2520text%2520augmentation%2520paradigm%2520leveraging%2520LLMs.%250AThe%2520core%2520idea%2520of%2520LMTransplant%2520is%2520transplant-then-regenerate%253A%2520incorporating%2520seed%250Atext%2520into%2520a%2520context%2520expanded%2520by%2520LLM%252C%2520and%2520asking%2520the%2520LLM%2520to%2520regenerate%2520a%2520variant%250Abased%2520on%2520the%2520expanded%2520context.%2520This%2520strategy%2520allows%2520the%2520model%2520to%2520create%2520more%250Adiverse%2520and%2520creative%2520content-level%2520variants%2520by%2520fully%2520leveraging%2520the%2520knowledge%250Aembedded%2520in%2520LLMs%252C%2520while%2520preserving%2520the%2520core%2520attributes%2520of%2520the%2520original%2520text.%2520We%250Aevaluate%2520LMTransplant%2520across%2520various%2520text-related%2520tasks%252C%2520demonstrating%2520its%250Asuperior%2520performance%2520over%2520existing%2520text%2520augmentation%2520methods.%2520Moreover%252C%250ALMTransplant%2520demonstrates%2520exceptional%2520scalability%2520as%2520the%2520size%2520of%2520augmented%2520data%250Agrows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14723v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transplant%20Then%20Regenerate%3A%20A%20New%20Paradigm%20for%20Text%20Data%20Augmentation&entry.906535625=Guangzhan%20Wang%20and%20Hongyu%20Zhang%20and%20Beijun%20Shen%20and%20Xiaodong%20Gu&entry.1292438233=%20%20Data%20augmentation%20is%20a%20critical%20technique%20in%20deep%20learning.%20Traditional%0Amethods%20like%20Back-translation%20typically%20focus%20on%20lexical-level%20rephrasing%2C%0Awhich%20primarily%20produces%20variations%20with%20the%20same%20semantics.%20While%20large%0Alanguage%20models%20%28LLMs%29%20have%20enhanced%20text%20augmentation%20by%20their%20%22knowledge%0Aemergence%22%20capability%2C%20controlling%20the%20style%20and%20structure%20of%20these%20outputs%0Aremains%20challenging%20and%20requires%20meticulous%20prompt%20engineering.%20In%20this%20paper%2C%0Awe%20propose%20LMTransplant%2C%20a%20novel%20text%20augmentation%20paradigm%20leveraging%20LLMs.%0AThe%20core%20idea%20of%20LMTransplant%20is%20transplant-then-regenerate%3A%20incorporating%20seed%0Atext%20into%20a%20context%20expanded%20by%20LLM%2C%20and%20asking%20the%20LLM%20to%20regenerate%20a%20variant%0Abased%20on%20the%20expanded%20context.%20This%20strategy%20allows%20the%20model%20to%20create%20more%0Adiverse%20and%20creative%20content-level%20variants%20by%20fully%20leveraging%20the%20knowledge%0Aembedded%20in%20LLMs%2C%20while%20preserving%20the%20core%20attributes%20of%20the%20original%20text.%20We%0Aevaluate%20LMTransplant%20across%20various%20text-related%20tasks%2C%20demonstrating%20its%0Asuperior%20performance%20over%20existing%20text%20augmentation%20methods.%20Moreover%2C%0ALMTransplant%20demonstrates%20exceptional%20scalability%20as%20the%20size%20of%20augmented%20data%0Agrows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14723v2&entry.124074799=Read"},
{"title": "DUDE: Diffusion-Based Unsupervised Cross-Domain Image Retrieval", "author": "Ruohong Yang and Peng Hu and Yunfan Li and Xi Peng", "abstract": "  Unsupervised cross-domain image retrieval (UCIR) aims to retrieve images of\nthe same category across diverse domains without relying on annotations.\nExisting UCIR methods, which align cross-domain features for the entire image,\noften struggle with the domain gap, as the object features critical for\nretrieval are frequently entangled with domain-specific styles. To address this\nchallenge, we propose DUDE, a novel UCIR method building upon feature\ndisentanglement. In brief, DUDE leverages a text-to-image generative model to\ndisentangle object features from domain-specific styles, thus facilitating\nsemantical image retrieval. To further achieve reliable alignment of the\ndisentangled object features, DUDE aligns mutual neighbors from within domains\nto across domains in a progressive manner. Extensive experiments demonstrate\nthat DUDE achieves state-of-the-art performance across three benchmark datasets\nover 13 domains. The code will be released.\n", "link": "http://arxiv.org/abs/2509.04193v1", "date": "2025-09-04", "relevancy": 2.0908, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5463}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5237}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DUDE%3A%20Diffusion-Based%20Unsupervised%20Cross-Domain%20Image%20Retrieval&body=Title%3A%20DUDE%3A%20Diffusion-Based%20Unsupervised%20Cross-Domain%20Image%20Retrieval%0AAuthor%3A%20Ruohong%20Yang%20and%20Peng%20Hu%20and%20Yunfan%20Li%20and%20Xi%20Peng%0AAbstract%3A%20%20%20Unsupervised%20cross-domain%20image%20retrieval%20%28UCIR%29%20aims%20to%20retrieve%20images%20of%0Athe%20same%20category%20across%20diverse%20domains%20without%20relying%20on%20annotations.%0AExisting%20UCIR%20methods%2C%20which%20align%20cross-domain%20features%20for%20the%20entire%20image%2C%0Aoften%20struggle%20with%20the%20domain%20gap%2C%20as%20the%20object%20features%20critical%20for%0Aretrieval%20are%20frequently%20entangled%20with%20domain-specific%20styles.%20To%20address%20this%0Achallenge%2C%20we%20propose%20DUDE%2C%20a%20novel%20UCIR%20method%20building%20upon%20feature%0Adisentanglement.%20In%20brief%2C%20DUDE%20leverages%20a%20text-to-image%20generative%20model%20to%0Adisentangle%20object%20features%20from%20domain-specific%20styles%2C%20thus%20facilitating%0Asemantical%20image%20retrieval.%20To%20further%20achieve%20reliable%20alignment%20of%20the%0Adisentangled%20object%20features%2C%20DUDE%20aligns%20mutual%20neighbors%20from%20within%20domains%0Ato%20across%20domains%20in%20a%20progressive%20manner.%20Extensive%20experiments%20demonstrate%0Athat%20DUDE%20achieves%20state-of-the-art%20performance%20across%20three%20benchmark%20datasets%0Aover%2013%20domains.%20The%20code%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04193v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDUDE%253A%2520Diffusion-Based%2520Unsupervised%2520Cross-Domain%2520Image%2520Retrieval%26entry.906535625%3DRuohong%2520Yang%2520and%2520Peng%2520Hu%2520and%2520Yunfan%2520Li%2520and%2520Xi%2520Peng%26entry.1292438233%3D%2520%2520Unsupervised%2520cross-domain%2520image%2520retrieval%2520%2528UCIR%2529%2520aims%2520to%2520retrieve%2520images%2520of%250Athe%2520same%2520category%2520across%2520diverse%2520domains%2520without%2520relying%2520on%2520annotations.%250AExisting%2520UCIR%2520methods%252C%2520which%2520align%2520cross-domain%2520features%2520for%2520the%2520entire%2520image%252C%250Aoften%2520struggle%2520with%2520the%2520domain%2520gap%252C%2520as%2520the%2520object%2520features%2520critical%2520for%250Aretrieval%2520are%2520frequently%2520entangled%2520with%2520domain-specific%2520styles.%2520To%2520address%2520this%250Achallenge%252C%2520we%2520propose%2520DUDE%252C%2520a%2520novel%2520UCIR%2520method%2520building%2520upon%2520feature%250Adisentanglement.%2520In%2520brief%252C%2520DUDE%2520leverages%2520a%2520text-to-image%2520generative%2520model%2520to%250Adisentangle%2520object%2520features%2520from%2520domain-specific%2520styles%252C%2520thus%2520facilitating%250Asemantical%2520image%2520retrieval.%2520To%2520further%2520achieve%2520reliable%2520alignment%2520of%2520the%250Adisentangled%2520object%2520features%252C%2520DUDE%2520aligns%2520mutual%2520neighbors%2520from%2520within%2520domains%250Ato%2520across%2520domains%2520in%2520a%2520progressive%2520manner.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520DUDE%2520achieves%2520state-of-the-art%2520performance%2520across%2520three%2520benchmark%2520datasets%250Aover%252013%2520domains.%2520The%2520code%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04193v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DUDE%3A%20Diffusion-Based%20Unsupervised%20Cross-Domain%20Image%20Retrieval&entry.906535625=Ruohong%20Yang%20and%20Peng%20Hu%20and%20Yunfan%20Li%20and%20Xi%20Peng&entry.1292438233=%20%20Unsupervised%20cross-domain%20image%20retrieval%20%28UCIR%29%20aims%20to%20retrieve%20images%20of%0Athe%20same%20category%20across%20diverse%20domains%20without%20relying%20on%20annotations.%0AExisting%20UCIR%20methods%2C%20which%20align%20cross-domain%20features%20for%20the%20entire%20image%2C%0Aoften%20struggle%20with%20the%20domain%20gap%2C%20as%20the%20object%20features%20critical%20for%0Aretrieval%20are%20frequently%20entangled%20with%20domain-specific%20styles.%20To%20address%20this%0Achallenge%2C%20we%20propose%20DUDE%2C%20a%20novel%20UCIR%20method%20building%20upon%20feature%0Adisentanglement.%20In%20brief%2C%20DUDE%20leverages%20a%20text-to-image%20generative%20model%20to%0Adisentangle%20object%20features%20from%20domain-specific%20styles%2C%20thus%20facilitating%0Asemantical%20image%20retrieval.%20To%20further%20achieve%20reliable%20alignment%20of%20the%0Adisentangled%20object%20features%2C%20DUDE%20aligns%20mutual%20neighbors%20from%20within%20domains%0Ato%20across%20domains%20in%20a%20progressive%20manner.%20Extensive%20experiments%20demonstrate%0Athat%20DUDE%20achieves%20state-of-the-art%20performance%20across%20three%20benchmark%20datasets%0Aover%2013%20domains.%20The%20code%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04193v1&entry.124074799=Read"},
{"title": "Dual-Scale Volume Priors with Wasserstein-Based Consistency for\n  Semi-Supervised Medical Image Segmentation", "author": "Junying Meng and Gangxuan Zhou and Jun Liu and Weihong Guo", "abstract": "  Despite signi cant progress in semi-supervised medical image segmentation,\nmost existing segmentation networks overlook e ective methodological guidance\nfor feature extraction and important prior information from\n  datasets. In this paper, we develop a semi-supervised medical image\nsegmentation framework that e ectively integrates spatial regularization\nmethods and volume priors. Speci cally, our approach integrates a strong\nexplicit volume prior at the image scale and Threshold Dynamics spatial\nregularization, both derived from variational models, into the backbone\nsegmentation network. The target region volumes for each unlabeled image are\nestimated by a regression network, which e ectively regularizes the backbone\nsegmentation network through an image-scale Wasserstein distance constraint,\nensuring that the class ratios in the segmentation results for each unlabeled\nimage match those predicted by the regression network. Additionally, we design\na dataset-scale Wasserstein distance loss function based on a weak implicit\nvolume prior, which enforces that the volume distribution predicted for the\nunlabeled dataset is similar to that of labeled dataset. Experimental results\non the 2017 ACDC dataset, PROMISE12 dataset, and thigh muscle MR image dataset\nshow the superiority of the proposed method.\n", "link": "http://arxiv.org/abs/2509.04273v1", "date": "2025-09-04", "relevancy": 2.0907, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5482}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5198}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual-Scale%20Volume%20Priors%20with%20Wasserstein-Based%20Consistency%20for%0A%20%20Semi-Supervised%20Medical%20Image%20Segmentation&body=Title%3A%20Dual-Scale%20Volume%20Priors%20with%20Wasserstein-Based%20Consistency%20for%0A%20%20Semi-Supervised%20Medical%20Image%20Segmentation%0AAuthor%3A%20Junying%20Meng%20and%20Gangxuan%20Zhou%20and%20Jun%20Liu%20and%20Weihong%20Guo%0AAbstract%3A%20%20%20Despite%20signi%20cant%20progress%20in%20semi-supervised%20medical%20image%20segmentation%2C%0Amost%20existing%20segmentation%20networks%20overlook%20e%20ective%20methodological%20guidance%0Afor%20feature%20extraction%20and%20important%20prior%20information%20from%0A%20%20datasets.%20In%20this%20paper%2C%20we%20develop%20a%20semi-supervised%20medical%20image%0Asegmentation%20framework%20that%20e%20ectively%20integrates%20spatial%20regularization%0Amethods%20and%20volume%20priors.%20Speci%20cally%2C%20our%20approach%20integrates%20a%20strong%0Aexplicit%20volume%20prior%20at%20the%20image%20scale%20and%20Threshold%20Dynamics%20spatial%0Aregularization%2C%20both%20derived%20from%20variational%20models%2C%20into%20the%20backbone%0Asegmentation%20network.%20The%20target%20region%20volumes%20for%20each%20unlabeled%20image%20are%0Aestimated%20by%20a%20regression%20network%2C%20which%20e%20ectively%20regularizes%20the%20backbone%0Asegmentation%20network%20through%20an%20image-scale%20Wasserstein%20distance%20constraint%2C%0Aensuring%20that%20the%20class%20ratios%20in%20the%20segmentation%20results%20for%20each%20unlabeled%0Aimage%20match%20those%20predicted%20by%20the%20regression%20network.%20Additionally%2C%20we%20design%0Aa%20dataset-scale%20Wasserstein%20distance%20loss%20function%20based%20on%20a%20weak%20implicit%0Avolume%20prior%2C%20which%20enforces%20that%20the%20volume%20distribution%20predicted%20for%20the%0Aunlabeled%20dataset%20is%20similar%20to%20that%20of%20labeled%20dataset.%20Experimental%20results%0Aon%20the%202017%20ACDC%20dataset%2C%20PROMISE12%20dataset%2C%20and%20thigh%20muscle%20MR%20image%20dataset%0Ashow%20the%20superiority%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04273v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual-Scale%2520Volume%2520Priors%2520with%2520Wasserstein-Based%2520Consistency%2520for%250A%2520%2520Semi-Supervised%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DJunying%2520Meng%2520and%2520Gangxuan%2520Zhou%2520and%2520Jun%2520Liu%2520and%2520Weihong%2520Guo%26entry.1292438233%3D%2520%2520Despite%2520signi%2520cant%2520progress%2520in%2520semi-supervised%2520medical%2520image%2520segmentation%252C%250Amost%2520existing%2520segmentation%2520networks%2520overlook%2520e%2520ective%2520methodological%2520guidance%250Afor%2520feature%2520extraction%2520and%2520important%2520prior%2520information%2520from%250A%2520%2520datasets.%2520In%2520this%2520paper%252C%2520we%2520develop%2520a%2520semi-supervised%2520medical%2520image%250Asegmentation%2520framework%2520that%2520e%2520ectively%2520integrates%2520spatial%2520regularization%250Amethods%2520and%2520volume%2520priors.%2520Speci%2520cally%252C%2520our%2520approach%2520integrates%2520a%2520strong%250Aexplicit%2520volume%2520prior%2520at%2520the%2520image%2520scale%2520and%2520Threshold%2520Dynamics%2520spatial%250Aregularization%252C%2520both%2520derived%2520from%2520variational%2520models%252C%2520into%2520the%2520backbone%250Asegmentation%2520network.%2520The%2520target%2520region%2520volumes%2520for%2520each%2520unlabeled%2520image%2520are%250Aestimated%2520by%2520a%2520regression%2520network%252C%2520which%2520e%2520ectively%2520regularizes%2520the%2520backbone%250Asegmentation%2520network%2520through%2520an%2520image-scale%2520Wasserstein%2520distance%2520constraint%252C%250Aensuring%2520that%2520the%2520class%2520ratios%2520in%2520the%2520segmentation%2520results%2520for%2520each%2520unlabeled%250Aimage%2520match%2520those%2520predicted%2520by%2520the%2520regression%2520network.%2520Additionally%252C%2520we%2520design%250Aa%2520dataset-scale%2520Wasserstein%2520distance%2520loss%2520function%2520based%2520on%2520a%2520weak%2520implicit%250Avolume%2520prior%252C%2520which%2520enforces%2520that%2520the%2520volume%2520distribution%2520predicted%2520for%2520the%250Aunlabeled%2520dataset%2520is%2520similar%2520to%2520that%2520of%2520labeled%2520dataset.%2520Experimental%2520results%250Aon%2520the%25202017%2520ACDC%2520dataset%252C%2520PROMISE12%2520dataset%252C%2520and%2520thigh%2520muscle%2520MR%2520image%2520dataset%250Ashow%2520the%2520superiority%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04273v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-Scale%20Volume%20Priors%20with%20Wasserstein-Based%20Consistency%20for%0A%20%20Semi-Supervised%20Medical%20Image%20Segmentation&entry.906535625=Junying%20Meng%20and%20Gangxuan%20Zhou%20and%20Jun%20Liu%20and%20Weihong%20Guo&entry.1292438233=%20%20Despite%20signi%20cant%20progress%20in%20semi-supervised%20medical%20image%20segmentation%2C%0Amost%20existing%20segmentation%20networks%20overlook%20e%20ective%20methodological%20guidance%0Afor%20feature%20extraction%20and%20important%20prior%20information%20from%0A%20%20datasets.%20In%20this%20paper%2C%20we%20develop%20a%20semi-supervised%20medical%20image%0Asegmentation%20framework%20that%20e%20ectively%20integrates%20spatial%20regularization%0Amethods%20and%20volume%20priors.%20Speci%20cally%2C%20our%20approach%20integrates%20a%20strong%0Aexplicit%20volume%20prior%20at%20the%20image%20scale%20and%20Threshold%20Dynamics%20spatial%0Aregularization%2C%20both%20derived%20from%20variational%20models%2C%20into%20the%20backbone%0Asegmentation%20network.%20The%20target%20region%20volumes%20for%20each%20unlabeled%20image%20are%0Aestimated%20by%20a%20regression%20network%2C%20which%20e%20ectively%20regularizes%20the%20backbone%0Asegmentation%20network%20through%20an%20image-scale%20Wasserstein%20distance%20constraint%2C%0Aensuring%20that%20the%20class%20ratios%20in%20the%20segmentation%20results%20for%20each%20unlabeled%0Aimage%20match%20those%20predicted%20by%20the%20regression%20network.%20Additionally%2C%20we%20design%0Aa%20dataset-scale%20Wasserstein%20distance%20loss%20function%20based%20on%20a%20weak%20implicit%0Avolume%20prior%2C%20which%20enforces%20that%20the%20volume%20distribution%20predicted%20for%20the%0Aunlabeled%20dataset%20is%20similar%20to%20that%20of%20labeled%20dataset.%20Experimental%20results%0Aon%20the%202017%20ACDC%20dataset%2C%20PROMISE12%20dataset%2C%20and%20thigh%20muscle%20MR%20image%20dataset%0Ashow%20the%20superiority%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04273v1&entry.124074799=Read"},
{"title": "Set Block Decoding is a Language Model Inference Accelerator", "author": "Itai Gat and Heli Ben-Hamu and Marton Havasi and Daniel Haziza and Jeremy Reizenstein and Gabriel Synnaeve and David Lopez-Paz and Brian Karrer and Yaron Lipman", "abstract": "  Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training.\n", "link": "http://arxiv.org/abs/2509.04185v1", "date": "2025-09-04", "relevancy": 2.0816, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5711}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5303}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Set%20Block%20Decoding%20is%20a%20Language%20Model%20Inference%20Accelerator&body=Title%3A%20Set%20Block%20Decoding%20is%20a%20Language%20Model%20Inference%20Accelerator%0AAuthor%3A%20Itai%20Gat%20and%20Heli%20Ben-Hamu%20and%20Marton%20Havasi%20and%20Daniel%20Haziza%20and%20Jeremy%20Reizenstein%20and%20Gabriel%20Synnaeve%20and%20David%20Lopez-Paz%20and%20Brian%20Karrer%20and%20Yaron%20Lipman%0AAbstract%3A%20%20%20Autoregressive%20next%20token%20prediction%20language%20models%20offer%20powerful%0Acapabilities%20but%20face%20significant%20challenges%20in%20practical%20deployment%20due%20to%20the%0Ahigh%20computational%20and%20memory%20costs%20of%20inference%2C%20particularly%20during%20the%0Adecoding%20stage.%20We%20introduce%20Set%20Block%20Decoding%20%28SBD%29%2C%20a%20simple%20and%20flexible%0Aparadigm%20that%20accelerates%20generation%20by%20integrating%20standard%20next%20token%0Aprediction%20%28NTP%29%20and%20masked%20token%20prediction%20%28MATP%29%20within%20a%20single%0Aarchitecture.%20SBD%20allows%20the%20model%20to%20sample%20multiple%2C%20not%20necessarily%0Aconsecutive%2C%20future%20tokens%20in%20parallel%2C%20a%20key%20distinction%20from%20previous%0Aacceleration%20methods.%20This%20flexibility%20allows%20the%20use%20of%20advanced%20solvers%20from%0Athe%20discrete%20diffusion%20literature%2C%20offering%20significant%20speedups%20without%0Asacrificing%20accuracy.%20SBD%20requires%20no%20architectural%20changes%20or%20extra%20training%0Ahyperparameters%2C%20maintains%20compatibility%20with%20exact%20KV-caching%2C%20and%20can%20be%0Aimplemented%20by%20fine-tuning%20existing%20next%20token%20prediction%20models.%20By%0Afine-tuning%20Llama-3.1%208B%20and%20Qwen-3%208B%2C%20we%20demonstrate%20that%20SBD%20enables%20a%203-5x%0Areduction%20in%20the%20number%20of%20forward%20passes%20required%20for%20generation%20while%0Aachieving%20same%20performance%20as%20equivalent%20NTP%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04185v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSet%2520Block%2520Decoding%2520is%2520a%2520Language%2520Model%2520Inference%2520Accelerator%26entry.906535625%3DItai%2520Gat%2520and%2520Heli%2520Ben-Hamu%2520and%2520Marton%2520Havasi%2520and%2520Daniel%2520Haziza%2520and%2520Jeremy%2520Reizenstein%2520and%2520Gabriel%2520Synnaeve%2520and%2520David%2520Lopez-Paz%2520and%2520Brian%2520Karrer%2520and%2520Yaron%2520Lipman%26entry.1292438233%3D%2520%2520Autoregressive%2520next%2520token%2520prediction%2520language%2520models%2520offer%2520powerful%250Acapabilities%2520but%2520face%2520significant%2520challenges%2520in%2520practical%2520deployment%2520due%2520to%2520the%250Ahigh%2520computational%2520and%2520memory%2520costs%2520of%2520inference%252C%2520particularly%2520during%2520the%250Adecoding%2520stage.%2520We%2520introduce%2520Set%2520Block%2520Decoding%2520%2528SBD%2529%252C%2520a%2520simple%2520and%2520flexible%250Aparadigm%2520that%2520accelerates%2520generation%2520by%2520integrating%2520standard%2520next%2520token%250Aprediction%2520%2528NTP%2529%2520and%2520masked%2520token%2520prediction%2520%2528MATP%2529%2520within%2520a%2520single%250Aarchitecture.%2520SBD%2520allows%2520the%2520model%2520to%2520sample%2520multiple%252C%2520not%2520necessarily%250Aconsecutive%252C%2520future%2520tokens%2520in%2520parallel%252C%2520a%2520key%2520distinction%2520from%2520previous%250Aacceleration%2520methods.%2520This%2520flexibility%2520allows%2520the%2520use%2520of%2520advanced%2520solvers%2520from%250Athe%2520discrete%2520diffusion%2520literature%252C%2520offering%2520significant%2520speedups%2520without%250Asacrificing%2520accuracy.%2520SBD%2520requires%2520no%2520architectural%2520changes%2520or%2520extra%2520training%250Ahyperparameters%252C%2520maintains%2520compatibility%2520with%2520exact%2520KV-caching%252C%2520and%2520can%2520be%250Aimplemented%2520by%2520fine-tuning%2520existing%2520next%2520token%2520prediction%2520models.%2520By%250Afine-tuning%2520Llama-3.1%25208B%2520and%2520Qwen-3%25208B%252C%2520we%2520demonstrate%2520that%2520SBD%2520enables%2520a%25203-5x%250Areduction%2520in%2520the%2520number%2520of%2520forward%2520passes%2520required%2520for%2520generation%2520while%250Aachieving%2520same%2520performance%2520as%2520equivalent%2520NTP%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04185v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Set%20Block%20Decoding%20is%20a%20Language%20Model%20Inference%20Accelerator&entry.906535625=Itai%20Gat%20and%20Heli%20Ben-Hamu%20and%20Marton%20Havasi%20and%20Daniel%20Haziza%20and%20Jeremy%20Reizenstein%20and%20Gabriel%20Synnaeve%20and%20David%20Lopez-Paz%20and%20Brian%20Karrer%20and%20Yaron%20Lipman&entry.1292438233=%20%20Autoregressive%20next%20token%20prediction%20language%20models%20offer%20powerful%0Acapabilities%20but%20face%20significant%20challenges%20in%20practical%20deployment%20due%20to%20the%0Ahigh%20computational%20and%20memory%20costs%20of%20inference%2C%20particularly%20during%20the%0Adecoding%20stage.%20We%20introduce%20Set%20Block%20Decoding%20%28SBD%29%2C%20a%20simple%20and%20flexible%0Aparadigm%20that%20accelerates%20generation%20by%20integrating%20standard%20next%20token%0Aprediction%20%28NTP%29%20and%20masked%20token%20prediction%20%28MATP%29%20within%20a%20single%0Aarchitecture.%20SBD%20allows%20the%20model%20to%20sample%20multiple%2C%20not%20necessarily%0Aconsecutive%2C%20future%20tokens%20in%20parallel%2C%20a%20key%20distinction%20from%20previous%0Aacceleration%20methods.%20This%20flexibility%20allows%20the%20use%20of%20advanced%20solvers%20from%0Athe%20discrete%20diffusion%20literature%2C%20offering%20significant%20speedups%20without%0Asacrificing%20accuracy.%20SBD%20requires%20no%20architectural%20changes%20or%20extra%20training%0Ahyperparameters%2C%20maintains%20compatibility%20with%20exact%20KV-caching%2C%20and%20can%20be%0Aimplemented%20by%20fine-tuning%20existing%20next%20token%20prediction%20models.%20By%0Afine-tuning%20Llama-3.1%208B%20and%20Qwen-3%208B%2C%20we%20demonstrate%20that%20SBD%20enables%20a%203-5x%0Areduction%20in%20the%20number%20of%20forward%20passes%20required%20for%20generation%20while%0Aachieving%20same%20performance%20as%20equivalent%20NTP%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04185v1&entry.124074799=Read"},
{"title": "Stitching the Story: Creating Panoramic Incident Summaries from\n  Body-Worn Footage", "author": "Dor Cohen and Inga Efrosman and Yehudit Aperstein and Alexander Apartsin", "abstract": "  First responders widely adopt body-worn cameras to document incident scenes\nand support post-event analysis. However, reviewing lengthy video footage is\nimpractical in time-critical situations. Effective situational awareness\ndemands a concise visual summary that can be quickly interpreted. This work\npresents a computer vision pipeline that transforms body-camera footage into\ninformative panoramic images summarizing the incident scene. Our method\nleverages monocular Simultaneous Localization and Mapping (SLAM) to estimate\ncamera trajectories and reconstruct the spatial layout of the environment. Key\nviewpoints are identified by clustering camera poses along the trajectory, and\nrepresentative frames from each cluster are selected. These frames are fused\ninto spatially coherent panoramic images using multi-frame stitching\ntechniques. The resulting summaries enable rapid understanding of complex\nenvironments and facilitate efficient decision-making and incident review.\n", "link": "http://arxiv.org/abs/2509.04370v1", "date": "2025-09-04", "relevancy": 2.0755, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5416}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5257}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stitching%20the%20Story%3A%20Creating%20Panoramic%20Incident%20Summaries%20from%0A%20%20Body-Worn%20Footage&body=Title%3A%20Stitching%20the%20Story%3A%20Creating%20Panoramic%20Incident%20Summaries%20from%0A%20%20Body-Worn%20Footage%0AAuthor%3A%20Dor%20Cohen%20and%20Inga%20Efrosman%20and%20Yehudit%20Aperstein%20and%20Alexander%20Apartsin%0AAbstract%3A%20%20%20First%20responders%20widely%20adopt%20body-worn%20cameras%20to%20document%20incident%20scenes%0Aand%20support%20post-event%20analysis.%20However%2C%20reviewing%20lengthy%20video%20footage%20is%0Aimpractical%20in%20time-critical%20situations.%20Effective%20situational%20awareness%0Ademands%20a%20concise%20visual%20summary%20that%20can%20be%20quickly%20interpreted.%20This%20work%0Apresents%20a%20computer%20vision%20pipeline%20that%20transforms%20body-camera%20footage%20into%0Ainformative%20panoramic%20images%20summarizing%20the%20incident%20scene.%20Our%20method%0Aleverages%20monocular%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20to%20estimate%0Acamera%20trajectories%20and%20reconstruct%20the%20spatial%20layout%20of%20the%20environment.%20Key%0Aviewpoints%20are%20identified%20by%20clustering%20camera%20poses%20along%20the%20trajectory%2C%20and%0Arepresentative%20frames%20from%20each%20cluster%20are%20selected.%20These%20frames%20are%20fused%0Ainto%20spatially%20coherent%20panoramic%20images%20using%20multi-frame%20stitching%0Atechniques.%20The%20resulting%20summaries%20enable%20rapid%20understanding%20of%20complex%0Aenvironments%20and%20facilitate%20efficient%20decision-making%20and%20incident%20review.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04370v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStitching%2520the%2520Story%253A%2520Creating%2520Panoramic%2520Incident%2520Summaries%2520from%250A%2520%2520Body-Worn%2520Footage%26entry.906535625%3DDor%2520Cohen%2520and%2520Inga%2520Efrosman%2520and%2520Yehudit%2520Aperstein%2520and%2520Alexander%2520Apartsin%26entry.1292438233%3D%2520%2520First%2520responders%2520widely%2520adopt%2520body-worn%2520cameras%2520to%2520document%2520incident%2520scenes%250Aand%2520support%2520post-event%2520analysis.%2520However%252C%2520reviewing%2520lengthy%2520video%2520footage%2520is%250Aimpractical%2520in%2520time-critical%2520situations.%2520Effective%2520situational%2520awareness%250Ademands%2520a%2520concise%2520visual%2520summary%2520that%2520can%2520be%2520quickly%2520interpreted.%2520This%2520work%250Apresents%2520a%2520computer%2520vision%2520pipeline%2520that%2520transforms%2520body-camera%2520footage%2520into%250Ainformative%2520panoramic%2520images%2520summarizing%2520the%2520incident%2520scene.%2520Our%2520method%250Aleverages%2520monocular%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%2520to%2520estimate%250Acamera%2520trajectories%2520and%2520reconstruct%2520the%2520spatial%2520layout%2520of%2520the%2520environment.%2520Key%250Aviewpoints%2520are%2520identified%2520by%2520clustering%2520camera%2520poses%2520along%2520the%2520trajectory%252C%2520and%250Arepresentative%2520frames%2520from%2520each%2520cluster%2520are%2520selected.%2520These%2520frames%2520are%2520fused%250Ainto%2520spatially%2520coherent%2520panoramic%2520images%2520using%2520multi-frame%2520stitching%250Atechniques.%2520The%2520resulting%2520summaries%2520enable%2520rapid%2520understanding%2520of%2520complex%250Aenvironments%2520and%2520facilitate%2520efficient%2520decision-making%2520and%2520incident%2520review.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04370v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stitching%20the%20Story%3A%20Creating%20Panoramic%20Incident%20Summaries%20from%0A%20%20Body-Worn%20Footage&entry.906535625=Dor%20Cohen%20and%20Inga%20Efrosman%20and%20Yehudit%20Aperstein%20and%20Alexander%20Apartsin&entry.1292438233=%20%20First%20responders%20widely%20adopt%20body-worn%20cameras%20to%20document%20incident%20scenes%0Aand%20support%20post-event%20analysis.%20However%2C%20reviewing%20lengthy%20video%20footage%20is%0Aimpractical%20in%20time-critical%20situations.%20Effective%20situational%20awareness%0Ademands%20a%20concise%20visual%20summary%20that%20can%20be%20quickly%20interpreted.%20This%20work%0Apresents%20a%20computer%20vision%20pipeline%20that%20transforms%20body-camera%20footage%20into%0Ainformative%20panoramic%20images%20summarizing%20the%20incident%20scene.%20Our%20method%0Aleverages%20monocular%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20to%20estimate%0Acamera%20trajectories%20and%20reconstruct%20the%20spatial%20layout%20of%20the%20environment.%20Key%0Aviewpoints%20are%20identified%20by%20clustering%20camera%20poses%20along%20the%20trajectory%2C%20and%0Arepresentative%20frames%20from%20each%20cluster%20are%20selected.%20These%20frames%20are%20fused%0Ainto%20spatially%20coherent%20panoramic%20images%20using%20multi-frame%20stitching%0Atechniques.%20The%20resulting%20summaries%20enable%20rapid%20understanding%20of%20complex%0Aenvironments%20and%20facilitate%20efficient%20decision-making%20and%20incident%20review.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04370v1&entry.124074799=Read"},
{"title": "A Survey of Graph Retrieval-Augmented Generation for Customized Large\n  Language Models", "author": "Qinggang Zhang and Shengyuan Chen and Yuanchen Bei and Zheng Yuan and Huachi Zhou and Zijin Hong and Hao Chen and Yilin Xiao and Chuang Zhou and Yi Chang and Xiao Huang", "abstract": "  Large language models (LLMs) have demonstrated remarkable capabilities in a\nwide range of tasks, yet their application to specialized domains remains\nchallenging due to the need for deep expertise. Retrieval-Augmented generation\n(RAG) has emerged as a promising solution to customize LLMs for professional\nfields by seamlessly integrating external knowledge bases, enabling real-time\naccess to domain-specific expertise during inference. Despite its potential,\ntraditional RAG systems, based on flat text retrieval, face three critical\nchallenges: (i) complex query understanding in professional contexts, (ii)\ndifficulties in knowledge integration across distributed sources, and (iii)\nsystem efficiency bottlenecks at scale. This survey presents a systematic\nanalysis of Graph-based Retrieval-Augmented Generation (GraphRAG), a new\nparadigm that revolutionizes domain-specific LLM applications. GraphRAG\naddresses traditional RAG limitations through three key innovations: (i)\ngraph-structured knowledge representation that explicitly captures entity\nrelationships and domain hierarchies, (ii) efficient graph-based retrieval\ntechniques that enable context-preserving knowledge retrieval with multihop\nreasoning ability, and (iii) structure-aware knowledge integration algorithms\nthat leverage retrieved knowledge for accurate and logical coherent generation\nof LLMs. In this survey, we systematically analyze the technical foundations of\nGraphRAG and examine current implementations across various professional\ndomains, identifying key technical challenges and promising research\ndirections. All the related resources of GraphRAG, including research papers,\nopen-source data, and projects, are collected for the community in\nhttps://github.com/DEEP-PolyU/Awesome-GraphRAG.\n", "link": "http://arxiv.org/abs/2501.13958v2", "date": "2025-09-04", "relevancy": 2.074, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5427}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5339}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Graph%20Retrieval-Augmented%20Generation%20for%20Customized%20Large%0A%20%20Language%20Models&body=Title%3A%20A%20Survey%20of%20Graph%20Retrieval-Augmented%20Generation%20for%20Customized%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Qinggang%20Zhang%20and%20Shengyuan%20Chen%20and%20Yuanchen%20Bei%20and%20Zheng%20Yuan%20and%20Huachi%20Zhou%20and%20Zijin%20Hong%20and%20Hao%20Chen%20and%20Yilin%20Xiao%20and%20Chuang%20Zhou%20and%20Yi%20Chang%20and%20Xiao%20Huang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%20a%0Awide%20range%20of%20tasks%2C%20yet%20their%20application%20to%20specialized%20domains%20remains%0Achallenging%20due%20to%20the%20need%20for%20deep%20expertise.%20Retrieval-Augmented%20generation%0A%28RAG%29%20has%20emerged%20as%20a%20promising%20solution%20to%20customize%20LLMs%20for%20professional%0Afields%20by%20seamlessly%20integrating%20external%20knowledge%20bases%2C%20enabling%20real-time%0Aaccess%20to%20domain-specific%20expertise%20during%20inference.%20Despite%20its%20potential%2C%0Atraditional%20RAG%20systems%2C%20based%20on%20flat%20text%20retrieval%2C%20face%20three%20critical%0Achallenges%3A%20%28i%29%20complex%20query%20understanding%20in%20professional%20contexts%2C%20%28ii%29%0Adifficulties%20in%20knowledge%20integration%20across%20distributed%20sources%2C%20and%20%28iii%29%0Asystem%20efficiency%20bottlenecks%20at%20scale.%20This%20survey%20presents%20a%20systematic%0Aanalysis%20of%20Graph-based%20Retrieval-Augmented%20Generation%20%28GraphRAG%29%2C%20a%20new%0Aparadigm%20that%20revolutionizes%20domain-specific%20LLM%20applications.%20GraphRAG%0Aaddresses%20traditional%20RAG%20limitations%20through%20three%20key%20innovations%3A%20%28i%29%0Agraph-structured%20knowledge%20representation%20that%20explicitly%20captures%20entity%0Arelationships%20and%20domain%20hierarchies%2C%20%28ii%29%20efficient%20graph-based%20retrieval%0Atechniques%20that%20enable%20context-preserving%20knowledge%20retrieval%20with%20multihop%0Areasoning%20ability%2C%20and%20%28iii%29%20structure-aware%20knowledge%20integration%20algorithms%0Athat%20leverage%20retrieved%20knowledge%20for%20accurate%20and%20logical%20coherent%20generation%0Aof%20LLMs.%20In%20this%20survey%2C%20we%20systematically%20analyze%20the%20technical%20foundations%20of%0AGraphRAG%20and%20examine%20current%20implementations%20across%20various%20professional%0Adomains%2C%20identifying%20key%20technical%20challenges%20and%20promising%20research%0Adirections.%20All%20the%20related%20resources%20of%20GraphRAG%2C%20including%20research%20papers%2C%0Aopen-source%20data%2C%20and%20projects%2C%20are%20collected%20for%20the%20community%20in%0Ahttps%3A//github.com/DEEP-PolyU/Awesome-GraphRAG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13958v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520Graph%2520Retrieval-Augmented%2520Generation%2520for%2520Customized%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DQinggang%2520Zhang%2520and%2520Shengyuan%2520Chen%2520and%2520Yuanchen%2520Bei%2520and%2520Zheng%2520Yuan%2520and%2520Huachi%2520Zhou%2520and%2520Zijin%2520Hong%2520and%2520Hao%2520Chen%2520and%2520Yilin%2520Xiao%2520and%2520Chuang%2520Zhou%2520and%2520Yi%2520Chang%2520and%2520Xiao%2520Huang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520in%2520a%250Awide%2520range%2520of%2520tasks%252C%2520yet%2520their%2520application%2520to%2520specialized%2520domains%2520remains%250Achallenging%2520due%2520to%2520the%2520need%2520for%2520deep%2520expertise.%2520Retrieval-Augmented%2520generation%250A%2528RAG%2529%2520has%2520emerged%2520as%2520a%2520promising%2520solution%2520to%2520customize%2520LLMs%2520for%2520professional%250Afields%2520by%2520seamlessly%2520integrating%2520external%2520knowledge%2520bases%252C%2520enabling%2520real-time%250Aaccess%2520to%2520domain-specific%2520expertise%2520during%2520inference.%2520Despite%2520its%2520potential%252C%250Atraditional%2520RAG%2520systems%252C%2520based%2520on%2520flat%2520text%2520retrieval%252C%2520face%2520three%2520critical%250Achallenges%253A%2520%2528i%2529%2520complex%2520query%2520understanding%2520in%2520professional%2520contexts%252C%2520%2528ii%2529%250Adifficulties%2520in%2520knowledge%2520integration%2520across%2520distributed%2520sources%252C%2520and%2520%2528iii%2529%250Asystem%2520efficiency%2520bottlenecks%2520at%2520scale.%2520This%2520survey%2520presents%2520a%2520systematic%250Aanalysis%2520of%2520Graph-based%2520Retrieval-Augmented%2520Generation%2520%2528GraphRAG%2529%252C%2520a%2520new%250Aparadigm%2520that%2520revolutionizes%2520domain-specific%2520LLM%2520applications.%2520GraphRAG%250Aaddresses%2520traditional%2520RAG%2520limitations%2520through%2520three%2520key%2520innovations%253A%2520%2528i%2529%250Agraph-structured%2520knowledge%2520representation%2520that%2520explicitly%2520captures%2520entity%250Arelationships%2520and%2520domain%2520hierarchies%252C%2520%2528ii%2529%2520efficient%2520graph-based%2520retrieval%250Atechniques%2520that%2520enable%2520context-preserving%2520knowledge%2520retrieval%2520with%2520multihop%250Areasoning%2520ability%252C%2520and%2520%2528iii%2529%2520structure-aware%2520knowledge%2520integration%2520algorithms%250Athat%2520leverage%2520retrieved%2520knowledge%2520for%2520accurate%2520and%2520logical%2520coherent%2520generation%250Aof%2520LLMs.%2520In%2520this%2520survey%252C%2520we%2520systematically%2520analyze%2520the%2520technical%2520foundations%2520of%250AGraphRAG%2520and%2520examine%2520current%2520implementations%2520across%2520various%2520professional%250Adomains%252C%2520identifying%2520key%2520technical%2520challenges%2520and%2520promising%2520research%250Adirections.%2520All%2520the%2520related%2520resources%2520of%2520GraphRAG%252C%2520including%2520research%2520papers%252C%250Aopen-source%2520data%252C%2520and%2520projects%252C%2520are%2520collected%2520for%2520the%2520community%2520in%250Ahttps%253A//github.com/DEEP-PolyU/Awesome-GraphRAG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13958v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Graph%20Retrieval-Augmented%20Generation%20for%20Customized%20Large%0A%20%20Language%20Models&entry.906535625=Qinggang%20Zhang%20and%20Shengyuan%20Chen%20and%20Yuanchen%20Bei%20and%20Zheng%20Yuan%20and%20Huachi%20Zhou%20and%20Zijin%20Hong%20and%20Hao%20Chen%20and%20Yilin%20Xiao%20and%20Chuang%20Zhou%20and%20Yi%20Chang%20and%20Xiao%20Huang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%20a%0Awide%20range%20of%20tasks%2C%20yet%20their%20application%20to%20specialized%20domains%20remains%0Achallenging%20due%20to%20the%20need%20for%20deep%20expertise.%20Retrieval-Augmented%20generation%0A%28RAG%29%20has%20emerged%20as%20a%20promising%20solution%20to%20customize%20LLMs%20for%20professional%0Afields%20by%20seamlessly%20integrating%20external%20knowledge%20bases%2C%20enabling%20real-time%0Aaccess%20to%20domain-specific%20expertise%20during%20inference.%20Despite%20its%20potential%2C%0Atraditional%20RAG%20systems%2C%20based%20on%20flat%20text%20retrieval%2C%20face%20three%20critical%0Achallenges%3A%20%28i%29%20complex%20query%20understanding%20in%20professional%20contexts%2C%20%28ii%29%0Adifficulties%20in%20knowledge%20integration%20across%20distributed%20sources%2C%20and%20%28iii%29%0Asystem%20efficiency%20bottlenecks%20at%20scale.%20This%20survey%20presents%20a%20systematic%0Aanalysis%20of%20Graph-based%20Retrieval-Augmented%20Generation%20%28GraphRAG%29%2C%20a%20new%0Aparadigm%20that%20revolutionizes%20domain-specific%20LLM%20applications.%20GraphRAG%0Aaddresses%20traditional%20RAG%20limitations%20through%20three%20key%20innovations%3A%20%28i%29%0Agraph-structured%20knowledge%20representation%20that%20explicitly%20captures%20entity%0Arelationships%20and%20domain%20hierarchies%2C%20%28ii%29%20efficient%20graph-based%20retrieval%0Atechniques%20that%20enable%20context-preserving%20knowledge%20retrieval%20with%20multihop%0Areasoning%20ability%2C%20and%20%28iii%29%20structure-aware%20knowledge%20integration%20algorithms%0Athat%20leverage%20retrieved%20knowledge%20for%20accurate%20and%20logical%20coherent%20generation%0Aof%20LLMs.%20In%20this%20survey%2C%20we%20systematically%20analyze%20the%20technical%20foundations%20of%0AGraphRAG%20and%20examine%20current%20implementations%20across%20various%20professional%0Adomains%2C%20identifying%20key%20technical%20challenges%20and%20promising%20research%0Adirections.%20All%20the%20related%20resources%20of%20GraphRAG%2C%20including%20research%20papers%2C%0Aopen-source%20data%2C%20and%20projects%2C%20are%20collected%20for%20the%20community%20in%0Ahttps%3A//github.com/DEEP-PolyU/Awesome-GraphRAG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13958v2&entry.124074799=Read"},
{"title": "Zero-shot Generalization in Inventory Management: Train, then Estimate\n  and Decide", "author": "Tarkan Temiz\u00f6z and Christina Imdahl and Remco Dijkman and Douniel Lamghari-Idrissi and Willem van Jaarsveld", "abstract": "  Deploying deep reinforcement learning (DRL) in real-world inventory\nmanagement presents challenges, including dynamic environments and uncertain\nproblem parameters, e.g. demand and lead time distributions. These challenges\nhighlight a research gap, suggesting a need for a unifying framework to model\nand solve sequential decision-making under parameter uncertainty. We address\nthis by exploring an underexplored area of DRL for inventory management:\ntraining generally capable agents (GCAs) under zero-shot generalization (ZSG).\nHere, GCAs are advanced DRL policies designed to handle a broad range of\nsampled problem instances with diverse inventory challenges. ZSG refers to the\nability to successfully apply learned policies to unseen instances with unknown\nparameters without retraining.\n  We propose a unifying Super-Markov Decision Process formulation and the\nTrain, then Estimate and Decide (TED) framework to train and deploy a GCA\ntailored to inventory management applications. The TED framework consists of\nthree phases: training a GCA on varied problem instances, continuously\nestimating problem parameters during deployment, and making decisions based on\nthese estimates. Applied to periodic review inventory problems with lost sales,\ncyclic demand patterns, and stochastic lead times, our trained agent, the\nGenerally Capable Lost Sales Network (GC-LSN) consistently outperforms\nwell-known traditional policies when problem parameters are known. Moreover,\nunder conditions where demand and/or lead time distributions are initially\nunknown and must be estimated, we benchmark against online learning methods\nthat provide worst-case performance guarantees. Our GC-LSN policy, paired with\nthe Kaplan-Meier estimator, is demonstrated to complement these methods by\nproviding superior empirical performance.\n", "link": "http://arxiv.org/abs/2411.00515v2", "date": "2025-09-04", "relevancy": 2.067, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5304}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5287}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-shot%20Generalization%20in%20Inventory%20Management%3A%20Train%2C%20then%20Estimate%0A%20%20and%20Decide&body=Title%3A%20Zero-shot%20Generalization%20in%20Inventory%20Management%3A%20Train%2C%20then%20Estimate%0A%20%20and%20Decide%0AAuthor%3A%20Tarkan%20Temiz%C3%B6z%20and%20Christina%20Imdahl%20and%20Remco%20Dijkman%20and%20Douniel%20Lamghari-Idrissi%20and%20Willem%20van%20Jaarsveld%0AAbstract%3A%20%20%20Deploying%20deep%20reinforcement%20learning%20%28DRL%29%20in%20real-world%20inventory%0Amanagement%20presents%20challenges%2C%20including%20dynamic%20environments%20and%20uncertain%0Aproblem%20parameters%2C%20e.g.%20demand%20and%20lead%20time%20distributions.%20These%20challenges%0Ahighlight%20a%20research%20gap%2C%20suggesting%20a%20need%20for%20a%20unifying%20framework%20to%20model%0Aand%20solve%20sequential%20decision-making%20under%20parameter%20uncertainty.%20We%20address%0Athis%20by%20exploring%20an%20underexplored%20area%20of%20DRL%20for%20inventory%20management%3A%0Atraining%20generally%20capable%20agents%20%28GCAs%29%20under%20zero-shot%20generalization%20%28ZSG%29.%0AHere%2C%20GCAs%20are%20advanced%20DRL%20policies%20designed%20to%20handle%20a%20broad%20range%20of%0Asampled%20problem%20instances%20with%20diverse%20inventory%20challenges.%20ZSG%20refers%20to%20the%0Aability%20to%20successfully%20apply%20learned%20policies%20to%20unseen%20instances%20with%20unknown%0Aparameters%20without%20retraining.%0A%20%20We%20propose%20a%20unifying%20Super-Markov%20Decision%20Process%20formulation%20and%20the%0ATrain%2C%20then%20Estimate%20and%20Decide%20%28TED%29%20framework%20to%20train%20and%20deploy%20a%20GCA%0Atailored%20to%20inventory%20management%20applications.%20The%20TED%20framework%20consists%20of%0Athree%20phases%3A%20training%20a%20GCA%20on%20varied%20problem%20instances%2C%20continuously%0Aestimating%20problem%20parameters%20during%20deployment%2C%20and%20making%20decisions%20based%20on%0Athese%20estimates.%20Applied%20to%20periodic%20review%20inventory%20problems%20with%20lost%20sales%2C%0Acyclic%20demand%20patterns%2C%20and%20stochastic%20lead%20times%2C%20our%20trained%20agent%2C%20the%0AGenerally%20Capable%20Lost%20Sales%20Network%20%28GC-LSN%29%20consistently%20outperforms%0Awell-known%20traditional%20policies%20when%20problem%20parameters%20are%20known.%20Moreover%2C%0Aunder%20conditions%20where%20demand%20and/or%20lead%20time%20distributions%20are%20initially%0Aunknown%20and%20must%20be%20estimated%2C%20we%20benchmark%20against%20online%20learning%20methods%0Athat%20provide%20worst-case%20performance%20guarantees.%20Our%20GC-LSN%20policy%2C%20paired%20with%0Athe%20Kaplan-Meier%20estimator%2C%20is%20demonstrated%20to%20complement%20these%20methods%20by%0Aproviding%20superior%20empirical%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.00515v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-shot%2520Generalization%2520in%2520Inventory%2520Management%253A%2520Train%252C%2520then%2520Estimate%250A%2520%2520and%2520Decide%26entry.906535625%3DTarkan%2520Temiz%25C3%25B6z%2520and%2520Christina%2520Imdahl%2520and%2520Remco%2520Dijkman%2520and%2520Douniel%2520Lamghari-Idrissi%2520and%2520Willem%2520van%2520Jaarsveld%26entry.1292438233%3D%2520%2520Deploying%2520deep%2520reinforcement%2520learning%2520%2528DRL%2529%2520in%2520real-world%2520inventory%250Amanagement%2520presents%2520challenges%252C%2520including%2520dynamic%2520environments%2520and%2520uncertain%250Aproblem%2520parameters%252C%2520e.g.%2520demand%2520and%2520lead%2520time%2520distributions.%2520These%2520challenges%250Ahighlight%2520a%2520research%2520gap%252C%2520suggesting%2520a%2520need%2520for%2520a%2520unifying%2520framework%2520to%2520model%250Aand%2520solve%2520sequential%2520decision-making%2520under%2520parameter%2520uncertainty.%2520We%2520address%250Athis%2520by%2520exploring%2520an%2520underexplored%2520area%2520of%2520DRL%2520for%2520inventory%2520management%253A%250Atraining%2520generally%2520capable%2520agents%2520%2528GCAs%2529%2520under%2520zero-shot%2520generalization%2520%2528ZSG%2529.%250AHere%252C%2520GCAs%2520are%2520advanced%2520DRL%2520policies%2520designed%2520to%2520handle%2520a%2520broad%2520range%2520of%250Asampled%2520problem%2520instances%2520with%2520diverse%2520inventory%2520challenges.%2520ZSG%2520refers%2520to%2520the%250Aability%2520to%2520successfully%2520apply%2520learned%2520policies%2520to%2520unseen%2520instances%2520with%2520unknown%250Aparameters%2520without%2520retraining.%250A%2520%2520We%2520propose%2520a%2520unifying%2520Super-Markov%2520Decision%2520Process%2520formulation%2520and%2520the%250ATrain%252C%2520then%2520Estimate%2520and%2520Decide%2520%2528TED%2529%2520framework%2520to%2520train%2520and%2520deploy%2520a%2520GCA%250Atailored%2520to%2520inventory%2520management%2520applications.%2520The%2520TED%2520framework%2520consists%2520of%250Athree%2520phases%253A%2520training%2520a%2520GCA%2520on%2520varied%2520problem%2520instances%252C%2520continuously%250Aestimating%2520problem%2520parameters%2520during%2520deployment%252C%2520and%2520making%2520decisions%2520based%2520on%250Athese%2520estimates.%2520Applied%2520to%2520periodic%2520review%2520inventory%2520problems%2520with%2520lost%2520sales%252C%250Acyclic%2520demand%2520patterns%252C%2520and%2520stochastic%2520lead%2520times%252C%2520our%2520trained%2520agent%252C%2520the%250AGenerally%2520Capable%2520Lost%2520Sales%2520Network%2520%2528GC-LSN%2529%2520consistently%2520outperforms%250Awell-known%2520traditional%2520policies%2520when%2520problem%2520parameters%2520are%2520known.%2520Moreover%252C%250Aunder%2520conditions%2520where%2520demand%2520and/or%2520lead%2520time%2520distributions%2520are%2520initially%250Aunknown%2520and%2520must%2520be%2520estimated%252C%2520we%2520benchmark%2520against%2520online%2520learning%2520methods%250Athat%2520provide%2520worst-case%2520performance%2520guarantees.%2520Our%2520GC-LSN%2520policy%252C%2520paired%2520with%250Athe%2520Kaplan-Meier%2520estimator%252C%2520is%2520demonstrated%2520to%2520complement%2520these%2520methods%2520by%250Aproviding%2520superior%2520empirical%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00515v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-shot%20Generalization%20in%20Inventory%20Management%3A%20Train%2C%20then%20Estimate%0A%20%20and%20Decide&entry.906535625=Tarkan%20Temiz%C3%B6z%20and%20Christina%20Imdahl%20and%20Remco%20Dijkman%20and%20Douniel%20Lamghari-Idrissi%20and%20Willem%20van%20Jaarsveld&entry.1292438233=%20%20Deploying%20deep%20reinforcement%20learning%20%28DRL%29%20in%20real-world%20inventory%0Amanagement%20presents%20challenges%2C%20including%20dynamic%20environments%20and%20uncertain%0Aproblem%20parameters%2C%20e.g.%20demand%20and%20lead%20time%20distributions.%20These%20challenges%0Ahighlight%20a%20research%20gap%2C%20suggesting%20a%20need%20for%20a%20unifying%20framework%20to%20model%0Aand%20solve%20sequential%20decision-making%20under%20parameter%20uncertainty.%20We%20address%0Athis%20by%20exploring%20an%20underexplored%20area%20of%20DRL%20for%20inventory%20management%3A%0Atraining%20generally%20capable%20agents%20%28GCAs%29%20under%20zero-shot%20generalization%20%28ZSG%29.%0AHere%2C%20GCAs%20are%20advanced%20DRL%20policies%20designed%20to%20handle%20a%20broad%20range%20of%0Asampled%20problem%20instances%20with%20diverse%20inventory%20challenges.%20ZSG%20refers%20to%20the%0Aability%20to%20successfully%20apply%20learned%20policies%20to%20unseen%20instances%20with%20unknown%0Aparameters%20without%20retraining.%0A%20%20We%20propose%20a%20unifying%20Super-Markov%20Decision%20Process%20formulation%20and%20the%0ATrain%2C%20then%20Estimate%20and%20Decide%20%28TED%29%20framework%20to%20train%20and%20deploy%20a%20GCA%0Atailored%20to%20inventory%20management%20applications.%20The%20TED%20framework%20consists%20of%0Athree%20phases%3A%20training%20a%20GCA%20on%20varied%20problem%20instances%2C%20continuously%0Aestimating%20problem%20parameters%20during%20deployment%2C%20and%20making%20decisions%20based%20on%0Athese%20estimates.%20Applied%20to%20periodic%20review%20inventory%20problems%20with%20lost%20sales%2C%0Acyclic%20demand%20patterns%2C%20and%20stochastic%20lead%20times%2C%20our%20trained%20agent%2C%20the%0AGenerally%20Capable%20Lost%20Sales%20Network%20%28GC-LSN%29%20consistently%20outperforms%0Awell-known%20traditional%20policies%20when%20problem%20parameters%20are%20known.%20Moreover%2C%0Aunder%20conditions%20where%20demand%20and/or%20lead%20time%20distributions%20are%20initially%0Aunknown%20and%20must%20be%20estimated%2C%20we%20benchmark%20against%20online%20learning%20methods%0Athat%20provide%20worst-case%20performance%20guarantees.%20Our%20GC-LSN%20policy%2C%20paired%20with%0Athe%20Kaplan-Meier%20estimator%2C%20is%20demonstrated%20to%20complement%20these%20methods%20by%0Aproviding%20superior%20empirical%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.00515v2&entry.124074799=Read"},
{"title": "Unveiling the Role of Data Uncertainty in Tabular Deep Learning", "author": "Nikolay Kartashev and Ivan Rubachev and Artem Babenko", "abstract": "  Recent advancements in tabular deep learning have demonstrated exceptional\npractical performance, yet the field often lacks a clear understanding of why\nthese techniques actually succeed. To address this gap, our paper highlights\nthe importance of the concept of data uncertainty for explaining the\neffectiveness of the recent tabular DL methods. In particular, we reveal that\nthe success of many beneficial design choices in tabular DL, such as numerical\nfeature embeddings, retrieval-augmented models and advanced ensembling\nstrategies, can be largely attributed to their implicit mechanisms for managing\nhigh data uncertainty. By dissecting these mechanisms, we provide a unifying\nunderstanding of the recent performance improvements. Furthermore, the insights\nderived from this data-uncertainty perspective directly allowed us to develop\nmore effective numerical feature embeddings as an immediate practical outcome\nof our analysis. Overall, our work paves the way to foundational understanding\nof the benefits introduced by modern tabular methods that results in the\nconcrete advancements of existing techniques and outlines future research\ndirections for tabular DL.\n", "link": "http://arxiv.org/abs/2509.04430v1", "date": "2025-09-04", "relevancy": 2.0572, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6002}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5199}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20the%20Role%20of%20Data%20Uncertainty%20in%20Tabular%20Deep%20Learning&body=Title%3A%20Unveiling%20the%20Role%20of%20Data%20Uncertainty%20in%20Tabular%20Deep%20Learning%0AAuthor%3A%20Nikolay%20Kartashev%20and%20Ivan%20Rubachev%20and%20Artem%20Babenko%0AAbstract%3A%20%20%20Recent%20advancements%20in%20tabular%20deep%20learning%20have%20demonstrated%20exceptional%0Apractical%20performance%2C%20yet%20the%20field%20often%20lacks%20a%20clear%20understanding%20of%20why%0Athese%20techniques%20actually%20succeed.%20To%20address%20this%20gap%2C%20our%20paper%20highlights%0Athe%20importance%20of%20the%20concept%20of%20data%20uncertainty%20for%20explaining%20the%0Aeffectiveness%20of%20the%20recent%20tabular%20DL%20methods.%20In%20particular%2C%20we%20reveal%20that%0Athe%20success%20of%20many%20beneficial%20design%20choices%20in%20tabular%20DL%2C%20such%20as%20numerical%0Afeature%20embeddings%2C%20retrieval-augmented%20models%20and%20advanced%20ensembling%0Astrategies%2C%20can%20be%20largely%20attributed%20to%20their%20implicit%20mechanisms%20for%20managing%0Ahigh%20data%20uncertainty.%20By%20dissecting%20these%20mechanisms%2C%20we%20provide%20a%20unifying%0Aunderstanding%20of%20the%20recent%20performance%20improvements.%20Furthermore%2C%20the%20insights%0Aderived%20from%20this%20data-uncertainty%20perspective%20directly%20allowed%20us%20to%20develop%0Amore%20effective%20numerical%20feature%20embeddings%20as%20an%20immediate%20practical%20outcome%0Aof%20our%20analysis.%20Overall%2C%20our%20work%20paves%20the%20way%20to%20foundational%20understanding%0Aof%20the%20benefits%20introduced%20by%20modern%20tabular%20methods%20that%20results%20in%20the%0Aconcrete%20advancements%20of%20existing%20techniques%20and%20outlines%20future%20research%0Adirections%20for%20tabular%20DL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04430v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520the%2520Role%2520of%2520Data%2520Uncertainty%2520in%2520Tabular%2520Deep%2520Learning%26entry.906535625%3DNikolay%2520Kartashev%2520and%2520Ivan%2520Rubachev%2520and%2520Artem%2520Babenko%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520tabular%2520deep%2520learning%2520have%2520demonstrated%2520exceptional%250Apractical%2520performance%252C%2520yet%2520the%2520field%2520often%2520lacks%2520a%2520clear%2520understanding%2520of%2520why%250Athese%2520techniques%2520actually%2520succeed.%2520To%2520address%2520this%2520gap%252C%2520our%2520paper%2520highlights%250Athe%2520importance%2520of%2520the%2520concept%2520of%2520data%2520uncertainty%2520for%2520explaining%2520the%250Aeffectiveness%2520of%2520the%2520recent%2520tabular%2520DL%2520methods.%2520In%2520particular%252C%2520we%2520reveal%2520that%250Athe%2520success%2520of%2520many%2520beneficial%2520design%2520choices%2520in%2520tabular%2520DL%252C%2520such%2520as%2520numerical%250Afeature%2520embeddings%252C%2520retrieval-augmented%2520models%2520and%2520advanced%2520ensembling%250Astrategies%252C%2520can%2520be%2520largely%2520attributed%2520to%2520their%2520implicit%2520mechanisms%2520for%2520managing%250Ahigh%2520data%2520uncertainty.%2520By%2520dissecting%2520these%2520mechanisms%252C%2520we%2520provide%2520a%2520unifying%250Aunderstanding%2520of%2520the%2520recent%2520performance%2520improvements.%2520Furthermore%252C%2520the%2520insights%250Aderived%2520from%2520this%2520data-uncertainty%2520perspective%2520directly%2520allowed%2520us%2520to%2520develop%250Amore%2520effective%2520numerical%2520feature%2520embeddings%2520as%2520an%2520immediate%2520practical%2520outcome%250Aof%2520our%2520analysis.%2520Overall%252C%2520our%2520work%2520paves%2520the%2520way%2520to%2520foundational%2520understanding%250Aof%2520the%2520benefits%2520introduced%2520by%2520modern%2520tabular%2520methods%2520that%2520results%2520in%2520the%250Aconcrete%2520advancements%2520of%2520existing%2520techniques%2520and%2520outlines%2520future%2520research%250Adirections%2520for%2520tabular%2520DL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04430v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20the%20Role%20of%20Data%20Uncertainty%20in%20Tabular%20Deep%20Learning&entry.906535625=Nikolay%20Kartashev%20and%20Ivan%20Rubachev%20and%20Artem%20Babenko&entry.1292438233=%20%20Recent%20advancements%20in%20tabular%20deep%20learning%20have%20demonstrated%20exceptional%0Apractical%20performance%2C%20yet%20the%20field%20often%20lacks%20a%20clear%20understanding%20of%20why%0Athese%20techniques%20actually%20succeed.%20To%20address%20this%20gap%2C%20our%20paper%20highlights%0Athe%20importance%20of%20the%20concept%20of%20data%20uncertainty%20for%20explaining%20the%0Aeffectiveness%20of%20the%20recent%20tabular%20DL%20methods.%20In%20particular%2C%20we%20reveal%20that%0Athe%20success%20of%20many%20beneficial%20design%20choices%20in%20tabular%20DL%2C%20such%20as%20numerical%0Afeature%20embeddings%2C%20retrieval-augmented%20models%20and%20advanced%20ensembling%0Astrategies%2C%20can%20be%20largely%20attributed%20to%20their%20implicit%20mechanisms%20for%20managing%0Ahigh%20data%20uncertainty.%20By%20dissecting%20these%20mechanisms%2C%20we%20provide%20a%20unifying%0Aunderstanding%20of%20the%20recent%20performance%20improvements.%20Furthermore%2C%20the%20insights%0Aderived%20from%20this%20data-uncertainty%20perspective%20directly%20allowed%20us%20to%20develop%0Amore%20effective%20numerical%20feature%20embeddings%20as%20an%20immediate%20practical%20outcome%0Aof%20our%20analysis.%20Overall%2C%20our%20work%20paves%20the%20way%20to%20foundational%20understanding%0Aof%20the%20benefits%20introduced%20by%20modern%20tabular%20methods%20that%20results%20in%20the%0Aconcrete%20advancements%20of%20existing%20techniques%20and%20outlines%20future%20research%0Adirections%20for%20tabular%20DL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04430v1&entry.124074799=Read"},
{"title": "Aesthetic Image Captioning with Saliency Enhanced MLLMs", "author": "Yilin Tao and Jiashui Huang and Huaze Xu and Ling Shao", "abstract": "  Aesthetic Image Captioning (AIC) aims to generate textual descriptions of\nimage aesthetics, becoming a key research direction in the field of\ncomputational aesthetics. In recent years, pretrained Multimodal Large Language\nModels (MLLMs) have advanced rapidly, leading to a significant increase in\nimage aesthetics research that integrates both visual and textual modalities.\nHowever, most existing studies on image aesthetics primarily focus on\npredicting aesthetic ratings and have shown limited application in AIC.\nExisting AIC works leveraging MLLMs predominantly rely on fine-tuning methods\nwithout specifically adapting MLLMs to focus on target aesthetic content. To\naddress this limitation, we propose the Aesthetic Saliency Enhanced Multimodal\nLarge Language Model (ASE-MLLM), an end-to-end framework that explicitly\nincorporates aesthetic saliency into MLLMs. Within this framework, we introduce\nthe Image Aesthetic Saliency Module (IASM), which efficiently and effectively\nextracts aesthetic saliency features from images. Additionally, we design\nIAS-ViT as the image encoder for MLLMs, this module fuses aesthetic saliency\nfeatures with original image features via a cross-attention mechanism. To the\nbest of our knowledge, ASE-MLLM is the first framework to integrate image\naesthetic saliency into MLLMs specifically for AIC tasks. Extensive experiments\ndemonstrated that our approach significantly outperformed traditional methods\nand generic MLLMs on current mainstream AIC benchmarks, achieving\nstate-of-the-art (SOTA) performance.\n", "link": "http://arxiv.org/abs/2509.04378v1", "date": "2025-09-04", "relevancy": 2.0498, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5462}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5119}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aesthetic%20Image%20Captioning%20with%20Saliency%20Enhanced%20MLLMs&body=Title%3A%20Aesthetic%20Image%20Captioning%20with%20Saliency%20Enhanced%20MLLMs%0AAuthor%3A%20Yilin%20Tao%20and%20Jiashui%20Huang%20and%20Huaze%20Xu%20and%20Ling%20Shao%0AAbstract%3A%20%20%20Aesthetic%20Image%20Captioning%20%28AIC%29%20aims%20to%20generate%20textual%20descriptions%20of%0Aimage%20aesthetics%2C%20becoming%20a%20key%20research%20direction%20in%20the%20field%20of%0Acomputational%20aesthetics.%20In%20recent%20years%2C%20pretrained%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29%20have%20advanced%20rapidly%2C%20leading%20to%20a%20significant%20increase%20in%0Aimage%20aesthetics%20research%20that%20integrates%20both%20visual%20and%20textual%20modalities.%0AHowever%2C%20most%20existing%20studies%20on%20image%20aesthetics%20primarily%20focus%20on%0Apredicting%20aesthetic%20ratings%20and%20have%20shown%20limited%20application%20in%20AIC.%0AExisting%20AIC%20works%20leveraging%20MLLMs%20predominantly%20rely%20on%20fine-tuning%20methods%0Awithout%20specifically%20adapting%20MLLMs%20to%20focus%20on%20target%20aesthetic%20content.%20To%0Aaddress%20this%20limitation%2C%20we%20propose%20the%20Aesthetic%20Saliency%20Enhanced%20Multimodal%0ALarge%20Language%20Model%20%28ASE-MLLM%29%2C%20an%20end-to-end%20framework%20that%20explicitly%0Aincorporates%20aesthetic%20saliency%20into%20MLLMs.%20Within%20this%20framework%2C%20we%20introduce%0Athe%20Image%20Aesthetic%20Saliency%20Module%20%28IASM%29%2C%20which%20efficiently%20and%20effectively%0Aextracts%20aesthetic%20saliency%20features%20from%20images.%20Additionally%2C%20we%20design%0AIAS-ViT%20as%20the%20image%20encoder%20for%20MLLMs%2C%20this%20module%20fuses%20aesthetic%20saliency%0Afeatures%20with%20original%20image%20features%20via%20a%20cross-attention%20mechanism.%20To%20the%0Abest%20of%20our%20knowledge%2C%20ASE-MLLM%20is%20the%20first%20framework%20to%20integrate%20image%0Aaesthetic%20saliency%20into%20MLLMs%20specifically%20for%20AIC%20tasks.%20Extensive%20experiments%0Ademonstrated%20that%20our%20approach%20significantly%20outperformed%20traditional%20methods%0Aand%20generic%20MLLMs%20on%20current%20mainstream%20AIC%20benchmarks%2C%20achieving%0Astate-of-the-art%20%28SOTA%29%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04378v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAesthetic%2520Image%2520Captioning%2520with%2520Saliency%2520Enhanced%2520MLLMs%26entry.906535625%3DYilin%2520Tao%2520and%2520Jiashui%2520Huang%2520and%2520Huaze%2520Xu%2520and%2520Ling%2520Shao%26entry.1292438233%3D%2520%2520Aesthetic%2520Image%2520Captioning%2520%2528AIC%2529%2520aims%2520to%2520generate%2520textual%2520descriptions%2520of%250Aimage%2520aesthetics%252C%2520becoming%2520a%2520key%2520research%2520direction%2520in%2520the%2520field%2520of%250Acomputational%2520aesthetics.%2520In%2520recent%2520years%252C%2520pretrained%2520Multimodal%2520Large%2520Language%250AModels%2520%2528MLLMs%2529%2520have%2520advanced%2520rapidly%252C%2520leading%2520to%2520a%2520significant%2520increase%2520in%250Aimage%2520aesthetics%2520research%2520that%2520integrates%2520both%2520visual%2520and%2520textual%2520modalities.%250AHowever%252C%2520most%2520existing%2520studies%2520on%2520image%2520aesthetics%2520primarily%2520focus%2520on%250Apredicting%2520aesthetic%2520ratings%2520and%2520have%2520shown%2520limited%2520application%2520in%2520AIC.%250AExisting%2520AIC%2520works%2520leveraging%2520MLLMs%2520predominantly%2520rely%2520on%2520fine-tuning%2520methods%250Awithout%2520specifically%2520adapting%2520MLLMs%2520to%2520focus%2520on%2520target%2520aesthetic%2520content.%2520To%250Aaddress%2520this%2520limitation%252C%2520we%2520propose%2520the%2520Aesthetic%2520Saliency%2520Enhanced%2520Multimodal%250ALarge%2520Language%2520Model%2520%2528ASE-MLLM%2529%252C%2520an%2520end-to-end%2520framework%2520that%2520explicitly%250Aincorporates%2520aesthetic%2520saliency%2520into%2520MLLMs.%2520Within%2520this%2520framework%252C%2520we%2520introduce%250Athe%2520Image%2520Aesthetic%2520Saliency%2520Module%2520%2528IASM%2529%252C%2520which%2520efficiently%2520and%2520effectively%250Aextracts%2520aesthetic%2520saliency%2520features%2520from%2520images.%2520Additionally%252C%2520we%2520design%250AIAS-ViT%2520as%2520the%2520image%2520encoder%2520for%2520MLLMs%252C%2520this%2520module%2520fuses%2520aesthetic%2520saliency%250Afeatures%2520with%2520original%2520image%2520features%2520via%2520a%2520cross-attention%2520mechanism.%2520To%2520the%250Abest%2520of%2520our%2520knowledge%252C%2520ASE-MLLM%2520is%2520the%2520first%2520framework%2520to%2520integrate%2520image%250Aaesthetic%2520saliency%2520into%2520MLLMs%2520specifically%2520for%2520AIC%2520tasks.%2520Extensive%2520experiments%250Ademonstrated%2520that%2520our%2520approach%2520significantly%2520outperformed%2520traditional%2520methods%250Aand%2520generic%2520MLLMs%2520on%2520current%2520mainstream%2520AIC%2520benchmarks%252C%2520achieving%250Astate-of-the-art%2520%2528SOTA%2529%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04378v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aesthetic%20Image%20Captioning%20with%20Saliency%20Enhanced%20MLLMs&entry.906535625=Yilin%20Tao%20and%20Jiashui%20Huang%20and%20Huaze%20Xu%20and%20Ling%20Shao&entry.1292438233=%20%20Aesthetic%20Image%20Captioning%20%28AIC%29%20aims%20to%20generate%20textual%20descriptions%20of%0Aimage%20aesthetics%2C%20becoming%20a%20key%20research%20direction%20in%20the%20field%20of%0Acomputational%20aesthetics.%20In%20recent%20years%2C%20pretrained%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29%20have%20advanced%20rapidly%2C%20leading%20to%20a%20significant%20increase%20in%0Aimage%20aesthetics%20research%20that%20integrates%20both%20visual%20and%20textual%20modalities.%0AHowever%2C%20most%20existing%20studies%20on%20image%20aesthetics%20primarily%20focus%20on%0Apredicting%20aesthetic%20ratings%20and%20have%20shown%20limited%20application%20in%20AIC.%0AExisting%20AIC%20works%20leveraging%20MLLMs%20predominantly%20rely%20on%20fine-tuning%20methods%0Awithout%20specifically%20adapting%20MLLMs%20to%20focus%20on%20target%20aesthetic%20content.%20To%0Aaddress%20this%20limitation%2C%20we%20propose%20the%20Aesthetic%20Saliency%20Enhanced%20Multimodal%0ALarge%20Language%20Model%20%28ASE-MLLM%29%2C%20an%20end-to-end%20framework%20that%20explicitly%0Aincorporates%20aesthetic%20saliency%20into%20MLLMs.%20Within%20this%20framework%2C%20we%20introduce%0Athe%20Image%20Aesthetic%20Saliency%20Module%20%28IASM%29%2C%20which%20efficiently%20and%20effectively%0Aextracts%20aesthetic%20saliency%20features%20from%20images.%20Additionally%2C%20we%20design%0AIAS-ViT%20as%20the%20image%20encoder%20for%20MLLMs%2C%20this%20module%20fuses%20aesthetic%20saliency%0Afeatures%20with%20original%20image%20features%20via%20a%20cross-attention%20mechanism.%20To%20the%0Abest%20of%20our%20knowledge%2C%20ASE-MLLM%20is%20the%20first%20framework%20to%20integrate%20image%0Aaesthetic%20saliency%20into%20MLLMs%20specifically%20for%20AIC%20tasks.%20Extensive%20experiments%0Ademonstrated%20that%20our%20approach%20significantly%20outperformed%20traditional%20methods%0Aand%20generic%20MLLMs%20on%20current%20mainstream%20AIC%20benchmarks%2C%20achieving%0Astate-of-the-art%20%28SOTA%29%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04378v1&entry.124074799=Read"},
{"title": "SRWToolkit: An Open Source Wizard of Oz Toolkit to Create Social Robotic\n  Avatars", "author": "Atikkhan Faridkhan Nilgar and Kristof Van Laerhoven and Ayub Kinoti", "abstract": "  We present SRWToolkit, an open-source Wizard of Oz toolkit designed to\nfacilitate the rapid prototyping of social robotic avatars powered by local\nlarge language models (LLMs). Our web-based toolkit enables multimodal\ninteraction through text input, button-activated speech, and wake-word command.\nThe toolkit offers real-time configuration of avatar appearance, behavior,\nlanguage, and voice via an intuitive control panel. In contrast to prior works\nthat rely on cloud-based LLM services, SRWToolkit emphasizes modularity and\nensures on-device functionality through local LLM inference. In our small-scale\nuser study ($n=11$), participants created and interacted with diverse robotic\nroles (hospital receptionist, mathematics teacher, and driving assistant),\nwhich demonstrated positive outcomes in the toolkit's usability, trust, and\nuser experience. The toolkit enables rapid and efficient development of robot\ncharacters customized to researchers' needs, supporting scalable research in\nhuman-robot interaction.\n", "link": "http://arxiv.org/abs/2509.04356v1", "date": "2025-09-04", "relevancy": 2.0462, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5683}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5066}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SRWToolkit%3A%20An%20Open%20Source%20Wizard%20of%20Oz%20Toolkit%20to%20Create%20Social%20Robotic%0A%20%20Avatars&body=Title%3A%20SRWToolkit%3A%20An%20Open%20Source%20Wizard%20of%20Oz%20Toolkit%20to%20Create%20Social%20Robotic%0A%20%20Avatars%0AAuthor%3A%20Atikkhan%20Faridkhan%20Nilgar%20and%20Kristof%20Van%20Laerhoven%20and%20Ayub%20Kinoti%0AAbstract%3A%20%20%20We%20present%20SRWToolkit%2C%20an%20open-source%20Wizard%20of%20Oz%20toolkit%20designed%20to%0Afacilitate%20the%20rapid%20prototyping%20of%20social%20robotic%20avatars%20powered%20by%20local%0Alarge%20language%20models%20%28LLMs%29.%20Our%20web-based%20toolkit%20enables%20multimodal%0Ainteraction%20through%20text%20input%2C%20button-activated%20speech%2C%20and%20wake-word%20command.%0AThe%20toolkit%20offers%20real-time%20configuration%20of%20avatar%20appearance%2C%20behavior%2C%0Alanguage%2C%20and%20voice%20via%20an%20intuitive%20control%20panel.%20In%20contrast%20to%20prior%20works%0Athat%20rely%20on%20cloud-based%20LLM%20services%2C%20SRWToolkit%20emphasizes%20modularity%20and%0Aensures%20on-device%20functionality%20through%20local%20LLM%20inference.%20In%20our%20small-scale%0Auser%20study%20%28%24n%3D11%24%29%2C%20participants%20created%20and%20interacted%20with%20diverse%20robotic%0Aroles%20%28hospital%20receptionist%2C%20mathematics%20teacher%2C%20and%20driving%20assistant%29%2C%0Awhich%20demonstrated%20positive%20outcomes%20in%20the%20toolkit%27s%20usability%2C%20trust%2C%20and%0Auser%20experience.%20The%20toolkit%20enables%20rapid%20and%20efficient%20development%20of%20robot%0Acharacters%20customized%20to%20researchers%27%20needs%2C%20supporting%20scalable%20research%20in%0Ahuman-robot%20interaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04356v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSRWToolkit%253A%2520An%2520Open%2520Source%2520Wizard%2520of%2520Oz%2520Toolkit%2520to%2520Create%2520Social%2520Robotic%250A%2520%2520Avatars%26entry.906535625%3DAtikkhan%2520Faridkhan%2520Nilgar%2520and%2520Kristof%2520Van%2520Laerhoven%2520and%2520Ayub%2520Kinoti%26entry.1292438233%3D%2520%2520We%2520present%2520SRWToolkit%252C%2520an%2520open-source%2520Wizard%2520of%2520Oz%2520toolkit%2520designed%2520to%250Afacilitate%2520the%2520rapid%2520prototyping%2520of%2520social%2520robotic%2520avatars%2520powered%2520by%2520local%250Alarge%2520language%2520models%2520%2528LLMs%2529.%2520Our%2520web-based%2520toolkit%2520enables%2520multimodal%250Ainteraction%2520through%2520text%2520input%252C%2520button-activated%2520speech%252C%2520and%2520wake-word%2520command.%250AThe%2520toolkit%2520offers%2520real-time%2520configuration%2520of%2520avatar%2520appearance%252C%2520behavior%252C%250Alanguage%252C%2520and%2520voice%2520via%2520an%2520intuitive%2520control%2520panel.%2520In%2520contrast%2520to%2520prior%2520works%250Athat%2520rely%2520on%2520cloud-based%2520LLM%2520services%252C%2520SRWToolkit%2520emphasizes%2520modularity%2520and%250Aensures%2520on-device%2520functionality%2520through%2520local%2520LLM%2520inference.%2520In%2520our%2520small-scale%250Auser%2520study%2520%2528%2524n%253D11%2524%2529%252C%2520participants%2520created%2520and%2520interacted%2520with%2520diverse%2520robotic%250Aroles%2520%2528hospital%2520receptionist%252C%2520mathematics%2520teacher%252C%2520and%2520driving%2520assistant%2529%252C%250Awhich%2520demonstrated%2520positive%2520outcomes%2520in%2520the%2520toolkit%2527s%2520usability%252C%2520trust%252C%2520and%250Auser%2520experience.%2520The%2520toolkit%2520enables%2520rapid%2520and%2520efficient%2520development%2520of%2520robot%250Acharacters%2520customized%2520to%2520researchers%2527%2520needs%252C%2520supporting%2520scalable%2520research%2520in%250Ahuman-robot%2520interaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04356v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SRWToolkit%3A%20An%20Open%20Source%20Wizard%20of%20Oz%20Toolkit%20to%20Create%20Social%20Robotic%0A%20%20Avatars&entry.906535625=Atikkhan%20Faridkhan%20Nilgar%20and%20Kristof%20Van%20Laerhoven%20and%20Ayub%20Kinoti&entry.1292438233=%20%20We%20present%20SRWToolkit%2C%20an%20open-source%20Wizard%20of%20Oz%20toolkit%20designed%20to%0Afacilitate%20the%20rapid%20prototyping%20of%20social%20robotic%20avatars%20powered%20by%20local%0Alarge%20language%20models%20%28LLMs%29.%20Our%20web-based%20toolkit%20enables%20multimodal%0Ainteraction%20through%20text%20input%2C%20button-activated%20speech%2C%20and%20wake-word%20command.%0AThe%20toolkit%20offers%20real-time%20configuration%20of%20avatar%20appearance%2C%20behavior%2C%0Alanguage%2C%20and%20voice%20via%20an%20intuitive%20control%20panel.%20In%20contrast%20to%20prior%20works%0Athat%20rely%20on%20cloud-based%20LLM%20services%2C%20SRWToolkit%20emphasizes%20modularity%20and%0Aensures%20on-device%20functionality%20through%20local%20LLM%20inference.%20In%20our%20small-scale%0Auser%20study%20%28%24n%3D11%24%29%2C%20participants%20created%20and%20interacted%20with%20diverse%20robotic%0Aroles%20%28hospital%20receptionist%2C%20mathematics%20teacher%2C%20and%20driving%20assistant%29%2C%0Awhich%20demonstrated%20positive%20outcomes%20in%20the%20toolkit%27s%20usability%2C%20trust%2C%20and%0Auser%20experience.%20The%20toolkit%20enables%20rapid%20and%20efficient%20development%20of%20robot%0Acharacters%20customized%20to%20researchers%27%20needs%2C%20supporting%20scalable%20research%20in%0Ahuman-robot%20interaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04356v1&entry.124074799=Read"},
{"title": "Revisiting Simple Baselines for In-The-Wild Deepfake Detection", "author": "Orlando Castaneda and Kevin So-Tang and Kshitij Gurung", "abstract": "  The widespread adoption of synthetic media demands accessible deepfake\ndetectors and realistic benchmarks. While most existing research evaluates\ndeepfake detectors on highly controlled datasets, we focus on the recently\nreleased \"in-the-wild\" benchmark, Deepfake-Eval-2024. Initial reporting on\nDeepfake-Eval-2024 showed that three finetuned open-source models achieve\naccuracies between 61% and 69%, significantly lagging behind the leading\ncommercial deepfake detector with 82% accuracy. Our work revisits one of these\nbaseline approaches, originally introduced by Ojha et al., which adapts\nstandard pretrained vision backbones to produce generalizable deepfake\ndetectors. We demonstrate that with better-tuned hyperparameters, this simple\napproach actually yields much higher performance -- 81% accuracy on\nDeepfake-Eval-2024 -- surpassing the previously reported accuracy of this\nbaseline approach by 18% and competing with commercial deepfake detectors. We\ndiscuss tradeoffs in accuracy, computational costs, and interpretability,\nfocusing on how practical these deepfake detectors might be when deployed in\nreal-world settings. Our code can be found at\nhttps://github.com/Deepfake-Detection-KKO/deepfake-detection.\n", "link": "http://arxiv.org/abs/2509.04150v1", "date": "2025-09-04", "relevancy": 2.0438, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5134}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5105}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Simple%20Baselines%20for%20In-The-Wild%20Deepfake%20Detection&body=Title%3A%20Revisiting%20Simple%20Baselines%20for%20In-The-Wild%20Deepfake%20Detection%0AAuthor%3A%20Orlando%20Castaneda%20and%20Kevin%20So-Tang%20and%20Kshitij%20Gurung%0AAbstract%3A%20%20%20The%20widespread%20adoption%20of%20synthetic%20media%20demands%20accessible%20deepfake%0Adetectors%20and%20realistic%20benchmarks.%20While%20most%20existing%20research%20evaluates%0Adeepfake%20detectors%20on%20highly%20controlled%20datasets%2C%20we%20focus%20on%20the%20recently%0Areleased%20%22in-the-wild%22%20benchmark%2C%20Deepfake-Eval-2024.%20Initial%20reporting%20on%0ADeepfake-Eval-2024%20showed%20that%20three%20finetuned%20open-source%20models%20achieve%0Aaccuracies%20between%2061%25%20and%2069%25%2C%20significantly%20lagging%20behind%20the%20leading%0Acommercial%20deepfake%20detector%20with%2082%25%20accuracy.%20Our%20work%20revisits%20one%20of%20these%0Abaseline%20approaches%2C%20originally%20introduced%20by%20Ojha%20et%20al.%2C%20which%20adapts%0Astandard%20pretrained%20vision%20backbones%20to%20produce%20generalizable%20deepfake%0Adetectors.%20We%20demonstrate%20that%20with%20better-tuned%20hyperparameters%2C%20this%20simple%0Aapproach%20actually%20yields%20much%20higher%20performance%20--%2081%25%20accuracy%20on%0ADeepfake-Eval-2024%20--%20surpassing%20the%20previously%20reported%20accuracy%20of%20this%0Abaseline%20approach%20by%2018%25%20and%20competing%20with%20commercial%20deepfake%20detectors.%20We%0Adiscuss%20tradeoffs%20in%20accuracy%2C%20computational%20costs%2C%20and%20interpretability%2C%0Afocusing%20on%20how%20practical%20these%20deepfake%20detectors%20might%20be%20when%20deployed%20in%0Areal-world%20settings.%20Our%20code%20can%20be%20found%20at%0Ahttps%3A//github.com/Deepfake-Detection-KKO/deepfake-detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04150v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Simple%2520Baselines%2520for%2520In-The-Wild%2520Deepfake%2520Detection%26entry.906535625%3DOrlando%2520Castaneda%2520and%2520Kevin%2520So-Tang%2520and%2520Kshitij%2520Gurung%26entry.1292438233%3D%2520%2520The%2520widespread%2520adoption%2520of%2520synthetic%2520media%2520demands%2520accessible%2520deepfake%250Adetectors%2520and%2520realistic%2520benchmarks.%2520While%2520most%2520existing%2520research%2520evaluates%250Adeepfake%2520detectors%2520on%2520highly%2520controlled%2520datasets%252C%2520we%2520focus%2520on%2520the%2520recently%250Areleased%2520%2522in-the-wild%2522%2520benchmark%252C%2520Deepfake-Eval-2024.%2520Initial%2520reporting%2520on%250ADeepfake-Eval-2024%2520showed%2520that%2520three%2520finetuned%2520open-source%2520models%2520achieve%250Aaccuracies%2520between%252061%2525%2520and%252069%2525%252C%2520significantly%2520lagging%2520behind%2520the%2520leading%250Acommercial%2520deepfake%2520detector%2520with%252082%2525%2520accuracy.%2520Our%2520work%2520revisits%2520one%2520of%2520these%250Abaseline%2520approaches%252C%2520originally%2520introduced%2520by%2520Ojha%2520et%2520al.%252C%2520which%2520adapts%250Astandard%2520pretrained%2520vision%2520backbones%2520to%2520produce%2520generalizable%2520deepfake%250Adetectors.%2520We%2520demonstrate%2520that%2520with%2520better-tuned%2520hyperparameters%252C%2520this%2520simple%250Aapproach%2520actually%2520yields%2520much%2520higher%2520performance%2520--%252081%2525%2520accuracy%2520on%250ADeepfake-Eval-2024%2520--%2520surpassing%2520the%2520previously%2520reported%2520accuracy%2520of%2520this%250Abaseline%2520approach%2520by%252018%2525%2520and%2520competing%2520with%2520commercial%2520deepfake%2520detectors.%2520We%250Adiscuss%2520tradeoffs%2520in%2520accuracy%252C%2520computational%2520costs%252C%2520and%2520interpretability%252C%250Afocusing%2520on%2520how%2520practical%2520these%2520deepfake%2520detectors%2520might%2520be%2520when%2520deployed%2520in%250Areal-world%2520settings.%2520Our%2520code%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/Deepfake-Detection-KKO/deepfake-detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04150v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Simple%20Baselines%20for%20In-The-Wild%20Deepfake%20Detection&entry.906535625=Orlando%20Castaneda%20and%20Kevin%20So-Tang%20and%20Kshitij%20Gurung&entry.1292438233=%20%20The%20widespread%20adoption%20of%20synthetic%20media%20demands%20accessible%20deepfake%0Adetectors%20and%20realistic%20benchmarks.%20While%20most%20existing%20research%20evaluates%0Adeepfake%20detectors%20on%20highly%20controlled%20datasets%2C%20we%20focus%20on%20the%20recently%0Areleased%20%22in-the-wild%22%20benchmark%2C%20Deepfake-Eval-2024.%20Initial%20reporting%20on%0ADeepfake-Eval-2024%20showed%20that%20three%20finetuned%20open-source%20models%20achieve%0Aaccuracies%20between%2061%25%20and%2069%25%2C%20significantly%20lagging%20behind%20the%20leading%0Acommercial%20deepfake%20detector%20with%2082%25%20accuracy.%20Our%20work%20revisits%20one%20of%20these%0Abaseline%20approaches%2C%20originally%20introduced%20by%20Ojha%20et%20al.%2C%20which%20adapts%0Astandard%20pretrained%20vision%20backbones%20to%20produce%20generalizable%20deepfake%0Adetectors.%20We%20demonstrate%20that%20with%20better-tuned%20hyperparameters%2C%20this%20simple%0Aapproach%20actually%20yields%20much%20higher%20performance%20--%2081%25%20accuracy%20on%0ADeepfake-Eval-2024%20--%20surpassing%20the%20previously%20reported%20accuracy%20of%20this%0Abaseline%20approach%20by%2018%25%20and%20competing%20with%20commercial%20deepfake%20detectors.%20We%0Adiscuss%20tradeoffs%20in%20accuracy%2C%20computational%20costs%2C%20and%20interpretability%2C%0Afocusing%20on%20how%20practical%20these%20deepfake%20detectors%20might%20be%20when%20deployed%20in%0Areal-world%20settings.%20Our%20code%20can%20be%20found%20at%0Ahttps%3A//github.com/Deepfake-Detection-KKO/deepfake-detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04150v1&entry.124074799=Read"},
{"title": "Rethinking Layer-wise Gaussian Noise Injection: Bridging Implicit\n  Objectives and Privacy Budget Allocation", "author": "Qifeng Tan and Shusen Yang and Xuebin Ren and Yikai Zhang", "abstract": "  Layer-wise Gaussian mechanisms (LGM) enhance flexibility in differentially\nprivate deep learning by injecting noise into partitioned gradient vectors.\nHowever, existing methods often rely on heuristic noise allocation strategies,\nlacking a rigorous understanding of their theoretical grounding in connecting\nnoise allocation to formal privacy-utility tradeoffs. In this paper, we present\na unified analytical framework that systematically connects layer-wise noise\ninjection strategies with their implicit optimization objectives and associated\nprivacy budget allocations. Our analysis reveals that several existing\napproaches optimize ill-posed objectives -- either ignoring inter-layer\nsignal-to-noise ratio (SNR) consistency or leading to inefficient use of the\nprivacy budget. In response, we propose a SNR-Consistent noise allocation\nstrategy that unifies both aspects, yielding a noise allocation scheme that\nachieves better signal preservation and more efficient privacy budget\nutilization. Extensive experiments in both centralized and federated learning\nsettings demonstrate that our method consistently outperforms existing\nallocation strategies, achieving better privacy-utility tradeoffs. Our\nframework not only offers diagnostic insights into prior methods but also\nprovides theoretical guidance for designing adaptive and effective noise\ninjection schemes in deep models.\n", "link": "http://arxiv.org/abs/2509.04232v1", "date": "2025-09-04", "relevancy": 2.0168, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.509}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.508}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Layer-wise%20Gaussian%20Noise%20Injection%3A%20Bridging%20Implicit%0A%20%20Objectives%20and%20Privacy%20Budget%20Allocation&body=Title%3A%20Rethinking%20Layer-wise%20Gaussian%20Noise%20Injection%3A%20Bridging%20Implicit%0A%20%20Objectives%20and%20Privacy%20Budget%20Allocation%0AAuthor%3A%20Qifeng%20Tan%20and%20Shusen%20Yang%20and%20Xuebin%20Ren%20and%20Yikai%20Zhang%0AAbstract%3A%20%20%20Layer-wise%20Gaussian%20mechanisms%20%28LGM%29%20enhance%20flexibility%20in%20differentially%0Aprivate%20deep%20learning%20by%20injecting%20noise%20into%20partitioned%20gradient%20vectors.%0AHowever%2C%20existing%20methods%20often%20rely%20on%20heuristic%20noise%20allocation%20strategies%2C%0Alacking%20a%20rigorous%20understanding%20of%20their%20theoretical%20grounding%20in%20connecting%0Anoise%20allocation%20to%20formal%20privacy-utility%20tradeoffs.%20In%20this%20paper%2C%20we%20present%0Aa%20unified%20analytical%20framework%20that%20systematically%20connects%20layer-wise%20noise%0Ainjection%20strategies%20with%20their%20implicit%20optimization%20objectives%20and%20associated%0Aprivacy%20budget%20allocations.%20Our%20analysis%20reveals%20that%20several%20existing%0Aapproaches%20optimize%20ill-posed%20objectives%20--%20either%20ignoring%20inter-layer%0Asignal-to-noise%20ratio%20%28SNR%29%20consistency%20or%20leading%20to%20inefficient%20use%20of%20the%0Aprivacy%20budget.%20In%20response%2C%20we%20propose%20a%20SNR-Consistent%20noise%20allocation%0Astrategy%20that%20unifies%20both%20aspects%2C%20yielding%20a%20noise%20allocation%20scheme%20that%0Aachieves%20better%20signal%20preservation%20and%20more%20efficient%20privacy%20budget%0Autilization.%20Extensive%20experiments%20in%20both%20centralized%20and%20federated%20learning%0Asettings%20demonstrate%20that%20our%20method%20consistently%20outperforms%20existing%0Aallocation%20strategies%2C%20achieving%20better%20privacy-utility%20tradeoffs.%20Our%0Aframework%20not%20only%20offers%20diagnostic%20insights%20into%20prior%20methods%20but%20also%0Aprovides%20theoretical%20guidance%20for%20designing%20adaptive%20and%20effective%20noise%0Ainjection%20schemes%20in%20deep%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04232v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Layer-wise%2520Gaussian%2520Noise%2520Injection%253A%2520Bridging%2520Implicit%250A%2520%2520Objectives%2520and%2520Privacy%2520Budget%2520Allocation%26entry.906535625%3DQifeng%2520Tan%2520and%2520Shusen%2520Yang%2520and%2520Xuebin%2520Ren%2520and%2520Yikai%2520Zhang%26entry.1292438233%3D%2520%2520Layer-wise%2520Gaussian%2520mechanisms%2520%2528LGM%2529%2520enhance%2520flexibility%2520in%2520differentially%250Aprivate%2520deep%2520learning%2520by%2520injecting%2520noise%2520into%2520partitioned%2520gradient%2520vectors.%250AHowever%252C%2520existing%2520methods%2520often%2520rely%2520on%2520heuristic%2520noise%2520allocation%2520strategies%252C%250Alacking%2520a%2520rigorous%2520understanding%2520of%2520their%2520theoretical%2520grounding%2520in%2520connecting%250Anoise%2520allocation%2520to%2520formal%2520privacy-utility%2520tradeoffs.%2520In%2520this%2520paper%252C%2520we%2520present%250Aa%2520unified%2520analytical%2520framework%2520that%2520systematically%2520connects%2520layer-wise%2520noise%250Ainjection%2520strategies%2520with%2520their%2520implicit%2520optimization%2520objectives%2520and%2520associated%250Aprivacy%2520budget%2520allocations.%2520Our%2520analysis%2520reveals%2520that%2520several%2520existing%250Aapproaches%2520optimize%2520ill-posed%2520objectives%2520--%2520either%2520ignoring%2520inter-layer%250Asignal-to-noise%2520ratio%2520%2528SNR%2529%2520consistency%2520or%2520leading%2520to%2520inefficient%2520use%2520of%2520the%250Aprivacy%2520budget.%2520In%2520response%252C%2520we%2520propose%2520a%2520SNR-Consistent%2520noise%2520allocation%250Astrategy%2520that%2520unifies%2520both%2520aspects%252C%2520yielding%2520a%2520noise%2520allocation%2520scheme%2520that%250Aachieves%2520better%2520signal%2520preservation%2520and%2520more%2520efficient%2520privacy%2520budget%250Autilization.%2520Extensive%2520experiments%2520in%2520both%2520centralized%2520and%2520federated%2520learning%250Asettings%2520demonstrate%2520that%2520our%2520method%2520consistently%2520outperforms%2520existing%250Aallocation%2520strategies%252C%2520achieving%2520better%2520privacy-utility%2520tradeoffs.%2520Our%250Aframework%2520not%2520only%2520offers%2520diagnostic%2520insights%2520into%2520prior%2520methods%2520but%2520also%250Aprovides%2520theoretical%2520guidance%2520for%2520designing%2520adaptive%2520and%2520effective%2520noise%250Ainjection%2520schemes%2520in%2520deep%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04232v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Layer-wise%20Gaussian%20Noise%20Injection%3A%20Bridging%20Implicit%0A%20%20Objectives%20and%20Privacy%20Budget%20Allocation&entry.906535625=Qifeng%20Tan%20and%20Shusen%20Yang%20and%20Xuebin%20Ren%20and%20Yikai%20Zhang&entry.1292438233=%20%20Layer-wise%20Gaussian%20mechanisms%20%28LGM%29%20enhance%20flexibility%20in%20differentially%0Aprivate%20deep%20learning%20by%20injecting%20noise%20into%20partitioned%20gradient%20vectors.%0AHowever%2C%20existing%20methods%20often%20rely%20on%20heuristic%20noise%20allocation%20strategies%2C%0Alacking%20a%20rigorous%20understanding%20of%20their%20theoretical%20grounding%20in%20connecting%0Anoise%20allocation%20to%20formal%20privacy-utility%20tradeoffs.%20In%20this%20paper%2C%20we%20present%0Aa%20unified%20analytical%20framework%20that%20systematically%20connects%20layer-wise%20noise%0Ainjection%20strategies%20with%20their%20implicit%20optimization%20objectives%20and%20associated%0Aprivacy%20budget%20allocations.%20Our%20analysis%20reveals%20that%20several%20existing%0Aapproaches%20optimize%20ill-posed%20objectives%20--%20either%20ignoring%20inter-layer%0Asignal-to-noise%20ratio%20%28SNR%29%20consistency%20or%20leading%20to%20inefficient%20use%20of%20the%0Aprivacy%20budget.%20In%20response%2C%20we%20propose%20a%20SNR-Consistent%20noise%20allocation%0Astrategy%20that%20unifies%20both%20aspects%2C%20yielding%20a%20noise%20allocation%20scheme%20that%0Aachieves%20better%20signal%20preservation%20and%20more%20efficient%20privacy%20budget%0Autilization.%20Extensive%20experiments%20in%20both%20centralized%20and%20federated%20learning%0Asettings%20demonstrate%20that%20our%20method%20consistently%20outperforms%20existing%0Aallocation%20strategies%2C%20achieving%20better%20privacy-utility%20tradeoffs.%20Our%0Aframework%20not%20only%20offers%20diagnostic%20insights%20into%20prior%20methods%20but%20also%0Aprovides%20theoretical%20guidance%20for%20designing%20adaptive%20and%20effective%20noise%0Ainjection%20schemes%20in%20deep%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04232v1&entry.124074799=Read"},
{"title": "Sailing Towards Zero-Shot State Estimation using Foundation Models\n  Combined with a UKF", "author": "Tobin Holtmann and David Stenger and Andres Posada-Moreno and Friedrich Solowjow and Sebastian Trimpe", "abstract": "  State estimation in control and systems engineering traditionally requires\nextensive manual system identification or data-collection effort. However,\ntransformer-based foundation models in other domains have reduced data\nrequirements by leveraging pre-trained generalist models. Ultimately,\ndeveloping zero-shot foundation models of system dynamics could drastically\nreduce manual deployment effort. While recent work shows that transformer-based\nend-to-end approaches can achieve zero-shot performance on unseen systems, they\nare limited to sensor models seen during training. We introduce the foundation\nmodel unscented Kalman filter (FM-UKF), which combines a transformer-based\nmodel of system dynamics with analytically known sensor models via an UKF,\nenabling generalization across varying dynamics without retraining for new\nsensor configurations. We evaluate FM-UKF on a new benchmark of container ship\nmodels with complex dynamics, demonstrating a competitive accuracy, effort, and\nrobustness trade-off compared to classical methods with approximate system\nknowledge and to an end-to-end approach. The benchmark and dataset are open\nsourced to further support future research in zero-shot state estimation via\nfoundation models.\n", "link": "http://arxiv.org/abs/2509.04213v1", "date": "2025-09-04", "relevancy": 1.9941, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5013}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5013}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sailing%20Towards%20Zero-Shot%20State%20Estimation%20using%20Foundation%20Models%0A%20%20Combined%20with%20a%20UKF&body=Title%3A%20Sailing%20Towards%20Zero-Shot%20State%20Estimation%20using%20Foundation%20Models%0A%20%20Combined%20with%20a%20UKF%0AAuthor%3A%20Tobin%20Holtmann%20and%20David%20Stenger%20and%20Andres%20Posada-Moreno%20and%20Friedrich%20Solowjow%20and%20Sebastian%20Trimpe%0AAbstract%3A%20%20%20State%20estimation%20in%20control%20and%20systems%20engineering%20traditionally%20requires%0Aextensive%20manual%20system%20identification%20or%20data-collection%20effort.%20However%2C%0Atransformer-based%20foundation%20models%20in%20other%20domains%20have%20reduced%20data%0Arequirements%20by%20leveraging%20pre-trained%20generalist%20models.%20Ultimately%2C%0Adeveloping%20zero-shot%20foundation%20models%20of%20system%20dynamics%20could%20drastically%0Areduce%20manual%20deployment%20effort.%20While%20recent%20work%20shows%20that%20transformer-based%0Aend-to-end%20approaches%20can%20achieve%20zero-shot%20performance%20on%20unseen%20systems%2C%20they%0Aare%20limited%20to%20sensor%20models%20seen%20during%20training.%20We%20introduce%20the%20foundation%0Amodel%20unscented%20Kalman%20filter%20%28FM-UKF%29%2C%20which%20combines%20a%20transformer-based%0Amodel%20of%20system%20dynamics%20with%20analytically%20known%20sensor%20models%20via%20an%20UKF%2C%0Aenabling%20generalization%20across%20varying%20dynamics%20without%20retraining%20for%20new%0Asensor%20configurations.%20We%20evaluate%20FM-UKF%20on%20a%20new%20benchmark%20of%20container%20ship%0Amodels%20with%20complex%20dynamics%2C%20demonstrating%20a%20competitive%20accuracy%2C%20effort%2C%20and%0Arobustness%20trade-off%20compared%20to%20classical%20methods%20with%20approximate%20system%0Aknowledge%20and%20to%20an%20end-to-end%20approach.%20The%20benchmark%20and%20dataset%20are%20open%0Asourced%20to%20further%20support%20future%20research%20in%20zero-shot%20state%20estimation%20via%0Afoundation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04213v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSailing%2520Towards%2520Zero-Shot%2520State%2520Estimation%2520using%2520Foundation%2520Models%250A%2520%2520Combined%2520with%2520a%2520UKF%26entry.906535625%3DTobin%2520Holtmann%2520and%2520David%2520Stenger%2520and%2520Andres%2520Posada-Moreno%2520and%2520Friedrich%2520Solowjow%2520and%2520Sebastian%2520Trimpe%26entry.1292438233%3D%2520%2520State%2520estimation%2520in%2520control%2520and%2520systems%2520engineering%2520traditionally%2520requires%250Aextensive%2520manual%2520system%2520identification%2520or%2520data-collection%2520effort.%2520However%252C%250Atransformer-based%2520foundation%2520models%2520in%2520other%2520domains%2520have%2520reduced%2520data%250Arequirements%2520by%2520leveraging%2520pre-trained%2520generalist%2520models.%2520Ultimately%252C%250Adeveloping%2520zero-shot%2520foundation%2520models%2520of%2520system%2520dynamics%2520could%2520drastically%250Areduce%2520manual%2520deployment%2520effort.%2520While%2520recent%2520work%2520shows%2520that%2520transformer-based%250Aend-to-end%2520approaches%2520can%2520achieve%2520zero-shot%2520performance%2520on%2520unseen%2520systems%252C%2520they%250Aare%2520limited%2520to%2520sensor%2520models%2520seen%2520during%2520training.%2520We%2520introduce%2520the%2520foundation%250Amodel%2520unscented%2520Kalman%2520filter%2520%2528FM-UKF%2529%252C%2520which%2520combines%2520a%2520transformer-based%250Amodel%2520of%2520system%2520dynamics%2520with%2520analytically%2520known%2520sensor%2520models%2520via%2520an%2520UKF%252C%250Aenabling%2520generalization%2520across%2520varying%2520dynamics%2520without%2520retraining%2520for%2520new%250Asensor%2520configurations.%2520We%2520evaluate%2520FM-UKF%2520on%2520a%2520new%2520benchmark%2520of%2520container%2520ship%250Amodels%2520with%2520complex%2520dynamics%252C%2520demonstrating%2520a%2520competitive%2520accuracy%252C%2520effort%252C%2520and%250Arobustness%2520trade-off%2520compared%2520to%2520classical%2520methods%2520with%2520approximate%2520system%250Aknowledge%2520and%2520to%2520an%2520end-to-end%2520approach.%2520The%2520benchmark%2520and%2520dataset%2520are%2520open%250Asourced%2520to%2520further%2520support%2520future%2520research%2520in%2520zero-shot%2520state%2520estimation%2520via%250Afoundation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04213v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sailing%20Towards%20Zero-Shot%20State%20Estimation%20using%20Foundation%20Models%0A%20%20Combined%20with%20a%20UKF&entry.906535625=Tobin%20Holtmann%20and%20David%20Stenger%20and%20Andres%20Posada-Moreno%20and%20Friedrich%20Solowjow%20and%20Sebastian%20Trimpe&entry.1292438233=%20%20State%20estimation%20in%20control%20and%20systems%20engineering%20traditionally%20requires%0Aextensive%20manual%20system%20identification%20or%20data-collection%20effort.%20However%2C%0Atransformer-based%20foundation%20models%20in%20other%20domains%20have%20reduced%20data%0Arequirements%20by%20leveraging%20pre-trained%20generalist%20models.%20Ultimately%2C%0Adeveloping%20zero-shot%20foundation%20models%20of%20system%20dynamics%20could%20drastically%0Areduce%20manual%20deployment%20effort.%20While%20recent%20work%20shows%20that%20transformer-based%0Aend-to-end%20approaches%20can%20achieve%20zero-shot%20performance%20on%20unseen%20systems%2C%20they%0Aare%20limited%20to%20sensor%20models%20seen%20during%20training.%20We%20introduce%20the%20foundation%0Amodel%20unscented%20Kalman%20filter%20%28FM-UKF%29%2C%20which%20combines%20a%20transformer-based%0Amodel%20of%20system%20dynamics%20with%20analytically%20known%20sensor%20models%20via%20an%20UKF%2C%0Aenabling%20generalization%20across%20varying%20dynamics%20without%20retraining%20for%20new%0Asensor%20configurations.%20We%20evaluate%20FM-UKF%20on%20a%20new%20benchmark%20of%20container%20ship%0Amodels%20with%20complex%20dynamics%2C%20demonstrating%20a%20competitive%20accuracy%2C%20effort%2C%20and%0Arobustness%20trade-off%20compared%20to%20classical%20methods%20with%20approximate%20system%0Aknowledge%20and%20to%20an%20end-to-end%20approach.%20The%20benchmark%20and%20dataset%20are%20open%0Asourced%20to%20further%20support%20future%20research%20in%20zero-shot%20state%20estimation%20via%0Afoundation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04213v1&entry.124074799=Read"},
{"title": "MAGneT: Coordinated Multi-Agent Generation of Synthetic Multi-Turn\n  Mental Health Counseling Sessions", "author": "Aishik Mandal and Tanmoy Chakraborty and Iryna Gurevych", "abstract": "  The growing demand for scalable psychological counseling highlights the need\nfor fine-tuning open-source Large Language Models (LLMs) with high-quality,\nprivacy-compliant data, yet such data remains scarce. Here we introduce MAGneT,\na novel multi-agent framework for synthetic psychological counseling session\ngeneration that decomposes counselor response generation into coordinated\nsub-tasks handled by specialized LLM agents, each modeling a key psychological\ntechnique. Unlike prior single-agent approaches, MAGneT better captures the\nstructure and nuance of real counseling. In addition, we address\ninconsistencies in prior evaluation protocols by proposing a unified evaluation\nframework integrating diverse automatic and expert metrics. Furthermore, we\nexpand the expert evaluations from four aspects of counseling in previous works\nto nine aspects, enabling a more thorough and robust assessment of data\nquality. Empirical results show that MAGneT significantly outperforms existing\nmethods in quality, diversity, and therapeutic alignment of the generated\ncounseling sessions, improving general counseling skills by 3.2% and\nCBT-specific skills by 4.3% on average on cognitive therapy rating scale\n(CTRS). Crucially, experts prefer MAGneT-generated sessions in 77.2% of cases\non average across all aspects. Moreover, fine-tuning an open-source model on\nMAGneT-generated sessions shows better performance, with improvements of 6.3%\non general counseling skills and 7.3% on CBT-specific skills on average on CTRS\nover those fine-tuned with sessions generated by baseline methods. We also make\nour code and data public.\n", "link": "http://arxiv.org/abs/2509.04183v1", "date": "2025-09-04", "relevancy": 1.9933, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5134}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5044}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAGneT%3A%20Coordinated%20Multi-Agent%20Generation%20of%20Synthetic%20Multi-Turn%0A%20%20Mental%20Health%20Counseling%20Sessions&body=Title%3A%20MAGneT%3A%20Coordinated%20Multi-Agent%20Generation%20of%20Synthetic%20Multi-Turn%0A%20%20Mental%20Health%20Counseling%20Sessions%0AAuthor%3A%20Aishik%20Mandal%20and%20Tanmoy%20Chakraborty%20and%20Iryna%20Gurevych%0AAbstract%3A%20%20%20The%20growing%20demand%20for%20scalable%20psychological%20counseling%20highlights%20the%20need%0Afor%20fine-tuning%20open-source%20Large%20Language%20Models%20%28LLMs%29%20with%20high-quality%2C%0Aprivacy-compliant%20data%2C%20yet%20such%20data%20remains%20scarce.%20Here%20we%20introduce%20MAGneT%2C%0Aa%20novel%20multi-agent%20framework%20for%20synthetic%20psychological%20counseling%20session%0Ageneration%20that%20decomposes%20counselor%20response%20generation%20into%20coordinated%0Asub-tasks%20handled%20by%20specialized%20LLM%20agents%2C%20each%20modeling%20a%20key%20psychological%0Atechnique.%20Unlike%20prior%20single-agent%20approaches%2C%20MAGneT%20better%20captures%20the%0Astructure%20and%20nuance%20of%20real%20counseling.%20In%20addition%2C%20we%20address%0Ainconsistencies%20in%20prior%20evaluation%20protocols%20by%20proposing%20a%20unified%20evaluation%0Aframework%20integrating%20diverse%20automatic%20and%20expert%20metrics.%20Furthermore%2C%20we%0Aexpand%20the%20expert%20evaluations%20from%20four%20aspects%20of%20counseling%20in%20previous%20works%0Ato%20nine%20aspects%2C%20enabling%20a%20more%20thorough%20and%20robust%20assessment%20of%20data%0Aquality.%20Empirical%20results%20show%20that%20MAGneT%20significantly%20outperforms%20existing%0Amethods%20in%20quality%2C%20diversity%2C%20and%20therapeutic%20alignment%20of%20the%20generated%0Acounseling%20sessions%2C%20improving%20general%20counseling%20skills%20by%203.2%25%20and%0ACBT-specific%20skills%20by%204.3%25%20on%20average%20on%20cognitive%20therapy%20rating%20scale%0A%28CTRS%29.%20Crucially%2C%20experts%20prefer%20MAGneT-generated%20sessions%20in%2077.2%25%20of%20cases%0Aon%20average%20across%20all%20aspects.%20Moreover%2C%20fine-tuning%20an%20open-source%20model%20on%0AMAGneT-generated%20sessions%20shows%20better%20performance%2C%20with%20improvements%20of%206.3%25%0Aon%20general%20counseling%20skills%20and%207.3%25%20on%20CBT-specific%20skills%20on%20average%20on%20CTRS%0Aover%20those%20fine-tuned%20with%20sessions%20generated%20by%20baseline%20methods.%20We%20also%20make%0Aour%20code%20and%20data%20public.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04183v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAGneT%253A%2520Coordinated%2520Multi-Agent%2520Generation%2520of%2520Synthetic%2520Multi-Turn%250A%2520%2520Mental%2520Health%2520Counseling%2520Sessions%26entry.906535625%3DAishik%2520Mandal%2520and%2520Tanmoy%2520Chakraborty%2520and%2520Iryna%2520Gurevych%26entry.1292438233%3D%2520%2520The%2520growing%2520demand%2520for%2520scalable%2520psychological%2520counseling%2520highlights%2520the%2520need%250Afor%2520fine-tuning%2520open-source%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520high-quality%252C%250Aprivacy-compliant%2520data%252C%2520yet%2520such%2520data%2520remains%2520scarce.%2520Here%2520we%2520introduce%2520MAGneT%252C%250Aa%2520novel%2520multi-agent%2520framework%2520for%2520synthetic%2520psychological%2520counseling%2520session%250Ageneration%2520that%2520decomposes%2520counselor%2520response%2520generation%2520into%2520coordinated%250Asub-tasks%2520handled%2520by%2520specialized%2520LLM%2520agents%252C%2520each%2520modeling%2520a%2520key%2520psychological%250Atechnique.%2520Unlike%2520prior%2520single-agent%2520approaches%252C%2520MAGneT%2520better%2520captures%2520the%250Astructure%2520and%2520nuance%2520of%2520real%2520counseling.%2520In%2520addition%252C%2520we%2520address%250Ainconsistencies%2520in%2520prior%2520evaluation%2520protocols%2520by%2520proposing%2520a%2520unified%2520evaluation%250Aframework%2520integrating%2520diverse%2520automatic%2520and%2520expert%2520metrics.%2520Furthermore%252C%2520we%250Aexpand%2520the%2520expert%2520evaluations%2520from%2520four%2520aspects%2520of%2520counseling%2520in%2520previous%2520works%250Ato%2520nine%2520aspects%252C%2520enabling%2520a%2520more%2520thorough%2520and%2520robust%2520assessment%2520of%2520data%250Aquality.%2520Empirical%2520results%2520show%2520that%2520MAGneT%2520significantly%2520outperforms%2520existing%250Amethods%2520in%2520quality%252C%2520diversity%252C%2520and%2520therapeutic%2520alignment%2520of%2520the%2520generated%250Acounseling%2520sessions%252C%2520improving%2520general%2520counseling%2520skills%2520by%25203.2%2525%2520and%250ACBT-specific%2520skills%2520by%25204.3%2525%2520on%2520average%2520on%2520cognitive%2520therapy%2520rating%2520scale%250A%2528CTRS%2529.%2520Crucially%252C%2520experts%2520prefer%2520MAGneT-generated%2520sessions%2520in%252077.2%2525%2520of%2520cases%250Aon%2520average%2520across%2520all%2520aspects.%2520Moreover%252C%2520fine-tuning%2520an%2520open-source%2520model%2520on%250AMAGneT-generated%2520sessions%2520shows%2520better%2520performance%252C%2520with%2520improvements%2520of%25206.3%2525%250Aon%2520general%2520counseling%2520skills%2520and%25207.3%2525%2520on%2520CBT-specific%2520skills%2520on%2520average%2520on%2520CTRS%250Aover%2520those%2520fine-tuned%2520with%2520sessions%2520generated%2520by%2520baseline%2520methods.%2520We%2520also%2520make%250Aour%2520code%2520and%2520data%2520public.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04183v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAGneT%3A%20Coordinated%20Multi-Agent%20Generation%20of%20Synthetic%20Multi-Turn%0A%20%20Mental%20Health%20Counseling%20Sessions&entry.906535625=Aishik%20Mandal%20and%20Tanmoy%20Chakraborty%20and%20Iryna%20Gurevych&entry.1292438233=%20%20The%20growing%20demand%20for%20scalable%20psychological%20counseling%20highlights%20the%20need%0Afor%20fine-tuning%20open-source%20Large%20Language%20Models%20%28LLMs%29%20with%20high-quality%2C%0Aprivacy-compliant%20data%2C%20yet%20such%20data%20remains%20scarce.%20Here%20we%20introduce%20MAGneT%2C%0Aa%20novel%20multi-agent%20framework%20for%20synthetic%20psychological%20counseling%20session%0Ageneration%20that%20decomposes%20counselor%20response%20generation%20into%20coordinated%0Asub-tasks%20handled%20by%20specialized%20LLM%20agents%2C%20each%20modeling%20a%20key%20psychological%0Atechnique.%20Unlike%20prior%20single-agent%20approaches%2C%20MAGneT%20better%20captures%20the%0Astructure%20and%20nuance%20of%20real%20counseling.%20In%20addition%2C%20we%20address%0Ainconsistencies%20in%20prior%20evaluation%20protocols%20by%20proposing%20a%20unified%20evaluation%0Aframework%20integrating%20diverse%20automatic%20and%20expert%20metrics.%20Furthermore%2C%20we%0Aexpand%20the%20expert%20evaluations%20from%20four%20aspects%20of%20counseling%20in%20previous%20works%0Ato%20nine%20aspects%2C%20enabling%20a%20more%20thorough%20and%20robust%20assessment%20of%20data%0Aquality.%20Empirical%20results%20show%20that%20MAGneT%20significantly%20outperforms%20existing%0Amethods%20in%20quality%2C%20diversity%2C%20and%20therapeutic%20alignment%20of%20the%20generated%0Acounseling%20sessions%2C%20improving%20general%20counseling%20skills%20by%203.2%25%20and%0ACBT-specific%20skills%20by%204.3%25%20on%20average%20on%20cognitive%20therapy%20rating%20scale%0A%28CTRS%29.%20Crucially%2C%20experts%20prefer%20MAGneT-generated%20sessions%20in%2077.2%25%20of%20cases%0Aon%20average%20across%20all%20aspects.%20Moreover%2C%20fine-tuning%20an%20open-source%20model%20on%0AMAGneT-generated%20sessions%20shows%20better%20performance%2C%20with%20improvements%20of%206.3%25%0Aon%20general%20counseling%20skills%20and%207.3%25%20on%20CBT-specific%20skills%20on%20average%20on%20CTRS%0Aover%20those%20fine-tuned%20with%20sessions%20generated%20by%20baseline%20methods.%20We%20also%20make%0Aour%20code%20and%20data%20public.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04183v1&entry.124074799=Read"},
{"title": "Parking Availability Prediction via Fusing Multi-Source Data with A\n  Self-Supervised Learning Enhanced Spatio-Temporal Inverted Transformer", "author": "Yin Huang and Yongqi Dong and Youhua Tang and Li Li", "abstract": "  The rapid growth of private car ownership has worsened the urban parking\npredicament, underscoring the need for accurate and effective parking\navailability prediction to support urban planning and management. To address\nkey limitations in modeling spatio-temporal dependencies and exploiting\nmulti-source data for parking availability prediction, this study proposes a\nnovel approach with SST-iTransformer. The methodology leverages K-means\nclustering to establish parking cluster zones (PCZs), extracting and\nintegrating traffic demand characteristics from various transportation modes\n(i.e., metro, bus, online ride-hailing, and taxi) associated with the targeted\nparking lots. Upgraded on vanilla iTransformer, SST-iTransformer integrates\nmasking-reconstruction-based pretext tasks for self-supervised spatio-temporal\nrepresentation learning, and features an innovative dual-branch attention\nmechanism: Series Attention captures long-term temporal dependencies via\npatching operations, while Channel Attention models cross-variate interactions\nthrough inverted dimensions. Extensive experiments using real-world data from\nChengdu, China, demonstrate that SST-iTransformer outperforms baseline deep\nlearning models (including Informer, Autoformer, Crossformer, and\niTransformer), achieving state-of-the-art performance with the lowest mean\nsquared error (MSE) and competitive mean absolute error (MAE). Comprehensive\nablation studies quantitatively reveal the relative importance of different\ndata sources: incorporating ride-hailing data provides the largest performance\ngains, followed by taxi, whereas fixed-route transit features (bus/metro)\ncontribute marginally. Spatial correlation analysis further confirms that\nexcluding historical data from correlated parking lots within PCZs leads to\nsubstantial performance degradation, underscoring the importance of modeling\nspatial dependencies.\n", "link": "http://arxiv.org/abs/2509.04362v1", "date": "2025-09-04", "relevancy": 1.9925, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5138}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4954}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parking%20Availability%20Prediction%20via%20Fusing%20Multi-Source%20Data%20with%20A%0A%20%20Self-Supervised%20Learning%20Enhanced%20Spatio-Temporal%20Inverted%20Transformer&body=Title%3A%20Parking%20Availability%20Prediction%20via%20Fusing%20Multi-Source%20Data%20with%20A%0A%20%20Self-Supervised%20Learning%20Enhanced%20Spatio-Temporal%20Inverted%20Transformer%0AAuthor%3A%20Yin%20Huang%20and%20Yongqi%20Dong%20and%20Youhua%20Tang%20and%20Li%20Li%0AAbstract%3A%20%20%20The%20rapid%20growth%20of%20private%20car%20ownership%20has%20worsened%20the%20urban%20parking%0Apredicament%2C%20underscoring%20the%20need%20for%20accurate%20and%20effective%20parking%0Aavailability%20prediction%20to%20support%20urban%20planning%20and%20management.%20To%20address%0Akey%20limitations%20in%20modeling%20spatio-temporal%20dependencies%20and%20exploiting%0Amulti-source%20data%20for%20parking%20availability%20prediction%2C%20this%20study%20proposes%20a%0Anovel%20approach%20with%20SST-iTransformer.%20The%20methodology%20leverages%20K-means%0Aclustering%20to%20establish%20parking%20cluster%20zones%20%28PCZs%29%2C%20extracting%20and%0Aintegrating%20traffic%20demand%20characteristics%20from%20various%20transportation%20modes%0A%28i.e.%2C%20metro%2C%20bus%2C%20online%20ride-hailing%2C%20and%20taxi%29%20associated%20with%20the%20targeted%0Aparking%20lots.%20Upgraded%20on%20vanilla%20iTransformer%2C%20SST-iTransformer%20integrates%0Amasking-reconstruction-based%20pretext%20tasks%20for%20self-supervised%20spatio-temporal%0Arepresentation%20learning%2C%20and%20features%20an%20innovative%20dual-branch%20attention%0Amechanism%3A%20Series%20Attention%20captures%20long-term%20temporal%20dependencies%20via%0Apatching%20operations%2C%20while%20Channel%20Attention%20models%20cross-variate%20interactions%0Athrough%20inverted%20dimensions.%20Extensive%20experiments%20using%20real-world%20data%20from%0AChengdu%2C%20China%2C%20demonstrate%20that%20SST-iTransformer%20outperforms%20baseline%20deep%0Alearning%20models%20%28including%20Informer%2C%20Autoformer%2C%20Crossformer%2C%20and%0AiTransformer%29%2C%20achieving%20state-of-the-art%20performance%20with%20the%20lowest%20mean%0Asquared%20error%20%28MSE%29%20and%20competitive%20mean%20absolute%20error%20%28MAE%29.%20Comprehensive%0Aablation%20studies%20quantitatively%20reveal%20the%20relative%20importance%20of%20different%0Adata%20sources%3A%20incorporating%20ride-hailing%20data%20provides%20the%20largest%20performance%0Agains%2C%20followed%20by%20taxi%2C%20whereas%20fixed-route%20transit%20features%20%28bus/metro%29%0Acontribute%20marginally.%20Spatial%20correlation%20analysis%20further%20confirms%20that%0Aexcluding%20historical%20data%20from%20correlated%20parking%20lots%20within%20PCZs%20leads%20to%0Asubstantial%20performance%20degradation%2C%20underscoring%20the%20importance%20of%20modeling%0Aspatial%20dependencies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04362v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParking%2520Availability%2520Prediction%2520via%2520Fusing%2520Multi-Source%2520Data%2520with%2520A%250A%2520%2520Self-Supervised%2520Learning%2520Enhanced%2520Spatio-Temporal%2520Inverted%2520Transformer%26entry.906535625%3DYin%2520Huang%2520and%2520Yongqi%2520Dong%2520and%2520Youhua%2520Tang%2520and%2520Li%2520Li%26entry.1292438233%3D%2520%2520The%2520rapid%2520growth%2520of%2520private%2520car%2520ownership%2520has%2520worsened%2520the%2520urban%2520parking%250Apredicament%252C%2520underscoring%2520the%2520need%2520for%2520accurate%2520and%2520effective%2520parking%250Aavailability%2520prediction%2520to%2520support%2520urban%2520planning%2520and%2520management.%2520To%2520address%250Akey%2520limitations%2520in%2520modeling%2520spatio-temporal%2520dependencies%2520and%2520exploiting%250Amulti-source%2520data%2520for%2520parking%2520availability%2520prediction%252C%2520this%2520study%2520proposes%2520a%250Anovel%2520approach%2520with%2520SST-iTransformer.%2520The%2520methodology%2520leverages%2520K-means%250Aclustering%2520to%2520establish%2520parking%2520cluster%2520zones%2520%2528PCZs%2529%252C%2520extracting%2520and%250Aintegrating%2520traffic%2520demand%2520characteristics%2520from%2520various%2520transportation%2520modes%250A%2528i.e.%252C%2520metro%252C%2520bus%252C%2520online%2520ride-hailing%252C%2520and%2520taxi%2529%2520associated%2520with%2520the%2520targeted%250Aparking%2520lots.%2520Upgraded%2520on%2520vanilla%2520iTransformer%252C%2520SST-iTransformer%2520integrates%250Amasking-reconstruction-based%2520pretext%2520tasks%2520for%2520self-supervised%2520spatio-temporal%250Arepresentation%2520learning%252C%2520and%2520features%2520an%2520innovative%2520dual-branch%2520attention%250Amechanism%253A%2520Series%2520Attention%2520captures%2520long-term%2520temporal%2520dependencies%2520via%250Apatching%2520operations%252C%2520while%2520Channel%2520Attention%2520models%2520cross-variate%2520interactions%250Athrough%2520inverted%2520dimensions.%2520Extensive%2520experiments%2520using%2520real-world%2520data%2520from%250AChengdu%252C%2520China%252C%2520demonstrate%2520that%2520SST-iTransformer%2520outperforms%2520baseline%2520deep%250Alearning%2520models%2520%2528including%2520Informer%252C%2520Autoformer%252C%2520Crossformer%252C%2520and%250AiTransformer%2529%252C%2520achieving%2520state-of-the-art%2520performance%2520with%2520the%2520lowest%2520mean%250Asquared%2520error%2520%2528MSE%2529%2520and%2520competitive%2520mean%2520absolute%2520error%2520%2528MAE%2529.%2520Comprehensive%250Aablation%2520studies%2520quantitatively%2520reveal%2520the%2520relative%2520importance%2520of%2520different%250Adata%2520sources%253A%2520incorporating%2520ride-hailing%2520data%2520provides%2520the%2520largest%2520performance%250Agains%252C%2520followed%2520by%2520taxi%252C%2520whereas%2520fixed-route%2520transit%2520features%2520%2528bus/metro%2529%250Acontribute%2520marginally.%2520Spatial%2520correlation%2520analysis%2520further%2520confirms%2520that%250Aexcluding%2520historical%2520data%2520from%2520correlated%2520parking%2520lots%2520within%2520PCZs%2520leads%2520to%250Asubstantial%2520performance%2520degradation%252C%2520underscoring%2520the%2520importance%2520of%2520modeling%250Aspatial%2520dependencies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04362v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parking%20Availability%20Prediction%20via%20Fusing%20Multi-Source%20Data%20with%20A%0A%20%20Self-Supervised%20Learning%20Enhanced%20Spatio-Temporal%20Inverted%20Transformer&entry.906535625=Yin%20Huang%20and%20Yongqi%20Dong%20and%20Youhua%20Tang%20and%20Li%20Li&entry.1292438233=%20%20The%20rapid%20growth%20of%20private%20car%20ownership%20has%20worsened%20the%20urban%20parking%0Apredicament%2C%20underscoring%20the%20need%20for%20accurate%20and%20effective%20parking%0Aavailability%20prediction%20to%20support%20urban%20planning%20and%20management.%20To%20address%0Akey%20limitations%20in%20modeling%20spatio-temporal%20dependencies%20and%20exploiting%0Amulti-source%20data%20for%20parking%20availability%20prediction%2C%20this%20study%20proposes%20a%0Anovel%20approach%20with%20SST-iTransformer.%20The%20methodology%20leverages%20K-means%0Aclustering%20to%20establish%20parking%20cluster%20zones%20%28PCZs%29%2C%20extracting%20and%0Aintegrating%20traffic%20demand%20characteristics%20from%20various%20transportation%20modes%0A%28i.e.%2C%20metro%2C%20bus%2C%20online%20ride-hailing%2C%20and%20taxi%29%20associated%20with%20the%20targeted%0Aparking%20lots.%20Upgraded%20on%20vanilla%20iTransformer%2C%20SST-iTransformer%20integrates%0Amasking-reconstruction-based%20pretext%20tasks%20for%20self-supervised%20spatio-temporal%0Arepresentation%20learning%2C%20and%20features%20an%20innovative%20dual-branch%20attention%0Amechanism%3A%20Series%20Attention%20captures%20long-term%20temporal%20dependencies%20via%0Apatching%20operations%2C%20while%20Channel%20Attention%20models%20cross-variate%20interactions%0Athrough%20inverted%20dimensions.%20Extensive%20experiments%20using%20real-world%20data%20from%0AChengdu%2C%20China%2C%20demonstrate%20that%20SST-iTransformer%20outperforms%20baseline%20deep%0Alearning%20models%20%28including%20Informer%2C%20Autoformer%2C%20Crossformer%2C%20and%0AiTransformer%29%2C%20achieving%20state-of-the-art%20performance%20with%20the%20lowest%20mean%0Asquared%20error%20%28MSE%29%20and%20competitive%20mean%20absolute%20error%20%28MAE%29.%20Comprehensive%0Aablation%20studies%20quantitatively%20reveal%20the%20relative%20importance%20of%20different%0Adata%20sources%3A%20incorporating%20ride-hailing%20data%20provides%20the%20largest%20performance%0Agains%2C%20followed%20by%20taxi%2C%20whereas%20fixed-route%20transit%20features%20%28bus/metro%29%0Acontribute%20marginally.%20Spatial%20correlation%20analysis%20further%20confirms%20that%0Aexcluding%20historical%20data%20from%20correlated%20parking%20lots%20within%20PCZs%20leads%20to%0Asubstantial%20performance%20degradation%2C%20underscoring%20the%20importance%20of%20modeling%0Aspatial%20dependencies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04362v1&entry.124074799=Read"},
{"title": "Pilot Study on Generative AI and Critical Thinking in Higher Education\n  Classrooms", "author": "W. F. Lamberti and S. R. Lawrence and D. White and S. Kim and S. Abdullah", "abstract": "  Generative AI (GAI) tools have seen rapid adoption in educational settings,\nyet their role in fostering critical thinking remains underexplored. While\nprevious studies have examined GAI as a tutor for specific lessons or as a tool\nfor completing assignments, few have addressed how students critically evaluate\nthe accuracy and appropriateness of GAI-generated responses. This pilot study\ninvestigates students' ability to apply structured critical thinking when\nassessing Generative AI outputs in introductory Computational and Data Science\ncourses. Given that GAI tools often produce contextually flawed or factually\nincorrect answers, we designed learning activities that require students to\nanalyze, critique, and revise AI-generated solutions. Our findings offer\ninitial insights into students' ability to engage critically with GAI content\nand lay the groundwork for more comprehensive studies in future semesters.\n", "link": "http://arxiv.org/abs/2509.00167v2", "date": "2025-09-04", "relevancy": 1.986, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5228}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4814}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4685}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pilot%20Study%20on%20Generative%20AI%20and%20Critical%20Thinking%20in%20Higher%20Education%0A%20%20Classrooms&body=Title%3A%20Pilot%20Study%20on%20Generative%20AI%20and%20Critical%20Thinking%20in%20Higher%20Education%0A%20%20Classrooms%0AAuthor%3A%20W.%20F.%20Lamberti%20and%20S.%20R.%20Lawrence%20and%20D.%20White%20and%20S.%20Kim%20and%20S.%20Abdullah%0AAbstract%3A%20%20%20Generative%20AI%20%28GAI%29%20tools%20have%20seen%20rapid%20adoption%20in%20educational%20settings%2C%0Ayet%20their%20role%20in%20fostering%20critical%20thinking%20remains%20underexplored.%20While%0Aprevious%20studies%20have%20examined%20GAI%20as%20a%20tutor%20for%20specific%20lessons%20or%20as%20a%20tool%0Afor%20completing%20assignments%2C%20few%20have%20addressed%20how%20students%20critically%20evaluate%0Athe%20accuracy%20and%20appropriateness%20of%20GAI-generated%20responses.%20This%20pilot%20study%0Ainvestigates%20students%27%20ability%20to%20apply%20structured%20critical%20thinking%20when%0Aassessing%20Generative%20AI%20outputs%20in%20introductory%20Computational%20and%20Data%20Science%0Acourses.%20Given%20that%20GAI%20tools%20often%20produce%20contextually%20flawed%20or%20factually%0Aincorrect%20answers%2C%20we%20designed%20learning%20activities%20that%20require%20students%20to%0Aanalyze%2C%20critique%2C%20and%20revise%20AI-generated%20solutions.%20Our%20findings%20offer%0Ainitial%20insights%20into%20students%27%20ability%20to%20engage%20critically%20with%20GAI%20content%0Aand%20lay%20the%20groundwork%20for%20more%20comprehensive%20studies%20in%20future%20semesters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.00167v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPilot%2520Study%2520on%2520Generative%2520AI%2520and%2520Critical%2520Thinking%2520in%2520Higher%2520Education%250A%2520%2520Classrooms%26entry.906535625%3DW.%2520F.%2520Lamberti%2520and%2520S.%2520R.%2520Lawrence%2520and%2520D.%2520White%2520and%2520S.%2520Kim%2520and%2520S.%2520Abdullah%26entry.1292438233%3D%2520%2520Generative%2520AI%2520%2528GAI%2529%2520tools%2520have%2520seen%2520rapid%2520adoption%2520in%2520educational%2520settings%252C%250Ayet%2520their%2520role%2520in%2520fostering%2520critical%2520thinking%2520remains%2520underexplored.%2520While%250Aprevious%2520studies%2520have%2520examined%2520GAI%2520as%2520a%2520tutor%2520for%2520specific%2520lessons%2520or%2520as%2520a%2520tool%250Afor%2520completing%2520assignments%252C%2520few%2520have%2520addressed%2520how%2520students%2520critically%2520evaluate%250Athe%2520accuracy%2520and%2520appropriateness%2520of%2520GAI-generated%2520responses.%2520This%2520pilot%2520study%250Ainvestigates%2520students%2527%2520ability%2520to%2520apply%2520structured%2520critical%2520thinking%2520when%250Aassessing%2520Generative%2520AI%2520outputs%2520in%2520introductory%2520Computational%2520and%2520Data%2520Science%250Acourses.%2520Given%2520that%2520GAI%2520tools%2520often%2520produce%2520contextually%2520flawed%2520or%2520factually%250Aincorrect%2520answers%252C%2520we%2520designed%2520learning%2520activities%2520that%2520require%2520students%2520to%250Aanalyze%252C%2520critique%252C%2520and%2520revise%2520AI-generated%2520solutions.%2520Our%2520findings%2520offer%250Ainitial%2520insights%2520into%2520students%2527%2520ability%2520to%2520engage%2520critically%2520with%2520GAI%2520content%250Aand%2520lay%2520the%2520groundwork%2520for%2520more%2520comprehensive%2520studies%2520in%2520future%2520semesters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.00167v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pilot%20Study%20on%20Generative%20AI%20and%20Critical%20Thinking%20in%20Higher%20Education%0A%20%20Classrooms&entry.906535625=W.%20F.%20Lamberti%20and%20S.%20R.%20Lawrence%20and%20D.%20White%20and%20S.%20Kim%20and%20S.%20Abdullah&entry.1292438233=%20%20Generative%20AI%20%28GAI%29%20tools%20have%20seen%20rapid%20adoption%20in%20educational%20settings%2C%0Ayet%20their%20role%20in%20fostering%20critical%20thinking%20remains%20underexplored.%20While%0Aprevious%20studies%20have%20examined%20GAI%20as%20a%20tutor%20for%20specific%20lessons%20or%20as%20a%20tool%0Afor%20completing%20assignments%2C%20few%20have%20addressed%20how%20students%20critically%20evaluate%0Athe%20accuracy%20and%20appropriateness%20of%20GAI-generated%20responses.%20This%20pilot%20study%0Ainvestigates%20students%27%20ability%20to%20apply%20structured%20critical%20thinking%20when%0Aassessing%20Generative%20AI%20outputs%20in%20introductory%20Computational%20and%20Data%20Science%0Acourses.%20Given%20that%20GAI%20tools%20often%20produce%20contextually%20flawed%20or%20factually%0Aincorrect%20answers%2C%20we%20designed%20learning%20activities%20that%20require%20students%20to%0Aanalyze%2C%20critique%2C%20and%20revise%20AI-generated%20solutions.%20Our%20findings%20offer%0Ainitial%20insights%20into%20students%27%20ability%20to%20engage%20critically%20with%20GAI%20content%0Aand%20lay%20the%20groundwork%20for%20more%20comprehensive%20studies%20in%20future%20semesters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.00167v2&entry.124074799=Read"},
{"title": "Enhancing Technical Documents Retrieval for RAG", "author": "Songjiang Lai and Tsun-Hin Cheung and Ka-Chun Fung and Kaiwen Xue and Kwan-Ho Lin and Yan-Ming Choi and Vincent Ng and Kin-Man Lam", "abstract": "  In this paper, we introduce Technical-Embeddings, a novel framework designed\nto optimize semantic retrieval in technical documentation, with applications in\nboth hardware and software development. Our approach addresses the challenges\nof understanding and retrieving complex technical content by leveraging the\ncapabilities of Large Language Models (LLMs). First, we enhance user queries by\ngenerating expanded representations that better capture user intent and improve\ndataset diversity, thereby enriching the fine-tuning process for embedding\nmodels. Second, we apply summary extraction techniques to encode essential\ncontextual information, refining the representation of technical documents. To\nfurther enhance retrieval performance, we fine-tune a bi-encoder BERT model\nusing soft prompting, incorporating separate learning parameters for queries\nand document context to capture fine-grained semantic nuances. We evaluate our\napproach on two public datasets, RAG-EDA and Rust-Docs-QA, demonstrating that\nTechnical-Embeddings significantly outperforms baseline models in both\nprecision and recall. Our findings highlight the effectiveness of integrating\nquery expansion and contextual summarization to enhance information access and\ncomprehension in technical domains. This work advances the state of\nRetrieval-Augmented Generation (RAG) systems, offering new avenues for\nefficient and accurate technical document retrieval in engineering and product\ndevelopment workflows.\n", "link": "http://arxiv.org/abs/2509.04139v1", "date": "2025-09-04", "relevancy": 1.9858, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5069}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4944}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4944}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Technical%20Documents%20Retrieval%20for%20RAG&body=Title%3A%20Enhancing%20Technical%20Documents%20Retrieval%20for%20RAG%0AAuthor%3A%20Songjiang%20Lai%20and%20Tsun-Hin%20Cheung%20and%20Ka-Chun%20Fung%20and%20Kaiwen%20Xue%20and%20Kwan-Ho%20Lin%20and%20Yan-Ming%20Choi%20and%20Vincent%20Ng%20and%20Kin-Man%20Lam%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20Technical-Embeddings%2C%20a%20novel%20framework%20designed%0Ato%20optimize%20semantic%20retrieval%20in%20technical%20documentation%2C%20with%20applications%20in%0Aboth%20hardware%20and%20software%20development.%20Our%20approach%20addresses%20the%20challenges%0Aof%20understanding%20and%20retrieving%20complex%20technical%20content%20by%20leveraging%20the%0Acapabilities%20of%20Large%20Language%20Models%20%28LLMs%29.%20First%2C%20we%20enhance%20user%20queries%20by%0Agenerating%20expanded%20representations%20that%20better%20capture%20user%20intent%20and%20improve%0Adataset%20diversity%2C%20thereby%20enriching%20the%20fine-tuning%20process%20for%20embedding%0Amodels.%20Second%2C%20we%20apply%20summary%20extraction%20techniques%20to%20encode%20essential%0Acontextual%20information%2C%20refining%20the%20representation%20of%20technical%20documents.%20To%0Afurther%20enhance%20retrieval%20performance%2C%20we%20fine-tune%20a%20bi-encoder%20BERT%20model%0Ausing%20soft%20prompting%2C%20incorporating%20separate%20learning%20parameters%20for%20queries%0Aand%20document%20context%20to%20capture%20fine-grained%20semantic%20nuances.%20We%20evaluate%20our%0Aapproach%20on%20two%20public%20datasets%2C%20RAG-EDA%20and%20Rust-Docs-QA%2C%20demonstrating%20that%0ATechnical-Embeddings%20significantly%20outperforms%20baseline%20models%20in%20both%0Aprecision%20and%20recall.%20Our%20findings%20highlight%20the%20effectiveness%20of%20integrating%0Aquery%20expansion%20and%20contextual%20summarization%20to%20enhance%20information%20access%20and%0Acomprehension%20in%20technical%20domains.%20This%20work%20advances%20the%20state%20of%0ARetrieval-Augmented%20Generation%20%28RAG%29%20systems%2C%20offering%20new%20avenues%20for%0Aefficient%20and%20accurate%20technical%20document%20retrieval%20in%20engineering%20and%20product%0Adevelopment%20workflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04139v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Technical%2520Documents%2520Retrieval%2520for%2520RAG%26entry.906535625%3DSongjiang%2520Lai%2520and%2520Tsun-Hin%2520Cheung%2520and%2520Ka-Chun%2520Fung%2520and%2520Kaiwen%2520Xue%2520and%2520Kwan-Ho%2520Lin%2520and%2520Yan-Ming%2520Choi%2520and%2520Vincent%2520Ng%2520and%2520Kin-Man%2520Lam%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Technical-Embeddings%252C%2520a%2520novel%2520framework%2520designed%250Ato%2520optimize%2520semantic%2520retrieval%2520in%2520technical%2520documentation%252C%2520with%2520applications%2520in%250Aboth%2520hardware%2520and%2520software%2520development.%2520Our%2520approach%2520addresses%2520the%2520challenges%250Aof%2520understanding%2520and%2520retrieving%2520complex%2520technical%2520content%2520by%2520leveraging%2520the%250Acapabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520First%252C%2520we%2520enhance%2520user%2520queries%2520by%250Agenerating%2520expanded%2520representations%2520that%2520better%2520capture%2520user%2520intent%2520and%2520improve%250Adataset%2520diversity%252C%2520thereby%2520enriching%2520the%2520fine-tuning%2520process%2520for%2520embedding%250Amodels.%2520Second%252C%2520we%2520apply%2520summary%2520extraction%2520techniques%2520to%2520encode%2520essential%250Acontextual%2520information%252C%2520refining%2520the%2520representation%2520of%2520technical%2520documents.%2520To%250Afurther%2520enhance%2520retrieval%2520performance%252C%2520we%2520fine-tune%2520a%2520bi-encoder%2520BERT%2520model%250Ausing%2520soft%2520prompting%252C%2520incorporating%2520separate%2520learning%2520parameters%2520for%2520queries%250Aand%2520document%2520context%2520to%2520capture%2520fine-grained%2520semantic%2520nuances.%2520We%2520evaluate%2520our%250Aapproach%2520on%2520two%2520public%2520datasets%252C%2520RAG-EDA%2520and%2520Rust-Docs-QA%252C%2520demonstrating%2520that%250ATechnical-Embeddings%2520significantly%2520outperforms%2520baseline%2520models%2520in%2520both%250Aprecision%2520and%2520recall.%2520Our%2520findings%2520highlight%2520the%2520effectiveness%2520of%2520integrating%250Aquery%2520expansion%2520and%2520contextual%2520summarization%2520to%2520enhance%2520information%2520access%2520and%250Acomprehension%2520in%2520technical%2520domains.%2520This%2520work%2520advances%2520the%2520state%2520of%250ARetrieval-Augmented%2520Generation%2520%2528RAG%2529%2520systems%252C%2520offering%2520new%2520avenues%2520for%250Aefficient%2520and%2520accurate%2520technical%2520document%2520retrieval%2520in%2520engineering%2520and%2520product%250Adevelopment%2520workflows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04139v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Technical%20Documents%20Retrieval%20for%20RAG&entry.906535625=Songjiang%20Lai%20and%20Tsun-Hin%20Cheung%20and%20Ka-Chun%20Fung%20and%20Kaiwen%20Xue%20and%20Kwan-Ho%20Lin%20and%20Yan-Ming%20Choi%20and%20Vincent%20Ng%20and%20Kin-Man%20Lam&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20Technical-Embeddings%2C%20a%20novel%20framework%20designed%0Ato%20optimize%20semantic%20retrieval%20in%20technical%20documentation%2C%20with%20applications%20in%0Aboth%20hardware%20and%20software%20development.%20Our%20approach%20addresses%20the%20challenges%0Aof%20understanding%20and%20retrieving%20complex%20technical%20content%20by%20leveraging%20the%0Acapabilities%20of%20Large%20Language%20Models%20%28LLMs%29.%20First%2C%20we%20enhance%20user%20queries%20by%0Agenerating%20expanded%20representations%20that%20better%20capture%20user%20intent%20and%20improve%0Adataset%20diversity%2C%20thereby%20enriching%20the%20fine-tuning%20process%20for%20embedding%0Amodels.%20Second%2C%20we%20apply%20summary%20extraction%20techniques%20to%20encode%20essential%0Acontextual%20information%2C%20refining%20the%20representation%20of%20technical%20documents.%20To%0Afurther%20enhance%20retrieval%20performance%2C%20we%20fine-tune%20a%20bi-encoder%20BERT%20model%0Ausing%20soft%20prompting%2C%20incorporating%20separate%20learning%20parameters%20for%20queries%0Aand%20document%20context%20to%20capture%20fine-grained%20semantic%20nuances.%20We%20evaluate%20our%0Aapproach%20on%20two%20public%20datasets%2C%20RAG-EDA%20and%20Rust-Docs-QA%2C%20demonstrating%20that%0ATechnical-Embeddings%20significantly%20outperforms%20baseline%20models%20in%20both%0Aprecision%20and%20recall.%20Our%20findings%20highlight%20the%20effectiveness%20of%20integrating%0Aquery%20expansion%20and%20contextual%20summarization%20to%20enhance%20information%20access%20and%0Acomprehension%20in%20technical%20domains.%20This%20work%20advances%20the%20state%20of%0ARetrieval-Augmented%20Generation%20%28RAG%29%20systems%2C%20offering%20new%20avenues%20for%0Aefficient%20and%20accurate%20technical%20document%20retrieval%20in%20engineering%20and%20product%0Adevelopment%20workflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04139v1&entry.124074799=Read"},
{"title": "UniExtreme: A Universal Foundation Model for Extreme Weather Forecasting", "author": "Hang Ni and Weijia Zhang and Hao Liu", "abstract": "  Recent advancements in deep learning have led to the development of\nFoundation Models (FMs) for weather forecasting, yet their ability to predict\nextreme weather events remains limited. Existing approaches either focus on\ngeneral weather conditions or specialize in specific-type extremes, neglecting\nthe real-world atmospheric patterns of diversified extreme events. In this\nwork, we identify two key characteristics of extreme events: (1) the spectral\ndisparity against normal weather regimes, and (2) the hierarchical drivers and\ngeographic blending of diverse extremes. Along this line, we propose\nUniExtreme, a universal extreme weather forecasting foundation model that\nintegrates (1) an Adaptive Frequency Modulation (AFM) module that captures\nregion-wise spectral differences between normal and extreme weather, through\nlearnable Beta-distribution filters and multi-granularity spectral aggregation,\nand (2) an Event Prior Augmentation (EPA) module which incorporates\nregion-specific extreme event priors to resolve hierarchical extreme diversity\nand composite extreme schema, via a dual-level memory fusion network. Extensive\nexperiments demonstrate that UniExtreme outperforms state-of-the-art baselines\nin both extreme and general weather forecasting, showcasing superior\nadaptability across diverse extreme scenarios.\n", "link": "http://arxiv.org/abs/2508.01426v2", "date": "2025-09-04", "relevancy": 1.9812, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4954}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4954}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4945}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniExtreme%3A%20A%20Universal%20Foundation%20Model%20for%20Extreme%20Weather%20Forecasting&body=Title%3A%20UniExtreme%3A%20A%20Universal%20Foundation%20Model%20for%20Extreme%20Weather%20Forecasting%0AAuthor%3A%20Hang%20Ni%20and%20Weijia%20Zhang%20and%20Hao%20Liu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20deep%20learning%20have%20led%20to%20the%20development%20of%0AFoundation%20Models%20%28FMs%29%20for%20weather%20forecasting%2C%20yet%20their%20ability%20to%20predict%0Aextreme%20weather%20events%20remains%20limited.%20Existing%20approaches%20either%20focus%20on%0Ageneral%20weather%20conditions%20or%20specialize%20in%20specific-type%20extremes%2C%20neglecting%0Athe%20real-world%20atmospheric%20patterns%20of%20diversified%20extreme%20events.%20In%20this%0Awork%2C%20we%20identify%20two%20key%20characteristics%20of%20extreme%20events%3A%20%281%29%20the%20spectral%0Adisparity%20against%20normal%20weather%20regimes%2C%20and%20%282%29%20the%20hierarchical%20drivers%20and%0Ageographic%20blending%20of%20diverse%20extremes.%20Along%20this%20line%2C%20we%20propose%0AUniExtreme%2C%20a%20universal%20extreme%20weather%20forecasting%20foundation%20model%20that%0Aintegrates%20%281%29%20an%20Adaptive%20Frequency%20Modulation%20%28AFM%29%20module%20that%20captures%0Aregion-wise%20spectral%20differences%20between%20normal%20and%20extreme%20weather%2C%20through%0Alearnable%20Beta-distribution%20filters%20and%20multi-granularity%20spectral%20aggregation%2C%0Aand%20%282%29%20an%20Event%20Prior%20Augmentation%20%28EPA%29%20module%20which%20incorporates%0Aregion-specific%20extreme%20event%20priors%20to%20resolve%20hierarchical%20extreme%20diversity%0Aand%20composite%20extreme%20schema%2C%20via%20a%20dual-level%20memory%20fusion%20network.%20Extensive%0Aexperiments%20demonstrate%20that%20UniExtreme%20outperforms%20state-of-the-art%20baselines%0Ain%20both%20extreme%20and%20general%20weather%20forecasting%2C%20showcasing%20superior%0Aadaptability%20across%20diverse%20extreme%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.01426v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniExtreme%253A%2520A%2520Universal%2520Foundation%2520Model%2520for%2520Extreme%2520Weather%2520Forecasting%26entry.906535625%3DHang%2520Ni%2520and%2520Weijia%2520Zhang%2520and%2520Hao%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520deep%2520learning%2520have%2520led%2520to%2520the%2520development%2520of%250AFoundation%2520Models%2520%2528FMs%2529%2520for%2520weather%2520forecasting%252C%2520yet%2520their%2520ability%2520to%2520predict%250Aextreme%2520weather%2520events%2520remains%2520limited.%2520Existing%2520approaches%2520either%2520focus%2520on%250Ageneral%2520weather%2520conditions%2520or%2520specialize%2520in%2520specific-type%2520extremes%252C%2520neglecting%250Athe%2520real-world%2520atmospheric%2520patterns%2520of%2520diversified%2520extreme%2520events.%2520In%2520this%250Awork%252C%2520we%2520identify%2520two%2520key%2520characteristics%2520of%2520extreme%2520events%253A%2520%25281%2529%2520the%2520spectral%250Adisparity%2520against%2520normal%2520weather%2520regimes%252C%2520and%2520%25282%2529%2520the%2520hierarchical%2520drivers%2520and%250Ageographic%2520blending%2520of%2520diverse%2520extremes.%2520Along%2520this%2520line%252C%2520we%2520propose%250AUniExtreme%252C%2520a%2520universal%2520extreme%2520weather%2520forecasting%2520foundation%2520model%2520that%250Aintegrates%2520%25281%2529%2520an%2520Adaptive%2520Frequency%2520Modulation%2520%2528AFM%2529%2520module%2520that%2520captures%250Aregion-wise%2520spectral%2520differences%2520between%2520normal%2520and%2520extreme%2520weather%252C%2520through%250Alearnable%2520Beta-distribution%2520filters%2520and%2520multi-granularity%2520spectral%2520aggregation%252C%250Aand%2520%25282%2529%2520an%2520Event%2520Prior%2520Augmentation%2520%2528EPA%2529%2520module%2520which%2520incorporates%250Aregion-specific%2520extreme%2520event%2520priors%2520to%2520resolve%2520hierarchical%2520extreme%2520diversity%250Aand%2520composite%2520extreme%2520schema%252C%2520via%2520a%2520dual-level%2520memory%2520fusion%2520network.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520UniExtreme%2520outperforms%2520state-of-the-art%2520baselines%250Ain%2520both%2520extreme%2520and%2520general%2520weather%2520forecasting%252C%2520showcasing%2520superior%250Aadaptability%2520across%2520diverse%2520extreme%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.01426v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniExtreme%3A%20A%20Universal%20Foundation%20Model%20for%20Extreme%20Weather%20Forecasting&entry.906535625=Hang%20Ni%20and%20Weijia%20Zhang%20and%20Hao%20Liu&entry.1292438233=%20%20Recent%20advancements%20in%20deep%20learning%20have%20led%20to%20the%20development%20of%0AFoundation%20Models%20%28FMs%29%20for%20weather%20forecasting%2C%20yet%20their%20ability%20to%20predict%0Aextreme%20weather%20events%20remains%20limited.%20Existing%20approaches%20either%20focus%20on%0Ageneral%20weather%20conditions%20or%20specialize%20in%20specific-type%20extremes%2C%20neglecting%0Athe%20real-world%20atmospheric%20patterns%20of%20diversified%20extreme%20events.%20In%20this%0Awork%2C%20we%20identify%20two%20key%20characteristics%20of%20extreme%20events%3A%20%281%29%20the%20spectral%0Adisparity%20against%20normal%20weather%20regimes%2C%20and%20%282%29%20the%20hierarchical%20drivers%20and%0Ageographic%20blending%20of%20diverse%20extremes.%20Along%20this%20line%2C%20we%20propose%0AUniExtreme%2C%20a%20universal%20extreme%20weather%20forecasting%20foundation%20model%20that%0Aintegrates%20%281%29%20an%20Adaptive%20Frequency%20Modulation%20%28AFM%29%20module%20that%20captures%0Aregion-wise%20spectral%20differences%20between%20normal%20and%20extreme%20weather%2C%20through%0Alearnable%20Beta-distribution%20filters%20and%20multi-granularity%20spectral%20aggregation%2C%0Aand%20%282%29%20an%20Event%20Prior%20Augmentation%20%28EPA%29%20module%20which%20incorporates%0Aregion-specific%20extreme%20event%20priors%20to%20resolve%20hierarchical%20extreme%20diversity%0Aand%20composite%20extreme%20schema%2C%20via%20a%20dual-level%20memory%20fusion%20network.%20Extensive%0Aexperiments%20demonstrate%20that%20UniExtreme%20outperforms%20state-of-the-art%20baselines%0Ain%20both%20extreme%20and%20general%20weather%20forecasting%2C%20showcasing%20superior%0Aadaptability%20across%20diverse%20extreme%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.01426v2&entry.124074799=Read"},
{"title": "Understanding sparse autoencoder scaling in the presence of feature\n  manifolds", "author": "Eric J. Michaud and Liv Gorton and Tom McGrath", "abstract": "  Sparse autoencoders (SAEs) model the activations of a neural network as\nlinear combinations of sparsely occurring directions of variation (latents).\nThe ability of SAEs to reconstruct activations follows scaling laws w.r.t. the\nnumber of latents. In this work, we adapt a capacity-allocation model from the\nneural scaling literature (Brill, 2024) to understand SAE scaling, and in\nparticular, to understand how \"feature manifolds\" (multi-dimensional features)\ninfluence scaling behavior. Consistent with prior work, the model recovers\ndistinct scaling regimes. Notably, in one regime, feature manifolds have the\npathological effect of causing SAEs to learn far fewer features in data than\nthere are latents in the SAE. We provide some preliminary discussion on whether\nor not SAEs are in this pathological regime in the wild.\n", "link": "http://arxiv.org/abs/2509.02565v2", "date": "2025-09-04", "relevancy": 1.9617, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5125}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4748}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20sparse%20autoencoder%20scaling%20in%20the%20presence%20of%20feature%0A%20%20manifolds&body=Title%3A%20Understanding%20sparse%20autoencoder%20scaling%20in%20the%20presence%20of%20feature%0A%20%20manifolds%0AAuthor%3A%20Eric%20J.%20Michaud%20and%20Liv%20Gorton%20and%20Tom%20McGrath%0AAbstract%3A%20%20%20Sparse%20autoencoders%20%28SAEs%29%20model%20the%20activations%20of%20a%20neural%20network%20as%0Alinear%20combinations%20of%20sparsely%20occurring%20directions%20of%20variation%20%28latents%29.%0AThe%20ability%20of%20SAEs%20to%20reconstruct%20activations%20follows%20scaling%20laws%20w.r.t.%20the%0Anumber%20of%20latents.%20In%20this%20work%2C%20we%20adapt%20a%20capacity-allocation%20model%20from%20the%0Aneural%20scaling%20literature%20%28Brill%2C%202024%29%20to%20understand%20SAE%20scaling%2C%20and%20in%0Aparticular%2C%20to%20understand%20how%20%22feature%20manifolds%22%20%28multi-dimensional%20features%29%0Ainfluence%20scaling%20behavior.%20Consistent%20with%20prior%20work%2C%20the%20model%20recovers%0Adistinct%20scaling%20regimes.%20Notably%2C%20in%20one%20regime%2C%20feature%20manifolds%20have%20the%0Apathological%20effect%20of%20causing%20SAEs%20to%20learn%20far%20fewer%20features%20in%20data%20than%0Athere%20are%20latents%20in%20the%20SAE.%20We%20provide%20some%20preliminary%20discussion%20on%20whether%0Aor%20not%20SAEs%20are%20in%20this%20pathological%20regime%20in%20the%20wild.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.02565v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520sparse%2520autoencoder%2520scaling%2520in%2520the%2520presence%2520of%2520feature%250A%2520%2520manifolds%26entry.906535625%3DEric%2520J.%2520Michaud%2520and%2520Liv%2520Gorton%2520and%2520Tom%2520McGrath%26entry.1292438233%3D%2520%2520Sparse%2520autoencoders%2520%2528SAEs%2529%2520model%2520the%2520activations%2520of%2520a%2520neural%2520network%2520as%250Alinear%2520combinations%2520of%2520sparsely%2520occurring%2520directions%2520of%2520variation%2520%2528latents%2529.%250AThe%2520ability%2520of%2520SAEs%2520to%2520reconstruct%2520activations%2520follows%2520scaling%2520laws%2520w.r.t.%2520the%250Anumber%2520of%2520latents.%2520In%2520this%2520work%252C%2520we%2520adapt%2520a%2520capacity-allocation%2520model%2520from%2520the%250Aneural%2520scaling%2520literature%2520%2528Brill%252C%25202024%2529%2520to%2520understand%2520SAE%2520scaling%252C%2520and%2520in%250Aparticular%252C%2520to%2520understand%2520how%2520%2522feature%2520manifolds%2522%2520%2528multi-dimensional%2520features%2529%250Ainfluence%2520scaling%2520behavior.%2520Consistent%2520with%2520prior%2520work%252C%2520the%2520model%2520recovers%250Adistinct%2520scaling%2520regimes.%2520Notably%252C%2520in%2520one%2520regime%252C%2520feature%2520manifolds%2520have%2520the%250Apathological%2520effect%2520of%2520causing%2520SAEs%2520to%2520learn%2520far%2520fewer%2520features%2520in%2520data%2520than%250Athere%2520are%2520latents%2520in%2520the%2520SAE.%2520We%2520provide%2520some%2520preliminary%2520discussion%2520on%2520whether%250Aor%2520not%2520SAEs%2520are%2520in%2520this%2520pathological%2520regime%2520in%2520the%2520wild.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.02565v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20sparse%20autoencoder%20scaling%20in%20the%20presence%20of%20feature%0A%20%20manifolds&entry.906535625=Eric%20J.%20Michaud%20and%20Liv%20Gorton%20and%20Tom%20McGrath&entry.1292438233=%20%20Sparse%20autoencoders%20%28SAEs%29%20model%20the%20activations%20of%20a%20neural%20network%20as%0Alinear%20combinations%20of%20sparsely%20occurring%20directions%20of%20variation%20%28latents%29.%0AThe%20ability%20of%20SAEs%20to%20reconstruct%20activations%20follows%20scaling%20laws%20w.r.t.%20the%0Anumber%20of%20latents.%20In%20this%20work%2C%20we%20adapt%20a%20capacity-allocation%20model%20from%20the%0Aneural%20scaling%20literature%20%28Brill%2C%202024%29%20to%20understand%20SAE%20scaling%2C%20and%20in%0Aparticular%2C%20to%20understand%20how%20%22feature%20manifolds%22%20%28multi-dimensional%20features%29%0Ainfluence%20scaling%20behavior.%20Consistent%20with%20prior%20work%2C%20the%20model%20recovers%0Adistinct%20scaling%20regimes.%20Notably%2C%20in%20one%20regime%2C%20feature%20manifolds%20have%20the%0Apathological%20effect%20of%20causing%20SAEs%20to%20learn%20far%20fewer%20features%20in%20data%20than%0Athere%20are%20latents%20in%20the%20SAE.%20We%20provide%20some%20preliminary%20discussion%20on%20whether%0Aor%20not%20SAEs%20are%20in%20this%20pathological%20regime%20in%20the%20wild.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.02565v2&entry.124074799=Read"},
{"title": "Exposing Synthetic Speech: Model Attribution and Detection of\n  AI-generated Speech via Audio Fingerprints", "author": "Mat\u00edas Pizarro and Mike Laszkiewicz and Shawkat Hesso and Dorothea Kolossa and Asja Fischer", "abstract": "  As speech generation technologies continue to advance in quality and\naccessibility, the risk of malicious use cases, including impersonation,\nmisinformation, and spoofing, increases rapidly. This work addresses this\nthreat by introducing a simple, training-free, yet effective approach for\ndetecting AI-generated speech and attributing it to its source model.\nSpecifically, we tackle three key tasks: (1) single-model attribution in an\nopen-world setting, where the goal is to determine whether a given audio sample\nwas generated by a specific target neural speech synthesis system (with access\nonly to data from that system); (2) multi-model attribution in a closed-world\nsetting, where the objective is to identify the generating system from a known\npool of candidates; and last but not least (3) detection of synthetic versus\nreal speech. Our approach leverages standardized average residuals-the\ndifference between an input audio signal and its filtered version using either\na low-pass filter or the EnCodec audio autoencoder. We demonstrate that these\nresiduals consistently capture artifacts introduced by diverse speech synthesis\nsystems, serving as distinctive, model-agnostic fingerprints for attribution.\nAcross extensive experiments, our approach achieves AUROC scores exceeding 99%\nin most scenarios, evaluated on augmented benchmark datasets that pair real\nspeech with synthetic audio generated by multiple synthesis systems. In\naddition, our robustness analysis underscores the method's ability to maintain\nhigh performance even in the presence of moderate additive noise. Due to its\nsimplicity, efficiency, and strong generalization across speech synthesis\nsystems and languages, this technique offers a practical tool for digital\nforensics and security applications.\n", "link": "http://arxiv.org/abs/2411.14013v3", "date": "2025-09-04", "relevancy": 1.961, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4993}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4875}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exposing%20Synthetic%20Speech%3A%20Model%20Attribution%20and%20Detection%20of%0A%20%20AI-generated%20Speech%20via%20Audio%20Fingerprints&body=Title%3A%20Exposing%20Synthetic%20Speech%3A%20Model%20Attribution%20and%20Detection%20of%0A%20%20AI-generated%20Speech%20via%20Audio%20Fingerprints%0AAuthor%3A%20Mat%C3%ADas%20Pizarro%20and%20Mike%20Laszkiewicz%20and%20Shawkat%20Hesso%20and%20Dorothea%20Kolossa%20and%20Asja%20Fischer%0AAbstract%3A%20%20%20As%20speech%20generation%20technologies%20continue%20to%20advance%20in%20quality%20and%0Aaccessibility%2C%20the%20risk%20of%20malicious%20use%20cases%2C%20including%20impersonation%2C%0Amisinformation%2C%20and%20spoofing%2C%20increases%20rapidly.%20This%20work%20addresses%20this%0Athreat%20by%20introducing%20a%20simple%2C%20training-free%2C%20yet%20effective%20approach%20for%0Adetecting%20AI-generated%20speech%20and%20attributing%20it%20to%20its%20source%20model.%0ASpecifically%2C%20we%20tackle%20three%20key%20tasks%3A%20%281%29%20single-model%20attribution%20in%20an%0Aopen-world%20setting%2C%20where%20the%20goal%20is%20to%20determine%20whether%20a%20given%20audio%20sample%0Awas%20generated%20by%20a%20specific%20target%20neural%20speech%20synthesis%20system%20%28with%20access%0Aonly%20to%20data%20from%20that%20system%29%3B%20%282%29%20multi-model%20attribution%20in%20a%20closed-world%0Asetting%2C%20where%20the%20objective%20is%20to%20identify%20the%20generating%20system%20from%20a%20known%0Apool%20of%20candidates%3B%20and%20last%20but%20not%20least%20%283%29%20detection%20of%20synthetic%20versus%0Areal%20speech.%20Our%20approach%20leverages%20standardized%20average%20residuals-the%0Adifference%20between%20an%20input%20audio%20signal%20and%20its%20filtered%20version%20using%20either%0Aa%20low-pass%20filter%20or%20the%20EnCodec%20audio%20autoencoder.%20We%20demonstrate%20that%20these%0Aresiduals%20consistently%20capture%20artifacts%20introduced%20by%20diverse%20speech%20synthesis%0Asystems%2C%20serving%20as%20distinctive%2C%20model-agnostic%20fingerprints%20for%20attribution.%0AAcross%20extensive%20experiments%2C%20our%20approach%20achieves%20AUROC%20scores%20exceeding%2099%25%0Ain%20most%20scenarios%2C%20evaluated%20on%20augmented%20benchmark%20datasets%20that%20pair%20real%0Aspeech%20with%20synthetic%20audio%20generated%20by%20multiple%20synthesis%20systems.%20In%0Aaddition%2C%20our%20robustness%20analysis%20underscores%20the%20method%27s%20ability%20to%20maintain%0Ahigh%20performance%20even%20in%20the%20presence%20of%20moderate%20additive%20noise.%20Due%20to%20its%0Asimplicity%2C%20efficiency%2C%20and%20strong%20generalization%20across%20speech%20synthesis%0Asystems%20and%20languages%2C%20this%20technique%20offers%20a%20practical%20tool%20for%20digital%0Aforensics%20and%20security%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14013v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExposing%2520Synthetic%2520Speech%253A%2520Model%2520Attribution%2520and%2520Detection%2520of%250A%2520%2520AI-generated%2520Speech%2520via%2520Audio%2520Fingerprints%26entry.906535625%3DMat%25C3%25ADas%2520Pizarro%2520and%2520Mike%2520Laszkiewicz%2520and%2520Shawkat%2520Hesso%2520and%2520Dorothea%2520Kolossa%2520and%2520Asja%2520Fischer%26entry.1292438233%3D%2520%2520As%2520speech%2520generation%2520technologies%2520continue%2520to%2520advance%2520in%2520quality%2520and%250Aaccessibility%252C%2520the%2520risk%2520of%2520malicious%2520use%2520cases%252C%2520including%2520impersonation%252C%250Amisinformation%252C%2520and%2520spoofing%252C%2520increases%2520rapidly.%2520This%2520work%2520addresses%2520this%250Athreat%2520by%2520introducing%2520a%2520simple%252C%2520training-free%252C%2520yet%2520effective%2520approach%2520for%250Adetecting%2520AI-generated%2520speech%2520and%2520attributing%2520it%2520to%2520its%2520source%2520model.%250ASpecifically%252C%2520we%2520tackle%2520three%2520key%2520tasks%253A%2520%25281%2529%2520single-model%2520attribution%2520in%2520an%250Aopen-world%2520setting%252C%2520where%2520the%2520goal%2520is%2520to%2520determine%2520whether%2520a%2520given%2520audio%2520sample%250Awas%2520generated%2520by%2520a%2520specific%2520target%2520neural%2520speech%2520synthesis%2520system%2520%2528with%2520access%250Aonly%2520to%2520data%2520from%2520that%2520system%2529%253B%2520%25282%2529%2520multi-model%2520attribution%2520in%2520a%2520closed-world%250Asetting%252C%2520where%2520the%2520objective%2520is%2520to%2520identify%2520the%2520generating%2520system%2520from%2520a%2520known%250Apool%2520of%2520candidates%253B%2520and%2520last%2520but%2520not%2520least%2520%25283%2529%2520detection%2520of%2520synthetic%2520versus%250Areal%2520speech.%2520Our%2520approach%2520leverages%2520standardized%2520average%2520residuals-the%250Adifference%2520between%2520an%2520input%2520audio%2520signal%2520and%2520its%2520filtered%2520version%2520using%2520either%250Aa%2520low-pass%2520filter%2520or%2520the%2520EnCodec%2520audio%2520autoencoder.%2520We%2520demonstrate%2520that%2520these%250Aresiduals%2520consistently%2520capture%2520artifacts%2520introduced%2520by%2520diverse%2520speech%2520synthesis%250Asystems%252C%2520serving%2520as%2520distinctive%252C%2520model-agnostic%2520fingerprints%2520for%2520attribution.%250AAcross%2520extensive%2520experiments%252C%2520our%2520approach%2520achieves%2520AUROC%2520scores%2520exceeding%252099%2525%250Ain%2520most%2520scenarios%252C%2520evaluated%2520on%2520augmented%2520benchmark%2520datasets%2520that%2520pair%2520real%250Aspeech%2520with%2520synthetic%2520audio%2520generated%2520by%2520multiple%2520synthesis%2520systems.%2520In%250Aaddition%252C%2520our%2520robustness%2520analysis%2520underscores%2520the%2520method%2527s%2520ability%2520to%2520maintain%250Ahigh%2520performance%2520even%2520in%2520the%2520presence%2520of%2520moderate%2520additive%2520noise.%2520Due%2520to%2520its%250Asimplicity%252C%2520efficiency%252C%2520and%2520strong%2520generalization%2520across%2520speech%2520synthesis%250Asystems%2520and%2520languages%252C%2520this%2520technique%2520offers%2520a%2520practical%2520tool%2520for%2520digital%250Aforensics%2520and%2520security%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14013v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exposing%20Synthetic%20Speech%3A%20Model%20Attribution%20and%20Detection%20of%0A%20%20AI-generated%20Speech%20via%20Audio%20Fingerprints&entry.906535625=Mat%C3%ADas%20Pizarro%20and%20Mike%20Laszkiewicz%20and%20Shawkat%20Hesso%20and%20Dorothea%20Kolossa%20and%20Asja%20Fischer&entry.1292438233=%20%20As%20speech%20generation%20technologies%20continue%20to%20advance%20in%20quality%20and%0Aaccessibility%2C%20the%20risk%20of%20malicious%20use%20cases%2C%20including%20impersonation%2C%0Amisinformation%2C%20and%20spoofing%2C%20increases%20rapidly.%20This%20work%20addresses%20this%0Athreat%20by%20introducing%20a%20simple%2C%20training-free%2C%20yet%20effective%20approach%20for%0Adetecting%20AI-generated%20speech%20and%20attributing%20it%20to%20its%20source%20model.%0ASpecifically%2C%20we%20tackle%20three%20key%20tasks%3A%20%281%29%20single-model%20attribution%20in%20an%0Aopen-world%20setting%2C%20where%20the%20goal%20is%20to%20determine%20whether%20a%20given%20audio%20sample%0Awas%20generated%20by%20a%20specific%20target%20neural%20speech%20synthesis%20system%20%28with%20access%0Aonly%20to%20data%20from%20that%20system%29%3B%20%282%29%20multi-model%20attribution%20in%20a%20closed-world%0Asetting%2C%20where%20the%20objective%20is%20to%20identify%20the%20generating%20system%20from%20a%20known%0Apool%20of%20candidates%3B%20and%20last%20but%20not%20least%20%283%29%20detection%20of%20synthetic%20versus%0Areal%20speech.%20Our%20approach%20leverages%20standardized%20average%20residuals-the%0Adifference%20between%20an%20input%20audio%20signal%20and%20its%20filtered%20version%20using%20either%0Aa%20low-pass%20filter%20or%20the%20EnCodec%20audio%20autoencoder.%20We%20demonstrate%20that%20these%0Aresiduals%20consistently%20capture%20artifacts%20introduced%20by%20diverse%20speech%20synthesis%0Asystems%2C%20serving%20as%20distinctive%2C%20model-agnostic%20fingerprints%20for%20attribution.%0AAcross%20extensive%20experiments%2C%20our%20approach%20achieves%20AUROC%20scores%20exceeding%2099%25%0Ain%20most%20scenarios%2C%20evaluated%20on%20augmented%20benchmark%20datasets%20that%20pair%20real%0Aspeech%20with%20synthetic%20audio%20generated%20by%20multiple%20synthesis%20systems.%20In%0Aaddition%2C%20our%20robustness%20analysis%20underscores%20the%20method%27s%20ability%20to%20maintain%0Ahigh%20performance%20even%20in%20the%20presence%20of%20moderate%20additive%20noise.%20Due%20to%20its%0Asimplicity%2C%20efficiency%2C%20and%20strong%20generalization%20across%20speech%20synthesis%0Asystems%20and%20languages%2C%20this%20technique%20offers%20a%20practical%20tool%20for%20digital%0Aforensics%20and%20security%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14013v3&entry.124074799=Read"},
{"title": "AutoPETIII: The Tracer Frontier. What Frontier?", "author": "Zacharia Mesbah and L\u00e9o Mottay and Romain Modzelewski and Pierre Decazes and S\u00e9bastien Hapdey and Su Ruan and S\u00e9bastien Thureau", "abstract": "  For the last three years, the AutoPET competition gathered the medical\nimaging community around a hot topic: lesion segmentation on Positron Emitting\nTomography (PET) scans. Each year a different aspect of the problem is\npresented; in 2024 the multiplicity of existing and used tracers was at the\ncore of the challenge. Specifically, this year's edition aims to develop a\nfully automatic algorithm capable of performing lesion segmentation on a PET/CT\nscan, without knowing the tracer, which can either be a FDG or PSMA-based\ntracer. In this paper we describe how we used the nnUNetv2 framework to train\ntwo sets of 6 fold ensembles of models to perform fully automatic PET/CT lesion\nsegmentation as well as a MIP-CNN to choose which set of models to use for\nsegmentation.\n", "link": "http://arxiv.org/abs/2410.02807v2", "date": "2025-09-04", "relevancy": 1.96, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5214}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4892}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4782}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoPETIII%3A%20The%20Tracer%20Frontier.%20What%20Frontier%3F&body=Title%3A%20AutoPETIII%3A%20The%20Tracer%20Frontier.%20What%20Frontier%3F%0AAuthor%3A%20Zacharia%20Mesbah%20and%20L%C3%A9o%20Mottay%20and%20Romain%20Modzelewski%20and%20Pierre%20Decazes%20and%20S%C3%A9bastien%20Hapdey%20and%20Su%20Ruan%20and%20S%C3%A9bastien%20Thureau%0AAbstract%3A%20%20%20For%20the%20last%20three%20years%2C%20the%20AutoPET%20competition%20gathered%20the%20medical%0Aimaging%20community%20around%20a%20hot%20topic%3A%20lesion%20segmentation%20on%20Positron%20Emitting%0ATomography%20%28PET%29%20scans.%20Each%20year%20a%20different%20aspect%20of%20the%20problem%20is%0Apresented%3B%20in%202024%20the%20multiplicity%20of%20existing%20and%20used%20tracers%20was%20at%20the%0Acore%20of%20the%20challenge.%20Specifically%2C%20this%20year%27s%20edition%20aims%20to%20develop%20a%0Afully%20automatic%20algorithm%20capable%20of%20performing%20lesion%20segmentation%20on%20a%20PET/CT%0Ascan%2C%20without%20knowing%20the%20tracer%2C%20which%20can%20either%20be%20a%20FDG%20or%20PSMA-based%0Atracer.%20In%20this%20paper%20we%20describe%20how%20we%20used%20the%20nnUNetv2%20framework%20to%20train%0Atwo%20sets%20of%206%20fold%20ensembles%20of%20models%20to%20perform%20fully%20automatic%20PET/CT%20lesion%0Asegmentation%20as%20well%20as%20a%20MIP-CNN%20to%20choose%20which%20set%20of%20models%20to%20use%20for%0Asegmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02807v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoPETIII%253A%2520The%2520Tracer%2520Frontier.%2520What%2520Frontier%253F%26entry.906535625%3DZacharia%2520Mesbah%2520and%2520L%25C3%25A9o%2520Mottay%2520and%2520Romain%2520Modzelewski%2520and%2520Pierre%2520Decazes%2520and%2520S%25C3%25A9bastien%2520Hapdey%2520and%2520Su%2520Ruan%2520and%2520S%25C3%25A9bastien%2520Thureau%26entry.1292438233%3D%2520%2520For%2520the%2520last%2520three%2520years%252C%2520the%2520AutoPET%2520competition%2520gathered%2520the%2520medical%250Aimaging%2520community%2520around%2520a%2520hot%2520topic%253A%2520lesion%2520segmentation%2520on%2520Positron%2520Emitting%250ATomography%2520%2528PET%2529%2520scans.%2520Each%2520year%2520a%2520different%2520aspect%2520of%2520the%2520problem%2520is%250Apresented%253B%2520in%25202024%2520the%2520multiplicity%2520of%2520existing%2520and%2520used%2520tracers%2520was%2520at%2520the%250Acore%2520of%2520the%2520challenge.%2520Specifically%252C%2520this%2520year%2527s%2520edition%2520aims%2520to%2520develop%2520a%250Afully%2520automatic%2520algorithm%2520capable%2520of%2520performing%2520lesion%2520segmentation%2520on%2520a%2520PET/CT%250Ascan%252C%2520without%2520knowing%2520the%2520tracer%252C%2520which%2520can%2520either%2520be%2520a%2520FDG%2520or%2520PSMA-based%250Atracer.%2520In%2520this%2520paper%2520we%2520describe%2520how%2520we%2520used%2520the%2520nnUNetv2%2520framework%2520to%2520train%250Atwo%2520sets%2520of%25206%2520fold%2520ensembles%2520of%2520models%2520to%2520perform%2520fully%2520automatic%2520PET/CT%2520lesion%250Asegmentation%2520as%2520well%2520as%2520a%2520MIP-CNN%2520to%2520choose%2520which%2520set%2520of%2520models%2520to%2520use%2520for%250Asegmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02807v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoPETIII%3A%20The%20Tracer%20Frontier.%20What%20Frontier%3F&entry.906535625=Zacharia%20Mesbah%20and%20L%C3%A9o%20Mottay%20and%20Romain%20Modzelewski%20and%20Pierre%20Decazes%20and%20S%C3%A9bastien%20Hapdey%20and%20Su%20Ruan%20and%20S%C3%A9bastien%20Thureau&entry.1292438233=%20%20For%20the%20last%20three%20years%2C%20the%20AutoPET%20competition%20gathered%20the%20medical%0Aimaging%20community%20around%20a%20hot%20topic%3A%20lesion%20segmentation%20on%20Positron%20Emitting%0ATomography%20%28PET%29%20scans.%20Each%20year%20a%20different%20aspect%20of%20the%20problem%20is%0Apresented%3B%20in%202024%20the%20multiplicity%20of%20existing%20and%20used%20tracers%20was%20at%20the%0Acore%20of%20the%20challenge.%20Specifically%2C%20this%20year%27s%20edition%20aims%20to%20develop%20a%0Afully%20automatic%20algorithm%20capable%20of%20performing%20lesion%20segmentation%20on%20a%20PET/CT%0Ascan%2C%20without%20knowing%20the%20tracer%2C%20which%20can%20either%20be%20a%20FDG%20or%20PSMA-based%0Atracer.%20In%20this%20paper%20we%20describe%20how%20we%20used%20the%20nnUNetv2%20framework%20to%20train%0Atwo%20sets%20of%206%20fold%20ensembles%20of%20models%20to%20perform%20fully%20automatic%20PET/CT%20lesion%0Asegmentation%20as%20well%20as%20a%20MIP-CNN%20to%20choose%20which%20set%20of%20models%20to%20use%20for%0Asegmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02807v2&entry.124074799=Read"},
{"title": "Facts Fade Fast: Evaluating Memorization of Outdated Medical Knowledge\n  in Large Language Models", "author": "Juraj Vladika and Mahdi Dhaini and Florian Matthes", "abstract": "  The growing capabilities of Large Language Models (LLMs) show significant\npotential to enhance healthcare by assisting medical researchers and\nphysicians. However, their reliance on static training data is a major risk\nwhen medical recommendations evolve with new research and developments. When\nLLMs memorize outdated medical knowledge, they can provide harmful advice or\nfail at clinical reasoning tasks. To investigate this problem, we introduce two\nnovel question-answering (QA) datasets derived from systematic reviews:\nMedRevQA (16,501 QA pairs covering general biomedical knowledge) and\nMedChangeQA (a subset of 512 QA pairs where medical consensus has changed over\ntime). Our evaluation of eight prominent LLMs on the datasets reveals\nconsistent reliance on outdated knowledge across all models. We additionally\nanalyze the influence of obsolete pre-training data and training strategies to\nexplain this phenomenon and propose future directions for mitigation, laying\nthe groundwork for developing more current and reliable medical AI systems.\n", "link": "http://arxiv.org/abs/2509.04304v1", "date": "2025-09-04", "relevancy": 1.9554, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5152}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4836}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Facts%20Fade%20Fast%3A%20Evaluating%20Memorization%20of%20Outdated%20Medical%20Knowledge%0A%20%20in%20Large%20Language%20Models&body=Title%3A%20Facts%20Fade%20Fast%3A%20Evaluating%20Memorization%20of%20Outdated%20Medical%20Knowledge%0A%20%20in%20Large%20Language%20Models%0AAuthor%3A%20Juraj%20Vladika%20and%20Mahdi%20Dhaini%20and%20Florian%20Matthes%0AAbstract%3A%20%20%20The%20growing%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%20show%20significant%0Apotential%20to%20enhance%20healthcare%20by%20assisting%20medical%20researchers%20and%0Aphysicians.%20However%2C%20their%20reliance%20on%20static%20training%20data%20is%20a%20major%20risk%0Awhen%20medical%20recommendations%20evolve%20with%20new%20research%20and%20developments.%20When%0ALLMs%20memorize%20outdated%20medical%20knowledge%2C%20they%20can%20provide%20harmful%20advice%20or%0Afail%20at%20clinical%20reasoning%20tasks.%20To%20investigate%20this%20problem%2C%20we%20introduce%20two%0Anovel%20question-answering%20%28QA%29%20datasets%20derived%20from%20systematic%20reviews%3A%0AMedRevQA%20%2816%2C501%20QA%20pairs%20covering%20general%20biomedical%20knowledge%29%20and%0AMedChangeQA%20%28a%20subset%20of%20512%20QA%20pairs%20where%20medical%20consensus%20has%20changed%20over%0Atime%29.%20Our%20evaluation%20of%20eight%20prominent%20LLMs%20on%20the%20datasets%20reveals%0Aconsistent%20reliance%20on%20outdated%20knowledge%20across%20all%20models.%20We%20additionally%0Aanalyze%20the%20influence%20of%20obsolete%20pre-training%20data%20and%20training%20strategies%20to%0Aexplain%20this%20phenomenon%20and%20propose%20future%20directions%20for%20mitigation%2C%20laying%0Athe%20groundwork%20for%20developing%20more%20current%20and%20reliable%20medical%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04304v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFacts%2520Fade%2520Fast%253A%2520Evaluating%2520Memorization%2520of%2520Outdated%2520Medical%2520Knowledge%250A%2520%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DJuraj%2520Vladika%2520and%2520Mahdi%2520Dhaini%2520and%2520Florian%2520Matthes%26entry.1292438233%3D%2520%2520The%2520growing%2520capabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520show%2520significant%250Apotential%2520to%2520enhance%2520healthcare%2520by%2520assisting%2520medical%2520researchers%2520and%250Aphysicians.%2520However%252C%2520their%2520reliance%2520on%2520static%2520training%2520data%2520is%2520a%2520major%2520risk%250Awhen%2520medical%2520recommendations%2520evolve%2520with%2520new%2520research%2520and%2520developments.%2520When%250ALLMs%2520memorize%2520outdated%2520medical%2520knowledge%252C%2520they%2520can%2520provide%2520harmful%2520advice%2520or%250Afail%2520at%2520clinical%2520reasoning%2520tasks.%2520To%2520investigate%2520this%2520problem%252C%2520we%2520introduce%2520two%250Anovel%2520question-answering%2520%2528QA%2529%2520datasets%2520derived%2520from%2520systematic%2520reviews%253A%250AMedRevQA%2520%252816%252C501%2520QA%2520pairs%2520covering%2520general%2520biomedical%2520knowledge%2529%2520and%250AMedChangeQA%2520%2528a%2520subset%2520of%2520512%2520QA%2520pairs%2520where%2520medical%2520consensus%2520has%2520changed%2520over%250Atime%2529.%2520Our%2520evaluation%2520of%2520eight%2520prominent%2520LLMs%2520on%2520the%2520datasets%2520reveals%250Aconsistent%2520reliance%2520on%2520outdated%2520knowledge%2520across%2520all%2520models.%2520We%2520additionally%250Aanalyze%2520the%2520influence%2520of%2520obsolete%2520pre-training%2520data%2520and%2520training%2520strategies%2520to%250Aexplain%2520this%2520phenomenon%2520and%2520propose%2520future%2520directions%2520for%2520mitigation%252C%2520laying%250Athe%2520groundwork%2520for%2520developing%2520more%2520current%2520and%2520reliable%2520medical%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04304v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Facts%20Fade%20Fast%3A%20Evaluating%20Memorization%20of%20Outdated%20Medical%20Knowledge%0A%20%20in%20Large%20Language%20Models&entry.906535625=Juraj%20Vladika%20and%20Mahdi%20Dhaini%20and%20Florian%20Matthes&entry.1292438233=%20%20The%20growing%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%20show%20significant%0Apotential%20to%20enhance%20healthcare%20by%20assisting%20medical%20researchers%20and%0Aphysicians.%20However%2C%20their%20reliance%20on%20static%20training%20data%20is%20a%20major%20risk%0Awhen%20medical%20recommendations%20evolve%20with%20new%20research%20and%20developments.%20When%0ALLMs%20memorize%20outdated%20medical%20knowledge%2C%20they%20can%20provide%20harmful%20advice%20or%0Afail%20at%20clinical%20reasoning%20tasks.%20To%20investigate%20this%20problem%2C%20we%20introduce%20two%0Anovel%20question-answering%20%28QA%29%20datasets%20derived%20from%20systematic%20reviews%3A%0AMedRevQA%20%2816%2C501%20QA%20pairs%20covering%20general%20biomedical%20knowledge%29%20and%0AMedChangeQA%20%28a%20subset%20of%20512%20QA%20pairs%20where%20medical%20consensus%20has%20changed%20over%0Atime%29.%20Our%20evaluation%20of%20eight%20prominent%20LLMs%20on%20the%20datasets%20reveals%0Aconsistent%20reliance%20on%20outdated%20knowledge%20across%20all%20models.%20We%20additionally%0Aanalyze%20the%20influence%20of%20obsolete%20pre-training%20data%20and%20training%20strategies%20to%0Aexplain%20this%20phenomenon%20and%20propose%20future%20directions%20for%20mitigation%2C%20laying%0Athe%20groundwork%20for%20developing%20more%20current%20and%20reliable%20medical%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04304v1&entry.124074799=Read"},
{"title": "MiniCPM4: Ultra-Efficient LLMs on End Devices", "author": " MiniCPM Team and Chaojun Xiao and Yuxuan Li and Xu Han and Yuzhuo Bai and Jie Cai and Haotian Chen and Wentong Chen and Xin Cong and Ganqu Cui and Ning Ding and Shengda Fan and Yewei Fang and Zixuan Fu and Wenyu Guan and Yitong Guan and Junshao Guo and Yufeng Han and Bingxiang He and Yuxiang Huang and Baoxi Ji and Cunliang Kong and Qiuzuo Li and Siyuan Li and Wenhao Li and Xin Li and Yanghao Li and Yishan Li and Zhen Li and Dan Liu and Biyuan Lin and Yankai Lin and Xiang Long and Quanyu Lu and Yaxi Lu and Peiyan Luo and Hongya Lyu and Litu Ou and Yinxu Pan and Lushi Pu and Zekai Qu and Qundong Shi and Zijun Song and Jiayuan Su and Zhou Su and Ao Sun and Xianghui Sun and Peijun Tang and Fangzheng Wang and Feng Wang and Shuo Wang and Yudong Wang and Zheng Wang and Yesai Wu and Zhenyu Xiao and Jie Xie and Zihao Xie and Xiaoyue Xu and Yukun Yan and Jiarui Yuan and Jinqian Zhang and Kaihuo Zhang and Lei Zhang and Linyue Zhang and Xueren Zhang and Yudi Zhang and Hengyu Zhao and Weilin Zhao and Weilun Zhao and Yuanqian Zhao and Zhi Zheng and Chuyue Zhou and Ge Zhou and Jie Zhou and Wei Zhou and Yanghao Zhou and Zihan Zhou and Zixuan Zhou and Zhiyuan Liu and Guoyang Zeng and Chao Jia and Dahai Li and Maosong Sun", "abstract": "  This paper introduces MiniCPM4, a highly efficient large language model (LLM)\ndesigned explicitly for end-side devices. We achieve this efficiency through\nsystematic innovation in four key dimensions: model architecture, training\ndata, training algorithms, and inference systems. Specifically, in terms of\nmodel architecture, we propose InfLLM v2, a trainable sparse attention\nmechanism that accelerates both prefilling and decoding phases for long-context\nprocessing. Regarding training data, we propose UltraClean, an efficient and\naccurate pre-training data filtering and generation strategy, and UltraChat v2,\na comprehensive supervised fine-tuning dataset. These datasets enable\nsatisfactory model performance to be achieved using just 8 trillion training\ntokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient\npre-training strategy search, and improve existing post-training methods by\nintroducing chunk-wise rollout for load-balanced reinforcement learning and\ndata-efficient tenary LLM, BitCPM. Regarding inference systems, we propose\nCPM.cu that integrates sparse attention, model quantization, and speculative\nsampling to achieve efficient prefilling and decoding. To meet diverse\non-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B\nparameters, respectively. Furthermore, we construct a hybrid reasoning model,\nMiniCPM4.1, which can be used in both deep reasoning mode and non-reasoning\nmode. Evaluation results demonstrate that MiniCPM4 and MiniCPM4.1 outperform\nsimilar-sized open-source models across benchmarks, with the 8B variants\nshowing significant speed improvements on long sequence understanding and\ngeneration.\n", "link": "http://arxiv.org/abs/2506.07900v2", "date": "2025-09-04", "relevancy": 1.9538, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5176}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4927}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MiniCPM4%3A%20Ultra-Efficient%20LLMs%20on%20End%20Devices&body=Title%3A%20MiniCPM4%3A%20Ultra-Efficient%20LLMs%20on%20End%20Devices%0AAuthor%3A%20%20MiniCPM%20Team%20and%20Chaojun%20Xiao%20and%20Yuxuan%20Li%20and%20Xu%20Han%20and%20Yuzhuo%20Bai%20and%20Jie%20Cai%20and%20Haotian%20Chen%20and%20Wentong%20Chen%20and%20Xin%20Cong%20and%20Ganqu%20Cui%20and%20Ning%20Ding%20and%20Shengda%20Fan%20and%20Yewei%20Fang%20and%20Zixuan%20Fu%20and%20Wenyu%20Guan%20and%20Yitong%20Guan%20and%20Junshao%20Guo%20and%20Yufeng%20Han%20and%20Bingxiang%20He%20and%20Yuxiang%20Huang%20and%20Baoxi%20Ji%20and%20Cunliang%20Kong%20and%20Qiuzuo%20Li%20and%20Siyuan%20Li%20and%20Wenhao%20Li%20and%20Xin%20Li%20and%20Yanghao%20Li%20and%20Yishan%20Li%20and%20Zhen%20Li%20and%20Dan%20Liu%20and%20Biyuan%20Lin%20and%20Yankai%20Lin%20and%20Xiang%20Long%20and%20Quanyu%20Lu%20and%20Yaxi%20Lu%20and%20Peiyan%20Luo%20and%20Hongya%20Lyu%20and%20Litu%20Ou%20and%20Yinxu%20Pan%20and%20Lushi%20Pu%20and%20Zekai%20Qu%20and%20Qundong%20Shi%20and%20Zijun%20Song%20and%20Jiayuan%20Su%20and%20Zhou%20Su%20and%20Ao%20Sun%20and%20Xianghui%20Sun%20and%20Peijun%20Tang%20and%20Fangzheng%20Wang%20and%20Feng%20Wang%20and%20Shuo%20Wang%20and%20Yudong%20Wang%20and%20Zheng%20Wang%20and%20Yesai%20Wu%20and%20Zhenyu%20Xiao%20and%20Jie%20Xie%20and%20Zihao%20Xie%20and%20Xiaoyue%20Xu%20and%20Yukun%20Yan%20and%20Jiarui%20Yuan%20and%20Jinqian%20Zhang%20and%20Kaihuo%20Zhang%20and%20Lei%20Zhang%20and%20Linyue%20Zhang%20and%20Xueren%20Zhang%20and%20Yudi%20Zhang%20and%20Hengyu%20Zhao%20and%20Weilin%20Zhao%20and%20Weilun%20Zhao%20and%20Yuanqian%20Zhao%20and%20Zhi%20Zheng%20and%20Chuyue%20Zhou%20and%20Ge%20Zhou%20and%20Jie%20Zhou%20and%20Wei%20Zhou%20and%20Yanghao%20Zhou%20and%20Zihan%20Zhou%20and%20Zixuan%20Zhou%20and%20Zhiyuan%20Liu%20and%20Guoyang%20Zeng%20and%20Chao%20Jia%20and%20Dahai%20Li%20and%20Maosong%20Sun%0AAbstract%3A%20%20%20This%20paper%20introduces%20MiniCPM4%2C%20a%20highly%20efficient%20large%20language%20model%20%28LLM%29%0Adesigned%20explicitly%20for%20end-side%20devices.%20We%20achieve%20this%20efficiency%20through%0Asystematic%20innovation%20in%20four%20key%20dimensions%3A%20model%20architecture%2C%20training%0Adata%2C%20training%20algorithms%2C%20and%20inference%20systems.%20Specifically%2C%20in%20terms%20of%0Amodel%20architecture%2C%20we%20propose%20InfLLM%20v2%2C%20a%20trainable%20sparse%20attention%0Amechanism%20that%20accelerates%20both%20prefilling%20and%20decoding%20phases%20for%20long-context%0Aprocessing.%20Regarding%20training%20data%2C%20we%20propose%20UltraClean%2C%20an%20efficient%20and%0Aaccurate%20pre-training%20data%20filtering%20and%20generation%20strategy%2C%20and%20UltraChat%20v2%2C%0Aa%20comprehensive%20supervised%20fine-tuning%20dataset.%20These%20datasets%20enable%0Asatisfactory%20model%20performance%20to%20be%20achieved%20using%20just%208%20trillion%20training%0Atokens.%20Regarding%20training%20algorithms%2C%20we%20propose%20ModelTunnel%20v2%20for%20efficient%0Apre-training%20strategy%20search%2C%20and%20improve%20existing%20post-training%20methods%20by%0Aintroducing%20chunk-wise%20rollout%20for%20load-balanced%20reinforcement%20learning%20and%0Adata-efficient%20tenary%20LLM%2C%20BitCPM.%20Regarding%20inference%20systems%2C%20we%20propose%0ACPM.cu%20that%20integrates%20sparse%20attention%2C%20model%20quantization%2C%20and%20speculative%0Asampling%20to%20achieve%20efficient%20prefilling%20and%20decoding.%20To%20meet%20diverse%0Aon-device%20requirements%2C%20MiniCPM4%20is%20available%20in%20two%20versions%2C%20with%200.5B%20and%208B%0Aparameters%2C%20respectively.%20Furthermore%2C%20we%20construct%20a%20hybrid%20reasoning%20model%2C%0AMiniCPM4.1%2C%20which%20can%20be%20used%20in%20both%20deep%20reasoning%20mode%20and%20non-reasoning%0Amode.%20Evaluation%20results%20demonstrate%20that%20MiniCPM4%20and%20MiniCPM4.1%20outperform%0Asimilar-sized%20open-source%20models%20across%20benchmarks%2C%20with%20the%208B%20variants%0Ashowing%20significant%20speed%20improvements%20on%20long%20sequence%20understanding%20and%0Ageneration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07900v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMiniCPM4%253A%2520Ultra-Efficient%2520LLMs%2520on%2520End%2520Devices%26entry.906535625%3D%2520MiniCPM%2520Team%2520and%2520Chaojun%2520Xiao%2520and%2520Yuxuan%2520Li%2520and%2520Xu%2520Han%2520and%2520Yuzhuo%2520Bai%2520and%2520Jie%2520Cai%2520and%2520Haotian%2520Chen%2520and%2520Wentong%2520Chen%2520and%2520Xin%2520Cong%2520and%2520Ganqu%2520Cui%2520and%2520Ning%2520Ding%2520and%2520Shengda%2520Fan%2520and%2520Yewei%2520Fang%2520and%2520Zixuan%2520Fu%2520and%2520Wenyu%2520Guan%2520and%2520Yitong%2520Guan%2520and%2520Junshao%2520Guo%2520and%2520Yufeng%2520Han%2520and%2520Bingxiang%2520He%2520and%2520Yuxiang%2520Huang%2520and%2520Baoxi%2520Ji%2520and%2520Cunliang%2520Kong%2520and%2520Qiuzuo%2520Li%2520and%2520Siyuan%2520Li%2520and%2520Wenhao%2520Li%2520and%2520Xin%2520Li%2520and%2520Yanghao%2520Li%2520and%2520Yishan%2520Li%2520and%2520Zhen%2520Li%2520and%2520Dan%2520Liu%2520and%2520Biyuan%2520Lin%2520and%2520Yankai%2520Lin%2520and%2520Xiang%2520Long%2520and%2520Quanyu%2520Lu%2520and%2520Yaxi%2520Lu%2520and%2520Peiyan%2520Luo%2520and%2520Hongya%2520Lyu%2520and%2520Litu%2520Ou%2520and%2520Yinxu%2520Pan%2520and%2520Lushi%2520Pu%2520and%2520Zekai%2520Qu%2520and%2520Qundong%2520Shi%2520and%2520Zijun%2520Song%2520and%2520Jiayuan%2520Su%2520and%2520Zhou%2520Su%2520and%2520Ao%2520Sun%2520and%2520Xianghui%2520Sun%2520and%2520Peijun%2520Tang%2520and%2520Fangzheng%2520Wang%2520and%2520Feng%2520Wang%2520and%2520Shuo%2520Wang%2520and%2520Yudong%2520Wang%2520and%2520Zheng%2520Wang%2520and%2520Yesai%2520Wu%2520and%2520Zhenyu%2520Xiao%2520and%2520Jie%2520Xie%2520and%2520Zihao%2520Xie%2520and%2520Xiaoyue%2520Xu%2520and%2520Yukun%2520Yan%2520and%2520Jiarui%2520Yuan%2520and%2520Jinqian%2520Zhang%2520and%2520Kaihuo%2520Zhang%2520and%2520Lei%2520Zhang%2520and%2520Linyue%2520Zhang%2520and%2520Xueren%2520Zhang%2520and%2520Yudi%2520Zhang%2520and%2520Hengyu%2520Zhao%2520and%2520Weilin%2520Zhao%2520and%2520Weilun%2520Zhao%2520and%2520Yuanqian%2520Zhao%2520and%2520Zhi%2520Zheng%2520and%2520Chuyue%2520Zhou%2520and%2520Ge%2520Zhou%2520and%2520Jie%2520Zhou%2520and%2520Wei%2520Zhou%2520and%2520Yanghao%2520Zhou%2520and%2520Zihan%2520Zhou%2520and%2520Zixuan%2520Zhou%2520and%2520Zhiyuan%2520Liu%2520and%2520Guoyang%2520Zeng%2520and%2520Chao%2520Jia%2520and%2520Dahai%2520Li%2520and%2520Maosong%2520Sun%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520MiniCPM4%252C%2520a%2520highly%2520efficient%2520large%2520language%2520model%2520%2528LLM%2529%250Adesigned%2520explicitly%2520for%2520end-side%2520devices.%2520We%2520achieve%2520this%2520efficiency%2520through%250Asystematic%2520innovation%2520in%2520four%2520key%2520dimensions%253A%2520model%2520architecture%252C%2520training%250Adata%252C%2520training%2520algorithms%252C%2520and%2520inference%2520systems.%2520Specifically%252C%2520in%2520terms%2520of%250Amodel%2520architecture%252C%2520we%2520propose%2520InfLLM%2520v2%252C%2520a%2520trainable%2520sparse%2520attention%250Amechanism%2520that%2520accelerates%2520both%2520prefilling%2520and%2520decoding%2520phases%2520for%2520long-context%250Aprocessing.%2520Regarding%2520training%2520data%252C%2520we%2520propose%2520UltraClean%252C%2520an%2520efficient%2520and%250Aaccurate%2520pre-training%2520data%2520filtering%2520and%2520generation%2520strategy%252C%2520and%2520UltraChat%2520v2%252C%250Aa%2520comprehensive%2520supervised%2520fine-tuning%2520dataset.%2520These%2520datasets%2520enable%250Asatisfactory%2520model%2520performance%2520to%2520be%2520achieved%2520using%2520just%25208%2520trillion%2520training%250Atokens.%2520Regarding%2520training%2520algorithms%252C%2520we%2520propose%2520ModelTunnel%2520v2%2520for%2520efficient%250Apre-training%2520strategy%2520search%252C%2520and%2520improve%2520existing%2520post-training%2520methods%2520by%250Aintroducing%2520chunk-wise%2520rollout%2520for%2520load-balanced%2520reinforcement%2520learning%2520and%250Adata-efficient%2520tenary%2520LLM%252C%2520BitCPM.%2520Regarding%2520inference%2520systems%252C%2520we%2520propose%250ACPM.cu%2520that%2520integrates%2520sparse%2520attention%252C%2520model%2520quantization%252C%2520and%2520speculative%250Asampling%2520to%2520achieve%2520efficient%2520prefilling%2520and%2520decoding.%2520To%2520meet%2520diverse%250Aon-device%2520requirements%252C%2520MiniCPM4%2520is%2520available%2520in%2520two%2520versions%252C%2520with%25200.5B%2520and%25208B%250Aparameters%252C%2520respectively.%2520Furthermore%252C%2520we%2520construct%2520a%2520hybrid%2520reasoning%2520model%252C%250AMiniCPM4.1%252C%2520which%2520can%2520be%2520used%2520in%2520both%2520deep%2520reasoning%2520mode%2520and%2520non-reasoning%250Amode.%2520Evaluation%2520results%2520demonstrate%2520that%2520MiniCPM4%2520and%2520MiniCPM4.1%2520outperform%250Asimilar-sized%2520open-source%2520models%2520across%2520benchmarks%252C%2520with%2520the%25208B%2520variants%250Ashowing%2520significant%2520speed%2520improvements%2520on%2520long%2520sequence%2520understanding%2520and%250Ageneration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07900v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MiniCPM4%3A%20Ultra-Efficient%20LLMs%20on%20End%20Devices&entry.906535625=%20MiniCPM%20Team%20and%20Chaojun%20Xiao%20and%20Yuxuan%20Li%20and%20Xu%20Han%20and%20Yuzhuo%20Bai%20and%20Jie%20Cai%20and%20Haotian%20Chen%20and%20Wentong%20Chen%20and%20Xin%20Cong%20and%20Ganqu%20Cui%20and%20Ning%20Ding%20and%20Shengda%20Fan%20and%20Yewei%20Fang%20and%20Zixuan%20Fu%20and%20Wenyu%20Guan%20and%20Yitong%20Guan%20and%20Junshao%20Guo%20and%20Yufeng%20Han%20and%20Bingxiang%20He%20and%20Yuxiang%20Huang%20and%20Baoxi%20Ji%20and%20Cunliang%20Kong%20and%20Qiuzuo%20Li%20and%20Siyuan%20Li%20and%20Wenhao%20Li%20and%20Xin%20Li%20and%20Yanghao%20Li%20and%20Yishan%20Li%20and%20Zhen%20Li%20and%20Dan%20Liu%20and%20Biyuan%20Lin%20and%20Yankai%20Lin%20and%20Xiang%20Long%20and%20Quanyu%20Lu%20and%20Yaxi%20Lu%20and%20Peiyan%20Luo%20and%20Hongya%20Lyu%20and%20Litu%20Ou%20and%20Yinxu%20Pan%20and%20Lushi%20Pu%20and%20Zekai%20Qu%20and%20Qundong%20Shi%20and%20Zijun%20Song%20and%20Jiayuan%20Su%20and%20Zhou%20Su%20and%20Ao%20Sun%20and%20Xianghui%20Sun%20and%20Peijun%20Tang%20and%20Fangzheng%20Wang%20and%20Feng%20Wang%20and%20Shuo%20Wang%20and%20Yudong%20Wang%20and%20Zheng%20Wang%20and%20Yesai%20Wu%20and%20Zhenyu%20Xiao%20and%20Jie%20Xie%20and%20Zihao%20Xie%20and%20Xiaoyue%20Xu%20and%20Yukun%20Yan%20and%20Jiarui%20Yuan%20and%20Jinqian%20Zhang%20and%20Kaihuo%20Zhang%20and%20Lei%20Zhang%20and%20Linyue%20Zhang%20and%20Xueren%20Zhang%20and%20Yudi%20Zhang%20and%20Hengyu%20Zhao%20and%20Weilin%20Zhao%20and%20Weilun%20Zhao%20and%20Yuanqian%20Zhao%20and%20Zhi%20Zheng%20and%20Chuyue%20Zhou%20and%20Ge%20Zhou%20and%20Jie%20Zhou%20and%20Wei%20Zhou%20and%20Yanghao%20Zhou%20and%20Zihan%20Zhou%20and%20Zixuan%20Zhou%20and%20Zhiyuan%20Liu%20and%20Guoyang%20Zeng%20and%20Chao%20Jia%20and%20Dahai%20Li%20and%20Maosong%20Sun&entry.1292438233=%20%20This%20paper%20introduces%20MiniCPM4%2C%20a%20highly%20efficient%20large%20language%20model%20%28LLM%29%0Adesigned%20explicitly%20for%20end-side%20devices.%20We%20achieve%20this%20efficiency%20through%0Asystematic%20innovation%20in%20four%20key%20dimensions%3A%20model%20architecture%2C%20training%0Adata%2C%20training%20algorithms%2C%20and%20inference%20systems.%20Specifically%2C%20in%20terms%20of%0Amodel%20architecture%2C%20we%20propose%20InfLLM%20v2%2C%20a%20trainable%20sparse%20attention%0Amechanism%20that%20accelerates%20both%20prefilling%20and%20decoding%20phases%20for%20long-context%0Aprocessing.%20Regarding%20training%20data%2C%20we%20propose%20UltraClean%2C%20an%20efficient%20and%0Aaccurate%20pre-training%20data%20filtering%20and%20generation%20strategy%2C%20and%20UltraChat%20v2%2C%0Aa%20comprehensive%20supervised%20fine-tuning%20dataset.%20These%20datasets%20enable%0Asatisfactory%20model%20performance%20to%20be%20achieved%20using%20just%208%20trillion%20training%0Atokens.%20Regarding%20training%20algorithms%2C%20we%20propose%20ModelTunnel%20v2%20for%20efficient%0Apre-training%20strategy%20search%2C%20and%20improve%20existing%20post-training%20methods%20by%0Aintroducing%20chunk-wise%20rollout%20for%20load-balanced%20reinforcement%20learning%20and%0Adata-efficient%20tenary%20LLM%2C%20BitCPM.%20Regarding%20inference%20systems%2C%20we%20propose%0ACPM.cu%20that%20integrates%20sparse%20attention%2C%20model%20quantization%2C%20and%20speculative%0Asampling%20to%20achieve%20efficient%20prefilling%20and%20decoding.%20To%20meet%20diverse%0Aon-device%20requirements%2C%20MiniCPM4%20is%20available%20in%20two%20versions%2C%20with%200.5B%20and%208B%0Aparameters%2C%20respectively.%20Furthermore%2C%20we%20construct%20a%20hybrid%20reasoning%20model%2C%0AMiniCPM4.1%2C%20which%20can%20be%20used%20in%20both%20deep%20reasoning%20mode%20and%20non-reasoning%0Amode.%20Evaluation%20results%20demonstrate%20that%20MiniCPM4%20and%20MiniCPM4.1%20outperform%0Asimilar-sized%20open-source%20models%20across%20benchmarks%2C%20with%20the%208B%20variants%0Ashowing%20significant%20speed%20improvements%20on%20long%20sequence%20understanding%20and%0Ageneration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07900v2&entry.124074799=Read"},
{"title": "Towards Cognitively-Faithful Decision-Making Models to Improve AI\n  Alignment", "author": "Cyrus Cousins and Vijay Keswani and Vincent Conitzer and Hoda Heidari and Jana Schaich Borg and Walter Sinnott-Armstrong", "abstract": "  Recent AI work trends towards incorporating human-centric objectives, with\nthe explicit goal of aligning AI models to personal preferences and societal\nvalues. Using standard preference elicitation methods, researchers and\npractitioners build models of human decisions and judgments, which are then\nused to align AI behavior with that of humans. However, models commonly used in\nsuch elicitation processes often do not capture the true cognitive processes of\nhuman decision making, such as when people use heuristics to simplify\ninformation associated with a decision problem. As a result, models learned\nfrom people's decisions often do not align with their cognitive processes, and\ncan not be used to validate the learning framework for generalization to other\ndecision-making tasks. To address this limitation, we take an axiomatic\napproach to learning cognitively faithful decision processes from pairwise\ncomparisons. Building on the vast literature characterizing the cognitive\nprocesses that contribute to human decision-making, and recent work\ncharacterizing such processes in pairwise comparison tasks, we define a class\nof models in which individual features are first processed and compared across\nalternatives, and then the processed features are then aggregated via a fixed\nrule, such as the Bradley-Terry rule. This structured processing of information\nensures such models are realistic and feasible candidates to represent\nunderlying human decision-making processes. We demonstrate the efficacy of this\nmodeling approach in learning interpretable models of human decision making in\na kidney allocation task, and show that our proposed models match or surpass\nthe accuracy of prior models of human pairwise decision-making.\n", "link": "http://arxiv.org/abs/2509.04445v1", "date": "2025-09-04", "relevancy": 1.951, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5096}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5004}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Cognitively-Faithful%20Decision-Making%20Models%20to%20Improve%20AI%0A%20%20Alignment&body=Title%3A%20Towards%20Cognitively-Faithful%20Decision-Making%20Models%20to%20Improve%20AI%0A%20%20Alignment%0AAuthor%3A%20Cyrus%20Cousins%20and%20Vijay%20Keswani%20and%20Vincent%20Conitzer%20and%20Hoda%20Heidari%20and%20Jana%20Schaich%20Borg%20and%20Walter%20Sinnott-Armstrong%0AAbstract%3A%20%20%20Recent%20AI%20work%20trends%20towards%20incorporating%20human-centric%20objectives%2C%20with%0Athe%20explicit%20goal%20of%20aligning%20AI%20models%20to%20personal%20preferences%20and%20societal%0Avalues.%20Using%20standard%20preference%20elicitation%20methods%2C%20researchers%20and%0Apractitioners%20build%20models%20of%20human%20decisions%20and%20judgments%2C%20which%20are%20then%0Aused%20to%20align%20AI%20behavior%20with%20that%20of%20humans.%20However%2C%20models%20commonly%20used%20in%0Asuch%20elicitation%20processes%20often%20do%20not%20capture%20the%20true%20cognitive%20processes%20of%0Ahuman%20decision%20making%2C%20such%20as%20when%20people%20use%20heuristics%20to%20simplify%0Ainformation%20associated%20with%20a%20decision%20problem.%20As%20a%20result%2C%20models%20learned%0Afrom%20people%27s%20decisions%20often%20do%20not%20align%20with%20their%20cognitive%20processes%2C%20and%0Acan%20not%20be%20used%20to%20validate%20the%20learning%20framework%20for%20generalization%20to%20other%0Adecision-making%20tasks.%20To%20address%20this%20limitation%2C%20we%20take%20an%20axiomatic%0Aapproach%20to%20learning%20cognitively%20faithful%20decision%20processes%20from%20pairwise%0Acomparisons.%20Building%20on%20the%20vast%20literature%20characterizing%20the%20cognitive%0Aprocesses%20that%20contribute%20to%20human%20decision-making%2C%20and%20recent%20work%0Acharacterizing%20such%20processes%20in%20pairwise%20comparison%20tasks%2C%20we%20define%20a%20class%0Aof%20models%20in%20which%20individual%20features%20are%20first%20processed%20and%20compared%20across%0Aalternatives%2C%20and%20then%20the%20processed%20features%20are%20then%20aggregated%20via%20a%20fixed%0Arule%2C%20such%20as%20the%20Bradley-Terry%20rule.%20This%20structured%20processing%20of%20information%0Aensures%20such%20models%20are%20realistic%20and%20feasible%20candidates%20to%20represent%0Aunderlying%20human%20decision-making%20processes.%20We%20demonstrate%20the%20efficacy%20of%20this%0Amodeling%20approach%20in%20learning%20interpretable%20models%20of%20human%20decision%20making%20in%0Aa%20kidney%20allocation%20task%2C%20and%20show%20that%20our%20proposed%20models%20match%20or%20surpass%0Athe%20accuracy%20of%20prior%20models%20of%20human%20pairwise%20decision-making.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04445v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Cognitively-Faithful%2520Decision-Making%2520Models%2520to%2520Improve%2520AI%250A%2520%2520Alignment%26entry.906535625%3DCyrus%2520Cousins%2520and%2520Vijay%2520Keswani%2520and%2520Vincent%2520Conitzer%2520and%2520Hoda%2520Heidari%2520and%2520Jana%2520Schaich%2520Borg%2520and%2520Walter%2520Sinnott-Armstrong%26entry.1292438233%3D%2520%2520Recent%2520AI%2520work%2520trends%2520towards%2520incorporating%2520human-centric%2520objectives%252C%2520with%250Athe%2520explicit%2520goal%2520of%2520aligning%2520AI%2520models%2520to%2520personal%2520preferences%2520and%2520societal%250Avalues.%2520Using%2520standard%2520preference%2520elicitation%2520methods%252C%2520researchers%2520and%250Apractitioners%2520build%2520models%2520of%2520human%2520decisions%2520and%2520judgments%252C%2520which%2520are%2520then%250Aused%2520to%2520align%2520AI%2520behavior%2520with%2520that%2520of%2520humans.%2520However%252C%2520models%2520commonly%2520used%2520in%250Asuch%2520elicitation%2520processes%2520often%2520do%2520not%2520capture%2520the%2520true%2520cognitive%2520processes%2520of%250Ahuman%2520decision%2520making%252C%2520such%2520as%2520when%2520people%2520use%2520heuristics%2520to%2520simplify%250Ainformation%2520associated%2520with%2520a%2520decision%2520problem.%2520As%2520a%2520result%252C%2520models%2520learned%250Afrom%2520people%2527s%2520decisions%2520often%2520do%2520not%2520align%2520with%2520their%2520cognitive%2520processes%252C%2520and%250Acan%2520not%2520be%2520used%2520to%2520validate%2520the%2520learning%2520framework%2520for%2520generalization%2520to%2520other%250Adecision-making%2520tasks.%2520To%2520address%2520this%2520limitation%252C%2520we%2520take%2520an%2520axiomatic%250Aapproach%2520to%2520learning%2520cognitively%2520faithful%2520decision%2520processes%2520from%2520pairwise%250Acomparisons.%2520Building%2520on%2520the%2520vast%2520literature%2520characterizing%2520the%2520cognitive%250Aprocesses%2520that%2520contribute%2520to%2520human%2520decision-making%252C%2520and%2520recent%2520work%250Acharacterizing%2520such%2520processes%2520in%2520pairwise%2520comparison%2520tasks%252C%2520we%2520define%2520a%2520class%250Aof%2520models%2520in%2520which%2520individual%2520features%2520are%2520first%2520processed%2520and%2520compared%2520across%250Aalternatives%252C%2520and%2520then%2520the%2520processed%2520features%2520are%2520then%2520aggregated%2520via%2520a%2520fixed%250Arule%252C%2520such%2520as%2520the%2520Bradley-Terry%2520rule.%2520This%2520structured%2520processing%2520of%2520information%250Aensures%2520such%2520models%2520are%2520realistic%2520and%2520feasible%2520candidates%2520to%2520represent%250Aunderlying%2520human%2520decision-making%2520processes.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520this%250Amodeling%2520approach%2520in%2520learning%2520interpretable%2520models%2520of%2520human%2520decision%2520making%2520in%250Aa%2520kidney%2520allocation%2520task%252C%2520and%2520show%2520that%2520our%2520proposed%2520models%2520match%2520or%2520surpass%250Athe%2520accuracy%2520of%2520prior%2520models%2520of%2520human%2520pairwise%2520decision-making.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04445v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Cognitively-Faithful%20Decision-Making%20Models%20to%20Improve%20AI%0A%20%20Alignment&entry.906535625=Cyrus%20Cousins%20and%20Vijay%20Keswani%20and%20Vincent%20Conitzer%20and%20Hoda%20Heidari%20and%20Jana%20Schaich%20Borg%20and%20Walter%20Sinnott-Armstrong&entry.1292438233=%20%20Recent%20AI%20work%20trends%20towards%20incorporating%20human-centric%20objectives%2C%20with%0Athe%20explicit%20goal%20of%20aligning%20AI%20models%20to%20personal%20preferences%20and%20societal%0Avalues.%20Using%20standard%20preference%20elicitation%20methods%2C%20researchers%20and%0Apractitioners%20build%20models%20of%20human%20decisions%20and%20judgments%2C%20which%20are%20then%0Aused%20to%20align%20AI%20behavior%20with%20that%20of%20humans.%20However%2C%20models%20commonly%20used%20in%0Asuch%20elicitation%20processes%20often%20do%20not%20capture%20the%20true%20cognitive%20processes%20of%0Ahuman%20decision%20making%2C%20such%20as%20when%20people%20use%20heuristics%20to%20simplify%0Ainformation%20associated%20with%20a%20decision%20problem.%20As%20a%20result%2C%20models%20learned%0Afrom%20people%27s%20decisions%20often%20do%20not%20align%20with%20their%20cognitive%20processes%2C%20and%0Acan%20not%20be%20used%20to%20validate%20the%20learning%20framework%20for%20generalization%20to%20other%0Adecision-making%20tasks.%20To%20address%20this%20limitation%2C%20we%20take%20an%20axiomatic%0Aapproach%20to%20learning%20cognitively%20faithful%20decision%20processes%20from%20pairwise%0Acomparisons.%20Building%20on%20the%20vast%20literature%20characterizing%20the%20cognitive%0Aprocesses%20that%20contribute%20to%20human%20decision-making%2C%20and%20recent%20work%0Acharacterizing%20such%20processes%20in%20pairwise%20comparison%20tasks%2C%20we%20define%20a%20class%0Aof%20models%20in%20which%20individual%20features%20are%20first%20processed%20and%20compared%20across%0Aalternatives%2C%20and%20then%20the%20processed%20features%20are%20then%20aggregated%20via%20a%20fixed%0Arule%2C%20such%20as%20the%20Bradley-Terry%20rule.%20This%20structured%20processing%20of%20information%0Aensures%20such%20models%20are%20realistic%20and%20feasible%20candidates%20to%20represent%0Aunderlying%20human%20decision-making%20processes.%20We%20demonstrate%20the%20efficacy%20of%20this%0Amodeling%20approach%20in%20learning%20interpretable%20models%20of%20human%20decision%20making%20in%0Aa%20kidney%20allocation%20task%2C%20and%20show%20that%20our%20proposed%20models%20match%20or%20surpass%0Athe%20accuracy%20of%20prior%20models%20of%20human%20pairwise%20decision-making.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04445v1&entry.124074799=Read"},
{"title": "Classification of Vision-Based Tactile Sensors: A Review", "author": "Haoran Li and Yijiong Lin and Chenghua Lu and Max Yang and Efi Psomopoulou and Nathan F Lepora", "abstract": "  Vision-based tactile sensors (VBTS) have gained widespread application in\nrobotic hands, grippers and prosthetics due to their high spatial resolution,\nlow manufacturing costs, and ease of customization. While VBTSs have common\ndesign features, such as a camera module, they can differ in a rich diversity\nof sensing principles, material compositions, multimodal approaches, and data\ninterpretation methods. Here, we propose a novel classification of VBTS that\ncategorizes the technology into two primary sensing principles based on the\nunderlying transduction of contact into a tactile image: the Marker-Based\nTransduction Principle and the Intensity-Based Transduction Principle.\nMarker-Based Transduction interprets tactile information by detecting marker\ndisplacement and changes in marker density. In contrast, Intensity-Based\nTransduction maps external disturbances with variations in pixel values.\nDepending on the design of the contact module, Marker-Based Transduction can be\nfurther divided into two subtypes: Simple Marker-Based (SMB) and Morphological\nMarker-Based (MMB) mechanisms. Similarly, the Intensity-Based Transduction\nPrinciple encompasses the Reflective Layer-based (RLB) and Transparent\nLayer-Based (TLB) mechanisms. This paper provides a comparative study of the\nhardware characteristics of these four types of sensors including various\ncombination types, and discusses the commonly used methods for interpreting\ntactile information. This~comparison reveals some current challenges faced by\nVBTS technology and directions for future research.\n", "link": "http://arxiv.org/abs/2509.02478v2", "date": "2025-09-04", "relevancy": 1.9466, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5567}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5055}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Classification%20of%20Vision-Based%20Tactile%20Sensors%3A%20A%20Review&body=Title%3A%20Classification%20of%20Vision-Based%20Tactile%20Sensors%3A%20A%20Review%0AAuthor%3A%20Haoran%20Li%20and%20Yijiong%20Lin%20and%20Chenghua%20Lu%20and%20Max%20Yang%20and%20Efi%20Psomopoulou%20and%20Nathan%20F%20Lepora%0AAbstract%3A%20%20%20Vision-based%20tactile%20sensors%20%28VBTS%29%20have%20gained%20widespread%20application%20in%0Arobotic%20hands%2C%20grippers%20and%20prosthetics%20due%20to%20their%20high%20spatial%20resolution%2C%0Alow%20manufacturing%20costs%2C%20and%20ease%20of%20customization.%20While%20VBTSs%20have%20common%0Adesign%20features%2C%20such%20as%20a%20camera%20module%2C%20they%20can%20differ%20in%20a%20rich%20diversity%0Aof%20sensing%20principles%2C%20material%20compositions%2C%20multimodal%20approaches%2C%20and%20data%0Ainterpretation%20methods.%20Here%2C%20we%20propose%20a%20novel%20classification%20of%20VBTS%20that%0Acategorizes%20the%20technology%20into%20two%20primary%20sensing%20principles%20based%20on%20the%0Aunderlying%20transduction%20of%20contact%20into%20a%20tactile%20image%3A%20the%20Marker-Based%0ATransduction%20Principle%20and%20the%20Intensity-Based%20Transduction%20Principle.%0AMarker-Based%20Transduction%20interprets%20tactile%20information%20by%20detecting%20marker%0Adisplacement%20and%20changes%20in%20marker%20density.%20In%20contrast%2C%20Intensity-Based%0ATransduction%20maps%20external%20disturbances%20with%20variations%20in%20pixel%20values.%0ADepending%20on%20the%20design%20of%20the%20contact%20module%2C%20Marker-Based%20Transduction%20can%20be%0Afurther%20divided%20into%20two%20subtypes%3A%20Simple%20Marker-Based%20%28SMB%29%20and%20Morphological%0AMarker-Based%20%28MMB%29%20mechanisms.%20Similarly%2C%20the%20Intensity-Based%20Transduction%0APrinciple%20encompasses%20the%20Reflective%20Layer-based%20%28RLB%29%20and%20Transparent%0ALayer-Based%20%28TLB%29%20mechanisms.%20This%20paper%20provides%20a%20comparative%20study%20of%20the%0Ahardware%20characteristics%20of%20these%20four%20types%20of%20sensors%20including%20various%0Acombination%20types%2C%20and%20discusses%20the%20commonly%20used%20methods%20for%20interpreting%0Atactile%20information.%20This~comparison%20reveals%20some%20current%20challenges%20faced%20by%0AVBTS%20technology%20and%20directions%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.02478v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClassification%2520of%2520Vision-Based%2520Tactile%2520Sensors%253A%2520A%2520Review%26entry.906535625%3DHaoran%2520Li%2520and%2520Yijiong%2520Lin%2520and%2520Chenghua%2520Lu%2520and%2520Max%2520Yang%2520and%2520Efi%2520Psomopoulou%2520and%2520Nathan%2520F%2520Lepora%26entry.1292438233%3D%2520%2520Vision-based%2520tactile%2520sensors%2520%2528VBTS%2529%2520have%2520gained%2520widespread%2520application%2520in%250Arobotic%2520hands%252C%2520grippers%2520and%2520prosthetics%2520due%2520to%2520their%2520high%2520spatial%2520resolution%252C%250Alow%2520manufacturing%2520costs%252C%2520and%2520ease%2520of%2520customization.%2520While%2520VBTSs%2520have%2520common%250Adesign%2520features%252C%2520such%2520as%2520a%2520camera%2520module%252C%2520they%2520can%2520differ%2520in%2520a%2520rich%2520diversity%250Aof%2520sensing%2520principles%252C%2520material%2520compositions%252C%2520multimodal%2520approaches%252C%2520and%2520data%250Ainterpretation%2520methods.%2520Here%252C%2520we%2520propose%2520a%2520novel%2520classification%2520of%2520VBTS%2520that%250Acategorizes%2520the%2520technology%2520into%2520two%2520primary%2520sensing%2520principles%2520based%2520on%2520the%250Aunderlying%2520transduction%2520of%2520contact%2520into%2520a%2520tactile%2520image%253A%2520the%2520Marker-Based%250ATransduction%2520Principle%2520and%2520the%2520Intensity-Based%2520Transduction%2520Principle.%250AMarker-Based%2520Transduction%2520interprets%2520tactile%2520information%2520by%2520detecting%2520marker%250Adisplacement%2520and%2520changes%2520in%2520marker%2520density.%2520In%2520contrast%252C%2520Intensity-Based%250ATransduction%2520maps%2520external%2520disturbances%2520with%2520variations%2520in%2520pixel%2520values.%250ADepending%2520on%2520the%2520design%2520of%2520the%2520contact%2520module%252C%2520Marker-Based%2520Transduction%2520can%2520be%250Afurther%2520divided%2520into%2520two%2520subtypes%253A%2520Simple%2520Marker-Based%2520%2528SMB%2529%2520and%2520Morphological%250AMarker-Based%2520%2528MMB%2529%2520mechanisms.%2520Similarly%252C%2520the%2520Intensity-Based%2520Transduction%250APrinciple%2520encompasses%2520the%2520Reflective%2520Layer-based%2520%2528RLB%2529%2520and%2520Transparent%250ALayer-Based%2520%2528TLB%2529%2520mechanisms.%2520This%2520paper%2520provides%2520a%2520comparative%2520study%2520of%2520the%250Ahardware%2520characteristics%2520of%2520these%2520four%2520types%2520of%2520sensors%2520including%2520various%250Acombination%2520types%252C%2520and%2520discusses%2520the%2520commonly%2520used%2520methods%2520for%2520interpreting%250Atactile%2520information.%2520This~comparison%2520reveals%2520some%2520current%2520challenges%2520faced%2520by%250AVBTS%2520technology%2520and%2520directions%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.02478v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Classification%20of%20Vision-Based%20Tactile%20Sensors%3A%20A%20Review&entry.906535625=Haoran%20Li%20and%20Yijiong%20Lin%20and%20Chenghua%20Lu%20and%20Max%20Yang%20and%20Efi%20Psomopoulou%20and%20Nathan%20F%20Lepora&entry.1292438233=%20%20Vision-based%20tactile%20sensors%20%28VBTS%29%20have%20gained%20widespread%20application%20in%0Arobotic%20hands%2C%20grippers%20and%20prosthetics%20due%20to%20their%20high%20spatial%20resolution%2C%0Alow%20manufacturing%20costs%2C%20and%20ease%20of%20customization.%20While%20VBTSs%20have%20common%0Adesign%20features%2C%20such%20as%20a%20camera%20module%2C%20they%20can%20differ%20in%20a%20rich%20diversity%0Aof%20sensing%20principles%2C%20material%20compositions%2C%20multimodal%20approaches%2C%20and%20data%0Ainterpretation%20methods.%20Here%2C%20we%20propose%20a%20novel%20classification%20of%20VBTS%20that%0Acategorizes%20the%20technology%20into%20two%20primary%20sensing%20principles%20based%20on%20the%0Aunderlying%20transduction%20of%20contact%20into%20a%20tactile%20image%3A%20the%20Marker-Based%0ATransduction%20Principle%20and%20the%20Intensity-Based%20Transduction%20Principle.%0AMarker-Based%20Transduction%20interprets%20tactile%20information%20by%20detecting%20marker%0Adisplacement%20and%20changes%20in%20marker%20density.%20In%20contrast%2C%20Intensity-Based%0ATransduction%20maps%20external%20disturbances%20with%20variations%20in%20pixel%20values.%0ADepending%20on%20the%20design%20of%20the%20contact%20module%2C%20Marker-Based%20Transduction%20can%20be%0Afurther%20divided%20into%20two%20subtypes%3A%20Simple%20Marker-Based%20%28SMB%29%20and%20Morphological%0AMarker-Based%20%28MMB%29%20mechanisms.%20Similarly%2C%20the%20Intensity-Based%20Transduction%0APrinciple%20encompasses%20the%20Reflective%20Layer-based%20%28RLB%29%20and%20Transparent%0ALayer-Based%20%28TLB%29%20mechanisms.%20This%20paper%20provides%20a%20comparative%20study%20of%20the%0Ahardware%20characteristics%20of%20these%20four%20types%20of%20sensors%20including%20various%0Acombination%20types%2C%20and%20discusses%20the%20commonly%20used%20methods%20for%20interpreting%0Atactile%20information.%20This~comparison%20reveals%20some%20current%20challenges%20faced%20by%0AVBTS%20technology%20and%20directions%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.02478v2&entry.124074799=Read"},
{"title": "AUDETER: A Large-scale Dataset for Deepfake Audio Detection in Open\n  Worlds", "author": "Qizhou Wang and Hanxun Huang and Guansong Pang and Sarah Erfani and Christopher Leckie", "abstract": "  Speech generation systems can produce remarkably realistic vocalisations that\nare often indistinguishable from human speech, posing significant authenticity\nchallenges. Although numerous deepfake detection methods have been developed,\ntheir effectiveness in real-world environments remains unrealiable due to the\ndomain shift between training and test samples arising from diverse human\nspeech and fast evolving speech synthesis systems. This is not adequately\naddressed by current datasets, which lack real-world application challenges\nwith diverse and up-to-date audios in both real and deep-fake categories. To\nfill this gap, we introduce AUDETER (AUdio DEepfake TEst Range), a large-scale,\nhighly diverse deepfake audio dataset for comprehensive evaluation and robust\ndevelopment of generalised models for deepfake audio detection. It consists of\nover 4,500 hours of synthetic audio generated by 11 recent TTS models and 10\nvocoders with a broad range of TTS/vocoder patterns, totalling 3 million audio\nclips, making it the largest deepfake audio dataset by scale. Through extensive\nexperiments with AUDETER, we reveal that i) state-of-the-art (SOTA) methods\ntrained on existing datasets struggle to generalise to novel deepfake audio\nsamples and suffer from high false positive rates on unseen human voice,\nunderscoring the need for a comprehensive dataset; and ii) these methods\ntrained on AUDETER achieve highly generalised detection performance and\nsignificantly reduce detection error rate by 44.1% to 51.6%, achieving an error\nrate of only 4.17% on diverse cross-domain samples in the popular In-the-Wild\ndataset, paving the way for training generalist deepfake audio detectors.\nAUDETER is available on GitHub.\n", "link": "http://arxiv.org/abs/2509.04345v1", "date": "2025-09-04", "relevancy": 1.9079, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5164}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4762}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AUDETER%3A%20A%20Large-scale%20Dataset%20for%20Deepfake%20Audio%20Detection%20in%20Open%0A%20%20Worlds&body=Title%3A%20AUDETER%3A%20A%20Large-scale%20Dataset%20for%20Deepfake%20Audio%20Detection%20in%20Open%0A%20%20Worlds%0AAuthor%3A%20Qizhou%20Wang%20and%20Hanxun%20Huang%20and%20Guansong%20Pang%20and%20Sarah%20Erfani%20and%20Christopher%20Leckie%0AAbstract%3A%20%20%20Speech%20generation%20systems%20can%20produce%20remarkably%20realistic%20vocalisations%20that%0Aare%20often%20indistinguishable%20from%20human%20speech%2C%20posing%20significant%20authenticity%0Achallenges.%20Although%20numerous%20deepfake%20detection%20methods%20have%20been%20developed%2C%0Atheir%20effectiveness%20in%20real-world%20environments%20remains%20unrealiable%20due%20to%20the%0Adomain%20shift%20between%20training%20and%20test%20samples%20arising%20from%20diverse%20human%0Aspeech%20and%20fast%20evolving%20speech%20synthesis%20systems.%20This%20is%20not%20adequately%0Aaddressed%20by%20current%20datasets%2C%20which%20lack%20real-world%20application%20challenges%0Awith%20diverse%20and%20up-to-date%20audios%20in%20both%20real%20and%20deep-fake%20categories.%20To%0Afill%20this%20gap%2C%20we%20introduce%20AUDETER%20%28AUdio%20DEepfake%20TEst%20Range%29%2C%20a%20large-scale%2C%0Ahighly%20diverse%20deepfake%20audio%20dataset%20for%20comprehensive%20evaluation%20and%20robust%0Adevelopment%20of%20generalised%20models%20for%20deepfake%20audio%20detection.%20It%20consists%20of%0Aover%204%2C500%20hours%20of%20synthetic%20audio%20generated%20by%2011%20recent%20TTS%20models%20and%2010%0Avocoders%20with%20a%20broad%20range%20of%20TTS/vocoder%20patterns%2C%20totalling%203%20million%20audio%0Aclips%2C%20making%20it%20the%20largest%20deepfake%20audio%20dataset%20by%20scale.%20Through%20extensive%0Aexperiments%20with%20AUDETER%2C%20we%20reveal%20that%20i%29%20state-of-the-art%20%28SOTA%29%20methods%0Atrained%20on%20existing%20datasets%20struggle%20to%20generalise%20to%20novel%20deepfake%20audio%0Asamples%20and%20suffer%20from%20high%20false%20positive%20rates%20on%20unseen%20human%20voice%2C%0Aunderscoring%20the%20need%20for%20a%20comprehensive%20dataset%3B%20and%20ii%29%20these%20methods%0Atrained%20on%20AUDETER%20achieve%20highly%20generalised%20detection%20performance%20and%0Asignificantly%20reduce%20detection%20error%20rate%20by%2044.1%25%20to%2051.6%25%2C%20achieving%20an%20error%0Arate%20of%20only%204.17%25%20on%20diverse%20cross-domain%20samples%20in%20the%20popular%20In-the-Wild%0Adataset%2C%20paving%20the%20way%20for%20training%20generalist%20deepfake%20audio%20detectors.%0AAUDETER%20is%20available%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04345v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAUDETER%253A%2520A%2520Large-scale%2520Dataset%2520for%2520Deepfake%2520Audio%2520Detection%2520in%2520Open%250A%2520%2520Worlds%26entry.906535625%3DQizhou%2520Wang%2520and%2520Hanxun%2520Huang%2520and%2520Guansong%2520Pang%2520and%2520Sarah%2520Erfani%2520and%2520Christopher%2520Leckie%26entry.1292438233%3D%2520%2520Speech%2520generation%2520systems%2520can%2520produce%2520remarkably%2520realistic%2520vocalisations%2520that%250Aare%2520often%2520indistinguishable%2520from%2520human%2520speech%252C%2520posing%2520significant%2520authenticity%250Achallenges.%2520Although%2520numerous%2520deepfake%2520detection%2520methods%2520have%2520been%2520developed%252C%250Atheir%2520effectiveness%2520in%2520real-world%2520environments%2520remains%2520unrealiable%2520due%2520to%2520the%250Adomain%2520shift%2520between%2520training%2520and%2520test%2520samples%2520arising%2520from%2520diverse%2520human%250Aspeech%2520and%2520fast%2520evolving%2520speech%2520synthesis%2520systems.%2520This%2520is%2520not%2520adequately%250Aaddressed%2520by%2520current%2520datasets%252C%2520which%2520lack%2520real-world%2520application%2520challenges%250Awith%2520diverse%2520and%2520up-to-date%2520audios%2520in%2520both%2520real%2520and%2520deep-fake%2520categories.%2520To%250Afill%2520this%2520gap%252C%2520we%2520introduce%2520AUDETER%2520%2528AUdio%2520DEepfake%2520TEst%2520Range%2529%252C%2520a%2520large-scale%252C%250Ahighly%2520diverse%2520deepfake%2520audio%2520dataset%2520for%2520comprehensive%2520evaluation%2520and%2520robust%250Adevelopment%2520of%2520generalised%2520models%2520for%2520deepfake%2520audio%2520detection.%2520It%2520consists%2520of%250Aover%25204%252C500%2520hours%2520of%2520synthetic%2520audio%2520generated%2520by%252011%2520recent%2520TTS%2520models%2520and%252010%250Avocoders%2520with%2520a%2520broad%2520range%2520of%2520TTS/vocoder%2520patterns%252C%2520totalling%25203%2520million%2520audio%250Aclips%252C%2520making%2520it%2520the%2520largest%2520deepfake%2520audio%2520dataset%2520by%2520scale.%2520Through%2520extensive%250Aexperiments%2520with%2520AUDETER%252C%2520we%2520reveal%2520that%2520i%2529%2520state-of-the-art%2520%2528SOTA%2529%2520methods%250Atrained%2520on%2520existing%2520datasets%2520struggle%2520to%2520generalise%2520to%2520novel%2520deepfake%2520audio%250Asamples%2520and%2520suffer%2520from%2520high%2520false%2520positive%2520rates%2520on%2520unseen%2520human%2520voice%252C%250Aunderscoring%2520the%2520need%2520for%2520a%2520comprehensive%2520dataset%253B%2520and%2520ii%2529%2520these%2520methods%250Atrained%2520on%2520AUDETER%2520achieve%2520highly%2520generalised%2520detection%2520performance%2520and%250Asignificantly%2520reduce%2520detection%2520error%2520rate%2520by%252044.1%2525%2520to%252051.6%2525%252C%2520achieving%2520an%2520error%250Arate%2520of%2520only%25204.17%2525%2520on%2520diverse%2520cross-domain%2520samples%2520in%2520the%2520popular%2520In-the-Wild%250Adataset%252C%2520paving%2520the%2520way%2520for%2520training%2520generalist%2520deepfake%2520audio%2520detectors.%250AAUDETER%2520is%2520available%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04345v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AUDETER%3A%20A%20Large-scale%20Dataset%20for%20Deepfake%20Audio%20Detection%20in%20Open%0A%20%20Worlds&entry.906535625=Qizhou%20Wang%20and%20Hanxun%20Huang%20and%20Guansong%20Pang%20and%20Sarah%20Erfani%20and%20Christopher%20Leckie&entry.1292438233=%20%20Speech%20generation%20systems%20can%20produce%20remarkably%20realistic%20vocalisations%20that%0Aare%20often%20indistinguishable%20from%20human%20speech%2C%20posing%20significant%20authenticity%0Achallenges.%20Although%20numerous%20deepfake%20detection%20methods%20have%20been%20developed%2C%0Atheir%20effectiveness%20in%20real-world%20environments%20remains%20unrealiable%20due%20to%20the%0Adomain%20shift%20between%20training%20and%20test%20samples%20arising%20from%20diverse%20human%0Aspeech%20and%20fast%20evolving%20speech%20synthesis%20systems.%20This%20is%20not%20adequately%0Aaddressed%20by%20current%20datasets%2C%20which%20lack%20real-world%20application%20challenges%0Awith%20diverse%20and%20up-to-date%20audios%20in%20both%20real%20and%20deep-fake%20categories.%20To%0Afill%20this%20gap%2C%20we%20introduce%20AUDETER%20%28AUdio%20DEepfake%20TEst%20Range%29%2C%20a%20large-scale%2C%0Ahighly%20diverse%20deepfake%20audio%20dataset%20for%20comprehensive%20evaluation%20and%20robust%0Adevelopment%20of%20generalised%20models%20for%20deepfake%20audio%20detection.%20It%20consists%20of%0Aover%204%2C500%20hours%20of%20synthetic%20audio%20generated%20by%2011%20recent%20TTS%20models%20and%2010%0Avocoders%20with%20a%20broad%20range%20of%20TTS/vocoder%20patterns%2C%20totalling%203%20million%20audio%0Aclips%2C%20making%20it%20the%20largest%20deepfake%20audio%20dataset%20by%20scale.%20Through%20extensive%0Aexperiments%20with%20AUDETER%2C%20we%20reveal%20that%20i%29%20state-of-the-art%20%28SOTA%29%20methods%0Atrained%20on%20existing%20datasets%20struggle%20to%20generalise%20to%20novel%20deepfake%20audio%0Asamples%20and%20suffer%20from%20high%20false%20positive%20rates%20on%20unseen%20human%20voice%2C%0Aunderscoring%20the%20need%20for%20a%20comprehensive%20dataset%3B%20and%20ii%29%20these%20methods%0Atrained%20on%20AUDETER%20achieve%20highly%20generalised%20detection%20performance%20and%0Asignificantly%20reduce%20detection%20error%20rate%20by%2044.1%25%20to%2051.6%25%2C%20achieving%20an%20error%0Arate%20of%20only%204.17%25%20on%20diverse%20cross-domain%20samples%20in%20the%20popular%20In-the-Wild%0Adataset%2C%20paving%20the%20way%20for%20training%20generalist%20deepfake%20audio%20detectors.%0AAUDETER%20is%20available%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04345v1&entry.124074799=Read"},
{"title": "FedQuad: Federated Stochastic Quadruplet Learning to Mitigate Data\n  Heterogeneity", "author": "Ozgu Goksu and Nicolas Pugeault", "abstract": "  Federated Learning (FL) provides decentralised model training, which\neffectively tackles problems such as distributed data and privacy preservation.\nHowever, the generalisation of global models frequently faces challenges from\ndata heterogeneity among clients. This challenge becomes even more pronounced\nwhen datasets are limited in size and class imbalance. To address data\nheterogeneity, we propose a novel method, \\textit{FedQuad}, that explicitly\noptimises smaller intra-class variance and larger inter-class variance across\nclients, thereby decreasing the negative impact of model aggregation on the\nglobal model over client representations. Our approach minimises the distance\nbetween similar pairs while maximising the distance between negative pairs,\neffectively disentangling client data in the shared feature space. We evaluate\nour method on the CIFAR-10 and CIFAR-100 datasets under various data\ndistributions and with many clients, demonstrating superior performance\ncompared to existing approaches. Furthermore, we provide a detailed analysis of\nmetric learning-based strategies within both supervised and federated learning\nparadigms, highlighting their efficacy in addressing representational learning\nchallenges in federated settings.\n", "link": "http://arxiv.org/abs/2509.04107v1", "date": "2025-09-04", "relevancy": 1.9216, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4874}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4761}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedQuad%3A%20Federated%20Stochastic%20Quadruplet%20Learning%20to%20Mitigate%20Data%0A%20%20Heterogeneity&body=Title%3A%20FedQuad%3A%20Federated%20Stochastic%20Quadruplet%20Learning%20to%20Mitigate%20Data%0A%20%20Heterogeneity%0AAuthor%3A%20Ozgu%20Goksu%20and%20Nicolas%20Pugeault%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20provides%20decentralised%20model%20training%2C%20which%0Aeffectively%20tackles%20problems%20such%20as%20distributed%20data%20and%20privacy%20preservation.%0AHowever%2C%20the%20generalisation%20of%20global%20models%20frequently%20faces%20challenges%20from%0Adata%20heterogeneity%20among%20clients.%20This%20challenge%20becomes%20even%20more%20pronounced%0Awhen%20datasets%20are%20limited%20in%20size%20and%20class%20imbalance.%20To%20address%20data%0Aheterogeneity%2C%20we%20propose%20a%20novel%20method%2C%20%5Ctextit%7BFedQuad%7D%2C%20that%20explicitly%0Aoptimises%20smaller%20intra-class%20variance%20and%20larger%20inter-class%20variance%20across%0Aclients%2C%20thereby%20decreasing%20the%20negative%20impact%20of%20model%20aggregation%20on%20the%0Aglobal%20model%20over%20client%20representations.%20Our%20approach%20minimises%20the%20distance%0Abetween%20similar%20pairs%20while%20maximising%20the%20distance%20between%20negative%20pairs%2C%0Aeffectively%20disentangling%20client%20data%20in%20the%20shared%20feature%20space.%20We%20evaluate%0Aour%20method%20on%20the%20CIFAR-10%20and%20CIFAR-100%20datasets%20under%20various%20data%0Adistributions%20and%20with%20many%20clients%2C%20demonstrating%20superior%20performance%0Acompared%20to%20existing%20approaches.%20Furthermore%2C%20we%20provide%20a%20detailed%20analysis%20of%0Ametric%20learning-based%20strategies%20within%20both%20supervised%20and%20federated%20learning%0Aparadigms%2C%20highlighting%20their%20efficacy%20in%20addressing%20representational%20learning%0Achallenges%20in%20federated%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04107v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedQuad%253A%2520Federated%2520Stochastic%2520Quadruplet%2520Learning%2520to%2520Mitigate%2520Data%250A%2520%2520Heterogeneity%26entry.906535625%3DOzgu%2520Goksu%2520and%2520Nicolas%2520Pugeault%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520provides%2520decentralised%2520model%2520training%252C%2520which%250Aeffectively%2520tackles%2520problems%2520such%2520as%2520distributed%2520data%2520and%2520privacy%2520preservation.%250AHowever%252C%2520the%2520generalisation%2520of%2520global%2520models%2520frequently%2520faces%2520challenges%2520from%250Adata%2520heterogeneity%2520among%2520clients.%2520This%2520challenge%2520becomes%2520even%2520more%2520pronounced%250Awhen%2520datasets%2520are%2520limited%2520in%2520size%2520and%2520class%2520imbalance.%2520To%2520address%2520data%250Aheterogeneity%252C%2520we%2520propose%2520a%2520novel%2520method%252C%2520%255Ctextit%257BFedQuad%257D%252C%2520that%2520explicitly%250Aoptimises%2520smaller%2520intra-class%2520variance%2520and%2520larger%2520inter-class%2520variance%2520across%250Aclients%252C%2520thereby%2520decreasing%2520the%2520negative%2520impact%2520of%2520model%2520aggregation%2520on%2520the%250Aglobal%2520model%2520over%2520client%2520representations.%2520Our%2520approach%2520minimises%2520the%2520distance%250Abetween%2520similar%2520pairs%2520while%2520maximising%2520the%2520distance%2520between%2520negative%2520pairs%252C%250Aeffectively%2520disentangling%2520client%2520data%2520in%2520the%2520shared%2520feature%2520space.%2520We%2520evaluate%250Aour%2520method%2520on%2520the%2520CIFAR-10%2520and%2520CIFAR-100%2520datasets%2520under%2520various%2520data%250Adistributions%2520and%2520with%2520many%2520clients%252C%2520demonstrating%2520superior%2520performance%250Acompared%2520to%2520existing%2520approaches.%2520Furthermore%252C%2520we%2520provide%2520a%2520detailed%2520analysis%2520of%250Ametric%2520learning-based%2520strategies%2520within%2520both%2520supervised%2520and%2520federated%2520learning%250Aparadigms%252C%2520highlighting%2520their%2520efficacy%2520in%2520addressing%2520representational%2520learning%250Achallenges%2520in%2520federated%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04107v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedQuad%3A%20Federated%20Stochastic%20Quadruplet%20Learning%20to%20Mitigate%20Data%0A%20%20Heterogeneity&entry.906535625=Ozgu%20Goksu%20and%20Nicolas%20Pugeault&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20provides%20decentralised%20model%20training%2C%20which%0Aeffectively%20tackles%20problems%20such%20as%20distributed%20data%20and%20privacy%20preservation.%0AHowever%2C%20the%20generalisation%20of%20global%20models%20frequently%20faces%20challenges%20from%0Adata%20heterogeneity%20among%20clients.%20This%20challenge%20becomes%20even%20more%20pronounced%0Awhen%20datasets%20are%20limited%20in%20size%20and%20class%20imbalance.%20To%20address%20data%0Aheterogeneity%2C%20we%20propose%20a%20novel%20method%2C%20%5Ctextit%7BFedQuad%7D%2C%20that%20explicitly%0Aoptimises%20smaller%20intra-class%20variance%20and%20larger%20inter-class%20variance%20across%0Aclients%2C%20thereby%20decreasing%20the%20negative%20impact%20of%20model%20aggregation%20on%20the%0Aglobal%20model%20over%20client%20representations.%20Our%20approach%20minimises%20the%20distance%0Abetween%20similar%20pairs%20while%20maximising%20the%20distance%20between%20negative%20pairs%2C%0Aeffectively%20disentangling%20client%20data%20in%20the%20shared%20feature%20space.%20We%20evaluate%0Aour%20method%20on%20the%20CIFAR-10%20and%20CIFAR-100%20datasets%20under%20various%20data%0Adistributions%20and%20with%20many%20clients%2C%20demonstrating%20superior%20performance%0Acompared%20to%20existing%20approaches.%20Furthermore%2C%20we%20provide%20a%20detailed%20analysis%20of%0Ametric%20learning-based%20strategies%20within%20both%20supervised%20and%20federated%20learning%0Aparadigms%2C%20highlighting%20their%20efficacy%20in%20addressing%20representational%20learning%0Achallenges%20in%20federated%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04107v1&entry.124074799=Read"},
{"title": "Towards an Action-Centric Ontology for Cooking Procedures Using Temporal\n  Graphs", "author": "Aarush Kumbhakern and Saransh Kumar Gupta and Lipika Dey and Partha Pratim Das", "abstract": "  Formalizing cooking procedures remains a challenging task due to their\ninherent complexity and ambiguity. We introduce an extensible domain-specific\nlanguage for representing recipes as directed action graphs, capturing\nprocesses, transfers, environments, concurrency, and compositional structure.\nOur approach enables precise, modular modeling of complex culinary workflows.\nInitial manual evaluation on a full English breakfast recipe demonstrates the\nDSL's expressiveness and suitability for future automated recipe analysis and\nexecution. This work represents initial steps towards an action-centric\nontology for cooking, using temporal graphs to enable structured machine\nunderstanding, precise interpretation, and scalable automation of culinary\nprocesses - both in home kitchens and professional culinary settings.\n", "link": "http://arxiv.org/abs/2509.04159v1", "date": "2025-09-04", "relevancy": 1.7814, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5428}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4259}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20an%20Action-Centric%20Ontology%20for%20Cooking%20Procedures%20Using%20Temporal%0A%20%20Graphs&body=Title%3A%20Towards%20an%20Action-Centric%20Ontology%20for%20Cooking%20Procedures%20Using%20Temporal%0A%20%20Graphs%0AAuthor%3A%20Aarush%20Kumbhakern%20and%20Saransh%20Kumar%20Gupta%20and%20Lipika%20Dey%20and%20Partha%20Pratim%20Das%0AAbstract%3A%20%20%20Formalizing%20cooking%20procedures%20remains%20a%20challenging%20task%20due%20to%20their%0Ainherent%20complexity%20and%20ambiguity.%20We%20introduce%20an%20extensible%20domain-specific%0Alanguage%20for%20representing%20recipes%20as%20directed%20action%20graphs%2C%20capturing%0Aprocesses%2C%20transfers%2C%20environments%2C%20concurrency%2C%20and%20compositional%20structure.%0AOur%20approach%20enables%20precise%2C%20modular%20modeling%20of%20complex%20culinary%20workflows.%0AInitial%20manual%20evaluation%20on%20a%20full%20English%20breakfast%20recipe%20demonstrates%20the%0ADSL%27s%20expressiveness%20and%20suitability%20for%20future%20automated%20recipe%20analysis%20and%0Aexecution.%20This%20work%20represents%20initial%20steps%20towards%20an%20action-centric%0Aontology%20for%20cooking%2C%20using%20temporal%20graphs%20to%20enable%20structured%20machine%0Aunderstanding%2C%20precise%20interpretation%2C%20and%20scalable%20automation%20of%20culinary%0Aprocesses%20-%20both%20in%20home%20kitchens%20and%20professional%20culinary%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04159v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520an%2520Action-Centric%2520Ontology%2520for%2520Cooking%2520Procedures%2520Using%2520Temporal%250A%2520%2520Graphs%26entry.906535625%3DAarush%2520Kumbhakern%2520and%2520Saransh%2520Kumar%2520Gupta%2520and%2520Lipika%2520Dey%2520and%2520Partha%2520Pratim%2520Das%26entry.1292438233%3D%2520%2520Formalizing%2520cooking%2520procedures%2520remains%2520a%2520challenging%2520task%2520due%2520to%2520their%250Ainherent%2520complexity%2520and%2520ambiguity.%2520We%2520introduce%2520an%2520extensible%2520domain-specific%250Alanguage%2520for%2520representing%2520recipes%2520as%2520directed%2520action%2520graphs%252C%2520capturing%250Aprocesses%252C%2520transfers%252C%2520environments%252C%2520concurrency%252C%2520and%2520compositional%2520structure.%250AOur%2520approach%2520enables%2520precise%252C%2520modular%2520modeling%2520of%2520complex%2520culinary%2520workflows.%250AInitial%2520manual%2520evaluation%2520on%2520a%2520full%2520English%2520breakfast%2520recipe%2520demonstrates%2520the%250ADSL%2527s%2520expressiveness%2520and%2520suitability%2520for%2520future%2520automated%2520recipe%2520analysis%2520and%250Aexecution.%2520This%2520work%2520represents%2520initial%2520steps%2520towards%2520an%2520action-centric%250Aontology%2520for%2520cooking%252C%2520using%2520temporal%2520graphs%2520to%2520enable%2520structured%2520machine%250Aunderstanding%252C%2520precise%2520interpretation%252C%2520and%2520scalable%2520automation%2520of%2520culinary%250Aprocesses%2520-%2520both%2520in%2520home%2520kitchens%2520and%2520professional%2520culinary%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04159v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20an%20Action-Centric%20Ontology%20for%20Cooking%20Procedures%20Using%20Temporal%0A%20%20Graphs&entry.906535625=Aarush%20Kumbhakern%20and%20Saransh%20Kumar%20Gupta%20and%20Lipika%20Dey%20and%20Partha%20Pratim%20Das&entry.1292438233=%20%20Formalizing%20cooking%20procedures%20remains%20a%20challenging%20task%20due%20to%20their%0Ainherent%20complexity%20and%20ambiguity.%20We%20introduce%20an%20extensible%20domain-specific%0Alanguage%20for%20representing%20recipes%20as%20directed%20action%20graphs%2C%20capturing%0Aprocesses%2C%20transfers%2C%20environments%2C%20concurrency%2C%20and%20compositional%20structure.%0AOur%20approach%20enables%20precise%2C%20modular%20modeling%20of%20complex%20culinary%20workflows.%0AInitial%20manual%20evaluation%20on%20a%20full%20English%20breakfast%20recipe%20demonstrates%20the%0ADSL%27s%20expressiveness%20and%20suitability%20for%20future%20automated%20recipe%20analysis%20and%0Aexecution.%20This%20work%20represents%20initial%20steps%20towards%20an%20action-centric%0Aontology%20for%20cooking%2C%20using%20temporal%20graphs%20to%20enable%20structured%20machine%0Aunderstanding%2C%20precise%20interpretation%2C%20and%20scalable%20automation%20of%20culinary%0Aprocesses%20-%20both%20in%20home%20kitchens%20and%20professional%20culinary%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04159v1&entry.124074799=Read"},
{"title": "The human biological advantage over AI", "author": "William Stewart", "abstract": "  Recent advances in AI raise the possibility that AI systems will one day be\nable to do anything humans can do, only better. If artificial general\nintelligence (AGI) is achieved, AI systems may be able to understand, reason,\nproblem solve, create, and evolve at a level and speed that humans will\nincreasingly be unable to match, or even understand. These possibilities raise\na natural question as to whether AI will eventually become superior to humans,\na successor \"digital species\", with a rightful claim to assume leadership of\nthe universe. However, a deeper consideration suggests the overlooked\ndifferentiator between human beings and AI is not the brain, but the central\nnervous system (CNS), providing us with an immersive integration with physical\nreality. It is our CNS that enables us to experience emotion including pain,\njoy, suffering, and love, and therefore to fully appreciate the consequences of\nour actions on the world around us. And that emotional understanding of the\nconsequences of our actions is what is required to be able to develop\nsustainable ethical systems, and so be fully qualified to be the leaders of the\nuniverse. A CNS cannot be manufactured or simulated; it must be grown as a\nbiological construct. And so, even the development of consciousness will not be\nsufficient to make AI systems superior to humans. AI systems may become more\ncapable than humans on almost every measure and transform our society. However,\nthe best foundation for leadership of our universe will always be DNA, not\nsilicon.\n", "link": "http://arxiv.org/abs/2509.04130v1", "date": "2025-09-04", "relevancy": 1.8839, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.3868}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3718}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20human%20biological%20advantage%20over%20AI&body=Title%3A%20The%20human%20biological%20advantage%20over%20AI%0AAuthor%3A%20William%20Stewart%0AAbstract%3A%20%20%20Recent%20advances%20in%20AI%20raise%20the%20possibility%20that%20AI%20systems%20will%20one%20day%20be%0Aable%20to%20do%20anything%20humans%20can%20do%2C%20only%20better.%20If%20artificial%20general%0Aintelligence%20%28AGI%29%20is%20achieved%2C%20AI%20systems%20may%20be%20able%20to%20understand%2C%20reason%2C%0Aproblem%20solve%2C%20create%2C%20and%20evolve%20at%20a%20level%20and%20speed%20that%20humans%20will%0Aincreasingly%20be%20unable%20to%20match%2C%20or%20even%20understand.%20These%20possibilities%20raise%0Aa%20natural%20question%20as%20to%20whether%20AI%20will%20eventually%20become%20superior%20to%20humans%2C%0Aa%20successor%20%22digital%20species%22%2C%20with%20a%20rightful%20claim%20to%20assume%20leadership%20of%0Athe%20universe.%20However%2C%20a%20deeper%20consideration%20suggests%20the%20overlooked%0Adifferentiator%20between%20human%20beings%20and%20AI%20is%20not%20the%20brain%2C%20but%20the%20central%0Anervous%20system%20%28CNS%29%2C%20providing%20us%20with%20an%20immersive%20integration%20with%20physical%0Areality.%20It%20is%20our%20CNS%20that%20enables%20us%20to%20experience%20emotion%20including%20pain%2C%0Ajoy%2C%20suffering%2C%20and%20love%2C%20and%20therefore%20to%20fully%20appreciate%20the%20consequences%20of%0Aour%20actions%20on%20the%20world%20around%20us.%20And%20that%20emotional%20understanding%20of%20the%0Aconsequences%20of%20our%20actions%20is%20what%20is%20required%20to%20be%20able%20to%20develop%0Asustainable%20ethical%20systems%2C%20and%20so%20be%20fully%20qualified%20to%20be%20the%20leaders%20of%20the%0Auniverse.%20A%20CNS%20cannot%20be%20manufactured%20or%20simulated%3B%20it%20must%20be%20grown%20as%20a%0Abiological%20construct.%20And%20so%2C%20even%20the%20development%20of%20consciousness%20will%20not%20be%0Asufficient%20to%20make%20AI%20systems%20superior%20to%20humans.%20AI%20systems%20may%20become%20more%0Acapable%20than%20humans%20on%20almost%20every%20measure%20and%20transform%20our%20society.%20However%2C%0Athe%20best%20foundation%20for%20leadership%20of%20our%20universe%20will%20always%20be%20DNA%2C%20not%0Asilicon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04130v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520human%2520biological%2520advantage%2520over%2520AI%26entry.906535625%3DWilliam%2520Stewart%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520AI%2520raise%2520the%2520possibility%2520that%2520AI%2520systems%2520will%2520one%2520day%2520be%250Aable%2520to%2520do%2520anything%2520humans%2520can%2520do%252C%2520only%2520better.%2520If%2520artificial%2520general%250Aintelligence%2520%2528AGI%2529%2520is%2520achieved%252C%2520AI%2520systems%2520may%2520be%2520able%2520to%2520understand%252C%2520reason%252C%250Aproblem%2520solve%252C%2520create%252C%2520and%2520evolve%2520at%2520a%2520level%2520and%2520speed%2520that%2520humans%2520will%250Aincreasingly%2520be%2520unable%2520to%2520match%252C%2520or%2520even%2520understand.%2520These%2520possibilities%2520raise%250Aa%2520natural%2520question%2520as%2520to%2520whether%2520AI%2520will%2520eventually%2520become%2520superior%2520to%2520humans%252C%250Aa%2520successor%2520%2522digital%2520species%2522%252C%2520with%2520a%2520rightful%2520claim%2520to%2520assume%2520leadership%2520of%250Athe%2520universe.%2520However%252C%2520a%2520deeper%2520consideration%2520suggests%2520the%2520overlooked%250Adifferentiator%2520between%2520human%2520beings%2520and%2520AI%2520is%2520not%2520the%2520brain%252C%2520but%2520the%2520central%250Anervous%2520system%2520%2528CNS%2529%252C%2520providing%2520us%2520with%2520an%2520immersive%2520integration%2520with%2520physical%250Areality.%2520It%2520is%2520our%2520CNS%2520that%2520enables%2520us%2520to%2520experience%2520emotion%2520including%2520pain%252C%250Ajoy%252C%2520suffering%252C%2520and%2520love%252C%2520and%2520therefore%2520to%2520fully%2520appreciate%2520the%2520consequences%2520of%250Aour%2520actions%2520on%2520the%2520world%2520around%2520us.%2520And%2520that%2520emotional%2520understanding%2520of%2520the%250Aconsequences%2520of%2520our%2520actions%2520is%2520what%2520is%2520required%2520to%2520be%2520able%2520to%2520develop%250Asustainable%2520ethical%2520systems%252C%2520and%2520so%2520be%2520fully%2520qualified%2520to%2520be%2520the%2520leaders%2520of%2520the%250Auniverse.%2520A%2520CNS%2520cannot%2520be%2520manufactured%2520or%2520simulated%253B%2520it%2520must%2520be%2520grown%2520as%2520a%250Abiological%2520construct.%2520And%2520so%252C%2520even%2520the%2520development%2520of%2520consciousness%2520will%2520not%2520be%250Asufficient%2520to%2520make%2520AI%2520systems%2520superior%2520to%2520humans.%2520AI%2520systems%2520may%2520become%2520more%250Acapable%2520than%2520humans%2520on%2520almost%2520every%2520measure%2520and%2520transform%2520our%2520society.%2520However%252C%250Athe%2520best%2520foundation%2520for%2520leadership%2520of%2520our%2520universe%2520will%2520always%2520be%2520DNA%252C%2520not%250Asilicon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04130v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20human%20biological%20advantage%20over%20AI&entry.906535625=William%20Stewart&entry.1292438233=%20%20Recent%20advances%20in%20AI%20raise%20the%20possibility%20that%20AI%20systems%20will%20one%20day%20be%0Aable%20to%20do%20anything%20humans%20can%20do%2C%20only%20better.%20If%20artificial%20general%0Aintelligence%20%28AGI%29%20is%20achieved%2C%20AI%20systems%20may%20be%20able%20to%20understand%2C%20reason%2C%0Aproblem%20solve%2C%20create%2C%20and%20evolve%20at%20a%20level%20and%20speed%20that%20humans%20will%0Aincreasingly%20be%20unable%20to%20match%2C%20or%20even%20understand.%20These%20possibilities%20raise%0Aa%20natural%20question%20as%20to%20whether%20AI%20will%20eventually%20become%20superior%20to%20humans%2C%0Aa%20successor%20%22digital%20species%22%2C%20with%20a%20rightful%20claim%20to%20assume%20leadership%20of%0Athe%20universe.%20However%2C%20a%20deeper%20consideration%20suggests%20the%20overlooked%0Adifferentiator%20between%20human%20beings%20and%20AI%20is%20not%20the%20brain%2C%20but%20the%20central%0Anervous%20system%20%28CNS%29%2C%20providing%20us%20with%20an%20immersive%20integration%20with%20physical%0Areality.%20It%20is%20our%20CNS%20that%20enables%20us%20to%20experience%20emotion%20including%20pain%2C%0Ajoy%2C%20suffering%2C%20and%20love%2C%20and%20therefore%20to%20fully%20appreciate%20the%20consequences%20of%0Aour%20actions%20on%20the%20world%20around%20us.%20And%20that%20emotional%20understanding%20of%20the%0Aconsequences%20of%20our%20actions%20is%20what%20is%20required%20to%20be%20able%20to%20develop%0Asustainable%20ethical%20systems%2C%20and%20so%20be%20fully%20qualified%20to%20be%20the%20leaders%20of%20the%0Auniverse.%20A%20CNS%20cannot%20be%20manufactured%20or%20simulated%3B%20it%20must%20be%20grown%20as%20a%0Abiological%20construct.%20And%20so%2C%20even%20the%20development%20of%20consciousness%20will%20not%20be%0Asufficient%20to%20make%20AI%20systems%20superior%20to%20humans.%20AI%20systems%20may%20become%20more%0Acapable%20than%20humans%20on%20almost%20every%20measure%20and%20transform%20our%20society.%20However%2C%0Athe%20best%20foundation%20for%20leadership%20of%20our%20universe%20will%20always%20be%20DNA%2C%20not%0Asilicon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04130v1&entry.124074799=Read"},
{"title": "Attention as an Adaptive Filter", "author": "Peter Racioppo", "abstract": "  We introduce Adaptive Filter Attention (AFA), a novel attention mechanism\nthat incorporates a learnable dynamics model directly into the computation of\nattention weights. Rather than comparing queries and keys directly, we model\nthe input sequence as discrete observations of a linear stochastic differential\nequation (SDE). By imposing a linear dynamics model with simultaneously\ndiagonalizable state matrices and noise covariances, we can make use of a\nclosed-form solution to the differential Lyapunov equation to efficiently\npropagate pairwise uncertainties through the dynamics. Attention naturally\narises as the maximum likelihood solution for this linear SDE, with attention\nweights corresponding to robust residual-based reweightings of the propagated\npairwise precisions. Imposing an additional constraint on the state matrix's\neigenvalues leads to a simplified variant with the same computational and\nmemory complexity as standard attention. In the limit of vanishing dynamics and\nprocess noise, and using a small-angle approximation, we recover ordinary\ndot-product attention.\n", "link": "http://arxiv.org/abs/2509.04154v1", "date": "2025-09-04", "relevancy": 1.548, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.554}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4727}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention%20as%20an%20Adaptive%20Filter&body=Title%3A%20Attention%20as%20an%20Adaptive%20Filter%0AAuthor%3A%20Peter%20Racioppo%0AAbstract%3A%20%20%20We%20introduce%20Adaptive%20Filter%20Attention%20%28AFA%29%2C%20a%20novel%20attention%20mechanism%0Athat%20incorporates%20a%20learnable%20dynamics%20model%20directly%20into%20the%20computation%20of%0Aattention%20weights.%20Rather%20than%20comparing%20queries%20and%20keys%20directly%2C%20we%20model%0Athe%20input%20sequence%20as%20discrete%20observations%20of%20a%20linear%20stochastic%20differential%0Aequation%20%28SDE%29.%20By%20imposing%20a%20linear%20dynamics%20model%20with%20simultaneously%0Adiagonalizable%20state%20matrices%20and%20noise%20covariances%2C%20we%20can%20make%20use%20of%20a%0Aclosed-form%20solution%20to%20the%20differential%20Lyapunov%20equation%20to%20efficiently%0Apropagate%20pairwise%20uncertainties%20through%20the%20dynamics.%20Attention%20naturally%0Aarises%20as%20the%20maximum%20likelihood%20solution%20for%20this%20linear%20SDE%2C%20with%20attention%0Aweights%20corresponding%20to%20robust%20residual-based%20reweightings%20of%20the%20propagated%0Apairwise%20precisions.%20Imposing%20an%20additional%20constraint%20on%20the%20state%20matrix%27s%0Aeigenvalues%20leads%20to%20a%20simplified%20variant%20with%20the%20same%20computational%20and%0Amemory%20complexity%20as%20standard%20attention.%20In%20the%20limit%20of%20vanishing%20dynamics%20and%0Aprocess%20noise%2C%20and%20using%20a%20small-angle%20approximation%2C%20we%20recover%20ordinary%0Adot-product%20attention.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04154v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention%2520as%2520an%2520Adaptive%2520Filter%26entry.906535625%3DPeter%2520Racioppo%26entry.1292438233%3D%2520%2520We%2520introduce%2520Adaptive%2520Filter%2520Attention%2520%2528AFA%2529%252C%2520a%2520novel%2520attention%2520mechanism%250Athat%2520incorporates%2520a%2520learnable%2520dynamics%2520model%2520directly%2520into%2520the%2520computation%2520of%250Aattention%2520weights.%2520Rather%2520than%2520comparing%2520queries%2520and%2520keys%2520directly%252C%2520we%2520model%250Athe%2520input%2520sequence%2520as%2520discrete%2520observations%2520of%2520a%2520linear%2520stochastic%2520differential%250Aequation%2520%2528SDE%2529.%2520By%2520imposing%2520a%2520linear%2520dynamics%2520model%2520with%2520simultaneously%250Adiagonalizable%2520state%2520matrices%2520and%2520noise%2520covariances%252C%2520we%2520can%2520make%2520use%2520of%2520a%250Aclosed-form%2520solution%2520to%2520the%2520differential%2520Lyapunov%2520equation%2520to%2520efficiently%250Apropagate%2520pairwise%2520uncertainties%2520through%2520the%2520dynamics.%2520Attention%2520naturally%250Aarises%2520as%2520the%2520maximum%2520likelihood%2520solution%2520for%2520this%2520linear%2520SDE%252C%2520with%2520attention%250Aweights%2520corresponding%2520to%2520robust%2520residual-based%2520reweightings%2520of%2520the%2520propagated%250Apairwise%2520precisions.%2520Imposing%2520an%2520additional%2520constraint%2520on%2520the%2520state%2520matrix%2527s%250Aeigenvalues%2520leads%2520to%2520a%2520simplified%2520variant%2520with%2520the%2520same%2520computational%2520and%250Amemory%2520complexity%2520as%2520standard%2520attention.%2520In%2520the%2520limit%2520of%2520vanishing%2520dynamics%2520and%250Aprocess%2520noise%252C%2520and%2520using%2520a%2520small-angle%2520approximation%252C%2520we%2520recover%2520ordinary%250Adot-product%2520attention.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04154v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20as%20an%20Adaptive%20Filter&entry.906535625=Peter%20Racioppo&entry.1292438233=%20%20We%20introduce%20Adaptive%20Filter%20Attention%20%28AFA%29%2C%20a%20novel%20attention%20mechanism%0Athat%20incorporates%20a%20learnable%20dynamics%20model%20directly%20into%20the%20computation%20of%0Aattention%20weights.%20Rather%20than%20comparing%20queries%20and%20keys%20directly%2C%20we%20model%0Athe%20input%20sequence%20as%20discrete%20observations%20of%20a%20linear%20stochastic%20differential%0Aequation%20%28SDE%29.%20By%20imposing%20a%20linear%20dynamics%20model%20with%20simultaneously%0Adiagonalizable%20state%20matrices%20and%20noise%20covariances%2C%20we%20can%20make%20use%20of%20a%0Aclosed-form%20solution%20to%20the%20differential%20Lyapunov%20equation%20to%20efficiently%0Apropagate%20pairwise%20uncertainties%20through%20the%20dynamics.%20Attention%20naturally%0Aarises%20as%20the%20maximum%20likelihood%20solution%20for%20this%20linear%20SDE%2C%20with%20attention%0Aweights%20corresponding%20to%20robust%20residual-based%20reweightings%20of%20the%20propagated%0Apairwise%20precisions.%20Imposing%20an%20additional%20constraint%20on%20the%20state%20matrix%27s%0Aeigenvalues%20leads%20to%20a%20simplified%20variant%20with%20the%20same%20computational%20and%0Amemory%20complexity%20as%20standard%20attention.%20In%20the%20limit%20of%20vanishing%20dynamics%20and%0Aprocess%20noise%2C%20and%20using%20a%20small-angle%20approximation%2C%20we%20recover%20ordinary%0Adot-product%20attention.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04154v1&entry.124074799=Read"},
{"title": "Style Transfer to Calvin and Hobbes comics using Stable Diffusion", "author": "Asvin Kumar Venkataramanan and Sloke Shrestha and Sundar Sripada Venugopalaswamy Sriraman", "abstract": "  This project report summarizes our journey to perform stable diffusion\nfine-tuning on a dataset containing Calvin and Hobbes comics. The purpose is to\nconvert any given input image into the comic style of Calvin and Hobbes,\nessentially performing style transfer. We train stable-diffusion-v1.5 using Low\nRank Adaptation (LoRA) to efficiently speed up the fine-tuning process. The\ndiffusion itself is handled by a Variational Autoencoder (VAE), which is a\nU-net. Our results were visually appealing for the amount of training time and\nthe quality of input data that went into training.\n", "link": "http://arxiv.org/abs/2312.03993v2", "date": "2025-09-04", "relevancy": 1.5273, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5168}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5081}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Style%20Transfer%20to%20Calvin%20and%20Hobbes%20comics%20using%20Stable%20Diffusion&body=Title%3A%20Style%20Transfer%20to%20Calvin%20and%20Hobbes%20comics%20using%20Stable%20Diffusion%0AAuthor%3A%20Asvin%20Kumar%20Venkataramanan%20and%20Sloke%20Shrestha%20and%20Sundar%20Sripada%20Venugopalaswamy%20Sriraman%0AAbstract%3A%20%20%20This%20project%20report%20summarizes%20our%20journey%20to%20perform%20stable%20diffusion%0Afine-tuning%20on%20a%20dataset%20containing%20Calvin%20and%20Hobbes%20comics.%20The%20purpose%20is%20to%0Aconvert%20any%20given%20input%20image%20into%20the%20comic%20style%20of%20Calvin%20and%20Hobbes%2C%0Aessentially%20performing%20style%20transfer.%20We%20train%20stable-diffusion-v1.5%20using%20Low%0ARank%20Adaptation%20%28LoRA%29%20to%20efficiently%20speed%20up%20the%20fine-tuning%20process.%20The%0Adiffusion%20itself%20is%20handled%20by%20a%20Variational%20Autoencoder%20%28VAE%29%2C%20which%20is%20a%0AU-net.%20Our%20results%20were%20visually%20appealing%20for%20the%20amount%20of%20training%20time%20and%0Athe%20quality%20of%20input%20data%20that%20went%20into%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.03993v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStyle%2520Transfer%2520to%2520Calvin%2520and%2520Hobbes%2520comics%2520using%2520Stable%2520Diffusion%26entry.906535625%3DAsvin%2520Kumar%2520Venkataramanan%2520and%2520Sloke%2520Shrestha%2520and%2520Sundar%2520Sripada%2520Venugopalaswamy%2520Sriraman%26entry.1292438233%3D%2520%2520This%2520project%2520report%2520summarizes%2520our%2520journey%2520to%2520perform%2520stable%2520diffusion%250Afine-tuning%2520on%2520a%2520dataset%2520containing%2520Calvin%2520and%2520Hobbes%2520comics.%2520The%2520purpose%2520is%2520to%250Aconvert%2520any%2520given%2520input%2520image%2520into%2520the%2520comic%2520style%2520of%2520Calvin%2520and%2520Hobbes%252C%250Aessentially%2520performing%2520style%2520transfer.%2520We%2520train%2520stable-diffusion-v1.5%2520using%2520Low%250ARank%2520Adaptation%2520%2528LoRA%2529%2520to%2520efficiently%2520speed%2520up%2520the%2520fine-tuning%2520process.%2520The%250Adiffusion%2520itself%2520is%2520handled%2520by%2520a%2520Variational%2520Autoencoder%2520%2528VAE%2529%252C%2520which%2520is%2520a%250AU-net.%2520Our%2520results%2520were%2520visually%2520appealing%2520for%2520the%2520amount%2520of%2520training%2520time%2520and%250Athe%2520quality%2520of%2520input%2520data%2520that%2520went%2520into%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.03993v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Style%20Transfer%20to%20Calvin%20and%20Hobbes%20comics%20using%20Stable%20Diffusion&entry.906535625=Asvin%20Kumar%20Venkataramanan%20and%20Sloke%20Shrestha%20and%20Sundar%20Sripada%20Venugopalaswamy%20Sriraman&entry.1292438233=%20%20This%20project%20report%20summarizes%20our%20journey%20to%20perform%20stable%20diffusion%0Afine-tuning%20on%20a%20dataset%20containing%20Calvin%20and%20Hobbes%20comics.%20The%20purpose%20is%20to%0Aconvert%20any%20given%20input%20image%20into%20the%20comic%20style%20of%20Calvin%20and%20Hobbes%2C%0Aessentially%20performing%20style%20transfer.%20We%20train%20stable-diffusion-v1.5%20using%20Low%0ARank%20Adaptation%20%28LoRA%29%20to%20efficiently%20speed%20up%20the%20fine-tuning%20process.%20The%0Adiffusion%20itself%20is%20handled%20by%20a%20Variational%20Autoencoder%20%28VAE%29%2C%20which%20is%20a%0AU-net.%20Our%20results%20were%20visually%20appealing%20for%20the%20amount%20of%20training%20time%20and%0Athe%20quality%20of%20input%20data%20that%20went%20into%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.03993v2&entry.124074799=Read"},
{"title": "DVS-PedX: Synthetic-and-Real Event-Based Pedestrian Dataset", "author": "Mustafa Sakhai and Kaung Sithu and Min Khant Soe Oke and Maciej Wielgosz", "abstract": "  Event cameras like Dynamic Vision Sensors (DVS) report micro-timed brightness\nchanges instead of full frames, offering low latency, high dynamic range, and\nmotion robustness. DVS-PedX (Dynamic Vision Sensor Pedestrian eXploration) is a\nneuromorphic dataset designed for pedestrian detection and crossing-intention\nanalysis in normal and adverse weather conditions across two complementary\nsources: (1) synthetic event streams generated in the CARLA simulator for\ncontrolled \"approach-cross\" scenes under varied weather and lighting; and (2)\nreal-world JAAD dash-cam videos converted to event streams using the v2e tool,\npreserving natural behaviors and backgrounds. Each sequence includes paired RGB\nframes, per-frame DVS \"event frames\" (33 ms accumulations), and frame-level\nlabels (crossing vs. not crossing). We also provide raw AEDAT 2.0/AEDAT 4.0\nevent files and AVI DVS video files and metadata for flexible re-processing.\nBaseline spiking neural networks (SNNs) using SpikingJelly illustrate dataset\nusability and reveal a sim-to-real gap, motivating domain adaptation and\nmultimodal fusion. DVS-PedX aims to accelerate research in event-based\npedestrian safety, intention prediction, and neuromorphic perception.\n", "link": "http://arxiv.org/abs/2509.04117v1", "date": "2025-09-04", "relevancy": 1.5976, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.554}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5308}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DVS-PedX%3A%20Synthetic-and-Real%20Event-Based%20Pedestrian%20Dataset&body=Title%3A%20DVS-PedX%3A%20Synthetic-and-Real%20Event-Based%20Pedestrian%20Dataset%0AAuthor%3A%20Mustafa%20Sakhai%20and%20Kaung%20Sithu%20and%20Min%20Khant%20Soe%20Oke%20and%20Maciej%20Wielgosz%0AAbstract%3A%20%20%20Event%20cameras%20like%20Dynamic%20Vision%20Sensors%20%28DVS%29%20report%20micro-timed%20brightness%0Achanges%20instead%20of%20full%20frames%2C%20offering%20low%20latency%2C%20high%20dynamic%20range%2C%20and%0Amotion%20robustness.%20DVS-PedX%20%28Dynamic%20Vision%20Sensor%20Pedestrian%20eXploration%29%20is%20a%0Aneuromorphic%20dataset%20designed%20for%20pedestrian%20detection%20and%20crossing-intention%0Aanalysis%20in%20normal%20and%20adverse%20weather%20conditions%20across%20two%20complementary%0Asources%3A%20%281%29%20synthetic%20event%20streams%20generated%20in%20the%20CARLA%20simulator%20for%0Acontrolled%20%22approach-cross%22%20scenes%20under%20varied%20weather%20and%20lighting%3B%20and%20%282%29%0Areal-world%20JAAD%20dash-cam%20videos%20converted%20to%20event%20streams%20using%20the%20v2e%20tool%2C%0Apreserving%20natural%20behaviors%20and%20backgrounds.%20Each%20sequence%20includes%20paired%20RGB%0Aframes%2C%20per-frame%20DVS%20%22event%20frames%22%20%2833%20ms%20accumulations%29%2C%20and%20frame-level%0Alabels%20%28crossing%20vs.%20not%20crossing%29.%20We%20also%20provide%20raw%20AEDAT%202.0/AEDAT%204.0%0Aevent%20files%20and%20AVI%20DVS%20video%20files%20and%20metadata%20for%20flexible%20re-processing.%0ABaseline%20spiking%20neural%20networks%20%28SNNs%29%20using%20SpikingJelly%20illustrate%20dataset%0Ausability%20and%20reveal%20a%20sim-to-real%20gap%2C%20motivating%20domain%20adaptation%20and%0Amultimodal%20fusion.%20DVS-PedX%20aims%20to%20accelerate%20research%20in%20event-based%0Apedestrian%20safety%2C%20intention%20prediction%2C%20and%20neuromorphic%20perception.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04117v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDVS-PedX%253A%2520Synthetic-and-Real%2520Event-Based%2520Pedestrian%2520Dataset%26entry.906535625%3DMustafa%2520Sakhai%2520and%2520Kaung%2520Sithu%2520and%2520Min%2520Khant%2520Soe%2520Oke%2520and%2520Maciej%2520Wielgosz%26entry.1292438233%3D%2520%2520Event%2520cameras%2520like%2520Dynamic%2520Vision%2520Sensors%2520%2528DVS%2529%2520report%2520micro-timed%2520brightness%250Achanges%2520instead%2520of%2520full%2520frames%252C%2520offering%2520low%2520latency%252C%2520high%2520dynamic%2520range%252C%2520and%250Amotion%2520robustness.%2520DVS-PedX%2520%2528Dynamic%2520Vision%2520Sensor%2520Pedestrian%2520eXploration%2529%2520is%2520a%250Aneuromorphic%2520dataset%2520designed%2520for%2520pedestrian%2520detection%2520and%2520crossing-intention%250Aanalysis%2520in%2520normal%2520and%2520adverse%2520weather%2520conditions%2520across%2520two%2520complementary%250Asources%253A%2520%25281%2529%2520synthetic%2520event%2520streams%2520generated%2520in%2520the%2520CARLA%2520simulator%2520for%250Acontrolled%2520%2522approach-cross%2522%2520scenes%2520under%2520varied%2520weather%2520and%2520lighting%253B%2520and%2520%25282%2529%250Areal-world%2520JAAD%2520dash-cam%2520videos%2520converted%2520to%2520event%2520streams%2520using%2520the%2520v2e%2520tool%252C%250Apreserving%2520natural%2520behaviors%2520and%2520backgrounds.%2520Each%2520sequence%2520includes%2520paired%2520RGB%250Aframes%252C%2520per-frame%2520DVS%2520%2522event%2520frames%2522%2520%252833%2520ms%2520accumulations%2529%252C%2520and%2520frame-level%250Alabels%2520%2528crossing%2520vs.%2520not%2520crossing%2529.%2520We%2520also%2520provide%2520raw%2520AEDAT%25202.0/AEDAT%25204.0%250Aevent%2520files%2520and%2520AVI%2520DVS%2520video%2520files%2520and%2520metadata%2520for%2520flexible%2520re-processing.%250ABaseline%2520spiking%2520neural%2520networks%2520%2528SNNs%2529%2520using%2520SpikingJelly%2520illustrate%2520dataset%250Ausability%2520and%2520reveal%2520a%2520sim-to-real%2520gap%252C%2520motivating%2520domain%2520adaptation%2520and%250Amultimodal%2520fusion.%2520DVS-PedX%2520aims%2520to%2520accelerate%2520research%2520in%2520event-based%250Apedestrian%2520safety%252C%2520intention%2520prediction%252C%2520and%2520neuromorphic%2520perception.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04117v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DVS-PedX%3A%20Synthetic-and-Real%20Event-Based%20Pedestrian%20Dataset&entry.906535625=Mustafa%20Sakhai%20and%20Kaung%20Sithu%20and%20Min%20Khant%20Soe%20Oke%20and%20Maciej%20Wielgosz&entry.1292438233=%20%20Event%20cameras%20like%20Dynamic%20Vision%20Sensors%20%28DVS%29%20report%20micro-timed%20brightness%0Achanges%20instead%20of%20full%20frames%2C%20offering%20low%20latency%2C%20high%20dynamic%20range%2C%20and%0Amotion%20robustness.%20DVS-PedX%20%28Dynamic%20Vision%20Sensor%20Pedestrian%20eXploration%29%20is%20a%0Aneuromorphic%20dataset%20designed%20for%20pedestrian%20detection%20and%20crossing-intention%0Aanalysis%20in%20normal%20and%20adverse%20weather%20conditions%20across%20two%20complementary%0Asources%3A%20%281%29%20synthetic%20event%20streams%20generated%20in%20the%20CARLA%20simulator%20for%0Acontrolled%20%22approach-cross%22%20scenes%20under%20varied%20weather%20and%20lighting%3B%20and%20%282%29%0Areal-world%20JAAD%20dash-cam%20videos%20converted%20to%20event%20streams%20using%20the%20v2e%20tool%2C%0Apreserving%20natural%20behaviors%20and%20backgrounds.%20Each%20sequence%20includes%20paired%20RGB%0Aframes%2C%20per-frame%20DVS%20%22event%20frames%22%20%2833%20ms%20accumulations%29%2C%20and%20frame-level%0Alabels%20%28crossing%20vs.%20not%20crossing%29.%20We%20also%20provide%20raw%20AEDAT%202.0/AEDAT%204.0%0Aevent%20files%20and%20AVI%20DVS%20video%20files%20and%20metadata%20for%20flexible%20re-processing.%0ABaseline%20spiking%20neural%20networks%20%28SNNs%29%20using%20SpikingJelly%20illustrate%20dataset%0Ausability%20and%20reveal%20a%20sim-to-real%20gap%2C%20motivating%20domain%20adaptation%20and%0Amultimodal%20fusion.%20DVS-PedX%20aims%20to%20accelerate%20research%20in%20event-based%0Apedestrian%20safety%2C%20intention%20prediction%2C%20and%20neuromorphic%20perception.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04117v1&entry.124074799=Read"},
{"title": "Who Pays for Fairness? Rethinking Recourse under Social Burden", "author": "Ainhize Barrainkua and Giovanni De Toni and Jose Antonio Lozano and Novi Quadrianto", "abstract": "  Machine learning based predictions are increasingly used in sensitive\ndecision-making applications that directly affect our lives. This has led to\nextensive research into ensuring the fairness of classifiers. Beyond just fair\nclassification, emerging legislation now mandates that when a classifier\ndelivers a negative decision, it must also offer actionable steps an individual\ncan take to reverse that outcome. This concept is known as algorithmic\nrecourse. Nevertheless, many researchers have expressed concerns about the\nfairness guarantees within the recourse process itself. In this work, we\nprovide a holistic theoretical characterization of unfairness in algorithmic\nrecourse, formally linking fairness guarantees in recourse and classification,\nand highlighting limitations of the standard equal cost paradigm. We then\nintroduce a novel fairness framework based on social burden, along with a\npractical algorithm (MISOB), broadly applicable under real-world conditions.\nEmpirical results on real-world datasets show that MISOB reduces the social\nburden across all groups without compromising overall classifier accuracy.\n", "link": "http://arxiv.org/abs/2509.04128v1", "date": "2025-09-04", "relevancy": 1.808, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4979}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4533}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Who%20Pays%20for%20Fairness%3F%20Rethinking%20Recourse%20under%20Social%20Burden&body=Title%3A%20Who%20Pays%20for%20Fairness%3F%20Rethinking%20Recourse%20under%20Social%20Burden%0AAuthor%3A%20Ainhize%20Barrainkua%20and%20Giovanni%20De%20Toni%20and%20Jose%20Antonio%20Lozano%20and%20Novi%20Quadrianto%0AAbstract%3A%20%20%20Machine%20learning%20based%20predictions%20are%20increasingly%20used%20in%20sensitive%0Adecision-making%20applications%20that%20directly%20affect%20our%20lives.%20This%20has%20led%20to%0Aextensive%20research%20into%20ensuring%20the%20fairness%20of%20classifiers.%20Beyond%20just%20fair%0Aclassification%2C%20emerging%20legislation%20now%20mandates%20that%20when%20a%20classifier%0Adelivers%20a%20negative%20decision%2C%20it%20must%20also%20offer%20actionable%20steps%20an%20individual%0Acan%20take%20to%20reverse%20that%20outcome.%20This%20concept%20is%20known%20as%20algorithmic%0Arecourse.%20Nevertheless%2C%20many%20researchers%20have%20expressed%20concerns%20about%20the%0Afairness%20guarantees%20within%20the%20recourse%20process%20itself.%20In%20this%20work%2C%20we%0Aprovide%20a%20holistic%20theoretical%20characterization%20of%20unfairness%20in%20algorithmic%0Arecourse%2C%20formally%20linking%20fairness%20guarantees%20in%20recourse%20and%20classification%2C%0Aand%20highlighting%20limitations%20of%20the%20standard%20equal%20cost%20paradigm.%20We%20then%0Aintroduce%20a%20novel%20fairness%20framework%20based%20on%20social%20burden%2C%20along%20with%20a%0Apractical%20algorithm%20%28MISOB%29%2C%20broadly%20applicable%20under%20real-world%20conditions.%0AEmpirical%20results%20on%20real-world%20datasets%20show%20that%20MISOB%20reduces%20the%20social%0Aburden%20across%20all%20groups%20without%20compromising%20overall%20classifier%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04128v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWho%2520Pays%2520for%2520Fairness%253F%2520Rethinking%2520Recourse%2520under%2520Social%2520Burden%26entry.906535625%3DAinhize%2520Barrainkua%2520and%2520Giovanni%2520De%2520Toni%2520and%2520Jose%2520Antonio%2520Lozano%2520and%2520Novi%2520Quadrianto%26entry.1292438233%3D%2520%2520Machine%2520learning%2520based%2520predictions%2520are%2520increasingly%2520used%2520in%2520sensitive%250Adecision-making%2520applications%2520that%2520directly%2520affect%2520our%2520lives.%2520This%2520has%2520led%2520to%250Aextensive%2520research%2520into%2520ensuring%2520the%2520fairness%2520of%2520classifiers.%2520Beyond%2520just%2520fair%250Aclassification%252C%2520emerging%2520legislation%2520now%2520mandates%2520that%2520when%2520a%2520classifier%250Adelivers%2520a%2520negative%2520decision%252C%2520it%2520must%2520also%2520offer%2520actionable%2520steps%2520an%2520individual%250Acan%2520take%2520to%2520reverse%2520that%2520outcome.%2520This%2520concept%2520is%2520known%2520as%2520algorithmic%250Arecourse.%2520Nevertheless%252C%2520many%2520researchers%2520have%2520expressed%2520concerns%2520about%2520the%250Afairness%2520guarantees%2520within%2520the%2520recourse%2520process%2520itself.%2520In%2520this%2520work%252C%2520we%250Aprovide%2520a%2520holistic%2520theoretical%2520characterization%2520of%2520unfairness%2520in%2520algorithmic%250Arecourse%252C%2520formally%2520linking%2520fairness%2520guarantees%2520in%2520recourse%2520and%2520classification%252C%250Aand%2520highlighting%2520limitations%2520of%2520the%2520standard%2520equal%2520cost%2520paradigm.%2520We%2520then%250Aintroduce%2520a%2520novel%2520fairness%2520framework%2520based%2520on%2520social%2520burden%252C%2520along%2520with%2520a%250Apractical%2520algorithm%2520%2528MISOB%2529%252C%2520broadly%2520applicable%2520under%2520real-world%2520conditions.%250AEmpirical%2520results%2520on%2520real-world%2520datasets%2520show%2520that%2520MISOB%2520reduces%2520the%2520social%250Aburden%2520across%2520all%2520groups%2520without%2520compromising%2520overall%2520classifier%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04128v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Who%20Pays%20for%20Fairness%3F%20Rethinking%20Recourse%20under%20Social%20Burden&entry.906535625=Ainhize%20Barrainkua%20and%20Giovanni%20De%20Toni%20and%20Jose%20Antonio%20Lozano%20and%20Novi%20Quadrianto&entry.1292438233=%20%20Machine%20learning%20based%20predictions%20are%20increasingly%20used%20in%20sensitive%0Adecision-making%20applications%20that%20directly%20affect%20our%20lives.%20This%20has%20led%20to%0Aextensive%20research%20into%20ensuring%20the%20fairness%20of%20classifiers.%20Beyond%20just%20fair%0Aclassification%2C%20emerging%20legislation%20now%20mandates%20that%20when%20a%20classifier%0Adelivers%20a%20negative%20decision%2C%20it%20must%20also%20offer%20actionable%20steps%20an%20individual%0Acan%20take%20to%20reverse%20that%20outcome.%20This%20concept%20is%20known%20as%20algorithmic%0Arecourse.%20Nevertheless%2C%20many%20researchers%20have%20expressed%20concerns%20about%20the%0Afairness%20guarantees%20within%20the%20recourse%20process%20itself.%20In%20this%20work%2C%20we%0Aprovide%20a%20holistic%20theoretical%20characterization%20of%20unfairness%20in%20algorithmic%0Arecourse%2C%20formally%20linking%20fairness%20guarantees%20in%20recourse%20and%20classification%2C%0Aand%20highlighting%20limitations%20of%20the%20standard%20equal%20cost%20paradigm.%20We%20then%0Aintroduce%20a%20novel%20fairness%20framework%20based%20on%20social%20burden%2C%20along%20with%20a%0Apractical%20algorithm%20%28MISOB%29%2C%20broadly%20applicable%20under%20real-world%20conditions.%0AEmpirical%20results%20on%20real-world%20datasets%20show%20that%20MISOB%20reduces%20the%20social%0Aburden%20across%20all%20groups%20without%20compromising%20overall%20classifier%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04128v1&entry.124074799=Read"},
{"title": "Federated Isolation Forest for Efficient Anomaly Detection on Edge IoT\n  Systems", "author": "Pavle Vasiljevic and Milica Matic and Miroslav Popovic", "abstract": "  Recently, federated learning frameworks such as Python TestBed for Federated\nLearning Algorithms and MicroPython TestBed for Federated Learning Algorithms\nhave emerged to tackle user privacy concerns and efficiency in embedded\nsystems. Even more recently, an efficient federated anomaly detection\nalgorithm, FLiForest, based on Isolation Forests has been developed, offering a\nlow-resource, unsupervised method well-suited for edge deployment and\ncontinuous learning. In this paper, we present an application of Isolation\nForest-based temperature anomaly detection, developed using the previously\nmentioned federated learning frameworks, aimed at small edge devices and IoT\nsystems running MicroPython. The system has been experimentally evaluated,\nachieving over 96% accuracy in distinguishing normal from abnormal readings and\nabove 78% precision in detecting anomalies across all tested configurations,\nwhile maintaining a memory usage below 160 KB during model training. These\nresults highlight its suitability for resource-constrained environments and\nedge systems, while upholding federated learning principles of data privacy and\ncollaborative learning.\n", "link": "http://arxiv.org/abs/2506.05138v2", "date": "2025-09-04", "relevancy": 1.7449, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4497}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4407}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Isolation%20Forest%20for%20Efficient%20Anomaly%20Detection%20on%20Edge%20IoT%0A%20%20Systems&body=Title%3A%20Federated%20Isolation%20Forest%20for%20Efficient%20Anomaly%20Detection%20on%20Edge%20IoT%0A%20%20Systems%0AAuthor%3A%20Pavle%20Vasiljevic%20and%20Milica%20Matic%20and%20Miroslav%20Popovic%0AAbstract%3A%20%20%20Recently%2C%20federated%20learning%20frameworks%20such%20as%20Python%20TestBed%20for%20Federated%0ALearning%20Algorithms%20and%20MicroPython%20TestBed%20for%20Federated%20Learning%20Algorithms%0Ahave%20emerged%20to%20tackle%20user%20privacy%20concerns%20and%20efficiency%20in%20embedded%0Asystems.%20Even%20more%20recently%2C%20an%20efficient%20federated%20anomaly%20detection%0Aalgorithm%2C%20FLiForest%2C%20based%20on%20Isolation%20Forests%20has%20been%20developed%2C%20offering%20a%0Alow-resource%2C%20unsupervised%20method%20well-suited%20for%20edge%20deployment%20and%0Acontinuous%20learning.%20In%20this%20paper%2C%20we%20present%20an%20application%20of%20Isolation%0AForest-based%20temperature%20anomaly%20detection%2C%20developed%20using%20the%20previously%0Amentioned%20federated%20learning%20frameworks%2C%20aimed%20at%20small%20edge%20devices%20and%20IoT%0Asystems%20running%20MicroPython.%20The%20system%20has%20been%20experimentally%20evaluated%2C%0Aachieving%20over%2096%25%20accuracy%20in%20distinguishing%20normal%20from%20abnormal%20readings%20and%0Aabove%2078%25%20precision%20in%20detecting%20anomalies%20across%20all%20tested%20configurations%2C%0Awhile%20maintaining%20a%20memory%20usage%20below%20160%20KB%20during%20model%20training.%20These%0Aresults%20highlight%20its%20suitability%20for%20resource-constrained%20environments%20and%0Aedge%20systems%2C%20while%20upholding%20federated%20learning%20principles%20of%20data%20privacy%20and%0Acollaborative%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05138v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Isolation%2520Forest%2520for%2520Efficient%2520Anomaly%2520Detection%2520on%2520Edge%2520IoT%250A%2520%2520Systems%26entry.906535625%3DPavle%2520Vasiljevic%2520and%2520Milica%2520Matic%2520and%2520Miroslav%2520Popovic%26entry.1292438233%3D%2520%2520Recently%252C%2520federated%2520learning%2520frameworks%2520such%2520as%2520Python%2520TestBed%2520for%2520Federated%250ALearning%2520Algorithms%2520and%2520MicroPython%2520TestBed%2520for%2520Federated%2520Learning%2520Algorithms%250Ahave%2520emerged%2520to%2520tackle%2520user%2520privacy%2520concerns%2520and%2520efficiency%2520in%2520embedded%250Asystems.%2520Even%2520more%2520recently%252C%2520an%2520efficient%2520federated%2520anomaly%2520detection%250Aalgorithm%252C%2520FLiForest%252C%2520based%2520on%2520Isolation%2520Forests%2520has%2520been%2520developed%252C%2520offering%2520a%250Alow-resource%252C%2520unsupervised%2520method%2520well-suited%2520for%2520edge%2520deployment%2520and%250Acontinuous%2520learning.%2520In%2520this%2520paper%252C%2520we%2520present%2520an%2520application%2520of%2520Isolation%250AForest-based%2520temperature%2520anomaly%2520detection%252C%2520developed%2520using%2520the%2520previously%250Amentioned%2520federated%2520learning%2520frameworks%252C%2520aimed%2520at%2520small%2520edge%2520devices%2520and%2520IoT%250Asystems%2520running%2520MicroPython.%2520The%2520system%2520has%2520been%2520experimentally%2520evaluated%252C%250Aachieving%2520over%252096%2525%2520accuracy%2520in%2520distinguishing%2520normal%2520from%2520abnormal%2520readings%2520and%250Aabove%252078%2525%2520precision%2520in%2520detecting%2520anomalies%2520across%2520all%2520tested%2520configurations%252C%250Awhile%2520maintaining%2520a%2520memory%2520usage%2520below%2520160%2520KB%2520during%2520model%2520training.%2520These%250Aresults%2520highlight%2520its%2520suitability%2520for%2520resource-constrained%2520environments%2520and%250Aedge%2520systems%252C%2520while%2520upholding%2520federated%2520learning%2520principles%2520of%2520data%2520privacy%2520and%250Acollaborative%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05138v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Isolation%20Forest%20for%20Efficient%20Anomaly%20Detection%20on%20Edge%20IoT%0A%20%20Systems&entry.906535625=Pavle%20Vasiljevic%20and%20Milica%20Matic%20and%20Miroslav%20Popovic&entry.1292438233=%20%20Recently%2C%20federated%20learning%20frameworks%20such%20as%20Python%20TestBed%20for%20Federated%0ALearning%20Algorithms%20and%20MicroPython%20TestBed%20for%20Federated%20Learning%20Algorithms%0Ahave%20emerged%20to%20tackle%20user%20privacy%20concerns%20and%20efficiency%20in%20embedded%0Asystems.%20Even%20more%20recently%2C%20an%20efficient%20federated%20anomaly%20detection%0Aalgorithm%2C%20FLiForest%2C%20based%20on%20Isolation%20Forests%20has%20been%20developed%2C%20offering%20a%0Alow-resource%2C%20unsupervised%20method%20well-suited%20for%20edge%20deployment%20and%0Acontinuous%20learning.%20In%20this%20paper%2C%20we%20present%20an%20application%20of%20Isolation%0AForest-based%20temperature%20anomaly%20detection%2C%20developed%20using%20the%20previously%0Amentioned%20federated%20learning%20frameworks%2C%20aimed%20at%20small%20edge%20devices%20and%20IoT%0Asystems%20running%20MicroPython.%20The%20system%20has%20been%20experimentally%20evaluated%2C%0Aachieving%20over%2096%25%20accuracy%20in%20distinguishing%20normal%20from%20abnormal%20readings%20and%0Aabove%2078%25%20precision%20in%20detecting%20anomalies%20across%20all%20tested%20configurations%2C%0Awhile%20maintaining%20a%20memory%20usage%20below%20160%20KB%20during%20model%20training.%20These%0Aresults%20highlight%20its%20suitability%20for%20resource-constrained%20environments%20and%0Aedge%20systems%2C%20while%20upholding%20federated%20learning%20principles%20of%20data%20privacy%20and%0Acollaborative%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05138v2&entry.124074799=Read"},
{"title": "Autonomation, Not Automation: Activities and Needs of European\n  Fact-checkers as a Basis for Designing Human-Centered AI Systems", "author": "Andrea Hrckova and Robert Moro and Ivan Srba and Jakub Simko and Maria Bielikova", "abstract": "  To mitigate the negative effects of false information more effectively, the\ndevelopment of Artificial Intelligence (AI) systems to assist fact-checkers is\nneeded. Nevertheless, the lack of focus on the needs of these stakeholders\nresults in their limited acceptance and skepticism toward automating the whole\nfact-checking process. In this study, we conducted semi-structured in-depth\ninterviews with Central European fact-checkers. Their activities and problems\nwere analyzed using iterative content analysis. The most significant problems\nwere validated with a survey of European fact-checkers, in which we collected\n24 responses from 20 countries, i.e., 62% of active European signatories of the\nInternational Fact-Checking Network (IFCN). Our contributions include an\nin-depth examination of the variability of fact-checking work in\nnon-English-speaking regions, which still remained largely uncovered. By\naligning them with the knowledge from prior studies, we created conceptual\nmodels that help to understand the fact-checking processes. In addition, we\nmapped our findings on the fact-checkers' activities and needs to the relevant\ntasks for AI research, while providing a discussion on three AI tasks that were\nnot covered by previous similar studies. The new opportunities identified for\nAI researchers and developers have implications for the focus of AI research in\nthis domain.\n", "link": "http://arxiv.org/abs/2211.12143v3", "date": "2025-09-04", "relevancy": 1.5828, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4082}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.397}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3894}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autonomation%2C%20Not%20Automation%3A%20Activities%20and%20Needs%20of%20European%0A%20%20Fact-checkers%20as%20a%20Basis%20for%20Designing%20Human-Centered%20AI%20Systems&body=Title%3A%20Autonomation%2C%20Not%20Automation%3A%20Activities%20and%20Needs%20of%20European%0A%20%20Fact-checkers%20as%20a%20Basis%20for%20Designing%20Human-Centered%20AI%20Systems%0AAuthor%3A%20Andrea%20Hrckova%20and%20Robert%20Moro%20and%20Ivan%20Srba%20and%20Jakub%20Simko%20and%20Maria%20Bielikova%0AAbstract%3A%20%20%20To%20mitigate%20the%20negative%20effects%20of%20false%20information%20more%20effectively%2C%20the%0Adevelopment%20of%20Artificial%20Intelligence%20%28AI%29%20systems%20to%20assist%20fact-checkers%20is%0Aneeded.%20Nevertheless%2C%20the%20lack%20of%20focus%20on%20the%20needs%20of%20these%20stakeholders%0Aresults%20in%20their%20limited%20acceptance%20and%20skepticism%20toward%20automating%20the%20whole%0Afact-checking%20process.%20In%20this%20study%2C%20we%20conducted%20semi-structured%20in-depth%0Ainterviews%20with%20Central%20European%20fact-checkers.%20Their%20activities%20and%20problems%0Awere%20analyzed%20using%20iterative%20content%20analysis.%20The%20most%20significant%20problems%0Awere%20validated%20with%20a%20survey%20of%20European%20fact-checkers%2C%20in%20which%20we%20collected%0A24%20responses%20from%2020%20countries%2C%20i.e.%2C%2062%25%20of%20active%20European%20signatories%20of%20the%0AInternational%20Fact-Checking%20Network%20%28IFCN%29.%20Our%20contributions%20include%20an%0Ain-depth%20examination%20of%20the%20variability%20of%20fact-checking%20work%20in%0Anon-English-speaking%20regions%2C%20which%20still%20remained%20largely%20uncovered.%20By%0Aaligning%20them%20with%20the%20knowledge%20from%20prior%20studies%2C%20we%20created%20conceptual%0Amodels%20that%20help%20to%20understand%20the%20fact-checking%20processes.%20In%20addition%2C%20we%0Amapped%20our%20findings%20on%20the%20fact-checkers%27%20activities%20and%20needs%20to%20the%20relevant%0Atasks%20for%20AI%20research%2C%20while%20providing%20a%20discussion%20on%20three%20AI%20tasks%20that%20were%0Anot%20covered%20by%20previous%20similar%20studies.%20The%20new%20opportunities%20identified%20for%0AAI%20researchers%20and%20developers%20have%20implications%20for%20the%20focus%20of%20AI%20research%20in%0Athis%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.12143v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutonomation%252C%2520Not%2520Automation%253A%2520Activities%2520and%2520Needs%2520of%2520European%250A%2520%2520Fact-checkers%2520as%2520a%2520Basis%2520for%2520Designing%2520Human-Centered%2520AI%2520Systems%26entry.906535625%3DAndrea%2520Hrckova%2520and%2520Robert%2520Moro%2520and%2520Ivan%2520Srba%2520and%2520Jakub%2520Simko%2520and%2520Maria%2520Bielikova%26entry.1292438233%3D%2520%2520To%2520mitigate%2520the%2520negative%2520effects%2520of%2520false%2520information%2520more%2520effectively%252C%2520the%250Adevelopment%2520of%2520Artificial%2520Intelligence%2520%2528AI%2529%2520systems%2520to%2520assist%2520fact-checkers%2520is%250Aneeded.%2520Nevertheless%252C%2520the%2520lack%2520of%2520focus%2520on%2520the%2520needs%2520of%2520these%2520stakeholders%250Aresults%2520in%2520their%2520limited%2520acceptance%2520and%2520skepticism%2520toward%2520automating%2520the%2520whole%250Afact-checking%2520process.%2520In%2520this%2520study%252C%2520we%2520conducted%2520semi-structured%2520in-depth%250Ainterviews%2520with%2520Central%2520European%2520fact-checkers.%2520Their%2520activities%2520and%2520problems%250Awere%2520analyzed%2520using%2520iterative%2520content%2520analysis.%2520The%2520most%2520significant%2520problems%250Awere%2520validated%2520with%2520a%2520survey%2520of%2520European%2520fact-checkers%252C%2520in%2520which%2520we%2520collected%250A24%2520responses%2520from%252020%2520countries%252C%2520i.e.%252C%252062%2525%2520of%2520active%2520European%2520signatories%2520of%2520the%250AInternational%2520Fact-Checking%2520Network%2520%2528IFCN%2529.%2520Our%2520contributions%2520include%2520an%250Ain-depth%2520examination%2520of%2520the%2520variability%2520of%2520fact-checking%2520work%2520in%250Anon-English-speaking%2520regions%252C%2520which%2520still%2520remained%2520largely%2520uncovered.%2520By%250Aaligning%2520them%2520with%2520the%2520knowledge%2520from%2520prior%2520studies%252C%2520we%2520created%2520conceptual%250Amodels%2520that%2520help%2520to%2520understand%2520the%2520fact-checking%2520processes.%2520In%2520addition%252C%2520we%250Amapped%2520our%2520findings%2520on%2520the%2520fact-checkers%2527%2520activities%2520and%2520needs%2520to%2520the%2520relevant%250Atasks%2520for%2520AI%2520research%252C%2520while%2520providing%2520a%2520discussion%2520on%2520three%2520AI%2520tasks%2520that%2520were%250Anot%2520covered%2520by%2520previous%2520similar%2520studies.%2520The%2520new%2520opportunities%2520identified%2520for%250AAI%2520researchers%2520and%2520developers%2520have%2520implications%2520for%2520the%2520focus%2520of%2520AI%2520research%2520in%250Athis%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.12143v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomation%2C%20Not%20Automation%3A%20Activities%20and%20Needs%20of%20European%0A%20%20Fact-checkers%20as%20a%20Basis%20for%20Designing%20Human-Centered%20AI%20Systems&entry.906535625=Andrea%20Hrckova%20and%20Robert%20Moro%20and%20Ivan%20Srba%20and%20Jakub%20Simko%20and%20Maria%20Bielikova&entry.1292438233=%20%20To%20mitigate%20the%20negative%20effects%20of%20false%20information%20more%20effectively%2C%20the%0Adevelopment%20of%20Artificial%20Intelligence%20%28AI%29%20systems%20to%20assist%20fact-checkers%20is%0Aneeded.%20Nevertheless%2C%20the%20lack%20of%20focus%20on%20the%20needs%20of%20these%20stakeholders%0Aresults%20in%20their%20limited%20acceptance%20and%20skepticism%20toward%20automating%20the%20whole%0Afact-checking%20process.%20In%20this%20study%2C%20we%20conducted%20semi-structured%20in-depth%0Ainterviews%20with%20Central%20European%20fact-checkers.%20Their%20activities%20and%20problems%0Awere%20analyzed%20using%20iterative%20content%20analysis.%20The%20most%20significant%20problems%0Awere%20validated%20with%20a%20survey%20of%20European%20fact-checkers%2C%20in%20which%20we%20collected%0A24%20responses%20from%2020%20countries%2C%20i.e.%2C%2062%25%20of%20active%20European%20signatories%20of%20the%0AInternational%20Fact-Checking%20Network%20%28IFCN%29.%20Our%20contributions%20include%20an%0Ain-depth%20examination%20of%20the%20variability%20of%20fact-checking%20work%20in%0Anon-English-speaking%20regions%2C%20which%20still%20remained%20largely%20uncovered.%20By%0Aaligning%20them%20with%20the%20knowledge%20from%20prior%20studies%2C%20we%20created%20conceptual%0Amodels%20that%20help%20to%20understand%20the%20fact-checking%20processes.%20In%20addition%2C%20we%0Amapped%20our%20findings%20on%20the%20fact-checkers%27%20activities%20and%20needs%20to%20the%20relevant%0Atasks%20for%20AI%20research%2C%20while%20providing%20a%20discussion%20on%20three%20AI%20tasks%20that%20were%0Anot%20covered%20by%20previous%20similar%20studies.%20The%20new%20opportunities%20identified%20for%0AAI%20researchers%20and%20developers%20have%20implications%20for%20the%20focus%20of%20AI%20research%20in%0Athis%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.12143v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


