<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250521.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Dress-1-to-3: Single Image to Simulation-Ready 3D Outfit with Diffusion\n  Prior and Differentiable Physics", "author": "Xuan Li and Chang Yu and Wenxin Du and Ying Jiang and Tianyi Xie and Yunuo Chen and Yin Yang and Chenfanfu Jiang", "abstract": "  Recent advances in large models have significantly advanced image-to-3D\nreconstruction. However, the generated models are often fused into a single\npiece, limiting their applicability in downstream tasks. This paper focuses on\n3D garment generation, a key area for applications like virtual try-on with\ndynamic garment animations, which require garments to be separable and\nsimulation-ready. We introduce Dress-1-to-3, a novel pipeline that reconstructs\nphysics-plausible, simulation-ready separated garments with sewing patterns and\nhumans from an in-the-wild image. Starting with the image, our approach\ncombines a pre-trained image-to-sewing pattern generation model for creating\ncoarse sewing patterns with a pre-trained multi-view diffusion model to produce\nmulti-view images. The sewing pattern is further refined using a differentiable\ngarment simulator based on the generated multi-view images. Versatile\nexperiments demonstrate that our optimization approach substantially enhances\nthe geometric alignment of the reconstructed 3D garments and humans with the\ninput image. Furthermore, by integrating a texture generation module and a\nhuman motion generation module, we produce customized physics-plausible and\nrealistic dynamic garment demonstrations. Project page:\nhttps://dress-1-to-3.github.io/\n", "link": "http://arxiv.org/abs/2502.03449v2", "date": "2025-05-21", "relevancy": 3.7142, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.7665}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.7561}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dress-1-to-3%3A%20Single%20Image%20to%20Simulation-Ready%203D%20Outfit%20with%20Diffusion%0A%20%20Prior%20and%20Differentiable%20Physics&body=Title%3A%20Dress-1-to-3%3A%20Single%20Image%20to%20Simulation-Ready%203D%20Outfit%20with%20Diffusion%0A%20%20Prior%20and%20Differentiable%20Physics%0AAuthor%3A%20Xuan%20Li%20and%20Chang%20Yu%20and%20Wenxin%20Du%20and%20Ying%20Jiang%20and%20Tianyi%20Xie%20and%20Yunuo%20Chen%20and%20Yin%20Yang%20and%20Chenfanfu%20Jiang%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20models%20have%20significantly%20advanced%20image-to-3D%0Areconstruction.%20However%2C%20the%20generated%20models%20are%20often%20fused%20into%20a%20single%0Apiece%2C%20limiting%20their%20applicability%20in%20downstream%20tasks.%20This%20paper%20focuses%20on%0A3D%20garment%20generation%2C%20a%20key%20area%20for%20applications%20like%20virtual%20try-on%20with%0Adynamic%20garment%20animations%2C%20which%20require%20garments%20to%20be%20separable%20and%0Asimulation-ready.%20We%20introduce%20Dress-1-to-3%2C%20a%20novel%20pipeline%20that%20reconstructs%0Aphysics-plausible%2C%20simulation-ready%20separated%20garments%20with%20sewing%20patterns%20and%0Ahumans%20from%20an%20in-the-wild%20image.%20Starting%20with%20the%20image%2C%20our%20approach%0Acombines%20a%20pre-trained%20image-to-sewing%20pattern%20generation%20model%20for%20creating%0Acoarse%20sewing%20patterns%20with%20a%20pre-trained%20multi-view%20diffusion%20model%20to%20produce%0Amulti-view%20images.%20The%20sewing%20pattern%20is%20further%20refined%20using%20a%20differentiable%0Agarment%20simulator%20based%20on%20the%20generated%20multi-view%20images.%20Versatile%0Aexperiments%20demonstrate%20that%20our%20optimization%20approach%20substantially%20enhances%0Athe%20geometric%20alignment%20of%20the%20reconstructed%203D%20garments%20and%20humans%20with%20the%0Ainput%20image.%20Furthermore%2C%20by%20integrating%20a%20texture%20generation%20module%20and%20a%0Ahuman%20motion%20generation%20module%2C%20we%20produce%20customized%20physics-plausible%20and%0Arealistic%20dynamic%20garment%20demonstrations.%20Project%20page%3A%0Ahttps%3A//dress-1-to-3.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03449v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDress-1-to-3%253A%2520Single%2520Image%2520to%2520Simulation-Ready%25203D%2520Outfit%2520with%2520Diffusion%250A%2520%2520Prior%2520and%2520Differentiable%2520Physics%26entry.906535625%3DXuan%2520Li%2520and%2520Chang%2520Yu%2520and%2520Wenxin%2520Du%2520and%2520Ying%2520Jiang%2520and%2520Tianyi%2520Xie%2520and%2520Yunuo%2520Chen%2520and%2520Yin%2520Yang%2520and%2520Chenfanfu%2520Jiang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large%2520models%2520have%2520significantly%2520advanced%2520image-to-3D%250Areconstruction.%2520However%252C%2520the%2520generated%2520models%2520are%2520often%2520fused%2520into%2520a%2520single%250Apiece%252C%2520limiting%2520their%2520applicability%2520in%2520downstream%2520tasks.%2520This%2520paper%2520focuses%2520on%250A3D%2520garment%2520generation%252C%2520a%2520key%2520area%2520for%2520applications%2520like%2520virtual%2520try-on%2520with%250Adynamic%2520garment%2520animations%252C%2520which%2520require%2520garments%2520to%2520be%2520separable%2520and%250Asimulation-ready.%2520We%2520introduce%2520Dress-1-to-3%252C%2520a%2520novel%2520pipeline%2520that%2520reconstructs%250Aphysics-plausible%252C%2520simulation-ready%2520separated%2520garments%2520with%2520sewing%2520patterns%2520and%250Ahumans%2520from%2520an%2520in-the-wild%2520image.%2520Starting%2520with%2520the%2520image%252C%2520our%2520approach%250Acombines%2520a%2520pre-trained%2520image-to-sewing%2520pattern%2520generation%2520model%2520for%2520creating%250Acoarse%2520sewing%2520patterns%2520with%2520a%2520pre-trained%2520multi-view%2520diffusion%2520model%2520to%2520produce%250Amulti-view%2520images.%2520The%2520sewing%2520pattern%2520is%2520further%2520refined%2520using%2520a%2520differentiable%250Agarment%2520simulator%2520based%2520on%2520the%2520generated%2520multi-view%2520images.%2520Versatile%250Aexperiments%2520demonstrate%2520that%2520our%2520optimization%2520approach%2520substantially%2520enhances%250Athe%2520geometric%2520alignment%2520of%2520the%2520reconstructed%25203D%2520garments%2520and%2520humans%2520with%2520the%250Ainput%2520image.%2520Furthermore%252C%2520by%2520integrating%2520a%2520texture%2520generation%2520module%2520and%2520a%250Ahuman%2520motion%2520generation%2520module%252C%2520we%2520produce%2520customized%2520physics-plausible%2520and%250Arealistic%2520dynamic%2520garment%2520demonstrations.%2520Project%2520page%253A%250Ahttps%253A//dress-1-to-3.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03449v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dress-1-to-3%3A%20Single%20Image%20to%20Simulation-Ready%203D%20Outfit%20with%20Diffusion%0A%20%20Prior%20and%20Differentiable%20Physics&entry.906535625=Xuan%20Li%20and%20Chang%20Yu%20and%20Wenxin%20Du%20and%20Ying%20Jiang%20and%20Tianyi%20Xie%20and%20Yunuo%20Chen%20and%20Yin%20Yang%20and%20Chenfanfu%20Jiang&entry.1292438233=%20%20Recent%20advances%20in%20large%20models%20have%20significantly%20advanced%20image-to-3D%0Areconstruction.%20However%2C%20the%20generated%20models%20are%20often%20fused%20into%20a%20single%0Apiece%2C%20limiting%20their%20applicability%20in%20downstream%20tasks.%20This%20paper%20focuses%20on%0A3D%20garment%20generation%2C%20a%20key%20area%20for%20applications%20like%20virtual%20try-on%20with%0Adynamic%20garment%20animations%2C%20which%20require%20garments%20to%20be%20separable%20and%0Asimulation-ready.%20We%20introduce%20Dress-1-to-3%2C%20a%20novel%20pipeline%20that%20reconstructs%0Aphysics-plausible%2C%20simulation-ready%20separated%20garments%20with%20sewing%20patterns%20and%0Ahumans%20from%20an%20in-the-wild%20image.%20Starting%20with%20the%20image%2C%20our%20approach%0Acombines%20a%20pre-trained%20image-to-sewing%20pattern%20generation%20model%20for%20creating%0Acoarse%20sewing%20patterns%20with%20a%20pre-trained%20multi-view%20diffusion%20model%20to%20produce%0Amulti-view%20images.%20The%20sewing%20pattern%20is%20further%20refined%20using%20a%20differentiable%0Agarment%20simulator%20based%20on%20the%20generated%20multi-view%20images.%20Versatile%0Aexperiments%20demonstrate%20that%20our%20optimization%20approach%20substantially%20enhances%0Athe%20geometric%20alignment%20of%20the%20reconstructed%203D%20garments%20and%20humans%20with%20the%0Ainput%20image.%20Furthermore%2C%20by%20integrating%20a%20texture%20generation%20module%20and%20a%0Ahuman%20motion%20generation%20module%2C%20we%20produce%20customized%20physics-plausible%20and%0Arealistic%20dynamic%20garment%20demonstrations.%20Project%20page%3A%0Ahttps%3A//dress-1-to-3.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03449v2&entry.124074799=Read"},
{"title": "EVA: Expressive Virtual Avatars from Multi-view Videos", "author": "Hendrik Junkawitsch and Guoxing Sun and Heming Zhu and Christian Theobalt and Marc Habermann", "abstract": "  With recent advancements in neural rendering and motion capture algorithms,\nremarkable progress has been made in photorealistic human avatar modeling,\nunlocking immense potential for applications in virtual reality, augmented\nreality, remote communication, and industries such as gaming, film, and\nmedicine. However, existing methods fail to provide complete, faithful, and\nexpressive control over human avatars due to their entangled representation of\nfacial expressions and body movements. In this work, we introduce Expressive\nVirtual Avatars (EVA), an actor-specific, fully controllable, and expressive\nhuman avatar framework that achieves high-fidelity, lifelike renderings in real\ntime while enabling independent control of facial expressions, body movements,\nand hand gestures. Specifically, our approach designs the human avatar as a\ntwo-layer model: an expressive template geometry layer and a 3D Gaussian\nappearance layer. First, we present an expressive template tracking algorithm\nthat leverages coarse-to-fine optimization to accurately recover body motions,\nfacial expressions, and non-rigid deformation parameters from multi-view\nvideos. Next, we propose a novel decoupled 3D Gaussian appearance model\ndesigned to effectively disentangle body and facial appearance. Unlike unified\nGaussian estimation approaches, our method employs two specialized and\nindependent modules to model the body and face separately. Experimental results\ndemonstrate that EVA surpasses state-of-the-art methods in terms of rendering\nquality and expressiveness, validating its effectiveness in creating full-body\navatars. This work represents a significant advancement towards fully drivable\ndigital human models, enabling the creation of lifelike digital avatars that\nfaithfully replicate human geometry and appearance.\n", "link": "http://arxiv.org/abs/2505.15385v1", "date": "2025-05-21", "relevancy": 3.3576, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6924}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6924}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EVA%3A%20Expressive%20Virtual%20Avatars%20from%20Multi-view%20Videos&body=Title%3A%20EVA%3A%20Expressive%20Virtual%20Avatars%20from%20Multi-view%20Videos%0AAuthor%3A%20Hendrik%20Junkawitsch%20and%20Guoxing%20Sun%20and%20Heming%20Zhu%20and%20Christian%20Theobalt%20and%20Marc%20Habermann%0AAbstract%3A%20%20%20With%20recent%20advancements%20in%20neural%20rendering%20and%20motion%20capture%20algorithms%2C%0Aremarkable%20progress%20has%20been%20made%20in%20photorealistic%20human%20avatar%20modeling%2C%0Aunlocking%20immense%20potential%20for%20applications%20in%20virtual%20reality%2C%20augmented%0Areality%2C%20remote%20communication%2C%20and%20industries%20such%20as%20gaming%2C%20film%2C%20and%0Amedicine.%20However%2C%20existing%20methods%20fail%20to%20provide%20complete%2C%20faithful%2C%20and%0Aexpressive%20control%20over%20human%20avatars%20due%20to%20their%20entangled%20representation%20of%0Afacial%20expressions%20and%20body%20movements.%20In%20this%20work%2C%20we%20introduce%20Expressive%0AVirtual%20Avatars%20%28EVA%29%2C%20an%20actor-specific%2C%20fully%20controllable%2C%20and%20expressive%0Ahuman%20avatar%20framework%20that%20achieves%20high-fidelity%2C%20lifelike%20renderings%20in%20real%0Atime%20while%20enabling%20independent%20control%20of%20facial%20expressions%2C%20body%20movements%2C%0Aand%20hand%20gestures.%20Specifically%2C%20our%20approach%20designs%20the%20human%20avatar%20as%20a%0Atwo-layer%20model%3A%20an%20expressive%20template%20geometry%20layer%20and%20a%203D%20Gaussian%0Aappearance%20layer.%20First%2C%20we%20present%20an%20expressive%20template%20tracking%20algorithm%0Athat%20leverages%20coarse-to-fine%20optimization%20to%20accurately%20recover%20body%20motions%2C%0Afacial%20expressions%2C%20and%20non-rigid%20deformation%20parameters%20from%20multi-view%0Avideos.%20Next%2C%20we%20propose%20a%20novel%20decoupled%203D%20Gaussian%20appearance%20model%0Adesigned%20to%20effectively%20disentangle%20body%20and%20facial%20appearance.%20Unlike%20unified%0AGaussian%20estimation%20approaches%2C%20our%20method%20employs%20two%20specialized%20and%0Aindependent%20modules%20to%20model%20the%20body%20and%20face%20separately.%20Experimental%20results%0Ademonstrate%20that%20EVA%20surpasses%20state-of-the-art%20methods%20in%20terms%20of%20rendering%0Aquality%20and%20expressiveness%2C%20validating%20its%20effectiveness%20in%20creating%20full-body%0Aavatars.%20This%20work%20represents%20a%20significant%20advancement%20towards%20fully%20drivable%0Adigital%20human%20models%2C%20enabling%20the%20creation%20of%20lifelike%20digital%20avatars%20that%0Afaithfully%20replicate%20human%20geometry%20and%20appearance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15385v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEVA%253A%2520Expressive%2520Virtual%2520Avatars%2520from%2520Multi-view%2520Videos%26entry.906535625%3DHendrik%2520Junkawitsch%2520and%2520Guoxing%2520Sun%2520and%2520Heming%2520Zhu%2520and%2520Christian%2520Theobalt%2520and%2520Marc%2520Habermann%26entry.1292438233%3D%2520%2520With%2520recent%2520advancements%2520in%2520neural%2520rendering%2520and%2520motion%2520capture%2520algorithms%252C%250Aremarkable%2520progress%2520has%2520been%2520made%2520in%2520photorealistic%2520human%2520avatar%2520modeling%252C%250Aunlocking%2520immense%2520potential%2520for%2520applications%2520in%2520virtual%2520reality%252C%2520augmented%250Areality%252C%2520remote%2520communication%252C%2520and%2520industries%2520such%2520as%2520gaming%252C%2520film%252C%2520and%250Amedicine.%2520However%252C%2520existing%2520methods%2520fail%2520to%2520provide%2520complete%252C%2520faithful%252C%2520and%250Aexpressive%2520control%2520over%2520human%2520avatars%2520due%2520to%2520their%2520entangled%2520representation%2520of%250Afacial%2520expressions%2520and%2520body%2520movements.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Expressive%250AVirtual%2520Avatars%2520%2528EVA%2529%252C%2520an%2520actor-specific%252C%2520fully%2520controllable%252C%2520and%2520expressive%250Ahuman%2520avatar%2520framework%2520that%2520achieves%2520high-fidelity%252C%2520lifelike%2520renderings%2520in%2520real%250Atime%2520while%2520enabling%2520independent%2520control%2520of%2520facial%2520expressions%252C%2520body%2520movements%252C%250Aand%2520hand%2520gestures.%2520Specifically%252C%2520our%2520approach%2520designs%2520the%2520human%2520avatar%2520as%2520a%250Atwo-layer%2520model%253A%2520an%2520expressive%2520template%2520geometry%2520layer%2520and%2520a%25203D%2520Gaussian%250Aappearance%2520layer.%2520First%252C%2520we%2520present%2520an%2520expressive%2520template%2520tracking%2520algorithm%250Athat%2520leverages%2520coarse-to-fine%2520optimization%2520to%2520accurately%2520recover%2520body%2520motions%252C%250Afacial%2520expressions%252C%2520and%2520non-rigid%2520deformation%2520parameters%2520from%2520multi-view%250Avideos.%2520Next%252C%2520we%2520propose%2520a%2520novel%2520decoupled%25203D%2520Gaussian%2520appearance%2520model%250Adesigned%2520to%2520effectively%2520disentangle%2520body%2520and%2520facial%2520appearance.%2520Unlike%2520unified%250AGaussian%2520estimation%2520approaches%252C%2520our%2520method%2520employs%2520two%2520specialized%2520and%250Aindependent%2520modules%2520to%2520model%2520the%2520body%2520and%2520face%2520separately.%2520Experimental%2520results%250Ademonstrate%2520that%2520EVA%2520surpasses%2520state-of-the-art%2520methods%2520in%2520terms%2520of%2520rendering%250Aquality%2520and%2520expressiveness%252C%2520validating%2520its%2520effectiveness%2520in%2520creating%2520full-body%250Aavatars.%2520This%2520work%2520represents%2520a%2520significant%2520advancement%2520towards%2520fully%2520drivable%250Adigital%2520human%2520models%252C%2520enabling%2520the%2520creation%2520of%2520lifelike%2520digital%2520avatars%2520that%250Afaithfully%2520replicate%2520human%2520geometry%2520and%2520appearance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15385v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EVA%3A%20Expressive%20Virtual%20Avatars%20from%20Multi-view%20Videos&entry.906535625=Hendrik%20Junkawitsch%20and%20Guoxing%20Sun%20and%20Heming%20Zhu%20and%20Christian%20Theobalt%20and%20Marc%20Habermann&entry.1292438233=%20%20With%20recent%20advancements%20in%20neural%20rendering%20and%20motion%20capture%20algorithms%2C%0Aremarkable%20progress%20has%20been%20made%20in%20photorealistic%20human%20avatar%20modeling%2C%0Aunlocking%20immense%20potential%20for%20applications%20in%20virtual%20reality%2C%20augmented%0Areality%2C%20remote%20communication%2C%20and%20industries%20such%20as%20gaming%2C%20film%2C%20and%0Amedicine.%20However%2C%20existing%20methods%20fail%20to%20provide%20complete%2C%20faithful%2C%20and%0Aexpressive%20control%20over%20human%20avatars%20due%20to%20their%20entangled%20representation%20of%0Afacial%20expressions%20and%20body%20movements.%20In%20this%20work%2C%20we%20introduce%20Expressive%0AVirtual%20Avatars%20%28EVA%29%2C%20an%20actor-specific%2C%20fully%20controllable%2C%20and%20expressive%0Ahuman%20avatar%20framework%20that%20achieves%20high-fidelity%2C%20lifelike%20renderings%20in%20real%0Atime%20while%20enabling%20independent%20control%20of%20facial%20expressions%2C%20body%20movements%2C%0Aand%20hand%20gestures.%20Specifically%2C%20our%20approach%20designs%20the%20human%20avatar%20as%20a%0Atwo-layer%20model%3A%20an%20expressive%20template%20geometry%20layer%20and%20a%203D%20Gaussian%0Aappearance%20layer.%20First%2C%20we%20present%20an%20expressive%20template%20tracking%20algorithm%0Athat%20leverages%20coarse-to-fine%20optimization%20to%20accurately%20recover%20body%20motions%2C%0Afacial%20expressions%2C%20and%20non-rigid%20deformation%20parameters%20from%20multi-view%0Avideos.%20Next%2C%20we%20propose%20a%20novel%20decoupled%203D%20Gaussian%20appearance%20model%0Adesigned%20to%20effectively%20disentangle%20body%20and%20facial%20appearance.%20Unlike%20unified%0AGaussian%20estimation%20approaches%2C%20our%20method%20employs%20two%20specialized%20and%0Aindependent%20modules%20to%20model%20the%20body%20and%20face%20separately.%20Experimental%20results%0Ademonstrate%20that%20EVA%20surpasses%20state-of-the-art%20methods%20in%20terms%20of%20rendering%0Aquality%20and%20expressiveness%2C%20validating%20its%20effectiveness%20in%20creating%20full-body%0Aavatars.%20This%20work%20represents%20a%20significant%20advancement%20towards%20fully%20drivable%0Adigital%20human%20models%2C%20enabling%20the%20creation%20of%20lifelike%20digital%20avatars%20that%0Afaithfully%20replicate%20human%20geometry%20and%20appearance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15385v1&entry.124074799=Read"},
{"title": "RAZER: Robust Accelerated Zero-Shot 3D Open-Vocabulary Panoptic\n  Reconstruction with Spatio-Temporal Aggregation", "author": "Naman Patel and Prashanth Krishnamurthy and Farshad Khorrami", "abstract": "  Mapping and understanding complex 3D environments is fundamental to how\nautonomous systems perceive and interact with the physical world, requiring\nboth precise geometric reconstruction and rich semantic comprehension. While\nexisting 3D semantic mapping systems excel at reconstructing and identifying\npredefined object instances, they lack the flexibility to efficiently build\nsemantic maps with open-vocabulary during online operation. Although recent\nvision-language models have enabled open-vocabulary object recognition in 2D\nimages, they haven't yet bridged the gap to 3D spatial understanding. The\ncritical challenge lies in developing a training-free unified system that can\nsimultaneously construct accurate 3D maps while maintaining semantic\nconsistency and supporting natural language interactions in real time. In this\npaper, we develop a zero-shot framework that seamlessly integrates\nGPU-accelerated geometric reconstruction with open-vocabulary vision-language\nmodels through online instance-level semantic embedding fusion, guided by\nhierarchical object association with spatial indexing. Our training-free system\nachieves superior performance through incremental processing and unified\ngeometric-semantic updates, while robustly handling 2D segmentation\ninconsistencies. The proposed general-purpose 3D scene understanding framework\ncan be used for various tasks including zero-shot 3D instance retrieval,\nsegmentation, and object detection to reason about previously unseen objects\nand interpret natural language queries. The project page is available at\nhttps://razer-3d.github.io.\n", "link": "http://arxiv.org/abs/2505.15373v1", "date": "2025-05-21", "relevancy": 3.3416, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6708}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6708}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAZER%3A%20Robust%20Accelerated%20Zero-Shot%203D%20Open-Vocabulary%20Panoptic%0A%20%20Reconstruction%20with%20Spatio-Temporal%20Aggregation&body=Title%3A%20RAZER%3A%20Robust%20Accelerated%20Zero-Shot%203D%20Open-Vocabulary%20Panoptic%0A%20%20Reconstruction%20with%20Spatio-Temporal%20Aggregation%0AAuthor%3A%20Naman%20Patel%20and%20Prashanth%20Krishnamurthy%20and%20Farshad%20Khorrami%0AAbstract%3A%20%20%20Mapping%20and%20understanding%20complex%203D%20environments%20is%20fundamental%20to%20how%0Aautonomous%20systems%20perceive%20and%20interact%20with%20the%20physical%20world%2C%20requiring%0Aboth%20precise%20geometric%20reconstruction%20and%20rich%20semantic%20comprehension.%20While%0Aexisting%203D%20semantic%20mapping%20systems%20excel%20at%20reconstructing%20and%20identifying%0Apredefined%20object%20instances%2C%20they%20lack%20the%20flexibility%20to%20efficiently%20build%0Asemantic%20maps%20with%20open-vocabulary%20during%20online%20operation.%20Although%20recent%0Avision-language%20models%20have%20enabled%20open-vocabulary%20object%20recognition%20in%202D%0Aimages%2C%20they%20haven%27t%20yet%20bridged%20the%20gap%20to%203D%20spatial%20understanding.%20The%0Acritical%20challenge%20lies%20in%20developing%20a%20training-free%20unified%20system%20that%20can%0Asimultaneously%20construct%20accurate%203D%20maps%20while%20maintaining%20semantic%0Aconsistency%20and%20supporting%20natural%20language%20interactions%20in%20real%20time.%20In%20this%0Apaper%2C%20we%20develop%20a%20zero-shot%20framework%20that%20seamlessly%20integrates%0AGPU-accelerated%20geometric%20reconstruction%20with%20open-vocabulary%20vision-language%0Amodels%20through%20online%20instance-level%20semantic%20embedding%20fusion%2C%20guided%20by%0Ahierarchical%20object%20association%20with%20spatial%20indexing.%20Our%20training-free%20system%0Aachieves%20superior%20performance%20through%20incremental%20processing%20and%20unified%0Ageometric-semantic%20updates%2C%20while%20robustly%20handling%202D%20segmentation%0Ainconsistencies.%20The%20proposed%20general-purpose%203D%20scene%20understanding%20framework%0Acan%20be%20used%20for%20various%20tasks%20including%20zero-shot%203D%20instance%20retrieval%2C%0Asegmentation%2C%20and%20object%20detection%20to%20reason%20about%20previously%20unseen%20objects%0Aand%20interpret%20natural%20language%20queries.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//razer-3d.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15373v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAZER%253A%2520Robust%2520Accelerated%2520Zero-Shot%25203D%2520Open-Vocabulary%2520Panoptic%250A%2520%2520Reconstruction%2520with%2520Spatio-Temporal%2520Aggregation%26entry.906535625%3DNaman%2520Patel%2520and%2520Prashanth%2520Krishnamurthy%2520and%2520Farshad%2520Khorrami%26entry.1292438233%3D%2520%2520Mapping%2520and%2520understanding%2520complex%25203D%2520environments%2520is%2520fundamental%2520to%2520how%250Aautonomous%2520systems%2520perceive%2520and%2520interact%2520with%2520the%2520physical%2520world%252C%2520requiring%250Aboth%2520precise%2520geometric%2520reconstruction%2520and%2520rich%2520semantic%2520comprehension.%2520While%250Aexisting%25203D%2520semantic%2520mapping%2520systems%2520excel%2520at%2520reconstructing%2520and%2520identifying%250Apredefined%2520object%2520instances%252C%2520they%2520lack%2520the%2520flexibility%2520to%2520efficiently%2520build%250Asemantic%2520maps%2520with%2520open-vocabulary%2520during%2520online%2520operation.%2520Although%2520recent%250Avision-language%2520models%2520have%2520enabled%2520open-vocabulary%2520object%2520recognition%2520in%25202D%250Aimages%252C%2520they%2520haven%2527t%2520yet%2520bridged%2520the%2520gap%2520to%25203D%2520spatial%2520understanding.%2520The%250Acritical%2520challenge%2520lies%2520in%2520developing%2520a%2520training-free%2520unified%2520system%2520that%2520can%250Asimultaneously%2520construct%2520accurate%25203D%2520maps%2520while%2520maintaining%2520semantic%250Aconsistency%2520and%2520supporting%2520natural%2520language%2520interactions%2520in%2520real%2520time.%2520In%2520this%250Apaper%252C%2520we%2520develop%2520a%2520zero-shot%2520framework%2520that%2520seamlessly%2520integrates%250AGPU-accelerated%2520geometric%2520reconstruction%2520with%2520open-vocabulary%2520vision-language%250Amodels%2520through%2520online%2520instance-level%2520semantic%2520embedding%2520fusion%252C%2520guided%2520by%250Ahierarchical%2520object%2520association%2520with%2520spatial%2520indexing.%2520Our%2520training-free%2520system%250Aachieves%2520superior%2520performance%2520through%2520incremental%2520processing%2520and%2520unified%250Ageometric-semantic%2520updates%252C%2520while%2520robustly%2520handling%25202D%2520segmentation%250Ainconsistencies.%2520The%2520proposed%2520general-purpose%25203D%2520scene%2520understanding%2520framework%250Acan%2520be%2520used%2520for%2520various%2520tasks%2520including%2520zero-shot%25203D%2520instance%2520retrieval%252C%250Asegmentation%252C%2520and%2520object%2520detection%2520to%2520reason%2520about%2520previously%2520unseen%2520objects%250Aand%2520interpret%2520natural%2520language%2520queries.%2520The%2520project%2520page%2520is%2520available%2520at%250Ahttps%253A//razer-3d.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15373v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAZER%3A%20Robust%20Accelerated%20Zero-Shot%203D%20Open-Vocabulary%20Panoptic%0A%20%20Reconstruction%20with%20Spatio-Temporal%20Aggregation&entry.906535625=Naman%20Patel%20and%20Prashanth%20Krishnamurthy%20and%20Farshad%20Khorrami&entry.1292438233=%20%20Mapping%20and%20understanding%20complex%203D%20environments%20is%20fundamental%20to%20how%0Aautonomous%20systems%20perceive%20and%20interact%20with%20the%20physical%20world%2C%20requiring%0Aboth%20precise%20geometric%20reconstruction%20and%20rich%20semantic%20comprehension.%20While%0Aexisting%203D%20semantic%20mapping%20systems%20excel%20at%20reconstructing%20and%20identifying%0Apredefined%20object%20instances%2C%20they%20lack%20the%20flexibility%20to%20efficiently%20build%0Asemantic%20maps%20with%20open-vocabulary%20during%20online%20operation.%20Although%20recent%0Avision-language%20models%20have%20enabled%20open-vocabulary%20object%20recognition%20in%202D%0Aimages%2C%20they%20haven%27t%20yet%20bridged%20the%20gap%20to%203D%20spatial%20understanding.%20The%0Acritical%20challenge%20lies%20in%20developing%20a%20training-free%20unified%20system%20that%20can%0Asimultaneously%20construct%20accurate%203D%20maps%20while%20maintaining%20semantic%0Aconsistency%20and%20supporting%20natural%20language%20interactions%20in%20real%20time.%20In%20this%0Apaper%2C%20we%20develop%20a%20zero-shot%20framework%20that%20seamlessly%20integrates%0AGPU-accelerated%20geometric%20reconstruction%20with%20open-vocabulary%20vision-language%0Amodels%20through%20online%20instance-level%20semantic%20embedding%20fusion%2C%20guided%20by%0Ahierarchical%20object%20association%20with%20spatial%20indexing.%20Our%20training-free%20system%0Aachieves%20superior%20performance%20through%20incremental%20processing%20and%20unified%0Ageometric-semantic%20updates%2C%20while%20robustly%20handling%202D%20segmentation%0Ainconsistencies.%20The%20proposed%20general-purpose%203D%20scene%20understanding%20framework%0Acan%20be%20used%20for%20various%20tasks%20including%20zero-shot%203D%20instance%20retrieval%2C%0Asegmentation%2C%20and%20object%20detection%20to%20reason%20about%20previously%20unseen%20objects%0Aand%20interpret%20natural%20language%20queries.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//razer-3d.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15373v1&entry.124074799=Read"},
{"title": "RUSplatting: Robust 3D Gaussian Splatting for Sparse-View Underwater\n  Scene Reconstruction", "author": "Zhuodong Jiang and Haoran Wang and Guoxi Huang and Brett Seymour and Nantheera Anantrasirichai", "abstract": "  Reconstructing high-fidelity underwater scenes remains a challenging task due\nto light absorption, scattering, and limited visibility inherent in aquatic\nenvironments. This paper presents an enhanced Gaussian Splatting-based\nframework that improves both the visual quality and geometric accuracy of deep\nunderwater rendering. We propose decoupled learning for RGB channels, guided by\nthe physics of underwater attenuation, to enable more accurate colour\nrestoration. To address sparse-view limitations and improve view consistency,\nwe introduce a frame interpolation strategy with a novel adaptive weighting\nscheme. Additionally, we introduce a new loss function aimed at reducing noise\nwhile preserving edges, which is essential for deep-sea content. We also\nrelease a newly collected dataset, Submerged3D, captured specifically in\ndeep-sea environments. Experimental results demonstrate that our framework\nconsistently outperforms state-of-the-art methods with PSNR gains up to 1.90dB,\ndelivering superior perceptual quality and robustness, and offering promising\ndirections for marine robotics and underwater visual analytics.\n", "link": "http://arxiv.org/abs/2505.15737v1", "date": "2025-05-21", "relevancy": 3.289, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7288}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6337}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RUSplatting%3A%20Robust%203D%20Gaussian%20Splatting%20for%20Sparse-View%20Underwater%0A%20%20Scene%20Reconstruction&body=Title%3A%20RUSplatting%3A%20Robust%203D%20Gaussian%20Splatting%20for%20Sparse-View%20Underwater%0A%20%20Scene%20Reconstruction%0AAuthor%3A%20Zhuodong%20Jiang%20and%20Haoran%20Wang%20and%20Guoxi%20Huang%20and%20Brett%20Seymour%20and%20Nantheera%20Anantrasirichai%0AAbstract%3A%20%20%20Reconstructing%20high-fidelity%20underwater%20scenes%20remains%20a%20challenging%20task%20due%0Ato%20light%20absorption%2C%20scattering%2C%20and%20limited%20visibility%20inherent%20in%20aquatic%0Aenvironments.%20This%20paper%20presents%20an%20enhanced%20Gaussian%20Splatting-based%0Aframework%20that%20improves%20both%20the%20visual%20quality%20and%20geometric%20accuracy%20of%20deep%0Aunderwater%20rendering.%20We%20propose%20decoupled%20learning%20for%20RGB%20channels%2C%20guided%20by%0Athe%20physics%20of%20underwater%20attenuation%2C%20to%20enable%20more%20accurate%20colour%0Arestoration.%20To%20address%20sparse-view%20limitations%20and%20improve%20view%20consistency%2C%0Awe%20introduce%20a%20frame%20interpolation%20strategy%20with%20a%20novel%20adaptive%20weighting%0Ascheme.%20Additionally%2C%20we%20introduce%20a%20new%20loss%20function%20aimed%20at%20reducing%20noise%0Awhile%20preserving%20edges%2C%20which%20is%20essential%20for%20deep-sea%20content.%20We%20also%0Arelease%20a%20newly%20collected%20dataset%2C%20Submerged3D%2C%20captured%20specifically%20in%0Adeep-sea%20environments.%20Experimental%20results%20demonstrate%20that%20our%20framework%0Aconsistently%20outperforms%20state-of-the-art%20methods%20with%20PSNR%20gains%20up%20to%201.90dB%2C%0Adelivering%20superior%20perceptual%20quality%20and%20robustness%2C%20and%20offering%20promising%0Adirections%20for%20marine%20robotics%20and%20underwater%20visual%20analytics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15737v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRUSplatting%253A%2520Robust%25203D%2520Gaussian%2520Splatting%2520for%2520Sparse-View%2520Underwater%250A%2520%2520Scene%2520Reconstruction%26entry.906535625%3DZhuodong%2520Jiang%2520and%2520Haoran%2520Wang%2520and%2520Guoxi%2520Huang%2520and%2520Brett%2520Seymour%2520and%2520Nantheera%2520Anantrasirichai%26entry.1292438233%3D%2520%2520Reconstructing%2520high-fidelity%2520underwater%2520scenes%2520remains%2520a%2520challenging%2520task%2520due%250Ato%2520light%2520absorption%252C%2520scattering%252C%2520and%2520limited%2520visibility%2520inherent%2520in%2520aquatic%250Aenvironments.%2520This%2520paper%2520presents%2520an%2520enhanced%2520Gaussian%2520Splatting-based%250Aframework%2520that%2520improves%2520both%2520the%2520visual%2520quality%2520and%2520geometric%2520accuracy%2520of%2520deep%250Aunderwater%2520rendering.%2520We%2520propose%2520decoupled%2520learning%2520for%2520RGB%2520channels%252C%2520guided%2520by%250Athe%2520physics%2520of%2520underwater%2520attenuation%252C%2520to%2520enable%2520more%2520accurate%2520colour%250Arestoration.%2520To%2520address%2520sparse-view%2520limitations%2520and%2520improve%2520view%2520consistency%252C%250Awe%2520introduce%2520a%2520frame%2520interpolation%2520strategy%2520with%2520a%2520novel%2520adaptive%2520weighting%250Ascheme.%2520Additionally%252C%2520we%2520introduce%2520a%2520new%2520loss%2520function%2520aimed%2520at%2520reducing%2520noise%250Awhile%2520preserving%2520edges%252C%2520which%2520is%2520essential%2520for%2520deep-sea%2520content.%2520We%2520also%250Arelease%2520a%2520newly%2520collected%2520dataset%252C%2520Submerged3D%252C%2520captured%2520specifically%2520in%250Adeep-sea%2520environments.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520framework%250Aconsistently%2520outperforms%2520state-of-the-art%2520methods%2520with%2520PSNR%2520gains%2520up%2520to%25201.90dB%252C%250Adelivering%2520superior%2520perceptual%2520quality%2520and%2520robustness%252C%2520and%2520offering%2520promising%250Adirections%2520for%2520marine%2520robotics%2520and%2520underwater%2520visual%2520analytics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15737v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RUSplatting%3A%20Robust%203D%20Gaussian%20Splatting%20for%20Sparse-View%20Underwater%0A%20%20Scene%20Reconstruction&entry.906535625=Zhuodong%20Jiang%20and%20Haoran%20Wang%20and%20Guoxi%20Huang%20and%20Brett%20Seymour%20and%20Nantheera%20Anantrasirichai&entry.1292438233=%20%20Reconstructing%20high-fidelity%20underwater%20scenes%20remains%20a%20challenging%20task%20due%0Ato%20light%20absorption%2C%20scattering%2C%20and%20limited%20visibility%20inherent%20in%20aquatic%0Aenvironments.%20This%20paper%20presents%20an%20enhanced%20Gaussian%20Splatting-based%0Aframework%20that%20improves%20both%20the%20visual%20quality%20and%20geometric%20accuracy%20of%20deep%0Aunderwater%20rendering.%20We%20propose%20decoupled%20learning%20for%20RGB%20channels%2C%20guided%20by%0Athe%20physics%20of%20underwater%20attenuation%2C%20to%20enable%20more%20accurate%20colour%0Arestoration.%20To%20address%20sparse-view%20limitations%20and%20improve%20view%20consistency%2C%0Awe%20introduce%20a%20frame%20interpolation%20strategy%20with%20a%20novel%20adaptive%20weighting%0Ascheme.%20Additionally%2C%20we%20introduce%20a%20new%20loss%20function%20aimed%20at%20reducing%20noise%0Awhile%20preserving%20edges%2C%20which%20is%20essential%20for%20deep-sea%20content.%20We%20also%0Arelease%20a%20newly%20collected%20dataset%2C%20Submerged3D%2C%20captured%20specifically%20in%0Adeep-sea%20environments.%20Experimental%20results%20demonstrate%20that%20our%20framework%0Aconsistently%20outperforms%20state-of-the-art%20methods%20with%20PSNR%20gains%20up%20to%201.90dB%2C%0Adelivering%20superior%20perceptual%20quality%20and%20robustness%2C%20and%20offering%20promising%0Adirections%20for%20marine%20robotics%20and%20underwater%20visual%20analytics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15737v1&entry.124074799=Read"},
{"title": "DLO-Splatting: Tracking Deformable Linear Objects Using 3D Gaussian\n  Splatting", "author": "Holly Dinkel and Marcel B\u00fcsching and Alberta Longhini and Brian Coltin and Trey Smith and Danica Kragic and M\u00e5rten Bj\u00f6rkman and Timothy Bretl", "abstract": "  This work presents DLO-Splatting, an algorithm for estimating the 3D shape of\nDeformable Linear Objects (DLOs) from multi-view RGB images and gripper state\ninformation through prediction-update filtering. The DLO-Splatting algorithm\nuses a position-based dynamics model with shape smoothness and rigidity\ndampening corrections to predict the object shape. Optimization with a 3D\nGaussian Splatting-based rendering loss iteratively renders and refines the\nprediction to align it with the visual observations in the update step. Initial\nexperiments demonstrate promising results in a knot tying scenario, which is\nchallenging for existing vision-only methods.\n", "link": "http://arxiv.org/abs/2505.08644v2", "date": "2025-05-21", "relevancy": 3.1695, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.666}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6574}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DLO-Splatting%3A%20Tracking%20Deformable%20Linear%20Objects%20Using%203D%20Gaussian%0A%20%20Splatting&body=Title%3A%20DLO-Splatting%3A%20Tracking%20Deformable%20Linear%20Objects%20Using%203D%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Holly%20Dinkel%20and%20Marcel%20B%C3%BCsching%20and%20Alberta%20Longhini%20and%20Brian%20Coltin%20and%20Trey%20Smith%20and%20Danica%20Kragic%20and%20M%C3%A5rten%20Bj%C3%B6rkman%20and%20Timothy%20Bretl%0AAbstract%3A%20%20%20This%20work%20presents%20DLO-Splatting%2C%20an%20algorithm%20for%20estimating%20the%203D%20shape%20of%0ADeformable%20Linear%20Objects%20%28DLOs%29%20from%20multi-view%20RGB%20images%20and%20gripper%20state%0Ainformation%20through%20prediction-update%20filtering.%20The%20DLO-Splatting%20algorithm%0Auses%20a%20position-based%20dynamics%20model%20with%20shape%20smoothness%20and%20rigidity%0Adampening%20corrections%20to%20predict%20the%20object%20shape.%20Optimization%20with%20a%203D%0AGaussian%20Splatting-based%20rendering%20loss%20iteratively%20renders%20and%20refines%20the%0Aprediction%20to%20align%20it%20with%20the%20visual%20observations%20in%20the%20update%20step.%20Initial%0Aexperiments%20demonstrate%20promising%20results%20in%20a%20knot%20tying%20scenario%2C%20which%20is%0Achallenging%20for%20existing%20vision-only%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08644v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDLO-Splatting%253A%2520Tracking%2520Deformable%2520Linear%2520Objects%2520Using%25203D%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DHolly%2520Dinkel%2520and%2520Marcel%2520B%25C3%25BCsching%2520and%2520Alberta%2520Longhini%2520and%2520Brian%2520Coltin%2520and%2520Trey%2520Smith%2520and%2520Danica%2520Kragic%2520and%2520M%25C3%25A5rten%2520Bj%25C3%25B6rkman%2520and%2520Timothy%2520Bretl%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520DLO-Splatting%252C%2520an%2520algorithm%2520for%2520estimating%2520the%25203D%2520shape%2520of%250ADeformable%2520Linear%2520Objects%2520%2528DLOs%2529%2520from%2520multi-view%2520RGB%2520images%2520and%2520gripper%2520state%250Ainformation%2520through%2520prediction-update%2520filtering.%2520The%2520DLO-Splatting%2520algorithm%250Auses%2520a%2520position-based%2520dynamics%2520model%2520with%2520shape%2520smoothness%2520and%2520rigidity%250Adampening%2520corrections%2520to%2520predict%2520the%2520object%2520shape.%2520Optimization%2520with%2520a%25203D%250AGaussian%2520Splatting-based%2520rendering%2520loss%2520iteratively%2520renders%2520and%2520refines%2520the%250Aprediction%2520to%2520align%2520it%2520with%2520the%2520visual%2520observations%2520in%2520the%2520update%2520step.%2520Initial%250Aexperiments%2520demonstrate%2520promising%2520results%2520in%2520a%2520knot%2520tying%2520scenario%252C%2520which%2520is%250Achallenging%2520for%2520existing%2520vision-only%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08644v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DLO-Splatting%3A%20Tracking%20Deformable%20Linear%20Objects%20Using%203D%20Gaussian%0A%20%20Splatting&entry.906535625=Holly%20Dinkel%20and%20Marcel%20B%C3%BCsching%20and%20Alberta%20Longhini%20and%20Brian%20Coltin%20and%20Trey%20Smith%20and%20Danica%20Kragic%20and%20M%C3%A5rten%20Bj%C3%B6rkman%20and%20Timothy%20Bretl&entry.1292438233=%20%20This%20work%20presents%20DLO-Splatting%2C%20an%20algorithm%20for%20estimating%20the%203D%20shape%20of%0ADeformable%20Linear%20Objects%20%28DLOs%29%20from%20multi-view%20RGB%20images%20and%20gripper%20state%0Ainformation%20through%20prediction-update%20filtering.%20The%20DLO-Splatting%20algorithm%0Auses%20a%20position-based%20dynamics%20model%20with%20shape%20smoothness%20and%20rigidity%0Adampening%20corrections%20to%20predict%20the%20object%20shape.%20Optimization%20with%20a%203D%0AGaussian%20Splatting-based%20rendering%20loss%20iteratively%20renders%20and%20refines%20the%0Aprediction%20to%20align%20it%20with%20the%20visual%20observations%20in%20the%20update%20step.%20Initial%0Aexperiments%20demonstrate%20promising%20results%20in%20a%20knot%20tying%20scenario%2C%20which%20is%0Achallenging%20for%20existing%20vision-only%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08644v2&entry.124074799=Read"},
{"title": "Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal\n  Reasoning via RL", "author": "Xintong Zhang and Zhi Gao and Bofei Zhang and Pengxiang Li and Xiaowen Zhang and Yang Liu and Tao Yuan and Yuwei Wu and Yunde Jia and Song-Chun Zhu and Qing Li", "abstract": "  Vision language models (VLMs) have achieved impressive performance across a\nvariety of computer vision tasks. However, the multimodal reasoning capability\nhas not been fully explored in existing models. In this paper, we propose a\nChain-of-Focus (CoF) method that allows VLMs to perform adaptive focusing and\nzooming in on key image regions based on obtained visual cues and the given\nquestions, achieving efficient multimodal reasoning. To enable this CoF\ncapability, we present a two-stage training pipeline, including supervised\nfine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we\nconstruct the MM-CoF dataset, comprising 3K samples derived from a visual agent\ndesigned to adaptively identify key regions to solve visual tasks with\ndifferent image resolutions and questions. We use MM-CoF to fine-tune the\nQwen2.5-VL model for cold start. In the RL stage, we leverage the outcome\naccuracies and formats as rewards to update the Qwen2.5-VL model, enabling\nfurther refining the search and reasoning strategy of models without human\npriors. Our model achieves significant improvements on multiple benchmarks. On\nthe V* benchmark that requires strong visual reasoning capability, our model\noutperforms existing VLMs by 5% among 8 image resolutions ranging from 224 to\n4K, demonstrating the effectiveness of the proposed CoF method and facilitating\nthe more efficient deployment of VLMs in practical applications.\n", "link": "http://arxiv.org/abs/2505.15436v1", "date": "2025-05-21", "relevancy": 3.0239, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6199}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6199}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chain-of-Focus%3A%20Adaptive%20Visual%20Search%20and%20Zooming%20for%20Multimodal%0A%20%20Reasoning%20via%20RL&body=Title%3A%20Chain-of-Focus%3A%20Adaptive%20Visual%20Search%20and%20Zooming%20for%20Multimodal%0A%20%20Reasoning%20via%20RL%0AAuthor%3A%20Xintong%20Zhang%20and%20Zhi%20Gao%20and%20Bofei%20Zhang%20and%20Pengxiang%20Li%20and%20Xiaowen%20Zhang%20and%20Yang%20Liu%20and%20Tao%20Yuan%20and%20Yuwei%20Wu%20and%20Yunde%20Jia%20and%20Song-Chun%20Zhu%20and%20Qing%20Li%0AAbstract%3A%20%20%20Vision%20language%20models%20%28VLMs%29%20have%20achieved%20impressive%20performance%20across%20a%0Avariety%20of%20computer%20vision%20tasks.%20However%2C%20the%20multimodal%20reasoning%20capability%0Ahas%20not%20been%20fully%20explored%20in%20existing%20models.%20In%20this%20paper%2C%20we%20propose%20a%0AChain-of-Focus%20%28CoF%29%20method%20that%20allows%20VLMs%20to%20perform%20adaptive%20focusing%20and%0Azooming%20in%20on%20key%20image%20regions%20based%20on%20obtained%20visual%20cues%20and%20the%20given%0Aquestions%2C%20achieving%20efficient%20multimodal%20reasoning.%20To%20enable%20this%20CoF%0Acapability%2C%20we%20present%20a%20two-stage%20training%20pipeline%2C%20including%20supervised%0Afine-tuning%20%28SFT%29%20and%20reinforcement%20learning%20%28RL%29.%20In%20the%20SFT%20stage%2C%20we%0Aconstruct%20the%20MM-CoF%20dataset%2C%20comprising%203K%20samples%20derived%20from%20a%20visual%20agent%0Adesigned%20to%20adaptively%20identify%20key%20regions%20to%20solve%20visual%20tasks%20with%0Adifferent%20image%20resolutions%20and%20questions.%20We%20use%20MM-CoF%20to%20fine-tune%20the%0AQwen2.5-VL%20model%20for%20cold%20start.%20In%20the%20RL%20stage%2C%20we%20leverage%20the%20outcome%0Aaccuracies%20and%20formats%20as%20rewards%20to%20update%20the%20Qwen2.5-VL%20model%2C%20enabling%0Afurther%20refining%20the%20search%20and%20reasoning%20strategy%20of%20models%20without%20human%0Apriors.%20Our%20model%20achieves%20significant%20improvements%20on%20multiple%20benchmarks.%20On%0Athe%20V%2A%20benchmark%20that%20requires%20strong%20visual%20reasoning%20capability%2C%20our%20model%0Aoutperforms%20existing%20VLMs%20by%205%25%20among%208%20image%20resolutions%20ranging%20from%20224%20to%0A4K%2C%20demonstrating%20the%20effectiveness%20of%20the%20proposed%20CoF%20method%20and%20facilitating%0Athe%20more%20efficient%20deployment%20of%20VLMs%20in%20practical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15436v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChain-of-Focus%253A%2520Adaptive%2520Visual%2520Search%2520and%2520Zooming%2520for%2520Multimodal%250A%2520%2520Reasoning%2520via%2520RL%26entry.906535625%3DXintong%2520Zhang%2520and%2520Zhi%2520Gao%2520and%2520Bofei%2520Zhang%2520and%2520Pengxiang%2520Li%2520and%2520Xiaowen%2520Zhang%2520and%2520Yang%2520Liu%2520and%2520Tao%2520Yuan%2520and%2520Yuwei%2520Wu%2520and%2520Yunde%2520Jia%2520and%2520Song-Chun%2520Zhu%2520and%2520Qing%2520Li%26entry.1292438233%3D%2520%2520Vision%2520language%2520models%2520%2528VLMs%2529%2520have%2520achieved%2520impressive%2520performance%2520across%2520a%250Avariety%2520of%2520computer%2520vision%2520tasks.%2520However%252C%2520the%2520multimodal%2520reasoning%2520capability%250Ahas%2520not%2520been%2520fully%2520explored%2520in%2520existing%2520models.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250AChain-of-Focus%2520%2528CoF%2529%2520method%2520that%2520allows%2520VLMs%2520to%2520perform%2520adaptive%2520focusing%2520and%250Azooming%2520in%2520on%2520key%2520image%2520regions%2520based%2520on%2520obtained%2520visual%2520cues%2520and%2520the%2520given%250Aquestions%252C%2520achieving%2520efficient%2520multimodal%2520reasoning.%2520To%2520enable%2520this%2520CoF%250Acapability%252C%2520we%2520present%2520a%2520two-stage%2520training%2520pipeline%252C%2520including%2520supervised%250Afine-tuning%2520%2528SFT%2529%2520and%2520reinforcement%2520learning%2520%2528RL%2529.%2520In%2520the%2520SFT%2520stage%252C%2520we%250Aconstruct%2520the%2520MM-CoF%2520dataset%252C%2520comprising%25203K%2520samples%2520derived%2520from%2520a%2520visual%2520agent%250Adesigned%2520to%2520adaptively%2520identify%2520key%2520regions%2520to%2520solve%2520visual%2520tasks%2520with%250Adifferent%2520image%2520resolutions%2520and%2520questions.%2520We%2520use%2520MM-CoF%2520to%2520fine-tune%2520the%250AQwen2.5-VL%2520model%2520for%2520cold%2520start.%2520In%2520the%2520RL%2520stage%252C%2520we%2520leverage%2520the%2520outcome%250Aaccuracies%2520and%2520formats%2520as%2520rewards%2520to%2520update%2520the%2520Qwen2.5-VL%2520model%252C%2520enabling%250Afurther%2520refining%2520the%2520search%2520and%2520reasoning%2520strategy%2520of%2520models%2520without%2520human%250Apriors.%2520Our%2520model%2520achieves%2520significant%2520improvements%2520on%2520multiple%2520benchmarks.%2520On%250Athe%2520V%252A%2520benchmark%2520that%2520requires%2520strong%2520visual%2520reasoning%2520capability%252C%2520our%2520model%250Aoutperforms%2520existing%2520VLMs%2520by%25205%2525%2520among%25208%2520image%2520resolutions%2520ranging%2520from%2520224%2520to%250A4K%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520the%2520proposed%2520CoF%2520method%2520and%2520facilitating%250Athe%2520more%2520efficient%2520deployment%2520of%2520VLMs%2520in%2520practical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15436v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chain-of-Focus%3A%20Adaptive%20Visual%20Search%20and%20Zooming%20for%20Multimodal%0A%20%20Reasoning%20via%20RL&entry.906535625=Xintong%20Zhang%20and%20Zhi%20Gao%20and%20Bofei%20Zhang%20and%20Pengxiang%20Li%20and%20Xiaowen%20Zhang%20and%20Yang%20Liu%20and%20Tao%20Yuan%20and%20Yuwei%20Wu%20and%20Yunde%20Jia%20and%20Song-Chun%20Zhu%20and%20Qing%20Li&entry.1292438233=%20%20Vision%20language%20models%20%28VLMs%29%20have%20achieved%20impressive%20performance%20across%20a%0Avariety%20of%20computer%20vision%20tasks.%20However%2C%20the%20multimodal%20reasoning%20capability%0Ahas%20not%20been%20fully%20explored%20in%20existing%20models.%20In%20this%20paper%2C%20we%20propose%20a%0AChain-of-Focus%20%28CoF%29%20method%20that%20allows%20VLMs%20to%20perform%20adaptive%20focusing%20and%0Azooming%20in%20on%20key%20image%20regions%20based%20on%20obtained%20visual%20cues%20and%20the%20given%0Aquestions%2C%20achieving%20efficient%20multimodal%20reasoning.%20To%20enable%20this%20CoF%0Acapability%2C%20we%20present%20a%20two-stage%20training%20pipeline%2C%20including%20supervised%0Afine-tuning%20%28SFT%29%20and%20reinforcement%20learning%20%28RL%29.%20In%20the%20SFT%20stage%2C%20we%0Aconstruct%20the%20MM-CoF%20dataset%2C%20comprising%203K%20samples%20derived%20from%20a%20visual%20agent%0Adesigned%20to%20adaptively%20identify%20key%20regions%20to%20solve%20visual%20tasks%20with%0Adifferent%20image%20resolutions%20and%20questions.%20We%20use%20MM-CoF%20to%20fine-tune%20the%0AQwen2.5-VL%20model%20for%20cold%20start.%20In%20the%20RL%20stage%2C%20we%20leverage%20the%20outcome%0Aaccuracies%20and%20formats%20as%20rewards%20to%20update%20the%20Qwen2.5-VL%20model%2C%20enabling%0Afurther%20refining%20the%20search%20and%20reasoning%20strategy%20of%20models%20without%20human%0Apriors.%20Our%20model%20achieves%20significant%20improvements%20on%20multiple%20benchmarks.%20On%0Athe%20V%2A%20benchmark%20that%20requires%20strong%20visual%20reasoning%20capability%2C%20our%20model%0Aoutperforms%20existing%20VLMs%20by%205%25%20among%208%20image%20resolutions%20ranging%20from%20224%20to%0A4K%2C%20demonstrating%20the%20effectiveness%20of%20the%20proposed%20CoF%20method%20and%20facilitating%0Athe%20more%20efficient%20deployment%20of%20VLMs%20in%20practical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15436v1&entry.124074799=Read"},
{"title": "P3P: Pseudo-3D Pre-training for Scaling 3D Voxel-based Masked\n  Autoencoders", "author": "Xuechao Chen and Ying Chen and Jialin Li and Qiang Nie and Hanqiu Deng and Yong Liu and Qixing Huang and Yang Li", "abstract": "  3D pre-training is crucial to 3D perception tasks. Nevertheless, limited by\nthe difficulties in collecting clean and complete 3D data, 3D pre-training has\npersistently faced data scaling challenges. In this work, we introduce a novel\nself-supervised pre-training framework that incorporates millions of images\ninto 3D pre-training corpora by leveraging a large depth estimation model. New\npre-training corpora encounter new challenges in representation ability and\nembedding efficiency of models. Previous pre-training methods rely on farthest\npoint sampling and k-nearest neighbors to embed a fixed number of 3D tokens.\nHowever, these approaches prove inadequate when it comes to embedding millions\nof samples that feature a diverse range of point numbers, spanning from 1,000\nto 100,000. In contrast, we propose a tokenizer with linear-time complexity,\nwhich enables the efficient embedding of a flexible number of tokens.\nAccordingly, a new 3D reconstruction target is proposed to cooperate with our\n3D tokenizer. Our method achieves state-of-the-art performance in 3D\nclassification, few-shot learning, and 3D segmentation. Code is available at\nhttps://github.com/XuechaoChen/P3P-MAE.\n", "link": "http://arxiv.org/abs/2408.10007v3", "date": "2025-05-21", "relevancy": 3.0201, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6068}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6026}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20P3P%3A%20Pseudo-3D%20Pre-training%20for%20Scaling%203D%20Voxel-based%20Masked%0A%20%20Autoencoders&body=Title%3A%20P3P%3A%20Pseudo-3D%20Pre-training%20for%20Scaling%203D%20Voxel-based%20Masked%0A%20%20Autoencoders%0AAuthor%3A%20Xuechao%20Chen%20and%20Ying%20Chen%20and%20Jialin%20Li%20and%20Qiang%20Nie%20and%20Hanqiu%20Deng%20and%20Yong%20Liu%20and%20Qixing%20Huang%20and%20Yang%20Li%0AAbstract%3A%20%20%203D%20pre-training%20is%20crucial%20to%203D%20perception%20tasks.%20Nevertheless%2C%20limited%20by%0Athe%20difficulties%20in%20collecting%20clean%20and%20complete%203D%20data%2C%203D%20pre-training%20has%0Apersistently%20faced%20data%20scaling%20challenges.%20In%20this%20work%2C%20we%20introduce%20a%20novel%0Aself-supervised%20pre-training%20framework%20that%20incorporates%20millions%20of%20images%0Ainto%203D%20pre-training%20corpora%20by%20leveraging%20a%20large%20depth%20estimation%20model.%20New%0Apre-training%20corpora%20encounter%20new%20challenges%20in%20representation%20ability%20and%0Aembedding%20efficiency%20of%20models.%20Previous%20pre-training%20methods%20rely%20on%20farthest%0Apoint%20sampling%20and%20k-nearest%20neighbors%20to%20embed%20a%20fixed%20number%20of%203D%20tokens.%0AHowever%2C%20these%20approaches%20prove%20inadequate%20when%20it%20comes%20to%20embedding%20millions%0Aof%20samples%20that%20feature%20a%20diverse%20range%20of%20point%20numbers%2C%20spanning%20from%201%2C000%0Ato%20100%2C000.%20In%20contrast%2C%20we%20propose%20a%20tokenizer%20with%20linear-time%20complexity%2C%0Awhich%20enables%20the%20efficient%20embedding%20of%20a%20flexible%20number%20of%20tokens.%0AAccordingly%2C%20a%20new%203D%20reconstruction%20target%20is%20proposed%20to%20cooperate%20with%20our%0A3D%20tokenizer.%20Our%20method%20achieves%20state-of-the-art%20performance%20in%203D%0Aclassification%2C%20few-shot%20learning%2C%20and%203D%20segmentation.%20Code%20is%20available%20at%0Ahttps%3A//github.com/XuechaoChen/P3P-MAE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10007v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DP3P%253A%2520Pseudo-3D%2520Pre-training%2520for%2520Scaling%25203D%2520Voxel-based%2520Masked%250A%2520%2520Autoencoders%26entry.906535625%3DXuechao%2520Chen%2520and%2520Ying%2520Chen%2520and%2520Jialin%2520Li%2520and%2520Qiang%2520Nie%2520and%2520Hanqiu%2520Deng%2520and%2520Yong%2520Liu%2520and%2520Qixing%2520Huang%2520and%2520Yang%2520Li%26entry.1292438233%3D%2520%25203D%2520pre-training%2520is%2520crucial%2520to%25203D%2520perception%2520tasks.%2520Nevertheless%252C%2520limited%2520by%250Athe%2520difficulties%2520in%2520collecting%2520clean%2520and%2520complete%25203D%2520data%252C%25203D%2520pre-training%2520has%250Apersistently%2520faced%2520data%2520scaling%2520challenges.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%250Aself-supervised%2520pre-training%2520framework%2520that%2520incorporates%2520millions%2520of%2520images%250Ainto%25203D%2520pre-training%2520corpora%2520by%2520leveraging%2520a%2520large%2520depth%2520estimation%2520model.%2520New%250Apre-training%2520corpora%2520encounter%2520new%2520challenges%2520in%2520representation%2520ability%2520and%250Aembedding%2520efficiency%2520of%2520models.%2520Previous%2520pre-training%2520methods%2520rely%2520on%2520farthest%250Apoint%2520sampling%2520and%2520k-nearest%2520neighbors%2520to%2520embed%2520a%2520fixed%2520number%2520of%25203D%2520tokens.%250AHowever%252C%2520these%2520approaches%2520prove%2520inadequate%2520when%2520it%2520comes%2520to%2520embedding%2520millions%250Aof%2520samples%2520that%2520feature%2520a%2520diverse%2520range%2520of%2520point%2520numbers%252C%2520spanning%2520from%25201%252C000%250Ato%2520100%252C000.%2520In%2520contrast%252C%2520we%2520propose%2520a%2520tokenizer%2520with%2520linear-time%2520complexity%252C%250Awhich%2520enables%2520the%2520efficient%2520embedding%2520of%2520a%2520flexible%2520number%2520of%2520tokens.%250AAccordingly%252C%2520a%2520new%25203D%2520reconstruction%2520target%2520is%2520proposed%2520to%2520cooperate%2520with%2520our%250A3D%2520tokenizer.%2520Our%2520method%2520achieves%2520state-of-the-art%2520performance%2520in%25203D%250Aclassification%252C%2520few-shot%2520learning%252C%2520and%25203D%2520segmentation.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/XuechaoChen/P3P-MAE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10007v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=P3P%3A%20Pseudo-3D%20Pre-training%20for%20Scaling%203D%20Voxel-based%20Masked%0A%20%20Autoencoders&entry.906535625=Xuechao%20Chen%20and%20Ying%20Chen%20and%20Jialin%20Li%20and%20Qiang%20Nie%20and%20Hanqiu%20Deng%20and%20Yong%20Liu%20and%20Qixing%20Huang%20and%20Yang%20Li&entry.1292438233=%20%203D%20pre-training%20is%20crucial%20to%203D%20perception%20tasks.%20Nevertheless%2C%20limited%20by%0Athe%20difficulties%20in%20collecting%20clean%20and%20complete%203D%20data%2C%203D%20pre-training%20has%0Apersistently%20faced%20data%20scaling%20challenges.%20In%20this%20work%2C%20we%20introduce%20a%20novel%0Aself-supervised%20pre-training%20framework%20that%20incorporates%20millions%20of%20images%0Ainto%203D%20pre-training%20corpora%20by%20leveraging%20a%20large%20depth%20estimation%20model.%20New%0Apre-training%20corpora%20encounter%20new%20challenges%20in%20representation%20ability%20and%0Aembedding%20efficiency%20of%20models.%20Previous%20pre-training%20methods%20rely%20on%20farthest%0Apoint%20sampling%20and%20k-nearest%20neighbors%20to%20embed%20a%20fixed%20number%20of%203D%20tokens.%0AHowever%2C%20these%20approaches%20prove%20inadequate%20when%20it%20comes%20to%20embedding%20millions%0Aof%20samples%20that%20feature%20a%20diverse%20range%20of%20point%20numbers%2C%20spanning%20from%201%2C000%0Ato%20100%2C000.%20In%20contrast%2C%20we%20propose%20a%20tokenizer%20with%20linear-time%20complexity%2C%0Awhich%20enables%20the%20efficient%20embedding%20of%20a%20flexible%20number%20of%20tokens.%0AAccordingly%2C%20a%20new%203D%20reconstruction%20target%20is%20proposed%20to%20cooperate%20with%20our%0A3D%20tokenizer.%20Our%20method%20achieves%20state-of-the-art%20performance%20in%203D%0Aclassification%2C%20few-shot%20learning%2C%20and%203D%20segmentation.%20Code%20is%20available%20at%0Ahttps%3A//github.com/XuechaoChen/P3P-MAE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10007v3&entry.124074799=Read"},
{"title": "Meta-Learning an In-Context Transformer Model of Human Higher Visual\n  Cortex", "author": "Muquan Yu and Mu Nan and Hossein Adeli and Jacob S. Prince and John A. Pyles and Leila Wehbe and Margaret M. Henderson and Michael J. Tarr and Andrew F. Luo", "abstract": "  Understanding functional representations within higher visual cortex is a\nfundamental question in computational neuroscience. While artificial neural\nnetworks pretrained on large-scale datasets exhibit striking representational\nalignment with human neural responses, learning image-computable models of\nvisual cortex relies on individual-level, large-scale fMRI datasets. The\nnecessity for expensive, time-intensive, and often impractical data acquisition\nlimits the generalizability of encoders to new subjects and stimuli. BraInCoRL\nuses in-context learning to predict voxelwise neural responses from few-shot\nexamples without any additional finetuning for novel subjects and stimuli. We\nleverage a transformer architecture that can flexibly condition on a variable\nnumber of in-context image stimuli, learning an inductive bias over multiple\nsubjects. During training, we explicitly optimize the model for in-context\nlearning. By jointly conditioning on image features and voxel activations, our\nmodel learns to directly generate better performing voxelwise models of higher\nvisual cortex. We demonstrate that BraInCoRL consistently outperforms existing\nvoxelwise encoder designs in a low-data regime when evaluated on entirely novel\nimages, while also exhibiting strong test-time scaling behavior. The model also\ngeneralizes to an entirely new visual fMRI dataset, which uses different\nsubjects and fMRI data acquisition parameters. Further, BraInCoRL facilitates\nbetter interpretability of neural signals in higher visual cortex by attending\nto semantically relevant stimuli. Finally, we show that our framework enables\ninterpretable mappings from natural language queries to voxel selectivity.\n", "link": "http://arxiv.org/abs/2505.15813v1", "date": "2025-05-21", "relevancy": 3.0072, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6148}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6148}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meta-Learning%20an%20In-Context%20Transformer%20Model%20of%20Human%20Higher%20Visual%0A%20%20Cortex&body=Title%3A%20Meta-Learning%20an%20In-Context%20Transformer%20Model%20of%20Human%20Higher%20Visual%0A%20%20Cortex%0AAuthor%3A%20Muquan%20Yu%20and%20Mu%20Nan%20and%20Hossein%20Adeli%20and%20Jacob%20S.%20Prince%20and%20John%20A.%20Pyles%20and%20Leila%20Wehbe%20and%20Margaret%20M.%20Henderson%20and%20Michael%20J.%20Tarr%20and%20Andrew%20F.%20Luo%0AAbstract%3A%20%20%20Understanding%20functional%20representations%20within%20higher%20visual%20cortex%20is%20a%0Afundamental%20question%20in%20computational%20neuroscience.%20While%20artificial%20neural%0Anetworks%20pretrained%20on%20large-scale%20datasets%20exhibit%20striking%20representational%0Aalignment%20with%20human%20neural%20responses%2C%20learning%20image-computable%20models%20of%0Avisual%20cortex%20relies%20on%20individual-level%2C%20large-scale%20fMRI%20datasets.%20The%0Anecessity%20for%20expensive%2C%20time-intensive%2C%20and%20often%20impractical%20data%20acquisition%0Alimits%20the%20generalizability%20of%20encoders%20to%20new%20subjects%20and%20stimuli.%20BraInCoRL%0Auses%20in-context%20learning%20to%20predict%20voxelwise%20neural%20responses%20from%20few-shot%0Aexamples%20without%20any%20additional%20finetuning%20for%20novel%20subjects%20and%20stimuli.%20We%0Aleverage%20a%20transformer%20architecture%20that%20can%20flexibly%20condition%20on%20a%20variable%0Anumber%20of%20in-context%20image%20stimuli%2C%20learning%20an%20inductive%20bias%20over%20multiple%0Asubjects.%20During%20training%2C%20we%20explicitly%20optimize%20the%20model%20for%20in-context%0Alearning.%20By%20jointly%20conditioning%20on%20image%20features%20and%20voxel%20activations%2C%20our%0Amodel%20learns%20to%20directly%20generate%20better%20performing%20voxelwise%20models%20of%20higher%0Avisual%20cortex.%20We%20demonstrate%20that%20BraInCoRL%20consistently%20outperforms%20existing%0Avoxelwise%20encoder%20designs%20in%20a%20low-data%20regime%20when%20evaluated%20on%20entirely%20novel%0Aimages%2C%20while%20also%20exhibiting%20strong%20test-time%20scaling%20behavior.%20The%20model%20also%0Ageneralizes%20to%20an%20entirely%20new%20visual%20fMRI%20dataset%2C%20which%20uses%20different%0Asubjects%20and%20fMRI%20data%20acquisition%20parameters.%20Further%2C%20BraInCoRL%20facilitates%0Abetter%20interpretability%20of%20neural%20signals%20in%20higher%20visual%20cortex%20by%20attending%0Ato%20semantically%20relevant%20stimuli.%20Finally%2C%20we%20show%20that%20our%20framework%20enables%0Ainterpretable%20mappings%20from%20natural%20language%20queries%20to%20voxel%20selectivity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15813v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeta-Learning%2520an%2520In-Context%2520Transformer%2520Model%2520of%2520Human%2520Higher%2520Visual%250A%2520%2520Cortex%26entry.906535625%3DMuquan%2520Yu%2520and%2520Mu%2520Nan%2520and%2520Hossein%2520Adeli%2520and%2520Jacob%2520S.%2520Prince%2520and%2520John%2520A.%2520Pyles%2520and%2520Leila%2520Wehbe%2520and%2520Margaret%2520M.%2520Henderson%2520and%2520Michael%2520J.%2520Tarr%2520and%2520Andrew%2520F.%2520Luo%26entry.1292438233%3D%2520%2520Understanding%2520functional%2520representations%2520within%2520higher%2520visual%2520cortex%2520is%2520a%250Afundamental%2520question%2520in%2520computational%2520neuroscience.%2520While%2520artificial%2520neural%250Anetworks%2520pretrained%2520on%2520large-scale%2520datasets%2520exhibit%2520striking%2520representational%250Aalignment%2520with%2520human%2520neural%2520responses%252C%2520learning%2520image-computable%2520models%2520of%250Avisual%2520cortex%2520relies%2520on%2520individual-level%252C%2520large-scale%2520fMRI%2520datasets.%2520The%250Anecessity%2520for%2520expensive%252C%2520time-intensive%252C%2520and%2520often%2520impractical%2520data%2520acquisition%250Alimits%2520the%2520generalizability%2520of%2520encoders%2520to%2520new%2520subjects%2520and%2520stimuli.%2520BraInCoRL%250Auses%2520in-context%2520learning%2520to%2520predict%2520voxelwise%2520neural%2520responses%2520from%2520few-shot%250Aexamples%2520without%2520any%2520additional%2520finetuning%2520for%2520novel%2520subjects%2520and%2520stimuli.%2520We%250Aleverage%2520a%2520transformer%2520architecture%2520that%2520can%2520flexibly%2520condition%2520on%2520a%2520variable%250Anumber%2520of%2520in-context%2520image%2520stimuli%252C%2520learning%2520an%2520inductive%2520bias%2520over%2520multiple%250Asubjects.%2520During%2520training%252C%2520we%2520explicitly%2520optimize%2520the%2520model%2520for%2520in-context%250Alearning.%2520By%2520jointly%2520conditioning%2520on%2520image%2520features%2520and%2520voxel%2520activations%252C%2520our%250Amodel%2520learns%2520to%2520directly%2520generate%2520better%2520performing%2520voxelwise%2520models%2520of%2520higher%250Avisual%2520cortex.%2520We%2520demonstrate%2520that%2520BraInCoRL%2520consistently%2520outperforms%2520existing%250Avoxelwise%2520encoder%2520designs%2520in%2520a%2520low-data%2520regime%2520when%2520evaluated%2520on%2520entirely%2520novel%250Aimages%252C%2520while%2520also%2520exhibiting%2520strong%2520test-time%2520scaling%2520behavior.%2520The%2520model%2520also%250Ageneralizes%2520to%2520an%2520entirely%2520new%2520visual%2520fMRI%2520dataset%252C%2520which%2520uses%2520different%250Asubjects%2520and%2520fMRI%2520data%2520acquisition%2520parameters.%2520Further%252C%2520BraInCoRL%2520facilitates%250Abetter%2520interpretability%2520of%2520neural%2520signals%2520in%2520higher%2520visual%2520cortex%2520by%2520attending%250Ato%2520semantically%2520relevant%2520stimuli.%2520Finally%252C%2520we%2520show%2520that%2520our%2520framework%2520enables%250Ainterpretable%2520mappings%2520from%2520natural%2520language%2520queries%2520to%2520voxel%2520selectivity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15813v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta-Learning%20an%20In-Context%20Transformer%20Model%20of%20Human%20Higher%20Visual%0A%20%20Cortex&entry.906535625=Muquan%20Yu%20and%20Mu%20Nan%20and%20Hossein%20Adeli%20and%20Jacob%20S.%20Prince%20and%20John%20A.%20Pyles%20and%20Leila%20Wehbe%20and%20Margaret%20M.%20Henderson%20and%20Michael%20J.%20Tarr%20and%20Andrew%20F.%20Luo&entry.1292438233=%20%20Understanding%20functional%20representations%20within%20higher%20visual%20cortex%20is%20a%0Afundamental%20question%20in%20computational%20neuroscience.%20While%20artificial%20neural%0Anetworks%20pretrained%20on%20large-scale%20datasets%20exhibit%20striking%20representational%0Aalignment%20with%20human%20neural%20responses%2C%20learning%20image-computable%20models%20of%0Avisual%20cortex%20relies%20on%20individual-level%2C%20large-scale%20fMRI%20datasets.%20The%0Anecessity%20for%20expensive%2C%20time-intensive%2C%20and%20often%20impractical%20data%20acquisition%0Alimits%20the%20generalizability%20of%20encoders%20to%20new%20subjects%20and%20stimuli.%20BraInCoRL%0Auses%20in-context%20learning%20to%20predict%20voxelwise%20neural%20responses%20from%20few-shot%0Aexamples%20without%20any%20additional%20finetuning%20for%20novel%20subjects%20and%20stimuli.%20We%0Aleverage%20a%20transformer%20architecture%20that%20can%20flexibly%20condition%20on%20a%20variable%0Anumber%20of%20in-context%20image%20stimuli%2C%20learning%20an%20inductive%20bias%20over%20multiple%0Asubjects.%20During%20training%2C%20we%20explicitly%20optimize%20the%20model%20for%20in-context%0Alearning.%20By%20jointly%20conditioning%20on%20image%20features%20and%20voxel%20activations%2C%20our%0Amodel%20learns%20to%20directly%20generate%20better%20performing%20voxelwise%20models%20of%20higher%0Avisual%20cortex.%20We%20demonstrate%20that%20BraInCoRL%20consistently%20outperforms%20existing%0Avoxelwise%20encoder%20designs%20in%20a%20low-data%20regime%20when%20evaluated%20on%20entirely%20novel%0Aimages%2C%20while%20also%20exhibiting%20strong%20test-time%20scaling%20behavior.%20The%20model%20also%0Ageneralizes%20to%20an%20entirely%20new%20visual%20fMRI%20dataset%2C%20which%20uses%20different%0Asubjects%20and%20fMRI%20data%20acquisition%20parameters.%20Further%2C%20BraInCoRL%20facilitates%0Abetter%20interpretability%20of%20neural%20signals%20in%20higher%20visual%20cortex%20by%20attending%0Ato%20semantically%20relevant%20stimuli.%20Finally%2C%20we%20show%20that%20our%20framework%20enables%0Ainterpretable%20mappings%20from%20natural%20language%20queries%20to%20voxel%20selectivity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15813v1&entry.124074799=Read"},
{"title": "LENS: Multi-level Evaluation of Multimodal Reasoning with Large Language\n  Models", "author": "Ruilin Yao and Bo Zhang and Jirui Huang and Xinwei Long and Yifang Zhang and Tianyu Zou and Yufei Wu and Shichao Su and Yifan Xu and Wenxi Zeng and Zhaoyu Yang and Guoyou Li and Shilan Zhang and Zichan Li and Yaxiong Chen and Shengwu Xiong and Peng Xu and Jiajun Zhang and Bowen Zhou and David Clifton and Luc Van Gool", "abstract": "  Multimodal Large Language Models (MLLMs) have achieved significant advances\nin integrating visual and linguistic information, yet their ability to reason\nabout complex and real-world scenarios remains limited. The existing benchmarks\nare usually constructed in the task-oriented manner without guarantee that\ndifferent task samples come from the same data distribution, thus they often\nfall short in evaluating the synergistic effects of lower-level perceptual\ncapabilities on higher-order reasoning. To lift this limitation, we contribute\nLens, a multi-level benchmark with 3.4K contemporary images and 60K+\nhuman-authored questions covering eight tasks and 12 daily scenarios, forming\nthree progressive task tiers, i.e., perception, understanding, and reasoning.\nOne feature is that each image is equipped with rich annotations for all tasks.\nThus, this dataset intrinsically supports to evaluate MLLMs to handle\nimage-invariable prompts, from basic perception to compositional reasoning. In\naddition, our images are manully collected from the social media, in which 53%\nwere published later than Jan. 2025. We evaluate 15+ frontier MLLMs such as\nQwen2.5-VL-72B, InternVL3-78B, GPT-4o and two reasoning models QVQ-72B-preview\nand Kimi-VL. These models are released later than Dec. 2024, and none of them\nachieve an accuracy greater than 60% in the reasoning tasks. Project page:\nhttps://github.com/Lens4MLLMs/lens. ICCV 2025 workshop page:\nhttps://lens4mllms.github.io/mars2-workshop-iccv2025/\n", "link": "http://arxiv.org/abs/2505.15616v1", "date": "2025-05-21", "relevancy": 2.9549, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5985}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5985}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LENS%3A%20Multi-level%20Evaluation%20of%20Multimodal%20Reasoning%20with%20Large%20Language%0A%20%20Models&body=Title%3A%20LENS%3A%20Multi-level%20Evaluation%20of%20Multimodal%20Reasoning%20with%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Ruilin%20Yao%20and%20Bo%20Zhang%20and%20Jirui%20Huang%20and%20Xinwei%20Long%20and%20Yifang%20Zhang%20and%20Tianyu%20Zou%20and%20Yufei%20Wu%20and%20Shichao%20Su%20and%20Yifan%20Xu%20and%20Wenxi%20Zeng%20and%20Zhaoyu%20Yang%20and%20Guoyou%20Li%20and%20Shilan%20Zhang%20and%20Zichan%20Li%20and%20Yaxiong%20Chen%20and%20Shengwu%20Xiong%20and%20Peng%20Xu%20and%20Jiajun%20Zhang%20and%20Bowen%20Zhou%20and%20David%20Clifton%20and%20Luc%20Van%20Gool%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20significant%20advances%0Ain%20integrating%20visual%20and%20linguistic%20information%2C%20yet%20their%20ability%20to%20reason%0Aabout%20complex%20and%20real-world%20scenarios%20remains%20limited.%20The%20existing%20benchmarks%0Aare%20usually%20constructed%20in%20the%20task-oriented%20manner%20without%20guarantee%20that%0Adifferent%20task%20samples%20come%20from%20the%20same%20data%20distribution%2C%20thus%20they%20often%0Afall%20short%20in%20evaluating%20the%20synergistic%20effects%20of%20lower-level%20perceptual%0Acapabilities%20on%20higher-order%20reasoning.%20To%20lift%20this%20limitation%2C%20we%20contribute%0ALens%2C%20a%20multi-level%20benchmark%20with%203.4K%20contemporary%20images%20and%2060K%2B%0Ahuman-authored%20questions%20covering%20eight%20tasks%20and%2012%20daily%20scenarios%2C%20forming%0Athree%20progressive%20task%20tiers%2C%20i.e.%2C%20perception%2C%20understanding%2C%20and%20reasoning.%0AOne%20feature%20is%20that%20each%20image%20is%20equipped%20with%20rich%20annotations%20for%20all%20tasks.%0AThus%2C%20this%20dataset%20intrinsically%20supports%20to%20evaluate%20MLLMs%20to%20handle%0Aimage-invariable%20prompts%2C%20from%20basic%20perception%20to%20compositional%20reasoning.%20In%0Aaddition%2C%20our%20images%20are%20manully%20collected%20from%20the%20social%20media%2C%20in%20which%2053%25%0Awere%20published%20later%20than%20Jan.%202025.%20We%20evaluate%2015%2B%20frontier%20MLLMs%20such%20as%0AQwen2.5-VL-72B%2C%20InternVL3-78B%2C%20GPT-4o%20and%20two%20reasoning%20models%20QVQ-72B-preview%0Aand%20Kimi-VL.%20These%20models%20are%20released%20later%20than%20Dec.%202024%2C%20and%20none%20of%20them%0Aachieve%20an%20accuracy%20greater%20than%2060%25%20in%20the%20reasoning%20tasks.%20Project%20page%3A%0Ahttps%3A//github.com/Lens4MLLMs/lens.%20ICCV%202025%20workshop%20page%3A%0Ahttps%3A//lens4mllms.github.io/mars2-workshop-iccv2025/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15616v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLENS%253A%2520Multi-level%2520Evaluation%2520of%2520Multimodal%2520Reasoning%2520with%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DRuilin%2520Yao%2520and%2520Bo%2520Zhang%2520and%2520Jirui%2520Huang%2520and%2520Xinwei%2520Long%2520and%2520Yifang%2520Zhang%2520and%2520Tianyu%2520Zou%2520and%2520Yufei%2520Wu%2520and%2520Shichao%2520Su%2520and%2520Yifan%2520Xu%2520and%2520Wenxi%2520Zeng%2520and%2520Zhaoyu%2520Yang%2520and%2520Guoyou%2520Li%2520and%2520Shilan%2520Zhang%2520and%2520Zichan%2520Li%2520and%2520Yaxiong%2520Chen%2520and%2520Shengwu%2520Xiong%2520and%2520Peng%2520Xu%2520and%2520Jiajun%2520Zhang%2520and%2520Bowen%2520Zhou%2520and%2520David%2520Clifton%2520and%2520Luc%2520Van%2520Gool%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520achieved%2520significant%2520advances%250Ain%2520integrating%2520visual%2520and%2520linguistic%2520information%252C%2520yet%2520their%2520ability%2520to%2520reason%250Aabout%2520complex%2520and%2520real-world%2520scenarios%2520remains%2520limited.%2520The%2520existing%2520benchmarks%250Aare%2520usually%2520constructed%2520in%2520the%2520task-oriented%2520manner%2520without%2520guarantee%2520that%250Adifferent%2520task%2520samples%2520come%2520from%2520the%2520same%2520data%2520distribution%252C%2520thus%2520they%2520often%250Afall%2520short%2520in%2520evaluating%2520the%2520synergistic%2520effects%2520of%2520lower-level%2520perceptual%250Acapabilities%2520on%2520higher-order%2520reasoning.%2520To%2520lift%2520this%2520limitation%252C%2520we%2520contribute%250ALens%252C%2520a%2520multi-level%2520benchmark%2520with%25203.4K%2520contemporary%2520images%2520and%252060K%252B%250Ahuman-authored%2520questions%2520covering%2520eight%2520tasks%2520and%252012%2520daily%2520scenarios%252C%2520forming%250Athree%2520progressive%2520task%2520tiers%252C%2520i.e.%252C%2520perception%252C%2520understanding%252C%2520and%2520reasoning.%250AOne%2520feature%2520is%2520that%2520each%2520image%2520is%2520equipped%2520with%2520rich%2520annotations%2520for%2520all%2520tasks.%250AThus%252C%2520this%2520dataset%2520intrinsically%2520supports%2520to%2520evaluate%2520MLLMs%2520to%2520handle%250Aimage-invariable%2520prompts%252C%2520from%2520basic%2520perception%2520to%2520compositional%2520reasoning.%2520In%250Aaddition%252C%2520our%2520images%2520are%2520manully%2520collected%2520from%2520the%2520social%2520media%252C%2520in%2520which%252053%2525%250Awere%2520published%2520later%2520than%2520Jan.%25202025.%2520We%2520evaluate%252015%252B%2520frontier%2520MLLMs%2520such%2520as%250AQwen2.5-VL-72B%252C%2520InternVL3-78B%252C%2520GPT-4o%2520and%2520two%2520reasoning%2520models%2520QVQ-72B-preview%250Aand%2520Kimi-VL.%2520These%2520models%2520are%2520released%2520later%2520than%2520Dec.%25202024%252C%2520and%2520none%2520of%2520them%250Aachieve%2520an%2520accuracy%2520greater%2520than%252060%2525%2520in%2520the%2520reasoning%2520tasks.%2520Project%2520page%253A%250Ahttps%253A//github.com/Lens4MLLMs/lens.%2520ICCV%25202025%2520workshop%2520page%253A%250Ahttps%253A//lens4mllms.github.io/mars2-workshop-iccv2025/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15616v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LENS%3A%20Multi-level%20Evaluation%20of%20Multimodal%20Reasoning%20with%20Large%20Language%0A%20%20Models&entry.906535625=Ruilin%20Yao%20and%20Bo%20Zhang%20and%20Jirui%20Huang%20and%20Xinwei%20Long%20and%20Yifang%20Zhang%20and%20Tianyu%20Zou%20and%20Yufei%20Wu%20and%20Shichao%20Su%20and%20Yifan%20Xu%20and%20Wenxi%20Zeng%20and%20Zhaoyu%20Yang%20and%20Guoyou%20Li%20and%20Shilan%20Zhang%20and%20Zichan%20Li%20and%20Yaxiong%20Chen%20and%20Shengwu%20Xiong%20and%20Peng%20Xu%20and%20Jiajun%20Zhang%20and%20Bowen%20Zhou%20and%20David%20Clifton%20and%20Luc%20Van%20Gool&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20significant%20advances%0Ain%20integrating%20visual%20and%20linguistic%20information%2C%20yet%20their%20ability%20to%20reason%0Aabout%20complex%20and%20real-world%20scenarios%20remains%20limited.%20The%20existing%20benchmarks%0Aare%20usually%20constructed%20in%20the%20task-oriented%20manner%20without%20guarantee%20that%0Adifferent%20task%20samples%20come%20from%20the%20same%20data%20distribution%2C%20thus%20they%20often%0Afall%20short%20in%20evaluating%20the%20synergistic%20effects%20of%20lower-level%20perceptual%0Acapabilities%20on%20higher-order%20reasoning.%20To%20lift%20this%20limitation%2C%20we%20contribute%0ALens%2C%20a%20multi-level%20benchmark%20with%203.4K%20contemporary%20images%20and%2060K%2B%0Ahuman-authored%20questions%20covering%20eight%20tasks%20and%2012%20daily%20scenarios%2C%20forming%0Athree%20progressive%20task%20tiers%2C%20i.e.%2C%20perception%2C%20understanding%2C%20and%20reasoning.%0AOne%20feature%20is%20that%20each%20image%20is%20equipped%20with%20rich%20annotations%20for%20all%20tasks.%0AThus%2C%20this%20dataset%20intrinsically%20supports%20to%20evaluate%20MLLMs%20to%20handle%0Aimage-invariable%20prompts%2C%20from%20basic%20perception%20to%20compositional%20reasoning.%20In%0Aaddition%2C%20our%20images%20are%20manully%20collected%20from%20the%20social%20media%2C%20in%20which%2053%25%0Awere%20published%20later%20than%20Jan.%202025.%20We%20evaluate%2015%2B%20frontier%20MLLMs%20such%20as%0AQwen2.5-VL-72B%2C%20InternVL3-78B%2C%20GPT-4o%20and%20two%20reasoning%20models%20QVQ-72B-preview%0Aand%20Kimi-VL.%20These%20models%20are%20released%20later%20than%20Dec.%202024%2C%20and%20none%20of%20them%0Aachieve%20an%20accuracy%20greater%20than%2060%25%20in%20the%20reasoning%20tasks.%20Project%20page%3A%0Ahttps%3A//github.com/Lens4MLLMs/lens.%20ICCV%202025%20workshop%20page%3A%0Ahttps%3A//lens4mllms.github.io/mars2-workshop-iccv2025/%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15616v1&entry.124074799=Read"},
{"title": "Visual Question Answering on Multiple Remote Sensing Image Modalities", "author": "Hichem Boussaid and Lucrezia Tosato and Flora Weissgerber and Camille Kurtz and Laurent Wendling and Sylvain Lobry", "abstract": "  The extraction of visual features is an essential step in Visual Question\nAnswering (VQA). Building a good visual representation of the analyzed scene is\nindeed one of the essential keys for the system to be able to correctly\nunderstand the latter in order to answer complex questions. In many fields such\nas remote sensing, the visual feature extraction step could benefit\nsignificantly from leveraging different image modalities carrying complementary\nspectral, spatial and contextual information. In this work, we propose to add\nmultiple image modalities to VQA in the particular context of remote sensing,\nleading to a novel task for the computer vision community. To this end, we\nintroduce a new VQA dataset, named TAMMI (Text and Multi-Modal Imagery) with\ndiverse questions on scenes described by three different modalities (very high\nresolution RGB, multi-spectral imaging data and synthetic aperture radar).\nThanks to an automated pipeline, this dataset can be easily extended according\nto experimental needs. We also propose the MM-RSVQA (Multi-modal\nMulti-resolution Remote Sensing Visual Question Answering) model, based on\nVisualBERT, a vision-language transformer, to effectively combine the multiple\nimage modalities and text through a trainable fusion process. A preliminary\nexperimental study shows promising results of our methodology on this\nchallenging dataset, with an accuracy of 65.56% on the targeted VQA task. This\npioneering work paves the way for the community to a new multi-modal\nmulti-resolution VQA task that can be applied in other imaging domains (such as\nmedical imaging) where multi-modality can enrich the visual representation of a\nscene. The dataset and code are available at https://tammi.sylvainlobry.com/.\n", "link": "http://arxiv.org/abs/2505.15401v1", "date": "2025-05-21", "relevancy": 2.9385, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.615}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5741}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Question%20Answering%20on%20Multiple%20Remote%20Sensing%20Image%20Modalities&body=Title%3A%20Visual%20Question%20Answering%20on%20Multiple%20Remote%20Sensing%20Image%20Modalities%0AAuthor%3A%20Hichem%20Boussaid%20and%20Lucrezia%20Tosato%20and%20Flora%20Weissgerber%20and%20Camille%20Kurtz%20and%20Laurent%20Wendling%20and%20Sylvain%20Lobry%0AAbstract%3A%20%20%20The%20extraction%20of%20visual%20features%20is%20an%20essential%20step%20in%20Visual%20Question%0AAnswering%20%28VQA%29.%20Building%20a%20good%20visual%20representation%20of%20the%20analyzed%20scene%20is%0Aindeed%20one%20of%20the%20essential%20keys%20for%20the%20system%20to%20be%20able%20to%20correctly%0Aunderstand%20the%20latter%20in%20order%20to%20answer%20complex%20questions.%20In%20many%20fields%20such%0Aas%20remote%20sensing%2C%20the%20visual%20feature%20extraction%20step%20could%20benefit%0Asignificantly%20from%20leveraging%20different%20image%20modalities%20carrying%20complementary%0Aspectral%2C%20spatial%20and%20contextual%20information.%20In%20this%20work%2C%20we%20propose%20to%20add%0Amultiple%20image%20modalities%20to%20VQA%20in%20the%20particular%20context%20of%20remote%20sensing%2C%0Aleading%20to%20a%20novel%20task%20for%20the%20computer%20vision%20community.%20To%20this%20end%2C%20we%0Aintroduce%20a%20new%20VQA%20dataset%2C%20named%20TAMMI%20%28Text%20and%20Multi-Modal%20Imagery%29%20with%0Adiverse%20questions%20on%20scenes%20described%20by%20three%20different%20modalities%20%28very%20high%0Aresolution%20RGB%2C%20multi-spectral%20imaging%20data%20and%20synthetic%20aperture%20radar%29.%0AThanks%20to%20an%20automated%20pipeline%2C%20this%20dataset%20can%20be%20easily%20extended%20according%0Ato%20experimental%20needs.%20We%20also%20propose%20the%20MM-RSVQA%20%28Multi-modal%0AMulti-resolution%20Remote%20Sensing%20Visual%20Question%20Answering%29%20model%2C%20based%20on%0AVisualBERT%2C%20a%20vision-language%20transformer%2C%20to%20effectively%20combine%20the%20multiple%0Aimage%20modalities%20and%20text%20through%20a%20trainable%20fusion%20process.%20A%20preliminary%0Aexperimental%20study%20shows%20promising%20results%20of%20our%20methodology%20on%20this%0Achallenging%20dataset%2C%20with%20an%20accuracy%20of%2065.56%25%20on%20the%20targeted%20VQA%20task.%20This%0Apioneering%20work%20paves%20the%20way%20for%20the%20community%20to%20a%20new%20multi-modal%0Amulti-resolution%20VQA%20task%20that%20can%20be%20applied%20in%20other%20imaging%20domains%20%28such%20as%0Amedical%20imaging%29%20where%20multi-modality%20can%20enrich%20the%20visual%20representation%20of%20a%0Ascene.%20The%20dataset%20and%20code%20are%20available%20at%20https%3A//tammi.sylvainlobry.com/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15401v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Question%2520Answering%2520on%2520Multiple%2520Remote%2520Sensing%2520Image%2520Modalities%26entry.906535625%3DHichem%2520Boussaid%2520and%2520Lucrezia%2520Tosato%2520and%2520Flora%2520Weissgerber%2520and%2520Camille%2520Kurtz%2520and%2520Laurent%2520Wendling%2520and%2520Sylvain%2520Lobry%26entry.1292438233%3D%2520%2520The%2520extraction%2520of%2520visual%2520features%2520is%2520an%2520essential%2520step%2520in%2520Visual%2520Question%250AAnswering%2520%2528VQA%2529.%2520Building%2520a%2520good%2520visual%2520representation%2520of%2520the%2520analyzed%2520scene%2520is%250Aindeed%2520one%2520of%2520the%2520essential%2520keys%2520for%2520the%2520system%2520to%2520be%2520able%2520to%2520correctly%250Aunderstand%2520the%2520latter%2520in%2520order%2520to%2520answer%2520complex%2520questions.%2520In%2520many%2520fields%2520such%250Aas%2520remote%2520sensing%252C%2520the%2520visual%2520feature%2520extraction%2520step%2520could%2520benefit%250Asignificantly%2520from%2520leveraging%2520different%2520image%2520modalities%2520carrying%2520complementary%250Aspectral%252C%2520spatial%2520and%2520contextual%2520information.%2520In%2520this%2520work%252C%2520we%2520propose%2520to%2520add%250Amultiple%2520image%2520modalities%2520to%2520VQA%2520in%2520the%2520particular%2520context%2520of%2520remote%2520sensing%252C%250Aleading%2520to%2520a%2520novel%2520task%2520for%2520the%2520computer%2520vision%2520community.%2520To%2520this%2520end%252C%2520we%250Aintroduce%2520a%2520new%2520VQA%2520dataset%252C%2520named%2520TAMMI%2520%2528Text%2520and%2520Multi-Modal%2520Imagery%2529%2520with%250Adiverse%2520questions%2520on%2520scenes%2520described%2520by%2520three%2520different%2520modalities%2520%2528very%2520high%250Aresolution%2520RGB%252C%2520multi-spectral%2520imaging%2520data%2520and%2520synthetic%2520aperture%2520radar%2529.%250AThanks%2520to%2520an%2520automated%2520pipeline%252C%2520this%2520dataset%2520can%2520be%2520easily%2520extended%2520according%250Ato%2520experimental%2520needs.%2520We%2520also%2520propose%2520the%2520MM-RSVQA%2520%2528Multi-modal%250AMulti-resolution%2520Remote%2520Sensing%2520Visual%2520Question%2520Answering%2529%2520model%252C%2520based%2520on%250AVisualBERT%252C%2520a%2520vision-language%2520transformer%252C%2520to%2520effectively%2520combine%2520the%2520multiple%250Aimage%2520modalities%2520and%2520text%2520through%2520a%2520trainable%2520fusion%2520process.%2520A%2520preliminary%250Aexperimental%2520study%2520shows%2520promising%2520results%2520of%2520our%2520methodology%2520on%2520this%250Achallenging%2520dataset%252C%2520with%2520an%2520accuracy%2520of%252065.56%2525%2520on%2520the%2520targeted%2520VQA%2520task.%2520This%250Apioneering%2520work%2520paves%2520the%2520way%2520for%2520the%2520community%2520to%2520a%2520new%2520multi-modal%250Amulti-resolution%2520VQA%2520task%2520that%2520can%2520be%2520applied%2520in%2520other%2520imaging%2520domains%2520%2528such%2520as%250Amedical%2520imaging%2529%2520where%2520multi-modality%2520can%2520enrich%2520the%2520visual%2520representation%2520of%2520a%250Ascene.%2520The%2520dataset%2520and%2520code%2520are%2520available%2520at%2520https%253A//tammi.sylvainlobry.com/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15401v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Question%20Answering%20on%20Multiple%20Remote%20Sensing%20Image%20Modalities&entry.906535625=Hichem%20Boussaid%20and%20Lucrezia%20Tosato%20and%20Flora%20Weissgerber%20and%20Camille%20Kurtz%20and%20Laurent%20Wendling%20and%20Sylvain%20Lobry&entry.1292438233=%20%20The%20extraction%20of%20visual%20features%20is%20an%20essential%20step%20in%20Visual%20Question%0AAnswering%20%28VQA%29.%20Building%20a%20good%20visual%20representation%20of%20the%20analyzed%20scene%20is%0Aindeed%20one%20of%20the%20essential%20keys%20for%20the%20system%20to%20be%20able%20to%20correctly%0Aunderstand%20the%20latter%20in%20order%20to%20answer%20complex%20questions.%20In%20many%20fields%20such%0Aas%20remote%20sensing%2C%20the%20visual%20feature%20extraction%20step%20could%20benefit%0Asignificantly%20from%20leveraging%20different%20image%20modalities%20carrying%20complementary%0Aspectral%2C%20spatial%20and%20contextual%20information.%20In%20this%20work%2C%20we%20propose%20to%20add%0Amultiple%20image%20modalities%20to%20VQA%20in%20the%20particular%20context%20of%20remote%20sensing%2C%0Aleading%20to%20a%20novel%20task%20for%20the%20computer%20vision%20community.%20To%20this%20end%2C%20we%0Aintroduce%20a%20new%20VQA%20dataset%2C%20named%20TAMMI%20%28Text%20and%20Multi-Modal%20Imagery%29%20with%0Adiverse%20questions%20on%20scenes%20described%20by%20three%20different%20modalities%20%28very%20high%0Aresolution%20RGB%2C%20multi-spectral%20imaging%20data%20and%20synthetic%20aperture%20radar%29.%0AThanks%20to%20an%20automated%20pipeline%2C%20this%20dataset%20can%20be%20easily%20extended%20according%0Ato%20experimental%20needs.%20We%20also%20propose%20the%20MM-RSVQA%20%28Multi-modal%0AMulti-resolution%20Remote%20Sensing%20Visual%20Question%20Answering%29%20model%2C%20based%20on%0AVisualBERT%2C%20a%20vision-language%20transformer%2C%20to%20effectively%20combine%20the%20multiple%0Aimage%20modalities%20and%20text%20through%20a%20trainable%20fusion%20process.%20A%20preliminary%0Aexperimental%20study%20shows%20promising%20results%20of%20our%20methodology%20on%20this%0Achallenging%20dataset%2C%20with%20an%20accuracy%20of%2065.56%25%20on%20the%20targeted%20VQA%20task.%20This%0Apioneering%20work%20paves%20the%20way%20for%20the%20community%20to%20a%20new%20multi-modal%0Amulti-resolution%20VQA%20task%20that%20can%20be%20applied%20in%20other%20imaging%20domains%20%28such%20as%0Amedical%20imaging%29%20where%20multi-modality%20can%20enrich%20the%20visual%20representation%20of%20a%0Ascene.%20The%20dataset%20and%20code%20are%20available%20at%20https%3A//tammi.sylvainlobry.com/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15401v1&entry.124074799=Read"},
{"title": "Visual Thoughts: A Unified Perspective of Understanding Multimodal\n  Chain-of-Thought", "author": "Zihui Cheng and Qiguang Chen and Xiao Xu and Jiaqi Wang and Weiyun Wang and Hao Fei and Yidong Wang and Alex Jinpeng Wang and Zhi Chen and Wanxiang Che and Libo Qin", "abstract": "  Large Vision-Language Models (LVLMs) have achieved significant success in\nmultimodal tasks, with multimodal chain-of-thought (MCoT) further enhancing\nperformance and interpretability. Recent MCoT methods fall into two categories:\n(i) Textual-MCoT (T-MCoT), which takes multimodal input and produces textual\noutput; and (ii) Interleaved-MCoT (I-MCoT), which generates interleaved\nimage-text outputs. Despite advances in both approaches, the mechanisms driving\nthese improvements are not fully understood. To fill this gap, we first reveal\nthat MCoT boosts LVLMs by incorporating visual thoughts, which convey image\ninformation to the reasoning process regardless of the MCoT format, depending\nonly on clarity and conciseness of expression. Furthermore, to explore visual\nthoughts systematically, we define four distinct forms of visual thought\nexpressions and analyze them comprehensively. Our findings demonstrate that\nthese forms differ in clarity and conciseness, yielding varying levels of MCoT\nimprovement. Additionally, we explore the internal nature of visual thoughts,\nfinding that visual thoughts serve as intermediaries between the input image\nand reasoning to deeper transformer layers, enabling more advanced visual\ninformation transmission. We hope that the visual thoughts can inspire further\nbreakthroughs for future MCoT research.\n", "link": "http://arxiv.org/abs/2505.15510v1", "date": "2025-05-21", "relevancy": 2.9144, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.588}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.588}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Thoughts%3A%20A%20Unified%20Perspective%20of%20Understanding%20Multimodal%0A%20%20Chain-of-Thought&body=Title%3A%20Visual%20Thoughts%3A%20A%20Unified%20Perspective%20of%20Understanding%20Multimodal%0A%20%20Chain-of-Thought%0AAuthor%3A%20Zihui%20Cheng%20and%20Qiguang%20Chen%20and%20Xiao%20Xu%20and%20Jiaqi%20Wang%20and%20Weiyun%20Wang%20and%20Hao%20Fei%20and%20Yidong%20Wang%20and%20Alex%20Jinpeng%20Wang%20and%20Zhi%20Chen%20and%20Wanxiang%20Che%20and%20Libo%20Qin%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20achieved%20significant%20success%20in%0Amultimodal%20tasks%2C%20with%20multimodal%20chain-of-thought%20%28MCoT%29%20further%20enhancing%0Aperformance%20and%20interpretability.%20Recent%20MCoT%20methods%20fall%20into%20two%20categories%3A%0A%28i%29%20Textual-MCoT%20%28T-MCoT%29%2C%20which%20takes%20multimodal%20input%20and%20produces%20textual%0Aoutput%3B%20and%20%28ii%29%20Interleaved-MCoT%20%28I-MCoT%29%2C%20which%20generates%20interleaved%0Aimage-text%20outputs.%20Despite%20advances%20in%20both%20approaches%2C%20the%20mechanisms%20driving%0Athese%20improvements%20are%20not%20fully%20understood.%20To%20fill%20this%20gap%2C%20we%20first%20reveal%0Athat%20MCoT%20boosts%20LVLMs%20by%20incorporating%20visual%20thoughts%2C%20which%20convey%20image%0Ainformation%20to%20the%20reasoning%20process%20regardless%20of%20the%20MCoT%20format%2C%20depending%0Aonly%20on%20clarity%20and%20conciseness%20of%20expression.%20Furthermore%2C%20to%20explore%20visual%0Athoughts%20systematically%2C%20we%20define%20four%20distinct%20forms%20of%20visual%20thought%0Aexpressions%20and%20analyze%20them%20comprehensively.%20Our%20findings%20demonstrate%20that%0Athese%20forms%20differ%20in%20clarity%20and%20conciseness%2C%20yielding%20varying%20levels%20of%20MCoT%0Aimprovement.%20Additionally%2C%20we%20explore%20the%20internal%20nature%20of%20visual%20thoughts%2C%0Afinding%20that%20visual%20thoughts%20serve%20as%20intermediaries%20between%20the%20input%20image%0Aand%20reasoning%20to%20deeper%20transformer%20layers%2C%20enabling%20more%20advanced%20visual%0Ainformation%20transmission.%20We%20hope%20that%20the%20visual%20thoughts%20can%20inspire%20further%0Abreakthroughs%20for%20future%20MCoT%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15510v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Thoughts%253A%2520A%2520Unified%2520Perspective%2520of%2520Understanding%2520Multimodal%250A%2520%2520Chain-of-Thought%26entry.906535625%3DZihui%2520Cheng%2520and%2520Qiguang%2520Chen%2520and%2520Xiao%2520Xu%2520and%2520Jiaqi%2520Wang%2520and%2520Weiyun%2520Wang%2520and%2520Hao%2520Fei%2520and%2520Yidong%2520Wang%2520and%2520Alex%2520Jinpeng%2520Wang%2520and%2520Zhi%2520Chen%2520and%2520Wanxiang%2520Che%2520and%2520Libo%2520Qin%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520achieved%2520significant%2520success%2520in%250Amultimodal%2520tasks%252C%2520with%2520multimodal%2520chain-of-thought%2520%2528MCoT%2529%2520further%2520enhancing%250Aperformance%2520and%2520interpretability.%2520Recent%2520MCoT%2520methods%2520fall%2520into%2520two%2520categories%253A%250A%2528i%2529%2520Textual-MCoT%2520%2528T-MCoT%2529%252C%2520which%2520takes%2520multimodal%2520input%2520and%2520produces%2520textual%250Aoutput%253B%2520and%2520%2528ii%2529%2520Interleaved-MCoT%2520%2528I-MCoT%2529%252C%2520which%2520generates%2520interleaved%250Aimage-text%2520outputs.%2520Despite%2520advances%2520in%2520both%2520approaches%252C%2520the%2520mechanisms%2520driving%250Athese%2520improvements%2520are%2520not%2520fully%2520understood.%2520To%2520fill%2520this%2520gap%252C%2520we%2520first%2520reveal%250Athat%2520MCoT%2520boosts%2520LVLMs%2520by%2520incorporating%2520visual%2520thoughts%252C%2520which%2520convey%2520image%250Ainformation%2520to%2520the%2520reasoning%2520process%2520regardless%2520of%2520the%2520MCoT%2520format%252C%2520depending%250Aonly%2520on%2520clarity%2520and%2520conciseness%2520of%2520expression.%2520Furthermore%252C%2520to%2520explore%2520visual%250Athoughts%2520systematically%252C%2520we%2520define%2520four%2520distinct%2520forms%2520of%2520visual%2520thought%250Aexpressions%2520and%2520analyze%2520them%2520comprehensively.%2520Our%2520findings%2520demonstrate%2520that%250Athese%2520forms%2520differ%2520in%2520clarity%2520and%2520conciseness%252C%2520yielding%2520varying%2520levels%2520of%2520MCoT%250Aimprovement.%2520Additionally%252C%2520we%2520explore%2520the%2520internal%2520nature%2520of%2520visual%2520thoughts%252C%250Afinding%2520that%2520visual%2520thoughts%2520serve%2520as%2520intermediaries%2520between%2520the%2520input%2520image%250Aand%2520reasoning%2520to%2520deeper%2520transformer%2520layers%252C%2520enabling%2520more%2520advanced%2520visual%250Ainformation%2520transmission.%2520We%2520hope%2520that%2520the%2520visual%2520thoughts%2520can%2520inspire%2520further%250Abreakthroughs%2520for%2520future%2520MCoT%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15510v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Thoughts%3A%20A%20Unified%20Perspective%20of%20Understanding%20Multimodal%0A%20%20Chain-of-Thought&entry.906535625=Zihui%20Cheng%20and%20Qiguang%20Chen%20and%20Xiao%20Xu%20and%20Jiaqi%20Wang%20and%20Weiyun%20Wang%20and%20Hao%20Fei%20and%20Yidong%20Wang%20and%20Alex%20Jinpeng%20Wang%20and%20Zhi%20Chen%20and%20Wanxiang%20Che%20and%20Libo%20Qin&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20achieved%20significant%20success%20in%0Amultimodal%20tasks%2C%20with%20multimodal%20chain-of-thought%20%28MCoT%29%20further%20enhancing%0Aperformance%20and%20interpretability.%20Recent%20MCoT%20methods%20fall%20into%20two%20categories%3A%0A%28i%29%20Textual-MCoT%20%28T-MCoT%29%2C%20which%20takes%20multimodal%20input%20and%20produces%20textual%0Aoutput%3B%20and%20%28ii%29%20Interleaved-MCoT%20%28I-MCoT%29%2C%20which%20generates%20interleaved%0Aimage-text%20outputs.%20Despite%20advances%20in%20both%20approaches%2C%20the%20mechanisms%20driving%0Athese%20improvements%20are%20not%20fully%20understood.%20To%20fill%20this%20gap%2C%20we%20first%20reveal%0Athat%20MCoT%20boosts%20LVLMs%20by%20incorporating%20visual%20thoughts%2C%20which%20convey%20image%0Ainformation%20to%20the%20reasoning%20process%20regardless%20of%20the%20MCoT%20format%2C%20depending%0Aonly%20on%20clarity%20and%20conciseness%20of%20expression.%20Furthermore%2C%20to%20explore%20visual%0Athoughts%20systematically%2C%20we%20define%20four%20distinct%20forms%20of%20visual%20thought%0Aexpressions%20and%20analyze%20them%20comprehensively.%20Our%20findings%20demonstrate%20that%0Athese%20forms%20differ%20in%20clarity%20and%20conciseness%2C%20yielding%20varying%20levels%20of%20MCoT%0Aimprovement.%20Additionally%2C%20we%20explore%20the%20internal%20nature%20of%20visual%20thoughts%2C%0Afinding%20that%20visual%20thoughts%20serve%20as%20intermediaries%20between%20the%20input%20image%0Aand%20reasoning%20to%20deeper%20transformer%20layers%2C%20enabling%20more%20advanced%20visual%0Ainformation%20transmission.%20We%20hope%20that%20the%20visual%20thoughts%20can%20inspire%20further%0Abreakthroughs%20for%20future%20MCoT%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15510v1&entry.124074799=Read"},
{"title": "IA-T2I: Internet-Augmented Text-to-Image Generation", "author": "Chuanhao Li and Jianwen Sun and Yukang Feng and Mingliang Zhai and Yifan Chang and Kaipeng Zhang", "abstract": "  Current text-to-image (T2I) generation models achieve promising results, but\nthey fail on the scenarios where the knowledge implied in the text prompt is\nuncertain. For example, a T2I model released in February would struggle to\ngenerate a suitable poster for a movie premiering in April, because the\ncharacter designs and styles are uncertain to the model. To solve this problem,\nwe propose an Internet-Augmented text-to-image generation (IA-T2I) framework to\ncompel T2I models clear about such uncertain knowledge by providing them with\nreference images. Specifically, an active retrieval module is designed to\ndetermine whether a reference image is needed based on the given text prompt; a\nhierarchical image selection module is introduced to find the most suitable\nimage returned by an image search engine to enhance the T2I model; a\nself-reflection mechanism is presented to continuously evaluate and refine the\ngenerated image to ensure faithful alignment with the text prompt. To evaluate\nthe proposed framework's performance, we collect a dataset named Img-Ref-T2I,\nwhere text prompts include three types of uncertain knowledge: (1) known but\nrare. (2) unknown. (3) ambiguous. Moreover, we carefully craft a complex prompt\nto guide GPT-4o in making preference evaluation, which has been shown to have\nan evaluation accuracy similar to that of human preference evaluation.\nExperimental results demonstrate the effectiveness of our framework,\noutperforming GPT-4o by about 30% in human evaluation.\n", "link": "http://arxiv.org/abs/2505.15779v1", "date": "2025-05-21", "relevancy": 2.869, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5936}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5736}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IA-T2I%3A%20Internet-Augmented%20Text-to-Image%20Generation&body=Title%3A%20IA-T2I%3A%20Internet-Augmented%20Text-to-Image%20Generation%0AAuthor%3A%20Chuanhao%20Li%20and%20Jianwen%20Sun%20and%20Yukang%20Feng%20and%20Mingliang%20Zhai%20and%20Yifan%20Chang%20and%20Kaipeng%20Zhang%0AAbstract%3A%20%20%20Current%20text-to-image%20%28T2I%29%20generation%20models%20achieve%20promising%20results%2C%20but%0Athey%20fail%20on%20the%20scenarios%20where%20the%20knowledge%20implied%20in%20the%20text%20prompt%20is%0Auncertain.%20For%20example%2C%20a%20T2I%20model%20released%20in%20February%20would%20struggle%20to%0Agenerate%20a%20suitable%20poster%20for%20a%20movie%20premiering%20in%20April%2C%20because%20the%0Acharacter%20designs%20and%20styles%20are%20uncertain%20to%20the%20model.%20To%20solve%20this%20problem%2C%0Awe%20propose%20an%20Internet-Augmented%20text-to-image%20generation%20%28IA-T2I%29%20framework%20to%0Acompel%20T2I%20models%20clear%20about%20such%20uncertain%20knowledge%20by%20providing%20them%20with%0Areference%20images.%20Specifically%2C%20an%20active%20retrieval%20module%20is%20designed%20to%0Adetermine%20whether%20a%20reference%20image%20is%20needed%20based%20on%20the%20given%20text%20prompt%3B%20a%0Ahierarchical%20image%20selection%20module%20is%20introduced%20to%20find%20the%20most%20suitable%0Aimage%20returned%20by%20an%20image%20search%20engine%20to%20enhance%20the%20T2I%20model%3B%20a%0Aself-reflection%20mechanism%20is%20presented%20to%20continuously%20evaluate%20and%20refine%20the%0Agenerated%20image%20to%20ensure%20faithful%20alignment%20with%20the%20text%20prompt.%20To%20evaluate%0Athe%20proposed%20framework%27s%20performance%2C%20we%20collect%20a%20dataset%20named%20Img-Ref-T2I%2C%0Awhere%20text%20prompts%20include%20three%20types%20of%20uncertain%20knowledge%3A%20%281%29%20known%20but%0Arare.%20%282%29%20unknown.%20%283%29%20ambiguous.%20Moreover%2C%20we%20carefully%20craft%20a%20complex%20prompt%0Ato%20guide%20GPT-4o%20in%20making%20preference%20evaluation%2C%20which%20has%20been%20shown%20to%20have%0Aan%20evaluation%20accuracy%20similar%20to%20that%20of%20human%20preference%20evaluation.%0AExperimental%20results%20demonstrate%20the%20effectiveness%20of%20our%20framework%2C%0Aoutperforming%20GPT-4o%20by%20about%2030%25%20in%20human%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15779v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIA-T2I%253A%2520Internet-Augmented%2520Text-to-Image%2520Generation%26entry.906535625%3DChuanhao%2520Li%2520and%2520Jianwen%2520Sun%2520and%2520Yukang%2520Feng%2520and%2520Mingliang%2520Zhai%2520and%2520Yifan%2520Chang%2520and%2520Kaipeng%2520Zhang%26entry.1292438233%3D%2520%2520Current%2520text-to-image%2520%2528T2I%2529%2520generation%2520models%2520achieve%2520promising%2520results%252C%2520but%250Athey%2520fail%2520on%2520the%2520scenarios%2520where%2520the%2520knowledge%2520implied%2520in%2520the%2520text%2520prompt%2520is%250Auncertain.%2520For%2520example%252C%2520a%2520T2I%2520model%2520released%2520in%2520February%2520would%2520struggle%2520to%250Agenerate%2520a%2520suitable%2520poster%2520for%2520a%2520movie%2520premiering%2520in%2520April%252C%2520because%2520the%250Acharacter%2520designs%2520and%2520styles%2520are%2520uncertain%2520to%2520the%2520model.%2520To%2520solve%2520this%2520problem%252C%250Awe%2520propose%2520an%2520Internet-Augmented%2520text-to-image%2520generation%2520%2528IA-T2I%2529%2520framework%2520to%250Acompel%2520T2I%2520models%2520clear%2520about%2520such%2520uncertain%2520knowledge%2520by%2520providing%2520them%2520with%250Areference%2520images.%2520Specifically%252C%2520an%2520active%2520retrieval%2520module%2520is%2520designed%2520to%250Adetermine%2520whether%2520a%2520reference%2520image%2520is%2520needed%2520based%2520on%2520the%2520given%2520text%2520prompt%253B%2520a%250Ahierarchical%2520image%2520selection%2520module%2520is%2520introduced%2520to%2520find%2520the%2520most%2520suitable%250Aimage%2520returned%2520by%2520an%2520image%2520search%2520engine%2520to%2520enhance%2520the%2520T2I%2520model%253B%2520a%250Aself-reflection%2520mechanism%2520is%2520presented%2520to%2520continuously%2520evaluate%2520and%2520refine%2520the%250Agenerated%2520image%2520to%2520ensure%2520faithful%2520alignment%2520with%2520the%2520text%2520prompt.%2520To%2520evaluate%250Athe%2520proposed%2520framework%2527s%2520performance%252C%2520we%2520collect%2520a%2520dataset%2520named%2520Img-Ref-T2I%252C%250Awhere%2520text%2520prompts%2520include%2520three%2520types%2520of%2520uncertain%2520knowledge%253A%2520%25281%2529%2520known%2520but%250Arare.%2520%25282%2529%2520unknown.%2520%25283%2529%2520ambiguous.%2520Moreover%252C%2520we%2520carefully%2520craft%2520a%2520complex%2520prompt%250Ato%2520guide%2520GPT-4o%2520in%2520making%2520preference%2520evaluation%252C%2520which%2520has%2520been%2520shown%2520to%2520have%250Aan%2520evaluation%2520accuracy%2520similar%2520to%2520that%2520of%2520human%2520preference%2520evaluation.%250AExperimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520framework%252C%250Aoutperforming%2520GPT-4o%2520by%2520about%252030%2525%2520in%2520human%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15779v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IA-T2I%3A%20Internet-Augmented%20Text-to-Image%20Generation&entry.906535625=Chuanhao%20Li%20and%20Jianwen%20Sun%20and%20Yukang%20Feng%20and%20Mingliang%20Zhai%20and%20Yifan%20Chang%20and%20Kaipeng%20Zhang&entry.1292438233=%20%20Current%20text-to-image%20%28T2I%29%20generation%20models%20achieve%20promising%20results%2C%20but%0Athey%20fail%20on%20the%20scenarios%20where%20the%20knowledge%20implied%20in%20the%20text%20prompt%20is%0Auncertain.%20For%20example%2C%20a%20T2I%20model%20released%20in%20February%20would%20struggle%20to%0Agenerate%20a%20suitable%20poster%20for%20a%20movie%20premiering%20in%20April%2C%20because%20the%0Acharacter%20designs%20and%20styles%20are%20uncertain%20to%20the%20model.%20To%20solve%20this%20problem%2C%0Awe%20propose%20an%20Internet-Augmented%20text-to-image%20generation%20%28IA-T2I%29%20framework%20to%0Acompel%20T2I%20models%20clear%20about%20such%20uncertain%20knowledge%20by%20providing%20them%20with%0Areference%20images.%20Specifically%2C%20an%20active%20retrieval%20module%20is%20designed%20to%0Adetermine%20whether%20a%20reference%20image%20is%20needed%20based%20on%20the%20given%20text%20prompt%3B%20a%0Ahierarchical%20image%20selection%20module%20is%20introduced%20to%20find%20the%20most%20suitable%0Aimage%20returned%20by%20an%20image%20search%20engine%20to%20enhance%20the%20T2I%20model%3B%20a%0Aself-reflection%20mechanism%20is%20presented%20to%20continuously%20evaluate%20and%20refine%20the%0Agenerated%20image%20to%20ensure%20faithful%20alignment%20with%20the%20text%20prompt.%20To%20evaluate%0Athe%20proposed%20framework%27s%20performance%2C%20we%20collect%20a%20dataset%20named%20Img-Ref-T2I%2C%0Awhere%20text%20prompts%20include%20three%20types%20of%20uncertain%20knowledge%3A%20%281%29%20known%20but%0Arare.%20%282%29%20unknown.%20%283%29%20ambiguous.%20Moreover%2C%20we%20carefully%20craft%20a%20complex%20prompt%0Ato%20guide%20GPT-4o%20in%20making%20preference%20evaluation%2C%20which%20has%20been%20shown%20to%20have%0Aan%20evaluation%20accuracy%20similar%20to%20that%20of%20human%20preference%20evaluation.%0AExperimental%20results%20demonstrate%20the%20effectiveness%20of%20our%20framework%2C%0Aoutperforming%20GPT-4o%20by%20about%2030%25%20in%20human%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15779v1&entry.124074799=Read"},
{"title": "Visual Perturbation and Adaptive Hard Negative Contrastive Learning for\n  Compositional Reasoning in Vision-Language Models", "author": "Xin Huang and Ruibin Li and Tong Jia and Wei Zheng and Ya Wang", "abstract": "  Vision-Language Models (VLMs) are essential for multimodal tasks, especially\ncompositional reasoning (CR) tasks, which require distinguishing fine-grained\nsemantic differences between visual and textual embeddings. However, existing\nmethods primarily fine-tune the model by generating text-based hard negative\nsamples, neglecting the importance of image-based negative samples, which\nresults in insufficient training of the visual encoder and ultimately impacts\nthe overall performance of the model. Moreover, negative samples are typically\ntreated uniformly, without considering their difficulty levels, and the\nalignment of positive samples is insufficient, which leads to challenges in\naligning difficult sample pairs. To address these issues, we propose Adaptive\nHard Negative Perturbation Learning (AHNPL). AHNPL translates text-based hard\nnegatives into the visual domain to generate semantically disturbed image-based\nnegatives for training the model, thereby enhancing its overall performance.\nAHNPL also introduces a contrastive learning approach using a multimodal hard\nnegative loss to improve the model's discrimination of hard negatives within\neach modality and a dynamic margin loss that adjusts the contrastive margin\naccording to sample difficulty to enhance the distinction of challenging sample\npairs. Experiments on three public datasets demonstrate that our method\neffectively boosts VLMs' performance on complex CR tasks. The source code is\navailable at https://github.com/nynu-BDAI/AHNPL.\n", "link": "http://arxiv.org/abs/2505.15576v1", "date": "2025-05-21", "relevancy": 2.8687, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5774}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5774}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Perturbation%20and%20Adaptive%20Hard%20Negative%20Contrastive%20Learning%20for%0A%20%20Compositional%20Reasoning%20in%20Vision-Language%20Models&body=Title%3A%20Visual%20Perturbation%20and%20Adaptive%20Hard%20Negative%20Contrastive%20Learning%20for%0A%20%20Compositional%20Reasoning%20in%20Vision-Language%20Models%0AAuthor%3A%20Xin%20Huang%20and%20Ruibin%20Li%20and%20Tong%20Jia%20and%20Wei%20Zheng%20and%20Ya%20Wang%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20are%20essential%20for%20multimodal%20tasks%2C%20especially%0Acompositional%20reasoning%20%28CR%29%20tasks%2C%20which%20require%20distinguishing%20fine-grained%0Asemantic%20differences%20between%20visual%20and%20textual%20embeddings.%20However%2C%20existing%0Amethods%20primarily%20fine-tune%20the%20model%20by%20generating%20text-based%20hard%20negative%0Asamples%2C%20neglecting%20the%20importance%20of%20image-based%20negative%20samples%2C%20which%0Aresults%20in%20insufficient%20training%20of%20the%20visual%20encoder%20and%20ultimately%20impacts%0Athe%20overall%20performance%20of%20the%20model.%20Moreover%2C%20negative%20samples%20are%20typically%0Atreated%20uniformly%2C%20without%20considering%20their%20difficulty%20levels%2C%20and%20the%0Aalignment%20of%20positive%20samples%20is%20insufficient%2C%20which%20leads%20to%20challenges%20in%0Aaligning%20difficult%20sample%20pairs.%20To%20address%20these%20issues%2C%20we%20propose%20Adaptive%0AHard%20Negative%20Perturbation%20Learning%20%28AHNPL%29.%20AHNPL%20translates%20text-based%20hard%0Anegatives%20into%20the%20visual%20domain%20to%20generate%20semantically%20disturbed%20image-based%0Anegatives%20for%20training%20the%20model%2C%20thereby%20enhancing%20its%20overall%20performance.%0AAHNPL%20also%20introduces%20a%20contrastive%20learning%20approach%20using%20a%20multimodal%20hard%0Anegative%20loss%20to%20improve%20the%20model%27s%20discrimination%20of%20hard%20negatives%20within%0Aeach%20modality%20and%20a%20dynamic%20margin%20loss%20that%20adjusts%20the%20contrastive%20margin%0Aaccording%20to%20sample%20difficulty%20to%20enhance%20the%20distinction%20of%20challenging%20sample%0Apairs.%20Experiments%20on%20three%20public%20datasets%20demonstrate%20that%20our%20method%0Aeffectively%20boosts%20VLMs%27%20performance%20on%20complex%20CR%20tasks.%20The%20source%20code%20is%0Aavailable%20at%20https%3A//github.com/nynu-BDAI/AHNPL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15576v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Perturbation%2520and%2520Adaptive%2520Hard%2520Negative%2520Contrastive%2520Learning%2520for%250A%2520%2520Compositional%2520Reasoning%2520in%2520Vision-Language%2520Models%26entry.906535625%3DXin%2520Huang%2520and%2520Ruibin%2520Li%2520and%2520Tong%2520Jia%2520and%2520Wei%2520Zheng%2520and%2520Ya%2520Wang%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520are%2520essential%2520for%2520multimodal%2520tasks%252C%2520especially%250Acompositional%2520reasoning%2520%2528CR%2529%2520tasks%252C%2520which%2520require%2520distinguishing%2520fine-grained%250Asemantic%2520differences%2520between%2520visual%2520and%2520textual%2520embeddings.%2520However%252C%2520existing%250Amethods%2520primarily%2520fine-tune%2520the%2520model%2520by%2520generating%2520text-based%2520hard%2520negative%250Asamples%252C%2520neglecting%2520the%2520importance%2520of%2520image-based%2520negative%2520samples%252C%2520which%250Aresults%2520in%2520insufficient%2520training%2520of%2520the%2520visual%2520encoder%2520and%2520ultimately%2520impacts%250Athe%2520overall%2520performance%2520of%2520the%2520model.%2520Moreover%252C%2520negative%2520samples%2520are%2520typically%250Atreated%2520uniformly%252C%2520without%2520considering%2520their%2520difficulty%2520levels%252C%2520and%2520the%250Aalignment%2520of%2520positive%2520samples%2520is%2520insufficient%252C%2520which%2520leads%2520to%2520challenges%2520in%250Aaligning%2520difficult%2520sample%2520pairs.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520Adaptive%250AHard%2520Negative%2520Perturbation%2520Learning%2520%2528AHNPL%2529.%2520AHNPL%2520translates%2520text-based%2520hard%250Anegatives%2520into%2520the%2520visual%2520domain%2520to%2520generate%2520semantically%2520disturbed%2520image-based%250Anegatives%2520for%2520training%2520the%2520model%252C%2520thereby%2520enhancing%2520its%2520overall%2520performance.%250AAHNPL%2520also%2520introduces%2520a%2520contrastive%2520learning%2520approach%2520using%2520a%2520multimodal%2520hard%250Anegative%2520loss%2520to%2520improve%2520the%2520model%2527s%2520discrimination%2520of%2520hard%2520negatives%2520within%250Aeach%2520modality%2520and%2520a%2520dynamic%2520margin%2520loss%2520that%2520adjusts%2520the%2520contrastive%2520margin%250Aaccording%2520to%2520sample%2520difficulty%2520to%2520enhance%2520the%2520distinction%2520of%2520challenging%2520sample%250Apairs.%2520Experiments%2520on%2520three%2520public%2520datasets%2520demonstrate%2520that%2520our%2520method%250Aeffectively%2520boosts%2520VLMs%2527%2520performance%2520on%2520complex%2520CR%2520tasks.%2520The%2520source%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/nynu-BDAI/AHNPL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15576v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Perturbation%20and%20Adaptive%20Hard%20Negative%20Contrastive%20Learning%20for%0A%20%20Compositional%20Reasoning%20in%20Vision-Language%20Models&entry.906535625=Xin%20Huang%20and%20Ruibin%20Li%20and%20Tong%20Jia%20and%20Wei%20Zheng%20and%20Ya%20Wang&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20are%20essential%20for%20multimodal%20tasks%2C%20especially%0Acompositional%20reasoning%20%28CR%29%20tasks%2C%20which%20require%20distinguishing%20fine-grained%0Asemantic%20differences%20between%20visual%20and%20textual%20embeddings.%20However%2C%20existing%0Amethods%20primarily%20fine-tune%20the%20model%20by%20generating%20text-based%20hard%20negative%0Asamples%2C%20neglecting%20the%20importance%20of%20image-based%20negative%20samples%2C%20which%0Aresults%20in%20insufficient%20training%20of%20the%20visual%20encoder%20and%20ultimately%20impacts%0Athe%20overall%20performance%20of%20the%20model.%20Moreover%2C%20negative%20samples%20are%20typically%0Atreated%20uniformly%2C%20without%20considering%20their%20difficulty%20levels%2C%20and%20the%0Aalignment%20of%20positive%20samples%20is%20insufficient%2C%20which%20leads%20to%20challenges%20in%0Aaligning%20difficult%20sample%20pairs.%20To%20address%20these%20issues%2C%20we%20propose%20Adaptive%0AHard%20Negative%20Perturbation%20Learning%20%28AHNPL%29.%20AHNPL%20translates%20text-based%20hard%0Anegatives%20into%20the%20visual%20domain%20to%20generate%20semantically%20disturbed%20image-based%0Anegatives%20for%20training%20the%20model%2C%20thereby%20enhancing%20its%20overall%20performance.%0AAHNPL%20also%20introduces%20a%20contrastive%20learning%20approach%20using%20a%20multimodal%20hard%0Anegative%20loss%20to%20improve%20the%20model%27s%20discrimination%20of%20hard%20negatives%20within%0Aeach%20modality%20and%20a%20dynamic%20margin%20loss%20that%20adjusts%20the%20contrastive%20margin%0Aaccording%20to%20sample%20difficulty%20to%20enhance%20the%20distinction%20of%20challenging%20sample%0Apairs.%20Experiments%20on%20three%20public%20datasets%20demonstrate%20that%20our%20method%0Aeffectively%20boosts%20VLMs%27%20performance%20on%20complex%20CR%20tasks.%20The%20source%20code%20is%0Aavailable%20at%20https%3A//github.com/nynu-BDAI/AHNPL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15576v1&entry.124074799=Read"},
{"title": "STAR-R1: Spacial TrAnsformation Reasoning by Reinforcing Multimodal LLMs", "author": "Zongzhao Li and Zongyang Ma and Mingze Li and Songyou Li and Yu Rong and Tingyang Xu and Ziqi Zhang and Deli Zhao and Wenbing Huang", "abstract": "  Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities across diverse tasks, yet they lag significantly behind humans in\nspatial reasoning. We investigate this gap through Transformation-Driven Visual\nReasoning (TVR), a challenging task requiring identification of object\ntransformations across images under varying viewpoints. While traditional\nSupervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in\ncross-view settings, sparse-reward Reinforcement Learning (RL) suffers from\ninefficient exploration and slow convergence. To address these limitations, we\npropose STAR-R1, a novel framework that integrates a single-stage RL paradigm\nwith a fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1\nrewards partial correctness while penalizing excessive enumeration and passive\ninaction, enabling efficient exploration and precise reasoning. Comprehensive\nevaluations demonstrate that STAR-R1 achieves state-of-the-art performance\nacross all 11 metrics, outperforming SFT by 23% in cross-view scenarios.\nFurther analysis reveals STAR-R1's anthropomorphic behavior and highlights its\nunique ability to compare all objects for improving spatial reasoning. Our work\nprovides critical insights in advancing the research of MLLMs and reasoning\nmodels. The codes, model weights, and data will be publicly available at\nhttps://github.com/zongzhao23/STAR-R1.\n", "link": "http://arxiv.org/abs/2505.15804v1", "date": "2025-05-21", "relevancy": 2.8658, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5878}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5658}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STAR-R1%3A%20Spacial%20TrAnsformation%20Reasoning%20by%20Reinforcing%20Multimodal%20LLMs&body=Title%3A%20STAR-R1%3A%20Spacial%20TrAnsformation%20Reasoning%20by%20Reinforcing%20Multimodal%20LLMs%0AAuthor%3A%20Zongzhao%20Li%20and%20Zongyang%20Ma%20and%20Mingze%20Li%20and%20Songyou%20Li%20and%20Yu%20Rong%20and%20Tingyang%20Xu%20and%20Ziqi%20Zhang%20and%20Deli%20Zhao%20and%20Wenbing%20Huang%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20remarkable%0Acapabilities%20across%20diverse%20tasks%2C%20yet%20they%20lag%20significantly%20behind%20humans%20in%0Aspatial%20reasoning.%20We%20investigate%20this%20gap%20through%20Transformation-Driven%20Visual%0AReasoning%20%28TVR%29%2C%20a%20challenging%20task%20requiring%20identification%20of%20object%0Atransformations%20across%20images%20under%20varying%20viewpoints.%20While%20traditional%0ASupervised%20Fine-Tuning%20%28SFT%29%20fails%20to%20generate%20coherent%20reasoning%20paths%20in%0Across-view%20settings%2C%20sparse-reward%20Reinforcement%20Learning%20%28RL%29%20suffers%20from%0Ainefficient%20exploration%20and%20slow%20convergence.%20To%20address%20these%20limitations%2C%20we%0Apropose%20STAR-R1%2C%20a%20novel%20framework%20that%20integrates%20a%20single-stage%20RL%20paradigm%0Awith%20a%20fine-grained%20reward%20mechanism%20tailored%20for%20TVR.%20Specifically%2C%20STAR-R1%0Arewards%20partial%20correctness%20while%20penalizing%20excessive%20enumeration%20and%20passive%0Ainaction%2C%20enabling%20efficient%20exploration%20and%20precise%20reasoning.%20Comprehensive%0Aevaluations%20demonstrate%20that%20STAR-R1%20achieves%20state-of-the-art%20performance%0Aacross%20all%2011%20metrics%2C%20outperforming%20SFT%20by%2023%25%20in%20cross-view%20scenarios.%0AFurther%20analysis%20reveals%20STAR-R1%27s%20anthropomorphic%20behavior%20and%20highlights%20its%0Aunique%20ability%20to%20compare%20all%20objects%20for%20improving%20spatial%20reasoning.%20Our%20work%0Aprovides%20critical%20insights%20in%20advancing%20the%20research%20of%20MLLMs%20and%20reasoning%0Amodels.%20The%20codes%2C%20model%20weights%2C%20and%20data%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/zongzhao23/STAR-R1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15804v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTAR-R1%253A%2520Spacial%2520TrAnsformation%2520Reasoning%2520by%2520Reinforcing%2520Multimodal%2520LLMs%26entry.906535625%3DZongzhao%2520Li%2520and%2520Zongyang%2520Ma%2520and%2520Mingze%2520Li%2520and%2520Songyou%2520Li%2520and%2520Yu%2520Rong%2520and%2520Tingyang%2520Xu%2520and%2520Ziqi%2520Zhang%2520and%2520Deli%2520Zhao%2520and%2520Wenbing%2520Huang%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520remarkable%250Acapabilities%2520across%2520diverse%2520tasks%252C%2520yet%2520they%2520lag%2520significantly%2520behind%2520humans%2520in%250Aspatial%2520reasoning.%2520We%2520investigate%2520this%2520gap%2520through%2520Transformation-Driven%2520Visual%250AReasoning%2520%2528TVR%2529%252C%2520a%2520challenging%2520task%2520requiring%2520identification%2520of%2520object%250Atransformations%2520across%2520images%2520under%2520varying%2520viewpoints.%2520While%2520traditional%250ASupervised%2520Fine-Tuning%2520%2528SFT%2529%2520fails%2520to%2520generate%2520coherent%2520reasoning%2520paths%2520in%250Across-view%2520settings%252C%2520sparse-reward%2520Reinforcement%2520Learning%2520%2528RL%2529%2520suffers%2520from%250Ainefficient%2520exploration%2520and%2520slow%2520convergence.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apropose%2520STAR-R1%252C%2520a%2520novel%2520framework%2520that%2520integrates%2520a%2520single-stage%2520RL%2520paradigm%250Awith%2520a%2520fine-grained%2520reward%2520mechanism%2520tailored%2520for%2520TVR.%2520Specifically%252C%2520STAR-R1%250Arewards%2520partial%2520correctness%2520while%2520penalizing%2520excessive%2520enumeration%2520and%2520passive%250Ainaction%252C%2520enabling%2520efficient%2520exploration%2520and%2520precise%2520reasoning.%2520Comprehensive%250Aevaluations%2520demonstrate%2520that%2520STAR-R1%2520achieves%2520state-of-the-art%2520performance%250Aacross%2520all%252011%2520metrics%252C%2520outperforming%2520SFT%2520by%252023%2525%2520in%2520cross-view%2520scenarios.%250AFurther%2520analysis%2520reveals%2520STAR-R1%2527s%2520anthropomorphic%2520behavior%2520and%2520highlights%2520its%250Aunique%2520ability%2520to%2520compare%2520all%2520objects%2520for%2520improving%2520spatial%2520reasoning.%2520Our%2520work%250Aprovides%2520critical%2520insights%2520in%2520advancing%2520the%2520research%2520of%2520MLLMs%2520and%2520reasoning%250Amodels.%2520The%2520codes%252C%2520model%2520weights%252C%2520and%2520data%2520will%2520be%2520publicly%2520available%2520at%250Ahttps%253A//github.com/zongzhao23/STAR-R1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15804v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STAR-R1%3A%20Spacial%20TrAnsformation%20Reasoning%20by%20Reinforcing%20Multimodal%20LLMs&entry.906535625=Zongzhao%20Li%20and%20Zongyang%20Ma%20and%20Mingze%20Li%20and%20Songyou%20Li%20and%20Yu%20Rong%20and%20Tingyang%20Xu%20and%20Ziqi%20Zhang%20and%20Deli%20Zhao%20and%20Wenbing%20Huang&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20remarkable%0Acapabilities%20across%20diverse%20tasks%2C%20yet%20they%20lag%20significantly%20behind%20humans%20in%0Aspatial%20reasoning.%20We%20investigate%20this%20gap%20through%20Transformation-Driven%20Visual%0AReasoning%20%28TVR%29%2C%20a%20challenging%20task%20requiring%20identification%20of%20object%0Atransformations%20across%20images%20under%20varying%20viewpoints.%20While%20traditional%0ASupervised%20Fine-Tuning%20%28SFT%29%20fails%20to%20generate%20coherent%20reasoning%20paths%20in%0Across-view%20settings%2C%20sparse-reward%20Reinforcement%20Learning%20%28RL%29%20suffers%20from%0Ainefficient%20exploration%20and%20slow%20convergence.%20To%20address%20these%20limitations%2C%20we%0Apropose%20STAR-R1%2C%20a%20novel%20framework%20that%20integrates%20a%20single-stage%20RL%20paradigm%0Awith%20a%20fine-grained%20reward%20mechanism%20tailored%20for%20TVR.%20Specifically%2C%20STAR-R1%0Arewards%20partial%20correctness%20while%20penalizing%20excessive%20enumeration%20and%20passive%0Ainaction%2C%20enabling%20efficient%20exploration%20and%20precise%20reasoning.%20Comprehensive%0Aevaluations%20demonstrate%20that%20STAR-R1%20achieves%20state-of-the-art%20performance%0Aacross%20all%2011%20metrics%2C%20outperforming%20SFT%20by%2023%25%20in%20cross-view%20scenarios.%0AFurther%20analysis%20reveals%20STAR-R1%27s%20anthropomorphic%20behavior%20and%20highlights%20its%0Aunique%20ability%20to%20compare%20all%20objects%20for%20improving%20spatial%20reasoning.%20Our%20work%0Aprovides%20critical%20insights%20in%20advancing%20the%20research%20of%20MLLMs%20and%20reasoning%0Amodels.%20The%20codes%2C%20model%20weights%2C%20and%20data%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/zongzhao23/STAR-R1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15804v1&entry.124074799=Read"},
{"title": "Better Safe Than Sorry? Overreaction Problem of Vision Language Models\n  in Visual Emergency Recognition", "author": "Dasol Choi and Seunghyun Lee and Youngsook Song", "abstract": "  Vision-Language Models (VLMs) have demonstrated impressive capabilities in\nunderstanding visual content, but their reliability in safety-critical contexts\nremains under-explored. We introduce VERI (Visual Emergency Recognition\nDataset), a carefully designed diagnostic benchmark of 200 images (100\ncontrastive pairs). Each emergency scene is matched with a visually similar but\nsafe counterpart through multi-stage human verification and iterative\nrefinement. Using a two-stage protocol - risk identification and emergency\nresponse - we evaluate 14 VLMs (2B-124B parameters) across medical emergencies,\naccidents, and natural disasters. Our analysis reveals a systematic\noverreaction problem: models excel at identifying real emergencies (70-100\npercent success rate) but suffer from an alarming rate of false alarms,\nmisidentifying 31-96 percent of safe situations as dangerous, with 10 scenarios\nfailed by all models regardless of scale. This \"better-safe-than-sorry\" bias\nmanifests primarily through contextual overinterpretation (88-93 percent of\nerrors), challenging VLMs' reliability for safety applications. These findings\nhighlight persistent limitations that are not resolved by increasing model\nscale, motivating targeted approaches for improving contextual safety\nassessment in visually misleading scenarios.\n", "link": "http://arxiv.org/abs/2505.15367v1", "date": "2025-05-21", "relevancy": 2.8582, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5741}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5741}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Better%20Safe%20Than%20Sorry%3F%20Overreaction%20Problem%20of%20Vision%20Language%20Models%0A%20%20in%20Visual%20Emergency%20Recognition&body=Title%3A%20Better%20Safe%20Than%20Sorry%3F%20Overreaction%20Problem%20of%20Vision%20Language%20Models%0A%20%20in%20Visual%20Emergency%20Recognition%0AAuthor%3A%20Dasol%20Choi%20and%20Seunghyun%20Lee%20and%20Youngsook%20Song%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20have%20demonstrated%20impressive%20capabilities%20in%0Aunderstanding%20visual%20content%2C%20but%20their%20reliability%20in%20safety-critical%20contexts%0Aremains%20under-explored.%20We%20introduce%20VERI%20%28Visual%20Emergency%20Recognition%0ADataset%29%2C%20a%20carefully%20designed%20diagnostic%20benchmark%20of%20200%20images%20%28100%0Acontrastive%20pairs%29.%20Each%20emergency%20scene%20is%20matched%20with%20a%20visually%20similar%20but%0Asafe%20counterpart%20through%20multi-stage%20human%20verification%20and%20iterative%0Arefinement.%20Using%20a%20two-stage%20protocol%20-%20risk%20identification%20and%20emergency%0Aresponse%20-%20we%20evaluate%2014%20VLMs%20%282B-124B%20parameters%29%20across%20medical%20emergencies%2C%0Aaccidents%2C%20and%20natural%20disasters.%20Our%20analysis%20reveals%20a%20systematic%0Aoverreaction%20problem%3A%20models%20excel%20at%20identifying%20real%20emergencies%20%2870-100%0Apercent%20success%20rate%29%20but%20suffer%20from%20an%20alarming%20rate%20of%20false%20alarms%2C%0Amisidentifying%2031-96%20percent%20of%20safe%20situations%20as%20dangerous%2C%20with%2010%20scenarios%0Afailed%20by%20all%20models%20regardless%20of%20scale.%20This%20%22better-safe-than-sorry%22%20bias%0Amanifests%20primarily%20through%20contextual%20overinterpretation%20%2888-93%20percent%20of%0Aerrors%29%2C%20challenging%20VLMs%27%20reliability%20for%20safety%20applications.%20These%20findings%0Ahighlight%20persistent%20limitations%20that%20are%20not%20resolved%20by%20increasing%20model%0Ascale%2C%20motivating%20targeted%20approaches%20for%20improving%20contextual%20safety%0Aassessment%20in%20visually%20misleading%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15367v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBetter%2520Safe%2520Than%2520Sorry%253F%2520Overreaction%2520Problem%2520of%2520Vision%2520Language%2520Models%250A%2520%2520in%2520Visual%2520Emergency%2520Recognition%26entry.906535625%3DDasol%2520Choi%2520and%2520Seunghyun%2520Lee%2520and%2520Youngsook%2520Song%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520demonstrated%2520impressive%2520capabilities%2520in%250Aunderstanding%2520visual%2520content%252C%2520but%2520their%2520reliability%2520in%2520safety-critical%2520contexts%250Aremains%2520under-explored.%2520We%2520introduce%2520VERI%2520%2528Visual%2520Emergency%2520Recognition%250ADataset%2529%252C%2520a%2520carefully%2520designed%2520diagnostic%2520benchmark%2520of%2520200%2520images%2520%2528100%250Acontrastive%2520pairs%2529.%2520Each%2520emergency%2520scene%2520is%2520matched%2520with%2520a%2520visually%2520similar%2520but%250Asafe%2520counterpart%2520through%2520multi-stage%2520human%2520verification%2520and%2520iterative%250Arefinement.%2520Using%2520a%2520two-stage%2520protocol%2520-%2520risk%2520identification%2520and%2520emergency%250Aresponse%2520-%2520we%2520evaluate%252014%2520VLMs%2520%25282B-124B%2520parameters%2529%2520across%2520medical%2520emergencies%252C%250Aaccidents%252C%2520and%2520natural%2520disasters.%2520Our%2520analysis%2520reveals%2520a%2520systematic%250Aoverreaction%2520problem%253A%2520models%2520excel%2520at%2520identifying%2520real%2520emergencies%2520%252870-100%250Apercent%2520success%2520rate%2529%2520but%2520suffer%2520from%2520an%2520alarming%2520rate%2520of%2520false%2520alarms%252C%250Amisidentifying%252031-96%2520percent%2520of%2520safe%2520situations%2520as%2520dangerous%252C%2520with%252010%2520scenarios%250Afailed%2520by%2520all%2520models%2520regardless%2520of%2520scale.%2520This%2520%2522better-safe-than-sorry%2522%2520bias%250Amanifests%2520primarily%2520through%2520contextual%2520overinterpretation%2520%252888-93%2520percent%2520of%250Aerrors%2529%252C%2520challenging%2520VLMs%2527%2520reliability%2520for%2520safety%2520applications.%2520These%2520findings%250Ahighlight%2520persistent%2520limitations%2520that%2520are%2520not%2520resolved%2520by%2520increasing%2520model%250Ascale%252C%2520motivating%2520targeted%2520approaches%2520for%2520improving%2520contextual%2520safety%250Aassessment%2520in%2520visually%2520misleading%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15367v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Better%20Safe%20Than%20Sorry%3F%20Overreaction%20Problem%20of%20Vision%20Language%20Models%0A%20%20in%20Visual%20Emergency%20Recognition&entry.906535625=Dasol%20Choi%20and%20Seunghyun%20Lee%20and%20Youngsook%20Song&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20have%20demonstrated%20impressive%20capabilities%20in%0Aunderstanding%20visual%20content%2C%20but%20their%20reliability%20in%20safety-critical%20contexts%0Aremains%20under-explored.%20We%20introduce%20VERI%20%28Visual%20Emergency%20Recognition%0ADataset%29%2C%20a%20carefully%20designed%20diagnostic%20benchmark%20of%20200%20images%20%28100%0Acontrastive%20pairs%29.%20Each%20emergency%20scene%20is%20matched%20with%20a%20visually%20similar%20but%0Asafe%20counterpart%20through%20multi-stage%20human%20verification%20and%20iterative%0Arefinement.%20Using%20a%20two-stage%20protocol%20-%20risk%20identification%20and%20emergency%0Aresponse%20-%20we%20evaluate%2014%20VLMs%20%282B-124B%20parameters%29%20across%20medical%20emergencies%2C%0Aaccidents%2C%20and%20natural%20disasters.%20Our%20analysis%20reveals%20a%20systematic%0Aoverreaction%20problem%3A%20models%20excel%20at%20identifying%20real%20emergencies%20%2870-100%0Apercent%20success%20rate%29%20but%20suffer%20from%20an%20alarming%20rate%20of%20false%20alarms%2C%0Amisidentifying%2031-96%20percent%20of%20safe%20situations%20as%20dangerous%2C%20with%2010%20scenarios%0Afailed%20by%20all%20models%20regardless%20of%20scale.%20This%20%22better-safe-than-sorry%22%20bias%0Amanifests%20primarily%20through%20contextual%20overinterpretation%20%2888-93%20percent%20of%0Aerrors%29%2C%20challenging%20VLMs%27%20reliability%20for%20safety%20applications.%20These%20findings%0Ahighlight%20persistent%20limitations%20that%20are%20not%20resolved%20by%20increasing%20model%0Ascale%2C%20motivating%20targeted%20approaches%20for%20improving%20contextual%20safety%0Aassessment%20in%20visually%20misleading%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15367v1&entry.124074799=Read"},
{"title": "BusterX: MLLM-Powered AI-Generated Video Forgery Detection and\n  Explanation", "author": "Haiquan Wen and Yiwei He and Zhenglin Huang and Tianxiao Li and Zihan Yu and Xingru Huang and Lu Qi and Baoyuan Wu and Xiangtai Li and Guangliang Cheng", "abstract": "  Advances in AI generative models facilitate super-realistic video synthesis,\namplifying misinformation risks via social media and eroding trust in digital\ncontent. Several research works have explored new deepfake detection methods on\nAI-generated images to alleviate these risks. However, with the fast\ndevelopment of video generation models, such as Sora and WanX, there is\ncurrently a lack of large-scale, high-quality AI-generated video datasets for\nforgery detection. In addition, existing detection approaches predominantly\ntreat the task as binary classification, lacking explainability in model\ndecision-making and failing to provide actionable insights or guidance for the\npublic. To address these challenges, we propose \\textbf{GenBuster-200K}, a\nlarge-scale AI-generated video dataset featuring 200K high-resolution video\nclips, diverse latest generative techniques, and real-world scenes. We further\nintroduce \\textbf{BusterX}, a novel AI-generated video detection and\nexplanation framework leveraging multimodal large language model (MLLM) and\nreinforcement learning for authenticity determination and explainable\nrationale. To our knowledge, GenBuster-200K is the {\\it \\textbf{first}}\nlarge-scale, high-quality AI-generated video dataset that incorporates the\nlatest generative techniques for real-world scenarios. BusterX is the {\\it\n\\textbf{first}} framework to integrate MLLM with reinforcement learning for\nexplainable AI-generated video detection. Extensive comparisons with\nstate-of-the-art methods and ablation studies validate the effectiveness and\ngeneralizability of BusterX. The code, models, and datasets will be released.\n", "link": "http://arxiv.org/abs/2505.12620v2", "date": "2025-05-21", "relevancy": 2.8253, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5736}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5731}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BusterX%3A%20MLLM-Powered%20AI-Generated%20Video%20Forgery%20Detection%20and%0A%20%20Explanation&body=Title%3A%20BusterX%3A%20MLLM-Powered%20AI-Generated%20Video%20Forgery%20Detection%20and%0A%20%20Explanation%0AAuthor%3A%20Haiquan%20Wen%20and%20Yiwei%20He%20and%20Zhenglin%20Huang%20and%20Tianxiao%20Li%20and%20Zihan%20Yu%20and%20Xingru%20Huang%20and%20Lu%20Qi%20and%20Baoyuan%20Wu%20and%20Xiangtai%20Li%20and%20Guangliang%20Cheng%0AAbstract%3A%20%20%20Advances%20in%20AI%20generative%20models%20facilitate%20super-realistic%20video%20synthesis%2C%0Aamplifying%20misinformation%20risks%20via%20social%20media%20and%20eroding%20trust%20in%20digital%0Acontent.%20Several%20research%20works%20have%20explored%20new%20deepfake%20detection%20methods%20on%0AAI-generated%20images%20to%20alleviate%20these%20risks.%20However%2C%20with%20the%20fast%0Adevelopment%20of%20video%20generation%20models%2C%20such%20as%20Sora%20and%20WanX%2C%20there%20is%0Acurrently%20a%20lack%20of%20large-scale%2C%20high-quality%20AI-generated%20video%20datasets%20for%0Aforgery%20detection.%20In%20addition%2C%20existing%20detection%20approaches%20predominantly%0Atreat%20the%20task%20as%20binary%20classification%2C%20lacking%20explainability%20in%20model%0Adecision-making%20and%20failing%20to%20provide%20actionable%20insights%20or%20guidance%20for%20the%0Apublic.%20To%20address%20these%20challenges%2C%20we%20propose%20%5Ctextbf%7BGenBuster-200K%7D%2C%20a%0Alarge-scale%20AI-generated%20video%20dataset%20featuring%20200K%20high-resolution%20video%0Aclips%2C%20diverse%20latest%20generative%20techniques%2C%20and%20real-world%20scenes.%20We%20further%0Aintroduce%20%5Ctextbf%7BBusterX%7D%2C%20a%20novel%20AI-generated%20video%20detection%20and%0Aexplanation%20framework%20leveraging%20multimodal%20large%20language%20model%20%28MLLM%29%20and%0Areinforcement%20learning%20for%20authenticity%20determination%20and%20explainable%0Arationale.%20To%20our%20knowledge%2C%20GenBuster-200K%20is%20the%20%7B%5Cit%20%5Ctextbf%7Bfirst%7D%7D%0Alarge-scale%2C%20high-quality%20AI-generated%20video%20dataset%20that%20incorporates%20the%0Alatest%20generative%20techniques%20for%20real-world%20scenarios.%20BusterX%20is%20the%20%7B%5Cit%0A%5Ctextbf%7Bfirst%7D%7D%20framework%20to%20integrate%20MLLM%20with%20reinforcement%20learning%20for%0Aexplainable%20AI-generated%20video%20detection.%20Extensive%20comparisons%20with%0Astate-of-the-art%20methods%20and%20ablation%20studies%20validate%20the%20effectiveness%20and%0Ageneralizability%20of%20BusterX.%20The%20code%2C%20models%2C%20and%20datasets%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12620v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBusterX%253A%2520MLLM-Powered%2520AI-Generated%2520Video%2520Forgery%2520Detection%2520and%250A%2520%2520Explanation%26entry.906535625%3DHaiquan%2520Wen%2520and%2520Yiwei%2520He%2520and%2520Zhenglin%2520Huang%2520and%2520Tianxiao%2520Li%2520and%2520Zihan%2520Yu%2520and%2520Xingru%2520Huang%2520and%2520Lu%2520Qi%2520and%2520Baoyuan%2520Wu%2520and%2520Xiangtai%2520Li%2520and%2520Guangliang%2520Cheng%26entry.1292438233%3D%2520%2520Advances%2520in%2520AI%2520generative%2520models%2520facilitate%2520super-realistic%2520video%2520synthesis%252C%250Aamplifying%2520misinformation%2520risks%2520via%2520social%2520media%2520and%2520eroding%2520trust%2520in%2520digital%250Acontent.%2520Several%2520research%2520works%2520have%2520explored%2520new%2520deepfake%2520detection%2520methods%2520on%250AAI-generated%2520images%2520to%2520alleviate%2520these%2520risks.%2520However%252C%2520with%2520the%2520fast%250Adevelopment%2520of%2520video%2520generation%2520models%252C%2520such%2520as%2520Sora%2520and%2520WanX%252C%2520there%2520is%250Acurrently%2520a%2520lack%2520of%2520large-scale%252C%2520high-quality%2520AI-generated%2520video%2520datasets%2520for%250Aforgery%2520detection.%2520In%2520addition%252C%2520existing%2520detection%2520approaches%2520predominantly%250Atreat%2520the%2520task%2520as%2520binary%2520classification%252C%2520lacking%2520explainability%2520in%2520model%250Adecision-making%2520and%2520failing%2520to%2520provide%2520actionable%2520insights%2520or%2520guidance%2520for%2520the%250Apublic.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520%255Ctextbf%257BGenBuster-200K%257D%252C%2520a%250Alarge-scale%2520AI-generated%2520video%2520dataset%2520featuring%2520200K%2520high-resolution%2520video%250Aclips%252C%2520diverse%2520latest%2520generative%2520techniques%252C%2520and%2520real-world%2520scenes.%2520We%2520further%250Aintroduce%2520%255Ctextbf%257BBusterX%257D%252C%2520a%2520novel%2520AI-generated%2520video%2520detection%2520and%250Aexplanation%2520framework%2520leveraging%2520multimodal%2520large%2520language%2520model%2520%2528MLLM%2529%2520and%250Areinforcement%2520learning%2520for%2520authenticity%2520determination%2520and%2520explainable%250Arationale.%2520To%2520our%2520knowledge%252C%2520GenBuster-200K%2520is%2520the%2520%257B%255Cit%2520%255Ctextbf%257Bfirst%257D%257D%250Alarge-scale%252C%2520high-quality%2520AI-generated%2520video%2520dataset%2520that%2520incorporates%2520the%250Alatest%2520generative%2520techniques%2520for%2520real-world%2520scenarios.%2520BusterX%2520is%2520the%2520%257B%255Cit%250A%255Ctextbf%257Bfirst%257D%257D%2520framework%2520to%2520integrate%2520MLLM%2520with%2520reinforcement%2520learning%2520for%250Aexplainable%2520AI-generated%2520video%2520detection.%2520Extensive%2520comparisons%2520with%250Astate-of-the-art%2520methods%2520and%2520ablation%2520studies%2520validate%2520the%2520effectiveness%2520and%250Ageneralizability%2520of%2520BusterX.%2520The%2520code%252C%2520models%252C%2520and%2520datasets%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12620v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BusterX%3A%20MLLM-Powered%20AI-Generated%20Video%20Forgery%20Detection%20and%0A%20%20Explanation&entry.906535625=Haiquan%20Wen%20and%20Yiwei%20He%20and%20Zhenglin%20Huang%20and%20Tianxiao%20Li%20and%20Zihan%20Yu%20and%20Xingru%20Huang%20and%20Lu%20Qi%20and%20Baoyuan%20Wu%20and%20Xiangtai%20Li%20and%20Guangliang%20Cheng&entry.1292438233=%20%20Advances%20in%20AI%20generative%20models%20facilitate%20super-realistic%20video%20synthesis%2C%0Aamplifying%20misinformation%20risks%20via%20social%20media%20and%20eroding%20trust%20in%20digital%0Acontent.%20Several%20research%20works%20have%20explored%20new%20deepfake%20detection%20methods%20on%0AAI-generated%20images%20to%20alleviate%20these%20risks.%20However%2C%20with%20the%20fast%0Adevelopment%20of%20video%20generation%20models%2C%20such%20as%20Sora%20and%20WanX%2C%20there%20is%0Acurrently%20a%20lack%20of%20large-scale%2C%20high-quality%20AI-generated%20video%20datasets%20for%0Aforgery%20detection.%20In%20addition%2C%20existing%20detection%20approaches%20predominantly%0Atreat%20the%20task%20as%20binary%20classification%2C%20lacking%20explainability%20in%20model%0Adecision-making%20and%20failing%20to%20provide%20actionable%20insights%20or%20guidance%20for%20the%0Apublic.%20To%20address%20these%20challenges%2C%20we%20propose%20%5Ctextbf%7BGenBuster-200K%7D%2C%20a%0Alarge-scale%20AI-generated%20video%20dataset%20featuring%20200K%20high-resolution%20video%0Aclips%2C%20diverse%20latest%20generative%20techniques%2C%20and%20real-world%20scenes.%20We%20further%0Aintroduce%20%5Ctextbf%7BBusterX%7D%2C%20a%20novel%20AI-generated%20video%20detection%20and%0Aexplanation%20framework%20leveraging%20multimodal%20large%20language%20model%20%28MLLM%29%20and%0Areinforcement%20learning%20for%20authenticity%20determination%20and%20explainable%0Arationale.%20To%20our%20knowledge%2C%20GenBuster-200K%20is%20the%20%7B%5Cit%20%5Ctextbf%7Bfirst%7D%7D%0Alarge-scale%2C%20high-quality%20AI-generated%20video%20dataset%20that%20incorporates%20the%0Alatest%20generative%20techniques%20for%20real-world%20scenarios.%20BusterX%20is%20the%20%7B%5Cit%0A%5Ctextbf%7Bfirst%7D%7D%20framework%20to%20integrate%20MLLM%20with%20reinforcement%20learning%20for%0Aexplainable%20AI-generated%20video%20detection.%20Extensive%20comparisons%20with%0Astate-of-the-art%20methods%20and%20ablation%20studies%20validate%20the%20effectiveness%20and%0Ageneralizability%20of%20BusterX.%20The%20code%2C%20models%2C%20and%20datasets%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12620v2&entry.124074799=Read"},
{"title": "OpenFly: A Comprehensive Platform for Aerial Vision-Language Navigation", "author": "Yunpeng Gao and Chenhui Li and Zhongrui You and Junli Liu and Zhen Li and Pengan Chen and Qizhi Chen and Zhonghan Tang and Liansheng Wang and Penghui Yang and Yiwen Tang and Yuhang Tang and Shuai Liang and Songyi Zhu and Ziqin Xiong and Yifei Su and Xinyi Ye and Jianan Li and Yan Ding and Dong Wang and Zhigang Wang and Bin Zhao and Xuelong Li", "abstract": "  Vision-Language Navigation (VLN) aims to guide agents by leveraging language\ninstructions and visual cues, playing a pivotal role in embodied AI. Indoor VLN\nhas been extensively studied, whereas outdoor aerial VLN remains underexplored.\nThe potential reason is that outdoor aerial view encompasses vast areas, making\ndata collection more challenging, which results in a lack of benchmarks. To\naddress this problem, we propose OpenFly, a platform comprising various\nrendering engines, a versatile toolchain, and a large-scale benchmark for\naerial VLN. Firstly, we integrate diverse rendering engines and advanced\ntechniques for environment simulation, including Unreal Engine, GTA V, Google\nEarth, and 3D Gaussian Splatting (3D GS). Particularly, 3D GS supports\nreal-to-sim rendering, further enhancing the realism of our environments.\nSecondly, we develop a highly automated toolchain for aerial VLN data\ncollection, streamlining point cloud acquisition, scene semantic segmentation,\nflight trajectory creation, and instruction generation. Thirdly, based on the\ntoolchain, we construct a large-scale aerial VLN dataset with 100k\ntrajectories, covering diverse heights and lengths across 18 scenes. Moreover,\nwe propose OpenFly-Agent, a keyframe-aware VLN model emphasizing key\nobservations during flight. For benchmarking, extensive experiments and\nanalyses are conducted, evaluating several recent VLN methods and showcasing\nthe superiority of our OpenFly platform and agent. The toolchain, dataset, and\ncodes will be open-sourced.\n", "link": "http://arxiv.org/abs/2502.18041v5", "date": "2025-05-21", "relevancy": 2.8175, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5688}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5688}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenFly%3A%20A%20Comprehensive%20Platform%20for%20Aerial%20Vision-Language%20Navigation&body=Title%3A%20OpenFly%3A%20A%20Comprehensive%20Platform%20for%20Aerial%20Vision-Language%20Navigation%0AAuthor%3A%20Yunpeng%20Gao%20and%20Chenhui%20Li%20and%20Zhongrui%20You%20and%20Junli%20Liu%20and%20Zhen%20Li%20and%20Pengan%20Chen%20and%20Qizhi%20Chen%20and%20Zhonghan%20Tang%20and%20Liansheng%20Wang%20and%20Penghui%20Yang%20and%20Yiwen%20Tang%20and%20Yuhang%20Tang%20and%20Shuai%20Liang%20and%20Songyi%20Zhu%20and%20Ziqin%20Xiong%20and%20Yifei%20Su%20and%20Xinyi%20Ye%20and%20Jianan%20Li%20and%20Yan%20Ding%20and%20Dong%20Wang%20and%20Zhigang%20Wang%20and%20Bin%20Zhao%20and%20Xuelong%20Li%0AAbstract%3A%20%20%20Vision-Language%20Navigation%20%28VLN%29%20aims%20to%20guide%20agents%20by%20leveraging%20language%0Ainstructions%20and%20visual%20cues%2C%20playing%20a%20pivotal%20role%20in%20embodied%20AI.%20Indoor%20VLN%0Ahas%20been%20extensively%20studied%2C%20whereas%20outdoor%20aerial%20VLN%20remains%20underexplored.%0AThe%20potential%20reason%20is%20that%20outdoor%20aerial%20view%20encompasses%20vast%20areas%2C%20making%0Adata%20collection%20more%20challenging%2C%20which%20results%20in%20a%20lack%20of%20benchmarks.%20To%0Aaddress%20this%20problem%2C%20we%20propose%20OpenFly%2C%20a%20platform%20comprising%20various%0Arendering%20engines%2C%20a%20versatile%20toolchain%2C%20and%20a%20large-scale%20benchmark%20for%0Aaerial%20VLN.%20Firstly%2C%20we%20integrate%20diverse%20rendering%20engines%20and%20advanced%0Atechniques%20for%20environment%20simulation%2C%20including%20Unreal%20Engine%2C%20GTA%20V%2C%20Google%0AEarth%2C%20and%203D%20Gaussian%20Splatting%20%283D%20GS%29.%20Particularly%2C%203D%20GS%20supports%0Areal-to-sim%20rendering%2C%20further%20enhancing%20the%20realism%20of%20our%20environments.%0ASecondly%2C%20we%20develop%20a%20highly%20automated%20toolchain%20for%20aerial%20VLN%20data%0Acollection%2C%20streamlining%20point%20cloud%20acquisition%2C%20scene%20semantic%20segmentation%2C%0Aflight%20trajectory%20creation%2C%20and%20instruction%20generation.%20Thirdly%2C%20based%20on%20the%0Atoolchain%2C%20we%20construct%20a%20large-scale%20aerial%20VLN%20dataset%20with%20100k%0Atrajectories%2C%20covering%20diverse%20heights%20and%20lengths%20across%2018%20scenes.%20Moreover%2C%0Awe%20propose%20OpenFly-Agent%2C%20a%20keyframe-aware%20VLN%20model%20emphasizing%20key%0Aobservations%20during%20flight.%20For%20benchmarking%2C%20extensive%20experiments%20and%0Aanalyses%20are%20conducted%2C%20evaluating%20several%20recent%20VLN%20methods%20and%20showcasing%0Athe%20superiority%20of%20our%20OpenFly%20platform%20and%20agent.%20The%20toolchain%2C%20dataset%2C%20and%0Acodes%20will%20be%20open-sourced.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18041v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenFly%253A%2520A%2520Comprehensive%2520Platform%2520for%2520Aerial%2520Vision-Language%2520Navigation%26entry.906535625%3DYunpeng%2520Gao%2520and%2520Chenhui%2520Li%2520and%2520Zhongrui%2520You%2520and%2520Junli%2520Liu%2520and%2520Zhen%2520Li%2520and%2520Pengan%2520Chen%2520and%2520Qizhi%2520Chen%2520and%2520Zhonghan%2520Tang%2520and%2520Liansheng%2520Wang%2520and%2520Penghui%2520Yang%2520and%2520Yiwen%2520Tang%2520and%2520Yuhang%2520Tang%2520and%2520Shuai%2520Liang%2520and%2520Songyi%2520Zhu%2520and%2520Ziqin%2520Xiong%2520and%2520Yifei%2520Su%2520and%2520Xinyi%2520Ye%2520and%2520Jianan%2520Li%2520and%2520Yan%2520Ding%2520and%2520Dong%2520Wang%2520and%2520Zhigang%2520Wang%2520and%2520Bin%2520Zhao%2520and%2520Xuelong%2520Li%26entry.1292438233%3D%2520%2520Vision-Language%2520Navigation%2520%2528VLN%2529%2520aims%2520to%2520guide%2520agents%2520by%2520leveraging%2520language%250Ainstructions%2520and%2520visual%2520cues%252C%2520playing%2520a%2520pivotal%2520role%2520in%2520embodied%2520AI.%2520Indoor%2520VLN%250Ahas%2520been%2520extensively%2520studied%252C%2520whereas%2520outdoor%2520aerial%2520VLN%2520remains%2520underexplored.%250AThe%2520potential%2520reason%2520is%2520that%2520outdoor%2520aerial%2520view%2520encompasses%2520vast%2520areas%252C%2520making%250Adata%2520collection%2520more%2520challenging%252C%2520which%2520results%2520in%2520a%2520lack%2520of%2520benchmarks.%2520To%250Aaddress%2520this%2520problem%252C%2520we%2520propose%2520OpenFly%252C%2520a%2520platform%2520comprising%2520various%250Arendering%2520engines%252C%2520a%2520versatile%2520toolchain%252C%2520and%2520a%2520large-scale%2520benchmark%2520for%250Aaerial%2520VLN.%2520Firstly%252C%2520we%2520integrate%2520diverse%2520rendering%2520engines%2520and%2520advanced%250Atechniques%2520for%2520environment%2520simulation%252C%2520including%2520Unreal%2520Engine%252C%2520GTA%2520V%252C%2520Google%250AEarth%252C%2520and%25203D%2520Gaussian%2520Splatting%2520%25283D%2520GS%2529.%2520Particularly%252C%25203D%2520GS%2520supports%250Areal-to-sim%2520rendering%252C%2520further%2520enhancing%2520the%2520realism%2520of%2520our%2520environments.%250ASecondly%252C%2520we%2520develop%2520a%2520highly%2520automated%2520toolchain%2520for%2520aerial%2520VLN%2520data%250Acollection%252C%2520streamlining%2520point%2520cloud%2520acquisition%252C%2520scene%2520semantic%2520segmentation%252C%250Aflight%2520trajectory%2520creation%252C%2520and%2520instruction%2520generation.%2520Thirdly%252C%2520based%2520on%2520the%250Atoolchain%252C%2520we%2520construct%2520a%2520large-scale%2520aerial%2520VLN%2520dataset%2520with%2520100k%250Atrajectories%252C%2520covering%2520diverse%2520heights%2520and%2520lengths%2520across%252018%2520scenes.%2520Moreover%252C%250Awe%2520propose%2520OpenFly-Agent%252C%2520a%2520keyframe-aware%2520VLN%2520model%2520emphasizing%2520key%250Aobservations%2520during%2520flight.%2520For%2520benchmarking%252C%2520extensive%2520experiments%2520and%250Aanalyses%2520are%2520conducted%252C%2520evaluating%2520several%2520recent%2520VLN%2520methods%2520and%2520showcasing%250Athe%2520superiority%2520of%2520our%2520OpenFly%2520platform%2520and%2520agent.%2520The%2520toolchain%252C%2520dataset%252C%2520and%250Acodes%2520will%2520be%2520open-sourced.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18041v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenFly%3A%20A%20Comprehensive%20Platform%20for%20Aerial%20Vision-Language%20Navigation&entry.906535625=Yunpeng%20Gao%20and%20Chenhui%20Li%20and%20Zhongrui%20You%20and%20Junli%20Liu%20and%20Zhen%20Li%20and%20Pengan%20Chen%20and%20Qizhi%20Chen%20and%20Zhonghan%20Tang%20and%20Liansheng%20Wang%20and%20Penghui%20Yang%20and%20Yiwen%20Tang%20and%20Yuhang%20Tang%20and%20Shuai%20Liang%20and%20Songyi%20Zhu%20and%20Ziqin%20Xiong%20and%20Yifei%20Su%20and%20Xinyi%20Ye%20and%20Jianan%20Li%20and%20Yan%20Ding%20and%20Dong%20Wang%20and%20Zhigang%20Wang%20and%20Bin%20Zhao%20and%20Xuelong%20Li&entry.1292438233=%20%20Vision-Language%20Navigation%20%28VLN%29%20aims%20to%20guide%20agents%20by%20leveraging%20language%0Ainstructions%20and%20visual%20cues%2C%20playing%20a%20pivotal%20role%20in%20embodied%20AI.%20Indoor%20VLN%0Ahas%20been%20extensively%20studied%2C%20whereas%20outdoor%20aerial%20VLN%20remains%20underexplored.%0AThe%20potential%20reason%20is%20that%20outdoor%20aerial%20view%20encompasses%20vast%20areas%2C%20making%0Adata%20collection%20more%20challenging%2C%20which%20results%20in%20a%20lack%20of%20benchmarks.%20To%0Aaddress%20this%20problem%2C%20we%20propose%20OpenFly%2C%20a%20platform%20comprising%20various%0Arendering%20engines%2C%20a%20versatile%20toolchain%2C%20and%20a%20large-scale%20benchmark%20for%0Aaerial%20VLN.%20Firstly%2C%20we%20integrate%20diverse%20rendering%20engines%20and%20advanced%0Atechniques%20for%20environment%20simulation%2C%20including%20Unreal%20Engine%2C%20GTA%20V%2C%20Google%0AEarth%2C%20and%203D%20Gaussian%20Splatting%20%283D%20GS%29.%20Particularly%2C%203D%20GS%20supports%0Areal-to-sim%20rendering%2C%20further%20enhancing%20the%20realism%20of%20our%20environments.%0ASecondly%2C%20we%20develop%20a%20highly%20automated%20toolchain%20for%20aerial%20VLN%20data%0Acollection%2C%20streamlining%20point%20cloud%20acquisition%2C%20scene%20semantic%20segmentation%2C%0Aflight%20trajectory%20creation%2C%20and%20instruction%20generation.%20Thirdly%2C%20based%20on%20the%0Atoolchain%2C%20we%20construct%20a%20large-scale%20aerial%20VLN%20dataset%20with%20100k%0Atrajectories%2C%20covering%20diverse%20heights%20and%20lengths%20across%2018%20scenes.%20Moreover%2C%0Awe%20propose%20OpenFly-Agent%2C%20a%20keyframe-aware%20VLN%20model%20emphasizing%20key%0Aobservations%20during%20flight.%20For%20benchmarking%2C%20extensive%20experiments%20and%0Aanalyses%20are%20conducted%2C%20evaluating%20several%20recent%20VLN%20methods%20and%20showcasing%0Athe%20superiority%20of%20our%20OpenFly%20platform%20and%20agent.%20The%20toolchain%2C%20dataset%2C%20and%0Acodes%20will%20be%20open-sourced.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18041v5&entry.124074799=Read"},
{"title": "GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI\n  Agents", "author": "Yuqi Zhou and Sunhao Dai and Shuai Wang and Kaiwen Zhou and Qinqlin Jia and  Junxu", "abstract": "  Recent Graphical User Interface (GUI) agents replicate the R1-Zero paradigm,\ncoupling online Reinforcement Learning (RL) with explicit chain-of-thought\nreasoning prior to object grounding and thereby achieving substantial\nperformance gains. In this paper, we first conduct extensive analysis\nexperiments of three key components of that training pipeline: input design,\noutput evaluation, and policy update-each revealing distinct challenges arising\nfrom blindly applying general-purpose RL without adapting to GUI grounding\ntasks. Input design: Current templates encourage the model to generate\nchain-of-thought reasoning, but longer chains unexpectedly lead to worse\ngrounding performance. Output evaluation: Reward functions based on hit signals\nor box area allow models to exploit box size, leading to reward hacking and\npoor localization quality. Policy update: Online RL tends to overfit easy\nexamples due to biases in length and sample difficulty, leading to\nunder-optimization on harder cases. To address these issues, we propose three\ntargeted solutions. First, we adopt a Fast Thinking Template that encourages\ndirect answer generation, reducing excessive reasoning during training. Second,\nwe incorporate a box size constraint into the reward function to mitigate\nreward hacking. Third, we revise the RL objective by adjusting length\nnormalization and adding a difficulty-aware scaling factor, enabling better\noptimization on hard samples. Our GUI-G1-3B, trained on 17K public samples with\nQwen2.5-VL-3B-Instruct, achieves 90.3% accuracy on ScreenSpot and 37.1% on\nScreenSpot-Pro. This surpasses all prior models of similar size and even\noutperforms the larger UI-TARS-7B, establishing a new state-of-the-art in GUI\nagent grounding. The project repository is available at\nhttps://github.com/Yuqi-Zhou/GUI-G1.\n", "link": "http://arxiv.org/abs/2505.15810v1", "date": "2025-05-21", "relevancy": 2.805, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5689}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5674}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GUI-G1%3A%20Understanding%20R1-Zero-Like%20Training%20for%20Visual%20Grounding%20in%20GUI%0A%20%20Agents&body=Title%3A%20GUI-G1%3A%20Understanding%20R1-Zero-Like%20Training%20for%20Visual%20Grounding%20in%20GUI%0A%20%20Agents%0AAuthor%3A%20Yuqi%20Zhou%20and%20Sunhao%20Dai%20and%20Shuai%20Wang%20and%20Kaiwen%20Zhou%20and%20Qinqlin%20Jia%20and%20%20Junxu%0AAbstract%3A%20%20%20Recent%20Graphical%20User%20Interface%20%28GUI%29%20agents%20replicate%20the%20R1-Zero%20paradigm%2C%0Acoupling%20online%20Reinforcement%20Learning%20%28RL%29%20with%20explicit%20chain-of-thought%0Areasoning%20prior%20to%20object%20grounding%20and%20thereby%20achieving%20substantial%0Aperformance%20gains.%20In%20this%20paper%2C%20we%20first%20conduct%20extensive%20analysis%0Aexperiments%20of%20three%20key%20components%20of%20that%20training%20pipeline%3A%20input%20design%2C%0Aoutput%20evaluation%2C%20and%20policy%20update-each%20revealing%20distinct%20challenges%20arising%0Afrom%20blindly%20applying%20general-purpose%20RL%20without%20adapting%20to%20GUI%20grounding%0Atasks.%20Input%20design%3A%20Current%20templates%20encourage%20the%20model%20to%20generate%0Achain-of-thought%20reasoning%2C%20but%20longer%20chains%20unexpectedly%20lead%20to%20worse%0Agrounding%20performance.%20Output%20evaluation%3A%20Reward%20functions%20based%20on%20hit%20signals%0Aor%20box%20area%20allow%20models%20to%20exploit%20box%20size%2C%20leading%20to%20reward%20hacking%20and%0Apoor%20localization%20quality.%20Policy%20update%3A%20Online%20RL%20tends%20to%20overfit%20easy%0Aexamples%20due%20to%20biases%20in%20length%20and%20sample%20difficulty%2C%20leading%20to%0Aunder-optimization%20on%20harder%20cases.%20To%20address%20these%20issues%2C%20we%20propose%20three%0Atargeted%20solutions.%20First%2C%20we%20adopt%20a%20Fast%20Thinking%20Template%20that%20encourages%0Adirect%20answer%20generation%2C%20reducing%20excessive%20reasoning%20during%20training.%20Second%2C%0Awe%20incorporate%20a%20box%20size%20constraint%20into%20the%20reward%20function%20to%20mitigate%0Areward%20hacking.%20Third%2C%20we%20revise%20the%20RL%20objective%20by%20adjusting%20length%0Anormalization%20and%20adding%20a%20difficulty-aware%20scaling%20factor%2C%20enabling%20better%0Aoptimization%20on%20hard%20samples.%20Our%20GUI-G1-3B%2C%20trained%20on%2017K%20public%20samples%20with%0AQwen2.5-VL-3B-Instruct%2C%20achieves%2090.3%25%20accuracy%20on%20ScreenSpot%20and%2037.1%25%20on%0AScreenSpot-Pro.%20This%20surpasses%20all%20prior%20models%20of%20similar%20size%20and%20even%0Aoutperforms%20the%20larger%20UI-TARS-7B%2C%20establishing%20a%20new%20state-of-the-art%20in%20GUI%0Aagent%20grounding.%20The%20project%20repository%20is%20available%20at%0Ahttps%3A//github.com/Yuqi-Zhou/GUI-G1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15810v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGUI-G1%253A%2520Understanding%2520R1-Zero-Like%2520Training%2520for%2520Visual%2520Grounding%2520in%2520GUI%250A%2520%2520Agents%26entry.906535625%3DYuqi%2520Zhou%2520and%2520Sunhao%2520Dai%2520and%2520Shuai%2520Wang%2520and%2520Kaiwen%2520Zhou%2520and%2520Qinqlin%2520Jia%2520and%2520%2520Junxu%26entry.1292438233%3D%2520%2520Recent%2520Graphical%2520User%2520Interface%2520%2528GUI%2529%2520agents%2520replicate%2520the%2520R1-Zero%2520paradigm%252C%250Acoupling%2520online%2520Reinforcement%2520Learning%2520%2528RL%2529%2520with%2520explicit%2520chain-of-thought%250Areasoning%2520prior%2520to%2520object%2520grounding%2520and%2520thereby%2520achieving%2520substantial%250Aperformance%2520gains.%2520In%2520this%2520paper%252C%2520we%2520first%2520conduct%2520extensive%2520analysis%250Aexperiments%2520of%2520three%2520key%2520components%2520of%2520that%2520training%2520pipeline%253A%2520input%2520design%252C%250Aoutput%2520evaluation%252C%2520and%2520policy%2520update-each%2520revealing%2520distinct%2520challenges%2520arising%250Afrom%2520blindly%2520applying%2520general-purpose%2520RL%2520without%2520adapting%2520to%2520GUI%2520grounding%250Atasks.%2520Input%2520design%253A%2520Current%2520templates%2520encourage%2520the%2520model%2520to%2520generate%250Achain-of-thought%2520reasoning%252C%2520but%2520longer%2520chains%2520unexpectedly%2520lead%2520to%2520worse%250Agrounding%2520performance.%2520Output%2520evaluation%253A%2520Reward%2520functions%2520based%2520on%2520hit%2520signals%250Aor%2520box%2520area%2520allow%2520models%2520to%2520exploit%2520box%2520size%252C%2520leading%2520to%2520reward%2520hacking%2520and%250Apoor%2520localization%2520quality.%2520Policy%2520update%253A%2520Online%2520RL%2520tends%2520to%2520overfit%2520easy%250Aexamples%2520due%2520to%2520biases%2520in%2520length%2520and%2520sample%2520difficulty%252C%2520leading%2520to%250Aunder-optimization%2520on%2520harder%2520cases.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520three%250Atargeted%2520solutions.%2520First%252C%2520we%2520adopt%2520a%2520Fast%2520Thinking%2520Template%2520that%2520encourages%250Adirect%2520answer%2520generation%252C%2520reducing%2520excessive%2520reasoning%2520during%2520training.%2520Second%252C%250Awe%2520incorporate%2520a%2520box%2520size%2520constraint%2520into%2520the%2520reward%2520function%2520to%2520mitigate%250Areward%2520hacking.%2520Third%252C%2520we%2520revise%2520the%2520RL%2520objective%2520by%2520adjusting%2520length%250Anormalization%2520and%2520adding%2520a%2520difficulty-aware%2520scaling%2520factor%252C%2520enabling%2520better%250Aoptimization%2520on%2520hard%2520samples.%2520Our%2520GUI-G1-3B%252C%2520trained%2520on%252017K%2520public%2520samples%2520with%250AQwen2.5-VL-3B-Instruct%252C%2520achieves%252090.3%2525%2520accuracy%2520on%2520ScreenSpot%2520and%252037.1%2525%2520on%250AScreenSpot-Pro.%2520This%2520surpasses%2520all%2520prior%2520models%2520of%2520similar%2520size%2520and%2520even%250Aoutperforms%2520the%2520larger%2520UI-TARS-7B%252C%2520establishing%2520a%2520new%2520state-of-the-art%2520in%2520GUI%250Aagent%2520grounding.%2520The%2520project%2520repository%2520is%2520available%2520at%250Ahttps%253A//github.com/Yuqi-Zhou/GUI-G1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15810v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GUI-G1%3A%20Understanding%20R1-Zero-Like%20Training%20for%20Visual%20Grounding%20in%20GUI%0A%20%20Agents&entry.906535625=Yuqi%20Zhou%20and%20Sunhao%20Dai%20and%20Shuai%20Wang%20and%20Kaiwen%20Zhou%20and%20Qinqlin%20Jia%20and%20%20Junxu&entry.1292438233=%20%20Recent%20Graphical%20User%20Interface%20%28GUI%29%20agents%20replicate%20the%20R1-Zero%20paradigm%2C%0Acoupling%20online%20Reinforcement%20Learning%20%28RL%29%20with%20explicit%20chain-of-thought%0Areasoning%20prior%20to%20object%20grounding%20and%20thereby%20achieving%20substantial%0Aperformance%20gains.%20In%20this%20paper%2C%20we%20first%20conduct%20extensive%20analysis%0Aexperiments%20of%20three%20key%20components%20of%20that%20training%20pipeline%3A%20input%20design%2C%0Aoutput%20evaluation%2C%20and%20policy%20update-each%20revealing%20distinct%20challenges%20arising%0Afrom%20blindly%20applying%20general-purpose%20RL%20without%20adapting%20to%20GUI%20grounding%0Atasks.%20Input%20design%3A%20Current%20templates%20encourage%20the%20model%20to%20generate%0Achain-of-thought%20reasoning%2C%20but%20longer%20chains%20unexpectedly%20lead%20to%20worse%0Agrounding%20performance.%20Output%20evaluation%3A%20Reward%20functions%20based%20on%20hit%20signals%0Aor%20box%20area%20allow%20models%20to%20exploit%20box%20size%2C%20leading%20to%20reward%20hacking%20and%0Apoor%20localization%20quality.%20Policy%20update%3A%20Online%20RL%20tends%20to%20overfit%20easy%0Aexamples%20due%20to%20biases%20in%20length%20and%20sample%20difficulty%2C%20leading%20to%0Aunder-optimization%20on%20harder%20cases.%20To%20address%20these%20issues%2C%20we%20propose%20three%0Atargeted%20solutions.%20First%2C%20we%20adopt%20a%20Fast%20Thinking%20Template%20that%20encourages%0Adirect%20answer%20generation%2C%20reducing%20excessive%20reasoning%20during%20training.%20Second%2C%0Awe%20incorporate%20a%20box%20size%20constraint%20into%20the%20reward%20function%20to%20mitigate%0Areward%20hacking.%20Third%2C%20we%20revise%20the%20RL%20objective%20by%20adjusting%20length%0Anormalization%20and%20adding%20a%20difficulty-aware%20scaling%20factor%2C%20enabling%20better%0Aoptimization%20on%20hard%20samples.%20Our%20GUI-G1-3B%2C%20trained%20on%2017K%20public%20samples%20with%0AQwen2.5-VL-3B-Instruct%2C%20achieves%2090.3%25%20accuracy%20on%20ScreenSpot%20and%2037.1%25%20on%0AScreenSpot-Pro.%20This%20surpasses%20all%20prior%20models%20of%20similar%20size%20and%20even%0Aoutperforms%20the%20larger%20UI-TARS-7B%2C%20establishing%20a%20new%20state-of-the-art%20in%20GUI%0Aagent%20grounding.%20The%20project%20repository%20is%20available%20at%0Ahttps%3A//github.com/Yuqi-Zhou/GUI-G1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15810v1&entry.124074799=Read"},
{"title": "InstructSAM: A Training-Free Framework for Instruction-Oriented Remote\n  Sensing Object Recognition", "author": "Yijie Zheng and Weijie Wu and Qingyun Li and Xuehui Wang and Xu Zhou and Aiai Ren and Jun Shen and Long Zhao and Guoqing Li and Xue Yang", "abstract": "  Language-Guided object recognition in remote sensing imagery is crucial for\nlarge-scale mapping and automated data annotation. However, existing\nopen-vocabulary and visual grounding methods rely on explicit category cues,\nlimiting their ability to handle complex or implicit queries that require\nadvanced reasoning. To address this issue, we introduce a new suite of tasks,\nincluding Instruction-Oriented Object Counting, Detection, and Segmentation\n(InstructCDS), covering open-vocabulary, open-ended, and open-subclass\nscenarios. We further present EarthInstruct, the first InstructCDS benchmark\nfor earth observation. It is constructed from two diverse remote sensing\ndatasets with varying spatial resolutions and annotation rules across 20\ncategories, necessitating models to interpret dataset-specific instructions.\nGiven the scarcity of semantically rich labeled data in remote sensing, we\npropose InstructSAM, a training-free framework for instruction-driven object\nrecognition. InstructSAM leverages large vision-language models to interpret\nuser instructions and estimate object counts, employs SAM2 for mask proposal,\nand formulates mask-label assignment as a binary integer programming problem.\nBy integrating semantic similarity with counting constraints, InstructSAM\nefficiently assigns categories to predicted masks without relying on confidence\nthresholds. Experiments demonstrate that InstructSAM matches or surpasses\nspecialized baselines across multiple tasks while maintaining near-constant\ninference time regardless of object count, reducing output tokens by 89% and\noverall runtime by over 32% compared to direct generation approaches. We\nbelieve the contributions of the proposed tasks, benchmark, and effective\napproach will advance future research in developing versatile object\nrecognition systems.\n", "link": "http://arxiv.org/abs/2505.15818v1", "date": "2025-05-21", "relevancy": 2.8046, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.57}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.57}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InstructSAM%3A%20A%20Training-Free%20Framework%20for%20Instruction-Oriented%20Remote%0A%20%20Sensing%20Object%20Recognition&body=Title%3A%20InstructSAM%3A%20A%20Training-Free%20Framework%20for%20Instruction-Oriented%20Remote%0A%20%20Sensing%20Object%20Recognition%0AAuthor%3A%20Yijie%20Zheng%20and%20Weijie%20Wu%20and%20Qingyun%20Li%20and%20Xuehui%20Wang%20and%20Xu%20Zhou%20and%20Aiai%20Ren%20and%20Jun%20Shen%20and%20Long%20Zhao%20and%20Guoqing%20Li%20and%20Xue%20Yang%0AAbstract%3A%20%20%20Language-Guided%20object%20recognition%20in%20remote%20sensing%20imagery%20is%20crucial%20for%0Alarge-scale%20mapping%20and%20automated%20data%20annotation.%20However%2C%20existing%0Aopen-vocabulary%20and%20visual%20grounding%20methods%20rely%20on%20explicit%20category%20cues%2C%0Alimiting%20their%20ability%20to%20handle%20complex%20or%20implicit%20queries%20that%20require%0Aadvanced%20reasoning.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20new%20suite%20of%20tasks%2C%0Aincluding%20Instruction-Oriented%20Object%20Counting%2C%20Detection%2C%20and%20Segmentation%0A%28InstructCDS%29%2C%20covering%20open-vocabulary%2C%20open-ended%2C%20and%20open-subclass%0Ascenarios.%20We%20further%20present%20EarthInstruct%2C%20the%20first%20InstructCDS%20benchmark%0Afor%20earth%20observation.%20It%20is%20constructed%20from%20two%20diverse%20remote%20sensing%0Adatasets%20with%20varying%20spatial%20resolutions%20and%20annotation%20rules%20across%2020%0Acategories%2C%20necessitating%20models%20to%20interpret%20dataset-specific%20instructions.%0AGiven%20the%20scarcity%20of%20semantically%20rich%20labeled%20data%20in%20remote%20sensing%2C%20we%0Apropose%20InstructSAM%2C%20a%20training-free%20framework%20for%20instruction-driven%20object%0Arecognition.%20InstructSAM%20leverages%20large%20vision-language%20models%20to%20interpret%0Auser%20instructions%20and%20estimate%20object%20counts%2C%20employs%20SAM2%20for%20mask%20proposal%2C%0Aand%20formulates%20mask-label%20assignment%20as%20a%20binary%20integer%20programming%20problem.%0ABy%20integrating%20semantic%20similarity%20with%20counting%20constraints%2C%20InstructSAM%0Aefficiently%20assigns%20categories%20to%20predicted%20masks%20without%20relying%20on%20confidence%0Athresholds.%20Experiments%20demonstrate%20that%20InstructSAM%20matches%20or%20surpasses%0Aspecialized%20baselines%20across%20multiple%20tasks%20while%20maintaining%20near-constant%0Ainference%20time%20regardless%20of%20object%20count%2C%20reducing%20output%20tokens%20by%2089%25%20and%0Aoverall%20runtime%20by%20over%2032%25%20compared%20to%20direct%20generation%20approaches.%20We%0Abelieve%20the%20contributions%20of%20the%20proposed%20tasks%2C%20benchmark%2C%20and%20effective%0Aapproach%20will%20advance%20future%20research%20in%20developing%20versatile%20object%0Arecognition%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15818v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstructSAM%253A%2520A%2520Training-Free%2520Framework%2520for%2520Instruction-Oriented%2520Remote%250A%2520%2520Sensing%2520Object%2520Recognition%26entry.906535625%3DYijie%2520Zheng%2520and%2520Weijie%2520Wu%2520and%2520Qingyun%2520Li%2520and%2520Xuehui%2520Wang%2520and%2520Xu%2520Zhou%2520and%2520Aiai%2520Ren%2520and%2520Jun%2520Shen%2520and%2520Long%2520Zhao%2520and%2520Guoqing%2520Li%2520and%2520Xue%2520Yang%26entry.1292438233%3D%2520%2520Language-Guided%2520object%2520recognition%2520in%2520remote%2520sensing%2520imagery%2520is%2520crucial%2520for%250Alarge-scale%2520mapping%2520and%2520automated%2520data%2520annotation.%2520However%252C%2520existing%250Aopen-vocabulary%2520and%2520visual%2520grounding%2520methods%2520rely%2520on%2520explicit%2520category%2520cues%252C%250Alimiting%2520their%2520ability%2520to%2520handle%2520complex%2520or%2520implicit%2520queries%2520that%2520require%250Aadvanced%2520reasoning.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520a%2520new%2520suite%2520of%2520tasks%252C%250Aincluding%2520Instruction-Oriented%2520Object%2520Counting%252C%2520Detection%252C%2520and%2520Segmentation%250A%2528InstructCDS%2529%252C%2520covering%2520open-vocabulary%252C%2520open-ended%252C%2520and%2520open-subclass%250Ascenarios.%2520We%2520further%2520present%2520EarthInstruct%252C%2520the%2520first%2520InstructCDS%2520benchmark%250Afor%2520earth%2520observation.%2520It%2520is%2520constructed%2520from%2520two%2520diverse%2520remote%2520sensing%250Adatasets%2520with%2520varying%2520spatial%2520resolutions%2520and%2520annotation%2520rules%2520across%252020%250Acategories%252C%2520necessitating%2520models%2520to%2520interpret%2520dataset-specific%2520instructions.%250AGiven%2520the%2520scarcity%2520of%2520semantically%2520rich%2520labeled%2520data%2520in%2520remote%2520sensing%252C%2520we%250Apropose%2520InstructSAM%252C%2520a%2520training-free%2520framework%2520for%2520instruction-driven%2520object%250Arecognition.%2520InstructSAM%2520leverages%2520large%2520vision-language%2520models%2520to%2520interpret%250Auser%2520instructions%2520and%2520estimate%2520object%2520counts%252C%2520employs%2520SAM2%2520for%2520mask%2520proposal%252C%250Aand%2520formulates%2520mask-label%2520assignment%2520as%2520a%2520binary%2520integer%2520programming%2520problem.%250ABy%2520integrating%2520semantic%2520similarity%2520with%2520counting%2520constraints%252C%2520InstructSAM%250Aefficiently%2520assigns%2520categories%2520to%2520predicted%2520masks%2520without%2520relying%2520on%2520confidence%250Athresholds.%2520Experiments%2520demonstrate%2520that%2520InstructSAM%2520matches%2520or%2520surpasses%250Aspecialized%2520baselines%2520across%2520multiple%2520tasks%2520while%2520maintaining%2520near-constant%250Ainference%2520time%2520regardless%2520of%2520object%2520count%252C%2520reducing%2520output%2520tokens%2520by%252089%2525%2520and%250Aoverall%2520runtime%2520by%2520over%252032%2525%2520compared%2520to%2520direct%2520generation%2520approaches.%2520We%250Abelieve%2520the%2520contributions%2520of%2520the%2520proposed%2520tasks%252C%2520benchmark%252C%2520and%2520effective%250Aapproach%2520will%2520advance%2520future%2520research%2520in%2520developing%2520versatile%2520object%250Arecognition%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15818v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstructSAM%3A%20A%20Training-Free%20Framework%20for%20Instruction-Oriented%20Remote%0A%20%20Sensing%20Object%20Recognition&entry.906535625=Yijie%20Zheng%20and%20Weijie%20Wu%20and%20Qingyun%20Li%20and%20Xuehui%20Wang%20and%20Xu%20Zhou%20and%20Aiai%20Ren%20and%20Jun%20Shen%20and%20Long%20Zhao%20and%20Guoqing%20Li%20and%20Xue%20Yang&entry.1292438233=%20%20Language-Guided%20object%20recognition%20in%20remote%20sensing%20imagery%20is%20crucial%20for%0Alarge-scale%20mapping%20and%20automated%20data%20annotation.%20However%2C%20existing%0Aopen-vocabulary%20and%20visual%20grounding%20methods%20rely%20on%20explicit%20category%20cues%2C%0Alimiting%20their%20ability%20to%20handle%20complex%20or%20implicit%20queries%20that%20require%0Aadvanced%20reasoning.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20new%20suite%20of%20tasks%2C%0Aincluding%20Instruction-Oriented%20Object%20Counting%2C%20Detection%2C%20and%20Segmentation%0A%28InstructCDS%29%2C%20covering%20open-vocabulary%2C%20open-ended%2C%20and%20open-subclass%0Ascenarios.%20We%20further%20present%20EarthInstruct%2C%20the%20first%20InstructCDS%20benchmark%0Afor%20earth%20observation.%20It%20is%20constructed%20from%20two%20diverse%20remote%20sensing%0Adatasets%20with%20varying%20spatial%20resolutions%20and%20annotation%20rules%20across%2020%0Acategories%2C%20necessitating%20models%20to%20interpret%20dataset-specific%20instructions.%0AGiven%20the%20scarcity%20of%20semantically%20rich%20labeled%20data%20in%20remote%20sensing%2C%20we%0Apropose%20InstructSAM%2C%20a%20training-free%20framework%20for%20instruction-driven%20object%0Arecognition.%20InstructSAM%20leverages%20large%20vision-language%20models%20to%20interpret%0Auser%20instructions%20and%20estimate%20object%20counts%2C%20employs%20SAM2%20for%20mask%20proposal%2C%0Aand%20formulates%20mask-label%20assignment%20as%20a%20binary%20integer%20programming%20problem.%0ABy%20integrating%20semantic%20similarity%20with%20counting%20constraints%2C%20InstructSAM%0Aefficiently%20assigns%20categories%20to%20predicted%20masks%20without%20relying%20on%20confidence%0Athresholds.%20Experiments%20demonstrate%20that%20InstructSAM%20matches%20or%20surpasses%0Aspecialized%20baselines%20across%20multiple%20tasks%20while%20maintaining%20near-constant%0Ainference%20time%20regardless%20of%20object%20count%2C%20reducing%20output%20tokens%20by%2089%25%20and%0Aoverall%20runtime%20by%20over%2032%25%20compared%20to%20direct%20generation%20approaches.%20We%0Abelieve%20the%20contributions%20of%20the%20proposed%20tasks%2C%20benchmark%2C%20and%20effective%0Aapproach%20will%20advance%20future%20research%20in%20developing%20versatile%20object%0Arecognition%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15818v1&entry.124074799=Read"},
{"title": "TinyDrive: Multiscale Visual Question Answering with Selective Token\n  Routing for Autonomous Driving", "author": "Hossein Hassani and Soodeh Nikan and Abdallah Shami", "abstract": "  Vision Language Models (VLMs) employed for visual question-answering (VQA) in\nautonomous driving often require substantial computational resources that pose\na challenge for their deployment in resource-constrained vehicles. To address\nthis challenge, we introduce TinyDrive, a lightweight yet effective VLM for\nmulti-view VQA in driving scenarios. Our model comprises two key components\nincluding a multiscale vision encoder and a dual-level prioritization mechanism\nfor tokens and sequences. The multiscale encoder facilitates the processing of\nmulti-view images at diverse resolutions through scale injection and\ncross-scale gating to generate enhanced visual representations. At the token\nlevel, we design a token routing mechanism that dynamically selects and process\nthe most informative tokens based on learned importance scores. At the sequence\nlevel, we propose integrating normalized loss, uncertainty estimates, and a\ndiversity metric to formulate sequence scores that rank and preserve samples\nwithin a sequence priority buffer. Samples with higher scores are more\nfrequently selected for training. TinyDrive is first evaluated on our\ncustom-curated VQA dataset, and it is subsequently tested on the public DriveLM\nbenchmark, where it achieves state-of-the-art language understanding\nperformance. Notably, it achieves relative improvements of 11.1% and 35.4% in\nBLEU-4 and METEOR scores, respectively, despite having a significantly smaller\nparameter count.\n", "link": "http://arxiv.org/abs/2505.15564v1", "date": "2025-05-21", "relevancy": 2.7868, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5675}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5675}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TinyDrive%3A%20Multiscale%20Visual%20Question%20Answering%20with%20Selective%20Token%0A%20%20Routing%20for%20Autonomous%20Driving&body=Title%3A%20TinyDrive%3A%20Multiscale%20Visual%20Question%20Answering%20with%20Selective%20Token%0A%20%20Routing%20for%20Autonomous%20Driving%0AAuthor%3A%20Hossein%20Hassani%20and%20Soodeh%20Nikan%20and%20Abdallah%20Shami%0AAbstract%3A%20%20%20Vision%20Language%20Models%20%28VLMs%29%20employed%20for%20visual%20question-answering%20%28VQA%29%20in%0Aautonomous%20driving%20often%20require%20substantial%20computational%20resources%20that%20pose%0Aa%20challenge%20for%20their%20deployment%20in%20resource-constrained%20vehicles.%20To%20address%0Athis%20challenge%2C%20we%20introduce%20TinyDrive%2C%20a%20lightweight%20yet%20effective%20VLM%20for%0Amulti-view%20VQA%20in%20driving%20scenarios.%20Our%20model%20comprises%20two%20key%20components%0Aincluding%20a%20multiscale%20vision%20encoder%20and%20a%20dual-level%20prioritization%20mechanism%0Afor%20tokens%20and%20sequences.%20The%20multiscale%20encoder%20facilitates%20the%20processing%20of%0Amulti-view%20images%20at%20diverse%20resolutions%20through%20scale%20injection%20and%0Across-scale%20gating%20to%20generate%20enhanced%20visual%20representations.%20At%20the%20token%0Alevel%2C%20we%20design%20a%20token%20routing%20mechanism%20that%20dynamically%20selects%20and%20process%0Athe%20most%20informative%20tokens%20based%20on%20learned%20importance%20scores.%20At%20the%20sequence%0Alevel%2C%20we%20propose%20integrating%20normalized%20loss%2C%20uncertainty%20estimates%2C%20and%20a%0Adiversity%20metric%20to%20formulate%20sequence%20scores%20that%20rank%20and%20preserve%20samples%0Awithin%20a%20sequence%20priority%20buffer.%20Samples%20with%20higher%20scores%20are%20more%0Afrequently%20selected%20for%20training.%20TinyDrive%20is%20first%20evaluated%20on%20our%0Acustom-curated%20VQA%20dataset%2C%20and%20it%20is%20subsequently%20tested%20on%20the%20public%20DriveLM%0Abenchmark%2C%20where%20it%20achieves%20state-of-the-art%20language%20understanding%0Aperformance.%20Notably%2C%20it%20achieves%20relative%20improvements%20of%2011.1%25%20and%2035.4%25%20in%0ABLEU-4%20and%20METEOR%20scores%2C%20respectively%2C%20despite%20having%20a%20significantly%20smaller%0Aparameter%20count.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTinyDrive%253A%2520Multiscale%2520Visual%2520Question%2520Answering%2520with%2520Selective%2520Token%250A%2520%2520Routing%2520for%2520Autonomous%2520Driving%26entry.906535625%3DHossein%2520Hassani%2520and%2520Soodeh%2520Nikan%2520and%2520Abdallah%2520Shami%26entry.1292438233%3D%2520%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520employed%2520for%2520visual%2520question-answering%2520%2528VQA%2529%2520in%250Aautonomous%2520driving%2520often%2520require%2520substantial%2520computational%2520resources%2520that%2520pose%250Aa%2520challenge%2520for%2520their%2520deployment%2520in%2520resource-constrained%2520vehicles.%2520To%2520address%250Athis%2520challenge%252C%2520we%2520introduce%2520TinyDrive%252C%2520a%2520lightweight%2520yet%2520effective%2520VLM%2520for%250Amulti-view%2520VQA%2520in%2520driving%2520scenarios.%2520Our%2520model%2520comprises%2520two%2520key%2520components%250Aincluding%2520a%2520multiscale%2520vision%2520encoder%2520and%2520a%2520dual-level%2520prioritization%2520mechanism%250Afor%2520tokens%2520and%2520sequences.%2520The%2520multiscale%2520encoder%2520facilitates%2520the%2520processing%2520of%250Amulti-view%2520images%2520at%2520diverse%2520resolutions%2520through%2520scale%2520injection%2520and%250Across-scale%2520gating%2520to%2520generate%2520enhanced%2520visual%2520representations.%2520At%2520the%2520token%250Alevel%252C%2520we%2520design%2520a%2520token%2520routing%2520mechanism%2520that%2520dynamically%2520selects%2520and%2520process%250Athe%2520most%2520informative%2520tokens%2520based%2520on%2520learned%2520importance%2520scores.%2520At%2520the%2520sequence%250Alevel%252C%2520we%2520propose%2520integrating%2520normalized%2520loss%252C%2520uncertainty%2520estimates%252C%2520and%2520a%250Adiversity%2520metric%2520to%2520formulate%2520sequence%2520scores%2520that%2520rank%2520and%2520preserve%2520samples%250Awithin%2520a%2520sequence%2520priority%2520buffer.%2520Samples%2520with%2520higher%2520scores%2520are%2520more%250Afrequently%2520selected%2520for%2520training.%2520TinyDrive%2520is%2520first%2520evaluated%2520on%2520our%250Acustom-curated%2520VQA%2520dataset%252C%2520and%2520it%2520is%2520subsequently%2520tested%2520on%2520the%2520public%2520DriveLM%250Abenchmark%252C%2520where%2520it%2520achieves%2520state-of-the-art%2520language%2520understanding%250Aperformance.%2520Notably%252C%2520it%2520achieves%2520relative%2520improvements%2520of%252011.1%2525%2520and%252035.4%2525%2520in%250ABLEU-4%2520and%2520METEOR%2520scores%252C%2520respectively%252C%2520despite%2520having%2520a%2520significantly%2520smaller%250Aparameter%2520count.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TinyDrive%3A%20Multiscale%20Visual%20Question%20Answering%20with%20Selective%20Token%0A%20%20Routing%20for%20Autonomous%20Driving&entry.906535625=Hossein%20Hassani%20and%20Soodeh%20Nikan%20and%20Abdallah%20Shami&entry.1292438233=%20%20Vision%20Language%20Models%20%28VLMs%29%20employed%20for%20visual%20question-answering%20%28VQA%29%20in%0Aautonomous%20driving%20often%20require%20substantial%20computational%20resources%20that%20pose%0Aa%20challenge%20for%20their%20deployment%20in%20resource-constrained%20vehicles.%20To%20address%0Athis%20challenge%2C%20we%20introduce%20TinyDrive%2C%20a%20lightweight%20yet%20effective%20VLM%20for%0Amulti-view%20VQA%20in%20driving%20scenarios.%20Our%20model%20comprises%20two%20key%20components%0Aincluding%20a%20multiscale%20vision%20encoder%20and%20a%20dual-level%20prioritization%20mechanism%0Afor%20tokens%20and%20sequences.%20The%20multiscale%20encoder%20facilitates%20the%20processing%20of%0Amulti-view%20images%20at%20diverse%20resolutions%20through%20scale%20injection%20and%0Across-scale%20gating%20to%20generate%20enhanced%20visual%20representations.%20At%20the%20token%0Alevel%2C%20we%20design%20a%20token%20routing%20mechanism%20that%20dynamically%20selects%20and%20process%0Athe%20most%20informative%20tokens%20based%20on%20learned%20importance%20scores.%20At%20the%20sequence%0Alevel%2C%20we%20propose%20integrating%20normalized%20loss%2C%20uncertainty%20estimates%2C%20and%20a%0Adiversity%20metric%20to%20formulate%20sequence%20scores%20that%20rank%20and%20preserve%20samples%0Awithin%20a%20sequence%20priority%20buffer.%20Samples%20with%20higher%20scores%20are%20more%0Afrequently%20selected%20for%20training.%20TinyDrive%20is%20first%20evaluated%20on%20our%0Acustom-curated%20VQA%20dataset%2C%20and%20it%20is%20subsequently%20tested%20on%20the%20public%20DriveLM%0Abenchmark%2C%20where%20it%20achieves%20state-of-the-art%20language%20understanding%0Aperformance.%20Notably%2C%20it%20achieves%20relative%20improvements%20of%2011.1%25%20and%2035.4%25%20in%0ABLEU-4%20and%20METEOR%20scores%2C%20respectively%2C%20despite%20having%20a%20significantly%20smaller%0Aparameter%20count.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15564v1&entry.124074799=Read"},
{"title": "ViaRL: Adaptive Temporal Grounding via Visual Iterated Amplification\n  Reinforcement Learning", "author": "Ziqiang Xu and Qi Dai and Tian Xie and Yifan Yang and Kai Qiu and DongDong Chen and Zuxuan Wu and Chong Luo", "abstract": "  Video understanding is inherently intention-driven-humans naturally focus on\nrelevant frames based on their goals. Recent advancements in multimodal large\nlanguage models (MLLMs) have enabled flexible query-driven reasoning; however,\nvideo-based frameworks like Video Chain-of-Thought lack direct training signals\nto effectively identify relevant frames. Current approaches often rely on\nheuristic methods or pseudo-label supervised annotations, which are both costly\nand limited in scalability across diverse scenarios. To overcome these\nchallenges, we introduce ViaRL, the first framework to leverage rule-based\nreinforcement learning (RL) for optimizing frame selection in intention-driven\nvideo understanding. An iterated amplification strategy is adopted to perform\nalternating cyclic training in the video CoT system, where each component\nundergoes iterative cycles of refinement to improve its capabilities. ViaRL\nutilizes the answer accuracy of a downstream model as a reward signal to train\na frame selector through trial-and-error, eliminating the need for expensive\nannotations while closely aligning with human-like learning processes.\nComprehensive experiments across multiple benchmarks, including VideoMME,\nLVBench, and MLVU, demonstrate that ViaRL consistently delivers superior\ntemporal grounding performance and robust generalization across diverse video\nunderstanding tasks, highlighting its effectiveness and scalability. Notably,\nViaRL achieves a nearly 15\\% improvement on Needle QA, a subset of MLVU, which\nis required to search a specific needle within a long video and regarded as one\nof the most suitable benchmarks for evaluating temporal grounding.\n", "link": "http://arxiv.org/abs/2505.15447v1", "date": "2025-05-21", "relevancy": 2.7828, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5598}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5598}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViaRL%3A%20Adaptive%20Temporal%20Grounding%20via%20Visual%20Iterated%20Amplification%0A%20%20Reinforcement%20Learning&body=Title%3A%20ViaRL%3A%20Adaptive%20Temporal%20Grounding%20via%20Visual%20Iterated%20Amplification%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Ziqiang%20Xu%20and%20Qi%20Dai%20and%20Tian%20Xie%20and%20Yifan%20Yang%20and%20Kai%20Qiu%20and%20DongDong%20Chen%20and%20Zuxuan%20Wu%20and%20Chong%20Luo%0AAbstract%3A%20%20%20Video%20understanding%20is%20inherently%20intention-driven-humans%20naturally%20focus%20on%0Arelevant%20frames%20based%20on%20their%20goals.%20Recent%20advancements%20in%20multimodal%20large%0Alanguage%20models%20%28MLLMs%29%20have%20enabled%20flexible%20query-driven%20reasoning%3B%20however%2C%0Avideo-based%20frameworks%20like%20Video%20Chain-of-Thought%20lack%20direct%20training%20signals%0Ato%20effectively%20identify%20relevant%20frames.%20Current%20approaches%20often%20rely%20on%0Aheuristic%20methods%20or%20pseudo-label%20supervised%20annotations%2C%20which%20are%20both%20costly%0Aand%20limited%20in%20scalability%20across%20diverse%20scenarios.%20To%20overcome%20these%0Achallenges%2C%20we%20introduce%20ViaRL%2C%20the%20first%20framework%20to%20leverage%20rule-based%0Areinforcement%20learning%20%28RL%29%20for%20optimizing%20frame%20selection%20in%20intention-driven%0Avideo%20understanding.%20An%20iterated%20amplification%20strategy%20is%20adopted%20to%20perform%0Aalternating%20cyclic%20training%20in%20the%20video%20CoT%20system%2C%20where%20each%20component%0Aundergoes%20iterative%20cycles%20of%20refinement%20to%20improve%20its%20capabilities.%20ViaRL%0Autilizes%20the%20answer%20accuracy%20of%20a%20downstream%20model%20as%20a%20reward%20signal%20to%20train%0Aa%20frame%20selector%20through%20trial-and-error%2C%20eliminating%20the%20need%20for%20expensive%0Aannotations%20while%20closely%20aligning%20with%20human-like%20learning%20processes.%0AComprehensive%20experiments%20across%20multiple%20benchmarks%2C%20including%20VideoMME%2C%0ALVBench%2C%20and%20MLVU%2C%20demonstrate%20that%20ViaRL%20consistently%20delivers%20superior%0Atemporal%20grounding%20performance%20and%20robust%20generalization%20across%20diverse%20video%0Aunderstanding%20tasks%2C%20highlighting%20its%20effectiveness%20and%20scalability.%20Notably%2C%0AViaRL%20achieves%20a%20nearly%2015%5C%25%20improvement%20on%20Needle%20QA%2C%20a%20subset%20of%20MLVU%2C%20which%0Ais%20required%20to%20search%20a%20specific%20needle%20within%20a%20long%20video%20and%20regarded%20as%20one%0Aof%20the%20most%20suitable%20benchmarks%20for%20evaluating%20temporal%20grounding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15447v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViaRL%253A%2520Adaptive%2520Temporal%2520Grounding%2520via%2520Visual%2520Iterated%2520Amplification%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DZiqiang%2520Xu%2520and%2520Qi%2520Dai%2520and%2520Tian%2520Xie%2520and%2520Yifan%2520Yang%2520and%2520Kai%2520Qiu%2520and%2520DongDong%2520Chen%2520and%2520Zuxuan%2520Wu%2520and%2520Chong%2520Luo%26entry.1292438233%3D%2520%2520Video%2520understanding%2520is%2520inherently%2520intention-driven-humans%2520naturally%2520focus%2520on%250Arelevant%2520frames%2520based%2520on%2520their%2520goals.%2520Recent%2520advancements%2520in%2520multimodal%2520large%250Alanguage%2520models%2520%2528MLLMs%2529%2520have%2520enabled%2520flexible%2520query-driven%2520reasoning%253B%2520however%252C%250Avideo-based%2520frameworks%2520like%2520Video%2520Chain-of-Thought%2520lack%2520direct%2520training%2520signals%250Ato%2520effectively%2520identify%2520relevant%2520frames.%2520Current%2520approaches%2520often%2520rely%2520on%250Aheuristic%2520methods%2520or%2520pseudo-label%2520supervised%2520annotations%252C%2520which%2520are%2520both%2520costly%250Aand%2520limited%2520in%2520scalability%2520across%2520diverse%2520scenarios.%2520To%2520overcome%2520these%250Achallenges%252C%2520we%2520introduce%2520ViaRL%252C%2520the%2520first%2520framework%2520to%2520leverage%2520rule-based%250Areinforcement%2520learning%2520%2528RL%2529%2520for%2520optimizing%2520frame%2520selection%2520in%2520intention-driven%250Avideo%2520understanding.%2520An%2520iterated%2520amplification%2520strategy%2520is%2520adopted%2520to%2520perform%250Aalternating%2520cyclic%2520training%2520in%2520the%2520video%2520CoT%2520system%252C%2520where%2520each%2520component%250Aundergoes%2520iterative%2520cycles%2520of%2520refinement%2520to%2520improve%2520its%2520capabilities.%2520ViaRL%250Autilizes%2520the%2520answer%2520accuracy%2520of%2520a%2520downstream%2520model%2520as%2520a%2520reward%2520signal%2520to%2520train%250Aa%2520frame%2520selector%2520through%2520trial-and-error%252C%2520eliminating%2520the%2520need%2520for%2520expensive%250Aannotations%2520while%2520closely%2520aligning%2520with%2520human-like%2520learning%2520processes.%250AComprehensive%2520experiments%2520across%2520multiple%2520benchmarks%252C%2520including%2520VideoMME%252C%250ALVBench%252C%2520and%2520MLVU%252C%2520demonstrate%2520that%2520ViaRL%2520consistently%2520delivers%2520superior%250Atemporal%2520grounding%2520performance%2520and%2520robust%2520generalization%2520across%2520diverse%2520video%250Aunderstanding%2520tasks%252C%2520highlighting%2520its%2520effectiveness%2520and%2520scalability.%2520Notably%252C%250AViaRL%2520achieves%2520a%2520nearly%252015%255C%2525%2520improvement%2520on%2520Needle%2520QA%252C%2520a%2520subset%2520of%2520MLVU%252C%2520which%250Ais%2520required%2520to%2520search%2520a%2520specific%2520needle%2520within%2520a%2520long%2520video%2520and%2520regarded%2520as%2520one%250Aof%2520the%2520most%2520suitable%2520benchmarks%2520for%2520evaluating%2520temporal%2520grounding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15447v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViaRL%3A%20Adaptive%20Temporal%20Grounding%20via%20Visual%20Iterated%20Amplification%0A%20%20Reinforcement%20Learning&entry.906535625=Ziqiang%20Xu%20and%20Qi%20Dai%20and%20Tian%20Xie%20and%20Yifan%20Yang%20and%20Kai%20Qiu%20and%20DongDong%20Chen%20and%20Zuxuan%20Wu%20and%20Chong%20Luo&entry.1292438233=%20%20Video%20understanding%20is%20inherently%20intention-driven-humans%20naturally%20focus%20on%0Arelevant%20frames%20based%20on%20their%20goals.%20Recent%20advancements%20in%20multimodal%20large%0Alanguage%20models%20%28MLLMs%29%20have%20enabled%20flexible%20query-driven%20reasoning%3B%20however%2C%0Avideo-based%20frameworks%20like%20Video%20Chain-of-Thought%20lack%20direct%20training%20signals%0Ato%20effectively%20identify%20relevant%20frames.%20Current%20approaches%20often%20rely%20on%0Aheuristic%20methods%20or%20pseudo-label%20supervised%20annotations%2C%20which%20are%20both%20costly%0Aand%20limited%20in%20scalability%20across%20diverse%20scenarios.%20To%20overcome%20these%0Achallenges%2C%20we%20introduce%20ViaRL%2C%20the%20first%20framework%20to%20leverage%20rule-based%0Areinforcement%20learning%20%28RL%29%20for%20optimizing%20frame%20selection%20in%20intention-driven%0Avideo%20understanding.%20An%20iterated%20amplification%20strategy%20is%20adopted%20to%20perform%0Aalternating%20cyclic%20training%20in%20the%20video%20CoT%20system%2C%20where%20each%20component%0Aundergoes%20iterative%20cycles%20of%20refinement%20to%20improve%20its%20capabilities.%20ViaRL%0Autilizes%20the%20answer%20accuracy%20of%20a%20downstream%20model%20as%20a%20reward%20signal%20to%20train%0Aa%20frame%20selector%20through%20trial-and-error%2C%20eliminating%20the%20need%20for%20expensive%0Aannotations%20while%20closely%20aligning%20with%20human-like%20learning%20processes.%0AComprehensive%20experiments%20across%20multiple%20benchmarks%2C%20including%20VideoMME%2C%0ALVBench%2C%20and%20MLVU%2C%20demonstrate%20that%20ViaRL%20consistently%20delivers%20superior%0Atemporal%20grounding%20performance%20and%20robust%20generalization%20across%20diverse%20video%0Aunderstanding%20tasks%2C%20highlighting%20its%20effectiveness%20and%20scalability.%20Notably%2C%0AViaRL%20achieves%20a%20nearly%2015%5C%25%20improvement%20on%20Needle%20QA%2C%20a%20subset%20of%20MLVU%2C%20which%0Ais%20required%20to%20search%20a%20specific%20needle%20within%20a%20long%20video%20and%20regarded%20as%20one%0Aof%20the%20most%20suitable%20benchmarks%20for%20evaluating%20temporal%20grounding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15447v1&entry.124074799=Read"},
{"title": "Streamline Without Sacrifice -- Squeeze out Computation Redundancy in\n  LMM", "author": "Penghao Wu and Lewei Lu and Ziwei Liu", "abstract": "  Large multimodal models excel in multimodal tasks but face significant\ncomputational challenges due to excessive computation on visual tokens. Unlike\ntoken reduction methods that focus on token-level redundancy, we identify and\nstudy the computation-level redundancy on vision tokens to ensure no\ninformation loss. Our key insight is that vision tokens from the pretrained\nvision encoder do not necessarily require all the heavy operations (e.g.,\nself-attention, FFNs) in decoder-only LMMs and could be processed more lightly\nwith proper designs. We designed a series of experiments to discover and\nprogressively squeeze out the vision-related computation redundancy. Based on\nour findings, we propose ProxyV, a novel approach that utilizes proxy vision\ntokens to alleviate the computational burden on original vision tokens. ProxyV\nenhances efficiency without compromising performance and can even yield notable\nperformance gains in scenarios with more moderate efficiency improvements.\nFurthermore, the flexibility of ProxyV is demonstrated through its combination\nwith token reduction methods to boost efficiency further. The code will be made\npublic at this https://github.com/penghao-wu/ProxyV URL.\n", "link": "http://arxiv.org/abs/2505.15816v1", "date": "2025-05-21", "relevancy": 2.7501, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.556}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.547}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Streamline%20Without%20Sacrifice%20--%20Squeeze%20out%20Computation%20Redundancy%20in%0A%20%20LMM&body=Title%3A%20Streamline%20Without%20Sacrifice%20--%20Squeeze%20out%20Computation%20Redundancy%20in%0A%20%20LMM%0AAuthor%3A%20Penghao%20Wu%20and%20Lewei%20Lu%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Large%20multimodal%20models%20excel%20in%20multimodal%20tasks%20but%20face%20significant%0Acomputational%20challenges%20due%20to%20excessive%20computation%20on%20visual%20tokens.%20Unlike%0Atoken%20reduction%20methods%20that%20focus%20on%20token-level%20redundancy%2C%20we%20identify%20and%0Astudy%20the%20computation-level%20redundancy%20on%20vision%20tokens%20to%20ensure%20no%0Ainformation%20loss.%20Our%20key%20insight%20is%20that%20vision%20tokens%20from%20the%20pretrained%0Avision%20encoder%20do%20not%20necessarily%20require%20all%20the%20heavy%20operations%20%28e.g.%2C%0Aself-attention%2C%20FFNs%29%20in%20decoder-only%20LMMs%20and%20could%20be%20processed%20more%20lightly%0Awith%20proper%20designs.%20We%20designed%20a%20series%20of%20experiments%20to%20discover%20and%0Aprogressively%20squeeze%20out%20the%20vision-related%20computation%20redundancy.%20Based%20on%0Aour%20findings%2C%20we%20propose%20ProxyV%2C%20a%20novel%20approach%20that%20utilizes%20proxy%20vision%0Atokens%20to%20alleviate%20the%20computational%20burden%20on%20original%20vision%20tokens.%20ProxyV%0Aenhances%20efficiency%20without%20compromising%20performance%20and%20can%20even%20yield%20notable%0Aperformance%20gains%20in%20scenarios%20with%20more%20moderate%20efficiency%20improvements.%0AFurthermore%2C%20the%20flexibility%20of%20ProxyV%20is%20demonstrated%20through%20its%20combination%0Awith%20token%20reduction%20methods%20to%20boost%20efficiency%20further.%20The%20code%20will%20be%20made%0Apublic%20at%20this%20https%3A//github.com/penghao-wu/ProxyV%20URL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15816v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStreamline%2520Without%2520Sacrifice%2520--%2520Squeeze%2520out%2520Computation%2520Redundancy%2520in%250A%2520%2520LMM%26entry.906535625%3DPenghao%2520Wu%2520and%2520Lewei%2520Lu%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Large%2520multimodal%2520models%2520excel%2520in%2520multimodal%2520tasks%2520but%2520face%2520significant%250Acomputational%2520challenges%2520due%2520to%2520excessive%2520computation%2520on%2520visual%2520tokens.%2520Unlike%250Atoken%2520reduction%2520methods%2520that%2520focus%2520on%2520token-level%2520redundancy%252C%2520we%2520identify%2520and%250Astudy%2520the%2520computation-level%2520redundancy%2520on%2520vision%2520tokens%2520to%2520ensure%2520no%250Ainformation%2520loss.%2520Our%2520key%2520insight%2520is%2520that%2520vision%2520tokens%2520from%2520the%2520pretrained%250Avision%2520encoder%2520do%2520not%2520necessarily%2520require%2520all%2520the%2520heavy%2520operations%2520%2528e.g.%252C%250Aself-attention%252C%2520FFNs%2529%2520in%2520decoder-only%2520LMMs%2520and%2520could%2520be%2520processed%2520more%2520lightly%250Awith%2520proper%2520designs.%2520We%2520designed%2520a%2520series%2520of%2520experiments%2520to%2520discover%2520and%250Aprogressively%2520squeeze%2520out%2520the%2520vision-related%2520computation%2520redundancy.%2520Based%2520on%250Aour%2520findings%252C%2520we%2520propose%2520ProxyV%252C%2520a%2520novel%2520approach%2520that%2520utilizes%2520proxy%2520vision%250Atokens%2520to%2520alleviate%2520the%2520computational%2520burden%2520on%2520original%2520vision%2520tokens.%2520ProxyV%250Aenhances%2520efficiency%2520without%2520compromising%2520performance%2520and%2520can%2520even%2520yield%2520notable%250Aperformance%2520gains%2520in%2520scenarios%2520with%2520more%2520moderate%2520efficiency%2520improvements.%250AFurthermore%252C%2520the%2520flexibility%2520of%2520ProxyV%2520is%2520demonstrated%2520through%2520its%2520combination%250Awith%2520token%2520reduction%2520methods%2520to%2520boost%2520efficiency%2520further.%2520The%2520code%2520will%2520be%2520made%250Apublic%2520at%2520this%2520https%253A//github.com/penghao-wu/ProxyV%2520URL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15816v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Streamline%20Without%20Sacrifice%20--%20Squeeze%20out%20Computation%20Redundancy%20in%0A%20%20LMM&entry.906535625=Penghao%20Wu%20and%20Lewei%20Lu%20and%20Ziwei%20Liu&entry.1292438233=%20%20Large%20multimodal%20models%20excel%20in%20multimodal%20tasks%20but%20face%20significant%0Acomputational%20challenges%20due%20to%20excessive%20computation%20on%20visual%20tokens.%20Unlike%0Atoken%20reduction%20methods%20that%20focus%20on%20token-level%20redundancy%2C%20we%20identify%20and%0Astudy%20the%20computation-level%20redundancy%20on%20vision%20tokens%20to%20ensure%20no%0Ainformation%20loss.%20Our%20key%20insight%20is%20that%20vision%20tokens%20from%20the%20pretrained%0Avision%20encoder%20do%20not%20necessarily%20require%20all%20the%20heavy%20operations%20%28e.g.%2C%0Aself-attention%2C%20FFNs%29%20in%20decoder-only%20LMMs%20and%20could%20be%20processed%20more%20lightly%0Awith%20proper%20designs.%20We%20designed%20a%20series%20of%20experiments%20to%20discover%20and%0Aprogressively%20squeeze%20out%20the%20vision-related%20computation%20redundancy.%20Based%20on%0Aour%20findings%2C%20we%20propose%20ProxyV%2C%20a%20novel%20approach%20that%20utilizes%20proxy%20vision%0Atokens%20to%20alleviate%20the%20computational%20burden%20on%20original%20vision%20tokens.%20ProxyV%0Aenhances%20efficiency%20without%20compromising%20performance%20and%20can%20even%20yield%20notable%0Aperformance%20gains%20in%20scenarios%20with%20more%20moderate%20efficiency%20improvements.%0AFurthermore%2C%20the%20flexibility%20of%20ProxyV%20is%20demonstrated%20through%20its%20combination%0Awith%20token%20reduction%20methods%20to%20boost%20efficiency%20further.%20The%20code%20will%20be%20made%0Apublic%20at%20this%20https%3A//github.com/penghao-wu/ProxyV%20URL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15816v1&entry.124074799=Read"},
{"title": "MIRACL-VISION: A Large, multilingual, visual document retrieval\n  benchmark", "author": "Radek Osmulski and Gabriel de Souza P. Moreira and Ronay Ak and Mengyao Xu and Benedikt Schifferer and Even Oldridge", "abstract": "  Document retrieval is an important task for search and Retrieval-Augmented\nGeneration (RAG) applications. Large Language Models (LLMs) have contributed to\nimproving the accuracy of text-based document retrieval. However, documents\nwith complex layout and visual elements like tables, charts and infographics\nare not perfectly represented in textual format. Recently, image-based document\nretrieval pipelines have become popular, which use visual large language models\n(VLMs) to retrieve relevant page images given a query. Current evaluation\nbenchmarks on visual document retrieval are limited, as they primarily focus\nonly English language, rely on synthetically generated questions and offer a\nsmall corpus size. Therefore, we introduce MIRACL-VISION, a multilingual visual\ndocument retrieval evaluation benchmark. MIRACL-VISION covers 18 languages, and\nis an extension of the MIRACL dataset, a popular benchmark to evaluate\ntext-based multilingual retrieval pipelines. MIRACL was built using a\nhuman-intensive annotation process to generate high-quality questions. In order\nto reduce MIRACL-VISION corpus size to make evaluation more compute friendly\nwhile keeping the datasets challenging, we have designed a method for\neliminating the \"easy\" negatives from the corpus. We conducted extensive\nexperiments comparing MIRACL-VISION with other benchmarks, using popular public\ntext and image models. We observe a gap in state-of-the-art VLM-based embedding\nmodels on multilingual capabilities, with up to 59.7% lower retrieval accuracy\nthan a text-based retrieval models. Even for the English language, the visual\nmodels retrieval accuracy is 12.1% lower compared to text-based models.\nMIRACL-VISION is a challenging, representative, multilingual evaluation\nbenchmark for visual retrieval pipelines and will help the community build\nrobust models for document retrieval.\n", "link": "http://arxiv.org/abs/2505.11651v2", "date": "2025-05-21", "relevancy": 2.7492, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5629}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5629}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MIRACL-VISION%3A%20A%20Large%2C%20multilingual%2C%20visual%20document%20retrieval%0A%20%20benchmark&body=Title%3A%20MIRACL-VISION%3A%20A%20Large%2C%20multilingual%2C%20visual%20document%20retrieval%0A%20%20benchmark%0AAuthor%3A%20Radek%20Osmulski%20and%20Gabriel%20de%20Souza%20P.%20Moreira%20and%20Ronay%20Ak%20and%20Mengyao%20Xu%20and%20Benedikt%20Schifferer%20and%20Even%20Oldridge%0AAbstract%3A%20%20%20Document%20retrieval%20is%20an%20important%20task%20for%20search%20and%20Retrieval-Augmented%0AGeneration%20%28RAG%29%20applications.%20Large%20Language%20Models%20%28LLMs%29%20have%20contributed%20to%0Aimproving%20the%20accuracy%20of%20text-based%20document%20retrieval.%20However%2C%20documents%0Awith%20complex%20layout%20and%20visual%20elements%20like%20tables%2C%20charts%20and%20infographics%0Aare%20not%20perfectly%20represented%20in%20textual%20format.%20Recently%2C%20image-based%20document%0Aretrieval%20pipelines%20have%20become%20popular%2C%20which%20use%20visual%20large%20language%20models%0A%28VLMs%29%20to%20retrieve%20relevant%20page%20images%20given%20a%20query.%20Current%20evaluation%0Abenchmarks%20on%20visual%20document%20retrieval%20are%20limited%2C%20as%20they%20primarily%20focus%0Aonly%20English%20language%2C%20rely%20on%20synthetically%20generated%20questions%20and%20offer%20a%0Asmall%20corpus%20size.%20Therefore%2C%20we%20introduce%20MIRACL-VISION%2C%20a%20multilingual%20visual%0Adocument%20retrieval%20evaluation%20benchmark.%20MIRACL-VISION%20covers%2018%20languages%2C%20and%0Ais%20an%20extension%20of%20the%20MIRACL%20dataset%2C%20a%20popular%20benchmark%20to%20evaluate%0Atext-based%20multilingual%20retrieval%20pipelines.%20MIRACL%20was%20built%20using%20a%0Ahuman-intensive%20annotation%20process%20to%20generate%20high-quality%20questions.%20In%20order%0Ato%20reduce%20MIRACL-VISION%20corpus%20size%20to%20make%20evaluation%20more%20compute%20friendly%0Awhile%20keeping%20the%20datasets%20challenging%2C%20we%20have%20designed%20a%20method%20for%0Aeliminating%20the%20%22easy%22%20negatives%20from%20the%20corpus.%20We%20conducted%20extensive%0Aexperiments%20comparing%20MIRACL-VISION%20with%20other%20benchmarks%2C%20using%20popular%20public%0Atext%20and%20image%20models.%20We%20observe%20a%20gap%20in%20state-of-the-art%20VLM-based%20embedding%0Amodels%20on%20multilingual%20capabilities%2C%20with%20up%20to%2059.7%25%20lower%20retrieval%20accuracy%0Athan%20a%20text-based%20retrieval%20models.%20Even%20for%20the%20English%20language%2C%20the%20visual%0Amodels%20retrieval%20accuracy%20is%2012.1%25%20lower%20compared%20to%20text-based%20models.%0AMIRACL-VISION%20is%20a%20challenging%2C%20representative%2C%20multilingual%20evaluation%0Abenchmark%20for%20visual%20retrieval%20pipelines%20and%20will%20help%20the%20community%20build%0Arobust%20models%20for%20document%20retrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11651v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMIRACL-VISION%253A%2520A%2520Large%252C%2520multilingual%252C%2520visual%2520document%2520retrieval%250A%2520%2520benchmark%26entry.906535625%3DRadek%2520Osmulski%2520and%2520Gabriel%2520de%2520Souza%2520P.%2520Moreira%2520and%2520Ronay%2520Ak%2520and%2520Mengyao%2520Xu%2520and%2520Benedikt%2520Schifferer%2520and%2520Even%2520Oldridge%26entry.1292438233%3D%2520%2520Document%2520retrieval%2520is%2520an%2520important%2520task%2520for%2520search%2520and%2520Retrieval-Augmented%250AGeneration%2520%2528RAG%2529%2520applications.%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520contributed%2520to%250Aimproving%2520the%2520accuracy%2520of%2520text-based%2520document%2520retrieval.%2520However%252C%2520documents%250Awith%2520complex%2520layout%2520and%2520visual%2520elements%2520like%2520tables%252C%2520charts%2520and%2520infographics%250Aare%2520not%2520perfectly%2520represented%2520in%2520textual%2520format.%2520Recently%252C%2520image-based%2520document%250Aretrieval%2520pipelines%2520have%2520become%2520popular%252C%2520which%2520use%2520visual%2520large%2520language%2520models%250A%2528VLMs%2529%2520to%2520retrieve%2520relevant%2520page%2520images%2520given%2520a%2520query.%2520Current%2520evaluation%250Abenchmarks%2520on%2520visual%2520document%2520retrieval%2520are%2520limited%252C%2520as%2520they%2520primarily%2520focus%250Aonly%2520English%2520language%252C%2520rely%2520on%2520synthetically%2520generated%2520questions%2520and%2520offer%2520a%250Asmall%2520corpus%2520size.%2520Therefore%252C%2520we%2520introduce%2520MIRACL-VISION%252C%2520a%2520multilingual%2520visual%250Adocument%2520retrieval%2520evaluation%2520benchmark.%2520MIRACL-VISION%2520covers%252018%2520languages%252C%2520and%250Ais%2520an%2520extension%2520of%2520the%2520MIRACL%2520dataset%252C%2520a%2520popular%2520benchmark%2520to%2520evaluate%250Atext-based%2520multilingual%2520retrieval%2520pipelines.%2520MIRACL%2520was%2520built%2520using%2520a%250Ahuman-intensive%2520annotation%2520process%2520to%2520generate%2520high-quality%2520questions.%2520In%2520order%250Ato%2520reduce%2520MIRACL-VISION%2520corpus%2520size%2520to%2520make%2520evaluation%2520more%2520compute%2520friendly%250Awhile%2520keeping%2520the%2520datasets%2520challenging%252C%2520we%2520have%2520designed%2520a%2520method%2520for%250Aeliminating%2520the%2520%2522easy%2522%2520negatives%2520from%2520the%2520corpus.%2520We%2520conducted%2520extensive%250Aexperiments%2520comparing%2520MIRACL-VISION%2520with%2520other%2520benchmarks%252C%2520using%2520popular%2520public%250Atext%2520and%2520image%2520models.%2520We%2520observe%2520a%2520gap%2520in%2520state-of-the-art%2520VLM-based%2520embedding%250Amodels%2520on%2520multilingual%2520capabilities%252C%2520with%2520up%2520to%252059.7%2525%2520lower%2520retrieval%2520accuracy%250Athan%2520a%2520text-based%2520retrieval%2520models.%2520Even%2520for%2520the%2520English%2520language%252C%2520the%2520visual%250Amodels%2520retrieval%2520accuracy%2520is%252012.1%2525%2520lower%2520compared%2520to%2520text-based%2520models.%250AMIRACL-VISION%2520is%2520a%2520challenging%252C%2520representative%252C%2520multilingual%2520evaluation%250Abenchmark%2520for%2520visual%2520retrieval%2520pipelines%2520and%2520will%2520help%2520the%2520community%2520build%250Arobust%2520models%2520for%2520document%2520retrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11651v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MIRACL-VISION%3A%20A%20Large%2C%20multilingual%2C%20visual%20document%20retrieval%0A%20%20benchmark&entry.906535625=Radek%20Osmulski%20and%20Gabriel%20de%20Souza%20P.%20Moreira%20and%20Ronay%20Ak%20and%20Mengyao%20Xu%20and%20Benedikt%20Schifferer%20and%20Even%20Oldridge&entry.1292438233=%20%20Document%20retrieval%20is%20an%20important%20task%20for%20search%20and%20Retrieval-Augmented%0AGeneration%20%28RAG%29%20applications.%20Large%20Language%20Models%20%28LLMs%29%20have%20contributed%20to%0Aimproving%20the%20accuracy%20of%20text-based%20document%20retrieval.%20However%2C%20documents%0Awith%20complex%20layout%20and%20visual%20elements%20like%20tables%2C%20charts%20and%20infographics%0Aare%20not%20perfectly%20represented%20in%20textual%20format.%20Recently%2C%20image-based%20document%0Aretrieval%20pipelines%20have%20become%20popular%2C%20which%20use%20visual%20large%20language%20models%0A%28VLMs%29%20to%20retrieve%20relevant%20page%20images%20given%20a%20query.%20Current%20evaluation%0Abenchmarks%20on%20visual%20document%20retrieval%20are%20limited%2C%20as%20they%20primarily%20focus%0Aonly%20English%20language%2C%20rely%20on%20synthetically%20generated%20questions%20and%20offer%20a%0Asmall%20corpus%20size.%20Therefore%2C%20we%20introduce%20MIRACL-VISION%2C%20a%20multilingual%20visual%0Adocument%20retrieval%20evaluation%20benchmark.%20MIRACL-VISION%20covers%2018%20languages%2C%20and%0Ais%20an%20extension%20of%20the%20MIRACL%20dataset%2C%20a%20popular%20benchmark%20to%20evaluate%0Atext-based%20multilingual%20retrieval%20pipelines.%20MIRACL%20was%20built%20using%20a%0Ahuman-intensive%20annotation%20process%20to%20generate%20high-quality%20questions.%20In%20order%0Ato%20reduce%20MIRACL-VISION%20corpus%20size%20to%20make%20evaluation%20more%20compute%20friendly%0Awhile%20keeping%20the%20datasets%20challenging%2C%20we%20have%20designed%20a%20method%20for%0Aeliminating%20the%20%22easy%22%20negatives%20from%20the%20corpus.%20We%20conducted%20extensive%0Aexperiments%20comparing%20MIRACL-VISION%20with%20other%20benchmarks%2C%20using%20popular%20public%0Atext%20and%20image%20models.%20We%20observe%20a%20gap%20in%20state-of-the-art%20VLM-based%20embedding%0Amodels%20on%20multilingual%20capabilities%2C%20with%20up%20to%2059.7%25%20lower%20retrieval%20accuracy%0Athan%20a%20text-based%20retrieval%20models.%20Even%20for%20the%20English%20language%2C%20the%20visual%0Amodels%20retrieval%20accuracy%20is%2012.1%25%20lower%20compared%20to%20text-based%20models.%0AMIRACL-VISION%20is%20a%20challenging%2C%20representative%2C%20multilingual%20evaluation%0Abenchmark%20for%20visual%20retrieval%20pipelines%20and%20will%20help%20the%20community%20build%0Arobust%20models%20for%20document%20retrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11651v2&entry.124074799=Read"},
{"title": "CAV-MAE Sync: Improving Contrastive Audio-Visual Mask Autoencoders via\n  Fine-Grained Alignment", "author": "Edson Araujo and Andrew Rouditchenko and Yuan Gong and Saurabhchand Bhati and Samuel Thomas and Brian Kingsbury and Leonid Karlinsky and Rogerio Feris and James R. Glass and Hilde Kuehne", "abstract": "  Recent advances in audio-visual learning have shown promising results in\nlearning representations across modalities. However, most approaches rely on\nglobal audio representations that fail to capture fine-grained temporal\ncorrespondences with visual frames. Additionally, existing methods often\nstruggle with conflicting optimization objectives when trying to jointly learn\nreconstruction and cross-modal alignment. In this work, we propose CAV-MAE Sync\nas a simple yet effective extension of the original CAV-MAE framework for\nself-supervised audio-visual learning. We address three key challenges: First,\nwe tackle the granularity mismatch between modalities by treating audio as a\ntemporal sequence aligned with video frames, rather than using global\nrepresentations. Second, we resolve conflicting optimization goals by\nseparating contrastive and reconstruction objectives through dedicated global\ntokens. Third, we improve spatial localization by introducing learnable\nregister tokens that reduce semantic load on patch tokens. We evaluate the\nproposed approach on AudioSet, VGG Sound, and the ADE20K Sound dataset on\nzero-shot retrieval, classification and localization tasks demonstrating\nstate-of-the-art performance and outperforming more complex architectures.\n", "link": "http://arxiv.org/abs/2505.01237v2", "date": "2025-05-21", "relevancy": 2.7326, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5517}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5488}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAV-MAE%20Sync%3A%20Improving%20Contrastive%20Audio-Visual%20Mask%20Autoencoders%20via%0A%20%20Fine-Grained%20Alignment&body=Title%3A%20CAV-MAE%20Sync%3A%20Improving%20Contrastive%20Audio-Visual%20Mask%20Autoencoders%20via%0A%20%20Fine-Grained%20Alignment%0AAuthor%3A%20Edson%20Araujo%20and%20Andrew%20Rouditchenko%20and%20Yuan%20Gong%20and%20Saurabhchand%20Bhati%20and%20Samuel%20Thomas%20and%20Brian%20Kingsbury%20and%20Leonid%20Karlinsky%20and%20Rogerio%20Feris%20and%20James%20R.%20Glass%20and%20Hilde%20Kuehne%0AAbstract%3A%20%20%20Recent%20advances%20in%20audio-visual%20learning%20have%20shown%20promising%20results%20in%0Alearning%20representations%20across%20modalities.%20However%2C%20most%20approaches%20rely%20on%0Aglobal%20audio%20representations%20that%20fail%20to%20capture%20fine-grained%20temporal%0Acorrespondences%20with%20visual%20frames.%20Additionally%2C%20existing%20methods%20often%0Astruggle%20with%20conflicting%20optimization%20objectives%20when%20trying%20to%20jointly%20learn%0Areconstruction%20and%20cross-modal%20alignment.%20In%20this%20work%2C%20we%20propose%20CAV-MAE%20Sync%0Aas%20a%20simple%20yet%20effective%20extension%20of%20the%20original%20CAV-MAE%20framework%20for%0Aself-supervised%20audio-visual%20learning.%20We%20address%20three%20key%20challenges%3A%20First%2C%0Awe%20tackle%20the%20granularity%20mismatch%20between%20modalities%20by%20treating%20audio%20as%20a%0Atemporal%20sequence%20aligned%20with%20video%20frames%2C%20rather%20than%20using%20global%0Arepresentations.%20Second%2C%20we%20resolve%20conflicting%20optimization%20goals%20by%0Aseparating%20contrastive%20and%20reconstruction%20objectives%20through%20dedicated%20global%0Atokens.%20Third%2C%20we%20improve%20spatial%20localization%20by%20introducing%20learnable%0Aregister%20tokens%20that%20reduce%20semantic%20load%20on%20patch%20tokens.%20We%20evaluate%20the%0Aproposed%20approach%20on%20AudioSet%2C%20VGG%20Sound%2C%20and%20the%20ADE20K%20Sound%20dataset%20on%0Azero-shot%20retrieval%2C%20classification%20and%20localization%20tasks%20demonstrating%0Astate-of-the-art%20performance%20and%20outperforming%20more%20complex%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01237v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAV-MAE%2520Sync%253A%2520Improving%2520Contrastive%2520Audio-Visual%2520Mask%2520Autoencoders%2520via%250A%2520%2520Fine-Grained%2520Alignment%26entry.906535625%3DEdson%2520Araujo%2520and%2520Andrew%2520Rouditchenko%2520and%2520Yuan%2520Gong%2520and%2520Saurabhchand%2520Bhati%2520and%2520Samuel%2520Thomas%2520and%2520Brian%2520Kingsbury%2520and%2520Leonid%2520Karlinsky%2520and%2520Rogerio%2520Feris%2520and%2520James%2520R.%2520Glass%2520and%2520Hilde%2520Kuehne%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520audio-visual%2520learning%2520have%2520shown%2520promising%2520results%2520in%250Alearning%2520representations%2520across%2520modalities.%2520However%252C%2520most%2520approaches%2520rely%2520on%250Aglobal%2520audio%2520representations%2520that%2520fail%2520to%2520capture%2520fine-grained%2520temporal%250Acorrespondences%2520with%2520visual%2520frames.%2520Additionally%252C%2520existing%2520methods%2520often%250Astruggle%2520with%2520conflicting%2520optimization%2520objectives%2520when%2520trying%2520to%2520jointly%2520learn%250Areconstruction%2520and%2520cross-modal%2520alignment.%2520In%2520this%2520work%252C%2520we%2520propose%2520CAV-MAE%2520Sync%250Aas%2520a%2520simple%2520yet%2520effective%2520extension%2520of%2520the%2520original%2520CAV-MAE%2520framework%2520for%250Aself-supervised%2520audio-visual%2520learning.%2520We%2520address%2520three%2520key%2520challenges%253A%2520First%252C%250Awe%2520tackle%2520the%2520granularity%2520mismatch%2520between%2520modalities%2520by%2520treating%2520audio%2520as%2520a%250Atemporal%2520sequence%2520aligned%2520with%2520video%2520frames%252C%2520rather%2520than%2520using%2520global%250Arepresentations.%2520Second%252C%2520we%2520resolve%2520conflicting%2520optimization%2520goals%2520by%250Aseparating%2520contrastive%2520and%2520reconstruction%2520objectives%2520through%2520dedicated%2520global%250Atokens.%2520Third%252C%2520we%2520improve%2520spatial%2520localization%2520by%2520introducing%2520learnable%250Aregister%2520tokens%2520that%2520reduce%2520semantic%2520load%2520on%2520patch%2520tokens.%2520We%2520evaluate%2520the%250Aproposed%2520approach%2520on%2520AudioSet%252C%2520VGG%2520Sound%252C%2520and%2520the%2520ADE20K%2520Sound%2520dataset%2520on%250Azero-shot%2520retrieval%252C%2520classification%2520and%2520localization%2520tasks%2520demonstrating%250Astate-of-the-art%2520performance%2520and%2520outperforming%2520more%2520complex%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01237v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAV-MAE%20Sync%3A%20Improving%20Contrastive%20Audio-Visual%20Mask%20Autoencoders%20via%0A%20%20Fine-Grained%20Alignment&entry.906535625=Edson%20Araujo%20and%20Andrew%20Rouditchenko%20and%20Yuan%20Gong%20and%20Saurabhchand%20Bhati%20and%20Samuel%20Thomas%20and%20Brian%20Kingsbury%20and%20Leonid%20Karlinsky%20and%20Rogerio%20Feris%20and%20James%20R.%20Glass%20and%20Hilde%20Kuehne&entry.1292438233=%20%20Recent%20advances%20in%20audio-visual%20learning%20have%20shown%20promising%20results%20in%0Alearning%20representations%20across%20modalities.%20However%2C%20most%20approaches%20rely%20on%0Aglobal%20audio%20representations%20that%20fail%20to%20capture%20fine-grained%20temporal%0Acorrespondences%20with%20visual%20frames.%20Additionally%2C%20existing%20methods%20often%0Astruggle%20with%20conflicting%20optimization%20objectives%20when%20trying%20to%20jointly%20learn%0Areconstruction%20and%20cross-modal%20alignment.%20In%20this%20work%2C%20we%20propose%20CAV-MAE%20Sync%0Aas%20a%20simple%20yet%20effective%20extension%20of%20the%20original%20CAV-MAE%20framework%20for%0Aself-supervised%20audio-visual%20learning.%20We%20address%20three%20key%20challenges%3A%20First%2C%0Awe%20tackle%20the%20granularity%20mismatch%20between%20modalities%20by%20treating%20audio%20as%20a%0Atemporal%20sequence%20aligned%20with%20video%20frames%2C%20rather%20than%20using%20global%0Arepresentations.%20Second%2C%20we%20resolve%20conflicting%20optimization%20goals%20by%0Aseparating%20contrastive%20and%20reconstruction%20objectives%20through%20dedicated%20global%0Atokens.%20Third%2C%20we%20improve%20spatial%20localization%20by%20introducing%20learnable%0Aregister%20tokens%20that%20reduce%20semantic%20load%20on%20patch%20tokens.%20We%20evaluate%20the%0Aproposed%20approach%20on%20AudioSet%2C%20VGG%20Sound%2C%20and%20the%20ADE20K%20Sound%20dataset%20on%0Azero-shot%20retrieval%2C%20classification%20and%20localization%20tasks%20demonstrating%0Astate-of-the-art%20performance%20and%20outperforming%20more%20complex%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01237v2&entry.124074799=Read"},
{"title": "Efficient Data Driven Mixture-of-Expert Extraction from Trained Networks", "author": "Uranik Berisha and Jens Mehnert and Alexandru Paul Condurache", "abstract": "  Vision Transformers have emerged as the state-of-the-art models in various\nComputer Vision tasks, but their high computational and resource demands pose\nsignificant challenges. While Mixture-of-Experts (MoE) can make these models\nmore efficient, they often require costly retraining or even training from\nscratch. Recent developments aim to reduce these computational costs by\nleveraging pretrained networks. These have been shown to produce sparse\nactivation patterns in the Multi-Layer Perceptrons (MLPs) of the encoder\nblocks, allowing for conditional activation of only relevant subnetworks for\neach sample. Building on this idea, we propose a new method to construct MoE\nvariants from pretrained models. Our approach extracts expert subnetworks from\nthe model's MLP layers post-training in two phases. First, we cluster output\nactivations to identify distinct activation patterns. In the second phase, we\nuse these clusters to extract the corresponding subnetworks responsible for\nproducing them. On ImageNet-1k recognition tasks, we demonstrate that these\nextracted experts can perform surprisingly well out of the box and require only\nminimal fine-tuning to regain 98% of the original performance, all while\nreducing MACs and model size, by up to 36% and 32% respectively.\n", "link": "http://arxiv.org/abs/2505.15414v1", "date": "2025-05-21", "relevancy": 2.724, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5503}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.542}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Data%20Driven%20Mixture-of-Expert%20Extraction%20from%20Trained%20Networks&body=Title%3A%20Efficient%20Data%20Driven%20Mixture-of-Expert%20Extraction%20from%20Trained%20Networks%0AAuthor%3A%20Uranik%20Berisha%20and%20Jens%20Mehnert%20and%20Alexandru%20Paul%20Condurache%0AAbstract%3A%20%20%20Vision%20Transformers%20have%20emerged%20as%20the%20state-of-the-art%20models%20in%20various%0AComputer%20Vision%20tasks%2C%20but%20their%20high%20computational%20and%20resource%20demands%20pose%0Asignificant%20challenges.%20While%20Mixture-of-Experts%20%28MoE%29%20can%20make%20these%20models%0Amore%20efficient%2C%20they%20often%20require%20costly%20retraining%20or%20even%20training%20from%0Ascratch.%20Recent%20developments%20aim%20to%20reduce%20these%20computational%20costs%20by%0Aleveraging%20pretrained%20networks.%20These%20have%20been%20shown%20to%20produce%20sparse%0Aactivation%20patterns%20in%20the%20Multi-Layer%20Perceptrons%20%28MLPs%29%20of%20the%20encoder%0Ablocks%2C%20allowing%20for%20conditional%20activation%20of%20only%20relevant%20subnetworks%20for%0Aeach%20sample.%20Building%20on%20this%20idea%2C%20we%20propose%20a%20new%20method%20to%20construct%20MoE%0Avariants%20from%20pretrained%20models.%20Our%20approach%20extracts%20expert%20subnetworks%20from%0Athe%20model%27s%20MLP%20layers%20post-training%20in%20two%20phases.%20First%2C%20we%20cluster%20output%0Aactivations%20to%20identify%20distinct%20activation%20patterns.%20In%20the%20second%20phase%2C%20we%0Ause%20these%20clusters%20to%20extract%20the%20corresponding%20subnetworks%20responsible%20for%0Aproducing%20them.%20On%20ImageNet-1k%20recognition%20tasks%2C%20we%20demonstrate%20that%20these%0Aextracted%20experts%20can%20perform%20surprisingly%20well%20out%20of%20the%20box%20and%20require%20only%0Aminimal%20fine-tuning%20to%20regain%2098%25%20of%20the%20original%20performance%2C%20all%20while%0Areducing%20MACs%20and%20model%20size%2C%20by%20up%20to%2036%25%20and%2032%25%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15414v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Data%2520Driven%2520Mixture-of-Expert%2520Extraction%2520from%2520Trained%2520Networks%26entry.906535625%3DUranik%2520Berisha%2520and%2520Jens%2520Mehnert%2520and%2520Alexandru%2520Paul%2520Condurache%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520have%2520emerged%2520as%2520the%2520state-of-the-art%2520models%2520in%2520various%250AComputer%2520Vision%2520tasks%252C%2520but%2520their%2520high%2520computational%2520and%2520resource%2520demands%2520pose%250Asignificant%2520challenges.%2520While%2520Mixture-of-Experts%2520%2528MoE%2529%2520can%2520make%2520these%2520models%250Amore%2520efficient%252C%2520they%2520often%2520require%2520costly%2520retraining%2520or%2520even%2520training%2520from%250Ascratch.%2520Recent%2520developments%2520aim%2520to%2520reduce%2520these%2520computational%2520costs%2520by%250Aleveraging%2520pretrained%2520networks.%2520These%2520have%2520been%2520shown%2520to%2520produce%2520sparse%250Aactivation%2520patterns%2520in%2520the%2520Multi-Layer%2520Perceptrons%2520%2528MLPs%2529%2520of%2520the%2520encoder%250Ablocks%252C%2520allowing%2520for%2520conditional%2520activation%2520of%2520only%2520relevant%2520subnetworks%2520for%250Aeach%2520sample.%2520Building%2520on%2520this%2520idea%252C%2520we%2520propose%2520a%2520new%2520method%2520to%2520construct%2520MoE%250Avariants%2520from%2520pretrained%2520models.%2520Our%2520approach%2520extracts%2520expert%2520subnetworks%2520from%250Athe%2520model%2527s%2520MLP%2520layers%2520post-training%2520in%2520two%2520phases.%2520First%252C%2520we%2520cluster%2520output%250Aactivations%2520to%2520identify%2520distinct%2520activation%2520patterns.%2520In%2520the%2520second%2520phase%252C%2520we%250Ause%2520these%2520clusters%2520to%2520extract%2520the%2520corresponding%2520subnetworks%2520responsible%2520for%250Aproducing%2520them.%2520On%2520ImageNet-1k%2520recognition%2520tasks%252C%2520we%2520demonstrate%2520that%2520these%250Aextracted%2520experts%2520can%2520perform%2520surprisingly%2520well%2520out%2520of%2520the%2520box%2520and%2520require%2520only%250Aminimal%2520fine-tuning%2520to%2520regain%252098%2525%2520of%2520the%2520original%2520performance%252C%2520all%2520while%250Areducing%2520MACs%2520and%2520model%2520size%252C%2520by%2520up%2520to%252036%2525%2520and%252032%2525%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15414v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Data%20Driven%20Mixture-of-Expert%20Extraction%20from%20Trained%20Networks&entry.906535625=Uranik%20Berisha%20and%20Jens%20Mehnert%20and%20Alexandru%20Paul%20Condurache&entry.1292438233=%20%20Vision%20Transformers%20have%20emerged%20as%20the%20state-of-the-art%20models%20in%20various%0AComputer%20Vision%20tasks%2C%20but%20their%20high%20computational%20and%20resource%20demands%20pose%0Asignificant%20challenges.%20While%20Mixture-of-Experts%20%28MoE%29%20can%20make%20these%20models%0Amore%20efficient%2C%20they%20often%20require%20costly%20retraining%20or%20even%20training%20from%0Ascratch.%20Recent%20developments%20aim%20to%20reduce%20these%20computational%20costs%20by%0Aleveraging%20pretrained%20networks.%20These%20have%20been%20shown%20to%20produce%20sparse%0Aactivation%20patterns%20in%20the%20Multi-Layer%20Perceptrons%20%28MLPs%29%20of%20the%20encoder%0Ablocks%2C%20allowing%20for%20conditional%20activation%20of%20only%20relevant%20subnetworks%20for%0Aeach%20sample.%20Building%20on%20this%20idea%2C%20we%20propose%20a%20new%20method%20to%20construct%20MoE%0Avariants%20from%20pretrained%20models.%20Our%20approach%20extracts%20expert%20subnetworks%20from%0Athe%20model%27s%20MLP%20layers%20post-training%20in%20two%20phases.%20First%2C%20we%20cluster%20output%0Aactivations%20to%20identify%20distinct%20activation%20patterns.%20In%20the%20second%20phase%2C%20we%0Ause%20these%20clusters%20to%20extract%20the%20corresponding%20subnetworks%20responsible%20for%0Aproducing%20them.%20On%20ImageNet-1k%20recognition%20tasks%2C%20we%20demonstrate%20that%20these%0Aextracted%20experts%20can%20perform%20surprisingly%20well%20out%20of%20the%20box%20and%20require%20only%0Aminimal%20fine-tuning%20to%20regain%2098%25%20of%20the%20original%20performance%2C%20all%20while%0Areducing%20MACs%20and%20model%20size%2C%20by%20up%20to%2036%25%20and%2032%25%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15414v1&entry.124074799=Read"},
{"title": "M3TR: A Generalist Model for Real-World HD Map Completion", "author": "Fabian Immel and Richard Fehler and Frank Bieder and Jan-Hendrik Pauls and Christoph Stiller", "abstract": "  Autonomous vehicles rely on HD maps for their operation, but offline HD maps\neventually become outdated. For this reason, online HD map construction methods\nuse live sensor data to infer map information instead. Research on real map\nchanges shows that oftentimes entire parts of an HD map remain unchanged and\ncan be used as a prior. We therefore introduce M3TR (Multi-Masking Map\nTransformer), a generalist approach for HD map completion both with and without\noffline HD map priors. As a necessary foundation, we address shortcomings in\nground truth labels for Argoverse 2 and nuScenes and propose the first\ncomprehensive benchmark for HD map completion. Unlike existing models that\nspecialize in a single kind of map change, which is unrealistic for deployment,\nour Generalist model handles all kinds of changes, matching the effectiveness\nof Expert models. With our map masking as augmentation regime, we can even\nachieve a +1.4 mAP improvement without a prior. Finally, by fully utilizing\nprior HD map elements and optimizing query designs, M3TR outperforms existing\nmethods by +4.3 mAP while being the first real-world deployable model for\noffline HD map priors. Code is available at https://github.com/immel-f/m3tr\n", "link": "http://arxiv.org/abs/2411.10316v4", "date": "2025-05-21", "relevancy": 2.7162, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.55}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5446}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M3TR%3A%20A%20Generalist%20Model%20for%20Real-World%20HD%20Map%20Completion&body=Title%3A%20M3TR%3A%20A%20Generalist%20Model%20for%20Real-World%20HD%20Map%20Completion%0AAuthor%3A%20Fabian%20Immel%20and%20Richard%20Fehler%20and%20Frank%20Bieder%20and%20Jan-Hendrik%20Pauls%20and%20Christoph%20Stiller%0AAbstract%3A%20%20%20Autonomous%20vehicles%20rely%20on%20HD%20maps%20for%20their%20operation%2C%20but%20offline%20HD%20maps%0Aeventually%20become%20outdated.%20For%20this%20reason%2C%20online%20HD%20map%20construction%20methods%0Ause%20live%20sensor%20data%20to%20infer%20map%20information%20instead.%20Research%20on%20real%20map%0Achanges%20shows%20that%20oftentimes%20entire%20parts%20of%20an%20HD%20map%20remain%20unchanged%20and%0Acan%20be%20used%20as%20a%20prior.%20We%20therefore%20introduce%20M3TR%20%28Multi-Masking%20Map%0ATransformer%29%2C%20a%20generalist%20approach%20for%20HD%20map%20completion%20both%20with%20and%20without%0Aoffline%20HD%20map%20priors.%20As%20a%20necessary%20foundation%2C%20we%20address%20shortcomings%20in%0Aground%20truth%20labels%20for%20Argoverse%202%20and%20nuScenes%20and%20propose%20the%20first%0Acomprehensive%20benchmark%20for%20HD%20map%20completion.%20Unlike%20existing%20models%20that%0Aspecialize%20in%20a%20single%20kind%20of%20map%20change%2C%20which%20is%20unrealistic%20for%20deployment%2C%0Aour%20Generalist%20model%20handles%20all%20kinds%20of%20changes%2C%20matching%20the%20effectiveness%0Aof%20Expert%20models.%20With%20our%20map%20masking%20as%20augmentation%20regime%2C%20we%20can%20even%0Aachieve%20a%20%2B1.4%20mAP%20improvement%20without%20a%20prior.%20Finally%2C%20by%20fully%20utilizing%0Aprior%20HD%20map%20elements%20and%20optimizing%20query%20designs%2C%20M3TR%20outperforms%20existing%0Amethods%20by%20%2B4.3%20mAP%20while%20being%20the%20first%20real-world%20deployable%20model%20for%0Aoffline%20HD%20map%20priors.%20Code%20is%20available%20at%20https%3A//github.com/immel-f/m3tr%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10316v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM3TR%253A%2520A%2520Generalist%2520Model%2520for%2520Real-World%2520HD%2520Map%2520Completion%26entry.906535625%3DFabian%2520Immel%2520and%2520Richard%2520Fehler%2520and%2520Frank%2520Bieder%2520and%2520Jan-Hendrik%2520Pauls%2520and%2520Christoph%2520Stiller%26entry.1292438233%3D%2520%2520Autonomous%2520vehicles%2520rely%2520on%2520HD%2520maps%2520for%2520their%2520operation%252C%2520but%2520offline%2520HD%2520maps%250Aeventually%2520become%2520outdated.%2520For%2520this%2520reason%252C%2520online%2520HD%2520map%2520construction%2520methods%250Ause%2520live%2520sensor%2520data%2520to%2520infer%2520map%2520information%2520instead.%2520Research%2520on%2520real%2520map%250Achanges%2520shows%2520that%2520oftentimes%2520entire%2520parts%2520of%2520an%2520HD%2520map%2520remain%2520unchanged%2520and%250Acan%2520be%2520used%2520as%2520a%2520prior.%2520We%2520therefore%2520introduce%2520M3TR%2520%2528Multi-Masking%2520Map%250ATransformer%2529%252C%2520a%2520generalist%2520approach%2520for%2520HD%2520map%2520completion%2520both%2520with%2520and%2520without%250Aoffline%2520HD%2520map%2520priors.%2520As%2520a%2520necessary%2520foundation%252C%2520we%2520address%2520shortcomings%2520in%250Aground%2520truth%2520labels%2520for%2520Argoverse%25202%2520and%2520nuScenes%2520and%2520propose%2520the%2520first%250Acomprehensive%2520benchmark%2520for%2520HD%2520map%2520completion.%2520Unlike%2520existing%2520models%2520that%250Aspecialize%2520in%2520a%2520single%2520kind%2520of%2520map%2520change%252C%2520which%2520is%2520unrealistic%2520for%2520deployment%252C%250Aour%2520Generalist%2520model%2520handles%2520all%2520kinds%2520of%2520changes%252C%2520matching%2520the%2520effectiveness%250Aof%2520Expert%2520models.%2520With%2520our%2520map%2520masking%2520as%2520augmentation%2520regime%252C%2520we%2520can%2520even%250Aachieve%2520a%2520%252B1.4%2520mAP%2520improvement%2520without%2520a%2520prior.%2520Finally%252C%2520by%2520fully%2520utilizing%250Aprior%2520HD%2520map%2520elements%2520and%2520optimizing%2520query%2520designs%252C%2520M3TR%2520outperforms%2520existing%250Amethods%2520by%2520%252B4.3%2520mAP%2520while%2520being%2520the%2520first%2520real-world%2520deployable%2520model%2520for%250Aoffline%2520HD%2520map%2520priors.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/immel-f/m3tr%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10316v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M3TR%3A%20A%20Generalist%20Model%20for%20Real-World%20HD%20Map%20Completion&entry.906535625=Fabian%20Immel%20and%20Richard%20Fehler%20and%20Frank%20Bieder%20and%20Jan-Hendrik%20Pauls%20and%20Christoph%20Stiller&entry.1292438233=%20%20Autonomous%20vehicles%20rely%20on%20HD%20maps%20for%20their%20operation%2C%20but%20offline%20HD%20maps%0Aeventually%20become%20outdated.%20For%20this%20reason%2C%20online%20HD%20map%20construction%20methods%0Ause%20live%20sensor%20data%20to%20infer%20map%20information%20instead.%20Research%20on%20real%20map%0Achanges%20shows%20that%20oftentimes%20entire%20parts%20of%20an%20HD%20map%20remain%20unchanged%20and%0Acan%20be%20used%20as%20a%20prior.%20We%20therefore%20introduce%20M3TR%20%28Multi-Masking%20Map%0ATransformer%29%2C%20a%20generalist%20approach%20for%20HD%20map%20completion%20both%20with%20and%20without%0Aoffline%20HD%20map%20priors.%20As%20a%20necessary%20foundation%2C%20we%20address%20shortcomings%20in%0Aground%20truth%20labels%20for%20Argoverse%202%20and%20nuScenes%20and%20propose%20the%20first%0Acomprehensive%20benchmark%20for%20HD%20map%20completion.%20Unlike%20existing%20models%20that%0Aspecialize%20in%20a%20single%20kind%20of%20map%20change%2C%20which%20is%20unrealistic%20for%20deployment%2C%0Aour%20Generalist%20model%20handles%20all%20kinds%20of%20changes%2C%20matching%20the%20effectiveness%0Aof%20Expert%20models.%20With%20our%20map%20masking%20as%20augmentation%20regime%2C%20we%20can%20even%0Aachieve%20a%20%2B1.4%20mAP%20improvement%20without%20a%20prior.%20Finally%2C%20by%20fully%20utilizing%0Aprior%20HD%20map%20elements%20and%20optimizing%20query%20designs%2C%20M3TR%20outperforms%20existing%0Amethods%20by%20%2B4.3%20mAP%20while%20being%20the%20first%20real-world%20deployable%20model%20for%0Aoffline%20HD%20map%20priors.%20Code%20is%20available%20at%20https%3A//github.com/immel-f/m3tr%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10316v4&entry.124074799=Read"},
{"title": "The P$^3$ dataset: Pixels, Points and Polygons for Multimodal Building\n  Vectorization", "author": "Raphael Sulzer and Liuyun Duan and Nicolas Girard and Florent Lafarge", "abstract": "  We present the P$^3$ dataset, a large-scale multimodal benchmark for building\nvectorization, constructed from aerial LiDAR point clouds, high-resolution\naerial imagery, and vectorized 2D building outlines, collected across three\ncontinents. The dataset contains over 10 billion LiDAR points with\ndecimeter-level accuracy and RGB images at a ground sampling distance of 25\ncentimeter. While many existing datasets primarily focus on the image modality,\nP$^3$ offers a complementary perspective by also incorporating dense 3D\ninformation. We demonstrate that LiDAR point clouds serve as a robust modality\nfor predicting building polygons, both in hybrid and end-to-end learning\nframeworks. Moreover, fusing aerial LiDAR and imagery further improves accuracy\nand geometric quality of predicted polygons. The P$^3$ dataset is publicly\navailable, along with code and pretrained weights of three state-of-the-art\nmodels for building polygon prediction at\nhttps://github.com/raphaelsulzer/PixelsPointsPolygons .\n", "link": "http://arxiv.org/abs/2505.15379v1", "date": "2025-05-21", "relevancy": 2.6876, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5416}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5357}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20P%24%5E3%24%20dataset%3A%20Pixels%2C%20Points%20and%20Polygons%20for%20Multimodal%20Building%0A%20%20Vectorization&body=Title%3A%20The%20P%24%5E3%24%20dataset%3A%20Pixels%2C%20Points%20and%20Polygons%20for%20Multimodal%20Building%0A%20%20Vectorization%0AAuthor%3A%20Raphael%20Sulzer%20and%20Liuyun%20Duan%20and%20Nicolas%20Girard%20and%20Florent%20Lafarge%0AAbstract%3A%20%20%20We%20present%20the%20P%24%5E3%24%20dataset%2C%20a%20large-scale%20multimodal%20benchmark%20for%20building%0Avectorization%2C%20constructed%20from%20aerial%20LiDAR%20point%20clouds%2C%20high-resolution%0Aaerial%20imagery%2C%20and%20vectorized%202D%20building%20outlines%2C%20collected%20across%20three%0Acontinents.%20The%20dataset%20contains%20over%2010%20billion%20LiDAR%20points%20with%0Adecimeter-level%20accuracy%20and%20RGB%20images%20at%20a%20ground%20sampling%20distance%20of%2025%0Acentimeter.%20While%20many%20existing%20datasets%20primarily%20focus%20on%20the%20image%20modality%2C%0AP%24%5E3%24%20offers%20a%20complementary%20perspective%20by%20also%20incorporating%20dense%203D%0Ainformation.%20We%20demonstrate%20that%20LiDAR%20point%20clouds%20serve%20as%20a%20robust%20modality%0Afor%20predicting%20building%20polygons%2C%20both%20in%20hybrid%20and%20end-to-end%20learning%0Aframeworks.%20Moreover%2C%20fusing%20aerial%20LiDAR%20and%20imagery%20further%20improves%20accuracy%0Aand%20geometric%20quality%20of%20predicted%20polygons.%20The%20P%24%5E3%24%20dataset%20is%20publicly%0Aavailable%2C%20along%20with%20code%20and%20pretrained%20weights%20of%20three%20state-of-the-art%0Amodels%20for%20building%20polygon%20prediction%20at%0Ahttps%3A//github.com/raphaelsulzer/PixelsPointsPolygons%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15379v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520P%2524%255E3%2524%2520dataset%253A%2520Pixels%252C%2520Points%2520and%2520Polygons%2520for%2520Multimodal%2520Building%250A%2520%2520Vectorization%26entry.906535625%3DRaphael%2520Sulzer%2520and%2520Liuyun%2520Duan%2520and%2520Nicolas%2520Girard%2520and%2520Florent%2520Lafarge%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520P%2524%255E3%2524%2520dataset%252C%2520a%2520large-scale%2520multimodal%2520benchmark%2520for%2520building%250Avectorization%252C%2520constructed%2520from%2520aerial%2520LiDAR%2520point%2520clouds%252C%2520high-resolution%250Aaerial%2520imagery%252C%2520and%2520vectorized%25202D%2520building%2520outlines%252C%2520collected%2520across%2520three%250Acontinents.%2520The%2520dataset%2520contains%2520over%252010%2520billion%2520LiDAR%2520points%2520with%250Adecimeter-level%2520accuracy%2520and%2520RGB%2520images%2520at%2520a%2520ground%2520sampling%2520distance%2520of%252025%250Acentimeter.%2520While%2520many%2520existing%2520datasets%2520primarily%2520focus%2520on%2520the%2520image%2520modality%252C%250AP%2524%255E3%2524%2520offers%2520a%2520complementary%2520perspective%2520by%2520also%2520incorporating%2520dense%25203D%250Ainformation.%2520We%2520demonstrate%2520that%2520LiDAR%2520point%2520clouds%2520serve%2520as%2520a%2520robust%2520modality%250Afor%2520predicting%2520building%2520polygons%252C%2520both%2520in%2520hybrid%2520and%2520end-to-end%2520learning%250Aframeworks.%2520Moreover%252C%2520fusing%2520aerial%2520LiDAR%2520and%2520imagery%2520further%2520improves%2520accuracy%250Aand%2520geometric%2520quality%2520of%2520predicted%2520polygons.%2520The%2520P%2524%255E3%2524%2520dataset%2520is%2520publicly%250Aavailable%252C%2520along%2520with%2520code%2520and%2520pretrained%2520weights%2520of%2520three%2520state-of-the-art%250Amodels%2520for%2520building%2520polygon%2520prediction%2520at%250Ahttps%253A//github.com/raphaelsulzer/PixelsPointsPolygons%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15379v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20P%24%5E3%24%20dataset%3A%20Pixels%2C%20Points%20and%20Polygons%20for%20Multimodal%20Building%0A%20%20Vectorization&entry.906535625=Raphael%20Sulzer%20and%20Liuyun%20Duan%20and%20Nicolas%20Girard%20and%20Florent%20Lafarge&entry.1292438233=%20%20We%20present%20the%20P%24%5E3%24%20dataset%2C%20a%20large-scale%20multimodal%20benchmark%20for%20building%0Avectorization%2C%20constructed%20from%20aerial%20LiDAR%20point%20clouds%2C%20high-resolution%0Aaerial%20imagery%2C%20and%20vectorized%202D%20building%20outlines%2C%20collected%20across%20three%0Acontinents.%20The%20dataset%20contains%20over%2010%20billion%20LiDAR%20points%20with%0Adecimeter-level%20accuracy%20and%20RGB%20images%20at%20a%20ground%20sampling%20distance%20of%2025%0Acentimeter.%20While%20many%20existing%20datasets%20primarily%20focus%20on%20the%20image%20modality%2C%0AP%24%5E3%24%20offers%20a%20complementary%20perspective%20by%20also%20incorporating%20dense%203D%0Ainformation.%20We%20demonstrate%20that%20LiDAR%20point%20clouds%20serve%20as%20a%20robust%20modality%0Afor%20predicting%20building%20polygons%2C%20both%20in%20hybrid%20and%20end-to-end%20learning%0Aframeworks.%20Moreover%2C%20fusing%20aerial%20LiDAR%20and%20imagery%20further%20improves%20accuracy%0Aand%20geometric%20quality%20of%20predicted%20polygons.%20The%20P%24%5E3%24%20dataset%20is%20publicly%0Aavailable%2C%20along%20with%20code%20and%20pretrained%20weights%20of%20three%20state-of-the-art%0Amodels%20for%20building%20polygon%20prediction%20at%0Ahttps%3A//github.com/raphaelsulzer/PixelsPointsPolygons%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15379v1&entry.124074799=Read"},
{"title": "Scaling and Enhancing LLM-based AVSR: A Sparse Mixture of Projectors\n  Approach", "author": "Umberto Cappellazzo and Minsu Kim and Stavros Petridis and Daniele Falavigna and Alessio Brutti", "abstract": "  Audio-Visual Speech Recognition (AVSR) enhances robustness in noisy\nenvironments by integrating visual cues. While recent advances integrate Large\nLanguage Models (LLMs) into AVSR, their high computational cost hinders\ndeployment in resource-constrained settings. To address this, we propose\nLlama-SMoP, an efficient Multimodal LLM that employs a Sparse Mixture of\nProjectors (SMoP) module to scale model capacity without increasing inference\ncosts. By incorporating sparsely-gated mixture-of-experts (MoE) projectors,\nLlama-SMoP enables the use of smaller LLMs while maintaining strong\nperformance. We explore three SMoP configurations and show that Llama-SMoP DEDR\n(Disjoint-Experts, Disjoint-Routers), which uses modality-specific routers and\nexperts, achieves superior performance on ASR, VSR, and AVSR tasks. Ablation\nstudies confirm its effectiveness in expert activation, scalability, and noise\nrobustness.\n", "link": "http://arxiv.org/abs/2505.14336v2", "date": "2025-05-21", "relevancy": 2.6791, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5376}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5376}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20and%20Enhancing%20LLM-based%20AVSR%3A%20A%20Sparse%20Mixture%20of%20Projectors%0A%20%20Approach&body=Title%3A%20Scaling%20and%20Enhancing%20LLM-based%20AVSR%3A%20A%20Sparse%20Mixture%20of%20Projectors%0A%20%20Approach%0AAuthor%3A%20Umberto%20Cappellazzo%20and%20Minsu%20Kim%20and%20Stavros%20Petridis%20and%20Daniele%20Falavigna%20and%20Alessio%20Brutti%0AAbstract%3A%20%20%20Audio-Visual%20Speech%20Recognition%20%28AVSR%29%20enhances%20robustness%20in%20noisy%0Aenvironments%20by%20integrating%20visual%20cues.%20While%20recent%20advances%20integrate%20Large%0ALanguage%20Models%20%28LLMs%29%20into%20AVSR%2C%20their%20high%20computational%20cost%20hinders%0Adeployment%20in%20resource-constrained%20settings.%20To%20address%20this%2C%20we%20propose%0ALlama-SMoP%2C%20an%20efficient%20Multimodal%20LLM%20that%20employs%20a%20Sparse%20Mixture%20of%0AProjectors%20%28SMoP%29%20module%20to%20scale%20model%20capacity%20without%20increasing%20inference%0Acosts.%20By%20incorporating%20sparsely-gated%20mixture-of-experts%20%28MoE%29%20projectors%2C%0ALlama-SMoP%20enables%20the%20use%20of%20smaller%20LLMs%20while%20maintaining%20strong%0Aperformance.%20We%20explore%20three%20SMoP%20configurations%20and%20show%20that%20Llama-SMoP%20DEDR%0A%28Disjoint-Experts%2C%20Disjoint-Routers%29%2C%20which%20uses%20modality-specific%20routers%20and%0Aexperts%2C%20achieves%20superior%20performance%20on%20ASR%2C%20VSR%2C%20and%20AVSR%20tasks.%20Ablation%0Astudies%20confirm%20its%20effectiveness%20in%20expert%20activation%2C%20scalability%2C%20and%20noise%0Arobustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14336v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520and%2520Enhancing%2520LLM-based%2520AVSR%253A%2520A%2520Sparse%2520Mixture%2520of%2520Projectors%250A%2520%2520Approach%26entry.906535625%3DUmberto%2520Cappellazzo%2520and%2520Minsu%2520Kim%2520and%2520Stavros%2520Petridis%2520and%2520Daniele%2520Falavigna%2520and%2520Alessio%2520Brutti%26entry.1292438233%3D%2520%2520Audio-Visual%2520Speech%2520Recognition%2520%2528AVSR%2529%2520enhances%2520robustness%2520in%2520noisy%250Aenvironments%2520by%2520integrating%2520visual%2520cues.%2520While%2520recent%2520advances%2520integrate%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520into%2520AVSR%252C%2520their%2520high%2520computational%2520cost%2520hinders%250Adeployment%2520in%2520resource-constrained%2520settings.%2520To%2520address%2520this%252C%2520we%2520propose%250ALlama-SMoP%252C%2520an%2520efficient%2520Multimodal%2520LLM%2520that%2520employs%2520a%2520Sparse%2520Mixture%2520of%250AProjectors%2520%2528SMoP%2529%2520module%2520to%2520scale%2520model%2520capacity%2520without%2520increasing%2520inference%250Acosts.%2520By%2520incorporating%2520sparsely-gated%2520mixture-of-experts%2520%2528MoE%2529%2520projectors%252C%250ALlama-SMoP%2520enables%2520the%2520use%2520of%2520smaller%2520LLMs%2520while%2520maintaining%2520strong%250Aperformance.%2520We%2520explore%2520three%2520SMoP%2520configurations%2520and%2520show%2520that%2520Llama-SMoP%2520DEDR%250A%2528Disjoint-Experts%252C%2520Disjoint-Routers%2529%252C%2520which%2520uses%2520modality-specific%2520routers%2520and%250Aexperts%252C%2520achieves%2520superior%2520performance%2520on%2520ASR%252C%2520VSR%252C%2520and%2520AVSR%2520tasks.%2520Ablation%250Astudies%2520confirm%2520its%2520effectiveness%2520in%2520expert%2520activation%252C%2520scalability%252C%2520and%2520noise%250Arobustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14336v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20and%20Enhancing%20LLM-based%20AVSR%3A%20A%20Sparse%20Mixture%20of%20Projectors%0A%20%20Approach&entry.906535625=Umberto%20Cappellazzo%20and%20Minsu%20Kim%20and%20Stavros%20Petridis%20and%20Daniele%20Falavigna%20and%20Alessio%20Brutti&entry.1292438233=%20%20Audio-Visual%20Speech%20Recognition%20%28AVSR%29%20enhances%20robustness%20in%20noisy%0Aenvironments%20by%20integrating%20visual%20cues.%20While%20recent%20advances%20integrate%20Large%0ALanguage%20Models%20%28LLMs%29%20into%20AVSR%2C%20their%20high%20computational%20cost%20hinders%0Adeployment%20in%20resource-constrained%20settings.%20To%20address%20this%2C%20we%20propose%0ALlama-SMoP%2C%20an%20efficient%20Multimodal%20LLM%20that%20employs%20a%20Sparse%20Mixture%20of%0AProjectors%20%28SMoP%29%20module%20to%20scale%20model%20capacity%20without%20increasing%20inference%0Acosts.%20By%20incorporating%20sparsely-gated%20mixture-of-experts%20%28MoE%29%20projectors%2C%0ALlama-SMoP%20enables%20the%20use%20of%20smaller%20LLMs%20while%20maintaining%20strong%0Aperformance.%20We%20explore%20three%20SMoP%20configurations%20and%20show%20that%20Llama-SMoP%20DEDR%0A%28Disjoint-Experts%2C%20Disjoint-Routers%29%2C%20which%20uses%20modality-specific%20routers%20and%0Aexperts%2C%20achieves%20superior%20performance%20on%20ASR%2C%20VSR%2C%20and%20AVSR%20tasks.%20Ablation%0Astudies%20confirm%20its%20effectiveness%20in%20expert%20activation%2C%20scalability%2C%20and%20noise%0Arobustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14336v2&entry.124074799=Read"},
{"title": "SoftHGNN: Soft Hypergraph Neural Networks for General Visual Recognition", "author": "Mengqi Lei and Yihong Wu and Siqi Li and Xinhu Zheng and Juan Wang and Yue Gao and Shaoyi Du", "abstract": "  Visual recognition relies on understanding both the semantics of image tokens\nand the complex interactions among them. Mainstream self-attention methods,\nwhile effective at modeling global pair-wise relations, fail to capture\nhigh-order associations inherent in real-world scenes and often suffer from\nredundant computation. Hypergraphs extend conventional graphs by modeling\nhigh-order interactions and offer a promising framework for addressing these\nlimitations. However, existing hypergraph neural networks typically rely on\nstatic and hard hyperedge assignments, leading to excessive and redundant\nhyperedges with hard binary vertex memberships that overlook the continuity of\nvisual semantics. To overcome these issues, we present Soft Hypergraph Neural\nNetworks (SoftHGNNs), which extend the methodology of hypergraph computation,\nto make it truly efficient and versatile in visual recognition tasks. Our\nframework introduces the concept of soft hyperedges, where each vertex is\nassociated with hyperedges via continuous participation weights rather than\nhard binary assignments. This dynamic and differentiable association is\nachieved by using the learnable hyperedge prototype. Through similarity\nmeasurements between token features and the prototype, the model generates\nsemantically rich soft hyperedges. SoftHGNN then aggregates messages over soft\nhyperedges to capture high-order semantics. To further enhance efficiency when\nscaling up the number of soft hyperedges, we incorporate a sparse hyperedge\nselection mechanism that activates only the top-k important hyperedges, along\nwith a load-balancing regularizer to ensure balanced hyperedge utilization.\nExperimental results across three tasks on five datasets demonstrate that\nSoftHGNN efficiently captures high-order associations in visual scenes,\nachieving significant performance improvements.\n", "link": "http://arxiv.org/abs/2505.15325v1", "date": "2025-05-21", "relevancy": 2.677, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5445}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5325}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SoftHGNN%3A%20Soft%20Hypergraph%20Neural%20Networks%20for%20General%20Visual%20Recognition&body=Title%3A%20SoftHGNN%3A%20Soft%20Hypergraph%20Neural%20Networks%20for%20General%20Visual%20Recognition%0AAuthor%3A%20Mengqi%20Lei%20and%20Yihong%20Wu%20and%20Siqi%20Li%20and%20Xinhu%20Zheng%20and%20Juan%20Wang%20and%20Yue%20Gao%20and%20Shaoyi%20Du%0AAbstract%3A%20%20%20Visual%20recognition%20relies%20on%20understanding%20both%20the%20semantics%20of%20image%20tokens%0Aand%20the%20complex%20interactions%20among%20them.%20Mainstream%20self-attention%20methods%2C%0Awhile%20effective%20at%20modeling%20global%20pair-wise%20relations%2C%20fail%20to%20capture%0Ahigh-order%20associations%20inherent%20in%20real-world%20scenes%20and%20often%20suffer%20from%0Aredundant%20computation.%20Hypergraphs%20extend%20conventional%20graphs%20by%20modeling%0Ahigh-order%20interactions%20and%20offer%20a%20promising%20framework%20for%20addressing%20these%0Alimitations.%20However%2C%20existing%20hypergraph%20neural%20networks%20typically%20rely%20on%0Astatic%20and%20hard%20hyperedge%20assignments%2C%20leading%20to%20excessive%20and%20redundant%0Ahyperedges%20with%20hard%20binary%20vertex%20memberships%20that%20overlook%20the%20continuity%20of%0Avisual%20semantics.%20To%20overcome%20these%20issues%2C%20we%20present%20Soft%20Hypergraph%20Neural%0ANetworks%20%28SoftHGNNs%29%2C%20which%20extend%20the%20methodology%20of%20hypergraph%20computation%2C%0Ato%20make%20it%20truly%20efficient%20and%20versatile%20in%20visual%20recognition%20tasks.%20Our%0Aframework%20introduces%20the%20concept%20of%20soft%20hyperedges%2C%20where%20each%20vertex%20is%0Aassociated%20with%20hyperedges%20via%20continuous%20participation%20weights%20rather%20than%0Ahard%20binary%20assignments.%20This%20dynamic%20and%20differentiable%20association%20is%0Aachieved%20by%20using%20the%20learnable%20hyperedge%20prototype.%20Through%20similarity%0Ameasurements%20between%20token%20features%20and%20the%20prototype%2C%20the%20model%20generates%0Asemantically%20rich%20soft%20hyperedges.%20SoftHGNN%20then%20aggregates%20messages%20over%20soft%0Ahyperedges%20to%20capture%20high-order%20semantics.%20To%20further%20enhance%20efficiency%20when%0Ascaling%20up%20the%20number%20of%20soft%20hyperedges%2C%20we%20incorporate%20a%20sparse%20hyperedge%0Aselection%20mechanism%20that%20activates%20only%20the%20top-k%20important%20hyperedges%2C%20along%0Awith%20a%20load-balancing%20regularizer%20to%20ensure%20balanced%20hyperedge%20utilization.%0AExperimental%20results%20across%20three%20tasks%20on%20five%20datasets%20demonstrate%20that%0ASoftHGNN%20efficiently%20captures%20high-order%20associations%20in%20visual%20scenes%2C%0Aachieving%20significant%20performance%20improvements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15325v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoftHGNN%253A%2520Soft%2520Hypergraph%2520Neural%2520Networks%2520for%2520General%2520Visual%2520Recognition%26entry.906535625%3DMengqi%2520Lei%2520and%2520Yihong%2520Wu%2520and%2520Siqi%2520Li%2520and%2520Xinhu%2520Zheng%2520and%2520Juan%2520Wang%2520and%2520Yue%2520Gao%2520and%2520Shaoyi%2520Du%26entry.1292438233%3D%2520%2520Visual%2520recognition%2520relies%2520on%2520understanding%2520both%2520the%2520semantics%2520of%2520image%2520tokens%250Aand%2520the%2520complex%2520interactions%2520among%2520them.%2520Mainstream%2520self-attention%2520methods%252C%250Awhile%2520effective%2520at%2520modeling%2520global%2520pair-wise%2520relations%252C%2520fail%2520to%2520capture%250Ahigh-order%2520associations%2520inherent%2520in%2520real-world%2520scenes%2520and%2520often%2520suffer%2520from%250Aredundant%2520computation.%2520Hypergraphs%2520extend%2520conventional%2520graphs%2520by%2520modeling%250Ahigh-order%2520interactions%2520and%2520offer%2520a%2520promising%2520framework%2520for%2520addressing%2520these%250Alimitations.%2520However%252C%2520existing%2520hypergraph%2520neural%2520networks%2520typically%2520rely%2520on%250Astatic%2520and%2520hard%2520hyperedge%2520assignments%252C%2520leading%2520to%2520excessive%2520and%2520redundant%250Ahyperedges%2520with%2520hard%2520binary%2520vertex%2520memberships%2520that%2520overlook%2520the%2520continuity%2520of%250Avisual%2520semantics.%2520To%2520overcome%2520these%2520issues%252C%2520we%2520present%2520Soft%2520Hypergraph%2520Neural%250ANetworks%2520%2528SoftHGNNs%2529%252C%2520which%2520extend%2520the%2520methodology%2520of%2520hypergraph%2520computation%252C%250Ato%2520make%2520it%2520truly%2520efficient%2520and%2520versatile%2520in%2520visual%2520recognition%2520tasks.%2520Our%250Aframework%2520introduces%2520the%2520concept%2520of%2520soft%2520hyperedges%252C%2520where%2520each%2520vertex%2520is%250Aassociated%2520with%2520hyperedges%2520via%2520continuous%2520participation%2520weights%2520rather%2520than%250Ahard%2520binary%2520assignments.%2520This%2520dynamic%2520and%2520differentiable%2520association%2520is%250Aachieved%2520by%2520using%2520the%2520learnable%2520hyperedge%2520prototype.%2520Through%2520similarity%250Ameasurements%2520between%2520token%2520features%2520and%2520the%2520prototype%252C%2520the%2520model%2520generates%250Asemantically%2520rich%2520soft%2520hyperedges.%2520SoftHGNN%2520then%2520aggregates%2520messages%2520over%2520soft%250Ahyperedges%2520to%2520capture%2520high-order%2520semantics.%2520To%2520further%2520enhance%2520efficiency%2520when%250Ascaling%2520up%2520the%2520number%2520of%2520soft%2520hyperedges%252C%2520we%2520incorporate%2520a%2520sparse%2520hyperedge%250Aselection%2520mechanism%2520that%2520activates%2520only%2520the%2520top-k%2520important%2520hyperedges%252C%2520along%250Awith%2520a%2520load-balancing%2520regularizer%2520to%2520ensure%2520balanced%2520hyperedge%2520utilization.%250AExperimental%2520results%2520across%2520three%2520tasks%2520on%2520five%2520datasets%2520demonstrate%2520that%250ASoftHGNN%2520efficiently%2520captures%2520high-order%2520associations%2520in%2520visual%2520scenes%252C%250Aachieving%2520significant%2520performance%2520improvements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15325v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SoftHGNN%3A%20Soft%20Hypergraph%20Neural%20Networks%20for%20General%20Visual%20Recognition&entry.906535625=Mengqi%20Lei%20and%20Yihong%20Wu%20and%20Siqi%20Li%20and%20Xinhu%20Zheng%20and%20Juan%20Wang%20and%20Yue%20Gao%20and%20Shaoyi%20Du&entry.1292438233=%20%20Visual%20recognition%20relies%20on%20understanding%20both%20the%20semantics%20of%20image%20tokens%0Aand%20the%20complex%20interactions%20among%20them.%20Mainstream%20self-attention%20methods%2C%0Awhile%20effective%20at%20modeling%20global%20pair-wise%20relations%2C%20fail%20to%20capture%0Ahigh-order%20associations%20inherent%20in%20real-world%20scenes%20and%20often%20suffer%20from%0Aredundant%20computation.%20Hypergraphs%20extend%20conventional%20graphs%20by%20modeling%0Ahigh-order%20interactions%20and%20offer%20a%20promising%20framework%20for%20addressing%20these%0Alimitations.%20However%2C%20existing%20hypergraph%20neural%20networks%20typically%20rely%20on%0Astatic%20and%20hard%20hyperedge%20assignments%2C%20leading%20to%20excessive%20and%20redundant%0Ahyperedges%20with%20hard%20binary%20vertex%20memberships%20that%20overlook%20the%20continuity%20of%0Avisual%20semantics.%20To%20overcome%20these%20issues%2C%20we%20present%20Soft%20Hypergraph%20Neural%0ANetworks%20%28SoftHGNNs%29%2C%20which%20extend%20the%20methodology%20of%20hypergraph%20computation%2C%0Ato%20make%20it%20truly%20efficient%20and%20versatile%20in%20visual%20recognition%20tasks.%20Our%0Aframework%20introduces%20the%20concept%20of%20soft%20hyperedges%2C%20where%20each%20vertex%20is%0Aassociated%20with%20hyperedges%20via%20continuous%20participation%20weights%20rather%20than%0Ahard%20binary%20assignments.%20This%20dynamic%20and%20differentiable%20association%20is%0Aachieved%20by%20using%20the%20learnable%20hyperedge%20prototype.%20Through%20similarity%0Ameasurements%20between%20token%20features%20and%20the%20prototype%2C%20the%20model%20generates%0Asemantically%20rich%20soft%20hyperedges.%20SoftHGNN%20then%20aggregates%20messages%20over%20soft%0Ahyperedges%20to%20capture%20high-order%20semantics.%20To%20further%20enhance%20efficiency%20when%0Ascaling%20up%20the%20number%20of%20soft%20hyperedges%2C%20we%20incorporate%20a%20sparse%20hyperedge%0Aselection%20mechanism%20that%20activates%20only%20the%20top-k%20important%20hyperedges%2C%20along%0Awith%20a%20load-balancing%20regularizer%20to%20ensure%20balanced%20hyperedge%20utilization.%0AExperimental%20results%20across%20three%20tasks%20on%20five%20datasets%20demonstrate%20that%0ASoftHGNN%20efficiently%20captures%20high-order%20associations%20in%20visual%20scenes%2C%0Aachieving%20significant%20performance%20improvements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15325v1&entry.124074799=Read"},
{"title": "Interspatial Attention for Efficient 4D Human Video Generation", "author": "Ruizhi Shao and Yinghao Xu and Yujun Shen and Ceyuan Yang and Yang Zheng and Changan Chen and Yebin Liu and Gordon Wetzstein", "abstract": "  Generating photorealistic videos of digital humans in a controllable manner\nis crucial for a plethora of applications. Existing approaches either build on\nmethods that employ template-based 3D representations or emerging video\ngeneration models but suffer from poor quality or limited consistency and\nidentity preservation when generating individual or multiple digital humans. In\nthis paper, we introduce a new interspatial attention (ISA) mechanism as a\nscalable building block for modern diffusion transformer (DiT)--based video\ngeneration models. ISA is a new type of cross attention that uses relative\npositional encodings tailored for the generation of human videos. Leveraging a\ncustom-developed video variation autoencoder, we train a latent ISA-based\ndiffusion model on a large corpus of video data. Our model achieves\nstate-of-the-art performance for 4D human video synthesis, demonstrating\nremarkable motion consistency and identity preservation while providing precise\ncontrol of the camera and body poses. Our code and model are publicly released\nat https://dsaurus.github.io/isa4d/.\n", "link": "http://arxiv.org/abs/2505.15800v1", "date": "2025-05-21", "relevancy": 2.6698, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.698}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6578}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6153}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interspatial%20Attention%20for%20Efficient%204D%20Human%20Video%20Generation&body=Title%3A%20Interspatial%20Attention%20for%20Efficient%204D%20Human%20Video%20Generation%0AAuthor%3A%20Ruizhi%20Shao%20and%20Yinghao%20Xu%20and%20Yujun%20Shen%20and%20Ceyuan%20Yang%20and%20Yang%20Zheng%20and%20Changan%20Chen%20and%20Yebin%20Liu%20and%20Gordon%20Wetzstein%0AAbstract%3A%20%20%20Generating%20photorealistic%20videos%20of%20digital%20humans%20in%20a%20controllable%20manner%0Ais%20crucial%20for%20a%20plethora%20of%20applications.%20Existing%20approaches%20either%20build%20on%0Amethods%20that%20employ%20template-based%203D%20representations%20or%20emerging%20video%0Ageneration%20models%20but%20suffer%20from%20poor%20quality%20or%20limited%20consistency%20and%0Aidentity%20preservation%20when%20generating%20individual%20or%20multiple%20digital%20humans.%20In%0Athis%20paper%2C%20we%20introduce%20a%20new%20interspatial%20attention%20%28ISA%29%20mechanism%20as%20a%0Ascalable%20building%20block%20for%20modern%20diffusion%20transformer%20%28DiT%29--based%20video%0Ageneration%20models.%20ISA%20is%20a%20new%20type%20of%20cross%20attention%20that%20uses%20relative%0Apositional%20encodings%20tailored%20for%20the%20generation%20of%20human%20videos.%20Leveraging%20a%0Acustom-developed%20video%20variation%20autoencoder%2C%20we%20train%20a%20latent%20ISA-based%0Adiffusion%20model%20on%20a%20large%20corpus%20of%20video%20data.%20Our%20model%20achieves%0Astate-of-the-art%20performance%20for%204D%20human%20video%20synthesis%2C%20demonstrating%0Aremarkable%20motion%20consistency%20and%20identity%20preservation%20while%20providing%20precise%0Acontrol%20of%20the%20camera%20and%20body%20poses.%20Our%20code%20and%20model%20are%20publicly%20released%0Aat%20https%3A//dsaurus.github.io/isa4d/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15800v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterspatial%2520Attention%2520for%2520Efficient%25204D%2520Human%2520Video%2520Generation%26entry.906535625%3DRuizhi%2520Shao%2520and%2520Yinghao%2520Xu%2520and%2520Yujun%2520Shen%2520and%2520Ceyuan%2520Yang%2520and%2520Yang%2520Zheng%2520and%2520Changan%2520Chen%2520and%2520Yebin%2520Liu%2520and%2520Gordon%2520Wetzstein%26entry.1292438233%3D%2520%2520Generating%2520photorealistic%2520videos%2520of%2520digital%2520humans%2520in%2520a%2520controllable%2520manner%250Ais%2520crucial%2520for%2520a%2520plethora%2520of%2520applications.%2520Existing%2520approaches%2520either%2520build%2520on%250Amethods%2520that%2520employ%2520template-based%25203D%2520representations%2520or%2520emerging%2520video%250Ageneration%2520models%2520but%2520suffer%2520from%2520poor%2520quality%2520or%2520limited%2520consistency%2520and%250Aidentity%2520preservation%2520when%2520generating%2520individual%2520or%2520multiple%2520digital%2520humans.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520a%2520new%2520interspatial%2520attention%2520%2528ISA%2529%2520mechanism%2520as%2520a%250Ascalable%2520building%2520block%2520for%2520modern%2520diffusion%2520transformer%2520%2528DiT%2529--based%2520video%250Ageneration%2520models.%2520ISA%2520is%2520a%2520new%2520type%2520of%2520cross%2520attention%2520that%2520uses%2520relative%250Apositional%2520encodings%2520tailored%2520for%2520the%2520generation%2520of%2520human%2520videos.%2520Leveraging%2520a%250Acustom-developed%2520video%2520variation%2520autoencoder%252C%2520we%2520train%2520a%2520latent%2520ISA-based%250Adiffusion%2520model%2520on%2520a%2520large%2520corpus%2520of%2520video%2520data.%2520Our%2520model%2520achieves%250Astate-of-the-art%2520performance%2520for%25204D%2520human%2520video%2520synthesis%252C%2520demonstrating%250Aremarkable%2520motion%2520consistency%2520and%2520identity%2520preservation%2520while%2520providing%2520precise%250Acontrol%2520of%2520the%2520camera%2520and%2520body%2520poses.%2520Our%2520code%2520and%2520model%2520are%2520publicly%2520released%250Aat%2520https%253A//dsaurus.github.io/isa4d/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15800v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interspatial%20Attention%20for%20Efficient%204D%20Human%20Video%20Generation&entry.906535625=Ruizhi%20Shao%20and%20Yinghao%20Xu%20and%20Yujun%20Shen%20and%20Ceyuan%20Yang%20and%20Yang%20Zheng%20and%20Changan%20Chen%20and%20Yebin%20Liu%20and%20Gordon%20Wetzstein&entry.1292438233=%20%20Generating%20photorealistic%20videos%20of%20digital%20humans%20in%20a%20controllable%20manner%0Ais%20crucial%20for%20a%20plethora%20of%20applications.%20Existing%20approaches%20either%20build%20on%0Amethods%20that%20employ%20template-based%203D%20representations%20or%20emerging%20video%0Ageneration%20models%20but%20suffer%20from%20poor%20quality%20or%20limited%20consistency%20and%0Aidentity%20preservation%20when%20generating%20individual%20or%20multiple%20digital%20humans.%20In%0Athis%20paper%2C%20we%20introduce%20a%20new%20interspatial%20attention%20%28ISA%29%20mechanism%20as%20a%0Ascalable%20building%20block%20for%20modern%20diffusion%20transformer%20%28DiT%29--based%20video%0Ageneration%20models.%20ISA%20is%20a%20new%20type%20of%20cross%20attention%20that%20uses%20relative%0Apositional%20encodings%20tailored%20for%20the%20generation%20of%20human%20videos.%20Leveraging%20a%0Acustom-developed%20video%20variation%20autoencoder%2C%20we%20train%20a%20latent%20ISA-based%0Adiffusion%20model%20on%20a%20large%20corpus%20of%20video%20data.%20Our%20model%20achieves%0Astate-of-the-art%20performance%20for%204D%20human%20video%20synthesis%2C%20demonstrating%0Aremarkable%20motion%20consistency%20and%20identity%20preservation%20while%20providing%20precise%0Acontrol%20of%20the%20camera%20and%20body%20poses.%20Our%20code%20and%20model%20are%20publicly%20released%0Aat%20https%3A//dsaurus.github.io/isa4d/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15800v1&entry.124074799=Read"},
{"title": "MV-MATH: Evaluating Multimodal Math Reasoning in Multi-Visual Contexts", "author": "Peijie Wang and Zhong-Zhi Li and Fei Yin and Xin Yang and Dekang Ran and Cheng-Lin Liu", "abstract": "  Multimodal Large Language Models (MLLMs) have shown promising capabilities in\nmathematical reasoning within visual contexts across various datasets. However,\nmost existing multimodal math benchmarks are limited to single-visual contexts,\nwhich diverges from the multi-visual scenarios commonly encountered in\nreal-world mathematical applications. To address this gap, we introduce\nMV-MATH: a meticulously curated dataset of 2,009 high-quality mathematical\nproblems. Each problem integrates multiple images interleaved with text,\nderived from authentic K-12 scenarios, and enriched with detailed annotations.\nMV-MATH includes multiple-choice, free-form, and multi-step questions, covering\n11 subject areas across 3 difficulty levels, and serves as a comprehensive and\nrigorous benchmark for assessing MLLMs' mathematical reasoning in multi-visual\ncontexts. Through extensive experimentation, we observe that MLLMs encounter\nsubstantial challenges in multi-visual math tasks, with a considerable\nperformance gap relative to human capabilities on MV-MATH. Furthermore, we\nanalyze the performance and error patterns of various models, providing\ninsights into MLLMs' mathematical reasoning capabilities within multi-visual\nsettings.\n", "link": "http://arxiv.org/abs/2502.20808v5", "date": "2025-05-21", "relevancy": 2.6692, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5355}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5355}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MV-MATH%3A%20Evaluating%20Multimodal%20Math%20Reasoning%20in%20Multi-Visual%20Contexts&body=Title%3A%20MV-MATH%3A%20Evaluating%20Multimodal%20Math%20Reasoning%20in%20Multi-Visual%20Contexts%0AAuthor%3A%20Peijie%20Wang%20and%20Zhong-Zhi%20Li%20and%20Fei%20Yin%20and%20Xin%20Yang%20and%20Dekang%20Ran%20and%20Cheng-Lin%20Liu%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20shown%20promising%20capabilities%20in%0Amathematical%20reasoning%20within%20visual%20contexts%20across%20various%20datasets.%20However%2C%0Amost%20existing%20multimodal%20math%20benchmarks%20are%20limited%20to%20single-visual%20contexts%2C%0Awhich%20diverges%20from%20the%20multi-visual%20scenarios%20commonly%20encountered%20in%0Areal-world%20mathematical%20applications.%20To%20address%20this%20gap%2C%20we%20introduce%0AMV-MATH%3A%20a%20meticulously%20curated%20dataset%20of%202%2C009%20high-quality%20mathematical%0Aproblems.%20Each%20problem%20integrates%20multiple%20images%20interleaved%20with%20text%2C%0Aderived%20from%20authentic%20K-12%20scenarios%2C%20and%20enriched%20with%20detailed%20annotations.%0AMV-MATH%20includes%20multiple-choice%2C%20free-form%2C%20and%20multi-step%20questions%2C%20covering%0A11%20subject%20areas%20across%203%20difficulty%20levels%2C%20and%20serves%20as%20a%20comprehensive%20and%0Arigorous%20benchmark%20for%20assessing%20MLLMs%27%20mathematical%20reasoning%20in%20multi-visual%0Acontexts.%20Through%20extensive%20experimentation%2C%20we%20observe%20that%20MLLMs%20encounter%0Asubstantial%20challenges%20in%20multi-visual%20math%20tasks%2C%20with%20a%20considerable%0Aperformance%20gap%20relative%20to%20human%20capabilities%20on%20MV-MATH.%20Furthermore%2C%20we%0Aanalyze%20the%20performance%20and%20error%20patterns%20of%20various%20models%2C%20providing%0Ainsights%20into%20MLLMs%27%20mathematical%20reasoning%20capabilities%20within%20multi-visual%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.20808v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMV-MATH%253A%2520Evaluating%2520Multimodal%2520Math%2520Reasoning%2520in%2520Multi-Visual%2520Contexts%26entry.906535625%3DPeijie%2520Wang%2520and%2520Zhong-Zhi%2520Li%2520and%2520Fei%2520Yin%2520and%2520Xin%2520Yang%2520and%2520Dekang%2520Ran%2520and%2520Cheng-Lin%2520Liu%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520shown%2520promising%2520capabilities%2520in%250Amathematical%2520reasoning%2520within%2520visual%2520contexts%2520across%2520various%2520datasets.%2520However%252C%250Amost%2520existing%2520multimodal%2520math%2520benchmarks%2520are%2520limited%2520to%2520single-visual%2520contexts%252C%250Awhich%2520diverges%2520from%2520the%2520multi-visual%2520scenarios%2520commonly%2520encountered%2520in%250Areal-world%2520mathematical%2520applications.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%250AMV-MATH%253A%2520a%2520meticulously%2520curated%2520dataset%2520of%25202%252C009%2520high-quality%2520mathematical%250Aproblems.%2520Each%2520problem%2520integrates%2520multiple%2520images%2520interleaved%2520with%2520text%252C%250Aderived%2520from%2520authentic%2520K-12%2520scenarios%252C%2520and%2520enriched%2520with%2520detailed%2520annotations.%250AMV-MATH%2520includes%2520multiple-choice%252C%2520free-form%252C%2520and%2520multi-step%2520questions%252C%2520covering%250A11%2520subject%2520areas%2520across%25203%2520difficulty%2520levels%252C%2520and%2520serves%2520as%2520a%2520comprehensive%2520and%250Arigorous%2520benchmark%2520for%2520assessing%2520MLLMs%2527%2520mathematical%2520reasoning%2520in%2520multi-visual%250Acontexts.%2520Through%2520extensive%2520experimentation%252C%2520we%2520observe%2520that%2520MLLMs%2520encounter%250Asubstantial%2520challenges%2520in%2520multi-visual%2520math%2520tasks%252C%2520with%2520a%2520considerable%250Aperformance%2520gap%2520relative%2520to%2520human%2520capabilities%2520on%2520MV-MATH.%2520Furthermore%252C%2520we%250Aanalyze%2520the%2520performance%2520and%2520error%2520patterns%2520of%2520various%2520models%252C%2520providing%250Ainsights%2520into%2520MLLMs%2527%2520mathematical%2520reasoning%2520capabilities%2520within%2520multi-visual%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.20808v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MV-MATH%3A%20Evaluating%20Multimodal%20Math%20Reasoning%20in%20Multi-Visual%20Contexts&entry.906535625=Peijie%20Wang%20and%20Zhong-Zhi%20Li%20and%20Fei%20Yin%20and%20Xin%20Yang%20and%20Dekang%20Ran%20and%20Cheng-Lin%20Liu&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20shown%20promising%20capabilities%20in%0Amathematical%20reasoning%20within%20visual%20contexts%20across%20various%20datasets.%20However%2C%0Amost%20existing%20multimodal%20math%20benchmarks%20are%20limited%20to%20single-visual%20contexts%2C%0Awhich%20diverges%20from%20the%20multi-visual%20scenarios%20commonly%20encountered%20in%0Areal-world%20mathematical%20applications.%20To%20address%20this%20gap%2C%20we%20introduce%0AMV-MATH%3A%20a%20meticulously%20curated%20dataset%20of%202%2C009%20high-quality%20mathematical%0Aproblems.%20Each%20problem%20integrates%20multiple%20images%20interleaved%20with%20text%2C%0Aderived%20from%20authentic%20K-12%20scenarios%2C%20and%20enriched%20with%20detailed%20annotations.%0AMV-MATH%20includes%20multiple-choice%2C%20free-form%2C%20and%20multi-step%20questions%2C%20covering%0A11%20subject%20areas%20across%203%20difficulty%20levels%2C%20and%20serves%20as%20a%20comprehensive%20and%0Arigorous%20benchmark%20for%20assessing%20MLLMs%27%20mathematical%20reasoning%20in%20multi-visual%0Acontexts.%20Through%20extensive%20experimentation%2C%20we%20observe%20that%20MLLMs%20encounter%0Asubstantial%20challenges%20in%20multi-visual%20math%20tasks%2C%20with%20a%20considerable%0Aperformance%20gap%20relative%20to%20human%20capabilities%20on%20MV-MATH.%20Furthermore%2C%20we%0Aanalyze%20the%20performance%20and%20error%20patterns%20of%20various%20models%2C%20providing%0Ainsights%20into%20MLLMs%27%20mathematical%20reasoning%20capabilities%20within%20multi-visual%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.20808v5&entry.124074799=Read"},
{"title": "seg_3D_by_PC2D: Multi-View Projection for Domain Generalization and\n  Adaptation in 3D Semantic Segmentation", "author": "Andrew Caunes and Thierry Chateau and Vincent Fremont", "abstract": "  3D semantic segmentation plays a pivotal role in autonomous driving and road\ninfrastructure analysis, yet state-of-the-art 3D models are prone to severe\ndomain shift when deployed across different datasets. We propose a novel\nmulti-view projection framework that excels in both domain generalization (DG)\nand unsupervised domain adaptation (UDA). Our approach first aligns Lidar scans\ninto coherent 3D scenes and renders them from multiple virtual camera poses to\ncreate a large-scale synthetic 2D dataset (PC2D). We then use it to train a 2D\nsegmentation model in-domain. During inference, the model processes hundreds of\nviews per scene; the resulting logits are back-projected to 3D with an\nocclusion-aware voting scheme to generate final point-wise labels. Our\nframework is modular and enables extensive exploration of key design\nparameters, such as view generation optimization (VGO), visualization modality\noptimization (MODO), and 2D model choice. We evaluate on the nuScenes and\nSemanticKITTI datasets under both the DG and UDA settings. We achieve\nstate-of-the-art results in UDA and close to state-of-the-art in DG, with\nparticularly large gains on large, static classes. Our code and dataset\ngeneration tools will be publicly available at\nhttps://github.com/andrewcaunes/ia4markings\n", "link": "http://arxiv.org/abs/2505.15545v1", "date": "2025-05-21", "relevancy": 2.6141, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.657}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.657}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20seg_3D_by_PC2D%3A%20Multi-View%20Projection%20for%20Domain%20Generalization%20and%0A%20%20Adaptation%20in%203D%20Semantic%20Segmentation&body=Title%3A%20seg_3D_by_PC2D%3A%20Multi-View%20Projection%20for%20Domain%20Generalization%20and%0A%20%20Adaptation%20in%203D%20Semantic%20Segmentation%0AAuthor%3A%20Andrew%20Caunes%20and%20Thierry%20Chateau%20and%20Vincent%20Fremont%0AAbstract%3A%20%20%203D%20semantic%20segmentation%20plays%20a%20pivotal%20role%20in%20autonomous%20driving%20and%20road%0Ainfrastructure%20analysis%2C%20yet%20state-of-the-art%203D%20models%20are%20prone%20to%20severe%0Adomain%20shift%20when%20deployed%20across%20different%20datasets.%20We%20propose%20a%20novel%0Amulti-view%20projection%20framework%20that%20excels%20in%20both%20domain%20generalization%20%28DG%29%0Aand%20unsupervised%20domain%20adaptation%20%28UDA%29.%20Our%20approach%20first%20aligns%20Lidar%20scans%0Ainto%20coherent%203D%20scenes%20and%20renders%20them%20from%20multiple%20virtual%20camera%20poses%20to%0Acreate%20a%20large-scale%20synthetic%202D%20dataset%20%28PC2D%29.%20We%20then%20use%20it%20to%20train%20a%202D%0Asegmentation%20model%20in-domain.%20During%20inference%2C%20the%20model%20processes%20hundreds%20of%0Aviews%20per%20scene%3B%20the%20resulting%20logits%20are%20back-projected%20to%203D%20with%20an%0Aocclusion-aware%20voting%20scheme%20to%20generate%20final%20point-wise%20labels.%20Our%0Aframework%20is%20modular%20and%20enables%20extensive%20exploration%20of%20key%20design%0Aparameters%2C%20such%20as%20view%20generation%20optimization%20%28VGO%29%2C%20visualization%20modality%0Aoptimization%20%28MODO%29%2C%20and%202D%20model%20choice.%20We%20evaluate%20on%20the%20nuScenes%20and%0ASemanticKITTI%20datasets%20under%20both%20the%20DG%20and%20UDA%20settings.%20We%20achieve%0Astate-of-the-art%20results%20in%20UDA%20and%20close%20to%20state-of-the-art%20in%20DG%2C%20with%0Aparticularly%20large%20gains%20on%20large%2C%20static%20classes.%20Our%20code%20and%20dataset%0Ageneration%20tools%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/andrewcaunes/ia4markings%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15545v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dseg_3D_by_PC2D%253A%2520Multi-View%2520Projection%2520for%2520Domain%2520Generalization%2520and%250A%2520%2520Adaptation%2520in%25203D%2520Semantic%2520Segmentation%26entry.906535625%3DAndrew%2520Caunes%2520and%2520Thierry%2520Chateau%2520and%2520Vincent%2520Fremont%26entry.1292438233%3D%2520%25203D%2520semantic%2520segmentation%2520plays%2520a%2520pivotal%2520role%2520in%2520autonomous%2520driving%2520and%2520road%250Ainfrastructure%2520analysis%252C%2520yet%2520state-of-the-art%25203D%2520models%2520are%2520prone%2520to%2520severe%250Adomain%2520shift%2520when%2520deployed%2520across%2520different%2520datasets.%2520We%2520propose%2520a%2520novel%250Amulti-view%2520projection%2520framework%2520that%2520excels%2520in%2520both%2520domain%2520generalization%2520%2528DG%2529%250Aand%2520unsupervised%2520domain%2520adaptation%2520%2528UDA%2529.%2520Our%2520approach%2520first%2520aligns%2520Lidar%2520scans%250Ainto%2520coherent%25203D%2520scenes%2520and%2520renders%2520them%2520from%2520multiple%2520virtual%2520camera%2520poses%2520to%250Acreate%2520a%2520large-scale%2520synthetic%25202D%2520dataset%2520%2528PC2D%2529.%2520We%2520then%2520use%2520it%2520to%2520train%2520a%25202D%250Asegmentation%2520model%2520in-domain.%2520During%2520inference%252C%2520the%2520model%2520processes%2520hundreds%2520of%250Aviews%2520per%2520scene%253B%2520the%2520resulting%2520logits%2520are%2520back-projected%2520to%25203D%2520with%2520an%250Aocclusion-aware%2520voting%2520scheme%2520to%2520generate%2520final%2520point-wise%2520labels.%2520Our%250Aframework%2520is%2520modular%2520and%2520enables%2520extensive%2520exploration%2520of%2520key%2520design%250Aparameters%252C%2520such%2520as%2520view%2520generation%2520optimization%2520%2528VGO%2529%252C%2520visualization%2520modality%250Aoptimization%2520%2528MODO%2529%252C%2520and%25202D%2520model%2520choice.%2520We%2520evaluate%2520on%2520the%2520nuScenes%2520and%250ASemanticKITTI%2520datasets%2520under%2520both%2520the%2520DG%2520and%2520UDA%2520settings.%2520We%2520achieve%250Astate-of-the-art%2520results%2520in%2520UDA%2520and%2520close%2520to%2520state-of-the-art%2520in%2520DG%252C%2520with%250Aparticularly%2520large%2520gains%2520on%2520large%252C%2520static%2520classes.%2520Our%2520code%2520and%2520dataset%250Ageneration%2520tools%2520will%2520be%2520publicly%2520available%2520at%250Ahttps%253A//github.com/andrewcaunes/ia4markings%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15545v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=seg_3D_by_PC2D%3A%20Multi-View%20Projection%20for%20Domain%20Generalization%20and%0A%20%20Adaptation%20in%203D%20Semantic%20Segmentation&entry.906535625=Andrew%20Caunes%20and%20Thierry%20Chateau%20and%20Vincent%20Fremont&entry.1292438233=%20%203D%20semantic%20segmentation%20plays%20a%20pivotal%20role%20in%20autonomous%20driving%20and%20road%0Ainfrastructure%20analysis%2C%20yet%20state-of-the-art%203D%20models%20are%20prone%20to%20severe%0Adomain%20shift%20when%20deployed%20across%20different%20datasets.%20We%20propose%20a%20novel%0Amulti-view%20projection%20framework%20that%20excels%20in%20both%20domain%20generalization%20%28DG%29%0Aand%20unsupervised%20domain%20adaptation%20%28UDA%29.%20Our%20approach%20first%20aligns%20Lidar%20scans%0Ainto%20coherent%203D%20scenes%20and%20renders%20them%20from%20multiple%20virtual%20camera%20poses%20to%0Acreate%20a%20large-scale%20synthetic%202D%20dataset%20%28PC2D%29.%20We%20then%20use%20it%20to%20train%20a%202D%0Asegmentation%20model%20in-domain.%20During%20inference%2C%20the%20model%20processes%20hundreds%20of%0Aviews%20per%20scene%3B%20the%20resulting%20logits%20are%20back-projected%20to%203D%20with%20an%0Aocclusion-aware%20voting%20scheme%20to%20generate%20final%20point-wise%20labels.%20Our%0Aframework%20is%20modular%20and%20enables%20extensive%20exploration%20of%20key%20design%0Aparameters%2C%20such%20as%20view%20generation%20optimization%20%28VGO%29%2C%20visualization%20modality%0Aoptimization%20%28MODO%29%2C%20and%202D%20model%20choice.%20We%20evaluate%20on%20the%20nuScenes%20and%0ASemanticKITTI%20datasets%20under%20both%20the%20DG%20and%20UDA%20settings.%20We%20achieve%0Astate-of-the-art%20results%20in%20UDA%20and%20close%20to%20state-of-the-art%20in%20DG%2C%20with%0Aparticularly%20large%20gains%20on%20large%2C%20static%20classes.%20Our%20code%20and%20dataset%0Ageneration%20tools%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/andrewcaunes/ia4markings%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15545v1&entry.124074799=Read"},
{"title": "Constructing a 3D Town from a Single Image", "author": "Kaizhi Zheng and Ruijian Zhang and Jing Gu and Jie Yang and Xin Eric Wang", "abstract": "  Acquiring detailed 3D scenes typically demands costly equipment, multi-view\ndata, or labor-intensive modeling. Therefore, a lightweight alternative,\ngenerating complex 3D scenes from a single top-down image, plays an essential\nrole in real-world applications. While recent 3D generative models have\nachieved remarkable results at the object level, their extension to full-scene\ngeneration often leads to inconsistent geometry, layout hallucinations, and\nlow-quality meshes. In this work, we introduce 3DTown, a training-free\nframework designed to synthesize realistic and coherent 3D scenes from a single\ntop-down view. Our method is grounded in two principles: region-based\ngeneration to improve image-to-3D alignment and resolution, and spatial-aware\n3D inpainting to ensure global scene coherence and high-quality geometry\ngeneration. Specifically, we decompose the input image into overlapping regions\nand generate each using a pretrained 3D object generator, followed by a masked\nrectified flow inpainting process that fills in missing geometry while\nmaintaining structural continuity. This modular design allows us to overcome\nresolution bottlenecks and preserve spatial structure without requiring 3D\nsupervision or fine-tuning. Extensive experiments across diverse scenes show\nthat 3DTown outperforms state-of-the-art baselines, including Trellis,\nHunyuan3D-2, and TripoSG, in terms of geometry quality, spatial coherence, and\ntexture fidelity. Our results demonstrate that high-quality 3D town generation\nis achievable from a single image using a principled, training-free approach.\n", "link": "http://arxiv.org/abs/2505.15765v1", "date": "2025-05-21", "relevancy": 2.6102, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6665}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6665}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Constructing%20a%203D%20Town%20from%20a%20Single%20Image&body=Title%3A%20Constructing%20a%203D%20Town%20from%20a%20Single%20Image%0AAuthor%3A%20Kaizhi%20Zheng%20and%20Ruijian%20Zhang%20and%20Jing%20Gu%20and%20Jie%20Yang%20and%20Xin%20Eric%20Wang%0AAbstract%3A%20%20%20Acquiring%20detailed%203D%20scenes%20typically%20demands%20costly%20equipment%2C%20multi-view%0Adata%2C%20or%20labor-intensive%20modeling.%20Therefore%2C%20a%20lightweight%20alternative%2C%0Agenerating%20complex%203D%20scenes%20from%20a%20single%20top-down%20image%2C%20plays%20an%20essential%0Arole%20in%20real-world%20applications.%20While%20recent%203D%20generative%20models%20have%0Aachieved%20remarkable%20results%20at%20the%20object%20level%2C%20their%20extension%20to%20full-scene%0Ageneration%20often%20leads%20to%20inconsistent%20geometry%2C%20layout%20hallucinations%2C%20and%0Alow-quality%20meshes.%20In%20this%20work%2C%20we%20introduce%203DTown%2C%20a%20training-free%0Aframework%20designed%20to%20synthesize%20realistic%20and%20coherent%203D%20scenes%20from%20a%20single%0Atop-down%20view.%20Our%20method%20is%20grounded%20in%20two%20principles%3A%20region-based%0Ageneration%20to%20improve%20image-to-3D%20alignment%20and%20resolution%2C%20and%20spatial-aware%0A3D%20inpainting%20to%20ensure%20global%20scene%20coherence%20and%20high-quality%20geometry%0Ageneration.%20Specifically%2C%20we%20decompose%20the%20input%20image%20into%20overlapping%20regions%0Aand%20generate%20each%20using%20a%20pretrained%203D%20object%20generator%2C%20followed%20by%20a%20masked%0Arectified%20flow%20inpainting%20process%20that%20fills%20in%20missing%20geometry%20while%0Amaintaining%20structural%20continuity.%20This%20modular%20design%20allows%20us%20to%20overcome%0Aresolution%20bottlenecks%20and%20preserve%20spatial%20structure%20without%20requiring%203D%0Asupervision%20or%20fine-tuning.%20Extensive%20experiments%20across%20diverse%20scenes%20show%0Athat%203DTown%20outperforms%20state-of-the-art%20baselines%2C%20including%20Trellis%2C%0AHunyuan3D-2%2C%20and%20TripoSG%2C%20in%20terms%20of%20geometry%20quality%2C%20spatial%20coherence%2C%20and%0Atexture%20fidelity.%20Our%20results%20demonstrate%20that%20high-quality%203D%20town%20generation%0Ais%20achievable%20from%20a%20single%20image%20using%20a%20principled%2C%20training-free%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15765v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConstructing%2520a%25203D%2520Town%2520from%2520a%2520Single%2520Image%26entry.906535625%3DKaizhi%2520Zheng%2520and%2520Ruijian%2520Zhang%2520and%2520Jing%2520Gu%2520and%2520Jie%2520Yang%2520and%2520Xin%2520Eric%2520Wang%26entry.1292438233%3D%2520%2520Acquiring%2520detailed%25203D%2520scenes%2520typically%2520demands%2520costly%2520equipment%252C%2520multi-view%250Adata%252C%2520or%2520labor-intensive%2520modeling.%2520Therefore%252C%2520a%2520lightweight%2520alternative%252C%250Agenerating%2520complex%25203D%2520scenes%2520from%2520a%2520single%2520top-down%2520image%252C%2520plays%2520an%2520essential%250Arole%2520in%2520real-world%2520applications.%2520While%2520recent%25203D%2520generative%2520models%2520have%250Aachieved%2520remarkable%2520results%2520at%2520the%2520object%2520level%252C%2520their%2520extension%2520to%2520full-scene%250Ageneration%2520often%2520leads%2520to%2520inconsistent%2520geometry%252C%2520layout%2520hallucinations%252C%2520and%250Alow-quality%2520meshes.%2520In%2520this%2520work%252C%2520we%2520introduce%25203DTown%252C%2520a%2520training-free%250Aframework%2520designed%2520to%2520synthesize%2520realistic%2520and%2520coherent%25203D%2520scenes%2520from%2520a%2520single%250Atop-down%2520view.%2520Our%2520method%2520is%2520grounded%2520in%2520two%2520principles%253A%2520region-based%250Ageneration%2520to%2520improve%2520image-to-3D%2520alignment%2520and%2520resolution%252C%2520and%2520spatial-aware%250A3D%2520inpainting%2520to%2520ensure%2520global%2520scene%2520coherence%2520and%2520high-quality%2520geometry%250Ageneration.%2520Specifically%252C%2520we%2520decompose%2520the%2520input%2520image%2520into%2520overlapping%2520regions%250Aand%2520generate%2520each%2520using%2520a%2520pretrained%25203D%2520object%2520generator%252C%2520followed%2520by%2520a%2520masked%250Arectified%2520flow%2520inpainting%2520process%2520that%2520fills%2520in%2520missing%2520geometry%2520while%250Amaintaining%2520structural%2520continuity.%2520This%2520modular%2520design%2520allows%2520us%2520to%2520overcome%250Aresolution%2520bottlenecks%2520and%2520preserve%2520spatial%2520structure%2520without%2520requiring%25203D%250Asupervision%2520or%2520fine-tuning.%2520Extensive%2520experiments%2520across%2520diverse%2520scenes%2520show%250Athat%25203DTown%2520outperforms%2520state-of-the-art%2520baselines%252C%2520including%2520Trellis%252C%250AHunyuan3D-2%252C%2520and%2520TripoSG%252C%2520in%2520terms%2520of%2520geometry%2520quality%252C%2520spatial%2520coherence%252C%2520and%250Atexture%2520fidelity.%2520Our%2520results%2520demonstrate%2520that%2520high-quality%25203D%2520town%2520generation%250Ais%2520achievable%2520from%2520a%2520single%2520image%2520using%2520a%2520principled%252C%2520training-free%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15765v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constructing%20a%203D%20Town%20from%20a%20Single%20Image&entry.906535625=Kaizhi%20Zheng%20and%20Ruijian%20Zhang%20and%20Jing%20Gu%20and%20Jie%20Yang%20and%20Xin%20Eric%20Wang&entry.1292438233=%20%20Acquiring%20detailed%203D%20scenes%20typically%20demands%20costly%20equipment%2C%20multi-view%0Adata%2C%20or%20labor-intensive%20modeling.%20Therefore%2C%20a%20lightweight%20alternative%2C%0Agenerating%20complex%203D%20scenes%20from%20a%20single%20top-down%20image%2C%20plays%20an%20essential%0Arole%20in%20real-world%20applications.%20While%20recent%203D%20generative%20models%20have%0Aachieved%20remarkable%20results%20at%20the%20object%20level%2C%20their%20extension%20to%20full-scene%0Ageneration%20often%20leads%20to%20inconsistent%20geometry%2C%20layout%20hallucinations%2C%20and%0Alow-quality%20meshes.%20In%20this%20work%2C%20we%20introduce%203DTown%2C%20a%20training-free%0Aframework%20designed%20to%20synthesize%20realistic%20and%20coherent%203D%20scenes%20from%20a%20single%0Atop-down%20view.%20Our%20method%20is%20grounded%20in%20two%20principles%3A%20region-based%0Ageneration%20to%20improve%20image-to-3D%20alignment%20and%20resolution%2C%20and%20spatial-aware%0A3D%20inpainting%20to%20ensure%20global%20scene%20coherence%20and%20high-quality%20geometry%0Ageneration.%20Specifically%2C%20we%20decompose%20the%20input%20image%20into%20overlapping%20regions%0Aand%20generate%20each%20using%20a%20pretrained%203D%20object%20generator%2C%20followed%20by%20a%20masked%0Arectified%20flow%20inpainting%20process%20that%20fills%20in%20missing%20geometry%20while%0Amaintaining%20structural%20continuity.%20This%20modular%20design%20allows%20us%20to%20overcome%0Aresolution%20bottlenecks%20and%20preserve%20spatial%20structure%20without%20requiring%203D%0Asupervision%20or%20fine-tuning.%20Extensive%20experiments%20across%20diverse%20scenes%20show%0Athat%203DTown%20outperforms%20state-of-the-art%20baselines%2C%20including%20Trellis%2C%0AHunyuan3D-2%2C%20and%20TripoSG%2C%20in%20terms%20of%20geometry%20quality%2C%20spatial%20coherence%2C%20and%0Atexture%20fidelity.%20Our%20results%20demonstrate%20that%20high-quality%203D%20town%20generation%0Ais%20achievable%20from%20a%20single%20image%20using%20a%20principled%2C%20training-free%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15765v1&entry.124074799=Read"},
{"title": "Synthetic Enclosed Echoes: A New Dataset to Mitigate the Gap Between\n  Simulated and Real-World Sonar Data", "author": "Guilherme de Oliveira and Matheus M. dos Santos and Paulo L. J. Drews-Jr", "abstract": "  This paper introduces Synthetic Enclosed Echoes (SEE), a novel dataset\ndesigned to enhance robot perception and 3D reconstruction capabilities in\nunderwater environments. SEE comprises high-fidelity synthetic sonar data,\ncomplemented by a smaller subset of real-world sonar data. To facilitate\nflexible data acquisition, a simulated environment has been developed, enabling\nthe generation of additional data through modifications such as the inclusion\nof new structures or imaging sonar configurations. This hybrid approach\nleverages the advantages of synthetic data, including readily available ground\ntruth and the ability to generate diverse datasets, while bridging the\nsimulation-to-reality gap with real-world data acquired in a similar\nenvironment. The SEE dataset comprehensively evaluates acoustic data-based\nmethods, including mathematics-based sonar approaches and deep learning\nalgorithms. These techniques were employed to validate the dataset, confirming\nits suitability for underwater 3D reconstruction. Furthermore, this paper\nproposes a novel modification to a state-of-the-art algorithm, demonstrating\nimproved performance compared to existing methods. The SEE dataset enables the\nevaluation of acoustic data-based methods in realistic scenarios, thereby\nimproving their feasibility for real-world underwater applications.\n", "link": "http://arxiv.org/abs/2505.15465v1", "date": "2025-05-21", "relevancy": 2.601, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5217}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5217}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthetic%20Enclosed%20Echoes%3A%20A%20New%20Dataset%20to%20Mitigate%20the%20Gap%20Between%0A%20%20Simulated%20and%20Real-World%20Sonar%20Data&body=Title%3A%20Synthetic%20Enclosed%20Echoes%3A%20A%20New%20Dataset%20to%20Mitigate%20the%20Gap%20Between%0A%20%20Simulated%20and%20Real-World%20Sonar%20Data%0AAuthor%3A%20Guilherme%20de%20Oliveira%20and%20Matheus%20M.%20dos%20Santos%20and%20Paulo%20L.%20J.%20Drews-Jr%0AAbstract%3A%20%20%20This%20paper%20introduces%20Synthetic%20Enclosed%20Echoes%20%28SEE%29%2C%20a%20novel%20dataset%0Adesigned%20to%20enhance%20robot%20perception%20and%203D%20reconstruction%20capabilities%20in%0Aunderwater%20environments.%20SEE%20comprises%20high-fidelity%20synthetic%20sonar%20data%2C%0Acomplemented%20by%20a%20smaller%20subset%20of%20real-world%20sonar%20data.%20To%20facilitate%0Aflexible%20data%20acquisition%2C%20a%20simulated%20environment%20has%20been%20developed%2C%20enabling%0Athe%20generation%20of%20additional%20data%20through%20modifications%20such%20as%20the%20inclusion%0Aof%20new%20structures%20or%20imaging%20sonar%20configurations.%20This%20hybrid%20approach%0Aleverages%20the%20advantages%20of%20synthetic%20data%2C%20including%20readily%20available%20ground%0Atruth%20and%20the%20ability%20to%20generate%20diverse%20datasets%2C%20while%20bridging%20the%0Asimulation-to-reality%20gap%20with%20real-world%20data%20acquired%20in%20a%20similar%0Aenvironment.%20The%20SEE%20dataset%20comprehensively%20evaluates%20acoustic%20data-based%0Amethods%2C%20including%20mathematics-based%20sonar%20approaches%20and%20deep%20learning%0Aalgorithms.%20These%20techniques%20were%20employed%20to%20validate%20the%20dataset%2C%20confirming%0Aits%20suitability%20for%20underwater%203D%20reconstruction.%20Furthermore%2C%20this%20paper%0Aproposes%20a%20novel%20modification%20to%20a%20state-of-the-art%20algorithm%2C%20demonstrating%0Aimproved%20performance%20compared%20to%20existing%20methods.%20The%20SEE%20dataset%20enables%20the%0Aevaluation%20of%20acoustic%20data-based%20methods%20in%20realistic%20scenarios%2C%20thereby%0Aimproving%20their%20feasibility%20for%20real-world%20underwater%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15465v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthetic%2520Enclosed%2520Echoes%253A%2520A%2520New%2520Dataset%2520to%2520Mitigate%2520the%2520Gap%2520Between%250A%2520%2520Simulated%2520and%2520Real-World%2520Sonar%2520Data%26entry.906535625%3DGuilherme%2520de%2520Oliveira%2520and%2520Matheus%2520M.%2520dos%2520Santos%2520and%2520Paulo%2520L.%2520J.%2520Drews-Jr%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520Synthetic%2520Enclosed%2520Echoes%2520%2528SEE%2529%252C%2520a%2520novel%2520dataset%250Adesigned%2520to%2520enhance%2520robot%2520perception%2520and%25203D%2520reconstruction%2520capabilities%2520in%250Aunderwater%2520environments.%2520SEE%2520comprises%2520high-fidelity%2520synthetic%2520sonar%2520data%252C%250Acomplemented%2520by%2520a%2520smaller%2520subset%2520of%2520real-world%2520sonar%2520data.%2520To%2520facilitate%250Aflexible%2520data%2520acquisition%252C%2520a%2520simulated%2520environment%2520has%2520been%2520developed%252C%2520enabling%250Athe%2520generation%2520of%2520additional%2520data%2520through%2520modifications%2520such%2520as%2520the%2520inclusion%250Aof%2520new%2520structures%2520or%2520imaging%2520sonar%2520configurations.%2520This%2520hybrid%2520approach%250Aleverages%2520the%2520advantages%2520of%2520synthetic%2520data%252C%2520including%2520readily%2520available%2520ground%250Atruth%2520and%2520the%2520ability%2520to%2520generate%2520diverse%2520datasets%252C%2520while%2520bridging%2520the%250Asimulation-to-reality%2520gap%2520with%2520real-world%2520data%2520acquired%2520in%2520a%2520similar%250Aenvironment.%2520The%2520SEE%2520dataset%2520comprehensively%2520evaluates%2520acoustic%2520data-based%250Amethods%252C%2520including%2520mathematics-based%2520sonar%2520approaches%2520and%2520deep%2520learning%250Aalgorithms.%2520These%2520techniques%2520were%2520employed%2520to%2520validate%2520the%2520dataset%252C%2520confirming%250Aits%2520suitability%2520for%2520underwater%25203D%2520reconstruction.%2520Furthermore%252C%2520this%2520paper%250Aproposes%2520a%2520novel%2520modification%2520to%2520a%2520state-of-the-art%2520algorithm%252C%2520demonstrating%250Aimproved%2520performance%2520compared%2520to%2520existing%2520methods.%2520The%2520SEE%2520dataset%2520enables%2520the%250Aevaluation%2520of%2520acoustic%2520data-based%2520methods%2520in%2520realistic%2520scenarios%252C%2520thereby%250Aimproving%2520their%2520feasibility%2520for%2520real-world%2520underwater%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15465v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthetic%20Enclosed%20Echoes%3A%20A%20New%20Dataset%20to%20Mitigate%20the%20Gap%20Between%0A%20%20Simulated%20and%20Real-World%20Sonar%20Data&entry.906535625=Guilherme%20de%20Oliveira%20and%20Matheus%20M.%20dos%20Santos%20and%20Paulo%20L.%20J.%20Drews-Jr&entry.1292438233=%20%20This%20paper%20introduces%20Synthetic%20Enclosed%20Echoes%20%28SEE%29%2C%20a%20novel%20dataset%0Adesigned%20to%20enhance%20robot%20perception%20and%203D%20reconstruction%20capabilities%20in%0Aunderwater%20environments.%20SEE%20comprises%20high-fidelity%20synthetic%20sonar%20data%2C%0Acomplemented%20by%20a%20smaller%20subset%20of%20real-world%20sonar%20data.%20To%20facilitate%0Aflexible%20data%20acquisition%2C%20a%20simulated%20environment%20has%20been%20developed%2C%20enabling%0Athe%20generation%20of%20additional%20data%20through%20modifications%20such%20as%20the%20inclusion%0Aof%20new%20structures%20or%20imaging%20sonar%20configurations.%20This%20hybrid%20approach%0Aleverages%20the%20advantages%20of%20synthetic%20data%2C%20including%20readily%20available%20ground%0Atruth%20and%20the%20ability%20to%20generate%20diverse%20datasets%2C%20while%20bridging%20the%0Asimulation-to-reality%20gap%20with%20real-world%20data%20acquired%20in%20a%20similar%0Aenvironment.%20The%20SEE%20dataset%20comprehensively%20evaluates%20acoustic%20data-based%0Amethods%2C%20including%20mathematics-based%20sonar%20approaches%20and%20deep%20learning%0Aalgorithms.%20These%20techniques%20were%20employed%20to%20validate%20the%20dataset%2C%20confirming%0Aits%20suitability%20for%20underwater%203D%20reconstruction.%20Furthermore%2C%20this%20paper%0Aproposes%20a%20novel%20modification%20to%20a%20state-of-the-art%20algorithm%2C%20demonstrating%0Aimproved%20performance%20compared%20to%20existing%20methods.%20The%20SEE%20dataset%20enables%20the%0Aevaluation%20of%20acoustic%20data-based%20methods%20in%20realistic%20scenarios%2C%20thereby%0Aimproving%20their%20feasibility%20for%20real-world%20underwater%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15465v1&entry.124074799=Read"},
{"title": "FRN: Fractal-Based Recursive Spectral Reconstruction Network", "author": "Ge Meng and Zhongnan Cai and Ruizhe Chen and Jingyan Tu and Yingying Wang and Yue Huang and Xinghao Ding", "abstract": "  Generating hyperspectral images (HSIs) from RGB images through spectral\nreconstruction can significantly reduce the cost of HSI acquisition. In this\npaper, we propose a Fractal-Based Recursive Spectral Reconstruction Network\n(FRN), which differs from existing paradigms that attempt to directly integrate\nthe full-spectrum information from the R, G, and B channels in a one-shot\nmanner. Instead, it treats spectral reconstruction as a progressive process,\npredicting from broad to narrow bands or employing a coarse-to-fine approach\nfor predicting the next wavelength. Inspired by fractals in mathematics, FRN\nestablishes a novel spectral reconstruction paradigm by recursively invoking an\natomic reconstruction module. In each invocation, only the spectral information\nfrom neighboring bands is used to provide clues for the generation of the image\nat the next wavelength, which follows the low-rank property of spectral data.\nMoreover, we design a band-aware state space model that employs a\npixel-differentiated scanning strategy at different stages of the generation\nprocess, further suppressing interference from low-correlation regions caused\nby reflectance differences. Through extensive experimentation across different\ndatasets, FRN achieves superior reconstruction performance compared to\nstate-of-the-art methods in both quantitative and qualitative evaluations.\n", "link": "http://arxiv.org/abs/2505.15439v1", "date": "2025-05-21", "relevancy": 2.5892, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.564}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.498}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FRN%3A%20Fractal-Based%20Recursive%20Spectral%20Reconstruction%20Network&body=Title%3A%20FRN%3A%20Fractal-Based%20Recursive%20Spectral%20Reconstruction%20Network%0AAuthor%3A%20Ge%20Meng%20and%20Zhongnan%20Cai%20and%20Ruizhe%20Chen%20and%20Jingyan%20Tu%20and%20Yingying%20Wang%20and%20Yue%20Huang%20and%20Xinghao%20Ding%0AAbstract%3A%20%20%20Generating%20hyperspectral%20images%20%28HSIs%29%20from%20RGB%20images%20through%20spectral%0Areconstruction%20can%20significantly%20reduce%20the%20cost%20of%20HSI%20acquisition.%20In%20this%0Apaper%2C%20we%20propose%20a%20Fractal-Based%20Recursive%20Spectral%20Reconstruction%20Network%0A%28FRN%29%2C%20which%20differs%20from%20existing%20paradigms%20that%20attempt%20to%20directly%20integrate%0Athe%20full-spectrum%20information%20from%20the%20R%2C%20G%2C%20and%20B%20channels%20in%20a%20one-shot%0Amanner.%20Instead%2C%20it%20treats%20spectral%20reconstruction%20as%20a%20progressive%20process%2C%0Apredicting%20from%20broad%20to%20narrow%20bands%20or%20employing%20a%20coarse-to-fine%20approach%0Afor%20predicting%20the%20next%20wavelength.%20Inspired%20by%20fractals%20in%20mathematics%2C%20FRN%0Aestablishes%20a%20novel%20spectral%20reconstruction%20paradigm%20by%20recursively%20invoking%20an%0Aatomic%20reconstruction%20module.%20In%20each%20invocation%2C%20only%20the%20spectral%20information%0Afrom%20neighboring%20bands%20is%20used%20to%20provide%20clues%20for%20the%20generation%20of%20the%20image%0Aat%20the%20next%20wavelength%2C%20which%20follows%20the%20low-rank%20property%20of%20spectral%20data.%0AMoreover%2C%20we%20design%20a%20band-aware%20state%20space%20model%20that%20employs%20a%0Apixel-differentiated%20scanning%20strategy%20at%20different%20stages%20of%20the%20generation%0Aprocess%2C%20further%20suppressing%20interference%20from%20low-correlation%20regions%20caused%0Aby%20reflectance%20differences.%20Through%20extensive%20experimentation%20across%20different%0Adatasets%2C%20FRN%20achieves%20superior%20reconstruction%20performance%20compared%20to%0Astate-of-the-art%20methods%20in%20both%20quantitative%20and%20qualitative%20evaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15439v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFRN%253A%2520Fractal-Based%2520Recursive%2520Spectral%2520Reconstruction%2520Network%26entry.906535625%3DGe%2520Meng%2520and%2520Zhongnan%2520Cai%2520and%2520Ruizhe%2520Chen%2520and%2520Jingyan%2520Tu%2520and%2520Yingying%2520Wang%2520and%2520Yue%2520Huang%2520and%2520Xinghao%2520Ding%26entry.1292438233%3D%2520%2520Generating%2520hyperspectral%2520images%2520%2528HSIs%2529%2520from%2520RGB%2520images%2520through%2520spectral%250Areconstruction%2520can%2520significantly%2520reduce%2520the%2520cost%2520of%2520HSI%2520acquisition.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520Fractal-Based%2520Recursive%2520Spectral%2520Reconstruction%2520Network%250A%2528FRN%2529%252C%2520which%2520differs%2520from%2520existing%2520paradigms%2520that%2520attempt%2520to%2520directly%2520integrate%250Athe%2520full-spectrum%2520information%2520from%2520the%2520R%252C%2520G%252C%2520and%2520B%2520channels%2520in%2520a%2520one-shot%250Amanner.%2520Instead%252C%2520it%2520treats%2520spectral%2520reconstruction%2520as%2520a%2520progressive%2520process%252C%250Apredicting%2520from%2520broad%2520to%2520narrow%2520bands%2520or%2520employing%2520a%2520coarse-to-fine%2520approach%250Afor%2520predicting%2520the%2520next%2520wavelength.%2520Inspired%2520by%2520fractals%2520in%2520mathematics%252C%2520FRN%250Aestablishes%2520a%2520novel%2520spectral%2520reconstruction%2520paradigm%2520by%2520recursively%2520invoking%2520an%250Aatomic%2520reconstruction%2520module.%2520In%2520each%2520invocation%252C%2520only%2520the%2520spectral%2520information%250Afrom%2520neighboring%2520bands%2520is%2520used%2520to%2520provide%2520clues%2520for%2520the%2520generation%2520of%2520the%2520image%250Aat%2520the%2520next%2520wavelength%252C%2520which%2520follows%2520the%2520low-rank%2520property%2520of%2520spectral%2520data.%250AMoreover%252C%2520we%2520design%2520a%2520band-aware%2520state%2520space%2520model%2520that%2520employs%2520a%250Apixel-differentiated%2520scanning%2520strategy%2520at%2520different%2520stages%2520of%2520the%2520generation%250Aprocess%252C%2520further%2520suppressing%2520interference%2520from%2520low-correlation%2520regions%2520caused%250Aby%2520reflectance%2520differences.%2520Through%2520extensive%2520experimentation%2520across%2520different%250Adatasets%252C%2520FRN%2520achieves%2520superior%2520reconstruction%2520performance%2520compared%2520to%250Astate-of-the-art%2520methods%2520in%2520both%2520quantitative%2520and%2520qualitative%2520evaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15439v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FRN%3A%20Fractal-Based%20Recursive%20Spectral%20Reconstruction%20Network&entry.906535625=Ge%20Meng%20and%20Zhongnan%20Cai%20and%20Ruizhe%20Chen%20and%20Jingyan%20Tu%20and%20Yingying%20Wang%20and%20Yue%20Huang%20and%20Xinghao%20Ding&entry.1292438233=%20%20Generating%20hyperspectral%20images%20%28HSIs%29%20from%20RGB%20images%20through%20spectral%0Areconstruction%20can%20significantly%20reduce%20the%20cost%20of%20HSI%20acquisition.%20In%20this%0Apaper%2C%20we%20propose%20a%20Fractal-Based%20Recursive%20Spectral%20Reconstruction%20Network%0A%28FRN%29%2C%20which%20differs%20from%20existing%20paradigms%20that%20attempt%20to%20directly%20integrate%0Athe%20full-spectrum%20information%20from%20the%20R%2C%20G%2C%20and%20B%20channels%20in%20a%20one-shot%0Amanner.%20Instead%2C%20it%20treats%20spectral%20reconstruction%20as%20a%20progressive%20process%2C%0Apredicting%20from%20broad%20to%20narrow%20bands%20or%20employing%20a%20coarse-to-fine%20approach%0Afor%20predicting%20the%20next%20wavelength.%20Inspired%20by%20fractals%20in%20mathematics%2C%20FRN%0Aestablishes%20a%20novel%20spectral%20reconstruction%20paradigm%20by%20recursively%20invoking%20an%0Aatomic%20reconstruction%20module.%20In%20each%20invocation%2C%20only%20the%20spectral%20information%0Afrom%20neighboring%20bands%20is%20used%20to%20provide%20clues%20for%20the%20generation%20of%20the%20image%0Aat%20the%20next%20wavelength%2C%20which%20follows%20the%20low-rank%20property%20of%20spectral%20data.%0AMoreover%2C%20we%20design%20a%20band-aware%20state%20space%20model%20that%20employs%20a%0Apixel-differentiated%20scanning%20strategy%20at%20different%20stages%20of%20the%20generation%0Aprocess%2C%20further%20suppressing%20interference%20from%20low-correlation%20regions%20caused%0Aby%20reflectance%20differences.%20Through%20extensive%20experimentation%20across%20different%0Adatasets%2C%20FRN%20achieves%20superior%20reconstruction%20performance%20compared%20to%0Astate-of-the-art%20methods%20in%20both%20quantitative%20and%20qualitative%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15439v1&entry.124074799=Read"},
{"title": "Leveraging Unit Language Guidance to Advance Speech Modeling in Textless\n  Speech-to-Speech Translation", "author": "Yuhao Zhang and Xiangnan Ma and Kaiqi Kou and Peizhuo Liu and Weiqiao Shan and Benyou Wang and Tong Xiao and Yuxin Huang and Zhengtao Yu and Jingbo Zhu", "abstract": "  The success of building textless speech-to-speech translation (S2ST) models\nhas attracted much attention. However, S2ST still faces two main challenges: 1)\nextracting linguistic features for various speech signals, called cross-modal\n(CM), and 2) learning alignment of difference languages in long sequences,\ncalled cross-lingual (CL). We propose the unit language to overcome the two\nmodeling challenges. The unit language can be considered a text-like\nrepresentation format, constructed using $n$-gram language modeling. We\nimplement multi-task learning to utilize the unit language in guiding the\nspeech modeling process. Our initial results reveal a conflict when applying\nsource and target unit languages simultaneously. We propose task prompt\nmodeling to mitigate this conflict. We conduct experiments on four languages of\nthe Voxpupil dataset. Our method demonstrates significant improvements over a\nstrong baseline and achieves performance comparable to models trained with\ntext.\n", "link": "http://arxiv.org/abs/2505.15333v1", "date": "2025-05-21", "relevancy": 2.5699, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5185}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5185}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Unit%20Language%20Guidance%20to%20Advance%20Speech%20Modeling%20in%20Textless%0A%20%20Speech-to-Speech%20Translation&body=Title%3A%20Leveraging%20Unit%20Language%20Guidance%20to%20Advance%20Speech%20Modeling%20in%20Textless%0A%20%20Speech-to-Speech%20Translation%0AAuthor%3A%20Yuhao%20Zhang%20and%20Xiangnan%20Ma%20and%20Kaiqi%20Kou%20and%20Peizhuo%20Liu%20and%20Weiqiao%20Shan%20and%20Benyou%20Wang%20and%20Tong%20Xiao%20and%20Yuxin%20Huang%20and%20Zhengtao%20Yu%20and%20Jingbo%20Zhu%0AAbstract%3A%20%20%20The%20success%20of%20building%20textless%20speech-to-speech%20translation%20%28S2ST%29%20models%0Ahas%20attracted%20much%20attention.%20However%2C%20S2ST%20still%20faces%20two%20main%20challenges%3A%201%29%0Aextracting%20linguistic%20features%20for%20various%20speech%20signals%2C%20called%20cross-modal%0A%28CM%29%2C%20and%202%29%20learning%20alignment%20of%20difference%20languages%20in%20long%20sequences%2C%0Acalled%20cross-lingual%20%28CL%29.%20We%20propose%20the%20unit%20language%20to%20overcome%20the%20two%0Amodeling%20challenges.%20The%20unit%20language%20can%20be%20considered%20a%20text-like%0Arepresentation%20format%2C%20constructed%20using%20%24n%24-gram%20language%20modeling.%20We%0Aimplement%20multi-task%20learning%20to%20utilize%20the%20unit%20language%20in%20guiding%20the%0Aspeech%20modeling%20process.%20Our%20initial%20results%20reveal%20a%20conflict%20when%20applying%0Asource%20and%20target%20unit%20languages%20simultaneously.%20We%20propose%20task%20prompt%0Amodeling%20to%20mitigate%20this%20conflict.%20We%20conduct%20experiments%20on%20four%20languages%20of%0Athe%20Voxpupil%20dataset.%20Our%20method%20demonstrates%20significant%20improvements%20over%20a%0Astrong%20baseline%20and%20achieves%20performance%20comparable%20to%20models%20trained%20with%0Atext.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15333v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Unit%2520Language%2520Guidance%2520to%2520Advance%2520Speech%2520Modeling%2520in%2520Textless%250A%2520%2520Speech-to-Speech%2520Translation%26entry.906535625%3DYuhao%2520Zhang%2520and%2520Xiangnan%2520Ma%2520and%2520Kaiqi%2520Kou%2520and%2520Peizhuo%2520Liu%2520and%2520Weiqiao%2520Shan%2520and%2520Benyou%2520Wang%2520and%2520Tong%2520Xiao%2520and%2520Yuxin%2520Huang%2520and%2520Zhengtao%2520Yu%2520and%2520Jingbo%2520Zhu%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520building%2520textless%2520speech-to-speech%2520translation%2520%2528S2ST%2529%2520models%250Ahas%2520attracted%2520much%2520attention.%2520However%252C%2520S2ST%2520still%2520faces%2520two%2520main%2520challenges%253A%25201%2529%250Aextracting%2520linguistic%2520features%2520for%2520various%2520speech%2520signals%252C%2520called%2520cross-modal%250A%2528CM%2529%252C%2520and%25202%2529%2520learning%2520alignment%2520of%2520difference%2520languages%2520in%2520long%2520sequences%252C%250Acalled%2520cross-lingual%2520%2528CL%2529.%2520We%2520propose%2520the%2520unit%2520language%2520to%2520overcome%2520the%2520two%250Amodeling%2520challenges.%2520The%2520unit%2520language%2520can%2520be%2520considered%2520a%2520text-like%250Arepresentation%2520format%252C%2520constructed%2520using%2520%2524n%2524-gram%2520language%2520modeling.%2520We%250Aimplement%2520multi-task%2520learning%2520to%2520utilize%2520the%2520unit%2520language%2520in%2520guiding%2520the%250Aspeech%2520modeling%2520process.%2520Our%2520initial%2520results%2520reveal%2520a%2520conflict%2520when%2520applying%250Asource%2520and%2520target%2520unit%2520languages%2520simultaneously.%2520We%2520propose%2520task%2520prompt%250Amodeling%2520to%2520mitigate%2520this%2520conflict.%2520We%2520conduct%2520experiments%2520on%2520four%2520languages%2520of%250Athe%2520Voxpupil%2520dataset.%2520Our%2520method%2520demonstrates%2520significant%2520improvements%2520over%2520a%250Astrong%2520baseline%2520and%2520achieves%2520performance%2520comparable%2520to%2520models%2520trained%2520with%250Atext.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15333v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Unit%20Language%20Guidance%20to%20Advance%20Speech%20Modeling%20in%20Textless%0A%20%20Speech-to-Speech%20Translation&entry.906535625=Yuhao%20Zhang%20and%20Xiangnan%20Ma%20and%20Kaiqi%20Kou%20and%20Peizhuo%20Liu%20and%20Weiqiao%20Shan%20and%20Benyou%20Wang%20and%20Tong%20Xiao%20and%20Yuxin%20Huang%20and%20Zhengtao%20Yu%20and%20Jingbo%20Zhu&entry.1292438233=%20%20The%20success%20of%20building%20textless%20speech-to-speech%20translation%20%28S2ST%29%20models%0Ahas%20attracted%20much%20attention.%20However%2C%20S2ST%20still%20faces%20two%20main%20challenges%3A%201%29%0Aextracting%20linguistic%20features%20for%20various%20speech%20signals%2C%20called%20cross-modal%0A%28CM%29%2C%20and%202%29%20learning%20alignment%20of%20difference%20languages%20in%20long%20sequences%2C%0Acalled%20cross-lingual%20%28CL%29.%20We%20propose%20the%20unit%20language%20to%20overcome%20the%20two%0Amodeling%20challenges.%20The%20unit%20language%20can%20be%20considered%20a%20text-like%0Arepresentation%20format%2C%20constructed%20using%20%24n%24-gram%20language%20modeling.%20We%0Aimplement%20multi-task%20learning%20to%20utilize%20the%20unit%20language%20in%20guiding%20the%0Aspeech%20modeling%20process.%20Our%20initial%20results%20reveal%20a%20conflict%20when%20applying%0Asource%20and%20target%20unit%20languages%20simultaneously.%20We%20propose%20task%20prompt%0Amodeling%20to%20mitigate%20this%20conflict.%20We%20conduct%20experiments%20on%20four%20languages%20of%0Athe%20Voxpupil%20dataset.%20Our%20method%20demonstrates%20significant%20improvements%20over%20a%0Astrong%20baseline%20and%20achieves%20performance%20comparable%20to%20models%20trained%20with%0Atext.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15333v1&entry.124074799=Read"},
{"title": "HOPSE: Scalable Higher-Order Positional and Structural Encoder for\n  Combinatorial Representations", "author": "Martin Carrasco and Guillermo Bernardez and Marco Montagna and Nina Miolane and Lev Telyatnikov", "abstract": "  While Graph Neural Networks (GNNs) have proven highly effective at modeling\nrelational data, pairwise connections cannot fully capture multi-way\nrelationships naturally present in complex real-world systems. In response to\nthis, Topological Deep Learning (TDL) leverages more general combinatorial\nrepresentations -- such as simplicial or cellular complexes -- to accommodate\nhigher-order interactions. Existing TDL methods often extend GNNs through\nHigher-Order Message Passing (HOMP), but face critical \\emph{scalability\nchallenges} due to \\textit{(i)} a combinatorial explosion of message-passing\nroutes, and \\textit{(ii)} significant complexity overhead from the propagation\nmechanism. To overcome these limitations, we propose HOPSE (Higher-Order\nPositional and Structural Encoder) -- a \\emph{message passing-free} framework\nthat uses Hasse graph decompositions to derive efficient and expressive\nencodings over \\emph{arbitrary higher-order domains}. Notably, HOPSE scales\nlinearly with dataset size while preserving expressive power and permutation\nequivariance. Experiments on molecular, expressivity and topological benchmarks\nshow that HOPSE matches or surpasses state-of-the-art performance while\nachieving up to 7 $times$ speedups over HOMP-based models, opening a new path\nfor scalable TDL.\n", "link": "http://arxiv.org/abs/2505.15405v1", "date": "2025-05-21", "relevancy": 2.5687, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5138}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5138}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HOPSE%3A%20Scalable%20Higher-Order%20Positional%20and%20Structural%20Encoder%20for%0A%20%20Combinatorial%20Representations&body=Title%3A%20HOPSE%3A%20Scalable%20Higher-Order%20Positional%20and%20Structural%20Encoder%20for%0A%20%20Combinatorial%20Representations%0AAuthor%3A%20Martin%20Carrasco%20and%20Guillermo%20Bernardez%20and%20Marco%20Montagna%20and%20Nina%20Miolane%20and%20Lev%20Telyatnikov%0AAbstract%3A%20%20%20While%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20proven%20highly%20effective%20at%20modeling%0Arelational%20data%2C%20pairwise%20connections%20cannot%20fully%20capture%20multi-way%0Arelationships%20naturally%20present%20in%20complex%20real-world%20systems.%20In%20response%20to%0Athis%2C%20Topological%20Deep%20Learning%20%28TDL%29%20leverages%20more%20general%20combinatorial%0Arepresentations%20--%20such%20as%20simplicial%20or%20cellular%20complexes%20--%20to%20accommodate%0Ahigher-order%20interactions.%20Existing%20TDL%20methods%20often%20extend%20GNNs%20through%0AHigher-Order%20Message%20Passing%20%28HOMP%29%2C%20but%20face%20critical%20%5Cemph%7Bscalability%0Achallenges%7D%20due%20to%20%5Ctextit%7B%28i%29%7D%20a%20combinatorial%20explosion%20of%20message-passing%0Aroutes%2C%20and%20%5Ctextit%7B%28ii%29%7D%20significant%20complexity%20overhead%20from%20the%20propagation%0Amechanism.%20To%20overcome%20these%20limitations%2C%20we%20propose%20HOPSE%20%28Higher-Order%0APositional%20and%20Structural%20Encoder%29%20--%20a%20%5Cemph%7Bmessage%20passing-free%7D%20framework%0Athat%20uses%20Hasse%20graph%20decompositions%20to%20derive%20efficient%20and%20expressive%0Aencodings%20over%20%5Cemph%7Barbitrary%20higher-order%20domains%7D.%20Notably%2C%20HOPSE%20scales%0Alinearly%20with%20dataset%20size%20while%20preserving%20expressive%20power%20and%20permutation%0Aequivariance.%20Experiments%20on%20molecular%2C%20expressivity%20and%20topological%20benchmarks%0Ashow%20that%20HOPSE%20matches%20or%20surpasses%20state-of-the-art%20performance%20while%0Aachieving%20up%20to%207%20%24times%24%20speedups%20over%20HOMP-based%20models%2C%20opening%20a%20new%20path%0Afor%20scalable%20TDL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15405v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHOPSE%253A%2520Scalable%2520Higher-Order%2520Positional%2520and%2520Structural%2520Encoder%2520for%250A%2520%2520Combinatorial%2520Representations%26entry.906535625%3DMartin%2520Carrasco%2520and%2520Guillermo%2520Bernardez%2520and%2520Marco%2520Montagna%2520and%2520Nina%2520Miolane%2520and%2520Lev%2520Telyatnikov%26entry.1292438233%3D%2520%2520While%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520proven%2520highly%2520effective%2520at%2520modeling%250Arelational%2520data%252C%2520pairwise%2520connections%2520cannot%2520fully%2520capture%2520multi-way%250Arelationships%2520naturally%2520present%2520in%2520complex%2520real-world%2520systems.%2520In%2520response%2520to%250Athis%252C%2520Topological%2520Deep%2520Learning%2520%2528TDL%2529%2520leverages%2520more%2520general%2520combinatorial%250Arepresentations%2520--%2520such%2520as%2520simplicial%2520or%2520cellular%2520complexes%2520--%2520to%2520accommodate%250Ahigher-order%2520interactions.%2520Existing%2520TDL%2520methods%2520often%2520extend%2520GNNs%2520through%250AHigher-Order%2520Message%2520Passing%2520%2528HOMP%2529%252C%2520but%2520face%2520critical%2520%255Cemph%257Bscalability%250Achallenges%257D%2520due%2520to%2520%255Ctextit%257B%2528i%2529%257D%2520a%2520combinatorial%2520explosion%2520of%2520message-passing%250Aroutes%252C%2520and%2520%255Ctextit%257B%2528ii%2529%257D%2520significant%2520complexity%2520overhead%2520from%2520the%2520propagation%250Amechanism.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520HOPSE%2520%2528Higher-Order%250APositional%2520and%2520Structural%2520Encoder%2529%2520--%2520a%2520%255Cemph%257Bmessage%2520passing-free%257D%2520framework%250Athat%2520uses%2520Hasse%2520graph%2520decompositions%2520to%2520derive%2520efficient%2520and%2520expressive%250Aencodings%2520over%2520%255Cemph%257Barbitrary%2520higher-order%2520domains%257D.%2520Notably%252C%2520HOPSE%2520scales%250Alinearly%2520with%2520dataset%2520size%2520while%2520preserving%2520expressive%2520power%2520and%2520permutation%250Aequivariance.%2520Experiments%2520on%2520molecular%252C%2520expressivity%2520and%2520topological%2520benchmarks%250Ashow%2520that%2520HOPSE%2520matches%2520or%2520surpasses%2520state-of-the-art%2520performance%2520while%250Aachieving%2520up%2520to%25207%2520%2524times%2524%2520speedups%2520over%2520HOMP-based%2520models%252C%2520opening%2520a%2520new%2520path%250Afor%2520scalable%2520TDL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15405v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HOPSE%3A%20Scalable%20Higher-Order%20Positional%20and%20Structural%20Encoder%20for%0A%20%20Combinatorial%20Representations&entry.906535625=Martin%20Carrasco%20and%20Guillermo%20Bernardez%20and%20Marco%20Montagna%20and%20Nina%20Miolane%20and%20Lev%20Telyatnikov&entry.1292438233=%20%20While%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20proven%20highly%20effective%20at%20modeling%0Arelational%20data%2C%20pairwise%20connections%20cannot%20fully%20capture%20multi-way%0Arelationships%20naturally%20present%20in%20complex%20real-world%20systems.%20In%20response%20to%0Athis%2C%20Topological%20Deep%20Learning%20%28TDL%29%20leverages%20more%20general%20combinatorial%0Arepresentations%20--%20such%20as%20simplicial%20or%20cellular%20complexes%20--%20to%20accommodate%0Ahigher-order%20interactions.%20Existing%20TDL%20methods%20often%20extend%20GNNs%20through%0AHigher-Order%20Message%20Passing%20%28HOMP%29%2C%20but%20face%20critical%20%5Cemph%7Bscalability%0Achallenges%7D%20due%20to%20%5Ctextit%7B%28i%29%7D%20a%20combinatorial%20explosion%20of%20message-passing%0Aroutes%2C%20and%20%5Ctextit%7B%28ii%29%7D%20significant%20complexity%20overhead%20from%20the%20propagation%0Amechanism.%20To%20overcome%20these%20limitations%2C%20we%20propose%20HOPSE%20%28Higher-Order%0APositional%20and%20Structural%20Encoder%29%20--%20a%20%5Cemph%7Bmessage%20passing-free%7D%20framework%0Athat%20uses%20Hasse%20graph%20decompositions%20to%20derive%20efficient%20and%20expressive%0Aencodings%20over%20%5Cemph%7Barbitrary%20higher-order%20domains%7D.%20Notably%2C%20HOPSE%20scales%0Alinearly%20with%20dataset%20size%20while%20preserving%20expressive%20power%20and%20permutation%0Aequivariance.%20Experiments%20on%20molecular%2C%20expressivity%20and%20topological%20benchmarks%0Ashow%20that%20HOPSE%20matches%20or%20surpasses%20state-of-the-art%20performance%20while%0Aachieving%20up%20to%207%20%24times%24%20speedups%20over%20HOMP-based%20models%2C%20opening%20a%20new%20path%0Afor%20scalable%20TDL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15405v1&entry.124074799=Read"},
{"title": "Symmetry-Robust 3D Orientation Estimation", "author": "Christopher Scarvelis and David Benhaim and Paul Zhang", "abstract": "  Orientation estimation is a fundamental task in 3D shape analysis which\nconsists of estimating a shape's orientation axes: its side-, up-, and\nfront-axes. Using this data, one can rotate a shape into canonical orientation,\nwhere its orientation axes are aligned with the coordinate axes. Developing an\norientation algorithm that reliably estimates complete orientations of general\nshapes remains an open problem. We introduce a two-stage orientation pipeline\nthat achieves state of the art performance on up-axis estimation and further\ndemonstrate its efficacy on full-orientation estimation, where one seeks all\nthree orientation axes. Unlike previous work, we train and evaluate our method\non all of Shapenet rather than a subset of classes. We motivate our engineering\ncontributions by theory describing fundamental obstacles to orientation\nestimation for rotationally-symmetric shapes, and show how our method avoids\nthese obstacles.\n", "link": "http://arxiv.org/abs/2410.02101v3", "date": "2025-05-21", "relevancy": 2.565, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5324}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5192}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Symmetry-Robust%203D%20Orientation%20Estimation&body=Title%3A%20Symmetry-Robust%203D%20Orientation%20Estimation%0AAuthor%3A%20Christopher%20Scarvelis%20and%20David%20Benhaim%20and%20Paul%20Zhang%0AAbstract%3A%20%20%20Orientation%20estimation%20is%20a%20fundamental%20task%20in%203D%20shape%20analysis%20which%0Aconsists%20of%20estimating%20a%20shape%27s%20orientation%20axes%3A%20its%20side-%2C%20up-%2C%20and%0Afront-axes.%20Using%20this%20data%2C%20one%20can%20rotate%20a%20shape%20into%20canonical%20orientation%2C%0Awhere%20its%20orientation%20axes%20are%20aligned%20with%20the%20coordinate%20axes.%20Developing%20an%0Aorientation%20algorithm%20that%20reliably%20estimates%20complete%20orientations%20of%20general%0Ashapes%20remains%20an%20open%20problem.%20We%20introduce%20a%20two-stage%20orientation%20pipeline%0Athat%20achieves%20state%20of%20the%20art%20performance%20on%20up-axis%20estimation%20and%20further%0Ademonstrate%20its%20efficacy%20on%20full-orientation%20estimation%2C%20where%20one%20seeks%20all%0Athree%20orientation%20axes.%20Unlike%20previous%20work%2C%20we%20train%20and%20evaluate%20our%20method%0Aon%20all%20of%20Shapenet%20rather%20than%20a%20subset%20of%20classes.%20We%20motivate%20our%20engineering%0Acontributions%20by%20theory%20describing%20fundamental%20obstacles%20to%20orientation%0Aestimation%20for%20rotationally-symmetric%20shapes%2C%20and%20show%20how%20our%20method%20avoids%0Athese%20obstacles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02101v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSymmetry-Robust%25203D%2520Orientation%2520Estimation%26entry.906535625%3DChristopher%2520Scarvelis%2520and%2520David%2520Benhaim%2520and%2520Paul%2520Zhang%26entry.1292438233%3D%2520%2520Orientation%2520estimation%2520is%2520a%2520fundamental%2520task%2520in%25203D%2520shape%2520analysis%2520which%250Aconsists%2520of%2520estimating%2520a%2520shape%2527s%2520orientation%2520axes%253A%2520its%2520side-%252C%2520up-%252C%2520and%250Afront-axes.%2520Using%2520this%2520data%252C%2520one%2520can%2520rotate%2520a%2520shape%2520into%2520canonical%2520orientation%252C%250Awhere%2520its%2520orientation%2520axes%2520are%2520aligned%2520with%2520the%2520coordinate%2520axes.%2520Developing%2520an%250Aorientation%2520algorithm%2520that%2520reliably%2520estimates%2520complete%2520orientations%2520of%2520general%250Ashapes%2520remains%2520an%2520open%2520problem.%2520We%2520introduce%2520a%2520two-stage%2520orientation%2520pipeline%250Athat%2520achieves%2520state%2520of%2520the%2520art%2520performance%2520on%2520up-axis%2520estimation%2520and%2520further%250Ademonstrate%2520its%2520efficacy%2520on%2520full-orientation%2520estimation%252C%2520where%2520one%2520seeks%2520all%250Athree%2520orientation%2520axes.%2520Unlike%2520previous%2520work%252C%2520we%2520train%2520and%2520evaluate%2520our%2520method%250Aon%2520all%2520of%2520Shapenet%2520rather%2520than%2520a%2520subset%2520of%2520classes.%2520We%2520motivate%2520our%2520engineering%250Acontributions%2520by%2520theory%2520describing%2520fundamental%2520obstacles%2520to%2520orientation%250Aestimation%2520for%2520rotationally-symmetric%2520shapes%252C%2520and%2520show%2520how%2520our%2520method%2520avoids%250Athese%2520obstacles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02101v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Symmetry-Robust%203D%20Orientation%20Estimation&entry.906535625=Christopher%20Scarvelis%20and%20David%20Benhaim%20and%20Paul%20Zhang&entry.1292438233=%20%20Orientation%20estimation%20is%20a%20fundamental%20task%20in%203D%20shape%20analysis%20which%0Aconsists%20of%20estimating%20a%20shape%27s%20orientation%20axes%3A%20its%20side-%2C%20up-%2C%20and%0Afront-axes.%20Using%20this%20data%2C%20one%20can%20rotate%20a%20shape%20into%20canonical%20orientation%2C%0Awhere%20its%20orientation%20axes%20are%20aligned%20with%20the%20coordinate%20axes.%20Developing%20an%0Aorientation%20algorithm%20that%20reliably%20estimates%20complete%20orientations%20of%20general%0Ashapes%20remains%20an%20open%20problem.%20We%20introduce%20a%20two-stage%20orientation%20pipeline%0Athat%20achieves%20state%20of%20the%20art%20performance%20on%20up-axis%20estimation%20and%20further%0Ademonstrate%20its%20efficacy%20on%20full-orientation%20estimation%2C%20where%20one%20seeks%20all%0Athree%20orientation%20axes.%20Unlike%20previous%20work%2C%20we%20train%20and%20evaluate%20our%20method%0Aon%20all%20of%20Shapenet%20rather%20than%20a%20subset%20of%20classes.%20We%20motivate%20our%20engineering%0Acontributions%20by%20theory%20describing%20fundamental%20obstacles%20to%20orientation%0Aestimation%20for%20rotationally-symmetric%20shapes%2C%20and%20show%20how%20our%20method%20avoids%0Athese%20obstacles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02101v3&entry.124074799=Read"},
{"title": "Detection of Underwater Multi-Targets Based on Self-Supervised Learning\n  and Deformable Path Aggregation Feature Pyramid Network", "author": "Chang Liu", "abstract": "  To overcome the constraints of the underwater environment and improve the\naccuracy and robustness of underwater target detection models, this paper\ndevelops a specialized dataset for underwater target detection and proposes an\nefficient algorithm for underwater multi-target detection. A self-supervised\nlearning based on the SimSiam structure is employed for the pre-training of\nunderwater target detection network. To address the problems of low detection\naccuracy caused by low contrast, mutual occlusion and dense distribution of\nunderwater targets in underwater object detection, a detection model suitable\nfor underwater target detection is proposed by introducing deformable\nconvolution and dilated convolution. The proposed detection model can obtain\nmore effective information by increasing the receptive field. In addition, the\nregression loss function EIoU is introduced, which improves model performance\nby separately calculating the width and height losses of the predicted box.\nExperiment results show that the accuracy of the underwater target detection\nhas been improved by the proposed detector.\n", "link": "http://arxiv.org/abs/2505.15518v1", "date": "2025-05-21", "relevancy": 2.5646, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5297}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5081}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detection%20of%20Underwater%20Multi-Targets%20Based%20on%20Self-Supervised%20Learning%0A%20%20and%20Deformable%20Path%20Aggregation%20Feature%20Pyramid%20Network&body=Title%3A%20Detection%20of%20Underwater%20Multi-Targets%20Based%20on%20Self-Supervised%20Learning%0A%20%20and%20Deformable%20Path%20Aggregation%20Feature%20Pyramid%20Network%0AAuthor%3A%20Chang%20Liu%0AAbstract%3A%20%20%20To%20overcome%20the%20constraints%20of%20the%20underwater%20environment%20and%20improve%20the%0Aaccuracy%20and%20robustness%20of%20underwater%20target%20detection%20models%2C%20this%20paper%0Adevelops%20a%20specialized%20dataset%20for%20underwater%20target%20detection%20and%20proposes%20an%0Aefficient%20algorithm%20for%20underwater%20multi-target%20detection.%20A%20self-supervised%0Alearning%20based%20on%20the%20SimSiam%20structure%20is%20employed%20for%20the%20pre-training%20of%0Aunderwater%20target%20detection%20network.%20To%20address%20the%20problems%20of%20low%20detection%0Aaccuracy%20caused%20by%20low%20contrast%2C%20mutual%20occlusion%20and%20dense%20distribution%20of%0Aunderwater%20targets%20in%20underwater%20object%20detection%2C%20a%20detection%20model%20suitable%0Afor%20underwater%20target%20detection%20is%20proposed%20by%20introducing%20deformable%0Aconvolution%20and%20dilated%20convolution.%20The%20proposed%20detection%20model%20can%20obtain%0Amore%20effective%20information%20by%20increasing%20the%20receptive%20field.%20In%20addition%2C%20the%0Aregression%20loss%20function%20EIoU%20is%20introduced%2C%20which%20improves%20model%20performance%0Aby%20separately%20calculating%20the%20width%20and%20height%20losses%20of%20the%20predicted%20box.%0AExperiment%20results%20show%20that%20the%20accuracy%20of%20the%20underwater%20target%20detection%0Ahas%20been%20improved%20by%20the%20proposed%20detector.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15518v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetection%2520of%2520Underwater%2520Multi-Targets%2520Based%2520on%2520Self-Supervised%2520Learning%250A%2520%2520and%2520Deformable%2520Path%2520Aggregation%2520Feature%2520Pyramid%2520Network%26entry.906535625%3DChang%2520Liu%26entry.1292438233%3D%2520%2520To%2520overcome%2520the%2520constraints%2520of%2520the%2520underwater%2520environment%2520and%2520improve%2520the%250Aaccuracy%2520and%2520robustness%2520of%2520underwater%2520target%2520detection%2520models%252C%2520this%2520paper%250Adevelops%2520a%2520specialized%2520dataset%2520for%2520underwater%2520target%2520detection%2520and%2520proposes%2520an%250Aefficient%2520algorithm%2520for%2520underwater%2520multi-target%2520detection.%2520A%2520self-supervised%250Alearning%2520based%2520on%2520the%2520SimSiam%2520structure%2520is%2520employed%2520for%2520the%2520pre-training%2520of%250Aunderwater%2520target%2520detection%2520network.%2520To%2520address%2520the%2520problems%2520of%2520low%2520detection%250Aaccuracy%2520caused%2520by%2520low%2520contrast%252C%2520mutual%2520occlusion%2520and%2520dense%2520distribution%2520of%250Aunderwater%2520targets%2520in%2520underwater%2520object%2520detection%252C%2520a%2520detection%2520model%2520suitable%250Afor%2520underwater%2520target%2520detection%2520is%2520proposed%2520by%2520introducing%2520deformable%250Aconvolution%2520and%2520dilated%2520convolution.%2520The%2520proposed%2520detection%2520model%2520can%2520obtain%250Amore%2520effective%2520information%2520by%2520increasing%2520the%2520receptive%2520field.%2520In%2520addition%252C%2520the%250Aregression%2520loss%2520function%2520EIoU%2520is%2520introduced%252C%2520which%2520improves%2520model%2520performance%250Aby%2520separately%2520calculating%2520the%2520width%2520and%2520height%2520losses%2520of%2520the%2520predicted%2520box.%250AExperiment%2520results%2520show%2520that%2520the%2520accuracy%2520of%2520the%2520underwater%2520target%2520detection%250Ahas%2520been%2520improved%2520by%2520the%2520proposed%2520detector.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15518v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detection%20of%20Underwater%20Multi-Targets%20Based%20on%20Self-Supervised%20Learning%0A%20%20and%20Deformable%20Path%20Aggregation%20Feature%20Pyramid%20Network&entry.906535625=Chang%20Liu&entry.1292438233=%20%20To%20overcome%20the%20constraints%20of%20the%20underwater%20environment%20and%20improve%20the%0Aaccuracy%20and%20robustness%20of%20underwater%20target%20detection%20models%2C%20this%20paper%0Adevelops%20a%20specialized%20dataset%20for%20underwater%20target%20detection%20and%20proposes%20an%0Aefficient%20algorithm%20for%20underwater%20multi-target%20detection.%20A%20self-supervised%0Alearning%20based%20on%20the%20SimSiam%20structure%20is%20employed%20for%20the%20pre-training%20of%0Aunderwater%20target%20detection%20network.%20To%20address%20the%20problems%20of%20low%20detection%0Aaccuracy%20caused%20by%20low%20contrast%2C%20mutual%20occlusion%20and%20dense%20distribution%20of%0Aunderwater%20targets%20in%20underwater%20object%20detection%2C%20a%20detection%20model%20suitable%0Afor%20underwater%20target%20detection%20is%20proposed%20by%20introducing%20deformable%0Aconvolution%20and%20dilated%20convolution.%20The%20proposed%20detection%20model%20can%20obtain%0Amore%20effective%20information%20by%20increasing%20the%20receptive%20field.%20In%20addition%2C%20the%0Aregression%20loss%20function%20EIoU%20is%20introduced%2C%20which%20improves%20model%20performance%0Aby%20separately%20calculating%20the%20width%20and%20height%20losses%20of%20the%20predicted%20box.%0AExperiment%20results%20show%20that%20the%20accuracy%20of%20the%20underwater%20target%20detection%0Ahas%20been%20improved%20by%20the%20proposed%20detector.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15518v1&entry.124074799=Read"},
{"title": "Intermediate Languages Matter: Formal Choice Drives Neurosymbolic LLM\n  Reasoning", "author": "Alexander Beiser and David Penz and Nysret Musliu", "abstract": "  Large language models (LLMs) achieve astonishing results on a wide range of\ntasks. However, their formal reasoning ability still lags behind. A promising\napproach is Neurosymbolic LLM reasoning. It works by using LLMs as translators\nfrom natural to formal languages and symbolic solvers for deriving correct\nresults. Still, it remains unclear what the contributing factors to the success\nof Neurosymbolic LLM reasoning are. This paper shows that one important factor\nis the choice of the formal language. By comparing 4 formal languages on 3\ndatasets over 6 LLMs, we show that the choice of formal language affects both\nthe syntactic and the semantic reasoning capability. Thereby, we introduce the\nintermediate language challenge, which is the challenge of picking a suitable\nformal language for neurosymbolic reasoning. Further, we compare the effects of\nusing different in-context-learning examples in an ablation study. We conclude\nthat on average, context-aware encodings help LLMs to reason, while there is no\napparent effect of using comments or markdown syntax.\n", "link": "http://arxiv.org/abs/2502.17216v2", "date": "2025-05-21", "relevancy": 2.5635, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5402}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5402}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intermediate%20Languages%20Matter%3A%20Formal%20Choice%20Drives%20Neurosymbolic%20LLM%0A%20%20Reasoning&body=Title%3A%20Intermediate%20Languages%20Matter%3A%20Formal%20Choice%20Drives%20Neurosymbolic%20LLM%0A%20%20Reasoning%0AAuthor%3A%20Alexander%20Beiser%20and%20David%20Penz%20and%20Nysret%20Musliu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20achieve%20astonishing%20results%20on%20a%20wide%20range%20of%0Atasks.%20However%2C%20their%20formal%20reasoning%20ability%20still%20lags%20behind.%20A%20promising%0Aapproach%20is%20Neurosymbolic%20LLM%20reasoning.%20It%20works%20by%20using%20LLMs%20as%20translators%0Afrom%20natural%20to%20formal%20languages%20and%20symbolic%20solvers%20for%20deriving%20correct%0Aresults.%20Still%2C%20it%20remains%20unclear%20what%20the%20contributing%20factors%20to%20the%20success%0Aof%20Neurosymbolic%20LLM%20reasoning%20are.%20This%20paper%20shows%20that%20one%20important%20factor%0Ais%20the%20choice%20of%20the%20formal%20language.%20By%20comparing%204%20formal%20languages%20on%203%0Adatasets%20over%206%20LLMs%2C%20we%20show%20that%20the%20choice%20of%20formal%20language%20affects%20both%0Athe%20syntactic%20and%20the%20semantic%20reasoning%20capability.%20Thereby%2C%20we%20introduce%20the%0Aintermediate%20language%20challenge%2C%20which%20is%20the%20challenge%20of%20picking%20a%20suitable%0Aformal%20language%20for%20neurosymbolic%20reasoning.%20Further%2C%20we%20compare%20the%20effects%20of%0Ausing%20different%20in-context-learning%20examples%20in%20an%20ablation%20study.%20We%20conclude%0Athat%20on%20average%2C%20context-aware%20encodings%20help%20LLMs%20to%20reason%2C%20while%20there%20is%20no%0Aapparent%20effect%20of%20using%20comments%20or%20markdown%20syntax.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17216v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntermediate%2520Languages%2520Matter%253A%2520Formal%2520Choice%2520Drives%2520Neurosymbolic%2520LLM%250A%2520%2520Reasoning%26entry.906535625%3DAlexander%2520Beiser%2520and%2520David%2520Penz%2520and%2520Nysret%2520Musliu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520achieve%2520astonishing%2520results%2520on%2520a%2520wide%2520range%2520of%250Atasks.%2520However%252C%2520their%2520formal%2520reasoning%2520ability%2520still%2520lags%2520behind.%2520A%2520promising%250Aapproach%2520is%2520Neurosymbolic%2520LLM%2520reasoning.%2520It%2520works%2520by%2520using%2520LLMs%2520as%2520translators%250Afrom%2520natural%2520to%2520formal%2520languages%2520and%2520symbolic%2520solvers%2520for%2520deriving%2520correct%250Aresults.%2520Still%252C%2520it%2520remains%2520unclear%2520what%2520the%2520contributing%2520factors%2520to%2520the%2520success%250Aof%2520Neurosymbolic%2520LLM%2520reasoning%2520are.%2520This%2520paper%2520shows%2520that%2520one%2520important%2520factor%250Ais%2520the%2520choice%2520of%2520the%2520formal%2520language.%2520By%2520comparing%25204%2520formal%2520languages%2520on%25203%250Adatasets%2520over%25206%2520LLMs%252C%2520we%2520show%2520that%2520the%2520choice%2520of%2520formal%2520language%2520affects%2520both%250Athe%2520syntactic%2520and%2520the%2520semantic%2520reasoning%2520capability.%2520Thereby%252C%2520we%2520introduce%2520the%250Aintermediate%2520language%2520challenge%252C%2520which%2520is%2520the%2520challenge%2520of%2520picking%2520a%2520suitable%250Aformal%2520language%2520for%2520neurosymbolic%2520reasoning.%2520Further%252C%2520we%2520compare%2520the%2520effects%2520of%250Ausing%2520different%2520in-context-learning%2520examples%2520in%2520an%2520ablation%2520study.%2520We%2520conclude%250Athat%2520on%2520average%252C%2520context-aware%2520encodings%2520help%2520LLMs%2520to%2520reason%252C%2520while%2520there%2520is%2520no%250Aapparent%2520effect%2520of%2520using%2520comments%2520or%2520markdown%2520syntax.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17216v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intermediate%20Languages%20Matter%3A%20Formal%20Choice%20Drives%20Neurosymbolic%20LLM%0A%20%20Reasoning&entry.906535625=Alexander%20Beiser%20and%20David%20Penz%20and%20Nysret%20Musliu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20achieve%20astonishing%20results%20on%20a%20wide%20range%20of%0Atasks.%20However%2C%20their%20formal%20reasoning%20ability%20still%20lags%20behind.%20A%20promising%0Aapproach%20is%20Neurosymbolic%20LLM%20reasoning.%20It%20works%20by%20using%20LLMs%20as%20translators%0Afrom%20natural%20to%20formal%20languages%20and%20symbolic%20solvers%20for%20deriving%20correct%0Aresults.%20Still%2C%20it%20remains%20unclear%20what%20the%20contributing%20factors%20to%20the%20success%0Aof%20Neurosymbolic%20LLM%20reasoning%20are.%20This%20paper%20shows%20that%20one%20important%20factor%0Ais%20the%20choice%20of%20the%20formal%20language.%20By%20comparing%204%20formal%20languages%20on%203%0Adatasets%20over%206%20LLMs%2C%20we%20show%20that%20the%20choice%20of%20formal%20language%20affects%20both%0Athe%20syntactic%20and%20the%20semantic%20reasoning%20capability.%20Thereby%2C%20we%20introduce%20the%0Aintermediate%20language%20challenge%2C%20which%20is%20the%20challenge%20of%20picking%20a%20suitable%0Aformal%20language%20for%20neurosymbolic%20reasoning.%20Further%2C%20we%20compare%20the%20effects%20of%0Ausing%20different%20in-context-learning%20examples%20in%20an%20ablation%20study.%20We%20conclude%0Athat%20on%20average%2C%20context-aware%20encodings%20help%20LLMs%20to%20reason%2C%20while%20there%20is%20no%0Aapparent%20effect%20of%20using%20comments%20or%20markdown%20syntax.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17216v2&entry.124074799=Read"},
{"title": "A Taxonomy of Structure from Motion Methods", "author": "Federica Arrigoni", "abstract": "  Structure from Motion (SfM) refers to the problem of recovering both\nstructure (i.e., 3D coordinates of points in the scene) and motion (i.e.,\ncamera matrices) starting from point correspondences in multiple images. It has\nattracted significant attention over the years, counting practical\nreconstruction pipelines as well as theoretical results. This paper is\nconceived as a conceptual review of SfM methods, which are grouped into three\nmain categories, according to which part of the problem - between motion and\nstructure - they focus on. The proposed taxonomy brings a new perspective on\nexisting SfM approaches as well as insights into open problems and possible\nfuture research directions. Particular emphasis is given on identifying the\ntheoretical conditions that make SfM well posed, which depend on the problem\nformulation that is being considered.\n", "link": "http://arxiv.org/abs/2505.15814v1", "date": "2025-05-21", "relevancy": 2.5591, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5197}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5079}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Taxonomy%20of%20Structure%20from%20Motion%20Methods&body=Title%3A%20A%20Taxonomy%20of%20Structure%20from%20Motion%20Methods%0AAuthor%3A%20Federica%20Arrigoni%0AAbstract%3A%20%20%20Structure%20from%20Motion%20%28SfM%29%20refers%20to%20the%20problem%20of%20recovering%20both%0Astructure%20%28i.e.%2C%203D%20coordinates%20of%20points%20in%20the%20scene%29%20and%20motion%20%28i.e.%2C%0Acamera%20matrices%29%20starting%20from%20point%20correspondences%20in%20multiple%20images.%20It%20has%0Aattracted%20significant%20attention%20over%20the%20years%2C%20counting%20practical%0Areconstruction%20pipelines%20as%20well%20as%20theoretical%20results.%20This%20paper%20is%0Aconceived%20as%20a%20conceptual%20review%20of%20SfM%20methods%2C%20which%20are%20grouped%20into%20three%0Amain%20categories%2C%20according%20to%20which%20part%20of%20the%20problem%20-%20between%20motion%20and%0Astructure%20-%20they%20focus%20on.%20The%20proposed%20taxonomy%20brings%20a%20new%20perspective%20on%0Aexisting%20SfM%20approaches%20as%20well%20as%20insights%20into%20open%20problems%20and%20possible%0Afuture%20research%20directions.%20Particular%20emphasis%20is%20given%20on%20identifying%20the%0Atheoretical%20conditions%20that%20make%20SfM%20well%20posed%2C%20which%20depend%20on%20the%20problem%0Aformulation%20that%20is%20being%20considered.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Taxonomy%2520of%2520Structure%2520from%2520Motion%2520Methods%26entry.906535625%3DFederica%2520Arrigoni%26entry.1292438233%3D%2520%2520Structure%2520from%2520Motion%2520%2528SfM%2529%2520refers%2520to%2520the%2520problem%2520of%2520recovering%2520both%250Astructure%2520%2528i.e.%252C%25203D%2520coordinates%2520of%2520points%2520in%2520the%2520scene%2529%2520and%2520motion%2520%2528i.e.%252C%250Acamera%2520matrices%2529%2520starting%2520from%2520point%2520correspondences%2520in%2520multiple%2520images.%2520It%2520has%250Aattracted%2520significant%2520attention%2520over%2520the%2520years%252C%2520counting%2520practical%250Areconstruction%2520pipelines%2520as%2520well%2520as%2520theoretical%2520results.%2520This%2520paper%2520is%250Aconceived%2520as%2520a%2520conceptual%2520review%2520of%2520SfM%2520methods%252C%2520which%2520are%2520grouped%2520into%2520three%250Amain%2520categories%252C%2520according%2520to%2520which%2520part%2520of%2520the%2520problem%2520-%2520between%2520motion%2520and%250Astructure%2520-%2520they%2520focus%2520on.%2520The%2520proposed%2520taxonomy%2520brings%2520a%2520new%2520perspective%2520on%250Aexisting%2520SfM%2520approaches%2520as%2520well%2520as%2520insights%2520into%2520open%2520problems%2520and%2520possible%250Afuture%2520research%2520directions.%2520Particular%2520emphasis%2520is%2520given%2520on%2520identifying%2520the%250Atheoretical%2520conditions%2520that%2520make%2520SfM%2520well%2520posed%252C%2520which%2520depend%2520on%2520the%2520problem%250Aformulation%2520that%2520is%2520being%2520considered.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Taxonomy%20of%20Structure%20from%20Motion%20Methods&entry.906535625=Federica%20Arrigoni&entry.1292438233=%20%20Structure%20from%20Motion%20%28SfM%29%20refers%20to%20the%20problem%20of%20recovering%20both%0Astructure%20%28i.e.%2C%203D%20coordinates%20of%20points%20in%20the%20scene%29%20and%20motion%20%28i.e.%2C%0Acamera%20matrices%29%20starting%20from%20point%20correspondences%20in%20multiple%20images.%20It%20has%0Aattracted%20significant%20attention%20over%20the%20years%2C%20counting%20practical%0Areconstruction%20pipelines%20as%20well%20as%20theoretical%20results.%20This%20paper%20is%0Aconceived%20as%20a%20conceptual%20review%20of%20SfM%20methods%2C%20which%20are%20grouped%20into%20three%0Amain%20categories%2C%20according%20to%20which%20part%20of%20the%20problem%20-%20between%20motion%20and%0Astructure%20-%20they%20focus%20on.%20The%20proposed%20taxonomy%20brings%20a%20new%20perspective%20on%0Aexisting%20SfM%20approaches%20as%20well%20as%20insights%20into%20open%20problems%20and%20possible%0Afuture%20research%20directions.%20Particular%20emphasis%20is%20given%20on%20identifying%20the%0Atheoretical%20conditions%20that%20make%20SfM%20well%20posed%2C%20which%20depend%20on%20the%20problem%0Aformulation%20that%20is%20being%20considered.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15814v1&entry.124074799=Read"},
{"title": "Beyond Hard and Soft: Hybrid Context Compression for Balancing Local and\n  Global Information Retention", "author": "Huanxuan Liao and Wen Hu and Yao Xu and Shizhu He and Jun Zhao and Kang Liu", "abstract": "  Large Language Models (LLMs) encounter significant challenges in\nlong-sequence inference due to computational inefficiency and redundant\nprocessing, driving interest in context compression techniques. Existing\nmethods often rely on token importance to perform hard local compression or\nencode context into latent representations for soft global compression.\nHowever, the uneven distribution of textual content relevance and the diversity\nof demands for user instructions mean these approaches frequently lead to the\nloss of potentially valuable information. To address this, we propose\n$\\textbf{Hy}$brid $\\textbf{Co}$ntext $\\textbf{Co}$mpression (HyCo$_2$) for\nLLMs, which integrates both global and local perspectives to guide context\ncompression while retaining both the essential semantics and critical details\nfor task completion. Specifically, we employ a hybrid adapter to refine global\nsemantics with the global view, based on the observation that different\nadapters excel at different tasks. Then we incorporate a classification layer\nthat assigns a retention probability to each context token based on the local\nview, determining whether it should be retained or discarded. To foster a\nbalanced integration of global and local compression, we introduce auxiliary\nparaphrasing and completion pretraining before instruction tuning. This\npromotes a synergistic integration that emphasizes instruction-relevant\ninformation while preserving essential local details, ultimately balancing\nlocal and global information retention in context compression. Experiments show\nthat our HyCo$_2$ method significantly enhances long-text reasoning while\nreducing token usage. It improves the performance of various LLM series by an\naverage of 13.1\\% across seven knowledge-intensive QA benchmarks. Moreover,\nHyCo$_2$ matches the performance of uncompressed methods while reducing token\nconsumption by 88.8\\%.\n", "link": "http://arxiv.org/abs/2505.15774v1", "date": "2025-05-21", "relevancy": 2.5496, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5144}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5144}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Hard%20and%20Soft%3A%20Hybrid%20Context%20Compression%20for%20Balancing%20Local%20and%0A%20%20Global%20Information%20Retention&body=Title%3A%20Beyond%20Hard%20and%20Soft%3A%20Hybrid%20Context%20Compression%20for%20Balancing%20Local%20and%0A%20%20Global%20Information%20Retention%0AAuthor%3A%20Huanxuan%20Liao%20and%20Wen%20Hu%20and%20Yao%20Xu%20and%20Shizhu%20He%20and%20Jun%20Zhao%20and%20Kang%20Liu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20encounter%20significant%20challenges%20in%0Along-sequence%20inference%20due%20to%20computational%20inefficiency%20and%20redundant%0Aprocessing%2C%20driving%20interest%20in%20context%20compression%20techniques.%20Existing%0Amethods%20often%20rely%20on%20token%20importance%20to%20perform%20hard%20local%20compression%20or%0Aencode%20context%20into%20latent%20representations%20for%20soft%20global%20compression.%0AHowever%2C%20the%20uneven%20distribution%20of%20textual%20content%20relevance%20and%20the%20diversity%0Aof%20demands%20for%20user%20instructions%20mean%20these%20approaches%20frequently%20lead%20to%20the%0Aloss%20of%20potentially%20valuable%20information.%20To%20address%20this%2C%20we%20propose%0A%24%5Ctextbf%7BHy%7D%24brid%20%24%5Ctextbf%7BCo%7D%24ntext%20%24%5Ctextbf%7BCo%7D%24mpression%20%28HyCo%24_2%24%29%20for%0ALLMs%2C%20which%20integrates%20both%20global%20and%20local%20perspectives%20to%20guide%20context%0Acompression%20while%20retaining%20both%20the%20essential%20semantics%20and%20critical%20details%0Afor%20task%20completion.%20Specifically%2C%20we%20employ%20a%20hybrid%20adapter%20to%20refine%20global%0Asemantics%20with%20the%20global%20view%2C%20based%20on%20the%20observation%20that%20different%0Aadapters%20excel%20at%20different%20tasks.%20Then%20we%20incorporate%20a%20classification%20layer%0Athat%20assigns%20a%20retention%20probability%20to%20each%20context%20token%20based%20on%20the%20local%0Aview%2C%20determining%20whether%20it%20should%20be%20retained%20or%20discarded.%20To%20foster%20a%0Abalanced%20integration%20of%20global%20and%20local%20compression%2C%20we%20introduce%20auxiliary%0Aparaphrasing%20and%20completion%20pretraining%20before%20instruction%20tuning.%20This%0Apromotes%20a%20synergistic%20integration%20that%20emphasizes%20instruction-relevant%0Ainformation%20while%20preserving%20essential%20local%20details%2C%20ultimately%20balancing%0Alocal%20and%20global%20information%20retention%20in%20context%20compression.%20Experiments%20show%0Athat%20our%20HyCo%24_2%24%20method%20significantly%20enhances%20long-text%20reasoning%20while%0Areducing%20token%20usage.%20It%20improves%20the%20performance%20of%20various%20LLM%20series%20by%20an%0Aaverage%20of%2013.1%5C%25%20across%20seven%20knowledge-intensive%20QA%20benchmarks.%20Moreover%2C%0AHyCo%24_2%24%20matches%20the%20performance%20of%20uncompressed%20methods%20while%20reducing%20token%0Aconsumption%20by%2088.8%5C%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15774v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Hard%2520and%2520Soft%253A%2520Hybrid%2520Context%2520Compression%2520for%2520Balancing%2520Local%2520and%250A%2520%2520Global%2520Information%2520Retention%26entry.906535625%3DHuanxuan%2520Liao%2520and%2520Wen%2520Hu%2520and%2520Yao%2520Xu%2520and%2520Shizhu%2520He%2520and%2520Jun%2520Zhao%2520and%2520Kang%2520Liu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520encounter%2520significant%2520challenges%2520in%250Along-sequence%2520inference%2520due%2520to%2520computational%2520inefficiency%2520and%2520redundant%250Aprocessing%252C%2520driving%2520interest%2520in%2520context%2520compression%2520techniques.%2520Existing%250Amethods%2520often%2520rely%2520on%2520token%2520importance%2520to%2520perform%2520hard%2520local%2520compression%2520or%250Aencode%2520context%2520into%2520latent%2520representations%2520for%2520soft%2520global%2520compression.%250AHowever%252C%2520the%2520uneven%2520distribution%2520of%2520textual%2520content%2520relevance%2520and%2520the%2520diversity%250Aof%2520demands%2520for%2520user%2520instructions%2520mean%2520these%2520approaches%2520frequently%2520lead%2520to%2520the%250Aloss%2520of%2520potentially%2520valuable%2520information.%2520To%2520address%2520this%252C%2520we%2520propose%250A%2524%255Ctextbf%257BHy%257D%2524brid%2520%2524%255Ctextbf%257BCo%257D%2524ntext%2520%2524%255Ctextbf%257BCo%257D%2524mpression%2520%2528HyCo%2524_2%2524%2529%2520for%250ALLMs%252C%2520which%2520integrates%2520both%2520global%2520and%2520local%2520perspectives%2520to%2520guide%2520context%250Acompression%2520while%2520retaining%2520both%2520the%2520essential%2520semantics%2520and%2520critical%2520details%250Afor%2520task%2520completion.%2520Specifically%252C%2520we%2520employ%2520a%2520hybrid%2520adapter%2520to%2520refine%2520global%250Asemantics%2520with%2520the%2520global%2520view%252C%2520based%2520on%2520the%2520observation%2520that%2520different%250Aadapters%2520excel%2520at%2520different%2520tasks.%2520Then%2520we%2520incorporate%2520a%2520classification%2520layer%250Athat%2520assigns%2520a%2520retention%2520probability%2520to%2520each%2520context%2520token%2520based%2520on%2520the%2520local%250Aview%252C%2520determining%2520whether%2520it%2520should%2520be%2520retained%2520or%2520discarded.%2520To%2520foster%2520a%250Abalanced%2520integration%2520of%2520global%2520and%2520local%2520compression%252C%2520we%2520introduce%2520auxiliary%250Aparaphrasing%2520and%2520completion%2520pretraining%2520before%2520instruction%2520tuning.%2520This%250Apromotes%2520a%2520synergistic%2520integration%2520that%2520emphasizes%2520instruction-relevant%250Ainformation%2520while%2520preserving%2520essential%2520local%2520details%252C%2520ultimately%2520balancing%250Alocal%2520and%2520global%2520information%2520retention%2520in%2520context%2520compression.%2520Experiments%2520show%250Athat%2520our%2520HyCo%2524_2%2524%2520method%2520significantly%2520enhances%2520long-text%2520reasoning%2520while%250Areducing%2520token%2520usage.%2520It%2520improves%2520the%2520performance%2520of%2520various%2520LLM%2520series%2520by%2520an%250Aaverage%2520of%252013.1%255C%2525%2520across%2520seven%2520knowledge-intensive%2520QA%2520benchmarks.%2520Moreover%252C%250AHyCo%2524_2%2524%2520matches%2520the%2520performance%2520of%2520uncompressed%2520methods%2520while%2520reducing%2520token%250Aconsumption%2520by%252088.8%255C%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15774v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Hard%20and%20Soft%3A%20Hybrid%20Context%20Compression%20for%20Balancing%20Local%20and%0A%20%20Global%20Information%20Retention&entry.906535625=Huanxuan%20Liao%20and%20Wen%20Hu%20and%20Yao%20Xu%20and%20Shizhu%20He%20and%20Jun%20Zhao%20and%20Kang%20Liu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20encounter%20significant%20challenges%20in%0Along-sequence%20inference%20due%20to%20computational%20inefficiency%20and%20redundant%0Aprocessing%2C%20driving%20interest%20in%20context%20compression%20techniques.%20Existing%0Amethods%20often%20rely%20on%20token%20importance%20to%20perform%20hard%20local%20compression%20or%0Aencode%20context%20into%20latent%20representations%20for%20soft%20global%20compression.%0AHowever%2C%20the%20uneven%20distribution%20of%20textual%20content%20relevance%20and%20the%20diversity%0Aof%20demands%20for%20user%20instructions%20mean%20these%20approaches%20frequently%20lead%20to%20the%0Aloss%20of%20potentially%20valuable%20information.%20To%20address%20this%2C%20we%20propose%0A%24%5Ctextbf%7BHy%7D%24brid%20%24%5Ctextbf%7BCo%7D%24ntext%20%24%5Ctextbf%7BCo%7D%24mpression%20%28HyCo%24_2%24%29%20for%0ALLMs%2C%20which%20integrates%20both%20global%20and%20local%20perspectives%20to%20guide%20context%0Acompression%20while%20retaining%20both%20the%20essential%20semantics%20and%20critical%20details%0Afor%20task%20completion.%20Specifically%2C%20we%20employ%20a%20hybrid%20adapter%20to%20refine%20global%0Asemantics%20with%20the%20global%20view%2C%20based%20on%20the%20observation%20that%20different%0Aadapters%20excel%20at%20different%20tasks.%20Then%20we%20incorporate%20a%20classification%20layer%0Athat%20assigns%20a%20retention%20probability%20to%20each%20context%20token%20based%20on%20the%20local%0Aview%2C%20determining%20whether%20it%20should%20be%20retained%20or%20discarded.%20To%20foster%20a%0Abalanced%20integration%20of%20global%20and%20local%20compression%2C%20we%20introduce%20auxiliary%0Aparaphrasing%20and%20completion%20pretraining%20before%20instruction%20tuning.%20This%0Apromotes%20a%20synergistic%20integration%20that%20emphasizes%20instruction-relevant%0Ainformation%20while%20preserving%20essential%20local%20details%2C%20ultimately%20balancing%0Alocal%20and%20global%20information%20retention%20in%20context%20compression.%20Experiments%20show%0Athat%20our%20HyCo%24_2%24%20method%20significantly%20enhances%20long-text%20reasoning%20while%0Areducing%20token%20usage.%20It%20improves%20the%20performance%20of%20various%20LLM%20series%20by%20an%0Aaverage%20of%2013.1%5C%25%20across%20seven%20knowledge-intensive%20QA%20benchmarks.%20Moreover%2C%0AHyCo%24_2%24%20matches%20the%20performance%20of%20uncompressed%20methods%20while%20reducing%20token%0Aconsumption%20by%2088.8%5C%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15774v1&entry.124074799=Read"},
{"title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning", "author": "Haolei Xu and Yuchen Yan and Yongliang Shen and Wenqi Zhang and Guiyang Hou and Shengpei Jiang and Kaitao Song and Weiming Lu and Jun Xiao and Yueting Zhuang", "abstract": "  Large language models (LLMs) have achieved remarkable progress on\nmathematical tasks through Chain-of-Thought (CoT) reasoning. However, existing\nmathematical CoT datasets often suffer from Thought Leaps due to experts\nomitting intermediate steps, which negatively impacts model learning and\ngeneralization. We propose the CoT Thought Leap Bridge Task, which aims to\nautomatically detect leaps and generate missing intermediate reasoning steps to\nrestore the completeness and coherence of CoT. To facilitate this, we\nconstructed a specialized training dataset called ScaleQM+, based on the\nstructured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought\nleaps. Through comprehensive experiments on mathematical reasoning benchmarks,\nwe demonstrate that models fine-tuned on bridged datasets consistently\noutperform those trained on original datasets, with improvements of up to\n+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)\nand provides better starting points for reinforcement learning (+3.1%),\nfunctioning as a plug-and-play module compatible with existing optimization\ntechniques. Furthermore, CoT-Bridge demonstrate improved generalization to\nout-of-domain logical reasoning tasks, confirming that enhancing reasoning\ncompleteness yields broadly applicable benefits.\n", "link": "http://arxiv.org/abs/2505.14684v2", "date": "2025-05-21", "relevancy": 2.5409, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5145}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.505}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mind%20the%20Gap%3A%20Bridging%20Thought%20Leap%20for%20Improved%20Chain-of-Thought%20Tuning&body=Title%3A%20Mind%20the%20Gap%3A%20Bridging%20Thought%20Leap%20for%20Improved%20Chain-of-Thought%20Tuning%0AAuthor%3A%20Haolei%20Xu%20and%20Yuchen%20Yan%20and%20Yongliang%20Shen%20and%20Wenqi%20Zhang%20and%20Guiyang%20Hou%20and%20Shengpei%20Jiang%20and%20Kaitao%20Song%20and%20Weiming%20Lu%20and%20Jun%20Xiao%20and%20Yueting%20Zhuang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20progress%20on%0Amathematical%20tasks%20through%20Chain-of-Thought%20%28CoT%29%20reasoning.%20However%2C%20existing%0Amathematical%20CoT%20datasets%20often%20suffer%20from%20Thought%20Leaps%20due%20to%20experts%0Aomitting%20intermediate%20steps%2C%20which%20negatively%20impacts%20model%20learning%20and%0Ageneralization.%20We%20propose%20the%20CoT%20Thought%20Leap%20Bridge%20Task%2C%20which%20aims%20to%0Aautomatically%20detect%20leaps%20and%20generate%20missing%20intermediate%20reasoning%20steps%20to%0Arestore%20the%20completeness%20and%20coherence%20of%20CoT.%20To%20facilitate%20this%2C%20we%0Aconstructed%20a%20specialized%20training%20dataset%20called%20ScaleQM%2B%2C%20based%20on%20the%0Astructured%20ScaleQuestMath%20dataset%2C%20and%20trained%20CoT-Bridge%20to%20bridge%20thought%0Aleaps.%20Through%20comprehensive%20experiments%20on%20mathematical%20reasoning%20benchmarks%2C%0Awe%20demonstrate%20that%20models%20fine-tuned%20on%20bridged%20datasets%20consistently%0Aoutperform%20those%20trained%20on%20original%20datasets%2C%20with%20improvements%20of%20up%20to%0A%2B5.87%25%20on%20NuminaMath.%20Our%20approach%20effectively%20enhances%20distilled%20data%20%28%2B3.02%25%29%0Aand%20provides%20better%20starting%20points%20for%20reinforcement%20learning%20%28%2B3.1%25%29%2C%0Afunctioning%20as%20a%20plug-and-play%20module%20compatible%20with%20existing%20optimization%0Atechniques.%20Furthermore%2C%20CoT-Bridge%20demonstrate%20improved%20generalization%20to%0Aout-of-domain%20logical%20reasoning%20tasks%2C%20confirming%20that%20enhancing%20reasoning%0Acompleteness%20yields%20broadly%20applicable%20benefits.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14684v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMind%2520the%2520Gap%253A%2520Bridging%2520Thought%2520Leap%2520for%2520Improved%2520Chain-of-Thought%2520Tuning%26entry.906535625%3DHaolei%2520Xu%2520and%2520Yuchen%2520Yan%2520and%2520Yongliang%2520Shen%2520and%2520Wenqi%2520Zhang%2520and%2520Guiyang%2520Hou%2520and%2520Shengpei%2520Jiang%2520and%2520Kaitao%2520Song%2520and%2520Weiming%2520Lu%2520and%2520Jun%2520Xiao%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520achieved%2520remarkable%2520progress%2520on%250Amathematical%2520tasks%2520through%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning.%2520However%252C%2520existing%250Amathematical%2520CoT%2520datasets%2520often%2520suffer%2520from%2520Thought%2520Leaps%2520due%2520to%2520experts%250Aomitting%2520intermediate%2520steps%252C%2520which%2520negatively%2520impacts%2520model%2520learning%2520and%250Ageneralization.%2520We%2520propose%2520the%2520CoT%2520Thought%2520Leap%2520Bridge%2520Task%252C%2520which%2520aims%2520to%250Aautomatically%2520detect%2520leaps%2520and%2520generate%2520missing%2520intermediate%2520reasoning%2520steps%2520to%250Arestore%2520the%2520completeness%2520and%2520coherence%2520of%2520CoT.%2520To%2520facilitate%2520this%252C%2520we%250Aconstructed%2520a%2520specialized%2520training%2520dataset%2520called%2520ScaleQM%252B%252C%2520based%2520on%2520the%250Astructured%2520ScaleQuestMath%2520dataset%252C%2520and%2520trained%2520CoT-Bridge%2520to%2520bridge%2520thought%250Aleaps.%2520Through%2520comprehensive%2520experiments%2520on%2520mathematical%2520reasoning%2520benchmarks%252C%250Awe%2520demonstrate%2520that%2520models%2520fine-tuned%2520on%2520bridged%2520datasets%2520consistently%250Aoutperform%2520those%2520trained%2520on%2520original%2520datasets%252C%2520with%2520improvements%2520of%2520up%2520to%250A%252B5.87%2525%2520on%2520NuminaMath.%2520Our%2520approach%2520effectively%2520enhances%2520distilled%2520data%2520%2528%252B3.02%2525%2529%250Aand%2520provides%2520better%2520starting%2520points%2520for%2520reinforcement%2520learning%2520%2528%252B3.1%2525%2529%252C%250Afunctioning%2520as%2520a%2520plug-and-play%2520module%2520compatible%2520with%2520existing%2520optimization%250Atechniques.%2520Furthermore%252C%2520CoT-Bridge%2520demonstrate%2520improved%2520generalization%2520to%250Aout-of-domain%2520logical%2520reasoning%2520tasks%252C%2520confirming%2520that%2520enhancing%2520reasoning%250Acompleteness%2520yields%2520broadly%2520applicable%2520benefits.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14684v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mind%20the%20Gap%3A%20Bridging%20Thought%20Leap%20for%20Improved%20Chain-of-Thought%20Tuning&entry.906535625=Haolei%20Xu%20and%20Yuchen%20Yan%20and%20Yongliang%20Shen%20and%20Wenqi%20Zhang%20and%20Guiyang%20Hou%20and%20Shengpei%20Jiang%20and%20Kaitao%20Song%20and%20Weiming%20Lu%20and%20Jun%20Xiao%20and%20Yueting%20Zhuang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20progress%20on%0Amathematical%20tasks%20through%20Chain-of-Thought%20%28CoT%29%20reasoning.%20However%2C%20existing%0Amathematical%20CoT%20datasets%20often%20suffer%20from%20Thought%20Leaps%20due%20to%20experts%0Aomitting%20intermediate%20steps%2C%20which%20negatively%20impacts%20model%20learning%20and%0Ageneralization.%20We%20propose%20the%20CoT%20Thought%20Leap%20Bridge%20Task%2C%20which%20aims%20to%0Aautomatically%20detect%20leaps%20and%20generate%20missing%20intermediate%20reasoning%20steps%20to%0Arestore%20the%20completeness%20and%20coherence%20of%20CoT.%20To%20facilitate%20this%2C%20we%0Aconstructed%20a%20specialized%20training%20dataset%20called%20ScaleQM%2B%2C%20based%20on%20the%0Astructured%20ScaleQuestMath%20dataset%2C%20and%20trained%20CoT-Bridge%20to%20bridge%20thought%0Aleaps.%20Through%20comprehensive%20experiments%20on%20mathematical%20reasoning%20benchmarks%2C%0Awe%20demonstrate%20that%20models%20fine-tuned%20on%20bridged%20datasets%20consistently%0Aoutperform%20those%20trained%20on%20original%20datasets%2C%20with%20improvements%20of%20up%20to%0A%2B5.87%25%20on%20NuminaMath.%20Our%20approach%20effectively%20enhances%20distilled%20data%20%28%2B3.02%25%29%0Aand%20provides%20better%20starting%20points%20for%20reinforcement%20learning%20%28%2B3.1%25%29%2C%0Afunctioning%20as%20a%20plug-and-play%20module%20compatible%20with%20existing%20optimization%0Atechniques.%20Furthermore%2C%20CoT-Bridge%20demonstrate%20improved%20generalization%20to%0Aout-of-domain%20logical%20reasoning%20tasks%2C%20confirming%20that%20enhancing%20reasoning%0Acompleteness%20yields%20broadly%20applicable%20benefits.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14684v2&entry.124074799=Read"},
{"title": "BiSSL: Enhancing the Alignment Between Self-Supervised Pretraining and\n  Downstream Fine-Tuning via Bilevel Optimization", "author": "Gustav Wagner Zakarias and Lars Kai Hansen and Zheng-Hua Tan", "abstract": "  Models initialized from self-supervised pretraining may suffer from poor\nalignment with downstream tasks, reducing the extent to which subsequent\nfine-tuning can adapt pretrained features toward downstream objectives. To\nmitigate this, we introduce BiSSL, a novel bilevel training framework that\nenhances the alignment of self-supervised pretrained models with downstream\ntasks prior to fine-tuning. BiSSL acts as an intermediate training stage\nconducted after conventional self-supervised pretraining and is tasked with\nsolving a bilevel optimization problem that incorporates the pretext and\ndownstream training objectives in its lower- and upper-level objectives,\nrespectively. This approach explicitly models the interdependence between the\npretraining and fine-tuning stages within the conventional self-supervised\nlearning pipeline, facilitating enhanced information sharing between them that\nultimately leads to a model initialization better aligned with the downstream\ntask. We propose a general training algorithm for BiSSL that is compatible with\na broad range of pretext and downstream tasks. Using SimCLR and Bootstrap Your\nOwn Latent to pretrain ResNet-50 backbones on the ImageNet dataset, we\ndemonstrate that our proposed framework significantly improves accuracy on the\nvast majority of 12 downstream image classification datasets, as well as on\nobject detection. Exploratory analyses alongside investigative experiments\nfurther provide compelling evidence that BiSSL enhances downstream alignment.\n", "link": "http://arxiv.org/abs/2410.02387v4", "date": "2025-05-21", "relevancy": 2.5369, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5162}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5061}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BiSSL%3A%20Enhancing%20the%20Alignment%20Between%20Self-Supervised%20Pretraining%20and%0A%20%20Downstream%20Fine-Tuning%20via%20Bilevel%20Optimization&body=Title%3A%20BiSSL%3A%20Enhancing%20the%20Alignment%20Between%20Self-Supervised%20Pretraining%20and%0A%20%20Downstream%20Fine-Tuning%20via%20Bilevel%20Optimization%0AAuthor%3A%20Gustav%20Wagner%20Zakarias%20and%20Lars%20Kai%20Hansen%20and%20Zheng-Hua%20Tan%0AAbstract%3A%20%20%20Models%20initialized%20from%20self-supervised%20pretraining%20may%20suffer%20from%20poor%0Aalignment%20with%20downstream%20tasks%2C%20reducing%20the%20extent%20to%20which%20subsequent%0Afine-tuning%20can%20adapt%20pretrained%20features%20toward%20downstream%20objectives.%20To%0Amitigate%20this%2C%20we%20introduce%20BiSSL%2C%20a%20novel%20bilevel%20training%20framework%20that%0Aenhances%20the%20alignment%20of%20self-supervised%20pretrained%20models%20with%20downstream%0Atasks%20prior%20to%20fine-tuning.%20BiSSL%20acts%20as%20an%20intermediate%20training%20stage%0Aconducted%20after%20conventional%20self-supervised%20pretraining%20and%20is%20tasked%20with%0Asolving%20a%20bilevel%20optimization%20problem%20that%20incorporates%20the%20pretext%20and%0Adownstream%20training%20objectives%20in%20its%20lower-%20and%20upper-level%20objectives%2C%0Arespectively.%20This%20approach%20explicitly%20models%20the%20interdependence%20between%20the%0Apretraining%20and%20fine-tuning%20stages%20within%20the%20conventional%20self-supervised%0Alearning%20pipeline%2C%20facilitating%20enhanced%20information%20sharing%20between%20them%20that%0Aultimately%20leads%20to%20a%20model%20initialization%20better%20aligned%20with%20the%20downstream%0Atask.%20We%20propose%20a%20general%20training%20algorithm%20for%20BiSSL%20that%20is%20compatible%20with%0Aa%20broad%20range%20of%20pretext%20and%20downstream%20tasks.%20Using%20SimCLR%20and%20Bootstrap%20Your%0AOwn%20Latent%20to%20pretrain%20ResNet-50%20backbones%20on%20the%20ImageNet%20dataset%2C%20we%0Ademonstrate%20that%20our%20proposed%20framework%20significantly%20improves%20accuracy%20on%20the%0Avast%20majority%20of%2012%20downstream%20image%20classification%20datasets%2C%20as%20well%20as%20on%0Aobject%20detection.%20Exploratory%20analyses%20alongside%20investigative%20experiments%0Afurther%20provide%20compelling%20evidence%20that%20BiSSL%20enhances%20downstream%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02387v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiSSL%253A%2520Enhancing%2520the%2520Alignment%2520Between%2520Self-Supervised%2520Pretraining%2520and%250A%2520%2520Downstream%2520Fine-Tuning%2520via%2520Bilevel%2520Optimization%26entry.906535625%3DGustav%2520Wagner%2520Zakarias%2520and%2520Lars%2520Kai%2520Hansen%2520and%2520Zheng-Hua%2520Tan%26entry.1292438233%3D%2520%2520Models%2520initialized%2520from%2520self-supervised%2520pretraining%2520may%2520suffer%2520from%2520poor%250Aalignment%2520with%2520downstream%2520tasks%252C%2520reducing%2520the%2520extent%2520to%2520which%2520subsequent%250Afine-tuning%2520can%2520adapt%2520pretrained%2520features%2520toward%2520downstream%2520objectives.%2520To%250Amitigate%2520this%252C%2520we%2520introduce%2520BiSSL%252C%2520a%2520novel%2520bilevel%2520training%2520framework%2520that%250Aenhances%2520the%2520alignment%2520of%2520self-supervised%2520pretrained%2520models%2520with%2520downstream%250Atasks%2520prior%2520to%2520fine-tuning.%2520BiSSL%2520acts%2520as%2520an%2520intermediate%2520training%2520stage%250Aconducted%2520after%2520conventional%2520self-supervised%2520pretraining%2520and%2520is%2520tasked%2520with%250Asolving%2520a%2520bilevel%2520optimization%2520problem%2520that%2520incorporates%2520the%2520pretext%2520and%250Adownstream%2520training%2520objectives%2520in%2520its%2520lower-%2520and%2520upper-level%2520objectives%252C%250Arespectively.%2520This%2520approach%2520explicitly%2520models%2520the%2520interdependence%2520between%2520the%250Apretraining%2520and%2520fine-tuning%2520stages%2520within%2520the%2520conventional%2520self-supervised%250Alearning%2520pipeline%252C%2520facilitating%2520enhanced%2520information%2520sharing%2520between%2520them%2520that%250Aultimately%2520leads%2520to%2520a%2520model%2520initialization%2520better%2520aligned%2520with%2520the%2520downstream%250Atask.%2520We%2520propose%2520a%2520general%2520training%2520algorithm%2520for%2520BiSSL%2520that%2520is%2520compatible%2520with%250Aa%2520broad%2520range%2520of%2520pretext%2520and%2520downstream%2520tasks.%2520Using%2520SimCLR%2520and%2520Bootstrap%2520Your%250AOwn%2520Latent%2520to%2520pretrain%2520ResNet-50%2520backbones%2520on%2520the%2520ImageNet%2520dataset%252C%2520we%250Ademonstrate%2520that%2520our%2520proposed%2520framework%2520significantly%2520improves%2520accuracy%2520on%2520the%250Avast%2520majority%2520of%252012%2520downstream%2520image%2520classification%2520datasets%252C%2520as%2520well%2520as%2520on%250Aobject%2520detection.%2520Exploratory%2520analyses%2520alongside%2520investigative%2520experiments%250Afurther%2520provide%2520compelling%2520evidence%2520that%2520BiSSL%2520enhances%2520downstream%2520alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02387v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BiSSL%3A%20Enhancing%20the%20Alignment%20Between%20Self-Supervised%20Pretraining%20and%0A%20%20Downstream%20Fine-Tuning%20via%20Bilevel%20Optimization&entry.906535625=Gustav%20Wagner%20Zakarias%20and%20Lars%20Kai%20Hansen%20and%20Zheng-Hua%20Tan&entry.1292438233=%20%20Models%20initialized%20from%20self-supervised%20pretraining%20may%20suffer%20from%20poor%0Aalignment%20with%20downstream%20tasks%2C%20reducing%20the%20extent%20to%20which%20subsequent%0Afine-tuning%20can%20adapt%20pretrained%20features%20toward%20downstream%20objectives.%20To%0Amitigate%20this%2C%20we%20introduce%20BiSSL%2C%20a%20novel%20bilevel%20training%20framework%20that%0Aenhances%20the%20alignment%20of%20self-supervised%20pretrained%20models%20with%20downstream%0Atasks%20prior%20to%20fine-tuning.%20BiSSL%20acts%20as%20an%20intermediate%20training%20stage%0Aconducted%20after%20conventional%20self-supervised%20pretraining%20and%20is%20tasked%20with%0Asolving%20a%20bilevel%20optimization%20problem%20that%20incorporates%20the%20pretext%20and%0Adownstream%20training%20objectives%20in%20its%20lower-%20and%20upper-level%20objectives%2C%0Arespectively.%20This%20approach%20explicitly%20models%20the%20interdependence%20between%20the%0Apretraining%20and%20fine-tuning%20stages%20within%20the%20conventional%20self-supervised%0Alearning%20pipeline%2C%20facilitating%20enhanced%20information%20sharing%20between%20them%20that%0Aultimately%20leads%20to%20a%20model%20initialization%20better%20aligned%20with%20the%20downstream%0Atask.%20We%20propose%20a%20general%20training%20algorithm%20for%20BiSSL%20that%20is%20compatible%20with%0Aa%20broad%20range%20of%20pretext%20and%20downstream%20tasks.%20Using%20SimCLR%20and%20Bootstrap%20Your%0AOwn%20Latent%20to%20pretrain%20ResNet-50%20backbones%20on%20the%20ImageNet%20dataset%2C%20we%0Ademonstrate%20that%20our%20proposed%20framework%20significantly%20improves%20accuracy%20on%20the%0Avast%20majority%20of%2012%20downstream%20image%20classification%20datasets%2C%20as%20well%20as%20on%0Aobject%20detection.%20Exploratory%20analyses%20alongside%20investigative%20experiments%0Afurther%20provide%20compelling%20evidence%20that%20BiSSL%20enhances%20downstream%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02387v4&entry.124074799=Read"},
{"title": "ClickSight: Interpreting Student Clickstreams to Reveal Insights on\n  Learning Strategies via LLMs", "author": "Bahar Radmehr and Ekaterina Shved and Fatma Bet\u00fcl G\u00fcre\u015f and Adish Singla and Tanja K\u00e4ser", "abstract": "  Clickstream data from digital learning environments offer valuable insights\ninto students' learning behaviors, but are challenging to interpret due to\ntheir high dimensionality and granularity. Prior approaches have relied mainly\non handcrafted features, expert labeling, clustering, or supervised models,\ntherefore often lacking generalizability and scalability. In this work, we\nintroduce ClickSight, an in-context Large Language Model (LLM)-based pipeline\nthat interprets student clickstreams to reveal their learning strategies.\nClickSight takes raw clickstreams and a list of learning strategies as input\nand generates textual interpretations of students' behaviors during\ninteraction. We evaluate four different prompting strategies and investigate\nthe impact of self-refinement on interpretation quality. Our evaluation spans\ntwo open-ended learning environments and uses a rubric-based domain-expert\nevaluation. Results show that while LLMs can reasonably interpret learning\nstrategies from clickstreams, interpretation quality varies by prompting\nstrategy, and self-refinement offers limited improvement. ClickSight\ndemonstrates the potential of LLMs to generate theory-driven insights from\neducational interaction data.\n", "link": "http://arxiv.org/abs/2505.15410v1", "date": "2025-05-21", "relevancy": 2.5344, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5149}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5149}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ClickSight%3A%20Interpreting%20Student%20Clickstreams%20to%20Reveal%20Insights%20on%0A%20%20Learning%20Strategies%20via%20LLMs&body=Title%3A%20ClickSight%3A%20Interpreting%20Student%20Clickstreams%20to%20Reveal%20Insights%20on%0A%20%20Learning%20Strategies%20via%20LLMs%0AAuthor%3A%20Bahar%20Radmehr%20and%20Ekaterina%20Shved%20and%20Fatma%20Bet%C3%BCl%20G%C3%BCre%C5%9F%20and%20Adish%20Singla%20and%20Tanja%20K%C3%A4ser%0AAbstract%3A%20%20%20Clickstream%20data%20from%20digital%20learning%20environments%20offer%20valuable%20insights%0Ainto%20students%27%20learning%20behaviors%2C%20but%20are%20challenging%20to%20interpret%20due%20to%0Atheir%20high%20dimensionality%20and%20granularity.%20Prior%20approaches%20have%20relied%20mainly%0Aon%20handcrafted%20features%2C%20expert%20labeling%2C%20clustering%2C%20or%20supervised%20models%2C%0Atherefore%20often%20lacking%20generalizability%20and%20scalability.%20In%20this%20work%2C%20we%0Aintroduce%20ClickSight%2C%20an%20in-context%20Large%20Language%20Model%20%28LLM%29-based%20pipeline%0Athat%20interprets%20student%20clickstreams%20to%20reveal%20their%20learning%20strategies.%0AClickSight%20takes%20raw%20clickstreams%20and%20a%20list%20of%20learning%20strategies%20as%20input%0Aand%20generates%20textual%20interpretations%20of%20students%27%20behaviors%20during%0Ainteraction.%20We%20evaluate%20four%20different%20prompting%20strategies%20and%20investigate%0Athe%20impact%20of%20self-refinement%20on%20interpretation%20quality.%20Our%20evaluation%20spans%0Atwo%20open-ended%20learning%20environments%20and%20uses%20a%20rubric-based%20domain-expert%0Aevaluation.%20Results%20show%20that%20while%20LLMs%20can%20reasonably%20interpret%20learning%0Astrategies%20from%20clickstreams%2C%20interpretation%20quality%20varies%20by%20prompting%0Astrategy%2C%20and%20self-refinement%20offers%20limited%20improvement.%20ClickSight%0Ademonstrates%20the%20potential%20of%20LLMs%20to%20generate%20theory-driven%20insights%20from%0Aeducational%20interaction%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15410v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClickSight%253A%2520Interpreting%2520Student%2520Clickstreams%2520to%2520Reveal%2520Insights%2520on%250A%2520%2520Learning%2520Strategies%2520via%2520LLMs%26entry.906535625%3DBahar%2520Radmehr%2520and%2520Ekaterina%2520Shved%2520and%2520Fatma%2520Bet%25C3%25BCl%2520G%25C3%25BCre%25C5%259F%2520and%2520Adish%2520Singla%2520and%2520Tanja%2520K%25C3%25A4ser%26entry.1292438233%3D%2520%2520Clickstream%2520data%2520from%2520digital%2520learning%2520environments%2520offer%2520valuable%2520insights%250Ainto%2520students%2527%2520learning%2520behaviors%252C%2520but%2520are%2520challenging%2520to%2520interpret%2520due%2520to%250Atheir%2520high%2520dimensionality%2520and%2520granularity.%2520Prior%2520approaches%2520have%2520relied%2520mainly%250Aon%2520handcrafted%2520features%252C%2520expert%2520labeling%252C%2520clustering%252C%2520or%2520supervised%2520models%252C%250Atherefore%2520often%2520lacking%2520generalizability%2520and%2520scalability.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520ClickSight%252C%2520an%2520in-context%2520Large%2520Language%2520Model%2520%2528LLM%2529-based%2520pipeline%250Athat%2520interprets%2520student%2520clickstreams%2520to%2520reveal%2520their%2520learning%2520strategies.%250AClickSight%2520takes%2520raw%2520clickstreams%2520and%2520a%2520list%2520of%2520learning%2520strategies%2520as%2520input%250Aand%2520generates%2520textual%2520interpretations%2520of%2520students%2527%2520behaviors%2520during%250Ainteraction.%2520We%2520evaluate%2520four%2520different%2520prompting%2520strategies%2520and%2520investigate%250Athe%2520impact%2520of%2520self-refinement%2520on%2520interpretation%2520quality.%2520Our%2520evaluation%2520spans%250Atwo%2520open-ended%2520learning%2520environments%2520and%2520uses%2520a%2520rubric-based%2520domain-expert%250Aevaluation.%2520Results%2520show%2520that%2520while%2520LLMs%2520can%2520reasonably%2520interpret%2520learning%250Astrategies%2520from%2520clickstreams%252C%2520interpretation%2520quality%2520varies%2520by%2520prompting%250Astrategy%252C%2520and%2520self-refinement%2520offers%2520limited%2520improvement.%2520ClickSight%250Ademonstrates%2520the%2520potential%2520of%2520LLMs%2520to%2520generate%2520theory-driven%2520insights%2520from%250Aeducational%2520interaction%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15410v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ClickSight%3A%20Interpreting%20Student%20Clickstreams%20to%20Reveal%20Insights%20on%0A%20%20Learning%20Strategies%20via%20LLMs&entry.906535625=Bahar%20Radmehr%20and%20Ekaterina%20Shved%20and%20Fatma%20Bet%C3%BCl%20G%C3%BCre%C5%9F%20and%20Adish%20Singla%20and%20Tanja%20K%C3%A4ser&entry.1292438233=%20%20Clickstream%20data%20from%20digital%20learning%20environments%20offer%20valuable%20insights%0Ainto%20students%27%20learning%20behaviors%2C%20but%20are%20challenging%20to%20interpret%20due%20to%0Atheir%20high%20dimensionality%20and%20granularity.%20Prior%20approaches%20have%20relied%20mainly%0Aon%20handcrafted%20features%2C%20expert%20labeling%2C%20clustering%2C%20or%20supervised%20models%2C%0Atherefore%20often%20lacking%20generalizability%20and%20scalability.%20In%20this%20work%2C%20we%0Aintroduce%20ClickSight%2C%20an%20in-context%20Large%20Language%20Model%20%28LLM%29-based%20pipeline%0Athat%20interprets%20student%20clickstreams%20to%20reveal%20their%20learning%20strategies.%0AClickSight%20takes%20raw%20clickstreams%20and%20a%20list%20of%20learning%20strategies%20as%20input%0Aand%20generates%20textual%20interpretations%20of%20students%27%20behaviors%20during%0Ainteraction.%20We%20evaluate%20four%20different%20prompting%20strategies%20and%20investigate%0Athe%20impact%20of%20self-refinement%20on%20interpretation%20quality.%20Our%20evaluation%20spans%0Atwo%20open-ended%20learning%20environments%20and%20uses%20a%20rubric-based%20domain-expert%0Aevaluation.%20Results%20show%20that%20while%20LLMs%20can%20reasonably%20interpret%20learning%0Astrategies%20from%20clickstreams%2C%20interpretation%20quality%20varies%20by%20prompting%0Astrategy%2C%20and%20self-refinement%20offers%20limited%20improvement.%20ClickSight%0Ademonstrates%20the%20potential%20of%20LLMs%20to%20generate%20theory-driven%20insights%20from%0Aeducational%20interaction%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15410v1&entry.124074799=Read"},
{"title": "Exploring The Visual Feature Space for Multimodal Neural Decoding", "author": "Weihao Xia and Cengiz Oztireli", "abstract": "  The intrication of brain signals drives research that leverages multimodal AI\nto align brain modalities with visual and textual data for explainable\ndescriptions. However, most existing studies are limited to coarse\ninterpretations, lacking essential details on object descriptions, locations,\nattributes, and their relationships. This leads to imprecise and ambiguous\nreconstructions when using such cues for visual decoding. To address this, we\nanalyze different choices of vision feature spaces from pre-trained visual\ncomponents within Multimodal Large Language Models (MLLMs) and introduce a\nzero-shot multimodal brain decoding method that interacts with these models to\ndecode across multiple levels of granularities. % To assess a model's ability\nto decode fine details from brain signals, we propose the Multi-Granularity\nBrain Detail Understanding Benchmark (MG-BrainDub). This benchmark includes two\nkey tasks: detailed descriptions and salient question-answering, with metrics\nhighlighting key visual elements like objects, attributes, and relationships.\nOur approach enhances neural decoding precision and supports more accurate\nneuro-decoding applications. Code will be available at\nhttps://github.com/weihaox/VINDEX.\n", "link": "http://arxiv.org/abs/2505.15755v1", "date": "2025-05-21", "relevancy": 2.5324, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6492}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6492}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20The%20Visual%20Feature%20Space%20for%20Multimodal%20Neural%20Decoding&body=Title%3A%20Exploring%20The%20Visual%20Feature%20Space%20for%20Multimodal%20Neural%20Decoding%0AAuthor%3A%20Weihao%20Xia%20and%20Cengiz%20Oztireli%0AAbstract%3A%20%20%20The%20intrication%20of%20brain%20signals%20drives%20research%20that%20leverages%20multimodal%20AI%0Ato%20align%20brain%20modalities%20with%20visual%20and%20textual%20data%20for%20explainable%0Adescriptions.%20However%2C%20most%20existing%20studies%20are%20limited%20to%20coarse%0Ainterpretations%2C%20lacking%20essential%20details%20on%20object%20descriptions%2C%20locations%2C%0Aattributes%2C%20and%20their%20relationships.%20This%20leads%20to%20imprecise%20and%20ambiguous%0Areconstructions%20when%20using%20such%20cues%20for%20visual%20decoding.%20To%20address%20this%2C%20we%0Aanalyze%20different%20choices%20of%20vision%20feature%20spaces%20from%20pre-trained%20visual%0Acomponents%20within%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20and%20introduce%20a%0Azero-shot%20multimodal%20brain%20decoding%20method%20that%20interacts%20with%20these%20models%20to%0Adecode%20across%20multiple%20levels%20of%20granularities.%20%25%20To%20assess%20a%20model%27s%20ability%0Ato%20decode%20fine%20details%20from%20brain%20signals%2C%20we%20propose%20the%20Multi-Granularity%0ABrain%20Detail%20Understanding%20Benchmark%20%28MG-BrainDub%29.%20This%20benchmark%20includes%20two%0Akey%20tasks%3A%20detailed%20descriptions%20and%20salient%20question-answering%2C%20with%20metrics%0Ahighlighting%20key%20visual%20elements%20like%20objects%2C%20attributes%2C%20and%20relationships.%0AOur%20approach%20enhances%20neural%20decoding%20precision%20and%20supports%20more%20accurate%0Aneuro-decoding%20applications.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/weihaox/VINDEX.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15755v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520The%2520Visual%2520Feature%2520Space%2520for%2520Multimodal%2520Neural%2520Decoding%26entry.906535625%3DWeihao%2520Xia%2520and%2520Cengiz%2520Oztireli%26entry.1292438233%3D%2520%2520The%2520intrication%2520of%2520brain%2520signals%2520drives%2520research%2520that%2520leverages%2520multimodal%2520AI%250Ato%2520align%2520brain%2520modalities%2520with%2520visual%2520and%2520textual%2520data%2520for%2520explainable%250Adescriptions.%2520However%252C%2520most%2520existing%2520studies%2520are%2520limited%2520to%2520coarse%250Ainterpretations%252C%2520lacking%2520essential%2520details%2520on%2520object%2520descriptions%252C%2520locations%252C%250Aattributes%252C%2520and%2520their%2520relationships.%2520This%2520leads%2520to%2520imprecise%2520and%2520ambiguous%250Areconstructions%2520when%2520using%2520such%2520cues%2520for%2520visual%2520decoding.%2520To%2520address%2520this%252C%2520we%250Aanalyze%2520different%2520choices%2520of%2520vision%2520feature%2520spaces%2520from%2520pre-trained%2520visual%250Acomponents%2520within%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520and%2520introduce%2520a%250Azero-shot%2520multimodal%2520brain%2520decoding%2520method%2520that%2520interacts%2520with%2520these%2520models%2520to%250Adecode%2520across%2520multiple%2520levels%2520of%2520granularities.%2520%2525%2520To%2520assess%2520a%2520model%2527s%2520ability%250Ato%2520decode%2520fine%2520details%2520from%2520brain%2520signals%252C%2520we%2520propose%2520the%2520Multi-Granularity%250ABrain%2520Detail%2520Understanding%2520Benchmark%2520%2528MG-BrainDub%2529.%2520This%2520benchmark%2520includes%2520two%250Akey%2520tasks%253A%2520detailed%2520descriptions%2520and%2520salient%2520question-answering%252C%2520with%2520metrics%250Ahighlighting%2520key%2520visual%2520elements%2520like%2520objects%252C%2520attributes%252C%2520and%2520relationships.%250AOur%2520approach%2520enhances%2520neural%2520decoding%2520precision%2520and%2520supports%2520more%2520accurate%250Aneuro-decoding%2520applications.%2520Code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/weihaox/VINDEX.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15755v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20The%20Visual%20Feature%20Space%20for%20Multimodal%20Neural%20Decoding&entry.906535625=Weihao%20Xia%20and%20Cengiz%20Oztireli&entry.1292438233=%20%20The%20intrication%20of%20brain%20signals%20drives%20research%20that%20leverages%20multimodal%20AI%0Ato%20align%20brain%20modalities%20with%20visual%20and%20textual%20data%20for%20explainable%0Adescriptions.%20However%2C%20most%20existing%20studies%20are%20limited%20to%20coarse%0Ainterpretations%2C%20lacking%20essential%20details%20on%20object%20descriptions%2C%20locations%2C%0Aattributes%2C%20and%20their%20relationships.%20This%20leads%20to%20imprecise%20and%20ambiguous%0Areconstructions%20when%20using%20such%20cues%20for%20visual%20decoding.%20To%20address%20this%2C%20we%0Aanalyze%20different%20choices%20of%20vision%20feature%20spaces%20from%20pre-trained%20visual%0Acomponents%20within%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20and%20introduce%20a%0Azero-shot%20multimodal%20brain%20decoding%20method%20that%20interacts%20with%20these%20models%20to%0Adecode%20across%20multiple%20levels%20of%20granularities.%20%25%20To%20assess%20a%20model%27s%20ability%0Ato%20decode%20fine%20details%20from%20brain%20signals%2C%20we%20propose%20the%20Multi-Granularity%0ABrain%20Detail%20Understanding%20Benchmark%20%28MG-BrainDub%29.%20This%20benchmark%20includes%20two%0Akey%20tasks%3A%20detailed%20descriptions%20and%20salient%20question-answering%2C%20with%20metrics%0Ahighlighting%20key%20visual%20elements%20like%20objects%2C%20attributes%2C%20and%20relationships.%0AOur%20approach%20enhances%20neural%20decoding%20precision%20and%20supports%20more%20accurate%0Aneuro-decoding%20applications.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/weihaox/VINDEX.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15755v1&entry.124074799=Read"},
{"title": "Bridging Sign and Spoken Languages: Pseudo Gloss Generation for Sign\n  Language Translation", "author": "Jianyuan Guo and Peike Li and Trevor Cohn", "abstract": "  Sign Language Translation (SLT) aims to map sign language videos to spoken\nlanguage text. A common approach relies on gloss annotations as an intermediate\nrepresentation, decomposing SLT into two sub-tasks: video-to-gloss recognition\nand gloss-to-text translation. While effective, this paradigm depends on\nexpert-annotated gloss labels, which are costly and rarely available in\nexisting datasets, limiting its scalability. To address this challenge, we\npropose a gloss-free pseudo gloss generation framework that eliminates the need\nfor human-annotated glosses while preserving the structured intermediate\nrepresentation. Specifically, we prompt a Large Language Model (LLM) with a few\nexample text-gloss pairs using in-context learning to produce draft sign\nglosses from spoken language text. To enhance the correspondence between\nLLM-generated pseudo glosses and the sign sequences in video, we correct the\nordering in the pseudo glosses for better alignment via a weakly supervised\nlearning process. This reordering facilitates the incorporation of auxiliary\nalignment objectives, and allows for the use of efficient supervision via a\nConnectionist Temporal Classification (CTC) loss. We train our SLT mode, which\nconsists of a vision encoder and a translator, through a three-stage pipeline,\nwhich progressively narrows the modality gap between sign language and spoken\nlanguage. Despite its simplicity, our approach outperforms previous\nstate-of-the-art gloss-free frameworks on two SLT benchmarks and achieves\ncompetitive results compared to gloss-based methods.\n", "link": "http://arxiv.org/abs/2505.15438v1", "date": "2025-05-21", "relevancy": 2.5271, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5215}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4974}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Sign%20and%20Spoken%20Languages%3A%20Pseudo%20Gloss%20Generation%20for%20Sign%0A%20%20Language%20Translation&body=Title%3A%20Bridging%20Sign%20and%20Spoken%20Languages%3A%20Pseudo%20Gloss%20Generation%20for%20Sign%0A%20%20Language%20Translation%0AAuthor%3A%20Jianyuan%20Guo%20and%20Peike%20Li%20and%20Trevor%20Cohn%0AAbstract%3A%20%20%20Sign%20Language%20Translation%20%28SLT%29%20aims%20to%20map%20sign%20language%20videos%20to%20spoken%0Alanguage%20text.%20A%20common%20approach%20relies%20on%20gloss%20annotations%20as%20an%20intermediate%0Arepresentation%2C%20decomposing%20SLT%20into%20two%20sub-tasks%3A%20video-to-gloss%20recognition%0Aand%20gloss-to-text%20translation.%20While%20effective%2C%20this%20paradigm%20depends%20on%0Aexpert-annotated%20gloss%20labels%2C%20which%20are%20costly%20and%20rarely%20available%20in%0Aexisting%20datasets%2C%20limiting%20its%20scalability.%20To%20address%20this%20challenge%2C%20we%0Apropose%20a%20gloss-free%20pseudo%20gloss%20generation%20framework%20that%20eliminates%20the%20need%0Afor%20human-annotated%20glosses%20while%20preserving%20the%20structured%20intermediate%0Arepresentation.%20Specifically%2C%20we%20prompt%20a%20Large%20Language%20Model%20%28LLM%29%20with%20a%20few%0Aexample%20text-gloss%20pairs%20using%20in-context%20learning%20to%20produce%20draft%20sign%0Aglosses%20from%20spoken%20language%20text.%20To%20enhance%20the%20correspondence%20between%0ALLM-generated%20pseudo%20glosses%20and%20the%20sign%20sequences%20in%20video%2C%20we%20correct%20the%0Aordering%20in%20the%20pseudo%20glosses%20for%20better%20alignment%20via%20a%20weakly%20supervised%0Alearning%20process.%20This%20reordering%20facilitates%20the%20incorporation%20of%20auxiliary%0Aalignment%20objectives%2C%20and%20allows%20for%20the%20use%20of%20efficient%20supervision%20via%20a%0AConnectionist%20Temporal%20Classification%20%28CTC%29%20loss.%20We%20train%20our%20SLT%20mode%2C%20which%0Aconsists%20of%20a%20vision%20encoder%20and%20a%20translator%2C%20through%20a%20three-stage%20pipeline%2C%0Awhich%20progressively%20narrows%20the%20modality%20gap%20between%20sign%20language%20and%20spoken%0Alanguage.%20Despite%20its%20simplicity%2C%20our%20approach%20outperforms%20previous%0Astate-of-the-art%20gloss-free%20frameworks%20on%20two%20SLT%20benchmarks%20and%20achieves%0Acompetitive%20results%20compared%20to%20gloss-based%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15438v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Sign%2520and%2520Spoken%2520Languages%253A%2520Pseudo%2520Gloss%2520Generation%2520for%2520Sign%250A%2520%2520Language%2520Translation%26entry.906535625%3DJianyuan%2520Guo%2520and%2520Peike%2520Li%2520and%2520Trevor%2520Cohn%26entry.1292438233%3D%2520%2520Sign%2520Language%2520Translation%2520%2528SLT%2529%2520aims%2520to%2520map%2520sign%2520language%2520videos%2520to%2520spoken%250Alanguage%2520text.%2520A%2520common%2520approach%2520relies%2520on%2520gloss%2520annotations%2520as%2520an%2520intermediate%250Arepresentation%252C%2520decomposing%2520SLT%2520into%2520two%2520sub-tasks%253A%2520video-to-gloss%2520recognition%250Aand%2520gloss-to-text%2520translation.%2520While%2520effective%252C%2520this%2520paradigm%2520depends%2520on%250Aexpert-annotated%2520gloss%2520labels%252C%2520which%2520are%2520costly%2520and%2520rarely%2520available%2520in%250Aexisting%2520datasets%252C%2520limiting%2520its%2520scalability.%2520To%2520address%2520this%2520challenge%252C%2520we%250Apropose%2520a%2520gloss-free%2520pseudo%2520gloss%2520generation%2520framework%2520that%2520eliminates%2520the%2520need%250Afor%2520human-annotated%2520glosses%2520while%2520preserving%2520the%2520structured%2520intermediate%250Arepresentation.%2520Specifically%252C%2520we%2520prompt%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520with%2520a%2520few%250Aexample%2520text-gloss%2520pairs%2520using%2520in-context%2520learning%2520to%2520produce%2520draft%2520sign%250Aglosses%2520from%2520spoken%2520language%2520text.%2520To%2520enhance%2520the%2520correspondence%2520between%250ALLM-generated%2520pseudo%2520glosses%2520and%2520the%2520sign%2520sequences%2520in%2520video%252C%2520we%2520correct%2520the%250Aordering%2520in%2520the%2520pseudo%2520glosses%2520for%2520better%2520alignment%2520via%2520a%2520weakly%2520supervised%250Alearning%2520process.%2520This%2520reordering%2520facilitates%2520the%2520incorporation%2520of%2520auxiliary%250Aalignment%2520objectives%252C%2520and%2520allows%2520for%2520the%2520use%2520of%2520efficient%2520supervision%2520via%2520a%250AConnectionist%2520Temporal%2520Classification%2520%2528CTC%2529%2520loss.%2520We%2520train%2520our%2520SLT%2520mode%252C%2520which%250Aconsists%2520of%2520a%2520vision%2520encoder%2520and%2520a%2520translator%252C%2520through%2520a%2520three-stage%2520pipeline%252C%250Awhich%2520progressively%2520narrows%2520the%2520modality%2520gap%2520between%2520sign%2520language%2520and%2520spoken%250Alanguage.%2520Despite%2520its%2520simplicity%252C%2520our%2520approach%2520outperforms%2520previous%250Astate-of-the-art%2520gloss-free%2520frameworks%2520on%2520two%2520SLT%2520benchmarks%2520and%2520achieves%250Acompetitive%2520results%2520compared%2520to%2520gloss-based%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15438v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Sign%20and%20Spoken%20Languages%3A%20Pseudo%20Gloss%20Generation%20for%20Sign%0A%20%20Language%20Translation&entry.906535625=Jianyuan%20Guo%20and%20Peike%20Li%20and%20Trevor%20Cohn&entry.1292438233=%20%20Sign%20Language%20Translation%20%28SLT%29%20aims%20to%20map%20sign%20language%20videos%20to%20spoken%0Alanguage%20text.%20A%20common%20approach%20relies%20on%20gloss%20annotations%20as%20an%20intermediate%0Arepresentation%2C%20decomposing%20SLT%20into%20two%20sub-tasks%3A%20video-to-gloss%20recognition%0Aand%20gloss-to-text%20translation.%20While%20effective%2C%20this%20paradigm%20depends%20on%0Aexpert-annotated%20gloss%20labels%2C%20which%20are%20costly%20and%20rarely%20available%20in%0Aexisting%20datasets%2C%20limiting%20its%20scalability.%20To%20address%20this%20challenge%2C%20we%0Apropose%20a%20gloss-free%20pseudo%20gloss%20generation%20framework%20that%20eliminates%20the%20need%0Afor%20human-annotated%20glosses%20while%20preserving%20the%20structured%20intermediate%0Arepresentation.%20Specifically%2C%20we%20prompt%20a%20Large%20Language%20Model%20%28LLM%29%20with%20a%20few%0Aexample%20text-gloss%20pairs%20using%20in-context%20learning%20to%20produce%20draft%20sign%0Aglosses%20from%20spoken%20language%20text.%20To%20enhance%20the%20correspondence%20between%0ALLM-generated%20pseudo%20glosses%20and%20the%20sign%20sequences%20in%20video%2C%20we%20correct%20the%0Aordering%20in%20the%20pseudo%20glosses%20for%20better%20alignment%20via%20a%20weakly%20supervised%0Alearning%20process.%20This%20reordering%20facilitates%20the%20incorporation%20of%20auxiliary%0Aalignment%20objectives%2C%20and%20allows%20for%20the%20use%20of%20efficient%20supervision%20via%20a%0AConnectionist%20Temporal%20Classification%20%28CTC%29%20loss.%20We%20train%20our%20SLT%20mode%2C%20which%0Aconsists%20of%20a%20vision%20encoder%20and%20a%20translator%2C%20through%20a%20three-stage%20pipeline%2C%0Awhich%20progressively%20narrows%20the%20modality%20gap%20between%20sign%20language%20and%20spoken%0Alanguage.%20Despite%20its%20simplicity%2C%20our%20approach%20outperforms%20previous%0Astate-of-the-art%20gloss-free%20frameworks%20on%20two%20SLT%20benchmarks%20and%20achieves%0Acompetitive%20results%20compared%20to%20gloss-based%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15438v1&entry.124074799=Read"},
{"title": "Protoknowledge Shapes Behaviour of LLMs in Downstream Tasks:\n  Memorization and Generalization with Knowledge Graphs", "author": "Federico Ranaldi and Andrea Zugarini and Leonardo Ranaldi and Fabio Massimo Zanzotto", "abstract": "  We introduce the concept of protoknowledge to formalize and measure how\nsequences of tokens encoding Knowledge Graphs are internalized during\npretraining and utilized at inference time by Large Language Models (LLMs).\nIndeed, LLMs have demonstrated the ability to memorize vast amounts of token\nsequences during pretraining, and a central open question is how they leverage\nthis memorization as reusable knowledge through generalization. We then\ncategorize protoknowledge into lexical, hierarchical, and topological forms,\nvarying on the type of knowledge that needs to be activated. We measure\nprotoknowledge through Knowledge Activation Tasks (KATs), analyzing its general\nproperties such as semantic bias. We then investigate the impact of\nprotoknowledge on Text-to-SPARQL performance by varying prompting strategies\ndepending on input conditions. To this end, we adopt a novel analysis framework\nthat assesses whether model predictions align with the successful activation of\nthe relevant protoknowledge for each query. This methodology provides a\npractical tool to explore Semantic-Level Data Contamination and serves as an\neffective strategy for Closed-Pretraining models.\n", "link": "http://arxiv.org/abs/2505.15501v1", "date": "2025-05-21", "relevancy": 2.5245, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5053}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5047}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Protoknowledge%20Shapes%20Behaviour%20of%20LLMs%20in%20Downstream%20Tasks%3A%0A%20%20Memorization%20and%20Generalization%20with%20Knowledge%20Graphs&body=Title%3A%20Protoknowledge%20Shapes%20Behaviour%20of%20LLMs%20in%20Downstream%20Tasks%3A%0A%20%20Memorization%20and%20Generalization%20with%20Knowledge%20Graphs%0AAuthor%3A%20Federico%20Ranaldi%20and%20Andrea%20Zugarini%20and%20Leonardo%20Ranaldi%20and%20Fabio%20Massimo%20Zanzotto%0AAbstract%3A%20%20%20We%20introduce%20the%20concept%20of%20protoknowledge%20to%20formalize%20and%20measure%20how%0Asequences%20of%20tokens%20encoding%20Knowledge%20Graphs%20are%20internalized%20during%0Apretraining%20and%20utilized%20at%20inference%20time%20by%20Large%20Language%20Models%20%28LLMs%29.%0AIndeed%2C%20LLMs%20have%20demonstrated%20the%20ability%20to%20memorize%20vast%20amounts%20of%20token%0Asequences%20during%20pretraining%2C%20and%20a%20central%20open%20question%20is%20how%20they%20leverage%0Athis%20memorization%20as%20reusable%20knowledge%20through%20generalization.%20We%20then%0Acategorize%20protoknowledge%20into%20lexical%2C%20hierarchical%2C%20and%20topological%20forms%2C%0Avarying%20on%20the%20type%20of%20knowledge%20that%20needs%20to%20be%20activated.%20We%20measure%0Aprotoknowledge%20through%20Knowledge%20Activation%20Tasks%20%28KATs%29%2C%20analyzing%20its%20general%0Aproperties%20such%20as%20semantic%20bias.%20We%20then%20investigate%20the%20impact%20of%0Aprotoknowledge%20on%20Text-to-SPARQL%20performance%20by%20varying%20prompting%20strategies%0Adepending%20on%20input%20conditions.%20To%20this%20end%2C%20we%20adopt%20a%20novel%20analysis%20framework%0Athat%20assesses%20whether%20model%20predictions%20align%20with%20the%20successful%20activation%20of%0Athe%20relevant%20protoknowledge%20for%20each%20query.%20This%20methodology%20provides%20a%0Apractical%20tool%20to%20explore%20Semantic-Level%20Data%20Contamination%20and%20serves%20as%20an%0Aeffective%20strategy%20for%20Closed-Pretraining%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15501v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProtoknowledge%2520Shapes%2520Behaviour%2520of%2520LLMs%2520in%2520Downstream%2520Tasks%253A%250A%2520%2520Memorization%2520and%2520Generalization%2520with%2520Knowledge%2520Graphs%26entry.906535625%3DFederico%2520Ranaldi%2520and%2520Andrea%2520Zugarini%2520and%2520Leonardo%2520Ranaldi%2520and%2520Fabio%2520Massimo%2520Zanzotto%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520concept%2520of%2520protoknowledge%2520to%2520formalize%2520and%2520measure%2520how%250Asequences%2520of%2520tokens%2520encoding%2520Knowledge%2520Graphs%2520are%2520internalized%2520during%250Apretraining%2520and%2520utilized%2520at%2520inference%2520time%2520by%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%250AIndeed%252C%2520LLMs%2520have%2520demonstrated%2520the%2520ability%2520to%2520memorize%2520vast%2520amounts%2520of%2520token%250Asequences%2520during%2520pretraining%252C%2520and%2520a%2520central%2520open%2520question%2520is%2520how%2520they%2520leverage%250Athis%2520memorization%2520as%2520reusable%2520knowledge%2520through%2520generalization.%2520We%2520then%250Acategorize%2520protoknowledge%2520into%2520lexical%252C%2520hierarchical%252C%2520and%2520topological%2520forms%252C%250Avarying%2520on%2520the%2520type%2520of%2520knowledge%2520that%2520needs%2520to%2520be%2520activated.%2520We%2520measure%250Aprotoknowledge%2520through%2520Knowledge%2520Activation%2520Tasks%2520%2528KATs%2529%252C%2520analyzing%2520its%2520general%250Aproperties%2520such%2520as%2520semantic%2520bias.%2520We%2520then%2520investigate%2520the%2520impact%2520of%250Aprotoknowledge%2520on%2520Text-to-SPARQL%2520performance%2520by%2520varying%2520prompting%2520strategies%250Adepending%2520on%2520input%2520conditions.%2520To%2520this%2520end%252C%2520we%2520adopt%2520a%2520novel%2520analysis%2520framework%250Athat%2520assesses%2520whether%2520model%2520predictions%2520align%2520with%2520the%2520successful%2520activation%2520of%250Athe%2520relevant%2520protoknowledge%2520for%2520each%2520query.%2520This%2520methodology%2520provides%2520a%250Apractical%2520tool%2520to%2520explore%2520Semantic-Level%2520Data%2520Contamination%2520and%2520serves%2520as%2520an%250Aeffective%2520strategy%2520for%2520Closed-Pretraining%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15501v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Protoknowledge%20Shapes%20Behaviour%20of%20LLMs%20in%20Downstream%20Tasks%3A%0A%20%20Memorization%20and%20Generalization%20with%20Knowledge%20Graphs&entry.906535625=Federico%20Ranaldi%20and%20Andrea%20Zugarini%20and%20Leonardo%20Ranaldi%20and%20Fabio%20Massimo%20Zanzotto&entry.1292438233=%20%20We%20introduce%20the%20concept%20of%20protoknowledge%20to%20formalize%20and%20measure%20how%0Asequences%20of%20tokens%20encoding%20Knowledge%20Graphs%20are%20internalized%20during%0Apretraining%20and%20utilized%20at%20inference%20time%20by%20Large%20Language%20Models%20%28LLMs%29.%0AIndeed%2C%20LLMs%20have%20demonstrated%20the%20ability%20to%20memorize%20vast%20amounts%20of%20token%0Asequences%20during%20pretraining%2C%20and%20a%20central%20open%20question%20is%20how%20they%20leverage%0Athis%20memorization%20as%20reusable%20knowledge%20through%20generalization.%20We%20then%0Acategorize%20protoknowledge%20into%20lexical%2C%20hierarchical%2C%20and%20topological%20forms%2C%0Avarying%20on%20the%20type%20of%20knowledge%20that%20needs%20to%20be%20activated.%20We%20measure%0Aprotoknowledge%20through%20Knowledge%20Activation%20Tasks%20%28KATs%29%2C%20analyzing%20its%20general%0Aproperties%20such%20as%20semantic%20bias.%20We%20then%20investigate%20the%20impact%20of%0Aprotoknowledge%20on%20Text-to-SPARQL%20performance%20by%20varying%20prompting%20strategies%0Adepending%20on%20input%20conditions.%20To%20this%20end%2C%20we%20adopt%20a%20novel%20analysis%20framework%0Athat%20assesses%20whether%20model%20predictions%20align%20with%20the%20successful%20activation%20of%0Athe%20relevant%20protoknowledge%20for%20each%20query.%20This%20methodology%20provides%20a%0Apractical%20tool%20to%20explore%20Semantic-Level%20Data%20Contamination%20and%20serves%20as%20an%0Aeffective%20strategy%20for%20Closed-Pretraining%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15501v1&entry.124074799=Read"},
{"title": "VisionReasoner: Unified Visual Perception and Reasoning via\n  Reinforcement Learning", "author": "Yuqi Liu and Tianyuan Qu and Zhisheng Zhong and Bohao Peng and Shu Liu and Bei Yu and Jiaya Jia", "abstract": "  Large vision-language models exhibit inherent capabilities to handle diverse\nvisual perception tasks. In this paper, we introduce VisionReasoner, a unified\nframework capable of reasoning and solving multiple visual perception tasks\nwithin a shared model. Specifically, by designing novel multi-object cognitive\nlearning strategies and systematic task reformulation, VisionReasoner enhances\nits reasoning capabilities to analyze visual inputs, and addresses diverse\nperception tasks in a unified framework. The model generates a structured\nreasoning process before delivering the desired outputs responding to user\nqueries. To rigorously assess unified visual perception capabilities, we\nevaluate VisionReasoner on ten diverse tasks spanning three critical domains:\ndetection, segmentation, and counting. Experimental results show that\nVisionReasoner achieves superior performance as a unified model, outperforming\nQwen2.5VL by relative margins of 29.1% on COCO (detection), 22.1% on ReasonSeg\n(segmentation), and 15.3% on CountBench (counting).\n", "link": "http://arxiv.org/abs/2505.12081v2", "date": "2025-05-21", "relevancy": 2.5235, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6409}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6409}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisionReasoner%3A%20Unified%20Visual%20Perception%20and%20Reasoning%20via%0A%20%20Reinforcement%20Learning&body=Title%3A%20VisionReasoner%3A%20Unified%20Visual%20Perception%20and%20Reasoning%20via%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Yuqi%20Liu%20and%20Tianyuan%20Qu%20and%20Zhisheng%20Zhong%20and%20Bohao%20Peng%20and%20Shu%20Liu%20and%20Bei%20Yu%20and%20Jiaya%20Jia%0AAbstract%3A%20%20%20Large%20vision-language%20models%20exhibit%20inherent%20capabilities%20to%20handle%20diverse%0Avisual%20perception%20tasks.%20In%20this%20paper%2C%20we%20introduce%20VisionReasoner%2C%20a%20unified%0Aframework%20capable%20of%20reasoning%20and%20solving%20multiple%20visual%20perception%20tasks%0Awithin%20a%20shared%20model.%20Specifically%2C%20by%20designing%20novel%20multi-object%20cognitive%0Alearning%20strategies%20and%20systematic%20task%20reformulation%2C%20VisionReasoner%20enhances%0Aits%20reasoning%20capabilities%20to%20analyze%20visual%20inputs%2C%20and%20addresses%20diverse%0Aperception%20tasks%20in%20a%20unified%20framework.%20The%20model%20generates%20a%20structured%0Areasoning%20process%20before%20delivering%20the%20desired%20outputs%20responding%20to%20user%0Aqueries.%20To%20rigorously%20assess%20unified%20visual%20perception%20capabilities%2C%20we%0Aevaluate%20VisionReasoner%20on%20ten%20diverse%20tasks%20spanning%20three%20critical%20domains%3A%0Adetection%2C%20segmentation%2C%20and%20counting.%20Experimental%20results%20show%20that%0AVisionReasoner%20achieves%20superior%20performance%20as%20a%20unified%20model%2C%20outperforming%0AQwen2.5VL%20by%20relative%20margins%20of%2029.1%25%20on%20COCO%20%28detection%29%2C%2022.1%25%20on%20ReasonSeg%0A%28segmentation%29%2C%20and%2015.3%25%20on%20CountBench%20%28counting%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12081v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisionReasoner%253A%2520Unified%2520Visual%2520Perception%2520and%2520Reasoning%2520via%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DYuqi%2520Liu%2520and%2520Tianyuan%2520Qu%2520and%2520Zhisheng%2520Zhong%2520and%2520Bohao%2520Peng%2520and%2520Shu%2520Liu%2520and%2520Bei%2520Yu%2520and%2520Jiaya%2520Jia%26entry.1292438233%3D%2520%2520Large%2520vision-language%2520models%2520exhibit%2520inherent%2520capabilities%2520to%2520handle%2520diverse%250Avisual%2520perception%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520VisionReasoner%252C%2520a%2520unified%250Aframework%2520capable%2520of%2520reasoning%2520and%2520solving%2520multiple%2520visual%2520perception%2520tasks%250Awithin%2520a%2520shared%2520model.%2520Specifically%252C%2520by%2520designing%2520novel%2520multi-object%2520cognitive%250Alearning%2520strategies%2520and%2520systematic%2520task%2520reformulation%252C%2520VisionReasoner%2520enhances%250Aits%2520reasoning%2520capabilities%2520to%2520analyze%2520visual%2520inputs%252C%2520and%2520addresses%2520diverse%250Aperception%2520tasks%2520in%2520a%2520unified%2520framework.%2520The%2520model%2520generates%2520a%2520structured%250Areasoning%2520process%2520before%2520delivering%2520the%2520desired%2520outputs%2520responding%2520to%2520user%250Aqueries.%2520To%2520rigorously%2520assess%2520unified%2520visual%2520perception%2520capabilities%252C%2520we%250Aevaluate%2520VisionReasoner%2520on%2520ten%2520diverse%2520tasks%2520spanning%2520three%2520critical%2520domains%253A%250Adetection%252C%2520segmentation%252C%2520and%2520counting.%2520Experimental%2520results%2520show%2520that%250AVisionReasoner%2520achieves%2520superior%2520performance%2520as%2520a%2520unified%2520model%252C%2520outperforming%250AQwen2.5VL%2520by%2520relative%2520margins%2520of%252029.1%2525%2520on%2520COCO%2520%2528detection%2529%252C%252022.1%2525%2520on%2520ReasonSeg%250A%2528segmentation%2529%252C%2520and%252015.3%2525%2520on%2520CountBench%2520%2528counting%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12081v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisionReasoner%3A%20Unified%20Visual%20Perception%20and%20Reasoning%20via%0A%20%20Reinforcement%20Learning&entry.906535625=Yuqi%20Liu%20and%20Tianyuan%20Qu%20and%20Zhisheng%20Zhong%20and%20Bohao%20Peng%20and%20Shu%20Liu%20and%20Bei%20Yu%20and%20Jiaya%20Jia&entry.1292438233=%20%20Large%20vision-language%20models%20exhibit%20inherent%20capabilities%20to%20handle%20diverse%0Avisual%20perception%20tasks.%20In%20this%20paper%2C%20we%20introduce%20VisionReasoner%2C%20a%20unified%0Aframework%20capable%20of%20reasoning%20and%20solving%20multiple%20visual%20perception%20tasks%0Awithin%20a%20shared%20model.%20Specifically%2C%20by%20designing%20novel%20multi-object%20cognitive%0Alearning%20strategies%20and%20systematic%20task%20reformulation%2C%20VisionReasoner%20enhances%0Aits%20reasoning%20capabilities%20to%20analyze%20visual%20inputs%2C%20and%20addresses%20diverse%0Aperception%20tasks%20in%20a%20unified%20framework.%20The%20model%20generates%20a%20structured%0Areasoning%20process%20before%20delivering%20the%20desired%20outputs%20responding%20to%20user%0Aqueries.%20To%20rigorously%20assess%20unified%20visual%20perception%20capabilities%2C%20we%0Aevaluate%20VisionReasoner%20on%20ten%20diverse%20tasks%20spanning%20three%20critical%20domains%3A%0Adetection%2C%20segmentation%2C%20and%20counting.%20Experimental%20results%20show%20that%0AVisionReasoner%20achieves%20superior%20performance%20as%20a%20unified%20model%2C%20outperforming%0AQwen2.5VL%20by%20relative%20margins%20of%2029.1%25%20on%20COCO%20%28detection%29%2C%2022.1%25%20on%20ReasonSeg%0A%28segmentation%29%2C%20and%2015.3%25%20on%20CountBench%20%28counting%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12081v2&entry.124074799=Read"},
{"title": "Higher-order Structure Boosts Link Prediction on Temporal Graphs", "author": "Jingzhe Liu and Zhigang Hua and Yan Xie and Bingheng Li and Harry Shomer and Yu Song and Kaveh Hassani and Jiliang Tang", "abstract": "  Temporal Graph Neural Networks (TGNNs) have gained growing attention for\nmodeling and predicting structures in temporal graphs. However, existing TGNNs\nprimarily focus on pairwise interactions while overlooking higher-order\nstructures that are integral to link formation and evolution in real-world\ntemporal graphs. Meanwhile, these models often suffer from efficiency\nbottlenecks, further limiting their expressive power. To tackle these\nchallenges, we propose a Higher-order structure Temporal Graph Neural Network,\nwhich incorporates hypergraph representations into temporal graph learning. In\nparticular, we develop an algorithm to identify the underlying higher-order\nstructures, enhancing the model's ability to capture the group interactions.\nFurthermore, by aggregating multiple edge features into hyperedge\nrepresentations, HTGN effectively reduces memory cost during training. We\ntheoretically demonstrate the enhanced expressiveness of our approach and\nvalidate its effectiveness and efficiency through extensive experiments on\nvarious real-world temporal graphs. Experimental results show that HTGN\nachieves superior performance on dynamic link prediction while reducing memory\ncosts by up to 50\\% compared to existing methods.\n", "link": "http://arxiv.org/abs/2505.15746v1", "date": "2025-05-21", "relevancy": 2.5213, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5475}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4841}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Higher-order%20Structure%20Boosts%20Link%20Prediction%20on%20Temporal%20Graphs&body=Title%3A%20Higher-order%20Structure%20Boosts%20Link%20Prediction%20on%20Temporal%20Graphs%0AAuthor%3A%20Jingzhe%20Liu%20and%20Zhigang%20Hua%20and%20Yan%20Xie%20and%20Bingheng%20Li%20and%20Harry%20Shomer%20and%20Yu%20Song%20and%20Kaveh%20Hassani%20and%20Jiliang%20Tang%0AAbstract%3A%20%20%20Temporal%20Graph%20Neural%20Networks%20%28TGNNs%29%20have%20gained%20growing%20attention%20for%0Amodeling%20and%20predicting%20structures%20in%20temporal%20graphs.%20However%2C%20existing%20TGNNs%0Aprimarily%20focus%20on%20pairwise%20interactions%20while%20overlooking%20higher-order%0Astructures%20that%20are%20integral%20to%20link%20formation%20and%20evolution%20in%20real-world%0Atemporal%20graphs.%20Meanwhile%2C%20these%20models%20often%20suffer%20from%20efficiency%0Abottlenecks%2C%20further%20limiting%20their%20expressive%20power.%20To%20tackle%20these%0Achallenges%2C%20we%20propose%20a%20Higher-order%20structure%20Temporal%20Graph%20Neural%20Network%2C%0Awhich%20incorporates%20hypergraph%20representations%20into%20temporal%20graph%20learning.%20In%0Aparticular%2C%20we%20develop%20an%20algorithm%20to%20identify%20the%20underlying%20higher-order%0Astructures%2C%20enhancing%20the%20model%27s%20ability%20to%20capture%20the%20group%20interactions.%0AFurthermore%2C%20by%20aggregating%20multiple%20edge%20features%20into%20hyperedge%0Arepresentations%2C%20HTGN%20effectively%20reduces%20memory%20cost%20during%20training.%20We%0Atheoretically%20demonstrate%20the%20enhanced%20expressiveness%20of%20our%20approach%20and%0Avalidate%20its%20effectiveness%20and%20efficiency%20through%20extensive%20experiments%20on%0Avarious%20real-world%20temporal%20graphs.%20Experimental%20results%20show%20that%20HTGN%0Aachieves%20superior%20performance%20on%20dynamic%20link%20prediction%20while%20reducing%20memory%0Acosts%20by%20up%20to%2050%5C%25%20compared%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15746v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigher-order%2520Structure%2520Boosts%2520Link%2520Prediction%2520on%2520Temporal%2520Graphs%26entry.906535625%3DJingzhe%2520Liu%2520and%2520Zhigang%2520Hua%2520and%2520Yan%2520Xie%2520and%2520Bingheng%2520Li%2520and%2520Harry%2520Shomer%2520and%2520Yu%2520Song%2520and%2520Kaveh%2520Hassani%2520and%2520Jiliang%2520Tang%26entry.1292438233%3D%2520%2520Temporal%2520Graph%2520Neural%2520Networks%2520%2528TGNNs%2529%2520have%2520gained%2520growing%2520attention%2520for%250Amodeling%2520and%2520predicting%2520structures%2520in%2520temporal%2520graphs.%2520However%252C%2520existing%2520TGNNs%250Aprimarily%2520focus%2520on%2520pairwise%2520interactions%2520while%2520overlooking%2520higher-order%250Astructures%2520that%2520are%2520integral%2520to%2520link%2520formation%2520and%2520evolution%2520in%2520real-world%250Atemporal%2520graphs.%2520Meanwhile%252C%2520these%2520models%2520often%2520suffer%2520from%2520efficiency%250Abottlenecks%252C%2520further%2520limiting%2520their%2520expressive%2520power.%2520To%2520tackle%2520these%250Achallenges%252C%2520we%2520propose%2520a%2520Higher-order%2520structure%2520Temporal%2520Graph%2520Neural%2520Network%252C%250Awhich%2520incorporates%2520hypergraph%2520representations%2520into%2520temporal%2520graph%2520learning.%2520In%250Aparticular%252C%2520we%2520develop%2520an%2520algorithm%2520to%2520identify%2520the%2520underlying%2520higher-order%250Astructures%252C%2520enhancing%2520the%2520model%2527s%2520ability%2520to%2520capture%2520the%2520group%2520interactions.%250AFurthermore%252C%2520by%2520aggregating%2520multiple%2520edge%2520features%2520into%2520hyperedge%250Arepresentations%252C%2520HTGN%2520effectively%2520reduces%2520memory%2520cost%2520during%2520training.%2520We%250Atheoretically%2520demonstrate%2520the%2520enhanced%2520expressiveness%2520of%2520our%2520approach%2520and%250Avalidate%2520its%2520effectiveness%2520and%2520efficiency%2520through%2520extensive%2520experiments%2520on%250Avarious%2520real-world%2520temporal%2520graphs.%2520Experimental%2520results%2520show%2520that%2520HTGN%250Aachieves%2520superior%2520performance%2520on%2520dynamic%2520link%2520prediction%2520while%2520reducing%2520memory%250Acosts%2520by%2520up%2520to%252050%255C%2525%2520compared%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15746v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Higher-order%20Structure%20Boosts%20Link%20Prediction%20on%20Temporal%20Graphs&entry.906535625=Jingzhe%20Liu%20and%20Zhigang%20Hua%20and%20Yan%20Xie%20and%20Bingheng%20Li%20and%20Harry%20Shomer%20and%20Yu%20Song%20and%20Kaveh%20Hassani%20and%20Jiliang%20Tang&entry.1292438233=%20%20Temporal%20Graph%20Neural%20Networks%20%28TGNNs%29%20have%20gained%20growing%20attention%20for%0Amodeling%20and%20predicting%20structures%20in%20temporal%20graphs.%20However%2C%20existing%20TGNNs%0Aprimarily%20focus%20on%20pairwise%20interactions%20while%20overlooking%20higher-order%0Astructures%20that%20are%20integral%20to%20link%20formation%20and%20evolution%20in%20real-world%0Atemporal%20graphs.%20Meanwhile%2C%20these%20models%20often%20suffer%20from%20efficiency%0Abottlenecks%2C%20further%20limiting%20their%20expressive%20power.%20To%20tackle%20these%0Achallenges%2C%20we%20propose%20a%20Higher-order%20structure%20Temporal%20Graph%20Neural%20Network%2C%0Awhich%20incorporates%20hypergraph%20representations%20into%20temporal%20graph%20learning.%20In%0Aparticular%2C%20we%20develop%20an%20algorithm%20to%20identify%20the%20underlying%20higher-order%0Astructures%2C%20enhancing%20the%20model%27s%20ability%20to%20capture%20the%20group%20interactions.%0AFurthermore%2C%20by%20aggregating%20multiple%20edge%20features%20into%20hyperedge%0Arepresentations%2C%20HTGN%20effectively%20reduces%20memory%20cost%20during%20training.%20We%0Atheoretically%20demonstrate%20the%20enhanced%20expressiveness%20of%20our%20approach%20and%0Avalidate%20its%20effectiveness%20and%20efficiency%20through%20extensive%20experiments%20on%0Avarious%20real-world%20temporal%20graphs.%20Experimental%20results%20show%20that%20HTGN%0Aachieves%20superior%20performance%20on%20dynamic%20link%20prediction%20while%20reducing%20memory%0Acosts%20by%20up%20to%2050%5C%25%20compared%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15746v1&entry.124074799=Read"},
{"title": "Transfer of Structural Knowledge from Synthetic Languages", "author": "Mikhail Budnikov and Ivan Yamshchikov", "abstract": "  This work explores transfer learning from several synthetic languages to\nEnglish. We investigate the structure of the embeddings in the fine-tuned\nmodels, the information they contain, and the capabilities of the fine-tuned\nmodels on simple linguistic tasks. We also introduce a new synthetic language\nthat leads to better transfer to English than the languages used in previous\nresearch. Finally, we introduce Tiny-Cloze Benchmark - a new synthetic\nbenchmark for natural language understanding that is more informative for less\npowerful models. We use Tiny-Cloze Benchmark to evaluate fine-tuned models in\nseveral domains demonstrating that fine-tuning on a new synthetic language\nallows for better performance on a variety of tasks.\n", "link": "http://arxiv.org/abs/2505.15769v1", "date": "2025-05-21", "relevancy": 2.5206, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5115}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5115}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4893}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transfer%20of%20Structural%20Knowledge%20from%20Synthetic%20Languages&body=Title%3A%20Transfer%20of%20Structural%20Knowledge%20from%20Synthetic%20Languages%0AAuthor%3A%20Mikhail%20Budnikov%20and%20Ivan%20Yamshchikov%0AAbstract%3A%20%20%20This%20work%20explores%20transfer%20learning%20from%20several%20synthetic%20languages%20to%0AEnglish.%20We%20investigate%20the%20structure%20of%20the%20embeddings%20in%20the%20fine-tuned%0Amodels%2C%20the%20information%20they%20contain%2C%20and%20the%20capabilities%20of%20the%20fine-tuned%0Amodels%20on%20simple%20linguistic%20tasks.%20We%20also%20introduce%20a%20new%20synthetic%20language%0Athat%20leads%20to%20better%20transfer%20to%20English%20than%20the%20languages%20used%20in%20previous%0Aresearch.%20Finally%2C%20we%20introduce%20Tiny-Cloze%20Benchmark%20-%20a%20new%20synthetic%0Abenchmark%20for%20natural%20language%20understanding%20that%20is%20more%20informative%20for%20less%0Apowerful%20models.%20We%20use%20Tiny-Cloze%20Benchmark%20to%20evaluate%20fine-tuned%20models%20in%0Aseveral%20domains%20demonstrating%20that%20fine-tuning%20on%20a%20new%20synthetic%20language%0Aallows%20for%20better%20performance%20on%20a%20variety%20of%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15769v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransfer%2520of%2520Structural%2520Knowledge%2520from%2520Synthetic%2520Languages%26entry.906535625%3DMikhail%2520Budnikov%2520and%2520Ivan%2520Yamshchikov%26entry.1292438233%3D%2520%2520This%2520work%2520explores%2520transfer%2520learning%2520from%2520several%2520synthetic%2520languages%2520to%250AEnglish.%2520We%2520investigate%2520the%2520structure%2520of%2520the%2520embeddings%2520in%2520the%2520fine-tuned%250Amodels%252C%2520the%2520information%2520they%2520contain%252C%2520and%2520the%2520capabilities%2520of%2520the%2520fine-tuned%250Amodels%2520on%2520simple%2520linguistic%2520tasks.%2520We%2520also%2520introduce%2520a%2520new%2520synthetic%2520language%250Athat%2520leads%2520to%2520better%2520transfer%2520to%2520English%2520than%2520the%2520languages%2520used%2520in%2520previous%250Aresearch.%2520Finally%252C%2520we%2520introduce%2520Tiny-Cloze%2520Benchmark%2520-%2520a%2520new%2520synthetic%250Abenchmark%2520for%2520natural%2520language%2520understanding%2520that%2520is%2520more%2520informative%2520for%2520less%250Apowerful%2520models.%2520We%2520use%2520Tiny-Cloze%2520Benchmark%2520to%2520evaluate%2520fine-tuned%2520models%2520in%250Aseveral%2520domains%2520demonstrating%2520that%2520fine-tuning%2520on%2520a%2520new%2520synthetic%2520language%250Aallows%2520for%2520better%2520performance%2520on%2520a%2520variety%2520of%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15769v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transfer%20of%20Structural%20Knowledge%20from%20Synthetic%20Languages&entry.906535625=Mikhail%20Budnikov%20and%20Ivan%20Yamshchikov&entry.1292438233=%20%20This%20work%20explores%20transfer%20learning%20from%20several%20synthetic%20languages%20to%0AEnglish.%20We%20investigate%20the%20structure%20of%20the%20embeddings%20in%20the%20fine-tuned%0Amodels%2C%20the%20information%20they%20contain%2C%20and%20the%20capabilities%20of%20the%20fine-tuned%0Amodels%20on%20simple%20linguistic%20tasks.%20We%20also%20introduce%20a%20new%20synthetic%20language%0Athat%20leads%20to%20better%20transfer%20to%20English%20than%20the%20languages%20used%20in%20previous%0Aresearch.%20Finally%2C%20we%20introduce%20Tiny-Cloze%20Benchmark%20-%20a%20new%20synthetic%0Abenchmark%20for%20natural%20language%20understanding%20that%20is%20more%20informative%20for%20less%0Apowerful%20models.%20We%20use%20Tiny-Cloze%20Benchmark%20to%20evaluate%20fine-tuned%20models%20in%0Aseveral%20domains%20demonstrating%20that%20fine-tuning%20on%20a%20new%20synthetic%20language%0Aallows%20for%20better%20performance%20on%20a%20variety%20of%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15769v1&entry.124074799=Read"},
{"title": "Shared Path: Unraveling Memorization in Multilingual LLMs through\n  Language Similarities", "author": "Xiaoyu Luo and Yiyi Chen and Johannes Bjerva and Qiongxiu Li", "abstract": "  We present the first comprehensive study of Memorization in Multilingual\nLarge Language Models (MLLMs), analyzing 95 languages using models across\ndiverse model scales, architectures, and memorization definitions. As MLLMs are\nincreasingly deployed, understanding their memorization behavior has become\ncritical. Yet prior work has focused primarily on monolingual models, leaving\nmultilingual memorization underexplored, despite the inherently long-tailed\nnature of training corpora. We find that the prevailing assumption, that\nmemorization is highly correlated with training data availability, fails to\nfully explain memorization patterns in MLLMs. We hypothesize that treating\nlanguages in isolation - ignoring their similarities - obscures the true\npatterns of memorization. To address this, we propose a novel graph-based\ncorrelation metric that incorporates language similarity to analyze\ncross-lingual memorization. Our analysis reveals that among similar languages,\nthose with fewer training tokens tend to exhibit higher memorization, a trend\nthat only emerges when cross-lingual relationships are explicitly modeled.\nThese findings underscore the importance of a language-aware perspective in\nevaluating and mitigating memorization vulnerabilities in MLLMs. This also\nconstitutes empirical evidence that language similarity both explains\nMemorization in MLLMs and underpins Cross-lingual Transferability, with broad\nimplications for multilingual NLP.\n", "link": "http://arxiv.org/abs/2505.15722v1", "date": "2025-05-21", "relevancy": 2.5191, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5174}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.497}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shared%20Path%3A%20Unraveling%20Memorization%20in%20Multilingual%20LLMs%20through%0A%20%20Language%20Similarities&body=Title%3A%20Shared%20Path%3A%20Unraveling%20Memorization%20in%20Multilingual%20LLMs%20through%0A%20%20Language%20Similarities%0AAuthor%3A%20Xiaoyu%20Luo%20and%20Yiyi%20Chen%20and%20Johannes%20Bjerva%20and%20Qiongxiu%20Li%0AAbstract%3A%20%20%20We%20present%20the%20first%20comprehensive%20study%20of%20Memorization%20in%20Multilingual%0ALarge%20Language%20Models%20%28MLLMs%29%2C%20analyzing%2095%20languages%20using%20models%20across%0Adiverse%20model%20scales%2C%20architectures%2C%20and%20memorization%20definitions.%20As%20MLLMs%20are%0Aincreasingly%20deployed%2C%20understanding%20their%20memorization%20behavior%20has%20become%0Acritical.%20Yet%20prior%20work%20has%20focused%20primarily%20on%20monolingual%20models%2C%20leaving%0Amultilingual%20memorization%20underexplored%2C%20despite%20the%20inherently%20long-tailed%0Anature%20of%20training%20corpora.%20We%20find%20that%20the%20prevailing%20assumption%2C%20that%0Amemorization%20is%20highly%20correlated%20with%20training%20data%20availability%2C%20fails%20to%0Afully%20explain%20memorization%20patterns%20in%20MLLMs.%20We%20hypothesize%20that%20treating%0Alanguages%20in%20isolation%20-%20ignoring%20their%20similarities%20-%20obscures%20the%20true%0Apatterns%20of%20memorization.%20To%20address%20this%2C%20we%20propose%20a%20novel%20graph-based%0Acorrelation%20metric%20that%20incorporates%20language%20similarity%20to%20analyze%0Across-lingual%20memorization.%20Our%20analysis%20reveals%20that%20among%20similar%20languages%2C%0Athose%20with%20fewer%20training%20tokens%20tend%20to%20exhibit%20higher%20memorization%2C%20a%20trend%0Athat%20only%20emerges%20when%20cross-lingual%20relationships%20are%20explicitly%20modeled.%0AThese%20findings%20underscore%20the%20importance%20of%20a%20language-aware%20perspective%20in%0Aevaluating%20and%20mitigating%20memorization%20vulnerabilities%20in%20MLLMs.%20This%20also%0Aconstitutes%20empirical%20evidence%20that%20language%20similarity%20both%20explains%0AMemorization%20in%20MLLMs%20and%20underpins%20Cross-lingual%20Transferability%2C%20with%20broad%0Aimplications%20for%20multilingual%20NLP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15722v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShared%2520Path%253A%2520Unraveling%2520Memorization%2520in%2520Multilingual%2520LLMs%2520through%250A%2520%2520Language%2520Similarities%26entry.906535625%3DXiaoyu%2520Luo%2520and%2520Yiyi%2520Chen%2520and%2520Johannes%2520Bjerva%2520and%2520Qiongxiu%2520Li%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520first%2520comprehensive%2520study%2520of%2520Memorization%2520in%2520Multilingual%250ALarge%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520analyzing%252095%2520languages%2520using%2520models%2520across%250Adiverse%2520model%2520scales%252C%2520architectures%252C%2520and%2520memorization%2520definitions.%2520As%2520MLLMs%2520are%250Aincreasingly%2520deployed%252C%2520understanding%2520their%2520memorization%2520behavior%2520has%2520become%250Acritical.%2520Yet%2520prior%2520work%2520has%2520focused%2520primarily%2520on%2520monolingual%2520models%252C%2520leaving%250Amultilingual%2520memorization%2520underexplored%252C%2520despite%2520the%2520inherently%2520long-tailed%250Anature%2520of%2520training%2520corpora.%2520We%2520find%2520that%2520the%2520prevailing%2520assumption%252C%2520that%250Amemorization%2520is%2520highly%2520correlated%2520with%2520training%2520data%2520availability%252C%2520fails%2520to%250Afully%2520explain%2520memorization%2520patterns%2520in%2520MLLMs.%2520We%2520hypothesize%2520that%2520treating%250Alanguages%2520in%2520isolation%2520-%2520ignoring%2520their%2520similarities%2520-%2520obscures%2520the%2520true%250Apatterns%2520of%2520memorization.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520novel%2520graph-based%250Acorrelation%2520metric%2520that%2520incorporates%2520language%2520similarity%2520to%2520analyze%250Across-lingual%2520memorization.%2520Our%2520analysis%2520reveals%2520that%2520among%2520similar%2520languages%252C%250Athose%2520with%2520fewer%2520training%2520tokens%2520tend%2520to%2520exhibit%2520higher%2520memorization%252C%2520a%2520trend%250Athat%2520only%2520emerges%2520when%2520cross-lingual%2520relationships%2520are%2520explicitly%2520modeled.%250AThese%2520findings%2520underscore%2520the%2520importance%2520of%2520a%2520language-aware%2520perspective%2520in%250Aevaluating%2520and%2520mitigating%2520memorization%2520vulnerabilities%2520in%2520MLLMs.%2520This%2520also%250Aconstitutes%2520empirical%2520evidence%2520that%2520language%2520similarity%2520both%2520explains%250AMemorization%2520in%2520MLLMs%2520and%2520underpins%2520Cross-lingual%2520Transferability%252C%2520with%2520broad%250Aimplications%2520for%2520multilingual%2520NLP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15722v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shared%20Path%3A%20Unraveling%20Memorization%20in%20Multilingual%20LLMs%20through%0A%20%20Language%20Similarities&entry.906535625=Xiaoyu%20Luo%20and%20Yiyi%20Chen%20and%20Johannes%20Bjerva%20and%20Qiongxiu%20Li&entry.1292438233=%20%20We%20present%20the%20first%20comprehensive%20study%20of%20Memorization%20in%20Multilingual%0ALarge%20Language%20Models%20%28MLLMs%29%2C%20analyzing%2095%20languages%20using%20models%20across%0Adiverse%20model%20scales%2C%20architectures%2C%20and%20memorization%20definitions.%20As%20MLLMs%20are%0Aincreasingly%20deployed%2C%20understanding%20their%20memorization%20behavior%20has%20become%0Acritical.%20Yet%20prior%20work%20has%20focused%20primarily%20on%20monolingual%20models%2C%20leaving%0Amultilingual%20memorization%20underexplored%2C%20despite%20the%20inherently%20long-tailed%0Anature%20of%20training%20corpora.%20We%20find%20that%20the%20prevailing%20assumption%2C%20that%0Amemorization%20is%20highly%20correlated%20with%20training%20data%20availability%2C%20fails%20to%0Afully%20explain%20memorization%20patterns%20in%20MLLMs.%20We%20hypothesize%20that%20treating%0Alanguages%20in%20isolation%20-%20ignoring%20their%20similarities%20-%20obscures%20the%20true%0Apatterns%20of%20memorization.%20To%20address%20this%2C%20we%20propose%20a%20novel%20graph-based%0Acorrelation%20metric%20that%20incorporates%20language%20similarity%20to%20analyze%0Across-lingual%20memorization.%20Our%20analysis%20reveals%20that%20among%20similar%20languages%2C%0Athose%20with%20fewer%20training%20tokens%20tend%20to%20exhibit%20higher%20memorization%2C%20a%20trend%0Athat%20only%20emerges%20when%20cross-lingual%20relationships%20are%20explicitly%20modeled.%0AThese%20findings%20underscore%20the%20importance%20of%20a%20language-aware%20perspective%20in%0Aevaluating%20and%20mitigating%20memorization%20vulnerabilities%20in%20MLLMs.%20This%20also%0Aconstitutes%20empirical%20evidence%20that%20language%20similarity%20both%20explains%0AMemorization%20in%20MLLMs%20and%20underpins%20Cross-lingual%20Transferability%2C%20with%20broad%0Aimplications%20for%20multilingual%20NLP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15722v1&entry.124074799=Read"},
{"title": "Convex Relaxation for Robust Vanishing Point Estimation in Manhattan\n  World", "author": "Bangyan Liao and Zhenjun Zhao and Haoang Li and Yi Zhou and Yingping Zeng and Hao Li and Peidong Liu", "abstract": "  Determining the vanishing points (VPs) in a Manhattan world, as a fundamental\ntask in many 3D vision applications, consists of jointly inferring the line-VP\nassociation and locating each VP. Existing methods are, however, either\nsub-optimal solvers or pursuing global optimality at a significant cost of\ncomputing time. In contrast to prior works, we introduce convex relaxation\ntechniques to solve this task for the first time. Specifically, we employ a\n\"soft\" association scheme, realized via a truncated multi-selection error, that\nallows for joint estimation of VPs' locations and line-VP associations. This\napproach leads to a primal problem that can be reformulated into a\nquadratically constrained quadratic programming (QCQP) problem, which is then\nrelaxed into a convex semidefinite programming (SDP) problem. To solve this SDP\nproblem efficiently, we present a globally optimal outlier-robust iterative\nsolver (called GlobustVP), which independently searches for one VP and its\nassociated lines in each iteration, treating other lines as outliers. After\neach independent update of all VPs, the mutual orthogonality between the three\nVPs in a Manhattan world is reinforced via local refinement. Extensive\nexperiments on both synthetic and real-world data demonstrate that GlobustVP\nachieves a favorable balance between efficiency, robustness, and global\noptimality compared to previous works. The code is publicly available at\nhttps://github.com/WU-CVGL/GlobustVP.\n", "link": "http://arxiv.org/abs/2505.04788v2", "date": "2025-05-21", "relevancy": 2.5138, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5377}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4929}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4777}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convex%20Relaxation%20for%20Robust%20Vanishing%20Point%20Estimation%20in%20Manhattan%0A%20%20World&body=Title%3A%20Convex%20Relaxation%20for%20Robust%20Vanishing%20Point%20Estimation%20in%20Manhattan%0A%20%20World%0AAuthor%3A%20Bangyan%20Liao%20and%20Zhenjun%20Zhao%20and%20Haoang%20Li%20and%20Yi%20Zhou%20and%20Yingping%20Zeng%20and%20Hao%20Li%20and%20Peidong%20Liu%0AAbstract%3A%20%20%20Determining%20the%20vanishing%20points%20%28VPs%29%20in%20a%20Manhattan%20world%2C%20as%20a%20fundamental%0Atask%20in%20many%203D%20vision%20applications%2C%20consists%20of%20jointly%20inferring%20the%20line-VP%0Aassociation%20and%20locating%20each%20VP.%20Existing%20methods%20are%2C%20however%2C%20either%0Asub-optimal%20solvers%20or%20pursuing%20global%20optimality%20at%20a%20significant%20cost%20of%0Acomputing%20time.%20In%20contrast%20to%20prior%20works%2C%20we%20introduce%20convex%20relaxation%0Atechniques%20to%20solve%20this%20task%20for%20the%20first%20time.%20Specifically%2C%20we%20employ%20a%0A%22soft%22%20association%20scheme%2C%20realized%20via%20a%20truncated%20multi-selection%20error%2C%20that%0Aallows%20for%20joint%20estimation%20of%20VPs%27%20locations%20and%20line-VP%20associations.%20This%0Aapproach%20leads%20to%20a%20primal%20problem%20that%20can%20be%20reformulated%20into%20a%0Aquadratically%20constrained%20quadratic%20programming%20%28QCQP%29%20problem%2C%20which%20is%20then%0Arelaxed%20into%20a%20convex%20semidefinite%20programming%20%28SDP%29%20problem.%20To%20solve%20this%20SDP%0Aproblem%20efficiently%2C%20we%20present%20a%20globally%20optimal%20outlier-robust%20iterative%0Asolver%20%28called%20GlobustVP%29%2C%20which%20independently%20searches%20for%20one%20VP%20and%20its%0Aassociated%20lines%20in%20each%20iteration%2C%20treating%20other%20lines%20as%20outliers.%20After%0Aeach%20independent%20update%20of%20all%20VPs%2C%20the%20mutual%20orthogonality%20between%20the%20three%0AVPs%20in%20a%20Manhattan%20world%20is%20reinforced%20via%20local%20refinement.%20Extensive%0Aexperiments%20on%20both%20synthetic%20and%20real-world%20data%20demonstrate%20that%20GlobustVP%0Aachieves%20a%20favorable%20balance%20between%20efficiency%2C%20robustness%2C%20and%20global%0Aoptimality%20compared%20to%20previous%20works.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/WU-CVGL/GlobustVP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04788v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvex%2520Relaxation%2520for%2520Robust%2520Vanishing%2520Point%2520Estimation%2520in%2520Manhattan%250A%2520%2520World%26entry.906535625%3DBangyan%2520Liao%2520and%2520Zhenjun%2520Zhao%2520and%2520Haoang%2520Li%2520and%2520Yi%2520Zhou%2520and%2520Yingping%2520Zeng%2520and%2520Hao%2520Li%2520and%2520Peidong%2520Liu%26entry.1292438233%3D%2520%2520Determining%2520the%2520vanishing%2520points%2520%2528VPs%2529%2520in%2520a%2520Manhattan%2520world%252C%2520as%2520a%2520fundamental%250Atask%2520in%2520many%25203D%2520vision%2520applications%252C%2520consists%2520of%2520jointly%2520inferring%2520the%2520line-VP%250Aassociation%2520and%2520locating%2520each%2520VP.%2520Existing%2520methods%2520are%252C%2520however%252C%2520either%250Asub-optimal%2520solvers%2520or%2520pursuing%2520global%2520optimality%2520at%2520a%2520significant%2520cost%2520of%250Acomputing%2520time.%2520In%2520contrast%2520to%2520prior%2520works%252C%2520we%2520introduce%2520convex%2520relaxation%250Atechniques%2520to%2520solve%2520this%2520task%2520for%2520the%2520first%2520time.%2520Specifically%252C%2520we%2520employ%2520a%250A%2522soft%2522%2520association%2520scheme%252C%2520realized%2520via%2520a%2520truncated%2520multi-selection%2520error%252C%2520that%250Aallows%2520for%2520joint%2520estimation%2520of%2520VPs%2527%2520locations%2520and%2520line-VP%2520associations.%2520This%250Aapproach%2520leads%2520to%2520a%2520primal%2520problem%2520that%2520can%2520be%2520reformulated%2520into%2520a%250Aquadratically%2520constrained%2520quadratic%2520programming%2520%2528QCQP%2529%2520problem%252C%2520which%2520is%2520then%250Arelaxed%2520into%2520a%2520convex%2520semidefinite%2520programming%2520%2528SDP%2529%2520problem.%2520To%2520solve%2520this%2520SDP%250Aproblem%2520efficiently%252C%2520we%2520present%2520a%2520globally%2520optimal%2520outlier-robust%2520iterative%250Asolver%2520%2528called%2520GlobustVP%2529%252C%2520which%2520independently%2520searches%2520for%2520one%2520VP%2520and%2520its%250Aassociated%2520lines%2520in%2520each%2520iteration%252C%2520treating%2520other%2520lines%2520as%2520outliers.%2520After%250Aeach%2520independent%2520update%2520of%2520all%2520VPs%252C%2520the%2520mutual%2520orthogonality%2520between%2520the%2520three%250AVPs%2520in%2520a%2520Manhattan%2520world%2520is%2520reinforced%2520via%2520local%2520refinement.%2520Extensive%250Aexperiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520data%2520demonstrate%2520that%2520GlobustVP%250Aachieves%2520a%2520favorable%2520balance%2520between%2520efficiency%252C%2520robustness%252C%2520and%2520global%250Aoptimality%2520compared%2520to%2520previous%2520works.%2520The%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/WU-CVGL/GlobustVP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04788v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convex%20Relaxation%20for%20Robust%20Vanishing%20Point%20Estimation%20in%20Manhattan%0A%20%20World&entry.906535625=Bangyan%20Liao%20and%20Zhenjun%20Zhao%20and%20Haoang%20Li%20and%20Yi%20Zhou%20and%20Yingping%20Zeng%20and%20Hao%20Li%20and%20Peidong%20Liu&entry.1292438233=%20%20Determining%20the%20vanishing%20points%20%28VPs%29%20in%20a%20Manhattan%20world%2C%20as%20a%20fundamental%0Atask%20in%20many%203D%20vision%20applications%2C%20consists%20of%20jointly%20inferring%20the%20line-VP%0Aassociation%20and%20locating%20each%20VP.%20Existing%20methods%20are%2C%20however%2C%20either%0Asub-optimal%20solvers%20or%20pursuing%20global%20optimality%20at%20a%20significant%20cost%20of%0Acomputing%20time.%20In%20contrast%20to%20prior%20works%2C%20we%20introduce%20convex%20relaxation%0Atechniques%20to%20solve%20this%20task%20for%20the%20first%20time.%20Specifically%2C%20we%20employ%20a%0A%22soft%22%20association%20scheme%2C%20realized%20via%20a%20truncated%20multi-selection%20error%2C%20that%0Aallows%20for%20joint%20estimation%20of%20VPs%27%20locations%20and%20line-VP%20associations.%20This%0Aapproach%20leads%20to%20a%20primal%20problem%20that%20can%20be%20reformulated%20into%20a%0Aquadratically%20constrained%20quadratic%20programming%20%28QCQP%29%20problem%2C%20which%20is%20then%0Arelaxed%20into%20a%20convex%20semidefinite%20programming%20%28SDP%29%20problem.%20To%20solve%20this%20SDP%0Aproblem%20efficiently%2C%20we%20present%20a%20globally%20optimal%20outlier-robust%20iterative%0Asolver%20%28called%20GlobustVP%29%2C%20which%20independently%20searches%20for%20one%20VP%20and%20its%0Aassociated%20lines%20in%20each%20iteration%2C%20treating%20other%20lines%20as%20outliers.%20After%0Aeach%20independent%20update%20of%20all%20VPs%2C%20the%20mutual%20orthogonality%20between%20the%20three%0AVPs%20in%20a%20Manhattan%20world%20is%20reinforced%20via%20local%20refinement.%20Extensive%0Aexperiments%20on%20both%20synthetic%20and%20real-world%20data%20demonstrate%20that%20GlobustVP%0Aachieves%20a%20favorable%20balance%20between%20efficiency%2C%20robustness%2C%20and%20global%0Aoptimality%20compared%20to%20previous%20works.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/WU-CVGL/GlobustVP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04788v2&entry.124074799=Read"},
{"title": "The Atlas of In-Context Learning: How Attention Heads Shape In-Context\n  Retrieval Augmentation", "author": "Patrick Kahardipraja and Reduan Achtibat and Thomas Wiegand and Wojciech Samek and Sebastian Lapuschkin", "abstract": "  Large language models are able to exploit in-context learning to access\nexternal knowledge beyond their training data through retrieval-augmentation.\nWhile promising, its inner workings remain unclear. In this work, we shed light\non the mechanism of in-context retrieval augmentation for question answering by\nviewing a prompt as a composition of informational components. We propose an\nattribution-based method to identify specialized attention heads, revealing\nin-context heads that comprehend instructions and retrieve relevant contextual\ninformation, and parametric heads that store entities' relational knowledge. To\nbetter understand their roles, we extract function vectors and modify their\nattention weights to show how they can influence the answer generation process.\nFinally, we leverage the gained insights to trace the sources of knowledge used\nduring inference, paving the way towards more safe and transparent language\nmodels.\n", "link": "http://arxiv.org/abs/2505.15807v1", "date": "2025-05-21", "relevancy": 2.5092, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5198}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5198}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Atlas%20of%20In-Context%20Learning%3A%20How%20Attention%20Heads%20Shape%20In-Context%0A%20%20Retrieval%20Augmentation&body=Title%3A%20The%20Atlas%20of%20In-Context%20Learning%3A%20How%20Attention%20Heads%20Shape%20In-Context%0A%20%20Retrieval%20Augmentation%0AAuthor%3A%20Patrick%20Kahardipraja%20and%20Reduan%20Achtibat%20and%20Thomas%20Wiegand%20and%20Wojciech%20Samek%20and%20Sebastian%20Lapuschkin%0AAbstract%3A%20%20%20Large%20language%20models%20are%20able%20to%20exploit%20in-context%20learning%20to%20access%0Aexternal%20knowledge%20beyond%20their%20training%20data%20through%20retrieval-augmentation.%0AWhile%20promising%2C%20its%20inner%20workings%20remain%20unclear.%20In%20this%20work%2C%20we%20shed%20light%0Aon%20the%20mechanism%20of%20in-context%20retrieval%20augmentation%20for%20question%20answering%20by%0Aviewing%20a%20prompt%20as%20a%20composition%20of%20informational%20components.%20We%20propose%20an%0Aattribution-based%20method%20to%20identify%20specialized%20attention%20heads%2C%20revealing%0Ain-context%20heads%20that%20comprehend%20instructions%20and%20retrieve%20relevant%20contextual%0Ainformation%2C%20and%20parametric%20heads%20that%20store%20entities%27%20relational%20knowledge.%20To%0Abetter%20understand%20their%20roles%2C%20we%20extract%20function%20vectors%20and%20modify%20their%0Aattention%20weights%20to%20show%20how%20they%20can%20influence%20the%20answer%20generation%20process.%0AFinally%2C%20we%20leverage%20the%20gained%20insights%20to%20trace%20the%20sources%20of%20knowledge%20used%0Aduring%20inference%2C%20paving%20the%20way%20towards%20more%20safe%20and%20transparent%20language%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15807v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Atlas%2520of%2520In-Context%2520Learning%253A%2520How%2520Attention%2520Heads%2520Shape%2520In-Context%250A%2520%2520Retrieval%2520Augmentation%26entry.906535625%3DPatrick%2520Kahardipraja%2520and%2520Reduan%2520Achtibat%2520and%2520Thomas%2520Wiegand%2520and%2520Wojciech%2520Samek%2520and%2520Sebastian%2520Lapuschkin%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520are%2520able%2520to%2520exploit%2520in-context%2520learning%2520to%2520access%250Aexternal%2520knowledge%2520beyond%2520their%2520training%2520data%2520through%2520retrieval-augmentation.%250AWhile%2520promising%252C%2520its%2520inner%2520workings%2520remain%2520unclear.%2520In%2520this%2520work%252C%2520we%2520shed%2520light%250Aon%2520the%2520mechanism%2520of%2520in-context%2520retrieval%2520augmentation%2520for%2520question%2520answering%2520by%250Aviewing%2520a%2520prompt%2520as%2520a%2520composition%2520of%2520informational%2520components.%2520We%2520propose%2520an%250Aattribution-based%2520method%2520to%2520identify%2520specialized%2520attention%2520heads%252C%2520revealing%250Ain-context%2520heads%2520that%2520comprehend%2520instructions%2520and%2520retrieve%2520relevant%2520contextual%250Ainformation%252C%2520and%2520parametric%2520heads%2520that%2520store%2520entities%2527%2520relational%2520knowledge.%2520To%250Abetter%2520understand%2520their%2520roles%252C%2520we%2520extract%2520function%2520vectors%2520and%2520modify%2520their%250Aattention%2520weights%2520to%2520show%2520how%2520they%2520can%2520influence%2520the%2520answer%2520generation%2520process.%250AFinally%252C%2520we%2520leverage%2520the%2520gained%2520insights%2520to%2520trace%2520the%2520sources%2520of%2520knowledge%2520used%250Aduring%2520inference%252C%2520paving%2520the%2520way%2520towards%2520more%2520safe%2520and%2520transparent%2520language%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15807v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Atlas%20of%20In-Context%20Learning%3A%20How%20Attention%20Heads%20Shape%20In-Context%0A%20%20Retrieval%20Augmentation&entry.906535625=Patrick%20Kahardipraja%20and%20Reduan%20Achtibat%20and%20Thomas%20Wiegand%20and%20Wojciech%20Samek%20and%20Sebastian%20Lapuschkin&entry.1292438233=%20%20Large%20language%20models%20are%20able%20to%20exploit%20in-context%20learning%20to%20access%0Aexternal%20knowledge%20beyond%20their%20training%20data%20through%20retrieval-augmentation.%0AWhile%20promising%2C%20its%20inner%20workings%20remain%20unclear.%20In%20this%20work%2C%20we%20shed%20light%0Aon%20the%20mechanism%20of%20in-context%20retrieval%20augmentation%20for%20question%20answering%20by%0Aviewing%20a%20prompt%20as%20a%20composition%20of%20informational%20components.%20We%20propose%20an%0Aattribution-based%20method%20to%20identify%20specialized%20attention%20heads%2C%20revealing%0Ain-context%20heads%20that%20comprehend%20instructions%20and%20retrieve%20relevant%20contextual%0Ainformation%2C%20and%20parametric%20heads%20that%20store%20entities%27%20relational%20knowledge.%20To%0Abetter%20understand%20their%20roles%2C%20we%20extract%20function%20vectors%20and%20modify%20their%0Aattention%20weights%20to%20show%20how%20they%20can%20influence%20the%20answer%20generation%20process.%0AFinally%2C%20we%20leverage%20the%20gained%20insights%20to%20trace%20the%20sources%20of%20knowledge%20used%0Aduring%20inference%2C%20paving%20the%20way%20towards%20more%20safe%20and%20transparent%20language%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15807v1&entry.124074799=Read"},
{"title": "Gompertz Linear Units: Leveraging Asymmetry for Enhanced Learning\n  Dynamics", "author": "Indrashis Das and Mahmoud Safari and Steven Adriaensen and Frank Hutter", "abstract": "  Activation functions are fundamental elements of deep learning architectures\nas they significantly influence training dynamics. ReLU, while widely used, is\nprone to the dying neuron problem, which has been mitigated by variants such as\nLeakyReLU, PReLU, and ELU that better handle negative neuron outputs. Recently,\nself-gated activations like GELU and Swish have emerged as state-of-the-art\nalternatives, leveraging their smoothness to ensure stable gradient flow and\nprevent neuron inactivity. In this work, we introduce the Gompertz Linear Unit\n(GoLU), a novel self-gated activation function defined as $\\mathrm{GoLU}(x) = x\n\\, \\mathrm{Gompertz}(x)$, where $\\mathrm{Gompertz}(x) = e^{-e^{-x}}$. The GoLU\nactivation leverages the right-skewed asymmetry in the Gompertz function to\nreduce variance in the latent space more effectively compared to GELU and\nSwish, while preserving robust gradient flow. Extensive experiments across\ndiverse tasks, including Image Classification, Language Modeling, Semantic\nSegmentation, Object Detection, Instance Segmentation, and Diffusion, highlight\nGoLU's superior performance relative to state-of-the-art activation functions,\nestablishing GoLU as a robust alternative to existing activation functions.\n", "link": "http://arxiv.org/abs/2502.03654v2", "date": "2025-05-21", "relevancy": 2.5029, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5162}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4935}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gompertz%20Linear%20Units%3A%20Leveraging%20Asymmetry%20for%20Enhanced%20Learning%0A%20%20Dynamics&body=Title%3A%20Gompertz%20Linear%20Units%3A%20Leveraging%20Asymmetry%20for%20Enhanced%20Learning%0A%20%20Dynamics%0AAuthor%3A%20Indrashis%20Das%20and%20Mahmoud%20Safari%20and%20Steven%20Adriaensen%20and%20Frank%20Hutter%0AAbstract%3A%20%20%20Activation%20functions%20are%20fundamental%20elements%20of%20deep%20learning%20architectures%0Aas%20they%20significantly%20influence%20training%20dynamics.%20ReLU%2C%20while%20widely%20used%2C%20is%0Aprone%20to%20the%20dying%20neuron%20problem%2C%20which%20has%20been%20mitigated%20by%20variants%20such%20as%0ALeakyReLU%2C%20PReLU%2C%20and%20ELU%20that%20better%20handle%20negative%20neuron%20outputs.%20Recently%2C%0Aself-gated%20activations%20like%20GELU%20and%20Swish%20have%20emerged%20as%20state-of-the-art%0Aalternatives%2C%20leveraging%20their%20smoothness%20to%20ensure%20stable%20gradient%20flow%20and%0Aprevent%20neuron%20inactivity.%20In%20this%20work%2C%20we%20introduce%20the%20Gompertz%20Linear%20Unit%0A%28GoLU%29%2C%20a%20novel%20self-gated%20activation%20function%20defined%20as%20%24%5Cmathrm%7BGoLU%7D%28x%29%20%3D%20x%0A%5C%2C%20%5Cmathrm%7BGompertz%7D%28x%29%24%2C%20where%20%24%5Cmathrm%7BGompertz%7D%28x%29%20%3D%20e%5E%7B-e%5E%7B-x%7D%7D%24.%20The%20GoLU%0Aactivation%20leverages%20the%20right-skewed%20asymmetry%20in%20the%20Gompertz%20function%20to%0Areduce%20variance%20in%20the%20latent%20space%20more%20effectively%20compared%20to%20GELU%20and%0ASwish%2C%20while%20preserving%20robust%20gradient%20flow.%20Extensive%20experiments%20across%0Adiverse%20tasks%2C%20including%20Image%20Classification%2C%20Language%20Modeling%2C%20Semantic%0ASegmentation%2C%20Object%20Detection%2C%20Instance%20Segmentation%2C%20and%20Diffusion%2C%20highlight%0AGoLU%27s%20superior%20performance%20relative%20to%20state-of-the-art%20activation%20functions%2C%0Aestablishing%20GoLU%20as%20a%20robust%20alternative%20to%20existing%20activation%20functions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03654v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGompertz%2520Linear%2520Units%253A%2520Leveraging%2520Asymmetry%2520for%2520Enhanced%2520Learning%250A%2520%2520Dynamics%26entry.906535625%3DIndrashis%2520Das%2520and%2520Mahmoud%2520Safari%2520and%2520Steven%2520Adriaensen%2520and%2520Frank%2520Hutter%26entry.1292438233%3D%2520%2520Activation%2520functions%2520are%2520fundamental%2520elements%2520of%2520deep%2520learning%2520architectures%250Aas%2520they%2520significantly%2520influence%2520training%2520dynamics.%2520ReLU%252C%2520while%2520widely%2520used%252C%2520is%250Aprone%2520to%2520the%2520dying%2520neuron%2520problem%252C%2520which%2520has%2520been%2520mitigated%2520by%2520variants%2520such%2520as%250ALeakyReLU%252C%2520PReLU%252C%2520and%2520ELU%2520that%2520better%2520handle%2520negative%2520neuron%2520outputs.%2520Recently%252C%250Aself-gated%2520activations%2520like%2520GELU%2520and%2520Swish%2520have%2520emerged%2520as%2520state-of-the-art%250Aalternatives%252C%2520leveraging%2520their%2520smoothness%2520to%2520ensure%2520stable%2520gradient%2520flow%2520and%250Aprevent%2520neuron%2520inactivity.%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520Gompertz%2520Linear%2520Unit%250A%2528GoLU%2529%252C%2520a%2520novel%2520self-gated%2520activation%2520function%2520defined%2520as%2520%2524%255Cmathrm%257BGoLU%257D%2528x%2529%2520%253D%2520x%250A%255C%252C%2520%255Cmathrm%257BGompertz%257D%2528x%2529%2524%252C%2520where%2520%2524%255Cmathrm%257BGompertz%257D%2528x%2529%2520%253D%2520e%255E%257B-e%255E%257B-x%257D%257D%2524.%2520The%2520GoLU%250Aactivation%2520leverages%2520the%2520right-skewed%2520asymmetry%2520in%2520the%2520Gompertz%2520function%2520to%250Areduce%2520variance%2520in%2520the%2520latent%2520space%2520more%2520effectively%2520compared%2520to%2520GELU%2520and%250ASwish%252C%2520while%2520preserving%2520robust%2520gradient%2520flow.%2520Extensive%2520experiments%2520across%250Adiverse%2520tasks%252C%2520including%2520Image%2520Classification%252C%2520Language%2520Modeling%252C%2520Semantic%250ASegmentation%252C%2520Object%2520Detection%252C%2520Instance%2520Segmentation%252C%2520and%2520Diffusion%252C%2520highlight%250AGoLU%2527s%2520superior%2520performance%2520relative%2520to%2520state-of-the-art%2520activation%2520functions%252C%250Aestablishing%2520GoLU%2520as%2520a%2520robust%2520alternative%2520to%2520existing%2520activation%2520functions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03654v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gompertz%20Linear%20Units%3A%20Leveraging%20Asymmetry%20for%20Enhanced%20Learning%0A%20%20Dynamics&entry.906535625=Indrashis%20Das%20and%20Mahmoud%20Safari%20and%20Steven%20Adriaensen%20and%20Frank%20Hutter&entry.1292438233=%20%20Activation%20functions%20are%20fundamental%20elements%20of%20deep%20learning%20architectures%0Aas%20they%20significantly%20influence%20training%20dynamics.%20ReLU%2C%20while%20widely%20used%2C%20is%0Aprone%20to%20the%20dying%20neuron%20problem%2C%20which%20has%20been%20mitigated%20by%20variants%20such%20as%0ALeakyReLU%2C%20PReLU%2C%20and%20ELU%20that%20better%20handle%20negative%20neuron%20outputs.%20Recently%2C%0Aself-gated%20activations%20like%20GELU%20and%20Swish%20have%20emerged%20as%20state-of-the-art%0Aalternatives%2C%20leveraging%20their%20smoothness%20to%20ensure%20stable%20gradient%20flow%20and%0Aprevent%20neuron%20inactivity.%20In%20this%20work%2C%20we%20introduce%20the%20Gompertz%20Linear%20Unit%0A%28GoLU%29%2C%20a%20novel%20self-gated%20activation%20function%20defined%20as%20%24%5Cmathrm%7BGoLU%7D%28x%29%20%3D%20x%0A%5C%2C%20%5Cmathrm%7BGompertz%7D%28x%29%24%2C%20where%20%24%5Cmathrm%7BGompertz%7D%28x%29%20%3D%20e%5E%7B-e%5E%7B-x%7D%7D%24.%20The%20GoLU%0Aactivation%20leverages%20the%20right-skewed%20asymmetry%20in%20the%20Gompertz%20function%20to%0Areduce%20variance%20in%20the%20latent%20space%20more%20effectively%20compared%20to%20GELU%20and%0ASwish%2C%20while%20preserving%20robust%20gradient%20flow.%20Extensive%20experiments%20across%0Adiverse%20tasks%2C%20including%20Image%20Classification%2C%20Language%20Modeling%2C%20Semantic%0ASegmentation%2C%20Object%20Detection%2C%20Instance%20Segmentation%2C%20and%20Diffusion%2C%20highlight%0AGoLU%27s%20superior%20performance%20relative%20to%20state-of-the-art%20activation%20functions%2C%0Aestablishing%20GoLU%20as%20a%20robust%20alternative%20to%20existing%20activation%20functions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03654v2&entry.124074799=Read"},
{"title": "Antimatter Annihilation Vertex Reconstruction with Deep Learning for\n  ALPHA-g Radial Time Projection Chamber", "author": "Ashley Ferreira and Mahip Singh and Yukiya Saito and Andrea Capra and Ina Carli and Daniel Duque Quiceno and Wojciech T. Fedorko and Makoto C. Fujiwara and Muyan Li and Lars Martin and Gareth Smith and Anqui Xu", "abstract": "  The ALPHA-g experiment at CERN aims to precisely measure the terrestrial\ngravitational acceleration of antihydrogen atoms. A radial Time Projection\nChamber (rTPC), that surrounds the ALPHA-g magnetic trap, is employed to\ndetermine the annihilation location, called the vertex. The standard approach\nrequires identifying the trajectories of the ionizing particles in the rTPC\nfrom the location of their interaction in the gas (spacepoints), and inferring\nthe vertex positions by finding the point where those trajectories (helices)\npass closest to one another. In this work, we present a novel approach to\nvertex reconstruction using an ensemble of models based on the PointNet deep\nlearning architecture. The newly developed model, PointNet Ensemble for\nAnnihilation Reconstruction (PEAR), directly learns the relation between the\nlocation of the vertices and the rTPC spacepoints, thus eliminating the need to\nidentify and fit the particle tracks. PEAR shows strong performance in\nreconstructing vertical vertex positions from simulated data, that is superior\nto the standard approach for all metrics considered. Furthermore, the deep\nlearning approach can reconstruct the vertical vertex position when the\nstandard approach fails.\n", "link": "http://arxiv.org/abs/2502.12169v2", "date": "2025-05-21", "relevancy": 2.4968, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5157}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.492}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Antimatter%20Annihilation%20Vertex%20Reconstruction%20with%20Deep%20Learning%20for%0A%20%20ALPHA-g%20Radial%20Time%20Projection%20Chamber&body=Title%3A%20Antimatter%20Annihilation%20Vertex%20Reconstruction%20with%20Deep%20Learning%20for%0A%20%20ALPHA-g%20Radial%20Time%20Projection%20Chamber%0AAuthor%3A%20Ashley%20Ferreira%20and%20Mahip%20Singh%20and%20Yukiya%20Saito%20and%20Andrea%20Capra%20and%20Ina%20Carli%20and%20Daniel%20Duque%20Quiceno%20and%20Wojciech%20T.%20Fedorko%20and%20Makoto%20C.%20Fujiwara%20and%20Muyan%20Li%20and%20Lars%20Martin%20and%20Gareth%20Smith%20and%20Anqui%20Xu%0AAbstract%3A%20%20%20The%20ALPHA-g%20experiment%20at%20CERN%20aims%20to%20precisely%20measure%20the%20terrestrial%0Agravitational%20acceleration%20of%20antihydrogen%20atoms.%20A%20radial%20Time%20Projection%0AChamber%20%28rTPC%29%2C%20that%20surrounds%20the%20ALPHA-g%20magnetic%20trap%2C%20is%20employed%20to%0Adetermine%20the%20annihilation%20location%2C%20called%20the%20vertex.%20The%20standard%20approach%0Arequires%20identifying%20the%20trajectories%20of%20the%20ionizing%20particles%20in%20the%20rTPC%0Afrom%20the%20location%20of%20their%20interaction%20in%20the%20gas%20%28spacepoints%29%2C%20and%20inferring%0Athe%20vertex%20positions%20by%20finding%20the%20point%20where%20those%20trajectories%20%28helices%29%0Apass%20closest%20to%20one%20another.%20In%20this%20work%2C%20we%20present%20a%20novel%20approach%20to%0Avertex%20reconstruction%20using%20an%20ensemble%20of%20models%20based%20on%20the%20PointNet%20deep%0Alearning%20architecture.%20The%20newly%20developed%20model%2C%20PointNet%20Ensemble%20for%0AAnnihilation%20Reconstruction%20%28PEAR%29%2C%20directly%20learns%20the%20relation%20between%20the%0Alocation%20of%20the%20vertices%20and%20the%20rTPC%20spacepoints%2C%20thus%20eliminating%20the%20need%20to%0Aidentify%20and%20fit%20the%20particle%20tracks.%20PEAR%20shows%20strong%20performance%20in%0Areconstructing%20vertical%20vertex%20positions%20from%20simulated%20data%2C%20that%20is%20superior%0Ato%20the%20standard%20approach%20for%20all%20metrics%20considered.%20Furthermore%2C%20the%20deep%0Alearning%20approach%20can%20reconstruct%20the%20vertical%20vertex%20position%20when%20the%0Astandard%20approach%20fails.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12169v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAntimatter%2520Annihilation%2520Vertex%2520Reconstruction%2520with%2520Deep%2520Learning%2520for%250A%2520%2520ALPHA-g%2520Radial%2520Time%2520Projection%2520Chamber%26entry.906535625%3DAshley%2520Ferreira%2520and%2520Mahip%2520Singh%2520and%2520Yukiya%2520Saito%2520and%2520Andrea%2520Capra%2520and%2520Ina%2520Carli%2520and%2520Daniel%2520Duque%2520Quiceno%2520and%2520Wojciech%2520T.%2520Fedorko%2520and%2520Makoto%2520C.%2520Fujiwara%2520and%2520Muyan%2520Li%2520and%2520Lars%2520Martin%2520and%2520Gareth%2520Smith%2520and%2520Anqui%2520Xu%26entry.1292438233%3D%2520%2520The%2520ALPHA-g%2520experiment%2520at%2520CERN%2520aims%2520to%2520precisely%2520measure%2520the%2520terrestrial%250Agravitational%2520acceleration%2520of%2520antihydrogen%2520atoms.%2520A%2520radial%2520Time%2520Projection%250AChamber%2520%2528rTPC%2529%252C%2520that%2520surrounds%2520the%2520ALPHA-g%2520magnetic%2520trap%252C%2520is%2520employed%2520to%250Adetermine%2520the%2520annihilation%2520location%252C%2520called%2520the%2520vertex.%2520The%2520standard%2520approach%250Arequires%2520identifying%2520the%2520trajectories%2520of%2520the%2520ionizing%2520particles%2520in%2520the%2520rTPC%250Afrom%2520the%2520location%2520of%2520their%2520interaction%2520in%2520the%2520gas%2520%2528spacepoints%2529%252C%2520and%2520inferring%250Athe%2520vertex%2520positions%2520by%2520finding%2520the%2520point%2520where%2520those%2520trajectories%2520%2528helices%2529%250Apass%2520closest%2520to%2520one%2520another.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520novel%2520approach%2520to%250Avertex%2520reconstruction%2520using%2520an%2520ensemble%2520of%2520models%2520based%2520on%2520the%2520PointNet%2520deep%250Alearning%2520architecture.%2520The%2520newly%2520developed%2520model%252C%2520PointNet%2520Ensemble%2520for%250AAnnihilation%2520Reconstruction%2520%2528PEAR%2529%252C%2520directly%2520learns%2520the%2520relation%2520between%2520the%250Alocation%2520of%2520the%2520vertices%2520and%2520the%2520rTPC%2520spacepoints%252C%2520thus%2520eliminating%2520the%2520need%2520to%250Aidentify%2520and%2520fit%2520the%2520particle%2520tracks.%2520PEAR%2520shows%2520strong%2520performance%2520in%250Areconstructing%2520vertical%2520vertex%2520positions%2520from%2520simulated%2520data%252C%2520that%2520is%2520superior%250Ato%2520the%2520standard%2520approach%2520for%2520all%2520metrics%2520considered.%2520Furthermore%252C%2520the%2520deep%250Alearning%2520approach%2520can%2520reconstruct%2520the%2520vertical%2520vertex%2520position%2520when%2520the%250Astandard%2520approach%2520fails.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12169v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Antimatter%20Annihilation%20Vertex%20Reconstruction%20with%20Deep%20Learning%20for%0A%20%20ALPHA-g%20Radial%20Time%20Projection%20Chamber&entry.906535625=Ashley%20Ferreira%20and%20Mahip%20Singh%20and%20Yukiya%20Saito%20and%20Andrea%20Capra%20and%20Ina%20Carli%20and%20Daniel%20Duque%20Quiceno%20and%20Wojciech%20T.%20Fedorko%20and%20Makoto%20C.%20Fujiwara%20and%20Muyan%20Li%20and%20Lars%20Martin%20and%20Gareth%20Smith%20and%20Anqui%20Xu&entry.1292438233=%20%20The%20ALPHA-g%20experiment%20at%20CERN%20aims%20to%20precisely%20measure%20the%20terrestrial%0Agravitational%20acceleration%20of%20antihydrogen%20atoms.%20A%20radial%20Time%20Projection%0AChamber%20%28rTPC%29%2C%20that%20surrounds%20the%20ALPHA-g%20magnetic%20trap%2C%20is%20employed%20to%0Adetermine%20the%20annihilation%20location%2C%20called%20the%20vertex.%20The%20standard%20approach%0Arequires%20identifying%20the%20trajectories%20of%20the%20ionizing%20particles%20in%20the%20rTPC%0Afrom%20the%20location%20of%20their%20interaction%20in%20the%20gas%20%28spacepoints%29%2C%20and%20inferring%0Athe%20vertex%20positions%20by%20finding%20the%20point%20where%20those%20trajectories%20%28helices%29%0Apass%20closest%20to%20one%20another.%20In%20this%20work%2C%20we%20present%20a%20novel%20approach%20to%0Avertex%20reconstruction%20using%20an%20ensemble%20of%20models%20based%20on%20the%20PointNet%20deep%0Alearning%20architecture.%20The%20newly%20developed%20model%2C%20PointNet%20Ensemble%20for%0AAnnihilation%20Reconstruction%20%28PEAR%29%2C%20directly%20learns%20the%20relation%20between%20the%0Alocation%20of%20the%20vertices%20and%20the%20rTPC%20spacepoints%2C%20thus%20eliminating%20the%20need%20to%0Aidentify%20and%20fit%20the%20particle%20tracks.%20PEAR%20shows%20strong%20performance%20in%0Areconstructing%20vertical%20vertex%20positions%20from%20simulated%20data%2C%20that%20is%20superior%0Ato%20the%20standard%20approach%20for%20all%20metrics%20considered.%20Furthermore%2C%20the%20deep%0Alearning%20approach%20can%20reconstruct%20the%20vertical%20vertex%20position%20when%20the%0Astandard%20approach%20fails.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12169v2&entry.124074799=Read"},
{"title": "Learn to Reason Efficiently with Adaptive Length-based Reward Shaping", "author": "Wei Liu and Ruochen Zhou and Yiyun Deng and Yuzhen Huang and Junteng Liu and Yuntian Deng and Yizhe Zhang and Junxian He", "abstract": "  Large Reasoning Models (LRMs) have shown remarkable capabilities in solving\ncomplex problems through reinforcement learning (RL), particularly by\ngenerating long reasoning traces. However, these extended outputs often exhibit\nsubstantial redundancy, which limits the efficiency of LRMs. In this paper, we\ninvestigate RL-based approaches to promote reasoning efficiency. Specifically,\nwe first present a unified framework that formulates various efficient\nreasoning methods through the lens of length-based reward shaping. Building on\nthis perspective, we propose a novel Length-bAsed StEp Reward shaping method\n(LASER), which employs a step function as the reward, controlled by a target\nlength. LASER surpasses previous methods, achieving a superior Pareto-optimal\nbalance between performance and efficiency. Next, we further extend LASER based\non two key intuitions: (1) The reasoning behavior of the model evolves during\ntraining, necessitating reward specifications that are also adaptive and\ndynamic; (2) Rather than uniformly encouraging shorter or longer chains of\nthought (CoT), we posit that length-based reward shaping should be\ndifficulty-aware i.e., it should penalize lengthy CoTs more for easy queries.\nThis approach is expected to facilitate a combination of fast and slow\nthinking, leading to a better overall tradeoff. The resulting method is termed\nLASER-D (Dynamic and Difficulty-aware). Experiments on\nDeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and\nDeepSeek-R1-Distill-Qwen-32B show that our approach significantly enhances both\nreasoning performance and response length efficiency. For instance, LASER-D and\nits variant achieve a +6.1 improvement on AIME2024 while reducing token usage\nby 63%. Further analysis reveals our RL-based compression produces more concise\nreasoning patterns with less redundant \"self-reflections\". Resources are at\nhttps://github.com/hkust-nlp/Laser.\n", "link": "http://arxiv.org/abs/2505.15612v1", "date": "2025-05-21", "relevancy": 2.4958, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4992}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4992}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learn%20to%20Reason%20Efficiently%20with%20Adaptive%20Length-based%20Reward%20Shaping&body=Title%3A%20Learn%20to%20Reason%20Efficiently%20with%20Adaptive%20Length-based%20Reward%20Shaping%0AAuthor%3A%20Wei%20Liu%20and%20Ruochen%20Zhou%20and%20Yiyun%20Deng%20and%20Yuzhen%20Huang%20and%20Junteng%20Liu%20and%20Yuntian%20Deng%20and%20Yizhe%20Zhang%20and%20Junxian%20He%0AAbstract%3A%20%20%20Large%20Reasoning%20Models%20%28LRMs%29%20have%20shown%20remarkable%20capabilities%20in%20solving%0Acomplex%20problems%20through%20reinforcement%20learning%20%28RL%29%2C%20particularly%20by%0Agenerating%20long%20reasoning%20traces.%20However%2C%20these%20extended%20outputs%20often%20exhibit%0Asubstantial%20redundancy%2C%20which%20limits%20the%20efficiency%20of%20LRMs.%20In%20this%20paper%2C%20we%0Ainvestigate%20RL-based%20approaches%20to%20promote%20reasoning%20efficiency.%20Specifically%2C%0Awe%20first%20present%20a%20unified%20framework%20that%20formulates%20various%20efficient%0Areasoning%20methods%20through%20the%20lens%20of%20length-based%20reward%20shaping.%20Building%20on%0Athis%20perspective%2C%20we%20propose%20a%20novel%20Length-bAsed%20StEp%20Reward%20shaping%20method%0A%28LASER%29%2C%20which%20employs%20a%20step%20function%20as%20the%20reward%2C%20controlled%20by%20a%20target%0Alength.%20LASER%20surpasses%20previous%20methods%2C%20achieving%20a%20superior%20Pareto-optimal%0Abalance%20between%20performance%20and%20efficiency.%20Next%2C%20we%20further%20extend%20LASER%20based%0Aon%20two%20key%20intuitions%3A%20%281%29%20The%20reasoning%20behavior%20of%20the%20model%20evolves%20during%0Atraining%2C%20necessitating%20reward%20specifications%20that%20are%20also%20adaptive%20and%0Adynamic%3B%20%282%29%20Rather%20than%20uniformly%20encouraging%20shorter%20or%20longer%20chains%20of%0Athought%20%28CoT%29%2C%20we%20posit%20that%20length-based%20reward%20shaping%20should%20be%0Adifficulty-aware%20i.e.%2C%20it%20should%20penalize%20lengthy%20CoTs%20more%20for%20easy%20queries.%0AThis%20approach%20is%20expected%20to%20facilitate%20a%20combination%20of%20fast%20and%20slow%0Athinking%2C%20leading%20to%20a%20better%20overall%20tradeoff.%20The%20resulting%20method%20is%20termed%0ALASER-D%20%28Dynamic%20and%20Difficulty-aware%29.%20Experiments%20on%0ADeepSeek-R1-Distill-Qwen-1.5B%2C%20DeepSeek-R1-Distill-Qwen-7B%2C%20and%0ADeepSeek-R1-Distill-Qwen-32B%20show%20that%20our%20approach%20significantly%20enhances%20both%0Areasoning%20performance%20and%20response%20length%20efficiency.%20For%20instance%2C%20LASER-D%20and%0Aits%20variant%20achieve%20a%20%2B6.1%20improvement%20on%20AIME2024%20while%20reducing%20token%20usage%0Aby%2063%25.%20Further%20analysis%20reveals%20our%20RL-based%20compression%20produces%20more%20concise%0Areasoning%20patterns%20with%20less%20redundant%20%22self-reflections%22.%20Resources%20are%20at%0Ahttps%3A//github.com/hkust-nlp/Laser.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15612v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearn%2520to%2520Reason%2520Efficiently%2520with%2520Adaptive%2520Length-based%2520Reward%2520Shaping%26entry.906535625%3DWei%2520Liu%2520and%2520Ruochen%2520Zhou%2520and%2520Yiyun%2520Deng%2520and%2520Yuzhen%2520Huang%2520and%2520Junteng%2520Liu%2520and%2520Yuntian%2520Deng%2520and%2520Yizhe%2520Zhang%2520and%2520Junxian%2520He%26entry.1292438233%3D%2520%2520Large%2520Reasoning%2520Models%2520%2528LRMs%2529%2520have%2520shown%2520remarkable%2520capabilities%2520in%2520solving%250Acomplex%2520problems%2520through%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520particularly%2520by%250Agenerating%2520long%2520reasoning%2520traces.%2520However%252C%2520these%2520extended%2520outputs%2520often%2520exhibit%250Asubstantial%2520redundancy%252C%2520which%2520limits%2520the%2520efficiency%2520of%2520LRMs.%2520In%2520this%2520paper%252C%2520we%250Ainvestigate%2520RL-based%2520approaches%2520to%2520promote%2520reasoning%2520efficiency.%2520Specifically%252C%250Awe%2520first%2520present%2520a%2520unified%2520framework%2520that%2520formulates%2520various%2520efficient%250Areasoning%2520methods%2520through%2520the%2520lens%2520of%2520length-based%2520reward%2520shaping.%2520Building%2520on%250Athis%2520perspective%252C%2520we%2520propose%2520a%2520novel%2520Length-bAsed%2520StEp%2520Reward%2520shaping%2520method%250A%2528LASER%2529%252C%2520which%2520employs%2520a%2520step%2520function%2520as%2520the%2520reward%252C%2520controlled%2520by%2520a%2520target%250Alength.%2520LASER%2520surpasses%2520previous%2520methods%252C%2520achieving%2520a%2520superior%2520Pareto-optimal%250Abalance%2520between%2520performance%2520and%2520efficiency.%2520Next%252C%2520we%2520further%2520extend%2520LASER%2520based%250Aon%2520two%2520key%2520intuitions%253A%2520%25281%2529%2520The%2520reasoning%2520behavior%2520of%2520the%2520model%2520evolves%2520during%250Atraining%252C%2520necessitating%2520reward%2520specifications%2520that%2520are%2520also%2520adaptive%2520and%250Adynamic%253B%2520%25282%2529%2520Rather%2520than%2520uniformly%2520encouraging%2520shorter%2520or%2520longer%2520chains%2520of%250Athought%2520%2528CoT%2529%252C%2520we%2520posit%2520that%2520length-based%2520reward%2520shaping%2520should%2520be%250Adifficulty-aware%2520i.e.%252C%2520it%2520should%2520penalize%2520lengthy%2520CoTs%2520more%2520for%2520easy%2520queries.%250AThis%2520approach%2520is%2520expected%2520to%2520facilitate%2520a%2520combination%2520of%2520fast%2520and%2520slow%250Athinking%252C%2520leading%2520to%2520a%2520better%2520overall%2520tradeoff.%2520The%2520resulting%2520method%2520is%2520termed%250ALASER-D%2520%2528Dynamic%2520and%2520Difficulty-aware%2529.%2520Experiments%2520on%250ADeepSeek-R1-Distill-Qwen-1.5B%252C%2520DeepSeek-R1-Distill-Qwen-7B%252C%2520and%250ADeepSeek-R1-Distill-Qwen-32B%2520show%2520that%2520our%2520approach%2520significantly%2520enhances%2520both%250Areasoning%2520performance%2520and%2520response%2520length%2520efficiency.%2520For%2520instance%252C%2520LASER-D%2520and%250Aits%2520variant%2520achieve%2520a%2520%252B6.1%2520improvement%2520on%2520AIME2024%2520while%2520reducing%2520token%2520usage%250Aby%252063%2525.%2520Further%2520analysis%2520reveals%2520our%2520RL-based%2520compression%2520produces%2520more%2520concise%250Areasoning%2520patterns%2520with%2520less%2520redundant%2520%2522self-reflections%2522.%2520Resources%2520are%2520at%250Ahttps%253A//github.com/hkust-nlp/Laser.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15612v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learn%20to%20Reason%20Efficiently%20with%20Adaptive%20Length-based%20Reward%20Shaping&entry.906535625=Wei%20Liu%20and%20Ruochen%20Zhou%20and%20Yiyun%20Deng%20and%20Yuzhen%20Huang%20and%20Junteng%20Liu%20and%20Yuntian%20Deng%20and%20Yizhe%20Zhang%20and%20Junxian%20He&entry.1292438233=%20%20Large%20Reasoning%20Models%20%28LRMs%29%20have%20shown%20remarkable%20capabilities%20in%20solving%0Acomplex%20problems%20through%20reinforcement%20learning%20%28RL%29%2C%20particularly%20by%0Agenerating%20long%20reasoning%20traces.%20However%2C%20these%20extended%20outputs%20often%20exhibit%0Asubstantial%20redundancy%2C%20which%20limits%20the%20efficiency%20of%20LRMs.%20In%20this%20paper%2C%20we%0Ainvestigate%20RL-based%20approaches%20to%20promote%20reasoning%20efficiency.%20Specifically%2C%0Awe%20first%20present%20a%20unified%20framework%20that%20formulates%20various%20efficient%0Areasoning%20methods%20through%20the%20lens%20of%20length-based%20reward%20shaping.%20Building%20on%0Athis%20perspective%2C%20we%20propose%20a%20novel%20Length-bAsed%20StEp%20Reward%20shaping%20method%0A%28LASER%29%2C%20which%20employs%20a%20step%20function%20as%20the%20reward%2C%20controlled%20by%20a%20target%0Alength.%20LASER%20surpasses%20previous%20methods%2C%20achieving%20a%20superior%20Pareto-optimal%0Abalance%20between%20performance%20and%20efficiency.%20Next%2C%20we%20further%20extend%20LASER%20based%0Aon%20two%20key%20intuitions%3A%20%281%29%20The%20reasoning%20behavior%20of%20the%20model%20evolves%20during%0Atraining%2C%20necessitating%20reward%20specifications%20that%20are%20also%20adaptive%20and%0Adynamic%3B%20%282%29%20Rather%20than%20uniformly%20encouraging%20shorter%20or%20longer%20chains%20of%0Athought%20%28CoT%29%2C%20we%20posit%20that%20length-based%20reward%20shaping%20should%20be%0Adifficulty-aware%20i.e.%2C%20it%20should%20penalize%20lengthy%20CoTs%20more%20for%20easy%20queries.%0AThis%20approach%20is%20expected%20to%20facilitate%20a%20combination%20of%20fast%20and%20slow%0Athinking%2C%20leading%20to%20a%20better%20overall%20tradeoff.%20The%20resulting%20method%20is%20termed%0ALASER-D%20%28Dynamic%20and%20Difficulty-aware%29.%20Experiments%20on%0ADeepSeek-R1-Distill-Qwen-1.5B%2C%20DeepSeek-R1-Distill-Qwen-7B%2C%20and%0ADeepSeek-R1-Distill-Qwen-32B%20show%20that%20our%20approach%20significantly%20enhances%20both%0Areasoning%20performance%20and%20response%20length%20efficiency.%20For%20instance%2C%20LASER-D%20and%0Aits%20variant%20achieve%20a%20%2B6.1%20improvement%20on%20AIME2024%20while%20reducing%20token%20usage%0Aby%2063%25.%20Further%20analysis%20reveals%20our%20RL-based%20compression%20produces%20more%20concise%0Areasoning%20patterns%20with%20less%20redundant%20%22self-reflections%22.%20Resources%20are%20at%0Ahttps%3A//github.com/hkust-nlp/Laser.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15612v1&entry.124074799=Read"},
{"title": "Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark\n  Study", "author": "DongGeon Lee and Joonwon Jang and Jihae Jeong and Hwanjo Yu", "abstract": "  Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet\nmost evaluations rely on artificial images. This study asks: How safe are\ncurrent VLMs when confronted with meme images that ordinary users share? To\ninvestigate this question, we introduce MemeSafetyBench, a 50,430-instance\nbenchmark pairing real meme images with both harmful and benign instructions.\nUsing a comprehensive safety taxonomy and LLM-based instruction generation, we\nassess multiple VLMs across single and multi-turn interactions. We investigate\nhow real-world memes influence harmful outputs, the mitigating effects of\nconversational context, and the relationship between model scale and safety\nmetrics. Our findings demonstrate that VLMs show greater vulnerability to\nmeme-based harmful prompts than to synthetic or typographic images. Memes\nsignificantly increase harmful responses and decrease refusals compared to\ntext-only inputs. Though multi-turn interactions provide partial mitigation,\nelevated vulnerability persists. These results highlight the need for\necologically valid evaluations and stronger safety mechanisms.\n", "link": "http://arxiv.org/abs/2505.15389v1", "date": "2025-05-21", "relevancy": 2.4711, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5035}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5035}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Vision-Language%20Models%20Safe%20in%20the%20Wild%3F%20A%20Meme-Based%20Benchmark%0A%20%20Study&body=Title%3A%20Are%20Vision-Language%20Models%20Safe%20in%20the%20Wild%3F%20A%20Meme-Based%20Benchmark%0A%20%20Study%0AAuthor%3A%20DongGeon%20Lee%20and%20Joonwon%20Jang%20and%20Jihae%20Jeong%20and%20Hwanjo%20Yu%0AAbstract%3A%20%20%20Rapid%20deployment%20of%20vision-language%20models%20%28VLMs%29%20magnifies%20safety%20risks%2C%20yet%0Amost%20evaluations%20rely%20on%20artificial%20images.%20This%20study%20asks%3A%20How%20safe%20are%0Acurrent%20VLMs%20when%20confronted%20with%20meme%20images%20that%20ordinary%20users%20share%3F%20To%0Ainvestigate%20this%20question%2C%20we%20introduce%20MemeSafetyBench%2C%20a%2050%2C430-instance%0Abenchmark%20pairing%20real%20meme%20images%20with%20both%20harmful%20and%20benign%20instructions.%0AUsing%20a%20comprehensive%20safety%20taxonomy%20and%20LLM-based%20instruction%20generation%2C%20we%0Aassess%20multiple%20VLMs%20across%20single%20and%20multi-turn%20interactions.%20We%20investigate%0Ahow%20real-world%20memes%20influence%20harmful%20outputs%2C%20the%20mitigating%20effects%20of%0Aconversational%20context%2C%20and%20the%20relationship%20between%20model%20scale%20and%20safety%0Ametrics.%20Our%20findings%20demonstrate%20that%20VLMs%20show%20greater%20vulnerability%20to%0Ameme-based%20harmful%20prompts%20than%20to%20synthetic%20or%20typographic%20images.%20Memes%0Asignificantly%20increase%20harmful%20responses%20and%20decrease%20refusals%20compared%20to%0Atext-only%20inputs.%20Though%20multi-turn%20interactions%20provide%20partial%20mitigation%2C%0Aelevated%20vulnerability%20persists.%20These%20results%20highlight%20the%20need%20for%0Aecologically%20valid%20evaluations%20and%20stronger%20safety%20mechanisms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15389v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Vision-Language%2520Models%2520Safe%2520in%2520the%2520Wild%253F%2520A%2520Meme-Based%2520Benchmark%250A%2520%2520Study%26entry.906535625%3DDongGeon%2520Lee%2520and%2520Joonwon%2520Jang%2520and%2520Jihae%2520Jeong%2520and%2520Hwanjo%2520Yu%26entry.1292438233%3D%2520%2520Rapid%2520deployment%2520of%2520vision-language%2520models%2520%2528VLMs%2529%2520magnifies%2520safety%2520risks%252C%2520yet%250Amost%2520evaluations%2520rely%2520on%2520artificial%2520images.%2520This%2520study%2520asks%253A%2520How%2520safe%2520are%250Acurrent%2520VLMs%2520when%2520confronted%2520with%2520meme%2520images%2520that%2520ordinary%2520users%2520share%253F%2520To%250Ainvestigate%2520this%2520question%252C%2520we%2520introduce%2520MemeSafetyBench%252C%2520a%252050%252C430-instance%250Abenchmark%2520pairing%2520real%2520meme%2520images%2520with%2520both%2520harmful%2520and%2520benign%2520instructions.%250AUsing%2520a%2520comprehensive%2520safety%2520taxonomy%2520and%2520LLM-based%2520instruction%2520generation%252C%2520we%250Aassess%2520multiple%2520VLMs%2520across%2520single%2520and%2520multi-turn%2520interactions.%2520We%2520investigate%250Ahow%2520real-world%2520memes%2520influence%2520harmful%2520outputs%252C%2520the%2520mitigating%2520effects%2520of%250Aconversational%2520context%252C%2520and%2520the%2520relationship%2520between%2520model%2520scale%2520and%2520safety%250Ametrics.%2520Our%2520findings%2520demonstrate%2520that%2520VLMs%2520show%2520greater%2520vulnerability%2520to%250Ameme-based%2520harmful%2520prompts%2520than%2520to%2520synthetic%2520or%2520typographic%2520images.%2520Memes%250Asignificantly%2520increase%2520harmful%2520responses%2520and%2520decrease%2520refusals%2520compared%2520to%250Atext-only%2520inputs.%2520Though%2520multi-turn%2520interactions%2520provide%2520partial%2520mitigation%252C%250Aelevated%2520vulnerability%2520persists.%2520These%2520results%2520highlight%2520the%2520need%2520for%250Aecologically%2520valid%2520evaluations%2520and%2520stronger%2520safety%2520mechanisms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15389v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Vision-Language%20Models%20Safe%20in%20the%20Wild%3F%20A%20Meme-Based%20Benchmark%0A%20%20Study&entry.906535625=DongGeon%20Lee%20and%20Joonwon%20Jang%20and%20Jihae%20Jeong%20and%20Hwanjo%20Yu&entry.1292438233=%20%20Rapid%20deployment%20of%20vision-language%20models%20%28VLMs%29%20magnifies%20safety%20risks%2C%20yet%0Amost%20evaluations%20rely%20on%20artificial%20images.%20This%20study%20asks%3A%20How%20safe%20are%0Acurrent%20VLMs%20when%20confronted%20with%20meme%20images%20that%20ordinary%20users%20share%3F%20To%0Ainvestigate%20this%20question%2C%20we%20introduce%20MemeSafetyBench%2C%20a%2050%2C430-instance%0Abenchmark%20pairing%20real%20meme%20images%20with%20both%20harmful%20and%20benign%20instructions.%0AUsing%20a%20comprehensive%20safety%20taxonomy%20and%20LLM-based%20instruction%20generation%2C%20we%0Aassess%20multiple%20VLMs%20across%20single%20and%20multi-turn%20interactions.%20We%20investigate%0Ahow%20real-world%20memes%20influence%20harmful%20outputs%2C%20the%20mitigating%20effects%20of%0Aconversational%20context%2C%20and%20the%20relationship%20between%20model%20scale%20and%20safety%0Ametrics.%20Our%20findings%20demonstrate%20that%20VLMs%20show%20greater%20vulnerability%20to%0Ameme-based%20harmful%20prompts%20than%20to%20synthetic%20or%20typographic%20images.%20Memes%0Asignificantly%20increase%20harmful%20responses%20and%20decrease%20refusals%20compared%20to%0Atext-only%20inputs.%20Though%20multi-turn%20interactions%20provide%20partial%20mitigation%2C%0Aelevated%20vulnerability%20persists.%20These%20results%20highlight%20the%20need%20for%0Aecologically%20valid%20evaluations%20and%20stronger%20safety%20mechanisms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15389v1&entry.124074799=Read"},
{"title": "Benchmarking Post-Training Quantization in LLMs: Comprehensive Taxonomy,\n  Unified Evaluation, and Comparative Analysis", "author": "Jiaqi Zhao and Ming Wang and Miao Zhang and Yuzhang Shang and Xuebo Liu and Yaowei Wang and Min Zhang and Liqiang Nie", "abstract": "  Post-training Quantization (PTQ) technique has been extensively adopted for\nlarge language models (LLMs) compression owing to its efficiency and low\nresource requirement. However, current research lacks a in-depth analysis of\nthe superior and applicable scenarios of each PTQ strategy. In addition,\nexisting algorithms focus primarily on performance, overlooking the trade-off\namong model size, performance, and quantization bitwidth. To mitigate these\nconfusions, we provide a novel benchmark for LLMs PTQ in this paper. Firstly,\nin order to support our benchmark, we propose a comprehensive taxonomy for\nexisting mainstream methods by scrutinizing their computational strategies\n(e.g., optimization-based, compensation-based, etc.). Then, we conduct\nextensive experiments with the baseline within each class, covering models with\nvarious sizes (7B-70B), bitwidths, training levels (LLaMA1/2/3/3.1),\narchitectures (Mixtral, DeepSeekMoE and Mamba) and modality (LLaVA1.5 and\nVILA1.5) on a wide range of evaluation metrics.Through comparative analysis on\nthe results, we summarize the superior of each PTQ strategy and\nmodelsize-bitwidth trade-off considering the performance. For example, our\nbenchmark reveals that compensation-based technique demonstrates outstanding\ncross-architecture robustness and extremely low-bit PTQ for ultra large models\nshould be reexamined. Finally, we further accordingly claim that a practical\ncombination of compensation and other PTQ strategy can achieve SOTA various\nrobustness. We believe that our benchmark will provide valuable recommendations\nfor the deployment of LLMs and future research on PTQ approaches.We conduct an\nrepository for our benchmark at https://github.com/zjq0455/PTQ_Benchmark.\n", "link": "http://arxiv.org/abs/2502.13178v4", "date": "2025-05-21", "relevancy": 2.4674, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5064}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5064}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Post-Training%20Quantization%20in%20LLMs%3A%20Comprehensive%20Taxonomy%2C%0A%20%20Unified%20Evaluation%2C%20and%20Comparative%20Analysis&body=Title%3A%20Benchmarking%20Post-Training%20Quantization%20in%20LLMs%3A%20Comprehensive%20Taxonomy%2C%0A%20%20Unified%20Evaluation%2C%20and%20Comparative%20Analysis%0AAuthor%3A%20Jiaqi%20Zhao%20and%20Ming%20Wang%20and%20Miao%20Zhang%20and%20Yuzhang%20Shang%20and%20Xuebo%20Liu%20and%20Yaowei%20Wang%20and%20Min%20Zhang%20and%20Liqiang%20Nie%0AAbstract%3A%20%20%20Post-training%20Quantization%20%28PTQ%29%20technique%20has%20been%20extensively%20adopted%20for%0Alarge%20language%20models%20%28LLMs%29%20compression%20owing%20to%20its%20efficiency%20and%20low%0Aresource%20requirement.%20However%2C%20current%20research%20lacks%20a%20in-depth%20analysis%20of%0Athe%20superior%20and%20applicable%20scenarios%20of%20each%20PTQ%20strategy.%20In%20addition%2C%0Aexisting%20algorithms%20focus%20primarily%20on%20performance%2C%20overlooking%20the%20trade-off%0Aamong%20model%20size%2C%20performance%2C%20and%20quantization%20bitwidth.%20To%20mitigate%20these%0Aconfusions%2C%20we%20provide%20a%20novel%20benchmark%20for%20LLMs%20PTQ%20in%20this%20paper.%20Firstly%2C%0Ain%20order%20to%20support%20our%20benchmark%2C%20we%20propose%20a%20comprehensive%20taxonomy%20for%0Aexisting%20mainstream%20methods%20by%20scrutinizing%20their%20computational%20strategies%0A%28e.g.%2C%20optimization-based%2C%20compensation-based%2C%20etc.%29.%20Then%2C%20we%20conduct%0Aextensive%20experiments%20with%20the%20baseline%20within%20each%20class%2C%20covering%20models%20with%0Avarious%20sizes%20%287B-70B%29%2C%20bitwidths%2C%20training%20levels%20%28LLaMA1/2/3/3.1%29%2C%0Aarchitectures%20%28Mixtral%2C%20DeepSeekMoE%20and%20Mamba%29%20and%20modality%20%28LLaVA1.5%20and%0AVILA1.5%29%20on%20a%20wide%20range%20of%20evaluation%20metrics.Through%20comparative%20analysis%20on%0Athe%20results%2C%20we%20summarize%20the%20superior%20of%20each%20PTQ%20strategy%20and%0Amodelsize-bitwidth%20trade-off%20considering%20the%20performance.%20For%20example%2C%20our%0Abenchmark%20reveals%20that%20compensation-based%20technique%20demonstrates%20outstanding%0Across-architecture%20robustness%20and%20extremely%20low-bit%20PTQ%20for%20ultra%20large%20models%0Ashould%20be%20reexamined.%20Finally%2C%20we%20further%20accordingly%20claim%20that%20a%20practical%0Acombination%20of%20compensation%20and%20other%20PTQ%20strategy%20can%20achieve%20SOTA%20various%0Arobustness.%20We%20believe%20that%20our%20benchmark%20will%20provide%20valuable%20recommendations%0Afor%20the%20deployment%20of%20LLMs%20and%20future%20research%20on%20PTQ%20approaches.We%20conduct%20an%0Arepository%20for%20our%20benchmark%20at%20https%3A//github.com/zjq0455/PTQ_Benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13178v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Post-Training%2520Quantization%2520in%2520LLMs%253A%2520Comprehensive%2520Taxonomy%252C%250A%2520%2520Unified%2520Evaluation%252C%2520and%2520Comparative%2520Analysis%26entry.906535625%3DJiaqi%2520Zhao%2520and%2520Ming%2520Wang%2520and%2520Miao%2520Zhang%2520and%2520Yuzhang%2520Shang%2520and%2520Xuebo%2520Liu%2520and%2520Yaowei%2520Wang%2520and%2520Min%2520Zhang%2520and%2520Liqiang%2520Nie%26entry.1292438233%3D%2520%2520Post-training%2520Quantization%2520%2528PTQ%2529%2520technique%2520has%2520been%2520extensively%2520adopted%2520for%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520compression%2520owing%2520to%2520its%2520efficiency%2520and%2520low%250Aresource%2520requirement.%2520However%252C%2520current%2520research%2520lacks%2520a%2520in-depth%2520analysis%2520of%250Athe%2520superior%2520and%2520applicable%2520scenarios%2520of%2520each%2520PTQ%2520strategy.%2520In%2520addition%252C%250Aexisting%2520algorithms%2520focus%2520primarily%2520on%2520performance%252C%2520overlooking%2520the%2520trade-off%250Aamong%2520model%2520size%252C%2520performance%252C%2520and%2520quantization%2520bitwidth.%2520To%2520mitigate%2520these%250Aconfusions%252C%2520we%2520provide%2520a%2520novel%2520benchmark%2520for%2520LLMs%2520PTQ%2520in%2520this%2520paper.%2520Firstly%252C%250Ain%2520order%2520to%2520support%2520our%2520benchmark%252C%2520we%2520propose%2520a%2520comprehensive%2520taxonomy%2520for%250Aexisting%2520mainstream%2520methods%2520by%2520scrutinizing%2520their%2520computational%2520strategies%250A%2528e.g.%252C%2520optimization-based%252C%2520compensation-based%252C%2520etc.%2529.%2520Then%252C%2520we%2520conduct%250Aextensive%2520experiments%2520with%2520the%2520baseline%2520within%2520each%2520class%252C%2520covering%2520models%2520with%250Avarious%2520sizes%2520%25287B-70B%2529%252C%2520bitwidths%252C%2520training%2520levels%2520%2528LLaMA1/2/3/3.1%2529%252C%250Aarchitectures%2520%2528Mixtral%252C%2520DeepSeekMoE%2520and%2520Mamba%2529%2520and%2520modality%2520%2528LLaVA1.5%2520and%250AVILA1.5%2529%2520on%2520a%2520wide%2520range%2520of%2520evaluation%2520metrics.Through%2520comparative%2520analysis%2520on%250Athe%2520results%252C%2520we%2520summarize%2520the%2520superior%2520of%2520each%2520PTQ%2520strategy%2520and%250Amodelsize-bitwidth%2520trade-off%2520considering%2520the%2520performance.%2520For%2520example%252C%2520our%250Abenchmark%2520reveals%2520that%2520compensation-based%2520technique%2520demonstrates%2520outstanding%250Across-architecture%2520robustness%2520and%2520extremely%2520low-bit%2520PTQ%2520for%2520ultra%2520large%2520models%250Ashould%2520be%2520reexamined.%2520Finally%252C%2520we%2520further%2520accordingly%2520claim%2520that%2520a%2520practical%250Acombination%2520of%2520compensation%2520and%2520other%2520PTQ%2520strategy%2520can%2520achieve%2520SOTA%2520various%250Arobustness.%2520We%2520believe%2520that%2520our%2520benchmark%2520will%2520provide%2520valuable%2520recommendations%250Afor%2520the%2520deployment%2520of%2520LLMs%2520and%2520future%2520research%2520on%2520PTQ%2520approaches.We%2520conduct%2520an%250Arepository%2520for%2520our%2520benchmark%2520at%2520https%253A//github.com/zjq0455/PTQ_Benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13178v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Post-Training%20Quantization%20in%20LLMs%3A%20Comprehensive%20Taxonomy%2C%0A%20%20Unified%20Evaluation%2C%20and%20Comparative%20Analysis&entry.906535625=Jiaqi%20Zhao%20and%20Ming%20Wang%20and%20Miao%20Zhang%20and%20Yuzhang%20Shang%20and%20Xuebo%20Liu%20and%20Yaowei%20Wang%20and%20Min%20Zhang%20and%20Liqiang%20Nie&entry.1292438233=%20%20Post-training%20Quantization%20%28PTQ%29%20technique%20has%20been%20extensively%20adopted%20for%0Alarge%20language%20models%20%28LLMs%29%20compression%20owing%20to%20its%20efficiency%20and%20low%0Aresource%20requirement.%20However%2C%20current%20research%20lacks%20a%20in-depth%20analysis%20of%0Athe%20superior%20and%20applicable%20scenarios%20of%20each%20PTQ%20strategy.%20In%20addition%2C%0Aexisting%20algorithms%20focus%20primarily%20on%20performance%2C%20overlooking%20the%20trade-off%0Aamong%20model%20size%2C%20performance%2C%20and%20quantization%20bitwidth.%20To%20mitigate%20these%0Aconfusions%2C%20we%20provide%20a%20novel%20benchmark%20for%20LLMs%20PTQ%20in%20this%20paper.%20Firstly%2C%0Ain%20order%20to%20support%20our%20benchmark%2C%20we%20propose%20a%20comprehensive%20taxonomy%20for%0Aexisting%20mainstream%20methods%20by%20scrutinizing%20their%20computational%20strategies%0A%28e.g.%2C%20optimization-based%2C%20compensation-based%2C%20etc.%29.%20Then%2C%20we%20conduct%0Aextensive%20experiments%20with%20the%20baseline%20within%20each%20class%2C%20covering%20models%20with%0Avarious%20sizes%20%287B-70B%29%2C%20bitwidths%2C%20training%20levels%20%28LLaMA1/2/3/3.1%29%2C%0Aarchitectures%20%28Mixtral%2C%20DeepSeekMoE%20and%20Mamba%29%20and%20modality%20%28LLaVA1.5%20and%0AVILA1.5%29%20on%20a%20wide%20range%20of%20evaluation%20metrics.Through%20comparative%20analysis%20on%0Athe%20results%2C%20we%20summarize%20the%20superior%20of%20each%20PTQ%20strategy%20and%0Amodelsize-bitwidth%20trade-off%20considering%20the%20performance.%20For%20example%2C%20our%0Abenchmark%20reveals%20that%20compensation-based%20technique%20demonstrates%20outstanding%0Across-architecture%20robustness%20and%20extremely%20low-bit%20PTQ%20for%20ultra%20large%20models%0Ashould%20be%20reexamined.%20Finally%2C%20we%20further%20accordingly%20claim%20that%20a%20practical%0Acombination%20of%20compensation%20and%20other%20PTQ%20strategy%20can%20achieve%20SOTA%20various%0Arobustness.%20We%20believe%20that%20our%20benchmark%20will%20provide%20valuable%20recommendations%0Afor%20the%20deployment%20of%20LLMs%20and%20future%20research%20on%20PTQ%20approaches.We%20conduct%20an%0Arepository%20for%20our%20benchmark%20at%20https%3A//github.com/zjq0455/PTQ_Benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13178v4&entry.124074799=Read"},
{"title": "Deep greedy unfolding: Sorting out argsorting in greedy sparse recovery\n  algorithms", "author": "Sina Mohammad-Taheri and Matthew J. Colbrook and Simone Brugiapaglia", "abstract": "  Gradient-based learning imposes (deep) neural networks to be differentiable\nat all steps. This includes model-based architectures constructed by unrolling\niterations of an iterative algorithm onto layers of a neural network, known as\nalgorithm unrolling. However, greedy sparse recovery algorithms depend on the\nnon-differentiable argsort operator, which hinders their integration into\nneural networks. In this paper, we address this challenge in Orthogonal\nMatching Pursuit (OMP) and Iterative Hard Thresholding (IHT), two popular\nrepresentative algorithms in this class. We propose permutation-based variants\nof these algorithms and approximate permutation matrices using \"soft\"\npermutation matrices derived from softsort, a continuous relaxation of argsort.\nWe demonstrate -- both theoretically and numerically -- that Soft-OMP and\nSoft-IHT, as differentiable counterparts of OMP and IHT and fully compatible\nwith neural network training, effectively approximate these algorithms with a\ncontrollable degree of accuracy. This leads to the development of OMP- and\nIHT-Net, fully trainable network architectures based on Soft-OMP and Soft-IHT,\nrespectively. Finally, by choosing weights as \"structure-aware\" trainable\nparameters, we connect our approach to structured sparse recovery and\ndemonstrate its ability to extract latent sparsity patterns from data.\n", "link": "http://arxiv.org/abs/2505.15661v1", "date": "2025-05-21", "relevancy": 2.4571, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5016}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4932}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4795}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20greedy%20unfolding%3A%20Sorting%20out%20argsorting%20in%20greedy%20sparse%20recovery%0A%20%20algorithms&body=Title%3A%20Deep%20greedy%20unfolding%3A%20Sorting%20out%20argsorting%20in%20greedy%20sparse%20recovery%0A%20%20algorithms%0AAuthor%3A%20Sina%20Mohammad-Taheri%20and%20Matthew%20J.%20Colbrook%20and%20Simone%20Brugiapaglia%0AAbstract%3A%20%20%20Gradient-based%20learning%20imposes%20%28deep%29%20neural%20networks%20to%20be%20differentiable%0Aat%20all%20steps.%20This%20includes%20model-based%20architectures%20constructed%20by%20unrolling%0Aiterations%20of%20an%20iterative%20algorithm%20onto%20layers%20of%20a%20neural%20network%2C%20known%20as%0Aalgorithm%20unrolling.%20However%2C%20greedy%20sparse%20recovery%20algorithms%20depend%20on%20the%0Anon-differentiable%20argsort%20operator%2C%20which%20hinders%20their%20integration%20into%0Aneural%20networks.%20In%20this%20paper%2C%20we%20address%20this%20challenge%20in%20Orthogonal%0AMatching%20Pursuit%20%28OMP%29%20and%20Iterative%20Hard%20Thresholding%20%28IHT%29%2C%20two%20popular%0Arepresentative%20algorithms%20in%20this%20class.%20We%20propose%20permutation-based%20variants%0Aof%20these%20algorithms%20and%20approximate%20permutation%20matrices%20using%20%22soft%22%0Apermutation%20matrices%20derived%20from%20softsort%2C%20a%20continuous%20relaxation%20of%20argsort.%0AWe%20demonstrate%20--%20both%20theoretically%20and%20numerically%20--%20that%20Soft-OMP%20and%0ASoft-IHT%2C%20as%20differentiable%20counterparts%20of%20OMP%20and%20IHT%20and%20fully%20compatible%0Awith%20neural%20network%20training%2C%20effectively%20approximate%20these%20algorithms%20with%20a%0Acontrollable%20degree%20of%20accuracy.%20This%20leads%20to%20the%20development%20of%20OMP-%20and%0AIHT-Net%2C%20fully%20trainable%20network%20architectures%20based%20on%20Soft-OMP%20and%20Soft-IHT%2C%0Arespectively.%20Finally%2C%20by%20choosing%20weights%20as%20%22structure-aware%22%20trainable%0Aparameters%2C%20we%20connect%20our%20approach%20to%20structured%20sparse%20recovery%20and%0Ademonstrate%20its%20ability%20to%20extract%20latent%20sparsity%20patterns%20from%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520greedy%2520unfolding%253A%2520Sorting%2520out%2520argsorting%2520in%2520greedy%2520sparse%2520recovery%250A%2520%2520algorithms%26entry.906535625%3DSina%2520Mohammad-Taheri%2520and%2520Matthew%2520J.%2520Colbrook%2520and%2520Simone%2520Brugiapaglia%26entry.1292438233%3D%2520%2520Gradient-based%2520learning%2520imposes%2520%2528deep%2529%2520neural%2520networks%2520to%2520be%2520differentiable%250Aat%2520all%2520steps.%2520This%2520includes%2520model-based%2520architectures%2520constructed%2520by%2520unrolling%250Aiterations%2520of%2520an%2520iterative%2520algorithm%2520onto%2520layers%2520of%2520a%2520neural%2520network%252C%2520known%2520as%250Aalgorithm%2520unrolling.%2520However%252C%2520greedy%2520sparse%2520recovery%2520algorithms%2520depend%2520on%2520the%250Anon-differentiable%2520argsort%2520operator%252C%2520which%2520hinders%2520their%2520integration%2520into%250Aneural%2520networks.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%2520challenge%2520in%2520Orthogonal%250AMatching%2520Pursuit%2520%2528OMP%2529%2520and%2520Iterative%2520Hard%2520Thresholding%2520%2528IHT%2529%252C%2520two%2520popular%250Arepresentative%2520algorithms%2520in%2520this%2520class.%2520We%2520propose%2520permutation-based%2520variants%250Aof%2520these%2520algorithms%2520and%2520approximate%2520permutation%2520matrices%2520using%2520%2522soft%2522%250Apermutation%2520matrices%2520derived%2520from%2520softsort%252C%2520a%2520continuous%2520relaxation%2520of%2520argsort.%250AWe%2520demonstrate%2520--%2520both%2520theoretically%2520and%2520numerically%2520--%2520that%2520Soft-OMP%2520and%250ASoft-IHT%252C%2520as%2520differentiable%2520counterparts%2520of%2520OMP%2520and%2520IHT%2520and%2520fully%2520compatible%250Awith%2520neural%2520network%2520training%252C%2520effectively%2520approximate%2520these%2520algorithms%2520with%2520a%250Acontrollable%2520degree%2520of%2520accuracy.%2520This%2520leads%2520to%2520the%2520development%2520of%2520OMP-%2520and%250AIHT-Net%252C%2520fully%2520trainable%2520network%2520architectures%2520based%2520on%2520Soft-OMP%2520and%2520Soft-IHT%252C%250Arespectively.%2520Finally%252C%2520by%2520choosing%2520weights%2520as%2520%2522structure-aware%2522%2520trainable%250Aparameters%252C%2520we%2520connect%2520our%2520approach%2520to%2520structured%2520sparse%2520recovery%2520and%250Ademonstrate%2520its%2520ability%2520to%2520extract%2520latent%2520sparsity%2520patterns%2520from%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20greedy%20unfolding%3A%20Sorting%20out%20argsorting%20in%20greedy%20sparse%20recovery%0A%20%20algorithms&entry.906535625=Sina%20Mohammad-Taheri%20and%20Matthew%20J.%20Colbrook%20and%20Simone%20Brugiapaglia&entry.1292438233=%20%20Gradient-based%20learning%20imposes%20%28deep%29%20neural%20networks%20to%20be%20differentiable%0Aat%20all%20steps.%20This%20includes%20model-based%20architectures%20constructed%20by%20unrolling%0Aiterations%20of%20an%20iterative%20algorithm%20onto%20layers%20of%20a%20neural%20network%2C%20known%20as%0Aalgorithm%20unrolling.%20However%2C%20greedy%20sparse%20recovery%20algorithms%20depend%20on%20the%0Anon-differentiable%20argsort%20operator%2C%20which%20hinders%20their%20integration%20into%0Aneural%20networks.%20In%20this%20paper%2C%20we%20address%20this%20challenge%20in%20Orthogonal%0AMatching%20Pursuit%20%28OMP%29%20and%20Iterative%20Hard%20Thresholding%20%28IHT%29%2C%20two%20popular%0Arepresentative%20algorithms%20in%20this%20class.%20We%20propose%20permutation-based%20variants%0Aof%20these%20algorithms%20and%20approximate%20permutation%20matrices%20using%20%22soft%22%0Apermutation%20matrices%20derived%20from%20softsort%2C%20a%20continuous%20relaxation%20of%20argsort.%0AWe%20demonstrate%20--%20both%20theoretically%20and%20numerically%20--%20that%20Soft-OMP%20and%0ASoft-IHT%2C%20as%20differentiable%20counterparts%20of%20OMP%20and%20IHT%20and%20fully%20compatible%0Awith%20neural%20network%20training%2C%20effectively%20approximate%20these%20algorithms%20with%20a%0Acontrollable%20degree%20of%20accuracy.%20This%20leads%20to%20the%20development%20of%20OMP-%20and%0AIHT-Net%2C%20fully%20trainable%20network%20architectures%20based%20on%20Soft-OMP%20and%20Soft-IHT%2C%0Arespectively.%20Finally%2C%20by%20choosing%20weights%20as%20%22structure-aware%22%20trainable%0Aparameters%2C%20we%20connect%20our%20approach%20to%20structured%20sparse%20recovery%20and%0Ademonstrate%20its%20ability%20to%20extract%20latent%20sparsity%20patterns%20from%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15661v1&entry.124074799=Read"},
{"title": "CLIMB-3D: Continual Learning for Imbalanced 3D Instance Segmentation", "author": "Vishal Thengane and Jean Lahoud and Hisham Cholakkal and Rao Muhammad Anwer and Lu Yin and Xiatian Zhu and Salman Khan", "abstract": "  While 3D instance segmentation (3DIS) has advanced significantly, existing\nmethods typically assume that all object classes are known in advance and are\nuniformly distributed. However, this assumption is unrealistic in dynamic,\nreal-world environments where new classes emerge gradually and exhibit natural\nimbalance. Although some approaches have addressed class emergence, they often\noverlook class imbalance, resulting in suboptimal performance -- particularly\non rare categories. To tackle this challenge, we propose CLIMB-3D, a unified\nframework for \\textbf{CL}ass-incremental \\textbf{Imb}alance-aware\n\\textbf{3D}IS. Building upon established exemplar replay (ER) strategies, we\nshow that ER alone is insufficient to achieve robust performance under\nconstrained memory conditions. To mitigate this, we introduce a novel\npseudo-label generator (PLG) that extends supervision to previously learned\ncategories by leveraging predictions from a frozen prior model. Despite its\npromise, PLG tends to bias towards frequent classes. Therefore, we propose a\nclass-balanced re-weighting (CBR) scheme, that estimates object frequencies\nfrom pseudo-labels and dynamically adjusts training bias -- without requiring\naccess to past data. We design and evaluate three incremental scenarios for\n3DIS on the challenging ScanNet200 dataset, and additionally on semantic\nsegmentation on ScanNetV2. Our approach achieves state-of-the-art results,\nsurpassing prior work by up to 16.76\\% mAP for instance segmentation and\napproximately 30\\% mIoU for semantic segmentation, demonstrating strong\ngeneralization across both frequent and rare classes.\n", "link": "http://arxiv.org/abs/2502.17429v2", "date": "2025-05-21", "relevancy": 2.4342, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6406}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.598}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIMB-3D%3A%20Continual%20Learning%20for%20Imbalanced%203D%20Instance%20Segmentation&body=Title%3A%20CLIMB-3D%3A%20Continual%20Learning%20for%20Imbalanced%203D%20Instance%20Segmentation%0AAuthor%3A%20Vishal%20Thengane%20and%20Jean%20Lahoud%20and%20Hisham%20Cholakkal%20and%20Rao%20Muhammad%20Anwer%20and%20Lu%20Yin%20and%20Xiatian%20Zhu%20and%20Salman%20Khan%0AAbstract%3A%20%20%20While%203D%20instance%20segmentation%20%283DIS%29%20has%20advanced%20significantly%2C%20existing%0Amethods%20typically%20assume%20that%20all%20object%20classes%20are%20known%20in%20advance%20and%20are%0Auniformly%20distributed.%20However%2C%20this%20assumption%20is%20unrealistic%20in%20dynamic%2C%0Areal-world%20environments%20where%20new%20classes%20emerge%20gradually%20and%20exhibit%20natural%0Aimbalance.%20Although%20some%20approaches%20have%20addressed%20class%20emergence%2C%20they%20often%0Aoverlook%20class%20imbalance%2C%20resulting%20in%20suboptimal%20performance%20--%20particularly%0Aon%20rare%20categories.%20To%20tackle%20this%20challenge%2C%20we%20propose%20CLIMB-3D%2C%20a%20unified%0Aframework%20for%20%5Ctextbf%7BCL%7Dass-incremental%20%5Ctextbf%7BImb%7Dalance-aware%0A%5Ctextbf%7B3D%7DIS.%20Building%20upon%20established%20exemplar%20replay%20%28ER%29%20strategies%2C%20we%0Ashow%20that%20ER%20alone%20is%20insufficient%20to%20achieve%20robust%20performance%20under%0Aconstrained%20memory%20conditions.%20To%20mitigate%20this%2C%20we%20introduce%20a%20novel%0Apseudo-label%20generator%20%28PLG%29%20that%20extends%20supervision%20to%20previously%20learned%0Acategories%20by%20leveraging%20predictions%20from%20a%20frozen%20prior%20model.%20Despite%20its%0Apromise%2C%20PLG%20tends%20to%20bias%20towards%20frequent%20classes.%20Therefore%2C%20we%20propose%20a%0Aclass-balanced%20re-weighting%20%28CBR%29%20scheme%2C%20that%20estimates%20object%20frequencies%0Afrom%20pseudo-labels%20and%20dynamically%20adjusts%20training%20bias%20--%20without%20requiring%0Aaccess%20to%20past%20data.%20We%20design%20and%20evaluate%20three%20incremental%20scenarios%20for%0A3DIS%20on%20the%20challenging%20ScanNet200%20dataset%2C%20and%20additionally%20on%20semantic%0Asegmentation%20on%20ScanNetV2.%20Our%20approach%20achieves%20state-of-the-art%20results%2C%0Asurpassing%20prior%20work%20by%20up%20to%2016.76%5C%25%20mAP%20for%20instance%20segmentation%20and%0Aapproximately%2030%5C%25%20mIoU%20for%20semantic%20segmentation%2C%20demonstrating%20strong%0Ageneralization%20across%20both%20frequent%20and%20rare%20classes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17429v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIMB-3D%253A%2520Continual%2520Learning%2520for%2520Imbalanced%25203D%2520Instance%2520Segmentation%26entry.906535625%3DVishal%2520Thengane%2520and%2520Jean%2520Lahoud%2520and%2520Hisham%2520Cholakkal%2520and%2520Rao%2520Muhammad%2520Anwer%2520and%2520Lu%2520Yin%2520and%2520Xiatian%2520Zhu%2520and%2520Salman%2520Khan%26entry.1292438233%3D%2520%2520While%25203D%2520instance%2520segmentation%2520%25283DIS%2529%2520has%2520advanced%2520significantly%252C%2520existing%250Amethods%2520typically%2520assume%2520that%2520all%2520object%2520classes%2520are%2520known%2520in%2520advance%2520and%2520are%250Auniformly%2520distributed.%2520However%252C%2520this%2520assumption%2520is%2520unrealistic%2520in%2520dynamic%252C%250Areal-world%2520environments%2520where%2520new%2520classes%2520emerge%2520gradually%2520and%2520exhibit%2520natural%250Aimbalance.%2520Although%2520some%2520approaches%2520have%2520addressed%2520class%2520emergence%252C%2520they%2520often%250Aoverlook%2520class%2520imbalance%252C%2520resulting%2520in%2520suboptimal%2520performance%2520--%2520particularly%250Aon%2520rare%2520categories.%2520To%2520tackle%2520this%2520challenge%252C%2520we%2520propose%2520CLIMB-3D%252C%2520a%2520unified%250Aframework%2520for%2520%255Ctextbf%257BCL%257Dass-incremental%2520%255Ctextbf%257BImb%257Dalance-aware%250A%255Ctextbf%257B3D%257DIS.%2520Building%2520upon%2520established%2520exemplar%2520replay%2520%2528ER%2529%2520strategies%252C%2520we%250Ashow%2520that%2520ER%2520alone%2520is%2520insufficient%2520to%2520achieve%2520robust%2520performance%2520under%250Aconstrained%2520memory%2520conditions.%2520To%2520mitigate%2520this%252C%2520we%2520introduce%2520a%2520novel%250Apseudo-label%2520generator%2520%2528PLG%2529%2520that%2520extends%2520supervision%2520to%2520previously%2520learned%250Acategories%2520by%2520leveraging%2520predictions%2520from%2520a%2520frozen%2520prior%2520model.%2520Despite%2520its%250Apromise%252C%2520PLG%2520tends%2520to%2520bias%2520towards%2520frequent%2520classes.%2520Therefore%252C%2520we%2520propose%2520a%250Aclass-balanced%2520re-weighting%2520%2528CBR%2529%2520scheme%252C%2520that%2520estimates%2520object%2520frequencies%250Afrom%2520pseudo-labels%2520and%2520dynamically%2520adjusts%2520training%2520bias%2520--%2520without%2520requiring%250Aaccess%2520to%2520past%2520data.%2520We%2520design%2520and%2520evaluate%2520three%2520incremental%2520scenarios%2520for%250A3DIS%2520on%2520the%2520challenging%2520ScanNet200%2520dataset%252C%2520and%2520additionally%2520on%2520semantic%250Asegmentation%2520on%2520ScanNetV2.%2520Our%2520approach%2520achieves%2520state-of-the-art%2520results%252C%250Asurpassing%2520prior%2520work%2520by%2520up%2520to%252016.76%255C%2525%2520mAP%2520for%2520instance%2520segmentation%2520and%250Aapproximately%252030%255C%2525%2520mIoU%2520for%2520semantic%2520segmentation%252C%2520demonstrating%2520strong%250Ageneralization%2520across%2520both%2520frequent%2520and%2520rare%2520classes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17429v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIMB-3D%3A%20Continual%20Learning%20for%20Imbalanced%203D%20Instance%20Segmentation&entry.906535625=Vishal%20Thengane%20and%20Jean%20Lahoud%20and%20Hisham%20Cholakkal%20and%20Rao%20Muhammad%20Anwer%20and%20Lu%20Yin%20and%20Xiatian%20Zhu%20and%20Salman%20Khan&entry.1292438233=%20%20While%203D%20instance%20segmentation%20%283DIS%29%20has%20advanced%20significantly%2C%20existing%0Amethods%20typically%20assume%20that%20all%20object%20classes%20are%20known%20in%20advance%20and%20are%0Auniformly%20distributed.%20However%2C%20this%20assumption%20is%20unrealistic%20in%20dynamic%2C%0Areal-world%20environments%20where%20new%20classes%20emerge%20gradually%20and%20exhibit%20natural%0Aimbalance.%20Although%20some%20approaches%20have%20addressed%20class%20emergence%2C%20they%20often%0Aoverlook%20class%20imbalance%2C%20resulting%20in%20suboptimal%20performance%20--%20particularly%0Aon%20rare%20categories.%20To%20tackle%20this%20challenge%2C%20we%20propose%20CLIMB-3D%2C%20a%20unified%0Aframework%20for%20%5Ctextbf%7BCL%7Dass-incremental%20%5Ctextbf%7BImb%7Dalance-aware%0A%5Ctextbf%7B3D%7DIS.%20Building%20upon%20established%20exemplar%20replay%20%28ER%29%20strategies%2C%20we%0Ashow%20that%20ER%20alone%20is%20insufficient%20to%20achieve%20robust%20performance%20under%0Aconstrained%20memory%20conditions.%20To%20mitigate%20this%2C%20we%20introduce%20a%20novel%0Apseudo-label%20generator%20%28PLG%29%20that%20extends%20supervision%20to%20previously%20learned%0Acategories%20by%20leveraging%20predictions%20from%20a%20frozen%20prior%20model.%20Despite%20its%0Apromise%2C%20PLG%20tends%20to%20bias%20towards%20frequent%20classes.%20Therefore%2C%20we%20propose%20a%0Aclass-balanced%20re-weighting%20%28CBR%29%20scheme%2C%20that%20estimates%20object%20frequencies%0Afrom%20pseudo-labels%20and%20dynamically%20adjusts%20training%20bias%20--%20without%20requiring%0Aaccess%20to%20past%20data.%20We%20design%20and%20evaluate%20three%20incremental%20scenarios%20for%0A3DIS%20on%20the%20challenging%20ScanNet200%20dataset%2C%20and%20additionally%20on%20semantic%0Asegmentation%20on%20ScanNetV2.%20Our%20approach%20achieves%20state-of-the-art%20results%2C%0Asurpassing%20prior%20work%20by%20up%20to%2016.76%5C%25%20mAP%20for%20instance%20segmentation%20and%0Aapproximately%2030%5C%25%20mIoU%20for%20semantic%20segmentation%2C%20demonstrating%20strong%0Ageneralization%20across%20both%20frequent%20and%20rare%20classes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17429v2&entry.124074799=Read"},
{"title": "PlantDreamer: Achieving Realistic 3D Plant Models with Diffusion-Guided\n  Gaussian Splatting", "author": "Zane K J Hartley and Lewis A G Stuart and Andrew P French and Michael P Pound", "abstract": "  Recent years have seen substantial improvements in the ability to generate\nsynthetic 3D objects using AI. However, generating complex 3D objects, such as\nplants, remains a considerable challenge. Current generative 3D models struggle\nwith plant generation compared to general objects, limiting their usability in\nplant analysis tools, which require fine detail and accurate geometry. We\nintroduce PlantDreamer, a novel approach to 3D synthetic plant generation,\nwhich can achieve greater levels of realism for complex plant geometry and\ntextures than available text-to-3D models. To achieve this, our new generation\npipeline leverages a depth ControlNet, fine-tuned Low-Rank Adaptation and an\nadaptable Gaussian culling algorithm, which directly improve textural realism\nand geometric integrity of generated 3D plant models. Additionally,\nPlantDreamer enables both purely synthetic plant generation, by leveraging\nL-System-generated meshes, and the enhancement of real-world plant point clouds\nby converting them into 3D Gaussian Splats. We evaluate our approach by\ncomparing its outputs with state-of-the-art text-to-3D models, demonstrating\nthat PlantDreamer outperforms existing methods in producing high-fidelity\nsynthetic plants. Our results indicate that our approach not only advances\nsynthetic plant generation, but also facilitates the upgrading of legacy point\ncloud datasets, making it a valuable tool for 3D phenotyping applications.\n", "link": "http://arxiv.org/abs/2505.15528v1", "date": "2025-05-21", "relevancy": 2.4313, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6099}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6084}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PlantDreamer%3A%20Achieving%20Realistic%203D%20Plant%20Models%20with%20Diffusion-Guided%0A%20%20Gaussian%20Splatting&body=Title%3A%20PlantDreamer%3A%20Achieving%20Realistic%203D%20Plant%20Models%20with%20Diffusion-Guided%0A%20%20Gaussian%20Splatting%0AAuthor%3A%20Zane%20K%20J%20Hartley%20and%20Lewis%20A%20G%20Stuart%20and%20Andrew%20P%20French%20and%20Michael%20P%20Pound%0AAbstract%3A%20%20%20Recent%20years%20have%20seen%20substantial%20improvements%20in%20the%20ability%20to%20generate%0Asynthetic%203D%20objects%20using%20AI.%20However%2C%20generating%20complex%203D%20objects%2C%20such%20as%0Aplants%2C%20remains%20a%20considerable%20challenge.%20Current%20generative%203D%20models%20struggle%0Awith%20plant%20generation%20compared%20to%20general%20objects%2C%20limiting%20their%20usability%20in%0Aplant%20analysis%20tools%2C%20which%20require%20fine%20detail%20and%20accurate%20geometry.%20We%0Aintroduce%20PlantDreamer%2C%20a%20novel%20approach%20to%203D%20synthetic%20plant%20generation%2C%0Awhich%20can%20achieve%20greater%20levels%20of%20realism%20for%20complex%20plant%20geometry%20and%0Atextures%20than%20available%20text-to-3D%20models.%20To%20achieve%20this%2C%20our%20new%20generation%0Apipeline%20leverages%20a%20depth%20ControlNet%2C%20fine-tuned%20Low-Rank%20Adaptation%20and%20an%0Aadaptable%20Gaussian%20culling%20algorithm%2C%20which%20directly%20improve%20textural%20realism%0Aand%20geometric%20integrity%20of%20generated%203D%20plant%20models.%20Additionally%2C%0APlantDreamer%20enables%20both%20purely%20synthetic%20plant%20generation%2C%20by%20leveraging%0AL-System-generated%20meshes%2C%20and%20the%20enhancement%20of%20real-world%20plant%20point%20clouds%0Aby%20converting%20them%20into%203D%20Gaussian%20Splats.%20We%20evaluate%20our%20approach%20by%0Acomparing%20its%20outputs%20with%20state-of-the-art%20text-to-3D%20models%2C%20demonstrating%0Athat%20PlantDreamer%20outperforms%20existing%20methods%20in%20producing%20high-fidelity%0Asynthetic%20plants.%20Our%20results%20indicate%20that%20our%20approach%20not%20only%20advances%0Asynthetic%20plant%20generation%2C%20but%20also%20facilitates%20the%20upgrading%20of%20legacy%20point%0Acloud%20datasets%2C%20making%20it%20a%20valuable%20tool%20for%203D%20phenotyping%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15528v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlantDreamer%253A%2520Achieving%2520Realistic%25203D%2520Plant%2520Models%2520with%2520Diffusion-Guided%250A%2520%2520Gaussian%2520Splatting%26entry.906535625%3DZane%2520K%2520J%2520Hartley%2520and%2520Lewis%2520A%2520G%2520Stuart%2520and%2520Andrew%2520P%2520French%2520and%2520Michael%2520P%2520Pound%26entry.1292438233%3D%2520%2520Recent%2520years%2520have%2520seen%2520substantial%2520improvements%2520in%2520the%2520ability%2520to%2520generate%250Asynthetic%25203D%2520objects%2520using%2520AI.%2520However%252C%2520generating%2520complex%25203D%2520objects%252C%2520such%2520as%250Aplants%252C%2520remains%2520a%2520considerable%2520challenge.%2520Current%2520generative%25203D%2520models%2520struggle%250Awith%2520plant%2520generation%2520compared%2520to%2520general%2520objects%252C%2520limiting%2520their%2520usability%2520in%250Aplant%2520analysis%2520tools%252C%2520which%2520require%2520fine%2520detail%2520and%2520accurate%2520geometry.%2520We%250Aintroduce%2520PlantDreamer%252C%2520a%2520novel%2520approach%2520to%25203D%2520synthetic%2520plant%2520generation%252C%250Awhich%2520can%2520achieve%2520greater%2520levels%2520of%2520realism%2520for%2520complex%2520plant%2520geometry%2520and%250Atextures%2520than%2520available%2520text-to-3D%2520models.%2520To%2520achieve%2520this%252C%2520our%2520new%2520generation%250Apipeline%2520leverages%2520a%2520depth%2520ControlNet%252C%2520fine-tuned%2520Low-Rank%2520Adaptation%2520and%2520an%250Aadaptable%2520Gaussian%2520culling%2520algorithm%252C%2520which%2520directly%2520improve%2520textural%2520realism%250Aand%2520geometric%2520integrity%2520of%2520generated%25203D%2520plant%2520models.%2520Additionally%252C%250APlantDreamer%2520enables%2520both%2520purely%2520synthetic%2520plant%2520generation%252C%2520by%2520leveraging%250AL-System-generated%2520meshes%252C%2520and%2520the%2520enhancement%2520of%2520real-world%2520plant%2520point%2520clouds%250Aby%2520converting%2520them%2520into%25203D%2520Gaussian%2520Splats.%2520We%2520evaluate%2520our%2520approach%2520by%250Acomparing%2520its%2520outputs%2520with%2520state-of-the-art%2520text-to-3D%2520models%252C%2520demonstrating%250Athat%2520PlantDreamer%2520outperforms%2520existing%2520methods%2520in%2520producing%2520high-fidelity%250Asynthetic%2520plants.%2520Our%2520results%2520indicate%2520that%2520our%2520approach%2520not%2520only%2520advances%250Asynthetic%2520plant%2520generation%252C%2520but%2520also%2520facilitates%2520the%2520upgrading%2520of%2520legacy%2520point%250Acloud%2520datasets%252C%2520making%2520it%2520a%2520valuable%2520tool%2520for%25203D%2520phenotyping%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15528v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PlantDreamer%3A%20Achieving%20Realistic%203D%20Plant%20Models%20with%20Diffusion-Guided%0A%20%20Gaussian%20Splatting&entry.906535625=Zane%20K%20J%20Hartley%20and%20Lewis%20A%20G%20Stuart%20and%20Andrew%20P%20French%20and%20Michael%20P%20Pound&entry.1292438233=%20%20Recent%20years%20have%20seen%20substantial%20improvements%20in%20the%20ability%20to%20generate%0Asynthetic%203D%20objects%20using%20AI.%20However%2C%20generating%20complex%203D%20objects%2C%20such%20as%0Aplants%2C%20remains%20a%20considerable%20challenge.%20Current%20generative%203D%20models%20struggle%0Awith%20plant%20generation%20compared%20to%20general%20objects%2C%20limiting%20their%20usability%20in%0Aplant%20analysis%20tools%2C%20which%20require%20fine%20detail%20and%20accurate%20geometry.%20We%0Aintroduce%20PlantDreamer%2C%20a%20novel%20approach%20to%203D%20synthetic%20plant%20generation%2C%0Awhich%20can%20achieve%20greater%20levels%20of%20realism%20for%20complex%20plant%20geometry%20and%0Atextures%20than%20available%20text-to-3D%20models.%20To%20achieve%20this%2C%20our%20new%20generation%0Apipeline%20leverages%20a%20depth%20ControlNet%2C%20fine-tuned%20Low-Rank%20Adaptation%20and%20an%0Aadaptable%20Gaussian%20culling%20algorithm%2C%20which%20directly%20improve%20textural%20realism%0Aand%20geometric%20integrity%20of%20generated%203D%20plant%20models.%20Additionally%2C%0APlantDreamer%20enables%20both%20purely%20synthetic%20plant%20generation%2C%20by%20leveraging%0AL-System-generated%20meshes%2C%20and%20the%20enhancement%20of%20real-world%20plant%20point%20clouds%0Aby%20converting%20them%20into%203D%20Gaussian%20Splats.%20We%20evaluate%20our%20approach%20by%0Acomparing%20its%20outputs%20with%20state-of-the-art%20text-to-3D%20models%2C%20demonstrating%0Athat%20PlantDreamer%20outperforms%20existing%20methods%20in%20producing%20high-fidelity%0Asynthetic%20plants.%20Our%20results%20indicate%20that%20our%20approach%20not%20only%20advances%0Asynthetic%20plant%20generation%2C%20but%20also%20facilitates%20the%20upgrading%20of%20legacy%20point%0Acloud%20datasets%2C%20making%20it%20a%20valuable%20tool%20for%203D%20phenotyping%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15528v1&entry.124074799=Read"},
{"title": "Can LLMs $\\textit{understand}$ Math? -- Exploring the Pitfalls in\n  Mathematical Reasoning", "author": "Tiasa Singha Roy and Aditeya Baral and Ayush Rajesh Jhaveri and Yusuf Baig", "abstract": "  Large language models (LLMs) demonstrate considerable potential in various\nnatural language tasks but face significant challenges in mathematical\nreasoning, particularly in executing precise, multi-step logic. However,\ncurrent evaluation frameworks judge their performance solely based on accuracy,\nwhich only accounts for the final answer. This study explores these pitfalls by\nemploying a novel evaluation framework. We propose an evaluation metric called\nthe MAPLE score, which holistically quantifies reasoning misalignment by\nintegrating error rates, redundancy, and validity.\n", "link": "http://arxiv.org/abs/2505.15623v1", "date": "2025-05-21", "relevancy": 2.4273, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4991}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4991}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4582}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20LLMs%20%24%5Ctextit%7Bunderstand%7D%24%20Math%3F%20--%20Exploring%20the%20Pitfalls%20in%0A%20%20Mathematical%20Reasoning&body=Title%3A%20Can%20LLMs%20%24%5Ctextit%7Bunderstand%7D%24%20Math%3F%20--%20Exploring%20the%20Pitfalls%20in%0A%20%20Mathematical%20Reasoning%0AAuthor%3A%20Tiasa%20Singha%20Roy%20and%20Aditeya%20Baral%20and%20Ayush%20Rajesh%20Jhaveri%20and%20Yusuf%20Baig%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20demonstrate%20considerable%20potential%20in%20various%0Anatural%20language%20tasks%20but%20face%20significant%20challenges%20in%20mathematical%0Areasoning%2C%20particularly%20in%20executing%20precise%2C%20multi-step%20logic.%20However%2C%0Acurrent%20evaluation%20frameworks%20judge%20their%20performance%20solely%20based%20on%20accuracy%2C%0Awhich%20only%20accounts%20for%20the%20final%20answer.%20This%20study%20explores%20these%20pitfalls%20by%0Aemploying%20a%20novel%20evaluation%20framework.%20We%20propose%20an%20evaluation%20metric%20called%0Athe%20MAPLE%20score%2C%20which%20holistically%20quantifies%20reasoning%20misalignment%20by%0Aintegrating%20error%20rates%2C%20redundancy%2C%20and%20validity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15623v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520LLMs%2520%2524%255Ctextit%257Bunderstand%257D%2524%2520Math%253F%2520--%2520Exploring%2520the%2520Pitfalls%2520in%250A%2520%2520Mathematical%2520Reasoning%26entry.906535625%3DTiasa%2520Singha%2520Roy%2520and%2520Aditeya%2520Baral%2520and%2520Ayush%2520Rajesh%2520Jhaveri%2520and%2520Yusuf%2520Baig%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520demonstrate%2520considerable%2520potential%2520in%2520various%250Anatural%2520language%2520tasks%2520but%2520face%2520significant%2520challenges%2520in%2520mathematical%250Areasoning%252C%2520particularly%2520in%2520executing%2520precise%252C%2520multi-step%2520logic.%2520However%252C%250Acurrent%2520evaluation%2520frameworks%2520judge%2520their%2520performance%2520solely%2520based%2520on%2520accuracy%252C%250Awhich%2520only%2520accounts%2520for%2520the%2520final%2520answer.%2520This%2520study%2520explores%2520these%2520pitfalls%2520by%250Aemploying%2520a%2520novel%2520evaluation%2520framework.%2520We%2520propose%2520an%2520evaluation%2520metric%2520called%250Athe%2520MAPLE%2520score%252C%2520which%2520holistically%2520quantifies%2520reasoning%2520misalignment%2520by%250Aintegrating%2520error%2520rates%252C%2520redundancy%252C%2520and%2520validity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15623v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20LLMs%20%24%5Ctextit%7Bunderstand%7D%24%20Math%3F%20--%20Exploring%20the%20Pitfalls%20in%0A%20%20Mathematical%20Reasoning&entry.906535625=Tiasa%20Singha%20Roy%20and%20Aditeya%20Baral%20and%20Ayush%20Rajesh%20Jhaveri%20and%20Yusuf%20Baig&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20demonstrate%20considerable%20potential%20in%20various%0Anatural%20language%20tasks%20but%20face%20significant%20challenges%20in%20mathematical%0Areasoning%2C%20particularly%20in%20executing%20precise%2C%20multi-step%20logic.%20However%2C%0Acurrent%20evaluation%20frameworks%20judge%20their%20performance%20solely%20based%20on%20accuracy%2C%0Awhich%20only%20accounts%20for%20the%20final%20answer.%20This%20study%20explores%20these%20pitfalls%20by%0Aemploying%20a%20novel%20evaluation%20framework.%20We%20propose%20an%20evaluation%20metric%20called%0Athe%20MAPLE%20score%2C%20which%20holistically%20quantifies%20reasoning%20misalignment%20by%0Aintegrating%20error%20rates%2C%20redundancy%2C%20and%20validity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15623v1&entry.124074799=Read"},
{"title": "On the creation of narrow AI: hierarchy and nonlocality of neural\n  network skills", "author": "Eric J. Michaud and Asher Parker-Sartori and Max Tegmark", "abstract": "  We study the problem of creating strong, yet narrow, AI systems. While recent\nAI progress has been driven by the training of large general-purpose foundation\nmodels, the creation of smaller models specialized for narrow domains could be\nvaluable for both efficiency and safety. In this work, we explore two\nchallenges involved in creating such systems, having to do with basic\nproperties of how neural networks learn and structure their representations.\nThe first challenge regards when it is possible to train narrow models from\nscratch. Through experiments on a synthetic task, we find that it is sometimes\nnecessary to train networks on a wide distribution of data to learn certain\nnarrow skills within that distribution. This effect arises when skills depend\non each other hierarchically, and training on a broad distribution introduces a\ncurriculum which substantially accelerates learning. The second challenge\nregards how to transfer particular skills from large general models into small\nspecialized models. We find that model skills are often not perfectly localized\nto a particular set of prunable components. However, we find that methods based\non pruning can still outperform distillation. We investigate the use of a\nregularization objective to align desired skills with prunable components while\nunlearning unnecessary skills.\n", "link": "http://arxiv.org/abs/2505.15811v1", "date": "2025-05-21", "relevancy": 2.415, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4982}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4771}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20creation%20of%20narrow%20AI%3A%20hierarchy%20and%20nonlocality%20of%20neural%0A%20%20network%20skills&body=Title%3A%20On%20the%20creation%20of%20narrow%20AI%3A%20hierarchy%20and%20nonlocality%20of%20neural%0A%20%20network%20skills%0AAuthor%3A%20Eric%20J.%20Michaud%20and%20Asher%20Parker-Sartori%20and%20Max%20Tegmark%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20creating%20strong%2C%20yet%20narrow%2C%20AI%20systems.%20While%20recent%0AAI%20progress%20has%20been%20driven%20by%20the%20training%20of%20large%20general-purpose%20foundation%0Amodels%2C%20the%20creation%20of%20smaller%20models%20specialized%20for%20narrow%20domains%20could%20be%0Avaluable%20for%20both%20efficiency%20and%20safety.%20In%20this%20work%2C%20we%20explore%20two%0Achallenges%20involved%20in%20creating%20such%20systems%2C%20having%20to%20do%20with%20basic%0Aproperties%20of%20how%20neural%20networks%20learn%20and%20structure%20their%20representations.%0AThe%20first%20challenge%20regards%20when%20it%20is%20possible%20to%20train%20narrow%20models%20from%0Ascratch.%20Through%20experiments%20on%20a%20synthetic%20task%2C%20we%20find%20that%20it%20is%20sometimes%0Anecessary%20to%20train%20networks%20on%20a%20wide%20distribution%20of%20data%20to%20learn%20certain%0Anarrow%20skills%20within%20that%20distribution.%20This%20effect%20arises%20when%20skills%20depend%0Aon%20each%20other%20hierarchically%2C%20and%20training%20on%20a%20broad%20distribution%20introduces%20a%0Acurriculum%20which%20substantially%20accelerates%20learning.%20The%20second%20challenge%0Aregards%20how%20to%20transfer%20particular%20skills%20from%20large%20general%20models%20into%20small%0Aspecialized%20models.%20We%20find%20that%20model%20skills%20are%20often%20not%20perfectly%20localized%0Ato%20a%20particular%20set%20of%20prunable%20components.%20However%2C%20we%20find%20that%20methods%20based%0Aon%20pruning%20can%20still%20outperform%20distillation.%20We%20investigate%20the%20use%20of%20a%0Aregularization%20objective%20to%20align%20desired%20skills%20with%20prunable%20components%20while%0Aunlearning%20unnecessary%20skills.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15811v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520creation%2520of%2520narrow%2520AI%253A%2520hierarchy%2520and%2520nonlocality%2520of%2520neural%250A%2520%2520network%2520skills%26entry.906535625%3DEric%2520J.%2520Michaud%2520and%2520Asher%2520Parker-Sartori%2520and%2520Max%2520Tegmark%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520creating%2520strong%252C%2520yet%2520narrow%252C%2520AI%2520systems.%2520While%2520recent%250AAI%2520progress%2520has%2520been%2520driven%2520by%2520the%2520training%2520of%2520large%2520general-purpose%2520foundation%250Amodels%252C%2520the%2520creation%2520of%2520smaller%2520models%2520specialized%2520for%2520narrow%2520domains%2520could%2520be%250Avaluable%2520for%2520both%2520efficiency%2520and%2520safety.%2520In%2520this%2520work%252C%2520we%2520explore%2520two%250Achallenges%2520involved%2520in%2520creating%2520such%2520systems%252C%2520having%2520to%2520do%2520with%2520basic%250Aproperties%2520of%2520how%2520neural%2520networks%2520learn%2520and%2520structure%2520their%2520representations.%250AThe%2520first%2520challenge%2520regards%2520when%2520it%2520is%2520possible%2520to%2520train%2520narrow%2520models%2520from%250Ascratch.%2520Through%2520experiments%2520on%2520a%2520synthetic%2520task%252C%2520we%2520find%2520that%2520it%2520is%2520sometimes%250Anecessary%2520to%2520train%2520networks%2520on%2520a%2520wide%2520distribution%2520of%2520data%2520to%2520learn%2520certain%250Anarrow%2520skills%2520within%2520that%2520distribution.%2520This%2520effect%2520arises%2520when%2520skills%2520depend%250Aon%2520each%2520other%2520hierarchically%252C%2520and%2520training%2520on%2520a%2520broad%2520distribution%2520introduces%2520a%250Acurriculum%2520which%2520substantially%2520accelerates%2520learning.%2520The%2520second%2520challenge%250Aregards%2520how%2520to%2520transfer%2520particular%2520skills%2520from%2520large%2520general%2520models%2520into%2520small%250Aspecialized%2520models.%2520We%2520find%2520that%2520model%2520skills%2520are%2520often%2520not%2520perfectly%2520localized%250Ato%2520a%2520particular%2520set%2520of%2520prunable%2520components.%2520However%252C%2520we%2520find%2520that%2520methods%2520based%250Aon%2520pruning%2520can%2520still%2520outperform%2520distillation.%2520We%2520investigate%2520the%2520use%2520of%2520a%250Aregularization%2520objective%2520to%2520align%2520desired%2520skills%2520with%2520prunable%2520components%2520while%250Aunlearning%2520unnecessary%2520skills.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15811v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20creation%20of%20narrow%20AI%3A%20hierarchy%20and%20nonlocality%20of%20neural%0A%20%20network%20skills&entry.906535625=Eric%20J.%20Michaud%20and%20Asher%20Parker-Sartori%20and%20Max%20Tegmark&entry.1292438233=%20%20We%20study%20the%20problem%20of%20creating%20strong%2C%20yet%20narrow%2C%20AI%20systems.%20While%20recent%0AAI%20progress%20has%20been%20driven%20by%20the%20training%20of%20large%20general-purpose%20foundation%0Amodels%2C%20the%20creation%20of%20smaller%20models%20specialized%20for%20narrow%20domains%20could%20be%0Avaluable%20for%20both%20efficiency%20and%20safety.%20In%20this%20work%2C%20we%20explore%20two%0Achallenges%20involved%20in%20creating%20such%20systems%2C%20having%20to%20do%20with%20basic%0Aproperties%20of%20how%20neural%20networks%20learn%20and%20structure%20their%20representations.%0AThe%20first%20challenge%20regards%20when%20it%20is%20possible%20to%20train%20narrow%20models%20from%0Ascratch.%20Through%20experiments%20on%20a%20synthetic%20task%2C%20we%20find%20that%20it%20is%20sometimes%0Anecessary%20to%20train%20networks%20on%20a%20wide%20distribution%20of%20data%20to%20learn%20certain%0Anarrow%20skills%20within%20that%20distribution.%20This%20effect%20arises%20when%20skills%20depend%0Aon%20each%20other%20hierarchically%2C%20and%20training%20on%20a%20broad%20distribution%20introduces%20a%0Acurriculum%20which%20substantially%20accelerates%20learning.%20The%20second%20challenge%0Aregards%20how%20to%20transfer%20particular%20skills%20from%20large%20general%20models%20into%20small%0Aspecialized%20models.%20We%20find%20that%20model%20skills%20are%20often%20not%20perfectly%20localized%0Ato%20a%20particular%20set%20of%20prunable%20components.%20However%2C%20we%20find%20that%20methods%20based%0Aon%20pruning%20can%20still%20outperform%20distillation.%20We%20investigate%20the%20use%20of%20a%0Aregularization%20objective%20to%20align%20desired%20skills%20with%20prunable%20components%20while%0Aunlearning%20unnecessary%20skills.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15811v1&entry.124074799=Read"},
{"title": "The Devil is in Fine-tuning and Long-tailed Problems:A New Benchmark for\n  Scene Text Detection", "author": "Tianjiao Cao and Jiahao Lyu and Weichao Zeng and Weimin Mu and Yu Zhou", "abstract": "  Scene text detection has seen the emergence of high-performing methods that\nexcel on academic benchmarks. However, these detectors often fail to replicate\nsuch success in real-world scenarios. We uncover two key factors contributing\nto this discrepancy through extensive experiments. First, a \\textit{Fine-tuning\nGap}, where models leverage \\textit{Dataset-Specific Optimization} (DSO)\nparadigm for one domain at the cost of reduced effectiveness in others, leads\nto inflated performances on academic benchmarks. Second, the suboptimal\nperformance in practical settings is primarily attributed to the long-tailed\ndistribution of texts, where detectors struggle with rare and complex\ncategories as artistic or overlapped text. Given that the DSO paradigm might\nundermine the generalization ability of models, we advocate for a\n\\textit{Joint-Dataset Learning} (JDL) protocol to alleviate the Fine-tuning\nGap. Additionally, an error analysis is conducted to identify three major\ncategories and 13 subcategories of challenges in long-tailed scene text, upon\nwhich we propose a Long-Tailed Benchmark (LTB). LTB facilitates a comprehensive\nevaluation of ability to handle a diverse range of long-tailed challenges. We\nfurther introduce MAEDet, a self-supervised learning-based method, as a strong\nbaseline for LTB. The code is available at https://github.com/pd162/LTB.\n", "link": "http://arxiv.org/abs/2505.15649v1", "date": "2025-05-21", "relevancy": 2.4136, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6061}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6061}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Devil%20is%20in%20Fine-tuning%20and%20Long-tailed%20Problems%3AA%20New%20Benchmark%20for%0A%20%20Scene%20Text%20Detection&body=Title%3A%20The%20Devil%20is%20in%20Fine-tuning%20and%20Long-tailed%20Problems%3AA%20New%20Benchmark%20for%0A%20%20Scene%20Text%20Detection%0AAuthor%3A%20Tianjiao%20Cao%20and%20Jiahao%20Lyu%20and%20Weichao%20Zeng%20and%20Weimin%20Mu%20and%20Yu%20Zhou%0AAbstract%3A%20%20%20Scene%20text%20detection%20has%20seen%20the%20emergence%20of%20high-performing%20methods%20that%0Aexcel%20on%20academic%20benchmarks.%20However%2C%20these%20detectors%20often%20fail%20to%20replicate%0Asuch%20success%20in%20real-world%20scenarios.%20We%20uncover%20two%20key%20factors%20contributing%0Ato%20this%20discrepancy%20through%20extensive%20experiments.%20First%2C%20a%20%5Ctextit%7BFine-tuning%0AGap%7D%2C%20where%20models%20leverage%20%5Ctextit%7BDataset-Specific%20Optimization%7D%20%28DSO%29%0Aparadigm%20for%20one%20domain%20at%20the%20cost%20of%20reduced%20effectiveness%20in%20others%2C%20leads%0Ato%20inflated%20performances%20on%20academic%20benchmarks.%20Second%2C%20the%20suboptimal%0Aperformance%20in%20practical%20settings%20is%20primarily%20attributed%20to%20the%20long-tailed%0Adistribution%20of%20texts%2C%20where%20detectors%20struggle%20with%20rare%20and%20complex%0Acategories%20as%20artistic%20or%20overlapped%20text.%20Given%20that%20the%20DSO%20paradigm%20might%0Aundermine%20the%20generalization%20ability%20of%20models%2C%20we%20advocate%20for%20a%0A%5Ctextit%7BJoint-Dataset%20Learning%7D%20%28JDL%29%20protocol%20to%20alleviate%20the%20Fine-tuning%0AGap.%20Additionally%2C%20an%20error%20analysis%20is%20conducted%20to%20identify%20three%20major%0Acategories%20and%2013%20subcategories%20of%20challenges%20in%20long-tailed%20scene%20text%2C%20upon%0Awhich%20we%20propose%20a%20Long-Tailed%20Benchmark%20%28LTB%29.%20LTB%20facilitates%20a%20comprehensive%0Aevaluation%20of%20ability%20to%20handle%20a%20diverse%20range%20of%20long-tailed%20challenges.%20We%0Afurther%20introduce%20MAEDet%2C%20a%20self-supervised%20learning-based%20method%2C%20as%20a%20strong%0Abaseline%20for%20LTB.%20The%20code%20is%20available%20at%20https%3A//github.com/pd162/LTB.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15649v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Devil%2520is%2520in%2520Fine-tuning%2520and%2520Long-tailed%2520Problems%253AA%2520New%2520Benchmark%2520for%250A%2520%2520Scene%2520Text%2520Detection%26entry.906535625%3DTianjiao%2520Cao%2520and%2520Jiahao%2520Lyu%2520and%2520Weichao%2520Zeng%2520and%2520Weimin%2520Mu%2520and%2520Yu%2520Zhou%26entry.1292438233%3D%2520%2520Scene%2520text%2520detection%2520has%2520seen%2520the%2520emergence%2520of%2520high-performing%2520methods%2520that%250Aexcel%2520on%2520academic%2520benchmarks.%2520However%252C%2520these%2520detectors%2520often%2520fail%2520to%2520replicate%250Asuch%2520success%2520in%2520real-world%2520scenarios.%2520We%2520uncover%2520two%2520key%2520factors%2520contributing%250Ato%2520this%2520discrepancy%2520through%2520extensive%2520experiments.%2520First%252C%2520a%2520%255Ctextit%257BFine-tuning%250AGap%257D%252C%2520where%2520models%2520leverage%2520%255Ctextit%257BDataset-Specific%2520Optimization%257D%2520%2528DSO%2529%250Aparadigm%2520for%2520one%2520domain%2520at%2520the%2520cost%2520of%2520reduced%2520effectiveness%2520in%2520others%252C%2520leads%250Ato%2520inflated%2520performances%2520on%2520academic%2520benchmarks.%2520Second%252C%2520the%2520suboptimal%250Aperformance%2520in%2520practical%2520settings%2520is%2520primarily%2520attributed%2520to%2520the%2520long-tailed%250Adistribution%2520of%2520texts%252C%2520where%2520detectors%2520struggle%2520with%2520rare%2520and%2520complex%250Acategories%2520as%2520artistic%2520or%2520overlapped%2520text.%2520Given%2520that%2520the%2520DSO%2520paradigm%2520might%250Aundermine%2520the%2520generalization%2520ability%2520of%2520models%252C%2520we%2520advocate%2520for%2520a%250A%255Ctextit%257BJoint-Dataset%2520Learning%257D%2520%2528JDL%2529%2520protocol%2520to%2520alleviate%2520the%2520Fine-tuning%250AGap.%2520Additionally%252C%2520an%2520error%2520analysis%2520is%2520conducted%2520to%2520identify%2520three%2520major%250Acategories%2520and%252013%2520subcategories%2520of%2520challenges%2520in%2520long-tailed%2520scene%2520text%252C%2520upon%250Awhich%2520we%2520propose%2520a%2520Long-Tailed%2520Benchmark%2520%2528LTB%2529.%2520LTB%2520facilitates%2520a%2520comprehensive%250Aevaluation%2520of%2520ability%2520to%2520handle%2520a%2520diverse%2520range%2520of%2520long-tailed%2520challenges.%2520We%250Afurther%2520introduce%2520MAEDet%252C%2520a%2520self-supervised%2520learning-based%2520method%252C%2520as%2520a%2520strong%250Abaseline%2520for%2520LTB.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/pd162/LTB.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15649v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Devil%20is%20in%20Fine-tuning%20and%20Long-tailed%20Problems%3AA%20New%20Benchmark%20for%0A%20%20Scene%20Text%20Detection&entry.906535625=Tianjiao%20Cao%20and%20Jiahao%20Lyu%20and%20Weichao%20Zeng%20and%20Weimin%20Mu%20and%20Yu%20Zhou&entry.1292438233=%20%20Scene%20text%20detection%20has%20seen%20the%20emergence%20of%20high-performing%20methods%20that%0Aexcel%20on%20academic%20benchmarks.%20However%2C%20these%20detectors%20often%20fail%20to%20replicate%0Asuch%20success%20in%20real-world%20scenarios.%20We%20uncover%20two%20key%20factors%20contributing%0Ato%20this%20discrepancy%20through%20extensive%20experiments.%20First%2C%20a%20%5Ctextit%7BFine-tuning%0AGap%7D%2C%20where%20models%20leverage%20%5Ctextit%7BDataset-Specific%20Optimization%7D%20%28DSO%29%0Aparadigm%20for%20one%20domain%20at%20the%20cost%20of%20reduced%20effectiveness%20in%20others%2C%20leads%0Ato%20inflated%20performances%20on%20academic%20benchmarks.%20Second%2C%20the%20suboptimal%0Aperformance%20in%20practical%20settings%20is%20primarily%20attributed%20to%20the%20long-tailed%0Adistribution%20of%20texts%2C%20where%20detectors%20struggle%20with%20rare%20and%20complex%0Acategories%20as%20artistic%20or%20overlapped%20text.%20Given%20that%20the%20DSO%20paradigm%20might%0Aundermine%20the%20generalization%20ability%20of%20models%2C%20we%20advocate%20for%20a%0A%5Ctextit%7BJoint-Dataset%20Learning%7D%20%28JDL%29%20protocol%20to%20alleviate%20the%20Fine-tuning%0AGap.%20Additionally%2C%20an%20error%20analysis%20is%20conducted%20to%20identify%20three%20major%0Acategories%20and%2013%20subcategories%20of%20challenges%20in%20long-tailed%20scene%20text%2C%20upon%0Awhich%20we%20propose%20a%20Long-Tailed%20Benchmark%20%28LTB%29.%20LTB%20facilitates%20a%20comprehensive%0Aevaluation%20of%20ability%20to%20handle%20a%20diverse%20range%20of%20long-tailed%20challenges.%20We%0Afurther%20introduce%20MAEDet%2C%20a%20self-supervised%20learning-based%20method%2C%20as%20a%20strong%0Abaseline%20for%20LTB.%20The%20code%20is%20available%20at%20https%3A//github.com/pd162/LTB.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15649v1&entry.124074799=Read"},
{"title": "Refining Neural Activation Patterns for Layer-Level Concept Discovery in\n  Neural Network-Based Receivers", "author": "Marko Tuononen and Duy Vu and Dani Korpi and Vesa Starck and Ville Hautam\u00e4ki", "abstract": "  Concept discovery in neural networks often targets individual neurons or\nhuman-interpretable features, overlooking distributed layer-wide patterns. We\nstudy the Neural Activation Pattern (NAP) methodology, which clusters\nfull-layer activation distributions to identify such layer-level concepts.\nApplied to visual object recognition and radio receiver models, we propose\nimproved normalization, distribution estimation, distance metrics, and varied\ncluster selection. In the radio receiver model, distinct concepts did not\nemerge; instead, a continuous activation manifold shaped by Signal-to-Noise\nRatio (SNR) was observed -- highlighting SNR as a key learned factor,\nconsistent with classical receiver behavior and supporting physical\nplausibility. Our enhancements to NAP improved in-distribution vs.\nout-of-distribution separation, suggesting better generalization and indirectly\nvalidating clustering quality. These results underscore the importance of\nclustering design and activation manifolds in interpreting and troubleshooting\nneural network behavior.\n", "link": "http://arxiv.org/abs/2505.15570v1", "date": "2025-05-21", "relevancy": 2.4087, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4848}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4802}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Refining%20Neural%20Activation%20Patterns%20for%20Layer-Level%20Concept%20Discovery%20in%0A%20%20Neural%20Network-Based%20Receivers&body=Title%3A%20Refining%20Neural%20Activation%20Patterns%20for%20Layer-Level%20Concept%20Discovery%20in%0A%20%20Neural%20Network-Based%20Receivers%0AAuthor%3A%20Marko%20Tuononen%20and%20Duy%20Vu%20and%20Dani%20Korpi%20and%20Vesa%20Starck%20and%20Ville%20Hautam%C3%A4ki%0AAbstract%3A%20%20%20Concept%20discovery%20in%20neural%20networks%20often%20targets%20individual%20neurons%20or%0Ahuman-interpretable%20features%2C%20overlooking%20distributed%20layer-wide%20patterns.%20We%0Astudy%20the%20Neural%20Activation%20Pattern%20%28NAP%29%20methodology%2C%20which%20clusters%0Afull-layer%20activation%20distributions%20to%20identify%20such%20layer-level%20concepts.%0AApplied%20to%20visual%20object%20recognition%20and%20radio%20receiver%20models%2C%20we%20propose%0Aimproved%20normalization%2C%20distribution%20estimation%2C%20distance%20metrics%2C%20and%20varied%0Acluster%20selection.%20In%20the%20radio%20receiver%20model%2C%20distinct%20concepts%20did%20not%0Aemerge%3B%20instead%2C%20a%20continuous%20activation%20manifold%20shaped%20by%20Signal-to-Noise%0ARatio%20%28SNR%29%20was%20observed%20--%20highlighting%20SNR%20as%20a%20key%20learned%20factor%2C%0Aconsistent%20with%20classical%20receiver%20behavior%20and%20supporting%20physical%0Aplausibility.%20Our%20enhancements%20to%20NAP%20improved%20in-distribution%20vs.%0Aout-of-distribution%20separation%2C%20suggesting%20better%20generalization%20and%20indirectly%0Avalidating%20clustering%20quality.%20These%20results%20underscore%20the%20importance%20of%0Aclustering%20design%20and%20activation%20manifolds%20in%20interpreting%20and%20troubleshooting%0Aneural%20network%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15570v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRefining%2520Neural%2520Activation%2520Patterns%2520for%2520Layer-Level%2520Concept%2520Discovery%2520in%250A%2520%2520Neural%2520Network-Based%2520Receivers%26entry.906535625%3DMarko%2520Tuononen%2520and%2520Duy%2520Vu%2520and%2520Dani%2520Korpi%2520and%2520Vesa%2520Starck%2520and%2520Ville%2520Hautam%25C3%25A4ki%26entry.1292438233%3D%2520%2520Concept%2520discovery%2520in%2520neural%2520networks%2520often%2520targets%2520individual%2520neurons%2520or%250Ahuman-interpretable%2520features%252C%2520overlooking%2520distributed%2520layer-wide%2520patterns.%2520We%250Astudy%2520the%2520Neural%2520Activation%2520Pattern%2520%2528NAP%2529%2520methodology%252C%2520which%2520clusters%250Afull-layer%2520activation%2520distributions%2520to%2520identify%2520such%2520layer-level%2520concepts.%250AApplied%2520to%2520visual%2520object%2520recognition%2520and%2520radio%2520receiver%2520models%252C%2520we%2520propose%250Aimproved%2520normalization%252C%2520distribution%2520estimation%252C%2520distance%2520metrics%252C%2520and%2520varied%250Acluster%2520selection.%2520In%2520the%2520radio%2520receiver%2520model%252C%2520distinct%2520concepts%2520did%2520not%250Aemerge%253B%2520instead%252C%2520a%2520continuous%2520activation%2520manifold%2520shaped%2520by%2520Signal-to-Noise%250ARatio%2520%2528SNR%2529%2520was%2520observed%2520--%2520highlighting%2520SNR%2520as%2520a%2520key%2520learned%2520factor%252C%250Aconsistent%2520with%2520classical%2520receiver%2520behavior%2520and%2520supporting%2520physical%250Aplausibility.%2520Our%2520enhancements%2520to%2520NAP%2520improved%2520in-distribution%2520vs.%250Aout-of-distribution%2520separation%252C%2520suggesting%2520better%2520generalization%2520and%2520indirectly%250Avalidating%2520clustering%2520quality.%2520These%2520results%2520underscore%2520the%2520importance%2520of%250Aclustering%2520design%2520and%2520activation%2520manifolds%2520in%2520interpreting%2520and%2520troubleshooting%250Aneural%2520network%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15570v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Refining%20Neural%20Activation%20Patterns%20for%20Layer-Level%20Concept%20Discovery%20in%0A%20%20Neural%20Network-Based%20Receivers&entry.906535625=Marko%20Tuononen%20and%20Duy%20Vu%20and%20Dani%20Korpi%20and%20Vesa%20Starck%20and%20Ville%20Hautam%C3%A4ki&entry.1292438233=%20%20Concept%20discovery%20in%20neural%20networks%20often%20targets%20individual%20neurons%20or%0Ahuman-interpretable%20features%2C%20overlooking%20distributed%20layer-wide%20patterns.%20We%0Astudy%20the%20Neural%20Activation%20Pattern%20%28NAP%29%20methodology%2C%20which%20clusters%0Afull-layer%20activation%20distributions%20to%20identify%20such%20layer-level%20concepts.%0AApplied%20to%20visual%20object%20recognition%20and%20radio%20receiver%20models%2C%20we%20propose%0Aimproved%20normalization%2C%20distribution%20estimation%2C%20distance%20metrics%2C%20and%20varied%0Acluster%20selection.%20In%20the%20radio%20receiver%20model%2C%20distinct%20concepts%20did%20not%0Aemerge%3B%20instead%2C%20a%20continuous%20activation%20manifold%20shaped%20by%20Signal-to-Noise%0ARatio%20%28SNR%29%20was%20observed%20--%20highlighting%20SNR%20as%20a%20key%20learned%20factor%2C%0Aconsistent%20with%20classical%20receiver%20behavior%20and%20supporting%20physical%0Aplausibility.%20Our%20enhancements%20to%20NAP%20improved%20in-distribution%20vs.%0Aout-of-distribution%20separation%2C%20suggesting%20better%20generalization%20and%20indirectly%0Avalidating%20clustering%20quality.%20These%20results%20underscore%20the%20importance%20of%0Aclustering%20design%20and%20activation%20manifolds%20in%20interpreting%20and%20troubleshooting%0Aneural%20network%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15570v1&entry.124074799=Read"},
{"title": "Moonbeam: A MIDI Foundation Model Using Both Absolute and Relative Music\n  Attributes", "author": "Zixun Guo and Simon Dixon", "abstract": "  Moonbeam is a transformer-based foundation model for symbolic music,\npretrained on a large and diverse collection of MIDI data totaling 81.6K hours\nof music and 18 billion tokens. Moonbeam incorporates music-domain inductive\nbiases by capturing both absolute and relative musical attributes through the\nintroduction of a novel domain-knowledge-inspired tokenization method and\nMultidimensional Relative Attention (MRA), which captures relative music\ninformation without additional trainable parameters. Leveraging the pretrained\nMoonbeam, we propose 2 finetuning architectures with full anticipatory\ncapabilities, targeting 2 categories of downstream tasks: symbolic music\nunderstanding and conditional music generation (including music infilling). Our\nmodel outperforms other large-scale pretrained music models in most cases in\nterms of accuracy and F1 score across 3 downstream music classification tasks\non 4 datasets. Moreover, our finetuned conditional music generation model\noutperforms a strong transformer baseline with a REMI-like tokenizer. We\nopen-source the code, pretrained model, and generated samples on Github.\n", "link": "http://arxiv.org/abs/2505.15559v1", "date": "2025-05-21", "relevancy": 2.4086, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.496}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4746}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Moonbeam%3A%20A%20MIDI%20Foundation%20Model%20Using%20Both%20Absolute%20and%20Relative%20Music%0A%20%20Attributes&body=Title%3A%20Moonbeam%3A%20A%20MIDI%20Foundation%20Model%20Using%20Both%20Absolute%20and%20Relative%20Music%0A%20%20Attributes%0AAuthor%3A%20Zixun%20Guo%20and%20Simon%20Dixon%0AAbstract%3A%20%20%20Moonbeam%20is%20a%20transformer-based%20foundation%20model%20for%20symbolic%20music%2C%0Apretrained%20on%20a%20large%20and%20diverse%20collection%20of%20MIDI%20data%20totaling%2081.6K%20hours%0Aof%20music%20and%2018%20billion%20tokens.%20Moonbeam%20incorporates%20music-domain%20inductive%0Abiases%20by%20capturing%20both%20absolute%20and%20relative%20musical%20attributes%20through%20the%0Aintroduction%20of%20a%20novel%20domain-knowledge-inspired%20tokenization%20method%20and%0AMultidimensional%20Relative%20Attention%20%28MRA%29%2C%20which%20captures%20relative%20music%0Ainformation%20without%20additional%20trainable%20parameters.%20Leveraging%20the%20pretrained%0AMoonbeam%2C%20we%20propose%202%20finetuning%20architectures%20with%20full%20anticipatory%0Acapabilities%2C%20targeting%202%20categories%20of%20downstream%20tasks%3A%20symbolic%20music%0Aunderstanding%20and%20conditional%20music%20generation%20%28including%20music%20infilling%29.%20Our%0Amodel%20outperforms%20other%20large-scale%20pretrained%20music%20models%20in%20most%20cases%20in%0Aterms%20of%20accuracy%20and%20F1%20score%20across%203%20downstream%20music%20classification%20tasks%0Aon%204%20datasets.%20Moreover%2C%20our%20finetuned%20conditional%20music%20generation%20model%0Aoutperforms%20a%20strong%20transformer%20baseline%20with%20a%20REMI-like%20tokenizer.%20We%0Aopen-source%20the%20code%2C%20pretrained%20model%2C%20and%20generated%20samples%20on%20Github.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15559v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoonbeam%253A%2520A%2520MIDI%2520Foundation%2520Model%2520Using%2520Both%2520Absolute%2520and%2520Relative%2520Music%250A%2520%2520Attributes%26entry.906535625%3DZixun%2520Guo%2520and%2520Simon%2520Dixon%26entry.1292438233%3D%2520%2520Moonbeam%2520is%2520a%2520transformer-based%2520foundation%2520model%2520for%2520symbolic%2520music%252C%250Apretrained%2520on%2520a%2520large%2520and%2520diverse%2520collection%2520of%2520MIDI%2520data%2520totaling%252081.6K%2520hours%250Aof%2520music%2520and%252018%2520billion%2520tokens.%2520Moonbeam%2520incorporates%2520music-domain%2520inductive%250Abiases%2520by%2520capturing%2520both%2520absolute%2520and%2520relative%2520musical%2520attributes%2520through%2520the%250Aintroduction%2520of%2520a%2520novel%2520domain-knowledge-inspired%2520tokenization%2520method%2520and%250AMultidimensional%2520Relative%2520Attention%2520%2528MRA%2529%252C%2520which%2520captures%2520relative%2520music%250Ainformation%2520without%2520additional%2520trainable%2520parameters.%2520Leveraging%2520the%2520pretrained%250AMoonbeam%252C%2520we%2520propose%25202%2520finetuning%2520architectures%2520with%2520full%2520anticipatory%250Acapabilities%252C%2520targeting%25202%2520categories%2520of%2520downstream%2520tasks%253A%2520symbolic%2520music%250Aunderstanding%2520and%2520conditional%2520music%2520generation%2520%2528including%2520music%2520infilling%2529.%2520Our%250Amodel%2520outperforms%2520other%2520large-scale%2520pretrained%2520music%2520models%2520in%2520most%2520cases%2520in%250Aterms%2520of%2520accuracy%2520and%2520F1%2520score%2520across%25203%2520downstream%2520music%2520classification%2520tasks%250Aon%25204%2520datasets.%2520Moreover%252C%2520our%2520finetuned%2520conditional%2520music%2520generation%2520model%250Aoutperforms%2520a%2520strong%2520transformer%2520baseline%2520with%2520a%2520REMI-like%2520tokenizer.%2520We%250Aopen-source%2520the%2520code%252C%2520pretrained%2520model%252C%2520and%2520generated%2520samples%2520on%2520Github.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15559v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Moonbeam%3A%20A%20MIDI%20Foundation%20Model%20Using%20Both%20Absolute%20and%20Relative%20Music%0A%20%20Attributes&entry.906535625=Zixun%20Guo%20and%20Simon%20Dixon&entry.1292438233=%20%20Moonbeam%20is%20a%20transformer-based%20foundation%20model%20for%20symbolic%20music%2C%0Apretrained%20on%20a%20large%20and%20diverse%20collection%20of%20MIDI%20data%20totaling%2081.6K%20hours%0Aof%20music%20and%2018%20billion%20tokens.%20Moonbeam%20incorporates%20music-domain%20inductive%0Abiases%20by%20capturing%20both%20absolute%20and%20relative%20musical%20attributes%20through%20the%0Aintroduction%20of%20a%20novel%20domain-knowledge-inspired%20tokenization%20method%20and%0AMultidimensional%20Relative%20Attention%20%28MRA%29%2C%20which%20captures%20relative%20music%0Ainformation%20without%20additional%20trainable%20parameters.%20Leveraging%20the%20pretrained%0AMoonbeam%2C%20we%20propose%202%20finetuning%20architectures%20with%20full%20anticipatory%0Acapabilities%2C%20targeting%202%20categories%20of%20downstream%20tasks%3A%20symbolic%20music%0Aunderstanding%20and%20conditional%20music%20generation%20%28including%20music%20infilling%29.%20Our%0Amodel%20outperforms%20other%20large-scale%20pretrained%20music%20models%20in%20most%20cases%20in%0Aterms%20of%20accuracy%20and%20F1%20score%20across%203%20downstream%20music%20classification%20tasks%0Aon%204%20datasets.%20Moreover%2C%20our%20finetuned%20conditional%20music%20generation%20model%0Aoutperforms%20a%20strong%20transformer%20baseline%20with%20a%20REMI-like%20tokenizer.%20We%0Aopen-source%20the%20code%2C%20pretrained%20model%2C%20and%20generated%20samples%20on%20Github.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15559v1&entry.124074799=Read"},
{"title": "Exploring the Innovation Opportunities for Pre-trained Models", "author": "Minjung Park and Jodi Forlizzi and John Zimmerman", "abstract": "  Innovators transform the world by understanding where services are\nsuccessfully meeting customers' needs and then using this knowledge to identify\nfailsafe opportunities for innovation. Pre-trained models have changed the AI\ninnovation landscape, making it faster and easier to create new AI products and\nservices. Understanding where pre-trained models are successful is critical for\nsupporting AI innovation. Unfortunately, the hype cycle surrounding pre-trained\nmodels makes it hard to know where AI can really be successful. To address\nthis, we investigated pre-trained model applications developed by HCI\nresearchers as a proxy for commercially successful applications. The research\napplications demonstrate technical capabilities, address real user needs, and\navoid ethical challenges. Using an artifact analysis approach, we categorized\ncapabilities, opportunity domains, data types, and emerging interaction design\npatterns, uncovering some of the opportunity space for innovation with\npre-trained models.\n", "link": "http://arxiv.org/abs/2505.15790v1", "date": "2025-05-21", "relevancy": 2.4069, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4867}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4867}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Innovation%20Opportunities%20for%20Pre-trained%20Models&body=Title%3A%20Exploring%20the%20Innovation%20Opportunities%20for%20Pre-trained%20Models%0AAuthor%3A%20Minjung%20Park%20and%20Jodi%20Forlizzi%20and%20John%20Zimmerman%0AAbstract%3A%20%20%20Innovators%20transform%20the%20world%20by%20understanding%20where%20services%20are%0Asuccessfully%20meeting%20customers%27%20needs%20and%20then%20using%20this%20knowledge%20to%20identify%0Afailsafe%20opportunities%20for%20innovation.%20Pre-trained%20models%20have%20changed%20the%20AI%0Ainnovation%20landscape%2C%20making%20it%20faster%20and%20easier%20to%20create%20new%20AI%20products%20and%0Aservices.%20Understanding%20where%20pre-trained%20models%20are%20successful%20is%20critical%20for%0Asupporting%20AI%20innovation.%20Unfortunately%2C%20the%20hype%20cycle%20surrounding%20pre-trained%0Amodels%20makes%20it%20hard%20to%20know%20where%20AI%20can%20really%20be%20successful.%20To%20address%0Athis%2C%20we%20investigated%20pre-trained%20model%20applications%20developed%20by%20HCI%0Aresearchers%20as%20a%20proxy%20for%20commercially%20successful%20applications.%20The%20research%0Aapplications%20demonstrate%20technical%20capabilities%2C%20address%20real%20user%20needs%2C%20and%0Aavoid%20ethical%20challenges.%20Using%20an%20artifact%20analysis%20approach%2C%20we%20categorized%0Acapabilities%2C%20opportunity%20domains%2C%20data%20types%2C%20and%20emerging%20interaction%20design%0Apatterns%2C%20uncovering%20some%20of%20the%20opportunity%20space%20for%20innovation%20with%0Apre-trained%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15790v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Innovation%2520Opportunities%2520for%2520Pre-trained%2520Models%26entry.906535625%3DMinjung%2520Park%2520and%2520Jodi%2520Forlizzi%2520and%2520John%2520Zimmerman%26entry.1292438233%3D%2520%2520Innovators%2520transform%2520the%2520world%2520by%2520understanding%2520where%2520services%2520are%250Asuccessfully%2520meeting%2520customers%2527%2520needs%2520and%2520then%2520using%2520this%2520knowledge%2520to%2520identify%250Afailsafe%2520opportunities%2520for%2520innovation.%2520Pre-trained%2520models%2520have%2520changed%2520the%2520AI%250Ainnovation%2520landscape%252C%2520making%2520it%2520faster%2520and%2520easier%2520to%2520create%2520new%2520AI%2520products%2520and%250Aservices.%2520Understanding%2520where%2520pre-trained%2520models%2520are%2520successful%2520is%2520critical%2520for%250Asupporting%2520AI%2520innovation.%2520Unfortunately%252C%2520the%2520hype%2520cycle%2520surrounding%2520pre-trained%250Amodels%2520makes%2520it%2520hard%2520to%2520know%2520where%2520AI%2520can%2520really%2520be%2520successful.%2520To%2520address%250Athis%252C%2520we%2520investigated%2520pre-trained%2520model%2520applications%2520developed%2520by%2520HCI%250Aresearchers%2520as%2520a%2520proxy%2520for%2520commercially%2520successful%2520applications.%2520The%2520research%250Aapplications%2520demonstrate%2520technical%2520capabilities%252C%2520address%2520real%2520user%2520needs%252C%2520and%250Aavoid%2520ethical%2520challenges.%2520Using%2520an%2520artifact%2520analysis%2520approach%252C%2520we%2520categorized%250Acapabilities%252C%2520opportunity%2520domains%252C%2520data%2520types%252C%2520and%2520emerging%2520interaction%2520design%250Apatterns%252C%2520uncovering%2520some%2520of%2520the%2520opportunity%2520space%2520for%2520innovation%2520with%250Apre-trained%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15790v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Innovation%20Opportunities%20for%20Pre-trained%20Models&entry.906535625=Minjung%20Park%20and%20Jodi%20Forlizzi%20and%20John%20Zimmerman&entry.1292438233=%20%20Innovators%20transform%20the%20world%20by%20understanding%20where%20services%20are%0Asuccessfully%20meeting%20customers%27%20needs%20and%20then%20using%20this%20knowledge%20to%20identify%0Afailsafe%20opportunities%20for%20innovation.%20Pre-trained%20models%20have%20changed%20the%20AI%0Ainnovation%20landscape%2C%20making%20it%20faster%20and%20easier%20to%20create%20new%20AI%20products%20and%0Aservices.%20Understanding%20where%20pre-trained%20models%20are%20successful%20is%20critical%20for%0Asupporting%20AI%20innovation.%20Unfortunately%2C%20the%20hype%20cycle%20surrounding%20pre-trained%0Amodels%20makes%20it%20hard%20to%20know%20where%20AI%20can%20really%20be%20successful.%20To%20address%0Athis%2C%20we%20investigated%20pre-trained%20model%20applications%20developed%20by%20HCI%0Aresearchers%20as%20a%20proxy%20for%20commercially%20successful%20applications.%20The%20research%0Aapplications%20demonstrate%20technical%20capabilities%2C%20address%20real%20user%20needs%2C%20and%0Aavoid%20ethical%20challenges.%20Using%20an%20artifact%20analysis%20approach%2C%20we%20categorized%0Acapabilities%2C%20opportunity%20domains%2C%20data%20types%2C%20and%20emerging%20interaction%20design%0Apatterns%2C%20uncovering%20some%20of%20the%20opportunity%20space%20for%20innovation%20with%0Apre-trained%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15790v1&entry.124074799=Read"},
{"title": "Training NTK to Generalize with KARE", "author": "Johannes Schwab and Bryan Kelly and Semyon Malamud and Teng Andrea Xu", "abstract": "  The performance of the data-dependent neural tangent kernel (NTK; Jacot et\nal. (2018)) associated with a trained deep neural network (DNN) often matches\nor exceeds that of the full network. This implies that DNN training via\ngradient descent implicitly performs kernel learning by optimizing the NTK. In\nthis paper, we propose instead to optimize the NTK explicitly. Rather than\nminimizing empirical risk, we train the NTK to minimize its generalization\nerror using the recently developed Kernel Alignment Risk Estimator (KARE; Jacot\net al. (2020)). Our simulations and real data experiments show that NTKs\ntrained with KARE consistently match or significantly outperform the original\nDNN and the DNN- induced NTK (the after-kernel). These results suggest that\nexplicitly trained kernels can outperform traditional end-to-end DNN\noptimization in certain settings, challenging the conventional dominance of\nDNNs. We argue that explicit training of NTK is a form of over-parametrized\nfeature learning.\n", "link": "http://arxiv.org/abs/2505.11347v2", "date": "2025-05-21", "relevancy": 2.4009, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5201}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4779}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20NTK%20to%20Generalize%20with%20KARE&body=Title%3A%20Training%20NTK%20to%20Generalize%20with%20KARE%0AAuthor%3A%20Johannes%20Schwab%20and%20Bryan%20Kelly%20and%20Semyon%20Malamud%20and%20Teng%20Andrea%20Xu%0AAbstract%3A%20%20%20The%20performance%20of%20the%20data-dependent%20neural%20tangent%20kernel%20%28NTK%3B%20Jacot%20et%0Aal.%20%282018%29%29%20associated%20with%20a%20trained%20deep%20neural%20network%20%28DNN%29%20often%20matches%0Aor%20exceeds%20that%20of%20the%20full%20network.%20This%20implies%20that%20DNN%20training%20via%0Agradient%20descent%20implicitly%20performs%20kernel%20learning%20by%20optimizing%20the%20NTK.%20In%0Athis%20paper%2C%20we%20propose%20instead%20to%20optimize%20the%20NTK%20explicitly.%20Rather%20than%0Aminimizing%20empirical%20risk%2C%20we%20train%20the%20NTK%20to%20minimize%20its%20generalization%0Aerror%20using%20the%20recently%20developed%20Kernel%20Alignment%20Risk%20Estimator%20%28KARE%3B%20Jacot%0Aet%20al.%20%282020%29%29.%20Our%20simulations%20and%20real%20data%20experiments%20show%20that%20NTKs%0Atrained%20with%20KARE%20consistently%20match%20or%20significantly%20outperform%20the%20original%0ADNN%20and%20the%20DNN-%20induced%20NTK%20%28the%20after-kernel%29.%20These%20results%20suggest%20that%0Aexplicitly%20trained%20kernels%20can%20outperform%20traditional%20end-to-end%20DNN%0Aoptimization%20in%20certain%20settings%2C%20challenging%20the%20conventional%20dominance%20of%0ADNNs.%20We%20argue%20that%20explicit%20training%20of%20NTK%20is%20a%20form%20of%20over-parametrized%0Afeature%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11347v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520NTK%2520to%2520Generalize%2520with%2520KARE%26entry.906535625%3DJohannes%2520Schwab%2520and%2520Bryan%2520Kelly%2520and%2520Semyon%2520Malamud%2520and%2520Teng%2520Andrea%2520Xu%26entry.1292438233%3D%2520%2520The%2520performance%2520of%2520the%2520data-dependent%2520neural%2520tangent%2520kernel%2520%2528NTK%253B%2520Jacot%2520et%250Aal.%2520%25282018%2529%2529%2520associated%2520with%2520a%2520trained%2520deep%2520neural%2520network%2520%2528DNN%2529%2520often%2520matches%250Aor%2520exceeds%2520that%2520of%2520the%2520full%2520network.%2520This%2520implies%2520that%2520DNN%2520training%2520via%250Agradient%2520descent%2520implicitly%2520performs%2520kernel%2520learning%2520by%2520optimizing%2520the%2520NTK.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520instead%2520to%2520optimize%2520the%2520NTK%2520explicitly.%2520Rather%2520than%250Aminimizing%2520empirical%2520risk%252C%2520we%2520train%2520the%2520NTK%2520to%2520minimize%2520its%2520generalization%250Aerror%2520using%2520the%2520recently%2520developed%2520Kernel%2520Alignment%2520Risk%2520Estimator%2520%2528KARE%253B%2520Jacot%250Aet%2520al.%2520%25282020%2529%2529.%2520Our%2520simulations%2520and%2520real%2520data%2520experiments%2520show%2520that%2520NTKs%250Atrained%2520with%2520KARE%2520consistently%2520match%2520or%2520significantly%2520outperform%2520the%2520original%250ADNN%2520and%2520the%2520DNN-%2520induced%2520NTK%2520%2528the%2520after-kernel%2529.%2520These%2520results%2520suggest%2520that%250Aexplicitly%2520trained%2520kernels%2520can%2520outperform%2520traditional%2520end-to-end%2520DNN%250Aoptimization%2520in%2520certain%2520settings%252C%2520challenging%2520the%2520conventional%2520dominance%2520of%250ADNNs.%2520We%2520argue%2520that%2520explicit%2520training%2520of%2520NTK%2520is%2520a%2520form%2520of%2520over-parametrized%250Afeature%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11347v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20NTK%20to%20Generalize%20with%20KARE&entry.906535625=Johannes%20Schwab%20and%20Bryan%20Kelly%20and%20Semyon%20Malamud%20and%20Teng%20Andrea%20Xu&entry.1292438233=%20%20The%20performance%20of%20the%20data-dependent%20neural%20tangent%20kernel%20%28NTK%3B%20Jacot%20et%0Aal.%20%282018%29%29%20associated%20with%20a%20trained%20deep%20neural%20network%20%28DNN%29%20often%20matches%0Aor%20exceeds%20that%20of%20the%20full%20network.%20This%20implies%20that%20DNN%20training%20via%0Agradient%20descent%20implicitly%20performs%20kernel%20learning%20by%20optimizing%20the%20NTK.%20In%0Athis%20paper%2C%20we%20propose%20instead%20to%20optimize%20the%20NTK%20explicitly.%20Rather%20than%0Aminimizing%20empirical%20risk%2C%20we%20train%20the%20NTK%20to%20minimize%20its%20generalization%0Aerror%20using%20the%20recently%20developed%20Kernel%20Alignment%20Risk%20Estimator%20%28KARE%3B%20Jacot%0Aet%20al.%20%282020%29%29.%20Our%20simulations%20and%20real%20data%20experiments%20show%20that%20NTKs%0Atrained%20with%20KARE%20consistently%20match%20or%20significantly%20outperform%20the%20original%0ADNN%20and%20the%20DNN-%20induced%20NTK%20%28the%20after-kernel%29.%20These%20results%20suggest%20that%0Aexplicitly%20trained%20kernels%20can%20outperform%20traditional%20end-to-end%20DNN%0Aoptimization%20in%20certain%20settings%2C%20challenging%20the%20conventional%20dominance%20of%0ADNNs.%20We%20argue%20that%20explicit%20training%20of%20NTK%20is%20a%20form%20of%20over-parametrized%0Afeature%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11347v2&entry.124074799=Read"},
{"title": "HV-BEV: Decoupling Horizontal and Vertical Feature Sampling for\n  Multi-View 3D Object Detection", "author": "Di Wu and Feng Yang and Benlian Xu and Pan Liao and Wenhui Zhao and Dingwen Zhang", "abstract": "  The application of vision-based multi-view environmental perception system\nhas been increasingly recognized in autonomous driving technology, especially\nthe BEV-based models. Current state-of-the-art solutions primarily encode image\nfeatures from each camera view into the BEV space through explicit or implicit\ndepth prediction. However, these methods often overlook the structured\ncorrelations among different parts of objects in 3D space and the fact that\ndifferent categories of objects often occupy distinct local height ranges. For\nexample, trucks appear at higher elevations, whereas traffic cones are near the\nground. In this work, we propose a novel approach that decouples feature\nsampling in the \\textbf{BEV} grid queries paradigm into \\textbf{H}orizontal\nfeature aggregation and \\textbf{V}ertical adaptive height-aware reference point\nsampling (HV-BEV), aiming to improve both the aggregation of objects' complete\ninformation and awareness of diverse objects' height distribution.\nSpecifically, a set of relevant neighboring points is dynamically constructed\nfor each 3D reference point on the ground-aligned horizontal plane, enhancing\nthe association of the same instance across different BEV grids, especially\nwhen the instance spans multiple image views around the vehicle. Additionally,\ninstead of relying on uniform sampling within a fixed height range, we\nintroduce a height-aware module that incorporates historical information,\nenabling the reference points to adaptively focus on the varying heights at\nwhich objects appear in different scenes. Extensive experiments validate the\neffectiveness of our proposed method, demonstrating its superior performance\nover the baseline across the nuScenes dataset. Moreover, our best-performing\nmodel achieves a remarkable 50.5\\% mAP and 59.8\\% NDS on the nuScenes testing\nset. The code is available at https://github.com/Uddd821/HV-BEV.\n", "link": "http://arxiv.org/abs/2412.18884v3", "date": "2025-05-21", "relevancy": 2.3788, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6177}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5901}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HV-BEV%3A%20Decoupling%20Horizontal%20and%20Vertical%20Feature%20Sampling%20for%0A%20%20Multi-View%203D%20Object%20Detection&body=Title%3A%20HV-BEV%3A%20Decoupling%20Horizontal%20and%20Vertical%20Feature%20Sampling%20for%0A%20%20Multi-View%203D%20Object%20Detection%0AAuthor%3A%20Di%20Wu%20and%20Feng%20Yang%20and%20Benlian%20Xu%20and%20Pan%20Liao%20and%20Wenhui%20Zhao%20and%20Dingwen%20Zhang%0AAbstract%3A%20%20%20The%20application%20of%20vision-based%20multi-view%20environmental%20perception%20system%0Ahas%20been%20increasingly%20recognized%20in%20autonomous%20driving%20technology%2C%20especially%0Athe%20BEV-based%20models.%20Current%20state-of-the-art%20solutions%20primarily%20encode%20image%0Afeatures%20from%20each%20camera%20view%20into%20the%20BEV%20space%20through%20explicit%20or%20implicit%0Adepth%20prediction.%20However%2C%20these%20methods%20often%20overlook%20the%20structured%0Acorrelations%20among%20different%20parts%20of%20objects%20in%203D%20space%20and%20the%20fact%20that%0Adifferent%20categories%20of%20objects%20often%20occupy%20distinct%20local%20height%20ranges.%20For%0Aexample%2C%20trucks%20appear%20at%20higher%20elevations%2C%20whereas%20traffic%20cones%20are%20near%20the%0Aground.%20In%20this%20work%2C%20we%20propose%20a%20novel%20approach%20that%20decouples%20feature%0Asampling%20in%20the%20%5Ctextbf%7BBEV%7D%20grid%20queries%20paradigm%20into%20%5Ctextbf%7BH%7Dorizontal%0Afeature%20aggregation%20and%20%5Ctextbf%7BV%7Dertical%20adaptive%20height-aware%20reference%20point%0Asampling%20%28HV-BEV%29%2C%20aiming%20to%20improve%20both%20the%20aggregation%20of%20objects%27%20complete%0Ainformation%20and%20awareness%20of%20diverse%20objects%27%20height%20distribution.%0ASpecifically%2C%20a%20set%20of%20relevant%20neighboring%20points%20is%20dynamically%20constructed%0Afor%20each%203D%20reference%20point%20on%20the%20ground-aligned%20horizontal%20plane%2C%20enhancing%0Athe%20association%20of%20the%20same%20instance%20across%20different%20BEV%20grids%2C%20especially%0Awhen%20the%20instance%20spans%20multiple%20image%20views%20around%20the%20vehicle.%20Additionally%2C%0Ainstead%20of%20relying%20on%20uniform%20sampling%20within%20a%20fixed%20height%20range%2C%20we%0Aintroduce%20a%20height-aware%20module%20that%20incorporates%20historical%20information%2C%0Aenabling%20the%20reference%20points%20to%20adaptively%20focus%20on%20the%20varying%20heights%20at%0Awhich%20objects%20appear%20in%20different%20scenes.%20Extensive%20experiments%20validate%20the%0Aeffectiveness%20of%20our%20proposed%20method%2C%20demonstrating%20its%20superior%20performance%0Aover%20the%20baseline%20across%20the%20nuScenes%20dataset.%20Moreover%2C%20our%20best-performing%0Amodel%20achieves%20a%20remarkable%2050.5%5C%25%20mAP%20and%2059.8%5C%25%20NDS%20on%20the%20nuScenes%20testing%0Aset.%20The%20code%20is%20available%20at%20https%3A//github.com/Uddd821/HV-BEV.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18884v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHV-BEV%253A%2520Decoupling%2520Horizontal%2520and%2520Vertical%2520Feature%2520Sampling%2520for%250A%2520%2520Multi-View%25203D%2520Object%2520Detection%26entry.906535625%3DDi%2520Wu%2520and%2520Feng%2520Yang%2520and%2520Benlian%2520Xu%2520and%2520Pan%2520Liao%2520and%2520Wenhui%2520Zhao%2520and%2520Dingwen%2520Zhang%26entry.1292438233%3D%2520%2520The%2520application%2520of%2520vision-based%2520multi-view%2520environmental%2520perception%2520system%250Ahas%2520been%2520increasingly%2520recognized%2520in%2520autonomous%2520driving%2520technology%252C%2520especially%250Athe%2520BEV-based%2520models.%2520Current%2520state-of-the-art%2520solutions%2520primarily%2520encode%2520image%250Afeatures%2520from%2520each%2520camera%2520view%2520into%2520the%2520BEV%2520space%2520through%2520explicit%2520or%2520implicit%250Adepth%2520prediction.%2520However%252C%2520these%2520methods%2520often%2520overlook%2520the%2520structured%250Acorrelations%2520among%2520different%2520parts%2520of%2520objects%2520in%25203D%2520space%2520and%2520the%2520fact%2520that%250Adifferent%2520categories%2520of%2520objects%2520often%2520occupy%2520distinct%2520local%2520height%2520ranges.%2520For%250Aexample%252C%2520trucks%2520appear%2520at%2520higher%2520elevations%252C%2520whereas%2520traffic%2520cones%2520are%2520near%2520the%250Aground.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520approach%2520that%2520decouples%2520feature%250Asampling%2520in%2520the%2520%255Ctextbf%257BBEV%257D%2520grid%2520queries%2520paradigm%2520into%2520%255Ctextbf%257BH%257Dorizontal%250Afeature%2520aggregation%2520and%2520%255Ctextbf%257BV%257Dertical%2520adaptive%2520height-aware%2520reference%2520point%250Asampling%2520%2528HV-BEV%2529%252C%2520aiming%2520to%2520improve%2520both%2520the%2520aggregation%2520of%2520objects%2527%2520complete%250Ainformation%2520and%2520awareness%2520of%2520diverse%2520objects%2527%2520height%2520distribution.%250ASpecifically%252C%2520a%2520set%2520of%2520relevant%2520neighboring%2520points%2520is%2520dynamically%2520constructed%250Afor%2520each%25203D%2520reference%2520point%2520on%2520the%2520ground-aligned%2520horizontal%2520plane%252C%2520enhancing%250Athe%2520association%2520of%2520the%2520same%2520instance%2520across%2520different%2520BEV%2520grids%252C%2520especially%250Awhen%2520the%2520instance%2520spans%2520multiple%2520image%2520views%2520around%2520the%2520vehicle.%2520Additionally%252C%250Ainstead%2520of%2520relying%2520on%2520uniform%2520sampling%2520within%2520a%2520fixed%2520height%2520range%252C%2520we%250Aintroduce%2520a%2520height-aware%2520module%2520that%2520incorporates%2520historical%2520information%252C%250Aenabling%2520the%2520reference%2520points%2520to%2520adaptively%2520focus%2520on%2520the%2520varying%2520heights%2520at%250Awhich%2520objects%2520appear%2520in%2520different%2520scenes.%2520Extensive%2520experiments%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520proposed%2520method%252C%2520demonstrating%2520its%2520superior%2520performance%250Aover%2520the%2520baseline%2520across%2520the%2520nuScenes%2520dataset.%2520Moreover%252C%2520our%2520best-performing%250Amodel%2520achieves%2520a%2520remarkable%252050.5%255C%2525%2520mAP%2520and%252059.8%255C%2525%2520NDS%2520on%2520the%2520nuScenes%2520testing%250Aset.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/Uddd821/HV-BEV.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18884v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HV-BEV%3A%20Decoupling%20Horizontal%20and%20Vertical%20Feature%20Sampling%20for%0A%20%20Multi-View%203D%20Object%20Detection&entry.906535625=Di%20Wu%20and%20Feng%20Yang%20and%20Benlian%20Xu%20and%20Pan%20Liao%20and%20Wenhui%20Zhao%20and%20Dingwen%20Zhang&entry.1292438233=%20%20The%20application%20of%20vision-based%20multi-view%20environmental%20perception%20system%0Ahas%20been%20increasingly%20recognized%20in%20autonomous%20driving%20technology%2C%20especially%0Athe%20BEV-based%20models.%20Current%20state-of-the-art%20solutions%20primarily%20encode%20image%0Afeatures%20from%20each%20camera%20view%20into%20the%20BEV%20space%20through%20explicit%20or%20implicit%0Adepth%20prediction.%20However%2C%20these%20methods%20often%20overlook%20the%20structured%0Acorrelations%20among%20different%20parts%20of%20objects%20in%203D%20space%20and%20the%20fact%20that%0Adifferent%20categories%20of%20objects%20often%20occupy%20distinct%20local%20height%20ranges.%20For%0Aexample%2C%20trucks%20appear%20at%20higher%20elevations%2C%20whereas%20traffic%20cones%20are%20near%20the%0Aground.%20In%20this%20work%2C%20we%20propose%20a%20novel%20approach%20that%20decouples%20feature%0Asampling%20in%20the%20%5Ctextbf%7BBEV%7D%20grid%20queries%20paradigm%20into%20%5Ctextbf%7BH%7Dorizontal%0Afeature%20aggregation%20and%20%5Ctextbf%7BV%7Dertical%20adaptive%20height-aware%20reference%20point%0Asampling%20%28HV-BEV%29%2C%20aiming%20to%20improve%20both%20the%20aggregation%20of%20objects%27%20complete%0Ainformation%20and%20awareness%20of%20diverse%20objects%27%20height%20distribution.%0ASpecifically%2C%20a%20set%20of%20relevant%20neighboring%20points%20is%20dynamically%20constructed%0Afor%20each%203D%20reference%20point%20on%20the%20ground-aligned%20horizontal%20plane%2C%20enhancing%0Athe%20association%20of%20the%20same%20instance%20across%20different%20BEV%20grids%2C%20especially%0Awhen%20the%20instance%20spans%20multiple%20image%20views%20around%20the%20vehicle.%20Additionally%2C%0Ainstead%20of%20relying%20on%20uniform%20sampling%20within%20a%20fixed%20height%20range%2C%20we%0Aintroduce%20a%20height-aware%20module%20that%20incorporates%20historical%20information%2C%0Aenabling%20the%20reference%20points%20to%20adaptively%20focus%20on%20the%20varying%20heights%20at%0Awhich%20objects%20appear%20in%20different%20scenes.%20Extensive%20experiments%20validate%20the%0Aeffectiveness%20of%20our%20proposed%20method%2C%20demonstrating%20its%20superior%20performance%0Aover%20the%20baseline%20across%20the%20nuScenes%20dataset.%20Moreover%2C%20our%20best-performing%0Amodel%20achieves%20a%20remarkable%2050.5%5C%25%20mAP%20and%2059.8%5C%25%20NDS%20on%20the%20nuScenes%20testing%0Aset.%20The%20code%20is%20available%20at%20https%3A//github.com/Uddd821/HV-BEV.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18884v3&entry.124074799=Read"},
{"title": "VP Lab: a PEFT-Enabled Visual Prompting Laboratory for Semantic\n  Segmentation", "author": "Niccolo Avogaro and Thomas Frick and Yagmur G. Cinar and Daniel Caraballo and Cezary Skura and Filip M. Janicki and Piotr Kluska and Brown Ebouky and Nicola Farronato and Florian Scheidegger and Cristiano Malossi and Konrad Schindler and Andrea Bartezzaghi and Roy Assaf and Mattia Rigotti", "abstract": "  Large-scale pretrained vision backbones have transformed computer vision by\nproviding powerful feature extractors that enable various downstream tasks,\nincluding training-free approaches like visual prompting for semantic\nsegmentation. Despite their success in generic scenarios, these models often\nfall short when applied to specialized technical domains where the visual\nfeatures differ significantly from their training distribution. To bridge this\ngap, we introduce VP Lab, a comprehensive iterative framework that enhances\nvisual prompting for robust segmentation model development. At the core of VP\nLab lies E-PEFT, a novel ensemble of parameter-efficient fine-tuning techniques\nspecifically designed to adapt our visual prompting pipeline to specific\ndomains in a manner that is both parameter- and data-efficient. Our approach\nnot only surpasses the state-of-the-art in parameter-efficient fine-tuning for\nthe Segment Anything Model (SAM), but also facilitates an interactive,\nnear-real-time loop, allowing users to observe progressively improving results\nas they experiment within the framework. By integrating E-PEFT with visual\nprompting, we demonstrate a remarkable 50\\% increase in semantic segmentation\nmIoU performance across various technical datasets using only 5 validated\nimages, establishing a new paradigm for fast, efficient, and interactive model\ndeployment in new, challenging domains. This work comes in the form of a\ndemonstration.\n", "link": "http://arxiv.org/abs/2505.15592v1", "date": "2025-05-21", "relevancy": 2.3782, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6031}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6031}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VP%20Lab%3A%20a%20PEFT-Enabled%20Visual%20Prompting%20Laboratory%20for%20Semantic%0A%20%20Segmentation&body=Title%3A%20VP%20Lab%3A%20a%20PEFT-Enabled%20Visual%20Prompting%20Laboratory%20for%20Semantic%0A%20%20Segmentation%0AAuthor%3A%20Niccolo%20Avogaro%20and%20Thomas%20Frick%20and%20Yagmur%20G.%20Cinar%20and%20Daniel%20Caraballo%20and%20Cezary%20Skura%20and%20Filip%20M.%20Janicki%20and%20Piotr%20Kluska%20and%20Brown%20Ebouky%20and%20Nicola%20Farronato%20and%20Florian%20Scheidegger%20and%20Cristiano%20Malossi%20and%20Konrad%20Schindler%20and%20Andrea%20Bartezzaghi%20and%20Roy%20Assaf%20and%20Mattia%20Rigotti%0AAbstract%3A%20%20%20Large-scale%20pretrained%20vision%20backbones%20have%20transformed%20computer%20vision%20by%0Aproviding%20powerful%20feature%20extractors%20that%20enable%20various%20downstream%20tasks%2C%0Aincluding%20training-free%20approaches%20like%20visual%20prompting%20for%20semantic%0Asegmentation.%20Despite%20their%20success%20in%20generic%20scenarios%2C%20these%20models%20often%0Afall%20short%20when%20applied%20to%20specialized%20technical%20domains%20where%20the%20visual%0Afeatures%20differ%20significantly%20from%20their%20training%20distribution.%20To%20bridge%20this%0Agap%2C%20we%20introduce%20VP%20Lab%2C%20a%20comprehensive%20iterative%20framework%20that%20enhances%0Avisual%20prompting%20for%20robust%20segmentation%20model%20development.%20At%20the%20core%20of%20VP%0ALab%20lies%20E-PEFT%2C%20a%20novel%20ensemble%20of%20parameter-efficient%20fine-tuning%20techniques%0Aspecifically%20designed%20to%20adapt%20our%20visual%20prompting%20pipeline%20to%20specific%0Adomains%20in%20a%20manner%20that%20is%20both%20parameter-%20and%20data-efficient.%20Our%20approach%0Anot%20only%20surpasses%20the%20state-of-the-art%20in%20parameter-efficient%20fine-tuning%20for%0Athe%20Segment%20Anything%20Model%20%28SAM%29%2C%20but%20also%20facilitates%20an%20interactive%2C%0Anear-real-time%20loop%2C%20allowing%20users%20to%20observe%20progressively%20improving%20results%0Aas%20they%20experiment%20within%20the%20framework.%20By%20integrating%20E-PEFT%20with%20visual%0Aprompting%2C%20we%20demonstrate%20a%20remarkable%2050%5C%25%20increase%20in%20semantic%20segmentation%0AmIoU%20performance%20across%20various%20technical%20datasets%20using%20only%205%20validated%0Aimages%2C%20establishing%20a%20new%20paradigm%20for%20fast%2C%20efficient%2C%20and%20interactive%20model%0Adeployment%20in%20new%2C%20challenging%20domains.%20This%20work%20comes%20in%20the%20form%20of%20a%0Ademonstration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15592v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVP%2520Lab%253A%2520a%2520PEFT-Enabled%2520Visual%2520Prompting%2520Laboratory%2520for%2520Semantic%250A%2520%2520Segmentation%26entry.906535625%3DNiccolo%2520Avogaro%2520and%2520Thomas%2520Frick%2520and%2520Yagmur%2520G.%2520Cinar%2520and%2520Daniel%2520Caraballo%2520and%2520Cezary%2520Skura%2520and%2520Filip%2520M.%2520Janicki%2520and%2520Piotr%2520Kluska%2520and%2520Brown%2520Ebouky%2520and%2520Nicola%2520Farronato%2520and%2520Florian%2520Scheidegger%2520and%2520Cristiano%2520Malossi%2520and%2520Konrad%2520Schindler%2520and%2520Andrea%2520Bartezzaghi%2520and%2520Roy%2520Assaf%2520and%2520Mattia%2520Rigotti%26entry.1292438233%3D%2520%2520Large-scale%2520pretrained%2520vision%2520backbones%2520have%2520transformed%2520computer%2520vision%2520by%250Aproviding%2520powerful%2520feature%2520extractors%2520that%2520enable%2520various%2520downstream%2520tasks%252C%250Aincluding%2520training-free%2520approaches%2520like%2520visual%2520prompting%2520for%2520semantic%250Asegmentation.%2520Despite%2520their%2520success%2520in%2520generic%2520scenarios%252C%2520these%2520models%2520often%250Afall%2520short%2520when%2520applied%2520to%2520specialized%2520technical%2520domains%2520where%2520the%2520visual%250Afeatures%2520differ%2520significantly%2520from%2520their%2520training%2520distribution.%2520To%2520bridge%2520this%250Agap%252C%2520we%2520introduce%2520VP%2520Lab%252C%2520a%2520comprehensive%2520iterative%2520framework%2520that%2520enhances%250Avisual%2520prompting%2520for%2520robust%2520segmentation%2520model%2520development.%2520At%2520the%2520core%2520of%2520VP%250ALab%2520lies%2520E-PEFT%252C%2520a%2520novel%2520ensemble%2520of%2520parameter-efficient%2520fine-tuning%2520techniques%250Aspecifically%2520designed%2520to%2520adapt%2520our%2520visual%2520prompting%2520pipeline%2520to%2520specific%250Adomains%2520in%2520a%2520manner%2520that%2520is%2520both%2520parameter-%2520and%2520data-efficient.%2520Our%2520approach%250Anot%2520only%2520surpasses%2520the%2520state-of-the-art%2520in%2520parameter-efficient%2520fine-tuning%2520for%250Athe%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%252C%2520but%2520also%2520facilitates%2520an%2520interactive%252C%250Anear-real-time%2520loop%252C%2520allowing%2520users%2520to%2520observe%2520progressively%2520improving%2520results%250Aas%2520they%2520experiment%2520within%2520the%2520framework.%2520By%2520integrating%2520E-PEFT%2520with%2520visual%250Aprompting%252C%2520we%2520demonstrate%2520a%2520remarkable%252050%255C%2525%2520increase%2520in%2520semantic%2520segmentation%250AmIoU%2520performance%2520across%2520various%2520technical%2520datasets%2520using%2520only%25205%2520validated%250Aimages%252C%2520establishing%2520a%2520new%2520paradigm%2520for%2520fast%252C%2520efficient%252C%2520and%2520interactive%2520model%250Adeployment%2520in%2520new%252C%2520challenging%2520domains.%2520This%2520work%2520comes%2520in%2520the%2520form%2520of%2520a%250Ademonstration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15592v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VP%20Lab%3A%20a%20PEFT-Enabled%20Visual%20Prompting%20Laboratory%20for%20Semantic%0A%20%20Segmentation&entry.906535625=Niccolo%20Avogaro%20and%20Thomas%20Frick%20and%20Yagmur%20G.%20Cinar%20and%20Daniel%20Caraballo%20and%20Cezary%20Skura%20and%20Filip%20M.%20Janicki%20and%20Piotr%20Kluska%20and%20Brown%20Ebouky%20and%20Nicola%20Farronato%20and%20Florian%20Scheidegger%20and%20Cristiano%20Malossi%20and%20Konrad%20Schindler%20and%20Andrea%20Bartezzaghi%20and%20Roy%20Assaf%20and%20Mattia%20Rigotti&entry.1292438233=%20%20Large-scale%20pretrained%20vision%20backbones%20have%20transformed%20computer%20vision%20by%0Aproviding%20powerful%20feature%20extractors%20that%20enable%20various%20downstream%20tasks%2C%0Aincluding%20training-free%20approaches%20like%20visual%20prompting%20for%20semantic%0Asegmentation.%20Despite%20their%20success%20in%20generic%20scenarios%2C%20these%20models%20often%0Afall%20short%20when%20applied%20to%20specialized%20technical%20domains%20where%20the%20visual%0Afeatures%20differ%20significantly%20from%20their%20training%20distribution.%20To%20bridge%20this%0Agap%2C%20we%20introduce%20VP%20Lab%2C%20a%20comprehensive%20iterative%20framework%20that%20enhances%0Avisual%20prompting%20for%20robust%20segmentation%20model%20development.%20At%20the%20core%20of%20VP%0ALab%20lies%20E-PEFT%2C%20a%20novel%20ensemble%20of%20parameter-efficient%20fine-tuning%20techniques%0Aspecifically%20designed%20to%20adapt%20our%20visual%20prompting%20pipeline%20to%20specific%0Adomains%20in%20a%20manner%20that%20is%20both%20parameter-%20and%20data-efficient.%20Our%20approach%0Anot%20only%20surpasses%20the%20state-of-the-art%20in%20parameter-efficient%20fine-tuning%20for%0Athe%20Segment%20Anything%20Model%20%28SAM%29%2C%20but%20also%20facilitates%20an%20interactive%2C%0Anear-real-time%20loop%2C%20allowing%20users%20to%20observe%20progressively%20improving%20results%0Aas%20they%20experiment%20within%20the%20framework.%20By%20integrating%20E-PEFT%20with%20visual%0Aprompting%2C%20we%20demonstrate%20a%20remarkable%2050%5C%25%20increase%20in%20semantic%20segmentation%0AmIoU%20performance%20across%20various%20technical%20datasets%20using%20only%205%20validated%0Aimages%2C%20establishing%20a%20new%20paradigm%20for%20fast%2C%20efficient%2C%20and%20interactive%20model%0Adeployment%20in%20new%2C%20challenging%20domains.%20This%20work%20comes%20in%20the%20form%20of%20a%0Ademonstration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15592v1&entry.124074799=Read"},
{"title": "From Grounding to Manipulation: Case Studies of Foundation Model\n  Integration in Embodied Robotic Systems", "author": "Xiuchao Sui and Daiying Tian and Qi Sun and Ruirui Chen and Dongkyu Choi and Kenneth Kwok and Soujanya Poria", "abstract": "  Foundation models (FMs) are increasingly used to bridge language and action\nin embodied agents, yet the operational characteristics of different FM\nintegration strategies remain under-explored -- particularly for complex\ninstruction following and versatile action generation in changing environments.\nThis paper examines three paradigms for building robotic systems: end-to-end\nvision-language-action (VLA) models that implicitly integrate perception and\nplanning, and modular pipelines incorporating either vision-language models\n(VLMs) or multimodal large language models (LLMs). We evaluate these paradigms\nthrough two focused case studies: a complex instruction grounding task\nassessing fine-grained instruction understanding and cross-modal\ndisambiguation, and an object manipulation task targeting skill transfer via\nVLA finetuning. Our experiments in zero-shot and few-shot settings reveal\ntrade-offs in generalization and data efficiency. By exploring performance\nlimits, we distill design implications for developing language-driven physical\nagents and outline emerging challenges and opportunities for FM-powered\nrobotics in real-world conditions.\n", "link": "http://arxiv.org/abs/2505.15685v1", "date": "2025-05-21", "relevancy": 2.3677, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5993}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5904}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Grounding%20to%20Manipulation%3A%20Case%20Studies%20of%20Foundation%20Model%0A%20%20Integration%20in%20Embodied%20Robotic%20Systems&body=Title%3A%20From%20Grounding%20to%20Manipulation%3A%20Case%20Studies%20of%20Foundation%20Model%0A%20%20Integration%20in%20Embodied%20Robotic%20Systems%0AAuthor%3A%20Xiuchao%20Sui%20and%20Daiying%20Tian%20and%20Qi%20Sun%20and%20Ruirui%20Chen%20and%20Dongkyu%20Choi%20and%20Kenneth%20Kwok%20and%20Soujanya%20Poria%0AAbstract%3A%20%20%20Foundation%20models%20%28FMs%29%20are%20increasingly%20used%20to%20bridge%20language%20and%20action%0Ain%20embodied%20agents%2C%20yet%20the%20operational%20characteristics%20of%20different%20FM%0Aintegration%20strategies%20remain%20under-explored%20--%20particularly%20for%20complex%0Ainstruction%20following%20and%20versatile%20action%20generation%20in%20changing%20environments.%0AThis%20paper%20examines%20three%20paradigms%20for%20building%20robotic%20systems%3A%20end-to-end%0Avision-language-action%20%28VLA%29%20models%20that%20implicitly%20integrate%20perception%20and%0Aplanning%2C%20and%20modular%20pipelines%20incorporating%20either%20vision-language%20models%0A%28VLMs%29%20or%20multimodal%20large%20language%20models%20%28LLMs%29.%20We%20evaluate%20these%20paradigms%0Athrough%20two%20focused%20case%20studies%3A%20a%20complex%20instruction%20grounding%20task%0Aassessing%20fine-grained%20instruction%20understanding%20and%20cross-modal%0Adisambiguation%2C%20and%20an%20object%20manipulation%20task%20targeting%20skill%20transfer%20via%0AVLA%20finetuning.%20Our%20experiments%20in%20zero-shot%20and%20few-shot%20settings%20reveal%0Atrade-offs%20in%20generalization%20and%20data%20efficiency.%20By%20exploring%20performance%0Alimits%2C%20we%20distill%20design%20implications%20for%20developing%20language-driven%20physical%0Aagents%20and%20outline%20emerging%20challenges%20and%20opportunities%20for%20FM-powered%0Arobotics%20in%20real-world%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15685v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Grounding%2520to%2520Manipulation%253A%2520Case%2520Studies%2520of%2520Foundation%2520Model%250A%2520%2520Integration%2520in%2520Embodied%2520Robotic%2520Systems%26entry.906535625%3DXiuchao%2520Sui%2520and%2520Daiying%2520Tian%2520and%2520Qi%2520Sun%2520and%2520Ruirui%2520Chen%2520and%2520Dongkyu%2520Choi%2520and%2520Kenneth%2520Kwok%2520and%2520Soujanya%2520Poria%26entry.1292438233%3D%2520%2520Foundation%2520models%2520%2528FMs%2529%2520are%2520increasingly%2520used%2520to%2520bridge%2520language%2520and%2520action%250Ain%2520embodied%2520agents%252C%2520yet%2520the%2520operational%2520characteristics%2520of%2520different%2520FM%250Aintegration%2520strategies%2520remain%2520under-explored%2520--%2520particularly%2520for%2520complex%250Ainstruction%2520following%2520and%2520versatile%2520action%2520generation%2520in%2520changing%2520environments.%250AThis%2520paper%2520examines%2520three%2520paradigms%2520for%2520building%2520robotic%2520systems%253A%2520end-to-end%250Avision-language-action%2520%2528VLA%2529%2520models%2520that%2520implicitly%2520integrate%2520perception%2520and%250Aplanning%252C%2520and%2520modular%2520pipelines%2520incorporating%2520either%2520vision-language%2520models%250A%2528VLMs%2529%2520or%2520multimodal%2520large%2520language%2520models%2520%2528LLMs%2529.%2520We%2520evaluate%2520these%2520paradigms%250Athrough%2520two%2520focused%2520case%2520studies%253A%2520a%2520complex%2520instruction%2520grounding%2520task%250Aassessing%2520fine-grained%2520instruction%2520understanding%2520and%2520cross-modal%250Adisambiguation%252C%2520and%2520an%2520object%2520manipulation%2520task%2520targeting%2520skill%2520transfer%2520via%250AVLA%2520finetuning.%2520Our%2520experiments%2520in%2520zero-shot%2520and%2520few-shot%2520settings%2520reveal%250Atrade-offs%2520in%2520generalization%2520and%2520data%2520efficiency.%2520By%2520exploring%2520performance%250Alimits%252C%2520we%2520distill%2520design%2520implications%2520for%2520developing%2520language-driven%2520physical%250Aagents%2520and%2520outline%2520emerging%2520challenges%2520and%2520opportunities%2520for%2520FM-powered%250Arobotics%2520in%2520real-world%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15685v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Grounding%20to%20Manipulation%3A%20Case%20Studies%20of%20Foundation%20Model%0A%20%20Integration%20in%20Embodied%20Robotic%20Systems&entry.906535625=Xiuchao%20Sui%20and%20Daiying%20Tian%20and%20Qi%20Sun%20and%20Ruirui%20Chen%20and%20Dongkyu%20Choi%20and%20Kenneth%20Kwok%20and%20Soujanya%20Poria&entry.1292438233=%20%20Foundation%20models%20%28FMs%29%20are%20increasingly%20used%20to%20bridge%20language%20and%20action%0Ain%20embodied%20agents%2C%20yet%20the%20operational%20characteristics%20of%20different%20FM%0Aintegration%20strategies%20remain%20under-explored%20--%20particularly%20for%20complex%0Ainstruction%20following%20and%20versatile%20action%20generation%20in%20changing%20environments.%0AThis%20paper%20examines%20three%20paradigms%20for%20building%20robotic%20systems%3A%20end-to-end%0Avision-language-action%20%28VLA%29%20models%20that%20implicitly%20integrate%20perception%20and%0Aplanning%2C%20and%20modular%20pipelines%20incorporating%20either%20vision-language%20models%0A%28VLMs%29%20or%20multimodal%20large%20language%20models%20%28LLMs%29.%20We%20evaluate%20these%20paradigms%0Athrough%20two%20focused%20case%20studies%3A%20a%20complex%20instruction%20grounding%20task%0Aassessing%20fine-grained%20instruction%20understanding%20and%20cross-modal%0Adisambiguation%2C%20and%20an%20object%20manipulation%20task%20targeting%20skill%20transfer%20via%0AVLA%20finetuning.%20Our%20experiments%20in%20zero-shot%20and%20few-shot%20settings%20reveal%0Atrade-offs%20in%20generalization%20and%20data%20efficiency.%20By%20exploring%20performance%0Alimits%2C%20we%20distill%20design%20implications%20for%20developing%20language-driven%20physical%0Aagents%20and%20outline%20emerging%20challenges%20and%20opportunities%20for%20FM-powered%0Arobotics%20in%20real-world%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15685v1&entry.124074799=Read"},
{"title": "Clapper: Compact Learning and Video Representation in VLMs", "author": "Lingyu Kong and Hongzhi Zhang and Jingyuan Zhang and Jianzhao Huang and Kunze Li and Qi Wang and Fuzheng Zhang", "abstract": "  Current vision-language models (VLMs) have demonstrated remarkable\ncapabilities across diverse video understanding applications. Designing VLMs\nfor video inputs requires effectively modeling the temporal dimension (i.e.\ncapturing dependencies across frames) and balancing the processing of short and\nlong videos. Specifically, short videos demand preservation of fine-grained\ndetails, whereas long videos require strategic compression of visual\ninformation to handle extensive temporal contexts efficiently. However, our\nempirical analysis reveals a critical limitation: most existing VLMs suffer\nsevere performance degradation in long video understanding tasks when\ncompressing visual tokens below a quarter of their original visual tokens. To\nenable more effective modeling of both short and long video inputs, we propose\nClapper, a method that utilizes a slow-fast strategy for video representation\nand introduces a novel module named TimePerceiver for efficient\ntemporal-spatial encoding within existing VLM backbones. By using our method,\nwe achieves 13x compression of visual tokens per frame (averaging 61\ntokens/frame) without compromising QA accuracy. In our experiments, Clapper\nachieves 62.0% on VideoMME, 69.8% on MLVU, and 67.4% on TempCompass, all with\nfewer than 6,000 visual tokens per video. The code will be publicly available\non the homepage.\n", "link": "http://arxiv.org/abs/2505.15529v1", "date": "2025-05-21", "relevancy": 2.3637, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6243}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5679}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clapper%3A%20Compact%20Learning%20and%20Video%20Representation%20in%20VLMs&body=Title%3A%20Clapper%3A%20Compact%20Learning%20and%20Video%20Representation%20in%20VLMs%0AAuthor%3A%20Lingyu%20Kong%20and%20Hongzhi%20Zhang%20and%20Jingyuan%20Zhang%20and%20Jianzhao%20Huang%20and%20Kunze%20Li%20and%20Qi%20Wang%20and%20Fuzheng%20Zhang%0AAbstract%3A%20%20%20Current%20vision-language%20models%20%28VLMs%29%20have%20demonstrated%20remarkable%0Acapabilities%20across%20diverse%20video%20understanding%20applications.%20Designing%20VLMs%0Afor%20video%20inputs%20requires%20effectively%20modeling%20the%20temporal%20dimension%20%28i.e.%0Acapturing%20dependencies%20across%20frames%29%20and%20balancing%20the%20processing%20of%20short%20and%0Along%20videos.%20Specifically%2C%20short%20videos%20demand%20preservation%20of%20fine-grained%0Adetails%2C%20whereas%20long%20videos%20require%20strategic%20compression%20of%20visual%0Ainformation%20to%20handle%20extensive%20temporal%20contexts%20efficiently.%20However%2C%20our%0Aempirical%20analysis%20reveals%20a%20critical%20limitation%3A%20most%20existing%20VLMs%20suffer%0Asevere%20performance%20degradation%20in%20long%20video%20understanding%20tasks%20when%0Acompressing%20visual%20tokens%20below%20a%20quarter%20of%20their%20original%20visual%20tokens.%20To%0Aenable%20more%20effective%20modeling%20of%20both%20short%20and%20long%20video%20inputs%2C%20we%20propose%0AClapper%2C%20a%20method%20that%20utilizes%20a%20slow-fast%20strategy%20for%20video%20representation%0Aand%20introduces%20a%20novel%20module%20named%20TimePerceiver%20for%20efficient%0Atemporal-spatial%20encoding%20within%20existing%20VLM%20backbones.%20By%20using%20our%20method%2C%0Awe%20achieves%2013x%20compression%20of%20visual%20tokens%20per%20frame%20%28averaging%2061%0Atokens/frame%29%20without%20compromising%20QA%20accuracy.%20In%20our%20experiments%2C%20Clapper%0Aachieves%2062.0%25%20on%20VideoMME%2C%2069.8%25%20on%20MLVU%2C%20and%2067.4%25%20on%20TempCompass%2C%20all%20with%0Afewer%20than%206%2C000%20visual%20tokens%20per%20video.%20The%20code%20will%20be%20publicly%20available%0Aon%20the%20homepage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15529v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClapper%253A%2520Compact%2520Learning%2520and%2520Video%2520Representation%2520in%2520VLMs%26entry.906535625%3DLingyu%2520Kong%2520and%2520Hongzhi%2520Zhang%2520and%2520Jingyuan%2520Zhang%2520and%2520Jianzhao%2520Huang%2520and%2520Kunze%2520Li%2520and%2520Qi%2520Wang%2520and%2520Fuzheng%2520Zhang%26entry.1292438233%3D%2520%2520Current%2520vision-language%2520models%2520%2528VLMs%2529%2520have%2520demonstrated%2520remarkable%250Acapabilities%2520across%2520diverse%2520video%2520understanding%2520applications.%2520Designing%2520VLMs%250Afor%2520video%2520inputs%2520requires%2520effectively%2520modeling%2520the%2520temporal%2520dimension%2520%2528i.e.%250Acapturing%2520dependencies%2520across%2520frames%2529%2520and%2520balancing%2520the%2520processing%2520of%2520short%2520and%250Along%2520videos.%2520Specifically%252C%2520short%2520videos%2520demand%2520preservation%2520of%2520fine-grained%250Adetails%252C%2520whereas%2520long%2520videos%2520require%2520strategic%2520compression%2520of%2520visual%250Ainformation%2520to%2520handle%2520extensive%2520temporal%2520contexts%2520efficiently.%2520However%252C%2520our%250Aempirical%2520analysis%2520reveals%2520a%2520critical%2520limitation%253A%2520most%2520existing%2520VLMs%2520suffer%250Asevere%2520performance%2520degradation%2520in%2520long%2520video%2520understanding%2520tasks%2520when%250Acompressing%2520visual%2520tokens%2520below%2520a%2520quarter%2520of%2520their%2520original%2520visual%2520tokens.%2520To%250Aenable%2520more%2520effective%2520modeling%2520of%2520both%2520short%2520and%2520long%2520video%2520inputs%252C%2520we%2520propose%250AClapper%252C%2520a%2520method%2520that%2520utilizes%2520a%2520slow-fast%2520strategy%2520for%2520video%2520representation%250Aand%2520introduces%2520a%2520novel%2520module%2520named%2520TimePerceiver%2520for%2520efficient%250Atemporal-spatial%2520encoding%2520within%2520existing%2520VLM%2520backbones.%2520By%2520using%2520our%2520method%252C%250Awe%2520achieves%252013x%2520compression%2520of%2520visual%2520tokens%2520per%2520frame%2520%2528averaging%252061%250Atokens/frame%2529%2520without%2520compromising%2520QA%2520accuracy.%2520In%2520our%2520experiments%252C%2520Clapper%250Aachieves%252062.0%2525%2520on%2520VideoMME%252C%252069.8%2525%2520on%2520MLVU%252C%2520and%252067.4%2525%2520on%2520TempCompass%252C%2520all%2520with%250Afewer%2520than%25206%252C000%2520visual%2520tokens%2520per%2520video.%2520The%2520code%2520will%2520be%2520publicly%2520available%250Aon%2520the%2520homepage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15529v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clapper%3A%20Compact%20Learning%20and%20Video%20Representation%20in%20VLMs&entry.906535625=Lingyu%20Kong%20and%20Hongzhi%20Zhang%20and%20Jingyuan%20Zhang%20and%20Jianzhao%20Huang%20and%20Kunze%20Li%20and%20Qi%20Wang%20and%20Fuzheng%20Zhang&entry.1292438233=%20%20Current%20vision-language%20models%20%28VLMs%29%20have%20demonstrated%20remarkable%0Acapabilities%20across%20diverse%20video%20understanding%20applications.%20Designing%20VLMs%0Afor%20video%20inputs%20requires%20effectively%20modeling%20the%20temporal%20dimension%20%28i.e.%0Acapturing%20dependencies%20across%20frames%29%20and%20balancing%20the%20processing%20of%20short%20and%0Along%20videos.%20Specifically%2C%20short%20videos%20demand%20preservation%20of%20fine-grained%0Adetails%2C%20whereas%20long%20videos%20require%20strategic%20compression%20of%20visual%0Ainformation%20to%20handle%20extensive%20temporal%20contexts%20efficiently.%20However%2C%20our%0Aempirical%20analysis%20reveals%20a%20critical%20limitation%3A%20most%20existing%20VLMs%20suffer%0Asevere%20performance%20degradation%20in%20long%20video%20understanding%20tasks%20when%0Acompressing%20visual%20tokens%20below%20a%20quarter%20of%20their%20original%20visual%20tokens.%20To%0Aenable%20more%20effective%20modeling%20of%20both%20short%20and%20long%20video%20inputs%2C%20we%20propose%0AClapper%2C%20a%20method%20that%20utilizes%20a%20slow-fast%20strategy%20for%20video%20representation%0Aand%20introduces%20a%20novel%20module%20named%20TimePerceiver%20for%20efficient%0Atemporal-spatial%20encoding%20within%20existing%20VLM%20backbones.%20By%20using%20our%20method%2C%0Awe%20achieves%2013x%20compression%20of%20visual%20tokens%20per%20frame%20%28averaging%2061%0Atokens/frame%29%20without%20compromising%20QA%20accuracy.%20In%20our%20experiments%2C%20Clapper%0Aachieves%2062.0%25%20on%20VideoMME%2C%2069.8%25%20on%20MLVU%2C%20and%2067.4%25%20on%20TempCompass%2C%20all%20with%0Afewer%20than%206%2C000%20visual%20tokens%20per%20video.%20The%20code%20will%20be%20publicly%20available%0Aon%20the%20homepage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15529v1&entry.124074799=Read"},
{"title": "Spatially scalable recursive estimation of Gaussian process terrain maps\n  using local basis functions", "author": "Frida Marie Viset and Rudy Helmons and Manon Kok", "abstract": "  When an agent, person, vehicle or robot is moving through an unknown\nenvironment without GNSS signals, online mapping of nonlinear terrains can be\nused to improve position estimates when the agent returns to a previously\nmapped area. Mapping algorithms using online Gaussian process (GP) regression\nare commonly integrated in algorithms for simultaneous localisation and mapping\n(SLAM). However, GP mapping algorithms have increasing computational demands as\nthe mapped area expands relative to spatial field variations. This is due to\nthe need for estimating an increasing amount of map parameters as the area of\nthe map grows. Contrary to this, we propose a recursive GP mapping estimation\nalgorithm which uses local basis functions in an information filter to achieve\nspatial scalability. Our proposed approximation employs a global grid of finite\nsupport basis functions but restricts computations to a localized subset around\neach prediction point. As our proposed algorithm is recursive, it can naturally\nbe incorporated into existing algorithms that uses Gaussian process maps for\nSLAM. Incorporating our proposed algorithm into an extended Kalman filter (EKF)\nfor magnetic field SLAM reduces the overall computational complexity of the\nalgorithm. We show experimentally that our algorithm is faster than existing\nmethods when the mapped area is large and the map is based on many\nmeasurements, both for recursive mapping tasks and for magnetic field SLAM.\n", "link": "http://arxiv.org/abs/2210.09168v3", "date": "2025-05-21", "relevancy": 2.3479, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6067}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5869}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatially%20scalable%20recursive%20estimation%20of%20Gaussian%20process%20terrain%20maps%0A%20%20using%20local%20basis%20functions&body=Title%3A%20Spatially%20scalable%20recursive%20estimation%20of%20Gaussian%20process%20terrain%20maps%0A%20%20using%20local%20basis%20functions%0AAuthor%3A%20Frida%20Marie%20Viset%20and%20Rudy%20Helmons%20and%20Manon%20Kok%0AAbstract%3A%20%20%20When%20an%20agent%2C%20person%2C%20vehicle%20or%20robot%20is%20moving%20through%20an%20unknown%0Aenvironment%20without%20GNSS%20signals%2C%20online%20mapping%20of%20nonlinear%20terrains%20can%20be%0Aused%20to%20improve%20position%20estimates%20when%20the%20agent%20returns%20to%20a%20previously%0Amapped%20area.%20Mapping%20algorithms%20using%20online%20Gaussian%20process%20%28GP%29%20regression%0Aare%20commonly%20integrated%20in%20algorithms%20for%20simultaneous%20localisation%20and%20mapping%0A%28SLAM%29.%20However%2C%20GP%20mapping%20algorithms%20have%20increasing%20computational%20demands%20as%0Athe%20mapped%20area%20expands%20relative%20to%20spatial%20field%20variations.%20This%20is%20due%20to%0Athe%20need%20for%20estimating%20an%20increasing%20amount%20of%20map%20parameters%20as%20the%20area%20of%0Athe%20map%20grows.%20Contrary%20to%20this%2C%20we%20propose%20a%20recursive%20GP%20mapping%20estimation%0Aalgorithm%20which%20uses%20local%20basis%20functions%20in%20an%20information%20filter%20to%20achieve%0Aspatial%20scalability.%20Our%20proposed%20approximation%20employs%20a%20global%20grid%20of%20finite%0Asupport%20basis%20functions%20but%20restricts%20computations%20to%20a%20localized%20subset%20around%0Aeach%20prediction%20point.%20As%20our%20proposed%20algorithm%20is%20recursive%2C%20it%20can%20naturally%0Abe%20incorporated%20into%20existing%20algorithms%20that%20uses%20Gaussian%20process%20maps%20for%0ASLAM.%20Incorporating%20our%20proposed%20algorithm%20into%20an%20extended%20Kalman%20filter%20%28EKF%29%0Afor%20magnetic%20field%20SLAM%20reduces%20the%20overall%20computational%20complexity%20of%20the%0Aalgorithm.%20We%20show%20experimentally%20that%20our%20algorithm%20is%20faster%20than%20existing%0Amethods%20when%20the%20mapped%20area%20is%20large%20and%20the%20map%20is%20based%20on%20many%0Ameasurements%2C%20both%20for%20recursive%20mapping%20tasks%20and%20for%20magnetic%20field%20SLAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.09168v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatially%2520scalable%2520recursive%2520estimation%2520of%2520Gaussian%2520process%2520terrain%2520maps%250A%2520%2520using%2520local%2520basis%2520functions%26entry.906535625%3DFrida%2520Marie%2520Viset%2520and%2520Rudy%2520Helmons%2520and%2520Manon%2520Kok%26entry.1292438233%3D%2520%2520When%2520an%2520agent%252C%2520person%252C%2520vehicle%2520or%2520robot%2520is%2520moving%2520through%2520an%2520unknown%250Aenvironment%2520without%2520GNSS%2520signals%252C%2520online%2520mapping%2520of%2520nonlinear%2520terrains%2520can%2520be%250Aused%2520to%2520improve%2520position%2520estimates%2520when%2520the%2520agent%2520returns%2520to%2520a%2520previously%250Amapped%2520area.%2520Mapping%2520algorithms%2520using%2520online%2520Gaussian%2520process%2520%2528GP%2529%2520regression%250Aare%2520commonly%2520integrated%2520in%2520algorithms%2520for%2520simultaneous%2520localisation%2520and%2520mapping%250A%2528SLAM%2529.%2520However%252C%2520GP%2520mapping%2520algorithms%2520have%2520increasing%2520computational%2520demands%2520as%250Athe%2520mapped%2520area%2520expands%2520relative%2520to%2520spatial%2520field%2520variations.%2520This%2520is%2520due%2520to%250Athe%2520need%2520for%2520estimating%2520an%2520increasing%2520amount%2520of%2520map%2520parameters%2520as%2520the%2520area%2520of%250Athe%2520map%2520grows.%2520Contrary%2520to%2520this%252C%2520we%2520propose%2520a%2520recursive%2520GP%2520mapping%2520estimation%250Aalgorithm%2520which%2520uses%2520local%2520basis%2520functions%2520in%2520an%2520information%2520filter%2520to%2520achieve%250Aspatial%2520scalability.%2520Our%2520proposed%2520approximation%2520employs%2520a%2520global%2520grid%2520of%2520finite%250Asupport%2520basis%2520functions%2520but%2520restricts%2520computations%2520to%2520a%2520localized%2520subset%2520around%250Aeach%2520prediction%2520point.%2520As%2520our%2520proposed%2520algorithm%2520is%2520recursive%252C%2520it%2520can%2520naturally%250Abe%2520incorporated%2520into%2520existing%2520algorithms%2520that%2520uses%2520Gaussian%2520process%2520maps%2520for%250ASLAM.%2520Incorporating%2520our%2520proposed%2520algorithm%2520into%2520an%2520extended%2520Kalman%2520filter%2520%2528EKF%2529%250Afor%2520magnetic%2520field%2520SLAM%2520reduces%2520the%2520overall%2520computational%2520complexity%2520of%2520the%250Aalgorithm.%2520We%2520show%2520experimentally%2520that%2520our%2520algorithm%2520is%2520faster%2520than%2520existing%250Amethods%2520when%2520the%2520mapped%2520area%2520is%2520large%2520and%2520the%2520map%2520is%2520based%2520on%2520many%250Ameasurements%252C%2520both%2520for%2520recursive%2520mapping%2520tasks%2520and%2520for%2520magnetic%2520field%2520SLAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.09168v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatially%20scalable%20recursive%20estimation%20of%20Gaussian%20process%20terrain%20maps%0A%20%20using%20local%20basis%20functions&entry.906535625=Frida%20Marie%20Viset%20and%20Rudy%20Helmons%20and%20Manon%20Kok&entry.1292438233=%20%20When%20an%20agent%2C%20person%2C%20vehicle%20or%20robot%20is%20moving%20through%20an%20unknown%0Aenvironment%20without%20GNSS%20signals%2C%20online%20mapping%20of%20nonlinear%20terrains%20can%20be%0Aused%20to%20improve%20position%20estimates%20when%20the%20agent%20returns%20to%20a%20previously%0Amapped%20area.%20Mapping%20algorithms%20using%20online%20Gaussian%20process%20%28GP%29%20regression%0Aare%20commonly%20integrated%20in%20algorithms%20for%20simultaneous%20localisation%20and%20mapping%0A%28SLAM%29.%20However%2C%20GP%20mapping%20algorithms%20have%20increasing%20computational%20demands%20as%0Athe%20mapped%20area%20expands%20relative%20to%20spatial%20field%20variations.%20This%20is%20due%20to%0Athe%20need%20for%20estimating%20an%20increasing%20amount%20of%20map%20parameters%20as%20the%20area%20of%0Athe%20map%20grows.%20Contrary%20to%20this%2C%20we%20propose%20a%20recursive%20GP%20mapping%20estimation%0Aalgorithm%20which%20uses%20local%20basis%20functions%20in%20an%20information%20filter%20to%20achieve%0Aspatial%20scalability.%20Our%20proposed%20approximation%20employs%20a%20global%20grid%20of%20finite%0Asupport%20basis%20functions%20but%20restricts%20computations%20to%20a%20localized%20subset%20around%0Aeach%20prediction%20point.%20As%20our%20proposed%20algorithm%20is%20recursive%2C%20it%20can%20naturally%0Abe%20incorporated%20into%20existing%20algorithms%20that%20uses%20Gaussian%20process%20maps%20for%0ASLAM.%20Incorporating%20our%20proposed%20algorithm%20into%20an%20extended%20Kalman%20filter%20%28EKF%29%0Afor%20magnetic%20field%20SLAM%20reduces%20the%20overall%20computational%20complexity%20of%20the%0Aalgorithm.%20We%20show%20experimentally%20that%20our%20algorithm%20is%20faster%20than%20existing%0Amethods%20when%20the%20mapped%20area%20is%20large%20and%20the%20map%20is%20based%20on%20many%0Ameasurements%2C%20both%20for%20recursive%20mapping%20tasks%20and%20for%20magnetic%20field%20SLAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.09168v3&entry.124074799=Read"},
{"title": "Breast Cancer Classification Using Gradient Boosting Algorithms Focusing\n  on Reducing the False Negative and SHAP for Explainability", "author": "Jo\u00e3o Manoel Herrera Pinheiro and Marcelo Becker", "abstract": "  Cancer is one of the diseases that kill the most women in the world, with\nbreast cancer being responsible for the highest number of cancer cases and\nconsequently deaths. However, it can be prevented by early detection and,\nconsequently, early treatment. Any development for detection or perdition this\nkind of cancer is important for a better healthy life. Many studies focus on a\nmodel with high accuracy in cancer prediction, but sometimes accuracy alone may\nnot always be a reliable metric. This study implies an investigative approach\nto studying the performance of different machine learning algorithms based on\nboosting to predict breast cancer focusing on the recall metric. Boosting\nmachine learning algorithms has been proven to be an effective tool for\ndetecting medical diseases. The dataset of the University of California, Irvine\n(UCI) repository has been utilized to train and test the model classifier that\ncontains their attributes. The main objective of this study is to use\nstate-of-the-art boosting algorithms such as AdaBoost, XGBoost, CatBoost and\nLightGBM to predict and diagnose breast cancer and to find the most effective\nmetric regarding recall, ROC-AUC, and confusion matrix. Furthermore, our study\nis the first to use these four boosting algorithms with Optuna, a library for\nhyperparameter optimization, and the SHAP method to improve the\ninterpretability of our model, which can be used as a support to identify and\npredict breast cancer. We were able to improve AUC or recall for all the models\nand reduce the False Negative for AdaBoost and LigthGBM the final AUC were more\nthan 99.41\\% for all models.\n", "link": "http://arxiv.org/abs/2403.09548v3", "date": "2025-05-21", "relevancy": 2.3249, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5152}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4444}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breast%20Cancer%20Classification%20Using%20Gradient%20Boosting%20Algorithms%20Focusing%0A%20%20on%20Reducing%20the%20False%20Negative%20and%20SHAP%20for%20Explainability&body=Title%3A%20Breast%20Cancer%20Classification%20Using%20Gradient%20Boosting%20Algorithms%20Focusing%0A%20%20on%20Reducing%20the%20False%20Negative%20and%20SHAP%20for%20Explainability%0AAuthor%3A%20Jo%C3%A3o%20Manoel%20Herrera%20Pinheiro%20and%20Marcelo%20Becker%0AAbstract%3A%20%20%20Cancer%20is%20one%20of%20the%20diseases%20that%20kill%20the%20most%20women%20in%20the%20world%2C%20with%0Abreast%20cancer%20being%20responsible%20for%20the%20highest%20number%20of%20cancer%20cases%20and%0Aconsequently%20deaths.%20However%2C%20it%20can%20be%20prevented%20by%20early%20detection%20and%2C%0Aconsequently%2C%20early%20treatment.%20Any%20development%20for%20detection%20or%20perdition%20this%0Akind%20of%20cancer%20is%20important%20for%20a%20better%20healthy%20life.%20Many%20studies%20focus%20on%20a%0Amodel%20with%20high%20accuracy%20in%20cancer%20prediction%2C%20but%20sometimes%20accuracy%20alone%20may%0Anot%20always%20be%20a%20reliable%20metric.%20This%20study%20implies%20an%20investigative%20approach%0Ato%20studying%20the%20performance%20of%20different%20machine%20learning%20algorithms%20based%20on%0Aboosting%20to%20predict%20breast%20cancer%20focusing%20on%20the%20recall%20metric.%20Boosting%0Amachine%20learning%20algorithms%20has%20been%20proven%20to%20be%20an%20effective%20tool%20for%0Adetecting%20medical%20diseases.%20The%20dataset%20of%20the%20University%20of%20California%2C%20Irvine%0A%28UCI%29%20repository%20has%20been%20utilized%20to%20train%20and%20test%20the%20model%20classifier%20that%0Acontains%20their%20attributes.%20The%20main%20objective%20of%20this%20study%20is%20to%20use%0Astate-of-the-art%20boosting%20algorithms%20such%20as%20AdaBoost%2C%20XGBoost%2C%20CatBoost%20and%0ALightGBM%20to%20predict%20and%20diagnose%20breast%20cancer%20and%20to%20find%20the%20most%20effective%0Ametric%20regarding%20recall%2C%20ROC-AUC%2C%20and%20confusion%20matrix.%20Furthermore%2C%20our%20study%0Ais%20the%20first%20to%20use%20these%20four%20boosting%20algorithms%20with%20Optuna%2C%20a%20library%20for%0Ahyperparameter%20optimization%2C%20and%20the%20SHAP%20method%20to%20improve%20the%0Ainterpretability%20of%20our%20model%2C%20which%20can%20be%20used%20as%20a%20support%20to%20identify%20and%0Apredict%20breast%20cancer.%20We%20were%20able%20to%20improve%20AUC%20or%20recall%20for%20all%20the%20models%0Aand%20reduce%20the%20False%20Negative%20for%20AdaBoost%20and%20LigthGBM%20the%20final%20AUC%20were%20more%0Athan%2099.41%5C%25%20for%20all%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09548v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreast%2520Cancer%2520Classification%2520Using%2520Gradient%2520Boosting%2520Algorithms%2520Focusing%250A%2520%2520on%2520Reducing%2520the%2520False%2520Negative%2520and%2520SHAP%2520for%2520Explainability%26entry.906535625%3DJo%25C3%25A3o%2520Manoel%2520Herrera%2520Pinheiro%2520and%2520Marcelo%2520Becker%26entry.1292438233%3D%2520%2520Cancer%2520is%2520one%2520of%2520the%2520diseases%2520that%2520kill%2520the%2520most%2520women%2520in%2520the%2520world%252C%2520with%250Abreast%2520cancer%2520being%2520responsible%2520for%2520the%2520highest%2520number%2520of%2520cancer%2520cases%2520and%250Aconsequently%2520deaths.%2520However%252C%2520it%2520can%2520be%2520prevented%2520by%2520early%2520detection%2520and%252C%250Aconsequently%252C%2520early%2520treatment.%2520Any%2520development%2520for%2520detection%2520or%2520perdition%2520this%250Akind%2520of%2520cancer%2520is%2520important%2520for%2520a%2520better%2520healthy%2520life.%2520Many%2520studies%2520focus%2520on%2520a%250Amodel%2520with%2520high%2520accuracy%2520in%2520cancer%2520prediction%252C%2520but%2520sometimes%2520accuracy%2520alone%2520may%250Anot%2520always%2520be%2520a%2520reliable%2520metric.%2520This%2520study%2520implies%2520an%2520investigative%2520approach%250Ato%2520studying%2520the%2520performance%2520of%2520different%2520machine%2520learning%2520algorithms%2520based%2520on%250Aboosting%2520to%2520predict%2520breast%2520cancer%2520focusing%2520on%2520the%2520recall%2520metric.%2520Boosting%250Amachine%2520learning%2520algorithms%2520has%2520been%2520proven%2520to%2520be%2520an%2520effective%2520tool%2520for%250Adetecting%2520medical%2520diseases.%2520The%2520dataset%2520of%2520the%2520University%2520of%2520California%252C%2520Irvine%250A%2528UCI%2529%2520repository%2520has%2520been%2520utilized%2520to%2520train%2520and%2520test%2520the%2520model%2520classifier%2520that%250Acontains%2520their%2520attributes.%2520The%2520main%2520objective%2520of%2520this%2520study%2520is%2520to%2520use%250Astate-of-the-art%2520boosting%2520algorithms%2520such%2520as%2520AdaBoost%252C%2520XGBoost%252C%2520CatBoost%2520and%250ALightGBM%2520to%2520predict%2520and%2520diagnose%2520breast%2520cancer%2520and%2520to%2520find%2520the%2520most%2520effective%250Ametric%2520regarding%2520recall%252C%2520ROC-AUC%252C%2520and%2520confusion%2520matrix.%2520Furthermore%252C%2520our%2520study%250Ais%2520the%2520first%2520to%2520use%2520these%2520four%2520boosting%2520algorithms%2520with%2520Optuna%252C%2520a%2520library%2520for%250Ahyperparameter%2520optimization%252C%2520and%2520the%2520SHAP%2520method%2520to%2520improve%2520the%250Ainterpretability%2520of%2520our%2520model%252C%2520which%2520can%2520be%2520used%2520as%2520a%2520support%2520to%2520identify%2520and%250Apredict%2520breast%2520cancer.%2520We%2520were%2520able%2520to%2520improve%2520AUC%2520or%2520recall%2520for%2520all%2520the%2520models%250Aand%2520reduce%2520the%2520False%2520Negative%2520for%2520AdaBoost%2520and%2520LigthGBM%2520the%2520final%2520AUC%2520were%2520more%250Athan%252099.41%255C%2525%2520for%2520all%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09548v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breast%20Cancer%20Classification%20Using%20Gradient%20Boosting%20Algorithms%20Focusing%0A%20%20on%20Reducing%20the%20False%20Negative%20and%20SHAP%20for%20Explainability&entry.906535625=Jo%C3%A3o%20Manoel%20Herrera%20Pinheiro%20and%20Marcelo%20Becker&entry.1292438233=%20%20Cancer%20is%20one%20of%20the%20diseases%20that%20kill%20the%20most%20women%20in%20the%20world%2C%20with%0Abreast%20cancer%20being%20responsible%20for%20the%20highest%20number%20of%20cancer%20cases%20and%0Aconsequently%20deaths.%20However%2C%20it%20can%20be%20prevented%20by%20early%20detection%20and%2C%0Aconsequently%2C%20early%20treatment.%20Any%20development%20for%20detection%20or%20perdition%20this%0Akind%20of%20cancer%20is%20important%20for%20a%20better%20healthy%20life.%20Many%20studies%20focus%20on%20a%0Amodel%20with%20high%20accuracy%20in%20cancer%20prediction%2C%20but%20sometimes%20accuracy%20alone%20may%0Anot%20always%20be%20a%20reliable%20metric.%20This%20study%20implies%20an%20investigative%20approach%0Ato%20studying%20the%20performance%20of%20different%20machine%20learning%20algorithms%20based%20on%0Aboosting%20to%20predict%20breast%20cancer%20focusing%20on%20the%20recall%20metric.%20Boosting%0Amachine%20learning%20algorithms%20has%20been%20proven%20to%20be%20an%20effective%20tool%20for%0Adetecting%20medical%20diseases.%20The%20dataset%20of%20the%20University%20of%20California%2C%20Irvine%0A%28UCI%29%20repository%20has%20been%20utilized%20to%20train%20and%20test%20the%20model%20classifier%20that%0Acontains%20their%20attributes.%20The%20main%20objective%20of%20this%20study%20is%20to%20use%0Astate-of-the-art%20boosting%20algorithms%20such%20as%20AdaBoost%2C%20XGBoost%2C%20CatBoost%20and%0ALightGBM%20to%20predict%20and%20diagnose%20breast%20cancer%20and%20to%20find%20the%20most%20effective%0Ametric%20regarding%20recall%2C%20ROC-AUC%2C%20and%20confusion%20matrix.%20Furthermore%2C%20our%20study%0Ais%20the%20first%20to%20use%20these%20four%20boosting%20algorithms%20with%20Optuna%2C%20a%20library%20for%0Ahyperparameter%20optimization%2C%20and%20the%20SHAP%20method%20to%20improve%20the%0Ainterpretability%20of%20our%20model%2C%20which%20can%20be%20used%20as%20a%20support%20to%20identify%20and%0Apredict%20breast%20cancer.%20We%20were%20able%20to%20improve%20AUC%20or%20recall%20for%20all%20the%20models%0Aand%20reduce%20the%20False%20Negative%20for%20AdaBoost%20and%20LigthGBM%20the%20final%20AUC%20were%20more%0Athan%2099.41%5C%25%20for%20all%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09548v3&entry.124074799=Read"},
{"title": "Scaling Text-Rich Image Understanding via Code-Guided Synthetic\n  Multimodal Data Generation", "author": "Yue Yang and Ajay Patel and Matt Deitke and Tanmay Gupta and Luca Weihs and Andrew Head and Mark Yatskar and Chris Callison-Burch and Ranjay Krishna and Aniruddha Kembhavi and Christopher Clark", "abstract": "  Reasoning about images with rich text, such as charts and documents, is a\ncritical application of vision-language models (VLMs). However, VLMs often\nstruggle in these domains due to the scarcity of diverse text-rich\nvision-language data. To address this challenge, we present CoSyn, a framework\nthat leverages the coding capabilities of text-only large language models\n(LLMs) to automatically create synthetic text-rich multimodal data. Given input\ntext describing a target domain (e.g., \"nutrition fact labels\"), CoSyn prompts\nan LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic\nimages. With the underlying code as textual representations of the synthetic\nimages, CoSyn can generate high-quality instruction-tuning data, again relying\non a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K\nimages and 2.7M rows of vision-language instruction-tuning data. Comprehensive\nexperiments on seven benchmarks demonstrate that models trained on our\nsynthetic data achieve state-of-the-art performance among competitive\nopen-source models, including Llama 3.2, and surpass proprietary models such as\nGPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing\ndata, enabling VLMs to ground information within input images, showcasing its\npotential for developing multimodal agents capable of acting in real-world\nenvironments.\n", "link": "http://arxiv.org/abs/2502.14846v2", "date": "2025-05-21", "relevancy": 2.2924, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6083}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5697}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Text-Rich%20Image%20Understanding%20via%20Code-Guided%20Synthetic%0A%20%20Multimodal%20Data%20Generation&body=Title%3A%20Scaling%20Text-Rich%20Image%20Understanding%20via%20Code-Guided%20Synthetic%0A%20%20Multimodal%20Data%20Generation%0AAuthor%3A%20Yue%20Yang%20and%20Ajay%20Patel%20and%20Matt%20Deitke%20and%20Tanmay%20Gupta%20and%20Luca%20Weihs%20and%20Andrew%20Head%20and%20Mark%20Yatskar%20and%20Chris%20Callison-Burch%20and%20Ranjay%20Krishna%20and%20Aniruddha%20Kembhavi%20and%20Christopher%20Clark%0AAbstract%3A%20%20%20Reasoning%20about%20images%20with%20rich%20text%2C%20such%20as%20charts%20and%20documents%2C%20is%20a%0Acritical%20application%20of%20vision-language%20models%20%28VLMs%29.%20However%2C%20VLMs%20often%0Astruggle%20in%20these%20domains%20due%20to%20the%20scarcity%20of%20diverse%20text-rich%0Avision-language%20data.%20To%20address%20this%20challenge%2C%20we%20present%20CoSyn%2C%20a%20framework%0Athat%20leverages%20the%20coding%20capabilities%20of%20text-only%20large%20language%20models%0A%28LLMs%29%20to%20automatically%20create%20synthetic%20text-rich%20multimodal%20data.%20Given%20input%0Atext%20describing%20a%20target%20domain%20%28e.g.%2C%20%22nutrition%20fact%20labels%22%29%2C%20CoSyn%20prompts%0Aan%20LLM%20to%20generate%20code%20%28Python%2C%20HTML%2C%20LaTeX%2C%20etc.%29%20for%20rendering%20synthetic%0Aimages.%20With%20the%20underlying%20code%20as%20textual%20representations%20of%20the%20synthetic%0Aimages%2C%20CoSyn%20can%20generate%20high-quality%20instruction-tuning%20data%2C%20again%20relying%0Aon%20a%20text-only%20LLM.%20Using%20CoSyn%2C%20we%20constructed%20a%20dataset%20comprising%20400K%0Aimages%20and%202.7M%20rows%20of%20vision-language%20instruction-tuning%20data.%20Comprehensive%0Aexperiments%20on%20seven%20benchmarks%20demonstrate%20that%20models%20trained%20on%20our%0Asynthetic%20data%20achieve%20state-of-the-art%20performance%20among%20competitive%0Aopen-source%20models%2C%20including%20Llama%203.2%2C%20and%20surpass%20proprietary%20models%20such%20as%0AGPT-4V%20and%20Gemini%201.5%20Flash.%20Furthermore%2C%20CoSyn%20can%20produce%20synthetic%20pointing%0Adata%2C%20enabling%20VLMs%20to%20ground%20information%20within%20input%20images%2C%20showcasing%20its%0Apotential%20for%20developing%20multimodal%20agents%20capable%20of%20acting%20in%20real-world%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14846v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Text-Rich%2520Image%2520Understanding%2520via%2520Code-Guided%2520Synthetic%250A%2520%2520Multimodal%2520Data%2520Generation%26entry.906535625%3DYue%2520Yang%2520and%2520Ajay%2520Patel%2520and%2520Matt%2520Deitke%2520and%2520Tanmay%2520Gupta%2520and%2520Luca%2520Weihs%2520and%2520Andrew%2520Head%2520and%2520Mark%2520Yatskar%2520and%2520Chris%2520Callison-Burch%2520and%2520Ranjay%2520Krishna%2520and%2520Aniruddha%2520Kembhavi%2520and%2520Christopher%2520Clark%26entry.1292438233%3D%2520%2520Reasoning%2520about%2520images%2520with%2520rich%2520text%252C%2520such%2520as%2520charts%2520and%2520documents%252C%2520is%2520a%250Acritical%2520application%2520of%2520vision-language%2520models%2520%2528VLMs%2529.%2520However%252C%2520VLMs%2520often%250Astruggle%2520in%2520these%2520domains%2520due%2520to%2520the%2520scarcity%2520of%2520diverse%2520text-rich%250Avision-language%2520data.%2520To%2520address%2520this%2520challenge%252C%2520we%2520present%2520CoSyn%252C%2520a%2520framework%250Athat%2520leverages%2520the%2520coding%2520capabilities%2520of%2520text-only%2520large%2520language%2520models%250A%2528LLMs%2529%2520to%2520automatically%2520create%2520synthetic%2520text-rich%2520multimodal%2520data.%2520Given%2520input%250Atext%2520describing%2520a%2520target%2520domain%2520%2528e.g.%252C%2520%2522nutrition%2520fact%2520labels%2522%2529%252C%2520CoSyn%2520prompts%250Aan%2520LLM%2520to%2520generate%2520code%2520%2528Python%252C%2520HTML%252C%2520LaTeX%252C%2520etc.%2529%2520for%2520rendering%2520synthetic%250Aimages.%2520With%2520the%2520underlying%2520code%2520as%2520textual%2520representations%2520of%2520the%2520synthetic%250Aimages%252C%2520CoSyn%2520can%2520generate%2520high-quality%2520instruction-tuning%2520data%252C%2520again%2520relying%250Aon%2520a%2520text-only%2520LLM.%2520Using%2520CoSyn%252C%2520we%2520constructed%2520a%2520dataset%2520comprising%2520400K%250Aimages%2520and%25202.7M%2520rows%2520of%2520vision-language%2520instruction-tuning%2520data.%2520Comprehensive%250Aexperiments%2520on%2520seven%2520benchmarks%2520demonstrate%2520that%2520models%2520trained%2520on%2520our%250Asynthetic%2520data%2520achieve%2520state-of-the-art%2520performance%2520among%2520competitive%250Aopen-source%2520models%252C%2520including%2520Llama%25203.2%252C%2520and%2520surpass%2520proprietary%2520models%2520such%2520as%250AGPT-4V%2520and%2520Gemini%25201.5%2520Flash.%2520Furthermore%252C%2520CoSyn%2520can%2520produce%2520synthetic%2520pointing%250Adata%252C%2520enabling%2520VLMs%2520to%2520ground%2520information%2520within%2520input%2520images%252C%2520showcasing%2520its%250Apotential%2520for%2520developing%2520multimodal%2520agents%2520capable%2520of%2520acting%2520in%2520real-world%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14846v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Text-Rich%20Image%20Understanding%20via%20Code-Guided%20Synthetic%0A%20%20Multimodal%20Data%20Generation&entry.906535625=Yue%20Yang%20and%20Ajay%20Patel%20and%20Matt%20Deitke%20and%20Tanmay%20Gupta%20and%20Luca%20Weihs%20and%20Andrew%20Head%20and%20Mark%20Yatskar%20and%20Chris%20Callison-Burch%20and%20Ranjay%20Krishna%20and%20Aniruddha%20Kembhavi%20and%20Christopher%20Clark&entry.1292438233=%20%20Reasoning%20about%20images%20with%20rich%20text%2C%20such%20as%20charts%20and%20documents%2C%20is%20a%0Acritical%20application%20of%20vision-language%20models%20%28VLMs%29.%20However%2C%20VLMs%20often%0Astruggle%20in%20these%20domains%20due%20to%20the%20scarcity%20of%20diverse%20text-rich%0Avision-language%20data.%20To%20address%20this%20challenge%2C%20we%20present%20CoSyn%2C%20a%20framework%0Athat%20leverages%20the%20coding%20capabilities%20of%20text-only%20large%20language%20models%0A%28LLMs%29%20to%20automatically%20create%20synthetic%20text-rich%20multimodal%20data.%20Given%20input%0Atext%20describing%20a%20target%20domain%20%28e.g.%2C%20%22nutrition%20fact%20labels%22%29%2C%20CoSyn%20prompts%0Aan%20LLM%20to%20generate%20code%20%28Python%2C%20HTML%2C%20LaTeX%2C%20etc.%29%20for%20rendering%20synthetic%0Aimages.%20With%20the%20underlying%20code%20as%20textual%20representations%20of%20the%20synthetic%0Aimages%2C%20CoSyn%20can%20generate%20high-quality%20instruction-tuning%20data%2C%20again%20relying%0Aon%20a%20text-only%20LLM.%20Using%20CoSyn%2C%20we%20constructed%20a%20dataset%20comprising%20400K%0Aimages%20and%202.7M%20rows%20of%20vision-language%20instruction-tuning%20data.%20Comprehensive%0Aexperiments%20on%20seven%20benchmarks%20demonstrate%20that%20models%20trained%20on%20our%0Asynthetic%20data%20achieve%20state-of-the-art%20performance%20among%20competitive%0Aopen-source%20models%2C%20including%20Llama%203.2%2C%20and%20surpass%20proprietary%20models%20such%20as%0AGPT-4V%20and%20Gemini%201.5%20Flash.%20Furthermore%2C%20CoSyn%20can%20produce%20synthetic%20pointing%0Adata%2C%20enabling%20VLMs%20to%20ground%20information%20within%20input%20images%2C%20showcasing%20its%0Apotential%20for%20developing%20multimodal%20agents%20capable%20of%20acting%20in%20real-world%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14846v2&entry.124074799=Read"},
{"title": "SNAP: A Benchmark for Testing the Effects of Capture Conditions on\n  Fundamental Vision Tasks", "author": "Iuliia Kotseruba and John K. Tsotsos", "abstract": "  Generalization of deep-learning-based (DL) computer vision algorithms to\nvarious image perturbations is hard to establish and remains an active area of\nresearch. The majority of past analyses focused on the images already captured,\nwhereas effects of the image formation pipeline and environment are less\nstudied. In this paper, we address this issue by analyzing the impact of\ncapture conditions, such as camera parameters and lighting, on DL model\nperformance on 3 vision tasks -- image classification, object detection, and\nvisual question answering (VQA). To this end, we assess capture bias in common\nvision datasets and create a new benchmark, SNAP (for $\\textbf{S}$hutter speed,\nISO se$\\textbf{N}$sitivity, and $\\textbf{AP}$erture), consisting of images of\nobjects taken under controlled lighting conditions and with densely sampled\ncamera settings. We then evaluate a large number of DL vision models and show\nthe effects of capture conditions on each selected vision task. Lastly, we\nconduct an experiment to establish a human baseline for the VQA task. Our\nresults show that computer vision datasets are significantly biased, the models\ntrained on this data do not reach human accuracy even on the well-exposed\nimages, and are susceptible to both major exposure changes and minute\nvariations of camera settings. Code and data can be found at\nhttps://github.com/ykotseruba/SNAP\n", "link": "http://arxiv.org/abs/2505.15628v1", "date": "2025-05-21", "relevancy": 2.2919, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5806}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5715}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SNAP%3A%20A%20Benchmark%20for%20Testing%20the%20Effects%20of%20Capture%20Conditions%20on%0A%20%20Fundamental%20Vision%20Tasks&body=Title%3A%20SNAP%3A%20A%20Benchmark%20for%20Testing%20the%20Effects%20of%20Capture%20Conditions%20on%0A%20%20Fundamental%20Vision%20Tasks%0AAuthor%3A%20Iuliia%20Kotseruba%20and%20John%20K.%20Tsotsos%0AAbstract%3A%20%20%20Generalization%20of%20deep-learning-based%20%28DL%29%20computer%20vision%20algorithms%20to%0Avarious%20image%20perturbations%20is%20hard%20to%20establish%20and%20remains%20an%20active%20area%20of%0Aresearch.%20The%20majority%20of%20past%20analyses%20focused%20on%20the%20images%20already%20captured%2C%0Awhereas%20effects%20of%20the%20image%20formation%20pipeline%20and%20environment%20are%20less%0Astudied.%20In%20this%20paper%2C%20we%20address%20this%20issue%20by%20analyzing%20the%20impact%20of%0Acapture%20conditions%2C%20such%20as%20camera%20parameters%20and%20lighting%2C%20on%20DL%20model%0Aperformance%20on%203%20vision%20tasks%20--%20image%20classification%2C%20object%20detection%2C%20and%0Avisual%20question%20answering%20%28VQA%29.%20To%20this%20end%2C%20we%20assess%20capture%20bias%20in%20common%0Avision%20datasets%20and%20create%20a%20new%20benchmark%2C%20SNAP%20%28for%20%24%5Ctextbf%7BS%7D%24hutter%20speed%2C%0AISO%20se%24%5Ctextbf%7BN%7D%24sitivity%2C%20and%20%24%5Ctextbf%7BAP%7D%24erture%29%2C%20consisting%20of%20images%20of%0Aobjects%20taken%20under%20controlled%20lighting%20conditions%20and%20with%20densely%20sampled%0Acamera%20settings.%20We%20then%20evaluate%20a%20large%20number%20of%20DL%20vision%20models%20and%20show%0Athe%20effects%20of%20capture%20conditions%20on%20each%20selected%20vision%20task.%20Lastly%2C%20we%0Aconduct%20an%20experiment%20to%20establish%20a%20human%20baseline%20for%20the%20VQA%20task.%20Our%0Aresults%20show%20that%20computer%20vision%20datasets%20are%20significantly%20biased%2C%20the%20models%0Atrained%20on%20this%20data%20do%20not%20reach%20human%20accuracy%20even%20on%20the%20well-exposed%0Aimages%2C%20and%20are%20susceptible%20to%20both%20major%20exposure%20changes%20and%20minute%0Avariations%20of%20camera%20settings.%20Code%20and%20data%20can%20be%20found%20at%0Ahttps%3A//github.com/ykotseruba/SNAP%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSNAP%253A%2520A%2520Benchmark%2520for%2520Testing%2520the%2520Effects%2520of%2520Capture%2520Conditions%2520on%250A%2520%2520Fundamental%2520Vision%2520Tasks%26entry.906535625%3DIuliia%2520Kotseruba%2520and%2520John%2520K.%2520Tsotsos%26entry.1292438233%3D%2520%2520Generalization%2520of%2520deep-learning-based%2520%2528DL%2529%2520computer%2520vision%2520algorithms%2520to%250Avarious%2520image%2520perturbations%2520is%2520hard%2520to%2520establish%2520and%2520remains%2520an%2520active%2520area%2520of%250Aresearch.%2520The%2520majority%2520of%2520past%2520analyses%2520focused%2520on%2520the%2520images%2520already%2520captured%252C%250Awhereas%2520effects%2520of%2520the%2520image%2520formation%2520pipeline%2520and%2520environment%2520are%2520less%250Astudied.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%2520issue%2520by%2520analyzing%2520the%2520impact%2520of%250Acapture%2520conditions%252C%2520such%2520as%2520camera%2520parameters%2520and%2520lighting%252C%2520on%2520DL%2520model%250Aperformance%2520on%25203%2520vision%2520tasks%2520--%2520image%2520classification%252C%2520object%2520detection%252C%2520and%250Avisual%2520question%2520answering%2520%2528VQA%2529.%2520To%2520this%2520end%252C%2520we%2520assess%2520capture%2520bias%2520in%2520common%250Avision%2520datasets%2520and%2520create%2520a%2520new%2520benchmark%252C%2520SNAP%2520%2528for%2520%2524%255Ctextbf%257BS%257D%2524hutter%2520speed%252C%250AISO%2520se%2524%255Ctextbf%257BN%257D%2524sitivity%252C%2520and%2520%2524%255Ctextbf%257BAP%257D%2524erture%2529%252C%2520consisting%2520of%2520images%2520of%250Aobjects%2520taken%2520under%2520controlled%2520lighting%2520conditions%2520and%2520with%2520densely%2520sampled%250Acamera%2520settings.%2520We%2520then%2520evaluate%2520a%2520large%2520number%2520of%2520DL%2520vision%2520models%2520and%2520show%250Athe%2520effects%2520of%2520capture%2520conditions%2520on%2520each%2520selected%2520vision%2520task.%2520Lastly%252C%2520we%250Aconduct%2520an%2520experiment%2520to%2520establish%2520a%2520human%2520baseline%2520for%2520the%2520VQA%2520task.%2520Our%250Aresults%2520show%2520that%2520computer%2520vision%2520datasets%2520are%2520significantly%2520biased%252C%2520the%2520models%250Atrained%2520on%2520this%2520data%2520do%2520not%2520reach%2520human%2520accuracy%2520even%2520on%2520the%2520well-exposed%250Aimages%252C%2520and%2520are%2520susceptible%2520to%2520both%2520major%2520exposure%2520changes%2520and%2520minute%250Avariations%2520of%2520camera%2520settings.%2520Code%2520and%2520data%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/ykotseruba/SNAP%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SNAP%3A%20A%20Benchmark%20for%20Testing%20the%20Effects%20of%20Capture%20Conditions%20on%0A%20%20Fundamental%20Vision%20Tasks&entry.906535625=Iuliia%20Kotseruba%20and%20John%20K.%20Tsotsos&entry.1292438233=%20%20Generalization%20of%20deep-learning-based%20%28DL%29%20computer%20vision%20algorithms%20to%0Avarious%20image%20perturbations%20is%20hard%20to%20establish%20and%20remains%20an%20active%20area%20of%0Aresearch.%20The%20majority%20of%20past%20analyses%20focused%20on%20the%20images%20already%20captured%2C%0Awhereas%20effects%20of%20the%20image%20formation%20pipeline%20and%20environment%20are%20less%0Astudied.%20In%20this%20paper%2C%20we%20address%20this%20issue%20by%20analyzing%20the%20impact%20of%0Acapture%20conditions%2C%20such%20as%20camera%20parameters%20and%20lighting%2C%20on%20DL%20model%0Aperformance%20on%203%20vision%20tasks%20--%20image%20classification%2C%20object%20detection%2C%20and%0Avisual%20question%20answering%20%28VQA%29.%20To%20this%20end%2C%20we%20assess%20capture%20bias%20in%20common%0Avision%20datasets%20and%20create%20a%20new%20benchmark%2C%20SNAP%20%28for%20%24%5Ctextbf%7BS%7D%24hutter%20speed%2C%0AISO%20se%24%5Ctextbf%7BN%7D%24sitivity%2C%20and%20%24%5Ctextbf%7BAP%7D%24erture%29%2C%20consisting%20of%20images%20of%0Aobjects%20taken%20under%20controlled%20lighting%20conditions%20and%20with%20densely%20sampled%0Acamera%20settings.%20We%20then%20evaluate%20a%20large%20number%20of%20DL%20vision%20models%20and%20show%0Athe%20effects%20of%20capture%20conditions%20on%20each%20selected%20vision%20task.%20Lastly%2C%20we%0Aconduct%20an%20experiment%20to%20establish%20a%20human%20baseline%20for%20the%20VQA%20task.%20Our%0Aresults%20show%20that%20computer%20vision%20datasets%20are%20significantly%20biased%2C%20the%20models%0Atrained%20on%20this%20data%20do%20not%20reach%20human%20accuracy%20even%20on%20the%20well-exposed%0Aimages%2C%20and%20are%20susceptible%20to%20both%20major%20exposure%20changes%20and%20minute%0Avariations%20of%20camera%20settings.%20Code%20and%20data%20can%20be%20found%20at%0Ahttps%3A//github.com/ykotseruba/SNAP%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15628v1&entry.124074799=Read"},
{"title": "MaxPoolBERT: Enhancing BERT Classification via Layer- and Token-Wise\n  Aggregation", "author": "Maike Behrendt and Stefan Sylvius Wagner and Stefan Harmeling", "abstract": "  The [CLS] token in BERT is commonly used as a fixed-length representation for\nclassification tasks, yet prior work has shown that both other tokens and\nintermediate layers encode valuable contextual information. In this work, we\npropose MaxPoolBERT, a lightweight extension to BERT that refines the [CLS]\nrepresentation by aggregating information across layers and tokens.\nSpecifically, we explore three modifications: (i) max-pooling the [CLS] token\nacross multiple layers, (ii) enabling the [CLS] token to attend over the entire\nfinal layer using an additional multi-head attention (MHA) layer, and (iii)\ncombining max-pooling across the full sequence with MHA. Our approach enhances\nBERT's classification accuracy (especially on low-resource tasks) without\nrequiring pre-training or significantly increasing model size. Experiments on\nthe GLUE benchmark show that MaxPoolBERT consistently achieves a better\nperformance on the standard BERT-base model.\n", "link": "http://arxiv.org/abs/2505.15696v1", "date": "2025-05-21", "relevancy": 2.265, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4573}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4555}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MaxPoolBERT%3A%20Enhancing%20BERT%20Classification%20via%20Layer-%20and%20Token-Wise%0A%20%20Aggregation&body=Title%3A%20MaxPoolBERT%3A%20Enhancing%20BERT%20Classification%20via%20Layer-%20and%20Token-Wise%0A%20%20Aggregation%0AAuthor%3A%20Maike%20Behrendt%20and%20Stefan%20Sylvius%20Wagner%20and%20Stefan%20Harmeling%0AAbstract%3A%20%20%20The%20%5BCLS%5D%20token%20in%20BERT%20is%20commonly%20used%20as%20a%20fixed-length%20representation%20for%0Aclassification%20tasks%2C%20yet%20prior%20work%20has%20shown%20that%20both%20other%20tokens%20and%0Aintermediate%20layers%20encode%20valuable%20contextual%20information.%20In%20this%20work%2C%20we%0Apropose%20MaxPoolBERT%2C%20a%20lightweight%20extension%20to%20BERT%20that%20refines%20the%20%5BCLS%5D%0Arepresentation%20by%20aggregating%20information%20across%20layers%20and%20tokens.%0ASpecifically%2C%20we%20explore%20three%20modifications%3A%20%28i%29%20max-pooling%20the%20%5BCLS%5D%20token%0Aacross%20multiple%20layers%2C%20%28ii%29%20enabling%20the%20%5BCLS%5D%20token%20to%20attend%20over%20the%20entire%0Afinal%20layer%20using%20an%20additional%20multi-head%20attention%20%28MHA%29%20layer%2C%20and%20%28iii%29%0Acombining%20max-pooling%20across%20the%20full%20sequence%20with%20MHA.%20Our%20approach%20enhances%0ABERT%27s%20classification%20accuracy%20%28especially%20on%20low-resource%20tasks%29%20without%0Arequiring%20pre-training%20or%20significantly%20increasing%20model%20size.%20Experiments%20on%0Athe%20GLUE%20benchmark%20show%20that%20MaxPoolBERT%20consistently%20achieves%20a%20better%0Aperformance%20on%20the%20standard%20BERT-base%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15696v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaxPoolBERT%253A%2520Enhancing%2520BERT%2520Classification%2520via%2520Layer-%2520and%2520Token-Wise%250A%2520%2520Aggregation%26entry.906535625%3DMaike%2520Behrendt%2520and%2520Stefan%2520Sylvius%2520Wagner%2520and%2520Stefan%2520Harmeling%26entry.1292438233%3D%2520%2520The%2520%255BCLS%255D%2520token%2520in%2520BERT%2520is%2520commonly%2520used%2520as%2520a%2520fixed-length%2520representation%2520for%250Aclassification%2520tasks%252C%2520yet%2520prior%2520work%2520has%2520shown%2520that%2520both%2520other%2520tokens%2520and%250Aintermediate%2520layers%2520encode%2520valuable%2520contextual%2520information.%2520In%2520this%2520work%252C%2520we%250Apropose%2520MaxPoolBERT%252C%2520a%2520lightweight%2520extension%2520to%2520BERT%2520that%2520refines%2520the%2520%255BCLS%255D%250Arepresentation%2520by%2520aggregating%2520information%2520across%2520layers%2520and%2520tokens.%250ASpecifically%252C%2520we%2520explore%2520three%2520modifications%253A%2520%2528i%2529%2520max-pooling%2520the%2520%255BCLS%255D%2520token%250Aacross%2520multiple%2520layers%252C%2520%2528ii%2529%2520enabling%2520the%2520%255BCLS%255D%2520token%2520to%2520attend%2520over%2520the%2520entire%250Afinal%2520layer%2520using%2520an%2520additional%2520multi-head%2520attention%2520%2528MHA%2529%2520layer%252C%2520and%2520%2528iii%2529%250Acombining%2520max-pooling%2520across%2520the%2520full%2520sequence%2520with%2520MHA.%2520Our%2520approach%2520enhances%250ABERT%2527s%2520classification%2520accuracy%2520%2528especially%2520on%2520low-resource%2520tasks%2529%2520without%250Arequiring%2520pre-training%2520or%2520significantly%2520increasing%2520model%2520size.%2520Experiments%2520on%250Athe%2520GLUE%2520benchmark%2520show%2520that%2520MaxPoolBERT%2520consistently%2520achieves%2520a%2520better%250Aperformance%2520on%2520the%2520standard%2520BERT-base%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15696v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaxPoolBERT%3A%20Enhancing%20BERT%20Classification%20via%20Layer-%20and%20Token-Wise%0A%20%20Aggregation&entry.906535625=Maike%20Behrendt%20and%20Stefan%20Sylvius%20Wagner%20and%20Stefan%20Harmeling&entry.1292438233=%20%20The%20%5BCLS%5D%20token%20in%20BERT%20is%20commonly%20used%20as%20a%20fixed-length%20representation%20for%0Aclassification%20tasks%2C%20yet%20prior%20work%20has%20shown%20that%20both%20other%20tokens%20and%0Aintermediate%20layers%20encode%20valuable%20contextual%20information.%20In%20this%20work%2C%20we%0Apropose%20MaxPoolBERT%2C%20a%20lightweight%20extension%20to%20BERT%20that%20refines%20the%20%5BCLS%5D%0Arepresentation%20by%20aggregating%20information%20across%20layers%20and%20tokens.%0ASpecifically%2C%20we%20explore%20three%20modifications%3A%20%28i%29%20max-pooling%20the%20%5BCLS%5D%20token%0Aacross%20multiple%20layers%2C%20%28ii%29%20enabling%20the%20%5BCLS%5D%20token%20to%20attend%20over%20the%20entire%0Afinal%20layer%20using%20an%20additional%20multi-head%20attention%20%28MHA%29%20layer%2C%20and%20%28iii%29%0Acombining%20max-pooling%20across%20the%20full%20sequence%20with%20MHA.%20Our%20approach%20enhances%0ABERT%27s%20classification%20accuracy%20%28especially%20on%20low-resource%20tasks%29%20without%0Arequiring%20pre-training%20or%20significantly%20increasing%20model%20size.%20Experiments%20on%0Athe%20GLUE%20benchmark%20show%20that%20MaxPoolBERT%20consistently%20achieves%20a%20better%0Aperformance%20on%20the%20standard%20BERT-base%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15696v1&entry.124074799=Read"},
{"title": "Riemannian Optimization on the Oblique Manifold for Sparse Simplex\n  Constraints via Multiplicative Updates", "author": "Flavia Esposito and Andersen Ang", "abstract": "  Low-rank optimization problems with sparse simplex constraints involve\nvariables that must satisfy nonnegativity, sparsity, and sum-to-one conditions,\nmaking their optimization particularly challenging due to the interplay between\nlow-rank structures and constraints. These problems arise in various\napplications, including machine learning, signal processing, environmental\nfields, and computational biology. In this paper, we propose a novel manifold\noptimization approach to tackle these problems efficiently. Our method\nleverages the geometry of oblique rotation manifolds to reformulate the problem\nand introduces a new Riemannian optimization method based on Riemannian\ngradient descent that strictly maintains the simplex constraints. By exploiting\nthe underlying manifold structure, our approach improves optimization\nefficiency. Experiments on synthetic datasets compared to standard Euclidean\nand Riemannian methods show the effectiveness of the proposed method.\n", "link": "http://arxiv.org/abs/2503.24075v2", "date": "2025-05-21", "relevancy": 2.2583, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4546}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4542}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Riemannian%20Optimization%20on%20the%20Oblique%20Manifold%20for%20Sparse%20Simplex%0A%20%20Constraints%20via%20Multiplicative%20Updates&body=Title%3A%20Riemannian%20Optimization%20on%20the%20Oblique%20Manifold%20for%20Sparse%20Simplex%0A%20%20Constraints%20via%20Multiplicative%20Updates%0AAuthor%3A%20Flavia%20Esposito%20and%20Andersen%20Ang%0AAbstract%3A%20%20%20Low-rank%20optimization%20problems%20with%20sparse%20simplex%20constraints%20involve%0Avariables%20that%20must%20satisfy%20nonnegativity%2C%20sparsity%2C%20and%20sum-to-one%20conditions%2C%0Amaking%20their%20optimization%20particularly%20challenging%20due%20to%20the%20interplay%20between%0Alow-rank%20structures%20and%20constraints.%20These%20problems%20arise%20in%20various%0Aapplications%2C%20including%20machine%20learning%2C%20signal%20processing%2C%20environmental%0Afields%2C%20and%20computational%20biology.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20manifold%0Aoptimization%20approach%20to%20tackle%20these%20problems%20efficiently.%20Our%20method%0Aleverages%20the%20geometry%20of%20oblique%20rotation%20manifolds%20to%20reformulate%20the%20problem%0Aand%20introduces%20a%20new%20Riemannian%20optimization%20method%20based%20on%20Riemannian%0Agradient%20descent%20that%20strictly%20maintains%20the%20simplex%20constraints.%20By%20exploiting%0Athe%20underlying%20manifold%20structure%2C%20our%20approach%20improves%20optimization%0Aefficiency.%20Experiments%20on%20synthetic%20datasets%20compared%20to%20standard%20Euclidean%0Aand%20Riemannian%20methods%20show%20the%20effectiveness%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.24075v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRiemannian%2520Optimization%2520on%2520the%2520Oblique%2520Manifold%2520for%2520Sparse%2520Simplex%250A%2520%2520Constraints%2520via%2520Multiplicative%2520Updates%26entry.906535625%3DFlavia%2520Esposito%2520and%2520Andersen%2520Ang%26entry.1292438233%3D%2520%2520Low-rank%2520optimization%2520problems%2520with%2520sparse%2520simplex%2520constraints%2520involve%250Avariables%2520that%2520must%2520satisfy%2520nonnegativity%252C%2520sparsity%252C%2520and%2520sum-to-one%2520conditions%252C%250Amaking%2520their%2520optimization%2520particularly%2520challenging%2520due%2520to%2520the%2520interplay%2520between%250Alow-rank%2520structures%2520and%2520constraints.%2520These%2520problems%2520arise%2520in%2520various%250Aapplications%252C%2520including%2520machine%2520learning%252C%2520signal%2520processing%252C%2520environmental%250Afields%252C%2520and%2520computational%2520biology.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520manifold%250Aoptimization%2520approach%2520to%2520tackle%2520these%2520problems%2520efficiently.%2520Our%2520method%250Aleverages%2520the%2520geometry%2520of%2520oblique%2520rotation%2520manifolds%2520to%2520reformulate%2520the%2520problem%250Aand%2520introduces%2520a%2520new%2520Riemannian%2520optimization%2520method%2520based%2520on%2520Riemannian%250Agradient%2520descent%2520that%2520strictly%2520maintains%2520the%2520simplex%2520constraints.%2520By%2520exploiting%250Athe%2520underlying%2520manifold%2520structure%252C%2520our%2520approach%2520improves%2520optimization%250Aefficiency.%2520Experiments%2520on%2520synthetic%2520datasets%2520compared%2520to%2520standard%2520Euclidean%250Aand%2520Riemannian%2520methods%2520show%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.24075v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Riemannian%20Optimization%20on%20the%20Oblique%20Manifold%20for%20Sparse%20Simplex%0A%20%20Constraints%20via%20Multiplicative%20Updates&entry.906535625=Flavia%20Esposito%20and%20Andersen%20Ang&entry.1292438233=%20%20Low-rank%20optimization%20problems%20with%20sparse%20simplex%20constraints%20involve%0Avariables%20that%20must%20satisfy%20nonnegativity%2C%20sparsity%2C%20and%20sum-to-one%20conditions%2C%0Amaking%20their%20optimization%20particularly%20challenging%20due%20to%20the%20interplay%20between%0Alow-rank%20structures%20and%20constraints.%20These%20problems%20arise%20in%20various%0Aapplications%2C%20including%20machine%20learning%2C%20signal%20processing%2C%20environmental%0Afields%2C%20and%20computational%20biology.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20manifold%0Aoptimization%20approach%20to%20tackle%20these%20problems%20efficiently.%20Our%20method%0Aleverages%20the%20geometry%20of%20oblique%20rotation%20manifolds%20to%20reformulate%20the%20problem%0Aand%20introduces%20a%20new%20Riemannian%20optimization%20method%20based%20on%20Riemannian%0Agradient%20descent%20that%20strictly%20maintains%20the%20simplex%20constraints.%20By%20exploiting%0Athe%20underlying%20manifold%20structure%2C%20our%20approach%20improves%20optimization%0Aefficiency.%20Experiments%20on%20synthetic%20datasets%20compared%20to%20standard%20Euclidean%0Aand%20Riemannian%20methods%20show%20the%20effectiveness%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.24075v2&entry.124074799=Read"},
{"title": "Diversity-Driven View Subset Selection for Indoor Novel View Synthesis", "author": "Zehao Wang and Han Zhou and Matthew B. Blaschko and Tinne Tuytelaars and Minye Wu", "abstract": "  Novel view synthesis of indoor scenes can be achieved by capturing a\nmonocular video sequence of the environment. However, redundant information\ncaused by artificial movements in the input video data reduces the efficiency\nof scene modeling. To address this, we formulate the problem as a combinatorial\noptimization task for view subset selection. In this work, we propose a novel\nsubset selection framework that integrates a comprehensive diversity-based\nmeasurement with well-designed utility functions. We provide a theoretical\nanalysis of these utility functions and validate their effectiveness through\nextensive experiments. Furthermore, we introduce IndoorTraj, a novel dataset\ndesigned for indoor novel view synthesis, featuring complex and extended\ntrajectories that simulate intricate human behaviors. Experiments on IndoorTraj\nshow that our framework consistently outperforms baseline strategies while\nusing only 5-20% of the data, highlighting its remarkable efficiency and\neffectiveness. The code is available at:\nhttps://github.com/zehao-wang/IndoorTraj\n", "link": "http://arxiv.org/abs/2409.07098v2", "date": "2025-05-21", "relevancy": 2.2569, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5653}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5653}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diversity-Driven%20View%20Subset%20Selection%20for%20Indoor%20Novel%20View%20Synthesis&body=Title%3A%20Diversity-Driven%20View%20Subset%20Selection%20for%20Indoor%20Novel%20View%20Synthesis%0AAuthor%3A%20Zehao%20Wang%20and%20Han%20Zhou%20and%20Matthew%20B.%20Blaschko%20and%20Tinne%20Tuytelaars%20and%20Minye%20Wu%0AAbstract%3A%20%20%20Novel%20view%20synthesis%20of%20indoor%20scenes%20can%20be%20achieved%20by%20capturing%20a%0Amonocular%20video%20sequence%20of%20the%20environment.%20However%2C%20redundant%20information%0Acaused%20by%20artificial%20movements%20in%20the%20input%20video%20data%20reduces%20the%20efficiency%0Aof%20scene%20modeling.%20To%20address%20this%2C%20we%20formulate%20the%20problem%20as%20a%20combinatorial%0Aoptimization%20task%20for%20view%20subset%20selection.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Asubset%20selection%20framework%20that%20integrates%20a%20comprehensive%20diversity-based%0Ameasurement%20with%20well-designed%20utility%20functions.%20We%20provide%20a%20theoretical%0Aanalysis%20of%20these%20utility%20functions%20and%20validate%20their%20effectiveness%20through%0Aextensive%20experiments.%20Furthermore%2C%20we%20introduce%20IndoorTraj%2C%20a%20novel%20dataset%0Adesigned%20for%20indoor%20novel%20view%20synthesis%2C%20featuring%20complex%20and%20extended%0Atrajectories%20that%20simulate%20intricate%20human%20behaviors.%20Experiments%20on%20IndoorTraj%0Ashow%20that%20our%20framework%20consistently%20outperforms%20baseline%20strategies%20while%0Ausing%20only%205-20%25%20of%20the%20data%2C%20highlighting%20its%20remarkable%20efficiency%20and%0Aeffectiveness.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/zehao-wang/IndoorTraj%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07098v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiversity-Driven%2520View%2520Subset%2520Selection%2520for%2520Indoor%2520Novel%2520View%2520Synthesis%26entry.906535625%3DZehao%2520Wang%2520and%2520Han%2520Zhou%2520and%2520Matthew%2520B.%2520Blaschko%2520and%2520Tinne%2520Tuytelaars%2520and%2520Minye%2520Wu%26entry.1292438233%3D%2520%2520Novel%2520view%2520synthesis%2520of%2520indoor%2520scenes%2520can%2520be%2520achieved%2520by%2520capturing%2520a%250Amonocular%2520video%2520sequence%2520of%2520the%2520environment.%2520However%252C%2520redundant%2520information%250Acaused%2520by%2520artificial%2520movements%2520in%2520the%2520input%2520video%2520data%2520reduces%2520the%2520efficiency%250Aof%2520scene%2520modeling.%2520To%2520address%2520this%252C%2520we%2520formulate%2520the%2520problem%2520as%2520a%2520combinatorial%250Aoptimization%2520task%2520for%2520view%2520subset%2520selection.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%250Asubset%2520selection%2520framework%2520that%2520integrates%2520a%2520comprehensive%2520diversity-based%250Ameasurement%2520with%2520well-designed%2520utility%2520functions.%2520We%2520provide%2520a%2520theoretical%250Aanalysis%2520of%2520these%2520utility%2520functions%2520and%2520validate%2520their%2520effectiveness%2520through%250Aextensive%2520experiments.%2520Furthermore%252C%2520we%2520introduce%2520IndoorTraj%252C%2520a%2520novel%2520dataset%250Adesigned%2520for%2520indoor%2520novel%2520view%2520synthesis%252C%2520featuring%2520complex%2520and%2520extended%250Atrajectories%2520that%2520simulate%2520intricate%2520human%2520behaviors.%2520Experiments%2520on%2520IndoorTraj%250Ashow%2520that%2520our%2520framework%2520consistently%2520outperforms%2520baseline%2520strategies%2520while%250Ausing%2520only%25205-20%2525%2520of%2520the%2520data%252C%2520highlighting%2520its%2520remarkable%2520efficiency%2520and%250Aeffectiveness.%2520The%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/zehao-wang/IndoorTraj%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07098v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diversity-Driven%20View%20Subset%20Selection%20for%20Indoor%20Novel%20View%20Synthesis&entry.906535625=Zehao%20Wang%20and%20Han%20Zhou%20and%20Matthew%20B.%20Blaschko%20and%20Tinne%20Tuytelaars%20and%20Minye%20Wu&entry.1292438233=%20%20Novel%20view%20synthesis%20of%20indoor%20scenes%20can%20be%20achieved%20by%20capturing%20a%0Amonocular%20video%20sequence%20of%20the%20environment.%20However%2C%20redundant%20information%0Acaused%20by%20artificial%20movements%20in%20the%20input%20video%20data%20reduces%20the%20efficiency%0Aof%20scene%20modeling.%20To%20address%20this%2C%20we%20formulate%20the%20problem%20as%20a%20combinatorial%0Aoptimization%20task%20for%20view%20subset%20selection.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Asubset%20selection%20framework%20that%20integrates%20a%20comprehensive%20diversity-based%0Ameasurement%20with%20well-designed%20utility%20functions.%20We%20provide%20a%20theoretical%0Aanalysis%20of%20these%20utility%20functions%20and%20validate%20their%20effectiveness%20through%0Aextensive%20experiments.%20Furthermore%2C%20we%20introduce%20IndoorTraj%2C%20a%20novel%20dataset%0Adesigned%20for%20indoor%20novel%20view%20synthesis%2C%20featuring%20complex%20and%20extended%0Atrajectories%20that%20simulate%20intricate%20human%20behaviors.%20Experiments%20on%20IndoorTraj%0Ashow%20that%20our%20framework%20consistently%20outperforms%20baseline%20strategies%20while%0Ausing%20only%205-20%25%20of%20the%20data%2C%20highlighting%20its%20remarkable%20efficiency%20and%0Aeffectiveness.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/zehao-wang/IndoorTraj%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07098v2&entry.124074799=Read"},
{"title": "On the Robustness of Medical Vision-Language Models: Are they Truly\n  Generalizable?", "author": "Raza Imam and Rufael Marew and Mohammad Yaqub", "abstract": "  Medical Vision-Language Models (MVLMs) have achieved par excellence\ngeneralization in medical image analysis, yet their performance under noisy,\ncorrupted conditions remains largely untested. Clinical imaging is inherently\nsusceptible to acquisition artifacts and noise; however, existing evaluations\npredominantly assess generally clean datasets, overlooking robustness -- i.e.,\nthe model's ability to perform under real-world distortions. To address this\ngap, we first introduce MediMeta-C, a corruption benchmark that systematically\napplies several perturbations across multiple medical imaging datasets.\nCombined with MedMNIST-C, this establishes a comprehensive robustness\nevaluation framework for MVLMs. We further propose RobustMedCLIP, a visual\nencoder adaptation of a pretrained MVLM that incorporates few-shot tuning to\nenhance resilience against corruptions. Through extensive experiments, we\nbenchmark 5 major MVLMs across 5 medical imaging modalities, revealing that\nexisting models exhibit severe degradation under corruption and struggle with\ndomain-modality tradeoffs. Our findings highlight the necessity of diverse\ntraining and robust adaptation strategies, demonstrating that efficient\nlow-rank adaptation when paired with few-shot tuning, improves robustness while\npreserving generalization across modalities.\n", "link": "http://arxiv.org/abs/2505.15425v1", "date": "2025-05-21", "relevancy": 2.2484, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.564}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.564}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Robustness%20of%20Medical%20Vision-Language%20Models%3A%20Are%20they%20Truly%0A%20%20Generalizable%3F&body=Title%3A%20On%20the%20Robustness%20of%20Medical%20Vision-Language%20Models%3A%20Are%20they%20Truly%0A%20%20Generalizable%3F%0AAuthor%3A%20Raza%20Imam%20and%20Rufael%20Marew%20and%20Mohammad%20Yaqub%0AAbstract%3A%20%20%20Medical%20Vision-Language%20Models%20%28MVLMs%29%20have%20achieved%20par%20excellence%0Ageneralization%20in%20medical%20image%20analysis%2C%20yet%20their%20performance%20under%20noisy%2C%0Acorrupted%20conditions%20remains%20largely%20untested.%20Clinical%20imaging%20is%20inherently%0Asusceptible%20to%20acquisition%20artifacts%20and%20noise%3B%20however%2C%20existing%20evaluations%0Apredominantly%20assess%20generally%20clean%20datasets%2C%20overlooking%20robustness%20--%20i.e.%2C%0Athe%20model%27s%20ability%20to%20perform%20under%20real-world%20distortions.%20To%20address%20this%0Agap%2C%20we%20first%20introduce%20MediMeta-C%2C%20a%20corruption%20benchmark%20that%20systematically%0Aapplies%20several%20perturbations%20across%20multiple%20medical%20imaging%20datasets.%0ACombined%20with%20MedMNIST-C%2C%20this%20establishes%20a%20comprehensive%20robustness%0Aevaluation%20framework%20for%20MVLMs.%20We%20further%20propose%20RobustMedCLIP%2C%20a%20visual%0Aencoder%20adaptation%20of%20a%20pretrained%20MVLM%20that%20incorporates%20few-shot%20tuning%20to%0Aenhance%20resilience%20against%20corruptions.%20Through%20extensive%20experiments%2C%20we%0Abenchmark%205%20major%20MVLMs%20across%205%20medical%20imaging%20modalities%2C%20revealing%20that%0Aexisting%20models%20exhibit%20severe%20degradation%20under%20corruption%20and%20struggle%20with%0Adomain-modality%20tradeoffs.%20Our%20findings%20highlight%20the%20necessity%20of%20diverse%0Atraining%20and%20robust%20adaptation%20strategies%2C%20demonstrating%20that%20efficient%0Alow-rank%20adaptation%20when%20paired%20with%20few-shot%20tuning%2C%20improves%20robustness%20while%0Apreserving%20generalization%20across%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Robustness%2520of%2520Medical%2520Vision-Language%2520Models%253A%2520Are%2520they%2520Truly%250A%2520%2520Generalizable%253F%26entry.906535625%3DRaza%2520Imam%2520and%2520Rufael%2520Marew%2520and%2520Mohammad%2520Yaqub%26entry.1292438233%3D%2520%2520Medical%2520Vision-Language%2520Models%2520%2528MVLMs%2529%2520have%2520achieved%2520par%2520excellence%250Ageneralization%2520in%2520medical%2520image%2520analysis%252C%2520yet%2520their%2520performance%2520under%2520noisy%252C%250Acorrupted%2520conditions%2520remains%2520largely%2520untested.%2520Clinical%2520imaging%2520is%2520inherently%250Asusceptible%2520to%2520acquisition%2520artifacts%2520and%2520noise%253B%2520however%252C%2520existing%2520evaluations%250Apredominantly%2520assess%2520generally%2520clean%2520datasets%252C%2520overlooking%2520robustness%2520--%2520i.e.%252C%250Athe%2520model%2527s%2520ability%2520to%2520perform%2520under%2520real-world%2520distortions.%2520To%2520address%2520this%250Agap%252C%2520we%2520first%2520introduce%2520MediMeta-C%252C%2520a%2520corruption%2520benchmark%2520that%2520systematically%250Aapplies%2520several%2520perturbations%2520across%2520multiple%2520medical%2520imaging%2520datasets.%250ACombined%2520with%2520MedMNIST-C%252C%2520this%2520establishes%2520a%2520comprehensive%2520robustness%250Aevaluation%2520framework%2520for%2520MVLMs.%2520We%2520further%2520propose%2520RobustMedCLIP%252C%2520a%2520visual%250Aencoder%2520adaptation%2520of%2520a%2520pretrained%2520MVLM%2520that%2520incorporates%2520few-shot%2520tuning%2520to%250Aenhance%2520resilience%2520against%2520corruptions.%2520Through%2520extensive%2520experiments%252C%2520we%250Abenchmark%25205%2520major%2520MVLMs%2520across%25205%2520medical%2520imaging%2520modalities%252C%2520revealing%2520that%250Aexisting%2520models%2520exhibit%2520severe%2520degradation%2520under%2520corruption%2520and%2520struggle%2520with%250Adomain-modality%2520tradeoffs.%2520Our%2520findings%2520highlight%2520the%2520necessity%2520of%2520diverse%250Atraining%2520and%2520robust%2520adaptation%2520strategies%252C%2520demonstrating%2520that%2520efficient%250Alow-rank%2520adaptation%2520when%2520paired%2520with%2520few-shot%2520tuning%252C%2520improves%2520robustness%2520while%250Apreserving%2520generalization%2520across%2520modalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Robustness%20of%20Medical%20Vision-Language%20Models%3A%20Are%20they%20Truly%0A%20%20Generalizable%3F&entry.906535625=Raza%20Imam%20and%20Rufael%20Marew%20and%20Mohammad%20Yaqub&entry.1292438233=%20%20Medical%20Vision-Language%20Models%20%28MVLMs%29%20have%20achieved%20par%20excellence%0Ageneralization%20in%20medical%20image%20analysis%2C%20yet%20their%20performance%20under%20noisy%2C%0Acorrupted%20conditions%20remains%20largely%20untested.%20Clinical%20imaging%20is%20inherently%0Asusceptible%20to%20acquisition%20artifacts%20and%20noise%3B%20however%2C%20existing%20evaluations%0Apredominantly%20assess%20generally%20clean%20datasets%2C%20overlooking%20robustness%20--%20i.e.%2C%0Athe%20model%27s%20ability%20to%20perform%20under%20real-world%20distortions.%20To%20address%20this%0Agap%2C%20we%20first%20introduce%20MediMeta-C%2C%20a%20corruption%20benchmark%20that%20systematically%0Aapplies%20several%20perturbations%20across%20multiple%20medical%20imaging%20datasets.%0ACombined%20with%20MedMNIST-C%2C%20this%20establishes%20a%20comprehensive%20robustness%0Aevaluation%20framework%20for%20MVLMs.%20We%20further%20propose%20RobustMedCLIP%2C%20a%20visual%0Aencoder%20adaptation%20of%20a%20pretrained%20MVLM%20that%20incorporates%20few-shot%20tuning%20to%0Aenhance%20resilience%20against%20corruptions.%20Through%20extensive%20experiments%2C%20we%0Abenchmark%205%20major%20MVLMs%20across%205%20medical%20imaging%20modalities%2C%20revealing%20that%0Aexisting%20models%20exhibit%20severe%20degradation%20under%20corruption%20and%20struggle%20with%0Adomain-modality%20tradeoffs.%20Our%20findings%20highlight%20the%20necessity%20of%20diverse%0Atraining%20and%20robust%20adaptation%20strategies%2C%20demonstrating%20that%20efficient%0Alow-rank%20adaptation%20when%20paired%20with%20few-shot%20tuning%2C%20improves%20robustness%20while%0Apreserving%20generalization%20across%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15425v1&entry.124074799=Read"},
{"title": "Spectral-Aware Global Fusion for RGB-Thermal Semantic Segmentation", "author": "Ce Zhang and Zifu Wan and Simon Stepputtis and Katia Sycara and Yaqi Xie", "abstract": "  Semantic segmentation relying solely on RGB data often struggles in\nchallenging conditions such as low illumination and obscured views, limiting\nits reliability in critical applications like autonomous driving. To address\nthis, integrating additional thermal radiation data with RGB images\ndemonstrates enhanced performance and robustness. However, how to effectively\nreconcile the modality discrepancies and fuse the RGB and thermal features\nremains a well-known challenge. In this work, we address this challenge from a\nnovel spectral perspective. We observe that the multi-modal features can be\ncategorized into two spectral components: low-frequency features that provide\nbroad scene context, including color variations and smooth areas, and\nhigh-frequency features that capture modality-specific details such as edges\nand textures. Inspired by this, we propose the Spectral-aware Global Fusion\nNetwork (SGFNet) to effectively enhance and fuse the multi-modal features by\nexplicitly modeling the interactions between the high-frequency,\nmodality-specific features. Our experimental results demonstrate that SGFNet\noutperforms the state-of-the-art methods on the MFNet and PST900 datasets.\n", "link": "http://arxiv.org/abs/2505.15491v1", "date": "2025-05-21", "relevancy": 2.2336, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.583}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5468}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5258}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectral-Aware%20Global%20Fusion%20for%20RGB-Thermal%20Semantic%20Segmentation&body=Title%3A%20Spectral-Aware%20Global%20Fusion%20for%20RGB-Thermal%20Semantic%20Segmentation%0AAuthor%3A%20Ce%20Zhang%20and%20Zifu%20Wan%20and%20Simon%20Stepputtis%20and%20Katia%20Sycara%20and%20Yaqi%20Xie%0AAbstract%3A%20%20%20Semantic%20segmentation%20relying%20solely%20on%20RGB%20data%20often%20struggles%20in%0Achallenging%20conditions%20such%20as%20low%20illumination%20and%20obscured%20views%2C%20limiting%0Aits%20reliability%20in%20critical%20applications%20like%20autonomous%20driving.%20To%20address%0Athis%2C%20integrating%20additional%20thermal%20radiation%20data%20with%20RGB%20images%0Ademonstrates%20enhanced%20performance%20and%20robustness.%20However%2C%20how%20to%20effectively%0Areconcile%20the%20modality%20discrepancies%20and%20fuse%20the%20RGB%20and%20thermal%20features%0Aremains%20a%20well-known%20challenge.%20In%20this%20work%2C%20we%20address%20this%20challenge%20from%20a%0Anovel%20spectral%20perspective.%20We%20observe%20that%20the%20multi-modal%20features%20can%20be%0Acategorized%20into%20two%20spectral%20components%3A%20low-frequency%20features%20that%20provide%0Abroad%20scene%20context%2C%20including%20color%20variations%20and%20smooth%20areas%2C%20and%0Ahigh-frequency%20features%20that%20capture%20modality-specific%20details%20such%20as%20edges%0Aand%20textures.%20Inspired%20by%20this%2C%20we%20propose%20the%20Spectral-aware%20Global%20Fusion%0ANetwork%20%28SGFNet%29%20to%20effectively%20enhance%20and%20fuse%20the%20multi-modal%20features%20by%0Aexplicitly%20modeling%20the%20interactions%20between%20the%20high-frequency%2C%0Amodality-specific%20features.%20Our%20experimental%20results%20demonstrate%20that%20SGFNet%0Aoutperforms%20the%20state-of-the-art%20methods%20on%20the%20MFNet%20and%20PST900%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15491v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectral-Aware%2520Global%2520Fusion%2520for%2520RGB-Thermal%2520Semantic%2520Segmentation%26entry.906535625%3DCe%2520Zhang%2520and%2520Zifu%2520Wan%2520and%2520Simon%2520Stepputtis%2520and%2520Katia%2520Sycara%2520and%2520Yaqi%2520Xie%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520relying%2520solely%2520on%2520RGB%2520data%2520often%2520struggles%2520in%250Achallenging%2520conditions%2520such%2520as%2520low%2520illumination%2520and%2520obscured%2520views%252C%2520limiting%250Aits%2520reliability%2520in%2520critical%2520applications%2520like%2520autonomous%2520driving.%2520To%2520address%250Athis%252C%2520integrating%2520additional%2520thermal%2520radiation%2520data%2520with%2520RGB%2520images%250Ademonstrates%2520enhanced%2520performance%2520and%2520robustness.%2520However%252C%2520how%2520to%2520effectively%250Areconcile%2520the%2520modality%2520discrepancies%2520and%2520fuse%2520the%2520RGB%2520and%2520thermal%2520features%250Aremains%2520a%2520well-known%2520challenge.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520challenge%2520from%2520a%250Anovel%2520spectral%2520perspective.%2520We%2520observe%2520that%2520the%2520multi-modal%2520features%2520can%2520be%250Acategorized%2520into%2520two%2520spectral%2520components%253A%2520low-frequency%2520features%2520that%2520provide%250Abroad%2520scene%2520context%252C%2520including%2520color%2520variations%2520and%2520smooth%2520areas%252C%2520and%250Ahigh-frequency%2520features%2520that%2520capture%2520modality-specific%2520details%2520such%2520as%2520edges%250Aand%2520textures.%2520Inspired%2520by%2520this%252C%2520we%2520propose%2520the%2520Spectral-aware%2520Global%2520Fusion%250ANetwork%2520%2528SGFNet%2529%2520to%2520effectively%2520enhance%2520and%2520fuse%2520the%2520multi-modal%2520features%2520by%250Aexplicitly%2520modeling%2520the%2520interactions%2520between%2520the%2520high-frequency%252C%250Amodality-specific%2520features.%2520Our%2520experimental%2520results%2520demonstrate%2520that%2520SGFNet%250Aoutperforms%2520the%2520state-of-the-art%2520methods%2520on%2520the%2520MFNet%2520and%2520PST900%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15491v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectral-Aware%20Global%20Fusion%20for%20RGB-Thermal%20Semantic%20Segmentation&entry.906535625=Ce%20Zhang%20and%20Zifu%20Wan%20and%20Simon%20Stepputtis%20and%20Katia%20Sycara%20and%20Yaqi%20Xie&entry.1292438233=%20%20Semantic%20segmentation%20relying%20solely%20on%20RGB%20data%20often%20struggles%20in%0Achallenging%20conditions%20such%20as%20low%20illumination%20and%20obscured%20views%2C%20limiting%0Aits%20reliability%20in%20critical%20applications%20like%20autonomous%20driving.%20To%20address%0Athis%2C%20integrating%20additional%20thermal%20radiation%20data%20with%20RGB%20images%0Ademonstrates%20enhanced%20performance%20and%20robustness.%20However%2C%20how%20to%20effectively%0Areconcile%20the%20modality%20discrepancies%20and%20fuse%20the%20RGB%20and%20thermal%20features%0Aremains%20a%20well-known%20challenge.%20In%20this%20work%2C%20we%20address%20this%20challenge%20from%20a%0Anovel%20spectral%20perspective.%20We%20observe%20that%20the%20multi-modal%20features%20can%20be%0Acategorized%20into%20two%20spectral%20components%3A%20low-frequency%20features%20that%20provide%0Abroad%20scene%20context%2C%20including%20color%20variations%20and%20smooth%20areas%2C%20and%0Ahigh-frequency%20features%20that%20capture%20modality-specific%20details%20such%20as%20edges%0Aand%20textures.%20Inspired%20by%20this%2C%20we%20propose%20the%20Spectral-aware%20Global%20Fusion%0ANetwork%20%28SGFNet%29%20to%20effectively%20enhance%20and%20fuse%20the%20multi-modal%20features%20by%0Aexplicitly%20modeling%20the%20interactions%20between%20the%20high-frequency%2C%0Amodality-specific%20features.%20Our%20experimental%20results%20demonstrate%20that%20SGFNet%0Aoutperforms%20the%20state-of-the-art%20methods%20on%20the%20MFNet%20and%20PST900%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15491v1&entry.124074799=Read"},
{"title": "Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning\n  in Language Models", "author": "Zihao Li and Xu Wang and Yuzhe Yang and Ziyu Yao and Haoyi Xiong and Mengnan Du", "abstract": "  Large Language Models (LLMs) demonstrate the ability to solve reasoning and\nmathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT\nlength, as seen in models such as DeepSeek-R1, significantly enhances this\nreasoning for complex problems, but requires costly and high-quality long CoT\ndata and fine-tuning. This work, inspired by the deep thinking paradigm of\nDeepSeek-R1, utilizes a steering technique to enhance the reasoning ability of\nan LLM without external datasets. Our method first employs Sparse Autoencoders\n(SAEs) to extract interpretable features from vanilla CoT. These features are\nthen used to steer the LLM's internal states during generation. Recognizing\nthat many LLMs do not have corresponding pre-trained SAEs, we further introduce\na novel SAE-free steering algorithm, which directly computes steering\ndirections from the residual activations of an LLM, obviating the need for an\nexplicit SAE. Experimental results demonstrate that both our SAE-based and\nsubsequent SAE-free steering algorithms significantly enhance the reasoning\ncapabilities of LLMs.\n", "link": "http://arxiv.org/abs/2505.15634v1", "date": "2025-05-21", "relevancy": 2.2329, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5679}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5679}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature%20Extraction%20and%20Steering%20for%20Enhanced%20Chain-of-Thought%20Reasoning%0A%20%20in%20Language%20Models&body=Title%3A%20Feature%20Extraction%20and%20Steering%20for%20Enhanced%20Chain-of-Thought%20Reasoning%0A%20%20in%20Language%20Models%0AAuthor%3A%20Zihao%20Li%20and%20Xu%20Wang%20and%20Yuzhe%20Yang%20and%20Ziyu%20Yao%20and%20Haoyi%20Xiong%20and%20Mengnan%20Du%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20the%20ability%20to%20solve%20reasoning%20and%0Amathematical%20problems%20using%20the%20Chain-of-Thought%20%28CoT%29%20technique.%20Expanding%20CoT%0Alength%2C%20as%20seen%20in%20models%20such%20as%20DeepSeek-R1%2C%20significantly%20enhances%20this%0Areasoning%20for%20complex%20problems%2C%20but%20requires%20costly%20and%20high-quality%20long%20CoT%0Adata%20and%20fine-tuning.%20This%20work%2C%20inspired%20by%20the%20deep%20thinking%20paradigm%20of%0ADeepSeek-R1%2C%20utilizes%20a%20steering%20technique%20to%20enhance%20the%20reasoning%20ability%20of%0Aan%20LLM%20without%20external%20datasets.%20Our%20method%20first%20employs%20Sparse%20Autoencoders%0A%28SAEs%29%20to%20extract%20interpretable%20features%20from%20vanilla%20CoT.%20These%20features%20are%0Athen%20used%20to%20steer%20the%20LLM%27s%20internal%20states%20during%20generation.%20Recognizing%0Athat%20many%20LLMs%20do%20not%20have%20corresponding%20pre-trained%20SAEs%2C%20we%20further%20introduce%0Aa%20novel%20SAE-free%20steering%20algorithm%2C%20which%20directly%20computes%20steering%0Adirections%20from%20the%20residual%20activations%20of%20an%20LLM%2C%20obviating%20the%20need%20for%20an%0Aexplicit%20SAE.%20Experimental%20results%20demonstrate%20that%20both%20our%20SAE-based%20and%0Asubsequent%20SAE-free%20steering%20algorithms%20significantly%20enhance%20the%20reasoning%0Acapabilities%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15634v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature%2520Extraction%2520and%2520Steering%2520for%2520Enhanced%2520Chain-of-Thought%2520Reasoning%250A%2520%2520in%2520Language%2520Models%26entry.906535625%3DZihao%2520Li%2520and%2520Xu%2520Wang%2520and%2520Yuzhe%2520Yang%2520and%2520Ziyu%2520Yao%2520and%2520Haoyi%2520Xiong%2520and%2520Mengnan%2520Du%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520demonstrate%2520the%2520ability%2520to%2520solve%2520reasoning%2520and%250Amathematical%2520problems%2520using%2520the%2520Chain-of-Thought%2520%2528CoT%2529%2520technique.%2520Expanding%2520CoT%250Alength%252C%2520as%2520seen%2520in%2520models%2520such%2520as%2520DeepSeek-R1%252C%2520significantly%2520enhances%2520this%250Areasoning%2520for%2520complex%2520problems%252C%2520but%2520requires%2520costly%2520and%2520high-quality%2520long%2520CoT%250Adata%2520and%2520fine-tuning.%2520This%2520work%252C%2520inspired%2520by%2520the%2520deep%2520thinking%2520paradigm%2520of%250ADeepSeek-R1%252C%2520utilizes%2520a%2520steering%2520technique%2520to%2520enhance%2520the%2520reasoning%2520ability%2520of%250Aan%2520LLM%2520without%2520external%2520datasets.%2520Our%2520method%2520first%2520employs%2520Sparse%2520Autoencoders%250A%2528SAEs%2529%2520to%2520extract%2520interpretable%2520features%2520from%2520vanilla%2520CoT.%2520These%2520features%2520are%250Athen%2520used%2520to%2520steer%2520the%2520LLM%2527s%2520internal%2520states%2520during%2520generation.%2520Recognizing%250Athat%2520many%2520LLMs%2520do%2520not%2520have%2520corresponding%2520pre-trained%2520SAEs%252C%2520we%2520further%2520introduce%250Aa%2520novel%2520SAE-free%2520steering%2520algorithm%252C%2520which%2520directly%2520computes%2520steering%250Adirections%2520from%2520the%2520residual%2520activations%2520of%2520an%2520LLM%252C%2520obviating%2520the%2520need%2520for%2520an%250Aexplicit%2520SAE.%2520Experimental%2520results%2520demonstrate%2520that%2520both%2520our%2520SAE-based%2520and%250Asubsequent%2520SAE-free%2520steering%2520algorithms%2520significantly%2520enhance%2520the%2520reasoning%250Acapabilities%2520of%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15634v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature%20Extraction%20and%20Steering%20for%20Enhanced%20Chain-of-Thought%20Reasoning%0A%20%20in%20Language%20Models&entry.906535625=Zihao%20Li%20and%20Xu%20Wang%20and%20Yuzhe%20Yang%20and%20Ziyu%20Yao%20and%20Haoyi%20Xiong%20and%20Mengnan%20Du&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20the%20ability%20to%20solve%20reasoning%20and%0Amathematical%20problems%20using%20the%20Chain-of-Thought%20%28CoT%29%20technique.%20Expanding%20CoT%0Alength%2C%20as%20seen%20in%20models%20such%20as%20DeepSeek-R1%2C%20significantly%20enhances%20this%0Areasoning%20for%20complex%20problems%2C%20but%20requires%20costly%20and%20high-quality%20long%20CoT%0Adata%20and%20fine-tuning.%20This%20work%2C%20inspired%20by%20the%20deep%20thinking%20paradigm%20of%0ADeepSeek-R1%2C%20utilizes%20a%20steering%20technique%20to%20enhance%20the%20reasoning%20ability%20of%0Aan%20LLM%20without%20external%20datasets.%20Our%20method%20first%20employs%20Sparse%20Autoencoders%0A%28SAEs%29%20to%20extract%20interpretable%20features%20from%20vanilla%20CoT.%20These%20features%20are%0Athen%20used%20to%20steer%20the%20LLM%27s%20internal%20states%20during%20generation.%20Recognizing%0Athat%20many%20LLMs%20do%20not%20have%20corresponding%20pre-trained%20SAEs%2C%20we%20further%20introduce%0Aa%20novel%20SAE-free%20steering%20algorithm%2C%20which%20directly%20computes%20steering%0Adirections%20from%20the%20residual%20activations%20of%20an%20LLM%2C%20obviating%20the%20need%20for%20an%0Aexplicit%20SAE.%20Experimental%20results%20demonstrate%20that%20both%20our%20SAE-based%20and%0Asubsequent%20SAE-free%20steering%20algorithms%20significantly%20enhance%20the%20reasoning%0Acapabilities%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15634v1&entry.124074799=Read"},
{"title": "Enhancing Monte Carlo Dropout Performance for Uncertainty Quantification", "author": "Hamzeh Asgharnezhad and Afshar Shamsi and Roohallah Alizadehsani and Arash Mohammadi and Hamid Alinejad-Rokny", "abstract": "  Knowing the uncertainty associated with the output of a deep neural network\nis of paramount importance in making trustworthy decisions, particularly in\nhigh-stakes fields like medical diagnosis and autonomous systems. Monte Carlo\nDropout (MCD) is a widely used method for uncertainty quantification, as it can\nbe easily integrated into various deep architectures. However, conventional MCD\noften struggles with providing well-calibrated uncertainty estimates. To\naddress this, we introduce innovative frameworks that enhances MCD by\nintegrating different search solutions namely Grey Wolf Optimizer (GWO),\nBayesian Optimization (BO), and Particle Swarm Optimization (PSO) as well as an\nuncertainty-aware loss function, thereby improving the reliability of\nuncertainty quantification. We conduct comprehensive experiments using\ndifferent backbones, namely DenseNet121, ResNet50, and VGG16, on various\ndatasets, including Cats vs. Dogs, Myocarditis, Wisconsin, and a synthetic\ndataset (Circles). Our proposed algorithm outperforms the MCD baseline by 2-3%\non average in terms of both conventional accuracy and uncertainty accuracy\nwhile achieving significantly better calibration. These results highlight the\npotential of our approach to enhance the trustworthiness of deep learning\nmodels in safety-critical applications.\n", "link": "http://arxiv.org/abs/2505.15671v1", "date": "2025-05-21", "relevancy": 2.229, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5995}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5653}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Monte%20Carlo%20Dropout%20Performance%20for%20Uncertainty%20Quantification&body=Title%3A%20Enhancing%20Monte%20Carlo%20Dropout%20Performance%20for%20Uncertainty%20Quantification%0AAuthor%3A%20Hamzeh%20Asgharnezhad%20and%20Afshar%20Shamsi%20and%20Roohallah%20Alizadehsani%20and%20Arash%20Mohammadi%20and%20Hamid%20Alinejad-Rokny%0AAbstract%3A%20%20%20Knowing%20the%20uncertainty%20associated%20with%20the%20output%20of%20a%20deep%20neural%20network%0Ais%20of%20paramount%20importance%20in%20making%20trustworthy%20decisions%2C%20particularly%20in%0Ahigh-stakes%20fields%20like%20medical%20diagnosis%20and%20autonomous%20systems.%20Monte%20Carlo%0ADropout%20%28MCD%29%20is%20a%20widely%20used%20method%20for%20uncertainty%20quantification%2C%20as%20it%20can%0Abe%20easily%20integrated%20into%20various%20deep%20architectures.%20However%2C%20conventional%20MCD%0Aoften%20struggles%20with%20providing%20well-calibrated%20uncertainty%20estimates.%20To%0Aaddress%20this%2C%20we%20introduce%20innovative%20frameworks%20that%20enhances%20MCD%20by%0Aintegrating%20different%20search%20solutions%20namely%20Grey%20Wolf%20Optimizer%20%28GWO%29%2C%0ABayesian%20Optimization%20%28BO%29%2C%20and%20Particle%20Swarm%20Optimization%20%28PSO%29%20as%20well%20as%20an%0Auncertainty-aware%20loss%20function%2C%20thereby%20improving%20the%20reliability%20of%0Auncertainty%20quantification.%20We%20conduct%20comprehensive%20experiments%20using%0Adifferent%20backbones%2C%20namely%20DenseNet121%2C%20ResNet50%2C%20and%20VGG16%2C%20on%20various%0Adatasets%2C%20including%20Cats%20vs.%20Dogs%2C%20Myocarditis%2C%20Wisconsin%2C%20and%20a%20synthetic%0Adataset%20%28Circles%29.%20Our%20proposed%20algorithm%20outperforms%20the%20MCD%20baseline%20by%202-3%25%0Aon%20average%20in%20terms%20of%20both%20conventional%20accuracy%20and%20uncertainty%20accuracy%0Awhile%20achieving%20significantly%20better%20calibration.%20These%20results%20highlight%20the%0Apotential%20of%20our%20approach%20to%20enhance%20the%20trustworthiness%20of%20deep%20learning%0Amodels%20in%20safety-critical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15671v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Monte%2520Carlo%2520Dropout%2520Performance%2520for%2520Uncertainty%2520Quantification%26entry.906535625%3DHamzeh%2520Asgharnezhad%2520and%2520Afshar%2520Shamsi%2520and%2520Roohallah%2520Alizadehsani%2520and%2520Arash%2520Mohammadi%2520and%2520Hamid%2520Alinejad-Rokny%26entry.1292438233%3D%2520%2520Knowing%2520the%2520uncertainty%2520associated%2520with%2520the%2520output%2520of%2520a%2520deep%2520neural%2520network%250Ais%2520of%2520paramount%2520importance%2520in%2520making%2520trustworthy%2520decisions%252C%2520particularly%2520in%250Ahigh-stakes%2520fields%2520like%2520medical%2520diagnosis%2520and%2520autonomous%2520systems.%2520Monte%2520Carlo%250ADropout%2520%2528MCD%2529%2520is%2520a%2520widely%2520used%2520method%2520for%2520uncertainty%2520quantification%252C%2520as%2520it%2520can%250Abe%2520easily%2520integrated%2520into%2520various%2520deep%2520architectures.%2520However%252C%2520conventional%2520MCD%250Aoften%2520struggles%2520with%2520providing%2520well-calibrated%2520uncertainty%2520estimates.%2520To%250Aaddress%2520this%252C%2520we%2520introduce%2520innovative%2520frameworks%2520that%2520enhances%2520MCD%2520by%250Aintegrating%2520different%2520search%2520solutions%2520namely%2520Grey%2520Wolf%2520Optimizer%2520%2528GWO%2529%252C%250ABayesian%2520Optimization%2520%2528BO%2529%252C%2520and%2520Particle%2520Swarm%2520Optimization%2520%2528PSO%2529%2520as%2520well%2520as%2520an%250Auncertainty-aware%2520loss%2520function%252C%2520thereby%2520improving%2520the%2520reliability%2520of%250Auncertainty%2520quantification.%2520We%2520conduct%2520comprehensive%2520experiments%2520using%250Adifferent%2520backbones%252C%2520namely%2520DenseNet121%252C%2520ResNet50%252C%2520and%2520VGG16%252C%2520on%2520various%250Adatasets%252C%2520including%2520Cats%2520vs.%2520Dogs%252C%2520Myocarditis%252C%2520Wisconsin%252C%2520and%2520a%2520synthetic%250Adataset%2520%2528Circles%2529.%2520Our%2520proposed%2520algorithm%2520outperforms%2520the%2520MCD%2520baseline%2520by%25202-3%2525%250Aon%2520average%2520in%2520terms%2520of%2520both%2520conventional%2520accuracy%2520and%2520uncertainty%2520accuracy%250Awhile%2520achieving%2520significantly%2520better%2520calibration.%2520These%2520results%2520highlight%2520the%250Apotential%2520of%2520our%2520approach%2520to%2520enhance%2520the%2520trustworthiness%2520of%2520deep%2520learning%250Amodels%2520in%2520safety-critical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15671v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Monte%20Carlo%20Dropout%20Performance%20for%20Uncertainty%20Quantification&entry.906535625=Hamzeh%20Asgharnezhad%20and%20Afshar%20Shamsi%20and%20Roohallah%20Alizadehsani%20and%20Arash%20Mohammadi%20and%20Hamid%20Alinejad-Rokny&entry.1292438233=%20%20Knowing%20the%20uncertainty%20associated%20with%20the%20output%20of%20a%20deep%20neural%20network%0Ais%20of%20paramount%20importance%20in%20making%20trustworthy%20decisions%2C%20particularly%20in%0Ahigh-stakes%20fields%20like%20medical%20diagnosis%20and%20autonomous%20systems.%20Monte%20Carlo%0ADropout%20%28MCD%29%20is%20a%20widely%20used%20method%20for%20uncertainty%20quantification%2C%20as%20it%20can%0Abe%20easily%20integrated%20into%20various%20deep%20architectures.%20However%2C%20conventional%20MCD%0Aoften%20struggles%20with%20providing%20well-calibrated%20uncertainty%20estimates.%20To%0Aaddress%20this%2C%20we%20introduce%20innovative%20frameworks%20that%20enhances%20MCD%20by%0Aintegrating%20different%20search%20solutions%20namely%20Grey%20Wolf%20Optimizer%20%28GWO%29%2C%0ABayesian%20Optimization%20%28BO%29%2C%20and%20Particle%20Swarm%20Optimization%20%28PSO%29%20as%20well%20as%20an%0Auncertainty-aware%20loss%20function%2C%20thereby%20improving%20the%20reliability%20of%0Auncertainty%20quantification.%20We%20conduct%20comprehensive%20experiments%20using%0Adifferent%20backbones%2C%20namely%20DenseNet121%2C%20ResNet50%2C%20and%20VGG16%2C%20on%20various%0Adatasets%2C%20including%20Cats%20vs.%20Dogs%2C%20Myocarditis%2C%20Wisconsin%2C%20and%20a%20synthetic%0Adataset%20%28Circles%29.%20Our%20proposed%20algorithm%20outperforms%20the%20MCD%20baseline%20by%202-3%25%0Aon%20average%20in%20terms%20of%20both%20conventional%20accuracy%20and%20uncertainty%20accuracy%0Awhile%20achieving%20significantly%20better%20calibration.%20These%20results%20highlight%20the%0Apotential%20of%20our%20approach%20to%20enhance%20the%20trustworthiness%20of%20deep%20learning%0Amodels%20in%20safety-critical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15671v1&entry.124074799=Read"},
{"title": "Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot\n  Manipulation Datasets", "author": "Kaiyuan Chen and Shuangyu Xie and Zehan Ma and Ken Goldberg", "abstract": "  Vision-Language Models (VLMs) acquire real-world knowledge and general\nreasoning ability through Internet-scale image-text corpora. They can augment\nrobotic systems with scene understanding and task planning, and assist\nvisuomotor policies that are trained on robot trajectory data. We explore the\nreverse paradigm - using rich, real, multi-modal robot trajectory data to\nenhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual\nQuestion Answering (VQA) dataset generation framework for VLMs. Given a human\ntele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual\nand non-descriptive sensory modalities, such as end-effector pose, gripper\naperture, and force sensing. Based on these modalities, it segments the robot\ntrajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses\nscene and interaction understanding to identify 3D properties of the robot,\ntask goal, and the target object. The properties are used to generate\nrepresentative VQA queries - images with textural multiple-choice questions -\nbased on spatial, goal-conditioned, and interaction reasoning question\ntemplates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710\nquestions covering 463 distinct scenes and 3,396 robotic manipulation tasks\nfrom 176k real robot trajectories. Results suggest that Robo2VLM-1 can\nbenchmark and improve VLM capabilities in spatial and interaction reasoning.\n", "link": "http://arxiv.org/abs/2505.15517v1", "date": "2025-05-21", "relevancy": 2.2234, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6144}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5441}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robo2VLM%3A%20Visual%20Question%20Answering%20from%20Large-Scale%20In-the-Wild%20Robot%0A%20%20Manipulation%20Datasets&body=Title%3A%20Robo2VLM%3A%20Visual%20Question%20Answering%20from%20Large-Scale%20In-the-Wild%20Robot%0A%20%20Manipulation%20Datasets%0AAuthor%3A%20Kaiyuan%20Chen%20and%20Shuangyu%20Xie%20and%20Zehan%20Ma%20and%20Ken%20Goldberg%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20acquire%20real-world%20knowledge%20and%20general%0Areasoning%20ability%20through%20Internet-scale%20image-text%20corpora.%20They%20can%20augment%0Arobotic%20systems%20with%20scene%20understanding%20and%20task%20planning%2C%20and%20assist%0Avisuomotor%20policies%20that%20are%20trained%20on%20robot%20trajectory%20data.%20We%20explore%20the%0Areverse%20paradigm%20-%20using%20rich%2C%20real%2C%20multi-modal%20robot%20trajectory%20data%20to%0Aenhance%20and%20evaluate%20VLMs.%20In%20this%20paper%2C%20we%20present%20Robo2VLM%2C%20a%20Visual%0AQuestion%20Answering%20%28VQA%29%20dataset%20generation%20framework%20for%20VLMs.%20Given%20a%20human%0Atele-operated%20robot%20trajectory%2C%20Robo2VLM%20derives%20ground-truth%20from%20non-visual%0Aand%20non-descriptive%20sensory%20modalities%2C%20such%20as%20end-effector%20pose%2C%20gripper%0Aaperture%2C%20and%20force%20sensing.%20Based%20on%20these%20modalities%2C%20it%20segments%20the%20robot%0Atrajectory%20into%20a%20sequence%20of%20manipulation%20phases.%20At%20each%20phase%2C%20Robo2VLM%20uses%0Ascene%20and%20interaction%20understanding%20to%20identify%203D%20properties%20of%20the%20robot%2C%0Atask%20goal%2C%20and%20the%20target%20object.%20The%20properties%20are%20used%20to%20generate%0Arepresentative%20VQA%20queries%20-%20images%20with%20textural%20multiple-choice%20questions%20-%0Abased%20on%20spatial%2C%20goal-conditioned%2C%20and%20interaction%20reasoning%20question%0Atemplates.%20We%20curate%20Robo2VLM-1%2C%20a%20large-scale%20in-the-wild%20dataset%20with%20684%2C710%0Aquestions%20covering%20463%20distinct%20scenes%20and%203%2C396%20robotic%20manipulation%20tasks%0Afrom%20176k%20real%20robot%20trajectories.%20Results%20suggest%20that%20Robo2VLM-1%20can%0Abenchmark%20and%20improve%20VLM%20capabilities%20in%20spatial%20and%20interaction%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15517v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobo2VLM%253A%2520Visual%2520Question%2520Answering%2520from%2520Large-Scale%2520In-the-Wild%2520Robot%250A%2520%2520Manipulation%2520Datasets%26entry.906535625%3DKaiyuan%2520Chen%2520and%2520Shuangyu%2520Xie%2520and%2520Zehan%2520Ma%2520and%2520Ken%2520Goldberg%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520acquire%2520real-world%2520knowledge%2520and%2520general%250Areasoning%2520ability%2520through%2520Internet-scale%2520image-text%2520corpora.%2520They%2520can%2520augment%250Arobotic%2520systems%2520with%2520scene%2520understanding%2520and%2520task%2520planning%252C%2520and%2520assist%250Avisuomotor%2520policies%2520that%2520are%2520trained%2520on%2520robot%2520trajectory%2520data.%2520We%2520explore%2520the%250Areverse%2520paradigm%2520-%2520using%2520rich%252C%2520real%252C%2520multi-modal%2520robot%2520trajectory%2520data%2520to%250Aenhance%2520and%2520evaluate%2520VLMs.%2520In%2520this%2520paper%252C%2520we%2520present%2520Robo2VLM%252C%2520a%2520Visual%250AQuestion%2520Answering%2520%2528VQA%2529%2520dataset%2520generation%2520framework%2520for%2520VLMs.%2520Given%2520a%2520human%250Atele-operated%2520robot%2520trajectory%252C%2520Robo2VLM%2520derives%2520ground-truth%2520from%2520non-visual%250Aand%2520non-descriptive%2520sensory%2520modalities%252C%2520such%2520as%2520end-effector%2520pose%252C%2520gripper%250Aaperture%252C%2520and%2520force%2520sensing.%2520Based%2520on%2520these%2520modalities%252C%2520it%2520segments%2520the%2520robot%250Atrajectory%2520into%2520a%2520sequence%2520of%2520manipulation%2520phases.%2520At%2520each%2520phase%252C%2520Robo2VLM%2520uses%250Ascene%2520and%2520interaction%2520understanding%2520to%2520identify%25203D%2520properties%2520of%2520the%2520robot%252C%250Atask%2520goal%252C%2520and%2520the%2520target%2520object.%2520The%2520properties%2520are%2520used%2520to%2520generate%250Arepresentative%2520VQA%2520queries%2520-%2520images%2520with%2520textural%2520multiple-choice%2520questions%2520-%250Abased%2520on%2520spatial%252C%2520goal-conditioned%252C%2520and%2520interaction%2520reasoning%2520question%250Atemplates.%2520We%2520curate%2520Robo2VLM-1%252C%2520a%2520large-scale%2520in-the-wild%2520dataset%2520with%2520684%252C710%250Aquestions%2520covering%2520463%2520distinct%2520scenes%2520and%25203%252C396%2520robotic%2520manipulation%2520tasks%250Afrom%2520176k%2520real%2520robot%2520trajectories.%2520Results%2520suggest%2520that%2520Robo2VLM-1%2520can%250Abenchmark%2520and%2520improve%2520VLM%2520capabilities%2520in%2520spatial%2520and%2520interaction%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15517v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robo2VLM%3A%20Visual%20Question%20Answering%20from%20Large-Scale%20In-the-Wild%20Robot%0A%20%20Manipulation%20Datasets&entry.906535625=Kaiyuan%20Chen%20and%20Shuangyu%20Xie%20and%20Zehan%20Ma%20and%20Ken%20Goldberg&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20acquire%20real-world%20knowledge%20and%20general%0Areasoning%20ability%20through%20Internet-scale%20image-text%20corpora.%20They%20can%20augment%0Arobotic%20systems%20with%20scene%20understanding%20and%20task%20planning%2C%20and%20assist%0Avisuomotor%20policies%20that%20are%20trained%20on%20robot%20trajectory%20data.%20We%20explore%20the%0Areverse%20paradigm%20-%20using%20rich%2C%20real%2C%20multi-modal%20robot%20trajectory%20data%20to%0Aenhance%20and%20evaluate%20VLMs.%20In%20this%20paper%2C%20we%20present%20Robo2VLM%2C%20a%20Visual%0AQuestion%20Answering%20%28VQA%29%20dataset%20generation%20framework%20for%20VLMs.%20Given%20a%20human%0Atele-operated%20robot%20trajectory%2C%20Robo2VLM%20derives%20ground-truth%20from%20non-visual%0Aand%20non-descriptive%20sensory%20modalities%2C%20such%20as%20end-effector%20pose%2C%20gripper%0Aaperture%2C%20and%20force%20sensing.%20Based%20on%20these%20modalities%2C%20it%20segments%20the%20robot%0Atrajectory%20into%20a%20sequence%20of%20manipulation%20phases.%20At%20each%20phase%2C%20Robo2VLM%20uses%0Ascene%20and%20interaction%20understanding%20to%20identify%203D%20properties%20of%20the%20robot%2C%0Atask%20goal%2C%20and%20the%20target%20object.%20The%20properties%20are%20used%20to%20generate%0Arepresentative%20VQA%20queries%20-%20images%20with%20textural%20multiple-choice%20questions%20-%0Abased%20on%20spatial%2C%20goal-conditioned%2C%20and%20interaction%20reasoning%20question%0Atemplates.%20We%20curate%20Robo2VLM-1%2C%20a%20large-scale%20in-the-wild%20dataset%20with%20684%2C710%0Aquestions%20covering%20463%20distinct%20scenes%20and%203%2C396%20robotic%20manipulation%20tasks%0Afrom%20176k%20real%20robot%20trajectories.%20Results%20suggest%20that%20Robo2VLM-1%20can%0Abenchmark%20and%20improve%20VLM%20capabilities%20in%20spatial%20and%20interaction%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15517v1&entry.124074799=Read"},
{"title": "RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation\n  and Language Generation for Explainable QA Hallucination Detection", "author": "Yiming Huang and Junyan Zhang and Zihao Wang and Biquan Bie and Xuming Hu and Yi R. and  Fung and Xinlei He", "abstract": "  Large Language Models (LLMs) have become powerful, but hallucinations remain\na vital obstacle to their trustworthy use. While previous works improved the\ncapability of hallucination detection by measuring uncertainty, they all lack\nthe ability to explain the provenance behind why hallucinations occur, i.e.,\nwhich part of the inputs tends to trigger hallucinations. Recent works on the\nprompt attack indicate that uncertainty exists in semantic propagation, where\nattention mechanisms gradually fuse local token information into high-level\nsemantics across layers. Meanwhile, uncertainty also emerges in language\ngeneration, due to its probability-based selection of high-level semantics for\nsampled generations. Based on that, we propose RePPL to recalibrate uncertainty\nmeasurement by these two aspects, which dispatches explainable uncertainty\nscores to each token and aggregates in Perplexity-style Log-Average form as\ntotal score. Experiments show that our method achieves the best comprehensive\ndetection performance across various QA datasets on advanced models (average\nAUC of 0.833), and our method is capable of producing token-level uncertainty\nscores as explanations for the hallucination. Leveraging these scores, we\npreliminarily find the chaotic pattern of hallucination and showcase its\npromising usage.\n", "link": "http://arxiv.org/abs/2505.15386v1", "date": "2025-05-21", "relevancy": 2.22, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5872}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5665}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RePPL%3A%20Recalibrating%20Perplexity%20by%20Uncertainty%20in%20Semantic%20Propagation%0A%20%20and%20Language%20Generation%20for%20Explainable%20QA%20Hallucination%20Detection&body=Title%3A%20RePPL%3A%20Recalibrating%20Perplexity%20by%20Uncertainty%20in%20Semantic%20Propagation%0A%20%20and%20Language%20Generation%20for%20Explainable%20QA%20Hallucination%20Detection%0AAuthor%3A%20Yiming%20Huang%20and%20Junyan%20Zhang%20and%20Zihao%20Wang%20and%20Biquan%20Bie%20and%20Xuming%20Hu%20and%20Yi%20R.%20and%20%20Fung%20and%20Xinlei%20He%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20powerful%2C%20but%20hallucinations%20remain%0Aa%20vital%20obstacle%20to%20their%20trustworthy%20use.%20While%20previous%20works%20improved%20the%0Acapability%20of%20hallucination%20detection%20by%20measuring%20uncertainty%2C%20they%20all%20lack%0Athe%20ability%20to%20explain%20the%20provenance%20behind%20why%20hallucinations%20occur%2C%20i.e.%2C%0Awhich%20part%20of%20the%20inputs%20tends%20to%20trigger%20hallucinations.%20Recent%20works%20on%20the%0Aprompt%20attack%20indicate%20that%20uncertainty%20exists%20in%20semantic%20propagation%2C%20where%0Aattention%20mechanisms%20gradually%20fuse%20local%20token%20information%20into%20high-level%0Asemantics%20across%20layers.%20Meanwhile%2C%20uncertainty%20also%20emerges%20in%20language%0Ageneration%2C%20due%20to%20its%20probability-based%20selection%20of%20high-level%20semantics%20for%0Asampled%20generations.%20Based%20on%20that%2C%20we%20propose%20RePPL%20to%20recalibrate%20uncertainty%0Ameasurement%20by%20these%20two%20aspects%2C%20which%20dispatches%20explainable%20uncertainty%0Ascores%20to%20each%20token%20and%20aggregates%20in%20Perplexity-style%20Log-Average%20form%20as%0Atotal%20score.%20Experiments%20show%20that%20our%20method%20achieves%20the%20best%20comprehensive%0Adetection%20performance%20across%20various%20QA%20datasets%20on%20advanced%20models%20%28average%0AAUC%20of%200.833%29%2C%20and%20our%20method%20is%20capable%20of%20producing%20token-level%20uncertainty%0Ascores%20as%20explanations%20for%20the%20hallucination.%20Leveraging%20these%20scores%2C%20we%0Apreliminarily%20find%20the%20chaotic%20pattern%20of%20hallucination%20and%20showcase%20its%0Apromising%20usage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15386v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRePPL%253A%2520Recalibrating%2520Perplexity%2520by%2520Uncertainty%2520in%2520Semantic%2520Propagation%250A%2520%2520and%2520Language%2520Generation%2520for%2520Explainable%2520QA%2520Hallucination%2520Detection%26entry.906535625%3DYiming%2520Huang%2520and%2520Junyan%2520Zhang%2520and%2520Zihao%2520Wang%2520and%2520Biquan%2520Bie%2520and%2520Xuming%2520Hu%2520and%2520Yi%2520R.%2520and%2520%2520Fung%2520and%2520Xinlei%2520He%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520become%2520powerful%252C%2520but%2520hallucinations%2520remain%250Aa%2520vital%2520obstacle%2520to%2520their%2520trustworthy%2520use.%2520While%2520previous%2520works%2520improved%2520the%250Acapability%2520of%2520hallucination%2520detection%2520by%2520measuring%2520uncertainty%252C%2520they%2520all%2520lack%250Athe%2520ability%2520to%2520explain%2520the%2520provenance%2520behind%2520why%2520hallucinations%2520occur%252C%2520i.e.%252C%250Awhich%2520part%2520of%2520the%2520inputs%2520tends%2520to%2520trigger%2520hallucinations.%2520Recent%2520works%2520on%2520the%250Aprompt%2520attack%2520indicate%2520that%2520uncertainty%2520exists%2520in%2520semantic%2520propagation%252C%2520where%250Aattention%2520mechanisms%2520gradually%2520fuse%2520local%2520token%2520information%2520into%2520high-level%250Asemantics%2520across%2520layers.%2520Meanwhile%252C%2520uncertainty%2520also%2520emerges%2520in%2520language%250Ageneration%252C%2520due%2520to%2520its%2520probability-based%2520selection%2520of%2520high-level%2520semantics%2520for%250Asampled%2520generations.%2520Based%2520on%2520that%252C%2520we%2520propose%2520RePPL%2520to%2520recalibrate%2520uncertainty%250Ameasurement%2520by%2520these%2520two%2520aspects%252C%2520which%2520dispatches%2520explainable%2520uncertainty%250Ascores%2520to%2520each%2520token%2520and%2520aggregates%2520in%2520Perplexity-style%2520Log-Average%2520form%2520as%250Atotal%2520score.%2520Experiments%2520show%2520that%2520our%2520method%2520achieves%2520the%2520best%2520comprehensive%250Adetection%2520performance%2520across%2520various%2520QA%2520datasets%2520on%2520advanced%2520models%2520%2528average%250AAUC%2520of%25200.833%2529%252C%2520and%2520our%2520method%2520is%2520capable%2520of%2520producing%2520token-level%2520uncertainty%250Ascores%2520as%2520explanations%2520for%2520the%2520hallucination.%2520Leveraging%2520these%2520scores%252C%2520we%250Apreliminarily%2520find%2520the%2520chaotic%2520pattern%2520of%2520hallucination%2520and%2520showcase%2520its%250Apromising%2520usage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15386v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RePPL%3A%20Recalibrating%20Perplexity%20by%20Uncertainty%20in%20Semantic%20Propagation%0A%20%20and%20Language%20Generation%20for%20Explainable%20QA%20Hallucination%20Detection&entry.906535625=Yiming%20Huang%20and%20Junyan%20Zhang%20and%20Zihao%20Wang%20and%20Biquan%20Bie%20and%20Xuming%20Hu%20and%20Yi%20R.%20and%20%20Fung%20and%20Xinlei%20He&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20powerful%2C%20but%20hallucinations%20remain%0Aa%20vital%20obstacle%20to%20their%20trustworthy%20use.%20While%20previous%20works%20improved%20the%0Acapability%20of%20hallucination%20detection%20by%20measuring%20uncertainty%2C%20they%20all%20lack%0Athe%20ability%20to%20explain%20the%20provenance%20behind%20why%20hallucinations%20occur%2C%20i.e.%2C%0Awhich%20part%20of%20the%20inputs%20tends%20to%20trigger%20hallucinations.%20Recent%20works%20on%20the%0Aprompt%20attack%20indicate%20that%20uncertainty%20exists%20in%20semantic%20propagation%2C%20where%0Aattention%20mechanisms%20gradually%20fuse%20local%20token%20information%20into%20high-level%0Asemantics%20across%20layers.%20Meanwhile%2C%20uncertainty%20also%20emerges%20in%20language%0Ageneration%2C%20due%20to%20its%20probability-based%20selection%20of%20high-level%20semantics%20for%0Asampled%20generations.%20Based%20on%20that%2C%20we%20propose%20RePPL%20to%20recalibrate%20uncertainty%0Ameasurement%20by%20these%20two%20aspects%2C%20which%20dispatches%20explainable%20uncertainty%0Ascores%20to%20each%20token%20and%20aggregates%20in%20Perplexity-style%20Log-Average%20form%20as%0Atotal%20score.%20Experiments%20show%20that%20our%20method%20achieves%20the%20best%20comprehensive%0Adetection%20performance%20across%20various%20QA%20datasets%20on%20advanced%20models%20%28average%0AAUC%20of%200.833%29%2C%20and%20our%20method%20is%20capable%20of%20producing%20token-level%20uncertainty%0Ascores%20as%20explanations%20for%20the%20hallucination.%20Leveraging%20these%20scores%2C%20we%0Apreliminarily%20find%20the%20chaotic%20pattern%20of%20hallucination%20and%20showcase%20its%0Apromising%20usage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15386v1&entry.124074799=Read"},
{"title": "Robust Multimodal Learning via Entropy-Gated Contrastive Fusion", "author": "Leon Chlon and Maggie Chlon and MarcAntonio M. Awada", "abstract": "  Real-world multimodal systems routinely face missing-input scenarios, and in\nreality, robots lose audio in a factory or a clinical record omits lab tests at\ninference time. Standard fusion layers either preserve robustness or\ncalibration but never both. We introduce Adaptive Entropy-Gated Contrastive\nFusion (AECF), a single light-weight layer that (i) adapts its entropy\ncoefficient per instance, (ii) enforces monotone calibration across all\nmodality subsets, and (iii) drives a curriculum mask directly from\ntraining-time entropy. On AV-MNIST and MS-COCO, AECF improves masked-input mAP\nby +18 pp at a 50% drop rate while reducing ECE by up to 200%, yet adds 1%\nrun-time. All back-bones remain frozen, making AECF an easy drop-in layer for\nrobust, calibrated multimodal inference.\n", "link": "http://arxiv.org/abs/2505.15417v1", "date": "2025-05-21", "relevancy": 2.2024, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5575}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5531}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Multimodal%20Learning%20via%20Entropy-Gated%20Contrastive%20Fusion&body=Title%3A%20Robust%20Multimodal%20Learning%20via%20Entropy-Gated%20Contrastive%20Fusion%0AAuthor%3A%20Leon%20Chlon%20and%20Maggie%20Chlon%20and%20MarcAntonio%20M.%20Awada%0AAbstract%3A%20%20%20Real-world%20multimodal%20systems%20routinely%20face%20missing-input%20scenarios%2C%20and%20in%0Areality%2C%20robots%20lose%20audio%20in%20a%20factory%20or%20a%20clinical%20record%20omits%20lab%20tests%20at%0Ainference%20time.%20Standard%20fusion%20layers%20either%20preserve%20robustness%20or%0Acalibration%20but%20never%20both.%20We%20introduce%20Adaptive%20Entropy-Gated%20Contrastive%0AFusion%20%28AECF%29%2C%20a%20single%20light-weight%20layer%20that%20%28i%29%20adapts%20its%20entropy%0Acoefficient%20per%20instance%2C%20%28ii%29%20enforces%20monotone%20calibration%20across%20all%0Amodality%20subsets%2C%20and%20%28iii%29%20drives%20a%20curriculum%20mask%20directly%20from%0Atraining-time%20entropy.%20On%20AV-MNIST%20and%20MS-COCO%2C%20AECF%20improves%20masked-input%20mAP%0Aby%20%2B18%20pp%20at%20a%2050%25%20drop%20rate%20while%20reducing%20ECE%20by%20up%20to%20200%25%2C%20yet%20adds%201%25%0Arun-time.%20All%20back-bones%20remain%20frozen%2C%20making%20AECF%20an%20easy%20drop-in%20layer%20for%0Arobust%2C%20calibrated%20multimodal%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15417v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Multimodal%2520Learning%2520via%2520Entropy-Gated%2520Contrastive%2520Fusion%26entry.906535625%3DLeon%2520Chlon%2520and%2520Maggie%2520Chlon%2520and%2520MarcAntonio%2520M.%2520Awada%26entry.1292438233%3D%2520%2520Real-world%2520multimodal%2520systems%2520routinely%2520face%2520missing-input%2520scenarios%252C%2520and%2520in%250Areality%252C%2520robots%2520lose%2520audio%2520in%2520a%2520factory%2520or%2520a%2520clinical%2520record%2520omits%2520lab%2520tests%2520at%250Ainference%2520time.%2520Standard%2520fusion%2520layers%2520either%2520preserve%2520robustness%2520or%250Acalibration%2520but%2520never%2520both.%2520We%2520introduce%2520Adaptive%2520Entropy-Gated%2520Contrastive%250AFusion%2520%2528AECF%2529%252C%2520a%2520single%2520light-weight%2520layer%2520that%2520%2528i%2529%2520adapts%2520its%2520entropy%250Acoefficient%2520per%2520instance%252C%2520%2528ii%2529%2520enforces%2520monotone%2520calibration%2520across%2520all%250Amodality%2520subsets%252C%2520and%2520%2528iii%2529%2520drives%2520a%2520curriculum%2520mask%2520directly%2520from%250Atraining-time%2520entropy.%2520On%2520AV-MNIST%2520and%2520MS-COCO%252C%2520AECF%2520improves%2520masked-input%2520mAP%250Aby%2520%252B18%2520pp%2520at%2520a%252050%2525%2520drop%2520rate%2520while%2520reducing%2520ECE%2520by%2520up%2520to%2520200%2525%252C%2520yet%2520adds%25201%2525%250Arun-time.%2520All%2520back-bones%2520remain%2520frozen%252C%2520making%2520AECF%2520an%2520easy%2520drop-in%2520layer%2520for%250Arobust%252C%2520calibrated%2520multimodal%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15417v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Multimodal%20Learning%20via%20Entropy-Gated%20Contrastive%20Fusion&entry.906535625=Leon%20Chlon%20and%20Maggie%20Chlon%20and%20MarcAntonio%20M.%20Awada&entry.1292438233=%20%20Real-world%20multimodal%20systems%20routinely%20face%20missing-input%20scenarios%2C%20and%20in%0Areality%2C%20robots%20lose%20audio%20in%20a%20factory%20or%20a%20clinical%20record%20omits%20lab%20tests%20at%0Ainference%20time.%20Standard%20fusion%20layers%20either%20preserve%20robustness%20or%0Acalibration%20but%20never%20both.%20We%20introduce%20Adaptive%20Entropy-Gated%20Contrastive%0AFusion%20%28AECF%29%2C%20a%20single%20light-weight%20layer%20that%20%28i%29%20adapts%20its%20entropy%0Acoefficient%20per%20instance%2C%20%28ii%29%20enforces%20monotone%20calibration%20across%20all%0Amodality%20subsets%2C%20and%20%28iii%29%20drives%20a%20curriculum%20mask%20directly%20from%0Atraining-time%20entropy.%20On%20AV-MNIST%20and%20MS-COCO%2C%20AECF%20improves%20masked-input%20mAP%0Aby%20%2B18%20pp%20at%20a%2050%25%20drop%20rate%20while%20reducing%20ECE%20by%20up%20to%20200%25%2C%20yet%20adds%201%25%0Arun-time.%20All%20back-bones%20remain%20frozen%2C%20making%20AECF%20an%20easy%20drop-in%20layer%20for%0Arobust%2C%20calibrated%20multimodal%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15417v1&entry.124074799=Read"},
{"title": "Efficient Deep Learning with Decorrelated Backpropagation", "author": "Sander Dalm and Joshua Offergeld and Nasir Ahmad and Marcel van Gerven", "abstract": "  The backpropagation algorithm remains the dominant and most successful method\nfor training deep neural networks (DNNs). At the same time, training DNNs at\nscale comes at a significant computational cost and therefore a high carbon\nfootprint. Converging evidence suggests that input decorrelation may speed up\ndeep learning. However, to date, this has not yet translated into substantial\nimprovements in training efficiency in large-scale DNNs. This is mainly caused\nby the challenge of enforcing fast and stable network-wide decorrelation. Here,\nwe show for the first time that much more efficient training of deep\nconvolutional neural networks is feasible by embracing decorrelated\nbackpropagation as a mechanism for learning. To achieve this goal we made use\nof a novel algorithm which induces network-wide input decorrelation using\nminimal computational overhead. By combining this algorithm with careful\noptimizations, we achieve a more than two-fold speed-up and higher test\naccuracy compared to backpropagation when training several deep networks up to\na 50-layer ResNet model. This demonstrates that decorrelation provides exciting\nprospects for efficient deep learning at scale.\n", "link": "http://arxiv.org/abs/2405.02385v4", "date": "2025-05-21", "relevancy": 2.1806, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.569}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5292}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Deep%20Learning%20with%20Decorrelated%20Backpropagation&body=Title%3A%20Efficient%20Deep%20Learning%20with%20Decorrelated%20Backpropagation%0AAuthor%3A%20Sander%20Dalm%20and%20Joshua%20Offergeld%20and%20Nasir%20Ahmad%20and%20Marcel%20van%20Gerven%0AAbstract%3A%20%20%20The%20backpropagation%20algorithm%20remains%20the%20dominant%20and%20most%20successful%20method%0Afor%20training%20deep%20neural%20networks%20%28DNNs%29.%20At%20the%20same%20time%2C%20training%20DNNs%20at%0Ascale%20comes%20at%20a%20significant%20computational%20cost%20and%20therefore%20a%20high%20carbon%0Afootprint.%20Converging%20evidence%20suggests%20that%20input%20decorrelation%20may%20speed%20up%0Adeep%20learning.%20However%2C%20to%20date%2C%20this%20has%20not%20yet%20translated%20into%20substantial%0Aimprovements%20in%20training%20efficiency%20in%20large-scale%20DNNs.%20This%20is%20mainly%20caused%0Aby%20the%20challenge%20of%20enforcing%20fast%20and%20stable%20network-wide%20decorrelation.%20Here%2C%0Awe%20show%20for%20the%20first%20time%20that%20much%20more%20efficient%20training%20of%20deep%0Aconvolutional%20neural%20networks%20is%20feasible%20by%20embracing%20decorrelated%0Abackpropagation%20as%20a%20mechanism%20for%20learning.%20To%20achieve%20this%20goal%20we%20made%20use%0Aof%20a%20novel%20algorithm%20which%20induces%20network-wide%20input%20decorrelation%20using%0Aminimal%20computational%20overhead.%20By%20combining%20this%20algorithm%20with%20careful%0Aoptimizations%2C%20we%20achieve%20a%20more%20than%20two-fold%20speed-up%20and%20higher%20test%0Aaccuracy%20compared%20to%20backpropagation%20when%20training%20several%20deep%20networks%20up%20to%0Aa%2050-layer%20ResNet%20model.%20This%20demonstrates%20that%20decorrelation%20provides%20exciting%0Aprospects%20for%20efficient%20deep%20learning%20at%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02385v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Deep%2520Learning%2520with%2520Decorrelated%2520Backpropagation%26entry.906535625%3DSander%2520Dalm%2520and%2520Joshua%2520Offergeld%2520and%2520Nasir%2520Ahmad%2520and%2520Marcel%2520van%2520Gerven%26entry.1292438233%3D%2520%2520The%2520backpropagation%2520algorithm%2520remains%2520the%2520dominant%2520and%2520most%2520successful%2520method%250Afor%2520training%2520deep%2520neural%2520networks%2520%2528DNNs%2529.%2520At%2520the%2520same%2520time%252C%2520training%2520DNNs%2520at%250Ascale%2520comes%2520at%2520a%2520significant%2520computational%2520cost%2520and%2520therefore%2520a%2520high%2520carbon%250Afootprint.%2520Converging%2520evidence%2520suggests%2520that%2520input%2520decorrelation%2520may%2520speed%2520up%250Adeep%2520learning.%2520However%252C%2520to%2520date%252C%2520this%2520has%2520not%2520yet%2520translated%2520into%2520substantial%250Aimprovements%2520in%2520training%2520efficiency%2520in%2520large-scale%2520DNNs.%2520This%2520is%2520mainly%2520caused%250Aby%2520the%2520challenge%2520of%2520enforcing%2520fast%2520and%2520stable%2520network-wide%2520decorrelation.%2520Here%252C%250Awe%2520show%2520for%2520the%2520first%2520time%2520that%2520much%2520more%2520efficient%2520training%2520of%2520deep%250Aconvolutional%2520neural%2520networks%2520is%2520feasible%2520by%2520embracing%2520decorrelated%250Abackpropagation%2520as%2520a%2520mechanism%2520for%2520learning.%2520To%2520achieve%2520this%2520goal%2520we%2520made%2520use%250Aof%2520a%2520novel%2520algorithm%2520which%2520induces%2520network-wide%2520input%2520decorrelation%2520using%250Aminimal%2520computational%2520overhead.%2520By%2520combining%2520this%2520algorithm%2520with%2520careful%250Aoptimizations%252C%2520we%2520achieve%2520a%2520more%2520than%2520two-fold%2520speed-up%2520and%2520higher%2520test%250Aaccuracy%2520compared%2520to%2520backpropagation%2520when%2520training%2520several%2520deep%2520networks%2520up%2520to%250Aa%252050-layer%2520ResNet%2520model.%2520This%2520demonstrates%2520that%2520decorrelation%2520provides%2520exciting%250Aprospects%2520for%2520efficient%2520deep%2520learning%2520at%2520scale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02385v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Deep%20Learning%20with%20Decorrelated%20Backpropagation&entry.906535625=Sander%20Dalm%20and%20Joshua%20Offergeld%20and%20Nasir%20Ahmad%20and%20Marcel%20van%20Gerven&entry.1292438233=%20%20The%20backpropagation%20algorithm%20remains%20the%20dominant%20and%20most%20successful%20method%0Afor%20training%20deep%20neural%20networks%20%28DNNs%29.%20At%20the%20same%20time%2C%20training%20DNNs%20at%0Ascale%20comes%20at%20a%20significant%20computational%20cost%20and%20therefore%20a%20high%20carbon%0Afootprint.%20Converging%20evidence%20suggests%20that%20input%20decorrelation%20may%20speed%20up%0Adeep%20learning.%20However%2C%20to%20date%2C%20this%20has%20not%20yet%20translated%20into%20substantial%0Aimprovements%20in%20training%20efficiency%20in%20large-scale%20DNNs.%20This%20is%20mainly%20caused%0Aby%20the%20challenge%20of%20enforcing%20fast%20and%20stable%20network-wide%20decorrelation.%20Here%2C%0Awe%20show%20for%20the%20first%20time%20that%20much%20more%20efficient%20training%20of%20deep%0Aconvolutional%20neural%20networks%20is%20feasible%20by%20embracing%20decorrelated%0Abackpropagation%20as%20a%20mechanism%20for%20learning.%20To%20achieve%20this%20goal%20we%20made%20use%0Aof%20a%20novel%20algorithm%20which%20induces%20network-wide%20input%20decorrelation%20using%0Aminimal%20computational%20overhead.%20By%20combining%20this%20algorithm%20with%20careful%0Aoptimizations%2C%20we%20achieve%20a%20more%20than%20two-fold%20speed-up%20and%20higher%20test%0Aaccuracy%20compared%20to%20backpropagation%20when%20training%20several%20deep%20networks%20up%20to%0Aa%2050-layer%20ResNet%20model.%20This%20demonstrates%20that%20decorrelation%20provides%20exciting%0Aprospects%20for%20efficient%20deep%20learning%20at%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02385v4&entry.124074799=Read"},
{"title": "TimeCausality: Evaluating the Causal Ability in Time Dimension for\n  Vision Language Models", "author": "Zeqing Wang and Shiyuan Zhang and Chengpei Tang and Keze Wang", "abstract": "  Reasoning about temporal causality, particularly irreversible transformations\nof objects governed by real-world knowledge (e.g., fruit decay and human\naging), is a fundamental aspect of human visual understanding. Unlike temporal\nperception based on simple event sequences, this form of reasoning requires a\ndeeper comprehension of how object states change over time. Although the\ncurrent powerful Vision-Language Models (VLMs) have demonstrated impressive\nperformance on a wide range of downstream tasks, their capacity to reason about\ntemporal causality remains underexplored. To address this gap, we introduce\n\\textbf{TimeCausality}, a novel benchmark specifically designed to evaluate the\ncausal reasoning ability of VLMs in the temporal dimension. Based on our\nTimeCausality, we find that while the current SOTA open-source VLMs have\nachieved performance levels comparable to closed-source models like GPT-4o on\nvarious standard visual question answering tasks, they fall significantly\nbehind on our benchmark compared with their closed-source competitors.\nFurthermore, even GPT-4o exhibits a marked drop in performance on TimeCausality\ncompared to its results on other tasks. These findings underscore the critical\nneed to incorporate temporal causality into the evaluation and development of\nVLMs, and they highlight an important challenge for the open-source VLM\ncommunity moving forward. Code and Data are available at\n\\href{https://github.com/Zeqing-Wang/TimeCausality }{TimeCausality}.\n", "link": "http://arxiv.org/abs/2505.15435v1", "date": "2025-05-21", "relevancy": 2.171, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5478}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5478}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TimeCausality%3A%20Evaluating%20the%20Causal%20Ability%20in%20Time%20Dimension%20for%0A%20%20Vision%20Language%20Models&body=Title%3A%20TimeCausality%3A%20Evaluating%20the%20Causal%20Ability%20in%20Time%20Dimension%20for%0A%20%20Vision%20Language%20Models%0AAuthor%3A%20Zeqing%20Wang%20and%20Shiyuan%20Zhang%20and%20Chengpei%20Tang%20and%20Keze%20Wang%0AAbstract%3A%20%20%20Reasoning%20about%20temporal%20causality%2C%20particularly%20irreversible%20transformations%0Aof%20objects%20governed%20by%20real-world%20knowledge%20%28e.g.%2C%20fruit%20decay%20and%20human%0Aaging%29%2C%20is%20a%20fundamental%20aspect%20of%20human%20visual%20understanding.%20Unlike%20temporal%0Aperception%20based%20on%20simple%20event%20sequences%2C%20this%20form%20of%20reasoning%20requires%20a%0Adeeper%20comprehension%20of%20how%20object%20states%20change%20over%20time.%20Although%20the%0Acurrent%20powerful%20Vision-Language%20Models%20%28VLMs%29%20have%20demonstrated%20impressive%0Aperformance%20on%20a%20wide%20range%20of%20downstream%20tasks%2C%20their%20capacity%20to%20reason%20about%0Atemporal%20causality%20remains%20underexplored.%20To%20address%20this%20gap%2C%20we%20introduce%0A%5Ctextbf%7BTimeCausality%7D%2C%20a%20novel%20benchmark%20specifically%20designed%20to%20evaluate%20the%0Acausal%20reasoning%20ability%20of%20VLMs%20in%20the%20temporal%20dimension.%20Based%20on%20our%0ATimeCausality%2C%20we%20find%20that%20while%20the%20current%20SOTA%20open-source%20VLMs%20have%0Aachieved%20performance%20levels%20comparable%20to%20closed-source%20models%20like%20GPT-4o%20on%0Avarious%20standard%20visual%20question%20answering%20tasks%2C%20they%20fall%20significantly%0Abehind%20on%20our%20benchmark%20compared%20with%20their%20closed-source%20competitors.%0AFurthermore%2C%20even%20GPT-4o%20exhibits%20a%20marked%20drop%20in%20performance%20on%20TimeCausality%0Acompared%20to%20its%20results%20on%20other%20tasks.%20These%20findings%20underscore%20the%20critical%0Aneed%20to%20incorporate%20temporal%20causality%20into%20the%20evaluation%20and%20development%20of%0AVLMs%2C%20and%20they%20highlight%20an%20important%20challenge%20for%20the%20open-source%20VLM%0Acommunity%20moving%20forward.%20Code%20and%20Data%20are%20available%20at%0A%5Chref%7Bhttps%3A//github.com/Zeqing-Wang/TimeCausality%20%7D%7BTimeCausality%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15435v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimeCausality%253A%2520Evaluating%2520the%2520Causal%2520Ability%2520in%2520Time%2520Dimension%2520for%250A%2520%2520Vision%2520Language%2520Models%26entry.906535625%3DZeqing%2520Wang%2520and%2520Shiyuan%2520Zhang%2520and%2520Chengpei%2520Tang%2520and%2520Keze%2520Wang%26entry.1292438233%3D%2520%2520Reasoning%2520about%2520temporal%2520causality%252C%2520particularly%2520irreversible%2520transformations%250Aof%2520objects%2520governed%2520by%2520real-world%2520knowledge%2520%2528e.g.%252C%2520fruit%2520decay%2520and%2520human%250Aaging%2529%252C%2520is%2520a%2520fundamental%2520aspect%2520of%2520human%2520visual%2520understanding.%2520Unlike%2520temporal%250Aperception%2520based%2520on%2520simple%2520event%2520sequences%252C%2520this%2520form%2520of%2520reasoning%2520requires%2520a%250Adeeper%2520comprehension%2520of%2520how%2520object%2520states%2520change%2520over%2520time.%2520Although%2520the%250Acurrent%2520powerful%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520demonstrated%2520impressive%250Aperformance%2520on%2520a%2520wide%2520range%2520of%2520downstream%2520tasks%252C%2520their%2520capacity%2520to%2520reason%2520about%250Atemporal%2520causality%2520remains%2520underexplored.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%250A%255Ctextbf%257BTimeCausality%257D%252C%2520a%2520novel%2520benchmark%2520specifically%2520designed%2520to%2520evaluate%2520the%250Acausal%2520reasoning%2520ability%2520of%2520VLMs%2520in%2520the%2520temporal%2520dimension.%2520Based%2520on%2520our%250ATimeCausality%252C%2520we%2520find%2520that%2520while%2520the%2520current%2520SOTA%2520open-source%2520VLMs%2520have%250Aachieved%2520performance%2520levels%2520comparable%2520to%2520closed-source%2520models%2520like%2520GPT-4o%2520on%250Avarious%2520standard%2520visual%2520question%2520answering%2520tasks%252C%2520they%2520fall%2520significantly%250Abehind%2520on%2520our%2520benchmark%2520compared%2520with%2520their%2520closed-source%2520competitors.%250AFurthermore%252C%2520even%2520GPT-4o%2520exhibits%2520a%2520marked%2520drop%2520in%2520performance%2520on%2520TimeCausality%250Acompared%2520to%2520its%2520results%2520on%2520other%2520tasks.%2520These%2520findings%2520underscore%2520the%2520critical%250Aneed%2520to%2520incorporate%2520temporal%2520causality%2520into%2520the%2520evaluation%2520and%2520development%2520of%250AVLMs%252C%2520and%2520they%2520highlight%2520an%2520important%2520challenge%2520for%2520the%2520open-source%2520VLM%250Acommunity%2520moving%2520forward.%2520Code%2520and%2520Data%2520are%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/Zeqing-Wang/TimeCausality%2520%257D%257BTimeCausality%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15435v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TimeCausality%3A%20Evaluating%20the%20Causal%20Ability%20in%20Time%20Dimension%20for%0A%20%20Vision%20Language%20Models&entry.906535625=Zeqing%20Wang%20and%20Shiyuan%20Zhang%20and%20Chengpei%20Tang%20and%20Keze%20Wang&entry.1292438233=%20%20Reasoning%20about%20temporal%20causality%2C%20particularly%20irreversible%20transformations%0Aof%20objects%20governed%20by%20real-world%20knowledge%20%28e.g.%2C%20fruit%20decay%20and%20human%0Aaging%29%2C%20is%20a%20fundamental%20aspect%20of%20human%20visual%20understanding.%20Unlike%20temporal%0Aperception%20based%20on%20simple%20event%20sequences%2C%20this%20form%20of%20reasoning%20requires%20a%0Adeeper%20comprehension%20of%20how%20object%20states%20change%20over%20time.%20Although%20the%0Acurrent%20powerful%20Vision-Language%20Models%20%28VLMs%29%20have%20demonstrated%20impressive%0Aperformance%20on%20a%20wide%20range%20of%20downstream%20tasks%2C%20their%20capacity%20to%20reason%20about%0Atemporal%20causality%20remains%20underexplored.%20To%20address%20this%20gap%2C%20we%20introduce%0A%5Ctextbf%7BTimeCausality%7D%2C%20a%20novel%20benchmark%20specifically%20designed%20to%20evaluate%20the%0Acausal%20reasoning%20ability%20of%20VLMs%20in%20the%20temporal%20dimension.%20Based%20on%20our%0ATimeCausality%2C%20we%20find%20that%20while%20the%20current%20SOTA%20open-source%20VLMs%20have%0Aachieved%20performance%20levels%20comparable%20to%20closed-source%20models%20like%20GPT-4o%20on%0Avarious%20standard%20visual%20question%20answering%20tasks%2C%20they%20fall%20significantly%0Abehind%20on%20our%20benchmark%20compared%20with%20their%20closed-source%20competitors.%0AFurthermore%2C%20even%20GPT-4o%20exhibits%20a%20marked%20drop%20in%20performance%20on%20TimeCausality%0Acompared%20to%20its%20results%20on%20other%20tasks.%20These%20findings%20underscore%20the%20critical%0Aneed%20to%20incorporate%20temporal%20causality%20into%20the%20evaluation%20and%20development%20of%0AVLMs%2C%20and%20they%20highlight%20an%20important%20challenge%20for%20the%20open-source%20VLM%0Acommunity%20moving%20forward.%20Code%20and%20Data%20are%20available%20at%0A%5Chref%7Bhttps%3A//github.com/Zeqing-Wang/TimeCausality%20%7D%7BTimeCausality%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15435v1&entry.124074799=Read"},
{"title": "LCDB 1.1: A Database Illustrating Learning Curves Are More Ill-Behaved\n  Than Previously Thought", "author": "Cheng Yan and Felix Mohr and Tom Viering", "abstract": "  Sample-wise learning curves plot performance versus training set size. They\nare useful for studying scaling laws and speeding up hyperparameter tuning and\nmodel selection. Learning curves are often assumed to be well-behaved: monotone\n(i.e. improving with more data) and convex. By constructing the Learning Curves\nDatabase 1.1 (LCDB 1.1), a large-scale database with high-resolution learning\ncurves, we show that learning curves are less often well-behaved than\npreviously thought. Using statistically rigorous methods, we observe\nsignificant ill-behavior in approximately 14% of the learning curves, almost\ntwice as much as in previous estimates. We also identify which learners are to\nblame and show that specific learners are more ill-behaved than others.\nAdditionally, we demonstrate that different feature scalings rarely resolve\nill-behavior. We evaluate the impact of ill-behavior on downstream tasks, such\nas learning curve fitting and model selection, and find it poses significant\nchallenges, underscoring the relevance and potential of LCDB 1.1 as a\nchallenging benchmark for future research.\n", "link": "http://arxiv.org/abs/2505.15657v1", "date": "2025-05-21", "relevancy": 2.1705, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4415}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4304}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LCDB%201.1%3A%20A%20Database%20Illustrating%20Learning%20Curves%20Are%20More%20Ill-Behaved%0A%20%20Than%20Previously%20Thought&body=Title%3A%20LCDB%201.1%3A%20A%20Database%20Illustrating%20Learning%20Curves%20Are%20More%20Ill-Behaved%0A%20%20Than%20Previously%20Thought%0AAuthor%3A%20Cheng%20Yan%20and%20Felix%20Mohr%20and%20Tom%20Viering%0AAbstract%3A%20%20%20Sample-wise%20learning%20curves%20plot%20performance%20versus%20training%20set%20size.%20They%0Aare%20useful%20for%20studying%20scaling%20laws%20and%20speeding%20up%20hyperparameter%20tuning%20and%0Amodel%20selection.%20Learning%20curves%20are%20often%20assumed%20to%20be%20well-behaved%3A%20monotone%0A%28i.e.%20improving%20with%20more%20data%29%20and%20convex.%20By%20constructing%20the%20Learning%20Curves%0ADatabase%201.1%20%28LCDB%201.1%29%2C%20a%20large-scale%20database%20with%20high-resolution%20learning%0Acurves%2C%20we%20show%20that%20learning%20curves%20are%20less%20often%20well-behaved%20than%0Apreviously%20thought.%20Using%20statistically%20rigorous%20methods%2C%20we%20observe%0Asignificant%20ill-behavior%20in%20approximately%2014%25%20of%20the%20learning%20curves%2C%20almost%0Atwice%20as%20much%20as%20in%20previous%20estimates.%20We%20also%20identify%20which%20learners%20are%20to%0Ablame%20and%20show%20that%20specific%20learners%20are%20more%20ill-behaved%20than%20others.%0AAdditionally%2C%20we%20demonstrate%20that%20different%20feature%20scalings%20rarely%20resolve%0Aill-behavior.%20We%20evaluate%20the%20impact%20of%20ill-behavior%20on%20downstream%20tasks%2C%20such%0Aas%20learning%20curve%20fitting%20and%20model%20selection%2C%20and%20find%20it%20poses%20significant%0Achallenges%2C%20underscoring%20the%20relevance%20and%20potential%20of%20LCDB%201.1%20as%20a%0Achallenging%20benchmark%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15657v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLCDB%25201.1%253A%2520A%2520Database%2520Illustrating%2520Learning%2520Curves%2520Are%2520More%2520Ill-Behaved%250A%2520%2520Than%2520Previously%2520Thought%26entry.906535625%3DCheng%2520Yan%2520and%2520Felix%2520Mohr%2520and%2520Tom%2520Viering%26entry.1292438233%3D%2520%2520Sample-wise%2520learning%2520curves%2520plot%2520performance%2520versus%2520training%2520set%2520size.%2520They%250Aare%2520useful%2520for%2520studying%2520scaling%2520laws%2520and%2520speeding%2520up%2520hyperparameter%2520tuning%2520and%250Amodel%2520selection.%2520Learning%2520curves%2520are%2520often%2520assumed%2520to%2520be%2520well-behaved%253A%2520monotone%250A%2528i.e.%2520improving%2520with%2520more%2520data%2529%2520and%2520convex.%2520By%2520constructing%2520the%2520Learning%2520Curves%250ADatabase%25201.1%2520%2528LCDB%25201.1%2529%252C%2520a%2520large-scale%2520database%2520with%2520high-resolution%2520learning%250Acurves%252C%2520we%2520show%2520that%2520learning%2520curves%2520are%2520less%2520often%2520well-behaved%2520than%250Apreviously%2520thought.%2520Using%2520statistically%2520rigorous%2520methods%252C%2520we%2520observe%250Asignificant%2520ill-behavior%2520in%2520approximately%252014%2525%2520of%2520the%2520learning%2520curves%252C%2520almost%250Atwice%2520as%2520much%2520as%2520in%2520previous%2520estimates.%2520We%2520also%2520identify%2520which%2520learners%2520are%2520to%250Ablame%2520and%2520show%2520that%2520specific%2520learners%2520are%2520more%2520ill-behaved%2520than%2520others.%250AAdditionally%252C%2520we%2520demonstrate%2520that%2520different%2520feature%2520scalings%2520rarely%2520resolve%250Aill-behavior.%2520We%2520evaluate%2520the%2520impact%2520of%2520ill-behavior%2520on%2520downstream%2520tasks%252C%2520such%250Aas%2520learning%2520curve%2520fitting%2520and%2520model%2520selection%252C%2520and%2520find%2520it%2520poses%2520significant%250Achallenges%252C%2520underscoring%2520the%2520relevance%2520and%2520potential%2520of%2520LCDB%25201.1%2520as%2520a%250Achallenging%2520benchmark%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15657v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LCDB%201.1%3A%20A%20Database%20Illustrating%20Learning%20Curves%20Are%20More%20Ill-Behaved%0A%20%20Than%20Previously%20Thought&entry.906535625=Cheng%20Yan%20and%20Felix%20Mohr%20and%20Tom%20Viering&entry.1292438233=%20%20Sample-wise%20learning%20curves%20plot%20performance%20versus%20training%20set%20size.%20They%0Aare%20useful%20for%20studying%20scaling%20laws%20and%20speeding%20up%20hyperparameter%20tuning%20and%0Amodel%20selection.%20Learning%20curves%20are%20often%20assumed%20to%20be%20well-behaved%3A%20monotone%0A%28i.e.%20improving%20with%20more%20data%29%20and%20convex.%20By%20constructing%20the%20Learning%20Curves%0ADatabase%201.1%20%28LCDB%201.1%29%2C%20a%20large-scale%20database%20with%20high-resolution%20learning%0Acurves%2C%20we%20show%20that%20learning%20curves%20are%20less%20often%20well-behaved%20than%0Apreviously%20thought.%20Using%20statistically%20rigorous%20methods%2C%20we%20observe%0Asignificant%20ill-behavior%20in%20approximately%2014%25%20of%20the%20learning%20curves%2C%20almost%0Atwice%20as%20much%20as%20in%20previous%20estimates.%20We%20also%20identify%20which%20learners%20are%20to%0Ablame%20and%20show%20that%20specific%20learners%20are%20more%20ill-behaved%20than%20others.%0AAdditionally%2C%20we%20demonstrate%20that%20different%20feature%20scalings%20rarely%20resolve%0Aill-behavior.%20We%20evaluate%20the%20impact%20of%20ill-behavior%20on%20downstream%20tasks%2C%20such%0Aas%20learning%20curve%20fitting%20and%20model%20selection%2C%20and%20find%20it%20poses%20significant%0Achallenges%2C%20underscoring%20the%20relevance%20and%20potential%20of%20LCDB%201.1%20as%20a%0Achallenging%20benchmark%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15657v1&entry.124074799=Read"},
{"title": "Mouse Lockbox Dataset: Behavior Recognition for Mice Solving Lockboxes", "author": "Patrik Reiske and Marcus N. Boon and Niek Andresen and Sole Traverso and Katharina Hohlbaum and Lars Lewejohann and Christa Th\u00f6ne-Reineke and Olaf Hellwich and Henning Sprekeler", "abstract": "  Machine learning and computer vision methods have a major impact on the study\nof natural animal behavior, as they enable the (semi-)automatic analysis of\nvast amounts of video data. Mice are the standard mammalian model system in\nmost research fields, but the datasets available today to refine such methods\nfocus either on simple or social behaviors. In this work, we present a video\ndataset of individual mice solving complex mechanical puzzles, so-called\nlockboxes. The more than 110 hours of total playtime show their behavior\nrecorded from three different perspectives. As a benchmark for frame-level\naction classification methods, we provide human-annotated labels for all videos\nof two different mice, that equal 13% of our dataset. Our keypoint (pose)\ntracking-based action classification framework illustrates the challenges of\nautomated labeling of fine-grained behaviors, such as the manipulation of\nobjects. We hope that our work will help accelerate the advancement of\nautomated action and behavior classification in the computational neuroscience\ncommunity. Our dataset is publicly available at\nhttps://doi.org/10.14279/depositonce-23850\n", "link": "http://arxiv.org/abs/2505.15408v1", "date": "2025-05-21", "relevancy": 0.9446, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4733}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4728}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mouse%20Lockbox%20Dataset%3A%20Behavior%20Recognition%20for%20Mice%20Solving%20Lockboxes&body=Title%3A%20Mouse%20Lockbox%20Dataset%3A%20Behavior%20Recognition%20for%20Mice%20Solving%20Lockboxes%0AAuthor%3A%20Patrik%20Reiske%20and%20Marcus%20N.%20Boon%20and%20Niek%20Andresen%20and%20Sole%20Traverso%20and%20Katharina%20Hohlbaum%20and%20Lars%20Lewejohann%20and%20Christa%20Th%C3%B6ne-Reineke%20and%20Olaf%20Hellwich%20and%20Henning%20Sprekeler%0AAbstract%3A%20%20%20Machine%20learning%20and%20computer%20vision%20methods%20have%20a%20major%20impact%20on%20the%20study%0Aof%20natural%20animal%20behavior%2C%20as%20they%20enable%20the%20%28semi-%29automatic%20analysis%20of%0Avast%20amounts%20of%20video%20data.%20Mice%20are%20the%20standard%20mammalian%20model%20system%20in%0Amost%20research%20fields%2C%20but%20the%20datasets%20available%20today%20to%20refine%20such%20methods%0Afocus%20either%20on%20simple%20or%20social%20behaviors.%20In%20this%20work%2C%20we%20present%20a%20video%0Adataset%20of%20individual%20mice%20solving%20complex%20mechanical%20puzzles%2C%20so-called%0Alockboxes.%20The%20more%20than%20110%20hours%20of%20total%20playtime%20show%20their%20behavior%0Arecorded%20from%20three%20different%20perspectives.%20As%20a%20benchmark%20for%20frame-level%0Aaction%20classification%20methods%2C%20we%20provide%20human-annotated%20labels%20for%20all%20videos%0Aof%20two%20different%20mice%2C%20that%20equal%2013%25%20of%20our%20dataset.%20Our%20keypoint%20%28pose%29%0Atracking-based%20action%20classification%20framework%20illustrates%20the%20challenges%20of%0Aautomated%20labeling%20of%20fine-grained%20behaviors%2C%20such%20as%20the%20manipulation%20of%0Aobjects.%20We%20hope%20that%20our%20work%20will%20help%20accelerate%20the%20advancement%20of%0Aautomated%20action%20and%20behavior%20classification%20in%20the%20computational%20neuroscience%0Acommunity.%20Our%20dataset%20is%20publicly%20available%20at%0Ahttps%3A//doi.org/10.14279/depositonce-23850%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15408v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMouse%2520Lockbox%2520Dataset%253A%2520Behavior%2520Recognition%2520for%2520Mice%2520Solving%2520Lockboxes%26entry.906535625%3DPatrik%2520Reiske%2520and%2520Marcus%2520N.%2520Boon%2520and%2520Niek%2520Andresen%2520and%2520Sole%2520Traverso%2520and%2520Katharina%2520Hohlbaum%2520and%2520Lars%2520Lewejohann%2520and%2520Christa%2520Th%25C3%25B6ne-Reineke%2520and%2520Olaf%2520Hellwich%2520and%2520Henning%2520Sprekeler%26entry.1292438233%3D%2520%2520Machine%2520learning%2520and%2520computer%2520vision%2520methods%2520have%2520a%2520major%2520impact%2520on%2520the%2520study%250Aof%2520natural%2520animal%2520behavior%252C%2520as%2520they%2520enable%2520the%2520%2528semi-%2529automatic%2520analysis%2520of%250Avast%2520amounts%2520of%2520video%2520data.%2520Mice%2520are%2520the%2520standard%2520mammalian%2520model%2520system%2520in%250Amost%2520research%2520fields%252C%2520but%2520the%2520datasets%2520available%2520today%2520to%2520refine%2520such%2520methods%250Afocus%2520either%2520on%2520simple%2520or%2520social%2520behaviors.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520video%250Adataset%2520of%2520individual%2520mice%2520solving%2520complex%2520mechanical%2520puzzles%252C%2520so-called%250Alockboxes.%2520The%2520more%2520than%2520110%2520hours%2520of%2520total%2520playtime%2520show%2520their%2520behavior%250Arecorded%2520from%2520three%2520different%2520perspectives.%2520As%2520a%2520benchmark%2520for%2520frame-level%250Aaction%2520classification%2520methods%252C%2520we%2520provide%2520human-annotated%2520labels%2520for%2520all%2520videos%250Aof%2520two%2520different%2520mice%252C%2520that%2520equal%252013%2525%2520of%2520our%2520dataset.%2520Our%2520keypoint%2520%2528pose%2529%250Atracking-based%2520action%2520classification%2520framework%2520illustrates%2520the%2520challenges%2520of%250Aautomated%2520labeling%2520of%2520fine-grained%2520behaviors%252C%2520such%2520as%2520the%2520manipulation%2520of%250Aobjects.%2520We%2520hope%2520that%2520our%2520work%2520will%2520help%2520accelerate%2520the%2520advancement%2520of%250Aautomated%2520action%2520and%2520behavior%2520classification%2520in%2520the%2520computational%2520neuroscience%250Acommunity.%2520Our%2520dataset%2520is%2520publicly%2520available%2520at%250Ahttps%253A//doi.org/10.14279/depositonce-23850%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15408v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mouse%20Lockbox%20Dataset%3A%20Behavior%20Recognition%20for%20Mice%20Solving%20Lockboxes&entry.906535625=Patrik%20Reiske%20and%20Marcus%20N.%20Boon%20and%20Niek%20Andresen%20and%20Sole%20Traverso%20and%20Katharina%20Hohlbaum%20and%20Lars%20Lewejohann%20and%20Christa%20Th%C3%B6ne-Reineke%20and%20Olaf%20Hellwich%20and%20Henning%20Sprekeler&entry.1292438233=%20%20Machine%20learning%20and%20computer%20vision%20methods%20have%20a%20major%20impact%20on%20the%20study%0Aof%20natural%20animal%20behavior%2C%20as%20they%20enable%20the%20%28semi-%29automatic%20analysis%20of%0Avast%20amounts%20of%20video%20data.%20Mice%20are%20the%20standard%20mammalian%20model%20system%20in%0Amost%20research%20fields%2C%20but%20the%20datasets%20available%20today%20to%20refine%20such%20methods%0Afocus%20either%20on%20simple%20or%20social%20behaviors.%20In%20this%20work%2C%20we%20present%20a%20video%0Adataset%20of%20individual%20mice%20solving%20complex%20mechanical%20puzzles%2C%20so-called%0Alockboxes.%20The%20more%20than%20110%20hours%20of%20total%20playtime%20show%20their%20behavior%0Arecorded%20from%20three%20different%20perspectives.%20As%20a%20benchmark%20for%20frame-level%0Aaction%20classification%20methods%2C%20we%20provide%20human-annotated%20labels%20for%20all%20videos%0Aof%20two%20different%20mice%2C%20that%20equal%2013%25%20of%20our%20dataset.%20Our%20keypoint%20%28pose%29%0Atracking-based%20action%20classification%20framework%20illustrates%20the%20challenges%20of%0Aautomated%20labeling%20of%20fine-grained%20behaviors%2C%20such%20as%20the%20manipulation%20of%0Aobjects.%20We%20hope%20that%20our%20work%20will%20help%20accelerate%20the%20advancement%20of%0Aautomated%20action%20and%20behavior%20classification%20in%20the%20computational%20neuroscience%0Acommunity.%20Our%20dataset%20is%20publicly%20available%20at%0Ahttps%3A//doi.org/10.14279/depositonce-23850%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15408v1&entry.124074799=Read"},
{"title": "Probing Semantic Routing in Large Mixture-of-Expert Models", "author": "Matthew Lyle Olson and Neale Ratzlaff and Musashi Hinck and Man Luo and Sungduk Yu and Chendi Xue and Vasudev Lal", "abstract": "  In the past year, large (>100B parameter) mixture-of-expert (MoE) models have\nbecome increasingly common in the open domain. While their advantages are often\nframed in terms of efficiency, prior work has also explored functional\ndifferentiation through routing behavior. We investigate whether expert routing\nin large MoE models is influenced by the semantics of the inputs. To test this,\nwe design two controlled experiments. First, we compare activations on sentence\npairs with a shared target word used in the same or different senses. Second,\nwe fix context and substitute the target word with semantically similar or\ndissimilar alternatives. Comparing expert overlap across these conditions\nreveals clear, statistically significant evidence of semantic routing in large\nMoE models.\n", "link": "http://arxiv.org/abs/2502.10928v2", "date": "2025-05-21", "relevancy": 2.1629, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5421}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5421}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5338}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probing%20Semantic%20Routing%20in%20Large%20Mixture-of-Expert%20Models&body=Title%3A%20Probing%20Semantic%20Routing%20in%20Large%20Mixture-of-Expert%20Models%0AAuthor%3A%20Matthew%20Lyle%20Olson%20and%20Neale%20Ratzlaff%20and%20Musashi%20Hinck%20and%20Man%20Luo%20and%20Sungduk%20Yu%20and%20Chendi%20Xue%20and%20Vasudev%20Lal%0AAbstract%3A%20%20%20In%20the%20past%20year%2C%20large%20%28%3E100B%20parameter%29%20mixture-of-expert%20%28MoE%29%20models%20have%0Abecome%20increasingly%20common%20in%20the%20open%20domain.%20While%20their%20advantages%20are%20often%0Aframed%20in%20terms%20of%20efficiency%2C%20prior%20work%20has%20also%20explored%20functional%0Adifferentiation%20through%20routing%20behavior.%20We%20investigate%20whether%20expert%20routing%0Ain%20large%20MoE%20models%20is%20influenced%20by%20the%20semantics%20of%20the%20inputs.%20To%20test%20this%2C%0Awe%20design%20two%20controlled%20experiments.%20First%2C%20we%20compare%20activations%20on%20sentence%0Apairs%20with%20a%20shared%20target%20word%20used%20in%20the%20same%20or%20different%20senses.%20Second%2C%0Awe%20fix%20context%20and%20substitute%20the%20target%20word%20with%20semantically%20similar%20or%0Adissimilar%20alternatives.%20Comparing%20expert%20overlap%20across%20these%20conditions%0Areveals%20clear%2C%20statistically%20significant%20evidence%20of%20semantic%20routing%20in%20large%0AMoE%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10928v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbing%2520Semantic%2520Routing%2520in%2520Large%2520Mixture-of-Expert%2520Models%26entry.906535625%3DMatthew%2520Lyle%2520Olson%2520and%2520Neale%2520Ratzlaff%2520and%2520Musashi%2520Hinck%2520and%2520Man%2520Luo%2520and%2520Sungduk%2520Yu%2520and%2520Chendi%2520Xue%2520and%2520Vasudev%2520Lal%26entry.1292438233%3D%2520%2520In%2520the%2520past%2520year%252C%2520large%2520%2528%253E100B%2520parameter%2529%2520mixture-of-expert%2520%2528MoE%2529%2520models%2520have%250Abecome%2520increasingly%2520common%2520in%2520the%2520open%2520domain.%2520While%2520their%2520advantages%2520are%2520often%250Aframed%2520in%2520terms%2520of%2520efficiency%252C%2520prior%2520work%2520has%2520also%2520explored%2520functional%250Adifferentiation%2520through%2520routing%2520behavior.%2520We%2520investigate%2520whether%2520expert%2520routing%250Ain%2520large%2520MoE%2520models%2520is%2520influenced%2520by%2520the%2520semantics%2520of%2520the%2520inputs.%2520To%2520test%2520this%252C%250Awe%2520design%2520two%2520controlled%2520experiments.%2520First%252C%2520we%2520compare%2520activations%2520on%2520sentence%250Apairs%2520with%2520a%2520shared%2520target%2520word%2520used%2520in%2520the%2520same%2520or%2520different%2520senses.%2520Second%252C%250Awe%2520fix%2520context%2520and%2520substitute%2520the%2520target%2520word%2520with%2520semantically%2520similar%2520or%250Adissimilar%2520alternatives.%2520Comparing%2520expert%2520overlap%2520across%2520these%2520conditions%250Areveals%2520clear%252C%2520statistically%2520significant%2520evidence%2520of%2520semantic%2520routing%2520in%2520large%250AMoE%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10928v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probing%20Semantic%20Routing%20in%20Large%20Mixture-of-Expert%20Models&entry.906535625=Matthew%20Lyle%20Olson%20and%20Neale%20Ratzlaff%20and%20Musashi%20Hinck%20and%20Man%20Luo%20and%20Sungduk%20Yu%20and%20Chendi%20Xue%20and%20Vasudev%20Lal&entry.1292438233=%20%20In%20the%20past%20year%2C%20large%20%28%3E100B%20parameter%29%20mixture-of-expert%20%28MoE%29%20models%20have%0Abecome%20increasingly%20common%20in%20the%20open%20domain.%20While%20their%20advantages%20are%20often%0Aframed%20in%20terms%20of%20efficiency%2C%20prior%20work%20has%20also%20explored%20functional%0Adifferentiation%20through%20routing%20behavior.%20We%20investigate%20whether%20expert%20routing%0Ain%20large%20MoE%20models%20is%20influenced%20by%20the%20semantics%20of%20the%20inputs.%20To%20test%20this%2C%0Awe%20design%20two%20controlled%20experiments.%20First%2C%20we%20compare%20activations%20on%20sentence%0Apairs%20with%20a%20shared%20target%20word%20used%20in%20the%20same%20or%20different%20senses.%20Second%2C%0Awe%20fix%20context%20and%20substitute%20the%20target%20word%20with%20semantically%20similar%20or%0Adissimilar%20alternatives.%20Comparing%20expert%20overlap%20across%20these%20conditions%0Areveals%20clear%2C%20statistically%20significant%20evidence%20of%20semantic%20routing%20in%20large%0AMoE%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10928v2&entry.124074799=Read"},
{"title": "Leveraging the Powerful Attention of a Pre-trained Diffusion Model for\n  Exemplar-based Image Colorization", "author": "Satoshi Kosugi", "abstract": "  Exemplar-based image colorization aims to colorize a grayscale image using a\nreference color image, ensuring that reference colors are applied to\ncorresponding input regions based on their semantic similarity. To achieve\naccurate semantic matching between regions, we leverage the self-attention\nmodule of a pre-trained diffusion model, which is trained on a large dataset\nand exhibits powerful attention capabilities. To harness this power, we propose\na novel, fine-tuning-free approach based on a pre-trained diffusion model,\nmaking two key contributions. First, we introduce dual attention-guided color\ntransfer. We utilize the self-attention module to compute an attention map\nbetween the input and reference images, effectively capturing semantic\ncorrespondences. The color features from the reference image is then\ntransferred to the semantically matching regions of the input image, guided by\nthis attention map, and finally, the grayscale features are replaced with the\ncorresponding color features. Notably, we utilize dual attention to calculate\nattention maps separately for the grayscale and color images, achieving more\nprecise semantic alignment. Second, we propose classifier-free colorization\nguidance, which enhances the transferred colors by combining color-transferred\nand non-color-transferred outputs. This process improves the quality of\ncolorization. Our experimental results demonstrate that our method outperforms\nexisting techniques in terms of image quality and fidelity to the reference.\nSpecifically, we use 335 input-reference pairs from previous research,\nachieving an FID of 95.27 (image quality) and an SI-FID of 5.51 (fidelity to\nthe reference). Our source code is available at\nhttps://github.com/satoshi-kosugi/powerful-attention.\n", "link": "http://arxiv.org/abs/2505.15812v1", "date": "2025-05-21", "relevancy": 1.8112, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6169}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6021}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20the%20Powerful%20Attention%20of%20a%20Pre-trained%20Diffusion%20Model%20for%0A%20%20Exemplar-based%20Image%20Colorization&body=Title%3A%20Leveraging%20the%20Powerful%20Attention%20of%20a%20Pre-trained%20Diffusion%20Model%20for%0A%20%20Exemplar-based%20Image%20Colorization%0AAuthor%3A%20Satoshi%20Kosugi%0AAbstract%3A%20%20%20Exemplar-based%20image%20colorization%20aims%20to%20colorize%20a%20grayscale%20image%20using%20a%0Areference%20color%20image%2C%20ensuring%20that%20reference%20colors%20are%20applied%20to%0Acorresponding%20input%20regions%20based%20on%20their%20semantic%20similarity.%20To%20achieve%0Aaccurate%20semantic%20matching%20between%20regions%2C%20we%20leverage%20the%20self-attention%0Amodule%20of%20a%20pre-trained%20diffusion%20model%2C%20which%20is%20trained%20on%20a%20large%20dataset%0Aand%20exhibits%20powerful%20attention%20capabilities.%20To%20harness%20this%20power%2C%20we%20propose%0Aa%20novel%2C%20fine-tuning-free%20approach%20based%20on%20a%20pre-trained%20diffusion%20model%2C%0Amaking%20two%20key%20contributions.%20First%2C%20we%20introduce%20dual%20attention-guided%20color%0Atransfer.%20We%20utilize%20the%20self-attention%20module%20to%20compute%20an%20attention%20map%0Abetween%20the%20input%20and%20reference%20images%2C%20effectively%20capturing%20semantic%0Acorrespondences.%20The%20color%20features%20from%20the%20reference%20image%20is%20then%0Atransferred%20to%20the%20semantically%20matching%20regions%20of%20the%20input%20image%2C%20guided%20by%0Athis%20attention%20map%2C%20and%20finally%2C%20the%20grayscale%20features%20are%20replaced%20with%20the%0Acorresponding%20color%20features.%20Notably%2C%20we%20utilize%20dual%20attention%20to%20calculate%0Aattention%20maps%20separately%20for%20the%20grayscale%20and%20color%20images%2C%20achieving%20more%0Aprecise%20semantic%20alignment.%20Second%2C%20we%20propose%20classifier-free%20colorization%0Aguidance%2C%20which%20enhances%20the%20transferred%20colors%20by%20combining%20color-transferred%0Aand%20non-color-transferred%20outputs.%20This%20process%20improves%20the%20quality%20of%0Acolorization.%20Our%20experimental%20results%20demonstrate%20that%20our%20method%20outperforms%0Aexisting%20techniques%20in%20terms%20of%20image%20quality%20and%20fidelity%20to%20the%20reference.%0ASpecifically%2C%20we%20use%20335%20input-reference%20pairs%20from%20previous%20research%2C%0Aachieving%20an%20FID%20of%2095.27%20%28image%20quality%29%20and%20an%20SI-FID%20of%205.51%20%28fidelity%20to%0Athe%20reference%29.%20Our%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/satoshi-kosugi/powerful-attention.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15812v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520the%2520Powerful%2520Attention%2520of%2520a%2520Pre-trained%2520Diffusion%2520Model%2520for%250A%2520%2520Exemplar-based%2520Image%2520Colorization%26entry.906535625%3DSatoshi%2520Kosugi%26entry.1292438233%3D%2520%2520Exemplar-based%2520image%2520colorization%2520aims%2520to%2520colorize%2520a%2520grayscale%2520image%2520using%2520a%250Areference%2520color%2520image%252C%2520ensuring%2520that%2520reference%2520colors%2520are%2520applied%2520to%250Acorresponding%2520input%2520regions%2520based%2520on%2520their%2520semantic%2520similarity.%2520To%2520achieve%250Aaccurate%2520semantic%2520matching%2520between%2520regions%252C%2520we%2520leverage%2520the%2520self-attention%250Amodule%2520of%2520a%2520pre-trained%2520diffusion%2520model%252C%2520which%2520is%2520trained%2520on%2520a%2520large%2520dataset%250Aand%2520exhibits%2520powerful%2520attention%2520capabilities.%2520To%2520harness%2520this%2520power%252C%2520we%2520propose%250Aa%2520novel%252C%2520fine-tuning-free%2520approach%2520based%2520on%2520a%2520pre-trained%2520diffusion%2520model%252C%250Amaking%2520two%2520key%2520contributions.%2520First%252C%2520we%2520introduce%2520dual%2520attention-guided%2520color%250Atransfer.%2520We%2520utilize%2520the%2520self-attention%2520module%2520to%2520compute%2520an%2520attention%2520map%250Abetween%2520the%2520input%2520and%2520reference%2520images%252C%2520effectively%2520capturing%2520semantic%250Acorrespondences.%2520The%2520color%2520features%2520from%2520the%2520reference%2520image%2520is%2520then%250Atransferred%2520to%2520the%2520semantically%2520matching%2520regions%2520of%2520the%2520input%2520image%252C%2520guided%2520by%250Athis%2520attention%2520map%252C%2520and%2520finally%252C%2520the%2520grayscale%2520features%2520are%2520replaced%2520with%2520the%250Acorresponding%2520color%2520features.%2520Notably%252C%2520we%2520utilize%2520dual%2520attention%2520to%2520calculate%250Aattention%2520maps%2520separately%2520for%2520the%2520grayscale%2520and%2520color%2520images%252C%2520achieving%2520more%250Aprecise%2520semantic%2520alignment.%2520Second%252C%2520we%2520propose%2520classifier-free%2520colorization%250Aguidance%252C%2520which%2520enhances%2520the%2520transferred%2520colors%2520by%2520combining%2520color-transferred%250Aand%2520non-color-transferred%2520outputs.%2520This%2520process%2520improves%2520the%2520quality%2520of%250Acolorization.%2520Our%2520experimental%2520results%2520demonstrate%2520that%2520our%2520method%2520outperforms%250Aexisting%2520techniques%2520in%2520terms%2520of%2520image%2520quality%2520and%2520fidelity%2520to%2520the%2520reference.%250ASpecifically%252C%2520we%2520use%2520335%2520input-reference%2520pairs%2520from%2520previous%2520research%252C%250Aachieving%2520an%2520FID%2520of%252095.27%2520%2528image%2520quality%2529%2520and%2520an%2520SI-FID%2520of%25205.51%2520%2528fidelity%2520to%250Athe%2520reference%2529.%2520Our%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/satoshi-kosugi/powerful-attention.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15812v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20the%20Powerful%20Attention%20of%20a%20Pre-trained%20Diffusion%20Model%20for%0A%20%20Exemplar-based%20Image%20Colorization&entry.906535625=Satoshi%20Kosugi&entry.1292438233=%20%20Exemplar-based%20image%20colorization%20aims%20to%20colorize%20a%20grayscale%20image%20using%20a%0Areference%20color%20image%2C%20ensuring%20that%20reference%20colors%20are%20applied%20to%0Acorresponding%20input%20regions%20based%20on%20their%20semantic%20similarity.%20To%20achieve%0Aaccurate%20semantic%20matching%20between%20regions%2C%20we%20leverage%20the%20self-attention%0Amodule%20of%20a%20pre-trained%20diffusion%20model%2C%20which%20is%20trained%20on%20a%20large%20dataset%0Aand%20exhibits%20powerful%20attention%20capabilities.%20To%20harness%20this%20power%2C%20we%20propose%0Aa%20novel%2C%20fine-tuning-free%20approach%20based%20on%20a%20pre-trained%20diffusion%20model%2C%0Amaking%20two%20key%20contributions.%20First%2C%20we%20introduce%20dual%20attention-guided%20color%0Atransfer.%20We%20utilize%20the%20self-attention%20module%20to%20compute%20an%20attention%20map%0Abetween%20the%20input%20and%20reference%20images%2C%20effectively%20capturing%20semantic%0Acorrespondences.%20The%20color%20features%20from%20the%20reference%20image%20is%20then%0Atransferred%20to%20the%20semantically%20matching%20regions%20of%20the%20input%20image%2C%20guided%20by%0Athis%20attention%20map%2C%20and%20finally%2C%20the%20grayscale%20features%20are%20replaced%20with%20the%0Acorresponding%20color%20features.%20Notably%2C%20we%20utilize%20dual%20attention%20to%20calculate%0Aattention%20maps%20separately%20for%20the%20grayscale%20and%20color%20images%2C%20achieving%20more%0Aprecise%20semantic%20alignment.%20Second%2C%20we%20propose%20classifier-free%20colorization%0Aguidance%2C%20which%20enhances%20the%20transferred%20colors%20by%20combining%20color-transferred%0Aand%20non-color-transferred%20outputs.%20This%20process%20improves%20the%20quality%20of%0Acolorization.%20Our%20experimental%20results%20demonstrate%20that%20our%20method%20outperforms%0Aexisting%20techniques%20in%20terms%20of%20image%20quality%20and%20fidelity%20to%20the%20reference.%0ASpecifically%2C%20we%20use%20335%20input-reference%20pairs%20from%20previous%20research%2C%0Aachieving%20an%20FID%20of%2095.27%20%28image%20quality%29%20and%20an%20SI-FID%20of%205.51%20%28fidelity%20to%0Athe%20reference%29.%20Our%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/satoshi-kosugi/powerful-attention.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15812v1&entry.124074799=Read"},
{"title": "MMaDA: Multimodal Large Diffusion Language Models", "author": "Ling Yang and Ye Tian and Bowen Li and Xinchen Zhang and Ke Shen and Yunhai Tong and Mengdi Wang", "abstract": "  We introduce MMaDA, a novel class of multimodal diffusion foundation models\ndesigned to achieve superior performance across diverse domains such as textual\nreasoning, multimodal understanding, and text-to-image generation. The approach\nis distinguished by three key innovations: (i) MMaDA adopts a unified diffusion\narchitecture with a shared probabilistic formulation and a modality-agnostic\ndesign, eliminating the need for modality-specific components. This\narchitecture ensures seamless integration and processing across different data\ntypes. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning\nstrategy that curates a unified CoT format across modalities. By aligning\nreasoning processes between textual and visual domains, this strategy\nfacilitates cold-start training for the final reinforcement learning (RL)\nstage, thereby enhancing the model's ability to handle complex tasks from the\noutset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm\nspecifically tailored for diffusion foundation models. Utilizing diversified\nreward modeling, UniGRPO unifies post-training across both reasoning and\ngeneration tasks, ensuring consistent performance improvements. Experimental\nresults demonstrate that MMaDA-8B exhibits strong generalization capabilities\nas a unified multimodal foundation model. It surpasses powerful models like\nLLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in\nmultimodal understanding, and excels over SDXL and Janus in text-to-image\ngeneration. These achievements highlight MMaDA's effectiveness in bridging the\ngap between pretraining and post-training within unified diffusion\narchitectures, providing a comprehensive framework for future research and\ndevelopment. We open-source our code and trained models at:\nhttps://github.com/Gen-Verse/MMaDA\n", "link": "http://arxiv.org/abs/2505.15809v1", "date": "2025-05-21", "relevancy": 1.7379, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6228}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5729}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMaDA%3A%20Multimodal%20Large%20Diffusion%20Language%20Models&body=Title%3A%20MMaDA%3A%20Multimodal%20Large%20Diffusion%20Language%20Models%0AAuthor%3A%20Ling%20Yang%20and%20Ye%20Tian%20and%20Bowen%20Li%20and%20Xinchen%20Zhang%20and%20Ke%20Shen%20and%20Yunhai%20Tong%20and%20Mengdi%20Wang%0AAbstract%3A%20%20%20We%20introduce%20MMaDA%2C%20a%20novel%20class%20of%20multimodal%20diffusion%20foundation%20models%0Adesigned%20to%20achieve%20superior%20performance%20across%20diverse%20domains%20such%20as%20textual%0Areasoning%2C%20multimodal%20understanding%2C%20and%20text-to-image%20generation.%20The%20approach%0Ais%20distinguished%20by%20three%20key%20innovations%3A%20%28i%29%20MMaDA%20adopts%20a%20unified%20diffusion%0Aarchitecture%20with%20a%20shared%20probabilistic%20formulation%20and%20a%20modality-agnostic%0Adesign%2C%20eliminating%20the%20need%20for%20modality-specific%20components.%20This%0Aarchitecture%20ensures%20seamless%20integration%20and%20processing%20across%20different%20data%0Atypes.%20%28ii%29%20We%20implement%20a%20mixed%20long%20chain-of-thought%20%28CoT%29%20fine-tuning%0Astrategy%20that%20curates%20a%20unified%20CoT%20format%20across%20modalities.%20By%20aligning%0Areasoning%20processes%20between%20textual%20and%20visual%20domains%2C%20this%20strategy%0Afacilitates%20cold-start%20training%20for%20the%20final%20reinforcement%20learning%20%28RL%29%0Astage%2C%20thereby%20enhancing%20the%20model%27s%20ability%20to%20handle%20complex%20tasks%20from%20the%0Aoutset.%20%28iii%29%20We%20propose%20UniGRPO%2C%20a%20unified%20policy-gradient-based%20RL%20algorithm%0Aspecifically%20tailored%20for%20diffusion%20foundation%20models.%20Utilizing%20diversified%0Areward%20modeling%2C%20UniGRPO%20unifies%20post-training%20across%20both%20reasoning%20and%0Ageneration%20tasks%2C%20ensuring%20consistent%20performance%20improvements.%20Experimental%0Aresults%20demonstrate%20that%20MMaDA-8B%20exhibits%20strong%20generalization%20capabilities%0Aas%20a%20unified%20multimodal%20foundation%20model.%20It%20surpasses%20powerful%20models%20like%0ALLaMA-3-7B%20and%20Qwen2-7B%20in%20textual%20reasoning%2C%20outperforms%20Show-o%20and%20SEED-X%20in%0Amultimodal%20understanding%2C%20and%20excels%20over%20SDXL%20and%20Janus%20in%20text-to-image%0Ageneration.%20These%20achievements%20highlight%20MMaDA%27s%20effectiveness%20in%20bridging%20the%0Agap%20between%20pretraining%20and%20post-training%20within%20unified%20diffusion%0Aarchitectures%2C%20providing%20a%20comprehensive%20framework%20for%20future%20research%20and%0Adevelopment.%20We%20open-source%20our%20code%20and%20trained%20models%20at%3A%0Ahttps%3A//github.com/Gen-Verse/MMaDA%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15809v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMaDA%253A%2520Multimodal%2520Large%2520Diffusion%2520Language%2520Models%26entry.906535625%3DLing%2520Yang%2520and%2520Ye%2520Tian%2520and%2520Bowen%2520Li%2520and%2520Xinchen%2520Zhang%2520and%2520Ke%2520Shen%2520and%2520Yunhai%2520Tong%2520and%2520Mengdi%2520Wang%26entry.1292438233%3D%2520%2520We%2520introduce%2520MMaDA%252C%2520a%2520novel%2520class%2520of%2520multimodal%2520diffusion%2520foundation%2520models%250Adesigned%2520to%2520achieve%2520superior%2520performance%2520across%2520diverse%2520domains%2520such%2520as%2520textual%250Areasoning%252C%2520multimodal%2520understanding%252C%2520and%2520text-to-image%2520generation.%2520The%2520approach%250Ais%2520distinguished%2520by%2520three%2520key%2520innovations%253A%2520%2528i%2529%2520MMaDA%2520adopts%2520a%2520unified%2520diffusion%250Aarchitecture%2520with%2520a%2520shared%2520probabilistic%2520formulation%2520and%2520a%2520modality-agnostic%250Adesign%252C%2520eliminating%2520the%2520need%2520for%2520modality-specific%2520components.%2520This%250Aarchitecture%2520ensures%2520seamless%2520integration%2520and%2520processing%2520across%2520different%2520data%250Atypes.%2520%2528ii%2529%2520We%2520implement%2520a%2520mixed%2520long%2520chain-of-thought%2520%2528CoT%2529%2520fine-tuning%250Astrategy%2520that%2520curates%2520a%2520unified%2520CoT%2520format%2520across%2520modalities.%2520By%2520aligning%250Areasoning%2520processes%2520between%2520textual%2520and%2520visual%2520domains%252C%2520this%2520strategy%250Afacilitates%2520cold-start%2520training%2520for%2520the%2520final%2520reinforcement%2520learning%2520%2528RL%2529%250Astage%252C%2520thereby%2520enhancing%2520the%2520model%2527s%2520ability%2520to%2520handle%2520complex%2520tasks%2520from%2520the%250Aoutset.%2520%2528iii%2529%2520We%2520propose%2520UniGRPO%252C%2520a%2520unified%2520policy-gradient-based%2520RL%2520algorithm%250Aspecifically%2520tailored%2520for%2520diffusion%2520foundation%2520models.%2520Utilizing%2520diversified%250Areward%2520modeling%252C%2520UniGRPO%2520unifies%2520post-training%2520across%2520both%2520reasoning%2520and%250Ageneration%2520tasks%252C%2520ensuring%2520consistent%2520performance%2520improvements.%2520Experimental%250Aresults%2520demonstrate%2520that%2520MMaDA-8B%2520exhibits%2520strong%2520generalization%2520capabilities%250Aas%2520a%2520unified%2520multimodal%2520foundation%2520model.%2520It%2520surpasses%2520powerful%2520models%2520like%250ALLaMA-3-7B%2520and%2520Qwen2-7B%2520in%2520textual%2520reasoning%252C%2520outperforms%2520Show-o%2520and%2520SEED-X%2520in%250Amultimodal%2520understanding%252C%2520and%2520excels%2520over%2520SDXL%2520and%2520Janus%2520in%2520text-to-image%250Ageneration.%2520These%2520achievements%2520highlight%2520MMaDA%2527s%2520effectiveness%2520in%2520bridging%2520the%250Agap%2520between%2520pretraining%2520and%2520post-training%2520within%2520unified%2520diffusion%250Aarchitectures%252C%2520providing%2520a%2520comprehensive%2520framework%2520for%2520future%2520research%2520and%250Adevelopment.%2520We%2520open-source%2520our%2520code%2520and%2520trained%2520models%2520at%253A%250Ahttps%253A//github.com/Gen-Verse/MMaDA%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15809v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMaDA%3A%20Multimodal%20Large%20Diffusion%20Language%20Models&entry.906535625=Ling%20Yang%20and%20Ye%20Tian%20and%20Bowen%20Li%20and%20Xinchen%20Zhang%20and%20Ke%20Shen%20and%20Yunhai%20Tong%20and%20Mengdi%20Wang&entry.1292438233=%20%20We%20introduce%20MMaDA%2C%20a%20novel%20class%20of%20multimodal%20diffusion%20foundation%20models%0Adesigned%20to%20achieve%20superior%20performance%20across%20diverse%20domains%20such%20as%20textual%0Areasoning%2C%20multimodal%20understanding%2C%20and%20text-to-image%20generation.%20The%20approach%0Ais%20distinguished%20by%20three%20key%20innovations%3A%20%28i%29%20MMaDA%20adopts%20a%20unified%20diffusion%0Aarchitecture%20with%20a%20shared%20probabilistic%20formulation%20and%20a%20modality-agnostic%0Adesign%2C%20eliminating%20the%20need%20for%20modality-specific%20components.%20This%0Aarchitecture%20ensures%20seamless%20integration%20and%20processing%20across%20different%20data%0Atypes.%20%28ii%29%20We%20implement%20a%20mixed%20long%20chain-of-thought%20%28CoT%29%20fine-tuning%0Astrategy%20that%20curates%20a%20unified%20CoT%20format%20across%20modalities.%20By%20aligning%0Areasoning%20processes%20between%20textual%20and%20visual%20domains%2C%20this%20strategy%0Afacilitates%20cold-start%20training%20for%20the%20final%20reinforcement%20learning%20%28RL%29%0Astage%2C%20thereby%20enhancing%20the%20model%27s%20ability%20to%20handle%20complex%20tasks%20from%20the%0Aoutset.%20%28iii%29%20We%20propose%20UniGRPO%2C%20a%20unified%20policy-gradient-based%20RL%20algorithm%0Aspecifically%20tailored%20for%20diffusion%20foundation%20models.%20Utilizing%20diversified%0Areward%20modeling%2C%20UniGRPO%20unifies%20post-training%20across%20both%20reasoning%20and%0Ageneration%20tasks%2C%20ensuring%20consistent%20performance%20improvements.%20Experimental%0Aresults%20demonstrate%20that%20MMaDA-8B%20exhibits%20strong%20generalization%20capabilities%0Aas%20a%20unified%20multimodal%20foundation%20model.%20It%20surpasses%20powerful%20models%20like%0ALLaMA-3-7B%20and%20Qwen2-7B%20in%20textual%20reasoning%2C%20outperforms%20Show-o%20and%20SEED-X%20in%0Amultimodal%20understanding%2C%20and%20excels%20over%20SDXL%20and%20Janus%20in%20text-to-image%0Ageneration.%20These%20achievements%20highlight%20MMaDA%27s%20effectiveness%20in%20bridging%20the%0Agap%20between%20pretraining%20and%20post-training%20within%20unified%20diffusion%0Aarchitectures%2C%20providing%20a%20comprehensive%20framework%20for%20future%20research%20and%0Adevelopment.%20We%20open-source%20our%20code%20and%20trained%20models%20at%3A%0Ahttps%3A//github.com/Gen-Verse/MMaDA%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15809v1&entry.124074799=Read"},
{"title": "DisCoPatch: Batch Statistics Are All You Need For OOD Detection, But\n  Only If You Can Trust Them", "author": "Francisco Caetano and Christiaan Viviers and Luis A. Zavala-Mondrag\u00f3n and Peter H. N. de With and Fons van der Sommen", "abstract": "  Out-of-distribution (OOD) detection holds significant importance across many\napplications. While semantic and domain-shift OOD problems are well-studied,\nthis work focuses on covariate shifts - subtle variations in the data\ndistribution that can degrade machine learning performance. We hypothesize that\ndetecting these subtle shifts can improve our understanding of in-distribution\nboundaries, ultimately improving OOD detection. In adversarial discriminators\ntrained with Batch Normalization (BN), real and adversarial samples form\ndistinct domains with unique batch statistics - a property we exploit for OOD\ndetection. We introduce DisCoPatch, an unsupervised Adversarial Variational\nAutoencoder (VAE) framework that harnesses this mechanism. During inference,\nbatches consist of patches from the same image, ensuring a consistent data\ndistribution that allows the model to rely on batch statistics. DisCoPatch uses\nthe VAE's suboptimal outputs (generated and reconstructed) as negative samples\nto train the discriminator, thereby improving its ability to delineate the\nboundary between in-distribution samples and covariate shifts. By tightening\nthis boundary, DisCoPatch achieves state-of-the-art results in public OOD\ndetection benchmarks. The proposed model not only excels in detecting covariate\nshifts, achieving 95.5% AUROC on ImageNet-1K(-C) but also outperforms all prior\nmethods on public Near-OOD (95.0%) benchmarks. With a compact model size of\n25MB, it achieves high OOD detection performance at notably lower latency than\nexisting methods, making it an efficient and practical solution for real-world\nOOD detection applications. The code will be made publicly available\n", "link": "http://arxiv.org/abs/2501.08005v2", "date": "2025-05-21", "relevancy": 1.5706, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5321}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5148}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DisCoPatch%3A%20Batch%20Statistics%20Are%20All%20You%20Need%20For%20OOD%20Detection%2C%20But%0A%20%20Only%20If%20You%20Can%20Trust%20Them&body=Title%3A%20DisCoPatch%3A%20Batch%20Statistics%20Are%20All%20You%20Need%20For%20OOD%20Detection%2C%20But%0A%20%20Only%20If%20You%20Can%20Trust%20Them%0AAuthor%3A%20Francisco%20Caetano%20and%20Christiaan%20Viviers%20and%20Luis%20A.%20Zavala-Mondrag%C3%B3n%20and%20Peter%20H.%20N.%20de%20With%20and%20Fons%20van%20der%20Sommen%0AAbstract%3A%20%20%20Out-of-distribution%20%28OOD%29%20detection%20holds%20significant%20importance%20across%20many%0Aapplications.%20While%20semantic%20and%20domain-shift%20OOD%20problems%20are%20well-studied%2C%0Athis%20work%20focuses%20on%20covariate%20shifts%20-%20subtle%20variations%20in%20the%20data%0Adistribution%20that%20can%20degrade%20machine%20learning%20performance.%20We%20hypothesize%20that%0Adetecting%20these%20subtle%20shifts%20can%20improve%20our%20understanding%20of%20in-distribution%0Aboundaries%2C%20ultimately%20improving%20OOD%20detection.%20In%20adversarial%20discriminators%0Atrained%20with%20Batch%20Normalization%20%28BN%29%2C%20real%20and%20adversarial%20samples%20form%0Adistinct%20domains%20with%20unique%20batch%20statistics%20-%20a%20property%20we%20exploit%20for%20OOD%0Adetection.%20We%20introduce%20DisCoPatch%2C%20an%20unsupervised%20Adversarial%20Variational%0AAutoencoder%20%28VAE%29%20framework%20that%20harnesses%20this%20mechanism.%20During%20inference%2C%0Abatches%20consist%20of%20patches%20from%20the%20same%20image%2C%20ensuring%20a%20consistent%20data%0Adistribution%20that%20allows%20the%20model%20to%20rely%20on%20batch%20statistics.%20DisCoPatch%20uses%0Athe%20VAE%27s%20suboptimal%20outputs%20%28generated%20and%20reconstructed%29%20as%20negative%20samples%0Ato%20train%20the%20discriminator%2C%20thereby%20improving%20its%20ability%20to%20delineate%20the%0Aboundary%20between%20in-distribution%20samples%20and%20covariate%20shifts.%20By%20tightening%0Athis%20boundary%2C%20DisCoPatch%20achieves%20state-of-the-art%20results%20in%20public%20OOD%0Adetection%20benchmarks.%20The%20proposed%20model%20not%20only%20excels%20in%20detecting%20covariate%0Ashifts%2C%20achieving%2095.5%25%20AUROC%20on%20ImageNet-1K%28-C%29%20but%20also%20outperforms%20all%20prior%0Amethods%20on%20public%20Near-OOD%20%2895.0%25%29%20benchmarks.%20With%20a%20compact%20model%20size%20of%0A25MB%2C%20it%20achieves%20high%20OOD%20detection%20performance%20at%20notably%20lower%20latency%20than%0Aexisting%20methods%2C%20making%20it%20an%20efficient%20and%20practical%20solution%20for%20real-world%0AOOD%20detection%20applications.%20The%20code%20will%20be%20made%20publicly%20available%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08005v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisCoPatch%253A%2520Batch%2520Statistics%2520Are%2520All%2520You%2520Need%2520For%2520OOD%2520Detection%252C%2520But%250A%2520%2520Only%2520If%2520You%2520Can%2520Trust%2520Them%26entry.906535625%3DFrancisco%2520Caetano%2520and%2520Christiaan%2520Viviers%2520and%2520Luis%2520A.%2520Zavala-Mondrag%25C3%25B3n%2520and%2520Peter%2520H.%2520N.%2520de%2520With%2520and%2520Fons%2520van%2520der%2520Sommen%26entry.1292438233%3D%2520%2520Out-of-distribution%2520%2528OOD%2529%2520detection%2520holds%2520significant%2520importance%2520across%2520many%250Aapplications.%2520While%2520semantic%2520and%2520domain-shift%2520OOD%2520problems%2520are%2520well-studied%252C%250Athis%2520work%2520focuses%2520on%2520covariate%2520shifts%2520-%2520subtle%2520variations%2520in%2520the%2520data%250Adistribution%2520that%2520can%2520degrade%2520machine%2520learning%2520performance.%2520We%2520hypothesize%2520that%250Adetecting%2520these%2520subtle%2520shifts%2520can%2520improve%2520our%2520understanding%2520of%2520in-distribution%250Aboundaries%252C%2520ultimately%2520improving%2520OOD%2520detection.%2520In%2520adversarial%2520discriminators%250Atrained%2520with%2520Batch%2520Normalization%2520%2528BN%2529%252C%2520real%2520and%2520adversarial%2520samples%2520form%250Adistinct%2520domains%2520with%2520unique%2520batch%2520statistics%2520-%2520a%2520property%2520we%2520exploit%2520for%2520OOD%250Adetection.%2520We%2520introduce%2520DisCoPatch%252C%2520an%2520unsupervised%2520Adversarial%2520Variational%250AAutoencoder%2520%2528VAE%2529%2520framework%2520that%2520harnesses%2520this%2520mechanism.%2520During%2520inference%252C%250Abatches%2520consist%2520of%2520patches%2520from%2520the%2520same%2520image%252C%2520ensuring%2520a%2520consistent%2520data%250Adistribution%2520that%2520allows%2520the%2520model%2520to%2520rely%2520on%2520batch%2520statistics.%2520DisCoPatch%2520uses%250Athe%2520VAE%2527s%2520suboptimal%2520outputs%2520%2528generated%2520and%2520reconstructed%2529%2520as%2520negative%2520samples%250Ato%2520train%2520the%2520discriminator%252C%2520thereby%2520improving%2520its%2520ability%2520to%2520delineate%2520the%250Aboundary%2520between%2520in-distribution%2520samples%2520and%2520covariate%2520shifts.%2520By%2520tightening%250Athis%2520boundary%252C%2520DisCoPatch%2520achieves%2520state-of-the-art%2520results%2520in%2520public%2520OOD%250Adetection%2520benchmarks.%2520The%2520proposed%2520model%2520not%2520only%2520excels%2520in%2520detecting%2520covariate%250Ashifts%252C%2520achieving%252095.5%2525%2520AUROC%2520on%2520ImageNet-1K%2528-C%2529%2520but%2520also%2520outperforms%2520all%2520prior%250Amethods%2520on%2520public%2520Near-OOD%2520%252895.0%2525%2529%2520benchmarks.%2520With%2520a%2520compact%2520model%2520size%2520of%250A25MB%252C%2520it%2520achieves%2520high%2520OOD%2520detection%2520performance%2520at%2520notably%2520lower%2520latency%2520than%250Aexisting%2520methods%252C%2520making%2520it%2520an%2520efficient%2520and%2520practical%2520solution%2520for%2520real-world%250AOOD%2520detection%2520applications.%2520The%2520code%2520will%2520be%2520made%2520publicly%2520available%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08005v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DisCoPatch%3A%20Batch%20Statistics%20Are%20All%20You%20Need%20For%20OOD%20Detection%2C%20But%0A%20%20Only%20If%20You%20Can%20Trust%20Them&entry.906535625=Francisco%20Caetano%20and%20Christiaan%20Viviers%20and%20Luis%20A.%20Zavala-Mondrag%C3%B3n%20and%20Peter%20H.%20N.%20de%20With%20and%20Fons%20van%20der%20Sommen&entry.1292438233=%20%20Out-of-distribution%20%28OOD%29%20detection%20holds%20significant%20importance%20across%20many%0Aapplications.%20While%20semantic%20and%20domain-shift%20OOD%20problems%20are%20well-studied%2C%0Athis%20work%20focuses%20on%20covariate%20shifts%20-%20subtle%20variations%20in%20the%20data%0Adistribution%20that%20can%20degrade%20machine%20learning%20performance.%20We%20hypothesize%20that%0Adetecting%20these%20subtle%20shifts%20can%20improve%20our%20understanding%20of%20in-distribution%0Aboundaries%2C%20ultimately%20improving%20OOD%20detection.%20In%20adversarial%20discriminators%0Atrained%20with%20Batch%20Normalization%20%28BN%29%2C%20real%20and%20adversarial%20samples%20form%0Adistinct%20domains%20with%20unique%20batch%20statistics%20-%20a%20property%20we%20exploit%20for%20OOD%0Adetection.%20We%20introduce%20DisCoPatch%2C%20an%20unsupervised%20Adversarial%20Variational%0AAutoencoder%20%28VAE%29%20framework%20that%20harnesses%20this%20mechanism.%20During%20inference%2C%0Abatches%20consist%20of%20patches%20from%20the%20same%20image%2C%20ensuring%20a%20consistent%20data%0Adistribution%20that%20allows%20the%20model%20to%20rely%20on%20batch%20statistics.%20DisCoPatch%20uses%0Athe%20VAE%27s%20suboptimal%20outputs%20%28generated%20and%20reconstructed%29%20as%20negative%20samples%0Ato%20train%20the%20discriminator%2C%20thereby%20improving%20its%20ability%20to%20delineate%20the%0Aboundary%20between%20in-distribution%20samples%20and%20covariate%20shifts.%20By%20tightening%0Athis%20boundary%2C%20DisCoPatch%20achieves%20state-of-the-art%20results%20in%20public%20OOD%0Adetection%20benchmarks.%20The%20proposed%20model%20not%20only%20excels%20in%20detecting%20covariate%0Ashifts%2C%20achieving%2095.5%25%20AUROC%20on%20ImageNet-1K%28-C%29%20but%20also%20outperforms%20all%20prior%0Amethods%20on%20public%20Near-OOD%20%2895.0%25%29%20benchmarks.%20With%20a%20compact%20model%20size%20of%0A25MB%2C%20it%20achieves%20high%20OOD%20detection%20performance%20at%20notably%20lower%20latency%20than%0Aexisting%20methods%2C%20making%20it%20an%20efficient%20and%20practical%20solution%20for%20real-world%0AOOD%20detection%20applications.%20The%20code%20will%20be%20made%20publicly%20available%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08005v2&entry.124074799=Read"},
{"title": "From Low Intrinsic Dimensionality to Non-Vacuous Generalization Bounds\n  in Deep Multi-Task Learning", "author": "Hossein Zakerinia and Dorsa Ghobadi and Christoph H. Lampert", "abstract": "  Deep learning methods are known to generalize well from training to future\ndata, even in an overparametrized regime, where they could easily overfit. One\nexplanation for this phenomenon is that even when their *ambient\ndimensionality*, (i.e. the number of parameters) is large, the models'\n*intrinsic dimensionality* is small; specifically, their learning takes place\nin a small subspace of all possible weight configurations. In this work, we\nconfirm this phenomenon in the setting of *deep multi-task learning*. We\nintroduce a method to parametrize multi-task network directly in the\nlow-dimensional space, facilitated by the use of *random expansions*\ntechniques. We then show that high-accuracy multi-task solutions can be found\nwith much smaller intrinsic dimensionality (fewer free parameters) than what\nsingle-task learning requires. Subsequently, we show that the low-dimensional\nrepresentations in combination with *weight compression* and *PAC-Bayesian*\nreasoning lead to the *first non-vacuous generalization bounds* for deep\nmulti-task networks.\n", "link": "http://arxiv.org/abs/2501.19067v2", "date": "2025-05-21", "relevancy": 2.0354, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5125}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5082}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Low%20Intrinsic%20Dimensionality%20to%20Non-Vacuous%20Generalization%20Bounds%0A%20%20in%20Deep%20Multi-Task%20Learning&body=Title%3A%20From%20Low%20Intrinsic%20Dimensionality%20to%20Non-Vacuous%20Generalization%20Bounds%0A%20%20in%20Deep%20Multi-Task%20Learning%0AAuthor%3A%20Hossein%20Zakerinia%20and%20Dorsa%20Ghobadi%20and%20Christoph%20H.%20Lampert%0AAbstract%3A%20%20%20Deep%20learning%20methods%20are%20known%20to%20generalize%20well%20from%20training%20to%20future%0Adata%2C%20even%20in%20an%20overparametrized%20regime%2C%20where%20they%20could%20easily%20overfit.%20One%0Aexplanation%20for%20this%20phenomenon%20is%20that%20even%20when%20their%20%2Aambient%0Adimensionality%2A%2C%20%28i.e.%20the%20number%20of%20parameters%29%20is%20large%2C%20the%20models%27%0A%2Aintrinsic%20dimensionality%2A%20is%20small%3B%20specifically%2C%20their%20learning%20takes%20place%0Ain%20a%20small%20subspace%20of%20all%20possible%20weight%20configurations.%20In%20this%20work%2C%20we%0Aconfirm%20this%20phenomenon%20in%20the%20setting%20of%20%2Adeep%20multi-task%20learning%2A.%20We%0Aintroduce%20a%20method%20to%20parametrize%20multi-task%20network%20directly%20in%20the%0Alow-dimensional%20space%2C%20facilitated%20by%20the%20use%20of%20%2Arandom%20expansions%2A%0Atechniques.%20We%20then%20show%20that%20high-accuracy%20multi-task%20solutions%20can%20be%20found%0Awith%20much%20smaller%20intrinsic%20dimensionality%20%28fewer%20free%20parameters%29%20than%20what%0Asingle-task%20learning%20requires.%20Subsequently%2C%20we%20show%20that%20the%20low-dimensional%0Arepresentations%20in%20combination%20with%20%2Aweight%20compression%2A%20and%20%2APAC-Bayesian%2A%0Areasoning%20lead%20to%20the%20%2Afirst%20non-vacuous%20generalization%20bounds%2A%20for%20deep%0Amulti-task%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19067v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Low%2520Intrinsic%2520Dimensionality%2520to%2520Non-Vacuous%2520Generalization%2520Bounds%250A%2520%2520in%2520Deep%2520Multi-Task%2520Learning%26entry.906535625%3DHossein%2520Zakerinia%2520and%2520Dorsa%2520Ghobadi%2520and%2520Christoph%2520H.%2520Lampert%26entry.1292438233%3D%2520%2520Deep%2520learning%2520methods%2520are%2520known%2520to%2520generalize%2520well%2520from%2520training%2520to%2520future%250Adata%252C%2520even%2520in%2520an%2520overparametrized%2520regime%252C%2520where%2520they%2520could%2520easily%2520overfit.%2520One%250Aexplanation%2520for%2520this%2520phenomenon%2520is%2520that%2520even%2520when%2520their%2520%252Aambient%250Adimensionality%252A%252C%2520%2528i.e.%2520the%2520number%2520of%2520parameters%2529%2520is%2520large%252C%2520the%2520models%2527%250A%252Aintrinsic%2520dimensionality%252A%2520is%2520small%253B%2520specifically%252C%2520their%2520learning%2520takes%2520place%250Ain%2520a%2520small%2520subspace%2520of%2520all%2520possible%2520weight%2520configurations.%2520In%2520this%2520work%252C%2520we%250Aconfirm%2520this%2520phenomenon%2520in%2520the%2520setting%2520of%2520%252Adeep%2520multi-task%2520learning%252A.%2520We%250Aintroduce%2520a%2520method%2520to%2520parametrize%2520multi-task%2520network%2520directly%2520in%2520the%250Alow-dimensional%2520space%252C%2520facilitated%2520by%2520the%2520use%2520of%2520%252Arandom%2520expansions%252A%250Atechniques.%2520We%2520then%2520show%2520that%2520high-accuracy%2520multi-task%2520solutions%2520can%2520be%2520found%250Awith%2520much%2520smaller%2520intrinsic%2520dimensionality%2520%2528fewer%2520free%2520parameters%2529%2520than%2520what%250Asingle-task%2520learning%2520requires.%2520Subsequently%252C%2520we%2520show%2520that%2520the%2520low-dimensional%250Arepresentations%2520in%2520combination%2520with%2520%252Aweight%2520compression%252A%2520and%2520%252APAC-Bayesian%252A%250Areasoning%2520lead%2520to%2520the%2520%252Afirst%2520non-vacuous%2520generalization%2520bounds%252A%2520for%2520deep%250Amulti-task%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19067v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Low%20Intrinsic%20Dimensionality%20to%20Non-Vacuous%20Generalization%20Bounds%0A%20%20in%20Deep%20Multi-Task%20Learning&entry.906535625=Hossein%20Zakerinia%20and%20Dorsa%20Ghobadi%20and%20Christoph%20H.%20Lampert&entry.1292438233=%20%20Deep%20learning%20methods%20are%20known%20to%20generalize%20well%20from%20training%20to%20future%0Adata%2C%20even%20in%20an%20overparametrized%20regime%2C%20where%20they%20could%20easily%20overfit.%20One%0Aexplanation%20for%20this%20phenomenon%20is%20that%20even%20when%20their%20%2Aambient%0Adimensionality%2A%2C%20%28i.e.%20the%20number%20of%20parameters%29%20is%20large%2C%20the%20models%27%0A%2Aintrinsic%20dimensionality%2A%20is%20small%3B%20specifically%2C%20their%20learning%20takes%20place%0Ain%20a%20small%20subspace%20of%20all%20possible%20weight%20configurations.%20In%20this%20work%2C%20we%0Aconfirm%20this%20phenomenon%20in%20the%20setting%20of%20%2Adeep%20multi-task%20learning%2A.%20We%0Aintroduce%20a%20method%20to%20parametrize%20multi-task%20network%20directly%20in%20the%0Alow-dimensional%20space%2C%20facilitated%20by%20the%20use%20of%20%2Arandom%20expansions%2A%0Atechniques.%20We%20then%20show%20that%20high-accuracy%20multi-task%20solutions%20can%20be%20found%0Awith%20much%20smaller%20intrinsic%20dimensionality%20%28fewer%20free%20parameters%29%20than%20what%0Asingle-task%20learning%20requires.%20Subsequently%2C%20we%20show%20that%20the%20low-dimensional%0Arepresentations%20in%20combination%20with%20%2Aweight%20compression%2A%20and%20%2APAC-Bayesian%2A%0Areasoning%20lead%20to%20the%20%2Afirst%20non-vacuous%20generalization%20bounds%2A%20for%20deep%0Amulti-task%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19067v2&entry.124074799=Read"},
{"title": "Projection-Based Correction for Enhancing Deep Inverse Networks", "author": "Jorge Bacca", "abstract": "  Deep learning-based models have demonstrated remarkable success in solving\nillposed inverse problems; however, many fail to strictly adhere to the\nphysical constraints imposed by the measurement process. In this work, we\nintroduce a projection-based correction method to enhance the inference of deep\ninverse networks by ensuring consistency with the forward model. Specifically,\ngiven an initial estimate from a learned reconstruction network, we apply a\nprojection step that constrains the solution to lie within the valid solution\nspace of the inverse problem. We theoretically demonstrate that if the recovery\nmodel is a well-trained deep inverse network, the solution can be decomposed\ninto range-space and null-space components, where the projection-based\ncorrection reduces to an identity transformation. Extensive simulations and\nexperiments validate the proposed method, demonstrating improved reconstruction\naccuracy across diverse inverse problems and deep network architectures.\n", "link": "http://arxiv.org/abs/2505.15777v1", "date": "2025-05-21", "relevancy": 2.0374, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5119}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5104}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Projection-Based%20Correction%20for%20Enhancing%20Deep%20Inverse%20Networks&body=Title%3A%20Projection-Based%20Correction%20for%20Enhancing%20Deep%20Inverse%20Networks%0AAuthor%3A%20Jorge%20Bacca%0AAbstract%3A%20%20%20Deep%20learning-based%20models%20have%20demonstrated%20remarkable%20success%20in%20solving%0Aillposed%20inverse%20problems%3B%20however%2C%20many%20fail%20to%20strictly%20adhere%20to%20the%0Aphysical%20constraints%20imposed%20by%20the%20measurement%20process.%20In%20this%20work%2C%20we%0Aintroduce%20a%20projection-based%20correction%20method%20to%20enhance%20the%20inference%20of%20deep%0Ainverse%20networks%20by%20ensuring%20consistency%20with%20the%20forward%20model.%20Specifically%2C%0Agiven%20an%20initial%20estimate%20from%20a%20learned%20reconstruction%20network%2C%20we%20apply%20a%0Aprojection%20step%20that%20constrains%20the%20solution%20to%20lie%20within%20the%20valid%20solution%0Aspace%20of%20the%20inverse%20problem.%20We%20theoretically%20demonstrate%20that%20if%20the%20recovery%0Amodel%20is%20a%20well-trained%20deep%20inverse%20network%2C%20the%20solution%20can%20be%20decomposed%0Ainto%20range-space%20and%20null-space%20components%2C%20where%20the%20projection-based%0Acorrection%20reduces%20to%20an%20identity%20transformation.%20Extensive%20simulations%20and%0Aexperiments%20validate%20the%20proposed%20method%2C%20demonstrating%20improved%20reconstruction%0Aaccuracy%20across%20diverse%20inverse%20problems%20and%20deep%20network%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProjection-Based%2520Correction%2520for%2520Enhancing%2520Deep%2520Inverse%2520Networks%26entry.906535625%3DJorge%2520Bacca%26entry.1292438233%3D%2520%2520Deep%2520learning-based%2520models%2520have%2520demonstrated%2520remarkable%2520success%2520in%2520solving%250Aillposed%2520inverse%2520problems%253B%2520however%252C%2520many%2520fail%2520to%2520strictly%2520adhere%2520to%2520the%250Aphysical%2520constraints%2520imposed%2520by%2520the%2520measurement%2520process.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520a%2520projection-based%2520correction%2520method%2520to%2520enhance%2520the%2520inference%2520of%2520deep%250Ainverse%2520networks%2520by%2520ensuring%2520consistency%2520with%2520the%2520forward%2520model.%2520Specifically%252C%250Agiven%2520an%2520initial%2520estimate%2520from%2520a%2520learned%2520reconstruction%2520network%252C%2520we%2520apply%2520a%250Aprojection%2520step%2520that%2520constrains%2520the%2520solution%2520to%2520lie%2520within%2520the%2520valid%2520solution%250Aspace%2520of%2520the%2520inverse%2520problem.%2520We%2520theoretically%2520demonstrate%2520that%2520if%2520the%2520recovery%250Amodel%2520is%2520a%2520well-trained%2520deep%2520inverse%2520network%252C%2520the%2520solution%2520can%2520be%2520decomposed%250Ainto%2520range-space%2520and%2520null-space%2520components%252C%2520where%2520the%2520projection-based%250Acorrection%2520reduces%2520to%2520an%2520identity%2520transformation.%2520Extensive%2520simulations%2520and%250Aexperiments%2520validate%2520the%2520proposed%2520method%252C%2520demonstrating%2520improved%2520reconstruction%250Aaccuracy%2520across%2520diverse%2520inverse%2520problems%2520and%2520deep%2520network%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Projection-Based%20Correction%20for%20Enhancing%20Deep%20Inverse%20Networks&entry.906535625=Jorge%20Bacca&entry.1292438233=%20%20Deep%20learning-based%20models%20have%20demonstrated%20remarkable%20success%20in%20solving%0Aillposed%20inverse%20problems%3B%20however%2C%20many%20fail%20to%20strictly%20adhere%20to%20the%0Aphysical%20constraints%20imposed%20by%20the%20measurement%20process.%20In%20this%20work%2C%20we%0Aintroduce%20a%20projection-based%20correction%20method%20to%20enhance%20the%20inference%20of%20deep%0Ainverse%20networks%20by%20ensuring%20consistency%20with%20the%20forward%20model.%20Specifically%2C%0Agiven%20an%20initial%20estimate%20from%20a%20learned%20reconstruction%20network%2C%20we%20apply%20a%0Aprojection%20step%20that%20constrains%20the%20solution%20to%20lie%20within%20the%20valid%20solution%0Aspace%20of%20the%20inverse%20problem.%20We%20theoretically%20demonstrate%20that%20if%20the%20recovery%0Amodel%20is%20a%20well-trained%20deep%20inverse%20network%2C%20the%20solution%20can%20be%20decomposed%0Ainto%20range-space%20and%20null-space%20components%2C%20where%20the%20projection-based%0Acorrection%20reduces%20to%20an%20identity%20transformation.%20Extensive%20simulations%20and%0Aexperiments%20validate%20the%20proposed%20method%2C%20demonstrating%20improved%20reconstruction%0Aaccuracy%20across%20diverse%20inverse%20problems%20and%20deep%20network%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15777v1&entry.124074799=Read"},
{"title": "Uncertainty quantification for improving radiomic-based models in\n  radiation pneumonitis prediction", "author": "Chanon Puttanawarut and Romen Samuel Wabina and Nat Sirirutbunkajorn", "abstract": "  Background: Radiation pneumonitis is a side effect of thoracic radiation\ntherapy. Recently, machine learning models with radiomic features have improved\nradiation pneumonitis prediction by capturing spatial information. To further\nsupport clinical decision-making, this study explores the role of post hoc\nuncertainty quantification methods in enhancing model uncertainty estimate.\nMethods: We retrospectively analyzed a cohort of 101 esophageal cancer\npatients. This study evaluated four machine learning models: logistic\nregression, support vector machines, extreme gradient boosting, and random\nforest, using 15 dosimetric, 79 dosiomic, and 237 radiomic features to predict\nradiation pneumonitis. We applied uncertainty quantification methods, including\nPlatt scaling, isotonic regression, Venn-ABERS predictor, and conformal\nprediction, to quantify uncertainty. Model performance was assessed through an\narea under the receiver operating characteristic curve (AUROC), area under the\nprecision-recall curve (AUPRC), and adaptive calibration error using\nleave-one-out cross-validation. Results: Highest AUROC is achieved by the\nlogistic regression model with the conformal prediction method (AUROC\n0.75+-0.01, AUPRC 0.74+-0.01) at a certainty cut point of 0.8. Highest AUPRC of\n0.82+-0.02 (with AUROC of 0.67+-0.04) achieved by The extreme gradient boosting\nmodel with conformal prediction at the 0.9 certainty threshold. Radiomic and\ndosiomic features improve both discriminative and calibration performance.\nConclusions: Integrating uncertainty quantification into machine learning\nmodels with radiomic and dosiomic features may improve both predictive accuracy\nand calibration, supporting more reliable clinical decision-making. The\nfindings emphasize the value of uncertainty quantification methods in enhancing\napplicability of predictive models for radiation pneumonitis in healthcare\nsettings.\n", "link": "http://arxiv.org/abs/2412.19511v3", "date": "2025-05-21", "relevancy": 1.4775, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5155}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5019}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty%20quantification%20for%20improving%20radiomic-based%20models%20in%0A%20%20radiation%20pneumonitis%20prediction&body=Title%3A%20Uncertainty%20quantification%20for%20improving%20radiomic-based%20models%20in%0A%20%20radiation%20pneumonitis%20prediction%0AAuthor%3A%20Chanon%20Puttanawarut%20and%20Romen%20Samuel%20Wabina%20and%20Nat%20Sirirutbunkajorn%0AAbstract%3A%20%20%20Background%3A%20Radiation%20pneumonitis%20is%20a%20side%20effect%20of%20thoracic%20radiation%0Atherapy.%20Recently%2C%20machine%20learning%20models%20with%20radiomic%20features%20have%20improved%0Aradiation%20pneumonitis%20prediction%20by%20capturing%20spatial%20information.%20To%20further%0Asupport%20clinical%20decision-making%2C%20this%20study%20explores%20the%20role%20of%20post%20hoc%0Auncertainty%20quantification%20methods%20in%20enhancing%20model%20uncertainty%20estimate.%0AMethods%3A%20We%20retrospectively%20analyzed%20a%20cohort%20of%20101%20esophageal%20cancer%0Apatients.%20This%20study%20evaluated%20four%20machine%20learning%20models%3A%20logistic%0Aregression%2C%20support%20vector%20machines%2C%20extreme%20gradient%20boosting%2C%20and%20random%0Aforest%2C%20using%2015%20dosimetric%2C%2079%20dosiomic%2C%20and%20237%20radiomic%20features%20to%20predict%0Aradiation%20pneumonitis.%20We%20applied%20uncertainty%20quantification%20methods%2C%20including%0APlatt%20scaling%2C%20isotonic%20regression%2C%20Venn-ABERS%20predictor%2C%20and%20conformal%0Aprediction%2C%20to%20quantify%20uncertainty.%20Model%20performance%20was%20assessed%20through%20an%0Aarea%20under%20the%20receiver%20operating%20characteristic%20curve%20%28AUROC%29%2C%20area%20under%20the%0Aprecision-recall%20curve%20%28AUPRC%29%2C%20and%20adaptive%20calibration%20error%20using%0Aleave-one-out%20cross-validation.%20Results%3A%20Highest%20AUROC%20is%20achieved%20by%20the%0Alogistic%20regression%20model%20with%20the%20conformal%20prediction%20method%20%28AUROC%0A0.75%2B-0.01%2C%20AUPRC%200.74%2B-0.01%29%20at%20a%20certainty%20cut%20point%20of%200.8.%20Highest%20AUPRC%20of%0A0.82%2B-0.02%20%28with%20AUROC%20of%200.67%2B-0.04%29%20achieved%20by%20The%20extreme%20gradient%20boosting%0Amodel%20with%20conformal%20prediction%20at%20the%200.9%20certainty%20threshold.%20Radiomic%20and%0Adosiomic%20features%20improve%20both%20discriminative%20and%20calibration%20performance.%0AConclusions%3A%20Integrating%20uncertainty%20quantification%20into%20machine%20learning%0Amodels%20with%20radiomic%20and%20dosiomic%20features%20may%20improve%20both%20predictive%20accuracy%0Aand%20calibration%2C%20supporting%20more%20reliable%20clinical%20decision-making.%20The%0Afindings%20emphasize%20the%20value%20of%20uncertainty%20quantification%20methods%20in%20enhancing%0Aapplicability%20of%20predictive%20models%20for%20radiation%20pneumonitis%20in%20healthcare%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19511v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty%2520quantification%2520for%2520improving%2520radiomic-based%2520models%2520in%250A%2520%2520radiation%2520pneumonitis%2520prediction%26entry.906535625%3DChanon%2520Puttanawarut%2520and%2520Romen%2520Samuel%2520Wabina%2520and%2520Nat%2520Sirirutbunkajorn%26entry.1292438233%3D%2520%2520Background%253A%2520Radiation%2520pneumonitis%2520is%2520a%2520side%2520effect%2520of%2520thoracic%2520radiation%250Atherapy.%2520Recently%252C%2520machine%2520learning%2520models%2520with%2520radiomic%2520features%2520have%2520improved%250Aradiation%2520pneumonitis%2520prediction%2520by%2520capturing%2520spatial%2520information.%2520To%2520further%250Asupport%2520clinical%2520decision-making%252C%2520this%2520study%2520explores%2520the%2520role%2520of%2520post%2520hoc%250Auncertainty%2520quantification%2520methods%2520in%2520enhancing%2520model%2520uncertainty%2520estimate.%250AMethods%253A%2520We%2520retrospectively%2520analyzed%2520a%2520cohort%2520of%2520101%2520esophageal%2520cancer%250Apatients.%2520This%2520study%2520evaluated%2520four%2520machine%2520learning%2520models%253A%2520logistic%250Aregression%252C%2520support%2520vector%2520machines%252C%2520extreme%2520gradient%2520boosting%252C%2520and%2520random%250Aforest%252C%2520using%252015%2520dosimetric%252C%252079%2520dosiomic%252C%2520and%2520237%2520radiomic%2520features%2520to%2520predict%250Aradiation%2520pneumonitis.%2520We%2520applied%2520uncertainty%2520quantification%2520methods%252C%2520including%250APlatt%2520scaling%252C%2520isotonic%2520regression%252C%2520Venn-ABERS%2520predictor%252C%2520and%2520conformal%250Aprediction%252C%2520to%2520quantify%2520uncertainty.%2520Model%2520performance%2520was%2520assessed%2520through%2520an%250Aarea%2520under%2520the%2520receiver%2520operating%2520characteristic%2520curve%2520%2528AUROC%2529%252C%2520area%2520under%2520the%250Aprecision-recall%2520curve%2520%2528AUPRC%2529%252C%2520and%2520adaptive%2520calibration%2520error%2520using%250Aleave-one-out%2520cross-validation.%2520Results%253A%2520Highest%2520AUROC%2520is%2520achieved%2520by%2520the%250Alogistic%2520regression%2520model%2520with%2520the%2520conformal%2520prediction%2520method%2520%2528AUROC%250A0.75%252B-0.01%252C%2520AUPRC%25200.74%252B-0.01%2529%2520at%2520a%2520certainty%2520cut%2520point%2520of%25200.8.%2520Highest%2520AUPRC%2520of%250A0.82%252B-0.02%2520%2528with%2520AUROC%2520of%25200.67%252B-0.04%2529%2520achieved%2520by%2520The%2520extreme%2520gradient%2520boosting%250Amodel%2520with%2520conformal%2520prediction%2520at%2520the%25200.9%2520certainty%2520threshold.%2520Radiomic%2520and%250Adosiomic%2520features%2520improve%2520both%2520discriminative%2520and%2520calibration%2520performance.%250AConclusions%253A%2520Integrating%2520uncertainty%2520quantification%2520into%2520machine%2520learning%250Amodels%2520with%2520radiomic%2520and%2520dosiomic%2520features%2520may%2520improve%2520both%2520predictive%2520accuracy%250Aand%2520calibration%252C%2520supporting%2520more%2520reliable%2520clinical%2520decision-making.%2520The%250Afindings%2520emphasize%2520the%2520value%2520of%2520uncertainty%2520quantification%2520methods%2520in%2520enhancing%250Aapplicability%2520of%2520predictive%2520models%2520for%2520radiation%2520pneumonitis%2520in%2520healthcare%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19511v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20quantification%20for%20improving%20radiomic-based%20models%20in%0A%20%20radiation%20pneumonitis%20prediction&entry.906535625=Chanon%20Puttanawarut%20and%20Romen%20Samuel%20Wabina%20and%20Nat%20Sirirutbunkajorn&entry.1292438233=%20%20Background%3A%20Radiation%20pneumonitis%20is%20a%20side%20effect%20of%20thoracic%20radiation%0Atherapy.%20Recently%2C%20machine%20learning%20models%20with%20radiomic%20features%20have%20improved%0Aradiation%20pneumonitis%20prediction%20by%20capturing%20spatial%20information.%20To%20further%0Asupport%20clinical%20decision-making%2C%20this%20study%20explores%20the%20role%20of%20post%20hoc%0Auncertainty%20quantification%20methods%20in%20enhancing%20model%20uncertainty%20estimate.%0AMethods%3A%20We%20retrospectively%20analyzed%20a%20cohort%20of%20101%20esophageal%20cancer%0Apatients.%20This%20study%20evaluated%20four%20machine%20learning%20models%3A%20logistic%0Aregression%2C%20support%20vector%20machines%2C%20extreme%20gradient%20boosting%2C%20and%20random%0Aforest%2C%20using%2015%20dosimetric%2C%2079%20dosiomic%2C%20and%20237%20radiomic%20features%20to%20predict%0Aradiation%20pneumonitis.%20We%20applied%20uncertainty%20quantification%20methods%2C%20including%0APlatt%20scaling%2C%20isotonic%20regression%2C%20Venn-ABERS%20predictor%2C%20and%20conformal%0Aprediction%2C%20to%20quantify%20uncertainty.%20Model%20performance%20was%20assessed%20through%20an%0Aarea%20under%20the%20receiver%20operating%20characteristic%20curve%20%28AUROC%29%2C%20area%20under%20the%0Aprecision-recall%20curve%20%28AUPRC%29%2C%20and%20adaptive%20calibration%20error%20using%0Aleave-one-out%20cross-validation.%20Results%3A%20Highest%20AUROC%20is%20achieved%20by%20the%0Alogistic%20regression%20model%20with%20the%20conformal%20prediction%20method%20%28AUROC%0A0.75%2B-0.01%2C%20AUPRC%200.74%2B-0.01%29%20at%20a%20certainty%20cut%20point%20of%200.8.%20Highest%20AUPRC%20of%0A0.82%2B-0.02%20%28with%20AUROC%20of%200.67%2B-0.04%29%20achieved%20by%20The%20extreme%20gradient%20boosting%0Amodel%20with%20conformal%20prediction%20at%20the%200.9%20certainty%20threshold.%20Radiomic%20and%0Adosiomic%20features%20improve%20both%20discriminative%20and%20calibration%20performance.%0AConclusions%3A%20Integrating%20uncertainty%20quantification%20into%20machine%20learning%0Amodels%20with%20radiomic%20and%20dosiomic%20features%20may%20improve%20both%20predictive%20accuracy%0Aand%20calibration%2C%20supporting%20more%20reliable%20clinical%20decision-making.%20The%0Afindings%20emphasize%20the%20value%20of%20uncertainty%20quantification%20methods%20in%20enhancing%0Aapplicability%20of%20predictive%20models%20for%20radiation%20pneumonitis%20in%20healthcare%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19511v3&entry.124074799=Read"},
{"title": "Learning Heuristics for Transit Network Design and Improvement with Deep\n  Reinforcement Learning", "author": "Andrew Holliday and Ahmed El-Geneidy and Gregory Dudek", "abstract": "  Planning a network of public transit routes is a challenging optimization\nproblem. Metaheuristic algorithms search through the space of possible transit\nnetworks by applying heuristics that randomly alter routes in a network. The\ndesign of these heuristics has a major impact on the quality of the result. In\nthis paper, we use deep reinforcement learning to train a graph neural net to\nprovide heuristics for an evolutionary algorithm. These neural heuristics\nimprove the algorithm's results on benchmark synthetic cities with 70 nodes or\nmore, and achieve new state-of-the-art results on the challenging Mumford\nbenchmark. They also improve upon a simulation of the real transit network in\nthe city of Laval, Canada, by 52% and 25% on two key metrics, and offer cost\nsavings of up to 19% over the city's existing transit network.\n", "link": "http://arxiv.org/abs/2404.05894v5", "date": "2025-05-21", "relevancy": 1.8539, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4687}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4636}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4582}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Heuristics%20for%20Transit%20Network%20Design%20and%20Improvement%20with%20Deep%0A%20%20Reinforcement%20Learning&body=Title%3A%20Learning%20Heuristics%20for%20Transit%20Network%20Design%20and%20Improvement%20with%20Deep%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Andrew%20Holliday%20and%20Ahmed%20El-Geneidy%20and%20Gregory%20Dudek%0AAbstract%3A%20%20%20Planning%20a%20network%20of%20public%20transit%20routes%20is%20a%20challenging%20optimization%0Aproblem.%20Metaheuristic%20algorithms%20search%20through%20the%20space%20of%20possible%20transit%0Anetworks%20by%20applying%20heuristics%20that%20randomly%20alter%20routes%20in%20a%20network.%20The%0Adesign%20of%20these%20heuristics%20has%20a%20major%20impact%20on%20the%20quality%20of%20the%20result.%20In%0Athis%20paper%2C%20we%20use%20deep%20reinforcement%20learning%20to%20train%20a%20graph%20neural%20net%20to%0Aprovide%20heuristics%20for%20an%20evolutionary%20algorithm.%20These%20neural%20heuristics%0Aimprove%20the%20algorithm%27s%20results%20on%20benchmark%20synthetic%20cities%20with%2070%20nodes%20or%0Amore%2C%20and%20achieve%20new%20state-of-the-art%20results%20on%20the%20challenging%20Mumford%0Abenchmark.%20They%20also%20improve%20upon%20a%20simulation%20of%20the%20real%20transit%20network%20in%0Athe%20city%20of%20Laval%2C%20Canada%2C%20by%2052%25%20and%2025%25%20on%20two%20key%20metrics%2C%20and%20offer%20cost%0Asavings%20of%20up%20to%2019%25%20over%20the%20city%27s%20existing%20transit%20network.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05894v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Heuristics%2520for%2520Transit%2520Network%2520Design%2520and%2520Improvement%2520with%2520Deep%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DAndrew%2520Holliday%2520and%2520Ahmed%2520El-Geneidy%2520and%2520Gregory%2520Dudek%26entry.1292438233%3D%2520%2520Planning%2520a%2520network%2520of%2520public%2520transit%2520routes%2520is%2520a%2520challenging%2520optimization%250Aproblem.%2520Metaheuristic%2520algorithms%2520search%2520through%2520the%2520space%2520of%2520possible%2520transit%250Anetworks%2520by%2520applying%2520heuristics%2520that%2520randomly%2520alter%2520routes%2520in%2520a%2520network.%2520The%250Adesign%2520of%2520these%2520heuristics%2520has%2520a%2520major%2520impact%2520on%2520the%2520quality%2520of%2520the%2520result.%2520In%250Athis%2520paper%252C%2520we%2520use%2520deep%2520reinforcement%2520learning%2520to%2520train%2520a%2520graph%2520neural%2520net%2520to%250Aprovide%2520heuristics%2520for%2520an%2520evolutionary%2520algorithm.%2520These%2520neural%2520heuristics%250Aimprove%2520the%2520algorithm%2527s%2520results%2520on%2520benchmark%2520synthetic%2520cities%2520with%252070%2520nodes%2520or%250Amore%252C%2520and%2520achieve%2520new%2520state-of-the-art%2520results%2520on%2520the%2520challenging%2520Mumford%250Abenchmark.%2520They%2520also%2520improve%2520upon%2520a%2520simulation%2520of%2520the%2520real%2520transit%2520network%2520in%250Athe%2520city%2520of%2520Laval%252C%2520Canada%252C%2520by%252052%2525%2520and%252025%2525%2520on%2520two%2520key%2520metrics%252C%2520and%2520offer%2520cost%250Asavings%2520of%2520up%2520to%252019%2525%2520over%2520the%2520city%2527s%2520existing%2520transit%2520network.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.05894v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Heuristics%20for%20Transit%20Network%20Design%20and%20Improvement%20with%20Deep%0A%20%20Reinforcement%20Learning&entry.906535625=Andrew%20Holliday%20and%20Ahmed%20El-Geneidy%20and%20Gregory%20Dudek&entry.1292438233=%20%20Planning%20a%20network%20of%20public%20transit%20routes%20is%20a%20challenging%20optimization%0Aproblem.%20Metaheuristic%20algorithms%20search%20through%20the%20space%20of%20possible%20transit%0Anetworks%20by%20applying%20heuristics%20that%20randomly%20alter%20routes%20in%20a%20network.%20The%0Adesign%20of%20these%20heuristics%20has%20a%20major%20impact%20on%20the%20quality%20of%20the%20result.%20In%0Athis%20paper%2C%20we%20use%20deep%20reinforcement%20learning%20to%20train%20a%20graph%20neural%20net%20to%0Aprovide%20heuristics%20for%20an%20evolutionary%20algorithm.%20These%20neural%20heuristics%0Aimprove%20the%20algorithm%27s%20results%20on%20benchmark%20synthetic%20cities%20with%2070%20nodes%20or%0Amore%2C%20and%20achieve%20new%20state-of-the-art%20results%20on%20the%20challenging%20Mumford%0Abenchmark.%20They%20also%20improve%20upon%20a%20simulation%20of%20the%20real%20transit%20network%20in%0Athe%20city%20of%20Laval%2C%20Canada%2C%20by%2052%25%20and%2025%25%20on%20two%20key%20metrics%2C%20and%20offer%20cost%0Asavings%20of%20up%20to%2019%25%20over%20the%20city%27s%20existing%20transit%20network.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05894v5&entry.124074799=Read"},
{"title": "HybridProver: Augmenting Theorem Proving with LLM-Driven Proof Synthesis\n  and Refinement", "author": "Jilin Hu and Jianyu Zhang and Yongwang Zhao and Talia Ringer", "abstract": "  Formal methods is pivotal for verifying the reliability of critical systems\nthrough rigorous mathematical proofs. However, its adoption is hindered by\nlabor-intensive manual proofs and the expertise required to use theorem\nprovers. Recent advancements in large language models (LLMs) offer new\nopportunities for automated theorem proving. Two promising approaches are\ngenerating tactics step by step and generating a whole proof directly with an\nLLM. However, existing work makes no attempt to combine the two approaches. In\nthis work, we introduce HybridProver, a dual-model proof synthesis framework\nthat combines tactic-based generation and whole-proof synthesis to harness the\nbenefits of both approaches. HybridProver generates whole proof candidates for\nevaluation directly, then extracts proof sketches from those candidates. It\nthen uses a tactic-based generation model that integrates automated tools to\ncomplete the sketches via stepwise refinement. We implement HybridProver for\nthe Isabelle theorem prover and fine-tune LLMs on our optimized Isabelle\ndatasets. Evaluation on the miniF2F dataset illustrates HybridProver's\neffectiveness. We achieve a 59.4% success rate on miniF2F, where the previous\nSOTA is 56.1%. Our ablation studies show that this SOTA result is attributable\nto combining whole-proof and tactic-based generation. Additionally, we show how\nthe dataset quality, training parameters, and sampling diversity affect the\nfinal result during automated theorem proving with LLMs. All of our code,\ndatasets, and LLMs are open source.\n", "link": "http://arxiv.org/abs/2505.15740v1", "date": "2025-05-21", "relevancy": 1.8452, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4676}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.46}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.46}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HybridProver%3A%20Augmenting%20Theorem%20Proving%20with%20LLM-Driven%20Proof%20Synthesis%0A%20%20and%20Refinement&body=Title%3A%20HybridProver%3A%20Augmenting%20Theorem%20Proving%20with%20LLM-Driven%20Proof%20Synthesis%0A%20%20and%20Refinement%0AAuthor%3A%20Jilin%20Hu%20and%20Jianyu%20Zhang%20and%20Yongwang%20Zhao%20and%20Talia%20Ringer%0AAbstract%3A%20%20%20Formal%20methods%20is%20pivotal%20for%20verifying%20the%20reliability%20of%20critical%20systems%0Athrough%20rigorous%20mathematical%20proofs.%20However%2C%20its%20adoption%20is%20hindered%20by%0Alabor-intensive%20manual%20proofs%20and%20the%20expertise%20required%20to%20use%20theorem%0Aprovers.%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20offer%20new%0Aopportunities%20for%20automated%20theorem%20proving.%20Two%20promising%20approaches%20are%0Agenerating%20tactics%20step%20by%20step%20and%20generating%20a%20whole%20proof%20directly%20with%20an%0ALLM.%20However%2C%20existing%20work%20makes%20no%20attempt%20to%20combine%20the%20two%20approaches.%20In%0Athis%20work%2C%20we%20introduce%20HybridProver%2C%20a%20dual-model%20proof%20synthesis%20framework%0Athat%20combines%20tactic-based%20generation%20and%20whole-proof%20synthesis%20to%20harness%20the%0Abenefits%20of%20both%20approaches.%20HybridProver%20generates%20whole%20proof%20candidates%20for%0Aevaluation%20directly%2C%20then%20extracts%20proof%20sketches%20from%20those%20candidates.%20It%0Athen%20uses%20a%20tactic-based%20generation%20model%20that%20integrates%20automated%20tools%20to%0Acomplete%20the%20sketches%20via%20stepwise%20refinement.%20We%20implement%20HybridProver%20for%0Athe%20Isabelle%20theorem%20prover%20and%20fine-tune%20LLMs%20on%20our%20optimized%20Isabelle%0Adatasets.%20Evaluation%20on%20the%20miniF2F%20dataset%20illustrates%20HybridProver%27s%0Aeffectiveness.%20We%20achieve%20a%2059.4%25%20success%20rate%20on%20miniF2F%2C%20where%20the%20previous%0ASOTA%20is%2056.1%25.%20Our%20ablation%20studies%20show%20that%20this%20SOTA%20result%20is%20attributable%0Ato%20combining%20whole-proof%20and%20tactic-based%20generation.%20Additionally%2C%20we%20show%20how%0Athe%20dataset%20quality%2C%20training%20parameters%2C%20and%20sampling%20diversity%20affect%20the%0Afinal%20result%20during%20automated%20theorem%20proving%20with%20LLMs.%20All%20of%20our%20code%2C%0Adatasets%2C%20and%20LLMs%20are%20open%20source.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15740v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybridProver%253A%2520Augmenting%2520Theorem%2520Proving%2520with%2520LLM-Driven%2520Proof%2520Synthesis%250A%2520%2520and%2520Refinement%26entry.906535625%3DJilin%2520Hu%2520and%2520Jianyu%2520Zhang%2520and%2520Yongwang%2520Zhao%2520and%2520Talia%2520Ringer%26entry.1292438233%3D%2520%2520Formal%2520methods%2520is%2520pivotal%2520for%2520verifying%2520the%2520reliability%2520of%2520critical%2520systems%250Athrough%2520rigorous%2520mathematical%2520proofs.%2520However%252C%2520its%2520adoption%2520is%2520hindered%2520by%250Alabor-intensive%2520manual%2520proofs%2520and%2520the%2520expertise%2520required%2520to%2520use%2520theorem%250Aprovers.%2520Recent%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520offer%2520new%250Aopportunities%2520for%2520automated%2520theorem%2520proving.%2520Two%2520promising%2520approaches%2520are%250Agenerating%2520tactics%2520step%2520by%2520step%2520and%2520generating%2520a%2520whole%2520proof%2520directly%2520with%2520an%250ALLM.%2520However%252C%2520existing%2520work%2520makes%2520no%2520attempt%2520to%2520combine%2520the%2520two%2520approaches.%2520In%250Athis%2520work%252C%2520we%2520introduce%2520HybridProver%252C%2520a%2520dual-model%2520proof%2520synthesis%2520framework%250Athat%2520combines%2520tactic-based%2520generation%2520and%2520whole-proof%2520synthesis%2520to%2520harness%2520the%250Abenefits%2520of%2520both%2520approaches.%2520HybridProver%2520generates%2520whole%2520proof%2520candidates%2520for%250Aevaluation%2520directly%252C%2520then%2520extracts%2520proof%2520sketches%2520from%2520those%2520candidates.%2520It%250Athen%2520uses%2520a%2520tactic-based%2520generation%2520model%2520that%2520integrates%2520automated%2520tools%2520to%250Acomplete%2520the%2520sketches%2520via%2520stepwise%2520refinement.%2520We%2520implement%2520HybridProver%2520for%250Athe%2520Isabelle%2520theorem%2520prover%2520and%2520fine-tune%2520LLMs%2520on%2520our%2520optimized%2520Isabelle%250Adatasets.%2520Evaluation%2520on%2520the%2520miniF2F%2520dataset%2520illustrates%2520HybridProver%2527s%250Aeffectiveness.%2520We%2520achieve%2520a%252059.4%2525%2520success%2520rate%2520on%2520miniF2F%252C%2520where%2520the%2520previous%250ASOTA%2520is%252056.1%2525.%2520Our%2520ablation%2520studies%2520show%2520that%2520this%2520SOTA%2520result%2520is%2520attributable%250Ato%2520combining%2520whole-proof%2520and%2520tactic-based%2520generation.%2520Additionally%252C%2520we%2520show%2520how%250Athe%2520dataset%2520quality%252C%2520training%2520parameters%252C%2520and%2520sampling%2520diversity%2520affect%2520the%250Afinal%2520result%2520during%2520automated%2520theorem%2520proving%2520with%2520LLMs.%2520All%2520of%2520our%2520code%252C%250Adatasets%252C%2520and%2520LLMs%2520are%2520open%2520source.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15740v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HybridProver%3A%20Augmenting%20Theorem%20Proving%20with%20LLM-Driven%20Proof%20Synthesis%0A%20%20and%20Refinement&entry.906535625=Jilin%20Hu%20and%20Jianyu%20Zhang%20and%20Yongwang%20Zhao%20and%20Talia%20Ringer&entry.1292438233=%20%20Formal%20methods%20is%20pivotal%20for%20verifying%20the%20reliability%20of%20critical%20systems%0Athrough%20rigorous%20mathematical%20proofs.%20However%2C%20its%20adoption%20is%20hindered%20by%0Alabor-intensive%20manual%20proofs%20and%20the%20expertise%20required%20to%20use%20theorem%0Aprovers.%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20offer%20new%0Aopportunities%20for%20automated%20theorem%20proving.%20Two%20promising%20approaches%20are%0Agenerating%20tactics%20step%20by%20step%20and%20generating%20a%20whole%20proof%20directly%20with%20an%0ALLM.%20However%2C%20existing%20work%20makes%20no%20attempt%20to%20combine%20the%20two%20approaches.%20In%0Athis%20work%2C%20we%20introduce%20HybridProver%2C%20a%20dual-model%20proof%20synthesis%20framework%0Athat%20combines%20tactic-based%20generation%20and%20whole-proof%20synthesis%20to%20harness%20the%0Abenefits%20of%20both%20approaches.%20HybridProver%20generates%20whole%20proof%20candidates%20for%0Aevaluation%20directly%2C%20then%20extracts%20proof%20sketches%20from%20those%20candidates.%20It%0Athen%20uses%20a%20tactic-based%20generation%20model%20that%20integrates%20automated%20tools%20to%0Acomplete%20the%20sketches%20via%20stepwise%20refinement.%20We%20implement%20HybridProver%20for%0Athe%20Isabelle%20theorem%20prover%20and%20fine-tune%20LLMs%20on%20our%20optimized%20Isabelle%0Adatasets.%20Evaluation%20on%20the%20miniF2F%20dataset%20illustrates%20HybridProver%27s%0Aeffectiveness.%20We%20achieve%20a%2059.4%25%20success%20rate%20on%20miniF2F%2C%20where%20the%20previous%0ASOTA%20is%2056.1%25.%20Our%20ablation%20studies%20show%20that%20this%20SOTA%20result%20is%20attributable%0Ato%20combining%20whole-proof%20and%20tactic-based%20generation.%20Additionally%2C%20we%20show%20how%0Athe%20dataset%20quality%2C%20training%20parameters%2C%20and%20sampling%20diversity%20affect%20the%0Afinal%20result%20during%20automated%20theorem%20proving%20with%20LLMs.%20All%20of%20our%20code%2C%0Adatasets%2C%20and%20LLMs%20are%20open%20source.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15740v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


