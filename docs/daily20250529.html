<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250528.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "UP-SLAM: Adaptively Structured Gaussian SLAM with Uncertainty Prediction\n  in Dynamic Environments", "author": "Wancai Zheng and Linlin Ou and Jiajie He and Libo Zhou and Xinyi Yu and Yan Wei", "abstract": "  Recent 3D Gaussian Splatting (3DGS) techniques for Visual Simultaneous\nLocalization and Mapping (SLAM) have significantly progressed in tracking and\nhigh-fidelity mapping. However, their sequential optimization framework and\nsensitivity to dynamic objects limit real-time performance and robustness in\nreal-world scenarios. We present UP-SLAM, a real-time RGB-D SLAM system for\ndynamic environments that decouples tracking and mapping through a parallelized\nframework. A probabilistic octree is employed to manage Gaussian primitives\nadaptively, enabling efficient initialization and pruning without hand-crafted\nthresholds. To robustly filter dynamic regions during tracking, we propose a\ntraining-free uncertainty estimator that fuses multi-modal residuals to\nestimate per-pixel motion uncertainty, achieving open-set dynamic object\nhandling without reliance on semantic labels. Furthermore, a temporal encoder\nis designed to enhance rendering quality. Concurrently, low-dimensional\nfeatures are efficiently transformed via a shallow multilayer perceptron to\nconstruct DINO features, which are then employed to enrich the Gaussian field\nand improve the robustness of uncertainty prediction. Extensive experiments on\nmultiple challenging datasets suggest that UP-SLAM outperforms state-of-the-art\nmethods in both localization accuracy (by 59.8%) and rendering quality (by 4.57\ndB PSNR), while maintaining real-time performance and producing reusable,\nartifact-free static maps in dynamic environments.The project:\nhttps://aczheng-cai.github.io/up_slam.github.io/\n", "link": "http://arxiv.org/abs/2505.22335v1", "date": "2025-05-28", "relevancy": 3.6175, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7738}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.7467}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.65}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UP-SLAM%3A%20Adaptively%20Structured%20Gaussian%20SLAM%20with%20Uncertainty%20Prediction%0A%20%20in%20Dynamic%20Environments&body=Title%3A%20UP-SLAM%3A%20Adaptively%20Structured%20Gaussian%20SLAM%20with%20Uncertainty%20Prediction%0A%20%20in%20Dynamic%20Environments%0AAuthor%3A%20Wancai%20Zheng%20and%20Linlin%20Ou%20and%20Jiajie%20He%20and%20Libo%20Zhou%20and%20Xinyi%20Yu%20and%20Yan%20Wei%0AAbstract%3A%20%20%20Recent%203D%20Gaussian%20Splatting%20%283DGS%29%20techniques%20for%20Visual%20Simultaneous%0ALocalization%20and%20Mapping%20%28SLAM%29%20have%20significantly%20progressed%20in%20tracking%20and%0Ahigh-fidelity%20mapping.%20However%2C%20their%20sequential%20optimization%20framework%20and%0Asensitivity%20to%20dynamic%20objects%20limit%20real-time%20performance%20and%20robustness%20in%0Areal-world%20scenarios.%20We%20present%20UP-SLAM%2C%20a%20real-time%20RGB-D%20SLAM%20system%20for%0Adynamic%20environments%20that%20decouples%20tracking%20and%20mapping%20through%20a%20parallelized%0Aframework.%20A%20probabilistic%20octree%20is%20employed%20to%20manage%20Gaussian%20primitives%0Aadaptively%2C%20enabling%20efficient%20initialization%20and%20pruning%20without%20hand-crafted%0Athresholds.%20To%20robustly%20filter%20dynamic%20regions%20during%20tracking%2C%20we%20propose%20a%0Atraining-free%20uncertainty%20estimator%20that%20fuses%20multi-modal%20residuals%20to%0Aestimate%20per-pixel%20motion%20uncertainty%2C%20achieving%20open-set%20dynamic%20object%0Ahandling%20without%20reliance%20on%20semantic%20labels.%20Furthermore%2C%20a%20temporal%20encoder%0Ais%20designed%20to%20enhance%20rendering%20quality.%20Concurrently%2C%20low-dimensional%0Afeatures%20are%20efficiently%20transformed%20via%20a%20shallow%20multilayer%20perceptron%20to%0Aconstruct%20DINO%20features%2C%20which%20are%20then%20employed%20to%20enrich%20the%20Gaussian%20field%0Aand%20improve%20the%20robustness%20of%20uncertainty%20prediction.%20Extensive%20experiments%20on%0Amultiple%20challenging%20datasets%20suggest%20that%20UP-SLAM%20outperforms%20state-of-the-art%0Amethods%20in%20both%20localization%20accuracy%20%28by%2059.8%25%29%20and%20rendering%20quality%20%28by%204.57%0AdB%20PSNR%29%2C%20while%20maintaining%20real-time%20performance%20and%20producing%20reusable%2C%0Aartifact-free%20static%20maps%20in%20dynamic%20environments.The%20project%3A%0Ahttps%3A//aczheng-cai.github.io/up_slam.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22335v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUP-SLAM%253A%2520Adaptively%2520Structured%2520Gaussian%2520SLAM%2520with%2520Uncertainty%2520Prediction%250A%2520%2520in%2520Dynamic%2520Environments%26entry.906535625%3DWancai%2520Zheng%2520and%2520Linlin%2520Ou%2520and%2520Jiajie%2520He%2520and%2520Libo%2520Zhou%2520and%2520Xinyi%2520Yu%2520and%2520Yan%2520Wei%26entry.1292438233%3D%2520%2520Recent%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520techniques%2520for%2520Visual%2520Simultaneous%250ALocalization%2520and%2520Mapping%2520%2528SLAM%2529%2520have%2520significantly%2520progressed%2520in%2520tracking%2520and%250Ahigh-fidelity%2520mapping.%2520However%252C%2520their%2520sequential%2520optimization%2520framework%2520and%250Asensitivity%2520to%2520dynamic%2520objects%2520limit%2520real-time%2520performance%2520and%2520robustness%2520in%250Areal-world%2520scenarios.%2520We%2520present%2520UP-SLAM%252C%2520a%2520real-time%2520RGB-D%2520SLAM%2520system%2520for%250Adynamic%2520environments%2520that%2520decouples%2520tracking%2520and%2520mapping%2520through%2520a%2520parallelized%250Aframework.%2520A%2520probabilistic%2520octree%2520is%2520employed%2520to%2520manage%2520Gaussian%2520primitives%250Aadaptively%252C%2520enabling%2520efficient%2520initialization%2520and%2520pruning%2520without%2520hand-crafted%250Athresholds.%2520To%2520robustly%2520filter%2520dynamic%2520regions%2520during%2520tracking%252C%2520we%2520propose%2520a%250Atraining-free%2520uncertainty%2520estimator%2520that%2520fuses%2520multi-modal%2520residuals%2520to%250Aestimate%2520per-pixel%2520motion%2520uncertainty%252C%2520achieving%2520open-set%2520dynamic%2520object%250Ahandling%2520without%2520reliance%2520on%2520semantic%2520labels.%2520Furthermore%252C%2520a%2520temporal%2520encoder%250Ais%2520designed%2520to%2520enhance%2520rendering%2520quality.%2520Concurrently%252C%2520low-dimensional%250Afeatures%2520are%2520efficiently%2520transformed%2520via%2520a%2520shallow%2520multilayer%2520perceptron%2520to%250Aconstruct%2520DINO%2520features%252C%2520which%2520are%2520then%2520employed%2520to%2520enrich%2520the%2520Gaussian%2520field%250Aand%2520improve%2520the%2520robustness%2520of%2520uncertainty%2520prediction.%2520Extensive%2520experiments%2520on%250Amultiple%2520challenging%2520datasets%2520suggest%2520that%2520UP-SLAM%2520outperforms%2520state-of-the-art%250Amethods%2520in%2520both%2520localization%2520accuracy%2520%2528by%252059.8%2525%2529%2520and%2520rendering%2520quality%2520%2528by%25204.57%250AdB%2520PSNR%2529%252C%2520while%2520maintaining%2520real-time%2520performance%2520and%2520producing%2520reusable%252C%250Aartifact-free%2520static%2520maps%2520in%2520dynamic%2520environments.The%2520project%253A%250Ahttps%253A//aczheng-cai.github.io/up_slam.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22335v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UP-SLAM%3A%20Adaptively%20Structured%20Gaussian%20SLAM%20with%20Uncertainty%20Prediction%0A%20%20in%20Dynamic%20Environments&entry.906535625=Wancai%20Zheng%20and%20Linlin%20Ou%20and%20Jiajie%20He%20and%20Libo%20Zhou%20and%20Xinyi%20Yu%20and%20Yan%20Wei&entry.1292438233=%20%20Recent%203D%20Gaussian%20Splatting%20%283DGS%29%20techniques%20for%20Visual%20Simultaneous%0ALocalization%20and%20Mapping%20%28SLAM%29%20have%20significantly%20progressed%20in%20tracking%20and%0Ahigh-fidelity%20mapping.%20However%2C%20their%20sequential%20optimization%20framework%20and%0Asensitivity%20to%20dynamic%20objects%20limit%20real-time%20performance%20and%20robustness%20in%0Areal-world%20scenarios.%20We%20present%20UP-SLAM%2C%20a%20real-time%20RGB-D%20SLAM%20system%20for%0Adynamic%20environments%20that%20decouples%20tracking%20and%20mapping%20through%20a%20parallelized%0Aframework.%20A%20probabilistic%20octree%20is%20employed%20to%20manage%20Gaussian%20primitives%0Aadaptively%2C%20enabling%20efficient%20initialization%20and%20pruning%20without%20hand-crafted%0Athresholds.%20To%20robustly%20filter%20dynamic%20regions%20during%20tracking%2C%20we%20propose%20a%0Atraining-free%20uncertainty%20estimator%20that%20fuses%20multi-modal%20residuals%20to%0Aestimate%20per-pixel%20motion%20uncertainty%2C%20achieving%20open-set%20dynamic%20object%0Ahandling%20without%20reliance%20on%20semantic%20labels.%20Furthermore%2C%20a%20temporal%20encoder%0Ais%20designed%20to%20enhance%20rendering%20quality.%20Concurrently%2C%20low-dimensional%0Afeatures%20are%20efficiently%20transformed%20via%20a%20shallow%20multilayer%20perceptron%20to%0Aconstruct%20DINO%20features%2C%20which%20are%20then%20employed%20to%20enrich%20the%20Gaussian%20field%0Aand%20improve%20the%20robustness%20of%20uncertainty%20prediction.%20Extensive%20experiments%20on%0Amultiple%20challenging%20datasets%20suggest%20that%20UP-SLAM%20outperforms%20state-of-the-art%0Amethods%20in%20both%20localization%20accuracy%20%28by%2059.8%25%29%20and%20rendering%20quality%20%28by%204.57%0AdB%20PSNR%29%2C%20while%20maintaining%20real-time%20performance%20and%20producing%20reusable%2C%0Aartifact-free%20static%20maps%20in%20dynamic%20environments.The%20project%3A%0Ahttps%3A//aczheng-cai.github.io/up_slam.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22335v1&entry.124074799=Read"},
{"title": "STDR: Spatio-Temporal Decoupling for Real-Time Dynamic Scene Rendering", "author": "Zehao Li and Hao Jiang and Yujun Cai and Jianing Chen and Baolong Bi and Shuqin Gao and Honglong Zhao and Yiwei Wang and Tianlu Mao and Zhaoqi Wang", "abstract": "  Although dynamic scene reconstruction has long been a fundamental challenge\nin 3D vision, the recent emergence of 3D Gaussian Splatting (3DGS) offers a\npromising direction by enabling high-quality, real-time rendering through\nexplicit Gaussian primitives. However, existing 3DGS-based methods for dynamic\nreconstruction often suffer from \\textit{spatio-temporal incoherence} during\ninitialization, where canonical Gaussians are constructed by aggregating\nobservations from multiple frames without temporal distinction. This results in\nspatio-temporally entangled representations, making it difficult to model\ndynamic motion accurately. To overcome this limitation, we propose\n\\textbf{STDR} (Spatio-Temporal Decoupling for Real-time rendering), a\nplug-and-play module that learns spatio-temporal probability distributions for\neach Gaussian. STDR introduces a spatio-temporal mask, a separated deformation\nfield, and a consistency regularization to jointly disentangle spatial and\ntemporal patterns. Extensive experiments demonstrate that incorporating our\nmodule into existing 3DGS-based dynamic scene reconstruction frameworks leads\nto notable improvements in both reconstruction quality and spatio-temporal\nconsistency across synthetic and real-world benchmarks.\n", "link": "http://arxiv.org/abs/2505.22400v1", "date": "2025-05-28", "relevancy": 3.3602, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7168}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6731}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6262}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STDR%3A%20Spatio-Temporal%20Decoupling%20for%20Real-Time%20Dynamic%20Scene%20Rendering&body=Title%3A%20STDR%3A%20Spatio-Temporal%20Decoupling%20for%20Real-Time%20Dynamic%20Scene%20Rendering%0AAuthor%3A%20Zehao%20Li%20and%20Hao%20Jiang%20and%20Yujun%20Cai%20and%20Jianing%20Chen%20and%20Baolong%20Bi%20and%20Shuqin%20Gao%20and%20Honglong%20Zhao%20and%20Yiwei%20Wang%20and%20Tianlu%20Mao%20and%20Zhaoqi%20Wang%0AAbstract%3A%20%20%20Although%20dynamic%20scene%20reconstruction%20has%20long%20been%20a%20fundamental%20challenge%0Ain%203D%20vision%2C%20the%20recent%20emergence%20of%203D%20Gaussian%20Splatting%20%283DGS%29%20offers%20a%0Apromising%20direction%20by%20enabling%20high-quality%2C%20real-time%20rendering%20through%0Aexplicit%20Gaussian%20primitives.%20However%2C%20existing%203DGS-based%20methods%20for%20dynamic%0Areconstruction%20often%20suffer%20from%20%5Ctextit%7Bspatio-temporal%20incoherence%7D%20during%0Ainitialization%2C%20where%20canonical%20Gaussians%20are%20constructed%20by%20aggregating%0Aobservations%20from%20multiple%20frames%20without%20temporal%20distinction.%20This%20results%20in%0Aspatio-temporally%20entangled%20representations%2C%20making%20it%20difficult%20to%20model%0Adynamic%20motion%20accurately.%20To%20overcome%20this%20limitation%2C%20we%20propose%0A%5Ctextbf%7BSTDR%7D%20%28Spatio-Temporal%20Decoupling%20for%20Real-time%20rendering%29%2C%20a%0Aplug-and-play%20module%20that%20learns%20spatio-temporal%20probability%20distributions%20for%0Aeach%20Gaussian.%20STDR%20introduces%20a%20spatio-temporal%20mask%2C%20a%20separated%20deformation%0Afield%2C%20and%20a%20consistency%20regularization%20to%20jointly%20disentangle%20spatial%20and%0Atemporal%20patterns.%20Extensive%20experiments%20demonstrate%20that%20incorporating%20our%0Amodule%20into%20existing%203DGS-based%20dynamic%20scene%20reconstruction%20frameworks%20leads%0Ato%20notable%20improvements%20in%20both%20reconstruction%20quality%20and%20spatio-temporal%0Aconsistency%20across%20synthetic%20and%20real-world%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22400v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTDR%253A%2520Spatio-Temporal%2520Decoupling%2520for%2520Real-Time%2520Dynamic%2520Scene%2520Rendering%26entry.906535625%3DZehao%2520Li%2520and%2520Hao%2520Jiang%2520and%2520Yujun%2520Cai%2520and%2520Jianing%2520Chen%2520and%2520Baolong%2520Bi%2520and%2520Shuqin%2520Gao%2520and%2520Honglong%2520Zhao%2520and%2520Yiwei%2520Wang%2520and%2520Tianlu%2520Mao%2520and%2520Zhaoqi%2520Wang%26entry.1292438233%3D%2520%2520Although%2520dynamic%2520scene%2520reconstruction%2520has%2520long%2520been%2520a%2520fundamental%2520challenge%250Ain%25203D%2520vision%252C%2520the%2520recent%2520emergence%2520of%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520offers%2520a%250Apromising%2520direction%2520by%2520enabling%2520high-quality%252C%2520real-time%2520rendering%2520through%250Aexplicit%2520Gaussian%2520primitives.%2520However%252C%2520existing%25203DGS-based%2520methods%2520for%2520dynamic%250Areconstruction%2520often%2520suffer%2520from%2520%255Ctextit%257Bspatio-temporal%2520incoherence%257D%2520during%250Ainitialization%252C%2520where%2520canonical%2520Gaussians%2520are%2520constructed%2520by%2520aggregating%250Aobservations%2520from%2520multiple%2520frames%2520without%2520temporal%2520distinction.%2520This%2520results%2520in%250Aspatio-temporally%2520entangled%2520representations%252C%2520making%2520it%2520difficult%2520to%2520model%250Adynamic%2520motion%2520accurately.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%250A%255Ctextbf%257BSTDR%257D%2520%2528Spatio-Temporal%2520Decoupling%2520for%2520Real-time%2520rendering%2529%252C%2520a%250Aplug-and-play%2520module%2520that%2520learns%2520spatio-temporal%2520probability%2520distributions%2520for%250Aeach%2520Gaussian.%2520STDR%2520introduces%2520a%2520spatio-temporal%2520mask%252C%2520a%2520separated%2520deformation%250Afield%252C%2520and%2520a%2520consistency%2520regularization%2520to%2520jointly%2520disentangle%2520spatial%2520and%250Atemporal%2520patterns.%2520Extensive%2520experiments%2520demonstrate%2520that%2520incorporating%2520our%250Amodule%2520into%2520existing%25203DGS-based%2520dynamic%2520scene%2520reconstruction%2520frameworks%2520leads%250Ato%2520notable%2520improvements%2520in%2520both%2520reconstruction%2520quality%2520and%2520spatio-temporal%250Aconsistency%2520across%2520synthetic%2520and%2520real-world%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22400v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STDR%3A%20Spatio-Temporal%20Decoupling%20for%20Real-Time%20Dynamic%20Scene%20Rendering&entry.906535625=Zehao%20Li%20and%20Hao%20Jiang%20and%20Yujun%20Cai%20and%20Jianing%20Chen%20and%20Baolong%20Bi%20and%20Shuqin%20Gao%20and%20Honglong%20Zhao%20and%20Yiwei%20Wang%20and%20Tianlu%20Mao%20and%20Zhaoqi%20Wang&entry.1292438233=%20%20Although%20dynamic%20scene%20reconstruction%20has%20long%20been%20a%20fundamental%20challenge%0Ain%203D%20vision%2C%20the%20recent%20emergence%20of%203D%20Gaussian%20Splatting%20%283DGS%29%20offers%20a%0Apromising%20direction%20by%20enabling%20high-quality%2C%20real-time%20rendering%20through%0Aexplicit%20Gaussian%20primitives.%20However%2C%20existing%203DGS-based%20methods%20for%20dynamic%0Areconstruction%20often%20suffer%20from%20%5Ctextit%7Bspatio-temporal%20incoherence%7D%20during%0Ainitialization%2C%20where%20canonical%20Gaussians%20are%20constructed%20by%20aggregating%0Aobservations%20from%20multiple%20frames%20without%20temporal%20distinction.%20This%20results%20in%0Aspatio-temporally%20entangled%20representations%2C%20making%20it%20difficult%20to%20model%0Adynamic%20motion%20accurately.%20To%20overcome%20this%20limitation%2C%20we%20propose%0A%5Ctextbf%7BSTDR%7D%20%28Spatio-Temporal%20Decoupling%20for%20Real-time%20rendering%29%2C%20a%0Aplug-and-play%20module%20that%20learns%20spatio-temporal%20probability%20distributions%20for%0Aeach%20Gaussian.%20STDR%20introduces%20a%20spatio-temporal%20mask%2C%20a%20separated%20deformation%0Afield%2C%20and%20a%20consistency%20regularization%20to%20jointly%20disentangle%20spatial%20and%0Atemporal%20patterns.%20Extensive%20experiments%20demonstrate%20that%20incorporating%20our%0Amodule%20into%20existing%203DGS-based%20dynamic%20scene%20reconstruction%20frameworks%20leads%0Ato%20notable%20improvements%20in%20both%20reconstruction%20quality%20and%20spatio-temporal%0Aconsistency%20across%20synthetic%20and%20real-world%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22400v1&entry.124074799=Read"},
{"title": "Cascaded 3D Diffusion Models for Whole-body 3D 18-F FDG PET/CT synthesis\n  from Demographics", "author": "Siyeop Yoon and Sifan Song and Pengfei Jin and Matthew Tivnan and Yujin Oh and Sekeun Kim and Dufan Wu and Xiang Li and Quanzheng Li", "abstract": "  We propose a cascaded 3D diffusion model framework to synthesize\nhigh-fidelity 3D PET/CT volumes directly from demographic variables, addressing\nthe growing need for realistic digital twins in oncologic imaging, virtual\ntrials, and AI-driven data augmentation. Unlike deterministic phantoms, which\nrely on predefined anatomical and metabolic templates, our method employs a\ntwo-stage generative process. An initial score-based diffusion model\nsynthesizes low-resolution PET/CT volumes from demographic variables alone,\nproviding global anatomical structures and approximate metabolic activity. This\nis followed by a super-resolution residual diffusion model that refines spatial\nresolution. Our framework was trained on 18-F FDG PET/CT scans from the AutoPET\ndataset and evaluated using organ-wise volume and standardized uptake value\n(SUV) distributions, comparing synthetic and real data between demographic\nsubgroups. The organ-wise comparison demonstrated strong concordance between\nsynthetic and real images. In particular, most deviations in metabolic uptake\nvalues remained within 3-5% of the ground truth in subgroup analysis. These\nfindings highlight the potential of cascaded 3D diffusion models to generate\nanatomically and metabolically accurate PET/CT images, offering a robust\nalternative to traditional phantoms and enabling scalable, population-informed\nsynthetic imaging for clinical and research applications.\n", "link": "http://arxiv.org/abs/2505.22489v1", "date": "2025-05-28", "relevancy": 3.1727, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6589}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6589}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cascaded%203D%20Diffusion%20Models%20for%20Whole-body%203D%2018-F%20FDG%20PET/CT%20synthesis%0A%20%20from%20Demographics&body=Title%3A%20Cascaded%203D%20Diffusion%20Models%20for%20Whole-body%203D%2018-F%20FDG%20PET/CT%20synthesis%0A%20%20from%20Demographics%0AAuthor%3A%20Siyeop%20Yoon%20and%20Sifan%20Song%20and%20Pengfei%20Jin%20and%20Matthew%20Tivnan%20and%20Yujin%20Oh%20and%20Sekeun%20Kim%20and%20Dufan%20Wu%20and%20Xiang%20Li%20and%20Quanzheng%20Li%0AAbstract%3A%20%20%20We%20propose%20a%20cascaded%203D%20diffusion%20model%20framework%20to%20synthesize%0Ahigh-fidelity%203D%20PET/CT%20volumes%20directly%20from%20demographic%20variables%2C%20addressing%0Athe%20growing%20need%20for%20realistic%20digital%20twins%20in%20oncologic%20imaging%2C%20virtual%0Atrials%2C%20and%20AI-driven%20data%20augmentation.%20Unlike%20deterministic%20phantoms%2C%20which%0Arely%20on%20predefined%20anatomical%20and%20metabolic%20templates%2C%20our%20method%20employs%20a%0Atwo-stage%20generative%20process.%20An%20initial%20score-based%20diffusion%20model%0Asynthesizes%20low-resolution%20PET/CT%20volumes%20from%20demographic%20variables%20alone%2C%0Aproviding%20global%20anatomical%20structures%20and%20approximate%20metabolic%20activity.%20This%0Ais%20followed%20by%20a%20super-resolution%20residual%20diffusion%20model%20that%20refines%20spatial%0Aresolution.%20Our%20framework%20was%20trained%20on%2018-F%20FDG%20PET/CT%20scans%20from%20the%20AutoPET%0Adataset%20and%20evaluated%20using%20organ-wise%20volume%20and%20standardized%20uptake%20value%0A%28SUV%29%20distributions%2C%20comparing%20synthetic%20and%20real%20data%20between%20demographic%0Asubgroups.%20The%20organ-wise%20comparison%20demonstrated%20strong%20concordance%20between%0Asynthetic%20and%20real%20images.%20In%20particular%2C%20most%20deviations%20in%20metabolic%20uptake%0Avalues%20remained%20within%203-5%25%20of%20the%20ground%20truth%20in%20subgroup%20analysis.%20These%0Afindings%20highlight%20the%20potential%20of%20cascaded%203D%20diffusion%20models%20to%20generate%0Aanatomically%20and%20metabolically%20accurate%20PET/CT%20images%2C%20offering%20a%20robust%0Aalternative%20to%20traditional%20phantoms%20and%20enabling%20scalable%2C%20population-informed%0Asynthetic%20imaging%20for%20clinical%20and%20research%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22489v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCascaded%25203D%2520Diffusion%2520Models%2520for%2520Whole-body%25203D%252018-F%2520FDG%2520PET/CT%2520synthesis%250A%2520%2520from%2520Demographics%26entry.906535625%3DSiyeop%2520Yoon%2520and%2520Sifan%2520Song%2520and%2520Pengfei%2520Jin%2520and%2520Matthew%2520Tivnan%2520and%2520Yujin%2520Oh%2520and%2520Sekeun%2520Kim%2520and%2520Dufan%2520Wu%2520and%2520Xiang%2520Li%2520and%2520Quanzheng%2520Li%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520cascaded%25203D%2520diffusion%2520model%2520framework%2520to%2520synthesize%250Ahigh-fidelity%25203D%2520PET/CT%2520volumes%2520directly%2520from%2520demographic%2520variables%252C%2520addressing%250Athe%2520growing%2520need%2520for%2520realistic%2520digital%2520twins%2520in%2520oncologic%2520imaging%252C%2520virtual%250Atrials%252C%2520and%2520AI-driven%2520data%2520augmentation.%2520Unlike%2520deterministic%2520phantoms%252C%2520which%250Arely%2520on%2520predefined%2520anatomical%2520and%2520metabolic%2520templates%252C%2520our%2520method%2520employs%2520a%250Atwo-stage%2520generative%2520process.%2520An%2520initial%2520score-based%2520diffusion%2520model%250Asynthesizes%2520low-resolution%2520PET/CT%2520volumes%2520from%2520demographic%2520variables%2520alone%252C%250Aproviding%2520global%2520anatomical%2520structures%2520and%2520approximate%2520metabolic%2520activity.%2520This%250Ais%2520followed%2520by%2520a%2520super-resolution%2520residual%2520diffusion%2520model%2520that%2520refines%2520spatial%250Aresolution.%2520Our%2520framework%2520was%2520trained%2520on%252018-F%2520FDG%2520PET/CT%2520scans%2520from%2520the%2520AutoPET%250Adataset%2520and%2520evaluated%2520using%2520organ-wise%2520volume%2520and%2520standardized%2520uptake%2520value%250A%2528SUV%2529%2520distributions%252C%2520comparing%2520synthetic%2520and%2520real%2520data%2520between%2520demographic%250Asubgroups.%2520The%2520organ-wise%2520comparison%2520demonstrated%2520strong%2520concordance%2520between%250Asynthetic%2520and%2520real%2520images.%2520In%2520particular%252C%2520most%2520deviations%2520in%2520metabolic%2520uptake%250Avalues%2520remained%2520within%25203-5%2525%2520of%2520the%2520ground%2520truth%2520in%2520subgroup%2520analysis.%2520These%250Afindings%2520highlight%2520the%2520potential%2520of%2520cascaded%25203D%2520diffusion%2520models%2520to%2520generate%250Aanatomically%2520and%2520metabolically%2520accurate%2520PET/CT%2520images%252C%2520offering%2520a%2520robust%250Aalternative%2520to%2520traditional%2520phantoms%2520and%2520enabling%2520scalable%252C%2520population-informed%250Asynthetic%2520imaging%2520for%2520clinical%2520and%2520research%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22489v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cascaded%203D%20Diffusion%20Models%20for%20Whole-body%203D%2018-F%20FDG%20PET/CT%20synthesis%0A%20%20from%20Demographics&entry.906535625=Siyeop%20Yoon%20and%20Sifan%20Song%20and%20Pengfei%20Jin%20and%20Matthew%20Tivnan%20and%20Yujin%20Oh%20and%20Sekeun%20Kim%20and%20Dufan%20Wu%20and%20Xiang%20Li%20and%20Quanzheng%20Li&entry.1292438233=%20%20We%20propose%20a%20cascaded%203D%20diffusion%20model%20framework%20to%20synthesize%0Ahigh-fidelity%203D%20PET/CT%20volumes%20directly%20from%20demographic%20variables%2C%20addressing%0Athe%20growing%20need%20for%20realistic%20digital%20twins%20in%20oncologic%20imaging%2C%20virtual%0Atrials%2C%20and%20AI-driven%20data%20augmentation.%20Unlike%20deterministic%20phantoms%2C%20which%0Arely%20on%20predefined%20anatomical%20and%20metabolic%20templates%2C%20our%20method%20employs%20a%0Atwo-stage%20generative%20process.%20An%20initial%20score-based%20diffusion%20model%0Asynthesizes%20low-resolution%20PET/CT%20volumes%20from%20demographic%20variables%20alone%2C%0Aproviding%20global%20anatomical%20structures%20and%20approximate%20metabolic%20activity.%20This%0Ais%20followed%20by%20a%20super-resolution%20residual%20diffusion%20model%20that%20refines%20spatial%0Aresolution.%20Our%20framework%20was%20trained%20on%2018-F%20FDG%20PET/CT%20scans%20from%20the%20AutoPET%0Adataset%20and%20evaluated%20using%20organ-wise%20volume%20and%20standardized%20uptake%20value%0A%28SUV%29%20distributions%2C%20comparing%20synthetic%20and%20real%20data%20between%20demographic%0Asubgroups.%20The%20organ-wise%20comparison%20demonstrated%20strong%20concordance%20between%0Asynthetic%20and%20real%20images.%20In%20particular%2C%20most%20deviations%20in%20metabolic%20uptake%0Avalues%20remained%20within%203-5%25%20of%20the%20ground%20truth%20in%20subgroup%20analysis.%20These%0Afindings%20highlight%20the%20potential%20of%20cascaded%203D%20diffusion%20models%20to%20generate%0Aanatomically%20and%20metabolically%20accurate%20PET/CT%20images%2C%20offering%20a%20robust%0Aalternative%20to%20traditional%20phantoms%20and%20enabling%20scalable%2C%20population-informed%0Asynthetic%20imaging%20for%20clinical%20and%20research%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22489v1&entry.124074799=Read"},
{"title": "Progressive Language-guided Visual Learning for Multi-Task Visual\n  Grounding", "author": "Jingchao Wang and Hong Wang and Wenlong Zhang and Kunhua Ji and Dingjiang Huang and Yefeng Zheng", "abstract": "  Multi-task visual grounding (MTVG) includes two sub-tasks, i.e., Referring\nExpression Comprehension (REC) and Referring Expression Segmentation (RES). The\nexisting representative approaches generally follow the research pipeline which\nmainly consists of three core procedures, including independent feature\nextraction for visual and linguistic modalities, respectively, cross-modal\ninteraction module, and independent prediction heads for different sub-tasks.\nAlbeit achieving remarkable performance, this research line has two\nlimitations: 1) The linguistic content has not been fully injected into the\nentire visual backbone for boosting more effective visual feature extraction\nand it needs an extra cross-modal interaction module; 2) The relationship\nbetween REC and RES tasks is not effectively exploited to help the\ncollaborative prediction for more accurate output. To deal with these problems,\nin this paper, we propose a Progressive Language-guided Visual Learning\nframework for multi-task visual grounding, called PLVL, which not only finely\nmine the inherent feature expression of the visual modality itself but also\nprogressively inject the language information to help learn linguistic-related\nvisual features. In this manner, our PLVL does not need additional cross-modal\nfusion module while fully introducing the language guidance. Furthermore, we\nanalyze that the localization center for REC would help identify the\nto-be-segmented object region for RES to some extent. Inspired by this\ninvestigation, we design a multi-task head to accomplish collaborative\npredictions for these two sub-tasks. Extensive experiments conducted on several\nbenchmark datasets comprehensively substantiate that our PLVL obviously\noutperforms the representative methods in both REC and RES tasks.\nhttps://github.com/jcwang0602/PLVL\n", "link": "http://arxiv.org/abs/2504.16145v2", "date": "2025-05-28", "relevancy": 3.124, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6307}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6307}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Progressive%20Language-guided%20Visual%20Learning%20for%20Multi-Task%20Visual%0A%20%20Grounding&body=Title%3A%20Progressive%20Language-guided%20Visual%20Learning%20for%20Multi-Task%20Visual%0A%20%20Grounding%0AAuthor%3A%20Jingchao%20Wang%20and%20Hong%20Wang%20and%20Wenlong%20Zhang%20and%20Kunhua%20Ji%20and%20Dingjiang%20Huang%20and%20Yefeng%20Zheng%0AAbstract%3A%20%20%20Multi-task%20visual%20grounding%20%28MTVG%29%20includes%20two%20sub-tasks%2C%20i.e.%2C%20Referring%0AExpression%20Comprehension%20%28REC%29%20and%20Referring%20Expression%20Segmentation%20%28RES%29.%20The%0Aexisting%20representative%20approaches%20generally%20follow%20the%20research%20pipeline%20which%0Amainly%20consists%20of%20three%20core%20procedures%2C%20including%20independent%20feature%0Aextraction%20for%20visual%20and%20linguistic%20modalities%2C%20respectively%2C%20cross-modal%0Ainteraction%20module%2C%20and%20independent%20prediction%20heads%20for%20different%20sub-tasks.%0AAlbeit%20achieving%20remarkable%20performance%2C%20this%20research%20line%20has%20two%0Alimitations%3A%201%29%20The%20linguistic%20content%20has%20not%20been%20fully%20injected%20into%20the%0Aentire%20visual%20backbone%20for%20boosting%20more%20effective%20visual%20feature%20extraction%0Aand%20it%20needs%20an%20extra%20cross-modal%20interaction%20module%3B%202%29%20The%20relationship%0Abetween%20REC%20and%20RES%20tasks%20is%20not%20effectively%20exploited%20to%20help%20the%0Acollaborative%20prediction%20for%20more%20accurate%20output.%20To%20deal%20with%20these%20problems%2C%0Ain%20this%20paper%2C%20we%20propose%20a%20Progressive%20Language-guided%20Visual%20Learning%0Aframework%20for%20multi-task%20visual%20grounding%2C%20called%20PLVL%2C%20which%20not%20only%20finely%0Amine%20the%20inherent%20feature%20expression%20of%20the%20visual%20modality%20itself%20but%20also%0Aprogressively%20inject%20the%20language%20information%20to%20help%20learn%20linguistic-related%0Avisual%20features.%20In%20this%20manner%2C%20our%20PLVL%20does%20not%20need%20additional%20cross-modal%0Afusion%20module%20while%20fully%20introducing%20the%20language%20guidance.%20Furthermore%2C%20we%0Aanalyze%20that%20the%20localization%20center%20for%20REC%20would%20help%20identify%20the%0Ato-be-segmented%20object%20region%20for%20RES%20to%20some%20extent.%20Inspired%20by%20this%0Ainvestigation%2C%20we%20design%20a%20multi-task%20head%20to%20accomplish%20collaborative%0Apredictions%20for%20these%20two%20sub-tasks.%20Extensive%20experiments%20conducted%20on%20several%0Abenchmark%20datasets%20comprehensively%20substantiate%20that%20our%20PLVL%20obviously%0Aoutperforms%20the%20representative%20methods%20in%20both%20REC%20and%20RES%20tasks.%0Ahttps%3A//github.com/jcwang0602/PLVL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16145v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgressive%2520Language-guided%2520Visual%2520Learning%2520for%2520Multi-Task%2520Visual%250A%2520%2520Grounding%26entry.906535625%3DJingchao%2520Wang%2520and%2520Hong%2520Wang%2520and%2520Wenlong%2520Zhang%2520and%2520Kunhua%2520Ji%2520and%2520Dingjiang%2520Huang%2520and%2520Yefeng%2520Zheng%26entry.1292438233%3D%2520%2520Multi-task%2520visual%2520grounding%2520%2528MTVG%2529%2520includes%2520two%2520sub-tasks%252C%2520i.e.%252C%2520Referring%250AExpression%2520Comprehension%2520%2528REC%2529%2520and%2520Referring%2520Expression%2520Segmentation%2520%2528RES%2529.%2520The%250Aexisting%2520representative%2520approaches%2520generally%2520follow%2520the%2520research%2520pipeline%2520which%250Amainly%2520consists%2520of%2520three%2520core%2520procedures%252C%2520including%2520independent%2520feature%250Aextraction%2520for%2520visual%2520and%2520linguistic%2520modalities%252C%2520respectively%252C%2520cross-modal%250Ainteraction%2520module%252C%2520and%2520independent%2520prediction%2520heads%2520for%2520different%2520sub-tasks.%250AAlbeit%2520achieving%2520remarkable%2520performance%252C%2520this%2520research%2520line%2520has%2520two%250Alimitations%253A%25201%2529%2520The%2520linguistic%2520content%2520has%2520not%2520been%2520fully%2520injected%2520into%2520the%250Aentire%2520visual%2520backbone%2520for%2520boosting%2520more%2520effective%2520visual%2520feature%2520extraction%250Aand%2520it%2520needs%2520an%2520extra%2520cross-modal%2520interaction%2520module%253B%25202%2529%2520The%2520relationship%250Abetween%2520REC%2520and%2520RES%2520tasks%2520is%2520not%2520effectively%2520exploited%2520to%2520help%2520the%250Acollaborative%2520prediction%2520for%2520more%2520accurate%2520output.%2520To%2520deal%2520with%2520these%2520problems%252C%250Ain%2520this%2520paper%252C%2520we%2520propose%2520a%2520Progressive%2520Language-guided%2520Visual%2520Learning%250Aframework%2520for%2520multi-task%2520visual%2520grounding%252C%2520called%2520PLVL%252C%2520which%2520not%2520only%2520finely%250Amine%2520the%2520inherent%2520feature%2520expression%2520of%2520the%2520visual%2520modality%2520itself%2520but%2520also%250Aprogressively%2520inject%2520the%2520language%2520information%2520to%2520help%2520learn%2520linguistic-related%250Avisual%2520features.%2520In%2520this%2520manner%252C%2520our%2520PLVL%2520does%2520not%2520need%2520additional%2520cross-modal%250Afusion%2520module%2520while%2520fully%2520introducing%2520the%2520language%2520guidance.%2520Furthermore%252C%2520we%250Aanalyze%2520that%2520the%2520localization%2520center%2520for%2520REC%2520would%2520help%2520identify%2520the%250Ato-be-segmented%2520object%2520region%2520for%2520RES%2520to%2520some%2520extent.%2520Inspired%2520by%2520this%250Ainvestigation%252C%2520we%2520design%2520a%2520multi-task%2520head%2520to%2520accomplish%2520collaborative%250Apredictions%2520for%2520these%2520two%2520sub-tasks.%2520Extensive%2520experiments%2520conducted%2520on%2520several%250Abenchmark%2520datasets%2520comprehensively%2520substantiate%2520that%2520our%2520PLVL%2520obviously%250Aoutperforms%2520the%2520representative%2520methods%2520in%2520both%2520REC%2520and%2520RES%2520tasks.%250Ahttps%253A//github.com/jcwang0602/PLVL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16145v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Progressive%20Language-guided%20Visual%20Learning%20for%20Multi-Task%20Visual%0A%20%20Grounding&entry.906535625=Jingchao%20Wang%20and%20Hong%20Wang%20and%20Wenlong%20Zhang%20and%20Kunhua%20Ji%20and%20Dingjiang%20Huang%20and%20Yefeng%20Zheng&entry.1292438233=%20%20Multi-task%20visual%20grounding%20%28MTVG%29%20includes%20two%20sub-tasks%2C%20i.e.%2C%20Referring%0AExpression%20Comprehension%20%28REC%29%20and%20Referring%20Expression%20Segmentation%20%28RES%29.%20The%0Aexisting%20representative%20approaches%20generally%20follow%20the%20research%20pipeline%20which%0Amainly%20consists%20of%20three%20core%20procedures%2C%20including%20independent%20feature%0Aextraction%20for%20visual%20and%20linguistic%20modalities%2C%20respectively%2C%20cross-modal%0Ainteraction%20module%2C%20and%20independent%20prediction%20heads%20for%20different%20sub-tasks.%0AAlbeit%20achieving%20remarkable%20performance%2C%20this%20research%20line%20has%20two%0Alimitations%3A%201%29%20The%20linguistic%20content%20has%20not%20been%20fully%20injected%20into%20the%0Aentire%20visual%20backbone%20for%20boosting%20more%20effective%20visual%20feature%20extraction%0Aand%20it%20needs%20an%20extra%20cross-modal%20interaction%20module%3B%202%29%20The%20relationship%0Abetween%20REC%20and%20RES%20tasks%20is%20not%20effectively%20exploited%20to%20help%20the%0Acollaborative%20prediction%20for%20more%20accurate%20output.%20To%20deal%20with%20these%20problems%2C%0Ain%20this%20paper%2C%20we%20propose%20a%20Progressive%20Language-guided%20Visual%20Learning%0Aframework%20for%20multi-task%20visual%20grounding%2C%20called%20PLVL%2C%20which%20not%20only%20finely%0Amine%20the%20inherent%20feature%20expression%20of%20the%20visual%20modality%20itself%20but%20also%0Aprogressively%20inject%20the%20language%20information%20to%20help%20learn%20linguistic-related%0Avisual%20features.%20In%20this%20manner%2C%20our%20PLVL%20does%20not%20need%20additional%20cross-modal%0Afusion%20module%20while%20fully%20introducing%20the%20language%20guidance.%20Furthermore%2C%20we%0Aanalyze%20that%20the%20localization%20center%20for%20REC%20would%20help%20identify%20the%0Ato-be-segmented%20object%20region%20for%20RES%20to%20some%20extent.%20Inspired%20by%20this%0Ainvestigation%2C%20we%20design%20a%20multi-task%20head%20to%20accomplish%20collaborative%0Apredictions%20for%20these%20two%20sub-tasks.%20Extensive%20experiments%20conducted%20on%20several%0Abenchmark%20datasets%20comprehensively%20substantiate%20that%20our%20PLVL%20obviously%0Aoutperforms%20the%20representative%20methods%20in%20both%20REC%20and%20RES%20tasks.%0Ahttps%3A//github.com/jcwang0602/PLVL%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16145v2&entry.124074799=Read"},
{"title": "Zero-Shot 3D Visual Grounding from Vision-Language Models", "author": "Rong Li and Shijie Li and Lingdong Kong and Xulei Yang and Junwei Liang", "abstract": "  3D Visual Grounding (3DVG) seeks to locate target objects in 3D scenes using\nnatural language descriptions, enabling downstream applications such as\naugmented reality and robotics. Existing approaches typically rely on labeled\n3D data and predefined categories, limiting scalability to open-world settings.\nWe present SeeGround, a zero-shot 3DVG framework that leverages 2D\nVision-Language Models (VLMs) to bypass the need for 3D-specific training. To\nbridge the modality gap, we introduce a hybrid input format that pairs\nquery-aligned rendered views with spatially enriched textual descriptions. Our\nframework incorporates two core components: a Perspective Adaptation Module\nthat dynamically selects optimal viewpoints based on the query, and a Fusion\nAlignment Module that integrates visual and spatial signals to enhance\nlocalization precision. Extensive evaluations on ScanRefer and Nr3D confirm\nthat SeeGround achieves substantial improvements over existing zero-shot\nbaselines -- outperforming them by 7.7% and 7.1%, respectively -- and even\nrivals fully supervised alternatives, demonstrating strong generalization under\nchallenging conditions.\n", "link": "http://arxiv.org/abs/2505.22429v1", "date": "2025-05-28", "relevancy": 3.0994, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6366}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6366}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%203D%20Visual%20Grounding%20from%20Vision-Language%20Models&body=Title%3A%20Zero-Shot%203D%20Visual%20Grounding%20from%20Vision-Language%20Models%0AAuthor%3A%20Rong%20Li%20and%20Shijie%20Li%20and%20Lingdong%20Kong%20and%20Xulei%20Yang%20and%20Junwei%20Liang%0AAbstract%3A%20%20%203D%20Visual%20Grounding%20%283DVG%29%20seeks%20to%20locate%20target%20objects%20in%203D%20scenes%20using%0Anatural%20language%20descriptions%2C%20enabling%20downstream%20applications%20such%20as%0Aaugmented%20reality%20and%20robotics.%20Existing%20approaches%20typically%20rely%20on%20labeled%0A3D%20data%20and%20predefined%20categories%2C%20limiting%20scalability%20to%20open-world%20settings.%0AWe%20present%20SeeGround%2C%20a%20zero-shot%203DVG%20framework%20that%20leverages%202D%0AVision-Language%20Models%20%28VLMs%29%20to%20bypass%20the%20need%20for%203D-specific%20training.%20To%0Abridge%20the%20modality%20gap%2C%20we%20introduce%20a%20hybrid%20input%20format%20that%20pairs%0Aquery-aligned%20rendered%20views%20with%20spatially%20enriched%20textual%20descriptions.%20Our%0Aframework%20incorporates%20two%20core%20components%3A%20a%20Perspective%20Adaptation%20Module%0Athat%20dynamically%20selects%20optimal%20viewpoints%20based%20on%20the%20query%2C%20and%20a%20Fusion%0AAlignment%20Module%20that%20integrates%20visual%20and%20spatial%20signals%20to%20enhance%0Alocalization%20precision.%20Extensive%20evaluations%20on%20ScanRefer%20and%20Nr3D%20confirm%0Athat%20SeeGround%20achieves%20substantial%20improvements%20over%20existing%20zero-shot%0Abaselines%20--%20outperforming%20them%20by%207.7%25%20and%207.1%25%2C%20respectively%20--%20and%20even%0Arivals%20fully%20supervised%20alternatives%2C%20demonstrating%20strong%20generalization%20under%0Achallenging%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22429v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%25203D%2520Visual%2520Grounding%2520from%2520Vision-Language%2520Models%26entry.906535625%3DRong%2520Li%2520and%2520Shijie%2520Li%2520and%2520Lingdong%2520Kong%2520and%2520Xulei%2520Yang%2520and%2520Junwei%2520Liang%26entry.1292438233%3D%2520%25203D%2520Visual%2520Grounding%2520%25283DVG%2529%2520seeks%2520to%2520locate%2520target%2520objects%2520in%25203D%2520scenes%2520using%250Anatural%2520language%2520descriptions%252C%2520enabling%2520downstream%2520applications%2520such%2520as%250Aaugmented%2520reality%2520and%2520robotics.%2520Existing%2520approaches%2520typically%2520rely%2520on%2520labeled%250A3D%2520data%2520and%2520predefined%2520categories%252C%2520limiting%2520scalability%2520to%2520open-world%2520settings.%250AWe%2520present%2520SeeGround%252C%2520a%2520zero-shot%25203DVG%2520framework%2520that%2520leverages%25202D%250AVision-Language%2520Models%2520%2528VLMs%2529%2520to%2520bypass%2520the%2520need%2520for%25203D-specific%2520training.%2520To%250Abridge%2520the%2520modality%2520gap%252C%2520we%2520introduce%2520a%2520hybrid%2520input%2520format%2520that%2520pairs%250Aquery-aligned%2520rendered%2520views%2520with%2520spatially%2520enriched%2520textual%2520descriptions.%2520Our%250Aframework%2520incorporates%2520two%2520core%2520components%253A%2520a%2520Perspective%2520Adaptation%2520Module%250Athat%2520dynamically%2520selects%2520optimal%2520viewpoints%2520based%2520on%2520the%2520query%252C%2520and%2520a%2520Fusion%250AAlignment%2520Module%2520that%2520integrates%2520visual%2520and%2520spatial%2520signals%2520to%2520enhance%250Alocalization%2520precision.%2520Extensive%2520evaluations%2520on%2520ScanRefer%2520and%2520Nr3D%2520confirm%250Athat%2520SeeGround%2520achieves%2520substantial%2520improvements%2520over%2520existing%2520zero-shot%250Abaselines%2520--%2520outperforming%2520them%2520by%25207.7%2525%2520and%25207.1%2525%252C%2520respectively%2520--%2520and%2520even%250Arivals%2520fully%2520supervised%2520alternatives%252C%2520demonstrating%2520strong%2520generalization%2520under%250Achallenging%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22429v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%203D%20Visual%20Grounding%20from%20Vision-Language%20Models&entry.906535625=Rong%20Li%20and%20Shijie%20Li%20and%20Lingdong%20Kong%20and%20Xulei%20Yang%20and%20Junwei%20Liang&entry.1292438233=%20%203D%20Visual%20Grounding%20%283DVG%29%20seeks%20to%20locate%20target%20objects%20in%203D%20scenes%20using%0Anatural%20language%20descriptions%2C%20enabling%20downstream%20applications%20such%20as%0Aaugmented%20reality%20and%20robotics.%20Existing%20approaches%20typically%20rely%20on%20labeled%0A3D%20data%20and%20predefined%20categories%2C%20limiting%20scalability%20to%20open-world%20settings.%0AWe%20present%20SeeGround%2C%20a%20zero-shot%203DVG%20framework%20that%20leverages%202D%0AVision-Language%20Models%20%28VLMs%29%20to%20bypass%20the%20need%20for%203D-specific%20training.%20To%0Abridge%20the%20modality%20gap%2C%20we%20introduce%20a%20hybrid%20input%20format%20that%20pairs%0Aquery-aligned%20rendered%20views%20with%20spatially%20enriched%20textual%20descriptions.%20Our%0Aframework%20incorporates%20two%20core%20components%3A%20a%20Perspective%20Adaptation%20Module%0Athat%20dynamically%20selects%20optimal%20viewpoints%20based%20on%20the%20query%2C%20and%20a%20Fusion%0AAlignment%20Module%20that%20integrates%20visual%20and%20spatial%20signals%20to%20enhance%0Alocalization%20precision.%20Extensive%20evaluations%20on%20ScanRefer%20and%20Nr3D%20confirm%0Athat%20SeeGround%20achieves%20substantial%20improvements%20over%20existing%20zero-shot%0Abaselines%20--%20outperforming%20them%20by%207.7%25%20and%207.1%25%2C%20respectively%20--%20and%20even%0Arivals%20fully%20supervised%20alternatives%2C%20demonstrating%20strong%20generalization%20under%0Achallenging%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22429v1&entry.124074799=Read"},
{"title": "3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large\n  Language Model", "author": "Wenbo Hu and Yining Hong and Yanjun Wang and Leison Gao and Zibu Wei and Xingcheng Yao and Nanyun Peng and Yonatan Bitton and Idan Szpektor and Kai-Wei Chang", "abstract": "  Humans excel at performing complex tasks by leveraging long-term memory\nacross temporal and spatial experiences. In contrast, current Large Language\nModels (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D\nenvironments. We posit that part of this limitation is due to the lack of\nproper 3D spatial-temporal memory modeling in LLMs. To address this, we first\nintroduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000\ntrajectories and 2,892 embodied tasks, question-answering and captioning,\ndesigned to evaluate an agent's ability to reason over long-term memory in 3D\nenvironments. Second, we propose 3DLLM-Mem, a novel dynamic memory management\nand fusion model for embodied spatial-temporal reasoning and actions in LLMs.\nOur model uses working memory tokens, which represents current observations, as\nqueries to selectively attend to and fuse the most useful spatial and temporal\nfeatures from episodic memory, which stores past observations and interactions.\nOur approach allows the agent to focus on task-relevant information while\nmaintaining memory efficiency in complex, long-horizon environments.\nExperimental results demonstrate that 3DLLM-Mem achieves state-of-the-art\nperformance across various tasks, outperforming the strongest baselines by\n16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied\ntasks.\n", "link": "http://arxiv.org/abs/2505.22657v1", "date": "2025-05-28", "relevancy": 3.0432, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6139}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6139}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5981}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DLLM-Mem%3A%20Long-Term%20Spatial-Temporal%20Memory%20for%20Embodied%203D%20Large%0A%20%20Language%20Model&body=Title%3A%203DLLM-Mem%3A%20Long-Term%20Spatial-Temporal%20Memory%20for%20Embodied%203D%20Large%0A%20%20Language%20Model%0AAuthor%3A%20Wenbo%20Hu%20and%20Yining%20Hong%20and%20Yanjun%20Wang%20and%20Leison%20Gao%20and%20Zibu%20Wei%20and%20Xingcheng%20Yao%20and%20Nanyun%20Peng%20and%20Yonatan%20Bitton%20and%20Idan%20Szpektor%20and%20Kai-Wei%20Chang%0AAbstract%3A%20%20%20Humans%20excel%20at%20performing%20complex%20tasks%20by%20leveraging%20long-term%20memory%0Aacross%20temporal%20and%20spatial%20experiences.%20In%20contrast%2C%20current%20Large%20Language%0AModels%20%28LLMs%29%20struggle%20to%20effectively%20plan%20and%20act%20in%20dynamic%2C%20multi-room%203D%0Aenvironments.%20We%20posit%20that%20part%20of%20this%20limitation%20is%20due%20to%20the%20lack%20of%0Aproper%203D%20spatial-temporal%20memory%20modeling%20in%20LLMs.%20To%20address%20this%2C%20we%20first%0Aintroduce%203DMem-Bench%2C%20a%20comprehensive%20benchmark%20comprising%20over%2026%2C000%0Atrajectories%20and%202%2C892%20embodied%20tasks%2C%20question-answering%20and%20captioning%2C%0Adesigned%20to%20evaluate%20an%20agent%27s%20ability%20to%20reason%20over%20long-term%20memory%20in%203D%0Aenvironments.%20Second%2C%20we%20propose%203DLLM-Mem%2C%20a%20novel%20dynamic%20memory%20management%0Aand%20fusion%20model%20for%20embodied%20spatial-temporal%20reasoning%20and%20actions%20in%20LLMs.%0AOur%20model%20uses%20working%20memory%20tokens%2C%20which%20represents%20current%20observations%2C%20as%0Aqueries%20to%20selectively%20attend%20to%20and%20fuse%20the%20most%20useful%20spatial%20and%20temporal%0Afeatures%20from%20episodic%20memory%2C%20which%20stores%20past%20observations%20and%20interactions.%0AOur%20approach%20allows%20the%20agent%20to%20focus%20on%20task-relevant%20information%20while%0Amaintaining%20memory%20efficiency%20in%20complex%2C%20long-horizon%20environments.%0AExperimental%20results%20demonstrate%20that%203DLLM-Mem%20achieves%20state-of-the-art%0Aperformance%20across%20various%20tasks%2C%20outperforming%20the%20strongest%20baselines%20by%0A16.5%25%20in%20success%20rate%20on%203DMem-Bench%27s%20most%20challenging%20in-the-wild%20embodied%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22657v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DLLM-Mem%253A%2520Long-Term%2520Spatial-Temporal%2520Memory%2520for%2520Embodied%25203D%2520Large%250A%2520%2520Language%2520Model%26entry.906535625%3DWenbo%2520Hu%2520and%2520Yining%2520Hong%2520and%2520Yanjun%2520Wang%2520and%2520Leison%2520Gao%2520and%2520Zibu%2520Wei%2520and%2520Xingcheng%2520Yao%2520and%2520Nanyun%2520Peng%2520and%2520Yonatan%2520Bitton%2520and%2520Idan%2520Szpektor%2520and%2520Kai-Wei%2520Chang%26entry.1292438233%3D%2520%2520Humans%2520excel%2520at%2520performing%2520complex%2520tasks%2520by%2520leveraging%2520long-term%2520memory%250Aacross%2520temporal%2520and%2520spatial%2520experiences.%2520In%2520contrast%252C%2520current%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520struggle%2520to%2520effectively%2520plan%2520and%2520act%2520in%2520dynamic%252C%2520multi-room%25203D%250Aenvironments.%2520We%2520posit%2520that%2520part%2520of%2520this%2520limitation%2520is%2520due%2520to%2520the%2520lack%2520of%250Aproper%25203D%2520spatial-temporal%2520memory%2520modeling%2520in%2520LLMs.%2520To%2520address%2520this%252C%2520we%2520first%250Aintroduce%25203DMem-Bench%252C%2520a%2520comprehensive%2520benchmark%2520comprising%2520over%252026%252C000%250Atrajectories%2520and%25202%252C892%2520embodied%2520tasks%252C%2520question-answering%2520and%2520captioning%252C%250Adesigned%2520to%2520evaluate%2520an%2520agent%2527s%2520ability%2520to%2520reason%2520over%2520long-term%2520memory%2520in%25203D%250Aenvironments.%2520Second%252C%2520we%2520propose%25203DLLM-Mem%252C%2520a%2520novel%2520dynamic%2520memory%2520management%250Aand%2520fusion%2520model%2520for%2520embodied%2520spatial-temporal%2520reasoning%2520and%2520actions%2520in%2520LLMs.%250AOur%2520model%2520uses%2520working%2520memory%2520tokens%252C%2520which%2520represents%2520current%2520observations%252C%2520as%250Aqueries%2520to%2520selectively%2520attend%2520to%2520and%2520fuse%2520the%2520most%2520useful%2520spatial%2520and%2520temporal%250Afeatures%2520from%2520episodic%2520memory%252C%2520which%2520stores%2520past%2520observations%2520and%2520interactions.%250AOur%2520approach%2520allows%2520the%2520agent%2520to%2520focus%2520on%2520task-relevant%2520information%2520while%250Amaintaining%2520memory%2520efficiency%2520in%2520complex%252C%2520long-horizon%2520environments.%250AExperimental%2520results%2520demonstrate%2520that%25203DLLM-Mem%2520achieves%2520state-of-the-art%250Aperformance%2520across%2520various%2520tasks%252C%2520outperforming%2520the%2520strongest%2520baselines%2520by%250A16.5%2525%2520in%2520success%2520rate%2520on%25203DMem-Bench%2527s%2520most%2520challenging%2520in-the-wild%2520embodied%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22657v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DLLM-Mem%3A%20Long-Term%20Spatial-Temporal%20Memory%20for%20Embodied%203D%20Large%0A%20%20Language%20Model&entry.906535625=Wenbo%20Hu%20and%20Yining%20Hong%20and%20Yanjun%20Wang%20and%20Leison%20Gao%20and%20Zibu%20Wei%20and%20Xingcheng%20Yao%20and%20Nanyun%20Peng%20and%20Yonatan%20Bitton%20and%20Idan%20Szpektor%20and%20Kai-Wei%20Chang&entry.1292438233=%20%20Humans%20excel%20at%20performing%20complex%20tasks%20by%20leveraging%20long-term%20memory%0Aacross%20temporal%20and%20spatial%20experiences.%20In%20contrast%2C%20current%20Large%20Language%0AModels%20%28LLMs%29%20struggle%20to%20effectively%20plan%20and%20act%20in%20dynamic%2C%20multi-room%203D%0Aenvironments.%20We%20posit%20that%20part%20of%20this%20limitation%20is%20due%20to%20the%20lack%20of%0Aproper%203D%20spatial-temporal%20memory%20modeling%20in%20LLMs.%20To%20address%20this%2C%20we%20first%0Aintroduce%203DMem-Bench%2C%20a%20comprehensive%20benchmark%20comprising%20over%2026%2C000%0Atrajectories%20and%202%2C892%20embodied%20tasks%2C%20question-answering%20and%20captioning%2C%0Adesigned%20to%20evaluate%20an%20agent%27s%20ability%20to%20reason%20over%20long-term%20memory%20in%203D%0Aenvironments.%20Second%2C%20we%20propose%203DLLM-Mem%2C%20a%20novel%20dynamic%20memory%20management%0Aand%20fusion%20model%20for%20embodied%20spatial-temporal%20reasoning%20and%20actions%20in%20LLMs.%0AOur%20model%20uses%20working%20memory%20tokens%2C%20which%20represents%20current%20observations%2C%20as%0Aqueries%20to%20selectively%20attend%20to%20and%20fuse%20the%20most%20useful%20spatial%20and%20temporal%0Afeatures%20from%20episodic%20memory%2C%20which%20stores%20past%20observations%20and%20interactions.%0AOur%20approach%20allows%20the%20agent%20to%20focus%20on%20task-relevant%20information%20while%0Amaintaining%20memory%20efficiency%20in%20complex%2C%20long-horizon%20environments.%0AExperimental%20results%20demonstrate%20that%203DLLM-Mem%20achieves%20state-of-the-art%0Aperformance%20across%20various%20tasks%2C%20outperforming%20the%20strongest%20baselines%20by%0A16.5%25%20in%20success%20rate%20on%203DMem-Bench%27s%20most%20challenging%20in-the-wild%20embodied%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22657v1&entry.124074799=Read"},
{"title": "VScan: Rethinking Visual Token Reduction for Efficient Large\n  Vision-Language Models", "author": "Ce Zhang and Kaixin Ma and Tianqing Fang and Wenhao Yu and Hongming Zhang and Zhisong Zhang and Yaqi Xie and Katia Sycara and Haitao Mi and Dong Yu", "abstract": "  Recent Large Vision-Language Models (LVLMs) have advanced multi-modal\nunderstanding by incorporating finer-grained visual perception and encoding.\nHowever, such methods incur significant computational costs due to longer\nvisual token sequences, posing challenges for real-time deployment. To mitigate\nthis, prior studies have explored pruning unimportant visual tokens either at\nthe output layer of the visual encoder or at the early layers of the language\nmodel. In this work, we revisit these design choices and reassess their\neffectiveness through comprehensive empirical studies of how visual tokens are\nprocessed throughout the visual encoding and language decoding stages. Guided\nby these insights, we propose VScan, a two-stage visual token reduction\nframework that addresses token redundancy by: (1) integrating complementary\nglobal and local scans with token merging during visual encoding, and (2)\nintroducing pruning at intermediate layers of the language model. Extensive\nexperimental results across four LVLMs validate the effectiveness of VScan in\naccelerating inference and demonstrate its superior performance over current\nstate-of-the-arts on sixteen benchmarks. Notably, when applied to\nLLaVA-NeXT-7B, VScan achieves a 2.91$\\times$ speedup in prefilling and a\n10$\\times$ reduction in FLOPs, while retaining 95.4% of the original\nperformance.\n", "link": "http://arxiv.org/abs/2505.22654v1", "date": "2025-05-28", "relevancy": 3.0251, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6213}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6213}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5724}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VScan%3A%20Rethinking%20Visual%20Token%20Reduction%20for%20Efficient%20Large%0A%20%20Vision-Language%20Models&body=Title%3A%20VScan%3A%20Rethinking%20Visual%20Token%20Reduction%20for%20Efficient%20Large%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Ce%20Zhang%20and%20Kaixin%20Ma%20and%20Tianqing%20Fang%20and%20Wenhao%20Yu%20and%20Hongming%20Zhang%20and%20Zhisong%20Zhang%20and%20Yaqi%20Xie%20and%20Katia%20Sycara%20and%20Haitao%20Mi%20and%20Dong%20Yu%0AAbstract%3A%20%20%20Recent%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20advanced%20multi-modal%0Aunderstanding%20by%20incorporating%20finer-grained%20visual%20perception%20and%20encoding.%0AHowever%2C%20such%20methods%20incur%20significant%20computational%20costs%20due%20to%20longer%0Avisual%20token%20sequences%2C%20posing%20challenges%20for%20real-time%20deployment.%20To%20mitigate%0Athis%2C%20prior%20studies%20have%20explored%20pruning%20unimportant%20visual%20tokens%20either%20at%0Athe%20output%20layer%20of%20the%20visual%20encoder%20or%20at%20the%20early%20layers%20of%20the%20language%0Amodel.%20In%20this%20work%2C%20we%20revisit%20these%20design%20choices%20and%20reassess%20their%0Aeffectiveness%20through%20comprehensive%20empirical%20studies%20of%20how%20visual%20tokens%20are%0Aprocessed%20throughout%20the%20visual%20encoding%20and%20language%20decoding%20stages.%20Guided%0Aby%20these%20insights%2C%20we%20propose%20VScan%2C%20a%20two-stage%20visual%20token%20reduction%0Aframework%20that%20addresses%20token%20redundancy%20by%3A%20%281%29%20integrating%20complementary%0Aglobal%20and%20local%20scans%20with%20token%20merging%20during%20visual%20encoding%2C%20and%20%282%29%0Aintroducing%20pruning%20at%20intermediate%20layers%20of%20the%20language%20model.%20Extensive%0Aexperimental%20results%20across%20four%20LVLMs%20validate%20the%20effectiveness%20of%20VScan%20in%0Aaccelerating%20inference%20and%20demonstrate%20its%20superior%20performance%20over%20current%0Astate-of-the-arts%20on%20sixteen%20benchmarks.%20Notably%2C%20when%20applied%20to%0ALLaVA-NeXT-7B%2C%20VScan%20achieves%20a%202.91%24%5Ctimes%24%20speedup%20in%20prefilling%20and%20a%0A10%24%5Ctimes%24%20reduction%20in%20FLOPs%2C%20while%20retaining%2095.4%25%20of%20the%20original%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVScan%253A%2520Rethinking%2520Visual%2520Token%2520Reduction%2520for%2520Efficient%2520Large%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DCe%2520Zhang%2520and%2520Kaixin%2520Ma%2520and%2520Tianqing%2520Fang%2520and%2520Wenhao%2520Yu%2520and%2520Hongming%2520Zhang%2520and%2520Zhisong%2520Zhang%2520and%2520Yaqi%2520Xie%2520and%2520Katia%2520Sycara%2520and%2520Haitao%2520Mi%2520and%2520Dong%2520Yu%26entry.1292438233%3D%2520%2520Recent%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520advanced%2520multi-modal%250Aunderstanding%2520by%2520incorporating%2520finer-grained%2520visual%2520perception%2520and%2520encoding.%250AHowever%252C%2520such%2520methods%2520incur%2520significant%2520computational%2520costs%2520due%2520to%2520longer%250Avisual%2520token%2520sequences%252C%2520posing%2520challenges%2520for%2520real-time%2520deployment.%2520To%2520mitigate%250Athis%252C%2520prior%2520studies%2520have%2520explored%2520pruning%2520unimportant%2520visual%2520tokens%2520either%2520at%250Athe%2520output%2520layer%2520of%2520the%2520visual%2520encoder%2520or%2520at%2520the%2520early%2520layers%2520of%2520the%2520language%250Amodel.%2520In%2520this%2520work%252C%2520we%2520revisit%2520these%2520design%2520choices%2520and%2520reassess%2520their%250Aeffectiveness%2520through%2520comprehensive%2520empirical%2520studies%2520of%2520how%2520visual%2520tokens%2520are%250Aprocessed%2520throughout%2520the%2520visual%2520encoding%2520and%2520language%2520decoding%2520stages.%2520Guided%250Aby%2520these%2520insights%252C%2520we%2520propose%2520VScan%252C%2520a%2520two-stage%2520visual%2520token%2520reduction%250Aframework%2520that%2520addresses%2520token%2520redundancy%2520by%253A%2520%25281%2529%2520integrating%2520complementary%250Aglobal%2520and%2520local%2520scans%2520with%2520token%2520merging%2520during%2520visual%2520encoding%252C%2520and%2520%25282%2529%250Aintroducing%2520pruning%2520at%2520intermediate%2520layers%2520of%2520the%2520language%2520model.%2520Extensive%250Aexperimental%2520results%2520across%2520four%2520LVLMs%2520validate%2520the%2520effectiveness%2520of%2520VScan%2520in%250Aaccelerating%2520inference%2520and%2520demonstrate%2520its%2520superior%2520performance%2520over%2520current%250Astate-of-the-arts%2520on%2520sixteen%2520benchmarks.%2520Notably%252C%2520when%2520applied%2520to%250ALLaVA-NeXT-7B%252C%2520VScan%2520achieves%2520a%25202.91%2524%255Ctimes%2524%2520speedup%2520in%2520prefilling%2520and%2520a%250A10%2524%255Ctimes%2524%2520reduction%2520in%2520FLOPs%252C%2520while%2520retaining%252095.4%2525%2520of%2520the%2520original%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VScan%3A%20Rethinking%20Visual%20Token%20Reduction%20for%20Efficient%20Large%0A%20%20Vision-Language%20Models&entry.906535625=Ce%20Zhang%20and%20Kaixin%20Ma%20and%20Tianqing%20Fang%20and%20Wenhao%20Yu%20and%20Hongming%20Zhang%20and%20Zhisong%20Zhang%20and%20Yaqi%20Xie%20and%20Katia%20Sycara%20and%20Haitao%20Mi%20and%20Dong%20Yu&entry.1292438233=%20%20Recent%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20advanced%20multi-modal%0Aunderstanding%20by%20incorporating%20finer-grained%20visual%20perception%20and%20encoding.%0AHowever%2C%20such%20methods%20incur%20significant%20computational%20costs%20due%20to%20longer%0Avisual%20token%20sequences%2C%20posing%20challenges%20for%20real-time%20deployment.%20To%20mitigate%0Athis%2C%20prior%20studies%20have%20explored%20pruning%20unimportant%20visual%20tokens%20either%20at%0Athe%20output%20layer%20of%20the%20visual%20encoder%20or%20at%20the%20early%20layers%20of%20the%20language%0Amodel.%20In%20this%20work%2C%20we%20revisit%20these%20design%20choices%20and%20reassess%20their%0Aeffectiveness%20through%20comprehensive%20empirical%20studies%20of%20how%20visual%20tokens%20are%0Aprocessed%20throughout%20the%20visual%20encoding%20and%20language%20decoding%20stages.%20Guided%0Aby%20these%20insights%2C%20we%20propose%20VScan%2C%20a%20two-stage%20visual%20token%20reduction%0Aframework%20that%20addresses%20token%20redundancy%20by%3A%20%281%29%20integrating%20complementary%0Aglobal%20and%20local%20scans%20with%20token%20merging%20during%20visual%20encoding%2C%20and%20%282%29%0Aintroducing%20pruning%20at%20intermediate%20layers%20of%20the%20language%20model.%20Extensive%0Aexperimental%20results%20across%20four%20LVLMs%20validate%20the%20effectiveness%20of%20VScan%20in%0Aaccelerating%20inference%20and%20demonstrate%20its%20superior%20performance%20over%20current%0Astate-of-the-arts%20on%20sixteen%20benchmarks.%20Notably%2C%20when%20applied%20to%0ALLaVA-NeXT-7B%2C%20VScan%20achieves%20a%202.91%24%5Ctimes%24%20speedup%20in%20prefilling%20and%20a%0A10%24%5Ctimes%24%20reduction%20in%20FLOPs%2C%20while%20retaining%2095.4%25%20of%20the%20original%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22654v1&entry.124074799=Read"},
{"title": "PacTure: Efficient PBR Texture Generation on Packed Views with Visual\n  Autoregressive Models", "author": "Fan Fei and Jiajun Tang and Fei-Peng Tian and Boxin Shi and Ping Tan", "abstract": "  We present PacTure, a novel framework for generating physically-based\nrendering (PBR) material textures from an untextured 3D mesh, a text\ndescription, and an optional image prompt. Early 2D generation-based texturing\napproaches generate textures sequentially from different views, resulting in\nlong inference times and globally inconsistent textures. More recent approaches\nadopt multi-view generation with cross-view attention to enhance global\nconsistency, which, however, limits the resolution for each view. In response\nto these weaknesses, we first introduce view packing, a novel technique that\nsignificantly increases the effective resolution for each view during\nmulti-view generation without imposing additional inference cost, by\nformulating the arrangement of multi-view maps as a 2D rectangle bin packing\nproblem. In contrast to UV mapping, it preserves the spatial proximity\nessential for image generation and maintains full compatibility with current 2D\ngenerative models. To further reduce the inference cost, we enable fine-grained\ncontrol and multi-domain generation within the next-scale prediction\nautoregressive framework to create an efficient multi-view multi-domain\ngenerative backbone. Extensive experiments show that PacTure outperforms\nstate-of-the-art methods in both quality of generated PBR textures and\nefficiency in training and inference.\n", "link": "http://arxiv.org/abs/2505.22394v1", "date": "2025-05-28", "relevancy": 2.9798, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.601}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.601}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PacTure%3A%20Efficient%20PBR%20Texture%20Generation%20on%20Packed%20Views%20with%20Visual%0A%20%20Autoregressive%20Models&body=Title%3A%20PacTure%3A%20Efficient%20PBR%20Texture%20Generation%20on%20Packed%20Views%20with%20Visual%0A%20%20Autoregressive%20Models%0AAuthor%3A%20Fan%20Fei%20and%20Jiajun%20Tang%20and%20Fei-Peng%20Tian%20and%20Boxin%20Shi%20and%20Ping%20Tan%0AAbstract%3A%20%20%20We%20present%20PacTure%2C%20a%20novel%20framework%20for%20generating%20physically-based%0Arendering%20%28PBR%29%20material%20textures%20from%20an%20untextured%203D%20mesh%2C%20a%20text%0Adescription%2C%20and%20an%20optional%20image%20prompt.%20Early%202D%20generation-based%20texturing%0Aapproaches%20generate%20textures%20sequentially%20from%20different%20views%2C%20resulting%20in%0Along%20inference%20times%20and%20globally%20inconsistent%20textures.%20More%20recent%20approaches%0Aadopt%20multi-view%20generation%20with%20cross-view%20attention%20to%20enhance%20global%0Aconsistency%2C%20which%2C%20however%2C%20limits%20the%20resolution%20for%20each%20view.%20In%20response%0Ato%20these%20weaknesses%2C%20we%20first%20introduce%20view%20packing%2C%20a%20novel%20technique%20that%0Asignificantly%20increases%20the%20effective%20resolution%20for%20each%20view%20during%0Amulti-view%20generation%20without%20imposing%20additional%20inference%20cost%2C%20by%0Aformulating%20the%20arrangement%20of%20multi-view%20maps%20as%20a%202D%20rectangle%20bin%20packing%0Aproblem.%20In%20contrast%20to%20UV%20mapping%2C%20it%20preserves%20the%20spatial%20proximity%0Aessential%20for%20image%20generation%20and%20maintains%20full%20compatibility%20with%20current%202D%0Agenerative%20models.%20To%20further%20reduce%20the%20inference%20cost%2C%20we%20enable%20fine-grained%0Acontrol%20and%20multi-domain%20generation%20within%20the%20next-scale%20prediction%0Aautoregressive%20framework%20to%20create%20an%20efficient%20multi-view%20multi-domain%0Agenerative%20backbone.%20Extensive%20experiments%20show%20that%20PacTure%20outperforms%0Astate-of-the-art%20methods%20in%20both%20quality%20of%20generated%20PBR%20textures%20and%0Aefficiency%20in%20training%20and%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPacTure%253A%2520Efficient%2520PBR%2520Texture%2520Generation%2520on%2520Packed%2520Views%2520with%2520Visual%250A%2520%2520Autoregressive%2520Models%26entry.906535625%3DFan%2520Fei%2520and%2520Jiajun%2520Tang%2520and%2520Fei-Peng%2520Tian%2520and%2520Boxin%2520Shi%2520and%2520Ping%2520Tan%26entry.1292438233%3D%2520%2520We%2520present%2520PacTure%252C%2520a%2520novel%2520framework%2520for%2520generating%2520physically-based%250Arendering%2520%2528PBR%2529%2520material%2520textures%2520from%2520an%2520untextured%25203D%2520mesh%252C%2520a%2520text%250Adescription%252C%2520and%2520an%2520optional%2520image%2520prompt.%2520Early%25202D%2520generation-based%2520texturing%250Aapproaches%2520generate%2520textures%2520sequentially%2520from%2520different%2520views%252C%2520resulting%2520in%250Along%2520inference%2520times%2520and%2520globally%2520inconsistent%2520textures.%2520More%2520recent%2520approaches%250Aadopt%2520multi-view%2520generation%2520with%2520cross-view%2520attention%2520to%2520enhance%2520global%250Aconsistency%252C%2520which%252C%2520however%252C%2520limits%2520the%2520resolution%2520for%2520each%2520view.%2520In%2520response%250Ato%2520these%2520weaknesses%252C%2520we%2520first%2520introduce%2520view%2520packing%252C%2520a%2520novel%2520technique%2520that%250Asignificantly%2520increases%2520the%2520effective%2520resolution%2520for%2520each%2520view%2520during%250Amulti-view%2520generation%2520without%2520imposing%2520additional%2520inference%2520cost%252C%2520by%250Aformulating%2520the%2520arrangement%2520of%2520multi-view%2520maps%2520as%2520a%25202D%2520rectangle%2520bin%2520packing%250Aproblem.%2520In%2520contrast%2520to%2520UV%2520mapping%252C%2520it%2520preserves%2520the%2520spatial%2520proximity%250Aessential%2520for%2520image%2520generation%2520and%2520maintains%2520full%2520compatibility%2520with%2520current%25202D%250Agenerative%2520models.%2520To%2520further%2520reduce%2520the%2520inference%2520cost%252C%2520we%2520enable%2520fine-grained%250Acontrol%2520and%2520multi-domain%2520generation%2520within%2520the%2520next-scale%2520prediction%250Aautoregressive%2520framework%2520to%2520create%2520an%2520efficient%2520multi-view%2520multi-domain%250Agenerative%2520backbone.%2520Extensive%2520experiments%2520show%2520that%2520PacTure%2520outperforms%250Astate-of-the-art%2520methods%2520in%2520both%2520quality%2520of%2520generated%2520PBR%2520textures%2520and%250Aefficiency%2520in%2520training%2520and%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PacTure%3A%20Efficient%20PBR%20Texture%20Generation%20on%20Packed%20Views%20with%20Visual%0A%20%20Autoregressive%20Models&entry.906535625=Fan%20Fei%20and%20Jiajun%20Tang%20and%20Fei-Peng%20Tian%20and%20Boxin%20Shi%20and%20Ping%20Tan&entry.1292438233=%20%20We%20present%20PacTure%2C%20a%20novel%20framework%20for%20generating%20physically-based%0Arendering%20%28PBR%29%20material%20textures%20from%20an%20untextured%203D%20mesh%2C%20a%20text%0Adescription%2C%20and%20an%20optional%20image%20prompt.%20Early%202D%20generation-based%20texturing%0Aapproaches%20generate%20textures%20sequentially%20from%20different%20views%2C%20resulting%20in%0Along%20inference%20times%20and%20globally%20inconsistent%20textures.%20More%20recent%20approaches%0Aadopt%20multi-view%20generation%20with%20cross-view%20attention%20to%20enhance%20global%0Aconsistency%2C%20which%2C%20however%2C%20limits%20the%20resolution%20for%20each%20view.%20In%20response%0Ato%20these%20weaknesses%2C%20we%20first%20introduce%20view%20packing%2C%20a%20novel%20technique%20that%0Asignificantly%20increases%20the%20effective%20resolution%20for%20each%20view%20during%0Amulti-view%20generation%20without%20imposing%20additional%20inference%20cost%2C%20by%0Aformulating%20the%20arrangement%20of%20multi-view%20maps%20as%20a%202D%20rectangle%20bin%20packing%0Aproblem.%20In%20contrast%20to%20UV%20mapping%2C%20it%20preserves%20the%20spatial%20proximity%0Aessential%20for%20image%20generation%20and%20maintains%20full%20compatibility%20with%20current%202D%0Agenerative%20models.%20To%20further%20reduce%20the%20inference%20cost%2C%20we%20enable%20fine-grained%0Acontrol%20and%20multi-domain%20generation%20within%20the%20next-scale%20prediction%0Aautoregressive%20framework%20to%20create%20an%20efficient%20multi-view%20multi-domain%0Agenerative%20backbone.%20Extensive%20experiments%20show%20that%20PacTure%20outperforms%0Astate-of-the-art%20methods%20in%20both%20quality%20of%20generated%20PBR%20textures%20and%0Aefficiency%20in%20training%20and%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22394v1&entry.124074799=Read"},
{"title": "Surf2CT: Cascaded 3D Flow Matching Models for Torso 3D CT Synthesis from\n  Skin Surface", "author": "Siyeop Yoon and Yujin Oh and Pengfei Jin and Sifan Song and Matthew Tivnan and Dufan Wu and Xiang Li and Quanzheng Li", "abstract": "  We present Surf2CT, a novel cascaded flow matching framework that synthesizes\nfull 3D computed tomography (CT) volumes of the human torso from external\nsurface scans and simple demographic data (age, sex, height, weight). This is\nthe first approach capable of generating realistic volumetric internal anatomy\nimages solely based on external body shape and demographics, without any\ninternal imaging. Surf2CT proceeds through three sequential stages: (1) Surface\nCompletion, reconstructing a complete signed distance function (SDF) from\npartial torso scans using conditional 3D flow matching; (2) Coarse CT\nSynthesis, generating a low-resolution CT volume from the completed SDF and\ndemographic information; and (3) CT Super-Resolution, refining the coarse\nvolume into a high-resolution CT via a patch-wise conditional flow model. Each\nstage utilizes a 3D-adapted EDM2 backbone trained via flow matching. We trained\nour model on a combined dataset of 3,198 torso CT scans (approximately 1.13\nmillion axial slices) sourced from Massachusetts General Hospital (MGH) and the\nAutoPET challenge. Evaluation on 700 paired torso surface-CT cases demonstrated\nstrong anatomical fidelity: organ volumes exhibited small mean percentage\ndifferences (range from -11.1% to 4.4%), and muscle/fat body composition\nmetrics matched ground truth with strong correlation (range from 0.67 to 0.96).\nLung localization had minimal bias (mean difference -2.5 mm), and surface\ncompletion significantly improved metrics (Chamfer distance: from 521.8 mm to\n2.7 mm; Intersection-over-Union: from 0.87 to 0.98). Surf2CT establishes a new\nparadigm for non-invasive internal anatomical imaging using only external data,\nopening opportunities for home-based healthcare, preventive medicine, and\npersonalized clinical assessments without the risks associated with\nconventional imaging techniques.\n", "link": "http://arxiv.org/abs/2505.22511v1", "date": "2025-05-28", "relevancy": 2.9795, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5965}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5956}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Surf2CT%3A%20Cascaded%203D%20Flow%20Matching%20Models%20for%20Torso%203D%20CT%20Synthesis%20from%0A%20%20Skin%20Surface&body=Title%3A%20Surf2CT%3A%20Cascaded%203D%20Flow%20Matching%20Models%20for%20Torso%203D%20CT%20Synthesis%20from%0A%20%20Skin%20Surface%0AAuthor%3A%20Siyeop%20Yoon%20and%20Yujin%20Oh%20and%20Pengfei%20Jin%20and%20Sifan%20Song%20and%20Matthew%20Tivnan%20and%20Dufan%20Wu%20and%20Xiang%20Li%20and%20Quanzheng%20Li%0AAbstract%3A%20%20%20We%20present%20Surf2CT%2C%20a%20novel%20cascaded%20flow%20matching%20framework%20that%20synthesizes%0Afull%203D%20computed%20tomography%20%28CT%29%20volumes%20of%20the%20human%20torso%20from%20external%0Asurface%20scans%20and%20simple%20demographic%20data%20%28age%2C%20sex%2C%20height%2C%20weight%29.%20This%20is%0Athe%20first%20approach%20capable%20of%20generating%20realistic%20volumetric%20internal%20anatomy%0Aimages%20solely%20based%20on%20external%20body%20shape%20and%20demographics%2C%20without%20any%0Ainternal%20imaging.%20Surf2CT%20proceeds%20through%20three%20sequential%20stages%3A%20%281%29%20Surface%0ACompletion%2C%20reconstructing%20a%20complete%20signed%20distance%20function%20%28SDF%29%20from%0Apartial%20torso%20scans%20using%20conditional%203D%20flow%20matching%3B%20%282%29%20Coarse%20CT%0ASynthesis%2C%20generating%20a%20low-resolution%20CT%20volume%20from%20the%20completed%20SDF%20and%0Ademographic%20information%3B%20and%20%283%29%20CT%20Super-Resolution%2C%20refining%20the%20coarse%0Avolume%20into%20a%20high-resolution%20CT%20via%20a%20patch-wise%20conditional%20flow%20model.%20Each%0Astage%20utilizes%20a%203D-adapted%20EDM2%20backbone%20trained%20via%20flow%20matching.%20We%20trained%0Aour%20model%20on%20a%20combined%20dataset%20of%203%2C198%20torso%20CT%20scans%20%28approximately%201.13%0Amillion%20axial%20slices%29%20sourced%20from%20Massachusetts%20General%20Hospital%20%28MGH%29%20and%20the%0AAutoPET%20challenge.%20Evaluation%20on%20700%20paired%20torso%20surface-CT%20cases%20demonstrated%0Astrong%20anatomical%20fidelity%3A%20organ%20volumes%20exhibited%20small%20mean%20percentage%0Adifferences%20%28range%20from%20-11.1%25%20to%204.4%25%29%2C%20and%20muscle/fat%20body%20composition%0Ametrics%20matched%20ground%20truth%20with%20strong%20correlation%20%28range%20from%200.67%20to%200.96%29.%0ALung%20localization%20had%20minimal%20bias%20%28mean%20difference%20-2.5%20mm%29%2C%20and%20surface%0Acompletion%20significantly%20improved%20metrics%20%28Chamfer%20distance%3A%20from%20521.8%20mm%20to%0A2.7%20mm%3B%20Intersection-over-Union%3A%20from%200.87%20to%200.98%29.%20Surf2CT%20establishes%20a%20new%0Aparadigm%20for%20non-invasive%20internal%20anatomical%20imaging%20using%20only%20external%20data%2C%0Aopening%20opportunities%20for%20home-based%20healthcare%2C%20preventive%20medicine%2C%20and%0Apersonalized%20clinical%20assessments%20without%20the%20risks%20associated%20with%0Aconventional%20imaging%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22511v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurf2CT%253A%2520Cascaded%25203D%2520Flow%2520Matching%2520Models%2520for%2520Torso%25203D%2520CT%2520Synthesis%2520from%250A%2520%2520Skin%2520Surface%26entry.906535625%3DSiyeop%2520Yoon%2520and%2520Yujin%2520Oh%2520and%2520Pengfei%2520Jin%2520and%2520Sifan%2520Song%2520and%2520Matthew%2520Tivnan%2520and%2520Dufan%2520Wu%2520and%2520Xiang%2520Li%2520and%2520Quanzheng%2520Li%26entry.1292438233%3D%2520%2520We%2520present%2520Surf2CT%252C%2520a%2520novel%2520cascaded%2520flow%2520matching%2520framework%2520that%2520synthesizes%250Afull%25203D%2520computed%2520tomography%2520%2528CT%2529%2520volumes%2520of%2520the%2520human%2520torso%2520from%2520external%250Asurface%2520scans%2520and%2520simple%2520demographic%2520data%2520%2528age%252C%2520sex%252C%2520height%252C%2520weight%2529.%2520This%2520is%250Athe%2520first%2520approach%2520capable%2520of%2520generating%2520realistic%2520volumetric%2520internal%2520anatomy%250Aimages%2520solely%2520based%2520on%2520external%2520body%2520shape%2520and%2520demographics%252C%2520without%2520any%250Ainternal%2520imaging.%2520Surf2CT%2520proceeds%2520through%2520three%2520sequential%2520stages%253A%2520%25281%2529%2520Surface%250ACompletion%252C%2520reconstructing%2520a%2520complete%2520signed%2520distance%2520function%2520%2528SDF%2529%2520from%250Apartial%2520torso%2520scans%2520using%2520conditional%25203D%2520flow%2520matching%253B%2520%25282%2529%2520Coarse%2520CT%250ASynthesis%252C%2520generating%2520a%2520low-resolution%2520CT%2520volume%2520from%2520the%2520completed%2520SDF%2520and%250Ademographic%2520information%253B%2520and%2520%25283%2529%2520CT%2520Super-Resolution%252C%2520refining%2520the%2520coarse%250Avolume%2520into%2520a%2520high-resolution%2520CT%2520via%2520a%2520patch-wise%2520conditional%2520flow%2520model.%2520Each%250Astage%2520utilizes%2520a%25203D-adapted%2520EDM2%2520backbone%2520trained%2520via%2520flow%2520matching.%2520We%2520trained%250Aour%2520model%2520on%2520a%2520combined%2520dataset%2520of%25203%252C198%2520torso%2520CT%2520scans%2520%2528approximately%25201.13%250Amillion%2520axial%2520slices%2529%2520sourced%2520from%2520Massachusetts%2520General%2520Hospital%2520%2528MGH%2529%2520and%2520the%250AAutoPET%2520challenge.%2520Evaluation%2520on%2520700%2520paired%2520torso%2520surface-CT%2520cases%2520demonstrated%250Astrong%2520anatomical%2520fidelity%253A%2520organ%2520volumes%2520exhibited%2520small%2520mean%2520percentage%250Adifferences%2520%2528range%2520from%2520-11.1%2525%2520to%25204.4%2525%2529%252C%2520and%2520muscle/fat%2520body%2520composition%250Ametrics%2520matched%2520ground%2520truth%2520with%2520strong%2520correlation%2520%2528range%2520from%25200.67%2520to%25200.96%2529.%250ALung%2520localization%2520had%2520minimal%2520bias%2520%2528mean%2520difference%2520-2.5%2520mm%2529%252C%2520and%2520surface%250Acompletion%2520significantly%2520improved%2520metrics%2520%2528Chamfer%2520distance%253A%2520from%2520521.8%2520mm%2520to%250A2.7%2520mm%253B%2520Intersection-over-Union%253A%2520from%25200.87%2520to%25200.98%2529.%2520Surf2CT%2520establishes%2520a%2520new%250Aparadigm%2520for%2520non-invasive%2520internal%2520anatomical%2520imaging%2520using%2520only%2520external%2520data%252C%250Aopening%2520opportunities%2520for%2520home-based%2520healthcare%252C%2520preventive%2520medicine%252C%2520and%250Apersonalized%2520clinical%2520assessments%2520without%2520the%2520risks%2520associated%2520with%250Aconventional%2520imaging%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22511v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Surf2CT%3A%20Cascaded%203D%20Flow%20Matching%20Models%20for%20Torso%203D%20CT%20Synthesis%20from%0A%20%20Skin%20Surface&entry.906535625=Siyeop%20Yoon%20and%20Yujin%20Oh%20and%20Pengfei%20Jin%20and%20Sifan%20Song%20and%20Matthew%20Tivnan%20and%20Dufan%20Wu%20and%20Xiang%20Li%20and%20Quanzheng%20Li&entry.1292438233=%20%20We%20present%20Surf2CT%2C%20a%20novel%20cascaded%20flow%20matching%20framework%20that%20synthesizes%0Afull%203D%20computed%20tomography%20%28CT%29%20volumes%20of%20the%20human%20torso%20from%20external%0Asurface%20scans%20and%20simple%20demographic%20data%20%28age%2C%20sex%2C%20height%2C%20weight%29.%20This%20is%0Athe%20first%20approach%20capable%20of%20generating%20realistic%20volumetric%20internal%20anatomy%0Aimages%20solely%20based%20on%20external%20body%20shape%20and%20demographics%2C%20without%20any%0Ainternal%20imaging.%20Surf2CT%20proceeds%20through%20three%20sequential%20stages%3A%20%281%29%20Surface%0ACompletion%2C%20reconstructing%20a%20complete%20signed%20distance%20function%20%28SDF%29%20from%0Apartial%20torso%20scans%20using%20conditional%203D%20flow%20matching%3B%20%282%29%20Coarse%20CT%0ASynthesis%2C%20generating%20a%20low-resolution%20CT%20volume%20from%20the%20completed%20SDF%20and%0Ademographic%20information%3B%20and%20%283%29%20CT%20Super-Resolution%2C%20refining%20the%20coarse%0Avolume%20into%20a%20high-resolution%20CT%20via%20a%20patch-wise%20conditional%20flow%20model.%20Each%0Astage%20utilizes%20a%203D-adapted%20EDM2%20backbone%20trained%20via%20flow%20matching.%20We%20trained%0Aour%20model%20on%20a%20combined%20dataset%20of%203%2C198%20torso%20CT%20scans%20%28approximately%201.13%0Amillion%20axial%20slices%29%20sourced%20from%20Massachusetts%20General%20Hospital%20%28MGH%29%20and%20the%0AAutoPET%20challenge.%20Evaluation%20on%20700%20paired%20torso%20surface-CT%20cases%20demonstrated%0Astrong%20anatomical%20fidelity%3A%20organ%20volumes%20exhibited%20small%20mean%20percentage%0Adifferences%20%28range%20from%20-11.1%25%20to%204.4%25%29%2C%20and%20muscle/fat%20body%20composition%0Ametrics%20matched%20ground%20truth%20with%20strong%20correlation%20%28range%20from%200.67%20to%200.96%29.%0ALung%20localization%20had%20minimal%20bias%20%28mean%20difference%20-2.5%20mm%29%2C%20and%20surface%0Acompletion%20significantly%20improved%20metrics%20%28Chamfer%20distance%3A%20from%20521.8%20mm%20to%0A2.7%20mm%3B%20Intersection-over-Union%3A%20from%200.87%20to%200.98%29.%20Surf2CT%20establishes%20a%20new%0Aparadigm%20for%20non-invasive%20internal%20anatomical%20imaging%20using%20only%20external%20data%2C%0Aopening%20opportunities%20for%20home-based%20healthcare%2C%20preventive%20medicine%2C%20and%0Apersonalized%20clinical%20assessments%20without%20the%20risks%20associated%20with%0Aconventional%20imaging%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22511v1&entry.124074799=Read"},
{"title": "Universal Visuo-Tactile Video Understanding for Embodied Interaction", "author": "Yifan Xie and Mingyang Li and Shoujie Li and Xingting Li and Guangyu Chen and Fei Ma and Fei Richard Yu and Wenbo Ding", "abstract": "  Tactile perception is essential for embodied agents to understand physical\nattributes of objects that cannot be determined through visual inspection\nalone. While existing approaches have made progress in visual and language\nmodalities for physical understanding, they fail to effectively incorporate\ntactile information that provides crucial haptic feedback for real-world\ninteraction. In this paper, we present VTV-LLM, the first multi-modal large\nlanguage model for universal Visuo-Tactile Video (VTV) understanding that\nbridges the gap between tactile perception and natural language. To address the\nchallenges of cross-sensor and cross-modal integration, we contribute VTV150K,\na comprehensive dataset comprising 150,000 video frames from 100 diverse\nobjects captured across three different tactile sensors (GelSight Mini, DIGIT,\nand Tac3D), annotated with four fundamental tactile attributes (hardness,\nprotrusion, elasticity, and friction). We develop a novel three-stage training\nparadigm that includes VTV enhancement for robust visuo-tactile representation,\nVTV-text alignment for cross-modal correspondence, and text prompt finetuning\nfor natural language generation. Our framework enables sophisticated tactile\nreasoning capabilities including feature assessment, comparative analysis,\nscenario-based decision making and so on. Experimental evaluations demonstrate\nthat VTV-LLM achieves superior performance in tactile video understanding\ntasks, establishing a foundation for more intuitive human-machine interaction\nin tactile domains.\n", "link": "http://arxiv.org/abs/2505.22566v1", "date": "2025-05-28", "relevancy": 2.9234, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6007}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5767}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Universal%20Visuo-Tactile%20Video%20Understanding%20for%20Embodied%20Interaction&body=Title%3A%20Universal%20Visuo-Tactile%20Video%20Understanding%20for%20Embodied%20Interaction%0AAuthor%3A%20Yifan%20Xie%20and%20Mingyang%20Li%20and%20Shoujie%20Li%20and%20Xingting%20Li%20and%20Guangyu%20Chen%20and%20Fei%20Ma%20and%20Fei%20Richard%20Yu%20and%20Wenbo%20Ding%0AAbstract%3A%20%20%20Tactile%20perception%20is%20essential%20for%20embodied%20agents%20to%20understand%20physical%0Aattributes%20of%20objects%20that%20cannot%20be%20determined%20through%20visual%20inspection%0Aalone.%20While%20existing%20approaches%20have%20made%20progress%20in%20visual%20and%20language%0Amodalities%20for%20physical%20understanding%2C%20they%20fail%20to%20effectively%20incorporate%0Atactile%20information%20that%20provides%20crucial%20haptic%20feedback%20for%20real-world%0Ainteraction.%20In%20this%20paper%2C%20we%20present%20VTV-LLM%2C%20the%20first%20multi-modal%20large%0Alanguage%20model%20for%20universal%20Visuo-Tactile%20Video%20%28VTV%29%20understanding%20that%0Abridges%20the%20gap%20between%20tactile%20perception%20and%20natural%20language.%20To%20address%20the%0Achallenges%20of%20cross-sensor%20and%20cross-modal%20integration%2C%20we%20contribute%20VTV150K%2C%0Aa%20comprehensive%20dataset%20comprising%20150%2C000%20video%20frames%20from%20100%20diverse%0Aobjects%20captured%20across%20three%20different%20tactile%20sensors%20%28GelSight%20Mini%2C%20DIGIT%2C%0Aand%20Tac3D%29%2C%20annotated%20with%20four%20fundamental%20tactile%20attributes%20%28hardness%2C%0Aprotrusion%2C%20elasticity%2C%20and%20friction%29.%20We%20develop%20a%20novel%20three-stage%20training%0Aparadigm%20that%20includes%20VTV%20enhancement%20for%20robust%20visuo-tactile%20representation%2C%0AVTV-text%20alignment%20for%20cross-modal%20correspondence%2C%20and%20text%20prompt%20finetuning%0Afor%20natural%20language%20generation.%20Our%20framework%20enables%20sophisticated%20tactile%0Areasoning%20capabilities%20including%20feature%20assessment%2C%20comparative%20analysis%2C%0Ascenario-based%20decision%20making%20and%20so%20on.%20Experimental%20evaluations%20demonstrate%0Athat%20VTV-LLM%20achieves%20superior%20performance%20in%20tactile%20video%20understanding%0Atasks%2C%20establishing%20a%20foundation%20for%20more%20intuitive%20human-machine%20interaction%0Ain%20tactile%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22566v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniversal%2520Visuo-Tactile%2520Video%2520Understanding%2520for%2520Embodied%2520Interaction%26entry.906535625%3DYifan%2520Xie%2520and%2520Mingyang%2520Li%2520and%2520Shoujie%2520Li%2520and%2520Xingting%2520Li%2520and%2520Guangyu%2520Chen%2520and%2520Fei%2520Ma%2520and%2520Fei%2520Richard%2520Yu%2520and%2520Wenbo%2520Ding%26entry.1292438233%3D%2520%2520Tactile%2520perception%2520is%2520essential%2520for%2520embodied%2520agents%2520to%2520understand%2520physical%250Aattributes%2520of%2520objects%2520that%2520cannot%2520be%2520determined%2520through%2520visual%2520inspection%250Aalone.%2520While%2520existing%2520approaches%2520have%2520made%2520progress%2520in%2520visual%2520and%2520language%250Amodalities%2520for%2520physical%2520understanding%252C%2520they%2520fail%2520to%2520effectively%2520incorporate%250Atactile%2520information%2520that%2520provides%2520crucial%2520haptic%2520feedback%2520for%2520real-world%250Ainteraction.%2520In%2520this%2520paper%252C%2520we%2520present%2520VTV-LLM%252C%2520the%2520first%2520multi-modal%2520large%250Alanguage%2520model%2520for%2520universal%2520Visuo-Tactile%2520Video%2520%2528VTV%2529%2520understanding%2520that%250Abridges%2520the%2520gap%2520between%2520tactile%2520perception%2520and%2520natural%2520language.%2520To%2520address%2520the%250Achallenges%2520of%2520cross-sensor%2520and%2520cross-modal%2520integration%252C%2520we%2520contribute%2520VTV150K%252C%250Aa%2520comprehensive%2520dataset%2520comprising%2520150%252C000%2520video%2520frames%2520from%2520100%2520diverse%250Aobjects%2520captured%2520across%2520three%2520different%2520tactile%2520sensors%2520%2528GelSight%2520Mini%252C%2520DIGIT%252C%250Aand%2520Tac3D%2529%252C%2520annotated%2520with%2520four%2520fundamental%2520tactile%2520attributes%2520%2528hardness%252C%250Aprotrusion%252C%2520elasticity%252C%2520and%2520friction%2529.%2520We%2520develop%2520a%2520novel%2520three-stage%2520training%250Aparadigm%2520that%2520includes%2520VTV%2520enhancement%2520for%2520robust%2520visuo-tactile%2520representation%252C%250AVTV-text%2520alignment%2520for%2520cross-modal%2520correspondence%252C%2520and%2520text%2520prompt%2520finetuning%250Afor%2520natural%2520language%2520generation.%2520Our%2520framework%2520enables%2520sophisticated%2520tactile%250Areasoning%2520capabilities%2520including%2520feature%2520assessment%252C%2520comparative%2520analysis%252C%250Ascenario-based%2520decision%2520making%2520and%2520so%2520on.%2520Experimental%2520evaluations%2520demonstrate%250Athat%2520VTV-LLM%2520achieves%2520superior%2520performance%2520in%2520tactile%2520video%2520understanding%250Atasks%252C%2520establishing%2520a%2520foundation%2520for%2520more%2520intuitive%2520human-machine%2520interaction%250Ain%2520tactile%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22566v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universal%20Visuo-Tactile%20Video%20Understanding%20for%20Embodied%20Interaction&entry.906535625=Yifan%20Xie%20and%20Mingyang%20Li%20and%20Shoujie%20Li%20and%20Xingting%20Li%20and%20Guangyu%20Chen%20and%20Fei%20Ma%20and%20Fei%20Richard%20Yu%20and%20Wenbo%20Ding&entry.1292438233=%20%20Tactile%20perception%20is%20essential%20for%20embodied%20agents%20to%20understand%20physical%0Aattributes%20of%20objects%20that%20cannot%20be%20determined%20through%20visual%20inspection%0Aalone.%20While%20existing%20approaches%20have%20made%20progress%20in%20visual%20and%20language%0Amodalities%20for%20physical%20understanding%2C%20they%20fail%20to%20effectively%20incorporate%0Atactile%20information%20that%20provides%20crucial%20haptic%20feedback%20for%20real-world%0Ainteraction.%20In%20this%20paper%2C%20we%20present%20VTV-LLM%2C%20the%20first%20multi-modal%20large%0Alanguage%20model%20for%20universal%20Visuo-Tactile%20Video%20%28VTV%29%20understanding%20that%0Abridges%20the%20gap%20between%20tactile%20perception%20and%20natural%20language.%20To%20address%20the%0Achallenges%20of%20cross-sensor%20and%20cross-modal%20integration%2C%20we%20contribute%20VTV150K%2C%0Aa%20comprehensive%20dataset%20comprising%20150%2C000%20video%20frames%20from%20100%20diverse%0Aobjects%20captured%20across%20three%20different%20tactile%20sensors%20%28GelSight%20Mini%2C%20DIGIT%2C%0Aand%20Tac3D%29%2C%20annotated%20with%20four%20fundamental%20tactile%20attributes%20%28hardness%2C%0Aprotrusion%2C%20elasticity%2C%20and%20friction%29.%20We%20develop%20a%20novel%20three-stage%20training%0Aparadigm%20that%20includes%20VTV%20enhancement%20for%20robust%20visuo-tactile%20representation%2C%0AVTV-text%20alignment%20for%20cross-modal%20correspondence%2C%20and%20text%20prompt%20finetuning%0Afor%20natural%20language%20generation.%20Our%20framework%20enables%20sophisticated%20tactile%0Areasoning%20capabilities%20including%20feature%20assessment%2C%20comparative%20analysis%2C%0Ascenario-based%20decision%20making%20and%20so%20on.%20Experimental%20evaluations%20demonstrate%0Athat%20VTV-LLM%20achieves%20superior%20performance%20in%20tactile%20video%20understanding%0Atasks%2C%20establishing%20a%20foundation%20for%20more%20intuitive%20human-machine%20interaction%0Ain%20tactile%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22566v1&entry.124074799=Read"},
{"title": "Learning to Infer Parameterized Representations of Plants from 3D Scans", "author": "Samara Ghrer and Christophe Godin and Stefanie Wuhrer", "abstract": "  Reconstructing faithfully the 3D architecture of plants from unstructured\nobservations is a challenging task. Plants frequently contain numerous organs,\norganized in branching systems in more or less complex spatial networks,\nleading to specific computational issues due to self-occlusion or spatial\nproximity between organs. Existing works either consider inverse modeling where\nthe aim is to recover the procedural rules that allow to simulate virtual\nplants, or focus on specific tasks such as segmentation or skeletonization. We\npropose a unified approach that, given a 3D scan of a plant, allows to infer a\nparameterized representation of the plant. This representation describes the\nplant's branching structure, contains parametric information for each plant\norgan, and can therefore be used directly in a variety of tasks. In this\ndata-driven approach, we train a recursive neural network with virtual plants\ngenerated using an L-systems-based procedural model. After training, the\nnetwork allows to infer a parametric tree-like representation based on an input\n3D point cloud. Our method is applicable to any plant that can be represented\nas binary axial tree. We evaluate our approach on Chenopodium Album plants,\nusing experiments on synthetic plants to show that our unified framework allows\nfor different tasks including reconstruction, segmentation and skeletonization,\nwhile achieving results on-par with state-of-the-art for each task.\n", "link": "http://arxiv.org/abs/2505.22337v1", "date": "2025-05-28", "relevancy": 2.9101, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5908}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5908}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Infer%20Parameterized%20Representations%20of%20Plants%20from%203D%20Scans&body=Title%3A%20Learning%20to%20Infer%20Parameterized%20Representations%20of%20Plants%20from%203D%20Scans%0AAuthor%3A%20Samara%20Ghrer%20and%20Christophe%20Godin%20and%20Stefanie%20Wuhrer%0AAbstract%3A%20%20%20Reconstructing%20faithfully%20the%203D%20architecture%20of%20plants%20from%20unstructured%0Aobservations%20is%20a%20challenging%20task.%20Plants%20frequently%20contain%20numerous%20organs%2C%0Aorganized%20in%20branching%20systems%20in%20more%20or%20less%20complex%20spatial%20networks%2C%0Aleading%20to%20specific%20computational%20issues%20due%20to%20self-occlusion%20or%20spatial%0Aproximity%20between%20organs.%20Existing%20works%20either%20consider%20inverse%20modeling%20where%0Athe%20aim%20is%20to%20recover%20the%20procedural%20rules%20that%20allow%20to%20simulate%20virtual%0Aplants%2C%20or%20focus%20on%20specific%20tasks%20such%20as%20segmentation%20or%20skeletonization.%20We%0Apropose%20a%20unified%20approach%20that%2C%20given%20a%203D%20scan%20of%20a%20plant%2C%20allows%20to%20infer%20a%0Aparameterized%20representation%20of%20the%20plant.%20This%20representation%20describes%20the%0Aplant%27s%20branching%20structure%2C%20contains%20parametric%20information%20for%20each%20plant%0Aorgan%2C%20and%20can%20therefore%20be%20used%20directly%20in%20a%20variety%20of%20tasks.%20In%20this%0Adata-driven%20approach%2C%20we%20train%20a%20recursive%20neural%20network%20with%20virtual%20plants%0Agenerated%20using%20an%20L-systems-based%20procedural%20model.%20After%20training%2C%20the%0Anetwork%20allows%20to%20infer%20a%20parametric%20tree-like%20representation%20based%20on%20an%20input%0A3D%20point%20cloud.%20Our%20method%20is%20applicable%20to%20any%20plant%20that%20can%20be%20represented%0Aas%20binary%20axial%20tree.%20We%20evaluate%20our%20approach%20on%20Chenopodium%20Album%20plants%2C%0Ausing%20experiments%20on%20synthetic%20plants%20to%20show%20that%20our%20unified%20framework%20allows%0Afor%20different%20tasks%20including%20reconstruction%2C%20segmentation%20and%20skeletonization%2C%0Awhile%20achieving%20results%20on-par%20with%20state-of-the-art%20for%20each%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22337v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Infer%2520Parameterized%2520Representations%2520of%2520Plants%2520from%25203D%2520Scans%26entry.906535625%3DSamara%2520Ghrer%2520and%2520Christophe%2520Godin%2520and%2520Stefanie%2520Wuhrer%26entry.1292438233%3D%2520%2520Reconstructing%2520faithfully%2520the%25203D%2520architecture%2520of%2520plants%2520from%2520unstructured%250Aobservations%2520is%2520a%2520challenging%2520task.%2520Plants%2520frequently%2520contain%2520numerous%2520organs%252C%250Aorganized%2520in%2520branching%2520systems%2520in%2520more%2520or%2520less%2520complex%2520spatial%2520networks%252C%250Aleading%2520to%2520specific%2520computational%2520issues%2520due%2520to%2520self-occlusion%2520or%2520spatial%250Aproximity%2520between%2520organs.%2520Existing%2520works%2520either%2520consider%2520inverse%2520modeling%2520where%250Athe%2520aim%2520is%2520to%2520recover%2520the%2520procedural%2520rules%2520that%2520allow%2520to%2520simulate%2520virtual%250Aplants%252C%2520or%2520focus%2520on%2520specific%2520tasks%2520such%2520as%2520segmentation%2520or%2520skeletonization.%2520We%250Apropose%2520a%2520unified%2520approach%2520that%252C%2520given%2520a%25203D%2520scan%2520of%2520a%2520plant%252C%2520allows%2520to%2520infer%2520a%250Aparameterized%2520representation%2520of%2520the%2520plant.%2520This%2520representation%2520describes%2520the%250Aplant%2527s%2520branching%2520structure%252C%2520contains%2520parametric%2520information%2520for%2520each%2520plant%250Aorgan%252C%2520and%2520can%2520therefore%2520be%2520used%2520directly%2520in%2520a%2520variety%2520of%2520tasks.%2520In%2520this%250Adata-driven%2520approach%252C%2520we%2520train%2520a%2520recursive%2520neural%2520network%2520with%2520virtual%2520plants%250Agenerated%2520using%2520an%2520L-systems-based%2520procedural%2520model.%2520After%2520training%252C%2520the%250Anetwork%2520allows%2520to%2520infer%2520a%2520parametric%2520tree-like%2520representation%2520based%2520on%2520an%2520input%250A3D%2520point%2520cloud.%2520Our%2520method%2520is%2520applicable%2520to%2520any%2520plant%2520that%2520can%2520be%2520represented%250Aas%2520binary%2520axial%2520tree.%2520We%2520evaluate%2520our%2520approach%2520on%2520Chenopodium%2520Album%2520plants%252C%250Ausing%2520experiments%2520on%2520synthetic%2520plants%2520to%2520show%2520that%2520our%2520unified%2520framework%2520allows%250Afor%2520different%2520tasks%2520including%2520reconstruction%252C%2520segmentation%2520and%2520skeletonization%252C%250Awhile%2520achieving%2520results%2520on-par%2520with%2520state-of-the-art%2520for%2520each%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22337v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Infer%20Parameterized%20Representations%20of%20Plants%20from%203D%20Scans&entry.906535625=Samara%20Ghrer%20and%20Christophe%20Godin%20and%20Stefanie%20Wuhrer&entry.1292438233=%20%20Reconstructing%20faithfully%20the%203D%20architecture%20of%20plants%20from%20unstructured%0Aobservations%20is%20a%20challenging%20task.%20Plants%20frequently%20contain%20numerous%20organs%2C%0Aorganized%20in%20branching%20systems%20in%20more%20or%20less%20complex%20spatial%20networks%2C%0Aleading%20to%20specific%20computational%20issues%20due%20to%20self-occlusion%20or%20spatial%0Aproximity%20between%20organs.%20Existing%20works%20either%20consider%20inverse%20modeling%20where%0Athe%20aim%20is%20to%20recover%20the%20procedural%20rules%20that%20allow%20to%20simulate%20virtual%0Aplants%2C%20or%20focus%20on%20specific%20tasks%20such%20as%20segmentation%20or%20skeletonization.%20We%0Apropose%20a%20unified%20approach%20that%2C%20given%20a%203D%20scan%20of%20a%20plant%2C%20allows%20to%20infer%20a%0Aparameterized%20representation%20of%20the%20plant.%20This%20representation%20describes%20the%0Aplant%27s%20branching%20structure%2C%20contains%20parametric%20information%20for%20each%20plant%0Aorgan%2C%20and%20can%20therefore%20be%20used%20directly%20in%20a%20variety%20of%20tasks.%20In%20this%0Adata-driven%20approach%2C%20we%20train%20a%20recursive%20neural%20network%20with%20virtual%20plants%0Agenerated%20using%20an%20L-systems-based%20procedural%20model.%20After%20training%2C%20the%0Anetwork%20allows%20to%20infer%20a%20parametric%20tree-like%20representation%20based%20on%20an%20input%0A3D%20point%20cloud.%20Our%20method%20is%20applicable%20to%20any%20plant%20that%20can%20be%20represented%0Aas%20binary%20axial%20tree.%20We%20evaluate%20our%20approach%20on%20Chenopodium%20Album%20plants%2C%0Ausing%20experiments%20on%20synthetic%20plants%20to%20show%20that%20our%20unified%20framework%20allows%0Afor%20different%20tasks%20including%20reconstruction%2C%20segmentation%20and%20skeletonization%2C%0Awhile%20achieving%20results%20on-par%20with%20state-of-the-art%20for%20each%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22337v1&entry.124074799=Read"},
{"title": "NFR: Neural Feature-Guided Non-Rigid Shape Registration", "author": "Puhua Jiang and Zhangquan Chen and Mingze Sun and Ruqi Huang", "abstract": "  In this paper, we propose a novel learning-based framework for 3D shape\nregistration, which overcomes the challenges of significant non-rigid\ndeformation and partiality undergoing among input shapes, and, remarkably,\nrequires no correspondence annotation during training. Our key insight is to\nincorporate neural features learned by deep learning-based shape matching\nnetworks into an iterative, geometric shape registration pipeline. The\nadvantage of our approach is two-fold -- On one hand, neural features provide\nmore accurate and semantically meaningful correspondence estimation than\nspatial features (e.g., coordinates), which is critical in the presence of\nlarge non-rigid deformations; On the other hand, the correspondences are\ndynamically updated according to the intermediate registrations and filtered by\nconsistency prior, which prominently robustify the overall pipeline. Empirical\nresults show that, with as few as dozens of training shapes of limited\nvariability, our pipeline achieves state-of-the-art results on several\nbenchmarks of non-rigid point cloud matching and partial shape matching across\nvarying settings, but also delivers high-quality correspondences between unseen\nchallenging shape pairs that undergo both significant extrinsic and intrinsic\ndeformations, in which case neither traditional registration methods nor\nintrinsic methods work.\n", "link": "http://arxiv.org/abs/2505.22445v1", "date": "2025-05-28", "relevancy": 2.8942, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6481}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5527}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5357}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NFR%3A%20Neural%20Feature-Guided%20Non-Rigid%20Shape%20Registration&body=Title%3A%20NFR%3A%20Neural%20Feature-Guided%20Non-Rigid%20Shape%20Registration%0AAuthor%3A%20Puhua%20Jiang%20and%20Zhangquan%20Chen%20and%20Mingze%20Sun%20and%20Ruqi%20Huang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20learning-based%20framework%20for%203D%20shape%0Aregistration%2C%20which%20overcomes%20the%20challenges%20of%20significant%20non-rigid%0Adeformation%20and%20partiality%20undergoing%20among%20input%20shapes%2C%20and%2C%20remarkably%2C%0Arequires%20no%20correspondence%20annotation%20during%20training.%20Our%20key%20insight%20is%20to%0Aincorporate%20neural%20features%20learned%20by%20deep%20learning-based%20shape%20matching%0Anetworks%20into%20an%20iterative%2C%20geometric%20shape%20registration%20pipeline.%20The%0Aadvantage%20of%20our%20approach%20is%20two-fold%20--%20On%20one%20hand%2C%20neural%20features%20provide%0Amore%20accurate%20and%20semantically%20meaningful%20correspondence%20estimation%20than%0Aspatial%20features%20%28e.g.%2C%20coordinates%29%2C%20which%20is%20critical%20in%20the%20presence%20of%0Alarge%20non-rigid%20deformations%3B%20On%20the%20other%20hand%2C%20the%20correspondences%20are%0Adynamically%20updated%20according%20to%20the%20intermediate%20registrations%20and%20filtered%20by%0Aconsistency%20prior%2C%20which%20prominently%20robustify%20the%20overall%20pipeline.%20Empirical%0Aresults%20show%20that%2C%20with%20as%20few%20as%20dozens%20of%20training%20shapes%20of%20limited%0Avariability%2C%20our%20pipeline%20achieves%20state-of-the-art%20results%20on%20several%0Abenchmarks%20of%20non-rigid%20point%20cloud%20matching%20and%20partial%20shape%20matching%20across%0Avarying%20settings%2C%20but%20also%20delivers%20high-quality%20correspondences%20between%20unseen%0Achallenging%20shape%20pairs%20that%20undergo%20both%20significant%20extrinsic%20and%20intrinsic%0Adeformations%2C%20in%20which%20case%20neither%20traditional%20registration%20methods%20nor%0Aintrinsic%20methods%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22445v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNFR%253A%2520Neural%2520Feature-Guided%2520Non-Rigid%2520Shape%2520Registration%26entry.906535625%3DPuhua%2520Jiang%2520and%2520Zhangquan%2520Chen%2520and%2520Mingze%2520Sun%2520and%2520Ruqi%2520Huang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520learning-based%2520framework%2520for%25203D%2520shape%250Aregistration%252C%2520which%2520overcomes%2520the%2520challenges%2520of%2520significant%2520non-rigid%250Adeformation%2520and%2520partiality%2520undergoing%2520among%2520input%2520shapes%252C%2520and%252C%2520remarkably%252C%250Arequires%2520no%2520correspondence%2520annotation%2520during%2520training.%2520Our%2520key%2520insight%2520is%2520to%250Aincorporate%2520neural%2520features%2520learned%2520by%2520deep%2520learning-based%2520shape%2520matching%250Anetworks%2520into%2520an%2520iterative%252C%2520geometric%2520shape%2520registration%2520pipeline.%2520The%250Aadvantage%2520of%2520our%2520approach%2520is%2520two-fold%2520--%2520On%2520one%2520hand%252C%2520neural%2520features%2520provide%250Amore%2520accurate%2520and%2520semantically%2520meaningful%2520correspondence%2520estimation%2520than%250Aspatial%2520features%2520%2528e.g.%252C%2520coordinates%2529%252C%2520which%2520is%2520critical%2520in%2520the%2520presence%2520of%250Alarge%2520non-rigid%2520deformations%253B%2520On%2520the%2520other%2520hand%252C%2520the%2520correspondences%2520are%250Adynamically%2520updated%2520according%2520to%2520the%2520intermediate%2520registrations%2520and%2520filtered%2520by%250Aconsistency%2520prior%252C%2520which%2520prominently%2520robustify%2520the%2520overall%2520pipeline.%2520Empirical%250Aresults%2520show%2520that%252C%2520with%2520as%2520few%2520as%2520dozens%2520of%2520training%2520shapes%2520of%2520limited%250Avariability%252C%2520our%2520pipeline%2520achieves%2520state-of-the-art%2520results%2520on%2520several%250Abenchmarks%2520of%2520non-rigid%2520point%2520cloud%2520matching%2520and%2520partial%2520shape%2520matching%2520across%250Avarying%2520settings%252C%2520but%2520also%2520delivers%2520high-quality%2520correspondences%2520between%2520unseen%250Achallenging%2520shape%2520pairs%2520that%2520undergo%2520both%2520significant%2520extrinsic%2520and%2520intrinsic%250Adeformations%252C%2520in%2520which%2520case%2520neither%2520traditional%2520registration%2520methods%2520nor%250Aintrinsic%2520methods%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22445v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NFR%3A%20Neural%20Feature-Guided%20Non-Rigid%20Shape%20Registration&entry.906535625=Puhua%20Jiang%20and%20Zhangquan%20Chen%20and%20Mingze%20Sun%20and%20Ruqi%20Huang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20learning-based%20framework%20for%203D%20shape%0Aregistration%2C%20which%20overcomes%20the%20challenges%20of%20significant%20non-rigid%0Adeformation%20and%20partiality%20undergoing%20among%20input%20shapes%2C%20and%2C%20remarkably%2C%0Arequires%20no%20correspondence%20annotation%20during%20training.%20Our%20key%20insight%20is%20to%0Aincorporate%20neural%20features%20learned%20by%20deep%20learning-based%20shape%20matching%0Anetworks%20into%20an%20iterative%2C%20geometric%20shape%20registration%20pipeline.%20The%0Aadvantage%20of%20our%20approach%20is%20two-fold%20--%20On%20one%20hand%2C%20neural%20features%20provide%0Amore%20accurate%20and%20semantically%20meaningful%20correspondence%20estimation%20than%0Aspatial%20features%20%28e.g.%2C%20coordinates%29%2C%20which%20is%20critical%20in%20the%20presence%20of%0Alarge%20non-rigid%20deformations%3B%20On%20the%20other%20hand%2C%20the%20correspondences%20are%0Adynamically%20updated%20according%20to%20the%20intermediate%20registrations%20and%20filtered%20by%0Aconsistency%20prior%2C%20which%20prominently%20robustify%20the%20overall%20pipeline.%20Empirical%0Aresults%20show%20that%2C%20with%20as%20few%20as%20dozens%20of%20training%20shapes%20of%20limited%0Avariability%2C%20our%20pipeline%20achieves%20state-of-the-art%20results%20on%20several%0Abenchmarks%20of%20non-rigid%20point%20cloud%20matching%20and%20partial%20shape%20matching%20across%0Avarying%20settings%2C%20but%20also%20delivers%20high-quality%20correspondences%20between%20unseen%0Achallenging%20shape%20pairs%20that%20undergo%20both%20significant%20extrinsic%20and%20intrinsic%0Adeformations%2C%20in%20which%20case%20neither%20traditional%20registration%20methods%20nor%0Aintrinsic%20methods%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22445v1&entry.124074799=Read"},
{"title": "Hunyuan-Game: Industrial-grade Intelligent Game Creation Model", "author": "Ruihuang Li and Caijin Zhou and Shoujian Zheng and Jianxiang Lu and Jiabin Huang and Comi Chen and Junshu Tang and Guangzheng Xu and Jiale Tao and Hongmei Wang and Donghao Li and Wenqing Yu and Senbo Wang and Zhimin Li and Yetshuan Shi and Haoyu Yang and Yukun Wang and Wenxun Dai and Jiaqi Li and Linqing Wang and Qixun Wang and Zhiyong Xu and Yingfang Zhang and Jiangfeng Xiong and Weijie Kong and Chao Zhang and Hongxin Zhang and Qiaoling Zheng and Weiting Guo and Xinchi Deng and Yixuan Li and Renjia Wei and Yulin Jian and Duojun Huang and Xuhua Ren and Junkun Yuan and Zhengguang Zhou and Jiaxiang Cheng and Bing Ma and Shirui Huang and Jiawang Bai and Chao Li and Sihuan Lin and Yifu Sun and Yuan Zhou and Joey Wang and Qin Lin and Tianxiang Zheng and Jingmiao Yu and Jihong Zhang and Caesar Zhong and Di Wang and Yuhong Liu and  Linus and Jie Jiang and Longhuang Wu and Shuai Shao and Qinglin Lu", "abstract": "  Intelligent game creation represents a transformative advancement in game\ndevelopment, utilizing generative artificial intelligence to dynamically\ngenerate and enhance game content. Despite notable progress in generative\nmodels, the comprehensive synthesis of high-quality game assets, including both\nimages and videos, remains a challenging frontier. To create high-fidelity game\ncontent that simultaneously aligns with player preferences and significantly\nboosts designer efficiency, we present Hunyuan-Game, an innovative project\ndesigned to revolutionize intelligent game production. Hunyuan-Game encompasses\ntwo primary branches: image generation and video generation. The image\ngeneration component is built upon a vast dataset comprising billions of game\nimages, leading to the development of a group of customized image generation\nmodels tailored for game scenarios: (1) General Text-to-Image Generation. (2)\nGame Visual Effects Generation, involving text-to-effect and reference\nimage-based game visual effect generation. (3) Transparent Image Generation for\ncharacters, scenes, and game visual effects. (4) Game Character Generation\nbased on sketches, black-and-white images, and white models. The video\ngeneration component is built upon a comprehensive dataset of millions of game\nand anime videos, leading to the development of five core algorithmic models,\neach targeting critical pain points in game development and having robust\nadaptation to diverse game video scenarios: (1) Image-to-Video Generation. (2)\n360 A/T Pose Avatar Video Synthesis. (3) Dynamic Illustration Generation. (4)\nGenerative Video Super-Resolution. (5) Interactive Game Video Generation. These\nimage and video generation models not only exhibit high-level aesthetic\nexpression but also deeply integrate domain-specific knowledge, establishing a\nsystematic understanding of diverse game and anime art styles.\n", "link": "http://arxiv.org/abs/2505.14135v2", "date": "2025-05-28", "relevancy": 2.8863, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5885}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.58}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hunyuan-Game%3A%20Industrial-grade%20Intelligent%20Game%20Creation%20Model&body=Title%3A%20Hunyuan-Game%3A%20Industrial-grade%20Intelligent%20Game%20Creation%20Model%0AAuthor%3A%20Ruihuang%20Li%20and%20Caijin%20Zhou%20and%20Shoujian%20Zheng%20and%20Jianxiang%20Lu%20and%20Jiabin%20Huang%20and%20Comi%20Chen%20and%20Junshu%20Tang%20and%20Guangzheng%20Xu%20and%20Jiale%20Tao%20and%20Hongmei%20Wang%20and%20Donghao%20Li%20and%20Wenqing%20Yu%20and%20Senbo%20Wang%20and%20Zhimin%20Li%20and%20Yetshuan%20Shi%20and%20Haoyu%20Yang%20and%20Yukun%20Wang%20and%20Wenxun%20Dai%20and%20Jiaqi%20Li%20and%20Linqing%20Wang%20and%20Qixun%20Wang%20and%20Zhiyong%20Xu%20and%20Yingfang%20Zhang%20and%20Jiangfeng%20Xiong%20and%20Weijie%20Kong%20and%20Chao%20Zhang%20and%20Hongxin%20Zhang%20and%20Qiaoling%20Zheng%20and%20Weiting%20Guo%20and%20Xinchi%20Deng%20and%20Yixuan%20Li%20and%20Renjia%20Wei%20and%20Yulin%20Jian%20and%20Duojun%20Huang%20and%20Xuhua%20Ren%20and%20Junkun%20Yuan%20and%20Zhengguang%20Zhou%20and%20Jiaxiang%20Cheng%20and%20Bing%20Ma%20and%20Shirui%20Huang%20and%20Jiawang%20Bai%20and%20Chao%20Li%20and%20Sihuan%20Lin%20and%20Yifu%20Sun%20and%20Yuan%20Zhou%20and%20Joey%20Wang%20and%20Qin%20Lin%20and%20Tianxiang%20Zheng%20and%20Jingmiao%20Yu%20and%20Jihong%20Zhang%20and%20Caesar%20Zhong%20and%20Di%20Wang%20and%20Yuhong%20Liu%20and%20%20Linus%20and%20Jie%20Jiang%20and%20Longhuang%20Wu%20and%20Shuai%20Shao%20and%20Qinglin%20Lu%0AAbstract%3A%20%20%20Intelligent%20game%20creation%20represents%20a%20transformative%20advancement%20in%20game%0Adevelopment%2C%20utilizing%20generative%20artificial%20intelligence%20to%20dynamically%0Agenerate%20and%20enhance%20game%20content.%20Despite%20notable%20progress%20in%20generative%0Amodels%2C%20the%20comprehensive%20synthesis%20of%20high-quality%20game%20assets%2C%20including%20both%0Aimages%20and%20videos%2C%20remains%20a%20challenging%20frontier.%20To%20create%20high-fidelity%20game%0Acontent%20that%20simultaneously%20aligns%20with%20player%20preferences%20and%20significantly%0Aboosts%20designer%20efficiency%2C%20we%20present%20Hunyuan-Game%2C%20an%20innovative%20project%0Adesigned%20to%20revolutionize%20intelligent%20game%20production.%20Hunyuan-Game%20encompasses%0Atwo%20primary%20branches%3A%20image%20generation%20and%20video%20generation.%20The%20image%0Ageneration%20component%20is%20built%20upon%20a%20vast%20dataset%20comprising%20billions%20of%20game%0Aimages%2C%20leading%20to%20the%20development%20of%20a%20group%20of%20customized%20image%20generation%0Amodels%20tailored%20for%20game%20scenarios%3A%20%281%29%20General%20Text-to-Image%20Generation.%20%282%29%0AGame%20Visual%20Effects%20Generation%2C%20involving%20text-to-effect%20and%20reference%0Aimage-based%20game%20visual%20effect%20generation.%20%283%29%20Transparent%20Image%20Generation%20for%0Acharacters%2C%20scenes%2C%20and%20game%20visual%20effects.%20%284%29%20Game%20Character%20Generation%0Abased%20on%20sketches%2C%20black-and-white%20images%2C%20and%20white%20models.%20The%20video%0Ageneration%20component%20is%20built%20upon%20a%20comprehensive%20dataset%20of%20millions%20of%20game%0Aand%20anime%20videos%2C%20leading%20to%20the%20development%20of%20five%20core%20algorithmic%20models%2C%0Aeach%20targeting%20critical%20pain%20points%20in%20game%20development%20and%20having%20robust%0Aadaptation%20to%20diverse%20game%20video%20scenarios%3A%20%281%29%20Image-to-Video%20Generation.%20%282%29%0A360%20A/T%20Pose%20Avatar%20Video%20Synthesis.%20%283%29%20Dynamic%20Illustration%20Generation.%20%284%29%0AGenerative%20Video%20Super-Resolution.%20%285%29%20Interactive%20Game%20Video%20Generation.%20These%0Aimage%20and%20video%20generation%20models%20not%20only%20exhibit%20high-level%20aesthetic%0Aexpression%20but%20also%20deeply%20integrate%20domain-specific%20knowledge%2C%20establishing%20a%0Asystematic%20understanding%20of%20diverse%20game%20and%20anime%20art%20styles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14135v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHunyuan-Game%253A%2520Industrial-grade%2520Intelligent%2520Game%2520Creation%2520Model%26entry.906535625%3DRuihuang%2520Li%2520and%2520Caijin%2520Zhou%2520and%2520Shoujian%2520Zheng%2520and%2520Jianxiang%2520Lu%2520and%2520Jiabin%2520Huang%2520and%2520Comi%2520Chen%2520and%2520Junshu%2520Tang%2520and%2520Guangzheng%2520Xu%2520and%2520Jiale%2520Tao%2520and%2520Hongmei%2520Wang%2520and%2520Donghao%2520Li%2520and%2520Wenqing%2520Yu%2520and%2520Senbo%2520Wang%2520and%2520Zhimin%2520Li%2520and%2520Yetshuan%2520Shi%2520and%2520Haoyu%2520Yang%2520and%2520Yukun%2520Wang%2520and%2520Wenxun%2520Dai%2520and%2520Jiaqi%2520Li%2520and%2520Linqing%2520Wang%2520and%2520Qixun%2520Wang%2520and%2520Zhiyong%2520Xu%2520and%2520Yingfang%2520Zhang%2520and%2520Jiangfeng%2520Xiong%2520and%2520Weijie%2520Kong%2520and%2520Chao%2520Zhang%2520and%2520Hongxin%2520Zhang%2520and%2520Qiaoling%2520Zheng%2520and%2520Weiting%2520Guo%2520and%2520Xinchi%2520Deng%2520and%2520Yixuan%2520Li%2520and%2520Renjia%2520Wei%2520and%2520Yulin%2520Jian%2520and%2520Duojun%2520Huang%2520and%2520Xuhua%2520Ren%2520and%2520Junkun%2520Yuan%2520and%2520Zhengguang%2520Zhou%2520and%2520Jiaxiang%2520Cheng%2520and%2520Bing%2520Ma%2520and%2520Shirui%2520Huang%2520and%2520Jiawang%2520Bai%2520and%2520Chao%2520Li%2520and%2520Sihuan%2520Lin%2520and%2520Yifu%2520Sun%2520and%2520Yuan%2520Zhou%2520and%2520Joey%2520Wang%2520and%2520Qin%2520Lin%2520and%2520Tianxiang%2520Zheng%2520and%2520Jingmiao%2520Yu%2520and%2520Jihong%2520Zhang%2520and%2520Caesar%2520Zhong%2520and%2520Di%2520Wang%2520and%2520Yuhong%2520Liu%2520and%2520%2520Linus%2520and%2520Jie%2520Jiang%2520and%2520Longhuang%2520Wu%2520and%2520Shuai%2520Shao%2520and%2520Qinglin%2520Lu%26entry.1292438233%3D%2520%2520Intelligent%2520game%2520creation%2520represents%2520a%2520transformative%2520advancement%2520in%2520game%250Adevelopment%252C%2520utilizing%2520generative%2520artificial%2520intelligence%2520to%2520dynamically%250Agenerate%2520and%2520enhance%2520game%2520content.%2520Despite%2520notable%2520progress%2520in%2520generative%250Amodels%252C%2520the%2520comprehensive%2520synthesis%2520of%2520high-quality%2520game%2520assets%252C%2520including%2520both%250Aimages%2520and%2520videos%252C%2520remains%2520a%2520challenging%2520frontier.%2520To%2520create%2520high-fidelity%2520game%250Acontent%2520that%2520simultaneously%2520aligns%2520with%2520player%2520preferences%2520and%2520significantly%250Aboosts%2520designer%2520efficiency%252C%2520we%2520present%2520Hunyuan-Game%252C%2520an%2520innovative%2520project%250Adesigned%2520to%2520revolutionize%2520intelligent%2520game%2520production.%2520Hunyuan-Game%2520encompasses%250Atwo%2520primary%2520branches%253A%2520image%2520generation%2520and%2520video%2520generation.%2520The%2520image%250Ageneration%2520component%2520is%2520built%2520upon%2520a%2520vast%2520dataset%2520comprising%2520billions%2520of%2520game%250Aimages%252C%2520leading%2520to%2520the%2520development%2520of%2520a%2520group%2520of%2520customized%2520image%2520generation%250Amodels%2520tailored%2520for%2520game%2520scenarios%253A%2520%25281%2529%2520General%2520Text-to-Image%2520Generation.%2520%25282%2529%250AGame%2520Visual%2520Effects%2520Generation%252C%2520involving%2520text-to-effect%2520and%2520reference%250Aimage-based%2520game%2520visual%2520effect%2520generation.%2520%25283%2529%2520Transparent%2520Image%2520Generation%2520for%250Acharacters%252C%2520scenes%252C%2520and%2520game%2520visual%2520effects.%2520%25284%2529%2520Game%2520Character%2520Generation%250Abased%2520on%2520sketches%252C%2520black-and-white%2520images%252C%2520and%2520white%2520models.%2520The%2520video%250Ageneration%2520component%2520is%2520built%2520upon%2520a%2520comprehensive%2520dataset%2520of%2520millions%2520of%2520game%250Aand%2520anime%2520videos%252C%2520leading%2520to%2520the%2520development%2520of%2520five%2520core%2520algorithmic%2520models%252C%250Aeach%2520targeting%2520critical%2520pain%2520points%2520in%2520game%2520development%2520and%2520having%2520robust%250Aadaptation%2520to%2520diverse%2520game%2520video%2520scenarios%253A%2520%25281%2529%2520Image-to-Video%2520Generation.%2520%25282%2529%250A360%2520A/T%2520Pose%2520Avatar%2520Video%2520Synthesis.%2520%25283%2529%2520Dynamic%2520Illustration%2520Generation.%2520%25284%2529%250AGenerative%2520Video%2520Super-Resolution.%2520%25285%2529%2520Interactive%2520Game%2520Video%2520Generation.%2520These%250Aimage%2520and%2520video%2520generation%2520models%2520not%2520only%2520exhibit%2520high-level%2520aesthetic%250Aexpression%2520but%2520also%2520deeply%2520integrate%2520domain-specific%2520knowledge%252C%2520establishing%2520a%250Asystematic%2520understanding%2520of%2520diverse%2520game%2520and%2520anime%2520art%2520styles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14135v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hunyuan-Game%3A%20Industrial-grade%20Intelligent%20Game%20Creation%20Model&entry.906535625=Ruihuang%20Li%20and%20Caijin%20Zhou%20and%20Shoujian%20Zheng%20and%20Jianxiang%20Lu%20and%20Jiabin%20Huang%20and%20Comi%20Chen%20and%20Junshu%20Tang%20and%20Guangzheng%20Xu%20and%20Jiale%20Tao%20and%20Hongmei%20Wang%20and%20Donghao%20Li%20and%20Wenqing%20Yu%20and%20Senbo%20Wang%20and%20Zhimin%20Li%20and%20Yetshuan%20Shi%20and%20Haoyu%20Yang%20and%20Yukun%20Wang%20and%20Wenxun%20Dai%20and%20Jiaqi%20Li%20and%20Linqing%20Wang%20and%20Qixun%20Wang%20and%20Zhiyong%20Xu%20and%20Yingfang%20Zhang%20and%20Jiangfeng%20Xiong%20and%20Weijie%20Kong%20and%20Chao%20Zhang%20and%20Hongxin%20Zhang%20and%20Qiaoling%20Zheng%20and%20Weiting%20Guo%20and%20Xinchi%20Deng%20and%20Yixuan%20Li%20and%20Renjia%20Wei%20and%20Yulin%20Jian%20and%20Duojun%20Huang%20and%20Xuhua%20Ren%20and%20Junkun%20Yuan%20and%20Zhengguang%20Zhou%20and%20Jiaxiang%20Cheng%20and%20Bing%20Ma%20and%20Shirui%20Huang%20and%20Jiawang%20Bai%20and%20Chao%20Li%20and%20Sihuan%20Lin%20and%20Yifu%20Sun%20and%20Yuan%20Zhou%20and%20Joey%20Wang%20and%20Qin%20Lin%20and%20Tianxiang%20Zheng%20and%20Jingmiao%20Yu%20and%20Jihong%20Zhang%20and%20Caesar%20Zhong%20and%20Di%20Wang%20and%20Yuhong%20Liu%20and%20%20Linus%20and%20Jie%20Jiang%20and%20Longhuang%20Wu%20and%20Shuai%20Shao%20and%20Qinglin%20Lu&entry.1292438233=%20%20Intelligent%20game%20creation%20represents%20a%20transformative%20advancement%20in%20game%0Adevelopment%2C%20utilizing%20generative%20artificial%20intelligence%20to%20dynamically%0Agenerate%20and%20enhance%20game%20content.%20Despite%20notable%20progress%20in%20generative%0Amodels%2C%20the%20comprehensive%20synthesis%20of%20high-quality%20game%20assets%2C%20including%20both%0Aimages%20and%20videos%2C%20remains%20a%20challenging%20frontier.%20To%20create%20high-fidelity%20game%0Acontent%20that%20simultaneously%20aligns%20with%20player%20preferences%20and%20significantly%0Aboosts%20designer%20efficiency%2C%20we%20present%20Hunyuan-Game%2C%20an%20innovative%20project%0Adesigned%20to%20revolutionize%20intelligent%20game%20production.%20Hunyuan-Game%20encompasses%0Atwo%20primary%20branches%3A%20image%20generation%20and%20video%20generation.%20The%20image%0Ageneration%20component%20is%20built%20upon%20a%20vast%20dataset%20comprising%20billions%20of%20game%0Aimages%2C%20leading%20to%20the%20development%20of%20a%20group%20of%20customized%20image%20generation%0Amodels%20tailored%20for%20game%20scenarios%3A%20%281%29%20General%20Text-to-Image%20Generation.%20%282%29%0AGame%20Visual%20Effects%20Generation%2C%20involving%20text-to-effect%20and%20reference%0Aimage-based%20game%20visual%20effect%20generation.%20%283%29%20Transparent%20Image%20Generation%20for%0Acharacters%2C%20scenes%2C%20and%20game%20visual%20effects.%20%284%29%20Game%20Character%20Generation%0Abased%20on%20sketches%2C%20black-and-white%20images%2C%20and%20white%20models.%20The%20video%0Ageneration%20component%20is%20built%20upon%20a%20comprehensive%20dataset%20of%20millions%20of%20game%0Aand%20anime%20videos%2C%20leading%20to%20the%20development%20of%20five%20core%20algorithmic%20models%2C%0Aeach%20targeting%20critical%20pain%20points%20in%20game%20development%20and%20having%20robust%0Aadaptation%20to%20diverse%20game%20video%20scenarios%3A%20%281%29%20Image-to-Video%20Generation.%20%282%29%0A360%20A/T%20Pose%20Avatar%20Video%20Synthesis.%20%283%29%20Dynamic%20Illustration%20Generation.%20%284%29%0AGenerative%20Video%20Super-Resolution.%20%285%29%20Interactive%20Game%20Video%20Generation.%20These%0Aimage%20and%20video%20generation%20models%20not%20only%20exhibit%20high-level%20aesthetic%0Aexpression%20but%20also%20deeply%20integrate%20domain-specific%20knowledge%2C%20establishing%20a%0Asystematic%20understanding%20of%20diverse%20game%20and%20anime%20art%20styles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14135v2&entry.124074799=Read"},
{"title": "SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation", "author": "Dekai Zhu and Yixuan Hu and Youquan Liu and Dongyue Lu and Lingdong Kong and Slobodan Ilic", "abstract": "  Leveraging recent diffusion models, LiDAR-based large-scale 3D scene\ngeneration has achieved great success. While recent voxel-based approaches can\ngenerate both geometric structures and semantic labels, existing range-view\nmethods are limited to producing unlabeled LiDAR scenes. Relying on pretrained\nsegmentation models to predict the semantic maps often results in suboptimal\ncross-modal consistency. To address this limitation while preserving the\nadvantages of range-view representations, such as computational efficiency and\nsimplified network design, we propose Spiral, a novel range-view LiDAR\ndiffusion model that simultaneously generates depth, reflectance images, and\nsemantic maps. Furthermore, we introduce novel semantic-aware metrics to\nevaluate the quality of the generated labeled range-view data. Experiments on\nthe SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves\nstate-of-the-art performance with the smallest parameter size, outperforming\ntwo-step methods that combine the generative and segmentation models.\nAdditionally, we validate that range images generated by Spiral can be\neffectively used for synthetic data augmentation in the downstream segmentation\ntraining, significantly reducing the labeling effort on LiDAR data.\n", "link": "http://arxiv.org/abs/2505.22643v1", "date": "2025-05-28", "relevancy": 2.8811, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6071}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5608}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPIRAL%3A%20Semantic-Aware%20Progressive%20LiDAR%20Scene%20Generation&body=Title%3A%20SPIRAL%3A%20Semantic-Aware%20Progressive%20LiDAR%20Scene%20Generation%0AAuthor%3A%20Dekai%20Zhu%20and%20Yixuan%20Hu%20and%20Youquan%20Liu%20and%20Dongyue%20Lu%20and%20Lingdong%20Kong%20and%20Slobodan%20Ilic%0AAbstract%3A%20%20%20Leveraging%20recent%20diffusion%20models%2C%20LiDAR-based%20large-scale%203D%20scene%0Ageneration%20has%20achieved%20great%20success.%20While%20recent%20voxel-based%20approaches%20can%0Agenerate%20both%20geometric%20structures%20and%20semantic%20labels%2C%20existing%20range-view%0Amethods%20are%20limited%20to%20producing%20unlabeled%20LiDAR%20scenes.%20Relying%20on%20pretrained%0Asegmentation%20models%20to%20predict%20the%20semantic%20maps%20often%20results%20in%20suboptimal%0Across-modal%20consistency.%20To%20address%20this%20limitation%20while%20preserving%20the%0Aadvantages%20of%20range-view%20representations%2C%20such%20as%20computational%20efficiency%20and%0Asimplified%20network%20design%2C%20we%20propose%20Spiral%2C%20a%20novel%20range-view%20LiDAR%0Adiffusion%20model%20that%20simultaneously%20generates%20depth%2C%20reflectance%20images%2C%20and%0Asemantic%20maps.%20Furthermore%2C%20we%20introduce%20novel%20semantic-aware%20metrics%20to%0Aevaluate%20the%20quality%20of%20the%20generated%20labeled%20range-view%20data.%20Experiments%20on%0Athe%20SemanticKITTI%20and%20nuScenes%20datasets%20demonstrate%20that%20Spiral%20achieves%0Astate-of-the-art%20performance%20with%20the%20smallest%20parameter%20size%2C%20outperforming%0Atwo-step%20methods%20that%20combine%20the%20generative%20and%20segmentation%20models.%0AAdditionally%2C%20we%20validate%20that%20range%20images%20generated%20by%20Spiral%20can%20be%0Aeffectively%20used%20for%20synthetic%20data%20augmentation%20in%20the%20downstream%20segmentation%0Atraining%2C%20significantly%20reducing%20the%20labeling%20effort%20on%20LiDAR%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPIRAL%253A%2520Semantic-Aware%2520Progressive%2520LiDAR%2520Scene%2520Generation%26entry.906535625%3DDekai%2520Zhu%2520and%2520Yixuan%2520Hu%2520and%2520Youquan%2520Liu%2520and%2520Dongyue%2520Lu%2520and%2520Lingdong%2520Kong%2520and%2520Slobodan%2520Ilic%26entry.1292438233%3D%2520%2520Leveraging%2520recent%2520diffusion%2520models%252C%2520LiDAR-based%2520large-scale%25203D%2520scene%250Ageneration%2520has%2520achieved%2520great%2520success.%2520While%2520recent%2520voxel-based%2520approaches%2520can%250Agenerate%2520both%2520geometric%2520structures%2520and%2520semantic%2520labels%252C%2520existing%2520range-view%250Amethods%2520are%2520limited%2520to%2520producing%2520unlabeled%2520LiDAR%2520scenes.%2520Relying%2520on%2520pretrained%250Asegmentation%2520models%2520to%2520predict%2520the%2520semantic%2520maps%2520often%2520results%2520in%2520suboptimal%250Across-modal%2520consistency.%2520To%2520address%2520this%2520limitation%2520while%2520preserving%2520the%250Aadvantages%2520of%2520range-view%2520representations%252C%2520such%2520as%2520computational%2520efficiency%2520and%250Asimplified%2520network%2520design%252C%2520we%2520propose%2520Spiral%252C%2520a%2520novel%2520range-view%2520LiDAR%250Adiffusion%2520model%2520that%2520simultaneously%2520generates%2520depth%252C%2520reflectance%2520images%252C%2520and%250Asemantic%2520maps.%2520Furthermore%252C%2520we%2520introduce%2520novel%2520semantic-aware%2520metrics%2520to%250Aevaluate%2520the%2520quality%2520of%2520the%2520generated%2520labeled%2520range-view%2520data.%2520Experiments%2520on%250Athe%2520SemanticKITTI%2520and%2520nuScenes%2520datasets%2520demonstrate%2520that%2520Spiral%2520achieves%250Astate-of-the-art%2520performance%2520with%2520the%2520smallest%2520parameter%2520size%252C%2520outperforming%250Atwo-step%2520methods%2520that%2520combine%2520the%2520generative%2520and%2520segmentation%2520models.%250AAdditionally%252C%2520we%2520validate%2520that%2520range%2520images%2520generated%2520by%2520Spiral%2520can%2520be%250Aeffectively%2520used%2520for%2520synthetic%2520data%2520augmentation%2520in%2520the%2520downstream%2520segmentation%250Atraining%252C%2520significantly%2520reducing%2520the%2520labeling%2520effort%2520on%2520LiDAR%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPIRAL%3A%20Semantic-Aware%20Progressive%20LiDAR%20Scene%20Generation&entry.906535625=Dekai%20Zhu%20and%20Yixuan%20Hu%20and%20Youquan%20Liu%20and%20Dongyue%20Lu%20and%20Lingdong%20Kong%20and%20Slobodan%20Ilic&entry.1292438233=%20%20Leveraging%20recent%20diffusion%20models%2C%20LiDAR-based%20large-scale%203D%20scene%0Ageneration%20has%20achieved%20great%20success.%20While%20recent%20voxel-based%20approaches%20can%0Agenerate%20both%20geometric%20structures%20and%20semantic%20labels%2C%20existing%20range-view%0Amethods%20are%20limited%20to%20producing%20unlabeled%20LiDAR%20scenes.%20Relying%20on%20pretrained%0Asegmentation%20models%20to%20predict%20the%20semantic%20maps%20often%20results%20in%20suboptimal%0Across-modal%20consistency.%20To%20address%20this%20limitation%20while%20preserving%20the%0Aadvantages%20of%20range-view%20representations%2C%20such%20as%20computational%20efficiency%20and%0Asimplified%20network%20design%2C%20we%20propose%20Spiral%2C%20a%20novel%20range-view%20LiDAR%0Adiffusion%20model%20that%20simultaneously%20generates%20depth%2C%20reflectance%20images%2C%20and%0Asemantic%20maps.%20Furthermore%2C%20we%20introduce%20novel%20semantic-aware%20metrics%20to%0Aevaluate%20the%20quality%20of%20the%20generated%20labeled%20range-view%20data.%20Experiments%20on%0Athe%20SemanticKITTI%20and%20nuScenes%20datasets%20demonstrate%20that%20Spiral%20achieves%0Astate-of-the-art%20performance%20with%20the%20smallest%20parameter%20size%2C%20outperforming%0Atwo-step%20methods%20that%20combine%20the%20generative%20and%20segmentation%20models.%0AAdditionally%2C%20we%20validate%20that%20range%20images%20generated%20by%20Spiral%20can%20be%0Aeffectively%20used%20for%20synthetic%20data%20augmentation%20in%20the%20downstream%20segmentation%0Atraining%2C%20significantly%20reducing%20the%20labeling%20effort%20on%20LiDAR%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22643v1&entry.124074799=Read"},
{"title": "Neural Face Skinning for Mesh-agnostic Facial Expression Cloning", "author": "Sihun Cha and Serin Yoon and Kwanggyoon Seo and Junyong Noh", "abstract": "  Accurately retargeting facial expressions to a face mesh while enabling\nmanipulation is a key challenge in facial animation retargeting. Recent\ndeep-learning methods address this by encoding facial expressions into a global\nlatent code, but they often fail to capture fine-grained details in local\nregions. While some methods improve local accuracy by transferring deformations\nlocally, this often complicates overall control of the facial expression. To\naddress this, we propose a method that combines the strengths of both global\nand local deformation models. Our approach enables intuitive control and\ndetailed expression cloning across diverse face meshes, regardless of their\nunderlying structures. The core idea is to localize the influence of the global\nlatent code on the target mesh. Our model learns to predict skinning weights\nfor each vertex of the target face mesh through indirect supervision from\npredefined segmentation labels. These predicted weights localize the global\nlatent code, enabling precise and region-specific deformations even for meshes\nwith unseen shapes. We supervise the latent code using Facial Action Coding\nSystem (FACS)-based blendshapes to ensure interpretability and allow\nstraightforward editing of the generated animation. Through extensive\nexperiments, we demonstrate improved performance over state-of-the-art methods\nin terms of expression fidelity, deformation transfer accuracy, and\nadaptability across diverse mesh structures.\n", "link": "http://arxiv.org/abs/2505.22416v1", "date": "2025-05-28", "relevancy": 2.874, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5755}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5755}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Face%20Skinning%20for%20Mesh-agnostic%20Facial%20Expression%20Cloning&body=Title%3A%20Neural%20Face%20Skinning%20for%20Mesh-agnostic%20Facial%20Expression%20Cloning%0AAuthor%3A%20Sihun%20Cha%20and%20Serin%20Yoon%20and%20Kwanggyoon%20Seo%20and%20Junyong%20Noh%0AAbstract%3A%20%20%20Accurately%20retargeting%20facial%20expressions%20to%20a%20face%20mesh%20while%20enabling%0Amanipulation%20is%20a%20key%20challenge%20in%20facial%20animation%20retargeting.%20Recent%0Adeep-learning%20methods%20address%20this%20by%20encoding%20facial%20expressions%20into%20a%20global%0Alatent%20code%2C%20but%20they%20often%20fail%20to%20capture%20fine-grained%20details%20in%20local%0Aregions.%20While%20some%20methods%20improve%20local%20accuracy%20by%20transferring%20deformations%0Alocally%2C%20this%20often%20complicates%20overall%20control%20of%20the%20facial%20expression.%20To%0Aaddress%20this%2C%20we%20propose%20a%20method%20that%20combines%20the%20strengths%20of%20both%20global%0Aand%20local%20deformation%20models.%20Our%20approach%20enables%20intuitive%20control%20and%0Adetailed%20expression%20cloning%20across%20diverse%20face%20meshes%2C%20regardless%20of%20their%0Aunderlying%20structures.%20The%20core%20idea%20is%20to%20localize%20the%20influence%20of%20the%20global%0Alatent%20code%20on%20the%20target%20mesh.%20Our%20model%20learns%20to%20predict%20skinning%20weights%0Afor%20each%20vertex%20of%20the%20target%20face%20mesh%20through%20indirect%20supervision%20from%0Apredefined%20segmentation%20labels.%20These%20predicted%20weights%20localize%20the%20global%0Alatent%20code%2C%20enabling%20precise%20and%20region-specific%20deformations%20even%20for%20meshes%0Awith%20unseen%20shapes.%20We%20supervise%20the%20latent%20code%20using%20Facial%20Action%20Coding%0ASystem%20%28FACS%29-based%20blendshapes%20to%20ensure%20interpretability%20and%20allow%0Astraightforward%20editing%20of%20the%20generated%20animation.%20Through%20extensive%0Aexperiments%2C%20we%20demonstrate%20improved%20performance%20over%20state-of-the-art%20methods%0Ain%20terms%20of%20expression%20fidelity%2C%20deformation%20transfer%20accuracy%2C%20and%0Aadaptability%20across%20diverse%20mesh%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22416v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Face%2520Skinning%2520for%2520Mesh-agnostic%2520Facial%2520Expression%2520Cloning%26entry.906535625%3DSihun%2520Cha%2520and%2520Serin%2520Yoon%2520and%2520Kwanggyoon%2520Seo%2520and%2520Junyong%2520Noh%26entry.1292438233%3D%2520%2520Accurately%2520retargeting%2520facial%2520expressions%2520to%2520a%2520face%2520mesh%2520while%2520enabling%250Amanipulation%2520is%2520a%2520key%2520challenge%2520in%2520facial%2520animation%2520retargeting.%2520Recent%250Adeep-learning%2520methods%2520address%2520this%2520by%2520encoding%2520facial%2520expressions%2520into%2520a%2520global%250Alatent%2520code%252C%2520but%2520they%2520often%2520fail%2520to%2520capture%2520fine-grained%2520details%2520in%2520local%250Aregions.%2520While%2520some%2520methods%2520improve%2520local%2520accuracy%2520by%2520transferring%2520deformations%250Alocally%252C%2520this%2520often%2520complicates%2520overall%2520control%2520of%2520the%2520facial%2520expression.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520a%2520method%2520that%2520combines%2520the%2520strengths%2520of%2520both%2520global%250Aand%2520local%2520deformation%2520models.%2520Our%2520approach%2520enables%2520intuitive%2520control%2520and%250Adetailed%2520expression%2520cloning%2520across%2520diverse%2520face%2520meshes%252C%2520regardless%2520of%2520their%250Aunderlying%2520structures.%2520The%2520core%2520idea%2520is%2520to%2520localize%2520the%2520influence%2520of%2520the%2520global%250Alatent%2520code%2520on%2520the%2520target%2520mesh.%2520Our%2520model%2520learns%2520to%2520predict%2520skinning%2520weights%250Afor%2520each%2520vertex%2520of%2520the%2520target%2520face%2520mesh%2520through%2520indirect%2520supervision%2520from%250Apredefined%2520segmentation%2520labels.%2520These%2520predicted%2520weights%2520localize%2520the%2520global%250Alatent%2520code%252C%2520enabling%2520precise%2520and%2520region-specific%2520deformations%2520even%2520for%2520meshes%250Awith%2520unseen%2520shapes.%2520We%2520supervise%2520the%2520latent%2520code%2520using%2520Facial%2520Action%2520Coding%250ASystem%2520%2528FACS%2529-based%2520blendshapes%2520to%2520ensure%2520interpretability%2520and%2520allow%250Astraightforward%2520editing%2520of%2520the%2520generated%2520animation.%2520Through%2520extensive%250Aexperiments%252C%2520we%2520demonstrate%2520improved%2520performance%2520over%2520state-of-the-art%2520methods%250Ain%2520terms%2520of%2520expression%2520fidelity%252C%2520deformation%2520transfer%2520accuracy%252C%2520and%250Aadaptability%2520across%2520diverse%2520mesh%2520structures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22416v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Face%20Skinning%20for%20Mesh-agnostic%20Facial%20Expression%20Cloning&entry.906535625=Sihun%20Cha%20and%20Serin%20Yoon%20and%20Kwanggyoon%20Seo%20and%20Junyong%20Noh&entry.1292438233=%20%20Accurately%20retargeting%20facial%20expressions%20to%20a%20face%20mesh%20while%20enabling%0Amanipulation%20is%20a%20key%20challenge%20in%20facial%20animation%20retargeting.%20Recent%0Adeep-learning%20methods%20address%20this%20by%20encoding%20facial%20expressions%20into%20a%20global%0Alatent%20code%2C%20but%20they%20often%20fail%20to%20capture%20fine-grained%20details%20in%20local%0Aregions.%20While%20some%20methods%20improve%20local%20accuracy%20by%20transferring%20deformations%0Alocally%2C%20this%20often%20complicates%20overall%20control%20of%20the%20facial%20expression.%20To%0Aaddress%20this%2C%20we%20propose%20a%20method%20that%20combines%20the%20strengths%20of%20both%20global%0Aand%20local%20deformation%20models.%20Our%20approach%20enables%20intuitive%20control%20and%0Adetailed%20expression%20cloning%20across%20diverse%20face%20meshes%2C%20regardless%20of%20their%0Aunderlying%20structures.%20The%20core%20idea%20is%20to%20localize%20the%20influence%20of%20the%20global%0Alatent%20code%20on%20the%20target%20mesh.%20Our%20model%20learns%20to%20predict%20skinning%20weights%0Afor%20each%20vertex%20of%20the%20target%20face%20mesh%20through%20indirect%20supervision%20from%0Apredefined%20segmentation%20labels.%20These%20predicted%20weights%20localize%20the%20global%0Alatent%20code%2C%20enabling%20precise%20and%20region-specific%20deformations%20even%20for%20meshes%0Awith%20unseen%20shapes.%20We%20supervise%20the%20latent%20code%20using%20Facial%20Action%20Coding%0ASystem%20%28FACS%29-based%20blendshapes%20to%20ensure%20interpretability%20and%20allow%0Astraightforward%20editing%20of%20the%20generated%20animation.%20Through%20extensive%0Aexperiments%2C%20we%20demonstrate%20improved%20performance%20over%20state-of-the-art%20methods%0Ain%20terms%20of%20expression%20fidelity%2C%20deformation%20transfer%20accuracy%2C%20and%0Aadaptability%20across%20diverse%20mesh%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22416v1&entry.124074799=Read"},
{"title": "A Plug-and-Play Method for Guided Multi-contrast MRI Reconstruction\n  based on Content/Style Modeling", "author": "Chinmay Rao and Matthias van Osch and Nicola Pezzotti and Jeroen de Bresser and Laurens Beljaards and Jakob Meineke and Elwin de Weerdt and Huangling Lu and Mariya Doneva and Marius Staring", "abstract": "  Since multiple MRI contrasts of the same anatomy contain redundant\ninformation, one contrast can guide the reconstruction of an undersampled\nsubsequent contrast. To this end, several end-to-end learning-based guided\nreconstruction methods have been proposed. However, a key challenge is the\nrequirement of large paired training datasets comprising raw data and aligned\nreference images. We propose a modular two-stage approach addressing this\nissue, additionally providing an explanatory framework for the multi-contrast\nproblem based on the shared and non-shared generative factors underlying two\ngiven contrasts. A content/style model of two-contrast image data is learned\nfrom a largely unpaired image-domain dataset and is subsequently applied as a\nplug-and-play operator in iterative reconstruction. The disentanglement of\ncontent and style allows explicit representation of contrast-independent and\ncontrast-specific factors. Consequently, incorporating prior information into\nthe reconstruction reduces to a simple replacement of the aliased content of\nthe reconstruction iterate with high-quality content derived from the reference\nscan. Combining this component with a data consistency step and introducing a\ngeneral corrective process for the content yields an iterative scheme. We name\nthis novel approach PnP-CoSMo. Various aspects like interpretability and\nconvergence are explored via simulations. Furthermore, its practicality is\ndemonstrated on the NYU fastMRI DICOM dataset, showing improved\ngeneralizability compared to end-to-end methods, and on two in-house multi-coil\nraw datasets, offering up to 32.6% more acceleration over learning-based\nnon-guided reconstruction for a given SSIM. In a small radiological task,\nPnP-CoSMo allowed 33.3% more acceleration over clinical reconstruction at\ndiagnostic quality.\n", "link": "http://arxiv.org/abs/2409.13477v3", "date": "2025-05-28", "relevancy": 2.8696, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5826}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5826}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Plug-and-Play%20Method%20for%20Guided%20Multi-contrast%20MRI%20Reconstruction%0A%20%20based%20on%20Content/Style%20Modeling&body=Title%3A%20A%20Plug-and-Play%20Method%20for%20Guided%20Multi-contrast%20MRI%20Reconstruction%0A%20%20based%20on%20Content/Style%20Modeling%0AAuthor%3A%20Chinmay%20Rao%20and%20Matthias%20van%20Osch%20and%20Nicola%20Pezzotti%20and%20Jeroen%20de%20Bresser%20and%20Laurens%20Beljaards%20and%20Jakob%20Meineke%20and%20Elwin%20de%20Weerdt%20and%20Huangling%20Lu%20and%20Mariya%20Doneva%20and%20Marius%20Staring%0AAbstract%3A%20%20%20Since%20multiple%20MRI%20contrasts%20of%20the%20same%20anatomy%20contain%20redundant%0Ainformation%2C%20one%20contrast%20can%20guide%20the%20reconstruction%20of%20an%20undersampled%0Asubsequent%20contrast.%20To%20this%20end%2C%20several%20end-to-end%20learning-based%20guided%0Areconstruction%20methods%20have%20been%20proposed.%20However%2C%20a%20key%20challenge%20is%20the%0Arequirement%20of%20large%20paired%20training%20datasets%20comprising%20raw%20data%20and%20aligned%0Areference%20images.%20We%20propose%20a%20modular%20two-stage%20approach%20addressing%20this%0Aissue%2C%20additionally%20providing%20an%20explanatory%20framework%20for%20the%20multi-contrast%0Aproblem%20based%20on%20the%20shared%20and%20non-shared%20generative%20factors%20underlying%20two%0Agiven%20contrasts.%20A%20content/style%20model%20of%20two-contrast%20image%20data%20is%20learned%0Afrom%20a%20largely%20unpaired%20image-domain%20dataset%20and%20is%20subsequently%20applied%20as%20a%0Aplug-and-play%20operator%20in%20iterative%20reconstruction.%20The%20disentanglement%20of%0Acontent%20and%20style%20allows%20explicit%20representation%20of%20contrast-independent%20and%0Acontrast-specific%20factors.%20Consequently%2C%20incorporating%20prior%20information%20into%0Athe%20reconstruction%20reduces%20to%20a%20simple%20replacement%20of%20the%20aliased%20content%20of%0Athe%20reconstruction%20iterate%20with%20high-quality%20content%20derived%20from%20the%20reference%0Ascan.%20Combining%20this%20component%20with%20a%20data%20consistency%20step%20and%20introducing%20a%0Ageneral%20corrective%20process%20for%20the%20content%20yields%20an%20iterative%20scheme.%20We%20name%0Athis%20novel%20approach%20PnP-CoSMo.%20Various%20aspects%20like%20interpretability%20and%0Aconvergence%20are%20explored%20via%20simulations.%20Furthermore%2C%20its%20practicality%20is%0Ademonstrated%20on%20the%20NYU%20fastMRI%20DICOM%20dataset%2C%20showing%20improved%0Ageneralizability%20compared%20to%20end-to-end%20methods%2C%20and%20on%20two%20in-house%20multi-coil%0Araw%20datasets%2C%20offering%20up%20to%2032.6%25%20more%20acceleration%20over%20learning-based%0Anon-guided%20reconstruction%20for%20a%20given%20SSIM.%20In%20a%20small%20radiological%20task%2C%0APnP-CoSMo%20allowed%2033.3%25%20more%20acceleration%20over%20clinical%20reconstruction%20at%0Adiagnostic%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.13477v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Plug-and-Play%2520Method%2520for%2520Guided%2520Multi-contrast%2520MRI%2520Reconstruction%250A%2520%2520based%2520on%2520Content/Style%2520Modeling%26entry.906535625%3DChinmay%2520Rao%2520and%2520Matthias%2520van%2520Osch%2520and%2520Nicola%2520Pezzotti%2520and%2520Jeroen%2520de%2520Bresser%2520and%2520Laurens%2520Beljaards%2520and%2520Jakob%2520Meineke%2520and%2520Elwin%2520de%2520Weerdt%2520and%2520Huangling%2520Lu%2520and%2520Mariya%2520Doneva%2520and%2520Marius%2520Staring%26entry.1292438233%3D%2520%2520Since%2520multiple%2520MRI%2520contrasts%2520of%2520the%2520same%2520anatomy%2520contain%2520redundant%250Ainformation%252C%2520one%2520contrast%2520can%2520guide%2520the%2520reconstruction%2520of%2520an%2520undersampled%250Asubsequent%2520contrast.%2520To%2520this%2520end%252C%2520several%2520end-to-end%2520learning-based%2520guided%250Areconstruction%2520methods%2520have%2520been%2520proposed.%2520However%252C%2520a%2520key%2520challenge%2520is%2520the%250Arequirement%2520of%2520large%2520paired%2520training%2520datasets%2520comprising%2520raw%2520data%2520and%2520aligned%250Areference%2520images.%2520We%2520propose%2520a%2520modular%2520two-stage%2520approach%2520addressing%2520this%250Aissue%252C%2520additionally%2520providing%2520an%2520explanatory%2520framework%2520for%2520the%2520multi-contrast%250Aproblem%2520based%2520on%2520the%2520shared%2520and%2520non-shared%2520generative%2520factors%2520underlying%2520two%250Agiven%2520contrasts.%2520A%2520content/style%2520model%2520of%2520two-contrast%2520image%2520data%2520is%2520learned%250Afrom%2520a%2520largely%2520unpaired%2520image-domain%2520dataset%2520and%2520is%2520subsequently%2520applied%2520as%2520a%250Aplug-and-play%2520operator%2520in%2520iterative%2520reconstruction.%2520The%2520disentanglement%2520of%250Acontent%2520and%2520style%2520allows%2520explicit%2520representation%2520of%2520contrast-independent%2520and%250Acontrast-specific%2520factors.%2520Consequently%252C%2520incorporating%2520prior%2520information%2520into%250Athe%2520reconstruction%2520reduces%2520to%2520a%2520simple%2520replacement%2520of%2520the%2520aliased%2520content%2520of%250Athe%2520reconstruction%2520iterate%2520with%2520high-quality%2520content%2520derived%2520from%2520the%2520reference%250Ascan.%2520Combining%2520this%2520component%2520with%2520a%2520data%2520consistency%2520step%2520and%2520introducing%2520a%250Ageneral%2520corrective%2520process%2520for%2520the%2520content%2520yields%2520an%2520iterative%2520scheme.%2520We%2520name%250Athis%2520novel%2520approach%2520PnP-CoSMo.%2520Various%2520aspects%2520like%2520interpretability%2520and%250Aconvergence%2520are%2520explored%2520via%2520simulations.%2520Furthermore%252C%2520its%2520practicality%2520is%250Ademonstrated%2520on%2520the%2520NYU%2520fastMRI%2520DICOM%2520dataset%252C%2520showing%2520improved%250Ageneralizability%2520compared%2520to%2520end-to-end%2520methods%252C%2520and%2520on%2520two%2520in-house%2520multi-coil%250Araw%2520datasets%252C%2520offering%2520up%2520to%252032.6%2525%2520more%2520acceleration%2520over%2520learning-based%250Anon-guided%2520reconstruction%2520for%2520a%2520given%2520SSIM.%2520In%2520a%2520small%2520radiological%2520task%252C%250APnP-CoSMo%2520allowed%252033.3%2525%2520more%2520acceleration%2520over%2520clinical%2520reconstruction%2520at%250Adiagnostic%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.13477v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Plug-and-Play%20Method%20for%20Guided%20Multi-contrast%20MRI%20Reconstruction%0A%20%20based%20on%20Content/Style%20Modeling&entry.906535625=Chinmay%20Rao%20and%20Matthias%20van%20Osch%20and%20Nicola%20Pezzotti%20and%20Jeroen%20de%20Bresser%20and%20Laurens%20Beljaards%20and%20Jakob%20Meineke%20and%20Elwin%20de%20Weerdt%20and%20Huangling%20Lu%20and%20Mariya%20Doneva%20and%20Marius%20Staring&entry.1292438233=%20%20Since%20multiple%20MRI%20contrasts%20of%20the%20same%20anatomy%20contain%20redundant%0Ainformation%2C%20one%20contrast%20can%20guide%20the%20reconstruction%20of%20an%20undersampled%0Asubsequent%20contrast.%20To%20this%20end%2C%20several%20end-to-end%20learning-based%20guided%0Areconstruction%20methods%20have%20been%20proposed.%20However%2C%20a%20key%20challenge%20is%20the%0Arequirement%20of%20large%20paired%20training%20datasets%20comprising%20raw%20data%20and%20aligned%0Areference%20images.%20We%20propose%20a%20modular%20two-stage%20approach%20addressing%20this%0Aissue%2C%20additionally%20providing%20an%20explanatory%20framework%20for%20the%20multi-contrast%0Aproblem%20based%20on%20the%20shared%20and%20non-shared%20generative%20factors%20underlying%20two%0Agiven%20contrasts.%20A%20content/style%20model%20of%20two-contrast%20image%20data%20is%20learned%0Afrom%20a%20largely%20unpaired%20image-domain%20dataset%20and%20is%20subsequently%20applied%20as%20a%0Aplug-and-play%20operator%20in%20iterative%20reconstruction.%20The%20disentanglement%20of%0Acontent%20and%20style%20allows%20explicit%20representation%20of%20contrast-independent%20and%0Acontrast-specific%20factors.%20Consequently%2C%20incorporating%20prior%20information%20into%0Athe%20reconstruction%20reduces%20to%20a%20simple%20replacement%20of%20the%20aliased%20content%20of%0Athe%20reconstruction%20iterate%20with%20high-quality%20content%20derived%20from%20the%20reference%0Ascan.%20Combining%20this%20component%20with%20a%20data%20consistency%20step%20and%20introducing%20a%0Ageneral%20corrective%20process%20for%20the%20content%20yields%20an%20iterative%20scheme.%20We%20name%0Athis%20novel%20approach%20PnP-CoSMo.%20Various%20aspects%20like%20interpretability%20and%0Aconvergence%20are%20explored%20via%20simulations.%20Furthermore%2C%20its%20practicality%20is%0Ademonstrated%20on%20the%20NYU%20fastMRI%20DICOM%20dataset%2C%20showing%20improved%0Ageneralizability%20compared%20to%20end-to-end%20methods%2C%20and%20on%20two%20in-house%20multi-coil%0Araw%20datasets%2C%20offering%20up%20to%2032.6%25%20more%20acceleration%20over%20learning-based%0Anon-guided%20reconstruction%20for%20a%20given%20SSIM.%20In%20a%20small%20radiological%20task%2C%0APnP-CoSMo%20allowed%2033.3%25%20more%20acceleration%20over%20clinical%20reconstruction%20at%0Adiagnostic%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.13477v3&entry.124074799=Read"},
{"title": "Zooming from Context to Cue: Hierarchical Preference Optimization for\n  Multi-Image MLLMs", "author": "Xudong Li and Mengdan Zhang and Peixian Chen and Xiawu Zheng and Yan Zhang and Jingyuan Zheng and Yunhang Shen and Ke Li and Chaoyou Fu and Xing Sun and Rongrong Ji", "abstract": "  Multi-modal Large Language Models (MLLMs) excel at single-image tasks but\nstruggle with multi-image understanding due to cross-modal misalignment,\nleading to hallucinations (context omission, conflation, and\nmisinterpretation). Existing methods using Direct Preference Optimization (DPO)\nconstrain optimization to a solitary image reference within the input sequence,\nneglecting holistic context modeling. We propose Context-to-Cue Direct\nPreference Optimization (CcDPO), a multi-level preference optimization\nframework that enhances per-image perception in multi-image settings by zooming\ninto visual clues -- from sequential context to local details. It features: (i)\nContext-Level Optimization : Re-evaluates cognitive biases underlying MLLMs'\nmulti-image context comprehension and integrates a spectrum of low-cost global\nsequence preferences for bias mitigation. (ii) Needle-Level Optimization :\nDirects attention to fine-grained visual details through region-targeted visual\nprompts and multimodal preference supervision. To support scalable\noptimization, we also construct MultiScope-42k, an automatically generated\ndataset with high-quality multi-level preference pairs. Experiments show that\nCcDPO significantly reduces hallucinations and yields consistent performance\ngains across general single- and multi-image tasks.\n", "link": "http://arxiv.org/abs/2505.22396v1", "date": "2025-05-28", "relevancy": 2.8483, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5802}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5802}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zooming%20from%20Context%20to%20Cue%3A%20Hierarchical%20Preference%20Optimization%20for%0A%20%20Multi-Image%20MLLMs&body=Title%3A%20Zooming%20from%20Context%20to%20Cue%3A%20Hierarchical%20Preference%20Optimization%20for%0A%20%20Multi-Image%20MLLMs%0AAuthor%3A%20Xudong%20Li%20and%20Mengdan%20Zhang%20and%20Peixian%20Chen%20and%20Xiawu%20Zheng%20and%20Yan%20Zhang%20and%20Jingyuan%20Zheng%20and%20Yunhang%20Shen%20and%20Ke%20Li%20and%20Chaoyou%20Fu%20and%20Xing%20Sun%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20excel%20at%20single-image%20tasks%20but%0Astruggle%20with%20multi-image%20understanding%20due%20to%20cross-modal%20misalignment%2C%0Aleading%20to%20hallucinations%20%28context%20omission%2C%20conflation%2C%20and%0Amisinterpretation%29.%20Existing%20methods%20using%20Direct%20Preference%20Optimization%20%28DPO%29%0Aconstrain%20optimization%20to%20a%20solitary%20image%20reference%20within%20the%20input%20sequence%2C%0Aneglecting%20holistic%20context%20modeling.%20We%20propose%20Context-to-Cue%20Direct%0APreference%20Optimization%20%28CcDPO%29%2C%20a%20multi-level%20preference%20optimization%0Aframework%20that%20enhances%20per-image%20perception%20in%20multi-image%20settings%20by%20zooming%0Ainto%20visual%20clues%20--%20from%20sequential%20context%20to%20local%20details.%20It%20features%3A%20%28i%29%0AContext-Level%20Optimization%20%3A%20Re-evaluates%20cognitive%20biases%20underlying%20MLLMs%27%0Amulti-image%20context%20comprehension%20and%20integrates%20a%20spectrum%20of%20low-cost%20global%0Asequence%20preferences%20for%20bias%20mitigation.%20%28ii%29%20Needle-Level%20Optimization%20%3A%0ADirects%20attention%20to%20fine-grained%20visual%20details%20through%20region-targeted%20visual%0Aprompts%20and%20multimodal%20preference%20supervision.%20To%20support%20scalable%0Aoptimization%2C%20we%20also%20construct%20MultiScope-42k%2C%20an%20automatically%20generated%0Adataset%20with%20high-quality%20multi-level%20preference%20pairs.%20Experiments%20show%20that%0ACcDPO%20significantly%20reduces%20hallucinations%20and%20yields%20consistent%20performance%0Agains%20across%20general%20single-%20and%20multi-image%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22396v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZooming%2520from%2520Context%2520to%2520Cue%253A%2520Hierarchical%2520Preference%2520Optimization%2520for%250A%2520%2520Multi-Image%2520MLLMs%26entry.906535625%3DXudong%2520Li%2520and%2520Mengdan%2520Zhang%2520and%2520Peixian%2520Chen%2520and%2520Xiawu%2520Zheng%2520and%2520Yan%2520Zhang%2520and%2520Jingyuan%2520Zheng%2520and%2520Yunhang%2520Shen%2520and%2520Ke%2520Li%2520and%2520Chaoyou%2520Fu%2520and%2520Xing%2520Sun%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520excel%2520at%2520single-image%2520tasks%2520but%250Astruggle%2520with%2520multi-image%2520understanding%2520due%2520to%2520cross-modal%2520misalignment%252C%250Aleading%2520to%2520hallucinations%2520%2528context%2520omission%252C%2520conflation%252C%2520and%250Amisinterpretation%2529.%2520Existing%2520methods%2520using%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%250Aconstrain%2520optimization%2520to%2520a%2520solitary%2520image%2520reference%2520within%2520the%2520input%2520sequence%252C%250Aneglecting%2520holistic%2520context%2520modeling.%2520We%2520propose%2520Context-to-Cue%2520Direct%250APreference%2520Optimization%2520%2528CcDPO%2529%252C%2520a%2520multi-level%2520preference%2520optimization%250Aframework%2520that%2520enhances%2520per-image%2520perception%2520in%2520multi-image%2520settings%2520by%2520zooming%250Ainto%2520visual%2520clues%2520--%2520from%2520sequential%2520context%2520to%2520local%2520details.%2520It%2520features%253A%2520%2528i%2529%250AContext-Level%2520Optimization%2520%253A%2520Re-evaluates%2520cognitive%2520biases%2520underlying%2520MLLMs%2527%250Amulti-image%2520context%2520comprehension%2520and%2520integrates%2520a%2520spectrum%2520of%2520low-cost%2520global%250Asequence%2520preferences%2520for%2520bias%2520mitigation.%2520%2528ii%2529%2520Needle-Level%2520Optimization%2520%253A%250ADirects%2520attention%2520to%2520fine-grained%2520visual%2520details%2520through%2520region-targeted%2520visual%250Aprompts%2520and%2520multimodal%2520preference%2520supervision.%2520To%2520support%2520scalable%250Aoptimization%252C%2520we%2520also%2520construct%2520MultiScope-42k%252C%2520an%2520automatically%2520generated%250Adataset%2520with%2520high-quality%2520multi-level%2520preference%2520pairs.%2520Experiments%2520show%2520that%250ACcDPO%2520significantly%2520reduces%2520hallucinations%2520and%2520yields%2520consistent%2520performance%250Agains%2520across%2520general%2520single-%2520and%2520multi-image%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22396v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zooming%20from%20Context%20to%20Cue%3A%20Hierarchical%20Preference%20Optimization%20for%0A%20%20Multi-Image%20MLLMs&entry.906535625=Xudong%20Li%20and%20Mengdan%20Zhang%20and%20Peixian%20Chen%20and%20Xiawu%20Zheng%20and%20Yan%20Zhang%20and%20Jingyuan%20Zheng%20and%20Yunhang%20Shen%20and%20Ke%20Li%20and%20Chaoyou%20Fu%20and%20Xing%20Sun%20and%20Rongrong%20Ji&entry.1292438233=%20%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20excel%20at%20single-image%20tasks%20but%0Astruggle%20with%20multi-image%20understanding%20due%20to%20cross-modal%20misalignment%2C%0Aleading%20to%20hallucinations%20%28context%20omission%2C%20conflation%2C%20and%0Amisinterpretation%29.%20Existing%20methods%20using%20Direct%20Preference%20Optimization%20%28DPO%29%0Aconstrain%20optimization%20to%20a%20solitary%20image%20reference%20within%20the%20input%20sequence%2C%0Aneglecting%20holistic%20context%20modeling.%20We%20propose%20Context-to-Cue%20Direct%0APreference%20Optimization%20%28CcDPO%29%2C%20a%20multi-level%20preference%20optimization%0Aframework%20that%20enhances%20per-image%20perception%20in%20multi-image%20settings%20by%20zooming%0Ainto%20visual%20clues%20--%20from%20sequential%20context%20to%20local%20details.%20It%20features%3A%20%28i%29%0AContext-Level%20Optimization%20%3A%20Re-evaluates%20cognitive%20biases%20underlying%20MLLMs%27%0Amulti-image%20context%20comprehension%20and%20integrates%20a%20spectrum%20of%20low-cost%20global%0Asequence%20preferences%20for%20bias%20mitigation.%20%28ii%29%20Needle-Level%20Optimization%20%3A%0ADirects%20attention%20to%20fine-grained%20visual%20details%20through%20region-targeted%20visual%0Aprompts%20and%20multimodal%20preference%20supervision.%20To%20support%20scalable%0Aoptimization%2C%20we%20also%20construct%20MultiScope-42k%2C%20an%20automatically%20generated%0Adataset%20with%20high-quality%20multi-level%20preference%20pairs.%20Experiments%20show%20that%0ACcDPO%20significantly%20reduces%20hallucinations%20and%20yields%20consistent%20performance%0Agains%20across%20general%20single-%20and%20multi-image%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22396v1&entry.124074799=Read"},
{"title": "Fast 3D point clouds retrieval for Large-scale 3D Place Recognition", "author": "Chahine-Nicolas Zede and Laurent Carrafa and Val\u00e9rie Gouet-Brunet", "abstract": "  Retrieval in 3D point clouds is a challenging task that consists in\nretrieving the most similar point clouds to a given query within a reference of\n3D points. Current methods focus on comparing descriptors of point clouds in\norder to identify similar ones. Due to the complexity of this latter step, here\nwe focus on the acceleration of the retrieval by adapting the Differentiable\nSearch Index (DSI), a transformer-based approach initially designed for text\ninformation retrieval, for 3D point clouds retrieval. Our approach generates 1D\nidentifiers based on the point descriptors, enabling direct retrieval in\nconstant time. To adapt DSI to 3D data, we integrate Vision Transformers to map\ndescriptors to these identifiers while incorporating positional and semantic\nencoding. The approach is evaluated for place recognition on a public benchmark\ncomparing its retrieval capabilities against state-of-the-art methods, in terms\nof quality and speed of returned point clouds.\n", "link": "http://arxiv.org/abs/2502.21067v2", "date": "2025-05-28", "relevancy": 2.8026, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5735}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.554}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%203D%20point%20clouds%20retrieval%20for%20Large-scale%203D%20Place%20Recognition&body=Title%3A%20Fast%203D%20point%20clouds%20retrieval%20for%20Large-scale%203D%20Place%20Recognition%0AAuthor%3A%20Chahine-Nicolas%20Zede%20and%20Laurent%20Carrafa%20and%20Val%C3%A9rie%20Gouet-Brunet%0AAbstract%3A%20%20%20Retrieval%20in%203D%20point%20clouds%20is%20a%20challenging%20task%20that%20consists%20in%0Aretrieving%20the%20most%20similar%20point%20clouds%20to%20a%20given%20query%20within%20a%20reference%20of%0A3D%20points.%20Current%20methods%20focus%20on%20comparing%20descriptors%20of%20point%20clouds%20in%0Aorder%20to%20identify%20similar%20ones.%20Due%20to%20the%20complexity%20of%20this%20latter%20step%2C%20here%0Awe%20focus%20on%20the%20acceleration%20of%20the%20retrieval%20by%20adapting%20the%20Differentiable%0ASearch%20Index%20%28DSI%29%2C%20a%20transformer-based%20approach%20initially%20designed%20for%20text%0Ainformation%20retrieval%2C%20for%203D%20point%20clouds%20retrieval.%20Our%20approach%20generates%201D%0Aidentifiers%20based%20on%20the%20point%20descriptors%2C%20enabling%20direct%20retrieval%20in%0Aconstant%20time.%20To%20adapt%20DSI%20to%203D%20data%2C%20we%20integrate%20Vision%20Transformers%20to%20map%0Adescriptors%20to%20these%20identifiers%20while%20incorporating%20positional%20and%20semantic%0Aencoding.%20The%20approach%20is%20evaluated%20for%20place%20recognition%20on%20a%20public%20benchmark%0Acomparing%20its%20retrieval%20capabilities%20against%20state-of-the-art%20methods%2C%20in%20terms%0Aof%20quality%20and%20speed%20of%20returned%20point%20clouds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.21067v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%25203D%2520point%2520clouds%2520retrieval%2520for%2520Large-scale%25203D%2520Place%2520Recognition%26entry.906535625%3DChahine-Nicolas%2520Zede%2520and%2520Laurent%2520Carrafa%2520and%2520Val%25C3%25A9rie%2520Gouet-Brunet%26entry.1292438233%3D%2520%2520Retrieval%2520in%25203D%2520point%2520clouds%2520is%2520a%2520challenging%2520task%2520that%2520consists%2520in%250Aretrieving%2520the%2520most%2520similar%2520point%2520clouds%2520to%2520a%2520given%2520query%2520within%2520a%2520reference%2520of%250A3D%2520points.%2520Current%2520methods%2520focus%2520on%2520comparing%2520descriptors%2520of%2520point%2520clouds%2520in%250Aorder%2520to%2520identify%2520similar%2520ones.%2520Due%2520to%2520the%2520complexity%2520of%2520this%2520latter%2520step%252C%2520here%250Awe%2520focus%2520on%2520the%2520acceleration%2520of%2520the%2520retrieval%2520by%2520adapting%2520the%2520Differentiable%250ASearch%2520Index%2520%2528DSI%2529%252C%2520a%2520transformer-based%2520approach%2520initially%2520designed%2520for%2520text%250Ainformation%2520retrieval%252C%2520for%25203D%2520point%2520clouds%2520retrieval.%2520Our%2520approach%2520generates%25201D%250Aidentifiers%2520based%2520on%2520the%2520point%2520descriptors%252C%2520enabling%2520direct%2520retrieval%2520in%250Aconstant%2520time.%2520To%2520adapt%2520DSI%2520to%25203D%2520data%252C%2520we%2520integrate%2520Vision%2520Transformers%2520to%2520map%250Adescriptors%2520to%2520these%2520identifiers%2520while%2520incorporating%2520positional%2520and%2520semantic%250Aencoding.%2520The%2520approach%2520is%2520evaluated%2520for%2520place%2520recognition%2520on%2520a%2520public%2520benchmark%250Acomparing%2520its%2520retrieval%2520capabilities%2520against%2520state-of-the-art%2520methods%252C%2520in%2520terms%250Aof%2520quality%2520and%2520speed%2520of%2520returned%2520point%2520clouds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.21067v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%203D%20point%20clouds%20retrieval%20for%20Large-scale%203D%20Place%20Recognition&entry.906535625=Chahine-Nicolas%20Zede%20and%20Laurent%20Carrafa%20and%20Val%C3%A9rie%20Gouet-Brunet&entry.1292438233=%20%20Retrieval%20in%203D%20point%20clouds%20is%20a%20challenging%20task%20that%20consists%20in%0Aretrieving%20the%20most%20similar%20point%20clouds%20to%20a%20given%20query%20within%20a%20reference%20of%0A3D%20points.%20Current%20methods%20focus%20on%20comparing%20descriptors%20of%20point%20clouds%20in%0Aorder%20to%20identify%20similar%20ones.%20Due%20to%20the%20complexity%20of%20this%20latter%20step%2C%20here%0Awe%20focus%20on%20the%20acceleration%20of%20the%20retrieval%20by%20adapting%20the%20Differentiable%0ASearch%20Index%20%28DSI%29%2C%20a%20transformer-based%20approach%20initially%20designed%20for%20text%0Ainformation%20retrieval%2C%20for%203D%20point%20clouds%20retrieval.%20Our%20approach%20generates%201D%0Aidentifiers%20based%20on%20the%20point%20descriptors%2C%20enabling%20direct%20retrieval%20in%0Aconstant%20time.%20To%20adapt%20DSI%20to%203D%20data%2C%20we%20integrate%20Vision%20Transformers%20to%20map%0Adescriptors%20to%20these%20identifiers%20while%20incorporating%20positional%20and%20semantic%0Aencoding.%20The%20approach%20is%20evaluated%20for%20place%20recognition%20on%20a%20public%20benchmark%0Acomparing%20its%20retrieval%20capabilities%20against%20state-of-the-art%20methods%2C%20in%20terms%0Aof%20quality%20and%20speed%20of%20returned%20point%20clouds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.21067v2&entry.124074799=Read"},
{"title": "On Geometry-Enhanced Parameter-Efficient Fine-Tuning for 3D Scene\n  Segmentation", "author": "Liyao Tang and Zhe Chen and Dacheng Tao", "abstract": "  The emergence of large-scale pre-trained point cloud models has significantly\nadvanced 3D scene understanding, but adapting these models to specific\ndownstream tasks typically demands full fine-tuning, incurring high\ncomputational and storage costs. Parameter-efficient fine-tuning (PEFT)\ntechniques, successful in natural language processing and 2D vision tasks,\nwould underperform when naively applied to 3D point cloud models due to\nsignificant geometric and spatial distribution shifts. Existing PEFT methods\ncommonly treat points as orderless tokens, neglecting important local spatial\nstructures and global geometric contexts in 3D modeling. To bridge this gap, we\nintroduce the Geometric Encoding Mixer (GEM), a novel geometry-aware PEFT\nmodule specifically designed for 3D point cloud transformers. GEM explicitly\nintegrates fine-grained local positional encodings with a lightweight latent\nattention mechanism to capture comprehensive global context, thereby\neffectively addressing the spatial and geometric distribution mismatch.\nExtensive experiments demonstrate that GEM achieves performance comparable to\nor sometimes even exceeding full fine-tuning, while only updating 1.6% of the\nmodel's parameters, fewer than other PEFT methods. With significantly reduced\ntraining time and memory requirements, our approach thus sets a new benchmark\nfor efficient, scalable, and geometry-aware fine-tuning of large-scale 3D point\ncloud models. Code will be released.\n", "link": "http://arxiv.org/abs/2505.22444v1", "date": "2025-05-28", "relevancy": 2.7675, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5749}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5437}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Geometry-Enhanced%20Parameter-Efficient%20Fine-Tuning%20for%203D%20Scene%0A%20%20Segmentation&body=Title%3A%20On%20Geometry-Enhanced%20Parameter-Efficient%20Fine-Tuning%20for%203D%20Scene%0A%20%20Segmentation%0AAuthor%3A%20Liyao%20Tang%20and%20Zhe%20Chen%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20The%20emergence%20of%20large-scale%20pre-trained%20point%20cloud%20models%20has%20significantly%0Aadvanced%203D%20scene%20understanding%2C%20but%20adapting%20these%20models%20to%20specific%0Adownstream%20tasks%20typically%20demands%20full%20fine-tuning%2C%20incurring%20high%0Acomputational%20and%20storage%20costs.%20Parameter-efficient%20fine-tuning%20%28PEFT%29%0Atechniques%2C%20successful%20in%20natural%20language%20processing%20and%202D%20vision%20tasks%2C%0Awould%20underperform%20when%20naively%20applied%20to%203D%20point%20cloud%20models%20due%20to%0Asignificant%20geometric%20and%20spatial%20distribution%20shifts.%20Existing%20PEFT%20methods%0Acommonly%20treat%20points%20as%20orderless%20tokens%2C%20neglecting%20important%20local%20spatial%0Astructures%20and%20global%20geometric%20contexts%20in%203D%20modeling.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20the%20Geometric%20Encoding%20Mixer%20%28GEM%29%2C%20a%20novel%20geometry-aware%20PEFT%0Amodule%20specifically%20designed%20for%203D%20point%20cloud%20transformers.%20GEM%20explicitly%0Aintegrates%20fine-grained%20local%20positional%20encodings%20with%20a%20lightweight%20latent%0Aattention%20mechanism%20to%20capture%20comprehensive%20global%20context%2C%20thereby%0Aeffectively%20addressing%20the%20spatial%20and%20geometric%20distribution%20mismatch.%0AExtensive%20experiments%20demonstrate%20that%20GEM%20achieves%20performance%20comparable%20to%0Aor%20sometimes%20even%20exceeding%20full%20fine-tuning%2C%20while%20only%20updating%201.6%25%20of%20the%0Amodel%27s%20parameters%2C%20fewer%20than%20other%20PEFT%20methods.%20With%20significantly%20reduced%0Atraining%20time%20and%20memory%20requirements%2C%20our%20approach%20thus%20sets%20a%20new%20benchmark%0Afor%20efficient%2C%20scalable%2C%20and%20geometry-aware%20fine-tuning%20of%20large-scale%203D%20point%0Acloud%20models.%20Code%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22444v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Geometry-Enhanced%2520Parameter-Efficient%2520Fine-Tuning%2520for%25203D%2520Scene%250A%2520%2520Segmentation%26entry.906535625%3DLiyao%2520Tang%2520and%2520Zhe%2520Chen%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520large-scale%2520pre-trained%2520point%2520cloud%2520models%2520has%2520significantly%250Aadvanced%25203D%2520scene%2520understanding%252C%2520but%2520adapting%2520these%2520models%2520to%2520specific%250Adownstream%2520tasks%2520typically%2520demands%2520full%2520fine-tuning%252C%2520incurring%2520high%250Acomputational%2520and%2520storage%2520costs.%2520Parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%250Atechniques%252C%2520successful%2520in%2520natural%2520language%2520processing%2520and%25202D%2520vision%2520tasks%252C%250Awould%2520underperform%2520when%2520naively%2520applied%2520to%25203D%2520point%2520cloud%2520models%2520due%2520to%250Asignificant%2520geometric%2520and%2520spatial%2520distribution%2520shifts.%2520Existing%2520PEFT%2520methods%250Acommonly%2520treat%2520points%2520as%2520orderless%2520tokens%252C%2520neglecting%2520important%2520local%2520spatial%250Astructures%2520and%2520global%2520geometric%2520contexts%2520in%25203D%2520modeling.%2520To%2520bridge%2520this%2520gap%252C%2520we%250Aintroduce%2520the%2520Geometric%2520Encoding%2520Mixer%2520%2528GEM%2529%252C%2520a%2520novel%2520geometry-aware%2520PEFT%250Amodule%2520specifically%2520designed%2520for%25203D%2520point%2520cloud%2520transformers.%2520GEM%2520explicitly%250Aintegrates%2520fine-grained%2520local%2520positional%2520encodings%2520with%2520a%2520lightweight%2520latent%250Aattention%2520mechanism%2520to%2520capture%2520comprehensive%2520global%2520context%252C%2520thereby%250Aeffectively%2520addressing%2520the%2520spatial%2520and%2520geometric%2520distribution%2520mismatch.%250AExtensive%2520experiments%2520demonstrate%2520that%2520GEM%2520achieves%2520performance%2520comparable%2520to%250Aor%2520sometimes%2520even%2520exceeding%2520full%2520fine-tuning%252C%2520while%2520only%2520updating%25201.6%2525%2520of%2520the%250Amodel%2527s%2520parameters%252C%2520fewer%2520than%2520other%2520PEFT%2520methods.%2520With%2520significantly%2520reduced%250Atraining%2520time%2520and%2520memory%2520requirements%252C%2520our%2520approach%2520thus%2520sets%2520a%2520new%2520benchmark%250Afor%2520efficient%252C%2520scalable%252C%2520and%2520geometry-aware%2520fine-tuning%2520of%2520large-scale%25203D%2520point%250Acloud%2520models.%2520Code%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22444v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Geometry-Enhanced%20Parameter-Efficient%20Fine-Tuning%20for%203D%20Scene%0A%20%20Segmentation&entry.906535625=Liyao%20Tang%20and%20Zhe%20Chen%20and%20Dacheng%20Tao&entry.1292438233=%20%20The%20emergence%20of%20large-scale%20pre-trained%20point%20cloud%20models%20has%20significantly%0Aadvanced%203D%20scene%20understanding%2C%20but%20adapting%20these%20models%20to%20specific%0Adownstream%20tasks%20typically%20demands%20full%20fine-tuning%2C%20incurring%20high%0Acomputational%20and%20storage%20costs.%20Parameter-efficient%20fine-tuning%20%28PEFT%29%0Atechniques%2C%20successful%20in%20natural%20language%20processing%20and%202D%20vision%20tasks%2C%0Awould%20underperform%20when%20naively%20applied%20to%203D%20point%20cloud%20models%20due%20to%0Asignificant%20geometric%20and%20spatial%20distribution%20shifts.%20Existing%20PEFT%20methods%0Acommonly%20treat%20points%20as%20orderless%20tokens%2C%20neglecting%20important%20local%20spatial%0Astructures%20and%20global%20geometric%20contexts%20in%203D%20modeling.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20the%20Geometric%20Encoding%20Mixer%20%28GEM%29%2C%20a%20novel%20geometry-aware%20PEFT%0Amodule%20specifically%20designed%20for%203D%20point%20cloud%20transformers.%20GEM%20explicitly%0Aintegrates%20fine-grained%20local%20positional%20encodings%20with%20a%20lightweight%20latent%0Aattention%20mechanism%20to%20capture%20comprehensive%20global%20context%2C%20thereby%0Aeffectively%20addressing%20the%20spatial%20and%20geometric%20distribution%20mismatch.%0AExtensive%20experiments%20demonstrate%20that%20GEM%20achieves%20performance%20comparable%20to%0Aor%20sometimes%20even%20exceeding%20full%20fine-tuning%2C%20while%20only%20updating%201.6%25%20of%20the%0Amodel%27s%20parameters%2C%20fewer%20than%20other%20PEFT%20methods.%20With%20significantly%20reduced%0Atraining%20time%20and%20memory%20requirements%2C%20our%20approach%20thus%20sets%20a%20new%20benchmark%0Afor%20efficient%2C%20scalable%2C%20and%20geometry-aware%20fine-tuning%20of%20large-scale%203D%20point%0Acloud%20models.%20Code%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22444v1&entry.124074799=Read"},
{"title": "GraphOmni: A Comprehensive and Extendable Benchmark Framework for Large\n  Language Models on Graph-theoretic Tasks", "author": "Hao Xu and Xiangru Jian and Xinjian Zhao and Wei Pang and Chao Zhang and Suyuchen Wang and Qixin Zhang and Zhengyuan Dong and Joao Monteiro and Bang Liu and Qiuzhuang Sun and Tianshu Yu", "abstract": "  This paper introduces GraphOmni, a comprehensive benchmark designed to\nevaluate the reasoning capabilities of LLMs on graph-theoretic tasks\narticulated in natural language. GraphOmni encompasses diverse graph types,\nserialization formats, and prompting schemes, significantly exceeding prior\nefforts in both scope and depth. Through extensive systematic evaluation, we\nidentify critical interactions among these dimensions, demonstrating their\nsubstantial impact on model performance. Our experiments reveal that\nstate-of-the-art models like Claude-3.5 and o4-mini consistently outperform\nother models, yet even these leading models exhibit substantial room for\nimprovement. Performance variability is evident depending on the specific\ncombinations of factors we considered, underscoring the necessity of\ncomprehensive evaluations across these interconnected dimensions. Additionally,\nwe observe distinct impacts of serialization and prompting strategies between\nopen-source and closed-source models, encouraging the development of tailored\napproaches. Motivated by the findings, we also propose a reinforcement\nlearning-inspired framework that adaptively selects the optimal factors\ninfluencing LLM reasoning capabilities. This flexible and extendable benchmark\nnot only deepens our understanding of LLM performance on structured tasks but\nalso provides a robust foundation for advancing research in LLM-based graph\nreasoning. The code and datasets are available at\nhttps://github.com/GAI-Community/GraphOmni.\n", "link": "http://arxiv.org/abs/2504.12764v3", "date": "2025-05-28", "relevancy": 2.7473, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5655}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5655}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphOmni%3A%20A%20Comprehensive%20and%20Extendable%20Benchmark%20Framework%20for%20Large%0A%20%20Language%20Models%20on%20Graph-theoretic%20Tasks&body=Title%3A%20GraphOmni%3A%20A%20Comprehensive%20and%20Extendable%20Benchmark%20Framework%20for%20Large%0A%20%20Language%20Models%20on%20Graph-theoretic%20Tasks%0AAuthor%3A%20Hao%20Xu%20and%20Xiangru%20Jian%20and%20Xinjian%20Zhao%20and%20Wei%20Pang%20and%20Chao%20Zhang%20and%20Suyuchen%20Wang%20and%20Qixin%20Zhang%20and%20Zhengyuan%20Dong%20and%20Joao%20Monteiro%20and%20Bang%20Liu%20and%20Qiuzhuang%20Sun%20and%20Tianshu%20Yu%0AAbstract%3A%20%20%20This%20paper%20introduces%20GraphOmni%2C%20a%20comprehensive%20benchmark%20designed%20to%0Aevaluate%20the%20reasoning%20capabilities%20of%20LLMs%20on%20graph-theoretic%20tasks%0Aarticulated%20in%20natural%20language.%20GraphOmni%20encompasses%20diverse%20graph%20types%2C%0Aserialization%20formats%2C%20and%20prompting%20schemes%2C%20significantly%20exceeding%20prior%0Aefforts%20in%20both%20scope%20and%20depth.%20Through%20extensive%20systematic%20evaluation%2C%20we%0Aidentify%20critical%20interactions%20among%20these%20dimensions%2C%20demonstrating%20their%0Asubstantial%20impact%20on%20model%20performance.%20Our%20experiments%20reveal%20that%0Astate-of-the-art%20models%20like%20Claude-3.5%20and%20o4-mini%20consistently%20outperform%0Aother%20models%2C%20yet%20even%20these%20leading%20models%20exhibit%20substantial%20room%20for%0Aimprovement.%20Performance%20variability%20is%20evident%20depending%20on%20the%20specific%0Acombinations%20of%20factors%20we%20considered%2C%20underscoring%20the%20necessity%20of%0Acomprehensive%20evaluations%20across%20these%20interconnected%20dimensions.%20Additionally%2C%0Awe%20observe%20distinct%20impacts%20of%20serialization%20and%20prompting%20strategies%20between%0Aopen-source%20and%20closed-source%20models%2C%20encouraging%20the%20development%20of%20tailored%0Aapproaches.%20Motivated%20by%20the%20findings%2C%20we%20also%20propose%20a%20reinforcement%0Alearning-inspired%20framework%20that%20adaptively%20selects%20the%20optimal%20factors%0Ainfluencing%20LLM%20reasoning%20capabilities.%20This%20flexible%20and%20extendable%20benchmark%0Anot%20only%20deepens%20our%20understanding%20of%20LLM%20performance%20on%20structured%20tasks%20but%0Aalso%20provides%20a%20robust%20foundation%20for%20advancing%20research%20in%20LLM-based%20graph%0Areasoning.%20The%20code%20and%20datasets%20are%20available%20at%0Ahttps%3A//github.com/GAI-Community/GraphOmni.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12764v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphOmni%253A%2520A%2520Comprehensive%2520and%2520Extendable%2520Benchmark%2520Framework%2520for%2520Large%250A%2520%2520Language%2520Models%2520on%2520Graph-theoretic%2520Tasks%26entry.906535625%3DHao%2520Xu%2520and%2520Xiangru%2520Jian%2520and%2520Xinjian%2520Zhao%2520and%2520Wei%2520Pang%2520and%2520Chao%2520Zhang%2520and%2520Suyuchen%2520Wang%2520and%2520Qixin%2520Zhang%2520and%2520Zhengyuan%2520Dong%2520and%2520Joao%2520Monteiro%2520and%2520Bang%2520Liu%2520and%2520Qiuzhuang%2520Sun%2520and%2520Tianshu%2520Yu%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520GraphOmni%252C%2520a%2520comprehensive%2520benchmark%2520designed%2520to%250Aevaluate%2520the%2520reasoning%2520capabilities%2520of%2520LLMs%2520on%2520graph-theoretic%2520tasks%250Aarticulated%2520in%2520natural%2520language.%2520GraphOmni%2520encompasses%2520diverse%2520graph%2520types%252C%250Aserialization%2520formats%252C%2520and%2520prompting%2520schemes%252C%2520significantly%2520exceeding%2520prior%250Aefforts%2520in%2520both%2520scope%2520and%2520depth.%2520Through%2520extensive%2520systematic%2520evaluation%252C%2520we%250Aidentify%2520critical%2520interactions%2520among%2520these%2520dimensions%252C%2520demonstrating%2520their%250Asubstantial%2520impact%2520on%2520model%2520performance.%2520Our%2520experiments%2520reveal%2520that%250Astate-of-the-art%2520models%2520like%2520Claude-3.5%2520and%2520o4-mini%2520consistently%2520outperform%250Aother%2520models%252C%2520yet%2520even%2520these%2520leading%2520models%2520exhibit%2520substantial%2520room%2520for%250Aimprovement.%2520Performance%2520variability%2520is%2520evident%2520depending%2520on%2520the%2520specific%250Acombinations%2520of%2520factors%2520we%2520considered%252C%2520underscoring%2520the%2520necessity%2520of%250Acomprehensive%2520evaluations%2520across%2520these%2520interconnected%2520dimensions.%2520Additionally%252C%250Awe%2520observe%2520distinct%2520impacts%2520of%2520serialization%2520and%2520prompting%2520strategies%2520between%250Aopen-source%2520and%2520closed-source%2520models%252C%2520encouraging%2520the%2520development%2520of%2520tailored%250Aapproaches.%2520Motivated%2520by%2520the%2520findings%252C%2520we%2520also%2520propose%2520a%2520reinforcement%250Alearning-inspired%2520framework%2520that%2520adaptively%2520selects%2520the%2520optimal%2520factors%250Ainfluencing%2520LLM%2520reasoning%2520capabilities.%2520This%2520flexible%2520and%2520extendable%2520benchmark%250Anot%2520only%2520deepens%2520our%2520understanding%2520of%2520LLM%2520performance%2520on%2520structured%2520tasks%2520but%250Aalso%2520provides%2520a%2520robust%2520foundation%2520for%2520advancing%2520research%2520in%2520LLM-based%2520graph%250Areasoning.%2520The%2520code%2520and%2520datasets%2520are%2520available%2520at%250Ahttps%253A//github.com/GAI-Community/GraphOmni.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12764v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphOmni%3A%20A%20Comprehensive%20and%20Extendable%20Benchmark%20Framework%20for%20Large%0A%20%20Language%20Models%20on%20Graph-theoretic%20Tasks&entry.906535625=Hao%20Xu%20and%20Xiangru%20Jian%20and%20Xinjian%20Zhao%20and%20Wei%20Pang%20and%20Chao%20Zhang%20and%20Suyuchen%20Wang%20and%20Qixin%20Zhang%20and%20Zhengyuan%20Dong%20and%20Joao%20Monteiro%20and%20Bang%20Liu%20and%20Qiuzhuang%20Sun%20and%20Tianshu%20Yu&entry.1292438233=%20%20This%20paper%20introduces%20GraphOmni%2C%20a%20comprehensive%20benchmark%20designed%20to%0Aevaluate%20the%20reasoning%20capabilities%20of%20LLMs%20on%20graph-theoretic%20tasks%0Aarticulated%20in%20natural%20language.%20GraphOmni%20encompasses%20diverse%20graph%20types%2C%0Aserialization%20formats%2C%20and%20prompting%20schemes%2C%20significantly%20exceeding%20prior%0Aefforts%20in%20both%20scope%20and%20depth.%20Through%20extensive%20systematic%20evaluation%2C%20we%0Aidentify%20critical%20interactions%20among%20these%20dimensions%2C%20demonstrating%20their%0Asubstantial%20impact%20on%20model%20performance.%20Our%20experiments%20reveal%20that%0Astate-of-the-art%20models%20like%20Claude-3.5%20and%20o4-mini%20consistently%20outperform%0Aother%20models%2C%20yet%20even%20these%20leading%20models%20exhibit%20substantial%20room%20for%0Aimprovement.%20Performance%20variability%20is%20evident%20depending%20on%20the%20specific%0Acombinations%20of%20factors%20we%20considered%2C%20underscoring%20the%20necessity%20of%0Acomprehensive%20evaluations%20across%20these%20interconnected%20dimensions.%20Additionally%2C%0Awe%20observe%20distinct%20impacts%20of%20serialization%20and%20prompting%20strategies%20between%0Aopen-source%20and%20closed-source%20models%2C%20encouraging%20the%20development%20of%20tailored%0Aapproaches.%20Motivated%20by%20the%20findings%2C%20we%20also%20propose%20a%20reinforcement%0Alearning-inspired%20framework%20that%20adaptively%20selects%20the%20optimal%20factors%0Ainfluencing%20LLM%20reasoning%20capabilities.%20This%20flexible%20and%20extendable%20benchmark%0Anot%20only%20deepens%20our%20understanding%20of%20LLM%20performance%20on%20structured%20tasks%20but%0Aalso%20provides%20a%20robust%20foundation%20for%20advancing%20research%20in%20LLM-based%20graph%0Areasoning.%20The%20code%20and%20datasets%20are%20available%20at%0Ahttps%3A//github.com/GAI-Community/GraphOmni.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12764v3&entry.124074799=Read"},
{"title": "Sparsification and Reconstruction from the Perspective of Representation\n  Geometry", "author": "Wenjie Sun and Bingzhe Wu and Zhile Yang and Chengke Wu", "abstract": "  Sparse Autoencoders (SAEs) have emerged as a predominant tool in mechanistic\ninterpretability, aiming to identify interpretable monosemantic features.\nHowever, how does sparse encoding organize the representations of activation\nvector from language models? What is the relationship between this\norganizational paradigm and feature disentanglement as well as reconstruction\nperformance? To address these questions, we propose the SAEMA, which validates\nthe stratified structure of the representation by observing the variability of\nthe rank of the symmetric semipositive definite (SSPD) matrix corresponding to\nthe modal tensor unfolded along the latent tensor with the level of noise added\nto the residual stream. To systematically investigate how sparse encoding\nalters representational structures, we define local and global representations,\ndemonstrating that they amplify inter-feature distinctions by merging similar\nsemantic features and introducing additional dimensionality. Furthermore, we\nintervene the global representation from an optimization perspective, proving a\nsignificant causal relationship between their separability and the\nreconstruction performance. This study explains the principles of sparsity from\nthe perspective of representational geometry and demonstrates the impact of\nchanges in representational structure on reconstruction performance.\nParticularly emphasizes the necessity of understanding representations and\nincorporating representational constraints, providing empirical references for\ndeveloping new interpretable tools and improving SAEs. The code is available at\n\\hyperlink{https://github.com/wenjie1835/SAERepGeo}{https://github.com/wenjie1835/SAERepGeo}.\n", "link": "http://arxiv.org/abs/2505.22506v1", "date": "2025-05-28", "relevancy": 2.7428, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5586}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5435}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparsification%20and%20Reconstruction%20from%20the%20Perspective%20of%20Representation%0A%20%20Geometry&body=Title%3A%20Sparsification%20and%20Reconstruction%20from%20the%20Perspective%20of%20Representation%0A%20%20Geometry%0AAuthor%3A%20Wenjie%20Sun%20and%20Bingzhe%20Wu%20and%20Zhile%20Yang%20and%20Chengke%20Wu%0AAbstract%3A%20%20%20Sparse%20Autoencoders%20%28SAEs%29%20have%20emerged%20as%20a%20predominant%20tool%20in%20mechanistic%0Ainterpretability%2C%20aiming%20to%20identify%20interpretable%20monosemantic%20features.%0AHowever%2C%20how%20does%20sparse%20encoding%20organize%20the%20representations%20of%20activation%0Avector%20from%20language%20models%3F%20What%20is%20the%20relationship%20between%20this%0Aorganizational%20paradigm%20and%20feature%20disentanglement%20as%20well%20as%20reconstruction%0Aperformance%3F%20To%20address%20these%20questions%2C%20we%20propose%20the%20SAEMA%2C%20which%20validates%0Athe%20stratified%20structure%20of%20the%20representation%20by%20observing%20the%20variability%20of%0Athe%20rank%20of%20the%20symmetric%20semipositive%20definite%20%28SSPD%29%20matrix%20corresponding%20to%0Athe%20modal%20tensor%20unfolded%20along%20the%20latent%20tensor%20with%20the%20level%20of%20noise%20added%0Ato%20the%20residual%20stream.%20To%20systematically%20investigate%20how%20sparse%20encoding%0Aalters%20representational%20structures%2C%20we%20define%20local%20and%20global%20representations%2C%0Ademonstrating%20that%20they%20amplify%20inter-feature%20distinctions%20by%20merging%20similar%0Asemantic%20features%20and%20introducing%20additional%20dimensionality.%20Furthermore%2C%20we%0Aintervene%20the%20global%20representation%20from%20an%20optimization%20perspective%2C%20proving%20a%0Asignificant%20causal%20relationship%20between%20their%20separability%20and%20the%0Areconstruction%20performance.%20This%20study%20explains%20the%20principles%20of%20sparsity%20from%0Athe%20perspective%20of%20representational%20geometry%20and%20demonstrates%20the%20impact%20of%0Achanges%20in%20representational%20structure%20on%20reconstruction%20performance.%0AParticularly%20emphasizes%20the%20necessity%20of%20understanding%20representations%20and%0Aincorporating%20representational%20constraints%2C%20providing%20empirical%20references%20for%0Adeveloping%20new%20interpretable%20tools%20and%20improving%20SAEs.%20The%20code%20is%20available%20at%0A%5Chyperlink%7Bhttps%3A//github.com/wenjie1835/SAERepGeo%7D%7Bhttps%3A//github.com/wenjie1835/SAERepGeo%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22506v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparsification%2520and%2520Reconstruction%2520from%2520the%2520Perspective%2520of%2520Representation%250A%2520%2520Geometry%26entry.906535625%3DWenjie%2520Sun%2520and%2520Bingzhe%2520Wu%2520and%2520Zhile%2520Yang%2520and%2520Chengke%2520Wu%26entry.1292438233%3D%2520%2520Sparse%2520Autoencoders%2520%2528SAEs%2529%2520have%2520emerged%2520as%2520a%2520predominant%2520tool%2520in%2520mechanistic%250Ainterpretability%252C%2520aiming%2520to%2520identify%2520interpretable%2520monosemantic%2520features.%250AHowever%252C%2520how%2520does%2520sparse%2520encoding%2520organize%2520the%2520representations%2520of%2520activation%250Avector%2520from%2520language%2520models%253F%2520What%2520is%2520the%2520relationship%2520between%2520this%250Aorganizational%2520paradigm%2520and%2520feature%2520disentanglement%2520as%2520well%2520as%2520reconstruction%250Aperformance%253F%2520To%2520address%2520these%2520questions%252C%2520we%2520propose%2520the%2520SAEMA%252C%2520which%2520validates%250Athe%2520stratified%2520structure%2520of%2520the%2520representation%2520by%2520observing%2520the%2520variability%2520of%250Athe%2520rank%2520of%2520the%2520symmetric%2520semipositive%2520definite%2520%2528SSPD%2529%2520matrix%2520corresponding%2520to%250Athe%2520modal%2520tensor%2520unfolded%2520along%2520the%2520latent%2520tensor%2520with%2520the%2520level%2520of%2520noise%2520added%250Ato%2520the%2520residual%2520stream.%2520To%2520systematically%2520investigate%2520how%2520sparse%2520encoding%250Aalters%2520representational%2520structures%252C%2520we%2520define%2520local%2520and%2520global%2520representations%252C%250Ademonstrating%2520that%2520they%2520amplify%2520inter-feature%2520distinctions%2520by%2520merging%2520similar%250Asemantic%2520features%2520and%2520introducing%2520additional%2520dimensionality.%2520Furthermore%252C%2520we%250Aintervene%2520the%2520global%2520representation%2520from%2520an%2520optimization%2520perspective%252C%2520proving%2520a%250Asignificant%2520causal%2520relationship%2520between%2520their%2520separability%2520and%2520the%250Areconstruction%2520performance.%2520This%2520study%2520explains%2520the%2520principles%2520of%2520sparsity%2520from%250Athe%2520perspective%2520of%2520representational%2520geometry%2520and%2520demonstrates%2520the%2520impact%2520of%250Achanges%2520in%2520representational%2520structure%2520on%2520reconstruction%2520performance.%250AParticularly%2520emphasizes%2520the%2520necessity%2520of%2520understanding%2520representations%2520and%250Aincorporating%2520representational%2520constraints%252C%2520providing%2520empirical%2520references%2520for%250Adeveloping%2520new%2520interpretable%2520tools%2520and%2520improving%2520SAEs.%2520The%2520code%2520is%2520available%2520at%250A%255Chyperlink%257Bhttps%253A//github.com/wenjie1835/SAERepGeo%257D%257Bhttps%253A//github.com/wenjie1835/SAERepGeo%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22506v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparsification%20and%20Reconstruction%20from%20the%20Perspective%20of%20Representation%0A%20%20Geometry&entry.906535625=Wenjie%20Sun%20and%20Bingzhe%20Wu%20and%20Zhile%20Yang%20and%20Chengke%20Wu&entry.1292438233=%20%20Sparse%20Autoencoders%20%28SAEs%29%20have%20emerged%20as%20a%20predominant%20tool%20in%20mechanistic%0Ainterpretability%2C%20aiming%20to%20identify%20interpretable%20monosemantic%20features.%0AHowever%2C%20how%20does%20sparse%20encoding%20organize%20the%20representations%20of%20activation%0Avector%20from%20language%20models%3F%20What%20is%20the%20relationship%20between%20this%0Aorganizational%20paradigm%20and%20feature%20disentanglement%20as%20well%20as%20reconstruction%0Aperformance%3F%20To%20address%20these%20questions%2C%20we%20propose%20the%20SAEMA%2C%20which%20validates%0Athe%20stratified%20structure%20of%20the%20representation%20by%20observing%20the%20variability%20of%0Athe%20rank%20of%20the%20symmetric%20semipositive%20definite%20%28SSPD%29%20matrix%20corresponding%20to%0Athe%20modal%20tensor%20unfolded%20along%20the%20latent%20tensor%20with%20the%20level%20of%20noise%20added%0Ato%20the%20residual%20stream.%20To%20systematically%20investigate%20how%20sparse%20encoding%0Aalters%20representational%20structures%2C%20we%20define%20local%20and%20global%20representations%2C%0Ademonstrating%20that%20they%20amplify%20inter-feature%20distinctions%20by%20merging%20similar%0Asemantic%20features%20and%20introducing%20additional%20dimensionality.%20Furthermore%2C%20we%0Aintervene%20the%20global%20representation%20from%20an%20optimization%20perspective%2C%20proving%20a%0Asignificant%20causal%20relationship%20between%20their%20separability%20and%20the%0Areconstruction%20performance.%20This%20study%20explains%20the%20principles%20of%20sparsity%20from%0Athe%20perspective%20of%20representational%20geometry%20and%20demonstrates%20the%20impact%20of%0Achanges%20in%20representational%20structure%20on%20reconstruction%20performance.%0AParticularly%20emphasizes%20the%20necessity%20of%20understanding%20representations%20and%0Aincorporating%20representational%20constraints%2C%20providing%20empirical%20references%20for%0Adeveloping%20new%20interpretable%20tools%20and%20improving%20SAEs.%20The%20code%20is%20available%20at%0A%5Chyperlink%7Bhttps%3A//github.com/wenjie1835/SAERepGeo%7D%7Bhttps%3A//github.com/wenjie1835/SAERepGeo%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22506v1&entry.124074799=Read"},
{"title": "PathFL: Multi-Alignment Federated Learning for Pathology Image\n  Segmentation", "author": "Yuan Zhang and Feng Chen and Yaolei Qi and Guanyu Yang and Huazhu Fu", "abstract": "  Pathology image segmentation across multiple centers encounters significant\nchallenges due to diverse sources of heterogeneity including imaging\nmodalities, organs, and scanning equipment, whose variability brings\nrepresentation bias and impedes the development of generalizable segmentation\nmodels. In this paper, we propose PathFL, a novel multi-alignment Federated\nLearning framework for pathology image segmentation that addresses these\nchallenges through three-level alignment strategies of image, feature, and\nmodel aggregation. Firstly, at the image level, a collaborative style\nenhancement module aligns and diversifies local data by facilitating style\ninformation exchange across clients. Secondly, at the feature level, an\nadaptive feature alignment module ensures implicit alignment in the\nrepresentation space by infusing local features with global insights, promoting\nconsistency across heterogeneous client features learning. Finally, at the\nmodel aggregation level, a stratified similarity aggregation strategy\nhierarchically aligns and aggregates models on the server, using layer-specific\nsimilarity to account for client discrepancies and enhance global\ngeneralization. Comprehensive evaluations on four sets of heterogeneous\npathology image datasets, encompassing cross-source, cross-modality,\ncross-organ, and cross-scanner variations, validate the effectiveness of our\nPathFL in achieving better performance and robustness against data\nheterogeneity.\n", "link": "http://arxiv.org/abs/2505.22522v1", "date": "2025-05-28", "relevancy": 2.705, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5583}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5351}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PathFL%3A%20Multi-Alignment%20Federated%20Learning%20for%20Pathology%20Image%0A%20%20Segmentation&body=Title%3A%20PathFL%3A%20Multi-Alignment%20Federated%20Learning%20for%20Pathology%20Image%0A%20%20Segmentation%0AAuthor%3A%20Yuan%20Zhang%20and%20Feng%20Chen%20and%20Yaolei%20Qi%20and%20Guanyu%20Yang%20and%20Huazhu%20Fu%0AAbstract%3A%20%20%20Pathology%20image%20segmentation%20across%20multiple%20centers%20encounters%20significant%0Achallenges%20due%20to%20diverse%20sources%20of%20heterogeneity%20including%20imaging%0Amodalities%2C%20organs%2C%20and%20scanning%20equipment%2C%20whose%20variability%20brings%0Arepresentation%20bias%20and%20impedes%20the%20development%20of%20generalizable%20segmentation%0Amodels.%20In%20this%20paper%2C%20we%20propose%20PathFL%2C%20a%20novel%20multi-alignment%20Federated%0ALearning%20framework%20for%20pathology%20image%20segmentation%20that%20addresses%20these%0Achallenges%20through%20three-level%20alignment%20strategies%20of%20image%2C%20feature%2C%20and%0Amodel%20aggregation.%20Firstly%2C%20at%20the%20image%20level%2C%20a%20collaborative%20style%0Aenhancement%20module%20aligns%20and%20diversifies%20local%20data%20by%20facilitating%20style%0Ainformation%20exchange%20across%20clients.%20Secondly%2C%20at%20the%20feature%20level%2C%20an%0Aadaptive%20feature%20alignment%20module%20ensures%20implicit%20alignment%20in%20the%0Arepresentation%20space%20by%20infusing%20local%20features%20with%20global%20insights%2C%20promoting%0Aconsistency%20across%20heterogeneous%20client%20features%20learning.%20Finally%2C%20at%20the%0Amodel%20aggregation%20level%2C%20a%20stratified%20similarity%20aggregation%20strategy%0Ahierarchically%20aligns%20and%20aggregates%20models%20on%20the%20server%2C%20using%20layer-specific%0Asimilarity%20to%20account%20for%20client%20discrepancies%20and%20enhance%20global%0Ageneralization.%20Comprehensive%20evaluations%20on%20four%20sets%20of%20heterogeneous%0Apathology%20image%20datasets%2C%20encompassing%20cross-source%2C%20cross-modality%2C%0Across-organ%2C%20and%20cross-scanner%20variations%2C%20validate%20the%20effectiveness%20of%20our%0APathFL%20in%20achieving%20better%20performance%20and%20robustness%20against%20data%0Aheterogeneity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22522v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPathFL%253A%2520Multi-Alignment%2520Federated%2520Learning%2520for%2520Pathology%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DYuan%2520Zhang%2520and%2520Feng%2520Chen%2520and%2520Yaolei%2520Qi%2520and%2520Guanyu%2520Yang%2520and%2520Huazhu%2520Fu%26entry.1292438233%3D%2520%2520Pathology%2520image%2520segmentation%2520across%2520multiple%2520centers%2520encounters%2520significant%250Achallenges%2520due%2520to%2520diverse%2520sources%2520of%2520heterogeneity%2520including%2520imaging%250Amodalities%252C%2520organs%252C%2520and%2520scanning%2520equipment%252C%2520whose%2520variability%2520brings%250Arepresentation%2520bias%2520and%2520impedes%2520the%2520development%2520of%2520generalizable%2520segmentation%250Amodels.%2520In%2520this%2520paper%252C%2520we%2520propose%2520PathFL%252C%2520a%2520novel%2520multi-alignment%2520Federated%250ALearning%2520framework%2520for%2520pathology%2520image%2520segmentation%2520that%2520addresses%2520these%250Achallenges%2520through%2520three-level%2520alignment%2520strategies%2520of%2520image%252C%2520feature%252C%2520and%250Amodel%2520aggregation.%2520Firstly%252C%2520at%2520the%2520image%2520level%252C%2520a%2520collaborative%2520style%250Aenhancement%2520module%2520aligns%2520and%2520diversifies%2520local%2520data%2520by%2520facilitating%2520style%250Ainformation%2520exchange%2520across%2520clients.%2520Secondly%252C%2520at%2520the%2520feature%2520level%252C%2520an%250Aadaptive%2520feature%2520alignment%2520module%2520ensures%2520implicit%2520alignment%2520in%2520the%250Arepresentation%2520space%2520by%2520infusing%2520local%2520features%2520with%2520global%2520insights%252C%2520promoting%250Aconsistency%2520across%2520heterogeneous%2520client%2520features%2520learning.%2520Finally%252C%2520at%2520the%250Amodel%2520aggregation%2520level%252C%2520a%2520stratified%2520similarity%2520aggregation%2520strategy%250Ahierarchically%2520aligns%2520and%2520aggregates%2520models%2520on%2520the%2520server%252C%2520using%2520layer-specific%250Asimilarity%2520to%2520account%2520for%2520client%2520discrepancies%2520and%2520enhance%2520global%250Ageneralization.%2520Comprehensive%2520evaluations%2520on%2520four%2520sets%2520of%2520heterogeneous%250Apathology%2520image%2520datasets%252C%2520encompassing%2520cross-source%252C%2520cross-modality%252C%250Across-organ%252C%2520and%2520cross-scanner%2520variations%252C%2520validate%2520the%2520effectiveness%2520of%2520our%250APathFL%2520in%2520achieving%2520better%2520performance%2520and%2520robustness%2520against%2520data%250Aheterogeneity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22522v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PathFL%3A%20Multi-Alignment%20Federated%20Learning%20for%20Pathology%20Image%0A%20%20Segmentation&entry.906535625=Yuan%20Zhang%20and%20Feng%20Chen%20and%20Yaolei%20Qi%20and%20Guanyu%20Yang%20and%20Huazhu%20Fu&entry.1292438233=%20%20Pathology%20image%20segmentation%20across%20multiple%20centers%20encounters%20significant%0Achallenges%20due%20to%20diverse%20sources%20of%20heterogeneity%20including%20imaging%0Amodalities%2C%20organs%2C%20and%20scanning%20equipment%2C%20whose%20variability%20brings%0Arepresentation%20bias%20and%20impedes%20the%20development%20of%20generalizable%20segmentation%0Amodels.%20In%20this%20paper%2C%20we%20propose%20PathFL%2C%20a%20novel%20multi-alignment%20Federated%0ALearning%20framework%20for%20pathology%20image%20segmentation%20that%20addresses%20these%0Achallenges%20through%20three-level%20alignment%20strategies%20of%20image%2C%20feature%2C%20and%0Amodel%20aggregation.%20Firstly%2C%20at%20the%20image%20level%2C%20a%20collaborative%20style%0Aenhancement%20module%20aligns%20and%20diversifies%20local%20data%20by%20facilitating%20style%0Ainformation%20exchange%20across%20clients.%20Secondly%2C%20at%20the%20feature%20level%2C%20an%0Aadaptive%20feature%20alignment%20module%20ensures%20implicit%20alignment%20in%20the%0Arepresentation%20space%20by%20infusing%20local%20features%20with%20global%20insights%2C%20promoting%0Aconsistency%20across%20heterogeneous%20client%20features%20learning.%20Finally%2C%20at%20the%0Amodel%20aggregation%20level%2C%20a%20stratified%20similarity%20aggregation%20strategy%0Ahierarchically%20aligns%20and%20aggregates%20models%20on%20the%20server%2C%20using%20layer-specific%0Asimilarity%20to%20account%20for%20client%20discrepancies%20and%20enhance%20global%0Ageneralization.%20Comprehensive%20evaluations%20on%20four%20sets%20of%20heterogeneous%0Apathology%20image%20datasets%2C%20encompassing%20cross-source%2C%20cross-modality%2C%0Across-organ%2C%20and%20cross-scanner%20variations%2C%20validate%20the%20effectiveness%20of%20our%0APathFL%20in%20achieving%20better%20performance%20and%20robustness%20against%20data%0Aheterogeneity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22522v1&entry.124074799=Read"},
{"title": "MagicTryOn: Harnessing Diffusion Transformer for Garment-Preserving\n  Video Virtual Try-on", "author": "Guangyuan Li and Siming Zheng and Hao Zhang and Jinwei Chen and Junsheng Luan and Binkai Ou and Lei Zhao and Bo Li and Peng-Tao Jiang", "abstract": "  Video Virtual Try-On (VVT) aims to simulate the natural appearance of\ngarments across consecutive video frames, capturing their dynamic variations\nand interactions with human body motion. However, current VVT methods still\nface challenges in terms of spatiotemporal consistency and garment content\npreservation. First, they use diffusion models based on the U-Net, which are\nlimited in their expressive capability and struggle to reconstruct complex\ndetails. Second, they adopt a separative modeling approach for spatial and\ntemporal attention, which hinders the effective capture of structural\nrelationships and dynamic consistency across frames. Third, their expression of\ngarment details remains insufficient, affecting the realism and stability of\nthe overall synthesized results, especially during human motion. To address the\nabove challenges, we propose MagicTryOn, a video virtual try-on framework built\nupon the large-scale video diffusion Transformer. We replace the U-Net\narchitecture with a diffusion Transformer and combine full self-attention to\njointly model the spatiotemporal consistency of videos. We design a\ncoarse-to-fine garment preservation strategy. The coarse strategy integrates\ngarment tokens during the embedding stage, while the fine strategy incorporates\nmultiple garment-based conditions, such as semantics, textures, and contour\nlines during the denoising stage. Moreover, we introduce a mask-aware loss to\nfurther optimize garment region fidelity. Extensive experiments on both image\nand video try-on datasets demonstrate that our method outperforms existing SOTA\nmethods in comprehensive evaluations and generalizes to in-the-wild scenarios.\n", "link": "http://arxiv.org/abs/2505.21325v2", "date": "2025-05-28", "relevancy": 2.7001, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6784}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6763}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MagicTryOn%3A%20Harnessing%20Diffusion%20Transformer%20for%20Garment-Preserving%0A%20%20Video%20Virtual%20Try-on&body=Title%3A%20MagicTryOn%3A%20Harnessing%20Diffusion%20Transformer%20for%20Garment-Preserving%0A%20%20Video%20Virtual%20Try-on%0AAuthor%3A%20Guangyuan%20Li%20and%20Siming%20Zheng%20and%20Hao%20Zhang%20and%20Jinwei%20Chen%20and%20Junsheng%20Luan%20and%20Binkai%20Ou%20and%20Lei%20Zhao%20and%20Bo%20Li%20and%20Peng-Tao%20Jiang%0AAbstract%3A%20%20%20Video%20Virtual%20Try-On%20%28VVT%29%20aims%20to%20simulate%20the%20natural%20appearance%20of%0Agarments%20across%20consecutive%20video%20frames%2C%20capturing%20their%20dynamic%20variations%0Aand%20interactions%20with%20human%20body%20motion.%20However%2C%20current%20VVT%20methods%20still%0Aface%20challenges%20in%20terms%20of%20spatiotemporal%20consistency%20and%20garment%20content%0Apreservation.%20First%2C%20they%20use%20diffusion%20models%20based%20on%20the%20U-Net%2C%20which%20are%0Alimited%20in%20their%20expressive%20capability%20and%20struggle%20to%20reconstruct%20complex%0Adetails.%20Second%2C%20they%20adopt%20a%20separative%20modeling%20approach%20for%20spatial%20and%0Atemporal%20attention%2C%20which%20hinders%20the%20effective%20capture%20of%20structural%0Arelationships%20and%20dynamic%20consistency%20across%20frames.%20Third%2C%20their%20expression%20of%0Agarment%20details%20remains%20insufficient%2C%20affecting%20the%20realism%20and%20stability%20of%0Athe%20overall%20synthesized%20results%2C%20especially%20during%20human%20motion.%20To%20address%20the%0Aabove%20challenges%2C%20we%20propose%20MagicTryOn%2C%20a%20video%20virtual%20try-on%20framework%20built%0Aupon%20the%20large-scale%20video%20diffusion%20Transformer.%20We%20replace%20the%20U-Net%0Aarchitecture%20with%20a%20diffusion%20Transformer%20and%20combine%20full%20self-attention%20to%0Ajointly%20model%20the%20spatiotemporal%20consistency%20of%20videos.%20We%20design%20a%0Acoarse-to-fine%20garment%20preservation%20strategy.%20The%20coarse%20strategy%20integrates%0Agarment%20tokens%20during%20the%20embedding%20stage%2C%20while%20the%20fine%20strategy%20incorporates%0Amultiple%20garment-based%20conditions%2C%20such%20as%20semantics%2C%20textures%2C%20and%20contour%0Alines%20during%20the%20denoising%20stage.%20Moreover%2C%20we%20introduce%20a%20mask-aware%20loss%20to%0Afurther%20optimize%20garment%20region%20fidelity.%20Extensive%20experiments%20on%20both%20image%0Aand%20video%20try-on%20datasets%20demonstrate%20that%20our%20method%20outperforms%20existing%20SOTA%0Amethods%20in%20comprehensive%20evaluations%20and%20generalizes%20to%20in-the-wild%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21325v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMagicTryOn%253A%2520Harnessing%2520Diffusion%2520Transformer%2520for%2520Garment-Preserving%250A%2520%2520Video%2520Virtual%2520Try-on%26entry.906535625%3DGuangyuan%2520Li%2520and%2520Siming%2520Zheng%2520and%2520Hao%2520Zhang%2520and%2520Jinwei%2520Chen%2520and%2520Junsheng%2520Luan%2520and%2520Binkai%2520Ou%2520and%2520Lei%2520Zhao%2520and%2520Bo%2520Li%2520and%2520Peng-Tao%2520Jiang%26entry.1292438233%3D%2520%2520Video%2520Virtual%2520Try-On%2520%2528VVT%2529%2520aims%2520to%2520simulate%2520the%2520natural%2520appearance%2520of%250Agarments%2520across%2520consecutive%2520video%2520frames%252C%2520capturing%2520their%2520dynamic%2520variations%250Aand%2520interactions%2520with%2520human%2520body%2520motion.%2520However%252C%2520current%2520VVT%2520methods%2520still%250Aface%2520challenges%2520in%2520terms%2520of%2520spatiotemporal%2520consistency%2520and%2520garment%2520content%250Apreservation.%2520First%252C%2520they%2520use%2520diffusion%2520models%2520based%2520on%2520the%2520U-Net%252C%2520which%2520are%250Alimited%2520in%2520their%2520expressive%2520capability%2520and%2520struggle%2520to%2520reconstruct%2520complex%250Adetails.%2520Second%252C%2520they%2520adopt%2520a%2520separative%2520modeling%2520approach%2520for%2520spatial%2520and%250Atemporal%2520attention%252C%2520which%2520hinders%2520the%2520effective%2520capture%2520of%2520structural%250Arelationships%2520and%2520dynamic%2520consistency%2520across%2520frames.%2520Third%252C%2520their%2520expression%2520of%250Agarment%2520details%2520remains%2520insufficient%252C%2520affecting%2520the%2520realism%2520and%2520stability%2520of%250Athe%2520overall%2520synthesized%2520results%252C%2520especially%2520during%2520human%2520motion.%2520To%2520address%2520the%250Aabove%2520challenges%252C%2520we%2520propose%2520MagicTryOn%252C%2520a%2520video%2520virtual%2520try-on%2520framework%2520built%250Aupon%2520the%2520large-scale%2520video%2520diffusion%2520Transformer.%2520We%2520replace%2520the%2520U-Net%250Aarchitecture%2520with%2520a%2520diffusion%2520Transformer%2520and%2520combine%2520full%2520self-attention%2520to%250Ajointly%2520model%2520the%2520spatiotemporal%2520consistency%2520of%2520videos.%2520We%2520design%2520a%250Acoarse-to-fine%2520garment%2520preservation%2520strategy.%2520The%2520coarse%2520strategy%2520integrates%250Agarment%2520tokens%2520during%2520the%2520embedding%2520stage%252C%2520while%2520the%2520fine%2520strategy%2520incorporates%250Amultiple%2520garment-based%2520conditions%252C%2520such%2520as%2520semantics%252C%2520textures%252C%2520and%2520contour%250Alines%2520during%2520the%2520denoising%2520stage.%2520Moreover%252C%2520we%2520introduce%2520a%2520mask-aware%2520loss%2520to%250Afurther%2520optimize%2520garment%2520region%2520fidelity.%2520Extensive%2520experiments%2520on%2520both%2520image%250Aand%2520video%2520try-on%2520datasets%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520existing%2520SOTA%250Amethods%2520in%2520comprehensive%2520evaluations%2520and%2520generalizes%2520to%2520in-the-wild%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21325v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MagicTryOn%3A%20Harnessing%20Diffusion%20Transformer%20for%20Garment-Preserving%0A%20%20Video%20Virtual%20Try-on&entry.906535625=Guangyuan%20Li%20and%20Siming%20Zheng%20and%20Hao%20Zhang%20and%20Jinwei%20Chen%20and%20Junsheng%20Luan%20and%20Binkai%20Ou%20and%20Lei%20Zhao%20and%20Bo%20Li%20and%20Peng-Tao%20Jiang&entry.1292438233=%20%20Video%20Virtual%20Try-On%20%28VVT%29%20aims%20to%20simulate%20the%20natural%20appearance%20of%0Agarments%20across%20consecutive%20video%20frames%2C%20capturing%20their%20dynamic%20variations%0Aand%20interactions%20with%20human%20body%20motion.%20However%2C%20current%20VVT%20methods%20still%0Aface%20challenges%20in%20terms%20of%20spatiotemporal%20consistency%20and%20garment%20content%0Apreservation.%20First%2C%20they%20use%20diffusion%20models%20based%20on%20the%20U-Net%2C%20which%20are%0Alimited%20in%20their%20expressive%20capability%20and%20struggle%20to%20reconstruct%20complex%0Adetails.%20Second%2C%20they%20adopt%20a%20separative%20modeling%20approach%20for%20spatial%20and%0Atemporal%20attention%2C%20which%20hinders%20the%20effective%20capture%20of%20structural%0Arelationships%20and%20dynamic%20consistency%20across%20frames.%20Third%2C%20their%20expression%20of%0Agarment%20details%20remains%20insufficient%2C%20affecting%20the%20realism%20and%20stability%20of%0Athe%20overall%20synthesized%20results%2C%20especially%20during%20human%20motion.%20To%20address%20the%0Aabove%20challenges%2C%20we%20propose%20MagicTryOn%2C%20a%20video%20virtual%20try-on%20framework%20built%0Aupon%20the%20large-scale%20video%20diffusion%20Transformer.%20We%20replace%20the%20U-Net%0Aarchitecture%20with%20a%20diffusion%20Transformer%20and%20combine%20full%20self-attention%20to%0Ajointly%20model%20the%20spatiotemporal%20consistency%20of%20videos.%20We%20design%20a%0Acoarse-to-fine%20garment%20preservation%20strategy.%20The%20coarse%20strategy%20integrates%0Agarment%20tokens%20during%20the%20embedding%20stage%2C%20while%20the%20fine%20strategy%20incorporates%0Amultiple%20garment-based%20conditions%2C%20such%20as%20semantics%2C%20textures%2C%20and%20contour%0Alines%20during%20the%20denoising%20stage.%20Moreover%2C%20we%20introduce%20a%20mask-aware%20loss%20to%0Afurther%20optimize%20garment%20region%20fidelity.%20Extensive%20experiments%20on%20both%20image%0Aand%20video%20try-on%20datasets%20demonstrate%20that%20our%20method%20outperforms%20existing%20SOTA%0Amethods%20in%20comprehensive%20evaluations%20and%20generalizes%20to%20in-the-wild%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21325v2&entry.124074799=Read"},
{"title": "VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on\n  Multimodal Tool Use", "author": "Mingyuan Wu and Jingcheng Yang and Jize Jiang and Meitang Li and Kaizhuo Yan and Hanchao Yu and Minjia Zhang and Chengxiang Zhai and Klara Nahrstedt", "abstract": "  Reinforcement Learning Finetuning (RFT) has significantly advanced the\nreasoning capabilities of large language models (LLMs) by enabling long chains\nof thought, self-correction, and effective tool use. While recent works attempt\nto extend RFT to vision-language models (VLMs), these efforts largely produce\ntext-only reasoning conditioned on static image inputs, falling short of true\nmultimodal reasoning in the response. In contrast, test-time methods like\nVisual Sketchpad incorporate visual steps but lack training mechanisms.\n  We introduce VTool-R1, the first framework that trains VLMs to generate\nmultimodal chains of thought by interleaving text and intermediate visual\nreasoning steps. VTool-R1 integrates Python-based visual editing tools into the\nRFT process, enabling VLMs to learn when and how to generate visual reasoning\nsteps that benefit final reasoning. Trained with outcome-based rewards tied to\ntask accuracy, our approach elicits strategic visual tool use for reasoning\nwithout relying on process-based supervision. Experiments on structured visual\nquestion answering over charts and tables show that VTool-R1 enhances reasoning\nperformance by teaching VLMs to \"think with images\" and generate multimodal\nchain of thoughts with tools.\n", "link": "http://arxiv.org/abs/2505.19255v2", "date": "2025-05-28", "relevancy": 2.6798, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5529}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5275}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VTool-R1%3A%20VLMs%20Learn%20to%20Think%20with%20Images%20via%20Reinforcement%20Learning%20on%0A%20%20Multimodal%20Tool%20Use&body=Title%3A%20VTool-R1%3A%20VLMs%20Learn%20to%20Think%20with%20Images%20via%20Reinforcement%20Learning%20on%0A%20%20Multimodal%20Tool%20Use%0AAuthor%3A%20Mingyuan%20Wu%20and%20Jingcheng%20Yang%20and%20Jize%20Jiang%20and%20Meitang%20Li%20and%20Kaizhuo%20Yan%20and%20Hanchao%20Yu%20and%20Minjia%20Zhang%20and%20Chengxiang%20Zhai%20and%20Klara%20Nahrstedt%0AAbstract%3A%20%20%20Reinforcement%20Learning%20Finetuning%20%28RFT%29%20has%20significantly%20advanced%20the%0Areasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20by%20enabling%20long%20chains%0Aof%20thought%2C%20self-correction%2C%20and%20effective%20tool%20use.%20While%20recent%20works%20attempt%0Ato%20extend%20RFT%20to%20vision-language%20models%20%28VLMs%29%2C%20these%20efforts%20largely%20produce%0Atext-only%20reasoning%20conditioned%20on%20static%20image%20inputs%2C%20falling%20short%20of%20true%0Amultimodal%20reasoning%20in%20the%20response.%20In%20contrast%2C%20test-time%20methods%20like%0AVisual%20Sketchpad%20incorporate%20visual%20steps%20but%20lack%20training%20mechanisms.%0A%20%20We%20introduce%20VTool-R1%2C%20the%20first%20framework%20that%20trains%20VLMs%20to%20generate%0Amultimodal%20chains%20of%20thought%20by%20interleaving%20text%20and%20intermediate%20visual%0Areasoning%20steps.%20VTool-R1%20integrates%20Python-based%20visual%20editing%20tools%20into%20the%0ARFT%20process%2C%20enabling%20VLMs%20to%20learn%20when%20and%20how%20to%20generate%20visual%20reasoning%0Asteps%20that%20benefit%20final%20reasoning.%20Trained%20with%20outcome-based%20rewards%20tied%20to%0Atask%20accuracy%2C%20our%20approach%20elicits%20strategic%20visual%20tool%20use%20for%20reasoning%0Awithout%20relying%20on%20process-based%20supervision.%20Experiments%20on%20structured%20visual%0Aquestion%20answering%20over%20charts%20and%20tables%20show%20that%20VTool-R1%20enhances%20reasoning%0Aperformance%20by%20teaching%20VLMs%20to%20%22think%20with%20images%22%20and%20generate%20multimodal%0Achain%20of%20thoughts%20with%20tools.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19255v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVTool-R1%253A%2520VLMs%2520Learn%2520to%2520Think%2520with%2520Images%2520via%2520Reinforcement%2520Learning%2520on%250A%2520%2520Multimodal%2520Tool%2520Use%26entry.906535625%3DMingyuan%2520Wu%2520and%2520Jingcheng%2520Yang%2520and%2520Jize%2520Jiang%2520and%2520Meitang%2520Li%2520and%2520Kaizhuo%2520Yan%2520and%2520Hanchao%2520Yu%2520and%2520Minjia%2520Zhang%2520and%2520Chengxiang%2520Zhai%2520and%2520Klara%2520Nahrstedt%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520Finetuning%2520%2528RFT%2529%2520has%2520significantly%2520advanced%2520the%250Areasoning%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520by%2520enabling%2520long%2520chains%250Aof%2520thought%252C%2520self-correction%252C%2520and%2520effective%2520tool%2520use.%2520While%2520recent%2520works%2520attempt%250Ato%2520extend%2520RFT%2520to%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520these%2520efforts%2520largely%2520produce%250Atext-only%2520reasoning%2520conditioned%2520on%2520static%2520image%2520inputs%252C%2520falling%2520short%2520of%2520true%250Amultimodal%2520reasoning%2520in%2520the%2520response.%2520In%2520contrast%252C%2520test-time%2520methods%2520like%250AVisual%2520Sketchpad%2520incorporate%2520visual%2520steps%2520but%2520lack%2520training%2520mechanisms.%250A%2520%2520We%2520introduce%2520VTool-R1%252C%2520the%2520first%2520framework%2520that%2520trains%2520VLMs%2520to%2520generate%250Amultimodal%2520chains%2520of%2520thought%2520by%2520interleaving%2520text%2520and%2520intermediate%2520visual%250Areasoning%2520steps.%2520VTool-R1%2520integrates%2520Python-based%2520visual%2520editing%2520tools%2520into%2520the%250ARFT%2520process%252C%2520enabling%2520VLMs%2520to%2520learn%2520when%2520and%2520how%2520to%2520generate%2520visual%2520reasoning%250Asteps%2520that%2520benefit%2520final%2520reasoning.%2520Trained%2520with%2520outcome-based%2520rewards%2520tied%2520to%250Atask%2520accuracy%252C%2520our%2520approach%2520elicits%2520strategic%2520visual%2520tool%2520use%2520for%2520reasoning%250Awithout%2520relying%2520on%2520process-based%2520supervision.%2520Experiments%2520on%2520structured%2520visual%250Aquestion%2520answering%2520over%2520charts%2520and%2520tables%2520show%2520that%2520VTool-R1%2520enhances%2520reasoning%250Aperformance%2520by%2520teaching%2520VLMs%2520to%2520%2522think%2520with%2520images%2522%2520and%2520generate%2520multimodal%250Achain%2520of%2520thoughts%2520with%2520tools.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19255v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VTool-R1%3A%20VLMs%20Learn%20to%20Think%20with%20Images%20via%20Reinforcement%20Learning%20on%0A%20%20Multimodal%20Tool%20Use&entry.906535625=Mingyuan%20Wu%20and%20Jingcheng%20Yang%20and%20Jize%20Jiang%20and%20Meitang%20Li%20and%20Kaizhuo%20Yan%20and%20Hanchao%20Yu%20and%20Minjia%20Zhang%20and%20Chengxiang%20Zhai%20and%20Klara%20Nahrstedt&entry.1292438233=%20%20Reinforcement%20Learning%20Finetuning%20%28RFT%29%20has%20significantly%20advanced%20the%0Areasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20by%20enabling%20long%20chains%0Aof%20thought%2C%20self-correction%2C%20and%20effective%20tool%20use.%20While%20recent%20works%20attempt%0Ato%20extend%20RFT%20to%20vision-language%20models%20%28VLMs%29%2C%20these%20efforts%20largely%20produce%0Atext-only%20reasoning%20conditioned%20on%20static%20image%20inputs%2C%20falling%20short%20of%20true%0Amultimodal%20reasoning%20in%20the%20response.%20In%20contrast%2C%20test-time%20methods%20like%0AVisual%20Sketchpad%20incorporate%20visual%20steps%20but%20lack%20training%20mechanisms.%0A%20%20We%20introduce%20VTool-R1%2C%20the%20first%20framework%20that%20trains%20VLMs%20to%20generate%0Amultimodal%20chains%20of%20thought%20by%20interleaving%20text%20and%20intermediate%20visual%0Areasoning%20steps.%20VTool-R1%20integrates%20Python-based%20visual%20editing%20tools%20into%20the%0ARFT%20process%2C%20enabling%20VLMs%20to%20learn%20when%20and%20how%20to%20generate%20visual%20reasoning%0Asteps%20that%20benefit%20final%20reasoning.%20Trained%20with%20outcome-based%20rewards%20tied%20to%0Atask%20accuracy%2C%20our%20approach%20elicits%20strategic%20visual%20tool%20use%20for%20reasoning%0Awithout%20relying%20on%20process-based%20supervision.%20Experiments%20on%20structured%20visual%0Aquestion%20answering%20over%20charts%20and%20tables%20show%20that%20VTool-R1%20enhances%20reasoning%0Aperformance%20by%20teaching%20VLMs%20to%20%22think%20with%20images%22%20and%20generate%20multimodal%0Achain%20of%20thoughts%20with%20tools.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19255v2&entry.124074799=Read"},
{"title": "IKIWISI: An Interactive Visual Pattern Generator for Evaluating the\n  Reliability of Vision-Language Models Without Ground Truth", "author": "Md Touhidul Islam and Imran Kabir and Md Alimoor Reza and Syed Masum Billah", "abstract": "  We present IKIWISI (\"I Know It When I See It\"), an interactive visual pattern\ngenerator for assessing vision-language models in video object recognition when\nground truth is unavailable. IKIWISI transforms model outputs into a binary\nheatmap where green cells indicate object presence and red cells indicate\nobject absence. This visualization leverages humans' innate pattern recognition\nabilities to evaluate model reliability. IKIWISI introduces \"spy objects\":\nadversarial instances users know are absent, to discern models hallucinating on\nnonexistent items. The tool functions as a cognitive audit mechanism, surfacing\nmismatches between human and machine perception by visualizing where models\ndiverge from human understanding.\n  Our study with 15 participants found that users considered IKIWISI easy to\nuse, made assessments that correlated with objective metrics when available,\nand reached informed conclusions by examining only a small fraction of heatmap\ncells. This approach not only complements traditional evaluation methods\nthrough visual assessment of model behavior with custom object sets, but also\nreveals opportunities for improving alignment between human perception and\nmachine understanding in vision-language systems.\n", "link": "http://arxiv.org/abs/2505.22305v1", "date": "2025-05-28", "relevancy": 2.6671, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5406}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5303}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IKIWISI%3A%20An%20Interactive%20Visual%20Pattern%20Generator%20for%20Evaluating%20the%0A%20%20Reliability%20of%20Vision-Language%20Models%20Without%20Ground%20Truth&body=Title%3A%20IKIWISI%3A%20An%20Interactive%20Visual%20Pattern%20Generator%20for%20Evaluating%20the%0A%20%20Reliability%20of%20Vision-Language%20Models%20Without%20Ground%20Truth%0AAuthor%3A%20Md%20Touhidul%20Islam%20and%20Imran%20Kabir%20and%20Md%20Alimoor%20Reza%20and%20Syed%20Masum%20Billah%0AAbstract%3A%20%20%20We%20present%20IKIWISI%20%28%22I%20Know%20It%20When%20I%20See%20It%22%29%2C%20an%20interactive%20visual%20pattern%0Agenerator%20for%20assessing%20vision-language%20models%20in%20video%20object%20recognition%20when%0Aground%20truth%20is%20unavailable.%20IKIWISI%20transforms%20model%20outputs%20into%20a%20binary%0Aheatmap%20where%20green%20cells%20indicate%20object%20presence%20and%20red%20cells%20indicate%0Aobject%20absence.%20This%20visualization%20leverages%20humans%27%20innate%20pattern%20recognition%0Aabilities%20to%20evaluate%20model%20reliability.%20IKIWISI%20introduces%20%22spy%20objects%22%3A%0Aadversarial%20instances%20users%20know%20are%20absent%2C%20to%20discern%20models%20hallucinating%20on%0Anonexistent%20items.%20The%20tool%20functions%20as%20a%20cognitive%20audit%20mechanism%2C%20surfacing%0Amismatches%20between%20human%20and%20machine%20perception%20by%20visualizing%20where%20models%0Adiverge%20from%20human%20understanding.%0A%20%20Our%20study%20with%2015%20participants%20found%20that%20users%20considered%20IKIWISI%20easy%20to%0Ause%2C%20made%20assessments%20that%20correlated%20with%20objective%20metrics%20when%20available%2C%0Aand%20reached%20informed%20conclusions%20by%20examining%20only%20a%20small%20fraction%20of%20heatmap%0Acells.%20This%20approach%20not%20only%20complements%20traditional%20evaluation%20methods%0Athrough%20visual%20assessment%20of%20model%20behavior%20with%20custom%20object%20sets%2C%20but%20also%0Areveals%20opportunities%20for%20improving%20alignment%20between%20human%20perception%20and%0Amachine%20understanding%20in%20vision-language%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22305v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIKIWISI%253A%2520An%2520Interactive%2520Visual%2520Pattern%2520Generator%2520for%2520Evaluating%2520the%250A%2520%2520Reliability%2520of%2520Vision-Language%2520Models%2520Without%2520Ground%2520Truth%26entry.906535625%3DMd%2520Touhidul%2520Islam%2520and%2520Imran%2520Kabir%2520and%2520Md%2520Alimoor%2520Reza%2520and%2520Syed%2520Masum%2520Billah%26entry.1292438233%3D%2520%2520We%2520present%2520IKIWISI%2520%2528%2522I%2520Know%2520It%2520When%2520I%2520See%2520It%2522%2529%252C%2520an%2520interactive%2520visual%2520pattern%250Agenerator%2520for%2520assessing%2520vision-language%2520models%2520in%2520video%2520object%2520recognition%2520when%250Aground%2520truth%2520is%2520unavailable.%2520IKIWISI%2520transforms%2520model%2520outputs%2520into%2520a%2520binary%250Aheatmap%2520where%2520green%2520cells%2520indicate%2520object%2520presence%2520and%2520red%2520cells%2520indicate%250Aobject%2520absence.%2520This%2520visualization%2520leverages%2520humans%2527%2520innate%2520pattern%2520recognition%250Aabilities%2520to%2520evaluate%2520model%2520reliability.%2520IKIWISI%2520introduces%2520%2522spy%2520objects%2522%253A%250Aadversarial%2520instances%2520users%2520know%2520are%2520absent%252C%2520to%2520discern%2520models%2520hallucinating%2520on%250Anonexistent%2520items.%2520The%2520tool%2520functions%2520as%2520a%2520cognitive%2520audit%2520mechanism%252C%2520surfacing%250Amismatches%2520between%2520human%2520and%2520machine%2520perception%2520by%2520visualizing%2520where%2520models%250Adiverge%2520from%2520human%2520understanding.%250A%2520%2520Our%2520study%2520with%252015%2520participants%2520found%2520that%2520users%2520considered%2520IKIWISI%2520easy%2520to%250Ause%252C%2520made%2520assessments%2520that%2520correlated%2520with%2520objective%2520metrics%2520when%2520available%252C%250Aand%2520reached%2520informed%2520conclusions%2520by%2520examining%2520only%2520a%2520small%2520fraction%2520of%2520heatmap%250Acells.%2520This%2520approach%2520not%2520only%2520complements%2520traditional%2520evaluation%2520methods%250Athrough%2520visual%2520assessment%2520of%2520model%2520behavior%2520with%2520custom%2520object%2520sets%252C%2520but%2520also%250Areveals%2520opportunities%2520for%2520improving%2520alignment%2520between%2520human%2520perception%2520and%250Amachine%2520understanding%2520in%2520vision-language%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22305v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IKIWISI%3A%20An%20Interactive%20Visual%20Pattern%20Generator%20for%20Evaluating%20the%0A%20%20Reliability%20of%20Vision-Language%20Models%20Without%20Ground%20Truth&entry.906535625=Md%20Touhidul%20Islam%20and%20Imran%20Kabir%20and%20Md%20Alimoor%20Reza%20and%20Syed%20Masum%20Billah&entry.1292438233=%20%20We%20present%20IKIWISI%20%28%22I%20Know%20It%20When%20I%20See%20It%22%29%2C%20an%20interactive%20visual%20pattern%0Agenerator%20for%20assessing%20vision-language%20models%20in%20video%20object%20recognition%20when%0Aground%20truth%20is%20unavailable.%20IKIWISI%20transforms%20model%20outputs%20into%20a%20binary%0Aheatmap%20where%20green%20cells%20indicate%20object%20presence%20and%20red%20cells%20indicate%0Aobject%20absence.%20This%20visualization%20leverages%20humans%27%20innate%20pattern%20recognition%0Aabilities%20to%20evaluate%20model%20reliability.%20IKIWISI%20introduces%20%22spy%20objects%22%3A%0Aadversarial%20instances%20users%20know%20are%20absent%2C%20to%20discern%20models%20hallucinating%20on%0Anonexistent%20items.%20The%20tool%20functions%20as%20a%20cognitive%20audit%20mechanism%2C%20surfacing%0Amismatches%20between%20human%20and%20machine%20perception%20by%20visualizing%20where%20models%0Adiverge%20from%20human%20understanding.%0A%20%20Our%20study%20with%2015%20participants%20found%20that%20users%20considered%20IKIWISI%20easy%20to%0Ause%2C%20made%20assessments%20that%20correlated%20with%20objective%20metrics%20when%20available%2C%0Aand%20reached%20informed%20conclusions%20by%20examining%20only%20a%20small%20fraction%20of%20heatmap%0Acells.%20This%20approach%20not%20only%20complements%20traditional%20evaluation%20methods%0Athrough%20visual%20assessment%20of%20model%20behavior%20with%20custom%20object%20sets%2C%20but%20also%0Areveals%20opportunities%20for%20improving%20alignment%20between%20human%20perception%20and%0Amachine%20understanding%20in%20vision-language%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22305v1&entry.124074799=Read"},
{"title": "Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions", "author": "Yijun Shen and Delong Chen and Fan Liu and Xingyu Wang and Chuanyi Zhang and Liang Yao and Yuhui Zheng", "abstract": "  While densely annotated image captions significantly facilitate the learning\nof robust vision-language alignment, methodologies for systematically\noptimizing human annotation efforts remain underexplored. We introduce\nChain-of-Talkers (CoTalk), an AI-in-the-loop methodology designed to maximize\nthe number of annotated samples and improve their comprehensiveness under fixed\nbudget constraints (e.g., total human annotation time). The framework is built\nupon two key insights. First, sequential annotation reduces redundant workload\ncompared to conventional parallel annotation, as subsequent annotators only\nneed to annotate the ``residual'' -- the missing visual information that\nprevious annotations have not covered. Second, humans process textual input\nfaster by reading while outputting annotations with much higher throughput via\ntalking; thus a multimodal interface enables optimized efficiency. We evaluate\nour framework from two aspects: intrinsic evaluations that assess the\ncomprehensiveness of semantic units, obtained by parsing detailed captions into\nobject-attribute trees and analyzing their effective connections; extrinsic\nevaluation measures the practical usage of the annotated captions in\nfacilitating vision-language alignment. Experiments with eight participants\nshow our Chain-of-Talkers (CoTalk) improves annotation speed (0.42 vs. 0.30\nunits/sec) and retrieval performance (41.13\\% vs. 40.52\\%) over the parallel\nmethod.\n", "link": "http://arxiv.org/abs/2505.22627v1", "date": "2025-05-28", "relevancy": 2.6567, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5627}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5215}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chain-of-Talkers%20%28CoTalk%29%3A%20Fast%20Human%20Annotation%20of%20Dense%20Image%20Captions&body=Title%3A%20Chain-of-Talkers%20%28CoTalk%29%3A%20Fast%20Human%20Annotation%20of%20Dense%20Image%20Captions%0AAuthor%3A%20Yijun%20Shen%20and%20Delong%20Chen%20and%20Fan%20Liu%20and%20Xingyu%20Wang%20and%20Chuanyi%20Zhang%20and%20Liang%20Yao%20and%20Yuhui%20Zheng%0AAbstract%3A%20%20%20While%20densely%20annotated%20image%20captions%20significantly%20facilitate%20the%20learning%0Aof%20robust%20vision-language%20alignment%2C%20methodologies%20for%20systematically%0Aoptimizing%20human%20annotation%20efforts%20remain%20underexplored.%20We%20introduce%0AChain-of-Talkers%20%28CoTalk%29%2C%20an%20AI-in-the-loop%20methodology%20designed%20to%20maximize%0Athe%20number%20of%20annotated%20samples%20and%20improve%20their%20comprehensiveness%20under%20fixed%0Abudget%20constraints%20%28e.g.%2C%20total%20human%20annotation%20time%29.%20The%20framework%20is%20built%0Aupon%20two%20key%20insights.%20First%2C%20sequential%20annotation%20reduces%20redundant%20workload%0Acompared%20to%20conventional%20parallel%20annotation%2C%20as%20subsequent%20annotators%20only%0Aneed%20to%20annotate%20the%20%60%60residual%27%27%20--%20the%20missing%20visual%20information%20that%0Aprevious%20annotations%20have%20not%20covered.%20Second%2C%20humans%20process%20textual%20input%0Afaster%20by%20reading%20while%20outputting%20annotations%20with%20much%20higher%20throughput%20via%0Atalking%3B%20thus%20a%20multimodal%20interface%20enables%20optimized%20efficiency.%20We%20evaluate%0Aour%20framework%20from%20two%20aspects%3A%20intrinsic%20evaluations%20that%20assess%20the%0Acomprehensiveness%20of%20semantic%20units%2C%20obtained%20by%20parsing%20detailed%20captions%20into%0Aobject-attribute%20trees%20and%20analyzing%20their%20effective%20connections%3B%20extrinsic%0Aevaluation%20measures%20the%20practical%20usage%20of%20the%20annotated%20captions%20in%0Afacilitating%20vision-language%20alignment.%20Experiments%20with%20eight%20participants%0Ashow%20our%20Chain-of-Talkers%20%28CoTalk%29%20improves%20annotation%20speed%20%280.42%20vs.%200.30%0Aunits/sec%29%20and%20retrieval%20performance%20%2841.13%5C%25%20vs.%2040.52%5C%25%29%20over%20the%20parallel%0Amethod.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22627v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChain-of-Talkers%2520%2528CoTalk%2529%253A%2520Fast%2520Human%2520Annotation%2520of%2520Dense%2520Image%2520Captions%26entry.906535625%3DYijun%2520Shen%2520and%2520Delong%2520Chen%2520and%2520Fan%2520Liu%2520and%2520Xingyu%2520Wang%2520and%2520Chuanyi%2520Zhang%2520and%2520Liang%2520Yao%2520and%2520Yuhui%2520Zheng%26entry.1292438233%3D%2520%2520While%2520densely%2520annotated%2520image%2520captions%2520significantly%2520facilitate%2520the%2520learning%250Aof%2520robust%2520vision-language%2520alignment%252C%2520methodologies%2520for%2520systematically%250Aoptimizing%2520human%2520annotation%2520efforts%2520remain%2520underexplored.%2520We%2520introduce%250AChain-of-Talkers%2520%2528CoTalk%2529%252C%2520an%2520AI-in-the-loop%2520methodology%2520designed%2520to%2520maximize%250Athe%2520number%2520of%2520annotated%2520samples%2520and%2520improve%2520their%2520comprehensiveness%2520under%2520fixed%250Abudget%2520constraints%2520%2528e.g.%252C%2520total%2520human%2520annotation%2520time%2529.%2520The%2520framework%2520is%2520built%250Aupon%2520two%2520key%2520insights.%2520First%252C%2520sequential%2520annotation%2520reduces%2520redundant%2520workload%250Acompared%2520to%2520conventional%2520parallel%2520annotation%252C%2520as%2520subsequent%2520annotators%2520only%250Aneed%2520to%2520annotate%2520the%2520%2560%2560residual%2527%2527%2520--%2520the%2520missing%2520visual%2520information%2520that%250Aprevious%2520annotations%2520have%2520not%2520covered.%2520Second%252C%2520humans%2520process%2520textual%2520input%250Afaster%2520by%2520reading%2520while%2520outputting%2520annotations%2520with%2520much%2520higher%2520throughput%2520via%250Atalking%253B%2520thus%2520a%2520multimodal%2520interface%2520enables%2520optimized%2520efficiency.%2520We%2520evaluate%250Aour%2520framework%2520from%2520two%2520aspects%253A%2520intrinsic%2520evaluations%2520that%2520assess%2520the%250Acomprehensiveness%2520of%2520semantic%2520units%252C%2520obtained%2520by%2520parsing%2520detailed%2520captions%2520into%250Aobject-attribute%2520trees%2520and%2520analyzing%2520their%2520effective%2520connections%253B%2520extrinsic%250Aevaluation%2520measures%2520the%2520practical%2520usage%2520of%2520the%2520annotated%2520captions%2520in%250Afacilitating%2520vision-language%2520alignment.%2520Experiments%2520with%2520eight%2520participants%250Ashow%2520our%2520Chain-of-Talkers%2520%2528CoTalk%2529%2520improves%2520annotation%2520speed%2520%25280.42%2520vs.%25200.30%250Aunits/sec%2529%2520and%2520retrieval%2520performance%2520%252841.13%255C%2525%2520vs.%252040.52%255C%2525%2529%2520over%2520the%2520parallel%250Amethod.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22627v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chain-of-Talkers%20%28CoTalk%29%3A%20Fast%20Human%20Annotation%20of%20Dense%20Image%20Captions&entry.906535625=Yijun%20Shen%20and%20Delong%20Chen%20and%20Fan%20Liu%20and%20Xingyu%20Wang%20and%20Chuanyi%20Zhang%20and%20Liang%20Yao%20and%20Yuhui%20Zheng&entry.1292438233=%20%20While%20densely%20annotated%20image%20captions%20significantly%20facilitate%20the%20learning%0Aof%20robust%20vision-language%20alignment%2C%20methodologies%20for%20systematically%0Aoptimizing%20human%20annotation%20efforts%20remain%20underexplored.%20We%20introduce%0AChain-of-Talkers%20%28CoTalk%29%2C%20an%20AI-in-the-loop%20methodology%20designed%20to%20maximize%0Athe%20number%20of%20annotated%20samples%20and%20improve%20their%20comprehensiveness%20under%20fixed%0Abudget%20constraints%20%28e.g.%2C%20total%20human%20annotation%20time%29.%20The%20framework%20is%20built%0Aupon%20two%20key%20insights.%20First%2C%20sequential%20annotation%20reduces%20redundant%20workload%0Acompared%20to%20conventional%20parallel%20annotation%2C%20as%20subsequent%20annotators%20only%0Aneed%20to%20annotate%20the%20%60%60residual%27%27%20--%20the%20missing%20visual%20information%20that%0Aprevious%20annotations%20have%20not%20covered.%20Second%2C%20humans%20process%20textual%20input%0Afaster%20by%20reading%20while%20outputting%20annotations%20with%20much%20higher%20throughput%20via%0Atalking%3B%20thus%20a%20multimodal%20interface%20enables%20optimized%20efficiency.%20We%20evaluate%0Aour%20framework%20from%20two%20aspects%3A%20intrinsic%20evaluations%20that%20assess%20the%0Acomprehensiveness%20of%20semantic%20units%2C%20obtained%20by%20parsing%20detailed%20captions%20into%0Aobject-attribute%20trees%20and%20analyzing%20their%20effective%20connections%3B%20extrinsic%0Aevaluation%20measures%20the%20practical%20usage%20of%20the%20annotated%20captions%20in%0Afacilitating%20vision-language%20alignment.%20Experiments%20with%20eight%20participants%0Ashow%20our%20Chain-of-Talkers%20%28CoTalk%29%20improves%20annotation%20speed%20%280.42%20vs.%200.30%0Aunits/sec%29%20and%20retrieval%20performance%20%2841.13%5C%25%20vs.%2040.52%5C%25%29%20over%20the%20parallel%0Amethod.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22627v1&entry.124074799=Read"},
{"title": "RICO: Improving Accuracy and Completeness in Image Recaptioning via\n  Visual Reconstruction", "author": "Yuchi Wang and Yishuo Cai and Shuhuai Ren and Sihan Yang and Linli Yao and Yuanxin Liu and Yuanxing Zhang and Pengfei Wan and Xu Sun", "abstract": "  Image recaptioning is widely used to generate training datasets with enhanced\nquality for various multimodal tasks. Existing recaptioning methods typically\nrely on powerful multimodal large language models (MLLMs) to enhance textual\ndescriptions, but often suffer from inaccuracies due to hallucinations and\nincompleteness caused by missing fine-grained details. To address these\nlimitations, we propose RICO, a novel framework that refines captions through\nvisual reconstruction. Specifically, we leverage a text-to-image model to\nreconstruct a caption into a reference image, and prompt an MLLM to identify\ndiscrepancies between the original and reconstructed images to refine the\ncaption. This process is performed iteratively, further progressively promoting\nthe generation of more faithful and comprehensive descriptions. To mitigate the\nadditional computational cost induced by the iterative process, we introduce\nRICO-Flash, which learns to generate captions like RICO using DPO. Extensive\nexperiments demonstrate that our approach significantly improves caption\naccuracy and completeness, outperforms most baselines by approximately 10% on\nboth CapsBench and CompreCap. Code released at\nhttps://github.com/wangyuchi369/RICO.\n", "link": "http://arxiv.org/abs/2505.22613v1", "date": "2025-05-28", "relevancy": 2.6553, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5365}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5365}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RICO%3A%20Improving%20Accuracy%20and%20Completeness%20in%20Image%20Recaptioning%20via%0A%20%20Visual%20Reconstruction&body=Title%3A%20RICO%3A%20Improving%20Accuracy%20and%20Completeness%20in%20Image%20Recaptioning%20via%0A%20%20Visual%20Reconstruction%0AAuthor%3A%20Yuchi%20Wang%20and%20Yishuo%20Cai%20and%20Shuhuai%20Ren%20and%20Sihan%20Yang%20and%20Linli%20Yao%20and%20Yuanxin%20Liu%20and%20Yuanxing%20Zhang%20and%20Pengfei%20Wan%20and%20Xu%20Sun%0AAbstract%3A%20%20%20Image%20recaptioning%20is%20widely%20used%20to%20generate%20training%20datasets%20with%20enhanced%0Aquality%20for%20various%20multimodal%20tasks.%20Existing%20recaptioning%20methods%20typically%0Arely%20on%20powerful%20multimodal%20large%20language%20models%20%28MLLMs%29%20to%20enhance%20textual%0Adescriptions%2C%20but%20often%20suffer%20from%20inaccuracies%20due%20to%20hallucinations%20and%0Aincompleteness%20caused%20by%20missing%20fine-grained%20details.%20To%20address%20these%0Alimitations%2C%20we%20propose%20RICO%2C%20a%20novel%20framework%20that%20refines%20captions%20through%0Avisual%20reconstruction.%20Specifically%2C%20we%20leverage%20a%20text-to-image%20model%20to%0Areconstruct%20a%20caption%20into%20a%20reference%20image%2C%20and%20prompt%20an%20MLLM%20to%20identify%0Adiscrepancies%20between%20the%20original%20and%20reconstructed%20images%20to%20refine%20the%0Acaption.%20This%20process%20is%20performed%20iteratively%2C%20further%20progressively%20promoting%0Athe%20generation%20of%20more%20faithful%20and%20comprehensive%20descriptions.%20To%20mitigate%20the%0Aadditional%20computational%20cost%20induced%20by%20the%20iterative%20process%2C%20we%20introduce%0ARICO-Flash%2C%20which%20learns%20to%20generate%20captions%20like%20RICO%20using%20DPO.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20approach%20significantly%20improves%20caption%0Aaccuracy%20and%20completeness%2C%20outperforms%20most%20baselines%20by%20approximately%2010%25%20on%0Aboth%20CapsBench%20and%20CompreCap.%20Code%20released%20at%0Ahttps%3A//github.com/wangyuchi369/RICO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22613v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRICO%253A%2520Improving%2520Accuracy%2520and%2520Completeness%2520in%2520Image%2520Recaptioning%2520via%250A%2520%2520Visual%2520Reconstruction%26entry.906535625%3DYuchi%2520Wang%2520and%2520Yishuo%2520Cai%2520and%2520Shuhuai%2520Ren%2520and%2520Sihan%2520Yang%2520and%2520Linli%2520Yao%2520and%2520Yuanxin%2520Liu%2520and%2520Yuanxing%2520Zhang%2520and%2520Pengfei%2520Wan%2520and%2520Xu%2520Sun%26entry.1292438233%3D%2520%2520Image%2520recaptioning%2520is%2520widely%2520used%2520to%2520generate%2520training%2520datasets%2520with%2520enhanced%250Aquality%2520for%2520various%2520multimodal%2520tasks.%2520Existing%2520recaptioning%2520methods%2520typically%250Arely%2520on%2520powerful%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520to%2520enhance%2520textual%250Adescriptions%252C%2520but%2520often%2520suffer%2520from%2520inaccuracies%2520due%2520to%2520hallucinations%2520and%250Aincompleteness%2520caused%2520by%2520missing%2520fine-grained%2520details.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520RICO%252C%2520a%2520novel%2520framework%2520that%2520refines%2520captions%2520through%250Avisual%2520reconstruction.%2520Specifically%252C%2520we%2520leverage%2520a%2520text-to-image%2520model%2520to%250Areconstruct%2520a%2520caption%2520into%2520a%2520reference%2520image%252C%2520and%2520prompt%2520an%2520MLLM%2520to%2520identify%250Adiscrepancies%2520between%2520the%2520original%2520and%2520reconstructed%2520images%2520to%2520refine%2520the%250Acaption.%2520This%2520process%2520is%2520performed%2520iteratively%252C%2520further%2520progressively%2520promoting%250Athe%2520generation%2520of%2520more%2520faithful%2520and%2520comprehensive%2520descriptions.%2520To%2520mitigate%2520the%250Aadditional%2520computational%2520cost%2520induced%2520by%2520the%2520iterative%2520process%252C%2520we%2520introduce%250ARICO-Flash%252C%2520which%2520learns%2520to%2520generate%2520captions%2520like%2520RICO%2520using%2520DPO.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520improves%2520caption%250Aaccuracy%2520and%2520completeness%252C%2520outperforms%2520most%2520baselines%2520by%2520approximately%252010%2525%2520on%250Aboth%2520CapsBench%2520and%2520CompreCap.%2520Code%2520released%2520at%250Ahttps%253A//github.com/wangyuchi369/RICO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22613v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RICO%3A%20Improving%20Accuracy%20and%20Completeness%20in%20Image%20Recaptioning%20via%0A%20%20Visual%20Reconstruction&entry.906535625=Yuchi%20Wang%20and%20Yishuo%20Cai%20and%20Shuhuai%20Ren%20and%20Sihan%20Yang%20and%20Linli%20Yao%20and%20Yuanxin%20Liu%20and%20Yuanxing%20Zhang%20and%20Pengfei%20Wan%20and%20Xu%20Sun&entry.1292438233=%20%20Image%20recaptioning%20is%20widely%20used%20to%20generate%20training%20datasets%20with%20enhanced%0Aquality%20for%20various%20multimodal%20tasks.%20Existing%20recaptioning%20methods%20typically%0Arely%20on%20powerful%20multimodal%20large%20language%20models%20%28MLLMs%29%20to%20enhance%20textual%0Adescriptions%2C%20but%20often%20suffer%20from%20inaccuracies%20due%20to%20hallucinations%20and%0Aincompleteness%20caused%20by%20missing%20fine-grained%20details.%20To%20address%20these%0Alimitations%2C%20we%20propose%20RICO%2C%20a%20novel%20framework%20that%20refines%20captions%20through%0Avisual%20reconstruction.%20Specifically%2C%20we%20leverage%20a%20text-to-image%20model%20to%0Areconstruct%20a%20caption%20into%20a%20reference%20image%2C%20and%20prompt%20an%20MLLM%20to%20identify%0Adiscrepancies%20between%20the%20original%20and%20reconstructed%20images%20to%20refine%20the%0Acaption.%20This%20process%20is%20performed%20iteratively%2C%20further%20progressively%20promoting%0Athe%20generation%20of%20more%20faithful%20and%20comprehensive%20descriptions.%20To%20mitigate%20the%0Aadditional%20computational%20cost%20induced%20by%20the%20iterative%20process%2C%20we%20introduce%0ARICO-Flash%2C%20which%20learns%20to%20generate%20captions%20like%20RICO%20using%20DPO.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20approach%20significantly%20improves%20caption%0Aaccuracy%20and%20completeness%2C%20outperforms%20most%20baselines%20by%20approximately%2010%25%20on%0Aboth%20CapsBench%20and%20CompreCap.%20Code%20released%20at%0Ahttps%3A//github.com/wangyuchi369/RICO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22613v1&entry.124074799=Read"},
{"title": "Beyond External Monitors: Enhancing Transparency of Large Language\n  Models for Easier Monitoring", "author": "Guanxu Chen and Dongrui Liu and Tao Luo and Lijie Hu and Jing Shao", "abstract": "  Large language models (LLMs) are becoming increasingly capable, but the\nmechanisms of their thinking and decision-making process remain unclear.\nChain-of-thoughts (CoTs) have been commonly utilized to monitor LLMs, but this\nstrategy fails to accurately reflect LLMs' thinking process. Techniques based\non LLMs' hidden representations provide an inner perspective to monitor their\nlatent thinking. However, previous methods only try to develop external\nmonitors instead of making LLMs themselves easier to monitor. In this paper, we\npropose a novel method TELLME, improving the transparency of LLMs and helping\nmonitors identify unsuitable and sensitive behaviors. Furthermore, we showcase\nthe applications of TELLME on trustworthiness tasks (\\eg, safety risks\nmonitoring tasks and detoxification tasks), where LLMs achieve consistent\nimprovement in transparency and task performance. More crucially, we\ntheoretically analyze the improvement of TELLME on LLMs' generalization ability\nthrough optimal transport theory.\n", "link": "http://arxiv.org/abs/2502.05242v2", "date": "2025-05-28", "relevancy": 2.6452, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5358}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5358}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20External%20Monitors%3A%20Enhancing%20Transparency%20of%20Large%20Language%0A%20%20Models%20for%20Easier%20Monitoring&body=Title%3A%20Beyond%20External%20Monitors%3A%20Enhancing%20Transparency%20of%20Large%20Language%0A%20%20Models%20for%20Easier%20Monitoring%0AAuthor%3A%20Guanxu%20Chen%20and%20Dongrui%20Liu%20and%20Tao%20Luo%20and%20Lijie%20Hu%20and%20Jing%20Shao%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20becoming%20increasingly%20capable%2C%20but%20the%0Amechanisms%20of%20their%20thinking%20and%20decision-making%20process%20remain%20unclear.%0AChain-of-thoughts%20%28CoTs%29%20have%20been%20commonly%20utilized%20to%20monitor%20LLMs%2C%20but%20this%0Astrategy%20fails%20to%20accurately%20reflect%20LLMs%27%20thinking%20process.%20Techniques%20based%0Aon%20LLMs%27%20hidden%20representations%20provide%20an%20inner%20perspective%20to%20monitor%20their%0Alatent%20thinking.%20However%2C%20previous%20methods%20only%20try%20to%20develop%20external%0Amonitors%20instead%20of%20making%20LLMs%20themselves%20easier%20to%20monitor.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20method%20TELLME%2C%20improving%20the%20transparency%20of%20LLMs%20and%20helping%0Amonitors%20identify%20unsuitable%20and%20sensitive%20behaviors.%20Furthermore%2C%20we%20showcase%0Athe%20applications%20of%20TELLME%20on%20trustworthiness%20tasks%20%28%5Ceg%2C%20safety%20risks%0Amonitoring%20tasks%20and%20detoxification%20tasks%29%2C%20where%20LLMs%20achieve%20consistent%0Aimprovement%20in%20transparency%20and%20task%20performance.%20More%20crucially%2C%20we%0Atheoretically%20analyze%20the%20improvement%20of%20TELLME%20on%20LLMs%27%20generalization%20ability%0Athrough%20optimal%20transport%20theory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05242v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520External%2520Monitors%253A%2520Enhancing%2520Transparency%2520of%2520Large%2520Language%250A%2520%2520Models%2520for%2520Easier%2520Monitoring%26entry.906535625%3DGuanxu%2520Chen%2520and%2520Dongrui%2520Liu%2520and%2520Tao%2520Luo%2520and%2520Lijie%2520Hu%2520and%2520Jing%2520Shao%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520becoming%2520increasingly%2520capable%252C%2520but%2520the%250Amechanisms%2520of%2520their%2520thinking%2520and%2520decision-making%2520process%2520remain%2520unclear.%250AChain-of-thoughts%2520%2528CoTs%2529%2520have%2520been%2520commonly%2520utilized%2520to%2520monitor%2520LLMs%252C%2520but%2520this%250Astrategy%2520fails%2520to%2520accurately%2520reflect%2520LLMs%2527%2520thinking%2520process.%2520Techniques%2520based%250Aon%2520LLMs%2527%2520hidden%2520representations%2520provide%2520an%2520inner%2520perspective%2520to%2520monitor%2520their%250Alatent%2520thinking.%2520However%252C%2520previous%2520methods%2520only%2520try%2520to%2520develop%2520external%250Amonitors%2520instead%2520of%2520making%2520LLMs%2520themselves%2520easier%2520to%2520monitor.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520method%2520TELLME%252C%2520improving%2520the%2520transparency%2520of%2520LLMs%2520and%2520helping%250Amonitors%2520identify%2520unsuitable%2520and%2520sensitive%2520behaviors.%2520Furthermore%252C%2520we%2520showcase%250Athe%2520applications%2520of%2520TELLME%2520on%2520trustworthiness%2520tasks%2520%2528%255Ceg%252C%2520safety%2520risks%250Amonitoring%2520tasks%2520and%2520detoxification%2520tasks%2529%252C%2520where%2520LLMs%2520achieve%2520consistent%250Aimprovement%2520in%2520transparency%2520and%2520task%2520performance.%2520More%2520crucially%252C%2520we%250Atheoretically%2520analyze%2520the%2520improvement%2520of%2520TELLME%2520on%2520LLMs%2527%2520generalization%2520ability%250Athrough%2520optimal%2520transport%2520theory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05242v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20External%20Monitors%3A%20Enhancing%20Transparency%20of%20Large%20Language%0A%20%20Models%20for%20Easier%20Monitoring&entry.906535625=Guanxu%20Chen%20and%20Dongrui%20Liu%20and%20Tao%20Luo%20and%20Lijie%20Hu%20and%20Jing%20Shao&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20becoming%20increasingly%20capable%2C%20but%20the%0Amechanisms%20of%20their%20thinking%20and%20decision-making%20process%20remain%20unclear.%0AChain-of-thoughts%20%28CoTs%29%20have%20been%20commonly%20utilized%20to%20monitor%20LLMs%2C%20but%20this%0Astrategy%20fails%20to%20accurately%20reflect%20LLMs%27%20thinking%20process.%20Techniques%20based%0Aon%20LLMs%27%20hidden%20representations%20provide%20an%20inner%20perspective%20to%20monitor%20their%0Alatent%20thinking.%20However%2C%20previous%20methods%20only%20try%20to%20develop%20external%0Amonitors%20instead%20of%20making%20LLMs%20themselves%20easier%20to%20monitor.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20method%20TELLME%2C%20improving%20the%20transparency%20of%20LLMs%20and%20helping%0Amonitors%20identify%20unsuitable%20and%20sensitive%20behaviors.%20Furthermore%2C%20we%20showcase%0Athe%20applications%20of%20TELLME%20on%20trustworthiness%20tasks%20%28%5Ceg%2C%20safety%20risks%0Amonitoring%20tasks%20and%20detoxification%20tasks%29%2C%20where%20LLMs%20achieve%20consistent%0Aimprovement%20in%20transparency%20and%20task%20performance.%20More%20crucially%2C%20we%0Atheoretically%20analyze%20the%20improvement%20of%20TELLME%20on%20LLMs%27%20generalization%20ability%0Athrough%20optimal%20transport%20theory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05242v2&entry.124074799=Read"},
{"title": "SplitLoRA: Balancing Stability and Plasticity in Continual Learning\n  Through Gradient Space Splitting", "author": "Haomiao Qiu and Miao Zhang and Ziyue Qiao and Weili Guan and Min Zhang and Liqiang Nie", "abstract": "  Continual Learning requires a model to learn multiple tasks in sequence while\nmaintaining both stability:preserving knowledge from previously learned tasks,\nand plasticity:effectively learning new tasks. Gradient projection has emerged\nas an effective and popular paradigm in CL, where it partitions the gradient\nspace of previously learned tasks into two orthogonal subspaces: a primary\nsubspace and a minor subspace. New tasks are learned effectively within the\nminor subspace, thereby reducing interference with previously acquired\nknowledge. However, existing Gradient Projection methods struggle to achieve an\noptimal balance between plasticity and stability, as it is hard to\nappropriately partition the gradient space. In this work, we consider a\ncontinual learning paradigm based on Low-Rank Adaptation, which has gained\nconsiderable attention due to its efficiency and wide applicability, and\npropose a novel approach for continual learning, called SplitLoRA. We first\nprovide a theoretical analysis of how subspace partitioning affects model\nstability and plasticity. Informed by this analysis, we then introduce an\neffective method that derives the optimal partition of the gradient space for\npreviously learned tasks. This approach effectively balances stability and\nplasticity in continual learning. Experimental results on multiple datasets\ndemonstrate that the proposed method achieves state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2505.22370v1", "date": "2025-05-28", "relevancy": 2.6419, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5318}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5303}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SplitLoRA%3A%20Balancing%20Stability%20and%20Plasticity%20in%20Continual%20Learning%0A%20%20Through%20Gradient%20Space%20Splitting&body=Title%3A%20SplitLoRA%3A%20Balancing%20Stability%20and%20Plasticity%20in%20Continual%20Learning%0A%20%20Through%20Gradient%20Space%20Splitting%0AAuthor%3A%20Haomiao%20Qiu%20and%20Miao%20Zhang%20and%20Ziyue%20Qiao%20and%20Weili%20Guan%20and%20Min%20Zhang%20and%20Liqiang%20Nie%0AAbstract%3A%20%20%20Continual%20Learning%20requires%20a%20model%20to%20learn%20multiple%20tasks%20in%20sequence%20while%0Amaintaining%20both%20stability%3Apreserving%20knowledge%20from%20previously%20learned%20tasks%2C%0Aand%20plasticity%3Aeffectively%20learning%20new%20tasks.%20Gradient%20projection%20has%20emerged%0Aas%20an%20effective%20and%20popular%20paradigm%20in%20CL%2C%20where%20it%20partitions%20the%20gradient%0Aspace%20of%20previously%20learned%20tasks%20into%20two%20orthogonal%20subspaces%3A%20a%20primary%0Asubspace%20and%20a%20minor%20subspace.%20New%20tasks%20are%20learned%20effectively%20within%20the%0Aminor%20subspace%2C%20thereby%20reducing%20interference%20with%20previously%20acquired%0Aknowledge.%20However%2C%20existing%20Gradient%20Projection%20methods%20struggle%20to%20achieve%20an%0Aoptimal%20balance%20between%20plasticity%20and%20stability%2C%20as%20it%20is%20hard%20to%0Aappropriately%20partition%20the%20gradient%20space.%20In%20this%20work%2C%20we%20consider%20a%0Acontinual%20learning%20paradigm%20based%20on%20Low-Rank%20Adaptation%2C%20which%20has%20gained%0Aconsiderable%20attention%20due%20to%20its%20efficiency%20and%20wide%20applicability%2C%20and%0Apropose%20a%20novel%20approach%20for%20continual%20learning%2C%20called%20SplitLoRA.%20We%20first%0Aprovide%20a%20theoretical%20analysis%20of%20how%20subspace%20partitioning%20affects%20model%0Astability%20and%20plasticity.%20Informed%20by%20this%20analysis%2C%20we%20then%20introduce%20an%0Aeffective%20method%20that%20derives%20the%20optimal%20partition%20of%20the%20gradient%20space%20for%0Apreviously%20learned%20tasks.%20This%20approach%20effectively%20balances%20stability%20and%0Aplasticity%20in%20continual%20learning.%20Experimental%20results%20on%20multiple%20datasets%0Ademonstrate%20that%20the%20proposed%20method%20achieves%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22370v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSplitLoRA%253A%2520Balancing%2520Stability%2520and%2520Plasticity%2520in%2520Continual%2520Learning%250A%2520%2520Through%2520Gradient%2520Space%2520Splitting%26entry.906535625%3DHaomiao%2520Qiu%2520and%2520Miao%2520Zhang%2520and%2520Ziyue%2520Qiao%2520and%2520Weili%2520Guan%2520and%2520Min%2520Zhang%2520and%2520Liqiang%2520Nie%26entry.1292438233%3D%2520%2520Continual%2520Learning%2520requires%2520a%2520model%2520to%2520learn%2520multiple%2520tasks%2520in%2520sequence%2520while%250Amaintaining%2520both%2520stability%253Apreserving%2520knowledge%2520from%2520previously%2520learned%2520tasks%252C%250Aand%2520plasticity%253Aeffectively%2520learning%2520new%2520tasks.%2520Gradient%2520projection%2520has%2520emerged%250Aas%2520an%2520effective%2520and%2520popular%2520paradigm%2520in%2520CL%252C%2520where%2520it%2520partitions%2520the%2520gradient%250Aspace%2520of%2520previously%2520learned%2520tasks%2520into%2520two%2520orthogonal%2520subspaces%253A%2520a%2520primary%250Asubspace%2520and%2520a%2520minor%2520subspace.%2520New%2520tasks%2520are%2520learned%2520effectively%2520within%2520the%250Aminor%2520subspace%252C%2520thereby%2520reducing%2520interference%2520with%2520previously%2520acquired%250Aknowledge.%2520However%252C%2520existing%2520Gradient%2520Projection%2520methods%2520struggle%2520to%2520achieve%2520an%250Aoptimal%2520balance%2520between%2520plasticity%2520and%2520stability%252C%2520as%2520it%2520is%2520hard%2520to%250Aappropriately%2520partition%2520the%2520gradient%2520space.%2520In%2520this%2520work%252C%2520we%2520consider%2520a%250Acontinual%2520learning%2520paradigm%2520based%2520on%2520Low-Rank%2520Adaptation%252C%2520which%2520has%2520gained%250Aconsiderable%2520attention%2520due%2520to%2520its%2520efficiency%2520and%2520wide%2520applicability%252C%2520and%250Apropose%2520a%2520novel%2520approach%2520for%2520continual%2520learning%252C%2520called%2520SplitLoRA.%2520We%2520first%250Aprovide%2520a%2520theoretical%2520analysis%2520of%2520how%2520subspace%2520partitioning%2520affects%2520model%250Astability%2520and%2520plasticity.%2520Informed%2520by%2520this%2520analysis%252C%2520we%2520then%2520introduce%2520an%250Aeffective%2520method%2520that%2520derives%2520the%2520optimal%2520partition%2520of%2520the%2520gradient%2520space%2520for%250Apreviously%2520learned%2520tasks.%2520This%2520approach%2520effectively%2520balances%2520stability%2520and%250Aplasticity%2520in%2520continual%2520learning.%2520Experimental%2520results%2520on%2520multiple%2520datasets%250Ademonstrate%2520that%2520the%2520proposed%2520method%2520achieves%2520state-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22370v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SplitLoRA%3A%20Balancing%20Stability%20and%20Plasticity%20in%20Continual%20Learning%0A%20%20Through%20Gradient%20Space%20Splitting&entry.906535625=Haomiao%20Qiu%20and%20Miao%20Zhang%20and%20Ziyue%20Qiao%20and%20Weili%20Guan%20and%20Min%20Zhang%20and%20Liqiang%20Nie&entry.1292438233=%20%20Continual%20Learning%20requires%20a%20model%20to%20learn%20multiple%20tasks%20in%20sequence%20while%0Amaintaining%20both%20stability%3Apreserving%20knowledge%20from%20previously%20learned%20tasks%2C%0Aand%20plasticity%3Aeffectively%20learning%20new%20tasks.%20Gradient%20projection%20has%20emerged%0Aas%20an%20effective%20and%20popular%20paradigm%20in%20CL%2C%20where%20it%20partitions%20the%20gradient%0Aspace%20of%20previously%20learned%20tasks%20into%20two%20orthogonal%20subspaces%3A%20a%20primary%0Asubspace%20and%20a%20minor%20subspace.%20New%20tasks%20are%20learned%20effectively%20within%20the%0Aminor%20subspace%2C%20thereby%20reducing%20interference%20with%20previously%20acquired%0Aknowledge.%20However%2C%20existing%20Gradient%20Projection%20methods%20struggle%20to%20achieve%20an%0Aoptimal%20balance%20between%20plasticity%20and%20stability%2C%20as%20it%20is%20hard%20to%0Aappropriately%20partition%20the%20gradient%20space.%20In%20this%20work%2C%20we%20consider%20a%0Acontinual%20learning%20paradigm%20based%20on%20Low-Rank%20Adaptation%2C%20which%20has%20gained%0Aconsiderable%20attention%20due%20to%20its%20efficiency%20and%20wide%20applicability%2C%20and%0Apropose%20a%20novel%20approach%20for%20continual%20learning%2C%20called%20SplitLoRA.%20We%20first%0Aprovide%20a%20theoretical%20analysis%20of%20how%20subspace%20partitioning%20affects%20model%0Astability%20and%20plasticity.%20Informed%20by%20this%20analysis%2C%20we%20then%20introduce%20an%0Aeffective%20method%20that%20derives%20the%20optimal%20partition%20of%20the%20gradient%20space%20for%0Apreviously%20learned%20tasks.%20This%20approach%20effectively%20balances%20stability%20and%0Aplasticity%20in%20continual%20learning.%20Experimental%20results%20on%20multiple%20datasets%0Ademonstrate%20that%20the%20proposed%20method%20achieves%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22370v1&entry.124074799=Read"},
{"title": "Intrinsic User-Centric Interpretability through Global Mixture of\n  Experts", "author": "Vinitra Swamy and Syrielle Montariol and Julian Blackwell and Jibril Frej and Martin Jaggi and Tanja K\u00e4ser", "abstract": "  In human-centric settings like education or healthcare, model accuracy and\nmodel explainability are key factors for user adoption. Towards these two\ngoals, intrinsically interpretable deep learning models have gained popularity,\nfocusing on accurate predictions alongside faithful explanations. However,\nthere exists a gap in the human-centeredness of these approaches, which often\nproduce nuanced and complex explanations that are not easily actionable for\ndownstream users. We present InterpretCC (interpretable conditional\ncomputation), a family of intrinsically interpretable neural networks at a\nunique point in the design space that optimizes for ease of human understanding\nand explanation faithfulness, while maintaining comparable performance to\nstate-of-the-art models. InterpretCC achieves this through adaptive sparse\nactivation of features before prediction, allowing the model to use a\ndifferent, minimal set of features for each instance. We extend this idea into\nan interpretable, global mixture-of-experts (MoE) model that allows users to\nspecify topics of interest, discretely separates the feature space for each\ndata point into topical subnetworks, and adaptively and sparsely activates\nthese topical subnetworks for prediction. We apply InterpretCC for text, time\nseries and tabular data across several real-world datasets, demonstrating\ncomparable performance with non-interpretable baselines and outperforming\nintrinsically interpretable baselines. Through a user study involving 56\nteachers, InterpretCC explanations are found to have higher actionability and\nusefulness over other intrinsically interpretable approaches.\n", "link": "http://arxiv.org/abs/2402.02933v4", "date": "2025-05-28", "relevancy": 2.6283, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5299}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5299}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intrinsic%20User-Centric%20Interpretability%20through%20Global%20Mixture%20of%0A%20%20Experts&body=Title%3A%20Intrinsic%20User-Centric%20Interpretability%20through%20Global%20Mixture%20of%0A%20%20Experts%0AAuthor%3A%20Vinitra%20Swamy%20and%20Syrielle%20Montariol%20and%20Julian%20Blackwell%20and%20Jibril%20Frej%20and%20Martin%20Jaggi%20and%20Tanja%20K%C3%A4ser%0AAbstract%3A%20%20%20In%20human-centric%20settings%20like%20education%20or%20healthcare%2C%20model%20accuracy%20and%0Amodel%20explainability%20are%20key%20factors%20for%20user%20adoption.%20Towards%20these%20two%0Agoals%2C%20intrinsically%20interpretable%20deep%20learning%20models%20have%20gained%20popularity%2C%0Afocusing%20on%20accurate%20predictions%20alongside%20faithful%20explanations.%20However%2C%0Athere%20exists%20a%20gap%20in%20the%20human-centeredness%20of%20these%20approaches%2C%20which%20often%0Aproduce%20nuanced%20and%20complex%20explanations%20that%20are%20not%20easily%20actionable%20for%0Adownstream%20users.%20We%20present%20InterpretCC%20%28interpretable%20conditional%0Acomputation%29%2C%20a%20family%20of%20intrinsically%20interpretable%20neural%20networks%20at%20a%0Aunique%20point%20in%20the%20design%20space%20that%20optimizes%20for%20ease%20of%20human%20understanding%0Aand%20explanation%20faithfulness%2C%20while%20maintaining%20comparable%20performance%20to%0Astate-of-the-art%20models.%20InterpretCC%20achieves%20this%20through%20adaptive%20sparse%0Aactivation%20of%20features%20before%20prediction%2C%20allowing%20the%20model%20to%20use%20a%0Adifferent%2C%20minimal%20set%20of%20features%20for%20each%20instance.%20We%20extend%20this%20idea%20into%0Aan%20interpretable%2C%20global%20mixture-of-experts%20%28MoE%29%20model%20that%20allows%20users%20to%0Aspecify%20topics%20of%20interest%2C%20discretely%20separates%20the%20feature%20space%20for%20each%0Adata%20point%20into%20topical%20subnetworks%2C%20and%20adaptively%20and%20sparsely%20activates%0Athese%20topical%20subnetworks%20for%20prediction.%20We%20apply%20InterpretCC%20for%20text%2C%20time%0Aseries%20and%20tabular%20data%20across%20several%20real-world%20datasets%2C%20demonstrating%0Acomparable%20performance%20with%20non-interpretable%20baselines%20and%20outperforming%0Aintrinsically%20interpretable%20baselines.%20Through%20a%20user%20study%20involving%2056%0Ateachers%2C%20InterpretCC%20explanations%20are%20found%20to%20have%20higher%20actionability%20and%0Ausefulness%20over%20other%20intrinsically%20interpretable%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02933v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntrinsic%2520User-Centric%2520Interpretability%2520through%2520Global%2520Mixture%2520of%250A%2520%2520Experts%26entry.906535625%3DVinitra%2520Swamy%2520and%2520Syrielle%2520Montariol%2520and%2520Julian%2520Blackwell%2520and%2520Jibril%2520Frej%2520and%2520Martin%2520Jaggi%2520and%2520Tanja%2520K%25C3%25A4ser%26entry.1292438233%3D%2520%2520In%2520human-centric%2520settings%2520like%2520education%2520or%2520healthcare%252C%2520model%2520accuracy%2520and%250Amodel%2520explainability%2520are%2520key%2520factors%2520for%2520user%2520adoption.%2520Towards%2520these%2520two%250Agoals%252C%2520intrinsically%2520interpretable%2520deep%2520learning%2520models%2520have%2520gained%2520popularity%252C%250Afocusing%2520on%2520accurate%2520predictions%2520alongside%2520faithful%2520explanations.%2520However%252C%250Athere%2520exists%2520a%2520gap%2520in%2520the%2520human-centeredness%2520of%2520these%2520approaches%252C%2520which%2520often%250Aproduce%2520nuanced%2520and%2520complex%2520explanations%2520that%2520are%2520not%2520easily%2520actionable%2520for%250Adownstream%2520users.%2520We%2520present%2520InterpretCC%2520%2528interpretable%2520conditional%250Acomputation%2529%252C%2520a%2520family%2520of%2520intrinsically%2520interpretable%2520neural%2520networks%2520at%2520a%250Aunique%2520point%2520in%2520the%2520design%2520space%2520that%2520optimizes%2520for%2520ease%2520of%2520human%2520understanding%250Aand%2520explanation%2520faithfulness%252C%2520while%2520maintaining%2520comparable%2520performance%2520to%250Astate-of-the-art%2520models.%2520InterpretCC%2520achieves%2520this%2520through%2520adaptive%2520sparse%250Aactivation%2520of%2520features%2520before%2520prediction%252C%2520allowing%2520the%2520model%2520to%2520use%2520a%250Adifferent%252C%2520minimal%2520set%2520of%2520features%2520for%2520each%2520instance.%2520We%2520extend%2520this%2520idea%2520into%250Aan%2520interpretable%252C%2520global%2520mixture-of-experts%2520%2528MoE%2529%2520model%2520that%2520allows%2520users%2520to%250Aspecify%2520topics%2520of%2520interest%252C%2520discretely%2520separates%2520the%2520feature%2520space%2520for%2520each%250Adata%2520point%2520into%2520topical%2520subnetworks%252C%2520and%2520adaptively%2520and%2520sparsely%2520activates%250Athese%2520topical%2520subnetworks%2520for%2520prediction.%2520We%2520apply%2520InterpretCC%2520for%2520text%252C%2520time%250Aseries%2520and%2520tabular%2520data%2520across%2520several%2520real-world%2520datasets%252C%2520demonstrating%250Acomparable%2520performance%2520with%2520non-interpretable%2520baselines%2520and%2520outperforming%250Aintrinsically%2520interpretable%2520baselines.%2520Through%2520a%2520user%2520study%2520involving%252056%250Ateachers%252C%2520InterpretCC%2520explanations%2520are%2520found%2520to%2520have%2520higher%2520actionability%2520and%250Ausefulness%2520over%2520other%2520intrinsically%2520interpretable%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02933v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intrinsic%20User-Centric%20Interpretability%20through%20Global%20Mixture%20of%0A%20%20Experts&entry.906535625=Vinitra%20Swamy%20and%20Syrielle%20Montariol%20and%20Julian%20Blackwell%20and%20Jibril%20Frej%20and%20Martin%20Jaggi%20and%20Tanja%20K%C3%A4ser&entry.1292438233=%20%20In%20human-centric%20settings%20like%20education%20or%20healthcare%2C%20model%20accuracy%20and%0Amodel%20explainability%20are%20key%20factors%20for%20user%20adoption.%20Towards%20these%20two%0Agoals%2C%20intrinsically%20interpretable%20deep%20learning%20models%20have%20gained%20popularity%2C%0Afocusing%20on%20accurate%20predictions%20alongside%20faithful%20explanations.%20However%2C%0Athere%20exists%20a%20gap%20in%20the%20human-centeredness%20of%20these%20approaches%2C%20which%20often%0Aproduce%20nuanced%20and%20complex%20explanations%20that%20are%20not%20easily%20actionable%20for%0Adownstream%20users.%20We%20present%20InterpretCC%20%28interpretable%20conditional%0Acomputation%29%2C%20a%20family%20of%20intrinsically%20interpretable%20neural%20networks%20at%20a%0Aunique%20point%20in%20the%20design%20space%20that%20optimizes%20for%20ease%20of%20human%20understanding%0Aand%20explanation%20faithfulness%2C%20while%20maintaining%20comparable%20performance%20to%0Astate-of-the-art%20models.%20InterpretCC%20achieves%20this%20through%20adaptive%20sparse%0Aactivation%20of%20features%20before%20prediction%2C%20allowing%20the%20model%20to%20use%20a%0Adifferent%2C%20minimal%20set%20of%20features%20for%20each%20instance.%20We%20extend%20this%20idea%20into%0Aan%20interpretable%2C%20global%20mixture-of-experts%20%28MoE%29%20model%20that%20allows%20users%20to%0Aspecify%20topics%20of%20interest%2C%20discretely%20separates%20the%20feature%20space%20for%20each%0Adata%20point%20into%20topical%20subnetworks%2C%20and%20adaptively%20and%20sparsely%20activates%0Athese%20topical%20subnetworks%20for%20prediction.%20We%20apply%20InterpretCC%20for%20text%2C%20time%0Aseries%20and%20tabular%20data%20across%20several%20real-world%20datasets%2C%20demonstrating%0Acomparable%20performance%20with%20non-interpretable%20baselines%20and%20outperforming%0Aintrinsically%20interpretable%20baselines.%20Through%20a%20user%20study%20involving%2056%0Ateachers%2C%20InterpretCC%20explanations%20are%20found%20to%20have%20higher%20actionability%20and%0Ausefulness%20over%20other%20intrinsically%20interpretable%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02933v4&entry.124074799=Read"},
{"title": "Zero-Shot Vision Encoder Grafting via LLM Surrogates", "author": "Kaiyu Yue and Vasu Singla and Menglin Jia and John Kirchenbauer and Rifaa Qadri and Zikui Cai and Abhinav Bhatele and Furong Huang and Tom Goldstein", "abstract": "  Vision language models (VLMs) typically pair a modestly sized vision encoder\nwith a large language model (LLM), e.g., Llama-70B, making the decoder the\nprimary computational burden during training. To reduce costs, a potential\npromising strategy is to first train the vision encoder using a small language\nmodel before transferring it to the large one. We construct small \"surrogate\nmodels\" that share the same embedding space and representation language as the\nlarge target LLM by directly inheriting its shallow layers. Vision encoders\ntrained on the surrogate can then be directly transferred to the larger model,\na process we call zero-shot grafting -- when plugged directly into the\nfull-size target LLM, the grafted pair surpasses the encoder-surrogate pair\nand, on some benchmarks, even performs on par with full decoder training with\nthe target LLM. Furthermore, our surrogate training approach reduces overall\nVLM training costs by ~45% when using Llama-70B as the decoder.\n", "link": "http://arxiv.org/abs/2505.22664v1", "date": "2025-05-28", "relevancy": 2.6267, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5433}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5433}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4894}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Vision%20Encoder%20Grafting%20via%20LLM%20Surrogates&body=Title%3A%20Zero-Shot%20Vision%20Encoder%20Grafting%20via%20LLM%20Surrogates%0AAuthor%3A%20Kaiyu%20Yue%20and%20Vasu%20Singla%20and%20Menglin%20Jia%20and%20John%20Kirchenbauer%20and%20Rifaa%20Qadri%20and%20Zikui%20Cai%20and%20Abhinav%20Bhatele%20and%20Furong%20Huang%20and%20Tom%20Goldstein%0AAbstract%3A%20%20%20Vision%20language%20models%20%28VLMs%29%20typically%20pair%20a%20modestly%20sized%20vision%20encoder%0Awith%20a%20large%20language%20model%20%28LLM%29%2C%20e.g.%2C%20Llama-70B%2C%20making%20the%20decoder%20the%0Aprimary%20computational%20burden%20during%20training.%20To%20reduce%20costs%2C%20a%20potential%0Apromising%20strategy%20is%20to%20first%20train%20the%20vision%20encoder%20using%20a%20small%20language%0Amodel%20before%20transferring%20it%20to%20the%20large%20one.%20We%20construct%20small%20%22surrogate%0Amodels%22%20that%20share%20the%20same%20embedding%20space%20and%20representation%20language%20as%20the%0Alarge%20target%20LLM%20by%20directly%20inheriting%20its%20shallow%20layers.%20Vision%20encoders%0Atrained%20on%20the%20surrogate%20can%20then%20be%20directly%20transferred%20to%20the%20larger%20model%2C%0Aa%20process%20we%20call%20zero-shot%20grafting%20--%20when%20plugged%20directly%20into%20the%0Afull-size%20target%20LLM%2C%20the%20grafted%20pair%20surpasses%20the%20encoder-surrogate%20pair%0Aand%2C%20on%20some%20benchmarks%2C%20even%20performs%20on%20par%20with%20full%20decoder%20training%20with%0Athe%20target%20LLM.%20Furthermore%2C%20our%20surrogate%20training%20approach%20reduces%20overall%0AVLM%20training%20costs%20by%20~45%25%20when%20using%20Llama-70B%20as%20the%20decoder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22664v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Vision%2520Encoder%2520Grafting%2520via%2520LLM%2520Surrogates%26entry.906535625%3DKaiyu%2520Yue%2520and%2520Vasu%2520Singla%2520and%2520Menglin%2520Jia%2520and%2520John%2520Kirchenbauer%2520and%2520Rifaa%2520Qadri%2520and%2520Zikui%2520Cai%2520and%2520Abhinav%2520Bhatele%2520and%2520Furong%2520Huang%2520and%2520Tom%2520Goldstein%26entry.1292438233%3D%2520%2520Vision%2520language%2520models%2520%2528VLMs%2529%2520typically%2520pair%2520a%2520modestly%2520sized%2520vision%2520encoder%250Awith%2520a%2520large%2520language%2520model%2520%2528LLM%2529%252C%2520e.g.%252C%2520Llama-70B%252C%2520making%2520the%2520decoder%2520the%250Aprimary%2520computational%2520burden%2520during%2520training.%2520To%2520reduce%2520costs%252C%2520a%2520potential%250Apromising%2520strategy%2520is%2520to%2520first%2520train%2520the%2520vision%2520encoder%2520using%2520a%2520small%2520language%250Amodel%2520before%2520transferring%2520it%2520to%2520the%2520large%2520one.%2520We%2520construct%2520small%2520%2522surrogate%250Amodels%2522%2520that%2520share%2520the%2520same%2520embedding%2520space%2520and%2520representation%2520language%2520as%2520the%250Alarge%2520target%2520LLM%2520by%2520directly%2520inheriting%2520its%2520shallow%2520layers.%2520Vision%2520encoders%250Atrained%2520on%2520the%2520surrogate%2520can%2520then%2520be%2520directly%2520transferred%2520to%2520the%2520larger%2520model%252C%250Aa%2520process%2520we%2520call%2520zero-shot%2520grafting%2520--%2520when%2520plugged%2520directly%2520into%2520the%250Afull-size%2520target%2520LLM%252C%2520the%2520grafted%2520pair%2520surpasses%2520the%2520encoder-surrogate%2520pair%250Aand%252C%2520on%2520some%2520benchmarks%252C%2520even%2520performs%2520on%2520par%2520with%2520full%2520decoder%2520training%2520with%250Athe%2520target%2520LLM.%2520Furthermore%252C%2520our%2520surrogate%2520training%2520approach%2520reduces%2520overall%250AVLM%2520training%2520costs%2520by%2520~45%2525%2520when%2520using%2520Llama-70B%2520as%2520the%2520decoder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22664v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Vision%20Encoder%20Grafting%20via%20LLM%20Surrogates&entry.906535625=Kaiyu%20Yue%20and%20Vasu%20Singla%20and%20Menglin%20Jia%20and%20John%20Kirchenbauer%20and%20Rifaa%20Qadri%20and%20Zikui%20Cai%20and%20Abhinav%20Bhatele%20and%20Furong%20Huang%20and%20Tom%20Goldstein&entry.1292438233=%20%20Vision%20language%20models%20%28VLMs%29%20typically%20pair%20a%20modestly%20sized%20vision%20encoder%0Awith%20a%20large%20language%20model%20%28LLM%29%2C%20e.g.%2C%20Llama-70B%2C%20making%20the%20decoder%20the%0Aprimary%20computational%20burden%20during%20training.%20To%20reduce%20costs%2C%20a%20potential%0Apromising%20strategy%20is%20to%20first%20train%20the%20vision%20encoder%20using%20a%20small%20language%0Amodel%20before%20transferring%20it%20to%20the%20large%20one.%20We%20construct%20small%20%22surrogate%0Amodels%22%20that%20share%20the%20same%20embedding%20space%20and%20representation%20language%20as%20the%0Alarge%20target%20LLM%20by%20directly%20inheriting%20its%20shallow%20layers.%20Vision%20encoders%0Atrained%20on%20the%20surrogate%20can%20then%20be%20directly%20transferred%20to%20the%20larger%20model%2C%0Aa%20process%20we%20call%20zero-shot%20grafting%20--%20when%20plugged%20directly%20into%20the%0Afull-size%20target%20LLM%2C%20the%20grafted%20pair%20surpasses%20the%20encoder-surrogate%20pair%0Aand%2C%20on%20some%20benchmarks%2C%20even%20performs%20on%20par%20with%20full%20decoder%20training%20with%0Athe%20target%20LLM.%20Furthermore%2C%20our%20surrogate%20training%20approach%20reduces%20overall%0AVLM%20training%20costs%20by%20~45%25%20when%20using%20Llama-70B%20as%20the%20decoder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22664v1&entry.124074799=Read"},
{"title": "Fostering Video Reasoning via Next-Event Prediction", "author": "Haonan Wang and Hongfu Liu and Xiangyan Liu and Chao Du and Kenji Kawaguchi and Ye Wang and Tianyu Pang", "abstract": "  Next-token prediction serves as the foundational learning task enabling\nreasoning in LLMs. But what should the learning task be when aiming to equip\nMLLMs with temporal reasoning capabilities over video inputs? Existing tasks\nsuch as video question answering often rely on annotations from humans or much\nstronger MLLMs, while video captioning tends to entangle temporal reasoning\nwith spatial information. To address this gap, we propose next-event prediction\n(NEP), a learning task that harnesses future video segments as a rich,\nself-supervised signal to foster temporal reasoning. We segment each video into\npast and future frames: the MLLM takes the past frames as input and predicts a\nsummary of events derived from the future frames, thereby encouraging the model\nto reason temporally in order to complete the task. To support this task, we\ncurate V1-33K, a dataset comprising 33,000 automatically extracted video\nsegments spanning diverse real-world scenarios. We further explore a range of\nvideo instruction-tuning strategies to study their effects on temporal\nreasoning. To evaluate progress, we introduce FutureBench to assess coherence\nin predicting unseen future events. Experiments validate that NEP offers a\nscalable and effective training paradigm for fostering temporal reasoning in\nMLLMs.\n", "link": "http://arxiv.org/abs/2505.22457v1", "date": "2025-05-28", "relevancy": 2.6156, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5289}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5202}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fostering%20Video%20Reasoning%20via%20Next-Event%20Prediction&body=Title%3A%20Fostering%20Video%20Reasoning%20via%20Next-Event%20Prediction%0AAuthor%3A%20Haonan%20Wang%20and%20Hongfu%20Liu%20and%20Xiangyan%20Liu%20and%20Chao%20Du%20and%20Kenji%20Kawaguchi%20and%20Ye%20Wang%20and%20Tianyu%20Pang%0AAbstract%3A%20%20%20Next-token%20prediction%20serves%20as%20the%20foundational%20learning%20task%20enabling%0Areasoning%20in%20LLMs.%20But%20what%20should%20the%20learning%20task%20be%20when%20aiming%20to%20equip%0AMLLMs%20with%20temporal%20reasoning%20capabilities%20over%20video%20inputs%3F%20Existing%20tasks%0Asuch%20as%20video%20question%20answering%20often%20rely%20on%20annotations%20from%20humans%20or%20much%0Astronger%20MLLMs%2C%20while%20video%20captioning%20tends%20to%20entangle%20temporal%20reasoning%0Awith%20spatial%20information.%20To%20address%20this%20gap%2C%20we%20propose%20next-event%20prediction%0A%28NEP%29%2C%20a%20learning%20task%20that%20harnesses%20future%20video%20segments%20as%20a%20rich%2C%0Aself-supervised%20signal%20to%20foster%20temporal%20reasoning.%20We%20segment%20each%20video%20into%0Apast%20and%20future%20frames%3A%20the%20MLLM%20takes%20the%20past%20frames%20as%20input%20and%20predicts%20a%0Asummary%20of%20events%20derived%20from%20the%20future%20frames%2C%20thereby%20encouraging%20the%20model%0Ato%20reason%20temporally%20in%20order%20to%20complete%20the%20task.%20To%20support%20this%20task%2C%20we%0Acurate%20V1-33K%2C%20a%20dataset%20comprising%2033%2C000%20automatically%20extracted%20video%0Asegments%20spanning%20diverse%20real-world%20scenarios.%20We%20further%20explore%20a%20range%20of%0Avideo%20instruction-tuning%20strategies%20to%20study%20their%20effects%20on%20temporal%0Areasoning.%20To%20evaluate%20progress%2C%20we%20introduce%20FutureBench%20to%20assess%20coherence%0Ain%20predicting%20unseen%20future%20events.%20Experiments%20validate%20that%20NEP%20offers%20a%0Ascalable%20and%20effective%20training%20paradigm%20for%20fostering%20temporal%20reasoning%20in%0AMLLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22457v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFostering%2520Video%2520Reasoning%2520via%2520Next-Event%2520Prediction%26entry.906535625%3DHaonan%2520Wang%2520and%2520Hongfu%2520Liu%2520and%2520Xiangyan%2520Liu%2520and%2520Chao%2520Du%2520and%2520Kenji%2520Kawaguchi%2520and%2520Ye%2520Wang%2520and%2520Tianyu%2520Pang%26entry.1292438233%3D%2520%2520Next-token%2520prediction%2520serves%2520as%2520the%2520foundational%2520learning%2520task%2520enabling%250Areasoning%2520in%2520LLMs.%2520But%2520what%2520should%2520the%2520learning%2520task%2520be%2520when%2520aiming%2520to%2520equip%250AMLLMs%2520with%2520temporal%2520reasoning%2520capabilities%2520over%2520video%2520inputs%253F%2520Existing%2520tasks%250Asuch%2520as%2520video%2520question%2520answering%2520often%2520rely%2520on%2520annotations%2520from%2520humans%2520or%2520much%250Astronger%2520MLLMs%252C%2520while%2520video%2520captioning%2520tends%2520to%2520entangle%2520temporal%2520reasoning%250Awith%2520spatial%2520information.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520next-event%2520prediction%250A%2528NEP%2529%252C%2520a%2520learning%2520task%2520that%2520harnesses%2520future%2520video%2520segments%2520as%2520a%2520rich%252C%250Aself-supervised%2520signal%2520to%2520foster%2520temporal%2520reasoning.%2520We%2520segment%2520each%2520video%2520into%250Apast%2520and%2520future%2520frames%253A%2520the%2520MLLM%2520takes%2520the%2520past%2520frames%2520as%2520input%2520and%2520predicts%2520a%250Asummary%2520of%2520events%2520derived%2520from%2520the%2520future%2520frames%252C%2520thereby%2520encouraging%2520the%2520model%250Ato%2520reason%2520temporally%2520in%2520order%2520to%2520complete%2520the%2520task.%2520To%2520support%2520this%2520task%252C%2520we%250Acurate%2520V1-33K%252C%2520a%2520dataset%2520comprising%252033%252C000%2520automatically%2520extracted%2520video%250Asegments%2520spanning%2520diverse%2520real-world%2520scenarios.%2520We%2520further%2520explore%2520a%2520range%2520of%250Avideo%2520instruction-tuning%2520strategies%2520to%2520study%2520their%2520effects%2520on%2520temporal%250Areasoning.%2520To%2520evaluate%2520progress%252C%2520we%2520introduce%2520FutureBench%2520to%2520assess%2520coherence%250Ain%2520predicting%2520unseen%2520future%2520events.%2520Experiments%2520validate%2520that%2520NEP%2520offers%2520a%250Ascalable%2520and%2520effective%2520training%2520paradigm%2520for%2520fostering%2520temporal%2520reasoning%2520in%250AMLLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22457v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fostering%20Video%20Reasoning%20via%20Next-Event%20Prediction&entry.906535625=Haonan%20Wang%20and%20Hongfu%20Liu%20and%20Xiangyan%20Liu%20and%20Chao%20Du%20and%20Kenji%20Kawaguchi%20and%20Ye%20Wang%20and%20Tianyu%20Pang&entry.1292438233=%20%20Next-token%20prediction%20serves%20as%20the%20foundational%20learning%20task%20enabling%0Areasoning%20in%20LLMs.%20But%20what%20should%20the%20learning%20task%20be%20when%20aiming%20to%20equip%0AMLLMs%20with%20temporal%20reasoning%20capabilities%20over%20video%20inputs%3F%20Existing%20tasks%0Asuch%20as%20video%20question%20answering%20often%20rely%20on%20annotations%20from%20humans%20or%20much%0Astronger%20MLLMs%2C%20while%20video%20captioning%20tends%20to%20entangle%20temporal%20reasoning%0Awith%20spatial%20information.%20To%20address%20this%20gap%2C%20we%20propose%20next-event%20prediction%0A%28NEP%29%2C%20a%20learning%20task%20that%20harnesses%20future%20video%20segments%20as%20a%20rich%2C%0Aself-supervised%20signal%20to%20foster%20temporal%20reasoning.%20We%20segment%20each%20video%20into%0Apast%20and%20future%20frames%3A%20the%20MLLM%20takes%20the%20past%20frames%20as%20input%20and%20predicts%20a%0Asummary%20of%20events%20derived%20from%20the%20future%20frames%2C%20thereby%20encouraging%20the%20model%0Ato%20reason%20temporally%20in%20order%20to%20complete%20the%20task.%20To%20support%20this%20task%2C%20we%0Acurate%20V1-33K%2C%20a%20dataset%20comprising%2033%2C000%20automatically%20extracted%20video%0Asegments%20spanning%20diverse%20real-world%20scenarios.%20We%20further%20explore%20a%20range%20of%0Avideo%20instruction-tuning%20strategies%20to%20study%20their%20effects%20on%20temporal%0Areasoning.%20To%20evaluate%20progress%2C%20we%20introduce%20FutureBench%20to%20assess%20coherence%0Ain%20predicting%20unseen%20future%20events.%20Experiments%20validate%20that%20NEP%20offers%20a%0Ascalable%20and%20effective%20training%20paradigm%20for%20fostering%20temporal%20reasoning%20in%0AMLLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22457v1&entry.124074799=Read"},
{"title": "Copresheaf Topological Neural Networks: A Generalized Deep Learning\n  Framework", "author": "Mustafa Hajij and Lennart Bastian and Sarah Osentoski and Hardik Kabaria and John L. Davenport and Sheik Dawood and Balaji Cherukuri and Joseph G. Kocheemoolayil and Nastaran Shahmansouri and Adrian Lew and Theodore Papamarkou and Tolga Birdal", "abstract": "  We introduce copresheaf topological neural networks (CTNNs), a powerful and\nunifying framework that encapsulates a wide spectrum of deep learning\narchitectures, designed to operate on structured data: including images, point\nclouds, graphs, meshes, and topological manifolds. While deep learning has\nprofoundly impacted domains ranging from digital assistants to autonomous\nsystems, the principled design of neural architectures tailored to specific\ntasks and data types remains one of the field's most persistent open\nchallenges. CTNNs address this gap by grounding model design in the language of\ncopresheaves, a concept from algebraic topology that generalizes and subsumes\nmost practical deep learning models in use today. This abstract yet\nconstructive formulation yields a rich design space from which theoretically\nsound and practically effective solutions can be derived to tackle core\nchallenges in representation learning: long-range dependencies, oversmoothing,\nheterophily, and non-Euclidean domains. Our empirical results on structured\ndata benchmarks demonstrate that CTNNs consistently outperform conventional\nbaselines, particularly in tasks requiring hierarchical or localized\nsensitivity. These results underscore CTNNs as a principled, multi-scale\nfoundation for the next generation of deep learning architectures.\n", "link": "http://arxiv.org/abs/2505.21251v2", "date": "2025-05-28", "relevancy": 2.6092, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5387}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5202}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Copresheaf%20Topological%20Neural%20Networks%3A%20A%20Generalized%20Deep%20Learning%0A%20%20Framework&body=Title%3A%20Copresheaf%20Topological%20Neural%20Networks%3A%20A%20Generalized%20Deep%20Learning%0A%20%20Framework%0AAuthor%3A%20Mustafa%20Hajij%20and%20Lennart%20Bastian%20and%20Sarah%20Osentoski%20and%20Hardik%20Kabaria%20and%20John%20L.%20Davenport%20and%20Sheik%20Dawood%20and%20Balaji%20Cherukuri%20and%20Joseph%20G.%20Kocheemoolayil%20and%20Nastaran%20Shahmansouri%20and%20Adrian%20Lew%20and%20Theodore%20Papamarkou%20and%20Tolga%20Birdal%0AAbstract%3A%20%20%20We%20introduce%20copresheaf%20topological%20neural%20networks%20%28CTNNs%29%2C%20a%20powerful%20and%0Aunifying%20framework%20that%20encapsulates%20a%20wide%20spectrum%20of%20deep%20learning%0Aarchitectures%2C%20designed%20to%20operate%20on%20structured%20data%3A%20including%20images%2C%20point%0Aclouds%2C%20graphs%2C%20meshes%2C%20and%20topological%20manifolds.%20While%20deep%20learning%20has%0Aprofoundly%20impacted%20domains%20ranging%20from%20digital%20assistants%20to%20autonomous%0Asystems%2C%20the%20principled%20design%20of%20neural%20architectures%20tailored%20to%20specific%0Atasks%20and%20data%20types%20remains%20one%20of%20the%20field%27s%20most%20persistent%20open%0Achallenges.%20CTNNs%20address%20this%20gap%20by%20grounding%20model%20design%20in%20the%20language%20of%0Acopresheaves%2C%20a%20concept%20from%20algebraic%20topology%20that%20generalizes%20and%20subsumes%0Amost%20practical%20deep%20learning%20models%20in%20use%20today.%20This%20abstract%20yet%0Aconstructive%20formulation%20yields%20a%20rich%20design%20space%20from%20which%20theoretically%0Asound%20and%20practically%20effective%20solutions%20can%20be%20derived%20to%20tackle%20core%0Achallenges%20in%20representation%20learning%3A%20long-range%20dependencies%2C%20oversmoothing%2C%0Aheterophily%2C%20and%20non-Euclidean%20domains.%20Our%20empirical%20results%20on%20structured%0Adata%20benchmarks%20demonstrate%20that%20CTNNs%20consistently%20outperform%20conventional%0Abaselines%2C%20particularly%20in%20tasks%20requiring%20hierarchical%20or%20localized%0Asensitivity.%20These%20results%20underscore%20CTNNs%20as%20a%20principled%2C%20multi-scale%0Afoundation%20for%20the%20next%20generation%20of%20deep%20learning%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21251v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCopresheaf%2520Topological%2520Neural%2520Networks%253A%2520A%2520Generalized%2520Deep%2520Learning%250A%2520%2520Framework%26entry.906535625%3DMustafa%2520Hajij%2520and%2520Lennart%2520Bastian%2520and%2520Sarah%2520Osentoski%2520and%2520Hardik%2520Kabaria%2520and%2520John%2520L.%2520Davenport%2520and%2520Sheik%2520Dawood%2520and%2520Balaji%2520Cherukuri%2520and%2520Joseph%2520G.%2520Kocheemoolayil%2520and%2520Nastaran%2520Shahmansouri%2520and%2520Adrian%2520Lew%2520and%2520Theodore%2520Papamarkou%2520and%2520Tolga%2520Birdal%26entry.1292438233%3D%2520%2520We%2520introduce%2520copresheaf%2520topological%2520neural%2520networks%2520%2528CTNNs%2529%252C%2520a%2520powerful%2520and%250Aunifying%2520framework%2520that%2520encapsulates%2520a%2520wide%2520spectrum%2520of%2520deep%2520learning%250Aarchitectures%252C%2520designed%2520to%2520operate%2520on%2520structured%2520data%253A%2520including%2520images%252C%2520point%250Aclouds%252C%2520graphs%252C%2520meshes%252C%2520and%2520topological%2520manifolds.%2520While%2520deep%2520learning%2520has%250Aprofoundly%2520impacted%2520domains%2520ranging%2520from%2520digital%2520assistants%2520to%2520autonomous%250Asystems%252C%2520the%2520principled%2520design%2520of%2520neural%2520architectures%2520tailored%2520to%2520specific%250Atasks%2520and%2520data%2520types%2520remains%2520one%2520of%2520the%2520field%2527s%2520most%2520persistent%2520open%250Achallenges.%2520CTNNs%2520address%2520this%2520gap%2520by%2520grounding%2520model%2520design%2520in%2520the%2520language%2520of%250Acopresheaves%252C%2520a%2520concept%2520from%2520algebraic%2520topology%2520that%2520generalizes%2520and%2520subsumes%250Amost%2520practical%2520deep%2520learning%2520models%2520in%2520use%2520today.%2520This%2520abstract%2520yet%250Aconstructive%2520formulation%2520yields%2520a%2520rich%2520design%2520space%2520from%2520which%2520theoretically%250Asound%2520and%2520practically%2520effective%2520solutions%2520can%2520be%2520derived%2520to%2520tackle%2520core%250Achallenges%2520in%2520representation%2520learning%253A%2520long-range%2520dependencies%252C%2520oversmoothing%252C%250Aheterophily%252C%2520and%2520non-Euclidean%2520domains.%2520Our%2520empirical%2520results%2520on%2520structured%250Adata%2520benchmarks%2520demonstrate%2520that%2520CTNNs%2520consistently%2520outperform%2520conventional%250Abaselines%252C%2520particularly%2520in%2520tasks%2520requiring%2520hierarchical%2520or%2520localized%250Asensitivity.%2520These%2520results%2520underscore%2520CTNNs%2520as%2520a%2520principled%252C%2520multi-scale%250Afoundation%2520for%2520the%2520next%2520generation%2520of%2520deep%2520learning%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21251v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Copresheaf%20Topological%20Neural%20Networks%3A%20A%20Generalized%20Deep%20Learning%0A%20%20Framework&entry.906535625=Mustafa%20Hajij%20and%20Lennart%20Bastian%20and%20Sarah%20Osentoski%20and%20Hardik%20Kabaria%20and%20John%20L.%20Davenport%20and%20Sheik%20Dawood%20and%20Balaji%20Cherukuri%20and%20Joseph%20G.%20Kocheemoolayil%20and%20Nastaran%20Shahmansouri%20and%20Adrian%20Lew%20and%20Theodore%20Papamarkou%20and%20Tolga%20Birdal&entry.1292438233=%20%20We%20introduce%20copresheaf%20topological%20neural%20networks%20%28CTNNs%29%2C%20a%20powerful%20and%0Aunifying%20framework%20that%20encapsulates%20a%20wide%20spectrum%20of%20deep%20learning%0Aarchitectures%2C%20designed%20to%20operate%20on%20structured%20data%3A%20including%20images%2C%20point%0Aclouds%2C%20graphs%2C%20meshes%2C%20and%20topological%20manifolds.%20While%20deep%20learning%20has%0Aprofoundly%20impacted%20domains%20ranging%20from%20digital%20assistants%20to%20autonomous%0Asystems%2C%20the%20principled%20design%20of%20neural%20architectures%20tailored%20to%20specific%0Atasks%20and%20data%20types%20remains%20one%20of%20the%20field%27s%20most%20persistent%20open%0Achallenges.%20CTNNs%20address%20this%20gap%20by%20grounding%20model%20design%20in%20the%20language%20of%0Acopresheaves%2C%20a%20concept%20from%20algebraic%20topology%20that%20generalizes%20and%20subsumes%0Amost%20practical%20deep%20learning%20models%20in%20use%20today.%20This%20abstract%20yet%0Aconstructive%20formulation%20yields%20a%20rich%20design%20space%20from%20which%20theoretically%0Asound%20and%20practically%20effective%20solutions%20can%20be%20derived%20to%20tackle%20core%0Achallenges%20in%20representation%20learning%3A%20long-range%20dependencies%2C%20oversmoothing%2C%0Aheterophily%2C%20and%20non-Euclidean%20domains.%20Our%20empirical%20results%20on%20structured%0Adata%20benchmarks%20demonstrate%20that%20CTNNs%20consistently%20outperform%20conventional%0Abaselines%2C%20particularly%20in%20tasks%20requiring%20hierarchical%20or%20localized%0Asensitivity.%20These%20results%20underscore%20CTNNs%20as%20a%20principled%2C%20multi-scale%0Afoundation%20for%20the%20next%20generation%20of%20deep%20learning%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21251v2&entry.124074799=Read"},
{"title": "Directed Homophily-Aware Graph Neural Network", "author": "Aihu Zhang and Jiaxing Xu and Mengcheng Lan and Shili Xiang and Yiping Ke", "abstract": "  Graph Neural Networks (GNNs) have achieved significant success in various\nlearning tasks on graph-structured data. Nevertheless, most GNNs struggle to\ngeneralize to heterophilic neighborhoods. Additionally, many GNNs ignore the\ndirectional nature of real-world graphs, resulting in suboptimal performance on\ndirected graphs with asymmetric structures. In this work, we propose Directed\nHomophily-aware Graph Neural Network (DHGNN), a novel framework that addresses\nthese limitations by incorporating homophily-aware and direction-sensitive\ncomponents. DHGNN employs a resettable gating mechanism to adaptively modulate\nmessage contributions based on homophily levels and informativeness, and a\nstructure-aware noise-tolerant fusion module to effectively integrate node\nrepresentations from the original and reverse directions. Extensive experiments\non both homophilic and heterophilic directed graph datasets demonstrate that\nDHGNN outperforms state-of-the-art methods in node classification and link\nprediction. In particular, DHGNN improves over the best baseline by up to\n15.07% in link prediction. Our analysis further shows that the gating mechanism\ncaptures directional homophily gaps and fluctuating homophily across layers,\nproviding deeper insights into message-passing behavior on complex graph\nstructures.\n", "link": "http://arxiv.org/abs/2505.22362v1", "date": "2025-05-28", "relevancy": 2.5991, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5496}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5095}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Directed%20Homophily-Aware%20Graph%20Neural%20Network&body=Title%3A%20Directed%20Homophily-Aware%20Graph%20Neural%20Network%0AAuthor%3A%20Aihu%20Zhang%20and%20Jiaxing%20Xu%20and%20Mengcheng%20Lan%20and%20Shili%20Xiang%20and%20Yiping%20Ke%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20achieved%20significant%20success%20in%20various%0Alearning%20tasks%20on%20graph-structured%20data.%20Nevertheless%2C%20most%20GNNs%20struggle%20to%0Ageneralize%20to%20heterophilic%20neighborhoods.%20Additionally%2C%20many%20GNNs%20ignore%20the%0Adirectional%20nature%20of%20real-world%20graphs%2C%20resulting%20in%20suboptimal%20performance%20on%0Adirected%20graphs%20with%20asymmetric%20structures.%20In%20this%20work%2C%20we%20propose%20Directed%0AHomophily-aware%20Graph%20Neural%20Network%20%28DHGNN%29%2C%20a%20novel%20framework%20that%20addresses%0Athese%20limitations%20by%20incorporating%20homophily-aware%20and%20direction-sensitive%0Acomponents.%20DHGNN%20employs%20a%20resettable%20gating%20mechanism%20to%20adaptively%20modulate%0Amessage%20contributions%20based%20on%20homophily%20levels%20and%20informativeness%2C%20and%20a%0Astructure-aware%20noise-tolerant%20fusion%20module%20to%20effectively%20integrate%20node%0Arepresentations%20from%20the%20original%20and%20reverse%20directions.%20Extensive%20experiments%0Aon%20both%20homophilic%20and%20heterophilic%20directed%20graph%20datasets%20demonstrate%20that%0ADHGNN%20outperforms%20state-of-the-art%20methods%20in%20node%20classification%20and%20link%0Aprediction.%20In%20particular%2C%20DHGNN%20improves%20over%20the%20best%20baseline%20by%20up%20to%0A15.07%25%20in%20link%20prediction.%20Our%20analysis%20further%20shows%20that%20the%20gating%20mechanism%0Acaptures%20directional%20homophily%20gaps%20and%20fluctuating%20homophily%20across%20layers%2C%0Aproviding%20deeper%20insights%20into%20message-passing%20behavior%20on%20complex%20graph%0Astructures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22362v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDirected%2520Homophily-Aware%2520Graph%2520Neural%2520Network%26entry.906535625%3DAihu%2520Zhang%2520and%2520Jiaxing%2520Xu%2520and%2520Mengcheng%2520Lan%2520and%2520Shili%2520Xiang%2520and%2520Yiping%2520Ke%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520achieved%2520significant%2520success%2520in%2520various%250Alearning%2520tasks%2520on%2520graph-structured%2520data.%2520Nevertheless%252C%2520most%2520GNNs%2520struggle%2520to%250Ageneralize%2520to%2520heterophilic%2520neighborhoods.%2520Additionally%252C%2520many%2520GNNs%2520ignore%2520the%250Adirectional%2520nature%2520of%2520real-world%2520graphs%252C%2520resulting%2520in%2520suboptimal%2520performance%2520on%250Adirected%2520graphs%2520with%2520asymmetric%2520structures.%2520In%2520this%2520work%252C%2520we%2520propose%2520Directed%250AHomophily-aware%2520Graph%2520Neural%2520Network%2520%2528DHGNN%2529%252C%2520a%2520novel%2520framework%2520that%2520addresses%250Athese%2520limitations%2520by%2520incorporating%2520homophily-aware%2520and%2520direction-sensitive%250Acomponents.%2520DHGNN%2520employs%2520a%2520resettable%2520gating%2520mechanism%2520to%2520adaptively%2520modulate%250Amessage%2520contributions%2520based%2520on%2520homophily%2520levels%2520and%2520informativeness%252C%2520and%2520a%250Astructure-aware%2520noise-tolerant%2520fusion%2520module%2520to%2520effectively%2520integrate%2520node%250Arepresentations%2520from%2520the%2520original%2520and%2520reverse%2520directions.%2520Extensive%2520experiments%250Aon%2520both%2520homophilic%2520and%2520heterophilic%2520directed%2520graph%2520datasets%2520demonstrate%2520that%250ADHGNN%2520outperforms%2520state-of-the-art%2520methods%2520in%2520node%2520classification%2520and%2520link%250Aprediction.%2520In%2520particular%252C%2520DHGNN%2520improves%2520over%2520the%2520best%2520baseline%2520by%2520up%2520to%250A15.07%2525%2520in%2520link%2520prediction.%2520Our%2520analysis%2520further%2520shows%2520that%2520the%2520gating%2520mechanism%250Acaptures%2520directional%2520homophily%2520gaps%2520and%2520fluctuating%2520homophily%2520across%2520layers%252C%250Aproviding%2520deeper%2520insights%2520into%2520message-passing%2520behavior%2520on%2520complex%2520graph%250Astructures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22362v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Directed%20Homophily-Aware%20Graph%20Neural%20Network&entry.906535625=Aihu%20Zhang%20and%20Jiaxing%20Xu%20and%20Mengcheng%20Lan%20and%20Shili%20Xiang%20and%20Yiping%20Ke&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20achieved%20significant%20success%20in%20various%0Alearning%20tasks%20on%20graph-structured%20data.%20Nevertheless%2C%20most%20GNNs%20struggle%20to%0Ageneralize%20to%20heterophilic%20neighborhoods.%20Additionally%2C%20many%20GNNs%20ignore%20the%0Adirectional%20nature%20of%20real-world%20graphs%2C%20resulting%20in%20suboptimal%20performance%20on%0Adirected%20graphs%20with%20asymmetric%20structures.%20In%20this%20work%2C%20we%20propose%20Directed%0AHomophily-aware%20Graph%20Neural%20Network%20%28DHGNN%29%2C%20a%20novel%20framework%20that%20addresses%0Athese%20limitations%20by%20incorporating%20homophily-aware%20and%20direction-sensitive%0Acomponents.%20DHGNN%20employs%20a%20resettable%20gating%20mechanism%20to%20adaptively%20modulate%0Amessage%20contributions%20based%20on%20homophily%20levels%20and%20informativeness%2C%20and%20a%0Astructure-aware%20noise-tolerant%20fusion%20module%20to%20effectively%20integrate%20node%0Arepresentations%20from%20the%20original%20and%20reverse%20directions.%20Extensive%20experiments%0Aon%20both%20homophilic%20and%20heterophilic%20directed%20graph%20datasets%20demonstrate%20that%0ADHGNN%20outperforms%20state-of-the-art%20methods%20in%20node%20classification%20and%20link%0Aprediction.%20In%20particular%2C%20DHGNN%20improves%20over%20the%20best%20baseline%20by%20up%20to%0A15.07%25%20in%20link%20prediction.%20Our%20analysis%20further%20shows%20that%20the%20gating%20mechanism%0Acaptures%20directional%20homophily%20gaps%20and%20fluctuating%20homophily%20across%20layers%2C%0Aproviding%20deeper%20insights%20into%20message-passing%20behavior%20on%20complex%20graph%0Astructures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22362v1&entry.124074799=Read"},
{"title": "VisCRA: A Visual Chain Reasoning Attack for Jailbreaking Multimodal\n  Large Language Models", "author": "Bingrui Sima and Linhua Cong and Wenxuan Wang and Kun He", "abstract": "  The emergence of Multimodal Large Language Models (MLRMs) has enabled\nsophisticated visual reasoning capabilities by integrating reinforcement\nlearning and Chain-of-Thought (CoT) supervision. However, while these enhanced\nreasoning capabilities improve performance, they also introduce new and\nunderexplored safety risks. In this work, we systematically investigate the\nsecurity implications of advanced visual reasoning in MLRMs. Our analysis\nreveals a fundamental trade-off: as visual reasoning improves, models become\nmore vulnerable to jailbreak attacks. Motivated by this critical finding, we\nintroduce VisCRA (Visual Chain Reasoning Attack), a novel jailbreak framework\nthat exploits the visual reasoning chains to bypass safety mechanisms. VisCRA\ncombines targeted visual attention masking with a two-stage reasoning induction\nstrategy to precisely control harmful outputs. Extensive experiments\ndemonstrate VisCRA's significant effectiveness, achieving high attack success\nrates on leading closed-source MLRMs: 76.48% on Gemini 2.0 Flash Thinking,\n68.56% on QvQ-Max, and 56.60% on GPT-4o. Our findings highlight a critical\ninsight: the very capability that empowers MLRMs -- their visual reasoning --\ncan also serve as an attack vector, posing significant security risks.\n", "link": "http://arxiv.org/abs/2505.19684v2", "date": "2025-05-28", "relevancy": 2.5698, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5204}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5204}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisCRA%3A%20A%20Visual%20Chain%20Reasoning%20Attack%20for%20Jailbreaking%20Multimodal%0A%20%20Large%20Language%20Models&body=Title%3A%20VisCRA%3A%20A%20Visual%20Chain%20Reasoning%20Attack%20for%20Jailbreaking%20Multimodal%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Bingrui%20Sima%20and%20Linhua%20Cong%20and%20Wenxuan%20Wang%20and%20Kun%20He%0AAbstract%3A%20%20%20The%20emergence%20of%20Multimodal%20Large%20Language%20Models%20%28MLRMs%29%20has%20enabled%0Asophisticated%20visual%20reasoning%20capabilities%20by%20integrating%20reinforcement%0Alearning%20and%20Chain-of-Thought%20%28CoT%29%20supervision.%20However%2C%20while%20these%20enhanced%0Areasoning%20capabilities%20improve%20performance%2C%20they%20also%20introduce%20new%20and%0Aunderexplored%20safety%20risks.%20In%20this%20work%2C%20we%20systematically%20investigate%20the%0Asecurity%20implications%20of%20advanced%20visual%20reasoning%20in%20MLRMs.%20Our%20analysis%0Areveals%20a%20fundamental%20trade-off%3A%20as%20visual%20reasoning%20improves%2C%20models%20become%0Amore%20vulnerable%20to%20jailbreak%20attacks.%20Motivated%20by%20this%20critical%20finding%2C%20we%0Aintroduce%20VisCRA%20%28Visual%20Chain%20Reasoning%20Attack%29%2C%20a%20novel%20jailbreak%20framework%0Athat%20exploits%20the%20visual%20reasoning%20chains%20to%20bypass%20safety%20mechanisms.%20VisCRA%0Acombines%20targeted%20visual%20attention%20masking%20with%20a%20two-stage%20reasoning%20induction%0Astrategy%20to%20precisely%20control%20harmful%20outputs.%20Extensive%20experiments%0Ademonstrate%20VisCRA%27s%20significant%20effectiveness%2C%20achieving%20high%20attack%20success%0Arates%20on%20leading%20closed-source%20MLRMs%3A%2076.48%25%20on%20Gemini%202.0%20Flash%20Thinking%2C%0A68.56%25%20on%20QvQ-Max%2C%20and%2056.60%25%20on%20GPT-4o.%20Our%20findings%20highlight%20a%20critical%0Ainsight%3A%20the%20very%20capability%20that%20empowers%20MLRMs%20--%20their%20visual%20reasoning%20--%0Acan%20also%20serve%20as%20an%20attack%20vector%2C%20posing%20significant%20security%20risks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19684v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisCRA%253A%2520A%2520Visual%2520Chain%2520Reasoning%2520Attack%2520for%2520Jailbreaking%2520Multimodal%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DBingrui%2520Sima%2520and%2520Linhua%2520Cong%2520and%2520Wenxuan%2520Wang%2520and%2520Kun%2520He%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLRMs%2529%2520has%2520enabled%250Asophisticated%2520visual%2520reasoning%2520capabilities%2520by%2520integrating%2520reinforcement%250Alearning%2520and%2520Chain-of-Thought%2520%2528CoT%2529%2520supervision.%2520However%252C%2520while%2520these%2520enhanced%250Areasoning%2520capabilities%2520improve%2520performance%252C%2520they%2520also%2520introduce%2520new%2520and%250Aunderexplored%2520safety%2520risks.%2520In%2520this%2520work%252C%2520we%2520systematically%2520investigate%2520the%250Asecurity%2520implications%2520of%2520advanced%2520visual%2520reasoning%2520in%2520MLRMs.%2520Our%2520analysis%250Areveals%2520a%2520fundamental%2520trade-off%253A%2520as%2520visual%2520reasoning%2520improves%252C%2520models%2520become%250Amore%2520vulnerable%2520to%2520jailbreak%2520attacks.%2520Motivated%2520by%2520this%2520critical%2520finding%252C%2520we%250Aintroduce%2520VisCRA%2520%2528Visual%2520Chain%2520Reasoning%2520Attack%2529%252C%2520a%2520novel%2520jailbreak%2520framework%250Athat%2520exploits%2520the%2520visual%2520reasoning%2520chains%2520to%2520bypass%2520safety%2520mechanisms.%2520VisCRA%250Acombines%2520targeted%2520visual%2520attention%2520masking%2520with%2520a%2520two-stage%2520reasoning%2520induction%250Astrategy%2520to%2520precisely%2520control%2520harmful%2520outputs.%2520Extensive%2520experiments%250Ademonstrate%2520VisCRA%2527s%2520significant%2520effectiveness%252C%2520achieving%2520high%2520attack%2520success%250Arates%2520on%2520leading%2520closed-source%2520MLRMs%253A%252076.48%2525%2520on%2520Gemini%25202.0%2520Flash%2520Thinking%252C%250A68.56%2525%2520on%2520QvQ-Max%252C%2520and%252056.60%2525%2520on%2520GPT-4o.%2520Our%2520findings%2520highlight%2520a%2520critical%250Ainsight%253A%2520the%2520very%2520capability%2520that%2520empowers%2520MLRMs%2520--%2520their%2520visual%2520reasoning%2520--%250Acan%2520also%2520serve%2520as%2520an%2520attack%2520vector%252C%2520posing%2520significant%2520security%2520risks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19684v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisCRA%3A%20A%20Visual%20Chain%20Reasoning%20Attack%20for%20Jailbreaking%20Multimodal%0A%20%20Large%20Language%20Models&entry.906535625=Bingrui%20Sima%20and%20Linhua%20Cong%20and%20Wenxuan%20Wang%20and%20Kun%20He&entry.1292438233=%20%20The%20emergence%20of%20Multimodal%20Large%20Language%20Models%20%28MLRMs%29%20has%20enabled%0Asophisticated%20visual%20reasoning%20capabilities%20by%20integrating%20reinforcement%0Alearning%20and%20Chain-of-Thought%20%28CoT%29%20supervision.%20However%2C%20while%20these%20enhanced%0Areasoning%20capabilities%20improve%20performance%2C%20they%20also%20introduce%20new%20and%0Aunderexplored%20safety%20risks.%20In%20this%20work%2C%20we%20systematically%20investigate%20the%0Asecurity%20implications%20of%20advanced%20visual%20reasoning%20in%20MLRMs.%20Our%20analysis%0Areveals%20a%20fundamental%20trade-off%3A%20as%20visual%20reasoning%20improves%2C%20models%20become%0Amore%20vulnerable%20to%20jailbreak%20attacks.%20Motivated%20by%20this%20critical%20finding%2C%20we%0Aintroduce%20VisCRA%20%28Visual%20Chain%20Reasoning%20Attack%29%2C%20a%20novel%20jailbreak%20framework%0Athat%20exploits%20the%20visual%20reasoning%20chains%20to%20bypass%20safety%20mechanisms.%20VisCRA%0Acombines%20targeted%20visual%20attention%20masking%20with%20a%20two-stage%20reasoning%20induction%0Astrategy%20to%20precisely%20control%20harmful%20outputs.%20Extensive%20experiments%0Ademonstrate%20VisCRA%27s%20significant%20effectiveness%2C%20achieving%20high%20attack%20success%0Arates%20on%20leading%20closed-source%20MLRMs%3A%2076.48%25%20on%20Gemini%202.0%20Flash%20Thinking%2C%0A68.56%25%20on%20QvQ-Max%2C%20and%2056.60%25%20on%20GPT-4o.%20Our%20findings%20highlight%20a%20critical%0Ainsight%3A%20the%20very%20capability%20that%20empowers%20MLRMs%20--%20their%20visual%20reasoning%20--%0Acan%20also%20serve%20as%20an%20attack%20vector%2C%20posing%20significant%20security%20risks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19684v2&entry.124074799=Read"},
{"title": "Sherlock: Self-Correcting Reasoning in Vision-Language Models", "author": "Yi Ding and Ruqi Zhang", "abstract": "  Reasoning Vision-Language Models (VLMs) have shown promising performance on\ncomplex multimodal tasks. However, they still face significant challenges: they\nare highly sensitive to reasoning errors, require large volumes of annotated\ndata or accurate verifiers, and struggle to generalize beyond specific domains.\nTo address these limitations, we explore self-correction as a strategy to\nenhance reasoning VLMs. We first conduct an in-depth analysis of reasoning\nVLMs' self-correction abilities and identify key gaps. Based on our findings,\nwe introduce Sherlock, a self-correction and self-improvement training\nframework. Sherlock introduces a trajectory-level self-correction objective, a\npreference data construction method based on visual perturbation, and a dynamic\n$\\beta$ for preference tuning. Once the model acquires self-correction\ncapabilities using only 20k randomly sampled annotated data, it continues to\nself-improve without external supervision. Built on the Llama3.2-Vision-11B\nmodel, Sherlock achieves remarkable results across eight benchmarks, reaching\nan average accuracy of 64.1 with direct generation and 65.4 after\nself-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and\nLlamaV-o1 (63.4) while using less than 20% of the annotated data.\n", "link": "http://arxiv.org/abs/2505.22651v1", "date": "2025-05-28", "relevancy": 2.5599, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5287}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5287}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sherlock%3A%20Self-Correcting%20Reasoning%20in%20Vision-Language%20Models&body=Title%3A%20Sherlock%3A%20Self-Correcting%20Reasoning%20in%20Vision-Language%20Models%0AAuthor%3A%20Yi%20Ding%20and%20Ruqi%20Zhang%0AAbstract%3A%20%20%20Reasoning%20Vision-Language%20Models%20%28VLMs%29%20have%20shown%20promising%20performance%20on%0Acomplex%20multimodal%20tasks.%20However%2C%20they%20still%20face%20significant%20challenges%3A%20they%0Aare%20highly%20sensitive%20to%20reasoning%20errors%2C%20require%20large%20volumes%20of%20annotated%0Adata%20or%20accurate%20verifiers%2C%20and%20struggle%20to%20generalize%20beyond%20specific%20domains.%0ATo%20address%20these%20limitations%2C%20we%20explore%20self-correction%20as%20a%20strategy%20to%0Aenhance%20reasoning%20VLMs.%20We%20first%20conduct%20an%20in-depth%20analysis%20of%20reasoning%0AVLMs%27%20self-correction%20abilities%20and%20identify%20key%20gaps.%20Based%20on%20our%20findings%2C%0Awe%20introduce%20Sherlock%2C%20a%20self-correction%20and%20self-improvement%20training%0Aframework.%20Sherlock%20introduces%20a%20trajectory-level%20self-correction%20objective%2C%20a%0Apreference%20data%20construction%20method%20based%20on%20visual%20perturbation%2C%20and%20a%20dynamic%0A%24%5Cbeta%24%20for%20preference%20tuning.%20Once%20the%20model%20acquires%20self-correction%0Acapabilities%20using%20only%2020k%20randomly%20sampled%20annotated%20data%2C%20it%20continues%20to%0Aself-improve%20without%20external%20supervision.%20Built%20on%20the%20Llama3.2-Vision-11B%0Amodel%2C%20Sherlock%20achieves%20remarkable%20results%20across%20eight%20benchmarks%2C%20reaching%0Aan%20average%20accuracy%20of%2064.1%20with%20direct%20generation%20and%2065.4%20after%0Aself-correction.%20It%20outperforms%20LLaVA-CoT%20%2863.2%29%2C%20Mulberry%20%2863.9%29%2C%20and%0ALlamaV-o1%20%2863.4%29%20while%20using%20less%20than%2020%25%20of%20the%20annotated%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22651v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSherlock%253A%2520Self-Correcting%2520Reasoning%2520in%2520Vision-Language%2520Models%26entry.906535625%3DYi%2520Ding%2520and%2520Ruqi%2520Zhang%26entry.1292438233%3D%2520%2520Reasoning%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520shown%2520promising%2520performance%2520on%250Acomplex%2520multimodal%2520tasks.%2520However%252C%2520they%2520still%2520face%2520significant%2520challenges%253A%2520they%250Aare%2520highly%2520sensitive%2520to%2520reasoning%2520errors%252C%2520require%2520large%2520volumes%2520of%2520annotated%250Adata%2520or%2520accurate%2520verifiers%252C%2520and%2520struggle%2520to%2520generalize%2520beyond%2520specific%2520domains.%250ATo%2520address%2520these%2520limitations%252C%2520we%2520explore%2520self-correction%2520as%2520a%2520strategy%2520to%250Aenhance%2520reasoning%2520VLMs.%2520We%2520first%2520conduct%2520an%2520in-depth%2520analysis%2520of%2520reasoning%250AVLMs%2527%2520self-correction%2520abilities%2520and%2520identify%2520key%2520gaps.%2520Based%2520on%2520our%2520findings%252C%250Awe%2520introduce%2520Sherlock%252C%2520a%2520self-correction%2520and%2520self-improvement%2520training%250Aframework.%2520Sherlock%2520introduces%2520a%2520trajectory-level%2520self-correction%2520objective%252C%2520a%250Apreference%2520data%2520construction%2520method%2520based%2520on%2520visual%2520perturbation%252C%2520and%2520a%2520dynamic%250A%2524%255Cbeta%2524%2520for%2520preference%2520tuning.%2520Once%2520the%2520model%2520acquires%2520self-correction%250Acapabilities%2520using%2520only%252020k%2520randomly%2520sampled%2520annotated%2520data%252C%2520it%2520continues%2520to%250Aself-improve%2520without%2520external%2520supervision.%2520Built%2520on%2520the%2520Llama3.2-Vision-11B%250Amodel%252C%2520Sherlock%2520achieves%2520remarkable%2520results%2520across%2520eight%2520benchmarks%252C%2520reaching%250Aan%2520average%2520accuracy%2520of%252064.1%2520with%2520direct%2520generation%2520and%252065.4%2520after%250Aself-correction.%2520It%2520outperforms%2520LLaVA-CoT%2520%252863.2%2529%252C%2520Mulberry%2520%252863.9%2529%252C%2520and%250ALlamaV-o1%2520%252863.4%2529%2520while%2520using%2520less%2520than%252020%2525%2520of%2520the%2520annotated%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22651v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sherlock%3A%20Self-Correcting%20Reasoning%20in%20Vision-Language%20Models&entry.906535625=Yi%20Ding%20and%20Ruqi%20Zhang&entry.1292438233=%20%20Reasoning%20Vision-Language%20Models%20%28VLMs%29%20have%20shown%20promising%20performance%20on%0Acomplex%20multimodal%20tasks.%20However%2C%20they%20still%20face%20significant%20challenges%3A%20they%0Aare%20highly%20sensitive%20to%20reasoning%20errors%2C%20require%20large%20volumes%20of%20annotated%0Adata%20or%20accurate%20verifiers%2C%20and%20struggle%20to%20generalize%20beyond%20specific%20domains.%0ATo%20address%20these%20limitations%2C%20we%20explore%20self-correction%20as%20a%20strategy%20to%0Aenhance%20reasoning%20VLMs.%20We%20first%20conduct%20an%20in-depth%20analysis%20of%20reasoning%0AVLMs%27%20self-correction%20abilities%20and%20identify%20key%20gaps.%20Based%20on%20our%20findings%2C%0Awe%20introduce%20Sherlock%2C%20a%20self-correction%20and%20self-improvement%20training%0Aframework.%20Sherlock%20introduces%20a%20trajectory-level%20self-correction%20objective%2C%20a%0Apreference%20data%20construction%20method%20based%20on%20visual%20perturbation%2C%20and%20a%20dynamic%0A%24%5Cbeta%24%20for%20preference%20tuning.%20Once%20the%20model%20acquires%20self-correction%0Acapabilities%20using%20only%2020k%20randomly%20sampled%20annotated%20data%2C%20it%20continues%20to%0Aself-improve%20without%20external%20supervision.%20Built%20on%20the%20Llama3.2-Vision-11B%0Amodel%2C%20Sherlock%20achieves%20remarkable%20results%20across%20eight%20benchmarks%2C%20reaching%0Aan%20average%20accuracy%20of%2064.1%20with%20direct%20generation%20and%2065.4%20after%0Aself-correction.%20It%20outperforms%20LLaVA-CoT%20%2863.2%29%2C%20Mulberry%20%2863.9%29%2C%20and%0ALlamaV-o1%20%2863.4%29%20while%20using%20less%20than%2020%25%20of%20the%20annotated%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22651v1&entry.124074799=Read"},
{"title": "Robust Localization, Mapping, and Navigation for Quadruped Robots", "author": "Dyuman Aditya and Junning Huang and Nico Bohlinger and Piotr Kicki and Krzysztof Walas and Jan Peters and Matteo Luperto and Davide Tateo", "abstract": "  Quadruped robots are currently a widespread platform for robotics research,\nthanks to powerful Reinforcement Learning controllers and the availability of\ncheap and robust commercial platforms. However, to broaden the adoption of the\ntechnology in the real world, we require robust navigation stacks relying only\non low-cost sensors such as depth cameras. This paper presents a first step\ntowards a robust localization, mapping, and navigation system for low-cost\nquadruped robots. In pursuit of this objective we combine contact-aided\nkinematic, visual-inertial odometry, and depth-stabilized vision, enhancing\nstability and accuracy of the system. Our results in simulation and two\ndifferent real-world quadruped platforms show that our system can generate an\naccurate 2D map of the environment, robustly localize itself, and navigate\nautonomously. Furthermore, we present in-depth ablation studies of the\nimportant components of the system and their impact on localization accuracy.\nVideos, code, and additional experiments can be found on the project website:\nhttps://sites.google.com/view/low-cost-quadruped-slam\n", "link": "http://arxiv.org/abs/2505.02272v2", "date": "2025-05-28", "relevancy": 2.5455, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6489}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6427}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Localization%2C%20Mapping%2C%20and%20Navigation%20for%20Quadruped%20Robots&body=Title%3A%20Robust%20Localization%2C%20Mapping%2C%20and%20Navigation%20for%20Quadruped%20Robots%0AAuthor%3A%20Dyuman%20Aditya%20and%20Junning%20Huang%20and%20Nico%20Bohlinger%20and%20Piotr%20Kicki%20and%20Krzysztof%20Walas%20and%20Jan%20Peters%20and%20Matteo%20Luperto%20and%20Davide%20Tateo%0AAbstract%3A%20%20%20Quadruped%20robots%20are%20currently%20a%20widespread%20platform%20for%20robotics%20research%2C%0Athanks%20to%20powerful%20Reinforcement%20Learning%20controllers%20and%20the%20availability%20of%0Acheap%20and%20robust%20commercial%20platforms.%20However%2C%20to%20broaden%20the%20adoption%20of%20the%0Atechnology%20in%20the%20real%20world%2C%20we%20require%20robust%20navigation%20stacks%20relying%20only%0Aon%20low-cost%20sensors%20such%20as%20depth%20cameras.%20This%20paper%20presents%20a%20first%20step%0Atowards%20a%20robust%20localization%2C%20mapping%2C%20and%20navigation%20system%20for%20low-cost%0Aquadruped%20robots.%20In%20pursuit%20of%20this%20objective%20we%20combine%20contact-aided%0Akinematic%2C%20visual-inertial%20odometry%2C%20and%20depth-stabilized%20vision%2C%20enhancing%0Astability%20and%20accuracy%20of%20the%20system.%20Our%20results%20in%20simulation%20and%20two%0Adifferent%20real-world%20quadruped%20platforms%20show%20that%20our%20system%20can%20generate%20an%0Aaccurate%202D%20map%20of%20the%20environment%2C%20robustly%20localize%20itself%2C%20and%20navigate%0Aautonomously.%20Furthermore%2C%20we%20present%20in-depth%20ablation%20studies%20of%20the%0Aimportant%20components%20of%20the%20system%20and%20their%20impact%20on%20localization%20accuracy.%0AVideos%2C%20code%2C%20and%20additional%20experiments%20can%20be%20found%20on%20the%20project%20website%3A%0Ahttps%3A//sites.google.com/view/low-cost-quadruped-slam%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02272v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Localization%252C%2520Mapping%252C%2520and%2520Navigation%2520for%2520Quadruped%2520Robots%26entry.906535625%3DDyuman%2520Aditya%2520and%2520Junning%2520Huang%2520and%2520Nico%2520Bohlinger%2520and%2520Piotr%2520Kicki%2520and%2520Krzysztof%2520Walas%2520and%2520Jan%2520Peters%2520and%2520Matteo%2520Luperto%2520and%2520Davide%2520Tateo%26entry.1292438233%3D%2520%2520Quadruped%2520robots%2520are%2520currently%2520a%2520widespread%2520platform%2520for%2520robotics%2520research%252C%250Athanks%2520to%2520powerful%2520Reinforcement%2520Learning%2520controllers%2520and%2520the%2520availability%2520of%250Acheap%2520and%2520robust%2520commercial%2520platforms.%2520However%252C%2520to%2520broaden%2520the%2520adoption%2520of%2520the%250Atechnology%2520in%2520the%2520real%2520world%252C%2520we%2520require%2520robust%2520navigation%2520stacks%2520relying%2520only%250Aon%2520low-cost%2520sensors%2520such%2520as%2520depth%2520cameras.%2520This%2520paper%2520presents%2520a%2520first%2520step%250Atowards%2520a%2520robust%2520localization%252C%2520mapping%252C%2520and%2520navigation%2520system%2520for%2520low-cost%250Aquadruped%2520robots.%2520In%2520pursuit%2520of%2520this%2520objective%2520we%2520combine%2520contact-aided%250Akinematic%252C%2520visual-inertial%2520odometry%252C%2520and%2520depth-stabilized%2520vision%252C%2520enhancing%250Astability%2520and%2520accuracy%2520of%2520the%2520system.%2520Our%2520results%2520in%2520simulation%2520and%2520two%250Adifferent%2520real-world%2520quadruped%2520platforms%2520show%2520that%2520our%2520system%2520can%2520generate%2520an%250Aaccurate%25202D%2520map%2520of%2520the%2520environment%252C%2520robustly%2520localize%2520itself%252C%2520and%2520navigate%250Aautonomously.%2520Furthermore%252C%2520we%2520present%2520in-depth%2520ablation%2520studies%2520of%2520the%250Aimportant%2520components%2520of%2520the%2520system%2520and%2520their%2520impact%2520on%2520localization%2520accuracy.%250AVideos%252C%2520code%252C%2520and%2520additional%2520experiments%2520can%2520be%2520found%2520on%2520the%2520project%2520website%253A%250Ahttps%253A//sites.google.com/view/low-cost-quadruped-slam%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02272v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Localization%2C%20Mapping%2C%20and%20Navigation%20for%20Quadruped%20Robots&entry.906535625=Dyuman%20Aditya%20and%20Junning%20Huang%20and%20Nico%20Bohlinger%20and%20Piotr%20Kicki%20and%20Krzysztof%20Walas%20and%20Jan%20Peters%20and%20Matteo%20Luperto%20and%20Davide%20Tateo&entry.1292438233=%20%20Quadruped%20robots%20are%20currently%20a%20widespread%20platform%20for%20robotics%20research%2C%0Athanks%20to%20powerful%20Reinforcement%20Learning%20controllers%20and%20the%20availability%20of%0Acheap%20and%20robust%20commercial%20platforms.%20However%2C%20to%20broaden%20the%20adoption%20of%20the%0Atechnology%20in%20the%20real%20world%2C%20we%20require%20robust%20navigation%20stacks%20relying%20only%0Aon%20low-cost%20sensors%20such%20as%20depth%20cameras.%20This%20paper%20presents%20a%20first%20step%0Atowards%20a%20robust%20localization%2C%20mapping%2C%20and%20navigation%20system%20for%20low-cost%0Aquadruped%20robots.%20In%20pursuit%20of%20this%20objective%20we%20combine%20contact-aided%0Akinematic%2C%20visual-inertial%20odometry%2C%20and%20depth-stabilized%20vision%2C%20enhancing%0Astability%20and%20accuracy%20of%20the%20system.%20Our%20results%20in%20simulation%20and%20two%0Adifferent%20real-world%20quadruped%20platforms%20show%20that%20our%20system%20can%20generate%20an%0Aaccurate%202D%20map%20of%20the%20environment%2C%20robustly%20localize%20itself%2C%20and%20navigate%0Aautonomously.%20Furthermore%2C%20we%20present%20in-depth%20ablation%20studies%20of%20the%0Aimportant%20components%20of%20the%20system%20and%20their%20impact%20on%20localization%20accuracy.%0AVideos%2C%20code%2C%20and%20additional%20experiments%20can%20be%20found%20on%20the%20project%20website%3A%0Ahttps%3A//sites.google.com/view/low-cost-quadruped-slam%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02272v2&entry.124074799=Read"},
{"title": "Budget-Adaptive Adapter Tuning in Orthogonal Subspaces for Continual\n  Learning in LLMs", "author": "Zhiyi Wan and Wanrou Du and Liang Li and Miao Pan and Xiaoqi Qin", "abstract": "  Large language models (LLMs) often suffer from catastrophic forgetting in\ncontinual learning (CL) scenarios, where performance on previously learned\ntasks degrades severely while training on sequentially arriving tasks. Although\npioneering CL approaches using orthogonal subspaces can mitigate task\ninterference, they typically employ fixed budget allocation, neglecting the\nvarying complexity across tasks and layers. Besides, recent budget-adaptive\ntuning methods for LLMs often adopt multi-stage paradigms that decouple\noptimization and budget allocation. Such decoupling results in potential\nmisalignment, which hinders those approaches' practical application in CL\nscenarios. To address these limitations, we propose OA-Adapter, a novel\nparameter-efficient approach for continual learning in LLMs that unifies\ndynamic budget adaptation with orthogonal subspace learning in a single\nend-to-end training stage. Specifically, OA-Adapter introduces a dynamic\nbottleneck dimension adaptation mechanism that simultaneously allocates an\nefficient parameter budget and optimizes task objectives without misalignment.\nTo effectively preserve previously acquired knowledge while coordinating with\nthe dynamic budget allocation, orthogonal constraints are applied specifically\nbetween the parameter subspace of the current task and the dynamically\nallocated parameter subspaces of historical tasks. Experimental results on\ncontinual learning benchmarks demonstrate that OA-Adapter outperforms\nstate-of-the-art methods in both accuracy and parameter efficiency, achieving\nhigher average accuracy while using 58.5% fewer parameters on the standard CL\nbenchmark.\n", "link": "http://arxiv.org/abs/2505.22358v1", "date": "2025-05-28", "relevancy": 2.5416, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5367}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4949}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Budget-Adaptive%20Adapter%20Tuning%20in%20Orthogonal%20Subspaces%20for%20Continual%0A%20%20Learning%20in%20LLMs&body=Title%3A%20Budget-Adaptive%20Adapter%20Tuning%20in%20Orthogonal%20Subspaces%20for%20Continual%0A%20%20Learning%20in%20LLMs%0AAuthor%3A%20Zhiyi%20Wan%20and%20Wanrou%20Du%20and%20Liang%20Li%20and%20Miao%20Pan%20and%20Xiaoqi%20Qin%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20often%20suffer%20from%20catastrophic%20forgetting%20in%0Acontinual%20learning%20%28CL%29%20scenarios%2C%20where%20performance%20on%20previously%20learned%0Atasks%20degrades%20severely%20while%20training%20on%20sequentially%20arriving%20tasks.%20Although%0Apioneering%20CL%20approaches%20using%20orthogonal%20subspaces%20can%20mitigate%20task%0Ainterference%2C%20they%20typically%20employ%20fixed%20budget%20allocation%2C%20neglecting%20the%0Avarying%20complexity%20across%20tasks%20and%20layers.%20Besides%2C%20recent%20budget-adaptive%0Atuning%20methods%20for%20LLMs%20often%20adopt%20multi-stage%20paradigms%20that%20decouple%0Aoptimization%20and%20budget%20allocation.%20Such%20decoupling%20results%20in%20potential%0Amisalignment%2C%20which%20hinders%20those%20approaches%27%20practical%20application%20in%20CL%0Ascenarios.%20To%20address%20these%20limitations%2C%20we%20propose%20OA-Adapter%2C%20a%20novel%0Aparameter-efficient%20approach%20for%20continual%20learning%20in%20LLMs%20that%20unifies%0Adynamic%20budget%20adaptation%20with%20orthogonal%20subspace%20learning%20in%20a%20single%0Aend-to-end%20training%20stage.%20Specifically%2C%20OA-Adapter%20introduces%20a%20dynamic%0Abottleneck%20dimension%20adaptation%20mechanism%20that%20simultaneously%20allocates%20an%0Aefficient%20parameter%20budget%20and%20optimizes%20task%20objectives%20without%20misalignment.%0ATo%20effectively%20preserve%20previously%20acquired%20knowledge%20while%20coordinating%20with%0Athe%20dynamic%20budget%20allocation%2C%20orthogonal%20constraints%20are%20applied%20specifically%0Abetween%20the%20parameter%20subspace%20of%20the%20current%20task%20and%20the%20dynamically%0Aallocated%20parameter%20subspaces%20of%20historical%20tasks.%20Experimental%20results%20on%0Acontinual%20learning%20benchmarks%20demonstrate%20that%20OA-Adapter%20outperforms%0Astate-of-the-art%20methods%20in%20both%20accuracy%20and%20parameter%20efficiency%2C%20achieving%0Ahigher%20average%20accuracy%20while%20using%2058.5%25%20fewer%20parameters%20on%20the%20standard%20CL%0Abenchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22358v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBudget-Adaptive%2520Adapter%2520Tuning%2520in%2520Orthogonal%2520Subspaces%2520for%2520Continual%250A%2520%2520Learning%2520in%2520LLMs%26entry.906535625%3DZhiyi%2520Wan%2520and%2520Wanrou%2520Du%2520and%2520Liang%2520Li%2520and%2520Miao%2520Pan%2520and%2520Xiaoqi%2520Qin%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520often%2520suffer%2520from%2520catastrophic%2520forgetting%2520in%250Acontinual%2520learning%2520%2528CL%2529%2520scenarios%252C%2520where%2520performance%2520on%2520previously%2520learned%250Atasks%2520degrades%2520severely%2520while%2520training%2520on%2520sequentially%2520arriving%2520tasks.%2520Although%250Apioneering%2520CL%2520approaches%2520using%2520orthogonal%2520subspaces%2520can%2520mitigate%2520task%250Ainterference%252C%2520they%2520typically%2520employ%2520fixed%2520budget%2520allocation%252C%2520neglecting%2520the%250Avarying%2520complexity%2520across%2520tasks%2520and%2520layers.%2520Besides%252C%2520recent%2520budget-adaptive%250Atuning%2520methods%2520for%2520LLMs%2520often%2520adopt%2520multi-stage%2520paradigms%2520that%2520decouple%250Aoptimization%2520and%2520budget%2520allocation.%2520Such%2520decoupling%2520results%2520in%2520potential%250Amisalignment%252C%2520which%2520hinders%2520those%2520approaches%2527%2520practical%2520application%2520in%2520CL%250Ascenarios.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520OA-Adapter%252C%2520a%2520novel%250Aparameter-efficient%2520approach%2520for%2520continual%2520learning%2520in%2520LLMs%2520that%2520unifies%250Adynamic%2520budget%2520adaptation%2520with%2520orthogonal%2520subspace%2520learning%2520in%2520a%2520single%250Aend-to-end%2520training%2520stage.%2520Specifically%252C%2520OA-Adapter%2520introduces%2520a%2520dynamic%250Abottleneck%2520dimension%2520adaptation%2520mechanism%2520that%2520simultaneously%2520allocates%2520an%250Aefficient%2520parameter%2520budget%2520and%2520optimizes%2520task%2520objectives%2520without%2520misalignment.%250ATo%2520effectively%2520preserve%2520previously%2520acquired%2520knowledge%2520while%2520coordinating%2520with%250Athe%2520dynamic%2520budget%2520allocation%252C%2520orthogonal%2520constraints%2520are%2520applied%2520specifically%250Abetween%2520the%2520parameter%2520subspace%2520of%2520the%2520current%2520task%2520and%2520the%2520dynamically%250Aallocated%2520parameter%2520subspaces%2520of%2520historical%2520tasks.%2520Experimental%2520results%2520on%250Acontinual%2520learning%2520benchmarks%2520demonstrate%2520that%2520OA-Adapter%2520outperforms%250Astate-of-the-art%2520methods%2520in%2520both%2520accuracy%2520and%2520parameter%2520efficiency%252C%2520achieving%250Ahigher%2520average%2520accuracy%2520while%2520using%252058.5%2525%2520fewer%2520parameters%2520on%2520the%2520standard%2520CL%250Abenchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22358v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Budget-Adaptive%20Adapter%20Tuning%20in%20Orthogonal%20Subspaces%20for%20Continual%0A%20%20Learning%20in%20LLMs&entry.906535625=Zhiyi%20Wan%20and%20Wanrou%20Du%20and%20Liang%20Li%20and%20Miao%20Pan%20and%20Xiaoqi%20Qin&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20often%20suffer%20from%20catastrophic%20forgetting%20in%0Acontinual%20learning%20%28CL%29%20scenarios%2C%20where%20performance%20on%20previously%20learned%0Atasks%20degrades%20severely%20while%20training%20on%20sequentially%20arriving%20tasks.%20Although%0Apioneering%20CL%20approaches%20using%20orthogonal%20subspaces%20can%20mitigate%20task%0Ainterference%2C%20they%20typically%20employ%20fixed%20budget%20allocation%2C%20neglecting%20the%0Avarying%20complexity%20across%20tasks%20and%20layers.%20Besides%2C%20recent%20budget-adaptive%0Atuning%20methods%20for%20LLMs%20often%20adopt%20multi-stage%20paradigms%20that%20decouple%0Aoptimization%20and%20budget%20allocation.%20Such%20decoupling%20results%20in%20potential%0Amisalignment%2C%20which%20hinders%20those%20approaches%27%20practical%20application%20in%20CL%0Ascenarios.%20To%20address%20these%20limitations%2C%20we%20propose%20OA-Adapter%2C%20a%20novel%0Aparameter-efficient%20approach%20for%20continual%20learning%20in%20LLMs%20that%20unifies%0Adynamic%20budget%20adaptation%20with%20orthogonal%20subspace%20learning%20in%20a%20single%0Aend-to-end%20training%20stage.%20Specifically%2C%20OA-Adapter%20introduces%20a%20dynamic%0Abottleneck%20dimension%20adaptation%20mechanism%20that%20simultaneously%20allocates%20an%0Aefficient%20parameter%20budget%20and%20optimizes%20task%20objectives%20without%20misalignment.%0ATo%20effectively%20preserve%20previously%20acquired%20knowledge%20while%20coordinating%20with%0Athe%20dynamic%20budget%20allocation%2C%20orthogonal%20constraints%20are%20applied%20specifically%0Abetween%20the%20parameter%20subspace%20of%20the%20current%20task%20and%20the%20dynamically%0Aallocated%20parameter%20subspaces%20of%20historical%20tasks.%20Experimental%20results%20on%0Acontinual%20learning%20benchmarks%20demonstrate%20that%20OA-Adapter%20outperforms%0Astate-of-the-art%20methods%20in%20both%20accuracy%20and%20parameter%20efficiency%2C%20achieving%0Ahigher%20average%20accuracy%20while%20using%2058.5%25%20fewer%20parameters%20on%20the%20standard%20CL%0Abenchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22358v1&entry.124074799=Read"},
{"title": "CADReview: Automatically Reviewing CAD Programs with Error Detection and\n  Correction", "author": "Jiali Chen and Xusen Hei and HongFei Liu and Yuancheng Wei and Zikun Deng and Jiayuan Xie and Yi Cai and Li Qing", "abstract": "  Computer-aided design (CAD) is crucial in prototyping 3D objects through\ngeometric instructions (i.e., CAD programs). In practical design workflows,\ndesigners often engage in time-consuming reviews and refinements of these\nprototypes by comparing them with reference images. To bridge this gap, we\nintroduce the CAD review task to automatically detect and correct potential\nerrors, ensuring consistency between the constructed 3D objects and reference\nimages. However, recent advanced multimodal large language models (MLLMs)\nstruggle to recognize multiple geometric components and perform spatial\ngeometric operations within the CAD program, leading to inaccurate reviews. In\nthis paper, we propose the CAD program repairer (ReCAD) framework to\neffectively detect program errors and provide helpful feedback on error\ncorrection. Additionally, we create a dataset, CADReview, consisting of over\n20K program-image pairs, with diverse errors for the CAD review task. Extensive\nexperiments demonstrate that our ReCAD significantly outperforms existing\nMLLMs, which shows great potential in design applications.\n", "link": "http://arxiv.org/abs/2505.22304v1", "date": "2025-05-28", "relevancy": 2.5336, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5304}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4949}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CADReview%3A%20Automatically%20Reviewing%20CAD%20Programs%20with%20Error%20Detection%20and%0A%20%20Correction&body=Title%3A%20CADReview%3A%20Automatically%20Reviewing%20CAD%20Programs%20with%20Error%20Detection%20and%0A%20%20Correction%0AAuthor%3A%20Jiali%20Chen%20and%20Xusen%20Hei%20and%20HongFei%20Liu%20and%20Yuancheng%20Wei%20and%20Zikun%20Deng%20and%20Jiayuan%20Xie%20and%20Yi%20Cai%20and%20Li%20Qing%0AAbstract%3A%20%20%20Computer-aided%20design%20%28CAD%29%20is%20crucial%20in%20prototyping%203D%20objects%20through%0Ageometric%20instructions%20%28i.e.%2C%20CAD%20programs%29.%20In%20practical%20design%20workflows%2C%0Adesigners%20often%20engage%20in%20time-consuming%20reviews%20and%20refinements%20of%20these%0Aprototypes%20by%20comparing%20them%20with%20reference%20images.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20the%20CAD%20review%20task%20to%20automatically%20detect%20and%20correct%20potential%0Aerrors%2C%20ensuring%20consistency%20between%20the%20constructed%203D%20objects%20and%20reference%0Aimages.%20However%2C%20recent%20advanced%20multimodal%20large%20language%20models%20%28MLLMs%29%0Astruggle%20to%20recognize%20multiple%20geometric%20components%20and%20perform%20spatial%0Ageometric%20operations%20within%20the%20CAD%20program%2C%20leading%20to%20inaccurate%20reviews.%20In%0Athis%20paper%2C%20we%20propose%20the%20CAD%20program%20repairer%20%28ReCAD%29%20framework%20to%0Aeffectively%20detect%20program%20errors%20and%20provide%20helpful%20feedback%20on%20error%0Acorrection.%20Additionally%2C%20we%20create%20a%20dataset%2C%20CADReview%2C%20consisting%20of%20over%0A20K%20program-image%20pairs%2C%20with%20diverse%20errors%20for%20the%20CAD%20review%20task.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20ReCAD%20significantly%20outperforms%20existing%0AMLLMs%2C%20which%20shows%20great%20potential%20in%20design%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22304v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCADReview%253A%2520Automatically%2520Reviewing%2520CAD%2520Programs%2520with%2520Error%2520Detection%2520and%250A%2520%2520Correction%26entry.906535625%3DJiali%2520Chen%2520and%2520Xusen%2520Hei%2520and%2520HongFei%2520Liu%2520and%2520Yuancheng%2520Wei%2520and%2520Zikun%2520Deng%2520and%2520Jiayuan%2520Xie%2520and%2520Yi%2520Cai%2520and%2520Li%2520Qing%26entry.1292438233%3D%2520%2520Computer-aided%2520design%2520%2528CAD%2529%2520is%2520crucial%2520in%2520prototyping%25203D%2520objects%2520through%250Ageometric%2520instructions%2520%2528i.e.%252C%2520CAD%2520programs%2529.%2520In%2520practical%2520design%2520workflows%252C%250Adesigners%2520often%2520engage%2520in%2520time-consuming%2520reviews%2520and%2520refinements%2520of%2520these%250Aprototypes%2520by%2520comparing%2520them%2520with%2520reference%2520images.%2520To%2520bridge%2520this%2520gap%252C%2520we%250Aintroduce%2520the%2520CAD%2520review%2520task%2520to%2520automatically%2520detect%2520and%2520correct%2520potential%250Aerrors%252C%2520ensuring%2520consistency%2520between%2520the%2520constructed%25203D%2520objects%2520and%2520reference%250Aimages.%2520However%252C%2520recent%2520advanced%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%250Astruggle%2520to%2520recognize%2520multiple%2520geometric%2520components%2520and%2520perform%2520spatial%250Ageometric%2520operations%2520within%2520the%2520CAD%2520program%252C%2520leading%2520to%2520inaccurate%2520reviews.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520the%2520CAD%2520program%2520repairer%2520%2528ReCAD%2529%2520framework%2520to%250Aeffectively%2520detect%2520program%2520errors%2520and%2520provide%2520helpful%2520feedback%2520on%2520error%250Acorrection.%2520Additionally%252C%2520we%2520create%2520a%2520dataset%252C%2520CADReview%252C%2520consisting%2520of%2520over%250A20K%2520program-image%2520pairs%252C%2520with%2520diverse%2520errors%2520for%2520the%2520CAD%2520review%2520task.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520ReCAD%2520significantly%2520outperforms%2520existing%250AMLLMs%252C%2520which%2520shows%2520great%2520potential%2520in%2520design%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22304v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CADReview%3A%20Automatically%20Reviewing%20CAD%20Programs%20with%20Error%20Detection%20and%0A%20%20Correction&entry.906535625=Jiali%20Chen%20and%20Xusen%20Hei%20and%20HongFei%20Liu%20and%20Yuancheng%20Wei%20and%20Zikun%20Deng%20and%20Jiayuan%20Xie%20and%20Yi%20Cai%20and%20Li%20Qing&entry.1292438233=%20%20Computer-aided%20design%20%28CAD%29%20is%20crucial%20in%20prototyping%203D%20objects%20through%0Ageometric%20instructions%20%28i.e.%2C%20CAD%20programs%29.%20In%20practical%20design%20workflows%2C%0Adesigners%20often%20engage%20in%20time-consuming%20reviews%20and%20refinements%20of%20these%0Aprototypes%20by%20comparing%20them%20with%20reference%20images.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20the%20CAD%20review%20task%20to%20automatically%20detect%20and%20correct%20potential%0Aerrors%2C%20ensuring%20consistency%20between%20the%20constructed%203D%20objects%20and%20reference%0Aimages.%20However%2C%20recent%20advanced%20multimodal%20large%20language%20models%20%28MLLMs%29%0Astruggle%20to%20recognize%20multiple%20geometric%20components%20and%20perform%20spatial%0Ageometric%20operations%20within%20the%20CAD%20program%2C%20leading%20to%20inaccurate%20reviews.%20In%0Athis%20paper%2C%20we%20propose%20the%20CAD%20program%20repairer%20%28ReCAD%29%20framework%20to%0Aeffectively%20detect%20program%20errors%20and%20provide%20helpful%20feedback%20on%20error%0Acorrection.%20Additionally%2C%20we%20create%20a%20dataset%2C%20CADReview%2C%20consisting%20of%20over%0A20K%20program-image%20pairs%2C%20with%20diverse%20errors%20for%20the%20CAD%20review%20task.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20ReCAD%20significantly%20outperforms%20existing%0AMLLMs%2C%20which%20shows%20great%20potential%20in%20design%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22304v1&entry.124074799=Read"},
{"title": "Core Context Aware Transformers for Long Context Language Modeling", "author": "Yaofo Chen and Zeng You and Shuhai Zhang and Haokun Li and Yirui Li and Yaowei Wang and Mingkui Tan", "abstract": "  Transformer-based Large Language Models (LLMs) have exhibited remarkable\nsuccess in extensive tasks primarily attributed to self-attention mechanism,\nwhich requires a token to consider all preceding tokens as its context to\ncompute attention. However, when the context length L becomes very large (e.g.,\n128K), the amount of potentially redundant information in the context tends to\nincrease. The redundant context not only hampers the modeling representation\nperformance but also incurs unnecessary computational and storage overhead. In\nthis paper, we propose a plug-and-play Core Context Aware (CCA) Attention for\nefficient long-context modeling, comprising two complementary modules: 1)\nGlobality-aware pooling module groups input tokens and dynamically compresses\neach group into one core token based on their significance. In this way, our\nmethod automatically focuses and strengthens core context while diminishing\nredundancy during the learning process, leading to effective long-term\ndependency modeling. 2) Locality-preserving module incorporates neighboring\ntokens to preserve local context for detailed representation. Notably, our\nCCA-Attention is able to replace the self-attention module in existing LLMs\nwith minimal fine-tuning cost. Extensive experimental results show the\nsuperiority of our method in both long-context modeling and computational\nefficiency over state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2412.12465v2", "date": "2025-05-28", "relevancy": 2.5324, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.51}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5047}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Core%20Context%20Aware%20Transformers%20for%20Long%20Context%20Language%20Modeling&body=Title%3A%20Core%20Context%20Aware%20Transformers%20for%20Long%20Context%20Language%20Modeling%0AAuthor%3A%20Yaofo%20Chen%20and%20Zeng%20You%20and%20Shuhai%20Zhang%20and%20Haokun%20Li%20and%20Yirui%20Li%20and%20Yaowei%20Wang%20and%20Mingkui%20Tan%0AAbstract%3A%20%20%20Transformer-based%20Large%20Language%20Models%20%28LLMs%29%20have%20exhibited%20remarkable%0Asuccess%20in%20extensive%20tasks%20primarily%20attributed%20to%20self-attention%20mechanism%2C%0Awhich%20requires%20a%20token%20to%20consider%20all%20preceding%20tokens%20as%20its%20context%20to%0Acompute%20attention.%20However%2C%20when%20the%20context%20length%20L%20becomes%20very%20large%20%28e.g.%2C%0A128K%29%2C%20the%20amount%20of%20potentially%20redundant%20information%20in%20the%20context%20tends%20to%0Aincrease.%20The%20redundant%20context%20not%20only%20hampers%20the%20modeling%20representation%0Aperformance%20but%20also%20incurs%20unnecessary%20computational%20and%20storage%20overhead.%20In%0Athis%20paper%2C%20we%20propose%20a%20plug-and-play%20Core%20Context%20Aware%20%28CCA%29%20Attention%20for%0Aefficient%20long-context%20modeling%2C%20comprising%20two%20complementary%20modules%3A%201%29%0AGlobality-aware%20pooling%20module%20groups%20input%20tokens%20and%20dynamically%20compresses%0Aeach%20group%20into%20one%20core%20token%20based%20on%20their%20significance.%20In%20this%20way%2C%20our%0Amethod%20automatically%20focuses%20and%20strengthens%20core%20context%20while%20diminishing%0Aredundancy%20during%20the%20learning%20process%2C%20leading%20to%20effective%20long-term%0Adependency%20modeling.%202%29%20Locality-preserving%20module%20incorporates%20neighboring%0Atokens%20to%20preserve%20local%20context%20for%20detailed%20representation.%20Notably%2C%20our%0ACCA-Attention%20is%20able%20to%20replace%20the%20self-attention%20module%20in%20existing%20LLMs%0Awith%20minimal%20fine-tuning%20cost.%20Extensive%20experimental%20results%20show%20the%0Asuperiority%20of%20our%20method%20in%20both%20long-context%20modeling%20and%20computational%0Aefficiency%20over%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12465v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCore%2520Context%2520Aware%2520Transformers%2520for%2520Long%2520Context%2520Language%2520Modeling%26entry.906535625%3DYaofo%2520Chen%2520and%2520Zeng%2520You%2520and%2520Shuhai%2520Zhang%2520and%2520Haokun%2520Li%2520and%2520Yirui%2520Li%2520and%2520Yaowei%2520Wang%2520and%2520Mingkui%2520Tan%26entry.1292438233%3D%2520%2520Transformer-based%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520exhibited%2520remarkable%250Asuccess%2520in%2520extensive%2520tasks%2520primarily%2520attributed%2520to%2520self-attention%2520mechanism%252C%250Awhich%2520requires%2520a%2520token%2520to%2520consider%2520all%2520preceding%2520tokens%2520as%2520its%2520context%2520to%250Acompute%2520attention.%2520However%252C%2520when%2520the%2520context%2520length%2520L%2520becomes%2520very%2520large%2520%2528e.g.%252C%250A128K%2529%252C%2520the%2520amount%2520of%2520potentially%2520redundant%2520information%2520in%2520the%2520context%2520tends%2520to%250Aincrease.%2520The%2520redundant%2520context%2520not%2520only%2520hampers%2520the%2520modeling%2520representation%250Aperformance%2520but%2520also%2520incurs%2520unnecessary%2520computational%2520and%2520storage%2520overhead.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520a%2520plug-and-play%2520Core%2520Context%2520Aware%2520%2528CCA%2529%2520Attention%2520for%250Aefficient%2520long-context%2520modeling%252C%2520comprising%2520two%2520complementary%2520modules%253A%25201%2529%250AGlobality-aware%2520pooling%2520module%2520groups%2520input%2520tokens%2520and%2520dynamically%2520compresses%250Aeach%2520group%2520into%2520one%2520core%2520token%2520based%2520on%2520their%2520significance.%2520In%2520this%2520way%252C%2520our%250Amethod%2520automatically%2520focuses%2520and%2520strengthens%2520core%2520context%2520while%2520diminishing%250Aredundancy%2520during%2520the%2520learning%2520process%252C%2520leading%2520to%2520effective%2520long-term%250Adependency%2520modeling.%25202%2529%2520Locality-preserving%2520module%2520incorporates%2520neighboring%250Atokens%2520to%2520preserve%2520local%2520context%2520for%2520detailed%2520representation.%2520Notably%252C%2520our%250ACCA-Attention%2520is%2520able%2520to%2520replace%2520the%2520self-attention%2520module%2520in%2520existing%2520LLMs%250Awith%2520minimal%2520fine-tuning%2520cost.%2520Extensive%2520experimental%2520results%2520show%2520the%250Asuperiority%2520of%2520our%2520method%2520in%2520both%2520long-context%2520modeling%2520and%2520computational%250Aefficiency%2520over%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12465v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Core%20Context%20Aware%20Transformers%20for%20Long%20Context%20Language%20Modeling&entry.906535625=Yaofo%20Chen%20and%20Zeng%20You%20and%20Shuhai%20Zhang%20and%20Haokun%20Li%20and%20Yirui%20Li%20and%20Yaowei%20Wang%20and%20Mingkui%20Tan&entry.1292438233=%20%20Transformer-based%20Large%20Language%20Models%20%28LLMs%29%20have%20exhibited%20remarkable%0Asuccess%20in%20extensive%20tasks%20primarily%20attributed%20to%20self-attention%20mechanism%2C%0Awhich%20requires%20a%20token%20to%20consider%20all%20preceding%20tokens%20as%20its%20context%20to%0Acompute%20attention.%20However%2C%20when%20the%20context%20length%20L%20becomes%20very%20large%20%28e.g.%2C%0A128K%29%2C%20the%20amount%20of%20potentially%20redundant%20information%20in%20the%20context%20tends%20to%0Aincrease.%20The%20redundant%20context%20not%20only%20hampers%20the%20modeling%20representation%0Aperformance%20but%20also%20incurs%20unnecessary%20computational%20and%20storage%20overhead.%20In%0Athis%20paper%2C%20we%20propose%20a%20plug-and-play%20Core%20Context%20Aware%20%28CCA%29%20Attention%20for%0Aefficient%20long-context%20modeling%2C%20comprising%20two%20complementary%20modules%3A%201%29%0AGlobality-aware%20pooling%20module%20groups%20input%20tokens%20and%20dynamically%20compresses%0Aeach%20group%20into%20one%20core%20token%20based%20on%20their%20significance.%20In%20this%20way%2C%20our%0Amethod%20automatically%20focuses%20and%20strengthens%20core%20context%20while%20diminishing%0Aredundancy%20during%20the%20learning%20process%2C%20leading%20to%20effective%20long-term%0Adependency%20modeling.%202%29%20Locality-preserving%20module%20incorporates%20neighboring%0Atokens%20to%20preserve%20local%20context%20for%20detailed%20representation.%20Notably%2C%20our%0ACCA-Attention%20is%20able%20to%20replace%20the%20self-attention%20module%20in%20existing%20LLMs%0Awith%20minimal%20fine-tuning%20cost.%20Extensive%20experimental%20results%20show%20the%0Asuperiority%20of%20our%20method%20in%20both%20long-context%20modeling%20and%20computational%0Aefficiency%20over%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12465v2&entry.124074799=Read"},
{"title": "Geometric GNNs for Charged Particle Tracking at GlueX", "author": "Ahmed Hossam Mohammed and Kishansingh Rajput and Simon Taylor and Denis Furletov and Sergey Furletov and Malachi Schram", "abstract": "  Nuclear physics experiments are aimed at uncovering the fundamental building\nblocks of matter. The experiments involve high-energy collisions that produce\ncomplex events with many particle trajectories. Tracking charged particles\nresulting from collisions in the presence of a strong magnetic field is\ncritical to enable the reconstruction of particle trajectories and precise\ndetermination of interactions. It is traditionally achieved through\ncombinatorial approaches that scale worse than linearly as the number of hits\ngrows. Since particle hit data naturally form a 3-dimensional point cloud and\ncan be structured as graphs, Graph Neural Networks (GNNs) emerge as an\nintuitive and effective choice for this task. In this study, we evaluate the\nGNN model for track finding on the data from the GlueX experiment at Jefferson\nLab. We use simulation data to train the model and test on both simulation and\nreal GlueX measurements. We demonstrate that GNN-based track finding\noutperforms the currently used traditional method at GlueX in terms of\nsegment-based efficiency at a fixed purity while providing faster inferences.\nWe show that the GNN model can achieve significant speedup by processing\nmultiple events in batches, which exploits the parallel computation capability\nof Graphical Processing Units (GPUs). Finally, we compare the GNN\nimplementation on GPU and FPGA and describe the trade-off.\n", "link": "http://arxiv.org/abs/2505.22504v1", "date": "2025-05-28", "relevancy": 2.5181, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5318}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5179}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometric%20GNNs%20for%20Charged%20Particle%20Tracking%20at%20GlueX&body=Title%3A%20Geometric%20GNNs%20for%20Charged%20Particle%20Tracking%20at%20GlueX%0AAuthor%3A%20Ahmed%20Hossam%20Mohammed%20and%20Kishansingh%20Rajput%20and%20Simon%20Taylor%20and%20Denis%20Furletov%20and%20Sergey%20Furletov%20and%20Malachi%20Schram%0AAbstract%3A%20%20%20Nuclear%20physics%20experiments%20are%20aimed%20at%20uncovering%20the%20fundamental%20building%0Ablocks%20of%20matter.%20The%20experiments%20involve%20high-energy%20collisions%20that%20produce%0Acomplex%20events%20with%20many%20particle%20trajectories.%20Tracking%20charged%20particles%0Aresulting%20from%20collisions%20in%20the%20presence%20of%20a%20strong%20magnetic%20field%20is%0Acritical%20to%20enable%20the%20reconstruction%20of%20particle%20trajectories%20and%20precise%0Adetermination%20of%20interactions.%20It%20is%20traditionally%20achieved%20through%0Acombinatorial%20approaches%20that%20scale%20worse%20than%20linearly%20as%20the%20number%20of%20hits%0Agrows.%20Since%20particle%20hit%20data%20naturally%20form%20a%203-dimensional%20point%20cloud%20and%0Acan%20be%20structured%20as%20graphs%2C%20Graph%20Neural%20Networks%20%28GNNs%29%20emerge%20as%20an%0Aintuitive%20and%20effective%20choice%20for%20this%20task.%20In%20this%20study%2C%20we%20evaluate%20the%0AGNN%20model%20for%20track%20finding%20on%20the%20data%20from%20the%20GlueX%20experiment%20at%20Jefferson%0ALab.%20We%20use%20simulation%20data%20to%20train%20the%20model%20and%20test%20on%20both%20simulation%20and%0Areal%20GlueX%20measurements.%20We%20demonstrate%20that%20GNN-based%20track%20finding%0Aoutperforms%20the%20currently%20used%20traditional%20method%20at%20GlueX%20in%20terms%20of%0Asegment-based%20efficiency%20at%20a%20fixed%20purity%20while%20providing%20faster%20inferences.%0AWe%20show%20that%20the%20GNN%20model%20can%20achieve%20significant%20speedup%20by%20processing%0Amultiple%20events%20in%20batches%2C%20which%20exploits%20the%20parallel%20computation%20capability%0Aof%20Graphical%20Processing%20Units%20%28GPUs%29.%20Finally%2C%20we%20compare%20the%20GNN%0Aimplementation%20on%20GPU%20and%20FPGA%20and%20describe%20the%20trade-off.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22504v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometric%2520GNNs%2520for%2520Charged%2520Particle%2520Tracking%2520at%2520GlueX%26entry.906535625%3DAhmed%2520Hossam%2520Mohammed%2520and%2520Kishansingh%2520Rajput%2520and%2520Simon%2520Taylor%2520and%2520Denis%2520Furletov%2520and%2520Sergey%2520Furletov%2520and%2520Malachi%2520Schram%26entry.1292438233%3D%2520%2520Nuclear%2520physics%2520experiments%2520are%2520aimed%2520at%2520uncovering%2520the%2520fundamental%2520building%250Ablocks%2520of%2520matter.%2520The%2520experiments%2520involve%2520high-energy%2520collisions%2520that%2520produce%250Acomplex%2520events%2520with%2520many%2520particle%2520trajectories.%2520Tracking%2520charged%2520particles%250Aresulting%2520from%2520collisions%2520in%2520the%2520presence%2520of%2520a%2520strong%2520magnetic%2520field%2520is%250Acritical%2520to%2520enable%2520the%2520reconstruction%2520of%2520particle%2520trajectories%2520and%2520precise%250Adetermination%2520of%2520interactions.%2520It%2520is%2520traditionally%2520achieved%2520through%250Acombinatorial%2520approaches%2520that%2520scale%2520worse%2520than%2520linearly%2520as%2520the%2520number%2520of%2520hits%250Agrows.%2520Since%2520particle%2520hit%2520data%2520naturally%2520form%2520a%25203-dimensional%2520point%2520cloud%2520and%250Acan%2520be%2520structured%2520as%2520graphs%252C%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520emerge%2520as%2520an%250Aintuitive%2520and%2520effective%2520choice%2520for%2520this%2520task.%2520In%2520this%2520study%252C%2520we%2520evaluate%2520the%250AGNN%2520model%2520for%2520track%2520finding%2520on%2520the%2520data%2520from%2520the%2520GlueX%2520experiment%2520at%2520Jefferson%250ALab.%2520We%2520use%2520simulation%2520data%2520to%2520train%2520the%2520model%2520and%2520test%2520on%2520both%2520simulation%2520and%250Areal%2520GlueX%2520measurements.%2520We%2520demonstrate%2520that%2520GNN-based%2520track%2520finding%250Aoutperforms%2520the%2520currently%2520used%2520traditional%2520method%2520at%2520GlueX%2520in%2520terms%2520of%250Asegment-based%2520efficiency%2520at%2520a%2520fixed%2520purity%2520while%2520providing%2520faster%2520inferences.%250AWe%2520show%2520that%2520the%2520GNN%2520model%2520can%2520achieve%2520significant%2520speedup%2520by%2520processing%250Amultiple%2520events%2520in%2520batches%252C%2520which%2520exploits%2520the%2520parallel%2520computation%2520capability%250Aof%2520Graphical%2520Processing%2520Units%2520%2528GPUs%2529.%2520Finally%252C%2520we%2520compare%2520the%2520GNN%250Aimplementation%2520on%2520GPU%2520and%2520FPGA%2520and%2520describe%2520the%2520trade-off.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22504v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometric%20GNNs%20for%20Charged%20Particle%20Tracking%20at%20GlueX&entry.906535625=Ahmed%20Hossam%20Mohammed%20and%20Kishansingh%20Rajput%20and%20Simon%20Taylor%20and%20Denis%20Furletov%20and%20Sergey%20Furletov%20and%20Malachi%20Schram&entry.1292438233=%20%20Nuclear%20physics%20experiments%20are%20aimed%20at%20uncovering%20the%20fundamental%20building%0Ablocks%20of%20matter.%20The%20experiments%20involve%20high-energy%20collisions%20that%20produce%0Acomplex%20events%20with%20many%20particle%20trajectories.%20Tracking%20charged%20particles%0Aresulting%20from%20collisions%20in%20the%20presence%20of%20a%20strong%20magnetic%20field%20is%0Acritical%20to%20enable%20the%20reconstruction%20of%20particle%20trajectories%20and%20precise%0Adetermination%20of%20interactions.%20It%20is%20traditionally%20achieved%20through%0Acombinatorial%20approaches%20that%20scale%20worse%20than%20linearly%20as%20the%20number%20of%20hits%0Agrows.%20Since%20particle%20hit%20data%20naturally%20form%20a%203-dimensional%20point%20cloud%20and%0Acan%20be%20structured%20as%20graphs%2C%20Graph%20Neural%20Networks%20%28GNNs%29%20emerge%20as%20an%0Aintuitive%20and%20effective%20choice%20for%20this%20task.%20In%20this%20study%2C%20we%20evaluate%20the%0AGNN%20model%20for%20track%20finding%20on%20the%20data%20from%20the%20GlueX%20experiment%20at%20Jefferson%0ALab.%20We%20use%20simulation%20data%20to%20train%20the%20model%20and%20test%20on%20both%20simulation%20and%0Areal%20GlueX%20measurements.%20We%20demonstrate%20that%20GNN-based%20track%20finding%0Aoutperforms%20the%20currently%20used%20traditional%20method%20at%20GlueX%20in%20terms%20of%0Asegment-based%20efficiency%20at%20a%20fixed%20purity%20while%20providing%20faster%20inferences.%0AWe%20show%20that%20the%20GNN%20model%20can%20achieve%20significant%20speedup%20by%20processing%0Amultiple%20events%20in%20batches%2C%20which%20exploits%20the%20parallel%20computation%20capability%0Aof%20Graphical%20Processing%20Units%20%28GPUs%29.%20Finally%2C%20we%20compare%20the%20GNN%0Aimplementation%20on%20GPU%20and%20FPGA%20and%20describe%20the%20trade-off.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22504v1&entry.124074799=Read"},
{"title": "Empowering Intelligent Low-altitude Economy with Large AI Model\n  Deployment", "author": "Zhonghao Lyu and Yulan Gao and Junting Chen and Hongyang Du and Jie Xu and Kaibin Huang and Dong In Kim", "abstract": "  Low-altitude economy (LAE) represents an emerging economic paradigm that\nredefines commercial and social aerial activities. Large artificial\nintelligence models (LAIMs) offer transformative potential to further enhance\nthe intelligence of LAE services. However, deploying LAIMs in LAE poses several\nchallenges, including the significant gap between their computational/storage\ndemands and the limited onboard resources of LAE entities, the mismatch between\nlab-trained LAIMs and dynamic physical environments, and the inefficiencies of\ntraditional decoupled designs for sensing, communication, and computation. To\naddress these issues, we first propose a hierarchical system architecture\ntailored for LAIM deployment and present representative LAE application\nscenarios. Next, we explore key enabling techniques that facilitate the mutual\nco-evolution of LAIMs and low-altitude systems, and introduce a task-oriented\nexecution pipeline for scalable and adaptive service delivery. Then, the\nproposed framework is validated through real-world case studies. Finally, we\noutline open challenges to inspire future research.\n", "link": "http://arxiv.org/abs/2505.22343v1", "date": "2025-05-28", "relevancy": 2.5066, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.516}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4944}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empowering%20Intelligent%20Low-altitude%20Economy%20with%20Large%20AI%20Model%0A%20%20Deployment&body=Title%3A%20Empowering%20Intelligent%20Low-altitude%20Economy%20with%20Large%20AI%20Model%0A%20%20Deployment%0AAuthor%3A%20Zhonghao%20Lyu%20and%20Yulan%20Gao%20and%20Junting%20Chen%20and%20Hongyang%20Du%20and%20Jie%20Xu%20and%20Kaibin%20Huang%20and%20Dong%20In%20Kim%0AAbstract%3A%20%20%20Low-altitude%20economy%20%28LAE%29%20represents%20an%20emerging%20economic%20paradigm%20that%0Aredefines%20commercial%20and%20social%20aerial%20activities.%20Large%20artificial%0Aintelligence%20models%20%28LAIMs%29%20offer%20transformative%20potential%20to%20further%20enhance%0Athe%20intelligence%20of%20LAE%20services.%20However%2C%20deploying%20LAIMs%20in%20LAE%20poses%20several%0Achallenges%2C%20including%20the%20significant%20gap%20between%20their%20computational/storage%0Ademands%20and%20the%20limited%20onboard%20resources%20of%20LAE%20entities%2C%20the%20mismatch%20between%0Alab-trained%20LAIMs%20and%20dynamic%20physical%20environments%2C%20and%20the%20inefficiencies%20of%0Atraditional%20decoupled%20designs%20for%20sensing%2C%20communication%2C%20and%20computation.%20To%0Aaddress%20these%20issues%2C%20we%20first%20propose%20a%20hierarchical%20system%20architecture%0Atailored%20for%20LAIM%20deployment%20and%20present%20representative%20LAE%20application%0Ascenarios.%20Next%2C%20we%20explore%20key%20enabling%20techniques%20that%20facilitate%20the%20mutual%0Aco-evolution%20of%20LAIMs%20and%20low-altitude%20systems%2C%20and%20introduce%20a%20task-oriented%0Aexecution%20pipeline%20for%20scalable%20and%20adaptive%20service%20delivery.%20Then%2C%20the%0Aproposed%20framework%20is%20validated%20through%20real-world%20case%20studies.%20Finally%2C%20we%0Aoutline%20open%20challenges%20to%20inspire%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22343v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpowering%2520Intelligent%2520Low-altitude%2520Economy%2520with%2520Large%2520AI%2520Model%250A%2520%2520Deployment%26entry.906535625%3DZhonghao%2520Lyu%2520and%2520Yulan%2520Gao%2520and%2520Junting%2520Chen%2520and%2520Hongyang%2520Du%2520and%2520Jie%2520Xu%2520and%2520Kaibin%2520Huang%2520and%2520Dong%2520In%2520Kim%26entry.1292438233%3D%2520%2520Low-altitude%2520economy%2520%2528LAE%2529%2520represents%2520an%2520emerging%2520economic%2520paradigm%2520that%250Aredefines%2520commercial%2520and%2520social%2520aerial%2520activities.%2520Large%2520artificial%250Aintelligence%2520models%2520%2528LAIMs%2529%2520offer%2520transformative%2520potential%2520to%2520further%2520enhance%250Athe%2520intelligence%2520of%2520LAE%2520services.%2520However%252C%2520deploying%2520LAIMs%2520in%2520LAE%2520poses%2520several%250Achallenges%252C%2520including%2520the%2520significant%2520gap%2520between%2520their%2520computational/storage%250Ademands%2520and%2520the%2520limited%2520onboard%2520resources%2520of%2520LAE%2520entities%252C%2520the%2520mismatch%2520between%250Alab-trained%2520LAIMs%2520and%2520dynamic%2520physical%2520environments%252C%2520and%2520the%2520inefficiencies%2520of%250Atraditional%2520decoupled%2520designs%2520for%2520sensing%252C%2520communication%252C%2520and%2520computation.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520first%2520propose%2520a%2520hierarchical%2520system%2520architecture%250Atailored%2520for%2520LAIM%2520deployment%2520and%2520present%2520representative%2520LAE%2520application%250Ascenarios.%2520Next%252C%2520we%2520explore%2520key%2520enabling%2520techniques%2520that%2520facilitate%2520the%2520mutual%250Aco-evolution%2520of%2520LAIMs%2520and%2520low-altitude%2520systems%252C%2520and%2520introduce%2520a%2520task-oriented%250Aexecution%2520pipeline%2520for%2520scalable%2520and%2520adaptive%2520service%2520delivery.%2520Then%252C%2520the%250Aproposed%2520framework%2520is%2520validated%2520through%2520real-world%2520case%2520studies.%2520Finally%252C%2520we%250Aoutline%2520open%2520challenges%2520to%2520inspire%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22343v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empowering%20Intelligent%20Low-altitude%20Economy%20with%20Large%20AI%20Model%0A%20%20Deployment&entry.906535625=Zhonghao%20Lyu%20and%20Yulan%20Gao%20and%20Junting%20Chen%20and%20Hongyang%20Du%20and%20Jie%20Xu%20and%20Kaibin%20Huang%20and%20Dong%20In%20Kim&entry.1292438233=%20%20Low-altitude%20economy%20%28LAE%29%20represents%20an%20emerging%20economic%20paradigm%20that%0Aredefines%20commercial%20and%20social%20aerial%20activities.%20Large%20artificial%0Aintelligence%20models%20%28LAIMs%29%20offer%20transformative%20potential%20to%20further%20enhance%0Athe%20intelligence%20of%20LAE%20services.%20However%2C%20deploying%20LAIMs%20in%20LAE%20poses%20several%0Achallenges%2C%20including%20the%20significant%20gap%20between%20their%20computational/storage%0Ademands%20and%20the%20limited%20onboard%20resources%20of%20LAE%20entities%2C%20the%20mismatch%20between%0Alab-trained%20LAIMs%20and%20dynamic%20physical%20environments%2C%20and%20the%20inefficiencies%20of%0Atraditional%20decoupled%20designs%20for%20sensing%2C%20communication%2C%20and%20computation.%20To%0Aaddress%20these%20issues%2C%20we%20first%20propose%20a%20hierarchical%20system%20architecture%0Atailored%20for%20LAIM%20deployment%20and%20present%20representative%20LAE%20application%0Ascenarios.%20Next%2C%20we%20explore%20key%20enabling%20techniques%20that%20facilitate%20the%20mutual%0Aco-evolution%20of%20LAIMs%20and%20low-altitude%20systems%2C%20and%20introduce%20a%20task-oriented%0Aexecution%20pipeline%20for%20scalable%20and%20adaptive%20service%20delivery.%20Then%2C%20the%0Aproposed%20framework%20is%20validated%20through%20real-world%20case%20studies.%20Finally%2C%20we%0Aoutline%20open%20challenges%20to%20inspire%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22343v1&entry.124074799=Read"},
{"title": "Closed-Form Training Dynamics Reveal Learned Features and Linear\n  Structure in Word2Vec-like Models", "author": "Dhruva Karkada and James B. Simon and Yasaman Bahri and Michael R. DeWeese", "abstract": "  Self-supervised word embedding algorithms such as word2vec provide a minimal\nsetting for studying representation learning in language modeling. We examine\nthe quartic Taylor approximation of the word2vec loss around the origin, and we\nshow that both the resulting training dynamics and the final performance on\ndownstream tasks are empirically very similar to those of word2vec. Our main\ncontribution is to analytically solve for both the gradient flow training\ndynamics and the final word embeddings in terms of only the corpus statistics\nand training hyperparameters. The solutions reveal that these models learn\northogonal linear subspaces one at a time, each one incrementing the effective\nrank of the embeddings until model capacity is saturated. Training on\nWikipedia, we find that each of the top linear subspaces represents an\ninterpretable topic-level concept. Finally, we apply our theory to describe how\nlinear representations of more abstract semantic concepts emerge during\ntraining; these can be used to complete analogies via vector addition.\n", "link": "http://arxiv.org/abs/2502.09863v2", "date": "2025-05-28", "relevancy": 2.4938, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5047}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Closed-Form%20Training%20Dynamics%20Reveal%20Learned%20Features%20and%20Linear%0A%20%20Structure%20in%20Word2Vec-like%20Models&body=Title%3A%20Closed-Form%20Training%20Dynamics%20Reveal%20Learned%20Features%20and%20Linear%0A%20%20Structure%20in%20Word2Vec-like%20Models%0AAuthor%3A%20Dhruva%20Karkada%20and%20James%20B.%20Simon%20and%20Yasaman%20Bahri%20and%20Michael%20R.%20DeWeese%0AAbstract%3A%20%20%20Self-supervised%20word%20embedding%20algorithms%20such%20as%20word2vec%20provide%20a%20minimal%0Asetting%20for%20studying%20representation%20learning%20in%20language%20modeling.%20We%20examine%0Athe%20quartic%20Taylor%20approximation%20of%20the%20word2vec%20loss%20around%20the%20origin%2C%20and%20we%0Ashow%20that%20both%20the%20resulting%20training%20dynamics%20and%20the%20final%20performance%20on%0Adownstream%20tasks%20are%20empirically%20very%20similar%20to%20those%20of%20word2vec.%20Our%20main%0Acontribution%20is%20to%20analytically%20solve%20for%20both%20the%20gradient%20flow%20training%0Adynamics%20and%20the%20final%20word%20embeddings%20in%20terms%20of%20only%20the%20corpus%20statistics%0Aand%20training%20hyperparameters.%20The%20solutions%20reveal%20that%20these%20models%20learn%0Aorthogonal%20linear%20subspaces%20one%20at%20a%20time%2C%20each%20one%20incrementing%20the%20effective%0Arank%20of%20the%20embeddings%20until%20model%20capacity%20is%20saturated.%20Training%20on%0AWikipedia%2C%20we%20find%20that%20each%20of%20the%20top%20linear%20subspaces%20represents%20an%0Ainterpretable%20topic-level%20concept.%20Finally%2C%20we%20apply%20our%20theory%20to%20describe%20how%0Alinear%20representations%20of%20more%20abstract%20semantic%20concepts%20emerge%20during%0Atraining%3B%20these%20can%20be%20used%20to%20complete%20analogies%20via%20vector%20addition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09863v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClosed-Form%2520Training%2520Dynamics%2520Reveal%2520Learned%2520Features%2520and%2520Linear%250A%2520%2520Structure%2520in%2520Word2Vec-like%2520Models%26entry.906535625%3DDhruva%2520Karkada%2520and%2520James%2520B.%2520Simon%2520and%2520Yasaman%2520Bahri%2520and%2520Michael%2520R.%2520DeWeese%26entry.1292438233%3D%2520%2520Self-supervised%2520word%2520embedding%2520algorithms%2520such%2520as%2520word2vec%2520provide%2520a%2520minimal%250Asetting%2520for%2520studying%2520representation%2520learning%2520in%2520language%2520modeling.%2520We%2520examine%250Athe%2520quartic%2520Taylor%2520approximation%2520of%2520the%2520word2vec%2520loss%2520around%2520the%2520origin%252C%2520and%2520we%250Ashow%2520that%2520both%2520the%2520resulting%2520training%2520dynamics%2520and%2520the%2520final%2520performance%2520on%250Adownstream%2520tasks%2520are%2520empirically%2520very%2520similar%2520to%2520those%2520of%2520word2vec.%2520Our%2520main%250Acontribution%2520is%2520to%2520analytically%2520solve%2520for%2520both%2520the%2520gradient%2520flow%2520training%250Adynamics%2520and%2520the%2520final%2520word%2520embeddings%2520in%2520terms%2520of%2520only%2520the%2520corpus%2520statistics%250Aand%2520training%2520hyperparameters.%2520The%2520solutions%2520reveal%2520that%2520these%2520models%2520learn%250Aorthogonal%2520linear%2520subspaces%2520one%2520at%2520a%2520time%252C%2520each%2520one%2520incrementing%2520the%2520effective%250Arank%2520of%2520the%2520embeddings%2520until%2520model%2520capacity%2520is%2520saturated.%2520Training%2520on%250AWikipedia%252C%2520we%2520find%2520that%2520each%2520of%2520the%2520top%2520linear%2520subspaces%2520represents%2520an%250Ainterpretable%2520topic-level%2520concept.%2520Finally%252C%2520we%2520apply%2520our%2520theory%2520to%2520describe%2520how%250Alinear%2520representations%2520of%2520more%2520abstract%2520semantic%2520concepts%2520emerge%2520during%250Atraining%253B%2520these%2520can%2520be%2520used%2520to%2520complete%2520analogies%2520via%2520vector%2520addition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09863v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Closed-Form%20Training%20Dynamics%20Reveal%20Learned%20Features%20and%20Linear%0A%20%20Structure%20in%20Word2Vec-like%20Models&entry.906535625=Dhruva%20Karkada%20and%20James%20B.%20Simon%20and%20Yasaman%20Bahri%20and%20Michael%20R.%20DeWeese&entry.1292438233=%20%20Self-supervised%20word%20embedding%20algorithms%20such%20as%20word2vec%20provide%20a%20minimal%0Asetting%20for%20studying%20representation%20learning%20in%20language%20modeling.%20We%20examine%0Athe%20quartic%20Taylor%20approximation%20of%20the%20word2vec%20loss%20around%20the%20origin%2C%20and%20we%0Ashow%20that%20both%20the%20resulting%20training%20dynamics%20and%20the%20final%20performance%20on%0Adownstream%20tasks%20are%20empirically%20very%20similar%20to%20those%20of%20word2vec.%20Our%20main%0Acontribution%20is%20to%20analytically%20solve%20for%20both%20the%20gradient%20flow%20training%0Adynamics%20and%20the%20final%20word%20embeddings%20in%20terms%20of%20only%20the%20corpus%20statistics%0Aand%20training%20hyperparameters.%20The%20solutions%20reveal%20that%20these%20models%20learn%0Aorthogonal%20linear%20subspaces%20one%20at%20a%20time%2C%20each%20one%20incrementing%20the%20effective%0Arank%20of%20the%20embeddings%20until%20model%20capacity%20is%20saturated.%20Training%20on%0AWikipedia%2C%20we%20find%20that%20each%20of%20the%20top%20linear%20subspaces%20represents%20an%0Ainterpretable%20topic-level%20concept.%20Finally%2C%20we%20apply%20our%20theory%20to%20describe%20how%0Alinear%20representations%20of%20more%20abstract%20semantic%20concepts%20emerge%20during%0Atraining%3B%20these%20can%20be%20used%20to%20complete%20analogies%20via%20vector%20addition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09863v2&entry.124074799=Read"},
{"title": "ImageReFL: Balancing Quality and Diversity in Human-Aligned Diffusion\n  Models", "author": "Dmitrii Sorokin and Maksim Nakhodnov and Andrey Kuznetsov and Aibek Alanov", "abstract": "  Recent advances in diffusion models have led to impressive image generation\ncapabilities, but aligning these models with human preferences remains\nchallenging. Reward-based fine-tuning using models trained on human feedback\nimproves alignment but often harms diversity, producing less varied outputs. In\nthis work, we address this trade-off with two contributions. First, we\nintroduce \\textit{combined generation}, a novel sampling strategy that applies\na reward-tuned diffusion model only in the later stages of the generation\nprocess, while preserving the base model for earlier steps. This approach\nmitigates early-stage overfitting and helps retain global structure and\ndiversity. Second, we propose \\textit{ImageReFL}, a fine-tuning method that\nimproves image diversity with minimal loss in quality by training on real\nimages and incorporating multiple regularizers, including diffusion and ReFL\nlosses. Our approach outperforms conventional reward tuning methods on standard\nquality and diversity metrics. A user study further confirms that our method\nbetter balances human preference alignment and visual diversity. The source\ncode can be found at https://github.com/ControlGenAI/ImageReFL .\n", "link": "http://arxiv.org/abs/2505.22569v1", "date": "2025-05-28", "relevancy": 2.4853, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6446}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6289}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ImageReFL%3A%20Balancing%20Quality%20and%20Diversity%20in%20Human-Aligned%20Diffusion%0A%20%20Models&body=Title%3A%20ImageReFL%3A%20Balancing%20Quality%20and%20Diversity%20in%20Human-Aligned%20Diffusion%0A%20%20Models%0AAuthor%3A%20Dmitrii%20Sorokin%20and%20Maksim%20Nakhodnov%20and%20Andrey%20Kuznetsov%20and%20Aibek%20Alanov%0AAbstract%3A%20%20%20Recent%20advances%20in%20diffusion%20models%20have%20led%20to%20impressive%20image%20generation%0Acapabilities%2C%20but%20aligning%20these%20models%20with%20human%20preferences%20remains%0Achallenging.%20Reward-based%20fine-tuning%20using%20models%20trained%20on%20human%20feedback%0Aimproves%20alignment%20but%20often%20harms%20diversity%2C%20producing%20less%20varied%20outputs.%20In%0Athis%20work%2C%20we%20address%20this%20trade-off%20with%20two%20contributions.%20First%2C%20we%0Aintroduce%20%5Ctextit%7Bcombined%20generation%7D%2C%20a%20novel%20sampling%20strategy%20that%20applies%0Aa%20reward-tuned%20diffusion%20model%20only%20in%20the%20later%20stages%20of%20the%20generation%0Aprocess%2C%20while%20preserving%20the%20base%20model%20for%20earlier%20steps.%20This%20approach%0Amitigates%20early-stage%20overfitting%20and%20helps%20retain%20global%20structure%20and%0Adiversity.%20Second%2C%20we%20propose%20%5Ctextit%7BImageReFL%7D%2C%20a%20fine-tuning%20method%20that%0Aimproves%20image%20diversity%20with%20minimal%20loss%20in%20quality%20by%20training%20on%20real%0Aimages%20and%20incorporating%20multiple%20regularizers%2C%20including%20diffusion%20and%20ReFL%0Alosses.%20Our%20approach%20outperforms%20conventional%20reward%20tuning%20methods%20on%20standard%0Aquality%20and%20diversity%20metrics.%20A%20user%20study%20further%20confirms%20that%20our%20method%0Abetter%20balances%20human%20preference%20alignment%20and%20visual%20diversity.%20The%20source%0Acode%20can%20be%20found%20at%20https%3A//github.com/ControlGenAI/ImageReFL%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22569v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImageReFL%253A%2520Balancing%2520Quality%2520and%2520Diversity%2520in%2520Human-Aligned%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DDmitrii%2520Sorokin%2520and%2520Maksim%2520Nakhodnov%2520and%2520Andrey%2520Kuznetsov%2520and%2520Aibek%2520Alanov%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520diffusion%2520models%2520have%2520led%2520to%2520impressive%2520image%2520generation%250Acapabilities%252C%2520but%2520aligning%2520these%2520models%2520with%2520human%2520preferences%2520remains%250Achallenging.%2520Reward-based%2520fine-tuning%2520using%2520models%2520trained%2520on%2520human%2520feedback%250Aimproves%2520alignment%2520but%2520often%2520harms%2520diversity%252C%2520producing%2520less%2520varied%2520outputs.%2520In%250Athis%2520work%252C%2520we%2520address%2520this%2520trade-off%2520with%2520two%2520contributions.%2520First%252C%2520we%250Aintroduce%2520%255Ctextit%257Bcombined%2520generation%257D%252C%2520a%2520novel%2520sampling%2520strategy%2520that%2520applies%250Aa%2520reward-tuned%2520diffusion%2520model%2520only%2520in%2520the%2520later%2520stages%2520of%2520the%2520generation%250Aprocess%252C%2520while%2520preserving%2520the%2520base%2520model%2520for%2520earlier%2520steps.%2520This%2520approach%250Amitigates%2520early-stage%2520overfitting%2520and%2520helps%2520retain%2520global%2520structure%2520and%250Adiversity.%2520Second%252C%2520we%2520propose%2520%255Ctextit%257BImageReFL%257D%252C%2520a%2520fine-tuning%2520method%2520that%250Aimproves%2520image%2520diversity%2520with%2520minimal%2520loss%2520in%2520quality%2520by%2520training%2520on%2520real%250Aimages%2520and%2520incorporating%2520multiple%2520regularizers%252C%2520including%2520diffusion%2520and%2520ReFL%250Alosses.%2520Our%2520approach%2520outperforms%2520conventional%2520reward%2520tuning%2520methods%2520on%2520standard%250Aquality%2520and%2520diversity%2520metrics.%2520A%2520user%2520study%2520further%2520confirms%2520that%2520our%2520method%250Abetter%2520balances%2520human%2520preference%2520alignment%2520and%2520visual%2520diversity.%2520The%2520source%250Acode%2520can%2520be%2520found%2520at%2520https%253A//github.com/ControlGenAI/ImageReFL%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22569v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ImageReFL%3A%20Balancing%20Quality%20and%20Diversity%20in%20Human-Aligned%20Diffusion%0A%20%20Models&entry.906535625=Dmitrii%20Sorokin%20and%20Maksim%20Nakhodnov%20and%20Andrey%20Kuznetsov%20and%20Aibek%20Alanov&entry.1292438233=%20%20Recent%20advances%20in%20diffusion%20models%20have%20led%20to%20impressive%20image%20generation%0Acapabilities%2C%20but%20aligning%20these%20models%20with%20human%20preferences%20remains%0Achallenging.%20Reward-based%20fine-tuning%20using%20models%20trained%20on%20human%20feedback%0Aimproves%20alignment%20but%20often%20harms%20diversity%2C%20producing%20less%20varied%20outputs.%20In%0Athis%20work%2C%20we%20address%20this%20trade-off%20with%20two%20contributions.%20First%2C%20we%0Aintroduce%20%5Ctextit%7Bcombined%20generation%7D%2C%20a%20novel%20sampling%20strategy%20that%20applies%0Aa%20reward-tuned%20diffusion%20model%20only%20in%20the%20later%20stages%20of%20the%20generation%0Aprocess%2C%20while%20preserving%20the%20base%20model%20for%20earlier%20steps.%20This%20approach%0Amitigates%20early-stage%20overfitting%20and%20helps%20retain%20global%20structure%20and%0Adiversity.%20Second%2C%20we%20propose%20%5Ctextit%7BImageReFL%7D%2C%20a%20fine-tuning%20method%20that%0Aimproves%20image%20diversity%20with%20minimal%20loss%20in%20quality%20by%20training%20on%20real%0Aimages%20and%20incorporating%20multiple%20regularizers%2C%20including%20diffusion%20and%20ReFL%0Alosses.%20Our%20approach%20outperforms%20conventional%20reward%20tuning%20methods%20on%20standard%0Aquality%20and%20diversity%20metrics.%20A%20user%20study%20further%20confirms%20that%20our%20method%0Abetter%20balances%20human%20preference%20alignment%20and%20visual%20diversity.%20The%20source%0Acode%20can%20be%20found%20at%20https%3A//github.com/ControlGenAI/ImageReFL%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22569v1&entry.124074799=Read"},
{"title": "Identity-Preserving Text-to-Image Generation via Dual-Level Feature\n  Decoupling and Expert-Guided Fusion", "author": "Kewen Chen and Xiaobin Hu and Wenqi Ren", "abstract": "  Recent advances in large-scale text-to-image generation models have led to a\nsurge in subject-driven text-to-image generation, which aims to produce\ncustomized images that align with textual descriptions while preserving the\nidentity of specific subjects. Despite significant progress, current methods\nstruggle to disentangle identity-relevant information from identity-irrelevant\ndetails in the input images, resulting in overfitting or failure to maintain\nsubject identity. In this work, we propose a novel framework that improves the\nseparation of identity-related and identity-unrelated features and introduces\nan innovative feature fusion mechanism to improve the quality and text\nalignment of generated images. Our framework consists of two key components: an\nImplicit-Explicit foreground-background Decoupling Module (IEDM) and a Feature\nFusion Module (FFM) based on a Mixture of Experts (MoE). IEDM combines\nlearnable adapters for implicit decoupling at the feature level with inpainting\ntechniques for explicit foreground-background separation at the image level.\nFFM dynamically integrates identity-irrelevant features with identity-related\nfeatures, enabling refined feature representations even in cases of incomplete\ndecoupling. In addition, we introduce three complementary loss functions to\nguide the decoupling process. Extensive experiments demonstrate the\neffectiveness of our proposed method in enhancing image generation quality,\nimproving flexibility in scene adaptation, and increasing the diversity of\ngenerated outputs across various textual descriptions.\n", "link": "http://arxiv.org/abs/2505.22360v1", "date": "2025-05-28", "relevancy": 2.458, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6735}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.575}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identity-Preserving%20Text-to-Image%20Generation%20via%20Dual-Level%20Feature%0A%20%20Decoupling%20and%20Expert-Guided%20Fusion&body=Title%3A%20Identity-Preserving%20Text-to-Image%20Generation%20via%20Dual-Level%20Feature%0A%20%20Decoupling%20and%20Expert-Guided%20Fusion%0AAuthor%3A%20Kewen%20Chen%20and%20Xiaobin%20Hu%20and%20Wenqi%20Ren%0AAbstract%3A%20%20%20Recent%20advances%20in%20large-scale%20text-to-image%20generation%20models%20have%20led%20to%20a%0Asurge%20in%20subject-driven%20text-to-image%20generation%2C%20which%20aims%20to%20produce%0Acustomized%20images%20that%20align%20with%20textual%20descriptions%20while%20preserving%20the%0Aidentity%20of%20specific%20subjects.%20Despite%20significant%20progress%2C%20current%20methods%0Astruggle%20to%20disentangle%20identity-relevant%20information%20from%20identity-irrelevant%0Adetails%20in%20the%20input%20images%2C%20resulting%20in%20overfitting%20or%20failure%20to%20maintain%0Asubject%20identity.%20In%20this%20work%2C%20we%20propose%20a%20novel%20framework%20that%20improves%20the%0Aseparation%20of%20identity-related%20and%20identity-unrelated%20features%20and%20introduces%0Aan%20innovative%20feature%20fusion%20mechanism%20to%20improve%20the%20quality%20and%20text%0Aalignment%20of%20generated%20images.%20Our%20framework%20consists%20of%20two%20key%20components%3A%20an%0AImplicit-Explicit%20foreground-background%20Decoupling%20Module%20%28IEDM%29%20and%20a%20Feature%0AFusion%20Module%20%28FFM%29%20based%20on%20a%20Mixture%20of%20Experts%20%28MoE%29.%20IEDM%20combines%0Alearnable%20adapters%20for%20implicit%20decoupling%20at%20the%20feature%20level%20with%20inpainting%0Atechniques%20for%20explicit%20foreground-background%20separation%20at%20the%20image%20level.%0AFFM%20dynamically%20integrates%20identity-irrelevant%20features%20with%20identity-related%0Afeatures%2C%20enabling%20refined%20feature%20representations%20even%20in%20cases%20of%20incomplete%0Adecoupling.%20In%20addition%2C%20we%20introduce%20three%20complementary%20loss%20functions%20to%0Aguide%20the%20decoupling%20process.%20Extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20proposed%20method%20in%20enhancing%20image%20generation%20quality%2C%0Aimproving%20flexibility%20in%20scene%20adaptation%2C%20and%20increasing%20the%20diversity%20of%0Agenerated%20outputs%20across%20various%20textual%20descriptions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22360v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentity-Preserving%2520Text-to-Image%2520Generation%2520via%2520Dual-Level%2520Feature%250A%2520%2520Decoupling%2520and%2520Expert-Guided%2520Fusion%26entry.906535625%3DKewen%2520Chen%2520and%2520Xiaobin%2520Hu%2520and%2520Wenqi%2520Ren%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large-scale%2520text-to-image%2520generation%2520models%2520have%2520led%2520to%2520a%250Asurge%2520in%2520subject-driven%2520text-to-image%2520generation%252C%2520which%2520aims%2520to%2520produce%250Acustomized%2520images%2520that%2520align%2520with%2520textual%2520descriptions%2520while%2520preserving%2520the%250Aidentity%2520of%2520specific%2520subjects.%2520Despite%2520significant%2520progress%252C%2520current%2520methods%250Astruggle%2520to%2520disentangle%2520identity-relevant%2520information%2520from%2520identity-irrelevant%250Adetails%2520in%2520the%2520input%2520images%252C%2520resulting%2520in%2520overfitting%2520or%2520failure%2520to%2520maintain%250Asubject%2520identity.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520framework%2520that%2520improves%2520the%250Aseparation%2520of%2520identity-related%2520and%2520identity-unrelated%2520features%2520and%2520introduces%250Aan%2520innovative%2520feature%2520fusion%2520mechanism%2520to%2520improve%2520the%2520quality%2520and%2520text%250Aalignment%2520of%2520generated%2520images.%2520Our%2520framework%2520consists%2520of%2520two%2520key%2520components%253A%2520an%250AImplicit-Explicit%2520foreground-background%2520Decoupling%2520Module%2520%2528IEDM%2529%2520and%2520a%2520Feature%250AFusion%2520Module%2520%2528FFM%2529%2520based%2520on%2520a%2520Mixture%2520of%2520Experts%2520%2528MoE%2529.%2520IEDM%2520combines%250Alearnable%2520adapters%2520for%2520implicit%2520decoupling%2520at%2520the%2520feature%2520level%2520with%2520inpainting%250Atechniques%2520for%2520explicit%2520foreground-background%2520separation%2520at%2520the%2520image%2520level.%250AFFM%2520dynamically%2520integrates%2520identity-irrelevant%2520features%2520with%2520identity-related%250Afeatures%252C%2520enabling%2520refined%2520feature%2520representations%2520even%2520in%2520cases%2520of%2520incomplete%250Adecoupling.%2520In%2520addition%252C%2520we%2520introduce%2520three%2520complementary%2520loss%2520functions%2520to%250Aguide%2520the%2520decoupling%2520process.%2520Extensive%2520experiments%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520proposed%2520method%2520in%2520enhancing%2520image%2520generation%2520quality%252C%250Aimproving%2520flexibility%2520in%2520scene%2520adaptation%252C%2520and%2520increasing%2520the%2520diversity%2520of%250Agenerated%2520outputs%2520across%2520various%2520textual%2520descriptions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22360v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identity-Preserving%20Text-to-Image%20Generation%20via%20Dual-Level%20Feature%0A%20%20Decoupling%20and%20Expert-Guided%20Fusion&entry.906535625=Kewen%20Chen%20and%20Xiaobin%20Hu%20and%20Wenqi%20Ren&entry.1292438233=%20%20Recent%20advances%20in%20large-scale%20text-to-image%20generation%20models%20have%20led%20to%20a%0Asurge%20in%20subject-driven%20text-to-image%20generation%2C%20which%20aims%20to%20produce%0Acustomized%20images%20that%20align%20with%20textual%20descriptions%20while%20preserving%20the%0Aidentity%20of%20specific%20subjects.%20Despite%20significant%20progress%2C%20current%20methods%0Astruggle%20to%20disentangle%20identity-relevant%20information%20from%20identity-irrelevant%0Adetails%20in%20the%20input%20images%2C%20resulting%20in%20overfitting%20or%20failure%20to%20maintain%0Asubject%20identity.%20In%20this%20work%2C%20we%20propose%20a%20novel%20framework%20that%20improves%20the%0Aseparation%20of%20identity-related%20and%20identity-unrelated%20features%20and%20introduces%0Aan%20innovative%20feature%20fusion%20mechanism%20to%20improve%20the%20quality%20and%20text%0Aalignment%20of%20generated%20images.%20Our%20framework%20consists%20of%20two%20key%20components%3A%20an%0AImplicit-Explicit%20foreground-background%20Decoupling%20Module%20%28IEDM%29%20and%20a%20Feature%0AFusion%20Module%20%28FFM%29%20based%20on%20a%20Mixture%20of%20Experts%20%28MoE%29.%20IEDM%20combines%0Alearnable%20adapters%20for%20implicit%20decoupling%20at%20the%20feature%20level%20with%20inpainting%0Atechniques%20for%20explicit%20foreground-background%20separation%20at%20the%20image%20level.%0AFFM%20dynamically%20integrates%20identity-irrelevant%20features%20with%20identity-related%0Afeatures%2C%20enabling%20refined%20feature%20representations%20even%20in%20cases%20of%20incomplete%0Adecoupling.%20In%20addition%2C%20we%20introduce%20three%20complementary%20loss%20functions%20to%0Aguide%20the%20decoupling%20process.%20Extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20proposed%20method%20in%20enhancing%20image%20generation%20quality%2C%0Aimproving%20flexibility%20in%20scene%20adaptation%2C%20and%20increasing%20the%20diversity%20of%0Agenerated%20outputs%20across%20various%20textual%20descriptions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22360v1&entry.124074799=Read"},
{"title": "Thinking with Generated Images", "author": "Ethan Chern and Zhulin Hu and Steffi Chern and Siqi Kou and Jiadi Su and Yan Ma and Zhijie Deng and Pengfei Liu", "abstract": "  We present Thinking with Generated Images, a novel paradigm that\nfundamentally transforms how large multimodal models (LMMs) engage with visual\nreasoning by enabling them to natively think across text and vision modalities\nthrough spontaneous generation of intermediate visual thinking steps. Current\nvisual reasoning with LMMs is constrained to either processing fixed\nuser-provided images or reasoning solely through text-based chain-of-thought\n(CoT). Thinking with Generated Images unlocks a new dimension of cognitive\ncapability where models can actively construct intermediate visual thoughts,\ncritique their own visual hypotheses, and refine them as integral components of\ntheir reasoning process. We demonstrate the effectiveness of our approach\nthrough two complementary mechanisms: (1) vision generation with intermediate\nvisual subgoals, where models decompose complex visual tasks into manageable\ncomponents that are generated and integrated progressively, and (2) vision\ngeneration with self-critique, where models generate an initial visual\nhypothesis, analyze its shortcomings through textual reasoning, and produce\nrefined outputs based on their own critiques. Our experiments on vision\ngeneration benchmarks show substantial improvements over baseline approaches,\nwith our models achieving up to 50% (from 38% to 57%) relative improvement in\nhandling complex multi-object scenarios. From biochemists exploring novel\nprotein structures, and architects iterating on spatial designs, to forensic\nanalysts reconstructing crime scenes, and basketball players envisioning\nstrategic plays, our approach enables AI models to engage in the kind of visual\nimagination and iterative refinement that characterizes human creative,\nanalytical, and strategic thinking. We release our open-source suite at\nhttps://github.com/GAIR-NLP/thinking-with-generated-images.\n", "link": "http://arxiv.org/abs/2505.22525v1", "date": "2025-05-28", "relevancy": 2.4508, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6334}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.626}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thinking%20with%20Generated%20Images&body=Title%3A%20Thinking%20with%20Generated%20Images%0AAuthor%3A%20Ethan%20Chern%20and%20Zhulin%20Hu%20and%20Steffi%20Chern%20and%20Siqi%20Kou%20and%20Jiadi%20Su%20and%20Yan%20Ma%20and%20Zhijie%20Deng%20and%20Pengfei%20Liu%0AAbstract%3A%20%20%20We%20present%20Thinking%20with%20Generated%20Images%2C%20a%20novel%20paradigm%20that%0Afundamentally%20transforms%20how%20large%20multimodal%20models%20%28LMMs%29%20engage%20with%20visual%0Areasoning%20by%20enabling%20them%20to%20natively%20think%20across%20text%20and%20vision%20modalities%0Athrough%20spontaneous%20generation%20of%20intermediate%20visual%20thinking%20steps.%20Current%0Avisual%20reasoning%20with%20LMMs%20is%20constrained%20to%20either%20processing%20fixed%0Auser-provided%20images%20or%20reasoning%20solely%20through%20text-based%20chain-of-thought%0A%28CoT%29.%20Thinking%20with%20Generated%20Images%20unlocks%20a%20new%20dimension%20of%20cognitive%0Acapability%20where%20models%20can%20actively%20construct%20intermediate%20visual%20thoughts%2C%0Acritique%20their%20own%20visual%20hypotheses%2C%20and%20refine%20them%20as%20integral%20components%20of%0Atheir%20reasoning%20process.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%0Athrough%20two%20complementary%20mechanisms%3A%20%281%29%20vision%20generation%20with%20intermediate%0Avisual%20subgoals%2C%20where%20models%20decompose%20complex%20visual%20tasks%20into%20manageable%0Acomponents%20that%20are%20generated%20and%20integrated%20progressively%2C%20and%20%282%29%20vision%0Ageneration%20with%20self-critique%2C%20where%20models%20generate%20an%20initial%20visual%0Ahypothesis%2C%20analyze%20its%20shortcomings%20through%20textual%20reasoning%2C%20and%20produce%0Arefined%20outputs%20based%20on%20their%20own%20critiques.%20Our%20experiments%20on%20vision%0Ageneration%20benchmarks%20show%20substantial%20improvements%20over%20baseline%20approaches%2C%0Awith%20our%20models%20achieving%20up%20to%2050%25%20%28from%2038%25%20to%2057%25%29%20relative%20improvement%20in%0Ahandling%20complex%20multi-object%20scenarios.%20From%20biochemists%20exploring%20novel%0Aprotein%20structures%2C%20and%20architects%20iterating%20on%20spatial%20designs%2C%20to%20forensic%0Aanalysts%20reconstructing%20crime%20scenes%2C%20and%20basketball%20players%20envisioning%0Astrategic%20plays%2C%20our%20approach%20enables%20AI%20models%20to%20engage%20in%20the%20kind%20of%20visual%0Aimagination%20and%20iterative%20refinement%20that%20characterizes%20human%20creative%2C%0Aanalytical%2C%20and%20strategic%20thinking.%20We%20release%20our%20open-source%20suite%20at%0Ahttps%3A//github.com/GAIR-NLP/thinking-with-generated-images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThinking%2520with%2520Generated%2520Images%26entry.906535625%3DEthan%2520Chern%2520and%2520Zhulin%2520Hu%2520and%2520Steffi%2520Chern%2520and%2520Siqi%2520Kou%2520and%2520Jiadi%2520Su%2520and%2520Yan%2520Ma%2520and%2520Zhijie%2520Deng%2520and%2520Pengfei%2520Liu%26entry.1292438233%3D%2520%2520We%2520present%2520Thinking%2520with%2520Generated%2520Images%252C%2520a%2520novel%2520paradigm%2520that%250Afundamentally%2520transforms%2520how%2520large%2520multimodal%2520models%2520%2528LMMs%2529%2520engage%2520with%2520visual%250Areasoning%2520by%2520enabling%2520them%2520to%2520natively%2520think%2520across%2520text%2520and%2520vision%2520modalities%250Athrough%2520spontaneous%2520generation%2520of%2520intermediate%2520visual%2520thinking%2520steps.%2520Current%250Avisual%2520reasoning%2520with%2520LMMs%2520is%2520constrained%2520to%2520either%2520processing%2520fixed%250Auser-provided%2520images%2520or%2520reasoning%2520solely%2520through%2520text-based%2520chain-of-thought%250A%2528CoT%2529.%2520Thinking%2520with%2520Generated%2520Images%2520unlocks%2520a%2520new%2520dimension%2520of%2520cognitive%250Acapability%2520where%2520models%2520can%2520actively%2520construct%2520intermediate%2520visual%2520thoughts%252C%250Acritique%2520their%2520own%2520visual%2520hypotheses%252C%2520and%2520refine%2520them%2520as%2520integral%2520components%2520of%250Atheir%2520reasoning%2520process.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%250Athrough%2520two%2520complementary%2520mechanisms%253A%2520%25281%2529%2520vision%2520generation%2520with%2520intermediate%250Avisual%2520subgoals%252C%2520where%2520models%2520decompose%2520complex%2520visual%2520tasks%2520into%2520manageable%250Acomponents%2520that%2520are%2520generated%2520and%2520integrated%2520progressively%252C%2520and%2520%25282%2529%2520vision%250Ageneration%2520with%2520self-critique%252C%2520where%2520models%2520generate%2520an%2520initial%2520visual%250Ahypothesis%252C%2520analyze%2520its%2520shortcomings%2520through%2520textual%2520reasoning%252C%2520and%2520produce%250Arefined%2520outputs%2520based%2520on%2520their%2520own%2520critiques.%2520Our%2520experiments%2520on%2520vision%250Ageneration%2520benchmarks%2520show%2520substantial%2520improvements%2520over%2520baseline%2520approaches%252C%250Awith%2520our%2520models%2520achieving%2520up%2520to%252050%2525%2520%2528from%252038%2525%2520to%252057%2525%2529%2520relative%2520improvement%2520in%250Ahandling%2520complex%2520multi-object%2520scenarios.%2520From%2520biochemists%2520exploring%2520novel%250Aprotein%2520structures%252C%2520and%2520architects%2520iterating%2520on%2520spatial%2520designs%252C%2520to%2520forensic%250Aanalysts%2520reconstructing%2520crime%2520scenes%252C%2520and%2520basketball%2520players%2520envisioning%250Astrategic%2520plays%252C%2520our%2520approach%2520enables%2520AI%2520models%2520to%2520engage%2520in%2520the%2520kind%2520of%2520visual%250Aimagination%2520and%2520iterative%2520refinement%2520that%2520characterizes%2520human%2520creative%252C%250Aanalytical%252C%2520and%2520strategic%2520thinking.%2520We%2520release%2520our%2520open-source%2520suite%2520at%250Ahttps%253A//github.com/GAIR-NLP/thinking-with-generated-images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thinking%20with%20Generated%20Images&entry.906535625=Ethan%20Chern%20and%20Zhulin%20Hu%20and%20Steffi%20Chern%20and%20Siqi%20Kou%20and%20Jiadi%20Su%20and%20Yan%20Ma%20and%20Zhijie%20Deng%20and%20Pengfei%20Liu&entry.1292438233=%20%20We%20present%20Thinking%20with%20Generated%20Images%2C%20a%20novel%20paradigm%20that%0Afundamentally%20transforms%20how%20large%20multimodal%20models%20%28LMMs%29%20engage%20with%20visual%0Areasoning%20by%20enabling%20them%20to%20natively%20think%20across%20text%20and%20vision%20modalities%0Athrough%20spontaneous%20generation%20of%20intermediate%20visual%20thinking%20steps.%20Current%0Avisual%20reasoning%20with%20LMMs%20is%20constrained%20to%20either%20processing%20fixed%0Auser-provided%20images%20or%20reasoning%20solely%20through%20text-based%20chain-of-thought%0A%28CoT%29.%20Thinking%20with%20Generated%20Images%20unlocks%20a%20new%20dimension%20of%20cognitive%0Acapability%20where%20models%20can%20actively%20construct%20intermediate%20visual%20thoughts%2C%0Acritique%20their%20own%20visual%20hypotheses%2C%20and%20refine%20them%20as%20integral%20components%20of%0Atheir%20reasoning%20process.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%0Athrough%20two%20complementary%20mechanisms%3A%20%281%29%20vision%20generation%20with%20intermediate%0Avisual%20subgoals%2C%20where%20models%20decompose%20complex%20visual%20tasks%20into%20manageable%0Acomponents%20that%20are%20generated%20and%20integrated%20progressively%2C%20and%20%282%29%20vision%0Ageneration%20with%20self-critique%2C%20where%20models%20generate%20an%20initial%20visual%0Ahypothesis%2C%20analyze%20its%20shortcomings%20through%20textual%20reasoning%2C%20and%20produce%0Arefined%20outputs%20based%20on%20their%20own%20critiques.%20Our%20experiments%20on%20vision%0Ageneration%20benchmarks%20show%20substantial%20improvements%20over%20baseline%20approaches%2C%0Awith%20our%20models%20achieving%20up%20to%2050%25%20%28from%2038%25%20to%2057%25%29%20relative%20improvement%20in%0Ahandling%20complex%20multi-object%20scenarios.%20From%20biochemists%20exploring%20novel%0Aprotein%20structures%2C%20and%20architects%20iterating%20on%20spatial%20designs%2C%20to%20forensic%0Aanalysts%20reconstructing%20crime%20scenes%2C%20and%20basketball%20players%20envisioning%0Astrategic%20plays%2C%20our%20approach%20enables%20AI%20models%20to%20engage%20in%20the%20kind%20of%20visual%0Aimagination%20and%20iterative%20refinement%20that%20characterizes%20human%20creative%2C%0Aanalytical%2C%20and%20strategic%20thinking.%20We%20release%20our%20open-source%20suite%20at%0Ahttps%3A//github.com/GAIR-NLP/thinking-with-generated-images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22525v1&entry.124074799=Read"},
{"title": "DIPO: Dual-State Images Controlled Articulated Object Generation Powered\n  by Diverse Data", "author": "Ruiqi Wu and Xinjie Wang and Liu Liu and Chunle Guo and Jiaxiong Qiu and Chongyi Li and Lichao Huang and Zhizhong Su and Ming-Ming Cheng", "abstract": "  We present DIPO, a novel framework for the controllable generation of\narticulated 3D objects from a pair of images: one depicting the object in a\nresting state and the other in an articulated state. Compared to the\nsingle-image approach, our dual-image input imposes only a modest overhead for\ndata collection, but at the same time provides important motion information,\nwhich is a reliable guide for predicting kinematic relationships between parts.\nSpecifically, we propose a dual-image diffusion model that captures\nrelationships between the image pair to generate part layouts and joint\nparameters. In addition, we introduce a Chain-of-Thought (CoT) based graph\nreasoner that explicitly infers part connectivity relationships. To further\nimprove robustness and generalization on complex articulated objects, we\ndevelop a fully automated dataset expansion pipeline, name LEGO-Art, that\nenriches the diversity and complexity of PartNet-Mobility dataset. We propose\nPM-X, a large-scale dataset of complex articulated 3D objects, accompanied by\nrendered images, URDF annotations, and textual descriptions. Extensive\nexperiments demonstrate that DIPO significantly outperforms existing baselines\nin both the resting state and the articulated state, while the proposed PM-X\ndataset further enhances generalization to diverse and structurally complex\narticulated objects. Our code and dataset will be released to the community\nupon publication.\n", "link": "http://arxiv.org/abs/2505.20460v2", "date": "2025-05-28", "relevancy": 2.4261, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6324}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5997}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DIPO%3A%20Dual-State%20Images%20Controlled%20Articulated%20Object%20Generation%20Powered%0A%20%20by%20Diverse%20Data&body=Title%3A%20DIPO%3A%20Dual-State%20Images%20Controlled%20Articulated%20Object%20Generation%20Powered%0A%20%20by%20Diverse%20Data%0AAuthor%3A%20Ruiqi%20Wu%20and%20Xinjie%20Wang%20and%20Liu%20Liu%20and%20Chunle%20Guo%20and%20Jiaxiong%20Qiu%20and%20Chongyi%20Li%20and%20Lichao%20Huang%20and%20Zhizhong%20Su%20and%20Ming-Ming%20Cheng%0AAbstract%3A%20%20%20We%20present%20DIPO%2C%20a%20novel%20framework%20for%20the%20controllable%20generation%20of%0Aarticulated%203D%20objects%20from%20a%20pair%20of%20images%3A%20one%20depicting%20the%20object%20in%20a%0Aresting%20state%20and%20the%20other%20in%20an%20articulated%20state.%20Compared%20to%20the%0Asingle-image%20approach%2C%20our%20dual-image%20input%20imposes%20only%20a%20modest%20overhead%20for%0Adata%20collection%2C%20but%20at%20the%20same%20time%20provides%20important%20motion%20information%2C%0Awhich%20is%20a%20reliable%20guide%20for%20predicting%20kinematic%20relationships%20between%20parts.%0ASpecifically%2C%20we%20propose%20a%20dual-image%20diffusion%20model%20that%20captures%0Arelationships%20between%20the%20image%20pair%20to%20generate%20part%20layouts%20and%20joint%0Aparameters.%20In%20addition%2C%20we%20introduce%20a%20Chain-of-Thought%20%28CoT%29%20based%20graph%0Areasoner%20that%20explicitly%20infers%20part%20connectivity%20relationships.%20To%20further%0Aimprove%20robustness%20and%20generalization%20on%20complex%20articulated%20objects%2C%20we%0Adevelop%20a%20fully%20automated%20dataset%20expansion%20pipeline%2C%20name%20LEGO-Art%2C%20that%0Aenriches%20the%20diversity%20and%20complexity%20of%20PartNet-Mobility%20dataset.%20We%20propose%0APM-X%2C%20a%20large-scale%20dataset%20of%20complex%20articulated%203D%20objects%2C%20accompanied%20by%0Arendered%20images%2C%20URDF%20annotations%2C%20and%20textual%20descriptions.%20Extensive%0Aexperiments%20demonstrate%20that%20DIPO%20significantly%20outperforms%20existing%20baselines%0Ain%20both%20the%20resting%20state%20and%20the%20articulated%20state%2C%20while%20the%20proposed%20PM-X%0Adataset%20further%20enhances%20generalization%20to%20diverse%20and%20structurally%20complex%0Aarticulated%20objects.%20Our%20code%20and%20dataset%20will%20be%20released%20to%20the%20community%0Aupon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20460v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDIPO%253A%2520Dual-State%2520Images%2520Controlled%2520Articulated%2520Object%2520Generation%2520Powered%250A%2520%2520by%2520Diverse%2520Data%26entry.906535625%3DRuiqi%2520Wu%2520and%2520Xinjie%2520Wang%2520and%2520Liu%2520Liu%2520and%2520Chunle%2520Guo%2520and%2520Jiaxiong%2520Qiu%2520and%2520Chongyi%2520Li%2520and%2520Lichao%2520Huang%2520and%2520Zhizhong%2520Su%2520and%2520Ming-Ming%2520Cheng%26entry.1292438233%3D%2520%2520We%2520present%2520DIPO%252C%2520a%2520novel%2520framework%2520for%2520the%2520controllable%2520generation%2520of%250Aarticulated%25203D%2520objects%2520from%2520a%2520pair%2520of%2520images%253A%2520one%2520depicting%2520the%2520object%2520in%2520a%250Aresting%2520state%2520and%2520the%2520other%2520in%2520an%2520articulated%2520state.%2520Compared%2520to%2520the%250Asingle-image%2520approach%252C%2520our%2520dual-image%2520input%2520imposes%2520only%2520a%2520modest%2520overhead%2520for%250Adata%2520collection%252C%2520but%2520at%2520the%2520same%2520time%2520provides%2520important%2520motion%2520information%252C%250Awhich%2520is%2520a%2520reliable%2520guide%2520for%2520predicting%2520kinematic%2520relationships%2520between%2520parts.%250ASpecifically%252C%2520we%2520propose%2520a%2520dual-image%2520diffusion%2520model%2520that%2520captures%250Arelationships%2520between%2520the%2520image%2520pair%2520to%2520generate%2520part%2520layouts%2520and%2520joint%250Aparameters.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520Chain-of-Thought%2520%2528CoT%2529%2520based%2520graph%250Areasoner%2520that%2520explicitly%2520infers%2520part%2520connectivity%2520relationships.%2520To%2520further%250Aimprove%2520robustness%2520and%2520generalization%2520on%2520complex%2520articulated%2520objects%252C%2520we%250Adevelop%2520a%2520fully%2520automated%2520dataset%2520expansion%2520pipeline%252C%2520name%2520LEGO-Art%252C%2520that%250Aenriches%2520the%2520diversity%2520and%2520complexity%2520of%2520PartNet-Mobility%2520dataset.%2520We%2520propose%250APM-X%252C%2520a%2520large-scale%2520dataset%2520of%2520complex%2520articulated%25203D%2520objects%252C%2520accompanied%2520by%250Arendered%2520images%252C%2520URDF%2520annotations%252C%2520and%2520textual%2520descriptions.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520DIPO%2520significantly%2520outperforms%2520existing%2520baselines%250Ain%2520both%2520the%2520resting%2520state%2520and%2520the%2520articulated%2520state%252C%2520while%2520the%2520proposed%2520PM-X%250Adataset%2520further%2520enhances%2520generalization%2520to%2520diverse%2520and%2520structurally%2520complex%250Aarticulated%2520objects.%2520Our%2520code%2520and%2520dataset%2520will%2520be%2520released%2520to%2520the%2520community%250Aupon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20460v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DIPO%3A%20Dual-State%20Images%20Controlled%20Articulated%20Object%20Generation%20Powered%0A%20%20by%20Diverse%20Data&entry.906535625=Ruiqi%20Wu%20and%20Xinjie%20Wang%20and%20Liu%20Liu%20and%20Chunle%20Guo%20and%20Jiaxiong%20Qiu%20and%20Chongyi%20Li%20and%20Lichao%20Huang%20and%20Zhizhong%20Su%20and%20Ming-Ming%20Cheng&entry.1292438233=%20%20We%20present%20DIPO%2C%20a%20novel%20framework%20for%20the%20controllable%20generation%20of%0Aarticulated%203D%20objects%20from%20a%20pair%20of%20images%3A%20one%20depicting%20the%20object%20in%20a%0Aresting%20state%20and%20the%20other%20in%20an%20articulated%20state.%20Compared%20to%20the%0Asingle-image%20approach%2C%20our%20dual-image%20input%20imposes%20only%20a%20modest%20overhead%20for%0Adata%20collection%2C%20but%20at%20the%20same%20time%20provides%20important%20motion%20information%2C%0Awhich%20is%20a%20reliable%20guide%20for%20predicting%20kinematic%20relationships%20between%20parts.%0ASpecifically%2C%20we%20propose%20a%20dual-image%20diffusion%20model%20that%20captures%0Arelationships%20between%20the%20image%20pair%20to%20generate%20part%20layouts%20and%20joint%0Aparameters.%20In%20addition%2C%20we%20introduce%20a%20Chain-of-Thought%20%28CoT%29%20based%20graph%0Areasoner%20that%20explicitly%20infers%20part%20connectivity%20relationships.%20To%20further%0Aimprove%20robustness%20and%20generalization%20on%20complex%20articulated%20objects%2C%20we%0Adevelop%20a%20fully%20automated%20dataset%20expansion%20pipeline%2C%20name%20LEGO-Art%2C%20that%0Aenriches%20the%20diversity%20and%20complexity%20of%20PartNet-Mobility%20dataset.%20We%20propose%0APM-X%2C%20a%20large-scale%20dataset%20of%20complex%20articulated%203D%20objects%2C%20accompanied%20by%0Arendered%20images%2C%20URDF%20annotations%2C%20and%20textual%20descriptions.%20Extensive%0Aexperiments%20demonstrate%20that%20DIPO%20significantly%20outperforms%20existing%20baselines%0Ain%20both%20the%20resting%20state%20and%20the%20articulated%20state%2C%20while%20the%20proposed%20PM-X%0Adataset%20further%20enhances%20generalization%20to%20diverse%20and%20structurally%20complex%0Aarticulated%20objects.%20Our%20code%20and%20dataset%20will%20be%20released%20to%20the%20community%0Aupon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20460v2&entry.124074799=Read"},
{"title": "Pre-training for Recommendation Unlearning", "author": "Guoxuan Chen and Lianghao Xia and Chao Huang", "abstract": "  Modern recommender systems powered by Graph Neural Networks (GNNs) excel at\nmodeling complex user-item interactions, yet increasingly face scenarios\nrequiring selective forgetting of training data. Beyond user requests to remove\nspecific interactions due to privacy concerns or preference changes, regulatory\nframeworks mandate recommender systems' ability to eliminate the influence of\ncertain user data from models. This recommendation unlearning challenge\npresents unique difficulties as removing connections within interaction graphs\ncreates ripple effects throughout the model, potentially impacting\nrecommendations for numerous users. Traditional approaches suffer from\nsignificant drawbacks: fragmentation methods damage graph structure and\ndiminish performance, while influence function techniques make assumptions that\nmay not hold in complex GNNs, particularly with self-supervised or random\narchitectures. To address these limitations, we propose a novel model-agnostic\npre-training paradigm UnlearnRec that prepares systems for efficient unlearning\noperations. Our Influence Encoder takes unlearning requests together with\nexisting model parameters and directly produces updated parameters of unlearned\nmodel with little fine-tuning, avoiding complete retraining while preserving\nmodel performance characteristics. Extensive evaluation on public benchmarks\ndemonstrates that our method delivers exceptional unlearning effectiveness\nwhile providing more than 10x speedup compared to retraining approaches. We\nrelease our method implementation at: https://github.com/HKUDS/UnlearnRec.\n", "link": "http://arxiv.org/abs/2505.22649v1", "date": "2025-05-28", "relevancy": 2.4118, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4926}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.483}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pre-training%20for%20Recommendation%20Unlearning&body=Title%3A%20Pre-training%20for%20Recommendation%20Unlearning%0AAuthor%3A%20Guoxuan%20Chen%20and%20Lianghao%20Xia%20and%20Chao%20Huang%0AAbstract%3A%20%20%20Modern%20recommender%20systems%20powered%20by%20Graph%20Neural%20Networks%20%28GNNs%29%20excel%20at%0Amodeling%20complex%20user-item%20interactions%2C%20yet%20increasingly%20face%20scenarios%0Arequiring%20selective%20forgetting%20of%20training%20data.%20Beyond%20user%20requests%20to%20remove%0Aspecific%20interactions%20due%20to%20privacy%20concerns%20or%20preference%20changes%2C%20regulatory%0Aframeworks%20mandate%20recommender%20systems%27%20ability%20to%20eliminate%20the%20influence%20of%0Acertain%20user%20data%20from%20models.%20This%20recommendation%20unlearning%20challenge%0Apresents%20unique%20difficulties%20as%20removing%20connections%20within%20interaction%20graphs%0Acreates%20ripple%20effects%20throughout%20the%20model%2C%20potentially%20impacting%0Arecommendations%20for%20numerous%20users.%20Traditional%20approaches%20suffer%20from%0Asignificant%20drawbacks%3A%20fragmentation%20methods%20damage%20graph%20structure%20and%0Adiminish%20performance%2C%20while%20influence%20function%20techniques%20make%20assumptions%20that%0Amay%20not%20hold%20in%20complex%20GNNs%2C%20particularly%20with%20self-supervised%20or%20random%0Aarchitectures.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20model-agnostic%0Apre-training%20paradigm%20UnlearnRec%20that%20prepares%20systems%20for%20efficient%20unlearning%0Aoperations.%20Our%20Influence%20Encoder%20takes%20unlearning%20requests%20together%20with%0Aexisting%20model%20parameters%20and%20directly%20produces%20updated%20parameters%20of%20unlearned%0Amodel%20with%20little%20fine-tuning%2C%20avoiding%20complete%20retraining%20while%20preserving%0Amodel%20performance%20characteristics.%20Extensive%20evaluation%20on%20public%20benchmarks%0Ademonstrates%20that%20our%20method%20delivers%20exceptional%20unlearning%20effectiveness%0Awhile%20providing%20more%20than%2010x%20speedup%20compared%20to%20retraining%20approaches.%20We%0Arelease%20our%20method%20implementation%20at%3A%20https%3A//github.com/HKUDS/UnlearnRec.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22649v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPre-training%2520for%2520Recommendation%2520Unlearning%26entry.906535625%3DGuoxuan%2520Chen%2520and%2520Lianghao%2520Xia%2520and%2520Chao%2520Huang%26entry.1292438233%3D%2520%2520Modern%2520recommender%2520systems%2520powered%2520by%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520excel%2520at%250Amodeling%2520complex%2520user-item%2520interactions%252C%2520yet%2520increasingly%2520face%2520scenarios%250Arequiring%2520selective%2520forgetting%2520of%2520training%2520data.%2520Beyond%2520user%2520requests%2520to%2520remove%250Aspecific%2520interactions%2520due%2520to%2520privacy%2520concerns%2520or%2520preference%2520changes%252C%2520regulatory%250Aframeworks%2520mandate%2520recommender%2520systems%2527%2520ability%2520to%2520eliminate%2520the%2520influence%2520of%250Acertain%2520user%2520data%2520from%2520models.%2520This%2520recommendation%2520unlearning%2520challenge%250Apresents%2520unique%2520difficulties%2520as%2520removing%2520connections%2520within%2520interaction%2520graphs%250Acreates%2520ripple%2520effects%2520throughout%2520the%2520model%252C%2520potentially%2520impacting%250Arecommendations%2520for%2520numerous%2520users.%2520Traditional%2520approaches%2520suffer%2520from%250Asignificant%2520drawbacks%253A%2520fragmentation%2520methods%2520damage%2520graph%2520structure%2520and%250Adiminish%2520performance%252C%2520while%2520influence%2520function%2520techniques%2520make%2520assumptions%2520that%250Amay%2520not%2520hold%2520in%2520complex%2520GNNs%252C%2520particularly%2520with%2520self-supervised%2520or%2520random%250Aarchitectures.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%2520model-agnostic%250Apre-training%2520paradigm%2520UnlearnRec%2520that%2520prepares%2520systems%2520for%2520efficient%2520unlearning%250Aoperations.%2520Our%2520Influence%2520Encoder%2520takes%2520unlearning%2520requests%2520together%2520with%250Aexisting%2520model%2520parameters%2520and%2520directly%2520produces%2520updated%2520parameters%2520of%2520unlearned%250Amodel%2520with%2520little%2520fine-tuning%252C%2520avoiding%2520complete%2520retraining%2520while%2520preserving%250Amodel%2520performance%2520characteristics.%2520Extensive%2520evaluation%2520on%2520public%2520benchmarks%250Ademonstrates%2520that%2520our%2520method%2520delivers%2520exceptional%2520unlearning%2520effectiveness%250Awhile%2520providing%2520more%2520than%252010x%2520speedup%2520compared%2520to%2520retraining%2520approaches.%2520We%250Arelease%2520our%2520method%2520implementation%2520at%253A%2520https%253A//github.com/HKUDS/UnlearnRec.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22649v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pre-training%20for%20Recommendation%20Unlearning&entry.906535625=Guoxuan%20Chen%20and%20Lianghao%20Xia%20and%20Chao%20Huang&entry.1292438233=%20%20Modern%20recommender%20systems%20powered%20by%20Graph%20Neural%20Networks%20%28GNNs%29%20excel%20at%0Amodeling%20complex%20user-item%20interactions%2C%20yet%20increasingly%20face%20scenarios%0Arequiring%20selective%20forgetting%20of%20training%20data.%20Beyond%20user%20requests%20to%20remove%0Aspecific%20interactions%20due%20to%20privacy%20concerns%20or%20preference%20changes%2C%20regulatory%0Aframeworks%20mandate%20recommender%20systems%27%20ability%20to%20eliminate%20the%20influence%20of%0Acertain%20user%20data%20from%20models.%20This%20recommendation%20unlearning%20challenge%0Apresents%20unique%20difficulties%20as%20removing%20connections%20within%20interaction%20graphs%0Acreates%20ripple%20effects%20throughout%20the%20model%2C%20potentially%20impacting%0Arecommendations%20for%20numerous%20users.%20Traditional%20approaches%20suffer%20from%0Asignificant%20drawbacks%3A%20fragmentation%20methods%20damage%20graph%20structure%20and%0Adiminish%20performance%2C%20while%20influence%20function%20techniques%20make%20assumptions%20that%0Amay%20not%20hold%20in%20complex%20GNNs%2C%20particularly%20with%20self-supervised%20or%20random%0Aarchitectures.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20model-agnostic%0Apre-training%20paradigm%20UnlearnRec%20that%20prepares%20systems%20for%20efficient%20unlearning%0Aoperations.%20Our%20Influence%20Encoder%20takes%20unlearning%20requests%20together%20with%0Aexisting%20model%20parameters%20and%20directly%20produces%20updated%20parameters%20of%20unlearned%0Amodel%20with%20little%20fine-tuning%2C%20avoiding%20complete%20retraining%20while%20preserving%0Amodel%20performance%20characteristics.%20Extensive%20evaluation%20on%20public%20benchmarks%0Ademonstrates%20that%20our%20method%20delivers%20exceptional%20unlearning%20effectiveness%0Awhile%20providing%20more%20than%2010x%20speedup%20compared%20to%20retraining%20approaches.%20We%0Arelease%20our%20method%20implementation%20at%3A%20https%3A//github.com/HKUDS/UnlearnRec.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22649v1&entry.124074799=Read"},
{"title": "TabularQGAN: A Quantum Generative Model for Tabular Data", "author": "Pallavi Bhardwaj and Caitlin Jones and Lasse Dierich and Aleksandar Vu\u010dkovi\u0107", "abstract": "  In this paper, we introduce a novel quantum generative model for synthesizing\ntabular data. Synthetic data is valuable in scenarios where real-world data is\nscarce or private, it can be used to augment or replace existing datasets.\nReal-world enterprise data is predominantly tabular and heterogeneous, often\ncomprising a mixture of categorical and numerical features, making it highly\nrelevant across various industries such as healthcare, finance, and software.\nWe propose a quantum generative adversarial network architecture with flexible\ndata encoding and a novel quantum circuit ansatz to effectively model tabular\ndata. The proposed approach is tested on the MIMIC III healthcare and Adult\nCensus datasets, with extensive benchmarking against leading classical models,\nCTGAN, and CopulaGAN. Experimental results demonstrate that our quantum model\noutperforms classical models by an average of 8.5% with respect to an overall\nsimilarity score from SDMetrics, while using only 0.072% of the parameters of\nthe classical models. Additionally, we evaluate the generalization capabilities\nof the models using two custom-designed metrics that demonstrate the ability of\nthe proposed quantum model to generate useful and novel samples. To our\nknowledge, this is one of the first demonstrations of a successful quantum\ngenerative model for handling tabular data, indicating that this task could be\nwell-suited to quantum computers.\n", "link": "http://arxiv.org/abs/2505.22533v1", "date": "2025-05-28", "relevancy": 2.4068, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4844}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4833}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TabularQGAN%3A%20A%20Quantum%20Generative%20Model%20for%20Tabular%20Data&body=Title%3A%20TabularQGAN%3A%20A%20Quantum%20Generative%20Model%20for%20Tabular%20Data%0AAuthor%3A%20Pallavi%20Bhardwaj%20and%20Caitlin%20Jones%20and%20Lasse%20Dierich%20and%20Aleksandar%20Vu%C4%8Dkovi%C4%87%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20quantum%20generative%20model%20for%20synthesizing%0Atabular%20data.%20Synthetic%20data%20is%20valuable%20in%20scenarios%20where%20real-world%20data%20is%0Ascarce%20or%20private%2C%20it%20can%20be%20used%20to%20augment%20or%20replace%20existing%20datasets.%0AReal-world%20enterprise%20data%20is%20predominantly%20tabular%20and%20heterogeneous%2C%20often%0Acomprising%20a%20mixture%20of%20categorical%20and%20numerical%20features%2C%20making%20it%20highly%0Arelevant%20across%20various%20industries%20such%20as%20healthcare%2C%20finance%2C%20and%20software.%0AWe%20propose%20a%20quantum%20generative%20adversarial%20network%20architecture%20with%20flexible%0Adata%20encoding%20and%20a%20novel%20quantum%20circuit%20ansatz%20to%20effectively%20model%20tabular%0Adata.%20The%20proposed%20approach%20is%20tested%20on%20the%20MIMIC%20III%20healthcare%20and%20Adult%0ACensus%20datasets%2C%20with%20extensive%20benchmarking%20against%20leading%20classical%20models%2C%0ACTGAN%2C%20and%20CopulaGAN.%20Experimental%20results%20demonstrate%20that%20our%20quantum%20model%0Aoutperforms%20classical%20models%20by%20an%20average%20of%208.5%25%20with%20respect%20to%20an%20overall%0Asimilarity%20score%20from%20SDMetrics%2C%20while%20using%20only%200.072%25%20of%20the%20parameters%20of%0Athe%20classical%20models.%20Additionally%2C%20we%20evaluate%20the%20generalization%20capabilities%0Aof%20the%20models%20using%20two%20custom-designed%20metrics%20that%20demonstrate%20the%20ability%20of%0Athe%20proposed%20quantum%20model%20to%20generate%20useful%20and%20novel%20samples.%20To%20our%0Aknowledge%2C%20this%20is%20one%20of%20the%20first%20demonstrations%20of%20a%20successful%20quantum%0Agenerative%20model%20for%20handling%20tabular%20data%2C%20indicating%20that%20this%20task%20could%20be%0Awell-suited%20to%20quantum%20computers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22533v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTabularQGAN%253A%2520A%2520Quantum%2520Generative%2520Model%2520for%2520Tabular%2520Data%26entry.906535625%3DPallavi%2520Bhardwaj%2520and%2520Caitlin%2520Jones%2520and%2520Lasse%2520Dierich%2520and%2520Aleksandar%2520Vu%25C4%258Dkovi%25C4%2587%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520quantum%2520generative%2520model%2520for%2520synthesizing%250Atabular%2520data.%2520Synthetic%2520data%2520is%2520valuable%2520in%2520scenarios%2520where%2520real-world%2520data%2520is%250Ascarce%2520or%2520private%252C%2520it%2520can%2520be%2520used%2520to%2520augment%2520or%2520replace%2520existing%2520datasets.%250AReal-world%2520enterprise%2520data%2520is%2520predominantly%2520tabular%2520and%2520heterogeneous%252C%2520often%250Acomprising%2520a%2520mixture%2520of%2520categorical%2520and%2520numerical%2520features%252C%2520making%2520it%2520highly%250Arelevant%2520across%2520various%2520industries%2520such%2520as%2520healthcare%252C%2520finance%252C%2520and%2520software.%250AWe%2520propose%2520a%2520quantum%2520generative%2520adversarial%2520network%2520architecture%2520with%2520flexible%250Adata%2520encoding%2520and%2520a%2520novel%2520quantum%2520circuit%2520ansatz%2520to%2520effectively%2520model%2520tabular%250Adata.%2520The%2520proposed%2520approach%2520is%2520tested%2520on%2520the%2520MIMIC%2520III%2520healthcare%2520and%2520Adult%250ACensus%2520datasets%252C%2520with%2520extensive%2520benchmarking%2520against%2520leading%2520classical%2520models%252C%250ACTGAN%252C%2520and%2520CopulaGAN.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520quantum%2520model%250Aoutperforms%2520classical%2520models%2520by%2520an%2520average%2520of%25208.5%2525%2520with%2520respect%2520to%2520an%2520overall%250Asimilarity%2520score%2520from%2520SDMetrics%252C%2520while%2520using%2520only%25200.072%2525%2520of%2520the%2520parameters%2520of%250Athe%2520classical%2520models.%2520Additionally%252C%2520we%2520evaluate%2520the%2520generalization%2520capabilities%250Aof%2520the%2520models%2520using%2520two%2520custom-designed%2520metrics%2520that%2520demonstrate%2520the%2520ability%2520of%250Athe%2520proposed%2520quantum%2520model%2520to%2520generate%2520useful%2520and%2520novel%2520samples.%2520To%2520our%250Aknowledge%252C%2520this%2520is%2520one%2520of%2520the%2520first%2520demonstrations%2520of%2520a%2520successful%2520quantum%250Agenerative%2520model%2520for%2520handling%2520tabular%2520data%252C%2520indicating%2520that%2520this%2520task%2520could%2520be%250Awell-suited%2520to%2520quantum%2520computers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22533v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TabularQGAN%3A%20A%20Quantum%20Generative%20Model%20for%20Tabular%20Data&entry.906535625=Pallavi%20Bhardwaj%20and%20Caitlin%20Jones%20and%20Lasse%20Dierich%20and%20Aleksandar%20Vu%C4%8Dkovi%C4%87&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20quantum%20generative%20model%20for%20synthesizing%0Atabular%20data.%20Synthetic%20data%20is%20valuable%20in%20scenarios%20where%20real-world%20data%20is%0Ascarce%20or%20private%2C%20it%20can%20be%20used%20to%20augment%20or%20replace%20existing%20datasets.%0AReal-world%20enterprise%20data%20is%20predominantly%20tabular%20and%20heterogeneous%2C%20often%0Acomprising%20a%20mixture%20of%20categorical%20and%20numerical%20features%2C%20making%20it%20highly%0Arelevant%20across%20various%20industries%20such%20as%20healthcare%2C%20finance%2C%20and%20software.%0AWe%20propose%20a%20quantum%20generative%20adversarial%20network%20architecture%20with%20flexible%0Adata%20encoding%20and%20a%20novel%20quantum%20circuit%20ansatz%20to%20effectively%20model%20tabular%0Adata.%20The%20proposed%20approach%20is%20tested%20on%20the%20MIMIC%20III%20healthcare%20and%20Adult%0ACensus%20datasets%2C%20with%20extensive%20benchmarking%20against%20leading%20classical%20models%2C%0ACTGAN%2C%20and%20CopulaGAN.%20Experimental%20results%20demonstrate%20that%20our%20quantum%20model%0Aoutperforms%20classical%20models%20by%20an%20average%20of%208.5%25%20with%20respect%20to%20an%20overall%0Asimilarity%20score%20from%20SDMetrics%2C%20while%20using%20only%200.072%25%20of%20the%20parameters%20of%0Athe%20classical%20models.%20Additionally%2C%20we%20evaluate%20the%20generalization%20capabilities%0Aof%20the%20models%20using%20two%20custom-designed%20metrics%20that%20demonstrate%20the%20ability%20of%0Athe%20proposed%20quantum%20model%20to%20generate%20useful%20and%20novel%20samples.%20To%20our%0Aknowledge%2C%20this%20is%20one%20of%20the%20first%20demonstrations%20of%20a%20successful%20quantum%0Agenerative%20model%20for%20handling%20tabular%20data%2C%20indicating%20that%20this%20task%20could%20be%0Awell-suited%20to%20quantum%20computers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22533v1&entry.124074799=Read"},
{"title": "RC-AutoCalib: An End-to-End Radar-Camera Automatic Calibration Network", "author": "Van-Tin Luu and Yon-Lin Cai and Vu-Hoang Tran and Wei-Chen Chiu and Yi-Ting Chen and Ching-Chun Huang", "abstract": "  This paper presents a groundbreaking approach - the first online automatic\ngeometric calibration method for radar and camera systems. Given the\nsignificant data sparsity and measurement uncertainty in radar height data,\nachieving automatic calibration during system operation has long been a\nchallenge. To address the sparsity issue, we propose a Dual-Perspective\nrepresentation that gathers features from both frontal and bird's-eye views.\nThe frontal view contains rich but sensitive height information, whereas the\nbird's-eye view provides robust features against height uncertainty. We thereby\npropose a novel Selective Fusion Mechanism to identify and fuse reliable\nfeatures from both perspectives, reducing the effect of height uncertainty.\nMoreover, for each view, we incorporate a Multi-Modal Cross-Attention Mechanism\nto explicitly find location correspondences through cross-modal matching.\nDuring the training phase, we also design a Noise-Resistant Matcher to provide\nbetter supervision and enhance the robustness of the matching mechanism against\nsparsity and height uncertainty. Our experimental results, tested on the\nnuScenes dataset, demonstrate that our method significantly outperforms\nprevious radar-camera auto-calibration methods, as well as existing\nstate-of-the-art LiDAR-camera calibration techniques, establishing a new\nbenchmark for future research. The code is available at\nhttps://github.com/nycu-acm/RC-AutoCalib.\n", "link": "http://arxiv.org/abs/2505.22427v1", "date": "2025-05-28", "relevancy": 2.3792, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6143}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5942}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RC-AutoCalib%3A%20An%20End-to-End%20Radar-Camera%20Automatic%20Calibration%20Network&body=Title%3A%20RC-AutoCalib%3A%20An%20End-to-End%20Radar-Camera%20Automatic%20Calibration%20Network%0AAuthor%3A%20Van-Tin%20Luu%20and%20Yon-Lin%20Cai%20and%20Vu-Hoang%20Tran%20and%20Wei-Chen%20Chiu%20and%20Yi-Ting%20Chen%20and%20Ching-Chun%20Huang%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20groundbreaking%20approach%20-%20the%20first%20online%20automatic%0Ageometric%20calibration%20method%20for%20radar%20and%20camera%20systems.%20Given%20the%0Asignificant%20data%20sparsity%20and%20measurement%20uncertainty%20in%20radar%20height%20data%2C%0Aachieving%20automatic%20calibration%20during%20system%20operation%20has%20long%20been%20a%0Achallenge.%20To%20address%20the%20sparsity%20issue%2C%20we%20propose%20a%20Dual-Perspective%0Arepresentation%20that%20gathers%20features%20from%20both%20frontal%20and%20bird%27s-eye%20views.%0AThe%20frontal%20view%20contains%20rich%20but%20sensitive%20height%20information%2C%20whereas%20the%0Abird%27s-eye%20view%20provides%20robust%20features%20against%20height%20uncertainty.%20We%20thereby%0Apropose%20a%20novel%20Selective%20Fusion%20Mechanism%20to%20identify%20and%20fuse%20reliable%0Afeatures%20from%20both%20perspectives%2C%20reducing%20the%20effect%20of%20height%20uncertainty.%0AMoreover%2C%20for%20each%20view%2C%20we%20incorporate%20a%20Multi-Modal%20Cross-Attention%20Mechanism%0Ato%20explicitly%20find%20location%20correspondences%20through%20cross-modal%20matching.%0ADuring%20the%20training%20phase%2C%20we%20also%20design%20a%20Noise-Resistant%20Matcher%20to%20provide%0Abetter%20supervision%20and%20enhance%20the%20robustness%20of%20the%20matching%20mechanism%20against%0Asparsity%20and%20height%20uncertainty.%20Our%20experimental%20results%2C%20tested%20on%20the%0AnuScenes%20dataset%2C%20demonstrate%20that%20our%20method%20significantly%20outperforms%0Aprevious%20radar-camera%20auto-calibration%20methods%2C%20as%20well%20as%20existing%0Astate-of-the-art%20LiDAR-camera%20calibration%20techniques%2C%20establishing%20a%20new%0Abenchmark%20for%20future%20research.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/nycu-acm/RC-AutoCalib.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22427v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRC-AutoCalib%253A%2520An%2520End-to-End%2520Radar-Camera%2520Automatic%2520Calibration%2520Network%26entry.906535625%3DVan-Tin%2520Luu%2520and%2520Yon-Lin%2520Cai%2520and%2520Vu-Hoang%2520Tran%2520and%2520Wei-Chen%2520Chiu%2520and%2520Yi-Ting%2520Chen%2520and%2520Ching-Chun%2520Huang%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520groundbreaking%2520approach%2520-%2520the%2520first%2520online%2520automatic%250Ageometric%2520calibration%2520method%2520for%2520radar%2520and%2520camera%2520systems.%2520Given%2520the%250Asignificant%2520data%2520sparsity%2520and%2520measurement%2520uncertainty%2520in%2520radar%2520height%2520data%252C%250Aachieving%2520automatic%2520calibration%2520during%2520system%2520operation%2520has%2520long%2520been%2520a%250Achallenge.%2520To%2520address%2520the%2520sparsity%2520issue%252C%2520we%2520propose%2520a%2520Dual-Perspective%250Arepresentation%2520that%2520gathers%2520features%2520from%2520both%2520frontal%2520and%2520bird%2527s-eye%2520views.%250AThe%2520frontal%2520view%2520contains%2520rich%2520but%2520sensitive%2520height%2520information%252C%2520whereas%2520the%250Abird%2527s-eye%2520view%2520provides%2520robust%2520features%2520against%2520height%2520uncertainty.%2520We%2520thereby%250Apropose%2520a%2520novel%2520Selective%2520Fusion%2520Mechanism%2520to%2520identify%2520and%2520fuse%2520reliable%250Afeatures%2520from%2520both%2520perspectives%252C%2520reducing%2520the%2520effect%2520of%2520height%2520uncertainty.%250AMoreover%252C%2520for%2520each%2520view%252C%2520we%2520incorporate%2520a%2520Multi-Modal%2520Cross-Attention%2520Mechanism%250Ato%2520explicitly%2520find%2520location%2520correspondences%2520through%2520cross-modal%2520matching.%250ADuring%2520the%2520training%2520phase%252C%2520we%2520also%2520design%2520a%2520Noise-Resistant%2520Matcher%2520to%2520provide%250Abetter%2520supervision%2520and%2520enhance%2520the%2520robustness%2520of%2520the%2520matching%2520mechanism%2520against%250Asparsity%2520and%2520height%2520uncertainty.%2520Our%2520experimental%2520results%252C%2520tested%2520on%2520the%250AnuScenes%2520dataset%252C%2520demonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%250Aprevious%2520radar-camera%2520auto-calibration%2520methods%252C%2520as%2520well%2520as%2520existing%250Astate-of-the-art%2520LiDAR-camera%2520calibration%2520techniques%252C%2520establishing%2520a%2520new%250Abenchmark%2520for%2520future%2520research.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/nycu-acm/RC-AutoCalib.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22427v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RC-AutoCalib%3A%20An%20End-to-End%20Radar-Camera%20Automatic%20Calibration%20Network&entry.906535625=Van-Tin%20Luu%20and%20Yon-Lin%20Cai%20and%20Vu-Hoang%20Tran%20and%20Wei-Chen%20Chiu%20and%20Yi-Ting%20Chen%20and%20Ching-Chun%20Huang&entry.1292438233=%20%20This%20paper%20presents%20a%20groundbreaking%20approach%20-%20the%20first%20online%20automatic%0Ageometric%20calibration%20method%20for%20radar%20and%20camera%20systems.%20Given%20the%0Asignificant%20data%20sparsity%20and%20measurement%20uncertainty%20in%20radar%20height%20data%2C%0Aachieving%20automatic%20calibration%20during%20system%20operation%20has%20long%20been%20a%0Achallenge.%20To%20address%20the%20sparsity%20issue%2C%20we%20propose%20a%20Dual-Perspective%0Arepresentation%20that%20gathers%20features%20from%20both%20frontal%20and%20bird%27s-eye%20views.%0AThe%20frontal%20view%20contains%20rich%20but%20sensitive%20height%20information%2C%20whereas%20the%0Abird%27s-eye%20view%20provides%20robust%20features%20against%20height%20uncertainty.%20We%20thereby%0Apropose%20a%20novel%20Selective%20Fusion%20Mechanism%20to%20identify%20and%20fuse%20reliable%0Afeatures%20from%20both%20perspectives%2C%20reducing%20the%20effect%20of%20height%20uncertainty.%0AMoreover%2C%20for%20each%20view%2C%20we%20incorporate%20a%20Multi-Modal%20Cross-Attention%20Mechanism%0Ato%20explicitly%20find%20location%20correspondences%20through%20cross-modal%20matching.%0ADuring%20the%20training%20phase%2C%20we%20also%20design%20a%20Noise-Resistant%20Matcher%20to%20provide%0Abetter%20supervision%20and%20enhance%20the%20robustness%20of%20the%20matching%20mechanism%20against%0Asparsity%20and%20height%20uncertainty.%20Our%20experimental%20results%2C%20tested%20on%20the%0AnuScenes%20dataset%2C%20demonstrate%20that%20our%20method%20significantly%20outperforms%0Aprevious%20radar-camera%20auto-calibration%20methods%2C%20as%20well%20as%20existing%0Astate-of-the-art%20LiDAR-camera%20calibration%20techniques%2C%20establishing%20a%20new%0Abenchmark%20for%20future%20research.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/nycu-acm/RC-AutoCalib.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22427v1&entry.124074799=Read"},
{"title": "Evaluating Supervised Learning Models for Fraud Detection: A Comparative\n  Study of Classical and Deep Architectures on Imbalanced Transaction Data", "author": "Chao Wang and Chuanhao Nie and Yunbo Liu", "abstract": "  Fraud detection remains a critical task in high-stakes domains such as\nfinance and e-commerce, where undetected fraudulent transactions can lead to\nsignificant economic losses. In this study, we systematically compare the\nperformance of four supervised learning models - Logistic Regression, Random\nForest, Light Gradient Boosting Machine (LightGBM), and a Gated Recurrent Unit\n(GRU) network - on a large-scale, highly imbalanced online transaction dataset.\nWhile ensemble methods such as Random Forest and LightGBM demonstrated superior\nperformance in both overall and class-specific metrics, Logistic Regression\noffered a reliable and interpretable baseline. The GRU model showed strong\nrecall for the minority fraud class, though at the cost of precision,\nhighlighting a trade-off relevant for real-world deployment. Our evaluation\nemphasizes not only weighted averages but also per-class precision, recall, and\nF1-scores, providing a nuanced view of each model's effectiveness in detecting\nrare but consequential fraudulent activity. The findings underscore the\nimportance of choosing models based on the specific risk tolerance and\noperational needs of fraud detection systems.\n", "link": "http://arxiv.org/abs/2505.22521v1", "date": "2025-05-28", "relevancy": 2.366, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.477}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4743}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Supervised%20Learning%20Models%20for%20Fraud%20Detection%3A%20A%20Comparative%0A%20%20Study%20of%20Classical%20and%20Deep%20Architectures%20on%20Imbalanced%20Transaction%20Data&body=Title%3A%20Evaluating%20Supervised%20Learning%20Models%20for%20Fraud%20Detection%3A%20A%20Comparative%0A%20%20Study%20of%20Classical%20and%20Deep%20Architectures%20on%20Imbalanced%20Transaction%20Data%0AAuthor%3A%20Chao%20Wang%20and%20Chuanhao%20Nie%20and%20Yunbo%20Liu%0AAbstract%3A%20%20%20Fraud%20detection%20remains%20a%20critical%20task%20in%20high-stakes%20domains%20such%20as%0Afinance%20and%20e-commerce%2C%20where%20undetected%20fraudulent%20transactions%20can%20lead%20to%0Asignificant%20economic%20losses.%20In%20this%20study%2C%20we%20systematically%20compare%20the%0Aperformance%20of%20four%20supervised%20learning%20models%20-%20Logistic%20Regression%2C%20Random%0AForest%2C%20Light%20Gradient%20Boosting%20Machine%20%28LightGBM%29%2C%20and%20a%20Gated%20Recurrent%20Unit%0A%28GRU%29%20network%20-%20on%20a%20large-scale%2C%20highly%20imbalanced%20online%20transaction%20dataset.%0AWhile%20ensemble%20methods%20such%20as%20Random%20Forest%20and%20LightGBM%20demonstrated%20superior%0Aperformance%20in%20both%20overall%20and%20class-specific%20metrics%2C%20Logistic%20Regression%0Aoffered%20a%20reliable%20and%20interpretable%20baseline.%20The%20GRU%20model%20showed%20strong%0Arecall%20for%20the%20minority%20fraud%20class%2C%20though%20at%20the%20cost%20of%20precision%2C%0Ahighlighting%20a%20trade-off%20relevant%20for%20real-world%20deployment.%20Our%20evaluation%0Aemphasizes%20not%20only%20weighted%20averages%20but%20also%20per-class%20precision%2C%20recall%2C%20and%0AF1-scores%2C%20providing%20a%20nuanced%20view%20of%20each%20model%27s%20effectiveness%20in%20detecting%0Arare%20but%20consequential%20fraudulent%20activity.%20The%20findings%20underscore%20the%0Aimportance%20of%20choosing%20models%20based%20on%20the%20specific%20risk%20tolerance%20and%0Aoperational%20needs%20of%20fraud%20detection%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22521v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Supervised%2520Learning%2520Models%2520for%2520Fraud%2520Detection%253A%2520A%2520Comparative%250A%2520%2520Study%2520of%2520Classical%2520and%2520Deep%2520Architectures%2520on%2520Imbalanced%2520Transaction%2520Data%26entry.906535625%3DChao%2520Wang%2520and%2520Chuanhao%2520Nie%2520and%2520Yunbo%2520Liu%26entry.1292438233%3D%2520%2520Fraud%2520detection%2520remains%2520a%2520critical%2520task%2520in%2520high-stakes%2520domains%2520such%2520as%250Afinance%2520and%2520e-commerce%252C%2520where%2520undetected%2520fraudulent%2520transactions%2520can%2520lead%2520to%250Asignificant%2520economic%2520losses.%2520In%2520this%2520study%252C%2520we%2520systematically%2520compare%2520the%250Aperformance%2520of%2520four%2520supervised%2520learning%2520models%2520-%2520Logistic%2520Regression%252C%2520Random%250AForest%252C%2520Light%2520Gradient%2520Boosting%2520Machine%2520%2528LightGBM%2529%252C%2520and%2520a%2520Gated%2520Recurrent%2520Unit%250A%2528GRU%2529%2520network%2520-%2520on%2520a%2520large-scale%252C%2520highly%2520imbalanced%2520online%2520transaction%2520dataset.%250AWhile%2520ensemble%2520methods%2520such%2520as%2520Random%2520Forest%2520and%2520LightGBM%2520demonstrated%2520superior%250Aperformance%2520in%2520both%2520overall%2520and%2520class-specific%2520metrics%252C%2520Logistic%2520Regression%250Aoffered%2520a%2520reliable%2520and%2520interpretable%2520baseline.%2520The%2520GRU%2520model%2520showed%2520strong%250Arecall%2520for%2520the%2520minority%2520fraud%2520class%252C%2520though%2520at%2520the%2520cost%2520of%2520precision%252C%250Ahighlighting%2520a%2520trade-off%2520relevant%2520for%2520real-world%2520deployment.%2520Our%2520evaluation%250Aemphasizes%2520not%2520only%2520weighted%2520averages%2520but%2520also%2520per-class%2520precision%252C%2520recall%252C%2520and%250AF1-scores%252C%2520providing%2520a%2520nuanced%2520view%2520of%2520each%2520model%2527s%2520effectiveness%2520in%2520detecting%250Arare%2520but%2520consequential%2520fraudulent%2520activity.%2520The%2520findings%2520underscore%2520the%250Aimportance%2520of%2520choosing%2520models%2520based%2520on%2520the%2520specific%2520risk%2520tolerance%2520and%250Aoperational%2520needs%2520of%2520fraud%2520detection%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22521v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Supervised%20Learning%20Models%20for%20Fraud%20Detection%3A%20A%20Comparative%0A%20%20Study%20of%20Classical%20and%20Deep%20Architectures%20on%20Imbalanced%20Transaction%20Data&entry.906535625=Chao%20Wang%20and%20Chuanhao%20Nie%20and%20Yunbo%20Liu&entry.1292438233=%20%20Fraud%20detection%20remains%20a%20critical%20task%20in%20high-stakes%20domains%20such%20as%0Afinance%20and%20e-commerce%2C%20where%20undetected%20fraudulent%20transactions%20can%20lead%20to%0Asignificant%20economic%20losses.%20In%20this%20study%2C%20we%20systematically%20compare%20the%0Aperformance%20of%20four%20supervised%20learning%20models%20-%20Logistic%20Regression%2C%20Random%0AForest%2C%20Light%20Gradient%20Boosting%20Machine%20%28LightGBM%29%2C%20and%20a%20Gated%20Recurrent%20Unit%0A%28GRU%29%20network%20-%20on%20a%20large-scale%2C%20highly%20imbalanced%20online%20transaction%20dataset.%0AWhile%20ensemble%20methods%20such%20as%20Random%20Forest%20and%20LightGBM%20demonstrated%20superior%0Aperformance%20in%20both%20overall%20and%20class-specific%20metrics%2C%20Logistic%20Regression%0Aoffered%20a%20reliable%20and%20interpretable%20baseline.%20The%20GRU%20model%20showed%20strong%0Arecall%20for%20the%20minority%20fraud%20class%2C%20though%20at%20the%20cost%20of%20precision%2C%0Ahighlighting%20a%20trade-off%20relevant%20for%20real-world%20deployment.%20Our%20evaluation%0Aemphasizes%20not%20only%20weighted%20averages%20but%20also%20per-class%20precision%2C%20recall%2C%20and%0AF1-scores%2C%20providing%20a%20nuanced%20view%20of%20each%20model%27s%20effectiveness%20in%20detecting%0Arare%20but%20consequential%20fraudulent%20activity.%20The%20findings%20underscore%20the%0Aimportance%20of%20choosing%20models%20based%20on%20the%20specific%20risk%20tolerance%20and%0Aoperational%20needs%20of%20fraud%20detection%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22521v1&entry.124074799=Read"},
{"title": "360-LLaMA-Factory: Plug & Play Sequence Parallelism for Long\n  Post-Training", "author": "Haosheng Zou and Xiaowei Lv and Shousheng Jia and Xiangzheng Zhang", "abstract": "  Adding sequence parallelism into LLaMA-Factory, we open-sourced\n360-LLaMA-Factory at https://github.com/Qihoo360/360-LLaMA-Factory.\n360-LLaMA-Factory has received wide recognition and used in models such as\nLight-R1 arXiv:2503.10460, TinyR1 arXiv:2503.04872, Kaggle AIMO math models and\nalso in large companies' training frameworks. This technical report delves\ndeeper into the different sequence parallel modes behind 360-LLaMA-Factory and\ndiscusses our implementation insights.\n", "link": "http://arxiv.org/abs/2505.22296v1", "date": "2025-05-28", "relevancy": 2.3608, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4735}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4735}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20360-LLaMA-Factory%3A%20Plug%20%26%20Play%20Sequence%20Parallelism%20for%20Long%0A%20%20Post-Training&body=Title%3A%20360-LLaMA-Factory%3A%20Plug%20%26%20Play%20Sequence%20Parallelism%20for%20Long%0A%20%20Post-Training%0AAuthor%3A%20Haosheng%20Zou%20and%20Xiaowei%20Lv%20and%20Shousheng%20Jia%20and%20Xiangzheng%20Zhang%0AAbstract%3A%20%20%20Adding%20sequence%20parallelism%20into%20LLaMA-Factory%2C%20we%20open-sourced%0A360-LLaMA-Factory%20at%20https%3A//github.com/Qihoo360/360-LLaMA-Factory.%0A360-LLaMA-Factory%20has%20received%20wide%20recognition%20and%20used%20in%20models%20such%20as%0ALight-R1%20arXiv%3A2503.10460%2C%20TinyR1%20arXiv%3A2503.04872%2C%20Kaggle%20AIMO%20math%20models%20and%0Aalso%20in%20large%20companies%27%20training%20frameworks.%20This%20technical%20report%20delves%0Adeeper%20into%20the%20different%20sequence%20parallel%20modes%20behind%20360-LLaMA-Factory%20and%0Adiscusses%20our%20implementation%20insights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22296v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D360-LLaMA-Factory%253A%2520Plug%2520%2526%2520Play%2520Sequence%2520Parallelism%2520for%2520Long%250A%2520%2520Post-Training%26entry.906535625%3DHaosheng%2520Zou%2520and%2520Xiaowei%2520Lv%2520and%2520Shousheng%2520Jia%2520and%2520Xiangzheng%2520Zhang%26entry.1292438233%3D%2520%2520Adding%2520sequence%2520parallelism%2520into%2520LLaMA-Factory%252C%2520we%2520open-sourced%250A360-LLaMA-Factory%2520at%2520https%253A//github.com/Qihoo360/360-LLaMA-Factory.%250A360-LLaMA-Factory%2520has%2520received%2520wide%2520recognition%2520and%2520used%2520in%2520models%2520such%2520as%250ALight-R1%2520arXiv%253A2503.10460%252C%2520TinyR1%2520arXiv%253A2503.04872%252C%2520Kaggle%2520AIMO%2520math%2520models%2520and%250Aalso%2520in%2520large%2520companies%2527%2520training%2520frameworks.%2520This%2520technical%2520report%2520delves%250Adeeper%2520into%2520the%2520different%2520sequence%2520parallel%2520modes%2520behind%2520360-LLaMA-Factory%2520and%250Adiscusses%2520our%2520implementation%2520insights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22296v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=360-LLaMA-Factory%3A%20Plug%20%26%20Play%20Sequence%20Parallelism%20for%20Long%0A%20%20Post-Training&entry.906535625=Haosheng%20Zou%20and%20Xiaowei%20Lv%20and%20Shousheng%20Jia%20and%20Xiangzheng%20Zhang&entry.1292438233=%20%20Adding%20sequence%20parallelism%20into%20LLaMA-Factory%2C%20we%20open-sourced%0A360-LLaMA-Factory%20at%20https%3A//github.com/Qihoo360/360-LLaMA-Factory.%0A360-LLaMA-Factory%20has%20received%20wide%20recognition%20and%20used%20in%20models%20such%20as%0ALight-R1%20arXiv%3A2503.10460%2C%20TinyR1%20arXiv%3A2503.04872%2C%20Kaggle%20AIMO%20math%20models%20and%0Aalso%20in%20large%20companies%27%20training%20frameworks.%20This%20technical%20report%20delves%0Adeeper%20into%20the%20different%20sequence%20parallel%20modes%20behind%20360-LLaMA-Factory%20and%0Adiscusses%20our%20implementation%20insights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22296v1&entry.124074799=Read"},
{"title": "Diss-l-ECT: Dissecting Graph Data with Local Euler Characteristic\n  Transforms", "author": "Julius von Rohrscheidt and Bastian Rieck", "abstract": "  The Euler Characteristic Transform (ECT) is an efficiently-computable\ngeometrical-topological invariant that characterizes the global shape of data.\nIn this paper, we introduce the Local Euler Characteristic Transform\n($\\ell$-ECT), a novel extension of the ECT particularly designed to enhance\nexpressivity and interpretability in graph representation learning. Unlike\ntraditional Graph Neural Networks (GNNs), which may lose critical local details\nthrough aggregation, the $\\ell$-ECT provides a lossless representation of local\nneighborhoods. This approach addresses key limitations in GNNs by preserving\nnuanced local structures while maintaining global interpretability. Moreover,\nwe construct a rotation-invariant metric based on $\\ell$-ECTs for spatial\nalignment of data spaces. Our method exhibits superior performance compared to\nstandard GNNs on a variety of node-classification tasks, while also offering\ntheoretical guarantees that demonstrate its effectiveness.\n", "link": "http://arxiv.org/abs/2410.02622v2", "date": "2025-05-28", "relevancy": 2.3517, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5027}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4686}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diss-l-ECT%3A%20Dissecting%20Graph%20Data%20with%20Local%20Euler%20Characteristic%0A%20%20Transforms&body=Title%3A%20Diss-l-ECT%3A%20Dissecting%20Graph%20Data%20with%20Local%20Euler%20Characteristic%0A%20%20Transforms%0AAuthor%3A%20Julius%20von%20Rohrscheidt%20and%20Bastian%20Rieck%0AAbstract%3A%20%20%20The%20Euler%20Characteristic%20Transform%20%28ECT%29%20is%20an%20efficiently-computable%0Ageometrical-topological%20invariant%20that%20characterizes%20the%20global%20shape%20of%20data.%0AIn%20this%20paper%2C%20we%20introduce%20the%20Local%20Euler%20Characteristic%20Transform%0A%28%24%5Cell%24-ECT%29%2C%20a%20novel%20extension%20of%20the%20ECT%20particularly%20designed%20to%20enhance%0Aexpressivity%20and%20interpretability%20in%20graph%20representation%20learning.%20Unlike%0Atraditional%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20which%20may%20lose%20critical%20local%20details%0Athrough%20aggregation%2C%20the%20%24%5Cell%24-ECT%20provides%20a%20lossless%20representation%20of%20local%0Aneighborhoods.%20This%20approach%20addresses%20key%20limitations%20in%20GNNs%20by%20preserving%0Anuanced%20local%20structures%20while%20maintaining%20global%20interpretability.%20Moreover%2C%0Awe%20construct%20a%20rotation-invariant%20metric%20based%20on%20%24%5Cell%24-ECTs%20for%20spatial%0Aalignment%20of%20data%20spaces.%20Our%20method%20exhibits%20superior%20performance%20compared%20to%0Astandard%20GNNs%20on%20a%20variety%20of%20node-classification%20tasks%2C%20while%20also%20offering%0Atheoretical%20guarantees%20that%20demonstrate%20its%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02622v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiss-l-ECT%253A%2520Dissecting%2520Graph%2520Data%2520with%2520Local%2520Euler%2520Characteristic%250A%2520%2520Transforms%26entry.906535625%3DJulius%2520von%2520Rohrscheidt%2520and%2520Bastian%2520Rieck%26entry.1292438233%3D%2520%2520The%2520Euler%2520Characteristic%2520Transform%2520%2528ECT%2529%2520is%2520an%2520efficiently-computable%250Ageometrical-topological%2520invariant%2520that%2520characterizes%2520the%2520global%2520shape%2520of%2520data.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520the%2520Local%2520Euler%2520Characteristic%2520Transform%250A%2528%2524%255Cell%2524-ECT%2529%252C%2520a%2520novel%2520extension%2520of%2520the%2520ECT%2520particularly%2520designed%2520to%2520enhance%250Aexpressivity%2520and%2520interpretability%2520in%2520graph%2520representation%2520learning.%2520Unlike%250Atraditional%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%252C%2520which%2520may%2520lose%2520critical%2520local%2520details%250Athrough%2520aggregation%252C%2520the%2520%2524%255Cell%2524-ECT%2520provides%2520a%2520lossless%2520representation%2520of%2520local%250Aneighborhoods.%2520This%2520approach%2520addresses%2520key%2520limitations%2520in%2520GNNs%2520by%2520preserving%250Anuanced%2520local%2520structures%2520while%2520maintaining%2520global%2520interpretability.%2520Moreover%252C%250Awe%2520construct%2520a%2520rotation-invariant%2520metric%2520based%2520on%2520%2524%255Cell%2524-ECTs%2520for%2520spatial%250Aalignment%2520of%2520data%2520spaces.%2520Our%2520method%2520exhibits%2520superior%2520performance%2520compared%2520to%250Astandard%2520GNNs%2520on%2520a%2520variety%2520of%2520node-classification%2520tasks%252C%2520while%2520also%2520offering%250Atheoretical%2520guarantees%2520that%2520demonstrate%2520its%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02622v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diss-l-ECT%3A%20Dissecting%20Graph%20Data%20with%20Local%20Euler%20Characteristic%0A%20%20Transforms&entry.906535625=Julius%20von%20Rohrscheidt%20and%20Bastian%20Rieck&entry.1292438233=%20%20The%20Euler%20Characteristic%20Transform%20%28ECT%29%20is%20an%20efficiently-computable%0Ageometrical-topological%20invariant%20that%20characterizes%20the%20global%20shape%20of%20data.%0AIn%20this%20paper%2C%20we%20introduce%20the%20Local%20Euler%20Characteristic%20Transform%0A%28%24%5Cell%24-ECT%29%2C%20a%20novel%20extension%20of%20the%20ECT%20particularly%20designed%20to%20enhance%0Aexpressivity%20and%20interpretability%20in%20graph%20representation%20learning.%20Unlike%0Atraditional%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20which%20may%20lose%20critical%20local%20details%0Athrough%20aggregation%2C%20the%20%24%5Cell%24-ECT%20provides%20a%20lossless%20representation%20of%20local%0Aneighborhoods.%20This%20approach%20addresses%20key%20limitations%20in%20GNNs%20by%20preserving%0Anuanced%20local%20structures%20while%20maintaining%20global%20interpretability.%20Moreover%2C%0Awe%20construct%20a%20rotation-invariant%20metric%20based%20on%20%24%5Cell%24-ECTs%20for%20spatial%0Aalignment%20of%20data%20spaces.%20Our%20method%20exhibits%20superior%20performance%20compared%20to%0Astandard%20GNNs%20on%20a%20variety%20of%20node-classification%20tasks%2C%20while%20also%20offering%0Atheoretical%20guarantees%20that%20demonstrate%20its%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02622v2&entry.124074799=Read"},
{"title": "Universal Domain Adaptation for Semantic Segmentation", "author": "Seun-An Choe and Keon-Hee Park and Jinwoo Choi and Gyeong-Moon Park", "abstract": "  Unsupervised domain adaptation for semantic segmentation (UDA-SS) aims to\ntransfer knowledge from labeled source data to unlabeled target data. However,\ntraditional UDA-SS methods assume that category settings between source and\ntarget domains are known, which is unrealistic in real-world scenarios. This\nleads to performance degradation if private classes exist. To address this\nlimitation, we propose Universal Domain Adaptation for Semantic Segmentation\n(UniDA-SS), achieving robust adaptation even without prior knowledge of\ncategory settings. We define the problem in the UniDA-SS scenario as low\nconfidence scores of common classes in the target domain, which leads to\nconfusion with private classes. To solve this problem, we propose UniMAP:\nUniDA-SS with Image Matching and Prototype-based Distinction, a novel framework\ncomposed of two key components. First, Domain-Specific Prototype-based\nDistinction (DSPD) divides each class into two domain-specific prototypes,\nenabling finer separation of domain-specific features and enhancing the\nidentification of common classes across domains. Second, Target-based Image\nMatching (TIM) selects a source image containing the most common-class pixels\nbased on the target pseudo-label and pairs it in a batch to promote effective\nlearning of common classes. We also introduce a new UniDA-SS benchmark and\ndemonstrate through various experiments that UniMAP significantly outperforms\nbaselines. The code is available at\n\\href{https://github.com/KU-VGI/UniMAP}{this https URL}.\n", "link": "http://arxiv.org/abs/2505.22458v1", "date": "2025-05-28", "relevancy": 2.3441, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5902}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5831}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Universal%20Domain%20Adaptation%20for%20Semantic%20Segmentation&body=Title%3A%20Universal%20Domain%20Adaptation%20for%20Semantic%20Segmentation%0AAuthor%3A%20Seun-An%20Choe%20and%20Keon-Hee%20Park%20and%20Jinwoo%20Choi%20and%20Gyeong-Moon%20Park%0AAbstract%3A%20%20%20Unsupervised%20domain%20adaptation%20for%20semantic%20segmentation%20%28UDA-SS%29%20aims%20to%0Atransfer%20knowledge%20from%20labeled%20source%20data%20to%20unlabeled%20target%20data.%20However%2C%0Atraditional%20UDA-SS%20methods%20assume%20that%20category%20settings%20between%20source%20and%0Atarget%20domains%20are%20known%2C%20which%20is%20unrealistic%20in%20real-world%20scenarios.%20This%0Aleads%20to%20performance%20degradation%20if%20private%20classes%20exist.%20To%20address%20this%0Alimitation%2C%20we%20propose%20Universal%20Domain%20Adaptation%20for%20Semantic%20Segmentation%0A%28UniDA-SS%29%2C%20achieving%20robust%20adaptation%20even%20without%20prior%20knowledge%20of%0Acategory%20settings.%20We%20define%20the%20problem%20in%20the%20UniDA-SS%20scenario%20as%20low%0Aconfidence%20scores%20of%20common%20classes%20in%20the%20target%20domain%2C%20which%20leads%20to%0Aconfusion%20with%20private%20classes.%20To%20solve%20this%20problem%2C%20we%20propose%20UniMAP%3A%0AUniDA-SS%20with%20Image%20Matching%20and%20Prototype-based%20Distinction%2C%20a%20novel%20framework%0Acomposed%20of%20two%20key%20components.%20First%2C%20Domain-Specific%20Prototype-based%0ADistinction%20%28DSPD%29%20divides%20each%20class%20into%20two%20domain-specific%20prototypes%2C%0Aenabling%20finer%20separation%20of%20domain-specific%20features%20and%20enhancing%20the%0Aidentification%20of%20common%20classes%20across%20domains.%20Second%2C%20Target-based%20Image%0AMatching%20%28TIM%29%20selects%20a%20source%20image%20containing%20the%20most%20common-class%20pixels%0Abased%20on%20the%20target%20pseudo-label%20and%20pairs%20it%20in%20a%20batch%20to%20promote%20effective%0Alearning%20of%20common%20classes.%20We%20also%20introduce%20a%20new%20UniDA-SS%20benchmark%20and%0Ademonstrate%20through%20various%20experiments%20that%20UniMAP%20significantly%20outperforms%0Abaselines.%20The%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/KU-VGI/UniMAP%7D%7Bthis%20https%20URL%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22458v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniversal%2520Domain%2520Adaptation%2520for%2520Semantic%2520Segmentation%26entry.906535625%3DSeun-An%2520Choe%2520and%2520Keon-Hee%2520Park%2520and%2520Jinwoo%2520Choi%2520and%2520Gyeong-Moon%2520Park%26entry.1292438233%3D%2520%2520Unsupervised%2520domain%2520adaptation%2520for%2520semantic%2520segmentation%2520%2528UDA-SS%2529%2520aims%2520to%250Atransfer%2520knowledge%2520from%2520labeled%2520source%2520data%2520to%2520unlabeled%2520target%2520data.%2520However%252C%250Atraditional%2520UDA-SS%2520methods%2520assume%2520that%2520category%2520settings%2520between%2520source%2520and%250Atarget%2520domains%2520are%2520known%252C%2520which%2520is%2520unrealistic%2520in%2520real-world%2520scenarios.%2520This%250Aleads%2520to%2520performance%2520degradation%2520if%2520private%2520classes%2520exist.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520propose%2520Universal%2520Domain%2520Adaptation%2520for%2520Semantic%2520Segmentation%250A%2528UniDA-SS%2529%252C%2520achieving%2520robust%2520adaptation%2520even%2520without%2520prior%2520knowledge%2520of%250Acategory%2520settings.%2520We%2520define%2520the%2520problem%2520in%2520the%2520UniDA-SS%2520scenario%2520as%2520low%250Aconfidence%2520scores%2520of%2520common%2520classes%2520in%2520the%2520target%2520domain%252C%2520which%2520leads%2520to%250Aconfusion%2520with%2520private%2520classes.%2520To%2520solve%2520this%2520problem%252C%2520we%2520propose%2520UniMAP%253A%250AUniDA-SS%2520with%2520Image%2520Matching%2520and%2520Prototype-based%2520Distinction%252C%2520a%2520novel%2520framework%250Acomposed%2520of%2520two%2520key%2520components.%2520First%252C%2520Domain-Specific%2520Prototype-based%250ADistinction%2520%2528DSPD%2529%2520divides%2520each%2520class%2520into%2520two%2520domain-specific%2520prototypes%252C%250Aenabling%2520finer%2520separation%2520of%2520domain-specific%2520features%2520and%2520enhancing%2520the%250Aidentification%2520of%2520common%2520classes%2520across%2520domains.%2520Second%252C%2520Target-based%2520Image%250AMatching%2520%2528TIM%2529%2520selects%2520a%2520source%2520image%2520containing%2520the%2520most%2520common-class%2520pixels%250Abased%2520on%2520the%2520target%2520pseudo-label%2520and%2520pairs%2520it%2520in%2520a%2520batch%2520to%2520promote%2520effective%250Alearning%2520of%2520common%2520classes.%2520We%2520also%2520introduce%2520a%2520new%2520UniDA-SS%2520benchmark%2520and%250Ademonstrate%2520through%2520various%2520experiments%2520that%2520UniMAP%2520significantly%2520outperforms%250Abaselines.%2520The%2520code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/KU-VGI/UniMAP%257D%257Bthis%2520https%2520URL%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22458v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universal%20Domain%20Adaptation%20for%20Semantic%20Segmentation&entry.906535625=Seun-An%20Choe%20and%20Keon-Hee%20Park%20and%20Jinwoo%20Choi%20and%20Gyeong-Moon%20Park&entry.1292438233=%20%20Unsupervised%20domain%20adaptation%20for%20semantic%20segmentation%20%28UDA-SS%29%20aims%20to%0Atransfer%20knowledge%20from%20labeled%20source%20data%20to%20unlabeled%20target%20data.%20However%2C%0Atraditional%20UDA-SS%20methods%20assume%20that%20category%20settings%20between%20source%20and%0Atarget%20domains%20are%20known%2C%20which%20is%20unrealistic%20in%20real-world%20scenarios.%20This%0Aleads%20to%20performance%20degradation%20if%20private%20classes%20exist.%20To%20address%20this%0Alimitation%2C%20we%20propose%20Universal%20Domain%20Adaptation%20for%20Semantic%20Segmentation%0A%28UniDA-SS%29%2C%20achieving%20robust%20adaptation%20even%20without%20prior%20knowledge%20of%0Acategory%20settings.%20We%20define%20the%20problem%20in%20the%20UniDA-SS%20scenario%20as%20low%0Aconfidence%20scores%20of%20common%20classes%20in%20the%20target%20domain%2C%20which%20leads%20to%0Aconfusion%20with%20private%20classes.%20To%20solve%20this%20problem%2C%20we%20propose%20UniMAP%3A%0AUniDA-SS%20with%20Image%20Matching%20and%20Prototype-based%20Distinction%2C%20a%20novel%20framework%0Acomposed%20of%20two%20key%20components.%20First%2C%20Domain-Specific%20Prototype-based%0ADistinction%20%28DSPD%29%20divides%20each%20class%20into%20two%20domain-specific%20prototypes%2C%0Aenabling%20finer%20separation%20of%20domain-specific%20features%20and%20enhancing%20the%0Aidentification%20of%20common%20classes%20across%20domains.%20Second%2C%20Target-based%20Image%0AMatching%20%28TIM%29%20selects%20a%20source%20image%20containing%20the%20most%20common-class%20pixels%0Abased%20on%20the%20target%20pseudo-label%20and%20pairs%20it%20in%20a%20batch%20to%20promote%20effective%0Alearning%20of%20common%20classes.%20We%20also%20introduce%20a%20new%20UniDA-SS%20benchmark%20and%0Ademonstrate%20through%20various%20experiments%20that%20UniMAP%20significantly%20outperforms%0Abaselines.%20The%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/KU-VGI/UniMAP%7D%7Bthis%20https%20URL%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22458v1&entry.124074799=Read"},
{"title": "Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation", "author": "Zhe Kong and Feng Gao and Yong Zhang and Zhuoliang Kang and Xiaoming Wei and Xunliang Cai and Guanying Chen and Wenhan Luo", "abstract": "  Audio-driven human animation methods, such as talking head and talking body\ngeneration, have made remarkable progress in generating synchronized facial\nmovements and appealing visual quality videos. However, existing methods\nprimarily focus on single human animation and struggle with multi-stream audio\ninputs, facing incorrect binding problems between audio and persons.\nAdditionally, they exhibit limitations in instruction-following capabilities.\nTo solve this problem, in this paper, we propose a novel task: Multi-Person\nConversational Video Generation, and introduce a new framework, MultiTalk, to\naddress the challenges during multi-person generation. Specifically, for audio\ninjection, we investigate several schemes and propose the Label Rotary Position\nEmbedding (L-RoPE) method to resolve the audio and person binding problem.\nFurthermore, during training, we observe that partial parameter training and\nmulti-task training are crucial for preserving the instruction-following\nability of the base model. MultiTalk achieves superior performance compared to\nother methods on several datasets, including talking head, talking body, and\nmulti-person datasets, demonstrating the powerful generation capabilities of\nour approach.\n", "link": "http://arxiv.org/abs/2505.22647v1", "date": "2025-05-28", "relevancy": 2.344, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6002}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5763}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Let%20Them%20Talk%3A%20Audio-Driven%20Multi-Person%20Conversational%20Video%20Generation&body=Title%3A%20Let%20Them%20Talk%3A%20Audio-Driven%20Multi-Person%20Conversational%20Video%20Generation%0AAuthor%3A%20Zhe%20Kong%20and%20Feng%20Gao%20and%20Yong%20Zhang%20and%20Zhuoliang%20Kang%20and%20Xiaoming%20Wei%20and%20Xunliang%20Cai%20and%20Guanying%20Chen%20and%20Wenhan%20Luo%0AAbstract%3A%20%20%20Audio-driven%20human%20animation%20methods%2C%20such%20as%20talking%20head%20and%20talking%20body%0Ageneration%2C%20have%20made%20remarkable%20progress%20in%20generating%20synchronized%20facial%0Amovements%20and%20appealing%20visual%20quality%20videos.%20However%2C%20existing%20methods%0Aprimarily%20focus%20on%20single%20human%20animation%20and%20struggle%20with%20multi-stream%20audio%0Ainputs%2C%20facing%20incorrect%20binding%20problems%20between%20audio%20and%20persons.%0AAdditionally%2C%20they%20exhibit%20limitations%20in%20instruction-following%20capabilities.%0ATo%20solve%20this%20problem%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%20task%3A%20Multi-Person%0AConversational%20Video%20Generation%2C%20and%20introduce%20a%20new%20framework%2C%20MultiTalk%2C%20to%0Aaddress%20the%20challenges%20during%20multi-person%20generation.%20Specifically%2C%20for%20audio%0Ainjection%2C%20we%20investigate%20several%20schemes%20and%20propose%20the%20Label%20Rotary%20Position%0AEmbedding%20%28L-RoPE%29%20method%20to%20resolve%20the%20audio%20and%20person%20binding%20problem.%0AFurthermore%2C%20during%20training%2C%20we%20observe%20that%20partial%20parameter%20training%20and%0Amulti-task%20training%20are%20crucial%20for%20preserving%20the%20instruction-following%0Aability%20of%20the%20base%20model.%20MultiTalk%20achieves%20superior%20performance%20compared%20to%0Aother%20methods%20on%20several%20datasets%2C%20including%20talking%20head%2C%20talking%20body%2C%20and%0Amulti-person%20datasets%2C%20demonstrating%20the%20powerful%20generation%20capabilities%20of%0Aour%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22647v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLet%2520Them%2520Talk%253A%2520Audio-Driven%2520Multi-Person%2520Conversational%2520Video%2520Generation%26entry.906535625%3DZhe%2520Kong%2520and%2520Feng%2520Gao%2520and%2520Yong%2520Zhang%2520and%2520Zhuoliang%2520Kang%2520and%2520Xiaoming%2520Wei%2520and%2520Xunliang%2520Cai%2520and%2520Guanying%2520Chen%2520and%2520Wenhan%2520Luo%26entry.1292438233%3D%2520%2520Audio-driven%2520human%2520animation%2520methods%252C%2520such%2520as%2520talking%2520head%2520and%2520talking%2520body%250Ageneration%252C%2520have%2520made%2520remarkable%2520progress%2520in%2520generating%2520synchronized%2520facial%250Amovements%2520and%2520appealing%2520visual%2520quality%2520videos.%2520However%252C%2520existing%2520methods%250Aprimarily%2520focus%2520on%2520single%2520human%2520animation%2520and%2520struggle%2520with%2520multi-stream%2520audio%250Ainputs%252C%2520facing%2520incorrect%2520binding%2520problems%2520between%2520audio%2520and%2520persons.%250AAdditionally%252C%2520they%2520exhibit%2520limitations%2520in%2520instruction-following%2520capabilities.%250ATo%2520solve%2520this%2520problem%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520task%253A%2520Multi-Person%250AConversational%2520Video%2520Generation%252C%2520and%2520introduce%2520a%2520new%2520framework%252C%2520MultiTalk%252C%2520to%250Aaddress%2520the%2520challenges%2520during%2520multi-person%2520generation.%2520Specifically%252C%2520for%2520audio%250Ainjection%252C%2520we%2520investigate%2520several%2520schemes%2520and%2520propose%2520the%2520Label%2520Rotary%2520Position%250AEmbedding%2520%2528L-RoPE%2529%2520method%2520to%2520resolve%2520the%2520audio%2520and%2520person%2520binding%2520problem.%250AFurthermore%252C%2520during%2520training%252C%2520we%2520observe%2520that%2520partial%2520parameter%2520training%2520and%250Amulti-task%2520training%2520are%2520crucial%2520for%2520preserving%2520the%2520instruction-following%250Aability%2520of%2520the%2520base%2520model.%2520MultiTalk%2520achieves%2520superior%2520performance%2520compared%2520to%250Aother%2520methods%2520on%2520several%2520datasets%252C%2520including%2520talking%2520head%252C%2520talking%2520body%252C%2520and%250Amulti-person%2520datasets%252C%2520demonstrating%2520the%2520powerful%2520generation%2520capabilities%2520of%250Aour%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22647v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Let%20Them%20Talk%3A%20Audio-Driven%20Multi-Person%20Conversational%20Video%20Generation&entry.906535625=Zhe%20Kong%20and%20Feng%20Gao%20and%20Yong%20Zhang%20and%20Zhuoliang%20Kang%20and%20Xiaoming%20Wei%20and%20Xunliang%20Cai%20and%20Guanying%20Chen%20and%20Wenhan%20Luo&entry.1292438233=%20%20Audio-driven%20human%20animation%20methods%2C%20such%20as%20talking%20head%20and%20talking%20body%0Ageneration%2C%20have%20made%20remarkable%20progress%20in%20generating%20synchronized%20facial%0Amovements%20and%20appealing%20visual%20quality%20videos.%20However%2C%20existing%20methods%0Aprimarily%20focus%20on%20single%20human%20animation%20and%20struggle%20with%20multi-stream%20audio%0Ainputs%2C%20facing%20incorrect%20binding%20problems%20between%20audio%20and%20persons.%0AAdditionally%2C%20they%20exhibit%20limitations%20in%20instruction-following%20capabilities.%0ATo%20solve%20this%20problem%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%20task%3A%20Multi-Person%0AConversational%20Video%20Generation%2C%20and%20introduce%20a%20new%20framework%2C%20MultiTalk%2C%20to%0Aaddress%20the%20challenges%20during%20multi-person%20generation.%20Specifically%2C%20for%20audio%0Ainjection%2C%20we%20investigate%20several%20schemes%20and%20propose%20the%20Label%20Rotary%20Position%0AEmbedding%20%28L-RoPE%29%20method%20to%20resolve%20the%20audio%20and%20person%20binding%20problem.%0AFurthermore%2C%20during%20training%2C%20we%20observe%20that%20partial%20parameter%20training%20and%0Amulti-task%20training%20are%20crucial%20for%20preserving%20the%20instruction-following%0Aability%20of%20the%20base%20model.%20MultiTalk%20achieves%20superior%20performance%20compared%20to%0Aother%20methods%20on%20several%20datasets%2C%20including%20talking%20head%2C%20talking%20body%2C%20and%0Amulti-person%20datasets%2C%20demonstrating%20the%20powerful%20generation%20capabilities%20of%0Aour%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22647v1&entry.124074799=Read"},
{"title": "Task-Driven Implicit Representations for Automated Design of LiDAR\n  Systems", "author": "Nikhil Behari and Aaron Young and Akshat Dave and Ramesh Raskar", "abstract": "  Imaging system design is a complex, time-consuming, and largely manual\nprocess; LiDAR design, ubiquitous in mobile devices, autonomous vehicles, and\naerial imaging platforms, adds further complexity through unique spatial and\ntemporal sampling requirements. In this work, we propose a framework for\nautomated, task-driven LiDAR system design under arbitrary constraints. To\nachieve this, we represent LiDAR configurations in a continuous six-dimensional\ndesign space and learn task-specific implicit densities in this space via\nflow-based generative modeling. We then synthesize new LiDAR systems by\nmodeling sensors as parametric distributions in 6D space and fitting these\ndistributions to our learned implicit density using expectation-maximization,\nenabling efficient, constraint-aware LiDAR system design. We validate our\nmethod on diverse tasks in 3D vision, enabling automated LiDAR system design\nacross real-world-inspired applications in face scanning, robotic tracking, and\nobject detection.\n", "link": "http://arxiv.org/abs/2505.22344v1", "date": "2025-05-28", "relevancy": 2.3426, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6001}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5838}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task-Driven%20Implicit%20Representations%20for%20Automated%20Design%20of%20LiDAR%0A%20%20Systems&body=Title%3A%20Task-Driven%20Implicit%20Representations%20for%20Automated%20Design%20of%20LiDAR%0A%20%20Systems%0AAuthor%3A%20Nikhil%20Behari%20and%20Aaron%20Young%20and%20Akshat%20Dave%20and%20Ramesh%20Raskar%0AAbstract%3A%20%20%20Imaging%20system%20design%20is%20a%20complex%2C%20time-consuming%2C%20and%20largely%20manual%0Aprocess%3B%20LiDAR%20design%2C%20ubiquitous%20in%20mobile%20devices%2C%20autonomous%20vehicles%2C%20and%0Aaerial%20imaging%20platforms%2C%20adds%20further%20complexity%20through%20unique%20spatial%20and%0Atemporal%20sampling%20requirements.%20In%20this%20work%2C%20we%20propose%20a%20framework%20for%0Aautomated%2C%20task-driven%20LiDAR%20system%20design%20under%20arbitrary%20constraints.%20To%0Aachieve%20this%2C%20we%20represent%20LiDAR%20configurations%20in%20a%20continuous%20six-dimensional%0Adesign%20space%20and%20learn%20task-specific%20implicit%20densities%20in%20this%20space%20via%0Aflow-based%20generative%20modeling.%20We%20then%20synthesize%20new%20LiDAR%20systems%20by%0Amodeling%20sensors%20as%20parametric%20distributions%20in%206D%20space%20and%20fitting%20these%0Adistributions%20to%20our%20learned%20implicit%20density%20using%20expectation-maximization%2C%0Aenabling%20efficient%2C%20constraint-aware%20LiDAR%20system%20design.%20We%20validate%20our%0Amethod%20on%20diverse%20tasks%20in%203D%20vision%2C%20enabling%20automated%20LiDAR%20system%20design%0Aacross%20real-world-inspired%20applications%20in%20face%20scanning%2C%20robotic%20tracking%2C%20and%0Aobject%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22344v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask-Driven%2520Implicit%2520Representations%2520for%2520Automated%2520Design%2520of%2520LiDAR%250A%2520%2520Systems%26entry.906535625%3DNikhil%2520Behari%2520and%2520Aaron%2520Young%2520and%2520Akshat%2520Dave%2520and%2520Ramesh%2520Raskar%26entry.1292438233%3D%2520%2520Imaging%2520system%2520design%2520is%2520a%2520complex%252C%2520time-consuming%252C%2520and%2520largely%2520manual%250Aprocess%253B%2520LiDAR%2520design%252C%2520ubiquitous%2520in%2520mobile%2520devices%252C%2520autonomous%2520vehicles%252C%2520and%250Aaerial%2520imaging%2520platforms%252C%2520adds%2520further%2520complexity%2520through%2520unique%2520spatial%2520and%250Atemporal%2520sampling%2520requirements.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520framework%2520for%250Aautomated%252C%2520task-driven%2520LiDAR%2520system%2520design%2520under%2520arbitrary%2520constraints.%2520To%250Aachieve%2520this%252C%2520we%2520represent%2520LiDAR%2520configurations%2520in%2520a%2520continuous%2520six-dimensional%250Adesign%2520space%2520and%2520learn%2520task-specific%2520implicit%2520densities%2520in%2520this%2520space%2520via%250Aflow-based%2520generative%2520modeling.%2520We%2520then%2520synthesize%2520new%2520LiDAR%2520systems%2520by%250Amodeling%2520sensors%2520as%2520parametric%2520distributions%2520in%25206D%2520space%2520and%2520fitting%2520these%250Adistributions%2520to%2520our%2520learned%2520implicit%2520density%2520using%2520expectation-maximization%252C%250Aenabling%2520efficient%252C%2520constraint-aware%2520LiDAR%2520system%2520design.%2520We%2520validate%2520our%250Amethod%2520on%2520diverse%2520tasks%2520in%25203D%2520vision%252C%2520enabling%2520automated%2520LiDAR%2520system%2520design%250Aacross%2520real-world-inspired%2520applications%2520in%2520face%2520scanning%252C%2520robotic%2520tracking%252C%2520and%250Aobject%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22344v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task-Driven%20Implicit%20Representations%20for%20Automated%20Design%20of%20LiDAR%0A%20%20Systems&entry.906535625=Nikhil%20Behari%20and%20Aaron%20Young%20and%20Akshat%20Dave%20and%20Ramesh%20Raskar&entry.1292438233=%20%20Imaging%20system%20design%20is%20a%20complex%2C%20time-consuming%2C%20and%20largely%20manual%0Aprocess%3B%20LiDAR%20design%2C%20ubiquitous%20in%20mobile%20devices%2C%20autonomous%20vehicles%2C%20and%0Aaerial%20imaging%20platforms%2C%20adds%20further%20complexity%20through%20unique%20spatial%20and%0Atemporal%20sampling%20requirements.%20In%20this%20work%2C%20we%20propose%20a%20framework%20for%0Aautomated%2C%20task-driven%20LiDAR%20system%20design%20under%20arbitrary%20constraints.%20To%0Aachieve%20this%2C%20we%20represent%20LiDAR%20configurations%20in%20a%20continuous%20six-dimensional%0Adesign%20space%20and%20learn%20task-specific%20implicit%20densities%20in%20this%20space%20via%0Aflow-based%20generative%20modeling.%20We%20then%20synthesize%20new%20LiDAR%20systems%20by%0Amodeling%20sensors%20as%20parametric%20distributions%20in%206D%20space%20and%20fitting%20these%0Adistributions%20to%20our%20learned%20implicit%20density%20using%20expectation-maximization%2C%0Aenabling%20efficient%2C%20constraint-aware%20LiDAR%20system%20design.%20We%20validate%20our%0Amethod%20on%20diverse%20tasks%20in%203D%20vision%2C%20enabling%20automated%20LiDAR%20system%20design%0Aacross%20real-world-inspired%20applications%20in%20face%20scanning%2C%20robotic%20tracking%2C%20and%0Aobject%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22344v1&entry.124074799=Read"},
{"title": "GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action\n  Control", "author": "Anthony Chen and Wenzhao Zheng and Yida Wang and Xueyang Zhang and Kun Zhan and Peng Jia and Kurt Keutzer and Shangbang Zhang", "abstract": "  Recent advancements in world models have revolutionized dynamic environment\nsimulation, allowing systems to foresee future states and assess potential\nactions. In autonomous driving, these capabilities help vehicles anticipate the\nbehavior of other road users, perform risk-aware planning, accelerate training\nin simulation, and adapt to novel scenarios, thereby enhancing safety and\nreliability. Current approaches exhibit deficiencies in maintaining robust 3D\ngeometric consistency or accumulating artifacts during occlusion handling, both\ncritical for reliable safety assessment in autonomous navigation tasks. To\naddress this, we introduce GeoDrive, which explicitly integrates robust 3D\ngeometry conditions into driving world models to enhance spatial understanding\nand action controllability. Specifically, we first extract a 3D representation\nfrom the input frame and then obtain its 2D rendering based on the\nuser-specified ego-car trajectory. To enable dynamic modeling, we propose a\ndynamic editing module during training to enhance the renderings by editing the\npositions of the vehicles. Extensive experiments demonstrate that our method\nsignificantly outperforms existing models in both action accuracy and 3D\nspatial awareness, leading to more realistic, adaptable, and reliable scene\nmodeling for safer autonomous driving. Additionally, our model can generalize\nto novel trajectories and offers interactive scene editing capabilities, such\nas object editing and object trajectory control.\n", "link": "http://arxiv.org/abs/2505.22421v1", "date": "2025-05-28", "relevancy": 2.333, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5999}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5835}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoDrive%3A%203D%20Geometry-Informed%20Driving%20World%20Model%20with%20Precise%20Action%0A%20%20Control&body=Title%3A%20GeoDrive%3A%203D%20Geometry-Informed%20Driving%20World%20Model%20with%20Precise%20Action%0A%20%20Control%0AAuthor%3A%20Anthony%20Chen%20and%20Wenzhao%20Zheng%20and%20Yida%20Wang%20and%20Xueyang%20Zhang%20and%20Kun%20Zhan%20and%20Peng%20Jia%20and%20Kurt%20Keutzer%20and%20Shangbang%20Zhang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20world%20models%20have%20revolutionized%20dynamic%20environment%0Asimulation%2C%20allowing%20systems%20to%20foresee%20future%20states%20and%20assess%20potential%0Aactions.%20In%20autonomous%20driving%2C%20these%20capabilities%20help%20vehicles%20anticipate%20the%0Abehavior%20of%20other%20road%20users%2C%20perform%20risk-aware%20planning%2C%20accelerate%20training%0Ain%20simulation%2C%20and%20adapt%20to%20novel%20scenarios%2C%20thereby%20enhancing%20safety%20and%0Areliability.%20Current%20approaches%20exhibit%20deficiencies%20in%20maintaining%20robust%203D%0Ageometric%20consistency%20or%20accumulating%20artifacts%20during%20occlusion%20handling%2C%20both%0Acritical%20for%20reliable%20safety%20assessment%20in%20autonomous%20navigation%20tasks.%20To%0Aaddress%20this%2C%20we%20introduce%20GeoDrive%2C%20which%20explicitly%20integrates%20robust%203D%0Ageometry%20conditions%20into%20driving%20world%20models%20to%20enhance%20spatial%20understanding%0Aand%20action%20controllability.%20Specifically%2C%20we%20first%20extract%20a%203D%20representation%0Afrom%20the%20input%20frame%20and%20then%20obtain%20its%202D%20rendering%20based%20on%20the%0Auser-specified%20ego-car%20trajectory.%20To%20enable%20dynamic%20modeling%2C%20we%20propose%20a%0Adynamic%20editing%20module%20during%20training%20to%20enhance%20the%20renderings%20by%20editing%20the%0Apositions%20of%20the%20vehicles.%20Extensive%20experiments%20demonstrate%20that%20our%20method%0Asignificantly%20outperforms%20existing%20models%20in%20both%20action%20accuracy%20and%203D%0Aspatial%20awareness%2C%20leading%20to%20more%20realistic%2C%20adaptable%2C%20and%20reliable%20scene%0Amodeling%20for%20safer%20autonomous%20driving.%20Additionally%2C%20our%20model%20can%20generalize%0Ato%20novel%20trajectories%20and%20offers%20interactive%20scene%20editing%20capabilities%2C%20such%0Aas%20object%20editing%20and%20object%20trajectory%20control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22421v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoDrive%253A%25203D%2520Geometry-Informed%2520Driving%2520World%2520Model%2520with%2520Precise%2520Action%250A%2520%2520Control%26entry.906535625%3DAnthony%2520Chen%2520and%2520Wenzhao%2520Zheng%2520and%2520Yida%2520Wang%2520and%2520Xueyang%2520Zhang%2520and%2520Kun%2520Zhan%2520and%2520Peng%2520Jia%2520and%2520Kurt%2520Keutzer%2520and%2520Shangbang%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520world%2520models%2520have%2520revolutionized%2520dynamic%2520environment%250Asimulation%252C%2520allowing%2520systems%2520to%2520foresee%2520future%2520states%2520and%2520assess%2520potential%250Aactions.%2520In%2520autonomous%2520driving%252C%2520these%2520capabilities%2520help%2520vehicles%2520anticipate%2520the%250Abehavior%2520of%2520other%2520road%2520users%252C%2520perform%2520risk-aware%2520planning%252C%2520accelerate%2520training%250Ain%2520simulation%252C%2520and%2520adapt%2520to%2520novel%2520scenarios%252C%2520thereby%2520enhancing%2520safety%2520and%250Areliability.%2520Current%2520approaches%2520exhibit%2520deficiencies%2520in%2520maintaining%2520robust%25203D%250Ageometric%2520consistency%2520or%2520accumulating%2520artifacts%2520during%2520occlusion%2520handling%252C%2520both%250Acritical%2520for%2520reliable%2520safety%2520assessment%2520in%2520autonomous%2520navigation%2520tasks.%2520To%250Aaddress%2520this%252C%2520we%2520introduce%2520GeoDrive%252C%2520which%2520explicitly%2520integrates%2520robust%25203D%250Ageometry%2520conditions%2520into%2520driving%2520world%2520models%2520to%2520enhance%2520spatial%2520understanding%250Aand%2520action%2520controllability.%2520Specifically%252C%2520we%2520first%2520extract%2520a%25203D%2520representation%250Afrom%2520the%2520input%2520frame%2520and%2520then%2520obtain%2520its%25202D%2520rendering%2520based%2520on%2520the%250Auser-specified%2520ego-car%2520trajectory.%2520To%2520enable%2520dynamic%2520modeling%252C%2520we%2520propose%2520a%250Adynamic%2520editing%2520module%2520during%2520training%2520to%2520enhance%2520the%2520renderings%2520by%2520editing%2520the%250Apositions%2520of%2520the%2520vehicles.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%250Asignificantly%2520outperforms%2520existing%2520models%2520in%2520both%2520action%2520accuracy%2520and%25203D%250Aspatial%2520awareness%252C%2520leading%2520to%2520more%2520realistic%252C%2520adaptable%252C%2520and%2520reliable%2520scene%250Amodeling%2520for%2520safer%2520autonomous%2520driving.%2520Additionally%252C%2520our%2520model%2520can%2520generalize%250Ato%2520novel%2520trajectories%2520and%2520offers%2520interactive%2520scene%2520editing%2520capabilities%252C%2520such%250Aas%2520object%2520editing%2520and%2520object%2520trajectory%2520control.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22421v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoDrive%3A%203D%20Geometry-Informed%20Driving%20World%20Model%20with%20Precise%20Action%0A%20%20Control&entry.906535625=Anthony%20Chen%20and%20Wenzhao%20Zheng%20and%20Yida%20Wang%20and%20Xueyang%20Zhang%20and%20Kun%20Zhan%20and%20Peng%20Jia%20and%20Kurt%20Keutzer%20and%20Shangbang%20Zhang&entry.1292438233=%20%20Recent%20advancements%20in%20world%20models%20have%20revolutionized%20dynamic%20environment%0Asimulation%2C%20allowing%20systems%20to%20foresee%20future%20states%20and%20assess%20potential%0Aactions.%20In%20autonomous%20driving%2C%20these%20capabilities%20help%20vehicles%20anticipate%20the%0Abehavior%20of%20other%20road%20users%2C%20perform%20risk-aware%20planning%2C%20accelerate%20training%0Ain%20simulation%2C%20and%20adapt%20to%20novel%20scenarios%2C%20thereby%20enhancing%20safety%20and%0Areliability.%20Current%20approaches%20exhibit%20deficiencies%20in%20maintaining%20robust%203D%0Ageometric%20consistency%20or%20accumulating%20artifacts%20during%20occlusion%20handling%2C%20both%0Acritical%20for%20reliable%20safety%20assessment%20in%20autonomous%20navigation%20tasks.%20To%0Aaddress%20this%2C%20we%20introduce%20GeoDrive%2C%20which%20explicitly%20integrates%20robust%203D%0Ageometry%20conditions%20into%20driving%20world%20models%20to%20enhance%20spatial%20understanding%0Aand%20action%20controllability.%20Specifically%2C%20we%20first%20extract%20a%203D%20representation%0Afrom%20the%20input%20frame%20and%20then%20obtain%20its%202D%20rendering%20based%20on%20the%0Auser-specified%20ego-car%20trajectory.%20To%20enable%20dynamic%20modeling%2C%20we%20propose%20a%0Adynamic%20editing%20module%20during%20training%20to%20enhance%20the%20renderings%20by%20editing%20the%0Apositions%20of%20the%20vehicles.%20Extensive%20experiments%20demonstrate%20that%20our%20method%0Asignificantly%20outperforms%20existing%20models%20in%20both%20action%20accuracy%20and%203D%0Aspatial%20awareness%2C%20leading%20to%20more%20realistic%2C%20adaptable%2C%20and%20reliable%20scene%0Amodeling%20for%20safer%20autonomous%20driving.%20Additionally%2C%20our%20model%20can%20generalize%0Ato%20novel%20trajectories%20and%20offers%20interactive%20scene%20editing%20capabilities%2C%20such%0Aas%20object%20editing%20and%20object%20trajectory%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22421v1&entry.124074799=Read"},
{"title": "Benignity of loss landscape with weight decay requires both large\n  overparametrization and initialization", "author": "Etienne Boursier and Matthew Bowditch and Matthias Englert and Ranko Lazic", "abstract": "  The optimization of neural networks under weight decay remains poorly\nunderstood from a theoretical standpoint. While weight decay is standard\npractice in modern training procedures, most theoretical analyses focus on\nunregularized settings. In this work, we investigate the loss landscape of the\n$\\ell_2$-regularized training loss for two-layer ReLU networks. We show that\nthe landscape becomes benign -- i.e., free of spurious local minima -- under\nlarge overparametrization, specifically when the network width $m$ satisfies $m\n\\gtrsim \\min(n^d, 2^n)$, where $n$ is the number of data points and $d$ the\ninput dimension. More precisely in this regime, almost all constant activation\nregions contain a global minimum and no spurious local minima. We further show\nthat this level of overparametrization is not only sufficient but also\nnecessary via the example of orthogonal data. Finally, we demonstrate that such\nloss landscape results primarily hold relevance in the large initialization\nregime. In contrast, for small initializations -- corresponding to the feature\nlearning regime -- optimization can still converge to spurious local minima,\ndespite the global benignity of the landscape.\n", "link": "http://arxiv.org/abs/2505.22578v1", "date": "2025-05-28", "relevancy": 2.3294, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4757}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4699}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benignity%20of%20loss%20landscape%20with%20weight%20decay%20requires%20both%20large%0A%20%20overparametrization%20and%20initialization&body=Title%3A%20Benignity%20of%20loss%20landscape%20with%20weight%20decay%20requires%20both%20large%0A%20%20overparametrization%20and%20initialization%0AAuthor%3A%20Etienne%20Boursier%20and%20Matthew%20Bowditch%20and%20Matthias%20Englert%20and%20Ranko%20Lazic%0AAbstract%3A%20%20%20The%20optimization%20of%20neural%20networks%20under%20weight%20decay%20remains%20poorly%0Aunderstood%20from%20a%20theoretical%20standpoint.%20While%20weight%20decay%20is%20standard%0Apractice%20in%20modern%20training%20procedures%2C%20most%20theoretical%20analyses%20focus%20on%0Aunregularized%20settings.%20In%20this%20work%2C%20we%20investigate%20the%20loss%20landscape%20of%20the%0A%24%5Cell_2%24-regularized%20training%20loss%20for%20two-layer%20ReLU%20networks.%20We%20show%20that%0Athe%20landscape%20becomes%20benign%20--%20i.e.%2C%20free%20of%20spurious%20local%20minima%20--%20under%0Alarge%20overparametrization%2C%20specifically%20when%20the%20network%20width%20%24m%24%20satisfies%20%24m%0A%5Cgtrsim%20%5Cmin%28n%5Ed%2C%202%5En%29%24%2C%20where%20%24n%24%20is%20the%20number%20of%20data%20points%20and%20%24d%24%20the%0Ainput%20dimension.%20More%20precisely%20in%20this%20regime%2C%20almost%20all%20constant%20activation%0Aregions%20contain%20a%20global%20minimum%20and%20no%20spurious%20local%20minima.%20We%20further%20show%0Athat%20this%20level%20of%20overparametrization%20is%20not%20only%20sufficient%20but%20also%0Anecessary%20via%20the%20example%20of%20orthogonal%20data.%20Finally%2C%20we%20demonstrate%20that%20such%0Aloss%20landscape%20results%20primarily%20hold%20relevance%20in%20the%20large%20initialization%0Aregime.%20In%20contrast%2C%20for%20small%20initializations%20--%20corresponding%20to%20the%20feature%0Alearning%20regime%20--%20optimization%20can%20still%20converge%20to%20spurious%20local%20minima%2C%0Adespite%20the%20global%20benignity%20of%20the%20landscape.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22578v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenignity%2520of%2520loss%2520landscape%2520with%2520weight%2520decay%2520requires%2520both%2520large%250A%2520%2520overparametrization%2520and%2520initialization%26entry.906535625%3DEtienne%2520Boursier%2520and%2520Matthew%2520Bowditch%2520and%2520Matthias%2520Englert%2520and%2520Ranko%2520Lazic%26entry.1292438233%3D%2520%2520The%2520optimization%2520of%2520neural%2520networks%2520under%2520weight%2520decay%2520remains%2520poorly%250Aunderstood%2520from%2520a%2520theoretical%2520standpoint.%2520While%2520weight%2520decay%2520is%2520standard%250Apractice%2520in%2520modern%2520training%2520procedures%252C%2520most%2520theoretical%2520analyses%2520focus%2520on%250Aunregularized%2520settings.%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%2520loss%2520landscape%2520of%2520the%250A%2524%255Cell_2%2524-regularized%2520training%2520loss%2520for%2520two-layer%2520ReLU%2520networks.%2520We%2520show%2520that%250Athe%2520landscape%2520becomes%2520benign%2520--%2520i.e.%252C%2520free%2520of%2520spurious%2520local%2520minima%2520--%2520under%250Alarge%2520overparametrization%252C%2520specifically%2520when%2520the%2520network%2520width%2520%2524m%2524%2520satisfies%2520%2524m%250A%255Cgtrsim%2520%255Cmin%2528n%255Ed%252C%25202%255En%2529%2524%252C%2520where%2520%2524n%2524%2520is%2520the%2520number%2520of%2520data%2520points%2520and%2520%2524d%2524%2520the%250Ainput%2520dimension.%2520More%2520precisely%2520in%2520this%2520regime%252C%2520almost%2520all%2520constant%2520activation%250Aregions%2520contain%2520a%2520global%2520minimum%2520and%2520no%2520spurious%2520local%2520minima.%2520We%2520further%2520show%250Athat%2520this%2520level%2520of%2520overparametrization%2520is%2520not%2520only%2520sufficient%2520but%2520also%250Anecessary%2520via%2520the%2520example%2520of%2520orthogonal%2520data.%2520Finally%252C%2520we%2520demonstrate%2520that%2520such%250Aloss%2520landscape%2520results%2520primarily%2520hold%2520relevance%2520in%2520the%2520large%2520initialization%250Aregime.%2520In%2520contrast%252C%2520for%2520small%2520initializations%2520--%2520corresponding%2520to%2520the%2520feature%250Alearning%2520regime%2520--%2520optimization%2520can%2520still%2520converge%2520to%2520spurious%2520local%2520minima%252C%250Adespite%2520the%2520global%2520benignity%2520of%2520the%2520landscape.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22578v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benignity%20of%20loss%20landscape%20with%20weight%20decay%20requires%20both%20large%0A%20%20overparametrization%20and%20initialization&entry.906535625=Etienne%20Boursier%20and%20Matthew%20Bowditch%20and%20Matthias%20Englert%20and%20Ranko%20Lazic&entry.1292438233=%20%20The%20optimization%20of%20neural%20networks%20under%20weight%20decay%20remains%20poorly%0Aunderstood%20from%20a%20theoretical%20standpoint.%20While%20weight%20decay%20is%20standard%0Apractice%20in%20modern%20training%20procedures%2C%20most%20theoretical%20analyses%20focus%20on%0Aunregularized%20settings.%20In%20this%20work%2C%20we%20investigate%20the%20loss%20landscape%20of%20the%0A%24%5Cell_2%24-regularized%20training%20loss%20for%20two-layer%20ReLU%20networks.%20We%20show%20that%0Athe%20landscape%20becomes%20benign%20--%20i.e.%2C%20free%20of%20spurious%20local%20minima%20--%20under%0Alarge%20overparametrization%2C%20specifically%20when%20the%20network%20width%20%24m%24%20satisfies%20%24m%0A%5Cgtrsim%20%5Cmin%28n%5Ed%2C%202%5En%29%24%2C%20where%20%24n%24%20is%20the%20number%20of%20data%20points%20and%20%24d%24%20the%0Ainput%20dimension.%20More%20precisely%20in%20this%20regime%2C%20almost%20all%20constant%20activation%0Aregions%20contain%20a%20global%20minimum%20and%20no%20spurious%20local%20minima.%20We%20further%20show%0Athat%20this%20level%20of%20overparametrization%20is%20not%20only%20sufficient%20but%20also%0Anecessary%20via%20the%20example%20of%20orthogonal%20data.%20Finally%2C%20we%20demonstrate%20that%20such%0Aloss%20landscape%20results%20primarily%20hold%20relevance%20in%20the%20large%20initialization%0Aregime.%20In%20contrast%2C%20for%20small%20initializations%20--%20corresponding%20to%20the%20feature%0Alearning%20regime%20--%20optimization%20can%20still%20converge%20to%20spurious%20local%20minima%2C%0Adespite%20the%20global%20benignity%20of%20the%20landscape.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22578v1&entry.124074799=Read"},
{"title": "PS4PRO: Pixel-to-pixel Supervision for Photorealistic Rendering and\n  Optimization", "author": "Yezhi Shen and Qiuchen Zhai and Fengqing Zhu", "abstract": "  Neural rendering methods have gained significant attention for their ability\nto reconstruct 3D scenes from 2D images. The core idea is to take multiple\nviews as input and optimize the reconstructed scene by minimizing the\nuncertainty in geometry and appearance across the views. However, the\nreconstruction quality is limited by the number of input views. This limitation\nis further pronounced in complex and dynamic scenes, where certain angles of\nobjects are never seen. In this paper, we propose to use video frame\ninterpolation as the data augmentation method for neural rendering.\nFurthermore, we design a lightweight yet high-quality video frame interpolation\nmodel, PS4PRO (Pixel-to-pixel Supervision for Photorealistic Rendering and\nOptimization). PS4PRO is trained on diverse video datasets, implicitly modeling\ncamera movement as well as real-world 3D geometry. Our model performs as an\nimplicit world prior, enriching the photo supervision for 3D reconstruction. By\nleveraging the proposed method, we effectively augment existing datasets for\nneural rendering methods. Our experimental results indicate that our method\nimproves the reconstruction performance on both static and dynamic scenes.\n", "link": "http://arxiv.org/abs/2505.22616v1", "date": "2025-05-28", "relevancy": 2.3178, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5982}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5768}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PS4PRO%3A%20Pixel-to-pixel%20Supervision%20for%20Photorealistic%20Rendering%20and%0A%20%20Optimization&body=Title%3A%20PS4PRO%3A%20Pixel-to-pixel%20Supervision%20for%20Photorealistic%20Rendering%20and%0A%20%20Optimization%0AAuthor%3A%20Yezhi%20Shen%20and%20Qiuchen%20Zhai%20and%20Fengqing%20Zhu%0AAbstract%3A%20%20%20Neural%20rendering%20methods%20have%20gained%20significant%20attention%20for%20their%20ability%0Ato%20reconstruct%203D%20scenes%20from%202D%20images.%20The%20core%20idea%20is%20to%20take%20multiple%0Aviews%20as%20input%20and%20optimize%20the%20reconstructed%20scene%20by%20minimizing%20the%0Auncertainty%20in%20geometry%20and%20appearance%20across%20the%20views.%20However%2C%20the%0Areconstruction%20quality%20is%20limited%20by%20the%20number%20of%20input%20views.%20This%20limitation%0Ais%20further%20pronounced%20in%20complex%20and%20dynamic%20scenes%2C%20where%20certain%20angles%20of%0Aobjects%20are%20never%20seen.%20In%20this%20paper%2C%20we%20propose%20to%20use%20video%20frame%0Ainterpolation%20as%20the%20data%20augmentation%20method%20for%20neural%20rendering.%0AFurthermore%2C%20we%20design%20a%20lightweight%20yet%20high-quality%20video%20frame%20interpolation%0Amodel%2C%20PS4PRO%20%28Pixel-to-pixel%20Supervision%20for%20Photorealistic%20Rendering%20and%0AOptimization%29.%20PS4PRO%20is%20trained%20on%20diverse%20video%20datasets%2C%20implicitly%20modeling%0Acamera%20movement%20as%20well%20as%20real-world%203D%20geometry.%20Our%20model%20performs%20as%20an%0Aimplicit%20world%20prior%2C%20enriching%20the%20photo%20supervision%20for%203D%20reconstruction.%20By%0Aleveraging%20the%20proposed%20method%2C%20we%20effectively%20augment%20existing%20datasets%20for%0Aneural%20rendering%20methods.%20Our%20experimental%20results%20indicate%20that%20our%20method%0Aimproves%20the%20reconstruction%20performance%20on%20both%20static%20and%20dynamic%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22616v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPS4PRO%253A%2520Pixel-to-pixel%2520Supervision%2520for%2520Photorealistic%2520Rendering%2520and%250A%2520%2520Optimization%26entry.906535625%3DYezhi%2520Shen%2520and%2520Qiuchen%2520Zhai%2520and%2520Fengqing%2520Zhu%26entry.1292438233%3D%2520%2520Neural%2520rendering%2520methods%2520have%2520gained%2520significant%2520attention%2520for%2520their%2520ability%250Ato%2520reconstruct%25203D%2520scenes%2520from%25202D%2520images.%2520The%2520core%2520idea%2520is%2520to%2520take%2520multiple%250Aviews%2520as%2520input%2520and%2520optimize%2520the%2520reconstructed%2520scene%2520by%2520minimizing%2520the%250Auncertainty%2520in%2520geometry%2520and%2520appearance%2520across%2520the%2520views.%2520However%252C%2520the%250Areconstruction%2520quality%2520is%2520limited%2520by%2520the%2520number%2520of%2520input%2520views.%2520This%2520limitation%250Ais%2520further%2520pronounced%2520in%2520complex%2520and%2520dynamic%2520scenes%252C%2520where%2520certain%2520angles%2520of%250Aobjects%2520are%2520never%2520seen.%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%2520use%2520video%2520frame%250Ainterpolation%2520as%2520the%2520data%2520augmentation%2520method%2520for%2520neural%2520rendering.%250AFurthermore%252C%2520we%2520design%2520a%2520lightweight%2520yet%2520high-quality%2520video%2520frame%2520interpolation%250Amodel%252C%2520PS4PRO%2520%2528Pixel-to-pixel%2520Supervision%2520for%2520Photorealistic%2520Rendering%2520and%250AOptimization%2529.%2520PS4PRO%2520is%2520trained%2520on%2520diverse%2520video%2520datasets%252C%2520implicitly%2520modeling%250Acamera%2520movement%2520as%2520well%2520as%2520real-world%25203D%2520geometry.%2520Our%2520model%2520performs%2520as%2520an%250Aimplicit%2520world%2520prior%252C%2520enriching%2520the%2520photo%2520supervision%2520for%25203D%2520reconstruction.%2520By%250Aleveraging%2520the%2520proposed%2520method%252C%2520we%2520effectively%2520augment%2520existing%2520datasets%2520for%250Aneural%2520rendering%2520methods.%2520Our%2520experimental%2520results%2520indicate%2520that%2520our%2520method%250Aimproves%2520the%2520reconstruction%2520performance%2520on%2520both%2520static%2520and%2520dynamic%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22616v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PS4PRO%3A%20Pixel-to-pixel%20Supervision%20for%20Photorealistic%20Rendering%20and%0A%20%20Optimization&entry.906535625=Yezhi%20Shen%20and%20Qiuchen%20Zhai%20and%20Fengqing%20Zhu&entry.1292438233=%20%20Neural%20rendering%20methods%20have%20gained%20significant%20attention%20for%20their%20ability%0Ato%20reconstruct%203D%20scenes%20from%202D%20images.%20The%20core%20idea%20is%20to%20take%20multiple%0Aviews%20as%20input%20and%20optimize%20the%20reconstructed%20scene%20by%20minimizing%20the%0Auncertainty%20in%20geometry%20and%20appearance%20across%20the%20views.%20However%2C%20the%0Areconstruction%20quality%20is%20limited%20by%20the%20number%20of%20input%20views.%20This%20limitation%0Ais%20further%20pronounced%20in%20complex%20and%20dynamic%20scenes%2C%20where%20certain%20angles%20of%0Aobjects%20are%20never%20seen.%20In%20this%20paper%2C%20we%20propose%20to%20use%20video%20frame%0Ainterpolation%20as%20the%20data%20augmentation%20method%20for%20neural%20rendering.%0AFurthermore%2C%20we%20design%20a%20lightweight%20yet%20high-quality%20video%20frame%20interpolation%0Amodel%2C%20PS4PRO%20%28Pixel-to-pixel%20Supervision%20for%20Photorealistic%20Rendering%20and%0AOptimization%29.%20PS4PRO%20is%20trained%20on%20diverse%20video%20datasets%2C%20implicitly%20modeling%0Acamera%20movement%20as%20well%20as%20real-world%203D%20geometry.%20Our%20model%20performs%20as%20an%0Aimplicit%20world%20prior%2C%20enriching%20the%20photo%20supervision%20for%203D%20reconstruction.%20By%0Aleveraging%20the%20proposed%20method%2C%20we%20effectively%20augment%20existing%20datasets%20for%0Aneural%20rendering%20methods.%20Our%20experimental%20results%20indicate%20that%20our%20method%0Aimproves%20the%20reconstruction%20performance%20on%20both%20static%20and%20dynamic%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22616v1&entry.124074799=Read"},
{"title": "If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?", "author": "Ishwar B Balappanawar and Vamshi Krishna Bonagiri and Anish R Joishy and Manas Gaur and Krishnaprasad Thirunarayan and Ponnurangam Kumaraguru", "abstract": "  Large Language Models (LLMs) demonstrate impressive reasoning capabilities in\nfamiliar contexts, but struggle when the context conflicts with their\nparametric knowledge. To investigate this phenomenon, we introduce\nCounterLogic, a dataset containing 1,800 examples across 9 logical schemas,\nexplicitly designed to evaluate logical reasoning through counterfactual\n(hypothetical knowledge-conflicting) scenarios. Our systematic evaluation of 11\nLLMs across 6 different datasets reveals a consistent performance degradation,\nwith accuracies dropping by 27% on average when reasoning through\ncounterfactual information. We propose Self-Segregate, a prompting method\nenabling metacognitive awareness (explicitly identifying knowledge conflicts)\nbefore reasoning. Our method dramatically narrows the average performance gaps\nfrom 27% to just 11%, while significantly increasing the overall accuracy\n(+7.5%). We discuss the implications of these findings and draw parallels to\nhuman cognitive processes, particularly on how humans disambiguate conflicting\ninformation during reasoning tasks. Our findings offer practical insights for\nunderstanding and enhancing LLMs reasoning capabilities in real-world\napplications, especially where models must logically reason independently of\ntheir factual knowledge.\n", "link": "http://arxiv.org/abs/2505.22318v1", "date": "2025-05-28", "relevancy": 2.3165, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4664}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4617}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20If%20Pigs%20Could%20Fly...%20Can%20LLMs%20Logically%20Reason%20Through%20Counterfactuals%3F&body=Title%3A%20If%20Pigs%20Could%20Fly...%20Can%20LLMs%20Logically%20Reason%20Through%20Counterfactuals%3F%0AAuthor%3A%20Ishwar%20B%20Balappanawar%20and%20Vamshi%20Krishna%20Bonagiri%20and%20Anish%20R%20Joishy%20and%20Manas%20Gaur%20and%20Krishnaprasad%20Thirunarayan%20and%20Ponnurangam%20Kumaraguru%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20impressive%20reasoning%20capabilities%20in%0Afamiliar%20contexts%2C%20but%20struggle%20when%20the%20context%20conflicts%20with%20their%0Aparametric%20knowledge.%20To%20investigate%20this%20phenomenon%2C%20we%20introduce%0ACounterLogic%2C%20a%20dataset%20containing%201%2C800%20examples%20across%209%20logical%20schemas%2C%0Aexplicitly%20designed%20to%20evaluate%20logical%20reasoning%20through%20counterfactual%0A%28hypothetical%20knowledge-conflicting%29%20scenarios.%20Our%20systematic%20evaluation%20of%2011%0ALLMs%20across%206%20different%20datasets%20reveals%20a%20consistent%20performance%20degradation%2C%0Awith%20accuracies%20dropping%20by%2027%25%20on%20average%20when%20reasoning%20through%0Acounterfactual%20information.%20We%20propose%20Self-Segregate%2C%20a%20prompting%20method%0Aenabling%20metacognitive%20awareness%20%28explicitly%20identifying%20knowledge%20conflicts%29%0Abefore%20reasoning.%20Our%20method%20dramatically%20narrows%20the%20average%20performance%20gaps%0Afrom%2027%25%20to%20just%2011%25%2C%20while%20significantly%20increasing%20the%20overall%20accuracy%0A%28%2B7.5%25%29.%20We%20discuss%20the%20implications%20of%20these%20findings%20and%20draw%20parallels%20to%0Ahuman%20cognitive%20processes%2C%20particularly%20on%20how%20humans%20disambiguate%20conflicting%0Ainformation%20during%20reasoning%20tasks.%20Our%20findings%20offer%20practical%20insights%20for%0Aunderstanding%20and%20enhancing%20LLMs%20reasoning%20capabilities%20in%20real-world%0Aapplications%2C%20especially%20where%20models%20must%20logically%20reason%20independently%20of%0Atheir%20factual%20knowledge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22318v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIf%2520Pigs%2520Could%2520Fly...%2520Can%2520LLMs%2520Logically%2520Reason%2520Through%2520Counterfactuals%253F%26entry.906535625%3DIshwar%2520B%2520Balappanawar%2520and%2520Vamshi%2520Krishna%2520Bonagiri%2520and%2520Anish%2520R%2520Joishy%2520and%2520Manas%2520Gaur%2520and%2520Krishnaprasad%2520Thirunarayan%2520and%2520Ponnurangam%2520Kumaraguru%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520demonstrate%2520impressive%2520reasoning%2520capabilities%2520in%250Afamiliar%2520contexts%252C%2520but%2520struggle%2520when%2520the%2520context%2520conflicts%2520with%2520their%250Aparametric%2520knowledge.%2520To%2520investigate%2520this%2520phenomenon%252C%2520we%2520introduce%250ACounterLogic%252C%2520a%2520dataset%2520containing%25201%252C800%2520examples%2520across%25209%2520logical%2520schemas%252C%250Aexplicitly%2520designed%2520to%2520evaluate%2520logical%2520reasoning%2520through%2520counterfactual%250A%2528hypothetical%2520knowledge-conflicting%2529%2520scenarios.%2520Our%2520systematic%2520evaluation%2520of%252011%250ALLMs%2520across%25206%2520different%2520datasets%2520reveals%2520a%2520consistent%2520performance%2520degradation%252C%250Awith%2520accuracies%2520dropping%2520by%252027%2525%2520on%2520average%2520when%2520reasoning%2520through%250Acounterfactual%2520information.%2520We%2520propose%2520Self-Segregate%252C%2520a%2520prompting%2520method%250Aenabling%2520metacognitive%2520awareness%2520%2528explicitly%2520identifying%2520knowledge%2520conflicts%2529%250Abefore%2520reasoning.%2520Our%2520method%2520dramatically%2520narrows%2520the%2520average%2520performance%2520gaps%250Afrom%252027%2525%2520to%2520just%252011%2525%252C%2520while%2520significantly%2520increasing%2520the%2520overall%2520accuracy%250A%2528%252B7.5%2525%2529.%2520We%2520discuss%2520the%2520implications%2520of%2520these%2520findings%2520and%2520draw%2520parallels%2520to%250Ahuman%2520cognitive%2520processes%252C%2520particularly%2520on%2520how%2520humans%2520disambiguate%2520conflicting%250Ainformation%2520during%2520reasoning%2520tasks.%2520Our%2520findings%2520offer%2520practical%2520insights%2520for%250Aunderstanding%2520and%2520enhancing%2520LLMs%2520reasoning%2520capabilities%2520in%2520real-world%250Aapplications%252C%2520especially%2520where%2520models%2520must%2520logically%2520reason%2520independently%2520of%250Atheir%2520factual%2520knowledge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22318v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=If%20Pigs%20Could%20Fly...%20Can%20LLMs%20Logically%20Reason%20Through%20Counterfactuals%3F&entry.906535625=Ishwar%20B%20Balappanawar%20and%20Vamshi%20Krishna%20Bonagiri%20and%20Anish%20R%20Joishy%20and%20Manas%20Gaur%20and%20Krishnaprasad%20Thirunarayan%20and%20Ponnurangam%20Kumaraguru&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20impressive%20reasoning%20capabilities%20in%0Afamiliar%20contexts%2C%20but%20struggle%20when%20the%20context%20conflicts%20with%20their%0Aparametric%20knowledge.%20To%20investigate%20this%20phenomenon%2C%20we%20introduce%0ACounterLogic%2C%20a%20dataset%20containing%201%2C800%20examples%20across%209%20logical%20schemas%2C%0Aexplicitly%20designed%20to%20evaluate%20logical%20reasoning%20through%20counterfactual%0A%28hypothetical%20knowledge-conflicting%29%20scenarios.%20Our%20systematic%20evaluation%20of%2011%0ALLMs%20across%206%20different%20datasets%20reveals%20a%20consistent%20performance%20degradation%2C%0Awith%20accuracies%20dropping%20by%2027%25%20on%20average%20when%20reasoning%20through%0Acounterfactual%20information.%20We%20propose%20Self-Segregate%2C%20a%20prompting%20method%0Aenabling%20metacognitive%20awareness%20%28explicitly%20identifying%20knowledge%20conflicts%29%0Abefore%20reasoning.%20Our%20method%20dramatically%20narrows%20the%20average%20performance%20gaps%0Afrom%2027%25%20to%20just%2011%25%2C%20while%20significantly%20increasing%20the%20overall%20accuracy%0A%28%2B7.5%25%29.%20We%20discuss%20the%20implications%20of%20these%20findings%20and%20draw%20parallels%20to%0Ahuman%20cognitive%20processes%2C%20particularly%20on%20how%20humans%20disambiguate%20conflicting%0Ainformation%20during%20reasoning%20tasks.%20Our%20findings%20offer%20practical%20insights%20for%0Aunderstanding%20and%20enhancing%20LLMs%20reasoning%20capabilities%20in%20real-world%0Aapplications%2C%20especially%20where%20models%20must%20logically%20reason%20independently%20of%0Atheir%20factual%20knowledge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22318v1&entry.124074799=Read"},
{"title": "Look Within or Look Beyond? A Theoretical Comparison Between\n  Parameter-Efficient and Full Fine-Tuning", "author": "Yongkang Liu and Xingle Xu and Ercong Nie and Zijing Wang and Shi Feng and Daling Wang and Qian Li and Hinrich Sch\u00fctze", "abstract": "  Parameter-Efficient Fine-Tuning (PEFT) methods achieve performance comparable\nto Full Fine-Tuning (FFT) while requiring significantly fewer computing\nresources, making it the go-to choice for researchers. We find that although\nPEFT can achieve competitive results on some benchmarks, its performance falls\nshort of FFT in complex tasks, such as reasoning and instruction-based\nfine-tuning. In this paper, we compare the characteristics of PEFT and FFT in\nterms of representational capacity and robustness based on optimization theory.\nWe theoretically demonstrate that PEFT is a strict subset of FFT. By providing\ntheoretical upper bounds for PEFT, we show that the limited parameter space\nconstrains the model's representational ability, making it more susceptible to\nperturbations. Experiments on 15 datasets encompassing classification,\ngeneration, reasoning, instruction fine-tuning tasks and 11 adversarial test\nsets validate our theories. We hope that these results spark further research\nbeyond the realms of well established PEFT. The source code is in the anonymous\nGithub repository\\footnote{https://github.com/misonsky/PEFTEval}.\n", "link": "http://arxiv.org/abs/2505.22355v1", "date": "2025-05-28", "relevancy": 2.29, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4636}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4575}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Look%20Within%20or%20Look%20Beyond%3F%20A%20Theoretical%20Comparison%20Between%0A%20%20Parameter-Efficient%20and%20Full%20Fine-Tuning&body=Title%3A%20Look%20Within%20or%20Look%20Beyond%3F%20A%20Theoretical%20Comparison%20Between%0A%20%20Parameter-Efficient%20and%20Full%20Fine-Tuning%0AAuthor%3A%20Yongkang%20Liu%20and%20Xingle%20Xu%20and%20Ercong%20Nie%20and%20Zijing%20Wang%20and%20Shi%20Feng%20and%20Daling%20Wang%20and%20Qian%20Li%20and%20Hinrich%20Sch%C3%BCtze%0AAbstract%3A%20%20%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%20achieve%20performance%20comparable%0Ato%20Full%20Fine-Tuning%20%28FFT%29%20while%20requiring%20significantly%20fewer%20computing%0Aresources%2C%20making%20it%20the%20go-to%20choice%20for%20researchers.%20We%20find%20that%20although%0APEFT%20can%20achieve%20competitive%20results%20on%20some%20benchmarks%2C%20its%20performance%20falls%0Ashort%20of%20FFT%20in%20complex%20tasks%2C%20such%20as%20reasoning%20and%20instruction-based%0Afine-tuning.%20In%20this%20paper%2C%20we%20compare%20the%20characteristics%20of%20PEFT%20and%20FFT%20in%0Aterms%20of%20representational%20capacity%20and%20robustness%20based%20on%20optimization%20theory.%0AWe%20theoretically%20demonstrate%20that%20PEFT%20is%20a%20strict%20subset%20of%20FFT.%20By%20providing%0Atheoretical%20upper%20bounds%20for%20PEFT%2C%20we%20show%20that%20the%20limited%20parameter%20space%0Aconstrains%20the%20model%27s%20representational%20ability%2C%20making%20it%20more%20susceptible%20to%0Aperturbations.%20Experiments%20on%2015%20datasets%20encompassing%20classification%2C%0Ageneration%2C%20reasoning%2C%20instruction%20fine-tuning%20tasks%20and%2011%20adversarial%20test%0Asets%20validate%20our%20theories.%20We%20hope%20that%20these%20results%20spark%20further%20research%0Abeyond%20the%20realms%20of%20well%20established%20PEFT.%20The%20source%20code%20is%20in%20the%20anonymous%0AGithub%20repository%5Cfootnote%7Bhttps%3A//github.com/misonsky/PEFTEval%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22355v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLook%2520Within%2520or%2520Look%2520Beyond%253F%2520A%2520Theoretical%2520Comparison%2520Between%250A%2520%2520Parameter-Efficient%2520and%2520Full%2520Fine-Tuning%26entry.906535625%3DYongkang%2520Liu%2520and%2520Xingle%2520Xu%2520and%2520Ercong%2520Nie%2520and%2520Zijing%2520Wang%2520and%2520Shi%2520Feng%2520and%2520Daling%2520Wang%2520and%2520Qian%2520Li%2520and%2520Hinrich%2520Sch%25C3%25BCtze%26entry.1292438233%3D%2520%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520methods%2520achieve%2520performance%2520comparable%250Ato%2520Full%2520Fine-Tuning%2520%2528FFT%2529%2520while%2520requiring%2520significantly%2520fewer%2520computing%250Aresources%252C%2520making%2520it%2520the%2520go-to%2520choice%2520for%2520researchers.%2520We%2520find%2520that%2520although%250APEFT%2520can%2520achieve%2520competitive%2520results%2520on%2520some%2520benchmarks%252C%2520its%2520performance%2520falls%250Ashort%2520of%2520FFT%2520in%2520complex%2520tasks%252C%2520such%2520as%2520reasoning%2520and%2520instruction-based%250Afine-tuning.%2520In%2520this%2520paper%252C%2520we%2520compare%2520the%2520characteristics%2520of%2520PEFT%2520and%2520FFT%2520in%250Aterms%2520of%2520representational%2520capacity%2520and%2520robustness%2520based%2520on%2520optimization%2520theory.%250AWe%2520theoretically%2520demonstrate%2520that%2520PEFT%2520is%2520a%2520strict%2520subset%2520of%2520FFT.%2520By%2520providing%250Atheoretical%2520upper%2520bounds%2520for%2520PEFT%252C%2520we%2520show%2520that%2520the%2520limited%2520parameter%2520space%250Aconstrains%2520the%2520model%2527s%2520representational%2520ability%252C%2520making%2520it%2520more%2520susceptible%2520to%250Aperturbations.%2520Experiments%2520on%252015%2520datasets%2520encompassing%2520classification%252C%250Ageneration%252C%2520reasoning%252C%2520instruction%2520fine-tuning%2520tasks%2520and%252011%2520adversarial%2520test%250Asets%2520validate%2520our%2520theories.%2520We%2520hope%2520that%2520these%2520results%2520spark%2520further%2520research%250Abeyond%2520the%2520realms%2520of%2520well%2520established%2520PEFT.%2520The%2520source%2520code%2520is%2520in%2520the%2520anonymous%250AGithub%2520repository%255Cfootnote%257Bhttps%253A//github.com/misonsky/PEFTEval%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22355v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Look%20Within%20or%20Look%20Beyond%3F%20A%20Theoretical%20Comparison%20Between%0A%20%20Parameter-Efficient%20and%20Full%20Fine-Tuning&entry.906535625=Yongkang%20Liu%20and%20Xingle%20Xu%20and%20Ercong%20Nie%20and%20Zijing%20Wang%20and%20Shi%20Feng%20and%20Daling%20Wang%20and%20Qian%20Li%20and%20Hinrich%20Sch%C3%BCtze&entry.1292438233=%20%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%20achieve%20performance%20comparable%0Ato%20Full%20Fine-Tuning%20%28FFT%29%20while%20requiring%20significantly%20fewer%20computing%0Aresources%2C%20making%20it%20the%20go-to%20choice%20for%20researchers.%20We%20find%20that%20although%0APEFT%20can%20achieve%20competitive%20results%20on%20some%20benchmarks%2C%20its%20performance%20falls%0Ashort%20of%20FFT%20in%20complex%20tasks%2C%20such%20as%20reasoning%20and%20instruction-based%0Afine-tuning.%20In%20this%20paper%2C%20we%20compare%20the%20characteristics%20of%20PEFT%20and%20FFT%20in%0Aterms%20of%20representational%20capacity%20and%20robustness%20based%20on%20optimization%20theory.%0AWe%20theoretically%20demonstrate%20that%20PEFT%20is%20a%20strict%20subset%20of%20FFT.%20By%20providing%0Atheoretical%20upper%20bounds%20for%20PEFT%2C%20we%20show%20that%20the%20limited%20parameter%20space%0Aconstrains%20the%20model%27s%20representational%20ability%2C%20making%20it%20more%20susceptible%20to%0Aperturbations.%20Experiments%20on%2015%20datasets%20encompassing%20classification%2C%0Ageneration%2C%20reasoning%2C%20instruction%20fine-tuning%20tasks%20and%2011%20adversarial%20test%0Asets%20validate%20our%20theories.%20We%20hope%20that%20these%20results%20spark%20further%20research%0Abeyond%20the%20realms%20of%20well%20established%20PEFT.%20The%20source%20code%20is%20in%20the%20anonymous%0AGithub%20repository%5Cfootnote%7Bhttps%3A//github.com/misonsky/PEFTEval%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22355v1&entry.124074799=Read"},
{"title": "Transformers Pretrained on Procedural Data Contain Modular Structures\n  for Algorithmic Reasoning", "author": "Zachary Shinnick and Liangze Jiang and Hemanth Saratchandran and Anton van den Hengel and Damien Teney", "abstract": "  Pretraining on large, semantically rich datasets is key for developing\nlanguage models. Surprisingly, recent studies have shown that even synthetic\ndata, generated procedurally through simple semantic-free algorithms, can yield\nsome of the same benefits as natural language pretraining. It is unclear what\nspecific capabilities such simple synthetic data instils in a model, where\nthese capabilities reside in the architecture, and how they manifest within its\nweights. In this short paper, we identify several beneficial forms of\nprocedural data, together with specific algorithmic reasoning skills that\nimprove in small transformers. Our core finding is that different procedural\nrules instil distinct but complementary inductive structures in the model. With\nextensive ablations and partial-transfer experiments, we discover that these\nstructures reside in different parts of the model. Attention layers often carry\nthe most transferable information, but some pretraining rules impart useful\nstructure to MLP blocks instead. Most interestingly, the structures induced by\nmultiple rules can be composed to jointly reinforce multiple capabilities.\nThese results suggest an exciting possibility of disentangling the acquisition\nof knowledge from reasoning in language models, with the goal of improving\ntheir robustness and data efficiency.\n", "link": "http://arxiv.org/abs/2505.22308v1", "date": "2025-05-28", "relevancy": 2.2752, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5701}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5701}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformers%20Pretrained%20on%20Procedural%20Data%20Contain%20Modular%20Structures%0A%20%20for%20Algorithmic%20Reasoning&body=Title%3A%20Transformers%20Pretrained%20on%20Procedural%20Data%20Contain%20Modular%20Structures%0A%20%20for%20Algorithmic%20Reasoning%0AAuthor%3A%20Zachary%20Shinnick%20and%20Liangze%20Jiang%20and%20Hemanth%20Saratchandran%20and%20Anton%20van%20den%20Hengel%20and%20Damien%20Teney%0AAbstract%3A%20%20%20Pretraining%20on%20large%2C%20semantically%20rich%20datasets%20is%20key%20for%20developing%0Alanguage%20models.%20Surprisingly%2C%20recent%20studies%20have%20shown%20that%20even%20synthetic%0Adata%2C%20generated%20procedurally%20through%20simple%20semantic-free%20algorithms%2C%20can%20yield%0Asome%20of%20the%20same%20benefits%20as%20natural%20language%20pretraining.%20It%20is%20unclear%20what%0Aspecific%20capabilities%20such%20simple%20synthetic%20data%20instils%20in%20a%20model%2C%20where%0Athese%20capabilities%20reside%20in%20the%20architecture%2C%20and%20how%20they%20manifest%20within%20its%0Aweights.%20In%20this%20short%20paper%2C%20we%20identify%20several%20beneficial%20forms%20of%0Aprocedural%20data%2C%20together%20with%20specific%20algorithmic%20reasoning%20skills%20that%0Aimprove%20in%20small%20transformers.%20Our%20core%20finding%20is%20that%20different%20procedural%0Arules%20instil%20distinct%20but%20complementary%20inductive%20structures%20in%20the%20model.%20With%0Aextensive%20ablations%20and%20partial-transfer%20experiments%2C%20we%20discover%20that%20these%0Astructures%20reside%20in%20different%20parts%20of%20the%20model.%20Attention%20layers%20often%20carry%0Athe%20most%20transferable%20information%2C%20but%20some%20pretraining%20rules%20impart%20useful%0Astructure%20to%20MLP%20blocks%20instead.%20Most%20interestingly%2C%20the%20structures%20induced%20by%0Amultiple%20rules%20can%20be%20composed%20to%20jointly%20reinforce%20multiple%20capabilities.%0AThese%20results%20suggest%20an%20exciting%20possibility%20of%20disentangling%20the%20acquisition%0Aof%20knowledge%20from%20reasoning%20in%20language%20models%2C%20with%20the%20goal%20of%20improving%0Atheir%20robustness%20and%20data%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22308v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformers%2520Pretrained%2520on%2520Procedural%2520Data%2520Contain%2520Modular%2520Structures%250A%2520%2520for%2520Algorithmic%2520Reasoning%26entry.906535625%3DZachary%2520Shinnick%2520and%2520Liangze%2520Jiang%2520and%2520Hemanth%2520Saratchandran%2520and%2520Anton%2520van%2520den%2520Hengel%2520and%2520Damien%2520Teney%26entry.1292438233%3D%2520%2520Pretraining%2520on%2520large%252C%2520semantically%2520rich%2520datasets%2520is%2520key%2520for%2520developing%250Alanguage%2520models.%2520Surprisingly%252C%2520recent%2520studies%2520have%2520shown%2520that%2520even%2520synthetic%250Adata%252C%2520generated%2520procedurally%2520through%2520simple%2520semantic-free%2520algorithms%252C%2520can%2520yield%250Asome%2520of%2520the%2520same%2520benefits%2520as%2520natural%2520language%2520pretraining.%2520It%2520is%2520unclear%2520what%250Aspecific%2520capabilities%2520such%2520simple%2520synthetic%2520data%2520instils%2520in%2520a%2520model%252C%2520where%250Athese%2520capabilities%2520reside%2520in%2520the%2520architecture%252C%2520and%2520how%2520they%2520manifest%2520within%2520its%250Aweights.%2520In%2520this%2520short%2520paper%252C%2520we%2520identify%2520several%2520beneficial%2520forms%2520of%250Aprocedural%2520data%252C%2520together%2520with%2520specific%2520algorithmic%2520reasoning%2520skills%2520that%250Aimprove%2520in%2520small%2520transformers.%2520Our%2520core%2520finding%2520is%2520that%2520different%2520procedural%250Arules%2520instil%2520distinct%2520but%2520complementary%2520inductive%2520structures%2520in%2520the%2520model.%2520With%250Aextensive%2520ablations%2520and%2520partial-transfer%2520experiments%252C%2520we%2520discover%2520that%2520these%250Astructures%2520reside%2520in%2520different%2520parts%2520of%2520the%2520model.%2520Attention%2520layers%2520often%2520carry%250Athe%2520most%2520transferable%2520information%252C%2520but%2520some%2520pretraining%2520rules%2520impart%2520useful%250Astructure%2520to%2520MLP%2520blocks%2520instead.%2520Most%2520interestingly%252C%2520the%2520structures%2520induced%2520by%250Amultiple%2520rules%2520can%2520be%2520composed%2520to%2520jointly%2520reinforce%2520multiple%2520capabilities.%250AThese%2520results%2520suggest%2520an%2520exciting%2520possibility%2520of%2520disentangling%2520the%2520acquisition%250Aof%2520knowledge%2520from%2520reasoning%2520in%2520language%2520models%252C%2520with%2520the%2520goal%2520of%2520improving%250Atheir%2520robustness%2520and%2520data%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22308v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformers%20Pretrained%20on%20Procedural%20Data%20Contain%20Modular%20Structures%0A%20%20for%20Algorithmic%20Reasoning&entry.906535625=Zachary%20Shinnick%20and%20Liangze%20Jiang%20and%20Hemanth%20Saratchandran%20and%20Anton%20van%20den%20Hengel%20and%20Damien%20Teney&entry.1292438233=%20%20Pretraining%20on%20large%2C%20semantically%20rich%20datasets%20is%20key%20for%20developing%0Alanguage%20models.%20Surprisingly%2C%20recent%20studies%20have%20shown%20that%20even%20synthetic%0Adata%2C%20generated%20procedurally%20through%20simple%20semantic-free%20algorithms%2C%20can%20yield%0Asome%20of%20the%20same%20benefits%20as%20natural%20language%20pretraining.%20It%20is%20unclear%20what%0Aspecific%20capabilities%20such%20simple%20synthetic%20data%20instils%20in%20a%20model%2C%20where%0Athese%20capabilities%20reside%20in%20the%20architecture%2C%20and%20how%20they%20manifest%20within%20its%0Aweights.%20In%20this%20short%20paper%2C%20we%20identify%20several%20beneficial%20forms%20of%0Aprocedural%20data%2C%20together%20with%20specific%20algorithmic%20reasoning%20skills%20that%0Aimprove%20in%20small%20transformers.%20Our%20core%20finding%20is%20that%20different%20procedural%0Arules%20instil%20distinct%20but%20complementary%20inductive%20structures%20in%20the%20model.%20With%0Aextensive%20ablations%20and%20partial-transfer%20experiments%2C%20we%20discover%20that%20these%0Astructures%20reside%20in%20different%20parts%20of%20the%20model.%20Attention%20layers%20often%20carry%0Athe%20most%20transferable%20information%2C%20but%20some%20pretraining%20rules%20impart%20useful%0Astructure%20to%20MLP%20blocks%20instead.%20Most%20interestingly%2C%20the%20structures%20induced%20by%0Amultiple%20rules%20can%20be%20composed%20to%20jointly%20reinforce%20multiple%20capabilities.%0AThese%20results%20suggest%20an%20exciting%20possibility%20of%20disentangling%20the%20acquisition%0Aof%20knowledge%20from%20reasoning%20in%20language%20models%2C%20with%20the%20goal%20of%20improving%0Atheir%20robustness%20and%20data%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22308v1&entry.124074799=Read"},
{"title": "ChatPD: An LLM-driven Paper-Dataset Networking System", "author": "Anjie Xu and Ruiqing Ding and Leye Wang", "abstract": "  Scientific research heavily depends on suitable datasets for method\nvalidation, but existing academic platforms with dataset management like\nPapersWithCode suffer from inefficiencies in their manual workflow. To overcome\nthis bottleneck, we present a system, called ChatPD, that utilizes Large\nLanguage Models (LLMs) to automate dataset information extraction from academic\npapers and construct a structured paper-dataset network. Our system consists of\nthree key modules: \\textit{paper collection}, \\textit{dataset information\nextraction}, and \\textit{dataset entity resolution} to construct paper-dataset\nnetworks. Specifically, we propose a \\textit{Graph Completion and Inference}\nstrategy to map dataset descriptions to their corresponding entities. Through\nextensive experiments, we demonstrate that ChatPD not only outperforms the\nexisting platform PapersWithCode in dataset usage extraction but also achieves\nabout 90\\% precision and recall in entity resolution tasks. Moreover, we have\ndeployed ChatPD to continuously extract which datasets are used in papers, and\nprovide a dataset discovery service, such as task-specific dataset queries and\nsimilar dataset recommendations. We open source ChatPD and the current\npaper-dataset network on this [GitHub\nrepository]{https://github.com/ChatPD-web/ChatPD}.\n", "link": "http://arxiv.org/abs/2505.22349v1", "date": "2025-05-28", "relevancy": 2.271, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4564}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4541}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChatPD%3A%20An%20LLM-driven%20Paper-Dataset%20Networking%20System&body=Title%3A%20ChatPD%3A%20An%20LLM-driven%20Paper-Dataset%20Networking%20System%0AAuthor%3A%20Anjie%20Xu%20and%20Ruiqing%20Ding%20and%20Leye%20Wang%0AAbstract%3A%20%20%20Scientific%20research%20heavily%20depends%20on%20suitable%20datasets%20for%20method%0Avalidation%2C%20but%20existing%20academic%20platforms%20with%20dataset%20management%20like%0APapersWithCode%20suffer%20from%20inefficiencies%20in%20their%20manual%20workflow.%20To%20overcome%0Athis%20bottleneck%2C%20we%20present%20a%20system%2C%20called%20ChatPD%2C%20that%20utilizes%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20automate%20dataset%20information%20extraction%20from%20academic%0Apapers%20and%20construct%20a%20structured%20paper-dataset%20network.%20Our%20system%20consists%20of%0Athree%20key%20modules%3A%20%5Ctextit%7Bpaper%20collection%7D%2C%20%5Ctextit%7Bdataset%20information%0Aextraction%7D%2C%20and%20%5Ctextit%7Bdataset%20entity%20resolution%7D%20to%20construct%20paper-dataset%0Anetworks.%20Specifically%2C%20we%20propose%20a%20%5Ctextit%7BGraph%20Completion%20and%20Inference%7D%0Astrategy%20to%20map%20dataset%20descriptions%20to%20their%20corresponding%20entities.%20Through%0Aextensive%20experiments%2C%20we%20demonstrate%20that%20ChatPD%20not%20only%20outperforms%20the%0Aexisting%20platform%20PapersWithCode%20in%20dataset%20usage%20extraction%20but%20also%20achieves%0Aabout%2090%5C%25%20precision%20and%20recall%20in%20entity%20resolution%20tasks.%20Moreover%2C%20we%20have%0Adeployed%20ChatPD%20to%20continuously%20extract%20which%20datasets%20are%20used%20in%20papers%2C%20and%0Aprovide%20a%20dataset%20discovery%20service%2C%20such%20as%20task-specific%20dataset%20queries%20and%0Asimilar%20dataset%20recommendations.%20We%20open%20source%20ChatPD%20and%20the%20current%0Apaper-dataset%20network%20on%20this%20%5BGitHub%0Arepository%5D%7Bhttps%3A//github.com/ChatPD-web/ChatPD%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22349v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChatPD%253A%2520An%2520LLM-driven%2520Paper-Dataset%2520Networking%2520System%26entry.906535625%3DAnjie%2520Xu%2520and%2520Ruiqing%2520Ding%2520and%2520Leye%2520Wang%26entry.1292438233%3D%2520%2520Scientific%2520research%2520heavily%2520depends%2520on%2520suitable%2520datasets%2520for%2520method%250Avalidation%252C%2520but%2520existing%2520academic%2520platforms%2520with%2520dataset%2520management%2520like%250APapersWithCode%2520suffer%2520from%2520inefficiencies%2520in%2520their%2520manual%2520workflow.%2520To%2520overcome%250Athis%2520bottleneck%252C%2520we%2520present%2520a%2520system%252C%2520called%2520ChatPD%252C%2520that%2520utilizes%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520to%2520automate%2520dataset%2520information%2520extraction%2520from%2520academic%250Apapers%2520and%2520construct%2520a%2520structured%2520paper-dataset%2520network.%2520Our%2520system%2520consists%2520of%250Athree%2520key%2520modules%253A%2520%255Ctextit%257Bpaper%2520collection%257D%252C%2520%255Ctextit%257Bdataset%2520information%250Aextraction%257D%252C%2520and%2520%255Ctextit%257Bdataset%2520entity%2520resolution%257D%2520to%2520construct%2520paper-dataset%250Anetworks.%2520Specifically%252C%2520we%2520propose%2520a%2520%255Ctextit%257BGraph%2520Completion%2520and%2520Inference%257D%250Astrategy%2520to%2520map%2520dataset%2520descriptions%2520to%2520their%2520corresponding%2520entities.%2520Through%250Aextensive%2520experiments%252C%2520we%2520demonstrate%2520that%2520ChatPD%2520not%2520only%2520outperforms%2520the%250Aexisting%2520platform%2520PapersWithCode%2520in%2520dataset%2520usage%2520extraction%2520but%2520also%2520achieves%250Aabout%252090%255C%2525%2520precision%2520and%2520recall%2520in%2520entity%2520resolution%2520tasks.%2520Moreover%252C%2520we%2520have%250Adeployed%2520ChatPD%2520to%2520continuously%2520extract%2520which%2520datasets%2520are%2520used%2520in%2520papers%252C%2520and%250Aprovide%2520a%2520dataset%2520discovery%2520service%252C%2520such%2520as%2520task-specific%2520dataset%2520queries%2520and%250Asimilar%2520dataset%2520recommendations.%2520We%2520open%2520source%2520ChatPD%2520and%2520the%2520current%250Apaper-dataset%2520network%2520on%2520this%2520%255BGitHub%250Arepository%255D%257Bhttps%253A//github.com/ChatPD-web/ChatPD%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22349v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChatPD%3A%20An%20LLM-driven%20Paper-Dataset%20Networking%20System&entry.906535625=Anjie%20Xu%20and%20Ruiqing%20Ding%20and%20Leye%20Wang&entry.1292438233=%20%20Scientific%20research%20heavily%20depends%20on%20suitable%20datasets%20for%20method%0Avalidation%2C%20but%20existing%20academic%20platforms%20with%20dataset%20management%20like%0APapersWithCode%20suffer%20from%20inefficiencies%20in%20their%20manual%20workflow.%20To%20overcome%0Athis%20bottleneck%2C%20we%20present%20a%20system%2C%20called%20ChatPD%2C%20that%20utilizes%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20automate%20dataset%20information%20extraction%20from%20academic%0Apapers%20and%20construct%20a%20structured%20paper-dataset%20network.%20Our%20system%20consists%20of%0Athree%20key%20modules%3A%20%5Ctextit%7Bpaper%20collection%7D%2C%20%5Ctextit%7Bdataset%20information%0Aextraction%7D%2C%20and%20%5Ctextit%7Bdataset%20entity%20resolution%7D%20to%20construct%20paper-dataset%0Anetworks.%20Specifically%2C%20we%20propose%20a%20%5Ctextit%7BGraph%20Completion%20and%20Inference%7D%0Astrategy%20to%20map%20dataset%20descriptions%20to%20their%20corresponding%20entities.%20Through%0Aextensive%20experiments%2C%20we%20demonstrate%20that%20ChatPD%20not%20only%20outperforms%20the%0Aexisting%20platform%20PapersWithCode%20in%20dataset%20usage%20extraction%20but%20also%20achieves%0Aabout%2090%5C%25%20precision%20and%20recall%20in%20entity%20resolution%20tasks.%20Moreover%2C%20we%20have%0Adeployed%20ChatPD%20to%20continuously%20extract%20which%20datasets%20are%20used%20in%20papers%2C%20and%0Aprovide%20a%20dataset%20discovery%20service%2C%20such%20as%20task-specific%20dataset%20queries%20and%0Asimilar%20dataset%20recommendations.%20We%20open%20source%20ChatPD%20and%20the%20current%0Apaper-dataset%20network%20on%20this%20%5BGitHub%0Arepository%5D%7Bhttps%3A//github.com/ChatPD-web/ChatPD%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22349v1&entry.124074799=Read"},
{"title": "SHTOcc: Effective 3D Occupancy Prediction with Sparse Head and Tail\n  Voxels", "author": "Qiucheng Yu and Yuan Xie and Xin Tan", "abstract": "  3D occupancy prediction has attracted much attention in the field of\nautonomous driving due to its powerful geometric perception and object\nrecognition capabilities. However, existing methods have not explored the most\nessential distribution patterns of voxels, resulting in unsatisfactory results.\nThis paper first explores the inter-class distribution and geometric\ndistribution of voxels, thereby solving the long-tail problem caused by the\ninter-class distribution and the poor performance caused by the geometric\ndistribution. Specifically, this paper proposes SHTOcc (Sparse Head-Tail\nOccupancy), which uses sparse head-tail voxel construction to accurately\nidentify and balance key voxels in the head and tail classes, while using\ndecoupled learning to reduce the model's bias towards the dominant (head)\ncategory and enhance the focus on the tail class. Experiments show that\nsignificant improvements have been made on multiple baselines: SHTOcc reduces\nGPU memory usage by 42.2%, increases inference speed by 58.6%, and improves\naccuracy by about 7%, verifying its effectiveness and efficiency. The code is\navailable at https://github.com/ge95net/SHTOcc\n", "link": "http://arxiv.org/abs/2505.22461v1", "date": "2025-05-28", "relevancy": 2.2677, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5882}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5575}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SHTOcc%3A%20Effective%203D%20Occupancy%20Prediction%20with%20Sparse%20Head%20and%20Tail%0A%20%20Voxels&body=Title%3A%20SHTOcc%3A%20Effective%203D%20Occupancy%20Prediction%20with%20Sparse%20Head%20and%20Tail%0A%20%20Voxels%0AAuthor%3A%20Qiucheng%20Yu%20and%20Yuan%20Xie%20and%20Xin%20Tan%0AAbstract%3A%20%20%203D%20occupancy%20prediction%20has%20attracted%20much%20attention%20in%20the%20field%20of%0Aautonomous%20driving%20due%20to%20its%20powerful%20geometric%20perception%20and%20object%0Arecognition%20capabilities.%20However%2C%20existing%20methods%20have%20not%20explored%20the%20most%0Aessential%20distribution%20patterns%20of%20voxels%2C%20resulting%20in%20unsatisfactory%20results.%0AThis%20paper%20first%20explores%20the%20inter-class%20distribution%20and%20geometric%0Adistribution%20of%20voxels%2C%20thereby%20solving%20the%20long-tail%20problem%20caused%20by%20the%0Ainter-class%20distribution%20and%20the%20poor%20performance%20caused%20by%20the%20geometric%0Adistribution.%20Specifically%2C%20this%20paper%20proposes%20SHTOcc%20%28Sparse%20Head-Tail%0AOccupancy%29%2C%20which%20uses%20sparse%20head-tail%20voxel%20construction%20to%20accurately%0Aidentify%20and%20balance%20key%20voxels%20in%20the%20head%20and%20tail%20classes%2C%20while%20using%0Adecoupled%20learning%20to%20reduce%20the%20model%27s%20bias%20towards%20the%20dominant%20%28head%29%0Acategory%20and%20enhance%20the%20focus%20on%20the%20tail%20class.%20Experiments%20show%20that%0Asignificant%20improvements%20have%20been%20made%20on%20multiple%20baselines%3A%20SHTOcc%20reduces%0AGPU%20memory%20usage%20by%2042.2%25%2C%20increases%20inference%20speed%20by%2058.6%25%2C%20and%20improves%0Aaccuracy%20by%20about%207%25%2C%20verifying%20its%20effectiveness%20and%20efficiency.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/ge95net/SHTOcc%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22461v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSHTOcc%253A%2520Effective%25203D%2520Occupancy%2520Prediction%2520with%2520Sparse%2520Head%2520and%2520Tail%250A%2520%2520Voxels%26entry.906535625%3DQiucheng%2520Yu%2520and%2520Yuan%2520Xie%2520and%2520Xin%2520Tan%26entry.1292438233%3D%2520%25203D%2520occupancy%2520prediction%2520has%2520attracted%2520much%2520attention%2520in%2520the%2520field%2520of%250Aautonomous%2520driving%2520due%2520to%2520its%2520powerful%2520geometric%2520perception%2520and%2520object%250Arecognition%2520capabilities.%2520However%252C%2520existing%2520methods%2520have%2520not%2520explored%2520the%2520most%250Aessential%2520distribution%2520patterns%2520of%2520voxels%252C%2520resulting%2520in%2520unsatisfactory%2520results.%250AThis%2520paper%2520first%2520explores%2520the%2520inter-class%2520distribution%2520and%2520geometric%250Adistribution%2520of%2520voxels%252C%2520thereby%2520solving%2520the%2520long-tail%2520problem%2520caused%2520by%2520the%250Ainter-class%2520distribution%2520and%2520the%2520poor%2520performance%2520caused%2520by%2520the%2520geometric%250Adistribution.%2520Specifically%252C%2520this%2520paper%2520proposes%2520SHTOcc%2520%2528Sparse%2520Head-Tail%250AOccupancy%2529%252C%2520which%2520uses%2520sparse%2520head-tail%2520voxel%2520construction%2520to%2520accurately%250Aidentify%2520and%2520balance%2520key%2520voxels%2520in%2520the%2520head%2520and%2520tail%2520classes%252C%2520while%2520using%250Adecoupled%2520learning%2520to%2520reduce%2520the%2520model%2527s%2520bias%2520towards%2520the%2520dominant%2520%2528head%2529%250Acategory%2520and%2520enhance%2520the%2520focus%2520on%2520the%2520tail%2520class.%2520Experiments%2520show%2520that%250Asignificant%2520improvements%2520have%2520been%2520made%2520on%2520multiple%2520baselines%253A%2520SHTOcc%2520reduces%250AGPU%2520memory%2520usage%2520by%252042.2%2525%252C%2520increases%2520inference%2520speed%2520by%252058.6%2525%252C%2520and%2520improves%250Aaccuracy%2520by%2520about%25207%2525%252C%2520verifying%2520its%2520effectiveness%2520and%2520efficiency.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/ge95net/SHTOcc%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22461v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SHTOcc%3A%20Effective%203D%20Occupancy%20Prediction%20with%20Sparse%20Head%20and%20Tail%0A%20%20Voxels&entry.906535625=Qiucheng%20Yu%20and%20Yuan%20Xie%20and%20Xin%20Tan&entry.1292438233=%20%203D%20occupancy%20prediction%20has%20attracted%20much%20attention%20in%20the%20field%20of%0Aautonomous%20driving%20due%20to%20its%20powerful%20geometric%20perception%20and%20object%0Arecognition%20capabilities.%20However%2C%20existing%20methods%20have%20not%20explored%20the%20most%0Aessential%20distribution%20patterns%20of%20voxels%2C%20resulting%20in%20unsatisfactory%20results.%0AThis%20paper%20first%20explores%20the%20inter-class%20distribution%20and%20geometric%0Adistribution%20of%20voxels%2C%20thereby%20solving%20the%20long-tail%20problem%20caused%20by%20the%0Ainter-class%20distribution%20and%20the%20poor%20performance%20caused%20by%20the%20geometric%0Adistribution.%20Specifically%2C%20this%20paper%20proposes%20SHTOcc%20%28Sparse%20Head-Tail%0AOccupancy%29%2C%20which%20uses%20sparse%20head-tail%20voxel%20construction%20to%20accurately%0Aidentify%20and%20balance%20key%20voxels%20in%20the%20head%20and%20tail%20classes%2C%20while%20using%0Adecoupled%20learning%20to%20reduce%20the%20model%27s%20bias%20towards%20the%20dominant%20%28head%29%0Acategory%20and%20enhance%20the%20focus%20on%20the%20tail%20class.%20Experiments%20show%20that%0Asignificant%20improvements%20have%20been%20made%20on%20multiple%20baselines%3A%20SHTOcc%20reduces%0AGPU%20memory%20usage%20by%2042.2%25%2C%20increases%20inference%20speed%20by%2058.6%25%2C%20and%20improves%0Aaccuracy%20by%20about%207%25%2C%20verifying%20its%20effectiveness%20and%20efficiency.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/ge95net/SHTOcc%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22461v1&entry.124074799=Read"},
{"title": "The Meeseeks Mesh: Spatially Consistent 3D Adversarial Objects for BEV\n  Detector", "author": "Aixuan Li and Mochu Xiang and Jing Zhang and Yuchao Dai", "abstract": "  3D object detection is a critical component in autonomous driving systems. It\nallows real-time recognition and detection of vehicles, pedestrians and\nobstacles under varying environmental conditions. Among existing methods, 3D\nobject detection in the Bird's Eye View (BEV) has emerged as the mainstream\nframework. To guarantee a safe, robust and trustworthy 3D object detection, 3D\nadversarial attacks are investigated, where attacks are placed in 3D\nenvironments to evaluate the model performance, e.g., putting a film on a car,\nclothing a pedestrian. The vulnerability of 3D object detection models to 3D\nadversarial attacks serves as an important indicator to evaluate the robustness\nof the model against perturbations. To investigate this vulnerability, we\ngenerate non-invasive 3D adversarial objects tailored for real-world attack\nscenarios. Our method verifies the existence of universal adversarial objects\nthat are spatially consistent across time and camera views. Specifically, we\nemploy differentiable rendering techniques to accurately model the spatial\nrelationship between adversarial objects and the target vehicle. Furthermore,\nwe introduce an occlusion-aware module to enhance visual consistency and\nrealism under different viewpoints. To maintain attack effectiveness across\nmultiple frames, we design a BEV spatial feature-guided optimization strategy.\nExperimental results demonstrate that our approach can reliably suppress\nvehicle predictions from state-of-the-art 3D object detectors, serving as an\nimportant tool to test robustness of 3D object detection models before\ndeployment. Moreover, the generated adversarial objects exhibit strong\ngeneralization capabilities, retaining its effectiveness at various positions\nand distances in the scene.\n", "link": "http://arxiv.org/abs/2505.22499v1", "date": "2025-05-28", "relevancy": 2.2582, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5969}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5614}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Meeseeks%20Mesh%3A%20Spatially%20Consistent%203D%20Adversarial%20Objects%20for%20BEV%0A%20%20Detector&body=Title%3A%20The%20Meeseeks%20Mesh%3A%20Spatially%20Consistent%203D%20Adversarial%20Objects%20for%20BEV%0A%20%20Detector%0AAuthor%3A%20Aixuan%20Li%20and%20Mochu%20Xiang%20and%20Jing%20Zhang%20and%20Yuchao%20Dai%0AAbstract%3A%20%20%203D%20object%20detection%20is%20a%20critical%20component%20in%20autonomous%20driving%20systems.%20It%0Aallows%20real-time%20recognition%20and%20detection%20of%20vehicles%2C%20pedestrians%20and%0Aobstacles%20under%20varying%20environmental%20conditions.%20Among%20existing%20methods%2C%203D%0Aobject%20detection%20in%20the%20Bird%27s%20Eye%20View%20%28BEV%29%20has%20emerged%20as%20the%20mainstream%0Aframework.%20To%20guarantee%20a%20safe%2C%20robust%20and%20trustworthy%203D%20object%20detection%2C%203D%0Aadversarial%20attacks%20are%20investigated%2C%20where%20attacks%20are%20placed%20in%203D%0Aenvironments%20to%20evaluate%20the%20model%20performance%2C%20e.g.%2C%20putting%20a%20film%20on%20a%20car%2C%0Aclothing%20a%20pedestrian.%20The%20vulnerability%20of%203D%20object%20detection%20models%20to%203D%0Aadversarial%20attacks%20serves%20as%20an%20important%20indicator%20to%20evaluate%20the%20robustness%0Aof%20the%20model%20against%20perturbations.%20To%20investigate%20this%20vulnerability%2C%20we%0Agenerate%20non-invasive%203D%20adversarial%20objects%20tailored%20for%20real-world%20attack%0Ascenarios.%20Our%20method%20verifies%20the%20existence%20of%20universal%20adversarial%20objects%0Athat%20are%20spatially%20consistent%20across%20time%20and%20camera%20views.%20Specifically%2C%20we%0Aemploy%20differentiable%20rendering%20techniques%20to%20accurately%20model%20the%20spatial%0Arelationship%20between%20adversarial%20objects%20and%20the%20target%20vehicle.%20Furthermore%2C%0Awe%20introduce%20an%20occlusion-aware%20module%20to%20enhance%20visual%20consistency%20and%0Arealism%20under%20different%20viewpoints.%20To%20maintain%20attack%20effectiveness%20across%0Amultiple%20frames%2C%20we%20design%20a%20BEV%20spatial%20feature-guided%20optimization%20strategy.%0AExperimental%20results%20demonstrate%20that%20our%20approach%20can%20reliably%20suppress%0Avehicle%20predictions%20from%20state-of-the-art%203D%20object%20detectors%2C%20serving%20as%20an%0Aimportant%20tool%20to%20test%20robustness%20of%203D%20object%20detection%20models%20before%0Adeployment.%20Moreover%2C%20the%20generated%20adversarial%20objects%20exhibit%20strong%0Ageneralization%20capabilities%2C%20retaining%20its%20effectiveness%20at%20various%20positions%0Aand%20distances%20in%20the%20scene.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22499v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Meeseeks%2520Mesh%253A%2520Spatially%2520Consistent%25203D%2520Adversarial%2520Objects%2520for%2520BEV%250A%2520%2520Detector%26entry.906535625%3DAixuan%2520Li%2520and%2520Mochu%2520Xiang%2520and%2520Jing%2520Zhang%2520and%2520Yuchao%2520Dai%26entry.1292438233%3D%2520%25203D%2520object%2520detection%2520is%2520a%2520critical%2520component%2520in%2520autonomous%2520driving%2520systems.%2520It%250Aallows%2520real-time%2520recognition%2520and%2520detection%2520of%2520vehicles%252C%2520pedestrians%2520and%250Aobstacles%2520under%2520varying%2520environmental%2520conditions.%2520Among%2520existing%2520methods%252C%25203D%250Aobject%2520detection%2520in%2520the%2520Bird%2527s%2520Eye%2520View%2520%2528BEV%2529%2520has%2520emerged%2520as%2520the%2520mainstream%250Aframework.%2520To%2520guarantee%2520a%2520safe%252C%2520robust%2520and%2520trustworthy%25203D%2520object%2520detection%252C%25203D%250Aadversarial%2520attacks%2520are%2520investigated%252C%2520where%2520attacks%2520are%2520placed%2520in%25203D%250Aenvironments%2520to%2520evaluate%2520the%2520model%2520performance%252C%2520e.g.%252C%2520putting%2520a%2520film%2520on%2520a%2520car%252C%250Aclothing%2520a%2520pedestrian.%2520The%2520vulnerability%2520of%25203D%2520object%2520detection%2520models%2520to%25203D%250Aadversarial%2520attacks%2520serves%2520as%2520an%2520important%2520indicator%2520to%2520evaluate%2520the%2520robustness%250Aof%2520the%2520model%2520against%2520perturbations.%2520To%2520investigate%2520this%2520vulnerability%252C%2520we%250Agenerate%2520non-invasive%25203D%2520adversarial%2520objects%2520tailored%2520for%2520real-world%2520attack%250Ascenarios.%2520Our%2520method%2520verifies%2520the%2520existence%2520of%2520universal%2520adversarial%2520objects%250Athat%2520are%2520spatially%2520consistent%2520across%2520time%2520and%2520camera%2520views.%2520Specifically%252C%2520we%250Aemploy%2520differentiable%2520rendering%2520techniques%2520to%2520accurately%2520model%2520the%2520spatial%250Arelationship%2520between%2520adversarial%2520objects%2520and%2520the%2520target%2520vehicle.%2520Furthermore%252C%250Awe%2520introduce%2520an%2520occlusion-aware%2520module%2520to%2520enhance%2520visual%2520consistency%2520and%250Arealism%2520under%2520different%2520viewpoints.%2520To%2520maintain%2520attack%2520effectiveness%2520across%250Amultiple%2520frames%252C%2520we%2520design%2520a%2520BEV%2520spatial%2520feature-guided%2520optimization%2520strategy.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520approach%2520can%2520reliably%2520suppress%250Avehicle%2520predictions%2520from%2520state-of-the-art%25203D%2520object%2520detectors%252C%2520serving%2520as%2520an%250Aimportant%2520tool%2520to%2520test%2520robustness%2520of%25203D%2520object%2520detection%2520models%2520before%250Adeployment.%2520Moreover%252C%2520the%2520generated%2520adversarial%2520objects%2520exhibit%2520strong%250Ageneralization%2520capabilities%252C%2520retaining%2520its%2520effectiveness%2520at%2520various%2520positions%250Aand%2520distances%2520in%2520the%2520scene.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22499v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Meeseeks%20Mesh%3A%20Spatially%20Consistent%203D%20Adversarial%20Objects%20for%20BEV%0A%20%20Detector&entry.906535625=Aixuan%20Li%20and%20Mochu%20Xiang%20and%20Jing%20Zhang%20and%20Yuchao%20Dai&entry.1292438233=%20%203D%20object%20detection%20is%20a%20critical%20component%20in%20autonomous%20driving%20systems.%20It%0Aallows%20real-time%20recognition%20and%20detection%20of%20vehicles%2C%20pedestrians%20and%0Aobstacles%20under%20varying%20environmental%20conditions.%20Among%20existing%20methods%2C%203D%0Aobject%20detection%20in%20the%20Bird%27s%20Eye%20View%20%28BEV%29%20has%20emerged%20as%20the%20mainstream%0Aframework.%20To%20guarantee%20a%20safe%2C%20robust%20and%20trustworthy%203D%20object%20detection%2C%203D%0Aadversarial%20attacks%20are%20investigated%2C%20where%20attacks%20are%20placed%20in%203D%0Aenvironments%20to%20evaluate%20the%20model%20performance%2C%20e.g.%2C%20putting%20a%20film%20on%20a%20car%2C%0Aclothing%20a%20pedestrian.%20The%20vulnerability%20of%203D%20object%20detection%20models%20to%203D%0Aadversarial%20attacks%20serves%20as%20an%20important%20indicator%20to%20evaluate%20the%20robustness%0Aof%20the%20model%20against%20perturbations.%20To%20investigate%20this%20vulnerability%2C%20we%0Agenerate%20non-invasive%203D%20adversarial%20objects%20tailored%20for%20real-world%20attack%0Ascenarios.%20Our%20method%20verifies%20the%20existence%20of%20universal%20adversarial%20objects%0Athat%20are%20spatially%20consistent%20across%20time%20and%20camera%20views.%20Specifically%2C%20we%0Aemploy%20differentiable%20rendering%20techniques%20to%20accurately%20model%20the%20spatial%0Arelationship%20between%20adversarial%20objects%20and%20the%20target%20vehicle.%20Furthermore%2C%0Awe%20introduce%20an%20occlusion-aware%20module%20to%20enhance%20visual%20consistency%20and%0Arealism%20under%20different%20viewpoints.%20To%20maintain%20attack%20effectiveness%20across%0Amultiple%20frames%2C%20we%20design%20a%20BEV%20spatial%20feature-guided%20optimization%20strategy.%0AExperimental%20results%20demonstrate%20that%20our%20approach%20can%20reliably%20suppress%0Avehicle%20predictions%20from%20state-of-the-art%203D%20object%20detectors%2C%20serving%20as%20an%0Aimportant%20tool%20to%20test%20robustness%20of%203D%20object%20detection%20models%20before%0Adeployment.%20Moreover%2C%20the%20generated%20adversarial%20objects%20exhibit%20strong%0Ageneralization%20capabilities%2C%20retaining%20its%20effectiveness%20at%20various%20positions%0Aand%20distances%20in%20the%20scene.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22499v1&entry.124074799=Read"},
{"title": "Decoupled Subgraph Federated Learning", "author": "Javad Aliakbari and Johan \u00d6stman and Alexandre Graell i Amat", "abstract": "  We address the challenge of federated learning on graph-structured data\ndistributed across multiple clients. Specifically, we focus on the prevalent\nscenario of interconnected subgraphs, where interconnections between different\nclients play a critical role. We present a novel framework for this scenario,\nnamed FedStruct, that harnesses deep structural dependencies. To uphold\nprivacy, unlike existing methods, FedStruct eliminates the necessity of sharing\nor generating sensitive node features or embeddings among clients. Instead, it\nleverages explicit global graph structure information to capture inter-node\ndependencies. We validate the effectiveness of FedStruct through experimental\nresults conducted on six datasets for semi-supervised node classification,\nshowcasing performance close to the centralized approach across various\nscenarios, including different data partitioning methods, varying levels of\nlabel availability, and number of clients.\n", "link": "http://arxiv.org/abs/2402.19163v3", "date": "2025-05-28", "relevancy": 2.2487, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4812}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4404}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoupled%20Subgraph%20Federated%20Learning&body=Title%3A%20Decoupled%20Subgraph%20Federated%20Learning%0AAuthor%3A%20Javad%20Aliakbari%20and%20Johan%20%C3%96stman%20and%20Alexandre%20Graell%20i%20Amat%0AAbstract%3A%20%20%20We%20address%20the%20challenge%20of%20federated%20learning%20on%20graph-structured%20data%0Adistributed%20across%20multiple%20clients.%20Specifically%2C%20we%20focus%20on%20the%20prevalent%0Ascenario%20of%20interconnected%20subgraphs%2C%20where%20interconnections%20between%20different%0Aclients%20play%20a%20critical%20role.%20We%20present%20a%20novel%20framework%20for%20this%20scenario%2C%0Anamed%20FedStruct%2C%20that%20harnesses%20deep%20structural%20dependencies.%20To%20uphold%0Aprivacy%2C%20unlike%20existing%20methods%2C%20FedStruct%20eliminates%20the%20necessity%20of%20sharing%0Aor%20generating%20sensitive%20node%20features%20or%20embeddings%20among%20clients.%20Instead%2C%20it%0Aleverages%20explicit%20global%20graph%20structure%20information%20to%20capture%20inter-node%0Adependencies.%20We%20validate%20the%20effectiveness%20of%20FedStruct%20through%20experimental%0Aresults%20conducted%20on%20six%20datasets%20for%20semi-supervised%20node%20classification%2C%0Ashowcasing%20performance%20close%20to%20the%20centralized%20approach%20across%20various%0Ascenarios%2C%20including%20different%20data%20partitioning%20methods%2C%20varying%20levels%20of%0Alabel%20availability%2C%20and%20number%20of%20clients.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19163v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoupled%2520Subgraph%2520Federated%2520Learning%26entry.906535625%3DJavad%2520Aliakbari%2520and%2520Johan%2520%25C3%2596stman%2520and%2520Alexandre%2520Graell%2520i%2520Amat%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520challenge%2520of%2520federated%2520learning%2520on%2520graph-structured%2520data%250Adistributed%2520across%2520multiple%2520clients.%2520Specifically%252C%2520we%2520focus%2520on%2520the%2520prevalent%250Ascenario%2520of%2520interconnected%2520subgraphs%252C%2520where%2520interconnections%2520between%2520different%250Aclients%2520play%2520a%2520critical%2520role.%2520We%2520present%2520a%2520novel%2520framework%2520for%2520this%2520scenario%252C%250Anamed%2520FedStruct%252C%2520that%2520harnesses%2520deep%2520structural%2520dependencies.%2520To%2520uphold%250Aprivacy%252C%2520unlike%2520existing%2520methods%252C%2520FedStruct%2520eliminates%2520the%2520necessity%2520of%2520sharing%250Aor%2520generating%2520sensitive%2520node%2520features%2520or%2520embeddings%2520among%2520clients.%2520Instead%252C%2520it%250Aleverages%2520explicit%2520global%2520graph%2520structure%2520information%2520to%2520capture%2520inter-node%250Adependencies.%2520We%2520validate%2520the%2520effectiveness%2520of%2520FedStruct%2520through%2520experimental%250Aresults%2520conducted%2520on%2520six%2520datasets%2520for%2520semi-supervised%2520node%2520classification%252C%250Ashowcasing%2520performance%2520close%2520to%2520the%2520centralized%2520approach%2520across%2520various%250Ascenarios%252C%2520including%2520different%2520data%2520partitioning%2520methods%252C%2520varying%2520levels%2520of%250Alabel%2520availability%252C%2520and%2520number%2520of%2520clients.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.19163v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoupled%20Subgraph%20Federated%20Learning&entry.906535625=Javad%20Aliakbari%20and%20Johan%20%C3%96stman%20and%20Alexandre%20Graell%20i%20Amat&entry.1292438233=%20%20We%20address%20the%20challenge%20of%20federated%20learning%20on%20graph-structured%20data%0Adistributed%20across%20multiple%20clients.%20Specifically%2C%20we%20focus%20on%20the%20prevalent%0Ascenario%20of%20interconnected%20subgraphs%2C%20where%20interconnections%20between%20different%0Aclients%20play%20a%20critical%20role.%20We%20present%20a%20novel%20framework%20for%20this%20scenario%2C%0Anamed%20FedStruct%2C%20that%20harnesses%20deep%20structural%20dependencies.%20To%20uphold%0Aprivacy%2C%20unlike%20existing%20methods%2C%20FedStruct%20eliminates%20the%20necessity%20of%20sharing%0Aor%20generating%20sensitive%20node%20features%20or%20embeddings%20among%20clients.%20Instead%2C%20it%0Aleverages%20explicit%20global%20graph%20structure%20information%20to%20capture%20inter-node%0Adependencies.%20We%20validate%20the%20effectiveness%20of%20FedStruct%20through%20experimental%0Aresults%20conducted%20on%20six%20datasets%20for%20semi-supervised%20node%20classification%2C%0Ashowcasing%20performance%20close%20to%20the%20centralized%20approach%20across%20various%0Ascenarios%2C%20including%20different%20data%20partitioning%20methods%2C%20varying%20levels%20of%0Alabel%20availability%2C%20and%20number%20of%20clients.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19163v3&entry.124074799=Read"},
{"title": "Preference Adaptive and Sequential Text-to-Image Generation", "author": "Ofir Nabati and Guy Tennenholtz and ChihWei Hsu and Moonkyung Ryu and Deepak Ramachandran and Yinlam Chow and Xiang Li and Craig Boutilier", "abstract": "  We address the problem of interactive text-to-image (T2I) generation,\ndesigning a reinforcement learning (RL) agent which iteratively improves a set\nof generated images for a user through a sequence of prompt expansions. Using\nhuman raters, we create a novel dataset of sequential preferences, which we\nleverage, together with large-scale open-source (non-sequential) datasets. We\nconstruct user-preference and user-choice models using an EM strategy and\nidentify varying user preference types. We then leverage a large multimodal\nlanguage model (LMM) and a value-based RL approach to suggest an adaptive and\ndiverse slate of prompt expansions to the user. Our Preference Adaptive and\nSequential Text-to-image Agent (PASTA) extends T2I models with adaptive\nmulti-turn capabilities, fostering collaborative co-creation and addressing\nuncertainty or underspecification in a user's intent. We evaluate PASTA using\nhuman raters, showing significant improvement compared to baseline methods. We\nalso open-source our sequential rater dataset and simulated user-rater\ninteractions to support future research in user-centric multi-turn T2I systems.\n", "link": "http://arxiv.org/abs/2412.10419v2", "date": "2025-05-28", "relevancy": 2.2412, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5818}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5648}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Preference%20Adaptive%20and%20Sequential%20Text-to-Image%20Generation&body=Title%3A%20Preference%20Adaptive%20and%20Sequential%20Text-to-Image%20Generation%0AAuthor%3A%20Ofir%20Nabati%20and%20Guy%20Tennenholtz%20and%20ChihWei%20Hsu%20and%20Moonkyung%20Ryu%20and%20Deepak%20Ramachandran%20and%20Yinlam%20Chow%20and%20Xiang%20Li%20and%20Craig%20Boutilier%0AAbstract%3A%20%20%20We%20address%20the%20problem%20of%20interactive%20text-to-image%20%28T2I%29%20generation%2C%0Adesigning%20a%20reinforcement%20learning%20%28RL%29%20agent%20which%20iteratively%20improves%20a%20set%0Aof%20generated%20images%20for%20a%20user%20through%20a%20sequence%20of%20prompt%20expansions.%20Using%0Ahuman%20raters%2C%20we%20create%20a%20novel%20dataset%20of%20sequential%20preferences%2C%20which%20we%0Aleverage%2C%20together%20with%20large-scale%20open-source%20%28non-sequential%29%20datasets.%20We%0Aconstruct%20user-preference%20and%20user-choice%20models%20using%20an%20EM%20strategy%20and%0Aidentify%20varying%20user%20preference%20types.%20We%20then%20leverage%20a%20large%20multimodal%0Alanguage%20model%20%28LMM%29%20and%20a%20value-based%20RL%20approach%20to%20suggest%20an%20adaptive%20and%0Adiverse%20slate%20of%20prompt%20expansions%20to%20the%20user.%20Our%20Preference%20Adaptive%20and%0ASequential%20Text-to-image%20Agent%20%28PASTA%29%20extends%20T2I%20models%20with%20adaptive%0Amulti-turn%20capabilities%2C%20fostering%20collaborative%20co-creation%20and%20addressing%0Auncertainty%20or%20underspecification%20in%20a%20user%27s%20intent.%20We%20evaluate%20PASTA%20using%0Ahuman%20raters%2C%20showing%20significant%20improvement%20compared%20to%20baseline%20methods.%20We%0Aalso%20open-source%20our%20sequential%20rater%20dataset%20and%20simulated%20user-rater%0Ainteractions%20to%20support%20future%20research%20in%20user-centric%20multi-turn%20T2I%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10419v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreference%2520Adaptive%2520and%2520Sequential%2520Text-to-Image%2520Generation%26entry.906535625%3DOfir%2520Nabati%2520and%2520Guy%2520Tennenholtz%2520and%2520ChihWei%2520Hsu%2520and%2520Moonkyung%2520Ryu%2520and%2520Deepak%2520Ramachandran%2520and%2520Yinlam%2520Chow%2520and%2520Xiang%2520Li%2520and%2520Craig%2520Boutilier%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520problem%2520of%2520interactive%2520text-to-image%2520%2528T2I%2529%2520generation%252C%250Adesigning%2520a%2520reinforcement%2520learning%2520%2528RL%2529%2520agent%2520which%2520iteratively%2520improves%2520a%2520set%250Aof%2520generated%2520images%2520for%2520a%2520user%2520through%2520a%2520sequence%2520of%2520prompt%2520expansions.%2520Using%250Ahuman%2520raters%252C%2520we%2520create%2520a%2520novel%2520dataset%2520of%2520sequential%2520preferences%252C%2520which%2520we%250Aleverage%252C%2520together%2520with%2520large-scale%2520open-source%2520%2528non-sequential%2529%2520datasets.%2520We%250Aconstruct%2520user-preference%2520and%2520user-choice%2520models%2520using%2520an%2520EM%2520strategy%2520and%250Aidentify%2520varying%2520user%2520preference%2520types.%2520We%2520then%2520leverage%2520a%2520large%2520multimodal%250Alanguage%2520model%2520%2528LMM%2529%2520and%2520a%2520value-based%2520RL%2520approach%2520to%2520suggest%2520an%2520adaptive%2520and%250Adiverse%2520slate%2520of%2520prompt%2520expansions%2520to%2520the%2520user.%2520Our%2520Preference%2520Adaptive%2520and%250ASequential%2520Text-to-image%2520Agent%2520%2528PASTA%2529%2520extends%2520T2I%2520models%2520with%2520adaptive%250Amulti-turn%2520capabilities%252C%2520fostering%2520collaborative%2520co-creation%2520and%2520addressing%250Auncertainty%2520or%2520underspecification%2520in%2520a%2520user%2527s%2520intent.%2520We%2520evaluate%2520PASTA%2520using%250Ahuman%2520raters%252C%2520showing%2520significant%2520improvement%2520compared%2520to%2520baseline%2520methods.%2520We%250Aalso%2520open-source%2520our%2520sequential%2520rater%2520dataset%2520and%2520simulated%2520user-rater%250Ainteractions%2520to%2520support%2520future%2520research%2520in%2520user-centric%2520multi-turn%2520T2I%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10419v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preference%20Adaptive%20and%20Sequential%20Text-to-Image%20Generation&entry.906535625=Ofir%20Nabati%20and%20Guy%20Tennenholtz%20and%20ChihWei%20Hsu%20and%20Moonkyung%20Ryu%20and%20Deepak%20Ramachandran%20and%20Yinlam%20Chow%20and%20Xiang%20Li%20and%20Craig%20Boutilier&entry.1292438233=%20%20We%20address%20the%20problem%20of%20interactive%20text-to-image%20%28T2I%29%20generation%2C%0Adesigning%20a%20reinforcement%20learning%20%28RL%29%20agent%20which%20iteratively%20improves%20a%20set%0Aof%20generated%20images%20for%20a%20user%20through%20a%20sequence%20of%20prompt%20expansions.%20Using%0Ahuman%20raters%2C%20we%20create%20a%20novel%20dataset%20of%20sequential%20preferences%2C%20which%20we%0Aleverage%2C%20together%20with%20large-scale%20open-source%20%28non-sequential%29%20datasets.%20We%0Aconstruct%20user-preference%20and%20user-choice%20models%20using%20an%20EM%20strategy%20and%0Aidentify%20varying%20user%20preference%20types.%20We%20then%20leverage%20a%20large%20multimodal%0Alanguage%20model%20%28LMM%29%20and%20a%20value-based%20RL%20approach%20to%20suggest%20an%20adaptive%20and%0Adiverse%20slate%20of%20prompt%20expansions%20to%20the%20user.%20Our%20Preference%20Adaptive%20and%0ASequential%20Text-to-image%20Agent%20%28PASTA%29%20extends%20T2I%20models%20with%20adaptive%0Amulti-turn%20capabilities%2C%20fostering%20collaborative%20co-creation%20and%20addressing%0Auncertainty%20or%20underspecification%20in%20a%20user%27s%20intent.%20We%20evaluate%20PASTA%20using%0Ahuman%20raters%2C%20showing%20significant%20improvement%20compared%20to%20baseline%20methods.%20We%0Aalso%20open-source%20our%20sequential%20rater%20dataset%20and%20simulated%20user-rater%0Ainteractions%20to%20support%20future%20research%20in%20user-centric%20multi-turn%20T2I%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10419v2&entry.124074799=Read"},
{"title": "Scaling-up Perceptual Video Quality Assessment", "author": "Ziheng Jia and Zicheng Zhang and Zeyu Zhang and Yingji Liang and Xiaorong Zhu and Chunyi Li and Jinliang Han and Haoning Wu and Bin Wang and Haoran Zhang and Guanyu Zhu and Qiyong Zhao and Xiaohong Liu and Guangtao Zhai and Xiongkuo Min", "abstract": "  The data scaling law has been shown to significantly enhance the performance\nof large multi-modal models (LMMs) across various downstream tasks. However, in\nthe domain of perceptual video quality assessment (VQA), the potential of\nscaling law remains unprecedented due to the scarcity of labeled resources and\nthe insufficient scale of datasets. To address this, we propose\n\\textbf{OmniVQA}, an efficient framework designed to efficiently build\nhigh-quality, human-in-the-loop VQA multi-modal instruction databases (MIDBs).\nWe then scale up to create \\textbf{OmniVQA-Chat-400K}, the largest MIDB in the\nVQA field concurrently. Our focus is on the technical and aesthetic quality\ndimensions, with abundant in-context instruction data to provide fine-grained\nVQA knowledge. Additionally, we have built the \\textbf{OmniVQA-MOS-20K} dataset\nto enhance the model's quantitative quality rating capabilities. We then\nintroduce a \\textbf{complementary} training strategy that effectively leverages\nthe knowledge from datasets for quality understanding and quality rating tasks.\nFurthermore, we propose the \\textbf{OmniVQA-FG (fine-grain)-Benchmark} to\nevaluate the fine-grained performance of the models. Our results demonstrate\nthat our models achieve state-of-the-art performance in both quality\nunderstanding and rating tasks.\n", "link": "http://arxiv.org/abs/2505.22543v1", "date": "2025-05-28", "relevancy": 2.2382, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5648}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5648}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling-up%20Perceptual%20Video%20Quality%20Assessment&body=Title%3A%20Scaling-up%20Perceptual%20Video%20Quality%20Assessment%0AAuthor%3A%20Ziheng%20Jia%20and%20Zicheng%20Zhang%20and%20Zeyu%20Zhang%20and%20Yingji%20Liang%20and%20Xiaorong%20Zhu%20and%20Chunyi%20Li%20and%20Jinliang%20Han%20and%20Haoning%20Wu%20and%20Bin%20Wang%20and%20Haoran%20Zhang%20and%20Guanyu%20Zhu%20and%20Qiyong%20Zhao%20and%20Xiaohong%20Liu%20and%20Guangtao%20Zhai%20and%20Xiongkuo%20Min%0AAbstract%3A%20%20%20The%20data%20scaling%20law%20has%20been%20shown%20to%20significantly%20enhance%20the%20performance%0Aof%20large%20multi-modal%20models%20%28LMMs%29%20across%20various%20downstream%20tasks.%20However%2C%20in%0Athe%20domain%20of%20perceptual%20video%20quality%20assessment%20%28VQA%29%2C%20the%20potential%20of%0Ascaling%20law%20remains%20unprecedented%20due%20to%20the%20scarcity%20of%20labeled%20resources%20and%0Athe%20insufficient%20scale%20of%20datasets.%20To%20address%20this%2C%20we%20propose%0A%5Ctextbf%7BOmniVQA%7D%2C%20an%20efficient%20framework%20designed%20to%20efficiently%20build%0Ahigh-quality%2C%20human-in-the-loop%20VQA%20multi-modal%20instruction%20databases%20%28MIDBs%29.%0AWe%20then%20scale%20up%20to%20create%20%5Ctextbf%7BOmniVQA-Chat-400K%7D%2C%20the%20largest%20MIDB%20in%20the%0AVQA%20field%20concurrently.%20Our%20focus%20is%20on%20the%20technical%20and%20aesthetic%20quality%0Adimensions%2C%20with%20abundant%20in-context%20instruction%20data%20to%20provide%20fine-grained%0AVQA%20knowledge.%20Additionally%2C%20we%20have%20built%20the%20%5Ctextbf%7BOmniVQA-MOS-20K%7D%20dataset%0Ato%20enhance%20the%20model%27s%20quantitative%20quality%20rating%20capabilities.%20We%20then%0Aintroduce%20a%20%5Ctextbf%7Bcomplementary%7D%20training%20strategy%20that%20effectively%20leverages%0Athe%20knowledge%20from%20datasets%20for%20quality%20understanding%20and%20quality%20rating%20tasks.%0AFurthermore%2C%20we%20propose%20the%20%5Ctextbf%7BOmniVQA-FG%20%28fine-grain%29-Benchmark%7D%20to%0Aevaluate%20the%20fine-grained%20performance%20of%20the%20models.%20Our%20results%20demonstrate%0Athat%20our%20models%20achieve%20state-of-the-art%20performance%20in%20both%20quality%0Aunderstanding%20and%20rating%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22543v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling-up%2520Perceptual%2520Video%2520Quality%2520Assessment%26entry.906535625%3DZiheng%2520Jia%2520and%2520Zicheng%2520Zhang%2520and%2520Zeyu%2520Zhang%2520and%2520Yingji%2520Liang%2520and%2520Xiaorong%2520Zhu%2520and%2520Chunyi%2520Li%2520and%2520Jinliang%2520Han%2520and%2520Haoning%2520Wu%2520and%2520Bin%2520Wang%2520and%2520Haoran%2520Zhang%2520and%2520Guanyu%2520Zhu%2520and%2520Qiyong%2520Zhao%2520and%2520Xiaohong%2520Liu%2520and%2520Guangtao%2520Zhai%2520and%2520Xiongkuo%2520Min%26entry.1292438233%3D%2520%2520The%2520data%2520scaling%2520law%2520has%2520been%2520shown%2520to%2520significantly%2520enhance%2520the%2520performance%250Aof%2520large%2520multi-modal%2520models%2520%2528LMMs%2529%2520across%2520various%2520downstream%2520tasks.%2520However%252C%2520in%250Athe%2520domain%2520of%2520perceptual%2520video%2520quality%2520assessment%2520%2528VQA%2529%252C%2520the%2520potential%2520of%250Ascaling%2520law%2520remains%2520unprecedented%2520due%2520to%2520the%2520scarcity%2520of%2520labeled%2520resources%2520and%250Athe%2520insufficient%2520scale%2520of%2520datasets.%2520To%2520address%2520this%252C%2520we%2520propose%250A%255Ctextbf%257BOmniVQA%257D%252C%2520an%2520efficient%2520framework%2520designed%2520to%2520efficiently%2520build%250Ahigh-quality%252C%2520human-in-the-loop%2520VQA%2520multi-modal%2520instruction%2520databases%2520%2528MIDBs%2529.%250AWe%2520then%2520scale%2520up%2520to%2520create%2520%255Ctextbf%257BOmniVQA-Chat-400K%257D%252C%2520the%2520largest%2520MIDB%2520in%2520the%250AVQA%2520field%2520concurrently.%2520Our%2520focus%2520is%2520on%2520the%2520technical%2520and%2520aesthetic%2520quality%250Adimensions%252C%2520with%2520abundant%2520in-context%2520instruction%2520data%2520to%2520provide%2520fine-grained%250AVQA%2520knowledge.%2520Additionally%252C%2520we%2520have%2520built%2520the%2520%255Ctextbf%257BOmniVQA-MOS-20K%257D%2520dataset%250Ato%2520enhance%2520the%2520model%2527s%2520quantitative%2520quality%2520rating%2520capabilities.%2520We%2520then%250Aintroduce%2520a%2520%255Ctextbf%257Bcomplementary%257D%2520training%2520strategy%2520that%2520effectively%2520leverages%250Athe%2520knowledge%2520from%2520datasets%2520for%2520quality%2520understanding%2520and%2520quality%2520rating%2520tasks.%250AFurthermore%252C%2520we%2520propose%2520the%2520%255Ctextbf%257BOmniVQA-FG%2520%2528fine-grain%2529-Benchmark%257D%2520to%250Aevaluate%2520the%2520fine-grained%2520performance%2520of%2520the%2520models.%2520Our%2520results%2520demonstrate%250Athat%2520our%2520models%2520achieve%2520state-of-the-art%2520performance%2520in%2520both%2520quality%250Aunderstanding%2520and%2520rating%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22543v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling-up%20Perceptual%20Video%20Quality%20Assessment&entry.906535625=Ziheng%20Jia%20and%20Zicheng%20Zhang%20and%20Zeyu%20Zhang%20and%20Yingji%20Liang%20and%20Xiaorong%20Zhu%20and%20Chunyi%20Li%20and%20Jinliang%20Han%20and%20Haoning%20Wu%20and%20Bin%20Wang%20and%20Haoran%20Zhang%20and%20Guanyu%20Zhu%20and%20Qiyong%20Zhao%20and%20Xiaohong%20Liu%20and%20Guangtao%20Zhai%20and%20Xiongkuo%20Min&entry.1292438233=%20%20The%20data%20scaling%20law%20has%20been%20shown%20to%20significantly%20enhance%20the%20performance%0Aof%20large%20multi-modal%20models%20%28LMMs%29%20across%20various%20downstream%20tasks.%20However%2C%20in%0Athe%20domain%20of%20perceptual%20video%20quality%20assessment%20%28VQA%29%2C%20the%20potential%20of%0Ascaling%20law%20remains%20unprecedented%20due%20to%20the%20scarcity%20of%20labeled%20resources%20and%0Athe%20insufficient%20scale%20of%20datasets.%20To%20address%20this%2C%20we%20propose%0A%5Ctextbf%7BOmniVQA%7D%2C%20an%20efficient%20framework%20designed%20to%20efficiently%20build%0Ahigh-quality%2C%20human-in-the-loop%20VQA%20multi-modal%20instruction%20databases%20%28MIDBs%29.%0AWe%20then%20scale%20up%20to%20create%20%5Ctextbf%7BOmniVQA-Chat-400K%7D%2C%20the%20largest%20MIDB%20in%20the%0AVQA%20field%20concurrently.%20Our%20focus%20is%20on%20the%20technical%20and%20aesthetic%20quality%0Adimensions%2C%20with%20abundant%20in-context%20instruction%20data%20to%20provide%20fine-grained%0AVQA%20knowledge.%20Additionally%2C%20we%20have%20built%20the%20%5Ctextbf%7BOmniVQA-MOS-20K%7D%20dataset%0Ato%20enhance%20the%20model%27s%20quantitative%20quality%20rating%20capabilities.%20We%20then%0Aintroduce%20a%20%5Ctextbf%7Bcomplementary%7D%20training%20strategy%20that%20effectively%20leverages%0Athe%20knowledge%20from%20datasets%20for%20quality%20understanding%20and%20quality%20rating%20tasks.%0AFurthermore%2C%20we%20propose%20the%20%5Ctextbf%7BOmniVQA-FG%20%28fine-grain%29-Benchmark%7D%20to%0Aevaluate%20the%20fine-grained%20performance%20of%20the%20models.%20Our%20results%20demonstrate%0Athat%20our%20models%20achieve%20state-of-the-art%20performance%20in%20both%20quality%0Aunderstanding%20and%20rating%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22543v1&entry.124074799=Read"},
{"title": "AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of\n  Cross-Modal Embeddings", "author": "Yilin Ye and Junchao Huang and Xingchen Zeng and Jiazhi Xia and Wei Zeng", "abstract": "  Cross-modal embeddings form the foundation for multi-modal models. However,\nvisualization methods for interpreting cross-modal embeddings have been\nprimarily confined to traditional dimensionality reduction (DR) techniques like\nPCA and t-SNE. These DR methods primarily focus on feature distributions within\na single modality, whilst failing to incorporate metrics (e.g., CLIPScore)\nacross multiple modalities. This paper introduces AKRMap, a new DR technique\ndesigned to visualize cross-modal embeddings metric with enhanced accuracy by\nlearning kernel regression of the metric landscape in the projection space.\nSpecifically, AKRMap constructs a supervised projection network guided by a\npost-projection kernel regression loss, and employs adaptive generalized\nkernels that can be jointly optimized with the projection. This approach\nenables AKRMap to efficiently generate visualizations that capture complex\nmetric distributions, while also supporting interactive features such as zoom\nand overlay for deeper exploration. Quantitative experiments demonstrate that\nAKRMap outperforms existing DR methods in generating more accurate and\ntrustworthy visualizations. We further showcase the effectiveness of AKRMap in\nvisualizing and comparing cross-modal embeddings for text-to-image models. Code\nand demo are available at https://github.com/yilinye/AKRMap.\n", "link": "http://arxiv.org/abs/2505.14664v2", "date": "2025-05-28", "relevancy": 2.2213, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5797}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.54}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5326}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AKRMap%3A%20Adaptive%20Kernel%20Regression%20for%20Trustworthy%20Visualization%20of%0A%20%20Cross-Modal%20Embeddings&body=Title%3A%20AKRMap%3A%20Adaptive%20Kernel%20Regression%20for%20Trustworthy%20Visualization%20of%0A%20%20Cross-Modal%20Embeddings%0AAuthor%3A%20Yilin%20Ye%20and%20Junchao%20Huang%20and%20Xingchen%20Zeng%20and%20Jiazhi%20Xia%20and%20Wei%20Zeng%0AAbstract%3A%20%20%20Cross-modal%20embeddings%20form%20the%20foundation%20for%20multi-modal%20models.%20However%2C%0Avisualization%20methods%20for%20interpreting%20cross-modal%20embeddings%20have%20been%0Aprimarily%20confined%20to%20traditional%20dimensionality%20reduction%20%28DR%29%20techniques%20like%0APCA%20and%20t-SNE.%20These%20DR%20methods%20primarily%20focus%20on%20feature%20distributions%20within%0Aa%20single%20modality%2C%20whilst%20failing%20to%20incorporate%20metrics%20%28e.g.%2C%20CLIPScore%29%0Aacross%20multiple%20modalities.%20This%20paper%20introduces%20AKRMap%2C%20a%20new%20DR%20technique%0Adesigned%20to%20visualize%20cross-modal%20embeddings%20metric%20with%20enhanced%20accuracy%20by%0Alearning%20kernel%20regression%20of%20the%20metric%20landscape%20in%20the%20projection%20space.%0ASpecifically%2C%20AKRMap%20constructs%20a%20supervised%20projection%20network%20guided%20by%20a%0Apost-projection%20kernel%20regression%20loss%2C%20and%20employs%20adaptive%20generalized%0Akernels%20that%20can%20be%20jointly%20optimized%20with%20the%20projection.%20This%20approach%0Aenables%20AKRMap%20to%20efficiently%20generate%20visualizations%20that%20capture%20complex%0Ametric%20distributions%2C%20while%20also%20supporting%20interactive%20features%20such%20as%20zoom%0Aand%20overlay%20for%20deeper%20exploration.%20Quantitative%20experiments%20demonstrate%20that%0AAKRMap%20outperforms%20existing%20DR%20methods%20in%20generating%20more%20accurate%20and%0Atrustworthy%20visualizations.%20We%20further%20showcase%20the%20effectiveness%20of%20AKRMap%20in%0Avisualizing%20and%20comparing%20cross-modal%20embeddings%20for%20text-to-image%20models.%20Code%0Aand%20demo%20are%20available%20at%20https%3A//github.com/yilinye/AKRMap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14664v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAKRMap%253A%2520Adaptive%2520Kernel%2520Regression%2520for%2520Trustworthy%2520Visualization%2520of%250A%2520%2520Cross-Modal%2520Embeddings%26entry.906535625%3DYilin%2520Ye%2520and%2520Junchao%2520Huang%2520and%2520Xingchen%2520Zeng%2520and%2520Jiazhi%2520Xia%2520and%2520Wei%2520Zeng%26entry.1292438233%3D%2520%2520Cross-modal%2520embeddings%2520form%2520the%2520foundation%2520for%2520multi-modal%2520models.%2520However%252C%250Avisualization%2520methods%2520for%2520interpreting%2520cross-modal%2520embeddings%2520have%2520been%250Aprimarily%2520confined%2520to%2520traditional%2520dimensionality%2520reduction%2520%2528DR%2529%2520techniques%2520like%250APCA%2520and%2520t-SNE.%2520These%2520DR%2520methods%2520primarily%2520focus%2520on%2520feature%2520distributions%2520within%250Aa%2520single%2520modality%252C%2520whilst%2520failing%2520to%2520incorporate%2520metrics%2520%2528e.g.%252C%2520CLIPScore%2529%250Aacross%2520multiple%2520modalities.%2520This%2520paper%2520introduces%2520AKRMap%252C%2520a%2520new%2520DR%2520technique%250Adesigned%2520to%2520visualize%2520cross-modal%2520embeddings%2520metric%2520with%2520enhanced%2520accuracy%2520by%250Alearning%2520kernel%2520regression%2520of%2520the%2520metric%2520landscape%2520in%2520the%2520projection%2520space.%250ASpecifically%252C%2520AKRMap%2520constructs%2520a%2520supervised%2520projection%2520network%2520guided%2520by%2520a%250Apost-projection%2520kernel%2520regression%2520loss%252C%2520and%2520employs%2520adaptive%2520generalized%250Akernels%2520that%2520can%2520be%2520jointly%2520optimized%2520with%2520the%2520projection.%2520This%2520approach%250Aenables%2520AKRMap%2520to%2520efficiently%2520generate%2520visualizations%2520that%2520capture%2520complex%250Ametric%2520distributions%252C%2520while%2520also%2520supporting%2520interactive%2520features%2520such%2520as%2520zoom%250Aand%2520overlay%2520for%2520deeper%2520exploration.%2520Quantitative%2520experiments%2520demonstrate%2520that%250AAKRMap%2520outperforms%2520existing%2520DR%2520methods%2520in%2520generating%2520more%2520accurate%2520and%250Atrustworthy%2520visualizations.%2520We%2520further%2520showcase%2520the%2520effectiveness%2520of%2520AKRMap%2520in%250Avisualizing%2520and%2520comparing%2520cross-modal%2520embeddings%2520for%2520text-to-image%2520models.%2520Code%250Aand%2520demo%2520are%2520available%2520at%2520https%253A//github.com/yilinye/AKRMap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14664v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AKRMap%3A%20Adaptive%20Kernel%20Regression%20for%20Trustworthy%20Visualization%20of%0A%20%20Cross-Modal%20Embeddings&entry.906535625=Yilin%20Ye%20and%20Junchao%20Huang%20and%20Xingchen%20Zeng%20and%20Jiazhi%20Xia%20and%20Wei%20Zeng&entry.1292438233=%20%20Cross-modal%20embeddings%20form%20the%20foundation%20for%20multi-modal%20models.%20However%2C%0Avisualization%20methods%20for%20interpreting%20cross-modal%20embeddings%20have%20been%0Aprimarily%20confined%20to%20traditional%20dimensionality%20reduction%20%28DR%29%20techniques%20like%0APCA%20and%20t-SNE.%20These%20DR%20methods%20primarily%20focus%20on%20feature%20distributions%20within%0Aa%20single%20modality%2C%20whilst%20failing%20to%20incorporate%20metrics%20%28e.g.%2C%20CLIPScore%29%0Aacross%20multiple%20modalities.%20This%20paper%20introduces%20AKRMap%2C%20a%20new%20DR%20technique%0Adesigned%20to%20visualize%20cross-modal%20embeddings%20metric%20with%20enhanced%20accuracy%20by%0Alearning%20kernel%20regression%20of%20the%20metric%20landscape%20in%20the%20projection%20space.%0ASpecifically%2C%20AKRMap%20constructs%20a%20supervised%20projection%20network%20guided%20by%20a%0Apost-projection%20kernel%20regression%20loss%2C%20and%20employs%20adaptive%20generalized%0Akernels%20that%20can%20be%20jointly%20optimized%20with%20the%20projection.%20This%20approach%0Aenables%20AKRMap%20to%20efficiently%20generate%20visualizations%20that%20capture%20complex%0Ametric%20distributions%2C%20while%20also%20supporting%20interactive%20features%20such%20as%20zoom%0Aand%20overlay%20for%20deeper%20exploration.%20Quantitative%20experiments%20demonstrate%20that%0AAKRMap%20outperforms%20existing%20DR%20methods%20in%20generating%20more%20accurate%20and%0Atrustworthy%20visualizations.%20We%20further%20showcase%20the%20effectiveness%20of%20AKRMap%20in%0Avisualizing%20and%20comparing%20cross-modal%20embeddings%20for%20text-to-image%20models.%20Code%0Aand%20demo%20are%20available%20at%20https%3A//github.com/yilinye/AKRMap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14664v2&entry.124074799=Read"},
{"title": "VLM Can Be a Good Assistant: Enhancing Embodied Visual Tracking with\n  Self-Improving Vision-Language Models", "author": "Kui Wu and Shuhang Xu and Hao Chen and Churan Wang and Zhoujun Li and Yizhou Wang and Fangwei Zhong", "abstract": "  We introduce a novel self-improving framework that enhances Embodied Visual\nTracking (EVT) with Vision-Language Models (VLMs) to address the limitations of\ncurrent active visual tracking systems in recovering from tracking failure. Our\napproach combines the off-the-shelf active tracking methods with VLMs'\nreasoning capabilities, deploying a fast visual policy for normal tracking and\nactivating VLM reasoning only upon failure detection. The framework features a\nmemory-augmented self-reflection mechanism that enables the VLM to\nprogressively improve by learning from past experiences, effectively addressing\nVLMs' limitations in 3D spatial reasoning. Experimental results demonstrate\nsignificant performance improvements, with our framework boosting success rates\nby $72\\%$ with state-of-the-art RL-based approaches and $220\\%$ with PID-based\nmethods in challenging environments. This work represents the first integration\nof VLM-based reasoning to assist EVT agents in proactive failure recovery,\noffering substantial advances for real-world robotic applications that require\ncontinuous target monitoring in dynamic, unstructured environments. Project\nwebsite: https://sites.google.com/view/evt-recovery-assistant.\n", "link": "http://arxiv.org/abs/2505.20718v2", "date": "2025-05-28", "relevancy": 2.2061, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5672}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.561}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLM%20Can%20Be%20a%20Good%20Assistant%3A%20Enhancing%20Embodied%20Visual%20Tracking%20with%0A%20%20Self-Improving%20Vision-Language%20Models&body=Title%3A%20VLM%20Can%20Be%20a%20Good%20Assistant%3A%20Enhancing%20Embodied%20Visual%20Tracking%20with%0A%20%20Self-Improving%20Vision-Language%20Models%0AAuthor%3A%20Kui%20Wu%20and%20Shuhang%20Xu%20and%20Hao%20Chen%20and%20Churan%20Wang%20and%20Zhoujun%20Li%20and%20Yizhou%20Wang%20and%20Fangwei%20Zhong%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20self-improving%20framework%20that%20enhances%20Embodied%20Visual%0ATracking%20%28EVT%29%20with%20Vision-Language%20Models%20%28VLMs%29%20to%20address%20the%20limitations%20of%0Acurrent%20active%20visual%20tracking%20systems%20in%20recovering%20from%20tracking%20failure.%20Our%0Aapproach%20combines%20the%20off-the-shelf%20active%20tracking%20methods%20with%20VLMs%27%0Areasoning%20capabilities%2C%20deploying%20a%20fast%20visual%20policy%20for%20normal%20tracking%20and%0Aactivating%20VLM%20reasoning%20only%20upon%20failure%20detection.%20The%20framework%20features%20a%0Amemory-augmented%20self-reflection%20mechanism%20that%20enables%20the%20VLM%20to%0Aprogressively%20improve%20by%20learning%20from%20past%20experiences%2C%20effectively%20addressing%0AVLMs%27%20limitations%20in%203D%20spatial%20reasoning.%20Experimental%20results%20demonstrate%0Asignificant%20performance%20improvements%2C%20with%20our%20framework%20boosting%20success%20rates%0Aby%20%2472%5C%25%24%20with%20state-of-the-art%20RL-based%20approaches%20and%20%24220%5C%25%24%20with%20PID-based%0Amethods%20in%20challenging%20environments.%20This%20work%20represents%20the%20first%20integration%0Aof%20VLM-based%20reasoning%20to%20assist%20EVT%20agents%20in%20proactive%20failure%20recovery%2C%0Aoffering%20substantial%20advances%20for%20real-world%20robotic%20applications%20that%20require%0Acontinuous%20target%20monitoring%20in%20dynamic%2C%20unstructured%20environments.%20Project%0Awebsite%3A%20https%3A//sites.google.com/view/evt-recovery-assistant.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20718v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLM%2520Can%2520Be%2520a%2520Good%2520Assistant%253A%2520Enhancing%2520Embodied%2520Visual%2520Tracking%2520with%250A%2520%2520Self-Improving%2520Vision-Language%2520Models%26entry.906535625%3DKui%2520Wu%2520and%2520Shuhang%2520Xu%2520and%2520Hao%2520Chen%2520and%2520Churan%2520Wang%2520and%2520Zhoujun%2520Li%2520and%2520Yizhou%2520Wang%2520and%2520Fangwei%2520Zhong%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520self-improving%2520framework%2520that%2520enhances%2520Embodied%2520Visual%250ATracking%2520%2528EVT%2529%2520with%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520to%2520address%2520the%2520limitations%2520of%250Acurrent%2520active%2520visual%2520tracking%2520systems%2520in%2520recovering%2520from%2520tracking%2520failure.%2520Our%250Aapproach%2520combines%2520the%2520off-the-shelf%2520active%2520tracking%2520methods%2520with%2520VLMs%2527%250Areasoning%2520capabilities%252C%2520deploying%2520a%2520fast%2520visual%2520policy%2520for%2520normal%2520tracking%2520and%250Aactivating%2520VLM%2520reasoning%2520only%2520upon%2520failure%2520detection.%2520The%2520framework%2520features%2520a%250Amemory-augmented%2520self-reflection%2520mechanism%2520that%2520enables%2520the%2520VLM%2520to%250Aprogressively%2520improve%2520by%2520learning%2520from%2520past%2520experiences%252C%2520effectively%2520addressing%250AVLMs%2527%2520limitations%2520in%25203D%2520spatial%2520reasoning.%2520Experimental%2520results%2520demonstrate%250Asignificant%2520performance%2520improvements%252C%2520with%2520our%2520framework%2520boosting%2520success%2520rates%250Aby%2520%252472%255C%2525%2524%2520with%2520state-of-the-art%2520RL-based%2520approaches%2520and%2520%2524220%255C%2525%2524%2520with%2520PID-based%250Amethods%2520in%2520challenging%2520environments.%2520This%2520work%2520represents%2520the%2520first%2520integration%250Aof%2520VLM-based%2520reasoning%2520to%2520assist%2520EVT%2520agents%2520in%2520proactive%2520failure%2520recovery%252C%250Aoffering%2520substantial%2520advances%2520for%2520real-world%2520robotic%2520applications%2520that%2520require%250Acontinuous%2520target%2520monitoring%2520in%2520dynamic%252C%2520unstructured%2520environments.%2520Project%250Awebsite%253A%2520https%253A//sites.google.com/view/evt-recovery-assistant.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20718v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLM%20Can%20Be%20a%20Good%20Assistant%3A%20Enhancing%20Embodied%20Visual%20Tracking%20with%0A%20%20Self-Improving%20Vision-Language%20Models&entry.906535625=Kui%20Wu%20and%20Shuhang%20Xu%20and%20Hao%20Chen%20and%20Churan%20Wang%20and%20Zhoujun%20Li%20and%20Yizhou%20Wang%20and%20Fangwei%20Zhong&entry.1292438233=%20%20We%20introduce%20a%20novel%20self-improving%20framework%20that%20enhances%20Embodied%20Visual%0ATracking%20%28EVT%29%20with%20Vision-Language%20Models%20%28VLMs%29%20to%20address%20the%20limitations%20of%0Acurrent%20active%20visual%20tracking%20systems%20in%20recovering%20from%20tracking%20failure.%20Our%0Aapproach%20combines%20the%20off-the-shelf%20active%20tracking%20methods%20with%20VLMs%27%0Areasoning%20capabilities%2C%20deploying%20a%20fast%20visual%20policy%20for%20normal%20tracking%20and%0Aactivating%20VLM%20reasoning%20only%20upon%20failure%20detection.%20The%20framework%20features%20a%0Amemory-augmented%20self-reflection%20mechanism%20that%20enables%20the%20VLM%20to%0Aprogressively%20improve%20by%20learning%20from%20past%20experiences%2C%20effectively%20addressing%0AVLMs%27%20limitations%20in%203D%20spatial%20reasoning.%20Experimental%20results%20demonstrate%0Asignificant%20performance%20improvements%2C%20with%20our%20framework%20boosting%20success%20rates%0Aby%20%2472%5C%25%24%20with%20state-of-the-art%20RL-based%20approaches%20and%20%24220%5C%25%24%20with%20PID-based%0Amethods%20in%20challenging%20environments.%20This%20work%20represents%20the%20first%20integration%0Aof%20VLM-based%20reasoning%20to%20assist%20EVT%20agents%20in%20proactive%20failure%20recovery%2C%0Aoffering%20substantial%20advances%20for%20real-world%20robotic%20applications%20that%20require%0Acontinuous%20target%20monitoring%20in%20dynamic%2C%20unstructured%20environments.%20Project%0Awebsite%3A%20https%3A//sites.google.com/view/evt-recovery-assistant.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20718v2&entry.124074799=Read"},
{"title": "Assessing Quantum Advantage for Gaussian Process Regression", "author": "Dominic Lowe and M. S. Kim and Roberto Bondesan", "abstract": "  Gaussian Process Regression is a well-known machine learning technique for\nwhich several quantum algorithms have been proposed. We show here that in a\nwide range of scenarios these algorithms show no exponential speedup. We\nachieve this by rigorously proving that the condition number of a kernel matrix\nscales at least linearly with the matrix size under general assumptions on the\ndata and kernel. We additionally prove that the sparsity and Frobenius norm of\na kernel matrix scale linearly under similar assumptions. The implications for\nthe quantum algorithms runtime are independent of the complexity of loading\nclassical data on a quantum computer and also apply to dequantised algorithms.\nWe supplement our theoretical analysis with numerical verification for popular\nkernels in machine learning.\n", "link": "http://arxiv.org/abs/2505.22502v1", "date": "2025-05-28", "relevancy": 2.2059, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4494}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4393}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assessing%20Quantum%20Advantage%20for%20Gaussian%20Process%20Regression&body=Title%3A%20Assessing%20Quantum%20Advantage%20for%20Gaussian%20Process%20Regression%0AAuthor%3A%20Dominic%20Lowe%20and%20M.%20S.%20Kim%20and%20Roberto%20Bondesan%0AAbstract%3A%20%20%20Gaussian%20Process%20Regression%20is%20a%20well-known%20machine%20learning%20technique%20for%0Awhich%20several%20quantum%20algorithms%20have%20been%20proposed.%20We%20show%20here%20that%20in%20a%0Awide%20range%20of%20scenarios%20these%20algorithms%20show%20no%20exponential%20speedup.%20We%0Aachieve%20this%20by%20rigorously%20proving%20that%20the%20condition%20number%20of%20a%20kernel%20matrix%0Ascales%20at%20least%20linearly%20with%20the%20matrix%20size%20under%20general%20assumptions%20on%20the%0Adata%20and%20kernel.%20We%20additionally%20prove%20that%20the%20sparsity%20and%20Frobenius%20norm%20of%0Aa%20kernel%20matrix%20scale%20linearly%20under%20similar%20assumptions.%20The%20implications%20for%0Athe%20quantum%20algorithms%20runtime%20are%20independent%20of%20the%20complexity%20of%20loading%0Aclassical%20data%20on%20a%20quantum%20computer%20and%20also%20apply%20to%20dequantised%20algorithms.%0AWe%20supplement%20our%20theoretical%20analysis%20with%20numerical%20verification%20for%20popular%0Akernels%20in%20machine%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22502v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssessing%2520Quantum%2520Advantage%2520for%2520Gaussian%2520Process%2520Regression%26entry.906535625%3DDominic%2520Lowe%2520and%2520M.%2520S.%2520Kim%2520and%2520Roberto%2520Bondesan%26entry.1292438233%3D%2520%2520Gaussian%2520Process%2520Regression%2520is%2520a%2520well-known%2520machine%2520learning%2520technique%2520for%250Awhich%2520several%2520quantum%2520algorithms%2520have%2520been%2520proposed.%2520We%2520show%2520here%2520that%2520in%2520a%250Awide%2520range%2520of%2520scenarios%2520these%2520algorithms%2520show%2520no%2520exponential%2520speedup.%2520We%250Aachieve%2520this%2520by%2520rigorously%2520proving%2520that%2520the%2520condition%2520number%2520of%2520a%2520kernel%2520matrix%250Ascales%2520at%2520least%2520linearly%2520with%2520the%2520matrix%2520size%2520under%2520general%2520assumptions%2520on%2520the%250Adata%2520and%2520kernel.%2520We%2520additionally%2520prove%2520that%2520the%2520sparsity%2520and%2520Frobenius%2520norm%2520of%250Aa%2520kernel%2520matrix%2520scale%2520linearly%2520under%2520similar%2520assumptions.%2520The%2520implications%2520for%250Athe%2520quantum%2520algorithms%2520runtime%2520are%2520independent%2520of%2520the%2520complexity%2520of%2520loading%250Aclassical%2520data%2520on%2520a%2520quantum%2520computer%2520and%2520also%2520apply%2520to%2520dequantised%2520algorithms.%250AWe%2520supplement%2520our%2520theoretical%2520analysis%2520with%2520numerical%2520verification%2520for%2520popular%250Akernels%2520in%2520machine%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22502v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20Quantum%20Advantage%20for%20Gaussian%20Process%20Regression&entry.906535625=Dominic%20Lowe%20and%20M.%20S.%20Kim%20and%20Roberto%20Bondesan&entry.1292438233=%20%20Gaussian%20Process%20Regression%20is%20a%20well-known%20machine%20learning%20technique%20for%0Awhich%20several%20quantum%20algorithms%20have%20been%20proposed.%20We%20show%20here%20that%20in%20a%0Awide%20range%20of%20scenarios%20these%20algorithms%20show%20no%20exponential%20speedup.%20We%0Aachieve%20this%20by%20rigorously%20proving%20that%20the%20condition%20number%20of%20a%20kernel%20matrix%0Ascales%20at%20least%20linearly%20with%20the%20matrix%20size%20under%20general%20assumptions%20on%20the%0Adata%20and%20kernel.%20We%20additionally%20prove%20that%20the%20sparsity%20and%20Frobenius%20norm%20of%0Aa%20kernel%20matrix%20scale%20linearly%20under%20similar%20assumptions.%20The%20implications%20for%0Athe%20quantum%20algorithms%20runtime%20are%20independent%20of%20the%20complexity%20of%20loading%0Aclassical%20data%20on%20a%20quantum%20computer%20and%20also%20apply%20to%20dequantised%20algorithms.%0AWe%20supplement%20our%20theoretical%20analysis%20with%20numerical%20verification%20for%20popular%0Akernels%20in%20machine%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22502v1&entry.124074799=Read"},
{"title": "Smooth Sailing: Lipschitz-Driven Uncertainty Quantification for Spatial\n  Association", "author": "David R. Burt and Renato Berlinghieri and Stephen Bates and Tamara Broderick", "abstract": "  Estimating associations between spatial covariates and responses - rather\nthan merely predicting responses - is central to environmental science,\nepidemiology, and economics. For instance, public health officials might be\ninterested in whether air pollution has a strictly positive association with a\nhealth outcome, and the magnitude of any effect. Standard machine learning\nmethods often provide accurate predictions but offer limited insight into\ncovariate-response relationships. And we show that existing methods for\nconstructing confidence (or credible) intervals for associations fail to\nprovide nominal coverage in the face of model misspecification and distribution\nshift - despite both being essentially always present in spatial problems. We\nintroduce a method that constructs valid frequentist confidence intervals for\nassociations in spatial settings. Our method requires minimal assumptions\nbeyond a form of spatial smoothness. In particular, we do not require model\ncorrectness or covariate overlap between training and target locations. Our\napproach is the first to guarantee nominal coverage in this setting and\noutperforms existing techniques in both real and simulated experiments.\n", "link": "http://arxiv.org/abs/2502.06067v2", "date": "2025-05-28", "relevancy": 2.203, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5906}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5412}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Smooth%20Sailing%3A%20Lipschitz-Driven%20Uncertainty%20Quantification%20for%20Spatial%0A%20%20Association&body=Title%3A%20Smooth%20Sailing%3A%20Lipschitz-Driven%20Uncertainty%20Quantification%20for%20Spatial%0A%20%20Association%0AAuthor%3A%20David%20R.%20Burt%20and%20Renato%20Berlinghieri%20and%20Stephen%20Bates%20and%20Tamara%20Broderick%0AAbstract%3A%20%20%20Estimating%20associations%20between%20spatial%20covariates%20and%20responses%20-%20rather%0Athan%20merely%20predicting%20responses%20-%20is%20central%20to%20environmental%20science%2C%0Aepidemiology%2C%20and%20economics.%20For%20instance%2C%20public%20health%20officials%20might%20be%0Ainterested%20in%20whether%20air%20pollution%20has%20a%20strictly%20positive%20association%20with%20a%0Ahealth%20outcome%2C%20and%20the%20magnitude%20of%20any%20effect.%20Standard%20machine%20learning%0Amethods%20often%20provide%20accurate%20predictions%20but%20offer%20limited%20insight%20into%0Acovariate-response%20relationships.%20And%20we%20show%20that%20existing%20methods%20for%0Aconstructing%20confidence%20%28or%20credible%29%20intervals%20for%20associations%20fail%20to%0Aprovide%20nominal%20coverage%20in%20the%20face%20of%20model%20misspecification%20and%20distribution%0Ashift%20-%20despite%20both%20being%20essentially%20always%20present%20in%20spatial%20problems.%20We%0Aintroduce%20a%20method%20that%20constructs%20valid%20frequentist%20confidence%20intervals%20for%0Aassociations%20in%20spatial%20settings.%20Our%20method%20requires%20minimal%20assumptions%0Abeyond%20a%20form%20of%20spatial%20smoothness.%20In%20particular%2C%20we%20do%20not%20require%20model%0Acorrectness%20or%20covariate%20overlap%20between%20training%20and%20target%20locations.%20Our%0Aapproach%20is%20the%20first%20to%20guarantee%20nominal%20coverage%20in%20this%20setting%20and%0Aoutperforms%20existing%20techniques%20in%20both%20real%20and%20simulated%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06067v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmooth%2520Sailing%253A%2520Lipschitz-Driven%2520Uncertainty%2520Quantification%2520for%2520Spatial%250A%2520%2520Association%26entry.906535625%3DDavid%2520R.%2520Burt%2520and%2520Renato%2520Berlinghieri%2520and%2520Stephen%2520Bates%2520and%2520Tamara%2520Broderick%26entry.1292438233%3D%2520%2520Estimating%2520associations%2520between%2520spatial%2520covariates%2520and%2520responses%2520-%2520rather%250Athan%2520merely%2520predicting%2520responses%2520-%2520is%2520central%2520to%2520environmental%2520science%252C%250Aepidemiology%252C%2520and%2520economics.%2520For%2520instance%252C%2520public%2520health%2520officials%2520might%2520be%250Ainterested%2520in%2520whether%2520air%2520pollution%2520has%2520a%2520strictly%2520positive%2520association%2520with%2520a%250Ahealth%2520outcome%252C%2520and%2520the%2520magnitude%2520of%2520any%2520effect.%2520Standard%2520machine%2520learning%250Amethods%2520often%2520provide%2520accurate%2520predictions%2520but%2520offer%2520limited%2520insight%2520into%250Acovariate-response%2520relationships.%2520And%2520we%2520show%2520that%2520existing%2520methods%2520for%250Aconstructing%2520confidence%2520%2528or%2520credible%2529%2520intervals%2520for%2520associations%2520fail%2520to%250Aprovide%2520nominal%2520coverage%2520in%2520the%2520face%2520of%2520model%2520misspecification%2520and%2520distribution%250Ashift%2520-%2520despite%2520both%2520being%2520essentially%2520always%2520present%2520in%2520spatial%2520problems.%2520We%250Aintroduce%2520a%2520method%2520that%2520constructs%2520valid%2520frequentist%2520confidence%2520intervals%2520for%250Aassociations%2520in%2520spatial%2520settings.%2520Our%2520method%2520requires%2520minimal%2520assumptions%250Abeyond%2520a%2520form%2520of%2520spatial%2520smoothness.%2520In%2520particular%252C%2520we%2520do%2520not%2520require%2520model%250Acorrectness%2520or%2520covariate%2520overlap%2520between%2520training%2520and%2520target%2520locations.%2520Our%250Aapproach%2520is%2520the%2520first%2520to%2520guarantee%2520nominal%2520coverage%2520in%2520this%2520setting%2520and%250Aoutperforms%2520existing%2520techniques%2520in%2520both%2520real%2520and%2520simulated%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06067v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Smooth%20Sailing%3A%20Lipschitz-Driven%20Uncertainty%20Quantification%20for%20Spatial%0A%20%20Association&entry.906535625=David%20R.%20Burt%20and%20Renato%20Berlinghieri%20and%20Stephen%20Bates%20and%20Tamara%20Broderick&entry.1292438233=%20%20Estimating%20associations%20between%20spatial%20covariates%20and%20responses%20-%20rather%0Athan%20merely%20predicting%20responses%20-%20is%20central%20to%20environmental%20science%2C%0Aepidemiology%2C%20and%20economics.%20For%20instance%2C%20public%20health%20officials%20might%20be%0Ainterested%20in%20whether%20air%20pollution%20has%20a%20strictly%20positive%20association%20with%20a%0Ahealth%20outcome%2C%20and%20the%20magnitude%20of%20any%20effect.%20Standard%20machine%20learning%0Amethods%20often%20provide%20accurate%20predictions%20but%20offer%20limited%20insight%20into%0Acovariate-response%20relationships.%20And%20we%20show%20that%20existing%20methods%20for%0Aconstructing%20confidence%20%28or%20credible%29%20intervals%20for%20associations%20fail%20to%0Aprovide%20nominal%20coverage%20in%20the%20face%20of%20model%20misspecification%20and%20distribution%0Ashift%20-%20despite%20both%20being%20essentially%20always%20present%20in%20spatial%20problems.%20We%0Aintroduce%20a%20method%20that%20constructs%20valid%20frequentist%20confidence%20intervals%20for%0Aassociations%20in%20spatial%20settings.%20Our%20method%20requires%20minimal%20assumptions%0Abeyond%20a%20form%20of%20spatial%20smoothness.%20In%20particular%2C%20we%20do%20not%20require%20model%0Acorrectness%20or%20covariate%20overlap%20between%20training%20and%20target%20locations.%20Our%0Aapproach%20is%20the%20first%20to%20guarantee%20nominal%20coverage%20in%20this%20setting%20and%0Aoutperforms%20existing%20techniques%20in%20both%20real%20and%20simulated%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06067v2&entry.124074799=Read"},
{"title": "Community Detection in Networks: A Rough Sets and Consensus Clustering\n  Approach", "author": "Darian H. Grass-Boada and Leandro Gonz\u00e1lez-Montesino and Rub\u00e9n Arma\u00f1anzas", "abstract": "  The objective of this paper is to propose a framework, called Rough\nClustering-based Consensus Community Detection (RC-CCD), to effectively address\nthe challenge of identifying community structures in complex networks from a\nset of different community partitions. The method uses a consensus approach\nbased on Rough Set Theory (RST) to manage uncertainty and improve the\nreliability of community detection. The RC-CCD framework is tested on synthetic\nbenchmark networks generated by the Lancichinetti-Fortunato-Radicchi (LFR)\nmethod, which simulate varying network scales, node degrees, and community\nsizes. Key findings demonstrate that RC-CCD outperforms established algorithms\nlike Louvain, Greedy, and LPA in terms of normalized mutual information,\nshowing superior accuracy and adaptability, particularly in networks with\nhigher complexity, both in terms of size and dispersion. These results have\nsignificant implications for enhancing community detection in fields such as\nsocial and biological network analysis.\n", "link": "http://arxiv.org/abs/2406.12412v2", "date": "2025-05-28", "relevancy": 2.1956, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4396}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4396}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4383}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Community%20Detection%20in%20Networks%3A%20A%20Rough%20Sets%20and%20Consensus%20Clustering%0A%20%20Approach&body=Title%3A%20Community%20Detection%20in%20Networks%3A%20A%20Rough%20Sets%20and%20Consensus%20Clustering%0A%20%20Approach%0AAuthor%3A%20Darian%20H.%20Grass-Boada%20and%20Leandro%20Gonz%C3%A1lez-Montesino%20and%20Rub%C3%A9n%20Arma%C3%B1anzas%0AAbstract%3A%20%20%20The%20objective%20of%20this%20paper%20is%20to%20propose%20a%20framework%2C%20called%20Rough%0AClustering-based%20Consensus%20Community%20Detection%20%28RC-CCD%29%2C%20to%20effectively%20address%0Athe%20challenge%20of%20identifying%20community%20structures%20in%20complex%20networks%20from%20a%0Aset%20of%20different%20community%20partitions.%20The%20method%20uses%20a%20consensus%20approach%0Abased%20on%20Rough%20Set%20Theory%20%28RST%29%20to%20manage%20uncertainty%20and%20improve%20the%0Areliability%20of%20community%20detection.%20The%20RC-CCD%20framework%20is%20tested%20on%20synthetic%0Abenchmark%20networks%20generated%20by%20the%20Lancichinetti-Fortunato-Radicchi%20%28LFR%29%0Amethod%2C%20which%20simulate%20varying%20network%20scales%2C%20node%20degrees%2C%20and%20community%0Asizes.%20Key%20findings%20demonstrate%20that%20RC-CCD%20outperforms%20established%20algorithms%0Alike%20Louvain%2C%20Greedy%2C%20and%20LPA%20in%20terms%20of%20normalized%20mutual%20information%2C%0Ashowing%20superior%20accuracy%20and%20adaptability%2C%20particularly%20in%20networks%20with%0Ahigher%20complexity%2C%20both%20in%20terms%20of%20size%20and%20dispersion.%20These%20results%20have%0Asignificant%20implications%20for%20enhancing%20community%20detection%20in%20fields%20such%20as%0Asocial%20and%20biological%20network%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12412v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommunity%2520Detection%2520in%2520Networks%253A%2520A%2520Rough%2520Sets%2520and%2520Consensus%2520Clustering%250A%2520%2520Approach%26entry.906535625%3DDarian%2520H.%2520Grass-Boada%2520and%2520Leandro%2520Gonz%25C3%25A1lez-Montesino%2520and%2520Rub%25C3%25A9n%2520Arma%25C3%25B1anzas%26entry.1292438233%3D%2520%2520The%2520objective%2520of%2520this%2520paper%2520is%2520to%2520propose%2520a%2520framework%252C%2520called%2520Rough%250AClustering-based%2520Consensus%2520Community%2520Detection%2520%2528RC-CCD%2529%252C%2520to%2520effectively%2520address%250Athe%2520challenge%2520of%2520identifying%2520community%2520structures%2520in%2520complex%2520networks%2520from%2520a%250Aset%2520of%2520different%2520community%2520partitions.%2520The%2520method%2520uses%2520a%2520consensus%2520approach%250Abased%2520on%2520Rough%2520Set%2520Theory%2520%2528RST%2529%2520to%2520manage%2520uncertainty%2520and%2520improve%2520the%250Areliability%2520of%2520community%2520detection.%2520The%2520RC-CCD%2520framework%2520is%2520tested%2520on%2520synthetic%250Abenchmark%2520networks%2520generated%2520by%2520the%2520Lancichinetti-Fortunato-Radicchi%2520%2528LFR%2529%250Amethod%252C%2520which%2520simulate%2520varying%2520network%2520scales%252C%2520node%2520degrees%252C%2520and%2520community%250Asizes.%2520Key%2520findings%2520demonstrate%2520that%2520RC-CCD%2520outperforms%2520established%2520algorithms%250Alike%2520Louvain%252C%2520Greedy%252C%2520and%2520LPA%2520in%2520terms%2520of%2520normalized%2520mutual%2520information%252C%250Ashowing%2520superior%2520accuracy%2520and%2520adaptability%252C%2520particularly%2520in%2520networks%2520with%250Ahigher%2520complexity%252C%2520both%2520in%2520terms%2520of%2520size%2520and%2520dispersion.%2520These%2520results%2520have%250Asignificant%2520implications%2520for%2520enhancing%2520community%2520detection%2520in%2520fields%2520such%2520as%250Asocial%2520and%2520biological%2520network%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12412v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Community%20Detection%20in%20Networks%3A%20A%20Rough%20Sets%20and%20Consensus%20Clustering%0A%20%20Approach&entry.906535625=Darian%20H.%20Grass-Boada%20and%20Leandro%20Gonz%C3%A1lez-Montesino%20and%20Rub%C3%A9n%20Arma%C3%B1anzas&entry.1292438233=%20%20The%20objective%20of%20this%20paper%20is%20to%20propose%20a%20framework%2C%20called%20Rough%0AClustering-based%20Consensus%20Community%20Detection%20%28RC-CCD%29%2C%20to%20effectively%20address%0Athe%20challenge%20of%20identifying%20community%20structures%20in%20complex%20networks%20from%20a%0Aset%20of%20different%20community%20partitions.%20The%20method%20uses%20a%20consensus%20approach%0Abased%20on%20Rough%20Set%20Theory%20%28RST%29%20to%20manage%20uncertainty%20and%20improve%20the%0Areliability%20of%20community%20detection.%20The%20RC-CCD%20framework%20is%20tested%20on%20synthetic%0Abenchmark%20networks%20generated%20by%20the%20Lancichinetti-Fortunato-Radicchi%20%28LFR%29%0Amethod%2C%20which%20simulate%20varying%20network%20scales%2C%20node%20degrees%2C%20and%20community%0Asizes.%20Key%20findings%20demonstrate%20that%20RC-CCD%20outperforms%20established%20algorithms%0Alike%20Louvain%2C%20Greedy%2C%20and%20LPA%20in%20terms%20of%20normalized%20mutual%20information%2C%0Ashowing%20superior%20accuracy%20and%20adaptability%2C%20particularly%20in%20networks%20with%0Ahigher%20complexity%2C%20both%20in%20terms%20of%20size%20and%20dispersion.%20These%20results%20have%0Asignificant%20implications%20for%20enhancing%20community%20detection%20in%20fields%20such%20as%0Asocial%20and%20biological%20network%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12412v2&entry.124074799=Read"},
{"title": "MultiFormer: A Multi-Person Pose Estimation System Based on CSI and\n  Attention Mechanism", "author": "Yanyi Qu and Haoyang Ma and Wenhui Xiong", "abstract": "  Human pose estimation based on Channel State Information (CSI) has emerged as\na promising approach for non-intrusive and precise human activity monitoring,\nyet faces challenges including accurate multi-person pose recognition and\neffective CSI feature learning. This paper presents MultiFormer, a wireless\nsensing system that accurately estimates human pose through CSI. The proposed\nsystem adopts a Transformer based time-frequency dual-token feature extractor\nwith multi-head self-attention. This feature extractor is able to model\ninter-subcarrier correlations and temporal dependencies of the CSI. The\nextracted CSI features and the pose probability heatmaps are then fused by\nMulti-Stage Feature Fusion Network (MSFN) to enforce the anatomical\nconstraints. Extensive experiments conducted on on the public MM-Fi dataset and\nour self-collected dataset show that the MultiFormer achieves higher accuracy\nover state-of-the-art approaches, especially for high-mobility keypoints\n(wrists, elbows) that are particularly difficult for previous methods to\naccurately estimate.\n", "link": "http://arxiv.org/abs/2505.22555v1", "date": "2025-05-28", "relevancy": 2.1945, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5592}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5427}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiFormer%3A%20A%20Multi-Person%20Pose%20Estimation%20System%20Based%20on%20CSI%20and%0A%20%20Attention%20Mechanism&body=Title%3A%20MultiFormer%3A%20A%20Multi-Person%20Pose%20Estimation%20System%20Based%20on%20CSI%20and%0A%20%20Attention%20Mechanism%0AAuthor%3A%20Yanyi%20Qu%20and%20Haoyang%20Ma%20and%20Wenhui%20Xiong%0AAbstract%3A%20%20%20Human%20pose%20estimation%20based%20on%20Channel%20State%20Information%20%28CSI%29%20has%20emerged%20as%0Aa%20promising%20approach%20for%20non-intrusive%20and%20precise%20human%20activity%20monitoring%2C%0Ayet%20faces%20challenges%20including%20accurate%20multi-person%20pose%20recognition%20and%0Aeffective%20CSI%20feature%20learning.%20This%20paper%20presents%20MultiFormer%2C%20a%20wireless%0Asensing%20system%20that%20accurately%20estimates%20human%20pose%20through%20CSI.%20The%20proposed%0Asystem%20adopts%20a%20Transformer%20based%20time-frequency%20dual-token%20feature%20extractor%0Awith%20multi-head%20self-attention.%20This%20feature%20extractor%20is%20able%20to%20model%0Ainter-subcarrier%20correlations%20and%20temporal%20dependencies%20of%20the%20CSI.%20The%0Aextracted%20CSI%20features%20and%20the%20pose%20probability%20heatmaps%20are%20then%20fused%20by%0AMulti-Stage%20Feature%20Fusion%20Network%20%28MSFN%29%20to%20enforce%20the%20anatomical%0Aconstraints.%20Extensive%20experiments%20conducted%20on%20on%20the%20public%20MM-Fi%20dataset%20and%0Aour%20self-collected%20dataset%20show%20that%20the%20MultiFormer%20achieves%20higher%20accuracy%0Aover%20state-of-the-art%20approaches%2C%20especially%20for%20high-mobility%20keypoints%0A%28wrists%2C%20elbows%29%20that%20are%20particularly%20difficult%20for%20previous%20methods%20to%0Aaccurately%20estimate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22555v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiFormer%253A%2520A%2520Multi-Person%2520Pose%2520Estimation%2520System%2520Based%2520on%2520CSI%2520and%250A%2520%2520Attention%2520Mechanism%26entry.906535625%3DYanyi%2520Qu%2520and%2520Haoyang%2520Ma%2520and%2520Wenhui%2520Xiong%26entry.1292438233%3D%2520%2520Human%2520pose%2520estimation%2520based%2520on%2520Channel%2520State%2520Information%2520%2528CSI%2529%2520has%2520emerged%2520as%250Aa%2520promising%2520approach%2520for%2520non-intrusive%2520and%2520precise%2520human%2520activity%2520monitoring%252C%250Ayet%2520faces%2520challenges%2520including%2520accurate%2520multi-person%2520pose%2520recognition%2520and%250Aeffective%2520CSI%2520feature%2520learning.%2520This%2520paper%2520presents%2520MultiFormer%252C%2520a%2520wireless%250Asensing%2520system%2520that%2520accurately%2520estimates%2520human%2520pose%2520through%2520CSI.%2520The%2520proposed%250Asystem%2520adopts%2520a%2520Transformer%2520based%2520time-frequency%2520dual-token%2520feature%2520extractor%250Awith%2520multi-head%2520self-attention.%2520This%2520feature%2520extractor%2520is%2520able%2520to%2520model%250Ainter-subcarrier%2520correlations%2520and%2520temporal%2520dependencies%2520of%2520the%2520CSI.%2520The%250Aextracted%2520CSI%2520features%2520and%2520the%2520pose%2520probability%2520heatmaps%2520are%2520then%2520fused%2520by%250AMulti-Stage%2520Feature%2520Fusion%2520Network%2520%2528MSFN%2529%2520to%2520enforce%2520the%2520anatomical%250Aconstraints.%2520Extensive%2520experiments%2520conducted%2520on%2520on%2520the%2520public%2520MM-Fi%2520dataset%2520and%250Aour%2520self-collected%2520dataset%2520show%2520that%2520the%2520MultiFormer%2520achieves%2520higher%2520accuracy%250Aover%2520state-of-the-art%2520approaches%252C%2520especially%2520for%2520high-mobility%2520keypoints%250A%2528wrists%252C%2520elbows%2529%2520that%2520are%2520particularly%2520difficult%2520for%2520previous%2520methods%2520to%250Aaccurately%2520estimate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22555v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiFormer%3A%20A%20Multi-Person%20Pose%20Estimation%20System%20Based%20on%20CSI%20and%0A%20%20Attention%20Mechanism&entry.906535625=Yanyi%20Qu%20and%20Haoyang%20Ma%20and%20Wenhui%20Xiong&entry.1292438233=%20%20Human%20pose%20estimation%20based%20on%20Channel%20State%20Information%20%28CSI%29%20has%20emerged%20as%0Aa%20promising%20approach%20for%20non-intrusive%20and%20precise%20human%20activity%20monitoring%2C%0Ayet%20faces%20challenges%20including%20accurate%20multi-person%20pose%20recognition%20and%0Aeffective%20CSI%20feature%20learning.%20This%20paper%20presents%20MultiFormer%2C%20a%20wireless%0Asensing%20system%20that%20accurately%20estimates%20human%20pose%20through%20CSI.%20The%20proposed%0Asystem%20adopts%20a%20Transformer%20based%20time-frequency%20dual-token%20feature%20extractor%0Awith%20multi-head%20self-attention.%20This%20feature%20extractor%20is%20able%20to%20model%0Ainter-subcarrier%20correlations%20and%20temporal%20dependencies%20of%20the%20CSI.%20The%0Aextracted%20CSI%20features%20and%20the%20pose%20probability%20heatmaps%20are%20then%20fused%20by%0AMulti-Stage%20Feature%20Fusion%20Network%20%28MSFN%29%20to%20enforce%20the%20anatomical%0Aconstraints.%20Extensive%20experiments%20conducted%20on%20on%20the%20public%20MM-Fi%20dataset%20and%0Aour%20self-collected%20dataset%20show%20that%20the%20MultiFormer%20achieves%20higher%20accuracy%0Aover%20state-of-the-art%20approaches%2C%20especially%20for%20high-mobility%20keypoints%0A%28wrists%2C%20elbows%29%20that%20are%20particularly%20difficult%20for%20previous%20methods%20to%0Aaccurately%20estimate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22555v1&entry.124074799=Read"},
{"title": "SAM-R1: Leveraging SAM for Reward Feedback in Multimodal Segmentation\n  via Reinforcement Learning", "author": "Jiaqi Huang and Zunnan Xu and Jun Zhou and Ting Liu and Yicheng Xiao and Mingwen Ou and Bowen Ji and Xiu Li and Kehong Yuan", "abstract": "  Leveraging multimodal large models for image segmentation has become a\nprominent research direction. However, existing approaches typically rely\nheavily on manually annotated datasets that include explicit reasoning\nprocesses, which are costly and time-consuming to produce. Recent advances\nsuggest that reinforcement learning (RL) can endow large models with reasoning\ncapabilities without requiring such reasoning-annotated data. In this paper, we\npropose SAM-R1, a novel framework that enables multimodal large models to\nperform fine-grained reasoning in image understanding tasks. Our approach is\nthe first to incorporate fine-grained segmentation settings during the training\nof multimodal reasoning models. By integrating task-specific, fine-grained\nrewards with a tailored optimization objective, we further enhance the model's\nreasoning and segmentation alignment. We also leverage the Segment Anything\nModel (SAM) as a strong and flexible reward provider to guide the learning\nprocess. With only 3k training samples, SAM-R1 achieves strong performance\nacross multiple benchmarks, demonstrating the effectiveness of reinforcement\nlearning in equipping multimodal models with segmentation-oriented reasoning\ncapabilities.\n", "link": "http://arxiv.org/abs/2505.22596v1", "date": "2025-05-28", "relevancy": 2.1727, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5536}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5525}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM-R1%3A%20Leveraging%20SAM%20for%20Reward%20Feedback%20in%20Multimodal%20Segmentation%0A%20%20via%20Reinforcement%20Learning&body=Title%3A%20SAM-R1%3A%20Leveraging%20SAM%20for%20Reward%20Feedback%20in%20Multimodal%20Segmentation%0A%20%20via%20Reinforcement%20Learning%0AAuthor%3A%20Jiaqi%20Huang%20and%20Zunnan%20Xu%20and%20Jun%20Zhou%20and%20Ting%20Liu%20and%20Yicheng%20Xiao%20and%20Mingwen%20Ou%20and%20Bowen%20Ji%20and%20Xiu%20Li%20and%20Kehong%20Yuan%0AAbstract%3A%20%20%20Leveraging%20multimodal%20large%20models%20for%20image%20segmentation%20has%20become%20a%0Aprominent%20research%20direction.%20However%2C%20existing%20approaches%20typically%20rely%0Aheavily%20on%20manually%20annotated%20datasets%20that%20include%20explicit%20reasoning%0Aprocesses%2C%20which%20are%20costly%20and%20time-consuming%20to%20produce.%20Recent%20advances%0Asuggest%20that%20reinforcement%20learning%20%28RL%29%20can%20endow%20large%20models%20with%20reasoning%0Acapabilities%20without%20requiring%20such%20reasoning-annotated%20data.%20In%20this%20paper%2C%20we%0Apropose%20SAM-R1%2C%20a%20novel%20framework%20that%20enables%20multimodal%20large%20models%20to%0Aperform%20fine-grained%20reasoning%20in%20image%20understanding%20tasks.%20Our%20approach%20is%0Athe%20first%20to%20incorporate%20fine-grained%20segmentation%20settings%20during%20the%20training%0Aof%20multimodal%20reasoning%20models.%20By%20integrating%20task-specific%2C%20fine-grained%0Arewards%20with%20a%20tailored%20optimization%20objective%2C%20we%20further%20enhance%20the%20model%27s%0Areasoning%20and%20segmentation%20alignment.%20We%20also%20leverage%20the%20Segment%20Anything%0AModel%20%28SAM%29%20as%20a%20strong%20and%20flexible%20reward%20provider%20to%20guide%20the%20learning%0Aprocess.%20With%20only%203k%20training%20samples%2C%20SAM-R1%20achieves%20strong%20performance%0Aacross%20multiple%20benchmarks%2C%20demonstrating%20the%20effectiveness%20of%20reinforcement%0Alearning%20in%20equipping%20multimodal%20models%20with%20segmentation-oriented%20reasoning%0Acapabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22596v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM-R1%253A%2520Leveraging%2520SAM%2520for%2520Reward%2520Feedback%2520in%2520Multimodal%2520Segmentation%250A%2520%2520via%2520Reinforcement%2520Learning%26entry.906535625%3DJiaqi%2520Huang%2520and%2520Zunnan%2520Xu%2520and%2520Jun%2520Zhou%2520and%2520Ting%2520Liu%2520and%2520Yicheng%2520Xiao%2520and%2520Mingwen%2520Ou%2520and%2520Bowen%2520Ji%2520and%2520Xiu%2520Li%2520and%2520Kehong%2520Yuan%26entry.1292438233%3D%2520%2520Leveraging%2520multimodal%2520large%2520models%2520for%2520image%2520segmentation%2520has%2520become%2520a%250Aprominent%2520research%2520direction.%2520However%252C%2520existing%2520approaches%2520typically%2520rely%250Aheavily%2520on%2520manually%2520annotated%2520datasets%2520that%2520include%2520explicit%2520reasoning%250Aprocesses%252C%2520which%2520are%2520costly%2520and%2520time-consuming%2520to%2520produce.%2520Recent%2520advances%250Asuggest%2520that%2520reinforcement%2520learning%2520%2528RL%2529%2520can%2520endow%2520large%2520models%2520with%2520reasoning%250Acapabilities%2520without%2520requiring%2520such%2520reasoning-annotated%2520data.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520SAM-R1%252C%2520a%2520novel%2520framework%2520that%2520enables%2520multimodal%2520large%2520models%2520to%250Aperform%2520fine-grained%2520reasoning%2520in%2520image%2520understanding%2520tasks.%2520Our%2520approach%2520is%250Athe%2520first%2520to%2520incorporate%2520fine-grained%2520segmentation%2520settings%2520during%2520the%2520training%250Aof%2520multimodal%2520reasoning%2520models.%2520By%2520integrating%2520task-specific%252C%2520fine-grained%250Arewards%2520with%2520a%2520tailored%2520optimization%2520objective%252C%2520we%2520further%2520enhance%2520the%2520model%2527s%250Areasoning%2520and%2520segmentation%2520alignment.%2520We%2520also%2520leverage%2520the%2520Segment%2520Anything%250AModel%2520%2528SAM%2529%2520as%2520a%2520strong%2520and%2520flexible%2520reward%2520provider%2520to%2520guide%2520the%2520learning%250Aprocess.%2520With%2520only%25203k%2520training%2520samples%252C%2520SAM-R1%2520achieves%2520strong%2520performance%250Aacross%2520multiple%2520benchmarks%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520reinforcement%250Alearning%2520in%2520equipping%2520multimodal%2520models%2520with%2520segmentation-oriented%2520reasoning%250Acapabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22596v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM-R1%3A%20Leveraging%20SAM%20for%20Reward%20Feedback%20in%20Multimodal%20Segmentation%0A%20%20via%20Reinforcement%20Learning&entry.906535625=Jiaqi%20Huang%20and%20Zunnan%20Xu%20and%20Jun%20Zhou%20and%20Ting%20Liu%20and%20Yicheng%20Xiao%20and%20Mingwen%20Ou%20and%20Bowen%20Ji%20and%20Xiu%20Li%20and%20Kehong%20Yuan&entry.1292438233=%20%20Leveraging%20multimodal%20large%20models%20for%20image%20segmentation%20has%20become%20a%0Aprominent%20research%20direction.%20However%2C%20existing%20approaches%20typically%20rely%0Aheavily%20on%20manually%20annotated%20datasets%20that%20include%20explicit%20reasoning%0Aprocesses%2C%20which%20are%20costly%20and%20time-consuming%20to%20produce.%20Recent%20advances%0Asuggest%20that%20reinforcement%20learning%20%28RL%29%20can%20endow%20large%20models%20with%20reasoning%0Acapabilities%20without%20requiring%20such%20reasoning-annotated%20data.%20In%20this%20paper%2C%20we%0Apropose%20SAM-R1%2C%20a%20novel%20framework%20that%20enables%20multimodal%20large%20models%20to%0Aperform%20fine-grained%20reasoning%20in%20image%20understanding%20tasks.%20Our%20approach%20is%0Athe%20first%20to%20incorporate%20fine-grained%20segmentation%20settings%20during%20the%20training%0Aof%20multimodal%20reasoning%20models.%20By%20integrating%20task-specific%2C%20fine-grained%0Arewards%20with%20a%20tailored%20optimization%20objective%2C%20we%20further%20enhance%20the%20model%27s%0Areasoning%20and%20segmentation%20alignment.%20We%20also%20leverage%20the%20Segment%20Anything%0AModel%20%28SAM%29%20as%20a%20strong%20and%20flexible%20reward%20provider%20to%20guide%20the%20learning%0Aprocess.%20With%20only%203k%20training%20samples%2C%20SAM-R1%20achieves%20strong%20performance%0Aacross%20multiple%20benchmarks%2C%20demonstrating%20the%20effectiveness%20of%20reinforcement%0Alearning%20in%20equipping%20multimodal%20models%20with%20segmentation-oriented%20reasoning%0Acapabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22596v1&entry.124074799=Read"},
{"title": "Progressive Data Dropout: An Embarrassingly Simple Approach to Faster\n  Training", "author": "Shriram M S and Xinyue Hao and Shihao Hou and Yang Lu and Laura Sevilla-Lara and Anurag Arnab and Shreyank N Gowda", "abstract": "  The success of the machine learning field has reliably depended on training\non large datasets. While effective, this trend comes at an extraordinary cost.\nThis is due to two deeply intertwined factors: the size of models and the size\nof datasets. While promising research efforts focus on reducing the size of\nmodels, the other half of the equation remains fairly mysterious. Indeed, it is\nsurprising that the standard approach to training remains to iterate over and\nover, uniformly sampling the training dataset. In this paper we explore a\nseries of alternative training paradigms that leverage insights from\nhard-data-mining and dropout, simple enough to implement and use that can\nbecome the new training standard. The proposed Progressive Data Dropout reduces\nthe number of effective epochs to as little as 12.4% of the baseline. This\nsavings actually do not come at any cost for accuracy. Surprisingly, the\nproposed method improves accuracy by up to 4.82%. Our approach requires no\nchanges to model architecture or optimizer, and can be applied across standard\ntraining pipelines, thus posing an excellent opportunity for wide adoption.\nCode can be found here: https://github.com/bazyagami/LearningWithRevision\n", "link": "http://arxiv.org/abs/2505.22342v1", "date": "2025-05-28", "relevancy": 2.1659, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.566}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5314}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Progressive%20Data%20Dropout%3A%20An%20Embarrassingly%20Simple%20Approach%20to%20Faster%0A%20%20Training&body=Title%3A%20Progressive%20Data%20Dropout%3A%20An%20Embarrassingly%20Simple%20Approach%20to%20Faster%0A%20%20Training%0AAuthor%3A%20Shriram%20M%20S%20and%20Xinyue%20Hao%20and%20Shihao%20Hou%20and%20Yang%20Lu%20and%20Laura%20Sevilla-Lara%20and%20Anurag%20Arnab%20and%20Shreyank%20N%20Gowda%0AAbstract%3A%20%20%20The%20success%20of%20the%20machine%20learning%20field%20has%20reliably%20depended%20on%20training%0Aon%20large%20datasets.%20While%20effective%2C%20this%20trend%20comes%20at%20an%20extraordinary%20cost.%0AThis%20is%20due%20to%20two%20deeply%20intertwined%20factors%3A%20the%20size%20of%20models%20and%20the%20size%0Aof%20datasets.%20While%20promising%20research%20efforts%20focus%20on%20reducing%20the%20size%20of%0Amodels%2C%20the%20other%20half%20of%20the%20equation%20remains%20fairly%20mysterious.%20Indeed%2C%20it%20is%0Asurprising%20that%20the%20standard%20approach%20to%20training%20remains%20to%20iterate%20over%20and%0Aover%2C%20uniformly%20sampling%20the%20training%20dataset.%20In%20this%20paper%20we%20explore%20a%0Aseries%20of%20alternative%20training%20paradigms%20that%20leverage%20insights%20from%0Ahard-data-mining%20and%20dropout%2C%20simple%20enough%20to%20implement%20and%20use%20that%20can%0Abecome%20the%20new%20training%20standard.%20The%20proposed%20Progressive%20Data%20Dropout%20reduces%0Athe%20number%20of%20effective%20epochs%20to%20as%20little%20as%2012.4%25%20of%20the%20baseline.%20This%0Asavings%20actually%20do%20not%20come%20at%20any%20cost%20for%20accuracy.%20Surprisingly%2C%20the%0Aproposed%20method%20improves%20accuracy%20by%20up%20to%204.82%25.%20Our%20approach%20requires%20no%0Achanges%20to%20model%20architecture%20or%20optimizer%2C%20and%20can%20be%20applied%20across%20standard%0Atraining%20pipelines%2C%20thus%20posing%20an%20excellent%20opportunity%20for%20wide%20adoption.%0ACode%20can%20be%20found%20here%3A%20https%3A//github.com/bazyagami/LearningWithRevision%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22342v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgressive%2520Data%2520Dropout%253A%2520An%2520Embarrassingly%2520Simple%2520Approach%2520to%2520Faster%250A%2520%2520Training%26entry.906535625%3DShriram%2520M%2520S%2520and%2520Xinyue%2520Hao%2520and%2520Shihao%2520Hou%2520and%2520Yang%2520Lu%2520and%2520Laura%2520Sevilla-Lara%2520and%2520Anurag%2520Arnab%2520and%2520Shreyank%2520N%2520Gowda%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520the%2520machine%2520learning%2520field%2520has%2520reliably%2520depended%2520on%2520training%250Aon%2520large%2520datasets.%2520While%2520effective%252C%2520this%2520trend%2520comes%2520at%2520an%2520extraordinary%2520cost.%250AThis%2520is%2520due%2520to%2520two%2520deeply%2520intertwined%2520factors%253A%2520the%2520size%2520of%2520models%2520and%2520the%2520size%250Aof%2520datasets.%2520While%2520promising%2520research%2520efforts%2520focus%2520on%2520reducing%2520the%2520size%2520of%250Amodels%252C%2520the%2520other%2520half%2520of%2520the%2520equation%2520remains%2520fairly%2520mysterious.%2520Indeed%252C%2520it%2520is%250Asurprising%2520that%2520the%2520standard%2520approach%2520to%2520training%2520remains%2520to%2520iterate%2520over%2520and%250Aover%252C%2520uniformly%2520sampling%2520the%2520training%2520dataset.%2520In%2520this%2520paper%2520we%2520explore%2520a%250Aseries%2520of%2520alternative%2520training%2520paradigms%2520that%2520leverage%2520insights%2520from%250Ahard-data-mining%2520and%2520dropout%252C%2520simple%2520enough%2520to%2520implement%2520and%2520use%2520that%2520can%250Abecome%2520the%2520new%2520training%2520standard.%2520The%2520proposed%2520Progressive%2520Data%2520Dropout%2520reduces%250Athe%2520number%2520of%2520effective%2520epochs%2520to%2520as%2520little%2520as%252012.4%2525%2520of%2520the%2520baseline.%2520This%250Asavings%2520actually%2520do%2520not%2520come%2520at%2520any%2520cost%2520for%2520accuracy.%2520Surprisingly%252C%2520the%250Aproposed%2520method%2520improves%2520accuracy%2520by%2520up%2520to%25204.82%2525.%2520Our%2520approach%2520requires%2520no%250Achanges%2520to%2520model%2520architecture%2520or%2520optimizer%252C%2520and%2520can%2520be%2520applied%2520across%2520standard%250Atraining%2520pipelines%252C%2520thus%2520posing%2520an%2520excellent%2520opportunity%2520for%2520wide%2520adoption.%250ACode%2520can%2520be%2520found%2520here%253A%2520https%253A//github.com/bazyagami/LearningWithRevision%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22342v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Progressive%20Data%20Dropout%3A%20An%20Embarrassingly%20Simple%20Approach%20to%20Faster%0A%20%20Training&entry.906535625=Shriram%20M%20S%20and%20Xinyue%20Hao%20and%20Shihao%20Hou%20and%20Yang%20Lu%20and%20Laura%20Sevilla-Lara%20and%20Anurag%20Arnab%20and%20Shreyank%20N%20Gowda&entry.1292438233=%20%20The%20success%20of%20the%20machine%20learning%20field%20has%20reliably%20depended%20on%20training%0Aon%20large%20datasets.%20While%20effective%2C%20this%20trend%20comes%20at%20an%20extraordinary%20cost.%0AThis%20is%20due%20to%20two%20deeply%20intertwined%20factors%3A%20the%20size%20of%20models%20and%20the%20size%0Aof%20datasets.%20While%20promising%20research%20efforts%20focus%20on%20reducing%20the%20size%20of%0Amodels%2C%20the%20other%20half%20of%20the%20equation%20remains%20fairly%20mysterious.%20Indeed%2C%20it%20is%0Asurprising%20that%20the%20standard%20approach%20to%20training%20remains%20to%20iterate%20over%20and%0Aover%2C%20uniformly%20sampling%20the%20training%20dataset.%20In%20this%20paper%20we%20explore%20a%0Aseries%20of%20alternative%20training%20paradigms%20that%20leverage%20insights%20from%0Ahard-data-mining%20and%20dropout%2C%20simple%20enough%20to%20implement%20and%20use%20that%20can%0Abecome%20the%20new%20training%20standard.%20The%20proposed%20Progressive%20Data%20Dropout%20reduces%0Athe%20number%20of%20effective%20epochs%20to%20as%20little%20as%2012.4%25%20of%20the%20baseline.%20This%0Asavings%20actually%20do%20not%20come%20at%20any%20cost%20for%20accuracy.%20Surprisingly%2C%20the%0Aproposed%20method%20improves%20accuracy%20by%20up%20to%204.82%25.%20Our%20approach%20requires%20no%0Achanges%20to%20model%20architecture%20or%20optimizer%2C%20and%20can%20be%20applied%20across%20standard%0Atraining%20pipelines%2C%20thus%20posing%20an%20excellent%20opportunity%20for%20wide%20adoption.%0ACode%20can%20be%20found%20here%3A%20https%3A//github.com/bazyagami/LearningWithRevision%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22342v1&entry.124074799=Read"},
{"title": "Frugal Incremental Generative Modeling using Variational Autoencoders", "author": "Victor Enescu and Hichem Sahbi", "abstract": "  Continual or incremental learning holds tremendous potential in deep learning\nwith different challenges including catastrophic forgetting. The advent of\npowerful foundation and generative models has propelled this paradigm even\nfurther, making it one of the most viable solution to train these models.\nHowever, one of the persisting issues lies in the increasing volume of data\nparticularly with replay-based methods. This growth introduces challenges with\nscalability since continuously expanding data becomes increasingly demanding as\nthe number of tasks grows. In this paper, we attenuate this issue by devising a\nnovel replay-free incremental learning model based on Variational Autoencoders\n(VAEs). The main contribution of this work includes (i) a novel incremental\ngenerative modelling, built upon a well designed multi-modal latent space, and\nalso (ii) an orthogonality criterion that mitigates catastrophic forgetting of\nthe learned VAEs. The proposed method considers two variants of these VAEs:\nstatic and dynamic with no (or at most a controlled) growth in the number of\nparameters. Extensive experiments show that our method is (at least) an order\nof magnitude more ``memory-frugal'' compared to the closely related works while\nachieving SOTA accuracy scores.\n", "link": "http://arxiv.org/abs/2505.22408v1", "date": "2025-05-28", "relevancy": 2.1654, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5429}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5409}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frugal%20Incremental%20Generative%20Modeling%20using%20Variational%20Autoencoders&body=Title%3A%20Frugal%20Incremental%20Generative%20Modeling%20using%20Variational%20Autoencoders%0AAuthor%3A%20Victor%20Enescu%20and%20Hichem%20Sahbi%0AAbstract%3A%20%20%20Continual%20or%20incremental%20learning%20holds%20tremendous%20potential%20in%20deep%20learning%0Awith%20different%20challenges%20including%20catastrophic%20forgetting.%20The%20advent%20of%0Apowerful%20foundation%20and%20generative%20models%20has%20propelled%20this%20paradigm%20even%0Afurther%2C%20making%20it%20one%20of%20the%20most%20viable%20solution%20to%20train%20these%20models.%0AHowever%2C%20one%20of%20the%20persisting%20issues%20lies%20in%20the%20increasing%20volume%20of%20data%0Aparticularly%20with%20replay-based%20methods.%20This%20growth%20introduces%20challenges%20with%0Ascalability%20since%20continuously%20expanding%20data%20becomes%20increasingly%20demanding%20as%0Athe%20number%20of%20tasks%20grows.%20In%20this%20paper%2C%20we%20attenuate%20this%20issue%20by%20devising%20a%0Anovel%20replay-free%20incremental%20learning%20model%20based%20on%20Variational%20Autoencoders%0A%28VAEs%29.%20The%20main%20contribution%20of%20this%20work%20includes%20%28i%29%20a%20novel%20incremental%0Agenerative%20modelling%2C%20built%20upon%20a%20well%20designed%20multi-modal%20latent%20space%2C%20and%0Aalso%20%28ii%29%20an%20orthogonality%20criterion%20that%20mitigates%20catastrophic%20forgetting%20of%0Athe%20learned%20VAEs.%20The%20proposed%20method%20considers%20two%20variants%20of%20these%20VAEs%3A%0Astatic%20and%20dynamic%20with%20no%20%28or%20at%20most%20a%20controlled%29%20growth%20in%20the%20number%20of%0Aparameters.%20Extensive%20experiments%20show%20that%20our%20method%20is%20%28at%20least%29%20an%20order%0Aof%20magnitude%20more%20%60%60memory-frugal%27%27%20compared%20to%20the%20closely%20related%20works%20while%0Aachieving%20SOTA%20accuracy%20scores.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22408v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrugal%2520Incremental%2520Generative%2520Modeling%2520using%2520Variational%2520Autoencoders%26entry.906535625%3DVictor%2520Enescu%2520and%2520Hichem%2520Sahbi%26entry.1292438233%3D%2520%2520Continual%2520or%2520incremental%2520learning%2520holds%2520tremendous%2520potential%2520in%2520deep%2520learning%250Awith%2520different%2520challenges%2520including%2520catastrophic%2520forgetting.%2520The%2520advent%2520of%250Apowerful%2520foundation%2520and%2520generative%2520models%2520has%2520propelled%2520this%2520paradigm%2520even%250Afurther%252C%2520making%2520it%2520one%2520of%2520the%2520most%2520viable%2520solution%2520to%2520train%2520these%2520models.%250AHowever%252C%2520one%2520of%2520the%2520persisting%2520issues%2520lies%2520in%2520the%2520increasing%2520volume%2520of%2520data%250Aparticularly%2520with%2520replay-based%2520methods.%2520This%2520growth%2520introduces%2520challenges%2520with%250Ascalability%2520since%2520continuously%2520expanding%2520data%2520becomes%2520increasingly%2520demanding%2520as%250Athe%2520number%2520of%2520tasks%2520grows.%2520In%2520this%2520paper%252C%2520we%2520attenuate%2520this%2520issue%2520by%2520devising%2520a%250Anovel%2520replay-free%2520incremental%2520learning%2520model%2520based%2520on%2520Variational%2520Autoencoders%250A%2528VAEs%2529.%2520The%2520main%2520contribution%2520of%2520this%2520work%2520includes%2520%2528i%2529%2520a%2520novel%2520incremental%250Agenerative%2520modelling%252C%2520built%2520upon%2520a%2520well%2520designed%2520multi-modal%2520latent%2520space%252C%2520and%250Aalso%2520%2528ii%2529%2520an%2520orthogonality%2520criterion%2520that%2520mitigates%2520catastrophic%2520forgetting%2520of%250Athe%2520learned%2520VAEs.%2520The%2520proposed%2520method%2520considers%2520two%2520variants%2520of%2520these%2520VAEs%253A%250Astatic%2520and%2520dynamic%2520with%2520no%2520%2528or%2520at%2520most%2520a%2520controlled%2529%2520growth%2520in%2520the%2520number%2520of%250Aparameters.%2520Extensive%2520experiments%2520show%2520that%2520our%2520method%2520is%2520%2528at%2520least%2529%2520an%2520order%250Aof%2520magnitude%2520more%2520%2560%2560memory-frugal%2527%2527%2520compared%2520to%2520the%2520closely%2520related%2520works%2520while%250Aachieving%2520SOTA%2520accuracy%2520scores.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22408v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frugal%20Incremental%20Generative%20Modeling%20using%20Variational%20Autoencoders&entry.906535625=Victor%20Enescu%20and%20Hichem%20Sahbi&entry.1292438233=%20%20Continual%20or%20incremental%20learning%20holds%20tremendous%20potential%20in%20deep%20learning%0Awith%20different%20challenges%20including%20catastrophic%20forgetting.%20The%20advent%20of%0Apowerful%20foundation%20and%20generative%20models%20has%20propelled%20this%20paradigm%20even%0Afurther%2C%20making%20it%20one%20of%20the%20most%20viable%20solution%20to%20train%20these%20models.%0AHowever%2C%20one%20of%20the%20persisting%20issues%20lies%20in%20the%20increasing%20volume%20of%20data%0Aparticularly%20with%20replay-based%20methods.%20This%20growth%20introduces%20challenges%20with%0Ascalability%20since%20continuously%20expanding%20data%20becomes%20increasingly%20demanding%20as%0Athe%20number%20of%20tasks%20grows.%20In%20this%20paper%2C%20we%20attenuate%20this%20issue%20by%20devising%20a%0Anovel%20replay-free%20incremental%20learning%20model%20based%20on%20Variational%20Autoencoders%0A%28VAEs%29.%20The%20main%20contribution%20of%20this%20work%20includes%20%28i%29%20a%20novel%20incremental%0Agenerative%20modelling%2C%20built%20upon%20a%20well%20designed%20multi-modal%20latent%20space%2C%20and%0Aalso%20%28ii%29%20an%20orthogonality%20criterion%20that%20mitigates%20catastrophic%20forgetting%20of%0Athe%20learned%20VAEs.%20The%20proposed%20method%20considers%20two%20variants%20of%20these%20VAEs%3A%0Astatic%20and%20dynamic%20with%20no%20%28or%20at%20most%20a%20controlled%29%20growth%20in%20the%20number%20of%0Aparameters.%20Extensive%20experiments%20show%20that%20our%20method%20is%20%28at%20least%29%20an%20order%0Aof%20magnitude%20more%20%60%60memory-frugal%27%27%20compared%20to%20the%20closely%20related%20works%20while%0Aachieving%20SOTA%20accuracy%20scores.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22408v1&entry.124074799=Read"},
{"title": "Single Domain Generalization for Alzheimer's Detection from 3D MRIs with\n  Pseudo-Morphological Augmentations and Contrastive Learning", "author": "Zobia Batool and Huseyin Ozkan and Erchan Aptoula", "abstract": "  Although Alzheimer's disease detection via MRIs has advanced significantly\nthanks to contemporary deep learning models, challenges such as class\nimbalance, protocol variations, and limited dataset diversity often hinder\ntheir generalization capacity. To address this issue, this article focuses on\nthe single domain generalization setting, where given the data of one domain, a\nmodel is designed and developed with maximal performance w.r.t. an unseen\ndomain of distinct distribution. Since brain morphology is known to play a\ncrucial role in Alzheimer's diagnosis, we propose the use of learnable\npseudo-morphological modules aimed at producing shape-aware, anatomically\nmeaningful class-specific augmentations in combination with a supervised\ncontrastive learning module to extract robust class-specific representations.\nExperiments conducted across three datasets show improved performance and\ngeneralization capacity, especially under class imbalance and imaging protocol\nvariations. The source code will be made available upon acceptance at\nhttps://github.com/zobia111/SDG-Alzheimer.\n", "link": "http://arxiv.org/abs/2505.22465v1", "date": "2025-05-28", "relevancy": 2.1652, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5786}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5252}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single%20Domain%20Generalization%20for%20Alzheimer%27s%20Detection%20from%203D%20MRIs%20with%0A%20%20Pseudo-Morphological%20Augmentations%20and%20Contrastive%20Learning&body=Title%3A%20Single%20Domain%20Generalization%20for%20Alzheimer%27s%20Detection%20from%203D%20MRIs%20with%0A%20%20Pseudo-Morphological%20Augmentations%20and%20Contrastive%20Learning%0AAuthor%3A%20Zobia%20Batool%20and%20Huseyin%20Ozkan%20and%20Erchan%20Aptoula%0AAbstract%3A%20%20%20Although%20Alzheimer%27s%20disease%20detection%20via%20MRIs%20has%20advanced%20significantly%0Athanks%20to%20contemporary%20deep%20learning%20models%2C%20challenges%20such%20as%20class%0Aimbalance%2C%20protocol%20variations%2C%20and%20limited%20dataset%20diversity%20often%20hinder%0Atheir%20generalization%20capacity.%20To%20address%20this%20issue%2C%20this%20article%20focuses%20on%0Athe%20single%20domain%20generalization%20setting%2C%20where%20given%20the%20data%20of%20one%20domain%2C%20a%0Amodel%20is%20designed%20and%20developed%20with%20maximal%20performance%20w.r.t.%20an%20unseen%0Adomain%20of%20distinct%20distribution.%20Since%20brain%20morphology%20is%20known%20to%20play%20a%0Acrucial%20role%20in%20Alzheimer%27s%20diagnosis%2C%20we%20propose%20the%20use%20of%20learnable%0Apseudo-morphological%20modules%20aimed%20at%20producing%20shape-aware%2C%20anatomically%0Ameaningful%20class-specific%20augmentations%20in%20combination%20with%20a%20supervised%0Acontrastive%20learning%20module%20to%20extract%20robust%20class-specific%20representations.%0AExperiments%20conducted%20across%20three%20datasets%20show%20improved%20performance%20and%0Ageneralization%20capacity%2C%20especially%20under%20class%20imbalance%20and%20imaging%20protocol%0Avariations.%20The%20source%20code%20will%20be%20made%20available%20upon%20acceptance%20at%0Ahttps%3A//github.com/zobia111/SDG-Alzheimer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22465v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle%2520Domain%2520Generalization%2520for%2520Alzheimer%2527s%2520Detection%2520from%25203D%2520MRIs%2520with%250A%2520%2520Pseudo-Morphological%2520Augmentations%2520and%2520Contrastive%2520Learning%26entry.906535625%3DZobia%2520Batool%2520and%2520Huseyin%2520Ozkan%2520and%2520Erchan%2520Aptoula%26entry.1292438233%3D%2520%2520Although%2520Alzheimer%2527s%2520disease%2520detection%2520via%2520MRIs%2520has%2520advanced%2520significantly%250Athanks%2520to%2520contemporary%2520deep%2520learning%2520models%252C%2520challenges%2520such%2520as%2520class%250Aimbalance%252C%2520protocol%2520variations%252C%2520and%2520limited%2520dataset%2520diversity%2520often%2520hinder%250Atheir%2520generalization%2520capacity.%2520To%2520address%2520this%2520issue%252C%2520this%2520article%2520focuses%2520on%250Athe%2520single%2520domain%2520generalization%2520setting%252C%2520where%2520given%2520the%2520data%2520of%2520one%2520domain%252C%2520a%250Amodel%2520is%2520designed%2520and%2520developed%2520with%2520maximal%2520performance%2520w.r.t.%2520an%2520unseen%250Adomain%2520of%2520distinct%2520distribution.%2520Since%2520brain%2520morphology%2520is%2520known%2520to%2520play%2520a%250Acrucial%2520role%2520in%2520Alzheimer%2527s%2520diagnosis%252C%2520we%2520propose%2520the%2520use%2520of%2520learnable%250Apseudo-morphological%2520modules%2520aimed%2520at%2520producing%2520shape-aware%252C%2520anatomically%250Ameaningful%2520class-specific%2520augmentations%2520in%2520combination%2520with%2520a%2520supervised%250Acontrastive%2520learning%2520module%2520to%2520extract%2520robust%2520class-specific%2520representations.%250AExperiments%2520conducted%2520across%2520three%2520datasets%2520show%2520improved%2520performance%2520and%250Ageneralization%2520capacity%252C%2520especially%2520under%2520class%2520imbalance%2520and%2520imaging%2520protocol%250Avariations.%2520The%2520source%2520code%2520will%2520be%2520made%2520available%2520upon%2520acceptance%2520at%250Ahttps%253A//github.com/zobia111/SDG-Alzheimer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22465v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single%20Domain%20Generalization%20for%20Alzheimer%27s%20Detection%20from%203D%20MRIs%20with%0A%20%20Pseudo-Morphological%20Augmentations%20and%20Contrastive%20Learning&entry.906535625=Zobia%20Batool%20and%20Huseyin%20Ozkan%20and%20Erchan%20Aptoula&entry.1292438233=%20%20Although%20Alzheimer%27s%20disease%20detection%20via%20MRIs%20has%20advanced%20significantly%0Athanks%20to%20contemporary%20deep%20learning%20models%2C%20challenges%20such%20as%20class%0Aimbalance%2C%20protocol%20variations%2C%20and%20limited%20dataset%20diversity%20often%20hinder%0Atheir%20generalization%20capacity.%20To%20address%20this%20issue%2C%20this%20article%20focuses%20on%0Athe%20single%20domain%20generalization%20setting%2C%20where%20given%20the%20data%20of%20one%20domain%2C%20a%0Amodel%20is%20designed%20and%20developed%20with%20maximal%20performance%20w.r.t.%20an%20unseen%0Adomain%20of%20distinct%20distribution.%20Since%20brain%20morphology%20is%20known%20to%20play%20a%0Acrucial%20role%20in%20Alzheimer%27s%20diagnosis%2C%20we%20propose%20the%20use%20of%20learnable%0Apseudo-morphological%20modules%20aimed%20at%20producing%20shape-aware%2C%20anatomically%0Ameaningful%20class-specific%20augmentations%20in%20combination%20with%20a%20supervised%0Acontrastive%20learning%20module%20to%20extract%20robust%20class-specific%20representations.%0AExperiments%20conducted%20across%20three%20datasets%20show%20improved%20performance%20and%0Ageneralization%20capacity%2C%20especially%20under%20class%20imbalance%20and%20imaging%20protocol%0Avariations.%20The%20source%20code%20will%20be%20made%20available%20upon%20acceptance%20at%0Ahttps%3A//github.com/zobia111/SDG-Alzheimer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22465v1&entry.124074799=Read"},
{"title": "Scaling Reasoning without Attention", "author": "Xueliang Zhao and Wei Wu and Lingpeng Kong", "abstract": "  Large language models (LLMs) have made significant advances in complex\nreasoning tasks, yet they remain bottlenecked by two core challenges:\narchitectural inefficiency due to reliance on Transformers, and a lack of\nstructured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an\nattention-free language model that addresses both issues through architectural\nand data-centric innovations. Built on the state space dual (SSD) layers of\nMamba-2, our model eliminates the need for self-attention and key-value\ncaching, enabling fixed-memory, constant-time inference. To train it for\ncomplex reasoning, we propose a two-phase curriculum fine-tuning strategy based\non the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically\nstructured problems via abstract concept selection and rationale-guided\ngeneration. On benchmark evaluations, \\ourmodel-7B outperforms strong\nTransformer and hybrid models of comparable scale, and even surpasses the much\nlarger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on\nLivecodebench. These results highlight the potential of state space models as\nefficient and scalable alternatives to attention-based architectures for\nhigh-capacity reasoning.\n", "link": "http://arxiv.org/abs/2505.22425v1", "date": "2025-05-28", "relevancy": 2.1643, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5517}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5389}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Reasoning%20without%20Attention&body=Title%3A%20Scaling%20Reasoning%20without%20Attention%0AAuthor%3A%20Xueliang%20Zhao%20and%20Wei%20Wu%20and%20Lingpeng%20Kong%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20made%20significant%20advances%20in%20complex%0Areasoning%20tasks%2C%20yet%20they%20remain%20bottlenecked%20by%20two%20core%20challenges%3A%0Aarchitectural%20inefficiency%20due%20to%20reliance%20on%20Transformers%2C%20and%20a%20lack%20of%0Astructured%20fine-tuning%20for%20high-difficulty%20domains.%20We%20introduce%20%5Courmodel%2C%20an%0Aattention-free%20language%20model%20that%20addresses%20both%20issues%20through%20architectural%0Aand%20data-centric%20innovations.%20Built%20on%20the%20state%20space%20dual%20%28SSD%29%20layers%20of%0AMamba-2%2C%20our%20model%20eliminates%20the%20need%20for%20self-attention%20and%20key-value%0Acaching%2C%20enabling%20fixed-memory%2C%20constant-time%20inference.%20To%20train%20it%20for%0Acomplex%20reasoning%2C%20we%20propose%20a%20two-phase%20curriculum%20fine-tuning%20strategy%20based%0Aon%20the%20%5Ctextsc%7BPromptCoT%7D%20synthesis%20paradigm%2C%20which%20generates%20pedagogically%0Astructured%20problems%20via%20abstract%20concept%20selection%20and%20rationale-guided%0Ageneration.%20On%20benchmark%20evaluations%2C%20%5Courmodel-7B%20outperforms%20strong%0ATransformer%20and%20hybrid%20models%20of%20comparable%20scale%2C%20and%20even%20surpasses%20the%20much%0Alarger%20Gemma3-27B%20by%202.6%5C%25%20on%20AIME%2024%2C%200.6%5C%25%20on%20AIME%2025%2C%20and%203.0%5C%25%20on%0ALivecodebench.%20These%20results%20highlight%20the%20potential%20of%20state%20space%20models%20as%0Aefficient%20and%20scalable%20alternatives%20to%20attention-based%20architectures%20for%0Ahigh-capacity%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Reasoning%2520without%2520Attention%26entry.906535625%3DXueliang%2520Zhao%2520and%2520Wei%2520Wu%2520and%2520Lingpeng%2520Kong%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520made%2520significant%2520advances%2520in%2520complex%250Areasoning%2520tasks%252C%2520yet%2520they%2520remain%2520bottlenecked%2520by%2520two%2520core%2520challenges%253A%250Aarchitectural%2520inefficiency%2520due%2520to%2520reliance%2520on%2520Transformers%252C%2520and%2520a%2520lack%2520of%250Astructured%2520fine-tuning%2520for%2520high-difficulty%2520domains.%2520We%2520introduce%2520%255Courmodel%252C%2520an%250Aattention-free%2520language%2520model%2520that%2520addresses%2520both%2520issues%2520through%2520architectural%250Aand%2520data-centric%2520innovations.%2520Built%2520on%2520the%2520state%2520space%2520dual%2520%2528SSD%2529%2520layers%2520of%250AMamba-2%252C%2520our%2520model%2520eliminates%2520the%2520need%2520for%2520self-attention%2520and%2520key-value%250Acaching%252C%2520enabling%2520fixed-memory%252C%2520constant-time%2520inference.%2520To%2520train%2520it%2520for%250Acomplex%2520reasoning%252C%2520we%2520propose%2520a%2520two-phase%2520curriculum%2520fine-tuning%2520strategy%2520based%250Aon%2520the%2520%255Ctextsc%257BPromptCoT%257D%2520synthesis%2520paradigm%252C%2520which%2520generates%2520pedagogically%250Astructured%2520problems%2520via%2520abstract%2520concept%2520selection%2520and%2520rationale-guided%250Ageneration.%2520On%2520benchmark%2520evaluations%252C%2520%255Courmodel-7B%2520outperforms%2520strong%250ATransformer%2520and%2520hybrid%2520models%2520of%2520comparable%2520scale%252C%2520and%2520even%2520surpasses%2520the%2520much%250Alarger%2520Gemma3-27B%2520by%25202.6%255C%2525%2520on%2520AIME%252024%252C%25200.6%255C%2525%2520on%2520AIME%252025%252C%2520and%25203.0%255C%2525%2520on%250ALivecodebench.%2520These%2520results%2520highlight%2520the%2520potential%2520of%2520state%2520space%2520models%2520as%250Aefficient%2520and%2520scalable%2520alternatives%2520to%2520attention-based%2520architectures%2520for%250Ahigh-capacity%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Reasoning%20without%20Attention&entry.906535625=Xueliang%20Zhao%20and%20Wei%20Wu%20and%20Lingpeng%20Kong&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20made%20significant%20advances%20in%20complex%0Areasoning%20tasks%2C%20yet%20they%20remain%20bottlenecked%20by%20two%20core%20challenges%3A%0Aarchitectural%20inefficiency%20due%20to%20reliance%20on%20Transformers%2C%20and%20a%20lack%20of%0Astructured%20fine-tuning%20for%20high-difficulty%20domains.%20We%20introduce%20%5Courmodel%2C%20an%0Aattention-free%20language%20model%20that%20addresses%20both%20issues%20through%20architectural%0Aand%20data-centric%20innovations.%20Built%20on%20the%20state%20space%20dual%20%28SSD%29%20layers%20of%0AMamba-2%2C%20our%20model%20eliminates%20the%20need%20for%20self-attention%20and%20key-value%0Acaching%2C%20enabling%20fixed-memory%2C%20constant-time%20inference.%20To%20train%20it%20for%0Acomplex%20reasoning%2C%20we%20propose%20a%20two-phase%20curriculum%20fine-tuning%20strategy%20based%0Aon%20the%20%5Ctextsc%7BPromptCoT%7D%20synthesis%20paradigm%2C%20which%20generates%20pedagogically%0Astructured%20problems%20via%20abstract%20concept%20selection%20and%20rationale-guided%0Ageneration.%20On%20benchmark%20evaluations%2C%20%5Courmodel-7B%20outperforms%20strong%0ATransformer%20and%20hybrid%20models%20of%20comparable%20scale%2C%20and%20even%20surpasses%20the%20much%0Alarger%20Gemma3-27B%20by%202.6%5C%25%20on%20AIME%2024%2C%200.6%5C%25%20on%20AIME%2025%2C%20and%203.0%5C%25%20on%0ALivecodebench.%20These%20results%20highlight%20the%20potential%20of%20state%20space%20models%20as%0Aefficient%20and%20scalable%20alternatives%20to%20attention-based%20architectures%20for%0Ahigh-capacity%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22425v1&entry.124074799=Read"},
{"title": "Position: Uncertainty Quantification Needs Reassessment for\n  Large-language Model Agents", "author": "Michael Kirchhof and Gjergji Kasneci and Enkelejda Kasneci", "abstract": "  Large-language models (LLMs) and chatbot agents are known to provide wrong\noutputs at times, and it was recently found that this can never be fully\nprevented. Hence, uncertainty quantification plays a crucial role, aiming to\nquantify the level of ambiguity in either one overall number or two numbers for\naleatoric and epistemic uncertainty. This position paper argues that this\ntraditional dichotomy of uncertainties is too limited for the open and\ninteractive setup that LLM agents operate in when communicating with a user,\nand that we need to research avenues that enrich uncertainties in this novel\nscenario. We review the literature and find that popular definitions of\naleatoric and epistemic uncertainties directly contradict each other and lose\ntheir meaning in interactive LLM agent settings. Hence, we propose three novel\nresearch directions that focus on uncertainties in such human-computer\ninteractions: Underspecification uncertainties, for when users do not provide\nall information or define the exact task at the first go, interactive learning,\nto ask follow-up questions and reduce the uncertainty about the current\ncontext, and output uncertainties, to utilize the rich language and speech\nspace to express uncertainties as more than mere numbers. We expect that these\nnew ways of dealing with and communicating uncertainties will lead to LLM agent\ninteractions that are more transparent, trustworthy, and intuitive.\n", "link": "http://arxiv.org/abs/2505.22655v1", "date": "2025-05-28", "relevancy": 2.1544, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5759}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5527}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Position%3A%20Uncertainty%20Quantification%20Needs%20Reassessment%20for%0A%20%20Large-language%20Model%20Agents&body=Title%3A%20Position%3A%20Uncertainty%20Quantification%20Needs%20Reassessment%20for%0A%20%20Large-language%20Model%20Agents%0AAuthor%3A%20Michael%20Kirchhof%20and%20Gjergji%20Kasneci%20and%20Enkelejda%20Kasneci%0AAbstract%3A%20%20%20Large-language%20models%20%28LLMs%29%20and%20chatbot%20agents%20are%20known%20to%20provide%20wrong%0Aoutputs%20at%20times%2C%20and%20it%20was%20recently%20found%20that%20this%20can%20never%20be%20fully%0Aprevented.%20Hence%2C%20uncertainty%20quantification%20plays%20a%20crucial%20role%2C%20aiming%20to%0Aquantify%20the%20level%20of%20ambiguity%20in%20either%20one%20overall%20number%20or%20two%20numbers%20for%0Aaleatoric%20and%20epistemic%20uncertainty.%20This%20position%20paper%20argues%20that%20this%0Atraditional%20dichotomy%20of%20uncertainties%20is%20too%20limited%20for%20the%20open%20and%0Ainteractive%20setup%20that%20LLM%20agents%20operate%20in%20when%20communicating%20with%20a%20user%2C%0Aand%20that%20we%20need%20to%20research%20avenues%20that%20enrich%20uncertainties%20in%20this%20novel%0Ascenario.%20We%20review%20the%20literature%20and%20find%20that%20popular%20definitions%20of%0Aaleatoric%20and%20epistemic%20uncertainties%20directly%20contradict%20each%20other%20and%20lose%0Atheir%20meaning%20in%20interactive%20LLM%20agent%20settings.%20Hence%2C%20we%20propose%20three%20novel%0Aresearch%20directions%20that%20focus%20on%20uncertainties%20in%20such%20human-computer%0Ainteractions%3A%20Underspecification%20uncertainties%2C%20for%20when%20users%20do%20not%20provide%0Aall%20information%20or%20define%20the%20exact%20task%20at%20the%20first%20go%2C%20interactive%20learning%2C%0Ato%20ask%20follow-up%20questions%20and%20reduce%20the%20uncertainty%20about%20the%20current%0Acontext%2C%20and%20output%20uncertainties%2C%20to%20utilize%20the%20rich%20language%20and%20speech%0Aspace%20to%20express%20uncertainties%20as%20more%20than%20mere%20numbers.%20We%20expect%20that%20these%0Anew%20ways%20of%20dealing%20with%20and%20communicating%20uncertainties%20will%20lead%20to%20LLM%20agent%0Ainteractions%20that%20are%20more%20transparent%2C%20trustworthy%2C%20and%20intuitive.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22655v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPosition%253A%2520Uncertainty%2520Quantification%2520Needs%2520Reassessment%2520for%250A%2520%2520Large-language%2520Model%2520Agents%26entry.906535625%3DMichael%2520Kirchhof%2520and%2520Gjergji%2520Kasneci%2520and%2520Enkelejda%2520Kasneci%26entry.1292438233%3D%2520%2520Large-language%2520models%2520%2528LLMs%2529%2520and%2520chatbot%2520agents%2520are%2520known%2520to%2520provide%2520wrong%250Aoutputs%2520at%2520times%252C%2520and%2520it%2520was%2520recently%2520found%2520that%2520this%2520can%2520never%2520be%2520fully%250Aprevented.%2520Hence%252C%2520uncertainty%2520quantification%2520plays%2520a%2520crucial%2520role%252C%2520aiming%2520to%250Aquantify%2520the%2520level%2520of%2520ambiguity%2520in%2520either%2520one%2520overall%2520number%2520or%2520two%2520numbers%2520for%250Aaleatoric%2520and%2520epistemic%2520uncertainty.%2520This%2520position%2520paper%2520argues%2520that%2520this%250Atraditional%2520dichotomy%2520of%2520uncertainties%2520is%2520too%2520limited%2520for%2520the%2520open%2520and%250Ainteractive%2520setup%2520that%2520LLM%2520agents%2520operate%2520in%2520when%2520communicating%2520with%2520a%2520user%252C%250Aand%2520that%2520we%2520need%2520to%2520research%2520avenues%2520that%2520enrich%2520uncertainties%2520in%2520this%2520novel%250Ascenario.%2520We%2520review%2520the%2520literature%2520and%2520find%2520that%2520popular%2520definitions%2520of%250Aaleatoric%2520and%2520epistemic%2520uncertainties%2520directly%2520contradict%2520each%2520other%2520and%2520lose%250Atheir%2520meaning%2520in%2520interactive%2520LLM%2520agent%2520settings.%2520Hence%252C%2520we%2520propose%2520three%2520novel%250Aresearch%2520directions%2520that%2520focus%2520on%2520uncertainties%2520in%2520such%2520human-computer%250Ainteractions%253A%2520Underspecification%2520uncertainties%252C%2520for%2520when%2520users%2520do%2520not%2520provide%250Aall%2520information%2520or%2520define%2520the%2520exact%2520task%2520at%2520the%2520first%2520go%252C%2520interactive%2520learning%252C%250Ato%2520ask%2520follow-up%2520questions%2520and%2520reduce%2520the%2520uncertainty%2520about%2520the%2520current%250Acontext%252C%2520and%2520output%2520uncertainties%252C%2520to%2520utilize%2520the%2520rich%2520language%2520and%2520speech%250Aspace%2520to%2520express%2520uncertainties%2520as%2520more%2520than%2520mere%2520numbers.%2520We%2520expect%2520that%2520these%250Anew%2520ways%2520of%2520dealing%2520with%2520and%2520communicating%2520uncertainties%2520will%2520lead%2520to%2520LLM%2520agent%250Ainteractions%2520that%2520are%2520more%2520transparent%252C%2520trustworthy%252C%2520and%2520intuitive.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22655v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Position%3A%20Uncertainty%20Quantification%20Needs%20Reassessment%20for%0A%20%20Large-language%20Model%20Agents&entry.906535625=Michael%20Kirchhof%20and%20Gjergji%20Kasneci%20and%20Enkelejda%20Kasneci&entry.1292438233=%20%20Large-language%20models%20%28LLMs%29%20and%20chatbot%20agents%20are%20known%20to%20provide%20wrong%0Aoutputs%20at%20times%2C%20and%20it%20was%20recently%20found%20that%20this%20can%20never%20be%20fully%0Aprevented.%20Hence%2C%20uncertainty%20quantification%20plays%20a%20crucial%20role%2C%20aiming%20to%0Aquantify%20the%20level%20of%20ambiguity%20in%20either%20one%20overall%20number%20or%20two%20numbers%20for%0Aaleatoric%20and%20epistemic%20uncertainty.%20This%20position%20paper%20argues%20that%20this%0Atraditional%20dichotomy%20of%20uncertainties%20is%20too%20limited%20for%20the%20open%20and%0Ainteractive%20setup%20that%20LLM%20agents%20operate%20in%20when%20communicating%20with%20a%20user%2C%0Aand%20that%20we%20need%20to%20research%20avenues%20that%20enrich%20uncertainties%20in%20this%20novel%0Ascenario.%20We%20review%20the%20literature%20and%20find%20that%20popular%20definitions%20of%0Aaleatoric%20and%20epistemic%20uncertainties%20directly%20contradict%20each%20other%20and%20lose%0Atheir%20meaning%20in%20interactive%20LLM%20agent%20settings.%20Hence%2C%20we%20propose%20three%20novel%0Aresearch%20directions%20that%20focus%20on%20uncertainties%20in%20such%20human-computer%0Ainteractions%3A%20Underspecification%20uncertainties%2C%20for%20when%20users%20do%20not%20provide%0Aall%20information%20or%20define%20the%20exact%20task%20at%20the%20first%20go%2C%20interactive%20learning%2C%0Ato%20ask%20follow-up%20questions%20and%20reduce%20the%20uncertainty%20about%20the%20current%0Acontext%2C%20and%20output%20uncertainties%2C%20to%20utilize%20the%20rich%20language%20and%20speech%0Aspace%20to%20express%20uncertainties%20as%20more%20than%20mere%20numbers.%20We%20expect%20that%20these%0Anew%20ways%20of%20dealing%20with%20and%20communicating%20uncertainties%20will%20lead%20to%20LLM%20agent%0Ainteractions%20that%20are%20more%20transparent%2C%20trustworthy%2C%20and%20intuitive.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22655v1&entry.124074799=Read"},
{"title": "AstroVisBench: A Code Benchmark for Scientific Computing and\n  Visualization in Astronomy", "author": "Sebastian Antony Joseph and Syed Murtaza Husain and Stella S. R. Offner and St\u00e9phanie Juneau and Paul Torrey and Adam S. Bolton and Juan P. Farias and Niall Gaffney and Greg Durrett and Junyi Jessy Li", "abstract": "  Large Language Models (LLMs) are being explored for applications in\nscientific research, including their capabilities to synthesize literature,\nanswer research questions, generate research ideas, and even conduct\ncomputational experiments. Ultimately, our goal is for these to help scientists\nderive novel scientific insights. In many areas of science, such insights often\narise from processing and visualizing data to understand its patterns. However,\nevaluating whether an LLM-mediated scientific workflow produces outputs\nconveying the correct scientific insights is challenging to evaluate and has\nnot been addressed in past work. We introduce AstroVisBench, the first\nbenchmark for both scientific computing and visualization in the astronomy\ndomain. AstroVisBench judges a language model's ability to both (1) create\nastronomy-specific workflows to process and analyze data and (2) visualize the\nresults of these workflows through complex plots. Our evaluation of\nvisualizations uses a novel LLM-as-a-judge workflow, which is validated against\nannotation by five professional astronomers. Using AstroVisBench we present an\nevaluation of state-of-the-art language models, showing a significant gap in\ntheir ability to engage in astronomy research as useful assistants. This\nevaluation provides a strong end-to-end evaluation for AI scientists that\noffers a path forward for the development of visualization-based workflows,\nwhich are central to a broad range of domains from physics to biology.\n", "link": "http://arxiv.org/abs/2505.20538v2", "date": "2025-05-28", "relevancy": 2.1541, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5514}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5514}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AstroVisBench%3A%20A%20Code%20Benchmark%20for%20Scientific%20Computing%20and%0A%20%20Visualization%20in%20Astronomy&body=Title%3A%20AstroVisBench%3A%20A%20Code%20Benchmark%20for%20Scientific%20Computing%20and%0A%20%20Visualization%20in%20Astronomy%0AAuthor%3A%20Sebastian%20Antony%20Joseph%20and%20Syed%20Murtaza%20Husain%20and%20Stella%20S.%20R.%20Offner%20and%20St%C3%A9phanie%20Juneau%20and%20Paul%20Torrey%20and%20Adam%20S.%20Bolton%20and%20Juan%20P.%20Farias%20and%20Niall%20Gaffney%20and%20Greg%20Durrett%20and%20Junyi%20Jessy%20Li%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20being%20explored%20for%20applications%20in%0Ascientific%20research%2C%20including%20their%20capabilities%20to%20synthesize%20literature%2C%0Aanswer%20research%20questions%2C%20generate%20research%20ideas%2C%20and%20even%20conduct%0Acomputational%20experiments.%20Ultimately%2C%20our%20goal%20is%20for%20these%20to%20help%20scientists%0Aderive%20novel%20scientific%20insights.%20In%20many%20areas%20of%20science%2C%20such%20insights%20often%0Aarise%20from%20processing%20and%20visualizing%20data%20to%20understand%20its%20patterns.%20However%2C%0Aevaluating%20whether%20an%20LLM-mediated%20scientific%20workflow%20produces%20outputs%0Aconveying%20the%20correct%20scientific%20insights%20is%20challenging%20to%20evaluate%20and%20has%0Anot%20been%20addressed%20in%20past%20work.%20We%20introduce%20AstroVisBench%2C%20the%20first%0Abenchmark%20for%20both%20scientific%20computing%20and%20visualization%20in%20the%20astronomy%0Adomain.%20AstroVisBench%20judges%20a%20language%20model%27s%20ability%20to%20both%20%281%29%20create%0Aastronomy-specific%20workflows%20to%20process%20and%20analyze%20data%20and%20%282%29%20visualize%20the%0Aresults%20of%20these%20workflows%20through%20complex%20plots.%20Our%20evaluation%20of%0Avisualizations%20uses%20a%20novel%20LLM-as-a-judge%20workflow%2C%20which%20is%20validated%20against%0Aannotation%20by%20five%20professional%20astronomers.%20Using%20AstroVisBench%20we%20present%20an%0Aevaluation%20of%20state-of-the-art%20language%20models%2C%20showing%20a%20significant%20gap%20in%0Atheir%20ability%20to%20engage%20in%20astronomy%20research%20as%20useful%20assistants.%20This%0Aevaluation%20provides%20a%20strong%20end-to-end%20evaluation%20for%20AI%20scientists%20that%0Aoffers%20a%20path%20forward%20for%20the%20development%20of%20visualization-based%20workflows%2C%0Awhich%20are%20central%20to%20a%20broad%20range%20of%20domains%20from%20physics%20to%20biology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20538v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAstroVisBench%253A%2520A%2520Code%2520Benchmark%2520for%2520Scientific%2520Computing%2520and%250A%2520%2520Visualization%2520in%2520Astronomy%26entry.906535625%3DSebastian%2520Antony%2520Joseph%2520and%2520Syed%2520Murtaza%2520Husain%2520and%2520Stella%2520S.%2520R.%2520Offner%2520and%2520St%25C3%25A9phanie%2520Juneau%2520and%2520Paul%2520Torrey%2520and%2520Adam%2520S.%2520Bolton%2520and%2520Juan%2520P.%2520Farias%2520and%2520Niall%2520Gaffney%2520and%2520Greg%2520Durrett%2520and%2520Junyi%2520Jessy%2520Li%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520being%2520explored%2520for%2520applications%2520in%250Ascientific%2520research%252C%2520including%2520their%2520capabilities%2520to%2520synthesize%2520literature%252C%250Aanswer%2520research%2520questions%252C%2520generate%2520research%2520ideas%252C%2520and%2520even%2520conduct%250Acomputational%2520experiments.%2520Ultimately%252C%2520our%2520goal%2520is%2520for%2520these%2520to%2520help%2520scientists%250Aderive%2520novel%2520scientific%2520insights.%2520In%2520many%2520areas%2520of%2520science%252C%2520such%2520insights%2520often%250Aarise%2520from%2520processing%2520and%2520visualizing%2520data%2520to%2520understand%2520its%2520patterns.%2520However%252C%250Aevaluating%2520whether%2520an%2520LLM-mediated%2520scientific%2520workflow%2520produces%2520outputs%250Aconveying%2520the%2520correct%2520scientific%2520insights%2520is%2520challenging%2520to%2520evaluate%2520and%2520has%250Anot%2520been%2520addressed%2520in%2520past%2520work.%2520We%2520introduce%2520AstroVisBench%252C%2520the%2520first%250Abenchmark%2520for%2520both%2520scientific%2520computing%2520and%2520visualization%2520in%2520the%2520astronomy%250Adomain.%2520AstroVisBench%2520judges%2520a%2520language%2520model%2527s%2520ability%2520to%2520both%2520%25281%2529%2520create%250Aastronomy-specific%2520workflows%2520to%2520process%2520and%2520analyze%2520data%2520and%2520%25282%2529%2520visualize%2520the%250Aresults%2520of%2520these%2520workflows%2520through%2520complex%2520plots.%2520Our%2520evaluation%2520of%250Avisualizations%2520uses%2520a%2520novel%2520LLM-as-a-judge%2520workflow%252C%2520which%2520is%2520validated%2520against%250Aannotation%2520by%2520five%2520professional%2520astronomers.%2520Using%2520AstroVisBench%2520we%2520present%2520an%250Aevaluation%2520of%2520state-of-the-art%2520language%2520models%252C%2520showing%2520a%2520significant%2520gap%2520in%250Atheir%2520ability%2520to%2520engage%2520in%2520astronomy%2520research%2520as%2520useful%2520assistants.%2520This%250Aevaluation%2520provides%2520a%2520strong%2520end-to-end%2520evaluation%2520for%2520AI%2520scientists%2520that%250Aoffers%2520a%2520path%2520forward%2520for%2520the%2520development%2520of%2520visualization-based%2520workflows%252C%250Awhich%2520are%2520central%2520to%2520a%2520broad%2520range%2520of%2520domains%2520from%2520physics%2520to%2520biology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20538v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AstroVisBench%3A%20A%20Code%20Benchmark%20for%20Scientific%20Computing%20and%0A%20%20Visualization%20in%20Astronomy&entry.906535625=Sebastian%20Antony%20Joseph%20and%20Syed%20Murtaza%20Husain%20and%20Stella%20S.%20R.%20Offner%20and%20St%C3%A9phanie%20Juneau%20and%20Paul%20Torrey%20and%20Adam%20S.%20Bolton%20and%20Juan%20P.%20Farias%20and%20Niall%20Gaffney%20and%20Greg%20Durrett%20and%20Junyi%20Jessy%20Li&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20being%20explored%20for%20applications%20in%0Ascientific%20research%2C%20including%20their%20capabilities%20to%20synthesize%20literature%2C%0Aanswer%20research%20questions%2C%20generate%20research%20ideas%2C%20and%20even%20conduct%0Acomputational%20experiments.%20Ultimately%2C%20our%20goal%20is%20for%20these%20to%20help%20scientists%0Aderive%20novel%20scientific%20insights.%20In%20many%20areas%20of%20science%2C%20such%20insights%20often%0Aarise%20from%20processing%20and%20visualizing%20data%20to%20understand%20its%20patterns.%20However%2C%0Aevaluating%20whether%20an%20LLM-mediated%20scientific%20workflow%20produces%20outputs%0Aconveying%20the%20correct%20scientific%20insights%20is%20challenging%20to%20evaluate%20and%20has%0Anot%20been%20addressed%20in%20past%20work.%20We%20introduce%20AstroVisBench%2C%20the%20first%0Abenchmark%20for%20both%20scientific%20computing%20and%20visualization%20in%20the%20astronomy%0Adomain.%20AstroVisBench%20judges%20a%20language%20model%27s%20ability%20to%20both%20%281%29%20create%0Aastronomy-specific%20workflows%20to%20process%20and%20analyze%20data%20and%20%282%29%20visualize%20the%0Aresults%20of%20these%20workflows%20through%20complex%20plots.%20Our%20evaluation%20of%0Avisualizations%20uses%20a%20novel%20LLM-as-a-judge%20workflow%2C%20which%20is%20validated%20against%0Aannotation%20by%20five%20professional%20astronomers.%20Using%20AstroVisBench%20we%20present%20an%0Aevaluation%20of%20state-of-the-art%20language%20models%2C%20showing%20a%20significant%20gap%20in%0Atheir%20ability%20to%20engage%20in%20astronomy%20research%20as%20useful%20assistants.%20This%0Aevaluation%20provides%20a%20strong%20end-to-end%20evaluation%20for%20AI%20scientists%20that%0Aoffers%20a%20path%20forward%20for%20the%20development%20of%20visualization-based%20workflows%2C%0Awhich%20are%20central%20to%20a%20broad%20range%20of%20domains%20from%20physics%20to%20biology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20538v2&entry.124074799=Read"},
{"title": "X-GAN: A Generative AI-Powered Unsupervised Model for Main Vessel\n  Segmentation of Glaucoma Screening", "author": "Cheng Huang and Weizheng Xie and Tsengdar J. Lee and Jui-Kai Wang and Karanjit Kooner and Ning Zhang and Jia Zhang", "abstract": "  Structural changes in main retinal blood vessels serve as critical biomarkers\nfor the onset and progression of glaucoma. Identifying these vessels is vital\nfor vascular modeling yet highly challenging. This paper proposes X-GAN, a\ngenerative AI-powered unsupervised segmentation model designed for extracting\nmain blood vessels from Optical Coherence Tomography Angiography (OCTA) images.\nThe process begins with the Space Colonization Algorithm (SCA) to rapidly\ngenerate a skeleton of vessels, featuring their radii. By synergistically\nintegrating the generative adversarial network (GAN) with biostatistical\nmodeling of vessel radii, X-GAN enables a fast reconstruction of both 2D and 3D\nrepresentations of the vessels. Based on this reconstruction, X-GAN achieves\nnearly 100% segmentation accuracy without relying on labeled data or\nhigh-performance computing resources. Experimental results confirm X-GAN's\nsuperiority in evaluating main vessel segmentation compared to existing deep\nlearning models. Code is here: https://github.com/VikiXie/SatMar8.\n", "link": "http://arxiv.org/abs/2503.06743v3", "date": "2025-05-28", "relevancy": 2.1343, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5466}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5243}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5243}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20X-GAN%3A%20A%20Generative%20AI-Powered%20Unsupervised%20Model%20for%20Main%20Vessel%0A%20%20Segmentation%20of%20Glaucoma%20Screening&body=Title%3A%20X-GAN%3A%20A%20Generative%20AI-Powered%20Unsupervised%20Model%20for%20Main%20Vessel%0A%20%20Segmentation%20of%20Glaucoma%20Screening%0AAuthor%3A%20Cheng%20Huang%20and%20Weizheng%20Xie%20and%20Tsengdar%20J.%20Lee%20and%20Jui-Kai%20Wang%20and%20Karanjit%20Kooner%20and%20Ning%20Zhang%20and%20Jia%20Zhang%0AAbstract%3A%20%20%20Structural%20changes%20in%20main%20retinal%20blood%20vessels%20serve%20as%20critical%20biomarkers%0Afor%20the%20onset%20and%20progression%20of%20glaucoma.%20Identifying%20these%20vessels%20is%20vital%0Afor%20vascular%20modeling%20yet%20highly%20challenging.%20This%20paper%20proposes%20X-GAN%2C%20a%0Agenerative%20AI-powered%20unsupervised%20segmentation%20model%20designed%20for%20extracting%0Amain%20blood%20vessels%20from%20Optical%20Coherence%20Tomography%20Angiography%20%28OCTA%29%20images.%0AThe%20process%20begins%20with%20the%20Space%20Colonization%20Algorithm%20%28SCA%29%20to%20rapidly%0Agenerate%20a%20skeleton%20of%20vessels%2C%20featuring%20their%20radii.%20By%20synergistically%0Aintegrating%20the%20generative%20adversarial%20network%20%28GAN%29%20with%20biostatistical%0Amodeling%20of%20vessel%20radii%2C%20X-GAN%20enables%20a%20fast%20reconstruction%20of%20both%202D%20and%203D%0Arepresentations%20of%20the%20vessels.%20Based%20on%20this%20reconstruction%2C%20X-GAN%20achieves%0Anearly%20100%25%20segmentation%20accuracy%20without%20relying%20on%20labeled%20data%20or%0Ahigh-performance%20computing%20resources.%20Experimental%20results%20confirm%20X-GAN%27s%0Asuperiority%20in%20evaluating%20main%20vessel%20segmentation%20compared%20to%20existing%20deep%0Alearning%20models.%20Code%20is%20here%3A%20https%3A//github.com/VikiXie/SatMar8.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.06743v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DX-GAN%253A%2520A%2520Generative%2520AI-Powered%2520Unsupervised%2520Model%2520for%2520Main%2520Vessel%250A%2520%2520Segmentation%2520of%2520Glaucoma%2520Screening%26entry.906535625%3DCheng%2520Huang%2520and%2520Weizheng%2520Xie%2520and%2520Tsengdar%2520J.%2520Lee%2520and%2520Jui-Kai%2520Wang%2520and%2520Karanjit%2520Kooner%2520and%2520Ning%2520Zhang%2520and%2520Jia%2520Zhang%26entry.1292438233%3D%2520%2520Structural%2520changes%2520in%2520main%2520retinal%2520blood%2520vessels%2520serve%2520as%2520critical%2520biomarkers%250Afor%2520the%2520onset%2520and%2520progression%2520of%2520glaucoma.%2520Identifying%2520these%2520vessels%2520is%2520vital%250Afor%2520vascular%2520modeling%2520yet%2520highly%2520challenging.%2520This%2520paper%2520proposes%2520X-GAN%252C%2520a%250Agenerative%2520AI-powered%2520unsupervised%2520segmentation%2520model%2520designed%2520for%2520extracting%250Amain%2520blood%2520vessels%2520from%2520Optical%2520Coherence%2520Tomography%2520Angiography%2520%2528OCTA%2529%2520images.%250AThe%2520process%2520begins%2520with%2520the%2520Space%2520Colonization%2520Algorithm%2520%2528SCA%2529%2520to%2520rapidly%250Agenerate%2520a%2520skeleton%2520of%2520vessels%252C%2520featuring%2520their%2520radii.%2520By%2520synergistically%250Aintegrating%2520the%2520generative%2520adversarial%2520network%2520%2528GAN%2529%2520with%2520biostatistical%250Amodeling%2520of%2520vessel%2520radii%252C%2520X-GAN%2520enables%2520a%2520fast%2520reconstruction%2520of%2520both%25202D%2520and%25203D%250Arepresentations%2520of%2520the%2520vessels.%2520Based%2520on%2520this%2520reconstruction%252C%2520X-GAN%2520achieves%250Anearly%2520100%2525%2520segmentation%2520accuracy%2520without%2520relying%2520on%2520labeled%2520data%2520or%250Ahigh-performance%2520computing%2520resources.%2520Experimental%2520results%2520confirm%2520X-GAN%2527s%250Asuperiority%2520in%2520evaluating%2520main%2520vessel%2520segmentation%2520compared%2520to%2520existing%2520deep%250Alearning%2520models.%2520Code%2520is%2520here%253A%2520https%253A//github.com/VikiXie/SatMar8.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.06743v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=X-GAN%3A%20A%20Generative%20AI-Powered%20Unsupervised%20Model%20for%20Main%20Vessel%0A%20%20Segmentation%20of%20Glaucoma%20Screening&entry.906535625=Cheng%20Huang%20and%20Weizheng%20Xie%20and%20Tsengdar%20J.%20Lee%20and%20Jui-Kai%20Wang%20and%20Karanjit%20Kooner%20and%20Ning%20Zhang%20and%20Jia%20Zhang&entry.1292438233=%20%20Structural%20changes%20in%20main%20retinal%20blood%20vessels%20serve%20as%20critical%20biomarkers%0Afor%20the%20onset%20and%20progression%20of%20glaucoma.%20Identifying%20these%20vessels%20is%20vital%0Afor%20vascular%20modeling%20yet%20highly%20challenging.%20This%20paper%20proposes%20X-GAN%2C%20a%0Agenerative%20AI-powered%20unsupervised%20segmentation%20model%20designed%20for%20extracting%0Amain%20blood%20vessels%20from%20Optical%20Coherence%20Tomography%20Angiography%20%28OCTA%29%20images.%0AThe%20process%20begins%20with%20the%20Space%20Colonization%20Algorithm%20%28SCA%29%20to%20rapidly%0Agenerate%20a%20skeleton%20of%20vessels%2C%20featuring%20their%20radii.%20By%20synergistically%0Aintegrating%20the%20generative%20adversarial%20network%20%28GAN%29%20with%20biostatistical%0Amodeling%20of%20vessel%20radii%2C%20X-GAN%20enables%20a%20fast%20reconstruction%20of%20both%202D%20and%203D%0Arepresentations%20of%20the%20vessels.%20Based%20on%20this%20reconstruction%2C%20X-GAN%20achieves%0Anearly%20100%25%20segmentation%20accuracy%20without%20relying%20on%20labeled%20data%20or%0Ahigh-performance%20computing%20resources.%20Experimental%20results%20confirm%20X-GAN%27s%0Asuperiority%20in%20evaluating%20main%20vessel%20segmentation%20compared%20to%20existing%20deep%0Alearning%20models.%20Code%20is%20here%3A%20https%3A//github.com/VikiXie/SatMar8.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.06743v3&entry.124074799=Read"},
{"title": "Skywork Open Reasoner 1 Technical Report", "author": "Jujie He and Jiacai Liu and Chris Yuhao Liu and Rui Yan and Chaojie Wang and Peng Cheng and Xiaoyu Zhang and Fuxiang Zhang and Jiacheng Xu and Wei Shen and Siyuan Li and Liang Zeng and Tianwen Wei and Cheng Cheng and Bo An and Yang Liu and Yahui Zhou", "abstract": "  The success of DeepSeek-R1 underscores the significant role of reinforcement\nlearning (RL) in enhancing the reasoning capabilities of large language models\n(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL\nimplementation for long Chain-of-Thought (CoT) models. Building on the\nDeepSeek-R1-Distill model series, our RL approach achieves notable performance\ngains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench\nfrom 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)\nfor the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and\nQwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable\nresults on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models\ndemonstrate competitive reasoning capabilities among models of similar size. We\nperform comprehensive ablation studies on the core components of our training\npipeline to validate their effectiveness. Additionally, we thoroughly\ninvestigate the phenomenon of entropy collapse, identify key factors affecting\nentropy dynamics, and demonstrate that mitigating premature entropy collapse is\ncritical for improved test performance. To support community research, we fully\nopen-source our model weights, training code, and training datasets.\n", "link": "http://arxiv.org/abs/2505.22312v1", "date": "2025-05-28", "relevancy": 2.1232, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5379}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5379}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Skywork%20Open%20Reasoner%201%20Technical%20Report&body=Title%3A%20Skywork%20Open%20Reasoner%201%20Technical%20Report%0AAuthor%3A%20Jujie%20He%20and%20Jiacai%20Liu%20and%20Chris%20Yuhao%20Liu%20and%20Rui%20Yan%20and%20Chaojie%20Wang%20and%20Peng%20Cheng%20and%20Xiaoyu%20Zhang%20and%20Fuxiang%20Zhang%20and%20Jiacheng%20Xu%20and%20Wei%20Shen%20and%20Siyuan%20Li%20and%20Liang%20Zeng%20and%20Tianwen%20Wei%20and%20Cheng%20Cheng%20and%20Bo%20An%20and%20Yang%20Liu%20and%20Yahui%20Zhou%0AAbstract%3A%20%20%20The%20success%20of%20DeepSeek-R1%20underscores%20the%20significant%20role%20of%20reinforcement%0Alearning%20%28RL%29%20in%20enhancing%20the%20reasoning%20capabilities%20of%20large%20language%20models%0A%28LLMs%29.%20In%20this%20work%2C%20we%20present%20Skywork-OR1%2C%20an%20effective%20and%20scalable%20RL%0Aimplementation%20for%20long%20Chain-of-Thought%20%28CoT%29%20models.%20Building%20on%20the%0ADeepSeek-R1-Distill%20model%20series%2C%20our%20RL%20approach%20achieves%20notable%20performance%0Agains%2C%20increasing%20average%20accuracy%20across%20AIME24%2C%20AIME25%2C%20and%20LiveCodeBench%0Afrom%2057.8%25%20to%2072.8%25%20%28%2B15.0%25%29%20for%20the%2032B%20model%20and%20from%2043.6%25%20to%2057.5%25%20%28%2B13.9%25%29%0Afor%20the%207B%20model.%20Our%20Skywork-OR1-32B%20model%20surpasses%20both%20DeepSeek-R1%20and%0AQwen3-32B%20on%20the%20AIME24%20and%20AIME25%20benchmarks%2C%20while%20achieving%20comparable%0Aresults%20on%20LiveCodeBench.%20The%20Skywork-OR1-7B%20and%20Skywork-OR1-Math-7B%20models%0Ademonstrate%20competitive%20reasoning%20capabilities%20among%20models%20of%20similar%20size.%20We%0Aperform%20comprehensive%20ablation%20studies%20on%20the%20core%20components%20of%20our%20training%0Apipeline%20to%20validate%20their%20effectiveness.%20Additionally%2C%20we%20thoroughly%0Ainvestigate%20the%20phenomenon%20of%20entropy%20collapse%2C%20identify%20key%20factors%20affecting%0Aentropy%20dynamics%2C%20and%20demonstrate%20that%20mitigating%20premature%20entropy%20collapse%20is%0Acritical%20for%20improved%20test%20performance.%20To%20support%20community%20research%2C%20we%20fully%0Aopen-source%20our%20model%20weights%2C%20training%20code%2C%20and%20training%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22312v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkywork%2520Open%2520Reasoner%25201%2520Technical%2520Report%26entry.906535625%3DJujie%2520He%2520and%2520Jiacai%2520Liu%2520and%2520Chris%2520Yuhao%2520Liu%2520and%2520Rui%2520Yan%2520and%2520Chaojie%2520Wang%2520and%2520Peng%2520Cheng%2520and%2520Xiaoyu%2520Zhang%2520and%2520Fuxiang%2520Zhang%2520and%2520Jiacheng%2520Xu%2520and%2520Wei%2520Shen%2520and%2520Siyuan%2520Li%2520and%2520Liang%2520Zeng%2520and%2520Tianwen%2520Wei%2520and%2520Cheng%2520Cheng%2520and%2520Bo%2520An%2520and%2520Yang%2520Liu%2520and%2520Yahui%2520Zhou%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520DeepSeek-R1%2520underscores%2520the%2520significant%2520role%2520of%2520reinforcement%250Alearning%2520%2528RL%2529%2520in%2520enhancing%2520the%2520reasoning%2520capabilities%2520of%2520large%2520language%2520models%250A%2528LLMs%2529.%2520In%2520this%2520work%252C%2520we%2520present%2520Skywork-OR1%252C%2520an%2520effective%2520and%2520scalable%2520RL%250Aimplementation%2520for%2520long%2520Chain-of-Thought%2520%2528CoT%2529%2520models.%2520Building%2520on%2520the%250ADeepSeek-R1-Distill%2520model%2520series%252C%2520our%2520RL%2520approach%2520achieves%2520notable%2520performance%250Agains%252C%2520increasing%2520average%2520accuracy%2520across%2520AIME24%252C%2520AIME25%252C%2520and%2520LiveCodeBench%250Afrom%252057.8%2525%2520to%252072.8%2525%2520%2528%252B15.0%2525%2529%2520for%2520the%252032B%2520model%2520and%2520from%252043.6%2525%2520to%252057.5%2525%2520%2528%252B13.9%2525%2529%250Afor%2520the%25207B%2520model.%2520Our%2520Skywork-OR1-32B%2520model%2520surpasses%2520both%2520DeepSeek-R1%2520and%250AQwen3-32B%2520on%2520the%2520AIME24%2520and%2520AIME25%2520benchmarks%252C%2520while%2520achieving%2520comparable%250Aresults%2520on%2520LiveCodeBench.%2520The%2520Skywork-OR1-7B%2520and%2520Skywork-OR1-Math-7B%2520models%250Ademonstrate%2520competitive%2520reasoning%2520capabilities%2520among%2520models%2520of%2520similar%2520size.%2520We%250Aperform%2520comprehensive%2520ablation%2520studies%2520on%2520the%2520core%2520components%2520of%2520our%2520training%250Apipeline%2520to%2520validate%2520their%2520effectiveness.%2520Additionally%252C%2520we%2520thoroughly%250Ainvestigate%2520the%2520phenomenon%2520of%2520entropy%2520collapse%252C%2520identify%2520key%2520factors%2520affecting%250Aentropy%2520dynamics%252C%2520and%2520demonstrate%2520that%2520mitigating%2520premature%2520entropy%2520collapse%2520is%250Acritical%2520for%2520improved%2520test%2520performance.%2520To%2520support%2520community%2520research%252C%2520we%2520fully%250Aopen-source%2520our%2520model%2520weights%252C%2520training%2520code%252C%2520and%2520training%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22312v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Skywork%20Open%20Reasoner%201%20Technical%20Report&entry.906535625=Jujie%20He%20and%20Jiacai%20Liu%20and%20Chris%20Yuhao%20Liu%20and%20Rui%20Yan%20and%20Chaojie%20Wang%20and%20Peng%20Cheng%20and%20Xiaoyu%20Zhang%20and%20Fuxiang%20Zhang%20and%20Jiacheng%20Xu%20and%20Wei%20Shen%20and%20Siyuan%20Li%20and%20Liang%20Zeng%20and%20Tianwen%20Wei%20and%20Cheng%20Cheng%20and%20Bo%20An%20and%20Yang%20Liu%20and%20Yahui%20Zhou&entry.1292438233=%20%20The%20success%20of%20DeepSeek-R1%20underscores%20the%20significant%20role%20of%20reinforcement%0Alearning%20%28RL%29%20in%20enhancing%20the%20reasoning%20capabilities%20of%20large%20language%20models%0A%28LLMs%29.%20In%20this%20work%2C%20we%20present%20Skywork-OR1%2C%20an%20effective%20and%20scalable%20RL%0Aimplementation%20for%20long%20Chain-of-Thought%20%28CoT%29%20models.%20Building%20on%20the%0ADeepSeek-R1-Distill%20model%20series%2C%20our%20RL%20approach%20achieves%20notable%20performance%0Agains%2C%20increasing%20average%20accuracy%20across%20AIME24%2C%20AIME25%2C%20and%20LiveCodeBench%0Afrom%2057.8%25%20to%2072.8%25%20%28%2B15.0%25%29%20for%20the%2032B%20model%20and%20from%2043.6%25%20to%2057.5%25%20%28%2B13.9%25%29%0Afor%20the%207B%20model.%20Our%20Skywork-OR1-32B%20model%20surpasses%20both%20DeepSeek-R1%20and%0AQwen3-32B%20on%20the%20AIME24%20and%20AIME25%20benchmarks%2C%20while%20achieving%20comparable%0Aresults%20on%20LiveCodeBench.%20The%20Skywork-OR1-7B%20and%20Skywork-OR1-Math-7B%20models%0Ademonstrate%20competitive%20reasoning%20capabilities%20among%20models%20of%20similar%20size.%20We%0Aperform%20comprehensive%20ablation%20studies%20on%20the%20core%20components%20of%20our%20training%0Apipeline%20to%20validate%20their%20effectiveness.%20Additionally%2C%20we%20thoroughly%0Ainvestigate%20the%20phenomenon%20of%20entropy%20collapse%2C%20identify%20key%20factors%20affecting%0Aentropy%20dynamics%2C%20and%20demonstrate%20that%20mitigating%20premature%20entropy%20collapse%20is%0Acritical%20for%20improved%20test%20performance.%20To%20support%20community%20research%2C%20we%20fully%0Aopen-source%20our%20model%20weights%2C%20training%20code%2C%20and%20training%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22312v1&entry.124074799=Read"},
{"title": "Advancing Multimodal Reasoning via Reinforcement Learning with Cold\n  Start", "author": "Lai Wei and Yuting Li and Kaipeng Zheng and Chen Wang and Yue Wang and Linghe Kong and Lichao Sun and Weiran Huang", "abstract": "  Recent advancements in large language models (LLMs) have demonstrated\nimpressive chain-of-thought reasoning capabilities, with reinforcement learning\n(RL) playing a crucial role in this progress. While \"aha moment\"\npatterns--where models exhibit self-correction through reflection--are often\nattributed to emergent properties from RL, we first demonstrate that these\npatterns exist in multimodal LLMs (MLLMs) prior to RL training but may not\nnecessarily correlate with improved reasoning performance. Building on these\ninsights, we present a comprehensive study on enhancing multimodal reasoning\nthrough a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start\nwith structured chain-of-thought reasoning patterns, followed by (2)\nreinforcement learning via GRPO to further refine these capabilities. Our\nextensive experiments show that this combined approach consistently outperforms\nboth SFT-only and RL-only methods across challenging multimodal reasoning\nbenchmarks. The resulting models achieve state-of-the-art performance among\nopen-source MLLMs at both 3B and 7B scales, with our 7B model showing\nsubstantial improvements over base models (e.g., 66.3 %$\\rightarrow$73.4 % on\nMathVista, 62.9 %$\\rightarrow$70.4 % on We-Math) and our 3B model achieving\nperformance competitive with several 7B models. Overall, this work provides\npractical guidance for building advanced multimodal reasoning models. Our code\nis available at https://github.com/waltonfuture/RL-with-Cold-Start.\n", "link": "http://arxiv.org/abs/2505.22334v1", "date": "2025-05-28", "relevancy": 2.1177, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5349}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5283}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Multimodal%20Reasoning%20via%20Reinforcement%20Learning%20with%20Cold%0A%20%20Start&body=Title%3A%20Advancing%20Multimodal%20Reasoning%20via%20Reinforcement%20Learning%20with%20Cold%0A%20%20Start%0AAuthor%3A%20Lai%20Wei%20and%20Yuting%20Li%20and%20Kaipeng%20Zheng%20and%20Chen%20Wang%20and%20Yue%20Wang%20and%20Linghe%20Kong%20and%20Lichao%20Sun%20and%20Weiran%20Huang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%0Aimpressive%20chain-of-thought%20reasoning%20capabilities%2C%20with%20reinforcement%20learning%0A%28RL%29%20playing%20a%20crucial%20role%20in%20this%20progress.%20While%20%22aha%20moment%22%0Apatterns--where%20models%20exhibit%20self-correction%20through%20reflection--are%20often%0Aattributed%20to%20emergent%20properties%20from%20RL%2C%20we%20first%20demonstrate%20that%20these%0Apatterns%20exist%20in%20multimodal%20LLMs%20%28MLLMs%29%20prior%20to%20RL%20training%20but%20may%20not%0Anecessarily%20correlate%20with%20improved%20reasoning%20performance.%20Building%20on%20these%0Ainsights%2C%20we%20present%20a%20comprehensive%20study%20on%20enhancing%20multimodal%20reasoning%0Athrough%20a%20two-stage%20approach%3A%20%281%29%20supervised%20fine-tuning%20%28SFT%29%20as%20a%20cold%20start%0Awith%20structured%20chain-of-thought%20reasoning%20patterns%2C%20followed%20by%20%282%29%0Areinforcement%20learning%20via%20GRPO%20to%20further%20refine%20these%20capabilities.%20Our%0Aextensive%20experiments%20show%20that%20this%20combined%20approach%20consistently%20outperforms%0Aboth%20SFT-only%20and%20RL-only%20methods%20across%20challenging%20multimodal%20reasoning%0Abenchmarks.%20The%20resulting%20models%20achieve%20state-of-the-art%20performance%20among%0Aopen-source%20MLLMs%20at%20both%203B%20and%207B%20scales%2C%20with%20our%207B%20model%20showing%0Asubstantial%20improvements%20over%20base%20models%20%28e.g.%2C%2066.3%20%25%24%5Crightarrow%2473.4%20%25%20on%0AMathVista%2C%2062.9%20%25%24%5Crightarrow%2470.4%20%25%20on%20We-Math%29%20and%20our%203B%20model%20achieving%0Aperformance%20competitive%20with%20several%207B%20models.%20Overall%2C%20this%20work%20provides%0Apractical%20guidance%20for%20building%20advanced%20multimodal%20reasoning%20models.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/waltonfuture/RL-with-Cold-Start.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22334v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Multimodal%2520Reasoning%2520via%2520Reinforcement%2520Learning%2520with%2520Cold%250A%2520%2520Start%26entry.906535625%3DLai%2520Wei%2520and%2520Yuting%2520Li%2520and%2520Kaipeng%2520Zheng%2520and%2520Chen%2520Wang%2520and%2520Yue%2520Wang%2520and%2520Linghe%2520Kong%2520and%2520Lichao%2520Sun%2520and%2520Weiran%2520Huang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%250Aimpressive%2520chain-of-thought%2520reasoning%2520capabilities%252C%2520with%2520reinforcement%2520learning%250A%2528RL%2529%2520playing%2520a%2520crucial%2520role%2520in%2520this%2520progress.%2520While%2520%2522aha%2520moment%2522%250Apatterns--where%2520models%2520exhibit%2520self-correction%2520through%2520reflection--are%2520often%250Aattributed%2520to%2520emergent%2520properties%2520from%2520RL%252C%2520we%2520first%2520demonstrate%2520that%2520these%250Apatterns%2520exist%2520in%2520multimodal%2520LLMs%2520%2528MLLMs%2529%2520prior%2520to%2520RL%2520training%2520but%2520may%2520not%250Anecessarily%2520correlate%2520with%2520improved%2520reasoning%2520performance.%2520Building%2520on%2520these%250Ainsights%252C%2520we%2520present%2520a%2520comprehensive%2520study%2520on%2520enhancing%2520multimodal%2520reasoning%250Athrough%2520a%2520two-stage%2520approach%253A%2520%25281%2529%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520as%2520a%2520cold%2520start%250Awith%2520structured%2520chain-of-thought%2520reasoning%2520patterns%252C%2520followed%2520by%2520%25282%2529%250Areinforcement%2520learning%2520via%2520GRPO%2520to%2520further%2520refine%2520these%2520capabilities.%2520Our%250Aextensive%2520experiments%2520show%2520that%2520this%2520combined%2520approach%2520consistently%2520outperforms%250Aboth%2520SFT-only%2520and%2520RL-only%2520methods%2520across%2520challenging%2520multimodal%2520reasoning%250Abenchmarks.%2520The%2520resulting%2520models%2520achieve%2520state-of-the-art%2520performance%2520among%250Aopen-source%2520MLLMs%2520at%2520both%25203B%2520and%25207B%2520scales%252C%2520with%2520our%25207B%2520model%2520showing%250Asubstantial%2520improvements%2520over%2520base%2520models%2520%2528e.g.%252C%252066.3%2520%2525%2524%255Crightarrow%252473.4%2520%2525%2520on%250AMathVista%252C%252062.9%2520%2525%2524%255Crightarrow%252470.4%2520%2525%2520on%2520We-Math%2529%2520and%2520our%25203B%2520model%2520achieving%250Aperformance%2520competitive%2520with%2520several%25207B%2520models.%2520Overall%252C%2520this%2520work%2520provides%250Apractical%2520guidance%2520for%2520building%2520advanced%2520multimodal%2520reasoning%2520models.%2520Our%2520code%250Ais%2520available%2520at%2520https%253A//github.com/waltonfuture/RL-with-Cold-Start.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22334v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Multimodal%20Reasoning%20via%20Reinforcement%20Learning%20with%20Cold%0A%20%20Start&entry.906535625=Lai%20Wei%20and%20Yuting%20Li%20and%20Kaipeng%20Zheng%20and%20Chen%20Wang%20and%20Yue%20Wang%20and%20Linghe%20Kong%20and%20Lichao%20Sun%20and%20Weiran%20Huang&entry.1292438233=%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%0Aimpressive%20chain-of-thought%20reasoning%20capabilities%2C%20with%20reinforcement%20learning%0A%28RL%29%20playing%20a%20crucial%20role%20in%20this%20progress.%20While%20%22aha%20moment%22%0Apatterns--where%20models%20exhibit%20self-correction%20through%20reflection--are%20often%0Aattributed%20to%20emergent%20properties%20from%20RL%2C%20we%20first%20demonstrate%20that%20these%0Apatterns%20exist%20in%20multimodal%20LLMs%20%28MLLMs%29%20prior%20to%20RL%20training%20but%20may%20not%0Anecessarily%20correlate%20with%20improved%20reasoning%20performance.%20Building%20on%20these%0Ainsights%2C%20we%20present%20a%20comprehensive%20study%20on%20enhancing%20multimodal%20reasoning%0Athrough%20a%20two-stage%20approach%3A%20%281%29%20supervised%20fine-tuning%20%28SFT%29%20as%20a%20cold%20start%0Awith%20structured%20chain-of-thought%20reasoning%20patterns%2C%20followed%20by%20%282%29%0Areinforcement%20learning%20via%20GRPO%20to%20further%20refine%20these%20capabilities.%20Our%0Aextensive%20experiments%20show%20that%20this%20combined%20approach%20consistently%20outperforms%0Aboth%20SFT-only%20and%20RL-only%20methods%20across%20challenging%20multimodal%20reasoning%0Abenchmarks.%20The%20resulting%20models%20achieve%20state-of-the-art%20performance%20among%0Aopen-source%20MLLMs%20at%20both%203B%20and%207B%20scales%2C%20with%20our%207B%20model%20showing%0Asubstantial%20improvements%20over%20base%20models%20%28e.g.%2C%2066.3%20%25%24%5Crightarrow%2473.4%20%25%20on%0AMathVista%2C%2062.9%20%25%24%5Crightarrow%2470.4%20%25%20on%20We-Math%29%20and%20our%203B%20model%20achieving%0Aperformance%20competitive%20with%20several%207B%20models.%20Overall%2C%20this%20work%20provides%0Apractical%20guidance%20for%20building%20advanced%20multimodal%20reasoning%20models.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/waltonfuture/RL-with-Cold-Start.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22334v1&entry.124074799=Read"},
{"title": "Train with Perturbation, Infer after Merging: A Two-Stage Framework for\n  Continual Learning", "author": "Haomiao Qiu and Miao Zhang and Ziyue Qiao and Liqiang Nie", "abstract": "  Continual Learning (CL) aims to enable models to continuously acquire new\nknowledge from a sequence of tasks with avoiding the forgetting of learned\ninformation. However, existing CL methods only rely on the parameters of the\nmost recent task for inference, which makes them susceptible to catastrophic\nforgetting. Inspired by the recent success of model merging techniques, we\npropose \\textbf{Perturb-and-Merge (P\\&M)}, a novel continual learning framework\nthat integrates model merging into the CL paradigm to mitigate forgetting.\nSpecifically, after training on each task, P\\&M constructs a new model by\nforming a convex combination of the previous model and the newly trained\ntask-specific model. Through theoretical analysis, we minimize the total loss\nincrease across all tasks and derive an analytical solution for the optimal\nmerging coefficient. To further improve the performance of the merged model, we\nobserve that the degradation introduced during merging can be alleviated by a\nregularization term composed of the task vector and the Hessian matrix of the\nloss function. Interestingly, we show that this term can be efficiently\napproximated using second-order symmetric finite differences, and a stochastic\nperturbation strategy along the task vector direction is accordingly devised\nwhich incurs no additional forward or backward passes while providing an\neffective approximation of the regularization term. Finally, we combine P\\&M\nwith LoRA, a parameter-efficient fine-tuning method, to reduce memory overhead.\nOur proposed approach achieves state-of-the-art performance on several\ncontinual learning benchmark datasets.\n", "link": "http://arxiv.org/abs/2505.22389v1", "date": "2025-05-28", "relevancy": 2.106, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5529}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5203}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Train%20with%20Perturbation%2C%20Infer%20after%20Merging%3A%20A%20Two-Stage%20Framework%20for%0A%20%20Continual%20Learning&body=Title%3A%20Train%20with%20Perturbation%2C%20Infer%20after%20Merging%3A%20A%20Two-Stage%20Framework%20for%0A%20%20Continual%20Learning%0AAuthor%3A%20Haomiao%20Qiu%20and%20Miao%20Zhang%20and%20Ziyue%20Qiao%20and%20Liqiang%20Nie%0AAbstract%3A%20%20%20Continual%20Learning%20%28CL%29%20aims%20to%20enable%20models%20to%20continuously%20acquire%20new%0Aknowledge%20from%20a%20sequence%20of%20tasks%20with%20avoiding%20the%20forgetting%20of%20learned%0Ainformation.%20However%2C%20existing%20CL%20methods%20only%20rely%20on%20the%20parameters%20of%20the%0Amost%20recent%20task%20for%20inference%2C%20which%20makes%20them%20susceptible%20to%20catastrophic%0Aforgetting.%20Inspired%20by%20the%20recent%20success%20of%20model%20merging%20techniques%2C%20we%0Apropose%20%5Ctextbf%7BPerturb-and-Merge%20%28P%5C%26M%29%7D%2C%20a%20novel%20continual%20learning%20framework%0Athat%20integrates%20model%20merging%20into%20the%20CL%20paradigm%20to%20mitigate%20forgetting.%0ASpecifically%2C%20after%20training%20on%20each%20task%2C%20P%5C%26M%20constructs%20a%20new%20model%20by%0Aforming%20a%20convex%20combination%20of%20the%20previous%20model%20and%20the%20newly%20trained%0Atask-specific%20model.%20Through%20theoretical%20analysis%2C%20we%20minimize%20the%20total%20loss%0Aincrease%20across%20all%20tasks%20and%20derive%20an%20analytical%20solution%20for%20the%20optimal%0Amerging%20coefficient.%20To%20further%20improve%20the%20performance%20of%20the%20merged%20model%2C%20we%0Aobserve%20that%20the%20degradation%20introduced%20during%20merging%20can%20be%20alleviated%20by%20a%0Aregularization%20term%20composed%20of%20the%20task%20vector%20and%20the%20Hessian%20matrix%20of%20the%0Aloss%20function.%20Interestingly%2C%20we%20show%20that%20this%20term%20can%20be%20efficiently%0Aapproximated%20using%20second-order%20symmetric%20finite%20differences%2C%20and%20a%20stochastic%0Aperturbation%20strategy%20along%20the%20task%20vector%20direction%20is%20accordingly%20devised%0Awhich%20incurs%20no%20additional%20forward%20or%20backward%20passes%20while%20providing%20an%0Aeffective%20approximation%20of%20the%20regularization%20term.%20Finally%2C%20we%20combine%20P%5C%26M%0Awith%20LoRA%2C%20a%20parameter-efficient%20fine-tuning%20method%2C%20to%20reduce%20memory%20overhead.%0AOur%20proposed%20approach%20achieves%20state-of-the-art%20performance%20on%20several%0Acontinual%20learning%20benchmark%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22389v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrain%2520with%2520Perturbation%252C%2520Infer%2520after%2520Merging%253A%2520A%2520Two-Stage%2520Framework%2520for%250A%2520%2520Continual%2520Learning%26entry.906535625%3DHaomiao%2520Qiu%2520and%2520Miao%2520Zhang%2520and%2520Ziyue%2520Qiao%2520and%2520Liqiang%2520Nie%26entry.1292438233%3D%2520%2520Continual%2520Learning%2520%2528CL%2529%2520aims%2520to%2520enable%2520models%2520to%2520continuously%2520acquire%2520new%250Aknowledge%2520from%2520a%2520sequence%2520of%2520tasks%2520with%2520avoiding%2520the%2520forgetting%2520of%2520learned%250Ainformation.%2520However%252C%2520existing%2520CL%2520methods%2520only%2520rely%2520on%2520the%2520parameters%2520of%2520the%250Amost%2520recent%2520task%2520for%2520inference%252C%2520which%2520makes%2520them%2520susceptible%2520to%2520catastrophic%250Aforgetting.%2520Inspired%2520by%2520the%2520recent%2520success%2520of%2520model%2520merging%2520techniques%252C%2520we%250Apropose%2520%255Ctextbf%257BPerturb-and-Merge%2520%2528P%255C%2526M%2529%257D%252C%2520a%2520novel%2520continual%2520learning%2520framework%250Athat%2520integrates%2520model%2520merging%2520into%2520the%2520CL%2520paradigm%2520to%2520mitigate%2520forgetting.%250ASpecifically%252C%2520after%2520training%2520on%2520each%2520task%252C%2520P%255C%2526M%2520constructs%2520a%2520new%2520model%2520by%250Aforming%2520a%2520convex%2520combination%2520of%2520the%2520previous%2520model%2520and%2520the%2520newly%2520trained%250Atask-specific%2520model.%2520Through%2520theoretical%2520analysis%252C%2520we%2520minimize%2520the%2520total%2520loss%250Aincrease%2520across%2520all%2520tasks%2520and%2520derive%2520an%2520analytical%2520solution%2520for%2520the%2520optimal%250Amerging%2520coefficient.%2520To%2520further%2520improve%2520the%2520performance%2520of%2520the%2520merged%2520model%252C%2520we%250Aobserve%2520that%2520the%2520degradation%2520introduced%2520during%2520merging%2520can%2520be%2520alleviated%2520by%2520a%250Aregularization%2520term%2520composed%2520of%2520the%2520task%2520vector%2520and%2520the%2520Hessian%2520matrix%2520of%2520the%250Aloss%2520function.%2520Interestingly%252C%2520we%2520show%2520that%2520this%2520term%2520can%2520be%2520efficiently%250Aapproximated%2520using%2520second-order%2520symmetric%2520finite%2520differences%252C%2520and%2520a%2520stochastic%250Aperturbation%2520strategy%2520along%2520the%2520task%2520vector%2520direction%2520is%2520accordingly%2520devised%250Awhich%2520incurs%2520no%2520additional%2520forward%2520or%2520backward%2520passes%2520while%2520providing%2520an%250Aeffective%2520approximation%2520of%2520the%2520regularization%2520term.%2520Finally%252C%2520we%2520combine%2520P%255C%2526M%250Awith%2520LoRA%252C%2520a%2520parameter-efficient%2520fine-tuning%2520method%252C%2520to%2520reduce%2520memory%2520overhead.%250AOur%2520proposed%2520approach%2520achieves%2520state-of-the-art%2520performance%2520on%2520several%250Acontinual%2520learning%2520benchmark%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22389v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Train%20with%20Perturbation%2C%20Infer%20after%20Merging%3A%20A%20Two-Stage%20Framework%20for%0A%20%20Continual%20Learning&entry.906535625=Haomiao%20Qiu%20and%20Miao%20Zhang%20and%20Ziyue%20Qiao%20and%20Liqiang%20Nie&entry.1292438233=%20%20Continual%20Learning%20%28CL%29%20aims%20to%20enable%20models%20to%20continuously%20acquire%20new%0Aknowledge%20from%20a%20sequence%20of%20tasks%20with%20avoiding%20the%20forgetting%20of%20learned%0Ainformation.%20However%2C%20existing%20CL%20methods%20only%20rely%20on%20the%20parameters%20of%20the%0Amost%20recent%20task%20for%20inference%2C%20which%20makes%20them%20susceptible%20to%20catastrophic%0Aforgetting.%20Inspired%20by%20the%20recent%20success%20of%20model%20merging%20techniques%2C%20we%0Apropose%20%5Ctextbf%7BPerturb-and-Merge%20%28P%5C%26M%29%7D%2C%20a%20novel%20continual%20learning%20framework%0Athat%20integrates%20model%20merging%20into%20the%20CL%20paradigm%20to%20mitigate%20forgetting.%0ASpecifically%2C%20after%20training%20on%20each%20task%2C%20P%5C%26M%20constructs%20a%20new%20model%20by%0Aforming%20a%20convex%20combination%20of%20the%20previous%20model%20and%20the%20newly%20trained%0Atask-specific%20model.%20Through%20theoretical%20analysis%2C%20we%20minimize%20the%20total%20loss%0Aincrease%20across%20all%20tasks%20and%20derive%20an%20analytical%20solution%20for%20the%20optimal%0Amerging%20coefficient.%20To%20further%20improve%20the%20performance%20of%20the%20merged%20model%2C%20we%0Aobserve%20that%20the%20degradation%20introduced%20during%20merging%20can%20be%20alleviated%20by%20a%0Aregularization%20term%20composed%20of%20the%20task%20vector%20and%20the%20Hessian%20matrix%20of%20the%0Aloss%20function.%20Interestingly%2C%20we%20show%20that%20this%20term%20can%20be%20efficiently%0Aapproximated%20using%20second-order%20symmetric%20finite%20differences%2C%20and%20a%20stochastic%0Aperturbation%20strategy%20along%20the%20task%20vector%20direction%20is%20accordingly%20devised%0Awhich%20incurs%20no%20additional%20forward%20or%20backward%20passes%20while%20providing%20an%0Aeffective%20approximation%20of%20the%20regularization%20term.%20Finally%2C%20we%20combine%20P%5C%26M%0Awith%20LoRA%2C%20a%20parameter-efficient%20fine-tuning%20method%2C%20to%20reduce%20memory%20overhead.%0AOur%20proposed%20approach%20achieves%20state-of-the-art%20performance%20on%20several%0Acontinual%20learning%20benchmark%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22389v1&entry.124074799=Read"},
{"title": "FastTD3: Simple, Fast, and Capable Reinforcement Learning for Humanoid\n  Control", "author": "Younggyo Seo and Carmelo Sferrazza and Haoran Geng and Michal Nauman and Zhao-Heng Yin and Pieter Abbeel", "abstract": "  Reinforcement learning (RL) has driven significant progress in robotics, but\nits complexity and long training times remain major bottlenecks. In this\nreport, we introduce FastTD3, a simple, fast, and capable RL algorithm that\nsignificantly speeds up training for humanoid robots in popular suites such as\nHumanoidBench, IsaacLab, and MuJoCo Playground. Our recipe is remarkably\nsimple: we train an off-policy TD3 agent with several modifications -- parallel\nsimulation, large-batch updates, a distributional critic, and carefully tuned\nhyperparameters. FastTD3 solves a range of HumanoidBench tasks in under 3 hours\non a single A100 GPU, while remaining stable during training. We also provide a\nlightweight and easy-to-use implementation of FastTD3 to accelerate RL research\nin robotics.\n", "link": "http://arxiv.org/abs/2505.22642v1", "date": "2025-05-28", "relevancy": 2.1042, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5403}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5199}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FastTD3%3A%20Simple%2C%20Fast%2C%20and%20Capable%20Reinforcement%20Learning%20for%20Humanoid%0A%20%20Control&body=Title%3A%20FastTD3%3A%20Simple%2C%20Fast%2C%20and%20Capable%20Reinforcement%20Learning%20for%20Humanoid%0A%20%20Control%0AAuthor%3A%20Younggyo%20Seo%20and%20Carmelo%20Sferrazza%20and%20Haoran%20Geng%20and%20Michal%20Nauman%20and%20Zhao-Heng%20Yin%20and%20Pieter%20Abbeel%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20has%20driven%20significant%20progress%20in%20robotics%2C%20but%0Aits%20complexity%20and%20long%20training%20times%20remain%20major%20bottlenecks.%20In%20this%0Areport%2C%20we%20introduce%20FastTD3%2C%20a%20simple%2C%20fast%2C%20and%20capable%20RL%20algorithm%20that%0Asignificantly%20speeds%20up%20training%20for%20humanoid%20robots%20in%20popular%20suites%20such%20as%0AHumanoidBench%2C%20IsaacLab%2C%20and%20MuJoCo%20Playground.%20Our%20recipe%20is%20remarkably%0Asimple%3A%20we%20train%20an%20off-policy%20TD3%20agent%20with%20several%20modifications%20--%20parallel%0Asimulation%2C%20large-batch%20updates%2C%20a%20distributional%20critic%2C%20and%20carefully%20tuned%0Ahyperparameters.%20FastTD3%20solves%20a%20range%20of%20HumanoidBench%20tasks%20in%20under%203%20hours%0Aon%20a%20single%20A100%20GPU%2C%20while%20remaining%20stable%20during%20training.%20We%20also%20provide%20a%0Alightweight%20and%20easy-to-use%20implementation%20of%20FastTD3%20to%20accelerate%20RL%20research%0Ain%20robotics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22642v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFastTD3%253A%2520Simple%252C%2520Fast%252C%2520and%2520Capable%2520Reinforcement%2520Learning%2520for%2520Humanoid%250A%2520%2520Control%26entry.906535625%3DYounggyo%2520Seo%2520and%2520Carmelo%2520Sferrazza%2520and%2520Haoran%2520Geng%2520and%2520Michal%2520Nauman%2520and%2520Zhao-Heng%2520Yin%2520and%2520Pieter%2520Abbeel%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520has%2520driven%2520significant%2520progress%2520in%2520robotics%252C%2520but%250Aits%2520complexity%2520and%2520long%2520training%2520times%2520remain%2520major%2520bottlenecks.%2520In%2520this%250Areport%252C%2520we%2520introduce%2520FastTD3%252C%2520a%2520simple%252C%2520fast%252C%2520and%2520capable%2520RL%2520algorithm%2520that%250Asignificantly%2520speeds%2520up%2520training%2520for%2520humanoid%2520robots%2520in%2520popular%2520suites%2520such%2520as%250AHumanoidBench%252C%2520IsaacLab%252C%2520and%2520MuJoCo%2520Playground.%2520Our%2520recipe%2520is%2520remarkably%250Asimple%253A%2520we%2520train%2520an%2520off-policy%2520TD3%2520agent%2520with%2520several%2520modifications%2520--%2520parallel%250Asimulation%252C%2520large-batch%2520updates%252C%2520a%2520distributional%2520critic%252C%2520and%2520carefully%2520tuned%250Ahyperparameters.%2520FastTD3%2520solves%2520a%2520range%2520of%2520HumanoidBench%2520tasks%2520in%2520under%25203%2520hours%250Aon%2520a%2520single%2520A100%2520GPU%252C%2520while%2520remaining%2520stable%2520during%2520training.%2520We%2520also%2520provide%2520a%250Alightweight%2520and%2520easy-to-use%2520implementation%2520of%2520FastTD3%2520to%2520accelerate%2520RL%2520research%250Ain%2520robotics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22642v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FastTD3%3A%20Simple%2C%20Fast%2C%20and%20Capable%20Reinforcement%20Learning%20for%20Humanoid%0A%20%20Control&entry.906535625=Younggyo%20Seo%20and%20Carmelo%20Sferrazza%20and%20Haoran%20Geng%20and%20Michal%20Nauman%20and%20Zhao-Heng%20Yin%20and%20Pieter%20Abbeel&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20has%20driven%20significant%20progress%20in%20robotics%2C%20but%0Aits%20complexity%20and%20long%20training%20times%20remain%20major%20bottlenecks.%20In%20this%0Areport%2C%20we%20introduce%20FastTD3%2C%20a%20simple%2C%20fast%2C%20and%20capable%20RL%20algorithm%20that%0Asignificantly%20speeds%20up%20training%20for%20humanoid%20robots%20in%20popular%20suites%20such%20as%0AHumanoidBench%2C%20IsaacLab%2C%20and%20MuJoCo%20Playground.%20Our%20recipe%20is%20remarkably%0Asimple%3A%20we%20train%20an%20off-policy%20TD3%20agent%20with%20several%20modifications%20--%20parallel%0Asimulation%2C%20large-batch%20updates%2C%20a%20distributional%20critic%2C%20and%20carefully%20tuned%0Ahyperparameters.%20FastTD3%20solves%20a%20range%20of%20HumanoidBench%20tasks%20in%20under%203%20hours%0Aon%20a%20single%20A100%20GPU%2C%20while%20remaining%20stable%20during%20training.%20We%20also%20provide%20a%0Alightweight%20and%20easy-to-use%20implementation%20of%20FastTD3%20to%20accelerate%20RL%20research%0Ain%20robotics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22642v1&entry.124074799=Read"},
{"title": "Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and\n  Beyond", "author": "Liang Wen and Yunke Cai and Fenrui Xiao and Xin He and Qi An and Zhenyu Duan and Yimin Du and Junchen Liu and Lifu Tang and Xiaowei Lv and Haosheng Zou and Yongchao Deng and Shousheng Jia and Xiangzheng Zhang", "abstract": "  This paper introduces Light-R1, an open-source suite for training long\nreasoning models using reproducible and cost-effective methodology. Given the\nproprietary nature of data used in the DeepSeek-R1 series, we develop an\nalternative approach leveraging exclusively public data and models. Our\ncurriculum training progressively increases data difficulty, combined with\nmulti-staged post-training. Our Light-R1-32B model, trained from\nQwen2.5-32B-Instruct, outperforms DeepSeek-R1-Distill-Qwen-32B in math\nreasoning.\n  Experimental results show that this curriculum approach becomes more\neffective when distinct, diverse datasets are available for different training\nstages: fine-tuning DeepSeek-R1-Distilled models (pre-tuned by DeepSeek team on\nproprietary data) with 3,000 challenging examples from our curriculum dataset\nyielded state-of-the-art 7B and 14B models, while the 32B model,\nLight-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.\n  Furthermore, we extend our work by applying GRPO on long reasoning models.\nOur final Light-R1-14B-DS achieves SOTA performance among 14B models in math,\nwith AIME24 & 25 scores of 74.0 and 60.2 respectively, surpassing many 32B\nmodels and DeepSeek-R1-Distill-Llama-70B. Despite math-focused training,\nLight-R1-14B-DS demonstrates strong cross-domain generalization.\n  Light-R1 represents a significant advancement in making sophisticated\nreasoning models more accessible and implementable in real-world applications.\nOur models, training data and code have been made available at\nhttps://github.com/Qihoo360/Light-R1.\n", "link": "http://arxiv.org/abs/2503.10460v4", "date": "2025-05-28", "relevancy": 2.1039, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5307}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5307}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Light-R1%3A%20Curriculum%20SFT%2C%20DPO%20and%20RL%20for%20Long%20COT%20from%20Scratch%20and%0A%20%20Beyond&body=Title%3A%20Light-R1%3A%20Curriculum%20SFT%2C%20DPO%20and%20RL%20for%20Long%20COT%20from%20Scratch%20and%0A%20%20Beyond%0AAuthor%3A%20Liang%20Wen%20and%20Yunke%20Cai%20and%20Fenrui%20Xiao%20and%20Xin%20He%20and%20Qi%20An%20and%20Zhenyu%20Duan%20and%20Yimin%20Du%20and%20Junchen%20Liu%20and%20Lifu%20Tang%20and%20Xiaowei%20Lv%20and%20Haosheng%20Zou%20and%20Yongchao%20Deng%20and%20Shousheng%20Jia%20and%20Xiangzheng%20Zhang%0AAbstract%3A%20%20%20This%20paper%20introduces%20Light-R1%2C%20an%20open-source%20suite%20for%20training%20long%0Areasoning%20models%20using%20reproducible%20and%20cost-effective%20methodology.%20Given%20the%0Aproprietary%20nature%20of%20data%20used%20in%20the%20DeepSeek-R1%20series%2C%20we%20develop%20an%0Aalternative%20approach%20leveraging%20exclusively%20public%20data%20and%20models.%20Our%0Acurriculum%20training%20progressively%20increases%20data%20difficulty%2C%20combined%20with%0Amulti-staged%20post-training.%20Our%20Light-R1-32B%20model%2C%20trained%20from%0AQwen2.5-32B-Instruct%2C%20outperforms%20DeepSeek-R1-Distill-Qwen-32B%20in%20math%0Areasoning.%0A%20%20Experimental%20results%20show%20that%20this%20curriculum%20approach%20becomes%20more%0Aeffective%20when%20distinct%2C%20diverse%20datasets%20are%20available%20for%20different%20training%0Astages%3A%20fine-tuning%20DeepSeek-R1-Distilled%20models%20%28pre-tuned%20by%20DeepSeek%20team%20on%0Aproprietary%20data%29%20with%203%2C000%20challenging%20examples%20from%20our%20curriculum%20dataset%0Ayielded%20state-of-the-art%207B%20and%2014B%20models%2C%20while%20the%2032B%20model%2C%0ALight-R1-32B-DS%20performed%20comparably%20to%20QwQ-32B%20and%20DeepSeek-R1.%0A%20%20Furthermore%2C%20we%20extend%20our%20work%20by%20applying%20GRPO%20on%20long%20reasoning%20models.%0AOur%20final%20Light-R1-14B-DS%20achieves%20SOTA%20performance%20among%2014B%20models%20in%20math%2C%0Awith%20AIME24%20%26%2025%20scores%20of%2074.0%20and%2060.2%20respectively%2C%20surpassing%20many%2032B%0Amodels%20and%20DeepSeek-R1-Distill-Llama-70B.%20Despite%20math-focused%20training%2C%0ALight-R1-14B-DS%20demonstrates%20strong%20cross-domain%20generalization.%0A%20%20Light-R1%20represents%20a%20significant%20advancement%20in%20making%20sophisticated%0Areasoning%20models%20more%20accessible%20and%20implementable%20in%20real-world%20applications.%0AOur%20models%2C%20training%20data%20and%20code%20have%20been%20made%20available%20at%0Ahttps%3A//github.com/Qihoo360/Light-R1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.10460v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLight-R1%253A%2520Curriculum%2520SFT%252C%2520DPO%2520and%2520RL%2520for%2520Long%2520COT%2520from%2520Scratch%2520and%250A%2520%2520Beyond%26entry.906535625%3DLiang%2520Wen%2520and%2520Yunke%2520Cai%2520and%2520Fenrui%2520Xiao%2520and%2520Xin%2520He%2520and%2520Qi%2520An%2520and%2520Zhenyu%2520Duan%2520and%2520Yimin%2520Du%2520and%2520Junchen%2520Liu%2520and%2520Lifu%2520Tang%2520and%2520Xiaowei%2520Lv%2520and%2520Haosheng%2520Zou%2520and%2520Yongchao%2520Deng%2520and%2520Shousheng%2520Jia%2520and%2520Xiangzheng%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520Light-R1%252C%2520an%2520open-source%2520suite%2520for%2520training%2520long%250Areasoning%2520models%2520using%2520reproducible%2520and%2520cost-effective%2520methodology.%2520Given%2520the%250Aproprietary%2520nature%2520of%2520data%2520used%2520in%2520the%2520DeepSeek-R1%2520series%252C%2520we%2520develop%2520an%250Aalternative%2520approach%2520leveraging%2520exclusively%2520public%2520data%2520and%2520models.%2520Our%250Acurriculum%2520training%2520progressively%2520increases%2520data%2520difficulty%252C%2520combined%2520with%250Amulti-staged%2520post-training.%2520Our%2520Light-R1-32B%2520model%252C%2520trained%2520from%250AQwen2.5-32B-Instruct%252C%2520outperforms%2520DeepSeek-R1-Distill-Qwen-32B%2520in%2520math%250Areasoning.%250A%2520%2520Experimental%2520results%2520show%2520that%2520this%2520curriculum%2520approach%2520becomes%2520more%250Aeffective%2520when%2520distinct%252C%2520diverse%2520datasets%2520are%2520available%2520for%2520different%2520training%250Astages%253A%2520fine-tuning%2520DeepSeek-R1-Distilled%2520models%2520%2528pre-tuned%2520by%2520DeepSeek%2520team%2520on%250Aproprietary%2520data%2529%2520with%25203%252C000%2520challenging%2520examples%2520from%2520our%2520curriculum%2520dataset%250Ayielded%2520state-of-the-art%25207B%2520and%252014B%2520models%252C%2520while%2520the%252032B%2520model%252C%250ALight-R1-32B-DS%2520performed%2520comparably%2520to%2520QwQ-32B%2520and%2520DeepSeek-R1.%250A%2520%2520Furthermore%252C%2520we%2520extend%2520our%2520work%2520by%2520applying%2520GRPO%2520on%2520long%2520reasoning%2520models.%250AOur%2520final%2520Light-R1-14B-DS%2520achieves%2520SOTA%2520performance%2520among%252014B%2520models%2520in%2520math%252C%250Awith%2520AIME24%2520%2526%252025%2520scores%2520of%252074.0%2520and%252060.2%2520respectively%252C%2520surpassing%2520many%252032B%250Amodels%2520and%2520DeepSeek-R1-Distill-Llama-70B.%2520Despite%2520math-focused%2520training%252C%250ALight-R1-14B-DS%2520demonstrates%2520strong%2520cross-domain%2520generalization.%250A%2520%2520Light-R1%2520represents%2520a%2520significant%2520advancement%2520in%2520making%2520sophisticated%250Areasoning%2520models%2520more%2520accessible%2520and%2520implementable%2520in%2520real-world%2520applications.%250AOur%2520models%252C%2520training%2520data%2520and%2520code%2520have%2520been%2520made%2520available%2520at%250Ahttps%253A//github.com/Qihoo360/Light-R1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.10460v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Light-R1%3A%20Curriculum%20SFT%2C%20DPO%20and%20RL%20for%20Long%20COT%20from%20Scratch%20and%0A%20%20Beyond&entry.906535625=Liang%20Wen%20and%20Yunke%20Cai%20and%20Fenrui%20Xiao%20and%20Xin%20He%20and%20Qi%20An%20and%20Zhenyu%20Duan%20and%20Yimin%20Du%20and%20Junchen%20Liu%20and%20Lifu%20Tang%20and%20Xiaowei%20Lv%20and%20Haosheng%20Zou%20and%20Yongchao%20Deng%20and%20Shousheng%20Jia%20and%20Xiangzheng%20Zhang&entry.1292438233=%20%20This%20paper%20introduces%20Light-R1%2C%20an%20open-source%20suite%20for%20training%20long%0Areasoning%20models%20using%20reproducible%20and%20cost-effective%20methodology.%20Given%20the%0Aproprietary%20nature%20of%20data%20used%20in%20the%20DeepSeek-R1%20series%2C%20we%20develop%20an%0Aalternative%20approach%20leveraging%20exclusively%20public%20data%20and%20models.%20Our%0Acurriculum%20training%20progressively%20increases%20data%20difficulty%2C%20combined%20with%0Amulti-staged%20post-training.%20Our%20Light-R1-32B%20model%2C%20trained%20from%0AQwen2.5-32B-Instruct%2C%20outperforms%20DeepSeek-R1-Distill-Qwen-32B%20in%20math%0Areasoning.%0A%20%20Experimental%20results%20show%20that%20this%20curriculum%20approach%20becomes%20more%0Aeffective%20when%20distinct%2C%20diverse%20datasets%20are%20available%20for%20different%20training%0Astages%3A%20fine-tuning%20DeepSeek-R1-Distilled%20models%20%28pre-tuned%20by%20DeepSeek%20team%20on%0Aproprietary%20data%29%20with%203%2C000%20challenging%20examples%20from%20our%20curriculum%20dataset%0Ayielded%20state-of-the-art%207B%20and%2014B%20models%2C%20while%20the%2032B%20model%2C%0ALight-R1-32B-DS%20performed%20comparably%20to%20QwQ-32B%20and%20DeepSeek-R1.%0A%20%20Furthermore%2C%20we%20extend%20our%20work%20by%20applying%20GRPO%20on%20long%20reasoning%20models.%0AOur%20final%20Light-R1-14B-DS%20achieves%20SOTA%20performance%20among%2014B%20models%20in%20math%2C%0Awith%20AIME24%20%26%2025%20scores%20of%2074.0%20and%2060.2%20respectively%2C%20surpassing%20many%2032B%0Amodels%20and%20DeepSeek-R1-Distill-Llama-70B.%20Despite%20math-focused%20training%2C%0ALight-R1-14B-DS%20demonstrates%20strong%20cross-domain%20generalization.%0A%20%20Light-R1%20represents%20a%20significant%20advancement%20in%20making%20sophisticated%0Areasoning%20models%20more%20accessible%20and%20implementable%20in%20real-world%20applications.%0AOur%20models%2C%20training%20data%20and%20code%20have%20been%20made%20available%20at%0Ahttps%3A//github.com/Qihoo360/Light-R1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.10460v4&entry.124074799=Read"},
{"title": "Adjoint Sampling: Highly Scalable Diffusion Samplers via Adjoint\n  Matching", "author": "Aaron Havens and Benjamin Kurt Miller and Bing Yan and Carles Domingo-Enrich and Anuroop Sriram and Brandon Wood and Daniel Levine and Bin Hu and Brandon Amos and Brian Karrer and Xiang Fu and Guan-Horng Liu and Ricky T. Q. Chen", "abstract": "  We introduce Adjoint Sampling, a highly scalable and efficient algorithm for\nlearning diffusion processes that sample from unnormalized densities, or energy\nfunctions. It is the first on-policy approach that allows significantly more\ngradient updates than the number of energy evaluations and model samples,\nallowing us to scale to much larger problem settings than previously explored\nby similar methods. Our framework is theoretically grounded in stochastic\noptimal control and shares the same theoretical guarantees as Adjoint Matching,\nbeing able to train without the need for corrective measures that push samples\ntowards the target distribution. We show how to incorporate key symmetries, as\nwell as periodic boundary conditions, for modeling molecules in both cartesian\nand torsional coordinates. We demonstrate the effectiveness of our approach\nthrough extensive experiments on classical energy functions, and further scale\nup to neural network-based energy models where we perform amortized conformer\ngeneration across many molecular systems. To encourage further research in\ndeveloping highly scalable sampling methods, we plan to open source these\nchallenging benchmarks, where successful methods can directly impact progress\nin computational chemistry.\n", "link": "http://arxiv.org/abs/2504.11713v3", "date": "2025-05-28", "relevancy": 2.1013, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5308}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5222}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adjoint%20Sampling%3A%20Highly%20Scalable%20Diffusion%20Samplers%20via%20Adjoint%0A%20%20Matching&body=Title%3A%20Adjoint%20Sampling%3A%20Highly%20Scalable%20Diffusion%20Samplers%20via%20Adjoint%0A%20%20Matching%0AAuthor%3A%20Aaron%20Havens%20and%20Benjamin%20Kurt%20Miller%20and%20Bing%20Yan%20and%20Carles%20Domingo-Enrich%20and%20Anuroop%20Sriram%20and%20Brandon%20Wood%20and%20Daniel%20Levine%20and%20Bin%20Hu%20and%20Brandon%20Amos%20and%20Brian%20Karrer%20and%20Xiang%20Fu%20and%20Guan-Horng%20Liu%20and%20Ricky%20T.%20Q.%20Chen%0AAbstract%3A%20%20%20We%20introduce%20Adjoint%20Sampling%2C%20a%20highly%20scalable%20and%20efficient%20algorithm%20for%0Alearning%20diffusion%20processes%20that%20sample%20from%20unnormalized%20densities%2C%20or%20energy%0Afunctions.%20It%20is%20the%20first%20on-policy%20approach%20that%20allows%20significantly%20more%0Agradient%20updates%20than%20the%20number%20of%20energy%20evaluations%20and%20model%20samples%2C%0Aallowing%20us%20to%20scale%20to%20much%20larger%20problem%20settings%20than%20previously%20explored%0Aby%20similar%20methods.%20Our%20framework%20is%20theoretically%20grounded%20in%20stochastic%0Aoptimal%20control%20and%20shares%20the%20same%20theoretical%20guarantees%20as%20Adjoint%20Matching%2C%0Abeing%20able%20to%20train%20without%20the%20need%20for%20corrective%20measures%20that%20push%20samples%0Atowards%20the%20target%20distribution.%20We%20show%20how%20to%20incorporate%20key%20symmetries%2C%20as%0Awell%20as%20periodic%20boundary%20conditions%2C%20for%20modeling%20molecules%20in%20both%20cartesian%0Aand%20torsional%20coordinates.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%0Athrough%20extensive%20experiments%20on%20classical%20energy%20functions%2C%20and%20further%20scale%0Aup%20to%20neural%20network-based%20energy%20models%20where%20we%20perform%20amortized%20conformer%0Ageneration%20across%20many%20molecular%20systems.%20To%20encourage%20further%20research%20in%0Adeveloping%20highly%20scalable%20sampling%20methods%2C%20we%20plan%20to%20open%20source%20these%0Achallenging%20benchmarks%2C%20where%20successful%20methods%20can%20directly%20impact%20progress%0Ain%20computational%20chemistry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.11713v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdjoint%2520Sampling%253A%2520Highly%2520Scalable%2520Diffusion%2520Samplers%2520via%2520Adjoint%250A%2520%2520Matching%26entry.906535625%3DAaron%2520Havens%2520and%2520Benjamin%2520Kurt%2520Miller%2520and%2520Bing%2520Yan%2520and%2520Carles%2520Domingo-Enrich%2520and%2520Anuroop%2520Sriram%2520and%2520Brandon%2520Wood%2520and%2520Daniel%2520Levine%2520and%2520Bin%2520Hu%2520and%2520Brandon%2520Amos%2520and%2520Brian%2520Karrer%2520and%2520Xiang%2520Fu%2520and%2520Guan-Horng%2520Liu%2520and%2520Ricky%2520T.%2520Q.%2520Chen%26entry.1292438233%3D%2520%2520We%2520introduce%2520Adjoint%2520Sampling%252C%2520a%2520highly%2520scalable%2520and%2520efficient%2520algorithm%2520for%250Alearning%2520diffusion%2520processes%2520that%2520sample%2520from%2520unnormalized%2520densities%252C%2520or%2520energy%250Afunctions.%2520It%2520is%2520the%2520first%2520on-policy%2520approach%2520that%2520allows%2520significantly%2520more%250Agradient%2520updates%2520than%2520the%2520number%2520of%2520energy%2520evaluations%2520and%2520model%2520samples%252C%250Aallowing%2520us%2520to%2520scale%2520to%2520much%2520larger%2520problem%2520settings%2520than%2520previously%2520explored%250Aby%2520similar%2520methods.%2520Our%2520framework%2520is%2520theoretically%2520grounded%2520in%2520stochastic%250Aoptimal%2520control%2520and%2520shares%2520the%2520same%2520theoretical%2520guarantees%2520as%2520Adjoint%2520Matching%252C%250Abeing%2520able%2520to%2520train%2520without%2520the%2520need%2520for%2520corrective%2520measures%2520that%2520push%2520samples%250Atowards%2520the%2520target%2520distribution.%2520We%2520show%2520how%2520to%2520incorporate%2520key%2520symmetries%252C%2520as%250Awell%2520as%2520periodic%2520boundary%2520conditions%252C%2520for%2520modeling%2520molecules%2520in%2520both%2520cartesian%250Aand%2520torsional%2520coordinates.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%250Athrough%2520extensive%2520experiments%2520on%2520classical%2520energy%2520functions%252C%2520and%2520further%2520scale%250Aup%2520to%2520neural%2520network-based%2520energy%2520models%2520where%2520we%2520perform%2520amortized%2520conformer%250Ageneration%2520across%2520many%2520molecular%2520systems.%2520To%2520encourage%2520further%2520research%2520in%250Adeveloping%2520highly%2520scalable%2520sampling%2520methods%252C%2520we%2520plan%2520to%2520open%2520source%2520these%250Achallenging%2520benchmarks%252C%2520where%2520successful%2520methods%2520can%2520directly%2520impact%2520progress%250Ain%2520computational%2520chemistry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.11713v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adjoint%20Sampling%3A%20Highly%20Scalable%20Diffusion%20Samplers%20via%20Adjoint%0A%20%20Matching&entry.906535625=Aaron%20Havens%20and%20Benjamin%20Kurt%20Miller%20and%20Bing%20Yan%20and%20Carles%20Domingo-Enrich%20and%20Anuroop%20Sriram%20and%20Brandon%20Wood%20and%20Daniel%20Levine%20and%20Bin%20Hu%20and%20Brandon%20Amos%20and%20Brian%20Karrer%20and%20Xiang%20Fu%20and%20Guan-Horng%20Liu%20and%20Ricky%20T.%20Q.%20Chen&entry.1292438233=%20%20We%20introduce%20Adjoint%20Sampling%2C%20a%20highly%20scalable%20and%20efficient%20algorithm%20for%0Alearning%20diffusion%20processes%20that%20sample%20from%20unnormalized%20densities%2C%20or%20energy%0Afunctions.%20It%20is%20the%20first%20on-policy%20approach%20that%20allows%20significantly%20more%0Agradient%20updates%20than%20the%20number%20of%20energy%20evaluations%20and%20model%20samples%2C%0Aallowing%20us%20to%20scale%20to%20much%20larger%20problem%20settings%20than%20previously%20explored%0Aby%20similar%20methods.%20Our%20framework%20is%20theoretically%20grounded%20in%20stochastic%0Aoptimal%20control%20and%20shares%20the%20same%20theoretical%20guarantees%20as%20Adjoint%20Matching%2C%0Abeing%20able%20to%20train%20without%20the%20need%20for%20corrective%20measures%20that%20push%20samples%0Atowards%20the%20target%20distribution.%20We%20show%20how%20to%20incorporate%20key%20symmetries%2C%20as%0Awell%20as%20periodic%20boundary%20conditions%2C%20for%20modeling%20molecules%20in%20both%20cartesian%0Aand%20torsional%20coordinates.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%0Athrough%20extensive%20experiments%20on%20classical%20energy%20functions%2C%20and%20further%20scale%0Aup%20to%20neural%20network-based%20energy%20models%20where%20we%20perform%20amortized%20conformer%0Ageneration%20across%20many%20molecular%20systems.%20To%20encourage%20further%20research%20in%0Adeveloping%20highly%20scalable%20sampling%20methods%2C%20we%20plan%20to%20open%20source%20these%0Achallenging%20benchmarks%2C%20where%20successful%20methods%20can%20directly%20impact%20progress%0Ain%20computational%20chemistry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.11713v3&entry.124074799=Read"},
{"title": "Fully Heteroscedastic Count Regression with Deep Double Poisson Networks", "author": "Spencer Young and Porter Jenkins and Longchao Da and Jeff Dotson and Hua Wei", "abstract": "  Neural networks capable of accurate, input-conditional uncertainty\nrepresentation are essential for real-world AI systems. Deep ensembles of\nGaussian networks have proven highly effective for continuous regression due to\ntheir ability to flexibly represent aleatoric uncertainty via unrestricted\nheteroscedastic variance, which in turn enables accurate epistemic uncertainty\nestimation. However, no analogous approach exists for count regression, despite\nmany important applications. To address this gap, we propose the Deep Double\nPoisson Network (DDPN), a novel neural discrete count regression model that\noutputs the parameters of the Double Poisson distribution, enabling arbitrarily\nhigh or low predictive aleatoric uncertainty for count data and improving\nepistemic uncertainty estimation when ensembled. We formalize and prove that\nDDPN exhibits robust regression properties similar to heteroscedastic Gaussian\nmodels via learnable loss attenuation, and introduce a simple loss modification\nto control this behavior. Experiments on diverse datasets demonstrate that DDPN\noutperforms current baselines in accuracy, calibration, and out-of-distribution\ndetection, establishing a new state-of-the-art in deep count regression.\n", "link": "http://arxiv.org/abs/2406.09262v4", "date": "2025-05-28", "relevancy": 2.0945, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5598}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4991}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4945}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fully%20Heteroscedastic%20Count%20Regression%20with%20Deep%20Double%20Poisson%20Networks&body=Title%3A%20Fully%20Heteroscedastic%20Count%20Regression%20with%20Deep%20Double%20Poisson%20Networks%0AAuthor%3A%20Spencer%20Young%20and%20Porter%20Jenkins%20and%20Longchao%20Da%20and%20Jeff%20Dotson%20and%20Hua%20Wei%0AAbstract%3A%20%20%20Neural%20networks%20capable%20of%20accurate%2C%20input-conditional%20uncertainty%0Arepresentation%20are%20essential%20for%20real-world%20AI%20systems.%20Deep%20ensembles%20of%0AGaussian%20networks%20have%20proven%20highly%20effective%20for%20continuous%20regression%20due%20to%0Atheir%20ability%20to%20flexibly%20represent%20aleatoric%20uncertainty%20via%20unrestricted%0Aheteroscedastic%20variance%2C%20which%20in%20turn%20enables%20accurate%20epistemic%20uncertainty%0Aestimation.%20However%2C%20no%20analogous%20approach%20exists%20for%20count%20regression%2C%20despite%0Amany%20important%20applications.%20To%20address%20this%20gap%2C%20we%20propose%20the%20Deep%20Double%0APoisson%20Network%20%28DDPN%29%2C%20a%20novel%20neural%20discrete%20count%20regression%20model%20that%0Aoutputs%20the%20parameters%20of%20the%20Double%20Poisson%20distribution%2C%20enabling%20arbitrarily%0Ahigh%20or%20low%20predictive%20aleatoric%20uncertainty%20for%20count%20data%20and%20improving%0Aepistemic%20uncertainty%20estimation%20when%20ensembled.%20We%20formalize%20and%20prove%20that%0ADDPN%20exhibits%20robust%20regression%20properties%20similar%20to%20heteroscedastic%20Gaussian%0Amodels%20via%20learnable%20loss%20attenuation%2C%20and%20introduce%20a%20simple%20loss%20modification%0Ato%20control%20this%20behavior.%20Experiments%20on%20diverse%20datasets%20demonstrate%20that%20DDPN%0Aoutperforms%20current%20baselines%20in%20accuracy%2C%20calibration%2C%20and%20out-of-distribution%0Adetection%2C%20establishing%20a%20new%20state-of-the-art%20in%20deep%20count%20regression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09262v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFully%2520Heteroscedastic%2520Count%2520Regression%2520with%2520Deep%2520Double%2520Poisson%2520Networks%26entry.906535625%3DSpencer%2520Young%2520and%2520Porter%2520Jenkins%2520and%2520Longchao%2520Da%2520and%2520Jeff%2520Dotson%2520and%2520Hua%2520Wei%26entry.1292438233%3D%2520%2520Neural%2520networks%2520capable%2520of%2520accurate%252C%2520input-conditional%2520uncertainty%250Arepresentation%2520are%2520essential%2520for%2520real-world%2520AI%2520systems.%2520Deep%2520ensembles%2520of%250AGaussian%2520networks%2520have%2520proven%2520highly%2520effective%2520for%2520continuous%2520regression%2520due%2520to%250Atheir%2520ability%2520to%2520flexibly%2520represent%2520aleatoric%2520uncertainty%2520via%2520unrestricted%250Aheteroscedastic%2520variance%252C%2520which%2520in%2520turn%2520enables%2520accurate%2520epistemic%2520uncertainty%250Aestimation.%2520However%252C%2520no%2520analogous%2520approach%2520exists%2520for%2520count%2520regression%252C%2520despite%250Amany%2520important%2520applications.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520the%2520Deep%2520Double%250APoisson%2520Network%2520%2528DDPN%2529%252C%2520a%2520novel%2520neural%2520discrete%2520count%2520regression%2520model%2520that%250Aoutputs%2520the%2520parameters%2520of%2520the%2520Double%2520Poisson%2520distribution%252C%2520enabling%2520arbitrarily%250Ahigh%2520or%2520low%2520predictive%2520aleatoric%2520uncertainty%2520for%2520count%2520data%2520and%2520improving%250Aepistemic%2520uncertainty%2520estimation%2520when%2520ensembled.%2520We%2520formalize%2520and%2520prove%2520that%250ADDPN%2520exhibits%2520robust%2520regression%2520properties%2520similar%2520to%2520heteroscedastic%2520Gaussian%250Amodels%2520via%2520learnable%2520loss%2520attenuation%252C%2520and%2520introduce%2520a%2520simple%2520loss%2520modification%250Ato%2520control%2520this%2520behavior.%2520Experiments%2520on%2520diverse%2520datasets%2520demonstrate%2520that%2520DDPN%250Aoutperforms%2520current%2520baselines%2520in%2520accuracy%252C%2520calibration%252C%2520and%2520out-of-distribution%250Adetection%252C%2520establishing%2520a%2520new%2520state-of-the-art%2520in%2520deep%2520count%2520regression.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09262v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fully%20Heteroscedastic%20Count%20Regression%20with%20Deep%20Double%20Poisson%20Networks&entry.906535625=Spencer%20Young%20and%20Porter%20Jenkins%20and%20Longchao%20Da%20and%20Jeff%20Dotson%20and%20Hua%20Wei&entry.1292438233=%20%20Neural%20networks%20capable%20of%20accurate%2C%20input-conditional%20uncertainty%0Arepresentation%20are%20essential%20for%20real-world%20AI%20systems.%20Deep%20ensembles%20of%0AGaussian%20networks%20have%20proven%20highly%20effective%20for%20continuous%20regression%20due%20to%0Atheir%20ability%20to%20flexibly%20represent%20aleatoric%20uncertainty%20via%20unrestricted%0Aheteroscedastic%20variance%2C%20which%20in%20turn%20enables%20accurate%20epistemic%20uncertainty%0Aestimation.%20However%2C%20no%20analogous%20approach%20exists%20for%20count%20regression%2C%20despite%0Amany%20important%20applications.%20To%20address%20this%20gap%2C%20we%20propose%20the%20Deep%20Double%0APoisson%20Network%20%28DDPN%29%2C%20a%20novel%20neural%20discrete%20count%20regression%20model%20that%0Aoutputs%20the%20parameters%20of%20the%20Double%20Poisson%20distribution%2C%20enabling%20arbitrarily%0Ahigh%20or%20low%20predictive%20aleatoric%20uncertainty%20for%20count%20data%20and%20improving%0Aepistemic%20uncertainty%20estimation%20when%20ensembled.%20We%20formalize%20and%20prove%20that%0ADDPN%20exhibits%20robust%20regression%20properties%20similar%20to%20heteroscedastic%20Gaussian%0Amodels%20via%20learnable%20loss%20attenuation%2C%20and%20introduce%20a%20simple%20loss%20modification%0Ato%20control%20this%20behavior.%20Experiments%20on%20diverse%20datasets%20demonstrate%20that%20DDPN%0Aoutperforms%20current%20baselines%20in%20accuracy%2C%20calibration%2C%20and%20out-of-distribution%0Adetection%2C%20establishing%20a%20new%20state-of-the-art%20in%20deep%20count%20regression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09262v4&entry.124074799=Read"},
{"title": "Addressing and Visualizing Misalignments in Human Task-Solving\n  Trajectories", "author": "Sejin Kim and Hosung Lee and Sundong Kim", "abstract": "  Understanding misalignments in human task-solving trajectories is crucial for\nenhancing AI models trained to closely mimic human reasoning. This study\ncategorizes such misalignments into three types: (1) lack of functions to\nexpress intent, (2) inefficient action sequences, and (3) incorrect intentions\nthat cannot solve the task. To address these issues, we first formalize and\ndefine these three misalignment types in a unified framework. We then propose a\nheuristic algorithm to detect misalignments in ARCTraj trajectories and analyze\ntheir impact hierarchically and quantitatively. We also present an intention\nestimation method based on our formalism that infers missing alignment between\nuser actions and intentions. Through trajectory alignment, we experimentally\ndemonstrate that AI models trained on human task-solving trajectories improve\nperformance in mimicking human reasoning. Based on hierarchical analysis and\nexperiments, we highlight the importance of trajectory-intention alignment and\ndemonstrate the effectiveness of intention-aligned training.\n", "link": "http://arxiv.org/abs/2409.14191v4", "date": "2025-05-28", "relevancy": 2.0941, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5315}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5196}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Addressing%20and%20Visualizing%20Misalignments%20in%20Human%20Task-Solving%0A%20%20Trajectories&body=Title%3A%20Addressing%20and%20Visualizing%20Misalignments%20in%20Human%20Task-Solving%0A%20%20Trajectories%0AAuthor%3A%20Sejin%20Kim%20and%20Hosung%20Lee%20and%20Sundong%20Kim%0AAbstract%3A%20%20%20Understanding%20misalignments%20in%20human%20task-solving%20trajectories%20is%20crucial%20for%0Aenhancing%20AI%20models%20trained%20to%20closely%20mimic%20human%20reasoning.%20This%20study%0Acategorizes%20such%20misalignments%20into%20three%20types%3A%20%281%29%20lack%20of%20functions%20to%0Aexpress%20intent%2C%20%282%29%20inefficient%20action%20sequences%2C%20and%20%283%29%20incorrect%20intentions%0Athat%20cannot%20solve%20the%20task.%20To%20address%20these%20issues%2C%20we%20first%20formalize%20and%0Adefine%20these%20three%20misalignment%20types%20in%20a%20unified%20framework.%20We%20then%20propose%20a%0Aheuristic%20algorithm%20to%20detect%20misalignments%20in%20ARCTraj%20trajectories%20and%20analyze%0Atheir%20impact%20hierarchically%20and%20quantitatively.%20We%20also%20present%20an%20intention%0Aestimation%20method%20based%20on%20our%20formalism%20that%20infers%20missing%20alignment%20between%0Auser%20actions%20and%20intentions.%20Through%20trajectory%20alignment%2C%20we%20experimentally%0Ademonstrate%20that%20AI%20models%20trained%20on%20human%20task-solving%20trajectories%20improve%0Aperformance%20in%20mimicking%20human%20reasoning.%20Based%20on%20hierarchical%20analysis%20and%0Aexperiments%2C%20we%20highlight%20the%20importance%20of%20trajectory-intention%20alignment%20and%0Ademonstrate%20the%20effectiveness%20of%20intention-aligned%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.14191v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAddressing%2520and%2520Visualizing%2520Misalignments%2520in%2520Human%2520Task-Solving%250A%2520%2520Trajectories%26entry.906535625%3DSejin%2520Kim%2520and%2520Hosung%2520Lee%2520and%2520Sundong%2520Kim%26entry.1292438233%3D%2520%2520Understanding%2520misalignments%2520in%2520human%2520task-solving%2520trajectories%2520is%2520crucial%2520for%250Aenhancing%2520AI%2520models%2520trained%2520to%2520closely%2520mimic%2520human%2520reasoning.%2520This%2520study%250Acategorizes%2520such%2520misalignments%2520into%2520three%2520types%253A%2520%25281%2529%2520lack%2520of%2520functions%2520to%250Aexpress%2520intent%252C%2520%25282%2529%2520inefficient%2520action%2520sequences%252C%2520and%2520%25283%2529%2520incorrect%2520intentions%250Athat%2520cannot%2520solve%2520the%2520task.%2520To%2520address%2520these%2520issues%252C%2520we%2520first%2520formalize%2520and%250Adefine%2520these%2520three%2520misalignment%2520types%2520in%2520a%2520unified%2520framework.%2520We%2520then%2520propose%2520a%250Aheuristic%2520algorithm%2520to%2520detect%2520misalignments%2520in%2520ARCTraj%2520trajectories%2520and%2520analyze%250Atheir%2520impact%2520hierarchically%2520and%2520quantitatively.%2520We%2520also%2520present%2520an%2520intention%250Aestimation%2520method%2520based%2520on%2520our%2520formalism%2520that%2520infers%2520missing%2520alignment%2520between%250Auser%2520actions%2520and%2520intentions.%2520Through%2520trajectory%2520alignment%252C%2520we%2520experimentally%250Ademonstrate%2520that%2520AI%2520models%2520trained%2520on%2520human%2520task-solving%2520trajectories%2520improve%250Aperformance%2520in%2520mimicking%2520human%2520reasoning.%2520Based%2520on%2520hierarchical%2520analysis%2520and%250Aexperiments%252C%2520we%2520highlight%2520the%2520importance%2520of%2520trajectory-intention%2520alignment%2520and%250Ademonstrate%2520the%2520effectiveness%2520of%2520intention-aligned%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.14191v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Addressing%20and%20Visualizing%20Misalignments%20in%20Human%20Task-Solving%0A%20%20Trajectories&entry.906535625=Sejin%20Kim%20and%20Hosung%20Lee%20and%20Sundong%20Kim&entry.1292438233=%20%20Understanding%20misalignments%20in%20human%20task-solving%20trajectories%20is%20crucial%20for%0Aenhancing%20AI%20models%20trained%20to%20closely%20mimic%20human%20reasoning.%20This%20study%0Acategorizes%20such%20misalignments%20into%20three%20types%3A%20%281%29%20lack%20of%20functions%20to%0Aexpress%20intent%2C%20%282%29%20inefficient%20action%20sequences%2C%20and%20%283%29%20incorrect%20intentions%0Athat%20cannot%20solve%20the%20task.%20To%20address%20these%20issues%2C%20we%20first%20formalize%20and%0Adefine%20these%20three%20misalignment%20types%20in%20a%20unified%20framework.%20We%20then%20propose%20a%0Aheuristic%20algorithm%20to%20detect%20misalignments%20in%20ARCTraj%20trajectories%20and%20analyze%0Atheir%20impact%20hierarchically%20and%20quantitatively.%20We%20also%20present%20an%20intention%0Aestimation%20method%20based%20on%20our%20formalism%20that%20infers%20missing%20alignment%20between%0Auser%20actions%20and%20intentions.%20Through%20trajectory%20alignment%2C%20we%20experimentally%0Ademonstrate%20that%20AI%20models%20trained%20on%20human%20task-solving%20trajectories%20improve%0Aperformance%20in%20mimicking%20human%20reasoning.%20Based%20on%20hierarchical%20analysis%20and%0Aexperiments%2C%20we%20highlight%20the%20importance%20of%20trajectory-intention%20alignment%20and%0Ademonstrate%20the%20effectiveness%20of%20intention-aligned%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.14191v4&entry.124074799=Read"},
{"title": "C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in\n  Large Language Models", "author": "Amir Hossein Rahmati and Sanket Jantre and Weifeng Zhang and Yucheng Wang and Byung-Jun Yoon and Nathan M. Urban and Xiaoning Qian", "abstract": "  Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning\nlarge language models (LLMs), but it often produces overconfident predictions\nin data-scarce few-shot settings. To address this issue, several classical\nstatistical learning approaches have been repurposed for scalable\nuncertainty-aware LoRA fine-tuning. However, these approaches neglect how input\ncharacteristics affect the predictive uncertainty estimates. To address this\nlimitation, we propose Contextual Low-Rank Adaptation (\\textbf{C-LoRA}) as a\nnovel uncertainty-aware and parameter efficient fine-tuning approach, by\ndeveloping new lightweight LoRA modules contextualized to each input data\nsample to dynamically adapt uncertainty estimates. Incorporating data-driven\ncontexts into the parameter posteriors, C-LoRA mitigates overfitting, achieves\nwell-calibrated uncertainties, and yields robust predictions. Extensive\nexperiments demonstrate that C-LoRA consistently outperforms the\nstate-of-the-art uncertainty-aware LoRA methods in both uncertainty\nquantification and model generalization. Ablation studies further confirm the\ncritical role of our contextual modules in capturing sample-specific\nuncertainties. C-LoRA sets a new standard for robust, uncertainty-aware LLM\nfine-tuning in few-shot regimes.\n", "link": "http://arxiv.org/abs/2505.17773v2", "date": "2025-05-28", "relevancy": 2.028, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.552}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5278}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C-LoRA%3A%20Contextual%20Low-Rank%20Adaptation%20for%20Uncertainty%20Estimation%20in%0A%20%20Large%20Language%20Models&body=Title%3A%20C-LoRA%3A%20Contextual%20Low-Rank%20Adaptation%20for%20Uncertainty%20Estimation%20in%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Amir%20Hossein%20Rahmati%20and%20Sanket%20Jantre%20and%20Weifeng%20Zhang%20and%20Yucheng%20Wang%20and%20Byung-Jun%20Yoon%20and%20Nathan%20M.%20Urban%20and%20Xiaoning%20Qian%0AAbstract%3A%20%20%20Low-Rank%20Adaptation%20%28LoRA%29%20offers%20a%20cost-effective%20solution%20for%20fine-tuning%0Alarge%20language%20models%20%28LLMs%29%2C%20but%20it%20often%20produces%20overconfident%20predictions%0Ain%20data-scarce%20few-shot%20settings.%20To%20address%20this%20issue%2C%20several%20classical%0Astatistical%20learning%20approaches%20have%20been%20repurposed%20for%20scalable%0Auncertainty-aware%20LoRA%20fine-tuning.%20However%2C%20these%20approaches%20neglect%20how%20input%0Acharacteristics%20affect%20the%20predictive%20uncertainty%20estimates.%20To%20address%20this%0Alimitation%2C%20we%20propose%20Contextual%20Low-Rank%20Adaptation%20%28%5Ctextbf%7BC-LoRA%7D%29%20as%20a%0Anovel%20uncertainty-aware%20and%20parameter%20efficient%20fine-tuning%20approach%2C%20by%0Adeveloping%20new%20lightweight%20LoRA%20modules%20contextualized%20to%20each%20input%20data%0Asample%20to%20dynamically%20adapt%20uncertainty%20estimates.%20Incorporating%20data-driven%0Acontexts%20into%20the%20parameter%20posteriors%2C%20C-LoRA%20mitigates%20overfitting%2C%20achieves%0Awell-calibrated%20uncertainties%2C%20and%20yields%20robust%20predictions.%20Extensive%0Aexperiments%20demonstrate%20that%20C-LoRA%20consistently%20outperforms%20the%0Astate-of-the-art%20uncertainty-aware%20LoRA%20methods%20in%20both%20uncertainty%0Aquantification%20and%20model%20generalization.%20Ablation%20studies%20further%20confirm%20the%0Acritical%20role%20of%20our%20contextual%20modules%20in%20capturing%20sample-specific%0Auncertainties.%20C-LoRA%20sets%20a%20new%20standard%20for%20robust%2C%20uncertainty-aware%20LLM%0Afine-tuning%20in%20few-shot%20regimes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17773v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC-LoRA%253A%2520Contextual%2520Low-Rank%2520Adaptation%2520for%2520Uncertainty%2520Estimation%2520in%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DAmir%2520Hossein%2520Rahmati%2520and%2520Sanket%2520Jantre%2520and%2520Weifeng%2520Zhang%2520and%2520Yucheng%2520Wang%2520and%2520Byung-Jun%2520Yoon%2520and%2520Nathan%2520M.%2520Urban%2520and%2520Xiaoning%2520Qian%26entry.1292438233%3D%2520%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520offers%2520a%2520cost-effective%2520solution%2520for%2520fine-tuning%250Alarge%2520language%2520models%2520%2528LLMs%2529%252C%2520but%2520it%2520often%2520produces%2520overconfident%2520predictions%250Ain%2520data-scarce%2520few-shot%2520settings.%2520To%2520address%2520this%2520issue%252C%2520several%2520classical%250Astatistical%2520learning%2520approaches%2520have%2520been%2520repurposed%2520for%2520scalable%250Auncertainty-aware%2520LoRA%2520fine-tuning.%2520However%252C%2520these%2520approaches%2520neglect%2520how%2520input%250Acharacteristics%2520affect%2520the%2520predictive%2520uncertainty%2520estimates.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520propose%2520Contextual%2520Low-Rank%2520Adaptation%2520%2528%255Ctextbf%257BC-LoRA%257D%2529%2520as%2520a%250Anovel%2520uncertainty-aware%2520and%2520parameter%2520efficient%2520fine-tuning%2520approach%252C%2520by%250Adeveloping%2520new%2520lightweight%2520LoRA%2520modules%2520contextualized%2520to%2520each%2520input%2520data%250Asample%2520to%2520dynamically%2520adapt%2520uncertainty%2520estimates.%2520Incorporating%2520data-driven%250Acontexts%2520into%2520the%2520parameter%2520posteriors%252C%2520C-LoRA%2520mitigates%2520overfitting%252C%2520achieves%250Awell-calibrated%2520uncertainties%252C%2520and%2520yields%2520robust%2520predictions.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520C-LoRA%2520consistently%2520outperforms%2520the%250Astate-of-the-art%2520uncertainty-aware%2520LoRA%2520methods%2520in%2520both%2520uncertainty%250Aquantification%2520and%2520model%2520generalization.%2520Ablation%2520studies%2520further%2520confirm%2520the%250Acritical%2520role%2520of%2520our%2520contextual%2520modules%2520in%2520capturing%2520sample-specific%250Auncertainties.%2520C-LoRA%2520sets%2520a%2520new%2520standard%2520for%2520robust%252C%2520uncertainty-aware%2520LLM%250Afine-tuning%2520in%2520few-shot%2520regimes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17773v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C-LoRA%3A%20Contextual%20Low-Rank%20Adaptation%20for%20Uncertainty%20Estimation%20in%0A%20%20Large%20Language%20Models&entry.906535625=Amir%20Hossein%20Rahmati%20and%20Sanket%20Jantre%20and%20Weifeng%20Zhang%20and%20Yucheng%20Wang%20and%20Byung-Jun%20Yoon%20and%20Nathan%20M.%20Urban%20and%20Xiaoning%20Qian&entry.1292438233=%20%20Low-Rank%20Adaptation%20%28LoRA%29%20offers%20a%20cost-effective%20solution%20for%20fine-tuning%0Alarge%20language%20models%20%28LLMs%29%2C%20but%20it%20often%20produces%20overconfident%20predictions%0Ain%20data-scarce%20few-shot%20settings.%20To%20address%20this%20issue%2C%20several%20classical%0Astatistical%20learning%20approaches%20have%20been%20repurposed%20for%20scalable%0Auncertainty-aware%20LoRA%20fine-tuning.%20However%2C%20these%20approaches%20neglect%20how%20input%0Acharacteristics%20affect%20the%20predictive%20uncertainty%20estimates.%20To%20address%20this%0Alimitation%2C%20we%20propose%20Contextual%20Low-Rank%20Adaptation%20%28%5Ctextbf%7BC-LoRA%7D%29%20as%20a%0Anovel%20uncertainty-aware%20and%20parameter%20efficient%20fine-tuning%20approach%2C%20by%0Adeveloping%20new%20lightweight%20LoRA%20modules%20contextualized%20to%20each%20input%20data%0Asample%20to%20dynamically%20adapt%20uncertainty%20estimates.%20Incorporating%20data-driven%0Acontexts%20into%20the%20parameter%20posteriors%2C%20C-LoRA%20mitigates%20overfitting%2C%20achieves%0Awell-calibrated%20uncertainties%2C%20and%20yields%20robust%20predictions.%20Extensive%0Aexperiments%20demonstrate%20that%20C-LoRA%20consistently%20outperforms%20the%0Astate-of-the-art%20uncertainty-aware%20LoRA%20methods%20in%20both%20uncertainty%0Aquantification%20and%20model%20generalization.%20Ablation%20studies%20further%20confirm%20the%0Acritical%20role%20of%20our%20contextual%20modules%20in%20capturing%20sample-specific%0Auncertainties.%20C-LoRA%20sets%20a%20new%20standard%20for%20robust%2C%20uncertainty-aware%20LLM%0Afine-tuning%20in%20few-shot%20regimes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17773v2&entry.124074799=Read"},
{"title": "Learning Composable Chains-of-Thought", "author": "Fangcong Yin and Zeyu Leo Liu and Liu Leqi and Xi Ye and Greg Durrett", "abstract": "  A common approach for teaching large language models (LLMs) to reason is to\ntrain on chain-of-thought (CoT) traces of in-distribution reasoning problems,\nbut such annotated data is costly to obtain for every problem of interest. We\nwant reasoning models to generalize beyond their training distribution, and\nideally to generalize compositionally: combine atomic reasoning skills to solve\nharder, unseen reasoning tasks. We take a step towards compositional\ngeneralization of reasoning skills when addressing a target compositional task\nthat has no labeled CoT data. We find that simply training models on CoT data\nof atomic tasks leads to limited generalization, but minimally modifying CoT\nformats of constituent atomic tasks to be composable can lead to improvements.\nWe can train \"atomic CoT\" models on the atomic tasks with Composable CoT data\nand combine them with multitask learning or model merging for better zero-shot\nperformance on the target compositional task. Such a combined model can be\nfurther bootstrapped on a small amount of compositional data using rejection\nsampling fine-tuning (RFT). Results on string operations and natural language\nskill compositions show that training LLMs on Composable CoT outperforms\nmultitask learning and continued fine-tuning baselines within a given training\ndata budget.\n", "link": "http://arxiv.org/abs/2505.22635v1", "date": "2025-05-28", "relevancy": 1.8827, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5026}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4684}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Composable%20Chains-of-Thought&body=Title%3A%20Learning%20Composable%20Chains-of-Thought%0AAuthor%3A%20Fangcong%20Yin%20and%20Zeyu%20Leo%20Liu%20and%20Liu%20Leqi%20and%20Xi%20Ye%20and%20Greg%20Durrett%0AAbstract%3A%20%20%20A%20common%20approach%20for%20teaching%20large%20language%20models%20%28LLMs%29%20to%20reason%20is%20to%0Atrain%20on%20chain-of-thought%20%28CoT%29%20traces%20of%20in-distribution%20reasoning%20problems%2C%0Abut%20such%20annotated%20data%20is%20costly%20to%20obtain%20for%20every%20problem%20of%20interest.%20We%0Awant%20reasoning%20models%20to%20generalize%20beyond%20their%20training%20distribution%2C%20and%0Aideally%20to%20generalize%20compositionally%3A%20combine%20atomic%20reasoning%20skills%20to%20solve%0Aharder%2C%20unseen%20reasoning%20tasks.%20We%20take%20a%20step%20towards%20compositional%0Ageneralization%20of%20reasoning%20skills%20when%20addressing%20a%20target%20compositional%20task%0Athat%20has%20no%20labeled%20CoT%20data.%20We%20find%20that%20simply%20training%20models%20on%20CoT%20data%0Aof%20atomic%20tasks%20leads%20to%20limited%20generalization%2C%20but%20minimally%20modifying%20CoT%0Aformats%20of%20constituent%20atomic%20tasks%20to%20be%20composable%20can%20lead%20to%20improvements.%0AWe%20can%20train%20%22atomic%20CoT%22%20models%20on%20the%20atomic%20tasks%20with%20Composable%20CoT%20data%0Aand%20combine%20them%20with%20multitask%20learning%20or%20model%20merging%20for%20better%20zero-shot%0Aperformance%20on%20the%20target%20compositional%20task.%20Such%20a%20combined%20model%20can%20be%0Afurther%20bootstrapped%20on%20a%20small%20amount%20of%20compositional%20data%20using%20rejection%0Asampling%20fine-tuning%20%28RFT%29.%20Results%20on%20string%20operations%20and%20natural%20language%0Askill%20compositions%20show%20that%20training%20LLMs%20on%20Composable%20CoT%20outperforms%0Amultitask%20learning%20and%20continued%20fine-tuning%20baselines%20within%20a%20given%20training%0Adata%20budget.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Composable%2520Chains-of-Thought%26entry.906535625%3DFangcong%2520Yin%2520and%2520Zeyu%2520Leo%2520Liu%2520and%2520Liu%2520Leqi%2520and%2520Xi%2520Ye%2520and%2520Greg%2520Durrett%26entry.1292438233%3D%2520%2520A%2520common%2520approach%2520for%2520teaching%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520reason%2520is%2520to%250Atrain%2520on%2520chain-of-thought%2520%2528CoT%2529%2520traces%2520of%2520in-distribution%2520reasoning%2520problems%252C%250Abut%2520such%2520annotated%2520data%2520is%2520costly%2520to%2520obtain%2520for%2520every%2520problem%2520of%2520interest.%2520We%250Awant%2520reasoning%2520models%2520to%2520generalize%2520beyond%2520their%2520training%2520distribution%252C%2520and%250Aideally%2520to%2520generalize%2520compositionally%253A%2520combine%2520atomic%2520reasoning%2520skills%2520to%2520solve%250Aharder%252C%2520unseen%2520reasoning%2520tasks.%2520We%2520take%2520a%2520step%2520towards%2520compositional%250Ageneralization%2520of%2520reasoning%2520skills%2520when%2520addressing%2520a%2520target%2520compositional%2520task%250Athat%2520has%2520no%2520labeled%2520CoT%2520data.%2520We%2520find%2520that%2520simply%2520training%2520models%2520on%2520CoT%2520data%250Aof%2520atomic%2520tasks%2520leads%2520to%2520limited%2520generalization%252C%2520but%2520minimally%2520modifying%2520CoT%250Aformats%2520of%2520constituent%2520atomic%2520tasks%2520to%2520be%2520composable%2520can%2520lead%2520to%2520improvements.%250AWe%2520can%2520train%2520%2522atomic%2520CoT%2522%2520models%2520on%2520the%2520atomic%2520tasks%2520with%2520Composable%2520CoT%2520data%250Aand%2520combine%2520them%2520with%2520multitask%2520learning%2520or%2520model%2520merging%2520for%2520better%2520zero-shot%250Aperformance%2520on%2520the%2520target%2520compositional%2520task.%2520Such%2520a%2520combined%2520model%2520can%2520be%250Afurther%2520bootstrapped%2520on%2520a%2520small%2520amount%2520of%2520compositional%2520data%2520using%2520rejection%250Asampling%2520fine-tuning%2520%2528RFT%2529.%2520Results%2520on%2520string%2520operations%2520and%2520natural%2520language%250Askill%2520compositions%2520show%2520that%2520training%2520LLMs%2520on%2520Composable%2520CoT%2520outperforms%250Amultitask%2520learning%2520and%2520continued%2520fine-tuning%2520baselines%2520within%2520a%2520given%2520training%250Adata%2520budget.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Composable%20Chains-of-Thought&entry.906535625=Fangcong%20Yin%20and%20Zeyu%20Leo%20Liu%20and%20Liu%20Leqi%20and%20Xi%20Ye%20and%20Greg%20Durrett&entry.1292438233=%20%20A%20common%20approach%20for%20teaching%20large%20language%20models%20%28LLMs%29%20to%20reason%20is%20to%0Atrain%20on%20chain-of-thought%20%28CoT%29%20traces%20of%20in-distribution%20reasoning%20problems%2C%0Abut%20such%20annotated%20data%20is%20costly%20to%20obtain%20for%20every%20problem%20of%20interest.%20We%0Awant%20reasoning%20models%20to%20generalize%20beyond%20their%20training%20distribution%2C%20and%0Aideally%20to%20generalize%20compositionally%3A%20combine%20atomic%20reasoning%20skills%20to%20solve%0Aharder%2C%20unseen%20reasoning%20tasks.%20We%20take%20a%20step%20towards%20compositional%0Ageneralization%20of%20reasoning%20skills%20when%20addressing%20a%20target%20compositional%20task%0Athat%20has%20no%20labeled%20CoT%20data.%20We%20find%20that%20simply%20training%20models%20on%20CoT%20data%0Aof%20atomic%20tasks%20leads%20to%20limited%20generalization%2C%20but%20minimally%20modifying%20CoT%0Aformats%20of%20constituent%20atomic%20tasks%20to%20be%20composable%20can%20lead%20to%20improvements.%0AWe%20can%20train%20%22atomic%20CoT%22%20models%20on%20the%20atomic%20tasks%20with%20Composable%20CoT%20data%0Aand%20combine%20them%20with%20multitask%20learning%20or%20model%20merging%20for%20better%20zero-shot%0Aperformance%20on%20the%20target%20compositional%20task.%20Such%20a%20combined%20model%20can%20be%0Afurther%20bootstrapped%20on%20a%20small%20amount%20of%20compositional%20data%20using%20rejection%0Asampling%20fine-tuning%20%28RFT%29.%20Results%20on%20string%20operations%20and%20natural%20language%0Askill%20compositions%20show%20that%20training%20LLMs%20on%20Composable%20CoT%20outperforms%0Amultitask%20learning%20and%20continued%20fine-tuning%20baselines%20within%20a%20given%20training%0Adata%20budget.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22635v1&entry.124074799=Read"},
{"title": "Tell me Habibi, is it Real or Fake?", "author": "Kartik Kuckreja and Parul Gupta and Injy Hamed and Thamar Solorio and Muhammad Haris Khan and Abhinav Dhall", "abstract": "  Deepfake generation methods are evolving fast, making fake media harder to\ndetect and raising serious societal concerns. Most deepfake detection and\ndataset creation research focuses on monolingual content, often overlooking the\nchallenges of multilingual and code-switched speech, where multiple languages\nare mixed within the same discourse. Code-switching, especially between Arabic\nand English, is common in the Arab world and is widely used in digital\ncommunication. This linguistic mixing poses extra challenges for deepfake\ndetection, as it can confuse models trained mostly on monolingual data. To\naddress this, we introduce \\textbf{ArEnAV}, the first large-scale\nArabic-English audio-visual deepfake dataset featuring intra-utterance\ncode-switching, dialectal variation, and monolingual Arabic content. It\n\\textbf{contains 387k videos and over 765 hours of real and fake videos}. Our\ndataset is generated using a novel pipeline integrating four Text-To-Speech and\ntwo lip-sync models, enabling comprehensive analysis of multilingual multimodal\ndeepfake detection. We benchmark our dataset against existing monolingual and\nmultilingual datasets, state-of-the-art deepfake detection models, and a human\nevaluation, highlighting its potential to advance deepfake research. The\ndataset can be accessed\n\\href{https://huggingface.co/datasets/kartik060702/ArEnAV-Full}{here}.\n", "link": "http://arxiv.org/abs/2505.22581v1", "date": "2025-05-28", "relevancy": 1.8772, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.479}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4746}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tell%20me%20Habibi%2C%20is%20it%20Real%20or%20Fake%3F&body=Title%3A%20Tell%20me%20Habibi%2C%20is%20it%20Real%20or%20Fake%3F%0AAuthor%3A%20Kartik%20Kuckreja%20and%20Parul%20Gupta%20and%20Injy%20Hamed%20and%20Thamar%20Solorio%20and%20Muhammad%20Haris%20Khan%20and%20Abhinav%20Dhall%0AAbstract%3A%20%20%20Deepfake%20generation%20methods%20are%20evolving%20fast%2C%20making%20fake%20media%20harder%20to%0Adetect%20and%20raising%20serious%20societal%20concerns.%20Most%20deepfake%20detection%20and%0Adataset%20creation%20research%20focuses%20on%20monolingual%20content%2C%20often%20overlooking%20the%0Achallenges%20of%20multilingual%20and%20code-switched%20speech%2C%20where%20multiple%20languages%0Aare%20mixed%20within%20the%20same%20discourse.%20Code-switching%2C%20especially%20between%20Arabic%0Aand%20English%2C%20is%20common%20in%20the%20Arab%20world%20and%20is%20widely%20used%20in%20digital%0Acommunication.%20This%20linguistic%20mixing%20poses%20extra%20challenges%20for%20deepfake%0Adetection%2C%20as%20it%20can%20confuse%20models%20trained%20mostly%20on%20monolingual%20data.%20To%0Aaddress%20this%2C%20we%20introduce%20%5Ctextbf%7BArEnAV%7D%2C%20the%20first%20large-scale%0AArabic-English%20audio-visual%20deepfake%20dataset%20featuring%20intra-utterance%0Acode-switching%2C%20dialectal%20variation%2C%20and%20monolingual%20Arabic%20content.%20It%0A%5Ctextbf%7Bcontains%20387k%20videos%20and%20over%20765%20hours%20of%20real%20and%20fake%20videos%7D.%20Our%0Adataset%20is%20generated%20using%20a%20novel%20pipeline%20integrating%20four%20Text-To-Speech%20and%0Atwo%20lip-sync%20models%2C%20enabling%20comprehensive%20analysis%20of%20multilingual%20multimodal%0Adeepfake%20detection.%20We%20benchmark%20our%20dataset%20against%20existing%20monolingual%20and%0Amultilingual%20datasets%2C%20state-of-the-art%20deepfake%20detection%20models%2C%20and%20a%20human%0Aevaluation%2C%20highlighting%20its%20potential%20to%20advance%20deepfake%20research.%20The%0Adataset%20can%20be%20accessed%0A%5Chref%7Bhttps%3A//huggingface.co/datasets/kartik060702/ArEnAV-Full%7D%7Bhere%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22581v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTell%2520me%2520Habibi%252C%2520is%2520it%2520Real%2520or%2520Fake%253F%26entry.906535625%3DKartik%2520Kuckreja%2520and%2520Parul%2520Gupta%2520and%2520Injy%2520Hamed%2520and%2520Thamar%2520Solorio%2520and%2520Muhammad%2520Haris%2520Khan%2520and%2520Abhinav%2520Dhall%26entry.1292438233%3D%2520%2520Deepfake%2520generation%2520methods%2520are%2520evolving%2520fast%252C%2520making%2520fake%2520media%2520harder%2520to%250Adetect%2520and%2520raising%2520serious%2520societal%2520concerns.%2520Most%2520deepfake%2520detection%2520and%250Adataset%2520creation%2520research%2520focuses%2520on%2520monolingual%2520content%252C%2520often%2520overlooking%2520the%250Achallenges%2520of%2520multilingual%2520and%2520code-switched%2520speech%252C%2520where%2520multiple%2520languages%250Aare%2520mixed%2520within%2520the%2520same%2520discourse.%2520Code-switching%252C%2520especially%2520between%2520Arabic%250Aand%2520English%252C%2520is%2520common%2520in%2520the%2520Arab%2520world%2520and%2520is%2520widely%2520used%2520in%2520digital%250Acommunication.%2520This%2520linguistic%2520mixing%2520poses%2520extra%2520challenges%2520for%2520deepfake%250Adetection%252C%2520as%2520it%2520can%2520confuse%2520models%2520trained%2520mostly%2520on%2520monolingual%2520data.%2520To%250Aaddress%2520this%252C%2520we%2520introduce%2520%255Ctextbf%257BArEnAV%257D%252C%2520the%2520first%2520large-scale%250AArabic-English%2520audio-visual%2520deepfake%2520dataset%2520featuring%2520intra-utterance%250Acode-switching%252C%2520dialectal%2520variation%252C%2520and%2520monolingual%2520Arabic%2520content.%2520It%250A%255Ctextbf%257Bcontains%2520387k%2520videos%2520and%2520over%2520765%2520hours%2520of%2520real%2520and%2520fake%2520videos%257D.%2520Our%250Adataset%2520is%2520generated%2520using%2520a%2520novel%2520pipeline%2520integrating%2520four%2520Text-To-Speech%2520and%250Atwo%2520lip-sync%2520models%252C%2520enabling%2520comprehensive%2520analysis%2520of%2520multilingual%2520multimodal%250Adeepfake%2520detection.%2520We%2520benchmark%2520our%2520dataset%2520against%2520existing%2520monolingual%2520and%250Amultilingual%2520datasets%252C%2520state-of-the-art%2520deepfake%2520detection%2520models%252C%2520and%2520a%2520human%250Aevaluation%252C%2520highlighting%2520its%2520potential%2520to%2520advance%2520deepfake%2520research.%2520The%250Adataset%2520can%2520be%2520accessed%250A%255Chref%257Bhttps%253A//huggingface.co/datasets/kartik060702/ArEnAV-Full%257D%257Bhere%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22581v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tell%20me%20Habibi%2C%20is%20it%20Real%20or%20Fake%3F&entry.906535625=Kartik%20Kuckreja%20and%20Parul%20Gupta%20and%20Injy%20Hamed%20and%20Thamar%20Solorio%20and%20Muhammad%20Haris%20Khan%20and%20Abhinav%20Dhall&entry.1292438233=%20%20Deepfake%20generation%20methods%20are%20evolving%20fast%2C%20making%20fake%20media%20harder%20to%0Adetect%20and%20raising%20serious%20societal%20concerns.%20Most%20deepfake%20detection%20and%0Adataset%20creation%20research%20focuses%20on%20monolingual%20content%2C%20often%20overlooking%20the%0Achallenges%20of%20multilingual%20and%20code-switched%20speech%2C%20where%20multiple%20languages%0Aare%20mixed%20within%20the%20same%20discourse.%20Code-switching%2C%20especially%20between%20Arabic%0Aand%20English%2C%20is%20common%20in%20the%20Arab%20world%20and%20is%20widely%20used%20in%20digital%0Acommunication.%20This%20linguistic%20mixing%20poses%20extra%20challenges%20for%20deepfake%0Adetection%2C%20as%20it%20can%20confuse%20models%20trained%20mostly%20on%20monolingual%20data.%20To%0Aaddress%20this%2C%20we%20introduce%20%5Ctextbf%7BArEnAV%7D%2C%20the%20first%20large-scale%0AArabic-English%20audio-visual%20deepfake%20dataset%20featuring%20intra-utterance%0Acode-switching%2C%20dialectal%20variation%2C%20and%20monolingual%20Arabic%20content.%20It%0A%5Ctextbf%7Bcontains%20387k%20videos%20and%20over%20765%20hours%20of%20real%20and%20fake%20videos%7D.%20Our%0Adataset%20is%20generated%20using%20a%20novel%20pipeline%20integrating%20four%20Text-To-Speech%20and%0Atwo%20lip-sync%20models%2C%20enabling%20comprehensive%20analysis%20of%20multilingual%20multimodal%0Adeepfake%20detection.%20We%20benchmark%20our%20dataset%20against%20existing%20monolingual%20and%0Amultilingual%20datasets%2C%20state-of-the-art%20deepfake%20detection%20models%2C%20and%20a%20human%0Aevaluation%2C%20highlighting%20its%20potential%20to%20advance%20deepfake%20research.%20The%0Adataset%20can%20be%20accessed%0A%5Chref%7Bhttps%3A//huggingface.co/datasets/kartik060702/ArEnAV-Full%7D%7Bhere%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22581v1&entry.124074799=Read"},
{"title": "Bridging Supervised Learning and Reinforcement Learning in Math\n  Reasoning", "author": "Huayu Chen and Kaiwen Zheng and Qinsheng Zhang and Ganqu Cui and Yin Cui and Haotian Ye and Tsung-Yi Lin and Ming-Yu Liu and Jun Zhu and Haoxiang Wang", "abstract": "  Reinforcement Learning (RL) has played a central role in the recent surge of\nLLMs' math abilities by enabling self-improvement through binary verifier\nsignals. In contrast, Supervised Learning (SL) is rarely considered for such\nverification-driven training, largely due to its heavy reliance on reference\nanswers and inability to reflect on mistakes. In this work, we challenge the\nprevailing notion that self-improvement is exclusive to RL and propose\nNegative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to\nreflect on their failures and improve autonomously with no external teachers.\nIn online training, instead of throwing away self-generated negative answers,\nNFT constructs an implicit negative policy to model them. This implicit policy\nis parameterized with the same positive LLM we target to optimize on positive\ndata, enabling direct policy optimization on all LLMs' generations. We conduct\nexperiments on 7B and 32B models in math reasoning tasks. Results consistently\nshow that through the additional leverage of negative feedback, NFT\nsignificantly improves over SL baselines like Rejection sampling Fine-Tuning,\nmatching or even surpassing leading RL algorithms like GRPO and DAPO.\nFurthermore, we demonstrate that NFT and GRPO are actually equivalent in\nstrict-on-policy training, even though they originate from entirely different\ntheoretical foundations. Our experiments and theoretical findings bridge the\ngap between SL and RL methods in binary-feedback learning systems.\n", "link": "http://arxiv.org/abs/2505.18116v2", "date": "2025-05-28", "relevancy": 1.9528, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5051}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4775}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Supervised%20Learning%20and%20Reinforcement%20Learning%20in%20Math%0A%20%20Reasoning&body=Title%3A%20Bridging%20Supervised%20Learning%20and%20Reinforcement%20Learning%20in%20Math%0A%20%20Reasoning%0AAuthor%3A%20Huayu%20Chen%20and%20Kaiwen%20Zheng%20and%20Qinsheng%20Zhang%20and%20Ganqu%20Cui%20and%20Yin%20Cui%20and%20Haotian%20Ye%20and%20Tsung-Yi%20Lin%20and%20Ming-Yu%20Liu%20and%20Jun%20Zhu%20and%20Haoxiang%20Wang%0AAbstract%3A%20%20%20Reinforcement%20Learning%20%28RL%29%20has%20played%20a%20central%20role%20in%20the%20recent%20surge%20of%0ALLMs%27%20math%20abilities%20by%20enabling%20self-improvement%20through%20binary%20verifier%0Asignals.%20In%20contrast%2C%20Supervised%20Learning%20%28SL%29%20is%20rarely%20considered%20for%20such%0Averification-driven%20training%2C%20largely%20due%20to%20its%20heavy%20reliance%20on%20reference%0Aanswers%20and%20inability%20to%20reflect%20on%20mistakes.%20In%20this%20work%2C%20we%20challenge%20the%0Aprevailing%20notion%20that%20self-improvement%20is%20exclusive%20to%20RL%20and%20propose%0ANegative-aware%20Fine-Tuning%20%28NFT%29%20--%20a%20supervised%20approach%20that%20enables%20LLMs%20to%0Areflect%20on%20their%20failures%20and%20improve%20autonomously%20with%20no%20external%20teachers.%0AIn%20online%20training%2C%20instead%20of%20throwing%20away%20self-generated%20negative%20answers%2C%0ANFT%20constructs%20an%20implicit%20negative%20policy%20to%20model%20them.%20This%20implicit%20policy%0Ais%20parameterized%20with%20the%20same%20positive%20LLM%20we%20target%20to%20optimize%20on%20positive%0Adata%2C%20enabling%20direct%20policy%20optimization%20on%20all%20LLMs%27%20generations.%20We%20conduct%0Aexperiments%20on%207B%20and%2032B%20models%20in%20math%20reasoning%20tasks.%20Results%20consistently%0Ashow%20that%20through%20the%20additional%20leverage%20of%20negative%20feedback%2C%20NFT%0Asignificantly%20improves%20over%20SL%20baselines%20like%20Rejection%20sampling%20Fine-Tuning%2C%0Amatching%20or%20even%20surpassing%20leading%20RL%20algorithms%20like%20GRPO%20and%20DAPO.%0AFurthermore%2C%20we%20demonstrate%20that%20NFT%20and%20GRPO%20are%20actually%20equivalent%20in%0Astrict-on-policy%20training%2C%20even%20though%20they%20originate%20from%20entirely%20different%0Atheoretical%20foundations.%20Our%20experiments%20and%20theoretical%20findings%20bridge%20the%0Agap%20between%20SL%20and%20RL%20methods%20in%20binary-feedback%20learning%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18116v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Supervised%2520Learning%2520and%2520Reinforcement%2520Learning%2520in%2520Math%250A%2520%2520Reasoning%26entry.906535625%3DHuayu%2520Chen%2520and%2520Kaiwen%2520Zheng%2520and%2520Qinsheng%2520Zhang%2520and%2520Ganqu%2520Cui%2520and%2520Yin%2520Cui%2520and%2520Haotian%2520Ye%2520and%2520Tsung-Yi%2520Lin%2520and%2520Ming-Yu%2520Liu%2520and%2520Jun%2520Zhu%2520and%2520Haoxiang%2520Wang%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520%2528RL%2529%2520has%2520played%2520a%2520central%2520role%2520in%2520the%2520recent%2520surge%2520of%250ALLMs%2527%2520math%2520abilities%2520by%2520enabling%2520self-improvement%2520through%2520binary%2520verifier%250Asignals.%2520In%2520contrast%252C%2520Supervised%2520Learning%2520%2528SL%2529%2520is%2520rarely%2520considered%2520for%2520such%250Averification-driven%2520training%252C%2520largely%2520due%2520to%2520its%2520heavy%2520reliance%2520on%2520reference%250Aanswers%2520and%2520inability%2520to%2520reflect%2520on%2520mistakes.%2520In%2520this%2520work%252C%2520we%2520challenge%2520the%250Aprevailing%2520notion%2520that%2520self-improvement%2520is%2520exclusive%2520to%2520RL%2520and%2520propose%250ANegative-aware%2520Fine-Tuning%2520%2528NFT%2529%2520--%2520a%2520supervised%2520approach%2520that%2520enables%2520LLMs%2520to%250Areflect%2520on%2520their%2520failures%2520and%2520improve%2520autonomously%2520with%2520no%2520external%2520teachers.%250AIn%2520online%2520training%252C%2520instead%2520of%2520throwing%2520away%2520self-generated%2520negative%2520answers%252C%250ANFT%2520constructs%2520an%2520implicit%2520negative%2520policy%2520to%2520model%2520them.%2520This%2520implicit%2520policy%250Ais%2520parameterized%2520with%2520the%2520same%2520positive%2520LLM%2520we%2520target%2520to%2520optimize%2520on%2520positive%250Adata%252C%2520enabling%2520direct%2520policy%2520optimization%2520on%2520all%2520LLMs%2527%2520generations.%2520We%2520conduct%250Aexperiments%2520on%25207B%2520and%252032B%2520models%2520in%2520math%2520reasoning%2520tasks.%2520Results%2520consistently%250Ashow%2520that%2520through%2520the%2520additional%2520leverage%2520of%2520negative%2520feedback%252C%2520NFT%250Asignificantly%2520improves%2520over%2520SL%2520baselines%2520like%2520Rejection%2520sampling%2520Fine-Tuning%252C%250Amatching%2520or%2520even%2520surpassing%2520leading%2520RL%2520algorithms%2520like%2520GRPO%2520and%2520DAPO.%250AFurthermore%252C%2520we%2520demonstrate%2520that%2520NFT%2520and%2520GRPO%2520are%2520actually%2520equivalent%2520in%250Astrict-on-policy%2520training%252C%2520even%2520though%2520they%2520originate%2520from%2520entirely%2520different%250Atheoretical%2520foundations.%2520Our%2520experiments%2520and%2520theoretical%2520findings%2520bridge%2520the%250Agap%2520between%2520SL%2520and%2520RL%2520methods%2520in%2520binary-feedback%2520learning%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18116v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Supervised%20Learning%20and%20Reinforcement%20Learning%20in%20Math%0A%20%20Reasoning&entry.906535625=Huayu%20Chen%20and%20Kaiwen%20Zheng%20and%20Qinsheng%20Zhang%20and%20Ganqu%20Cui%20and%20Yin%20Cui%20and%20Haotian%20Ye%20and%20Tsung-Yi%20Lin%20and%20Ming-Yu%20Liu%20and%20Jun%20Zhu%20and%20Haoxiang%20Wang&entry.1292438233=%20%20Reinforcement%20Learning%20%28RL%29%20has%20played%20a%20central%20role%20in%20the%20recent%20surge%20of%0ALLMs%27%20math%20abilities%20by%20enabling%20self-improvement%20through%20binary%20verifier%0Asignals.%20In%20contrast%2C%20Supervised%20Learning%20%28SL%29%20is%20rarely%20considered%20for%20such%0Averification-driven%20training%2C%20largely%20due%20to%20its%20heavy%20reliance%20on%20reference%0Aanswers%20and%20inability%20to%20reflect%20on%20mistakes.%20In%20this%20work%2C%20we%20challenge%20the%0Aprevailing%20notion%20that%20self-improvement%20is%20exclusive%20to%20RL%20and%20propose%0ANegative-aware%20Fine-Tuning%20%28NFT%29%20--%20a%20supervised%20approach%20that%20enables%20LLMs%20to%0Areflect%20on%20their%20failures%20and%20improve%20autonomously%20with%20no%20external%20teachers.%0AIn%20online%20training%2C%20instead%20of%20throwing%20away%20self-generated%20negative%20answers%2C%0ANFT%20constructs%20an%20implicit%20negative%20policy%20to%20model%20them.%20This%20implicit%20policy%0Ais%20parameterized%20with%20the%20same%20positive%20LLM%20we%20target%20to%20optimize%20on%20positive%0Adata%2C%20enabling%20direct%20policy%20optimization%20on%20all%20LLMs%27%20generations.%20We%20conduct%0Aexperiments%20on%207B%20and%2032B%20models%20in%20math%20reasoning%20tasks.%20Results%20consistently%0Ashow%20that%20through%20the%20additional%20leverage%20of%20negative%20feedback%2C%20NFT%0Asignificantly%20improves%20over%20SL%20baselines%20like%20Rejection%20sampling%20Fine-Tuning%2C%0Amatching%20or%20even%20surpassing%20leading%20RL%20algorithms%20like%20GRPO%20and%20DAPO.%0AFurthermore%2C%20we%20demonstrate%20that%20NFT%20and%20GRPO%20are%20actually%20equivalent%20in%0Astrict-on-policy%20training%2C%20even%20though%20they%20originate%20from%20entirely%20different%0Atheoretical%20foundations.%20Our%20experiments%20and%20theoretical%20findings%20bridge%20the%0Agap%20between%20SL%20and%20RL%20methods%20in%20binary-feedback%20learning%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18116v2&entry.124074799=Read"},
{"title": "ProCrop: Learning Aesthetic Image Cropping from Professional\n  Compositions", "author": "Ke Zhang and Tianyu Ding and Jiachen Jiang and Tianyi Chen and Ilya Zharkov and Vishal M. Patel and Luming Liang", "abstract": "  Image cropping is crucial for enhancing the visual appeal and narrative\nimpact of photographs, yet existing rule-based and data-driven approaches often\nlack diversity or require annotated training data. We introduce ProCrop, a\nretrieval-based method that leverages professional photography to guide\ncropping decisions. By fusing features from professional photographs with those\nof the query image, ProCrop learns from professional compositions,\nsignificantly boosting performance. Additionally, we present a large-scale\ndataset of 242K weakly-annotated images, generated by out-painting professional\nimages and iteratively refining diverse crop proposals. This composition-aware\ndataset generation offers diverse high-quality crop proposals guided by\naesthetic principles and becomes the largest publicly available dataset for\nimage cropping. Extensive experiments show that ProCrop significantly\noutperforms existing methods in both supervised and weakly-supervised settings.\nNotably, when trained on the new dataset, our ProCrop surpasses previous\nweakly-supervised methods and even matches fully supervised approaches. Both\nthe code and dataset will be made publicly available to advance research in\nimage aesthetics and composition analysis.\n", "link": "http://arxiv.org/abs/2505.22490v1", "date": "2025-05-28", "relevancy": 1.7321, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6023}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5827}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProCrop%3A%20Learning%20Aesthetic%20Image%20Cropping%20from%20Professional%0A%20%20Compositions&body=Title%3A%20ProCrop%3A%20Learning%20Aesthetic%20Image%20Cropping%20from%20Professional%0A%20%20Compositions%0AAuthor%3A%20Ke%20Zhang%20and%20Tianyu%20Ding%20and%20Jiachen%20Jiang%20and%20Tianyi%20Chen%20and%20Ilya%20Zharkov%20and%20Vishal%20M.%20Patel%20and%20Luming%20Liang%0AAbstract%3A%20%20%20Image%20cropping%20is%20crucial%20for%20enhancing%20the%20visual%20appeal%20and%20narrative%0Aimpact%20of%20photographs%2C%20yet%20existing%20rule-based%20and%20data-driven%20approaches%20often%0Alack%20diversity%20or%20require%20annotated%20training%20data.%20We%20introduce%20ProCrop%2C%20a%0Aretrieval-based%20method%20that%20leverages%20professional%20photography%20to%20guide%0Acropping%20decisions.%20By%20fusing%20features%20from%20professional%20photographs%20with%20those%0Aof%20the%20query%20image%2C%20ProCrop%20learns%20from%20professional%20compositions%2C%0Asignificantly%20boosting%20performance.%20Additionally%2C%20we%20present%20a%20large-scale%0Adataset%20of%20242K%20weakly-annotated%20images%2C%20generated%20by%20out-painting%20professional%0Aimages%20and%20iteratively%20refining%20diverse%20crop%20proposals.%20This%20composition-aware%0Adataset%20generation%20offers%20diverse%20high-quality%20crop%20proposals%20guided%20by%0Aaesthetic%20principles%20and%20becomes%20the%20largest%20publicly%20available%20dataset%20for%0Aimage%20cropping.%20Extensive%20experiments%20show%20that%20ProCrop%20significantly%0Aoutperforms%20existing%20methods%20in%20both%20supervised%20and%20weakly-supervised%20settings.%0ANotably%2C%20when%20trained%20on%20the%20new%20dataset%2C%20our%20ProCrop%20surpasses%20previous%0Aweakly-supervised%20methods%20and%20even%20matches%20fully%20supervised%20approaches.%20Both%0Athe%20code%20and%20dataset%20will%20be%20made%20publicly%20available%20to%20advance%20research%20in%0Aimage%20aesthetics%20and%20composition%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22490v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProCrop%253A%2520Learning%2520Aesthetic%2520Image%2520Cropping%2520from%2520Professional%250A%2520%2520Compositions%26entry.906535625%3DKe%2520Zhang%2520and%2520Tianyu%2520Ding%2520and%2520Jiachen%2520Jiang%2520and%2520Tianyi%2520Chen%2520and%2520Ilya%2520Zharkov%2520and%2520Vishal%2520M.%2520Patel%2520and%2520Luming%2520Liang%26entry.1292438233%3D%2520%2520Image%2520cropping%2520is%2520crucial%2520for%2520enhancing%2520the%2520visual%2520appeal%2520and%2520narrative%250Aimpact%2520of%2520photographs%252C%2520yet%2520existing%2520rule-based%2520and%2520data-driven%2520approaches%2520often%250Alack%2520diversity%2520or%2520require%2520annotated%2520training%2520data.%2520We%2520introduce%2520ProCrop%252C%2520a%250Aretrieval-based%2520method%2520that%2520leverages%2520professional%2520photography%2520to%2520guide%250Acropping%2520decisions.%2520By%2520fusing%2520features%2520from%2520professional%2520photographs%2520with%2520those%250Aof%2520the%2520query%2520image%252C%2520ProCrop%2520learns%2520from%2520professional%2520compositions%252C%250Asignificantly%2520boosting%2520performance.%2520Additionally%252C%2520we%2520present%2520a%2520large-scale%250Adataset%2520of%2520242K%2520weakly-annotated%2520images%252C%2520generated%2520by%2520out-painting%2520professional%250Aimages%2520and%2520iteratively%2520refining%2520diverse%2520crop%2520proposals.%2520This%2520composition-aware%250Adataset%2520generation%2520offers%2520diverse%2520high-quality%2520crop%2520proposals%2520guided%2520by%250Aaesthetic%2520principles%2520and%2520becomes%2520the%2520largest%2520publicly%2520available%2520dataset%2520for%250Aimage%2520cropping.%2520Extensive%2520experiments%2520show%2520that%2520ProCrop%2520significantly%250Aoutperforms%2520existing%2520methods%2520in%2520both%2520supervised%2520and%2520weakly-supervised%2520settings.%250ANotably%252C%2520when%2520trained%2520on%2520the%2520new%2520dataset%252C%2520our%2520ProCrop%2520surpasses%2520previous%250Aweakly-supervised%2520methods%2520and%2520even%2520matches%2520fully%2520supervised%2520approaches.%2520Both%250Athe%2520code%2520and%2520dataset%2520will%2520be%2520made%2520publicly%2520available%2520to%2520advance%2520research%2520in%250Aimage%2520aesthetics%2520and%2520composition%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22490v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProCrop%3A%20Learning%20Aesthetic%20Image%20Cropping%20from%20Professional%0A%20%20Compositions&entry.906535625=Ke%20Zhang%20and%20Tianyu%20Ding%20and%20Jiachen%20Jiang%20and%20Tianyi%20Chen%20and%20Ilya%20Zharkov%20and%20Vishal%20M.%20Patel%20and%20Luming%20Liang&entry.1292438233=%20%20Image%20cropping%20is%20crucial%20for%20enhancing%20the%20visual%20appeal%20and%20narrative%0Aimpact%20of%20photographs%2C%20yet%20existing%20rule-based%20and%20data-driven%20approaches%20often%0Alack%20diversity%20or%20require%20annotated%20training%20data.%20We%20introduce%20ProCrop%2C%20a%0Aretrieval-based%20method%20that%20leverages%20professional%20photography%20to%20guide%0Acropping%20decisions.%20By%20fusing%20features%20from%20professional%20photographs%20with%20those%0Aof%20the%20query%20image%2C%20ProCrop%20learns%20from%20professional%20compositions%2C%0Asignificantly%20boosting%20performance.%20Additionally%2C%20we%20present%20a%20large-scale%0Adataset%20of%20242K%20weakly-annotated%20images%2C%20generated%20by%20out-painting%20professional%0Aimages%20and%20iteratively%20refining%20diverse%20crop%20proposals.%20This%20composition-aware%0Adataset%20generation%20offers%20diverse%20high-quality%20crop%20proposals%20guided%20by%0Aaesthetic%20principles%20and%20becomes%20the%20largest%20publicly%20available%20dataset%20for%0Aimage%20cropping.%20Extensive%20experiments%20show%20that%20ProCrop%20significantly%0Aoutperforms%20existing%20methods%20in%20both%20supervised%20and%20weakly-supervised%20settings.%0ANotably%2C%20when%20trained%20on%20the%20new%20dataset%2C%20our%20ProCrop%20surpasses%20previous%0Aweakly-supervised%20methods%20and%20even%20matches%20fully%20supervised%20approaches.%20Both%0Athe%20code%20and%20dataset%20will%20be%20made%20publicly%20available%20to%20advance%20research%20in%0Aimage%20aesthetics%20and%20composition%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22490v1&entry.124074799=Read"},
{"title": "Demystifying the Paradox of Importance Sampling with an Estimated\n  History-Dependent Behavior Policy in Off-Policy Evaluation", "author": "Hongyi Zhou and Josiah P. Hanna and Jin Zhu and Ying Yang and Chengchun Shi", "abstract": "  This paper studies off-policy evaluation (OPE) in reinforcement learning with\na focus on behavior policy estimation for importance sampling. Prior work has\nshown empirically that estimating a history-dependent behavior policy can lead\nto lower mean squared error (MSE) even when the true behavior policy is\nMarkovian. However, the question of why the use of history should lower MSE\nremains open. In this paper, we theoretically demystify this paradox by\nderiving a bias-variance decomposition of the MSE of ordinary importance\nsampling (IS) estimators, demonstrating that history-dependent behavior policy\nestimation decreases their asymptotic variances while increasing their\nfinite-sample biases. Additionally, as the estimated behavior policy conditions\non a longer history, we show a consistent decrease in variance. We extend these\nfindings to a range of other OPE estimators, including the sequential IS\nestimator, the doubly robust estimator and the marginalized IS estimator, with\nthe behavior policy estimated either parametrically or non-parametrically.\n", "link": "http://arxiv.org/abs/2505.22492v1", "date": "2025-05-28", "relevancy": 1.7118, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5036}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4162}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Demystifying%20the%20Paradox%20of%20Importance%20Sampling%20with%20an%20Estimated%0A%20%20History-Dependent%20Behavior%20Policy%20in%20Off-Policy%20Evaluation&body=Title%3A%20Demystifying%20the%20Paradox%20of%20Importance%20Sampling%20with%20an%20Estimated%0A%20%20History-Dependent%20Behavior%20Policy%20in%20Off-Policy%20Evaluation%0AAuthor%3A%20Hongyi%20Zhou%20and%20Josiah%20P.%20Hanna%20and%20Jin%20Zhu%20and%20Ying%20Yang%20and%20Chengchun%20Shi%0AAbstract%3A%20%20%20This%20paper%20studies%20off-policy%20evaluation%20%28OPE%29%20in%20reinforcement%20learning%20with%0Aa%20focus%20on%20behavior%20policy%20estimation%20for%20importance%20sampling.%20Prior%20work%20has%0Ashown%20empirically%20that%20estimating%20a%20history-dependent%20behavior%20policy%20can%20lead%0Ato%20lower%20mean%20squared%20error%20%28MSE%29%20even%20when%20the%20true%20behavior%20policy%20is%0AMarkovian.%20However%2C%20the%20question%20of%20why%20the%20use%20of%20history%20should%20lower%20MSE%0Aremains%20open.%20In%20this%20paper%2C%20we%20theoretically%20demystify%20this%20paradox%20by%0Aderiving%20a%20bias-variance%20decomposition%20of%20the%20MSE%20of%20ordinary%20importance%0Asampling%20%28IS%29%20estimators%2C%20demonstrating%20that%20history-dependent%20behavior%20policy%0Aestimation%20decreases%20their%20asymptotic%20variances%20while%20increasing%20their%0Afinite-sample%20biases.%20Additionally%2C%20as%20the%20estimated%20behavior%20policy%20conditions%0Aon%20a%20longer%20history%2C%20we%20show%20a%20consistent%20decrease%20in%20variance.%20We%20extend%20these%0Afindings%20to%20a%20range%20of%20other%20OPE%20estimators%2C%20including%20the%20sequential%20IS%0Aestimator%2C%20the%20doubly%20robust%20estimator%20and%20the%20marginalized%20IS%20estimator%2C%20with%0Athe%20behavior%20policy%20estimated%20either%20parametrically%20or%20non-parametrically.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22492v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDemystifying%2520the%2520Paradox%2520of%2520Importance%2520Sampling%2520with%2520an%2520Estimated%250A%2520%2520History-Dependent%2520Behavior%2520Policy%2520in%2520Off-Policy%2520Evaluation%26entry.906535625%3DHongyi%2520Zhou%2520and%2520Josiah%2520P.%2520Hanna%2520and%2520Jin%2520Zhu%2520and%2520Ying%2520Yang%2520and%2520Chengchun%2520Shi%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520off-policy%2520evaluation%2520%2528OPE%2529%2520in%2520reinforcement%2520learning%2520with%250Aa%2520focus%2520on%2520behavior%2520policy%2520estimation%2520for%2520importance%2520sampling.%2520Prior%2520work%2520has%250Ashown%2520empirically%2520that%2520estimating%2520a%2520history-dependent%2520behavior%2520policy%2520can%2520lead%250Ato%2520lower%2520mean%2520squared%2520error%2520%2528MSE%2529%2520even%2520when%2520the%2520true%2520behavior%2520policy%2520is%250AMarkovian.%2520However%252C%2520the%2520question%2520of%2520why%2520the%2520use%2520of%2520history%2520should%2520lower%2520MSE%250Aremains%2520open.%2520In%2520this%2520paper%252C%2520we%2520theoretically%2520demystify%2520this%2520paradox%2520by%250Aderiving%2520a%2520bias-variance%2520decomposition%2520of%2520the%2520MSE%2520of%2520ordinary%2520importance%250Asampling%2520%2528IS%2529%2520estimators%252C%2520demonstrating%2520that%2520history-dependent%2520behavior%2520policy%250Aestimation%2520decreases%2520their%2520asymptotic%2520variances%2520while%2520increasing%2520their%250Afinite-sample%2520biases.%2520Additionally%252C%2520as%2520the%2520estimated%2520behavior%2520policy%2520conditions%250Aon%2520a%2520longer%2520history%252C%2520we%2520show%2520a%2520consistent%2520decrease%2520in%2520variance.%2520We%2520extend%2520these%250Afindings%2520to%2520a%2520range%2520of%2520other%2520OPE%2520estimators%252C%2520including%2520the%2520sequential%2520IS%250Aestimator%252C%2520the%2520doubly%2520robust%2520estimator%2520and%2520the%2520marginalized%2520IS%2520estimator%252C%2520with%250Athe%2520behavior%2520policy%2520estimated%2520either%2520parametrically%2520or%2520non-parametrically.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22492v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Demystifying%20the%20Paradox%20of%20Importance%20Sampling%20with%20an%20Estimated%0A%20%20History-Dependent%20Behavior%20Policy%20in%20Off-Policy%20Evaluation&entry.906535625=Hongyi%20Zhou%20and%20Josiah%20P.%20Hanna%20and%20Jin%20Zhu%20and%20Ying%20Yang%20and%20Chengchun%20Shi&entry.1292438233=%20%20This%20paper%20studies%20off-policy%20evaluation%20%28OPE%29%20in%20reinforcement%20learning%20with%0Aa%20focus%20on%20behavior%20policy%20estimation%20for%20importance%20sampling.%20Prior%20work%20has%0Ashown%20empirically%20that%20estimating%20a%20history-dependent%20behavior%20policy%20can%20lead%0Ato%20lower%20mean%20squared%20error%20%28MSE%29%20even%20when%20the%20true%20behavior%20policy%20is%0AMarkovian.%20However%2C%20the%20question%20of%20why%20the%20use%20of%20history%20should%20lower%20MSE%0Aremains%20open.%20In%20this%20paper%2C%20we%20theoretically%20demystify%20this%20paradox%20by%0Aderiving%20a%20bias-variance%20decomposition%20of%20the%20MSE%20of%20ordinary%20importance%0Asampling%20%28IS%29%20estimators%2C%20demonstrating%20that%20history-dependent%20behavior%20policy%0Aestimation%20decreases%20their%20asymptotic%20variances%20while%20increasing%20their%0Afinite-sample%20biases.%20Additionally%2C%20as%20the%20estimated%20behavior%20policy%20conditions%0Aon%20a%20longer%20history%2C%20we%20show%20a%20consistent%20decrease%20in%20variance.%20We%20extend%20these%0Afindings%20to%20a%20range%20of%20other%20OPE%20estimators%2C%20including%20the%20sequential%20IS%0Aestimator%2C%20the%20doubly%20robust%20estimator%20and%20the%20marginalized%20IS%20estimator%2C%20with%0Athe%20behavior%20policy%20estimated%20either%20parametrically%20or%20non-parametrically.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22492v1&entry.124074799=Read"},
{"title": "SCIZOR: A Self-Supervised Approach to Data Curation for Large-Scale\n  Imitation Learning", "author": "Yu Zhang and Yuqi Xie and Huihan Liu and Rutav Shah and Michael Wan and Linxi Fan and Yuke Zhu", "abstract": "  Imitation learning advances robot capabilities by enabling the acquisition of\ndiverse behaviors from human demonstrations. However, large-scale datasets used\nfor policy training often introduce substantial variability in quality, which\ncan negatively impact performance. As a result, automatically curating datasets\nby filtering low-quality samples to improve quality becomes essential. Existing\nrobotic curation approaches rely on costly manual annotations and perform\ncuration at a coarse granularity, such as the dataset or trajectory level,\nfailing to account for the quality of individual state-action pairs. To address\nthis, we introduce SCIZOR, a self-supervised data curation framework that\nfilters out low-quality state-action pairs to improve the performance of\nimitation learning policies. SCIZOR targets two complementary sources of\nlow-quality data: suboptimal data, which hinders learning with undesirable\nactions, and redundant data, which dilutes training with repetitive patterns.\nSCIZOR leverages a self-supervised task progress predictor for suboptimal data\nto remove samples lacking task progression, and a deduplication module\noperating on joint state-action representation for samples with redundant\npatterns. Empirically, we show that SCIZOR enables imitation learning policies\nto achieve higher performance with less data, yielding an average improvement\nof 15.4% across multiple benchmarks. More information is available at:\nhttps://ut-austin-rpl.github.io/SCIZOR/\n", "link": "http://arxiv.org/abs/2505.22626v1", "date": "2025-05-28", "relevancy": 1.5756, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5697}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5191}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCIZOR%3A%20A%20Self-Supervised%20Approach%20to%20Data%20Curation%20for%20Large-Scale%0A%20%20Imitation%20Learning&body=Title%3A%20SCIZOR%3A%20A%20Self-Supervised%20Approach%20to%20Data%20Curation%20for%20Large-Scale%0A%20%20Imitation%20Learning%0AAuthor%3A%20Yu%20Zhang%20and%20Yuqi%20Xie%20and%20Huihan%20Liu%20and%20Rutav%20Shah%20and%20Michael%20Wan%20and%20Linxi%20Fan%20and%20Yuke%20Zhu%0AAbstract%3A%20%20%20Imitation%20learning%20advances%20robot%20capabilities%20by%20enabling%20the%20acquisition%20of%0Adiverse%20behaviors%20from%20human%20demonstrations.%20However%2C%20large-scale%20datasets%20used%0Afor%20policy%20training%20often%20introduce%20substantial%20variability%20in%20quality%2C%20which%0Acan%20negatively%20impact%20performance.%20As%20a%20result%2C%20automatically%20curating%20datasets%0Aby%20filtering%20low-quality%20samples%20to%20improve%20quality%20becomes%20essential.%20Existing%0Arobotic%20curation%20approaches%20rely%20on%20costly%20manual%20annotations%20and%20perform%0Acuration%20at%20a%20coarse%20granularity%2C%20such%20as%20the%20dataset%20or%20trajectory%20level%2C%0Afailing%20to%20account%20for%20the%20quality%20of%20individual%20state-action%20pairs.%20To%20address%0Athis%2C%20we%20introduce%20SCIZOR%2C%20a%20self-supervised%20data%20curation%20framework%20that%0Afilters%20out%20low-quality%20state-action%20pairs%20to%20improve%20the%20performance%20of%0Aimitation%20learning%20policies.%20SCIZOR%20targets%20two%20complementary%20sources%20of%0Alow-quality%20data%3A%20suboptimal%20data%2C%20which%20hinders%20learning%20with%20undesirable%0Aactions%2C%20and%20redundant%20data%2C%20which%20dilutes%20training%20with%20repetitive%20patterns.%0ASCIZOR%20leverages%20a%20self-supervised%20task%20progress%20predictor%20for%20suboptimal%20data%0Ato%20remove%20samples%20lacking%20task%20progression%2C%20and%20a%20deduplication%20module%0Aoperating%20on%20joint%20state-action%20representation%20for%20samples%20with%20redundant%0Apatterns.%20Empirically%2C%20we%20show%20that%20SCIZOR%20enables%20imitation%20learning%20policies%0Ato%20achieve%20higher%20performance%20with%20less%20data%2C%20yielding%20an%20average%20improvement%0Aof%2015.4%25%20across%20multiple%20benchmarks.%20More%20information%20is%20available%20at%3A%0Ahttps%3A//ut-austin-rpl.github.io/SCIZOR/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22626v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCIZOR%253A%2520A%2520Self-Supervised%2520Approach%2520to%2520Data%2520Curation%2520for%2520Large-Scale%250A%2520%2520Imitation%2520Learning%26entry.906535625%3DYu%2520Zhang%2520and%2520Yuqi%2520Xie%2520and%2520Huihan%2520Liu%2520and%2520Rutav%2520Shah%2520and%2520Michael%2520Wan%2520and%2520Linxi%2520Fan%2520and%2520Yuke%2520Zhu%26entry.1292438233%3D%2520%2520Imitation%2520learning%2520advances%2520robot%2520capabilities%2520by%2520enabling%2520the%2520acquisition%2520of%250Adiverse%2520behaviors%2520from%2520human%2520demonstrations.%2520However%252C%2520large-scale%2520datasets%2520used%250Afor%2520policy%2520training%2520often%2520introduce%2520substantial%2520variability%2520in%2520quality%252C%2520which%250Acan%2520negatively%2520impact%2520performance.%2520As%2520a%2520result%252C%2520automatically%2520curating%2520datasets%250Aby%2520filtering%2520low-quality%2520samples%2520to%2520improve%2520quality%2520becomes%2520essential.%2520Existing%250Arobotic%2520curation%2520approaches%2520rely%2520on%2520costly%2520manual%2520annotations%2520and%2520perform%250Acuration%2520at%2520a%2520coarse%2520granularity%252C%2520such%2520as%2520the%2520dataset%2520or%2520trajectory%2520level%252C%250Afailing%2520to%2520account%2520for%2520the%2520quality%2520of%2520individual%2520state-action%2520pairs.%2520To%2520address%250Athis%252C%2520we%2520introduce%2520SCIZOR%252C%2520a%2520self-supervised%2520data%2520curation%2520framework%2520that%250Afilters%2520out%2520low-quality%2520state-action%2520pairs%2520to%2520improve%2520the%2520performance%2520of%250Aimitation%2520learning%2520policies.%2520SCIZOR%2520targets%2520two%2520complementary%2520sources%2520of%250Alow-quality%2520data%253A%2520suboptimal%2520data%252C%2520which%2520hinders%2520learning%2520with%2520undesirable%250Aactions%252C%2520and%2520redundant%2520data%252C%2520which%2520dilutes%2520training%2520with%2520repetitive%2520patterns.%250ASCIZOR%2520leverages%2520a%2520self-supervised%2520task%2520progress%2520predictor%2520for%2520suboptimal%2520data%250Ato%2520remove%2520samples%2520lacking%2520task%2520progression%252C%2520and%2520a%2520deduplication%2520module%250Aoperating%2520on%2520joint%2520state-action%2520representation%2520for%2520samples%2520with%2520redundant%250Apatterns.%2520Empirically%252C%2520we%2520show%2520that%2520SCIZOR%2520enables%2520imitation%2520learning%2520policies%250Ato%2520achieve%2520higher%2520performance%2520with%2520less%2520data%252C%2520yielding%2520an%2520average%2520improvement%250Aof%252015.4%2525%2520across%2520multiple%2520benchmarks.%2520More%2520information%2520is%2520available%2520at%253A%250Ahttps%253A//ut-austin-rpl.github.io/SCIZOR/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22626v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCIZOR%3A%20A%20Self-Supervised%20Approach%20to%20Data%20Curation%20for%20Large-Scale%0A%20%20Imitation%20Learning&entry.906535625=Yu%20Zhang%20and%20Yuqi%20Xie%20and%20Huihan%20Liu%20and%20Rutav%20Shah%20and%20Michael%20Wan%20and%20Linxi%20Fan%20and%20Yuke%20Zhu&entry.1292438233=%20%20Imitation%20learning%20advances%20robot%20capabilities%20by%20enabling%20the%20acquisition%20of%0Adiverse%20behaviors%20from%20human%20demonstrations.%20However%2C%20large-scale%20datasets%20used%0Afor%20policy%20training%20often%20introduce%20substantial%20variability%20in%20quality%2C%20which%0Acan%20negatively%20impact%20performance.%20As%20a%20result%2C%20automatically%20curating%20datasets%0Aby%20filtering%20low-quality%20samples%20to%20improve%20quality%20becomes%20essential.%20Existing%0Arobotic%20curation%20approaches%20rely%20on%20costly%20manual%20annotations%20and%20perform%0Acuration%20at%20a%20coarse%20granularity%2C%20such%20as%20the%20dataset%20or%20trajectory%20level%2C%0Afailing%20to%20account%20for%20the%20quality%20of%20individual%20state-action%20pairs.%20To%20address%0Athis%2C%20we%20introduce%20SCIZOR%2C%20a%20self-supervised%20data%20curation%20framework%20that%0Afilters%20out%20low-quality%20state-action%20pairs%20to%20improve%20the%20performance%20of%0Aimitation%20learning%20policies.%20SCIZOR%20targets%20two%20complementary%20sources%20of%0Alow-quality%20data%3A%20suboptimal%20data%2C%20which%20hinders%20learning%20with%20undesirable%0Aactions%2C%20and%20redundant%20data%2C%20which%20dilutes%20training%20with%20repetitive%20patterns.%0ASCIZOR%20leverages%20a%20self-supervised%20task%20progress%20predictor%20for%20suboptimal%20data%0Ato%20remove%20samples%20lacking%20task%20progression%2C%20and%20a%20deduplication%20module%0Aoperating%20on%20joint%20state-action%20representation%20for%20samples%20with%20redundant%0Apatterns.%20Empirically%2C%20we%20show%20that%20SCIZOR%20enables%20imitation%20learning%20policies%0Ato%20achieve%20higher%20performance%20with%20less%20data%2C%20yielding%20an%20average%20improvement%0Aof%2015.4%25%20across%20multiple%20benchmarks.%20More%20information%20is%20available%20at%3A%0Ahttps%3A//ut-austin-rpl.github.io/SCIZOR/%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22626v1&entry.124074799=Read"},
{"title": "MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway\n  Dynamic Dense Connections", "author": "Da Xiao and Qingye Meng and Shengping Li and Xingyuan Yuan", "abstract": "  We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective\nmethod to address the limitations of residual connections and enhance\ncross-layer information flow in Transformers. Unlike existing dense connection\napproaches with static and shared connection weights, MUDD generates connection\nweights dynamically depending on hidden states at each sequence position and\nfor each decoupled input stream (the query, key, value or residual) of a\nTransformer block. MUDD connections can be seamlessly integrated into any\nTransformer architecture to create MUDDFormer. Extensive experiments show that\nMUDDFormer significantly outperforms Transformers across various model\narchitectures and scales in language modeling, achieving the performance of\nTransformers trained with 1.8X-2.4X compute. Notably, MUDDPythia-2.8B matches\nPythia-6.9B in pretraining ppl and downstream tasks and even rivals Pythia-12B\nin five-shot settings, while adding only 0.23% parameters and 0.4% computation.\nCode in JAX and PyTorch and pre-trained models are available at\nhttps://github.com/Caiyun-AI/MUDDFormer .\n", "link": "http://arxiv.org/abs/2502.12170v2", "date": "2025-05-28", "relevancy": 2.0801, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5566}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5136}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MUDDFormer%3A%20Breaking%20Residual%20Bottlenecks%20in%20Transformers%20via%20Multiway%0A%20%20Dynamic%20Dense%20Connections&body=Title%3A%20MUDDFormer%3A%20Breaking%20Residual%20Bottlenecks%20in%20Transformers%20via%20Multiway%0A%20%20Dynamic%20Dense%20Connections%0AAuthor%3A%20Da%20Xiao%20and%20Qingye%20Meng%20and%20Shengping%20Li%20and%20Xingyuan%20Yuan%0AAbstract%3A%20%20%20We%20propose%20MUltiway%20Dynamic%20Dense%20%28MUDD%29%20connections%2C%20a%20simple%20yet%20effective%0Amethod%20to%20address%20the%20limitations%20of%20residual%20connections%20and%20enhance%0Across-layer%20information%20flow%20in%20Transformers.%20Unlike%20existing%20dense%20connection%0Aapproaches%20with%20static%20and%20shared%20connection%20weights%2C%20MUDD%20generates%20connection%0Aweights%20dynamically%20depending%20on%20hidden%20states%20at%20each%20sequence%20position%20and%0Afor%20each%20decoupled%20input%20stream%20%28the%20query%2C%20key%2C%20value%20or%20residual%29%20of%20a%0ATransformer%20block.%20MUDD%20connections%20can%20be%20seamlessly%20integrated%20into%20any%0ATransformer%20architecture%20to%20create%20MUDDFormer.%20Extensive%20experiments%20show%20that%0AMUDDFormer%20significantly%20outperforms%20Transformers%20across%20various%20model%0Aarchitectures%20and%20scales%20in%20language%20modeling%2C%20achieving%20the%20performance%20of%0ATransformers%20trained%20with%201.8X-2.4X%20compute.%20Notably%2C%20MUDDPythia-2.8B%20matches%0APythia-6.9B%20in%20pretraining%20ppl%20and%20downstream%20tasks%20and%20even%20rivals%20Pythia-12B%0Ain%20five-shot%20settings%2C%20while%20adding%20only%200.23%25%20parameters%20and%200.4%25%20computation.%0ACode%20in%20JAX%20and%20PyTorch%20and%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/Caiyun-AI/MUDDFormer%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12170v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMUDDFormer%253A%2520Breaking%2520Residual%2520Bottlenecks%2520in%2520Transformers%2520via%2520Multiway%250A%2520%2520Dynamic%2520Dense%2520Connections%26entry.906535625%3DDa%2520Xiao%2520and%2520Qingye%2520Meng%2520and%2520Shengping%2520Li%2520and%2520Xingyuan%2520Yuan%26entry.1292438233%3D%2520%2520We%2520propose%2520MUltiway%2520Dynamic%2520Dense%2520%2528MUDD%2529%2520connections%252C%2520a%2520simple%2520yet%2520effective%250Amethod%2520to%2520address%2520the%2520limitations%2520of%2520residual%2520connections%2520and%2520enhance%250Across-layer%2520information%2520flow%2520in%2520Transformers.%2520Unlike%2520existing%2520dense%2520connection%250Aapproaches%2520with%2520static%2520and%2520shared%2520connection%2520weights%252C%2520MUDD%2520generates%2520connection%250Aweights%2520dynamically%2520depending%2520on%2520hidden%2520states%2520at%2520each%2520sequence%2520position%2520and%250Afor%2520each%2520decoupled%2520input%2520stream%2520%2528the%2520query%252C%2520key%252C%2520value%2520or%2520residual%2529%2520of%2520a%250ATransformer%2520block.%2520MUDD%2520connections%2520can%2520be%2520seamlessly%2520integrated%2520into%2520any%250ATransformer%2520architecture%2520to%2520create%2520MUDDFormer.%2520Extensive%2520experiments%2520show%2520that%250AMUDDFormer%2520significantly%2520outperforms%2520Transformers%2520across%2520various%2520model%250Aarchitectures%2520and%2520scales%2520in%2520language%2520modeling%252C%2520achieving%2520the%2520performance%2520of%250ATransformers%2520trained%2520with%25201.8X-2.4X%2520compute.%2520Notably%252C%2520MUDDPythia-2.8B%2520matches%250APythia-6.9B%2520in%2520pretraining%2520ppl%2520and%2520downstream%2520tasks%2520and%2520even%2520rivals%2520Pythia-12B%250Ain%2520five-shot%2520settings%252C%2520while%2520adding%2520only%25200.23%2525%2520parameters%2520and%25200.4%2525%2520computation.%250ACode%2520in%2520JAX%2520and%2520PyTorch%2520and%2520pre-trained%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/Caiyun-AI/MUDDFormer%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12170v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MUDDFormer%3A%20Breaking%20Residual%20Bottlenecks%20in%20Transformers%20via%20Multiway%0A%20%20Dynamic%20Dense%20Connections&entry.906535625=Da%20Xiao%20and%20Qingye%20Meng%20and%20Shengping%20Li%20and%20Xingyuan%20Yuan&entry.1292438233=%20%20We%20propose%20MUltiway%20Dynamic%20Dense%20%28MUDD%29%20connections%2C%20a%20simple%20yet%20effective%0Amethod%20to%20address%20the%20limitations%20of%20residual%20connections%20and%20enhance%0Across-layer%20information%20flow%20in%20Transformers.%20Unlike%20existing%20dense%20connection%0Aapproaches%20with%20static%20and%20shared%20connection%20weights%2C%20MUDD%20generates%20connection%0Aweights%20dynamically%20depending%20on%20hidden%20states%20at%20each%20sequence%20position%20and%0Afor%20each%20decoupled%20input%20stream%20%28the%20query%2C%20key%2C%20value%20or%20residual%29%20of%20a%0ATransformer%20block.%20MUDD%20connections%20can%20be%20seamlessly%20integrated%20into%20any%0ATransformer%20architecture%20to%20create%20MUDDFormer.%20Extensive%20experiments%20show%20that%0AMUDDFormer%20significantly%20outperforms%20Transformers%20across%20various%20model%0Aarchitectures%20and%20scales%20in%20language%20modeling%2C%20achieving%20the%20performance%20of%0ATransformers%20trained%20with%201.8X-2.4X%20compute.%20Notably%2C%20MUDDPythia-2.8B%20matches%0APythia-6.9B%20in%20pretraining%20ppl%20and%20downstream%20tasks%20and%20even%20rivals%20Pythia-12B%0Ain%20five-shot%20settings%2C%20while%20adding%20only%200.23%25%20parameters%20and%200.4%25%20computation.%0ACode%20in%20JAX%20and%20PyTorch%20and%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/Caiyun-AI/MUDDFormer%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12170v2&entry.124074799=Read"},
{"title": "Fully Packed and Ready to Go: High-Density, Rearrangement-Free,\n  Grid-Based Storage and Retrieval", "author": "Tzvika Geft and Kostas Bekris and Jingjin Yu", "abstract": "  Grid-based storage systems with uniformly shaped loads (e.g., containers,\npallets, totes) are commonplace in logistics, industrial, and transportation\ndomains. A key performance metric for such systems is the maximization of space\nutilization, which requires some loads to be placed behind or below others,\npreventing direct access to them. Consequently, dense storage settings bring up\nthe challenge of determining how to place loads while minimizing costly\nrearrangement efforts necessary during retrieval. This paper considers the\nsetting involving an inbound phase, during which loads arrive, followed by an\noutbound phase, during which loads depart. The setting is prevalent in\ndistribution centers, automated parking garages, and container ports. In both\nphases, minimizing the number of rearrangement actions results in more optimal\n(e.g., fast, energy-efficient, etc.) operations. In contrast to previous work\nfocusing on stack-based systems, this effort examines the case where loads can\nbe freely moved along the grid, e.g., by a mobile robot, expanding the range of\npossible motions. We establish that for a range of scenarios, such as having\nlimited prior knowledge of the loads' arrival sequences or grids with a narrow\nopening, a (best possible) rearrangement-free solution always exists, including\nwhen the loads fill the grid to its capacity. In particular, when the sequences\nare fully known, we establish an intriguing characterization showing that\nrearrangement can always be avoided if and only if the open side of the grid\n(used to access the storage) is at least 3 cells wide. We further discuss\nuseful practical implications of our solutions.\n", "link": "http://arxiv.org/abs/2505.22497v1", "date": "2025-05-28", "relevancy": 1.7059, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4621}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4312}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fully%20Packed%20and%20Ready%20to%20Go%3A%20High-Density%2C%20Rearrangement-Free%2C%0A%20%20Grid-Based%20Storage%20and%20Retrieval&body=Title%3A%20Fully%20Packed%20and%20Ready%20to%20Go%3A%20High-Density%2C%20Rearrangement-Free%2C%0A%20%20Grid-Based%20Storage%20and%20Retrieval%0AAuthor%3A%20Tzvika%20Geft%20and%20Kostas%20Bekris%20and%20Jingjin%20Yu%0AAbstract%3A%20%20%20Grid-based%20storage%20systems%20with%20uniformly%20shaped%20loads%20%28e.g.%2C%20containers%2C%0Apallets%2C%20totes%29%20are%20commonplace%20in%20logistics%2C%20industrial%2C%20and%20transportation%0Adomains.%20A%20key%20performance%20metric%20for%20such%20systems%20is%20the%20maximization%20of%20space%0Autilization%2C%20which%20requires%20some%20loads%20to%20be%20placed%20behind%20or%20below%20others%2C%0Apreventing%20direct%20access%20to%20them.%20Consequently%2C%20dense%20storage%20settings%20bring%20up%0Athe%20challenge%20of%20determining%20how%20to%20place%20loads%20while%20minimizing%20costly%0Arearrangement%20efforts%20necessary%20during%20retrieval.%20This%20paper%20considers%20the%0Asetting%20involving%20an%20inbound%20phase%2C%20during%20which%20loads%20arrive%2C%20followed%20by%20an%0Aoutbound%20phase%2C%20during%20which%20loads%20depart.%20The%20setting%20is%20prevalent%20in%0Adistribution%20centers%2C%20automated%20parking%20garages%2C%20and%20container%20ports.%20In%20both%0Aphases%2C%20minimizing%20the%20number%20of%20rearrangement%20actions%20results%20in%20more%20optimal%0A%28e.g.%2C%20fast%2C%20energy-efficient%2C%20etc.%29%20operations.%20In%20contrast%20to%20previous%20work%0Afocusing%20on%20stack-based%20systems%2C%20this%20effort%20examines%20the%20case%20where%20loads%20can%0Abe%20freely%20moved%20along%20the%20grid%2C%20e.g.%2C%20by%20a%20mobile%20robot%2C%20expanding%20the%20range%20of%0Apossible%20motions.%20We%20establish%20that%20for%20a%20range%20of%20scenarios%2C%20such%20as%20having%0Alimited%20prior%20knowledge%20of%20the%20loads%27%20arrival%20sequences%20or%20grids%20with%20a%20narrow%0Aopening%2C%20a%20%28best%20possible%29%20rearrangement-free%20solution%20always%20exists%2C%20including%0Awhen%20the%20loads%20fill%20the%20grid%20to%20its%20capacity.%20In%20particular%2C%20when%20the%20sequences%0Aare%20fully%20known%2C%20we%20establish%20an%20intriguing%20characterization%20showing%20that%0Arearrangement%20can%20always%20be%20avoided%20if%20and%20only%20if%20the%20open%20side%20of%20the%20grid%0A%28used%20to%20access%20the%20storage%29%20is%20at%20least%203%20cells%20wide.%20We%20further%20discuss%0Auseful%20practical%20implications%20of%20our%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22497v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFully%2520Packed%2520and%2520Ready%2520to%2520Go%253A%2520High-Density%252C%2520Rearrangement-Free%252C%250A%2520%2520Grid-Based%2520Storage%2520and%2520Retrieval%26entry.906535625%3DTzvika%2520Geft%2520and%2520Kostas%2520Bekris%2520and%2520Jingjin%2520Yu%26entry.1292438233%3D%2520%2520Grid-based%2520storage%2520systems%2520with%2520uniformly%2520shaped%2520loads%2520%2528e.g.%252C%2520containers%252C%250Apallets%252C%2520totes%2529%2520are%2520commonplace%2520in%2520logistics%252C%2520industrial%252C%2520and%2520transportation%250Adomains.%2520A%2520key%2520performance%2520metric%2520for%2520such%2520systems%2520is%2520the%2520maximization%2520of%2520space%250Autilization%252C%2520which%2520requires%2520some%2520loads%2520to%2520be%2520placed%2520behind%2520or%2520below%2520others%252C%250Apreventing%2520direct%2520access%2520to%2520them.%2520Consequently%252C%2520dense%2520storage%2520settings%2520bring%2520up%250Athe%2520challenge%2520of%2520determining%2520how%2520to%2520place%2520loads%2520while%2520minimizing%2520costly%250Arearrangement%2520efforts%2520necessary%2520during%2520retrieval.%2520This%2520paper%2520considers%2520the%250Asetting%2520involving%2520an%2520inbound%2520phase%252C%2520during%2520which%2520loads%2520arrive%252C%2520followed%2520by%2520an%250Aoutbound%2520phase%252C%2520during%2520which%2520loads%2520depart.%2520The%2520setting%2520is%2520prevalent%2520in%250Adistribution%2520centers%252C%2520automated%2520parking%2520garages%252C%2520and%2520container%2520ports.%2520In%2520both%250Aphases%252C%2520minimizing%2520the%2520number%2520of%2520rearrangement%2520actions%2520results%2520in%2520more%2520optimal%250A%2528e.g.%252C%2520fast%252C%2520energy-efficient%252C%2520etc.%2529%2520operations.%2520In%2520contrast%2520to%2520previous%2520work%250Afocusing%2520on%2520stack-based%2520systems%252C%2520this%2520effort%2520examines%2520the%2520case%2520where%2520loads%2520can%250Abe%2520freely%2520moved%2520along%2520the%2520grid%252C%2520e.g.%252C%2520by%2520a%2520mobile%2520robot%252C%2520expanding%2520the%2520range%2520of%250Apossible%2520motions.%2520We%2520establish%2520that%2520for%2520a%2520range%2520of%2520scenarios%252C%2520such%2520as%2520having%250Alimited%2520prior%2520knowledge%2520of%2520the%2520loads%2527%2520arrival%2520sequences%2520or%2520grids%2520with%2520a%2520narrow%250Aopening%252C%2520a%2520%2528best%2520possible%2529%2520rearrangement-free%2520solution%2520always%2520exists%252C%2520including%250Awhen%2520the%2520loads%2520fill%2520the%2520grid%2520to%2520its%2520capacity.%2520In%2520particular%252C%2520when%2520the%2520sequences%250Aare%2520fully%2520known%252C%2520we%2520establish%2520an%2520intriguing%2520characterization%2520showing%2520that%250Arearrangement%2520can%2520always%2520be%2520avoided%2520if%2520and%2520only%2520if%2520the%2520open%2520side%2520of%2520the%2520grid%250A%2528used%2520to%2520access%2520the%2520storage%2529%2520is%2520at%2520least%25203%2520cells%2520wide.%2520We%2520further%2520discuss%250Auseful%2520practical%2520implications%2520of%2520our%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22497v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fully%20Packed%20and%20Ready%20to%20Go%3A%20High-Density%2C%20Rearrangement-Free%2C%0A%20%20Grid-Based%20Storage%20and%20Retrieval&entry.906535625=Tzvika%20Geft%20and%20Kostas%20Bekris%20and%20Jingjin%20Yu&entry.1292438233=%20%20Grid-based%20storage%20systems%20with%20uniformly%20shaped%20loads%20%28e.g.%2C%20containers%2C%0Apallets%2C%20totes%29%20are%20commonplace%20in%20logistics%2C%20industrial%2C%20and%20transportation%0Adomains.%20A%20key%20performance%20metric%20for%20such%20systems%20is%20the%20maximization%20of%20space%0Autilization%2C%20which%20requires%20some%20loads%20to%20be%20placed%20behind%20or%20below%20others%2C%0Apreventing%20direct%20access%20to%20them.%20Consequently%2C%20dense%20storage%20settings%20bring%20up%0Athe%20challenge%20of%20determining%20how%20to%20place%20loads%20while%20minimizing%20costly%0Arearrangement%20efforts%20necessary%20during%20retrieval.%20This%20paper%20considers%20the%0Asetting%20involving%20an%20inbound%20phase%2C%20during%20which%20loads%20arrive%2C%20followed%20by%20an%0Aoutbound%20phase%2C%20during%20which%20loads%20depart.%20The%20setting%20is%20prevalent%20in%0Adistribution%20centers%2C%20automated%20parking%20garages%2C%20and%20container%20ports.%20In%20both%0Aphases%2C%20minimizing%20the%20number%20of%20rearrangement%20actions%20results%20in%20more%20optimal%0A%28e.g.%2C%20fast%2C%20energy-efficient%2C%20etc.%29%20operations.%20In%20contrast%20to%20previous%20work%0Afocusing%20on%20stack-based%20systems%2C%20this%20effort%20examines%20the%20case%20where%20loads%20can%0Abe%20freely%20moved%20along%20the%20grid%2C%20e.g.%2C%20by%20a%20mobile%20robot%2C%20expanding%20the%20range%20of%0Apossible%20motions.%20We%20establish%20that%20for%20a%20range%20of%20scenarios%2C%20such%20as%20having%0Alimited%20prior%20knowledge%20of%20the%20loads%27%20arrival%20sequences%20or%20grids%20with%20a%20narrow%0Aopening%2C%20a%20%28best%20possible%29%20rearrangement-free%20solution%20always%20exists%2C%20including%0Awhen%20the%20loads%20fill%20the%20grid%20to%20its%20capacity.%20In%20particular%2C%20when%20the%20sequences%0Aare%20fully%20known%2C%20we%20establish%20an%20intriguing%20characterization%20showing%20that%0Arearrangement%20can%20always%20be%20avoided%20if%20and%20only%20if%20the%20open%20side%20of%20the%20grid%0A%28used%20to%20access%20the%20storage%29%20is%20at%20least%203%20cells%20wide.%20We%20further%20discuss%0Auseful%20practical%20implications%20of%20our%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22497v1&entry.124074799=Read"},
{"title": "Inference-Time Scaling for Flow Models via Stochastic Generation and\n  Rollover Budget Forcing", "author": "Jaihoon Kim and Taehoon Yoon and Jisung Hwang and Minhyuk Sung", "abstract": "  We propose an inference-time scaling approach for pretrained flow models.\nRecently, inference-time scaling has gained significant attention in LLMs and\ndiffusion models, improving sample quality or better aligning outputs with user\npreferences by leveraging additional computation. For diffusion models,\nparticle sampling has allowed more efficient scaling due to the stochasticity\nat intermediate denoising steps. On the contrary, while flow models have gained\npopularity as an alternative to diffusion models--offering faster generation\nand high-quality outputs in state-of-the-art image and video generative\nmodels--efficient inference-time scaling methods used for diffusion models\ncannot be directly applied due to their deterministic generative process. To\nenable efficient inference-time scaling for flow models, we propose three key\nideas: 1) SDE-based generation, enabling particle sampling in flow models, 2)\nInterpolant conversion, broadening the search space and enhancing sample\ndiversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of\ncomputational resources across timesteps to maximize budget utilization. Our\nexperiments show that SDE-based generation, particularly variance-preserving\n(VP) interpolant-based generation, improves the performance of particle\nsampling methods for inference-time scaling in flow models. Additionally, we\ndemonstrate that RBF with VP-SDE achieves the best performance, outperforming\nall previous inference-time scaling approaches.\n", "link": "http://arxiv.org/abs/2503.19385v4", "date": "2025-05-28", "relevancy": 1.765, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.7075}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5645}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inference-Time%20Scaling%20for%20Flow%20Models%20via%20Stochastic%20Generation%20and%0A%20%20Rollover%20Budget%20Forcing&body=Title%3A%20Inference-Time%20Scaling%20for%20Flow%20Models%20via%20Stochastic%20Generation%20and%0A%20%20Rollover%20Budget%20Forcing%0AAuthor%3A%20Jaihoon%20Kim%20and%20Taehoon%20Yoon%20and%20Jisung%20Hwang%20and%20Minhyuk%20Sung%0AAbstract%3A%20%20%20We%20propose%20an%20inference-time%20scaling%20approach%20for%20pretrained%20flow%20models.%0ARecently%2C%20inference-time%20scaling%20has%20gained%20significant%20attention%20in%20LLMs%20and%0Adiffusion%20models%2C%20improving%20sample%20quality%20or%20better%20aligning%20outputs%20with%20user%0Apreferences%20by%20leveraging%20additional%20computation.%20For%20diffusion%20models%2C%0Aparticle%20sampling%20has%20allowed%20more%20efficient%20scaling%20due%20to%20the%20stochasticity%0Aat%20intermediate%20denoising%20steps.%20On%20the%20contrary%2C%20while%20flow%20models%20have%20gained%0Apopularity%20as%20an%20alternative%20to%20diffusion%20models--offering%20faster%20generation%0Aand%20high-quality%20outputs%20in%20state-of-the-art%20image%20and%20video%20generative%0Amodels--efficient%20inference-time%20scaling%20methods%20used%20for%20diffusion%20models%0Acannot%20be%20directly%20applied%20due%20to%20their%20deterministic%20generative%20process.%20To%0Aenable%20efficient%20inference-time%20scaling%20for%20flow%20models%2C%20we%20propose%20three%20key%0Aideas%3A%201%29%20SDE-based%20generation%2C%20enabling%20particle%20sampling%20in%20flow%20models%2C%202%29%0AInterpolant%20conversion%2C%20broadening%20the%20search%20space%20and%20enhancing%20sample%0Adiversity%2C%20and%203%29%20Rollover%20Budget%20Forcing%20%28RBF%29%2C%20an%20adaptive%20allocation%20of%0Acomputational%20resources%20across%20timesteps%20to%20maximize%20budget%20utilization.%20Our%0Aexperiments%20show%20that%20SDE-based%20generation%2C%20particularly%20variance-preserving%0A%28VP%29%20interpolant-based%20generation%2C%20improves%20the%20performance%20of%20particle%0Asampling%20methods%20for%20inference-time%20scaling%20in%20flow%20models.%20Additionally%2C%20we%0Ademonstrate%20that%20RBF%20with%20VP-SDE%20achieves%20the%20best%20performance%2C%20outperforming%0Aall%20previous%20inference-time%20scaling%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.19385v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInference-Time%2520Scaling%2520for%2520Flow%2520Models%2520via%2520Stochastic%2520Generation%2520and%250A%2520%2520Rollover%2520Budget%2520Forcing%26entry.906535625%3DJaihoon%2520Kim%2520and%2520Taehoon%2520Yoon%2520and%2520Jisung%2520Hwang%2520and%2520Minhyuk%2520Sung%26entry.1292438233%3D%2520%2520We%2520propose%2520an%2520inference-time%2520scaling%2520approach%2520for%2520pretrained%2520flow%2520models.%250ARecently%252C%2520inference-time%2520scaling%2520has%2520gained%2520significant%2520attention%2520in%2520LLMs%2520and%250Adiffusion%2520models%252C%2520improving%2520sample%2520quality%2520or%2520better%2520aligning%2520outputs%2520with%2520user%250Apreferences%2520by%2520leveraging%2520additional%2520computation.%2520For%2520diffusion%2520models%252C%250Aparticle%2520sampling%2520has%2520allowed%2520more%2520efficient%2520scaling%2520due%2520to%2520the%2520stochasticity%250Aat%2520intermediate%2520denoising%2520steps.%2520On%2520the%2520contrary%252C%2520while%2520flow%2520models%2520have%2520gained%250Apopularity%2520as%2520an%2520alternative%2520to%2520diffusion%2520models--offering%2520faster%2520generation%250Aand%2520high-quality%2520outputs%2520in%2520state-of-the-art%2520image%2520and%2520video%2520generative%250Amodels--efficient%2520inference-time%2520scaling%2520methods%2520used%2520for%2520diffusion%2520models%250Acannot%2520be%2520directly%2520applied%2520due%2520to%2520their%2520deterministic%2520generative%2520process.%2520To%250Aenable%2520efficient%2520inference-time%2520scaling%2520for%2520flow%2520models%252C%2520we%2520propose%2520three%2520key%250Aideas%253A%25201%2529%2520SDE-based%2520generation%252C%2520enabling%2520particle%2520sampling%2520in%2520flow%2520models%252C%25202%2529%250AInterpolant%2520conversion%252C%2520broadening%2520the%2520search%2520space%2520and%2520enhancing%2520sample%250Adiversity%252C%2520and%25203%2529%2520Rollover%2520Budget%2520Forcing%2520%2528RBF%2529%252C%2520an%2520adaptive%2520allocation%2520of%250Acomputational%2520resources%2520across%2520timesteps%2520to%2520maximize%2520budget%2520utilization.%2520Our%250Aexperiments%2520show%2520that%2520SDE-based%2520generation%252C%2520particularly%2520variance-preserving%250A%2528VP%2529%2520interpolant-based%2520generation%252C%2520improves%2520the%2520performance%2520of%2520particle%250Asampling%2520methods%2520for%2520inference-time%2520scaling%2520in%2520flow%2520models.%2520Additionally%252C%2520we%250Ademonstrate%2520that%2520RBF%2520with%2520VP-SDE%2520achieves%2520the%2520best%2520performance%252C%2520outperforming%250Aall%2520previous%2520inference-time%2520scaling%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.19385v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inference-Time%20Scaling%20for%20Flow%20Models%20via%20Stochastic%20Generation%20and%0A%20%20Rollover%20Budget%20Forcing&entry.906535625=Jaihoon%20Kim%20and%20Taehoon%20Yoon%20and%20Jisung%20Hwang%20and%20Minhyuk%20Sung&entry.1292438233=%20%20We%20propose%20an%20inference-time%20scaling%20approach%20for%20pretrained%20flow%20models.%0ARecently%2C%20inference-time%20scaling%20has%20gained%20significant%20attention%20in%20LLMs%20and%0Adiffusion%20models%2C%20improving%20sample%20quality%20or%20better%20aligning%20outputs%20with%20user%0Apreferences%20by%20leveraging%20additional%20computation.%20For%20diffusion%20models%2C%0Aparticle%20sampling%20has%20allowed%20more%20efficient%20scaling%20due%20to%20the%20stochasticity%0Aat%20intermediate%20denoising%20steps.%20On%20the%20contrary%2C%20while%20flow%20models%20have%20gained%0Apopularity%20as%20an%20alternative%20to%20diffusion%20models--offering%20faster%20generation%0Aand%20high-quality%20outputs%20in%20state-of-the-art%20image%20and%20video%20generative%0Amodels--efficient%20inference-time%20scaling%20methods%20used%20for%20diffusion%20models%0Acannot%20be%20directly%20applied%20due%20to%20their%20deterministic%20generative%20process.%20To%0Aenable%20efficient%20inference-time%20scaling%20for%20flow%20models%2C%20we%20propose%20three%20key%0Aideas%3A%201%29%20SDE-based%20generation%2C%20enabling%20particle%20sampling%20in%20flow%20models%2C%202%29%0AInterpolant%20conversion%2C%20broadening%20the%20search%20space%20and%20enhancing%20sample%0Adiversity%2C%20and%203%29%20Rollover%20Budget%20Forcing%20%28RBF%29%2C%20an%20adaptive%20allocation%20of%0Acomputational%20resources%20across%20timesteps%20to%20maximize%20budget%20utilization.%20Our%0Aexperiments%20show%20that%20SDE-based%20generation%2C%20particularly%20variance-preserving%0A%28VP%29%20interpolant-based%20generation%2C%20improves%20the%20performance%20of%20particle%0Asampling%20methods%20for%20inference-time%20scaling%20in%20flow%20models.%20Additionally%2C%20we%0Ademonstrate%20that%20RBF%20with%20VP-SDE%20achieves%20the%20best%20performance%2C%20outperforming%0Aall%20previous%20inference-time%20scaling%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.19385v4&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


