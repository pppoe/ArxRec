<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251218.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation", "author": "Kaiwen Jiang and Xueting Li and Seonwook Park and Ravi Ramamoorthi and Shalini De Mello and Koki Nagano", "abstract": "Portrait animation has witnessed tremendous quality improvements thanks to recent advances in video diffusion models. However, these 2D methods often compromise 3D consistency and speed, limiting their applicability in real-world scenarios, such as digital twins or telepresence. In contrast, 3D-aware facial animation feedforward methods -- built upon explicit 3D representations, such as neural radiance fields or Gaussian splatting -- ensure 3D consistency and achieve faster inference speed, but come with inferior expression details. In this paper, we aim to combine their strengths by distilling knowledge from a 2D diffusion-based method into a feed-forward encoder, which instantly converts an in-the-wild single image into a 3D-consistent, fast yet expressive animatable representation. Our animation representation is decoupled from the face's 3D representation and learns motion implicitly from data, eliminating the dependency on pre-defined parametric models that often constrain animation capabilities. Unlike previous computationally intensive global fusion mechanisms (e.g., multiple attention layers) for fusing 3D structural and animation information, our design employs an efficient lightweight local fusion strategy to achieve high animation expressivity. As a result, our method runs at 107.31 FPS for animation and pose control while achieving comparable animation quality to the state-of-the-art, surpassing alternative designs that trade speed for quality or vice versa. Project website is https://research.nvidia.com/labs/amri/projects/instant4d", "link": "http://arxiv.org/abs/2512.16893v1", "date": "2025-12-18", "relevancy": 3.6865, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.762}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.762}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instant%20Expressive%20Gaussian%20Head%20Avatar%20via%203D-Aware%20Expression%20Distillation&body=Title%3A%20Instant%20Expressive%20Gaussian%20Head%20Avatar%20via%203D-Aware%20Expression%20Distillation%0AAuthor%3A%20Kaiwen%20Jiang%20and%20Xueting%20Li%20and%20Seonwook%20Park%20and%20Ravi%20Ramamoorthi%20and%20Shalini%20De%20Mello%20and%20Koki%20Nagano%0AAbstract%3A%20Portrait%20animation%20has%20witnessed%20tremendous%20quality%20improvements%20thanks%20to%20recent%20advances%20in%20video%20diffusion%20models.%20However%2C%20these%202D%20methods%20often%20compromise%203D%20consistency%20and%20speed%2C%20limiting%20their%20applicability%20in%20real-world%20scenarios%2C%20such%20as%20digital%20twins%20or%20telepresence.%20In%20contrast%2C%203D-aware%20facial%20animation%20feedforward%20methods%20--%20built%20upon%20explicit%203D%20representations%2C%20such%20as%20neural%20radiance%20fields%20or%20Gaussian%20splatting%20--%20ensure%203D%20consistency%20and%20achieve%20faster%20inference%20speed%2C%20but%20come%20with%20inferior%20expression%20details.%20In%20this%20paper%2C%20we%20aim%20to%20combine%20their%20strengths%20by%20distilling%20knowledge%20from%20a%202D%20diffusion-based%20method%20into%20a%20feed-forward%20encoder%2C%20which%20instantly%20converts%20an%20in-the-wild%20single%20image%20into%20a%203D-consistent%2C%20fast%20yet%20expressive%20animatable%20representation.%20Our%20animation%20representation%20is%20decoupled%20from%20the%20face%27s%203D%20representation%20and%20learns%20motion%20implicitly%20from%20data%2C%20eliminating%20the%20dependency%20on%20pre-defined%20parametric%20models%20that%20often%20constrain%20animation%20capabilities.%20Unlike%20previous%20computationally%20intensive%20global%20fusion%20mechanisms%20%28e.g.%2C%20multiple%20attention%20layers%29%20for%20fusing%203D%20structural%20and%20animation%20information%2C%20our%20design%20employs%20an%20efficient%20lightweight%20local%20fusion%20strategy%20to%20achieve%20high%20animation%20expressivity.%20As%20a%20result%2C%20our%20method%20runs%20at%20107.31%20FPS%20for%20animation%20and%20pose%20control%20while%20achieving%20comparable%20animation%20quality%20to%20the%20state-of-the-art%2C%20surpassing%20alternative%20designs%20that%20trade%20speed%20for%20quality%20or%20vice%20versa.%20Project%20website%20is%20https%3A//research.nvidia.com/labs/amri/projects/instant4d%0ALink%3A%20http%3A//arxiv.org/abs/2512.16893v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstant%2520Expressive%2520Gaussian%2520Head%2520Avatar%2520via%25203D-Aware%2520Expression%2520Distillation%26entry.906535625%3DKaiwen%2520Jiang%2520and%2520Xueting%2520Li%2520and%2520Seonwook%2520Park%2520and%2520Ravi%2520Ramamoorthi%2520and%2520Shalini%2520De%2520Mello%2520and%2520Koki%2520Nagano%26entry.1292438233%3DPortrait%2520animation%2520has%2520witnessed%2520tremendous%2520quality%2520improvements%2520thanks%2520to%2520recent%2520advances%2520in%2520video%2520diffusion%2520models.%2520However%252C%2520these%25202D%2520methods%2520often%2520compromise%25203D%2520consistency%2520and%2520speed%252C%2520limiting%2520their%2520applicability%2520in%2520real-world%2520scenarios%252C%2520such%2520as%2520digital%2520twins%2520or%2520telepresence.%2520In%2520contrast%252C%25203D-aware%2520facial%2520animation%2520feedforward%2520methods%2520--%2520built%2520upon%2520explicit%25203D%2520representations%252C%2520such%2520as%2520neural%2520radiance%2520fields%2520or%2520Gaussian%2520splatting%2520--%2520ensure%25203D%2520consistency%2520and%2520achieve%2520faster%2520inference%2520speed%252C%2520but%2520come%2520with%2520inferior%2520expression%2520details.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520combine%2520their%2520strengths%2520by%2520distilling%2520knowledge%2520from%2520a%25202D%2520diffusion-based%2520method%2520into%2520a%2520feed-forward%2520encoder%252C%2520which%2520instantly%2520converts%2520an%2520in-the-wild%2520single%2520image%2520into%2520a%25203D-consistent%252C%2520fast%2520yet%2520expressive%2520animatable%2520representation.%2520Our%2520animation%2520representation%2520is%2520decoupled%2520from%2520the%2520face%2527s%25203D%2520representation%2520and%2520learns%2520motion%2520implicitly%2520from%2520data%252C%2520eliminating%2520the%2520dependency%2520on%2520pre-defined%2520parametric%2520models%2520that%2520often%2520constrain%2520animation%2520capabilities.%2520Unlike%2520previous%2520computationally%2520intensive%2520global%2520fusion%2520mechanisms%2520%2528e.g.%252C%2520multiple%2520attention%2520layers%2529%2520for%2520fusing%25203D%2520structural%2520and%2520animation%2520information%252C%2520our%2520design%2520employs%2520an%2520efficient%2520lightweight%2520local%2520fusion%2520strategy%2520to%2520achieve%2520high%2520animation%2520expressivity.%2520As%2520a%2520result%252C%2520our%2520method%2520runs%2520at%2520107.31%2520FPS%2520for%2520animation%2520and%2520pose%2520control%2520while%2520achieving%2520comparable%2520animation%2520quality%2520to%2520the%2520state-of-the-art%252C%2520surpassing%2520alternative%2520designs%2520that%2520trade%2520speed%2520for%2520quality%2520or%2520vice%2520versa.%2520Project%2520website%2520is%2520https%253A//research.nvidia.com/labs/amri/projects/instant4d%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16893v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instant%20Expressive%20Gaussian%20Head%20Avatar%20via%203D-Aware%20Expression%20Distillation&entry.906535625=Kaiwen%20Jiang%20and%20Xueting%20Li%20and%20Seonwook%20Park%20and%20Ravi%20Ramamoorthi%20and%20Shalini%20De%20Mello%20and%20Koki%20Nagano&entry.1292438233=Portrait%20animation%20has%20witnessed%20tremendous%20quality%20improvements%20thanks%20to%20recent%20advances%20in%20video%20diffusion%20models.%20However%2C%20these%202D%20methods%20often%20compromise%203D%20consistency%20and%20speed%2C%20limiting%20their%20applicability%20in%20real-world%20scenarios%2C%20such%20as%20digital%20twins%20or%20telepresence.%20In%20contrast%2C%203D-aware%20facial%20animation%20feedforward%20methods%20--%20built%20upon%20explicit%203D%20representations%2C%20such%20as%20neural%20radiance%20fields%20or%20Gaussian%20splatting%20--%20ensure%203D%20consistency%20and%20achieve%20faster%20inference%20speed%2C%20but%20come%20with%20inferior%20expression%20details.%20In%20this%20paper%2C%20we%20aim%20to%20combine%20their%20strengths%20by%20distilling%20knowledge%20from%20a%202D%20diffusion-based%20method%20into%20a%20feed-forward%20encoder%2C%20which%20instantly%20converts%20an%20in-the-wild%20single%20image%20into%20a%203D-consistent%2C%20fast%20yet%20expressive%20animatable%20representation.%20Our%20animation%20representation%20is%20decoupled%20from%20the%20face%27s%203D%20representation%20and%20learns%20motion%20implicitly%20from%20data%2C%20eliminating%20the%20dependency%20on%20pre-defined%20parametric%20models%20that%20often%20constrain%20animation%20capabilities.%20Unlike%20previous%20computationally%20intensive%20global%20fusion%20mechanisms%20%28e.g.%2C%20multiple%20attention%20layers%29%20for%20fusing%203D%20structural%20and%20animation%20information%2C%20our%20design%20employs%20an%20efficient%20lightweight%20local%20fusion%20strategy%20to%20achieve%20high%20animation%20expressivity.%20As%20a%20result%2C%20our%20method%20runs%20at%20107.31%20FPS%20for%20animation%20and%20pose%20control%20while%20achieving%20comparable%20animation%20quality%20to%20the%20state-of-the-art%2C%20surpassing%20alternative%20designs%20that%20trade%20speed%20for%20quality%20or%20vice%20versa.%20Project%20website%20is%20https%3A//research.nvidia.com/labs/amri/projects/instant4d&entry.1838667208=http%3A//arxiv.org/abs/2512.16893v1&entry.124074799=Read"},
{"title": "Using Gaussian Splats to Create High-Fidelity Facial Geometry and Texture", "author": "Haodi He and Jihun Yu and Ronald Fedkiw", "abstract": "We leverage increasingly popular three-dimensional neural representations in order to construct a unified and consistent explanation of a collection of uncalibrated images of the human face. Our approach utilizes Gaussian Splatting, since it is more explicit and thus more amenable to constraints than NeRFs. We leverage segmentation annotations to align the semantic regions of the face, facilitating the reconstruction of a neutral pose from only 11 images (as opposed to requiring a long video). We soft constrain the Gaussians to an underlying triangulated surface in order to provide a more structured Gaussian Splat reconstruction, which in turn informs subsequent perturbations to increase the accuracy of the underlying triangulated surface. The resulting triangulated surface can then be used in a standard graphics pipeline. In addition, and perhaps most impactful, we show how accurate geometry enables the Gaussian Splats to be transformed into texture space where they can be treated as a view-dependent neural texture. This allows one to use high visual fidelity Gaussian Splatting on any asset in a scene without the need to modify any other asset or any other aspect (geometry, lighting, renderer, etc.) of the graphics pipeline. We utilize a relightable Gaussian model to disentangle texture from lighting in order to obtain a delit high-resolution albedo texture that is also readily usable in a standard graphics pipeline. The flexibility of our system allows for training with disparate images, even with incompatible lighting, facilitating robust regularization. Finally, we demonstrate the efficacy of our approach by illustrating its use in a text-driven asset creation pipeline.", "link": "http://arxiv.org/abs/2512.16397v1", "date": "2025-12-18", "relevancy": 3.5875, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7333}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7279}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20Gaussian%20Splats%20to%20Create%20High-Fidelity%20Facial%20Geometry%20and%20Texture&body=Title%3A%20Using%20Gaussian%20Splats%20to%20Create%20High-Fidelity%20Facial%20Geometry%20and%20Texture%0AAuthor%3A%20Haodi%20He%20and%20Jihun%20Yu%20and%20Ronald%20Fedkiw%0AAbstract%3A%20We%20leverage%20increasingly%20popular%20three-dimensional%20neural%20representations%20in%20order%20to%20construct%20a%20unified%20and%20consistent%20explanation%20of%20a%20collection%20of%20uncalibrated%20images%20of%20the%20human%20face.%20Our%20approach%20utilizes%20Gaussian%20Splatting%2C%20since%20it%20is%20more%20explicit%20and%20thus%20more%20amenable%20to%20constraints%20than%20NeRFs.%20We%20leverage%20segmentation%20annotations%20to%20align%20the%20semantic%20regions%20of%20the%20face%2C%20facilitating%20the%20reconstruction%20of%20a%20neutral%20pose%20from%20only%2011%20images%20%28as%20opposed%20to%20requiring%20a%20long%20video%29.%20We%20soft%20constrain%20the%20Gaussians%20to%20an%20underlying%20triangulated%20surface%20in%20order%20to%20provide%20a%20more%20structured%20Gaussian%20Splat%20reconstruction%2C%20which%20in%20turn%20informs%20subsequent%20perturbations%20to%20increase%20the%20accuracy%20of%20the%20underlying%20triangulated%20surface.%20The%20resulting%20triangulated%20surface%20can%20then%20be%20used%20in%20a%20standard%20graphics%20pipeline.%20In%20addition%2C%20and%20perhaps%20most%20impactful%2C%20we%20show%20how%20accurate%20geometry%20enables%20the%20Gaussian%20Splats%20to%20be%20transformed%20into%20texture%20space%20where%20they%20can%20be%20treated%20as%20a%20view-dependent%20neural%20texture.%20This%20allows%20one%20to%20use%20high%20visual%20fidelity%20Gaussian%20Splatting%20on%20any%20asset%20in%20a%20scene%20without%20the%20need%20to%20modify%20any%20other%20asset%20or%20any%20other%20aspect%20%28geometry%2C%20lighting%2C%20renderer%2C%20etc.%29%20of%20the%20graphics%20pipeline.%20We%20utilize%20a%20relightable%20Gaussian%20model%20to%20disentangle%20texture%20from%20lighting%20in%20order%20to%20obtain%20a%20delit%20high-resolution%20albedo%20texture%20that%20is%20also%20readily%20usable%20in%20a%20standard%20graphics%20pipeline.%20The%20flexibility%20of%20our%20system%20allows%20for%20training%20with%20disparate%20images%2C%20even%20with%20incompatible%20lighting%2C%20facilitating%20robust%20regularization.%20Finally%2C%20we%20demonstrate%20the%20efficacy%20of%20our%20approach%20by%20illustrating%20its%20use%20in%20a%20text-driven%20asset%20creation%20pipeline.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16397v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520Gaussian%2520Splats%2520to%2520Create%2520High-Fidelity%2520Facial%2520Geometry%2520and%2520Texture%26entry.906535625%3DHaodi%2520He%2520and%2520Jihun%2520Yu%2520and%2520Ronald%2520Fedkiw%26entry.1292438233%3DWe%2520leverage%2520increasingly%2520popular%2520three-dimensional%2520neural%2520representations%2520in%2520order%2520to%2520construct%2520a%2520unified%2520and%2520consistent%2520explanation%2520of%2520a%2520collection%2520of%2520uncalibrated%2520images%2520of%2520the%2520human%2520face.%2520Our%2520approach%2520utilizes%2520Gaussian%2520Splatting%252C%2520since%2520it%2520is%2520more%2520explicit%2520and%2520thus%2520more%2520amenable%2520to%2520constraints%2520than%2520NeRFs.%2520We%2520leverage%2520segmentation%2520annotations%2520to%2520align%2520the%2520semantic%2520regions%2520of%2520the%2520face%252C%2520facilitating%2520the%2520reconstruction%2520of%2520a%2520neutral%2520pose%2520from%2520only%252011%2520images%2520%2528as%2520opposed%2520to%2520requiring%2520a%2520long%2520video%2529.%2520We%2520soft%2520constrain%2520the%2520Gaussians%2520to%2520an%2520underlying%2520triangulated%2520surface%2520in%2520order%2520to%2520provide%2520a%2520more%2520structured%2520Gaussian%2520Splat%2520reconstruction%252C%2520which%2520in%2520turn%2520informs%2520subsequent%2520perturbations%2520to%2520increase%2520the%2520accuracy%2520of%2520the%2520underlying%2520triangulated%2520surface.%2520The%2520resulting%2520triangulated%2520surface%2520can%2520then%2520be%2520used%2520in%2520a%2520standard%2520graphics%2520pipeline.%2520In%2520addition%252C%2520and%2520perhaps%2520most%2520impactful%252C%2520we%2520show%2520how%2520accurate%2520geometry%2520enables%2520the%2520Gaussian%2520Splats%2520to%2520be%2520transformed%2520into%2520texture%2520space%2520where%2520they%2520can%2520be%2520treated%2520as%2520a%2520view-dependent%2520neural%2520texture.%2520This%2520allows%2520one%2520to%2520use%2520high%2520visual%2520fidelity%2520Gaussian%2520Splatting%2520on%2520any%2520asset%2520in%2520a%2520scene%2520without%2520the%2520need%2520to%2520modify%2520any%2520other%2520asset%2520or%2520any%2520other%2520aspect%2520%2528geometry%252C%2520lighting%252C%2520renderer%252C%2520etc.%2529%2520of%2520the%2520graphics%2520pipeline.%2520We%2520utilize%2520a%2520relightable%2520Gaussian%2520model%2520to%2520disentangle%2520texture%2520from%2520lighting%2520in%2520order%2520to%2520obtain%2520a%2520delit%2520high-resolution%2520albedo%2520texture%2520that%2520is%2520also%2520readily%2520usable%2520in%2520a%2520standard%2520graphics%2520pipeline.%2520The%2520flexibility%2520of%2520our%2520system%2520allows%2520for%2520training%2520with%2520disparate%2520images%252C%2520even%2520with%2520incompatible%2520lighting%252C%2520facilitating%2520robust%2520regularization.%2520Finally%252C%2520we%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520approach%2520by%2520illustrating%2520its%2520use%2520in%2520a%2520text-driven%2520asset%2520creation%2520pipeline.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16397v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Gaussian%20Splats%20to%20Create%20High-Fidelity%20Facial%20Geometry%20and%20Texture&entry.906535625=Haodi%20He%20and%20Jihun%20Yu%20and%20Ronald%20Fedkiw&entry.1292438233=We%20leverage%20increasingly%20popular%20three-dimensional%20neural%20representations%20in%20order%20to%20construct%20a%20unified%20and%20consistent%20explanation%20of%20a%20collection%20of%20uncalibrated%20images%20of%20the%20human%20face.%20Our%20approach%20utilizes%20Gaussian%20Splatting%2C%20since%20it%20is%20more%20explicit%20and%20thus%20more%20amenable%20to%20constraints%20than%20NeRFs.%20We%20leverage%20segmentation%20annotations%20to%20align%20the%20semantic%20regions%20of%20the%20face%2C%20facilitating%20the%20reconstruction%20of%20a%20neutral%20pose%20from%20only%2011%20images%20%28as%20opposed%20to%20requiring%20a%20long%20video%29.%20We%20soft%20constrain%20the%20Gaussians%20to%20an%20underlying%20triangulated%20surface%20in%20order%20to%20provide%20a%20more%20structured%20Gaussian%20Splat%20reconstruction%2C%20which%20in%20turn%20informs%20subsequent%20perturbations%20to%20increase%20the%20accuracy%20of%20the%20underlying%20triangulated%20surface.%20The%20resulting%20triangulated%20surface%20can%20then%20be%20used%20in%20a%20standard%20graphics%20pipeline.%20In%20addition%2C%20and%20perhaps%20most%20impactful%2C%20we%20show%20how%20accurate%20geometry%20enables%20the%20Gaussian%20Splats%20to%20be%20transformed%20into%20texture%20space%20where%20they%20can%20be%20treated%20as%20a%20view-dependent%20neural%20texture.%20This%20allows%20one%20to%20use%20high%20visual%20fidelity%20Gaussian%20Splatting%20on%20any%20asset%20in%20a%20scene%20without%20the%20need%20to%20modify%20any%20other%20asset%20or%20any%20other%20aspect%20%28geometry%2C%20lighting%2C%20renderer%2C%20etc.%29%20of%20the%20graphics%20pipeline.%20We%20utilize%20a%20relightable%20Gaussian%20model%20to%20disentangle%20texture%20from%20lighting%20in%20order%20to%20obtain%20a%20delit%20high-resolution%20albedo%20texture%20that%20is%20also%20readily%20usable%20in%20a%20standard%20graphics%20pipeline.%20The%20flexibility%20of%20our%20system%20allows%20for%20training%20with%20disparate%20images%2C%20even%20with%20incompatible%20lighting%2C%20facilitating%20robust%20regularization.%20Finally%2C%20we%20demonstrate%20the%20efficacy%20of%20our%20approach%20by%20illustrating%20its%20use%20in%20a%20text-driven%20asset%20creation%20pipeline.&entry.1838667208=http%3A//arxiv.org/abs/2512.16397v1&entry.124074799=Read"},
{"title": "Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation", "author": "Zhiyang Guo and Ori Zhang and Jax Xiang and Alan Zhao and Wengang Zhou and Houqiang Li", "abstract": "Posing 3D characters is a fundamental task in computer graphics and vision. However, existing methods like auto-rigging and pose-conditioned generation often struggle with challenges such as inaccurate skinning weight prediction, topological imperfections, and poor pose conformance, limiting their robustness and generalizability. To overcome these limitations, we introduce Make-It-Poseable, a novel feed-forward framework that reformulates character posing as a latent-space transformation problem. Instead of deforming mesh vertices as in traditional pipelines, our method reconstructs the character in new poses by directly manipulating its latent representation. At the core of our method is a latent posing transformer that manipulates shape tokens based on skeletal motion. This process is facilitated by a dense pose representation for precise control. To ensure high-fidelity geometry and accommodate topological changes, we also introduce a latent-space supervision strategy and an adaptive completion module. Our method demonstrates superior performance in posing quality. It also naturally extends to 3D editing applications like part replacement and refinement.", "link": "http://arxiv.org/abs/2512.16767v1", "date": "2025-12-18", "relevancy": 3.34, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7192}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6921}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Make-It-Poseable%3A%20Feed-forward%20Latent%20Posing%20Model%20for%203D%20Humanoid%20Character%20Animation&body=Title%3A%20Make-It-Poseable%3A%20Feed-forward%20Latent%20Posing%20Model%20for%203D%20Humanoid%20Character%20Animation%0AAuthor%3A%20Zhiyang%20Guo%20and%20Ori%20Zhang%20and%20Jax%20Xiang%20and%20Alan%20Zhao%20and%20Wengang%20Zhou%20and%20Houqiang%20Li%0AAbstract%3A%20Posing%203D%20characters%20is%20a%20fundamental%20task%20in%20computer%20graphics%20and%20vision.%20However%2C%20existing%20methods%20like%20auto-rigging%20and%20pose-conditioned%20generation%20often%20struggle%20with%20challenges%20such%20as%20inaccurate%20skinning%20weight%20prediction%2C%20topological%20imperfections%2C%20and%20poor%20pose%20conformance%2C%20limiting%20their%20robustness%20and%20generalizability.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20Make-It-Poseable%2C%20a%20novel%20feed-forward%20framework%20that%20reformulates%20character%20posing%20as%20a%20latent-space%20transformation%20problem.%20Instead%20of%20deforming%20mesh%20vertices%20as%20in%20traditional%20pipelines%2C%20our%20method%20reconstructs%20the%20character%20in%20new%20poses%20by%20directly%20manipulating%20its%20latent%20representation.%20At%20the%20core%20of%20our%20method%20is%20a%20latent%20posing%20transformer%20that%20manipulates%20shape%20tokens%20based%20on%20skeletal%20motion.%20This%20process%20is%20facilitated%20by%20a%20dense%20pose%20representation%20for%20precise%20control.%20To%20ensure%20high-fidelity%20geometry%20and%20accommodate%20topological%20changes%2C%20we%20also%20introduce%20a%20latent-space%20supervision%20strategy%20and%20an%20adaptive%20completion%20module.%20Our%20method%20demonstrates%20superior%20performance%20in%20posing%20quality.%20It%20also%20naturally%20extends%20to%203D%20editing%20applications%20like%20part%20replacement%20and%20refinement.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16767v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMake-It-Poseable%253A%2520Feed-forward%2520Latent%2520Posing%2520Model%2520for%25203D%2520Humanoid%2520Character%2520Animation%26entry.906535625%3DZhiyang%2520Guo%2520and%2520Ori%2520Zhang%2520and%2520Jax%2520Xiang%2520and%2520Alan%2520Zhao%2520and%2520Wengang%2520Zhou%2520and%2520Houqiang%2520Li%26entry.1292438233%3DPosing%25203D%2520characters%2520is%2520a%2520fundamental%2520task%2520in%2520computer%2520graphics%2520and%2520vision.%2520However%252C%2520existing%2520methods%2520like%2520auto-rigging%2520and%2520pose-conditioned%2520generation%2520often%2520struggle%2520with%2520challenges%2520such%2520as%2520inaccurate%2520skinning%2520weight%2520prediction%252C%2520topological%2520imperfections%252C%2520and%2520poor%2520pose%2520conformance%252C%2520limiting%2520their%2520robustness%2520and%2520generalizability.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520introduce%2520Make-It-Poseable%252C%2520a%2520novel%2520feed-forward%2520framework%2520that%2520reformulates%2520character%2520posing%2520as%2520a%2520latent-space%2520transformation%2520problem.%2520Instead%2520of%2520deforming%2520mesh%2520vertices%2520as%2520in%2520traditional%2520pipelines%252C%2520our%2520method%2520reconstructs%2520the%2520character%2520in%2520new%2520poses%2520by%2520directly%2520manipulating%2520its%2520latent%2520representation.%2520At%2520the%2520core%2520of%2520our%2520method%2520is%2520a%2520latent%2520posing%2520transformer%2520that%2520manipulates%2520shape%2520tokens%2520based%2520on%2520skeletal%2520motion.%2520This%2520process%2520is%2520facilitated%2520by%2520a%2520dense%2520pose%2520representation%2520for%2520precise%2520control.%2520To%2520ensure%2520high-fidelity%2520geometry%2520and%2520accommodate%2520topological%2520changes%252C%2520we%2520also%2520introduce%2520a%2520latent-space%2520supervision%2520strategy%2520and%2520an%2520adaptive%2520completion%2520module.%2520Our%2520method%2520demonstrates%2520superior%2520performance%2520in%2520posing%2520quality.%2520It%2520also%2520naturally%2520extends%2520to%25203D%2520editing%2520applications%2520like%2520part%2520replacement%2520and%2520refinement.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16767v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Make-It-Poseable%3A%20Feed-forward%20Latent%20Posing%20Model%20for%203D%20Humanoid%20Character%20Animation&entry.906535625=Zhiyang%20Guo%20and%20Ori%20Zhang%20and%20Jax%20Xiang%20and%20Alan%20Zhao%20and%20Wengang%20Zhou%20and%20Houqiang%20Li&entry.1292438233=Posing%203D%20characters%20is%20a%20fundamental%20task%20in%20computer%20graphics%20and%20vision.%20However%2C%20existing%20methods%20like%20auto-rigging%20and%20pose-conditioned%20generation%20often%20struggle%20with%20challenges%20such%20as%20inaccurate%20skinning%20weight%20prediction%2C%20topological%20imperfections%2C%20and%20poor%20pose%20conformance%2C%20limiting%20their%20robustness%20and%20generalizability.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20Make-It-Poseable%2C%20a%20novel%20feed-forward%20framework%20that%20reformulates%20character%20posing%20as%20a%20latent-space%20transformation%20problem.%20Instead%20of%20deforming%20mesh%20vertices%20as%20in%20traditional%20pipelines%2C%20our%20method%20reconstructs%20the%20character%20in%20new%20poses%20by%20directly%20manipulating%20its%20latent%20representation.%20At%20the%20core%20of%20our%20method%20is%20a%20latent%20posing%20transformer%20that%20manipulates%20shape%20tokens%20based%20on%20skeletal%20motion.%20This%20process%20is%20facilitated%20by%20a%20dense%20pose%20representation%20for%20precise%20control.%20To%20ensure%20high-fidelity%20geometry%20and%20accommodate%20topological%20changes%2C%20we%20also%20introduce%20a%20latent-space%20supervision%20strategy%20and%20an%20adaptive%20completion%20module.%20Our%20method%20demonstrates%20superior%20performance%20in%20posing%20quality.%20It%20also%20naturally%20extends%20to%203D%20editing%20applications%20like%20part%20replacement%20and%20refinement.&entry.1838667208=http%3A//arxiv.org/abs/2512.16767v1&entry.124074799=Read"},
{"title": "N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models", "author": "Yuxin Wang and Lei Ke and Boqiang Zhang and Tianyuan Qu and Hanxun Yu and Zhenpeng Huang and Meng Yu and Dan Xu and Dong Yu", "abstract": "While current multimodal models can answer questions based on 2D images, they lack intrinsic 3D object perception, limiting their ability to comprehend spatial relationships and depth cues in 3D scenes. In this work, we propose N3D-VLM, a novel unified framework that seamlessly integrates native 3D object perception with 3D-aware visual reasoning, enabling both precise 3D grounding and interpretable spatial understanding. Unlike conventional end-to-end models that directly predict answers from RGB/RGB-D inputs, our approach equips the model with native 3D object perception capabilities, enabling it to directly localize objects in 3D space based on textual descriptions. Building upon accurate 3D object localization, the model further performs explicit reasoning in 3D, achieving more interpretable and structured spatial understanding. To support robust training for these capabilities, we develop a scalable data construction pipeline that leverages depth estimation to lift large-scale 2D annotations into 3D space, significantly increasing the diversity and coverage for 3D object grounding data, yielding over six times larger than the largest existing single-image 3D detection dataset. Moreover, the pipeline generates spatial question-answering datasets that target chain-of-thought (CoT) reasoning in 3D, facilitating joint training for both 3D object localization and 3D spatial reasoning. Experimental results demonstrate that our unified framework not only achieves state-of-the-art performance on 3D grounding tasks, but also consistently surpasses existing methods in 3D spatial reasoning in vision-language model.", "link": "http://arxiv.org/abs/2512.16561v1", "date": "2025-12-18", "relevancy": 3.2999, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.677}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.677}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20N3D-VLM%3A%20Native%203D%20Grounding%20Enables%20Accurate%20Spatial%20Reasoning%20in%20Vision-Language%20Models&body=Title%3A%20N3D-VLM%3A%20Native%203D%20Grounding%20Enables%20Accurate%20Spatial%20Reasoning%20in%20Vision-Language%20Models%0AAuthor%3A%20Yuxin%20Wang%20and%20Lei%20Ke%20and%20Boqiang%20Zhang%20and%20Tianyuan%20Qu%20and%20Hanxun%20Yu%20and%20Zhenpeng%20Huang%20and%20Meng%20Yu%20and%20Dan%20Xu%20and%20Dong%20Yu%0AAbstract%3A%20While%20current%20multimodal%20models%20can%20answer%20questions%20based%20on%202D%20images%2C%20they%20lack%20intrinsic%203D%20object%20perception%2C%20limiting%20their%20ability%20to%20comprehend%20spatial%20relationships%20and%20depth%20cues%20in%203D%20scenes.%20In%20this%20work%2C%20we%20propose%20N3D-VLM%2C%20a%20novel%20unified%20framework%20that%20seamlessly%20integrates%20native%203D%20object%20perception%20with%203D-aware%20visual%20reasoning%2C%20enabling%20both%20precise%203D%20grounding%20and%20interpretable%20spatial%20understanding.%20Unlike%20conventional%20end-to-end%20models%20that%20directly%20predict%20answers%20from%20RGB/RGB-D%20inputs%2C%20our%20approach%20equips%20the%20model%20with%20native%203D%20object%20perception%20capabilities%2C%20enabling%20it%20to%20directly%20localize%20objects%20in%203D%20space%20based%20on%20textual%20descriptions.%20Building%20upon%20accurate%203D%20object%20localization%2C%20the%20model%20further%20performs%20explicit%20reasoning%20in%203D%2C%20achieving%20more%20interpretable%20and%20structured%20spatial%20understanding.%20To%20support%20robust%20training%20for%20these%20capabilities%2C%20we%20develop%20a%20scalable%20data%20construction%20pipeline%20that%20leverages%20depth%20estimation%20to%20lift%20large-scale%202D%20annotations%20into%203D%20space%2C%20significantly%20increasing%20the%20diversity%20and%20coverage%20for%203D%20object%20grounding%20data%2C%20yielding%20over%20six%20times%20larger%20than%20the%20largest%20existing%20single-image%203D%20detection%20dataset.%20Moreover%2C%20the%20pipeline%20generates%20spatial%20question-answering%20datasets%20that%20target%20chain-of-thought%20%28CoT%29%20reasoning%20in%203D%2C%20facilitating%20joint%20training%20for%20both%203D%20object%20localization%20and%203D%20spatial%20reasoning.%20Experimental%20results%20demonstrate%20that%20our%20unified%20framework%20not%20only%20achieves%20state-of-the-art%20performance%20on%203D%20grounding%20tasks%2C%20but%20also%20consistently%20surpasses%20existing%20methods%20in%203D%20spatial%20reasoning%20in%20vision-language%20model.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16561v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DN3D-VLM%253A%2520Native%25203D%2520Grounding%2520Enables%2520Accurate%2520Spatial%2520Reasoning%2520in%2520Vision-Language%2520Models%26entry.906535625%3DYuxin%2520Wang%2520and%2520Lei%2520Ke%2520and%2520Boqiang%2520Zhang%2520and%2520Tianyuan%2520Qu%2520and%2520Hanxun%2520Yu%2520and%2520Zhenpeng%2520Huang%2520and%2520Meng%2520Yu%2520and%2520Dan%2520Xu%2520and%2520Dong%2520Yu%26entry.1292438233%3DWhile%2520current%2520multimodal%2520models%2520can%2520answer%2520questions%2520based%2520on%25202D%2520images%252C%2520they%2520lack%2520intrinsic%25203D%2520object%2520perception%252C%2520limiting%2520their%2520ability%2520to%2520comprehend%2520spatial%2520relationships%2520and%2520depth%2520cues%2520in%25203D%2520scenes.%2520In%2520this%2520work%252C%2520we%2520propose%2520N3D-VLM%252C%2520a%2520novel%2520unified%2520framework%2520that%2520seamlessly%2520integrates%2520native%25203D%2520object%2520perception%2520with%25203D-aware%2520visual%2520reasoning%252C%2520enabling%2520both%2520precise%25203D%2520grounding%2520and%2520interpretable%2520spatial%2520understanding.%2520Unlike%2520conventional%2520end-to-end%2520models%2520that%2520directly%2520predict%2520answers%2520from%2520RGB/RGB-D%2520inputs%252C%2520our%2520approach%2520equips%2520the%2520model%2520with%2520native%25203D%2520object%2520perception%2520capabilities%252C%2520enabling%2520it%2520to%2520directly%2520localize%2520objects%2520in%25203D%2520space%2520based%2520on%2520textual%2520descriptions.%2520Building%2520upon%2520accurate%25203D%2520object%2520localization%252C%2520the%2520model%2520further%2520performs%2520explicit%2520reasoning%2520in%25203D%252C%2520achieving%2520more%2520interpretable%2520and%2520structured%2520spatial%2520understanding.%2520To%2520support%2520robust%2520training%2520for%2520these%2520capabilities%252C%2520we%2520develop%2520a%2520scalable%2520data%2520construction%2520pipeline%2520that%2520leverages%2520depth%2520estimation%2520to%2520lift%2520large-scale%25202D%2520annotations%2520into%25203D%2520space%252C%2520significantly%2520increasing%2520the%2520diversity%2520and%2520coverage%2520for%25203D%2520object%2520grounding%2520data%252C%2520yielding%2520over%2520six%2520times%2520larger%2520than%2520the%2520largest%2520existing%2520single-image%25203D%2520detection%2520dataset.%2520Moreover%252C%2520the%2520pipeline%2520generates%2520spatial%2520question-answering%2520datasets%2520that%2520target%2520chain-of-thought%2520%2528CoT%2529%2520reasoning%2520in%25203D%252C%2520facilitating%2520joint%2520training%2520for%2520both%25203D%2520object%2520localization%2520and%25203D%2520spatial%2520reasoning.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520unified%2520framework%2520not%2520only%2520achieves%2520state-of-the-art%2520performance%2520on%25203D%2520grounding%2520tasks%252C%2520but%2520also%2520consistently%2520surpasses%2520existing%2520methods%2520in%25203D%2520spatial%2520reasoning%2520in%2520vision-language%2520model.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16561v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=N3D-VLM%3A%20Native%203D%20Grounding%20Enables%20Accurate%20Spatial%20Reasoning%20in%20Vision-Language%20Models&entry.906535625=Yuxin%20Wang%20and%20Lei%20Ke%20and%20Boqiang%20Zhang%20and%20Tianyuan%20Qu%20and%20Hanxun%20Yu%20and%20Zhenpeng%20Huang%20and%20Meng%20Yu%20and%20Dan%20Xu%20and%20Dong%20Yu&entry.1292438233=While%20current%20multimodal%20models%20can%20answer%20questions%20based%20on%202D%20images%2C%20they%20lack%20intrinsic%203D%20object%20perception%2C%20limiting%20their%20ability%20to%20comprehend%20spatial%20relationships%20and%20depth%20cues%20in%203D%20scenes.%20In%20this%20work%2C%20we%20propose%20N3D-VLM%2C%20a%20novel%20unified%20framework%20that%20seamlessly%20integrates%20native%203D%20object%20perception%20with%203D-aware%20visual%20reasoning%2C%20enabling%20both%20precise%203D%20grounding%20and%20interpretable%20spatial%20understanding.%20Unlike%20conventional%20end-to-end%20models%20that%20directly%20predict%20answers%20from%20RGB/RGB-D%20inputs%2C%20our%20approach%20equips%20the%20model%20with%20native%203D%20object%20perception%20capabilities%2C%20enabling%20it%20to%20directly%20localize%20objects%20in%203D%20space%20based%20on%20textual%20descriptions.%20Building%20upon%20accurate%203D%20object%20localization%2C%20the%20model%20further%20performs%20explicit%20reasoning%20in%203D%2C%20achieving%20more%20interpretable%20and%20structured%20spatial%20understanding.%20To%20support%20robust%20training%20for%20these%20capabilities%2C%20we%20develop%20a%20scalable%20data%20construction%20pipeline%20that%20leverages%20depth%20estimation%20to%20lift%20large-scale%202D%20annotations%20into%203D%20space%2C%20significantly%20increasing%20the%20diversity%20and%20coverage%20for%203D%20object%20grounding%20data%2C%20yielding%20over%20six%20times%20larger%20than%20the%20largest%20existing%20single-image%203D%20detection%20dataset.%20Moreover%2C%20the%20pipeline%20generates%20spatial%20question-answering%20datasets%20that%20target%20chain-of-thought%20%28CoT%29%20reasoning%20in%203D%2C%20facilitating%20joint%20training%20for%20both%203D%20object%20localization%20and%203D%20spatial%20reasoning.%20Experimental%20results%20demonstrate%20that%20our%20unified%20framework%20not%20only%20achieves%20state-of-the-art%20performance%20on%203D%20grounding%20tasks%2C%20but%20also%20consistently%20surpasses%20existing%20methods%20in%203D%20spatial%20reasoning%20in%20vision-language%20model.&entry.1838667208=http%3A//arxiv.org/abs/2512.16561v1&entry.124074799=Read"},
{"title": "4D Primitive-M\u00e2ch\u00e9: Glueing Primitives for Persistent 4D Scene Reconstruction", "author": "Kirill Mazur and Marwan Taher and Andrew J. Davison", "abstract": "We present a dynamic reconstruction system that receives a casual monocular RGB video as input, and outputs a complete and persistent reconstruction of the scene. In other words, we reconstruct not only the the currently visible parts of the scene, but also all previously viewed parts, which enables replaying the complete reconstruction across all timesteps.\n  Our method decomposes the scene into a set of rigid 3D primitives, which are assumed to be moving throughout the scene. Using estimated dense 2D correspondences, we jointly infer the rigid motion of these primitives through an optimisation pipeline, yielding a 4D reconstruction of the scene, i.e. providing 3D geometry dynamically moving through time. To achieve this, we also introduce a mechanism to extrapolate motion for objects that become invisible, employing motion-grouping techniques to maintain continuity.\n  The resulting system enables 4D spatio-temporal awareness, offering capabilities such as replayable 3D reconstructions of articulated objects through time, multi-object scanning, and object permanence. On object scanning and multi-object datasets, our system significantly outperforms existing methods both quantitatively and qualitatively.", "link": "http://arxiv.org/abs/2512.16564v1", "date": "2025-12-18", "relevancy": 3.1406, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6391}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6267}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204D%20Primitive-M%C3%A2ch%C3%A9%3A%20Glueing%20Primitives%20for%20Persistent%204D%20Scene%20Reconstruction&body=Title%3A%204D%20Primitive-M%C3%A2ch%C3%A9%3A%20Glueing%20Primitives%20for%20Persistent%204D%20Scene%20Reconstruction%0AAuthor%3A%20Kirill%20Mazur%20and%20Marwan%20Taher%20and%20Andrew%20J.%20Davison%0AAbstract%3A%20We%20present%20a%20dynamic%20reconstruction%20system%20that%20receives%20a%20casual%20monocular%20RGB%20video%20as%20input%2C%20and%20outputs%20a%20complete%20and%20persistent%20reconstruction%20of%20the%20scene.%20In%20other%20words%2C%20we%20reconstruct%20not%20only%20the%20the%20currently%20visible%20parts%20of%20the%20scene%2C%20but%20also%20all%20previously%20viewed%20parts%2C%20which%20enables%20replaying%20the%20complete%20reconstruction%20across%20all%20timesteps.%0A%20%20Our%20method%20decomposes%20the%20scene%20into%20a%20set%20of%20rigid%203D%20primitives%2C%20which%20are%20assumed%20to%20be%20moving%20throughout%20the%20scene.%20Using%20estimated%20dense%202D%20correspondences%2C%20we%20jointly%20infer%20the%20rigid%20motion%20of%20these%20primitives%20through%20an%20optimisation%20pipeline%2C%20yielding%20a%204D%20reconstruction%20of%20the%20scene%2C%20i.e.%20providing%203D%20geometry%20dynamically%20moving%20through%20time.%20To%20achieve%20this%2C%20we%20also%20introduce%20a%20mechanism%20to%20extrapolate%20motion%20for%20objects%20that%20become%20invisible%2C%20employing%20motion-grouping%20techniques%20to%20maintain%20continuity.%0A%20%20The%20resulting%20system%20enables%204D%20spatio-temporal%20awareness%2C%20offering%20capabilities%20such%20as%20replayable%203D%20reconstructions%20of%20articulated%20objects%20through%20time%2C%20multi-object%20scanning%2C%20and%20object%20permanence.%20On%20object%20scanning%20and%20multi-object%20datasets%2C%20our%20system%20significantly%20outperforms%20existing%20methods%20both%20quantitatively%20and%20qualitatively.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4D%2520Primitive-M%25C3%25A2ch%25C3%25A9%253A%2520Glueing%2520Primitives%2520for%2520Persistent%25204D%2520Scene%2520Reconstruction%26entry.906535625%3DKirill%2520Mazur%2520and%2520Marwan%2520Taher%2520and%2520Andrew%2520J.%2520Davison%26entry.1292438233%3DWe%2520present%2520a%2520dynamic%2520reconstruction%2520system%2520that%2520receives%2520a%2520casual%2520monocular%2520RGB%2520video%2520as%2520input%252C%2520and%2520outputs%2520a%2520complete%2520and%2520persistent%2520reconstruction%2520of%2520the%2520scene.%2520In%2520other%2520words%252C%2520we%2520reconstruct%2520not%2520only%2520the%2520the%2520currently%2520visible%2520parts%2520of%2520the%2520scene%252C%2520but%2520also%2520all%2520previously%2520viewed%2520parts%252C%2520which%2520enables%2520replaying%2520the%2520complete%2520reconstruction%2520across%2520all%2520timesteps.%250A%2520%2520Our%2520method%2520decomposes%2520the%2520scene%2520into%2520a%2520set%2520of%2520rigid%25203D%2520primitives%252C%2520which%2520are%2520assumed%2520to%2520be%2520moving%2520throughout%2520the%2520scene.%2520Using%2520estimated%2520dense%25202D%2520correspondences%252C%2520we%2520jointly%2520infer%2520the%2520rigid%2520motion%2520of%2520these%2520primitives%2520through%2520an%2520optimisation%2520pipeline%252C%2520yielding%2520a%25204D%2520reconstruction%2520of%2520the%2520scene%252C%2520i.e.%2520providing%25203D%2520geometry%2520dynamically%2520moving%2520through%2520time.%2520To%2520achieve%2520this%252C%2520we%2520also%2520introduce%2520a%2520mechanism%2520to%2520extrapolate%2520motion%2520for%2520objects%2520that%2520become%2520invisible%252C%2520employing%2520motion-grouping%2520techniques%2520to%2520maintain%2520continuity.%250A%2520%2520The%2520resulting%2520system%2520enables%25204D%2520spatio-temporal%2520awareness%252C%2520offering%2520capabilities%2520such%2520as%2520replayable%25203D%2520reconstructions%2520of%2520articulated%2520objects%2520through%2520time%252C%2520multi-object%2520scanning%252C%2520and%2520object%2520permanence.%2520On%2520object%2520scanning%2520and%2520multi-object%2520datasets%252C%2520our%2520system%2520significantly%2520outperforms%2520existing%2520methods%2520both%2520quantitatively%2520and%2520qualitatively.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4D%20Primitive-M%C3%A2ch%C3%A9%3A%20Glueing%20Primitives%20for%20Persistent%204D%20Scene%20Reconstruction&entry.906535625=Kirill%20Mazur%20and%20Marwan%20Taher%20and%20Andrew%20J.%20Davison&entry.1292438233=We%20present%20a%20dynamic%20reconstruction%20system%20that%20receives%20a%20casual%20monocular%20RGB%20video%20as%20input%2C%20and%20outputs%20a%20complete%20and%20persistent%20reconstruction%20of%20the%20scene.%20In%20other%20words%2C%20we%20reconstruct%20not%20only%20the%20the%20currently%20visible%20parts%20of%20the%20scene%2C%20but%20also%20all%20previously%20viewed%20parts%2C%20which%20enables%20replaying%20the%20complete%20reconstruction%20across%20all%20timesteps.%0A%20%20Our%20method%20decomposes%20the%20scene%20into%20a%20set%20of%20rigid%203D%20primitives%2C%20which%20are%20assumed%20to%20be%20moving%20throughout%20the%20scene.%20Using%20estimated%20dense%202D%20correspondences%2C%20we%20jointly%20infer%20the%20rigid%20motion%20of%20these%20primitives%20through%20an%20optimisation%20pipeline%2C%20yielding%20a%204D%20reconstruction%20of%20the%20scene%2C%20i.e.%20providing%203D%20geometry%20dynamically%20moving%20through%20time.%20To%20achieve%20this%2C%20we%20also%20introduce%20a%20mechanism%20to%20extrapolate%20motion%20for%20objects%20that%20become%20invisible%2C%20employing%20motion-grouping%20techniques%20to%20maintain%20continuity.%0A%20%20The%20resulting%20system%20enables%204D%20spatio-temporal%20awareness%2C%20offering%20capabilities%20such%20as%20replayable%203D%20reconstructions%20of%20articulated%20objects%20through%20time%2C%20multi-object%20scanning%2C%20and%20object%20permanence.%20On%20object%20scanning%20and%20multi-object%20datasets%2C%20our%20system%20significantly%20outperforms%20existing%20methods%20both%20quantitatively%20and%20qualitatively.&entry.1838667208=http%3A//arxiv.org/abs/2512.16564v1&entry.124074799=Read"},
{"title": "Prime and Reach: Synthesising Body Motion for Gaze-Primed Object Reach", "author": "Masashi Hatano and Saptarshi Sinha and Jacob Chalk and Wei-Hong Li and Hideo Saito and Dima Damen", "abstract": "Human motion generation is a challenging task that aims to create realistic motion imitating natural human behaviour. We focus on the well-studied behaviour of priming an object/location for pick up or put down -- that is, the spotting of an object/location from a distance, known as gaze priming, followed by the motion of approaching and reaching the target location. To that end, we curate, for the first time, 23.7K gaze-primed human motion sequences for reaching target object locations from five publicly available datasets, i.e., HD-EPIC, MoGaze, HOT3D, ADT, and GIMO. We pre-train a text-conditioned diffusion-based motion generation model, then fine-tune it conditioned on goal pose or location, on our curated sequences. Importantly, we evaluate the ability of the generated motion to imitate natural human movement through several metrics, including the 'Reach Success' and a newly introduced 'Prime Success' metric. On the largest dataset, HD-EPIC, our model achieves 60% prime success and 89% reach success when conditioned on the goal object location.", "link": "http://arxiv.org/abs/2512.16456v1", "date": "2025-12-18", "relevancy": 3.0279, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6537}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5816}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5814}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prime%20and%20Reach%3A%20Synthesising%20Body%20Motion%20for%20Gaze-Primed%20Object%20Reach&body=Title%3A%20Prime%20and%20Reach%3A%20Synthesising%20Body%20Motion%20for%20Gaze-Primed%20Object%20Reach%0AAuthor%3A%20Masashi%20Hatano%20and%20Saptarshi%20Sinha%20and%20Jacob%20Chalk%20and%20Wei-Hong%20Li%20and%20Hideo%20Saito%20and%20Dima%20Damen%0AAbstract%3A%20Human%20motion%20generation%20is%20a%20challenging%20task%20that%20aims%20to%20create%20realistic%20motion%20imitating%20natural%20human%20behaviour.%20We%20focus%20on%20the%20well-studied%20behaviour%20of%20priming%20an%20object/location%20for%20pick%20up%20or%20put%20down%20--%20that%20is%2C%20the%20spotting%20of%20an%20object/location%20from%20a%20distance%2C%20known%20as%20gaze%20priming%2C%20followed%20by%20the%20motion%20of%20approaching%20and%20reaching%20the%20target%20location.%20To%20that%20end%2C%20we%20curate%2C%20for%20the%20first%20time%2C%2023.7K%20gaze-primed%20human%20motion%20sequences%20for%20reaching%20target%20object%20locations%20from%20five%20publicly%20available%20datasets%2C%20i.e.%2C%20HD-EPIC%2C%20MoGaze%2C%20HOT3D%2C%20ADT%2C%20and%20GIMO.%20We%20pre-train%20a%20text-conditioned%20diffusion-based%20motion%20generation%20model%2C%20then%20fine-tune%20it%20conditioned%20on%20goal%20pose%20or%20location%2C%20on%20our%20curated%20sequences.%20Importantly%2C%20we%20evaluate%20the%20ability%20of%20the%20generated%20motion%20to%20imitate%20natural%20human%20movement%20through%20several%20metrics%2C%20including%20the%20%27Reach%20Success%27%20and%20a%20newly%20introduced%20%27Prime%20Success%27%20metric.%20On%20the%20largest%20dataset%2C%20HD-EPIC%2C%20our%20model%20achieves%2060%25%20prime%20success%20and%2089%25%20reach%20success%20when%20conditioned%20on%20the%20goal%20object%20location.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16456v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrime%2520and%2520Reach%253A%2520Synthesising%2520Body%2520Motion%2520for%2520Gaze-Primed%2520Object%2520Reach%26entry.906535625%3DMasashi%2520Hatano%2520and%2520Saptarshi%2520Sinha%2520and%2520Jacob%2520Chalk%2520and%2520Wei-Hong%2520Li%2520and%2520Hideo%2520Saito%2520and%2520Dima%2520Damen%26entry.1292438233%3DHuman%2520motion%2520generation%2520is%2520a%2520challenging%2520task%2520that%2520aims%2520to%2520create%2520realistic%2520motion%2520imitating%2520natural%2520human%2520behaviour.%2520We%2520focus%2520on%2520the%2520well-studied%2520behaviour%2520of%2520priming%2520an%2520object/location%2520for%2520pick%2520up%2520or%2520put%2520down%2520--%2520that%2520is%252C%2520the%2520spotting%2520of%2520an%2520object/location%2520from%2520a%2520distance%252C%2520known%2520as%2520gaze%2520priming%252C%2520followed%2520by%2520the%2520motion%2520of%2520approaching%2520and%2520reaching%2520the%2520target%2520location.%2520To%2520that%2520end%252C%2520we%2520curate%252C%2520for%2520the%2520first%2520time%252C%252023.7K%2520gaze-primed%2520human%2520motion%2520sequences%2520for%2520reaching%2520target%2520object%2520locations%2520from%2520five%2520publicly%2520available%2520datasets%252C%2520i.e.%252C%2520HD-EPIC%252C%2520MoGaze%252C%2520HOT3D%252C%2520ADT%252C%2520and%2520GIMO.%2520We%2520pre-train%2520a%2520text-conditioned%2520diffusion-based%2520motion%2520generation%2520model%252C%2520then%2520fine-tune%2520it%2520conditioned%2520on%2520goal%2520pose%2520or%2520location%252C%2520on%2520our%2520curated%2520sequences.%2520Importantly%252C%2520we%2520evaluate%2520the%2520ability%2520of%2520the%2520generated%2520motion%2520to%2520imitate%2520natural%2520human%2520movement%2520through%2520several%2520metrics%252C%2520including%2520the%2520%2527Reach%2520Success%2527%2520and%2520a%2520newly%2520introduced%2520%2527Prime%2520Success%2527%2520metric.%2520On%2520the%2520largest%2520dataset%252C%2520HD-EPIC%252C%2520our%2520model%2520achieves%252060%2525%2520prime%2520success%2520and%252089%2525%2520reach%2520success%2520when%2520conditioned%2520on%2520the%2520goal%2520object%2520location.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16456v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prime%20and%20Reach%3A%20Synthesising%20Body%20Motion%20for%20Gaze-Primed%20Object%20Reach&entry.906535625=Masashi%20Hatano%20and%20Saptarshi%20Sinha%20and%20Jacob%20Chalk%20and%20Wei-Hong%20Li%20and%20Hideo%20Saito%20and%20Dima%20Damen&entry.1292438233=Human%20motion%20generation%20is%20a%20challenging%20task%20that%20aims%20to%20create%20realistic%20motion%20imitating%20natural%20human%20behaviour.%20We%20focus%20on%20the%20well-studied%20behaviour%20of%20priming%20an%20object/location%20for%20pick%20up%20or%20put%20down%20--%20that%20is%2C%20the%20spotting%20of%20an%20object/location%20from%20a%20distance%2C%20known%20as%20gaze%20priming%2C%20followed%20by%20the%20motion%20of%20approaching%20and%20reaching%20the%20target%20location.%20To%20that%20end%2C%20we%20curate%2C%20for%20the%20first%20time%2C%2023.7K%20gaze-primed%20human%20motion%20sequences%20for%20reaching%20target%20object%20locations%20from%20five%20publicly%20available%20datasets%2C%20i.e.%2C%20HD-EPIC%2C%20MoGaze%2C%20HOT3D%2C%20ADT%2C%20and%20GIMO.%20We%20pre-train%20a%20text-conditioned%20diffusion-based%20motion%20generation%20model%2C%20then%20fine-tune%20it%20conditioned%20on%20goal%20pose%20or%20location%2C%20on%20our%20curated%20sequences.%20Importantly%2C%20we%20evaluate%20the%20ability%20of%20the%20generated%20motion%20to%20imitate%20natural%20human%20movement%20through%20several%20metrics%2C%20including%20the%20%27Reach%20Success%27%20and%20a%20newly%20introduced%20%27Prime%20Success%27%20metric.%20On%20the%20largest%20dataset%2C%20HD-EPIC%2C%20our%20model%20achieves%2060%25%20prime%20success%20and%2089%25%20reach%20success%20when%20conditioned%20on%20the%20goal%20object%20location.&entry.1838667208=http%3A//arxiv.org/abs/2512.16456v1&entry.124074799=Read"},
{"title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "author": "Yubo Huang and Hailong Guo and Fangtai Wu and Shifeng Zhang and Shijie Huang and Qijun Gan and Lin Liu and Sirui Zhao and Enhong Chen and Jiaming Liu and Steven Hoi", "abstract": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "link": "http://arxiv.org/abs/2512.04677v3", "date": "2025-12-18", "relevancy": 3.0095, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6061}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6021}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Live%20Avatar%3A%20Streaming%20Real-time%20Audio-Driven%20Avatar%20Generation%20with%20Infinite%20Length&body=Title%3A%20Live%20Avatar%3A%20Streaming%20Real-time%20Audio-Driven%20Avatar%20Generation%20with%20Infinite%20Length%0AAuthor%3A%20Yubo%20Huang%20and%20Hailong%20Guo%20and%20Fangtai%20Wu%20and%20Shifeng%20Zhang%20and%20Shijie%20Huang%20and%20Qijun%20Gan%20and%20Lin%20Liu%20and%20Sirui%20Zhao%20and%20Enhong%20Chen%20and%20Jiaming%20Liu%20and%20Steven%20Hoi%0AAbstract%3A%20Existing%20diffusion-based%20video%20generation%20methods%20are%20fundamentally%20constrained%20by%20sequential%20computation%20and%20long-horizon%20inconsistency%2C%20limiting%20their%20practical%20adoption%20in%20real-time%2C%20streaming%20audio-driven%20avatar%20synthesis.%20We%20present%20Live%20Avatar%2C%20an%20algorithm-system%20co-designed%20framework%20that%20enables%20efficient%2C%20high-fidelity%2C%20and%20infinite-length%20avatar%20generation%20using%20a%2014-billion-parameter%20diffusion%20model.%20Our%20approach%20introduces%20Timestep-forcing%20Pipeline%20Parallelism%20%28TPP%29%2C%20a%20distributed%20inference%20paradigm%20that%20pipelines%20denoising%20steps%20across%20multiple%20GPUs%2C%20effectively%20breaking%20the%20autoregressive%20bottleneck%20and%20ensuring%20stable%2C%20low-latency%20real-time%20streaming.%20To%20further%20enhance%20temporal%20consistency%20and%20mitigate%20identity%20drift%20and%20color%20artifacts%2C%20we%20propose%20the%20Rolling%20Sink%20Frame%20Mechanism%20%28RSFM%29%2C%20which%20maintains%20sequence%20fidelity%20by%20dynamically%20recalibrating%20appearance%20using%20a%20cached%20reference%20image.%20Additionally%2C%20we%20leverage%20Self-Forcing%20Distribution%20Matching%20Distillation%20to%20facilitate%20causal%2C%20streamable%20adaptation%20of%20large-scale%20models%20without%20sacrificing%20visual%20quality.%20Live%20Avatar%20demonstrates%20state-of-the-art%20performance%2C%20reaching%2020%20FPS%20end-to-end%20generation%20on%205%20H800%20GPUs%2C%20and%2C%20to%20the%20best%20of%20our%20knowledge%2C%20is%20the%20first%20to%20achieve%20practical%2C%20real-time%2C%20high-fidelity%20avatar%20generation%20at%20this%20scale.%20Our%20work%20establishes%20a%20new%20paradigm%20for%20deploying%20advanced%20diffusion%20models%20in%20industrial%20long-form%20video%20synthesis%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04677v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLive%2520Avatar%253A%2520Streaming%2520Real-time%2520Audio-Driven%2520Avatar%2520Generation%2520with%2520Infinite%2520Length%26entry.906535625%3DYubo%2520Huang%2520and%2520Hailong%2520Guo%2520and%2520Fangtai%2520Wu%2520and%2520Shifeng%2520Zhang%2520and%2520Shijie%2520Huang%2520and%2520Qijun%2520Gan%2520and%2520Lin%2520Liu%2520and%2520Sirui%2520Zhao%2520and%2520Enhong%2520Chen%2520and%2520Jiaming%2520Liu%2520and%2520Steven%2520Hoi%26entry.1292438233%3DExisting%2520diffusion-based%2520video%2520generation%2520methods%2520are%2520fundamentally%2520constrained%2520by%2520sequential%2520computation%2520and%2520long-horizon%2520inconsistency%252C%2520limiting%2520their%2520practical%2520adoption%2520in%2520real-time%252C%2520streaming%2520audio-driven%2520avatar%2520synthesis.%2520We%2520present%2520Live%2520Avatar%252C%2520an%2520algorithm-system%2520co-designed%2520framework%2520that%2520enables%2520efficient%252C%2520high-fidelity%252C%2520and%2520infinite-length%2520avatar%2520generation%2520using%2520a%252014-billion-parameter%2520diffusion%2520model.%2520Our%2520approach%2520introduces%2520Timestep-forcing%2520Pipeline%2520Parallelism%2520%2528TPP%2529%252C%2520a%2520distributed%2520inference%2520paradigm%2520that%2520pipelines%2520denoising%2520steps%2520across%2520multiple%2520GPUs%252C%2520effectively%2520breaking%2520the%2520autoregressive%2520bottleneck%2520and%2520ensuring%2520stable%252C%2520low-latency%2520real-time%2520streaming.%2520To%2520further%2520enhance%2520temporal%2520consistency%2520and%2520mitigate%2520identity%2520drift%2520and%2520color%2520artifacts%252C%2520we%2520propose%2520the%2520Rolling%2520Sink%2520Frame%2520Mechanism%2520%2528RSFM%2529%252C%2520which%2520maintains%2520sequence%2520fidelity%2520by%2520dynamically%2520recalibrating%2520appearance%2520using%2520a%2520cached%2520reference%2520image.%2520Additionally%252C%2520we%2520leverage%2520Self-Forcing%2520Distribution%2520Matching%2520Distillation%2520to%2520facilitate%2520causal%252C%2520streamable%2520adaptation%2520of%2520large-scale%2520models%2520without%2520sacrificing%2520visual%2520quality.%2520Live%2520Avatar%2520demonstrates%2520state-of-the-art%2520performance%252C%2520reaching%252020%2520FPS%2520end-to-end%2520generation%2520on%25205%2520H800%2520GPUs%252C%2520and%252C%2520to%2520the%2520best%2520of%2520our%2520knowledge%252C%2520is%2520the%2520first%2520to%2520achieve%2520practical%252C%2520real-time%252C%2520high-fidelity%2520avatar%2520generation%2520at%2520this%2520scale.%2520Our%2520work%2520establishes%2520a%2520new%2520paradigm%2520for%2520deploying%2520advanced%2520diffusion%2520models%2520in%2520industrial%2520long-form%2520video%2520synthesis%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04677v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Live%20Avatar%3A%20Streaming%20Real-time%20Audio-Driven%20Avatar%20Generation%20with%20Infinite%20Length&entry.906535625=Yubo%20Huang%20and%20Hailong%20Guo%20and%20Fangtai%20Wu%20and%20Shifeng%20Zhang%20and%20Shijie%20Huang%20and%20Qijun%20Gan%20and%20Lin%20Liu%20and%20Sirui%20Zhao%20and%20Enhong%20Chen%20and%20Jiaming%20Liu%20and%20Steven%20Hoi&entry.1292438233=Existing%20diffusion-based%20video%20generation%20methods%20are%20fundamentally%20constrained%20by%20sequential%20computation%20and%20long-horizon%20inconsistency%2C%20limiting%20their%20practical%20adoption%20in%20real-time%2C%20streaming%20audio-driven%20avatar%20synthesis.%20We%20present%20Live%20Avatar%2C%20an%20algorithm-system%20co-designed%20framework%20that%20enables%20efficient%2C%20high-fidelity%2C%20and%20infinite-length%20avatar%20generation%20using%20a%2014-billion-parameter%20diffusion%20model.%20Our%20approach%20introduces%20Timestep-forcing%20Pipeline%20Parallelism%20%28TPP%29%2C%20a%20distributed%20inference%20paradigm%20that%20pipelines%20denoising%20steps%20across%20multiple%20GPUs%2C%20effectively%20breaking%20the%20autoregressive%20bottleneck%20and%20ensuring%20stable%2C%20low-latency%20real-time%20streaming.%20To%20further%20enhance%20temporal%20consistency%20and%20mitigate%20identity%20drift%20and%20color%20artifacts%2C%20we%20propose%20the%20Rolling%20Sink%20Frame%20Mechanism%20%28RSFM%29%2C%20which%20maintains%20sequence%20fidelity%20by%20dynamically%20recalibrating%20appearance%20using%20a%20cached%20reference%20image.%20Additionally%2C%20we%20leverage%20Self-Forcing%20Distribution%20Matching%20Distillation%20to%20facilitate%20causal%2C%20streamable%20adaptation%20of%20large-scale%20models%20without%20sacrificing%20visual%20quality.%20Live%20Avatar%20demonstrates%20state-of-the-art%20performance%2C%20reaching%2020%20FPS%20end-to-end%20generation%20on%205%20H800%20GPUs%2C%20and%2C%20to%20the%20best%20of%20our%20knowledge%2C%20is%20the%20first%20to%20achieve%20practical%2C%20real-time%2C%20high-fidelity%20avatar%20generation%20at%20this%20scale.%20Our%20work%20establishes%20a%20new%20paradigm%20for%20deploying%20advanced%20diffusion%20models%20in%20industrial%20long-form%20video%20synthesis%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2512.04677v3&entry.124074799=Read"},
{"title": "EverybodyDance: Bipartite Graph-Based Identity Correspondence for Multi-Character Animation", "author": "Haotian Ling and Zequn Chen and Qiuying Chen and Donglin Di and Yongjia Ma and Hao Li and Chen Wei and Zhulin Tao and Xun Yang", "abstract": "Consistent pose-driven character animation has achieved remarkable progress in single-character scenarios. However, extending these advances to multi-character settings is non-trivial, especially when position swap is involved. Beyond mere scaling, the core challenge lies in enforcing correct Identity Correspondence (IC) between characters in reference and generated frames. To address this, we introduce EverybodyDance, a systematic solution targeting IC correctness in multi-character animation. EverybodyDance is built around the Identity Matching Graph (IMG), which models characters in the generated and reference frames as two node sets in a weighted complete bipartite graph. Edge weights, computed via our proposed Mask-Query Attention (MQA), quantify the affinity between each pair of characters. Our key insight is to formalize IC correctness as a graph structural metric and to optimize it during training. We also propose a series of targeted strategies tailored for multi-character animation, including identity-embedded guidance, a multi-scale matching strategy, and pre-classified sampling, which work synergistically. Finally, to evaluate IC performance, we curate the Identity Correspondence Evaluation benchmark, dedicated to multi-character IC correctness. Extensive experiments demonstrate that EverybodyDance substantially outperforms state-of-the-art baselines in both IC and visual fidelity.", "link": "http://arxiv.org/abs/2512.16360v1", "date": "2025-12-18", "relevancy": 2.9921, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.651}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.595}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EverybodyDance%3A%20Bipartite%20Graph-Based%20Identity%20Correspondence%20for%20Multi-Character%20Animation&body=Title%3A%20EverybodyDance%3A%20Bipartite%20Graph-Based%20Identity%20Correspondence%20for%20Multi-Character%20Animation%0AAuthor%3A%20Haotian%20Ling%20and%20Zequn%20Chen%20and%20Qiuying%20Chen%20and%20Donglin%20Di%20and%20Yongjia%20Ma%20and%20Hao%20Li%20and%20Chen%20Wei%20and%20Zhulin%20Tao%20and%20Xun%20Yang%0AAbstract%3A%20Consistent%20pose-driven%20character%20animation%20has%20achieved%20remarkable%20progress%20in%20single-character%20scenarios.%20However%2C%20extending%20these%20advances%20to%20multi-character%20settings%20is%20non-trivial%2C%20especially%20when%20position%20swap%20is%20involved.%20Beyond%20mere%20scaling%2C%20the%20core%20challenge%20lies%20in%20enforcing%20correct%20Identity%20Correspondence%20%28IC%29%20between%20characters%20in%20reference%20and%20generated%20frames.%20To%20address%20this%2C%20we%20introduce%20EverybodyDance%2C%20a%20systematic%20solution%20targeting%20IC%20correctness%20in%20multi-character%20animation.%20EverybodyDance%20is%20built%20around%20the%20Identity%20Matching%20Graph%20%28IMG%29%2C%20which%20models%20characters%20in%20the%20generated%20and%20reference%20frames%20as%20two%20node%20sets%20in%20a%20weighted%20complete%20bipartite%20graph.%20Edge%20weights%2C%20computed%20via%20our%20proposed%20Mask-Query%20Attention%20%28MQA%29%2C%20quantify%20the%20affinity%20between%20each%20pair%20of%20characters.%20Our%20key%20insight%20is%20to%20formalize%20IC%20correctness%20as%20a%20graph%20structural%20metric%20and%20to%20optimize%20it%20during%20training.%20We%20also%20propose%20a%20series%20of%20targeted%20strategies%20tailored%20for%20multi-character%20animation%2C%20including%20identity-embedded%20guidance%2C%20a%20multi-scale%20matching%20strategy%2C%20and%20pre-classified%20sampling%2C%20which%20work%20synergistically.%20Finally%2C%20to%20evaluate%20IC%20performance%2C%20we%20curate%20the%20Identity%20Correspondence%20Evaluation%20benchmark%2C%20dedicated%20to%20multi-character%20IC%20correctness.%20Extensive%20experiments%20demonstrate%20that%20EverybodyDance%20substantially%20outperforms%20state-of-the-art%20baselines%20in%20both%20IC%20and%20visual%20fidelity.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16360v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEverybodyDance%253A%2520Bipartite%2520Graph-Based%2520Identity%2520Correspondence%2520for%2520Multi-Character%2520Animation%26entry.906535625%3DHaotian%2520Ling%2520and%2520Zequn%2520Chen%2520and%2520Qiuying%2520Chen%2520and%2520Donglin%2520Di%2520and%2520Yongjia%2520Ma%2520and%2520Hao%2520Li%2520and%2520Chen%2520Wei%2520and%2520Zhulin%2520Tao%2520and%2520Xun%2520Yang%26entry.1292438233%3DConsistent%2520pose-driven%2520character%2520animation%2520has%2520achieved%2520remarkable%2520progress%2520in%2520single-character%2520scenarios.%2520However%252C%2520extending%2520these%2520advances%2520to%2520multi-character%2520settings%2520is%2520non-trivial%252C%2520especially%2520when%2520position%2520swap%2520is%2520involved.%2520Beyond%2520mere%2520scaling%252C%2520the%2520core%2520challenge%2520lies%2520in%2520enforcing%2520correct%2520Identity%2520Correspondence%2520%2528IC%2529%2520between%2520characters%2520in%2520reference%2520and%2520generated%2520frames.%2520To%2520address%2520this%252C%2520we%2520introduce%2520EverybodyDance%252C%2520a%2520systematic%2520solution%2520targeting%2520IC%2520correctness%2520in%2520multi-character%2520animation.%2520EverybodyDance%2520is%2520built%2520around%2520the%2520Identity%2520Matching%2520Graph%2520%2528IMG%2529%252C%2520which%2520models%2520characters%2520in%2520the%2520generated%2520and%2520reference%2520frames%2520as%2520two%2520node%2520sets%2520in%2520a%2520weighted%2520complete%2520bipartite%2520graph.%2520Edge%2520weights%252C%2520computed%2520via%2520our%2520proposed%2520Mask-Query%2520Attention%2520%2528MQA%2529%252C%2520quantify%2520the%2520affinity%2520between%2520each%2520pair%2520of%2520characters.%2520Our%2520key%2520insight%2520is%2520to%2520formalize%2520IC%2520correctness%2520as%2520a%2520graph%2520structural%2520metric%2520and%2520to%2520optimize%2520it%2520during%2520training.%2520We%2520also%2520propose%2520a%2520series%2520of%2520targeted%2520strategies%2520tailored%2520for%2520multi-character%2520animation%252C%2520including%2520identity-embedded%2520guidance%252C%2520a%2520multi-scale%2520matching%2520strategy%252C%2520and%2520pre-classified%2520sampling%252C%2520which%2520work%2520synergistically.%2520Finally%252C%2520to%2520evaluate%2520IC%2520performance%252C%2520we%2520curate%2520the%2520Identity%2520Correspondence%2520Evaluation%2520benchmark%252C%2520dedicated%2520to%2520multi-character%2520IC%2520correctness.%2520Extensive%2520experiments%2520demonstrate%2520that%2520EverybodyDance%2520substantially%2520outperforms%2520state-of-the-art%2520baselines%2520in%2520both%2520IC%2520and%2520visual%2520fidelity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16360v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EverybodyDance%3A%20Bipartite%20Graph-Based%20Identity%20Correspondence%20for%20Multi-Character%20Animation&entry.906535625=Haotian%20Ling%20and%20Zequn%20Chen%20and%20Qiuying%20Chen%20and%20Donglin%20Di%20and%20Yongjia%20Ma%20and%20Hao%20Li%20and%20Chen%20Wei%20and%20Zhulin%20Tao%20and%20Xun%20Yang&entry.1292438233=Consistent%20pose-driven%20character%20animation%20has%20achieved%20remarkable%20progress%20in%20single-character%20scenarios.%20However%2C%20extending%20these%20advances%20to%20multi-character%20settings%20is%20non-trivial%2C%20especially%20when%20position%20swap%20is%20involved.%20Beyond%20mere%20scaling%2C%20the%20core%20challenge%20lies%20in%20enforcing%20correct%20Identity%20Correspondence%20%28IC%29%20between%20characters%20in%20reference%20and%20generated%20frames.%20To%20address%20this%2C%20we%20introduce%20EverybodyDance%2C%20a%20systematic%20solution%20targeting%20IC%20correctness%20in%20multi-character%20animation.%20EverybodyDance%20is%20built%20around%20the%20Identity%20Matching%20Graph%20%28IMG%29%2C%20which%20models%20characters%20in%20the%20generated%20and%20reference%20frames%20as%20two%20node%20sets%20in%20a%20weighted%20complete%20bipartite%20graph.%20Edge%20weights%2C%20computed%20via%20our%20proposed%20Mask-Query%20Attention%20%28MQA%29%2C%20quantify%20the%20affinity%20between%20each%20pair%20of%20characters.%20Our%20key%20insight%20is%20to%20formalize%20IC%20correctness%20as%20a%20graph%20structural%20metric%20and%20to%20optimize%20it%20during%20training.%20We%20also%20propose%20a%20series%20of%20targeted%20strategies%20tailored%20for%20multi-character%20animation%2C%20including%20identity-embedded%20guidance%2C%20a%20multi-scale%20matching%20strategy%2C%20and%20pre-classified%20sampling%2C%20which%20work%20synergistically.%20Finally%2C%20to%20evaluate%20IC%20performance%2C%20we%20curate%20the%20Identity%20Correspondence%20Evaluation%20benchmark%2C%20dedicated%20to%20multi-character%20IC%20correctness.%20Extensive%20experiments%20demonstrate%20that%20EverybodyDance%20substantially%20outperforms%20state-of-the-art%20baselines%20in%20both%20IC%20and%20visual%20fidelity.&entry.1838667208=http%3A//arxiv.org/abs/2512.16360v1&entry.124074799=Read"},
{"title": "Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation", "author": "Xin Lin and Meixi Song and Dizhe Zhang and Wenxuan Lu and Haodong Li and Bo Du and Ming-Hsuan Yang and Truong Nguyen and Lu Qi", "abstract": "In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: \\href{https://insta360-research-team.github.io/DAP_website/} {https://insta360-research-team.github.io/DAP\\_website/}", "link": "http://arxiv.org/abs/2512.16913v1", "date": "2025-12-18", "relevancy": 2.9844, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6019}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6019}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth%20Any%20Panoramas%3A%20A%20Foundation%20Model%20for%20Panoramic%20Depth%20Estimation&body=Title%3A%20Depth%20Any%20Panoramas%3A%20A%20Foundation%20Model%20for%20Panoramic%20Depth%20Estimation%0AAuthor%3A%20Xin%20Lin%20and%20Meixi%20Song%20and%20Dizhe%20Zhang%20and%20Wenxuan%20Lu%20and%20Haodong%20Li%20and%20Bo%20Du%20and%20Ming-Hsuan%20Yang%20and%20Truong%20Nguyen%20and%20Lu%20Qi%0AAbstract%3A%20In%20this%20work%2C%20we%20present%20a%20panoramic%20metric%20depth%20foundation%20model%20that%20generalizes%20across%20diverse%20scene%20distances.%20We%20explore%20a%20data-in-the-loop%20paradigm%20from%20the%20view%20of%20both%20data%20construction%20and%20framework%20design.%20We%20collect%20a%20large-scale%20dataset%20by%20combining%20public%20datasets%2C%20high-quality%20synthetic%20data%20from%20our%20UE5%20simulator%20and%20text-to-image%20models%2C%20and%20real%20panoramic%20images%20from%20the%20web.%20To%20reduce%20domain%20gaps%20between%20indoor/outdoor%20and%20synthetic/real%20data%2C%20we%20introduce%20a%20three-stage%20pseudo-label%20curation%20pipeline%20to%20generate%20reliable%20ground%20truth%20for%20unlabeled%20images.%20For%20the%20model%2C%20we%20adopt%20DINOv3-Large%20as%20the%20backbone%20for%20its%20strong%20pre-trained%20generalization%2C%20and%20introduce%20a%20plug-and-play%20range%20mask%20head%2C%20sharpness-centric%20optimization%2C%20and%20geometry-centric%20optimization%20to%20improve%20robustness%20to%20varying%20distances%20and%20enforce%20geometric%20consistency%20across%20views.%20Experiments%20on%20multiple%20benchmarks%20%28e.g.%2C%20Stanford2D3D%2C%20Matterport3D%2C%20and%20Deep360%29%20demonstrate%20strong%20performance%20and%20zero-shot%20generalization%2C%20with%20particularly%20robust%20and%20stable%20metric%20predictions%20in%20diverse%20real-world%20scenes.%20The%20project%20page%20can%20be%20found%20at%3A%20%5Chref%7Bhttps%3A//insta360-research-team.github.io/DAP_website/%7D%20%7Bhttps%3A//insta360-research-team.github.io/DAP%5C_website/%7D%0ALink%3A%20http%3A//arxiv.org/abs/2512.16913v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth%2520Any%2520Panoramas%253A%2520A%2520Foundation%2520Model%2520for%2520Panoramic%2520Depth%2520Estimation%26entry.906535625%3DXin%2520Lin%2520and%2520Meixi%2520Song%2520and%2520Dizhe%2520Zhang%2520and%2520Wenxuan%2520Lu%2520and%2520Haodong%2520Li%2520and%2520Bo%2520Du%2520and%2520Ming-Hsuan%2520Yang%2520and%2520Truong%2520Nguyen%2520and%2520Lu%2520Qi%26entry.1292438233%3DIn%2520this%2520work%252C%2520we%2520present%2520a%2520panoramic%2520metric%2520depth%2520foundation%2520model%2520that%2520generalizes%2520across%2520diverse%2520scene%2520distances.%2520We%2520explore%2520a%2520data-in-the-loop%2520paradigm%2520from%2520the%2520view%2520of%2520both%2520data%2520construction%2520and%2520framework%2520design.%2520We%2520collect%2520a%2520large-scale%2520dataset%2520by%2520combining%2520public%2520datasets%252C%2520high-quality%2520synthetic%2520data%2520from%2520our%2520UE5%2520simulator%2520and%2520text-to-image%2520models%252C%2520and%2520real%2520panoramic%2520images%2520from%2520the%2520web.%2520To%2520reduce%2520domain%2520gaps%2520between%2520indoor/outdoor%2520and%2520synthetic/real%2520data%252C%2520we%2520introduce%2520a%2520three-stage%2520pseudo-label%2520curation%2520pipeline%2520to%2520generate%2520reliable%2520ground%2520truth%2520for%2520unlabeled%2520images.%2520For%2520the%2520model%252C%2520we%2520adopt%2520DINOv3-Large%2520as%2520the%2520backbone%2520for%2520its%2520strong%2520pre-trained%2520generalization%252C%2520and%2520introduce%2520a%2520plug-and-play%2520range%2520mask%2520head%252C%2520sharpness-centric%2520optimization%252C%2520and%2520geometry-centric%2520optimization%2520to%2520improve%2520robustness%2520to%2520varying%2520distances%2520and%2520enforce%2520geometric%2520consistency%2520across%2520views.%2520Experiments%2520on%2520multiple%2520benchmarks%2520%2528e.g.%252C%2520Stanford2D3D%252C%2520Matterport3D%252C%2520and%2520Deep360%2529%2520demonstrate%2520strong%2520performance%2520and%2520zero-shot%2520generalization%252C%2520with%2520particularly%2520robust%2520and%2520stable%2520metric%2520predictions%2520in%2520diverse%2520real-world%2520scenes.%2520The%2520project%2520page%2520can%2520be%2520found%2520at%253A%2520%255Chref%257Bhttps%253A//insta360-research-team.github.io/DAP_website/%257D%2520%257Bhttps%253A//insta360-research-team.github.io/DAP%255C_website/%257D%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16913v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth%20Any%20Panoramas%3A%20A%20Foundation%20Model%20for%20Panoramic%20Depth%20Estimation&entry.906535625=Xin%20Lin%20and%20Meixi%20Song%20and%20Dizhe%20Zhang%20and%20Wenxuan%20Lu%20and%20Haodong%20Li%20and%20Bo%20Du%20and%20Ming-Hsuan%20Yang%20and%20Truong%20Nguyen%20and%20Lu%20Qi&entry.1292438233=In%20this%20work%2C%20we%20present%20a%20panoramic%20metric%20depth%20foundation%20model%20that%20generalizes%20across%20diverse%20scene%20distances.%20We%20explore%20a%20data-in-the-loop%20paradigm%20from%20the%20view%20of%20both%20data%20construction%20and%20framework%20design.%20We%20collect%20a%20large-scale%20dataset%20by%20combining%20public%20datasets%2C%20high-quality%20synthetic%20data%20from%20our%20UE5%20simulator%20and%20text-to-image%20models%2C%20and%20real%20panoramic%20images%20from%20the%20web.%20To%20reduce%20domain%20gaps%20between%20indoor/outdoor%20and%20synthetic/real%20data%2C%20we%20introduce%20a%20three-stage%20pseudo-label%20curation%20pipeline%20to%20generate%20reliable%20ground%20truth%20for%20unlabeled%20images.%20For%20the%20model%2C%20we%20adopt%20DINOv3-Large%20as%20the%20backbone%20for%20its%20strong%20pre-trained%20generalization%2C%20and%20introduce%20a%20plug-and-play%20range%20mask%20head%2C%20sharpness-centric%20optimization%2C%20and%20geometry-centric%20optimization%20to%20improve%20robustness%20to%20varying%20distances%20and%20enforce%20geometric%20consistency%20across%20views.%20Experiments%20on%20multiple%20benchmarks%20%28e.g.%2C%20Stanford2D3D%2C%20Matterport3D%2C%20and%20Deep360%29%20demonstrate%20strong%20performance%20and%20zero-shot%20generalization%2C%20with%20particularly%20robust%20and%20stable%20metric%20predictions%20in%20diverse%20real-world%20scenes.%20The%20project%20page%20can%20be%20found%20at%3A%20%5Chref%7Bhttps%3A//insta360-research-team.github.io/DAP_website/%7D%20%7Bhttps%3A//insta360-research-team.github.io/DAP%5C_website/%7D&entry.1838667208=http%3A//arxiv.org/abs/2512.16913v1&entry.124074799=Read"},
{"title": "Skeleton-Snippet Contrastive Learning with Multiscale Feature Fusion for Action Localization", "author": "Qiushuo Cheng and Jingjing Liu and Catherine Morgan and Alan Whone and Majid Mirmehdi", "abstract": "The self-supervised pretraining paradigm has achieved great success in learning 3D action representations for skeleton-based action recognition using contrastive learning. However, learning effective representations for skeleton-based temporal action localization remains challenging and underexplored. Unlike video-level {action} recognition, detecting action boundaries requires temporally sensitive features that capture subtle differences between adjacent frames where labels change. To this end, we formulate a snippet discrimination pretext task for self-supervised pretraining, which densely projects skeleton sequences into non-overlapping segments and promotes features that distinguish them across videos via contrastive learning. Additionally, we build on strong backbones of skeleton-based action recognition models by fusing intermediate features with a U-shaped module to enhance feature resolution for frame-level localization. Our approach consistently improves existing skeleton-based contrastive learning methods for action localization on BABEL across diverse subsets and evaluation protocols. We also achieve state-of-the-art transfer learning performance on PKUMMD with pretraining on NTU RGB+D and BABEL.", "link": "http://arxiv.org/abs/2512.16504v1", "date": "2025-12-18", "relevancy": 2.9498, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6299}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5889}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Skeleton-Snippet%20Contrastive%20Learning%20with%20Multiscale%20Feature%20Fusion%20for%20Action%20Localization&body=Title%3A%20Skeleton-Snippet%20Contrastive%20Learning%20with%20Multiscale%20Feature%20Fusion%20for%20Action%20Localization%0AAuthor%3A%20Qiushuo%20Cheng%20and%20Jingjing%20Liu%20and%20Catherine%20Morgan%20and%20Alan%20Whone%20and%20Majid%20Mirmehdi%0AAbstract%3A%20The%20self-supervised%20pretraining%20paradigm%20has%20achieved%20great%20success%20in%20learning%203D%20action%20representations%20for%20skeleton-based%20action%20recognition%20using%20contrastive%20learning.%20However%2C%20learning%20effective%20representations%20for%20skeleton-based%20temporal%20action%20localization%20remains%20challenging%20and%20underexplored.%20Unlike%20video-level%20%7Baction%7D%20recognition%2C%20detecting%20action%20boundaries%20requires%20temporally%20sensitive%20features%20that%20capture%20subtle%20differences%20between%20adjacent%20frames%20where%20labels%20change.%20To%20this%20end%2C%20we%20formulate%20a%20snippet%20discrimination%20pretext%20task%20for%20self-supervised%20pretraining%2C%20which%20densely%20projects%20skeleton%20sequences%20into%20non-overlapping%20segments%20and%20promotes%20features%20that%20distinguish%20them%20across%20videos%20via%20contrastive%20learning.%20Additionally%2C%20we%20build%20on%20strong%20backbones%20of%20skeleton-based%20action%20recognition%20models%20by%20fusing%20intermediate%20features%20with%20a%20U-shaped%20module%20to%20enhance%20feature%20resolution%20for%20frame-level%20localization.%20Our%20approach%20consistently%20improves%20existing%20skeleton-based%20contrastive%20learning%20methods%20for%20action%20localization%20on%20BABEL%20across%20diverse%20subsets%20and%20evaluation%20protocols.%20We%20also%20achieve%20state-of-the-art%20transfer%20learning%20performance%20on%20PKUMMD%20with%20pretraining%20on%20NTU%20RGB%2BD%20and%20BABEL.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16504v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkeleton-Snippet%2520Contrastive%2520Learning%2520with%2520Multiscale%2520Feature%2520Fusion%2520for%2520Action%2520Localization%26entry.906535625%3DQiushuo%2520Cheng%2520and%2520Jingjing%2520Liu%2520and%2520Catherine%2520Morgan%2520and%2520Alan%2520Whone%2520and%2520Majid%2520Mirmehdi%26entry.1292438233%3DThe%2520self-supervised%2520pretraining%2520paradigm%2520has%2520achieved%2520great%2520success%2520in%2520learning%25203D%2520action%2520representations%2520for%2520skeleton-based%2520action%2520recognition%2520using%2520contrastive%2520learning.%2520However%252C%2520learning%2520effective%2520representations%2520for%2520skeleton-based%2520temporal%2520action%2520localization%2520remains%2520challenging%2520and%2520underexplored.%2520Unlike%2520video-level%2520%257Baction%257D%2520recognition%252C%2520detecting%2520action%2520boundaries%2520requires%2520temporally%2520sensitive%2520features%2520that%2520capture%2520subtle%2520differences%2520between%2520adjacent%2520frames%2520where%2520labels%2520change.%2520To%2520this%2520end%252C%2520we%2520formulate%2520a%2520snippet%2520discrimination%2520pretext%2520task%2520for%2520self-supervised%2520pretraining%252C%2520which%2520densely%2520projects%2520skeleton%2520sequences%2520into%2520non-overlapping%2520segments%2520and%2520promotes%2520features%2520that%2520distinguish%2520them%2520across%2520videos%2520via%2520contrastive%2520learning.%2520Additionally%252C%2520we%2520build%2520on%2520strong%2520backbones%2520of%2520skeleton-based%2520action%2520recognition%2520models%2520by%2520fusing%2520intermediate%2520features%2520with%2520a%2520U-shaped%2520module%2520to%2520enhance%2520feature%2520resolution%2520for%2520frame-level%2520localization.%2520Our%2520approach%2520consistently%2520improves%2520existing%2520skeleton-based%2520contrastive%2520learning%2520methods%2520for%2520action%2520localization%2520on%2520BABEL%2520across%2520diverse%2520subsets%2520and%2520evaluation%2520protocols.%2520We%2520also%2520achieve%2520state-of-the-art%2520transfer%2520learning%2520performance%2520on%2520PKUMMD%2520with%2520pretraining%2520on%2520NTU%2520RGB%252BD%2520and%2520BABEL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16504v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Skeleton-Snippet%20Contrastive%20Learning%20with%20Multiscale%20Feature%20Fusion%20for%20Action%20Localization&entry.906535625=Qiushuo%20Cheng%20and%20Jingjing%20Liu%20and%20Catherine%20Morgan%20and%20Alan%20Whone%20and%20Majid%20Mirmehdi&entry.1292438233=The%20self-supervised%20pretraining%20paradigm%20has%20achieved%20great%20success%20in%20learning%203D%20action%20representations%20for%20skeleton-based%20action%20recognition%20using%20contrastive%20learning.%20However%2C%20learning%20effective%20representations%20for%20skeleton-based%20temporal%20action%20localization%20remains%20challenging%20and%20underexplored.%20Unlike%20video-level%20%7Baction%7D%20recognition%2C%20detecting%20action%20boundaries%20requires%20temporally%20sensitive%20features%20that%20capture%20subtle%20differences%20between%20adjacent%20frames%20where%20labels%20change.%20To%20this%20end%2C%20we%20formulate%20a%20snippet%20discrimination%20pretext%20task%20for%20self-supervised%20pretraining%2C%20which%20densely%20projects%20skeleton%20sequences%20into%20non-overlapping%20segments%20and%20promotes%20features%20that%20distinguish%20them%20across%20videos%20via%20contrastive%20learning.%20Additionally%2C%20we%20build%20on%20strong%20backbones%20of%20skeleton-based%20action%20recognition%20models%20by%20fusing%20intermediate%20features%20with%20a%20U-shaped%20module%20to%20enhance%20feature%20resolution%20for%20frame-level%20localization.%20Our%20approach%20consistently%20improves%20existing%20skeleton-based%20contrastive%20learning%20methods%20for%20action%20localization%20on%20BABEL%20across%20diverse%20subsets%20and%20evaluation%20protocols.%20We%20also%20achieve%20state-of-the-art%20transfer%20learning%20performance%20on%20PKUMMD%20with%20pretraining%20on%20NTU%20RGB%2BD%20and%20BABEL.&entry.1838667208=http%3A//arxiv.org/abs/2512.16504v1&entry.124074799=Read"},
{"title": "CitySeeker: How Do VLMS Explore Embodied Urban Navigation With Implicit Human Needs?", "author": "Siqi Wang and Chao Liang and Yunfan Gao and Erxin Yu and Sen Li and Yushi Li and Jing Li and Haofen Wang", "abstract": "Vision-Language Models (VLMs) have made significant progress in explicit instruction-based navigation; however, their ability to interpret implicit human needs (e.g., \"I am thirsty\") in dynamic urban environments remains underexplored. This paper introduces CitySeeker, a novel benchmark designed to assess VLMs' spatial reasoning and decision-making capabilities for exploring embodied urban navigation to address implicit needs. CitySeeker includes 6,440 trajectories across 8 cities, capturing diverse visual characteristics and implicit needs in 7 goal-driven scenarios. Extensive experiments reveal that even top-performing models (e.g., Qwen2.5-VL-32B-Instruct) achieve only 21.1% task completion. We find key bottlenecks in error accumulation in long-horizon reasoning, inadequate spatial cognition, and deficient experiential recall. To further analyze them, we investigate a series of exploratory strategies-Backtracking Mechanisms, Enriching Spatial Cognition, and Memory-Based Retrieval (BCR), inspired by human cognitive mapping's emphasis on iterative observation-reasoning cycles and adaptive path optimization. Our analysis provides actionable insights for developing VLMs with robust spatial intelligence required for tackling \"last-mile\" navigation challenges.", "link": "http://arxiv.org/abs/2512.16755v1", "date": "2025-12-18", "relevancy": 2.9081, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.585}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5799}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CitySeeker%3A%20How%20Do%20VLMS%20Explore%20Embodied%20Urban%20Navigation%20With%20Implicit%20Human%20Needs%3F&body=Title%3A%20CitySeeker%3A%20How%20Do%20VLMS%20Explore%20Embodied%20Urban%20Navigation%20With%20Implicit%20Human%20Needs%3F%0AAuthor%3A%20Siqi%20Wang%20and%20Chao%20Liang%20and%20Yunfan%20Gao%20and%20Erxin%20Yu%20and%20Sen%20Li%20and%20Yushi%20Li%20and%20Jing%20Li%20and%20Haofen%20Wang%0AAbstract%3A%20Vision-Language%20Models%20%28VLMs%29%20have%20made%20significant%20progress%20in%20explicit%20instruction-based%20navigation%3B%20however%2C%20their%20ability%20to%20interpret%20implicit%20human%20needs%20%28e.g.%2C%20%22I%20am%20thirsty%22%29%20in%20dynamic%20urban%20environments%20remains%20underexplored.%20This%20paper%20introduces%20CitySeeker%2C%20a%20novel%20benchmark%20designed%20to%20assess%20VLMs%27%20spatial%20reasoning%20and%20decision-making%20capabilities%20for%20exploring%20embodied%20urban%20navigation%20to%20address%20implicit%20needs.%20CitySeeker%20includes%206%2C440%20trajectories%20across%208%20cities%2C%20capturing%20diverse%20visual%20characteristics%20and%20implicit%20needs%20in%207%20goal-driven%20scenarios.%20Extensive%20experiments%20reveal%20that%20even%20top-performing%20models%20%28e.g.%2C%20Qwen2.5-VL-32B-Instruct%29%20achieve%20only%2021.1%25%20task%20completion.%20We%20find%20key%20bottlenecks%20in%20error%20accumulation%20in%20long-horizon%20reasoning%2C%20inadequate%20spatial%20cognition%2C%20and%20deficient%20experiential%20recall.%20To%20further%20analyze%20them%2C%20we%20investigate%20a%20series%20of%20exploratory%20strategies-Backtracking%20Mechanisms%2C%20Enriching%20Spatial%20Cognition%2C%20and%20Memory-Based%20Retrieval%20%28BCR%29%2C%20inspired%20by%20human%20cognitive%20mapping%27s%20emphasis%20on%20iterative%20observation-reasoning%20cycles%20and%20adaptive%20path%20optimization.%20Our%20analysis%20provides%20actionable%20insights%20for%20developing%20VLMs%20with%20robust%20spatial%20intelligence%20required%20for%20tackling%20%22last-mile%22%20navigation%20challenges.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16755v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCitySeeker%253A%2520How%2520Do%2520VLMS%2520Explore%2520Embodied%2520Urban%2520Navigation%2520With%2520Implicit%2520Human%2520Needs%253F%26entry.906535625%3DSiqi%2520Wang%2520and%2520Chao%2520Liang%2520and%2520Yunfan%2520Gao%2520and%2520Erxin%2520Yu%2520and%2520Sen%2520Li%2520and%2520Yushi%2520Li%2520and%2520Jing%2520Li%2520and%2520Haofen%2520Wang%26entry.1292438233%3DVision-Language%2520Models%2520%2528VLMs%2529%2520have%2520made%2520significant%2520progress%2520in%2520explicit%2520instruction-based%2520navigation%253B%2520however%252C%2520their%2520ability%2520to%2520interpret%2520implicit%2520human%2520needs%2520%2528e.g.%252C%2520%2522I%2520am%2520thirsty%2522%2529%2520in%2520dynamic%2520urban%2520environments%2520remains%2520underexplored.%2520This%2520paper%2520introduces%2520CitySeeker%252C%2520a%2520novel%2520benchmark%2520designed%2520to%2520assess%2520VLMs%2527%2520spatial%2520reasoning%2520and%2520decision-making%2520capabilities%2520for%2520exploring%2520embodied%2520urban%2520navigation%2520to%2520address%2520implicit%2520needs.%2520CitySeeker%2520includes%25206%252C440%2520trajectories%2520across%25208%2520cities%252C%2520capturing%2520diverse%2520visual%2520characteristics%2520and%2520implicit%2520needs%2520in%25207%2520goal-driven%2520scenarios.%2520Extensive%2520experiments%2520reveal%2520that%2520even%2520top-performing%2520models%2520%2528e.g.%252C%2520Qwen2.5-VL-32B-Instruct%2529%2520achieve%2520only%252021.1%2525%2520task%2520completion.%2520We%2520find%2520key%2520bottlenecks%2520in%2520error%2520accumulation%2520in%2520long-horizon%2520reasoning%252C%2520inadequate%2520spatial%2520cognition%252C%2520and%2520deficient%2520experiential%2520recall.%2520To%2520further%2520analyze%2520them%252C%2520we%2520investigate%2520a%2520series%2520of%2520exploratory%2520strategies-Backtracking%2520Mechanisms%252C%2520Enriching%2520Spatial%2520Cognition%252C%2520and%2520Memory-Based%2520Retrieval%2520%2528BCR%2529%252C%2520inspired%2520by%2520human%2520cognitive%2520mapping%2527s%2520emphasis%2520on%2520iterative%2520observation-reasoning%2520cycles%2520and%2520adaptive%2520path%2520optimization.%2520Our%2520analysis%2520provides%2520actionable%2520insights%2520for%2520developing%2520VLMs%2520with%2520robust%2520spatial%2520intelligence%2520required%2520for%2520tackling%2520%2522last-mile%2522%2520navigation%2520challenges.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16755v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CitySeeker%3A%20How%20Do%20VLMS%20Explore%20Embodied%20Urban%20Navigation%20With%20Implicit%20Human%20Needs%3F&entry.906535625=Siqi%20Wang%20and%20Chao%20Liang%20and%20Yunfan%20Gao%20and%20Erxin%20Yu%20and%20Sen%20Li%20and%20Yushi%20Li%20and%20Jing%20Li%20and%20Haofen%20Wang&entry.1292438233=Vision-Language%20Models%20%28VLMs%29%20have%20made%20significant%20progress%20in%20explicit%20instruction-based%20navigation%3B%20however%2C%20their%20ability%20to%20interpret%20implicit%20human%20needs%20%28e.g.%2C%20%22I%20am%20thirsty%22%29%20in%20dynamic%20urban%20environments%20remains%20underexplored.%20This%20paper%20introduces%20CitySeeker%2C%20a%20novel%20benchmark%20designed%20to%20assess%20VLMs%27%20spatial%20reasoning%20and%20decision-making%20capabilities%20for%20exploring%20embodied%20urban%20navigation%20to%20address%20implicit%20needs.%20CitySeeker%20includes%206%2C440%20trajectories%20across%208%20cities%2C%20capturing%20diverse%20visual%20characteristics%20and%20implicit%20needs%20in%207%20goal-driven%20scenarios.%20Extensive%20experiments%20reveal%20that%20even%20top-performing%20models%20%28e.g.%2C%20Qwen2.5-VL-32B-Instruct%29%20achieve%20only%2021.1%25%20task%20completion.%20We%20find%20key%20bottlenecks%20in%20error%20accumulation%20in%20long-horizon%20reasoning%2C%20inadequate%20spatial%20cognition%2C%20and%20deficient%20experiential%20recall.%20To%20further%20analyze%20them%2C%20we%20investigate%20a%20series%20of%20exploratory%20strategies-Backtracking%20Mechanisms%2C%20Enriching%20Spatial%20Cognition%2C%20and%20Memory-Based%20Retrieval%20%28BCR%29%2C%20inspired%20by%20human%20cognitive%20mapping%27s%20emphasis%20on%20iterative%20observation-reasoning%20cycles%20and%20adaptive%20path%20optimization.%20Our%20analysis%20provides%20actionable%20insights%20for%20developing%20VLMs%20with%20robust%20spatial%20intelligence%20required%20for%20tackling%20%22last-mile%22%20navigation%20challenges.&entry.1838667208=http%3A//arxiv.org/abs/2512.16755v1&entry.124074799=Read"},
{"title": "V-Thinker: Interactive Thinking with Images", "author": "Runqi Qiao and Qiuna Tan and Minghan Yang and Guanting Dong and Peiqing Yang and Shiqiang Lang and Enhui Wan and Xiaowan Wang and Yida Xu and Lan Yang and Chong Sun and Chen Li and Jing Lyu and Honggang Zhang", "abstract": "Empowering Large Multimodal Models (LMMs) to deeply integrate image interaction with long-horizon reasoning capabilities remains a long-standing challenge in this field. Recent advances in vision-centric reasoning explore a promising \"Thinking with Images\" paradigm for LMMs, marking a shift from image-assisted reasoning to image-interactive thinking. While this milestone enables models to focus on fine-grained image regions, progress remains constrained by limited visual tool spaces and task-specific workflow designs. To bridge this gap, we present V-Thinker, a general-purpose multimodal reasoning assistant that enables interactive, vision-centric thinking through end-to-end reinforcement learning. V-Thinker comprises two key components: (1) a Data Evolution Flywheel that automatically synthesizes, evolves, and verifies interactive reasoning datasets across three dimensions-diversity, quality, and difficulty; and (2) a Visual Progressive Training Curriculum that first aligns perception via point-level supervision, then integrates interactive reasoning through a two-stage reinforcement learning framework. Furthermore, we introduce VTBench, an expert-verified benchmark targeting vision-centric interactive reasoning tasks. Extensive experiments demonstrate that V-Thinker consistently outperforms strong LMM-based baselines in both general and interactive reasoning scenarios, providing valuable insights for advancing image-interactive reasoning applications.", "link": "http://arxiv.org/abs/2511.04460v2", "date": "2025-12-18", "relevancy": 2.8583, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.578}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.578}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V-Thinker%3A%20Interactive%20Thinking%20with%20Images&body=Title%3A%20V-Thinker%3A%20Interactive%20Thinking%20with%20Images%0AAuthor%3A%20Runqi%20Qiao%20and%20Qiuna%20Tan%20and%20Minghan%20Yang%20and%20Guanting%20Dong%20and%20Peiqing%20Yang%20and%20Shiqiang%20Lang%20and%20Enhui%20Wan%20and%20Xiaowan%20Wang%20and%20Yida%20Xu%20and%20Lan%20Yang%20and%20Chong%20Sun%20and%20Chen%20Li%20and%20Jing%20Lyu%20and%20Honggang%20Zhang%0AAbstract%3A%20Empowering%20Large%20Multimodal%20Models%20%28LMMs%29%20to%20deeply%20integrate%20image%20interaction%20with%20long-horizon%20reasoning%20capabilities%20remains%20a%20long-standing%20challenge%20in%20this%20field.%20Recent%20advances%20in%20vision-centric%20reasoning%20explore%20a%20promising%20%22Thinking%20with%20Images%22%20paradigm%20for%20LMMs%2C%20marking%20a%20shift%20from%20image-assisted%20reasoning%20to%20image-interactive%20thinking.%20While%20this%20milestone%20enables%20models%20to%20focus%20on%20fine-grained%20image%20regions%2C%20progress%20remains%20constrained%20by%20limited%20visual%20tool%20spaces%20and%20task-specific%20workflow%20designs.%20To%20bridge%20this%20gap%2C%20we%20present%20V-Thinker%2C%20a%20general-purpose%20multimodal%20reasoning%20assistant%20that%20enables%20interactive%2C%20vision-centric%20thinking%20through%20end-to-end%20reinforcement%20learning.%20V-Thinker%20comprises%20two%20key%20components%3A%20%281%29%20a%20Data%20Evolution%20Flywheel%20that%20automatically%20synthesizes%2C%20evolves%2C%20and%20verifies%20interactive%20reasoning%20datasets%20across%20three%20dimensions-diversity%2C%20quality%2C%20and%20difficulty%3B%20and%20%282%29%20a%20Visual%20Progressive%20Training%20Curriculum%20that%20first%20aligns%20perception%20via%20point-level%20supervision%2C%20then%20integrates%20interactive%20reasoning%20through%20a%20two-stage%20reinforcement%20learning%20framework.%20Furthermore%2C%20we%20introduce%20VTBench%2C%20an%20expert-verified%20benchmark%20targeting%20vision-centric%20interactive%20reasoning%20tasks.%20Extensive%20experiments%20demonstrate%20that%20V-Thinker%20consistently%20outperforms%20strong%20LMM-based%20baselines%20in%20both%20general%20and%20interactive%20reasoning%20scenarios%2C%20providing%20valuable%20insights%20for%20advancing%20image-interactive%20reasoning%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2511.04460v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV-Thinker%253A%2520Interactive%2520Thinking%2520with%2520Images%26entry.906535625%3DRunqi%2520Qiao%2520and%2520Qiuna%2520Tan%2520and%2520Minghan%2520Yang%2520and%2520Guanting%2520Dong%2520and%2520Peiqing%2520Yang%2520and%2520Shiqiang%2520Lang%2520and%2520Enhui%2520Wan%2520and%2520Xiaowan%2520Wang%2520and%2520Yida%2520Xu%2520and%2520Lan%2520Yang%2520and%2520Chong%2520Sun%2520and%2520Chen%2520Li%2520and%2520Jing%2520Lyu%2520and%2520Honggang%2520Zhang%26entry.1292438233%3DEmpowering%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520to%2520deeply%2520integrate%2520image%2520interaction%2520with%2520long-horizon%2520reasoning%2520capabilities%2520remains%2520a%2520long-standing%2520challenge%2520in%2520this%2520field.%2520Recent%2520advances%2520in%2520vision-centric%2520reasoning%2520explore%2520a%2520promising%2520%2522Thinking%2520with%2520Images%2522%2520paradigm%2520for%2520LMMs%252C%2520marking%2520a%2520shift%2520from%2520image-assisted%2520reasoning%2520to%2520image-interactive%2520thinking.%2520While%2520this%2520milestone%2520enables%2520models%2520to%2520focus%2520on%2520fine-grained%2520image%2520regions%252C%2520progress%2520remains%2520constrained%2520by%2520limited%2520visual%2520tool%2520spaces%2520and%2520task-specific%2520workflow%2520designs.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520present%2520V-Thinker%252C%2520a%2520general-purpose%2520multimodal%2520reasoning%2520assistant%2520that%2520enables%2520interactive%252C%2520vision-centric%2520thinking%2520through%2520end-to-end%2520reinforcement%2520learning.%2520V-Thinker%2520comprises%2520two%2520key%2520components%253A%2520%25281%2529%2520a%2520Data%2520Evolution%2520Flywheel%2520that%2520automatically%2520synthesizes%252C%2520evolves%252C%2520and%2520verifies%2520interactive%2520reasoning%2520datasets%2520across%2520three%2520dimensions-diversity%252C%2520quality%252C%2520and%2520difficulty%253B%2520and%2520%25282%2529%2520a%2520Visual%2520Progressive%2520Training%2520Curriculum%2520that%2520first%2520aligns%2520perception%2520via%2520point-level%2520supervision%252C%2520then%2520integrates%2520interactive%2520reasoning%2520through%2520a%2520two-stage%2520reinforcement%2520learning%2520framework.%2520Furthermore%252C%2520we%2520introduce%2520VTBench%252C%2520an%2520expert-verified%2520benchmark%2520targeting%2520vision-centric%2520interactive%2520reasoning%2520tasks.%2520Extensive%2520experiments%2520demonstrate%2520that%2520V-Thinker%2520consistently%2520outperforms%2520strong%2520LMM-based%2520baselines%2520in%2520both%2520general%2520and%2520interactive%2520reasoning%2520scenarios%252C%2520providing%2520valuable%2520insights%2520for%2520advancing%2520image-interactive%2520reasoning%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.04460v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V-Thinker%3A%20Interactive%20Thinking%20with%20Images&entry.906535625=Runqi%20Qiao%20and%20Qiuna%20Tan%20and%20Minghan%20Yang%20and%20Guanting%20Dong%20and%20Peiqing%20Yang%20and%20Shiqiang%20Lang%20and%20Enhui%20Wan%20and%20Xiaowan%20Wang%20and%20Yida%20Xu%20and%20Lan%20Yang%20and%20Chong%20Sun%20and%20Chen%20Li%20and%20Jing%20Lyu%20and%20Honggang%20Zhang&entry.1292438233=Empowering%20Large%20Multimodal%20Models%20%28LMMs%29%20to%20deeply%20integrate%20image%20interaction%20with%20long-horizon%20reasoning%20capabilities%20remains%20a%20long-standing%20challenge%20in%20this%20field.%20Recent%20advances%20in%20vision-centric%20reasoning%20explore%20a%20promising%20%22Thinking%20with%20Images%22%20paradigm%20for%20LMMs%2C%20marking%20a%20shift%20from%20image-assisted%20reasoning%20to%20image-interactive%20thinking.%20While%20this%20milestone%20enables%20models%20to%20focus%20on%20fine-grained%20image%20regions%2C%20progress%20remains%20constrained%20by%20limited%20visual%20tool%20spaces%20and%20task-specific%20workflow%20designs.%20To%20bridge%20this%20gap%2C%20we%20present%20V-Thinker%2C%20a%20general-purpose%20multimodal%20reasoning%20assistant%20that%20enables%20interactive%2C%20vision-centric%20thinking%20through%20end-to-end%20reinforcement%20learning.%20V-Thinker%20comprises%20two%20key%20components%3A%20%281%29%20a%20Data%20Evolution%20Flywheel%20that%20automatically%20synthesizes%2C%20evolves%2C%20and%20verifies%20interactive%20reasoning%20datasets%20across%20three%20dimensions-diversity%2C%20quality%2C%20and%20difficulty%3B%20and%20%282%29%20a%20Visual%20Progressive%20Training%20Curriculum%20that%20first%20aligns%20perception%20via%20point-level%20supervision%2C%20then%20integrates%20interactive%20reasoning%20through%20a%20two-stage%20reinforcement%20learning%20framework.%20Furthermore%2C%20we%20introduce%20VTBench%2C%20an%20expert-verified%20benchmark%20targeting%20vision-centric%20interactive%20reasoning%20tasks.%20Extensive%20experiments%20demonstrate%20that%20V-Thinker%20consistently%20outperforms%20strong%20LMM-based%20baselines%20in%20both%20general%20and%20interactive%20reasoning%20scenarios%2C%20providing%20valuable%20insights%20for%20advancing%20image-interactive%20reasoning%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2511.04460v2&entry.124074799=Read"},
{"title": "BrepLLM: Native Boundary Representation Understanding with Large Language Models", "author": "Liyuan Deng and Hao Guo and Yunpeng Bai and Yongkang Dai and Huaxi Huang and Yilei Shi", "abstract": "Current token-sequence-based Large Language Models (LLMs) are not well-suited for directly processing 3D Boundary Representation (Brep) models that contain complex geometric and topological information. We propose BrepLLM, the first framework that enables LLMs to parse and reason over raw Brep data, bridging the modality gap between structured 3D geometry and natural language. BrepLLM employs a two-stage training pipeline: Cross-modal Alignment Pre-training and Multi-stage LLM Fine-tuning. In the first stage, an adaptive UV sampling strategy converts Breps into graphs representation with geometric and topological information. We then design a hierarchical BrepEncoder to extract features from geometry (i.e., faces and edges) and topology, producing both a single global token and a sequence of node tokens. Then we align the global token with text embeddings from a frozen CLIP text encoder (ViT-L/14) via contrastive learning. In the second stage, we integrate the pretrained BrepEncoder into an LLM. We then align its sequence of node tokens using a three-stage progressive training strategy: (1) training an MLP-based semantic mapping from Brep representation to 2D with 2D-LLM priors. (2) performing fine-tuning of the LLM. (3) designing a Mixture-of-Query Experts (MQE) to enhance geometric diversity modeling. We also construct Brep2Text, a dataset comprising 269,444 Brep-text question-answer pairs. Experiments show that BrepLLM achieves state-of-the-art (SOTA) results on 3D object classification and captioning tasks.", "link": "http://arxiv.org/abs/2512.16413v1", "date": "2025-12-18", "relevancy": 2.8089, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5715}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5569}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BrepLLM%3A%20Native%20Boundary%20Representation%20Understanding%20with%20Large%20Language%20Models&body=Title%3A%20BrepLLM%3A%20Native%20Boundary%20Representation%20Understanding%20with%20Large%20Language%20Models%0AAuthor%3A%20Liyuan%20Deng%20and%20Hao%20Guo%20and%20Yunpeng%20Bai%20and%20Yongkang%20Dai%20and%20Huaxi%20Huang%20and%20Yilei%20Shi%0AAbstract%3A%20Current%20token-sequence-based%20Large%20Language%20Models%20%28LLMs%29%20are%20not%20well-suited%20for%20directly%20processing%203D%20Boundary%20Representation%20%28Brep%29%20models%20that%20contain%20complex%20geometric%20and%20topological%20information.%20We%20propose%20BrepLLM%2C%20the%20first%20framework%20that%20enables%20LLMs%20to%20parse%20and%20reason%20over%20raw%20Brep%20data%2C%20bridging%20the%20modality%20gap%20between%20structured%203D%20geometry%20and%20natural%20language.%20BrepLLM%20employs%20a%20two-stage%20training%20pipeline%3A%20Cross-modal%20Alignment%20Pre-training%20and%20Multi-stage%20LLM%20Fine-tuning.%20In%20the%20first%20stage%2C%20an%20adaptive%20UV%20sampling%20strategy%20converts%20Breps%20into%20graphs%20representation%20with%20geometric%20and%20topological%20information.%20We%20then%20design%20a%20hierarchical%20BrepEncoder%20to%20extract%20features%20from%20geometry%20%28i.e.%2C%20faces%20and%20edges%29%20and%20topology%2C%20producing%20both%20a%20single%20global%20token%20and%20a%20sequence%20of%20node%20tokens.%20Then%20we%20align%20the%20global%20token%20with%20text%20embeddings%20from%20a%20frozen%20CLIP%20text%20encoder%20%28ViT-L/14%29%20via%20contrastive%20learning.%20In%20the%20second%20stage%2C%20we%20integrate%20the%20pretrained%20BrepEncoder%20into%20an%20LLM.%20We%20then%20align%20its%20sequence%20of%20node%20tokens%20using%20a%20three-stage%20progressive%20training%20strategy%3A%20%281%29%20training%20an%20MLP-based%20semantic%20mapping%20from%20Brep%20representation%20to%202D%20with%202D-LLM%20priors.%20%282%29%20performing%20fine-tuning%20of%20the%20LLM.%20%283%29%20designing%20a%20Mixture-of-Query%20Experts%20%28MQE%29%20to%20enhance%20geometric%20diversity%20modeling.%20We%20also%20construct%20Brep2Text%2C%20a%20dataset%20comprising%20269%2C444%20Brep-text%20question-answer%20pairs.%20Experiments%20show%20that%20BrepLLM%20achieves%20state-of-the-art%20%28SOTA%29%20results%20on%203D%20object%20classification%20and%20captioning%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16413v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrepLLM%253A%2520Native%2520Boundary%2520Representation%2520Understanding%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DLiyuan%2520Deng%2520and%2520Hao%2520Guo%2520and%2520Yunpeng%2520Bai%2520and%2520Yongkang%2520Dai%2520and%2520Huaxi%2520Huang%2520and%2520Yilei%2520Shi%26entry.1292438233%3DCurrent%2520token-sequence-based%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520not%2520well-suited%2520for%2520directly%2520processing%25203D%2520Boundary%2520Representation%2520%2528Brep%2529%2520models%2520that%2520contain%2520complex%2520geometric%2520and%2520topological%2520information.%2520We%2520propose%2520BrepLLM%252C%2520the%2520first%2520framework%2520that%2520enables%2520LLMs%2520to%2520parse%2520and%2520reason%2520over%2520raw%2520Brep%2520data%252C%2520bridging%2520the%2520modality%2520gap%2520between%2520structured%25203D%2520geometry%2520and%2520natural%2520language.%2520BrepLLM%2520employs%2520a%2520two-stage%2520training%2520pipeline%253A%2520Cross-modal%2520Alignment%2520Pre-training%2520and%2520Multi-stage%2520LLM%2520Fine-tuning.%2520In%2520the%2520first%2520stage%252C%2520an%2520adaptive%2520UV%2520sampling%2520strategy%2520converts%2520Breps%2520into%2520graphs%2520representation%2520with%2520geometric%2520and%2520topological%2520information.%2520We%2520then%2520design%2520a%2520hierarchical%2520BrepEncoder%2520to%2520extract%2520features%2520from%2520geometry%2520%2528i.e.%252C%2520faces%2520and%2520edges%2529%2520and%2520topology%252C%2520producing%2520both%2520a%2520single%2520global%2520token%2520and%2520a%2520sequence%2520of%2520node%2520tokens.%2520Then%2520we%2520align%2520the%2520global%2520token%2520with%2520text%2520embeddings%2520from%2520a%2520frozen%2520CLIP%2520text%2520encoder%2520%2528ViT-L/14%2529%2520via%2520contrastive%2520learning.%2520In%2520the%2520second%2520stage%252C%2520we%2520integrate%2520the%2520pretrained%2520BrepEncoder%2520into%2520an%2520LLM.%2520We%2520then%2520align%2520its%2520sequence%2520of%2520node%2520tokens%2520using%2520a%2520three-stage%2520progressive%2520training%2520strategy%253A%2520%25281%2529%2520training%2520an%2520MLP-based%2520semantic%2520mapping%2520from%2520Brep%2520representation%2520to%25202D%2520with%25202D-LLM%2520priors.%2520%25282%2529%2520performing%2520fine-tuning%2520of%2520the%2520LLM.%2520%25283%2529%2520designing%2520a%2520Mixture-of-Query%2520Experts%2520%2528MQE%2529%2520to%2520enhance%2520geometric%2520diversity%2520modeling.%2520We%2520also%2520construct%2520Brep2Text%252C%2520a%2520dataset%2520comprising%2520269%252C444%2520Brep-text%2520question-answer%2520pairs.%2520Experiments%2520show%2520that%2520BrepLLM%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520results%2520on%25203D%2520object%2520classification%2520and%2520captioning%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16413v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BrepLLM%3A%20Native%20Boundary%20Representation%20Understanding%20with%20Large%20Language%20Models&entry.906535625=Liyuan%20Deng%20and%20Hao%20Guo%20and%20Yunpeng%20Bai%20and%20Yongkang%20Dai%20and%20Huaxi%20Huang%20and%20Yilei%20Shi&entry.1292438233=Current%20token-sequence-based%20Large%20Language%20Models%20%28LLMs%29%20are%20not%20well-suited%20for%20directly%20processing%203D%20Boundary%20Representation%20%28Brep%29%20models%20that%20contain%20complex%20geometric%20and%20topological%20information.%20We%20propose%20BrepLLM%2C%20the%20first%20framework%20that%20enables%20LLMs%20to%20parse%20and%20reason%20over%20raw%20Brep%20data%2C%20bridging%20the%20modality%20gap%20between%20structured%203D%20geometry%20and%20natural%20language.%20BrepLLM%20employs%20a%20two-stage%20training%20pipeline%3A%20Cross-modal%20Alignment%20Pre-training%20and%20Multi-stage%20LLM%20Fine-tuning.%20In%20the%20first%20stage%2C%20an%20adaptive%20UV%20sampling%20strategy%20converts%20Breps%20into%20graphs%20representation%20with%20geometric%20and%20topological%20information.%20We%20then%20design%20a%20hierarchical%20BrepEncoder%20to%20extract%20features%20from%20geometry%20%28i.e.%2C%20faces%20and%20edges%29%20and%20topology%2C%20producing%20both%20a%20single%20global%20token%20and%20a%20sequence%20of%20node%20tokens.%20Then%20we%20align%20the%20global%20token%20with%20text%20embeddings%20from%20a%20frozen%20CLIP%20text%20encoder%20%28ViT-L/14%29%20via%20contrastive%20learning.%20In%20the%20second%20stage%2C%20we%20integrate%20the%20pretrained%20BrepEncoder%20into%20an%20LLM.%20We%20then%20align%20its%20sequence%20of%20node%20tokens%20using%20a%20three-stage%20progressive%20training%20strategy%3A%20%281%29%20training%20an%20MLP-based%20semantic%20mapping%20from%20Brep%20representation%20to%202D%20with%202D-LLM%20priors.%20%282%29%20performing%20fine-tuning%20of%20the%20LLM.%20%283%29%20designing%20a%20Mixture-of-Query%20Experts%20%28MQE%29%20to%20enhance%20geometric%20diversity%20modeling.%20We%20also%20construct%20Brep2Text%2C%20a%20dataset%20comprising%20269%2C444%20Brep-text%20question-answer%20pairs.%20Experiments%20show%20that%20BrepLLM%20achieves%20state-of-the-art%20%28SOTA%29%20results%20on%203D%20object%20classification%20and%20captioning%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2512.16413v1&entry.124074799=Read"},
{"title": "Next-Embedding Prediction Makes Strong Vision Learners", "author": "Sihan Xu and Ziqiao Ma and Wenhao Chai and Xuweiyi Chen and Weiyang Jin and Joyce Chai and Saining Xie and Stella X. Yu", "abstract": "Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.", "link": "http://arxiv.org/abs/2512.16922v1", "date": "2025-12-18", "relevancy": 2.8067, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5656}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5628}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Next-Embedding%20Prediction%20Makes%20Strong%20Vision%20Learners&body=Title%3A%20Next-Embedding%20Prediction%20Makes%20Strong%20Vision%20Learners%0AAuthor%3A%20Sihan%20Xu%20and%20Ziqiao%20Ma%20and%20Wenhao%20Chai%20and%20Xuweiyi%20Chen%20and%20Weiyang%20Jin%20and%20Joyce%20Chai%20and%20Saining%20Xie%20and%20Stella%20X.%20Yu%0AAbstract%3A%20Inspired%20by%20the%20success%20of%20generative%20pretraining%20in%20natural%20language%2C%20we%20ask%20whether%20the%20same%20principles%20can%20yield%20strong%20self-supervised%20visual%20learners.%20Instead%20of%20training%20models%20to%20output%20features%20for%20downstream%20use%2C%20we%20train%20them%20to%20generate%20embeddings%20to%20perform%20predictive%20tasks%20directly.%20This%20work%20explores%20such%20a%20shift%20from%20learning%20representations%20to%20learning%20models.%20Specifically%2C%20models%20learn%20to%20predict%20future%20patch%20embeddings%20conditioned%20on%20past%20ones%2C%20using%20causal%20masking%20and%20stop%20gradient%2C%20which%20we%20refer%20to%20as%20Next-Embedding%20Predictive%20Autoregression%20%28NEPA%29.%20We%20demonstrate%20that%20a%20simple%20Transformer%20pretrained%20on%20ImageNet-1k%20with%20next%20embedding%20prediction%20as%20its%20sole%20learning%20objective%20is%20effective%20-%20no%20pixel%20reconstruction%2C%20discrete%20tokens%2C%20contrastive%20loss%2C%20or%20task-specific%20heads.%20This%20formulation%20retains%20architectural%20simplicity%20and%20scalability%2C%20without%20requiring%20additional%20design%20complexity.%20NEPA%20achieves%20strong%20results%20across%20tasks%2C%20attaining%2083.8%25%20and%2085.3%25%20top-1%20accuracy%20on%20ImageNet-1K%20with%20ViT-B%20and%20ViT-L%20backbones%20after%20fine-tuning%2C%20and%20transferring%20effectively%20to%20semantic%20segmentation%20on%20ADE20K.%20We%20believe%20generative%20pretraining%20from%20embeddings%20provides%20a%20simple%2C%20scalable%2C%20and%20potentially%20modality-agnostic%20alternative%20to%20visual%20self-supervised%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16922v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNext-Embedding%2520Prediction%2520Makes%2520Strong%2520Vision%2520Learners%26entry.906535625%3DSihan%2520Xu%2520and%2520Ziqiao%2520Ma%2520and%2520Wenhao%2520Chai%2520and%2520Xuweiyi%2520Chen%2520and%2520Weiyang%2520Jin%2520and%2520Joyce%2520Chai%2520and%2520Saining%2520Xie%2520and%2520Stella%2520X.%2520Yu%26entry.1292438233%3DInspired%2520by%2520the%2520success%2520of%2520generative%2520pretraining%2520in%2520natural%2520language%252C%2520we%2520ask%2520whether%2520the%2520same%2520principles%2520can%2520yield%2520strong%2520self-supervised%2520visual%2520learners.%2520Instead%2520of%2520training%2520models%2520to%2520output%2520features%2520for%2520downstream%2520use%252C%2520we%2520train%2520them%2520to%2520generate%2520embeddings%2520to%2520perform%2520predictive%2520tasks%2520directly.%2520This%2520work%2520explores%2520such%2520a%2520shift%2520from%2520learning%2520representations%2520to%2520learning%2520models.%2520Specifically%252C%2520models%2520learn%2520to%2520predict%2520future%2520patch%2520embeddings%2520conditioned%2520on%2520past%2520ones%252C%2520using%2520causal%2520masking%2520and%2520stop%2520gradient%252C%2520which%2520we%2520refer%2520to%2520as%2520Next-Embedding%2520Predictive%2520Autoregression%2520%2528NEPA%2529.%2520We%2520demonstrate%2520that%2520a%2520simple%2520Transformer%2520pretrained%2520on%2520ImageNet-1k%2520with%2520next%2520embedding%2520prediction%2520as%2520its%2520sole%2520learning%2520objective%2520is%2520effective%2520-%2520no%2520pixel%2520reconstruction%252C%2520discrete%2520tokens%252C%2520contrastive%2520loss%252C%2520or%2520task-specific%2520heads.%2520This%2520formulation%2520retains%2520architectural%2520simplicity%2520and%2520scalability%252C%2520without%2520requiring%2520additional%2520design%2520complexity.%2520NEPA%2520achieves%2520strong%2520results%2520across%2520tasks%252C%2520attaining%252083.8%2525%2520and%252085.3%2525%2520top-1%2520accuracy%2520on%2520ImageNet-1K%2520with%2520ViT-B%2520and%2520ViT-L%2520backbones%2520after%2520fine-tuning%252C%2520and%2520transferring%2520effectively%2520to%2520semantic%2520segmentation%2520on%2520ADE20K.%2520We%2520believe%2520generative%2520pretraining%2520from%2520embeddings%2520provides%2520a%2520simple%252C%2520scalable%252C%2520and%2520potentially%2520modality-agnostic%2520alternative%2520to%2520visual%2520self-supervised%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16922v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Next-Embedding%20Prediction%20Makes%20Strong%20Vision%20Learners&entry.906535625=Sihan%20Xu%20and%20Ziqiao%20Ma%20and%20Wenhao%20Chai%20and%20Xuweiyi%20Chen%20and%20Weiyang%20Jin%20and%20Joyce%20Chai%20and%20Saining%20Xie%20and%20Stella%20X.%20Yu&entry.1292438233=Inspired%20by%20the%20success%20of%20generative%20pretraining%20in%20natural%20language%2C%20we%20ask%20whether%20the%20same%20principles%20can%20yield%20strong%20self-supervised%20visual%20learners.%20Instead%20of%20training%20models%20to%20output%20features%20for%20downstream%20use%2C%20we%20train%20them%20to%20generate%20embeddings%20to%20perform%20predictive%20tasks%20directly.%20This%20work%20explores%20such%20a%20shift%20from%20learning%20representations%20to%20learning%20models.%20Specifically%2C%20models%20learn%20to%20predict%20future%20patch%20embeddings%20conditioned%20on%20past%20ones%2C%20using%20causal%20masking%20and%20stop%20gradient%2C%20which%20we%20refer%20to%20as%20Next-Embedding%20Predictive%20Autoregression%20%28NEPA%29.%20We%20demonstrate%20that%20a%20simple%20Transformer%20pretrained%20on%20ImageNet-1k%20with%20next%20embedding%20prediction%20as%20its%20sole%20learning%20objective%20is%20effective%20-%20no%20pixel%20reconstruction%2C%20discrete%20tokens%2C%20contrastive%20loss%2C%20or%20task-specific%20heads.%20This%20formulation%20retains%20architectural%20simplicity%20and%20scalability%2C%20without%20requiring%20additional%20design%20complexity.%20NEPA%20achieves%20strong%20results%20across%20tasks%2C%20attaining%2083.8%25%20and%2085.3%25%20top-1%20accuracy%20on%20ImageNet-1K%20with%20ViT-B%20and%20ViT-L%20backbones%20after%20fine-tuning%2C%20and%20transferring%20effectively%20to%20semantic%20segmentation%20on%20ADE20K.%20We%20believe%20generative%20pretraining%20from%20embeddings%20provides%20a%20simple%2C%20scalable%2C%20and%20potentially%20modality-agnostic%20alternative%20to%20visual%20self-supervised%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2512.16922v1&entry.124074799=Read"},
{"title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos", "author": "Chaoyang Wang and Kaituo Feng and Dongyang Chen and Zhongyu Wang and Zhixun Li and Sicheng Gao and Meng Meng and Xu Zhou and Manyuan Zhang and Yuzhang Shang and Xiangyu Yue", "abstract": "Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.", "link": "http://arxiv.org/abs/2512.16918v1", "date": "2025-12-18", "relevancy": 2.7953, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5858}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5457}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaTooler-V%3A%20Adaptive%20Tool-Use%20for%20Images%20and%20Videos&body=Title%3A%20AdaTooler-V%3A%20Adaptive%20Tool-Use%20for%20Images%20and%20Videos%0AAuthor%3A%20Chaoyang%20Wang%20and%20Kaituo%20Feng%20and%20Dongyang%20Chen%20and%20Zhongyu%20Wang%20and%20Zhixun%20Li%20and%20Sicheng%20Gao%20and%20Meng%20Meng%20and%20Xu%20Zhou%20and%20Manyuan%20Zhang%20and%20Yuzhang%20Shang%20and%20Xiangyu%20Yue%0AAbstract%3A%20Recent%20advances%20have%20shown%20that%20multimodal%20large%20language%20models%20%28MLLMs%29%20benefit%20from%20multimodal%20interleaved%20chain-of-thought%20%28CoT%29%20with%20vision%20tool%20interactions.%20However%2C%20existing%20open-source%20models%20often%20exhibit%20blind%20tool-use%20reasoning%20patterns%2C%20invoking%20vision%20tools%20even%20when%20they%20are%20unnecessary%2C%20which%20significantly%20increases%20inference%20overhead%20and%20degrades%20model%20performance.%20To%20this%20end%2C%20we%20propose%20AdaTooler-V%2C%20an%20MLLM%20that%20performs%20adaptive%20tool-use%20by%20determining%20whether%20a%20visual%20problem%20truly%20requires%20tools.%20First%2C%20we%20introduce%20AT-GRPO%2C%20a%20reinforcement%20learning%20algorithm%20that%20adaptively%20adjusts%20reward%20scales%20based%20on%20the%20Tool%20Benefit%20Score%20of%20each%20sample%2C%20encouraging%20the%20model%20to%20invoke%20tools%20only%20when%20they%20provide%20genuine%20improvements.%20Moreover%2C%20we%20construct%20two%20datasets%20to%20support%20training%3A%20AdaTooler-V-CoT-100k%20for%20SFT%20cold%20start%20and%20AdaTooler-V-300k%20for%20RL%20with%20verifiable%20rewards%20across%20single-image%2C%20multi-image%2C%20and%20video%20data.%20Experiments%20across%20twelve%20benchmarks%20demonstrate%20the%20strong%20reasoning%20capability%20of%20AdaTooler-V%2C%20outperforming%20existing%20methods%20in%20diverse%20visual%20reasoning%20tasks.%20Notably%2C%20AdaTooler-V-7B%20achieves%20an%20accuracy%20of%2089.8%5C%25%20on%20the%20high-resolution%20benchmark%20V%2A%2C%20surpassing%20the%20commercial%20proprietary%20model%20GPT-4o%20and%20Gemini%201.5%20Pro.%20All%20code%2C%20models%2C%20and%20data%20are%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16918v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaTooler-V%253A%2520Adaptive%2520Tool-Use%2520for%2520Images%2520and%2520Videos%26entry.906535625%3DChaoyang%2520Wang%2520and%2520Kaituo%2520Feng%2520and%2520Dongyang%2520Chen%2520and%2520Zhongyu%2520Wang%2520and%2520Zhixun%2520Li%2520and%2520Sicheng%2520Gao%2520and%2520Meng%2520Meng%2520and%2520Xu%2520Zhou%2520and%2520Manyuan%2520Zhang%2520and%2520Yuzhang%2520Shang%2520and%2520Xiangyu%2520Yue%26entry.1292438233%3DRecent%2520advances%2520have%2520shown%2520that%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520benefit%2520from%2520multimodal%2520interleaved%2520chain-of-thought%2520%2528CoT%2529%2520with%2520vision%2520tool%2520interactions.%2520However%252C%2520existing%2520open-source%2520models%2520often%2520exhibit%2520blind%2520tool-use%2520reasoning%2520patterns%252C%2520invoking%2520vision%2520tools%2520even%2520when%2520they%2520are%2520unnecessary%252C%2520which%2520significantly%2520increases%2520inference%2520overhead%2520and%2520degrades%2520model%2520performance.%2520To%2520this%2520end%252C%2520we%2520propose%2520AdaTooler-V%252C%2520an%2520MLLM%2520that%2520performs%2520adaptive%2520tool-use%2520by%2520determining%2520whether%2520a%2520visual%2520problem%2520truly%2520requires%2520tools.%2520First%252C%2520we%2520introduce%2520AT-GRPO%252C%2520a%2520reinforcement%2520learning%2520algorithm%2520that%2520adaptively%2520adjusts%2520reward%2520scales%2520based%2520on%2520the%2520Tool%2520Benefit%2520Score%2520of%2520each%2520sample%252C%2520encouraging%2520the%2520model%2520to%2520invoke%2520tools%2520only%2520when%2520they%2520provide%2520genuine%2520improvements.%2520Moreover%252C%2520we%2520construct%2520two%2520datasets%2520to%2520support%2520training%253A%2520AdaTooler-V-CoT-100k%2520for%2520SFT%2520cold%2520start%2520and%2520AdaTooler-V-300k%2520for%2520RL%2520with%2520verifiable%2520rewards%2520across%2520single-image%252C%2520multi-image%252C%2520and%2520video%2520data.%2520Experiments%2520across%2520twelve%2520benchmarks%2520demonstrate%2520the%2520strong%2520reasoning%2520capability%2520of%2520AdaTooler-V%252C%2520outperforming%2520existing%2520methods%2520in%2520diverse%2520visual%2520reasoning%2520tasks.%2520Notably%252C%2520AdaTooler-V-7B%2520achieves%2520an%2520accuracy%2520of%252089.8%255C%2525%2520on%2520the%2520high-resolution%2520benchmark%2520V%252A%252C%2520surpassing%2520the%2520commercial%2520proprietary%2520model%2520GPT-4o%2520and%2520Gemini%25201.5%2520Pro.%2520All%2520code%252C%2520models%252C%2520and%2520data%2520are%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16918v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaTooler-V%3A%20Adaptive%20Tool-Use%20for%20Images%20and%20Videos&entry.906535625=Chaoyang%20Wang%20and%20Kaituo%20Feng%20and%20Dongyang%20Chen%20and%20Zhongyu%20Wang%20and%20Zhixun%20Li%20and%20Sicheng%20Gao%20and%20Meng%20Meng%20and%20Xu%20Zhou%20and%20Manyuan%20Zhang%20and%20Yuzhang%20Shang%20and%20Xiangyu%20Yue&entry.1292438233=Recent%20advances%20have%20shown%20that%20multimodal%20large%20language%20models%20%28MLLMs%29%20benefit%20from%20multimodal%20interleaved%20chain-of-thought%20%28CoT%29%20with%20vision%20tool%20interactions.%20However%2C%20existing%20open-source%20models%20often%20exhibit%20blind%20tool-use%20reasoning%20patterns%2C%20invoking%20vision%20tools%20even%20when%20they%20are%20unnecessary%2C%20which%20significantly%20increases%20inference%20overhead%20and%20degrades%20model%20performance.%20To%20this%20end%2C%20we%20propose%20AdaTooler-V%2C%20an%20MLLM%20that%20performs%20adaptive%20tool-use%20by%20determining%20whether%20a%20visual%20problem%20truly%20requires%20tools.%20First%2C%20we%20introduce%20AT-GRPO%2C%20a%20reinforcement%20learning%20algorithm%20that%20adaptively%20adjusts%20reward%20scales%20based%20on%20the%20Tool%20Benefit%20Score%20of%20each%20sample%2C%20encouraging%20the%20model%20to%20invoke%20tools%20only%20when%20they%20provide%20genuine%20improvements.%20Moreover%2C%20we%20construct%20two%20datasets%20to%20support%20training%3A%20AdaTooler-V-CoT-100k%20for%20SFT%20cold%20start%20and%20AdaTooler-V-300k%20for%20RL%20with%20verifiable%20rewards%20across%20single-image%2C%20multi-image%2C%20and%20video%20data.%20Experiments%20across%20twelve%20benchmarks%20demonstrate%20the%20strong%20reasoning%20capability%20of%20AdaTooler-V%2C%20outperforming%20existing%20methods%20in%20diverse%20visual%20reasoning%20tasks.%20Notably%2C%20AdaTooler-V-7B%20achieves%20an%20accuracy%20of%2089.8%5C%25%20on%20the%20high-resolution%20benchmark%20V%2A%2C%20surpassing%20the%20commercial%20proprietary%20model%20GPT-4o%20and%20Gemini%201.5%20Pro.%20All%20code%2C%20models%2C%20and%20data%20are%20released.&entry.1838667208=http%3A//arxiv.org/abs/2512.16918v1&entry.124074799=Read"},
{"title": "Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future", "author": "Tianshuai Hu and Xiaolu Liu and Song Wang and Yiyao Zhu and Ao Liang and Lingdong Kong and Guoyang Zhao and Zeying Gong and Jun Cen and Zhiyu Huang and Xiaoshuai Hao and Linfeng Li and Hang Song and Xiangtai Li and Jun Ma and Shaojie Shen and Jianke Zhu and Dacheng Tao and Ziwei Liu and Junwei Liang", "abstract": "Autonomous driving has long relied on modular \"Perception-Decision-Action\" pipelines, where hand-crafted interfaces and rule-based components often break down in complex or long-tailed scenarios. Their cascaded design further propagates perception errors, degrading downstream planning and control. Vision-Action (VA) models address some limitations by learning direct mappings from visual inputs to actions, but they remain opaque, sensitive to distribution shifts, and lack structured reasoning or instruction-following capabilities. Recent progress in Large Language Models (LLMs) and multimodal learning has motivated the emergence of Vision-Language-Action (VLA) frameworks, which integrate perception with language-grounded decision making. By unifying visual understanding, linguistic reasoning, and actionable outputs, VLAs offer a pathway toward more interpretable, generalizable, and human-aligned driving policies. This work provides a structured characterization of the emerging VLA landscape for autonomous driving. We trace the evolution from early VA approaches to modern VLA frameworks and organize existing methods into two principal paradigms: End-to-End VLA, which integrates perception, reasoning, and planning within a single model, and Dual-System VLA, which separates slow deliberation (via VLMs) from fast, safety-critical execution (via planners). Within these paradigms, we further distinguish subclasses such as textual vs. numerical action generators and explicit vs. implicit guidance mechanisms. We also summarize representative datasets and benchmarks for evaluating VLA-based driving systems and highlight key challenges and open directions, including robustness, interpretability, and instruction fidelity. Overall, this work aims to establish a coherent foundation for advancing human-compatible autonomous driving systems.", "link": "http://arxiv.org/abs/2512.16760v1", "date": "2025-12-18", "relevancy": 2.7807, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5652}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5652}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-Language-Action%20Models%20for%20Autonomous%20Driving%3A%20Past%2C%20Present%2C%20and%20Future&body=Title%3A%20Vision-Language-Action%20Models%20for%20Autonomous%20Driving%3A%20Past%2C%20Present%2C%20and%20Future%0AAuthor%3A%20Tianshuai%20Hu%20and%20Xiaolu%20Liu%20and%20Song%20Wang%20and%20Yiyao%20Zhu%20and%20Ao%20Liang%20and%20Lingdong%20Kong%20and%20Guoyang%20Zhao%20and%20Zeying%20Gong%20and%20Jun%20Cen%20and%20Zhiyu%20Huang%20and%20Xiaoshuai%20Hao%20and%20Linfeng%20Li%20and%20Hang%20Song%20and%20Xiangtai%20Li%20and%20Jun%20Ma%20and%20Shaojie%20Shen%20and%20Jianke%20Zhu%20and%20Dacheng%20Tao%20and%20Ziwei%20Liu%20and%20Junwei%20Liang%0AAbstract%3A%20Autonomous%20driving%20has%20long%20relied%20on%20modular%20%22Perception-Decision-Action%22%20pipelines%2C%20where%20hand-crafted%20interfaces%20and%20rule-based%20components%20often%20break%20down%20in%20complex%20or%20long-tailed%20scenarios.%20Their%20cascaded%20design%20further%20propagates%20perception%20errors%2C%20degrading%20downstream%20planning%20and%20control.%20Vision-Action%20%28VA%29%20models%20address%20some%20limitations%20by%20learning%20direct%20mappings%20from%20visual%20inputs%20to%20actions%2C%20but%20they%20remain%20opaque%2C%20sensitive%20to%20distribution%20shifts%2C%20and%20lack%20structured%20reasoning%20or%20instruction-following%20capabilities.%20Recent%20progress%20in%20Large%20Language%20Models%20%28LLMs%29%20and%20multimodal%20learning%20has%20motivated%20the%20emergence%20of%20Vision-Language-Action%20%28VLA%29%20frameworks%2C%20which%20integrate%20perception%20with%20language-grounded%20decision%20making.%20By%20unifying%20visual%20understanding%2C%20linguistic%20reasoning%2C%20and%20actionable%20outputs%2C%20VLAs%20offer%20a%20pathway%20toward%20more%20interpretable%2C%20generalizable%2C%20and%20human-aligned%20driving%20policies.%20This%20work%20provides%20a%20structured%20characterization%20of%20the%20emerging%20VLA%20landscape%20for%20autonomous%20driving.%20We%20trace%20the%20evolution%20from%20early%20VA%20approaches%20to%20modern%20VLA%20frameworks%20and%20organize%20existing%20methods%20into%20two%20principal%20paradigms%3A%20End-to-End%20VLA%2C%20which%20integrates%20perception%2C%20reasoning%2C%20and%20planning%20within%20a%20single%20model%2C%20and%20Dual-System%20VLA%2C%20which%20separates%20slow%20deliberation%20%28via%20VLMs%29%20from%20fast%2C%20safety-critical%20execution%20%28via%20planners%29.%20Within%20these%20paradigms%2C%20we%20further%20distinguish%20subclasses%20such%20as%20textual%20vs.%20numerical%20action%20generators%20and%20explicit%20vs.%20implicit%20guidance%20mechanisms.%20We%20also%20summarize%20representative%20datasets%20and%20benchmarks%20for%20evaluating%20VLA-based%20driving%20systems%20and%20highlight%20key%20challenges%20and%20open%20directions%2C%20including%20robustness%2C%20interpretability%2C%20and%20instruction%20fidelity.%20Overall%2C%20this%20work%20aims%20to%20establish%20a%20coherent%20foundation%20for%20advancing%20human-compatible%20autonomous%20driving%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-Language-Action%2520Models%2520for%2520Autonomous%2520Driving%253A%2520Past%252C%2520Present%252C%2520and%2520Future%26entry.906535625%3DTianshuai%2520Hu%2520and%2520Xiaolu%2520Liu%2520and%2520Song%2520Wang%2520and%2520Yiyao%2520Zhu%2520and%2520Ao%2520Liang%2520and%2520Lingdong%2520Kong%2520and%2520Guoyang%2520Zhao%2520and%2520Zeying%2520Gong%2520and%2520Jun%2520Cen%2520and%2520Zhiyu%2520Huang%2520and%2520Xiaoshuai%2520Hao%2520and%2520Linfeng%2520Li%2520and%2520Hang%2520Song%2520and%2520Xiangtai%2520Li%2520and%2520Jun%2520Ma%2520and%2520Shaojie%2520Shen%2520and%2520Jianke%2520Zhu%2520and%2520Dacheng%2520Tao%2520and%2520Ziwei%2520Liu%2520and%2520Junwei%2520Liang%26entry.1292438233%3DAutonomous%2520driving%2520has%2520long%2520relied%2520on%2520modular%2520%2522Perception-Decision-Action%2522%2520pipelines%252C%2520where%2520hand-crafted%2520interfaces%2520and%2520rule-based%2520components%2520often%2520break%2520down%2520in%2520complex%2520or%2520long-tailed%2520scenarios.%2520Their%2520cascaded%2520design%2520further%2520propagates%2520perception%2520errors%252C%2520degrading%2520downstream%2520planning%2520and%2520control.%2520Vision-Action%2520%2528VA%2529%2520models%2520address%2520some%2520limitations%2520by%2520learning%2520direct%2520mappings%2520from%2520visual%2520inputs%2520to%2520actions%252C%2520but%2520they%2520remain%2520opaque%252C%2520sensitive%2520to%2520distribution%2520shifts%252C%2520and%2520lack%2520structured%2520reasoning%2520or%2520instruction-following%2520capabilities.%2520Recent%2520progress%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520multimodal%2520learning%2520has%2520motivated%2520the%2520emergence%2520of%2520Vision-Language-Action%2520%2528VLA%2529%2520frameworks%252C%2520which%2520integrate%2520perception%2520with%2520language-grounded%2520decision%2520making.%2520By%2520unifying%2520visual%2520understanding%252C%2520linguistic%2520reasoning%252C%2520and%2520actionable%2520outputs%252C%2520VLAs%2520offer%2520a%2520pathway%2520toward%2520more%2520interpretable%252C%2520generalizable%252C%2520and%2520human-aligned%2520driving%2520policies.%2520This%2520work%2520provides%2520a%2520structured%2520characterization%2520of%2520the%2520emerging%2520VLA%2520landscape%2520for%2520autonomous%2520driving.%2520We%2520trace%2520the%2520evolution%2520from%2520early%2520VA%2520approaches%2520to%2520modern%2520VLA%2520frameworks%2520and%2520organize%2520existing%2520methods%2520into%2520two%2520principal%2520paradigms%253A%2520End-to-End%2520VLA%252C%2520which%2520integrates%2520perception%252C%2520reasoning%252C%2520and%2520planning%2520within%2520a%2520single%2520model%252C%2520and%2520Dual-System%2520VLA%252C%2520which%2520separates%2520slow%2520deliberation%2520%2528via%2520VLMs%2529%2520from%2520fast%252C%2520safety-critical%2520execution%2520%2528via%2520planners%2529.%2520Within%2520these%2520paradigms%252C%2520we%2520further%2520distinguish%2520subclasses%2520such%2520as%2520textual%2520vs.%2520numerical%2520action%2520generators%2520and%2520explicit%2520vs.%2520implicit%2520guidance%2520mechanisms.%2520We%2520also%2520summarize%2520representative%2520datasets%2520and%2520benchmarks%2520for%2520evaluating%2520VLA-based%2520driving%2520systems%2520and%2520highlight%2520key%2520challenges%2520and%2520open%2520directions%252C%2520including%2520robustness%252C%2520interpretability%252C%2520and%2520instruction%2520fidelity.%2520Overall%252C%2520this%2520work%2520aims%2520to%2520establish%2520a%2520coherent%2520foundation%2520for%2520advancing%2520human-compatible%2520autonomous%2520driving%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-Language-Action%20Models%20for%20Autonomous%20Driving%3A%20Past%2C%20Present%2C%20and%20Future&entry.906535625=Tianshuai%20Hu%20and%20Xiaolu%20Liu%20and%20Song%20Wang%20and%20Yiyao%20Zhu%20and%20Ao%20Liang%20and%20Lingdong%20Kong%20and%20Guoyang%20Zhao%20and%20Zeying%20Gong%20and%20Jun%20Cen%20and%20Zhiyu%20Huang%20and%20Xiaoshuai%20Hao%20and%20Linfeng%20Li%20and%20Hang%20Song%20and%20Xiangtai%20Li%20and%20Jun%20Ma%20and%20Shaojie%20Shen%20and%20Jianke%20Zhu%20and%20Dacheng%20Tao%20and%20Ziwei%20Liu%20and%20Junwei%20Liang&entry.1292438233=Autonomous%20driving%20has%20long%20relied%20on%20modular%20%22Perception-Decision-Action%22%20pipelines%2C%20where%20hand-crafted%20interfaces%20and%20rule-based%20components%20often%20break%20down%20in%20complex%20or%20long-tailed%20scenarios.%20Their%20cascaded%20design%20further%20propagates%20perception%20errors%2C%20degrading%20downstream%20planning%20and%20control.%20Vision-Action%20%28VA%29%20models%20address%20some%20limitations%20by%20learning%20direct%20mappings%20from%20visual%20inputs%20to%20actions%2C%20but%20they%20remain%20opaque%2C%20sensitive%20to%20distribution%20shifts%2C%20and%20lack%20structured%20reasoning%20or%20instruction-following%20capabilities.%20Recent%20progress%20in%20Large%20Language%20Models%20%28LLMs%29%20and%20multimodal%20learning%20has%20motivated%20the%20emergence%20of%20Vision-Language-Action%20%28VLA%29%20frameworks%2C%20which%20integrate%20perception%20with%20language-grounded%20decision%20making.%20By%20unifying%20visual%20understanding%2C%20linguistic%20reasoning%2C%20and%20actionable%20outputs%2C%20VLAs%20offer%20a%20pathway%20toward%20more%20interpretable%2C%20generalizable%2C%20and%20human-aligned%20driving%20policies.%20This%20work%20provides%20a%20structured%20characterization%20of%20the%20emerging%20VLA%20landscape%20for%20autonomous%20driving.%20We%20trace%20the%20evolution%20from%20early%20VA%20approaches%20to%20modern%20VLA%20frameworks%20and%20organize%20existing%20methods%20into%20two%20principal%20paradigms%3A%20End-to-End%20VLA%2C%20which%20integrates%20perception%2C%20reasoning%2C%20and%20planning%20within%20a%20single%20model%2C%20and%20Dual-System%20VLA%2C%20which%20separates%20slow%20deliberation%20%28via%20VLMs%29%20from%20fast%2C%20safety-critical%20execution%20%28via%20planners%29.%20Within%20these%20paradigms%2C%20we%20further%20distinguish%20subclasses%20such%20as%20textual%20vs.%20numerical%20action%20generators%20and%20explicit%20vs.%20implicit%20guidance%20mechanisms.%20We%20also%20summarize%20representative%20datasets%20and%20benchmarks%20for%20evaluating%20VLA-based%20driving%20systems%20and%20highlight%20key%20challenges%20and%20open%20directions%2C%20including%20robustness%2C%20interpretability%2C%20and%20instruction%20fidelity.%20Overall%2C%20this%20work%20aims%20to%20establish%20a%20coherent%20foundation%20for%20advancing%20human-compatible%20autonomous%20driving%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2512.16760v1&entry.124074799=Read"},
{"title": "Sketch-in-Latents: Eliciting Unified Reasoning in MLLMs", "author": "Jintao Tong and Jiaqi Gu and Yujing Lou and Lubin Fan and Yixiong Zou and Yue Wu and Jieping Ye and Ruixuan Li", "abstract": "While Multimodal Large Language Models (MLLMs) excel at visual understanding tasks through text reasoning, they often fall short in scenarios requiring visual imagination. Unlike current works that take predefined external toolkits or generate images during thinking, however, humans can form flexible visual-text imagination and interactions during thinking without predefined toolkits, where one important reason is that humans construct the visual-text thinking process in a unified space inside the brain. Inspired by this capability, given that current MLLMs already encode visual and text information in the same feature space, we hold that visual tokens can be seamlessly inserted into the reasoning process carried by text tokens, where ideally, all visual imagination processes can be encoded by the latent features. To achieve this goal, we propose Sketch-in-Latents (SkiLa), a novel paradigm for unified multi-modal reasoning that expands the auto-regressive capabilities of MLLMs to natively generate continuous visual embeddings, termed latent sketch tokens, as visual thoughts. During multi-step reasoning, the model dynamically alternates between textual thinking mode for generating textual think tokens and visual sketching mode for generating latent sketch tokens. A latent visual semantics reconstruction mechanism is proposed to ensure these latent sketch tokens are semantically grounded. Extensive experiments demonstrate that SkiLa achieves superior performance on vision-centric tasks while exhibiting strong generalization to diverse general multi-modal benchmarks. Codes will be released at https://github.com/TungChintao/SkiLa.", "link": "http://arxiv.org/abs/2512.16584v1", "date": "2025-12-18", "relevancy": 2.7399, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5509}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5465}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sketch-in-Latents%3A%20Eliciting%20Unified%20Reasoning%20in%20MLLMs&body=Title%3A%20Sketch-in-Latents%3A%20Eliciting%20Unified%20Reasoning%20in%20MLLMs%0AAuthor%3A%20Jintao%20Tong%20and%20Jiaqi%20Gu%20and%20Yujing%20Lou%20and%20Lubin%20Fan%20and%20Yixiong%20Zou%20and%20Yue%20Wu%20and%20Jieping%20Ye%20and%20Ruixuan%20Li%0AAbstract%3A%20While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20excel%20at%20visual%20understanding%20tasks%20through%20text%20reasoning%2C%20they%20often%20fall%20short%20in%20scenarios%20requiring%20visual%20imagination.%20Unlike%20current%20works%20that%20take%20predefined%20external%20toolkits%20or%20generate%20images%20during%20thinking%2C%20however%2C%20humans%20can%20form%20flexible%20visual-text%20imagination%20and%20interactions%20during%20thinking%20without%20predefined%20toolkits%2C%20where%20one%20important%20reason%20is%20that%20humans%20construct%20the%20visual-text%20thinking%20process%20in%20a%20unified%20space%20inside%20the%20brain.%20Inspired%20by%20this%20capability%2C%20given%20that%20current%20MLLMs%20already%20encode%20visual%20and%20text%20information%20in%20the%20same%20feature%20space%2C%20we%20hold%20that%20visual%20tokens%20can%20be%20seamlessly%20inserted%20into%20the%20reasoning%20process%20carried%20by%20text%20tokens%2C%20where%20ideally%2C%20all%20visual%20imagination%20processes%20can%20be%20encoded%20by%20the%20latent%20features.%20To%20achieve%20this%20goal%2C%20we%20propose%20Sketch-in-Latents%20%28SkiLa%29%2C%20a%20novel%20paradigm%20for%20unified%20multi-modal%20reasoning%20that%20expands%20the%20auto-regressive%20capabilities%20of%20MLLMs%20to%20natively%20generate%20continuous%20visual%20embeddings%2C%20termed%20latent%20sketch%20tokens%2C%20as%20visual%20thoughts.%20During%20multi-step%20reasoning%2C%20the%20model%20dynamically%20alternates%20between%20textual%20thinking%20mode%20for%20generating%20textual%20think%20tokens%20and%20visual%20sketching%20mode%20for%20generating%20latent%20sketch%20tokens.%20A%20latent%20visual%20semantics%20reconstruction%20mechanism%20is%20proposed%20to%20ensure%20these%20latent%20sketch%20tokens%20are%20semantically%20grounded.%20Extensive%20experiments%20demonstrate%20that%20SkiLa%20achieves%20superior%20performance%20on%20vision-centric%20tasks%20while%20exhibiting%20strong%20generalization%20to%20diverse%20general%20multi-modal%20benchmarks.%20Codes%20will%20be%20released%20at%20https%3A//github.com/TungChintao/SkiLa.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16584v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSketch-in-Latents%253A%2520Eliciting%2520Unified%2520Reasoning%2520in%2520MLLMs%26entry.906535625%3DJintao%2520Tong%2520and%2520Jiaqi%2520Gu%2520and%2520Yujing%2520Lou%2520and%2520Lubin%2520Fan%2520and%2520Yixiong%2520Zou%2520and%2520Yue%2520Wu%2520and%2520Jieping%2520Ye%2520and%2520Ruixuan%2520Li%26entry.1292438233%3DWhile%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520excel%2520at%2520visual%2520understanding%2520tasks%2520through%2520text%2520reasoning%252C%2520they%2520often%2520fall%2520short%2520in%2520scenarios%2520requiring%2520visual%2520imagination.%2520Unlike%2520current%2520works%2520that%2520take%2520predefined%2520external%2520toolkits%2520or%2520generate%2520images%2520during%2520thinking%252C%2520however%252C%2520humans%2520can%2520form%2520flexible%2520visual-text%2520imagination%2520and%2520interactions%2520during%2520thinking%2520without%2520predefined%2520toolkits%252C%2520where%2520one%2520important%2520reason%2520is%2520that%2520humans%2520construct%2520the%2520visual-text%2520thinking%2520process%2520in%2520a%2520unified%2520space%2520inside%2520the%2520brain.%2520Inspired%2520by%2520this%2520capability%252C%2520given%2520that%2520current%2520MLLMs%2520already%2520encode%2520visual%2520and%2520text%2520information%2520in%2520the%2520same%2520feature%2520space%252C%2520we%2520hold%2520that%2520visual%2520tokens%2520can%2520be%2520seamlessly%2520inserted%2520into%2520the%2520reasoning%2520process%2520carried%2520by%2520text%2520tokens%252C%2520where%2520ideally%252C%2520all%2520visual%2520imagination%2520processes%2520can%2520be%2520encoded%2520by%2520the%2520latent%2520features.%2520To%2520achieve%2520this%2520goal%252C%2520we%2520propose%2520Sketch-in-Latents%2520%2528SkiLa%2529%252C%2520a%2520novel%2520paradigm%2520for%2520unified%2520multi-modal%2520reasoning%2520that%2520expands%2520the%2520auto-regressive%2520capabilities%2520of%2520MLLMs%2520to%2520natively%2520generate%2520continuous%2520visual%2520embeddings%252C%2520termed%2520latent%2520sketch%2520tokens%252C%2520as%2520visual%2520thoughts.%2520During%2520multi-step%2520reasoning%252C%2520the%2520model%2520dynamically%2520alternates%2520between%2520textual%2520thinking%2520mode%2520for%2520generating%2520textual%2520think%2520tokens%2520and%2520visual%2520sketching%2520mode%2520for%2520generating%2520latent%2520sketch%2520tokens.%2520A%2520latent%2520visual%2520semantics%2520reconstruction%2520mechanism%2520is%2520proposed%2520to%2520ensure%2520these%2520latent%2520sketch%2520tokens%2520are%2520semantically%2520grounded.%2520Extensive%2520experiments%2520demonstrate%2520that%2520SkiLa%2520achieves%2520superior%2520performance%2520on%2520vision-centric%2520tasks%2520while%2520exhibiting%2520strong%2520generalization%2520to%2520diverse%2520general%2520multi-modal%2520benchmarks.%2520Codes%2520will%2520be%2520released%2520at%2520https%253A//github.com/TungChintao/SkiLa.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16584v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sketch-in-Latents%3A%20Eliciting%20Unified%20Reasoning%20in%20MLLMs&entry.906535625=Jintao%20Tong%20and%20Jiaqi%20Gu%20and%20Yujing%20Lou%20and%20Lubin%20Fan%20and%20Yixiong%20Zou%20and%20Yue%20Wu%20and%20Jieping%20Ye%20and%20Ruixuan%20Li&entry.1292438233=While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20excel%20at%20visual%20understanding%20tasks%20through%20text%20reasoning%2C%20they%20often%20fall%20short%20in%20scenarios%20requiring%20visual%20imagination.%20Unlike%20current%20works%20that%20take%20predefined%20external%20toolkits%20or%20generate%20images%20during%20thinking%2C%20however%2C%20humans%20can%20form%20flexible%20visual-text%20imagination%20and%20interactions%20during%20thinking%20without%20predefined%20toolkits%2C%20where%20one%20important%20reason%20is%20that%20humans%20construct%20the%20visual-text%20thinking%20process%20in%20a%20unified%20space%20inside%20the%20brain.%20Inspired%20by%20this%20capability%2C%20given%20that%20current%20MLLMs%20already%20encode%20visual%20and%20text%20information%20in%20the%20same%20feature%20space%2C%20we%20hold%20that%20visual%20tokens%20can%20be%20seamlessly%20inserted%20into%20the%20reasoning%20process%20carried%20by%20text%20tokens%2C%20where%20ideally%2C%20all%20visual%20imagination%20processes%20can%20be%20encoded%20by%20the%20latent%20features.%20To%20achieve%20this%20goal%2C%20we%20propose%20Sketch-in-Latents%20%28SkiLa%29%2C%20a%20novel%20paradigm%20for%20unified%20multi-modal%20reasoning%20that%20expands%20the%20auto-regressive%20capabilities%20of%20MLLMs%20to%20natively%20generate%20continuous%20visual%20embeddings%2C%20termed%20latent%20sketch%20tokens%2C%20as%20visual%20thoughts.%20During%20multi-step%20reasoning%2C%20the%20model%20dynamically%20alternates%20between%20textual%20thinking%20mode%20for%20generating%20textual%20think%20tokens%20and%20visual%20sketching%20mode%20for%20generating%20latent%20sketch%20tokens.%20A%20latent%20visual%20semantics%20reconstruction%20mechanism%20is%20proposed%20to%20ensure%20these%20latent%20sketch%20tokens%20are%20semantically%20grounded.%20Extensive%20experiments%20demonstrate%20that%20SkiLa%20achieves%20superior%20performance%20on%20vision-centric%20tasks%20while%20exhibiting%20strong%20generalization%20to%20diverse%20general%20multi-modal%20benchmarks.%20Codes%20will%20be%20released%20at%20https%3A//github.com/TungChintao/SkiLa.&entry.1838667208=http%3A//arxiv.org/abs/2512.16584v1&entry.124074799=Read"},
{"title": "OMG-Bench: A New Challenging Benchmark for Skeleton-based Online Micro Hand Gesture Recognition", "author": "Haochen Chang and Pengfei Ren and Buyuan Zhang and Da Li and Tianhao Han and Haoyang Zhang and Liang Xie and Hongbo Chen and Erwei Yin", "abstract": "Online micro gesture recognition from hand skeletons is critical for VR/AR interaction but faces challenges due to limited public datasets and task-specific algorithms. Micro gestures involve subtle motion patterns, which make constructing datasets with precise skeletons and frame-level annotations difficult. To this end, we develop a multi-view self-supervised pipeline to automatically generate skeleton data, complemented by heuristic rules and expert refinement for semi-automatic annotation. Based on this pipeline, we introduce OMG-Bench, the first large-scale public benchmark for skeleton-based online micro gesture recognition. It features 40 fine-grained gesture classes with 13,948 instances across 1,272 sequences, characterized by subtle motions, rapid dynamics, and continuous execution. To tackle these challenges, we propose Hierarchical Memory-Augmented Transformer (HMATr), an end-to-end framework that unifies gesture detection and classification by leveraging hierarchical memory banks which store frame-level details and window-level semantics to preserve historical context. In addition, it employs learnable position-aware queries initialized from the memory to implicitly encode gesture positions and semantics. Experiments show that HMATr outperforms state-of-the-art methods by 7.6\\% in detection rate, establishing a strong baseline for online micro gesture recognition. Project page: https://omg-bench.github.io/", "link": "http://arxiv.org/abs/2512.16727v1", "date": "2025-12-18", "relevancy": 2.7078, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5621}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5372}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OMG-Bench%3A%20A%20New%20Challenging%20Benchmark%20for%20Skeleton-based%20Online%20Micro%20Hand%20Gesture%20Recognition&body=Title%3A%20OMG-Bench%3A%20A%20New%20Challenging%20Benchmark%20for%20Skeleton-based%20Online%20Micro%20Hand%20Gesture%20Recognition%0AAuthor%3A%20Haochen%20Chang%20and%20Pengfei%20Ren%20and%20Buyuan%20Zhang%20and%20Da%20Li%20and%20Tianhao%20Han%20and%20Haoyang%20Zhang%20and%20Liang%20Xie%20and%20Hongbo%20Chen%20and%20Erwei%20Yin%0AAbstract%3A%20Online%20micro%20gesture%20recognition%20from%20hand%20skeletons%20is%20critical%20for%20VR/AR%20interaction%20but%20faces%20challenges%20due%20to%20limited%20public%20datasets%20and%20task-specific%20algorithms.%20Micro%20gestures%20involve%20subtle%20motion%20patterns%2C%20which%20make%20constructing%20datasets%20with%20precise%20skeletons%20and%20frame-level%20annotations%20difficult.%20To%20this%20end%2C%20we%20develop%20a%20multi-view%20self-supervised%20pipeline%20to%20automatically%20generate%20skeleton%20data%2C%20complemented%20by%20heuristic%20rules%20and%20expert%20refinement%20for%20semi-automatic%20annotation.%20Based%20on%20this%20pipeline%2C%20we%20introduce%20OMG-Bench%2C%20the%20first%20large-scale%20public%20benchmark%20for%20skeleton-based%20online%20micro%20gesture%20recognition.%20It%20features%2040%20fine-grained%20gesture%20classes%20with%2013%2C948%20instances%20across%201%2C272%20sequences%2C%20characterized%20by%20subtle%20motions%2C%20rapid%20dynamics%2C%20and%20continuous%20execution.%20To%20tackle%20these%20challenges%2C%20we%20propose%20Hierarchical%20Memory-Augmented%20Transformer%20%28HMATr%29%2C%20an%20end-to-end%20framework%20that%20unifies%20gesture%20detection%20and%20classification%20by%20leveraging%20hierarchical%20memory%20banks%20which%20store%20frame-level%20details%20and%20window-level%20semantics%20to%20preserve%20historical%20context.%20In%20addition%2C%20it%20employs%20learnable%20position-aware%20queries%20initialized%20from%20the%20memory%20to%20implicitly%20encode%20gesture%20positions%20and%20semantics.%20Experiments%20show%20that%20HMATr%20outperforms%20state-of-the-art%20methods%20by%207.6%5C%25%20in%20detection%20rate%2C%20establishing%20a%20strong%20baseline%20for%20online%20micro%20gesture%20recognition.%20Project%20page%3A%20https%3A//omg-bench.github.io/%0ALink%3A%20http%3A//arxiv.org/abs/2512.16727v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOMG-Bench%253A%2520A%2520New%2520Challenging%2520Benchmark%2520for%2520Skeleton-based%2520Online%2520Micro%2520Hand%2520Gesture%2520Recognition%26entry.906535625%3DHaochen%2520Chang%2520and%2520Pengfei%2520Ren%2520and%2520Buyuan%2520Zhang%2520and%2520Da%2520Li%2520and%2520Tianhao%2520Han%2520and%2520Haoyang%2520Zhang%2520and%2520Liang%2520Xie%2520and%2520Hongbo%2520Chen%2520and%2520Erwei%2520Yin%26entry.1292438233%3DOnline%2520micro%2520gesture%2520recognition%2520from%2520hand%2520skeletons%2520is%2520critical%2520for%2520VR/AR%2520interaction%2520but%2520faces%2520challenges%2520due%2520to%2520limited%2520public%2520datasets%2520and%2520task-specific%2520algorithms.%2520Micro%2520gestures%2520involve%2520subtle%2520motion%2520patterns%252C%2520which%2520make%2520constructing%2520datasets%2520with%2520precise%2520skeletons%2520and%2520frame-level%2520annotations%2520difficult.%2520To%2520this%2520end%252C%2520we%2520develop%2520a%2520multi-view%2520self-supervised%2520pipeline%2520to%2520automatically%2520generate%2520skeleton%2520data%252C%2520complemented%2520by%2520heuristic%2520rules%2520and%2520expert%2520refinement%2520for%2520semi-automatic%2520annotation.%2520Based%2520on%2520this%2520pipeline%252C%2520we%2520introduce%2520OMG-Bench%252C%2520the%2520first%2520large-scale%2520public%2520benchmark%2520for%2520skeleton-based%2520online%2520micro%2520gesture%2520recognition.%2520It%2520features%252040%2520fine-grained%2520gesture%2520classes%2520with%252013%252C948%2520instances%2520across%25201%252C272%2520sequences%252C%2520characterized%2520by%2520subtle%2520motions%252C%2520rapid%2520dynamics%252C%2520and%2520continuous%2520execution.%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520propose%2520Hierarchical%2520Memory-Augmented%2520Transformer%2520%2528HMATr%2529%252C%2520an%2520end-to-end%2520framework%2520that%2520unifies%2520gesture%2520detection%2520and%2520classification%2520by%2520leveraging%2520hierarchical%2520memory%2520banks%2520which%2520store%2520frame-level%2520details%2520and%2520window-level%2520semantics%2520to%2520preserve%2520historical%2520context.%2520In%2520addition%252C%2520it%2520employs%2520learnable%2520position-aware%2520queries%2520initialized%2520from%2520the%2520memory%2520to%2520implicitly%2520encode%2520gesture%2520positions%2520and%2520semantics.%2520Experiments%2520show%2520that%2520HMATr%2520outperforms%2520state-of-the-art%2520methods%2520by%25207.6%255C%2525%2520in%2520detection%2520rate%252C%2520establishing%2520a%2520strong%2520baseline%2520for%2520online%2520micro%2520gesture%2520recognition.%2520Project%2520page%253A%2520https%253A//omg-bench.github.io/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16727v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OMG-Bench%3A%20A%20New%20Challenging%20Benchmark%20for%20Skeleton-based%20Online%20Micro%20Hand%20Gesture%20Recognition&entry.906535625=Haochen%20Chang%20and%20Pengfei%20Ren%20and%20Buyuan%20Zhang%20and%20Da%20Li%20and%20Tianhao%20Han%20and%20Haoyang%20Zhang%20and%20Liang%20Xie%20and%20Hongbo%20Chen%20and%20Erwei%20Yin&entry.1292438233=Online%20micro%20gesture%20recognition%20from%20hand%20skeletons%20is%20critical%20for%20VR/AR%20interaction%20but%20faces%20challenges%20due%20to%20limited%20public%20datasets%20and%20task-specific%20algorithms.%20Micro%20gestures%20involve%20subtle%20motion%20patterns%2C%20which%20make%20constructing%20datasets%20with%20precise%20skeletons%20and%20frame-level%20annotations%20difficult.%20To%20this%20end%2C%20we%20develop%20a%20multi-view%20self-supervised%20pipeline%20to%20automatically%20generate%20skeleton%20data%2C%20complemented%20by%20heuristic%20rules%20and%20expert%20refinement%20for%20semi-automatic%20annotation.%20Based%20on%20this%20pipeline%2C%20we%20introduce%20OMG-Bench%2C%20the%20first%20large-scale%20public%20benchmark%20for%20skeleton-based%20online%20micro%20gesture%20recognition.%20It%20features%2040%20fine-grained%20gesture%20classes%20with%2013%2C948%20instances%20across%201%2C272%20sequences%2C%20characterized%20by%20subtle%20motions%2C%20rapid%20dynamics%2C%20and%20continuous%20execution.%20To%20tackle%20these%20challenges%2C%20we%20propose%20Hierarchical%20Memory-Augmented%20Transformer%20%28HMATr%29%2C%20an%20end-to-end%20framework%20that%20unifies%20gesture%20detection%20and%20classification%20by%20leveraging%20hierarchical%20memory%20banks%20which%20store%20frame-level%20details%20and%20window-level%20semantics%20to%20preserve%20historical%20context.%20In%20addition%2C%20it%20employs%20learnable%20position-aware%20queries%20initialized%20from%20the%20memory%20to%20implicitly%20encode%20gesture%20positions%20and%20semantics.%20Experiments%20show%20that%20HMATr%20outperforms%20state-of-the-art%20methods%20by%207.6%5C%25%20in%20detection%20rate%2C%20establishing%20a%20strong%20baseline%20for%20online%20micro%20gesture%20recognition.%20Project%20page%3A%20https%3A//omg-bench.github.io/&entry.1838667208=http%3A//arxiv.org/abs/2512.16727v1&entry.124074799=Read"},
{"title": "Causal-Tune: Mining Causal Factors from Vision Foundation Models for Domain Generalized Semantic Segmentation", "author": "Yin Zhang and Yongqiang Zhang and Yaoyue Zheng and Bogdan Raducanu and Dan Liu", "abstract": "Fine-tuning Vision Foundation Models (VFMs) with a small number of parameters has shown remarkable performance in Domain Generalized Semantic Segmentation (DGSS). Most existing works either train lightweight adapters or refine intermediate features to achieve better generalization on unseen domains. However, they both overlook the fact that long-term pre-trained VFMs often exhibit artifacts, which hinder the utilization of valuable representations and ultimately degrade DGSS performance. Inspired by causal mechanisms, we observe that these artifacts are associated with non-causal factors, which usually reside in the low- and high-frequency components of the VFM spectrum. In this paper, we explicitly examine the causal and non-causal factors of features within VFMs for DGSS, and propose a simple yet effective method to identify and disentangle them, enabling more robust domain generalization. Specifically, we propose Causal-Tune, a novel fine-tuning strategy designed to extract causal factors and suppress non-causal ones from the features of VFMs. First, we extract the frequency spectrum of features from each layer using the Discrete Cosine Transform (DCT). A Gaussian band-pass filter is then applied to separate the spectrum into causal and non-causal components. To further refine the causal components, we introduce a set of causal-aware learnable tokens that operate in the frequency domain, while the non-causal components are discarded. Finally, refined features are transformed back into the spatial domain via inverse DCT and passed to the next layer. Extensive experiments conducted on various cross-domain tasks demonstrate the effectiveness of Causal-Tune. In particular, our method achieves superior performance under adverse weather conditions, improving +4.8% mIoU over the baseline in snow conditions.", "link": "http://arxiv.org/abs/2512.16567v1", "date": "2025-12-18", "relevancy": 2.702, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5455}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5379}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal-Tune%3A%20Mining%20Causal%20Factors%20from%20Vision%20Foundation%20Models%20for%20Domain%20Generalized%20Semantic%20Segmentation&body=Title%3A%20Causal-Tune%3A%20Mining%20Causal%20Factors%20from%20Vision%20Foundation%20Models%20for%20Domain%20Generalized%20Semantic%20Segmentation%0AAuthor%3A%20Yin%20Zhang%20and%20Yongqiang%20Zhang%20and%20Yaoyue%20Zheng%20and%20Bogdan%20Raducanu%20and%20Dan%20Liu%0AAbstract%3A%20Fine-tuning%20Vision%20Foundation%20Models%20%28VFMs%29%20with%20a%20small%20number%20of%20parameters%20has%20shown%20remarkable%20performance%20in%20Domain%20Generalized%20Semantic%20Segmentation%20%28DGSS%29.%20Most%20existing%20works%20either%20train%20lightweight%20adapters%20or%20refine%20intermediate%20features%20to%20achieve%20better%20generalization%20on%20unseen%20domains.%20However%2C%20they%20both%20overlook%20the%20fact%20that%20long-term%20pre-trained%20VFMs%20often%20exhibit%20artifacts%2C%20which%20hinder%20the%20utilization%20of%20valuable%20representations%20and%20ultimately%20degrade%20DGSS%20performance.%20Inspired%20by%20causal%20mechanisms%2C%20we%20observe%20that%20these%20artifacts%20are%20associated%20with%20non-causal%20factors%2C%20which%20usually%20reside%20in%20the%20low-%20and%20high-frequency%20components%20of%20the%20VFM%20spectrum.%20In%20this%20paper%2C%20we%20explicitly%20examine%20the%20causal%20and%20non-causal%20factors%20of%20features%20within%20VFMs%20for%20DGSS%2C%20and%20propose%20a%20simple%20yet%20effective%20method%20to%20identify%20and%20disentangle%20them%2C%20enabling%20more%20robust%20domain%20generalization.%20Specifically%2C%20we%20propose%20Causal-Tune%2C%20a%20novel%20fine-tuning%20strategy%20designed%20to%20extract%20causal%20factors%20and%20suppress%20non-causal%20ones%20from%20the%20features%20of%20VFMs.%20First%2C%20we%20extract%20the%20frequency%20spectrum%20of%20features%20from%20each%20layer%20using%20the%20Discrete%20Cosine%20Transform%20%28DCT%29.%20A%20Gaussian%20band-pass%20filter%20is%20then%20applied%20to%20separate%20the%20spectrum%20into%20causal%20and%20non-causal%20components.%20To%20further%20refine%20the%20causal%20components%2C%20we%20introduce%20a%20set%20of%20causal-aware%20learnable%20tokens%20that%20operate%20in%20the%20frequency%20domain%2C%20while%20the%20non-causal%20components%20are%20discarded.%20Finally%2C%20refined%20features%20are%20transformed%20back%20into%20the%20spatial%20domain%20via%20inverse%20DCT%20and%20passed%20to%20the%20next%20layer.%20Extensive%20experiments%20conducted%20on%20various%20cross-domain%20tasks%20demonstrate%20the%20effectiveness%20of%20Causal-Tune.%20In%20particular%2C%20our%20method%20achieves%20superior%20performance%20under%20adverse%20weather%20conditions%2C%20improving%20%2B4.8%25%20mIoU%20over%20the%20baseline%20in%20snow%20conditions.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16567v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal-Tune%253A%2520Mining%2520Causal%2520Factors%2520from%2520Vision%2520Foundation%2520Models%2520for%2520Domain%2520Generalized%2520Semantic%2520Segmentation%26entry.906535625%3DYin%2520Zhang%2520and%2520Yongqiang%2520Zhang%2520and%2520Yaoyue%2520Zheng%2520and%2520Bogdan%2520Raducanu%2520and%2520Dan%2520Liu%26entry.1292438233%3DFine-tuning%2520Vision%2520Foundation%2520Models%2520%2528VFMs%2529%2520with%2520a%2520small%2520number%2520of%2520parameters%2520has%2520shown%2520remarkable%2520performance%2520in%2520Domain%2520Generalized%2520Semantic%2520Segmentation%2520%2528DGSS%2529.%2520Most%2520existing%2520works%2520either%2520train%2520lightweight%2520adapters%2520or%2520refine%2520intermediate%2520features%2520to%2520achieve%2520better%2520generalization%2520on%2520unseen%2520domains.%2520However%252C%2520they%2520both%2520overlook%2520the%2520fact%2520that%2520long-term%2520pre-trained%2520VFMs%2520often%2520exhibit%2520artifacts%252C%2520which%2520hinder%2520the%2520utilization%2520of%2520valuable%2520representations%2520and%2520ultimately%2520degrade%2520DGSS%2520performance.%2520Inspired%2520by%2520causal%2520mechanisms%252C%2520we%2520observe%2520that%2520these%2520artifacts%2520are%2520associated%2520with%2520non-causal%2520factors%252C%2520which%2520usually%2520reside%2520in%2520the%2520low-%2520and%2520high-frequency%2520components%2520of%2520the%2520VFM%2520spectrum.%2520In%2520this%2520paper%252C%2520we%2520explicitly%2520examine%2520the%2520causal%2520and%2520non-causal%2520factors%2520of%2520features%2520within%2520VFMs%2520for%2520DGSS%252C%2520and%2520propose%2520a%2520simple%2520yet%2520effective%2520method%2520to%2520identify%2520and%2520disentangle%2520them%252C%2520enabling%2520more%2520robust%2520domain%2520generalization.%2520Specifically%252C%2520we%2520propose%2520Causal-Tune%252C%2520a%2520novel%2520fine-tuning%2520strategy%2520designed%2520to%2520extract%2520causal%2520factors%2520and%2520suppress%2520non-causal%2520ones%2520from%2520the%2520features%2520of%2520VFMs.%2520First%252C%2520we%2520extract%2520the%2520frequency%2520spectrum%2520of%2520features%2520from%2520each%2520layer%2520using%2520the%2520Discrete%2520Cosine%2520Transform%2520%2528DCT%2529.%2520A%2520Gaussian%2520band-pass%2520filter%2520is%2520then%2520applied%2520to%2520separate%2520the%2520spectrum%2520into%2520causal%2520and%2520non-causal%2520components.%2520To%2520further%2520refine%2520the%2520causal%2520components%252C%2520we%2520introduce%2520a%2520set%2520of%2520causal-aware%2520learnable%2520tokens%2520that%2520operate%2520in%2520the%2520frequency%2520domain%252C%2520while%2520the%2520non-causal%2520components%2520are%2520discarded.%2520Finally%252C%2520refined%2520features%2520are%2520transformed%2520back%2520into%2520the%2520spatial%2520domain%2520via%2520inverse%2520DCT%2520and%2520passed%2520to%2520the%2520next%2520layer.%2520Extensive%2520experiments%2520conducted%2520on%2520various%2520cross-domain%2520tasks%2520demonstrate%2520the%2520effectiveness%2520of%2520Causal-Tune.%2520In%2520particular%252C%2520our%2520method%2520achieves%2520superior%2520performance%2520under%2520adverse%2520weather%2520conditions%252C%2520improving%2520%252B4.8%2525%2520mIoU%2520over%2520the%2520baseline%2520in%2520snow%2520conditions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16567v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal-Tune%3A%20Mining%20Causal%20Factors%20from%20Vision%20Foundation%20Models%20for%20Domain%20Generalized%20Semantic%20Segmentation&entry.906535625=Yin%20Zhang%20and%20Yongqiang%20Zhang%20and%20Yaoyue%20Zheng%20and%20Bogdan%20Raducanu%20and%20Dan%20Liu&entry.1292438233=Fine-tuning%20Vision%20Foundation%20Models%20%28VFMs%29%20with%20a%20small%20number%20of%20parameters%20has%20shown%20remarkable%20performance%20in%20Domain%20Generalized%20Semantic%20Segmentation%20%28DGSS%29.%20Most%20existing%20works%20either%20train%20lightweight%20adapters%20or%20refine%20intermediate%20features%20to%20achieve%20better%20generalization%20on%20unseen%20domains.%20However%2C%20they%20both%20overlook%20the%20fact%20that%20long-term%20pre-trained%20VFMs%20often%20exhibit%20artifacts%2C%20which%20hinder%20the%20utilization%20of%20valuable%20representations%20and%20ultimately%20degrade%20DGSS%20performance.%20Inspired%20by%20causal%20mechanisms%2C%20we%20observe%20that%20these%20artifacts%20are%20associated%20with%20non-causal%20factors%2C%20which%20usually%20reside%20in%20the%20low-%20and%20high-frequency%20components%20of%20the%20VFM%20spectrum.%20In%20this%20paper%2C%20we%20explicitly%20examine%20the%20causal%20and%20non-causal%20factors%20of%20features%20within%20VFMs%20for%20DGSS%2C%20and%20propose%20a%20simple%20yet%20effective%20method%20to%20identify%20and%20disentangle%20them%2C%20enabling%20more%20robust%20domain%20generalization.%20Specifically%2C%20we%20propose%20Causal-Tune%2C%20a%20novel%20fine-tuning%20strategy%20designed%20to%20extract%20causal%20factors%20and%20suppress%20non-causal%20ones%20from%20the%20features%20of%20VFMs.%20First%2C%20we%20extract%20the%20frequency%20spectrum%20of%20features%20from%20each%20layer%20using%20the%20Discrete%20Cosine%20Transform%20%28DCT%29.%20A%20Gaussian%20band-pass%20filter%20is%20then%20applied%20to%20separate%20the%20spectrum%20into%20causal%20and%20non-causal%20components.%20To%20further%20refine%20the%20causal%20components%2C%20we%20introduce%20a%20set%20of%20causal-aware%20learnable%20tokens%20that%20operate%20in%20the%20frequency%20domain%2C%20while%20the%20non-causal%20components%20are%20discarded.%20Finally%2C%20refined%20features%20are%20transformed%20back%20into%20the%20spatial%20domain%20via%20inverse%20DCT%20and%20passed%20to%20the%20next%20layer.%20Extensive%20experiments%20conducted%20on%20various%20cross-domain%20tasks%20demonstrate%20the%20effectiveness%20of%20Causal-Tune.%20In%20particular%2C%20our%20method%20achieves%20superior%20performance%20under%20adverse%20weather%20conditions%2C%20improving%20%2B4.8%25%20mIoU%20over%20the%20baseline%20in%20snow%20conditions.&entry.1838667208=http%3A//arxiv.org/abs/2512.16567v1&entry.124074799=Read"},
{"title": "GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization", "author": "Yikun Wang and Zuyan Liu and Ziyi Wang and Han Hu and Pengfei Liu and Yongming Rao", "abstract": "Current research on agentic visual reasoning enables deep multimodal understanding but primarily focuses on image manipulation tools, leaving a gap toward more general-purpose agentic models. In this work, we revisit the geolocalization task, which requires not only nuanced visual grounding but also web search to confirm or refine hypotheses during reasoning. Since existing geolocalization benchmarks fail to meet the need for high-resolution imagery and the localization challenge for deep agentic reasoning, we curate GeoBench, a benchmark that includes photos and panoramas from around the world, along with a subset of satellite images of different cities to rigorously evaluate the geolocalization ability of agentic models. We also propose GeoVista, an agentic model that seamlessly integrates tool invocation within the reasoning loop, including an image-zoom-in tool to magnify regions of interest and a web-search tool to retrieve related web information. We develop a complete training pipeline for it, including a cold-start supervised fine-tuning (SFT) stage to learn reasoning patterns and tool-use priors, followed by a reinforcement learning (RL) stage to further enhance reasoning ability. We adopt a hierarchical reward to leverage multi-level geographical information and improve overall geolocalization performance. Experimental results show that GeoVista surpasses other open-source agentic models on the geolocalization task greatly and achieves performance comparable to closed-source models such as Gemini-2.5-flash and GPT-5 on most metrics.", "link": "http://arxiv.org/abs/2511.15705v2", "date": "2025-12-18", "relevancy": 2.6924, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5396}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.538}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoVista%3A%20Web-Augmented%20Agentic%20Visual%20Reasoning%20for%20Geolocalization&body=Title%3A%20GeoVista%3A%20Web-Augmented%20Agentic%20Visual%20Reasoning%20for%20Geolocalization%0AAuthor%3A%20Yikun%20Wang%20and%20Zuyan%20Liu%20and%20Ziyi%20Wang%20and%20Han%20Hu%20and%20Pengfei%20Liu%20and%20Yongming%20Rao%0AAbstract%3A%20Current%20research%20on%20agentic%20visual%20reasoning%20enables%20deep%20multimodal%20understanding%20but%20primarily%20focuses%20on%20image%20manipulation%20tools%2C%20leaving%20a%20gap%20toward%20more%20general-purpose%20agentic%20models.%20In%20this%20work%2C%20we%20revisit%20the%20geolocalization%20task%2C%20which%20requires%20not%20only%20nuanced%20visual%20grounding%20but%20also%20web%20search%20to%20confirm%20or%20refine%20hypotheses%20during%20reasoning.%20Since%20existing%20geolocalization%20benchmarks%20fail%20to%20meet%20the%20need%20for%20high-resolution%20imagery%20and%20the%20localization%20challenge%20for%20deep%20agentic%20reasoning%2C%20we%20curate%20GeoBench%2C%20a%20benchmark%20that%20includes%20photos%20and%20panoramas%20from%20around%20the%20world%2C%20along%20with%20a%20subset%20of%20satellite%20images%20of%20different%20cities%20to%20rigorously%20evaluate%20the%20geolocalization%20ability%20of%20agentic%20models.%20We%20also%20propose%20GeoVista%2C%20an%20agentic%20model%20that%20seamlessly%20integrates%20tool%20invocation%20within%20the%20reasoning%20loop%2C%20including%20an%20image-zoom-in%20tool%20to%20magnify%20regions%20of%20interest%20and%20a%20web-search%20tool%20to%20retrieve%20related%20web%20information.%20We%20develop%20a%20complete%20training%20pipeline%20for%20it%2C%20including%20a%20cold-start%20supervised%20fine-tuning%20%28SFT%29%20stage%20to%20learn%20reasoning%20patterns%20and%20tool-use%20priors%2C%20followed%20by%20a%20reinforcement%20learning%20%28RL%29%20stage%20to%20further%20enhance%20reasoning%20ability.%20We%20adopt%20a%20hierarchical%20reward%20to%20leverage%20multi-level%20geographical%20information%20and%20improve%20overall%20geolocalization%20performance.%20Experimental%20results%20show%20that%20GeoVista%20surpasses%20other%20open-source%20agentic%20models%20on%20the%20geolocalization%20task%20greatly%20and%20achieves%20performance%20comparable%20to%20closed-source%20models%20such%20as%20Gemini-2.5-flash%20and%20GPT-5%20on%20most%20metrics.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15705v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoVista%253A%2520Web-Augmented%2520Agentic%2520Visual%2520Reasoning%2520for%2520Geolocalization%26entry.906535625%3DYikun%2520Wang%2520and%2520Zuyan%2520Liu%2520and%2520Ziyi%2520Wang%2520and%2520Han%2520Hu%2520and%2520Pengfei%2520Liu%2520and%2520Yongming%2520Rao%26entry.1292438233%3DCurrent%2520research%2520on%2520agentic%2520visual%2520reasoning%2520enables%2520deep%2520multimodal%2520understanding%2520but%2520primarily%2520focuses%2520on%2520image%2520manipulation%2520tools%252C%2520leaving%2520a%2520gap%2520toward%2520more%2520general-purpose%2520agentic%2520models.%2520In%2520this%2520work%252C%2520we%2520revisit%2520the%2520geolocalization%2520task%252C%2520which%2520requires%2520not%2520only%2520nuanced%2520visual%2520grounding%2520but%2520also%2520web%2520search%2520to%2520confirm%2520or%2520refine%2520hypotheses%2520during%2520reasoning.%2520Since%2520existing%2520geolocalization%2520benchmarks%2520fail%2520to%2520meet%2520the%2520need%2520for%2520high-resolution%2520imagery%2520and%2520the%2520localization%2520challenge%2520for%2520deep%2520agentic%2520reasoning%252C%2520we%2520curate%2520GeoBench%252C%2520a%2520benchmark%2520that%2520includes%2520photos%2520and%2520panoramas%2520from%2520around%2520the%2520world%252C%2520along%2520with%2520a%2520subset%2520of%2520satellite%2520images%2520of%2520different%2520cities%2520to%2520rigorously%2520evaluate%2520the%2520geolocalization%2520ability%2520of%2520agentic%2520models.%2520We%2520also%2520propose%2520GeoVista%252C%2520an%2520agentic%2520model%2520that%2520seamlessly%2520integrates%2520tool%2520invocation%2520within%2520the%2520reasoning%2520loop%252C%2520including%2520an%2520image-zoom-in%2520tool%2520to%2520magnify%2520regions%2520of%2520interest%2520and%2520a%2520web-search%2520tool%2520to%2520retrieve%2520related%2520web%2520information.%2520We%2520develop%2520a%2520complete%2520training%2520pipeline%2520for%2520it%252C%2520including%2520a%2520cold-start%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520stage%2520to%2520learn%2520reasoning%2520patterns%2520and%2520tool-use%2520priors%252C%2520followed%2520by%2520a%2520reinforcement%2520learning%2520%2528RL%2529%2520stage%2520to%2520further%2520enhance%2520reasoning%2520ability.%2520We%2520adopt%2520a%2520hierarchical%2520reward%2520to%2520leverage%2520multi-level%2520geographical%2520information%2520and%2520improve%2520overall%2520geolocalization%2520performance.%2520Experimental%2520results%2520show%2520that%2520GeoVista%2520surpasses%2520other%2520open-source%2520agentic%2520models%2520on%2520the%2520geolocalization%2520task%2520greatly%2520and%2520achieves%2520performance%2520comparable%2520to%2520closed-source%2520models%2520such%2520as%2520Gemini-2.5-flash%2520and%2520GPT-5%2520on%2520most%2520metrics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15705v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoVista%3A%20Web-Augmented%20Agentic%20Visual%20Reasoning%20for%20Geolocalization&entry.906535625=Yikun%20Wang%20and%20Zuyan%20Liu%20and%20Ziyi%20Wang%20and%20Han%20Hu%20and%20Pengfei%20Liu%20and%20Yongming%20Rao&entry.1292438233=Current%20research%20on%20agentic%20visual%20reasoning%20enables%20deep%20multimodal%20understanding%20but%20primarily%20focuses%20on%20image%20manipulation%20tools%2C%20leaving%20a%20gap%20toward%20more%20general-purpose%20agentic%20models.%20In%20this%20work%2C%20we%20revisit%20the%20geolocalization%20task%2C%20which%20requires%20not%20only%20nuanced%20visual%20grounding%20but%20also%20web%20search%20to%20confirm%20or%20refine%20hypotheses%20during%20reasoning.%20Since%20existing%20geolocalization%20benchmarks%20fail%20to%20meet%20the%20need%20for%20high-resolution%20imagery%20and%20the%20localization%20challenge%20for%20deep%20agentic%20reasoning%2C%20we%20curate%20GeoBench%2C%20a%20benchmark%20that%20includes%20photos%20and%20panoramas%20from%20around%20the%20world%2C%20along%20with%20a%20subset%20of%20satellite%20images%20of%20different%20cities%20to%20rigorously%20evaluate%20the%20geolocalization%20ability%20of%20agentic%20models.%20We%20also%20propose%20GeoVista%2C%20an%20agentic%20model%20that%20seamlessly%20integrates%20tool%20invocation%20within%20the%20reasoning%20loop%2C%20including%20an%20image-zoom-in%20tool%20to%20magnify%20regions%20of%20interest%20and%20a%20web-search%20tool%20to%20retrieve%20related%20web%20information.%20We%20develop%20a%20complete%20training%20pipeline%20for%20it%2C%20including%20a%20cold-start%20supervised%20fine-tuning%20%28SFT%29%20stage%20to%20learn%20reasoning%20patterns%20and%20tool-use%20priors%2C%20followed%20by%20a%20reinforcement%20learning%20%28RL%29%20stage%20to%20further%20enhance%20reasoning%20ability.%20We%20adopt%20a%20hierarchical%20reward%20to%20leverage%20multi-level%20geographical%20information%20and%20improve%20overall%20geolocalization%20performance.%20Experimental%20results%20show%20that%20GeoVista%20surpasses%20other%20open-source%20agentic%20models%20on%20the%20geolocalization%20task%20greatly%20and%20achieves%20performance%20comparable%20to%20closed-source%20models%20such%20as%20Gemini-2.5-flash%20and%20GPT-5%20on%20most%20metrics.&entry.1838667208=http%3A//arxiv.org/abs/2511.15705v2&entry.124074799=Read"},
{"title": "Few-Shot Fingerprinting Subject Re-Identification in 3D-MRI and 2D-X-Ray", "author": "Gon\u00e7alo Gaspar Alves and Shekoufeh Gorgi Zadeh and Andreas Husch and Ben Bausch", "abstract": "Combining open-source datasets can introduce data leakage if the same subject appears in multiple sets, leading to inflated model performance. To address this, we explore subject fingerprinting, mapping all images of a subject to a distinct region in latent space, to enable subject re-identification via similarity matching. Using a ResNet-50 trained with triplet margin loss, we evaluate few-shot fingerprinting on 3D MRI and 2D X-ray data in both standard (20-way 1-shot) and challenging (1000-way 1-shot) scenarios. The model achieves high Mean- Recall-@-K scores: 99.10% (20-way 1-shot) and 90.06% (500-way 5-shot) on ChestXray-14; 99.20% (20-way 1-shot) and 98.86% (100-way 3-shot) on BraTS- 2021.", "link": "http://arxiv.org/abs/2512.16685v1", "date": "2025-12-18", "relevancy": 2.6726, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.549}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5304}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-Shot%20Fingerprinting%20Subject%20Re-Identification%20in%203D-MRI%20and%202D-X-Ray&body=Title%3A%20Few-Shot%20Fingerprinting%20Subject%20Re-Identification%20in%203D-MRI%20and%202D-X-Ray%0AAuthor%3A%20Gon%C3%A7alo%20Gaspar%20Alves%20and%20Shekoufeh%20Gorgi%20Zadeh%20and%20Andreas%20Husch%20and%20Ben%20Bausch%0AAbstract%3A%20Combining%20open-source%20datasets%20can%20introduce%20data%20leakage%20if%20the%20same%20subject%20appears%20in%20multiple%20sets%2C%20leading%20to%20inflated%20model%20performance.%20To%20address%20this%2C%20we%20explore%20subject%20fingerprinting%2C%20mapping%20all%20images%20of%20a%20subject%20to%20a%20distinct%20region%20in%20latent%20space%2C%20to%20enable%20subject%20re-identification%20via%20similarity%20matching.%20Using%20a%20ResNet-50%20trained%20with%20triplet%20margin%20loss%2C%20we%20evaluate%20few-shot%20fingerprinting%20on%203D%20MRI%20and%202D%20X-ray%20data%20in%20both%20standard%20%2820-way%201-shot%29%20and%20challenging%20%281000-way%201-shot%29%20scenarios.%20The%20model%20achieves%20high%20Mean-%20Recall-%40-K%20scores%3A%2099.10%25%20%2820-way%201-shot%29%20and%2090.06%25%20%28500-way%205-shot%29%20on%20ChestXray-14%3B%2099.20%25%20%2820-way%201-shot%29%20and%2098.86%25%20%28100-way%203-shot%29%20on%20BraTS-%202021.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16685v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-Shot%2520Fingerprinting%2520Subject%2520Re-Identification%2520in%25203D-MRI%2520and%25202D-X-Ray%26entry.906535625%3DGon%25C3%25A7alo%2520Gaspar%2520Alves%2520and%2520Shekoufeh%2520Gorgi%2520Zadeh%2520and%2520Andreas%2520Husch%2520and%2520Ben%2520Bausch%26entry.1292438233%3DCombining%2520open-source%2520datasets%2520can%2520introduce%2520data%2520leakage%2520if%2520the%2520same%2520subject%2520appears%2520in%2520multiple%2520sets%252C%2520leading%2520to%2520inflated%2520model%2520performance.%2520To%2520address%2520this%252C%2520we%2520explore%2520subject%2520fingerprinting%252C%2520mapping%2520all%2520images%2520of%2520a%2520subject%2520to%2520a%2520distinct%2520region%2520in%2520latent%2520space%252C%2520to%2520enable%2520subject%2520re-identification%2520via%2520similarity%2520matching.%2520Using%2520a%2520ResNet-50%2520trained%2520with%2520triplet%2520margin%2520loss%252C%2520we%2520evaluate%2520few-shot%2520fingerprinting%2520on%25203D%2520MRI%2520and%25202D%2520X-ray%2520data%2520in%2520both%2520standard%2520%252820-way%25201-shot%2529%2520and%2520challenging%2520%25281000-way%25201-shot%2529%2520scenarios.%2520The%2520model%2520achieves%2520high%2520Mean-%2520Recall-%2540-K%2520scores%253A%252099.10%2525%2520%252820-way%25201-shot%2529%2520and%252090.06%2525%2520%2528500-way%25205-shot%2529%2520on%2520ChestXray-14%253B%252099.20%2525%2520%252820-way%25201-shot%2529%2520and%252098.86%2525%2520%2528100-way%25203-shot%2529%2520on%2520BraTS-%25202021.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16685v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-Shot%20Fingerprinting%20Subject%20Re-Identification%20in%203D-MRI%20and%202D-X-Ray&entry.906535625=Gon%C3%A7alo%20Gaspar%20Alves%20and%20Shekoufeh%20Gorgi%20Zadeh%20and%20Andreas%20Husch%20and%20Ben%20Bausch&entry.1292438233=Combining%20open-source%20datasets%20can%20introduce%20data%20leakage%20if%20the%20same%20subject%20appears%20in%20multiple%20sets%2C%20leading%20to%20inflated%20model%20performance.%20To%20address%20this%2C%20we%20explore%20subject%20fingerprinting%2C%20mapping%20all%20images%20of%20a%20subject%20to%20a%20distinct%20region%20in%20latent%20space%2C%20to%20enable%20subject%20re-identification%20via%20similarity%20matching.%20Using%20a%20ResNet-50%20trained%20with%20triplet%20margin%20loss%2C%20we%20evaluate%20few-shot%20fingerprinting%20on%203D%20MRI%20and%202D%20X-ray%20data%20in%20both%20standard%20%2820-way%201-shot%29%20and%20challenging%20%281000-way%201-shot%29%20scenarios.%20The%20model%20achieves%20high%20Mean-%20Recall-%40-K%20scores%3A%2099.10%25%20%2820-way%201-shot%29%20and%2090.06%25%20%28500-way%205-shot%29%20on%20ChestXray-14%3B%2099.20%25%20%2820-way%201-shot%29%20and%2098.86%25%20%28100-way%203-shot%29%20on%20BraTS-%202021.&entry.1838667208=http%3A//arxiv.org/abs/2512.16685v1&entry.124074799=Read"},
{"title": "ViStoryBench: Comprehensive Benchmark Suite for Story Visualization", "author": "Cailin Zhuang and Ailin Huang and Yaoqi Hu and Jingwei Wu and Wei Cheng and Jiaqi Liao and Hongyuan Wang and Xinyao Liao and Weiwei Cai and Hengyuan Xu and Xuanyang Zhang and Xianfang Zeng and Zhewei Huang and Gang Yu and Chi Zhang", "abstract": "Story visualization aims to generate coherent image sequences that faithfully depict a narrative and align with character references. Despite progress in generative models, existing benchmarks are narrow in scope, often limited to short prompts, lacking character references, or single-image cases, and fail to capture real-world storytelling complexity. This hinders a nuanced understanding of model capabilities and limitations. We present \\textbf{ViStoryBench}, a comprehensive benchmark designed to evaluate story visualization models across diverse narrative structures, visual styles, and character settings. The benchmark features richly annotated multi-shot scripts derived from curated stories spanning literature, film, and folklore. Large language models assist in story summarization and script generation, with all outputs human-verified to ensure coherence and fidelity. Character references are carefully curated to maintain intra-story consistency across varying artistic styles. To enable thorough evaluation, ViStoryBench introduces a set of automated metrics that assess character consistency, style similarity, prompt alignment, aesthetic quality, and generation artifacts such as copy-paste behavior. These metrics are validated through human studies, and used to benchmark a broad range of open-source and commercial models. ViStoryBench offers a multi-dimensional evaluation suite that facilitates systematic analysis and fosters future progress in visual storytelling.", "link": "http://arxiv.org/abs/2505.24862v4", "date": "2025-12-18", "relevancy": 2.6538, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5375}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5375}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViStoryBench%3A%20Comprehensive%20Benchmark%20Suite%20for%20Story%20Visualization&body=Title%3A%20ViStoryBench%3A%20Comprehensive%20Benchmark%20Suite%20for%20Story%20Visualization%0AAuthor%3A%20Cailin%20Zhuang%20and%20Ailin%20Huang%20and%20Yaoqi%20Hu%20and%20Jingwei%20Wu%20and%20Wei%20Cheng%20and%20Jiaqi%20Liao%20and%20Hongyuan%20Wang%20and%20Xinyao%20Liao%20and%20Weiwei%20Cai%20and%20Hengyuan%20Xu%20and%20Xuanyang%20Zhang%20and%20Xianfang%20Zeng%20and%20Zhewei%20Huang%20and%20Gang%20Yu%20and%20Chi%20Zhang%0AAbstract%3A%20Story%20visualization%20aims%20to%20generate%20coherent%20image%20sequences%20that%20faithfully%20depict%20a%20narrative%20and%20align%20with%20character%20references.%20Despite%20progress%20in%20generative%20models%2C%20existing%20benchmarks%20are%20narrow%20in%20scope%2C%20often%20limited%20to%20short%20prompts%2C%20lacking%20character%20references%2C%20or%20single-image%20cases%2C%20and%20fail%20to%20capture%20real-world%20storytelling%20complexity.%20This%20hinders%20a%20nuanced%20understanding%20of%20model%20capabilities%20and%20limitations.%20We%20present%20%5Ctextbf%7BViStoryBench%7D%2C%20a%20comprehensive%20benchmark%20designed%20to%20evaluate%20story%20visualization%20models%20across%20diverse%20narrative%20structures%2C%20visual%20styles%2C%20and%20character%20settings.%20The%20benchmark%20features%20richly%20annotated%20multi-shot%20scripts%20derived%20from%20curated%20stories%20spanning%20literature%2C%20film%2C%20and%20folklore.%20Large%20language%20models%20assist%20in%20story%20summarization%20and%20script%20generation%2C%20with%20all%20outputs%20human-verified%20to%20ensure%20coherence%20and%20fidelity.%20Character%20references%20are%20carefully%20curated%20to%20maintain%20intra-story%20consistency%20across%20varying%20artistic%20styles.%20To%20enable%20thorough%20evaluation%2C%20ViStoryBench%20introduces%20a%20set%20of%20automated%20metrics%20that%20assess%20character%20consistency%2C%20style%20similarity%2C%20prompt%20alignment%2C%20aesthetic%20quality%2C%20and%20generation%20artifacts%20such%20as%20copy-paste%20behavior.%20These%20metrics%20are%20validated%20through%20human%20studies%2C%20and%20used%20to%20benchmark%20a%20broad%20range%20of%20open-source%20and%20commercial%20models.%20ViStoryBench%20offers%20a%20multi-dimensional%20evaluation%20suite%20that%20facilitates%20systematic%20analysis%20and%20fosters%20future%20progress%20in%20visual%20storytelling.%0ALink%3A%20http%3A//arxiv.org/abs/2505.24862v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViStoryBench%253A%2520Comprehensive%2520Benchmark%2520Suite%2520for%2520Story%2520Visualization%26entry.906535625%3DCailin%2520Zhuang%2520and%2520Ailin%2520Huang%2520and%2520Yaoqi%2520Hu%2520and%2520Jingwei%2520Wu%2520and%2520Wei%2520Cheng%2520and%2520Jiaqi%2520Liao%2520and%2520Hongyuan%2520Wang%2520and%2520Xinyao%2520Liao%2520and%2520Weiwei%2520Cai%2520and%2520Hengyuan%2520Xu%2520and%2520Xuanyang%2520Zhang%2520and%2520Xianfang%2520Zeng%2520and%2520Zhewei%2520Huang%2520and%2520Gang%2520Yu%2520and%2520Chi%2520Zhang%26entry.1292438233%3DStory%2520visualization%2520aims%2520to%2520generate%2520coherent%2520image%2520sequences%2520that%2520faithfully%2520depict%2520a%2520narrative%2520and%2520align%2520with%2520character%2520references.%2520Despite%2520progress%2520in%2520generative%2520models%252C%2520existing%2520benchmarks%2520are%2520narrow%2520in%2520scope%252C%2520often%2520limited%2520to%2520short%2520prompts%252C%2520lacking%2520character%2520references%252C%2520or%2520single-image%2520cases%252C%2520and%2520fail%2520to%2520capture%2520real-world%2520storytelling%2520complexity.%2520This%2520hinders%2520a%2520nuanced%2520understanding%2520of%2520model%2520capabilities%2520and%2520limitations.%2520We%2520present%2520%255Ctextbf%257BViStoryBench%257D%252C%2520a%2520comprehensive%2520benchmark%2520designed%2520to%2520evaluate%2520story%2520visualization%2520models%2520across%2520diverse%2520narrative%2520structures%252C%2520visual%2520styles%252C%2520and%2520character%2520settings.%2520The%2520benchmark%2520features%2520richly%2520annotated%2520multi-shot%2520scripts%2520derived%2520from%2520curated%2520stories%2520spanning%2520literature%252C%2520film%252C%2520and%2520folklore.%2520Large%2520language%2520models%2520assist%2520in%2520story%2520summarization%2520and%2520script%2520generation%252C%2520with%2520all%2520outputs%2520human-verified%2520to%2520ensure%2520coherence%2520and%2520fidelity.%2520Character%2520references%2520are%2520carefully%2520curated%2520to%2520maintain%2520intra-story%2520consistency%2520across%2520varying%2520artistic%2520styles.%2520To%2520enable%2520thorough%2520evaluation%252C%2520ViStoryBench%2520introduces%2520a%2520set%2520of%2520automated%2520metrics%2520that%2520assess%2520character%2520consistency%252C%2520style%2520similarity%252C%2520prompt%2520alignment%252C%2520aesthetic%2520quality%252C%2520and%2520generation%2520artifacts%2520such%2520as%2520copy-paste%2520behavior.%2520These%2520metrics%2520are%2520validated%2520through%2520human%2520studies%252C%2520and%2520used%2520to%2520benchmark%2520a%2520broad%2520range%2520of%2520open-source%2520and%2520commercial%2520models.%2520ViStoryBench%2520offers%2520a%2520multi-dimensional%2520evaluation%2520suite%2520that%2520facilitates%2520systematic%2520analysis%2520and%2520fosters%2520future%2520progress%2520in%2520visual%2520storytelling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24862v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViStoryBench%3A%20Comprehensive%20Benchmark%20Suite%20for%20Story%20Visualization&entry.906535625=Cailin%20Zhuang%20and%20Ailin%20Huang%20and%20Yaoqi%20Hu%20and%20Jingwei%20Wu%20and%20Wei%20Cheng%20and%20Jiaqi%20Liao%20and%20Hongyuan%20Wang%20and%20Xinyao%20Liao%20and%20Weiwei%20Cai%20and%20Hengyuan%20Xu%20and%20Xuanyang%20Zhang%20and%20Xianfang%20Zeng%20and%20Zhewei%20Huang%20and%20Gang%20Yu%20and%20Chi%20Zhang&entry.1292438233=Story%20visualization%20aims%20to%20generate%20coherent%20image%20sequences%20that%20faithfully%20depict%20a%20narrative%20and%20align%20with%20character%20references.%20Despite%20progress%20in%20generative%20models%2C%20existing%20benchmarks%20are%20narrow%20in%20scope%2C%20often%20limited%20to%20short%20prompts%2C%20lacking%20character%20references%2C%20or%20single-image%20cases%2C%20and%20fail%20to%20capture%20real-world%20storytelling%20complexity.%20This%20hinders%20a%20nuanced%20understanding%20of%20model%20capabilities%20and%20limitations.%20We%20present%20%5Ctextbf%7BViStoryBench%7D%2C%20a%20comprehensive%20benchmark%20designed%20to%20evaluate%20story%20visualization%20models%20across%20diverse%20narrative%20structures%2C%20visual%20styles%2C%20and%20character%20settings.%20The%20benchmark%20features%20richly%20annotated%20multi-shot%20scripts%20derived%20from%20curated%20stories%20spanning%20literature%2C%20film%2C%20and%20folklore.%20Large%20language%20models%20assist%20in%20story%20summarization%20and%20script%20generation%2C%20with%20all%20outputs%20human-verified%20to%20ensure%20coherence%20and%20fidelity.%20Character%20references%20are%20carefully%20curated%20to%20maintain%20intra-story%20consistency%20across%20varying%20artistic%20styles.%20To%20enable%20thorough%20evaluation%2C%20ViStoryBench%20introduces%20a%20set%20of%20automated%20metrics%20that%20assess%20character%20consistency%2C%20style%20similarity%2C%20prompt%20alignment%2C%20aesthetic%20quality%2C%20and%20generation%20artifacts%20such%20as%20copy-paste%20behavior.%20These%20metrics%20are%20validated%20through%20human%20studies%2C%20and%20used%20to%20benchmark%20a%20broad%20range%20of%20open-source%20and%20commercial%20models.%20ViStoryBench%20offers%20a%20multi-dimensional%20evaluation%20suite%20that%20facilitates%20systematic%20analysis%20and%20fosters%20future%20progress%20in%20visual%20storytelling.&entry.1838667208=http%3A//arxiv.org/abs/2505.24862v4&entry.124074799=Read"},
{"title": "Quantifying and Bridging the Fidelity Gap: A Decisive-Feature Approach to Comparing Synthetic and Real Imagery", "author": "Danial Safaei and Siddartha Khastgir and Mohsen Alirezaei and Jeroen Ploeg and Son Tong and Xingyu Zhao", "abstract": "Virtual testing using synthetic data has become a cornerstone of autonomous vehicle (AV) safety assurance. Despite progress in improving visual realism through advanced simulators and generative AI, recent studies reveal that pixel-level fidelity alone does not ensure reliable transfer from simulation to the real world. What truly matters is whether the system-under-test (SUT) bases its decisions on the same causal evidence in both real and simulated environments - not just whether images \"look real\" to humans. This paper addresses the lack of such a behavior-grounded fidelity measure by introducing Decisive Feature Fidelity (DFF), a new SUT-specific metric that extends the existing fidelity spectrum to capture mechanism parity - the agreement in causal evidence underlying the SUT's decisions across domains. DFF leverages explainable-AI (XAI) methods to identify and compare the decisive features driving the SUT's outputs for matched real-synthetic pairs. We further propose practical estimators based on counterfactual explanations, along with a DFF-guided calibration scheme to enhance simulator fidelity. Experiments on 2126 matched KITTI-VirtualKITTI2 pairs demonstrate that DFF reveals discrepancies overlooked by conventional output-value fidelity. Furthermore, results show that DFF-guided calibration improves decisive-feature and input-level fidelity without sacrificing output value fidelity across diverse SUTs.", "link": "http://arxiv.org/abs/2512.16468v1", "date": "2025-12-18", "relevancy": 2.6467, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5426}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5227}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantifying%20and%20Bridging%20the%20Fidelity%20Gap%3A%20A%20Decisive-Feature%20Approach%20to%20Comparing%20Synthetic%20and%20Real%20Imagery&body=Title%3A%20Quantifying%20and%20Bridging%20the%20Fidelity%20Gap%3A%20A%20Decisive-Feature%20Approach%20to%20Comparing%20Synthetic%20and%20Real%20Imagery%0AAuthor%3A%20Danial%20Safaei%20and%20Siddartha%20Khastgir%20and%20Mohsen%20Alirezaei%20and%20Jeroen%20Ploeg%20and%20Son%20Tong%20and%20Xingyu%20Zhao%0AAbstract%3A%20Virtual%20testing%20using%20synthetic%20data%20has%20become%20a%20cornerstone%20of%20autonomous%20vehicle%20%28AV%29%20safety%20assurance.%20Despite%20progress%20in%20improving%20visual%20realism%20through%20advanced%20simulators%20and%20generative%20AI%2C%20recent%20studies%20reveal%20that%20pixel-level%20fidelity%20alone%20does%20not%20ensure%20reliable%20transfer%20from%20simulation%20to%20the%20real%20world.%20What%20truly%20matters%20is%20whether%20the%20system-under-test%20%28SUT%29%20bases%20its%20decisions%20on%20the%20same%20causal%20evidence%20in%20both%20real%20and%20simulated%20environments%20-%20not%20just%20whether%20images%20%22look%20real%22%20to%20humans.%20This%20paper%20addresses%20the%20lack%20of%20such%20a%20behavior-grounded%20fidelity%20measure%20by%20introducing%20Decisive%20Feature%20Fidelity%20%28DFF%29%2C%20a%20new%20SUT-specific%20metric%20that%20extends%20the%20existing%20fidelity%20spectrum%20to%20capture%20mechanism%20parity%20-%20the%20agreement%20in%20causal%20evidence%20underlying%20the%20SUT%27s%20decisions%20across%20domains.%20DFF%20leverages%20explainable-AI%20%28XAI%29%20methods%20to%20identify%20and%20compare%20the%20decisive%20features%20driving%20the%20SUT%27s%20outputs%20for%20matched%20real-synthetic%20pairs.%20We%20further%20propose%20practical%20estimators%20based%20on%20counterfactual%20explanations%2C%20along%20with%20a%20DFF-guided%20calibration%20scheme%20to%20enhance%20simulator%20fidelity.%20Experiments%20on%202126%20matched%20KITTI-VirtualKITTI2%20pairs%20demonstrate%20that%20DFF%20reveals%20discrepancies%20overlooked%20by%20conventional%20output-value%20fidelity.%20Furthermore%2C%20results%20show%20that%20DFF-guided%20calibration%20improves%20decisive-feature%20and%20input-level%20fidelity%20without%20sacrificing%20output%20value%20fidelity%20across%20diverse%20SUTs.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16468v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantifying%2520and%2520Bridging%2520the%2520Fidelity%2520Gap%253A%2520A%2520Decisive-Feature%2520Approach%2520to%2520Comparing%2520Synthetic%2520and%2520Real%2520Imagery%26entry.906535625%3DDanial%2520Safaei%2520and%2520Siddartha%2520Khastgir%2520and%2520Mohsen%2520Alirezaei%2520and%2520Jeroen%2520Ploeg%2520and%2520Son%2520Tong%2520and%2520Xingyu%2520Zhao%26entry.1292438233%3DVirtual%2520testing%2520using%2520synthetic%2520data%2520has%2520become%2520a%2520cornerstone%2520of%2520autonomous%2520vehicle%2520%2528AV%2529%2520safety%2520assurance.%2520Despite%2520progress%2520in%2520improving%2520visual%2520realism%2520through%2520advanced%2520simulators%2520and%2520generative%2520AI%252C%2520recent%2520studies%2520reveal%2520that%2520pixel-level%2520fidelity%2520alone%2520does%2520not%2520ensure%2520reliable%2520transfer%2520from%2520simulation%2520to%2520the%2520real%2520world.%2520What%2520truly%2520matters%2520is%2520whether%2520the%2520system-under-test%2520%2528SUT%2529%2520bases%2520its%2520decisions%2520on%2520the%2520same%2520causal%2520evidence%2520in%2520both%2520real%2520and%2520simulated%2520environments%2520-%2520not%2520just%2520whether%2520images%2520%2522look%2520real%2522%2520to%2520humans.%2520This%2520paper%2520addresses%2520the%2520lack%2520of%2520such%2520a%2520behavior-grounded%2520fidelity%2520measure%2520by%2520introducing%2520Decisive%2520Feature%2520Fidelity%2520%2528DFF%2529%252C%2520a%2520new%2520SUT-specific%2520metric%2520that%2520extends%2520the%2520existing%2520fidelity%2520spectrum%2520to%2520capture%2520mechanism%2520parity%2520-%2520the%2520agreement%2520in%2520causal%2520evidence%2520underlying%2520the%2520SUT%2527s%2520decisions%2520across%2520domains.%2520DFF%2520leverages%2520explainable-AI%2520%2528XAI%2529%2520methods%2520to%2520identify%2520and%2520compare%2520the%2520decisive%2520features%2520driving%2520the%2520SUT%2527s%2520outputs%2520for%2520matched%2520real-synthetic%2520pairs.%2520We%2520further%2520propose%2520practical%2520estimators%2520based%2520on%2520counterfactual%2520explanations%252C%2520along%2520with%2520a%2520DFF-guided%2520calibration%2520scheme%2520to%2520enhance%2520simulator%2520fidelity.%2520Experiments%2520on%25202126%2520matched%2520KITTI-VirtualKITTI2%2520pairs%2520demonstrate%2520that%2520DFF%2520reveals%2520discrepancies%2520overlooked%2520by%2520conventional%2520output-value%2520fidelity.%2520Furthermore%252C%2520results%2520show%2520that%2520DFF-guided%2520calibration%2520improves%2520decisive-feature%2520and%2520input-level%2520fidelity%2520without%2520sacrificing%2520output%2520value%2520fidelity%2520across%2520diverse%2520SUTs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16468v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20and%20Bridging%20the%20Fidelity%20Gap%3A%20A%20Decisive-Feature%20Approach%20to%20Comparing%20Synthetic%20and%20Real%20Imagery&entry.906535625=Danial%20Safaei%20and%20Siddartha%20Khastgir%20and%20Mohsen%20Alirezaei%20and%20Jeroen%20Ploeg%20and%20Son%20Tong%20and%20Xingyu%20Zhao&entry.1292438233=Virtual%20testing%20using%20synthetic%20data%20has%20become%20a%20cornerstone%20of%20autonomous%20vehicle%20%28AV%29%20safety%20assurance.%20Despite%20progress%20in%20improving%20visual%20realism%20through%20advanced%20simulators%20and%20generative%20AI%2C%20recent%20studies%20reveal%20that%20pixel-level%20fidelity%20alone%20does%20not%20ensure%20reliable%20transfer%20from%20simulation%20to%20the%20real%20world.%20What%20truly%20matters%20is%20whether%20the%20system-under-test%20%28SUT%29%20bases%20its%20decisions%20on%20the%20same%20causal%20evidence%20in%20both%20real%20and%20simulated%20environments%20-%20not%20just%20whether%20images%20%22look%20real%22%20to%20humans.%20This%20paper%20addresses%20the%20lack%20of%20such%20a%20behavior-grounded%20fidelity%20measure%20by%20introducing%20Decisive%20Feature%20Fidelity%20%28DFF%29%2C%20a%20new%20SUT-specific%20metric%20that%20extends%20the%20existing%20fidelity%20spectrum%20to%20capture%20mechanism%20parity%20-%20the%20agreement%20in%20causal%20evidence%20underlying%20the%20SUT%27s%20decisions%20across%20domains.%20DFF%20leverages%20explainable-AI%20%28XAI%29%20methods%20to%20identify%20and%20compare%20the%20decisive%20features%20driving%20the%20SUT%27s%20outputs%20for%20matched%20real-synthetic%20pairs.%20We%20further%20propose%20practical%20estimators%20based%20on%20counterfactual%20explanations%2C%20along%20with%20a%20DFF-guided%20calibration%20scheme%20to%20enhance%20simulator%20fidelity.%20Experiments%20on%202126%20matched%20KITTI-VirtualKITTI2%20pairs%20demonstrate%20that%20DFF%20reveals%20discrepancies%20overlooked%20by%20conventional%20output-value%20fidelity.%20Furthermore%2C%20results%20show%20that%20DFF-guided%20calibration%20improves%20decisive-feature%20and%20input-level%20fidelity%20without%20sacrificing%20output%20value%20fidelity%20across%20diverse%20SUTs.&entry.1838667208=http%3A//arxiv.org/abs/2512.16468v1&entry.124074799=Read"},
{"title": "RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing", "author": "Tianyuan Qu and Lei Ke and Xiaohang Zhan and Longxiang Tang and Yuqi Liu and Bohao Peng and Bei Yu and Dong Yu and Jiaya Jia", "abstract": "Instruction-based image editing enables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples a vision-language planner with a diffusion editor. The planner decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions; the editor then applies changes using a training-free attention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we apply GRPO-based reinforcement learning using 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improving regional precision and overall fidelity. Our project page: https://replan-iv-edit.github.io", "link": "http://arxiv.org/abs/2512.16864v1", "date": "2025-12-18", "relevancy": 2.6421, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5287}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5283}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RePlan%3A%20Reasoning-guided%20Region%20Planning%20for%20Complex%20Instruction-based%20Image%20Editing&body=Title%3A%20RePlan%3A%20Reasoning-guided%20Region%20Planning%20for%20Complex%20Instruction-based%20Image%20Editing%0AAuthor%3A%20Tianyuan%20Qu%20and%20Lei%20Ke%20and%20Xiaohang%20Zhan%20and%20Longxiang%20Tang%20and%20Yuqi%20Liu%20and%20Bohao%20Peng%20and%20Bei%20Yu%20and%20Dong%20Yu%20and%20Jiaya%20Jia%0AAbstract%3A%20Instruction-based%20image%20editing%20enables%20natural-language%20control%20over%20visual%20modifications%2C%20yet%20existing%20models%20falter%20under%20Instruction-Visual%20Complexity%20%28IV-Complexity%29%2C%20where%20intricate%20instructions%20meet%20cluttered%20or%20ambiguous%20scenes.%20We%20introduce%20RePlan%20%28Region-aligned%20Planning%29%2C%20a%20plan-then-execute%20framework%20that%20couples%20a%20vision-language%20planner%20with%20a%20diffusion%20editor.%20The%20planner%20decomposes%20instructions%20via%20step-by-step%20reasoning%20and%20explicitly%20grounds%20them%20to%20target%20regions%3B%20the%20editor%20then%20applies%20changes%20using%20a%20training-free%20attention-region%20injection%20mechanism%2C%20enabling%20precise%2C%20parallel%20multi-region%20edits%20without%20iterative%20inpainting.%20To%20strengthen%20planning%2C%20we%20apply%20GRPO-based%20reinforcement%20learning%20using%201K%20instruction-only%20examples%2C%20yielding%20substantial%20gains%20in%20reasoning%20fidelity%20and%20format%20reliability.%20We%20further%20present%20IV-Edit%2C%20a%20benchmark%20focused%20on%20fine-grained%20grounding%20and%20knowledge-intensive%20edits.%20Across%20IV-Complex%20settings%2C%20RePlan%20consistently%20outperforms%20strong%20baselines%20trained%20on%20far%20larger%20datasets%2C%20improving%20regional%20precision%20and%20overall%20fidelity.%20Our%20project%20page%3A%20https%3A//replan-iv-edit.github.io%0ALink%3A%20http%3A//arxiv.org/abs/2512.16864v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRePlan%253A%2520Reasoning-guided%2520Region%2520Planning%2520for%2520Complex%2520Instruction-based%2520Image%2520Editing%26entry.906535625%3DTianyuan%2520Qu%2520and%2520Lei%2520Ke%2520and%2520Xiaohang%2520Zhan%2520and%2520Longxiang%2520Tang%2520and%2520Yuqi%2520Liu%2520and%2520Bohao%2520Peng%2520and%2520Bei%2520Yu%2520and%2520Dong%2520Yu%2520and%2520Jiaya%2520Jia%26entry.1292438233%3DInstruction-based%2520image%2520editing%2520enables%2520natural-language%2520control%2520over%2520visual%2520modifications%252C%2520yet%2520existing%2520models%2520falter%2520under%2520Instruction-Visual%2520Complexity%2520%2528IV-Complexity%2529%252C%2520where%2520intricate%2520instructions%2520meet%2520cluttered%2520or%2520ambiguous%2520scenes.%2520We%2520introduce%2520RePlan%2520%2528Region-aligned%2520Planning%2529%252C%2520a%2520plan-then-execute%2520framework%2520that%2520couples%2520a%2520vision-language%2520planner%2520with%2520a%2520diffusion%2520editor.%2520The%2520planner%2520decomposes%2520instructions%2520via%2520step-by-step%2520reasoning%2520and%2520explicitly%2520grounds%2520them%2520to%2520target%2520regions%253B%2520the%2520editor%2520then%2520applies%2520changes%2520using%2520a%2520training-free%2520attention-region%2520injection%2520mechanism%252C%2520enabling%2520precise%252C%2520parallel%2520multi-region%2520edits%2520without%2520iterative%2520inpainting.%2520To%2520strengthen%2520planning%252C%2520we%2520apply%2520GRPO-based%2520reinforcement%2520learning%2520using%25201K%2520instruction-only%2520examples%252C%2520yielding%2520substantial%2520gains%2520in%2520reasoning%2520fidelity%2520and%2520format%2520reliability.%2520We%2520further%2520present%2520IV-Edit%252C%2520a%2520benchmark%2520focused%2520on%2520fine-grained%2520grounding%2520and%2520knowledge-intensive%2520edits.%2520Across%2520IV-Complex%2520settings%252C%2520RePlan%2520consistently%2520outperforms%2520strong%2520baselines%2520trained%2520on%2520far%2520larger%2520datasets%252C%2520improving%2520regional%2520precision%2520and%2520overall%2520fidelity.%2520Our%2520project%2520page%253A%2520https%253A//replan-iv-edit.github.io%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16864v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RePlan%3A%20Reasoning-guided%20Region%20Planning%20for%20Complex%20Instruction-based%20Image%20Editing&entry.906535625=Tianyuan%20Qu%20and%20Lei%20Ke%20and%20Xiaohang%20Zhan%20and%20Longxiang%20Tang%20and%20Yuqi%20Liu%20and%20Bohao%20Peng%20and%20Bei%20Yu%20and%20Dong%20Yu%20and%20Jiaya%20Jia&entry.1292438233=Instruction-based%20image%20editing%20enables%20natural-language%20control%20over%20visual%20modifications%2C%20yet%20existing%20models%20falter%20under%20Instruction-Visual%20Complexity%20%28IV-Complexity%29%2C%20where%20intricate%20instructions%20meet%20cluttered%20or%20ambiguous%20scenes.%20We%20introduce%20RePlan%20%28Region-aligned%20Planning%29%2C%20a%20plan-then-execute%20framework%20that%20couples%20a%20vision-language%20planner%20with%20a%20diffusion%20editor.%20The%20planner%20decomposes%20instructions%20via%20step-by-step%20reasoning%20and%20explicitly%20grounds%20them%20to%20target%20regions%3B%20the%20editor%20then%20applies%20changes%20using%20a%20training-free%20attention-region%20injection%20mechanism%2C%20enabling%20precise%2C%20parallel%20multi-region%20edits%20without%20iterative%20inpainting.%20To%20strengthen%20planning%2C%20we%20apply%20GRPO-based%20reinforcement%20learning%20using%201K%20instruction-only%20examples%2C%20yielding%20substantial%20gains%20in%20reasoning%20fidelity%20and%20format%20reliability.%20We%20further%20present%20IV-Edit%2C%20a%20benchmark%20focused%20on%20fine-grained%20grounding%20and%20knowledge-intensive%20edits.%20Across%20IV-Complex%20settings%2C%20RePlan%20consistently%20outperforms%20strong%20baselines%20trained%20on%20far%20larger%20datasets%2C%20improving%20regional%20precision%20and%20overall%20fidelity.%20Our%20project%20page%3A%20https%3A//replan-iv-edit.github.io&entry.1838667208=http%3A//arxiv.org/abs/2512.16864v1&entry.124074799=Read"},
{"title": "Kling-Omni Technical Report", "author": " Kling Team and Jialu Chen and Yuanzheng Ci and Xiangyu Du and Zipeng Feng and Kun Gai and Sainan Guo and Feng Han and Jingbin He and Kang He and Xiao Hu and Xiaohua Hu and Boyuan Jiang and Fangyuan Kong and Hang Li and Jie Li and Qingyu Li and Shen Li and Xiaohan Li and Yan Li and Jiajun Liang and Borui Liao and Yiqiao Liao and Weihong Lin and Quande Liu and Xiaokun Liu and Yilun Liu and Yuliang Liu and Shun Lu and Hangyu Mao and Yunyao Mao and Haodong Ouyang and Wenyu Qin and Wanqi Shi and Xiaoyu Shi and Lianghao Su and Haozhi Sun and Peiqin Sun and Pengfei Wan and Chao Wang and Chenyu Wang and Meng Wang and Qiulin Wang and Runqi Wang and Xintao Wang and Xuebo Wang and Zekun Wang and Min Wei and Tiancheng Wen and Guohao Wu and Xiaoshi Wu and Zhenhua Wu and Da Xie and Yingtong Xiong and Yulong Xu and Sile Yang and Zikang Yang and Weicai Ye and Ziyang Yuan and Shenglong Zhang and Shuaiyu Zhang and Yuanxing Zhang and Yufan Zhang and Wenzheng Zhao and Ruiliang Zhou and Yan Zhou and Guosheng Zhu and Yongjie Zhu", "abstract": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "link": "http://arxiv.org/abs/2512.16776v1", "date": "2025-12-18", "relevancy": 2.6398, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5297}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5297}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kling-Omni%20Technical%20Report&body=Title%3A%20Kling-Omni%20Technical%20Report%0AAuthor%3A%20%20Kling%20Team%20and%20Jialu%20Chen%20and%20Yuanzheng%20Ci%20and%20Xiangyu%20Du%20and%20Zipeng%20Feng%20and%20Kun%20Gai%20and%20Sainan%20Guo%20and%20Feng%20Han%20and%20Jingbin%20He%20and%20Kang%20He%20and%20Xiao%20Hu%20and%20Xiaohua%20Hu%20and%20Boyuan%20Jiang%20and%20Fangyuan%20Kong%20and%20Hang%20Li%20and%20Jie%20Li%20and%20Qingyu%20Li%20and%20Shen%20Li%20and%20Xiaohan%20Li%20and%20Yan%20Li%20and%20Jiajun%20Liang%20and%20Borui%20Liao%20and%20Yiqiao%20Liao%20and%20Weihong%20Lin%20and%20Quande%20Liu%20and%20Xiaokun%20Liu%20and%20Yilun%20Liu%20and%20Yuliang%20Liu%20and%20Shun%20Lu%20and%20Hangyu%20Mao%20and%20Yunyao%20Mao%20and%20Haodong%20Ouyang%20and%20Wenyu%20Qin%20and%20Wanqi%20Shi%20and%20Xiaoyu%20Shi%20and%20Lianghao%20Su%20and%20Haozhi%20Sun%20and%20Peiqin%20Sun%20and%20Pengfei%20Wan%20and%20Chao%20Wang%20and%20Chenyu%20Wang%20and%20Meng%20Wang%20and%20Qiulin%20Wang%20and%20Runqi%20Wang%20and%20Xintao%20Wang%20and%20Xuebo%20Wang%20and%20Zekun%20Wang%20and%20Min%20Wei%20and%20Tiancheng%20Wen%20and%20Guohao%20Wu%20and%20Xiaoshi%20Wu%20and%20Zhenhua%20Wu%20and%20Da%20Xie%20and%20Yingtong%20Xiong%20and%20Yulong%20Xu%20and%20Sile%20Yang%20and%20Zikang%20Yang%20and%20Weicai%20Ye%20and%20Ziyang%20Yuan%20and%20Shenglong%20Zhang%20and%20Shuaiyu%20Zhang%20and%20Yuanxing%20Zhang%20and%20Yufan%20Zhang%20and%20Wenzheng%20Zhao%20and%20Ruiliang%20Zhou%20and%20Yan%20Zhou%20and%20Guosheng%20Zhu%20and%20Yongjie%20Zhu%0AAbstract%3A%20We%20present%20Kling-Omni%2C%20a%20generalist%20generative%20framework%20designed%20to%20synthesize%20high-fidelity%20videos%20directly%20from%20multimodal%20visual%20language%20inputs.%20Adopting%20an%20end-to-end%20perspective%2C%20Kling-Omni%20bridges%20the%20functional%20separation%20among%20diverse%20video%20generation%2C%20editing%2C%20and%20intelligent%20reasoning%20tasks%2C%20integrating%20them%20into%20a%20holistic%20system.%20Unlike%20disjointed%20pipeline%20approaches%2C%20Kling-Omni%20supports%20a%20diverse%20range%20of%20user%20inputs%2C%20including%20text%20instructions%2C%20reference%20images%2C%20and%20video%20contexts%2C%20processing%20them%20into%20a%20unified%20multimodal%20representation%20to%20deliver%20cinematic-quality%20and%20highly-intelligent%20video%20content%20creation.%20To%20support%20these%20capabilities%2C%20we%20constructed%20a%20comprehensive%20data%20system%20that%20serves%20as%20the%20foundation%20for%20multimodal%20video%20creation.%20The%20framework%20is%20further%20empowered%20by%20efficient%20large-scale%20pre-training%20strategies%20and%20infrastructure%20optimizations%20for%20inference.%20Comprehensive%20evaluations%20reveal%20that%20Kling-Omni%20demonstrates%20exceptional%20capabilities%20in%20in-context%20generation%2C%20reasoning-based%20editing%2C%20and%20multimodal%20instruction%20following.%20Moving%20beyond%20a%20content%20creation%20tool%2C%20we%20believe%20Kling-Omni%20is%20a%20pivotal%20advancement%20toward%20multimodal%20world%20simulators%20capable%20of%20perceiving%2C%20reasoning%2C%20generating%20and%20interacting%20with%20the%20dynamic%20and%20complex%20worlds.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16776v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKling-Omni%2520Technical%2520Report%26entry.906535625%3D%2520Kling%2520Team%2520and%2520Jialu%2520Chen%2520and%2520Yuanzheng%2520Ci%2520and%2520Xiangyu%2520Du%2520and%2520Zipeng%2520Feng%2520and%2520Kun%2520Gai%2520and%2520Sainan%2520Guo%2520and%2520Feng%2520Han%2520and%2520Jingbin%2520He%2520and%2520Kang%2520He%2520and%2520Xiao%2520Hu%2520and%2520Xiaohua%2520Hu%2520and%2520Boyuan%2520Jiang%2520and%2520Fangyuan%2520Kong%2520and%2520Hang%2520Li%2520and%2520Jie%2520Li%2520and%2520Qingyu%2520Li%2520and%2520Shen%2520Li%2520and%2520Xiaohan%2520Li%2520and%2520Yan%2520Li%2520and%2520Jiajun%2520Liang%2520and%2520Borui%2520Liao%2520and%2520Yiqiao%2520Liao%2520and%2520Weihong%2520Lin%2520and%2520Quande%2520Liu%2520and%2520Xiaokun%2520Liu%2520and%2520Yilun%2520Liu%2520and%2520Yuliang%2520Liu%2520and%2520Shun%2520Lu%2520and%2520Hangyu%2520Mao%2520and%2520Yunyao%2520Mao%2520and%2520Haodong%2520Ouyang%2520and%2520Wenyu%2520Qin%2520and%2520Wanqi%2520Shi%2520and%2520Xiaoyu%2520Shi%2520and%2520Lianghao%2520Su%2520and%2520Haozhi%2520Sun%2520and%2520Peiqin%2520Sun%2520and%2520Pengfei%2520Wan%2520and%2520Chao%2520Wang%2520and%2520Chenyu%2520Wang%2520and%2520Meng%2520Wang%2520and%2520Qiulin%2520Wang%2520and%2520Runqi%2520Wang%2520and%2520Xintao%2520Wang%2520and%2520Xuebo%2520Wang%2520and%2520Zekun%2520Wang%2520and%2520Min%2520Wei%2520and%2520Tiancheng%2520Wen%2520and%2520Guohao%2520Wu%2520and%2520Xiaoshi%2520Wu%2520and%2520Zhenhua%2520Wu%2520and%2520Da%2520Xie%2520and%2520Yingtong%2520Xiong%2520and%2520Yulong%2520Xu%2520and%2520Sile%2520Yang%2520and%2520Zikang%2520Yang%2520and%2520Weicai%2520Ye%2520and%2520Ziyang%2520Yuan%2520and%2520Shenglong%2520Zhang%2520and%2520Shuaiyu%2520Zhang%2520and%2520Yuanxing%2520Zhang%2520and%2520Yufan%2520Zhang%2520and%2520Wenzheng%2520Zhao%2520and%2520Ruiliang%2520Zhou%2520and%2520Yan%2520Zhou%2520and%2520Guosheng%2520Zhu%2520and%2520Yongjie%2520Zhu%26entry.1292438233%3DWe%2520present%2520Kling-Omni%252C%2520a%2520generalist%2520generative%2520framework%2520designed%2520to%2520synthesize%2520high-fidelity%2520videos%2520directly%2520from%2520multimodal%2520visual%2520language%2520inputs.%2520Adopting%2520an%2520end-to-end%2520perspective%252C%2520Kling-Omni%2520bridges%2520the%2520functional%2520separation%2520among%2520diverse%2520video%2520generation%252C%2520editing%252C%2520and%2520intelligent%2520reasoning%2520tasks%252C%2520integrating%2520them%2520into%2520a%2520holistic%2520system.%2520Unlike%2520disjointed%2520pipeline%2520approaches%252C%2520Kling-Omni%2520supports%2520a%2520diverse%2520range%2520of%2520user%2520inputs%252C%2520including%2520text%2520instructions%252C%2520reference%2520images%252C%2520and%2520video%2520contexts%252C%2520processing%2520them%2520into%2520a%2520unified%2520multimodal%2520representation%2520to%2520deliver%2520cinematic-quality%2520and%2520highly-intelligent%2520video%2520content%2520creation.%2520To%2520support%2520these%2520capabilities%252C%2520we%2520constructed%2520a%2520comprehensive%2520data%2520system%2520that%2520serves%2520as%2520the%2520foundation%2520for%2520multimodal%2520video%2520creation.%2520The%2520framework%2520is%2520further%2520empowered%2520by%2520efficient%2520large-scale%2520pre-training%2520strategies%2520and%2520infrastructure%2520optimizations%2520for%2520inference.%2520Comprehensive%2520evaluations%2520reveal%2520that%2520Kling-Omni%2520demonstrates%2520exceptional%2520capabilities%2520in%2520in-context%2520generation%252C%2520reasoning-based%2520editing%252C%2520and%2520multimodal%2520instruction%2520following.%2520Moving%2520beyond%2520a%2520content%2520creation%2520tool%252C%2520we%2520believe%2520Kling-Omni%2520is%2520a%2520pivotal%2520advancement%2520toward%2520multimodal%2520world%2520simulators%2520capable%2520of%2520perceiving%252C%2520reasoning%252C%2520generating%2520and%2520interacting%2520with%2520the%2520dynamic%2520and%2520complex%2520worlds.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16776v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kling-Omni%20Technical%20Report&entry.906535625=%20Kling%20Team%20and%20Jialu%20Chen%20and%20Yuanzheng%20Ci%20and%20Xiangyu%20Du%20and%20Zipeng%20Feng%20and%20Kun%20Gai%20and%20Sainan%20Guo%20and%20Feng%20Han%20and%20Jingbin%20He%20and%20Kang%20He%20and%20Xiao%20Hu%20and%20Xiaohua%20Hu%20and%20Boyuan%20Jiang%20and%20Fangyuan%20Kong%20and%20Hang%20Li%20and%20Jie%20Li%20and%20Qingyu%20Li%20and%20Shen%20Li%20and%20Xiaohan%20Li%20and%20Yan%20Li%20and%20Jiajun%20Liang%20and%20Borui%20Liao%20and%20Yiqiao%20Liao%20and%20Weihong%20Lin%20and%20Quande%20Liu%20and%20Xiaokun%20Liu%20and%20Yilun%20Liu%20and%20Yuliang%20Liu%20and%20Shun%20Lu%20and%20Hangyu%20Mao%20and%20Yunyao%20Mao%20and%20Haodong%20Ouyang%20and%20Wenyu%20Qin%20and%20Wanqi%20Shi%20and%20Xiaoyu%20Shi%20and%20Lianghao%20Su%20and%20Haozhi%20Sun%20and%20Peiqin%20Sun%20and%20Pengfei%20Wan%20and%20Chao%20Wang%20and%20Chenyu%20Wang%20and%20Meng%20Wang%20and%20Qiulin%20Wang%20and%20Runqi%20Wang%20and%20Xintao%20Wang%20and%20Xuebo%20Wang%20and%20Zekun%20Wang%20and%20Min%20Wei%20and%20Tiancheng%20Wen%20and%20Guohao%20Wu%20and%20Xiaoshi%20Wu%20and%20Zhenhua%20Wu%20and%20Da%20Xie%20and%20Yingtong%20Xiong%20and%20Yulong%20Xu%20and%20Sile%20Yang%20and%20Zikang%20Yang%20and%20Weicai%20Ye%20and%20Ziyang%20Yuan%20and%20Shenglong%20Zhang%20and%20Shuaiyu%20Zhang%20and%20Yuanxing%20Zhang%20and%20Yufan%20Zhang%20and%20Wenzheng%20Zhao%20and%20Ruiliang%20Zhou%20and%20Yan%20Zhou%20and%20Guosheng%20Zhu%20and%20Yongjie%20Zhu&entry.1292438233=We%20present%20Kling-Omni%2C%20a%20generalist%20generative%20framework%20designed%20to%20synthesize%20high-fidelity%20videos%20directly%20from%20multimodal%20visual%20language%20inputs.%20Adopting%20an%20end-to-end%20perspective%2C%20Kling-Omni%20bridges%20the%20functional%20separation%20among%20diverse%20video%20generation%2C%20editing%2C%20and%20intelligent%20reasoning%20tasks%2C%20integrating%20them%20into%20a%20holistic%20system.%20Unlike%20disjointed%20pipeline%20approaches%2C%20Kling-Omni%20supports%20a%20diverse%20range%20of%20user%20inputs%2C%20including%20text%20instructions%2C%20reference%20images%2C%20and%20video%20contexts%2C%20processing%20them%20into%20a%20unified%20multimodal%20representation%20to%20deliver%20cinematic-quality%20and%20highly-intelligent%20video%20content%20creation.%20To%20support%20these%20capabilities%2C%20we%20constructed%20a%20comprehensive%20data%20system%20that%20serves%20as%20the%20foundation%20for%20multimodal%20video%20creation.%20The%20framework%20is%20further%20empowered%20by%20efficient%20large-scale%20pre-training%20strategies%20and%20infrastructure%20optimizations%20for%20inference.%20Comprehensive%20evaluations%20reveal%20that%20Kling-Omni%20demonstrates%20exceptional%20capabilities%20in%20in-context%20generation%2C%20reasoning-based%20editing%2C%20and%20multimodal%20instruction%20following.%20Moving%20beyond%20a%20content%20creation%20tool%2C%20we%20believe%20Kling-Omni%20is%20a%20pivotal%20advancement%20toward%20multimodal%20world%20simulators%20capable%20of%20perceiving%2C%20reasoning%2C%20generating%20and%20interacting%20with%20the%20dynamic%20and%20complex%20worlds.&entry.1838667208=http%3A//arxiv.org/abs/2512.16776v1&entry.124074799=Read"},
{"title": "Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image", "author": "Yushi Hu and Reyhane Askari-Hemmat and Melissa Hall and Emily Dinan and Luke Zettlemoyer and Marjan Ghazvininejad", "abstract": "Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning (\"thinking-with-images\"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.", "link": "http://arxiv.org/abs/2512.16899v1", "date": "2025-12-18", "relevancy": 2.6373, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5297}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5264}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20RewardBench%202%3A%20Evaluating%20Omni%20Reward%20Models%20for%20Interleaved%20Text%20and%20Image&body=Title%3A%20Multimodal%20RewardBench%202%3A%20Evaluating%20Omni%20Reward%20Models%20for%20Interleaved%20Text%20and%20Image%0AAuthor%3A%20Yushi%20Hu%20and%20Reyhane%20Askari-Hemmat%20and%20Melissa%20Hall%20and%20Emily%20Dinan%20and%20Luke%20Zettlemoyer%20and%20Marjan%20Ghazvininejad%0AAbstract%3A%20Reward%20models%20%28RMs%29%20are%20essential%20for%20training%20large%20language%20models%20%28LLMs%29%2C%20but%20remain%20underexplored%20for%20omni%20models%20that%20handle%20interleaved%20image%20and%20text%20sequences.%20We%20introduce%20Multimodal%20RewardBench%202%20%28MMRB2%29%2C%20the%20first%20comprehensive%20benchmark%20for%20reward%20models%20on%20multimodal%20understanding%20and%20%28interleaved%29%20generation.%20MMRB2%20spans%20four%20tasks%3A%20text-to-image%2C%20image%20editing%2C%20interleaved%20generation%2C%20and%20multimodal%20reasoning%20%28%22thinking-with-images%22%29%2C%20providing%201%2C000%20expert-annotated%20preference%20pairs%20per%20task%20from%2023%20models%20and%20agents%20across%2021%20source%20tasks.%20MMRB2%20is%20designed%20with%3A%20%281%29%20practical%20but%20challenging%20prompts%3B%20%282%29%20responses%20from%20state-of-the-art%20models%20and%20agents%3B%20and%20%283%29%20preference%20pairs%20with%20strong%20human-expert%20consensus%2C%20curated%20via%20an%20ensemble%20filtering%20strategy.%20Using%20MMRB2%2C%20we%20study%20existing%20judges%20for%20each%20subtask%2C%20including%20multimodal%20LLM-as-a-judge%20and%20models%20trained%20with%20human%20preferences.%20The%20latest%20Gemini%203%20Pro%20attains%2075-80%25%20accuracy.%20GPT-5%20and%20Gemini%202.5%20Pro%20reach%2066-75%25%20accuracy%2C%20compared%20to%20%3E90%25%20for%20humans%2C%20yet%20surpass%20the%20widely%20used%20GPT-4o%20%2859%25%29.%20The%20best%20performing%20open-source%20model%20Qwen3-VL-32B%20achieves%20similar%20accuracies%20as%20Gemini%202.5%20Flash%20%2864%25%29.%20We%20also%20show%20that%20MMRB2%20performance%20strongly%20correlates%20with%20downstream%20task%20success%20using%20Best-of-N%20sampling%20and%20conduct%20an%20in-depth%20analysis%20that%20shows%20key%20areas%20to%20improve%20the%20reward%20models%20going%20forward.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16899v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520RewardBench%25202%253A%2520Evaluating%2520Omni%2520Reward%2520Models%2520for%2520Interleaved%2520Text%2520and%2520Image%26entry.906535625%3DYushi%2520Hu%2520and%2520Reyhane%2520Askari-Hemmat%2520and%2520Melissa%2520Hall%2520and%2520Emily%2520Dinan%2520and%2520Luke%2520Zettlemoyer%2520and%2520Marjan%2520Ghazvininejad%26entry.1292438233%3DReward%2520models%2520%2528RMs%2529%2520are%2520essential%2520for%2520training%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520but%2520remain%2520underexplored%2520for%2520omni%2520models%2520that%2520handle%2520interleaved%2520image%2520and%2520text%2520sequences.%2520We%2520introduce%2520Multimodal%2520RewardBench%25202%2520%2528MMRB2%2529%252C%2520the%2520first%2520comprehensive%2520benchmark%2520for%2520reward%2520models%2520on%2520multimodal%2520understanding%2520and%2520%2528interleaved%2529%2520generation.%2520MMRB2%2520spans%2520four%2520tasks%253A%2520text-to-image%252C%2520image%2520editing%252C%2520interleaved%2520generation%252C%2520and%2520multimodal%2520reasoning%2520%2528%2522thinking-with-images%2522%2529%252C%2520providing%25201%252C000%2520expert-annotated%2520preference%2520pairs%2520per%2520task%2520from%252023%2520models%2520and%2520agents%2520across%252021%2520source%2520tasks.%2520MMRB2%2520is%2520designed%2520with%253A%2520%25281%2529%2520practical%2520but%2520challenging%2520prompts%253B%2520%25282%2529%2520responses%2520from%2520state-of-the-art%2520models%2520and%2520agents%253B%2520and%2520%25283%2529%2520preference%2520pairs%2520with%2520strong%2520human-expert%2520consensus%252C%2520curated%2520via%2520an%2520ensemble%2520filtering%2520strategy.%2520Using%2520MMRB2%252C%2520we%2520study%2520existing%2520judges%2520for%2520each%2520subtask%252C%2520including%2520multimodal%2520LLM-as-a-judge%2520and%2520models%2520trained%2520with%2520human%2520preferences.%2520The%2520latest%2520Gemini%25203%2520Pro%2520attains%252075-80%2525%2520accuracy.%2520GPT-5%2520and%2520Gemini%25202.5%2520Pro%2520reach%252066-75%2525%2520accuracy%252C%2520compared%2520to%2520%253E90%2525%2520for%2520humans%252C%2520yet%2520surpass%2520the%2520widely%2520used%2520GPT-4o%2520%252859%2525%2529.%2520The%2520best%2520performing%2520open-source%2520model%2520Qwen3-VL-32B%2520achieves%2520similar%2520accuracies%2520as%2520Gemini%25202.5%2520Flash%2520%252864%2525%2529.%2520We%2520also%2520show%2520that%2520MMRB2%2520performance%2520strongly%2520correlates%2520with%2520downstream%2520task%2520success%2520using%2520Best-of-N%2520sampling%2520and%2520conduct%2520an%2520in-depth%2520analysis%2520that%2520shows%2520key%2520areas%2520to%2520improve%2520the%2520reward%2520models%2520going%2520forward.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16899v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20RewardBench%202%3A%20Evaluating%20Omni%20Reward%20Models%20for%20Interleaved%20Text%20and%20Image&entry.906535625=Yushi%20Hu%20and%20Reyhane%20Askari-Hemmat%20and%20Melissa%20Hall%20and%20Emily%20Dinan%20and%20Luke%20Zettlemoyer%20and%20Marjan%20Ghazvininejad&entry.1292438233=Reward%20models%20%28RMs%29%20are%20essential%20for%20training%20large%20language%20models%20%28LLMs%29%2C%20but%20remain%20underexplored%20for%20omni%20models%20that%20handle%20interleaved%20image%20and%20text%20sequences.%20We%20introduce%20Multimodal%20RewardBench%202%20%28MMRB2%29%2C%20the%20first%20comprehensive%20benchmark%20for%20reward%20models%20on%20multimodal%20understanding%20and%20%28interleaved%29%20generation.%20MMRB2%20spans%20four%20tasks%3A%20text-to-image%2C%20image%20editing%2C%20interleaved%20generation%2C%20and%20multimodal%20reasoning%20%28%22thinking-with-images%22%29%2C%20providing%201%2C000%20expert-annotated%20preference%20pairs%20per%20task%20from%2023%20models%20and%20agents%20across%2021%20source%20tasks.%20MMRB2%20is%20designed%20with%3A%20%281%29%20practical%20but%20challenging%20prompts%3B%20%282%29%20responses%20from%20state-of-the-art%20models%20and%20agents%3B%20and%20%283%29%20preference%20pairs%20with%20strong%20human-expert%20consensus%2C%20curated%20via%20an%20ensemble%20filtering%20strategy.%20Using%20MMRB2%2C%20we%20study%20existing%20judges%20for%20each%20subtask%2C%20including%20multimodal%20LLM-as-a-judge%20and%20models%20trained%20with%20human%20preferences.%20The%20latest%20Gemini%203%20Pro%20attains%2075-80%25%20accuracy.%20GPT-5%20and%20Gemini%202.5%20Pro%20reach%2066-75%25%20accuracy%2C%20compared%20to%20%3E90%25%20for%20humans%2C%20yet%20surpass%20the%20widely%20used%20GPT-4o%20%2859%25%29.%20The%20best%20performing%20open-source%20model%20Qwen3-VL-32B%20achieves%20similar%20accuracies%20as%20Gemini%202.5%20Flash%20%2864%25%29.%20We%20also%20show%20that%20MMRB2%20performance%20strongly%20correlates%20with%20downstream%20task%20success%20using%20Best-of-N%20sampling%20and%20conduct%20an%20in-depth%20analysis%20that%20shows%20key%20areas%20to%20improve%20the%20reward%20models%20going%20forward.&entry.1838667208=http%3A//arxiv.org/abs/2512.16899v1&entry.124074799=Read"},
{"title": "Guiding Perception-Reasoning Closer to Human in Blind Image Quality Assessment", "author": "Yuan Li and Yahan Yu and Youyuan Lin and Yong-Hao Yang and Chenhui Chu and Shin'ya Nishida", "abstract": "Humans assess image quality through a perception-reasoning cascade, integrating sensory cues with implicit reasoning to form self-consistent judgments. In this work, we investigate how a model can acquire both human-like and self-consistent reasoning capability for blind image quality assessment (BIQA). We first collect human evaluation data that capture several aspects of human perception-reasoning pipeline. Then, we adopt reinforcement learning, using human annotations as reward signals to guide the model toward human-like perception and reasoning. To enable the model to internalize self-consistent reasoning capability, we design a reward that drives the model to infer the image quality purely from self-generated descriptions. Empirically, our approach achieves score prediction performance comparable to state-of-the-art BIQA systems under general metrics, including Pearson and Spearman correlation coefficients. In addition to the rating score, we assess human-model alignment using ROUGE-1 to measure the similarity between model-generated and human perception-reasoning chains. On over 1,000 human-annotated samples, our model reaches a ROUGE-1 score of 0.512 (cf. 0.443 for baseline), indicating substantial coverage of human explanations and marking a step toward human-like interpretable reasoning in BIQA.", "link": "http://arxiv.org/abs/2512.16484v1", "date": "2025-12-18", "relevancy": 2.6358, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5276}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5276}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guiding%20Perception-Reasoning%20Closer%20to%20Human%20in%20Blind%20Image%20Quality%20Assessment&body=Title%3A%20Guiding%20Perception-Reasoning%20Closer%20to%20Human%20in%20Blind%20Image%20Quality%20Assessment%0AAuthor%3A%20Yuan%20Li%20and%20Yahan%20Yu%20and%20Youyuan%20Lin%20and%20Yong-Hao%20Yang%20and%20Chenhui%20Chu%20and%20Shin%27ya%20Nishida%0AAbstract%3A%20Humans%20assess%20image%20quality%20through%20a%20perception-reasoning%20cascade%2C%20integrating%20sensory%20cues%20with%20implicit%20reasoning%20to%20form%20self-consistent%20judgments.%20In%20this%20work%2C%20we%20investigate%20how%20a%20model%20can%20acquire%20both%20human-like%20and%20self-consistent%20reasoning%20capability%20for%20blind%20image%20quality%20assessment%20%28BIQA%29.%20We%20first%20collect%20human%20evaluation%20data%20that%20capture%20several%20aspects%20of%20human%20perception-reasoning%20pipeline.%20Then%2C%20we%20adopt%20reinforcement%20learning%2C%20using%20human%20annotations%20as%20reward%20signals%20to%20guide%20the%20model%20toward%20human-like%20perception%20and%20reasoning.%20To%20enable%20the%20model%20to%20internalize%20self-consistent%20reasoning%20capability%2C%20we%20design%20a%20reward%20that%20drives%20the%20model%20to%20infer%20the%20image%20quality%20purely%20from%20self-generated%20descriptions.%20Empirically%2C%20our%20approach%20achieves%20score%20prediction%20performance%20comparable%20to%20state-of-the-art%20BIQA%20systems%20under%20general%20metrics%2C%20including%20Pearson%20and%20Spearman%20correlation%20coefficients.%20In%20addition%20to%20the%20rating%20score%2C%20we%20assess%20human-model%20alignment%20using%20ROUGE-1%20to%20measure%20the%20similarity%20between%20model-generated%20and%20human%20perception-reasoning%20chains.%20On%20over%201%2C000%20human-annotated%20samples%2C%20our%20model%20reaches%20a%20ROUGE-1%20score%20of%200.512%20%28cf.%200.443%20for%20baseline%29%2C%20indicating%20substantial%20coverage%20of%20human%20explanations%20and%20marking%20a%20step%20toward%20human-like%20interpretable%20reasoning%20in%20BIQA.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16484v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuiding%2520Perception-Reasoning%2520Closer%2520to%2520Human%2520in%2520Blind%2520Image%2520Quality%2520Assessment%26entry.906535625%3DYuan%2520Li%2520and%2520Yahan%2520Yu%2520and%2520Youyuan%2520Lin%2520and%2520Yong-Hao%2520Yang%2520and%2520Chenhui%2520Chu%2520and%2520Shin%2527ya%2520Nishida%26entry.1292438233%3DHumans%2520assess%2520image%2520quality%2520through%2520a%2520perception-reasoning%2520cascade%252C%2520integrating%2520sensory%2520cues%2520with%2520implicit%2520reasoning%2520to%2520form%2520self-consistent%2520judgments.%2520In%2520this%2520work%252C%2520we%2520investigate%2520how%2520a%2520model%2520can%2520acquire%2520both%2520human-like%2520and%2520self-consistent%2520reasoning%2520capability%2520for%2520blind%2520image%2520quality%2520assessment%2520%2528BIQA%2529.%2520We%2520first%2520collect%2520human%2520evaluation%2520data%2520that%2520capture%2520several%2520aspects%2520of%2520human%2520perception-reasoning%2520pipeline.%2520Then%252C%2520we%2520adopt%2520reinforcement%2520learning%252C%2520using%2520human%2520annotations%2520as%2520reward%2520signals%2520to%2520guide%2520the%2520model%2520toward%2520human-like%2520perception%2520and%2520reasoning.%2520To%2520enable%2520the%2520model%2520to%2520internalize%2520self-consistent%2520reasoning%2520capability%252C%2520we%2520design%2520a%2520reward%2520that%2520drives%2520the%2520model%2520to%2520infer%2520the%2520image%2520quality%2520purely%2520from%2520self-generated%2520descriptions.%2520Empirically%252C%2520our%2520approach%2520achieves%2520score%2520prediction%2520performance%2520comparable%2520to%2520state-of-the-art%2520BIQA%2520systems%2520under%2520general%2520metrics%252C%2520including%2520Pearson%2520and%2520Spearman%2520correlation%2520coefficients.%2520In%2520addition%2520to%2520the%2520rating%2520score%252C%2520we%2520assess%2520human-model%2520alignment%2520using%2520ROUGE-1%2520to%2520measure%2520the%2520similarity%2520between%2520model-generated%2520and%2520human%2520perception-reasoning%2520chains.%2520On%2520over%25201%252C000%2520human-annotated%2520samples%252C%2520our%2520model%2520reaches%2520a%2520ROUGE-1%2520score%2520of%25200.512%2520%2528cf.%25200.443%2520for%2520baseline%2529%252C%2520indicating%2520substantial%2520coverage%2520of%2520human%2520explanations%2520and%2520marking%2520a%2520step%2520toward%2520human-like%2520interpretable%2520reasoning%2520in%2520BIQA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16484v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guiding%20Perception-Reasoning%20Closer%20to%20Human%20in%20Blind%20Image%20Quality%20Assessment&entry.906535625=Yuan%20Li%20and%20Yahan%20Yu%20and%20Youyuan%20Lin%20and%20Yong-Hao%20Yang%20and%20Chenhui%20Chu%20and%20Shin%27ya%20Nishida&entry.1292438233=Humans%20assess%20image%20quality%20through%20a%20perception-reasoning%20cascade%2C%20integrating%20sensory%20cues%20with%20implicit%20reasoning%20to%20form%20self-consistent%20judgments.%20In%20this%20work%2C%20we%20investigate%20how%20a%20model%20can%20acquire%20both%20human-like%20and%20self-consistent%20reasoning%20capability%20for%20blind%20image%20quality%20assessment%20%28BIQA%29.%20We%20first%20collect%20human%20evaluation%20data%20that%20capture%20several%20aspects%20of%20human%20perception-reasoning%20pipeline.%20Then%2C%20we%20adopt%20reinforcement%20learning%2C%20using%20human%20annotations%20as%20reward%20signals%20to%20guide%20the%20model%20toward%20human-like%20perception%20and%20reasoning.%20To%20enable%20the%20model%20to%20internalize%20self-consistent%20reasoning%20capability%2C%20we%20design%20a%20reward%20that%20drives%20the%20model%20to%20infer%20the%20image%20quality%20purely%20from%20self-generated%20descriptions.%20Empirically%2C%20our%20approach%20achieves%20score%20prediction%20performance%20comparable%20to%20state-of-the-art%20BIQA%20systems%20under%20general%20metrics%2C%20including%20Pearson%20and%20Spearman%20correlation%20coefficients.%20In%20addition%20to%20the%20rating%20score%2C%20we%20assess%20human-model%20alignment%20using%20ROUGE-1%20to%20measure%20the%20similarity%20between%20model-generated%20and%20human%20perception-reasoning%20chains.%20On%20over%201%2C000%20human-annotated%20samples%2C%20our%20model%20reaches%20a%20ROUGE-1%20score%20of%200.512%20%28cf.%200.443%20for%20baseline%29%2C%20indicating%20substantial%20coverage%20of%20human%20explanations%20and%20marking%20a%20step%20toward%20human-like%20interpretable%20reasoning%20in%20BIQA.&entry.1838667208=http%3A//arxiv.org/abs/2512.16484v1&entry.124074799=Read"},
{"title": "SARMAE: Masked Autoencoder for SAR Representation Learning", "author": "Danxu Liu and Di Wang and Hebaixu Wang and Haoyang Chen and Wentao Jiang and Yilin Cheng and Haonan Guo and Wei Cui and Jing Zhang", "abstract": "Synthetic Aperture Radar (SAR) imagery plays a critical role in all-weather, day-and-night remote sensing applications. However, existing SAR-oriented deep learning is constrained by data scarcity, while the physically grounded speckle noise in SAR imagery further hampers fine-grained semantic representation learning. To address these challenges, we propose SARMAE, a Noise-Aware Masked Autoencoder for self-supervised SAR representation learning. Specifically, we construct SAR-1M, the first million-scale SAR dataset, with additional paired optical images, to enable large-scale pre-training. Building upon this, we design Speckle-Aware Representation Enhancement (SARE), which injects SAR-specific speckle noise into masked autoencoders to facilitate noise-aware and robust representation learning. Furthermore, we introduce Semantic Anchor Representation Constraint (SARC), which leverages paired optical priors to align SAR features and ensure semantic consistency. Extensive experiments across multiple SAR datasets demonstrate that SARMAE achieves state-of-the-art performance on classification, detection, and segmentation tasks. Code and models will be available at https://github.com/MiliLab/SARMAE.", "link": "http://arxiv.org/abs/2512.16635v1", "date": "2025-12-18", "relevancy": 2.6131, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5593}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5167}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4919}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SARMAE%3A%20Masked%20Autoencoder%20for%20SAR%20Representation%20Learning&body=Title%3A%20SARMAE%3A%20Masked%20Autoencoder%20for%20SAR%20Representation%20Learning%0AAuthor%3A%20Danxu%20Liu%20and%20Di%20Wang%20and%20Hebaixu%20Wang%20and%20Haoyang%20Chen%20and%20Wentao%20Jiang%20and%20Yilin%20Cheng%20and%20Haonan%20Guo%20and%20Wei%20Cui%20and%20Jing%20Zhang%0AAbstract%3A%20Synthetic%20Aperture%20Radar%20%28SAR%29%20imagery%20plays%20a%20critical%20role%20in%20all-weather%2C%20day-and-night%20remote%20sensing%20applications.%20However%2C%20existing%20SAR-oriented%20deep%20learning%20is%20constrained%20by%20data%20scarcity%2C%20while%20the%20physically%20grounded%20speckle%20noise%20in%20SAR%20imagery%20further%20hampers%20fine-grained%20semantic%20representation%20learning.%20To%20address%20these%20challenges%2C%20we%20propose%20SARMAE%2C%20a%20Noise-Aware%20Masked%20Autoencoder%20for%20self-supervised%20SAR%20representation%20learning.%20Specifically%2C%20we%20construct%20SAR-1M%2C%20the%20first%20million-scale%20SAR%20dataset%2C%20with%20additional%20paired%20optical%20images%2C%20to%20enable%20large-scale%20pre-training.%20Building%20upon%20this%2C%20we%20design%20Speckle-Aware%20Representation%20Enhancement%20%28SARE%29%2C%20which%20injects%20SAR-specific%20speckle%20noise%20into%20masked%20autoencoders%20to%20facilitate%20noise-aware%20and%20robust%20representation%20learning.%20Furthermore%2C%20we%20introduce%20Semantic%20Anchor%20Representation%20Constraint%20%28SARC%29%2C%20which%20leverages%20paired%20optical%20priors%20to%20align%20SAR%20features%20and%20ensure%20semantic%20consistency.%20Extensive%20experiments%20across%20multiple%20SAR%20datasets%20demonstrate%20that%20SARMAE%20achieves%20state-of-the-art%20performance%20on%20classification%2C%20detection%2C%20and%20segmentation%20tasks.%20Code%20and%20models%20will%20be%20available%20at%20https%3A//github.com/MiliLab/SARMAE.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSARMAE%253A%2520Masked%2520Autoencoder%2520for%2520SAR%2520Representation%2520Learning%26entry.906535625%3DDanxu%2520Liu%2520and%2520Di%2520Wang%2520and%2520Hebaixu%2520Wang%2520and%2520Haoyang%2520Chen%2520and%2520Wentao%2520Jiang%2520and%2520Yilin%2520Cheng%2520and%2520Haonan%2520Guo%2520and%2520Wei%2520Cui%2520and%2520Jing%2520Zhang%26entry.1292438233%3DSynthetic%2520Aperture%2520Radar%2520%2528SAR%2529%2520imagery%2520plays%2520a%2520critical%2520role%2520in%2520all-weather%252C%2520day-and-night%2520remote%2520sensing%2520applications.%2520However%252C%2520existing%2520SAR-oriented%2520deep%2520learning%2520is%2520constrained%2520by%2520data%2520scarcity%252C%2520while%2520the%2520physically%2520grounded%2520speckle%2520noise%2520in%2520SAR%2520imagery%2520further%2520hampers%2520fine-grained%2520semantic%2520representation%2520learning.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520SARMAE%252C%2520a%2520Noise-Aware%2520Masked%2520Autoencoder%2520for%2520self-supervised%2520SAR%2520representation%2520learning.%2520Specifically%252C%2520we%2520construct%2520SAR-1M%252C%2520the%2520first%2520million-scale%2520SAR%2520dataset%252C%2520with%2520additional%2520paired%2520optical%2520images%252C%2520to%2520enable%2520large-scale%2520pre-training.%2520Building%2520upon%2520this%252C%2520we%2520design%2520Speckle-Aware%2520Representation%2520Enhancement%2520%2528SARE%2529%252C%2520which%2520injects%2520SAR-specific%2520speckle%2520noise%2520into%2520masked%2520autoencoders%2520to%2520facilitate%2520noise-aware%2520and%2520robust%2520representation%2520learning.%2520Furthermore%252C%2520we%2520introduce%2520Semantic%2520Anchor%2520Representation%2520Constraint%2520%2528SARC%2529%252C%2520which%2520leverages%2520paired%2520optical%2520priors%2520to%2520align%2520SAR%2520features%2520and%2520ensure%2520semantic%2520consistency.%2520Extensive%2520experiments%2520across%2520multiple%2520SAR%2520datasets%2520demonstrate%2520that%2520SARMAE%2520achieves%2520state-of-the-art%2520performance%2520on%2520classification%252C%2520detection%252C%2520and%2520segmentation%2520tasks.%2520Code%2520and%2520models%2520will%2520be%2520available%2520at%2520https%253A//github.com/MiliLab/SARMAE.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SARMAE%3A%20Masked%20Autoencoder%20for%20SAR%20Representation%20Learning&entry.906535625=Danxu%20Liu%20and%20Di%20Wang%20and%20Hebaixu%20Wang%20and%20Haoyang%20Chen%20and%20Wentao%20Jiang%20and%20Yilin%20Cheng%20and%20Haonan%20Guo%20and%20Wei%20Cui%20and%20Jing%20Zhang&entry.1292438233=Synthetic%20Aperture%20Radar%20%28SAR%29%20imagery%20plays%20a%20critical%20role%20in%20all-weather%2C%20day-and-night%20remote%20sensing%20applications.%20However%2C%20existing%20SAR-oriented%20deep%20learning%20is%20constrained%20by%20data%20scarcity%2C%20while%20the%20physically%20grounded%20speckle%20noise%20in%20SAR%20imagery%20further%20hampers%20fine-grained%20semantic%20representation%20learning.%20To%20address%20these%20challenges%2C%20we%20propose%20SARMAE%2C%20a%20Noise-Aware%20Masked%20Autoencoder%20for%20self-supervised%20SAR%20representation%20learning.%20Specifically%2C%20we%20construct%20SAR-1M%2C%20the%20first%20million-scale%20SAR%20dataset%2C%20with%20additional%20paired%20optical%20images%2C%20to%20enable%20large-scale%20pre-training.%20Building%20upon%20this%2C%20we%20design%20Speckle-Aware%20Representation%20Enhancement%20%28SARE%29%2C%20which%20injects%20SAR-specific%20speckle%20noise%20into%20masked%20autoencoders%20to%20facilitate%20noise-aware%20and%20robust%20representation%20learning.%20Furthermore%2C%20we%20introduce%20Semantic%20Anchor%20Representation%20Constraint%20%28SARC%29%2C%20which%20leverages%20paired%20optical%20priors%20to%20align%20SAR%20features%20and%20ensure%20semantic%20consistency.%20Extensive%20experiments%20across%20multiple%20SAR%20datasets%20demonstrate%20that%20SARMAE%20achieves%20state-of-the-art%20performance%20on%20classification%2C%20detection%2C%20and%20segmentation%20tasks.%20Code%20and%20models%20will%20be%20available%20at%20https%3A//github.com/MiliLab/SARMAE.&entry.1838667208=http%3A//arxiv.org/abs/2512.16635v1&entry.124074799=Read"},
{"title": "Abacus: Self-Supervised Event Counting-Aligned Distributional Pretraining for Sequential User Modeling", "author": "Sullivan Castro and Artem Betlei and Thomas Di Martino and Nadir El Manouzi", "abstract": "Modeling user purchase behavior is a critical challenge in display advertising systems, necessary for real-time bidding. The difficulty arises from the sparsity of positive user events and the stochasticity of user actions, leading to severe class imbalance and irregular event timing. Predictive systems usually rely on hand-crafted \"counter\" features, overlooking the fine-grained temporal evolution of user intent. Meanwhile, current sequential models extract direct sequential signal, missing useful event-counting statistics. We enhance deep sequential models with self-supervised pretraining strategies for display advertising. Especially, we introduce Abacus, a novel approach of predicting the empirical frequency distribution of user events. We further propose a hybrid objective unifying Abacus with sequential learning objectives, combining stability of aggregated statistics with the sequence modeling sensitivity. Experiments on two real-world datasets show that Abacus pretraining outperforms existing methods accelerating downstream task convergence, while hybrid approach yields up to +6.1% AUC compared to the baselines.", "link": "http://arxiv.org/abs/2512.16581v1", "date": "2025-12-18", "relevancy": 2.5966, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5549}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5169}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Abacus%3A%20Self-Supervised%20Event%20Counting-Aligned%20Distributional%20Pretraining%20for%20Sequential%20User%20Modeling&body=Title%3A%20Abacus%3A%20Self-Supervised%20Event%20Counting-Aligned%20Distributional%20Pretraining%20for%20Sequential%20User%20Modeling%0AAuthor%3A%20Sullivan%20Castro%20and%20Artem%20Betlei%20and%20Thomas%20Di%20Martino%20and%20Nadir%20El%20Manouzi%0AAbstract%3A%20Modeling%20user%20purchase%20behavior%20is%20a%20critical%20challenge%20in%20display%20advertising%20systems%2C%20necessary%20for%20real-time%20bidding.%20The%20difficulty%20arises%20from%20the%20sparsity%20of%20positive%20user%20events%20and%20the%20stochasticity%20of%20user%20actions%2C%20leading%20to%20severe%20class%20imbalance%20and%20irregular%20event%20timing.%20Predictive%20systems%20usually%20rely%20on%20hand-crafted%20%22counter%22%20features%2C%20overlooking%20the%20fine-grained%20temporal%20evolution%20of%20user%20intent.%20Meanwhile%2C%20current%20sequential%20models%20extract%20direct%20sequential%20signal%2C%20missing%20useful%20event-counting%20statistics.%20We%20enhance%20deep%20sequential%20models%20with%20self-supervised%20pretraining%20strategies%20for%20display%20advertising.%20Especially%2C%20we%20introduce%20Abacus%2C%20a%20novel%20approach%20of%20predicting%20the%20empirical%20frequency%20distribution%20of%20user%20events.%20We%20further%20propose%20a%20hybrid%20objective%20unifying%20Abacus%20with%20sequential%20learning%20objectives%2C%20combining%20stability%20of%20aggregated%20statistics%20with%20the%20sequence%20modeling%20sensitivity.%20Experiments%20on%20two%20real-world%20datasets%20show%20that%20Abacus%20pretraining%20outperforms%20existing%20methods%20accelerating%20downstream%20task%20convergence%2C%20while%20hybrid%20approach%20yields%20up%20to%20%2B6.1%25%20AUC%20compared%20to%20the%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16581v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAbacus%253A%2520Self-Supervised%2520Event%2520Counting-Aligned%2520Distributional%2520Pretraining%2520for%2520Sequential%2520User%2520Modeling%26entry.906535625%3DSullivan%2520Castro%2520and%2520Artem%2520Betlei%2520and%2520Thomas%2520Di%2520Martino%2520and%2520Nadir%2520El%2520Manouzi%26entry.1292438233%3DModeling%2520user%2520purchase%2520behavior%2520is%2520a%2520critical%2520challenge%2520in%2520display%2520advertising%2520systems%252C%2520necessary%2520for%2520real-time%2520bidding.%2520The%2520difficulty%2520arises%2520from%2520the%2520sparsity%2520of%2520positive%2520user%2520events%2520and%2520the%2520stochasticity%2520of%2520user%2520actions%252C%2520leading%2520to%2520severe%2520class%2520imbalance%2520and%2520irregular%2520event%2520timing.%2520Predictive%2520systems%2520usually%2520rely%2520on%2520hand-crafted%2520%2522counter%2522%2520features%252C%2520overlooking%2520the%2520fine-grained%2520temporal%2520evolution%2520of%2520user%2520intent.%2520Meanwhile%252C%2520current%2520sequential%2520models%2520extract%2520direct%2520sequential%2520signal%252C%2520missing%2520useful%2520event-counting%2520statistics.%2520We%2520enhance%2520deep%2520sequential%2520models%2520with%2520self-supervised%2520pretraining%2520strategies%2520for%2520display%2520advertising.%2520Especially%252C%2520we%2520introduce%2520Abacus%252C%2520a%2520novel%2520approach%2520of%2520predicting%2520the%2520empirical%2520frequency%2520distribution%2520of%2520user%2520events.%2520We%2520further%2520propose%2520a%2520hybrid%2520objective%2520unifying%2520Abacus%2520with%2520sequential%2520learning%2520objectives%252C%2520combining%2520stability%2520of%2520aggregated%2520statistics%2520with%2520the%2520sequence%2520modeling%2520sensitivity.%2520Experiments%2520on%2520two%2520real-world%2520datasets%2520show%2520that%2520Abacus%2520pretraining%2520outperforms%2520existing%2520methods%2520accelerating%2520downstream%2520task%2520convergence%252C%2520while%2520hybrid%2520approach%2520yields%2520up%2520to%2520%252B6.1%2525%2520AUC%2520compared%2520to%2520the%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16581v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Abacus%3A%20Self-Supervised%20Event%20Counting-Aligned%20Distributional%20Pretraining%20for%20Sequential%20User%20Modeling&entry.906535625=Sullivan%20Castro%20and%20Artem%20Betlei%20and%20Thomas%20Di%20Martino%20and%20Nadir%20El%20Manouzi&entry.1292438233=Modeling%20user%20purchase%20behavior%20is%20a%20critical%20challenge%20in%20display%20advertising%20systems%2C%20necessary%20for%20real-time%20bidding.%20The%20difficulty%20arises%20from%20the%20sparsity%20of%20positive%20user%20events%20and%20the%20stochasticity%20of%20user%20actions%2C%20leading%20to%20severe%20class%20imbalance%20and%20irregular%20event%20timing.%20Predictive%20systems%20usually%20rely%20on%20hand-crafted%20%22counter%22%20features%2C%20overlooking%20the%20fine-grained%20temporal%20evolution%20of%20user%20intent.%20Meanwhile%2C%20current%20sequential%20models%20extract%20direct%20sequential%20signal%2C%20missing%20useful%20event-counting%20statistics.%20We%20enhance%20deep%20sequential%20models%20with%20self-supervised%20pretraining%20strategies%20for%20display%20advertising.%20Especially%2C%20we%20introduce%20Abacus%2C%20a%20novel%20approach%20of%20predicting%20the%20empirical%20frequency%20distribution%20of%20user%20events.%20We%20further%20propose%20a%20hybrid%20objective%20unifying%20Abacus%20with%20sequential%20learning%20objectives%2C%20combining%20stability%20of%20aggregated%20statistics%20with%20the%20sequence%20modeling%20sensitivity.%20Experiments%20on%20two%20real-world%20datasets%20show%20that%20Abacus%20pretraining%20outperforms%20existing%20methods%20accelerating%20downstream%20task%20convergence%2C%20while%20hybrid%20approach%20yields%20up%20to%20%2B6.1%25%20AUC%20compared%20to%20the%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2512.16581v1&entry.124074799=Read"},
{"title": "Core-Set Selection for Data-efficient Land Cover Segmentation", "author": "Keiller Nogueira and Akram Zaytar and Wanli Ma and Ribana Roscher and Ronny Hansch and Caleb Robinson and Anthony Ortiz and Simone Nsutezo and Rahul Dodhia and Juan M. Lavista Ferres and Oktay Karakus and Paul L. Rosin", "abstract": "The increasing accessibility of remotely sensed data and their potential to support large-scale decision-making have driven the development of deep learning models for many Earth Observation tasks. Traditionally, such models rely on large datasets. However, the common assumption that larger training datasets lead to better performance tends to overlook issues related to data redundancy, noise, and the computational cost of processing massive datasets. Effective solutions must therefore consider not only the quantity but also the quality of data. Towards this, in this paper, we introduce six basic core-set selection approaches -- that rely on imagery only, labels only, or a combination of both -- and investigate whether they can identify high-quality subsets of data capable of maintaining -- or even surpassing -- the performance achieved when using full datasets for remote sensing semantic segmentation. We benchmark such approaches against two traditional baselines on three widely used land-cover classification datasets (DFC2022, Vaihingen, and Potsdam) using two different architectures (SegFormer and U-Net), thus establishing a general baseline for future works. Our experiments show that all proposed methods consistently outperform the baselines across multiple subset sizes, with some approaches even selecting core sets that surpass training on all available data. Notably, on DFC2022, a selected subset comprising only 25% of the training data yields slightly higher SegFormer performance than training with the entire dataset. This result shows the importance and potential of data-centric learning for the remote sensing domain. The code is available at https://github.com/keillernogueira/data-centric-rs-classification/.", "link": "http://arxiv.org/abs/2505.01225v3", "date": "2025-12-18", "relevancy": 2.5935, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5263}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5263}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Core-Set%20Selection%20for%20Data-efficient%20Land%20Cover%20Segmentation&body=Title%3A%20Core-Set%20Selection%20for%20Data-efficient%20Land%20Cover%20Segmentation%0AAuthor%3A%20Keiller%20Nogueira%20and%20Akram%20Zaytar%20and%20Wanli%20Ma%20and%20Ribana%20Roscher%20and%20Ronny%20Hansch%20and%20Caleb%20Robinson%20and%20Anthony%20Ortiz%20and%20Simone%20Nsutezo%20and%20Rahul%20Dodhia%20and%20Juan%20M.%20Lavista%20Ferres%20and%20Oktay%20Karakus%20and%20Paul%20L.%20Rosin%0AAbstract%3A%20The%20increasing%20accessibility%20of%20remotely%20sensed%20data%20and%20their%20potential%20to%20support%20large-scale%20decision-making%20have%20driven%20the%20development%20of%20deep%20learning%20models%20for%20many%20Earth%20Observation%20tasks.%20Traditionally%2C%20such%20models%20rely%20on%20large%20datasets.%20However%2C%20the%20common%20assumption%20that%20larger%20training%20datasets%20lead%20to%20better%20performance%20tends%20to%20overlook%20issues%20related%20to%20data%20redundancy%2C%20noise%2C%20and%20the%20computational%20cost%20of%20processing%20massive%20datasets.%20Effective%20solutions%20must%20therefore%20consider%20not%20only%20the%20quantity%20but%20also%20the%20quality%20of%20data.%20Towards%20this%2C%20in%20this%20paper%2C%20we%20introduce%20six%20basic%20core-set%20selection%20approaches%20--%20that%20rely%20on%20imagery%20only%2C%20labels%20only%2C%20or%20a%20combination%20of%20both%20--%20and%20investigate%20whether%20they%20can%20identify%20high-quality%20subsets%20of%20data%20capable%20of%20maintaining%20--%20or%20even%20surpassing%20--%20the%20performance%20achieved%20when%20using%20full%20datasets%20for%20remote%20sensing%20semantic%20segmentation.%20We%20benchmark%20such%20approaches%20against%20two%20traditional%20baselines%20on%20three%20widely%20used%20land-cover%20classification%20datasets%20%28DFC2022%2C%20Vaihingen%2C%20and%20Potsdam%29%20using%20two%20different%20architectures%20%28SegFormer%20and%20U-Net%29%2C%20thus%20establishing%20a%20general%20baseline%20for%20future%20works.%20Our%20experiments%20show%20that%20all%20proposed%20methods%20consistently%20outperform%20the%20baselines%20across%20multiple%20subset%20sizes%2C%20with%20some%20approaches%20even%20selecting%20core%20sets%20that%20surpass%20training%20on%20all%20available%20data.%20Notably%2C%20on%20DFC2022%2C%20a%20selected%20subset%20comprising%20only%2025%25%20of%20the%20training%20data%20yields%20slightly%20higher%20SegFormer%20performance%20than%20training%20with%20the%20entire%20dataset.%20This%20result%20shows%20the%20importance%20and%20potential%20of%20data-centric%20learning%20for%20the%20remote%20sensing%20domain.%20The%20code%20is%20available%20at%20https%3A//github.com/keillernogueira/data-centric-rs-classification/.%0ALink%3A%20http%3A//arxiv.org/abs/2505.01225v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCore-Set%2520Selection%2520for%2520Data-efficient%2520Land%2520Cover%2520Segmentation%26entry.906535625%3DKeiller%2520Nogueira%2520and%2520Akram%2520Zaytar%2520and%2520Wanli%2520Ma%2520and%2520Ribana%2520Roscher%2520and%2520Ronny%2520Hansch%2520and%2520Caleb%2520Robinson%2520and%2520Anthony%2520Ortiz%2520and%2520Simone%2520Nsutezo%2520and%2520Rahul%2520Dodhia%2520and%2520Juan%2520M.%2520Lavista%2520Ferres%2520and%2520Oktay%2520Karakus%2520and%2520Paul%2520L.%2520Rosin%26entry.1292438233%3DThe%2520increasing%2520accessibility%2520of%2520remotely%2520sensed%2520data%2520and%2520their%2520potential%2520to%2520support%2520large-scale%2520decision-making%2520have%2520driven%2520the%2520development%2520of%2520deep%2520learning%2520models%2520for%2520many%2520Earth%2520Observation%2520tasks.%2520Traditionally%252C%2520such%2520models%2520rely%2520on%2520large%2520datasets.%2520However%252C%2520the%2520common%2520assumption%2520that%2520larger%2520training%2520datasets%2520lead%2520to%2520better%2520performance%2520tends%2520to%2520overlook%2520issues%2520related%2520to%2520data%2520redundancy%252C%2520noise%252C%2520and%2520the%2520computational%2520cost%2520of%2520processing%2520massive%2520datasets.%2520Effective%2520solutions%2520must%2520therefore%2520consider%2520not%2520only%2520the%2520quantity%2520but%2520also%2520the%2520quality%2520of%2520data.%2520Towards%2520this%252C%2520in%2520this%2520paper%252C%2520we%2520introduce%2520six%2520basic%2520core-set%2520selection%2520approaches%2520--%2520that%2520rely%2520on%2520imagery%2520only%252C%2520labels%2520only%252C%2520or%2520a%2520combination%2520of%2520both%2520--%2520and%2520investigate%2520whether%2520they%2520can%2520identify%2520high-quality%2520subsets%2520of%2520data%2520capable%2520of%2520maintaining%2520--%2520or%2520even%2520surpassing%2520--%2520the%2520performance%2520achieved%2520when%2520using%2520full%2520datasets%2520for%2520remote%2520sensing%2520semantic%2520segmentation.%2520We%2520benchmark%2520such%2520approaches%2520against%2520two%2520traditional%2520baselines%2520on%2520three%2520widely%2520used%2520land-cover%2520classification%2520datasets%2520%2528DFC2022%252C%2520Vaihingen%252C%2520and%2520Potsdam%2529%2520using%2520two%2520different%2520architectures%2520%2528SegFormer%2520and%2520U-Net%2529%252C%2520thus%2520establishing%2520a%2520general%2520baseline%2520for%2520future%2520works.%2520Our%2520experiments%2520show%2520that%2520all%2520proposed%2520methods%2520consistently%2520outperform%2520the%2520baselines%2520across%2520multiple%2520subset%2520sizes%252C%2520with%2520some%2520approaches%2520even%2520selecting%2520core%2520sets%2520that%2520surpass%2520training%2520on%2520all%2520available%2520data.%2520Notably%252C%2520on%2520DFC2022%252C%2520a%2520selected%2520subset%2520comprising%2520only%252025%2525%2520of%2520the%2520training%2520data%2520yields%2520slightly%2520higher%2520SegFormer%2520performance%2520than%2520training%2520with%2520the%2520entire%2520dataset.%2520This%2520result%2520shows%2520the%2520importance%2520and%2520potential%2520of%2520data-centric%2520learning%2520for%2520the%2520remote%2520sensing%2520domain.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/keillernogueira/data-centric-rs-classification/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01225v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Core-Set%20Selection%20for%20Data-efficient%20Land%20Cover%20Segmentation&entry.906535625=Keiller%20Nogueira%20and%20Akram%20Zaytar%20and%20Wanli%20Ma%20and%20Ribana%20Roscher%20and%20Ronny%20Hansch%20and%20Caleb%20Robinson%20and%20Anthony%20Ortiz%20and%20Simone%20Nsutezo%20and%20Rahul%20Dodhia%20and%20Juan%20M.%20Lavista%20Ferres%20and%20Oktay%20Karakus%20and%20Paul%20L.%20Rosin&entry.1292438233=The%20increasing%20accessibility%20of%20remotely%20sensed%20data%20and%20their%20potential%20to%20support%20large-scale%20decision-making%20have%20driven%20the%20development%20of%20deep%20learning%20models%20for%20many%20Earth%20Observation%20tasks.%20Traditionally%2C%20such%20models%20rely%20on%20large%20datasets.%20However%2C%20the%20common%20assumption%20that%20larger%20training%20datasets%20lead%20to%20better%20performance%20tends%20to%20overlook%20issues%20related%20to%20data%20redundancy%2C%20noise%2C%20and%20the%20computational%20cost%20of%20processing%20massive%20datasets.%20Effective%20solutions%20must%20therefore%20consider%20not%20only%20the%20quantity%20but%20also%20the%20quality%20of%20data.%20Towards%20this%2C%20in%20this%20paper%2C%20we%20introduce%20six%20basic%20core-set%20selection%20approaches%20--%20that%20rely%20on%20imagery%20only%2C%20labels%20only%2C%20or%20a%20combination%20of%20both%20--%20and%20investigate%20whether%20they%20can%20identify%20high-quality%20subsets%20of%20data%20capable%20of%20maintaining%20--%20or%20even%20surpassing%20--%20the%20performance%20achieved%20when%20using%20full%20datasets%20for%20remote%20sensing%20semantic%20segmentation.%20We%20benchmark%20such%20approaches%20against%20two%20traditional%20baselines%20on%20three%20widely%20used%20land-cover%20classification%20datasets%20%28DFC2022%2C%20Vaihingen%2C%20and%20Potsdam%29%20using%20two%20different%20architectures%20%28SegFormer%20and%20U-Net%29%2C%20thus%20establishing%20a%20general%20baseline%20for%20future%20works.%20Our%20experiments%20show%20that%20all%20proposed%20methods%20consistently%20outperform%20the%20baselines%20across%20multiple%20subset%20sizes%2C%20with%20some%20approaches%20even%20selecting%20core%20sets%20that%20surpass%20training%20on%20all%20available%20data.%20Notably%2C%20on%20DFC2022%2C%20a%20selected%20subset%20comprising%20only%2025%25%20of%20the%20training%20data%20yields%20slightly%20higher%20SegFormer%20performance%20than%20training%20with%20the%20entire%20dataset.%20This%20result%20shows%20the%20importance%20and%20potential%20of%20data-centric%20learning%20for%20the%20remote%20sensing%20domain.%20The%20code%20is%20available%20at%20https%3A//github.com/keillernogueira/data-centric-rs-classification/.&entry.1838667208=http%3A//arxiv.org/abs/2505.01225v3&entry.124074799=Read"},
{"title": "Factorized Video Generation: Decoupling Scene Construction and Temporal Synthesis in Text-to-Video Diffusion Models", "author": "Mariam Hassan and Bastien Van Delft and Wuyang Li and Alexandre Alahi", "abstract": "State-of-the-art Text-to-Video (T2V) diffusion models can generate visually impressive results, yet they still frequently fail to compose complex scenes or follow logical temporal instructions. In this paper, we argue that many errors, including apparent motion failures, originate from the model's inability to construct a semantically correct or logically consistent initial frame. We introduce Factorized Video Generation (FVG), a pipeline that decouples these tasks by decomposing the Text-to-Video generation into three specialized stages: (1) Reasoning, where a Large Language Model (LLM) rewrites the video prompt to describe only the initial scene, resolving temporal ambiguities; (2) Composition, where a Text-to-Image (T2I) model synthesizes a high-quality, compositionally-correct anchor frame from this new prompt; and (3) Temporal Synthesis, where a video model, finetuned to understand this anchor, focuses its entire capacity on animating the scene and following the prompt. Our decomposed approach sets a new state-of-the-art on the T2V CompBench benchmark and significantly improves all tested models on VBench2. Furthermore, we show that visual anchoring allows us to cut the number of sampling steps by 70% without any loss in performance, leading to a substantial speed-up in sampling. Factorized Video Generation offers a simple yet practical path toward more efficient, robust, and controllable video synthesis", "link": "http://arxiv.org/abs/2512.16371v1", "date": "2025-12-18", "relevancy": 2.5935, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.678}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6632}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Factorized%20Video%20Generation%3A%20Decoupling%20Scene%20Construction%20and%20Temporal%20Synthesis%20in%20Text-to-Video%20Diffusion%20Models&body=Title%3A%20Factorized%20Video%20Generation%3A%20Decoupling%20Scene%20Construction%20and%20Temporal%20Synthesis%20in%20Text-to-Video%20Diffusion%20Models%0AAuthor%3A%20Mariam%20Hassan%20and%20Bastien%20Van%20Delft%20and%20Wuyang%20Li%20and%20Alexandre%20Alahi%0AAbstract%3A%20State-of-the-art%20Text-to-Video%20%28T2V%29%20diffusion%20models%20can%20generate%20visually%20impressive%20results%2C%20yet%20they%20still%20frequently%20fail%20to%20compose%20complex%20scenes%20or%20follow%20logical%20temporal%20instructions.%20In%20this%20paper%2C%20we%20argue%20that%20many%20errors%2C%20including%20apparent%20motion%20failures%2C%20originate%20from%20the%20model%27s%20inability%20to%20construct%20a%20semantically%20correct%20or%20logically%20consistent%20initial%20frame.%20We%20introduce%20Factorized%20Video%20Generation%20%28FVG%29%2C%20a%20pipeline%20that%20decouples%20these%20tasks%20by%20decomposing%20the%20Text-to-Video%20generation%20into%20three%20specialized%20stages%3A%20%281%29%20Reasoning%2C%20where%20a%20Large%20Language%20Model%20%28LLM%29%20rewrites%20the%20video%20prompt%20to%20describe%20only%20the%20initial%20scene%2C%20resolving%20temporal%20ambiguities%3B%20%282%29%20Composition%2C%20where%20a%20Text-to-Image%20%28T2I%29%20model%20synthesizes%20a%20high-quality%2C%20compositionally-correct%20anchor%20frame%20from%20this%20new%20prompt%3B%20and%20%283%29%20Temporal%20Synthesis%2C%20where%20a%20video%20model%2C%20finetuned%20to%20understand%20this%20anchor%2C%20focuses%20its%20entire%20capacity%20on%20animating%20the%20scene%20and%20following%20the%20prompt.%20Our%20decomposed%20approach%20sets%20a%20new%20state-of-the-art%20on%20the%20T2V%20CompBench%20benchmark%20and%20significantly%20improves%20all%20tested%20models%20on%20VBench2.%20Furthermore%2C%20we%20show%20that%20visual%20anchoring%20allows%20us%20to%20cut%20the%20number%20of%20sampling%20steps%20by%2070%25%20without%20any%20loss%20in%20performance%2C%20leading%20to%20a%20substantial%20speed-up%20in%20sampling.%20Factorized%20Video%20Generation%20offers%20a%20simple%20yet%20practical%20path%20toward%20more%20efficient%2C%20robust%2C%20and%20controllable%20video%20synthesis%0ALink%3A%20http%3A//arxiv.org/abs/2512.16371v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFactorized%2520Video%2520Generation%253A%2520Decoupling%2520Scene%2520Construction%2520and%2520Temporal%2520Synthesis%2520in%2520Text-to-Video%2520Diffusion%2520Models%26entry.906535625%3DMariam%2520Hassan%2520and%2520Bastien%2520Van%2520Delft%2520and%2520Wuyang%2520Li%2520and%2520Alexandre%2520Alahi%26entry.1292438233%3DState-of-the-art%2520Text-to-Video%2520%2528T2V%2529%2520diffusion%2520models%2520can%2520generate%2520visually%2520impressive%2520results%252C%2520yet%2520they%2520still%2520frequently%2520fail%2520to%2520compose%2520complex%2520scenes%2520or%2520follow%2520logical%2520temporal%2520instructions.%2520In%2520this%2520paper%252C%2520we%2520argue%2520that%2520many%2520errors%252C%2520including%2520apparent%2520motion%2520failures%252C%2520originate%2520from%2520the%2520model%2527s%2520inability%2520to%2520construct%2520a%2520semantically%2520correct%2520or%2520logically%2520consistent%2520initial%2520frame.%2520We%2520introduce%2520Factorized%2520Video%2520Generation%2520%2528FVG%2529%252C%2520a%2520pipeline%2520that%2520decouples%2520these%2520tasks%2520by%2520decomposing%2520the%2520Text-to-Video%2520generation%2520into%2520three%2520specialized%2520stages%253A%2520%25281%2529%2520Reasoning%252C%2520where%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520rewrites%2520the%2520video%2520prompt%2520to%2520describe%2520only%2520the%2520initial%2520scene%252C%2520resolving%2520temporal%2520ambiguities%253B%2520%25282%2529%2520Composition%252C%2520where%2520a%2520Text-to-Image%2520%2528T2I%2529%2520model%2520synthesizes%2520a%2520high-quality%252C%2520compositionally-correct%2520anchor%2520frame%2520from%2520this%2520new%2520prompt%253B%2520and%2520%25283%2529%2520Temporal%2520Synthesis%252C%2520where%2520a%2520video%2520model%252C%2520finetuned%2520to%2520understand%2520this%2520anchor%252C%2520focuses%2520its%2520entire%2520capacity%2520on%2520animating%2520the%2520scene%2520and%2520following%2520the%2520prompt.%2520Our%2520decomposed%2520approach%2520sets%2520a%2520new%2520state-of-the-art%2520on%2520the%2520T2V%2520CompBench%2520benchmark%2520and%2520significantly%2520improves%2520all%2520tested%2520models%2520on%2520VBench2.%2520Furthermore%252C%2520we%2520show%2520that%2520visual%2520anchoring%2520allows%2520us%2520to%2520cut%2520the%2520number%2520of%2520sampling%2520steps%2520by%252070%2525%2520without%2520any%2520loss%2520in%2520performance%252C%2520leading%2520to%2520a%2520substantial%2520speed-up%2520in%2520sampling.%2520Factorized%2520Video%2520Generation%2520offers%2520a%2520simple%2520yet%2520practical%2520path%2520toward%2520more%2520efficient%252C%2520robust%252C%2520and%2520controllable%2520video%2520synthesis%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16371v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Factorized%20Video%20Generation%3A%20Decoupling%20Scene%20Construction%20and%20Temporal%20Synthesis%20in%20Text-to-Video%20Diffusion%20Models&entry.906535625=Mariam%20Hassan%20and%20Bastien%20Van%20Delft%20and%20Wuyang%20Li%20and%20Alexandre%20Alahi&entry.1292438233=State-of-the-art%20Text-to-Video%20%28T2V%29%20diffusion%20models%20can%20generate%20visually%20impressive%20results%2C%20yet%20they%20still%20frequently%20fail%20to%20compose%20complex%20scenes%20or%20follow%20logical%20temporal%20instructions.%20In%20this%20paper%2C%20we%20argue%20that%20many%20errors%2C%20including%20apparent%20motion%20failures%2C%20originate%20from%20the%20model%27s%20inability%20to%20construct%20a%20semantically%20correct%20or%20logically%20consistent%20initial%20frame.%20We%20introduce%20Factorized%20Video%20Generation%20%28FVG%29%2C%20a%20pipeline%20that%20decouples%20these%20tasks%20by%20decomposing%20the%20Text-to-Video%20generation%20into%20three%20specialized%20stages%3A%20%281%29%20Reasoning%2C%20where%20a%20Large%20Language%20Model%20%28LLM%29%20rewrites%20the%20video%20prompt%20to%20describe%20only%20the%20initial%20scene%2C%20resolving%20temporal%20ambiguities%3B%20%282%29%20Composition%2C%20where%20a%20Text-to-Image%20%28T2I%29%20model%20synthesizes%20a%20high-quality%2C%20compositionally-correct%20anchor%20frame%20from%20this%20new%20prompt%3B%20and%20%283%29%20Temporal%20Synthesis%2C%20where%20a%20video%20model%2C%20finetuned%20to%20understand%20this%20anchor%2C%20focuses%20its%20entire%20capacity%20on%20animating%20the%20scene%20and%20following%20the%20prompt.%20Our%20decomposed%20approach%20sets%20a%20new%20state-of-the-art%20on%20the%20T2V%20CompBench%20benchmark%20and%20significantly%20improves%20all%20tested%20models%20on%20VBench2.%20Furthermore%2C%20we%20show%20that%20visual%20anchoring%20allows%20us%20to%20cut%20the%20number%20of%20sampling%20steps%20by%2070%25%20without%20any%20loss%20in%20performance%2C%20leading%20to%20a%20substantial%20speed-up%20in%20sampling.%20Factorized%20Video%20Generation%20offers%20a%20simple%20yet%20practical%20path%20toward%20more%20efficient%2C%20robust%2C%20and%20controllable%20video%20synthesis&entry.1838667208=http%3A//arxiv.org/abs/2512.16371v1&entry.124074799=Read"},
{"title": "Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward", "author": "Peter Chen and Xiaopeng Li and Ziniu Li and Wotao Yin and Xi Chen and Tianyi Lin", "abstract": "This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.", "link": "http://arxiv.org/abs/2512.16912v1", "date": "2025-12-18", "relevancy": 2.5883, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5309}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.511}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploration%20v.s.%20Exploitation%3A%20Rethinking%20RLVR%20through%20Clipping%2C%20Entropy%2C%20and%20Spurious%20Reward&body=Title%3A%20Exploration%20v.s.%20Exploitation%3A%20Rethinking%20RLVR%20through%20Clipping%2C%20Entropy%2C%20and%20Spurious%20Reward%0AAuthor%3A%20Peter%20Chen%20and%20Xiaopeng%20Li%20and%20Ziniu%20Li%20and%20Wotao%20Yin%20and%20Xi%20Chen%20and%20Tianyi%20Lin%0AAbstract%3A%20This%20paper%20examines%20the%20exploration-exploitation%20trade-off%20in%20reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%2C%20a%20framework%20for%20improving%20the%20reasoning%20of%20Large%20Language%20Models%20%28LLMs%29.%20Recent%20studies%20suggest%20that%20RLVR%20can%20elicit%20strong%20mathematical%20reasoning%20in%20LLMs%20through%20two%20seemingly%20paradoxical%20mechanisms%3A%20spurious%20rewards%2C%20which%20suppress%20exploitation%20by%20rewarding%20outcomes%20unrelated%20to%20the%20ground%20truth%2C%20and%20entropy%20minimization%2C%20which%20suppresses%20exploration%20by%20pushing%20the%20model%20toward%20more%20confident%20and%20deterministic%20outputs%2C%20highlighting%20a%20puzzling%20dynamic%3A%20both%20discouraging%20exploitation%20and%20discouraging%20exploration%20improve%20reasoning%20performance%2C%20yet%20the%20underlying%20principles%20that%20reconcile%20these%20effects%20remain%20poorly%20understood.%20We%20focus%20on%20two%20fundamental%20questions%3A%20%28i%29%20how%20policy%20entropy%20relates%20to%20performance%2C%20and%20%28ii%29%20whether%20spurious%20rewards%20yield%20gains%2C%20potentially%20through%20the%20interplay%20of%20clipping%20bias%20and%20model%20contamination.%20Our%20results%20show%20that%20clipping%20bias%20under%20spurious%20rewards%20reduces%20policy%20entropy%2C%20leading%20to%20more%20confident%20and%20deterministic%20outputs%2C%20while%20entropy%20minimization%20alone%20is%20insufficient%20for%20improvement.%20We%20further%20propose%20a%20reward-misalignment%20model%20explaining%20why%20spurious%20rewards%20can%20enhance%20performance%20beyond%20contaminated%20settings.%20Our%20findings%20clarify%20the%20mechanisms%20behind%20spurious-reward%20benefits%20and%20provide%20principles%20for%20more%20effective%20RLVR%20training.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16912v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploration%2520v.s.%2520Exploitation%253A%2520Rethinking%2520RLVR%2520through%2520Clipping%252C%2520Entropy%252C%2520and%2520Spurious%2520Reward%26entry.906535625%3DPeter%2520Chen%2520and%2520Xiaopeng%2520Li%2520and%2520Ziniu%2520Li%2520and%2520Wotao%2520Yin%2520and%2520Xi%2520Chen%2520and%2520Tianyi%2520Lin%26entry.1292438233%3DThis%2520paper%2520examines%2520the%2520exploration-exploitation%2520trade-off%2520in%2520reinforcement%2520learning%2520with%2520verifiable%2520rewards%2520%2528RLVR%2529%252C%2520a%2520framework%2520for%2520improving%2520the%2520reasoning%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520Recent%2520studies%2520suggest%2520that%2520RLVR%2520can%2520elicit%2520strong%2520mathematical%2520reasoning%2520in%2520LLMs%2520through%2520two%2520seemingly%2520paradoxical%2520mechanisms%253A%2520spurious%2520rewards%252C%2520which%2520suppress%2520exploitation%2520by%2520rewarding%2520outcomes%2520unrelated%2520to%2520the%2520ground%2520truth%252C%2520and%2520entropy%2520minimization%252C%2520which%2520suppresses%2520exploration%2520by%2520pushing%2520the%2520model%2520toward%2520more%2520confident%2520and%2520deterministic%2520outputs%252C%2520highlighting%2520a%2520puzzling%2520dynamic%253A%2520both%2520discouraging%2520exploitation%2520and%2520discouraging%2520exploration%2520improve%2520reasoning%2520performance%252C%2520yet%2520the%2520underlying%2520principles%2520that%2520reconcile%2520these%2520effects%2520remain%2520poorly%2520understood.%2520We%2520focus%2520on%2520two%2520fundamental%2520questions%253A%2520%2528i%2529%2520how%2520policy%2520entropy%2520relates%2520to%2520performance%252C%2520and%2520%2528ii%2529%2520whether%2520spurious%2520rewards%2520yield%2520gains%252C%2520potentially%2520through%2520the%2520interplay%2520of%2520clipping%2520bias%2520and%2520model%2520contamination.%2520Our%2520results%2520show%2520that%2520clipping%2520bias%2520under%2520spurious%2520rewards%2520reduces%2520policy%2520entropy%252C%2520leading%2520to%2520more%2520confident%2520and%2520deterministic%2520outputs%252C%2520while%2520entropy%2520minimization%2520alone%2520is%2520insufficient%2520for%2520improvement.%2520We%2520further%2520propose%2520a%2520reward-misalignment%2520model%2520explaining%2520why%2520spurious%2520rewards%2520can%2520enhance%2520performance%2520beyond%2520contaminated%2520settings.%2520Our%2520findings%2520clarify%2520the%2520mechanisms%2520behind%2520spurious-reward%2520benefits%2520and%2520provide%2520principles%2520for%2520more%2520effective%2520RLVR%2520training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16912v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploration%20v.s.%20Exploitation%3A%20Rethinking%20RLVR%20through%20Clipping%2C%20Entropy%2C%20and%20Spurious%20Reward&entry.906535625=Peter%20Chen%20and%20Xiaopeng%20Li%20and%20Ziniu%20Li%20and%20Wotao%20Yin%20and%20Xi%20Chen%20and%20Tianyi%20Lin&entry.1292438233=This%20paper%20examines%20the%20exploration-exploitation%20trade-off%20in%20reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%2C%20a%20framework%20for%20improving%20the%20reasoning%20of%20Large%20Language%20Models%20%28LLMs%29.%20Recent%20studies%20suggest%20that%20RLVR%20can%20elicit%20strong%20mathematical%20reasoning%20in%20LLMs%20through%20two%20seemingly%20paradoxical%20mechanisms%3A%20spurious%20rewards%2C%20which%20suppress%20exploitation%20by%20rewarding%20outcomes%20unrelated%20to%20the%20ground%20truth%2C%20and%20entropy%20minimization%2C%20which%20suppresses%20exploration%20by%20pushing%20the%20model%20toward%20more%20confident%20and%20deterministic%20outputs%2C%20highlighting%20a%20puzzling%20dynamic%3A%20both%20discouraging%20exploitation%20and%20discouraging%20exploration%20improve%20reasoning%20performance%2C%20yet%20the%20underlying%20principles%20that%20reconcile%20these%20effects%20remain%20poorly%20understood.%20We%20focus%20on%20two%20fundamental%20questions%3A%20%28i%29%20how%20policy%20entropy%20relates%20to%20performance%2C%20and%20%28ii%29%20whether%20spurious%20rewards%20yield%20gains%2C%20potentially%20through%20the%20interplay%20of%20clipping%20bias%20and%20model%20contamination.%20Our%20results%20show%20that%20clipping%20bias%20under%20spurious%20rewards%20reduces%20policy%20entropy%2C%20leading%20to%20more%20confident%20and%20deterministic%20outputs%2C%20while%20entropy%20minimization%20alone%20is%20insufficient%20for%20improvement.%20We%20further%20propose%20a%20reward-misalignment%20model%20explaining%20why%20spurious%20rewards%20can%20enhance%20performance%20beyond%20contaminated%20settings.%20Our%20findings%20clarify%20the%20mechanisms%20behind%20spurious-reward%20benefits%20and%20provide%20principles%20for%20more%20effective%20RLVR%20training.&entry.1838667208=http%3A//arxiv.org/abs/2512.16912v1&entry.124074799=Read"},
{"title": "Hypernetworks That Evolve Themselves", "author": "Joachim Winther Pedersen and Erwan Plantec and Eleni Nisioti and Marcello Barylli and Milton Montero and Kathrin Korte and Sebastian Risi", "abstract": "How can neural networks evolve themselves without relying on external optimizers? We propose Self-Referential Graph HyperNetworks, systems where the very machinery of variation and inheritance is embedded within the network. By uniting hypernetworks, stochastic parameter generation, and graph-based representations, Self-Referential GHNs mutate and evaluate themselves while adapting mutation rates as selectable traits. Through new reinforcement learning benchmarks with environmental shifts (CartPoleSwitch, LunarLander-Switch), Self-Referential GHNs show swift, reliable adaptation and emergent population dynamics. In the locomotion benchmark Ant-v5, they evolve coherent gaits, showing promising fine-tuning capabilities by autonomously decreasing variation in the population to concentrate around promising solutions. Our findings support the idea that evolvability itself can emerge from neural self-reference. Self-Referential GHNs reflect a step toward synthetic systems that more closely mirror biological evolution, offering tools for autonomous, open-ended learning agents.", "link": "http://arxiv.org/abs/2512.16406v1", "date": "2025-12-18", "relevancy": 2.5734, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.545}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5164}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hypernetworks%20That%20Evolve%20Themselves&body=Title%3A%20Hypernetworks%20That%20Evolve%20Themselves%0AAuthor%3A%20Joachim%20Winther%20Pedersen%20and%20Erwan%20Plantec%20and%20Eleni%20Nisioti%20and%20Marcello%20Barylli%20and%20Milton%20Montero%20and%20Kathrin%20Korte%20and%20Sebastian%20Risi%0AAbstract%3A%20How%20can%20neural%20networks%20evolve%20themselves%20without%20relying%20on%20external%20optimizers%3F%20We%20propose%20Self-Referential%20Graph%20HyperNetworks%2C%20systems%20where%20the%20very%20machinery%20of%20variation%20and%20inheritance%20is%20embedded%20within%20the%20network.%20By%20uniting%20hypernetworks%2C%20stochastic%20parameter%20generation%2C%20and%20graph-based%20representations%2C%20Self-Referential%20GHNs%20mutate%20and%20evaluate%20themselves%20while%20adapting%20mutation%20rates%20as%20selectable%20traits.%20Through%20new%20reinforcement%20learning%20benchmarks%20with%20environmental%20shifts%20%28CartPoleSwitch%2C%20LunarLander-Switch%29%2C%20Self-Referential%20GHNs%20show%20swift%2C%20reliable%20adaptation%20and%20emergent%20population%20dynamics.%20In%20the%20locomotion%20benchmark%20Ant-v5%2C%20they%20evolve%20coherent%20gaits%2C%20showing%20promising%20fine-tuning%20capabilities%20by%20autonomously%20decreasing%20variation%20in%20the%20population%20to%20concentrate%20around%20promising%20solutions.%20Our%20findings%20support%20the%20idea%20that%20evolvability%20itself%20can%20emerge%20from%20neural%20self-reference.%20Self-Referential%20GHNs%20reflect%20a%20step%20toward%20synthetic%20systems%20that%20more%20closely%20mirror%20biological%20evolution%2C%20offering%20tools%20for%20autonomous%2C%20open-ended%20learning%20agents.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16406v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHypernetworks%2520That%2520Evolve%2520Themselves%26entry.906535625%3DJoachim%2520Winther%2520Pedersen%2520and%2520Erwan%2520Plantec%2520and%2520Eleni%2520Nisioti%2520and%2520Marcello%2520Barylli%2520and%2520Milton%2520Montero%2520and%2520Kathrin%2520Korte%2520and%2520Sebastian%2520Risi%26entry.1292438233%3DHow%2520can%2520neural%2520networks%2520evolve%2520themselves%2520without%2520relying%2520on%2520external%2520optimizers%253F%2520We%2520propose%2520Self-Referential%2520Graph%2520HyperNetworks%252C%2520systems%2520where%2520the%2520very%2520machinery%2520of%2520variation%2520and%2520inheritance%2520is%2520embedded%2520within%2520the%2520network.%2520By%2520uniting%2520hypernetworks%252C%2520stochastic%2520parameter%2520generation%252C%2520and%2520graph-based%2520representations%252C%2520Self-Referential%2520GHNs%2520mutate%2520and%2520evaluate%2520themselves%2520while%2520adapting%2520mutation%2520rates%2520as%2520selectable%2520traits.%2520Through%2520new%2520reinforcement%2520learning%2520benchmarks%2520with%2520environmental%2520shifts%2520%2528CartPoleSwitch%252C%2520LunarLander-Switch%2529%252C%2520Self-Referential%2520GHNs%2520show%2520swift%252C%2520reliable%2520adaptation%2520and%2520emergent%2520population%2520dynamics.%2520In%2520the%2520locomotion%2520benchmark%2520Ant-v5%252C%2520they%2520evolve%2520coherent%2520gaits%252C%2520showing%2520promising%2520fine-tuning%2520capabilities%2520by%2520autonomously%2520decreasing%2520variation%2520in%2520the%2520population%2520to%2520concentrate%2520around%2520promising%2520solutions.%2520Our%2520findings%2520support%2520the%2520idea%2520that%2520evolvability%2520itself%2520can%2520emerge%2520from%2520neural%2520self-reference.%2520Self-Referential%2520GHNs%2520reflect%2520a%2520step%2520toward%2520synthetic%2520systems%2520that%2520more%2520closely%2520mirror%2520biological%2520evolution%252C%2520offering%2520tools%2520for%2520autonomous%252C%2520open-ended%2520learning%2520agents.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16406v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hypernetworks%20That%20Evolve%20Themselves&entry.906535625=Joachim%20Winther%20Pedersen%20and%20Erwan%20Plantec%20and%20Eleni%20Nisioti%20and%20Marcello%20Barylli%20and%20Milton%20Montero%20and%20Kathrin%20Korte%20and%20Sebastian%20Risi&entry.1292438233=How%20can%20neural%20networks%20evolve%20themselves%20without%20relying%20on%20external%20optimizers%3F%20We%20propose%20Self-Referential%20Graph%20HyperNetworks%2C%20systems%20where%20the%20very%20machinery%20of%20variation%20and%20inheritance%20is%20embedded%20within%20the%20network.%20By%20uniting%20hypernetworks%2C%20stochastic%20parameter%20generation%2C%20and%20graph-based%20representations%2C%20Self-Referential%20GHNs%20mutate%20and%20evaluate%20themselves%20while%20adapting%20mutation%20rates%20as%20selectable%20traits.%20Through%20new%20reinforcement%20learning%20benchmarks%20with%20environmental%20shifts%20%28CartPoleSwitch%2C%20LunarLander-Switch%29%2C%20Self-Referential%20GHNs%20show%20swift%2C%20reliable%20adaptation%20and%20emergent%20population%20dynamics.%20In%20the%20locomotion%20benchmark%20Ant-v5%2C%20they%20evolve%20coherent%20gaits%2C%20showing%20promising%20fine-tuning%20capabilities%20by%20autonomously%20decreasing%20variation%20in%20the%20population%20to%20concentrate%20around%20promising%20solutions.%20Our%20findings%20support%20the%20idea%20that%20evolvability%20itself%20can%20emerge%20from%20neural%20self-reference.%20Self-Referential%20GHNs%20reflect%20a%20step%20toward%20synthetic%20systems%20that%20more%20closely%20mirror%20biological%20evolution%2C%20offering%20tools%20for%20autonomous%2C%20open-ended%20learning%20agents.&entry.1838667208=http%3A//arxiv.org/abs/2512.16406v1&entry.124074799=Read"},
{"title": "GeoGraph: Geometric and Graph-based Ensemble Descriptors for Intrinsically Disordered Proteins", "author": "Eoin Quinn and Marco Carobene and Jean Quentin and Sebastien Boyer and Miguel Arbes\u00fa and Oliver Bent", "abstract": "While deep learning has revolutionized the prediction of rigid protein structures, modelling the conformational ensembles of Intrinsically Disordered Proteins (IDPs) remains a key frontier. Current AI paradigms present a trade-off: Protein Language Models (PLMs) capture evolutionary statistics but lack explicit physical grounding, while generative models trained to model full ensembles are computationally expensive. In this work we critically assess these limits and propose a path forward. We introduce GeoGraph, a simulation-informed surrogate trained to predict ensemble-averaged statistics of residue-residue contact-map topology directly from sequence. By featurizing coarse-grained molecular dynamics simulations into residue- and sequence-level graph descriptors, we create a robust and information-rich learning target. Our evaluation demonstrates that this approach yields representations that are more predictive of key biophysical properties than existing methods.", "link": "http://arxiv.org/abs/2510.00774v2", "date": "2025-12-18", "relevancy": 2.5645, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5196}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5112}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoGraph%3A%20Geometric%20and%20Graph-based%20Ensemble%20Descriptors%20for%20Intrinsically%20Disordered%20Proteins&body=Title%3A%20GeoGraph%3A%20Geometric%20and%20Graph-based%20Ensemble%20Descriptors%20for%20Intrinsically%20Disordered%20Proteins%0AAuthor%3A%20Eoin%20Quinn%20and%20Marco%20Carobene%20and%20Jean%20Quentin%20and%20Sebastien%20Boyer%20and%20Miguel%20Arbes%C3%BA%20and%20Oliver%20Bent%0AAbstract%3A%20While%20deep%20learning%20has%20revolutionized%20the%20prediction%20of%20rigid%20protein%20structures%2C%20modelling%20the%20conformational%20ensembles%20of%20Intrinsically%20Disordered%20Proteins%20%28IDPs%29%20remains%20a%20key%20frontier.%20Current%20AI%20paradigms%20present%20a%20trade-off%3A%20Protein%20Language%20Models%20%28PLMs%29%20capture%20evolutionary%20statistics%20but%20lack%20explicit%20physical%20grounding%2C%20while%20generative%20models%20trained%20to%20model%20full%20ensembles%20are%20computationally%20expensive.%20In%20this%20work%20we%20critically%20assess%20these%20limits%20and%20propose%20a%20path%20forward.%20We%20introduce%20GeoGraph%2C%20a%20simulation-informed%20surrogate%20trained%20to%20predict%20ensemble-averaged%20statistics%20of%20residue-residue%20contact-map%20topology%20directly%20from%20sequence.%20By%20featurizing%20coarse-grained%20molecular%20dynamics%20simulations%20into%20residue-%20and%20sequence-level%20graph%20descriptors%2C%20we%20create%20a%20robust%20and%20information-rich%20learning%20target.%20Our%20evaluation%20demonstrates%20that%20this%20approach%20yields%20representations%20that%20are%20more%20predictive%20of%20key%20biophysical%20properties%20than%20existing%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2510.00774v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoGraph%253A%2520Geometric%2520and%2520Graph-based%2520Ensemble%2520Descriptors%2520for%2520Intrinsically%2520Disordered%2520Proteins%26entry.906535625%3DEoin%2520Quinn%2520and%2520Marco%2520Carobene%2520and%2520Jean%2520Quentin%2520and%2520Sebastien%2520Boyer%2520and%2520Miguel%2520Arbes%25C3%25BA%2520and%2520Oliver%2520Bent%26entry.1292438233%3DWhile%2520deep%2520learning%2520has%2520revolutionized%2520the%2520prediction%2520of%2520rigid%2520protein%2520structures%252C%2520modelling%2520the%2520conformational%2520ensembles%2520of%2520Intrinsically%2520Disordered%2520Proteins%2520%2528IDPs%2529%2520remains%2520a%2520key%2520frontier.%2520Current%2520AI%2520paradigms%2520present%2520a%2520trade-off%253A%2520Protein%2520Language%2520Models%2520%2528PLMs%2529%2520capture%2520evolutionary%2520statistics%2520but%2520lack%2520explicit%2520physical%2520grounding%252C%2520while%2520generative%2520models%2520trained%2520to%2520model%2520full%2520ensembles%2520are%2520computationally%2520expensive.%2520In%2520this%2520work%2520we%2520critically%2520assess%2520these%2520limits%2520and%2520propose%2520a%2520path%2520forward.%2520We%2520introduce%2520GeoGraph%252C%2520a%2520simulation-informed%2520surrogate%2520trained%2520to%2520predict%2520ensemble-averaged%2520statistics%2520of%2520residue-residue%2520contact-map%2520topology%2520directly%2520from%2520sequence.%2520By%2520featurizing%2520coarse-grained%2520molecular%2520dynamics%2520simulations%2520into%2520residue-%2520and%2520sequence-level%2520graph%2520descriptors%252C%2520we%2520create%2520a%2520robust%2520and%2520information-rich%2520learning%2520target.%2520Our%2520evaluation%2520demonstrates%2520that%2520this%2520approach%2520yields%2520representations%2520that%2520are%2520more%2520predictive%2520of%2520key%2520biophysical%2520properties%2520than%2520existing%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.00774v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoGraph%3A%20Geometric%20and%20Graph-based%20Ensemble%20Descriptors%20for%20Intrinsically%20Disordered%20Proteins&entry.906535625=Eoin%20Quinn%20and%20Marco%20Carobene%20and%20Jean%20Quentin%20and%20Sebastien%20Boyer%20and%20Miguel%20Arbes%C3%BA%20and%20Oliver%20Bent&entry.1292438233=While%20deep%20learning%20has%20revolutionized%20the%20prediction%20of%20rigid%20protein%20structures%2C%20modelling%20the%20conformational%20ensembles%20of%20Intrinsically%20Disordered%20Proteins%20%28IDPs%29%20remains%20a%20key%20frontier.%20Current%20AI%20paradigms%20present%20a%20trade-off%3A%20Protein%20Language%20Models%20%28PLMs%29%20capture%20evolutionary%20statistics%20but%20lack%20explicit%20physical%20grounding%2C%20while%20generative%20models%20trained%20to%20model%20full%20ensembles%20are%20computationally%20expensive.%20In%20this%20work%20we%20critically%20assess%20these%20limits%20and%20propose%20a%20path%20forward.%20We%20introduce%20GeoGraph%2C%20a%20simulation-informed%20surrogate%20trained%20to%20predict%20ensemble-averaged%20statistics%20of%20residue-residue%20contact-map%20topology%20directly%20from%20sequence.%20By%20featurizing%20coarse-grained%20molecular%20dynamics%20simulations%20into%20residue-%20and%20sequence-level%20graph%20descriptors%2C%20we%20create%20a%20robust%20and%20information-rich%20learning%20target.%20Our%20evaluation%20demonstrates%20that%20this%20approach%20yields%20representations%20that%20are%20more%20predictive%20of%20key%20biophysical%20properties%20than%20existing%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2510.00774v2&entry.124074799=Read"},
{"title": "SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning", "author": "Tin Stribor Sohn and Maximilian Dillitzer and Jason J. Corso and Eric Sax", "abstract": "Autonomous robotic systems require spatio-temporal understanding of dynamic environments to ensure reliable navigation and interaction. While Vision-Language Models (VLMs) provide open-world semantic priors, they lack grounding in 3D geometry and temporal dynamics. Conversely, geometric perception captures structure and motion but remains semantically sparse. We propose SNOW (Scene Understanding with Open-World Knowledge), a training-free and backbone-agnostic framework for unified 4D scene understanding that integrates VLM-derived semantics with point cloud geometry and temporal consistency. SNOW processes synchronized RGB images and 3D point clouds, using HDBSCAN clustering to generate object-level proposals that guide SAM2-based segmentation. Each segmented region is encoded through our proposed Spatio-Temporal Tokenized Patch Encoding (STEP), producing multimodal tokens that capture localized semantic, geometric, and temporal attributes. These tokens are incrementally integrated into a 4D Scene Graph (4DSG), which serves as 4D prior for downstream reasoning. A lightweight SLAM backend anchors all STEP tokens spatially in the environment, providing the global reference alignment, and ensuring unambiguous spatial grounding across time. The resulting 4DSG forms a queryable, unified world model through which VLMs can directly interpret spatial scene structure and temporal dynamics. Experiments on a diverse set of benchmarks demonstrate that SNOW enables precise 4D scene understanding and spatially grounded inference, thereby setting new state-of-the-art performance in several settings, highlighting the importance of structured 4D priors for embodied reasoning and autonomous robotics.", "link": "http://arxiv.org/abs/2512.16461v1", "date": "2025-12-18", "relevancy": 2.5567, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6397}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6397}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SNOW%3A%20Spatio-Temporal%20Scene%20Understanding%20with%20World%20Knowledge%20for%20Open-World%20Embodied%20Reasoning&body=Title%3A%20SNOW%3A%20Spatio-Temporal%20Scene%20Understanding%20with%20World%20Knowledge%20for%20Open-World%20Embodied%20Reasoning%0AAuthor%3A%20Tin%20Stribor%20Sohn%20and%20Maximilian%20Dillitzer%20and%20Jason%20J.%20Corso%20and%20Eric%20Sax%0AAbstract%3A%20Autonomous%20robotic%20systems%20require%20spatio-temporal%20understanding%20of%20dynamic%20environments%20to%20ensure%20reliable%20navigation%20and%20interaction.%20While%20Vision-Language%20Models%20%28VLMs%29%20provide%20open-world%20semantic%20priors%2C%20they%20lack%20grounding%20in%203D%20geometry%20and%20temporal%20dynamics.%20Conversely%2C%20geometric%20perception%20captures%20structure%20and%20motion%20but%20remains%20semantically%20sparse.%20We%20propose%20SNOW%20%28Scene%20Understanding%20with%20Open-World%20Knowledge%29%2C%20a%20training-free%20and%20backbone-agnostic%20framework%20for%20unified%204D%20scene%20understanding%20that%20integrates%20VLM-derived%20semantics%20with%20point%20cloud%20geometry%20and%20temporal%20consistency.%20SNOW%20processes%20synchronized%20RGB%20images%20and%203D%20point%20clouds%2C%20using%20HDBSCAN%20clustering%20to%20generate%20object-level%20proposals%20that%20guide%20SAM2-based%20segmentation.%20Each%20segmented%20region%20is%20encoded%20through%20our%20proposed%20Spatio-Temporal%20Tokenized%20Patch%20Encoding%20%28STEP%29%2C%20producing%20multimodal%20tokens%20that%20capture%20localized%20semantic%2C%20geometric%2C%20and%20temporal%20attributes.%20These%20tokens%20are%20incrementally%20integrated%20into%20a%204D%20Scene%20Graph%20%284DSG%29%2C%20which%20serves%20as%204D%20prior%20for%20downstream%20reasoning.%20A%20lightweight%20SLAM%20backend%20anchors%20all%20STEP%20tokens%20spatially%20in%20the%20environment%2C%20providing%20the%20global%20reference%20alignment%2C%20and%20ensuring%20unambiguous%20spatial%20grounding%20across%20time.%20The%20resulting%204DSG%20forms%20a%20queryable%2C%20unified%20world%20model%20through%20which%20VLMs%20can%20directly%20interpret%20spatial%20scene%20structure%20and%20temporal%20dynamics.%20Experiments%20on%20a%20diverse%20set%20of%20benchmarks%20demonstrate%20that%20SNOW%20enables%20precise%204D%20scene%20understanding%20and%20spatially%20grounded%20inference%2C%20thereby%20setting%20new%20state-of-the-art%20performance%20in%20several%20settings%2C%20highlighting%20the%20importance%20of%20structured%204D%20priors%20for%20embodied%20reasoning%20and%20autonomous%20robotics.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16461v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSNOW%253A%2520Spatio-Temporal%2520Scene%2520Understanding%2520with%2520World%2520Knowledge%2520for%2520Open-World%2520Embodied%2520Reasoning%26entry.906535625%3DTin%2520Stribor%2520Sohn%2520and%2520Maximilian%2520Dillitzer%2520and%2520Jason%2520J.%2520Corso%2520and%2520Eric%2520Sax%26entry.1292438233%3DAutonomous%2520robotic%2520systems%2520require%2520spatio-temporal%2520understanding%2520of%2520dynamic%2520environments%2520to%2520ensure%2520reliable%2520navigation%2520and%2520interaction.%2520While%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520provide%2520open-world%2520semantic%2520priors%252C%2520they%2520lack%2520grounding%2520in%25203D%2520geometry%2520and%2520temporal%2520dynamics.%2520Conversely%252C%2520geometric%2520perception%2520captures%2520structure%2520and%2520motion%2520but%2520remains%2520semantically%2520sparse.%2520We%2520propose%2520SNOW%2520%2528Scene%2520Understanding%2520with%2520Open-World%2520Knowledge%2529%252C%2520a%2520training-free%2520and%2520backbone-agnostic%2520framework%2520for%2520unified%25204D%2520scene%2520understanding%2520that%2520integrates%2520VLM-derived%2520semantics%2520with%2520point%2520cloud%2520geometry%2520and%2520temporal%2520consistency.%2520SNOW%2520processes%2520synchronized%2520RGB%2520images%2520and%25203D%2520point%2520clouds%252C%2520using%2520HDBSCAN%2520clustering%2520to%2520generate%2520object-level%2520proposals%2520that%2520guide%2520SAM2-based%2520segmentation.%2520Each%2520segmented%2520region%2520is%2520encoded%2520through%2520our%2520proposed%2520Spatio-Temporal%2520Tokenized%2520Patch%2520Encoding%2520%2528STEP%2529%252C%2520producing%2520multimodal%2520tokens%2520that%2520capture%2520localized%2520semantic%252C%2520geometric%252C%2520and%2520temporal%2520attributes.%2520These%2520tokens%2520are%2520incrementally%2520integrated%2520into%2520a%25204D%2520Scene%2520Graph%2520%25284DSG%2529%252C%2520which%2520serves%2520as%25204D%2520prior%2520for%2520downstream%2520reasoning.%2520A%2520lightweight%2520SLAM%2520backend%2520anchors%2520all%2520STEP%2520tokens%2520spatially%2520in%2520the%2520environment%252C%2520providing%2520the%2520global%2520reference%2520alignment%252C%2520and%2520ensuring%2520unambiguous%2520spatial%2520grounding%2520across%2520time.%2520The%2520resulting%25204DSG%2520forms%2520a%2520queryable%252C%2520unified%2520world%2520model%2520through%2520which%2520VLMs%2520can%2520directly%2520interpret%2520spatial%2520scene%2520structure%2520and%2520temporal%2520dynamics.%2520Experiments%2520on%2520a%2520diverse%2520set%2520of%2520benchmarks%2520demonstrate%2520that%2520SNOW%2520enables%2520precise%25204D%2520scene%2520understanding%2520and%2520spatially%2520grounded%2520inference%252C%2520thereby%2520setting%2520new%2520state-of-the-art%2520performance%2520in%2520several%2520settings%252C%2520highlighting%2520the%2520importance%2520of%2520structured%25204D%2520priors%2520for%2520embodied%2520reasoning%2520and%2520autonomous%2520robotics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16461v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SNOW%3A%20Spatio-Temporal%20Scene%20Understanding%20with%20World%20Knowledge%20for%20Open-World%20Embodied%20Reasoning&entry.906535625=Tin%20Stribor%20Sohn%20and%20Maximilian%20Dillitzer%20and%20Jason%20J.%20Corso%20and%20Eric%20Sax&entry.1292438233=Autonomous%20robotic%20systems%20require%20spatio-temporal%20understanding%20of%20dynamic%20environments%20to%20ensure%20reliable%20navigation%20and%20interaction.%20While%20Vision-Language%20Models%20%28VLMs%29%20provide%20open-world%20semantic%20priors%2C%20they%20lack%20grounding%20in%203D%20geometry%20and%20temporal%20dynamics.%20Conversely%2C%20geometric%20perception%20captures%20structure%20and%20motion%20but%20remains%20semantically%20sparse.%20We%20propose%20SNOW%20%28Scene%20Understanding%20with%20Open-World%20Knowledge%29%2C%20a%20training-free%20and%20backbone-agnostic%20framework%20for%20unified%204D%20scene%20understanding%20that%20integrates%20VLM-derived%20semantics%20with%20point%20cloud%20geometry%20and%20temporal%20consistency.%20SNOW%20processes%20synchronized%20RGB%20images%20and%203D%20point%20clouds%2C%20using%20HDBSCAN%20clustering%20to%20generate%20object-level%20proposals%20that%20guide%20SAM2-based%20segmentation.%20Each%20segmented%20region%20is%20encoded%20through%20our%20proposed%20Spatio-Temporal%20Tokenized%20Patch%20Encoding%20%28STEP%29%2C%20producing%20multimodal%20tokens%20that%20capture%20localized%20semantic%2C%20geometric%2C%20and%20temporal%20attributes.%20These%20tokens%20are%20incrementally%20integrated%20into%20a%204D%20Scene%20Graph%20%284DSG%29%2C%20which%20serves%20as%204D%20prior%20for%20downstream%20reasoning.%20A%20lightweight%20SLAM%20backend%20anchors%20all%20STEP%20tokens%20spatially%20in%20the%20environment%2C%20providing%20the%20global%20reference%20alignment%2C%20and%20ensuring%20unambiguous%20spatial%20grounding%20across%20time.%20The%20resulting%204DSG%20forms%20a%20queryable%2C%20unified%20world%20model%20through%20which%20VLMs%20can%20directly%20interpret%20spatial%20scene%20structure%20and%20temporal%20dynamics.%20Experiments%20on%20a%20diverse%20set%20of%20benchmarks%20demonstrate%20that%20SNOW%20enables%20precise%204D%20scene%20understanding%20and%20spatially%20grounded%20inference%2C%20thereby%20setting%20new%20state-of-the-art%20performance%20in%20several%20settings%2C%20highlighting%20the%20importance%20of%20structured%204D%20priors%20for%20embodied%20reasoning%20and%20autonomous%20robotics.&entry.1838667208=http%3A//arxiv.org/abs/2512.16461v1&entry.124074799=Read"},
{"title": "Unified Semantic Transformer for 3D Scene Understanding", "author": "Sebastian Koch and Johanna Wald and Hidenobu Matsuki and Pedro Hermosilla and Timo Ropinski and Federico Tombari", "abstract": "Holistic 3D scene understanding involves capturing and parsing unstructured 3D environments. Due to the inherent complexity of the real world, existing models have predominantly been developed and limited to be task-specific. We introduce UNITE, a Unified Semantic Transformer for 3D scene understanding, a novel feed-forward neural network that unifies a diverse set of 3D semantic tasks within a single model. Our model operates on unseen scenes in a fully end-to-end manner and only takes a few seconds to infer the full 3D semantic geometry. Our approach is capable of directly predicting multiple semantic attributes, including 3D scene segmentation, instance embeddings, open-vocabulary features, as well as affordance and articulations, solely from RGB images. The method is trained using a combination of 2D distillation, heavily relying on self-supervision and leverages novel multi-view losses designed to ensure 3D view consistency. We demonstrate that UNITE achieves state-of-the-art performance on several different semantic tasks and even outperforms task-specific models, in many cases, surpassing methods that operate on ground truth 3D geometry. See the project website at unite-page.github.io", "link": "http://arxiv.org/abs/2512.14364v2", "date": "2025-12-18", "relevancy": 2.5547, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6422}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6422}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Semantic%20Transformer%20for%203D%20Scene%20Understanding&body=Title%3A%20Unified%20Semantic%20Transformer%20for%203D%20Scene%20Understanding%0AAuthor%3A%20Sebastian%20Koch%20and%20Johanna%20Wald%20and%20Hidenobu%20Matsuki%20and%20Pedro%20Hermosilla%20and%20Timo%20Ropinski%20and%20Federico%20Tombari%0AAbstract%3A%20Holistic%203D%20scene%20understanding%20involves%20capturing%20and%20parsing%20unstructured%203D%20environments.%20Due%20to%20the%20inherent%20complexity%20of%20the%20real%20world%2C%20existing%20models%20have%20predominantly%20been%20developed%20and%20limited%20to%20be%20task-specific.%20We%20introduce%20UNITE%2C%20a%20Unified%20Semantic%20Transformer%20for%203D%20scene%20understanding%2C%20a%20novel%20feed-forward%20neural%20network%20that%20unifies%20a%20diverse%20set%20of%203D%20semantic%20tasks%20within%20a%20single%20model.%20Our%20model%20operates%20on%20unseen%20scenes%20in%20a%20fully%20end-to-end%20manner%20and%20only%20takes%20a%20few%20seconds%20to%20infer%20the%20full%203D%20semantic%20geometry.%20Our%20approach%20is%20capable%20of%20directly%20predicting%20multiple%20semantic%20attributes%2C%20including%203D%20scene%20segmentation%2C%20instance%20embeddings%2C%20open-vocabulary%20features%2C%20as%20well%20as%20affordance%20and%20articulations%2C%20solely%20from%20RGB%20images.%20The%20method%20is%20trained%20using%20a%20combination%20of%202D%20distillation%2C%20heavily%20relying%20on%20self-supervision%20and%20leverages%20novel%20multi-view%20losses%20designed%20to%20ensure%203D%20view%20consistency.%20We%20demonstrate%20that%20UNITE%20achieves%20state-of-the-art%20performance%20on%20several%20different%20semantic%20tasks%20and%20even%20outperforms%20task-specific%20models%2C%20in%20many%20cases%2C%20surpassing%20methods%20that%20operate%20on%20ground%20truth%203D%20geometry.%20See%20the%20project%20website%20at%20unite-page.github.io%0ALink%3A%20http%3A//arxiv.org/abs/2512.14364v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Semantic%2520Transformer%2520for%25203D%2520Scene%2520Understanding%26entry.906535625%3DSebastian%2520Koch%2520and%2520Johanna%2520Wald%2520and%2520Hidenobu%2520Matsuki%2520and%2520Pedro%2520Hermosilla%2520and%2520Timo%2520Ropinski%2520and%2520Federico%2520Tombari%26entry.1292438233%3DHolistic%25203D%2520scene%2520understanding%2520involves%2520capturing%2520and%2520parsing%2520unstructured%25203D%2520environments.%2520Due%2520to%2520the%2520inherent%2520complexity%2520of%2520the%2520real%2520world%252C%2520existing%2520models%2520have%2520predominantly%2520been%2520developed%2520and%2520limited%2520to%2520be%2520task-specific.%2520We%2520introduce%2520UNITE%252C%2520a%2520Unified%2520Semantic%2520Transformer%2520for%25203D%2520scene%2520understanding%252C%2520a%2520novel%2520feed-forward%2520neural%2520network%2520that%2520unifies%2520a%2520diverse%2520set%2520of%25203D%2520semantic%2520tasks%2520within%2520a%2520single%2520model.%2520Our%2520model%2520operates%2520on%2520unseen%2520scenes%2520in%2520a%2520fully%2520end-to-end%2520manner%2520and%2520only%2520takes%2520a%2520few%2520seconds%2520to%2520infer%2520the%2520full%25203D%2520semantic%2520geometry.%2520Our%2520approach%2520is%2520capable%2520of%2520directly%2520predicting%2520multiple%2520semantic%2520attributes%252C%2520including%25203D%2520scene%2520segmentation%252C%2520instance%2520embeddings%252C%2520open-vocabulary%2520features%252C%2520as%2520well%2520as%2520affordance%2520and%2520articulations%252C%2520solely%2520from%2520RGB%2520images.%2520The%2520method%2520is%2520trained%2520using%2520a%2520combination%2520of%25202D%2520distillation%252C%2520heavily%2520relying%2520on%2520self-supervision%2520and%2520leverages%2520novel%2520multi-view%2520losses%2520designed%2520to%2520ensure%25203D%2520view%2520consistency.%2520We%2520demonstrate%2520that%2520UNITE%2520achieves%2520state-of-the-art%2520performance%2520on%2520several%2520different%2520semantic%2520tasks%2520and%2520even%2520outperforms%2520task-specific%2520models%252C%2520in%2520many%2520cases%252C%2520surpassing%2520methods%2520that%2520operate%2520on%2520ground%2520truth%25203D%2520geometry.%2520See%2520the%2520project%2520website%2520at%2520unite-page.github.io%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14364v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Semantic%20Transformer%20for%203D%20Scene%20Understanding&entry.906535625=Sebastian%20Koch%20and%20Johanna%20Wald%20and%20Hidenobu%20Matsuki%20and%20Pedro%20Hermosilla%20and%20Timo%20Ropinski%20and%20Federico%20Tombari&entry.1292438233=Holistic%203D%20scene%20understanding%20involves%20capturing%20and%20parsing%20unstructured%203D%20environments.%20Due%20to%20the%20inherent%20complexity%20of%20the%20real%20world%2C%20existing%20models%20have%20predominantly%20been%20developed%20and%20limited%20to%20be%20task-specific.%20We%20introduce%20UNITE%2C%20a%20Unified%20Semantic%20Transformer%20for%203D%20scene%20understanding%2C%20a%20novel%20feed-forward%20neural%20network%20that%20unifies%20a%20diverse%20set%20of%203D%20semantic%20tasks%20within%20a%20single%20model.%20Our%20model%20operates%20on%20unseen%20scenes%20in%20a%20fully%20end-to-end%20manner%20and%20only%20takes%20a%20few%20seconds%20to%20infer%20the%20full%203D%20semantic%20geometry.%20Our%20approach%20is%20capable%20of%20directly%20predicting%20multiple%20semantic%20attributes%2C%20including%203D%20scene%20segmentation%2C%20instance%20embeddings%2C%20open-vocabulary%20features%2C%20as%20well%20as%20affordance%20and%20articulations%2C%20solely%20from%20RGB%20images.%20The%20method%20is%20trained%20using%20a%20combination%20of%202D%20distillation%2C%20heavily%20relying%20on%20self-supervision%20and%20leverages%20novel%20multi-view%20losses%20designed%20to%20ensure%203D%20view%20consistency.%20We%20demonstrate%20that%20UNITE%20achieves%20state-of-the-art%20performance%20on%20several%20different%20semantic%20tasks%20and%20even%20outperforms%20task-specific%20models%2C%20in%20many%20cases%2C%20surpassing%20methods%20that%20operate%20on%20ground%20truth%203D%20geometry.%20See%20the%20project%20website%20at%20unite-page.github.io&entry.1838667208=http%3A//arxiv.org/abs/2512.14364v2&entry.124074799=Read"},
{"title": "Mat\u00e9rn Kernels for Tunable Implicit Surface Reconstruction", "author": "Maximilian Weiherer and Bernhard Egger", "abstract": "We propose to use the family of Mat\u00e9rn kernels for implicit surface reconstruction, building upon the recent success of kernel methods for 3D reconstruction of oriented point clouds. As we show from a theoretical and practical perspective, Mat\u00e9rn kernels have some appealing properties which make them particularly well suited for surface reconstruction -- outperforming state-of-the-art methods based on the arc-cosine kernel while being significantly easier to implement, faster to compute, and scalable. Being stationary, we demonstrate that Mat\u00e9rn kernels allow for tunable surface reconstruction in the same way as Fourier feature mappings help coordinate-based MLPs overcome spectral bias. Moreover, we theoretically analyze Mat\u00e9rn kernels' connection to SIREN networks as well as their relation to previously employed arc-cosine kernels. Finally, based on recently introduced Neural Kernel Fields, we present data-dependent Mat\u00e9rn kernels and conclude that especially the Laplace kernel (being part of the Mat\u00e9rn family) is extremely competitive, performing almost on par with state-of-the-art methods in the noise-free case while having a more than five times shorter training time.", "link": "http://arxiv.org/abs/2409.15466v3", "date": "2025-12-18", "relevancy": 2.543, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5265}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5173}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mat%C3%A9rn%20Kernels%20for%20Tunable%20Implicit%20Surface%20Reconstruction&body=Title%3A%20Mat%C3%A9rn%20Kernels%20for%20Tunable%20Implicit%20Surface%20Reconstruction%0AAuthor%3A%20Maximilian%20Weiherer%20and%20Bernhard%20Egger%0AAbstract%3A%20We%20propose%20to%20use%20the%20family%20of%20Mat%C3%A9rn%20kernels%20for%20implicit%20surface%20reconstruction%2C%20building%20upon%20the%20recent%20success%20of%20kernel%20methods%20for%203D%20reconstruction%20of%20oriented%20point%20clouds.%20As%20we%20show%20from%20a%20theoretical%20and%20practical%20perspective%2C%20Mat%C3%A9rn%20kernels%20have%20some%20appealing%20properties%20which%20make%20them%20particularly%20well%20suited%20for%20surface%20reconstruction%20--%20outperforming%20state-of-the-art%20methods%20based%20on%20the%20arc-cosine%20kernel%20while%20being%20significantly%20easier%20to%20implement%2C%20faster%20to%20compute%2C%20and%20scalable.%20Being%20stationary%2C%20we%20demonstrate%20that%20Mat%C3%A9rn%20kernels%20allow%20for%20tunable%20surface%20reconstruction%20in%20the%20same%20way%20as%20Fourier%20feature%20mappings%20help%20coordinate-based%20MLPs%20overcome%20spectral%20bias.%20Moreover%2C%20we%20theoretically%20analyze%20Mat%C3%A9rn%20kernels%27%20connection%20to%20SIREN%20networks%20as%20well%20as%20their%20relation%20to%20previously%20employed%20arc-cosine%20kernels.%20Finally%2C%20based%20on%20recently%20introduced%20Neural%20Kernel%20Fields%2C%20we%20present%20data-dependent%20Mat%C3%A9rn%20kernels%20and%20conclude%20that%20especially%20the%20Laplace%20kernel%20%28being%20part%20of%20the%20Mat%C3%A9rn%20family%29%20is%20extremely%20competitive%2C%20performing%20almost%20on%20par%20with%20state-of-the-art%20methods%20in%20the%20noise-free%20case%20while%20having%20a%20more%20than%20five%20times%20shorter%20training%20time.%0ALink%3A%20http%3A//arxiv.org/abs/2409.15466v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMat%25C3%25A9rn%2520Kernels%2520for%2520Tunable%2520Implicit%2520Surface%2520Reconstruction%26entry.906535625%3DMaximilian%2520Weiherer%2520and%2520Bernhard%2520Egger%26entry.1292438233%3DWe%2520propose%2520to%2520use%2520the%2520family%2520of%2520Mat%25C3%25A9rn%2520kernels%2520for%2520implicit%2520surface%2520reconstruction%252C%2520building%2520upon%2520the%2520recent%2520success%2520of%2520kernel%2520methods%2520for%25203D%2520reconstruction%2520of%2520oriented%2520point%2520clouds.%2520As%2520we%2520show%2520from%2520a%2520theoretical%2520and%2520practical%2520perspective%252C%2520Mat%25C3%25A9rn%2520kernels%2520have%2520some%2520appealing%2520properties%2520which%2520make%2520them%2520particularly%2520well%2520suited%2520for%2520surface%2520reconstruction%2520--%2520outperforming%2520state-of-the-art%2520methods%2520based%2520on%2520the%2520arc-cosine%2520kernel%2520while%2520being%2520significantly%2520easier%2520to%2520implement%252C%2520faster%2520to%2520compute%252C%2520and%2520scalable.%2520Being%2520stationary%252C%2520we%2520demonstrate%2520that%2520Mat%25C3%25A9rn%2520kernels%2520allow%2520for%2520tunable%2520surface%2520reconstruction%2520in%2520the%2520same%2520way%2520as%2520Fourier%2520feature%2520mappings%2520help%2520coordinate-based%2520MLPs%2520overcome%2520spectral%2520bias.%2520Moreover%252C%2520we%2520theoretically%2520analyze%2520Mat%25C3%25A9rn%2520kernels%2527%2520connection%2520to%2520SIREN%2520networks%2520as%2520well%2520as%2520their%2520relation%2520to%2520previously%2520employed%2520arc-cosine%2520kernels.%2520Finally%252C%2520based%2520on%2520recently%2520introduced%2520Neural%2520Kernel%2520Fields%252C%2520we%2520present%2520data-dependent%2520Mat%25C3%25A9rn%2520kernels%2520and%2520conclude%2520that%2520especially%2520the%2520Laplace%2520kernel%2520%2528being%2520part%2520of%2520the%2520Mat%25C3%25A9rn%2520family%2529%2520is%2520extremely%2520competitive%252C%2520performing%2520almost%2520on%2520par%2520with%2520state-of-the-art%2520methods%2520in%2520the%2520noise-free%2520case%2520while%2520having%2520a%2520more%2520than%2520five%2520times%2520shorter%2520training%2520time.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.15466v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mat%C3%A9rn%20Kernels%20for%20Tunable%20Implicit%20Surface%20Reconstruction&entry.906535625=Maximilian%20Weiherer%20and%20Bernhard%20Egger&entry.1292438233=We%20propose%20to%20use%20the%20family%20of%20Mat%C3%A9rn%20kernels%20for%20implicit%20surface%20reconstruction%2C%20building%20upon%20the%20recent%20success%20of%20kernel%20methods%20for%203D%20reconstruction%20of%20oriented%20point%20clouds.%20As%20we%20show%20from%20a%20theoretical%20and%20practical%20perspective%2C%20Mat%C3%A9rn%20kernels%20have%20some%20appealing%20properties%20which%20make%20them%20particularly%20well%20suited%20for%20surface%20reconstruction%20--%20outperforming%20state-of-the-art%20methods%20based%20on%20the%20arc-cosine%20kernel%20while%20being%20significantly%20easier%20to%20implement%2C%20faster%20to%20compute%2C%20and%20scalable.%20Being%20stationary%2C%20we%20demonstrate%20that%20Mat%C3%A9rn%20kernels%20allow%20for%20tunable%20surface%20reconstruction%20in%20the%20same%20way%20as%20Fourier%20feature%20mappings%20help%20coordinate-based%20MLPs%20overcome%20spectral%20bias.%20Moreover%2C%20we%20theoretically%20analyze%20Mat%C3%A9rn%20kernels%27%20connection%20to%20SIREN%20networks%20as%20well%20as%20their%20relation%20to%20previously%20employed%20arc-cosine%20kernels.%20Finally%2C%20based%20on%20recently%20introduced%20Neural%20Kernel%20Fields%2C%20we%20present%20data-dependent%20Mat%C3%A9rn%20kernels%20and%20conclude%20that%20especially%20the%20Laplace%20kernel%20%28being%20part%20of%20the%20Mat%C3%A9rn%20family%29%20is%20extremely%20competitive%2C%20performing%20almost%20on%20par%20with%20state-of-the-art%20methods%20in%20the%20noise-free%20case%20while%20having%20a%20more%20than%20five%20times%20shorter%20training%20time.&entry.1838667208=http%3A//arxiv.org/abs/2409.15466v3&entry.124074799=Read"},
{"title": "Plug to Place: Indoor Multimedia Geolocation from Electrical Sockets for Digital Investigation", "author": "Kanwal Aftab and Graham Adams and Mark Scanlon", "abstract": "Computer vision is a rapidly evolving field, giving rise to powerful new tools and techniques in digital forensic investigation, and shows great promise for novel digital forensic applications. One such application, indoor multimedia geolocation, has the potential to become a crucial aid for law enforcement in the fight against human trafficking, child exploitation, and other serious crimes. While outdoor multimedia geolocation has been widely explored, its indoor counterpart remains underdeveloped due to challenges such as similar room layouts, frequent renovations, visual ambiguity, indoor lighting variability, unreliable GPS signals, and limited datasets in sensitive domains. This paper introduces a pipeline that uses electric sockets as consistent indoor markers for geolocation, since plug socket types are standardised by country or region. The three-stage deep learning pipeline detects plug sockets (YOLOv11, mAP@0.5 = 0.843), classifies them into one of 12 plug socket types (Xception, accuracy = 0.912), and maps the detected socket types to countries (accuracy = 0.96 at >90% threshold confidence). To address data scarcity, two dedicated datasets were created: socket detection dataset of 2,328 annotated images expanded to 4,072 through augmentation, and a classification dataset of 3,187 images across 12 plug socket classes. The pipeline was evaluated on the Hotels-50K dataset, focusing on the TraffickCam subset of crowd-sourced hotel images, which capture real-world conditions such as poor lighting and amateur angles. This dataset provides a more realistic evaluation than using professional, well-lit, often wide-angle images from travel websites. This framework demonstrates a practical step toward real-world digital forensic applications. The code, trained models, and the data for this paper are available open source.", "link": "http://arxiv.org/abs/2512.16620v1", "date": "2025-12-18", "relevancy": 2.5379, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5327}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5055}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Plug%20to%20Place%3A%20Indoor%20Multimedia%20Geolocation%20from%20Electrical%20Sockets%20for%20Digital%20Investigation&body=Title%3A%20Plug%20to%20Place%3A%20Indoor%20Multimedia%20Geolocation%20from%20Electrical%20Sockets%20for%20Digital%20Investigation%0AAuthor%3A%20Kanwal%20Aftab%20and%20Graham%20Adams%20and%20Mark%20Scanlon%0AAbstract%3A%20Computer%20vision%20is%20a%20rapidly%20evolving%20field%2C%20giving%20rise%20to%20powerful%20new%20tools%20and%20techniques%20in%20digital%20forensic%20investigation%2C%20and%20shows%20great%20promise%20for%20novel%20digital%20forensic%20applications.%20One%20such%20application%2C%20indoor%20multimedia%20geolocation%2C%20has%20the%20potential%20to%20become%20a%20crucial%20aid%20for%20law%20enforcement%20in%20the%20fight%20against%20human%20trafficking%2C%20child%20exploitation%2C%20and%20other%20serious%20crimes.%20While%20outdoor%20multimedia%20geolocation%20has%20been%20widely%20explored%2C%20its%20indoor%20counterpart%20remains%20underdeveloped%20due%20to%20challenges%20such%20as%20similar%20room%20layouts%2C%20frequent%20renovations%2C%20visual%20ambiguity%2C%20indoor%20lighting%20variability%2C%20unreliable%20GPS%20signals%2C%20and%20limited%20datasets%20in%20sensitive%20domains.%20This%20paper%20introduces%20a%20pipeline%20that%20uses%20electric%20sockets%20as%20consistent%20indoor%20markers%20for%20geolocation%2C%20since%20plug%20socket%20types%20are%20standardised%20by%20country%20or%20region.%20The%20three-stage%20deep%20learning%20pipeline%20detects%20plug%20sockets%20%28YOLOv11%2C%20mAP%400.5%20%3D%200.843%29%2C%20classifies%20them%20into%20one%20of%2012%20plug%20socket%20types%20%28Xception%2C%20accuracy%20%3D%200.912%29%2C%20and%20maps%20the%20detected%20socket%20types%20to%20countries%20%28accuracy%20%3D%200.96%20at%20%3E90%25%20threshold%20confidence%29.%20To%20address%20data%20scarcity%2C%20two%20dedicated%20datasets%20were%20created%3A%20socket%20detection%20dataset%20of%202%2C328%20annotated%20images%20expanded%20to%204%2C072%20through%20augmentation%2C%20and%20a%20classification%20dataset%20of%203%2C187%20images%20across%2012%20plug%20socket%20classes.%20The%20pipeline%20was%20evaluated%20on%20the%20Hotels-50K%20dataset%2C%20focusing%20on%20the%20TraffickCam%20subset%20of%20crowd-sourced%20hotel%20images%2C%20which%20capture%20real-world%20conditions%20such%20as%20poor%20lighting%20and%20amateur%20angles.%20This%20dataset%20provides%20a%20more%20realistic%20evaluation%20than%20using%20professional%2C%20well-lit%2C%20often%20wide-angle%20images%20from%20travel%20websites.%20This%20framework%20demonstrates%20a%20practical%20step%20toward%20real-world%20digital%20forensic%20applications.%20The%20code%2C%20trained%20models%2C%20and%20the%20data%20for%20this%20paper%20are%20available%20open%20source.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16620v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlug%2520to%2520Place%253A%2520Indoor%2520Multimedia%2520Geolocation%2520from%2520Electrical%2520Sockets%2520for%2520Digital%2520Investigation%26entry.906535625%3DKanwal%2520Aftab%2520and%2520Graham%2520Adams%2520and%2520Mark%2520Scanlon%26entry.1292438233%3DComputer%2520vision%2520is%2520a%2520rapidly%2520evolving%2520field%252C%2520giving%2520rise%2520to%2520powerful%2520new%2520tools%2520and%2520techniques%2520in%2520digital%2520forensic%2520investigation%252C%2520and%2520shows%2520great%2520promise%2520for%2520novel%2520digital%2520forensic%2520applications.%2520One%2520such%2520application%252C%2520indoor%2520multimedia%2520geolocation%252C%2520has%2520the%2520potential%2520to%2520become%2520a%2520crucial%2520aid%2520for%2520law%2520enforcement%2520in%2520the%2520fight%2520against%2520human%2520trafficking%252C%2520child%2520exploitation%252C%2520and%2520other%2520serious%2520crimes.%2520While%2520outdoor%2520multimedia%2520geolocation%2520has%2520been%2520widely%2520explored%252C%2520its%2520indoor%2520counterpart%2520remains%2520underdeveloped%2520due%2520to%2520challenges%2520such%2520as%2520similar%2520room%2520layouts%252C%2520frequent%2520renovations%252C%2520visual%2520ambiguity%252C%2520indoor%2520lighting%2520variability%252C%2520unreliable%2520GPS%2520signals%252C%2520and%2520limited%2520datasets%2520in%2520sensitive%2520domains.%2520This%2520paper%2520introduces%2520a%2520pipeline%2520that%2520uses%2520electric%2520sockets%2520as%2520consistent%2520indoor%2520markers%2520for%2520geolocation%252C%2520since%2520plug%2520socket%2520types%2520are%2520standardised%2520by%2520country%2520or%2520region.%2520The%2520three-stage%2520deep%2520learning%2520pipeline%2520detects%2520plug%2520sockets%2520%2528YOLOv11%252C%2520mAP%25400.5%2520%253D%25200.843%2529%252C%2520classifies%2520them%2520into%2520one%2520of%252012%2520plug%2520socket%2520types%2520%2528Xception%252C%2520accuracy%2520%253D%25200.912%2529%252C%2520and%2520maps%2520the%2520detected%2520socket%2520types%2520to%2520countries%2520%2528accuracy%2520%253D%25200.96%2520at%2520%253E90%2525%2520threshold%2520confidence%2529.%2520To%2520address%2520data%2520scarcity%252C%2520two%2520dedicated%2520datasets%2520were%2520created%253A%2520socket%2520detection%2520dataset%2520of%25202%252C328%2520annotated%2520images%2520expanded%2520to%25204%252C072%2520through%2520augmentation%252C%2520and%2520a%2520classification%2520dataset%2520of%25203%252C187%2520images%2520across%252012%2520plug%2520socket%2520classes.%2520The%2520pipeline%2520was%2520evaluated%2520on%2520the%2520Hotels-50K%2520dataset%252C%2520focusing%2520on%2520the%2520TraffickCam%2520subset%2520of%2520crowd-sourced%2520hotel%2520images%252C%2520which%2520capture%2520real-world%2520conditions%2520such%2520as%2520poor%2520lighting%2520and%2520amateur%2520angles.%2520This%2520dataset%2520provides%2520a%2520more%2520realistic%2520evaluation%2520than%2520using%2520professional%252C%2520well-lit%252C%2520often%2520wide-angle%2520images%2520from%2520travel%2520websites.%2520This%2520framework%2520demonstrates%2520a%2520practical%2520step%2520toward%2520real-world%2520digital%2520forensic%2520applications.%2520The%2520code%252C%2520trained%2520models%252C%2520and%2520the%2520data%2520for%2520this%2520paper%2520are%2520available%2520open%2520source.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16620v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Plug%20to%20Place%3A%20Indoor%20Multimedia%20Geolocation%20from%20Electrical%20Sockets%20for%20Digital%20Investigation&entry.906535625=Kanwal%20Aftab%20and%20Graham%20Adams%20and%20Mark%20Scanlon&entry.1292438233=Computer%20vision%20is%20a%20rapidly%20evolving%20field%2C%20giving%20rise%20to%20powerful%20new%20tools%20and%20techniques%20in%20digital%20forensic%20investigation%2C%20and%20shows%20great%20promise%20for%20novel%20digital%20forensic%20applications.%20One%20such%20application%2C%20indoor%20multimedia%20geolocation%2C%20has%20the%20potential%20to%20become%20a%20crucial%20aid%20for%20law%20enforcement%20in%20the%20fight%20against%20human%20trafficking%2C%20child%20exploitation%2C%20and%20other%20serious%20crimes.%20While%20outdoor%20multimedia%20geolocation%20has%20been%20widely%20explored%2C%20its%20indoor%20counterpart%20remains%20underdeveloped%20due%20to%20challenges%20such%20as%20similar%20room%20layouts%2C%20frequent%20renovations%2C%20visual%20ambiguity%2C%20indoor%20lighting%20variability%2C%20unreliable%20GPS%20signals%2C%20and%20limited%20datasets%20in%20sensitive%20domains.%20This%20paper%20introduces%20a%20pipeline%20that%20uses%20electric%20sockets%20as%20consistent%20indoor%20markers%20for%20geolocation%2C%20since%20plug%20socket%20types%20are%20standardised%20by%20country%20or%20region.%20The%20three-stage%20deep%20learning%20pipeline%20detects%20plug%20sockets%20%28YOLOv11%2C%20mAP%400.5%20%3D%200.843%29%2C%20classifies%20them%20into%20one%20of%2012%20plug%20socket%20types%20%28Xception%2C%20accuracy%20%3D%200.912%29%2C%20and%20maps%20the%20detected%20socket%20types%20to%20countries%20%28accuracy%20%3D%200.96%20at%20%3E90%25%20threshold%20confidence%29.%20To%20address%20data%20scarcity%2C%20two%20dedicated%20datasets%20were%20created%3A%20socket%20detection%20dataset%20of%202%2C328%20annotated%20images%20expanded%20to%204%2C072%20through%20augmentation%2C%20and%20a%20classification%20dataset%20of%203%2C187%20images%20across%2012%20plug%20socket%20classes.%20The%20pipeline%20was%20evaluated%20on%20the%20Hotels-50K%20dataset%2C%20focusing%20on%20the%20TraffickCam%20subset%20of%20crowd-sourced%20hotel%20images%2C%20which%20capture%20real-world%20conditions%20such%20as%20poor%20lighting%20and%20amateur%20angles.%20This%20dataset%20provides%20a%20more%20realistic%20evaluation%20than%20using%20professional%2C%20well-lit%2C%20often%20wide-angle%20images%20from%20travel%20websites.%20This%20framework%20demonstrates%20a%20practical%20step%20toward%20real-world%20digital%20forensic%20applications.%20The%20code%2C%20trained%20models%2C%20and%20the%20data%20for%20this%20paper%20are%20available%20open%20source.&entry.1838667208=http%3A//arxiv.org/abs/2512.16620v1&entry.124074799=Read"},
{"title": "BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution", "author": "Terry Yue Zhuo and Xiaolong Jin and Hange Liu and Juyong Jiang and Tianyang Liu and Chen Gong and Bhupesh Bishnoi and Vaisakhi Mishra and Marek Suppa and Noah Ziems and Saiteja Utpala and Ming Xu and Guangyu Song and Kaixin Li and Yuhan Cao and Bo Liu and Zheng Liu and Sabina Abdurakhmanova and Wenhao Yu and Mengzhao Jia and Jihan Yao and Kenneth Hamilton and Kumar Shridhar and Minh Chien Vu and Dingmin Wang and Jiawei Liu and Zijian Wang and Qian Liu and Binyuan Hui and Meg Risdal and Ahsen Khaliq and Atin Sood and Zhenchang Xing and Wasi Uddin Ahmad and John Grundy and David Lo and Banghua Zhu and Xiaoning Du and Torsten Scholak and Leandro von Werra", "abstract": "Crowdsourced model evaluation platforms, such as Chatbot Arena, enable real-time evaluation from human perspectives to assess the quality of model responses. In the coding domain, manually examining the quality of LLM-generated content is extremely challenging, as it requires understanding long chunks of raw code and deliberately simulating code execution. To this end, we introduce BigCodeArena, an open human evaluation platform for code generation backed by a comprehensive and on-the-fly execution environment. Built on top of Chatbot Arena, BigCodeArena enables the execution of LLM-generated code and allows humans to interact with the execution process and outcomes. We collected over 14,000 raw code-centric conversation sessions across 10 widely used LLMs, spanning 10 languages and 8 types of execution environments. Among these conversations, we identified more than 4,700 multi-turn samples with pairwise human preferences. Further analysis uncovers underexplored preferences of LLMs in fine-grained domains characterized by tasks, languages, and frameworks. To systematically examine code understanding and generation capabilities of frontier LLMs, we curated two benchmarks based on the collected data, namely BigCodeReward and AutoCodeArena. For BigCodeReward, we post-processed the 4,700 conversations and evaluated the consistency between reward models and human preferences. The evaluation shows that most LLMs have superior performance in judging coding preferences when the execution results are available. Inspired by these findings, we propose AutoCodeArena, an automatic Elo rating benchmark designed to assess the coding quality of LLMs without human involvement. We find that proprietary LLMs like GPT-5, Claude-Sonnet-4, and Claude-Opus-4 still lead in code generation performance among recent emerging models.", "link": "http://arxiv.org/abs/2510.08697v2", "date": "2025-12-18", "relevancy": 2.5202, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5113}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5091}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BigCodeArena%3A%20Unveiling%20More%20Reliable%20Human%20Preferences%20in%20Code%20Generation%20via%20Execution&body=Title%3A%20BigCodeArena%3A%20Unveiling%20More%20Reliable%20Human%20Preferences%20in%20Code%20Generation%20via%20Execution%0AAuthor%3A%20Terry%20Yue%20Zhuo%20and%20Xiaolong%20Jin%20and%20Hange%20Liu%20and%20Juyong%20Jiang%20and%20Tianyang%20Liu%20and%20Chen%20Gong%20and%20Bhupesh%20Bishnoi%20and%20Vaisakhi%20Mishra%20and%20Marek%20Suppa%20and%20Noah%20Ziems%20and%20Saiteja%20Utpala%20and%20Ming%20Xu%20and%20Guangyu%20Song%20and%20Kaixin%20Li%20and%20Yuhan%20Cao%20and%20Bo%20Liu%20and%20Zheng%20Liu%20and%20Sabina%20Abdurakhmanova%20and%20Wenhao%20Yu%20and%20Mengzhao%20Jia%20and%20Jihan%20Yao%20and%20Kenneth%20Hamilton%20and%20Kumar%20Shridhar%20and%20Minh%20Chien%20Vu%20and%20Dingmin%20Wang%20and%20Jiawei%20Liu%20and%20Zijian%20Wang%20and%20Qian%20Liu%20and%20Binyuan%20Hui%20and%20Meg%20Risdal%20and%20Ahsen%20Khaliq%20and%20Atin%20Sood%20and%20Zhenchang%20Xing%20and%20Wasi%20Uddin%20Ahmad%20and%20John%20Grundy%20and%20David%20Lo%20and%20Banghua%20Zhu%20and%20Xiaoning%20Du%20and%20Torsten%20Scholak%20and%20Leandro%20von%20Werra%0AAbstract%3A%20Crowdsourced%20model%20evaluation%20platforms%2C%20such%20as%20Chatbot%20Arena%2C%20enable%20real-time%20evaluation%20from%20human%20perspectives%20to%20assess%20the%20quality%20of%20model%20responses.%20In%20the%20coding%20domain%2C%20manually%20examining%20the%20quality%20of%20LLM-generated%20content%20is%20extremely%20challenging%2C%20as%20it%20requires%20understanding%20long%20chunks%20of%20raw%20code%20and%20deliberately%20simulating%20code%20execution.%20To%20this%20end%2C%20we%20introduce%20BigCodeArena%2C%20an%20open%20human%20evaluation%20platform%20for%20code%20generation%20backed%20by%20a%20comprehensive%20and%20on-the-fly%20execution%20environment.%20Built%20on%20top%20of%20Chatbot%20Arena%2C%20BigCodeArena%20enables%20the%20execution%20of%20LLM-generated%20code%20and%20allows%20humans%20to%20interact%20with%20the%20execution%20process%20and%20outcomes.%20We%20collected%20over%2014%2C000%20raw%20code-centric%20conversation%20sessions%20across%2010%20widely%20used%20LLMs%2C%20spanning%2010%20languages%20and%208%20types%20of%20execution%20environments.%20Among%20these%20conversations%2C%20we%20identified%20more%20than%204%2C700%20multi-turn%20samples%20with%20pairwise%20human%20preferences.%20Further%20analysis%20uncovers%20underexplored%20preferences%20of%20LLMs%20in%20fine-grained%20domains%20characterized%20by%20tasks%2C%20languages%2C%20and%20frameworks.%20To%20systematically%20examine%20code%20understanding%20and%20generation%20capabilities%20of%20frontier%20LLMs%2C%20we%20curated%20two%20benchmarks%20based%20on%20the%20collected%20data%2C%20namely%20BigCodeReward%20and%20AutoCodeArena.%20For%20BigCodeReward%2C%20we%20post-processed%20the%204%2C700%20conversations%20and%20evaluated%20the%20consistency%20between%20reward%20models%20and%20human%20preferences.%20The%20evaluation%20shows%20that%20most%20LLMs%20have%20superior%20performance%20in%20judging%20coding%20preferences%20when%20the%20execution%20results%20are%20available.%20Inspired%20by%20these%20findings%2C%20we%20propose%20AutoCodeArena%2C%20an%20automatic%20Elo%20rating%20benchmark%20designed%20to%20assess%20the%20coding%20quality%20of%20LLMs%20without%20human%20involvement.%20We%20find%20that%20proprietary%20LLMs%20like%20GPT-5%2C%20Claude-Sonnet-4%2C%20and%20Claude-Opus-4%20still%20lead%20in%20code%20generation%20performance%20among%20recent%20emerging%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2510.08697v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBigCodeArena%253A%2520Unveiling%2520More%2520Reliable%2520Human%2520Preferences%2520in%2520Code%2520Generation%2520via%2520Execution%26entry.906535625%3DTerry%2520Yue%2520Zhuo%2520and%2520Xiaolong%2520Jin%2520and%2520Hange%2520Liu%2520and%2520Juyong%2520Jiang%2520and%2520Tianyang%2520Liu%2520and%2520Chen%2520Gong%2520and%2520Bhupesh%2520Bishnoi%2520and%2520Vaisakhi%2520Mishra%2520and%2520Marek%2520Suppa%2520and%2520Noah%2520Ziems%2520and%2520Saiteja%2520Utpala%2520and%2520Ming%2520Xu%2520and%2520Guangyu%2520Song%2520and%2520Kaixin%2520Li%2520and%2520Yuhan%2520Cao%2520and%2520Bo%2520Liu%2520and%2520Zheng%2520Liu%2520and%2520Sabina%2520Abdurakhmanova%2520and%2520Wenhao%2520Yu%2520and%2520Mengzhao%2520Jia%2520and%2520Jihan%2520Yao%2520and%2520Kenneth%2520Hamilton%2520and%2520Kumar%2520Shridhar%2520and%2520Minh%2520Chien%2520Vu%2520and%2520Dingmin%2520Wang%2520and%2520Jiawei%2520Liu%2520and%2520Zijian%2520Wang%2520and%2520Qian%2520Liu%2520and%2520Binyuan%2520Hui%2520and%2520Meg%2520Risdal%2520and%2520Ahsen%2520Khaliq%2520and%2520Atin%2520Sood%2520and%2520Zhenchang%2520Xing%2520and%2520Wasi%2520Uddin%2520Ahmad%2520and%2520John%2520Grundy%2520and%2520David%2520Lo%2520and%2520Banghua%2520Zhu%2520and%2520Xiaoning%2520Du%2520and%2520Torsten%2520Scholak%2520and%2520Leandro%2520von%2520Werra%26entry.1292438233%3DCrowdsourced%2520model%2520evaluation%2520platforms%252C%2520such%2520as%2520Chatbot%2520Arena%252C%2520enable%2520real-time%2520evaluation%2520from%2520human%2520perspectives%2520to%2520assess%2520the%2520quality%2520of%2520model%2520responses.%2520In%2520the%2520coding%2520domain%252C%2520manually%2520examining%2520the%2520quality%2520of%2520LLM-generated%2520content%2520is%2520extremely%2520challenging%252C%2520as%2520it%2520requires%2520understanding%2520long%2520chunks%2520of%2520raw%2520code%2520and%2520deliberately%2520simulating%2520code%2520execution.%2520To%2520this%2520end%252C%2520we%2520introduce%2520BigCodeArena%252C%2520an%2520open%2520human%2520evaluation%2520platform%2520for%2520code%2520generation%2520backed%2520by%2520a%2520comprehensive%2520and%2520on-the-fly%2520execution%2520environment.%2520Built%2520on%2520top%2520of%2520Chatbot%2520Arena%252C%2520BigCodeArena%2520enables%2520the%2520execution%2520of%2520LLM-generated%2520code%2520and%2520allows%2520humans%2520to%2520interact%2520with%2520the%2520execution%2520process%2520and%2520outcomes.%2520We%2520collected%2520over%252014%252C000%2520raw%2520code-centric%2520conversation%2520sessions%2520across%252010%2520widely%2520used%2520LLMs%252C%2520spanning%252010%2520languages%2520and%25208%2520types%2520of%2520execution%2520environments.%2520Among%2520these%2520conversations%252C%2520we%2520identified%2520more%2520than%25204%252C700%2520multi-turn%2520samples%2520with%2520pairwise%2520human%2520preferences.%2520Further%2520analysis%2520uncovers%2520underexplored%2520preferences%2520of%2520LLMs%2520in%2520fine-grained%2520domains%2520characterized%2520by%2520tasks%252C%2520languages%252C%2520and%2520frameworks.%2520To%2520systematically%2520examine%2520code%2520understanding%2520and%2520generation%2520capabilities%2520of%2520frontier%2520LLMs%252C%2520we%2520curated%2520two%2520benchmarks%2520based%2520on%2520the%2520collected%2520data%252C%2520namely%2520BigCodeReward%2520and%2520AutoCodeArena.%2520For%2520BigCodeReward%252C%2520we%2520post-processed%2520the%25204%252C700%2520conversations%2520and%2520evaluated%2520the%2520consistency%2520between%2520reward%2520models%2520and%2520human%2520preferences.%2520The%2520evaluation%2520shows%2520that%2520most%2520LLMs%2520have%2520superior%2520performance%2520in%2520judging%2520coding%2520preferences%2520when%2520the%2520execution%2520results%2520are%2520available.%2520Inspired%2520by%2520these%2520findings%252C%2520we%2520propose%2520AutoCodeArena%252C%2520an%2520automatic%2520Elo%2520rating%2520benchmark%2520designed%2520to%2520assess%2520the%2520coding%2520quality%2520of%2520LLMs%2520without%2520human%2520involvement.%2520We%2520find%2520that%2520proprietary%2520LLMs%2520like%2520GPT-5%252C%2520Claude-Sonnet-4%252C%2520and%2520Claude-Opus-4%2520still%2520lead%2520in%2520code%2520generation%2520performance%2520among%2520recent%2520emerging%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.08697v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BigCodeArena%3A%20Unveiling%20More%20Reliable%20Human%20Preferences%20in%20Code%20Generation%20via%20Execution&entry.906535625=Terry%20Yue%20Zhuo%20and%20Xiaolong%20Jin%20and%20Hange%20Liu%20and%20Juyong%20Jiang%20and%20Tianyang%20Liu%20and%20Chen%20Gong%20and%20Bhupesh%20Bishnoi%20and%20Vaisakhi%20Mishra%20and%20Marek%20Suppa%20and%20Noah%20Ziems%20and%20Saiteja%20Utpala%20and%20Ming%20Xu%20and%20Guangyu%20Song%20and%20Kaixin%20Li%20and%20Yuhan%20Cao%20and%20Bo%20Liu%20and%20Zheng%20Liu%20and%20Sabina%20Abdurakhmanova%20and%20Wenhao%20Yu%20and%20Mengzhao%20Jia%20and%20Jihan%20Yao%20and%20Kenneth%20Hamilton%20and%20Kumar%20Shridhar%20and%20Minh%20Chien%20Vu%20and%20Dingmin%20Wang%20and%20Jiawei%20Liu%20and%20Zijian%20Wang%20and%20Qian%20Liu%20and%20Binyuan%20Hui%20and%20Meg%20Risdal%20and%20Ahsen%20Khaliq%20and%20Atin%20Sood%20and%20Zhenchang%20Xing%20and%20Wasi%20Uddin%20Ahmad%20and%20John%20Grundy%20and%20David%20Lo%20and%20Banghua%20Zhu%20and%20Xiaoning%20Du%20and%20Torsten%20Scholak%20and%20Leandro%20von%20Werra&entry.1292438233=Crowdsourced%20model%20evaluation%20platforms%2C%20such%20as%20Chatbot%20Arena%2C%20enable%20real-time%20evaluation%20from%20human%20perspectives%20to%20assess%20the%20quality%20of%20model%20responses.%20In%20the%20coding%20domain%2C%20manually%20examining%20the%20quality%20of%20LLM-generated%20content%20is%20extremely%20challenging%2C%20as%20it%20requires%20understanding%20long%20chunks%20of%20raw%20code%20and%20deliberately%20simulating%20code%20execution.%20To%20this%20end%2C%20we%20introduce%20BigCodeArena%2C%20an%20open%20human%20evaluation%20platform%20for%20code%20generation%20backed%20by%20a%20comprehensive%20and%20on-the-fly%20execution%20environment.%20Built%20on%20top%20of%20Chatbot%20Arena%2C%20BigCodeArena%20enables%20the%20execution%20of%20LLM-generated%20code%20and%20allows%20humans%20to%20interact%20with%20the%20execution%20process%20and%20outcomes.%20We%20collected%20over%2014%2C000%20raw%20code-centric%20conversation%20sessions%20across%2010%20widely%20used%20LLMs%2C%20spanning%2010%20languages%20and%208%20types%20of%20execution%20environments.%20Among%20these%20conversations%2C%20we%20identified%20more%20than%204%2C700%20multi-turn%20samples%20with%20pairwise%20human%20preferences.%20Further%20analysis%20uncovers%20underexplored%20preferences%20of%20LLMs%20in%20fine-grained%20domains%20characterized%20by%20tasks%2C%20languages%2C%20and%20frameworks.%20To%20systematically%20examine%20code%20understanding%20and%20generation%20capabilities%20of%20frontier%20LLMs%2C%20we%20curated%20two%20benchmarks%20based%20on%20the%20collected%20data%2C%20namely%20BigCodeReward%20and%20AutoCodeArena.%20For%20BigCodeReward%2C%20we%20post-processed%20the%204%2C700%20conversations%20and%20evaluated%20the%20consistency%20between%20reward%20models%20and%20human%20preferences.%20The%20evaluation%20shows%20that%20most%20LLMs%20have%20superior%20performance%20in%20judging%20coding%20preferences%20when%20the%20execution%20results%20are%20available.%20Inspired%20by%20these%20findings%2C%20we%20propose%20AutoCodeArena%2C%20an%20automatic%20Elo%20rating%20benchmark%20designed%20to%20assess%20the%20coding%20quality%20of%20LLMs%20without%20human%20involvement.%20We%20find%20that%20proprietary%20LLMs%20like%20GPT-5%2C%20Claude-Sonnet-4%2C%20and%20Claude-Opus-4%20still%20lead%20in%20code%20generation%20performance%20among%20recent%20emerging%20models.&entry.1838667208=http%3A//arxiv.org/abs/2510.08697v2&entry.124074799=Read"},
{"title": "Prefix Probing: Lightweight Harmful Content Detection for Large Language Models", "author": "Jirui Yang and Hengqi Guo and Zhihui Lu and Yi Zhao and Yuansen Zhang and Shijing Hu and Qiang Duan and Yinggui Wang and Tao Wei", "abstract": "Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of \"agreement/execution\" versus \"refusal/safety\" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.", "link": "http://arxiv.org/abs/2512.16650v1", "date": "2025-12-18", "relevancy": 2.4903, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5072}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5072}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prefix%20Probing%3A%20Lightweight%20Harmful%20Content%20Detection%20for%20Large%20Language%20Models&body=Title%3A%20Prefix%20Probing%3A%20Lightweight%20Harmful%20Content%20Detection%20for%20Large%20Language%20Models%0AAuthor%3A%20Jirui%20Yang%20and%20Hengqi%20Guo%20and%20Zhihui%20Lu%20and%20Yi%20Zhao%20and%20Yuansen%20Zhang%20and%20Shijing%20Hu%20and%20Qiang%20Duan%20and%20Yinggui%20Wang%20and%20Tao%20Wei%0AAbstract%3A%20Large%20language%20models%20often%20face%20a%20three-way%20trade-off%20among%20detection%20accuracy%2C%20inference%20latency%2C%20and%20deployment%20cost%20when%20used%20in%20real-world%20safety-sensitive%20applications.%20This%20paper%20introduces%20Prefix%20Probing%2C%20a%20black-box%20harmful%20content%20detection%20method%20that%20compares%20the%20conditional%20log-probabilities%20of%20%22agreement/execution%22%20versus%20%22refusal/safety%22%20opening%20prefixes%20and%20leverages%20prefix%20caching%20to%20reduce%20detection%20overhead%20to%20near%20first-token%20latency.%20During%20inference%2C%20the%20method%20requires%20only%20a%20single%20log-probability%20computation%20over%20the%20probe%20prefixes%20to%20produce%20a%20harmfulness%20score%20and%20apply%20a%20threshold%2C%20without%20invoking%20any%20additional%20models%20or%20multi-stage%20inference.%20To%20further%20enhance%20the%20discriminative%20power%20of%20the%20prefixes%2C%20we%20design%20an%20efficient%20prefix%20construction%20algorithm%20that%20automatically%20discovers%20highly%20informative%20prefixes%2C%20substantially%20improving%20detection%20performance.%20Extensive%20experiments%20demonstrate%20that%20Prefix%20Probing%20achieves%20detection%20effectiveness%20comparable%20to%20mainstream%20external%20safety%20models%20while%20incurring%20only%20minimal%20computational%20cost%20and%20requiring%20no%20extra%20model%20deployment%2C%20highlighting%20its%20strong%20practicality%20and%20efficiency.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16650v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrefix%2520Probing%253A%2520Lightweight%2520Harmful%2520Content%2520Detection%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DJirui%2520Yang%2520and%2520Hengqi%2520Guo%2520and%2520Zhihui%2520Lu%2520and%2520Yi%2520Zhao%2520and%2520Yuansen%2520Zhang%2520and%2520Shijing%2520Hu%2520and%2520Qiang%2520Duan%2520and%2520Yinggui%2520Wang%2520and%2520Tao%2520Wei%26entry.1292438233%3DLarge%2520language%2520models%2520often%2520face%2520a%2520three-way%2520trade-off%2520among%2520detection%2520accuracy%252C%2520inference%2520latency%252C%2520and%2520deployment%2520cost%2520when%2520used%2520in%2520real-world%2520safety-sensitive%2520applications.%2520This%2520paper%2520introduces%2520Prefix%2520Probing%252C%2520a%2520black-box%2520harmful%2520content%2520detection%2520method%2520that%2520compares%2520the%2520conditional%2520log-probabilities%2520of%2520%2522agreement/execution%2522%2520versus%2520%2522refusal/safety%2522%2520opening%2520prefixes%2520and%2520leverages%2520prefix%2520caching%2520to%2520reduce%2520detection%2520overhead%2520to%2520near%2520first-token%2520latency.%2520During%2520inference%252C%2520the%2520method%2520requires%2520only%2520a%2520single%2520log-probability%2520computation%2520over%2520the%2520probe%2520prefixes%2520to%2520produce%2520a%2520harmfulness%2520score%2520and%2520apply%2520a%2520threshold%252C%2520without%2520invoking%2520any%2520additional%2520models%2520or%2520multi-stage%2520inference.%2520To%2520further%2520enhance%2520the%2520discriminative%2520power%2520of%2520the%2520prefixes%252C%2520we%2520design%2520an%2520efficient%2520prefix%2520construction%2520algorithm%2520that%2520automatically%2520discovers%2520highly%2520informative%2520prefixes%252C%2520substantially%2520improving%2520detection%2520performance.%2520Extensive%2520experiments%2520demonstrate%2520that%2520Prefix%2520Probing%2520achieves%2520detection%2520effectiveness%2520comparable%2520to%2520mainstream%2520external%2520safety%2520models%2520while%2520incurring%2520only%2520minimal%2520computational%2520cost%2520and%2520requiring%2520no%2520extra%2520model%2520deployment%252C%2520highlighting%2520its%2520strong%2520practicality%2520and%2520efficiency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16650v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prefix%20Probing%3A%20Lightweight%20Harmful%20Content%20Detection%20for%20Large%20Language%20Models&entry.906535625=Jirui%20Yang%20and%20Hengqi%20Guo%20and%20Zhihui%20Lu%20and%20Yi%20Zhao%20and%20Yuansen%20Zhang%20and%20Shijing%20Hu%20and%20Qiang%20Duan%20and%20Yinggui%20Wang%20and%20Tao%20Wei&entry.1292438233=Large%20language%20models%20often%20face%20a%20three-way%20trade-off%20among%20detection%20accuracy%2C%20inference%20latency%2C%20and%20deployment%20cost%20when%20used%20in%20real-world%20safety-sensitive%20applications.%20This%20paper%20introduces%20Prefix%20Probing%2C%20a%20black-box%20harmful%20content%20detection%20method%20that%20compares%20the%20conditional%20log-probabilities%20of%20%22agreement/execution%22%20versus%20%22refusal/safety%22%20opening%20prefixes%20and%20leverages%20prefix%20caching%20to%20reduce%20detection%20overhead%20to%20near%20first-token%20latency.%20During%20inference%2C%20the%20method%20requires%20only%20a%20single%20log-probability%20computation%20over%20the%20probe%20prefixes%20to%20produce%20a%20harmfulness%20score%20and%20apply%20a%20threshold%2C%20without%20invoking%20any%20additional%20models%20or%20multi-stage%20inference.%20To%20further%20enhance%20the%20discriminative%20power%20of%20the%20prefixes%2C%20we%20design%20an%20efficient%20prefix%20construction%20algorithm%20that%20automatically%20discovers%20highly%20informative%20prefixes%2C%20substantially%20improving%20detection%20performance.%20Extensive%20experiments%20demonstrate%20that%20Prefix%20Probing%20achieves%20detection%20effectiveness%20comparable%20to%20mainstream%20external%20safety%20models%20while%20incurring%20only%20minimal%20computational%20cost%20and%20requiring%20no%20extra%20model%20deployment%2C%20highlighting%20its%20strong%20practicality%20and%20efficiency.&entry.1838667208=http%3A//arxiv.org/abs/2512.16650v1&entry.124074799=Read"},
{"title": "TACE: A unified Irreducible Cartesian Tensor Framework for Atomistic Machine Learning", "author": "Zemin Xu and Wenbo Xie and Daiqian Xie and P. Hu", "abstract": "Here, we introduce the Tensor Atomic Cluster Expansion (TACE), a unified framework formulated entirely in Cartesian space, enabling systematic and consistent prediction of arbitrary structure-dependent tensorial properties. TACE achieves this by decomposing atomic environments into a complete hierarchy of irreducible Cartesian tensors, ensuring symmetry-consistent representations that naturally encode invariance and equivariance constraints. Beyond geometry, TACE incorporates universal embeddings that flexibly integrate diverse attributes including computational levels, charges, magnetic moments and field perturbations. This allows explicit control over external invariants and equivariants in the prediction process. Long-range interactions are also accurately described through the Latent Ewald Summation module within the short-range approximation, providing a rigorous yet computationally efficient treatment of electrostatic and dispersion effects. We demonstrate that TACE attains accuracy, stability, and efficiency on par with or surpassing leading equivariant frameworks across finite molecules and extended materials. This includes in-domain and out-of-domain benchmarks, spectra, Hessian, external-field responses, charged and magnetic systems, multi-fidelity training, heterogeneous catalysis, and even superior performance within the uMLIP benchmark. Crucially, TACE bridges scalar and tensorial modeling and establishes a Cartesian-space paradigm that unifies and extends beyond the design space of spherical-tensor-based methods. This work lays the foundation for a new generation of universal atomistic machine learning models capable of systematically capturing the rich interplay of geometry, fields and material properties within a single coherent framework.", "link": "http://arxiv.org/abs/2509.14961v2", "date": "2025-12-18", "relevancy": 2.4843, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5311}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4797}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TACE%3A%20A%20unified%20Irreducible%20Cartesian%20Tensor%20Framework%20for%20Atomistic%20Machine%20Learning&body=Title%3A%20TACE%3A%20A%20unified%20Irreducible%20Cartesian%20Tensor%20Framework%20for%20Atomistic%20Machine%20Learning%0AAuthor%3A%20Zemin%20Xu%20and%20Wenbo%20Xie%20and%20Daiqian%20Xie%20and%20P.%20Hu%0AAbstract%3A%20Here%2C%20we%20introduce%20the%20Tensor%20Atomic%20Cluster%20Expansion%20%28TACE%29%2C%20a%20unified%20framework%20formulated%20entirely%20in%20Cartesian%20space%2C%20enabling%20systematic%20and%20consistent%20prediction%20of%20arbitrary%20structure-dependent%20tensorial%20properties.%20TACE%20achieves%20this%20by%20decomposing%20atomic%20environments%20into%20a%20complete%20hierarchy%20of%20irreducible%20Cartesian%20tensors%2C%20ensuring%20symmetry-consistent%20representations%20that%20naturally%20encode%20invariance%20and%20equivariance%20constraints.%20Beyond%20geometry%2C%20TACE%20incorporates%20universal%20embeddings%20that%20flexibly%20integrate%20diverse%20attributes%20including%20computational%20levels%2C%20charges%2C%20magnetic%20moments%20and%20field%20perturbations.%20This%20allows%20explicit%20control%20over%20external%20invariants%20and%20equivariants%20in%20the%20prediction%20process.%20Long-range%20interactions%20are%20also%20accurately%20described%20through%20the%20Latent%20Ewald%20Summation%20module%20within%20the%20short-range%20approximation%2C%20providing%20a%20rigorous%20yet%20computationally%20efficient%20treatment%20of%20electrostatic%20and%20dispersion%20effects.%20We%20demonstrate%20that%20TACE%20attains%20accuracy%2C%20stability%2C%20and%20efficiency%20on%20par%20with%20or%20surpassing%20leading%20equivariant%20frameworks%20across%20finite%20molecules%20and%20extended%20materials.%20This%20includes%20in-domain%20and%20out-of-domain%20benchmarks%2C%20spectra%2C%20Hessian%2C%20external-field%20responses%2C%20charged%20and%20magnetic%20systems%2C%20multi-fidelity%20training%2C%20heterogeneous%20catalysis%2C%20and%20even%20superior%20performance%20within%20the%20uMLIP%20benchmark.%20Crucially%2C%20TACE%20bridges%20scalar%20and%20tensorial%20modeling%20and%20establishes%20a%20Cartesian-space%20paradigm%20that%20unifies%20and%20extends%20beyond%20the%20design%20space%20of%20spherical-tensor-based%20methods.%20This%20work%20lays%20the%20foundation%20for%20a%20new%20generation%20of%20universal%20atomistic%20machine%20learning%20models%20capable%20of%20systematically%20capturing%20the%20rich%20interplay%20of%20geometry%2C%20fields%20and%20material%20properties%20within%20a%20single%20coherent%20framework.%0ALink%3A%20http%3A//arxiv.org/abs/2509.14961v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTACE%253A%2520A%2520unified%2520Irreducible%2520Cartesian%2520Tensor%2520Framework%2520for%2520Atomistic%2520Machine%2520Learning%26entry.906535625%3DZemin%2520Xu%2520and%2520Wenbo%2520Xie%2520and%2520Daiqian%2520Xie%2520and%2520P.%2520Hu%26entry.1292438233%3DHere%252C%2520we%2520introduce%2520the%2520Tensor%2520Atomic%2520Cluster%2520Expansion%2520%2528TACE%2529%252C%2520a%2520unified%2520framework%2520formulated%2520entirely%2520in%2520Cartesian%2520space%252C%2520enabling%2520systematic%2520and%2520consistent%2520prediction%2520of%2520arbitrary%2520structure-dependent%2520tensorial%2520properties.%2520TACE%2520achieves%2520this%2520by%2520decomposing%2520atomic%2520environments%2520into%2520a%2520complete%2520hierarchy%2520of%2520irreducible%2520Cartesian%2520tensors%252C%2520ensuring%2520symmetry-consistent%2520representations%2520that%2520naturally%2520encode%2520invariance%2520and%2520equivariance%2520constraints.%2520Beyond%2520geometry%252C%2520TACE%2520incorporates%2520universal%2520embeddings%2520that%2520flexibly%2520integrate%2520diverse%2520attributes%2520including%2520computational%2520levels%252C%2520charges%252C%2520magnetic%2520moments%2520and%2520field%2520perturbations.%2520This%2520allows%2520explicit%2520control%2520over%2520external%2520invariants%2520and%2520equivariants%2520in%2520the%2520prediction%2520process.%2520Long-range%2520interactions%2520are%2520also%2520accurately%2520described%2520through%2520the%2520Latent%2520Ewald%2520Summation%2520module%2520within%2520the%2520short-range%2520approximation%252C%2520providing%2520a%2520rigorous%2520yet%2520computationally%2520efficient%2520treatment%2520of%2520electrostatic%2520and%2520dispersion%2520effects.%2520We%2520demonstrate%2520that%2520TACE%2520attains%2520accuracy%252C%2520stability%252C%2520and%2520efficiency%2520on%2520par%2520with%2520or%2520surpassing%2520leading%2520equivariant%2520frameworks%2520across%2520finite%2520molecules%2520and%2520extended%2520materials.%2520This%2520includes%2520in-domain%2520and%2520out-of-domain%2520benchmarks%252C%2520spectra%252C%2520Hessian%252C%2520external-field%2520responses%252C%2520charged%2520and%2520magnetic%2520systems%252C%2520multi-fidelity%2520training%252C%2520heterogeneous%2520catalysis%252C%2520and%2520even%2520superior%2520performance%2520within%2520the%2520uMLIP%2520benchmark.%2520Crucially%252C%2520TACE%2520bridges%2520scalar%2520and%2520tensorial%2520modeling%2520and%2520establishes%2520a%2520Cartesian-space%2520paradigm%2520that%2520unifies%2520and%2520extends%2520beyond%2520the%2520design%2520space%2520of%2520spherical-tensor-based%2520methods.%2520This%2520work%2520lays%2520the%2520foundation%2520for%2520a%2520new%2520generation%2520of%2520universal%2520atomistic%2520machine%2520learning%2520models%2520capable%2520of%2520systematically%2520capturing%2520the%2520rich%2520interplay%2520of%2520geometry%252C%2520fields%2520and%2520material%2520properties%2520within%2520a%2520single%2520coherent%2520framework.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14961v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TACE%3A%20A%20unified%20Irreducible%20Cartesian%20Tensor%20Framework%20for%20Atomistic%20Machine%20Learning&entry.906535625=Zemin%20Xu%20and%20Wenbo%20Xie%20and%20Daiqian%20Xie%20and%20P.%20Hu&entry.1292438233=Here%2C%20we%20introduce%20the%20Tensor%20Atomic%20Cluster%20Expansion%20%28TACE%29%2C%20a%20unified%20framework%20formulated%20entirely%20in%20Cartesian%20space%2C%20enabling%20systematic%20and%20consistent%20prediction%20of%20arbitrary%20structure-dependent%20tensorial%20properties.%20TACE%20achieves%20this%20by%20decomposing%20atomic%20environments%20into%20a%20complete%20hierarchy%20of%20irreducible%20Cartesian%20tensors%2C%20ensuring%20symmetry-consistent%20representations%20that%20naturally%20encode%20invariance%20and%20equivariance%20constraints.%20Beyond%20geometry%2C%20TACE%20incorporates%20universal%20embeddings%20that%20flexibly%20integrate%20diverse%20attributes%20including%20computational%20levels%2C%20charges%2C%20magnetic%20moments%20and%20field%20perturbations.%20This%20allows%20explicit%20control%20over%20external%20invariants%20and%20equivariants%20in%20the%20prediction%20process.%20Long-range%20interactions%20are%20also%20accurately%20described%20through%20the%20Latent%20Ewald%20Summation%20module%20within%20the%20short-range%20approximation%2C%20providing%20a%20rigorous%20yet%20computationally%20efficient%20treatment%20of%20electrostatic%20and%20dispersion%20effects.%20We%20demonstrate%20that%20TACE%20attains%20accuracy%2C%20stability%2C%20and%20efficiency%20on%20par%20with%20or%20surpassing%20leading%20equivariant%20frameworks%20across%20finite%20molecules%20and%20extended%20materials.%20This%20includes%20in-domain%20and%20out-of-domain%20benchmarks%2C%20spectra%2C%20Hessian%2C%20external-field%20responses%2C%20charged%20and%20magnetic%20systems%2C%20multi-fidelity%20training%2C%20heterogeneous%20catalysis%2C%20and%20even%20superior%20performance%20within%20the%20uMLIP%20benchmark.%20Crucially%2C%20TACE%20bridges%20scalar%20and%20tensorial%20modeling%20and%20establishes%20a%20Cartesian-space%20paradigm%20that%20unifies%20and%20extends%20beyond%20the%20design%20space%20of%20spherical-tensor-based%20methods.%20This%20work%20lays%20the%20foundation%20for%20a%20new%20generation%20of%20universal%20atomistic%20machine%20learning%20models%20capable%20of%20systematically%20capturing%20the%20rich%20interplay%20of%20geometry%2C%20fields%20and%20material%20properties%20within%20a%20single%20coherent%20framework.&entry.1838667208=http%3A//arxiv.org/abs/2509.14961v2&entry.124074799=Read"},
{"title": "Exploiting Radio Frequency Fingerprints for Device Identification: Tackling Cross-receiver Challenges in the Source-data-free Scenario", "author": "Liu Yang and Qiang Li and Luxiong Wen and Jian Yang", "abstract": "With the rapid proliferation of edge computing, Radio Frequency Fingerprint Identification (RFFI) has become increasingly important for secure device authentication. However, practical deployment of deep learning-based RFFI models is hindered by a critical challenge: their performance often degrades significantly when applied across receivers with different hardware characteristics due to distribution shifts introduced by receiver variation. To address this, we investigate the source-data-free cross-receiver RFFI (SCRFFI) problem, where a model pretrained on labeled signals from a source receiver must adapt to unlabeled signals from a target receiver, without access to any source-domain data during adaptation. We first formulate a novel constrained pseudo-labeling-based SCRFFI adaptation framework, and provide a theoretical analysis of its generalization performance. Our analysis highlights a key insight: the target-domain performance is highly sensitive to the quality of the pseudo-labels generated during adaptation. Motivated by this, we propose Momentum Soft pseudo-label Source Hypothesis Transfer (MS-SHOT), a new method for SCRFFI that incorporates momentum-center-guided soft pseudo-labeling and enforces global structural constraints to encourage confident and diverse predictions. Notably, MS-SHOT effectively addresses scenarios involving label shift or unknown, non-uniform class distributions in the target domain -- a significant limitation of prior methods. Extensive experiments on real-world datasets demonstrate that MS-SHOT consistently outperforms existing approaches in both accuracy and robustness, offering a practical and scalable solution for source-data-free cross-receiver adaptation in RFFI.", "link": "http://arxiv.org/abs/2512.16648v1", "date": "2025-12-18", "relevancy": 2.48, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5133}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4885}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Radio%20Frequency%20Fingerprints%20for%20Device%20Identification%3A%20Tackling%20Cross-receiver%20Challenges%20in%20the%20Source-data-free%20Scenario&body=Title%3A%20Exploiting%20Radio%20Frequency%20Fingerprints%20for%20Device%20Identification%3A%20Tackling%20Cross-receiver%20Challenges%20in%20the%20Source-data-free%20Scenario%0AAuthor%3A%20Liu%20Yang%20and%20Qiang%20Li%20and%20Luxiong%20Wen%20and%20Jian%20Yang%0AAbstract%3A%20With%20the%20rapid%20proliferation%20of%20edge%20computing%2C%20Radio%20Frequency%20Fingerprint%20Identification%20%28RFFI%29%20has%20become%20increasingly%20important%20for%20secure%20device%20authentication.%20However%2C%20practical%20deployment%20of%20deep%20learning-based%20RFFI%20models%20is%20hindered%20by%20a%20critical%20challenge%3A%20their%20performance%20often%20degrades%20significantly%20when%20applied%20across%20receivers%20with%20different%20hardware%20characteristics%20due%20to%20distribution%20shifts%20introduced%20by%20receiver%20variation.%20To%20address%20this%2C%20we%20investigate%20the%20source-data-free%20cross-receiver%20RFFI%20%28SCRFFI%29%20problem%2C%20where%20a%20model%20pretrained%20on%20labeled%20signals%20from%20a%20source%20receiver%20must%20adapt%20to%20unlabeled%20signals%20from%20a%20target%20receiver%2C%20without%20access%20to%20any%20source-domain%20data%20during%20adaptation.%20We%20first%20formulate%20a%20novel%20constrained%20pseudo-labeling-based%20SCRFFI%20adaptation%20framework%2C%20and%20provide%20a%20theoretical%20analysis%20of%20its%20generalization%20performance.%20Our%20analysis%20highlights%20a%20key%20insight%3A%20the%20target-domain%20performance%20is%20highly%20sensitive%20to%20the%20quality%20of%20the%20pseudo-labels%20generated%20during%20adaptation.%20Motivated%20by%20this%2C%20we%20propose%20Momentum%20Soft%20pseudo-label%20Source%20Hypothesis%20Transfer%20%28MS-SHOT%29%2C%20a%20new%20method%20for%20SCRFFI%20that%20incorporates%20momentum-center-guided%20soft%20pseudo-labeling%20and%20enforces%20global%20structural%20constraints%20to%20encourage%20confident%20and%20diverse%20predictions.%20Notably%2C%20MS-SHOT%20effectively%20addresses%20scenarios%20involving%20label%20shift%20or%20unknown%2C%20non-uniform%20class%20distributions%20in%20the%20target%20domain%20--%20a%20significant%20limitation%20of%20prior%20methods.%20Extensive%20experiments%20on%20real-world%20datasets%20demonstrate%20that%20MS-SHOT%20consistently%20outperforms%20existing%20approaches%20in%20both%20accuracy%20and%20robustness%2C%20offering%20a%20practical%20and%20scalable%20solution%20for%20source-data-free%20cross-receiver%20adaptation%20in%20RFFI.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16648v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520Radio%2520Frequency%2520Fingerprints%2520for%2520Device%2520Identification%253A%2520Tackling%2520Cross-receiver%2520Challenges%2520in%2520the%2520Source-data-free%2520Scenario%26entry.906535625%3DLiu%2520Yang%2520and%2520Qiang%2520Li%2520and%2520Luxiong%2520Wen%2520and%2520Jian%2520Yang%26entry.1292438233%3DWith%2520the%2520rapid%2520proliferation%2520of%2520edge%2520computing%252C%2520Radio%2520Frequency%2520Fingerprint%2520Identification%2520%2528RFFI%2529%2520has%2520become%2520increasingly%2520important%2520for%2520secure%2520device%2520authentication.%2520However%252C%2520practical%2520deployment%2520of%2520deep%2520learning-based%2520RFFI%2520models%2520is%2520hindered%2520by%2520a%2520critical%2520challenge%253A%2520their%2520performance%2520often%2520degrades%2520significantly%2520when%2520applied%2520across%2520receivers%2520with%2520different%2520hardware%2520characteristics%2520due%2520to%2520distribution%2520shifts%2520introduced%2520by%2520receiver%2520variation.%2520To%2520address%2520this%252C%2520we%2520investigate%2520the%2520source-data-free%2520cross-receiver%2520RFFI%2520%2528SCRFFI%2529%2520problem%252C%2520where%2520a%2520model%2520pretrained%2520on%2520labeled%2520signals%2520from%2520a%2520source%2520receiver%2520must%2520adapt%2520to%2520unlabeled%2520signals%2520from%2520a%2520target%2520receiver%252C%2520without%2520access%2520to%2520any%2520source-domain%2520data%2520during%2520adaptation.%2520We%2520first%2520formulate%2520a%2520novel%2520constrained%2520pseudo-labeling-based%2520SCRFFI%2520adaptation%2520framework%252C%2520and%2520provide%2520a%2520theoretical%2520analysis%2520of%2520its%2520generalization%2520performance.%2520Our%2520analysis%2520highlights%2520a%2520key%2520insight%253A%2520the%2520target-domain%2520performance%2520is%2520highly%2520sensitive%2520to%2520the%2520quality%2520of%2520the%2520pseudo-labels%2520generated%2520during%2520adaptation.%2520Motivated%2520by%2520this%252C%2520we%2520propose%2520Momentum%2520Soft%2520pseudo-label%2520Source%2520Hypothesis%2520Transfer%2520%2528MS-SHOT%2529%252C%2520a%2520new%2520method%2520for%2520SCRFFI%2520that%2520incorporates%2520momentum-center-guided%2520soft%2520pseudo-labeling%2520and%2520enforces%2520global%2520structural%2520constraints%2520to%2520encourage%2520confident%2520and%2520diverse%2520predictions.%2520Notably%252C%2520MS-SHOT%2520effectively%2520addresses%2520scenarios%2520involving%2520label%2520shift%2520or%2520unknown%252C%2520non-uniform%2520class%2520distributions%2520in%2520the%2520target%2520domain%2520--%2520a%2520significant%2520limitation%2520of%2520prior%2520methods.%2520Extensive%2520experiments%2520on%2520real-world%2520datasets%2520demonstrate%2520that%2520MS-SHOT%2520consistently%2520outperforms%2520existing%2520approaches%2520in%2520both%2520accuracy%2520and%2520robustness%252C%2520offering%2520a%2520practical%2520and%2520scalable%2520solution%2520for%2520source-data-free%2520cross-receiver%2520adaptation%2520in%2520RFFI.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16648v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Radio%20Frequency%20Fingerprints%20for%20Device%20Identification%3A%20Tackling%20Cross-receiver%20Challenges%20in%20the%20Source-data-free%20Scenario&entry.906535625=Liu%20Yang%20and%20Qiang%20Li%20and%20Luxiong%20Wen%20and%20Jian%20Yang&entry.1292438233=With%20the%20rapid%20proliferation%20of%20edge%20computing%2C%20Radio%20Frequency%20Fingerprint%20Identification%20%28RFFI%29%20has%20become%20increasingly%20important%20for%20secure%20device%20authentication.%20However%2C%20practical%20deployment%20of%20deep%20learning-based%20RFFI%20models%20is%20hindered%20by%20a%20critical%20challenge%3A%20their%20performance%20often%20degrades%20significantly%20when%20applied%20across%20receivers%20with%20different%20hardware%20characteristics%20due%20to%20distribution%20shifts%20introduced%20by%20receiver%20variation.%20To%20address%20this%2C%20we%20investigate%20the%20source-data-free%20cross-receiver%20RFFI%20%28SCRFFI%29%20problem%2C%20where%20a%20model%20pretrained%20on%20labeled%20signals%20from%20a%20source%20receiver%20must%20adapt%20to%20unlabeled%20signals%20from%20a%20target%20receiver%2C%20without%20access%20to%20any%20source-domain%20data%20during%20adaptation.%20We%20first%20formulate%20a%20novel%20constrained%20pseudo-labeling-based%20SCRFFI%20adaptation%20framework%2C%20and%20provide%20a%20theoretical%20analysis%20of%20its%20generalization%20performance.%20Our%20analysis%20highlights%20a%20key%20insight%3A%20the%20target-domain%20performance%20is%20highly%20sensitive%20to%20the%20quality%20of%20the%20pseudo-labels%20generated%20during%20adaptation.%20Motivated%20by%20this%2C%20we%20propose%20Momentum%20Soft%20pseudo-label%20Source%20Hypothesis%20Transfer%20%28MS-SHOT%29%2C%20a%20new%20method%20for%20SCRFFI%20that%20incorporates%20momentum-center-guided%20soft%20pseudo-labeling%20and%20enforces%20global%20structural%20constraints%20to%20encourage%20confident%20and%20diverse%20predictions.%20Notably%2C%20MS-SHOT%20effectively%20addresses%20scenarios%20involving%20label%20shift%20or%20unknown%2C%20non-uniform%20class%20distributions%20in%20the%20target%20domain%20--%20a%20significant%20limitation%20of%20prior%20methods.%20Extensive%20experiments%20on%20real-world%20datasets%20demonstrate%20that%20MS-SHOT%20consistently%20outperforms%20existing%20approaches%20in%20both%20accuracy%20and%20robustness%2C%20offering%20a%20practical%20and%20scalable%20solution%20for%20source-data-free%20cross-receiver%20adaptation%20in%20RFFI.&entry.1838667208=http%3A//arxiv.org/abs/2512.16648v1&entry.124074799=Read"},
{"title": "Synthetic Electrogram Generation with Variational Autoencoders for ECGI", "author": "Miriam Guti\u00e9rrez-Fern\u00e1ndez and Karen L\u00f3pez-Linares and Carlos Fambuena-Santos and Mar\u00eda S. Guillem and Andreu M. Climent and \u00d3scar Barquero-P\u00e9rez", "abstract": "Atrial fibrillation (AF) is the most prevalent sustained cardiac arrhythmia, and its clinical assessment requires accurate characterization of atrial electrical activity. Noninvasive electrocardiographic imaging (ECGI) combined with deep learning (DL) approaches for estimating intracardiac electrograms (EGMs) from body surface potentials (BSPMs) has shown promise, but progress is hindered by the limited availability of paired BSPM-EGM datasets. To address this limitation, we investigate variational autoencoders (VAEs) for the generation of synthetic multichannel atrial EGMs. Two models are proposed: a sinus rhythm-specific VAE (VAE-S) and a class-conditioned VAE (VAE-C) trained on both sinus rhythm and AF signals. Generated EGMs are evaluated using morphological, spectral, and distributional similarity metrics. VAE-S achieves higher fidelity with respect to in silico EGMs, while VAE-C enables rhythm-specific generation at the expense of reduced sinus reconstruction quality. As a proof of concept, the generated EGMs are used for data augmentation in a downstream noninvasive EGM reconstruction task, where moderate augmentation improves estimation performance. These results demonstrate the potential of VAE-based generative modeling to alleviate data scarcity and enhance deep learning-based ECGI pipelines.", "link": "http://arxiv.org/abs/2512.14537v2", "date": "2025-12-18", "relevancy": 2.4789, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5163}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4872}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthetic%20Electrogram%20Generation%20with%20Variational%20Autoencoders%20for%20ECGI&body=Title%3A%20Synthetic%20Electrogram%20Generation%20with%20Variational%20Autoencoders%20for%20ECGI%0AAuthor%3A%20Miriam%20Guti%C3%A9rrez-Fern%C3%A1ndez%20and%20Karen%20L%C3%B3pez-Linares%20and%20Carlos%20Fambuena-Santos%20and%20Mar%C3%ADa%20S.%20Guillem%20and%20Andreu%20M.%20Climent%20and%20%C3%93scar%20Barquero-P%C3%A9rez%0AAbstract%3A%20Atrial%20fibrillation%20%28AF%29%20is%20the%20most%20prevalent%20sustained%20cardiac%20arrhythmia%2C%20and%20its%20clinical%20assessment%20requires%20accurate%20characterization%20of%20atrial%20electrical%20activity.%20Noninvasive%20electrocardiographic%20imaging%20%28ECGI%29%20combined%20with%20deep%20learning%20%28DL%29%20approaches%20for%20estimating%20intracardiac%20electrograms%20%28EGMs%29%20from%20body%20surface%20potentials%20%28BSPMs%29%20has%20shown%20promise%2C%20but%20progress%20is%20hindered%20by%20the%20limited%20availability%20of%20paired%20BSPM-EGM%20datasets.%20To%20address%20this%20limitation%2C%20we%20investigate%20variational%20autoencoders%20%28VAEs%29%20for%20the%20generation%20of%20synthetic%20multichannel%20atrial%20EGMs.%20Two%20models%20are%20proposed%3A%20a%20sinus%20rhythm-specific%20VAE%20%28VAE-S%29%20and%20a%20class-conditioned%20VAE%20%28VAE-C%29%20trained%20on%20both%20sinus%20rhythm%20and%20AF%20signals.%20Generated%20EGMs%20are%20evaluated%20using%20morphological%2C%20spectral%2C%20and%20distributional%20similarity%20metrics.%20VAE-S%20achieves%20higher%20fidelity%20with%20respect%20to%20in%20silico%20EGMs%2C%20while%20VAE-C%20enables%20rhythm-specific%20generation%20at%20the%20expense%20of%20reduced%20sinus%20reconstruction%20quality.%20As%20a%20proof%20of%20concept%2C%20the%20generated%20EGMs%20are%20used%20for%20data%20augmentation%20in%20a%20downstream%20noninvasive%20EGM%20reconstruction%20task%2C%20where%20moderate%20augmentation%20improves%20estimation%20performance.%20These%20results%20demonstrate%20the%20potential%20of%20VAE-based%20generative%20modeling%20to%20alleviate%20data%20scarcity%20and%20enhance%20deep%20learning-based%20ECGI%20pipelines.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14537v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthetic%2520Electrogram%2520Generation%2520with%2520Variational%2520Autoencoders%2520for%2520ECGI%26entry.906535625%3DMiriam%2520Guti%25C3%25A9rrez-Fern%25C3%25A1ndez%2520and%2520Karen%2520L%25C3%25B3pez-Linares%2520and%2520Carlos%2520Fambuena-Santos%2520and%2520Mar%25C3%25ADa%2520S.%2520Guillem%2520and%2520Andreu%2520M.%2520Climent%2520and%2520%25C3%2593scar%2520Barquero-P%25C3%25A9rez%26entry.1292438233%3DAtrial%2520fibrillation%2520%2528AF%2529%2520is%2520the%2520most%2520prevalent%2520sustained%2520cardiac%2520arrhythmia%252C%2520and%2520its%2520clinical%2520assessment%2520requires%2520accurate%2520characterization%2520of%2520atrial%2520electrical%2520activity.%2520Noninvasive%2520electrocardiographic%2520imaging%2520%2528ECGI%2529%2520combined%2520with%2520deep%2520learning%2520%2528DL%2529%2520approaches%2520for%2520estimating%2520intracardiac%2520electrograms%2520%2528EGMs%2529%2520from%2520body%2520surface%2520potentials%2520%2528BSPMs%2529%2520has%2520shown%2520promise%252C%2520but%2520progress%2520is%2520hindered%2520by%2520the%2520limited%2520availability%2520of%2520paired%2520BSPM-EGM%2520datasets.%2520To%2520address%2520this%2520limitation%252C%2520we%2520investigate%2520variational%2520autoencoders%2520%2528VAEs%2529%2520for%2520the%2520generation%2520of%2520synthetic%2520multichannel%2520atrial%2520EGMs.%2520Two%2520models%2520are%2520proposed%253A%2520a%2520sinus%2520rhythm-specific%2520VAE%2520%2528VAE-S%2529%2520and%2520a%2520class-conditioned%2520VAE%2520%2528VAE-C%2529%2520trained%2520on%2520both%2520sinus%2520rhythm%2520and%2520AF%2520signals.%2520Generated%2520EGMs%2520are%2520evaluated%2520using%2520morphological%252C%2520spectral%252C%2520and%2520distributional%2520similarity%2520metrics.%2520VAE-S%2520achieves%2520higher%2520fidelity%2520with%2520respect%2520to%2520in%2520silico%2520EGMs%252C%2520while%2520VAE-C%2520enables%2520rhythm-specific%2520generation%2520at%2520the%2520expense%2520of%2520reduced%2520sinus%2520reconstruction%2520quality.%2520As%2520a%2520proof%2520of%2520concept%252C%2520the%2520generated%2520EGMs%2520are%2520used%2520for%2520data%2520augmentation%2520in%2520a%2520downstream%2520noninvasive%2520EGM%2520reconstruction%2520task%252C%2520where%2520moderate%2520augmentation%2520improves%2520estimation%2520performance.%2520These%2520results%2520demonstrate%2520the%2520potential%2520of%2520VAE-based%2520generative%2520modeling%2520to%2520alleviate%2520data%2520scarcity%2520and%2520enhance%2520deep%2520learning-based%2520ECGI%2520pipelines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14537v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthetic%20Electrogram%20Generation%20with%20Variational%20Autoencoders%20for%20ECGI&entry.906535625=Miriam%20Guti%C3%A9rrez-Fern%C3%A1ndez%20and%20Karen%20L%C3%B3pez-Linares%20and%20Carlos%20Fambuena-Santos%20and%20Mar%C3%ADa%20S.%20Guillem%20and%20Andreu%20M.%20Climent%20and%20%C3%93scar%20Barquero-P%C3%A9rez&entry.1292438233=Atrial%20fibrillation%20%28AF%29%20is%20the%20most%20prevalent%20sustained%20cardiac%20arrhythmia%2C%20and%20its%20clinical%20assessment%20requires%20accurate%20characterization%20of%20atrial%20electrical%20activity.%20Noninvasive%20electrocardiographic%20imaging%20%28ECGI%29%20combined%20with%20deep%20learning%20%28DL%29%20approaches%20for%20estimating%20intracardiac%20electrograms%20%28EGMs%29%20from%20body%20surface%20potentials%20%28BSPMs%29%20has%20shown%20promise%2C%20but%20progress%20is%20hindered%20by%20the%20limited%20availability%20of%20paired%20BSPM-EGM%20datasets.%20To%20address%20this%20limitation%2C%20we%20investigate%20variational%20autoencoders%20%28VAEs%29%20for%20the%20generation%20of%20synthetic%20multichannel%20atrial%20EGMs.%20Two%20models%20are%20proposed%3A%20a%20sinus%20rhythm-specific%20VAE%20%28VAE-S%29%20and%20a%20class-conditioned%20VAE%20%28VAE-C%29%20trained%20on%20both%20sinus%20rhythm%20and%20AF%20signals.%20Generated%20EGMs%20are%20evaluated%20using%20morphological%2C%20spectral%2C%20and%20distributional%20similarity%20metrics.%20VAE-S%20achieves%20higher%20fidelity%20with%20respect%20to%20in%20silico%20EGMs%2C%20while%20VAE-C%20enables%20rhythm-specific%20generation%20at%20the%20expense%20of%20reduced%20sinus%20reconstruction%20quality.%20As%20a%20proof%20of%20concept%2C%20the%20generated%20EGMs%20are%20used%20for%20data%20augmentation%20in%20a%20downstream%20noninvasive%20EGM%20reconstruction%20task%2C%20where%20moderate%20augmentation%20improves%20estimation%20performance.%20These%20results%20demonstrate%20the%20potential%20of%20VAE-based%20generative%20modeling%20to%20alleviate%20data%20scarcity%20and%20enhance%20deep%20learning-based%20ECGI%20pipelines.&entry.1838667208=http%3A//arxiv.org/abs/2512.14537v2&entry.124074799=Read"},
{"title": "Geometric Laplace Neural Operator", "author": "Hao Tang and Jiongyu Zhu and Zimeng Feng and Hao Li and Chao Li", "abstract": "Neural operators have emerged as powerful tools for learning mappings between function spaces, enabling efficient solutions to partial differential equations across varying inputs and domains. Despite the success, existing methods often struggle with non-periodic excitations, transient responses, and signals defined on irregular or non-Euclidean geometries. To address this, we propose a generalized operator learning framework based on a pole-residue decomposition enriched with exponential basis functions, enabling expressive modeling of aperiodic and decaying dynamics. Building on this formulation, we introduce the Geometric Laplace Neural Operator (GLNO), which embeds the Laplace spectral representation into the eigen-basis of the Laplace-Beltrami operator, extending operator learning to arbitrary Riemannian manifolds without requiring periodicity or uniform grids. We further design a grid-invariant network architecture (GLNONet) that realizes GLNO in practice. Extensive experiments on PDEs/ODEs and real-world datasets demonstrate our robust performance over other state-of-the-art models.", "link": "http://arxiv.org/abs/2512.16409v1", "date": "2025-12-18", "relevancy": 2.4615, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5042}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4903}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometric%20Laplace%20Neural%20Operator&body=Title%3A%20Geometric%20Laplace%20Neural%20Operator%0AAuthor%3A%20Hao%20Tang%20and%20Jiongyu%20Zhu%20and%20Zimeng%20Feng%20and%20Hao%20Li%20and%20Chao%20Li%0AAbstract%3A%20Neural%20operators%20have%20emerged%20as%20powerful%20tools%20for%20learning%20mappings%20between%20function%20spaces%2C%20enabling%20efficient%20solutions%20to%20partial%20differential%20equations%20across%20varying%20inputs%20and%20domains.%20Despite%20the%20success%2C%20existing%20methods%20often%20struggle%20with%20non-periodic%20excitations%2C%20transient%20responses%2C%20and%20signals%20defined%20on%20irregular%20or%20non-Euclidean%20geometries.%20To%20address%20this%2C%20we%20propose%20a%20generalized%20operator%20learning%20framework%20based%20on%20a%20pole-residue%20decomposition%20enriched%20with%20exponential%20basis%20functions%2C%20enabling%20expressive%20modeling%20of%20aperiodic%20and%20decaying%20dynamics.%20Building%20on%20this%20formulation%2C%20we%20introduce%20the%20Geometric%20Laplace%20Neural%20Operator%20%28GLNO%29%2C%20which%20embeds%20the%20Laplace%20spectral%20representation%20into%20the%20eigen-basis%20of%20the%20Laplace-Beltrami%20operator%2C%20extending%20operator%20learning%20to%20arbitrary%20Riemannian%20manifolds%20without%20requiring%20periodicity%20or%20uniform%20grids.%20We%20further%20design%20a%20grid-invariant%20network%20architecture%20%28GLNONet%29%20that%20realizes%20GLNO%20in%20practice.%20Extensive%20experiments%20on%20PDEs/ODEs%20and%20real-world%20datasets%20demonstrate%20our%20robust%20performance%20over%20other%20state-of-the-art%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16409v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometric%2520Laplace%2520Neural%2520Operator%26entry.906535625%3DHao%2520Tang%2520and%2520Jiongyu%2520Zhu%2520and%2520Zimeng%2520Feng%2520and%2520Hao%2520Li%2520and%2520Chao%2520Li%26entry.1292438233%3DNeural%2520operators%2520have%2520emerged%2520as%2520powerful%2520tools%2520for%2520learning%2520mappings%2520between%2520function%2520spaces%252C%2520enabling%2520efficient%2520solutions%2520to%2520partial%2520differential%2520equations%2520across%2520varying%2520inputs%2520and%2520domains.%2520Despite%2520the%2520success%252C%2520existing%2520methods%2520often%2520struggle%2520with%2520non-periodic%2520excitations%252C%2520transient%2520responses%252C%2520and%2520signals%2520defined%2520on%2520irregular%2520or%2520non-Euclidean%2520geometries.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520generalized%2520operator%2520learning%2520framework%2520based%2520on%2520a%2520pole-residue%2520decomposition%2520enriched%2520with%2520exponential%2520basis%2520functions%252C%2520enabling%2520expressive%2520modeling%2520of%2520aperiodic%2520and%2520decaying%2520dynamics.%2520Building%2520on%2520this%2520formulation%252C%2520we%2520introduce%2520the%2520Geometric%2520Laplace%2520Neural%2520Operator%2520%2528GLNO%2529%252C%2520which%2520embeds%2520the%2520Laplace%2520spectral%2520representation%2520into%2520the%2520eigen-basis%2520of%2520the%2520Laplace-Beltrami%2520operator%252C%2520extending%2520operator%2520learning%2520to%2520arbitrary%2520Riemannian%2520manifolds%2520without%2520requiring%2520periodicity%2520or%2520uniform%2520grids.%2520We%2520further%2520design%2520a%2520grid-invariant%2520network%2520architecture%2520%2528GLNONet%2529%2520that%2520realizes%2520GLNO%2520in%2520practice.%2520Extensive%2520experiments%2520on%2520PDEs/ODEs%2520and%2520real-world%2520datasets%2520demonstrate%2520our%2520robust%2520performance%2520over%2520other%2520state-of-the-art%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16409v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometric%20Laplace%20Neural%20Operator&entry.906535625=Hao%20Tang%20and%20Jiongyu%20Zhu%20and%20Zimeng%20Feng%20and%20Hao%20Li%20and%20Chao%20Li&entry.1292438233=Neural%20operators%20have%20emerged%20as%20powerful%20tools%20for%20learning%20mappings%20between%20function%20spaces%2C%20enabling%20efficient%20solutions%20to%20partial%20differential%20equations%20across%20varying%20inputs%20and%20domains.%20Despite%20the%20success%2C%20existing%20methods%20often%20struggle%20with%20non-periodic%20excitations%2C%20transient%20responses%2C%20and%20signals%20defined%20on%20irregular%20or%20non-Euclidean%20geometries.%20To%20address%20this%2C%20we%20propose%20a%20generalized%20operator%20learning%20framework%20based%20on%20a%20pole-residue%20decomposition%20enriched%20with%20exponential%20basis%20functions%2C%20enabling%20expressive%20modeling%20of%20aperiodic%20and%20decaying%20dynamics.%20Building%20on%20this%20formulation%2C%20we%20introduce%20the%20Geometric%20Laplace%20Neural%20Operator%20%28GLNO%29%2C%20which%20embeds%20the%20Laplace%20spectral%20representation%20into%20the%20eigen-basis%20of%20the%20Laplace-Beltrami%20operator%2C%20extending%20operator%20learning%20to%20arbitrary%20Riemannian%20manifolds%20without%20requiring%20periodicity%20or%20uniform%20grids.%20We%20further%20design%20a%20grid-invariant%20network%20architecture%20%28GLNONet%29%20that%20realizes%20GLNO%20in%20practice.%20Extensive%20experiments%20on%20PDEs/ODEs%20and%20real-world%20datasets%20demonstrate%20our%20robust%20performance%20over%20other%20state-of-the-art%20models.&entry.1838667208=http%3A//arxiv.org/abs/2512.16409v1&entry.124074799=Read"},
{"title": "FinAudio: A Benchmark for Audio Large Language Models in Financial Applications", "author": "Yupeng Cao and Haohang Li and Yangyang Yu and Shashidhar Reddy Javaji and Yueru He and Jimin Huang and Qianqian Xie and Fabrizio Dimino and Xiao-yang Liu and K. P. Subbalakshmi and Meikang Qiu and Sophia Ananiadou and Jian-Yun Nie", "abstract": "Audio Large Language Models (AudioLLMs) have received widespread attention and have significantly improved performance on audio tasks such as conversation, audio understanding, and automatic speech recognition (ASR). Despite these advancements, there is an absence of a benchmark for assessing AudioLLMs in financial scenarios, where audio data, such as earnings conference calls and CEO speeches, are crucial resources for financial analysis and investment decisions. In this paper, we introduce \\textsc{FinAudio}, the first benchmark designed to evaluate the capacity of AudioLLMs in the financial domain. We first define three tasks based on the unique characteristics of the financial domain: 1) ASR for short financial audio, 2) ASR for long financial audio, and 3) summarization of long financial audio. Then, we curate two short and two long audio datasets, respectively, and develop a novel dataset for financial audio summarization, comprising the \\textsc{FinAudio} benchmark. Then, we evaluate seven prevalent AudioLLMs on \\textsc{FinAudio}. Our evaluation reveals the limitations of existing AudioLLMs in the financial domain and offers insights for improving AudioLLMs. All datasets and codes will be released.", "link": "http://arxiv.org/abs/2503.20990v3", "date": "2025-12-18", "relevancy": 2.4568, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4949}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4896}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FinAudio%3A%20A%20Benchmark%20for%20Audio%20Large%20Language%20Models%20in%20Financial%20Applications&body=Title%3A%20FinAudio%3A%20A%20Benchmark%20for%20Audio%20Large%20Language%20Models%20in%20Financial%20Applications%0AAuthor%3A%20Yupeng%20Cao%20and%20Haohang%20Li%20and%20Yangyang%20Yu%20and%20Shashidhar%20Reddy%20Javaji%20and%20Yueru%20He%20and%20Jimin%20Huang%20and%20Qianqian%20Xie%20and%20Fabrizio%20Dimino%20and%20Xiao-yang%20Liu%20and%20K.%20P.%20Subbalakshmi%20and%20Meikang%20Qiu%20and%20Sophia%20Ananiadou%20and%20Jian-Yun%20Nie%0AAbstract%3A%20Audio%20Large%20Language%20Models%20%28AudioLLMs%29%20have%20received%20widespread%20attention%20and%20have%20significantly%20improved%20performance%20on%20audio%20tasks%20such%20as%20conversation%2C%20audio%20understanding%2C%20and%20automatic%20speech%20recognition%20%28ASR%29.%20Despite%20these%20advancements%2C%20there%20is%20an%20absence%20of%20a%20benchmark%20for%20assessing%20AudioLLMs%20in%20financial%20scenarios%2C%20where%20audio%20data%2C%20such%20as%20earnings%20conference%20calls%20and%20CEO%20speeches%2C%20are%20crucial%20resources%20for%20financial%20analysis%20and%20investment%20decisions.%20In%20this%20paper%2C%20we%20introduce%20%5Ctextsc%7BFinAudio%7D%2C%20the%20first%20benchmark%20designed%20to%20evaluate%20the%20capacity%20of%20AudioLLMs%20in%20the%20financial%20domain.%20We%20first%20define%20three%20tasks%20based%20on%20the%20unique%20characteristics%20of%20the%20financial%20domain%3A%201%29%20ASR%20for%20short%20financial%20audio%2C%202%29%20ASR%20for%20long%20financial%20audio%2C%20and%203%29%20summarization%20of%20long%20financial%20audio.%20Then%2C%20we%20curate%20two%20short%20and%20two%20long%20audio%20datasets%2C%20respectively%2C%20and%20develop%20a%20novel%20dataset%20for%20financial%20audio%20summarization%2C%20comprising%20the%20%5Ctextsc%7BFinAudio%7D%20benchmark.%20Then%2C%20we%20evaluate%20seven%20prevalent%20AudioLLMs%20on%20%5Ctextsc%7BFinAudio%7D.%20Our%20evaluation%20reveals%20the%20limitations%20of%20existing%20AudioLLMs%20in%20the%20financial%20domain%20and%20offers%20insights%20for%20improving%20AudioLLMs.%20All%20datasets%20and%20codes%20will%20be%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2503.20990v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFinAudio%253A%2520A%2520Benchmark%2520for%2520Audio%2520Large%2520Language%2520Models%2520in%2520Financial%2520Applications%26entry.906535625%3DYupeng%2520Cao%2520and%2520Haohang%2520Li%2520and%2520Yangyang%2520Yu%2520and%2520Shashidhar%2520Reddy%2520Javaji%2520and%2520Yueru%2520He%2520and%2520Jimin%2520Huang%2520and%2520Qianqian%2520Xie%2520and%2520Fabrizio%2520Dimino%2520and%2520Xiao-yang%2520Liu%2520and%2520K.%2520P.%2520Subbalakshmi%2520and%2520Meikang%2520Qiu%2520and%2520Sophia%2520Ananiadou%2520and%2520Jian-Yun%2520Nie%26entry.1292438233%3DAudio%2520Large%2520Language%2520Models%2520%2528AudioLLMs%2529%2520have%2520received%2520widespread%2520attention%2520and%2520have%2520significantly%2520improved%2520performance%2520on%2520audio%2520tasks%2520such%2520as%2520conversation%252C%2520audio%2520understanding%252C%2520and%2520automatic%2520speech%2520recognition%2520%2528ASR%2529.%2520Despite%2520these%2520advancements%252C%2520there%2520is%2520an%2520absence%2520of%2520a%2520benchmark%2520for%2520assessing%2520AudioLLMs%2520in%2520financial%2520scenarios%252C%2520where%2520audio%2520data%252C%2520such%2520as%2520earnings%2520conference%2520calls%2520and%2520CEO%2520speeches%252C%2520are%2520crucial%2520resources%2520for%2520financial%2520analysis%2520and%2520investment%2520decisions.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520%255Ctextsc%257BFinAudio%257D%252C%2520the%2520first%2520benchmark%2520designed%2520to%2520evaluate%2520the%2520capacity%2520of%2520AudioLLMs%2520in%2520the%2520financial%2520domain.%2520We%2520first%2520define%2520three%2520tasks%2520based%2520on%2520the%2520unique%2520characteristics%2520of%2520the%2520financial%2520domain%253A%25201%2529%2520ASR%2520for%2520short%2520financial%2520audio%252C%25202%2529%2520ASR%2520for%2520long%2520financial%2520audio%252C%2520and%25203%2529%2520summarization%2520of%2520long%2520financial%2520audio.%2520Then%252C%2520we%2520curate%2520two%2520short%2520and%2520two%2520long%2520audio%2520datasets%252C%2520respectively%252C%2520and%2520develop%2520a%2520novel%2520dataset%2520for%2520financial%2520audio%2520summarization%252C%2520comprising%2520the%2520%255Ctextsc%257BFinAudio%257D%2520benchmark.%2520Then%252C%2520we%2520evaluate%2520seven%2520prevalent%2520AudioLLMs%2520on%2520%255Ctextsc%257BFinAudio%257D.%2520Our%2520evaluation%2520reveals%2520the%2520limitations%2520of%2520existing%2520AudioLLMs%2520in%2520the%2520financial%2520domain%2520and%2520offers%2520insights%2520for%2520improving%2520AudioLLMs.%2520All%2520datasets%2520and%2520codes%2520will%2520be%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.20990v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FinAudio%3A%20A%20Benchmark%20for%20Audio%20Large%20Language%20Models%20in%20Financial%20Applications&entry.906535625=Yupeng%20Cao%20and%20Haohang%20Li%20and%20Yangyang%20Yu%20and%20Shashidhar%20Reddy%20Javaji%20and%20Yueru%20He%20and%20Jimin%20Huang%20and%20Qianqian%20Xie%20and%20Fabrizio%20Dimino%20and%20Xiao-yang%20Liu%20and%20K.%20P.%20Subbalakshmi%20and%20Meikang%20Qiu%20and%20Sophia%20Ananiadou%20and%20Jian-Yun%20Nie&entry.1292438233=Audio%20Large%20Language%20Models%20%28AudioLLMs%29%20have%20received%20widespread%20attention%20and%20have%20significantly%20improved%20performance%20on%20audio%20tasks%20such%20as%20conversation%2C%20audio%20understanding%2C%20and%20automatic%20speech%20recognition%20%28ASR%29.%20Despite%20these%20advancements%2C%20there%20is%20an%20absence%20of%20a%20benchmark%20for%20assessing%20AudioLLMs%20in%20financial%20scenarios%2C%20where%20audio%20data%2C%20such%20as%20earnings%20conference%20calls%20and%20CEO%20speeches%2C%20are%20crucial%20resources%20for%20financial%20analysis%20and%20investment%20decisions.%20In%20this%20paper%2C%20we%20introduce%20%5Ctextsc%7BFinAudio%7D%2C%20the%20first%20benchmark%20designed%20to%20evaluate%20the%20capacity%20of%20AudioLLMs%20in%20the%20financial%20domain.%20We%20first%20define%20three%20tasks%20based%20on%20the%20unique%20characteristics%20of%20the%20financial%20domain%3A%201%29%20ASR%20for%20short%20financial%20audio%2C%202%29%20ASR%20for%20long%20financial%20audio%2C%20and%203%29%20summarization%20of%20long%20financial%20audio.%20Then%2C%20we%20curate%20two%20short%20and%20two%20long%20audio%20datasets%2C%20respectively%2C%20and%20develop%20a%20novel%20dataset%20for%20financial%20audio%20summarization%2C%20comprising%20the%20%5Ctextsc%7BFinAudio%7D%20benchmark.%20Then%2C%20we%20evaluate%20seven%20prevalent%20AudioLLMs%20on%20%5Ctextsc%7BFinAudio%7D.%20Our%20evaluation%20reveals%20the%20limitations%20of%20existing%20AudioLLMs%20in%20the%20financial%20domain%20and%20offers%20insights%20for%20improving%20AudioLLMs.%20All%20datasets%20and%20codes%20will%20be%20released.&entry.1838667208=http%3A//arxiv.org/abs/2503.20990v3&entry.124074799=Read"},
{"title": "VAEER: Visual Attention-Inspired Emotion Elicitation Reasoning", "author": "Fanhang Man and Xiaoyue Chen and Huandong Wang and Baining Zhao and Han Li and Xinlei Chen", "abstract": "Images shared online strongly influence emotions and public well-being. Understanding the emotions an image elicits is therefore vital for fostering healthier and more sustainable digital communities, especially during public crises. We study Visual Emotion Elicitation (VEE), predicting the set of emotions that an image evokes in viewers. We introduce VAEER, an interpretable multi-label VEE framework that combines attention-inspired cue extraction with knowledge-grounded reasoning. VAEER isolates salient visual foci and contextual signals, aligns them with structured affective knowledge, and performs per-emotion inference to yield transparent, emotion-specific rationales. Across three heterogeneous benchmarks, including social imagery and disaster-related photos, VAEER achieves state-of-the-art results with up to 19% per-emotion improvements and a 12.3% average gain over strong CNN and VLM baselines. Our findings highlight interpretable multi-label emotion elicitation as a scalable foundation for responsible visual media analysis and emotionally sustainable online ecosystems.", "link": "http://arxiv.org/abs/2505.24342v2", "date": "2025-12-18", "relevancy": 2.4551, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4999}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4866}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VAEER%3A%20Visual%20Attention-Inspired%20Emotion%20Elicitation%20Reasoning&body=Title%3A%20VAEER%3A%20Visual%20Attention-Inspired%20Emotion%20Elicitation%20Reasoning%0AAuthor%3A%20Fanhang%20Man%20and%20Xiaoyue%20Chen%20and%20Huandong%20Wang%20and%20Baining%20Zhao%20and%20Han%20Li%20and%20Xinlei%20Chen%0AAbstract%3A%20Images%20shared%20online%20strongly%20influence%20emotions%20and%20public%20well-being.%20Understanding%20the%20emotions%20an%20image%20elicits%20is%20therefore%20vital%20for%20fostering%20healthier%20and%20more%20sustainable%20digital%20communities%2C%20especially%20during%20public%20crises.%20We%20study%20Visual%20Emotion%20Elicitation%20%28VEE%29%2C%20predicting%20the%20set%20of%20emotions%20that%20an%20image%20evokes%20in%20viewers.%20We%20introduce%20VAEER%2C%20an%20interpretable%20multi-label%20VEE%20framework%20that%20combines%20attention-inspired%20cue%20extraction%20with%20knowledge-grounded%20reasoning.%20VAEER%20isolates%20salient%20visual%20foci%20and%20contextual%20signals%2C%20aligns%20them%20with%20structured%20affective%20knowledge%2C%20and%20performs%20per-emotion%20inference%20to%20yield%20transparent%2C%20emotion-specific%20rationales.%20Across%20three%20heterogeneous%20benchmarks%2C%20including%20social%20imagery%20and%20disaster-related%20photos%2C%20VAEER%20achieves%20state-of-the-art%20results%20with%20up%20to%2019%25%20per-emotion%20improvements%20and%20a%2012.3%25%20average%20gain%20over%20strong%20CNN%20and%20VLM%20baselines.%20Our%20findings%20highlight%20interpretable%20multi-label%20emotion%20elicitation%20as%20a%20scalable%20foundation%20for%20responsible%20visual%20media%20analysis%20and%20emotionally%20sustainable%20online%20ecosystems.%0ALink%3A%20http%3A//arxiv.org/abs/2505.24342v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVAEER%253A%2520Visual%2520Attention-Inspired%2520Emotion%2520Elicitation%2520Reasoning%26entry.906535625%3DFanhang%2520Man%2520and%2520Xiaoyue%2520Chen%2520and%2520Huandong%2520Wang%2520and%2520Baining%2520Zhao%2520and%2520Han%2520Li%2520and%2520Xinlei%2520Chen%26entry.1292438233%3DImages%2520shared%2520online%2520strongly%2520influence%2520emotions%2520and%2520public%2520well-being.%2520Understanding%2520the%2520emotions%2520an%2520image%2520elicits%2520is%2520therefore%2520vital%2520for%2520fostering%2520healthier%2520and%2520more%2520sustainable%2520digital%2520communities%252C%2520especially%2520during%2520public%2520crises.%2520We%2520study%2520Visual%2520Emotion%2520Elicitation%2520%2528VEE%2529%252C%2520predicting%2520the%2520set%2520of%2520emotions%2520that%2520an%2520image%2520evokes%2520in%2520viewers.%2520We%2520introduce%2520VAEER%252C%2520an%2520interpretable%2520multi-label%2520VEE%2520framework%2520that%2520combines%2520attention-inspired%2520cue%2520extraction%2520with%2520knowledge-grounded%2520reasoning.%2520VAEER%2520isolates%2520salient%2520visual%2520foci%2520and%2520contextual%2520signals%252C%2520aligns%2520them%2520with%2520structured%2520affective%2520knowledge%252C%2520and%2520performs%2520per-emotion%2520inference%2520to%2520yield%2520transparent%252C%2520emotion-specific%2520rationales.%2520Across%2520three%2520heterogeneous%2520benchmarks%252C%2520including%2520social%2520imagery%2520and%2520disaster-related%2520photos%252C%2520VAEER%2520achieves%2520state-of-the-art%2520results%2520with%2520up%2520to%252019%2525%2520per-emotion%2520improvements%2520and%2520a%252012.3%2525%2520average%2520gain%2520over%2520strong%2520CNN%2520and%2520VLM%2520baselines.%2520Our%2520findings%2520highlight%2520interpretable%2520multi-label%2520emotion%2520elicitation%2520as%2520a%2520scalable%2520foundation%2520for%2520responsible%2520visual%2520media%2520analysis%2520and%2520emotionally%2520sustainable%2520online%2520ecosystems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24342v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VAEER%3A%20Visual%20Attention-Inspired%20Emotion%20Elicitation%20Reasoning&entry.906535625=Fanhang%20Man%20and%20Xiaoyue%20Chen%20and%20Huandong%20Wang%20and%20Baining%20Zhao%20and%20Han%20Li%20and%20Xinlei%20Chen&entry.1292438233=Images%20shared%20online%20strongly%20influence%20emotions%20and%20public%20well-being.%20Understanding%20the%20emotions%20an%20image%20elicits%20is%20therefore%20vital%20for%20fostering%20healthier%20and%20more%20sustainable%20digital%20communities%2C%20especially%20during%20public%20crises.%20We%20study%20Visual%20Emotion%20Elicitation%20%28VEE%29%2C%20predicting%20the%20set%20of%20emotions%20that%20an%20image%20evokes%20in%20viewers.%20We%20introduce%20VAEER%2C%20an%20interpretable%20multi-label%20VEE%20framework%20that%20combines%20attention-inspired%20cue%20extraction%20with%20knowledge-grounded%20reasoning.%20VAEER%20isolates%20salient%20visual%20foci%20and%20contextual%20signals%2C%20aligns%20them%20with%20structured%20affective%20knowledge%2C%20and%20performs%20per-emotion%20inference%20to%20yield%20transparent%2C%20emotion-specific%20rationales.%20Across%20three%20heterogeneous%20benchmarks%2C%20including%20social%20imagery%20and%20disaster-related%20photos%2C%20VAEER%20achieves%20state-of-the-art%20results%20with%20up%20to%2019%25%20per-emotion%20improvements%20and%20a%2012.3%25%20average%20gain%20over%20strong%20CNN%20and%20VLM%20baselines.%20Our%20findings%20highlight%20interpretable%20multi-label%20emotion%20elicitation%20as%20a%20scalable%20foundation%20for%20responsible%20visual%20media%20analysis%20and%20emotionally%20sustainable%20online%20ecosystems.&entry.1838667208=http%3A//arxiv.org/abs/2505.24342v2&entry.124074799=Read"},
{"title": "SceneDiff: A Benchmark and Method for Multiview Object Change Detection", "author": "Yuqun Wu and Chih-hao Lin and Henry Che and Aditi Tiwari and Chuhang Zou and Shenlong Wang and Derek Hoiem", "abstract": "We investigate the problem of identifying objects that have been added, removed, or moved between a pair of captures (images or videos) of the same scene at different times. Detecting such changes is important for many applications, such as robotic tidying or construction progress and safety monitoring. A major challenge is that varying viewpoints can cause objects to falsely appear changed. We introduce SceneDiff Benchmark, the first multiview change detection benchmark with object instance annotations, comprising 350 diverse video pairs with thousands of changed objects. We also introduce the SceneDiff method, a new training-free approach for multiview object change detection that leverages pretrained 3D, segmentation, and image encoding models to robustly predict across multiple benchmarks. Our method aligns the captures in 3D, extracts object regions, and compares spatial and semantic region features to detect changes. Experiments on multi-view and two-view benchmarks demonstrate that our method outperforms existing approaches by large margins (94% and 37.4% relative AP improvements). The benchmark and code will be publicly released.", "link": "http://arxiv.org/abs/2512.16908v1", "date": "2025-12-18", "relevancy": 2.4461, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6156}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6107}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SceneDiff%3A%20A%20Benchmark%20and%20Method%20for%20Multiview%20Object%20Change%20Detection&body=Title%3A%20SceneDiff%3A%20A%20Benchmark%20and%20Method%20for%20Multiview%20Object%20Change%20Detection%0AAuthor%3A%20Yuqun%20Wu%20and%20Chih-hao%20Lin%20and%20Henry%20Che%20and%20Aditi%20Tiwari%20and%20Chuhang%20Zou%20and%20Shenlong%20Wang%20and%20Derek%20Hoiem%0AAbstract%3A%20We%20investigate%20the%20problem%20of%20identifying%20objects%20that%20have%20been%20added%2C%20removed%2C%20or%20moved%20between%20a%20pair%20of%20captures%20%28images%20or%20videos%29%20of%20the%20same%20scene%20at%20different%20times.%20Detecting%20such%20changes%20is%20important%20for%20many%20applications%2C%20such%20as%20robotic%20tidying%20or%20construction%20progress%20and%20safety%20monitoring.%20A%20major%20challenge%20is%20that%20varying%20viewpoints%20can%20cause%20objects%20to%20falsely%20appear%20changed.%20We%20introduce%20SceneDiff%20Benchmark%2C%20the%20first%20multiview%20change%20detection%20benchmark%20with%20object%20instance%20annotations%2C%20comprising%20350%20diverse%20video%20pairs%20with%20thousands%20of%20changed%20objects.%20We%20also%20introduce%20the%20SceneDiff%20method%2C%20a%20new%20training-free%20approach%20for%20multiview%20object%20change%20detection%20that%20leverages%20pretrained%203D%2C%20segmentation%2C%20and%20image%20encoding%20models%20to%20robustly%20predict%20across%20multiple%20benchmarks.%20Our%20method%20aligns%20the%20captures%20in%203D%2C%20extracts%20object%20regions%2C%20and%20compares%20spatial%20and%20semantic%20region%20features%20to%20detect%20changes.%20Experiments%20on%20multi-view%20and%20two-view%20benchmarks%20demonstrate%20that%20our%20method%20outperforms%20existing%20approaches%20by%20large%20margins%20%2894%25%20and%2037.4%25%20relative%20AP%20improvements%29.%20The%20benchmark%20and%20code%20will%20be%20publicly%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16908v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSceneDiff%253A%2520A%2520Benchmark%2520and%2520Method%2520for%2520Multiview%2520Object%2520Change%2520Detection%26entry.906535625%3DYuqun%2520Wu%2520and%2520Chih-hao%2520Lin%2520and%2520Henry%2520Che%2520and%2520Aditi%2520Tiwari%2520and%2520Chuhang%2520Zou%2520and%2520Shenlong%2520Wang%2520and%2520Derek%2520Hoiem%26entry.1292438233%3DWe%2520investigate%2520the%2520problem%2520of%2520identifying%2520objects%2520that%2520have%2520been%2520added%252C%2520removed%252C%2520or%2520moved%2520between%2520a%2520pair%2520of%2520captures%2520%2528images%2520or%2520videos%2529%2520of%2520the%2520same%2520scene%2520at%2520different%2520times.%2520Detecting%2520such%2520changes%2520is%2520important%2520for%2520many%2520applications%252C%2520such%2520as%2520robotic%2520tidying%2520or%2520construction%2520progress%2520and%2520safety%2520monitoring.%2520A%2520major%2520challenge%2520is%2520that%2520varying%2520viewpoints%2520can%2520cause%2520objects%2520to%2520falsely%2520appear%2520changed.%2520We%2520introduce%2520SceneDiff%2520Benchmark%252C%2520the%2520first%2520multiview%2520change%2520detection%2520benchmark%2520with%2520object%2520instance%2520annotations%252C%2520comprising%2520350%2520diverse%2520video%2520pairs%2520with%2520thousands%2520of%2520changed%2520objects.%2520We%2520also%2520introduce%2520the%2520SceneDiff%2520method%252C%2520a%2520new%2520training-free%2520approach%2520for%2520multiview%2520object%2520change%2520detection%2520that%2520leverages%2520pretrained%25203D%252C%2520segmentation%252C%2520and%2520image%2520encoding%2520models%2520to%2520robustly%2520predict%2520across%2520multiple%2520benchmarks.%2520Our%2520method%2520aligns%2520the%2520captures%2520in%25203D%252C%2520extracts%2520object%2520regions%252C%2520and%2520compares%2520spatial%2520and%2520semantic%2520region%2520features%2520to%2520detect%2520changes.%2520Experiments%2520on%2520multi-view%2520and%2520two-view%2520benchmarks%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520existing%2520approaches%2520by%2520large%2520margins%2520%252894%2525%2520and%252037.4%2525%2520relative%2520AP%2520improvements%2529.%2520The%2520benchmark%2520and%2520code%2520will%2520be%2520publicly%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16908v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SceneDiff%3A%20A%20Benchmark%20and%20Method%20for%20Multiview%20Object%20Change%20Detection&entry.906535625=Yuqun%20Wu%20and%20Chih-hao%20Lin%20and%20Henry%20Che%20and%20Aditi%20Tiwari%20and%20Chuhang%20Zou%20and%20Shenlong%20Wang%20and%20Derek%20Hoiem&entry.1292438233=We%20investigate%20the%20problem%20of%20identifying%20objects%20that%20have%20been%20added%2C%20removed%2C%20or%20moved%20between%20a%20pair%20of%20captures%20%28images%20or%20videos%29%20of%20the%20same%20scene%20at%20different%20times.%20Detecting%20such%20changes%20is%20important%20for%20many%20applications%2C%20such%20as%20robotic%20tidying%20or%20construction%20progress%20and%20safety%20monitoring.%20A%20major%20challenge%20is%20that%20varying%20viewpoints%20can%20cause%20objects%20to%20falsely%20appear%20changed.%20We%20introduce%20SceneDiff%20Benchmark%2C%20the%20first%20multiview%20change%20detection%20benchmark%20with%20object%20instance%20annotations%2C%20comprising%20350%20diverse%20video%20pairs%20with%20thousands%20of%20changed%20objects.%20We%20also%20introduce%20the%20SceneDiff%20method%2C%20a%20new%20training-free%20approach%20for%20multiview%20object%20change%20detection%20that%20leverages%20pretrained%203D%2C%20segmentation%2C%20and%20image%20encoding%20models%20to%20robustly%20predict%20across%20multiple%20benchmarks.%20Our%20method%20aligns%20the%20captures%20in%203D%2C%20extracts%20object%20regions%2C%20and%20compares%20spatial%20and%20semantic%20region%20features%20to%20detect%20changes.%20Experiments%20on%20multi-view%20and%20two-view%20benchmarks%20demonstrate%20that%20our%20method%20outperforms%20existing%20approaches%20by%20large%20margins%20%2894%25%20and%2037.4%25%20relative%20AP%20improvements%29.%20The%20benchmark%20and%20code%20will%20be%20publicly%20released.&entry.1838667208=http%3A//arxiv.org/abs/2512.16908v1&entry.124074799=Read"},
{"title": "DVGT: Driving Visual Geometry Transformer", "author": "Sicheng Zuo and Zixun Xie and Wenzhao Zheng and Shaoqing Xu and Fang Li and Shengyin Jiang and Long Chen and Zhi-Xin Yang and Jiwen Lu", "abstract": "Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.", "link": "http://arxiv.org/abs/2512.16919v1", "date": "2025-12-18", "relevancy": 2.4459, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6364}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.611}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DVGT%3A%20Driving%20Visual%20Geometry%20Transformer&body=Title%3A%20DVGT%3A%20Driving%20Visual%20Geometry%20Transformer%0AAuthor%3A%20Sicheng%20Zuo%20and%20Zixun%20Xie%20and%20Wenzhao%20Zheng%20and%20Shaoqing%20Xu%20and%20Fang%20Li%20and%20Shengyin%20Jiang%20and%20Long%20Chen%20and%20Zhi-Xin%20Yang%20and%20Jiwen%20Lu%0AAbstract%3A%20Perceiving%20and%20reconstructing%203D%20scene%20geometry%20from%20visual%20inputs%20is%20crucial%20for%20autonomous%20driving.%20However%2C%20there%20still%20lacks%20a%20driving-targeted%20dense%20geometry%20perception%20model%20that%20can%20adapt%20to%20different%20scenarios%20and%20camera%20configurations.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%20Driving%20Visual%20Geometry%20Transformer%20%28DVGT%29%2C%20which%20reconstructs%20a%20global%20dense%203D%20point%20map%20from%20a%20sequence%20of%20unposed%20multi-view%20visual%20inputs.%20We%20first%20extract%20visual%20features%20for%20each%20image%20using%20a%20DINO%20backbone%2C%20and%20employ%20alternating%20intra-view%20local%20attention%2C%20cross-view%20spatial%20attention%2C%20and%20cross-frame%20temporal%20attention%20to%20infer%20geometric%20relations%20across%20images.%20We%20then%20use%20multiple%20heads%20to%20decode%20a%20global%20point%20map%20in%20the%20ego%20coordinate%20of%20the%20first%20frame%20and%20the%20ego%20poses%20for%20each%20frame.%20Unlike%20conventional%20methods%20that%20rely%20on%20precise%20camera%20parameters%2C%20DVGT%20is%20free%20of%20explicit%203D%20geometric%20priors%2C%20enabling%20flexible%20processing%20of%20arbitrary%20camera%20configurations.%20DVGT%20directly%20predicts%20metric-scaled%20geometry%20from%20image%20sequences%2C%20eliminating%20the%20need%20for%20post-alignment%20with%20external%20sensors.%20Trained%20on%20a%20large%20mixture%20of%20driving%20datasets%20including%20nuScenes%2C%20OpenScene%2C%20Waymo%2C%20KITTI%2C%20and%20DDAD%2C%20DVGT%20significantly%20outperforms%20existing%20models%20on%20various%20scenarios.%20Code%20is%20available%20at%20https%3A//github.com/wzzheng/DVGT.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16919v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDVGT%253A%2520Driving%2520Visual%2520Geometry%2520Transformer%26entry.906535625%3DSicheng%2520Zuo%2520and%2520Zixun%2520Xie%2520and%2520Wenzhao%2520Zheng%2520and%2520Shaoqing%2520Xu%2520and%2520Fang%2520Li%2520and%2520Shengyin%2520Jiang%2520and%2520Long%2520Chen%2520and%2520Zhi-Xin%2520Yang%2520and%2520Jiwen%2520Lu%26entry.1292438233%3DPerceiving%2520and%2520reconstructing%25203D%2520scene%2520geometry%2520from%2520visual%2520inputs%2520is%2520crucial%2520for%2520autonomous%2520driving.%2520However%252C%2520there%2520still%2520lacks%2520a%2520driving-targeted%2520dense%2520geometry%2520perception%2520model%2520that%2520can%2520adapt%2520to%2520different%2520scenarios%2520and%2520camera%2520configurations.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520a%2520Driving%2520Visual%2520Geometry%2520Transformer%2520%2528DVGT%2529%252C%2520which%2520reconstructs%2520a%2520global%2520dense%25203D%2520point%2520map%2520from%2520a%2520sequence%2520of%2520unposed%2520multi-view%2520visual%2520inputs.%2520We%2520first%2520extract%2520visual%2520features%2520for%2520each%2520image%2520using%2520a%2520DINO%2520backbone%252C%2520and%2520employ%2520alternating%2520intra-view%2520local%2520attention%252C%2520cross-view%2520spatial%2520attention%252C%2520and%2520cross-frame%2520temporal%2520attention%2520to%2520infer%2520geometric%2520relations%2520across%2520images.%2520We%2520then%2520use%2520multiple%2520heads%2520to%2520decode%2520a%2520global%2520point%2520map%2520in%2520the%2520ego%2520coordinate%2520of%2520the%2520first%2520frame%2520and%2520the%2520ego%2520poses%2520for%2520each%2520frame.%2520Unlike%2520conventional%2520methods%2520that%2520rely%2520on%2520precise%2520camera%2520parameters%252C%2520DVGT%2520is%2520free%2520of%2520explicit%25203D%2520geometric%2520priors%252C%2520enabling%2520flexible%2520processing%2520of%2520arbitrary%2520camera%2520configurations.%2520DVGT%2520directly%2520predicts%2520metric-scaled%2520geometry%2520from%2520image%2520sequences%252C%2520eliminating%2520the%2520need%2520for%2520post-alignment%2520with%2520external%2520sensors.%2520Trained%2520on%2520a%2520large%2520mixture%2520of%2520driving%2520datasets%2520including%2520nuScenes%252C%2520OpenScene%252C%2520Waymo%252C%2520KITTI%252C%2520and%2520DDAD%252C%2520DVGT%2520significantly%2520outperforms%2520existing%2520models%2520on%2520various%2520scenarios.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/wzzheng/DVGT.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16919v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DVGT%3A%20Driving%20Visual%20Geometry%20Transformer&entry.906535625=Sicheng%20Zuo%20and%20Zixun%20Xie%20and%20Wenzhao%20Zheng%20and%20Shaoqing%20Xu%20and%20Fang%20Li%20and%20Shengyin%20Jiang%20and%20Long%20Chen%20and%20Zhi-Xin%20Yang%20and%20Jiwen%20Lu&entry.1292438233=Perceiving%20and%20reconstructing%203D%20scene%20geometry%20from%20visual%20inputs%20is%20crucial%20for%20autonomous%20driving.%20However%2C%20there%20still%20lacks%20a%20driving-targeted%20dense%20geometry%20perception%20model%20that%20can%20adapt%20to%20different%20scenarios%20and%20camera%20configurations.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%20Driving%20Visual%20Geometry%20Transformer%20%28DVGT%29%2C%20which%20reconstructs%20a%20global%20dense%203D%20point%20map%20from%20a%20sequence%20of%20unposed%20multi-view%20visual%20inputs.%20We%20first%20extract%20visual%20features%20for%20each%20image%20using%20a%20DINO%20backbone%2C%20and%20employ%20alternating%20intra-view%20local%20attention%2C%20cross-view%20spatial%20attention%2C%20and%20cross-frame%20temporal%20attention%20to%20infer%20geometric%20relations%20across%20images.%20We%20then%20use%20multiple%20heads%20to%20decode%20a%20global%20point%20map%20in%20the%20ego%20coordinate%20of%20the%20first%20frame%20and%20the%20ego%20poses%20for%20each%20frame.%20Unlike%20conventional%20methods%20that%20rely%20on%20precise%20camera%20parameters%2C%20DVGT%20is%20free%20of%20explicit%203D%20geometric%20priors%2C%20enabling%20flexible%20processing%20of%20arbitrary%20camera%20configurations.%20DVGT%20directly%20predicts%20metric-scaled%20geometry%20from%20image%20sequences%2C%20eliminating%20the%20need%20for%20post-alignment%20with%20external%20sensors.%20Trained%20on%20a%20large%20mixture%20of%20driving%20datasets%20including%20nuScenes%2C%20OpenScene%2C%20Waymo%2C%20KITTI%2C%20and%20DDAD%2C%20DVGT%20significantly%20outperforms%20existing%20models%20on%20various%20scenarios.%20Code%20is%20available%20at%20https%3A//github.com/wzzheng/DVGT.&entry.1838667208=http%3A//arxiv.org/abs/2512.16919v1&entry.124074799=Read"},
{"title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs", "author": "Sara Papi and Javier Garcia Gilabert and Zachary Hopton and Vil\u00e9m Zouhar and Carlos Escolano and Gerard I. G\u00e1llego and Jorge Iranzo-S\u00e1nchez and Ahrii Kim and Dominik Mach\u00e1\u010dek and Patricia Schmidtova and Maike Z\u00fcfle", "abstract": "As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.", "link": "http://arxiv.org/abs/2512.16378v1", "date": "2025-12-18", "relevancy": 2.4455, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4974}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4974}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hearing%20to%20Translate%3A%20The%20Effectiveness%20of%20Speech%20Modality%20Integration%20into%20LLMs&body=Title%3A%20Hearing%20to%20Translate%3A%20The%20Effectiveness%20of%20Speech%20Modality%20Integration%20into%20LLMs%0AAuthor%3A%20Sara%20Papi%20and%20Javier%20Garcia%20Gilabert%20and%20Zachary%20Hopton%20and%20Vil%C3%A9m%20Zouhar%20and%20Carlos%20Escolano%20and%20Gerard%20I.%20G%C3%A1llego%20and%20Jorge%20Iranzo-S%C3%A1nchez%20and%20Ahrii%20Kim%20and%20Dominik%20Mach%C3%A1%C4%8Dek%20and%20Patricia%20Schmidtova%20and%20Maike%20Z%C3%BCfle%0AAbstract%3A%20As%20Large%20Language%20Models%20%28LLMs%29%20expand%20beyond%20text%2C%20integrating%20speech%20as%20a%20native%20modality%20has%20given%20rise%20to%20SpeechLLMs%2C%20which%20aim%20to%20translate%20spoken%20language%20directly%2C%20thereby%20bypassing%20traditional%20transcription-based%20pipelines.%20Whether%20this%20integration%20improves%20speech-to-text%20translation%20quality%20over%20established%20cascaded%20architectures%2C%20however%2C%20remains%20an%20open%20question.%20We%20present%20Hearing%20to%20Translate%2C%20the%20first%20comprehensive%20test%20suite%20rigorously%20benchmarking%205%20state-of-the-art%20SpeechLLMs%20against%2016%20strong%20direct%20and%20cascade%20systems%20that%20couple%20leading%20speech%20foundation%20models%20%28SFM%29%2C%20with%20multilingual%20LLMs.%20Our%20analysis%20spans%2016%20benchmarks%2C%2013%20language%20pairs%2C%20and%209%20challenging%20conditions%2C%20including%20disfluent%2C%20noisy%2C%20and%20long-form%20speech.%20Across%20this%20extensive%20evaluation%2C%20we%20find%20that%20cascaded%20systems%20remain%20the%20most%20reliable%20overall%2C%20while%20current%20SpeechLLMs%20only%20match%20cascades%20in%20selected%20settings%20and%20SFMs%20lag%20behind%20both%2C%20highlighting%20that%20integrating%20an%20LLM%2C%20either%20within%20the%20model%20or%20in%20a%20pipeline%2C%20is%20essential%20for%20high-quality%20speech%20translation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16378v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHearing%2520to%2520Translate%253A%2520The%2520Effectiveness%2520of%2520Speech%2520Modality%2520Integration%2520into%2520LLMs%26entry.906535625%3DSara%2520Papi%2520and%2520Javier%2520Garcia%2520Gilabert%2520and%2520Zachary%2520Hopton%2520and%2520Vil%25C3%25A9m%2520Zouhar%2520and%2520Carlos%2520Escolano%2520and%2520Gerard%2520I.%2520G%25C3%25A1llego%2520and%2520Jorge%2520Iranzo-S%25C3%25A1nchez%2520and%2520Ahrii%2520Kim%2520and%2520Dominik%2520Mach%25C3%25A1%25C4%258Dek%2520and%2520Patricia%2520Schmidtova%2520and%2520Maike%2520Z%25C3%25BCfle%26entry.1292438233%3DAs%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520expand%2520beyond%2520text%252C%2520integrating%2520speech%2520as%2520a%2520native%2520modality%2520has%2520given%2520rise%2520to%2520SpeechLLMs%252C%2520which%2520aim%2520to%2520translate%2520spoken%2520language%2520directly%252C%2520thereby%2520bypassing%2520traditional%2520transcription-based%2520pipelines.%2520Whether%2520this%2520integration%2520improves%2520speech-to-text%2520translation%2520quality%2520over%2520established%2520cascaded%2520architectures%252C%2520however%252C%2520remains%2520an%2520open%2520question.%2520We%2520present%2520Hearing%2520to%2520Translate%252C%2520the%2520first%2520comprehensive%2520test%2520suite%2520rigorously%2520benchmarking%25205%2520state-of-the-art%2520SpeechLLMs%2520against%252016%2520strong%2520direct%2520and%2520cascade%2520systems%2520that%2520couple%2520leading%2520speech%2520foundation%2520models%2520%2528SFM%2529%252C%2520with%2520multilingual%2520LLMs.%2520Our%2520analysis%2520spans%252016%2520benchmarks%252C%252013%2520language%2520pairs%252C%2520and%25209%2520challenging%2520conditions%252C%2520including%2520disfluent%252C%2520noisy%252C%2520and%2520long-form%2520speech.%2520Across%2520this%2520extensive%2520evaluation%252C%2520we%2520find%2520that%2520cascaded%2520systems%2520remain%2520the%2520most%2520reliable%2520overall%252C%2520while%2520current%2520SpeechLLMs%2520only%2520match%2520cascades%2520in%2520selected%2520settings%2520and%2520SFMs%2520lag%2520behind%2520both%252C%2520highlighting%2520that%2520integrating%2520an%2520LLM%252C%2520either%2520within%2520the%2520model%2520or%2520in%2520a%2520pipeline%252C%2520is%2520essential%2520for%2520high-quality%2520speech%2520translation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16378v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hearing%20to%20Translate%3A%20The%20Effectiveness%20of%20Speech%20Modality%20Integration%20into%20LLMs&entry.906535625=Sara%20Papi%20and%20Javier%20Garcia%20Gilabert%20and%20Zachary%20Hopton%20and%20Vil%C3%A9m%20Zouhar%20and%20Carlos%20Escolano%20and%20Gerard%20I.%20G%C3%A1llego%20and%20Jorge%20Iranzo-S%C3%A1nchez%20and%20Ahrii%20Kim%20and%20Dominik%20Mach%C3%A1%C4%8Dek%20and%20Patricia%20Schmidtova%20and%20Maike%20Z%C3%BCfle&entry.1292438233=As%20Large%20Language%20Models%20%28LLMs%29%20expand%20beyond%20text%2C%20integrating%20speech%20as%20a%20native%20modality%20has%20given%20rise%20to%20SpeechLLMs%2C%20which%20aim%20to%20translate%20spoken%20language%20directly%2C%20thereby%20bypassing%20traditional%20transcription-based%20pipelines.%20Whether%20this%20integration%20improves%20speech-to-text%20translation%20quality%20over%20established%20cascaded%20architectures%2C%20however%2C%20remains%20an%20open%20question.%20We%20present%20Hearing%20to%20Translate%2C%20the%20first%20comprehensive%20test%20suite%20rigorously%20benchmarking%205%20state-of-the-art%20SpeechLLMs%20against%2016%20strong%20direct%20and%20cascade%20systems%20that%20couple%20leading%20speech%20foundation%20models%20%28SFM%29%2C%20with%20multilingual%20LLMs.%20Our%20analysis%20spans%2016%20benchmarks%2C%2013%20language%20pairs%2C%20and%209%20challenging%20conditions%2C%20including%20disfluent%2C%20noisy%2C%20and%20long-form%20speech.%20Across%20this%20extensive%20evaluation%2C%20we%20find%20that%20cascaded%20systems%20remain%20the%20most%20reliable%20overall%2C%20while%20current%20SpeechLLMs%20only%20match%20cascades%20in%20selected%20settings%20and%20SFMs%20lag%20behind%20both%2C%20highlighting%20that%20integrating%20an%20LLM%2C%20either%20within%20the%20model%20or%20in%20a%20pipeline%2C%20is%20essential%20for%20high-quality%20speech%20translation.&entry.1838667208=http%3A//arxiv.org/abs/2512.16378v1&entry.124074799=Read"},
{"title": "Cyber Humanism in Education: Reclaiming Agency through AI and Learning Sciences", "author": "Giovanni Adorni", "abstract": "Generative Artificial Intelligence (GenAI) is rapidly reshaping how knowledge is produced and validated in education. Rather than adding another digital tool, large language models reconfigure reading, writing, and coding into hybrid human-AI workflows, raising concerns about epistemic automation, cognitive offloading, and the de-professiona\\-lisation of teachers. This paper proposes \\emph{Cyber Humanism in Education} as a framework for reclaiming human agency in this landscape. We conceptualise AI-enabled learning environments as socio-technical infrastructures co-authored by humans and machines, and position educators and learners as epistemic agents and \\emph{algorithmic citizens} who have both the right and the responsibility to shape these infrastructures.\n  We articulate three pillars for cyber-humanist design, \\emph{reflexive competence}, \\emph{algorithmic citizenship}, and \\emph{dialogic design}, and relate them to major international digital and AI competence frameworks. We then present higher-education case studies that operationalise these ideas through \\emph{prompt-based learning} and a new \\emph{Conversational AI Educator} certification within the EPICT ecosystem. The findings show how such practices can strengthen epistemic agency while surfacing tensions around workload, equity, and governance, and outline implications for the future of AI-rich, human-centred education.", "link": "http://arxiv.org/abs/2512.16701v1", "date": "2025-12-18", "relevancy": 2.4448, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5074}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5014}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cyber%20Humanism%20in%20Education%3A%20Reclaiming%20Agency%20through%20AI%20and%20Learning%20Sciences&body=Title%3A%20Cyber%20Humanism%20in%20Education%3A%20Reclaiming%20Agency%20through%20AI%20and%20Learning%20Sciences%0AAuthor%3A%20Giovanni%20Adorni%0AAbstract%3A%20Generative%20Artificial%20Intelligence%20%28GenAI%29%20is%20rapidly%20reshaping%20how%20knowledge%20is%20produced%20and%20validated%20in%20education.%20Rather%20than%20adding%20another%20digital%20tool%2C%20large%20language%20models%20reconfigure%20reading%2C%20writing%2C%20and%20coding%20into%20hybrid%20human-AI%20workflows%2C%20raising%20concerns%20about%20epistemic%20automation%2C%20cognitive%20offloading%2C%20and%20the%20de-professiona%5C-lisation%20of%20teachers.%20This%20paper%20proposes%20%5Cemph%7BCyber%20Humanism%20in%20Education%7D%20as%20a%20framework%20for%20reclaiming%20human%20agency%20in%20this%20landscape.%20We%20conceptualise%20AI-enabled%20learning%20environments%20as%20socio-technical%20infrastructures%20co-authored%20by%20humans%20and%20machines%2C%20and%20position%20educators%20and%20learners%20as%20epistemic%20agents%20and%20%5Cemph%7Balgorithmic%20citizens%7D%20who%20have%20both%20the%20right%20and%20the%20responsibility%20to%20shape%20these%20infrastructures.%0A%20%20We%20articulate%20three%20pillars%20for%20cyber-humanist%20design%2C%20%5Cemph%7Breflexive%20competence%7D%2C%20%5Cemph%7Balgorithmic%20citizenship%7D%2C%20and%20%5Cemph%7Bdialogic%20design%7D%2C%20and%20relate%20them%20to%20major%20international%20digital%20and%20AI%20competence%20frameworks.%20We%20then%20present%20higher-education%20case%20studies%20that%20operationalise%20these%20ideas%20through%20%5Cemph%7Bprompt-based%20learning%7D%20and%20a%20new%20%5Cemph%7BConversational%20AI%20Educator%7D%20certification%20within%20the%20EPICT%20ecosystem.%20The%20findings%20show%20how%20such%20practices%20can%20strengthen%20epistemic%20agency%20while%20surfacing%20tensions%20around%20workload%2C%20equity%2C%20and%20governance%2C%20and%20outline%20implications%20for%20the%20future%20of%20AI-rich%2C%20human-centred%20education.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16701v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCyber%2520Humanism%2520in%2520Education%253A%2520Reclaiming%2520Agency%2520through%2520AI%2520and%2520Learning%2520Sciences%26entry.906535625%3DGiovanni%2520Adorni%26entry.1292438233%3DGenerative%2520Artificial%2520Intelligence%2520%2528GenAI%2529%2520is%2520rapidly%2520reshaping%2520how%2520knowledge%2520is%2520produced%2520and%2520validated%2520in%2520education.%2520Rather%2520than%2520adding%2520another%2520digital%2520tool%252C%2520large%2520language%2520models%2520reconfigure%2520reading%252C%2520writing%252C%2520and%2520coding%2520into%2520hybrid%2520human-AI%2520workflows%252C%2520raising%2520concerns%2520about%2520epistemic%2520automation%252C%2520cognitive%2520offloading%252C%2520and%2520the%2520de-professiona%255C-lisation%2520of%2520teachers.%2520This%2520paper%2520proposes%2520%255Cemph%257BCyber%2520Humanism%2520in%2520Education%257D%2520as%2520a%2520framework%2520for%2520reclaiming%2520human%2520agency%2520in%2520this%2520landscape.%2520We%2520conceptualise%2520AI-enabled%2520learning%2520environments%2520as%2520socio-technical%2520infrastructures%2520co-authored%2520by%2520humans%2520and%2520machines%252C%2520and%2520position%2520educators%2520and%2520learners%2520as%2520epistemic%2520agents%2520and%2520%255Cemph%257Balgorithmic%2520citizens%257D%2520who%2520have%2520both%2520the%2520right%2520and%2520the%2520responsibility%2520to%2520shape%2520these%2520infrastructures.%250A%2520%2520We%2520articulate%2520three%2520pillars%2520for%2520cyber-humanist%2520design%252C%2520%255Cemph%257Breflexive%2520competence%257D%252C%2520%255Cemph%257Balgorithmic%2520citizenship%257D%252C%2520and%2520%255Cemph%257Bdialogic%2520design%257D%252C%2520and%2520relate%2520them%2520to%2520major%2520international%2520digital%2520and%2520AI%2520competence%2520frameworks.%2520We%2520then%2520present%2520higher-education%2520case%2520studies%2520that%2520operationalise%2520these%2520ideas%2520through%2520%255Cemph%257Bprompt-based%2520learning%257D%2520and%2520a%2520new%2520%255Cemph%257BConversational%2520AI%2520Educator%257D%2520certification%2520within%2520the%2520EPICT%2520ecosystem.%2520The%2520findings%2520show%2520how%2520such%2520practices%2520can%2520strengthen%2520epistemic%2520agency%2520while%2520surfacing%2520tensions%2520around%2520workload%252C%2520equity%252C%2520and%2520governance%252C%2520and%2520outline%2520implications%2520for%2520the%2520future%2520of%2520AI-rich%252C%2520human-centred%2520education.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16701v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cyber%20Humanism%20in%20Education%3A%20Reclaiming%20Agency%20through%20AI%20and%20Learning%20Sciences&entry.906535625=Giovanni%20Adorni&entry.1292438233=Generative%20Artificial%20Intelligence%20%28GenAI%29%20is%20rapidly%20reshaping%20how%20knowledge%20is%20produced%20and%20validated%20in%20education.%20Rather%20than%20adding%20another%20digital%20tool%2C%20large%20language%20models%20reconfigure%20reading%2C%20writing%2C%20and%20coding%20into%20hybrid%20human-AI%20workflows%2C%20raising%20concerns%20about%20epistemic%20automation%2C%20cognitive%20offloading%2C%20and%20the%20de-professiona%5C-lisation%20of%20teachers.%20This%20paper%20proposes%20%5Cemph%7BCyber%20Humanism%20in%20Education%7D%20as%20a%20framework%20for%20reclaiming%20human%20agency%20in%20this%20landscape.%20We%20conceptualise%20AI-enabled%20learning%20environments%20as%20socio-technical%20infrastructures%20co-authored%20by%20humans%20and%20machines%2C%20and%20position%20educators%20and%20learners%20as%20epistemic%20agents%20and%20%5Cemph%7Balgorithmic%20citizens%7D%20who%20have%20both%20the%20right%20and%20the%20responsibility%20to%20shape%20these%20infrastructures.%0A%20%20We%20articulate%20three%20pillars%20for%20cyber-humanist%20design%2C%20%5Cemph%7Breflexive%20competence%7D%2C%20%5Cemph%7Balgorithmic%20citizenship%7D%2C%20and%20%5Cemph%7Bdialogic%20design%7D%2C%20and%20relate%20them%20to%20major%20international%20digital%20and%20AI%20competence%20frameworks.%20We%20then%20present%20higher-education%20case%20studies%20that%20operationalise%20these%20ideas%20through%20%5Cemph%7Bprompt-based%20learning%7D%20and%20a%20new%20%5Cemph%7BConversational%20AI%20Educator%7D%20certification%20within%20the%20EPICT%20ecosystem.%20The%20findings%20show%20how%20such%20practices%20can%20strengthen%20epistemic%20agency%20while%20surfacing%20tensions%20around%20workload%2C%20equity%2C%20and%20governance%2C%20and%20outline%20implications%20for%20the%20future%20of%20AI-rich%2C%20human-centred%20education.&entry.1838667208=http%3A//arxiv.org/abs/2512.16701v1&entry.124074799=Read"},
{"title": "VERM: Leveraging Foundation Models to Create a Virtual Eye for Efficient 3D Robotic Manipulation", "author": "Yixiang Chen and Yan Huang and Keji He and Peiyan Li and Liang Wang", "abstract": "When performing 3D manipulation tasks, robots have to execute action planning based on perceptions from multiple fixed cameras. The multi-camera setup introduces substantial redundancy and irrelevant information, which increases computational costs and forces the model to spend extra training time extracting crucial task-relevant details. To filter out redundant information and accurately extract task-relevant features, we propose the VERM (Virtual Eye for Robotic Manipulation) method, leveraging the knowledge in foundation models to imagine a virtual task-adaptive view from the constructed 3D point cloud, which efficiently captures necessary information and mitigates occlusion. To facilitate 3D action planning and fine-grained manipulation, we further design a depth-aware module and a dynamic coarse-to-fine procedure. Extensive experimental results on both simulation benchmark RLBench and real-world evaluations demonstrate the effectiveness of our method, surpassing previous state-of-the-art methods while achieving 1.89x speedup in training time and 1.54x speedup in inference speed. More results can be found on our project website at https://verm-ral.github.io .", "link": "http://arxiv.org/abs/2512.16724v1", "date": "2025-12-18", "relevancy": 2.4436, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6146}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6146}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VERM%3A%20Leveraging%20Foundation%20Models%20to%20Create%20a%20Virtual%20Eye%20for%20Efficient%203D%20Robotic%20Manipulation&body=Title%3A%20VERM%3A%20Leveraging%20Foundation%20Models%20to%20Create%20a%20Virtual%20Eye%20for%20Efficient%203D%20Robotic%20Manipulation%0AAuthor%3A%20Yixiang%20Chen%20and%20Yan%20Huang%20and%20Keji%20He%20and%20Peiyan%20Li%20and%20Liang%20Wang%0AAbstract%3A%20When%20performing%203D%20manipulation%20tasks%2C%20robots%20have%20to%20execute%20action%20planning%20based%20on%20perceptions%20from%20multiple%20fixed%20cameras.%20The%20multi-camera%20setup%20introduces%20substantial%20redundancy%20and%20irrelevant%20information%2C%20which%20increases%20computational%20costs%20and%20forces%20the%20model%20to%20spend%20extra%20training%20time%20extracting%20crucial%20task-relevant%20details.%20To%20filter%20out%20redundant%20information%20and%20accurately%20extract%20task-relevant%20features%2C%20we%20propose%20the%20VERM%20%28Virtual%20Eye%20for%20Robotic%20Manipulation%29%20method%2C%20leveraging%20the%20knowledge%20in%20foundation%20models%20to%20imagine%20a%20virtual%20task-adaptive%20view%20from%20the%20constructed%203D%20point%20cloud%2C%20which%20efficiently%20captures%20necessary%20information%20and%20mitigates%20occlusion.%20To%20facilitate%203D%20action%20planning%20and%20fine-grained%20manipulation%2C%20we%20further%20design%20a%20depth-aware%20module%20and%20a%20dynamic%20coarse-to-fine%20procedure.%20Extensive%20experimental%20results%20on%20both%20simulation%20benchmark%20RLBench%20and%20real-world%20evaluations%20demonstrate%20the%20effectiveness%20of%20our%20method%2C%20surpassing%20previous%20state-of-the-art%20methods%20while%20achieving%201.89x%20speedup%20in%20training%20time%20and%201.54x%20speedup%20in%20inference%20speed.%20More%20results%20can%20be%20found%20on%20our%20project%20website%20at%20https%3A//verm-ral.github.io%20.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16724v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVERM%253A%2520Leveraging%2520Foundation%2520Models%2520to%2520Create%2520a%2520Virtual%2520Eye%2520for%2520Efficient%25203D%2520Robotic%2520Manipulation%26entry.906535625%3DYixiang%2520Chen%2520and%2520Yan%2520Huang%2520and%2520Keji%2520He%2520and%2520Peiyan%2520Li%2520and%2520Liang%2520Wang%26entry.1292438233%3DWhen%2520performing%25203D%2520manipulation%2520tasks%252C%2520robots%2520have%2520to%2520execute%2520action%2520planning%2520based%2520on%2520perceptions%2520from%2520multiple%2520fixed%2520cameras.%2520The%2520multi-camera%2520setup%2520introduces%2520substantial%2520redundancy%2520and%2520irrelevant%2520information%252C%2520which%2520increases%2520computational%2520costs%2520and%2520forces%2520the%2520model%2520to%2520spend%2520extra%2520training%2520time%2520extracting%2520crucial%2520task-relevant%2520details.%2520To%2520filter%2520out%2520redundant%2520information%2520and%2520accurately%2520extract%2520task-relevant%2520features%252C%2520we%2520propose%2520the%2520VERM%2520%2528Virtual%2520Eye%2520for%2520Robotic%2520Manipulation%2529%2520method%252C%2520leveraging%2520the%2520knowledge%2520in%2520foundation%2520models%2520to%2520imagine%2520a%2520virtual%2520task-adaptive%2520view%2520from%2520the%2520constructed%25203D%2520point%2520cloud%252C%2520which%2520efficiently%2520captures%2520necessary%2520information%2520and%2520mitigates%2520occlusion.%2520To%2520facilitate%25203D%2520action%2520planning%2520and%2520fine-grained%2520manipulation%252C%2520we%2520further%2520design%2520a%2520depth-aware%2520module%2520and%2520a%2520dynamic%2520coarse-to-fine%2520procedure.%2520Extensive%2520experimental%2520results%2520on%2520both%2520simulation%2520benchmark%2520RLBench%2520and%2520real-world%2520evaluations%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%252C%2520surpassing%2520previous%2520state-of-the-art%2520methods%2520while%2520achieving%25201.89x%2520speedup%2520in%2520training%2520time%2520and%25201.54x%2520speedup%2520in%2520inference%2520speed.%2520More%2520results%2520can%2520be%2520found%2520on%2520our%2520project%2520website%2520at%2520https%253A//verm-ral.github.io%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16724v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VERM%3A%20Leveraging%20Foundation%20Models%20to%20Create%20a%20Virtual%20Eye%20for%20Efficient%203D%20Robotic%20Manipulation&entry.906535625=Yixiang%20Chen%20and%20Yan%20Huang%20and%20Keji%20He%20and%20Peiyan%20Li%20and%20Liang%20Wang&entry.1292438233=When%20performing%203D%20manipulation%20tasks%2C%20robots%20have%20to%20execute%20action%20planning%20based%20on%20perceptions%20from%20multiple%20fixed%20cameras.%20The%20multi-camera%20setup%20introduces%20substantial%20redundancy%20and%20irrelevant%20information%2C%20which%20increases%20computational%20costs%20and%20forces%20the%20model%20to%20spend%20extra%20training%20time%20extracting%20crucial%20task-relevant%20details.%20To%20filter%20out%20redundant%20information%20and%20accurately%20extract%20task-relevant%20features%2C%20we%20propose%20the%20VERM%20%28Virtual%20Eye%20for%20Robotic%20Manipulation%29%20method%2C%20leveraging%20the%20knowledge%20in%20foundation%20models%20to%20imagine%20a%20virtual%20task-adaptive%20view%20from%20the%20constructed%203D%20point%20cloud%2C%20which%20efficiently%20captures%20necessary%20information%20and%20mitigates%20occlusion.%20To%20facilitate%203D%20action%20planning%20and%20fine-grained%20manipulation%2C%20we%20further%20design%20a%20depth-aware%20module%20and%20a%20dynamic%20coarse-to-fine%20procedure.%20Extensive%20experimental%20results%20on%20both%20simulation%20benchmark%20RLBench%20and%20real-world%20evaluations%20demonstrate%20the%20effectiveness%20of%20our%20method%2C%20surpassing%20previous%20state-of-the-art%20methods%20while%20achieving%201.89x%20speedup%20in%20training%20time%20and%201.54x%20speedup%20in%20inference%20speed.%20More%20results%20can%20be%20found%20on%20our%20project%20website%20at%20https%3A//verm-ral.github.io%20.&entry.1838667208=http%3A//arxiv.org/abs/2512.16724v1&entry.124074799=Read"},
{"title": "Next-Generation License Plate Detection and Recognition System using YOLOv8", "author": "Arslan Amin and Rafia Mumtaz and Muhammad Jawad Bashir and Syed Mohammad Hassan Zaidi", "abstract": "In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.", "link": "http://arxiv.org/abs/2512.16826v1", "date": "2025-12-18", "relevancy": 2.4415, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5051}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4872}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Next-Generation%20License%20Plate%20Detection%20and%20Recognition%20System%20using%20YOLOv8&body=Title%3A%20Next-Generation%20License%20Plate%20Detection%20and%20Recognition%20System%20using%20YOLOv8%0AAuthor%3A%20Arslan%20Amin%20and%20Rafia%20Mumtaz%20and%20Muhammad%20Jawad%20Bashir%20and%20Syed%20Mohammad%20Hassan%20Zaidi%0AAbstract%3A%20In%20the%20evolving%20landscape%20of%20traffic%20management%20and%20vehicle%20surveillance%2C%20efficient%20license%20plate%20detection%20and%20recognition%20are%20indispensable.%20Historically%2C%20many%20methodologies%20have%20tackled%20this%20challenge%2C%20but%20consistent%20real-time%20accuracy%2C%20especially%20in%20diverse%20environments%2C%20remains%20elusive.%20This%20study%20examines%20the%20performance%20of%20YOLOv8%20variants%20on%20License%20Plate%20Recognition%20%28LPR%29%20and%20Character%20Recognition%20tasks%2C%20crucial%20for%20advancing%20Intelligent%20Transportation%20Systems.%20Two%20distinct%20datasets%20were%20employed%20for%20training%20and%20evaluation%2C%20yielding%20notable%20findings.%20The%20YOLOv8%20Nano%20variant%20demonstrated%20a%20precision%20of%200.964%20and%20mAP50%20of%200.918%20on%20the%20LPR%20task%2C%20while%20the%20YOLOv8%20Small%20variant%20exhibited%20a%20precision%20of%200.92%20and%20mAP50%20of%200.91%20on%20the%20Character%20Recognition%20task.%20A%20custom%20method%20for%20character%20sequencing%20was%20introduced%2C%20effectively%20sequencing%20the%20detected%20characters%20based%20on%20their%20x-axis%20positions.%20An%20optimized%20pipeline%2C%20utilizing%20YOLOv8%20Nano%20for%20LPR%20and%20YOLOv8%20Small%20for%20Character%20Recognition%2C%20is%20proposed.%20This%20configuration%20not%20only%20maintains%20computational%20efficiency%20but%20also%20ensures%20high%20accuracy%2C%20establishing%20a%20robust%20foundation%20for%20future%20real-world%20deployments%20on%20edge%20devices%20within%20Intelligent%20Transportation%20Systems.%20This%20effort%20marks%20a%20significant%20stride%20towards%20the%20development%20of%20smarter%20and%20more%20efficient%20urban%20infrastructures.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16826v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNext-Generation%2520License%2520Plate%2520Detection%2520and%2520Recognition%2520System%2520using%2520YOLOv8%26entry.906535625%3DArslan%2520Amin%2520and%2520Rafia%2520Mumtaz%2520and%2520Muhammad%2520Jawad%2520Bashir%2520and%2520Syed%2520Mohammad%2520Hassan%2520Zaidi%26entry.1292438233%3DIn%2520the%2520evolving%2520landscape%2520of%2520traffic%2520management%2520and%2520vehicle%2520surveillance%252C%2520efficient%2520license%2520plate%2520detection%2520and%2520recognition%2520are%2520indispensable.%2520Historically%252C%2520many%2520methodologies%2520have%2520tackled%2520this%2520challenge%252C%2520but%2520consistent%2520real-time%2520accuracy%252C%2520especially%2520in%2520diverse%2520environments%252C%2520remains%2520elusive.%2520This%2520study%2520examines%2520the%2520performance%2520of%2520YOLOv8%2520variants%2520on%2520License%2520Plate%2520Recognition%2520%2528LPR%2529%2520and%2520Character%2520Recognition%2520tasks%252C%2520crucial%2520for%2520advancing%2520Intelligent%2520Transportation%2520Systems.%2520Two%2520distinct%2520datasets%2520were%2520employed%2520for%2520training%2520and%2520evaluation%252C%2520yielding%2520notable%2520findings.%2520The%2520YOLOv8%2520Nano%2520variant%2520demonstrated%2520a%2520precision%2520of%25200.964%2520and%2520mAP50%2520of%25200.918%2520on%2520the%2520LPR%2520task%252C%2520while%2520the%2520YOLOv8%2520Small%2520variant%2520exhibited%2520a%2520precision%2520of%25200.92%2520and%2520mAP50%2520of%25200.91%2520on%2520the%2520Character%2520Recognition%2520task.%2520A%2520custom%2520method%2520for%2520character%2520sequencing%2520was%2520introduced%252C%2520effectively%2520sequencing%2520the%2520detected%2520characters%2520based%2520on%2520their%2520x-axis%2520positions.%2520An%2520optimized%2520pipeline%252C%2520utilizing%2520YOLOv8%2520Nano%2520for%2520LPR%2520and%2520YOLOv8%2520Small%2520for%2520Character%2520Recognition%252C%2520is%2520proposed.%2520This%2520configuration%2520not%2520only%2520maintains%2520computational%2520efficiency%2520but%2520also%2520ensures%2520high%2520accuracy%252C%2520establishing%2520a%2520robust%2520foundation%2520for%2520future%2520real-world%2520deployments%2520on%2520edge%2520devices%2520within%2520Intelligent%2520Transportation%2520Systems.%2520This%2520effort%2520marks%2520a%2520significant%2520stride%2520towards%2520the%2520development%2520of%2520smarter%2520and%2520more%2520efficient%2520urban%2520infrastructures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16826v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Next-Generation%20License%20Plate%20Detection%20and%20Recognition%20System%20using%20YOLOv8&entry.906535625=Arslan%20Amin%20and%20Rafia%20Mumtaz%20and%20Muhammad%20Jawad%20Bashir%20and%20Syed%20Mohammad%20Hassan%20Zaidi&entry.1292438233=In%20the%20evolving%20landscape%20of%20traffic%20management%20and%20vehicle%20surveillance%2C%20efficient%20license%20plate%20detection%20and%20recognition%20are%20indispensable.%20Historically%2C%20many%20methodologies%20have%20tackled%20this%20challenge%2C%20but%20consistent%20real-time%20accuracy%2C%20especially%20in%20diverse%20environments%2C%20remains%20elusive.%20This%20study%20examines%20the%20performance%20of%20YOLOv8%20variants%20on%20License%20Plate%20Recognition%20%28LPR%29%20and%20Character%20Recognition%20tasks%2C%20crucial%20for%20advancing%20Intelligent%20Transportation%20Systems.%20Two%20distinct%20datasets%20were%20employed%20for%20training%20and%20evaluation%2C%20yielding%20notable%20findings.%20The%20YOLOv8%20Nano%20variant%20demonstrated%20a%20precision%20of%200.964%20and%20mAP50%20of%200.918%20on%20the%20LPR%20task%2C%20while%20the%20YOLOv8%20Small%20variant%20exhibited%20a%20precision%20of%200.92%20and%20mAP50%20of%200.91%20on%20the%20Character%20Recognition%20task.%20A%20custom%20method%20for%20character%20sequencing%20was%20introduced%2C%20effectively%20sequencing%20the%20detected%20characters%20based%20on%20their%20x-axis%20positions.%20An%20optimized%20pipeline%2C%20utilizing%20YOLOv8%20Nano%20for%20LPR%20and%20YOLOv8%20Small%20for%20Character%20Recognition%2C%20is%20proposed.%20This%20configuration%20not%20only%20maintains%20computational%20efficiency%20but%20also%20ensures%20high%20accuracy%2C%20establishing%20a%20robust%20foundation%20for%20future%20real-world%20deployments%20on%20edge%20devices%20within%20Intelligent%20Transportation%20Systems.%20This%20effort%20marks%20a%20significant%20stride%20towards%20the%20development%20of%20smarter%20and%20more%20efficient%20urban%20infrastructures.&entry.1838667208=http%3A//arxiv.org/abs/2512.16826v1&entry.124074799=Read"},
{"title": "EasyV2V: A High-quality Instruction-based Video Editing Framework", "author": "Jinjie Mai and Chaoyang Wang and Guocheng Gordon Qian and Willi Menapace and Sergey Tulyakov and Bernard Ghanem and Peter Wonka and Ashkan Mirzaei", "abstract": "While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \\emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/", "link": "http://arxiv.org/abs/2512.16920v1", "date": "2025-12-18", "relevancy": 2.429, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6678}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6222}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EasyV2V%3A%20A%20High-quality%20Instruction-based%20Video%20Editing%20Framework&body=Title%3A%20EasyV2V%3A%20A%20High-quality%20Instruction-based%20Video%20Editing%20Framework%0AAuthor%3A%20Jinjie%20Mai%20and%20Chaoyang%20Wang%20and%20Guocheng%20Gordon%20Qian%20and%20Willi%20Menapace%20and%20Sergey%20Tulyakov%20and%20Bernard%20Ghanem%20and%20Peter%20Wonka%20and%20Ashkan%20Mirzaei%0AAbstract%3A%20While%20image%20editing%20has%20advanced%20rapidly%2C%20video%20editing%20remains%20less%20explored%2C%20facing%20challenges%20in%20consistency%2C%20control%2C%20and%20generalization.%20We%20study%20the%20design%20space%20of%20data%2C%20architecture%2C%20and%20control%2C%20and%20introduce%20%5Cemph%7BEasyV2V%7D%2C%20a%20simple%20and%20effective%20framework%20for%20instruction-based%20video%20editing.%20On%20the%20data%20side%2C%20we%20compose%20existing%20experts%20with%20fast%20inverses%20to%20build%20diverse%20video%20pairs%2C%20lift%20image%20edit%20pairs%20into%20videos%20via%20single-frame%20supervision%20and%20pseudo%20pairs%20with%20shared%20affine%20motion%2C%20mine%20dense-captioned%20clips%20for%20video%20pairs%2C%20and%20add%20transition%20supervision%20to%20teach%20how%20edits%20unfold.%20On%20the%20model%20side%2C%20we%20observe%20that%20pretrained%20text-to-video%20models%20possess%20editing%20capability%2C%20motivating%20a%20simplified%20design.%20Simple%20sequence%20concatenation%20for%20conditioning%20with%20light%20LoRA%20fine-tuning%20suffices%20to%20train%20a%20strong%20model.%20For%20control%2C%20we%20unify%20spatiotemporal%20control%20via%20a%20single%20mask%20mechanism%20and%20support%20optional%20reference%20images.%20Overall%2C%20EasyV2V%20works%20with%20flexible%20inputs%2C%20e.g.%2C%20video%2Btext%2C%20video%2Bmask%2Btext%2C%20video%2Bmask%2Breference%2Btext%2C%20and%20achieves%20state-of-the-art%20video%20editing%20results%2C%20surpassing%20concurrent%20and%20commercial%20systems.%20Project%20page%3A%20https%3A//snap-research.github.io/easyv2v/%0ALink%3A%20http%3A//arxiv.org/abs/2512.16920v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEasyV2V%253A%2520A%2520High-quality%2520Instruction-based%2520Video%2520Editing%2520Framework%26entry.906535625%3DJinjie%2520Mai%2520and%2520Chaoyang%2520Wang%2520and%2520Guocheng%2520Gordon%2520Qian%2520and%2520Willi%2520Menapace%2520and%2520Sergey%2520Tulyakov%2520and%2520Bernard%2520Ghanem%2520and%2520Peter%2520Wonka%2520and%2520Ashkan%2520Mirzaei%26entry.1292438233%3DWhile%2520image%2520editing%2520has%2520advanced%2520rapidly%252C%2520video%2520editing%2520remains%2520less%2520explored%252C%2520facing%2520challenges%2520in%2520consistency%252C%2520control%252C%2520and%2520generalization.%2520We%2520study%2520the%2520design%2520space%2520of%2520data%252C%2520architecture%252C%2520and%2520control%252C%2520and%2520introduce%2520%255Cemph%257BEasyV2V%257D%252C%2520a%2520simple%2520and%2520effective%2520framework%2520for%2520instruction-based%2520video%2520editing.%2520On%2520the%2520data%2520side%252C%2520we%2520compose%2520existing%2520experts%2520with%2520fast%2520inverses%2520to%2520build%2520diverse%2520video%2520pairs%252C%2520lift%2520image%2520edit%2520pairs%2520into%2520videos%2520via%2520single-frame%2520supervision%2520and%2520pseudo%2520pairs%2520with%2520shared%2520affine%2520motion%252C%2520mine%2520dense-captioned%2520clips%2520for%2520video%2520pairs%252C%2520and%2520add%2520transition%2520supervision%2520to%2520teach%2520how%2520edits%2520unfold.%2520On%2520the%2520model%2520side%252C%2520we%2520observe%2520that%2520pretrained%2520text-to-video%2520models%2520possess%2520editing%2520capability%252C%2520motivating%2520a%2520simplified%2520design.%2520Simple%2520sequence%2520concatenation%2520for%2520conditioning%2520with%2520light%2520LoRA%2520fine-tuning%2520suffices%2520to%2520train%2520a%2520strong%2520model.%2520For%2520control%252C%2520we%2520unify%2520spatiotemporal%2520control%2520via%2520a%2520single%2520mask%2520mechanism%2520and%2520support%2520optional%2520reference%2520images.%2520Overall%252C%2520EasyV2V%2520works%2520with%2520flexible%2520inputs%252C%2520e.g.%252C%2520video%252Btext%252C%2520video%252Bmask%252Btext%252C%2520video%252Bmask%252Breference%252Btext%252C%2520and%2520achieves%2520state-of-the-art%2520video%2520editing%2520results%252C%2520surpassing%2520concurrent%2520and%2520commercial%2520systems.%2520Project%2520page%253A%2520https%253A//snap-research.github.io/easyv2v/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16920v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EasyV2V%3A%20A%20High-quality%20Instruction-based%20Video%20Editing%20Framework&entry.906535625=Jinjie%20Mai%20and%20Chaoyang%20Wang%20and%20Guocheng%20Gordon%20Qian%20and%20Willi%20Menapace%20and%20Sergey%20Tulyakov%20and%20Bernard%20Ghanem%20and%20Peter%20Wonka%20and%20Ashkan%20Mirzaei&entry.1292438233=While%20image%20editing%20has%20advanced%20rapidly%2C%20video%20editing%20remains%20less%20explored%2C%20facing%20challenges%20in%20consistency%2C%20control%2C%20and%20generalization.%20We%20study%20the%20design%20space%20of%20data%2C%20architecture%2C%20and%20control%2C%20and%20introduce%20%5Cemph%7BEasyV2V%7D%2C%20a%20simple%20and%20effective%20framework%20for%20instruction-based%20video%20editing.%20On%20the%20data%20side%2C%20we%20compose%20existing%20experts%20with%20fast%20inverses%20to%20build%20diverse%20video%20pairs%2C%20lift%20image%20edit%20pairs%20into%20videos%20via%20single-frame%20supervision%20and%20pseudo%20pairs%20with%20shared%20affine%20motion%2C%20mine%20dense-captioned%20clips%20for%20video%20pairs%2C%20and%20add%20transition%20supervision%20to%20teach%20how%20edits%20unfold.%20On%20the%20model%20side%2C%20we%20observe%20that%20pretrained%20text-to-video%20models%20possess%20editing%20capability%2C%20motivating%20a%20simplified%20design.%20Simple%20sequence%20concatenation%20for%20conditioning%20with%20light%20LoRA%20fine-tuning%20suffices%20to%20train%20a%20strong%20model.%20For%20control%2C%20we%20unify%20spatiotemporal%20control%20via%20a%20single%20mask%20mechanism%20and%20support%20optional%20reference%20images.%20Overall%2C%20EasyV2V%20works%20with%20flexible%20inputs%2C%20e.g.%2C%20video%2Btext%2C%20video%2Bmask%2Btext%2C%20video%2Bmask%2Breference%2Btext%2C%20and%20achieves%20state-of-the-art%20video%20editing%20results%2C%20surpassing%20concurrent%20and%20commercial%20systems.%20Project%20page%3A%20https%3A//snap-research.github.io/easyv2v/&entry.1838667208=http%3A//arxiv.org/abs/2512.16920v1&entry.124074799=Read"},
{"title": "FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction", "author": "Shuyuan Tu and Yueming Pan and Yinming Huang and Xintong Han and Zhen Xing and Qi Dai and Kai Qiu and Chong Luo and Zuxuan Wu", "abstract": "Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.", "link": "http://arxiv.org/abs/2512.16900v1", "date": "2025-12-18", "relevancy": 2.4252, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6249}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6059}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5992}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlashPortrait%3A%206x%20Faster%20Infinite%20Portrait%20Animation%20with%20Adaptive%20Latent%20Prediction&body=Title%3A%20FlashPortrait%3A%206x%20Faster%20Infinite%20Portrait%20Animation%20with%20Adaptive%20Latent%20Prediction%0AAuthor%3A%20Shuyuan%20Tu%20and%20Yueming%20Pan%20and%20Yinming%20Huang%20and%20Xintong%20Han%20and%20Zhen%20Xing%20and%20Qi%20Dai%20and%20Kai%20Qiu%20and%20Chong%20Luo%20and%20Zuxuan%20Wu%0AAbstract%3A%20Current%20diffusion-based%20acceleration%20methods%20for%20long-portrait%20animation%20struggle%20to%20ensure%20identity%20%28ID%29%20consistency.%20This%20paper%20presents%20FlashPortrait%2C%20an%20end-to-end%20video%20diffusion%20transformer%20capable%20of%20synthesizing%20ID-preserving%2C%20infinite-length%20videos%20while%20achieving%20up%20to%206x%20acceleration%20in%20inference%20speed.%20In%20particular%2C%20FlashPortrait%20begins%20by%20computing%20the%20identity-agnostic%20facial%20expression%20features%20with%20an%20off-the-shelf%20extractor.%20It%20then%20introduces%20a%20Normalized%20Facial%20Expression%20Block%20to%20align%20facial%20features%20with%20diffusion%20latents%20by%20normalizing%20them%20with%20their%20respective%20means%20and%20variances%2C%20thereby%20improving%20identity%20stability%20in%20facial%20modeling.%20During%20inference%2C%20FlashPortrait%20adopts%20a%20dynamic%20sliding-window%20scheme%20with%20weighted%20blending%20in%20overlapping%20areas%2C%20ensuring%20smooth%20transitions%20and%20ID%20consistency%20in%20long%20animations.%20In%20each%20context%20window%2C%20based%20on%20the%20latent%20variation%20rate%20at%20particular%20timesteps%20and%20the%20derivative%20magnitude%20ratio%20among%20diffusion%20layers%2C%20FlashPortrait%20utilizes%20higher-order%20latent%20derivatives%20at%20the%20current%20timestep%20to%20directly%20predict%20latents%20at%20future%20timesteps%2C%20thereby%20skipping%20several%20denoising%20steps%20and%20achieving%206x%20speed%20acceleration.%20Experiments%20on%20benchmarks%20show%20the%20effectiveness%20of%20FlashPortrait%20both%20qualitatively%20and%20quantitatively.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16900v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlashPortrait%253A%25206x%2520Faster%2520Infinite%2520Portrait%2520Animation%2520with%2520Adaptive%2520Latent%2520Prediction%26entry.906535625%3DShuyuan%2520Tu%2520and%2520Yueming%2520Pan%2520and%2520Yinming%2520Huang%2520and%2520Xintong%2520Han%2520and%2520Zhen%2520Xing%2520and%2520Qi%2520Dai%2520and%2520Kai%2520Qiu%2520and%2520Chong%2520Luo%2520and%2520Zuxuan%2520Wu%26entry.1292438233%3DCurrent%2520diffusion-based%2520acceleration%2520methods%2520for%2520long-portrait%2520animation%2520struggle%2520to%2520ensure%2520identity%2520%2528ID%2529%2520consistency.%2520This%2520paper%2520presents%2520FlashPortrait%252C%2520an%2520end-to-end%2520video%2520diffusion%2520transformer%2520capable%2520of%2520synthesizing%2520ID-preserving%252C%2520infinite-length%2520videos%2520while%2520achieving%2520up%2520to%25206x%2520acceleration%2520in%2520inference%2520speed.%2520In%2520particular%252C%2520FlashPortrait%2520begins%2520by%2520computing%2520the%2520identity-agnostic%2520facial%2520expression%2520features%2520with%2520an%2520off-the-shelf%2520extractor.%2520It%2520then%2520introduces%2520a%2520Normalized%2520Facial%2520Expression%2520Block%2520to%2520align%2520facial%2520features%2520with%2520diffusion%2520latents%2520by%2520normalizing%2520them%2520with%2520their%2520respective%2520means%2520and%2520variances%252C%2520thereby%2520improving%2520identity%2520stability%2520in%2520facial%2520modeling.%2520During%2520inference%252C%2520FlashPortrait%2520adopts%2520a%2520dynamic%2520sliding-window%2520scheme%2520with%2520weighted%2520blending%2520in%2520overlapping%2520areas%252C%2520ensuring%2520smooth%2520transitions%2520and%2520ID%2520consistency%2520in%2520long%2520animations.%2520In%2520each%2520context%2520window%252C%2520based%2520on%2520the%2520latent%2520variation%2520rate%2520at%2520particular%2520timesteps%2520and%2520the%2520derivative%2520magnitude%2520ratio%2520among%2520diffusion%2520layers%252C%2520FlashPortrait%2520utilizes%2520higher-order%2520latent%2520derivatives%2520at%2520the%2520current%2520timestep%2520to%2520directly%2520predict%2520latents%2520at%2520future%2520timesteps%252C%2520thereby%2520skipping%2520several%2520denoising%2520steps%2520and%2520achieving%25206x%2520speed%2520acceleration.%2520Experiments%2520on%2520benchmarks%2520show%2520the%2520effectiveness%2520of%2520FlashPortrait%2520both%2520qualitatively%2520and%2520quantitatively.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16900v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlashPortrait%3A%206x%20Faster%20Infinite%20Portrait%20Animation%20with%20Adaptive%20Latent%20Prediction&entry.906535625=Shuyuan%20Tu%20and%20Yueming%20Pan%20and%20Yinming%20Huang%20and%20Xintong%20Han%20and%20Zhen%20Xing%20and%20Qi%20Dai%20and%20Kai%20Qiu%20and%20Chong%20Luo%20and%20Zuxuan%20Wu&entry.1292438233=Current%20diffusion-based%20acceleration%20methods%20for%20long-portrait%20animation%20struggle%20to%20ensure%20identity%20%28ID%29%20consistency.%20This%20paper%20presents%20FlashPortrait%2C%20an%20end-to-end%20video%20diffusion%20transformer%20capable%20of%20synthesizing%20ID-preserving%2C%20infinite-length%20videos%20while%20achieving%20up%20to%206x%20acceleration%20in%20inference%20speed.%20In%20particular%2C%20FlashPortrait%20begins%20by%20computing%20the%20identity-agnostic%20facial%20expression%20features%20with%20an%20off-the-shelf%20extractor.%20It%20then%20introduces%20a%20Normalized%20Facial%20Expression%20Block%20to%20align%20facial%20features%20with%20diffusion%20latents%20by%20normalizing%20them%20with%20their%20respective%20means%20and%20variances%2C%20thereby%20improving%20identity%20stability%20in%20facial%20modeling.%20During%20inference%2C%20FlashPortrait%20adopts%20a%20dynamic%20sliding-window%20scheme%20with%20weighted%20blending%20in%20overlapping%20areas%2C%20ensuring%20smooth%20transitions%20and%20ID%20consistency%20in%20long%20animations.%20In%20each%20context%20window%2C%20based%20on%20the%20latent%20variation%20rate%20at%20particular%20timesteps%20and%20the%20derivative%20magnitude%20ratio%20among%20diffusion%20layers%2C%20FlashPortrait%20utilizes%20higher-order%20latent%20derivatives%20at%20the%20current%20timestep%20to%20directly%20predict%20latents%20at%20future%20timesteps%2C%20thereby%20skipping%20several%20denoising%20steps%20and%20achieving%206x%20speed%20acceleration.%20Experiments%20on%20benchmarks%20show%20the%20effectiveness%20of%20FlashPortrait%20both%20qualitatively%20and%20quantitatively.&entry.1838667208=http%3A//arxiv.org/abs/2512.16900v1&entry.124074799=Read"},
{"title": "Team Westwood Solution for MIDOG 2025 Challenge: An Ensemble-CNN-Based Approach For Mitosis Detection And Classification", "author": "Tengyou Xu and Haochen Yang and Xiang 'Anthony' Chen and Hongyan Gu and Mohammad Haeri", "abstract": "This abstract presents our solution (Team Westwood) for mitosis detection and atypical mitosis classification in the MItosis DOmain Generalization (MIDOG) 2025 challenge. For mitosis detection, we trained an nnUNetV2 for initial mitosis candidate screening with high sensitivity, followed by a random forest classifier ensembling predictions of three convolutional neural networks (CNNs): EfficientNet-b3, EfficientNet-b5, and EfficientNetV2-s. For the atypical mitosis classification, we trained another random forest classifier ensembling the predictions of three CNNs: EfficientNet-b3, EfficientNet-b5, and InceptionV3. On the preliminary test set, our solution achieved an F1 score of 0.7450 for track 1 mitosis detection, and a balanced accuracy of 0.8722 for track 2 atypical mitosis classification. On the final test set, our solution achieved an F1 score of 0.6972 for track 1 mitosis detection, and a balanced accuracy of 0.8242 for track 2 atypical mitosis classification.", "link": "http://arxiv.org/abs/2509.02600v3", "date": "2025-12-18", "relevancy": 2.4133, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4966}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4949}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Team%20Westwood%20Solution%20for%20MIDOG%202025%20Challenge%3A%20An%20Ensemble-CNN-Based%20Approach%20For%20Mitosis%20Detection%20And%20Classification&body=Title%3A%20Team%20Westwood%20Solution%20for%20MIDOG%202025%20Challenge%3A%20An%20Ensemble-CNN-Based%20Approach%20For%20Mitosis%20Detection%20And%20Classification%0AAuthor%3A%20Tengyou%20Xu%20and%20Haochen%20Yang%20and%20Xiang%20%27Anthony%27%20Chen%20and%20Hongyan%20Gu%20and%20Mohammad%20Haeri%0AAbstract%3A%20This%20abstract%20presents%20our%20solution%20%28Team%20Westwood%29%20for%20mitosis%20detection%20and%20atypical%20mitosis%20classification%20in%20the%20MItosis%20DOmain%20Generalization%20%28MIDOG%29%202025%20challenge.%20For%20mitosis%20detection%2C%20we%20trained%20an%20nnUNetV2%20for%20initial%20mitosis%20candidate%20screening%20with%20high%20sensitivity%2C%20followed%20by%20a%20random%20forest%20classifier%20ensembling%20predictions%20of%20three%20convolutional%20neural%20networks%20%28CNNs%29%3A%20EfficientNet-b3%2C%20EfficientNet-b5%2C%20and%20EfficientNetV2-s.%20For%20the%20atypical%20mitosis%20classification%2C%20we%20trained%20another%20random%20forest%20classifier%20ensembling%20the%20predictions%20of%20three%20CNNs%3A%20EfficientNet-b3%2C%20EfficientNet-b5%2C%20and%20InceptionV3.%20On%20the%20preliminary%20test%20set%2C%20our%20solution%20achieved%20an%20F1%20score%20of%200.7450%20for%20track%201%20mitosis%20detection%2C%20and%20a%20balanced%20accuracy%20of%200.8722%20for%20track%202%20atypical%20mitosis%20classification.%20On%20the%20final%20test%20set%2C%20our%20solution%20achieved%20an%20F1%20score%20of%200.6972%20for%20track%201%20mitosis%20detection%2C%20and%20a%20balanced%20accuracy%20of%200.8242%20for%20track%202%20atypical%20mitosis%20classification.%0ALink%3A%20http%3A//arxiv.org/abs/2509.02600v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeam%2520Westwood%2520Solution%2520for%2520MIDOG%25202025%2520Challenge%253A%2520An%2520Ensemble-CNN-Based%2520Approach%2520For%2520Mitosis%2520Detection%2520And%2520Classification%26entry.906535625%3DTengyou%2520Xu%2520and%2520Haochen%2520Yang%2520and%2520Xiang%2520%2527Anthony%2527%2520Chen%2520and%2520Hongyan%2520Gu%2520and%2520Mohammad%2520Haeri%26entry.1292438233%3DThis%2520abstract%2520presents%2520our%2520solution%2520%2528Team%2520Westwood%2529%2520for%2520mitosis%2520detection%2520and%2520atypical%2520mitosis%2520classification%2520in%2520the%2520MItosis%2520DOmain%2520Generalization%2520%2528MIDOG%2529%25202025%2520challenge.%2520For%2520mitosis%2520detection%252C%2520we%2520trained%2520an%2520nnUNetV2%2520for%2520initial%2520mitosis%2520candidate%2520screening%2520with%2520high%2520sensitivity%252C%2520followed%2520by%2520a%2520random%2520forest%2520classifier%2520ensembling%2520predictions%2520of%2520three%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%253A%2520EfficientNet-b3%252C%2520EfficientNet-b5%252C%2520and%2520EfficientNetV2-s.%2520For%2520the%2520atypical%2520mitosis%2520classification%252C%2520we%2520trained%2520another%2520random%2520forest%2520classifier%2520ensembling%2520the%2520predictions%2520of%2520three%2520CNNs%253A%2520EfficientNet-b3%252C%2520EfficientNet-b5%252C%2520and%2520InceptionV3.%2520On%2520the%2520preliminary%2520test%2520set%252C%2520our%2520solution%2520achieved%2520an%2520F1%2520score%2520of%25200.7450%2520for%2520track%25201%2520mitosis%2520detection%252C%2520and%2520a%2520balanced%2520accuracy%2520of%25200.8722%2520for%2520track%25202%2520atypical%2520mitosis%2520classification.%2520On%2520the%2520final%2520test%2520set%252C%2520our%2520solution%2520achieved%2520an%2520F1%2520score%2520of%25200.6972%2520for%2520track%25201%2520mitosis%2520detection%252C%2520and%2520a%2520balanced%2520accuracy%2520of%25200.8242%2520for%2520track%25202%2520atypical%2520mitosis%2520classification.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.02600v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Team%20Westwood%20Solution%20for%20MIDOG%202025%20Challenge%3A%20An%20Ensemble-CNN-Based%20Approach%20For%20Mitosis%20Detection%20And%20Classification&entry.906535625=Tengyou%20Xu%20and%20Haochen%20Yang%20and%20Xiang%20%27Anthony%27%20Chen%20and%20Hongyan%20Gu%20and%20Mohammad%20Haeri&entry.1292438233=This%20abstract%20presents%20our%20solution%20%28Team%20Westwood%29%20for%20mitosis%20detection%20and%20atypical%20mitosis%20classification%20in%20the%20MItosis%20DOmain%20Generalization%20%28MIDOG%29%202025%20challenge.%20For%20mitosis%20detection%2C%20we%20trained%20an%20nnUNetV2%20for%20initial%20mitosis%20candidate%20screening%20with%20high%20sensitivity%2C%20followed%20by%20a%20random%20forest%20classifier%20ensembling%20predictions%20of%20three%20convolutional%20neural%20networks%20%28CNNs%29%3A%20EfficientNet-b3%2C%20EfficientNet-b5%2C%20and%20EfficientNetV2-s.%20For%20the%20atypical%20mitosis%20classification%2C%20we%20trained%20another%20random%20forest%20classifier%20ensembling%20the%20predictions%20of%20three%20CNNs%3A%20EfficientNet-b3%2C%20EfficientNet-b5%2C%20and%20InceptionV3.%20On%20the%20preliminary%20test%20set%2C%20our%20solution%20achieved%20an%20F1%20score%20of%200.7450%20for%20track%201%20mitosis%20detection%2C%20and%20a%20balanced%20accuracy%20of%200.8722%20for%20track%202%20atypical%20mitosis%20classification.%20On%20the%20final%20test%20set%2C%20our%20solution%20achieved%20an%20F1%20score%20of%200.6972%20for%20track%201%20mitosis%20detection%2C%20and%20a%20balanced%20accuracy%20of%200.8242%20for%20track%202%20atypical%20mitosis%20classification.&entry.1838667208=http%3A//arxiv.org/abs/2509.02600v3&entry.124074799=Read"},
{"title": "M-PhyGs: Multi-Material Object Dynamics from Video", "author": "Norika Wada and Kohei Yamashita and Ryo Kawahara and Ko Nishino", "abstract": "Knowledge of the physical material properties governing the dynamics of a real-world object becomes necessary to accurately anticipate its response to unseen interactions. Existing methods for estimating such physical material parameters from visual data assume homogeneous single-material objects, pre-learned dynamics, or simplistic topologies. Real-world objects, however, are often complex in material composition and geometry lying outside the realm of these assumptions. In this paper, we particularly focus on flowers as a representative common object. We introduce Multi-material Physical Gaussians (M-PhyGs) to estimate the material composition and parameters of such multi-material complex natural objects from video. From a short video captured in a natural setting, M-PhyGs jointly segments the object into similar materials and recovers their continuum mechanical parameters while accounting for gravity. M-PhyGs achieves this efficiently with newly introduced cascaded 3D and 2D losses, and by leveraging temporal mini-batching. We introduce a dataset, Phlowers, of people interacting with flowers as a novel platform to evaluate the accuracy of this challenging task of multi-material physical parameter estimation. Experimental results on Phlowers dataset demonstrate the accuracy and effectiveness of M-PhyGs and its components.", "link": "http://arxiv.org/abs/2512.16885v1", "date": "2025-12-18", "relevancy": 2.4131, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6147}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6003}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M-PhyGs%3A%20Multi-Material%20Object%20Dynamics%20from%20Video&body=Title%3A%20M-PhyGs%3A%20Multi-Material%20Object%20Dynamics%20from%20Video%0AAuthor%3A%20Norika%20Wada%20and%20Kohei%20Yamashita%20and%20Ryo%20Kawahara%20and%20Ko%20Nishino%0AAbstract%3A%20Knowledge%20of%20the%20physical%20material%20properties%20governing%20the%20dynamics%20of%20a%20real-world%20object%20becomes%20necessary%20to%20accurately%20anticipate%20its%20response%20to%20unseen%20interactions.%20Existing%20methods%20for%20estimating%20such%20physical%20material%20parameters%20from%20visual%20data%20assume%20homogeneous%20single-material%20objects%2C%20pre-learned%20dynamics%2C%20or%20simplistic%20topologies.%20Real-world%20objects%2C%20however%2C%20are%20often%20complex%20in%20material%20composition%20and%20geometry%20lying%20outside%20the%20realm%20of%20these%20assumptions.%20In%20this%20paper%2C%20we%20particularly%20focus%20on%20flowers%20as%20a%20representative%20common%20object.%20We%20introduce%20Multi-material%20Physical%20Gaussians%20%28M-PhyGs%29%20to%20estimate%20the%20material%20composition%20and%20parameters%20of%20such%20multi-material%20complex%20natural%20objects%20from%20video.%20From%20a%20short%20video%20captured%20in%20a%20natural%20setting%2C%20M-PhyGs%20jointly%20segments%20the%20object%20into%20similar%20materials%20and%20recovers%20their%20continuum%20mechanical%20parameters%20while%20accounting%20for%20gravity.%20M-PhyGs%20achieves%20this%20efficiently%20with%20newly%20introduced%20cascaded%203D%20and%202D%20losses%2C%20and%20by%20leveraging%20temporal%20mini-batching.%20We%20introduce%20a%20dataset%2C%20Phlowers%2C%20of%20people%20interacting%20with%20flowers%20as%20a%20novel%20platform%20to%20evaluate%20the%20accuracy%20of%20this%20challenging%20task%20of%20multi-material%20physical%20parameter%20estimation.%20Experimental%20results%20on%20Phlowers%20dataset%20demonstrate%20the%20accuracy%20and%20effectiveness%20of%20M-PhyGs%20and%20its%20components.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16885v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM-PhyGs%253A%2520Multi-Material%2520Object%2520Dynamics%2520from%2520Video%26entry.906535625%3DNorika%2520Wada%2520and%2520Kohei%2520Yamashita%2520and%2520Ryo%2520Kawahara%2520and%2520Ko%2520Nishino%26entry.1292438233%3DKnowledge%2520of%2520the%2520physical%2520material%2520properties%2520governing%2520the%2520dynamics%2520of%2520a%2520real-world%2520object%2520becomes%2520necessary%2520to%2520accurately%2520anticipate%2520its%2520response%2520to%2520unseen%2520interactions.%2520Existing%2520methods%2520for%2520estimating%2520such%2520physical%2520material%2520parameters%2520from%2520visual%2520data%2520assume%2520homogeneous%2520single-material%2520objects%252C%2520pre-learned%2520dynamics%252C%2520or%2520simplistic%2520topologies.%2520Real-world%2520objects%252C%2520however%252C%2520are%2520often%2520complex%2520in%2520material%2520composition%2520and%2520geometry%2520lying%2520outside%2520the%2520realm%2520of%2520these%2520assumptions.%2520In%2520this%2520paper%252C%2520we%2520particularly%2520focus%2520on%2520flowers%2520as%2520a%2520representative%2520common%2520object.%2520We%2520introduce%2520Multi-material%2520Physical%2520Gaussians%2520%2528M-PhyGs%2529%2520to%2520estimate%2520the%2520material%2520composition%2520and%2520parameters%2520of%2520such%2520multi-material%2520complex%2520natural%2520objects%2520from%2520video.%2520From%2520a%2520short%2520video%2520captured%2520in%2520a%2520natural%2520setting%252C%2520M-PhyGs%2520jointly%2520segments%2520the%2520object%2520into%2520similar%2520materials%2520and%2520recovers%2520their%2520continuum%2520mechanical%2520parameters%2520while%2520accounting%2520for%2520gravity.%2520M-PhyGs%2520achieves%2520this%2520efficiently%2520with%2520newly%2520introduced%2520cascaded%25203D%2520and%25202D%2520losses%252C%2520and%2520by%2520leveraging%2520temporal%2520mini-batching.%2520We%2520introduce%2520a%2520dataset%252C%2520Phlowers%252C%2520of%2520people%2520interacting%2520with%2520flowers%2520as%2520a%2520novel%2520platform%2520to%2520evaluate%2520the%2520accuracy%2520of%2520this%2520challenging%2520task%2520of%2520multi-material%2520physical%2520parameter%2520estimation.%2520Experimental%2520results%2520on%2520Phlowers%2520dataset%2520demonstrate%2520the%2520accuracy%2520and%2520effectiveness%2520of%2520M-PhyGs%2520and%2520its%2520components.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16885v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M-PhyGs%3A%20Multi-Material%20Object%20Dynamics%20from%20Video&entry.906535625=Norika%20Wada%20and%20Kohei%20Yamashita%20and%20Ryo%20Kawahara%20and%20Ko%20Nishino&entry.1292438233=Knowledge%20of%20the%20physical%20material%20properties%20governing%20the%20dynamics%20of%20a%20real-world%20object%20becomes%20necessary%20to%20accurately%20anticipate%20its%20response%20to%20unseen%20interactions.%20Existing%20methods%20for%20estimating%20such%20physical%20material%20parameters%20from%20visual%20data%20assume%20homogeneous%20single-material%20objects%2C%20pre-learned%20dynamics%2C%20or%20simplistic%20topologies.%20Real-world%20objects%2C%20however%2C%20are%20often%20complex%20in%20material%20composition%20and%20geometry%20lying%20outside%20the%20realm%20of%20these%20assumptions.%20In%20this%20paper%2C%20we%20particularly%20focus%20on%20flowers%20as%20a%20representative%20common%20object.%20We%20introduce%20Multi-material%20Physical%20Gaussians%20%28M-PhyGs%29%20to%20estimate%20the%20material%20composition%20and%20parameters%20of%20such%20multi-material%20complex%20natural%20objects%20from%20video.%20From%20a%20short%20video%20captured%20in%20a%20natural%20setting%2C%20M-PhyGs%20jointly%20segments%20the%20object%20into%20similar%20materials%20and%20recovers%20their%20continuum%20mechanical%20parameters%20while%20accounting%20for%20gravity.%20M-PhyGs%20achieves%20this%20efficiently%20with%20newly%20introduced%20cascaded%203D%20and%202D%20losses%2C%20and%20by%20leveraging%20temporal%20mini-batching.%20We%20introduce%20a%20dataset%2C%20Phlowers%2C%20of%20people%20interacting%20with%20flowers%20as%20a%20novel%20platform%20to%20evaluate%20the%20accuracy%20of%20this%20challenging%20task%20of%20multi-material%20physical%20parameter%20estimation.%20Experimental%20results%20on%20Phlowers%20dataset%20demonstrate%20the%20accuracy%20and%20effectiveness%20of%20M-PhyGs%20and%20its%20components.&entry.1838667208=http%3A//arxiv.org/abs/2512.16885v1&entry.124074799=Read"},
{"title": "Sequencing to Mitigate Catastrophic Forgetting in Continual Learning", "author": "Hesham G. Moussa and Aroosa Hameed and Arashmid Akhavain", "abstract": "To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, and exploit knowledge throughout its lifetime. This ability, known as Continual learning, provides a foundation for AI systems to develop themselves adaptively. Catastrophic forgetting is a major challenge to the progress of Continual Learning approaches, where learning a new task usually results in a dramatic performance drop on previously learned ones. Many approaches have emerged to counteract the impact of CF. Most of the proposed approaches can be categorized into five classes: replay-based, regularization-based, optimization-based, representation-based, and architecture-based. In this work, we approach the problem from a different angle, specifically by considering the optimal sequencing of tasks as they are presented to the model. We investigate the role of task sequencing in mitigating CF and propose a method for determining the optimal task order. The proposed method leverages zero-shot scoring algorithms inspired by neural architecture search (NAS). Results demonstrate that intelligent task sequencing can substantially reduce CF. Moreover, when combined with traditional continual learning strategies, sequencing offers enhanced performance and robustness against forgetting. Additionally, the presented approaches can find applications in other fields, such as curriculum learning.", "link": "http://arxiv.org/abs/2512.16871v1", "date": "2025-12-18", "relevancy": 2.4117, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5001}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4735}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sequencing%20to%20Mitigate%20Catastrophic%20Forgetting%20in%20Continual%20Learning&body=Title%3A%20Sequencing%20to%20Mitigate%20Catastrophic%20Forgetting%20in%20Continual%20Learning%0AAuthor%3A%20Hesham%20G.%20Moussa%20and%20Aroosa%20Hameed%20and%20Arashmid%20Akhavain%0AAbstract%3A%20To%20cope%20with%20real-world%20dynamics%2C%20an%20intelligent%20system%20needs%20to%20incrementally%20acquire%2C%20update%2C%20and%20exploit%20knowledge%20throughout%20its%20lifetime.%20This%20ability%2C%20known%20as%20Continual%20learning%2C%20provides%20a%20foundation%20for%20AI%20systems%20to%20develop%20themselves%20adaptively.%20Catastrophic%20forgetting%20is%20a%20major%20challenge%20to%20the%20progress%20of%20Continual%20Learning%20approaches%2C%20where%20learning%20a%20new%20task%20usually%20results%20in%20a%20dramatic%20performance%20drop%20on%20previously%20learned%20ones.%20Many%20approaches%20have%20emerged%20to%20counteract%20the%20impact%20of%20CF.%20Most%20of%20the%20proposed%20approaches%20can%20be%20categorized%20into%20five%20classes%3A%20replay-based%2C%20regularization-based%2C%20optimization-based%2C%20representation-based%2C%20and%20architecture-based.%20In%20this%20work%2C%20we%20approach%20the%20problem%20from%20a%20different%20angle%2C%20specifically%20by%20considering%20the%20optimal%20sequencing%20of%20tasks%20as%20they%20are%20presented%20to%20the%20model.%20We%20investigate%20the%20role%20of%20task%20sequencing%20in%20mitigating%20CF%20and%20propose%20a%20method%20for%20determining%20the%20optimal%20task%20order.%20The%20proposed%20method%20leverages%20zero-shot%20scoring%20algorithms%20inspired%20by%20neural%20architecture%20search%20%28NAS%29.%20Results%20demonstrate%20that%20intelligent%20task%20sequencing%20can%20substantially%20reduce%20CF.%20Moreover%2C%20when%20combined%20with%20traditional%20continual%20learning%20strategies%2C%20sequencing%20offers%20enhanced%20performance%20and%20robustness%20against%20forgetting.%20Additionally%2C%20the%20presented%20approaches%20can%20find%20applications%20in%20other%20fields%2C%20such%20as%20curriculum%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16871v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSequencing%2520to%2520Mitigate%2520Catastrophic%2520Forgetting%2520in%2520Continual%2520Learning%26entry.906535625%3DHesham%2520G.%2520Moussa%2520and%2520Aroosa%2520Hameed%2520and%2520Arashmid%2520Akhavain%26entry.1292438233%3DTo%2520cope%2520with%2520real-world%2520dynamics%252C%2520an%2520intelligent%2520system%2520needs%2520to%2520incrementally%2520acquire%252C%2520update%252C%2520and%2520exploit%2520knowledge%2520throughout%2520its%2520lifetime.%2520This%2520ability%252C%2520known%2520as%2520Continual%2520learning%252C%2520provides%2520a%2520foundation%2520for%2520AI%2520systems%2520to%2520develop%2520themselves%2520adaptively.%2520Catastrophic%2520forgetting%2520is%2520a%2520major%2520challenge%2520to%2520the%2520progress%2520of%2520Continual%2520Learning%2520approaches%252C%2520where%2520learning%2520a%2520new%2520task%2520usually%2520results%2520in%2520a%2520dramatic%2520performance%2520drop%2520on%2520previously%2520learned%2520ones.%2520Many%2520approaches%2520have%2520emerged%2520to%2520counteract%2520the%2520impact%2520of%2520CF.%2520Most%2520of%2520the%2520proposed%2520approaches%2520can%2520be%2520categorized%2520into%2520five%2520classes%253A%2520replay-based%252C%2520regularization-based%252C%2520optimization-based%252C%2520representation-based%252C%2520and%2520architecture-based.%2520In%2520this%2520work%252C%2520we%2520approach%2520the%2520problem%2520from%2520a%2520different%2520angle%252C%2520specifically%2520by%2520considering%2520the%2520optimal%2520sequencing%2520of%2520tasks%2520as%2520they%2520are%2520presented%2520to%2520the%2520model.%2520We%2520investigate%2520the%2520role%2520of%2520task%2520sequencing%2520in%2520mitigating%2520CF%2520and%2520propose%2520a%2520method%2520for%2520determining%2520the%2520optimal%2520task%2520order.%2520The%2520proposed%2520method%2520leverages%2520zero-shot%2520scoring%2520algorithms%2520inspired%2520by%2520neural%2520architecture%2520search%2520%2528NAS%2529.%2520Results%2520demonstrate%2520that%2520intelligent%2520task%2520sequencing%2520can%2520substantially%2520reduce%2520CF.%2520Moreover%252C%2520when%2520combined%2520with%2520traditional%2520continual%2520learning%2520strategies%252C%2520sequencing%2520offers%2520enhanced%2520performance%2520and%2520robustness%2520against%2520forgetting.%2520Additionally%252C%2520the%2520presented%2520approaches%2520can%2520find%2520applications%2520in%2520other%2520fields%252C%2520such%2520as%2520curriculum%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16871v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sequencing%20to%20Mitigate%20Catastrophic%20Forgetting%20in%20Continual%20Learning&entry.906535625=Hesham%20G.%20Moussa%20and%20Aroosa%20Hameed%20and%20Arashmid%20Akhavain&entry.1292438233=To%20cope%20with%20real-world%20dynamics%2C%20an%20intelligent%20system%20needs%20to%20incrementally%20acquire%2C%20update%2C%20and%20exploit%20knowledge%20throughout%20its%20lifetime.%20This%20ability%2C%20known%20as%20Continual%20learning%2C%20provides%20a%20foundation%20for%20AI%20systems%20to%20develop%20themselves%20adaptively.%20Catastrophic%20forgetting%20is%20a%20major%20challenge%20to%20the%20progress%20of%20Continual%20Learning%20approaches%2C%20where%20learning%20a%20new%20task%20usually%20results%20in%20a%20dramatic%20performance%20drop%20on%20previously%20learned%20ones.%20Many%20approaches%20have%20emerged%20to%20counteract%20the%20impact%20of%20CF.%20Most%20of%20the%20proposed%20approaches%20can%20be%20categorized%20into%20five%20classes%3A%20replay-based%2C%20regularization-based%2C%20optimization-based%2C%20representation-based%2C%20and%20architecture-based.%20In%20this%20work%2C%20we%20approach%20the%20problem%20from%20a%20different%20angle%2C%20specifically%20by%20considering%20the%20optimal%20sequencing%20of%20tasks%20as%20they%20are%20presented%20to%20the%20model.%20We%20investigate%20the%20role%20of%20task%20sequencing%20in%20mitigating%20CF%20and%20propose%20a%20method%20for%20determining%20the%20optimal%20task%20order.%20The%20proposed%20method%20leverages%20zero-shot%20scoring%20algorithms%20inspired%20by%20neural%20architecture%20search%20%28NAS%29.%20Results%20demonstrate%20that%20intelligent%20task%20sequencing%20can%20substantially%20reduce%20CF.%20Moreover%2C%20when%20combined%20with%20traditional%20continual%20learning%20strategies%2C%20sequencing%20offers%20enhanced%20performance%20and%20robustness%20against%20forgetting.%20Additionally%2C%20the%20presented%20approaches%20can%20find%20applications%20in%20other%20fields%2C%20such%20as%20curriculum%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2512.16871v1&entry.124074799=Read"},
{"title": "Iterative Feature Exclusion Ranking for Deep Tabular Learning", "author": "Fathi Said Emhemed Shaninah and AbdulRahman M. A. Baraka and Mohd Halim Mohd Noor", "abstract": "Tabular data is a common format for storing information in rows and columns to represent data entries and their features. Although deep neural networks have become the main approach for modeling a wide range of domains including computer vision and NLP, many of them are not well-suited for tabular data. Recently, a few deep learning models have been proposed for deep tabular learning, featuring an internal feature selection mechanism with end-to-end gradient-based optimization. However, their feature selection mechanisms are unidimensional, and hence fail to account for the contextual dependence of feature importance, potentially overlooking crucial interactions that govern complex tasks. In addition, they overlook the bias of high-impact features and the risk associated with the limitations of attention generalization. To address this limitation, this study proposes a novel iterative feature exclusion module that enhances the feature importance ranking in tabular data. The proposed module iteratively excludes each feature from the input data and computes the attention scores, which represent the impact of the features on the prediction. By aggregating the attention scores from each iteration, the proposed module generates a refined representation of feature importance that captures both global and local interactions between features. The effectiveness of the proposed module is evaluated on four public datasets. The results demonstrate that the proposed module consistently outperforms state-of-the-art methods and baseline models in feature ranking and classification tasks. The code is publicly available at https://github.com/abaraka2020/Iterative-Feature-Exclusion-Ranking-Module and https://github.com/mohalim/IFENet", "link": "http://arxiv.org/abs/2412.16442v2", "date": "2025-12-18", "relevancy": 2.3955, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.499}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4825}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Iterative%20Feature%20Exclusion%20Ranking%20for%20Deep%20Tabular%20Learning&body=Title%3A%20Iterative%20Feature%20Exclusion%20Ranking%20for%20Deep%20Tabular%20Learning%0AAuthor%3A%20Fathi%20Said%20Emhemed%20Shaninah%20and%20AbdulRahman%20M.%20A.%20Baraka%20and%20Mohd%20Halim%20Mohd%20Noor%0AAbstract%3A%20Tabular%20data%20is%20a%20common%20format%20for%20storing%20information%20in%20rows%20and%20columns%20to%20represent%20data%20entries%20and%20their%20features.%20Although%20deep%20neural%20networks%20have%20become%20the%20main%20approach%20for%20modeling%20a%20wide%20range%20of%20domains%20including%20computer%20vision%20and%20NLP%2C%20many%20of%20them%20are%20not%20well-suited%20for%20tabular%20data.%20Recently%2C%20a%20few%20deep%20learning%20models%20have%20been%20proposed%20for%20deep%20tabular%20learning%2C%20featuring%20an%20internal%20feature%20selection%20mechanism%20with%20end-to-end%20gradient-based%20optimization.%20However%2C%20their%20feature%20selection%20mechanisms%20are%20unidimensional%2C%20and%20hence%20fail%20to%20account%20for%20the%20contextual%20dependence%20of%20feature%20importance%2C%20potentially%20overlooking%20crucial%20interactions%20that%20govern%20complex%20tasks.%20In%20addition%2C%20they%20overlook%20the%20bias%20of%20high-impact%20features%20and%20the%20risk%20associated%20with%20the%20limitations%20of%20attention%20generalization.%20To%20address%20this%20limitation%2C%20this%20study%20proposes%20a%20novel%20iterative%20feature%20exclusion%20module%20that%20enhances%20the%20feature%20importance%20ranking%20in%20tabular%20data.%20The%20proposed%20module%20iteratively%20excludes%20each%20feature%20from%20the%20input%20data%20and%20computes%20the%20attention%20scores%2C%20which%20represent%20the%20impact%20of%20the%20features%20on%20the%20prediction.%20By%20aggregating%20the%20attention%20scores%20from%20each%20iteration%2C%20the%20proposed%20module%20generates%20a%20refined%20representation%20of%20feature%20importance%20that%20captures%20both%20global%20and%20local%20interactions%20between%20features.%20The%20effectiveness%20of%20the%20proposed%20module%20is%20evaluated%20on%20four%20public%20datasets.%20The%20results%20demonstrate%20that%20the%20proposed%20module%20consistently%20outperforms%20state-of-the-art%20methods%20and%20baseline%20models%20in%20feature%20ranking%20and%20classification%20tasks.%20The%20code%20is%20publicly%20available%20at%20https%3A//github.com/abaraka2020/Iterative-Feature-Exclusion-Ranking-Module%20and%20https%3A//github.com/mohalim/IFENet%0ALink%3A%20http%3A//arxiv.org/abs/2412.16442v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIterative%2520Feature%2520Exclusion%2520Ranking%2520for%2520Deep%2520Tabular%2520Learning%26entry.906535625%3DFathi%2520Said%2520Emhemed%2520Shaninah%2520and%2520AbdulRahman%2520M.%2520A.%2520Baraka%2520and%2520Mohd%2520Halim%2520Mohd%2520Noor%26entry.1292438233%3DTabular%2520data%2520is%2520a%2520common%2520format%2520for%2520storing%2520information%2520in%2520rows%2520and%2520columns%2520to%2520represent%2520data%2520entries%2520and%2520their%2520features.%2520Although%2520deep%2520neural%2520networks%2520have%2520become%2520the%2520main%2520approach%2520for%2520modeling%2520a%2520wide%2520range%2520of%2520domains%2520including%2520computer%2520vision%2520and%2520NLP%252C%2520many%2520of%2520them%2520are%2520not%2520well-suited%2520for%2520tabular%2520data.%2520Recently%252C%2520a%2520few%2520deep%2520learning%2520models%2520have%2520been%2520proposed%2520for%2520deep%2520tabular%2520learning%252C%2520featuring%2520an%2520internal%2520feature%2520selection%2520mechanism%2520with%2520end-to-end%2520gradient-based%2520optimization.%2520However%252C%2520their%2520feature%2520selection%2520mechanisms%2520are%2520unidimensional%252C%2520and%2520hence%2520fail%2520to%2520account%2520for%2520the%2520contextual%2520dependence%2520of%2520feature%2520importance%252C%2520potentially%2520overlooking%2520crucial%2520interactions%2520that%2520govern%2520complex%2520tasks.%2520In%2520addition%252C%2520they%2520overlook%2520the%2520bias%2520of%2520high-impact%2520features%2520and%2520the%2520risk%2520associated%2520with%2520the%2520limitations%2520of%2520attention%2520generalization.%2520To%2520address%2520this%2520limitation%252C%2520this%2520study%2520proposes%2520a%2520novel%2520iterative%2520feature%2520exclusion%2520module%2520that%2520enhances%2520the%2520feature%2520importance%2520ranking%2520in%2520tabular%2520data.%2520The%2520proposed%2520module%2520iteratively%2520excludes%2520each%2520feature%2520from%2520the%2520input%2520data%2520and%2520computes%2520the%2520attention%2520scores%252C%2520which%2520represent%2520the%2520impact%2520of%2520the%2520features%2520on%2520the%2520prediction.%2520By%2520aggregating%2520the%2520attention%2520scores%2520from%2520each%2520iteration%252C%2520the%2520proposed%2520module%2520generates%2520a%2520refined%2520representation%2520of%2520feature%2520importance%2520that%2520captures%2520both%2520global%2520and%2520local%2520interactions%2520between%2520features.%2520The%2520effectiveness%2520of%2520the%2520proposed%2520module%2520is%2520evaluated%2520on%2520four%2520public%2520datasets.%2520The%2520results%2520demonstrate%2520that%2520the%2520proposed%2520module%2520consistently%2520outperforms%2520state-of-the-art%2520methods%2520and%2520baseline%2520models%2520in%2520feature%2520ranking%2520and%2520classification%2520tasks.%2520The%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/abaraka2020/Iterative-Feature-Exclusion-Ranking-Module%2520and%2520https%253A//github.com/mohalim/IFENet%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16442v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Iterative%20Feature%20Exclusion%20Ranking%20for%20Deep%20Tabular%20Learning&entry.906535625=Fathi%20Said%20Emhemed%20Shaninah%20and%20AbdulRahman%20M.%20A.%20Baraka%20and%20Mohd%20Halim%20Mohd%20Noor&entry.1292438233=Tabular%20data%20is%20a%20common%20format%20for%20storing%20information%20in%20rows%20and%20columns%20to%20represent%20data%20entries%20and%20their%20features.%20Although%20deep%20neural%20networks%20have%20become%20the%20main%20approach%20for%20modeling%20a%20wide%20range%20of%20domains%20including%20computer%20vision%20and%20NLP%2C%20many%20of%20them%20are%20not%20well-suited%20for%20tabular%20data.%20Recently%2C%20a%20few%20deep%20learning%20models%20have%20been%20proposed%20for%20deep%20tabular%20learning%2C%20featuring%20an%20internal%20feature%20selection%20mechanism%20with%20end-to-end%20gradient-based%20optimization.%20However%2C%20their%20feature%20selection%20mechanisms%20are%20unidimensional%2C%20and%20hence%20fail%20to%20account%20for%20the%20contextual%20dependence%20of%20feature%20importance%2C%20potentially%20overlooking%20crucial%20interactions%20that%20govern%20complex%20tasks.%20In%20addition%2C%20they%20overlook%20the%20bias%20of%20high-impact%20features%20and%20the%20risk%20associated%20with%20the%20limitations%20of%20attention%20generalization.%20To%20address%20this%20limitation%2C%20this%20study%20proposes%20a%20novel%20iterative%20feature%20exclusion%20module%20that%20enhances%20the%20feature%20importance%20ranking%20in%20tabular%20data.%20The%20proposed%20module%20iteratively%20excludes%20each%20feature%20from%20the%20input%20data%20and%20computes%20the%20attention%20scores%2C%20which%20represent%20the%20impact%20of%20the%20features%20on%20the%20prediction.%20By%20aggregating%20the%20attention%20scores%20from%20each%20iteration%2C%20the%20proposed%20module%20generates%20a%20refined%20representation%20of%20feature%20importance%20that%20captures%20both%20global%20and%20local%20interactions%20between%20features.%20The%20effectiveness%20of%20the%20proposed%20module%20is%20evaluated%20on%20four%20public%20datasets.%20The%20results%20demonstrate%20that%20the%20proposed%20module%20consistently%20outperforms%20state-of-the-art%20methods%20and%20baseline%20models%20in%20feature%20ranking%20and%20classification%20tasks.%20The%20code%20is%20publicly%20available%20at%20https%3A//github.com/abaraka2020/Iterative-Feature-Exclusion-Ranking-Module%20and%20https%3A//github.com/mohalim/IFENet&entry.1838667208=http%3A//arxiv.org/abs/2412.16442v2&entry.124074799=Read"},
{"title": "Online Continual Graph Learning", "author": "Giovanni Donghi and Luca Pasa and Daniele Zambon and Cesare Alippi and Nicol\u00f2 Navarin", "abstract": "Continual Learning (CL) aims to incrementally acquire new knowledge while mitigating catastrophic forgetting. Within this setting, Online Continual Learning (OCL) focuses on updating models promptly and incrementally from single or small batches of observations from a data stream. Extending OCL to graph-structured data is crucial, as many real-world networks evolve over time and require timely, online predictions. However, existing continual or streaming graph learning methods typically assume access to entire graph snapshots or multiple passes over tasks, violating the efficiency constraints of the online setting. To address this gap, we introduce the Online Continual Graph Learning (OCGL) setting, which formalizes node-level continual learning on evolving graphs under strict memory and computational budgets. OCGL defines how a model incrementally processes a stream of node-level information while maintaining anytime inference and respecting resource constraints. We further establish a comprehensive benchmark comprising seven datasets and nine CL strategies, suitably adapted to the OCGL setting, enabling a standardized evaluation setup. Finally, we present a minimalistic yet competitive baseline for OCGL, inspired by our benchmarking results, that achieves strong empirical performance with high efficiency.", "link": "http://arxiv.org/abs/2508.03283v2", "date": "2025-12-18", "relevancy": 2.3801, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.497}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4776}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Continual%20Graph%20Learning&body=Title%3A%20Online%20Continual%20Graph%20Learning%0AAuthor%3A%20Giovanni%20Donghi%20and%20Luca%20Pasa%20and%20Daniele%20Zambon%20and%20Cesare%20Alippi%20and%20Nicol%C3%B2%20Navarin%0AAbstract%3A%20Continual%20Learning%20%28CL%29%20aims%20to%20incrementally%20acquire%20new%20knowledge%20while%20mitigating%20catastrophic%20forgetting.%20Within%20this%20setting%2C%20Online%20Continual%20Learning%20%28OCL%29%20focuses%20on%20updating%20models%20promptly%20and%20incrementally%20from%20single%20or%20small%20batches%20of%20observations%20from%20a%20data%20stream.%20Extending%20OCL%20to%20graph-structured%20data%20is%20crucial%2C%20as%20many%20real-world%20networks%20evolve%20over%20time%20and%20require%20timely%2C%20online%20predictions.%20However%2C%20existing%20continual%20or%20streaming%20graph%20learning%20methods%20typically%20assume%20access%20to%20entire%20graph%20snapshots%20or%20multiple%20passes%20over%20tasks%2C%20violating%20the%20efficiency%20constraints%20of%20the%20online%20setting.%20To%20address%20this%20gap%2C%20we%20introduce%20the%20Online%20Continual%20Graph%20Learning%20%28OCGL%29%20setting%2C%20which%20formalizes%20node-level%20continual%20learning%20on%20evolving%20graphs%20under%20strict%20memory%20and%20computational%20budgets.%20OCGL%20defines%20how%20a%20model%20incrementally%20processes%20a%20stream%20of%20node-level%20information%20while%20maintaining%20anytime%20inference%20and%20respecting%20resource%20constraints.%20We%20further%20establish%20a%20comprehensive%20benchmark%20comprising%20seven%20datasets%20and%20nine%20CL%20strategies%2C%20suitably%20adapted%20to%20the%20OCGL%20setting%2C%20enabling%20a%20standardized%20evaluation%20setup.%20Finally%2C%20we%20present%20a%20minimalistic%20yet%20competitive%20baseline%20for%20OCGL%2C%20inspired%20by%20our%20benchmarking%20results%2C%20that%20achieves%20strong%20empirical%20performance%20with%20high%20efficiency.%0ALink%3A%20http%3A//arxiv.org/abs/2508.03283v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Continual%2520Graph%2520Learning%26entry.906535625%3DGiovanni%2520Donghi%2520and%2520Luca%2520Pasa%2520and%2520Daniele%2520Zambon%2520and%2520Cesare%2520Alippi%2520and%2520Nicol%25C3%25B2%2520Navarin%26entry.1292438233%3DContinual%2520Learning%2520%2528CL%2529%2520aims%2520to%2520incrementally%2520acquire%2520new%2520knowledge%2520while%2520mitigating%2520catastrophic%2520forgetting.%2520Within%2520this%2520setting%252C%2520Online%2520Continual%2520Learning%2520%2528OCL%2529%2520focuses%2520on%2520updating%2520models%2520promptly%2520and%2520incrementally%2520from%2520single%2520or%2520small%2520batches%2520of%2520observations%2520from%2520a%2520data%2520stream.%2520Extending%2520OCL%2520to%2520graph-structured%2520data%2520is%2520crucial%252C%2520as%2520many%2520real-world%2520networks%2520evolve%2520over%2520time%2520and%2520require%2520timely%252C%2520online%2520predictions.%2520However%252C%2520existing%2520continual%2520or%2520streaming%2520graph%2520learning%2520methods%2520typically%2520assume%2520access%2520to%2520entire%2520graph%2520snapshots%2520or%2520multiple%2520passes%2520over%2520tasks%252C%2520violating%2520the%2520efficiency%2520constraints%2520of%2520the%2520online%2520setting.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520the%2520Online%2520Continual%2520Graph%2520Learning%2520%2528OCGL%2529%2520setting%252C%2520which%2520formalizes%2520node-level%2520continual%2520learning%2520on%2520evolving%2520graphs%2520under%2520strict%2520memory%2520and%2520computational%2520budgets.%2520OCGL%2520defines%2520how%2520a%2520model%2520incrementally%2520processes%2520a%2520stream%2520of%2520node-level%2520information%2520while%2520maintaining%2520anytime%2520inference%2520and%2520respecting%2520resource%2520constraints.%2520We%2520further%2520establish%2520a%2520comprehensive%2520benchmark%2520comprising%2520seven%2520datasets%2520and%2520nine%2520CL%2520strategies%252C%2520suitably%2520adapted%2520to%2520the%2520OCGL%2520setting%252C%2520enabling%2520a%2520standardized%2520evaluation%2520setup.%2520Finally%252C%2520we%2520present%2520a%2520minimalistic%2520yet%2520competitive%2520baseline%2520for%2520OCGL%252C%2520inspired%2520by%2520our%2520benchmarking%2520results%252C%2520that%2520achieves%2520strong%2520empirical%2520performance%2520with%2520high%2520efficiency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03283v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Continual%20Graph%20Learning&entry.906535625=Giovanni%20Donghi%20and%20Luca%20Pasa%20and%20Daniele%20Zambon%20and%20Cesare%20Alippi%20and%20Nicol%C3%B2%20Navarin&entry.1292438233=Continual%20Learning%20%28CL%29%20aims%20to%20incrementally%20acquire%20new%20knowledge%20while%20mitigating%20catastrophic%20forgetting.%20Within%20this%20setting%2C%20Online%20Continual%20Learning%20%28OCL%29%20focuses%20on%20updating%20models%20promptly%20and%20incrementally%20from%20single%20or%20small%20batches%20of%20observations%20from%20a%20data%20stream.%20Extending%20OCL%20to%20graph-structured%20data%20is%20crucial%2C%20as%20many%20real-world%20networks%20evolve%20over%20time%20and%20require%20timely%2C%20online%20predictions.%20However%2C%20existing%20continual%20or%20streaming%20graph%20learning%20methods%20typically%20assume%20access%20to%20entire%20graph%20snapshots%20or%20multiple%20passes%20over%20tasks%2C%20violating%20the%20efficiency%20constraints%20of%20the%20online%20setting.%20To%20address%20this%20gap%2C%20we%20introduce%20the%20Online%20Continual%20Graph%20Learning%20%28OCGL%29%20setting%2C%20which%20formalizes%20node-level%20continual%20learning%20on%20evolving%20graphs%20under%20strict%20memory%20and%20computational%20budgets.%20OCGL%20defines%20how%20a%20model%20incrementally%20processes%20a%20stream%20of%20node-level%20information%20while%20maintaining%20anytime%20inference%20and%20respecting%20resource%20constraints.%20We%20further%20establish%20a%20comprehensive%20benchmark%20comprising%20seven%20datasets%20and%20nine%20CL%20strategies%2C%20suitably%20adapted%20to%20the%20OCGL%20setting%2C%20enabling%20a%20standardized%20evaluation%20setup.%20Finally%2C%20we%20present%20a%20minimalistic%20yet%20competitive%20baseline%20for%20OCGL%2C%20inspired%20by%20our%20benchmarking%20results%2C%20that%20achieves%20strong%20empirical%20performance%20with%20high%20efficiency.&entry.1838667208=http%3A//arxiv.org/abs/2508.03283v2&entry.124074799=Read"},
{"title": "AuON: A Linear-time Alternative to Orthogonal Momentum Updates", "author": "Dipan Maity", "abstract": "Orthogonal momentum gradient updates have emerged to overcome the limitations of vector-based optimizers like Adam. The vector-based optimizer Adam suffers from high memory costs and ill-conditioned momentum gradient updates. However, traditional Orthogonal momentum approaches, such as SVD/QR decomposition, suffer from high computational and memory costs and underperform compared to well-tuned SGD with momentum. Recent advances, such as Muon, improve efficiency by applying momentum before orthogonalization and approximate orthogonal matrices via Newton-Schulz iterations, which gives better GPU utilization, active high TFLOPS, and reduces memory usage by up to 3x. Nevertheless, Muon(Vanilla) suffers from exploding attention logits and has cubic computation complexity. In this paper, we deep dive into orthogonal momentum gradient updates to find the main properties that help Muon achieve remarkable performance. We propose AuON (Alternative Unit-norm momentum updates by Normalized nonlinear scaling), a linear-time optimizer that achieves strong performance without approximate orthogonal matrices, while preserving structural alignment and reconditioning ill-posed updates. AuON has an automatic \"emergency brake\" to handle exploding attention logits. We further introduce a hybrid variant, Hybrid-AuON, that applies the linear transformations with Newton-Schulz iterations, which outperforms Muon in the language modeling tasks. Code is available at: https://github.com/ryyzn9/AuON", "link": "http://arxiv.org/abs/2509.24320v4", "date": "2025-12-18", "relevancy": 2.3766, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4815}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4801}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AuON%3A%20A%20Linear-time%20Alternative%20to%20Orthogonal%20Momentum%20Updates&body=Title%3A%20AuON%3A%20A%20Linear-time%20Alternative%20to%20Orthogonal%20Momentum%20Updates%0AAuthor%3A%20Dipan%20Maity%0AAbstract%3A%20Orthogonal%20momentum%20gradient%20updates%20have%20emerged%20to%20overcome%20the%20limitations%20of%20vector-based%20optimizers%20like%20Adam.%20The%20vector-based%20optimizer%20Adam%20suffers%20from%20high%20memory%20costs%20and%20ill-conditioned%20momentum%20gradient%20updates.%20However%2C%20traditional%20Orthogonal%20momentum%20approaches%2C%20such%20as%20SVD/QR%20decomposition%2C%20suffer%20from%20high%20computational%20and%20memory%20costs%20and%20underperform%20compared%20to%20well-tuned%20SGD%20with%20momentum.%20Recent%20advances%2C%20such%20as%20Muon%2C%20improve%20efficiency%20by%20applying%20momentum%20before%20orthogonalization%20and%20approximate%20orthogonal%20matrices%20via%20Newton-Schulz%20iterations%2C%20which%20gives%20better%20GPU%20utilization%2C%20active%20high%20TFLOPS%2C%20and%20reduces%20memory%20usage%20by%20up%20to%203x.%20Nevertheless%2C%20Muon%28Vanilla%29%20suffers%20from%20exploding%20attention%20logits%20and%20has%20cubic%20computation%20complexity.%20In%20this%20paper%2C%20we%20deep%20dive%20into%20orthogonal%20momentum%20gradient%20updates%20to%20find%20the%20main%20properties%20that%20help%20Muon%20achieve%20remarkable%20performance.%20We%20propose%20AuON%20%28Alternative%20Unit-norm%20momentum%20updates%20by%20Normalized%20nonlinear%20scaling%29%2C%20a%20linear-time%20optimizer%20that%20achieves%20strong%20performance%20without%20approximate%20orthogonal%20matrices%2C%20while%20preserving%20structural%20alignment%20and%20reconditioning%20ill-posed%20updates.%20AuON%20has%20an%20automatic%20%22emergency%20brake%22%20to%20handle%20exploding%20attention%20logits.%20We%20further%20introduce%20a%20hybrid%20variant%2C%20Hybrid-AuON%2C%20that%20applies%20the%20linear%20transformations%20with%20Newton-Schulz%20iterations%2C%20which%20outperforms%20Muon%20in%20the%20language%20modeling%20tasks.%20Code%20is%20available%20at%3A%20https%3A//github.com/ryyzn9/AuON%0ALink%3A%20http%3A//arxiv.org/abs/2509.24320v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuON%253A%2520A%2520Linear-time%2520Alternative%2520to%2520Orthogonal%2520Momentum%2520Updates%26entry.906535625%3DDipan%2520Maity%26entry.1292438233%3DOrthogonal%2520momentum%2520gradient%2520updates%2520have%2520emerged%2520to%2520overcome%2520the%2520limitations%2520of%2520vector-based%2520optimizers%2520like%2520Adam.%2520The%2520vector-based%2520optimizer%2520Adam%2520suffers%2520from%2520high%2520memory%2520costs%2520and%2520ill-conditioned%2520momentum%2520gradient%2520updates.%2520However%252C%2520traditional%2520Orthogonal%2520momentum%2520approaches%252C%2520such%2520as%2520SVD/QR%2520decomposition%252C%2520suffer%2520from%2520high%2520computational%2520and%2520memory%2520costs%2520and%2520underperform%2520compared%2520to%2520well-tuned%2520SGD%2520with%2520momentum.%2520Recent%2520advances%252C%2520such%2520as%2520Muon%252C%2520improve%2520efficiency%2520by%2520applying%2520momentum%2520before%2520orthogonalization%2520and%2520approximate%2520orthogonal%2520matrices%2520via%2520Newton-Schulz%2520iterations%252C%2520which%2520gives%2520better%2520GPU%2520utilization%252C%2520active%2520high%2520TFLOPS%252C%2520and%2520reduces%2520memory%2520usage%2520by%2520up%2520to%25203x.%2520Nevertheless%252C%2520Muon%2528Vanilla%2529%2520suffers%2520from%2520exploding%2520attention%2520logits%2520and%2520has%2520cubic%2520computation%2520complexity.%2520In%2520this%2520paper%252C%2520we%2520deep%2520dive%2520into%2520orthogonal%2520momentum%2520gradient%2520updates%2520to%2520find%2520the%2520main%2520properties%2520that%2520help%2520Muon%2520achieve%2520remarkable%2520performance.%2520We%2520propose%2520AuON%2520%2528Alternative%2520Unit-norm%2520momentum%2520updates%2520by%2520Normalized%2520nonlinear%2520scaling%2529%252C%2520a%2520linear-time%2520optimizer%2520that%2520achieves%2520strong%2520performance%2520without%2520approximate%2520orthogonal%2520matrices%252C%2520while%2520preserving%2520structural%2520alignment%2520and%2520reconditioning%2520ill-posed%2520updates.%2520AuON%2520has%2520an%2520automatic%2520%2522emergency%2520brake%2522%2520to%2520handle%2520exploding%2520attention%2520logits.%2520We%2520further%2520introduce%2520a%2520hybrid%2520variant%252C%2520Hybrid-AuON%252C%2520that%2520applies%2520the%2520linear%2520transformations%2520with%2520Newton-Schulz%2520iterations%252C%2520which%2520outperforms%2520Muon%2520in%2520the%2520language%2520modeling%2520tasks.%2520Code%2520is%2520available%2520at%253A%2520https%253A//github.com/ryyzn9/AuON%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24320v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AuON%3A%20A%20Linear-time%20Alternative%20to%20Orthogonal%20Momentum%20Updates&entry.906535625=Dipan%20Maity&entry.1292438233=Orthogonal%20momentum%20gradient%20updates%20have%20emerged%20to%20overcome%20the%20limitations%20of%20vector-based%20optimizers%20like%20Adam.%20The%20vector-based%20optimizer%20Adam%20suffers%20from%20high%20memory%20costs%20and%20ill-conditioned%20momentum%20gradient%20updates.%20However%2C%20traditional%20Orthogonal%20momentum%20approaches%2C%20such%20as%20SVD/QR%20decomposition%2C%20suffer%20from%20high%20computational%20and%20memory%20costs%20and%20underperform%20compared%20to%20well-tuned%20SGD%20with%20momentum.%20Recent%20advances%2C%20such%20as%20Muon%2C%20improve%20efficiency%20by%20applying%20momentum%20before%20orthogonalization%20and%20approximate%20orthogonal%20matrices%20via%20Newton-Schulz%20iterations%2C%20which%20gives%20better%20GPU%20utilization%2C%20active%20high%20TFLOPS%2C%20and%20reduces%20memory%20usage%20by%20up%20to%203x.%20Nevertheless%2C%20Muon%28Vanilla%29%20suffers%20from%20exploding%20attention%20logits%20and%20has%20cubic%20computation%20complexity.%20In%20this%20paper%2C%20we%20deep%20dive%20into%20orthogonal%20momentum%20gradient%20updates%20to%20find%20the%20main%20properties%20that%20help%20Muon%20achieve%20remarkable%20performance.%20We%20propose%20AuON%20%28Alternative%20Unit-norm%20momentum%20updates%20by%20Normalized%20nonlinear%20scaling%29%2C%20a%20linear-time%20optimizer%20that%20achieves%20strong%20performance%20without%20approximate%20orthogonal%20matrices%2C%20while%20preserving%20structural%20alignment%20and%20reconditioning%20ill-posed%20updates.%20AuON%20has%20an%20automatic%20%22emergency%20brake%22%20to%20handle%20exploding%20attention%20logits.%20We%20further%20introduce%20a%20hybrid%20variant%2C%20Hybrid-AuON%2C%20that%20applies%20the%20linear%20transformations%20with%20Newton-Schulz%20iterations%2C%20which%20outperforms%20Muon%20in%20the%20language%20modeling%20tasks.%20Code%20is%20available%20at%3A%20https%3A//github.com/ryyzn9/AuON&entry.1838667208=http%3A//arxiv.org/abs/2509.24320v4&entry.124074799=Read"},
{"title": "Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos", "author": "Mingfei Chen and Yifan Wang and Zhengqin Li and Homanga Bharadhwaj and Yujin Chen and Chuan Qin and Ziyi Kou and Yuan Tian and Eric Whitmire and Rajinder Sodhi and Hrvoje Benko and Eli Shlizerman and Yue Liu", "abstract": "Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.", "link": "http://arxiv.org/abs/2512.16907v1", "date": "2025-12-18", "relevancy": 2.3702, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6127}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5849}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flowing%20from%20Reasoning%20to%20Motion%3A%20Learning%203D%20Hand%20Trajectory%20Prediction%20from%20Egocentric%20Human%20Interaction%20Videos&body=Title%3A%20Flowing%20from%20Reasoning%20to%20Motion%3A%20Learning%203D%20Hand%20Trajectory%20Prediction%20from%20Egocentric%20Human%20Interaction%20Videos%0AAuthor%3A%20Mingfei%20Chen%20and%20Yifan%20Wang%20and%20Zhengqin%20Li%20and%20Homanga%20Bharadhwaj%20and%20Yujin%20Chen%20and%20Chuan%20Qin%20and%20Ziyi%20Kou%20and%20Yuan%20Tian%20and%20Eric%20Whitmire%20and%20Rajinder%20Sodhi%20and%20Hrvoje%20Benko%20and%20Eli%20Shlizerman%20and%20Yue%20Liu%0AAbstract%3A%20Prior%20works%20on%203D%20hand%20trajectory%20prediction%20are%20constrained%20by%20datasets%20that%20decouple%20motion%20from%20semantic%20supervision%20and%20by%20models%20that%20weakly%20link%20reasoning%20and%20action.%20To%20address%20these%2C%20we%20first%20present%20the%20EgoMAN%20dataset%2C%20a%20large-scale%20egocentric%20dataset%20for%20interaction%20stage-aware%203D%20hand%20trajectory%20prediction%20with%20219K%206DoF%20trajectories%20and%203M%20structured%20QA%20pairs%20for%20semantic%2C%20spatial%2C%20and%20motion%20reasoning.%20We%20then%20introduce%20the%20EgoMAN%20model%2C%20a%20reasoning-to-motion%20framework%20that%20links%20vision-language%20reasoning%20and%20motion%20generation%20via%20a%20trajectory-token%20interface.%20Trained%20progressively%20to%20align%20reasoning%20with%20motion%20dynamics%2C%20our%20approach%20yields%20accurate%20and%20stage-aware%20trajectories%20with%20generalization%20across%20real-world%20scenes.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16907v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlowing%2520from%2520Reasoning%2520to%2520Motion%253A%2520Learning%25203D%2520Hand%2520Trajectory%2520Prediction%2520from%2520Egocentric%2520Human%2520Interaction%2520Videos%26entry.906535625%3DMingfei%2520Chen%2520and%2520Yifan%2520Wang%2520and%2520Zhengqin%2520Li%2520and%2520Homanga%2520Bharadhwaj%2520and%2520Yujin%2520Chen%2520and%2520Chuan%2520Qin%2520and%2520Ziyi%2520Kou%2520and%2520Yuan%2520Tian%2520and%2520Eric%2520Whitmire%2520and%2520Rajinder%2520Sodhi%2520and%2520Hrvoje%2520Benko%2520and%2520Eli%2520Shlizerman%2520and%2520Yue%2520Liu%26entry.1292438233%3DPrior%2520works%2520on%25203D%2520hand%2520trajectory%2520prediction%2520are%2520constrained%2520by%2520datasets%2520that%2520decouple%2520motion%2520from%2520semantic%2520supervision%2520and%2520by%2520models%2520that%2520weakly%2520link%2520reasoning%2520and%2520action.%2520To%2520address%2520these%252C%2520we%2520first%2520present%2520the%2520EgoMAN%2520dataset%252C%2520a%2520large-scale%2520egocentric%2520dataset%2520for%2520interaction%2520stage-aware%25203D%2520hand%2520trajectory%2520prediction%2520with%2520219K%25206DoF%2520trajectories%2520and%25203M%2520structured%2520QA%2520pairs%2520for%2520semantic%252C%2520spatial%252C%2520and%2520motion%2520reasoning.%2520We%2520then%2520introduce%2520the%2520EgoMAN%2520model%252C%2520a%2520reasoning-to-motion%2520framework%2520that%2520links%2520vision-language%2520reasoning%2520and%2520motion%2520generation%2520via%2520a%2520trajectory-token%2520interface.%2520Trained%2520progressively%2520to%2520align%2520reasoning%2520with%2520motion%2520dynamics%252C%2520our%2520approach%2520yields%2520accurate%2520and%2520stage-aware%2520trajectories%2520with%2520generalization%2520across%2520real-world%2520scenes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16907v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flowing%20from%20Reasoning%20to%20Motion%3A%20Learning%203D%20Hand%20Trajectory%20Prediction%20from%20Egocentric%20Human%20Interaction%20Videos&entry.906535625=Mingfei%20Chen%20and%20Yifan%20Wang%20and%20Zhengqin%20Li%20and%20Homanga%20Bharadhwaj%20and%20Yujin%20Chen%20and%20Chuan%20Qin%20and%20Ziyi%20Kou%20and%20Yuan%20Tian%20and%20Eric%20Whitmire%20and%20Rajinder%20Sodhi%20and%20Hrvoje%20Benko%20and%20Eli%20Shlizerman%20and%20Yue%20Liu&entry.1292438233=Prior%20works%20on%203D%20hand%20trajectory%20prediction%20are%20constrained%20by%20datasets%20that%20decouple%20motion%20from%20semantic%20supervision%20and%20by%20models%20that%20weakly%20link%20reasoning%20and%20action.%20To%20address%20these%2C%20we%20first%20present%20the%20EgoMAN%20dataset%2C%20a%20large-scale%20egocentric%20dataset%20for%20interaction%20stage-aware%203D%20hand%20trajectory%20prediction%20with%20219K%206DoF%20trajectories%20and%203M%20structured%20QA%20pairs%20for%20semantic%2C%20spatial%2C%20and%20motion%20reasoning.%20We%20then%20introduce%20the%20EgoMAN%20model%2C%20a%20reasoning-to-motion%20framework%20that%20links%20vision-language%20reasoning%20and%20motion%20generation%20via%20a%20trajectory-token%20interface.%20Trained%20progressively%20to%20align%20reasoning%20with%20motion%20dynamics%2C%20our%20approach%20yields%20accurate%20and%20stage-aware%20trajectories%20with%20generalization%20across%20real-world%20scenes.&entry.1838667208=http%3A//arxiv.org/abs/2512.16907v1&entry.124074799=Read"},
{"title": "VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks", "author": "Beitong Zhou and Zhexiao Huang and Yuan Guo and Zhangxuan Gu and Tianyu Xia and Zichen Luo and Fei Tang and Dehan Kong and Yanyi Shang and Suling Ou and Zhenlin Guo and Changhua Meng and Shuheng Shen", "abstract": "GUI grounding is a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a single platform and require highly specialized domain knowledge. In this work, we present VenusBench-GD, a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, enabling hierarchical evaluation for real-word applications. VenusBench-GD contributes as follows: (i) we introduce a large-scale, cross-platform benchmark with extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish a high-quality data construction pipeline for grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing a hierarchical task taxonomy that divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purpose multimodal models now match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favor GUI-specialized models, though they exhibit significant overfitting and poor robustness. These results underscore the necessity of comprehensive, multi-tiered evaluation frameworks.", "link": "http://arxiv.org/abs/2512.16501v1", "date": "2025-12-18", "relevancy": 2.3651, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.505}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.457}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VenusBench-GD%3A%20A%20Comprehensive%20Multi-Platform%20GUI%20Benchmark%20for%20Diverse%20Grounding%20Tasks&body=Title%3A%20VenusBench-GD%3A%20A%20Comprehensive%20Multi-Platform%20GUI%20Benchmark%20for%20Diverse%20Grounding%20Tasks%0AAuthor%3A%20Beitong%20Zhou%20and%20Zhexiao%20Huang%20and%20Yuan%20Guo%20and%20Zhangxuan%20Gu%20and%20Tianyu%20Xia%20and%20Zichen%20Luo%20and%20Fei%20Tang%20and%20Dehan%20Kong%20and%20Yanyi%20Shang%20and%20Suling%20Ou%20and%20Zhenlin%20Guo%20and%20Changhua%20Meng%20and%20Shuheng%20Shen%0AAbstract%3A%20GUI%20grounding%20is%20a%20critical%20component%20in%20building%20capable%20GUI%20agents.%20However%2C%20existing%20grounding%20benchmarks%20suffer%20from%20significant%20limitations%3A%20they%20either%20provide%20insufficient%20data%20volume%20and%20narrow%20domain%20coverage%2C%20or%20focus%20excessively%20on%20a%20single%20platform%20and%20require%20highly%20specialized%20domain%20knowledge.%20In%20this%20work%2C%20we%20present%20VenusBench-GD%2C%20a%20comprehensive%2C%20bilingual%20benchmark%20for%20GUI%20grounding%20that%20spans%20multiple%20platforms%2C%20enabling%20hierarchical%20evaluation%20for%20real-word%20applications.%20VenusBench-GD%20contributes%20as%20follows%3A%20%28i%29%20we%20introduce%20a%20large-scale%2C%20cross-platform%20benchmark%20with%20extensive%20coverage%20of%20applications%2C%20diverse%20UI%20elements%2C%20and%20rich%20annotated%20data%2C%20%28ii%29%20we%20establish%20a%20high-quality%20data%20construction%20pipeline%20for%20grounding%20tasks%2C%20achieving%20higher%20annotation%20accuracy%20than%20existing%20benchmarks%2C%20and%20%28iii%29%20we%20extend%20the%20scope%20of%20element%20grounding%20by%20proposing%20a%20hierarchical%20task%20taxonomy%20that%20divides%20grounding%20into%20basic%20and%20advanced%20categories%2C%20encompassing%20six%20distinct%20subtasks%20designed%20to%20evaluate%20models%20from%20complementary%20perspectives.%20Our%20experimental%20findings%20reveal%20critical%20insights%3A%20general-purpose%20multimodal%20models%20now%20match%20or%20even%20surpass%20specialized%20GUI%20models%20on%20basic%20grounding%20tasks.%20In%20contrast%2C%20advanced%20tasks%2C%20still%20favor%20GUI-specialized%20models%2C%20though%20they%20exhibit%20significant%20overfitting%20and%20poor%20robustness.%20These%20results%20underscore%20the%20necessity%20of%20comprehensive%2C%20multi-tiered%20evaluation%20frameworks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16501v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVenusBench-GD%253A%2520A%2520Comprehensive%2520Multi-Platform%2520GUI%2520Benchmark%2520for%2520Diverse%2520Grounding%2520Tasks%26entry.906535625%3DBeitong%2520Zhou%2520and%2520Zhexiao%2520Huang%2520and%2520Yuan%2520Guo%2520and%2520Zhangxuan%2520Gu%2520and%2520Tianyu%2520Xia%2520and%2520Zichen%2520Luo%2520and%2520Fei%2520Tang%2520and%2520Dehan%2520Kong%2520and%2520Yanyi%2520Shang%2520and%2520Suling%2520Ou%2520and%2520Zhenlin%2520Guo%2520and%2520Changhua%2520Meng%2520and%2520Shuheng%2520Shen%26entry.1292438233%3DGUI%2520grounding%2520is%2520a%2520critical%2520component%2520in%2520building%2520capable%2520GUI%2520agents.%2520However%252C%2520existing%2520grounding%2520benchmarks%2520suffer%2520from%2520significant%2520limitations%253A%2520they%2520either%2520provide%2520insufficient%2520data%2520volume%2520and%2520narrow%2520domain%2520coverage%252C%2520or%2520focus%2520excessively%2520on%2520a%2520single%2520platform%2520and%2520require%2520highly%2520specialized%2520domain%2520knowledge.%2520In%2520this%2520work%252C%2520we%2520present%2520VenusBench-GD%252C%2520a%2520comprehensive%252C%2520bilingual%2520benchmark%2520for%2520GUI%2520grounding%2520that%2520spans%2520multiple%2520platforms%252C%2520enabling%2520hierarchical%2520evaluation%2520for%2520real-word%2520applications.%2520VenusBench-GD%2520contributes%2520as%2520follows%253A%2520%2528i%2529%2520we%2520introduce%2520a%2520large-scale%252C%2520cross-platform%2520benchmark%2520with%2520extensive%2520coverage%2520of%2520applications%252C%2520diverse%2520UI%2520elements%252C%2520and%2520rich%2520annotated%2520data%252C%2520%2528ii%2529%2520we%2520establish%2520a%2520high-quality%2520data%2520construction%2520pipeline%2520for%2520grounding%2520tasks%252C%2520achieving%2520higher%2520annotation%2520accuracy%2520than%2520existing%2520benchmarks%252C%2520and%2520%2528iii%2529%2520we%2520extend%2520the%2520scope%2520of%2520element%2520grounding%2520by%2520proposing%2520a%2520hierarchical%2520task%2520taxonomy%2520that%2520divides%2520grounding%2520into%2520basic%2520and%2520advanced%2520categories%252C%2520encompassing%2520six%2520distinct%2520subtasks%2520designed%2520to%2520evaluate%2520models%2520from%2520complementary%2520perspectives.%2520Our%2520experimental%2520findings%2520reveal%2520critical%2520insights%253A%2520general-purpose%2520multimodal%2520models%2520now%2520match%2520or%2520even%2520surpass%2520specialized%2520GUI%2520models%2520on%2520basic%2520grounding%2520tasks.%2520In%2520contrast%252C%2520advanced%2520tasks%252C%2520still%2520favor%2520GUI-specialized%2520models%252C%2520though%2520they%2520exhibit%2520significant%2520overfitting%2520and%2520poor%2520robustness.%2520These%2520results%2520underscore%2520the%2520necessity%2520of%2520comprehensive%252C%2520multi-tiered%2520evaluation%2520frameworks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16501v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VenusBench-GD%3A%20A%20Comprehensive%20Multi-Platform%20GUI%20Benchmark%20for%20Diverse%20Grounding%20Tasks&entry.906535625=Beitong%20Zhou%20and%20Zhexiao%20Huang%20and%20Yuan%20Guo%20and%20Zhangxuan%20Gu%20and%20Tianyu%20Xia%20and%20Zichen%20Luo%20and%20Fei%20Tang%20and%20Dehan%20Kong%20and%20Yanyi%20Shang%20and%20Suling%20Ou%20and%20Zhenlin%20Guo%20and%20Changhua%20Meng%20and%20Shuheng%20Shen&entry.1292438233=GUI%20grounding%20is%20a%20critical%20component%20in%20building%20capable%20GUI%20agents.%20However%2C%20existing%20grounding%20benchmarks%20suffer%20from%20significant%20limitations%3A%20they%20either%20provide%20insufficient%20data%20volume%20and%20narrow%20domain%20coverage%2C%20or%20focus%20excessively%20on%20a%20single%20platform%20and%20require%20highly%20specialized%20domain%20knowledge.%20In%20this%20work%2C%20we%20present%20VenusBench-GD%2C%20a%20comprehensive%2C%20bilingual%20benchmark%20for%20GUI%20grounding%20that%20spans%20multiple%20platforms%2C%20enabling%20hierarchical%20evaluation%20for%20real-word%20applications.%20VenusBench-GD%20contributes%20as%20follows%3A%20%28i%29%20we%20introduce%20a%20large-scale%2C%20cross-platform%20benchmark%20with%20extensive%20coverage%20of%20applications%2C%20diverse%20UI%20elements%2C%20and%20rich%20annotated%20data%2C%20%28ii%29%20we%20establish%20a%20high-quality%20data%20construction%20pipeline%20for%20grounding%20tasks%2C%20achieving%20higher%20annotation%20accuracy%20than%20existing%20benchmarks%2C%20and%20%28iii%29%20we%20extend%20the%20scope%20of%20element%20grounding%20by%20proposing%20a%20hierarchical%20task%20taxonomy%20that%20divides%20grounding%20into%20basic%20and%20advanced%20categories%2C%20encompassing%20six%20distinct%20subtasks%20designed%20to%20evaluate%20models%20from%20complementary%20perspectives.%20Our%20experimental%20findings%20reveal%20critical%20insights%3A%20general-purpose%20multimodal%20models%20now%20match%20or%20even%20surpass%20specialized%20GUI%20models%20on%20basic%20grounding%20tasks.%20In%20contrast%2C%20advanced%20tasks%2C%20still%20favor%20GUI-specialized%20models%2C%20though%20they%20exhibit%20significant%20overfitting%20and%20poor%20robustness.%20These%20results%20underscore%20the%20necessity%20of%20comprehensive%2C%20multi-tiered%20evaluation%20frameworks.&entry.1838667208=http%3A//arxiv.org/abs/2512.16501v1&entry.124074799=Read"},
{"title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence", "author": "Xiaopeng Lin and Shijie Lian and Bin Yu and Ruoqi Yang and Changti Wu and Yuzhuo Miao and Yurun Jin and Yukun Shi and Cong Huang and Bojun Cheng and Kai Chen", "abstract": "Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.", "link": "http://arxiv.org/abs/2512.16793v1", "date": "2025-12-18", "relevancy": 2.36, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6021}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5995}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhysBrain%3A%20Human%20Egocentric%20Data%20as%20a%20Bridge%20from%20Vision%20Language%20Models%20to%20Physical%20Intelligence&body=Title%3A%20PhysBrain%3A%20Human%20Egocentric%20Data%20as%20a%20Bridge%20from%20Vision%20Language%20Models%20to%20Physical%20Intelligence%0AAuthor%3A%20Xiaopeng%20Lin%20and%20Shijie%20Lian%20and%20Bin%20Yu%20and%20Ruoqi%20Yang%20and%20Changti%20Wu%20and%20Yuzhuo%20Miao%20and%20Yurun%20Jin%20and%20Yukun%20Shi%20and%20Cong%20Huang%20and%20Bojun%20Cheng%20and%20Kai%20Chen%0AAbstract%3A%20Robotic%20generalization%20relies%20on%20physical%20intelligence%3A%20the%20ability%20to%20reason%20about%20state%20changes%2C%20contact-rich%20interactions%2C%20and%20long-horizon%20planning%20under%20egocentric%20perception%20and%20action.%20However%2C%20most%20VLMs%20are%20trained%20primarily%20on%20third-person%20data%2C%20creating%20a%20fundamental%20viewpoint%20mismatch%20for%20humanoid%20robots.%20Scaling%20robot%20egocentric%20data%20collection%20remains%20impractical%20due%20to%20high%20cost%20and%20limited%20diversity%2C%20whereas%20large-scale%20human%20egocentric%20videos%20offer%20a%20scalable%20alternative%20that%20naturally%20capture%20rich%20interaction%20context%20and%20causal%20structure.%20The%20key%20challenge%20is%20to%20convert%20raw%20egocentric%20videos%20into%20structured%20and%20reliable%20embodiment%20training%20supervision.%20Accordingly%2C%20we%20propose%20an%20Egocentric2Embodiment%20translation%20pipeline%20that%20transforms%20first-person%20videos%20into%20multi-level%2C%20schema-driven%20VQA%20supervision%20with%20enforced%20evidence%20grounding%20and%20temporal%20consistency%2C%20enabling%20the%20construction%20of%20the%20Egocentric2Embodiment%20dataset%20%28E2E-3M%29%20at%20scale.%20An%20egocentric-aware%20embodied%20brain%2C%20termed%20PhysBrain%2C%20is%20obtained%20by%20training%20on%20the%20E2E-3M%20dataset.%20PhysBrain%20exhibits%20substantially%20improved%20egocentric%20understanding%2C%20particularly%20for%20planning%20on%20EgoThink.%20It%20provides%20an%20egocentric-aware%20initialization%20that%20enables%20more%20sample-efficient%20VLA%20fine-tuning%20and%20higher%20SimplerEnv%20success%20rates%20%2853.9%5C%25%29%2C%20demonstrating%20effective%20transfer%20from%20human%20egocentric%20supervision%20to%20downstream%20robot%20control.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16793v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysBrain%253A%2520Human%2520Egocentric%2520Data%2520as%2520a%2520Bridge%2520from%2520Vision%2520Language%2520Models%2520to%2520Physical%2520Intelligence%26entry.906535625%3DXiaopeng%2520Lin%2520and%2520Shijie%2520Lian%2520and%2520Bin%2520Yu%2520and%2520Ruoqi%2520Yang%2520and%2520Changti%2520Wu%2520and%2520Yuzhuo%2520Miao%2520and%2520Yurun%2520Jin%2520and%2520Yukun%2520Shi%2520and%2520Cong%2520Huang%2520and%2520Bojun%2520Cheng%2520and%2520Kai%2520Chen%26entry.1292438233%3DRobotic%2520generalization%2520relies%2520on%2520physical%2520intelligence%253A%2520the%2520ability%2520to%2520reason%2520about%2520state%2520changes%252C%2520contact-rich%2520interactions%252C%2520and%2520long-horizon%2520planning%2520under%2520egocentric%2520perception%2520and%2520action.%2520However%252C%2520most%2520VLMs%2520are%2520trained%2520primarily%2520on%2520third-person%2520data%252C%2520creating%2520a%2520fundamental%2520viewpoint%2520mismatch%2520for%2520humanoid%2520robots.%2520Scaling%2520robot%2520egocentric%2520data%2520collection%2520remains%2520impractical%2520due%2520to%2520high%2520cost%2520and%2520limited%2520diversity%252C%2520whereas%2520large-scale%2520human%2520egocentric%2520videos%2520offer%2520a%2520scalable%2520alternative%2520that%2520naturally%2520capture%2520rich%2520interaction%2520context%2520and%2520causal%2520structure.%2520The%2520key%2520challenge%2520is%2520to%2520convert%2520raw%2520egocentric%2520videos%2520into%2520structured%2520and%2520reliable%2520embodiment%2520training%2520supervision.%2520Accordingly%252C%2520we%2520propose%2520an%2520Egocentric2Embodiment%2520translation%2520pipeline%2520that%2520transforms%2520first-person%2520videos%2520into%2520multi-level%252C%2520schema-driven%2520VQA%2520supervision%2520with%2520enforced%2520evidence%2520grounding%2520and%2520temporal%2520consistency%252C%2520enabling%2520the%2520construction%2520of%2520the%2520Egocentric2Embodiment%2520dataset%2520%2528E2E-3M%2529%2520at%2520scale.%2520An%2520egocentric-aware%2520embodied%2520brain%252C%2520termed%2520PhysBrain%252C%2520is%2520obtained%2520by%2520training%2520on%2520the%2520E2E-3M%2520dataset.%2520PhysBrain%2520exhibits%2520substantially%2520improved%2520egocentric%2520understanding%252C%2520particularly%2520for%2520planning%2520on%2520EgoThink.%2520It%2520provides%2520an%2520egocentric-aware%2520initialization%2520that%2520enables%2520more%2520sample-efficient%2520VLA%2520fine-tuning%2520and%2520higher%2520SimplerEnv%2520success%2520rates%2520%252853.9%255C%2525%2529%252C%2520demonstrating%2520effective%2520transfer%2520from%2520human%2520egocentric%2520supervision%2520to%2520downstream%2520robot%2520control.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16793v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhysBrain%3A%20Human%20Egocentric%20Data%20as%20a%20Bridge%20from%20Vision%20Language%20Models%20to%20Physical%20Intelligence&entry.906535625=Xiaopeng%20Lin%20and%20Shijie%20Lian%20and%20Bin%20Yu%20and%20Ruoqi%20Yang%20and%20Changti%20Wu%20and%20Yuzhuo%20Miao%20and%20Yurun%20Jin%20and%20Yukun%20Shi%20and%20Cong%20Huang%20and%20Bojun%20Cheng%20and%20Kai%20Chen&entry.1292438233=Robotic%20generalization%20relies%20on%20physical%20intelligence%3A%20the%20ability%20to%20reason%20about%20state%20changes%2C%20contact-rich%20interactions%2C%20and%20long-horizon%20planning%20under%20egocentric%20perception%20and%20action.%20However%2C%20most%20VLMs%20are%20trained%20primarily%20on%20third-person%20data%2C%20creating%20a%20fundamental%20viewpoint%20mismatch%20for%20humanoid%20robots.%20Scaling%20robot%20egocentric%20data%20collection%20remains%20impractical%20due%20to%20high%20cost%20and%20limited%20diversity%2C%20whereas%20large-scale%20human%20egocentric%20videos%20offer%20a%20scalable%20alternative%20that%20naturally%20capture%20rich%20interaction%20context%20and%20causal%20structure.%20The%20key%20challenge%20is%20to%20convert%20raw%20egocentric%20videos%20into%20structured%20and%20reliable%20embodiment%20training%20supervision.%20Accordingly%2C%20we%20propose%20an%20Egocentric2Embodiment%20translation%20pipeline%20that%20transforms%20first-person%20videos%20into%20multi-level%2C%20schema-driven%20VQA%20supervision%20with%20enforced%20evidence%20grounding%20and%20temporal%20consistency%2C%20enabling%20the%20construction%20of%20the%20Egocentric2Embodiment%20dataset%20%28E2E-3M%29%20at%20scale.%20An%20egocentric-aware%20embodied%20brain%2C%20termed%20PhysBrain%2C%20is%20obtained%20by%20training%20on%20the%20E2E-3M%20dataset.%20PhysBrain%20exhibits%20substantially%20improved%20egocentric%20understanding%2C%20particularly%20for%20planning%20on%20EgoThink.%20It%20provides%20an%20egocentric-aware%20initialization%20that%20enables%20more%20sample-efficient%20VLA%20fine-tuning%20and%20higher%20SimplerEnv%20success%20rates%20%2853.9%5C%25%29%2C%20demonstrating%20effective%20transfer%20from%20human%20egocentric%20supervision%20to%20downstream%20robot%20control.&entry.1838667208=http%3A//arxiv.org/abs/2512.16793v1&entry.124074799=Read"},
{"title": "VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization", "author": "Xiaoyan Cong and Haotian Yang and Angtian Wang and Yizhi Wang and Yiding Yang and Canyu Zhang and Chongyang Ma", "abstract": "Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io", "link": "http://arxiv.org/abs/2512.16906v1", "date": "2025-12-18", "relevancy": 2.3579, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6252}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6137}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIVA%3A%20VLM-Guided%20Instruction-Based%20Video%20Editing%20with%20Reward%20Optimization&body=Title%3A%20VIVA%3A%20VLM-Guided%20Instruction-Based%20Video%20Editing%20with%20Reward%20Optimization%0AAuthor%3A%20Xiaoyan%20Cong%20and%20Haotian%20Yang%20and%20Angtian%20Wang%20and%20Yizhi%20Wang%20and%20Yiding%20Yang%20and%20Canyu%20Zhang%20and%20Chongyang%20Ma%0AAbstract%3A%20Instruction-based%20video%20editing%20aims%20to%20modify%20an%20input%20video%20according%20to%20a%20natural-language%20instruction%20while%20preserving%20content%20fidelity%20and%20temporal%20coherence.%20However%2C%20existing%20diffusion-based%20approaches%20are%20often%20trained%20on%20paired%20data%20of%20simple%20editing%20operations%2C%20which%20fundamentally%20limits%20their%20ability%20to%20generalize%20to%20diverse%20and%20complex%2C%20real-world%20instructions.%20To%20address%20this%20generalization%20gap%2C%20we%20propose%20VIVA%2C%20a%20scalable%20framework%20for%20instruction-based%20video%20editing%20that%20leverages%20VLM-guided%20encoding%20and%20reward%20optimization.%20First%2C%20we%20introduce%20a%20VLM-based%20instructor%20that%20encodes%20the%20textual%20instruction%2C%20the%20first%20frame%20of%20the%20source%20video%2C%20and%20an%20optional%20reference%20image%20into%20visually-grounded%20instruction%20representations%2C%20providing%20fine-grained%20spatial%20and%20semantic%20context%20for%20the%20diffusion%20transformer%20backbone.%20Second%2C%20we%20propose%20a%20post-training%20stage%2C%20Edit-GRPO%2C%20which%20adapts%20Group%20Relative%20Policy%20Optimization%20to%20the%20domain%20of%20video%20editing%2C%20directly%20optimizing%20the%20model%20for%20instruction-faithful%2C%20content-preserving%2C%20and%20aesthetically%20pleasing%20edits%20using%20relative%20rewards.%20Furthermore%2C%20we%20propose%20a%20data%20construction%20pipeline%20designed%20to%20synthetically%20generate%20diverse%2C%20high-fidelity%20paired%20video-instruction%20data%20of%20basic%20editing%20operations.%20Extensive%20experiments%20show%20that%20VIVA%20achieves%20superior%20instruction%20following%2C%20generalization%2C%20and%20editing%20quality%20over%20state-of-the-art%20methods.%20Website%3A%20https%3A//viva-paper.github.io%0ALink%3A%20http%3A//arxiv.org/abs/2512.16906v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIVA%253A%2520VLM-Guided%2520Instruction-Based%2520Video%2520Editing%2520with%2520Reward%2520Optimization%26entry.906535625%3DXiaoyan%2520Cong%2520and%2520Haotian%2520Yang%2520and%2520Angtian%2520Wang%2520and%2520Yizhi%2520Wang%2520and%2520Yiding%2520Yang%2520and%2520Canyu%2520Zhang%2520and%2520Chongyang%2520Ma%26entry.1292438233%3DInstruction-based%2520video%2520editing%2520aims%2520to%2520modify%2520an%2520input%2520video%2520according%2520to%2520a%2520natural-language%2520instruction%2520while%2520preserving%2520content%2520fidelity%2520and%2520temporal%2520coherence.%2520However%252C%2520existing%2520diffusion-based%2520approaches%2520are%2520often%2520trained%2520on%2520paired%2520data%2520of%2520simple%2520editing%2520operations%252C%2520which%2520fundamentally%2520limits%2520their%2520ability%2520to%2520generalize%2520to%2520diverse%2520and%2520complex%252C%2520real-world%2520instructions.%2520To%2520address%2520this%2520generalization%2520gap%252C%2520we%2520propose%2520VIVA%252C%2520a%2520scalable%2520framework%2520for%2520instruction-based%2520video%2520editing%2520that%2520leverages%2520VLM-guided%2520encoding%2520and%2520reward%2520optimization.%2520First%252C%2520we%2520introduce%2520a%2520VLM-based%2520instructor%2520that%2520encodes%2520the%2520textual%2520instruction%252C%2520the%2520first%2520frame%2520of%2520the%2520source%2520video%252C%2520and%2520an%2520optional%2520reference%2520image%2520into%2520visually-grounded%2520instruction%2520representations%252C%2520providing%2520fine-grained%2520spatial%2520and%2520semantic%2520context%2520for%2520the%2520diffusion%2520transformer%2520backbone.%2520Second%252C%2520we%2520propose%2520a%2520post-training%2520stage%252C%2520Edit-GRPO%252C%2520which%2520adapts%2520Group%2520Relative%2520Policy%2520Optimization%2520to%2520the%2520domain%2520of%2520video%2520editing%252C%2520directly%2520optimizing%2520the%2520model%2520for%2520instruction-faithful%252C%2520content-preserving%252C%2520and%2520aesthetically%2520pleasing%2520edits%2520using%2520relative%2520rewards.%2520Furthermore%252C%2520we%2520propose%2520a%2520data%2520construction%2520pipeline%2520designed%2520to%2520synthetically%2520generate%2520diverse%252C%2520high-fidelity%2520paired%2520video-instruction%2520data%2520of%2520basic%2520editing%2520operations.%2520Extensive%2520experiments%2520show%2520that%2520VIVA%2520achieves%2520superior%2520instruction%2520following%252C%2520generalization%252C%2520and%2520editing%2520quality%2520over%2520state-of-the-art%2520methods.%2520Website%253A%2520https%253A//viva-paper.github.io%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16906v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIVA%3A%20VLM-Guided%20Instruction-Based%20Video%20Editing%20with%20Reward%20Optimization&entry.906535625=Xiaoyan%20Cong%20and%20Haotian%20Yang%20and%20Angtian%20Wang%20and%20Yizhi%20Wang%20and%20Yiding%20Yang%20and%20Canyu%20Zhang%20and%20Chongyang%20Ma&entry.1292438233=Instruction-based%20video%20editing%20aims%20to%20modify%20an%20input%20video%20according%20to%20a%20natural-language%20instruction%20while%20preserving%20content%20fidelity%20and%20temporal%20coherence.%20However%2C%20existing%20diffusion-based%20approaches%20are%20often%20trained%20on%20paired%20data%20of%20simple%20editing%20operations%2C%20which%20fundamentally%20limits%20their%20ability%20to%20generalize%20to%20diverse%20and%20complex%2C%20real-world%20instructions.%20To%20address%20this%20generalization%20gap%2C%20we%20propose%20VIVA%2C%20a%20scalable%20framework%20for%20instruction-based%20video%20editing%20that%20leverages%20VLM-guided%20encoding%20and%20reward%20optimization.%20First%2C%20we%20introduce%20a%20VLM-based%20instructor%20that%20encodes%20the%20textual%20instruction%2C%20the%20first%20frame%20of%20the%20source%20video%2C%20and%20an%20optional%20reference%20image%20into%20visually-grounded%20instruction%20representations%2C%20providing%20fine-grained%20spatial%20and%20semantic%20context%20for%20the%20diffusion%20transformer%20backbone.%20Second%2C%20we%20propose%20a%20post-training%20stage%2C%20Edit-GRPO%2C%20which%20adapts%20Group%20Relative%20Policy%20Optimization%20to%20the%20domain%20of%20video%20editing%2C%20directly%20optimizing%20the%20model%20for%20instruction-faithful%2C%20content-preserving%2C%20and%20aesthetically%20pleasing%20edits%20using%20relative%20rewards.%20Furthermore%2C%20we%20propose%20a%20data%20construction%20pipeline%20designed%20to%20synthetically%20generate%20diverse%2C%20high-fidelity%20paired%20video-instruction%20data%20of%20basic%20editing%20operations.%20Extensive%20experiments%20show%20that%20VIVA%20achieves%20superior%20instruction%20following%2C%20generalization%2C%20and%20editing%20quality%20over%20state-of-the-art%20methods.%20Website%3A%20https%3A//viva-paper.github.io&entry.1838667208=http%3A//arxiv.org/abs/2512.16906v1&entry.124074799=Read"},
{"title": "A2VISR: An Active and Adaptive Ground-Aerial Localization System Using Visual Inertial and Single-Range Fusion", "author": "Sijia Chen and Wei Dong", "abstract": "It's a practical approach using the ground-aerial collaborative system to enhance the localization robustness of flying robots in cluttered environments, especially when visual sensors degrade. Conventional approaches estimate the flying robot's position using fixed cameras observing pre-attached markers, which could be constrained by limited distance and susceptible to capture failure. To address this issue, we improve the ground-aerial localization framework in a more comprehensive manner, which integrates active vision, single-ranging, inertial odometry, and optical flow. First, the designed active vision subsystem mounted on the ground vehicle can be dynamically rotated to detect and track infrared markers on the aerial robot, improving the field of view and the target recognition with a single camera. Meanwhile, the incorporation of single-ranging extends the feasible distance and enhances re-capture capability under visual degradation. During estimation, a dimension-reduced estimator fuses multi-source measurements based on polynomial approximation with an extended sliding window, balancing computational efficiency and redundancy. Considering different sensor fidelities, an adaptive sliding confidence evaluation algorithm is implemented to assess measurement quality and dynamically adjust the weighting parameters based on moving variance. Finally, extensive experiments under conditions such as smoke interference, illumination variation, obstacle occlusion, prolonged visual loss, and extended operating range demonstrate that the proposed approach achieves robust online localization, with an average root mean square error of approximately 0.09 m, while maintaining resilience to capture loss and sensor failures.", "link": "http://arxiv.org/abs/2512.16367v1", "date": "2025-12-18", "relevancy": 2.3554, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6132}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5781}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A2VISR%3A%20An%20Active%20and%20Adaptive%20Ground-Aerial%20Localization%20System%20Using%20Visual%20Inertial%20and%20Single-Range%20Fusion&body=Title%3A%20A2VISR%3A%20An%20Active%20and%20Adaptive%20Ground-Aerial%20Localization%20System%20Using%20Visual%20Inertial%20and%20Single-Range%20Fusion%0AAuthor%3A%20Sijia%20Chen%20and%20Wei%20Dong%0AAbstract%3A%20It%27s%20a%20practical%20approach%20using%20the%20ground-aerial%20collaborative%20system%20to%20enhance%20the%20localization%20robustness%20of%20flying%20robots%20in%20cluttered%20environments%2C%20especially%20when%20visual%20sensors%20degrade.%20Conventional%20approaches%20estimate%20the%20flying%20robot%27s%20position%20using%20fixed%20cameras%20observing%20pre-attached%20markers%2C%20which%20could%20be%20constrained%20by%20limited%20distance%20and%20susceptible%20to%20capture%20failure.%20To%20address%20this%20issue%2C%20we%20improve%20the%20ground-aerial%20localization%20framework%20in%20a%20more%20comprehensive%20manner%2C%20which%20integrates%20active%20vision%2C%20single-ranging%2C%20inertial%20odometry%2C%20and%20optical%20flow.%20First%2C%20the%20designed%20active%20vision%20subsystem%20mounted%20on%20the%20ground%20vehicle%20can%20be%20dynamically%20rotated%20to%20detect%20and%20track%20infrared%20markers%20on%20the%20aerial%20robot%2C%20improving%20the%20field%20of%20view%20and%20the%20target%20recognition%20with%20a%20single%20camera.%20Meanwhile%2C%20the%20incorporation%20of%20single-ranging%20extends%20the%20feasible%20distance%20and%20enhances%20re-capture%20capability%20under%20visual%20degradation.%20During%20estimation%2C%20a%20dimension-reduced%20estimator%20fuses%20multi-source%20measurements%20based%20on%20polynomial%20approximation%20with%20an%20extended%20sliding%20window%2C%20balancing%20computational%20efficiency%20and%20redundancy.%20Considering%20different%20sensor%20fidelities%2C%20an%20adaptive%20sliding%20confidence%20evaluation%20algorithm%20is%20implemented%20to%20assess%20measurement%20quality%20and%20dynamically%20adjust%20the%20weighting%20parameters%20based%20on%20moving%20variance.%20Finally%2C%20extensive%20experiments%20under%20conditions%20such%20as%20smoke%20interference%2C%20illumination%20variation%2C%20obstacle%20occlusion%2C%20prolonged%20visual%20loss%2C%20and%20extended%20operating%20range%20demonstrate%20that%20the%20proposed%20approach%20achieves%20robust%20online%20localization%2C%20with%20an%20average%20root%20mean%20square%20error%20of%20approximately%200.09%20m%2C%20while%20maintaining%20resilience%20to%20capture%20loss%20and%20sensor%20failures.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16367v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA2VISR%253A%2520An%2520Active%2520and%2520Adaptive%2520Ground-Aerial%2520Localization%2520System%2520Using%2520Visual%2520Inertial%2520and%2520Single-Range%2520Fusion%26entry.906535625%3DSijia%2520Chen%2520and%2520Wei%2520Dong%26entry.1292438233%3DIt%2527s%2520a%2520practical%2520approach%2520using%2520the%2520ground-aerial%2520collaborative%2520system%2520to%2520enhance%2520the%2520localization%2520robustness%2520of%2520flying%2520robots%2520in%2520cluttered%2520environments%252C%2520especially%2520when%2520visual%2520sensors%2520degrade.%2520Conventional%2520approaches%2520estimate%2520the%2520flying%2520robot%2527s%2520position%2520using%2520fixed%2520cameras%2520observing%2520pre-attached%2520markers%252C%2520which%2520could%2520be%2520constrained%2520by%2520limited%2520distance%2520and%2520susceptible%2520to%2520capture%2520failure.%2520To%2520address%2520this%2520issue%252C%2520we%2520improve%2520the%2520ground-aerial%2520localization%2520framework%2520in%2520a%2520more%2520comprehensive%2520manner%252C%2520which%2520integrates%2520active%2520vision%252C%2520single-ranging%252C%2520inertial%2520odometry%252C%2520and%2520optical%2520flow.%2520First%252C%2520the%2520designed%2520active%2520vision%2520subsystem%2520mounted%2520on%2520the%2520ground%2520vehicle%2520can%2520be%2520dynamically%2520rotated%2520to%2520detect%2520and%2520track%2520infrared%2520markers%2520on%2520the%2520aerial%2520robot%252C%2520improving%2520the%2520field%2520of%2520view%2520and%2520the%2520target%2520recognition%2520with%2520a%2520single%2520camera.%2520Meanwhile%252C%2520the%2520incorporation%2520of%2520single-ranging%2520extends%2520the%2520feasible%2520distance%2520and%2520enhances%2520re-capture%2520capability%2520under%2520visual%2520degradation.%2520During%2520estimation%252C%2520a%2520dimension-reduced%2520estimator%2520fuses%2520multi-source%2520measurements%2520based%2520on%2520polynomial%2520approximation%2520with%2520an%2520extended%2520sliding%2520window%252C%2520balancing%2520computational%2520efficiency%2520and%2520redundancy.%2520Considering%2520different%2520sensor%2520fidelities%252C%2520an%2520adaptive%2520sliding%2520confidence%2520evaluation%2520algorithm%2520is%2520implemented%2520to%2520assess%2520measurement%2520quality%2520and%2520dynamically%2520adjust%2520the%2520weighting%2520parameters%2520based%2520on%2520moving%2520variance.%2520Finally%252C%2520extensive%2520experiments%2520under%2520conditions%2520such%2520as%2520smoke%2520interference%252C%2520illumination%2520variation%252C%2520obstacle%2520occlusion%252C%2520prolonged%2520visual%2520loss%252C%2520and%2520extended%2520operating%2520range%2520demonstrate%2520that%2520the%2520proposed%2520approach%2520achieves%2520robust%2520online%2520localization%252C%2520with%2520an%2520average%2520root%2520mean%2520square%2520error%2520of%2520approximately%25200.09%2520m%252C%2520while%2520maintaining%2520resilience%2520to%2520capture%2520loss%2520and%2520sensor%2520failures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16367v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A2VISR%3A%20An%20Active%20and%20Adaptive%20Ground-Aerial%20Localization%20System%20Using%20Visual%20Inertial%20and%20Single-Range%20Fusion&entry.906535625=Sijia%20Chen%20and%20Wei%20Dong&entry.1292438233=It%27s%20a%20practical%20approach%20using%20the%20ground-aerial%20collaborative%20system%20to%20enhance%20the%20localization%20robustness%20of%20flying%20robots%20in%20cluttered%20environments%2C%20especially%20when%20visual%20sensors%20degrade.%20Conventional%20approaches%20estimate%20the%20flying%20robot%27s%20position%20using%20fixed%20cameras%20observing%20pre-attached%20markers%2C%20which%20could%20be%20constrained%20by%20limited%20distance%20and%20susceptible%20to%20capture%20failure.%20To%20address%20this%20issue%2C%20we%20improve%20the%20ground-aerial%20localization%20framework%20in%20a%20more%20comprehensive%20manner%2C%20which%20integrates%20active%20vision%2C%20single-ranging%2C%20inertial%20odometry%2C%20and%20optical%20flow.%20First%2C%20the%20designed%20active%20vision%20subsystem%20mounted%20on%20the%20ground%20vehicle%20can%20be%20dynamically%20rotated%20to%20detect%20and%20track%20infrared%20markers%20on%20the%20aerial%20robot%2C%20improving%20the%20field%20of%20view%20and%20the%20target%20recognition%20with%20a%20single%20camera.%20Meanwhile%2C%20the%20incorporation%20of%20single-ranging%20extends%20the%20feasible%20distance%20and%20enhances%20re-capture%20capability%20under%20visual%20degradation.%20During%20estimation%2C%20a%20dimension-reduced%20estimator%20fuses%20multi-source%20measurements%20based%20on%20polynomial%20approximation%20with%20an%20extended%20sliding%20window%2C%20balancing%20computational%20efficiency%20and%20redundancy.%20Considering%20different%20sensor%20fidelities%2C%20an%20adaptive%20sliding%20confidence%20evaluation%20algorithm%20is%20implemented%20to%20assess%20measurement%20quality%20and%20dynamically%20adjust%20the%20weighting%20parameters%20based%20on%20moving%20variance.%20Finally%2C%20extensive%20experiments%20under%20conditions%20such%20as%20smoke%20interference%2C%20illumination%20variation%2C%20obstacle%20occlusion%2C%20prolonged%20visual%20loss%2C%20and%20extended%20operating%20range%20demonstrate%20that%20the%20proposed%20approach%20achieves%20robust%20online%20localization%2C%20with%20an%20average%20root%20mean%20square%20error%20of%20approximately%200.09%20m%2C%20while%20maintaining%20resilience%20to%20capture%20loss%20and%20sensor%20failures.&entry.1838667208=http%3A//arxiv.org/abs/2512.16367v1&entry.124074799=Read"},
{"title": "ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning", "author": "Zihan Zhou and Animesh Garg and Ajay Mandlekar and Caelan Garrett", "abstract": "Long-horizon manipulation has been a long-standing challenge in the robotics community. We propose ReinforceGen, a system that combines task decomposition, data generation, imitation learning, and motion planning to form an initial solution, and improves each component through reinforcement-learning-based fine-tuning. ReinforceGen first segments the task into multiple localized skills, which are connected through motion planning. The skills and motion planning targets are trained with imitation learning on a dataset generated from 10 human demonstrations, and then fine-tuned through online adaptation and reinforcement learning. When benchmarked on the Robosuite dataset, ReinforceGen reaches 80% success rate on all tasks with visuomotor controls in the highest reset range setting. Additional ablation studies show that our fine-tuning approaches contributes to an 89% average performance increase. More results and videos available in https://reinforcegen.github.io/", "link": "http://arxiv.org/abs/2512.16861v1", "date": "2025-12-18", "relevancy": 2.3453, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5972}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5857}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReinforceGen%3A%20Hybrid%20Skill%20Policies%20with%20Automated%20Data%20Generation%20and%20Reinforcement%20Learning&body=Title%3A%20ReinforceGen%3A%20Hybrid%20Skill%20Policies%20with%20Automated%20Data%20Generation%20and%20Reinforcement%20Learning%0AAuthor%3A%20Zihan%20Zhou%20and%20Animesh%20Garg%20and%20Ajay%20Mandlekar%20and%20Caelan%20Garrett%0AAbstract%3A%20Long-horizon%20manipulation%20has%20been%20a%20long-standing%20challenge%20in%20the%20robotics%20community.%20We%20propose%20ReinforceGen%2C%20a%20system%20that%20combines%20task%20decomposition%2C%20data%20generation%2C%20imitation%20learning%2C%20and%20motion%20planning%20to%20form%20an%20initial%20solution%2C%20and%20improves%20each%20component%20through%20reinforcement-learning-based%20fine-tuning.%20ReinforceGen%20first%20segments%20the%20task%20into%20multiple%20localized%20skills%2C%20which%20are%20connected%20through%20motion%20planning.%20The%20skills%20and%20motion%20planning%20targets%20are%20trained%20with%20imitation%20learning%20on%20a%20dataset%20generated%20from%2010%20human%20demonstrations%2C%20and%20then%20fine-tuned%20through%20online%20adaptation%20and%20reinforcement%20learning.%20When%20benchmarked%20on%20the%20Robosuite%20dataset%2C%20ReinforceGen%20reaches%2080%25%20success%20rate%20on%20all%20tasks%20with%20visuomotor%20controls%20in%20the%20highest%20reset%20range%20setting.%20Additional%20ablation%20studies%20show%20that%20our%20fine-tuning%20approaches%20contributes%20to%20an%2089%25%20average%20performance%20increase.%20More%20results%20and%20videos%20available%20in%20https%3A//reinforcegen.github.io/%0ALink%3A%20http%3A//arxiv.org/abs/2512.16861v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforceGen%253A%2520Hybrid%2520Skill%2520Policies%2520with%2520Automated%2520Data%2520Generation%2520and%2520Reinforcement%2520Learning%26entry.906535625%3DZihan%2520Zhou%2520and%2520Animesh%2520Garg%2520and%2520Ajay%2520Mandlekar%2520and%2520Caelan%2520Garrett%26entry.1292438233%3DLong-horizon%2520manipulation%2520has%2520been%2520a%2520long-standing%2520challenge%2520in%2520the%2520robotics%2520community.%2520We%2520propose%2520ReinforceGen%252C%2520a%2520system%2520that%2520combines%2520task%2520decomposition%252C%2520data%2520generation%252C%2520imitation%2520learning%252C%2520and%2520motion%2520planning%2520to%2520form%2520an%2520initial%2520solution%252C%2520and%2520improves%2520each%2520component%2520through%2520reinforcement-learning-based%2520fine-tuning.%2520ReinforceGen%2520first%2520segments%2520the%2520task%2520into%2520multiple%2520localized%2520skills%252C%2520which%2520are%2520connected%2520through%2520motion%2520planning.%2520The%2520skills%2520and%2520motion%2520planning%2520targets%2520are%2520trained%2520with%2520imitation%2520learning%2520on%2520a%2520dataset%2520generated%2520from%252010%2520human%2520demonstrations%252C%2520and%2520then%2520fine-tuned%2520through%2520online%2520adaptation%2520and%2520reinforcement%2520learning.%2520When%2520benchmarked%2520on%2520the%2520Robosuite%2520dataset%252C%2520ReinforceGen%2520reaches%252080%2525%2520success%2520rate%2520on%2520all%2520tasks%2520with%2520visuomotor%2520controls%2520in%2520the%2520highest%2520reset%2520range%2520setting.%2520Additional%2520ablation%2520studies%2520show%2520that%2520our%2520fine-tuning%2520approaches%2520contributes%2520to%2520an%252089%2525%2520average%2520performance%2520increase.%2520More%2520results%2520and%2520videos%2520available%2520in%2520https%253A//reinforcegen.github.io/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16861v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReinforceGen%3A%20Hybrid%20Skill%20Policies%20with%20Automated%20Data%20Generation%20and%20Reinforcement%20Learning&entry.906535625=Zihan%20Zhou%20and%20Animesh%20Garg%20and%20Ajay%20Mandlekar%20and%20Caelan%20Garrett&entry.1292438233=Long-horizon%20manipulation%20has%20been%20a%20long-standing%20challenge%20in%20the%20robotics%20community.%20We%20propose%20ReinforceGen%2C%20a%20system%20that%20combines%20task%20decomposition%2C%20data%20generation%2C%20imitation%20learning%2C%20and%20motion%20planning%20to%20form%20an%20initial%20solution%2C%20and%20improves%20each%20component%20through%20reinforcement-learning-based%20fine-tuning.%20ReinforceGen%20first%20segments%20the%20task%20into%20multiple%20localized%20skills%2C%20which%20are%20connected%20through%20motion%20planning.%20The%20skills%20and%20motion%20planning%20targets%20are%20trained%20with%20imitation%20learning%20on%20a%20dataset%20generated%20from%2010%20human%20demonstrations%2C%20and%20then%20fine-tuned%20through%20online%20adaptation%20and%20reinforcement%20learning.%20When%20benchmarked%20on%20the%20Robosuite%20dataset%2C%20ReinforceGen%20reaches%2080%25%20success%20rate%20on%20all%20tasks%20with%20visuomotor%20controls%20in%20the%20highest%20reset%20range%20setting.%20Additional%20ablation%20studies%20show%20that%20our%20fine-tuning%20approaches%20contributes%20to%20an%2089%25%20average%20performance%20increase.%20More%20results%20and%20videos%20available%20in%20https%3A//reinforcegen.github.io/&entry.1838667208=http%3A//arxiv.org/abs/2512.16861v1&entry.124074799=Read"},
{"title": "KineST: A Kinematics-guided Spatiotemporal State Space Model for Human Motion Tracking from Sparse Signals", "author": "Shuting Zhao and Zeyu Xiao and Xinrong Chen", "abstract": "Full-body motion tracking plays an essential role in AR/VR applications, bridging physical and virtual interactions. However, it is challenging to reconstruct realistic and diverse full-body poses based on sparse signals obtained by head-mounted displays, which are the main devices in AR/VR scenarios. Existing methods for pose reconstruction often incur high computational costs or rely on separately modeling spatial and temporal dependencies, making it difficult to balance accuracy, temporal coherence, and efficiency. To address this problem, we propose KineST, a novel kinematics-guided state space model, which effectively extracts spatiotemporal dependencies while integrating local and global pose perception. The innovation comes from two core ideas. Firstly, in order to better capture intricate joint relationships, the scanning strategy within the State Space Duality framework is reformulated into kinematics-guided bidirectional scanning, which embeds kinematic priors. Secondly, a mixed spatiotemporal representation learning approach is employed to tightly couple spatial and temporal contexts, balancing accuracy and smoothness. Additionally, a geometric angular velocity loss is introduced to impose physically meaningful constraints on rotational variations for further improving motion stability. Extensive experiments demonstrate that KineST has superior performance in both accuracy and temporal consistency within a lightweight framework. Project page: https://kaka-1314.github.io/KineST/", "link": "http://arxiv.org/abs/2512.16791v1", "date": "2025-12-18", "relevancy": 2.3364, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.624}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5701}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KineST%3A%20A%20Kinematics-guided%20Spatiotemporal%20State%20Space%20Model%20for%20Human%20Motion%20Tracking%20from%20Sparse%20Signals&body=Title%3A%20KineST%3A%20A%20Kinematics-guided%20Spatiotemporal%20State%20Space%20Model%20for%20Human%20Motion%20Tracking%20from%20Sparse%20Signals%0AAuthor%3A%20Shuting%20Zhao%20and%20Zeyu%20Xiao%20and%20Xinrong%20Chen%0AAbstract%3A%20Full-body%20motion%20tracking%20plays%20an%20essential%20role%20in%20AR/VR%20applications%2C%20bridging%20physical%20and%20virtual%20interactions.%20However%2C%20it%20is%20challenging%20to%20reconstruct%20realistic%20and%20diverse%20full-body%20poses%20based%20on%20sparse%20signals%20obtained%20by%20head-mounted%20displays%2C%20which%20are%20the%20main%20devices%20in%20AR/VR%20scenarios.%20Existing%20methods%20for%20pose%20reconstruction%20often%20incur%20high%20computational%20costs%20or%20rely%20on%20separately%20modeling%20spatial%20and%20temporal%20dependencies%2C%20making%20it%20difficult%20to%20balance%20accuracy%2C%20temporal%20coherence%2C%20and%20efficiency.%20To%20address%20this%20problem%2C%20we%20propose%20KineST%2C%20a%20novel%20kinematics-guided%20state%20space%20model%2C%20which%20effectively%20extracts%20spatiotemporal%20dependencies%20while%20integrating%20local%20and%20global%20pose%20perception.%20The%20innovation%20comes%20from%20two%20core%20ideas.%20Firstly%2C%20in%20order%20to%20better%20capture%20intricate%20joint%20relationships%2C%20the%20scanning%20strategy%20within%20the%20State%20Space%20Duality%20framework%20is%20reformulated%20into%20kinematics-guided%20bidirectional%20scanning%2C%20which%20embeds%20kinematic%20priors.%20Secondly%2C%20a%20mixed%20spatiotemporal%20representation%20learning%20approach%20is%20employed%20to%20tightly%20couple%20spatial%20and%20temporal%20contexts%2C%20balancing%20accuracy%20and%20smoothness.%20Additionally%2C%20a%20geometric%20angular%20velocity%20loss%20is%20introduced%20to%20impose%20physically%20meaningful%20constraints%20on%20rotational%20variations%20for%20further%20improving%20motion%20stability.%20Extensive%20experiments%20demonstrate%20that%20KineST%20has%20superior%20performance%20in%20both%20accuracy%20and%20temporal%20consistency%20within%20a%20lightweight%20framework.%20Project%20page%3A%20https%3A//kaka-1314.github.io/KineST/%0ALink%3A%20http%3A//arxiv.org/abs/2512.16791v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKineST%253A%2520A%2520Kinematics-guided%2520Spatiotemporal%2520State%2520Space%2520Model%2520for%2520Human%2520Motion%2520Tracking%2520from%2520Sparse%2520Signals%26entry.906535625%3DShuting%2520Zhao%2520and%2520Zeyu%2520Xiao%2520and%2520Xinrong%2520Chen%26entry.1292438233%3DFull-body%2520motion%2520tracking%2520plays%2520an%2520essential%2520role%2520in%2520AR/VR%2520applications%252C%2520bridging%2520physical%2520and%2520virtual%2520interactions.%2520However%252C%2520it%2520is%2520challenging%2520to%2520reconstruct%2520realistic%2520and%2520diverse%2520full-body%2520poses%2520based%2520on%2520sparse%2520signals%2520obtained%2520by%2520head-mounted%2520displays%252C%2520which%2520are%2520the%2520main%2520devices%2520in%2520AR/VR%2520scenarios.%2520Existing%2520methods%2520for%2520pose%2520reconstruction%2520often%2520incur%2520high%2520computational%2520costs%2520or%2520rely%2520on%2520separately%2520modeling%2520spatial%2520and%2520temporal%2520dependencies%252C%2520making%2520it%2520difficult%2520to%2520balance%2520accuracy%252C%2520temporal%2520coherence%252C%2520and%2520efficiency.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520KineST%252C%2520a%2520novel%2520kinematics-guided%2520state%2520space%2520model%252C%2520which%2520effectively%2520extracts%2520spatiotemporal%2520dependencies%2520while%2520integrating%2520local%2520and%2520global%2520pose%2520perception.%2520The%2520innovation%2520comes%2520from%2520two%2520core%2520ideas.%2520Firstly%252C%2520in%2520order%2520to%2520better%2520capture%2520intricate%2520joint%2520relationships%252C%2520the%2520scanning%2520strategy%2520within%2520the%2520State%2520Space%2520Duality%2520framework%2520is%2520reformulated%2520into%2520kinematics-guided%2520bidirectional%2520scanning%252C%2520which%2520embeds%2520kinematic%2520priors.%2520Secondly%252C%2520a%2520mixed%2520spatiotemporal%2520representation%2520learning%2520approach%2520is%2520employed%2520to%2520tightly%2520couple%2520spatial%2520and%2520temporal%2520contexts%252C%2520balancing%2520accuracy%2520and%2520smoothness.%2520Additionally%252C%2520a%2520geometric%2520angular%2520velocity%2520loss%2520is%2520introduced%2520to%2520impose%2520physically%2520meaningful%2520constraints%2520on%2520rotational%2520variations%2520for%2520further%2520improving%2520motion%2520stability.%2520Extensive%2520experiments%2520demonstrate%2520that%2520KineST%2520has%2520superior%2520performance%2520in%2520both%2520accuracy%2520and%2520temporal%2520consistency%2520within%2520a%2520lightweight%2520framework.%2520Project%2520page%253A%2520https%253A//kaka-1314.github.io/KineST/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16791v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KineST%3A%20A%20Kinematics-guided%20Spatiotemporal%20State%20Space%20Model%20for%20Human%20Motion%20Tracking%20from%20Sparse%20Signals&entry.906535625=Shuting%20Zhao%20and%20Zeyu%20Xiao%20and%20Xinrong%20Chen&entry.1292438233=Full-body%20motion%20tracking%20plays%20an%20essential%20role%20in%20AR/VR%20applications%2C%20bridging%20physical%20and%20virtual%20interactions.%20However%2C%20it%20is%20challenging%20to%20reconstruct%20realistic%20and%20diverse%20full-body%20poses%20based%20on%20sparse%20signals%20obtained%20by%20head-mounted%20displays%2C%20which%20are%20the%20main%20devices%20in%20AR/VR%20scenarios.%20Existing%20methods%20for%20pose%20reconstruction%20often%20incur%20high%20computational%20costs%20or%20rely%20on%20separately%20modeling%20spatial%20and%20temporal%20dependencies%2C%20making%20it%20difficult%20to%20balance%20accuracy%2C%20temporal%20coherence%2C%20and%20efficiency.%20To%20address%20this%20problem%2C%20we%20propose%20KineST%2C%20a%20novel%20kinematics-guided%20state%20space%20model%2C%20which%20effectively%20extracts%20spatiotemporal%20dependencies%20while%20integrating%20local%20and%20global%20pose%20perception.%20The%20innovation%20comes%20from%20two%20core%20ideas.%20Firstly%2C%20in%20order%20to%20better%20capture%20intricate%20joint%20relationships%2C%20the%20scanning%20strategy%20within%20the%20State%20Space%20Duality%20framework%20is%20reformulated%20into%20kinematics-guided%20bidirectional%20scanning%2C%20which%20embeds%20kinematic%20priors.%20Secondly%2C%20a%20mixed%20spatiotemporal%20representation%20learning%20approach%20is%20employed%20to%20tightly%20couple%20spatial%20and%20temporal%20contexts%2C%20balancing%20accuracy%20and%20smoothness.%20Additionally%2C%20a%20geometric%20angular%20velocity%20loss%20is%20introduced%20to%20impose%20physically%20meaningful%20constraints%20on%20rotational%20variations%20for%20further%20improving%20motion%20stability.%20Extensive%20experiments%20demonstrate%20that%20KineST%20has%20superior%20performance%20in%20both%20accuracy%20and%20temporal%20consistency%20within%20a%20lightweight%20framework.%20Project%20page%3A%20https%3A//kaka-1314.github.io/KineST/&entry.1838667208=http%3A//arxiv.org/abs/2512.16791v1&entry.124074799=Read"},
{"title": "A Systematic Study of Code Obfuscation Against LLM-based Vulnerability Detection", "author": "Xiao Li and Yue Li and Hao Wu and Yue Zhang and Yechao Zhang and Fengyuan Xu and Sheng Zhong", "abstract": "As large language models (LLMs) are increasingly adopted for code vulnerability detection, their reliability and robustness across diverse vulnerability types have become a pressing concern. In traditional adversarial settings, code obfuscation has long been used as a general strategy to bypass auditing tools, preserving exploitability without tampering with the tools themselves. Numerous efforts have explored obfuscation methods and tools, yet their capabilities differ in terms of supported techniques, granularity, and programming languages, making it difficult to systematically assess their impact on LLM-based vulnerability detection. To address this gap, we provide a structured systematization of obfuscation techniques and evaluate them under a unified framework. Specifically, we categorize existing obfuscation methods into three major classes (layout, data flow, and control flow) covering 11 subcategories and 19 concrete techniques. We implement these techniques across four programming languages (Solidity, C, C++, and Python) using a consistent LLM-driven approach, and evaluate their effects on 15 LLMs spanning four model families (DeepSeek, OpenAI, Qwen, and LLaMA), as well as on two coding agents (GitHub Copilot and Codex). Our findings reveal both positive and negative impacts of code obfuscation on LLM-based vulnerability detection, highlighting conditions under which obfuscation leads to performance improvements or degradations. We further analyze these outcomes with respect to vulnerability characteristics, code properties, and model attributes. Finally, we outline several open problems and propose future directions to enhance the robustness of LLMs for real-world vulnerability detection.", "link": "http://arxiv.org/abs/2512.16538v1", "date": "2025-12-18", "relevancy": 2.3361, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4749}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4749}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Systematic%20Study%20of%20Code%20Obfuscation%20Against%20LLM-based%20Vulnerability%20Detection&body=Title%3A%20A%20Systematic%20Study%20of%20Code%20Obfuscation%20Against%20LLM-based%20Vulnerability%20Detection%0AAuthor%3A%20Xiao%20Li%20and%20Yue%20Li%20and%20Hao%20Wu%20and%20Yue%20Zhang%20and%20Yechao%20Zhang%20and%20Fengyuan%20Xu%20and%20Sheng%20Zhong%0AAbstract%3A%20As%20large%20language%20models%20%28LLMs%29%20are%20increasingly%20adopted%20for%20code%20vulnerability%20detection%2C%20their%20reliability%20and%20robustness%20across%20diverse%20vulnerability%20types%20have%20become%20a%20pressing%20concern.%20In%20traditional%20adversarial%20settings%2C%20code%20obfuscation%20has%20long%20been%20used%20as%20a%20general%20strategy%20to%20bypass%20auditing%20tools%2C%20preserving%20exploitability%20without%20tampering%20with%20the%20tools%20themselves.%20Numerous%20efforts%20have%20explored%20obfuscation%20methods%20and%20tools%2C%20yet%20their%20capabilities%20differ%20in%20terms%20of%20supported%20techniques%2C%20granularity%2C%20and%20programming%20languages%2C%20making%20it%20difficult%20to%20systematically%20assess%20their%20impact%20on%20LLM-based%20vulnerability%20detection.%20To%20address%20this%20gap%2C%20we%20provide%20a%20structured%20systematization%20of%20obfuscation%20techniques%20and%20evaluate%20them%20under%20a%20unified%20framework.%20Specifically%2C%20we%20categorize%20existing%20obfuscation%20methods%20into%20three%20major%20classes%20%28layout%2C%20data%20flow%2C%20and%20control%20flow%29%20covering%2011%20subcategories%20and%2019%20concrete%20techniques.%20We%20implement%20these%20techniques%20across%20four%20programming%20languages%20%28Solidity%2C%20C%2C%20C%2B%2B%2C%20and%20Python%29%20using%20a%20consistent%20LLM-driven%20approach%2C%20and%20evaluate%20their%20effects%20on%2015%20LLMs%20spanning%20four%20model%20families%20%28DeepSeek%2C%20OpenAI%2C%20Qwen%2C%20and%20LLaMA%29%2C%20as%20well%20as%20on%20two%20coding%20agents%20%28GitHub%20Copilot%20and%20Codex%29.%20Our%20findings%20reveal%20both%20positive%20and%20negative%20impacts%20of%20code%20obfuscation%20on%20LLM-based%20vulnerability%20detection%2C%20highlighting%20conditions%20under%20which%20obfuscation%20leads%20to%20performance%20improvements%20or%20degradations.%20We%20further%20analyze%20these%20outcomes%20with%20respect%20to%20vulnerability%20characteristics%2C%20code%20properties%2C%20and%20model%20attributes.%20Finally%2C%20we%20outline%20several%20open%20problems%20and%20propose%20future%20directions%20to%20enhance%20the%20robustness%20of%20LLMs%20for%20real-world%20vulnerability%20detection.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16538v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Systematic%2520Study%2520of%2520Code%2520Obfuscation%2520Against%2520LLM-based%2520Vulnerability%2520Detection%26entry.906535625%3DXiao%2520Li%2520and%2520Yue%2520Li%2520and%2520Hao%2520Wu%2520and%2520Yue%2520Zhang%2520and%2520Yechao%2520Zhang%2520and%2520Fengyuan%2520Xu%2520and%2520Sheng%2520Zhong%26entry.1292438233%3DAs%2520large%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520adopted%2520for%2520code%2520vulnerability%2520detection%252C%2520their%2520reliability%2520and%2520robustness%2520across%2520diverse%2520vulnerability%2520types%2520have%2520become%2520a%2520pressing%2520concern.%2520In%2520traditional%2520adversarial%2520settings%252C%2520code%2520obfuscation%2520has%2520long%2520been%2520used%2520as%2520a%2520general%2520strategy%2520to%2520bypass%2520auditing%2520tools%252C%2520preserving%2520exploitability%2520without%2520tampering%2520with%2520the%2520tools%2520themselves.%2520Numerous%2520efforts%2520have%2520explored%2520obfuscation%2520methods%2520and%2520tools%252C%2520yet%2520their%2520capabilities%2520differ%2520in%2520terms%2520of%2520supported%2520techniques%252C%2520granularity%252C%2520and%2520programming%2520languages%252C%2520making%2520it%2520difficult%2520to%2520systematically%2520assess%2520their%2520impact%2520on%2520LLM-based%2520vulnerability%2520detection.%2520To%2520address%2520this%2520gap%252C%2520we%2520provide%2520a%2520structured%2520systematization%2520of%2520obfuscation%2520techniques%2520and%2520evaluate%2520them%2520under%2520a%2520unified%2520framework.%2520Specifically%252C%2520we%2520categorize%2520existing%2520obfuscation%2520methods%2520into%2520three%2520major%2520classes%2520%2528layout%252C%2520data%2520flow%252C%2520and%2520control%2520flow%2529%2520covering%252011%2520subcategories%2520and%252019%2520concrete%2520techniques.%2520We%2520implement%2520these%2520techniques%2520across%2520four%2520programming%2520languages%2520%2528Solidity%252C%2520C%252C%2520C%252B%252B%252C%2520and%2520Python%2529%2520using%2520a%2520consistent%2520LLM-driven%2520approach%252C%2520and%2520evaluate%2520their%2520effects%2520on%252015%2520LLMs%2520spanning%2520four%2520model%2520families%2520%2528DeepSeek%252C%2520OpenAI%252C%2520Qwen%252C%2520and%2520LLaMA%2529%252C%2520as%2520well%2520as%2520on%2520two%2520coding%2520agents%2520%2528GitHub%2520Copilot%2520and%2520Codex%2529.%2520Our%2520findings%2520reveal%2520both%2520positive%2520and%2520negative%2520impacts%2520of%2520code%2520obfuscation%2520on%2520LLM-based%2520vulnerability%2520detection%252C%2520highlighting%2520conditions%2520under%2520which%2520obfuscation%2520leads%2520to%2520performance%2520improvements%2520or%2520degradations.%2520We%2520further%2520analyze%2520these%2520outcomes%2520with%2520respect%2520to%2520vulnerability%2520characteristics%252C%2520code%2520properties%252C%2520and%2520model%2520attributes.%2520Finally%252C%2520we%2520outline%2520several%2520open%2520problems%2520and%2520propose%2520future%2520directions%2520to%2520enhance%2520the%2520robustness%2520of%2520LLMs%2520for%2520real-world%2520vulnerability%2520detection.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16538v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Systematic%20Study%20of%20Code%20Obfuscation%20Against%20LLM-based%20Vulnerability%20Detection&entry.906535625=Xiao%20Li%20and%20Yue%20Li%20and%20Hao%20Wu%20and%20Yue%20Zhang%20and%20Yechao%20Zhang%20and%20Fengyuan%20Xu%20and%20Sheng%20Zhong&entry.1292438233=As%20large%20language%20models%20%28LLMs%29%20are%20increasingly%20adopted%20for%20code%20vulnerability%20detection%2C%20their%20reliability%20and%20robustness%20across%20diverse%20vulnerability%20types%20have%20become%20a%20pressing%20concern.%20In%20traditional%20adversarial%20settings%2C%20code%20obfuscation%20has%20long%20been%20used%20as%20a%20general%20strategy%20to%20bypass%20auditing%20tools%2C%20preserving%20exploitability%20without%20tampering%20with%20the%20tools%20themselves.%20Numerous%20efforts%20have%20explored%20obfuscation%20methods%20and%20tools%2C%20yet%20their%20capabilities%20differ%20in%20terms%20of%20supported%20techniques%2C%20granularity%2C%20and%20programming%20languages%2C%20making%20it%20difficult%20to%20systematically%20assess%20their%20impact%20on%20LLM-based%20vulnerability%20detection.%20To%20address%20this%20gap%2C%20we%20provide%20a%20structured%20systematization%20of%20obfuscation%20techniques%20and%20evaluate%20them%20under%20a%20unified%20framework.%20Specifically%2C%20we%20categorize%20existing%20obfuscation%20methods%20into%20three%20major%20classes%20%28layout%2C%20data%20flow%2C%20and%20control%20flow%29%20covering%2011%20subcategories%20and%2019%20concrete%20techniques.%20We%20implement%20these%20techniques%20across%20four%20programming%20languages%20%28Solidity%2C%20C%2C%20C%2B%2B%2C%20and%20Python%29%20using%20a%20consistent%20LLM-driven%20approach%2C%20and%20evaluate%20their%20effects%20on%2015%20LLMs%20spanning%20four%20model%20families%20%28DeepSeek%2C%20OpenAI%2C%20Qwen%2C%20and%20LLaMA%29%2C%20as%20well%20as%20on%20two%20coding%20agents%20%28GitHub%20Copilot%20and%20Codex%29.%20Our%20findings%20reveal%20both%20positive%20and%20negative%20impacts%20of%20code%20obfuscation%20on%20LLM-based%20vulnerability%20detection%2C%20highlighting%20conditions%20under%20which%20obfuscation%20leads%20to%20performance%20improvements%20or%20degradations.%20We%20further%20analyze%20these%20outcomes%20with%20respect%20to%20vulnerability%20characteristics%2C%20code%20properties%2C%20and%20model%20attributes.%20Finally%2C%20we%20outline%20several%20open%20problems%20and%20propose%20future%20directions%20to%20enhance%20the%20robustness%20of%20LLMs%20for%20real-world%20vulnerability%20detection.&entry.1838667208=http%3A//arxiv.org/abs/2512.16538v1&entry.124074799=Read"},
{"title": "SDFoam: Signed-Distance Foam for explicit surface reconstruction", "author": "Antonella Rech and Nicola Conci and Nicola Garau", "abstract": "Neural radiance fields (NeRF) have driven impressive progress in view synthesis by using ray-traced volumetric rendering. Splatting-based methods such as 3D Gaussian Splatting (3DGS) provide faster rendering by rasterizing 3D primitives. RadiantFoam (RF) brought ray tracing back, achieving throughput comparable to Gaussian Splatting by organizing radiance with an explicit Voronoi Diagram (VD). Yet, all the mentioned methods still struggle with precise mesh reconstruction. We address this gap by jointly learning an explicit VD with an implicit Signed Distance Field (SDF). The scene is optimized via ray tracing and regularized by an Eikonal objective. The SDF introduces metric-consistent isosurfaces, which, in turn, bias near-surface Voronoi cell faces to align with the zero level set. The resulting model produces crisper, view-consistent surfaces with fewer floaters and improved topology, while preserving photometric quality and maintaining training speed on par with RadiantFoam. Across diverse scenes, our hybrid implicit-explicit formulation, which we name SDFoam, substantially improves mesh reconstruction accuracy (Chamfer distance) with comparable appearance (PSNR, SSIM), without sacrificing efficiency.", "link": "http://arxiv.org/abs/2512.16706v1", "date": "2025-12-18", "relevancy": 2.3314, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6123}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.58}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SDFoam%3A%20Signed-Distance%20Foam%20for%20explicit%20surface%20reconstruction&body=Title%3A%20SDFoam%3A%20Signed-Distance%20Foam%20for%20explicit%20surface%20reconstruction%0AAuthor%3A%20Antonella%20Rech%20and%20Nicola%20Conci%20and%20Nicola%20Garau%0AAbstract%3A%20Neural%20radiance%20fields%20%28NeRF%29%20have%20driven%20impressive%20progress%20in%20view%20synthesis%20by%20using%20ray-traced%20volumetric%20rendering.%20Splatting-based%20methods%20such%20as%203D%20Gaussian%20Splatting%20%283DGS%29%20provide%20faster%20rendering%20by%20rasterizing%203D%20primitives.%20RadiantFoam%20%28RF%29%20brought%20ray%20tracing%20back%2C%20achieving%20throughput%20comparable%20to%20Gaussian%20Splatting%20by%20organizing%20radiance%20with%20an%20explicit%20Voronoi%20Diagram%20%28VD%29.%20Yet%2C%20all%20the%20mentioned%20methods%20still%20struggle%20with%20precise%20mesh%20reconstruction.%20We%20address%20this%20gap%20by%20jointly%20learning%20an%20explicit%20VD%20with%20an%20implicit%20Signed%20Distance%20Field%20%28SDF%29.%20The%20scene%20is%20optimized%20via%20ray%20tracing%20and%20regularized%20by%20an%20Eikonal%20objective.%20The%20SDF%20introduces%20metric-consistent%20isosurfaces%2C%20which%2C%20in%20turn%2C%20bias%20near-surface%20Voronoi%20cell%20faces%20to%20align%20with%20the%20zero%20level%20set.%20The%20resulting%20model%20produces%20crisper%2C%20view-consistent%20surfaces%20with%20fewer%20floaters%20and%20improved%20topology%2C%20while%20preserving%20photometric%20quality%20and%20maintaining%20training%20speed%20on%20par%20with%20RadiantFoam.%20Across%20diverse%20scenes%2C%20our%20hybrid%20implicit-explicit%20formulation%2C%20which%20we%20name%20SDFoam%2C%20substantially%20improves%20mesh%20reconstruction%20accuracy%20%28Chamfer%20distance%29%20with%20comparable%20appearance%20%28PSNR%2C%20SSIM%29%2C%20without%20sacrificing%20efficiency.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16706v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSDFoam%253A%2520Signed-Distance%2520Foam%2520for%2520explicit%2520surface%2520reconstruction%26entry.906535625%3DAntonella%2520Rech%2520and%2520Nicola%2520Conci%2520and%2520Nicola%2520Garau%26entry.1292438233%3DNeural%2520radiance%2520fields%2520%2528NeRF%2529%2520have%2520driven%2520impressive%2520progress%2520in%2520view%2520synthesis%2520by%2520using%2520ray-traced%2520volumetric%2520rendering.%2520Splatting-based%2520methods%2520such%2520as%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520provide%2520faster%2520rendering%2520by%2520rasterizing%25203D%2520primitives.%2520RadiantFoam%2520%2528RF%2529%2520brought%2520ray%2520tracing%2520back%252C%2520achieving%2520throughput%2520comparable%2520to%2520Gaussian%2520Splatting%2520by%2520organizing%2520radiance%2520with%2520an%2520explicit%2520Voronoi%2520Diagram%2520%2528VD%2529.%2520Yet%252C%2520all%2520the%2520mentioned%2520methods%2520still%2520struggle%2520with%2520precise%2520mesh%2520reconstruction.%2520We%2520address%2520this%2520gap%2520by%2520jointly%2520learning%2520an%2520explicit%2520VD%2520with%2520an%2520implicit%2520Signed%2520Distance%2520Field%2520%2528SDF%2529.%2520The%2520scene%2520is%2520optimized%2520via%2520ray%2520tracing%2520and%2520regularized%2520by%2520an%2520Eikonal%2520objective.%2520The%2520SDF%2520introduces%2520metric-consistent%2520isosurfaces%252C%2520which%252C%2520in%2520turn%252C%2520bias%2520near-surface%2520Voronoi%2520cell%2520faces%2520to%2520align%2520with%2520the%2520zero%2520level%2520set.%2520The%2520resulting%2520model%2520produces%2520crisper%252C%2520view-consistent%2520surfaces%2520with%2520fewer%2520floaters%2520and%2520improved%2520topology%252C%2520while%2520preserving%2520photometric%2520quality%2520and%2520maintaining%2520training%2520speed%2520on%2520par%2520with%2520RadiantFoam.%2520Across%2520diverse%2520scenes%252C%2520our%2520hybrid%2520implicit-explicit%2520formulation%252C%2520which%2520we%2520name%2520SDFoam%252C%2520substantially%2520improves%2520mesh%2520reconstruction%2520accuracy%2520%2528Chamfer%2520distance%2529%2520with%2520comparable%2520appearance%2520%2528PSNR%252C%2520SSIM%2529%252C%2520without%2520sacrificing%2520efficiency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16706v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SDFoam%3A%20Signed-Distance%20Foam%20for%20explicit%20surface%20reconstruction&entry.906535625=Antonella%20Rech%20and%20Nicola%20Conci%20and%20Nicola%20Garau&entry.1292438233=Neural%20radiance%20fields%20%28NeRF%29%20have%20driven%20impressive%20progress%20in%20view%20synthesis%20by%20using%20ray-traced%20volumetric%20rendering.%20Splatting-based%20methods%20such%20as%203D%20Gaussian%20Splatting%20%283DGS%29%20provide%20faster%20rendering%20by%20rasterizing%203D%20primitives.%20RadiantFoam%20%28RF%29%20brought%20ray%20tracing%20back%2C%20achieving%20throughput%20comparable%20to%20Gaussian%20Splatting%20by%20organizing%20radiance%20with%20an%20explicit%20Voronoi%20Diagram%20%28VD%29.%20Yet%2C%20all%20the%20mentioned%20methods%20still%20struggle%20with%20precise%20mesh%20reconstruction.%20We%20address%20this%20gap%20by%20jointly%20learning%20an%20explicit%20VD%20with%20an%20implicit%20Signed%20Distance%20Field%20%28SDF%29.%20The%20scene%20is%20optimized%20via%20ray%20tracing%20and%20regularized%20by%20an%20Eikonal%20objective.%20The%20SDF%20introduces%20metric-consistent%20isosurfaces%2C%20which%2C%20in%20turn%2C%20bias%20near-surface%20Voronoi%20cell%20faces%20to%20align%20with%20the%20zero%20level%20set.%20The%20resulting%20model%20produces%20crisper%2C%20view-consistent%20surfaces%20with%20fewer%20floaters%20and%20improved%20topology%2C%20while%20preserving%20photometric%20quality%20and%20maintaining%20training%20speed%20on%20par%20with%20RadiantFoam.%20Across%20diverse%20scenes%2C%20our%20hybrid%20implicit-explicit%20formulation%2C%20which%20we%20name%20SDFoam%2C%20substantially%20improves%20mesh%20reconstruction%20accuracy%20%28Chamfer%20distance%29%20with%20comparable%20appearance%20%28PSNR%2C%20SSIM%29%2C%20without%20sacrificing%20efficiency.&entry.1838667208=http%3A//arxiv.org/abs/2512.16706v1&entry.124074799=Read"},
{"title": "Models That Prove Their Own Correctness", "author": "Noga Amit and Shafi Goldwasser and Orr Paradise and Guy Rothblum", "abstract": "How can we trust the correctness of a learned model on a particular input of interest? Model accuracy is typically measured on average over a distribution of inputs, giving no guarantee for any fixed input. This paper proposes a theoretically-founded solution to this problem: to train Self-Proving models that prove the correctness of their output to a verification algorithm $V$ via an Interactive Proof. Self-Proving models satisfy that, with high probability over an input sampled from a given distribution, the model generates a correct output and successfully proves its correctness to $V$. The soundness property of $V$ guarantees that, for every input, no model can convince $V$ of the correctness of an incorrect output. Thus, a Self-Proving model proves correctness of most of its outputs, while all incorrect outputs (of any model) are detected by $V$. We devise and analyze two generic methods for learning Self-Proving models: Transcript Learning (TL) which relies on access to transcripts of accepting interactions, and Reinforcement Learning from Verifier Feedback (RLVF) which trains a model by emulating interactions with the verifier.", "link": "http://arxiv.org/abs/2405.15722v4", "date": "2025-12-18", "relevancy": 2.3289, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.499}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4492}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Models%20That%20Prove%20Their%20Own%20Correctness&body=Title%3A%20Models%20That%20Prove%20Their%20Own%20Correctness%0AAuthor%3A%20Noga%20Amit%20and%20Shafi%20Goldwasser%20and%20Orr%20Paradise%20and%20Guy%20Rothblum%0AAbstract%3A%20How%20can%20we%20trust%20the%20correctness%20of%20a%20learned%20model%20on%20a%20particular%20input%20of%20interest%3F%20Model%20accuracy%20is%20typically%20measured%20on%20average%20over%20a%20distribution%20of%20inputs%2C%20giving%20no%20guarantee%20for%20any%20fixed%20input.%20This%20paper%20proposes%20a%20theoretically-founded%20solution%20to%20this%20problem%3A%20to%20train%20Self-Proving%20models%20that%20prove%20the%20correctness%20of%20their%20output%20to%20a%20verification%20algorithm%20%24V%24%20via%20an%20Interactive%20Proof.%20Self-Proving%20models%20satisfy%20that%2C%20with%20high%20probability%20over%20an%20input%20sampled%20from%20a%20given%20distribution%2C%20the%20model%20generates%20a%20correct%20output%20and%20successfully%20proves%20its%20correctness%20to%20%24V%24.%20The%20soundness%20property%20of%20%24V%24%20guarantees%20that%2C%20for%20every%20input%2C%20no%20model%20can%20convince%20%24V%24%20of%20the%20correctness%20of%20an%20incorrect%20output.%20Thus%2C%20a%20Self-Proving%20model%20proves%20correctness%20of%20most%20of%20its%20outputs%2C%20while%20all%20incorrect%20outputs%20%28of%20any%20model%29%20are%20detected%20by%20%24V%24.%20We%20devise%20and%20analyze%20two%20generic%20methods%20for%20learning%20Self-Proving%20models%3A%20Transcript%20Learning%20%28TL%29%20which%20relies%20on%20access%20to%20transcripts%20of%20accepting%20interactions%2C%20and%20Reinforcement%20Learning%20from%20Verifier%20Feedback%20%28RLVF%29%20which%20trains%20a%20model%20by%20emulating%20interactions%20with%20the%20verifier.%0ALink%3A%20http%3A//arxiv.org/abs/2405.15722v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModels%2520That%2520Prove%2520Their%2520Own%2520Correctness%26entry.906535625%3DNoga%2520Amit%2520and%2520Shafi%2520Goldwasser%2520and%2520Orr%2520Paradise%2520and%2520Guy%2520Rothblum%26entry.1292438233%3DHow%2520can%2520we%2520trust%2520the%2520correctness%2520of%2520a%2520learned%2520model%2520on%2520a%2520particular%2520input%2520of%2520interest%253F%2520Model%2520accuracy%2520is%2520typically%2520measured%2520on%2520average%2520over%2520a%2520distribution%2520of%2520inputs%252C%2520giving%2520no%2520guarantee%2520for%2520any%2520fixed%2520input.%2520This%2520paper%2520proposes%2520a%2520theoretically-founded%2520solution%2520to%2520this%2520problem%253A%2520to%2520train%2520Self-Proving%2520models%2520that%2520prove%2520the%2520correctness%2520of%2520their%2520output%2520to%2520a%2520verification%2520algorithm%2520%2524V%2524%2520via%2520an%2520Interactive%2520Proof.%2520Self-Proving%2520models%2520satisfy%2520that%252C%2520with%2520high%2520probability%2520over%2520an%2520input%2520sampled%2520from%2520a%2520given%2520distribution%252C%2520the%2520model%2520generates%2520a%2520correct%2520output%2520and%2520successfully%2520proves%2520its%2520correctness%2520to%2520%2524V%2524.%2520The%2520soundness%2520property%2520of%2520%2524V%2524%2520guarantees%2520that%252C%2520for%2520every%2520input%252C%2520no%2520model%2520can%2520convince%2520%2524V%2524%2520of%2520the%2520correctness%2520of%2520an%2520incorrect%2520output.%2520Thus%252C%2520a%2520Self-Proving%2520model%2520proves%2520correctness%2520of%2520most%2520of%2520its%2520outputs%252C%2520while%2520all%2520incorrect%2520outputs%2520%2528of%2520any%2520model%2529%2520are%2520detected%2520by%2520%2524V%2524.%2520We%2520devise%2520and%2520analyze%2520two%2520generic%2520methods%2520for%2520learning%2520Self-Proving%2520models%253A%2520Transcript%2520Learning%2520%2528TL%2529%2520which%2520relies%2520on%2520access%2520to%2520transcripts%2520of%2520accepting%2520interactions%252C%2520and%2520Reinforcement%2520Learning%2520from%2520Verifier%2520Feedback%2520%2528RLVF%2529%2520which%2520trains%2520a%2520model%2520by%2520emulating%2520interactions%2520with%2520the%2520verifier.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15722v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Models%20That%20Prove%20Their%20Own%20Correctness&entry.906535625=Noga%20Amit%20and%20Shafi%20Goldwasser%20and%20Orr%20Paradise%20and%20Guy%20Rothblum&entry.1292438233=How%20can%20we%20trust%20the%20correctness%20of%20a%20learned%20model%20on%20a%20particular%20input%20of%20interest%3F%20Model%20accuracy%20is%20typically%20measured%20on%20average%20over%20a%20distribution%20of%20inputs%2C%20giving%20no%20guarantee%20for%20any%20fixed%20input.%20This%20paper%20proposes%20a%20theoretically-founded%20solution%20to%20this%20problem%3A%20to%20train%20Self-Proving%20models%20that%20prove%20the%20correctness%20of%20their%20output%20to%20a%20verification%20algorithm%20%24V%24%20via%20an%20Interactive%20Proof.%20Self-Proving%20models%20satisfy%20that%2C%20with%20high%20probability%20over%20an%20input%20sampled%20from%20a%20given%20distribution%2C%20the%20model%20generates%20a%20correct%20output%20and%20successfully%20proves%20its%20correctness%20to%20%24V%24.%20The%20soundness%20property%20of%20%24V%24%20guarantees%20that%2C%20for%20every%20input%2C%20no%20model%20can%20convince%20%24V%24%20of%20the%20correctness%20of%20an%20incorrect%20output.%20Thus%2C%20a%20Self-Proving%20model%20proves%20correctness%20of%20most%20of%20its%20outputs%2C%20while%20all%20incorrect%20outputs%20%28of%20any%20model%29%20are%20detected%20by%20%24V%24.%20We%20devise%20and%20analyze%20two%20generic%20methods%20for%20learning%20Self-Proving%20models%3A%20Transcript%20Learning%20%28TL%29%20which%20relies%20on%20access%20to%20transcripts%20of%20accepting%20interactions%2C%20and%20Reinforcement%20Learning%20from%20Verifier%20Feedback%20%28RLVF%29%20which%20trains%20a%20model%20by%20emulating%20interactions%20with%20the%20verifier.&entry.1838667208=http%3A//arxiv.org/abs/2405.15722v4&entry.124074799=Read"},
{"title": "Topic Modelling Black Box Optimization", "author": "Roman Akramov and Artem Khamatullin and Svetlana Glazyrina and Maksim Kryzhanovskiy and Roman Ischenko", "abstract": "Choosing the number of topics $T$ in Latent Dirichlet Allocation (LDA) is a key design decision that strongly affects both the statistical fit and interpretability of topic models. In this work, we formulate the selection of $T$ as a discrete black-box optimization problem, where each function evaluation corresponds to training an LDA model and measuring its validation perplexity. Under a fixed evaluation budget, we compare four families of optimizers: two hand-designed evolutionary methods - Genetic Algorithm (GA) and Evolution Strategy (ES) - and two learned, amortized approaches, Preferential Amortized Black-Box Optimization (PABBO) and Sharpness-Aware Black-Box Optimization (SABBO). Our experiments show that, while GA, ES, PABBO, and SABBO eventually reach a similar band of final perplexity, the amortized optimizers are substantially more sample- and time-efficient. SABBO typically identifies a near-optimal topic number after essentially a single evaluation, and PABBO finds competitive configurations within a few evaluations, whereas GA and ES require almost the full budget to approach the same region.", "link": "http://arxiv.org/abs/2512.16445v1", "date": "2025-12-18", "relevancy": 2.326, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4724}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4616}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topic%20Modelling%20Black%20Box%20Optimization&body=Title%3A%20Topic%20Modelling%20Black%20Box%20Optimization%0AAuthor%3A%20Roman%20Akramov%20and%20Artem%20Khamatullin%20and%20Svetlana%20Glazyrina%20and%20Maksim%20Kryzhanovskiy%20and%20Roman%20Ischenko%0AAbstract%3A%20Choosing%20the%20number%20of%20topics%20%24T%24%20in%20Latent%20Dirichlet%20Allocation%20%28LDA%29%20is%20a%20key%20design%20decision%20that%20strongly%20affects%20both%20the%20statistical%20fit%20and%20interpretability%20of%20topic%20models.%20In%20this%20work%2C%20we%20formulate%20the%20selection%20of%20%24T%24%20as%20a%20discrete%20black-box%20optimization%20problem%2C%20where%20each%20function%20evaluation%20corresponds%20to%20training%20an%20LDA%20model%20and%20measuring%20its%20validation%20perplexity.%20Under%20a%20fixed%20evaluation%20budget%2C%20we%20compare%20four%20families%20of%20optimizers%3A%20two%20hand-designed%20evolutionary%20methods%20-%20Genetic%20Algorithm%20%28GA%29%20and%20Evolution%20Strategy%20%28ES%29%20-%20and%20two%20learned%2C%20amortized%20approaches%2C%20Preferential%20Amortized%20Black-Box%20Optimization%20%28PABBO%29%20and%20Sharpness-Aware%20Black-Box%20Optimization%20%28SABBO%29.%20Our%20experiments%20show%20that%2C%20while%20GA%2C%20ES%2C%20PABBO%2C%20and%20SABBO%20eventually%20reach%20a%20similar%20band%20of%20final%20perplexity%2C%20the%20amortized%20optimizers%20are%20substantially%20more%20sample-%20and%20time-efficient.%20SABBO%20typically%20identifies%20a%20near-optimal%20topic%20number%20after%20essentially%20a%20single%20evaluation%2C%20and%20PABBO%20finds%20competitive%20configurations%20within%20a%20few%20evaluations%2C%20whereas%20GA%20and%20ES%20require%20almost%20the%20full%20budget%20to%20approach%20the%20same%20region.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16445v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopic%2520Modelling%2520Black%2520Box%2520Optimization%26entry.906535625%3DRoman%2520Akramov%2520and%2520Artem%2520Khamatullin%2520and%2520Svetlana%2520Glazyrina%2520and%2520Maksim%2520Kryzhanovskiy%2520and%2520Roman%2520Ischenko%26entry.1292438233%3DChoosing%2520the%2520number%2520of%2520topics%2520%2524T%2524%2520in%2520Latent%2520Dirichlet%2520Allocation%2520%2528LDA%2529%2520is%2520a%2520key%2520design%2520decision%2520that%2520strongly%2520affects%2520both%2520the%2520statistical%2520fit%2520and%2520interpretability%2520of%2520topic%2520models.%2520In%2520this%2520work%252C%2520we%2520formulate%2520the%2520selection%2520of%2520%2524T%2524%2520as%2520a%2520discrete%2520black-box%2520optimization%2520problem%252C%2520where%2520each%2520function%2520evaluation%2520corresponds%2520to%2520training%2520an%2520LDA%2520model%2520and%2520measuring%2520its%2520validation%2520perplexity.%2520Under%2520a%2520fixed%2520evaluation%2520budget%252C%2520we%2520compare%2520four%2520families%2520of%2520optimizers%253A%2520two%2520hand-designed%2520evolutionary%2520methods%2520-%2520Genetic%2520Algorithm%2520%2528GA%2529%2520and%2520Evolution%2520Strategy%2520%2528ES%2529%2520-%2520and%2520two%2520learned%252C%2520amortized%2520approaches%252C%2520Preferential%2520Amortized%2520Black-Box%2520Optimization%2520%2528PABBO%2529%2520and%2520Sharpness-Aware%2520Black-Box%2520Optimization%2520%2528SABBO%2529.%2520Our%2520experiments%2520show%2520that%252C%2520while%2520GA%252C%2520ES%252C%2520PABBO%252C%2520and%2520SABBO%2520eventually%2520reach%2520a%2520similar%2520band%2520of%2520final%2520perplexity%252C%2520the%2520amortized%2520optimizers%2520are%2520substantially%2520more%2520sample-%2520and%2520time-efficient.%2520SABBO%2520typically%2520identifies%2520a%2520near-optimal%2520topic%2520number%2520after%2520essentially%2520a%2520single%2520evaluation%252C%2520and%2520PABBO%2520finds%2520competitive%2520configurations%2520within%2520a%2520few%2520evaluations%252C%2520whereas%2520GA%2520and%2520ES%2520require%2520almost%2520the%2520full%2520budget%2520to%2520approach%2520the%2520same%2520region.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16445v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topic%20Modelling%20Black%20Box%20Optimization&entry.906535625=Roman%20Akramov%20and%20Artem%20Khamatullin%20and%20Svetlana%20Glazyrina%20and%20Maksim%20Kryzhanovskiy%20and%20Roman%20Ischenko&entry.1292438233=Choosing%20the%20number%20of%20topics%20%24T%24%20in%20Latent%20Dirichlet%20Allocation%20%28LDA%29%20is%20a%20key%20design%20decision%20that%20strongly%20affects%20both%20the%20statistical%20fit%20and%20interpretability%20of%20topic%20models.%20In%20this%20work%2C%20we%20formulate%20the%20selection%20of%20%24T%24%20as%20a%20discrete%20black-box%20optimization%20problem%2C%20where%20each%20function%20evaluation%20corresponds%20to%20training%20an%20LDA%20model%20and%20measuring%20its%20validation%20perplexity.%20Under%20a%20fixed%20evaluation%20budget%2C%20we%20compare%20four%20families%20of%20optimizers%3A%20two%20hand-designed%20evolutionary%20methods%20-%20Genetic%20Algorithm%20%28GA%29%20and%20Evolution%20Strategy%20%28ES%29%20-%20and%20two%20learned%2C%20amortized%20approaches%2C%20Preferential%20Amortized%20Black-Box%20Optimization%20%28PABBO%29%20and%20Sharpness-Aware%20Black-Box%20Optimization%20%28SABBO%29.%20Our%20experiments%20show%20that%2C%20while%20GA%2C%20ES%2C%20PABBO%2C%20and%20SABBO%20eventually%20reach%20a%20similar%20band%20of%20final%20perplexity%2C%20the%20amortized%20optimizers%20are%20substantially%20more%20sample-%20and%20time-efficient.%20SABBO%20typically%20identifies%20a%20near-optimal%20topic%20number%20after%20essentially%20a%20single%20evaluation%2C%20and%20PABBO%20finds%20competitive%20configurations%20within%20a%20few%20evaluations%2C%20whereas%20GA%20and%20ES%20require%20almost%20the%20full%20budget%20to%20approach%20the%20same%20region.&entry.1838667208=http%3A//arxiv.org/abs/2512.16445v1&entry.124074799=Read"},
{"title": "R3ST: A Synthetic 3D Dataset With Realistic Trajectories", "author": "Simone Teglia and Claudia Melis Tonti and Francesco Pro and Leonardo Russo and Andrea Alfarano and Leonardo Pentassuglia and Irene Amerini", "abstract": "Datasets are essential to train and evaluate computer vision models used for traffic analysis and to enhance road safety. Existing real datasets fit real-world scenarios, capturing authentic road object behaviors, however, they typically lack precise ground-truth annotations. In contrast, synthetic datasets play a crucial role, allowing for the annotation of a large number of frames without additional costs or extra time. However, a general drawback of synthetic datasets is the lack of realistic vehicle motion, since trajectories are generated using AI models or rule-based systems. In this work, we introduce R3ST (Realistic 3D Synthetic Trajectories), a synthetic dataset that overcomes this limitation by generating a synthetic 3D environment and integrating real-world trajectories derived from SinD, a bird's-eye-view dataset recorded from drone footage. The proposed dataset closes the gap between synthetic data and realistic trajectories, advancing the research in trajectory forecasting of road vehicles, offering both accurate multimodal ground-truth annotations and authentic human-driven vehicle trajectories.", "link": "http://arxiv.org/abs/2512.16784v1", "date": "2025-12-18", "relevancy": 2.3171, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5943}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5754}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20R3ST%3A%20A%20Synthetic%203D%20Dataset%20With%20Realistic%20Trajectories&body=Title%3A%20R3ST%3A%20A%20Synthetic%203D%20Dataset%20With%20Realistic%20Trajectories%0AAuthor%3A%20Simone%20Teglia%20and%20Claudia%20Melis%20Tonti%20and%20Francesco%20Pro%20and%20Leonardo%20Russo%20and%20Andrea%20Alfarano%20and%20Leonardo%20Pentassuglia%20and%20Irene%20Amerini%0AAbstract%3A%20Datasets%20are%20essential%20to%20train%20and%20evaluate%20computer%20vision%20models%20used%20for%20traffic%20analysis%20and%20to%20enhance%20road%20safety.%20Existing%20real%20datasets%20fit%20real-world%20scenarios%2C%20capturing%20authentic%20road%20object%20behaviors%2C%20however%2C%20they%20typically%20lack%20precise%20ground-truth%20annotations.%20In%20contrast%2C%20synthetic%20datasets%20play%20a%20crucial%20role%2C%20allowing%20for%20the%20annotation%20of%20a%20large%20number%20of%20frames%20without%20additional%20costs%20or%20extra%20time.%20However%2C%20a%20general%20drawback%20of%20synthetic%20datasets%20is%20the%20lack%20of%20realistic%20vehicle%20motion%2C%20since%20trajectories%20are%20generated%20using%20AI%20models%20or%20rule-based%20systems.%20In%20this%20work%2C%20we%20introduce%20R3ST%20%28Realistic%203D%20Synthetic%20Trajectories%29%2C%20a%20synthetic%20dataset%20that%20overcomes%20this%20limitation%20by%20generating%20a%20synthetic%203D%20environment%20and%20integrating%20real-world%20trajectories%20derived%20from%20SinD%2C%20a%20bird%27s-eye-view%20dataset%20recorded%20from%20drone%20footage.%20The%20proposed%20dataset%20closes%20the%20gap%20between%20synthetic%20data%20and%20realistic%20trajectories%2C%20advancing%20the%20research%20in%20trajectory%20forecasting%20of%20road%20vehicles%2C%20offering%20both%20accurate%20multimodal%20ground-truth%20annotations%20and%20authentic%20human-driven%20vehicle%20trajectories.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DR3ST%253A%2520A%2520Synthetic%25203D%2520Dataset%2520With%2520Realistic%2520Trajectories%26entry.906535625%3DSimone%2520Teglia%2520and%2520Claudia%2520Melis%2520Tonti%2520and%2520Francesco%2520Pro%2520and%2520Leonardo%2520Russo%2520and%2520Andrea%2520Alfarano%2520and%2520Leonardo%2520Pentassuglia%2520and%2520Irene%2520Amerini%26entry.1292438233%3DDatasets%2520are%2520essential%2520to%2520train%2520and%2520evaluate%2520computer%2520vision%2520models%2520used%2520for%2520traffic%2520analysis%2520and%2520to%2520enhance%2520road%2520safety.%2520Existing%2520real%2520datasets%2520fit%2520real-world%2520scenarios%252C%2520capturing%2520authentic%2520road%2520object%2520behaviors%252C%2520however%252C%2520they%2520typically%2520lack%2520precise%2520ground-truth%2520annotations.%2520In%2520contrast%252C%2520synthetic%2520datasets%2520play%2520a%2520crucial%2520role%252C%2520allowing%2520for%2520the%2520annotation%2520of%2520a%2520large%2520number%2520of%2520frames%2520without%2520additional%2520costs%2520or%2520extra%2520time.%2520However%252C%2520a%2520general%2520drawback%2520of%2520synthetic%2520datasets%2520is%2520the%2520lack%2520of%2520realistic%2520vehicle%2520motion%252C%2520since%2520trajectories%2520are%2520generated%2520using%2520AI%2520models%2520or%2520rule-based%2520systems.%2520In%2520this%2520work%252C%2520we%2520introduce%2520R3ST%2520%2528Realistic%25203D%2520Synthetic%2520Trajectories%2529%252C%2520a%2520synthetic%2520dataset%2520that%2520overcomes%2520this%2520limitation%2520by%2520generating%2520a%2520synthetic%25203D%2520environment%2520and%2520integrating%2520real-world%2520trajectories%2520derived%2520from%2520SinD%252C%2520a%2520bird%2527s-eye-view%2520dataset%2520recorded%2520from%2520drone%2520footage.%2520The%2520proposed%2520dataset%2520closes%2520the%2520gap%2520between%2520synthetic%2520data%2520and%2520realistic%2520trajectories%252C%2520advancing%2520the%2520research%2520in%2520trajectory%2520forecasting%2520of%2520road%2520vehicles%252C%2520offering%2520both%2520accurate%2520multimodal%2520ground-truth%2520annotations%2520and%2520authentic%2520human-driven%2520vehicle%2520trajectories.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=R3ST%3A%20A%20Synthetic%203D%20Dataset%20With%20Realistic%20Trajectories&entry.906535625=Simone%20Teglia%20and%20Claudia%20Melis%20Tonti%20and%20Francesco%20Pro%20and%20Leonardo%20Russo%20and%20Andrea%20Alfarano%20and%20Leonardo%20Pentassuglia%20and%20Irene%20Amerini&entry.1292438233=Datasets%20are%20essential%20to%20train%20and%20evaluate%20computer%20vision%20models%20used%20for%20traffic%20analysis%20and%20to%20enhance%20road%20safety.%20Existing%20real%20datasets%20fit%20real-world%20scenarios%2C%20capturing%20authentic%20road%20object%20behaviors%2C%20however%2C%20they%20typically%20lack%20precise%20ground-truth%20annotations.%20In%20contrast%2C%20synthetic%20datasets%20play%20a%20crucial%20role%2C%20allowing%20for%20the%20annotation%20of%20a%20large%20number%20of%20frames%20without%20additional%20costs%20or%20extra%20time.%20However%2C%20a%20general%20drawback%20of%20synthetic%20datasets%20is%20the%20lack%20of%20realistic%20vehicle%20motion%2C%20since%20trajectories%20are%20generated%20using%20AI%20models%20or%20rule-based%20systems.%20In%20this%20work%2C%20we%20introduce%20R3ST%20%28Realistic%203D%20Synthetic%20Trajectories%29%2C%20a%20synthetic%20dataset%20that%20overcomes%20this%20limitation%20by%20generating%20a%20synthetic%203D%20environment%20and%20integrating%20real-world%20trajectories%20derived%20from%20SinD%2C%20a%20bird%27s-eye-view%20dataset%20recorded%20from%20drone%20footage.%20The%20proposed%20dataset%20closes%20the%20gap%20between%20synthetic%20data%20and%20realistic%20trajectories%2C%20advancing%20the%20research%20in%20trajectory%20forecasting%20of%20road%20vehicles%2C%20offering%20both%20accurate%20multimodal%20ground-truth%20annotations%20and%20authentic%20human-driven%20vehicle%20trajectories.&entry.1838667208=http%3A//arxiv.org/abs/2512.16784v1&entry.124074799=Read"},
{"title": "PoseMoE: Mixture-of-Experts Network for Monocular 3D Human Pose Estimation", "author": "Mengyuan Liu and Jiajie Liu and Jinyan Zhang and Wenhao Li and Junsong Yuan", "abstract": "The lifting-based methods have dominated monocular 3D human pose estimation by leveraging detected 2D poses as intermediate representations. The 2D component of the final 3D human pose benefits from the detected 2D poses, whereas its depth counterpart must be estimated from scratch. The lifting-based methods encode the detected 2D pose and unknown depth in an entangled feature space, explicitly introducing depth uncertainty to the detected 2D pose, thereby limiting overall estimation accuracy. This work reveals that the depth representation is pivotal for the estimation process. Specifically, when depth is in an initial, completely unknown state, jointly encoding depth features with 2D pose features is detrimental to the estimation process. In contrast, when depth is initially refined to a more dependable state via network-based estimation, encoding it together with 2D pose information is beneficial. To address this limitation, we present a Mixture-of-Experts network for monocular 3D pose estimation named PoseMoE. Our approach introduces: (1) A mixture-of-experts network where specialized expert modules refine the well-detected 2D pose features and learn the depth features. This mixture-of-experts design disentangles the feature encoding process for 2D pose and depth, therefore reducing the explicit influence of uncertain depth features on 2D pose features. (2) A cross-expert knowledge aggregation module is proposed to aggregate cross-expert spatio-temporal contextual information. This step enhances features through bidirectional mapping between 2D pose and depth. Extensive experiments show that our proposed PoseMoE outperforms the conventional lifting-based methods on three widely used datasets: Human3.6M, MPI-INF-3DHP, and 3DPW.", "link": "http://arxiv.org/abs/2512.16494v1", "date": "2025-12-18", "relevancy": 2.3128, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5818}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5817}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PoseMoE%3A%20Mixture-of-Experts%20Network%20for%20Monocular%203D%20Human%20Pose%20Estimation&body=Title%3A%20PoseMoE%3A%20Mixture-of-Experts%20Network%20for%20Monocular%203D%20Human%20Pose%20Estimation%0AAuthor%3A%20Mengyuan%20Liu%20and%20Jiajie%20Liu%20and%20Jinyan%20Zhang%20and%20Wenhao%20Li%20and%20Junsong%20Yuan%0AAbstract%3A%20The%20lifting-based%20methods%20have%20dominated%20monocular%203D%20human%20pose%20estimation%20by%20leveraging%20detected%202D%20poses%20as%20intermediate%20representations.%20The%202D%20component%20of%20the%20final%203D%20human%20pose%20benefits%20from%20the%20detected%202D%20poses%2C%20whereas%20its%20depth%20counterpart%20must%20be%20estimated%20from%20scratch.%20The%20lifting-based%20methods%20encode%20the%20detected%202D%20pose%20and%20unknown%20depth%20in%20an%20entangled%20feature%20space%2C%20explicitly%20introducing%20depth%20uncertainty%20to%20the%20detected%202D%20pose%2C%20thereby%20limiting%20overall%20estimation%20accuracy.%20This%20work%20reveals%20that%20the%20depth%20representation%20is%20pivotal%20for%20the%20estimation%20process.%20Specifically%2C%20when%20depth%20is%20in%20an%20initial%2C%20completely%20unknown%20state%2C%20jointly%20encoding%20depth%20features%20with%202D%20pose%20features%20is%20detrimental%20to%20the%20estimation%20process.%20In%20contrast%2C%20when%20depth%20is%20initially%20refined%20to%20a%20more%20dependable%20state%20via%20network-based%20estimation%2C%20encoding%20it%20together%20with%202D%20pose%20information%20is%20beneficial.%20To%20address%20this%20limitation%2C%20we%20present%20a%20Mixture-of-Experts%20network%20for%20monocular%203D%20pose%20estimation%20named%20PoseMoE.%20Our%20approach%20introduces%3A%20%281%29%20A%20mixture-of-experts%20network%20where%20specialized%20expert%20modules%20refine%20the%20well-detected%202D%20pose%20features%20and%20learn%20the%20depth%20features.%20This%20mixture-of-experts%20design%20disentangles%20the%20feature%20encoding%20process%20for%202D%20pose%20and%20depth%2C%20therefore%20reducing%20the%20explicit%20influence%20of%20uncertain%20depth%20features%20on%202D%20pose%20features.%20%282%29%20A%20cross-expert%20knowledge%20aggregation%20module%20is%20proposed%20to%20aggregate%20cross-expert%20spatio-temporal%20contextual%20information.%20This%20step%20enhances%20features%20through%20bidirectional%20mapping%20between%202D%20pose%20and%20depth.%20Extensive%20experiments%20show%20that%20our%20proposed%20PoseMoE%20outperforms%20the%20conventional%20lifting-based%20methods%20on%20three%20widely%20used%20datasets%3A%20Human3.6M%2C%20MPI-INF-3DHP%2C%20and%203DPW.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16494v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoseMoE%253A%2520Mixture-of-Experts%2520Network%2520for%2520Monocular%25203D%2520Human%2520Pose%2520Estimation%26entry.906535625%3DMengyuan%2520Liu%2520and%2520Jiajie%2520Liu%2520and%2520Jinyan%2520Zhang%2520and%2520Wenhao%2520Li%2520and%2520Junsong%2520Yuan%26entry.1292438233%3DThe%2520lifting-based%2520methods%2520have%2520dominated%2520monocular%25203D%2520human%2520pose%2520estimation%2520by%2520leveraging%2520detected%25202D%2520poses%2520as%2520intermediate%2520representations.%2520The%25202D%2520component%2520of%2520the%2520final%25203D%2520human%2520pose%2520benefits%2520from%2520the%2520detected%25202D%2520poses%252C%2520whereas%2520its%2520depth%2520counterpart%2520must%2520be%2520estimated%2520from%2520scratch.%2520The%2520lifting-based%2520methods%2520encode%2520the%2520detected%25202D%2520pose%2520and%2520unknown%2520depth%2520in%2520an%2520entangled%2520feature%2520space%252C%2520explicitly%2520introducing%2520depth%2520uncertainty%2520to%2520the%2520detected%25202D%2520pose%252C%2520thereby%2520limiting%2520overall%2520estimation%2520accuracy.%2520This%2520work%2520reveals%2520that%2520the%2520depth%2520representation%2520is%2520pivotal%2520for%2520the%2520estimation%2520process.%2520Specifically%252C%2520when%2520depth%2520is%2520in%2520an%2520initial%252C%2520completely%2520unknown%2520state%252C%2520jointly%2520encoding%2520depth%2520features%2520with%25202D%2520pose%2520features%2520is%2520detrimental%2520to%2520the%2520estimation%2520process.%2520In%2520contrast%252C%2520when%2520depth%2520is%2520initially%2520refined%2520to%2520a%2520more%2520dependable%2520state%2520via%2520network-based%2520estimation%252C%2520encoding%2520it%2520together%2520with%25202D%2520pose%2520information%2520is%2520beneficial.%2520To%2520address%2520this%2520limitation%252C%2520we%2520present%2520a%2520Mixture-of-Experts%2520network%2520for%2520monocular%25203D%2520pose%2520estimation%2520named%2520PoseMoE.%2520Our%2520approach%2520introduces%253A%2520%25281%2529%2520A%2520mixture-of-experts%2520network%2520where%2520specialized%2520expert%2520modules%2520refine%2520the%2520well-detected%25202D%2520pose%2520features%2520and%2520learn%2520the%2520depth%2520features.%2520This%2520mixture-of-experts%2520design%2520disentangles%2520the%2520feature%2520encoding%2520process%2520for%25202D%2520pose%2520and%2520depth%252C%2520therefore%2520reducing%2520the%2520explicit%2520influence%2520of%2520uncertain%2520depth%2520features%2520on%25202D%2520pose%2520features.%2520%25282%2529%2520A%2520cross-expert%2520knowledge%2520aggregation%2520module%2520is%2520proposed%2520to%2520aggregate%2520cross-expert%2520spatio-temporal%2520contextual%2520information.%2520This%2520step%2520enhances%2520features%2520through%2520bidirectional%2520mapping%2520between%25202D%2520pose%2520and%2520depth.%2520Extensive%2520experiments%2520show%2520that%2520our%2520proposed%2520PoseMoE%2520outperforms%2520the%2520conventional%2520lifting-based%2520methods%2520on%2520three%2520widely%2520used%2520datasets%253A%2520Human3.6M%252C%2520MPI-INF-3DHP%252C%2520and%25203DPW.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16494v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PoseMoE%3A%20Mixture-of-Experts%20Network%20for%20Monocular%203D%20Human%20Pose%20Estimation&entry.906535625=Mengyuan%20Liu%20and%20Jiajie%20Liu%20and%20Jinyan%20Zhang%20and%20Wenhao%20Li%20and%20Junsong%20Yuan&entry.1292438233=The%20lifting-based%20methods%20have%20dominated%20monocular%203D%20human%20pose%20estimation%20by%20leveraging%20detected%202D%20poses%20as%20intermediate%20representations.%20The%202D%20component%20of%20the%20final%203D%20human%20pose%20benefits%20from%20the%20detected%202D%20poses%2C%20whereas%20its%20depth%20counterpart%20must%20be%20estimated%20from%20scratch.%20The%20lifting-based%20methods%20encode%20the%20detected%202D%20pose%20and%20unknown%20depth%20in%20an%20entangled%20feature%20space%2C%20explicitly%20introducing%20depth%20uncertainty%20to%20the%20detected%202D%20pose%2C%20thereby%20limiting%20overall%20estimation%20accuracy.%20This%20work%20reveals%20that%20the%20depth%20representation%20is%20pivotal%20for%20the%20estimation%20process.%20Specifically%2C%20when%20depth%20is%20in%20an%20initial%2C%20completely%20unknown%20state%2C%20jointly%20encoding%20depth%20features%20with%202D%20pose%20features%20is%20detrimental%20to%20the%20estimation%20process.%20In%20contrast%2C%20when%20depth%20is%20initially%20refined%20to%20a%20more%20dependable%20state%20via%20network-based%20estimation%2C%20encoding%20it%20together%20with%202D%20pose%20information%20is%20beneficial.%20To%20address%20this%20limitation%2C%20we%20present%20a%20Mixture-of-Experts%20network%20for%20monocular%203D%20pose%20estimation%20named%20PoseMoE.%20Our%20approach%20introduces%3A%20%281%29%20A%20mixture-of-experts%20network%20where%20specialized%20expert%20modules%20refine%20the%20well-detected%202D%20pose%20features%20and%20learn%20the%20depth%20features.%20This%20mixture-of-experts%20design%20disentangles%20the%20feature%20encoding%20process%20for%202D%20pose%20and%20depth%2C%20therefore%20reducing%20the%20explicit%20influence%20of%20uncertain%20depth%20features%20on%202D%20pose%20features.%20%282%29%20A%20cross-expert%20knowledge%20aggregation%20module%20is%20proposed%20to%20aggregate%20cross-expert%20spatio-temporal%20contextual%20information.%20This%20step%20enhances%20features%20through%20bidirectional%20mapping%20between%202D%20pose%20and%20depth.%20Extensive%20experiments%20show%20that%20our%20proposed%20PoseMoE%20outperforms%20the%20conventional%20lifting-based%20methods%20on%20three%20widely%20used%20datasets%3A%20Human3.6M%2C%20MPI-INF-3DHP%2C%20and%203DPW.&entry.1838667208=http%3A//arxiv.org/abs/2512.16494v1&entry.124074799=Read"},
{"title": "Cartesian-nj: Extending e3nn to Irreducible Cartesian Tensor Product and Contracion", "author": "Zemin Xu and Chenyu Wu and Wenbo Xie and Daiqian Xie and P. Hu", "abstract": "Equivariant atomistic machine learning models have brought substantial gains in both extrapolation capability and predictive accuracy. Depending on the basis of the space, two distinct types of irreducible representations are utilized. From architectures built upon spherical tensors (STs) to more recent formulations employing irreducible Cartesian tensors (ICTs), STs have remained dominant owing to their compactness, elegance, and theoretical completeness. Nevertheless, questions have persisted regarding whether ST constructions are the only viable design principle, motivating continued development of Cartesian networks. In this work, we introduce the Cartesian-3j and Cartesian-nj symbol, which serve as direct analogues of the Wigner-3j and Wigner-nj symbol defined for tensor coupling. These coefficients enable the combination of any two ICTs into a new ICT. Building on this foundation, we extend e3nn to support irreducible Cartesian tensor product, and we release the resulting Python package as cartnn. Within this framework, we implement Cartesian counterparts of MACE, NequIP, and Allegro, allowing the first systematic comparison of Cartesian and spherical models to assess whether Cartesian formulations may offer advantages under specific conditions. Using TACE as a representative example, we further examine whether architectures constructed from irreducible Cartesian tensor product and contraction(ICTP and ICTC) are conceptually well-founded in Cartesian space and whether opportunities remain for improving their design.", "link": "http://arxiv.org/abs/2512.16882v1", "date": "2025-12-18", "relevancy": 2.3048, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4741}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4544}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cartesian-nj%3A%20Extending%20e3nn%20to%20Irreducible%20Cartesian%20Tensor%20Product%20and%20Contracion&body=Title%3A%20Cartesian-nj%3A%20Extending%20e3nn%20to%20Irreducible%20Cartesian%20Tensor%20Product%20and%20Contracion%0AAuthor%3A%20Zemin%20Xu%20and%20Chenyu%20Wu%20and%20Wenbo%20Xie%20and%20Daiqian%20Xie%20and%20P.%20Hu%0AAbstract%3A%20Equivariant%20atomistic%20machine%20learning%20models%20have%20brought%20substantial%20gains%20in%20both%20extrapolation%20capability%20and%20predictive%20accuracy.%20Depending%20on%20the%20basis%20of%20the%20space%2C%20two%20distinct%20types%20of%20irreducible%20representations%20are%20utilized.%20From%20architectures%20built%20upon%20spherical%20tensors%20%28STs%29%20to%20more%20recent%20formulations%20employing%20irreducible%20Cartesian%20tensors%20%28ICTs%29%2C%20STs%20have%20remained%20dominant%20owing%20to%20their%20compactness%2C%20elegance%2C%20and%20theoretical%20completeness.%20Nevertheless%2C%20questions%20have%20persisted%20regarding%20whether%20ST%20constructions%20are%20the%20only%20viable%20design%20principle%2C%20motivating%20continued%20development%20of%20Cartesian%20networks.%20In%20this%20work%2C%20we%20introduce%20the%20Cartesian-3j%20and%20Cartesian-nj%20symbol%2C%20which%20serve%20as%20direct%20analogues%20of%20the%20Wigner-3j%20and%20Wigner-nj%20symbol%20defined%20for%20tensor%20coupling.%20These%20coefficients%20enable%20the%20combination%20of%20any%20two%20ICTs%20into%20a%20new%20ICT.%20Building%20on%20this%20foundation%2C%20we%20extend%20e3nn%20to%20support%20irreducible%20Cartesian%20tensor%20product%2C%20and%20we%20release%20the%20resulting%20Python%20package%20as%20cartnn.%20Within%20this%20framework%2C%20we%20implement%20Cartesian%20counterparts%20of%20MACE%2C%20NequIP%2C%20and%20Allegro%2C%20allowing%20the%20first%20systematic%20comparison%20of%20Cartesian%20and%20spherical%20models%20to%20assess%20whether%20Cartesian%20formulations%20may%20offer%20advantages%20under%20specific%20conditions.%20Using%20TACE%20as%20a%20representative%20example%2C%20we%20further%20examine%20whether%20architectures%20constructed%20from%20irreducible%20Cartesian%20tensor%20product%20and%20contraction%28ICTP%20and%20ICTC%29%20are%20conceptually%20well-founded%20in%20Cartesian%20space%20and%20whether%20opportunities%20remain%20for%20improving%20their%20design.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16882v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCartesian-nj%253A%2520Extending%2520e3nn%2520to%2520Irreducible%2520Cartesian%2520Tensor%2520Product%2520and%2520Contracion%26entry.906535625%3DZemin%2520Xu%2520and%2520Chenyu%2520Wu%2520and%2520Wenbo%2520Xie%2520and%2520Daiqian%2520Xie%2520and%2520P.%2520Hu%26entry.1292438233%3DEquivariant%2520atomistic%2520machine%2520learning%2520models%2520have%2520brought%2520substantial%2520gains%2520in%2520both%2520extrapolation%2520capability%2520and%2520predictive%2520accuracy.%2520Depending%2520on%2520the%2520basis%2520of%2520the%2520space%252C%2520two%2520distinct%2520types%2520of%2520irreducible%2520representations%2520are%2520utilized.%2520From%2520architectures%2520built%2520upon%2520spherical%2520tensors%2520%2528STs%2529%2520to%2520more%2520recent%2520formulations%2520employing%2520irreducible%2520Cartesian%2520tensors%2520%2528ICTs%2529%252C%2520STs%2520have%2520remained%2520dominant%2520owing%2520to%2520their%2520compactness%252C%2520elegance%252C%2520and%2520theoretical%2520completeness.%2520Nevertheless%252C%2520questions%2520have%2520persisted%2520regarding%2520whether%2520ST%2520constructions%2520are%2520the%2520only%2520viable%2520design%2520principle%252C%2520motivating%2520continued%2520development%2520of%2520Cartesian%2520networks.%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520Cartesian-3j%2520and%2520Cartesian-nj%2520symbol%252C%2520which%2520serve%2520as%2520direct%2520analogues%2520of%2520the%2520Wigner-3j%2520and%2520Wigner-nj%2520symbol%2520defined%2520for%2520tensor%2520coupling.%2520These%2520coefficients%2520enable%2520the%2520combination%2520of%2520any%2520two%2520ICTs%2520into%2520a%2520new%2520ICT.%2520Building%2520on%2520this%2520foundation%252C%2520we%2520extend%2520e3nn%2520to%2520support%2520irreducible%2520Cartesian%2520tensor%2520product%252C%2520and%2520we%2520release%2520the%2520resulting%2520Python%2520package%2520as%2520cartnn.%2520Within%2520this%2520framework%252C%2520we%2520implement%2520Cartesian%2520counterparts%2520of%2520MACE%252C%2520NequIP%252C%2520and%2520Allegro%252C%2520allowing%2520the%2520first%2520systematic%2520comparison%2520of%2520Cartesian%2520and%2520spherical%2520models%2520to%2520assess%2520whether%2520Cartesian%2520formulations%2520may%2520offer%2520advantages%2520under%2520specific%2520conditions.%2520Using%2520TACE%2520as%2520a%2520representative%2520example%252C%2520we%2520further%2520examine%2520whether%2520architectures%2520constructed%2520from%2520irreducible%2520Cartesian%2520tensor%2520product%2520and%2520contraction%2528ICTP%2520and%2520ICTC%2529%2520are%2520conceptually%2520well-founded%2520in%2520Cartesian%2520space%2520and%2520whether%2520opportunities%2520remain%2520for%2520improving%2520their%2520design.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16882v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cartesian-nj%3A%20Extending%20e3nn%20to%20Irreducible%20Cartesian%20Tensor%20Product%20and%20Contracion&entry.906535625=Zemin%20Xu%20and%20Chenyu%20Wu%20and%20Wenbo%20Xie%20and%20Daiqian%20Xie%20and%20P.%20Hu&entry.1292438233=Equivariant%20atomistic%20machine%20learning%20models%20have%20brought%20substantial%20gains%20in%20both%20extrapolation%20capability%20and%20predictive%20accuracy.%20Depending%20on%20the%20basis%20of%20the%20space%2C%20two%20distinct%20types%20of%20irreducible%20representations%20are%20utilized.%20From%20architectures%20built%20upon%20spherical%20tensors%20%28STs%29%20to%20more%20recent%20formulations%20employing%20irreducible%20Cartesian%20tensors%20%28ICTs%29%2C%20STs%20have%20remained%20dominant%20owing%20to%20their%20compactness%2C%20elegance%2C%20and%20theoretical%20completeness.%20Nevertheless%2C%20questions%20have%20persisted%20regarding%20whether%20ST%20constructions%20are%20the%20only%20viable%20design%20principle%2C%20motivating%20continued%20development%20of%20Cartesian%20networks.%20In%20this%20work%2C%20we%20introduce%20the%20Cartesian-3j%20and%20Cartesian-nj%20symbol%2C%20which%20serve%20as%20direct%20analogues%20of%20the%20Wigner-3j%20and%20Wigner-nj%20symbol%20defined%20for%20tensor%20coupling.%20These%20coefficients%20enable%20the%20combination%20of%20any%20two%20ICTs%20into%20a%20new%20ICT.%20Building%20on%20this%20foundation%2C%20we%20extend%20e3nn%20to%20support%20irreducible%20Cartesian%20tensor%20product%2C%20and%20we%20release%20the%20resulting%20Python%20package%20as%20cartnn.%20Within%20this%20framework%2C%20we%20implement%20Cartesian%20counterparts%20of%20MACE%2C%20NequIP%2C%20and%20Allegro%2C%20allowing%20the%20first%20systematic%20comparison%20of%20Cartesian%20and%20spherical%20models%20to%20assess%20whether%20Cartesian%20formulations%20may%20offer%20advantages%20under%20specific%20conditions.%20Using%20TACE%20as%20a%20representative%20example%2C%20we%20further%20examine%20whether%20architectures%20constructed%20from%20irreducible%20Cartesian%20tensor%20product%20and%20contraction%28ICTP%20and%20ICTC%29%20are%20conceptually%20well-founded%20in%20Cartesian%20space%20and%20whether%20opportunities%20remain%20for%20improving%20their%20design.&entry.1838667208=http%3A//arxiv.org/abs/2512.16882v1&entry.124074799=Read"},
{"title": "VLG-Loc: Vision-Language Global Localization from Labeled Footprint Maps", "author": "Mizuho Aoki and Kohei Honda and Yasuhiro Yoshimura and Takeshi Ishita and Ryo Yonetani", "abstract": "This paper presents Vision-Language Global Localization (VLG-Loc), a novel global localization method that uses human-readable labeled footprint maps containing only names and areas of distinctive visual landmarks in an environment. While humans naturally localize themselves using such maps, translating this capability to robotic systems remains highly challenging due to the difficulty of establishing correspondences between observed landmarks and those in the map without geometric and appearance details. To address this challenge, VLG-Loc leverages a vision-language model (VLM) to search the robot's multi-directional image observations for the landmarks noted in the map. The method then identifies robot poses within a Monte Carlo localization framework, where the found landmarks are used to evaluate the likelihood of each pose hypothesis. Experimental validation in simulated and real-world retail environments demonstrates superior robustness compared to existing scan-based methods, particularly under environmental changes. Further improvements are achieved through the probabilistic fusion of visual and scan-based localization.", "link": "http://arxiv.org/abs/2512.12793v2", "date": "2025-12-18", "relevancy": 2.3044, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5995}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5675}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLG-Loc%3A%20Vision-Language%20Global%20Localization%20from%20Labeled%20Footprint%20Maps&body=Title%3A%20VLG-Loc%3A%20Vision-Language%20Global%20Localization%20from%20Labeled%20Footprint%20Maps%0AAuthor%3A%20Mizuho%20Aoki%20and%20Kohei%20Honda%20and%20Yasuhiro%20Yoshimura%20and%20Takeshi%20Ishita%20and%20Ryo%20Yonetani%0AAbstract%3A%20This%20paper%20presents%20Vision-Language%20Global%20Localization%20%28VLG-Loc%29%2C%20a%20novel%20global%20localization%20method%20that%20uses%20human-readable%20labeled%20footprint%20maps%20containing%20only%20names%20and%20areas%20of%20distinctive%20visual%20landmarks%20in%20an%20environment.%20While%20humans%20naturally%20localize%20themselves%20using%20such%20maps%2C%20translating%20this%20capability%20to%20robotic%20systems%20remains%20highly%20challenging%20due%20to%20the%20difficulty%20of%20establishing%20correspondences%20between%20observed%20landmarks%20and%20those%20in%20the%20map%20without%20geometric%20and%20appearance%20details.%20To%20address%20this%20challenge%2C%20VLG-Loc%20leverages%20a%20vision-language%20model%20%28VLM%29%20to%20search%20the%20robot%27s%20multi-directional%20image%20observations%20for%20the%20landmarks%20noted%20in%20the%20map.%20The%20method%20then%20identifies%20robot%20poses%20within%20a%20Monte%20Carlo%20localization%20framework%2C%20where%20the%20found%20landmarks%20are%20used%20to%20evaluate%20the%20likelihood%20of%20each%20pose%20hypothesis.%20Experimental%20validation%20in%20simulated%20and%20real-world%20retail%20environments%20demonstrates%20superior%20robustness%20compared%20to%20existing%20scan-based%20methods%2C%20particularly%20under%20environmental%20changes.%20Further%20improvements%20are%20achieved%20through%20the%20probabilistic%20fusion%20of%20visual%20and%20scan-based%20localization.%0ALink%3A%20http%3A//arxiv.org/abs/2512.12793v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLG-Loc%253A%2520Vision-Language%2520Global%2520Localization%2520from%2520Labeled%2520Footprint%2520Maps%26entry.906535625%3DMizuho%2520Aoki%2520and%2520Kohei%2520Honda%2520and%2520Yasuhiro%2520Yoshimura%2520and%2520Takeshi%2520Ishita%2520and%2520Ryo%2520Yonetani%26entry.1292438233%3DThis%2520paper%2520presents%2520Vision-Language%2520Global%2520Localization%2520%2528VLG-Loc%2529%252C%2520a%2520novel%2520global%2520localization%2520method%2520that%2520uses%2520human-readable%2520labeled%2520footprint%2520maps%2520containing%2520only%2520names%2520and%2520areas%2520of%2520distinctive%2520visual%2520landmarks%2520in%2520an%2520environment.%2520While%2520humans%2520naturally%2520localize%2520themselves%2520using%2520such%2520maps%252C%2520translating%2520this%2520capability%2520to%2520robotic%2520systems%2520remains%2520highly%2520challenging%2520due%2520to%2520the%2520difficulty%2520of%2520establishing%2520correspondences%2520between%2520observed%2520landmarks%2520and%2520those%2520in%2520the%2520map%2520without%2520geometric%2520and%2520appearance%2520details.%2520To%2520address%2520this%2520challenge%252C%2520VLG-Loc%2520leverages%2520a%2520vision-language%2520model%2520%2528VLM%2529%2520to%2520search%2520the%2520robot%2527s%2520multi-directional%2520image%2520observations%2520for%2520the%2520landmarks%2520noted%2520in%2520the%2520map.%2520The%2520method%2520then%2520identifies%2520robot%2520poses%2520within%2520a%2520Monte%2520Carlo%2520localization%2520framework%252C%2520where%2520the%2520found%2520landmarks%2520are%2520used%2520to%2520evaluate%2520the%2520likelihood%2520of%2520each%2520pose%2520hypothesis.%2520Experimental%2520validation%2520in%2520simulated%2520and%2520real-world%2520retail%2520environments%2520demonstrates%2520superior%2520robustness%2520compared%2520to%2520existing%2520scan-based%2520methods%252C%2520particularly%2520under%2520environmental%2520changes.%2520Further%2520improvements%2520are%2520achieved%2520through%2520the%2520probabilistic%2520fusion%2520of%2520visual%2520and%2520scan-based%2520localization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.12793v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLG-Loc%3A%20Vision-Language%20Global%20Localization%20from%20Labeled%20Footprint%20Maps&entry.906535625=Mizuho%20Aoki%20and%20Kohei%20Honda%20and%20Yasuhiro%20Yoshimura%20and%20Takeshi%20Ishita%20and%20Ryo%20Yonetani&entry.1292438233=This%20paper%20presents%20Vision-Language%20Global%20Localization%20%28VLG-Loc%29%2C%20a%20novel%20global%20localization%20method%20that%20uses%20human-readable%20labeled%20footprint%20maps%20containing%20only%20names%20and%20areas%20of%20distinctive%20visual%20landmarks%20in%20an%20environment.%20While%20humans%20naturally%20localize%20themselves%20using%20such%20maps%2C%20translating%20this%20capability%20to%20robotic%20systems%20remains%20highly%20challenging%20due%20to%20the%20difficulty%20of%20establishing%20correspondences%20between%20observed%20landmarks%20and%20those%20in%20the%20map%20without%20geometric%20and%20appearance%20details.%20To%20address%20this%20challenge%2C%20VLG-Loc%20leverages%20a%20vision-language%20model%20%28VLM%29%20to%20search%20the%20robot%27s%20multi-directional%20image%20observations%20for%20the%20landmarks%20noted%20in%20the%20map.%20The%20method%20then%20identifies%20robot%20poses%20within%20a%20Monte%20Carlo%20localization%20framework%2C%20where%20the%20found%20landmarks%20are%20used%20to%20evaluate%20the%20likelihood%20of%20each%20pose%20hypothesis.%20Experimental%20validation%20in%20simulated%20and%20real-world%20retail%20environments%20demonstrates%20superior%20robustness%20compared%20to%20existing%20scan-based%20methods%2C%20particularly%20under%20environmental%20changes.%20Further%20improvements%20are%20achieved%20through%20the%20probabilistic%20fusion%20of%20visual%20and%20scan-based%20localization.&entry.1838667208=http%3A//arxiv.org/abs/2512.12793v2&entry.124074799=Read"},
{"title": "DenseBEV: Transforming BEV Grid Cells into 3D Objects", "author": "Marius D\u00e4hling and Sebastian Krebs and J. Marius Z\u00f6llner", "abstract": "In current research, Bird's-Eye-View (BEV)-based transformers are increasingly utilized for multi-camera 3D object detection. Traditional models often employ random queries as anchors, optimizing them successively. Recent advancements complement or replace these random queries with detections from auxiliary networks. We propose a more intuitive and efficient approach by using BEV feature cells directly as anchors. This end-to-end approach leverages the dense grid of BEV queries, considering each cell as a potential object for the final detection task. As a result, we introduce a novel two-stage anchor generation method specifically designed for multi-camera 3D object detection. To address the scaling issues of attention with a large number of queries, we apply BEV-based Non-Maximum Suppression, allowing gradients to flow only through non-suppressed objects. This ensures efficient training without the need for post-processing. By using BEV features from encoders such as BEVFormer directly as object queries, temporal BEV information is inherently embedded. Building on the temporal BEV information already embedded in our object queries, we introduce a hybrid temporal modeling approach by integrating prior detections to further enhance detection performance. Evaluating our method on the nuScenes dataset shows consistent and significant improvements in NDS and mAP over the baseline, even with sparser BEV grids and therefore fewer initial anchors. It is particularly effective for small objects, enhancing pedestrian detection with a 3.8% mAP increase on nuScenes and an 8% increase in LET-mAP on Waymo. Applying our method, named DenseBEV, to the challenging Waymo Open dataset yields state-of-the-art performance, achieving a LET-mAP of 60.7%, surpassing the previous best by 5.4%. Code is available at https://github.com/mdaehl/DenseBEV.", "link": "http://arxiv.org/abs/2512.16818v1", "date": "2025-12-18", "relevancy": 2.2911, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5995}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5743}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DenseBEV%3A%20Transforming%20BEV%20Grid%20Cells%20into%203D%20Objects&body=Title%3A%20DenseBEV%3A%20Transforming%20BEV%20Grid%20Cells%20into%203D%20Objects%0AAuthor%3A%20Marius%20D%C3%A4hling%20and%20Sebastian%20Krebs%20and%20J.%20Marius%20Z%C3%B6llner%0AAbstract%3A%20In%20current%20research%2C%20Bird%27s-Eye-View%20%28BEV%29-based%20transformers%20are%20increasingly%20utilized%20for%20multi-camera%203D%20object%20detection.%20Traditional%20models%20often%20employ%20random%20queries%20as%20anchors%2C%20optimizing%20them%20successively.%20Recent%20advancements%20complement%20or%20replace%20these%20random%20queries%20with%20detections%20from%20auxiliary%20networks.%20We%20propose%20a%20more%20intuitive%20and%20efficient%20approach%20by%20using%20BEV%20feature%20cells%20directly%20as%20anchors.%20This%20end-to-end%20approach%20leverages%20the%20dense%20grid%20of%20BEV%20queries%2C%20considering%20each%20cell%20as%20a%20potential%20object%20for%20the%20final%20detection%20task.%20As%20a%20result%2C%20we%20introduce%20a%20novel%20two-stage%20anchor%20generation%20method%20specifically%20designed%20for%20multi-camera%203D%20object%20detection.%20To%20address%20the%20scaling%20issues%20of%20attention%20with%20a%20large%20number%20of%20queries%2C%20we%20apply%20BEV-based%20Non-Maximum%20Suppression%2C%20allowing%20gradients%20to%20flow%20only%20through%20non-suppressed%20objects.%20This%20ensures%20efficient%20training%20without%20the%20need%20for%20post-processing.%20By%20using%20BEV%20features%20from%20encoders%20such%20as%20BEVFormer%20directly%20as%20object%20queries%2C%20temporal%20BEV%20information%20is%20inherently%20embedded.%20Building%20on%20the%20temporal%20BEV%20information%20already%20embedded%20in%20our%20object%20queries%2C%20we%20introduce%20a%20hybrid%20temporal%20modeling%20approach%20by%20integrating%20prior%20detections%20to%20further%20enhance%20detection%20performance.%20Evaluating%20our%20method%20on%20the%20nuScenes%20dataset%20shows%20consistent%20and%20significant%20improvements%20in%20NDS%20and%20mAP%20over%20the%20baseline%2C%20even%20with%20sparser%20BEV%20grids%20and%20therefore%20fewer%20initial%20anchors.%20It%20is%20particularly%20effective%20for%20small%20objects%2C%20enhancing%20pedestrian%20detection%20with%20a%203.8%25%20mAP%20increase%20on%20nuScenes%20and%20an%208%25%20increase%20in%20LET-mAP%20on%20Waymo.%20Applying%20our%20method%2C%20named%20DenseBEV%2C%20to%20the%20challenging%20Waymo%20Open%20dataset%20yields%20state-of-the-art%20performance%2C%20achieving%20a%20LET-mAP%20of%2060.7%25%2C%20surpassing%20the%20previous%20best%20by%205.4%25.%20Code%20is%20available%20at%20https%3A//github.com/mdaehl/DenseBEV.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16818v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDenseBEV%253A%2520Transforming%2520BEV%2520Grid%2520Cells%2520into%25203D%2520Objects%26entry.906535625%3DMarius%2520D%25C3%25A4hling%2520and%2520Sebastian%2520Krebs%2520and%2520J.%2520Marius%2520Z%25C3%25B6llner%26entry.1292438233%3DIn%2520current%2520research%252C%2520Bird%2527s-Eye-View%2520%2528BEV%2529-based%2520transformers%2520are%2520increasingly%2520utilized%2520for%2520multi-camera%25203D%2520object%2520detection.%2520Traditional%2520models%2520often%2520employ%2520random%2520queries%2520as%2520anchors%252C%2520optimizing%2520them%2520successively.%2520Recent%2520advancements%2520complement%2520or%2520replace%2520these%2520random%2520queries%2520with%2520detections%2520from%2520auxiliary%2520networks.%2520We%2520propose%2520a%2520more%2520intuitive%2520and%2520efficient%2520approach%2520by%2520using%2520BEV%2520feature%2520cells%2520directly%2520as%2520anchors.%2520This%2520end-to-end%2520approach%2520leverages%2520the%2520dense%2520grid%2520of%2520BEV%2520queries%252C%2520considering%2520each%2520cell%2520as%2520a%2520potential%2520object%2520for%2520the%2520final%2520detection%2520task.%2520As%2520a%2520result%252C%2520we%2520introduce%2520a%2520novel%2520two-stage%2520anchor%2520generation%2520method%2520specifically%2520designed%2520for%2520multi-camera%25203D%2520object%2520detection.%2520To%2520address%2520the%2520scaling%2520issues%2520of%2520attention%2520with%2520a%2520large%2520number%2520of%2520queries%252C%2520we%2520apply%2520BEV-based%2520Non-Maximum%2520Suppression%252C%2520allowing%2520gradients%2520to%2520flow%2520only%2520through%2520non-suppressed%2520objects.%2520This%2520ensures%2520efficient%2520training%2520without%2520the%2520need%2520for%2520post-processing.%2520By%2520using%2520BEV%2520features%2520from%2520encoders%2520such%2520as%2520BEVFormer%2520directly%2520as%2520object%2520queries%252C%2520temporal%2520BEV%2520information%2520is%2520inherently%2520embedded.%2520Building%2520on%2520the%2520temporal%2520BEV%2520information%2520already%2520embedded%2520in%2520our%2520object%2520queries%252C%2520we%2520introduce%2520a%2520hybrid%2520temporal%2520modeling%2520approach%2520by%2520integrating%2520prior%2520detections%2520to%2520further%2520enhance%2520detection%2520performance.%2520Evaluating%2520our%2520method%2520on%2520the%2520nuScenes%2520dataset%2520shows%2520consistent%2520and%2520significant%2520improvements%2520in%2520NDS%2520and%2520mAP%2520over%2520the%2520baseline%252C%2520even%2520with%2520sparser%2520BEV%2520grids%2520and%2520therefore%2520fewer%2520initial%2520anchors.%2520It%2520is%2520particularly%2520effective%2520for%2520small%2520objects%252C%2520enhancing%2520pedestrian%2520detection%2520with%2520a%25203.8%2525%2520mAP%2520increase%2520on%2520nuScenes%2520and%2520an%25208%2525%2520increase%2520in%2520LET-mAP%2520on%2520Waymo.%2520Applying%2520our%2520method%252C%2520named%2520DenseBEV%252C%2520to%2520the%2520challenging%2520Waymo%2520Open%2520dataset%2520yields%2520state-of-the-art%2520performance%252C%2520achieving%2520a%2520LET-mAP%2520of%252060.7%2525%252C%2520surpassing%2520the%2520previous%2520best%2520by%25205.4%2525.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/mdaehl/DenseBEV.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16818v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DenseBEV%3A%20Transforming%20BEV%20Grid%20Cells%20into%203D%20Objects&entry.906535625=Marius%20D%C3%A4hling%20and%20Sebastian%20Krebs%20and%20J.%20Marius%20Z%C3%B6llner&entry.1292438233=In%20current%20research%2C%20Bird%27s-Eye-View%20%28BEV%29-based%20transformers%20are%20increasingly%20utilized%20for%20multi-camera%203D%20object%20detection.%20Traditional%20models%20often%20employ%20random%20queries%20as%20anchors%2C%20optimizing%20them%20successively.%20Recent%20advancements%20complement%20or%20replace%20these%20random%20queries%20with%20detections%20from%20auxiliary%20networks.%20We%20propose%20a%20more%20intuitive%20and%20efficient%20approach%20by%20using%20BEV%20feature%20cells%20directly%20as%20anchors.%20This%20end-to-end%20approach%20leverages%20the%20dense%20grid%20of%20BEV%20queries%2C%20considering%20each%20cell%20as%20a%20potential%20object%20for%20the%20final%20detection%20task.%20As%20a%20result%2C%20we%20introduce%20a%20novel%20two-stage%20anchor%20generation%20method%20specifically%20designed%20for%20multi-camera%203D%20object%20detection.%20To%20address%20the%20scaling%20issues%20of%20attention%20with%20a%20large%20number%20of%20queries%2C%20we%20apply%20BEV-based%20Non-Maximum%20Suppression%2C%20allowing%20gradients%20to%20flow%20only%20through%20non-suppressed%20objects.%20This%20ensures%20efficient%20training%20without%20the%20need%20for%20post-processing.%20By%20using%20BEV%20features%20from%20encoders%20such%20as%20BEVFormer%20directly%20as%20object%20queries%2C%20temporal%20BEV%20information%20is%20inherently%20embedded.%20Building%20on%20the%20temporal%20BEV%20information%20already%20embedded%20in%20our%20object%20queries%2C%20we%20introduce%20a%20hybrid%20temporal%20modeling%20approach%20by%20integrating%20prior%20detections%20to%20further%20enhance%20detection%20performance.%20Evaluating%20our%20method%20on%20the%20nuScenes%20dataset%20shows%20consistent%20and%20significant%20improvements%20in%20NDS%20and%20mAP%20over%20the%20baseline%2C%20even%20with%20sparser%20BEV%20grids%20and%20therefore%20fewer%20initial%20anchors.%20It%20is%20particularly%20effective%20for%20small%20objects%2C%20enhancing%20pedestrian%20detection%20with%20a%203.8%25%20mAP%20increase%20on%20nuScenes%20and%20an%208%25%20increase%20in%20LET-mAP%20on%20Waymo.%20Applying%20our%20method%2C%20named%20DenseBEV%2C%20to%20the%20challenging%20Waymo%20Open%20dataset%20yields%20state-of-the-art%20performance%2C%20achieving%20a%20LET-mAP%20of%2060.7%25%2C%20surpassing%20the%20previous%20best%20by%205.4%25.%20Code%20is%20available%20at%20https%3A//github.com/mdaehl/DenseBEV.&entry.1838667208=http%3A//arxiv.org/abs/2512.16818v1&entry.124074799=Read"},
{"title": "Single-View Shape Completion for Robotic Grasping in Clutter", "author": "Abhishek Kashyap and Yuxuan Yang and Henrik Andreasson and Todor Stoyanov", "abstract": "In vision-based robot manipulation, a single camera view can only capture one side of objects of interest, with additional occlusions in cluttered scenes further restricting visibility. As a result, the observed geometry is incomplete, and grasp estimation algorithms perform suboptimally. To address this limitation, we leverage diffusion models to perform category-level 3D shape completion from partial depth observations obtained from a single view, reconstructing complete object geometries to provide richer context for grasp planning. Our method focuses on common household items with diverse geometries, generating full 3D shapes that serve as input to downstream grasp inference networks. Unlike prior work, which primarily considers isolated objects or minimal clutter, we evaluate shape completion and grasping in realistic clutter scenarios with household objects. In preliminary evaluations on a cluttered scene, our approach consistently results in better grasp success rates than a naive baseline without shape completion by 23% and over a recent state of the art shape completion approach by 19%. Our code is available at https://amm.aass.oru.se/shape-completion-grasping/.", "link": "http://arxiv.org/abs/2512.16449v1", "date": "2025-12-18", "relevancy": 2.2906, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6025}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5817}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single-View%20Shape%20Completion%20for%20Robotic%20Grasping%20in%20Clutter&body=Title%3A%20Single-View%20Shape%20Completion%20for%20Robotic%20Grasping%20in%20Clutter%0AAuthor%3A%20Abhishek%20Kashyap%20and%20Yuxuan%20Yang%20and%20Henrik%20Andreasson%20and%20Todor%20Stoyanov%0AAbstract%3A%20In%20vision-based%20robot%20manipulation%2C%20a%20single%20camera%20view%20can%20only%20capture%20one%20side%20of%20objects%20of%20interest%2C%20with%20additional%20occlusions%20in%20cluttered%20scenes%20further%20restricting%20visibility.%20As%20a%20result%2C%20the%20observed%20geometry%20is%20incomplete%2C%20and%20grasp%20estimation%20algorithms%20perform%20suboptimally.%20To%20address%20this%20limitation%2C%20we%20leverage%20diffusion%20models%20to%20perform%20category-level%203D%20shape%20completion%20from%20partial%20depth%20observations%20obtained%20from%20a%20single%20view%2C%20reconstructing%20complete%20object%20geometries%20to%20provide%20richer%20context%20for%20grasp%20planning.%20Our%20method%20focuses%20on%20common%20household%20items%20with%20diverse%20geometries%2C%20generating%20full%203D%20shapes%20that%20serve%20as%20input%20to%20downstream%20grasp%20inference%20networks.%20Unlike%20prior%20work%2C%20which%20primarily%20considers%20isolated%20objects%20or%20minimal%20clutter%2C%20we%20evaluate%20shape%20completion%20and%20grasping%20in%20realistic%20clutter%20scenarios%20with%20household%20objects.%20In%20preliminary%20evaluations%20on%20a%20cluttered%20scene%2C%20our%20approach%20consistently%20results%20in%20better%20grasp%20success%20rates%20than%20a%20naive%20baseline%20without%20shape%20completion%20by%2023%25%20and%20over%20a%20recent%20state%20of%20the%20art%20shape%20completion%20approach%20by%2019%25.%20Our%20code%20is%20available%20at%20https%3A//amm.aass.oru.se/shape-completion-grasping/.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16449v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle-View%2520Shape%2520Completion%2520for%2520Robotic%2520Grasping%2520in%2520Clutter%26entry.906535625%3DAbhishek%2520Kashyap%2520and%2520Yuxuan%2520Yang%2520and%2520Henrik%2520Andreasson%2520and%2520Todor%2520Stoyanov%26entry.1292438233%3DIn%2520vision-based%2520robot%2520manipulation%252C%2520a%2520single%2520camera%2520view%2520can%2520only%2520capture%2520one%2520side%2520of%2520objects%2520of%2520interest%252C%2520with%2520additional%2520occlusions%2520in%2520cluttered%2520scenes%2520further%2520restricting%2520visibility.%2520As%2520a%2520result%252C%2520the%2520observed%2520geometry%2520is%2520incomplete%252C%2520and%2520grasp%2520estimation%2520algorithms%2520perform%2520suboptimally.%2520To%2520address%2520this%2520limitation%252C%2520we%2520leverage%2520diffusion%2520models%2520to%2520perform%2520category-level%25203D%2520shape%2520completion%2520from%2520partial%2520depth%2520observations%2520obtained%2520from%2520a%2520single%2520view%252C%2520reconstructing%2520complete%2520object%2520geometries%2520to%2520provide%2520richer%2520context%2520for%2520grasp%2520planning.%2520Our%2520method%2520focuses%2520on%2520common%2520household%2520items%2520with%2520diverse%2520geometries%252C%2520generating%2520full%25203D%2520shapes%2520that%2520serve%2520as%2520input%2520to%2520downstream%2520grasp%2520inference%2520networks.%2520Unlike%2520prior%2520work%252C%2520which%2520primarily%2520considers%2520isolated%2520objects%2520or%2520minimal%2520clutter%252C%2520we%2520evaluate%2520shape%2520completion%2520and%2520grasping%2520in%2520realistic%2520clutter%2520scenarios%2520with%2520household%2520objects.%2520In%2520preliminary%2520evaluations%2520on%2520a%2520cluttered%2520scene%252C%2520our%2520approach%2520consistently%2520results%2520in%2520better%2520grasp%2520success%2520rates%2520than%2520a%2520naive%2520baseline%2520without%2520shape%2520completion%2520by%252023%2525%2520and%2520over%2520a%2520recent%2520state%2520of%2520the%2520art%2520shape%2520completion%2520approach%2520by%252019%2525.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//amm.aass.oru.se/shape-completion-grasping/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16449v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single-View%20Shape%20Completion%20for%20Robotic%20Grasping%20in%20Clutter&entry.906535625=Abhishek%20Kashyap%20and%20Yuxuan%20Yang%20and%20Henrik%20Andreasson%20and%20Todor%20Stoyanov&entry.1292438233=In%20vision-based%20robot%20manipulation%2C%20a%20single%20camera%20view%20can%20only%20capture%20one%20side%20of%20objects%20of%20interest%2C%20with%20additional%20occlusions%20in%20cluttered%20scenes%20further%20restricting%20visibility.%20As%20a%20result%2C%20the%20observed%20geometry%20is%20incomplete%2C%20and%20grasp%20estimation%20algorithms%20perform%20suboptimally.%20To%20address%20this%20limitation%2C%20we%20leverage%20diffusion%20models%20to%20perform%20category-level%203D%20shape%20completion%20from%20partial%20depth%20observations%20obtained%20from%20a%20single%20view%2C%20reconstructing%20complete%20object%20geometries%20to%20provide%20richer%20context%20for%20grasp%20planning.%20Our%20method%20focuses%20on%20common%20household%20items%20with%20diverse%20geometries%2C%20generating%20full%203D%20shapes%20that%20serve%20as%20input%20to%20downstream%20grasp%20inference%20networks.%20Unlike%20prior%20work%2C%20which%20primarily%20considers%20isolated%20objects%20or%20minimal%20clutter%2C%20we%20evaluate%20shape%20completion%20and%20grasping%20in%20realistic%20clutter%20scenarios%20with%20household%20objects.%20In%20preliminary%20evaluations%20on%20a%20cluttered%20scene%2C%20our%20approach%20consistently%20results%20in%20better%20grasp%20success%20rates%20than%20a%20naive%20baseline%20without%20shape%20completion%20by%2023%25%20and%20over%20a%20recent%20state%20of%20the%20art%20shape%20completion%20approach%20by%2019%25.%20Our%20code%20is%20available%20at%20https%3A//amm.aass.oru.se/shape-completion-grasping/.&entry.1838667208=http%3A//arxiv.org/abs/2512.16449v1&entry.124074799=Read"},
{"title": "MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning", "author": "Yuanchen Ju and Yongyuan Liang and Yen-Jen Wang and Nandiraju Gireesh and Yuanliang Ju and Seungjae Lee and Qiao Gu and Elvis Hsieh and Furong Huang and Koushil Sreenath", "abstract": "Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.", "link": "http://arxiv.org/abs/2512.16909v1", "date": "2025-12-18", "relevancy": 2.2821, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6726}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5501}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MomaGraph%3A%20State-Aware%20Unified%20Scene%20Graphs%20with%20Vision-Language%20Model%20for%20Embodied%20Task%20Planning&body=Title%3A%20MomaGraph%3A%20State-Aware%20Unified%20Scene%20Graphs%20with%20Vision-Language%20Model%20for%20Embodied%20Task%20Planning%0AAuthor%3A%20Yuanchen%20Ju%20and%20Yongyuan%20Liang%20and%20Yen-Jen%20Wang%20and%20Nandiraju%20Gireesh%20and%20Yuanliang%20Ju%20and%20Seungjae%20Lee%20and%20Qiao%20Gu%20and%20Elvis%20Hsieh%20and%20Furong%20Huang%20and%20Koushil%20Sreenath%0AAbstract%3A%20Mobile%20manipulators%20in%20households%20must%20both%20navigate%20and%20manipulate.%20This%20requires%20a%20compact%2C%20semantically%20rich%20scene%20representation%20that%20captures%20where%20objects%20are%2C%20how%20they%20function%2C%20and%20which%20parts%20are%20actionable.%20Scene%20graphs%20are%20a%20natural%20choice%2C%20yet%20prior%20work%20often%20separates%20spatial%20and%20functional%20relations%2C%20treats%20scenes%20as%20static%20snapshots%20without%20object%20states%20or%20temporal%20updates%2C%20and%20overlooks%20information%20most%20relevant%20for%20accomplishing%20the%20current%20task.%20To%20address%20these%20limitations%2C%20we%20introduce%20MomaGraph%2C%20a%20unified%20scene%20representation%20for%20embodied%20agents%20that%20integrates%20spatial-functional%20relationships%20and%20part-level%20interactive%20elements.%20However%2C%20advancing%20such%20a%20representation%20requires%20both%20suitable%20data%20and%20rigorous%20evaluation%2C%20which%20have%20been%20largely%20missing.%20We%20thus%20contribute%20MomaGraph-Scenes%2C%20the%20first%20large-scale%20dataset%20of%20richly%20annotated%2C%20task-driven%20scene%20graphs%20in%20household%20environments%2C%20along%20with%20MomaGraph-Bench%2C%20a%20systematic%20evaluation%20suite%20spanning%20six%20reasoning%20capabilities%20from%20high-level%20planning%20to%20fine-grained%20scene%20understanding.%20Built%20upon%20this%20foundation%2C%20we%20further%20develop%20MomaGraph-R1%2C%20a%207B%20vision-language%20model%20trained%20with%20reinforcement%20learning%20on%20MomaGraph-Scenes.%20MomaGraph-R1%20predicts%20task-oriented%20scene%20graphs%20and%20serves%20as%20a%20zero-shot%20task%20planner%20under%20a%20Graph-then-Plan%20framework.%20Extensive%20experiments%20demonstrate%20that%20our%20model%20achieves%20state-of-the-art%20results%20among%20open-source%20models%2C%20reaching%2071.6%25%20accuracy%20on%20the%20benchmark%20%28%2B11.4%25%20over%20the%20best%20baseline%29%2C%20while%20generalizing%20across%20public%20benchmarks%20and%20transferring%20effectively%20to%20real-robot%20experiments.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16909v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMomaGraph%253A%2520State-Aware%2520Unified%2520Scene%2520Graphs%2520with%2520Vision-Language%2520Model%2520for%2520Embodied%2520Task%2520Planning%26entry.906535625%3DYuanchen%2520Ju%2520and%2520Yongyuan%2520Liang%2520and%2520Yen-Jen%2520Wang%2520and%2520Nandiraju%2520Gireesh%2520and%2520Yuanliang%2520Ju%2520and%2520Seungjae%2520Lee%2520and%2520Qiao%2520Gu%2520and%2520Elvis%2520Hsieh%2520and%2520Furong%2520Huang%2520and%2520Koushil%2520Sreenath%26entry.1292438233%3DMobile%2520manipulators%2520in%2520households%2520must%2520both%2520navigate%2520and%2520manipulate.%2520This%2520requires%2520a%2520compact%252C%2520semantically%2520rich%2520scene%2520representation%2520that%2520captures%2520where%2520objects%2520are%252C%2520how%2520they%2520function%252C%2520and%2520which%2520parts%2520are%2520actionable.%2520Scene%2520graphs%2520are%2520a%2520natural%2520choice%252C%2520yet%2520prior%2520work%2520often%2520separates%2520spatial%2520and%2520functional%2520relations%252C%2520treats%2520scenes%2520as%2520static%2520snapshots%2520without%2520object%2520states%2520or%2520temporal%2520updates%252C%2520and%2520overlooks%2520information%2520most%2520relevant%2520for%2520accomplishing%2520the%2520current%2520task.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520MomaGraph%252C%2520a%2520unified%2520scene%2520representation%2520for%2520embodied%2520agents%2520that%2520integrates%2520spatial-functional%2520relationships%2520and%2520part-level%2520interactive%2520elements.%2520However%252C%2520advancing%2520such%2520a%2520representation%2520requires%2520both%2520suitable%2520data%2520and%2520rigorous%2520evaluation%252C%2520which%2520have%2520been%2520largely%2520missing.%2520We%2520thus%2520contribute%2520MomaGraph-Scenes%252C%2520the%2520first%2520large-scale%2520dataset%2520of%2520richly%2520annotated%252C%2520task-driven%2520scene%2520graphs%2520in%2520household%2520environments%252C%2520along%2520with%2520MomaGraph-Bench%252C%2520a%2520systematic%2520evaluation%2520suite%2520spanning%2520six%2520reasoning%2520capabilities%2520from%2520high-level%2520planning%2520to%2520fine-grained%2520scene%2520understanding.%2520Built%2520upon%2520this%2520foundation%252C%2520we%2520further%2520develop%2520MomaGraph-R1%252C%2520a%25207B%2520vision-language%2520model%2520trained%2520with%2520reinforcement%2520learning%2520on%2520MomaGraph-Scenes.%2520MomaGraph-R1%2520predicts%2520task-oriented%2520scene%2520graphs%2520and%2520serves%2520as%2520a%2520zero-shot%2520task%2520planner%2520under%2520a%2520Graph-then-Plan%2520framework.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520model%2520achieves%2520state-of-the-art%2520results%2520among%2520open-source%2520models%252C%2520reaching%252071.6%2525%2520accuracy%2520on%2520the%2520benchmark%2520%2528%252B11.4%2525%2520over%2520the%2520best%2520baseline%2529%252C%2520while%2520generalizing%2520across%2520public%2520benchmarks%2520and%2520transferring%2520effectively%2520to%2520real-robot%2520experiments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16909v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MomaGraph%3A%20State-Aware%20Unified%20Scene%20Graphs%20with%20Vision-Language%20Model%20for%20Embodied%20Task%20Planning&entry.906535625=Yuanchen%20Ju%20and%20Yongyuan%20Liang%20and%20Yen-Jen%20Wang%20and%20Nandiraju%20Gireesh%20and%20Yuanliang%20Ju%20and%20Seungjae%20Lee%20and%20Qiao%20Gu%20and%20Elvis%20Hsieh%20and%20Furong%20Huang%20and%20Koushil%20Sreenath&entry.1292438233=Mobile%20manipulators%20in%20households%20must%20both%20navigate%20and%20manipulate.%20This%20requires%20a%20compact%2C%20semantically%20rich%20scene%20representation%20that%20captures%20where%20objects%20are%2C%20how%20they%20function%2C%20and%20which%20parts%20are%20actionable.%20Scene%20graphs%20are%20a%20natural%20choice%2C%20yet%20prior%20work%20often%20separates%20spatial%20and%20functional%20relations%2C%20treats%20scenes%20as%20static%20snapshots%20without%20object%20states%20or%20temporal%20updates%2C%20and%20overlooks%20information%20most%20relevant%20for%20accomplishing%20the%20current%20task.%20To%20address%20these%20limitations%2C%20we%20introduce%20MomaGraph%2C%20a%20unified%20scene%20representation%20for%20embodied%20agents%20that%20integrates%20spatial-functional%20relationships%20and%20part-level%20interactive%20elements.%20However%2C%20advancing%20such%20a%20representation%20requires%20both%20suitable%20data%20and%20rigorous%20evaluation%2C%20which%20have%20been%20largely%20missing.%20We%20thus%20contribute%20MomaGraph-Scenes%2C%20the%20first%20large-scale%20dataset%20of%20richly%20annotated%2C%20task-driven%20scene%20graphs%20in%20household%20environments%2C%20along%20with%20MomaGraph-Bench%2C%20a%20systematic%20evaluation%20suite%20spanning%20six%20reasoning%20capabilities%20from%20high-level%20planning%20to%20fine-grained%20scene%20understanding.%20Built%20upon%20this%20foundation%2C%20we%20further%20develop%20MomaGraph-R1%2C%20a%207B%20vision-language%20model%20trained%20with%20reinforcement%20learning%20on%20MomaGraph-Scenes.%20MomaGraph-R1%20predicts%20task-oriented%20scene%20graphs%20and%20serves%20as%20a%20zero-shot%20task%20planner%20under%20a%20Graph-then-Plan%20framework.%20Extensive%20experiments%20demonstrate%20that%20our%20model%20achieves%20state-of-the-art%20results%20among%20open-source%20models%2C%20reaching%2071.6%25%20accuracy%20on%20the%20benchmark%20%28%2B11.4%25%20over%20the%20best%20baseline%29%2C%20while%20generalizing%20across%20public%20benchmarks%20and%20transferring%20effectively%20to%20real-robot%20experiments.&entry.1838667208=http%3A//arxiv.org/abs/2512.16909v1&entry.124074799=Read"},
{"title": "CRONOS: Continuous Time Reconstruction for 4D Medical Longitudinal Series", "author": "Nico Albert Disch and Saikat Roy and Constantin Ulrich and Yannick Kirchhoff and Maximilian Rokuss and Robin Peretzke and David Zimmerer and Klaus Maier-Hein", "abstract": "Forecasting how 3D medical scans evolve over time is important for disease progression, treatment planning, and developmental assessment. Yet existing models either rely on a single prior scan, fixed grid times, or target global labels, which limits voxel-level forecasting under irregular sampling. We present CRONOS, a unified framework for many-to-one prediction from multiple past scans that supports both discrete (grid-based) and continuous (real-valued) timestamps in one model, to the best of our knowledge the first to achieve continuous sequence-to-image forecasting for 3D medical data. CRONOS learns a spatio-temporal velocity field that transports context volumes toward a target volume at an arbitrary time, while operating directly in 3D voxel space. Across three public datasets spanning Cine-MRI, perfusion CT, and longitudinal MRI, CRONOS outperforms other baselines, while remaining computationally competitive. We will release code and evaluation protocols to enable reproducible, multi-dataset benchmarking of multi-context, continuous-time forecasting.", "link": "http://arxiv.org/abs/2512.16577v1", "date": "2025-12-18", "relevancy": 2.2744, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5694}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5694}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CRONOS%3A%20Continuous%20Time%20Reconstruction%20for%204D%20Medical%20Longitudinal%20Series&body=Title%3A%20CRONOS%3A%20Continuous%20Time%20Reconstruction%20for%204D%20Medical%20Longitudinal%20Series%0AAuthor%3A%20Nico%20Albert%20Disch%20and%20Saikat%20Roy%20and%20Constantin%20Ulrich%20and%20Yannick%20Kirchhoff%20and%20Maximilian%20Rokuss%20and%20Robin%20Peretzke%20and%20David%20Zimmerer%20and%20Klaus%20Maier-Hein%0AAbstract%3A%20Forecasting%20how%203D%20medical%20scans%20evolve%20over%20time%20is%20important%20for%20disease%20progression%2C%20treatment%20planning%2C%20and%20developmental%20assessment.%20Yet%20existing%20models%20either%20rely%20on%20a%20single%20prior%20scan%2C%20fixed%20grid%20times%2C%20or%20target%20global%20labels%2C%20which%20limits%20voxel-level%20forecasting%20under%20irregular%20sampling.%20We%20present%20CRONOS%2C%20a%20unified%20framework%20for%20many-to-one%20prediction%20from%20multiple%20past%20scans%20that%20supports%20both%20discrete%20%28grid-based%29%20and%20continuous%20%28real-valued%29%20timestamps%20in%20one%20model%2C%20to%20the%20best%20of%20our%20knowledge%20the%20first%20to%20achieve%20continuous%20sequence-to-image%20forecasting%20for%203D%20medical%20data.%20CRONOS%20learns%20a%20spatio-temporal%20velocity%20field%20that%20transports%20context%20volumes%20toward%20a%20target%20volume%20at%20an%20arbitrary%20time%2C%20while%20operating%20directly%20in%203D%20voxel%20space.%20Across%20three%20public%20datasets%20spanning%20Cine-MRI%2C%20perfusion%20CT%2C%20and%20longitudinal%20MRI%2C%20CRONOS%20outperforms%20other%20baselines%2C%20while%20remaining%20computationally%20competitive.%20We%20will%20release%20code%20and%20evaluation%20protocols%20to%20enable%20reproducible%2C%20multi-dataset%20benchmarking%20of%20multi-context%2C%20continuous-time%20forecasting.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16577v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCRONOS%253A%2520Continuous%2520Time%2520Reconstruction%2520for%25204D%2520Medical%2520Longitudinal%2520Series%26entry.906535625%3DNico%2520Albert%2520Disch%2520and%2520Saikat%2520Roy%2520and%2520Constantin%2520Ulrich%2520and%2520Yannick%2520Kirchhoff%2520and%2520Maximilian%2520Rokuss%2520and%2520Robin%2520Peretzke%2520and%2520David%2520Zimmerer%2520and%2520Klaus%2520Maier-Hein%26entry.1292438233%3DForecasting%2520how%25203D%2520medical%2520scans%2520evolve%2520over%2520time%2520is%2520important%2520for%2520disease%2520progression%252C%2520treatment%2520planning%252C%2520and%2520developmental%2520assessment.%2520Yet%2520existing%2520models%2520either%2520rely%2520on%2520a%2520single%2520prior%2520scan%252C%2520fixed%2520grid%2520times%252C%2520or%2520target%2520global%2520labels%252C%2520which%2520limits%2520voxel-level%2520forecasting%2520under%2520irregular%2520sampling.%2520We%2520present%2520CRONOS%252C%2520a%2520unified%2520framework%2520for%2520many-to-one%2520prediction%2520from%2520multiple%2520past%2520scans%2520that%2520supports%2520both%2520discrete%2520%2528grid-based%2529%2520and%2520continuous%2520%2528real-valued%2529%2520timestamps%2520in%2520one%2520model%252C%2520to%2520the%2520best%2520of%2520our%2520knowledge%2520the%2520first%2520to%2520achieve%2520continuous%2520sequence-to-image%2520forecasting%2520for%25203D%2520medical%2520data.%2520CRONOS%2520learns%2520a%2520spatio-temporal%2520velocity%2520field%2520that%2520transports%2520context%2520volumes%2520toward%2520a%2520target%2520volume%2520at%2520an%2520arbitrary%2520time%252C%2520while%2520operating%2520directly%2520in%25203D%2520voxel%2520space.%2520Across%2520three%2520public%2520datasets%2520spanning%2520Cine-MRI%252C%2520perfusion%2520CT%252C%2520and%2520longitudinal%2520MRI%252C%2520CRONOS%2520outperforms%2520other%2520baselines%252C%2520while%2520remaining%2520computationally%2520competitive.%2520We%2520will%2520release%2520code%2520and%2520evaluation%2520protocols%2520to%2520enable%2520reproducible%252C%2520multi-dataset%2520benchmarking%2520of%2520multi-context%252C%2520continuous-time%2520forecasting.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16577v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CRONOS%3A%20Continuous%20Time%20Reconstruction%20for%204D%20Medical%20Longitudinal%20Series&entry.906535625=Nico%20Albert%20Disch%20and%20Saikat%20Roy%20and%20Constantin%20Ulrich%20and%20Yannick%20Kirchhoff%20and%20Maximilian%20Rokuss%20and%20Robin%20Peretzke%20and%20David%20Zimmerer%20and%20Klaus%20Maier-Hein&entry.1292438233=Forecasting%20how%203D%20medical%20scans%20evolve%20over%20time%20is%20important%20for%20disease%20progression%2C%20treatment%20planning%2C%20and%20developmental%20assessment.%20Yet%20existing%20models%20either%20rely%20on%20a%20single%20prior%20scan%2C%20fixed%20grid%20times%2C%20or%20target%20global%20labels%2C%20which%20limits%20voxel-level%20forecasting%20under%20irregular%20sampling.%20We%20present%20CRONOS%2C%20a%20unified%20framework%20for%20many-to-one%20prediction%20from%20multiple%20past%20scans%20that%20supports%20both%20discrete%20%28grid-based%29%20and%20continuous%20%28real-valued%29%20timestamps%20in%20one%20model%2C%20to%20the%20best%20of%20our%20knowledge%20the%20first%20to%20achieve%20continuous%20sequence-to-image%20forecasting%20for%203D%20medical%20data.%20CRONOS%20learns%20a%20spatio-temporal%20velocity%20field%20that%20transports%20context%20volumes%20toward%20a%20target%20volume%20at%20an%20arbitrary%20time%2C%20while%20operating%20directly%20in%203D%20voxel%20space.%20Across%20three%20public%20datasets%20spanning%20Cine-MRI%2C%20perfusion%20CT%2C%20and%20longitudinal%20MRI%2C%20CRONOS%20outperforms%20other%20baselines%2C%20while%20remaining%20computationally%20competitive.%20We%20will%20release%20code%20and%20evaluation%20protocols%20to%20enable%20reproducible%2C%20multi-dataset%20benchmarking%20of%20multi-context%2C%20continuous-time%20forecasting.&entry.1838667208=http%3A//arxiv.org/abs/2512.16577v1&entry.124074799=Read"},
{"title": "OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction", "author": "Yuxin Ray Song and Jinzhou Li and Rao Fu and Devin Murphy and Kaichen Zhou and Rishi Shiv and Yaqi Li and Haoyu Xiong and Crystal Elaine Owens and Yilun Du and Yiyue Luo and Xianyi Cheng and Antonio Torralba and Wojciech Matusik and Paul Pu Liang", "abstract": "The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.", "link": "http://arxiv.org/abs/2512.16842v1", "date": "2025-12-18", "relevancy": 2.2683, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5942}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5502}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OPENTOUCH%3A%20Bringing%20Full-Hand%20Touch%20to%20Real-World%20Interaction&body=Title%3A%20OPENTOUCH%3A%20Bringing%20Full-Hand%20Touch%20to%20Real-World%20Interaction%0AAuthor%3A%20Yuxin%20Ray%20Song%20and%20Jinzhou%20Li%20and%20Rao%20Fu%20and%20Devin%20Murphy%20and%20Kaichen%20Zhou%20and%20Rishi%20Shiv%20and%20Yaqi%20Li%20and%20Haoyu%20Xiong%20and%20Crystal%20Elaine%20Owens%20and%20Yilun%20Du%20and%20Yiyue%20Luo%20and%20Xianyi%20Cheng%20and%20Antonio%20Torralba%20and%20Wojciech%20Matusik%20and%20Paul%20Pu%20Liang%0AAbstract%3A%20The%20human%20hand%20is%20our%20primary%20interface%20to%20the%20physical%20world%2C%20yet%20egocentric%20perception%20rarely%20knows%20when%2C%20where%2C%20or%20how%20forcefully%20it%20makes%20contact.%20Robust%20wearable%20tactile%20sensors%20are%20scarce%2C%20and%20no%20existing%20in-the-wild%20datasets%20align%20first-person%20video%20with%20full-hand%20touch.%20To%20bridge%20the%20gap%20between%20visual%20perception%20and%20physical%20interaction%2C%20we%20present%20OpenTouch%2C%20the%20first%20in-the-wild%20egocentric%20full-hand%20tactile%20dataset%2C%20containing%205.1%20hours%20of%20synchronized%20video-touch-pose%20data%20and%202%2C900%20curated%20clips%20with%20detailed%20text%20annotations.%20Using%20OpenTouch%2C%20we%20introduce%20retrieval%20and%20classification%20benchmarks%20that%20probe%20how%20touch%20grounds%20perception%20and%20action.%20We%20show%20that%20tactile%20signals%20provide%20a%20compact%20yet%20powerful%20cue%20for%20grasp%20understanding%2C%20strengthen%20cross-modal%20alignment%2C%20and%20can%20be%20reliably%20retrieved%20from%20in-the-wild%20video%20queries.%20By%20releasing%20this%20annotated%20vision-touch-pose%20dataset%20and%20benchmark%2C%20we%20aim%20to%20advance%20multimodal%20egocentric%20perception%2C%20embodied%20learning%2C%20and%20contact-rich%20robotic%20manipulation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16842v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOPENTOUCH%253A%2520Bringing%2520Full-Hand%2520Touch%2520to%2520Real-World%2520Interaction%26entry.906535625%3DYuxin%2520Ray%2520Song%2520and%2520Jinzhou%2520Li%2520and%2520Rao%2520Fu%2520and%2520Devin%2520Murphy%2520and%2520Kaichen%2520Zhou%2520and%2520Rishi%2520Shiv%2520and%2520Yaqi%2520Li%2520and%2520Haoyu%2520Xiong%2520and%2520Crystal%2520Elaine%2520Owens%2520and%2520Yilun%2520Du%2520and%2520Yiyue%2520Luo%2520and%2520Xianyi%2520Cheng%2520and%2520Antonio%2520Torralba%2520and%2520Wojciech%2520Matusik%2520and%2520Paul%2520Pu%2520Liang%26entry.1292438233%3DThe%2520human%2520hand%2520is%2520our%2520primary%2520interface%2520to%2520the%2520physical%2520world%252C%2520yet%2520egocentric%2520perception%2520rarely%2520knows%2520when%252C%2520where%252C%2520or%2520how%2520forcefully%2520it%2520makes%2520contact.%2520Robust%2520wearable%2520tactile%2520sensors%2520are%2520scarce%252C%2520and%2520no%2520existing%2520in-the-wild%2520datasets%2520align%2520first-person%2520video%2520with%2520full-hand%2520touch.%2520To%2520bridge%2520the%2520gap%2520between%2520visual%2520perception%2520and%2520physical%2520interaction%252C%2520we%2520present%2520OpenTouch%252C%2520the%2520first%2520in-the-wild%2520egocentric%2520full-hand%2520tactile%2520dataset%252C%2520containing%25205.1%2520hours%2520of%2520synchronized%2520video-touch-pose%2520data%2520and%25202%252C900%2520curated%2520clips%2520with%2520detailed%2520text%2520annotations.%2520Using%2520OpenTouch%252C%2520we%2520introduce%2520retrieval%2520and%2520classification%2520benchmarks%2520that%2520probe%2520how%2520touch%2520grounds%2520perception%2520and%2520action.%2520We%2520show%2520that%2520tactile%2520signals%2520provide%2520a%2520compact%2520yet%2520powerful%2520cue%2520for%2520grasp%2520understanding%252C%2520strengthen%2520cross-modal%2520alignment%252C%2520and%2520can%2520be%2520reliably%2520retrieved%2520from%2520in-the-wild%2520video%2520queries.%2520By%2520releasing%2520this%2520annotated%2520vision-touch-pose%2520dataset%2520and%2520benchmark%252C%2520we%2520aim%2520to%2520advance%2520multimodal%2520egocentric%2520perception%252C%2520embodied%2520learning%252C%2520and%2520contact-rich%2520robotic%2520manipulation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16842v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OPENTOUCH%3A%20Bringing%20Full-Hand%20Touch%20to%20Real-World%20Interaction&entry.906535625=Yuxin%20Ray%20Song%20and%20Jinzhou%20Li%20and%20Rao%20Fu%20and%20Devin%20Murphy%20and%20Kaichen%20Zhou%20and%20Rishi%20Shiv%20and%20Yaqi%20Li%20and%20Haoyu%20Xiong%20and%20Crystal%20Elaine%20Owens%20and%20Yilun%20Du%20and%20Yiyue%20Luo%20and%20Xianyi%20Cheng%20and%20Antonio%20Torralba%20and%20Wojciech%20Matusik%20and%20Paul%20Pu%20Liang&entry.1292438233=The%20human%20hand%20is%20our%20primary%20interface%20to%20the%20physical%20world%2C%20yet%20egocentric%20perception%20rarely%20knows%20when%2C%20where%2C%20or%20how%20forcefully%20it%20makes%20contact.%20Robust%20wearable%20tactile%20sensors%20are%20scarce%2C%20and%20no%20existing%20in-the-wild%20datasets%20align%20first-person%20video%20with%20full-hand%20touch.%20To%20bridge%20the%20gap%20between%20visual%20perception%20and%20physical%20interaction%2C%20we%20present%20OpenTouch%2C%20the%20first%20in-the-wild%20egocentric%20full-hand%20tactile%20dataset%2C%20containing%205.1%20hours%20of%20synchronized%20video-touch-pose%20data%20and%202%2C900%20curated%20clips%20with%20detailed%20text%20annotations.%20Using%20OpenTouch%2C%20we%20introduce%20retrieval%20and%20classification%20benchmarks%20that%20probe%20how%20touch%20grounds%20perception%20and%20action.%20We%20show%20that%20tactile%20signals%20provide%20a%20compact%20yet%20powerful%20cue%20for%20grasp%20understanding%2C%20strengthen%20cross-modal%20alignment%2C%20and%20can%20be%20reliably%20retrieved%20from%20in-the-wild%20video%20queries.%20By%20releasing%20this%20annotated%20vision-touch-pose%20dataset%20and%20benchmark%2C%20we%20aim%20to%20advance%20multimodal%20egocentric%20perception%2C%20embodied%20learning%2C%20and%20contact-rich%20robotic%20manipulation.&entry.1838667208=http%3A//arxiv.org/abs/2512.16842v1&entry.124074799=Read"},
{"title": "GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation", "author": "Amita Kamath and Kai-Wei Chang and Ranjay Krishna and Luke Zettlemoyer and Yushi Hu and Marjan Ghazvininejad", "abstract": "Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.", "link": "http://arxiv.org/abs/2512.16853v1", "date": "2025-12-18", "relevancy": 2.2634, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.604}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5421}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5372}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenEval%202%3A%20Addressing%20Benchmark%20Drift%20in%20Text-to-Image%20Evaluation&body=Title%3A%20GenEval%202%3A%20Addressing%20Benchmark%20Drift%20in%20Text-to-Image%20Evaluation%0AAuthor%3A%20Amita%20Kamath%20and%20Kai-Wei%20Chang%20and%20Ranjay%20Krishna%20and%20Luke%20Zettlemoyer%20and%20Yushi%20Hu%20and%20Marjan%20Ghazvininejad%0AAbstract%3A%20Automating%20Text-to-Image%20%28T2I%29%20model%20evaluation%20is%20challenging%3B%20a%20judge%20model%20must%20be%20used%20to%20score%20correctness%2C%20and%20test%20prompts%20must%20be%20selected%20to%20be%20challenging%20for%20current%20T2I%20models%20but%20not%20the%20judge.%20We%20argue%20that%20satisfying%20these%20constraints%20can%20lead%20to%20benchmark%20drift%20over%20time%2C%20where%20the%20static%20benchmark%20judges%20fail%20to%20keep%20up%20with%20newer%20model%20capabilities.%20We%20show%20that%20benchmark%20drift%20is%20a%20significant%20problem%20for%20GenEval%2C%20one%20of%20the%20most%20popular%20T2I%20benchmarks.%20Although%20GenEval%20was%20well-aligned%20with%20human%20judgment%20at%20the%20time%20of%20its%20release%2C%20it%20has%20drifted%20far%20from%20human%20judgment%20over%20time%20--%20resulting%20in%20an%20absolute%20error%20of%20as%20much%20as%2017.7%25%20for%20current%20models.%20This%20level%20of%20drift%20strongly%20suggests%20that%20GenEval%20has%20been%20saturated%20for%20some%20time%2C%20as%20we%20verify%20via%20a%20large-scale%20human%20study.%20To%20help%20fill%20this%20benchmarking%20gap%2C%20we%20introduce%20a%20new%20benchmark%2C%20GenEval%202%2C%20with%20improved%20coverage%20of%20primitive%20visual%20concepts%20and%20higher%20degrees%20of%20compositionality%2C%20which%20we%20show%20is%20more%20challenging%20for%20current%20models.%20We%20also%20introduce%20Soft-TIFA%2C%20an%20evaluation%20method%20for%20GenEval%202%20that%20combines%20judgments%20for%20visual%20primitives%2C%20which%20we%20show%20is%20more%20well-aligned%20with%20human%20judgment%20and%20argue%20is%20less%20likely%20to%20drift%20from%20human-alignment%20over%20time%20%28as%20compared%20to%20more%20holistic%20judges%20such%20as%20VQAScore%29.%20Although%20we%20hope%20GenEval%202%20will%20provide%20a%20strong%20benchmark%20for%20many%20years%2C%20avoiding%20benchmark%20drift%20is%20far%20from%20guaranteed%20and%20our%20work%2C%20more%20generally%2C%20highlights%20the%20importance%20of%20continual%20audits%20and%20improvement%20for%20T2I%20and%20related%20automated%20model%20evaluation%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16853v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenEval%25202%253A%2520Addressing%2520Benchmark%2520Drift%2520in%2520Text-to-Image%2520Evaluation%26entry.906535625%3DAmita%2520Kamath%2520and%2520Kai-Wei%2520Chang%2520and%2520Ranjay%2520Krishna%2520and%2520Luke%2520Zettlemoyer%2520and%2520Yushi%2520Hu%2520and%2520Marjan%2520Ghazvininejad%26entry.1292438233%3DAutomating%2520Text-to-Image%2520%2528T2I%2529%2520model%2520evaluation%2520is%2520challenging%253B%2520a%2520judge%2520model%2520must%2520be%2520used%2520to%2520score%2520correctness%252C%2520and%2520test%2520prompts%2520must%2520be%2520selected%2520to%2520be%2520challenging%2520for%2520current%2520T2I%2520models%2520but%2520not%2520the%2520judge.%2520We%2520argue%2520that%2520satisfying%2520these%2520constraints%2520can%2520lead%2520to%2520benchmark%2520drift%2520over%2520time%252C%2520where%2520the%2520static%2520benchmark%2520judges%2520fail%2520to%2520keep%2520up%2520with%2520newer%2520model%2520capabilities.%2520We%2520show%2520that%2520benchmark%2520drift%2520is%2520a%2520significant%2520problem%2520for%2520GenEval%252C%2520one%2520of%2520the%2520most%2520popular%2520T2I%2520benchmarks.%2520Although%2520GenEval%2520was%2520well-aligned%2520with%2520human%2520judgment%2520at%2520the%2520time%2520of%2520its%2520release%252C%2520it%2520has%2520drifted%2520far%2520from%2520human%2520judgment%2520over%2520time%2520--%2520resulting%2520in%2520an%2520absolute%2520error%2520of%2520as%2520much%2520as%252017.7%2525%2520for%2520current%2520models.%2520This%2520level%2520of%2520drift%2520strongly%2520suggests%2520that%2520GenEval%2520has%2520been%2520saturated%2520for%2520some%2520time%252C%2520as%2520we%2520verify%2520via%2520a%2520large-scale%2520human%2520study.%2520To%2520help%2520fill%2520this%2520benchmarking%2520gap%252C%2520we%2520introduce%2520a%2520new%2520benchmark%252C%2520GenEval%25202%252C%2520with%2520improved%2520coverage%2520of%2520primitive%2520visual%2520concepts%2520and%2520higher%2520degrees%2520of%2520compositionality%252C%2520which%2520we%2520show%2520is%2520more%2520challenging%2520for%2520current%2520models.%2520We%2520also%2520introduce%2520Soft-TIFA%252C%2520an%2520evaluation%2520method%2520for%2520GenEval%25202%2520that%2520combines%2520judgments%2520for%2520visual%2520primitives%252C%2520which%2520we%2520show%2520is%2520more%2520well-aligned%2520with%2520human%2520judgment%2520and%2520argue%2520is%2520less%2520likely%2520to%2520drift%2520from%2520human-alignment%2520over%2520time%2520%2528as%2520compared%2520to%2520more%2520holistic%2520judges%2520such%2520as%2520VQAScore%2529.%2520Although%2520we%2520hope%2520GenEval%25202%2520will%2520provide%2520a%2520strong%2520benchmark%2520for%2520many%2520years%252C%2520avoiding%2520benchmark%2520drift%2520is%2520far%2520from%2520guaranteed%2520and%2520our%2520work%252C%2520more%2520generally%252C%2520highlights%2520the%2520importance%2520of%2520continual%2520audits%2520and%2520improvement%2520for%2520T2I%2520and%2520related%2520automated%2520model%2520evaluation%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16853v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenEval%202%3A%20Addressing%20Benchmark%20Drift%20in%20Text-to-Image%20Evaluation&entry.906535625=Amita%20Kamath%20and%20Kai-Wei%20Chang%20and%20Ranjay%20Krishna%20and%20Luke%20Zettlemoyer%20and%20Yushi%20Hu%20and%20Marjan%20Ghazvininejad&entry.1292438233=Automating%20Text-to-Image%20%28T2I%29%20model%20evaluation%20is%20challenging%3B%20a%20judge%20model%20must%20be%20used%20to%20score%20correctness%2C%20and%20test%20prompts%20must%20be%20selected%20to%20be%20challenging%20for%20current%20T2I%20models%20but%20not%20the%20judge.%20We%20argue%20that%20satisfying%20these%20constraints%20can%20lead%20to%20benchmark%20drift%20over%20time%2C%20where%20the%20static%20benchmark%20judges%20fail%20to%20keep%20up%20with%20newer%20model%20capabilities.%20We%20show%20that%20benchmark%20drift%20is%20a%20significant%20problem%20for%20GenEval%2C%20one%20of%20the%20most%20popular%20T2I%20benchmarks.%20Although%20GenEval%20was%20well-aligned%20with%20human%20judgment%20at%20the%20time%20of%20its%20release%2C%20it%20has%20drifted%20far%20from%20human%20judgment%20over%20time%20--%20resulting%20in%20an%20absolute%20error%20of%20as%20much%20as%2017.7%25%20for%20current%20models.%20This%20level%20of%20drift%20strongly%20suggests%20that%20GenEval%20has%20been%20saturated%20for%20some%20time%2C%20as%20we%20verify%20via%20a%20large-scale%20human%20study.%20To%20help%20fill%20this%20benchmarking%20gap%2C%20we%20introduce%20a%20new%20benchmark%2C%20GenEval%202%2C%20with%20improved%20coverage%20of%20primitive%20visual%20concepts%20and%20higher%20degrees%20of%20compositionality%2C%20which%20we%20show%20is%20more%20challenging%20for%20current%20models.%20We%20also%20introduce%20Soft-TIFA%2C%20an%20evaluation%20method%20for%20GenEval%202%20that%20combines%20judgments%20for%20visual%20primitives%2C%20which%20we%20show%20is%20more%20well-aligned%20with%20human%20judgment%20and%20argue%20is%20less%20likely%20to%20drift%20from%20human-alignment%20over%20time%20%28as%20compared%20to%20more%20holistic%20judges%20such%20as%20VQAScore%29.%20Although%20we%20hope%20GenEval%202%20will%20provide%20a%20strong%20benchmark%20for%20many%20years%2C%20avoiding%20benchmark%20drift%20is%20far%20from%20guaranteed%20and%20our%20work%2C%20more%20generally%2C%20highlights%20the%20importance%20of%20continual%20audits%20and%20improvement%20for%20T2I%20and%20related%20automated%20model%20evaluation%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2512.16853v1&entry.124074799=Read"},
{"title": "EnviSAgE: A Survey of Environment Scaling for Qualitative Agentic Experience Collection", "author": "Yuchen Huang and Sijia Li and Minghao Liu and Wei Liu and Shijue Huang and Zhiyuan Fan and Hou Pong Chan and Yi R. Fung", "abstract": "LLM-based agents can autonomously accomplish complex tasks across various domains. However, to further cultivate capabilities such as adaptive behavior and long-term decision-making, training on static datasets built from human-level knowledge is insufficient. These datasets are costly to construct and lack both dynamism and realism. A growing consensus is that agents should instead interact directly with environments and learn from experience through reinforcement learning. We formalize this iterative process as the Generation-Execution-Feedback (GEF) loop, where environments generate tasks to challenge agents, return observations in response to agents' actions during task execution, and provide evaluative feedback on rollouts for subsequent learning. Under this paradigm, environments function as indispensable producers of experiential data, highlighting the need to scale them toward greater complexity, realism, and interactivity. In this survey, we systematically review representative methods for environment scaling from a pioneering environment-centric perspective and organize them along the stages of the GEF loop, namely task generation, task execution, and feedback. We further analyze implementation frameworks, challenges, and applications, consolidating fragmented advances and outlining future research directions for agent intelligence.", "link": "http://arxiv.org/abs/2511.09586v2", "date": "2025-12-18", "relevancy": 2.2618, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6027}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6014}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EnviSAgE%3A%20A%20Survey%20of%20Environment%20Scaling%20for%20Qualitative%20Agentic%20Experience%20Collection&body=Title%3A%20EnviSAgE%3A%20A%20Survey%20of%20Environment%20Scaling%20for%20Qualitative%20Agentic%20Experience%20Collection%0AAuthor%3A%20Yuchen%20Huang%20and%20Sijia%20Li%20and%20Minghao%20Liu%20and%20Wei%20Liu%20and%20Shijue%20Huang%20and%20Zhiyuan%20Fan%20and%20Hou%20Pong%20Chan%20and%20Yi%20R.%20Fung%0AAbstract%3A%20LLM-based%20agents%20can%20autonomously%20accomplish%20complex%20tasks%20across%20various%20domains.%20However%2C%20to%20further%20cultivate%20capabilities%20such%20as%20adaptive%20behavior%20and%20long-term%20decision-making%2C%20training%20on%20static%20datasets%20built%20from%20human-level%20knowledge%20is%20insufficient.%20These%20datasets%20are%20costly%20to%20construct%20and%20lack%20both%20dynamism%20and%20realism.%20A%20growing%20consensus%20is%20that%20agents%20should%20instead%20interact%20directly%20with%20environments%20and%20learn%20from%20experience%20through%20reinforcement%20learning.%20We%20formalize%20this%20iterative%20process%20as%20the%20Generation-Execution-Feedback%20%28GEF%29%20loop%2C%20where%20environments%20generate%20tasks%20to%20challenge%20agents%2C%20return%20observations%20in%20response%20to%20agents%27%20actions%20during%20task%20execution%2C%20and%20provide%20evaluative%20feedback%20on%20rollouts%20for%20subsequent%20learning.%20Under%20this%20paradigm%2C%20environments%20function%20as%20indispensable%20producers%20of%20experiential%20data%2C%20highlighting%20the%20need%20to%20scale%20them%20toward%20greater%20complexity%2C%20realism%2C%20and%20interactivity.%20In%20this%20survey%2C%20we%20systematically%20review%20representative%20methods%20for%20environment%20scaling%20from%20a%20pioneering%20environment-centric%20perspective%20and%20organize%20them%20along%20the%20stages%20of%20the%20GEF%20loop%2C%20namely%20task%20generation%2C%20task%20execution%2C%20and%20feedback.%20We%20further%20analyze%20implementation%20frameworks%2C%20challenges%2C%20and%20applications%2C%20consolidating%20fragmented%20advances%20and%20outlining%20future%20research%20directions%20for%20agent%20intelligence.%0ALink%3A%20http%3A//arxiv.org/abs/2511.09586v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnviSAgE%253A%2520A%2520Survey%2520of%2520Environment%2520Scaling%2520for%2520Qualitative%2520Agentic%2520Experience%2520Collection%26entry.906535625%3DYuchen%2520Huang%2520and%2520Sijia%2520Li%2520and%2520Minghao%2520Liu%2520and%2520Wei%2520Liu%2520and%2520Shijue%2520Huang%2520and%2520Zhiyuan%2520Fan%2520and%2520Hou%2520Pong%2520Chan%2520and%2520Yi%2520R.%2520Fung%26entry.1292438233%3DLLM-based%2520agents%2520can%2520autonomously%2520accomplish%2520complex%2520tasks%2520across%2520various%2520domains.%2520However%252C%2520to%2520further%2520cultivate%2520capabilities%2520such%2520as%2520adaptive%2520behavior%2520and%2520long-term%2520decision-making%252C%2520training%2520on%2520static%2520datasets%2520built%2520from%2520human-level%2520knowledge%2520is%2520insufficient.%2520These%2520datasets%2520are%2520costly%2520to%2520construct%2520and%2520lack%2520both%2520dynamism%2520and%2520realism.%2520A%2520growing%2520consensus%2520is%2520that%2520agents%2520should%2520instead%2520interact%2520directly%2520with%2520environments%2520and%2520learn%2520from%2520experience%2520through%2520reinforcement%2520learning.%2520We%2520formalize%2520this%2520iterative%2520process%2520as%2520the%2520Generation-Execution-Feedback%2520%2528GEF%2529%2520loop%252C%2520where%2520environments%2520generate%2520tasks%2520to%2520challenge%2520agents%252C%2520return%2520observations%2520in%2520response%2520to%2520agents%2527%2520actions%2520during%2520task%2520execution%252C%2520and%2520provide%2520evaluative%2520feedback%2520on%2520rollouts%2520for%2520subsequent%2520learning.%2520Under%2520this%2520paradigm%252C%2520environments%2520function%2520as%2520indispensable%2520producers%2520of%2520experiential%2520data%252C%2520highlighting%2520the%2520need%2520to%2520scale%2520them%2520toward%2520greater%2520complexity%252C%2520realism%252C%2520and%2520interactivity.%2520In%2520this%2520survey%252C%2520we%2520systematically%2520review%2520representative%2520methods%2520for%2520environment%2520scaling%2520from%2520a%2520pioneering%2520environment-centric%2520perspective%2520and%2520organize%2520them%2520along%2520the%2520stages%2520of%2520the%2520GEF%2520loop%252C%2520namely%2520task%2520generation%252C%2520task%2520execution%252C%2520and%2520feedback.%2520We%2520further%2520analyze%2520implementation%2520frameworks%252C%2520challenges%252C%2520and%2520applications%252C%2520consolidating%2520fragmented%2520advances%2520and%2520outlining%2520future%2520research%2520directions%2520for%2520agent%2520intelligence.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.09586v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EnviSAgE%3A%20A%20Survey%20of%20Environment%20Scaling%20for%20Qualitative%20Agentic%20Experience%20Collection&entry.906535625=Yuchen%20Huang%20and%20Sijia%20Li%20and%20Minghao%20Liu%20and%20Wei%20Liu%20and%20Shijue%20Huang%20and%20Zhiyuan%20Fan%20and%20Hou%20Pong%20Chan%20and%20Yi%20R.%20Fung&entry.1292438233=LLM-based%20agents%20can%20autonomously%20accomplish%20complex%20tasks%20across%20various%20domains.%20However%2C%20to%20further%20cultivate%20capabilities%20such%20as%20adaptive%20behavior%20and%20long-term%20decision-making%2C%20training%20on%20static%20datasets%20built%20from%20human-level%20knowledge%20is%20insufficient.%20These%20datasets%20are%20costly%20to%20construct%20and%20lack%20both%20dynamism%20and%20realism.%20A%20growing%20consensus%20is%20that%20agents%20should%20instead%20interact%20directly%20with%20environments%20and%20learn%20from%20experience%20through%20reinforcement%20learning.%20We%20formalize%20this%20iterative%20process%20as%20the%20Generation-Execution-Feedback%20%28GEF%29%20loop%2C%20where%20environments%20generate%20tasks%20to%20challenge%20agents%2C%20return%20observations%20in%20response%20to%20agents%27%20actions%20during%20task%20execution%2C%20and%20provide%20evaluative%20feedback%20on%20rollouts%20for%20subsequent%20learning.%20Under%20this%20paradigm%2C%20environments%20function%20as%20indispensable%20producers%20of%20experiential%20data%2C%20highlighting%20the%20need%20to%20scale%20them%20toward%20greater%20complexity%2C%20realism%2C%20and%20interactivity.%20In%20this%20survey%2C%20we%20systematically%20review%20representative%20methods%20for%20environment%20scaling%20from%20a%20pioneering%20environment-centric%20perspective%20and%20organize%20them%20along%20the%20stages%20of%20the%20GEF%20loop%2C%20namely%20task%20generation%2C%20task%20execution%2C%20and%20feedback.%20We%20further%20analyze%20implementation%20frameworks%2C%20challenges%2C%20and%20applications%2C%20consolidating%20fragmented%20advances%20and%20outlining%20future%20research%20directions%20for%20agent%20intelligence.&entry.1838667208=http%3A//arxiv.org/abs/2511.09586v2&entry.124074799=Read"},
{"title": "Biologically-Informed Hybrid Membership Inference Attacks on Generative Genomic Models", "author": "Asia Belfiore and Jonathan Passerat-Palmbach and Dmitrii Usynin", "abstract": "The increased availability of genetic data has transformed genomics research, but raised many privacy concerns regarding its handling due to its sensitive nature. This work explores the use of language models (LMs) for the generation of synthetic genetic mutation profiles, leveraging differential privacy (DP) for the protection of sensitive genetic data. We empirically evaluate the privacy guarantees of our DP modes by introducing a novel Biologically-Informed Hybrid Membership Inference Attack (biHMIA), which combines traditional black box MIA with contextual genomics metrics for enhanced attack power. Our experiments show that both small and large transformer GPT-like models are viable synthetic variant generators for small-scale genomics, and that our hybrid attack leads, on average, to higher adversarial success compared to traditional metric-based MIAs.", "link": "http://arxiv.org/abs/2511.07503v3", "date": "2025-12-18", "relevancy": 2.2524, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4524}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4513}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Biologically-Informed%20Hybrid%20Membership%20Inference%20Attacks%20on%20Generative%20Genomic%20Models&body=Title%3A%20Biologically-Informed%20Hybrid%20Membership%20Inference%20Attacks%20on%20Generative%20Genomic%20Models%0AAuthor%3A%20Asia%20Belfiore%20and%20Jonathan%20Passerat-Palmbach%20and%20Dmitrii%20Usynin%0AAbstract%3A%20The%20increased%20availability%20of%20genetic%20data%20has%20transformed%20genomics%20research%2C%20but%20raised%20many%20privacy%20concerns%20regarding%20its%20handling%20due%20to%20its%20sensitive%20nature.%20This%20work%20explores%20the%20use%20of%20language%20models%20%28LMs%29%20for%20the%20generation%20of%20synthetic%20genetic%20mutation%20profiles%2C%20leveraging%20differential%20privacy%20%28DP%29%20for%20the%20protection%20of%20sensitive%20genetic%20data.%20We%20empirically%20evaluate%20the%20privacy%20guarantees%20of%20our%20DP%20modes%20by%20introducing%20a%20novel%20Biologically-Informed%20Hybrid%20Membership%20Inference%20Attack%20%28biHMIA%29%2C%20which%20combines%20traditional%20black%20box%20MIA%20with%20contextual%20genomics%20metrics%20for%20enhanced%20attack%20power.%20Our%20experiments%20show%20that%20both%20small%20and%20large%20transformer%20GPT-like%20models%20are%20viable%20synthetic%20variant%20generators%20for%20small-scale%20genomics%2C%20and%20that%20our%20hybrid%20attack%20leads%2C%20on%20average%2C%20to%20higher%20adversarial%20success%20compared%20to%20traditional%20metric-based%20MIAs.%0ALink%3A%20http%3A//arxiv.org/abs/2511.07503v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiologically-Informed%2520Hybrid%2520Membership%2520Inference%2520Attacks%2520on%2520Generative%2520Genomic%2520Models%26entry.906535625%3DAsia%2520Belfiore%2520and%2520Jonathan%2520Passerat-Palmbach%2520and%2520Dmitrii%2520Usynin%26entry.1292438233%3DThe%2520increased%2520availability%2520of%2520genetic%2520data%2520has%2520transformed%2520genomics%2520research%252C%2520but%2520raised%2520many%2520privacy%2520concerns%2520regarding%2520its%2520handling%2520due%2520to%2520its%2520sensitive%2520nature.%2520This%2520work%2520explores%2520the%2520use%2520of%2520language%2520models%2520%2528LMs%2529%2520for%2520the%2520generation%2520of%2520synthetic%2520genetic%2520mutation%2520profiles%252C%2520leveraging%2520differential%2520privacy%2520%2528DP%2529%2520for%2520the%2520protection%2520of%2520sensitive%2520genetic%2520data.%2520We%2520empirically%2520evaluate%2520the%2520privacy%2520guarantees%2520of%2520our%2520DP%2520modes%2520by%2520introducing%2520a%2520novel%2520Biologically-Informed%2520Hybrid%2520Membership%2520Inference%2520Attack%2520%2528biHMIA%2529%252C%2520which%2520combines%2520traditional%2520black%2520box%2520MIA%2520with%2520contextual%2520genomics%2520metrics%2520for%2520enhanced%2520attack%2520power.%2520Our%2520experiments%2520show%2520that%2520both%2520small%2520and%2520large%2520transformer%2520GPT-like%2520models%2520are%2520viable%2520synthetic%2520variant%2520generators%2520for%2520small-scale%2520genomics%252C%2520and%2520that%2520our%2520hybrid%2520attack%2520leads%252C%2520on%2520average%252C%2520to%2520higher%2520adversarial%2520success%2520compared%2520to%2520traditional%2520metric-based%2520MIAs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.07503v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Biologically-Informed%20Hybrid%20Membership%20Inference%20Attacks%20on%20Generative%20Genomic%20Models&entry.906535625=Asia%20Belfiore%20and%20Jonathan%20Passerat-Palmbach%20and%20Dmitrii%20Usynin&entry.1292438233=The%20increased%20availability%20of%20genetic%20data%20has%20transformed%20genomics%20research%2C%20but%20raised%20many%20privacy%20concerns%20regarding%20its%20handling%20due%20to%20its%20sensitive%20nature.%20This%20work%20explores%20the%20use%20of%20language%20models%20%28LMs%29%20for%20the%20generation%20of%20synthetic%20genetic%20mutation%20profiles%2C%20leveraging%20differential%20privacy%20%28DP%29%20for%20the%20protection%20of%20sensitive%20genetic%20data.%20We%20empirically%20evaluate%20the%20privacy%20guarantees%20of%20our%20DP%20modes%20by%20introducing%20a%20novel%20Biologically-Informed%20Hybrid%20Membership%20Inference%20Attack%20%28biHMIA%29%2C%20which%20combines%20traditional%20black%20box%20MIA%20with%20contextual%20genomics%20metrics%20for%20enhanced%20attack%20power.%20Our%20experiments%20show%20that%20both%20small%20and%20large%20transformer%20GPT-like%20models%20are%20viable%20synthetic%20variant%20generators%20for%20small-scale%20genomics%2C%20and%20that%20our%20hybrid%20attack%20leads%2C%20on%20average%2C%20to%20higher%20adversarial%20success%20compared%20to%20traditional%20metric-based%20MIAs.&entry.1838667208=http%3A//arxiv.org/abs/2511.07503v3&entry.124074799=Read"},
{"title": "Advantages and limitations in the use of transfer learning for individual treatment effects in causal machine learning", "author": "Seyda Betul Aydin and Holger Brandt", "abstract": "Generalizing causal knowledge across diverse environments is challenging, especially when estimates from large-scale datasets must be applied to smaller or systematically different contexts, where external validity is critical. Model-based estimators of individual treatment effects (ITE) from machine learning require large sample sizes, limiting their applicability in domains such as behavioral sciences with smaller datasets. We demonstrate how estimation of ITEs with Treatment Agnostic Representation Networks (TARNet; Shalit et al., 2017) can be improved by leveraging knowledge from source datasets and adapting it to new settings via transfer learning (TL-TARNet; Aloui et al., 2023). In simulations that vary source and sample sizes and consider both randomized and non-randomized intervention target settings, the transfer-learning extension TL-TARNet improves upon standard TARNet, reducing ITE error and attenuating bias when a large unbiased source is available and target samples are small. In an empirical application using the India Human Development Survey (IHDS-II), we estimate the effect of mothers' firewood collection time on children's weekly study time; transfer learning pulls the target mean ITEs toward the source ITE estimate, reducing bias in the estimates obtained without transfer. These results suggest that transfer learning for causal models can improve the estimation of ITE in small samples.", "link": "http://arxiv.org/abs/2512.16489v1", "date": "2025-12-18", "relevancy": 2.2467, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4527}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4518}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advantages%20and%20limitations%20in%20the%20use%20of%20transfer%20learning%20for%20individual%20treatment%20effects%20in%20causal%20machine%20learning&body=Title%3A%20Advantages%20and%20limitations%20in%20the%20use%20of%20transfer%20learning%20for%20individual%20treatment%20effects%20in%20causal%20machine%20learning%0AAuthor%3A%20Seyda%20Betul%20Aydin%20and%20Holger%20Brandt%0AAbstract%3A%20Generalizing%20causal%20knowledge%20across%20diverse%20environments%20is%20challenging%2C%20especially%20when%20estimates%20from%20large-scale%20datasets%20must%20be%20applied%20to%20smaller%20or%20systematically%20different%20contexts%2C%20where%20external%20validity%20is%20critical.%20Model-based%20estimators%20of%20individual%20treatment%20effects%20%28ITE%29%20from%20machine%20learning%20require%20large%20sample%20sizes%2C%20limiting%20their%20applicability%20in%20domains%20such%20as%20behavioral%20sciences%20with%20smaller%20datasets.%20We%20demonstrate%20how%20estimation%20of%20ITEs%20with%20Treatment%20Agnostic%20Representation%20Networks%20%28TARNet%3B%20Shalit%20et%20al.%2C%202017%29%20can%20be%20improved%20by%20leveraging%20knowledge%20from%20source%20datasets%20and%20adapting%20it%20to%20new%20settings%20via%20transfer%20learning%20%28TL-TARNet%3B%20Aloui%20et%20al.%2C%202023%29.%20In%20simulations%20that%20vary%20source%20and%20sample%20sizes%20and%20consider%20both%20randomized%20and%20non-randomized%20intervention%20target%20settings%2C%20the%20transfer-learning%20extension%20TL-TARNet%20improves%20upon%20standard%20TARNet%2C%20reducing%20ITE%20error%20and%20attenuating%20bias%20when%20a%20large%20unbiased%20source%20is%20available%20and%20target%20samples%20are%20small.%20In%20an%20empirical%20application%20using%20the%20India%20Human%20Development%20Survey%20%28IHDS-II%29%2C%20we%20estimate%20the%20effect%20of%20mothers%27%20firewood%20collection%20time%20on%20children%27s%20weekly%20study%20time%3B%20transfer%20learning%20pulls%20the%20target%20mean%20ITEs%20toward%20the%20source%20ITE%20estimate%2C%20reducing%20bias%20in%20the%20estimates%20obtained%20without%20transfer.%20These%20results%20suggest%20that%20transfer%20learning%20for%20causal%20models%20can%20improve%20the%20estimation%20of%20ITE%20in%20small%20samples.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16489v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvantages%2520and%2520limitations%2520in%2520the%2520use%2520of%2520transfer%2520learning%2520for%2520individual%2520treatment%2520effects%2520in%2520causal%2520machine%2520learning%26entry.906535625%3DSeyda%2520Betul%2520Aydin%2520and%2520Holger%2520Brandt%26entry.1292438233%3DGeneralizing%2520causal%2520knowledge%2520across%2520diverse%2520environments%2520is%2520challenging%252C%2520especially%2520when%2520estimates%2520from%2520large-scale%2520datasets%2520must%2520be%2520applied%2520to%2520smaller%2520or%2520systematically%2520different%2520contexts%252C%2520where%2520external%2520validity%2520is%2520critical.%2520Model-based%2520estimators%2520of%2520individual%2520treatment%2520effects%2520%2528ITE%2529%2520from%2520machine%2520learning%2520require%2520large%2520sample%2520sizes%252C%2520limiting%2520their%2520applicability%2520in%2520domains%2520such%2520as%2520behavioral%2520sciences%2520with%2520smaller%2520datasets.%2520We%2520demonstrate%2520how%2520estimation%2520of%2520ITEs%2520with%2520Treatment%2520Agnostic%2520Representation%2520Networks%2520%2528TARNet%253B%2520Shalit%2520et%2520al.%252C%25202017%2529%2520can%2520be%2520improved%2520by%2520leveraging%2520knowledge%2520from%2520source%2520datasets%2520and%2520adapting%2520it%2520to%2520new%2520settings%2520via%2520transfer%2520learning%2520%2528TL-TARNet%253B%2520Aloui%2520et%2520al.%252C%25202023%2529.%2520In%2520simulations%2520that%2520vary%2520source%2520and%2520sample%2520sizes%2520and%2520consider%2520both%2520randomized%2520and%2520non-randomized%2520intervention%2520target%2520settings%252C%2520the%2520transfer-learning%2520extension%2520TL-TARNet%2520improves%2520upon%2520standard%2520TARNet%252C%2520reducing%2520ITE%2520error%2520and%2520attenuating%2520bias%2520when%2520a%2520large%2520unbiased%2520source%2520is%2520available%2520and%2520target%2520samples%2520are%2520small.%2520In%2520an%2520empirical%2520application%2520using%2520the%2520India%2520Human%2520Development%2520Survey%2520%2528IHDS-II%2529%252C%2520we%2520estimate%2520the%2520effect%2520of%2520mothers%2527%2520firewood%2520collection%2520time%2520on%2520children%2527s%2520weekly%2520study%2520time%253B%2520transfer%2520learning%2520pulls%2520the%2520target%2520mean%2520ITEs%2520toward%2520the%2520source%2520ITE%2520estimate%252C%2520reducing%2520bias%2520in%2520the%2520estimates%2520obtained%2520without%2520transfer.%2520These%2520results%2520suggest%2520that%2520transfer%2520learning%2520for%2520causal%2520models%2520can%2520improve%2520the%2520estimation%2520of%2520ITE%2520in%2520small%2520samples.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16489v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advantages%20and%20limitations%20in%20the%20use%20of%20transfer%20learning%20for%20individual%20treatment%20effects%20in%20causal%20machine%20learning&entry.906535625=Seyda%20Betul%20Aydin%20and%20Holger%20Brandt&entry.1292438233=Generalizing%20causal%20knowledge%20across%20diverse%20environments%20is%20challenging%2C%20especially%20when%20estimates%20from%20large-scale%20datasets%20must%20be%20applied%20to%20smaller%20or%20systematically%20different%20contexts%2C%20where%20external%20validity%20is%20critical.%20Model-based%20estimators%20of%20individual%20treatment%20effects%20%28ITE%29%20from%20machine%20learning%20require%20large%20sample%20sizes%2C%20limiting%20their%20applicability%20in%20domains%20such%20as%20behavioral%20sciences%20with%20smaller%20datasets.%20We%20demonstrate%20how%20estimation%20of%20ITEs%20with%20Treatment%20Agnostic%20Representation%20Networks%20%28TARNet%3B%20Shalit%20et%20al.%2C%202017%29%20can%20be%20improved%20by%20leveraging%20knowledge%20from%20source%20datasets%20and%20adapting%20it%20to%20new%20settings%20via%20transfer%20learning%20%28TL-TARNet%3B%20Aloui%20et%20al.%2C%202023%29.%20In%20simulations%20that%20vary%20source%20and%20sample%20sizes%20and%20consider%20both%20randomized%20and%20non-randomized%20intervention%20target%20settings%2C%20the%20transfer-learning%20extension%20TL-TARNet%20improves%20upon%20standard%20TARNet%2C%20reducing%20ITE%20error%20and%20attenuating%20bias%20when%20a%20large%20unbiased%20source%20is%20available%20and%20target%20samples%20are%20small.%20In%20an%20empirical%20application%20using%20the%20India%20Human%20Development%20Survey%20%28IHDS-II%29%2C%20we%20estimate%20the%20effect%20of%20mothers%27%20firewood%20collection%20time%20on%20children%27s%20weekly%20study%20time%3B%20transfer%20learning%20pulls%20the%20target%20mean%20ITEs%20toward%20the%20source%20ITE%20estimate%2C%20reducing%20bias%20in%20the%20estimates%20obtained%20without%20transfer.%20These%20results%20suggest%20that%20transfer%20learning%20for%20causal%20models%20can%20improve%20the%20estimation%20of%20ITE%20in%20small%20samples.&entry.1838667208=http%3A//arxiv.org/abs/2512.16489v1&entry.124074799=Read"},
{"title": "LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation", "author": "Haichao Zhang and Yao Lu and Lichen Wang and Yunzhe Li and Daiwei Chen and Yunpeng Xu and Yun Fu", "abstract": "Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.", "link": "http://arxiv.org/abs/2512.16891v1", "date": "2025-12-18", "relevancy": 2.2464, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5634}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5634}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LinkedOut%3A%20Linking%20World%20Knowledge%20Representation%20Out%20of%20Video%20LLM%20for%20Next-Generation%20Video%20Recommendation&body=Title%3A%20LinkedOut%3A%20Linking%20World%20Knowledge%20Representation%20Out%20of%20Video%20LLM%20for%20Next-Generation%20Video%20Recommendation%0AAuthor%3A%20Haichao%20Zhang%20and%20Yao%20Lu%20and%20Lichen%20Wang%20and%20Yunzhe%20Li%20and%20Daiwei%20Chen%20and%20Yunpeng%20Xu%20and%20Yun%20Fu%0AAbstract%3A%20Video%20Large%20Language%20Models%20%28VLLMs%29%20unlock%20world-knowledge-aware%20video%20understanding%20through%20pretraining%20on%20internet-scale%20data%20and%20have%20already%20shown%20promise%20on%20tasks%20such%20as%20movie%20analysis%20and%20video%20question%20answering.%20However%2C%20deploying%20VLLMs%20for%20downstream%20tasks%20such%20as%20video%20recommendation%20remains%20challenging%2C%20since%20real%20systems%20require%20multi-video%20inputs%2C%20lightweight%20backbones%2C%20low-latency%20sequential%20inference%2C%20and%20rapid%20response.%20In%20practice%2C%20%281%29%20decode-only%20generation%20yields%20high%20latency%20for%20sequential%20inference%2C%20%282%29%20typical%20interfaces%20do%20not%20support%20multi-video%20inputs%2C%20and%20%283%29%20constraining%20outputs%20to%20language%20discards%20fine-grained%20visual%20details%20that%20matter%20for%20downstream%20vision%20tasks.%20We%20argue%20that%20these%20limitations%20stem%20from%20the%20absence%20of%20a%20representation%20that%20preserves%20pixel-level%20detail%20while%20leveraging%20world%20knowledge.%20We%20present%20LinkedOut%2C%20a%20representation%20that%20extracts%20VLLM%20world%20knowledge%20directly%20from%20video%20to%20enable%20fast%20inference%2C%20supports%20multi-video%20histories%2C%20and%20removes%20the%20language%20bottleneck.%20LinkedOut%20extracts%20semantically%20grounded%2C%20knowledge-aware%20tokens%20from%20raw%20frames%20using%20VLLMs%2C%20guided%20by%20promptable%20queries%20and%20optional%20auxiliary%20modalities.%20We%20introduce%20a%20cross-layer%20knowledge%20fusion%20MoE%20that%20selects%20the%20appropriate%20level%20of%20abstraction%20from%20the%20rich%20VLLM%20features%2C%20enabling%20personalized%2C%20interpretable%2C%20and%20low-latency%20recommendation.%20To%20our%20knowledge%2C%20LinkedOut%20is%20the%20first%20VLLM-based%20video%20recommendation%20method%20that%20operates%20on%20raw%20frames%20without%20handcrafted%20labels%2C%20achieving%20state-of-the-art%20results%20on%20standard%20benchmarks.%20Interpretability%20studies%20and%20ablations%20confirm%20the%20benefits%20of%20layer%20diversity%20and%20layer-wise%20fusion%2C%20pointing%20to%20a%20practical%20path%20that%20fully%20leverages%20VLLM%20world-knowledge%20priors%20and%20visual%20reasoning%20for%20downstream%20vision%20tasks%20such%20as%20recommendation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16891v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinkedOut%253A%2520Linking%2520World%2520Knowledge%2520Representation%2520Out%2520of%2520Video%2520LLM%2520for%2520Next-Generation%2520Video%2520Recommendation%26entry.906535625%3DHaichao%2520Zhang%2520and%2520Yao%2520Lu%2520and%2520Lichen%2520Wang%2520and%2520Yunzhe%2520Li%2520and%2520Daiwei%2520Chen%2520and%2520Yunpeng%2520Xu%2520and%2520Yun%2520Fu%26entry.1292438233%3DVideo%2520Large%2520Language%2520Models%2520%2528VLLMs%2529%2520unlock%2520world-knowledge-aware%2520video%2520understanding%2520through%2520pretraining%2520on%2520internet-scale%2520data%2520and%2520have%2520already%2520shown%2520promise%2520on%2520tasks%2520such%2520as%2520movie%2520analysis%2520and%2520video%2520question%2520answering.%2520However%252C%2520deploying%2520VLLMs%2520for%2520downstream%2520tasks%2520such%2520as%2520video%2520recommendation%2520remains%2520challenging%252C%2520since%2520real%2520systems%2520require%2520multi-video%2520inputs%252C%2520lightweight%2520backbones%252C%2520low-latency%2520sequential%2520inference%252C%2520and%2520rapid%2520response.%2520In%2520practice%252C%2520%25281%2529%2520decode-only%2520generation%2520yields%2520high%2520latency%2520for%2520sequential%2520inference%252C%2520%25282%2529%2520typical%2520interfaces%2520do%2520not%2520support%2520multi-video%2520inputs%252C%2520and%2520%25283%2529%2520constraining%2520outputs%2520to%2520language%2520discards%2520fine-grained%2520visual%2520details%2520that%2520matter%2520for%2520downstream%2520vision%2520tasks.%2520We%2520argue%2520that%2520these%2520limitations%2520stem%2520from%2520the%2520absence%2520of%2520a%2520representation%2520that%2520preserves%2520pixel-level%2520detail%2520while%2520leveraging%2520world%2520knowledge.%2520We%2520present%2520LinkedOut%252C%2520a%2520representation%2520that%2520extracts%2520VLLM%2520world%2520knowledge%2520directly%2520from%2520video%2520to%2520enable%2520fast%2520inference%252C%2520supports%2520multi-video%2520histories%252C%2520and%2520removes%2520the%2520language%2520bottleneck.%2520LinkedOut%2520extracts%2520semantically%2520grounded%252C%2520knowledge-aware%2520tokens%2520from%2520raw%2520frames%2520using%2520VLLMs%252C%2520guided%2520by%2520promptable%2520queries%2520and%2520optional%2520auxiliary%2520modalities.%2520We%2520introduce%2520a%2520cross-layer%2520knowledge%2520fusion%2520MoE%2520that%2520selects%2520the%2520appropriate%2520level%2520of%2520abstraction%2520from%2520the%2520rich%2520VLLM%2520features%252C%2520enabling%2520personalized%252C%2520interpretable%252C%2520and%2520low-latency%2520recommendation.%2520To%2520our%2520knowledge%252C%2520LinkedOut%2520is%2520the%2520first%2520VLLM-based%2520video%2520recommendation%2520method%2520that%2520operates%2520on%2520raw%2520frames%2520without%2520handcrafted%2520labels%252C%2520achieving%2520state-of-the-art%2520results%2520on%2520standard%2520benchmarks.%2520Interpretability%2520studies%2520and%2520ablations%2520confirm%2520the%2520benefits%2520of%2520layer%2520diversity%2520and%2520layer-wise%2520fusion%252C%2520pointing%2520to%2520a%2520practical%2520path%2520that%2520fully%2520leverages%2520VLLM%2520world-knowledge%2520priors%2520and%2520visual%2520reasoning%2520for%2520downstream%2520vision%2520tasks%2520such%2520as%2520recommendation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16891v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LinkedOut%3A%20Linking%20World%20Knowledge%20Representation%20Out%20of%20Video%20LLM%20for%20Next-Generation%20Video%20Recommendation&entry.906535625=Haichao%20Zhang%20and%20Yao%20Lu%20and%20Lichen%20Wang%20and%20Yunzhe%20Li%20and%20Daiwei%20Chen%20and%20Yunpeng%20Xu%20and%20Yun%20Fu&entry.1292438233=Video%20Large%20Language%20Models%20%28VLLMs%29%20unlock%20world-knowledge-aware%20video%20understanding%20through%20pretraining%20on%20internet-scale%20data%20and%20have%20already%20shown%20promise%20on%20tasks%20such%20as%20movie%20analysis%20and%20video%20question%20answering.%20However%2C%20deploying%20VLLMs%20for%20downstream%20tasks%20such%20as%20video%20recommendation%20remains%20challenging%2C%20since%20real%20systems%20require%20multi-video%20inputs%2C%20lightweight%20backbones%2C%20low-latency%20sequential%20inference%2C%20and%20rapid%20response.%20In%20practice%2C%20%281%29%20decode-only%20generation%20yields%20high%20latency%20for%20sequential%20inference%2C%20%282%29%20typical%20interfaces%20do%20not%20support%20multi-video%20inputs%2C%20and%20%283%29%20constraining%20outputs%20to%20language%20discards%20fine-grained%20visual%20details%20that%20matter%20for%20downstream%20vision%20tasks.%20We%20argue%20that%20these%20limitations%20stem%20from%20the%20absence%20of%20a%20representation%20that%20preserves%20pixel-level%20detail%20while%20leveraging%20world%20knowledge.%20We%20present%20LinkedOut%2C%20a%20representation%20that%20extracts%20VLLM%20world%20knowledge%20directly%20from%20video%20to%20enable%20fast%20inference%2C%20supports%20multi-video%20histories%2C%20and%20removes%20the%20language%20bottleneck.%20LinkedOut%20extracts%20semantically%20grounded%2C%20knowledge-aware%20tokens%20from%20raw%20frames%20using%20VLLMs%2C%20guided%20by%20promptable%20queries%20and%20optional%20auxiliary%20modalities.%20We%20introduce%20a%20cross-layer%20knowledge%20fusion%20MoE%20that%20selects%20the%20appropriate%20level%20of%20abstraction%20from%20the%20rich%20VLLM%20features%2C%20enabling%20personalized%2C%20interpretable%2C%20and%20low-latency%20recommendation.%20To%20our%20knowledge%2C%20LinkedOut%20is%20the%20first%20VLLM-based%20video%20recommendation%20method%20that%20operates%20on%20raw%20frames%20without%20handcrafted%20labels%2C%20achieving%20state-of-the-art%20results%20on%20standard%20benchmarks.%20Interpretability%20studies%20and%20ablations%20confirm%20the%20benefits%20of%20layer%20diversity%20and%20layer-wise%20fusion%2C%20pointing%20to%20a%20practical%20path%20that%20fully%20leverages%20VLLM%20world-knowledge%20priors%20and%20visual%20reasoning%20for%20downstream%20vision%20tasks%20such%20as%20recommendation.&entry.1838667208=http%3A//arxiv.org/abs/2512.16891v1&entry.124074799=Read"},
{"title": "GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation", "author": "Jingjing Qian and Boyao Han and Chen Shi and Lei Xiao and Long Yang and Shaoshuai Shi and Li Jiang", "abstract": "Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.", "link": "http://arxiv.org/abs/2512.16811v1", "date": "2025-12-18", "relevancy": 2.2439, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5974}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5582}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoPredict%3A%20Leveraging%20Predictive%20Kinematics%20and%203D%20Gaussian%20Geometry%20for%20Precise%20VLA%20Manipulation&body=Title%3A%20GeoPredict%3A%20Leveraging%20Predictive%20Kinematics%20and%203D%20Gaussian%20Geometry%20for%20Precise%20VLA%20Manipulation%0AAuthor%3A%20Jingjing%20Qian%20and%20Boyao%20Han%20and%20Chen%20Shi%20and%20Lei%20Xiao%20and%20Long%20Yang%20and%20Shaoshuai%20Shi%20and%20Li%20Jiang%0AAbstract%3A%20Vision-Language-Action%20%28VLA%29%20models%20achieve%20strong%20generalization%20in%20robotic%20manipulation%20but%20remain%20largely%20reactive%20and%202D-centric%2C%20making%20them%20unreliable%20in%20tasks%20that%20require%20precise%203D%20reasoning.%20We%20propose%20GeoPredict%2C%20a%20geometry-aware%20VLA%20framework%20that%20augments%20a%20continuous-action%20policy%20with%20predictive%20kinematic%20and%20geometric%20priors.%20GeoPredict%20introduces%20a%20trajectory-level%20module%20that%20encodes%20motion%20history%20and%20predicts%20multi-step%203D%20keypoint%20trajectories%20of%20robot%20arms%2C%20and%20a%20predictive%203D%20Gaussian%20geometry%20module%20that%20forecasts%20workspace%20geometry%20with%20track-guided%20refinement%20along%20future%20keypoint%20trajectories.%20These%20predictive%20modules%20serve%20exclusively%20as%20training-time%20supervision%20through%20depth-based%20rendering%2C%20while%20inference%20requires%20only%20lightweight%20additional%20query%20tokens%20without%20invoking%20any%203D%20decoding.%20Experiments%20on%20RoboCasa%20Human-50%2C%20LIBERO%2C%20and%20real-world%20manipulation%20tasks%20show%20that%20GeoPredict%20consistently%20outperforms%20strong%20VLA%20baselines%2C%20especially%20in%20geometry-intensive%20and%20spatially%20demanding%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16811v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoPredict%253A%2520Leveraging%2520Predictive%2520Kinematics%2520and%25203D%2520Gaussian%2520Geometry%2520for%2520Precise%2520VLA%2520Manipulation%26entry.906535625%3DJingjing%2520Qian%2520and%2520Boyao%2520Han%2520and%2520Chen%2520Shi%2520and%2520Lei%2520Xiao%2520and%2520Long%2520Yang%2520and%2520Shaoshuai%2520Shi%2520and%2520Li%2520Jiang%26entry.1292438233%3DVision-Language-Action%2520%2528VLA%2529%2520models%2520achieve%2520strong%2520generalization%2520in%2520robotic%2520manipulation%2520but%2520remain%2520largely%2520reactive%2520and%25202D-centric%252C%2520making%2520them%2520unreliable%2520in%2520tasks%2520that%2520require%2520precise%25203D%2520reasoning.%2520We%2520propose%2520GeoPredict%252C%2520a%2520geometry-aware%2520VLA%2520framework%2520that%2520augments%2520a%2520continuous-action%2520policy%2520with%2520predictive%2520kinematic%2520and%2520geometric%2520priors.%2520GeoPredict%2520introduces%2520a%2520trajectory-level%2520module%2520that%2520encodes%2520motion%2520history%2520and%2520predicts%2520multi-step%25203D%2520keypoint%2520trajectories%2520of%2520robot%2520arms%252C%2520and%2520a%2520predictive%25203D%2520Gaussian%2520geometry%2520module%2520that%2520forecasts%2520workspace%2520geometry%2520with%2520track-guided%2520refinement%2520along%2520future%2520keypoint%2520trajectories.%2520These%2520predictive%2520modules%2520serve%2520exclusively%2520as%2520training-time%2520supervision%2520through%2520depth-based%2520rendering%252C%2520while%2520inference%2520requires%2520only%2520lightweight%2520additional%2520query%2520tokens%2520without%2520invoking%2520any%25203D%2520decoding.%2520Experiments%2520on%2520RoboCasa%2520Human-50%252C%2520LIBERO%252C%2520and%2520real-world%2520manipulation%2520tasks%2520show%2520that%2520GeoPredict%2520consistently%2520outperforms%2520strong%2520VLA%2520baselines%252C%2520especially%2520in%2520geometry-intensive%2520and%2520spatially%2520demanding%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16811v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoPredict%3A%20Leveraging%20Predictive%20Kinematics%20and%203D%20Gaussian%20Geometry%20for%20Precise%20VLA%20Manipulation&entry.906535625=Jingjing%20Qian%20and%20Boyao%20Han%20and%20Chen%20Shi%20and%20Lei%20Xiao%20and%20Long%20Yang%20and%20Shaoshuai%20Shi%20and%20Li%20Jiang&entry.1292438233=Vision-Language-Action%20%28VLA%29%20models%20achieve%20strong%20generalization%20in%20robotic%20manipulation%20but%20remain%20largely%20reactive%20and%202D-centric%2C%20making%20them%20unreliable%20in%20tasks%20that%20require%20precise%203D%20reasoning.%20We%20propose%20GeoPredict%2C%20a%20geometry-aware%20VLA%20framework%20that%20augments%20a%20continuous-action%20policy%20with%20predictive%20kinematic%20and%20geometric%20priors.%20GeoPredict%20introduces%20a%20trajectory-level%20module%20that%20encodes%20motion%20history%20and%20predicts%20multi-step%203D%20keypoint%20trajectories%20of%20robot%20arms%2C%20and%20a%20predictive%203D%20Gaussian%20geometry%20module%20that%20forecasts%20workspace%20geometry%20with%20track-guided%20refinement%20along%20future%20keypoint%20trajectories.%20These%20predictive%20modules%20serve%20exclusively%20as%20training-time%20supervision%20through%20depth-based%20rendering%2C%20while%20inference%20requires%20only%20lightweight%20additional%20query%20tokens%20without%20invoking%20any%203D%20decoding.%20Experiments%20on%20RoboCasa%20Human-50%2C%20LIBERO%2C%20and%20real-world%20manipulation%20tasks%20show%20that%20GeoPredict%20consistently%20outperforms%20strong%20VLA%20baselines%2C%20especially%20in%20geometry-intensive%20and%20spatially%20demanding%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2512.16811v1&entry.124074799=Read"},
{"title": "Plain language adaptations of biomedical text using LLMs: Comparision of evaluation metrics", "author": "Primoz Kocbek and Leon Kopitar and Gregor Stiglic", "abstract": "This study investigated the application of Large Language Models (LLMs) for simplifying biomedical texts to enhance health literacy. Using a public dataset, which included plain language adaptations of biomedical abstracts, we developed and evaluated several approaches, specifically a baseline approach using a prompt template, a two AI agent approach, and a fine-tuning approach. We selected OpenAI gpt-4o and gpt-4o mini models as baselines for further research. We evaluated our approaches with quantitative metrics, such as Flesch-Kincaid grade level, SMOG Index, SARI, and BERTScore, G-Eval, as well as with qualitative metric, more precisely 5-point Likert scales for simplicity, accuracy, completeness, brevity. Results showed a superior performance of gpt-4o-mini and an underperformance of FT approaches. G-Eval, a LLM based quantitative metric, showed promising results, ranking the approaches similarly as the qualitative metric.", "link": "http://arxiv.org/abs/2512.16530v1", "date": "2025-12-18", "relevancy": 2.237, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4552}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4435}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Plain%20language%20adaptations%20of%20biomedical%20text%20using%20LLMs%3A%20Comparision%20of%20evaluation%20metrics&body=Title%3A%20Plain%20language%20adaptations%20of%20biomedical%20text%20using%20LLMs%3A%20Comparision%20of%20evaluation%20metrics%0AAuthor%3A%20Primoz%20Kocbek%20and%20Leon%20Kopitar%20and%20Gregor%20Stiglic%0AAbstract%3A%20This%20study%20investigated%20the%20application%20of%20Large%20Language%20Models%20%28LLMs%29%20for%20simplifying%20biomedical%20texts%20to%20enhance%20health%20literacy.%20Using%20a%20public%20dataset%2C%20which%20included%20plain%20language%20adaptations%20of%20biomedical%20abstracts%2C%20we%20developed%20and%20evaluated%20several%20approaches%2C%20specifically%20a%20baseline%20approach%20using%20a%20prompt%20template%2C%20a%20two%20AI%20agent%20approach%2C%20and%20a%20fine-tuning%20approach.%20We%20selected%20OpenAI%20gpt-4o%20and%20gpt-4o%20mini%20models%20as%20baselines%20for%20further%20research.%20We%20evaluated%20our%20approaches%20with%20quantitative%20metrics%2C%20such%20as%20Flesch-Kincaid%20grade%20level%2C%20SMOG%20Index%2C%20SARI%2C%20and%20BERTScore%2C%20G-Eval%2C%20as%20well%20as%20with%20qualitative%20metric%2C%20more%20precisely%205-point%20Likert%20scales%20for%20simplicity%2C%20accuracy%2C%20completeness%2C%20brevity.%20Results%20showed%20a%20superior%20performance%20of%20gpt-4o-mini%20and%20an%20underperformance%20of%20FT%20approaches.%20G-Eval%2C%20a%20LLM%20based%20quantitative%20metric%2C%20showed%20promising%20results%2C%20ranking%20the%20approaches%20similarly%20as%20the%20qualitative%20metric.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16530v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlain%2520language%2520adaptations%2520of%2520biomedical%2520text%2520using%2520LLMs%253A%2520Comparision%2520of%2520evaluation%2520metrics%26entry.906535625%3DPrimoz%2520Kocbek%2520and%2520Leon%2520Kopitar%2520and%2520Gregor%2520Stiglic%26entry.1292438233%3DThis%2520study%2520investigated%2520the%2520application%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520for%2520simplifying%2520biomedical%2520texts%2520to%2520enhance%2520health%2520literacy.%2520Using%2520a%2520public%2520dataset%252C%2520which%2520included%2520plain%2520language%2520adaptations%2520of%2520biomedical%2520abstracts%252C%2520we%2520developed%2520and%2520evaluated%2520several%2520approaches%252C%2520specifically%2520a%2520baseline%2520approach%2520using%2520a%2520prompt%2520template%252C%2520a%2520two%2520AI%2520agent%2520approach%252C%2520and%2520a%2520fine-tuning%2520approach.%2520We%2520selected%2520OpenAI%2520gpt-4o%2520and%2520gpt-4o%2520mini%2520models%2520as%2520baselines%2520for%2520further%2520research.%2520We%2520evaluated%2520our%2520approaches%2520with%2520quantitative%2520metrics%252C%2520such%2520as%2520Flesch-Kincaid%2520grade%2520level%252C%2520SMOG%2520Index%252C%2520SARI%252C%2520and%2520BERTScore%252C%2520G-Eval%252C%2520as%2520well%2520as%2520with%2520qualitative%2520metric%252C%2520more%2520precisely%25205-point%2520Likert%2520scales%2520for%2520simplicity%252C%2520accuracy%252C%2520completeness%252C%2520brevity.%2520Results%2520showed%2520a%2520superior%2520performance%2520of%2520gpt-4o-mini%2520and%2520an%2520underperformance%2520of%2520FT%2520approaches.%2520G-Eval%252C%2520a%2520LLM%2520based%2520quantitative%2520metric%252C%2520showed%2520promising%2520results%252C%2520ranking%2520the%2520approaches%2520similarly%2520as%2520the%2520qualitative%2520metric.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16530v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Plain%20language%20adaptations%20of%20biomedical%20text%20using%20LLMs%3A%20Comparision%20of%20evaluation%20metrics&entry.906535625=Primoz%20Kocbek%20and%20Leon%20Kopitar%20and%20Gregor%20Stiglic&entry.1292438233=This%20study%20investigated%20the%20application%20of%20Large%20Language%20Models%20%28LLMs%29%20for%20simplifying%20biomedical%20texts%20to%20enhance%20health%20literacy.%20Using%20a%20public%20dataset%2C%20which%20included%20plain%20language%20adaptations%20of%20biomedical%20abstracts%2C%20we%20developed%20and%20evaluated%20several%20approaches%2C%20specifically%20a%20baseline%20approach%20using%20a%20prompt%20template%2C%20a%20two%20AI%20agent%20approach%2C%20and%20a%20fine-tuning%20approach.%20We%20selected%20OpenAI%20gpt-4o%20and%20gpt-4o%20mini%20models%20as%20baselines%20for%20further%20research.%20We%20evaluated%20our%20approaches%20with%20quantitative%20metrics%2C%20such%20as%20Flesch-Kincaid%20grade%20level%2C%20SMOG%20Index%2C%20SARI%2C%20and%20BERTScore%2C%20G-Eval%2C%20as%20well%20as%20with%20qualitative%20metric%2C%20more%20precisely%205-point%20Likert%20scales%20for%20simplicity%2C%20accuracy%2C%20completeness%2C%20brevity.%20Results%20showed%20a%20superior%20performance%20of%20gpt-4o-mini%20and%20an%20underperformance%20of%20FT%20approaches.%20G-Eval%2C%20a%20LLM%20based%20quantitative%20metric%2C%20showed%20promising%20results%2C%20ranking%20the%20approaches%20similarly%20as%20the%20qualitative%20metric.&entry.1838667208=http%3A//arxiv.org/abs/2512.16530v1&entry.124074799=Read"},
{"title": "Digital Modeling of Spatial Pathway Activity from Histology Reveals Tumor Microenvironment Heterogeneity", "author": "Ling Liao and Changhuei Yang and Maxim Artyomov and Mark Watson and Adam Kepecs and Haowen Zhou and Alexey Sergushichev and Richard Cote", "abstract": "Spatial transcriptomics (ST) enables simultaneous mapping of tissue morphology and spatially resolved gene expression, offering unique opportunities to study tumor microenvironment heterogeneity. Here, we introduce a computational framework that predicts spatial pathway activity directly from hematoxylin-and-eosin-stained histology images at microscale resolution 55 and 100 um. Using image features derived from a computational pathology foundation model, we found that TGFb signaling was the most accurately predicted pathway across three independent breast and lung cancer ST datasets. In 87-88% of reliably predicted cases, the resulting spatial TGFb activity maps reflected the expected contrast between tumor and adjacent non-tumor regions, consistent with the known role of TGFb in regulating interactions within the tumor microenvironment. Notably, linear and nonlinear predictive models performed similarly, suggesting that image features may relate to pathway activity in a predominantly linear fashion or that nonlinear structure is small relative to measurement noise. These findings demonstrate that features extracted from routine histopathology may recover spatially coherent and biologically interpretable pathway patterns, offering a scalable strategy for integrating image-based inference with ST information in tumor microenvironment studies.", "link": "http://arxiv.org/abs/2512.09003v2", "date": "2025-12-18", "relevancy": 2.2353, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4508}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4452}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Digital%20Modeling%20of%20Spatial%20Pathway%20Activity%20from%20Histology%20Reveals%20Tumor%20Microenvironment%20Heterogeneity&body=Title%3A%20Digital%20Modeling%20of%20Spatial%20Pathway%20Activity%20from%20Histology%20Reveals%20Tumor%20Microenvironment%20Heterogeneity%0AAuthor%3A%20Ling%20Liao%20and%20Changhuei%20Yang%20and%20Maxim%20Artyomov%20and%20Mark%20Watson%20and%20Adam%20Kepecs%20and%20Haowen%20Zhou%20and%20Alexey%20Sergushichev%20and%20Richard%20Cote%0AAbstract%3A%20Spatial%20transcriptomics%20%28ST%29%20enables%20simultaneous%20mapping%20of%20tissue%20morphology%20and%20spatially%20resolved%20gene%20expression%2C%20offering%20unique%20opportunities%20to%20study%20tumor%20microenvironment%20heterogeneity.%20Here%2C%20we%20introduce%20a%20computational%20framework%20that%20predicts%20spatial%20pathway%20activity%20directly%20from%20hematoxylin-and-eosin-stained%20histology%20images%20at%20microscale%20resolution%2055%20and%20100%20um.%20Using%20image%20features%20derived%20from%20a%20computational%20pathology%20foundation%20model%2C%20we%20found%20that%20TGFb%20signaling%20was%20the%20most%20accurately%20predicted%20pathway%20across%20three%20independent%20breast%20and%20lung%20cancer%20ST%20datasets.%20In%2087-88%25%20of%20reliably%20predicted%20cases%2C%20the%20resulting%20spatial%20TGFb%20activity%20maps%20reflected%20the%20expected%20contrast%20between%20tumor%20and%20adjacent%20non-tumor%20regions%2C%20consistent%20with%20the%20known%20role%20of%20TGFb%20in%20regulating%20interactions%20within%20the%20tumor%20microenvironment.%20Notably%2C%20linear%20and%20nonlinear%20predictive%20models%20performed%20similarly%2C%20suggesting%20that%20image%20features%20may%20relate%20to%20pathway%20activity%20in%20a%20predominantly%20linear%20fashion%20or%20that%20nonlinear%20structure%20is%20small%20relative%20to%20measurement%20noise.%20These%20findings%20demonstrate%20that%20features%20extracted%20from%20routine%20histopathology%20may%20recover%20spatially%20coherent%20and%20biologically%20interpretable%20pathway%20patterns%2C%20offering%20a%20scalable%20strategy%20for%20integrating%20image-based%20inference%20with%20ST%20information%20in%20tumor%20microenvironment%20studies.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09003v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDigital%2520Modeling%2520of%2520Spatial%2520Pathway%2520Activity%2520from%2520Histology%2520Reveals%2520Tumor%2520Microenvironment%2520Heterogeneity%26entry.906535625%3DLing%2520Liao%2520and%2520Changhuei%2520Yang%2520and%2520Maxim%2520Artyomov%2520and%2520Mark%2520Watson%2520and%2520Adam%2520Kepecs%2520and%2520Haowen%2520Zhou%2520and%2520Alexey%2520Sergushichev%2520and%2520Richard%2520Cote%26entry.1292438233%3DSpatial%2520transcriptomics%2520%2528ST%2529%2520enables%2520simultaneous%2520mapping%2520of%2520tissue%2520morphology%2520and%2520spatially%2520resolved%2520gene%2520expression%252C%2520offering%2520unique%2520opportunities%2520to%2520study%2520tumor%2520microenvironment%2520heterogeneity.%2520Here%252C%2520we%2520introduce%2520a%2520computational%2520framework%2520that%2520predicts%2520spatial%2520pathway%2520activity%2520directly%2520from%2520hematoxylin-and-eosin-stained%2520histology%2520images%2520at%2520microscale%2520resolution%252055%2520and%2520100%2520um.%2520Using%2520image%2520features%2520derived%2520from%2520a%2520computational%2520pathology%2520foundation%2520model%252C%2520we%2520found%2520that%2520TGFb%2520signaling%2520was%2520the%2520most%2520accurately%2520predicted%2520pathway%2520across%2520three%2520independent%2520breast%2520and%2520lung%2520cancer%2520ST%2520datasets.%2520In%252087-88%2525%2520of%2520reliably%2520predicted%2520cases%252C%2520the%2520resulting%2520spatial%2520TGFb%2520activity%2520maps%2520reflected%2520the%2520expected%2520contrast%2520between%2520tumor%2520and%2520adjacent%2520non-tumor%2520regions%252C%2520consistent%2520with%2520the%2520known%2520role%2520of%2520TGFb%2520in%2520regulating%2520interactions%2520within%2520the%2520tumor%2520microenvironment.%2520Notably%252C%2520linear%2520and%2520nonlinear%2520predictive%2520models%2520performed%2520similarly%252C%2520suggesting%2520that%2520image%2520features%2520may%2520relate%2520to%2520pathway%2520activity%2520in%2520a%2520predominantly%2520linear%2520fashion%2520or%2520that%2520nonlinear%2520structure%2520is%2520small%2520relative%2520to%2520measurement%2520noise.%2520These%2520findings%2520demonstrate%2520that%2520features%2520extracted%2520from%2520routine%2520histopathology%2520may%2520recover%2520spatially%2520coherent%2520and%2520biologically%2520interpretable%2520pathway%2520patterns%252C%2520offering%2520a%2520scalable%2520strategy%2520for%2520integrating%2520image-based%2520inference%2520with%2520ST%2520information%2520in%2520tumor%2520microenvironment%2520studies.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09003v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Digital%20Modeling%20of%20Spatial%20Pathway%20Activity%20from%20Histology%20Reveals%20Tumor%20Microenvironment%20Heterogeneity&entry.906535625=Ling%20Liao%20and%20Changhuei%20Yang%20and%20Maxim%20Artyomov%20and%20Mark%20Watson%20and%20Adam%20Kepecs%20and%20Haowen%20Zhou%20and%20Alexey%20Sergushichev%20and%20Richard%20Cote&entry.1292438233=Spatial%20transcriptomics%20%28ST%29%20enables%20simultaneous%20mapping%20of%20tissue%20morphology%20and%20spatially%20resolved%20gene%20expression%2C%20offering%20unique%20opportunities%20to%20study%20tumor%20microenvironment%20heterogeneity.%20Here%2C%20we%20introduce%20a%20computational%20framework%20that%20predicts%20spatial%20pathway%20activity%20directly%20from%20hematoxylin-and-eosin-stained%20histology%20images%20at%20microscale%20resolution%2055%20and%20100%20um.%20Using%20image%20features%20derived%20from%20a%20computational%20pathology%20foundation%20model%2C%20we%20found%20that%20TGFb%20signaling%20was%20the%20most%20accurately%20predicted%20pathway%20across%20three%20independent%20breast%20and%20lung%20cancer%20ST%20datasets.%20In%2087-88%25%20of%20reliably%20predicted%20cases%2C%20the%20resulting%20spatial%20TGFb%20activity%20maps%20reflected%20the%20expected%20contrast%20between%20tumor%20and%20adjacent%20non-tumor%20regions%2C%20consistent%20with%20the%20known%20role%20of%20TGFb%20in%20regulating%20interactions%20within%20the%20tumor%20microenvironment.%20Notably%2C%20linear%20and%20nonlinear%20predictive%20models%20performed%20similarly%2C%20suggesting%20that%20image%20features%20may%20relate%20to%20pathway%20activity%20in%20a%20predominantly%20linear%20fashion%20or%20that%20nonlinear%20structure%20is%20small%20relative%20to%20measurement%20noise.%20These%20findings%20demonstrate%20that%20features%20extracted%20from%20routine%20histopathology%20may%20recover%20spatially%20coherent%20and%20biologically%20interpretable%20pathway%20patterns%2C%20offering%20a%20scalable%20strategy%20for%20integrating%20image-based%20inference%20with%20ST%20information%20in%20tumor%20microenvironment%20studies.&entry.1838667208=http%3A//arxiv.org/abs/2512.09003v2&entry.124074799=Read"},
{"title": "YOLO11-4K: An Efficient Architecture for Real-Time Small Object Detection in 4K Panoramic Images", "author": "Huma Hafeez and Matthew Garratt and Jo Plested and Sankaran Iyer and Arcot Sowmya", "abstract": "The processing of omnidirectional 360-degree images poses significant challenges for object detection due to inherent spatial distortions, wide fields of view, and ultra-high-resolution inputs. Conventional detectors such as YOLO are optimised for standard image sizes (for example, 640x640 pixels) and often struggle with the computational demands of 4K or higher-resolution imagery typical of 360-degree vision. To address these limitations, we introduce YOLO11-4K, an efficient real-time detection framework tailored for 4K panoramic images. The architecture incorporates a novel multi-scale detection head with a P2 layer to improve sensitivity to small objects often missed at coarser scales, and a GhostConv-based backbone to reduce computational complexity without sacrificing representational power. To enable evaluation, we manually annotated the CVIP360 dataset, generating 6,876 frame-level bounding boxes and producing a publicly available, detection-ready benchmark for 4K panoramic scenes. YOLO11-4K achieves 0.95 mAP at 0.50 IoU with 28.3 milliseconds inference per frame, representing a 75 percent latency reduction compared to YOLO11 (112.3 milliseconds), while also improving accuracy (mAP at 0.50 of 0.95 versus 0.908). This balance of efficiency and precision enables robust object detection in expansive 360-degree environments, making the framework suitable for real-world high-resolution panoramic applications. While this work focuses on 4K omnidirectional images, the approach is broadly applicable to high-resolution detection tasks in autonomous navigation, surveillance, and augmented reality.", "link": "http://arxiv.org/abs/2512.16493v1", "date": "2025-12-18", "relevancy": 2.2349, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5743}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5566}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20YOLO11-4K%3A%20An%20Efficient%20Architecture%20for%20Real-Time%20Small%20Object%20Detection%20in%204K%20Panoramic%20Images&body=Title%3A%20YOLO11-4K%3A%20An%20Efficient%20Architecture%20for%20Real-Time%20Small%20Object%20Detection%20in%204K%20Panoramic%20Images%0AAuthor%3A%20Huma%20Hafeez%20and%20Matthew%20Garratt%20and%20Jo%20Plested%20and%20Sankaran%20Iyer%20and%20Arcot%20Sowmya%0AAbstract%3A%20The%20processing%20of%20omnidirectional%20360-degree%20images%20poses%20significant%20challenges%20for%20object%20detection%20due%20to%20inherent%20spatial%20distortions%2C%20wide%20fields%20of%20view%2C%20and%20ultra-high-resolution%20inputs.%20Conventional%20detectors%20such%20as%20YOLO%20are%20optimised%20for%20standard%20image%20sizes%20%28for%20example%2C%20640x640%20pixels%29%20and%20often%20struggle%20with%20the%20computational%20demands%20of%204K%20or%20higher-resolution%20imagery%20typical%20of%20360-degree%20vision.%20To%20address%20these%20limitations%2C%20we%20introduce%20YOLO11-4K%2C%20an%20efficient%20real-time%20detection%20framework%20tailored%20for%204K%20panoramic%20images.%20The%20architecture%20incorporates%20a%20novel%20multi-scale%20detection%20head%20with%20a%20P2%20layer%20to%20improve%20sensitivity%20to%20small%20objects%20often%20missed%20at%20coarser%20scales%2C%20and%20a%20GhostConv-based%20backbone%20to%20reduce%20computational%20complexity%20without%20sacrificing%20representational%20power.%20To%20enable%20evaluation%2C%20we%20manually%20annotated%20the%20CVIP360%20dataset%2C%20generating%206%2C876%20frame-level%20bounding%20boxes%20and%20producing%20a%20publicly%20available%2C%20detection-ready%20benchmark%20for%204K%20panoramic%20scenes.%20YOLO11-4K%20achieves%200.95%20mAP%20at%200.50%20IoU%20with%2028.3%20milliseconds%20inference%20per%20frame%2C%20representing%20a%2075%20percent%20latency%20reduction%20compared%20to%20YOLO11%20%28112.3%20milliseconds%29%2C%20while%20also%20improving%20accuracy%20%28mAP%20at%200.50%20of%200.95%20versus%200.908%29.%20This%20balance%20of%20efficiency%20and%20precision%20enables%20robust%20object%20detection%20in%20expansive%20360-degree%20environments%2C%20making%20the%20framework%20suitable%20for%20real-world%20high-resolution%20panoramic%20applications.%20While%20this%20work%20focuses%20on%204K%20omnidirectional%20images%2C%20the%20approach%20is%20broadly%20applicable%20to%20high-resolution%20detection%20tasks%20in%20autonomous%20navigation%2C%20surveillance%2C%20and%20augmented%20reality.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16493v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYOLO11-4K%253A%2520An%2520Efficient%2520Architecture%2520for%2520Real-Time%2520Small%2520Object%2520Detection%2520in%25204K%2520Panoramic%2520Images%26entry.906535625%3DHuma%2520Hafeez%2520and%2520Matthew%2520Garratt%2520and%2520Jo%2520Plested%2520and%2520Sankaran%2520Iyer%2520and%2520Arcot%2520Sowmya%26entry.1292438233%3DThe%2520processing%2520of%2520omnidirectional%2520360-degree%2520images%2520poses%2520significant%2520challenges%2520for%2520object%2520detection%2520due%2520to%2520inherent%2520spatial%2520distortions%252C%2520wide%2520fields%2520of%2520view%252C%2520and%2520ultra-high-resolution%2520inputs.%2520Conventional%2520detectors%2520such%2520as%2520YOLO%2520are%2520optimised%2520for%2520standard%2520image%2520sizes%2520%2528for%2520example%252C%2520640x640%2520pixels%2529%2520and%2520often%2520struggle%2520with%2520the%2520computational%2520demands%2520of%25204K%2520or%2520higher-resolution%2520imagery%2520typical%2520of%2520360-degree%2520vision.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520YOLO11-4K%252C%2520an%2520efficient%2520real-time%2520detection%2520framework%2520tailored%2520for%25204K%2520panoramic%2520images.%2520The%2520architecture%2520incorporates%2520a%2520novel%2520multi-scale%2520detection%2520head%2520with%2520a%2520P2%2520layer%2520to%2520improve%2520sensitivity%2520to%2520small%2520objects%2520often%2520missed%2520at%2520coarser%2520scales%252C%2520and%2520a%2520GhostConv-based%2520backbone%2520to%2520reduce%2520computational%2520complexity%2520without%2520sacrificing%2520representational%2520power.%2520To%2520enable%2520evaluation%252C%2520we%2520manually%2520annotated%2520the%2520CVIP360%2520dataset%252C%2520generating%25206%252C876%2520frame-level%2520bounding%2520boxes%2520and%2520producing%2520a%2520publicly%2520available%252C%2520detection-ready%2520benchmark%2520for%25204K%2520panoramic%2520scenes.%2520YOLO11-4K%2520achieves%25200.95%2520mAP%2520at%25200.50%2520IoU%2520with%252028.3%2520milliseconds%2520inference%2520per%2520frame%252C%2520representing%2520a%252075%2520percent%2520latency%2520reduction%2520compared%2520to%2520YOLO11%2520%2528112.3%2520milliseconds%2529%252C%2520while%2520also%2520improving%2520accuracy%2520%2528mAP%2520at%25200.50%2520of%25200.95%2520versus%25200.908%2529.%2520This%2520balance%2520of%2520efficiency%2520and%2520precision%2520enables%2520robust%2520object%2520detection%2520in%2520expansive%2520360-degree%2520environments%252C%2520making%2520the%2520framework%2520suitable%2520for%2520real-world%2520high-resolution%2520panoramic%2520applications.%2520While%2520this%2520work%2520focuses%2520on%25204K%2520omnidirectional%2520images%252C%2520the%2520approach%2520is%2520broadly%2520applicable%2520to%2520high-resolution%2520detection%2520tasks%2520in%2520autonomous%2520navigation%252C%2520surveillance%252C%2520and%2520augmented%2520reality.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16493v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YOLO11-4K%3A%20An%20Efficient%20Architecture%20for%20Real-Time%20Small%20Object%20Detection%20in%204K%20Panoramic%20Images&entry.906535625=Huma%20Hafeez%20and%20Matthew%20Garratt%20and%20Jo%20Plested%20and%20Sankaran%20Iyer%20and%20Arcot%20Sowmya&entry.1292438233=The%20processing%20of%20omnidirectional%20360-degree%20images%20poses%20significant%20challenges%20for%20object%20detection%20due%20to%20inherent%20spatial%20distortions%2C%20wide%20fields%20of%20view%2C%20and%20ultra-high-resolution%20inputs.%20Conventional%20detectors%20such%20as%20YOLO%20are%20optimised%20for%20standard%20image%20sizes%20%28for%20example%2C%20640x640%20pixels%29%20and%20often%20struggle%20with%20the%20computational%20demands%20of%204K%20or%20higher-resolution%20imagery%20typical%20of%20360-degree%20vision.%20To%20address%20these%20limitations%2C%20we%20introduce%20YOLO11-4K%2C%20an%20efficient%20real-time%20detection%20framework%20tailored%20for%204K%20panoramic%20images.%20The%20architecture%20incorporates%20a%20novel%20multi-scale%20detection%20head%20with%20a%20P2%20layer%20to%20improve%20sensitivity%20to%20small%20objects%20often%20missed%20at%20coarser%20scales%2C%20and%20a%20GhostConv-based%20backbone%20to%20reduce%20computational%20complexity%20without%20sacrificing%20representational%20power.%20To%20enable%20evaluation%2C%20we%20manually%20annotated%20the%20CVIP360%20dataset%2C%20generating%206%2C876%20frame-level%20bounding%20boxes%20and%20producing%20a%20publicly%20available%2C%20detection-ready%20benchmark%20for%204K%20panoramic%20scenes.%20YOLO11-4K%20achieves%200.95%20mAP%20at%200.50%20IoU%20with%2028.3%20milliseconds%20inference%20per%20frame%2C%20representing%20a%2075%20percent%20latency%20reduction%20compared%20to%20YOLO11%20%28112.3%20milliseconds%29%2C%20while%20also%20improving%20accuracy%20%28mAP%20at%200.50%20of%200.95%20versus%200.908%29.%20This%20balance%20of%20efficiency%20and%20precision%20enables%20robust%20object%20detection%20in%20expansive%20360-degree%20environments%2C%20making%20the%20framework%20suitable%20for%20real-world%20high-resolution%20panoramic%20applications.%20While%20this%20work%20focuses%20on%204K%20omnidirectional%20images%2C%20the%20approach%20is%20broadly%20applicable%20to%20high-resolution%20detection%20tasks%20in%20autonomous%20navigation%2C%20surveillance%2C%20and%20augmented%20reality.&entry.1838667208=http%3A//arxiv.org/abs/2512.16493v1&entry.124074799=Read"},
{"title": "Persistent Multiscale Density-based Clustering", "author": "Dani\u00ebl Bot and Leland McInnes and Jan Aerts", "abstract": "Clustering is a cornerstone of modern data analysis. Detecting clusters in exploratory data analyses (EDA) requires algorithms that make few assumptions about the data. Density-based clustering algorithms are particularly well-suited for EDA because they describe high-density regions, assuming only that a density exists. Applying density-based clustering algorithms in practice, however, requires selecting appropriate hyperparameters, which is difficult without prior knowledge of the data distribution. For example, DBSCAN requires selecting a density threshold, and HDBSCAN* relies on a minimum cluster size parameter. In this work, we propose Persistent Leaves Spatial Clustering for Applications with Noise (PLSCAN). This novel density-based clustering algorithm efficiently identifies all minimum cluster sizes for which HDBSCAN* produces stable (leaf) clusters. PLSCAN applies scale-space clustering principles and is equivalent to persistent homology on a novel metric space. We compare its performance to HDBSCAN* on several real-world datasets, demonstrating that it achieves a higher average ARI and is less sensitive to changes in the number of mutual reachability neighbours. Additionally, we compare PLSCAN's computational costs to k-Means, demonstrating competitive run-times on low-dimensional datasets. At higher dimensions, run times scale more similarly to HDBSCAN*.", "link": "http://arxiv.org/abs/2512.16558v1", "date": "2025-12-18", "relevancy": 2.2348, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4609}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4423}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Persistent%20Multiscale%20Density-based%20Clustering&body=Title%3A%20Persistent%20Multiscale%20Density-based%20Clustering%0AAuthor%3A%20Dani%C3%ABl%20Bot%20and%20Leland%20McInnes%20and%20Jan%20Aerts%0AAbstract%3A%20Clustering%20is%20a%20cornerstone%20of%20modern%20data%20analysis.%20Detecting%20clusters%20in%20exploratory%20data%20analyses%20%28EDA%29%20requires%20algorithms%20that%20make%20few%20assumptions%20about%20the%20data.%20Density-based%20clustering%20algorithms%20are%20particularly%20well-suited%20for%20EDA%20because%20they%20describe%20high-density%20regions%2C%20assuming%20only%20that%20a%20density%20exists.%20Applying%20density-based%20clustering%20algorithms%20in%20practice%2C%20however%2C%20requires%20selecting%20appropriate%20hyperparameters%2C%20which%20is%20difficult%20without%20prior%20knowledge%20of%20the%20data%20distribution.%20For%20example%2C%20DBSCAN%20requires%20selecting%20a%20density%20threshold%2C%20and%20HDBSCAN%2A%20relies%20on%20a%20minimum%20cluster%20size%20parameter.%20In%20this%20work%2C%20we%20propose%20Persistent%20Leaves%20Spatial%20Clustering%20for%20Applications%20with%20Noise%20%28PLSCAN%29.%20This%20novel%20density-based%20clustering%20algorithm%20efficiently%20identifies%20all%20minimum%20cluster%20sizes%20for%20which%20HDBSCAN%2A%20produces%20stable%20%28leaf%29%20clusters.%20PLSCAN%20applies%20scale-space%20clustering%20principles%20and%20is%20equivalent%20to%20persistent%20homology%20on%20a%20novel%20metric%20space.%20We%20compare%20its%20performance%20to%20HDBSCAN%2A%20on%20several%20real-world%20datasets%2C%20demonstrating%20that%20it%20achieves%20a%20higher%20average%20ARI%20and%20is%20less%20sensitive%20to%20changes%20in%20the%20number%20of%20mutual%20reachability%20neighbours.%20Additionally%2C%20we%20compare%20PLSCAN%27s%20computational%20costs%20to%20k-Means%2C%20demonstrating%20competitive%20run-times%20on%20low-dimensional%20datasets.%20At%20higher%20dimensions%2C%20run%20times%20scale%20more%20similarly%20to%20HDBSCAN%2A.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16558v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersistent%2520Multiscale%2520Density-based%2520Clustering%26entry.906535625%3DDani%25C3%25ABl%2520Bot%2520and%2520Leland%2520McInnes%2520and%2520Jan%2520Aerts%26entry.1292438233%3DClustering%2520is%2520a%2520cornerstone%2520of%2520modern%2520data%2520analysis.%2520Detecting%2520clusters%2520in%2520exploratory%2520data%2520analyses%2520%2528EDA%2529%2520requires%2520algorithms%2520that%2520make%2520few%2520assumptions%2520about%2520the%2520data.%2520Density-based%2520clustering%2520algorithms%2520are%2520particularly%2520well-suited%2520for%2520EDA%2520because%2520they%2520describe%2520high-density%2520regions%252C%2520assuming%2520only%2520that%2520a%2520density%2520exists.%2520Applying%2520density-based%2520clustering%2520algorithms%2520in%2520practice%252C%2520however%252C%2520requires%2520selecting%2520appropriate%2520hyperparameters%252C%2520which%2520is%2520difficult%2520without%2520prior%2520knowledge%2520of%2520the%2520data%2520distribution.%2520For%2520example%252C%2520DBSCAN%2520requires%2520selecting%2520a%2520density%2520threshold%252C%2520and%2520HDBSCAN%252A%2520relies%2520on%2520a%2520minimum%2520cluster%2520size%2520parameter.%2520In%2520this%2520work%252C%2520we%2520propose%2520Persistent%2520Leaves%2520Spatial%2520Clustering%2520for%2520Applications%2520with%2520Noise%2520%2528PLSCAN%2529.%2520This%2520novel%2520density-based%2520clustering%2520algorithm%2520efficiently%2520identifies%2520all%2520minimum%2520cluster%2520sizes%2520for%2520which%2520HDBSCAN%252A%2520produces%2520stable%2520%2528leaf%2529%2520clusters.%2520PLSCAN%2520applies%2520scale-space%2520clustering%2520principles%2520and%2520is%2520equivalent%2520to%2520persistent%2520homology%2520on%2520a%2520novel%2520metric%2520space.%2520We%2520compare%2520its%2520performance%2520to%2520HDBSCAN%252A%2520on%2520several%2520real-world%2520datasets%252C%2520demonstrating%2520that%2520it%2520achieves%2520a%2520higher%2520average%2520ARI%2520and%2520is%2520less%2520sensitive%2520to%2520changes%2520in%2520the%2520number%2520of%2520mutual%2520reachability%2520neighbours.%2520Additionally%252C%2520we%2520compare%2520PLSCAN%2527s%2520computational%2520costs%2520to%2520k-Means%252C%2520demonstrating%2520competitive%2520run-times%2520on%2520low-dimensional%2520datasets.%2520At%2520higher%2520dimensions%252C%2520run%2520times%2520scale%2520more%2520similarly%2520to%2520HDBSCAN%252A.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16558v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Persistent%20Multiscale%20Density-based%20Clustering&entry.906535625=Dani%C3%ABl%20Bot%20and%20Leland%20McInnes%20and%20Jan%20Aerts&entry.1292438233=Clustering%20is%20a%20cornerstone%20of%20modern%20data%20analysis.%20Detecting%20clusters%20in%20exploratory%20data%20analyses%20%28EDA%29%20requires%20algorithms%20that%20make%20few%20assumptions%20about%20the%20data.%20Density-based%20clustering%20algorithms%20are%20particularly%20well-suited%20for%20EDA%20because%20they%20describe%20high-density%20regions%2C%20assuming%20only%20that%20a%20density%20exists.%20Applying%20density-based%20clustering%20algorithms%20in%20practice%2C%20however%2C%20requires%20selecting%20appropriate%20hyperparameters%2C%20which%20is%20difficult%20without%20prior%20knowledge%20of%20the%20data%20distribution.%20For%20example%2C%20DBSCAN%20requires%20selecting%20a%20density%20threshold%2C%20and%20HDBSCAN%2A%20relies%20on%20a%20minimum%20cluster%20size%20parameter.%20In%20this%20work%2C%20we%20propose%20Persistent%20Leaves%20Spatial%20Clustering%20for%20Applications%20with%20Noise%20%28PLSCAN%29.%20This%20novel%20density-based%20clustering%20algorithm%20efficiently%20identifies%20all%20minimum%20cluster%20sizes%20for%20which%20HDBSCAN%2A%20produces%20stable%20%28leaf%29%20clusters.%20PLSCAN%20applies%20scale-space%20clustering%20principles%20and%20is%20equivalent%20to%20persistent%20homology%20on%20a%20novel%20metric%20space.%20We%20compare%20its%20performance%20to%20HDBSCAN%2A%20on%20several%20real-world%20datasets%2C%20demonstrating%20that%20it%20achieves%20a%20higher%20average%20ARI%20and%20is%20less%20sensitive%20to%20changes%20in%20the%20number%20of%20mutual%20reachability%20neighbours.%20Additionally%2C%20we%20compare%20PLSCAN%27s%20computational%20costs%20to%20k-Means%2C%20demonstrating%20competitive%20run-times%20on%20low-dimensional%20datasets.%20At%20higher%20dimensions%2C%20run%20times%20scale%20more%20similarly%20to%20HDBSCAN%2A.&entry.1838667208=http%3A//arxiv.org/abs/2512.16558v1&entry.124074799=Read"},
{"title": "Memory Backdoor Attacks on Neural Networks", "author": "Eden Luzon and Guy Amit and Roy Weiss and Torsten Kraub and Alexandra Dmitrienko and Yisroel Mirsky", "abstract": "Neural networks are often trained on proprietary datasets, making them attractive attack targets. We present a novel dataset extraction method leveraging an innovative training time backdoor attack, allowing a malicious federated learning server to systematically and deterministically extract complete client training samples through a simple indexing process. Unlike prior techniques, our approach guarantees exact data recovery rather than probabilistic reconstructions or hallucinations, provides precise control over which samples are memorized and how many, and shows high capacity and robustness. Infected models output data samples when they receive a patternbased index trigger, enabling systematic extraction of meaningful patches from each clients local data without disrupting global model utility. To address small model output sizes, we extract patches and then recombined them. The attack requires only a minor modification to the training code that can easily evade detection during client-side verification. Hence, this vulnerability represents a realistic FL supply-chain threat, where a malicious server can distribute modified training code to clients and later recover private data from their updates. Evaluations across classifiers, segmentation models, and large language models demonstrate that thousands of sensitive training samples can be recovered from client models with minimal impact on task performance, and a clients entire dataset can be stolen after multiple FL rounds. For instance, a medical segmentation dataset can be extracted with only a 3 percent utility drop. These findings expose a critical privacy vulnerability in FL systems, emphasizing the need for stronger integrity and transparency in distributed training pipelines.", "link": "http://arxiv.org/abs/2411.14516v2", "date": "2025-12-18", "relevancy": 1.9776, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4992}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4971}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memory%20Backdoor%20Attacks%20on%20Neural%20Networks&body=Title%3A%20Memory%20Backdoor%20Attacks%20on%20Neural%20Networks%0AAuthor%3A%20Eden%20Luzon%20and%20Guy%20Amit%20and%20Roy%20Weiss%20and%20Torsten%20Kraub%20and%20Alexandra%20Dmitrienko%20and%20Yisroel%20Mirsky%0AAbstract%3A%20Neural%20networks%20are%20often%20trained%20on%20proprietary%20datasets%2C%20making%20them%20attractive%20attack%20targets.%20We%20present%20a%20novel%20dataset%20extraction%20method%20leveraging%20an%20innovative%20training%20time%20backdoor%20attack%2C%20allowing%20a%20malicious%20federated%20learning%20server%20to%20systematically%20and%20deterministically%20extract%20complete%20client%20training%20samples%20through%20a%20simple%20indexing%20process.%20Unlike%20prior%20techniques%2C%20our%20approach%20guarantees%20exact%20data%20recovery%20rather%20than%20probabilistic%20reconstructions%20or%20hallucinations%2C%20provides%20precise%20control%20over%20which%20samples%20are%20memorized%20and%20how%20many%2C%20and%20shows%20high%20capacity%20and%20robustness.%20Infected%20models%20output%20data%20samples%20when%20they%20receive%20a%20patternbased%20index%20trigger%2C%20enabling%20systematic%20extraction%20of%20meaningful%20patches%20from%20each%20clients%20local%20data%20without%20disrupting%20global%20model%20utility.%20To%20address%20small%20model%20output%20sizes%2C%20we%20extract%20patches%20and%20then%20recombined%20them.%20The%20attack%20requires%20only%20a%20minor%20modification%20to%20the%20training%20code%20that%20can%20easily%20evade%20detection%20during%20client-side%20verification.%20Hence%2C%20this%20vulnerability%20represents%20a%20realistic%20FL%20supply-chain%20threat%2C%20where%20a%20malicious%20server%20can%20distribute%20modified%20training%20code%20to%20clients%20and%20later%20recover%20private%20data%20from%20their%20updates.%20Evaluations%20across%20classifiers%2C%20segmentation%20models%2C%20and%20large%20language%20models%20demonstrate%20that%20thousands%20of%20sensitive%20training%20samples%20can%20be%20recovered%20from%20client%20models%20with%20minimal%20impact%20on%20task%20performance%2C%20and%20a%20clients%20entire%20dataset%20can%20be%20stolen%20after%20multiple%20FL%20rounds.%20For%20instance%2C%20a%20medical%20segmentation%20dataset%20can%20be%20extracted%20with%20only%20a%203%20percent%20utility%20drop.%20These%20findings%20expose%20a%20critical%20privacy%20vulnerability%20in%20FL%20systems%2C%20emphasizing%20the%20need%20for%20stronger%20integrity%20and%20transparency%20in%20distributed%20training%20pipelines.%0ALink%3A%20http%3A//arxiv.org/abs/2411.14516v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemory%2520Backdoor%2520Attacks%2520on%2520Neural%2520Networks%26entry.906535625%3DEden%2520Luzon%2520and%2520Guy%2520Amit%2520and%2520Roy%2520Weiss%2520and%2520Torsten%2520Kraub%2520and%2520Alexandra%2520Dmitrienko%2520and%2520Yisroel%2520Mirsky%26entry.1292438233%3DNeural%2520networks%2520are%2520often%2520trained%2520on%2520proprietary%2520datasets%252C%2520making%2520them%2520attractive%2520attack%2520targets.%2520We%2520present%2520a%2520novel%2520dataset%2520extraction%2520method%2520leveraging%2520an%2520innovative%2520training%2520time%2520backdoor%2520attack%252C%2520allowing%2520a%2520malicious%2520federated%2520learning%2520server%2520to%2520systematically%2520and%2520deterministically%2520extract%2520complete%2520client%2520training%2520samples%2520through%2520a%2520simple%2520indexing%2520process.%2520Unlike%2520prior%2520techniques%252C%2520our%2520approach%2520guarantees%2520exact%2520data%2520recovery%2520rather%2520than%2520probabilistic%2520reconstructions%2520or%2520hallucinations%252C%2520provides%2520precise%2520control%2520over%2520which%2520samples%2520are%2520memorized%2520and%2520how%2520many%252C%2520and%2520shows%2520high%2520capacity%2520and%2520robustness.%2520Infected%2520models%2520output%2520data%2520samples%2520when%2520they%2520receive%2520a%2520patternbased%2520index%2520trigger%252C%2520enabling%2520systematic%2520extraction%2520of%2520meaningful%2520patches%2520from%2520each%2520clients%2520local%2520data%2520without%2520disrupting%2520global%2520model%2520utility.%2520To%2520address%2520small%2520model%2520output%2520sizes%252C%2520we%2520extract%2520patches%2520and%2520then%2520recombined%2520them.%2520The%2520attack%2520requires%2520only%2520a%2520minor%2520modification%2520to%2520the%2520training%2520code%2520that%2520can%2520easily%2520evade%2520detection%2520during%2520client-side%2520verification.%2520Hence%252C%2520this%2520vulnerability%2520represents%2520a%2520realistic%2520FL%2520supply-chain%2520threat%252C%2520where%2520a%2520malicious%2520server%2520can%2520distribute%2520modified%2520training%2520code%2520to%2520clients%2520and%2520later%2520recover%2520private%2520data%2520from%2520their%2520updates.%2520Evaluations%2520across%2520classifiers%252C%2520segmentation%2520models%252C%2520and%2520large%2520language%2520models%2520demonstrate%2520that%2520thousands%2520of%2520sensitive%2520training%2520samples%2520can%2520be%2520recovered%2520from%2520client%2520models%2520with%2520minimal%2520impact%2520on%2520task%2520performance%252C%2520and%2520a%2520clients%2520entire%2520dataset%2520can%2520be%2520stolen%2520after%2520multiple%2520FL%2520rounds.%2520For%2520instance%252C%2520a%2520medical%2520segmentation%2520dataset%2520can%2520be%2520extracted%2520with%2520only%2520a%25203%2520percent%2520utility%2520drop.%2520These%2520findings%2520expose%2520a%2520critical%2520privacy%2520vulnerability%2520in%2520FL%2520systems%252C%2520emphasizing%2520the%2520need%2520for%2520stronger%2520integrity%2520and%2520transparency%2520in%2520distributed%2520training%2520pipelines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14516v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memory%20Backdoor%20Attacks%20on%20Neural%20Networks&entry.906535625=Eden%20Luzon%20and%20Guy%20Amit%20and%20Roy%20Weiss%20and%20Torsten%20Kraub%20and%20Alexandra%20Dmitrienko%20and%20Yisroel%20Mirsky&entry.1292438233=Neural%20networks%20are%20often%20trained%20on%20proprietary%20datasets%2C%20making%20them%20attractive%20attack%20targets.%20We%20present%20a%20novel%20dataset%20extraction%20method%20leveraging%20an%20innovative%20training%20time%20backdoor%20attack%2C%20allowing%20a%20malicious%20federated%20learning%20server%20to%20systematically%20and%20deterministically%20extract%20complete%20client%20training%20samples%20through%20a%20simple%20indexing%20process.%20Unlike%20prior%20techniques%2C%20our%20approach%20guarantees%20exact%20data%20recovery%20rather%20than%20probabilistic%20reconstructions%20or%20hallucinations%2C%20provides%20precise%20control%20over%20which%20samples%20are%20memorized%20and%20how%20many%2C%20and%20shows%20high%20capacity%20and%20robustness.%20Infected%20models%20output%20data%20samples%20when%20they%20receive%20a%20patternbased%20index%20trigger%2C%20enabling%20systematic%20extraction%20of%20meaningful%20patches%20from%20each%20clients%20local%20data%20without%20disrupting%20global%20model%20utility.%20To%20address%20small%20model%20output%20sizes%2C%20we%20extract%20patches%20and%20then%20recombined%20them.%20The%20attack%20requires%20only%20a%20minor%20modification%20to%20the%20training%20code%20that%20can%20easily%20evade%20detection%20during%20client-side%20verification.%20Hence%2C%20this%20vulnerability%20represents%20a%20realistic%20FL%20supply-chain%20threat%2C%20where%20a%20malicious%20server%20can%20distribute%20modified%20training%20code%20to%20clients%20and%20later%20recover%20private%20data%20from%20their%20updates.%20Evaluations%20across%20classifiers%2C%20segmentation%20models%2C%20and%20large%20language%20models%20demonstrate%20that%20thousands%20of%20sensitive%20training%20samples%20can%20be%20recovered%20from%20client%20models%20with%20minimal%20impact%20on%20task%20performance%2C%20and%20a%20clients%20entire%20dataset%20can%20be%20stolen%20after%20multiple%20FL%20rounds.%20For%20instance%2C%20a%20medical%20segmentation%20dataset%20can%20be%20extracted%20with%20only%20a%203%20percent%20utility%20drop.%20These%20findings%20expose%20a%20critical%20privacy%20vulnerability%20in%20FL%20systems%2C%20emphasizing%20the%20need%20for%20stronger%20integrity%20and%20transparency%20in%20distributed%20training%20pipelines.&entry.1838667208=http%3A//arxiv.org/abs/2411.14516v2&entry.124074799=Read"},
{"title": "Beyond Rate Coding: Surrogate Gradients Enable Spike Timing Learning in Spiking Neural Networks", "author": "Ziqiao Yu and Pengfei Sun and Danyal Akarca and Dan F. M. Goodman", "abstract": "The surrogate gradient descent algorithm enabled spiking neural networks to be trained to carry out challenging sensory processing tasks, an important step in understanding how spikes contribute to neural computations. However, it is unclear the extent to which these algorithms fully explore the space of possible spiking solutions to problems. We investigated whether spiking networks trained with surrogate gradient descent can learn to make use of information that is only encoded in the timing and not the rate of spikes. We constructed synthetic datasets with a range of types of spike timing information (interspike intervals, spatio-temporal spike patterns or polychrony, and coincidence codes). We find that surrogate gradient descent training can extract all of these types of information. In more realistic speech-based datasets, both timing and rate information is present. We therefore constructed variants of these datasets in which all rate information is removed, and find that surrogate gradient descent can still perform well. We tested all networks both with and without trainable axonal delays. We find that delays can give a significant increase in performance, particularly for more challenging tasks. To determine what types of spike timing information are being used by the networks trained on the speech-based tasks, we test these networks on time-reversed spikes which perturb spatio-temporal spike patterns but leave interspike intervals and coincidence information unchanged. We find that when axonal delays are not used, networks perform well under time reversal, whereas networks trained with delays perform poorly. This suggests that spiking neural networks with delays are better able to exploit temporal structure. To facilitate further studies of temporal coding, we have released our modified speech-based datasets.", "link": "http://arxiv.org/abs/2507.16043v3", "date": "2025-12-18", "relevancy": 1.8935, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4965}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4603}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Rate%20Coding%3A%20Surrogate%20Gradients%20Enable%20Spike%20Timing%20Learning%20in%20Spiking%20Neural%20Networks&body=Title%3A%20Beyond%20Rate%20Coding%3A%20Surrogate%20Gradients%20Enable%20Spike%20Timing%20Learning%20in%20Spiking%20Neural%20Networks%0AAuthor%3A%20Ziqiao%20Yu%20and%20Pengfei%20Sun%20and%20Danyal%20Akarca%20and%20Dan%20F.%20M.%20Goodman%0AAbstract%3A%20The%20surrogate%20gradient%20descent%20algorithm%20enabled%20spiking%20neural%20networks%20to%20be%20trained%20to%20carry%20out%20challenging%20sensory%20processing%20tasks%2C%20an%20important%20step%20in%20understanding%20how%20spikes%20contribute%20to%20neural%20computations.%20However%2C%20it%20is%20unclear%20the%20extent%20to%20which%20these%20algorithms%20fully%20explore%20the%20space%20of%20possible%20spiking%20solutions%20to%20problems.%20We%20investigated%20whether%20spiking%20networks%20trained%20with%20surrogate%20gradient%20descent%20can%20learn%20to%20make%20use%20of%20information%20that%20is%20only%20encoded%20in%20the%20timing%20and%20not%20the%20rate%20of%20spikes.%20We%20constructed%20synthetic%20datasets%20with%20a%20range%20of%20types%20of%20spike%20timing%20information%20%28interspike%20intervals%2C%20spatio-temporal%20spike%20patterns%20or%20polychrony%2C%20and%20coincidence%20codes%29.%20We%20find%20that%20surrogate%20gradient%20descent%20training%20can%20extract%20all%20of%20these%20types%20of%20information.%20In%20more%20realistic%20speech-based%20datasets%2C%20both%20timing%20and%20rate%20information%20is%20present.%20We%20therefore%20constructed%20variants%20of%20these%20datasets%20in%20which%20all%20rate%20information%20is%20removed%2C%20and%20find%20that%20surrogate%20gradient%20descent%20can%20still%20perform%20well.%20We%20tested%20all%20networks%20both%20with%20and%20without%20trainable%20axonal%20delays.%20We%20find%20that%20delays%20can%20give%20a%20significant%20increase%20in%20performance%2C%20particularly%20for%20more%20challenging%20tasks.%20To%20determine%20what%20types%20of%20spike%20timing%20information%20are%20being%20used%20by%20the%20networks%20trained%20on%20the%20speech-based%20tasks%2C%20we%20test%20these%20networks%20on%20time-reversed%20spikes%20which%20perturb%20spatio-temporal%20spike%20patterns%20but%20leave%20interspike%20intervals%20and%20coincidence%20information%20unchanged.%20We%20find%20that%20when%20axonal%20delays%20are%20not%20used%2C%20networks%20perform%20well%20under%20time%20reversal%2C%20whereas%20networks%20trained%20with%20delays%20perform%20poorly.%20This%20suggests%20that%20spiking%20neural%20networks%20with%20delays%20are%20better%20able%20to%20exploit%20temporal%20structure.%20To%20facilitate%20further%20studies%20of%20temporal%20coding%2C%20we%20have%20released%20our%20modified%20speech-based%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2507.16043v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Rate%2520Coding%253A%2520Surrogate%2520Gradients%2520Enable%2520Spike%2520Timing%2520Learning%2520in%2520Spiking%2520Neural%2520Networks%26entry.906535625%3DZiqiao%2520Yu%2520and%2520Pengfei%2520Sun%2520and%2520Danyal%2520Akarca%2520and%2520Dan%2520F.%2520M.%2520Goodman%26entry.1292438233%3DThe%2520surrogate%2520gradient%2520descent%2520algorithm%2520enabled%2520spiking%2520neural%2520networks%2520to%2520be%2520trained%2520to%2520carry%2520out%2520challenging%2520sensory%2520processing%2520tasks%252C%2520an%2520important%2520step%2520in%2520understanding%2520how%2520spikes%2520contribute%2520to%2520neural%2520computations.%2520However%252C%2520it%2520is%2520unclear%2520the%2520extent%2520to%2520which%2520these%2520algorithms%2520fully%2520explore%2520the%2520space%2520of%2520possible%2520spiking%2520solutions%2520to%2520problems.%2520We%2520investigated%2520whether%2520spiking%2520networks%2520trained%2520with%2520surrogate%2520gradient%2520descent%2520can%2520learn%2520to%2520make%2520use%2520of%2520information%2520that%2520is%2520only%2520encoded%2520in%2520the%2520timing%2520and%2520not%2520the%2520rate%2520of%2520spikes.%2520We%2520constructed%2520synthetic%2520datasets%2520with%2520a%2520range%2520of%2520types%2520of%2520spike%2520timing%2520information%2520%2528interspike%2520intervals%252C%2520spatio-temporal%2520spike%2520patterns%2520or%2520polychrony%252C%2520and%2520coincidence%2520codes%2529.%2520We%2520find%2520that%2520surrogate%2520gradient%2520descent%2520training%2520can%2520extract%2520all%2520of%2520these%2520types%2520of%2520information.%2520In%2520more%2520realistic%2520speech-based%2520datasets%252C%2520both%2520timing%2520and%2520rate%2520information%2520is%2520present.%2520We%2520therefore%2520constructed%2520variants%2520of%2520these%2520datasets%2520in%2520which%2520all%2520rate%2520information%2520is%2520removed%252C%2520and%2520find%2520that%2520surrogate%2520gradient%2520descent%2520can%2520still%2520perform%2520well.%2520We%2520tested%2520all%2520networks%2520both%2520with%2520and%2520without%2520trainable%2520axonal%2520delays.%2520We%2520find%2520that%2520delays%2520can%2520give%2520a%2520significant%2520increase%2520in%2520performance%252C%2520particularly%2520for%2520more%2520challenging%2520tasks.%2520To%2520determine%2520what%2520types%2520of%2520spike%2520timing%2520information%2520are%2520being%2520used%2520by%2520the%2520networks%2520trained%2520on%2520the%2520speech-based%2520tasks%252C%2520we%2520test%2520these%2520networks%2520on%2520time-reversed%2520spikes%2520which%2520perturb%2520spatio-temporal%2520spike%2520patterns%2520but%2520leave%2520interspike%2520intervals%2520and%2520coincidence%2520information%2520unchanged.%2520We%2520find%2520that%2520when%2520axonal%2520delays%2520are%2520not%2520used%252C%2520networks%2520perform%2520well%2520under%2520time%2520reversal%252C%2520whereas%2520networks%2520trained%2520with%2520delays%2520perform%2520poorly.%2520This%2520suggests%2520that%2520spiking%2520neural%2520networks%2520with%2520delays%2520are%2520better%2520able%2520to%2520exploit%2520temporal%2520structure.%2520To%2520facilitate%2520further%2520studies%2520of%2520temporal%2520coding%252C%2520we%2520have%2520released%2520our%2520modified%2520speech-based%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16043v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Rate%20Coding%3A%20Surrogate%20Gradients%20Enable%20Spike%20Timing%20Learning%20in%20Spiking%20Neural%20Networks&entry.906535625=Ziqiao%20Yu%20and%20Pengfei%20Sun%20and%20Danyal%20Akarca%20and%20Dan%20F.%20M.%20Goodman&entry.1292438233=The%20surrogate%20gradient%20descent%20algorithm%20enabled%20spiking%20neural%20networks%20to%20be%20trained%20to%20carry%20out%20challenging%20sensory%20processing%20tasks%2C%20an%20important%20step%20in%20understanding%20how%20spikes%20contribute%20to%20neural%20computations.%20However%2C%20it%20is%20unclear%20the%20extent%20to%20which%20these%20algorithms%20fully%20explore%20the%20space%20of%20possible%20spiking%20solutions%20to%20problems.%20We%20investigated%20whether%20spiking%20networks%20trained%20with%20surrogate%20gradient%20descent%20can%20learn%20to%20make%20use%20of%20information%20that%20is%20only%20encoded%20in%20the%20timing%20and%20not%20the%20rate%20of%20spikes.%20We%20constructed%20synthetic%20datasets%20with%20a%20range%20of%20types%20of%20spike%20timing%20information%20%28interspike%20intervals%2C%20spatio-temporal%20spike%20patterns%20or%20polychrony%2C%20and%20coincidence%20codes%29.%20We%20find%20that%20surrogate%20gradient%20descent%20training%20can%20extract%20all%20of%20these%20types%20of%20information.%20In%20more%20realistic%20speech-based%20datasets%2C%20both%20timing%20and%20rate%20information%20is%20present.%20We%20therefore%20constructed%20variants%20of%20these%20datasets%20in%20which%20all%20rate%20information%20is%20removed%2C%20and%20find%20that%20surrogate%20gradient%20descent%20can%20still%20perform%20well.%20We%20tested%20all%20networks%20both%20with%20and%20without%20trainable%20axonal%20delays.%20We%20find%20that%20delays%20can%20give%20a%20significant%20increase%20in%20performance%2C%20particularly%20for%20more%20challenging%20tasks.%20To%20determine%20what%20types%20of%20spike%20timing%20information%20are%20being%20used%20by%20the%20networks%20trained%20on%20the%20speech-based%20tasks%2C%20we%20test%20these%20networks%20on%20time-reversed%20spikes%20which%20perturb%20spatio-temporal%20spike%20patterns%20but%20leave%20interspike%20intervals%20and%20coincidence%20information%20unchanged.%20We%20find%20that%20when%20axonal%20delays%20are%20not%20used%2C%20networks%20perform%20well%20under%20time%20reversal%2C%20whereas%20networks%20trained%20with%20delays%20perform%20poorly.%20This%20suggests%20that%20spiking%20neural%20networks%20with%20delays%20are%20better%20able%20to%20exploit%20temporal%20structure.%20To%20facilitate%20further%20studies%20of%20temporal%20coding%2C%20we%20have%20released%20our%20modified%20speech-based%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2507.16043v3&entry.124074799=Read"},
{"title": "DCFO: Density-Based Counterfactuals for Outliers - Additional Material", "author": "Tommaso Amico and Pernille Matthews and Lena Krieger and Arthur Zimek and Ira Assent", "abstract": "Outlier detection identifies data points that significantly deviate from the majority of the data distribution. Explaining outliers is crucial for understanding the underlying factors that contribute to their detection, validating their significance, and identifying potential biases or errors. Effective explanations provide actionable insights, facilitating preventive measures to avoid similar outliers in the future. Counterfactual explanations clarify why specific data points are classified as outliers by identifying minimal changes required to alter their prediction. Although valuable, most existing counterfactual explanation methods overlook the unique challenges posed by outlier detection, and fail to target classical, widely adopted outlier detection algorithms. Local Outlier Factor (LOF) is one the most popular unsupervised outlier detection methods, quantifying outlierness through relative local density. Despite LOF's widespread use across diverse applications, it lacks interpretability. To address this limitation, we introduce Density-based Counterfactuals for Outliers (DCFO), a novel method specifically designed to generate counterfactual explanations for LOF. DCFO partitions the data space into regions where LOF behaves smoothly, enabling efficient gradient-based optimisation. Extensive experimental validation on 50 OpenML datasets demonstrates that DCFO consistently outperforms benchmarked competitors, offering superior proximity and validity of generated counterfactuals.", "link": "http://arxiv.org/abs/2512.10659v2", "date": "2025-12-18", "relevancy": 1.3088, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4476}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4353}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DCFO%3A%20Density-Based%20Counterfactuals%20for%20Outliers%20-%20Additional%20Material&body=Title%3A%20DCFO%3A%20Density-Based%20Counterfactuals%20for%20Outliers%20-%20Additional%20Material%0AAuthor%3A%20Tommaso%20Amico%20and%20Pernille%20Matthews%20and%20Lena%20Krieger%20and%20Arthur%20Zimek%20and%20Ira%20Assent%0AAbstract%3A%20Outlier%20detection%20identifies%20data%20points%20that%20significantly%20deviate%20from%20the%20majority%20of%20the%20data%20distribution.%20Explaining%20outliers%20is%20crucial%20for%20understanding%20the%20underlying%20factors%20that%20contribute%20to%20their%20detection%2C%20validating%20their%20significance%2C%20and%20identifying%20potential%20biases%20or%20errors.%20Effective%20explanations%20provide%20actionable%20insights%2C%20facilitating%20preventive%20measures%20to%20avoid%20similar%20outliers%20in%20the%20future.%20Counterfactual%20explanations%20clarify%20why%20specific%20data%20points%20are%20classified%20as%20outliers%20by%20identifying%20minimal%20changes%20required%20to%20alter%20their%20prediction.%20Although%20valuable%2C%20most%20existing%20counterfactual%20explanation%20methods%20overlook%20the%20unique%20challenges%20posed%20by%20outlier%20detection%2C%20and%20fail%20to%20target%20classical%2C%20widely%20adopted%20outlier%20detection%20algorithms.%20Local%20Outlier%20Factor%20%28LOF%29%20is%20one%20the%20most%20popular%20unsupervised%20outlier%20detection%20methods%2C%20quantifying%20outlierness%20through%20relative%20local%20density.%20Despite%20LOF%27s%20widespread%20use%20across%20diverse%20applications%2C%20it%20lacks%20interpretability.%20To%20address%20this%20limitation%2C%20we%20introduce%20Density-based%20Counterfactuals%20for%20Outliers%20%28DCFO%29%2C%20a%20novel%20method%20specifically%20designed%20to%20generate%20counterfactual%20explanations%20for%20LOF.%20DCFO%20partitions%20the%20data%20space%20into%20regions%20where%20LOF%20behaves%20smoothly%2C%20enabling%20efficient%20gradient-based%20optimisation.%20Extensive%20experimental%20validation%20on%2050%20OpenML%20datasets%20demonstrates%20that%20DCFO%20consistently%20outperforms%20benchmarked%20competitors%2C%20offering%20superior%20proximity%20and%20validity%20of%20generated%20counterfactuals.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10659v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDCFO%253A%2520Density-Based%2520Counterfactuals%2520for%2520Outliers%2520-%2520Additional%2520Material%26entry.906535625%3DTommaso%2520Amico%2520and%2520Pernille%2520Matthews%2520and%2520Lena%2520Krieger%2520and%2520Arthur%2520Zimek%2520and%2520Ira%2520Assent%26entry.1292438233%3DOutlier%2520detection%2520identifies%2520data%2520points%2520that%2520significantly%2520deviate%2520from%2520the%2520majority%2520of%2520the%2520data%2520distribution.%2520Explaining%2520outliers%2520is%2520crucial%2520for%2520understanding%2520the%2520underlying%2520factors%2520that%2520contribute%2520to%2520their%2520detection%252C%2520validating%2520their%2520significance%252C%2520and%2520identifying%2520potential%2520biases%2520or%2520errors.%2520Effective%2520explanations%2520provide%2520actionable%2520insights%252C%2520facilitating%2520preventive%2520measures%2520to%2520avoid%2520similar%2520outliers%2520in%2520the%2520future.%2520Counterfactual%2520explanations%2520clarify%2520why%2520specific%2520data%2520points%2520are%2520classified%2520as%2520outliers%2520by%2520identifying%2520minimal%2520changes%2520required%2520to%2520alter%2520their%2520prediction.%2520Although%2520valuable%252C%2520most%2520existing%2520counterfactual%2520explanation%2520methods%2520overlook%2520the%2520unique%2520challenges%2520posed%2520by%2520outlier%2520detection%252C%2520and%2520fail%2520to%2520target%2520classical%252C%2520widely%2520adopted%2520outlier%2520detection%2520algorithms.%2520Local%2520Outlier%2520Factor%2520%2528LOF%2529%2520is%2520one%2520the%2520most%2520popular%2520unsupervised%2520outlier%2520detection%2520methods%252C%2520quantifying%2520outlierness%2520through%2520relative%2520local%2520density.%2520Despite%2520LOF%2527s%2520widespread%2520use%2520across%2520diverse%2520applications%252C%2520it%2520lacks%2520interpretability.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%2520Density-based%2520Counterfactuals%2520for%2520Outliers%2520%2528DCFO%2529%252C%2520a%2520novel%2520method%2520specifically%2520designed%2520to%2520generate%2520counterfactual%2520explanations%2520for%2520LOF.%2520DCFO%2520partitions%2520the%2520data%2520space%2520into%2520regions%2520where%2520LOF%2520behaves%2520smoothly%252C%2520enabling%2520efficient%2520gradient-based%2520optimisation.%2520Extensive%2520experimental%2520validation%2520on%252050%2520OpenML%2520datasets%2520demonstrates%2520that%2520DCFO%2520consistently%2520outperforms%2520benchmarked%2520competitors%252C%2520offering%2520superior%2520proximity%2520and%2520validity%2520of%2520generated%2520counterfactuals.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10659v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DCFO%3A%20Density-Based%20Counterfactuals%20for%20Outliers%20-%20Additional%20Material&entry.906535625=Tommaso%20Amico%20and%20Pernille%20Matthews%20and%20Lena%20Krieger%20and%20Arthur%20Zimek%20and%20Ira%20Assent&entry.1292438233=Outlier%20detection%20identifies%20data%20points%20that%20significantly%20deviate%20from%20the%20majority%20of%20the%20data%20distribution.%20Explaining%20outliers%20is%20crucial%20for%20understanding%20the%20underlying%20factors%20that%20contribute%20to%20their%20detection%2C%20validating%20their%20significance%2C%20and%20identifying%20potential%20biases%20or%20errors.%20Effective%20explanations%20provide%20actionable%20insights%2C%20facilitating%20preventive%20measures%20to%20avoid%20similar%20outliers%20in%20the%20future.%20Counterfactual%20explanations%20clarify%20why%20specific%20data%20points%20are%20classified%20as%20outliers%20by%20identifying%20minimal%20changes%20required%20to%20alter%20their%20prediction.%20Although%20valuable%2C%20most%20existing%20counterfactual%20explanation%20methods%20overlook%20the%20unique%20challenges%20posed%20by%20outlier%20detection%2C%20and%20fail%20to%20target%20classical%2C%20widely%20adopted%20outlier%20detection%20algorithms.%20Local%20Outlier%20Factor%20%28LOF%29%20is%20one%20the%20most%20popular%20unsupervised%20outlier%20detection%20methods%2C%20quantifying%20outlierness%20through%20relative%20local%20density.%20Despite%20LOF%27s%20widespread%20use%20across%20diverse%20applications%2C%20it%20lacks%20interpretability.%20To%20address%20this%20limitation%2C%20we%20introduce%20Density-based%20Counterfactuals%20for%20Outliers%20%28DCFO%29%2C%20a%20novel%20method%20specifically%20designed%20to%20generate%20counterfactual%20explanations%20for%20LOF.%20DCFO%20partitions%20the%20data%20space%20into%20regions%20where%20LOF%20behaves%20smoothly%2C%20enabling%20efficient%20gradient-based%20optimisation.%20Extensive%20experimental%20validation%20on%2050%20OpenML%20datasets%20demonstrates%20that%20DCFO%20consistently%20outperforms%20benchmarked%20competitors%2C%20offering%20superior%20proximity%20and%20validity%20of%20generated%20counterfactuals.&entry.1838667208=http%3A//arxiv.org/abs/2512.10659v2&entry.124074799=Read"},
{"title": "Artificial Intelligence for Microbiology and Microbiome Research", "author": "Xu-Wen Wang and Tong Wang and Yang-Yu Liu", "abstract": "Advancements in artificial intelligence (AI) have transformed many scientific fields, with microbiology and microbiome research now experiencing significant breakthroughs through machine learning applications. This review provides a comprehensive overview of AI-driven approaches tailored for microbiology and microbiome studies, emphasizing both technical advancements and biological insights. We begin with an introduction to foundational AI techniques, including primary machine learning paradigms and various deep learning architectures, and offer guidance on choosing between traditional machine learning and sophisticated deep learning methods based on specific research goals. The primary section on application scenarios spans diverse research areas, from taxonomic profiling, functional annotation \\& prediction, microbe-X interactions, microbial ecology, metabolic modeling, precision nutrition, clinical microbiology, to prevention \\& therapeutics. Finally, we discuss challenges in this field and highlight some recent breakthroughs. Together, this review underscores AI's transformative role in microbiology and microbiome research, paving the way for innovative methodologies and applications that enhance our understanding of microbial life and its impact on our planet and our health.", "link": "http://arxiv.org/abs/2411.01098v2", "date": "2025-12-18", "relevancy": 1.6537, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4228}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4102}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4054}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Artificial%20Intelligence%20for%20Microbiology%20and%20Microbiome%20Research&body=Title%3A%20Artificial%20Intelligence%20for%20Microbiology%20and%20Microbiome%20Research%0AAuthor%3A%20Xu-Wen%20Wang%20and%20Tong%20Wang%20and%20Yang-Yu%20Liu%0AAbstract%3A%20Advancements%20in%20artificial%20intelligence%20%28AI%29%20have%20transformed%20many%20scientific%20fields%2C%20with%20microbiology%20and%20microbiome%20research%20now%20experiencing%20significant%20breakthroughs%20through%20machine%20learning%20applications.%20This%20review%20provides%20a%20comprehensive%20overview%20of%20AI-driven%20approaches%20tailored%20for%20microbiology%20and%20microbiome%20studies%2C%20emphasizing%20both%20technical%20advancements%20and%20biological%20insights.%20We%20begin%20with%20an%20introduction%20to%20foundational%20AI%20techniques%2C%20including%20primary%20machine%20learning%20paradigms%20and%20various%20deep%20learning%20architectures%2C%20and%20offer%20guidance%20on%20choosing%20between%20traditional%20machine%20learning%20and%20sophisticated%20deep%20learning%20methods%20based%20on%20specific%20research%20goals.%20The%20primary%20section%20on%20application%20scenarios%20spans%20diverse%20research%20areas%2C%20from%20taxonomic%20profiling%2C%20functional%20annotation%20%5C%26%20prediction%2C%20microbe-X%20interactions%2C%20microbial%20ecology%2C%20metabolic%20modeling%2C%20precision%20nutrition%2C%20clinical%20microbiology%2C%20to%20prevention%20%5C%26%20therapeutics.%20Finally%2C%20we%20discuss%20challenges%20in%20this%20field%20and%20highlight%20some%20recent%20breakthroughs.%20Together%2C%20this%20review%20underscores%20AI%27s%20transformative%20role%20in%20microbiology%20and%20microbiome%20research%2C%20paving%20the%20way%20for%20innovative%20methodologies%20and%20applications%20that%20enhance%20our%20understanding%20of%20microbial%20life%20and%20its%20impact%20on%20our%20planet%20and%20our%20health.%0ALink%3A%20http%3A//arxiv.org/abs/2411.01098v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtificial%2520Intelligence%2520for%2520Microbiology%2520and%2520Microbiome%2520Research%26entry.906535625%3DXu-Wen%2520Wang%2520and%2520Tong%2520Wang%2520and%2520Yang-Yu%2520Liu%26entry.1292438233%3DAdvancements%2520in%2520artificial%2520intelligence%2520%2528AI%2529%2520have%2520transformed%2520many%2520scientific%2520fields%252C%2520with%2520microbiology%2520and%2520microbiome%2520research%2520now%2520experiencing%2520significant%2520breakthroughs%2520through%2520machine%2520learning%2520applications.%2520This%2520review%2520provides%2520a%2520comprehensive%2520overview%2520of%2520AI-driven%2520approaches%2520tailored%2520for%2520microbiology%2520and%2520microbiome%2520studies%252C%2520emphasizing%2520both%2520technical%2520advancements%2520and%2520biological%2520insights.%2520We%2520begin%2520with%2520an%2520introduction%2520to%2520foundational%2520AI%2520techniques%252C%2520including%2520primary%2520machine%2520learning%2520paradigms%2520and%2520various%2520deep%2520learning%2520architectures%252C%2520and%2520offer%2520guidance%2520on%2520choosing%2520between%2520traditional%2520machine%2520learning%2520and%2520sophisticated%2520deep%2520learning%2520methods%2520based%2520on%2520specific%2520research%2520goals.%2520The%2520primary%2520section%2520on%2520application%2520scenarios%2520spans%2520diverse%2520research%2520areas%252C%2520from%2520taxonomic%2520profiling%252C%2520functional%2520annotation%2520%255C%2526%2520prediction%252C%2520microbe-X%2520interactions%252C%2520microbial%2520ecology%252C%2520metabolic%2520modeling%252C%2520precision%2520nutrition%252C%2520clinical%2520microbiology%252C%2520to%2520prevention%2520%255C%2526%2520therapeutics.%2520Finally%252C%2520we%2520discuss%2520challenges%2520in%2520this%2520field%2520and%2520highlight%2520some%2520recent%2520breakthroughs.%2520Together%252C%2520this%2520review%2520underscores%2520AI%2527s%2520transformative%2520role%2520in%2520microbiology%2520and%2520microbiome%2520research%252C%2520paving%2520the%2520way%2520for%2520innovative%2520methodologies%2520and%2520applications%2520that%2520enhance%2520our%2520understanding%2520of%2520microbial%2520life%2520and%2520its%2520impact%2520on%2520our%2520planet%2520and%2520our%2520health.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.01098v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Artificial%20Intelligence%20for%20Microbiology%20and%20Microbiome%20Research&entry.906535625=Xu-Wen%20Wang%20and%20Tong%20Wang%20and%20Yang-Yu%20Liu&entry.1292438233=Advancements%20in%20artificial%20intelligence%20%28AI%29%20have%20transformed%20many%20scientific%20fields%2C%20with%20microbiology%20and%20microbiome%20research%20now%20experiencing%20significant%20breakthroughs%20through%20machine%20learning%20applications.%20This%20review%20provides%20a%20comprehensive%20overview%20of%20AI-driven%20approaches%20tailored%20for%20microbiology%20and%20microbiome%20studies%2C%20emphasizing%20both%20technical%20advancements%20and%20biological%20insights.%20We%20begin%20with%20an%20introduction%20to%20foundational%20AI%20techniques%2C%20including%20primary%20machine%20learning%20paradigms%20and%20various%20deep%20learning%20architectures%2C%20and%20offer%20guidance%20on%20choosing%20between%20traditional%20machine%20learning%20and%20sophisticated%20deep%20learning%20methods%20based%20on%20specific%20research%20goals.%20The%20primary%20section%20on%20application%20scenarios%20spans%20diverse%20research%20areas%2C%20from%20taxonomic%20profiling%2C%20functional%20annotation%20%5C%26%20prediction%2C%20microbe-X%20interactions%2C%20microbial%20ecology%2C%20metabolic%20modeling%2C%20precision%20nutrition%2C%20clinical%20microbiology%2C%20to%20prevention%20%5C%26%20therapeutics.%20Finally%2C%20we%20discuss%20challenges%20in%20this%20field%20and%20highlight%20some%20recent%20breakthroughs.%20Together%2C%20this%20review%20underscores%20AI%27s%20transformative%20role%20in%20microbiology%20and%20microbiome%20research%2C%20paving%20the%20way%20for%20innovative%20methodologies%20and%20applications%20that%20enhance%20our%20understanding%20of%20microbial%20life%20and%20its%20impact%20on%20our%20planet%20and%20our%20health.&entry.1838667208=http%3A//arxiv.org/abs/2411.01098v2&entry.124074799=Read"},
{"title": "Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification", "author": "Qihao Liu and Chengzhi Mao and Yaojie Liu and Alan Yuille and Wen-Sheng Chu", "abstract": "Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.", "link": "http://arxiv.org/abs/2512.16921v1", "date": "2025-12-18", "relevancy": 2.0178, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5191}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5179}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differences%20That%20Matter%3A%20Auditing%20Models%20for%20Capability%20Gap%20Discovery%20and%20Rectification&body=Title%3A%20Differences%20That%20Matter%3A%20Auditing%20Models%20for%20Capability%20Gap%20Discovery%20and%20Rectification%0AAuthor%3A%20Qihao%20Liu%20and%20Chengzhi%20Mao%20and%20Yaojie%20Liu%20and%20Alan%20Yuille%20and%20Wen-Sheng%20Chu%0AAbstract%3A%20Conventional%20evaluation%20methods%20for%20multimodal%20LLMs%20%28MLLMs%29%20lack%20interpretability%20and%20are%20often%20insufficient%20to%20fully%20disclose%20significant%20capability%20gaps%20across%20models.%20To%20address%20this%2C%20we%20introduce%20AuditDM%2C%20an%20automated%20framework%20that%20actively%20discovers%20and%20rectifies%20MLLM%20failure%20modes%20by%20auditing%20their%20divergence.%20AuditDM%20fine-tunes%20an%20MLLM%20as%20an%20auditor%20via%20reinforcement%20learning%20to%20generate%20challenging%20questions%20and%20counterfactual%20images%20that%20maximize%20disagreement%20among%20target%20models.%20Once%20trained%2C%20the%20auditor%20uncovers%20diverse%2C%20interpretable%20exemplars%20that%20reveal%20model%20weaknesses%20and%20serve%20as%20annotation-free%20data%20for%20rectification.%20When%20applied%20to%20SoTA%20models%20like%20Gemma-3%20and%20PaliGemma-2%2C%20AuditDM%20discovers%20more%20than%2020%20distinct%20failure%20types.%20Fine-tuning%20on%20these%20discoveries%20consistently%20improves%20all%20models%20across%2016%20benchmarks%2C%20and%20enables%20a%203B%20model%20to%20surpass%20its%2028B%20counterpart.%20Our%20results%20suggest%20that%20as%20data%20scaling%20hits%20diminishing%20returns%2C%20targeted%20model%20auditing%20offers%20an%20effective%20path%20to%20model%20diagnosis%20and%20improvement.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16921v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferences%2520That%2520Matter%253A%2520Auditing%2520Models%2520for%2520Capability%2520Gap%2520Discovery%2520and%2520Rectification%26entry.906535625%3DQihao%2520Liu%2520and%2520Chengzhi%2520Mao%2520and%2520Yaojie%2520Liu%2520and%2520Alan%2520Yuille%2520and%2520Wen-Sheng%2520Chu%26entry.1292438233%3DConventional%2520evaluation%2520methods%2520for%2520multimodal%2520LLMs%2520%2528MLLMs%2529%2520lack%2520interpretability%2520and%2520are%2520often%2520insufficient%2520to%2520fully%2520disclose%2520significant%2520capability%2520gaps%2520across%2520models.%2520To%2520address%2520this%252C%2520we%2520introduce%2520AuditDM%252C%2520an%2520automated%2520framework%2520that%2520actively%2520discovers%2520and%2520rectifies%2520MLLM%2520failure%2520modes%2520by%2520auditing%2520their%2520divergence.%2520AuditDM%2520fine-tunes%2520an%2520MLLM%2520as%2520an%2520auditor%2520via%2520reinforcement%2520learning%2520to%2520generate%2520challenging%2520questions%2520and%2520counterfactual%2520images%2520that%2520maximize%2520disagreement%2520among%2520target%2520models.%2520Once%2520trained%252C%2520the%2520auditor%2520uncovers%2520diverse%252C%2520interpretable%2520exemplars%2520that%2520reveal%2520model%2520weaknesses%2520and%2520serve%2520as%2520annotation-free%2520data%2520for%2520rectification.%2520When%2520applied%2520to%2520SoTA%2520models%2520like%2520Gemma-3%2520and%2520PaliGemma-2%252C%2520AuditDM%2520discovers%2520more%2520than%252020%2520distinct%2520failure%2520types.%2520Fine-tuning%2520on%2520these%2520discoveries%2520consistently%2520improves%2520all%2520models%2520across%252016%2520benchmarks%252C%2520and%2520enables%2520a%25203B%2520model%2520to%2520surpass%2520its%252028B%2520counterpart.%2520Our%2520results%2520suggest%2520that%2520as%2520data%2520scaling%2520hits%2520diminishing%2520returns%252C%2520targeted%2520model%2520auditing%2520offers%2520an%2520effective%2520path%2520to%2520model%2520diagnosis%2520and%2520improvement.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16921v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differences%20That%20Matter%3A%20Auditing%20Models%20for%20Capability%20Gap%20Discovery%20and%20Rectification&entry.906535625=Qihao%20Liu%20and%20Chengzhi%20Mao%20and%20Yaojie%20Liu%20and%20Alan%20Yuille%20and%20Wen-Sheng%20Chu&entry.1292438233=Conventional%20evaluation%20methods%20for%20multimodal%20LLMs%20%28MLLMs%29%20lack%20interpretability%20and%20are%20often%20insufficient%20to%20fully%20disclose%20significant%20capability%20gaps%20across%20models.%20To%20address%20this%2C%20we%20introduce%20AuditDM%2C%20an%20automated%20framework%20that%20actively%20discovers%20and%20rectifies%20MLLM%20failure%20modes%20by%20auditing%20their%20divergence.%20AuditDM%20fine-tunes%20an%20MLLM%20as%20an%20auditor%20via%20reinforcement%20learning%20to%20generate%20challenging%20questions%20and%20counterfactual%20images%20that%20maximize%20disagreement%20among%20target%20models.%20Once%20trained%2C%20the%20auditor%20uncovers%20diverse%2C%20interpretable%20exemplars%20that%20reveal%20model%20weaknesses%20and%20serve%20as%20annotation-free%20data%20for%20rectification.%20When%20applied%20to%20SoTA%20models%20like%20Gemma-3%20and%20PaliGemma-2%2C%20AuditDM%20discovers%20more%20than%2020%20distinct%20failure%20types.%20Fine-tuning%20on%20these%20discoveries%20consistently%20improves%20all%20models%20across%2016%20benchmarks%2C%20and%20enables%20a%203B%20model%20to%20surpass%20its%2028B%20counterpart.%20Our%20results%20suggest%20that%20as%20data%20scaling%20hits%20diminishing%20returns%2C%20targeted%20model%20auditing%20offers%20an%20effective%20path%20to%20model%20diagnosis%20and%20improvement.&entry.1838667208=http%3A//arxiv.org/abs/2512.16921v1&entry.124074799=Read"},
{"title": "Distributional AGI Safety", "author": "Nenad Toma\u0161ev and Matija Franklin and Julian Jacobs and S\u00e9bastien Krier and Simon Osindero", "abstract": "AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.", "link": "http://arxiv.org/abs/2512.16856v1", "date": "2025-12-18", "relevancy": 1.8347, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4725}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4503}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distributional%20AGI%20Safety&body=Title%3A%20Distributional%20AGI%20Safety%0AAuthor%3A%20Nenad%20Toma%C5%A1ev%20and%20Matija%20Franklin%20and%20Julian%20Jacobs%20and%20S%C3%A9bastien%20Krier%20and%20Simon%20Osindero%0AAbstract%3A%20AI%20safety%20and%20alignment%20research%20has%20predominantly%20been%20focused%20on%20methods%20for%20safeguarding%20individual%20AI%20systems%2C%20resting%20on%20the%20assumption%20of%20an%20eventual%20emergence%20of%20a%20monolithic%20Artificial%20General%20Intelligence%20%28AGI%29.%20The%20alternative%20AGI%20emergence%20hypothesis%2C%20where%20general%20capability%20levels%20are%20first%20manifested%20through%20coordination%20in%20groups%20of%20sub-AGI%20individual%20agents%20with%20complementary%20skills%20and%20affordances%2C%20has%20received%20far%20less%20attention.%20Here%20we%20argue%20that%20this%20patchwork%20AGI%20hypothesis%20needs%20to%20be%20given%20serious%20consideration%2C%20and%20should%20inform%20the%20development%20of%20corresponding%20safeguards%20and%20mitigations.%20The%20rapid%20deployment%20of%20advanced%20AI%20agents%20with%20tool-use%20capabilities%20and%20the%20ability%20to%20communicate%20and%20coordinate%20makes%20this%20an%20urgent%20safety%20consideration.%20We%20therefore%20propose%20a%20framework%20for%20distributional%20AGI%20safety%20that%20moves%20beyond%20evaluating%20and%20aligning%20individual%20agents.%20This%20framework%20centers%20on%20the%20design%20and%20implementation%20of%20virtual%20agentic%20sandbox%20economies%20%28impermeable%20or%20semi-permeable%29%2C%20where%20agent-to-agent%20transactions%20are%20governed%20by%20robust%20market%20mechanisms%2C%20coupled%20with%20appropriate%20auditability%2C%20reputation%20management%2C%20and%20oversight%20to%20mitigate%20collective%20risks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16856v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistributional%2520AGI%2520Safety%26entry.906535625%3DNenad%2520Toma%25C5%25A1ev%2520and%2520Matija%2520Franklin%2520and%2520Julian%2520Jacobs%2520and%2520S%25C3%25A9bastien%2520Krier%2520and%2520Simon%2520Osindero%26entry.1292438233%3DAI%2520safety%2520and%2520alignment%2520research%2520has%2520predominantly%2520been%2520focused%2520on%2520methods%2520for%2520safeguarding%2520individual%2520AI%2520systems%252C%2520resting%2520on%2520the%2520assumption%2520of%2520an%2520eventual%2520emergence%2520of%2520a%2520monolithic%2520Artificial%2520General%2520Intelligence%2520%2528AGI%2529.%2520The%2520alternative%2520AGI%2520emergence%2520hypothesis%252C%2520where%2520general%2520capability%2520levels%2520are%2520first%2520manifested%2520through%2520coordination%2520in%2520groups%2520of%2520sub-AGI%2520individual%2520agents%2520with%2520complementary%2520skills%2520and%2520affordances%252C%2520has%2520received%2520far%2520less%2520attention.%2520Here%2520we%2520argue%2520that%2520this%2520patchwork%2520AGI%2520hypothesis%2520needs%2520to%2520be%2520given%2520serious%2520consideration%252C%2520and%2520should%2520inform%2520the%2520development%2520of%2520corresponding%2520safeguards%2520and%2520mitigations.%2520The%2520rapid%2520deployment%2520of%2520advanced%2520AI%2520agents%2520with%2520tool-use%2520capabilities%2520and%2520the%2520ability%2520to%2520communicate%2520and%2520coordinate%2520makes%2520this%2520an%2520urgent%2520safety%2520consideration.%2520We%2520therefore%2520propose%2520a%2520framework%2520for%2520distributional%2520AGI%2520safety%2520that%2520moves%2520beyond%2520evaluating%2520and%2520aligning%2520individual%2520agents.%2520This%2520framework%2520centers%2520on%2520the%2520design%2520and%2520implementation%2520of%2520virtual%2520agentic%2520sandbox%2520economies%2520%2528impermeable%2520or%2520semi-permeable%2529%252C%2520where%2520agent-to-agent%2520transactions%2520are%2520governed%2520by%2520robust%2520market%2520mechanisms%252C%2520coupled%2520with%2520appropriate%2520auditability%252C%2520reputation%2520management%252C%2520and%2520oversight%2520to%2520mitigate%2520collective%2520risks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16856v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributional%20AGI%20Safety&entry.906535625=Nenad%20Toma%C5%A1ev%20and%20Matija%20Franklin%20and%20Julian%20Jacobs%20and%20S%C3%A9bastien%20Krier%20and%20Simon%20Osindero&entry.1292438233=AI%20safety%20and%20alignment%20research%20has%20predominantly%20been%20focused%20on%20methods%20for%20safeguarding%20individual%20AI%20systems%2C%20resting%20on%20the%20assumption%20of%20an%20eventual%20emergence%20of%20a%20monolithic%20Artificial%20General%20Intelligence%20%28AGI%29.%20The%20alternative%20AGI%20emergence%20hypothesis%2C%20where%20general%20capability%20levels%20are%20first%20manifested%20through%20coordination%20in%20groups%20of%20sub-AGI%20individual%20agents%20with%20complementary%20skills%20and%20affordances%2C%20has%20received%20far%20less%20attention.%20Here%20we%20argue%20that%20this%20patchwork%20AGI%20hypothesis%20needs%20to%20be%20given%20serious%20consideration%2C%20and%20should%20inform%20the%20development%20of%20corresponding%20safeguards%20and%20mitigations.%20The%20rapid%20deployment%20of%20advanced%20AI%20agents%20with%20tool-use%20capabilities%20and%20the%20ability%20to%20communicate%20and%20coordinate%20makes%20this%20an%20urgent%20safety%20consideration.%20We%20therefore%20propose%20a%20framework%20for%20distributional%20AGI%20safety%20that%20moves%20beyond%20evaluating%20and%20aligning%20individual%20agents.%20This%20framework%20centers%20on%20the%20design%20and%20implementation%20of%20virtual%20agentic%20sandbox%20economies%20%28impermeable%20or%20semi-permeable%29%2C%20where%20agent-to-agent%20transactions%20are%20governed%20by%20robust%20market%20mechanisms%2C%20coupled%20with%20appropriate%20auditability%2C%20reputation%20management%2C%20and%20oversight%20to%20mitigate%20collective%20risks.&entry.1838667208=http%3A//arxiv.org/abs/2512.16856v1&entry.124074799=Read"},
{"title": "On The Hidden Biases of Flow Matching Samplers", "author": "Soon Hoe Lim", "abstract": "We study the implicit bias of flow matching (FM) samplers via the lens of empirical flow matching. Although population FM may produce gradient-field velocities resembling optimal transport (OT), we show that the empirical FM minimizer is almost never a gradient field, even when each conditional flow is. Consequently, empirical FM is intrinsically energetically suboptimal. In view of this, we analyze the kinetic energy of generated samples. With Gaussian sources, both instantaneous and integrated kinetic energies exhibit exponential concentration, while heavy-tailed sources lead to polynomial tails. These behaviors are governed primarily by the choice of source distribution rather than the data. Overall, these notes provide a concise mathematical account of the structural and energetic biases arising in empirical FM.", "link": "http://arxiv.org/abs/2512.16768v1", "date": "2025-12-18", "relevancy": 1.29, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4419}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4284}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20The%20Hidden%20Biases%20of%20Flow%20Matching%20Samplers&body=Title%3A%20On%20The%20Hidden%20Biases%20of%20Flow%20Matching%20Samplers%0AAuthor%3A%20Soon%20Hoe%20Lim%0AAbstract%3A%20We%20study%20the%20implicit%20bias%20of%20flow%20matching%20%28FM%29%20samplers%20via%20the%20lens%20of%20empirical%20flow%20matching.%20Although%20population%20FM%20may%20produce%20gradient-field%20velocities%20resembling%20optimal%20transport%20%28OT%29%2C%20we%20show%20that%20the%20empirical%20FM%20minimizer%20is%20almost%20never%20a%20gradient%20field%2C%20even%20when%20each%20conditional%20flow%20is.%20Consequently%2C%20empirical%20FM%20is%20intrinsically%20energetically%20suboptimal.%20In%20view%20of%20this%2C%20we%20analyze%20the%20kinetic%20energy%20of%20generated%20samples.%20With%20Gaussian%20sources%2C%20both%20instantaneous%20and%20integrated%20kinetic%20energies%20exhibit%20exponential%20concentration%2C%20while%20heavy-tailed%20sources%20lead%20to%20polynomial%20tails.%20These%20behaviors%20are%20governed%20primarily%20by%20the%20choice%20of%20source%20distribution%20rather%20than%20the%20data.%20Overall%2C%20these%20notes%20provide%20a%20concise%20mathematical%20account%20of%20the%20structural%20and%20energetic%20biases%20arising%20in%20empirical%20FM.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520The%2520Hidden%2520Biases%2520of%2520Flow%2520Matching%2520Samplers%26entry.906535625%3DSoon%2520Hoe%2520Lim%26entry.1292438233%3DWe%2520study%2520the%2520implicit%2520bias%2520of%2520flow%2520matching%2520%2528FM%2529%2520samplers%2520via%2520the%2520lens%2520of%2520empirical%2520flow%2520matching.%2520Although%2520population%2520FM%2520may%2520produce%2520gradient-field%2520velocities%2520resembling%2520optimal%2520transport%2520%2528OT%2529%252C%2520we%2520show%2520that%2520the%2520empirical%2520FM%2520minimizer%2520is%2520almost%2520never%2520a%2520gradient%2520field%252C%2520even%2520when%2520each%2520conditional%2520flow%2520is.%2520Consequently%252C%2520empirical%2520FM%2520is%2520intrinsically%2520energetically%2520suboptimal.%2520In%2520view%2520of%2520this%252C%2520we%2520analyze%2520the%2520kinetic%2520energy%2520of%2520generated%2520samples.%2520With%2520Gaussian%2520sources%252C%2520both%2520instantaneous%2520and%2520integrated%2520kinetic%2520energies%2520exhibit%2520exponential%2520concentration%252C%2520while%2520heavy-tailed%2520sources%2520lead%2520to%2520polynomial%2520tails.%2520These%2520behaviors%2520are%2520governed%2520primarily%2520by%2520the%2520choice%2520of%2520source%2520distribution%2520rather%2520than%2520the%2520data.%2520Overall%252C%2520these%2520notes%2520provide%2520a%2520concise%2520mathematical%2520account%2520of%2520the%2520structural%2520and%2520energetic%2520biases%2520arising%2520in%2520empirical%2520FM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20The%20Hidden%20Biases%20of%20Flow%20Matching%20Samplers&entry.906535625=Soon%20Hoe%20Lim&entry.1292438233=We%20study%20the%20implicit%20bias%20of%20flow%20matching%20%28FM%29%20samplers%20via%20the%20lens%20of%20empirical%20flow%20matching.%20Although%20population%20FM%20may%20produce%20gradient-field%20velocities%20resembling%20optimal%20transport%20%28OT%29%2C%20we%20show%20that%20the%20empirical%20FM%20minimizer%20is%20almost%20never%20a%20gradient%20field%2C%20even%20when%20each%20conditional%20flow%20is.%20Consequently%2C%20empirical%20FM%20is%20intrinsically%20energetically%20suboptimal.%20In%20view%20of%20this%2C%20we%20analyze%20the%20kinetic%20energy%20of%20generated%20samples.%20With%20Gaussian%20sources%2C%20both%20instantaneous%20and%20integrated%20kinetic%20energies%20exhibit%20exponential%20concentration%2C%20while%20heavy-tailed%20sources%20lead%20to%20polynomial%20tails.%20These%20behaviors%20are%20governed%20primarily%20by%20the%20choice%20of%20source%20distribution%20rather%20than%20the%20data.%20Overall%2C%20these%20notes%20provide%20a%20concise%20mathematical%20account%20of%20the%20structural%20and%20energetic%20biases%20arising%20in%20empirical%20FM.&entry.1838667208=http%3A//arxiv.org/abs/2512.16768v1&entry.124074799=Read"},
{"title": "Pixel Seal: Adversarial-only training for invisible image and video watermarking", "author": "Tom\u00e1\u0161 Sou\u010dek and Pierre Fernandez and Hady Elsahar and Sylvestre-Alvise Rebuffi and Valeriu Lacatusu and Tuan Tran and Tom Sander and Alexandre Mourachko", "abstract": "Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.", "link": "http://arxiv.org/abs/2512.16874v1", "date": "2025-12-18", "relevancy": 2.0145, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5133}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4995}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pixel%20Seal%3A%20Adversarial-only%20training%20for%20invisible%20image%20and%20video%20watermarking&body=Title%3A%20Pixel%20Seal%3A%20Adversarial-only%20training%20for%20invisible%20image%20and%20video%20watermarking%0AAuthor%3A%20Tom%C3%A1%C5%A1%20Sou%C4%8Dek%20and%20Pierre%20Fernandez%20and%20Hady%20Elsahar%20and%20Sylvestre-Alvise%20Rebuffi%20and%20Valeriu%20Lacatusu%20and%20Tuan%20Tran%20and%20Tom%20Sander%20and%20Alexandre%20Mourachko%0AAbstract%3A%20Invisible%20watermarking%20is%20essential%20for%20tracing%20the%20provenance%20of%20digital%20content.%20However%2C%20training%20state-of-the-art%20models%20remains%20notoriously%20difficult%2C%20with%20current%20approaches%20often%20struggling%20to%20balance%20robustness%20against%20true%20imperceptibility.%20This%20work%20introduces%20Pixel%20Seal%2C%20which%20sets%20a%20new%20state-of-the-art%20for%20image%20and%20video%20watermarking.%20We%20first%20identify%20three%20fundamental%20issues%20of%20existing%20methods%3A%20%28i%29%20the%20reliance%20on%20proxy%20perceptual%20losses%20such%20as%20MSE%20and%20LPIPS%20that%20fail%20to%20mimic%20human%20perception%20and%20result%20in%20visible%20watermark%20artifacts%3B%20%28ii%29%20the%20optimization%20instability%20caused%20by%20conflicting%20objectives%2C%20which%20necessitates%20exhaustive%20hyperparameter%20tuning%3B%20and%20%28iii%29%20reduced%20robustness%20and%20imperceptibility%20of%20watermarks%20when%20scaling%20models%20to%20high-resolution%20images%20and%20videos.%20To%20overcome%20these%20issues%2C%20we%20first%20propose%20an%20adversarial-only%20training%20paradigm%20that%20eliminates%20unreliable%20pixel-wise%20imperceptibility%20losses.%20Second%2C%20we%20introduce%20a%20three-stage%20training%20schedule%20that%20stabilizes%20convergence%20by%20decoupling%20robustness%20and%20imperceptibility.%20Third%2C%20we%20address%20the%20resolution%20gap%20via%20high-resolution%20adaptation%2C%20employing%20JND-based%20attenuation%20and%20training-time%20inference%20simulation%20to%20eliminate%20upscaling%20artifacts.%20We%20thoroughly%20evaluate%20the%20robustness%20and%20imperceptibility%20of%20Pixel%20Seal%20on%20different%20image%20types%20and%20across%20a%20wide%20range%20of%20transformations%2C%20and%20show%20clear%20improvements%20over%20the%20state-of-the-art.%20We%20finally%20demonstrate%20that%20the%20model%20efficiently%20adapts%20to%20video%20via%20temporal%20watermark%20pooling%2C%20positioning%20Pixel%20Seal%20as%20a%20practical%20and%20scalable%20solution%20for%20reliable%20provenance%20in%20real-world%20image%20and%20video%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16874v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPixel%2520Seal%253A%2520Adversarial-only%2520training%2520for%2520invisible%2520image%2520and%2520video%2520watermarking%26entry.906535625%3DTom%25C3%25A1%25C5%25A1%2520Sou%25C4%258Dek%2520and%2520Pierre%2520Fernandez%2520and%2520Hady%2520Elsahar%2520and%2520Sylvestre-Alvise%2520Rebuffi%2520and%2520Valeriu%2520Lacatusu%2520and%2520Tuan%2520Tran%2520and%2520Tom%2520Sander%2520and%2520Alexandre%2520Mourachko%26entry.1292438233%3DInvisible%2520watermarking%2520is%2520essential%2520for%2520tracing%2520the%2520provenance%2520of%2520digital%2520content.%2520However%252C%2520training%2520state-of-the-art%2520models%2520remains%2520notoriously%2520difficult%252C%2520with%2520current%2520approaches%2520often%2520struggling%2520to%2520balance%2520robustness%2520against%2520true%2520imperceptibility.%2520This%2520work%2520introduces%2520Pixel%2520Seal%252C%2520which%2520sets%2520a%2520new%2520state-of-the-art%2520for%2520image%2520and%2520video%2520watermarking.%2520We%2520first%2520identify%2520three%2520fundamental%2520issues%2520of%2520existing%2520methods%253A%2520%2528i%2529%2520the%2520reliance%2520on%2520proxy%2520perceptual%2520losses%2520such%2520as%2520MSE%2520and%2520LPIPS%2520that%2520fail%2520to%2520mimic%2520human%2520perception%2520and%2520result%2520in%2520visible%2520watermark%2520artifacts%253B%2520%2528ii%2529%2520the%2520optimization%2520instability%2520caused%2520by%2520conflicting%2520objectives%252C%2520which%2520necessitates%2520exhaustive%2520hyperparameter%2520tuning%253B%2520and%2520%2528iii%2529%2520reduced%2520robustness%2520and%2520imperceptibility%2520of%2520watermarks%2520when%2520scaling%2520models%2520to%2520high-resolution%2520images%2520and%2520videos.%2520To%2520overcome%2520these%2520issues%252C%2520we%2520first%2520propose%2520an%2520adversarial-only%2520training%2520paradigm%2520that%2520eliminates%2520unreliable%2520pixel-wise%2520imperceptibility%2520losses.%2520Second%252C%2520we%2520introduce%2520a%2520three-stage%2520training%2520schedule%2520that%2520stabilizes%2520convergence%2520by%2520decoupling%2520robustness%2520and%2520imperceptibility.%2520Third%252C%2520we%2520address%2520the%2520resolution%2520gap%2520via%2520high-resolution%2520adaptation%252C%2520employing%2520JND-based%2520attenuation%2520and%2520training-time%2520inference%2520simulation%2520to%2520eliminate%2520upscaling%2520artifacts.%2520We%2520thoroughly%2520evaluate%2520the%2520robustness%2520and%2520imperceptibility%2520of%2520Pixel%2520Seal%2520on%2520different%2520image%2520types%2520and%2520across%2520a%2520wide%2520range%2520of%2520transformations%252C%2520and%2520show%2520clear%2520improvements%2520over%2520the%2520state-of-the-art.%2520We%2520finally%2520demonstrate%2520that%2520the%2520model%2520efficiently%2520adapts%2520to%2520video%2520via%2520temporal%2520watermark%2520pooling%252C%2520positioning%2520Pixel%2520Seal%2520as%2520a%2520practical%2520and%2520scalable%2520solution%2520for%2520reliable%2520provenance%2520in%2520real-world%2520image%2520and%2520video%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16874v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pixel%20Seal%3A%20Adversarial-only%20training%20for%20invisible%20image%20and%20video%20watermarking&entry.906535625=Tom%C3%A1%C5%A1%20Sou%C4%8Dek%20and%20Pierre%20Fernandez%20and%20Hady%20Elsahar%20and%20Sylvestre-Alvise%20Rebuffi%20and%20Valeriu%20Lacatusu%20and%20Tuan%20Tran%20and%20Tom%20Sander%20and%20Alexandre%20Mourachko&entry.1292438233=Invisible%20watermarking%20is%20essential%20for%20tracing%20the%20provenance%20of%20digital%20content.%20However%2C%20training%20state-of-the-art%20models%20remains%20notoriously%20difficult%2C%20with%20current%20approaches%20often%20struggling%20to%20balance%20robustness%20against%20true%20imperceptibility.%20This%20work%20introduces%20Pixel%20Seal%2C%20which%20sets%20a%20new%20state-of-the-art%20for%20image%20and%20video%20watermarking.%20We%20first%20identify%20three%20fundamental%20issues%20of%20existing%20methods%3A%20%28i%29%20the%20reliance%20on%20proxy%20perceptual%20losses%20such%20as%20MSE%20and%20LPIPS%20that%20fail%20to%20mimic%20human%20perception%20and%20result%20in%20visible%20watermark%20artifacts%3B%20%28ii%29%20the%20optimization%20instability%20caused%20by%20conflicting%20objectives%2C%20which%20necessitates%20exhaustive%20hyperparameter%20tuning%3B%20and%20%28iii%29%20reduced%20robustness%20and%20imperceptibility%20of%20watermarks%20when%20scaling%20models%20to%20high-resolution%20images%20and%20videos.%20To%20overcome%20these%20issues%2C%20we%20first%20propose%20an%20adversarial-only%20training%20paradigm%20that%20eliminates%20unreliable%20pixel-wise%20imperceptibility%20losses.%20Second%2C%20we%20introduce%20a%20three-stage%20training%20schedule%20that%20stabilizes%20convergence%20by%20decoupling%20robustness%20and%20imperceptibility.%20Third%2C%20we%20address%20the%20resolution%20gap%20via%20high-resolution%20adaptation%2C%20employing%20JND-based%20attenuation%20and%20training-time%20inference%20simulation%20to%20eliminate%20upscaling%20artifacts.%20We%20thoroughly%20evaluate%20the%20robustness%20and%20imperceptibility%20of%20Pixel%20Seal%20on%20different%20image%20types%20and%20across%20a%20wide%20range%20of%20transformations%2C%20and%20show%20clear%20improvements%20over%20the%20state-of-the-art.%20We%20finally%20demonstrate%20that%20the%20model%20efficiently%20adapts%20to%20video%20via%20temporal%20watermark%20pooling%2C%20positioning%20Pixel%20Seal%20as%20a%20practical%20and%20scalable%20solution%20for%20reliable%20provenance%20in%20real-world%20image%20and%20video%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2512.16874v1&entry.124074799=Read"},
{"title": "Gated KalmaNet: A Fading Memory Layer Through Test-Time Ridge Regression", "author": "Liangzu Peng and Aditya Chattopadhyay and Luca Zancato and Elvis Nunez and Wei Xia and Stefano Soatto", "abstract": "As efficient alternatives to softmax Attention, linear State-Space Models (SSMs) achieve constant memory and linear compute, but maintain only a lossy, fading summary of the past, often leading to inferior performance in recall-oriented tasks. We propose Gated KalmaNet (GKA), a layer that accounts for the full past while maintaining SSM-style efficiency. We ground our approach in the Kalman Filter (KF) framework, which provides a principled solution for optimal inference in dynamical systems. We show that several existing SSM layers (DeltaNet, Gated DeltaNet, and Kimi Delta Attention) are approximations to the KF recurrence that assume identity error covariance, thereby ignoring how past measurements (keys and values) should optimally influence state updates. In contrast, GKA computes the exact Kalman gain by maintaining the full error covariance. Under a steady-state assumption that enables parallelization, this reduces to solving an online ridge regression problem with constant memory and linear compute cost. A critical insight is that standard KF equations are numerically unstable in low-precision environments (like bfloat16) and hard to parallelize on modern hardware. We address this through: (1) adaptive regularization with input-dependent gating to control the condition number of the ridge regression for numerical stability, and (2) Chebyshev Iteration, which we show is more stable than conventional iterative solvers in low-precision settings. We further develop hardware-aware chunk-wise kernels to enable efficient training. Empirically, GKA outperforms existing SSM layers (like Mamba2 and Gated DeltaNet) on short-context tasks and achieves more than 10\\% relative improvement on long-context RAG and LongQA tasks up to 128k tokens.", "link": "http://arxiv.org/abs/2511.21016v2", "date": "2025-12-18", "relevancy": 2.0604, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5259}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5205}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5054}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gated%20KalmaNet%3A%20A%20Fading%20Memory%20Layer%20Through%20Test-Time%20Ridge%20Regression&body=Title%3A%20Gated%20KalmaNet%3A%20A%20Fading%20Memory%20Layer%20Through%20Test-Time%20Ridge%20Regression%0AAuthor%3A%20Liangzu%20Peng%20and%20Aditya%20Chattopadhyay%20and%20Luca%20Zancato%20and%20Elvis%20Nunez%20and%20Wei%20Xia%20and%20Stefano%20Soatto%0AAbstract%3A%20As%20efficient%20alternatives%20to%20softmax%20Attention%2C%20linear%20State-Space%20Models%20%28SSMs%29%20achieve%20constant%20memory%20and%20linear%20compute%2C%20but%20maintain%20only%20a%20lossy%2C%20fading%20summary%20of%20the%20past%2C%20often%20leading%20to%20inferior%20performance%20in%20recall-oriented%20tasks.%20We%20propose%20Gated%20KalmaNet%20%28GKA%29%2C%20a%20layer%20that%20accounts%20for%20the%20full%20past%20while%20maintaining%20SSM-style%20efficiency.%20We%20ground%20our%20approach%20in%20the%20Kalman%20Filter%20%28KF%29%20framework%2C%20which%20provides%20a%20principled%20solution%20for%20optimal%20inference%20in%20dynamical%20systems.%20We%20show%20that%20several%20existing%20SSM%20layers%20%28DeltaNet%2C%20Gated%20DeltaNet%2C%20and%20Kimi%20Delta%20Attention%29%20are%20approximations%20to%20the%20KF%20recurrence%20that%20assume%20identity%20error%20covariance%2C%20thereby%20ignoring%20how%20past%20measurements%20%28keys%20and%20values%29%20should%20optimally%20influence%20state%20updates.%20In%20contrast%2C%20GKA%20computes%20the%20exact%20Kalman%20gain%20by%20maintaining%20the%20full%20error%20covariance.%20Under%20a%20steady-state%20assumption%20that%20enables%20parallelization%2C%20this%20reduces%20to%20solving%20an%20online%20ridge%20regression%20problem%20with%20constant%20memory%20and%20linear%20compute%20cost.%20A%20critical%20insight%20is%20that%20standard%20KF%20equations%20are%20numerically%20unstable%20in%20low-precision%20environments%20%28like%20bfloat16%29%20and%20hard%20to%20parallelize%20on%20modern%20hardware.%20We%20address%20this%20through%3A%20%281%29%20adaptive%20regularization%20with%20input-dependent%20gating%20to%20control%20the%20condition%20number%20of%20the%20ridge%20regression%20for%20numerical%20stability%2C%20and%20%282%29%20Chebyshev%20Iteration%2C%20which%20we%20show%20is%20more%20stable%20than%20conventional%20iterative%20solvers%20in%20low-precision%20settings.%20We%20further%20develop%20hardware-aware%20chunk-wise%20kernels%20to%20enable%20efficient%20training.%20Empirically%2C%20GKA%20outperforms%20existing%20SSM%20layers%20%28like%20Mamba2%20and%20Gated%20DeltaNet%29%20on%20short-context%20tasks%20and%20achieves%20more%20than%2010%5C%25%20relative%20improvement%20on%20long-context%20RAG%20and%20LongQA%20tasks%20up%20to%20128k%20tokens.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21016v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGated%2520KalmaNet%253A%2520A%2520Fading%2520Memory%2520Layer%2520Through%2520Test-Time%2520Ridge%2520Regression%26entry.906535625%3DLiangzu%2520Peng%2520and%2520Aditya%2520Chattopadhyay%2520and%2520Luca%2520Zancato%2520and%2520Elvis%2520Nunez%2520and%2520Wei%2520Xia%2520and%2520Stefano%2520Soatto%26entry.1292438233%3DAs%2520efficient%2520alternatives%2520to%2520softmax%2520Attention%252C%2520linear%2520State-Space%2520Models%2520%2528SSMs%2529%2520achieve%2520constant%2520memory%2520and%2520linear%2520compute%252C%2520but%2520maintain%2520only%2520a%2520lossy%252C%2520fading%2520summary%2520of%2520the%2520past%252C%2520often%2520leading%2520to%2520inferior%2520performance%2520in%2520recall-oriented%2520tasks.%2520We%2520propose%2520Gated%2520KalmaNet%2520%2528GKA%2529%252C%2520a%2520layer%2520that%2520accounts%2520for%2520the%2520full%2520past%2520while%2520maintaining%2520SSM-style%2520efficiency.%2520We%2520ground%2520our%2520approach%2520in%2520the%2520Kalman%2520Filter%2520%2528KF%2529%2520framework%252C%2520which%2520provides%2520a%2520principled%2520solution%2520for%2520optimal%2520inference%2520in%2520dynamical%2520systems.%2520We%2520show%2520that%2520several%2520existing%2520SSM%2520layers%2520%2528DeltaNet%252C%2520Gated%2520DeltaNet%252C%2520and%2520Kimi%2520Delta%2520Attention%2529%2520are%2520approximations%2520to%2520the%2520KF%2520recurrence%2520that%2520assume%2520identity%2520error%2520covariance%252C%2520thereby%2520ignoring%2520how%2520past%2520measurements%2520%2528keys%2520and%2520values%2529%2520should%2520optimally%2520influence%2520state%2520updates.%2520In%2520contrast%252C%2520GKA%2520computes%2520the%2520exact%2520Kalman%2520gain%2520by%2520maintaining%2520the%2520full%2520error%2520covariance.%2520Under%2520a%2520steady-state%2520assumption%2520that%2520enables%2520parallelization%252C%2520this%2520reduces%2520to%2520solving%2520an%2520online%2520ridge%2520regression%2520problem%2520with%2520constant%2520memory%2520and%2520linear%2520compute%2520cost.%2520A%2520critical%2520insight%2520is%2520that%2520standard%2520KF%2520equations%2520are%2520numerically%2520unstable%2520in%2520low-precision%2520environments%2520%2528like%2520bfloat16%2529%2520and%2520hard%2520to%2520parallelize%2520on%2520modern%2520hardware.%2520We%2520address%2520this%2520through%253A%2520%25281%2529%2520adaptive%2520regularization%2520with%2520input-dependent%2520gating%2520to%2520control%2520the%2520condition%2520number%2520of%2520the%2520ridge%2520regression%2520for%2520numerical%2520stability%252C%2520and%2520%25282%2529%2520Chebyshev%2520Iteration%252C%2520which%2520we%2520show%2520is%2520more%2520stable%2520than%2520conventional%2520iterative%2520solvers%2520in%2520low-precision%2520settings.%2520We%2520further%2520develop%2520hardware-aware%2520chunk-wise%2520kernels%2520to%2520enable%2520efficient%2520training.%2520Empirically%252C%2520GKA%2520outperforms%2520existing%2520SSM%2520layers%2520%2528like%2520Mamba2%2520and%2520Gated%2520DeltaNet%2529%2520on%2520short-context%2520tasks%2520and%2520achieves%2520more%2520than%252010%255C%2525%2520relative%2520improvement%2520on%2520long-context%2520RAG%2520and%2520LongQA%2520tasks%2520up%2520to%2520128k%2520tokens.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21016v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gated%20KalmaNet%3A%20A%20Fading%20Memory%20Layer%20Through%20Test-Time%20Ridge%20Regression&entry.906535625=Liangzu%20Peng%20and%20Aditya%20Chattopadhyay%20and%20Luca%20Zancato%20and%20Elvis%20Nunez%20and%20Wei%20Xia%20and%20Stefano%20Soatto&entry.1292438233=As%20efficient%20alternatives%20to%20softmax%20Attention%2C%20linear%20State-Space%20Models%20%28SSMs%29%20achieve%20constant%20memory%20and%20linear%20compute%2C%20but%20maintain%20only%20a%20lossy%2C%20fading%20summary%20of%20the%20past%2C%20often%20leading%20to%20inferior%20performance%20in%20recall-oriented%20tasks.%20We%20propose%20Gated%20KalmaNet%20%28GKA%29%2C%20a%20layer%20that%20accounts%20for%20the%20full%20past%20while%20maintaining%20SSM-style%20efficiency.%20We%20ground%20our%20approach%20in%20the%20Kalman%20Filter%20%28KF%29%20framework%2C%20which%20provides%20a%20principled%20solution%20for%20optimal%20inference%20in%20dynamical%20systems.%20We%20show%20that%20several%20existing%20SSM%20layers%20%28DeltaNet%2C%20Gated%20DeltaNet%2C%20and%20Kimi%20Delta%20Attention%29%20are%20approximations%20to%20the%20KF%20recurrence%20that%20assume%20identity%20error%20covariance%2C%20thereby%20ignoring%20how%20past%20measurements%20%28keys%20and%20values%29%20should%20optimally%20influence%20state%20updates.%20In%20contrast%2C%20GKA%20computes%20the%20exact%20Kalman%20gain%20by%20maintaining%20the%20full%20error%20covariance.%20Under%20a%20steady-state%20assumption%20that%20enables%20parallelization%2C%20this%20reduces%20to%20solving%20an%20online%20ridge%20regression%20problem%20with%20constant%20memory%20and%20linear%20compute%20cost.%20A%20critical%20insight%20is%20that%20standard%20KF%20equations%20are%20numerically%20unstable%20in%20low-precision%20environments%20%28like%20bfloat16%29%20and%20hard%20to%20parallelize%20on%20modern%20hardware.%20We%20address%20this%20through%3A%20%281%29%20adaptive%20regularization%20with%20input-dependent%20gating%20to%20control%20the%20condition%20number%20of%20the%20ridge%20regression%20for%20numerical%20stability%2C%20and%20%282%29%20Chebyshev%20Iteration%2C%20which%20we%20show%20is%20more%20stable%20than%20conventional%20iterative%20solvers%20in%20low-precision%20settings.%20We%20further%20develop%20hardware-aware%20chunk-wise%20kernels%20to%20enable%20efficient%20training.%20Empirically%2C%20GKA%20outperforms%20existing%20SSM%20layers%20%28like%20Mamba2%20and%20Gated%20DeltaNet%29%20on%20short-context%20tasks%20and%20achieves%20more%20than%2010%5C%25%20relative%20improvement%20on%20long-context%20RAG%20and%20LongQA%20tasks%20up%20to%20128k%20tokens.&entry.1838667208=http%3A//arxiv.org/abs/2511.21016v2&entry.124074799=Read"},
{"title": "Riemannian Stochastic Interpolants for Amorphous Particle Systems", "author": "Louis Grenioux and Leonardo Galliano and Ludovic Berthier and Giulio Biroli and Marylou Gabri\u00e9", "abstract": "Modern generative models hold great promise for accelerating diverse tasks involving the simulation of physical systems, but they must be adapted to the specific constraints of each domain. Significant progress has been made for biomolecules and crystalline materials. Here, we address amorphous materials (glasses), which are disordered particle systems lacking atomic periodicity. Sampling equilibrium configurations of glass-forming materials is a notoriously slow and difficult task. This obstacle could be overcome by developing a generative framework capable of producing equilibrium configurations with well-defined likelihoods. In this work, we address this challenge by leveraging an equivariant Riemannian stochastic interpolation framework which combines Riemannian stochastic interpolant and equivariant flow matching. Our method rigorously incorporates periodic boundary conditions and the symmetries of multi-component particle systems, adapting an equivariant graph neural network to operate directly on the torus. Our numerical experiments on model amorphous systems demonstrate that enforcing geometric and symmetry constraints significantly improves generative performance.", "link": "http://arxiv.org/abs/2512.16607v1", "date": "2025-12-18", "relevancy": 1.5637, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5306}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5215}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Riemannian%20Stochastic%20Interpolants%20for%20Amorphous%20Particle%20Systems&body=Title%3A%20Riemannian%20Stochastic%20Interpolants%20for%20Amorphous%20Particle%20Systems%0AAuthor%3A%20Louis%20Grenioux%20and%20Leonardo%20Galliano%20and%20Ludovic%20Berthier%20and%20Giulio%20Biroli%20and%20Marylou%20Gabri%C3%A9%0AAbstract%3A%20Modern%20generative%20models%20hold%20great%20promise%20for%20accelerating%20diverse%20tasks%20involving%20the%20simulation%20of%20physical%20systems%2C%20but%20they%20must%20be%20adapted%20to%20the%20specific%20constraints%20of%20each%20domain.%20Significant%20progress%20has%20been%20made%20for%20biomolecules%20and%20crystalline%20materials.%20Here%2C%20we%20address%20amorphous%20materials%20%28glasses%29%2C%20which%20are%20disordered%20particle%20systems%20lacking%20atomic%20periodicity.%20Sampling%20equilibrium%20configurations%20of%20glass-forming%20materials%20is%20a%20notoriously%20slow%20and%20difficult%20task.%20This%20obstacle%20could%20be%20overcome%20by%20developing%20a%20generative%20framework%20capable%20of%20producing%20equilibrium%20configurations%20with%20well-defined%20likelihoods.%20In%20this%20work%2C%20we%20address%20this%20challenge%20by%20leveraging%20an%20equivariant%20Riemannian%20stochastic%20interpolation%20framework%20which%20combines%20Riemannian%20stochastic%20interpolant%20and%20equivariant%20flow%20matching.%20Our%20method%20rigorously%20incorporates%20periodic%20boundary%20conditions%20and%20the%20symmetries%20of%20multi-component%20particle%20systems%2C%20adapting%20an%20equivariant%20graph%20neural%20network%20to%20operate%20directly%20on%20the%20torus.%20Our%20numerical%20experiments%20on%20model%20amorphous%20systems%20demonstrate%20that%20enforcing%20geometric%20and%20symmetry%20constraints%20significantly%20improves%20generative%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16607v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRiemannian%2520Stochastic%2520Interpolants%2520for%2520Amorphous%2520Particle%2520Systems%26entry.906535625%3DLouis%2520Grenioux%2520and%2520Leonardo%2520Galliano%2520and%2520Ludovic%2520Berthier%2520and%2520Giulio%2520Biroli%2520and%2520Marylou%2520Gabri%25C3%25A9%26entry.1292438233%3DModern%2520generative%2520models%2520hold%2520great%2520promise%2520for%2520accelerating%2520diverse%2520tasks%2520involving%2520the%2520simulation%2520of%2520physical%2520systems%252C%2520but%2520they%2520must%2520be%2520adapted%2520to%2520the%2520specific%2520constraints%2520of%2520each%2520domain.%2520Significant%2520progress%2520has%2520been%2520made%2520for%2520biomolecules%2520and%2520crystalline%2520materials.%2520Here%252C%2520we%2520address%2520amorphous%2520materials%2520%2528glasses%2529%252C%2520which%2520are%2520disordered%2520particle%2520systems%2520lacking%2520atomic%2520periodicity.%2520Sampling%2520equilibrium%2520configurations%2520of%2520glass-forming%2520materials%2520is%2520a%2520notoriously%2520slow%2520and%2520difficult%2520task.%2520This%2520obstacle%2520could%2520be%2520overcome%2520by%2520developing%2520a%2520generative%2520framework%2520capable%2520of%2520producing%2520equilibrium%2520configurations%2520with%2520well-defined%2520likelihoods.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520challenge%2520by%2520leveraging%2520an%2520equivariant%2520Riemannian%2520stochastic%2520interpolation%2520framework%2520which%2520combines%2520Riemannian%2520stochastic%2520interpolant%2520and%2520equivariant%2520flow%2520matching.%2520Our%2520method%2520rigorously%2520incorporates%2520periodic%2520boundary%2520conditions%2520and%2520the%2520symmetries%2520of%2520multi-component%2520particle%2520systems%252C%2520adapting%2520an%2520equivariant%2520graph%2520neural%2520network%2520to%2520operate%2520directly%2520on%2520the%2520torus.%2520Our%2520numerical%2520experiments%2520on%2520model%2520amorphous%2520systems%2520demonstrate%2520that%2520enforcing%2520geometric%2520and%2520symmetry%2520constraints%2520significantly%2520improves%2520generative%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16607v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Riemannian%20Stochastic%20Interpolants%20for%20Amorphous%20Particle%20Systems&entry.906535625=Louis%20Grenioux%20and%20Leonardo%20Galliano%20and%20Ludovic%20Berthier%20and%20Giulio%20Biroli%20and%20Marylou%20Gabri%C3%A9&entry.1292438233=Modern%20generative%20models%20hold%20great%20promise%20for%20accelerating%20diverse%20tasks%20involving%20the%20simulation%20of%20physical%20systems%2C%20but%20they%20must%20be%20adapted%20to%20the%20specific%20constraints%20of%20each%20domain.%20Significant%20progress%20has%20been%20made%20for%20biomolecules%20and%20crystalline%20materials.%20Here%2C%20we%20address%20amorphous%20materials%20%28glasses%29%2C%20which%20are%20disordered%20particle%20systems%20lacking%20atomic%20periodicity.%20Sampling%20equilibrium%20configurations%20of%20glass-forming%20materials%20is%20a%20notoriously%20slow%20and%20difficult%20task.%20This%20obstacle%20could%20be%20overcome%20by%20developing%20a%20generative%20framework%20capable%20of%20producing%20equilibrium%20configurations%20with%20well-defined%20likelihoods.%20In%20this%20work%2C%20we%20address%20this%20challenge%20by%20leveraging%20an%20equivariant%20Riemannian%20stochastic%20interpolation%20framework%20which%20combines%20Riemannian%20stochastic%20interpolant%20and%20equivariant%20flow%20matching.%20Our%20method%20rigorously%20incorporates%20periodic%20boundary%20conditions%20and%20the%20symmetries%20of%20multi-component%20particle%20systems%2C%20adapting%20an%20equivariant%20graph%20neural%20network%20to%20operate%20directly%20on%20the%20torus.%20Our%20numerical%20experiments%20on%20model%20amorphous%20systems%20demonstrate%20that%20enforcing%20geometric%20and%20symmetry%20constraints%20significantly%20improves%20generative%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2512.16607v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


