<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250109.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Scaffold-SLAM: Structured 3D Gaussians for Simultaneous Localization and\n  Photorealistic Mapping", "author": "Wen Tianci and Liu Zhiang and Lu Biao and Fang Yongchun", "abstract": "  3D Gaussian Splatting (3DGS) has recently revolutionized novel view synthesis\nin the Simultaneous Localization and Mapping (SLAM). However, existing SLAM\nmethods utilizing 3DGS have failed to provide high-quality novel view rendering\nfor monocular, stereo, and RGB-D cameras simultaneously. Notably, some methods\nperform well for RGB-D cameras but suffer significant degradation in rendering\nquality for monocular cameras. In this paper, we present Scaffold-SLAM, which\ndelivers simultaneous localization and high-quality photorealistic mapping\nacross monocular, stereo, and RGB-D cameras. We introduce two key innovations\nto achieve this state-of-the-art visual quality. First, we propose\nAppearance-from-Motion embedding, enabling 3D Gaussians to better model image\nappearance variations across different camera poses. Second, we introduce a\nfrequency regularization pyramid to guide the distribution of Gaussians,\nallowing the model to effectively capture finer details in the scene. Extensive\nexperiments on monocular, stereo, and RGB-D datasets demonstrate that\nScaffold-SLAM significantly outperforms state-of-the-art methods in\nphotorealistic mapping quality, e.g., PSNR is 16.76% higher in the TUM RGB-D\ndatasets for monocular cameras.\n", "link": "http://arxiv.org/abs/2501.05242v1", "date": "2025-01-09", "relevancy": 3.6162, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7972}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7027}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaffold-SLAM%3A%20Structured%203D%20Gaussians%20for%20Simultaneous%20Localization%20and%0A%20%20Photorealistic%20Mapping&body=Title%3A%20Scaffold-SLAM%3A%20Structured%203D%20Gaussians%20for%20Simultaneous%20Localization%20and%0A%20%20Photorealistic%20Mapping%0AAuthor%3A%20Wen%20Tianci%20and%20Liu%20Zhiang%20and%20Lu%20Biao%20and%20Fang%20Yongchun%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20revolutionized%20novel%20view%20synthesis%0Ain%20the%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29.%20However%2C%20existing%20SLAM%0Amethods%20utilizing%203DGS%20have%20failed%20to%20provide%20high-quality%20novel%20view%20rendering%0Afor%20monocular%2C%20stereo%2C%20and%20RGB-D%20cameras%20simultaneously.%20Notably%2C%20some%20methods%0Aperform%20well%20for%20RGB-D%20cameras%20but%20suffer%20significant%20degradation%20in%20rendering%0Aquality%20for%20monocular%20cameras.%20In%20this%20paper%2C%20we%20present%20Scaffold-SLAM%2C%20which%0Adelivers%20simultaneous%20localization%20and%20high-quality%20photorealistic%20mapping%0Aacross%20monocular%2C%20stereo%2C%20and%20RGB-D%20cameras.%20We%20introduce%20two%20key%20innovations%0Ato%20achieve%20this%20state-of-the-art%20visual%20quality.%20First%2C%20we%20propose%0AAppearance-from-Motion%20embedding%2C%20enabling%203D%20Gaussians%20to%20better%20model%20image%0Aappearance%20variations%20across%20different%20camera%20poses.%20Second%2C%20we%20introduce%20a%0Afrequency%20regularization%20pyramid%20to%20guide%20the%20distribution%20of%20Gaussians%2C%0Aallowing%20the%20model%20to%20effectively%20capture%20finer%20details%20in%20the%20scene.%20Extensive%0Aexperiments%20on%20monocular%2C%20stereo%2C%20and%20RGB-D%20datasets%20demonstrate%20that%0AScaffold-SLAM%20significantly%20outperforms%20state-of-the-art%20methods%20in%0Aphotorealistic%20mapping%20quality%2C%20e.g.%2C%20PSNR%20is%2016.76%25%20higher%20in%20the%20TUM%20RGB-D%0Adatasets%20for%20monocular%20cameras.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05242v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaffold-SLAM%253A%2520Structured%25203D%2520Gaussians%2520for%2520Simultaneous%2520Localization%2520and%250A%2520%2520Photorealistic%2520Mapping%26entry.906535625%3DWen%2520Tianci%2520and%2520Liu%2520Zhiang%2520and%2520Lu%2520Biao%2520and%2520Fang%2520Yongchun%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520recently%2520revolutionized%2520novel%2520view%2520synthesis%250Ain%2520the%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529.%2520However%252C%2520existing%2520SLAM%250Amethods%2520utilizing%25203DGS%2520have%2520failed%2520to%2520provide%2520high-quality%2520novel%2520view%2520rendering%250Afor%2520monocular%252C%2520stereo%252C%2520and%2520RGB-D%2520cameras%2520simultaneously.%2520Notably%252C%2520some%2520methods%250Aperform%2520well%2520for%2520RGB-D%2520cameras%2520but%2520suffer%2520significant%2520degradation%2520in%2520rendering%250Aquality%2520for%2520monocular%2520cameras.%2520In%2520this%2520paper%252C%2520we%2520present%2520Scaffold-SLAM%252C%2520which%250Adelivers%2520simultaneous%2520localization%2520and%2520high-quality%2520photorealistic%2520mapping%250Aacross%2520monocular%252C%2520stereo%252C%2520and%2520RGB-D%2520cameras.%2520We%2520introduce%2520two%2520key%2520innovations%250Ato%2520achieve%2520this%2520state-of-the-art%2520visual%2520quality.%2520First%252C%2520we%2520propose%250AAppearance-from-Motion%2520embedding%252C%2520enabling%25203D%2520Gaussians%2520to%2520better%2520model%2520image%250Aappearance%2520variations%2520across%2520different%2520camera%2520poses.%2520Second%252C%2520we%2520introduce%2520a%250Afrequency%2520regularization%2520pyramid%2520to%2520guide%2520the%2520distribution%2520of%2520Gaussians%252C%250Aallowing%2520the%2520model%2520to%2520effectively%2520capture%2520finer%2520details%2520in%2520the%2520scene.%2520Extensive%250Aexperiments%2520on%2520monocular%252C%2520stereo%252C%2520and%2520RGB-D%2520datasets%2520demonstrate%2520that%250AScaffold-SLAM%2520significantly%2520outperforms%2520state-of-the-art%2520methods%2520in%250Aphotorealistic%2520mapping%2520quality%252C%2520e.g.%252C%2520PSNR%2520is%252016.76%2525%2520higher%2520in%2520the%2520TUM%2520RGB-D%250Adatasets%2520for%2520monocular%2520cameras.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05242v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaffold-SLAM%3A%20Structured%203D%20Gaussians%20for%20Simultaneous%20Localization%20and%0A%20%20Photorealistic%20Mapping&entry.906535625=Wen%20Tianci%20and%20Liu%20Zhiang%20and%20Lu%20Biao%20and%20Fang%20Yongchun&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20revolutionized%20novel%20view%20synthesis%0Ain%20the%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29.%20However%2C%20existing%20SLAM%0Amethods%20utilizing%203DGS%20have%20failed%20to%20provide%20high-quality%20novel%20view%20rendering%0Afor%20monocular%2C%20stereo%2C%20and%20RGB-D%20cameras%20simultaneously.%20Notably%2C%20some%20methods%0Aperform%20well%20for%20RGB-D%20cameras%20but%20suffer%20significant%20degradation%20in%20rendering%0Aquality%20for%20monocular%20cameras.%20In%20this%20paper%2C%20we%20present%20Scaffold-SLAM%2C%20which%0Adelivers%20simultaneous%20localization%20and%20high-quality%20photorealistic%20mapping%0Aacross%20monocular%2C%20stereo%2C%20and%20RGB-D%20cameras.%20We%20introduce%20two%20key%20innovations%0Ato%20achieve%20this%20state-of-the-art%20visual%20quality.%20First%2C%20we%20propose%0AAppearance-from-Motion%20embedding%2C%20enabling%203D%20Gaussians%20to%20better%20model%20image%0Aappearance%20variations%20across%20different%20camera%20poses.%20Second%2C%20we%20introduce%20a%0Afrequency%20regularization%20pyramid%20to%20guide%20the%20distribution%20of%20Gaussians%2C%0Aallowing%20the%20model%20to%20effectively%20capture%20finer%20details%20in%20the%20scene.%20Extensive%0Aexperiments%20on%20monocular%2C%20stereo%2C%20and%20RGB-D%20datasets%20demonstrate%20that%0AScaffold-SLAM%20significantly%20outperforms%20state-of-the-art%20methods%20in%0Aphotorealistic%20mapping%20quality%2C%20e.g.%2C%20PSNR%20is%2016.76%25%20higher%20in%20the%20TUM%20RGB-D%0Adatasets%20for%20monocular%20cameras.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05242v1&entry.124074799=Read"},
{"title": "Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID\n  Guidance", "author": "Dimitrios Gerogiannis and Foivos Paraperas Papantoniou and Rolandos Alexandros Potamias and Alexandros Lattas and Stefanos Zafeiriou", "abstract": "  Inspired by the effectiveness of 3D Gaussian Splatting (3DGS) in\nreconstructing detailed 3D scenes within multi-view setups and the emergence of\nlarge 2D human foundation models, we introduce Arc2Avatar, the first SDS-based\nmethod utilizing a human face foundation model as guidance with just a single\nimage as input. To achieve that, we extend such a model for diverse-view human\nhead generation by fine-tuning on synthetic data and modifying its\nconditioning. Our avatars maintain a dense correspondence with a human face\nmesh template, allowing blendshape-based expression generation. This is\nachieved through a modified 3DGS approach, connectivity regularizers, and a\nstrategic initialization tailored for our task. Additionally, we propose an\noptional efficient SDS-based correction step to refine the blendshape\nexpressions, enhancing realism and diversity. Experiments demonstrate that\nArc2Avatar achieves state-of-the-art realism and identity preservation,\neffectively addressing color issues by allowing the use of very low guidance,\nenabled by our strong identity prior and initialization strategy, without\ncompromising detail.\n", "link": "http://arxiv.org/abs/2501.05379v1", "date": "2025-01-09", "relevancy": 3.4743, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7272}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7272}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Arc2Avatar%3A%20Generating%20Expressive%203D%20Avatars%20from%20a%20Single%20Image%20via%20ID%0A%20%20Guidance&body=Title%3A%20Arc2Avatar%3A%20Generating%20Expressive%203D%20Avatars%20from%20a%20Single%20Image%20via%20ID%0A%20%20Guidance%0AAuthor%3A%20Dimitrios%20Gerogiannis%20and%20Foivos%20Paraperas%20Papantoniou%20and%20Rolandos%20Alexandros%20Potamias%20and%20Alexandros%20Lattas%20and%20Stefanos%20Zafeiriou%0AAbstract%3A%20%20%20Inspired%20by%20the%20effectiveness%20of%203D%20Gaussian%20Splatting%20%283DGS%29%20in%0Areconstructing%20detailed%203D%20scenes%20within%20multi-view%20setups%20and%20the%20emergence%20of%0Alarge%202D%20human%20foundation%20models%2C%20we%20introduce%20Arc2Avatar%2C%20the%20first%20SDS-based%0Amethod%20utilizing%20a%20human%20face%20foundation%20model%20as%20guidance%20with%20just%20a%20single%0Aimage%20as%20input.%20To%20achieve%20that%2C%20we%20extend%20such%20a%20model%20for%20diverse-view%20human%0Ahead%20generation%20by%20fine-tuning%20on%20synthetic%20data%20and%20modifying%20its%0Aconditioning.%20Our%20avatars%20maintain%20a%20dense%20correspondence%20with%20a%20human%20face%0Amesh%20template%2C%20allowing%20blendshape-based%20expression%20generation.%20This%20is%0Aachieved%20through%20a%20modified%203DGS%20approach%2C%20connectivity%20regularizers%2C%20and%20a%0Astrategic%20initialization%20tailored%20for%20our%20task.%20Additionally%2C%20we%20propose%20an%0Aoptional%20efficient%20SDS-based%20correction%20step%20to%20refine%20the%20blendshape%0Aexpressions%2C%20enhancing%20realism%20and%20diversity.%20Experiments%20demonstrate%20that%0AArc2Avatar%20achieves%20state-of-the-art%20realism%20and%20identity%20preservation%2C%0Aeffectively%20addressing%20color%20issues%20by%20allowing%20the%20use%20of%20very%20low%20guidance%2C%0Aenabled%20by%20our%20strong%20identity%20prior%20and%20initialization%20strategy%2C%20without%0Acompromising%20detail.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05379v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArc2Avatar%253A%2520Generating%2520Expressive%25203D%2520Avatars%2520from%2520a%2520Single%2520Image%2520via%2520ID%250A%2520%2520Guidance%26entry.906535625%3DDimitrios%2520Gerogiannis%2520and%2520Foivos%2520Paraperas%2520Papantoniou%2520and%2520Rolandos%2520Alexandros%2520Potamias%2520and%2520Alexandros%2520Lattas%2520and%2520Stefanos%2520Zafeiriou%26entry.1292438233%3D%2520%2520Inspired%2520by%2520the%2520effectiveness%2520of%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520in%250Areconstructing%2520detailed%25203D%2520scenes%2520within%2520multi-view%2520setups%2520and%2520the%2520emergence%2520of%250Alarge%25202D%2520human%2520foundation%2520models%252C%2520we%2520introduce%2520Arc2Avatar%252C%2520the%2520first%2520SDS-based%250Amethod%2520utilizing%2520a%2520human%2520face%2520foundation%2520model%2520as%2520guidance%2520with%2520just%2520a%2520single%250Aimage%2520as%2520input.%2520To%2520achieve%2520that%252C%2520we%2520extend%2520such%2520a%2520model%2520for%2520diverse-view%2520human%250Ahead%2520generation%2520by%2520fine-tuning%2520on%2520synthetic%2520data%2520and%2520modifying%2520its%250Aconditioning.%2520Our%2520avatars%2520maintain%2520a%2520dense%2520correspondence%2520with%2520a%2520human%2520face%250Amesh%2520template%252C%2520allowing%2520blendshape-based%2520expression%2520generation.%2520This%2520is%250Aachieved%2520through%2520a%2520modified%25203DGS%2520approach%252C%2520connectivity%2520regularizers%252C%2520and%2520a%250Astrategic%2520initialization%2520tailored%2520for%2520our%2520task.%2520Additionally%252C%2520we%2520propose%2520an%250Aoptional%2520efficient%2520SDS-based%2520correction%2520step%2520to%2520refine%2520the%2520blendshape%250Aexpressions%252C%2520enhancing%2520realism%2520and%2520diversity.%2520Experiments%2520demonstrate%2520that%250AArc2Avatar%2520achieves%2520state-of-the-art%2520realism%2520and%2520identity%2520preservation%252C%250Aeffectively%2520addressing%2520color%2520issues%2520by%2520allowing%2520the%2520use%2520of%2520very%2520low%2520guidance%252C%250Aenabled%2520by%2520our%2520strong%2520identity%2520prior%2520and%2520initialization%2520strategy%252C%2520without%250Acompromising%2520detail.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05379v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Arc2Avatar%3A%20Generating%20Expressive%203D%20Avatars%20from%20a%20Single%20Image%20via%20ID%0A%20%20Guidance&entry.906535625=Dimitrios%20Gerogiannis%20and%20Foivos%20Paraperas%20Papantoniou%20and%20Rolandos%20Alexandros%20Potamias%20and%20Alexandros%20Lattas%20and%20Stefanos%20Zafeiriou&entry.1292438233=%20%20Inspired%20by%20the%20effectiveness%20of%203D%20Gaussian%20Splatting%20%283DGS%29%20in%0Areconstructing%20detailed%203D%20scenes%20within%20multi-view%20setups%20and%20the%20emergence%20of%0Alarge%202D%20human%20foundation%20models%2C%20we%20introduce%20Arc2Avatar%2C%20the%20first%20SDS-based%0Amethod%20utilizing%20a%20human%20face%20foundation%20model%20as%20guidance%20with%20just%20a%20single%0Aimage%20as%20input.%20To%20achieve%20that%2C%20we%20extend%20such%20a%20model%20for%20diverse-view%20human%0Ahead%20generation%20by%20fine-tuning%20on%20synthetic%20data%20and%20modifying%20its%0Aconditioning.%20Our%20avatars%20maintain%20a%20dense%20correspondence%20with%20a%20human%20face%0Amesh%20template%2C%20allowing%20blendshape-based%20expression%20generation.%20This%20is%0Aachieved%20through%20a%20modified%203DGS%20approach%2C%20connectivity%20regularizers%2C%20and%20a%0Astrategic%20initialization%20tailored%20for%20our%20task.%20Additionally%2C%20we%20propose%20an%0Aoptional%20efficient%20SDS-based%20correction%20step%20to%20refine%20the%20blendshape%0Aexpressions%2C%20enhancing%20realism%20and%20diversity.%20Experiments%20demonstrate%20that%0AArc2Avatar%20achieves%20state-of-the-art%20realism%20and%20identity%20preservation%2C%0Aeffectively%20addressing%20color%20issues%20by%20allowing%20the%20use%20of%20very%20low%20guidance%2C%0Aenabled%20by%20our%20strong%20identity%20prior%20and%20initialization%20strategy%2C%20without%0Acompromising%20detail.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05379v1&entry.124074799=Read"},
{"title": "GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models", "author": "Zhangyang Qi and Zhixiong Zhang and Ye Fang and Jiaqi Wang and Hengshuang Zhao", "abstract": "  In recent years, 2D Vision-Language Models (VLMs) have made significant\nstrides in image-text understanding tasks. However, their performance in 3D\nspatial comprehension, which is critical for embodied intelligence, remains\nlimited. Recent advances have leveraged 3D point clouds and multi-view images\nas inputs, yielding promising results. However, we propose exploring a purely\nvision-based solution inspired by human perception, which merely relies on\nvisual cues for 3D spatial understanding. This paper empirically investigates\nthe limitations of VLMs in 3D spatial knowledge, revealing that their primary\nshortcoming lies in the lack of global-local correspondence between the scene\nand individual frames. To address this, we introduce GPT4Scene, a novel visual\nprompting paradigm in VLM training and inference that helps build the\nglobal-local relationship, significantly improving the 3D spatial understanding\nof indoor scenes. Specifically, GPT4Scene constructs a 3D Bird's Eye View (BEV)\nimage from the video and marks consistent object IDs across both frames and the\nBEV image. The model then inputs the concatenated BEV image and video frames\nwith markers. In zero-shot evaluations, GPT4Scene improves performance over\nclosed-source VLMs like GPT-4o. Additionally, we prepare a processed video\ndataset consisting of 165K text annotation to fine-tune open-source VLMs,\nachieving state-of-the-art performance on all 3D understanding tasks.\nSurprisingly, after training with the GPT4Scene paradigm, VLMs consistently\nimprove during inference, even without visual prompting and BEV image as\nexplicit correspondence. It demonstrates that the proposed paradigm helps VLMs\ndevelop an intrinsic ability to understand 3D scenes, which paves the way for a\nnoninvasive approach to extending pre-trained VLMs for 3D scene understanding.\n", "link": "http://arxiv.org/abs/2501.01428v3", "date": "2025-01-09", "relevancy": 3.342, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.7085}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.7085}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPT4Scene%3A%20Understand%203D%20Scenes%20from%20Videos%20with%20Vision-Language%20Models&body=Title%3A%20GPT4Scene%3A%20Understand%203D%20Scenes%20from%20Videos%20with%20Vision-Language%20Models%0AAuthor%3A%20Zhangyang%20Qi%20and%20Zhixiong%20Zhang%20and%20Ye%20Fang%20and%20Jiaqi%20Wang%20and%20Hengshuang%20Zhao%0AAbstract%3A%20%20%20In%20recent%20years%2C%202D%20Vision-Language%20Models%20%28VLMs%29%20have%20made%20significant%0Astrides%20in%20image-text%20understanding%20tasks.%20However%2C%20their%20performance%20in%203D%0Aspatial%20comprehension%2C%20which%20is%20critical%20for%20embodied%20intelligence%2C%20remains%0Alimited.%20Recent%20advances%20have%20leveraged%203D%20point%20clouds%20and%20multi-view%20images%0Aas%20inputs%2C%20yielding%20promising%20results.%20However%2C%20we%20propose%20exploring%20a%20purely%0Avision-based%20solution%20inspired%20by%20human%20perception%2C%20which%20merely%20relies%20on%0Avisual%20cues%20for%203D%20spatial%20understanding.%20This%20paper%20empirically%20investigates%0Athe%20limitations%20of%20VLMs%20in%203D%20spatial%20knowledge%2C%20revealing%20that%20their%20primary%0Ashortcoming%20lies%20in%20the%20lack%20of%20global-local%20correspondence%20between%20the%20scene%0Aand%20individual%20frames.%20To%20address%20this%2C%20we%20introduce%20GPT4Scene%2C%20a%20novel%20visual%0Aprompting%20paradigm%20in%20VLM%20training%20and%20inference%20that%20helps%20build%20the%0Aglobal-local%20relationship%2C%20significantly%20improving%20the%203D%20spatial%20understanding%0Aof%20indoor%20scenes.%20Specifically%2C%20GPT4Scene%20constructs%20a%203D%20Bird%27s%20Eye%20View%20%28BEV%29%0Aimage%20from%20the%20video%20and%20marks%20consistent%20object%20IDs%20across%20both%20frames%20and%20the%0ABEV%20image.%20The%20model%20then%20inputs%20the%20concatenated%20BEV%20image%20and%20video%20frames%0Awith%20markers.%20In%20zero-shot%20evaluations%2C%20GPT4Scene%20improves%20performance%20over%0Aclosed-source%20VLMs%20like%20GPT-4o.%20Additionally%2C%20we%20prepare%20a%20processed%20video%0Adataset%20consisting%20of%20165K%20text%20annotation%20to%20fine-tune%20open-source%20VLMs%2C%0Aachieving%20state-of-the-art%20performance%20on%20all%203D%20understanding%20tasks.%0ASurprisingly%2C%20after%20training%20with%20the%20GPT4Scene%20paradigm%2C%20VLMs%20consistently%0Aimprove%20during%20inference%2C%20even%20without%20visual%20prompting%20and%20BEV%20image%20as%0Aexplicit%20correspondence.%20It%20demonstrates%20that%20the%20proposed%20paradigm%20helps%20VLMs%0Adevelop%20an%20intrinsic%20ability%20to%20understand%203D%20scenes%2C%20which%20paves%20the%20way%20for%20a%0Anoninvasive%20approach%20to%20extending%20pre-trained%20VLMs%20for%203D%20scene%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01428v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPT4Scene%253A%2520Understand%25203D%2520Scenes%2520from%2520Videos%2520with%2520Vision-Language%2520Models%26entry.906535625%3DZhangyang%2520Qi%2520and%2520Zhixiong%2520Zhang%2520and%2520Ye%2520Fang%2520and%2520Jiaqi%2520Wang%2520and%2520Hengshuang%2520Zhao%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%25202D%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520made%2520significant%250Astrides%2520in%2520image-text%2520understanding%2520tasks.%2520However%252C%2520their%2520performance%2520in%25203D%250Aspatial%2520comprehension%252C%2520which%2520is%2520critical%2520for%2520embodied%2520intelligence%252C%2520remains%250Alimited.%2520Recent%2520advances%2520have%2520leveraged%25203D%2520point%2520clouds%2520and%2520multi-view%2520images%250Aas%2520inputs%252C%2520yielding%2520promising%2520results.%2520However%252C%2520we%2520propose%2520exploring%2520a%2520purely%250Avision-based%2520solution%2520inspired%2520by%2520human%2520perception%252C%2520which%2520merely%2520relies%2520on%250Avisual%2520cues%2520for%25203D%2520spatial%2520understanding.%2520This%2520paper%2520empirically%2520investigates%250Athe%2520limitations%2520of%2520VLMs%2520in%25203D%2520spatial%2520knowledge%252C%2520revealing%2520that%2520their%2520primary%250Ashortcoming%2520lies%2520in%2520the%2520lack%2520of%2520global-local%2520correspondence%2520between%2520the%2520scene%250Aand%2520individual%2520frames.%2520To%2520address%2520this%252C%2520we%2520introduce%2520GPT4Scene%252C%2520a%2520novel%2520visual%250Aprompting%2520paradigm%2520in%2520VLM%2520training%2520and%2520inference%2520that%2520helps%2520build%2520the%250Aglobal-local%2520relationship%252C%2520significantly%2520improving%2520the%25203D%2520spatial%2520understanding%250Aof%2520indoor%2520scenes.%2520Specifically%252C%2520GPT4Scene%2520constructs%2520a%25203D%2520Bird%2527s%2520Eye%2520View%2520%2528BEV%2529%250Aimage%2520from%2520the%2520video%2520and%2520marks%2520consistent%2520object%2520IDs%2520across%2520both%2520frames%2520and%2520the%250ABEV%2520image.%2520The%2520model%2520then%2520inputs%2520the%2520concatenated%2520BEV%2520image%2520and%2520video%2520frames%250Awith%2520markers.%2520In%2520zero-shot%2520evaluations%252C%2520GPT4Scene%2520improves%2520performance%2520over%250Aclosed-source%2520VLMs%2520like%2520GPT-4o.%2520Additionally%252C%2520we%2520prepare%2520a%2520processed%2520video%250Adataset%2520consisting%2520of%2520165K%2520text%2520annotation%2520to%2520fine-tune%2520open-source%2520VLMs%252C%250Aachieving%2520state-of-the-art%2520performance%2520on%2520all%25203D%2520understanding%2520tasks.%250ASurprisingly%252C%2520after%2520training%2520with%2520the%2520GPT4Scene%2520paradigm%252C%2520VLMs%2520consistently%250Aimprove%2520during%2520inference%252C%2520even%2520without%2520visual%2520prompting%2520and%2520BEV%2520image%2520as%250Aexplicit%2520correspondence.%2520It%2520demonstrates%2520that%2520the%2520proposed%2520paradigm%2520helps%2520VLMs%250Adevelop%2520an%2520intrinsic%2520ability%2520to%2520understand%25203D%2520scenes%252C%2520which%2520paves%2520the%2520way%2520for%2520a%250Anoninvasive%2520approach%2520to%2520extending%2520pre-trained%2520VLMs%2520for%25203D%2520scene%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01428v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPT4Scene%3A%20Understand%203D%20Scenes%20from%20Videos%20with%20Vision-Language%20Models&entry.906535625=Zhangyang%20Qi%20and%20Zhixiong%20Zhang%20and%20Ye%20Fang%20and%20Jiaqi%20Wang%20and%20Hengshuang%20Zhao&entry.1292438233=%20%20In%20recent%20years%2C%202D%20Vision-Language%20Models%20%28VLMs%29%20have%20made%20significant%0Astrides%20in%20image-text%20understanding%20tasks.%20However%2C%20their%20performance%20in%203D%0Aspatial%20comprehension%2C%20which%20is%20critical%20for%20embodied%20intelligence%2C%20remains%0Alimited.%20Recent%20advances%20have%20leveraged%203D%20point%20clouds%20and%20multi-view%20images%0Aas%20inputs%2C%20yielding%20promising%20results.%20However%2C%20we%20propose%20exploring%20a%20purely%0Avision-based%20solution%20inspired%20by%20human%20perception%2C%20which%20merely%20relies%20on%0Avisual%20cues%20for%203D%20spatial%20understanding.%20This%20paper%20empirically%20investigates%0Athe%20limitations%20of%20VLMs%20in%203D%20spatial%20knowledge%2C%20revealing%20that%20their%20primary%0Ashortcoming%20lies%20in%20the%20lack%20of%20global-local%20correspondence%20between%20the%20scene%0Aand%20individual%20frames.%20To%20address%20this%2C%20we%20introduce%20GPT4Scene%2C%20a%20novel%20visual%0Aprompting%20paradigm%20in%20VLM%20training%20and%20inference%20that%20helps%20build%20the%0Aglobal-local%20relationship%2C%20significantly%20improving%20the%203D%20spatial%20understanding%0Aof%20indoor%20scenes.%20Specifically%2C%20GPT4Scene%20constructs%20a%203D%20Bird%27s%20Eye%20View%20%28BEV%29%0Aimage%20from%20the%20video%20and%20marks%20consistent%20object%20IDs%20across%20both%20frames%20and%20the%0ABEV%20image.%20The%20model%20then%20inputs%20the%20concatenated%20BEV%20image%20and%20video%20frames%0Awith%20markers.%20In%20zero-shot%20evaluations%2C%20GPT4Scene%20improves%20performance%20over%0Aclosed-source%20VLMs%20like%20GPT-4o.%20Additionally%2C%20we%20prepare%20a%20processed%20video%0Adataset%20consisting%20of%20165K%20text%20annotation%20to%20fine-tune%20open-source%20VLMs%2C%0Aachieving%20state-of-the-art%20performance%20on%20all%203D%20understanding%20tasks.%0ASurprisingly%2C%20after%20training%20with%20the%20GPT4Scene%20paradigm%2C%20VLMs%20consistently%0Aimprove%20during%20inference%2C%20even%20without%20visual%20prompting%20and%20BEV%20image%20as%0Aexplicit%20correspondence.%20It%20demonstrates%20that%20the%20proposed%20paradigm%20helps%20VLMs%0Adevelop%20an%20intrinsic%20ability%20to%20understand%203D%20scenes%2C%20which%20paves%20the%20way%20for%20a%0Anoninvasive%20approach%20to%20extending%20pre-trained%20VLMs%20for%203D%20scene%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01428v3&entry.124074799=Read"},
{"title": "LP-ICP: General Localizability-Aware Point Cloud Registration for Robust\n  Localization in Extreme Unstructured Environments", "author": "Haosong Yue and Qingyuan Xu and Fei Chen and Jia Pan and Weihai Chen", "abstract": "  The Iterative Closest Point (ICP) algorithm is a crucial component of\nLiDAR-based SLAM algorithms. However, its performance can be negatively\naffected in unstructured environments that lack features and geometric\nstructures, leading to low accuracy and poor robustness in localization and\nmapping. It is known that degeneracy caused by the lack of geometric\nconstraints can lead to errors in 6-DOF pose estimation along ill-conditioned\ndirections. Therefore, there is a need for a broader and more fine-grained\ndegeneracy detection and handling method. This paper proposes a new point cloud\nregistration framework, LP-ICP, that combines point-to-line and point-to-plane\ndistance metrics in the ICP algorithm, with localizability detection and\nhandling. LP-ICP consists of a localizability detection module and an\noptimization module. The localizability detection module performs\nlocalizability analysis by utilizing the correspondences between edge points\n(with low local smoothness) to lines and planar points (with high local\nsmoothness) to planes between the scan and the map. The localizability\ncontribution of individual correspondence constraints can be applied to a\nbroader range. The optimization module adds additional soft and hard\nconstraints to the optimization equations based on the localizability category.\nThis allows the pose to be constrained along ill-conditioned directions, with\nupdates either tending towards the constraint value or leaving the initial\nestimate unchanged. This improves accuracy and reduces fluctuations. The\nproposed method is extensively evaluated through experiments on both simulation\nand real-world datasets, demonstrating higher or comparable accuracy than the\nstate-of-the-art methods. The dataset and code of this paper will also be\nopen-sourced at https://github.com/xuqingyuan2000/LP-ICP.\n", "link": "http://arxiv.org/abs/2501.02580v2", "date": "2025-01-09", "relevancy": 3.0552, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6612}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6195}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LP-ICP%3A%20General%20Localizability-Aware%20Point%20Cloud%20Registration%20for%20Robust%0A%20%20Localization%20in%20Extreme%20Unstructured%20Environments&body=Title%3A%20LP-ICP%3A%20General%20Localizability-Aware%20Point%20Cloud%20Registration%20for%20Robust%0A%20%20Localization%20in%20Extreme%20Unstructured%20Environments%0AAuthor%3A%20Haosong%20Yue%20and%20Qingyuan%20Xu%20and%20Fei%20Chen%20and%20Jia%20Pan%20and%20Weihai%20Chen%0AAbstract%3A%20%20%20The%20Iterative%20Closest%20Point%20%28ICP%29%20algorithm%20is%20a%20crucial%20component%20of%0ALiDAR-based%20SLAM%20algorithms.%20However%2C%20its%20performance%20can%20be%20negatively%0Aaffected%20in%20unstructured%20environments%20that%20lack%20features%20and%20geometric%0Astructures%2C%20leading%20to%20low%20accuracy%20and%20poor%20robustness%20in%20localization%20and%0Amapping.%20It%20is%20known%20that%20degeneracy%20caused%20by%20the%20lack%20of%20geometric%0Aconstraints%20can%20lead%20to%20errors%20in%206-DOF%20pose%20estimation%20along%20ill-conditioned%0Adirections.%20Therefore%2C%20there%20is%20a%20need%20for%20a%20broader%20and%20more%20fine-grained%0Adegeneracy%20detection%20and%20handling%20method.%20This%20paper%20proposes%20a%20new%20point%20cloud%0Aregistration%20framework%2C%20LP-ICP%2C%20that%20combines%20point-to-line%20and%20point-to-plane%0Adistance%20metrics%20in%20the%20ICP%20algorithm%2C%20with%20localizability%20detection%20and%0Ahandling.%20LP-ICP%20consists%20of%20a%20localizability%20detection%20module%20and%20an%0Aoptimization%20module.%20The%20localizability%20detection%20module%20performs%0Alocalizability%20analysis%20by%20utilizing%20the%20correspondences%20between%20edge%20points%0A%28with%20low%20local%20smoothness%29%20to%20lines%20and%20planar%20points%20%28with%20high%20local%0Asmoothness%29%20to%20planes%20between%20the%20scan%20and%20the%20map.%20The%20localizability%0Acontribution%20of%20individual%20correspondence%20constraints%20can%20be%20applied%20to%20a%0Abroader%20range.%20The%20optimization%20module%20adds%20additional%20soft%20and%20hard%0Aconstraints%20to%20the%20optimization%20equations%20based%20on%20the%20localizability%20category.%0AThis%20allows%20the%20pose%20to%20be%20constrained%20along%20ill-conditioned%20directions%2C%20with%0Aupdates%20either%20tending%20towards%20the%20constraint%20value%20or%20leaving%20the%20initial%0Aestimate%20unchanged.%20This%20improves%20accuracy%20and%20reduces%20fluctuations.%20The%0Aproposed%20method%20is%20extensively%20evaluated%20through%20experiments%20on%20both%20simulation%0Aand%20real-world%20datasets%2C%20demonstrating%20higher%20or%20comparable%20accuracy%20than%20the%0Astate-of-the-art%20methods.%20The%20dataset%20and%20code%20of%20this%20paper%20will%20also%20be%0Aopen-sourced%20at%20https%3A//github.com/xuqingyuan2000/LP-ICP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02580v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLP-ICP%253A%2520General%2520Localizability-Aware%2520Point%2520Cloud%2520Registration%2520for%2520Robust%250A%2520%2520Localization%2520in%2520Extreme%2520Unstructured%2520Environments%26entry.906535625%3DHaosong%2520Yue%2520and%2520Qingyuan%2520Xu%2520and%2520Fei%2520Chen%2520and%2520Jia%2520Pan%2520and%2520Weihai%2520Chen%26entry.1292438233%3D%2520%2520The%2520Iterative%2520Closest%2520Point%2520%2528ICP%2529%2520algorithm%2520is%2520a%2520crucial%2520component%2520of%250ALiDAR-based%2520SLAM%2520algorithms.%2520However%252C%2520its%2520performance%2520can%2520be%2520negatively%250Aaffected%2520in%2520unstructured%2520environments%2520that%2520lack%2520features%2520and%2520geometric%250Astructures%252C%2520leading%2520to%2520low%2520accuracy%2520and%2520poor%2520robustness%2520in%2520localization%2520and%250Amapping.%2520It%2520is%2520known%2520that%2520degeneracy%2520caused%2520by%2520the%2520lack%2520of%2520geometric%250Aconstraints%2520can%2520lead%2520to%2520errors%2520in%25206-DOF%2520pose%2520estimation%2520along%2520ill-conditioned%250Adirections.%2520Therefore%252C%2520there%2520is%2520a%2520need%2520for%2520a%2520broader%2520and%2520more%2520fine-grained%250Adegeneracy%2520detection%2520and%2520handling%2520method.%2520This%2520paper%2520proposes%2520a%2520new%2520point%2520cloud%250Aregistration%2520framework%252C%2520LP-ICP%252C%2520that%2520combines%2520point-to-line%2520and%2520point-to-plane%250Adistance%2520metrics%2520in%2520the%2520ICP%2520algorithm%252C%2520with%2520localizability%2520detection%2520and%250Ahandling.%2520LP-ICP%2520consists%2520of%2520a%2520localizability%2520detection%2520module%2520and%2520an%250Aoptimization%2520module.%2520The%2520localizability%2520detection%2520module%2520performs%250Alocalizability%2520analysis%2520by%2520utilizing%2520the%2520correspondences%2520between%2520edge%2520points%250A%2528with%2520low%2520local%2520smoothness%2529%2520to%2520lines%2520and%2520planar%2520points%2520%2528with%2520high%2520local%250Asmoothness%2529%2520to%2520planes%2520between%2520the%2520scan%2520and%2520the%2520map.%2520The%2520localizability%250Acontribution%2520of%2520individual%2520correspondence%2520constraints%2520can%2520be%2520applied%2520to%2520a%250Abroader%2520range.%2520The%2520optimization%2520module%2520adds%2520additional%2520soft%2520and%2520hard%250Aconstraints%2520to%2520the%2520optimization%2520equations%2520based%2520on%2520the%2520localizability%2520category.%250AThis%2520allows%2520the%2520pose%2520to%2520be%2520constrained%2520along%2520ill-conditioned%2520directions%252C%2520with%250Aupdates%2520either%2520tending%2520towards%2520the%2520constraint%2520value%2520or%2520leaving%2520the%2520initial%250Aestimate%2520unchanged.%2520This%2520improves%2520accuracy%2520and%2520reduces%2520fluctuations.%2520The%250Aproposed%2520method%2520is%2520extensively%2520evaluated%2520through%2520experiments%2520on%2520both%2520simulation%250Aand%2520real-world%2520datasets%252C%2520demonstrating%2520higher%2520or%2520comparable%2520accuracy%2520than%2520the%250Astate-of-the-art%2520methods.%2520The%2520dataset%2520and%2520code%2520of%2520this%2520paper%2520will%2520also%2520be%250Aopen-sourced%2520at%2520https%253A//github.com/xuqingyuan2000/LP-ICP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02580v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LP-ICP%3A%20General%20Localizability-Aware%20Point%20Cloud%20Registration%20for%20Robust%0A%20%20Localization%20in%20Extreme%20Unstructured%20Environments&entry.906535625=Haosong%20Yue%20and%20Qingyuan%20Xu%20and%20Fei%20Chen%20and%20Jia%20Pan%20and%20Weihai%20Chen&entry.1292438233=%20%20The%20Iterative%20Closest%20Point%20%28ICP%29%20algorithm%20is%20a%20crucial%20component%20of%0ALiDAR-based%20SLAM%20algorithms.%20However%2C%20its%20performance%20can%20be%20negatively%0Aaffected%20in%20unstructured%20environments%20that%20lack%20features%20and%20geometric%0Astructures%2C%20leading%20to%20low%20accuracy%20and%20poor%20robustness%20in%20localization%20and%0Amapping.%20It%20is%20known%20that%20degeneracy%20caused%20by%20the%20lack%20of%20geometric%0Aconstraints%20can%20lead%20to%20errors%20in%206-DOF%20pose%20estimation%20along%20ill-conditioned%0Adirections.%20Therefore%2C%20there%20is%20a%20need%20for%20a%20broader%20and%20more%20fine-grained%0Adegeneracy%20detection%20and%20handling%20method.%20This%20paper%20proposes%20a%20new%20point%20cloud%0Aregistration%20framework%2C%20LP-ICP%2C%20that%20combines%20point-to-line%20and%20point-to-plane%0Adistance%20metrics%20in%20the%20ICP%20algorithm%2C%20with%20localizability%20detection%20and%0Ahandling.%20LP-ICP%20consists%20of%20a%20localizability%20detection%20module%20and%20an%0Aoptimization%20module.%20The%20localizability%20detection%20module%20performs%0Alocalizability%20analysis%20by%20utilizing%20the%20correspondences%20between%20edge%20points%0A%28with%20low%20local%20smoothness%29%20to%20lines%20and%20planar%20points%20%28with%20high%20local%0Asmoothness%29%20to%20planes%20between%20the%20scan%20and%20the%20map.%20The%20localizability%0Acontribution%20of%20individual%20correspondence%20constraints%20can%20be%20applied%20to%20a%0Abroader%20range.%20The%20optimization%20module%20adds%20additional%20soft%20and%20hard%0Aconstraints%20to%20the%20optimization%20equations%20based%20on%20the%20localizability%20category.%0AThis%20allows%20the%20pose%20to%20be%20constrained%20along%20ill-conditioned%20directions%2C%20with%0Aupdates%20either%20tending%20towards%20the%20constraint%20value%20or%20leaving%20the%20initial%0Aestimate%20unchanged.%20This%20improves%20accuracy%20and%20reduces%20fluctuations.%20The%0Aproposed%20method%20is%20extensively%20evaluated%20through%20experiments%20on%20both%20simulation%0Aand%20real-world%20datasets%2C%20demonstrating%20higher%20or%20comparable%20accuracy%20than%20the%0Astate-of-the-art%20methods.%20The%20dataset%20and%20code%20of%20this%20paper%20will%20also%20be%0Aopen-sourced%20at%20https%3A//github.com/xuqingyuan2000/LP-ICP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02580v2&entry.124074799=Read"},
{"title": "Voxel-Aggregated Feature Synthesis: Efficient Dense Mapping for\n  Simulated 3D Reasoning", "author": "Owen Burns and Rizwan Qureshi", "abstract": "  We address the issue of the exploding computational requirements of recent\nState-of-the-art (SOTA) open set multimodel 3D mapping (dense 3D mapping)\nalgorithms and present Voxel-Aggregated Feature Synthesis (VAFS), a novel\napproach to dense 3D mapping in simulation. Dense 3D mapping involves\nsegmenting and embedding sequential RGBD frames which are then fused into 3D.\nThis leads to redundant computation as the differences between frames are small\nbut all are individually segmented and embedded. This makes dense 3D mapping\nimpractical for research involving embodied agents in which the environment,\nand thus the mapping, must be modified with regularity. VAFS drastically\nreduces this computation by using the segmented point cloud computed by a\nsimulator's physics engine and synthesizing views of each region. This reduces\nthe number of features to embed from the number of captured RGBD frames to the\nnumber of objects in the scene, effectively allowing a \"ground truth\" semantic\nmap to be computed an order of magnitude faster than traditional methods. We\ntest the resulting representation by assessing the IoU scores of semantic\nqueries for different objects in the simulated scene, and find that VAFS\nexceeds the accuracy and speed of prior dense 3D mapping techniques.\n", "link": "http://arxiv.org/abs/2411.10616v2", "date": "2025-01-09", "relevancy": 3.0374, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6199}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6013}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Voxel-Aggregated%20Feature%20Synthesis%3A%20Efficient%20Dense%20Mapping%20for%0A%20%20Simulated%203D%20Reasoning&body=Title%3A%20Voxel-Aggregated%20Feature%20Synthesis%3A%20Efficient%20Dense%20Mapping%20for%0A%20%20Simulated%203D%20Reasoning%0AAuthor%3A%20Owen%20Burns%20and%20Rizwan%20Qureshi%0AAbstract%3A%20%20%20We%20address%20the%20issue%20of%20the%20exploding%20computational%20requirements%20of%20recent%0AState-of-the-art%20%28SOTA%29%20open%20set%20multimodel%203D%20mapping%20%28dense%203D%20mapping%29%0Aalgorithms%20and%20present%20Voxel-Aggregated%20Feature%20Synthesis%20%28VAFS%29%2C%20a%20novel%0Aapproach%20to%20dense%203D%20mapping%20in%20simulation.%20Dense%203D%20mapping%20involves%0Asegmenting%20and%20embedding%20sequential%20RGBD%20frames%20which%20are%20then%20fused%20into%203D.%0AThis%20leads%20to%20redundant%20computation%20as%20the%20differences%20between%20frames%20are%20small%0Abut%20all%20are%20individually%20segmented%20and%20embedded.%20This%20makes%20dense%203D%20mapping%0Aimpractical%20for%20research%20involving%20embodied%20agents%20in%20which%20the%20environment%2C%0Aand%20thus%20the%20mapping%2C%20must%20be%20modified%20with%20regularity.%20VAFS%20drastically%0Areduces%20this%20computation%20by%20using%20the%20segmented%20point%20cloud%20computed%20by%20a%0Asimulator%27s%20physics%20engine%20and%20synthesizing%20views%20of%20each%20region.%20This%20reduces%0Athe%20number%20of%20features%20to%20embed%20from%20the%20number%20of%20captured%20RGBD%20frames%20to%20the%0Anumber%20of%20objects%20in%20the%20scene%2C%20effectively%20allowing%20a%20%22ground%20truth%22%20semantic%0Amap%20to%20be%20computed%20an%20order%20of%20magnitude%20faster%20than%20traditional%20methods.%20We%0Atest%20the%20resulting%20representation%20by%20assessing%20the%20IoU%20scores%20of%20semantic%0Aqueries%20for%20different%20objects%20in%20the%20simulated%20scene%2C%20and%20find%20that%20VAFS%0Aexceeds%20the%20accuracy%20and%20speed%20of%20prior%20dense%203D%20mapping%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10616v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVoxel-Aggregated%2520Feature%2520Synthesis%253A%2520Efficient%2520Dense%2520Mapping%2520for%250A%2520%2520Simulated%25203D%2520Reasoning%26entry.906535625%3DOwen%2520Burns%2520and%2520Rizwan%2520Qureshi%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520issue%2520of%2520the%2520exploding%2520computational%2520requirements%2520of%2520recent%250AState-of-the-art%2520%2528SOTA%2529%2520open%2520set%2520multimodel%25203D%2520mapping%2520%2528dense%25203D%2520mapping%2529%250Aalgorithms%2520and%2520present%2520Voxel-Aggregated%2520Feature%2520Synthesis%2520%2528VAFS%2529%252C%2520a%2520novel%250Aapproach%2520to%2520dense%25203D%2520mapping%2520in%2520simulation.%2520Dense%25203D%2520mapping%2520involves%250Asegmenting%2520and%2520embedding%2520sequential%2520RGBD%2520frames%2520which%2520are%2520then%2520fused%2520into%25203D.%250AThis%2520leads%2520to%2520redundant%2520computation%2520as%2520the%2520differences%2520between%2520frames%2520are%2520small%250Abut%2520all%2520are%2520individually%2520segmented%2520and%2520embedded.%2520This%2520makes%2520dense%25203D%2520mapping%250Aimpractical%2520for%2520research%2520involving%2520embodied%2520agents%2520in%2520which%2520the%2520environment%252C%250Aand%2520thus%2520the%2520mapping%252C%2520must%2520be%2520modified%2520with%2520regularity.%2520VAFS%2520drastically%250Areduces%2520this%2520computation%2520by%2520using%2520the%2520segmented%2520point%2520cloud%2520computed%2520by%2520a%250Asimulator%2527s%2520physics%2520engine%2520and%2520synthesizing%2520views%2520of%2520each%2520region.%2520This%2520reduces%250Athe%2520number%2520of%2520features%2520to%2520embed%2520from%2520the%2520number%2520of%2520captured%2520RGBD%2520frames%2520to%2520the%250Anumber%2520of%2520objects%2520in%2520the%2520scene%252C%2520effectively%2520allowing%2520a%2520%2522ground%2520truth%2522%2520semantic%250Amap%2520to%2520be%2520computed%2520an%2520order%2520of%2520magnitude%2520faster%2520than%2520traditional%2520methods.%2520We%250Atest%2520the%2520resulting%2520representation%2520by%2520assessing%2520the%2520IoU%2520scores%2520of%2520semantic%250Aqueries%2520for%2520different%2520objects%2520in%2520the%2520simulated%2520scene%252C%2520and%2520find%2520that%2520VAFS%250Aexceeds%2520the%2520accuracy%2520and%2520speed%2520of%2520prior%2520dense%25203D%2520mapping%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10616v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Voxel-Aggregated%20Feature%20Synthesis%3A%20Efficient%20Dense%20Mapping%20for%0A%20%20Simulated%203D%20Reasoning&entry.906535625=Owen%20Burns%20and%20Rizwan%20Qureshi&entry.1292438233=%20%20We%20address%20the%20issue%20of%20the%20exploding%20computational%20requirements%20of%20recent%0AState-of-the-art%20%28SOTA%29%20open%20set%20multimodel%203D%20mapping%20%28dense%203D%20mapping%29%0Aalgorithms%20and%20present%20Voxel-Aggregated%20Feature%20Synthesis%20%28VAFS%29%2C%20a%20novel%0Aapproach%20to%20dense%203D%20mapping%20in%20simulation.%20Dense%203D%20mapping%20involves%0Asegmenting%20and%20embedding%20sequential%20RGBD%20frames%20which%20are%20then%20fused%20into%203D.%0AThis%20leads%20to%20redundant%20computation%20as%20the%20differences%20between%20frames%20are%20small%0Abut%20all%20are%20individually%20segmented%20and%20embedded.%20This%20makes%20dense%203D%20mapping%0Aimpractical%20for%20research%20involving%20embodied%20agents%20in%20which%20the%20environment%2C%0Aand%20thus%20the%20mapping%2C%20must%20be%20modified%20with%20regularity.%20VAFS%20drastically%0Areduces%20this%20computation%20by%20using%20the%20segmented%20point%20cloud%20computed%20by%20a%0Asimulator%27s%20physics%20engine%20and%20synthesizing%20views%20of%20each%20region.%20This%20reduces%0Athe%20number%20of%20features%20to%20embed%20from%20the%20number%20of%20captured%20RGBD%20frames%20to%20the%0Anumber%20of%20objects%20in%20the%20scene%2C%20effectively%20allowing%20a%20%22ground%20truth%22%20semantic%0Amap%20to%20be%20computed%20an%20order%20of%20magnitude%20faster%20than%20traditional%20methods.%20We%0Atest%20the%20resulting%20representation%20by%20assessing%20the%20IoU%20scores%20of%20semantic%0Aqueries%20for%20different%20objects%20in%20the%20simulated%20scene%2C%20and%20find%20that%20VAFS%0Aexceeds%20the%20accuracy%20and%20speed%20of%20prior%20dense%203D%20mapping%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10616v2&entry.124074799=Read"},
{"title": "Towards Balanced Continual Multi-Modal Learning in Human Pose Estimation", "author": "Jiaxuan Peng and Mengshi Qi and Dong Zhao and Huadong Ma", "abstract": "  3D human pose estimation (3D HPE) has emerged as a prominent research topic,\nparticularly in the realm of RGB-based methods. However, RGB images are\nsusceptible to limitations such as sensitivity to lighting conditions and\npotential user discomfort. Consequently, multi-modal sensing, which leverages\nnon-intrusive sensors, is gaining increasing attention. Nevertheless,\nmulti-modal 3D HPE still faces challenges, including modality imbalance and the\nimperative for continual learning. In this work, we introduce a novel balanced\ncontinual multi-modal learning method for 3D HPE, which harnesses the power of\nRGB, LiDAR, mmWave, and WiFi. Specifically, we propose a Shapley value-based\ncontribution algorithm to quantify the contribution of each modality and\nidentify modality imbalance. To address this imbalance, we employ a re-learning\nstrategy. Furthermore, recognizing that raw data is prone to noise\ncontamination, we develop a novel denoising continual learning approach. This\napproach incorporates a noise identification and separation module to mitigate\nthe adverse effects of noise and collaborates with the balanced learning\nstrategy to enhance optimization. Additionally, an adaptive EWC mechanism is\nemployed to alleviate catastrophic forgetting. We conduct extensive experiments\non the widely-adopted multi-modal dataset, MM-Fi, which demonstrate the\nsuperiority of our approach in boosting 3D pose estimation and mitigating\ncatastrophic forgetting in complex scenarios. We will release our codes.\n", "link": "http://arxiv.org/abs/2501.05264v1", "date": "2025-01-09", "relevancy": 3.0152, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6101}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5997}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Balanced%20Continual%20Multi-Modal%20Learning%20in%20Human%20Pose%20Estimation&body=Title%3A%20Towards%20Balanced%20Continual%20Multi-Modal%20Learning%20in%20Human%20Pose%20Estimation%0AAuthor%3A%20Jiaxuan%20Peng%20and%20Mengshi%20Qi%20and%20Dong%20Zhao%20and%20Huadong%20Ma%0AAbstract%3A%20%20%203D%20human%20pose%20estimation%20%283D%20HPE%29%20has%20emerged%20as%20a%20prominent%20research%20topic%2C%0Aparticularly%20in%20the%20realm%20of%20RGB-based%20methods.%20However%2C%20RGB%20images%20are%0Asusceptible%20to%20limitations%20such%20as%20sensitivity%20to%20lighting%20conditions%20and%0Apotential%20user%20discomfort.%20Consequently%2C%20multi-modal%20sensing%2C%20which%20leverages%0Anon-intrusive%20sensors%2C%20is%20gaining%20increasing%20attention.%20Nevertheless%2C%0Amulti-modal%203D%20HPE%20still%20faces%20challenges%2C%20including%20modality%20imbalance%20and%20the%0Aimperative%20for%20continual%20learning.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20balanced%0Acontinual%20multi-modal%20learning%20method%20for%203D%20HPE%2C%20which%20harnesses%20the%20power%20of%0ARGB%2C%20LiDAR%2C%20mmWave%2C%20and%20WiFi.%20Specifically%2C%20we%20propose%20a%20Shapley%20value-based%0Acontribution%20algorithm%20to%20quantify%20the%20contribution%20of%20each%20modality%20and%0Aidentify%20modality%20imbalance.%20To%20address%20this%20imbalance%2C%20we%20employ%20a%20re-learning%0Astrategy.%20Furthermore%2C%20recognizing%20that%20raw%20data%20is%20prone%20to%20noise%0Acontamination%2C%20we%20develop%20a%20novel%20denoising%20continual%20learning%20approach.%20This%0Aapproach%20incorporates%20a%20noise%20identification%20and%20separation%20module%20to%20mitigate%0Athe%20adverse%20effects%20of%20noise%20and%20collaborates%20with%20the%20balanced%20learning%0Astrategy%20to%20enhance%20optimization.%20Additionally%2C%20an%20adaptive%20EWC%20mechanism%20is%0Aemployed%20to%20alleviate%20catastrophic%20forgetting.%20We%20conduct%20extensive%20experiments%0Aon%20the%20widely-adopted%20multi-modal%20dataset%2C%20MM-Fi%2C%20which%20demonstrate%20the%0Asuperiority%20of%20our%20approach%20in%20boosting%203D%20pose%20estimation%20and%20mitigating%0Acatastrophic%20forgetting%20in%20complex%20scenarios.%20We%20will%20release%20our%20codes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05264v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Balanced%2520Continual%2520Multi-Modal%2520Learning%2520in%2520Human%2520Pose%2520Estimation%26entry.906535625%3DJiaxuan%2520Peng%2520and%2520Mengshi%2520Qi%2520and%2520Dong%2520Zhao%2520and%2520Huadong%2520Ma%26entry.1292438233%3D%2520%25203D%2520human%2520pose%2520estimation%2520%25283D%2520HPE%2529%2520has%2520emerged%2520as%2520a%2520prominent%2520research%2520topic%252C%250Aparticularly%2520in%2520the%2520realm%2520of%2520RGB-based%2520methods.%2520However%252C%2520RGB%2520images%2520are%250Asusceptible%2520to%2520limitations%2520such%2520as%2520sensitivity%2520to%2520lighting%2520conditions%2520and%250Apotential%2520user%2520discomfort.%2520Consequently%252C%2520multi-modal%2520sensing%252C%2520which%2520leverages%250Anon-intrusive%2520sensors%252C%2520is%2520gaining%2520increasing%2520attention.%2520Nevertheless%252C%250Amulti-modal%25203D%2520HPE%2520still%2520faces%2520challenges%252C%2520including%2520modality%2520imbalance%2520and%2520the%250Aimperative%2520for%2520continual%2520learning.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520balanced%250Acontinual%2520multi-modal%2520learning%2520method%2520for%25203D%2520HPE%252C%2520which%2520harnesses%2520the%2520power%2520of%250ARGB%252C%2520LiDAR%252C%2520mmWave%252C%2520and%2520WiFi.%2520Specifically%252C%2520we%2520propose%2520a%2520Shapley%2520value-based%250Acontribution%2520algorithm%2520to%2520quantify%2520the%2520contribution%2520of%2520each%2520modality%2520and%250Aidentify%2520modality%2520imbalance.%2520To%2520address%2520this%2520imbalance%252C%2520we%2520employ%2520a%2520re-learning%250Astrategy.%2520Furthermore%252C%2520recognizing%2520that%2520raw%2520data%2520is%2520prone%2520to%2520noise%250Acontamination%252C%2520we%2520develop%2520a%2520novel%2520denoising%2520continual%2520learning%2520approach.%2520This%250Aapproach%2520incorporates%2520a%2520noise%2520identification%2520and%2520separation%2520module%2520to%2520mitigate%250Athe%2520adverse%2520effects%2520of%2520noise%2520and%2520collaborates%2520with%2520the%2520balanced%2520learning%250Astrategy%2520to%2520enhance%2520optimization.%2520Additionally%252C%2520an%2520adaptive%2520EWC%2520mechanism%2520is%250Aemployed%2520to%2520alleviate%2520catastrophic%2520forgetting.%2520We%2520conduct%2520extensive%2520experiments%250Aon%2520the%2520widely-adopted%2520multi-modal%2520dataset%252C%2520MM-Fi%252C%2520which%2520demonstrate%2520the%250Asuperiority%2520of%2520our%2520approach%2520in%2520boosting%25203D%2520pose%2520estimation%2520and%2520mitigating%250Acatastrophic%2520forgetting%2520in%2520complex%2520scenarios.%2520We%2520will%2520release%2520our%2520codes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05264v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Balanced%20Continual%20Multi-Modal%20Learning%20in%20Human%20Pose%20Estimation&entry.906535625=Jiaxuan%20Peng%20and%20Mengshi%20Qi%20and%20Dong%20Zhao%20and%20Huadong%20Ma&entry.1292438233=%20%203D%20human%20pose%20estimation%20%283D%20HPE%29%20has%20emerged%20as%20a%20prominent%20research%20topic%2C%0Aparticularly%20in%20the%20realm%20of%20RGB-based%20methods.%20However%2C%20RGB%20images%20are%0Asusceptible%20to%20limitations%20such%20as%20sensitivity%20to%20lighting%20conditions%20and%0Apotential%20user%20discomfort.%20Consequently%2C%20multi-modal%20sensing%2C%20which%20leverages%0Anon-intrusive%20sensors%2C%20is%20gaining%20increasing%20attention.%20Nevertheless%2C%0Amulti-modal%203D%20HPE%20still%20faces%20challenges%2C%20including%20modality%20imbalance%20and%20the%0Aimperative%20for%20continual%20learning.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20balanced%0Acontinual%20multi-modal%20learning%20method%20for%203D%20HPE%2C%20which%20harnesses%20the%20power%20of%0ARGB%2C%20LiDAR%2C%20mmWave%2C%20and%20WiFi.%20Specifically%2C%20we%20propose%20a%20Shapley%20value-based%0Acontribution%20algorithm%20to%20quantify%20the%20contribution%20of%20each%20modality%20and%0Aidentify%20modality%20imbalance.%20To%20address%20this%20imbalance%2C%20we%20employ%20a%20re-learning%0Astrategy.%20Furthermore%2C%20recognizing%20that%20raw%20data%20is%20prone%20to%20noise%0Acontamination%2C%20we%20develop%20a%20novel%20denoising%20continual%20learning%20approach.%20This%0Aapproach%20incorporates%20a%20noise%20identification%20and%20separation%20module%20to%20mitigate%0Athe%20adverse%20effects%20of%20noise%20and%20collaborates%20with%20the%20balanced%20learning%0Astrategy%20to%20enhance%20optimization.%20Additionally%2C%20an%20adaptive%20EWC%20mechanism%20is%0Aemployed%20to%20alleviate%20catastrophic%20forgetting.%20We%20conduct%20extensive%20experiments%0Aon%20the%20widely-adopted%20multi-modal%20dataset%2C%20MM-Fi%2C%20which%20demonstrate%20the%0Asuperiority%20of%20our%20approach%20in%20boosting%203D%20pose%20estimation%20and%20mitigating%0Acatastrophic%20forgetting%20in%20complex%20scenarios.%20We%20will%20release%20our%20codes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05264v1&entry.124074799=Read"},
{"title": "Discovering Hidden Visual Concepts Beyond Linguistic Input in Infant\n  Learning", "author": "Xueyi Ke and Satoshi Tsutsui and Yayun Zhang and Bihan Wen", "abstract": "  Infants develop complex visual understanding rapidly, even preceding of the\nacquisition of linguistic inputs. As computer vision seeks to replicate the\nhuman vision system, understanding infant visual development may offer valuable\ninsights. In this paper, we present an interdisciplinary study exploring this\nquestion: can a computational model that imitates the infant learning process\ndevelop broader visual concepts that extend beyond the vocabulary it has heard,\nsimilar to how infants naturally learn? To investigate this, we analyze a\nrecently published model in Science by Vong et al.,which is trained on\nlongitudinal, egocentric images of a single child paired with transcribed\nparental speech. We introduce a training-free framework that can discover\nvisual concept neurons hidden in the model's internal representations. Our\nfindings show that these neurons can classify objects outside its original\nvocabulary. Furthermore, we compare the visual representations in infant-like\nmodels with those in moder computer vision models, such as CLIP or ImageNet\npre-trained model, highlighting key similarities and differences. Ultimately,\nour work bridges cognitive science and computer vision by analyzing the\ninternal representations of a computational model trained on an infant's visual\nand linguistic inputs.\n", "link": "http://arxiv.org/abs/2501.05205v1", "date": "2025-01-09", "relevancy": 3.0066, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6335}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6335}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discovering%20Hidden%20Visual%20Concepts%20Beyond%20Linguistic%20Input%20in%20Infant%0A%20%20Learning&body=Title%3A%20Discovering%20Hidden%20Visual%20Concepts%20Beyond%20Linguistic%20Input%20in%20Infant%0A%20%20Learning%0AAuthor%3A%20Xueyi%20Ke%20and%20Satoshi%20Tsutsui%20and%20Yayun%20Zhang%20and%20Bihan%20Wen%0AAbstract%3A%20%20%20Infants%20develop%20complex%20visual%20understanding%20rapidly%2C%20even%20preceding%20of%20the%0Aacquisition%20of%20linguistic%20inputs.%20As%20computer%20vision%20seeks%20to%20replicate%20the%0Ahuman%20vision%20system%2C%20understanding%20infant%20visual%20development%20may%20offer%20valuable%0Ainsights.%20In%20this%20paper%2C%20we%20present%20an%20interdisciplinary%20study%20exploring%20this%0Aquestion%3A%20can%20a%20computational%20model%20that%20imitates%20the%20infant%20learning%20process%0Adevelop%20broader%20visual%20concepts%20that%20extend%20beyond%20the%20vocabulary%20it%20has%20heard%2C%0Asimilar%20to%20how%20infants%20naturally%20learn%3F%20To%20investigate%20this%2C%20we%20analyze%20a%0Arecently%20published%20model%20in%20Science%20by%20Vong%20et%20al.%2Cwhich%20is%20trained%20on%0Alongitudinal%2C%20egocentric%20images%20of%20a%20single%20child%20paired%20with%20transcribed%0Aparental%20speech.%20We%20introduce%20a%20training-free%20framework%20that%20can%20discover%0Avisual%20concept%20neurons%20hidden%20in%20the%20model%27s%20internal%20representations.%20Our%0Afindings%20show%20that%20these%20neurons%20can%20classify%20objects%20outside%20its%20original%0Avocabulary.%20Furthermore%2C%20we%20compare%20the%20visual%20representations%20in%20infant-like%0Amodels%20with%20those%20in%20moder%20computer%20vision%20models%2C%20such%20as%20CLIP%20or%20ImageNet%0Apre-trained%20model%2C%20highlighting%20key%20similarities%20and%20differences.%20Ultimately%2C%0Aour%20work%20bridges%20cognitive%20science%20and%20computer%20vision%20by%20analyzing%20the%0Ainternal%20representations%20of%20a%20computational%20model%20trained%20on%20an%20infant%27s%20visual%0Aand%20linguistic%20inputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05205v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscovering%2520Hidden%2520Visual%2520Concepts%2520Beyond%2520Linguistic%2520Input%2520in%2520Infant%250A%2520%2520Learning%26entry.906535625%3DXueyi%2520Ke%2520and%2520Satoshi%2520Tsutsui%2520and%2520Yayun%2520Zhang%2520and%2520Bihan%2520Wen%26entry.1292438233%3D%2520%2520Infants%2520develop%2520complex%2520visual%2520understanding%2520rapidly%252C%2520even%2520preceding%2520of%2520the%250Aacquisition%2520of%2520linguistic%2520inputs.%2520As%2520computer%2520vision%2520seeks%2520to%2520replicate%2520the%250Ahuman%2520vision%2520system%252C%2520understanding%2520infant%2520visual%2520development%2520may%2520offer%2520valuable%250Ainsights.%2520In%2520this%2520paper%252C%2520we%2520present%2520an%2520interdisciplinary%2520study%2520exploring%2520this%250Aquestion%253A%2520can%2520a%2520computational%2520model%2520that%2520imitates%2520the%2520infant%2520learning%2520process%250Adevelop%2520broader%2520visual%2520concepts%2520that%2520extend%2520beyond%2520the%2520vocabulary%2520it%2520has%2520heard%252C%250Asimilar%2520to%2520how%2520infants%2520naturally%2520learn%253F%2520To%2520investigate%2520this%252C%2520we%2520analyze%2520a%250Arecently%2520published%2520model%2520in%2520Science%2520by%2520Vong%2520et%2520al.%252Cwhich%2520is%2520trained%2520on%250Alongitudinal%252C%2520egocentric%2520images%2520of%2520a%2520single%2520child%2520paired%2520with%2520transcribed%250Aparental%2520speech.%2520We%2520introduce%2520a%2520training-free%2520framework%2520that%2520can%2520discover%250Avisual%2520concept%2520neurons%2520hidden%2520in%2520the%2520model%2527s%2520internal%2520representations.%2520Our%250Afindings%2520show%2520that%2520these%2520neurons%2520can%2520classify%2520objects%2520outside%2520its%2520original%250Avocabulary.%2520Furthermore%252C%2520we%2520compare%2520the%2520visual%2520representations%2520in%2520infant-like%250Amodels%2520with%2520those%2520in%2520moder%2520computer%2520vision%2520models%252C%2520such%2520as%2520CLIP%2520or%2520ImageNet%250Apre-trained%2520model%252C%2520highlighting%2520key%2520similarities%2520and%2520differences.%2520Ultimately%252C%250Aour%2520work%2520bridges%2520cognitive%2520science%2520and%2520computer%2520vision%2520by%2520analyzing%2520the%250Ainternal%2520representations%2520of%2520a%2520computational%2520model%2520trained%2520on%2520an%2520infant%2527s%2520visual%250Aand%2520linguistic%2520inputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05205v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discovering%20Hidden%20Visual%20Concepts%20Beyond%20Linguistic%20Input%20in%20Infant%0A%20%20Learning&entry.906535625=Xueyi%20Ke%20and%20Satoshi%20Tsutsui%20and%20Yayun%20Zhang%20and%20Bihan%20Wen&entry.1292438233=%20%20Infants%20develop%20complex%20visual%20understanding%20rapidly%2C%20even%20preceding%20of%20the%0Aacquisition%20of%20linguistic%20inputs.%20As%20computer%20vision%20seeks%20to%20replicate%20the%0Ahuman%20vision%20system%2C%20understanding%20infant%20visual%20development%20may%20offer%20valuable%0Ainsights.%20In%20this%20paper%2C%20we%20present%20an%20interdisciplinary%20study%20exploring%20this%0Aquestion%3A%20can%20a%20computational%20model%20that%20imitates%20the%20infant%20learning%20process%0Adevelop%20broader%20visual%20concepts%20that%20extend%20beyond%20the%20vocabulary%20it%20has%20heard%2C%0Asimilar%20to%20how%20infants%20naturally%20learn%3F%20To%20investigate%20this%2C%20we%20analyze%20a%0Arecently%20published%20model%20in%20Science%20by%20Vong%20et%20al.%2Cwhich%20is%20trained%20on%0Alongitudinal%2C%20egocentric%20images%20of%20a%20single%20child%20paired%20with%20transcribed%0Aparental%20speech.%20We%20introduce%20a%20training-free%20framework%20that%20can%20discover%0Avisual%20concept%20neurons%20hidden%20in%20the%20model%27s%20internal%20representations.%20Our%0Afindings%20show%20that%20these%20neurons%20can%20classify%20objects%20outside%20its%20original%0Avocabulary.%20Furthermore%2C%20we%20compare%20the%20visual%20representations%20in%20infant-like%0Amodels%20with%20those%20in%20moder%20computer%20vision%20models%2C%20such%20as%20CLIP%20or%20ImageNet%0Apre-trained%20model%2C%20highlighting%20key%20similarities%20and%20differences.%20Ultimately%2C%0Aour%20work%20bridges%20cognitive%20science%20and%20computer%20vision%20by%20analyzing%20the%0Ainternal%20representations%20of%20a%20computational%20model%20trained%20on%20an%20infant%27s%20visual%0Aand%20linguistic%20inputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05205v1&entry.124074799=Read"},
{"title": "ReFocus: Visual Editing as a Chain of Thought for Structured Image\n  Understanding", "author": "Xingyu Fu and Minqian Liu and Zhengyuan Yang and John Corring and Yijuan Lu and Jianwei Yang and Dan Roth and Dinei Florencio and Cha Zhang", "abstract": "  Structured image understanding, such as interpreting tables and charts,\nrequires strategically refocusing across various structures and texts within an\nimage, forming a reasoning sequence to arrive at the final answer. However,\ncurrent multimodal large language models (LLMs) lack this multihop selective\nattention capability. In this work, we introduce ReFocus, a simple yet\neffective framework that equips multimodal LLMs with the ability to generate\n\"visual thoughts\" by performing visual editing on the input image through code,\nshifting and refining their visual focuses. Specifically, ReFocus enables\nmultimodal LLMs to generate Python codes to call tools and modify the input\nimage, sequentially drawing boxes, highlighting sections, and masking out\nareas, thereby enhancing the visual reasoning process. We experiment upon a\nwide range of structured image understanding tasks involving tables and charts.\nReFocus largely improves performance on all tasks over GPT-4o without visual\nediting, yielding an average gain of 11.0% on table tasks and 6.8% on chart\ntasks. We present an in-depth analysis of the effects of different visual\nedits, and reasons why ReFocus can improve the performance without introducing\nadditional information. Further, we collect a 14k training set using ReFocus,\nand prove that such visual chain-of-thought with intermediate information\noffers a better supervision than standard VQA data, reaching a 8.0% average\ngain over the same model trained with QA pairs and 2.6% over CoT.\n", "link": "http://arxiv.org/abs/2501.05452v1", "date": "2025-01-09", "relevancy": 2.9121, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6029}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6029}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReFocus%3A%20Visual%20Editing%20as%20a%20Chain%20of%20Thought%20for%20Structured%20Image%0A%20%20Understanding&body=Title%3A%20ReFocus%3A%20Visual%20Editing%20as%20a%20Chain%20of%20Thought%20for%20Structured%20Image%0A%20%20Understanding%0AAuthor%3A%20Xingyu%20Fu%20and%20Minqian%20Liu%20and%20Zhengyuan%20Yang%20and%20John%20Corring%20and%20Yijuan%20Lu%20and%20Jianwei%20Yang%20and%20Dan%20Roth%20and%20Dinei%20Florencio%20and%20Cha%20Zhang%0AAbstract%3A%20%20%20Structured%20image%20understanding%2C%20such%20as%20interpreting%20tables%20and%20charts%2C%0Arequires%20strategically%20refocusing%20across%20various%20structures%20and%20texts%20within%20an%0Aimage%2C%20forming%20a%20reasoning%20sequence%20to%20arrive%20at%20the%20final%20answer.%20However%2C%0Acurrent%20multimodal%20large%20language%20models%20%28LLMs%29%20lack%20this%20multihop%20selective%0Aattention%20capability.%20In%20this%20work%2C%20we%20introduce%20ReFocus%2C%20a%20simple%20yet%0Aeffective%20framework%20that%20equips%20multimodal%20LLMs%20with%20the%20ability%20to%20generate%0A%22visual%20thoughts%22%20by%20performing%20visual%20editing%20on%20the%20input%20image%20through%20code%2C%0Ashifting%20and%20refining%20their%20visual%20focuses.%20Specifically%2C%20ReFocus%20enables%0Amultimodal%20LLMs%20to%20generate%20Python%20codes%20to%20call%20tools%20and%20modify%20the%20input%0Aimage%2C%20sequentially%20drawing%20boxes%2C%20highlighting%20sections%2C%20and%20masking%20out%0Aareas%2C%20thereby%20enhancing%20the%20visual%20reasoning%20process.%20We%20experiment%20upon%20a%0Awide%20range%20of%20structured%20image%20understanding%20tasks%20involving%20tables%20and%20charts.%0AReFocus%20largely%20improves%20performance%20on%20all%20tasks%20over%20GPT-4o%20without%20visual%0Aediting%2C%20yielding%20an%20average%20gain%20of%2011.0%25%20on%20table%20tasks%20and%206.8%25%20on%20chart%0Atasks.%20We%20present%20an%20in-depth%20analysis%20of%20the%20effects%20of%20different%20visual%0Aedits%2C%20and%20reasons%20why%20ReFocus%20can%20improve%20the%20performance%20without%20introducing%0Aadditional%20information.%20Further%2C%20we%20collect%20a%2014k%20training%20set%20using%20ReFocus%2C%0Aand%20prove%20that%20such%20visual%20chain-of-thought%20with%20intermediate%20information%0Aoffers%20a%20better%20supervision%20than%20standard%20VQA%20data%2C%20reaching%20a%208.0%25%20average%0Again%20over%20the%20same%20model%20trained%20with%20QA%20pairs%20and%202.6%25%20over%20CoT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05452v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReFocus%253A%2520Visual%2520Editing%2520as%2520a%2520Chain%2520of%2520Thought%2520for%2520Structured%2520Image%250A%2520%2520Understanding%26entry.906535625%3DXingyu%2520Fu%2520and%2520Minqian%2520Liu%2520and%2520Zhengyuan%2520Yang%2520and%2520John%2520Corring%2520and%2520Yijuan%2520Lu%2520and%2520Jianwei%2520Yang%2520and%2520Dan%2520Roth%2520and%2520Dinei%2520Florencio%2520and%2520Cha%2520Zhang%26entry.1292438233%3D%2520%2520Structured%2520image%2520understanding%252C%2520such%2520as%2520interpreting%2520tables%2520and%2520charts%252C%250Arequires%2520strategically%2520refocusing%2520across%2520various%2520structures%2520and%2520texts%2520within%2520an%250Aimage%252C%2520forming%2520a%2520reasoning%2520sequence%2520to%2520arrive%2520at%2520the%2520final%2520answer.%2520However%252C%250Acurrent%2520multimodal%2520large%2520language%2520models%2520%2528LLMs%2529%2520lack%2520this%2520multihop%2520selective%250Aattention%2520capability.%2520In%2520this%2520work%252C%2520we%2520introduce%2520ReFocus%252C%2520a%2520simple%2520yet%250Aeffective%2520framework%2520that%2520equips%2520multimodal%2520LLMs%2520with%2520the%2520ability%2520to%2520generate%250A%2522visual%2520thoughts%2522%2520by%2520performing%2520visual%2520editing%2520on%2520the%2520input%2520image%2520through%2520code%252C%250Ashifting%2520and%2520refining%2520their%2520visual%2520focuses.%2520Specifically%252C%2520ReFocus%2520enables%250Amultimodal%2520LLMs%2520to%2520generate%2520Python%2520codes%2520to%2520call%2520tools%2520and%2520modify%2520the%2520input%250Aimage%252C%2520sequentially%2520drawing%2520boxes%252C%2520highlighting%2520sections%252C%2520and%2520masking%2520out%250Aareas%252C%2520thereby%2520enhancing%2520the%2520visual%2520reasoning%2520process.%2520We%2520experiment%2520upon%2520a%250Awide%2520range%2520of%2520structured%2520image%2520understanding%2520tasks%2520involving%2520tables%2520and%2520charts.%250AReFocus%2520largely%2520improves%2520performance%2520on%2520all%2520tasks%2520over%2520GPT-4o%2520without%2520visual%250Aediting%252C%2520yielding%2520an%2520average%2520gain%2520of%252011.0%2525%2520on%2520table%2520tasks%2520and%25206.8%2525%2520on%2520chart%250Atasks.%2520We%2520present%2520an%2520in-depth%2520analysis%2520of%2520the%2520effects%2520of%2520different%2520visual%250Aedits%252C%2520and%2520reasons%2520why%2520ReFocus%2520can%2520improve%2520the%2520performance%2520without%2520introducing%250Aadditional%2520information.%2520Further%252C%2520we%2520collect%2520a%252014k%2520training%2520set%2520using%2520ReFocus%252C%250Aand%2520prove%2520that%2520such%2520visual%2520chain-of-thought%2520with%2520intermediate%2520information%250Aoffers%2520a%2520better%2520supervision%2520than%2520standard%2520VQA%2520data%252C%2520reaching%2520a%25208.0%2525%2520average%250Again%2520over%2520the%2520same%2520model%2520trained%2520with%2520QA%2520pairs%2520and%25202.6%2525%2520over%2520CoT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05452v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReFocus%3A%20Visual%20Editing%20as%20a%20Chain%20of%20Thought%20for%20Structured%20Image%0A%20%20Understanding&entry.906535625=Xingyu%20Fu%20and%20Minqian%20Liu%20and%20Zhengyuan%20Yang%20and%20John%20Corring%20and%20Yijuan%20Lu%20and%20Jianwei%20Yang%20and%20Dan%20Roth%20and%20Dinei%20Florencio%20and%20Cha%20Zhang&entry.1292438233=%20%20Structured%20image%20understanding%2C%20such%20as%20interpreting%20tables%20and%20charts%2C%0Arequires%20strategically%20refocusing%20across%20various%20structures%20and%20texts%20within%20an%0Aimage%2C%20forming%20a%20reasoning%20sequence%20to%20arrive%20at%20the%20final%20answer.%20However%2C%0Acurrent%20multimodal%20large%20language%20models%20%28LLMs%29%20lack%20this%20multihop%20selective%0Aattention%20capability.%20In%20this%20work%2C%20we%20introduce%20ReFocus%2C%20a%20simple%20yet%0Aeffective%20framework%20that%20equips%20multimodal%20LLMs%20with%20the%20ability%20to%20generate%0A%22visual%20thoughts%22%20by%20performing%20visual%20editing%20on%20the%20input%20image%20through%20code%2C%0Ashifting%20and%20refining%20their%20visual%20focuses.%20Specifically%2C%20ReFocus%20enables%0Amultimodal%20LLMs%20to%20generate%20Python%20codes%20to%20call%20tools%20and%20modify%20the%20input%0Aimage%2C%20sequentially%20drawing%20boxes%2C%20highlighting%20sections%2C%20and%20masking%20out%0Aareas%2C%20thereby%20enhancing%20the%20visual%20reasoning%20process.%20We%20experiment%20upon%20a%0Awide%20range%20of%20structured%20image%20understanding%20tasks%20involving%20tables%20and%20charts.%0AReFocus%20largely%20improves%20performance%20on%20all%20tasks%20over%20GPT-4o%20without%20visual%0Aediting%2C%20yielding%20an%20average%20gain%20of%2011.0%25%20on%20table%20tasks%20and%206.8%25%20on%20chart%0Atasks.%20We%20present%20an%20in-depth%20analysis%20of%20the%20effects%20of%20different%20visual%0Aedits%2C%20and%20reasons%20why%20ReFocus%20can%20improve%20the%20performance%20without%20introducing%0Aadditional%20information.%20Further%2C%20we%20collect%20a%2014k%20training%20set%20using%20ReFocus%2C%0Aand%20prove%20that%20such%20visual%20chain-of-thought%20with%20intermediate%20information%0Aoffers%20a%20better%20supervision%20than%20standard%20VQA%20data%2C%20reaching%20a%208.0%25%20average%0Again%20over%20the%20same%20model%20trained%20with%20QA%20pairs%20and%202.6%25%20over%20CoT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05452v1&entry.124074799=Read"},
{"title": "Centurio: On Drivers of Multilingual Ability of Large Vision-Language\n  Model", "author": "Gregor Geigle and Florian Schneider and Carolin Holtermann and Chris Biemann and Radu Timofte and Anne Lauscher and Goran Glava\u0161", "abstract": "  Most Large Vision-Language Models (LVLMs) to date are trained predominantly\non English data, which makes them struggle to understand non-English input and\nfail to generate output in the desired target language. Existing efforts\nmitigate these issues by adding multilingual training data, but do so in a\nlargely ad-hoc manner, lacking insight into how different training mixes tip\nthe scale for different groups of languages. In this work, we present a\ncomprehensive investigation into the training strategies for massively\nmultilingual LVLMs. First, we conduct a series of multi-stage experiments\nspanning 13 downstream vision-language tasks and 43 languages, systematically\nexamining: (1) the number of training languages that can be included without\ndegrading English performance and (2) optimal language distributions of\npre-training as well as (3) instruction-tuning data. Further, we (4)\ninvestigate how to improve multilingual text-in-image understanding, and\nintroduce a new benchmark for the task. Surprisingly, our analysis reveals that\none can (i) include as many as 100 training languages simultaneously (ii) with\nas little as 25-50\\% of non-English data, to greatly improve multilingual\nperformance while retaining strong English performance. We further find that\n(iii) including non-English OCR data in pre-training and instruction-tuning is\nparamount for improving multilingual text-in-image understanding. Finally, we\nput all our findings together and train Centurio, a 100-language LVLM, offering\nstate-of-the-art performance in an evaluation covering 14 tasks and 56\nlanguages.\n", "link": "http://arxiv.org/abs/2501.05122v1", "date": "2025-01-09", "relevancy": 2.8921, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6037}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6037}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Centurio%3A%20On%20Drivers%20of%20Multilingual%20Ability%20of%20Large%20Vision-Language%0A%20%20Model&body=Title%3A%20Centurio%3A%20On%20Drivers%20of%20Multilingual%20Ability%20of%20Large%20Vision-Language%0A%20%20Model%0AAuthor%3A%20Gregor%20Geigle%20and%20Florian%20Schneider%20and%20Carolin%20Holtermann%20and%20Chris%20Biemann%20and%20Radu%20Timofte%20and%20Anne%20Lauscher%20and%20Goran%20Glava%C5%A1%0AAbstract%3A%20%20%20Most%20Large%20Vision-Language%20Models%20%28LVLMs%29%20to%20date%20are%20trained%20predominantly%0Aon%20English%20data%2C%20which%20makes%20them%20struggle%20to%20understand%20non-English%20input%20and%0Afail%20to%20generate%20output%20in%20the%20desired%20target%20language.%20Existing%20efforts%0Amitigate%20these%20issues%20by%20adding%20multilingual%20training%20data%2C%20but%20do%20so%20in%20a%0Alargely%20ad-hoc%20manner%2C%20lacking%20insight%20into%20how%20different%20training%20mixes%20tip%0Athe%20scale%20for%20different%20groups%20of%20languages.%20In%20this%20work%2C%20we%20present%20a%0Acomprehensive%20investigation%20into%20the%20training%20strategies%20for%20massively%0Amultilingual%20LVLMs.%20First%2C%20we%20conduct%20a%20series%20of%20multi-stage%20experiments%0Aspanning%2013%20downstream%20vision-language%20tasks%20and%2043%20languages%2C%20systematically%0Aexamining%3A%20%281%29%20the%20number%20of%20training%20languages%20that%20can%20be%20included%20without%0Adegrading%20English%20performance%20and%20%282%29%20optimal%20language%20distributions%20of%0Apre-training%20as%20well%20as%20%283%29%20instruction-tuning%20data.%20Further%2C%20we%20%284%29%0Ainvestigate%20how%20to%20improve%20multilingual%20text-in-image%20understanding%2C%20and%0Aintroduce%20a%20new%20benchmark%20for%20the%20task.%20Surprisingly%2C%20our%20analysis%20reveals%20that%0Aone%20can%20%28i%29%20include%20as%20many%20as%20100%20training%20languages%20simultaneously%20%28ii%29%20with%0Aas%20little%20as%2025-50%5C%25%20of%20non-English%20data%2C%20to%20greatly%20improve%20multilingual%0Aperformance%20while%20retaining%20strong%20English%20performance.%20We%20further%20find%20that%0A%28iii%29%20including%20non-English%20OCR%20data%20in%20pre-training%20and%20instruction-tuning%20is%0Aparamount%20for%20improving%20multilingual%20text-in-image%20understanding.%20Finally%2C%20we%0Aput%20all%20our%20findings%20together%20and%20train%20Centurio%2C%20a%20100-language%20LVLM%2C%20offering%0Astate-of-the-art%20performance%20in%20an%20evaluation%20covering%2014%20tasks%20and%2056%0Alanguages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05122v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCenturio%253A%2520On%2520Drivers%2520of%2520Multilingual%2520Ability%2520of%2520Large%2520Vision-Language%250A%2520%2520Model%26entry.906535625%3DGregor%2520Geigle%2520and%2520Florian%2520Schneider%2520and%2520Carolin%2520Holtermann%2520and%2520Chris%2520Biemann%2520and%2520Radu%2520Timofte%2520and%2520Anne%2520Lauscher%2520and%2520Goran%2520Glava%25C5%25A1%26entry.1292438233%3D%2520%2520Most%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520to%2520date%2520are%2520trained%2520predominantly%250Aon%2520English%2520data%252C%2520which%2520makes%2520them%2520struggle%2520to%2520understand%2520non-English%2520input%2520and%250Afail%2520to%2520generate%2520output%2520in%2520the%2520desired%2520target%2520language.%2520Existing%2520efforts%250Amitigate%2520these%2520issues%2520by%2520adding%2520multilingual%2520training%2520data%252C%2520but%2520do%2520so%2520in%2520a%250Alargely%2520ad-hoc%2520manner%252C%2520lacking%2520insight%2520into%2520how%2520different%2520training%2520mixes%2520tip%250Athe%2520scale%2520for%2520different%2520groups%2520of%2520languages.%2520In%2520this%2520work%252C%2520we%2520present%2520a%250Acomprehensive%2520investigation%2520into%2520the%2520training%2520strategies%2520for%2520massively%250Amultilingual%2520LVLMs.%2520First%252C%2520we%2520conduct%2520a%2520series%2520of%2520multi-stage%2520experiments%250Aspanning%252013%2520downstream%2520vision-language%2520tasks%2520and%252043%2520languages%252C%2520systematically%250Aexamining%253A%2520%25281%2529%2520the%2520number%2520of%2520training%2520languages%2520that%2520can%2520be%2520included%2520without%250Adegrading%2520English%2520performance%2520and%2520%25282%2529%2520optimal%2520language%2520distributions%2520of%250Apre-training%2520as%2520well%2520as%2520%25283%2529%2520instruction-tuning%2520data.%2520Further%252C%2520we%2520%25284%2529%250Ainvestigate%2520how%2520to%2520improve%2520multilingual%2520text-in-image%2520understanding%252C%2520and%250Aintroduce%2520a%2520new%2520benchmark%2520for%2520the%2520task.%2520Surprisingly%252C%2520our%2520analysis%2520reveals%2520that%250Aone%2520can%2520%2528i%2529%2520include%2520as%2520many%2520as%2520100%2520training%2520languages%2520simultaneously%2520%2528ii%2529%2520with%250Aas%2520little%2520as%252025-50%255C%2525%2520of%2520non-English%2520data%252C%2520to%2520greatly%2520improve%2520multilingual%250Aperformance%2520while%2520retaining%2520strong%2520English%2520performance.%2520We%2520further%2520find%2520that%250A%2528iii%2529%2520including%2520non-English%2520OCR%2520data%2520in%2520pre-training%2520and%2520instruction-tuning%2520is%250Aparamount%2520for%2520improving%2520multilingual%2520text-in-image%2520understanding.%2520Finally%252C%2520we%250Aput%2520all%2520our%2520findings%2520together%2520and%2520train%2520Centurio%252C%2520a%2520100-language%2520LVLM%252C%2520offering%250Astate-of-the-art%2520performance%2520in%2520an%2520evaluation%2520covering%252014%2520tasks%2520and%252056%250Alanguages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05122v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Centurio%3A%20On%20Drivers%20of%20Multilingual%20Ability%20of%20Large%20Vision-Language%0A%20%20Model&entry.906535625=Gregor%20Geigle%20and%20Florian%20Schneider%20and%20Carolin%20Holtermann%20and%20Chris%20Biemann%20and%20Radu%20Timofte%20and%20Anne%20Lauscher%20and%20Goran%20Glava%C5%A1&entry.1292438233=%20%20Most%20Large%20Vision-Language%20Models%20%28LVLMs%29%20to%20date%20are%20trained%20predominantly%0Aon%20English%20data%2C%20which%20makes%20them%20struggle%20to%20understand%20non-English%20input%20and%0Afail%20to%20generate%20output%20in%20the%20desired%20target%20language.%20Existing%20efforts%0Amitigate%20these%20issues%20by%20adding%20multilingual%20training%20data%2C%20but%20do%20so%20in%20a%0Alargely%20ad-hoc%20manner%2C%20lacking%20insight%20into%20how%20different%20training%20mixes%20tip%0Athe%20scale%20for%20different%20groups%20of%20languages.%20In%20this%20work%2C%20we%20present%20a%0Acomprehensive%20investigation%20into%20the%20training%20strategies%20for%20massively%0Amultilingual%20LVLMs.%20First%2C%20we%20conduct%20a%20series%20of%20multi-stage%20experiments%0Aspanning%2013%20downstream%20vision-language%20tasks%20and%2043%20languages%2C%20systematically%0Aexamining%3A%20%281%29%20the%20number%20of%20training%20languages%20that%20can%20be%20included%20without%0Adegrading%20English%20performance%20and%20%282%29%20optimal%20language%20distributions%20of%0Apre-training%20as%20well%20as%20%283%29%20instruction-tuning%20data.%20Further%2C%20we%20%284%29%0Ainvestigate%20how%20to%20improve%20multilingual%20text-in-image%20understanding%2C%20and%0Aintroduce%20a%20new%20benchmark%20for%20the%20task.%20Surprisingly%2C%20our%20analysis%20reveals%20that%0Aone%20can%20%28i%29%20include%20as%20many%20as%20100%20training%20languages%20simultaneously%20%28ii%29%20with%0Aas%20little%20as%2025-50%5C%25%20of%20non-English%20data%2C%20to%20greatly%20improve%20multilingual%0Aperformance%20while%20retaining%20strong%20English%20performance.%20We%20further%20find%20that%0A%28iii%29%20including%20non-English%20OCR%20data%20in%20pre-training%20and%20instruction-tuning%20is%0Aparamount%20for%20improving%20multilingual%20text-in-image%20understanding.%20Finally%2C%20we%0Aput%20all%20our%20findings%20together%20and%20train%20Centurio%2C%20a%20100-language%20LVLM%2C%20offering%0Astate-of-the-art%20performance%20in%20an%20evaluation%20covering%2014%20tasks%20and%2056%0Alanguages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05122v1&entry.124074799=Read"},
{"title": "CoE: Deep Coupled Embedding for Non-Rigid Point Cloud Correspondences", "author": "Huajian Zeng and Maolin Gao and Daniel Cremers", "abstract": "  The interest in matching non-rigidly deformed shapes represented as raw point\nclouds is rising due to the proliferation of low-cost 3D sensors. Yet, the task\nis challenging since point clouds are irregular and there is a lack of\nintrinsic shape information. We propose to tackle these challenges by learning\na new shape representation -- a per-point high dimensional embedding, in an\nembedding space where semantically similar points share similar embeddings. The\nlearned embedding has multiple beneficial properties: it is aware of the\nunderlying shape geometry and is robust to shape deformations and various shape\nartefacts, such as noise and partiality. Consequently, this embedding can be\ndirectly employed to retrieve high-quality dense correspondences through a\nsimple nearest neighbor search in the embedding space. Extensive experiments\ndemonstrate new state-of-the-art results and robustness in numerous challenging\nnon-rigid shape matching benchmarks and show its great potential in other shape\nanalysis tasks, such as segmentation.\n", "link": "http://arxiv.org/abs/2412.05557v2", "date": "2025-01-09", "relevancy": 2.8577, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5957}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5852}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoE%3A%20Deep%20Coupled%20Embedding%20for%20Non-Rigid%20Point%20Cloud%20Correspondences&body=Title%3A%20CoE%3A%20Deep%20Coupled%20Embedding%20for%20Non-Rigid%20Point%20Cloud%20Correspondences%0AAuthor%3A%20Huajian%20Zeng%20and%20Maolin%20Gao%20and%20Daniel%20Cremers%0AAbstract%3A%20%20%20The%20interest%20in%20matching%20non-rigidly%20deformed%20shapes%20represented%20as%20raw%20point%0Aclouds%20is%20rising%20due%20to%20the%20proliferation%20of%20low-cost%203D%20sensors.%20Yet%2C%20the%20task%0Ais%20challenging%20since%20point%20clouds%20are%20irregular%20and%20there%20is%20a%20lack%20of%0Aintrinsic%20shape%20information.%20We%20propose%20to%20tackle%20these%20challenges%20by%20learning%0Aa%20new%20shape%20representation%20--%20a%20per-point%20high%20dimensional%20embedding%2C%20in%20an%0Aembedding%20space%20where%20semantically%20similar%20points%20share%20similar%20embeddings.%20The%0Alearned%20embedding%20has%20multiple%20beneficial%20properties%3A%20it%20is%20aware%20of%20the%0Aunderlying%20shape%20geometry%20and%20is%20robust%20to%20shape%20deformations%20and%20various%20shape%0Aartefacts%2C%20such%20as%20noise%20and%20partiality.%20Consequently%2C%20this%20embedding%20can%20be%0Adirectly%20employed%20to%20retrieve%20high-quality%20dense%20correspondences%20through%20a%0Asimple%20nearest%20neighbor%20search%20in%20the%20embedding%20space.%20Extensive%20experiments%0Ademonstrate%20new%20state-of-the-art%20results%20and%20robustness%20in%20numerous%20challenging%0Anon-rigid%20shape%20matching%20benchmarks%20and%20show%20its%20great%20potential%20in%20other%20shape%0Aanalysis%20tasks%2C%20such%20as%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.05557v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoE%253A%2520Deep%2520Coupled%2520Embedding%2520for%2520Non-Rigid%2520Point%2520Cloud%2520Correspondences%26entry.906535625%3DHuajian%2520Zeng%2520and%2520Maolin%2520Gao%2520and%2520Daniel%2520Cremers%26entry.1292438233%3D%2520%2520The%2520interest%2520in%2520matching%2520non-rigidly%2520deformed%2520shapes%2520represented%2520as%2520raw%2520point%250Aclouds%2520is%2520rising%2520due%2520to%2520the%2520proliferation%2520of%2520low-cost%25203D%2520sensors.%2520Yet%252C%2520the%2520task%250Ais%2520challenging%2520since%2520point%2520clouds%2520are%2520irregular%2520and%2520there%2520is%2520a%2520lack%2520of%250Aintrinsic%2520shape%2520information.%2520We%2520propose%2520to%2520tackle%2520these%2520challenges%2520by%2520learning%250Aa%2520new%2520shape%2520representation%2520--%2520a%2520per-point%2520high%2520dimensional%2520embedding%252C%2520in%2520an%250Aembedding%2520space%2520where%2520semantically%2520similar%2520points%2520share%2520similar%2520embeddings.%2520The%250Alearned%2520embedding%2520has%2520multiple%2520beneficial%2520properties%253A%2520it%2520is%2520aware%2520of%2520the%250Aunderlying%2520shape%2520geometry%2520and%2520is%2520robust%2520to%2520shape%2520deformations%2520and%2520various%2520shape%250Aartefacts%252C%2520such%2520as%2520noise%2520and%2520partiality.%2520Consequently%252C%2520this%2520embedding%2520can%2520be%250Adirectly%2520employed%2520to%2520retrieve%2520high-quality%2520dense%2520correspondences%2520through%2520a%250Asimple%2520nearest%2520neighbor%2520search%2520in%2520the%2520embedding%2520space.%2520Extensive%2520experiments%250Ademonstrate%2520new%2520state-of-the-art%2520results%2520and%2520robustness%2520in%2520numerous%2520challenging%250Anon-rigid%2520shape%2520matching%2520benchmarks%2520and%2520show%2520its%2520great%2520potential%2520in%2520other%2520shape%250Aanalysis%2520tasks%252C%2520such%2520as%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.05557v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoE%3A%20Deep%20Coupled%20Embedding%20for%20Non-Rigid%20Point%20Cloud%20Correspondences&entry.906535625=Huajian%20Zeng%20and%20Maolin%20Gao%20and%20Daniel%20Cremers&entry.1292438233=%20%20The%20interest%20in%20matching%20non-rigidly%20deformed%20shapes%20represented%20as%20raw%20point%0Aclouds%20is%20rising%20due%20to%20the%20proliferation%20of%20low-cost%203D%20sensors.%20Yet%2C%20the%20task%0Ais%20challenging%20since%20point%20clouds%20are%20irregular%20and%20there%20is%20a%20lack%20of%0Aintrinsic%20shape%20information.%20We%20propose%20to%20tackle%20these%20challenges%20by%20learning%0Aa%20new%20shape%20representation%20--%20a%20per-point%20high%20dimensional%20embedding%2C%20in%20an%0Aembedding%20space%20where%20semantically%20similar%20points%20share%20similar%20embeddings.%20The%0Alearned%20embedding%20has%20multiple%20beneficial%20properties%3A%20it%20is%20aware%20of%20the%0Aunderlying%20shape%20geometry%20and%20is%20robust%20to%20shape%20deformations%20and%20various%20shape%0Aartefacts%2C%20such%20as%20noise%20and%20partiality.%20Consequently%2C%20this%20embedding%20can%20be%0Adirectly%20employed%20to%20retrieve%20high-quality%20dense%20correspondences%20through%20a%0Asimple%20nearest%20neighbor%20search%20in%20the%20embedding%20space.%20Extensive%20experiments%0Ademonstrate%20new%20state-of-the-art%20results%20and%20robustness%20in%20numerous%20challenging%0Anon-rigid%20shape%20matching%20benchmarks%20and%20show%20its%20great%20potential%20in%20other%20shape%0Aanalysis%20tasks%2C%20such%20as%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.05557v2&entry.124074799=Read"},
{"title": "BTMTrack: Robust RGB-T Tracking via Dual-template Bridging and\n  Temporal-Modal Candidate Elimination", "author": "Zhongxuan Zhang and Bi Zeng and Xinyu Ni and Yimin Du", "abstract": "  RGB-T tracking leverages the complementary strengths of RGB and thermal\ninfrared (TIR) modalities to address challenging scenarios such as low\nillumination and adverse weather. However, existing methods often fail to\neffectively integrate temporal information and perform efficient cross-modal\ninteractions, which constrain their adaptability to dynamic targets. In this\npaper, we propose BTMTrack, a novel framework for RGB-T tracking. The core of\nour approach lies in the dual-template backbone network and the Temporal-Modal\nCandidate Elimination (TMCE) strategy. The dual-template backbone effectively\nintegrates temporal information, while the TMCE strategy focuses the model on\ntarget-relevant tokens by evaluating temporal and modal correlations, reducing\ncomputational overhead and avoiding irrelevant background noise. Building upon\nthis foundation, we propose the Temporal Dual Template Bridging (TDTB) module,\nwhich facilitates precise cross-modal fusion through dynamically filtered\ntokens. This approach further strengthens the interaction between templates and\nthe search region. Extensive experiments conducted on three benchmark datasets\ndemonstrate the effectiveness of BTMTrack. Our method achieves state-of-the-art\nperformance, with a 72.3% precision rate on the LasHeR test set and competitive\nresults on RGBT210 and RGBT234 datasets.\n", "link": "http://arxiv.org/abs/2501.03616v2", "date": "2025-01-09", "relevancy": 2.8001, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5802}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5657}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5341}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BTMTrack%3A%20Robust%20RGB-T%20Tracking%20via%20Dual-template%20Bridging%20and%0A%20%20Temporal-Modal%20Candidate%20Elimination&body=Title%3A%20BTMTrack%3A%20Robust%20RGB-T%20Tracking%20via%20Dual-template%20Bridging%20and%0A%20%20Temporal-Modal%20Candidate%20Elimination%0AAuthor%3A%20Zhongxuan%20Zhang%20and%20Bi%20Zeng%20and%20Xinyu%20Ni%20and%20Yimin%20Du%0AAbstract%3A%20%20%20RGB-T%20tracking%20leverages%20the%20complementary%20strengths%20of%20RGB%20and%20thermal%0Ainfrared%20%28TIR%29%20modalities%20to%20address%20challenging%20scenarios%20such%20as%20low%0Aillumination%20and%20adverse%20weather.%20However%2C%20existing%20methods%20often%20fail%20to%0Aeffectively%20integrate%20temporal%20information%20and%20perform%20efficient%20cross-modal%0Ainteractions%2C%20which%20constrain%20their%20adaptability%20to%20dynamic%20targets.%20In%20this%0Apaper%2C%20we%20propose%20BTMTrack%2C%20a%20novel%20framework%20for%20RGB-T%20tracking.%20The%20core%20of%0Aour%20approach%20lies%20in%20the%20dual-template%20backbone%20network%20and%20the%20Temporal-Modal%0ACandidate%20Elimination%20%28TMCE%29%20strategy.%20The%20dual-template%20backbone%20effectively%0Aintegrates%20temporal%20information%2C%20while%20the%20TMCE%20strategy%20focuses%20the%20model%20on%0Atarget-relevant%20tokens%20by%20evaluating%20temporal%20and%20modal%20correlations%2C%20reducing%0Acomputational%20overhead%20and%20avoiding%20irrelevant%20background%20noise.%20Building%20upon%0Athis%20foundation%2C%20we%20propose%20the%20Temporal%20Dual%20Template%20Bridging%20%28TDTB%29%20module%2C%0Awhich%20facilitates%20precise%20cross-modal%20fusion%20through%20dynamically%20filtered%0Atokens.%20This%20approach%20further%20strengthens%20the%20interaction%20between%20templates%20and%0Athe%20search%20region.%20Extensive%20experiments%20conducted%20on%20three%20benchmark%20datasets%0Ademonstrate%20the%20effectiveness%20of%20BTMTrack.%20Our%20method%20achieves%20state-of-the-art%0Aperformance%2C%20with%20a%2072.3%25%20precision%20rate%20on%20the%20LasHeR%20test%20set%20and%20competitive%0Aresults%20on%20RGBT210%20and%20RGBT234%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03616v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBTMTrack%253A%2520Robust%2520RGB-T%2520Tracking%2520via%2520Dual-template%2520Bridging%2520and%250A%2520%2520Temporal-Modal%2520Candidate%2520Elimination%26entry.906535625%3DZhongxuan%2520Zhang%2520and%2520Bi%2520Zeng%2520and%2520Xinyu%2520Ni%2520and%2520Yimin%2520Du%26entry.1292438233%3D%2520%2520RGB-T%2520tracking%2520leverages%2520the%2520complementary%2520strengths%2520of%2520RGB%2520and%2520thermal%250Ainfrared%2520%2528TIR%2529%2520modalities%2520to%2520address%2520challenging%2520scenarios%2520such%2520as%2520low%250Aillumination%2520and%2520adverse%2520weather.%2520However%252C%2520existing%2520methods%2520often%2520fail%2520to%250Aeffectively%2520integrate%2520temporal%2520information%2520and%2520perform%2520efficient%2520cross-modal%250Ainteractions%252C%2520which%2520constrain%2520their%2520adaptability%2520to%2520dynamic%2520targets.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520BTMTrack%252C%2520a%2520novel%2520framework%2520for%2520RGB-T%2520tracking.%2520The%2520core%2520of%250Aour%2520approach%2520lies%2520in%2520the%2520dual-template%2520backbone%2520network%2520and%2520the%2520Temporal-Modal%250ACandidate%2520Elimination%2520%2528TMCE%2529%2520strategy.%2520The%2520dual-template%2520backbone%2520effectively%250Aintegrates%2520temporal%2520information%252C%2520while%2520the%2520TMCE%2520strategy%2520focuses%2520the%2520model%2520on%250Atarget-relevant%2520tokens%2520by%2520evaluating%2520temporal%2520and%2520modal%2520correlations%252C%2520reducing%250Acomputational%2520overhead%2520and%2520avoiding%2520irrelevant%2520background%2520noise.%2520Building%2520upon%250Athis%2520foundation%252C%2520we%2520propose%2520the%2520Temporal%2520Dual%2520Template%2520Bridging%2520%2528TDTB%2529%2520module%252C%250Awhich%2520facilitates%2520precise%2520cross-modal%2520fusion%2520through%2520dynamically%2520filtered%250Atokens.%2520This%2520approach%2520further%2520strengthens%2520the%2520interaction%2520between%2520templates%2520and%250Athe%2520search%2520region.%2520Extensive%2520experiments%2520conducted%2520on%2520three%2520benchmark%2520datasets%250Ademonstrate%2520the%2520effectiveness%2520of%2520BTMTrack.%2520Our%2520method%2520achieves%2520state-of-the-art%250Aperformance%252C%2520with%2520a%252072.3%2525%2520precision%2520rate%2520on%2520the%2520LasHeR%2520test%2520set%2520and%2520competitive%250Aresults%2520on%2520RGBT210%2520and%2520RGBT234%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03616v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BTMTrack%3A%20Robust%20RGB-T%20Tracking%20via%20Dual-template%20Bridging%20and%0A%20%20Temporal-Modal%20Candidate%20Elimination&entry.906535625=Zhongxuan%20Zhang%20and%20Bi%20Zeng%20and%20Xinyu%20Ni%20and%20Yimin%20Du&entry.1292438233=%20%20RGB-T%20tracking%20leverages%20the%20complementary%20strengths%20of%20RGB%20and%20thermal%0Ainfrared%20%28TIR%29%20modalities%20to%20address%20challenging%20scenarios%20such%20as%20low%0Aillumination%20and%20adverse%20weather.%20However%2C%20existing%20methods%20often%20fail%20to%0Aeffectively%20integrate%20temporal%20information%20and%20perform%20efficient%20cross-modal%0Ainteractions%2C%20which%20constrain%20their%20adaptability%20to%20dynamic%20targets.%20In%20this%0Apaper%2C%20we%20propose%20BTMTrack%2C%20a%20novel%20framework%20for%20RGB-T%20tracking.%20The%20core%20of%0Aour%20approach%20lies%20in%20the%20dual-template%20backbone%20network%20and%20the%20Temporal-Modal%0ACandidate%20Elimination%20%28TMCE%29%20strategy.%20The%20dual-template%20backbone%20effectively%0Aintegrates%20temporal%20information%2C%20while%20the%20TMCE%20strategy%20focuses%20the%20model%20on%0Atarget-relevant%20tokens%20by%20evaluating%20temporal%20and%20modal%20correlations%2C%20reducing%0Acomputational%20overhead%20and%20avoiding%20irrelevant%20background%20noise.%20Building%20upon%0Athis%20foundation%2C%20we%20propose%20the%20Temporal%20Dual%20Template%20Bridging%20%28TDTB%29%20module%2C%0Awhich%20facilitates%20precise%20cross-modal%20fusion%20through%20dynamically%20filtered%0Atokens.%20This%20approach%20further%20strengthens%20the%20interaction%20between%20templates%20and%0Athe%20search%20region.%20Extensive%20experiments%20conducted%20on%20three%20benchmark%20datasets%0Ademonstrate%20the%20effectiveness%20of%20BTMTrack.%20Our%20method%20achieves%20state-of-the-art%0Aperformance%2C%20with%20a%2072.3%25%20precision%20rate%20on%20the%20LasHeR%20test%20set%20and%20competitive%0Aresults%20on%20RGBT210%20and%20RGBT234%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03616v2&entry.124074799=Read"},
{"title": "DGNN-YOLO: Interpretable Dynamic Graph Neural Networks with YOLO11 for\n  Detecting and Tracking Small Occluded Objects in Urban Traffic", "author": "Shahriar Soudeep and M. F. Mridha and Md Abrar Jahin and Nilanjan Dey", "abstract": "  The detection and tracking of small, occluded objects such as pedestrians,\ncyclists, and motorbikes pose significant challenges for traffic surveillance\nsystems because of their erratic movement, frequent occlusion, and poor\nvisibility in dynamic urban environments. Traditional methods like YOLO11,\nwhile proficient in spatial feature extraction for precise detection, often\nstruggle with these small and dynamically moving objects, particularly in\nhandling real-time data updates and resource efficiency. This paper introduces\nDGNN-YOLO, a novel framework that integrates dynamic graph neural networks\n(DGNNs) with YOLO11 to address these limitations. Unlike standard GNNs, DGNNs\nare chosen for their superior ability to dynamically update graph structures in\nreal-time, which enables adaptive and robust tracking of objects in highly\nvariable urban traffic scenarios. This framework constructs and regularly\nupdates its graph representations, capturing objects as nodes and their\ninteractions as edges, thus effectively responding to rapidly changing\nconditions. Additionally, DGNN-YOLO incorporates Grad-CAM, Grad-CAM++, and\nEigen-CAM visualization techniques to enhance interpretability and foster\ntrust, offering insights into the model's decision-making process. Extensive\nexperiments validate the framework's performance, achieving a precision of\n0.8382, recall of 0.6875, and mAP@0.5:0.95 of 0.6476, significantly\noutperforming existing methods. This study offers a scalable and interpretable\nsolution for real-time traffic surveillance and significantly advances\nintelligent transportation systems' capabilities by addressing the critical\nchallenge of detecting and tracking small, occluded objects.\n", "link": "http://arxiv.org/abs/2411.17251v5", "date": "2025-01-09", "relevancy": 2.7781, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5711}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5601}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DGNN-YOLO%3A%20Interpretable%20Dynamic%20Graph%20Neural%20Networks%20with%20YOLO11%20for%0A%20%20Detecting%20and%20Tracking%20Small%20Occluded%20Objects%20in%20Urban%20Traffic&body=Title%3A%20DGNN-YOLO%3A%20Interpretable%20Dynamic%20Graph%20Neural%20Networks%20with%20YOLO11%20for%0A%20%20Detecting%20and%20Tracking%20Small%20Occluded%20Objects%20in%20Urban%20Traffic%0AAuthor%3A%20Shahriar%20Soudeep%20and%20M.%20F.%20Mridha%20and%20Md%20Abrar%20Jahin%20and%20Nilanjan%20Dey%0AAbstract%3A%20%20%20The%20detection%20and%20tracking%20of%20small%2C%20occluded%20objects%20such%20as%20pedestrians%2C%0Acyclists%2C%20and%20motorbikes%20pose%20significant%20challenges%20for%20traffic%20surveillance%0Asystems%20because%20of%20their%20erratic%20movement%2C%20frequent%20occlusion%2C%20and%20poor%0Avisibility%20in%20dynamic%20urban%20environments.%20Traditional%20methods%20like%20YOLO11%2C%0Awhile%20proficient%20in%20spatial%20feature%20extraction%20for%20precise%20detection%2C%20often%0Astruggle%20with%20these%20small%20and%20dynamically%20moving%20objects%2C%20particularly%20in%0Ahandling%20real-time%20data%20updates%20and%20resource%20efficiency.%20This%20paper%20introduces%0ADGNN-YOLO%2C%20a%20novel%20framework%20that%20integrates%20dynamic%20graph%20neural%20networks%0A%28DGNNs%29%20with%20YOLO11%20to%20address%20these%20limitations.%20Unlike%20standard%20GNNs%2C%20DGNNs%0Aare%20chosen%20for%20their%20superior%20ability%20to%20dynamically%20update%20graph%20structures%20in%0Areal-time%2C%20which%20enables%20adaptive%20and%20robust%20tracking%20of%20objects%20in%20highly%0Avariable%20urban%20traffic%20scenarios.%20This%20framework%20constructs%20and%20regularly%0Aupdates%20its%20graph%20representations%2C%20capturing%20objects%20as%20nodes%20and%20their%0Ainteractions%20as%20edges%2C%20thus%20effectively%20responding%20to%20rapidly%20changing%0Aconditions.%20Additionally%2C%20DGNN-YOLO%20incorporates%20Grad-CAM%2C%20Grad-CAM%2B%2B%2C%20and%0AEigen-CAM%20visualization%20techniques%20to%20enhance%20interpretability%20and%20foster%0Atrust%2C%20offering%20insights%20into%20the%20model%27s%20decision-making%20process.%20Extensive%0Aexperiments%20validate%20the%20framework%27s%20performance%2C%20achieving%20a%20precision%20of%0A0.8382%2C%20recall%20of%200.6875%2C%20and%20mAP%400.5%3A0.95%20of%200.6476%2C%20significantly%0Aoutperforming%20existing%20methods.%20This%20study%20offers%20a%20scalable%20and%20interpretable%0Asolution%20for%20real-time%20traffic%20surveillance%20and%20significantly%20advances%0Aintelligent%20transportation%20systems%27%20capabilities%20by%20addressing%20the%20critical%0Achallenge%20of%20detecting%20and%20tracking%20small%2C%20occluded%20objects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17251v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDGNN-YOLO%253A%2520Interpretable%2520Dynamic%2520Graph%2520Neural%2520Networks%2520with%2520YOLO11%2520for%250A%2520%2520Detecting%2520and%2520Tracking%2520Small%2520Occluded%2520Objects%2520in%2520Urban%2520Traffic%26entry.906535625%3DShahriar%2520Soudeep%2520and%2520M.%2520F.%2520Mridha%2520and%2520Md%2520Abrar%2520Jahin%2520and%2520Nilanjan%2520Dey%26entry.1292438233%3D%2520%2520The%2520detection%2520and%2520tracking%2520of%2520small%252C%2520occluded%2520objects%2520such%2520as%2520pedestrians%252C%250Acyclists%252C%2520and%2520motorbikes%2520pose%2520significant%2520challenges%2520for%2520traffic%2520surveillance%250Asystems%2520because%2520of%2520their%2520erratic%2520movement%252C%2520frequent%2520occlusion%252C%2520and%2520poor%250Avisibility%2520in%2520dynamic%2520urban%2520environments.%2520Traditional%2520methods%2520like%2520YOLO11%252C%250Awhile%2520proficient%2520in%2520spatial%2520feature%2520extraction%2520for%2520precise%2520detection%252C%2520often%250Astruggle%2520with%2520these%2520small%2520and%2520dynamically%2520moving%2520objects%252C%2520particularly%2520in%250Ahandling%2520real-time%2520data%2520updates%2520and%2520resource%2520efficiency.%2520This%2520paper%2520introduces%250ADGNN-YOLO%252C%2520a%2520novel%2520framework%2520that%2520integrates%2520dynamic%2520graph%2520neural%2520networks%250A%2528DGNNs%2529%2520with%2520YOLO11%2520to%2520address%2520these%2520limitations.%2520Unlike%2520standard%2520GNNs%252C%2520DGNNs%250Aare%2520chosen%2520for%2520their%2520superior%2520ability%2520to%2520dynamically%2520update%2520graph%2520structures%2520in%250Areal-time%252C%2520which%2520enables%2520adaptive%2520and%2520robust%2520tracking%2520of%2520objects%2520in%2520highly%250Avariable%2520urban%2520traffic%2520scenarios.%2520This%2520framework%2520constructs%2520and%2520regularly%250Aupdates%2520its%2520graph%2520representations%252C%2520capturing%2520objects%2520as%2520nodes%2520and%2520their%250Ainteractions%2520as%2520edges%252C%2520thus%2520effectively%2520responding%2520to%2520rapidly%2520changing%250Aconditions.%2520Additionally%252C%2520DGNN-YOLO%2520incorporates%2520Grad-CAM%252C%2520Grad-CAM%252B%252B%252C%2520and%250AEigen-CAM%2520visualization%2520techniques%2520to%2520enhance%2520interpretability%2520and%2520foster%250Atrust%252C%2520offering%2520insights%2520into%2520the%2520model%2527s%2520decision-making%2520process.%2520Extensive%250Aexperiments%2520validate%2520the%2520framework%2527s%2520performance%252C%2520achieving%2520a%2520precision%2520of%250A0.8382%252C%2520recall%2520of%25200.6875%252C%2520and%2520mAP%25400.5%253A0.95%2520of%25200.6476%252C%2520significantly%250Aoutperforming%2520existing%2520methods.%2520This%2520study%2520offers%2520a%2520scalable%2520and%2520interpretable%250Asolution%2520for%2520real-time%2520traffic%2520surveillance%2520and%2520significantly%2520advances%250Aintelligent%2520transportation%2520systems%2527%2520capabilities%2520by%2520addressing%2520the%2520critical%250Achallenge%2520of%2520detecting%2520and%2520tracking%2520small%252C%2520occluded%2520objects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17251v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DGNN-YOLO%3A%20Interpretable%20Dynamic%20Graph%20Neural%20Networks%20with%20YOLO11%20for%0A%20%20Detecting%20and%20Tracking%20Small%20Occluded%20Objects%20in%20Urban%20Traffic&entry.906535625=Shahriar%20Soudeep%20and%20M.%20F.%20Mridha%20and%20Md%20Abrar%20Jahin%20and%20Nilanjan%20Dey&entry.1292438233=%20%20The%20detection%20and%20tracking%20of%20small%2C%20occluded%20objects%20such%20as%20pedestrians%2C%0Acyclists%2C%20and%20motorbikes%20pose%20significant%20challenges%20for%20traffic%20surveillance%0Asystems%20because%20of%20their%20erratic%20movement%2C%20frequent%20occlusion%2C%20and%20poor%0Avisibility%20in%20dynamic%20urban%20environments.%20Traditional%20methods%20like%20YOLO11%2C%0Awhile%20proficient%20in%20spatial%20feature%20extraction%20for%20precise%20detection%2C%20often%0Astruggle%20with%20these%20small%20and%20dynamically%20moving%20objects%2C%20particularly%20in%0Ahandling%20real-time%20data%20updates%20and%20resource%20efficiency.%20This%20paper%20introduces%0ADGNN-YOLO%2C%20a%20novel%20framework%20that%20integrates%20dynamic%20graph%20neural%20networks%0A%28DGNNs%29%20with%20YOLO11%20to%20address%20these%20limitations.%20Unlike%20standard%20GNNs%2C%20DGNNs%0Aare%20chosen%20for%20their%20superior%20ability%20to%20dynamically%20update%20graph%20structures%20in%0Areal-time%2C%20which%20enables%20adaptive%20and%20robust%20tracking%20of%20objects%20in%20highly%0Avariable%20urban%20traffic%20scenarios.%20This%20framework%20constructs%20and%20regularly%0Aupdates%20its%20graph%20representations%2C%20capturing%20objects%20as%20nodes%20and%20their%0Ainteractions%20as%20edges%2C%20thus%20effectively%20responding%20to%20rapidly%20changing%0Aconditions.%20Additionally%2C%20DGNN-YOLO%20incorporates%20Grad-CAM%2C%20Grad-CAM%2B%2B%2C%20and%0AEigen-CAM%20visualization%20techniques%20to%20enhance%20interpretability%20and%20foster%0Atrust%2C%20offering%20insights%20into%20the%20model%27s%20decision-making%20process.%20Extensive%0Aexperiments%20validate%20the%20framework%27s%20performance%2C%20achieving%20a%20precision%20of%0A0.8382%2C%20recall%20of%200.6875%2C%20and%20mAP%400.5%3A0.95%20of%200.6476%2C%20significantly%0Aoutperforming%20existing%20methods.%20This%20study%20offers%20a%20scalable%20and%20interpretable%0Asolution%20for%20real-time%20traffic%20surveillance%20and%20significantly%20advances%0Aintelligent%20transportation%20systems%27%20capabilities%20by%20addressing%20the%20critical%0Achallenge%20of%20detecting%20and%20tracking%20small%2C%20occluded%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17251v5&entry.124074799=Read"},
{"title": "Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal\n  ReAsoning Benchmark", "author": "Yunzhuo Hao and Jiawei Gu and Huichen Will Wang and Linjie Li and Zhengyuan Yang and Lijuan Wang and Yu Cheng", "abstract": "  The ability to organically reason over and with both text and images is a\npillar of human intelligence, yet the ability of Multimodal Large Language\nModels (MLLMs) to perform such multimodal reasoning remains under-explored.\nExisting benchmarks often emphasize text-dominant reasoning or rely on shallow\nvisual cues, failing to adequately assess integrated visual and textual\nreasoning. We introduce EMMA (Enhanced MultiModal reAsoning), a benchmark\ntargeting organic multimodal reasoning across mathematics, physics, chemistry,\nand coding. EMMA tasks demand advanced cross-modal reasoning that cannot be\naddressed by reasoning independently in each modality, offering an enhanced\ntest suite for MLLMs' reasoning capabilities. Our evaluation of\nstate-of-the-art MLLMs on EMMA reveals significant limitations in handling\ncomplex multimodal and multi-step reasoning tasks, even with advanced\ntechniques like Chain-of-Thought prompting and test-time compute scaling\nunderperforming. These findings underscore the need for improved multimodal\narchitectures and training paradigms to close the gap between human and model\nreasoning in multimodality.\n", "link": "http://arxiv.org/abs/2501.05444v1", "date": "2025-01-09", "relevancy": 2.7733, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5607}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5607}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20MLLMs%20Reason%20in%20Multimodality%3F%20EMMA%3A%20An%20Enhanced%20MultiModal%0A%20%20ReAsoning%20Benchmark&body=Title%3A%20Can%20MLLMs%20Reason%20in%20Multimodality%3F%20EMMA%3A%20An%20Enhanced%20MultiModal%0A%20%20ReAsoning%20Benchmark%0AAuthor%3A%20Yunzhuo%20Hao%20and%20Jiawei%20Gu%20and%20Huichen%20Will%20Wang%20and%20Linjie%20Li%20and%20Zhengyuan%20Yang%20and%20Lijuan%20Wang%20and%20Yu%20Cheng%0AAbstract%3A%20%20%20The%20ability%20to%20organically%20reason%20over%20and%20with%20both%20text%20and%20images%20is%20a%0Apillar%20of%20human%20intelligence%2C%20yet%20the%20ability%20of%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29%20to%20perform%20such%20multimodal%20reasoning%20remains%20under-explored.%0AExisting%20benchmarks%20often%20emphasize%20text-dominant%20reasoning%20or%20rely%20on%20shallow%0Avisual%20cues%2C%20failing%20to%20adequately%20assess%20integrated%20visual%20and%20textual%0Areasoning.%20We%20introduce%20EMMA%20%28Enhanced%20MultiModal%20reAsoning%29%2C%20a%20benchmark%0Atargeting%20organic%20multimodal%20reasoning%20across%20mathematics%2C%20physics%2C%20chemistry%2C%0Aand%20coding.%20EMMA%20tasks%20demand%20advanced%20cross-modal%20reasoning%20that%20cannot%20be%0Aaddressed%20by%20reasoning%20independently%20in%20each%20modality%2C%20offering%20an%20enhanced%0Atest%20suite%20for%20MLLMs%27%20reasoning%20capabilities.%20Our%20evaluation%20of%0Astate-of-the-art%20MLLMs%20on%20EMMA%20reveals%20significant%20limitations%20in%20handling%0Acomplex%20multimodal%20and%20multi-step%20reasoning%20tasks%2C%20even%20with%20advanced%0Atechniques%20like%20Chain-of-Thought%20prompting%20and%20test-time%20compute%20scaling%0Aunderperforming.%20These%20findings%20underscore%20the%20need%20for%20improved%20multimodal%0Aarchitectures%20and%20training%20paradigms%20to%20close%20the%20gap%20between%20human%20and%20model%0Areasoning%20in%20multimodality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05444v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520MLLMs%2520Reason%2520in%2520Multimodality%253F%2520EMMA%253A%2520An%2520Enhanced%2520MultiModal%250A%2520%2520ReAsoning%2520Benchmark%26entry.906535625%3DYunzhuo%2520Hao%2520and%2520Jiawei%2520Gu%2520and%2520Huichen%2520Will%2520Wang%2520and%2520Linjie%2520Li%2520and%2520Zhengyuan%2520Yang%2520and%2520Lijuan%2520Wang%2520and%2520Yu%2520Cheng%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520organically%2520reason%2520over%2520and%2520with%2520both%2520text%2520and%2520images%2520is%2520a%250Apillar%2520of%2520human%2520intelligence%252C%2520yet%2520the%2520ability%2520of%2520Multimodal%2520Large%2520Language%250AModels%2520%2528MLLMs%2529%2520to%2520perform%2520such%2520multimodal%2520reasoning%2520remains%2520under-explored.%250AExisting%2520benchmarks%2520often%2520emphasize%2520text-dominant%2520reasoning%2520or%2520rely%2520on%2520shallow%250Avisual%2520cues%252C%2520failing%2520to%2520adequately%2520assess%2520integrated%2520visual%2520and%2520textual%250Areasoning.%2520We%2520introduce%2520EMMA%2520%2528Enhanced%2520MultiModal%2520reAsoning%2529%252C%2520a%2520benchmark%250Atargeting%2520organic%2520multimodal%2520reasoning%2520across%2520mathematics%252C%2520physics%252C%2520chemistry%252C%250Aand%2520coding.%2520EMMA%2520tasks%2520demand%2520advanced%2520cross-modal%2520reasoning%2520that%2520cannot%2520be%250Aaddressed%2520by%2520reasoning%2520independently%2520in%2520each%2520modality%252C%2520offering%2520an%2520enhanced%250Atest%2520suite%2520for%2520MLLMs%2527%2520reasoning%2520capabilities.%2520Our%2520evaluation%2520of%250Astate-of-the-art%2520MLLMs%2520on%2520EMMA%2520reveals%2520significant%2520limitations%2520in%2520handling%250Acomplex%2520multimodal%2520and%2520multi-step%2520reasoning%2520tasks%252C%2520even%2520with%2520advanced%250Atechniques%2520like%2520Chain-of-Thought%2520prompting%2520and%2520test-time%2520compute%2520scaling%250Aunderperforming.%2520These%2520findings%2520underscore%2520the%2520need%2520for%2520improved%2520multimodal%250Aarchitectures%2520and%2520training%2520paradigms%2520to%2520close%2520the%2520gap%2520between%2520human%2520and%2520model%250Areasoning%2520in%2520multimodality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05444v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20MLLMs%20Reason%20in%20Multimodality%3F%20EMMA%3A%20An%20Enhanced%20MultiModal%0A%20%20ReAsoning%20Benchmark&entry.906535625=Yunzhuo%20Hao%20and%20Jiawei%20Gu%20and%20Huichen%20Will%20Wang%20and%20Linjie%20Li%20and%20Zhengyuan%20Yang%20and%20Lijuan%20Wang%20and%20Yu%20Cheng&entry.1292438233=%20%20The%20ability%20to%20organically%20reason%20over%20and%20with%20both%20text%20and%20images%20is%20a%0Apillar%20of%20human%20intelligence%2C%20yet%20the%20ability%20of%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29%20to%20perform%20such%20multimodal%20reasoning%20remains%20under-explored.%0AExisting%20benchmarks%20often%20emphasize%20text-dominant%20reasoning%20or%20rely%20on%20shallow%0Avisual%20cues%2C%20failing%20to%20adequately%20assess%20integrated%20visual%20and%20textual%0Areasoning.%20We%20introduce%20EMMA%20%28Enhanced%20MultiModal%20reAsoning%29%2C%20a%20benchmark%0Atargeting%20organic%20multimodal%20reasoning%20across%20mathematics%2C%20physics%2C%20chemistry%2C%0Aand%20coding.%20EMMA%20tasks%20demand%20advanced%20cross-modal%20reasoning%20that%20cannot%20be%0Aaddressed%20by%20reasoning%20independently%20in%20each%20modality%2C%20offering%20an%20enhanced%0Atest%20suite%20for%20MLLMs%27%20reasoning%20capabilities.%20Our%20evaluation%20of%0Astate-of-the-art%20MLLMs%20on%20EMMA%20reveals%20significant%20limitations%20in%20handling%0Acomplex%20multimodal%20and%20multi-step%20reasoning%20tasks%2C%20even%20with%20advanced%0Atechniques%20like%20Chain-of-Thought%20prompting%20and%20test-time%20compute%20scaling%0Aunderperforming.%20These%20findings%20underscore%20the%20need%20for%20improved%20multimodal%0Aarchitectures%20and%20training%20paradigms%20to%20close%20the%20gap%20between%20human%20and%20model%0Areasoning%20in%20multimodality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05444v1&entry.124074799=Read"},
{"title": "Dynamic Localisation of Spatial-Temporal Graph Neural Network", "author": "Wenying Duan and Shujun Guo and Wei huang and Hong Rao and Xiaoxi He", "abstract": "  Spatial-temporal data, fundamental to many intelligent applications, reveals\ndependencies indicating causal links between present measurements at specific\nlocations and historical data at the same or other locations. Within this\ncontext, adaptive spatial-temporal graph neural networks (ASTGNNs) have emerged\nas valuable tools for modelling these dependencies, especially through a\ndata-driven approach rather than pre-defined spatial graphs. While this\napproach offers higher accuracy, it presents increased computational demands.\nAddressing this challenge, this paper delves into the concept of localisation\nwithin ASTGNNs, introducing an innovative perspective that spatial dependencies\nshould be dynamically evolving over time. We introduce \\textit{DynAGS}, a\nlocalised ASTGNN framework aimed at maximising efficiency and accuracy in\ndistributed deployment. This framework integrates dynamic localisation,\ntime-evolving spatial graphs, and personalised localisation, all orchestrated\naround the Dynamic Graph Generator, a light-weighted central module leveraging\ncross attention. The central module can integrate historical information in a\nnode-independent manner to enhance the feature representation of nodes at the\ncurrent moment. This improved feature representation is then used to generate a\ndynamic sparse graph without the need for costly data exchanges, and it\nsupports personalised localisation. Performance assessments across two core\nASTGNN architectures and nine real-world datasets from various applications\nreveal that \\textit{DynAGS} outshines current benchmarks, underscoring that the\ndynamic modelling of spatial dependencies can drastically improve model\nexpressibility, flexibility, and system efficiency, especially in distributed\nsettings.\n", "link": "http://arxiv.org/abs/2501.04239v2", "date": "2025-01-09", "relevancy": 2.7682, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5666}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5517}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Localisation%20of%20Spatial-Temporal%20Graph%20Neural%20Network&body=Title%3A%20Dynamic%20Localisation%20of%20Spatial-Temporal%20Graph%20Neural%20Network%0AAuthor%3A%20Wenying%20Duan%20and%20Shujun%20Guo%20and%20Wei%20huang%20and%20Hong%20Rao%20and%20Xiaoxi%20He%0AAbstract%3A%20%20%20Spatial-temporal%20data%2C%20fundamental%20to%20many%20intelligent%20applications%2C%20reveals%0Adependencies%20indicating%20causal%20links%20between%20present%20measurements%20at%20specific%0Alocations%20and%20historical%20data%20at%20the%20same%20or%20other%20locations.%20Within%20this%0Acontext%2C%20adaptive%20spatial-temporal%20graph%20neural%20networks%20%28ASTGNNs%29%20have%20emerged%0Aas%20valuable%20tools%20for%20modelling%20these%20dependencies%2C%20especially%20through%20a%0Adata-driven%20approach%20rather%20than%20pre-defined%20spatial%20graphs.%20While%20this%0Aapproach%20offers%20higher%20accuracy%2C%20it%20presents%20increased%20computational%20demands.%0AAddressing%20this%20challenge%2C%20this%20paper%20delves%20into%20the%20concept%20of%20localisation%0Awithin%20ASTGNNs%2C%20introducing%20an%20innovative%20perspective%20that%20spatial%20dependencies%0Ashould%20be%20dynamically%20evolving%20over%20time.%20We%20introduce%20%5Ctextit%7BDynAGS%7D%2C%20a%0Alocalised%20ASTGNN%20framework%20aimed%20at%20maximising%20efficiency%20and%20accuracy%20in%0Adistributed%20deployment.%20This%20framework%20integrates%20dynamic%20localisation%2C%0Atime-evolving%20spatial%20graphs%2C%20and%20personalised%20localisation%2C%20all%20orchestrated%0Aaround%20the%20Dynamic%20Graph%20Generator%2C%20a%20light-weighted%20central%20module%20leveraging%0Across%20attention.%20The%20central%20module%20can%20integrate%20historical%20information%20in%20a%0Anode-independent%20manner%20to%20enhance%20the%20feature%20representation%20of%20nodes%20at%20the%0Acurrent%20moment.%20This%20improved%20feature%20representation%20is%20then%20used%20to%20generate%20a%0Adynamic%20sparse%20graph%20without%20the%20need%20for%20costly%20data%20exchanges%2C%20and%20it%0Asupports%20personalised%20localisation.%20Performance%20assessments%20across%20two%20core%0AASTGNN%20architectures%20and%20nine%20real-world%20datasets%20from%20various%20applications%0Areveal%20that%20%5Ctextit%7BDynAGS%7D%20outshines%20current%20benchmarks%2C%20underscoring%20that%20the%0Adynamic%20modelling%20of%20spatial%20dependencies%20can%20drastically%20improve%20model%0Aexpressibility%2C%20flexibility%2C%20and%20system%20efficiency%2C%20especially%20in%20distributed%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04239v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Localisation%2520of%2520Spatial-Temporal%2520Graph%2520Neural%2520Network%26entry.906535625%3DWenying%2520Duan%2520and%2520Shujun%2520Guo%2520and%2520Wei%2520huang%2520and%2520Hong%2520Rao%2520and%2520Xiaoxi%2520He%26entry.1292438233%3D%2520%2520Spatial-temporal%2520data%252C%2520fundamental%2520to%2520many%2520intelligent%2520applications%252C%2520reveals%250Adependencies%2520indicating%2520causal%2520links%2520between%2520present%2520measurements%2520at%2520specific%250Alocations%2520and%2520historical%2520data%2520at%2520the%2520same%2520or%2520other%2520locations.%2520Within%2520this%250Acontext%252C%2520adaptive%2520spatial-temporal%2520graph%2520neural%2520networks%2520%2528ASTGNNs%2529%2520have%2520emerged%250Aas%2520valuable%2520tools%2520for%2520modelling%2520these%2520dependencies%252C%2520especially%2520through%2520a%250Adata-driven%2520approach%2520rather%2520than%2520pre-defined%2520spatial%2520graphs.%2520While%2520this%250Aapproach%2520offers%2520higher%2520accuracy%252C%2520it%2520presents%2520increased%2520computational%2520demands.%250AAddressing%2520this%2520challenge%252C%2520this%2520paper%2520delves%2520into%2520the%2520concept%2520of%2520localisation%250Awithin%2520ASTGNNs%252C%2520introducing%2520an%2520innovative%2520perspective%2520that%2520spatial%2520dependencies%250Ashould%2520be%2520dynamically%2520evolving%2520over%2520time.%2520We%2520introduce%2520%255Ctextit%257BDynAGS%257D%252C%2520a%250Alocalised%2520ASTGNN%2520framework%2520aimed%2520at%2520maximising%2520efficiency%2520and%2520accuracy%2520in%250Adistributed%2520deployment.%2520This%2520framework%2520integrates%2520dynamic%2520localisation%252C%250Atime-evolving%2520spatial%2520graphs%252C%2520and%2520personalised%2520localisation%252C%2520all%2520orchestrated%250Aaround%2520the%2520Dynamic%2520Graph%2520Generator%252C%2520a%2520light-weighted%2520central%2520module%2520leveraging%250Across%2520attention.%2520The%2520central%2520module%2520can%2520integrate%2520historical%2520information%2520in%2520a%250Anode-independent%2520manner%2520to%2520enhance%2520the%2520feature%2520representation%2520of%2520nodes%2520at%2520the%250Acurrent%2520moment.%2520This%2520improved%2520feature%2520representation%2520is%2520then%2520used%2520to%2520generate%2520a%250Adynamic%2520sparse%2520graph%2520without%2520the%2520need%2520for%2520costly%2520data%2520exchanges%252C%2520and%2520it%250Asupports%2520personalised%2520localisation.%2520Performance%2520assessments%2520across%2520two%2520core%250AASTGNN%2520architectures%2520and%2520nine%2520real-world%2520datasets%2520from%2520various%2520applications%250Areveal%2520that%2520%255Ctextit%257BDynAGS%257D%2520outshines%2520current%2520benchmarks%252C%2520underscoring%2520that%2520the%250Adynamic%2520modelling%2520of%2520spatial%2520dependencies%2520can%2520drastically%2520improve%2520model%250Aexpressibility%252C%2520flexibility%252C%2520and%2520system%2520efficiency%252C%2520especially%2520in%2520distributed%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04239v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Localisation%20of%20Spatial-Temporal%20Graph%20Neural%20Network&entry.906535625=Wenying%20Duan%20and%20Shujun%20Guo%20and%20Wei%20huang%20and%20Hong%20Rao%20and%20Xiaoxi%20He&entry.1292438233=%20%20Spatial-temporal%20data%2C%20fundamental%20to%20many%20intelligent%20applications%2C%20reveals%0Adependencies%20indicating%20causal%20links%20between%20present%20measurements%20at%20specific%0Alocations%20and%20historical%20data%20at%20the%20same%20or%20other%20locations.%20Within%20this%0Acontext%2C%20adaptive%20spatial-temporal%20graph%20neural%20networks%20%28ASTGNNs%29%20have%20emerged%0Aas%20valuable%20tools%20for%20modelling%20these%20dependencies%2C%20especially%20through%20a%0Adata-driven%20approach%20rather%20than%20pre-defined%20spatial%20graphs.%20While%20this%0Aapproach%20offers%20higher%20accuracy%2C%20it%20presents%20increased%20computational%20demands.%0AAddressing%20this%20challenge%2C%20this%20paper%20delves%20into%20the%20concept%20of%20localisation%0Awithin%20ASTGNNs%2C%20introducing%20an%20innovative%20perspective%20that%20spatial%20dependencies%0Ashould%20be%20dynamically%20evolving%20over%20time.%20We%20introduce%20%5Ctextit%7BDynAGS%7D%2C%20a%0Alocalised%20ASTGNN%20framework%20aimed%20at%20maximising%20efficiency%20and%20accuracy%20in%0Adistributed%20deployment.%20This%20framework%20integrates%20dynamic%20localisation%2C%0Atime-evolving%20spatial%20graphs%2C%20and%20personalised%20localisation%2C%20all%20orchestrated%0Aaround%20the%20Dynamic%20Graph%20Generator%2C%20a%20light-weighted%20central%20module%20leveraging%0Across%20attention.%20The%20central%20module%20can%20integrate%20historical%20information%20in%20a%0Anode-independent%20manner%20to%20enhance%20the%20feature%20representation%20of%20nodes%20at%20the%0Acurrent%20moment.%20This%20improved%20feature%20representation%20is%20then%20used%20to%20generate%20a%0Adynamic%20sparse%20graph%20without%20the%20need%20for%20costly%20data%20exchanges%2C%20and%20it%0Asupports%20personalised%20localisation.%20Performance%20assessments%20across%20two%20core%0AASTGNN%20architectures%20and%20nine%20real-world%20datasets%20from%20various%20applications%0Areveal%20that%20%5Ctextit%7BDynAGS%7D%20outshines%20current%20benchmarks%2C%20underscoring%20that%20the%0Adynamic%20modelling%20of%20spatial%20dependencies%20can%20drastically%20improve%20model%0Aexpressibility%2C%20flexibility%2C%20and%20system%20efficiency%2C%20especially%20in%20distributed%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04239v2&entry.124074799=Read"},
{"title": "Geometry Restoration and Dewarping of Camera-Captured Document Images", "author": "Valery Istomin and Oleg Pereziabov and Ilya Afanasyev", "abstract": "  This research focuses on developing a method for restoring the topology of\ndigital images of paper documents captured by a camera, using algorithms for\ndetection, segmentation, geometry restoration, and dewarping. Our methodology\nemploys deep learning (DL) for document outline detection, followed by computer\nvision (CV) to create a topological 2D grid using cubic polynomial\ninterpolation and correct nonlinear distortions by remapping the image. Using\nclassical CV methods makes the document topology restoration process more\nefficient and faster, as it requires significantly fewer computational\nresources and memory. We developed a new pipeline for automatic document\ndewarping and reconstruction, along with a framework and annotated dataset to\ndemonstrate its efficiency. Our experiments confirm the promise of our\nmethodology and its superiority over existing benchmarks (including mobile apps\nand popular DL solutions, such as RectiNet, DocGeoNet, and DocTr++) both\nvisually and in terms of document readability via Optical Character Recognition\n(OCR) and geometry restoration metrics. This paves the way for creating\nhigh-quality digital copies of paper documents and enhancing the efficiency of\nOCR systems. Project page: https://github.com/HorizonParadox/DRCCBI\n", "link": "http://arxiv.org/abs/2501.03145v2", "date": "2025-01-09", "relevancy": 2.7019, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5451}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5401}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry%20Restoration%20and%20Dewarping%20of%20Camera-Captured%20Document%20Images&body=Title%3A%20Geometry%20Restoration%20and%20Dewarping%20of%20Camera-Captured%20Document%20Images%0AAuthor%3A%20Valery%20Istomin%20and%20Oleg%20Pereziabov%20and%20Ilya%20Afanasyev%0AAbstract%3A%20%20%20This%20research%20focuses%20on%20developing%20a%20method%20for%20restoring%20the%20topology%20of%0Adigital%20images%20of%20paper%20documents%20captured%20by%20a%20camera%2C%20using%20algorithms%20for%0Adetection%2C%20segmentation%2C%20geometry%20restoration%2C%20and%20dewarping.%20Our%20methodology%0Aemploys%20deep%20learning%20%28DL%29%20for%20document%20outline%20detection%2C%20followed%20by%20computer%0Avision%20%28CV%29%20to%20create%20a%20topological%202D%20grid%20using%20cubic%20polynomial%0Ainterpolation%20and%20correct%20nonlinear%20distortions%20by%20remapping%20the%20image.%20Using%0Aclassical%20CV%20methods%20makes%20the%20document%20topology%20restoration%20process%20more%0Aefficient%20and%20faster%2C%20as%20it%20requires%20significantly%20fewer%20computational%0Aresources%20and%20memory.%20We%20developed%20a%20new%20pipeline%20for%20automatic%20document%0Adewarping%20and%20reconstruction%2C%20along%20with%20a%20framework%20and%20annotated%20dataset%20to%0Ademonstrate%20its%20efficiency.%20Our%20experiments%20confirm%20the%20promise%20of%20our%0Amethodology%20and%20its%20superiority%20over%20existing%20benchmarks%20%28including%20mobile%20apps%0Aand%20popular%20DL%20solutions%2C%20such%20as%20RectiNet%2C%20DocGeoNet%2C%20and%20DocTr%2B%2B%29%20both%0Avisually%20and%20in%20terms%20of%20document%20readability%20via%20Optical%20Character%20Recognition%0A%28OCR%29%20and%20geometry%20restoration%20metrics.%20This%20paves%20the%20way%20for%20creating%0Ahigh-quality%20digital%20copies%20of%20paper%20documents%20and%20enhancing%20the%20efficiency%20of%0AOCR%20systems.%20Project%20page%3A%20https%3A//github.com/HorizonParadox/DRCCBI%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03145v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry%2520Restoration%2520and%2520Dewarping%2520of%2520Camera-Captured%2520Document%2520Images%26entry.906535625%3DValery%2520Istomin%2520and%2520Oleg%2520Pereziabov%2520and%2520Ilya%2520Afanasyev%26entry.1292438233%3D%2520%2520This%2520research%2520focuses%2520on%2520developing%2520a%2520method%2520for%2520restoring%2520the%2520topology%2520of%250Adigital%2520images%2520of%2520paper%2520documents%2520captured%2520by%2520a%2520camera%252C%2520using%2520algorithms%2520for%250Adetection%252C%2520segmentation%252C%2520geometry%2520restoration%252C%2520and%2520dewarping.%2520Our%2520methodology%250Aemploys%2520deep%2520learning%2520%2528DL%2529%2520for%2520document%2520outline%2520detection%252C%2520followed%2520by%2520computer%250Avision%2520%2528CV%2529%2520to%2520create%2520a%2520topological%25202D%2520grid%2520using%2520cubic%2520polynomial%250Ainterpolation%2520and%2520correct%2520nonlinear%2520distortions%2520by%2520remapping%2520the%2520image.%2520Using%250Aclassical%2520CV%2520methods%2520makes%2520the%2520document%2520topology%2520restoration%2520process%2520more%250Aefficient%2520and%2520faster%252C%2520as%2520it%2520requires%2520significantly%2520fewer%2520computational%250Aresources%2520and%2520memory.%2520We%2520developed%2520a%2520new%2520pipeline%2520for%2520automatic%2520document%250Adewarping%2520and%2520reconstruction%252C%2520along%2520with%2520a%2520framework%2520and%2520annotated%2520dataset%2520to%250Ademonstrate%2520its%2520efficiency.%2520Our%2520experiments%2520confirm%2520the%2520promise%2520of%2520our%250Amethodology%2520and%2520its%2520superiority%2520over%2520existing%2520benchmarks%2520%2528including%2520mobile%2520apps%250Aand%2520popular%2520DL%2520solutions%252C%2520such%2520as%2520RectiNet%252C%2520DocGeoNet%252C%2520and%2520DocTr%252B%252B%2529%2520both%250Avisually%2520and%2520in%2520terms%2520of%2520document%2520readability%2520via%2520Optical%2520Character%2520Recognition%250A%2528OCR%2529%2520and%2520geometry%2520restoration%2520metrics.%2520This%2520paves%2520the%2520way%2520for%2520creating%250Ahigh-quality%2520digital%2520copies%2520of%2520paper%2520documents%2520and%2520enhancing%2520the%2520efficiency%2520of%250AOCR%2520systems.%2520Project%2520page%253A%2520https%253A//github.com/HorizonParadox/DRCCBI%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03145v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry%20Restoration%20and%20Dewarping%20of%20Camera-Captured%20Document%20Images&entry.906535625=Valery%20Istomin%20and%20Oleg%20Pereziabov%20and%20Ilya%20Afanasyev&entry.1292438233=%20%20This%20research%20focuses%20on%20developing%20a%20method%20for%20restoring%20the%20topology%20of%0Adigital%20images%20of%20paper%20documents%20captured%20by%20a%20camera%2C%20using%20algorithms%20for%0Adetection%2C%20segmentation%2C%20geometry%20restoration%2C%20and%20dewarping.%20Our%20methodology%0Aemploys%20deep%20learning%20%28DL%29%20for%20document%20outline%20detection%2C%20followed%20by%20computer%0Avision%20%28CV%29%20to%20create%20a%20topological%202D%20grid%20using%20cubic%20polynomial%0Ainterpolation%20and%20correct%20nonlinear%20distortions%20by%20remapping%20the%20image.%20Using%0Aclassical%20CV%20methods%20makes%20the%20document%20topology%20restoration%20process%20more%0Aefficient%20and%20faster%2C%20as%20it%20requires%20significantly%20fewer%20computational%0Aresources%20and%20memory.%20We%20developed%20a%20new%20pipeline%20for%20automatic%20document%0Adewarping%20and%20reconstruction%2C%20along%20with%20a%20framework%20and%20annotated%20dataset%20to%0Ademonstrate%20its%20efficiency.%20Our%20experiments%20confirm%20the%20promise%20of%20our%0Amethodology%20and%20its%20superiority%20over%20existing%20benchmarks%20%28including%20mobile%20apps%0Aand%20popular%20DL%20solutions%2C%20such%20as%20RectiNet%2C%20DocGeoNet%2C%20and%20DocTr%2B%2B%29%20both%0Avisually%20and%20in%20terms%20of%20document%20readability%20via%20Optical%20Character%20Recognition%0A%28OCR%29%20and%20geometry%20restoration%20metrics.%20This%20paves%20the%20way%20for%20creating%0Ahigh-quality%20digital%20copies%20of%20paper%20documents%20and%20enhancing%20the%20efficiency%20of%0AOCR%20systems.%20Project%20page%3A%20https%3A//github.com/HorizonParadox/DRCCBI%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03145v2&entry.124074799=Read"},
{"title": "OpenOmni: Large Language Models Pivot Zero-shot Omnimodal Alignment\n  across Language with Real-time Self-Aware Emotional Speech Synthesis", "author": "Run Luo and Ting-En Lin and Haonan Zhang and Yuchuan Wu and Xiong Liu and Min Yang and Yongbin Li and Longze Chen and Jiaming Li and Lei Zhang and Yangyi Chen and Hamid Alinejad-Rokny and Fei Huang", "abstract": "  Recent advancements in omnimodal learning have been achieved in understanding\nand generation across images, text, and speech, though mainly within\nproprietary models. Limited omnimodal datasets and the inherent challenges\nassociated with real-time emotional speech generation have hindered open-source\nprogress. To address these issues, we propose openomni, a two-stage training\nmethod combining omnimodal alignment and speech generation to develop a\nstate-of-the-art omnimodal large language model. In the alignment phase, a\npre-trained speech model is further trained on text-image tasks to generalize\nfrom vision to speech in a (near) zero-shot manner, outperforming models\ntrained on tri-modal datasets. In the speech generation phase, a lightweight\ndecoder facilitates real-time emotional speech through training on speech tasks\nand preference learning. Experiments demonstrate that openomni consistently\nimproves across omnimodal, vision-language, and speech-language evaluations,\nenabling natural, emotion-rich dialogues and real-time emotional speech\ngeneration.\n", "link": "http://arxiv.org/abs/2501.04561v2", "date": "2025-01-09", "relevancy": 2.6928, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5455}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5351}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenOmni%3A%20Large%20Language%20Models%20Pivot%20Zero-shot%20Omnimodal%20Alignment%0A%20%20across%20Language%20with%20Real-time%20Self-Aware%20Emotional%20Speech%20Synthesis&body=Title%3A%20OpenOmni%3A%20Large%20Language%20Models%20Pivot%20Zero-shot%20Omnimodal%20Alignment%0A%20%20across%20Language%20with%20Real-time%20Self-Aware%20Emotional%20Speech%20Synthesis%0AAuthor%3A%20Run%20Luo%20and%20Ting-En%20Lin%20and%20Haonan%20Zhang%20and%20Yuchuan%20Wu%20and%20Xiong%20Liu%20and%20Min%20Yang%20and%20Yongbin%20Li%20and%20Longze%20Chen%20and%20Jiaming%20Li%20and%20Lei%20Zhang%20and%20Yangyi%20Chen%20and%20Hamid%20Alinejad-Rokny%20and%20Fei%20Huang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20omnimodal%20learning%20have%20been%20achieved%20in%20understanding%0Aand%20generation%20across%20images%2C%20text%2C%20and%20speech%2C%20though%20mainly%20within%0Aproprietary%20models.%20Limited%20omnimodal%20datasets%20and%20the%20inherent%20challenges%0Aassociated%20with%20real-time%20emotional%20speech%20generation%20have%20hindered%20open-source%0Aprogress.%20To%20address%20these%20issues%2C%20we%20propose%20openomni%2C%20a%20two-stage%20training%0Amethod%20combining%20omnimodal%20alignment%20and%20speech%20generation%20to%20develop%20a%0Astate-of-the-art%20omnimodal%20large%20language%20model.%20In%20the%20alignment%20phase%2C%20a%0Apre-trained%20speech%20model%20is%20further%20trained%20on%20text-image%20tasks%20to%20generalize%0Afrom%20vision%20to%20speech%20in%20a%20%28near%29%20zero-shot%20manner%2C%20outperforming%20models%0Atrained%20on%20tri-modal%20datasets.%20In%20the%20speech%20generation%20phase%2C%20a%20lightweight%0Adecoder%20facilitates%20real-time%20emotional%20speech%20through%20training%20on%20speech%20tasks%0Aand%20preference%20learning.%20Experiments%20demonstrate%20that%20openomni%20consistently%0Aimproves%20across%20omnimodal%2C%20vision-language%2C%20and%20speech-language%20evaluations%2C%0Aenabling%20natural%2C%20emotion-rich%20dialogues%20and%20real-time%20emotional%20speech%0Ageneration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04561v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenOmni%253A%2520Large%2520Language%2520Models%2520Pivot%2520Zero-shot%2520Omnimodal%2520Alignment%250A%2520%2520across%2520Language%2520with%2520Real-time%2520Self-Aware%2520Emotional%2520Speech%2520Synthesis%26entry.906535625%3DRun%2520Luo%2520and%2520Ting-En%2520Lin%2520and%2520Haonan%2520Zhang%2520and%2520Yuchuan%2520Wu%2520and%2520Xiong%2520Liu%2520and%2520Min%2520Yang%2520and%2520Yongbin%2520Li%2520and%2520Longze%2520Chen%2520and%2520Jiaming%2520Li%2520and%2520Lei%2520Zhang%2520and%2520Yangyi%2520Chen%2520and%2520Hamid%2520Alinejad-Rokny%2520and%2520Fei%2520Huang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520omnimodal%2520learning%2520have%2520been%2520achieved%2520in%2520understanding%250Aand%2520generation%2520across%2520images%252C%2520text%252C%2520and%2520speech%252C%2520though%2520mainly%2520within%250Aproprietary%2520models.%2520Limited%2520omnimodal%2520datasets%2520and%2520the%2520inherent%2520challenges%250Aassociated%2520with%2520real-time%2520emotional%2520speech%2520generation%2520have%2520hindered%2520open-source%250Aprogress.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520openomni%252C%2520a%2520two-stage%2520training%250Amethod%2520combining%2520omnimodal%2520alignment%2520and%2520speech%2520generation%2520to%2520develop%2520a%250Astate-of-the-art%2520omnimodal%2520large%2520language%2520model.%2520In%2520the%2520alignment%2520phase%252C%2520a%250Apre-trained%2520speech%2520model%2520is%2520further%2520trained%2520on%2520text-image%2520tasks%2520to%2520generalize%250Afrom%2520vision%2520to%2520speech%2520in%2520a%2520%2528near%2529%2520zero-shot%2520manner%252C%2520outperforming%2520models%250Atrained%2520on%2520tri-modal%2520datasets.%2520In%2520the%2520speech%2520generation%2520phase%252C%2520a%2520lightweight%250Adecoder%2520facilitates%2520real-time%2520emotional%2520speech%2520through%2520training%2520on%2520speech%2520tasks%250Aand%2520preference%2520learning.%2520Experiments%2520demonstrate%2520that%2520openomni%2520consistently%250Aimproves%2520across%2520omnimodal%252C%2520vision-language%252C%2520and%2520speech-language%2520evaluations%252C%250Aenabling%2520natural%252C%2520emotion-rich%2520dialogues%2520and%2520real-time%2520emotional%2520speech%250Ageneration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04561v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenOmni%3A%20Large%20Language%20Models%20Pivot%20Zero-shot%20Omnimodal%20Alignment%0A%20%20across%20Language%20with%20Real-time%20Self-Aware%20Emotional%20Speech%20Synthesis&entry.906535625=Run%20Luo%20and%20Ting-En%20Lin%20and%20Haonan%20Zhang%20and%20Yuchuan%20Wu%20and%20Xiong%20Liu%20and%20Min%20Yang%20and%20Yongbin%20Li%20and%20Longze%20Chen%20and%20Jiaming%20Li%20and%20Lei%20Zhang%20and%20Yangyi%20Chen%20and%20Hamid%20Alinejad-Rokny%20and%20Fei%20Huang&entry.1292438233=%20%20Recent%20advancements%20in%20omnimodal%20learning%20have%20been%20achieved%20in%20understanding%0Aand%20generation%20across%20images%2C%20text%2C%20and%20speech%2C%20though%20mainly%20within%0Aproprietary%20models.%20Limited%20omnimodal%20datasets%20and%20the%20inherent%20challenges%0Aassociated%20with%20real-time%20emotional%20speech%20generation%20have%20hindered%20open-source%0Aprogress.%20To%20address%20these%20issues%2C%20we%20propose%20openomni%2C%20a%20two-stage%20training%0Amethod%20combining%20omnimodal%20alignment%20and%20speech%20generation%20to%20develop%20a%0Astate-of-the-art%20omnimodal%20large%20language%20model.%20In%20the%20alignment%20phase%2C%20a%0Apre-trained%20speech%20model%20is%20further%20trained%20on%20text-image%20tasks%20to%20generalize%0Afrom%20vision%20to%20speech%20in%20a%20%28near%29%20zero-shot%20manner%2C%20outperforming%20models%0Atrained%20on%20tri-modal%20datasets.%20In%20the%20speech%20generation%20phase%2C%20a%20lightweight%0Adecoder%20facilitates%20real-time%20emotional%20speech%20through%20training%20on%20speech%20tasks%0Aand%20preference%20learning.%20Experiments%20demonstrate%20that%20openomni%20consistently%0Aimproves%20across%20omnimodal%2C%20vision-language%2C%20and%20speech-language%20evaluations%2C%0Aenabling%20natural%2C%20emotion-rich%20dialogues%20and%20real-time%20emotional%20speech%0Ageneration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04561v2&entry.124074799=Read"},
{"title": "Optimized Sampling for Non-Line-of-Sight Imaging Using Modified Fast\n  Fourier Transforms", "author": "Talha Sultan and Alex Bocchieri and Chaoying Gu and Xiaochun Liu and Pavel Polynkin and Andreas Velten", "abstract": "  Non-line-of-Sight (NLOS) imaging systems collect light at a diffuse relay\nsurface and input this measurement into computational algorithms that output a\n3D volumetric reconstruction. These algorithms utilize the Fast Fourier\nTransform (FFT) to accelerate the reconstruction process but require both input\nand output to be sampled spatially with uniform grids. However, the geometry of\nNLOS imaging inherently results in non-uniform sampling on the relay surface\nwhen using multi-pixel detector arrays, even though such arrays significantly\nreduce acquisition times. Furthermore, using these arrays increases the data\nrate required for sensor readout, posing challenges for real-world deployment.\nIn this work, we utilize the phasor field framework to demonstrate that\nexisting NLOS imaging setups typically oversample the relay surface spatially,\nexplaining why the measurement can be compressed without significantly\nsacrificing reconstruction quality. This enables us to utilize the Non-Uniform\nFast Fourier Transform (NUFFT) to reconstruct from sparse measurements acquired\nfrom irregularly sampled relay surfaces of arbitrary shapes. Furthermore, we\nutilize the NUFFT to reconstruct at arbitrary locations in the hidden volume,\nensuring flexible sampling schemes for both the input and output. Finally, we\nutilize the Scaled Fast Fourier Transform (SFFT) to reconstruct larger volumes\nwithout increasing the number of samples stored in memory. All algorithms\nintroduced in this paper preserve the computational complexity of FFT-based\nmethods, ensuring scalability for practical NLOS imaging applications.\n", "link": "http://arxiv.org/abs/2501.05244v1", "date": "2025-01-09", "relevancy": 2.6769, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5934}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5064}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimized%20Sampling%20for%20Non-Line-of-Sight%20Imaging%20Using%20Modified%20Fast%0A%20%20Fourier%20Transforms&body=Title%3A%20Optimized%20Sampling%20for%20Non-Line-of-Sight%20Imaging%20Using%20Modified%20Fast%0A%20%20Fourier%20Transforms%0AAuthor%3A%20Talha%20Sultan%20and%20Alex%20Bocchieri%20and%20Chaoying%20Gu%20and%20Xiaochun%20Liu%20and%20Pavel%20Polynkin%20and%20Andreas%20Velten%0AAbstract%3A%20%20%20Non-line-of-Sight%20%28NLOS%29%20imaging%20systems%20collect%20light%20at%20a%20diffuse%20relay%0Asurface%20and%20input%20this%20measurement%20into%20computational%20algorithms%20that%20output%20a%0A3D%20volumetric%20reconstruction.%20These%20algorithms%20utilize%20the%20Fast%20Fourier%0ATransform%20%28FFT%29%20to%20accelerate%20the%20reconstruction%20process%20but%20require%20both%20input%0Aand%20output%20to%20be%20sampled%20spatially%20with%20uniform%20grids.%20However%2C%20the%20geometry%20of%0ANLOS%20imaging%20inherently%20results%20in%20non-uniform%20sampling%20on%20the%20relay%20surface%0Awhen%20using%20multi-pixel%20detector%20arrays%2C%20even%20though%20such%20arrays%20significantly%0Areduce%20acquisition%20times.%20Furthermore%2C%20using%20these%20arrays%20increases%20the%20data%0Arate%20required%20for%20sensor%20readout%2C%20posing%20challenges%20for%20real-world%20deployment.%0AIn%20this%20work%2C%20we%20utilize%20the%20phasor%20field%20framework%20to%20demonstrate%20that%0Aexisting%20NLOS%20imaging%20setups%20typically%20oversample%20the%20relay%20surface%20spatially%2C%0Aexplaining%20why%20the%20measurement%20can%20be%20compressed%20without%20significantly%0Asacrificing%20reconstruction%20quality.%20This%20enables%20us%20to%20utilize%20the%20Non-Uniform%0AFast%20Fourier%20Transform%20%28NUFFT%29%20to%20reconstruct%20from%20sparse%20measurements%20acquired%0Afrom%20irregularly%20sampled%20relay%20surfaces%20of%20arbitrary%20shapes.%20Furthermore%2C%20we%0Autilize%20the%20NUFFT%20to%20reconstruct%20at%20arbitrary%20locations%20in%20the%20hidden%20volume%2C%0Aensuring%20flexible%20sampling%20schemes%20for%20both%20the%20input%20and%20output.%20Finally%2C%20we%0Autilize%20the%20Scaled%20Fast%20Fourier%20Transform%20%28SFFT%29%20to%20reconstruct%20larger%20volumes%0Awithout%20increasing%20the%20number%20of%20samples%20stored%20in%20memory.%20All%20algorithms%0Aintroduced%20in%20this%20paper%20preserve%20the%20computational%20complexity%20of%20FFT-based%0Amethods%2C%20ensuring%20scalability%20for%20practical%20NLOS%20imaging%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05244v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimized%2520Sampling%2520for%2520Non-Line-of-Sight%2520Imaging%2520Using%2520Modified%2520Fast%250A%2520%2520Fourier%2520Transforms%26entry.906535625%3DTalha%2520Sultan%2520and%2520Alex%2520Bocchieri%2520and%2520Chaoying%2520Gu%2520and%2520Xiaochun%2520Liu%2520and%2520Pavel%2520Polynkin%2520and%2520Andreas%2520Velten%26entry.1292438233%3D%2520%2520Non-line-of-Sight%2520%2528NLOS%2529%2520imaging%2520systems%2520collect%2520light%2520at%2520a%2520diffuse%2520relay%250Asurface%2520and%2520input%2520this%2520measurement%2520into%2520computational%2520algorithms%2520that%2520output%2520a%250A3D%2520volumetric%2520reconstruction.%2520These%2520algorithms%2520utilize%2520the%2520Fast%2520Fourier%250ATransform%2520%2528FFT%2529%2520to%2520accelerate%2520the%2520reconstruction%2520process%2520but%2520require%2520both%2520input%250Aand%2520output%2520to%2520be%2520sampled%2520spatially%2520with%2520uniform%2520grids.%2520However%252C%2520the%2520geometry%2520of%250ANLOS%2520imaging%2520inherently%2520results%2520in%2520non-uniform%2520sampling%2520on%2520the%2520relay%2520surface%250Awhen%2520using%2520multi-pixel%2520detector%2520arrays%252C%2520even%2520though%2520such%2520arrays%2520significantly%250Areduce%2520acquisition%2520times.%2520Furthermore%252C%2520using%2520these%2520arrays%2520increases%2520the%2520data%250Arate%2520required%2520for%2520sensor%2520readout%252C%2520posing%2520challenges%2520for%2520real-world%2520deployment.%250AIn%2520this%2520work%252C%2520we%2520utilize%2520the%2520phasor%2520field%2520framework%2520to%2520demonstrate%2520that%250Aexisting%2520NLOS%2520imaging%2520setups%2520typically%2520oversample%2520the%2520relay%2520surface%2520spatially%252C%250Aexplaining%2520why%2520the%2520measurement%2520can%2520be%2520compressed%2520without%2520significantly%250Asacrificing%2520reconstruction%2520quality.%2520This%2520enables%2520us%2520to%2520utilize%2520the%2520Non-Uniform%250AFast%2520Fourier%2520Transform%2520%2528NUFFT%2529%2520to%2520reconstruct%2520from%2520sparse%2520measurements%2520acquired%250Afrom%2520irregularly%2520sampled%2520relay%2520surfaces%2520of%2520arbitrary%2520shapes.%2520Furthermore%252C%2520we%250Autilize%2520the%2520NUFFT%2520to%2520reconstruct%2520at%2520arbitrary%2520locations%2520in%2520the%2520hidden%2520volume%252C%250Aensuring%2520flexible%2520sampling%2520schemes%2520for%2520both%2520the%2520input%2520and%2520output.%2520Finally%252C%2520we%250Autilize%2520the%2520Scaled%2520Fast%2520Fourier%2520Transform%2520%2528SFFT%2529%2520to%2520reconstruct%2520larger%2520volumes%250Awithout%2520increasing%2520the%2520number%2520of%2520samples%2520stored%2520in%2520memory.%2520All%2520algorithms%250Aintroduced%2520in%2520this%2520paper%2520preserve%2520the%2520computational%2520complexity%2520of%2520FFT-based%250Amethods%252C%2520ensuring%2520scalability%2520for%2520practical%2520NLOS%2520imaging%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05244v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimized%20Sampling%20for%20Non-Line-of-Sight%20Imaging%20Using%20Modified%20Fast%0A%20%20Fourier%20Transforms&entry.906535625=Talha%20Sultan%20and%20Alex%20Bocchieri%20and%20Chaoying%20Gu%20and%20Xiaochun%20Liu%20and%20Pavel%20Polynkin%20and%20Andreas%20Velten&entry.1292438233=%20%20Non-line-of-Sight%20%28NLOS%29%20imaging%20systems%20collect%20light%20at%20a%20diffuse%20relay%0Asurface%20and%20input%20this%20measurement%20into%20computational%20algorithms%20that%20output%20a%0A3D%20volumetric%20reconstruction.%20These%20algorithms%20utilize%20the%20Fast%20Fourier%0ATransform%20%28FFT%29%20to%20accelerate%20the%20reconstruction%20process%20but%20require%20both%20input%0Aand%20output%20to%20be%20sampled%20spatially%20with%20uniform%20grids.%20However%2C%20the%20geometry%20of%0ANLOS%20imaging%20inherently%20results%20in%20non-uniform%20sampling%20on%20the%20relay%20surface%0Awhen%20using%20multi-pixel%20detector%20arrays%2C%20even%20though%20such%20arrays%20significantly%0Areduce%20acquisition%20times.%20Furthermore%2C%20using%20these%20arrays%20increases%20the%20data%0Arate%20required%20for%20sensor%20readout%2C%20posing%20challenges%20for%20real-world%20deployment.%0AIn%20this%20work%2C%20we%20utilize%20the%20phasor%20field%20framework%20to%20demonstrate%20that%0Aexisting%20NLOS%20imaging%20setups%20typically%20oversample%20the%20relay%20surface%20spatially%2C%0Aexplaining%20why%20the%20measurement%20can%20be%20compressed%20without%20significantly%0Asacrificing%20reconstruction%20quality.%20This%20enables%20us%20to%20utilize%20the%20Non-Uniform%0AFast%20Fourier%20Transform%20%28NUFFT%29%20to%20reconstruct%20from%20sparse%20measurements%20acquired%0Afrom%20irregularly%20sampled%20relay%20surfaces%20of%20arbitrary%20shapes.%20Furthermore%2C%20we%0Autilize%20the%20NUFFT%20to%20reconstruct%20at%20arbitrary%20locations%20in%20the%20hidden%20volume%2C%0Aensuring%20flexible%20sampling%20schemes%20for%20both%20the%20input%20and%20output.%20Finally%2C%20we%0Autilize%20the%20Scaled%20Fast%20Fourier%20Transform%20%28SFFT%29%20to%20reconstruct%20larger%20volumes%0Awithout%20increasing%20the%20number%20of%20samples%20stored%20in%20memory.%20All%20algorithms%0Aintroduced%20in%20this%20paper%20preserve%20the%20computational%20complexity%20of%20FFT-based%0Amethods%2C%20ensuring%20scalability%20for%20practical%20NLOS%20imaging%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05244v1&entry.124074799=Read"},
{"title": "Semi-supervised 3D Semantic Scene Completion with 2D Vision Foundation\n  Model Guidance", "author": "Duc-Hai Pham and Duc-Dung Nguyen and Anh Pham and Tuan Ho and Phong Nguyen and Khoi Nguyen and Rang Nguyen", "abstract": "  Accurate prediction of 3D semantic occupancy from 2D visual images is vital\nin enabling autonomous agents to comprehend their surroundings for planning and\nnavigation. State-of-the-art methods typically employ fully supervised\napproaches, necessitating a huge labeled dataset acquired through expensive\nLiDAR sensors and meticulous voxel-wise labeling by human annotators. The\nresource-intensive nature of this annotating process significantly hampers the\napplication and scalability of these methods. We introduce a novel\nsemi-supervised framework to alleviate the dependency on densely annotated\ndata. Our approach leverages 2D foundation models to generate essential 3D\nscene geometric and semantic cues, facilitating a more efficient training\nprocess. Our framework exhibits notable properties: (1) Generalizability,\napplicable to various 3D semantic scene completion approaches, including 2D-3D\nlifting and 3D-2D transformer methods. (2) Effectiveness, as demonstrated\nthrough experiments on SemanticKITTI and NYUv2, wherein our method achieves up\nto 85% of the fully-supervised performance using only 10% labeled data. This\napproach not only reduces the cost and labor associated with data annotation\nbut also demonstrates the potential for broader adoption in camera-based\nsystems for 3D semantic occupancy prediction.\n", "link": "http://arxiv.org/abs/2408.11559v4", "date": "2025-01-09", "relevancy": 2.6518, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6718}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6718}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semi-supervised%203D%20Semantic%20Scene%20Completion%20with%202D%20Vision%20Foundation%0A%20%20Model%20Guidance&body=Title%3A%20Semi-supervised%203D%20Semantic%20Scene%20Completion%20with%202D%20Vision%20Foundation%0A%20%20Model%20Guidance%0AAuthor%3A%20Duc-Hai%20Pham%20and%20Duc-Dung%20Nguyen%20and%20Anh%20Pham%20and%20Tuan%20Ho%20and%20Phong%20Nguyen%20and%20Khoi%20Nguyen%20and%20Rang%20Nguyen%0AAbstract%3A%20%20%20Accurate%20prediction%20of%203D%20semantic%20occupancy%20from%202D%20visual%20images%20is%20vital%0Ain%20enabling%20autonomous%20agents%20to%20comprehend%20their%20surroundings%20for%20planning%20and%0Anavigation.%20State-of-the-art%20methods%20typically%20employ%20fully%20supervised%0Aapproaches%2C%20necessitating%20a%20huge%20labeled%20dataset%20acquired%20through%20expensive%0ALiDAR%20sensors%20and%20meticulous%20voxel-wise%20labeling%20by%20human%20annotators.%20The%0Aresource-intensive%20nature%20of%20this%20annotating%20process%20significantly%20hampers%20the%0Aapplication%20and%20scalability%20of%20these%20methods.%20We%20introduce%20a%20novel%0Asemi-supervised%20framework%20to%20alleviate%20the%20dependency%20on%20densely%20annotated%0Adata.%20Our%20approach%20leverages%202D%20foundation%20models%20to%20generate%20essential%203D%0Ascene%20geometric%20and%20semantic%20cues%2C%20facilitating%20a%20more%20efficient%20training%0Aprocess.%20Our%20framework%20exhibits%20notable%20properties%3A%20%281%29%20Generalizability%2C%0Aapplicable%20to%20various%203D%20semantic%20scene%20completion%20approaches%2C%20including%202D-3D%0Alifting%20and%203D-2D%20transformer%20methods.%20%282%29%20Effectiveness%2C%20as%20demonstrated%0Athrough%20experiments%20on%20SemanticKITTI%20and%20NYUv2%2C%20wherein%20our%20method%20achieves%20up%0Ato%2085%25%20of%20the%20fully-supervised%20performance%20using%20only%2010%25%20labeled%20data.%20This%0Aapproach%20not%20only%20reduces%20the%20cost%20and%20labor%20associated%20with%20data%20annotation%0Abut%20also%20demonstrates%20the%20potential%20for%20broader%20adoption%20in%20camera-based%0Asystems%20for%203D%20semantic%20occupancy%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11559v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemi-supervised%25203D%2520Semantic%2520Scene%2520Completion%2520with%25202D%2520Vision%2520Foundation%250A%2520%2520Model%2520Guidance%26entry.906535625%3DDuc-Hai%2520Pham%2520and%2520Duc-Dung%2520Nguyen%2520and%2520Anh%2520Pham%2520and%2520Tuan%2520Ho%2520and%2520Phong%2520Nguyen%2520and%2520Khoi%2520Nguyen%2520and%2520Rang%2520Nguyen%26entry.1292438233%3D%2520%2520Accurate%2520prediction%2520of%25203D%2520semantic%2520occupancy%2520from%25202D%2520visual%2520images%2520is%2520vital%250Ain%2520enabling%2520autonomous%2520agents%2520to%2520comprehend%2520their%2520surroundings%2520for%2520planning%2520and%250Anavigation.%2520State-of-the-art%2520methods%2520typically%2520employ%2520fully%2520supervised%250Aapproaches%252C%2520necessitating%2520a%2520huge%2520labeled%2520dataset%2520acquired%2520through%2520expensive%250ALiDAR%2520sensors%2520and%2520meticulous%2520voxel-wise%2520labeling%2520by%2520human%2520annotators.%2520The%250Aresource-intensive%2520nature%2520of%2520this%2520annotating%2520process%2520significantly%2520hampers%2520the%250Aapplication%2520and%2520scalability%2520of%2520these%2520methods.%2520We%2520introduce%2520a%2520novel%250Asemi-supervised%2520framework%2520to%2520alleviate%2520the%2520dependency%2520on%2520densely%2520annotated%250Adata.%2520Our%2520approach%2520leverages%25202D%2520foundation%2520models%2520to%2520generate%2520essential%25203D%250Ascene%2520geometric%2520and%2520semantic%2520cues%252C%2520facilitating%2520a%2520more%2520efficient%2520training%250Aprocess.%2520Our%2520framework%2520exhibits%2520notable%2520properties%253A%2520%25281%2529%2520Generalizability%252C%250Aapplicable%2520to%2520various%25203D%2520semantic%2520scene%2520completion%2520approaches%252C%2520including%25202D-3D%250Alifting%2520and%25203D-2D%2520transformer%2520methods.%2520%25282%2529%2520Effectiveness%252C%2520as%2520demonstrated%250Athrough%2520experiments%2520on%2520SemanticKITTI%2520and%2520NYUv2%252C%2520wherein%2520our%2520method%2520achieves%2520up%250Ato%252085%2525%2520of%2520the%2520fully-supervised%2520performance%2520using%2520only%252010%2525%2520labeled%2520data.%2520This%250Aapproach%2520not%2520only%2520reduces%2520the%2520cost%2520and%2520labor%2520associated%2520with%2520data%2520annotation%250Abut%2520also%2520demonstrates%2520the%2520potential%2520for%2520broader%2520adoption%2520in%2520camera-based%250Asystems%2520for%25203D%2520semantic%2520occupancy%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11559v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-supervised%203D%20Semantic%20Scene%20Completion%20with%202D%20Vision%20Foundation%0A%20%20Model%20Guidance&entry.906535625=Duc-Hai%20Pham%20and%20Duc-Dung%20Nguyen%20and%20Anh%20Pham%20and%20Tuan%20Ho%20and%20Phong%20Nguyen%20and%20Khoi%20Nguyen%20and%20Rang%20Nguyen&entry.1292438233=%20%20Accurate%20prediction%20of%203D%20semantic%20occupancy%20from%202D%20visual%20images%20is%20vital%0Ain%20enabling%20autonomous%20agents%20to%20comprehend%20their%20surroundings%20for%20planning%20and%0Anavigation.%20State-of-the-art%20methods%20typically%20employ%20fully%20supervised%0Aapproaches%2C%20necessitating%20a%20huge%20labeled%20dataset%20acquired%20through%20expensive%0ALiDAR%20sensors%20and%20meticulous%20voxel-wise%20labeling%20by%20human%20annotators.%20The%0Aresource-intensive%20nature%20of%20this%20annotating%20process%20significantly%20hampers%20the%0Aapplication%20and%20scalability%20of%20these%20methods.%20We%20introduce%20a%20novel%0Asemi-supervised%20framework%20to%20alleviate%20the%20dependency%20on%20densely%20annotated%0Adata.%20Our%20approach%20leverages%202D%20foundation%20models%20to%20generate%20essential%203D%0Ascene%20geometric%20and%20semantic%20cues%2C%20facilitating%20a%20more%20efficient%20training%0Aprocess.%20Our%20framework%20exhibits%20notable%20properties%3A%20%281%29%20Generalizability%2C%0Aapplicable%20to%20various%203D%20semantic%20scene%20completion%20approaches%2C%20including%202D-3D%0Alifting%20and%203D-2D%20transformer%20methods.%20%282%29%20Effectiveness%2C%20as%20demonstrated%0Athrough%20experiments%20on%20SemanticKITTI%20and%20NYUv2%2C%20wherein%20our%20method%20achieves%20up%0Ato%2085%25%20of%20the%20fully-supervised%20performance%20using%20only%2010%25%20labeled%20data.%20This%0Aapproach%20not%20only%20reduces%20the%20cost%20and%20labor%20associated%20with%20data%20annotation%0Abut%20also%20demonstrates%20the%20potential%20for%20broader%20adoption%20in%20camera-based%0Asystems%20for%203D%20semantic%20occupancy%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11559v4&entry.124074799=Read"},
{"title": "CellViT++: Energy-Efficient and Adaptive Cell Segmentation and\n  Classification Using Foundation Models", "author": "Fabian H\u00f6rst and Moritz Rempe and Helmut Becker and Lukas Heine and Julius Keyl and Jens Kleesiek", "abstract": "  Digital Pathology is a cornerstone in the diagnosis and treatment of\ndiseases. A key task in this field is the identification and segmentation of\ncells in hematoxylin and eosin-stained images. Existing methods for cell\nsegmentation often require extensive annotated datasets for training and are\nlimited to a predefined cell classification scheme. To overcome these\nlimitations, we propose $\\text{CellViT}^{{\\scriptscriptstyle ++}}$, a framework\nfor generalized cell segmentation in digital pathology.\n$\\text{CellViT}^{{\\scriptscriptstyle ++}}$ utilizes Vision Transformers with\nfoundation models as encoders to compute deep cell features and segmentation\nmasks simultaneously. To adapt to unseen cell types, we rely on a\ncomputationally efficient approach. It requires minimal data for training and\nleads to a drastically reduced carbon footprint. We demonstrate excellent\nperformance on seven different datasets, covering a broad spectrum of cell\ntypes, organs, and clinical settings. The framework achieves remarkable\nzero-shot segmentation and data-efficient cell-type classification.\nFurthermore, we show that $\\text{CellViT}^{{\\scriptscriptstyle ++}}$ can\nleverage immunofluorescence stainings to generate training datasets without the\nneed for pathologist annotations. The automated dataset generation approach\nsurpasses the performance of networks trained on manually labeled data,\ndemonstrating its effectiveness in creating high-quality training datasets\nwithout expert annotations. To advance digital pathology,\n$\\text{CellViT}^{{\\scriptscriptstyle ++}}$ is available as an open-source\nframework featuring a user-friendly, web-based interface for visualization and\nannotation. The code is available under\nhttps://github.com/TIO-IKIM/CellViT-plus-plus.\n", "link": "http://arxiv.org/abs/2501.05269v1", "date": "2025-01-09", "relevancy": 2.6216, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5303}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5253}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CellViT%2B%2B%3A%20Energy-Efficient%20and%20Adaptive%20Cell%20Segmentation%20and%0A%20%20Classification%20Using%20Foundation%20Models&body=Title%3A%20CellViT%2B%2B%3A%20Energy-Efficient%20and%20Adaptive%20Cell%20Segmentation%20and%0A%20%20Classification%20Using%20Foundation%20Models%0AAuthor%3A%20Fabian%20H%C3%B6rst%20and%20Moritz%20Rempe%20and%20Helmut%20Becker%20and%20Lukas%20Heine%20and%20Julius%20Keyl%20and%20Jens%20Kleesiek%0AAbstract%3A%20%20%20Digital%20Pathology%20is%20a%20cornerstone%20in%20the%20diagnosis%20and%20treatment%20of%0Adiseases.%20A%20key%20task%20in%20this%20field%20is%20the%20identification%20and%20segmentation%20of%0Acells%20in%20hematoxylin%20and%20eosin-stained%20images.%20Existing%20methods%20for%20cell%0Asegmentation%20often%20require%20extensive%20annotated%20datasets%20for%20training%20and%20are%0Alimited%20to%20a%20predefined%20cell%20classification%20scheme.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20%24%5Ctext%7BCellViT%7D%5E%7B%7B%5Cscriptscriptstyle%20%2B%2B%7D%7D%24%2C%20a%20framework%0Afor%20generalized%20cell%20segmentation%20in%20digital%20pathology.%0A%24%5Ctext%7BCellViT%7D%5E%7B%7B%5Cscriptscriptstyle%20%2B%2B%7D%7D%24%20utilizes%20Vision%20Transformers%20with%0Afoundation%20models%20as%20encoders%20to%20compute%20deep%20cell%20features%20and%20segmentation%0Amasks%20simultaneously.%20To%20adapt%20to%20unseen%20cell%20types%2C%20we%20rely%20on%20a%0Acomputationally%20efficient%20approach.%20It%20requires%20minimal%20data%20for%20training%20and%0Aleads%20to%20a%20drastically%20reduced%20carbon%20footprint.%20We%20demonstrate%20excellent%0Aperformance%20on%20seven%20different%20datasets%2C%20covering%20a%20broad%20spectrum%20of%20cell%0Atypes%2C%20organs%2C%20and%20clinical%20settings.%20The%20framework%20achieves%20remarkable%0Azero-shot%20segmentation%20and%20data-efficient%20cell-type%20classification.%0AFurthermore%2C%20we%20show%20that%20%24%5Ctext%7BCellViT%7D%5E%7B%7B%5Cscriptscriptstyle%20%2B%2B%7D%7D%24%20can%0Aleverage%20immunofluorescence%20stainings%20to%20generate%20training%20datasets%20without%20the%0Aneed%20for%20pathologist%20annotations.%20The%20automated%20dataset%20generation%20approach%0Asurpasses%20the%20performance%20of%20networks%20trained%20on%20manually%20labeled%20data%2C%0Ademonstrating%20its%20effectiveness%20in%20creating%20high-quality%20training%20datasets%0Awithout%20expert%20annotations.%20To%20advance%20digital%20pathology%2C%0A%24%5Ctext%7BCellViT%7D%5E%7B%7B%5Cscriptscriptstyle%20%2B%2B%7D%7D%24%20is%20available%20as%20an%20open-source%0Aframework%20featuring%20a%20user-friendly%2C%20web-based%20interface%20for%20visualization%20and%0Aannotation.%20The%20code%20is%20available%20under%0Ahttps%3A//github.com/TIO-IKIM/CellViT-plus-plus.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05269v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCellViT%252B%252B%253A%2520Energy-Efficient%2520and%2520Adaptive%2520Cell%2520Segmentation%2520and%250A%2520%2520Classification%2520Using%2520Foundation%2520Models%26entry.906535625%3DFabian%2520H%25C3%25B6rst%2520and%2520Moritz%2520Rempe%2520and%2520Helmut%2520Becker%2520and%2520Lukas%2520Heine%2520and%2520Julius%2520Keyl%2520and%2520Jens%2520Kleesiek%26entry.1292438233%3D%2520%2520Digital%2520Pathology%2520is%2520a%2520cornerstone%2520in%2520the%2520diagnosis%2520and%2520treatment%2520of%250Adiseases.%2520A%2520key%2520task%2520in%2520this%2520field%2520is%2520the%2520identification%2520and%2520segmentation%2520of%250Acells%2520in%2520hematoxylin%2520and%2520eosin-stained%2520images.%2520Existing%2520methods%2520for%2520cell%250Asegmentation%2520often%2520require%2520extensive%2520annotated%2520datasets%2520for%2520training%2520and%2520are%250Alimited%2520to%2520a%2520predefined%2520cell%2520classification%2520scheme.%2520To%2520overcome%2520these%250Alimitations%252C%2520we%2520propose%2520%2524%255Ctext%257BCellViT%257D%255E%257B%257B%255Cscriptscriptstyle%2520%252B%252B%257D%257D%2524%252C%2520a%2520framework%250Afor%2520generalized%2520cell%2520segmentation%2520in%2520digital%2520pathology.%250A%2524%255Ctext%257BCellViT%257D%255E%257B%257B%255Cscriptscriptstyle%2520%252B%252B%257D%257D%2524%2520utilizes%2520Vision%2520Transformers%2520with%250Afoundation%2520models%2520as%2520encoders%2520to%2520compute%2520deep%2520cell%2520features%2520and%2520segmentation%250Amasks%2520simultaneously.%2520To%2520adapt%2520to%2520unseen%2520cell%2520types%252C%2520we%2520rely%2520on%2520a%250Acomputationally%2520efficient%2520approach.%2520It%2520requires%2520minimal%2520data%2520for%2520training%2520and%250Aleads%2520to%2520a%2520drastically%2520reduced%2520carbon%2520footprint.%2520We%2520demonstrate%2520excellent%250Aperformance%2520on%2520seven%2520different%2520datasets%252C%2520covering%2520a%2520broad%2520spectrum%2520of%2520cell%250Atypes%252C%2520organs%252C%2520and%2520clinical%2520settings.%2520The%2520framework%2520achieves%2520remarkable%250Azero-shot%2520segmentation%2520and%2520data-efficient%2520cell-type%2520classification.%250AFurthermore%252C%2520we%2520show%2520that%2520%2524%255Ctext%257BCellViT%257D%255E%257B%257B%255Cscriptscriptstyle%2520%252B%252B%257D%257D%2524%2520can%250Aleverage%2520immunofluorescence%2520stainings%2520to%2520generate%2520training%2520datasets%2520without%2520the%250Aneed%2520for%2520pathologist%2520annotations.%2520The%2520automated%2520dataset%2520generation%2520approach%250Asurpasses%2520the%2520performance%2520of%2520networks%2520trained%2520on%2520manually%2520labeled%2520data%252C%250Ademonstrating%2520its%2520effectiveness%2520in%2520creating%2520high-quality%2520training%2520datasets%250Awithout%2520expert%2520annotations.%2520To%2520advance%2520digital%2520pathology%252C%250A%2524%255Ctext%257BCellViT%257D%255E%257B%257B%255Cscriptscriptstyle%2520%252B%252B%257D%257D%2524%2520is%2520available%2520as%2520an%2520open-source%250Aframework%2520featuring%2520a%2520user-friendly%252C%2520web-based%2520interface%2520for%2520visualization%2520and%250Aannotation.%2520The%2520code%2520is%2520available%2520under%250Ahttps%253A//github.com/TIO-IKIM/CellViT-plus-plus.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05269v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CellViT%2B%2B%3A%20Energy-Efficient%20and%20Adaptive%20Cell%20Segmentation%20and%0A%20%20Classification%20Using%20Foundation%20Models&entry.906535625=Fabian%20H%C3%B6rst%20and%20Moritz%20Rempe%20and%20Helmut%20Becker%20and%20Lukas%20Heine%20and%20Julius%20Keyl%20and%20Jens%20Kleesiek&entry.1292438233=%20%20Digital%20Pathology%20is%20a%20cornerstone%20in%20the%20diagnosis%20and%20treatment%20of%0Adiseases.%20A%20key%20task%20in%20this%20field%20is%20the%20identification%20and%20segmentation%20of%0Acells%20in%20hematoxylin%20and%20eosin-stained%20images.%20Existing%20methods%20for%20cell%0Asegmentation%20often%20require%20extensive%20annotated%20datasets%20for%20training%20and%20are%0Alimited%20to%20a%20predefined%20cell%20classification%20scheme.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20%24%5Ctext%7BCellViT%7D%5E%7B%7B%5Cscriptscriptstyle%20%2B%2B%7D%7D%24%2C%20a%20framework%0Afor%20generalized%20cell%20segmentation%20in%20digital%20pathology.%0A%24%5Ctext%7BCellViT%7D%5E%7B%7B%5Cscriptscriptstyle%20%2B%2B%7D%7D%24%20utilizes%20Vision%20Transformers%20with%0Afoundation%20models%20as%20encoders%20to%20compute%20deep%20cell%20features%20and%20segmentation%0Amasks%20simultaneously.%20To%20adapt%20to%20unseen%20cell%20types%2C%20we%20rely%20on%20a%0Acomputationally%20efficient%20approach.%20It%20requires%20minimal%20data%20for%20training%20and%0Aleads%20to%20a%20drastically%20reduced%20carbon%20footprint.%20We%20demonstrate%20excellent%0Aperformance%20on%20seven%20different%20datasets%2C%20covering%20a%20broad%20spectrum%20of%20cell%0Atypes%2C%20organs%2C%20and%20clinical%20settings.%20The%20framework%20achieves%20remarkable%0Azero-shot%20segmentation%20and%20data-efficient%20cell-type%20classification.%0AFurthermore%2C%20we%20show%20that%20%24%5Ctext%7BCellViT%7D%5E%7B%7B%5Cscriptscriptstyle%20%2B%2B%7D%7D%24%20can%0Aleverage%20immunofluorescence%20stainings%20to%20generate%20training%20datasets%20without%20the%0Aneed%20for%20pathologist%20annotations.%20The%20automated%20dataset%20generation%20approach%0Asurpasses%20the%20performance%20of%20networks%20trained%20on%20manually%20labeled%20data%2C%0Ademonstrating%20its%20effectiveness%20in%20creating%20high-quality%20training%20datasets%0Awithout%20expert%20annotations.%20To%20advance%20digital%20pathology%2C%0A%24%5Ctext%7BCellViT%7D%5E%7B%7B%5Cscriptscriptstyle%20%2B%2B%7D%7D%24%20is%20available%20as%20an%20open-source%0Aframework%20featuring%20a%20user-friendly%2C%20web-based%20interface%20for%20visualization%20and%0Aannotation.%20The%20code%20is%20available%20under%0Ahttps%3A//github.com/TIO-IKIM/CellViT-plus-plus.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05269v1&entry.124074799=Read"},
{"title": "Filter-then-Generate: Large Language Models with Structure-Text Adapter\n  for Knowledge Graph Completion", "author": "Ben Liu and Jihai Zhang and Fangquan Lin and Cheng Yang and Min Peng", "abstract": "  Large Language Models (LLMs) present massive inherent knowledge and superior\nsemantic comprehension capability, which have revolutionized various tasks in\nnatural language processing. Despite their success, a critical gap remains in\nenabling LLMs to perform knowledge graph completion (KGC). Empirical evidence\nsuggests that LLMs consistently perform worse than conventional KGC approaches,\neven through sophisticated prompt design or tailored instruction-tuning.\nFundamentally, applying LLMs on KGC introduces several critical challenges,\nincluding a vast set of entity candidates, hallucination issue of LLMs, and\nunder-exploitation of the graph structure. To address these challenges, we\npropose a novel instruction-tuning-based method, namely FtG. Specifically, we\npresent a \\textit{filter-then-generate} paradigm and formulate the KGC task\ninto a multiple-choice question format. In this way, we can harness the\ncapability of LLMs while mitigating the issue casused by hallucinations.\nMoreover, we devise a flexible ego-graph serialization prompt and employ a\nstructure-text adapter to couple structure and text information in a\ncontextualized manner. Experimental results demonstrate that FtG achieves\nsubstantial performance gain compared to existing state-of-the-art methods. The\ninstruction dataset and code are available at\n\\url{https://github.com/LB0828/FtG}.\n", "link": "http://arxiv.org/abs/2412.09094v2", "date": "2025-01-09", "relevancy": 2.5958, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.533}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5165}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Filter-then-Generate%3A%20Large%20Language%20Models%20with%20Structure-Text%20Adapter%0A%20%20for%20Knowledge%20Graph%20Completion&body=Title%3A%20Filter-then-Generate%3A%20Large%20Language%20Models%20with%20Structure-Text%20Adapter%0A%20%20for%20Knowledge%20Graph%20Completion%0AAuthor%3A%20Ben%20Liu%20and%20Jihai%20Zhang%20and%20Fangquan%20Lin%20and%20Cheng%20Yang%20and%20Min%20Peng%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20present%20massive%20inherent%20knowledge%20and%20superior%0Asemantic%20comprehension%20capability%2C%20which%20have%20revolutionized%20various%20tasks%20in%0Anatural%20language%20processing.%20Despite%20their%20success%2C%20a%20critical%20gap%20remains%20in%0Aenabling%20LLMs%20to%20perform%20knowledge%20graph%20completion%20%28KGC%29.%20Empirical%20evidence%0Asuggests%20that%20LLMs%20consistently%20perform%20worse%20than%20conventional%20KGC%20approaches%2C%0Aeven%20through%20sophisticated%20prompt%20design%20or%20tailored%20instruction-tuning.%0AFundamentally%2C%20applying%20LLMs%20on%20KGC%20introduces%20several%20critical%20challenges%2C%0Aincluding%20a%20vast%20set%20of%20entity%20candidates%2C%20hallucination%20issue%20of%20LLMs%2C%20and%0Aunder-exploitation%20of%20the%20graph%20structure.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20novel%20instruction-tuning-based%20method%2C%20namely%20FtG.%20Specifically%2C%20we%0Apresent%20a%20%5Ctextit%7Bfilter-then-generate%7D%20paradigm%20and%20formulate%20the%20KGC%20task%0Ainto%20a%20multiple-choice%20question%20format.%20In%20this%20way%2C%20we%20can%20harness%20the%0Acapability%20of%20LLMs%20while%20mitigating%20the%20issue%20casused%20by%20hallucinations.%0AMoreover%2C%20we%20devise%20a%20flexible%20ego-graph%20serialization%20prompt%20and%20employ%20a%0Astructure-text%20adapter%20to%20couple%20structure%20and%20text%20information%20in%20a%0Acontextualized%20manner.%20Experimental%20results%20demonstrate%20that%20FtG%20achieves%0Asubstantial%20performance%20gain%20compared%20to%20existing%20state-of-the-art%20methods.%20The%0Ainstruction%20dataset%20and%20code%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/LB0828/FtG%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09094v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFilter-then-Generate%253A%2520Large%2520Language%2520Models%2520with%2520Structure-Text%2520Adapter%250A%2520%2520for%2520Knowledge%2520Graph%2520Completion%26entry.906535625%3DBen%2520Liu%2520and%2520Jihai%2520Zhang%2520and%2520Fangquan%2520Lin%2520and%2520Cheng%2520Yang%2520and%2520Min%2520Peng%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520present%2520massive%2520inherent%2520knowledge%2520and%2520superior%250Asemantic%2520comprehension%2520capability%252C%2520which%2520have%2520revolutionized%2520various%2520tasks%2520in%250Anatural%2520language%2520processing.%2520Despite%2520their%2520success%252C%2520a%2520critical%2520gap%2520remains%2520in%250Aenabling%2520LLMs%2520to%2520perform%2520knowledge%2520graph%2520completion%2520%2528KGC%2529.%2520Empirical%2520evidence%250Asuggests%2520that%2520LLMs%2520consistently%2520perform%2520worse%2520than%2520conventional%2520KGC%2520approaches%252C%250Aeven%2520through%2520sophisticated%2520prompt%2520design%2520or%2520tailored%2520instruction-tuning.%250AFundamentally%252C%2520applying%2520LLMs%2520on%2520KGC%2520introduces%2520several%2520critical%2520challenges%252C%250Aincluding%2520a%2520vast%2520set%2520of%2520entity%2520candidates%252C%2520hallucination%2520issue%2520of%2520LLMs%252C%2520and%250Aunder-exploitation%2520of%2520the%2520graph%2520structure.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520a%2520novel%2520instruction-tuning-based%2520method%252C%2520namely%2520FtG.%2520Specifically%252C%2520we%250Apresent%2520a%2520%255Ctextit%257Bfilter-then-generate%257D%2520paradigm%2520and%2520formulate%2520the%2520KGC%2520task%250Ainto%2520a%2520multiple-choice%2520question%2520format.%2520In%2520this%2520way%252C%2520we%2520can%2520harness%2520the%250Acapability%2520of%2520LLMs%2520while%2520mitigating%2520the%2520issue%2520casused%2520by%2520hallucinations.%250AMoreover%252C%2520we%2520devise%2520a%2520flexible%2520ego-graph%2520serialization%2520prompt%2520and%2520employ%2520a%250Astructure-text%2520adapter%2520to%2520couple%2520structure%2520and%2520text%2520information%2520in%2520a%250Acontextualized%2520manner.%2520Experimental%2520results%2520demonstrate%2520that%2520FtG%2520achieves%250Asubstantial%2520performance%2520gain%2520compared%2520to%2520existing%2520state-of-the-art%2520methods.%2520The%250Ainstruction%2520dataset%2520and%2520code%2520are%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/LB0828/FtG%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09094v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Filter-then-Generate%3A%20Large%20Language%20Models%20with%20Structure-Text%20Adapter%0A%20%20for%20Knowledge%20Graph%20Completion&entry.906535625=Ben%20Liu%20and%20Jihai%20Zhang%20and%20Fangquan%20Lin%20and%20Cheng%20Yang%20and%20Min%20Peng&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20present%20massive%20inherent%20knowledge%20and%20superior%0Asemantic%20comprehension%20capability%2C%20which%20have%20revolutionized%20various%20tasks%20in%0Anatural%20language%20processing.%20Despite%20their%20success%2C%20a%20critical%20gap%20remains%20in%0Aenabling%20LLMs%20to%20perform%20knowledge%20graph%20completion%20%28KGC%29.%20Empirical%20evidence%0Asuggests%20that%20LLMs%20consistently%20perform%20worse%20than%20conventional%20KGC%20approaches%2C%0Aeven%20through%20sophisticated%20prompt%20design%20or%20tailored%20instruction-tuning.%0AFundamentally%2C%20applying%20LLMs%20on%20KGC%20introduces%20several%20critical%20challenges%2C%0Aincluding%20a%20vast%20set%20of%20entity%20candidates%2C%20hallucination%20issue%20of%20LLMs%2C%20and%0Aunder-exploitation%20of%20the%20graph%20structure.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20novel%20instruction-tuning-based%20method%2C%20namely%20FtG.%20Specifically%2C%20we%0Apresent%20a%20%5Ctextit%7Bfilter-then-generate%7D%20paradigm%20and%20formulate%20the%20KGC%20task%0Ainto%20a%20multiple-choice%20question%20format.%20In%20this%20way%2C%20we%20can%20harness%20the%0Acapability%20of%20LLMs%20while%20mitigating%20the%20issue%20casused%20by%20hallucinations.%0AMoreover%2C%20we%20devise%20a%20flexible%20ego-graph%20serialization%20prompt%20and%20employ%20a%0Astructure-text%20adapter%20to%20couple%20structure%20and%20text%20information%20in%20a%0Acontextualized%20manner.%20Experimental%20results%20demonstrate%20that%20FtG%20achieves%0Asubstantial%20performance%20gain%20compared%20to%20existing%20state-of-the-art%20methods.%20The%0Ainstruction%20dataset%20and%20code%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/LB0828/FtG%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09094v2&entry.124074799=Read"},
{"title": "Solving the Catastrophic Forgetting Problem in Generalized Category\n  Discovery", "author": "Xinzi Cao and Xiawu Zheng and Guanhong Wang and Weijiang Yu and Yunhang Shen and Ke Li and Yutong Lu and Yonghong Tian", "abstract": "  Generalized Category Discovery (GCD) aims to identify a mix of known and\nnovel categories within unlabeled data sets, providing a more realistic setting\nfor image recognition. Essentially, GCD needs to remember existing patterns\nthoroughly to recognize novel categories. Recent state-of-the-art method SimGCD\ntransfers the knowledge from known-class data to the learning of novel classes\nthrough debiased learning. However, some patterns are catastrophically forgot\nduring adaptation and thus lead to poor performance in novel categories\nclassification. To address this issue, we propose a novel learning approach,\nLegoGCD, which is seamlessly integrated into previous methods to enhance the\ndiscrimination of novel classes while maintaining performance on previously\nencountered known classes. Specifically, we design two types of techniques\ntermed as Local Entropy Regularization (LER) and Dual-views Kullback Leibler\ndivergence constraint (DKL). The LER optimizes the distribution of potential\nknown class samples in unlabeled data, thus ensuring the preservation of\nknowledge related to known categories while learning novel classes. Meanwhile,\nDKL introduces Kullback Leibler divergence to encourage the model to produce a\nsimilar prediction distribution of two view samples from the same image. In\nthis way, it successfully avoids mismatched prediction and generates more\nreliable potential known class samples simultaneously. Extensive experiments\nvalidate that the proposed LegoGCD effectively addresses the known category\nforgetting issue across all datasets, eg, delivering a 7.74% and 2.51% accuracy\nboost on known and novel classes in CUB, respectively. Our code is available\nat: https://github.com/Cliffia123/LegoGCD.\n", "link": "http://arxiv.org/abs/2501.05272v1", "date": "2025-01-09", "relevancy": 2.5918, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5326}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5158}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solving%20the%20Catastrophic%20Forgetting%20Problem%20in%20Generalized%20Category%0A%20%20Discovery&body=Title%3A%20Solving%20the%20Catastrophic%20Forgetting%20Problem%20in%20Generalized%20Category%0A%20%20Discovery%0AAuthor%3A%20Xinzi%20Cao%20and%20Xiawu%20Zheng%20and%20Guanhong%20Wang%20and%20Weijiang%20Yu%20and%20Yunhang%20Shen%20and%20Ke%20Li%20and%20Yutong%20Lu%20and%20Yonghong%20Tian%0AAbstract%3A%20%20%20Generalized%20Category%20Discovery%20%28GCD%29%20aims%20to%20identify%20a%20mix%20of%20known%20and%0Anovel%20categories%20within%20unlabeled%20data%20sets%2C%20providing%20a%20more%20realistic%20setting%0Afor%20image%20recognition.%20Essentially%2C%20GCD%20needs%20to%20remember%20existing%20patterns%0Athoroughly%20to%20recognize%20novel%20categories.%20Recent%20state-of-the-art%20method%20SimGCD%0Atransfers%20the%20knowledge%20from%20known-class%20data%20to%20the%20learning%20of%20novel%20classes%0Athrough%20debiased%20learning.%20However%2C%20some%20patterns%20are%20catastrophically%20forgot%0Aduring%20adaptation%20and%20thus%20lead%20to%20poor%20performance%20in%20novel%20categories%0Aclassification.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20learning%20approach%2C%0ALegoGCD%2C%20which%20is%20seamlessly%20integrated%20into%20previous%20methods%20to%20enhance%20the%0Adiscrimination%20of%20novel%20classes%20while%20maintaining%20performance%20on%20previously%0Aencountered%20known%20classes.%20Specifically%2C%20we%20design%20two%20types%20of%20techniques%0Atermed%20as%20Local%20Entropy%20Regularization%20%28LER%29%20and%20Dual-views%20Kullback%20Leibler%0Adivergence%20constraint%20%28DKL%29.%20The%20LER%20optimizes%20the%20distribution%20of%20potential%0Aknown%20class%20samples%20in%20unlabeled%20data%2C%20thus%20ensuring%20the%20preservation%20of%0Aknowledge%20related%20to%20known%20categories%20while%20learning%20novel%20classes.%20Meanwhile%2C%0ADKL%20introduces%20Kullback%20Leibler%20divergence%20to%20encourage%20the%20model%20to%20produce%20a%0Asimilar%20prediction%20distribution%20of%20two%20view%20samples%20from%20the%20same%20image.%20In%0Athis%20way%2C%20it%20successfully%20avoids%20mismatched%20prediction%20and%20generates%20more%0Areliable%20potential%20known%20class%20samples%20simultaneously.%20Extensive%20experiments%0Avalidate%20that%20the%20proposed%20LegoGCD%20effectively%20addresses%20the%20known%20category%0Aforgetting%20issue%20across%20all%20datasets%2C%20eg%2C%20delivering%20a%207.74%25%20and%202.51%25%20accuracy%0Aboost%20on%20known%20and%20novel%20classes%20in%20CUB%2C%20respectively.%20Our%20code%20is%20available%0Aat%3A%20https%3A//github.com/Cliffia123/LegoGCD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05272v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolving%2520the%2520Catastrophic%2520Forgetting%2520Problem%2520in%2520Generalized%2520Category%250A%2520%2520Discovery%26entry.906535625%3DXinzi%2520Cao%2520and%2520Xiawu%2520Zheng%2520and%2520Guanhong%2520Wang%2520and%2520Weijiang%2520Yu%2520and%2520Yunhang%2520Shen%2520and%2520Ke%2520Li%2520and%2520Yutong%2520Lu%2520and%2520Yonghong%2520Tian%26entry.1292438233%3D%2520%2520Generalized%2520Category%2520Discovery%2520%2528GCD%2529%2520aims%2520to%2520identify%2520a%2520mix%2520of%2520known%2520and%250Anovel%2520categories%2520within%2520unlabeled%2520data%2520sets%252C%2520providing%2520a%2520more%2520realistic%2520setting%250Afor%2520image%2520recognition.%2520Essentially%252C%2520GCD%2520needs%2520to%2520remember%2520existing%2520patterns%250Athoroughly%2520to%2520recognize%2520novel%2520categories.%2520Recent%2520state-of-the-art%2520method%2520SimGCD%250Atransfers%2520the%2520knowledge%2520from%2520known-class%2520data%2520to%2520the%2520learning%2520of%2520novel%2520classes%250Athrough%2520debiased%2520learning.%2520However%252C%2520some%2520patterns%2520are%2520catastrophically%2520forgot%250Aduring%2520adaptation%2520and%2520thus%2520lead%2520to%2520poor%2520performance%2520in%2520novel%2520categories%250Aclassification.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520learning%2520approach%252C%250ALegoGCD%252C%2520which%2520is%2520seamlessly%2520integrated%2520into%2520previous%2520methods%2520to%2520enhance%2520the%250Adiscrimination%2520of%2520novel%2520classes%2520while%2520maintaining%2520performance%2520on%2520previously%250Aencountered%2520known%2520classes.%2520Specifically%252C%2520we%2520design%2520two%2520types%2520of%2520techniques%250Atermed%2520as%2520Local%2520Entropy%2520Regularization%2520%2528LER%2529%2520and%2520Dual-views%2520Kullback%2520Leibler%250Adivergence%2520constraint%2520%2528DKL%2529.%2520The%2520LER%2520optimizes%2520the%2520distribution%2520of%2520potential%250Aknown%2520class%2520samples%2520in%2520unlabeled%2520data%252C%2520thus%2520ensuring%2520the%2520preservation%2520of%250Aknowledge%2520related%2520to%2520known%2520categories%2520while%2520learning%2520novel%2520classes.%2520Meanwhile%252C%250ADKL%2520introduces%2520Kullback%2520Leibler%2520divergence%2520to%2520encourage%2520the%2520model%2520to%2520produce%2520a%250Asimilar%2520prediction%2520distribution%2520of%2520two%2520view%2520samples%2520from%2520the%2520same%2520image.%2520In%250Athis%2520way%252C%2520it%2520successfully%2520avoids%2520mismatched%2520prediction%2520and%2520generates%2520more%250Areliable%2520potential%2520known%2520class%2520samples%2520simultaneously.%2520Extensive%2520experiments%250Avalidate%2520that%2520the%2520proposed%2520LegoGCD%2520effectively%2520addresses%2520the%2520known%2520category%250Aforgetting%2520issue%2520across%2520all%2520datasets%252C%2520eg%252C%2520delivering%2520a%25207.74%2525%2520and%25202.51%2525%2520accuracy%250Aboost%2520on%2520known%2520and%2520novel%2520classes%2520in%2520CUB%252C%2520respectively.%2520Our%2520code%2520is%2520available%250Aat%253A%2520https%253A//github.com/Cliffia123/LegoGCD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05272v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20the%20Catastrophic%20Forgetting%20Problem%20in%20Generalized%20Category%0A%20%20Discovery&entry.906535625=Xinzi%20Cao%20and%20Xiawu%20Zheng%20and%20Guanhong%20Wang%20and%20Weijiang%20Yu%20and%20Yunhang%20Shen%20and%20Ke%20Li%20and%20Yutong%20Lu%20and%20Yonghong%20Tian&entry.1292438233=%20%20Generalized%20Category%20Discovery%20%28GCD%29%20aims%20to%20identify%20a%20mix%20of%20known%20and%0Anovel%20categories%20within%20unlabeled%20data%20sets%2C%20providing%20a%20more%20realistic%20setting%0Afor%20image%20recognition.%20Essentially%2C%20GCD%20needs%20to%20remember%20existing%20patterns%0Athoroughly%20to%20recognize%20novel%20categories.%20Recent%20state-of-the-art%20method%20SimGCD%0Atransfers%20the%20knowledge%20from%20known-class%20data%20to%20the%20learning%20of%20novel%20classes%0Athrough%20debiased%20learning.%20However%2C%20some%20patterns%20are%20catastrophically%20forgot%0Aduring%20adaptation%20and%20thus%20lead%20to%20poor%20performance%20in%20novel%20categories%0Aclassification.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20learning%20approach%2C%0ALegoGCD%2C%20which%20is%20seamlessly%20integrated%20into%20previous%20methods%20to%20enhance%20the%0Adiscrimination%20of%20novel%20classes%20while%20maintaining%20performance%20on%20previously%0Aencountered%20known%20classes.%20Specifically%2C%20we%20design%20two%20types%20of%20techniques%0Atermed%20as%20Local%20Entropy%20Regularization%20%28LER%29%20and%20Dual-views%20Kullback%20Leibler%0Adivergence%20constraint%20%28DKL%29.%20The%20LER%20optimizes%20the%20distribution%20of%20potential%0Aknown%20class%20samples%20in%20unlabeled%20data%2C%20thus%20ensuring%20the%20preservation%20of%0Aknowledge%20related%20to%20known%20categories%20while%20learning%20novel%20classes.%20Meanwhile%2C%0ADKL%20introduces%20Kullback%20Leibler%20divergence%20to%20encourage%20the%20model%20to%20produce%20a%0Asimilar%20prediction%20distribution%20of%20two%20view%20samples%20from%20the%20same%20image.%20In%0Athis%20way%2C%20it%20successfully%20avoids%20mismatched%20prediction%20and%20generates%20more%0Areliable%20potential%20known%20class%20samples%20simultaneously.%20Extensive%20experiments%0Avalidate%20that%20the%20proposed%20LegoGCD%20effectively%20addresses%20the%20known%20category%0Aforgetting%20issue%20across%20all%20datasets%2C%20eg%2C%20delivering%20a%207.74%25%20and%202.51%25%20accuracy%0Aboost%20on%20known%20and%20novel%20classes%20in%20CUB%2C%20respectively.%20Our%20code%20is%20available%0Aat%3A%20https%3A//github.com/Cliffia123/LegoGCD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05272v1&entry.124074799=Read"},
{"title": "HipyrNet: Hypernet-Guided Feature Pyramid network for mixed-exposure\n  correction", "author": "Shaurya Singh Rathore and Aravind Shenoy and Krish Didwania and Aditya Kasliwal and Ujjwal Verma", "abstract": "  Recent advancements in image translation for enhancing mixed-exposure images\nhave demonstrated the transformative potential of deep learning algorithms.\nHowever, addressing extreme exposure variations in images remains a significant\nchallenge due to the inherent complexity and contrast inconsistencies across\nregions. Current methods often struggle to adapt effectively to these\nvariations, resulting in suboptimal performance. In this work, we propose\nHipyrNet, a novel approach that integrates a HyperNetwork within a Laplacian\nPyramid-based framework to tackle the challenges of mixed-exposure image\nenhancement. The inclusion of a HyperNetwork allows the model to adapt to these\nexposure variations. HyperNetworks dynamically generates weights for another\nnetwork, allowing dynamic changes during deployment. In our model, the\nHyperNetwork employed is used to predict optimal kernels for Feature Pyramid\ndecomposition, which enables a tailored and adaptive decomposition process for\neach input image. Our enhanced translational network incorporates multiscale\ndecomposition and reconstruction, leveraging dynamic kernel prediction to\ncapture and manipulate features across varying scales. Extensive experiments\ndemonstrate that HipyrNet outperforms existing methods, particularly in\nscenarios with extreme exposure variations, achieving superior results in both\nqualitative and quantitative evaluations. Our approach sets a new benchmark for\nmixed-exposure image enhancement, paving the way for future research in\nadaptive image translation.\n", "link": "http://arxiv.org/abs/2501.05195v1", "date": "2025-01-09", "relevancy": 2.5836, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5333}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5158}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HipyrNet%3A%20Hypernet-Guided%20Feature%20Pyramid%20network%20for%20mixed-exposure%0A%20%20correction&body=Title%3A%20HipyrNet%3A%20Hypernet-Guided%20Feature%20Pyramid%20network%20for%20mixed-exposure%0A%20%20correction%0AAuthor%3A%20Shaurya%20Singh%20Rathore%20and%20Aravind%20Shenoy%20and%20Krish%20Didwania%20and%20Aditya%20Kasliwal%20and%20Ujjwal%20Verma%0AAbstract%3A%20%20%20Recent%20advancements%20in%20image%20translation%20for%20enhancing%20mixed-exposure%20images%0Ahave%20demonstrated%20the%20transformative%20potential%20of%20deep%20learning%20algorithms.%0AHowever%2C%20addressing%20extreme%20exposure%20variations%20in%20images%20remains%20a%20significant%0Achallenge%20due%20to%20the%20inherent%20complexity%20and%20contrast%20inconsistencies%20across%0Aregions.%20Current%20methods%20often%20struggle%20to%20adapt%20effectively%20to%20these%0Avariations%2C%20resulting%20in%20suboptimal%20performance.%20In%20this%20work%2C%20we%20propose%0AHipyrNet%2C%20a%20novel%20approach%20that%20integrates%20a%20HyperNetwork%20within%20a%20Laplacian%0APyramid-based%20framework%20to%20tackle%20the%20challenges%20of%20mixed-exposure%20image%0Aenhancement.%20The%20inclusion%20of%20a%20HyperNetwork%20allows%20the%20model%20to%20adapt%20to%20these%0Aexposure%20variations.%20HyperNetworks%20dynamically%20generates%20weights%20for%20another%0Anetwork%2C%20allowing%20dynamic%20changes%20during%20deployment.%20In%20our%20model%2C%20the%0AHyperNetwork%20employed%20is%20used%20to%20predict%20optimal%20kernels%20for%20Feature%20Pyramid%0Adecomposition%2C%20which%20enables%20a%20tailored%20and%20adaptive%20decomposition%20process%20for%0Aeach%20input%20image.%20Our%20enhanced%20translational%20network%20incorporates%20multiscale%0Adecomposition%20and%20reconstruction%2C%20leveraging%20dynamic%20kernel%20prediction%20to%0Acapture%20and%20manipulate%20features%20across%20varying%20scales.%20Extensive%20experiments%0Ademonstrate%20that%20HipyrNet%20outperforms%20existing%20methods%2C%20particularly%20in%0Ascenarios%20with%20extreme%20exposure%20variations%2C%20achieving%20superior%20results%20in%20both%0Aqualitative%20and%20quantitative%20evaluations.%20Our%20approach%20sets%20a%20new%20benchmark%20for%0Amixed-exposure%20image%20enhancement%2C%20paving%20the%20way%20for%20future%20research%20in%0Aadaptive%20image%20translation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05195v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHipyrNet%253A%2520Hypernet-Guided%2520Feature%2520Pyramid%2520network%2520for%2520mixed-exposure%250A%2520%2520correction%26entry.906535625%3DShaurya%2520Singh%2520Rathore%2520and%2520Aravind%2520Shenoy%2520and%2520Krish%2520Didwania%2520and%2520Aditya%2520Kasliwal%2520and%2520Ujjwal%2520Verma%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520image%2520translation%2520for%2520enhancing%2520mixed-exposure%2520images%250Ahave%2520demonstrated%2520the%2520transformative%2520potential%2520of%2520deep%2520learning%2520algorithms.%250AHowever%252C%2520addressing%2520extreme%2520exposure%2520variations%2520in%2520images%2520remains%2520a%2520significant%250Achallenge%2520due%2520to%2520the%2520inherent%2520complexity%2520and%2520contrast%2520inconsistencies%2520across%250Aregions.%2520Current%2520methods%2520often%2520struggle%2520to%2520adapt%2520effectively%2520to%2520these%250Avariations%252C%2520resulting%2520in%2520suboptimal%2520performance.%2520In%2520this%2520work%252C%2520we%2520propose%250AHipyrNet%252C%2520a%2520novel%2520approach%2520that%2520integrates%2520a%2520HyperNetwork%2520within%2520a%2520Laplacian%250APyramid-based%2520framework%2520to%2520tackle%2520the%2520challenges%2520of%2520mixed-exposure%2520image%250Aenhancement.%2520The%2520inclusion%2520of%2520a%2520HyperNetwork%2520allows%2520the%2520model%2520to%2520adapt%2520to%2520these%250Aexposure%2520variations.%2520HyperNetworks%2520dynamically%2520generates%2520weights%2520for%2520another%250Anetwork%252C%2520allowing%2520dynamic%2520changes%2520during%2520deployment.%2520In%2520our%2520model%252C%2520the%250AHyperNetwork%2520employed%2520is%2520used%2520to%2520predict%2520optimal%2520kernels%2520for%2520Feature%2520Pyramid%250Adecomposition%252C%2520which%2520enables%2520a%2520tailored%2520and%2520adaptive%2520decomposition%2520process%2520for%250Aeach%2520input%2520image.%2520Our%2520enhanced%2520translational%2520network%2520incorporates%2520multiscale%250Adecomposition%2520and%2520reconstruction%252C%2520leveraging%2520dynamic%2520kernel%2520prediction%2520to%250Acapture%2520and%2520manipulate%2520features%2520across%2520varying%2520scales.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520HipyrNet%2520outperforms%2520existing%2520methods%252C%2520particularly%2520in%250Ascenarios%2520with%2520extreme%2520exposure%2520variations%252C%2520achieving%2520superior%2520results%2520in%2520both%250Aqualitative%2520and%2520quantitative%2520evaluations.%2520Our%2520approach%2520sets%2520a%2520new%2520benchmark%2520for%250Amixed-exposure%2520image%2520enhancement%252C%2520paving%2520the%2520way%2520for%2520future%2520research%2520in%250Aadaptive%2520image%2520translation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05195v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HipyrNet%3A%20Hypernet-Guided%20Feature%20Pyramid%20network%20for%20mixed-exposure%0A%20%20correction&entry.906535625=Shaurya%20Singh%20Rathore%20and%20Aravind%20Shenoy%20and%20Krish%20Didwania%20and%20Aditya%20Kasliwal%20and%20Ujjwal%20Verma&entry.1292438233=%20%20Recent%20advancements%20in%20image%20translation%20for%20enhancing%20mixed-exposure%20images%0Ahave%20demonstrated%20the%20transformative%20potential%20of%20deep%20learning%20algorithms.%0AHowever%2C%20addressing%20extreme%20exposure%20variations%20in%20images%20remains%20a%20significant%0Achallenge%20due%20to%20the%20inherent%20complexity%20and%20contrast%20inconsistencies%20across%0Aregions.%20Current%20methods%20often%20struggle%20to%20adapt%20effectively%20to%20these%0Avariations%2C%20resulting%20in%20suboptimal%20performance.%20In%20this%20work%2C%20we%20propose%0AHipyrNet%2C%20a%20novel%20approach%20that%20integrates%20a%20HyperNetwork%20within%20a%20Laplacian%0APyramid-based%20framework%20to%20tackle%20the%20challenges%20of%20mixed-exposure%20image%0Aenhancement.%20The%20inclusion%20of%20a%20HyperNetwork%20allows%20the%20model%20to%20adapt%20to%20these%0Aexposure%20variations.%20HyperNetworks%20dynamically%20generates%20weights%20for%20another%0Anetwork%2C%20allowing%20dynamic%20changes%20during%20deployment.%20In%20our%20model%2C%20the%0AHyperNetwork%20employed%20is%20used%20to%20predict%20optimal%20kernels%20for%20Feature%20Pyramid%0Adecomposition%2C%20which%20enables%20a%20tailored%20and%20adaptive%20decomposition%20process%20for%0Aeach%20input%20image.%20Our%20enhanced%20translational%20network%20incorporates%20multiscale%0Adecomposition%20and%20reconstruction%2C%20leveraging%20dynamic%20kernel%20prediction%20to%0Acapture%20and%20manipulate%20features%20across%20varying%20scales.%20Extensive%20experiments%0Ademonstrate%20that%20HipyrNet%20outperforms%20existing%20methods%2C%20particularly%20in%0Ascenarios%20with%20extreme%20exposure%20variations%2C%20achieving%20superior%20results%20in%20both%0Aqualitative%20and%20quantitative%20evaluations.%20Our%20approach%20sets%20a%20new%20benchmark%20for%0Amixed-exposure%20image%20enhancement%2C%20paving%20the%20way%20for%20future%20research%20in%0Aadaptive%20image%20translation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05195v1&entry.124074799=Read"},
{"title": "3DIS-FLUX: simple and efficient multi-instance generation with DiT\n  rendering", "author": "Dewei Zhou and Ji Xie and Zongxin Yang and Yi Yang", "abstract": "  The growing demand for controllable outputs in text-to-image generation has\ndriven significant advancements in multi-instance generation (MIG), enabling\nusers to define both instance layouts and attributes. Currently, the\nstate-of-the-art methods in MIG are primarily adapter-based. However, these\nmethods necessitate retraining a new adapter each time a more advanced model is\nreleased, resulting in significant resource consumption. A methodology named\nDepth-Driven Decoupled Instance Synthesis (3DIS) has been introduced, which\ndecouples MIG into two distinct phases: 1) depth-based scene construction and\n2) detail rendering with widely pre-trained depth control models. The 3DIS\nmethod requires adapter training solely during the scene construction phase,\nwhile enabling various models to perform training-free detail rendering.\nInitially, 3DIS focused on rendering techniques utilizing U-Net architectures\nsuch as SD1.5, SD2, and SDXL, without exploring the potential of recent\nDiT-based models like FLUX. In this paper, we present 3DIS-FLUX, an extension\nof the 3DIS framework that integrates the FLUX model for enhanced rendering\ncapabilities. Specifically, we employ the FLUX.1-Depth-dev model for depth map\ncontrolled image generation and introduce a detail renderer that manipulates\nthe Attention Mask in FLUX's Joint Attention mechanism based on layout\ninformation. This approach allows for the precise rendering of fine-grained\nattributes of each instance. Our experimental results indicate that 3DIS-FLUX,\nleveraging the FLUX model, outperforms the original 3DIS method, which utilized\nSD2 and SDXL, and surpasses current state-of-the-art adapter-based methods in\nterms of both performance and image quality. Project Page:\nhttps://limuloo.github.io/3DIS/.\n", "link": "http://arxiv.org/abs/2501.05131v1", "date": "2025-01-09", "relevancy": 2.5698, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6454}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6419}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DIS-FLUX%3A%20simple%20and%20efficient%20multi-instance%20generation%20with%20DiT%0A%20%20rendering&body=Title%3A%203DIS-FLUX%3A%20simple%20and%20efficient%20multi-instance%20generation%20with%20DiT%0A%20%20rendering%0AAuthor%3A%20Dewei%20Zhou%20and%20Ji%20Xie%20and%20Zongxin%20Yang%20and%20Yi%20Yang%0AAbstract%3A%20%20%20The%20growing%20demand%20for%20controllable%20outputs%20in%20text-to-image%20generation%20has%0Adriven%20significant%20advancements%20in%20multi-instance%20generation%20%28MIG%29%2C%20enabling%0Ausers%20to%20define%20both%20instance%20layouts%20and%20attributes.%20Currently%2C%20the%0Astate-of-the-art%20methods%20in%20MIG%20are%20primarily%20adapter-based.%20However%2C%20these%0Amethods%20necessitate%20retraining%20a%20new%20adapter%20each%20time%20a%20more%20advanced%20model%20is%0Areleased%2C%20resulting%20in%20significant%20resource%20consumption.%20A%20methodology%20named%0ADepth-Driven%20Decoupled%20Instance%20Synthesis%20%283DIS%29%20has%20been%20introduced%2C%20which%0Adecouples%20MIG%20into%20two%20distinct%20phases%3A%201%29%20depth-based%20scene%20construction%20and%0A2%29%20detail%20rendering%20with%20widely%20pre-trained%20depth%20control%20models.%20The%203DIS%0Amethod%20requires%20adapter%20training%20solely%20during%20the%20scene%20construction%20phase%2C%0Awhile%20enabling%20various%20models%20to%20perform%20training-free%20detail%20rendering.%0AInitially%2C%203DIS%20focused%20on%20rendering%20techniques%20utilizing%20U-Net%20architectures%0Asuch%20as%20SD1.5%2C%20SD2%2C%20and%20SDXL%2C%20without%20exploring%20the%20potential%20of%20recent%0ADiT-based%20models%20like%20FLUX.%20In%20this%20paper%2C%20we%20present%203DIS-FLUX%2C%20an%20extension%0Aof%20the%203DIS%20framework%20that%20integrates%20the%20FLUX%20model%20for%20enhanced%20rendering%0Acapabilities.%20Specifically%2C%20we%20employ%20the%20FLUX.1-Depth-dev%20model%20for%20depth%20map%0Acontrolled%20image%20generation%20and%20introduce%20a%20detail%20renderer%20that%20manipulates%0Athe%20Attention%20Mask%20in%20FLUX%27s%20Joint%20Attention%20mechanism%20based%20on%20layout%0Ainformation.%20This%20approach%20allows%20for%20the%20precise%20rendering%20of%20fine-grained%0Aattributes%20of%20each%20instance.%20Our%20experimental%20results%20indicate%20that%203DIS-FLUX%2C%0Aleveraging%20the%20FLUX%20model%2C%20outperforms%20the%20original%203DIS%20method%2C%20which%20utilized%0ASD2%20and%20SDXL%2C%20and%20surpasses%20current%20state-of-the-art%20adapter-based%20methods%20in%0Aterms%20of%20both%20performance%20and%20image%20quality.%20Project%20Page%3A%0Ahttps%3A//limuloo.github.io/3DIS/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05131v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DIS-FLUX%253A%2520simple%2520and%2520efficient%2520multi-instance%2520generation%2520with%2520DiT%250A%2520%2520rendering%26entry.906535625%3DDewei%2520Zhou%2520and%2520Ji%2520Xie%2520and%2520Zongxin%2520Yang%2520and%2520Yi%2520Yang%26entry.1292438233%3D%2520%2520The%2520growing%2520demand%2520for%2520controllable%2520outputs%2520in%2520text-to-image%2520generation%2520has%250Adriven%2520significant%2520advancements%2520in%2520multi-instance%2520generation%2520%2528MIG%2529%252C%2520enabling%250Ausers%2520to%2520define%2520both%2520instance%2520layouts%2520and%2520attributes.%2520Currently%252C%2520the%250Astate-of-the-art%2520methods%2520in%2520MIG%2520are%2520primarily%2520adapter-based.%2520However%252C%2520these%250Amethods%2520necessitate%2520retraining%2520a%2520new%2520adapter%2520each%2520time%2520a%2520more%2520advanced%2520model%2520is%250Areleased%252C%2520resulting%2520in%2520significant%2520resource%2520consumption.%2520A%2520methodology%2520named%250ADepth-Driven%2520Decoupled%2520Instance%2520Synthesis%2520%25283DIS%2529%2520has%2520been%2520introduced%252C%2520which%250Adecouples%2520MIG%2520into%2520two%2520distinct%2520phases%253A%25201%2529%2520depth-based%2520scene%2520construction%2520and%250A2%2529%2520detail%2520rendering%2520with%2520widely%2520pre-trained%2520depth%2520control%2520models.%2520The%25203DIS%250Amethod%2520requires%2520adapter%2520training%2520solely%2520during%2520the%2520scene%2520construction%2520phase%252C%250Awhile%2520enabling%2520various%2520models%2520to%2520perform%2520training-free%2520detail%2520rendering.%250AInitially%252C%25203DIS%2520focused%2520on%2520rendering%2520techniques%2520utilizing%2520U-Net%2520architectures%250Asuch%2520as%2520SD1.5%252C%2520SD2%252C%2520and%2520SDXL%252C%2520without%2520exploring%2520the%2520potential%2520of%2520recent%250ADiT-based%2520models%2520like%2520FLUX.%2520In%2520this%2520paper%252C%2520we%2520present%25203DIS-FLUX%252C%2520an%2520extension%250Aof%2520the%25203DIS%2520framework%2520that%2520integrates%2520the%2520FLUX%2520model%2520for%2520enhanced%2520rendering%250Acapabilities.%2520Specifically%252C%2520we%2520employ%2520the%2520FLUX.1-Depth-dev%2520model%2520for%2520depth%2520map%250Acontrolled%2520image%2520generation%2520and%2520introduce%2520a%2520detail%2520renderer%2520that%2520manipulates%250Athe%2520Attention%2520Mask%2520in%2520FLUX%2527s%2520Joint%2520Attention%2520mechanism%2520based%2520on%2520layout%250Ainformation.%2520This%2520approach%2520allows%2520for%2520the%2520precise%2520rendering%2520of%2520fine-grained%250Aattributes%2520of%2520each%2520instance.%2520Our%2520experimental%2520results%2520indicate%2520that%25203DIS-FLUX%252C%250Aleveraging%2520the%2520FLUX%2520model%252C%2520outperforms%2520the%2520original%25203DIS%2520method%252C%2520which%2520utilized%250ASD2%2520and%2520SDXL%252C%2520and%2520surpasses%2520current%2520state-of-the-art%2520adapter-based%2520methods%2520in%250Aterms%2520of%2520both%2520performance%2520and%2520image%2520quality.%2520Project%2520Page%253A%250Ahttps%253A//limuloo.github.io/3DIS/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05131v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DIS-FLUX%3A%20simple%20and%20efficient%20multi-instance%20generation%20with%20DiT%0A%20%20rendering&entry.906535625=Dewei%20Zhou%20and%20Ji%20Xie%20and%20Zongxin%20Yang%20and%20Yi%20Yang&entry.1292438233=%20%20The%20growing%20demand%20for%20controllable%20outputs%20in%20text-to-image%20generation%20has%0Adriven%20significant%20advancements%20in%20multi-instance%20generation%20%28MIG%29%2C%20enabling%0Ausers%20to%20define%20both%20instance%20layouts%20and%20attributes.%20Currently%2C%20the%0Astate-of-the-art%20methods%20in%20MIG%20are%20primarily%20adapter-based.%20However%2C%20these%0Amethods%20necessitate%20retraining%20a%20new%20adapter%20each%20time%20a%20more%20advanced%20model%20is%0Areleased%2C%20resulting%20in%20significant%20resource%20consumption.%20A%20methodology%20named%0ADepth-Driven%20Decoupled%20Instance%20Synthesis%20%283DIS%29%20has%20been%20introduced%2C%20which%0Adecouples%20MIG%20into%20two%20distinct%20phases%3A%201%29%20depth-based%20scene%20construction%20and%0A2%29%20detail%20rendering%20with%20widely%20pre-trained%20depth%20control%20models.%20The%203DIS%0Amethod%20requires%20adapter%20training%20solely%20during%20the%20scene%20construction%20phase%2C%0Awhile%20enabling%20various%20models%20to%20perform%20training-free%20detail%20rendering.%0AInitially%2C%203DIS%20focused%20on%20rendering%20techniques%20utilizing%20U-Net%20architectures%0Asuch%20as%20SD1.5%2C%20SD2%2C%20and%20SDXL%2C%20without%20exploring%20the%20potential%20of%20recent%0ADiT-based%20models%20like%20FLUX.%20In%20this%20paper%2C%20we%20present%203DIS-FLUX%2C%20an%20extension%0Aof%20the%203DIS%20framework%20that%20integrates%20the%20FLUX%20model%20for%20enhanced%20rendering%0Acapabilities.%20Specifically%2C%20we%20employ%20the%20FLUX.1-Depth-dev%20model%20for%20depth%20map%0Acontrolled%20image%20generation%20and%20introduce%20a%20detail%20renderer%20that%20manipulates%0Athe%20Attention%20Mask%20in%20FLUX%27s%20Joint%20Attention%20mechanism%20based%20on%20layout%0Ainformation.%20This%20approach%20allows%20for%20the%20precise%20rendering%20of%20fine-grained%0Aattributes%20of%20each%20instance.%20Our%20experimental%20results%20indicate%20that%203DIS-FLUX%2C%0Aleveraging%20the%20FLUX%20model%2C%20outperforms%20the%20original%203DIS%20method%2C%20which%20utilized%0ASD2%20and%20SDXL%2C%20and%20surpasses%20current%20state-of-the-art%20adapter-based%20methods%20in%0Aterms%20of%20both%20performance%20and%20image%20quality.%20Project%20Page%3A%0Ahttps%3A//limuloo.github.io/3DIS/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05131v1&entry.124074799=Read"},
{"title": "A Novel Approach to Scalable and Automatic Topic-Controlled Question\n  Generation in Education", "author": "Ziqing Li and Mutlu Cukurova and Sahan Bulathwela", "abstract": "  The development of Automatic Question Generation (QG) models has the\npotential to significantly improve educational practices by reducing the\nteacher workload associated with creating educational content. This paper\nintroduces a novel approach to educational question generation that controls\nthe topical focus of questions. The proposed Topic-Controlled Question\nGeneration (T-CQG) method enhances the relevance and effectiveness of the\ngenerated content for educational purposes. Our approach uses fine-tuning on a\npre-trained T5-small model, employing specially created datasets tailored to\neducational needs. The research further explores the impacts of pre-training\nstrategies, quantisation, and data augmentation on the model's performance. We\nspecifically address the challenge of generating semantically aligned questions\nwith paragraph-level contexts, thereby improving the topic specificity of the\ngenerated questions. In addition, we introduce and explore novel evaluation\nmethods to assess the topical relatedness of the generated questions. Our\nresults, validated through rigorous offline and human-backed evaluations,\ndemonstrate that the proposed models effectively generate high-quality,\ntopic-focused questions. These models have the potential to reduce teacher\nworkload and support personalised tutoring systems by serving as bespoke\nquestion generators. With its relatively small number of parameters, the\nproposals not only advance the capabilities of question generation models for\nhandling specific educational topics but also offer a scalable solution that\nreduces infrastructure costs. This scalability makes them feasible for\nwidespread use in education without reliance on proprietary large language\nmodels like ChatGPT.\n", "link": "http://arxiv.org/abs/2501.05220v1", "date": "2025-01-09", "relevancy": 2.5617, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5268}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5064}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Approach%20to%20Scalable%20and%20Automatic%20Topic-Controlled%20Question%0A%20%20Generation%20in%20Education&body=Title%3A%20A%20Novel%20Approach%20to%20Scalable%20and%20Automatic%20Topic-Controlled%20Question%0A%20%20Generation%20in%20Education%0AAuthor%3A%20Ziqing%20Li%20and%20Mutlu%20Cukurova%20and%20Sahan%20Bulathwela%0AAbstract%3A%20%20%20The%20development%20of%20Automatic%20Question%20Generation%20%28QG%29%20models%20has%20the%0Apotential%20to%20significantly%20improve%20educational%20practices%20by%20reducing%20the%0Ateacher%20workload%20associated%20with%20creating%20educational%20content.%20This%20paper%0Aintroduces%20a%20novel%20approach%20to%20educational%20question%20generation%20that%20controls%0Athe%20topical%20focus%20of%20questions.%20The%20proposed%20Topic-Controlled%20Question%0AGeneration%20%28T-CQG%29%20method%20enhances%20the%20relevance%20and%20effectiveness%20of%20the%0Agenerated%20content%20for%20educational%20purposes.%20Our%20approach%20uses%20fine-tuning%20on%20a%0Apre-trained%20T5-small%20model%2C%20employing%20specially%20created%20datasets%20tailored%20to%0Aeducational%20needs.%20The%20research%20further%20explores%20the%20impacts%20of%20pre-training%0Astrategies%2C%20quantisation%2C%20and%20data%20augmentation%20on%20the%20model%27s%20performance.%20We%0Aspecifically%20address%20the%20challenge%20of%20generating%20semantically%20aligned%20questions%0Awith%20paragraph-level%20contexts%2C%20thereby%20improving%20the%20topic%20specificity%20of%20the%0Agenerated%20questions.%20In%20addition%2C%20we%20introduce%20and%20explore%20novel%20evaluation%0Amethods%20to%20assess%20the%20topical%20relatedness%20of%20the%20generated%20questions.%20Our%0Aresults%2C%20validated%20through%20rigorous%20offline%20and%20human-backed%20evaluations%2C%0Ademonstrate%20that%20the%20proposed%20models%20effectively%20generate%20high-quality%2C%0Atopic-focused%20questions.%20These%20models%20have%20the%20potential%20to%20reduce%20teacher%0Aworkload%20and%20support%20personalised%20tutoring%20systems%20by%20serving%20as%20bespoke%0Aquestion%20generators.%20With%20its%20relatively%20small%20number%20of%20parameters%2C%20the%0Aproposals%20not%20only%20advance%20the%20capabilities%20of%20question%20generation%20models%20for%0Ahandling%20specific%20educational%20topics%20but%20also%20offer%20a%20scalable%20solution%20that%0Areduces%20infrastructure%20costs.%20This%20scalability%20makes%20them%20feasible%20for%0Awidespread%20use%20in%20education%20without%20reliance%20on%20proprietary%20large%20language%0Amodels%20like%20ChatGPT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05220v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Approach%2520to%2520Scalable%2520and%2520Automatic%2520Topic-Controlled%2520Question%250A%2520%2520Generation%2520in%2520Education%26entry.906535625%3DZiqing%2520Li%2520and%2520Mutlu%2520Cukurova%2520and%2520Sahan%2520Bulathwela%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520Automatic%2520Question%2520Generation%2520%2528QG%2529%2520models%2520has%2520the%250Apotential%2520to%2520significantly%2520improve%2520educational%2520practices%2520by%2520reducing%2520the%250Ateacher%2520workload%2520associated%2520with%2520creating%2520educational%2520content.%2520This%2520paper%250Aintroduces%2520a%2520novel%2520approach%2520to%2520educational%2520question%2520generation%2520that%2520controls%250Athe%2520topical%2520focus%2520of%2520questions.%2520The%2520proposed%2520Topic-Controlled%2520Question%250AGeneration%2520%2528T-CQG%2529%2520method%2520enhances%2520the%2520relevance%2520and%2520effectiveness%2520of%2520the%250Agenerated%2520content%2520for%2520educational%2520purposes.%2520Our%2520approach%2520uses%2520fine-tuning%2520on%2520a%250Apre-trained%2520T5-small%2520model%252C%2520employing%2520specially%2520created%2520datasets%2520tailored%2520to%250Aeducational%2520needs.%2520The%2520research%2520further%2520explores%2520the%2520impacts%2520of%2520pre-training%250Astrategies%252C%2520quantisation%252C%2520and%2520data%2520augmentation%2520on%2520the%2520model%2527s%2520performance.%2520We%250Aspecifically%2520address%2520the%2520challenge%2520of%2520generating%2520semantically%2520aligned%2520questions%250Awith%2520paragraph-level%2520contexts%252C%2520thereby%2520improving%2520the%2520topic%2520specificity%2520of%2520the%250Agenerated%2520questions.%2520In%2520addition%252C%2520we%2520introduce%2520and%2520explore%2520novel%2520evaluation%250Amethods%2520to%2520assess%2520the%2520topical%2520relatedness%2520of%2520the%2520generated%2520questions.%2520Our%250Aresults%252C%2520validated%2520through%2520rigorous%2520offline%2520and%2520human-backed%2520evaluations%252C%250Ademonstrate%2520that%2520the%2520proposed%2520models%2520effectively%2520generate%2520high-quality%252C%250Atopic-focused%2520questions.%2520These%2520models%2520have%2520the%2520potential%2520to%2520reduce%2520teacher%250Aworkload%2520and%2520support%2520personalised%2520tutoring%2520systems%2520by%2520serving%2520as%2520bespoke%250Aquestion%2520generators.%2520With%2520its%2520relatively%2520small%2520number%2520of%2520parameters%252C%2520the%250Aproposals%2520not%2520only%2520advance%2520the%2520capabilities%2520of%2520question%2520generation%2520models%2520for%250Ahandling%2520specific%2520educational%2520topics%2520but%2520also%2520offer%2520a%2520scalable%2520solution%2520that%250Areduces%2520infrastructure%2520costs.%2520This%2520scalability%2520makes%2520them%2520feasible%2520for%250Awidespread%2520use%2520in%2520education%2520without%2520reliance%2520on%2520proprietary%2520large%2520language%250Amodels%2520like%2520ChatGPT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05220v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Approach%20to%20Scalable%20and%20Automatic%20Topic-Controlled%20Question%0A%20%20Generation%20in%20Education&entry.906535625=Ziqing%20Li%20and%20Mutlu%20Cukurova%20and%20Sahan%20Bulathwela&entry.1292438233=%20%20The%20development%20of%20Automatic%20Question%20Generation%20%28QG%29%20models%20has%20the%0Apotential%20to%20significantly%20improve%20educational%20practices%20by%20reducing%20the%0Ateacher%20workload%20associated%20with%20creating%20educational%20content.%20This%20paper%0Aintroduces%20a%20novel%20approach%20to%20educational%20question%20generation%20that%20controls%0Athe%20topical%20focus%20of%20questions.%20The%20proposed%20Topic-Controlled%20Question%0AGeneration%20%28T-CQG%29%20method%20enhances%20the%20relevance%20and%20effectiveness%20of%20the%0Agenerated%20content%20for%20educational%20purposes.%20Our%20approach%20uses%20fine-tuning%20on%20a%0Apre-trained%20T5-small%20model%2C%20employing%20specially%20created%20datasets%20tailored%20to%0Aeducational%20needs.%20The%20research%20further%20explores%20the%20impacts%20of%20pre-training%0Astrategies%2C%20quantisation%2C%20and%20data%20augmentation%20on%20the%20model%27s%20performance.%20We%0Aspecifically%20address%20the%20challenge%20of%20generating%20semantically%20aligned%20questions%0Awith%20paragraph-level%20contexts%2C%20thereby%20improving%20the%20topic%20specificity%20of%20the%0Agenerated%20questions.%20In%20addition%2C%20we%20introduce%20and%20explore%20novel%20evaluation%0Amethods%20to%20assess%20the%20topical%20relatedness%20of%20the%20generated%20questions.%20Our%0Aresults%2C%20validated%20through%20rigorous%20offline%20and%20human-backed%20evaluations%2C%0Ademonstrate%20that%20the%20proposed%20models%20effectively%20generate%20high-quality%2C%0Atopic-focused%20questions.%20These%20models%20have%20the%20potential%20to%20reduce%20teacher%0Aworkload%20and%20support%20personalised%20tutoring%20systems%20by%20serving%20as%20bespoke%0Aquestion%20generators.%20With%20its%20relatively%20small%20number%20of%20parameters%2C%20the%0Aproposals%20not%20only%20advance%20the%20capabilities%20of%20question%20generation%20models%20for%0Ahandling%20specific%20educational%20topics%20but%20also%20offer%20a%20scalable%20solution%20that%0Areduces%20infrastructure%20costs.%20This%20scalability%20makes%20them%20feasible%20for%0Awidespread%20use%20in%20education%20without%20reliance%20on%20proprietary%20large%20language%0Amodels%20like%20ChatGPT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05220v1&entry.124074799=Read"},
{"title": "Multi-class Decoding of Attended Speaker Direction Using\n  Electroencephalogram and Audio Spatial Spectrum", "author": "Yuanming Zhang and Jing Lu and Fei Chen and Haoliang Du and Xia Gao and Zhibin Lin", "abstract": "  Decoding the directional focus of an attended speaker from listeners'\nelectroencephalogram (EEG) signals is essential for developing brain-computer\ninterfaces to improve the quality of life for individuals with hearing\nimpairment. Previous works have concentrated on binary directional focus\ndecoding, i.e., determining whether the attended speaker is on the left or\nright side of the listener. However, a more precise decoding of the exact\ndirection of the attended speaker is necessary for effective speech processing.\nAdditionally, audio spatial information has not been effectively leveraged,\nresulting in suboptimal decoding results. In this paper, it is found that on\nthe recently presented dataset with 14-class directional focus, models relying\nexclusively on EEG inputs exhibit significantly lower accuracy when decoding\nthe directional focus in both leave-one-subject-out and leave-one-trial-out\nscenarios. By integrating audio spatial spectra with EEG features, the decoding\naccuracy can be effectively improved. The CNN, LSM-CNN, and Deformer models are\nemployed to decode the directional focus from listeners' EEG signals and audio\nspatial spectra. The proposed Sp-EEG-Deformer model achieves notable 14-class\ndecoding accuracies of 55.35% and 57.19% in leave-one-subject-out and\nleave-one-trial-out scenarios with a decision window of 1 second, respectively.\nExperiment results indicate increased decoding accuracy as the number of\nalternative directions reduces. These findings suggest the efficacy of our\nproposed dual modal directional focus decoding strategy.\n", "link": "http://arxiv.org/abs/2411.06928v2", "date": "2025-01-09", "relevancy": 2.56, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5257}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5257}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-class%20Decoding%20of%20Attended%20Speaker%20Direction%20Using%0A%20%20Electroencephalogram%20and%20Audio%20Spatial%20Spectrum&body=Title%3A%20Multi-class%20Decoding%20of%20Attended%20Speaker%20Direction%20Using%0A%20%20Electroencephalogram%20and%20Audio%20Spatial%20Spectrum%0AAuthor%3A%20Yuanming%20Zhang%20and%20Jing%20Lu%20and%20Fei%20Chen%20and%20Haoliang%20Du%20and%20Xia%20Gao%20and%20Zhibin%20Lin%0AAbstract%3A%20%20%20Decoding%20the%20directional%20focus%20of%20an%20attended%20speaker%20from%20listeners%27%0Aelectroencephalogram%20%28EEG%29%20signals%20is%20essential%20for%20developing%20brain-computer%0Ainterfaces%20to%20improve%20the%20quality%20of%20life%20for%20individuals%20with%20hearing%0Aimpairment.%20Previous%20works%20have%20concentrated%20on%20binary%20directional%20focus%0Adecoding%2C%20i.e.%2C%20determining%20whether%20the%20attended%20speaker%20is%20on%20the%20left%20or%0Aright%20side%20of%20the%20listener.%20However%2C%20a%20more%20precise%20decoding%20of%20the%20exact%0Adirection%20of%20the%20attended%20speaker%20is%20necessary%20for%20effective%20speech%20processing.%0AAdditionally%2C%20audio%20spatial%20information%20has%20not%20been%20effectively%20leveraged%2C%0Aresulting%20in%20suboptimal%20decoding%20results.%20In%20this%20paper%2C%20it%20is%20found%20that%20on%0Athe%20recently%20presented%20dataset%20with%2014-class%20directional%20focus%2C%20models%20relying%0Aexclusively%20on%20EEG%20inputs%20exhibit%20significantly%20lower%20accuracy%20when%20decoding%0Athe%20directional%20focus%20in%20both%20leave-one-subject-out%20and%20leave-one-trial-out%0Ascenarios.%20By%20integrating%20audio%20spatial%20spectra%20with%20EEG%20features%2C%20the%20decoding%0Aaccuracy%20can%20be%20effectively%20improved.%20The%20CNN%2C%20LSM-CNN%2C%20and%20Deformer%20models%20are%0Aemployed%20to%20decode%20the%20directional%20focus%20from%20listeners%27%20EEG%20signals%20and%20audio%0Aspatial%20spectra.%20The%20proposed%20Sp-EEG-Deformer%20model%20achieves%20notable%2014-class%0Adecoding%20accuracies%20of%2055.35%25%20and%2057.19%25%20in%20leave-one-subject-out%20and%0Aleave-one-trial-out%20scenarios%20with%20a%20decision%20window%20of%201%20second%2C%20respectively.%0AExperiment%20results%20indicate%20increased%20decoding%20accuracy%20as%20the%20number%20of%0Aalternative%20directions%20reduces.%20These%20findings%20suggest%20the%20efficacy%20of%20our%0Aproposed%20dual%20modal%20directional%20focus%20decoding%20strategy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06928v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-class%2520Decoding%2520of%2520Attended%2520Speaker%2520Direction%2520Using%250A%2520%2520Electroencephalogram%2520and%2520Audio%2520Spatial%2520Spectrum%26entry.906535625%3DYuanming%2520Zhang%2520and%2520Jing%2520Lu%2520and%2520Fei%2520Chen%2520and%2520Haoliang%2520Du%2520and%2520Xia%2520Gao%2520and%2520Zhibin%2520Lin%26entry.1292438233%3D%2520%2520Decoding%2520the%2520directional%2520focus%2520of%2520an%2520attended%2520speaker%2520from%2520listeners%2527%250Aelectroencephalogram%2520%2528EEG%2529%2520signals%2520is%2520essential%2520for%2520developing%2520brain-computer%250Ainterfaces%2520to%2520improve%2520the%2520quality%2520of%2520life%2520for%2520individuals%2520with%2520hearing%250Aimpairment.%2520Previous%2520works%2520have%2520concentrated%2520on%2520binary%2520directional%2520focus%250Adecoding%252C%2520i.e.%252C%2520determining%2520whether%2520the%2520attended%2520speaker%2520is%2520on%2520the%2520left%2520or%250Aright%2520side%2520of%2520the%2520listener.%2520However%252C%2520a%2520more%2520precise%2520decoding%2520of%2520the%2520exact%250Adirection%2520of%2520the%2520attended%2520speaker%2520is%2520necessary%2520for%2520effective%2520speech%2520processing.%250AAdditionally%252C%2520audio%2520spatial%2520information%2520has%2520not%2520been%2520effectively%2520leveraged%252C%250Aresulting%2520in%2520suboptimal%2520decoding%2520results.%2520In%2520this%2520paper%252C%2520it%2520is%2520found%2520that%2520on%250Athe%2520recently%2520presented%2520dataset%2520with%252014-class%2520directional%2520focus%252C%2520models%2520relying%250Aexclusively%2520on%2520EEG%2520inputs%2520exhibit%2520significantly%2520lower%2520accuracy%2520when%2520decoding%250Athe%2520directional%2520focus%2520in%2520both%2520leave-one-subject-out%2520and%2520leave-one-trial-out%250Ascenarios.%2520By%2520integrating%2520audio%2520spatial%2520spectra%2520with%2520EEG%2520features%252C%2520the%2520decoding%250Aaccuracy%2520can%2520be%2520effectively%2520improved.%2520The%2520CNN%252C%2520LSM-CNN%252C%2520and%2520Deformer%2520models%2520are%250Aemployed%2520to%2520decode%2520the%2520directional%2520focus%2520from%2520listeners%2527%2520EEG%2520signals%2520and%2520audio%250Aspatial%2520spectra.%2520The%2520proposed%2520Sp-EEG-Deformer%2520model%2520achieves%2520notable%252014-class%250Adecoding%2520accuracies%2520of%252055.35%2525%2520and%252057.19%2525%2520in%2520leave-one-subject-out%2520and%250Aleave-one-trial-out%2520scenarios%2520with%2520a%2520decision%2520window%2520of%25201%2520second%252C%2520respectively.%250AExperiment%2520results%2520indicate%2520increased%2520decoding%2520accuracy%2520as%2520the%2520number%2520of%250Aalternative%2520directions%2520reduces.%2520These%2520findings%2520suggest%2520the%2520efficacy%2520of%2520our%250Aproposed%2520dual%2520modal%2520directional%2520focus%2520decoding%2520strategy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06928v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-class%20Decoding%20of%20Attended%20Speaker%20Direction%20Using%0A%20%20Electroencephalogram%20and%20Audio%20Spatial%20Spectrum&entry.906535625=Yuanming%20Zhang%20and%20Jing%20Lu%20and%20Fei%20Chen%20and%20Haoliang%20Du%20and%20Xia%20Gao%20and%20Zhibin%20Lin&entry.1292438233=%20%20Decoding%20the%20directional%20focus%20of%20an%20attended%20speaker%20from%20listeners%27%0Aelectroencephalogram%20%28EEG%29%20signals%20is%20essential%20for%20developing%20brain-computer%0Ainterfaces%20to%20improve%20the%20quality%20of%20life%20for%20individuals%20with%20hearing%0Aimpairment.%20Previous%20works%20have%20concentrated%20on%20binary%20directional%20focus%0Adecoding%2C%20i.e.%2C%20determining%20whether%20the%20attended%20speaker%20is%20on%20the%20left%20or%0Aright%20side%20of%20the%20listener.%20However%2C%20a%20more%20precise%20decoding%20of%20the%20exact%0Adirection%20of%20the%20attended%20speaker%20is%20necessary%20for%20effective%20speech%20processing.%0AAdditionally%2C%20audio%20spatial%20information%20has%20not%20been%20effectively%20leveraged%2C%0Aresulting%20in%20suboptimal%20decoding%20results.%20In%20this%20paper%2C%20it%20is%20found%20that%20on%0Athe%20recently%20presented%20dataset%20with%2014-class%20directional%20focus%2C%20models%20relying%0Aexclusively%20on%20EEG%20inputs%20exhibit%20significantly%20lower%20accuracy%20when%20decoding%0Athe%20directional%20focus%20in%20both%20leave-one-subject-out%20and%20leave-one-trial-out%0Ascenarios.%20By%20integrating%20audio%20spatial%20spectra%20with%20EEG%20features%2C%20the%20decoding%0Aaccuracy%20can%20be%20effectively%20improved.%20The%20CNN%2C%20LSM-CNN%2C%20and%20Deformer%20models%20are%0Aemployed%20to%20decode%20the%20directional%20focus%20from%20listeners%27%20EEG%20signals%20and%20audio%0Aspatial%20spectra.%20The%20proposed%20Sp-EEG-Deformer%20model%20achieves%20notable%2014-class%0Adecoding%20accuracies%20of%2055.35%25%20and%2057.19%25%20in%20leave-one-subject-out%20and%0Aleave-one-trial-out%20scenarios%20with%20a%20decision%20window%20of%201%20second%2C%20respectively.%0AExperiment%20results%20indicate%20increased%20decoding%20accuracy%20as%20the%20number%20of%0Aalternative%20directions%20reduces.%20These%20findings%20suggest%20the%20efficacy%20of%20our%0Aproposed%20dual%20modal%20directional%20focus%20decoding%20strategy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06928v2&entry.124074799=Read"},
{"title": "Deriving Coding-Specific Sub-Models from LLMs using Resource-Efficient\n  Pruning", "author": "Laura Puccioni and Alireza Farshin and Mariano Scazzariello and Changjie Wang and Marco Chiesa and Dejan Kostic", "abstract": "  Large Language Models (LLMs) have demonstrated their exceptional performance\nin various complex code generation tasks. However, their broader adoption is\nlimited by significant computational demands and high resource requirements,\nparticularly memory and processing power. To mitigate such requirements, model\npruning techniques are used to create more compact models with significantly\nfewer parameters. However, current approaches do not focus on the efficient\nextraction of programming-language-specific sub-models. In this work, we\nexplore the idea of efficiently deriving coding-specific sub-models through\nunstructured pruning (i.e., Wanda). We investigate the impact of different\ndomain-specific calibration datasets on pruning outcomes across three distinct\ndomains and extend our analysis to extracting four language-specific\nsub-models: Python, Java, C++, and JavaScript. We are the first to efficiently\nextract programming-language-specific sub-models using appropriate calibration\ndatasets while maintaining acceptable accuracy w.r.t. full models. We are also\nthe first to provide analytical evidence that domain-specific tasks activate\ndistinct regions within LLMs, supporting the creation of specialized sub-models\nthrough unstructured pruning. We believe that this work has significant\npotential to enhance LLM accessibility for coding by reducing computational\nrequirements to enable local execution on consumer-grade hardware, and\nsupporting faster inference times critical for real-time development feedback.\n", "link": "http://arxiv.org/abs/2501.05248v1", "date": "2025-01-09", "relevancy": 2.5555, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5327}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5327}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deriving%20Coding-Specific%20Sub-Models%20from%20LLMs%20using%20Resource-Efficient%0A%20%20Pruning&body=Title%3A%20Deriving%20Coding-Specific%20Sub-Models%20from%20LLMs%20using%20Resource-Efficient%0A%20%20Pruning%0AAuthor%3A%20Laura%20Puccioni%20and%20Alireza%20Farshin%20and%20Mariano%20Scazzariello%20and%20Changjie%20Wang%20and%20Marco%20Chiesa%20and%20Dejan%20Kostic%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20their%20exceptional%20performance%0Ain%20various%20complex%20code%20generation%20tasks.%20However%2C%20their%20broader%20adoption%20is%0Alimited%20by%20significant%20computational%20demands%20and%20high%20resource%20requirements%2C%0Aparticularly%20memory%20and%20processing%20power.%20To%20mitigate%20such%20requirements%2C%20model%0Apruning%20techniques%20are%20used%20to%20create%20more%20compact%20models%20with%20significantly%0Afewer%20parameters.%20However%2C%20current%20approaches%20do%20not%20focus%20on%20the%20efficient%0Aextraction%20of%20programming-language-specific%20sub-models.%20In%20this%20work%2C%20we%0Aexplore%20the%20idea%20of%20efficiently%20deriving%20coding-specific%20sub-models%20through%0Aunstructured%20pruning%20%28i.e.%2C%20Wanda%29.%20We%20investigate%20the%20impact%20of%20different%0Adomain-specific%20calibration%20datasets%20on%20pruning%20outcomes%20across%20three%20distinct%0Adomains%20and%20extend%20our%20analysis%20to%20extracting%20four%20language-specific%0Asub-models%3A%20Python%2C%20Java%2C%20C%2B%2B%2C%20and%20JavaScript.%20We%20are%20the%20first%20to%20efficiently%0Aextract%20programming-language-specific%20sub-models%20using%20appropriate%20calibration%0Adatasets%20while%20maintaining%20acceptable%20accuracy%20w.r.t.%20full%20models.%20We%20are%20also%0Athe%20first%20to%20provide%20analytical%20evidence%20that%20domain-specific%20tasks%20activate%0Adistinct%20regions%20within%20LLMs%2C%20supporting%20the%20creation%20of%20specialized%20sub-models%0Athrough%20unstructured%20pruning.%20We%20believe%20that%20this%20work%20has%20significant%0Apotential%20to%20enhance%20LLM%20accessibility%20for%20coding%20by%20reducing%20computational%0Arequirements%20to%20enable%20local%20execution%20on%20consumer-grade%20hardware%2C%20and%0Asupporting%20faster%20inference%20times%20critical%20for%20real-time%20development%20feedback.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05248v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeriving%2520Coding-Specific%2520Sub-Models%2520from%2520LLMs%2520using%2520Resource-Efficient%250A%2520%2520Pruning%26entry.906535625%3DLaura%2520Puccioni%2520and%2520Alireza%2520Farshin%2520and%2520Mariano%2520Scazzariello%2520and%2520Changjie%2520Wang%2520and%2520Marco%2520Chiesa%2520and%2520Dejan%2520Kostic%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520their%2520exceptional%2520performance%250Ain%2520various%2520complex%2520code%2520generation%2520tasks.%2520However%252C%2520their%2520broader%2520adoption%2520is%250Alimited%2520by%2520significant%2520computational%2520demands%2520and%2520high%2520resource%2520requirements%252C%250Aparticularly%2520memory%2520and%2520processing%2520power.%2520To%2520mitigate%2520such%2520requirements%252C%2520model%250Apruning%2520techniques%2520are%2520used%2520to%2520create%2520more%2520compact%2520models%2520with%2520significantly%250Afewer%2520parameters.%2520However%252C%2520current%2520approaches%2520do%2520not%2520focus%2520on%2520the%2520efficient%250Aextraction%2520of%2520programming-language-specific%2520sub-models.%2520In%2520this%2520work%252C%2520we%250Aexplore%2520the%2520idea%2520of%2520efficiently%2520deriving%2520coding-specific%2520sub-models%2520through%250Aunstructured%2520pruning%2520%2528i.e.%252C%2520Wanda%2529.%2520We%2520investigate%2520the%2520impact%2520of%2520different%250Adomain-specific%2520calibration%2520datasets%2520on%2520pruning%2520outcomes%2520across%2520three%2520distinct%250Adomains%2520and%2520extend%2520our%2520analysis%2520to%2520extracting%2520four%2520language-specific%250Asub-models%253A%2520Python%252C%2520Java%252C%2520C%252B%252B%252C%2520and%2520JavaScript.%2520We%2520are%2520the%2520first%2520to%2520efficiently%250Aextract%2520programming-language-specific%2520sub-models%2520using%2520appropriate%2520calibration%250Adatasets%2520while%2520maintaining%2520acceptable%2520accuracy%2520w.r.t.%2520full%2520models.%2520We%2520are%2520also%250Athe%2520first%2520to%2520provide%2520analytical%2520evidence%2520that%2520domain-specific%2520tasks%2520activate%250Adistinct%2520regions%2520within%2520LLMs%252C%2520supporting%2520the%2520creation%2520of%2520specialized%2520sub-models%250Athrough%2520unstructured%2520pruning.%2520We%2520believe%2520that%2520this%2520work%2520has%2520significant%250Apotential%2520to%2520enhance%2520LLM%2520accessibility%2520for%2520coding%2520by%2520reducing%2520computational%250Arequirements%2520to%2520enable%2520local%2520execution%2520on%2520consumer-grade%2520hardware%252C%2520and%250Asupporting%2520faster%2520inference%2520times%2520critical%2520for%2520real-time%2520development%2520feedback.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05248v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deriving%20Coding-Specific%20Sub-Models%20from%20LLMs%20using%20Resource-Efficient%0A%20%20Pruning&entry.906535625=Laura%20Puccioni%20and%20Alireza%20Farshin%20and%20Mariano%20Scazzariello%20and%20Changjie%20Wang%20and%20Marco%20Chiesa%20and%20Dejan%20Kostic&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20their%20exceptional%20performance%0Ain%20various%20complex%20code%20generation%20tasks.%20However%2C%20their%20broader%20adoption%20is%0Alimited%20by%20significant%20computational%20demands%20and%20high%20resource%20requirements%2C%0Aparticularly%20memory%20and%20processing%20power.%20To%20mitigate%20such%20requirements%2C%20model%0Apruning%20techniques%20are%20used%20to%20create%20more%20compact%20models%20with%20significantly%0Afewer%20parameters.%20However%2C%20current%20approaches%20do%20not%20focus%20on%20the%20efficient%0Aextraction%20of%20programming-language-specific%20sub-models.%20In%20this%20work%2C%20we%0Aexplore%20the%20idea%20of%20efficiently%20deriving%20coding-specific%20sub-models%20through%0Aunstructured%20pruning%20%28i.e.%2C%20Wanda%29.%20We%20investigate%20the%20impact%20of%20different%0Adomain-specific%20calibration%20datasets%20on%20pruning%20outcomes%20across%20three%20distinct%0Adomains%20and%20extend%20our%20analysis%20to%20extracting%20four%20language-specific%0Asub-models%3A%20Python%2C%20Java%2C%20C%2B%2B%2C%20and%20JavaScript.%20We%20are%20the%20first%20to%20efficiently%0Aextract%20programming-language-specific%20sub-models%20using%20appropriate%20calibration%0Adatasets%20while%20maintaining%20acceptable%20accuracy%20w.r.t.%20full%20models.%20We%20are%20also%0Athe%20first%20to%20provide%20analytical%20evidence%20that%20domain-specific%20tasks%20activate%0Adistinct%20regions%20within%20LLMs%2C%20supporting%20the%20creation%20of%20specialized%20sub-models%0Athrough%20unstructured%20pruning.%20We%20believe%20that%20this%20work%20has%20significant%0Apotential%20to%20enhance%20LLM%20accessibility%20for%20coding%20by%20reducing%20computational%0Arequirements%20to%20enable%20local%20execution%20on%20consumer-grade%20hardware%2C%20and%0Asupporting%20faster%20inference%20times%20critical%20for%20real-time%20development%20feedback.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05248v1&entry.124074799=Read"},
{"title": "Light Transport-aware Diffusion Posterior Sampling for Single-View\n  Reconstruction of 3D Volumes", "author": "Ludwic Leonard and Nils Thuerey and Ruediger Westermann", "abstract": "  We introduce a single-view reconstruction technique of volumetric fields in\nwhich multiple light scattering effects are omnipresent, such as in clouds. We\nmodel the unknown distribution of volumetric fields using an unconditional\ndiffusion model trained on a novel benchmark dataset comprising 1,000\nsynthetically simulated volumetric density fields. The neural diffusion model\nis trained on the latent codes of a novel, diffusion-friendly, monoplanar\nrepresentation. The generative model is used to incorporate a tailored\nparametric diffusion posterior sampling technique into different reconstruction\ntasks. A physically-based differentiable volume renderer is employed to provide\ngradients with respect to light transport in the latent space. This stands in\ncontrast to classic NeRF approaches and makes the reconstructions better\naligned with observed data. Through various experiments, we demonstrate\nsingle-view reconstruction of volumetric clouds at a previously unattainable\nquality.\n", "link": "http://arxiv.org/abs/2501.05226v1", "date": "2025-01-09", "relevancy": 2.541, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6422}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6422}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Light%20Transport-aware%20Diffusion%20Posterior%20Sampling%20for%20Single-View%0A%20%20Reconstruction%20of%203D%20Volumes&body=Title%3A%20Light%20Transport-aware%20Diffusion%20Posterior%20Sampling%20for%20Single-View%0A%20%20Reconstruction%20of%203D%20Volumes%0AAuthor%3A%20Ludwic%20Leonard%20and%20Nils%20Thuerey%20and%20Ruediger%20Westermann%0AAbstract%3A%20%20%20We%20introduce%20a%20single-view%20reconstruction%20technique%20of%20volumetric%20fields%20in%0Awhich%20multiple%20light%20scattering%20effects%20are%20omnipresent%2C%20such%20as%20in%20clouds.%20We%0Amodel%20the%20unknown%20distribution%20of%20volumetric%20fields%20using%20an%20unconditional%0Adiffusion%20model%20trained%20on%20a%20novel%20benchmark%20dataset%20comprising%201%2C000%0Asynthetically%20simulated%20volumetric%20density%20fields.%20The%20neural%20diffusion%20model%0Ais%20trained%20on%20the%20latent%20codes%20of%20a%20novel%2C%20diffusion-friendly%2C%20monoplanar%0Arepresentation.%20The%20generative%20model%20is%20used%20to%20incorporate%20a%20tailored%0Aparametric%20diffusion%20posterior%20sampling%20technique%20into%20different%20reconstruction%0Atasks.%20A%20physically-based%20differentiable%20volume%20renderer%20is%20employed%20to%20provide%0Agradients%20with%20respect%20to%20light%20transport%20in%20the%20latent%20space.%20This%20stands%20in%0Acontrast%20to%20classic%20NeRF%20approaches%20and%20makes%20the%20reconstructions%20better%0Aaligned%20with%20observed%20data.%20Through%20various%20experiments%2C%20we%20demonstrate%0Asingle-view%20reconstruction%20of%20volumetric%20clouds%20at%20a%20previously%20unattainable%0Aquality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05226v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLight%2520Transport-aware%2520Diffusion%2520Posterior%2520Sampling%2520for%2520Single-View%250A%2520%2520Reconstruction%2520of%25203D%2520Volumes%26entry.906535625%3DLudwic%2520Leonard%2520and%2520Nils%2520Thuerey%2520and%2520Ruediger%2520Westermann%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520single-view%2520reconstruction%2520technique%2520of%2520volumetric%2520fields%2520in%250Awhich%2520multiple%2520light%2520scattering%2520effects%2520are%2520omnipresent%252C%2520such%2520as%2520in%2520clouds.%2520We%250Amodel%2520the%2520unknown%2520distribution%2520of%2520volumetric%2520fields%2520using%2520an%2520unconditional%250Adiffusion%2520model%2520trained%2520on%2520a%2520novel%2520benchmark%2520dataset%2520comprising%25201%252C000%250Asynthetically%2520simulated%2520volumetric%2520density%2520fields.%2520The%2520neural%2520diffusion%2520model%250Ais%2520trained%2520on%2520the%2520latent%2520codes%2520of%2520a%2520novel%252C%2520diffusion-friendly%252C%2520monoplanar%250Arepresentation.%2520The%2520generative%2520model%2520is%2520used%2520to%2520incorporate%2520a%2520tailored%250Aparametric%2520diffusion%2520posterior%2520sampling%2520technique%2520into%2520different%2520reconstruction%250Atasks.%2520A%2520physically-based%2520differentiable%2520volume%2520renderer%2520is%2520employed%2520to%2520provide%250Agradients%2520with%2520respect%2520to%2520light%2520transport%2520in%2520the%2520latent%2520space.%2520This%2520stands%2520in%250Acontrast%2520to%2520classic%2520NeRF%2520approaches%2520and%2520makes%2520the%2520reconstructions%2520better%250Aaligned%2520with%2520observed%2520data.%2520Through%2520various%2520experiments%252C%2520we%2520demonstrate%250Asingle-view%2520reconstruction%2520of%2520volumetric%2520clouds%2520at%2520a%2520previously%2520unattainable%250Aquality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05226v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Light%20Transport-aware%20Diffusion%20Posterior%20Sampling%20for%20Single-View%0A%20%20Reconstruction%20of%203D%20Volumes&entry.906535625=Ludwic%20Leonard%20and%20Nils%20Thuerey%20and%20Ruediger%20Westermann&entry.1292438233=%20%20We%20introduce%20a%20single-view%20reconstruction%20technique%20of%20volumetric%20fields%20in%0Awhich%20multiple%20light%20scattering%20effects%20are%20omnipresent%2C%20such%20as%20in%20clouds.%20We%0Amodel%20the%20unknown%20distribution%20of%20volumetric%20fields%20using%20an%20unconditional%0Adiffusion%20model%20trained%20on%20a%20novel%20benchmark%20dataset%20comprising%201%2C000%0Asynthetically%20simulated%20volumetric%20density%20fields.%20The%20neural%20diffusion%20model%0Ais%20trained%20on%20the%20latent%20codes%20of%20a%20novel%2C%20diffusion-friendly%2C%20monoplanar%0Arepresentation.%20The%20generative%20model%20is%20used%20to%20incorporate%20a%20tailored%0Aparametric%20diffusion%20posterior%20sampling%20technique%20into%20different%20reconstruction%0Atasks.%20A%20physically-based%20differentiable%20volume%20renderer%20is%20employed%20to%20provide%0Agradients%20with%20respect%20to%20light%20transport%20in%20the%20latent%20space.%20This%20stands%20in%0Acontrast%20to%20classic%20NeRF%20approaches%20and%20makes%20the%20reconstructions%20better%0Aaligned%20with%20observed%20data.%20Through%20various%20experiments%2C%20we%20demonstrate%0Asingle-view%20reconstruction%20of%20volumetric%20clouds%20at%20a%20previously%20unattainable%0Aquality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05226v1&entry.124074799=Read"},
{"title": "A Novel Pathology Foundation Model by Mayo Clinic, Charit\u00e9, and\n  Aignostics", "author": "Maximilian Alber and Stephan Tietz and Jonas Dippel and Timo Milbich and Timoth\u00e9e Lesort and Panos Korfiatis and Moritz Kr\u00fcgener and Beatriz Perez Cancer and Neelay Shah and Alexander M\u00f6llers and Philipp Seegerer and Alexandra Carpen-Amarie and Kai Standvoss and Gabriel Dernbach and Edwin de Jong and Simon Schallenberg and Andreas Kunft and Helmut Hoffer von Ankershoffen and Gavin Schaeferle and Patrick Duffy and Matt Redlon and Philipp Jurmeister and David Horst and Lukas Ruff and Klaus-Robert M\u00fcller and Frederick Klauschen and Andrew Norgan", "abstract": "  Recent advances in digital pathology have demonstrated the effectiveness of\nfoundation models across diverse applications. In this report, we present a\nnovel vision foundation model based on the RudolfV approach. Our model was\ntrained on a dataset comprising 1.2 million histopathology whole slide images,\ncollected from two medical institutions: Mayo Clinic and Charit\\'e -\nUniverst\\\"atsmedizin Berlin. Comprehensive evaluations show that our model\nachieves state-of-the-art performance across twenty-one public benchmark\ndatasets, even though it is neither the largest model by parameter count nor by\ntraining dataset size.\n", "link": "http://arxiv.org/abs/2501.05409v1", "date": "2025-01-09", "relevancy": 2.5322, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5171}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5171}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Pathology%20Foundation%20Model%20by%20Mayo%20Clinic%2C%20Charit%C3%A9%2C%20and%0A%20%20Aignostics&body=Title%3A%20A%20Novel%20Pathology%20Foundation%20Model%20by%20Mayo%20Clinic%2C%20Charit%C3%A9%2C%20and%0A%20%20Aignostics%0AAuthor%3A%20Maximilian%20Alber%20and%20Stephan%20Tietz%20and%20Jonas%20Dippel%20and%20Timo%20Milbich%20and%20Timoth%C3%A9e%20Lesort%20and%20Panos%20Korfiatis%20and%20Moritz%20Kr%C3%BCgener%20and%20Beatriz%20Perez%20Cancer%20and%20Neelay%20Shah%20and%20Alexander%20M%C3%B6llers%20and%20Philipp%20Seegerer%20and%20Alexandra%20Carpen-Amarie%20and%20Kai%20Standvoss%20and%20Gabriel%20Dernbach%20and%20Edwin%20de%20Jong%20and%20Simon%20Schallenberg%20and%20Andreas%20Kunft%20and%20Helmut%20Hoffer%20von%20Ankershoffen%20and%20Gavin%20Schaeferle%20and%20Patrick%20Duffy%20and%20Matt%20Redlon%20and%20Philipp%20Jurmeister%20and%20David%20Horst%20and%20Lukas%20Ruff%20and%20Klaus-Robert%20M%C3%BCller%20and%20Frederick%20Klauschen%20and%20Andrew%20Norgan%0AAbstract%3A%20%20%20Recent%20advances%20in%20digital%20pathology%20have%20demonstrated%20the%20effectiveness%20of%0Afoundation%20models%20across%20diverse%20applications.%20In%20this%20report%2C%20we%20present%20a%0Anovel%20vision%20foundation%20model%20based%20on%20the%20RudolfV%20approach.%20Our%20model%20was%0Atrained%20on%20a%20dataset%20comprising%201.2%20million%20histopathology%20whole%20slide%20images%2C%0Acollected%20from%20two%20medical%20institutions%3A%20Mayo%20Clinic%20and%20Charit%5C%27e%20-%0AUniverst%5C%22atsmedizin%20Berlin.%20Comprehensive%20evaluations%20show%20that%20our%20model%0Aachieves%20state-of-the-art%20performance%20across%20twenty-one%20public%20benchmark%0Adatasets%2C%20even%20though%20it%20is%20neither%20the%20largest%20model%20by%20parameter%20count%20nor%20by%0Atraining%20dataset%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05409v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Pathology%2520Foundation%2520Model%2520by%2520Mayo%2520Clinic%252C%2520Charit%25C3%25A9%252C%2520and%250A%2520%2520Aignostics%26entry.906535625%3DMaximilian%2520Alber%2520and%2520Stephan%2520Tietz%2520and%2520Jonas%2520Dippel%2520and%2520Timo%2520Milbich%2520and%2520Timoth%25C3%25A9e%2520Lesort%2520and%2520Panos%2520Korfiatis%2520and%2520Moritz%2520Kr%25C3%25BCgener%2520and%2520Beatriz%2520Perez%2520Cancer%2520and%2520Neelay%2520Shah%2520and%2520Alexander%2520M%25C3%25B6llers%2520and%2520Philipp%2520Seegerer%2520and%2520Alexandra%2520Carpen-Amarie%2520and%2520Kai%2520Standvoss%2520and%2520Gabriel%2520Dernbach%2520and%2520Edwin%2520de%2520Jong%2520and%2520Simon%2520Schallenberg%2520and%2520Andreas%2520Kunft%2520and%2520Helmut%2520Hoffer%2520von%2520Ankershoffen%2520and%2520Gavin%2520Schaeferle%2520and%2520Patrick%2520Duffy%2520and%2520Matt%2520Redlon%2520and%2520Philipp%2520Jurmeister%2520and%2520David%2520Horst%2520and%2520Lukas%2520Ruff%2520and%2520Klaus-Robert%2520M%25C3%25BCller%2520and%2520Frederick%2520Klauschen%2520and%2520Andrew%2520Norgan%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520digital%2520pathology%2520have%2520demonstrated%2520the%2520effectiveness%2520of%250Afoundation%2520models%2520across%2520diverse%2520applications.%2520In%2520this%2520report%252C%2520we%2520present%2520a%250Anovel%2520vision%2520foundation%2520model%2520based%2520on%2520the%2520RudolfV%2520approach.%2520Our%2520model%2520was%250Atrained%2520on%2520a%2520dataset%2520comprising%25201.2%2520million%2520histopathology%2520whole%2520slide%2520images%252C%250Acollected%2520from%2520two%2520medical%2520institutions%253A%2520Mayo%2520Clinic%2520and%2520Charit%255C%2527e%2520-%250AUniverst%255C%2522atsmedizin%2520Berlin.%2520Comprehensive%2520evaluations%2520show%2520that%2520our%2520model%250Aachieves%2520state-of-the-art%2520performance%2520across%2520twenty-one%2520public%2520benchmark%250Adatasets%252C%2520even%2520though%2520it%2520is%2520neither%2520the%2520largest%2520model%2520by%2520parameter%2520count%2520nor%2520by%250Atraining%2520dataset%2520size.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05409v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Pathology%20Foundation%20Model%20by%20Mayo%20Clinic%2C%20Charit%C3%A9%2C%20and%0A%20%20Aignostics&entry.906535625=Maximilian%20Alber%20and%20Stephan%20Tietz%20and%20Jonas%20Dippel%20and%20Timo%20Milbich%20and%20Timoth%C3%A9e%20Lesort%20and%20Panos%20Korfiatis%20and%20Moritz%20Kr%C3%BCgener%20and%20Beatriz%20Perez%20Cancer%20and%20Neelay%20Shah%20and%20Alexander%20M%C3%B6llers%20and%20Philipp%20Seegerer%20and%20Alexandra%20Carpen-Amarie%20and%20Kai%20Standvoss%20and%20Gabriel%20Dernbach%20and%20Edwin%20de%20Jong%20and%20Simon%20Schallenberg%20and%20Andreas%20Kunft%20and%20Helmut%20Hoffer%20von%20Ankershoffen%20and%20Gavin%20Schaeferle%20and%20Patrick%20Duffy%20and%20Matt%20Redlon%20and%20Philipp%20Jurmeister%20and%20David%20Horst%20and%20Lukas%20Ruff%20and%20Klaus-Robert%20M%C3%BCller%20and%20Frederick%20Klauschen%20and%20Andrew%20Norgan&entry.1292438233=%20%20Recent%20advances%20in%20digital%20pathology%20have%20demonstrated%20the%20effectiveness%20of%0Afoundation%20models%20across%20diverse%20applications.%20In%20this%20report%2C%20we%20present%20a%0Anovel%20vision%20foundation%20model%20based%20on%20the%20RudolfV%20approach.%20Our%20model%20was%0Atrained%20on%20a%20dataset%20comprising%201.2%20million%20histopathology%20whole%20slide%20images%2C%0Acollected%20from%20two%20medical%20institutions%3A%20Mayo%20Clinic%20and%20Charit%5C%27e%20-%0AUniverst%5C%22atsmedizin%20Berlin.%20Comprehensive%20evaluations%20show%20that%20our%20model%0Aachieves%20state-of-the-art%20performance%20across%20twenty-one%20public%20benchmark%0Adatasets%2C%20even%20though%20it%20is%20neither%20the%20largest%20model%20by%20parameter%20count%20nor%20by%0Atraining%20dataset%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05409v1&entry.124074799=Read"},
{"title": "CMTNet: Convolutional Meets Transformer Network for Hyperspectral Images\n  Classification", "author": "Faxu Guo and Quan Feng and Sen Yang and Wanxia Yang", "abstract": "  Hyperspectral remote sensing (HIS) enables the detailed capture of spectral\ninformation from the Earth's surface, facilitating precise classification and\nidentification of surface crops due to its superior spectral diagnostic\ncapabilities. However, current convolutional neural networks (CNNs) focus on\nlocal features in hyperspectral data, leading to suboptimal performance when\nclassifying intricate crop types and addressing imbalanced sample\ndistributions. In contrast, the Transformer framework excels at extracting\nglobal features from hyperspectral imagery. To leverage the strengths of both\napproaches, this research introduces the Convolutional Meet Transformer Network\n(CMTNet). This innovative model includes a spectral-spatial feature extraction\nmodule for shallow feature capture, a dual-branch structure combining CNN and\nTransformer branches for local and global feature extraction, and a\nmulti-output constraint module that enhances classification accuracy through\nmulti-output loss calculations and cross constraints across local,\ninternational, and joint features. Extensive experiments conducted on three\ndatasets (WHU-Hi-LongKou, WHU-Hi-HanChuan, and WHU-Hi-HongHu) demonstrate that\nCTDBNet significantly outperforms other state-of-the-art networks in\nclassification performance, validating its effectiveness in hyperspectral crop\nclassification.\n", "link": "http://arxiv.org/abs/2406.14080v3", "date": "2025-01-09", "relevancy": 2.528, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.52}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5115}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CMTNet%3A%20Convolutional%20Meets%20Transformer%20Network%20for%20Hyperspectral%20Images%0A%20%20Classification&body=Title%3A%20CMTNet%3A%20Convolutional%20Meets%20Transformer%20Network%20for%20Hyperspectral%20Images%0A%20%20Classification%0AAuthor%3A%20Faxu%20Guo%20and%20Quan%20Feng%20and%20Sen%20Yang%20and%20Wanxia%20Yang%0AAbstract%3A%20%20%20Hyperspectral%20remote%20sensing%20%28HIS%29%20enables%20the%20detailed%20capture%20of%20spectral%0Ainformation%20from%20the%20Earth%27s%20surface%2C%20facilitating%20precise%20classification%20and%0Aidentification%20of%20surface%20crops%20due%20to%20its%20superior%20spectral%20diagnostic%0Acapabilities.%20However%2C%20current%20convolutional%20neural%20networks%20%28CNNs%29%20focus%20on%0Alocal%20features%20in%20hyperspectral%20data%2C%20leading%20to%20suboptimal%20performance%20when%0Aclassifying%20intricate%20crop%20types%20and%20addressing%20imbalanced%20sample%0Adistributions.%20In%20contrast%2C%20the%20Transformer%20framework%20excels%20at%20extracting%0Aglobal%20features%20from%20hyperspectral%20imagery.%20To%20leverage%20the%20strengths%20of%20both%0Aapproaches%2C%20this%20research%20introduces%20the%20Convolutional%20Meet%20Transformer%20Network%0A%28CMTNet%29.%20This%20innovative%20model%20includes%20a%20spectral-spatial%20feature%20extraction%0Amodule%20for%20shallow%20feature%20capture%2C%20a%20dual-branch%20structure%20combining%20CNN%20and%0ATransformer%20branches%20for%20local%20and%20global%20feature%20extraction%2C%20and%20a%0Amulti-output%20constraint%20module%20that%20enhances%20classification%20accuracy%20through%0Amulti-output%20loss%20calculations%20and%20cross%20constraints%20across%20local%2C%0Ainternational%2C%20and%20joint%20features.%20Extensive%20experiments%20conducted%20on%20three%0Adatasets%20%28WHU-Hi-LongKou%2C%20WHU-Hi-HanChuan%2C%20and%20WHU-Hi-HongHu%29%20demonstrate%20that%0ACTDBNet%20significantly%20outperforms%20other%20state-of-the-art%20networks%20in%0Aclassification%20performance%2C%20validating%20its%20effectiveness%20in%20hyperspectral%20crop%0Aclassification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14080v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCMTNet%253A%2520Convolutional%2520Meets%2520Transformer%2520Network%2520for%2520Hyperspectral%2520Images%250A%2520%2520Classification%26entry.906535625%3DFaxu%2520Guo%2520and%2520Quan%2520Feng%2520and%2520Sen%2520Yang%2520and%2520Wanxia%2520Yang%26entry.1292438233%3D%2520%2520Hyperspectral%2520remote%2520sensing%2520%2528HIS%2529%2520enables%2520the%2520detailed%2520capture%2520of%2520spectral%250Ainformation%2520from%2520the%2520Earth%2527s%2520surface%252C%2520facilitating%2520precise%2520classification%2520and%250Aidentification%2520of%2520surface%2520crops%2520due%2520to%2520its%2520superior%2520spectral%2520diagnostic%250Acapabilities.%2520However%252C%2520current%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520focus%2520on%250Alocal%2520features%2520in%2520hyperspectral%2520data%252C%2520leading%2520to%2520suboptimal%2520performance%2520when%250Aclassifying%2520intricate%2520crop%2520types%2520and%2520addressing%2520imbalanced%2520sample%250Adistributions.%2520In%2520contrast%252C%2520the%2520Transformer%2520framework%2520excels%2520at%2520extracting%250Aglobal%2520features%2520from%2520hyperspectral%2520imagery.%2520To%2520leverage%2520the%2520strengths%2520of%2520both%250Aapproaches%252C%2520this%2520research%2520introduces%2520the%2520Convolutional%2520Meet%2520Transformer%2520Network%250A%2528CMTNet%2529.%2520This%2520innovative%2520model%2520includes%2520a%2520spectral-spatial%2520feature%2520extraction%250Amodule%2520for%2520shallow%2520feature%2520capture%252C%2520a%2520dual-branch%2520structure%2520combining%2520CNN%2520and%250ATransformer%2520branches%2520for%2520local%2520and%2520global%2520feature%2520extraction%252C%2520and%2520a%250Amulti-output%2520constraint%2520module%2520that%2520enhances%2520classification%2520accuracy%2520through%250Amulti-output%2520loss%2520calculations%2520and%2520cross%2520constraints%2520across%2520local%252C%250Ainternational%252C%2520and%2520joint%2520features.%2520Extensive%2520experiments%2520conducted%2520on%2520three%250Adatasets%2520%2528WHU-Hi-LongKou%252C%2520WHU-Hi-HanChuan%252C%2520and%2520WHU-Hi-HongHu%2529%2520demonstrate%2520that%250ACTDBNet%2520significantly%2520outperforms%2520other%2520state-of-the-art%2520networks%2520in%250Aclassification%2520performance%252C%2520validating%2520its%2520effectiveness%2520in%2520hyperspectral%2520crop%250Aclassification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14080v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CMTNet%3A%20Convolutional%20Meets%20Transformer%20Network%20for%20Hyperspectral%20Images%0A%20%20Classification&entry.906535625=Faxu%20Guo%20and%20Quan%20Feng%20and%20Sen%20Yang%20and%20Wanxia%20Yang&entry.1292438233=%20%20Hyperspectral%20remote%20sensing%20%28HIS%29%20enables%20the%20detailed%20capture%20of%20spectral%0Ainformation%20from%20the%20Earth%27s%20surface%2C%20facilitating%20precise%20classification%20and%0Aidentification%20of%20surface%20crops%20due%20to%20its%20superior%20spectral%20diagnostic%0Acapabilities.%20However%2C%20current%20convolutional%20neural%20networks%20%28CNNs%29%20focus%20on%0Alocal%20features%20in%20hyperspectral%20data%2C%20leading%20to%20suboptimal%20performance%20when%0Aclassifying%20intricate%20crop%20types%20and%20addressing%20imbalanced%20sample%0Adistributions.%20In%20contrast%2C%20the%20Transformer%20framework%20excels%20at%20extracting%0Aglobal%20features%20from%20hyperspectral%20imagery.%20To%20leverage%20the%20strengths%20of%20both%0Aapproaches%2C%20this%20research%20introduces%20the%20Convolutional%20Meet%20Transformer%20Network%0A%28CMTNet%29.%20This%20innovative%20model%20includes%20a%20spectral-spatial%20feature%20extraction%0Amodule%20for%20shallow%20feature%20capture%2C%20a%20dual-branch%20structure%20combining%20CNN%20and%0ATransformer%20branches%20for%20local%20and%20global%20feature%20extraction%2C%20and%20a%0Amulti-output%20constraint%20module%20that%20enhances%20classification%20accuracy%20through%0Amulti-output%20loss%20calculations%20and%20cross%20constraints%20across%20local%2C%0Ainternational%2C%20and%20joint%20features.%20Extensive%20experiments%20conducted%20on%20three%0Adatasets%20%28WHU-Hi-LongKou%2C%20WHU-Hi-HanChuan%2C%20and%20WHU-Hi-HongHu%29%20demonstrate%20that%0ACTDBNet%20significantly%20outperforms%20other%20state-of-the-art%20networks%20in%0Aclassification%20performance%2C%20validating%20its%20effectiveness%20in%20hyperspectral%20crop%0Aclassification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14080v3&entry.124074799=Read"},
{"title": "Developing a Foundation of Vector Symbolic Architectures Using Category\n  Theory", "author": "Nolan P Shaw and P Michael Furlong and Britt Anderson and Jeff Orchard", "abstract": "  At the risk of overstating the case, connectionist approaches to machine\nlearning, i.e. neural networks, are enjoying a small vogue right now. However,\nthese methods require large volumes of data and produce models that are\nuninterpretable to humans. An alternative framework that is compatible with\nneural networks and gradient-based learning, but explicitly models\ncompositionality, is Vector Symbolic Architectures (VSAs). VSAs are a family of\nalgebras on high-dimensional vector representations. They arose in cognitive\nscience from the need to unify neural processing and the kind of symbolic\nreasoning that humans perform. While machine learning methods have benefited\nfrom category theoretical analyses, VSAs have not yet received similar\ntreatment. In this paper, we present a first attempt at applying category\ntheory to VSAs. Specifically, we conduct a brief literature survey\ndemonstrating the lacking intersection of these two topics, provide a list of\ndesiderata for VSAs, and propose that VSAs may be understood as a (division)\nrig in a category enriched over a monoid in Met (the category of Lawvere metric\nspaces). This final contribution suggests that VSAs may be generalised beyond\ncurrent implementations. It is our hope that grounding VSAs in category theory\nwill lead to more rigorous connections with other research, both within and\nbeyond, learning and cognition.\n", "link": "http://arxiv.org/abs/2501.05368v1", "date": "2025-01-09", "relevancy": 2.4964, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5078}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5078}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Developing%20a%20Foundation%20of%20Vector%20Symbolic%20Architectures%20Using%20Category%0A%20%20Theory&body=Title%3A%20Developing%20a%20Foundation%20of%20Vector%20Symbolic%20Architectures%20Using%20Category%0A%20%20Theory%0AAuthor%3A%20Nolan%20P%20Shaw%20and%20P%20Michael%20Furlong%20and%20Britt%20Anderson%20and%20Jeff%20Orchard%0AAbstract%3A%20%20%20At%20the%20risk%20of%20overstating%20the%20case%2C%20connectionist%20approaches%20to%20machine%0Alearning%2C%20i.e.%20neural%20networks%2C%20are%20enjoying%20a%20small%20vogue%20right%20now.%20However%2C%0Athese%20methods%20require%20large%20volumes%20of%20data%20and%20produce%20models%20that%20are%0Auninterpretable%20to%20humans.%20An%20alternative%20framework%20that%20is%20compatible%20with%0Aneural%20networks%20and%20gradient-based%20learning%2C%20but%20explicitly%20models%0Acompositionality%2C%20is%20Vector%20Symbolic%20Architectures%20%28VSAs%29.%20VSAs%20are%20a%20family%20of%0Aalgebras%20on%20high-dimensional%20vector%20representations.%20They%20arose%20in%20cognitive%0Ascience%20from%20the%20need%20to%20unify%20neural%20processing%20and%20the%20kind%20of%20symbolic%0Areasoning%20that%20humans%20perform.%20While%20machine%20learning%20methods%20have%20benefited%0Afrom%20category%20theoretical%20analyses%2C%20VSAs%20have%20not%20yet%20received%20similar%0Atreatment.%20In%20this%20paper%2C%20we%20present%20a%20first%20attempt%20at%20applying%20category%0Atheory%20to%20VSAs.%20Specifically%2C%20we%20conduct%20a%20brief%20literature%20survey%0Ademonstrating%20the%20lacking%20intersection%20of%20these%20two%20topics%2C%20provide%20a%20list%20of%0Adesiderata%20for%20VSAs%2C%20and%20propose%20that%20VSAs%20may%20be%20understood%20as%20a%20%28division%29%0Arig%20in%20a%20category%20enriched%20over%20a%20monoid%20in%20Met%20%28the%20category%20of%20Lawvere%20metric%0Aspaces%29.%20This%20final%20contribution%20suggests%20that%20VSAs%20may%20be%20generalised%20beyond%0Acurrent%20implementations.%20It%20is%20our%20hope%20that%20grounding%20VSAs%20in%20category%20theory%0Awill%20lead%20to%20more%20rigorous%20connections%20with%20other%20research%2C%20both%20within%20and%0Abeyond%2C%20learning%20and%20cognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05368v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeveloping%2520a%2520Foundation%2520of%2520Vector%2520Symbolic%2520Architectures%2520Using%2520Category%250A%2520%2520Theory%26entry.906535625%3DNolan%2520P%2520Shaw%2520and%2520P%2520Michael%2520Furlong%2520and%2520Britt%2520Anderson%2520and%2520Jeff%2520Orchard%26entry.1292438233%3D%2520%2520At%2520the%2520risk%2520of%2520overstating%2520the%2520case%252C%2520connectionist%2520approaches%2520to%2520machine%250Alearning%252C%2520i.e.%2520neural%2520networks%252C%2520are%2520enjoying%2520a%2520small%2520vogue%2520right%2520now.%2520However%252C%250Athese%2520methods%2520require%2520large%2520volumes%2520of%2520data%2520and%2520produce%2520models%2520that%2520are%250Auninterpretable%2520to%2520humans.%2520An%2520alternative%2520framework%2520that%2520is%2520compatible%2520with%250Aneural%2520networks%2520and%2520gradient-based%2520learning%252C%2520but%2520explicitly%2520models%250Acompositionality%252C%2520is%2520Vector%2520Symbolic%2520Architectures%2520%2528VSAs%2529.%2520VSAs%2520are%2520a%2520family%2520of%250Aalgebras%2520on%2520high-dimensional%2520vector%2520representations.%2520They%2520arose%2520in%2520cognitive%250Ascience%2520from%2520the%2520need%2520to%2520unify%2520neural%2520processing%2520and%2520the%2520kind%2520of%2520symbolic%250Areasoning%2520that%2520humans%2520perform.%2520While%2520machine%2520learning%2520methods%2520have%2520benefited%250Afrom%2520category%2520theoretical%2520analyses%252C%2520VSAs%2520have%2520not%2520yet%2520received%2520similar%250Atreatment.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520first%2520attempt%2520at%2520applying%2520category%250Atheory%2520to%2520VSAs.%2520Specifically%252C%2520we%2520conduct%2520a%2520brief%2520literature%2520survey%250Ademonstrating%2520the%2520lacking%2520intersection%2520of%2520these%2520two%2520topics%252C%2520provide%2520a%2520list%2520of%250Adesiderata%2520for%2520VSAs%252C%2520and%2520propose%2520that%2520VSAs%2520may%2520be%2520understood%2520as%2520a%2520%2528division%2529%250Arig%2520in%2520a%2520category%2520enriched%2520over%2520a%2520monoid%2520in%2520Met%2520%2528the%2520category%2520of%2520Lawvere%2520metric%250Aspaces%2529.%2520This%2520final%2520contribution%2520suggests%2520that%2520VSAs%2520may%2520be%2520generalised%2520beyond%250Acurrent%2520implementations.%2520It%2520is%2520our%2520hope%2520that%2520grounding%2520VSAs%2520in%2520category%2520theory%250Awill%2520lead%2520to%2520more%2520rigorous%2520connections%2520with%2520other%2520research%252C%2520both%2520within%2520and%250Abeyond%252C%2520learning%2520and%2520cognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05368v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Developing%20a%20Foundation%20of%20Vector%20Symbolic%20Architectures%20Using%20Category%0A%20%20Theory&entry.906535625=Nolan%20P%20Shaw%20and%20P%20Michael%20Furlong%20and%20Britt%20Anderson%20and%20Jeff%20Orchard&entry.1292438233=%20%20At%20the%20risk%20of%20overstating%20the%20case%2C%20connectionist%20approaches%20to%20machine%0Alearning%2C%20i.e.%20neural%20networks%2C%20are%20enjoying%20a%20small%20vogue%20right%20now.%20However%2C%0Athese%20methods%20require%20large%20volumes%20of%20data%20and%20produce%20models%20that%20are%0Auninterpretable%20to%20humans.%20An%20alternative%20framework%20that%20is%20compatible%20with%0Aneural%20networks%20and%20gradient-based%20learning%2C%20but%20explicitly%20models%0Acompositionality%2C%20is%20Vector%20Symbolic%20Architectures%20%28VSAs%29.%20VSAs%20are%20a%20family%20of%0Aalgebras%20on%20high-dimensional%20vector%20representations.%20They%20arose%20in%20cognitive%0Ascience%20from%20the%20need%20to%20unify%20neural%20processing%20and%20the%20kind%20of%20symbolic%0Areasoning%20that%20humans%20perform.%20While%20machine%20learning%20methods%20have%20benefited%0Afrom%20category%20theoretical%20analyses%2C%20VSAs%20have%20not%20yet%20received%20similar%0Atreatment.%20In%20this%20paper%2C%20we%20present%20a%20first%20attempt%20at%20applying%20category%0Atheory%20to%20VSAs.%20Specifically%2C%20we%20conduct%20a%20brief%20literature%20survey%0Ademonstrating%20the%20lacking%20intersection%20of%20these%20two%20topics%2C%20provide%20a%20list%20of%0Adesiderata%20for%20VSAs%2C%20and%20propose%20that%20VSAs%20may%20be%20understood%20as%20a%20%28division%29%0Arig%20in%20a%20category%20enriched%20over%20a%20monoid%20in%20Met%20%28the%20category%20of%20Lawvere%20metric%0Aspaces%29.%20This%20final%20contribution%20suggests%20that%20VSAs%20may%20be%20generalised%20beyond%0Acurrent%20implementations.%20It%20is%20our%20hope%20that%20grounding%20VSAs%20in%20category%20theory%0Awill%20lead%20to%20more%20rigorous%20connections%20with%20other%20research%2C%20both%20within%20and%0Abeyond%2C%20learning%20and%20cognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05368v1&entry.124074799=Read"},
{"title": "Boosting Graph Neural Network Training by Focusing on Non-Robust Samples\n  from the Training Set", "author": "Yongyu Wang", "abstract": "  Graph Neural Networks (GNNs) are a highly effective neural network\narchitecture for processing graph-structured data. Unlike traditional neural\nnetworks that rely solely on the features of the data as input, GNNs leverage\nboth the graph structure, which represents the relationships between data\npoints, and the feature matrix of the data to optimize their feature\nrepresentation. This unique capability enables GNNs to achieve superior\nperformance across various tasks. However, it also makes GNNs more susceptible\nto noise from both the graph structure and data features, which can\nsignificantly increase the training difficulty and degrade their performance.\nTo address this issue, this paper proposes a novel method for selecting\nnoise-sensitive training samples from the original training set to construct a\nsmaller yet more effective training set for model training. These samples are\nthen used to enhance the model's ability to handle noise-prone instances\neffectively. We have evaluated our approach on three of the most classical GNN\nmodels -- GCN, GAT, and GraphSAGE -- as well as three widely used benchmark\ndatasets: Cora, Citeseer, and PubMed. Our experiments demonstrate that the\nproposed method can substantially boost the overall training of Graph Neural\nNetworks compared to using randomly constructed training sets.\n", "link": "http://arxiv.org/abs/2412.14738v5", "date": "2025-01-09", "relevancy": 2.4904, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5577}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4756}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Graph%20Neural%20Network%20Training%20by%20Focusing%20on%20Non-Robust%20Samples%0A%20%20from%20the%20Training%20Set&body=Title%3A%20Boosting%20Graph%20Neural%20Network%20Training%20by%20Focusing%20on%20Non-Robust%20Samples%0A%20%20from%20the%20Training%20Set%0AAuthor%3A%20Yongyu%20Wang%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20a%20highly%20effective%20neural%20network%0Aarchitecture%20for%20processing%20graph-structured%20data.%20Unlike%20traditional%20neural%0Anetworks%20that%20rely%20solely%20on%20the%20features%20of%20the%20data%20as%20input%2C%20GNNs%20leverage%0Aboth%20the%20graph%20structure%2C%20which%20represents%20the%20relationships%20between%20data%0Apoints%2C%20and%20the%20feature%20matrix%20of%20the%20data%20to%20optimize%20their%20feature%0Arepresentation.%20This%20unique%20capability%20enables%20GNNs%20to%20achieve%20superior%0Aperformance%20across%20various%20tasks.%20However%2C%20it%20also%20makes%20GNNs%20more%20susceptible%0Ato%20noise%20from%20both%20the%20graph%20structure%20and%20data%20features%2C%20which%20can%0Asignificantly%20increase%20the%20training%20difficulty%20and%20degrade%20their%20performance.%0ATo%20address%20this%20issue%2C%20this%20paper%20proposes%20a%20novel%20method%20for%20selecting%0Anoise-sensitive%20training%20samples%20from%20the%20original%20training%20set%20to%20construct%20a%0Asmaller%20yet%20more%20effective%20training%20set%20for%20model%20training.%20These%20samples%20are%0Athen%20used%20to%20enhance%20the%20model%27s%20ability%20to%20handle%20noise-prone%20instances%0Aeffectively.%20We%20have%20evaluated%20our%20approach%20on%20three%20of%20the%20most%20classical%20GNN%0Amodels%20--%20GCN%2C%20GAT%2C%20and%20GraphSAGE%20--%20as%20well%20as%20three%20widely%20used%20benchmark%0Adatasets%3A%20Cora%2C%20Citeseer%2C%20and%20PubMed.%20Our%20experiments%20demonstrate%20that%20the%0Aproposed%20method%20can%20substantially%20boost%20the%20overall%20training%20of%20Graph%20Neural%0ANetworks%20compared%20to%20using%20randomly%20constructed%20training%20sets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14738v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Graph%2520Neural%2520Network%2520Training%2520by%2520Focusing%2520on%2520Non-Robust%2520Samples%250A%2520%2520from%2520the%2520Training%2520Set%26entry.906535625%3DYongyu%2520Wang%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520a%2520highly%2520effective%2520neural%2520network%250Aarchitecture%2520for%2520processing%2520graph-structured%2520data.%2520Unlike%2520traditional%2520neural%250Anetworks%2520that%2520rely%2520solely%2520on%2520the%2520features%2520of%2520the%2520data%2520as%2520input%252C%2520GNNs%2520leverage%250Aboth%2520the%2520graph%2520structure%252C%2520which%2520represents%2520the%2520relationships%2520between%2520data%250Apoints%252C%2520and%2520the%2520feature%2520matrix%2520of%2520the%2520data%2520to%2520optimize%2520their%2520feature%250Arepresentation.%2520This%2520unique%2520capability%2520enables%2520GNNs%2520to%2520achieve%2520superior%250Aperformance%2520across%2520various%2520tasks.%2520However%252C%2520it%2520also%2520makes%2520GNNs%2520more%2520susceptible%250Ato%2520noise%2520from%2520both%2520the%2520graph%2520structure%2520and%2520data%2520features%252C%2520which%2520can%250Asignificantly%2520increase%2520the%2520training%2520difficulty%2520and%2520degrade%2520their%2520performance.%250ATo%2520address%2520this%2520issue%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520method%2520for%2520selecting%250Anoise-sensitive%2520training%2520samples%2520from%2520the%2520original%2520training%2520set%2520to%2520construct%2520a%250Asmaller%2520yet%2520more%2520effective%2520training%2520set%2520for%2520model%2520training.%2520These%2520samples%2520are%250Athen%2520used%2520to%2520enhance%2520the%2520model%2527s%2520ability%2520to%2520handle%2520noise-prone%2520instances%250Aeffectively.%2520We%2520have%2520evaluated%2520our%2520approach%2520on%2520three%2520of%2520the%2520most%2520classical%2520GNN%250Amodels%2520--%2520GCN%252C%2520GAT%252C%2520and%2520GraphSAGE%2520--%2520as%2520well%2520as%2520three%2520widely%2520used%2520benchmark%250Adatasets%253A%2520Cora%252C%2520Citeseer%252C%2520and%2520PubMed.%2520Our%2520experiments%2520demonstrate%2520that%2520the%250Aproposed%2520method%2520can%2520substantially%2520boost%2520the%2520overall%2520training%2520of%2520Graph%2520Neural%250ANetworks%2520compared%2520to%2520using%2520randomly%2520constructed%2520training%2520sets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14738v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Graph%20Neural%20Network%20Training%20by%20Focusing%20on%20Non-Robust%20Samples%0A%20%20from%20the%20Training%20Set&entry.906535625=Yongyu%20Wang&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20a%20highly%20effective%20neural%20network%0Aarchitecture%20for%20processing%20graph-structured%20data.%20Unlike%20traditional%20neural%0Anetworks%20that%20rely%20solely%20on%20the%20features%20of%20the%20data%20as%20input%2C%20GNNs%20leverage%0Aboth%20the%20graph%20structure%2C%20which%20represents%20the%20relationships%20between%20data%0Apoints%2C%20and%20the%20feature%20matrix%20of%20the%20data%20to%20optimize%20their%20feature%0Arepresentation.%20This%20unique%20capability%20enables%20GNNs%20to%20achieve%20superior%0Aperformance%20across%20various%20tasks.%20However%2C%20it%20also%20makes%20GNNs%20more%20susceptible%0Ato%20noise%20from%20both%20the%20graph%20structure%20and%20data%20features%2C%20which%20can%0Asignificantly%20increase%20the%20training%20difficulty%20and%20degrade%20their%20performance.%0ATo%20address%20this%20issue%2C%20this%20paper%20proposes%20a%20novel%20method%20for%20selecting%0Anoise-sensitive%20training%20samples%20from%20the%20original%20training%20set%20to%20construct%20a%0Asmaller%20yet%20more%20effective%20training%20set%20for%20model%20training.%20These%20samples%20are%0Athen%20used%20to%20enhance%20the%20model%27s%20ability%20to%20handle%20noise-prone%20instances%0Aeffectively.%20We%20have%20evaluated%20our%20approach%20on%20three%20of%20the%20most%20classical%20GNN%0Amodels%20--%20GCN%2C%20GAT%2C%20and%20GraphSAGE%20--%20as%20well%20as%20three%20widely%20used%20benchmark%0Adatasets%3A%20Cora%2C%20Citeseer%2C%20and%20PubMed.%20Our%20experiments%20demonstrate%20that%20the%0Aproposed%20method%20can%20substantially%20boost%20the%20overall%20training%20of%20Graph%20Neural%0ANetworks%20compared%20to%20using%20randomly%20constructed%20training%20sets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14738v5&entry.124074799=Read"},
{"title": "Consistent Flow Distillation for Text-to-3D Generation", "author": "Runjie Yan and Yinbo Chen and Xiaolong Wang", "abstract": "  Score Distillation Sampling (SDS) has made significant strides in distilling\nimage-generative models for 3D generation. However, its\nmaximum-likelihood-seeking behavior often leads to degraded visual quality and\ndiversity, limiting its effectiveness in 3D applications. In this work, we\npropose Consistent Flow Distillation (CFD), which addresses these limitations.\nWe begin by leveraging the gradient of the diffusion ODE or SDE sampling\nprocess to guide the 3D generation. From the gradient-based sampling\nperspective, we find that the consistency of 2D image flows across different\nviewpoints is important for high-quality 3D generation. To achieve this, we\nintroduce multi-view consistent Gaussian noise on the 3D object, which can be\nrendered from various viewpoints to compute the flow gradient. Our experiments\ndemonstrate that CFD, through consistent flows, significantly outperforms\nprevious methods in text-to-3D generation.\n", "link": "http://arxiv.org/abs/2501.05445v1", "date": "2025-01-09", "relevancy": 2.4762, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6695}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.609}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Consistent%20Flow%20Distillation%20for%20Text-to-3D%20Generation&body=Title%3A%20Consistent%20Flow%20Distillation%20for%20Text-to-3D%20Generation%0AAuthor%3A%20Runjie%20Yan%20and%20Yinbo%20Chen%20and%20Xiaolong%20Wang%0AAbstract%3A%20%20%20Score%20Distillation%20Sampling%20%28SDS%29%20has%20made%20significant%20strides%20in%20distilling%0Aimage-generative%20models%20for%203D%20generation.%20However%2C%20its%0Amaximum-likelihood-seeking%20behavior%20often%20leads%20to%20degraded%20visual%20quality%20and%0Adiversity%2C%20limiting%20its%20effectiveness%20in%203D%20applications.%20In%20this%20work%2C%20we%0Apropose%20Consistent%20Flow%20Distillation%20%28CFD%29%2C%20which%20addresses%20these%20limitations.%0AWe%20begin%20by%20leveraging%20the%20gradient%20of%20the%20diffusion%20ODE%20or%20SDE%20sampling%0Aprocess%20to%20guide%20the%203D%20generation.%20From%20the%20gradient-based%20sampling%0Aperspective%2C%20we%20find%20that%20the%20consistency%20of%202D%20image%20flows%20across%20different%0Aviewpoints%20is%20important%20for%20high-quality%203D%20generation.%20To%20achieve%20this%2C%20we%0Aintroduce%20multi-view%20consistent%20Gaussian%20noise%20on%20the%203D%20object%2C%20which%20can%20be%0Arendered%20from%20various%20viewpoints%20to%20compute%20the%20flow%20gradient.%20Our%20experiments%0Ademonstrate%20that%20CFD%2C%20through%20consistent%20flows%2C%20significantly%20outperforms%0Aprevious%20methods%20in%20text-to-3D%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05445v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsistent%2520Flow%2520Distillation%2520for%2520Text-to-3D%2520Generation%26entry.906535625%3DRunjie%2520Yan%2520and%2520Yinbo%2520Chen%2520and%2520Xiaolong%2520Wang%26entry.1292438233%3D%2520%2520Score%2520Distillation%2520Sampling%2520%2528SDS%2529%2520has%2520made%2520significant%2520strides%2520in%2520distilling%250Aimage-generative%2520models%2520for%25203D%2520generation.%2520However%252C%2520its%250Amaximum-likelihood-seeking%2520behavior%2520often%2520leads%2520to%2520degraded%2520visual%2520quality%2520and%250Adiversity%252C%2520limiting%2520its%2520effectiveness%2520in%25203D%2520applications.%2520In%2520this%2520work%252C%2520we%250Apropose%2520Consistent%2520Flow%2520Distillation%2520%2528CFD%2529%252C%2520which%2520addresses%2520these%2520limitations.%250AWe%2520begin%2520by%2520leveraging%2520the%2520gradient%2520of%2520the%2520diffusion%2520ODE%2520or%2520SDE%2520sampling%250Aprocess%2520to%2520guide%2520the%25203D%2520generation.%2520From%2520the%2520gradient-based%2520sampling%250Aperspective%252C%2520we%2520find%2520that%2520the%2520consistency%2520of%25202D%2520image%2520flows%2520across%2520different%250Aviewpoints%2520is%2520important%2520for%2520high-quality%25203D%2520generation.%2520To%2520achieve%2520this%252C%2520we%250Aintroduce%2520multi-view%2520consistent%2520Gaussian%2520noise%2520on%2520the%25203D%2520object%252C%2520which%2520can%2520be%250Arendered%2520from%2520various%2520viewpoints%2520to%2520compute%2520the%2520flow%2520gradient.%2520Our%2520experiments%250Ademonstrate%2520that%2520CFD%252C%2520through%2520consistent%2520flows%252C%2520significantly%2520outperforms%250Aprevious%2520methods%2520in%2520text-to-3D%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05445v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Consistent%20Flow%20Distillation%20for%20Text-to-3D%20Generation&entry.906535625=Runjie%20Yan%20and%20Yinbo%20Chen%20and%20Xiaolong%20Wang&entry.1292438233=%20%20Score%20Distillation%20Sampling%20%28SDS%29%20has%20made%20significant%20strides%20in%20distilling%0Aimage-generative%20models%20for%203D%20generation.%20However%2C%20its%0Amaximum-likelihood-seeking%20behavior%20often%20leads%20to%20degraded%20visual%20quality%20and%0Adiversity%2C%20limiting%20its%20effectiveness%20in%203D%20applications.%20In%20this%20work%2C%20we%0Apropose%20Consistent%20Flow%20Distillation%20%28CFD%29%2C%20which%20addresses%20these%20limitations.%0AWe%20begin%20by%20leveraging%20the%20gradient%20of%20the%20diffusion%20ODE%20or%20SDE%20sampling%0Aprocess%20to%20guide%20the%203D%20generation.%20From%20the%20gradient-based%20sampling%0Aperspective%2C%20we%20find%20that%20the%20consistency%20of%202D%20image%20flows%20across%20different%0Aviewpoints%20is%20important%20for%20high-quality%203D%20generation.%20To%20achieve%20this%2C%20we%0Aintroduce%20multi-view%20consistent%20Gaussian%20noise%20on%20the%203D%20object%2C%20which%20can%20be%0Arendered%20from%20various%20viewpoints%20to%20compute%20the%20flow%20gradient.%20Our%20experiments%0Ademonstrate%20that%20CFD%2C%20through%20consistent%20flows%2C%20significantly%20outperforms%0Aprevious%20methods%20in%20text-to-3D%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05445v1&entry.124074799=Read"},
{"title": "Tailored-LLaMA: Optimizing Few-Shot Learning in Pruned LLaMA Models with\n  Task-Specific Prompts", "author": "Danyal Aftab and Steven Davy", "abstract": "  Large language models demonstrate impressive proficiency in language\nunderstanding and generation. Nonetheless, training these models from scratch,\neven the least complex billion-parameter variant demands significant\ncomputational resources rendering it economically impractical for many\norganizations. With large language models functioning as general-purpose task\nsolvers, this paper investigates their task-specific fine-tuning. We employ\ntask-specific datasets and prompts to fine-tune two pruned LLaMA models having\n5 billion and 4 billion parameters. This process utilizes the pre-trained\nweights and focuses on a subset of weights using the LoRA method. One challenge\nin fine-tuning the LLaMA model is crafting a precise prompt tailored to the\nspecific task. To address this, we propose a novel approach to fine-tune the\nLLaMA model under two primary constraints: task specificity and prompt\neffectiveness. Our approach, Tailored LLaMA initially employs structural\npruning to reduce the model sizes from 7B to 5B and 4B parameters.\nSubsequently, it applies a carefully designed prompt specific to the task and\nutilizes the LoRA method to accelerate the fine-tuning process. Moreover,\nfine-tuning a model pruned by 50\\% for less than one hour restores the mean\naccuracy of classification tasks to 95.68\\% at a 20\\% compression ratio and to\n86.54\\% at a 50\\% compression ratio through few-shot learning with 50 shots.\nOur validation of Tailored LLaMA on these two pruned variants demonstrates that\neven when compressed to 50\\%, the models maintain over 65\\% of the baseline\nmodel accuracy in few-shot classification and generation tasks. These findings\nhighlight the efficacy of our tailored approach in maintaining high performance\nwith significantly reduced model sizes.\n", "link": "http://arxiv.org/abs/2410.19185v2", "date": "2025-01-09", "relevancy": 2.469, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5114}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4861}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tailored-LLaMA%3A%20Optimizing%20Few-Shot%20Learning%20in%20Pruned%20LLaMA%20Models%20with%0A%20%20Task-Specific%20Prompts&body=Title%3A%20Tailored-LLaMA%3A%20Optimizing%20Few-Shot%20Learning%20in%20Pruned%20LLaMA%20Models%20with%0A%20%20Task-Specific%20Prompts%0AAuthor%3A%20Danyal%20Aftab%20and%20Steven%20Davy%0AAbstract%3A%20%20%20Large%20language%20models%20demonstrate%20impressive%20proficiency%20in%20language%0Aunderstanding%20and%20generation.%20Nonetheless%2C%20training%20these%20models%20from%20scratch%2C%0Aeven%20the%20least%20complex%20billion-parameter%20variant%20demands%20significant%0Acomputational%20resources%20rendering%20it%20economically%20impractical%20for%20many%0Aorganizations.%20With%20large%20language%20models%20functioning%20as%20general-purpose%20task%0Asolvers%2C%20this%20paper%20investigates%20their%20task-specific%20fine-tuning.%20We%20employ%0Atask-specific%20datasets%20and%20prompts%20to%20fine-tune%20two%20pruned%20LLaMA%20models%20having%0A5%20billion%20and%204%20billion%20parameters.%20This%20process%20utilizes%20the%20pre-trained%0Aweights%20and%20focuses%20on%20a%20subset%20of%20weights%20using%20the%20LoRA%20method.%20One%20challenge%0Ain%20fine-tuning%20the%20LLaMA%20model%20is%20crafting%20a%20precise%20prompt%20tailored%20to%20the%0Aspecific%20task.%20To%20address%20this%2C%20we%20propose%20a%20novel%20approach%20to%20fine-tune%20the%0ALLaMA%20model%20under%20two%20primary%20constraints%3A%20task%20specificity%20and%20prompt%0Aeffectiveness.%20Our%20approach%2C%20Tailored%20LLaMA%20initially%20employs%20structural%0Apruning%20to%20reduce%20the%20model%20sizes%20from%207B%20to%205B%20and%204B%20parameters.%0ASubsequently%2C%20it%20applies%20a%20carefully%20designed%20prompt%20specific%20to%20the%20task%20and%0Autilizes%20the%20LoRA%20method%20to%20accelerate%20the%20fine-tuning%20process.%20Moreover%2C%0Afine-tuning%20a%20model%20pruned%20by%2050%5C%25%20for%20less%20than%20one%20hour%20restores%20the%20mean%0Aaccuracy%20of%20classification%20tasks%20to%2095.68%5C%25%20at%20a%2020%5C%25%20compression%20ratio%20and%20to%0A86.54%5C%25%20at%20a%2050%5C%25%20compression%20ratio%20through%20few-shot%20learning%20with%2050%20shots.%0AOur%20validation%20of%20Tailored%20LLaMA%20on%20these%20two%20pruned%20variants%20demonstrates%20that%0Aeven%20when%20compressed%20to%2050%5C%25%2C%20the%20models%20maintain%20over%2065%5C%25%20of%20the%20baseline%0Amodel%20accuracy%20in%20few-shot%20classification%20and%20generation%20tasks.%20These%20findings%0Ahighlight%20the%20efficacy%20of%20our%20tailored%20approach%20in%20maintaining%20high%20performance%0Awith%20significantly%20reduced%20model%20sizes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19185v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTailored-LLaMA%253A%2520Optimizing%2520Few-Shot%2520Learning%2520in%2520Pruned%2520LLaMA%2520Models%2520with%250A%2520%2520Task-Specific%2520Prompts%26entry.906535625%3DDanyal%2520Aftab%2520and%2520Steven%2520Davy%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520demonstrate%2520impressive%2520proficiency%2520in%2520language%250Aunderstanding%2520and%2520generation.%2520Nonetheless%252C%2520training%2520these%2520models%2520from%2520scratch%252C%250Aeven%2520the%2520least%2520complex%2520billion-parameter%2520variant%2520demands%2520significant%250Acomputational%2520resources%2520rendering%2520it%2520economically%2520impractical%2520for%2520many%250Aorganizations.%2520With%2520large%2520language%2520models%2520functioning%2520as%2520general-purpose%2520task%250Asolvers%252C%2520this%2520paper%2520investigates%2520their%2520task-specific%2520fine-tuning.%2520We%2520employ%250Atask-specific%2520datasets%2520and%2520prompts%2520to%2520fine-tune%2520two%2520pruned%2520LLaMA%2520models%2520having%250A5%2520billion%2520and%25204%2520billion%2520parameters.%2520This%2520process%2520utilizes%2520the%2520pre-trained%250Aweights%2520and%2520focuses%2520on%2520a%2520subset%2520of%2520weights%2520using%2520the%2520LoRA%2520method.%2520One%2520challenge%250Ain%2520fine-tuning%2520the%2520LLaMA%2520model%2520is%2520crafting%2520a%2520precise%2520prompt%2520tailored%2520to%2520the%250Aspecific%2520task.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520novel%2520approach%2520to%2520fine-tune%2520the%250ALLaMA%2520model%2520under%2520two%2520primary%2520constraints%253A%2520task%2520specificity%2520and%2520prompt%250Aeffectiveness.%2520Our%2520approach%252C%2520Tailored%2520LLaMA%2520initially%2520employs%2520structural%250Apruning%2520to%2520reduce%2520the%2520model%2520sizes%2520from%25207B%2520to%25205B%2520and%25204B%2520parameters.%250ASubsequently%252C%2520it%2520applies%2520a%2520carefully%2520designed%2520prompt%2520specific%2520to%2520the%2520task%2520and%250Autilizes%2520the%2520LoRA%2520method%2520to%2520accelerate%2520the%2520fine-tuning%2520process.%2520Moreover%252C%250Afine-tuning%2520a%2520model%2520pruned%2520by%252050%255C%2525%2520for%2520less%2520than%2520one%2520hour%2520restores%2520the%2520mean%250Aaccuracy%2520of%2520classification%2520tasks%2520to%252095.68%255C%2525%2520at%2520a%252020%255C%2525%2520compression%2520ratio%2520and%2520to%250A86.54%255C%2525%2520at%2520a%252050%255C%2525%2520compression%2520ratio%2520through%2520few-shot%2520learning%2520with%252050%2520shots.%250AOur%2520validation%2520of%2520Tailored%2520LLaMA%2520on%2520these%2520two%2520pruned%2520variants%2520demonstrates%2520that%250Aeven%2520when%2520compressed%2520to%252050%255C%2525%252C%2520the%2520models%2520maintain%2520over%252065%255C%2525%2520of%2520the%2520baseline%250Amodel%2520accuracy%2520in%2520few-shot%2520classification%2520and%2520generation%2520tasks.%2520These%2520findings%250Ahighlight%2520the%2520efficacy%2520of%2520our%2520tailored%2520approach%2520in%2520maintaining%2520high%2520performance%250Awith%2520significantly%2520reduced%2520model%2520sizes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19185v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tailored-LLaMA%3A%20Optimizing%20Few-Shot%20Learning%20in%20Pruned%20LLaMA%20Models%20with%0A%20%20Task-Specific%20Prompts&entry.906535625=Danyal%20Aftab%20and%20Steven%20Davy&entry.1292438233=%20%20Large%20language%20models%20demonstrate%20impressive%20proficiency%20in%20language%0Aunderstanding%20and%20generation.%20Nonetheless%2C%20training%20these%20models%20from%20scratch%2C%0Aeven%20the%20least%20complex%20billion-parameter%20variant%20demands%20significant%0Acomputational%20resources%20rendering%20it%20economically%20impractical%20for%20many%0Aorganizations.%20With%20large%20language%20models%20functioning%20as%20general-purpose%20task%0Asolvers%2C%20this%20paper%20investigates%20their%20task-specific%20fine-tuning.%20We%20employ%0Atask-specific%20datasets%20and%20prompts%20to%20fine-tune%20two%20pruned%20LLaMA%20models%20having%0A5%20billion%20and%204%20billion%20parameters.%20This%20process%20utilizes%20the%20pre-trained%0Aweights%20and%20focuses%20on%20a%20subset%20of%20weights%20using%20the%20LoRA%20method.%20One%20challenge%0Ain%20fine-tuning%20the%20LLaMA%20model%20is%20crafting%20a%20precise%20prompt%20tailored%20to%20the%0Aspecific%20task.%20To%20address%20this%2C%20we%20propose%20a%20novel%20approach%20to%20fine-tune%20the%0ALLaMA%20model%20under%20two%20primary%20constraints%3A%20task%20specificity%20and%20prompt%0Aeffectiveness.%20Our%20approach%2C%20Tailored%20LLaMA%20initially%20employs%20structural%0Apruning%20to%20reduce%20the%20model%20sizes%20from%207B%20to%205B%20and%204B%20parameters.%0ASubsequently%2C%20it%20applies%20a%20carefully%20designed%20prompt%20specific%20to%20the%20task%20and%0Autilizes%20the%20LoRA%20method%20to%20accelerate%20the%20fine-tuning%20process.%20Moreover%2C%0Afine-tuning%20a%20model%20pruned%20by%2050%5C%25%20for%20less%20than%20one%20hour%20restores%20the%20mean%0Aaccuracy%20of%20classification%20tasks%20to%2095.68%5C%25%20at%20a%2020%5C%25%20compression%20ratio%20and%20to%0A86.54%5C%25%20at%20a%2050%5C%25%20compression%20ratio%20through%20few-shot%20learning%20with%2050%20shots.%0AOur%20validation%20of%20Tailored%20LLaMA%20on%20these%20two%20pruned%20variants%20demonstrates%20that%0Aeven%20when%20compressed%20to%2050%5C%25%2C%20the%20models%20maintain%20over%2065%5C%25%20of%20the%20baseline%0Amodel%20accuracy%20in%20few-shot%20classification%20and%20generation%20tasks.%20These%20findings%0Ahighlight%20the%20efficacy%20of%20our%20tailored%20approach%20in%20maintaining%20high%20performance%0Awith%20significantly%20reduced%20model%20sizes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19185v2&entry.124074799=Read"},
{"title": "1-2-1: Renaissance of Single-Network Paradigm for Virtual Try-On", "author": "Shuliang Ning and Yipeng Qin and Xiaoguang Han", "abstract": "  Virtual Try-On (VTON) has become a crucial tool in ecommerce, enabling the\nrealistic simulation of garments on individuals while preserving their original\nappearance and pose. Early VTON methods relied on single generative networks,\nbut challenges remain in preserving fine-grained garment details due to\nlimitations in feature extraction and fusion. To address these issues, recent\napproaches have adopted a dual-network paradigm, incorporating a complementary\n\"ReferenceNet\" to enhance garment feature extraction and fusion. While\neffective, this dual-network approach introduces significant computational\noverhead, limiting its scalability for high-resolution and long-duration\nimage/video VTON applications. In this paper, we challenge the dual-network\nparadigm by proposing a novel single-network VTON method that overcomes the\nlimitations of existing techniques. Our method, namely MNVTON, introduces a\nModality-specific Normalization strategy that separately processes text, image\nand video inputs, enabling them to share the same attention layers in a VTON\nnetwork. Extensive experimental results demonstrate the effectiveness of our\napproach, showing that it consistently achieves higher-quality, more detailed\nresults for both image and video VTON tasks. Our results suggest that the\nsingle-network paradigm can rival the performance of dualnetwork approaches,\noffering a more efficient alternative for high-quality, scalable VTON\napplications.\n", "link": "http://arxiv.org/abs/2501.05369v1", "date": "2025-01-09", "relevancy": 2.433, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.628}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6147}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%201-2-1%3A%20Renaissance%20of%20Single-Network%20Paradigm%20for%20Virtual%20Try-On&body=Title%3A%201-2-1%3A%20Renaissance%20of%20Single-Network%20Paradigm%20for%20Virtual%20Try-On%0AAuthor%3A%20Shuliang%20Ning%20and%20Yipeng%20Qin%20and%20Xiaoguang%20Han%0AAbstract%3A%20%20%20Virtual%20Try-On%20%28VTON%29%20has%20become%20a%20crucial%20tool%20in%20ecommerce%2C%20enabling%20the%0Arealistic%20simulation%20of%20garments%20on%20individuals%20while%20preserving%20their%20original%0Aappearance%20and%20pose.%20Early%20VTON%20methods%20relied%20on%20single%20generative%20networks%2C%0Abut%20challenges%20remain%20in%20preserving%20fine-grained%20garment%20details%20due%20to%0Alimitations%20in%20feature%20extraction%20and%20fusion.%20To%20address%20these%20issues%2C%20recent%0Aapproaches%20have%20adopted%20a%20dual-network%20paradigm%2C%20incorporating%20a%20complementary%0A%22ReferenceNet%22%20to%20enhance%20garment%20feature%20extraction%20and%20fusion.%20While%0Aeffective%2C%20this%20dual-network%20approach%20introduces%20significant%20computational%0Aoverhead%2C%20limiting%20its%20scalability%20for%20high-resolution%20and%20long-duration%0Aimage/video%20VTON%20applications.%20In%20this%20paper%2C%20we%20challenge%20the%20dual-network%0Aparadigm%20by%20proposing%20a%20novel%20single-network%20VTON%20method%20that%20overcomes%20the%0Alimitations%20of%20existing%20techniques.%20Our%20method%2C%20namely%20MNVTON%2C%20introduces%20a%0AModality-specific%20Normalization%20strategy%20that%20separately%20processes%20text%2C%20image%0Aand%20video%20inputs%2C%20enabling%20them%20to%20share%20the%20same%20attention%20layers%20in%20a%20VTON%0Anetwork.%20Extensive%20experimental%20results%20demonstrate%20the%20effectiveness%20of%20our%0Aapproach%2C%20showing%20that%20it%20consistently%20achieves%20higher-quality%2C%20more%20detailed%0Aresults%20for%20both%20image%20and%20video%20VTON%20tasks.%20Our%20results%20suggest%20that%20the%0Asingle-network%20paradigm%20can%20rival%20the%20performance%20of%20dualnetwork%20approaches%2C%0Aoffering%20a%20more%20efficient%20alternative%20for%20high-quality%2C%20scalable%20VTON%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05369v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D1-2-1%253A%2520Renaissance%2520of%2520Single-Network%2520Paradigm%2520for%2520Virtual%2520Try-On%26entry.906535625%3DShuliang%2520Ning%2520and%2520Yipeng%2520Qin%2520and%2520Xiaoguang%2520Han%26entry.1292438233%3D%2520%2520Virtual%2520Try-On%2520%2528VTON%2529%2520has%2520become%2520a%2520crucial%2520tool%2520in%2520ecommerce%252C%2520enabling%2520the%250Arealistic%2520simulation%2520of%2520garments%2520on%2520individuals%2520while%2520preserving%2520their%2520original%250Aappearance%2520and%2520pose.%2520Early%2520VTON%2520methods%2520relied%2520on%2520single%2520generative%2520networks%252C%250Abut%2520challenges%2520remain%2520in%2520preserving%2520fine-grained%2520garment%2520details%2520due%2520to%250Alimitations%2520in%2520feature%2520extraction%2520and%2520fusion.%2520To%2520address%2520these%2520issues%252C%2520recent%250Aapproaches%2520have%2520adopted%2520a%2520dual-network%2520paradigm%252C%2520incorporating%2520a%2520complementary%250A%2522ReferenceNet%2522%2520to%2520enhance%2520garment%2520feature%2520extraction%2520and%2520fusion.%2520While%250Aeffective%252C%2520this%2520dual-network%2520approach%2520introduces%2520significant%2520computational%250Aoverhead%252C%2520limiting%2520its%2520scalability%2520for%2520high-resolution%2520and%2520long-duration%250Aimage/video%2520VTON%2520applications.%2520In%2520this%2520paper%252C%2520we%2520challenge%2520the%2520dual-network%250Aparadigm%2520by%2520proposing%2520a%2520novel%2520single-network%2520VTON%2520method%2520that%2520overcomes%2520the%250Alimitations%2520of%2520existing%2520techniques.%2520Our%2520method%252C%2520namely%2520MNVTON%252C%2520introduces%2520a%250AModality-specific%2520Normalization%2520strategy%2520that%2520separately%2520processes%2520text%252C%2520image%250Aand%2520video%2520inputs%252C%2520enabling%2520them%2520to%2520share%2520the%2520same%2520attention%2520layers%2520in%2520a%2520VTON%250Anetwork.%2520Extensive%2520experimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Aapproach%252C%2520showing%2520that%2520it%2520consistently%2520achieves%2520higher-quality%252C%2520more%2520detailed%250Aresults%2520for%2520both%2520image%2520and%2520video%2520VTON%2520tasks.%2520Our%2520results%2520suggest%2520that%2520the%250Asingle-network%2520paradigm%2520can%2520rival%2520the%2520performance%2520of%2520dualnetwork%2520approaches%252C%250Aoffering%2520a%2520more%2520efficient%2520alternative%2520for%2520high-quality%252C%2520scalable%2520VTON%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05369v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=1-2-1%3A%20Renaissance%20of%20Single-Network%20Paradigm%20for%20Virtual%20Try-On&entry.906535625=Shuliang%20Ning%20and%20Yipeng%20Qin%20and%20Xiaoguang%20Han&entry.1292438233=%20%20Virtual%20Try-On%20%28VTON%29%20has%20become%20a%20crucial%20tool%20in%20ecommerce%2C%20enabling%20the%0Arealistic%20simulation%20of%20garments%20on%20individuals%20while%20preserving%20their%20original%0Aappearance%20and%20pose.%20Early%20VTON%20methods%20relied%20on%20single%20generative%20networks%2C%0Abut%20challenges%20remain%20in%20preserving%20fine-grained%20garment%20details%20due%20to%0Alimitations%20in%20feature%20extraction%20and%20fusion.%20To%20address%20these%20issues%2C%20recent%0Aapproaches%20have%20adopted%20a%20dual-network%20paradigm%2C%20incorporating%20a%20complementary%0A%22ReferenceNet%22%20to%20enhance%20garment%20feature%20extraction%20and%20fusion.%20While%0Aeffective%2C%20this%20dual-network%20approach%20introduces%20significant%20computational%0Aoverhead%2C%20limiting%20its%20scalability%20for%20high-resolution%20and%20long-duration%0Aimage/video%20VTON%20applications.%20In%20this%20paper%2C%20we%20challenge%20the%20dual-network%0Aparadigm%20by%20proposing%20a%20novel%20single-network%20VTON%20method%20that%20overcomes%20the%0Alimitations%20of%20existing%20techniques.%20Our%20method%2C%20namely%20MNVTON%2C%20introduces%20a%0AModality-specific%20Normalization%20strategy%20that%20separately%20processes%20text%2C%20image%0Aand%20video%20inputs%2C%20enabling%20them%20to%20share%20the%20same%20attention%20layers%20in%20a%20VTON%0Anetwork.%20Extensive%20experimental%20results%20demonstrate%20the%20effectiveness%20of%20our%0Aapproach%2C%20showing%20that%20it%20consistently%20achieves%20higher-quality%2C%20more%20detailed%0Aresults%20for%20both%20image%20and%20video%20VTON%20tasks.%20Our%20results%20suggest%20that%20the%0Asingle-network%20paradigm%20can%20rival%20the%20performance%20of%20dualnetwork%20approaches%2C%0Aoffering%20a%20more%20efficient%20alternative%20for%20high-quality%2C%20scalable%20VTON%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05369v1&entry.124074799=Read"},
{"title": "Interpreting Deep Neural Network-Based Receiver Under Varying\n  Signal-To-Noise Ratios", "author": "Marko Tuononen and Dani Korpi and Ville Hautam\u00e4ki", "abstract": "  We propose a novel method for interpreting neural networks, focusing on\nconvolutional neural network-based receiver model. The method identifies which\nunit or units of the model contain most (or least) information about the\nchannel parameter(s) of the interest, providing insights at both global and\nlocal levels -- with global explanations aggregating local ones. Experiments on\nlink-level simulations demonstrate the method's effectiveness in identifying\nunits that contribute most (and least) to signal-to-noise ratio processing.\nAlthough we focus on a radio receiver model, the method generalizes to other\nneural network architectures and applications, offering robust estimation even\nin high-dimensional settings.\n", "link": "http://arxiv.org/abs/2409.16768v2", "date": "2025-01-09", "relevancy": 2.4198, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4881}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4881}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpreting%20Deep%20Neural%20Network-Based%20Receiver%20Under%20Varying%0A%20%20Signal-To-Noise%20Ratios&body=Title%3A%20Interpreting%20Deep%20Neural%20Network-Based%20Receiver%20Under%20Varying%0A%20%20Signal-To-Noise%20Ratios%0AAuthor%3A%20Marko%20Tuononen%20and%20Dani%20Korpi%20and%20Ville%20Hautam%C3%A4ki%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20method%20for%20interpreting%20neural%20networks%2C%20focusing%20on%0Aconvolutional%20neural%20network-based%20receiver%20model.%20The%20method%20identifies%20which%0Aunit%20or%20units%20of%20the%20model%20contain%20most%20%28or%20least%29%20information%20about%20the%0Achannel%20parameter%28s%29%20of%20the%20interest%2C%20providing%20insights%20at%20both%20global%20and%0Alocal%20levels%20--%20with%20global%20explanations%20aggregating%20local%20ones.%20Experiments%20on%0Alink-level%20simulations%20demonstrate%20the%20method%27s%20effectiveness%20in%20identifying%0Aunits%20that%20contribute%20most%20%28and%20least%29%20to%20signal-to-noise%20ratio%20processing.%0AAlthough%20we%20focus%20on%20a%20radio%20receiver%20model%2C%20the%20method%20generalizes%20to%20other%0Aneural%20network%20architectures%20and%20applications%2C%20offering%20robust%20estimation%20even%0Ain%20high-dimensional%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16768v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpreting%2520Deep%2520Neural%2520Network-Based%2520Receiver%2520Under%2520Varying%250A%2520%2520Signal-To-Noise%2520Ratios%26entry.906535625%3DMarko%2520Tuononen%2520and%2520Dani%2520Korpi%2520and%2520Ville%2520Hautam%25C3%25A4ki%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520method%2520for%2520interpreting%2520neural%2520networks%252C%2520focusing%2520on%250Aconvolutional%2520neural%2520network-based%2520receiver%2520model.%2520The%2520method%2520identifies%2520which%250Aunit%2520or%2520units%2520of%2520the%2520model%2520contain%2520most%2520%2528or%2520least%2529%2520information%2520about%2520the%250Achannel%2520parameter%2528s%2529%2520of%2520the%2520interest%252C%2520providing%2520insights%2520at%2520both%2520global%2520and%250Alocal%2520levels%2520--%2520with%2520global%2520explanations%2520aggregating%2520local%2520ones.%2520Experiments%2520on%250Alink-level%2520simulations%2520demonstrate%2520the%2520method%2527s%2520effectiveness%2520in%2520identifying%250Aunits%2520that%2520contribute%2520most%2520%2528and%2520least%2529%2520to%2520signal-to-noise%2520ratio%2520processing.%250AAlthough%2520we%2520focus%2520on%2520a%2520radio%2520receiver%2520model%252C%2520the%2520method%2520generalizes%2520to%2520other%250Aneural%2520network%2520architectures%2520and%2520applications%252C%2520offering%2520robust%2520estimation%2520even%250Ain%2520high-dimensional%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16768v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpreting%20Deep%20Neural%20Network-Based%20Receiver%20Under%20Varying%0A%20%20Signal-To-Noise%20Ratios&entry.906535625=Marko%20Tuononen%20and%20Dani%20Korpi%20and%20Ville%20Hautam%C3%A4ki&entry.1292438233=%20%20We%20propose%20a%20novel%20method%20for%20interpreting%20neural%20networks%2C%20focusing%20on%0Aconvolutional%20neural%20network-based%20receiver%20model.%20The%20method%20identifies%20which%0Aunit%20or%20units%20of%20the%20model%20contain%20most%20%28or%20least%29%20information%20about%20the%0Achannel%20parameter%28s%29%20of%20the%20interest%2C%20providing%20insights%20at%20both%20global%20and%0Alocal%20levels%20--%20with%20global%20explanations%20aggregating%20local%20ones.%20Experiments%20on%0Alink-level%20simulations%20demonstrate%20the%20method%27s%20effectiveness%20in%20identifying%0Aunits%20that%20contribute%20most%20%28and%20least%29%20to%20signal-to-noise%20ratio%20processing.%0AAlthough%20we%20focus%20on%20a%20radio%20receiver%20model%2C%20the%20method%20generalizes%20to%20other%0Aneural%20network%20architectures%20and%20applications%2C%20offering%20robust%20estimation%20even%0Ain%20high-dimensional%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16768v2&entry.124074799=Read"},
{"title": "Identity-Preserving Video Dubbing Using Motion Warping", "author": "Runzhen Liu and Qinjie Lin and Yunfei Liu and Lijian Lin and Ye Zhu and Yu Li and Chuhua Xian and Fa-Ting Hong", "abstract": "  Video dubbing aims to synthesize realistic, lip-synced videos from a\nreference video and a driving audio signal. Although existing methods can\naccurately generate mouth shapes driven by audio, they often fail to preserve\nidentity-specific features, largely because they do not effectively capture the\nnuanced interplay between audio cues and the visual attributes of reference\nidentity . As a result, the generated outputs frequently lack fidelity in\nreproducing the unique textural and structural details of the reference\nidentity. To address these limitations, we propose IPTalker, a novel and robust\nframework for video dubbing that achieves seamless alignment between driving\naudio and reference identity while ensuring both lip-sync accuracy and\nhigh-fidelity identity preservation. At the core of IPTalker is a\ntransformer-based alignment mechanism designed to dynamically capture and model\nthe correspondence between audio features and reference images, thereby\nenabling precise, identity-aware audio-visual integration. Building on this\nalignment, a motion warping strategy further refines the results by spatially\ndeforming reference images to match the target audio-driven configuration. A\ndedicated refinement process then mitigates occlusion artifacts and enhances\nthe preservation of fine-grained textures, such as mouth details and skin\nfeatures. Extensive qualitative and quantitative evaluations demonstrate that\nIPTalker consistently outperforms existing approaches in terms of realism, lip\nsynchronization, and identity retention, establishing a new state of the art\nfor high-quality, identity-consistent video dubbing.\n", "link": "http://arxiv.org/abs/2501.04586v2", "date": "2025-01-09", "relevancy": 2.4182, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6341}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6179}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identity-Preserving%20Video%20Dubbing%20Using%20Motion%20Warping&body=Title%3A%20Identity-Preserving%20Video%20Dubbing%20Using%20Motion%20Warping%0AAuthor%3A%20Runzhen%20Liu%20and%20Qinjie%20Lin%20and%20Yunfei%20Liu%20and%20Lijian%20Lin%20and%20Ye%20Zhu%20and%20Yu%20Li%20and%20Chuhua%20Xian%20and%20Fa-Ting%20Hong%0AAbstract%3A%20%20%20Video%20dubbing%20aims%20to%20synthesize%20realistic%2C%20lip-synced%20videos%20from%20a%0Areference%20video%20and%20a%20driving%20audio%20signal.%20Although%20existing%20methods%20can%0Aaccurately%20generate%20mouth%20shapes%20driven%20by%20audio%2C%20they%20often%20fail%20to%20preserve%0Aidentity-specific%20features%2C%20largely%20because%20they%20do%20not%20effectively%20capture%20the%0Anuanced%20interplay%20between%20audio%20cues%20and%20the%20visual%20attributes%20of%20reference%0Aidentity%20.%20As%20a%20result%2C%20the%20generated%20outputs%20frequently%20lack%20fidelity%20in%0Areproducing%20the%20unique%20textural%20and%20structural%20details%20of%20the%20reference%0Aidentity.%20To%20address%20these%20limitations%2C%20we%20propose%20IPTalker%2C%20a%20novel%20and%20robust%0Aframework%20for%20video%20dubbing%20that%20achieves%20seamless%20alignment%20between%20driving%0Aaudio%20and%20reference%20identity%20while%20ensuring%20both%20lip-sync%20accuracy%20and%0Ahigh-fidelity%20identity%20preservation.%20At%20the%20core%20of%20IPTalker%20is%20a%0Atransformer-based%20alignment%20mechanism%20designed%20to%20dynamically%20capture%20and%20model%0Athe%20correspondence%20between%20audio%20features%20and%20reference%20images%2C%20thereby%0Aenabling%20precise%2C%20identity-aware%20audio-visual%20integration.%20Building%20on%20this%0Aalignment%2C%20a%20motion%20warping%20strategy%20further%20refines%20the%20results%20by%20spatially%0Adeforming%20reference%20images%20to%20match%20the%20target%20audio-driven%20configuration.%20A%0Adedicated%20refinement%20process%20then%20mitigates%20occlusion%20artifacts%20and%20enhances%0Athe%20preservation%20of%20fine-grained%20textures%2C%20such%20as%20mouth%20details%20and%20skin%0Afeatures.%20Extensive%20qualitative%20and%20quantitative%20evaluations%20demonstrate%20that%0AIPTalker%20consistently%20outperforms%20existing%20approaches%20in%20terms%20of%20realism%2C%20lip%0Asynchronization%2C%20and%20identity%20retention%2C%20establishing%20a%20new%20state%20of%20the%20art%0Afor%20high-quality%2C%20identity-consistent%20video%20dubbing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04586v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentity-Preserving%2520Video%2520Dubbing%2520Using%2520Motion%2520Warping%26entry.906535625%3DRunzhen%2520Liu%2520and%2520Qinjie%2520Lin%2520and%2520Yunfei%2520Liu%2520and%2520Lijian%2520Lin%2520and%2520Ye%2520Zhu%2520and%2520Yu%2520Li%2520and%2520Chuhua%2520Xian%2520and%2520Fa-Ting%2520Hong%26entry.1292438233%3D%2520%2520Video%2520dubbing%2520aims%2520to%2520synthesize%2520realistic%252C%2520lip-synced%2520videos%2520from%2520a%250Areference%2520video%2520and%2520a%2520driving%2520audio%2520signal.%2520Although%2520existing%2520methods%2520can%250Aaccurately%2520generate%2520mouth%2520shapes%2520driven%2520by%2520audio%252C%2520they%2520often%2520fail%2520to%2520preserve%250Aidentity-specific%2520features%252C%2520largely%2520because%2520they%2520do%2520not%2520effectively%2520capture%2520the%250Anuanced%2520interplay%2520between%2520audio%2520cues%2520and%2520the%2520visual%2520attributes%2520of%2520reference%250Aidentity%2520.%2520As%2520a%2520result%252C%2520the%2520generated%2520outputs%2520frequently%2520lack%2520fidelity%2520in%250Areproducing%2520the%2520unique%2520textural%2520and%2520structural%2520details%2520of%2520the%2520reference%250Aidentity.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520IPTalker%252C%2520a%2520novel%2520and%2520robust%250Aframework%2520for%2520video%2520dubbing%2520that%2520achieves%2520seamless%2520alignment%2520between%2520driving%250Aaudio%2520and%2520reference%2520identity%2520while%2520ensuring%2520both%2520lip-sync%2520accuracy%2520and%250Ahigh-fidelity%2520identity%2520preservation.%2520At%2520the%2520core%2520of%2520IPTalker%2520is%2520a%250Atransformer-based%2520alignment%2520mechanism%2520designed%2520to%2520dynamically%2520capture%2520and%2520model%250Athe%2520correspondence%2520between%2520audio%2520features%2520and%2520reference%2520images%252C%2520thereby%250Aenabling%2520precise%252C%2520identity-aware%2520audio-visual%2520integration.%2520Building%2520on%2520this%250Aalignment%252C%2520a%2520motion%2520warping%2520strategy%2520further%2520refines%2520the%2520results%2520by%2520spatially%250Adeforming%2520reference%2520images%2520to%2520match%2520the%2520target%2520audio-driven%2520configuration.%2520A%250Adedicated%2520refinement%2520process%2520then%2520mitigates%2520occlusion%2520artifacts%2520and%2520enhances%250Athe%2520preservation%2520of%2520fine-grained%2520textures%252C%2520such%2520as%2520mouth%2520details%2520and%2520skin%250Afeatures.%2520Extensive%2520qualitative%2520and%2520quantitative%2520evaluations%2520demonstrate%2520that%250AIPTalker%2520consistently%2520outperforms%2520existing%2520approaches%2520in%2520terms%2520of%2520realism%252C%2520lip%250Asynchronization%252C%2520and%2520identity%2520retention%252C%2520establishing%2520a%2520new%2520state%2520of%2520the%2520art%250Afor%2520high-quality%252C%2520identity-consistent%2520video%2520dubbing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04586v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identity-Preserving%20Video%20Dubbing%20Using%20Motion%20Warping&entry.906535625=Runzhen%20Liu%20and%20Qinjie%20Lin%20and%20Yunfei%20Liu%20and%20Lijian%20Lin%20and%20Ye%20Zhu%20and%20Yu%20Li%20and%20Chuhua%20Xian%20and%20Fa-Ting%20Hong&entry.1292438233=%20%20Video%20dubbing%20aims%20to%20synthesize%20realistic%2C%20lip-synced%20videos%20from%20a%0Areference%20video%20and%20a%20driving%20audio%20signal.%20Although%20existing%20methods%20can%0Aaccurately%20generate%20mouth%20shapes%20driven%20by%20audio%2C%20they%20often%20fail%20to%20preserve%0Aidentity-specific%20features%2C%20largely%20because%20they%20do%20not%20effectively%20capture%20the%0Anuanced%20interplay%20between%20audio%20cues%20and%20the%20visual%20attributes%20of%20reference%0Aidentity%20.%20As%20a%20result%2C%20the%20generated%20outputs%20frequently%20lack%20fidelity%20in%0Areproducing%20the%20unique%20textural%20and%20structural%20details%20of%20the%20reference%0Aidentity.%20To%20address%20these%20limitations%2C%20we%20propose%20IPTalker%2C%20a%20novel%20and%20robust%0Aframework%20for%20video%20dubbing%20that%20achieves%20seamless%20alignment%20between%20driving%0Aaudio%20and%20reference%20identity%20while%20ensuring%20both%20lip-sync%20accuracy%20and%0Ahigh-fidelity%20identity%20preservation.%20At%20the%20core%20of%20IPTalker%20is%20a%0Atransformer-based%20alignment%20mechanism%20designed%20to%20dynamically%20capture%20and%20model%0Athe%20correspondence%20between%20audio%20features%20and%20reference%20images%2C%20thereby%0Aenabling%20precise%2C%20identity-aware%20audio-visual%20integration.%20Building%20on%20this%0Aalignment%2C%20a%20motion%20warping%20strategy%20further%20refines%20the%20results%20by%20spatially%0Adeforming%20reference%20images%20to%20match%20the%20target%20audio-driven%20configuration.%20A%0Adedicated%20refinement%20process%20then%20mitigates%20occlusion%20artifacts%20and%20enhances%0Athe%20preservation%20of%20fine-grained%20textures%2C%20such%20as%20mouth%20details%20and%20skin%0Afeatures.%20Extensive%20qualitative%20and%20quantitative%20evaluations%20demonstrate%20that%0AIPTalker%20consistently%20outperforms%20existing%20approaches%20in%20terms%20of%20realism%2C%20lip%0Asynchronization%2C%20and%20identity%20retention%2C%20establishing%20a%20new%20state%20of%20the%20art%0Afor%20high-quality%2C%20identity-consistent%20video%20dubbing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04586v2&entry.124074799=Read"},
{"title": "MHAFF: Multi-Head Attention Feature Fusion of CNN and Transformer for\n  Cattle Identification", "author": "Rabin Dulal and Lihong Zheng and Muhammad Ashad Kabir", "abstract": "  Convolutional Neural Networks (CNNs) have drawn researchers' attention to\nidentifying cattle using muzzle images. However, CNNs often fail to capture\nlong-range dependencies within the complex patterns of the muzzle. The\ntransformers handle these challenges. This inspired us to fuse the strengths of\nCNNs and transformers in muzzle-based cattle identification. Addition and\nconcatenation have been the most commonly used techniques for feature fusion.\nHowever, addition fails to preserve discriminative information, while\nconcatenation results in an increase in dimensionality. Both methods are simple\noperations and cannot discover the relationships or interactions between fusing\nfeatures. This research aims to overcome the issues faced by addition and\nconcatenation. This research introduces a novel approach called Multi-Head\nAttention Feature Fusion (MHAFF) for the first time in cattle identification.\nMHAFF captures relations between the different types of fusing features while\npreserving their originality. The experiments show that MHAFF outperformed\naddition and concatenation techniques and the existing cattle identification\nmethods in accuracy on two publicly available cattle datasets. MHAFF\ndemonstrates excellent performance and quickly converges to achieve optimum\naccuracy of 99.88% and 99.52% in two cattle datasets simultaneously.\n", "link": "http://arxiv.org/abs/2501.05209v1", "date": "2025-01-09", "relevancy": 2.379, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5002}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4651}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MHAFF%3A%20Multi-Head%20Attention%20Feature%20Fusion%20of%20CNN%20and%20Transformer%20for%0A%20%20Cattle%20Identification&body=Title%3A%20MHAFF%3A%20Multi-Head%20Attention%20Feature%20Fusion%20of%20CNN%20and%20Transformer%20for%0A%20%20Cattle%20Identification%0AAuthor%3A%20Rabin%20Dulal%20and%20Lihong%20Zheng%20and%20Muhammad%20Ashad%20Kabir%0AAbstract%3A%20%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20drawn%20researchers%27%20attention%20to%0Aidentifying%20cattle%20using%20muzzle%20images.%20However%2C%20CNNs%20often%20fail%20to%20capture%0Along-range%20dependencies%20within%20the%20complex%20patterns%20of%20the%20muzzle.%20The%0Atransformers%20handle%20these%20challenges.%20This%20inspired%20us%20to%20fuse%20the%20strengths%20of%0ACNNs%20and%20transformers%20in%20muzzle-based%20cattle%20identification.%20Addition%20and%0Aconcatenation%20have%20been%20the%20most%20commonly%20used%20techniques%20for%20feature%20fusion.%0AHowever%2C%20addition%20fails%20to%20preserve%20discriminative%20information%2C%20while%0Aconcatenation%20results%20in%20an%20increase%20in%20dimensionality.%20Both%20methods%20are%20simple%0Aoperations%20and%20cannot%20discover%20the%20relationships%20or%20interactions%20between%20fusing%0Afeatures.%20This%20research%20aims%20to%20overcome%20the%20issues%20faced%20by%20addition%20and%0Aconcatenation.%20This%20research%20introduces%20a%20novel%20approach%20called%20Multi-Head%0AAttention%20Feature%20Fusion%20%28MHAFF%29%20for%20the%20first%20time%20in%20cattle%20identification.%0AMHAFF%20captures%20relations%20between%20the%20different%20types%20of%20fusing%20features%20while%0Apreserving%20their%20originality.%20The%20experiments%20show%20that%20MHAFF%20outperformed%0Aaddition%20and%20concatenation%20techniques%20and%20the%20existing%20cattle%20identification%0Amethods%20in%20accuracy%20on%20two%20publicly%20available%20cattle%20datasets.%20MHAFF%0Ademonstrates%20excellent%20performance%20and%20quickly%20converges%20to%20achieve%20optimum%0Aaccuracy%20of%2099.88%25%20and%2099.52%25%20in%20two%20cattle%20datasets%20simultaneously.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05209v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMHAFF%253A%2520Multi-Head%2520Attention%2520Feature%2520Fusion%2520of%2520CNN%2520and%2520Transformer%2520for%250A%2520%2520Cattle%2520Identification%26entry.906535625%3DRabin%2520Dulal%2520and%2520Lihong%2520Zheng%2520and%2520Muhammad%2520Ashad%2520Kabir%26entry.1292438233%3D%2520%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520have%2520drawn%2520researchers%2527%2520attention%2520to%250Aidentifying%2520cattle%2520using%2520muzzle%2520images.%2520However%252C%2520CNNs%2520often%2520fail%2520to%2520capture%250Along-range%2520dependencies%2520within%2520the%2520complex%2520patterns%2520of%2520the%2520muzzle.%2520The%250Atransformers%2520handle%2520these%2520challenges.%2520This%2520inspired%2520us%2520to%2520fuse%2520the%2520strengths%2520of%250ACNNs%2520and%2520transformers%2520in%2520muzzle-based%2520cattle%2520identification.%2520Addition%2520and%250Aconcatenation%2520have%2520been%2520the%2520most%2520commonly%2520used%2520techniques%2520for%2520feature%2520fusion.%250AHowever%252C%2520addition%2520fails%2520to%2520preserve%2520discriminative%2520information%252C%2520while%250Aconcatenation%2520results%2520in%2520an%2520increase%2520in%2520dimensionality.%2520Both%2520methods%2520are%2520simple%250Aoperations%2520and%2520cannot%2520discover%2520the%2520relationships%2520or%2520interactions%2520between%2520fusing%250Afeatures.%2520This%2520research%2520aims%2520to%2520overcome%2520the%2520issues%2520faced%2520by%2520addition%2520and%250Aconcatenation.%2520This%2520research%2520introduces%2520a%2520novel%2520approach%2520called%2520Multi-Head%250AAttention%2520Feature%2520Fusion%2520%2528MHAFF%2529%2520for%2520the%2520first%2520time%2520in%2520cattle%2520identification.%250AMHAFF%2520captures%2520relations%2520between%2520the%2520different%2520types%2520of%2520fusing%2520features%2520while%250Apreserving%2520their%2520originality.%2520The%2520experiments%2520show%2520that%2520MHAFF%2520outperformed%250Aaddition%2520and%2520concatenation%2520techniques%2520and%2520the%2520existing%2520cattle%2520identification%250Amethods%2520in%2520accuracy%2520on%2520two%2520publicly%2520available%2520cattle%2520datasets.%2520MHAFF%250Ademonstrates%2520excellent%2520performance%2520and%2520quickly%2520converges%2520to%2520achieve%2520optimum%250Aaccuracy%2520of%252099.88%2525%2520and%252099.52%2525%2520in%2520two%2520cattle%2520datasets%2520simultaneously.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05209v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MHAFF%3A%20Multi-Head%20Attention%20Feature%20Fusion%20of%20CNN%20and%20Transformer%20for%0A%20%20Cattle%20Identification&entry.906535625=Rabin%20Dulal%20and%20Lihong%20Zheng%20and%20Muhammad%20Ashad%20Kabir&entry.1292438233=%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20drawn%20researchers%27%20attention%20to%0Aidentifying%20cattle%20using%20muzzle%20images.%20However%2C%20CNNs%20often%20fail%20to%20capture%0Along-range%20dependencies%20within%20the%20complex%20patterns%20of%20the%20muzzle.%20The%0Atransformers%20handle%20these%20challenges.%20This%20inspired%20us%20to%20fuse%20the%20strengths%20of%0ACNNs%20and%20transformers%20in%20muzzle-based%20cattle%20identification.%20Addition%20and%0Aconcatenation%20have%20been%20the%20most%20commonly%20used%20techniques%20for%20feature%20fusion.%0AHowever%2C%20addition%20fails%20to%20preserve%20discriminative%20information%2C%20while%0Aconcatenation%20results%20in%20an%20increase%20in%20dimensionality.%20Both%20methods%20are%20simple%0Aoperations%20and%20cannot%20discover%20the%20relationships%20or%20interactions%20between%20fusing%0Afeatures.%20This%20research%20aims%20to%20overcome%20the%20issues%20faced%20by%20addition%20and%0Aconcatenation.%20This%20research%20introduces%20a%20novel%20approach%20called%20Multi-Head%0AAttention%20Feature%20Fusion%20%28MHAFF%29%20for%20the%20first%20time%20in%20cattle%20identification.%0AMHAFF%20captures%20relations%20between%20the%20different%20types%20of%20fusing%20features%20while%0Apreserving%20their%20originality.%20The%20experiments%20show%20that%20MHAFF%20outperformed%0Aaddition%20and%20concatenation%20techniques%20and%20the%20existing%20cattle%20identification%0Amethods%20in%20accuracy%20on%20two%20publicly%20available%20cattle%20datasets.%20MHAFF%0Ademonstrates%20excellent%20performance%20and%20quickly%20converges%20to%20achieve%20optimum%0Aaccuracy%20of%2099.88%25%20and%2099.52%25%20in%20two%20cattle%20datasets%20simultaneously.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05209v1&entry.124074799=Read"},
{"title": "GLaM-Sign: Greek Language Multimodal Lip Reading with Integrated Sign\n  Language Accessibility", "author": "Dimitris Kouremenos and Klimis Ntalianis", "abstract": "  The Greek Language Multimodal Lip Reading with Integrated Sign Language\nAccessibility (GLaM-Sign) [1] is a groundbreaking resource in accessibility and\nmultimodal AI, designed to support Deaf and Hard-of-Hearing (DHH) individuals.\nDeveloped from the FEELIT project [2], it integrates high-resolution audio,\nvideo, textual transcriptions, and Greek Sign Language translations for\napplications like real-time sign language translation and enhanced subtitle\nsynchronization. While its primary focus is on promoting inclusivity in the\nGreek tourism sector, its adaptability extends to education, healthcare, and\npublic services. Future advancements will enhance word-level precision and\nscalability to additional languages, supported by advanced AI methodologies and\ncollaborations with diverse stakeholders. This dataset underscores the\ntransformative potential of multimodal resources in bridging communication\ngaps, fostering innovation, and setting a benchmark for ethical AI and\ninclusive technologies.\n", "link": "http://arxiv.org/abs/2501.05213v1", "date": "2025-01-09", "relevancy": 2.372, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5213}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4622}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLaM-Sign%3A%20Greek%20Language%20Multimodal%20Lip%20Reading%20with%20Integrated%20Sign%0A%20%20Language%20Accessibility&body=Title%3A%20GLaM-Sign%3A%20Greek%20Language%20Multimodal%20Lip%20Reading%20with%20Integrated%20Sign%0A%20%20Language%20Accessibility%0AAuthor%3A%20Dimitris%20Kouremenos%20and%20Klimis%20Ntalianis%0AAbstract%3A%20%20%20The%20Greek%20Language%20Multimodal%20Lip%20Reading%20with%20Integrated%20Sign%20Language%0AAccessibility%20%28GLaM-Sign%29%20%5B1%5D%20is%20a%20groundbreaking%20resource%20in%20accessibility%20and%0Amultimodal%20AI%2C%20designed%20to%20support%20Deaf%20and%20Hard-of-Hearing%20%28DHH%29%20individuals.%0ADeveloped%20from%20the%20FEELIT%20project%20%5B2%5D%2C%20it%20integrates%20high-resolution%20audio%2C%0Avideo%2C%20textual%20transcriptions%2C%20and%20Greek%20Sign%20Language%20translations%20for%0Aapplications%20like%20real-time%20sign%20language%20translation%20and%20enhanced%20subtitle%0Asynchronization.%20While%20its%20primary%20focus%20is%20on%20promoting%20inclusivity%20in%20the%0AGreek%20tourism%20sector%2C%20its%20adaptability%20extends%20to%20education%2C%20healthcare%2C%20and%0Apublic%20services.%20Future%20advancements%20will%20enhance%20word-level%20precision%20and%0Ascalability%20to%20additional%20languages%2C%20supported%20by%20advanced%20AI%20methodologies%20and%0Acollaborations%20with%20diverse%20stakeholders.%20This%20dataset%20underscores%20the%0Atransformative%20potential%20of%20multimodal%20resources%20in%20bridging%20communication%0Agaps%2C%20fostering%20innovation%2C%20and%20setting%20a%20benchmark%20for%20ethical%20AI%20and%0Ainclusive%20technologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05213v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLaM-Sign%253A%2520Greek%2520Language%2520Multimodal%2520Lip%2520Reading%2520with%2520Integrated%2520Sign%250A%2520%2520Language%2520Accessibility%26entry.906535625%3DDimitris%2520Kouremenos%2520and%2520Klimis%2520Ntalianis%26entry.1292438233%3D%2520%2520The%2520Greek%2520Language%2520Multimodal%2520Lip%2520Reading%2520with%2520Integrated%2520Sign%2520Language%250AAccessibility%2520%2528GLaM-Sign%2529%2520%255B1%255D%2520is%2520a%2520groundbreaking%2520resource%2520in%2520accessibility%2520and%250Amultimodal%2520AI%252C%2520designed%2520to%2520support%2520Deaf%2520and%2520Hard-of-Hearing%2520%2528DHH%2529%2520individuals.%250ADeveloped%2520from%2520the%2520FEELIT%2520project%2520%255B2%255D%252C%2520it%2520integrates%2520high-resolution%2520audio%252C%250Avideo%252C%2520textual%2520transcriptions%252C%2520and%2520Greek%2520Sign%2520Language%2520translations%2520for%250Aapplications%2520like%2520real-time%2520sign%2520language%2520translation%2520and%2520enhanced%2520subtitle%250Asynchronization.%2520While%2520its%2520primary%2520focus%2520is%2520on%2520promoting%2520inclusivity%2520in%2520the%250AGreek%2520tourism%2520sector%252C%2520its%2520adaptability%2520extends%2520to%2520education%252C%2520healthcare%252C%2520and%250Apublic%2520services.%2520Future%2520advancements%2520will%2520enhance%2520word-level%2520precision%2520and%250Ascalability%2520to%2520additional%2520languages%252C%2520supported%2520by%2520advanced%2520AI%2520methodologies%2520and%250Acollaborations%2520with%2520diverse%2520stakeholders.%2520This%2520dataset%2520underscores%2520the%250Atransformative%2520potential%2520of%2520multimodal%2520resources%2520in%2520bridging%2520communication%250Agaps%252C%2520fostering%2520innovation%252C%2520and%2520setting%2520a%2520benchmark%2520for%2520ethical%2520AI%2520and%250Ainclusive%2520technologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05213v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLaM-Sign%3A%20Greek%20Language%20Multimodal%20Lip%20Reading%20with%20Integrated%20Sign%0A%20%20Language%20Accessibility&entry.906535625=Dimitris%20Kouremenos%20and%20Klimis%20Ntalianis&entry.1292438233=%20%20The%20Greek%20Language%20Multimodal%20Lip%20Reading%20with%20Integrated%20Sign%20Language%0AAccessibility%20%28GLaM-Sign%29%20%5B1%5D%20is%20a%20groundbreaking%20resource%20in%20accessibility%20and%0Amultimodal%20AI%2C%20designed%20to%20support%20Deaf%20and%20Hard-of-Hearing%20%28DHH%29%20individuals.%0ADeveloped%20from%20the%20FEELIT%20project%20%5B2%5D%2C%20it%20integrates%20high-resolution%20audio%2C%0Avideo%2C%20textual%20transcriptions%2C%20and%20Greek%20Sign%20Language%20translations%20for%0Aapplications%20like%20real-time%20sign%20language%20translation%20and%20enhanced%20subtitle%0Asynchronization.%20While%20its%20primary%20focus%20is%20on%20promoting%20inclusivity%20in%20the%0AGreek%20tourism%20sector%2C%20its%20adaptability%20extends%20to%20education%2C%20healthcare%2C%20and%0Apublic%20services.%20Future%20advancements%20will%20enhance%20word-level%20precision%20and%0Ascalability%20to%20additional%20languages%2C%20supported%20by%20advanced%20AI%20methodologies%20and%0Acollaborations%20with%20diverse%20stakeholders.%20This%20dataset%20underscores%20the%0Atransformative%20potential%20of%20multimodal%20resources%20in%20bridging%20communication%0Agaps%2C%20fostering%20innovation%2C%20and%20setting%20a%20benchmark%20for%20ethical%20AI%20and%0Ainclusive%20technologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05213v1&entry.124074799=Read"},
{"title": "Cross-Attention Graph Neural Networks for Inferring Gene Regulatory\n  Networks with Skewed Degree Distribution", "author": "Jiaqi Xiong and Nan Yin and Shiyang Liang and Haoyang Li and Yingxu Wang and Duo Ai and Fang Pan and Jingjie Wang", "abstract": "  Inferencing Gene Regulatory Networks (GRNs) from gene expression data is a\npivotal challenge in systems biology, and several innovative computational\nmethods have been introduced. However, most of these studies have not\nconsidered the skewed degree distribution of genes. Specifically, some genes\nmay regulate multiple target genes while some genes may be regulated by\nmultiple regulator genes. Such a skewed degree distribution issue significantly\ncomplicates the application of directed graph embedding methods. To tackle this\nissue, we propose the Cross-Attention Complex Dual Graph Embedding Model\n(XATGRN). Our XATGRN employs a cross-attention mechanism to effectively capture\nintricate gene interactions from gene expression profiles. Additionally, it\nuses a Dual Complex Graph Embedding approach to manage the skewed degree\ndistribution, thereby ensuring precise prediction of regulatory relationships\nand their directionality. Our model consistently outperforms existing\nstate-of-the-art methods across various datasets, underscoring its efficacy in\nelucidating complex gene regulatory mechanisms. Our codes used in this paper\nare publicly available at: https://github.com/kikixiong/XATGRN.\n", "link": "http://arxiv.org/abs/2412.16220v3", "date": "2025-01-09", "relevancy": 2.3605, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4848}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4844}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Attention%20Graph%20Neural%20Networks%20for%20Inferring%20Gene%20Regulatory%0A%20%20Networks%20with%20Skewed%20Degree%20Distribution&body=Title%3A%20Cross-Attention%20Graph%20Neural%20Networks%20for%20Inferring%20Gene%20Regulatory%0A%20%20Networks%20with%20Skewed%20Degree%20Distribution%0AAuthor%3A%20Jiaqi%20Xiong%20and%20Nan%20Yin%20and%20Shiyang%20Liang%20and%20Haoyang%20Li%20and%20Yingxu%20Wang%20and%20Duo%20Ai%20and%20Fang%20Pan%20and%20Jingjie%20Wang%0AAbstract%3A%20%20%20Inferencing%20Gene%20Regulatory%20Networks%20%28GRNs%29%20from%20gene%20expression%20data%20is%20a%0Apivotal%20challenge%20in%20systems%20biology%2C%20and%20several%20innovative%20computational%0Amethods%20have%20been%20introduced.%20However%2C%20most%20of%20these%20studies%20have%20not%0Aconsidered%20the%20skewed%20degree%20distribution%20of%20genes.%20Specifically%2C%20some%20genes%0Amay%20regulate%20multiple%20target%20genes%20while%20some%20genes%20may%20be%20regulated%20by%0Amultiple%20regulator%20genes.%20Such%20a%20skewed%20degree%20distribution%20issue%20significantly%0Acomplicates%20the%20application%20of%20directed%20graph%20embedding%20methods.%20To%20tackle%20this%0Aissue%2C%20we%20propose%20the%20Cross-Attention%20Complex%20Dual%20Graph%20Embedding%20Model%0A%28XATGRN%29.%20Our%20XATGRN%20employs%20a%20cross-attention%20mechanism%20to%20effectively%20capture%0Aintricate%20gene%20interactions%20from%20gene%20expression%20profiles.%20Additionally%2C%20it%0Auses%20a%20Dual%20Complex%20Graph%20Embedding%20approach%20to%20manage%20the%20skewed%20degree%0Adistribution%2C%20thereby%20ensuring%20precise%20prediction%20of%20regulatory%20relationships%0Aand%20their%20directionality.%20Our%20model%20consistently%20outperforms%20existing%0Astate-of-the-art%20methods%20across%20various%20datasets%2C%20underscoring%20its%20efficacy%20in%0Aelucidating%20complex%20gene%20regulatory%20mechanisms.%20Our%20codes%20used%20in%20this%20paper%0Aare%20publicly%20available%20at%3A%20https%3A//github.com/kikixiong/XATGRN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16220v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Attention%2520Graph%2520Neural%2520Networks%2520for%2520Inferring%2520Gene%2520Regulatory%250A%2520%2520Networks%2520with%2520Skewed%2520Degree%2520Distribution%26entry.906535625%3DJiaqi%2520Xiong%2520and%2520Nan%2520Yin%2520and%2520Shiyang%2520Liang%2520and%2520Haoyang%2520Li%2520and%2520Yingxu%2520Wang%2520and%2520Duo%2520Ai%2520and%2520Fang%2520Pan%2520and%2520Jingjie%2520Wang%26entry.1292438233%3D%2520%2520Inferencing%2520Gene%2520Regulatory%2520Networks%2520%2528GRNs%2529%2520from%2520gene%2520expression%2520data%2520is%2520a%250Apivotal%2520challenge%2520in%2520systems%2520biology%252C%2520and%2520several%2520innovative%2520computational%250Amethods%2520have%2520been%2520introduced.%2520However%252C%2520most%2520of%2520these%2520studies%2520have%2520not%250Aconsidered%2520the%2520skewed%2520degree%2520distribution%2520of%2520genes.%2520Specifically%252C%2520some%2520genes%250Amay%2520regulate%2520multiple%2520target%2520genes%2520while%2520some%2520genes%2520may%2520be%2520regulated%2520by%250Amultiple%2520regulator%2520genes.%2520Such%2520a%2520skewed%2520degree%2520distribution%2520issue%2520significantly%250Acomplicates%2520the%2520application%2520of%2520directed%2520graph%2520embedding%2520methods.%2520To%2520tackle%2520this%250Aissue%252C%2520we%2520propose%2520the%2520Cross-Attention%2520Complex%2520Dual%2520Graph%2520Embedding%2520Model%250A%2528XATGRN%2529.%2520Our%2520XATGRN%2520employs%2520a%2520cross-attention%2520mechanism%2520to%2520effectively%2520capture%250Aintricate%2520gene%2520interactions%2520from%2520gene%2520expression%2520profiles.%2520Additionally%252C%2520it%250Auses%2520a%2520Dual%2520Complex%2520Graph%2520Embedding%2520approach%2520to%2520manage%2520the%2520skewed%2520degree%250Adistribution%252C%2520thereby%2520ensuring%2520precise%2520prediction%2520of%2520regulatory%2520relationships%250Aand%2520their%2520directionality.%2520Our%2520model%2520consistently%2520outperforms%2520existing%250Astate-of-the-art%2520methods%2520across%2520various%2520datasets%252C%2520underscoring%2520its%2520efficacy%2520in%250Aelucidating%2520complex%2520gene%2520regulatory%2520mechanisms.%2520Our%2520codes%2520used%2520in%2520this%2520paper%250Aare%2520publicly%2520available%2520at%253A%2520https%253A//github.com/kikixiong/XATGRN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16220v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Attention%20Graph%20Neural%20Networks%20for%20Inferring%20Gene%20Regulatory%0A%20%20Networks%20with%20Skewed%20Degree%20Distribution&entry.906535625=Jiaqi%20Xiong%20and%20Nan%20Yin%20and%20Shiyang%20Liang%20and%20Haoyang%20Li%20and%20Yingxu%20Wang%20and%20Duo%20Ai%20and%20Fang%20Pan%20and%20Jingjie%20Wang&entry.1292438233=%20%20Inferencing%20Gene%20Regulatory%20Networks%20%28GRNs%29%20from%20gene%20expression%20data%20is%20a%0Apivotal%20challenge%20in%20systems%20biology%2C%20and%20several%20innovative%20computational%0Amethods%20have%20been%20introduced.%20However%2C%20most%20of%20these%20studies%20have%20not%0Aconsidered%20the%20skewed%20degree%20distribution%20of%20genes.%20Specifically%2C%20some%20genes%0Amay%20regulate%20multiple%20target%20genes%20while%20some%20genes%20may%20be%20regulated%20by%0Amultiple%20regulator%20genes.%20Such%20a%20skewed%20degree%20distribution%20issue%20significantly%0Acomplicates%20the%20application%20of%20directed%20graph%20embedding%20methods.%20To%20tackle%20this%0Aissue%2C%20we%20propose%20the%20Cross-Attention%20Complex%20Dual%20Graph%20Embedding%20Model%0A%28XATGRN%29.%20Our%20XATGRN%20employs%20a%20cross-attention%20mechanism%20to%20effectively%20capture%0Aintricate%20gene%20interactions%20from%20gene%20expression%20profiles.%20Additionally%2C%20it%0Auses%20a%20Dual%20Complex%20Graph%20Embedding%20approach%20to%20manage%20the%20skewed%20degree%0Adistribution%2C%20thereby%20ensuring%20precise%20prediction%20of%20regulatory%20relationships%0Aand%20their%20directionality.%20Our%20model%20consistently%20outperforms%20existing%0Astate-of-the-art%20methods%20across%20various%20datasets%2C%20underscoring%20its%20efficacy%20in%0Aelucidating%20complex%20gene%20regulatory%20mechanisms.%20Our%20codes%20used%20in%20this%20paper%0Aare%20publicly%20available%20at%3A%20https%3A//github.com/kikixiong/XATGRN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16220v3&entry.124074799=Read"},
{"title": "Mechanistic understanding and validation of large AI models with\n  SemanticLens", "author": "Maximilian Dreyer and Jim Berend and Tobias Labarta and Johanna Vielhaben and Thomas Wiegand and Sebastian Lapuschkin and Wojciech Samek", "abstract": "  Unlike human-engineered systems such as aeroplanes, where each component's\nrole and dependencies are well understood, the inner workings of AI models\nremain largely opaque, hindering verifiability and undermining trust. This\npaper introduces SemanticLens, a universal explanation method for neural\nnetworks that maps hidden knowledge encoded by components (e.g., individual\nneurons) into the semantically structured, multimodal space of a foundation\nmodel such as CLIP. In this space, unique operations become possible, including\n(i) textual search to identify neurons encoding specific concepts, (ii)\nsystematic analysis and comparison of model representations, (iii) automated\nlabelling of neurons and explanation of their functional roles, and (iv) audits\nto validate decision-making against requirements. Fully scalable and operating\nwithout human input, SemanticLens is shown to be effective for debugging and\nvalidation, summarizing model knowledge, aligning reasoning with expectations\n(e.g., adherence to the ABCDE-rule in melanoma classification), and detecting\ncomponents tied to spurious correlations and their associated training data. By\nenabling component-level understanding and validation, the proposed approach\nhelps bridge the \"trust gap\" between AI models and traditional engineered\nsystems. We provide code for SemanticLens on\nhttps://github.com/jim-berend/semanticlens and a demo on\nhttps://semanticlens.hhi-research-insights.eu.\n", "link": "http://arxiv.org/abs/2501.05398v1", "date": "2025-01-09", "relevancy": 2.3548, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5919}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5881}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mechanistic%20understanding%20and%20validation%20of%20large%20AI%20models%20with%0A%20%20SemanticLens&body=Title%3A%20Mechanistic%20understanding%20and%20validation%20of%20large%20AI%20models%20with%0A%20%20SemanticLens%0AAuthor%3A%20Maximilian%20Dreyer%20and%20Jim%20Berend%20and%20Tobias%20Labarta%20and%20Johanna%20Vielhaben%20and%20Thomas%20Wiegand%20and%20Sebastian%20Lapuschkin%20and%20Wojciech%20Samek%0AAbstract%3A%20%20%20Unlike%20human-engineered%20systems%20such%20as%20aeroplanes%2C%20where%20each%20component%27s%0Arole%20and%20dependencies%20are%20well%20understood%2C%20the%20inner%20workings%20of%20AI%20models%0Aremain%20largely%20opaque%2C%20hindering%20verifiability%20and%20undermining%20trust.%20This%0Apaper%20introduces%20SemanticLens%2C%20a%20universal%20explanation%20method%20for%20neural%0Anetworks%20that%20maps%20hidden%20knowledge%20encoded%20by%20components%20%28e.g.%2C%20individual%0Aneurons%29%20into%20the%20semantically%20structured%2C%20multimodal%20space%20of%20a%20foundation%0Amodel%20such%20as%20CLIP.%20In%20this%20space%2C%20unique%20operations%20become%20possible%2C%20including%0A%28i%29%20textual%20search%20to%20identify%20neurons%20encoding%20specific%20concepts%2C%20%28ii%29%0Asystematic%20analysis%20and%20comparison%20of%20model%20representations%2C%20%28iii%29%20automated%0Alabelling%20of%20neurons%20and%20explanation%20of%20their%20functional%20roles%2C%20and%20%28iv%29%20audits%0Ato%20validate%20decision-making%20against%20requirements.%20Fully%20scalable%20and%20operating%0Awithout%20human%20input%2C%20SemanticLens%20is%20shown%20to%20be%20effective%20for%20debugging%20and%0Avalidation%2C%20summarizing%20model%20knowledge%2C%20aligning%20reasoning%20with%20expectations%0A%28e.g.%2C%20adherence%20to%20the%20ABCDE-rule%20in%20melanoma%20classification%29%2C%20and%20detecting%0Acomponents%20tied%20to%20spurious%20correlations%20and%20their%20associated%20training%20data.%20By%0Aenabling%20component-level%20understanding%20and%20validation%2C%20the%20proposed%20approach%0Ahelps%20bridge%20the%20%22trust%20gap%22%20between%20AI%20models%20and%20traditional%20engineered%0Asystems.%20We%20provide%20code%20for%20SemanticLens%20on%0Ahttps%3A//github.com/jim-berend/semanticlens%20and%20a%20demo%20on%0Ahttps%3A//semanticlens.hhi-research-insights.eu.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05398v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMechanistic%2520understanding%2520and%2520validation%2520of%2520large%2520AI%2520models%2520with%250A%2520%2520SemanticLens%26entry.906535625%3DMaximilian%2520Dreyer%2520and%2520Jim%2520Berend%2520and%2520Tobias%2520Labarta%2520and%2520Johanna%2520Vielhaben%2520and%2520Thomas%2520Wiegand%2520and%2520Sebastian%2520Lapuschkin%2520and%2520Wojciech%2520Samek%26entry.1292438233%3D%2520%2520Unlike%2520human-engineered%2520systems%2520such%2520as%2520aeroplanes%252C%2520where%2520each%2520component%2527s%250Arole%2520and%2520dependencies%2520are%2520well%2520understood%252C%2520the%2520inner%2520workings%2520of%2520AI%2520models%250Aremain%2520largely%2520opaque%252C%2520hindering%2520verifiability%2520and%2520undermining%2520trust.%2520This%250Apaper%2520introduces%2520SemanticLens%252C%2520a%2520universal%2520explanation%2520method%2520for%2520neural%250Anetworks%2520that%2520maps%2520hidden%2520knowledge%2520encoded%2520by%2520components%2520%2528e.g.%252C%2520individual%250Aneurons%2529%2520into%2520the%2520semantically%2520structured%252C%2520multimodal%2520space%2520of%2520a%2520foundation%250Amodel%2520such%2520as%2520CLIP.%2520In%2520this%2520space%252C%2520unique%2520operations%2520become%2520possible%252C%2520including%250A%2528i%2529%2520textual%2520search%2520to%2520identify%2520neurons%2520encoding%2520specific%2520concepts%252C%2520%2528ii%2529%250Asystematic%2520analysis%2520and%2520comparison%2520of%2520model%2520representations%252C%2520%2528iii%2529%2520automated%250Alabelling%2520of%2520neurons%2520and%2520explanation%2520of%2520their%2520functional%2520roles%252C%2520and%2520%2528iv%2529%2520audits%250Ato%2520validate%2520decision-making%2520against%2520requirements.%2520Fully%2520scalable%2520and%2520operating%250Awithout%2520human%2520input%252C%2520SemanticLens%2520is%2520shown%2520to%2520be%2520effective%2520for%2520debugging%2520and%250Avalidation%252C%2520summarizing%2520model%2520knowledge%252C%2520aligning%2520reasoning%2520with%2520expectations%250A%2528e.g.%252C%2520adherence%2520to%2520the%2520ABCDE-rule%2520in%2520melanoma%2520classification%2529%252C%2520and%2520detecting%250Acomponents%2520tied%2520to%2520spurious%2520correlations%2520and%2520their%2520associated%2520training%2520data.%2520By%250Aenabling%2520component-level%2520understanding%2520and%2520validation%252C%2520the%2520proposed%2520approach%250Ahelps%2520bridge%2520the%2520%2522trust%2520gap%2522%2520between%2520AI%2520models%2520and%2520traditional%2520engineered%250Asystems.%2520We%2520provide%2520code%2520for%2520SemanticLens%2520on%250Ahttps%253A//github.com/jim-berend/semanticlens%2520and%2520a%2520demo%2520on%250Ahttps%253A//semanticlens.hhi-research-insights.eu.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05398v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mechanistic%20understanding%20and%20validation%20of%20large%20AI%20models%20with%0A%20%20SemanticLens&entry.906535625=Maximilian%20Dreyer%20and%20Jim%20Berend%20and%20Tobias%20Labarta%20and%20Johanna%20Vielhaben%20and%20Thomas%20Wiegand%20and%20Sebastian%20Lapuschkin%20and%20Wojciech%20Samek&entry.1292438233=%20%20Unlike%20human-engineered%20systems%20such%20as%20aeroplanes%2C%20where%20each%20component%27s%0Arole%20and%20dependencies%20are%20well%20understood%2C%20the%20inner%20workings%20of%20AI%20models%0Aremain%20largely%20opaque%2C%20hindering%20verifiability%20and%20undermining%20trust.%20This%0Apaper%20introduces%20SemanticLens%2C%20a%20universal%20explanation%20method%20for%20neural%0Anetworks%20that%20maps%20hidden%20knowledge%20encoded%20by%20components%20%28e.g.%2C%20individual%0Aneurons%29%20into%20the%20semantically%20structured%2C%20multimodal%20space%20of%20a%20foundation%0Amodel%20such%20as%20CLIP.%20In%20this%20space%2C%20unique%20operations%20become%20possible%2C%20including%0A%28i%29%20textual%20search%20to%20identify%20neurons%20encoding%20specific%20concepts%2C%20%28ii%29%0Asystematic%20analysis%20and%20comparison%20of%20model%20representations%2C%20%28iii%29%20automated%0Alabelling%20of%20neurons%20and%20explanation%20of%20their%20functional%20roles%2C%20and%20%28iv%29%20audits%0Ato%20validate%20decision-making%20against%20requirements.%20Fully%20scalable%20and%20operating%0Awithout%20human%20input%2C%20SemanticLens%20is%20shown%20to%20be%20effective%20for%20debugging%20and%0Avalidation%2C%20summarizing%20model%20knowledge%2C%20aligning%20reasoning%20with%20expectations%0A%28e.g.%2C%20adherence%20to%20the%20ABCDE-rule%20in%20melanoma%20classification%29%2C%20and%20detecting%0Acomponents%20tied%20to%20spurious%20correlations%20and%20their%20associated%20training%20data.%20By%0Aenabling%20component-level%20understanding%20and%20validation%2C%20the%20proposed%20approach%0Ahelps%20bridge%20the%20%22trust%20gap%22%20between%20AI%20models%20and%20traditional%20engineered%0Asystems.%20We%20provide%20code%20for%20SemanticLens%20on%0Ahttps%3A//github.com/jim-berend/semanticlens%20and%20a%20demo%20on%0Ahttps%3A//semanticlens.hhi-research-insights.eu.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05398v1&entry.124074799=Read"},
{"title": "FaceMe: Robust Blind Face Restoration with Personal Identification", "author": "Siyu Liu and Zheng-Peng Duan and Jia OuYang and Jiayi Fu and Hyunhee Park and Zikun Liu and Chun-Le Guo and Chongyi Li", "abstract": "  Blind face restoration is a highly ill-posed problem due to the lack of\nnecessary context. Although existing methods produce high-quality outputs, they\noften fail to faithfully preserve the individual's identity. In this paper, we\npropose a personalized face restoration method, FaceMe, based on a diffusion\nmodel. Given a single or a few reference images, we use an identity encoder to\nextract identity-related features, which serve as prompts to guide the\ndiffusion model in restoring high-quality and identity-consistent facial\nimages. By simply combining identity-related features, we effectively minimize\nthe impact of identity-irrelevant features during training and support any\nnumber of reference image inputs during inference. Additionally, thanks to the\nrobustness of the identity encoder, synthesized images can be used as reference\nimages during training, and identity changing during inference does not require\nfine-tuning the model. We also propose a pipeline for constructing a reference\nimage training pool that simulates the poses and expressions that may appear in\nreal-world scenarios. Experimental results demonstrate that our FaceMe can\nrestore high-quality facial images while maintaining identity consistency,\nachieving excellent performance and robustness.\n", "link": "http://arxiv.org/abs/2501.05177v1", "date": "2025-01-09", "relevancy": 2.3503, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6299}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5982}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FaceMe%3A%20Robust%20Blind%20Face%20Restoration%20with%20Personal%20Identification&body=Title%3A%20FaceMe%3A%20Robust%20Blind%20Face%20Restoration%20with%20Personal%20Identification%0AAuthor%3A%20Siyu%20Liu%20and%20Zheng-Peng%20Duan%20and%20Jia%20OuYang%20and%20Jiayi%20Fu%20and%20Hyunhee%20Park%20and%20Zikun%20Liu%20and%20Chun-Le%20Guo%20and%20Chongyi%20Li%0AAbstract%3A%20%20%20Blind%20face%20restoration%20is%20a%20highly%20ill-posed%20problem%20due%20to%20the%20lack%20of%0Anecessary%20context.%20Although%20existing%20methods%20produce%20high-quality%20outputs%2C%20they%0Aoften%20fail%20to%20faithfully%20preserve%20the%20individual%27s%20identity.%20In%20this%20paper%2C%20we%0Apropose%20a%20personalized%20face%20restoration%20method%2C%20FaceMe%2C%20based%20on%20a%20diffusion%0Amodel.%20Given%20a%20single%20or%20a%20few%20reference%20images%2C%20we%20use%20an%20identity%20encoder%20to%0Aextract%20identity-related%20features%2C%20which%20serve%20as%20prompts%20to%20guide%20the%0Adiffusion%20model%20in%20restoring%20high-quality%20and%20identity-consistent%20facial%0Aimages.%20By%20simply%20combining%20identity-related%20features%2C%20we%20effectively%20minimize%0Athe%20impact%20of%20identity-irrelevant%20features%20during%20training%20and%20support%20any%0Anumber%20of%20reference%20image%20inputs%20during%20inference.%20Additionally%2C%20thanks%20to%20the%0Arobustness%20of%20the%20identity%20encoder%2C%20synthesized%20images%20can%20be%20used%20as%20reference%0Aimages%20during%20training%2C%20and%20identity%20changing%20during%20inference%20does%20not%20require%0Afine-tuning%20the%20model.%20We%20also%20propose%20a%20pipeline%20for%20constructing%20a%20reference%0Aimage%20training%20pool%20that%20simulates%20the%20poses%20and%20expressions%20that%20may%20appear%20in%0Areal-world%20scenarios.%20Experimental%20results%20demonstrate%20that%20our%20FaceMe%20can%0Arestore%20high-quality%20facial%20images%20while%20maintaining%20identity%20consistency%2C%0Aachieving%20excellent%20performance%20and%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaceMe%253A%2520Robust%2520Blind%2520Face%2520Restoration%2520with%2520Personal%2520Identification%26entry.906535625%3DSiyu%2520Liu%2520and%2520Zheng-Peng%2520Duan%2520and%2520Jia%2520OuYang%2520and%2520Jiayi%2520Fu%2520and%2520Hyunhee%2520Park%2520and%2520Zikun%2520Liu%2520and%2520Chun-Le%2520Guo%2520and%2520Chongyi%2520Li%26entry.1292438233%3D%2520%2520Blind%2520face%2520restoration%2520is%2520a%2520highly%2520ill-posed%2520problem%2520due%2520to%2520the%2520lack%2520of%250Anecessary%2520context.%2520Although%2520existing%2520methods%2520produce%2520high-quality%2520outputs%252C%2520they%250Aoften%2520fail%2520to%2520faithfully%2520preserve%2520the%2520individual%2527s%2520identity.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520personalized%2520face%2520restoration%2520method%252C%2520FaceMe%252C%2520based%2520on%2520a%2520diffusion%250Amodel.%2520Given%2520a%2520single%2520or%2520a%2520few%2520reference%2520images%252C%2520we%2520use%2520an%2520identity%2520encoder%2520to%250Aextract%2520identity-related%2520features%252C%2520which%2520serve%2520as%2520prompts%2520to%2520guide%2520the%250Adiffusion%2520model%2520in%2520restoring%2520high-quality%2520and%2520identity-consistent%2520facial%250Aimages.%2520By%2520simply%2520combining%2520identity-related%2520features%252C%2520we%2520effectively%2520minimize%250Athe%2520impact%2520of%2520identity-irrelevant%2520features%2520during%2520training%2520and%2520support%2520any%250Anumber%2520of%2520reference%2520image%2520inputs%2520during%2520inference.%2520Additionally%252C%2520thanks%2520to%2520the%250Arobustness%2520of%2520the%2520identity%2520encoder%252C%2520synthesized%2520images%2520can%2520be%2520used%2520as%2520reference%250Aimages%2520during%2520training%252C%2520and%2520identity%2520changing%2520during%2520inference%2520does%2520not%2520require%250Afine-tuning%2520the%2520model.%2520We%2520also%2520propose%2520a%2520pipeline%2520for%2520constructing%2520a%2520reference%250Aimage%2520training%2520pool%2520that%2520simulates%2520the%2520poses%2520and%2520expressions%2520that%2520may%2520appear%2520in%250Areal-world%2520scenarios.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520FaceMe%2520can%250Arestore%2520high-quality%2520facial%2520images%2520while%2520maintaining%2520identity%2520consistency%252C%250Aachieving%2520excellent%2520performance%2520and%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FaceMe%3A%20Robust%20Blind%20Face%20Restoration%20with%20Personal%20Identification&entry.906535625=Siyu%20Liu%20and%20Zheng-Peng%20Duan%20and%20Jia%20OuYang%20and%20Jiayi%20Fu%20and%20Hyunhee%20Park%20and%20Zikun%20Liu%20and%20Chun-Le%20Guo%20and%20Chongyi%20Li&entry.1292438233=%20%20Blind%20face%20restoration%20is%20a%20highly%20ill-posed%20problem%20due%20to%20the%20lack%20of%0Anecessary%20context.%20Although%20existing%20methods%20produce%20high-quality%20outputs%2C%20they%0Aoften%20fail%20to%20faithfully%20preserve%20the%20individual%27s%20identity.%20In%20this%20paper%2C%20we%0Apropose%20a%20personalized%20face%20restoration%20method%2C%20FaceMe%2C%20based%20on%20a%20diffusion%0Amodel.%20Given%20a%20single%20or%20a%20few%20reference%20images%2C%20we%20use%20an%20identity%20encoder%20to%0Aextract%20identity-related%20features%2C%20which%20serve%20as%20prompts%20to%20guide%20the%0Adiffusion%20model%20in%20restoring%20high-quality%20and%20identity-consistent%20facial%0Aimages.%20By%20simply%20combining%20identity-related%20features%2C%20we%20effectively%20minimize%0Athe%20impact%20of%20identity-irrelevant%20features%20during%20training%20and%20support%20any%0Anumber%20of%20reference%20image%20inputs%20during%20inference.%20Additionally%2C%20thanks%20to%20the%0Arobustness%20of%20the%20identity%20encoder%2C%20synthesized%20images%20can%20be%20used%20as%20reference%0Aimages%20during%20training%2C%20and%20identity%20changing%20during%20inference%20does%20not%20require%0Afine-tuning%20the%20model.%20We%20also%20propose%20a%20pipeline%20for%20constructing%20a%20reference%0Aimage%20training%20pool%20that%20simulates%20the%20poses%20and%20expressions%20that%20may%20appear%20in%0Areal-world%20scenarios.%20Experimental%20results%20demonstrate%20that%20our%20FaceMe%20can%0Arestore%20high-quality%20facial%20images%20while%20maintaining%20identity%20consistency%2C%0Aachieving%20excellent%20performance%20and%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05177v1&entry.124074799=Read"},
{"title": "Contrast-Free Myocardial Scar Segmentation in Cine MRI using Motion and\n  Texture Fusion", "author": "Guang Yang and Jingkun Chen and Xicheng Sheng and Shan Yang and Xiahai Zhuang and Betty Raman and Lei Li and Vicente Grau", "abstract": "  Late gadolinium enhancement MRI (LGE MRI) is the gold standard for the\ndetection of myocardial scars for post myocardial infarction (MI). LGE MRI\nrequires the injection of a contrast agent, which carries potential side\neffects and increases scanning time and patient discomfort. To address these\nissues, we propose a novel framework that combines cardiac motion observed in\ncine MRI with image texture information to segment the myocardium and scar\ntissue in the left ventricle. Cardiac motion tracking can be formulated as a\nfull cardiac image cycle registration problem, which can be solved via deep\nneural networks. Experimental results prove that the proposed method can\nachieve scar segmentation based on non-contrasted cine images with comparable\naccuracy to LGE MRI. This demonstrates its potential as an alternative to\ncontrast-enhanced techniques for scar detection.\n", "link": "http://arxiv.org/abs/2501.05241v1", "date": "2025-01-09", "relevancy": 2.3498, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4788}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4667}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrast-Free%20Myocardial%20Scar%20Segmentation%20in%20Cine%20MRI%20using%20Motion%20and%0A%20%20Texture%20Fusion&body=Title%3A%20Contrast-Free%20Myocardial%20Scar%20Segmentation%20in%20Cine%20MRI%20using%20Motion%20and%0A%20%20Texture%20Fusion%0AAuthor%3A%20Guang%20Yang%20and%20Jingkun%20Chen%20and%20Xicheng%20Sheng%20and%20Shan%20Yang%20and%20Xiahai%20Zhuang%20and%20Betty%20Raman%20and%20Lei%20Li%20and%20Vicente%20Grau%0AAbstract%3A%20%20%20Late%20gadolinium%20enhancement%20MRI%20%28LGE%20MRI%29%20is%20the%20gold%20standard%20for%20the%0Adetection%20of%20myocardial%20scars%20for%20post%20myocardial%20infarction%20%28MI%29.%20LGE%20MRI%0Arequires%20the%20injection%20of%20a%20contrast%20agent%2C%20which%20carries%20potential%20side%0Aeffects%20and%20increases%20scanning%20time%20and%20patient%20discomfort.%20To%20address%20these%0Aissues%2C%20we%20propose%20a%20novel%20framework%20that%20combines%20cardiac%20motion%20observed%20in%0Acine%20MRI%20with%20image%20texture%20information%20to%20segment%20the%20myocardium%20and%20scar%0Atissue%20in%20the%20left%20ventricle.%20Cardiac%20motion%20tracking%20can%20be%20formulated%20as%20a%0Afull%20cardiac%20image%20cycle%20registration%20problem%2C%20which%20can%20be%20solved%20via%20deep%0Aneural%20networks.%20Experimental%20results%20prove%20that%20the%20proposed%20method%20can%0Aachieve%20scar%20segmentation%20based%20on%20non-contrasted%20cine%20images%20with%20comparable%0Aaccuracy%20to%20LGE%20MRI.%20This%20demonstrates%20its%20potential%20as%20an%20alternative%20to%0Acontrast-enhanced%20techniques%20for%20scar%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05241v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrast-Free%2520Myocardial%2520Scar%2520Segmentation%2520in%2520Cine%2520MRI%2520using%2520Motion%2520and%250A%2520%2520Texture%2520Fusion%26entry.906535625%3DGuang%2520Yang%2520and%2520Jingkun%2520Chen%2520and%2520Xicheng%2520Sheng%2520and%2520Shan%2520Yang%2520and%2520Xiahai%2520Zhuang%2520and%2520Betty%2520Raman%2520and%2520Lei%2520Li%2520and%2520Vicente%2520Grau%26entry.1292438233%3D%2520%2520Late%2520gadolinium%2520enhancement%2520MRI%2520%2528LGE%2520MRI%2529%2520is%2520the%2520gold%2520standard%2520for%2520the%250Adetection%2520of%2520myocardial%2520scars%2520for%2520post%2520myocardial%2520infarction%2520%2528MI%2529.%2520LGE%2520MRI%250Arequires%2520the%2520injection%2520of%2520a%2520contrast%2520agent%252C%2520which%2520carries%2520potential%2520side%250Aeffects%2520and%2520increases%2520scanning%2520time%2520and%2520patient%2520discomfort.%2520To%2520address%2520these%250Aissues%252C%2520we%2520propose%2520a%2520novel%2520framework%2520that%2520combines%2520cardiac%2520motion%2520observed%2520in%250Acine%2520MRI%2520with%2520image%2520texture%2520information%2520to%2520segment%2520the%2520myocardium%2520and%2520scar%250Atissue%2520in%2520the%2520left%2520ventricle.%2520Cardiac%2520motion%2520tracking%2520can%2520be%2520formulated%2520as%2520a%250Afull%2520cardiac%2520image%2520cycle%2520registration%2520problem%252C%2520which%2520can%2520be%2520solved%2520via%2520deep%250Aneural%2520networks.%2520Experimental%2520results%2520prove%2520that%2520the%2520proposed%2520method%2520can%250Aachieve%2520scar%2520segmentation%2520based%2520on%2520non-contrasted%2520cine%2520images%2520with%2520comparable%250Aaccuracy%2520to%2520LGE%2520MRI.%2520This%2520demonstrates%2520its%2520potential%2520as%2520an%2520alternative%2520to%250Acontrast-enhanced%2520techniques%2520for%2520scar%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05241v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrast-Free%20Myocardial%20Scar%20Segmentation%20in%20Cine%20MRI%20using%20Motion%20and%0A%20%20Texture%20Fusion&entry.906535625=Guang%20Yang%20and%20Jingkun%20Chen%20and%20Xicheng%20Sheng%20and%20Shan%20Yang%20and%20Xiahai%20Zhuang%20and%20Betty%20Raman%20and%20Lei%20Li%20and%20Vicente%20Grau&entry.1292438233=%20%20Late%20gadolinium%20enhancement%20MRI%20%28LGE%20MRI%29%20is%20the%20gold%20standard%20for%20the%0Adetection%20of%20myocardial%20scars%20for%20post%20myocardial%20infarction%20%28MI%29.%20LGE%20MRI%0Arequires%20the%20injection%20of%20a%20contrast%20agent%2C%20which%20carries%20potential%20side%0Aeffects%20and%20increases%20scanning%20time%20and%20patient%20discomfort.%20To%20address%20these%0Aissues%2C%20we%20propose%20a%20novel%20framework%20that%20combines%20cardiac%20motion%20observed%20in%0Acine%20MRI%20with%20image%20texture%20information%20to%20segment%20the%20myocardium%20and%20scar%0Atissue%20in%20the%20left%20ventricle.%20Cardiac%20motion%20tracking%20can%20be%20formulated%20as%20a%0Afull%20cardiac%20image%20cycle%20registration%20problem%2C%20which%20can%20be%20solved%20via%20deep%0Aneural%20networks.%20Experimental%20results%20prove%20that%20the%20proposed%20method%20can%0Aachieve%20scar%20segmentation%20based%20on%20non-contrasted%20cine%20images%20with%20comparable%0Aaccuracy%20to%20LGE%20MRI.%20This%20demonstrates%20its%20potential%20as%20an%20alternative%20to%0Acontrast-enhanced%20techniques%20for%20scar%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05241v1&entry.124074799=Read"},
{"title": "Less is More: The Influence of Pruning on the Explainability of CNNs", "author": "David Weber and Florian Merkle and Pascal Sch\u00f6ttle and Stephan Schl\u00f6gl", "abstract": "  Modern, state-of-the-art Convolutional Neural Networks (CNNs) in computer\nvision have millions of parameters. Thus, explaining the complex decisions of\nsuch networks to humans is challenging. A technical approach to reduce CNN\ncomplexity is network pruning, where less important parameters are deleted. The\nwork presented in this paper investigates whether this technical complexity\nreduction also helps with perceived explainability. To do so, we conducted a\npre-study and two human-grounded experiments, assessing the effects of\ndifferent pruning ratios on CNN explainability. Overall, we evaluated four\ndifferent compression rates (i.e., CPR 2, 4, 8, and 32) with 37 500 tasks on\nMechanical Turk. Results indicate that lower compression rates have a positive\ninfluence on explainability, while higher compression rates show negative\neffects. Furthermore, we were able to identify sweet spots that increase both\nthe perceived explainability and the model's performance.\n", "link": "http://arxiv.org/abs/2302.08878v2", "date": "2025-01-09", "relevancy": 2.334, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4702}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4651}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4651}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Less%20is%20More%3A%20The%20Influence%20of%20Pruning%20on%20the%20Explainability%20of%20CNNs&body=Title%3A%20Less%20is%20More%3A%20The%20Influence%20of%20Pruning%20on%20the%20Explainability%20of%20CNNs%0AAuthor%3A%20David%20Weber%20and%20Florian%20Merkle%20and%20Pascal%20Sch%C3%B6ttle%20and%20Stephan%20Schl%C3%B6gl%0AAbstract%3A%20%20%20Modern%2C%20state-of-the-art%20Convolutional%20Neural%20Networks%20%28CNNs%29%20in%20computer%0Avision%20have%20millions%20of%20parameters.%20Thus%2C%20explaining%20the%20complex%20decisions%20of%0Asuch%20networks%20to%20humans%20is%20challenging.%20A%20technical%20approach%20to%20reduce%20CNN%0Acomplexity%20is%20network%20pruning%2C%20where%20less%20important%20parameters%20are%20deleted.%20The%0Awork%20presented%20in%20this%20paper%20investigates%20whether%20this%20technical%20complexity%0Areduction%20also%20helps%20with%20perceived%20explainability.%20To%20do%20so%2C%20we%20conducted%20a%0Apre-study%20and%20two%20human-grounded%20experiments%2C%20assessing%20the%20effects%20of%0Adifferent%20pruning%20ratios%20on%20CNN%20explainability.%20Overall%2C%20we%20evaluated%20four%0Adifferent%20compression%20rates%20%28i.e.%2C%20CPR%202%2C%204%2C%208%2C%20and%2032%29%20with%2037%20500%20tasks%20on%0AMechanical%20Turk.%20Results%20indicate%20that%20lower%20compression%20rates%20have%20a%20positive%0Ainfluence%20on%20explainability%2C%20while%20higher%20compression%20rates%20show%20negative%0Aeffects.%20Furthermore%2C%20we%20were%20able%20to%20identify%20sweet%20spots%20that%20increase%20both%0Athe%20perceived%20explainability%20and%20the%20model%27s%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.08878v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLess%2520is%2520More%253A%2520The%2520Influence%2520of%2520Pruning%2520on%2520the%2520Explainability%2520of%2520CNNs%26entry.906535625%3DDavid%2520Weber%2520and%2520Florian%2520Merkle%2520and%2520Pascal%2520Sch%25C3%25B6ttle%2520and%2520Stephan%2520Schl%25C3%25B6gl%26entry.1292438233%3D%2520%2520Modern%252C%2520state-of-the-art%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520in%2520computer%250Avision%2520have%2520millions%2520of%2520parameters.%2520Thus%252C%2520explaining%2520the%2520complex%2520decisions%2520of%250Asuch%2520networks%2520to%2520humans%2520is%2520challenging.%2520A%2520technical%2520approach%2520to%2520reduce%2520CNN%250Acomplexity%2520is%2520network%2520pruning%252C%2520where%2520less%2520important%2520parameters%2520are%2520deleted.%2520The%250Awork%2520presented%2520in%2520this%2520paper%2520investigates%2520whether%2520this%2520technical%2520complexity%250Areduction%2520also%2520helps%2520with%2520perceived%2520explainability.%2520To%2520do%2520so%252C%2520we%2520conducted%2520a%250Apre-study%2520and%2520two%2520human-grounded%2520experiments%252C%2520assessing%2520the%2520effects%2520of%250Adifferent%2520pruning%2520ratios%2520on%2520CNN%2520explainability.%2520Overall%252C%2520we%2520evaluated%2520four%250Adifferent%2520compression%2520rates%2520%2528i.e.%252C%2520CPR%25202%252C%25204%252C%25208%252C%2520and%252032%2529%2520with%252037%2520500%2520tasks%2520on%250AMechanical%2520Turk.%2520Results%2520indicate%2520that%2520lower%2520compression%2520rates%2520have%2520a%2520positive%250Ainfluence%2520on%2520explainability%252C%2520while%2520higher%2520compression%2520rates%2520show%2520negative%250Aeffects.%2520Furthermore%252C%2520we%2520were%2520able%2520to%2520identify%2520sweet%2520spots%2520that%2520increase%2520both%250Athe%2520perceived%2520explainability%2520and%2520the%2520model%2527s%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.08878v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Less%20is%20More%3A%20The%20Influence%20of%20Pruning%20on%20the%20Explainability%20of%20CNNs&entry.906535625=David%20Weber%20and%20Florian%20Merkle%20and%20Pascal%20Sch%C3%B6ttle%20and%20Stephan%20Schl%C3%B6gl&entry.1292438233=%20%20Modern%2C%20state-of-the-art%20Convolutional%20Neural%20Networks%20%28CNNs%29%20in%20computer%0Avision%20have%20millions%20of%20parameters.%20Thus%2C%20explaining%20the%20complex%20decisions%20of%0Asuch%20networks%20to%20humans%20is%20challenging.%20A%20technical%20approach%20to%20reduce%20CNN%0Acomplexity%20is%20network%20pruning%2C%20where%20less%20important%20parameters%20are%20deleted.%20The%0Awork%20presented%20in%20this%20paper%20investigates%20whether%20this%20technical%20complexity%0Areduction%20also%20helps%20with%20perceived%20explainability.%20To%20do%20so%2C%20we%20conducted%20a%0Apre-study%20and%20two%20human-grounded%20experiments%2C%20assessing%20the%20effects%20of%0Adifferent%20pruning%20ratios%20on%20CNN%20explainability.%20Overall%2C%20we%20evaluated%20four%0Adifferent%20compression%20rates%20%28i.e.%2C%20CPR%202%2C%204%2C%208%2C%20and%2032%29%20with%2037%20500%20tasks%20on%0AMechanical%20Turk.%20Results%20indicate%20that%20lower%20compression%20rates%20have%20a%20positive%0Ainfluence%20on%20explainability%2C%20while%20higher%20compression%20rates%20show%20negative%0Aeffects.%20Furthermore%2C%20we%20were%20able%20to%20identify%20sweet%20spots%20that%20increase%20both%0Athe%20perceived%20explainability%20and%20the%20model%27s%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.08878v2&entry.124074799=Read"},
{"title": "Gradient-based facial encoding for key generation to encrypt and decrypt\n  multimedia data", "author": "Ankit Kumar Patel and Dewanshi Paul and Sarthak Giri and Sneha Chaudhary and Bikalpa Gautam", "abstract": "  Security systems relying on passwords are vulnerable to being forgotten,\nguessed, or breached. Likewise, biometric systems that operate independently\nare at risk of template spoofing and replay incidents. This paper introduces a\nbiocryptosystem utilizing face recognition techniques to address these issues,\nallowing for the encryption and decryption of various file types through the\nAdvanced Encryption Standard (AES). The proposed system creates a distinct\n32-bit encryption key derived from facial features identified by Histogram of\nOriented Gradients (HOG) and categorized using Support Vector Machines (SVM).\nHOG efficiently identifies edge-aligned facial features, even in dim lighting,\nensuring that reliable biometric keys can be generated. This key is then used\nwith AES to encrypt and decrypt a variety of data formats, such as text, audio,\nand video files. This encryption key, derived from an individual's distinctive\nfacial traits, is exceedingly challenging for adversaries to reproduce or\nguess. The security and performance of the system have been validated through\nexperiments using several metrics, including correlation analysis, Shannon\nentropy, normalized Hamming distance, and the avalanche effect on 25 different\nfile types. Potential uses for the proposed system include secure file sharing,\nonline transactions, and data archiving, making it a strong and trustworthy\napproach to safeguarding sensitive information by integrating the uniqueness of\nfacial biometrics with the established security of AES encryption.\n", "link": "http://arxiv.org/abs/2412.06927v2", "date": "2025-01-09", "relevancy": 2.3295, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5098}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4439}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient-based%20facial%20encoding%20for%20key%20generation%20to%20encrypt%20and%20decrypt%0A%20%20multimedia%20data&body=Title%3A%20Gradient-based%20facial%20encoding%20for%20key%20generation%20to%20encrypt%20and%20decrypt%0A%20%20multimedia%20data%0AAuthor%3A%20Ankit%20Kumar%20Patel%20and%20Dewanshi%20Paul%20and%20Sarthak%20Giri%20and%20Sneha%20Chaudhary%20and%20Bikalpa%20Gautam%0AAbstract%3A%20%20%20Security%20systems%20relying%20on%20passwords%20are%20vulnerable%20to%20being%20forgotten%2C%0Aguessed%2C%20or%20breached.%20Likewise%2C%20biometric%20systems%20that%20operate%20independently%0Aare%20at%20risk%20of%20template%20spoofing%20and%20replay%20incidents.%20This%20paper%20introduces%20a%0Abiocryptosystem%20utilizing%20face%20recognition%20techniques%20to%20address%20these%20issues%2C%0Aallowing%20for%20the%20encryption%20and%20decryption%20of%20various%20file%20types%20through%20the%0AAdvanced%20Encryption%20Standard%20%28AES%29.%20The%20proposed%20system%20creates%20a%20distinct%0A32-bit%20encryption%20key%20derived%20from%20facial%20features%20identified%20by%20Histogram%20of%0AOriented%20Gradients%20%28HOG%29%20and%20categorized%20using%20Support%20Vector%20Machines%20%28SVM%29.%0AHOG%20efficiently%20identifies%20edge-aligned%20facial%20features%2C%20even%20in%20dim%20lighting%2C%0Aensuring%20that%20reliable%20biometric%20keys%20can%20be%20generated.%20This%20key%20is%20then%20used%0Awith%20AES%20to%20encrypt%20and%20decrypt%20a%20variety%20of%20data%20formats%2C%20such%20as%20text%2C%20audio%2C%0Aand%20video%20files.%20This%20encryption%20key%2C%20derived%20from%20an%20individual%27s%20distinctive%0Afacial%20traits%2C%20is%20exceedingly%20challenging%20for%20adversaries%20to%20reproduce%20or%0Aguess.%20The%20security%20and%20performance%20of%20the%20system%20have%20been%20validated%20through%0Aexperiments%20using%20several%20metrics%2C%20including%20correlation%20analysis%2C%20Shannon%0Aentropy%2C%20normalized%20Hamming%20distance%2C%20and%20the%20avalanche%20effect%20on%2025%20different%0Afile%20types.%20Potential%20uses%20for%20the%20proposed%20system%20include%20secure%20file%20sharing%2C%0Aonline%20transactions%2C%20and%20data%20archiving%2C%20making%20it%20a%20strong%20and%20trustworthy%0Aapproach%20to%20safeguarding%20sensitive%20information%20by%20integrating%20the%20uniqueness%20of%0Afacial%20biometrics%20with%20the%20established%20security%20of%20AES%20encryption.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.06927v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient-based%2520facial%2520encoding%2520for%2520key%2520generation%2520to%2520encrypt%2520and%2520decrypt%250A%2520%2520multimedia%2520data%26entry.906535625%3DAnkit%2520Kumar%2520Patel%2520and%2520Dewanshi%2520Paul%2520and%2520Sarthak%2520Giri%2520and%2520Sneha%2520Chaudhary%2520and%2520Bikalpa%2520Gautam%26entry.1292438233%3D%2520%2520Security%2520systems%2520relying%2520on%2520passwords%2520are%2520vulnerable%2520to%2520being%2520forgotten%252C%250Aguessed%252C%2520or%2520breached.%2520Likewise%252C%2520biometric%2520systems%2520that%2520operate%2520independently%250Aare%2520at%2520risk%2520of%2520template%2520spoofing%2520and%2520replay%2520incidents.%2520This%2520paper%2520introduces%2520a%250Abiocryptosystem%2520utilizing%2520face%2520recognition%2520techniques%2520to%2520address%2520these%2520issues%252C%250Aallowing%2520for%2520the%2520encryption%2520and%2520decryption%2520of%2520various%2520file%2520types%2520through%2520the%250AAdvanced%2520Encryption%2520Standard%2520%2528AES%2529.%2520The%2520proposed%2520system%2520creates%2520a%2520distinct%250A32-bit%2520encryption%2520key%2520derived%2520from%2520facial%2520features%2520identified%2520by%2520Histogram%2520of%250AOriented%2520Gradients%2520%2528HOG%2529%2520and%2520categorized%2520using%2520Support%2520Vector%2520Machines%2520%2528SVM%2529.%250AHOG%2520efficiently%2520identifies%2520edge-aligned%2520facial%2520features%252C%2520even%2520in%2520dim%2520lighting%252C%250Aensuring%2520that%2520reliable%2520biometric%2520keys%2520can%2520be%2520generated.%2520This%2520key%2520is%2520then%2520used%250Awith%2520AES%2520to%2520encrypt%2520and%2520decrypt%2520a%2520variety%2520of%2520data%2520formats%252C%2520such%2520as%2520text%252C%2520audio%252C%250Aand%2520video%2520files.%2520This%2520encryption%2520key%252C%2520derived%2520from%2520an%2520individual%2527s%2520distinctive%250Afacial%2520traits%252C%2520is%2520exceedingly%2520challenging%2520for%2520adversaries%2520to%2520reproduce%2520or%250Aguess.%2520The%2520security%2520and%2520performance%2520of%2520the%2520system%2520have%2520been%2520validated%2520through%250Aexperiments%2520using%2520several%2520metrics%252C%2520including%2520correlation%2520analysis%252C%2520Shannon%250Aentropy%252C%2520normalized%2520Hamming%2520distance%252C%2520and%2520the%2520avalanche%2520effect%2520on%252025%2520different%250Afile%2520types.%2520Potential%2520uses%2520for%2520the%2520proposed%2520system%2520include%2520secure%2520file%2520sharing%252C%250Aonline%2520transactions%252C%2520and%2520data%2520archiving%252C%2520making%2520it%2520a%2520strong%2520and%2520trustworthy%250Aapproach%2520to%2520safeguarding%2520sensitive%2520information%2520by%2520integrating%2520the%2520uniqueness%2520of%250Afacial%2520biometrics%2520with%2520the%2520established%2520security%2520of%2520AES%2520encryption.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.06927v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient-based%20facial%20encoding%20for%20key%20generation%20to%20encrypt%20and%20decrypt%0A%20%20multimedia%20data&entry.906535625=Ankit%20Kumar%20Patel%20and%20Dewanshi%20Paul%20and%20Sarthak%20Giri%20and%20Sneha%20Chaudhary%20and%20Bikalpa%20Gautam&entry.1292438233=%20%20Security%20systems%20relying%20on%20passwords%20are%20vulnerable%20to%20being%20forgotten%2C%0Aguessed%2C%20or%20breached.%20Likewise%2C%20biometric%20systems%20that%20operate%20independently%0Aare%20at%20risk%20of%20template%20spoofing%20and%20replay%20incidents.%20This%20paper%20introduces%20a%0Abiocryptosystem%20utilizing%20face%20recognition%20techniques%20to%20address%20these%20issues%2C%0Aallowing%20for%20the%20encryption%20and%20decryption%20of%20various%20file%20types%20through%20the%0AAdvanced%20Encryption%20Standard%20%28AES%29.%20The%20proposed%20system%20creates%20a%20distinct%0A32-bit%20encryption%20key%20derived%20from%20facial%20features%20identified%20by%20Histogram%20of%0AOriented%20Gradients%20%28HOG%29%20and%20categorized%20using%20Support%20Vector%20Machines%20%28SVM%29.%0AHOG%20efficiently%20identifies%20edge-aligned%20facial%20features%2C%20even%20in%20dim%20lighting%2C%0Aensuring%20that%20reliable%20biometric%20keys%20can%20be%20generated.%20This%20key%20is%20then%20used%0Awith%20AES%20to%20encrypt%20and%20decrypt%20a%20variety%20of%20data%20formats%2C%20such%20as%20text%2C%20audio%2C%0Aand%20video%20files.%20This%20encryption%20key%2C%20derived%20from%20an%20individual%27s%20distinctive%0Afacial%20traits%2C%20is%20exceedingly%20challenging%20for%20adversaries%20to%20reproduce%20or%0Aguess.%20The%20security%20and%20performance%20of%20the%20system%20have%20been%20validated%20through%0Aexperiments%20using%20several%20metrics%2C%20including%20correlation%20analysis%2C%20Shannon%0Aentropy%2C%20normalized%20Hamming%20distance%2C%20and%20the%20avalanche%20effect%20on%2025%20different%0Afile%20types.%20Potential%20uses%20for%20the%20proposed%20system%20include%20secure%20file%20sharing%2C%0Aonline%20transactions%2C%20and%20data%20archiving%2C%20making%20it%20a%20strong%20and%20trustworthy%0Aapproach%20to%20safeguarding%20sensitive%20information%20by%20integrating%20the%20uniqueness%20of%0Afacial%20biometrics%20with%20the%20established%20security%20of%20AES%20encryption.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.06927v2&entry.124074799=Read"},
{"title": "A Systematic Literature Review on Deep Learning-based Depth Estimation\n  in Computer Vision", "author": "Ali Rohan and Md Junayed Hasan and Andrei Petrovski", "abstract": "  Depth estimation (DE) provides spatial information about a scene and enables\ntasks such as 3D reconstruction, object detection, and scene understanding.\nRecently, there has been an increasing interest in using deep learning\n(DL)-based methods for DE. Traditional techniques rely on handcrafted features\nthat often struggle to generalise to diverse scenes and require extensive\nmanual tuning. However, DL models for DE can automatically extract relevant\nfeatures from input data, adapt to various scene conditions, and generalise\nwell to unseen environments. Numerous DL-based methods have been developed,\nmaking it necessary to survey and synthesize the state-of-the-art (SOTA).\nPrevious reviews on DE have mainly focused on either monocular or stereo-based\ntechniques, rather than comprehensively reviewing DE. Furthermore, to the best\nof our knowledge, there is no systematic literature review (SLR) that\ncomprehensively focuses on DE. Therefore, this SLR study is being conducted.\nInitially, electronic databases were searched for relevant publications,\nresulting in 1284 publications. Using defined exclusion and quality criteria,\n128 publications were shortlisted and further filtered to select 59\nhigh-quality primary studies. These studies were analysed to extract data and\nanswer defined research questions. Based on the results, DL methods were\ndeveloped for mainly three different types of DE: monocular, stereo, and\nmulti-view. 20 publicly available datasets were used to train, test, and\nevaluate DL models for DE, with KITTI, NYU Depth V2, and Make 3D being the most\nused datasets. 29 evaluation metrics were used to assess the performance of DE.\n35 base models were reported in the primary studies, and the top five most-used\nbase models were ResNet-50, ResNet-18, ResNet-101, U-Net, and VGG-16. Finally,\nthe lack of ground truth data was among the most significant challenges\nreported by primary studies.\n", "link": "http://arxiv.org/abs/2501.05147v1", "date": "2025-01-09", "relevancy": 2.3077, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5813}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5813}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Systematic%20Literature%20Review%20on%20Deep%20Learning-based%20Depth%20Estimation%0A%20%20in%20Computer%20Vision&body=Title%3A%20A%20Systematic%20Literature%20Review%20on%20Deep%20Learning-based%20Depth%20Estimation%0A%20%20in%20Computer%20Vision%0AAuthor%3A%20Ali%20Rohan%20and%20Md%20Junayed%20Hasan%20and%20Andrei%20Petrovski%0AAbstract%3A%20%20%20Depth%20estimation%20%28DE%29%20provides%20spatial%20information%20about%20a%20scene%20and%20enables%0Atasks%20such%20as%203D%20reconstruction%2C%20object%20detection%2C%20and%20scene%20understanding.%0ARecently%2C%20there%20has%20been%20an%20increasing%20interest%20in%20using%20deep%20learning%0A%28DL%29-based%20methods%20for%20DE.%20Traditional%20techniques%20rely%20on%20handcrafted%20features%0Athat%20often%20struggle%20to%20generalise%20to%20diverse%20scenes%20and%20require%20extensive%0Amanual%20tuning.%20However%2C%20DL%20models%20for%20DE%20can%20automatically%20extract%20relevant%0Afeatures%20from%20input%20data%2C%20adapt%20to%20various%20scene%20conditions%2C%20and%20generalise%0Awell%20to%20unseen%20environments.%20Numerous%20DL-based%20methods%20have%20been%20developed%2C%0Amaking%20it%20necessary%20to%20survey%20and%20synthesize%20the%20state-of-the-art%20%28SOTA%29.%0APrevious%20reviews%20on%20DE%20have%20mainly%20focused%20on%20either%20monocular%20or%20stereo-based%0Atechniques%2C%20rather%20than%20comprehensively%20reviewing%20DE.%20Furthermore%2C%20to%20the%20best%0Aof%20our%20knowledge%2C%20there%20is%20no%20systematic%20literature%20review%20%28SLR%29%20that%0Acomprehensively%20focuses%20on%20DE.%20Therefore%2C%20this%20SLR%20study%20is%20being%20conducted.%0AInitially%2C%20electronic%20databases%20were%20searched%20for%20relevant%20publications%2C%0Aresulting%20in%201284%20publications.%20Using%20defined%20exclusion%20and%20quality%20criteria%2C%0A128%20publications%20were%20shortlisted%20and%20further%20filtered%20to%20select%2059%0Ahigh-quality%20primary%20studies.%20These%20studies%20were%20analysed%20to%20extract%20data%20and%0Aanswer%20defined%20research%20questions.%20Based%20on%20the%20results%2C%20DL%20methods%20were%0Adeveloped%20for%20mainly%20three%20different%20types%20of%20DE%3A%20monocular%2C%20stereo%2C%20and%0Amulti-view.%2020%20publicly%20available%20datasets%20were%20used%20to%20train%2C%20test%2C%20and%0Aevaluate%20DL%20models%20for%20DE%2C%20with%20KITTI%2C%20NYU%20Depth%20V2%2C%20and%20Make%203D%20being%20the%20most%0Aused%20datasets.%2029%20evaluation%20metrics%20were%20used%20to%20assess%20the%20performance%20of%20DE.%0A35%20base%20models%20were%20reported%20in%20the%20primary%20studies%2C%20and%20the%20top%20five%20most-used%0Abase%20models%20were%20ResNet-50%2C%20ResNet-18%2C%20ResNet-101%2C%20U-Net%2C%20and%20VGG-16.%20Finally%2C%0Athe%20lack%20of%20ground%20truth%20data%20was%20among%20the%20most%20significant%20challenges%0Areported%20by%20primary%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05147v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Systematic%2520Literature%2520Review%2520on%2520Deep%2520Learning-based%2520Depth%2520Estimation%250A%2520%2520in%2520Computer%2520Vision%26entry.906535625%3DAli%2520Rohan%2520and%2520Md%2520Junayed%2520Hasan%2520and%2520Andrei%2520Petrovski%26entry.1292438233%3D%2520%2520Depth%2520estimation%2520%2528DE%2529%2520provides%2520spatial%2520information%2520about%2520a%2520scene%2520and%2520enables%250Atasks%2520such%2520as%25203D%2520reconstruction%252C%2520object%2520detection%252C%2520and%2520scene%2520understanding.%250ARecently%252C%2520there%2520has%2520been%2520an%2520increasing%2520interest%2520in%2520using%2520deep%2520learning%250A%2528DL%2529-based%2520methods%2520for%2520DE.%2520Traditional%2520techniques%2520rely%2520on%2520handcrafted%2520features%250Athat%2520often%2520struggle%2520to%2520generalise%2520to%2520diverse%2520scenes%2520and%2520require%2520extensive%250Amanual%2520tuning.%2520However%252C%2520DL%2520models%2520for%2520DE%2520can%2520automatically%2520extract%2520relevant%250Afeatures%2520from%2520input%2520data%252C%2520adapt%2520to%2520various%2520scene%2520conditions%252C%2520and%2520generalise%250Awell%2520to%2520unseen%2520environments.%2520Numerous%2520DL-based%2520methods%2520have%2520been%2520developed%252C%250Amaking%2520it%2520necessary%2520to%2520survey%2520and%2520synthesize%2520the%2520state-of-the-art%2520%2528SOTA%2529.%250APrevious%2520reviews%2520on%2520DE%2520have%2520mainly%2520focused%2520on%2520either%2520monocular%2520or%2520stereo-based%250Atechniques%252C%2520rather%2520than%2520comprehensively%2520reviewing%2520DE.%2520Furthermore%252C%2520to%2520the%2520best%250Aof%2520our%2520knowledge%252C%2520there%2520is%2520no%2520systematic%2520literature%2520review%2520%2528SLR%2529%2520that%250Acomprehensively%2520focuses%2520on%2520DE.%2520Therefore%252C%2520this%2520SLR%2520study%2520is%2520being%2520conducted.%250AInitially%252C%2520electronic%2520databases%2520were%2520searched%2520for%2520relevant%2520publications%252C%250Aresulting%2520in%25201284%2520publications.%2520Using%2520defined%2520exclusion%2520and%2520quality%2520criteria%252C%250A128%2520publications%2520were%2520shortlisted%2520and%2520further%2520filtered%2520to%2520select%252059%250Ahigh-quality%2520primary%2520studies.%2520These%2520studies%2520were%2520analysed%2520to%2520extract%2520data%2520and%250Aanswer%2520defined%2520research%2520questions.%2520Based%2520on%2520the%2520results%252C%2520DL%2520methods%2520were%250Adeveloped%2520for%2520mainly%2520three%2520different%2520types%2520of%2520DE%253A%2520monocular%252C%2520stereo%252C%2520and%250Amulti-view.%252020%2520publicly%2520available%2520datasets%2520were%2520used%2520to%2520train%252C%2520test%252C%2520and%250Aevaluate%2520DL%2520models%2520for%2520DE%252C%2520with%2520KITTI%252C%2520NYU%2520Depth%2520V2%252C%2520and%2520Make%25203D%2520being%2520the%2520most%250Aused%2520datasets.%252029%2520evaluation%2520metrics%2520were%2520used%2520to%2520assess%2520the%2520performance%2520of%2520DE.%250A35%2520base%2520models%2520were%2520reported%2520in%2520the%2520primary%2520studies%252C%2520and%2520the%2520top%2520five%2520most-used%250Abase%2520models%2520were%2520ResNet-50%252C%2520ResNet-18%252C%2520ResNet-101%252C%2520U-Net%252C%2520and%2520VGG-16.%2520Finally%252C%250Athe%2520lack%2520of%2520ground%2520truth%2520data%2520was%2520among%2520the%2520most%2520significant%2520challenges%250Areported%2520by%2520primary%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05147v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Systematic%20Literature%20Review%20on%20Deep%20Learning-based%20Depth%20Estimation%0A%20%20in%20Computer%20Vision&entry.906535625=Ali%20Rohan%20and%20Md%20Junayed%20Hasan%20and%20Andrei%20Petrovski&entry.1292438233=%20%20Depth%20estimation%20%28DE%29%20provides%20spatial%20information%20about%20a%20scene%20and%20enables%0Atasks%20such%20as%203D%20reconstruction%2C%20object%20detection%2C%20and%20scene%20understanding.%0ARecently%2C%20there%20has%20been%20an%20increasing%20interest%20in%20using%20deep%20learning%0A%28DL%29-based%20methods%20for%20DE.%20Traditional%20techniques%20rely%20on%20handcrafted%20features%0Athat%20often%20struggle%20to%20generalise%20to%20diverse%20scenes%20and%20require%20extensive%0Amanual%20tuning.%20However%2C%20DL%20models%20for%20DE%20can%20automatically%20extract%20relevant%0Afeatures%20from%20input%20data%2C%20adapt%20to%20various%20scene%20conditions%2C%20and%20generalise%0Awell%20to%20unseen%20environments.%20Numerous%20DL-based%20methods%20have%20been%20developed%2C%0Amaking%20it%20necessary%20to%20survey%20and%20synthesize%20the%20state-of-the-art%20%28SOTA%29.%0APrevious%20reviews%20on%20DE%20have%20mainly%20focused%20on%20either%20monocular%20or%20stereo-based%0Atechniques%2C%20rather%20than%20comprehensively%20reviewing%20DE.%20Furthermore%2C%20to%20the%20best%0Aof%20our%20knowledge%2C%20there%20is%20no%20systematic%20literature%20review%20%28SLR%29%20that%0Acomprehensively%20focuses%20on%20DE.%20Therefore%2C%20this%20SLR%20study%20is%20being%20conducted.%0AInitially%2C%20electronic%20databases%20were%20searched%20for%20relevant%20publications%2C%0Aresulting%20in%201284%20publications.%20Using%20defined%20exclusion%20and%20quality%20criteria%2C%0A128%20publications%20were%20shortlisted%20and%20further%20filtered%20to%20select%2059%0Ahigh-quality%20primary%20studies.%20These%20studies%20were%20analysed%20to%20extract%20data%20and%0Aanswer%20defined%20research%20questions.%20Based%20on%20the%20results%2C%20DL%20methods%20were%0Adeveloped%20for%20mainly%20three%20different%20types%20of%20DE%3A%20monocular%2C%20stereo%2C%20and%0Amulti-view.%2020%20publicly%20available%20datasets%20were%20used%20to%20train%2C%20test%2C%20and%0Aevaluate%20DL%20models%20for%20DE%2C%20with%20KITTI%2C%20NYU%20Depth%20V2%2C%20and%20Make%203D%20being%20the%20most%0Aused%20datasets.%2029%20evaluation%20metrics%20were%20used%20to%20assess%20the%20performance%20of%20DE.%0A35%20base%20models%20were%20reported%20in%20the%20primary%20studies%2C%20and%20the%20top%20five%20most-used%0Abase%20models%20were%20ResNet-50%2C%20ResNet-18%2C%20ResNet-101%2C%20U-Net%2C%20and%20VGG-16.%20Finally%2C%0Athe%20lack%20of%20ground%20truth%20data%20was%20among%20the%20most%20significant%20challenges%0Areported%20by%20primary%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05147v1&entry.124074799=Read"},
{"title": "Relative Pose Estimation through Affine Corrections of Monocular Depth\n  Priors", "author": "Yifan Yu and Shaohui Liu and R\u00e9mi Pautrat and Marc Pollefeys and Viktor Larsson", "abstract": "  Monocular depth estimation (MDE) models have undergone significant\nadvancements over recent years. Many MDE models aim to predict affine-invariant\nrelative depth from monocular images, while recent developments in large-scale\ntraining and vision foundation models enable reasonable estimation of metric\n(absolute) depth. However, effectively leveraging these predictions for\ngeometric vision tasks, in particular relative pose estimation, remains\nrelatively under explored. While depths provide rich constraints for cross-view\nimage alignment, the intrinsic noise and ambiguity from the monocular depth\npriors present practical challenges to improving upon classic keypoint-based\nsolutions. In this paper, we develop three solvers for relative pose estimation\nthat explicitly account for independent affine (scale and shift) ambiguities,\ncovering both calibrated and uncalibrated conditions. We further propose a\nhybrid estimation pipeline that combines our proposed solvers with classic\npoint-based solvers and epipolar constraints. We find that the affine\ncorrection modeling is beneficial to not only the relative depth priors but\nalso, surprisingly, the ``metric\" ones. Results across multiple datasets\ndemonstrate large improvements of our approach over classic keypoint-based\nbaselines and PnP-based solutions, under both calibrated and uncalibrated\nsetups. We also show that our method improves consistently with different\nfeature matchers and MDE models, and can further benefit from very recent\nadvances on both modules. Code is available at\nhttps://github.com/MarkYu98/madpose.\n", "link": "http://arxiv.org/abs/2501.05446v1", "date": "2025-01-09", "relevancy": 2.293, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5803}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.573}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relative%20Pose%20Estimation%20through%20Affine%20Corrections%20of%20Monocular%20Depth%0A%20%20Priors&body=Title%3A%20Relative%20Pose%20Estimation%20through%20Affine%20Corrections%20of%20Monocular%20Depth%0A%20%20Priors%0AAuthor%3A%20Yifan%20Yu%20and%20Shaohui%20Liu%20and%20R%C3%A9mi%20Pautrat%20and%20Marc%20Pollefeys%20and%20Viktor%20Larsson%0AAbstract%3A%20%20%20Monocular%20depth%20estimation%20%28MDE%29%20models%20have%20undergone%20significant%0Aadvancements%20over%20recent%20years.%20Many%20MDE%20models%20aim%20to%20predict%20affine-invariant%0Arelative%20depth%20from%20monocular%20images%2C%20while%20recent%20developments%20in%20large-scale%0Atraining%20and%20vision%20foundation%20models%20enable%20reasonable%20estimation%20of%20metric%0A%28absolute%29%20depth.%20However%2C%20effectively%20leveraging%20these%20predictions%20for%0Ageometric%20vision%20tasks%2C%20in%20particular%20relative%20pose%20estimation%2C%20remains%0Arelatively%20under%20explored.%20While%20depths%20provide%20rich%20constraints%20for%20cross-view%0Aimage%20alignment%2C%20the%20intrinsic%20noise%20and%20ambiguity%20from%20the%20monocular%20depth%0Apriors%20present%20practical%20challenges%20to%20improving%20upon%20classic%20keypoint-based%0Asolutions.%20In%20this%20paper%2C%20we%20develop%20three%20solvers%20for%20relative%20pose%20estimation%0Athat%20explicitly%20account%20for%20independent%20affine%20%28scale%20and%20shift%29%20ambiguities%2C%0Acovering%20both%20calibrated%20and%20uncalibrated%20conditions.%20We%20further%20propose%20a%0Ahybrid%20estimation%20pipeline%20that%20combines%20our%20proposed%20solvers%20with%20classic%0Apoint-based%20solvers%20and%20epipolar%20constraints.%20We%20find%20that%20the%20affine%0Acorrection%20modeling%20is%20beneficial%20to%20not%20only%20the%20relative%20depth%20priors%20but%0Aalso%2C%20surprisingly%2C%20the%20%60%60metric%22%20ones.%20Results%20across%20multiple%20datasets%0Ademonstrate%20large%20improvements%20of%20our%20approach%20over%20classic%20keypoint-based%0Abaselines%20and%20PnP-based%20solutions%2C%20under%20both%20calibrated%20and%20uncalibrated%0Asetups.%20We%20also%20show%20that%20our%20method%20improves%20consistently%20with%20different%0Afeature%20matchers%20and%20MDE%20models%2C%20and%20can%20further%20benefit%20from%20very%20recent%0Aadvances%20on%20both%20modules.%20Code%20is%20available%20at%0Ahttps%3A//github.com/MarkYu98/madpose.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05446v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelative%2520Pose%2520Estimation%2520through%2520Affine%2520Corrections%2520of%2520Monocular%2520Depth%250A%2520%2520Priors%26entry.906535625%3DYifan%2520Yu%2520and%2520Shaohui%2520Liu%2520and%2520R%25C3%25A9mi%2520Pautrat%2520and%2520Marc%2520Pollefeys%2520and%2520Viktor%2520Larsson%26entry.1292438233%3D%2520%2520Monocular%2520depth%2520estimation%2520%2528MDE%2529%2520models%2520have%2520undergone%2520significant%250Aadvancements%2520over%2520recent%2520years.%2520Many%2520MDE%2520models%2520aim%2520to%2520predict%2520affine-invariant%250Arelative%2520depth%2520from%2520monocular%2520images%252C%2520while%2520recent%2520developments%2520in%2520large-scale%250Atraining%2520and%2520vision%2520foundation%2520models%2520enable%2520reasonable%2520estimation%2520of%2520metric%250A%2528absolute%2529%2520depth.%2520However%252C%2520effectively%2520leveraging%2520these%2520predictions%2520for%250Ageometric%2520vision%2520tasks%252C%2520in%2520particular%2520relative%2520pose%2520estimation%252C%2520remains%250Arelatively%2520under%2520explored.%2520While%2520depths%2520provide%2520rich%2520constraints%2520for%2520cross-view%250Aimage%2520alignment%252C%2520the%2520intrinsic%2520noise%2520and%2520ambiguity%2520from%2520the%2520monocular%2520depth%250Apriors%2520present%2520practical%2520challenges%2520to%2520improving%2520upon%2520classic%2520keypoint-based%250Asolutions.%2520In%2520this%2520paper%252C%2520we%2520develop%2520three%2520solvers%2520for%2520relative%2520pose%2520estimation%250Athat%2520explicitly%2520account%2520for%2520independent%2520affine%2520%2528scale%2520and%2520shift%2529%2520ambiguities%252C%250Acovering%2520both%2520calibrated%2520and%2520uncalibrated%2520conditions.%2520We%2520further%2520propose%2520a%250Ahybrid%2520estimation%2520pipeline%2520that%2520combines%2520our%2520proposed%2520solvers%2520with%2520classic%250Apoint-based%2520solvers%2520and%2520epipolar%2520constraints.%2520We%2520find%2520that%2520the%2520affine%250Acorrection%2520modeling%2520is%2520beneficial%2520to%2520not%2520only%2520the%2520relative%2520depth%2520priors%2520but%250Aalso%252C%2520surprisingly%252C%2520the%2520%2560%2560metric%2522%2520ones.%2520Results%2520across%2520multiple%2520datasets%250Ademonstrate%2520large%2520improvements%2520of%2520our%2520approach%2520over%2520classic%2520keypoint-based%250Abaselines%2520and%2520PnP-based%2520solutions%252C%2520under%2520both%2520calibrated%2520and%2520uncalibrated%250Asetups.%2520We%2520also%2520show%2520that%2520our%2520method%2520improves%2520consistently%2520with%2520different%250Afeature%2520matchers%2520and%2520MDE%2520models%252C%2520and%2520can%2520further%2520benefit%2520from%2520very%2520recent%250Aadvances%2520on%2520both%2520modules.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/MarkYu98/madpose.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05446v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relative%20Pose%20Estimation%20through%20Affine%20Corrections%20of%20Monocular%20Depth%0A%20%20Priors&entry.906535625=Yifan%20Yu%20and%20Shaohui%20Liu%20and%20R%C3%A9mi%20Pautrat%20and%20Marc%20Pollefeys%20and%20Viktor%20Larsson&entry.1292438233=%20%20Monocular%20depth%20estimation%20%28MDE%29%20models%20have%20undergone%20significant%0Aadvancements%20over%20recent%20years.%20Many%20MDE%20models%20aim%20to%20predict%20affine-invariant%0Arelative%20depth%20from%20monocular%20images%2C%20while%20recent%20developments%20in%20large-scale%0Atraining%20and%20vision%20foundation%20models%20enable%20reasonable%20estimation%20of%20metric%0A%28absolute%29%20depth.%20However%2C%20effectively%20leveraging%20these%20predictions%20for%0Ageometric%20vision%20tasks%2C%20in%20particular%20relative%20pose%20estimation%2C%20remains%0Arelatively%20under%20explored.%20While%20depths%20provide%20rich%20constraints%20for%20cross-view%0Aimage%20alignment%2C%20the%20intrinsic%20noise%20and%20ambiguity%20from%20the%20monocular%20depth%0Apriors%20present%20practical%20challenges%20to%20improving%20upon%20classic%20keypoint-based%0Asolutions.%20In%20this%20paper%2C%20we%20develop%20three%20solvers%20for%20relative%20pose%20estimation%0Athat%20explicitly%20account%20for%20independent%20affine%20%28scale%20and%20shift%29%20ambiguities%2C%0Acovering%20both%20calibrated%20and%20uncalibrated%20conditions.%20We%20further%20propose%20a%0Ahybrid%20estimation%20pipeline%20that%20combines%20our%20proposed%20solvers%20with%20classic%0Apoint-based%20solvers%20and%20epipolar%20constraints.%20We%20find%20that%20the%20affine%0Acorrection%20modeling%20is%20beneficial%20to%20not%20only%20the%20relative%20depth%20priors%20but%0Aalso%2C%20surprisingly%2C%20the%20%60%60metric%22%20ones.%20Results%20across%20multiple%20datasets%0Ademonstrate%20large%20improvements%20of%20our%20approach%20over%20classic%20keypoint-based%0Abaselines%20and%20PnP-based%20solutions%2C%20under%20both%20calibrated%20and%20uncalibrated%0Asetups.%20We%20also%20show%20that%20our%20method%20improves%20consistently%20with%20different%0Afeature%20matchers%20and%20MDE%20models%2C%20and%20can%20further%20benefit%20from%20very%20recent%0Aadvances%20on%20both%20modules.%20Code%20is%20available%20at%0Ahttps%3A//github.com/MarkYu98/madpose.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05446v1&entry.124074799=Read"},
{"title": "Rendering-Oriented 3D Point Cloud Attribute Compression using Sparse\n  Tensor-based Transformer", "author": "Xiao Huo and Junhui Hou and Shuai Wan and Fuzheng Yang", "abstract": "  The evolution of 3D visualization techniques has fundamentally transformed\nhow we interact with digital content. At the forefront of this change is point\ncloud technology, offering an immersive experience that surpasses traditional\n2D representations. However, the massive data size of point clouds presents\nsignificant challenges in data compression. Current methods for lossy point\ncloud attribute compression (PCAC) generally focus on reconstructing the\noriginal point clouds with minimal error. However, for point cloud\nvisualization scenarios, the reconstructed point clouds with distortion still\nneed to undergo a complex rendering process, which affects the final\nuser-perceived quality. In this paper, we propose an end-to-end deep learning\nframework that seamlessly integrates PCAC with differentiable rendering,\ndenoted as rendering-oriented PCAC (RO-PCAC), directly targeting the quality of\nrendered multiview images for viewing. In a differentiable manner, the impact\nof the rendering process on the reconstructed point clouds is taken into\naccount. Moreover, we characterize point clouds as sparse tensors and propose a\nsparse tensor-based transformer, called SP-Trans. By aligning with the local\ndensity of the point cloud and utilizing an enhanced local attention mechanism,\nSP-Trans captures the intricate relationships within the point cloud, further\nimproving feature analysis and synthesis within the framework. Extensive\nexperiments demonstrate that the proposed RO-PCAC achieves state-of-the-art\ncompression performance, compared to existing reconstruction-oriented methods,\nincluding traditional, learning-based, and hybrid methods.\n", "link": "http://arxiv.org/abs/2411.07899v3", "date": "2025-01-09", "relevancy": 2.2771, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5889}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5615}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rendering-Oriented%203D%20Point%20Cloud%20Attribute%20Compression%20using%20Sparse%0A%20%20Tensor-based%20Transformer&body=Title%3A%20Rendering-Oriented%203D%20Point%20Cloud%20Attribute%20Compression%20using%20Sparse%0A%20%20Tensor-based%20Transformer%0AAuthor%3A%20Xiao%20Huo%20and%20Junhui%20Hou%20and%20Shuai%20Wan%20and%20Fuzheng%20Yang%0AAbstract%3A%20%20%20The%20evolution%20of%203D%20visualization%20techniques%20has%20fundamentally%20transformed%0Ahow%20we%20interact%20with%20digital%20content.%20At%20the%20forefront%20of%20this%20change%20is%20point%0Acloud%20technology%2C%20offering%20an%20immersive%20experience%20that%20surpasses%20traditional%0A2D%20representations.%20However%2C%20the%20massive%20data%20size%20of%20point%20clouds%20presents%0Asignificant%20challenges%20in%20data%20compression.%20Current%20methods%20for%20lossy%20point%0Acloud%20attribute%20compression%20%28PCAC%29%20generally%20focus%20on%20reconstructing%20the%0Aoriginal%20point%20clouds%20with%20minimal%20error.%20However%2C%20for%20point%20cloud%0Avisualization%20scenarios%2C%20the%20reconstructed%20point%20clouds%20with%20distortion%20still%0Aneed%20to%20undergo%20a%20complex%20rendering%20process%2C%20which%20affects%20the%20final%0Auser-perceived%20quality.%20In%20this%20paper%2C%20we%20propose%20an%20end-to-end%20deep%20learning%0Aframework%20that%20seamlessly%20integrates%20PCAC%20with%20differentiable%20rendering%2C%0Adenoted%20as%20rendering-oriented%20PCAC%20%28RO-PCAC%29%2C%20directly%20targeting%20the%20quality%20of%0Arendered%20multiview%20images%20for%20viewing.%20In%20a%20differentiable%20manner%2C%20the%20impact%0Aof%20the%20rendering%20process%20on%20the%20reconstructed%20point%20clouds%20is%20taken%20into%0Aaccount.%20Moreover%2C%20we%20characterize%20point%20clouds%20as%20sparse%20tensors%20and%20propose%20a%0Asparse%20tensor-based%20transformer%2C%20called%20SP-Trans.%20By%20aligning%20with%20the%20local%0Adensity%20of%20the%20point%20cloud%20and%20utilizing%20an%20enhanced%20local%20attention%20mechanism%2C%0ASP-Trans%20captures%20the%20intricate%20relationships%20within%20the%20point%20cloud%2C%20further%0Aimproving%20feature%20analysis%20and%20synthesis%20within%20the%20framework.%20Extensive%0Aexperiments%20demonstrate%20that%20the%20proposed%20RO-PCAC%20achieves%20state-of-the-art%0Acompression%20performance%2C%20compared%20to%20existing%20reconstruction-oriented%20methods%2C%0Aincluding%20traditional%2C%20learning-based%2C%20and%20hybrid%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07899v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRendering-Oriented%25203D%2520Point%2520Cloud%2520Attribute%2520Compression%2520using%2520Sparse%250A%2520%2520Tensor-based%2520Transformer%26entry.906535625%3DXiao%2520Huo%2520and%2520Junhui%2520Hou%2520and%2520Shuai%2520Wan%2520and%2520Fuzheng%2520Yang%26entry.1292438233%3D%2520%2520The%2520evolution%2520of%25203D%2520visualization%2520techniques%2520has%2520fundamentally%2520transformed%250Ahow%2520we%2520interact%2520with%2520digital%2520content.%2520At%2520the%2520forefront%2520of%2520this%2520change%2520is%2520point%250Acloud%2520technology%252C%2520offering%2520an%2520immersive%2520experience%2520that%2520surpasses%2520traditional%250A2D%2520representations.%2520However%252C%2520the%2520massive%2520data%2520size%2520of%2520point%2520clouds%2520presents%250Asignificant%2520challenges%2520in%2520data%2520compression.%2520Current%2520methods%2520for%2520lossy%2520point%250Acloud%2520attribute%2520compression%2520%2528PCAC%2529%2520generally%2520focus%2520on%2520reconstructing%2520the%250Aoriginal%2520point%2520clouds%2520with%2520minimal%2520error.%2520However%252C%2520for%2520point%2520cloud%250Avisualization%2520scenarios%252C%2520the%2520reconstructed%2520point%2520clouds%2520with%2520distortion%2520still%250Aneed%2520to%2520undergo%2520a%2520complex%2520rendering%2520process%252C%2520which%2520affects%2520the%2520final%250Auser-perceived%2520quality.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520end-to-end%2520deep%2520learning%250Aframework%2520that%2520seamlessly%2520integrates%2520PCAC%2520with%2520differentiable%2520rendering%252C%250Adenoted%2520as%2520rendering-oriented%2520PCAC%2520%2528RO-PCAC%2529%252C%2520directly%2520targeting%2520the%2520quality%2520of%250Arendered%2520multiview%2520images%2520for%2520viewing.%2520In%2520a%2520differentiable%2520manner%252C%2520the%2520impact%250Aof%2520the%2520rendering%2520process%2520on%2520the%2520reconstructed%2520point%2520clouds%2520is%2520taken%2520into%250Aaccount.%2520Moreover%252C%2520we%2520characterize%2520point%2520clouds%2520as%2520sparse%2520tensors%2520and%2520propose%2520a%250Asparse%2520tensor-based%2520transformer%252C%2520called%2520SP-Trans.%2520By%2520aligning%2520with%2520the%2520local%250Adensity%2520of%2520the%2520point%2520cloud%2520and%2520utilizing%2520an%2520enhanced%2520local%2520attention%2520mechanism%252C%250ASP-Trans%2520captures%2520the%2520intricate%2520relationships%2520within%2520the%2520point%2520cloud%252C%2520further%250Aimproving%2520feature%2520analysis%2520and%2520synthesis%2520within%2520the%2520framework.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520the%2520proposed%2520RO-PCAC%2520achieves%2520state-of-the-art%250Acompression%2520performance%252C%2520compared%2520to%2520existing%2520reconstruction-oriented%2520methods%252C%250Aincluding%2520traditional%252C%2520learning-based%252C%2520and%2520hybrid%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07899v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rendering-Oriented%203D%20Point%20Cloud%20Attribute%20Compression%20using%20Sparse%0A%20%20Tensor-based%20Transformer&entry.906535625=Xiao%20Huo%20and%20Junhui%20Hou%20and%20Shuai%20Wan%20and%20Fuzheng%20Yang&entry.1292438233=%20%20The%20evolution%20of%203D%20visualization%20techniques%20has%20fundamentally%20transformed%0Ahow%20we%20interact%20with%20digital%20content.%20At%20the%20forefront%20of%20this%20change%20is%20point%0Acloud%20technology%2C%20offering%20an%20immersive%20experience%20that%20surpasses%20traditional%0A2D%20representations.%20However%2C%20the%20massive%20data%20size%20of%20point%20clouds%20presents%0Asignificant%20challenges%20in%20data%20compression.%20Current%20methods%20for%20lossy%20point%0Acloud%20attribute%20compression%20%28PCAC%29%20generally%20focus%20on%20reconstructing%20the%0Aoriginal%20point%20clouds%20with%20minimal%20error.%20However%2C%20for%20point%20cloud%0Avisualization%20scenarios%2C%20the%20reconstructed%20point%20clouds%20with%20distortion%20still%0Aneed%20to%20undergo%20a%20complex%20rendering%20process%2C%20which%20affects%20the%20final%0Auser-perceived%20quality.%20In%20this%20paper%2C%20we%20propose%20an%20end-to-end%20deep%20learning%0Aframework%20that%20seamlessly%20integrates%20PCAC%20with%20differentiable%20rendering%2C%0Adenoted%20as%20rendering-oriented%20PCAC%20%28RO-PCAC%29%2C%20directly%20targeting%20the%20quality%20of%0Arendered%20multiview%20images%20for%20viewing.%20In%20a%20differentiable%20manner%2C%20the%20impact%0Aof%20the%20rendering%20process%20on%20the%20reconstructed%20point%20clouds%20is%20taken%20into%0Aaccount.%20Moreover%2C%20we%20characterize%20point%20clouds%20as%20sparse%20tensors%20and%20propose%20a%0Asparse%20tensor-based%20transformer%2C%20called%20SP-Trans.%20By%20aligning%20with%20the%20local%0Adensity%20of%20the%20point%20cloud%20and%20utilizing%20an%20enhanced%20local%20attention%20mechanism%2C%0ASP-Trans%20captures%20the%20intricate%20relationships%20within%20the%20point%20cloud%2C%20further%0Aimproving%20feature%20analysis%20and%20synthesis%20within%20the%20framework.%20Extensive%0Aexperiments%20demonstrate%20that%20the%20proposed%20RO-PCAC%20achieves%20state-of-the-art%0Acompression%20performance%2C%20compared%20to%20existing%20reconstruction-oriented%20methods%2C%0Aincluding%20traditional%2C%20learning-based%2C%20and%20hybrid%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07899v3&entry.124074799=Read"},
{"title": "Compression with Global Guidance: Towards Training-free High-Resolution\n  MLLMs Acceleration", "author": "Xuyang Liu and Ziming Wang and Yuhang Han and Yingyao Wang and Jiale Yuan and Jun Song and Bo Zheng and Linfeng Zhang and Siteng Huang and Honggang Chen", "abstract": "  Multimodal large language models (MLLMs) have attracted considerable\nattention due to their exceptional performance in visual content understanding\nand reasoning. However, their inference efficiency has been a notable concern,\nas the increasing length of multimodal contexts leads to quadratic complexity.\nToken compression techniques, which reduce the number of visual tokens, have\ndemonstrated their effectiveness in reducing computational costs. Yet, these\napproaches have struggled to keep pace with the rapid advancements in MLLMs,\nespecially the AnyRes strategy in the context of high-resolution image\nunderstanding. In this paper, we propose a novel token compression method,\nGlobalCom$^2$, tailored for high-resolution MLLMs that receive both the\nthumbnail and multiple crops. GlobalCom$^2$ treats the tokens derived from the\nthumbnail as the ``commander'' of the entire token compression process,\ndirecting the allocation of retention ratios and the specific compression for\neach crop. In this way, redundant tokens are eliminated while important local\ndetails are adaptively preserved to the highest extent feasible. Empirical\nresults across 10 benchmarks reveal that GlobalCom$^2$ achieves an optimal\nbalance between performance and efficiency, and consistently outperforms\nstate-of-the-art token compression methods with LLaVA-NeXT-7B/13B models. Our\ncode is released at \\url{https://github.com/xuyang-liu16/GlobalCom2}.\n", "link": "http://arxiv.org/abs/2501.05179v1", "date": "2025-01-09", "relevancy": 2.2727, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5747}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5675}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compression%20with%20Global%20Guidance%3A%20Towards%20Training-free%20High-Resolution%0A%20%20MLLMs%20Acceleration&body=Title%3A%20Compression%20with%20Global%20Guidance%3A%20Towards%20Training-free%20High-Resolution%0A%20%20MLLMs%20Acceleration%0AAuthor%3A%20Xuyang%20Liu%20and%20Ziming%20Wang%20and%20Yuhang%20Han%20and%20Yingyao%20Wang%20and%20Jiale%20Yuan%20and%20Jun%20Song%20and%20Bo%20Zheng%20and%20Linfeng%20Zhang%20and%20Siteng%20Huang%20and%20Honggang%20Chen%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20attracted%20considerable%0Aattention%20due%20to%20their%20exceptional%20performance%20in%20visual%20content%20understanding%0Aand%20reasoning.%20However%2C%20their%20inference%20efficiency%20has%20been%20a%20notable%20concern%2C%0Aas%20the%20increasing%20length%20of%20multimodal%20contexts%20leads%20to%20quadratic%20complexity.%0AToken%20compression%20techniques%2C%20which%20reduce%20the%20number%20of%20visual%20tokens%2C%20have%0Ademonstrated%20their%20effectiveness%20in%20reducing%20computational%20costs.%20Yet%2C%20these%0Aapproaches%20have%20struggled%20to%20keep%20pace%20with%20the%20rapid%20advancements%20in%20MLLMs%2C%0Aespecially%20the%20AnyRes%20strategy%20in%20the%20context%20of%20high-resolution%20image%0Aunderstanding.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20token%20compression%20method%2C%0AGlobalCom%24%5E2%24%2C%20tailored%20for%20high-resolution%20MLLMs%20that%20receive%20both%20the%0Athumbnail%20and%20multiple%20crops.%20GlobalCom%24%5E2%24%20treats%20the%20tokens%20derived%20from%20the%0Athumbnail%20as%20the%20%60%60commander%27%27%20of%20the%20entire%20token%20compression%20process%2C%0Adirecting%20the%20allocation%20of%20retention%20ratios%20and%20the%20specific%20compression%20for%0Aeach%20crop.%20In%20this%20way%2C%20redundant%20tokens%20are%20eliminated%20while%20important%20local%0Adetails%20are%20adaptively%20preserved%20to%20the%20highest%20extent%20feasible.%20Empirical%0Aresults%20across%2010%20benchmarks%20reveal%20that%20GlobalCom%24%5E2%24%20achieves%20an%20optimal%0Abalance%20between%20performance%20and%20efficiency%2C%20and%20consistently%20outperforms%0Astate-of-the-art%20token%20compression%20methods%20with%20LLaVA-NeXT-7B/13B%20models.%20Our%0Acode%20is%20released%20at%20%5Curl%7Bhttps%3A//github.com/xuyang-liu16/GlobalCom2%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05179v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompression%2520with%2520Global%2520Guidance%253A%2520Towards%2520Training-free%2520High-Resolution%250A%2520%2520MLLMs%2520Acceleration%26entry.906535625%3DXuyang%2520Liu%2520and%2520Ziming%2520Wang%2520and%2520Yuhang%2520Han%2520and%2520Yingyao%2520Wang%2520and%2520Jiale%2520Yuan%2520and%2520Jun%2520Song%2520and%2520Bo%2520Zheng%2520and%2520Linfeng%2520Zhang%2520and%2520Siteng%2520Huang%2520and%2520Honggang%2520Chen%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520attracted%2520considerable%250Aattention%2520due%2520to%2520their%2520exceptional%2520performance%2520in%2520visual%2520content%2520understanding%250Aand%2520reasoning.%2520However%252C%2520their%2520inference%2520efficiency%2520has%2520been%2520a%2520notable%2520concern%252C%250Aas%2520the%2520increasing%2520length%2520of%2520multimodal%2520contexts%2520leads%2520to%2520quadratic%2520complexity.%250AToken%2520compression%2520techniques%252C%2520which%2520reduce%2520the%2520number%2520of%2520visual%2520tokens%252C%2520have%250Ademonstrated%2520their%2520effectiveness%2520in%2520reducing%2520computational%2520costs.%2520Yet%252C%2520these%250Aapproaches%2520have%2520struggled%2520to%2520keep%2520pace%2520with%2520the%2520rapid%2520advancements%2520in%2520MLLMs%252C%250Aespecially%2520the%2520AnyRes%2520strategy%2520in%2520the%2520context%2520of%2520high-resolution%2520image%250Aunderstanding.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520token%2520compression%2520method%252C%250AGlobalCom%2524%255E2%2524%252C%2520tailored%2520for%2520high-resolution%2520MLLMs%2520that%2520receive%2520both%2520the%250Athumbnail%2520and%2520multiple%2520crops.%2520GlobalCom%2524%255E2%2524%2520treats%2520the%2520tokens%2520derived%2520from%2520the%250Athumbnail%2520as%2520the%2520%2560%2560commander%2527%2527%2520of%2520the%2520entire%2520token%2520compression%2520process%252C%250Adirecting%2520the%2520allocation%2520of%2520retention%2520ratios%2520and%2520the%2520specific%2520compression%2520for%250Aeach%2520crop.%2520In%2520this%2520way%252C%2520redundant%2520tokens%2520are%2520eliminated%2520while%2520important%2520local%250Adetails%2520are%2520adaptively%2520preserved%2520to%2520the%2520highest%2520extent%2520feasible.%2520Empirical%250Aresults%2520across%252010%2520benchmarks%2520reveal%2520that%2520GlobalCom%2524%255E2%2524%2520achieves%2520an%2520optimal%250Abalance%2520between%2520performance%2520and%2520efficiency%252C%2520and%2520consistently%2520outperforms%250Astate-of-the-art%2520token%2520compression%2520methods%2520with%2520LLaVA-NeXT-7B/13B%2520models.%2520Our%250Acode%2520is%2520released%2520at%2520%255Curl%257Bhttps%253A//github.com/xuyang-liu16/GlobalCom2%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05179v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compression%20with%20Global%20Guidance%3A%20Towards%20Training-free%20High-Resolution%0A%20%20MLLMs%20Acceleration&entry.906535625=Xuyang%20Liu%20and%20Ziming%20Wang%20and%20Yuhang%20Han%20and%20Yingyao%20Wang%20and%20Jiale%20Yuan%20and%20Jun%20Song%20and%20Bo%20Zheng%20and%20Linfeng%20Zhang%20and%20Siteng%20Huang%20and%20Honggang%20Chen&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20attracted%20considerable%0Aattention%20due%20to%20their%20exceptional%20performance%20in%20visual%20content%20understanding%0Aand%20reasoning.%20However%2C%20their%20inference%20efficiency%20has%20been%20a%20notable%20concern%2C%0Aas%20the%20increasing%20length%20of%20multimodal%20contexts%20leads%20to%20quadratic%20complexity.%0AToken%20compression%20techniques%2C%20which%20reduce%20the%20number%20of%20visual%20tokens%2C%20have%0Ademonstrated%20their%20effectiveness%20in%20reducing%20computational%20costs.%20Yet%2C%20these%0Aapproaches%20have%20struggled%20to%20keep%20pace%20with%20the%20rapid%20advancements%20in%20MLLMs%2C%0Aespecially%20the%20AnyRes%20strategy%20in%20the%20context%20of%20high-resolution%20image%0Aunderstanding.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20token%20compression%20method%2C%0AGlobalCom%24%5E2%24%2C%20tailored%20for%20high-resolution%20MLLMs%20that%20receive%20both%20the%0Athumbnail%20and%20multiple%20crops.%20GlobalCom%24%5E2%24%20treats%20the%20tokens%20derived%20from%20the%0Athumbnail%20as%20the%20%60%60commander%27%27%20of%20the%20entire%20token%20compression%20process%2C%0Adirecting%20the%20allocation%20of%20retention%20ratios%20and%20the%20specific%20compression%20for%0Aeach%20crop.%20In%20this%20way%2C%20redundant%20tokens%20are%20eliminated%20while%20important%20local%0Adetails%20are%20adaptively%20preserved%20to%20the%20highest%20extent%20feasible.%20Empirical%0Aresults%20across%2010%20benchmarks%20reveal%20that%20GlobalCom%24%5E2%24%20achieves%20an%20optimal%0Abalance%20between%20performance%20and%20efficiency%2C%20and%20consistently%20outperforms%0Astate-of-the-art%20token%20compression%20methods%20with%20LLaVA-NeXT-7B/13B%20models.%20Our%0Acode%20is%20released%20at%20%5Curl%7Bhttps%3A//github.com/xuyang-liu16/GlobalCom2%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05179v1&entry.124074799=Read"},
{"title": "A survey of textual cyber abuse detection using cutting-edge language\n  models and large language models", "author": "Jose A. Diaz-Garcia and Joao Paulo Carvalho", "abstract": "  The success of social media platforms has facilitated the emergence of\nvarious forms of online abuse within digital communities. This abuse manifests\nin multiple ways, including hate speech, cyberbullying, emotional abuse,\ngrooming, and sexting. In this paper, we present a comprehensive analysis of\nthe different forms of abuse prevalent in social media, with a particular focus\non how emerging technologies, such as Language Models (LMs) and Large Language\nModels (LLMs), are reshaping both the detection and generation of abusive\ncontent within these networks. We delve into the mechanisms through which\nsocial media abuse is perpetuated, exploring the psychological and social\nimpact. Additionally, we examine the dual role of advanced language\nmodels-highlighting their potential to enhance automated detection systems for\nabusive behavior while also acknowledging their capacity to generate harmful\ncontent. This paper aims to contribute to the ongoing discourse on online\nsafety and ethics, offering insights into the evolving landscape of cyberabuse\nand the technological innovations that both mitigate and exacerbate it.\n", "link": "http://arxiv.org/abs/2501.05443v1", "date": "2025-01-09", "relevancy": 2.2487, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4556}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4556}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20survey%20of%20textual%20cyber%20abuse%20detection%20using%20cutting-edge%20language%0A%20%20models%20and%20large%20language%20models&body=Title%3A%20A%20survey%20of%20textual%20cyber%20abuse%20detection%20using%20cutting-edge%20language%0A%20%20models%20and%20large%20language%20models%0AAuthor%3A%20Jose%20A.%20Diaz-Garcia%20and%20Joao%20Paulo%20Carvalho%0AAbstract%3A%20%20%20The%20success%20of%20social%20media%20platforms%20has%20facilitated%20the%20emergence%20of%0Avarious%20forms%20of%20online%20abuse%20within%20digital%20communities.%20This%20abuse%20manifests%0Ain%20multiple%20ways%2C%20including%20hate%20speech%2C%20cyberbullying%2C%20emotional%20abuse%2C%0Agrooming%2C%20and%20sexting.%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20analysis%20of%0Athe%20different%20forms%20of%20abuse%20prevalent%20in%20social%20media%2C%20with%20a%20particular%20focus%0Aon%20how%20emerging%20technologies%2C%20such%20as%20Language%20Models%20%28LMs%29%20and%20Large%20Language%0AModels%20%28LLMs%29%2C%20are%20reshaping%20both%20the%20detection%20and%20generation%20of%20abusive%0Acontent%20within%20these%20networks.%20We%20delve%20into%20the%20mechanisms%20through%20which%0Asocial%20media%20abuse%20is%20perpetuated%2C%20exploring%20the%20psychological%20and%20social%0Aimpact.%20Additionally%2C%20we%20examine%20the%20dual%20role%20of%20advanced%20language%0Amodels-highlighting%20their%20potential%20to%20enhance%20automated%20detection%20systems%20for%0Aabusive%20behavior%20while%20also%20acknowledging%20their%20capacity%20to%20generate%20harmful%0Acontent.%20This%20paper%20aims%20to%20contribute%20to%20the%20ongoing%20discourse%20on%20online%0Asafety%20and%20ethics%2C%20offering%20insights%20into%20the%20evolving%20landscape%20of%20cyberabuse%0Aand%20the%20technological%20innovations%20that%20both%20mitigate%20and%20exacerbate%20it.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05443v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520survey%2520of%2520textual%2520cyber%2520abuse%2520detection%2520using%2520cutting-edge%2520language%250A%2520%2520models%2520and%2520large%2520language%2520models%26entry.906535625%3DJose%2520A.%2520Diaz-Garcia%2520and%2520Joao%2520Paulo%2520Carvalho%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520social%2520media%2520platforms%2520has%2520facilitated%2520the%2520emergence%2520of%250Avarious%2520forms%2520of%2520online%2520abuse%2520within%2520digital%2520communities.%2520This%2520abuse%2520manifests%250Ain%2520multiple%2520ways%252C%2520including%2520hate%2520speech%252C%2520cyberbullying%252C%2520emotional%2520abuse%252C%250Agrooming%252C%2520and%2520sexting.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520comprehensive%2520analysis%2520of%250Athe%2520different%2520forms%2520of%2520abuse%2520prevalent%2520in%2520social%2520media%252C%2520with%2520a%2520particular%2520focus%250Aon%2520how%2520emerging%2520technologies%252C%2520such%2520as%2520Language%2520Models%2520%2528LMs%2529%2520and%2520Large%2520Language%250AModels%2520%2528LLMs%2529%252C%2520are%2520reshaping%2520both%2520the%2520detection%2520and%2520generation%2520of%2520abusive%250Acontent%2520within%2520these%2520networks.%2520We%2520delve%2520into%2520the%2520mechanisms%2520through%2520which%250Asocial%2520media%2520abuse%2520is%2520perpetuated%252C%2520exploring%2520the%2520psychological%2520and%2520social%250Aimpact.%2520Additionally%252C%2520we%2520examine%2520the%2520dual%2520role%2520of%2520advanced%2520language%250Amodels-highlighting%2520their%2520potential%2520to%2520enhance%2520automated%2520detection%2520systems%2520for%250Aabusive%2520behavior%2520while%2520also%2520acknowledging%2520their%2520capacity%2520to%2520generate%2520harmful%250Acontent.%2520This%2520paper%2520aims%2520to%2520contribute%2520to%2520the%2520ongoing%2520discourse%2520on%2520online%250Asafety%2520and%2520ethics%252C%2520offering%2520insights%2520into%2520the%2520evolving%2520landscape%2520of%2520cyberabuse%250Aand%2520the%2520technological%2520innovations%2520that%2520both%2520mitigate%2520and%2520exacerbate%2520it.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05443v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20survey%20of%20textual%20cyber%20abuse%20detection%20using%20cutting-edge%20language%0A%20%20models%20and%20large%20language%20models&entry.906535625=Jose%20A.%20Diaz-Garcia%20and%20Joao%20Paulo%20Carvalho&entry.1292438233=%20%20The%20success%20of%20social%20media%20platforms%20has%20facilitated%20the%20emergence%20of%0Avarious%20forms%20of%20online%20abuse%20within%20digital%20communities.%20This%20abuse%20manifests%0Ain%20multiple%20ways%2C%20including%20hate%20speech%2C%20cyberbullying%2C%20emotional%20abuse%2C%0Agrooming%2C%20and%20sexting.%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20analysis%20of%0Athe%20different%20forms%20of%20abuse%20prevalent%20in%20social%20media%2C%20with%20a%20particular%20focus%0Aon%20how%20emerging%20technologies%2C%20such%20as%20Language%20Models%20%28LMs%29%20and%20Large%20Language%0AModels%20%28LLMs%29%2C%20are%20reshaping%20both%20the%20detection%20and%20generation%20of%20abusive%0Acontent%20within%20these%20networks.%20We%20delve%20into%20the%20mechanisms%20through%20which%0Asocial%20media%20abuse%20is%20perpetuated%2C%20exploring%20the%20psychological%20and%20social%0Aimpact.%20Additionally%2C%20we%20examine%20the%20dual%20role%20of%20advanced%20language%0Amodels-highlighting%20their%20potential%20to%20enhance%20automated%20detection%20systems%20for%0Aabusive%20behavior%20while%20also%20acknowledging%20their%20capacity%20to%20generate%20harmful%0Acontent.%20This%20paper%20aims%20to%20contribute%20to%20the%20ongoing%20discourse%20on%20online%0Asafety%20and%20ethics%2C%20offering%20insights%20into%20the%20evolving%20landscape%20of%20cyberabuse%0Aand%20the%20technological%20innovations%20that%20both%20mitigate%20and%20exacerbate%20it.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05443v1&entry.124074799=Read"},
{"title": "Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training", "author": "Elia Cunegatti and Leonardo Lucio Custode and Giovanni Iacca", "abstract": "  Network pruning focuses on computational techniques that aim to reduce a\ngiven model's computational cost by removing a subset of its parameters while\nhaving minimal impact on performance. Throughout the last decade, the most\nwidely used pruning paradigm has been pruning and re-training, which nowadays\nis inconvenient due to the vast amount of pre-trained models, which are in any\ncase too expensive to re-train. In this paper, we exploit functional\ninformation from dense pre-trained models, i.e., their activations, to obtain\nsparse models that maximize the activations' alignment w.r.t. their\ncorresponding dense models. Hence, we propose \\textsc{NeuroAL}, a \\emph{top-up}\nalgorithm that can be used on top of any given pruning algorithm for LLMs,\nwhich modifies the block-wise and row-wise sparsity exploiting information from\nboth the dense model and its sparse version to maximize the \\emph{neuron\nalignment} among activations. Differently from existing methods, our approach\nadaptively selects the best hyperparameters for the block-wise and row-wise\nsparsity ratios w.r.t. the model and the desired sparsity, and requires\n\\emph{no re-training}. We test our method over 276 cases combining four LLM\nfamilies, three sparsity ratios, and ten language tasks (three language\nmodeling and seven zero-shot datasets), showing how it consistently outperforms\nthe latest state-of-the-art methods in terms of performance-runtime trade-off.\nThe code is available at\n\\href{https://github.com/eliacunegatti/NeuroAL}{https://github.com/eliacunegatti/NeuroAL}.\n", "link": "http://arxiv.org/abs/2411.07066v2", "date": "2025-01-09", "relevancy": 2.2249, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4489}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4432}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zeroth-Order%20Adaptive%20Neuron%20Alignment%20Based%20Pruning%20without%20Re-Training&body=Title%3A%20Zeroth-Order%20Adaptive%20Neuron%20Alignment%20Based%20Pruning%20without%20Re-Training%0AAuthor%3A%20Elia%20Cunegatti%20and%20Leonardo%20Lucio%20Custode%20and%20Giovanni%20Iacca%0AAbstract%3A%20%20%20Network%20pruning%20focuses%20on%20computational%20techniques%20that%20aim%20to%20reduce%20a%0Agiven%20model%27s%20computational%20cost%20by%20removing%20a%20subset%20of%20its%20parameters%20while%0Ahaving%20minimal%20impact%20on%20performance.%20Throughout%20the%20last%20decade%2C%20the%20most%0Awidely%20used%20pruning%20paradigm%20has%20been%20pruning%20and%20re-training%2C%20which%20nowadays%0Ais%20inconvenient%20due%20to%20the%20vast%20amount%20of%20pre-trained%20models%2C%20which%20are%20in%20any%0Acase%20too%20expensive%20to%20re-train.%20In%20this%20paper%2C%20we%20exploit%20functional%0Ainformation%20from%20dense%20pre-trained%20models%2C%20i.e.%2C%20their%20activations%2C%20to%20obtain%0Asparse%20models%20that%20maximize%20the%20activations%27%20alignment%20w.r.t.%20their%0Acorresponding%20dense%20models.%20Hence%2C%20we%20propose%20%5Ctextsc%7BNeuroAL%7D%2C%20a%20%5Cemph%7Btop-up%7D%0Aalgorithm%20that%20can%20be%20used%20on%20top%20of%20any%20given%20pruning%20algorithm%20for%20LLMs%2C%0Awhich%20modifies%20the%20block-wise%20and%20row-wise%20sparsity%20exploiting%20information%20from%0Aboth%20the%20dense%20model%20and%20its%20sparse%20version%20to%20maximize%20the%20%5Cemph%7Bneuron%0Aalignment%7D%20among%20activations.%20Differently%20from%20existing%20methods%2C%20our%20approach%0Aadaptively%20selects%20the%20best%20hyperparameters%20for%20the%20block-wise%20and%20row-wise%0Asparsity%20ratios%20w.r.t.%20the%20model%20and%20the%20desired%20sparsity%2C%20and%20requires%0A%5Cemph%7Bno%20re-training%7D.%20We%20test%20our%20method%20over%20276%20cases%20combining%20four%20LLM%0Afamilies%2C%20three%20sparsity%20ratios%2C%20and%20ten%20language%20tasks%20%28three%20language%0Amodeling%20and%20seven%20zero-shot%20datasets%29%2C%20showing%20how%20it%20consistently%20outperforms%0Athe%20latest%20state-of-the-art%20methods%20in%20terms%20of%20performance-runtime%20trade-off.%0AThe%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/eliacunegatti/NeuroAL%7D%7Bhttps%3A//github.com/eliacunegatti/NeuroAL%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07066v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZeroth-Order%2520Adaptive%2520Neuron%2520Alignment%2520Based%2520Pruning%2520without%2520Re-Training%26entry.906535625%3DElia%2520Cunegatti%2520and%2520Leonardo%2520Lucio%2520Custode%2520and%2520Giovanni%2520Iacca%26entry.1292438233%3D%2520%2520Network%2520pruning%2520focuses%2520on%2520computational%2520techniques%2520that%2520aim%2520to%2520reduce%2520a%250Agiven%2520model%2527s%2520computational%2520cost%2520by%2520removing%2520a%2520subset%2520of%2520its%2520parameters%2520while%250Ahaving%2520minimal%2520impact%2520on%2520performance.%2520Throughout%2520the%2520last%2520decade%252C%2520the%2520most%250Awidely%2520used%2520pruning%2520paradigm%2520has%2520been%2520pruning%2520and%2520re-training%252C%2520which%2520nowadays%250Ais%2520inconvenient%2520due%2520to%2520the%2520vast%2520amount%2520of%2520pre-trained%2520models%252C%2520which%2520are%2520in%2520any%250Acase%2520too%2520expensive%2520to%2520re-train.%2520In%2520this%2520paper%252C%2520we%2520exploit%2520functional%250Ainformation%2520from%2520dense%2520pre-trained%2520models%252C%2520i.e.%252C%2520their%2520activations%252C%2520to%2520obtain%250Asparse%2520models%2520that%2520maximize%2520the%2520activations%2527%2520alignment%2520w.r.t.%2520their%250Acorresponding%2520dense%2520models.%2520Hence%252C%2520we%2520propose%2520%255Ctextsc%257BNeuroAL%257D%252C%2520a%2520%255Cemph%257Btop-up%257D%250Aalgorithm%2520that%2520can%2520be%2520used%2520on%2520top%2520of%2520any%2520given%2520pruning%2520algorithm%2520for%2520LLMs%252C%250Awhich%2520modifies%2520the%2520block-wise%2520and%2520row-wise%2520sparsity%2520exploiting%2520information%2520from%250Aboth%2520the%2520dense%2520model%2520and%2520its%2520sparse%2520version%2520to%2520maximize%2520the%2520%255Cemph%257Bneuron%250Aalignment%257D%2520among%2520activations.%2520Differently%2520from%2520existing%2520methods%252C%2520our%2520approach%250Aadaptively%2520selects%2520the%2520best%2520hyperparameters%2520for%2520the%2520block-wise%2520and%2520row-wise%250Asparsity%2520ratios%2520w.r.t.%2520the%2520model%2520and%2520the%2520desired%2520sparsity%252C%2520and%2520requires%250A%255Cemph%257Bno%2520re-training%257D.%2520We%2520test%2520our%2520method%2520over%2520276%2520cases%2520combining%2520four%2520LLM%250Afamilies%252C%2520three%2520sparsity%2520ratios%252C%2520and%2520ten%2520language%2520tasks%2520%2528three%2520language%250Amodeling%2520and%2520seven%2520zero-shot%2520datasets%2529%252C%2520showing%2520how%2520it%2520consistently%2520outperforms%250Athe%2520latest%2520state-of-the-art%2520methods%2520in%2520terms%2520of%2520performance-runtime%2520trade-off.%250AThe%2520code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/eliacunegatti/NeuroAL%257D%257Bhttps%253A//github.com/eliacunegatti/NeuroAL%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07066v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zeroth-Order%20Adaptive%20Neuron%20Alignment%20Based%20Pruning%20without%20Re-Training&entry.906535625=Elia%20Cunegatti%20and%20Leonardo%20Lucio%20Custode%20and%20Giovanni%20Iacca&entry.1292438233=%20%20Network%20pruning%20focuses%20on%20computational%20techniques%20that%20aim%20to%20reduce%20a%0Agiven%20model%27s%20computational%20cost%20by%20removing%20a%20subset%20of%20its%20parameters%20while%0Ahaving%20minimal%20impact%20on%20performance.%20Throughout%20the%20last%20decade%2C%20the%20most%0Awidely%20used%20pruning%20paradigm%20has%20been%20pruning%20and%20re-training%2C%20which%20nowadays%0Ais%20inconvenient%20due%20to%20the%20vast%20amount%20of%20pre-trained%20models%2C%20which%20are%20in%20any%0Acase%20too%20expensive%20to%20re-train.%20In%20this%20paper%2C%20we%20exploit%20functional%0Ainformation%20from%20dense%20pre-trained%20models%2C%20i.e.%2C%20their%20activations%2C%20to%20obtain%0Asparse%20models%20that%20maximize%20the%20activations%27%20alignment%20w.r.t.%20their%0Acorresponding%20dense%20models.%20Hence%2C%20we%20propose%20%5Ctextsc%7BNeuroAL%7D%2C%20a%20%5Cemph%7Btop-up%7D%0Aalgorithm%20that%20can%20be%20used%20on%20top%20of%20any%20given%20pruning%20algorithm%20for%20LLMs%2C%0Awhich%20modifies%20the%20block-wise%20and%20row-wise%20sparsity%20exploiting%20information%20from%0Aboth%20the%20dense%20model%20and%20its%20sparse%20version%20to%20maximize%20the%20%5Cemph%7Bneuron%0Aalignment%7D%20among%20activations.%20Differently%20from%20existing%20methods%2C%20our%20approach%0Aadaptively%20selects%20the%20best%20hyperparameters%20for%20the%20block-wise%20and%20row-wise%0Asparsity%20ratios%20w.r.t.%20the%20model%20and%20the%20desired%20sparsity%2C%20and%20requires%0A%5Cemph%7Bno%20re-training%7D.%20We%20test%20our%20method%20over%20276%20cases%20combining%20four%20LLM%0Afamilies%2C%20three%20sparsity%20ratios%2C%20and%20ten%20language%20tasks%20%28three%20language%0Amodeling%20and%20seven%20zero-shot%20datasets%29%2C%20showing%20how%20it%20consistently%20outperforms%0Athe%20latest%20state-of-the-art%20methods%20in%20terms%20of%20performance-runtime%20trade-off.%0AThe%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/eliacunegatti/NeuroAL%7D%7Bhttps%3A//github.com/eliacunegatti/NeuroAL%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07066v2&entry.124074799=Read"},
{"title": "Automated external cervical resorption segmentation in cone-beam CT\n  using local texture features", "author": "Sadhana Ravikumar and Asma A. Khan and Matthew C. Davis and Beatriz Paniagua", "abstract": "  External cervical resorption (ECR) is a resorptive process affecting teeth.\nWhile in some patients, active resorption ceases and gets replaced by osseous\ntissue, in other cases, the resorption progresses and ultimately results in\ntooth loss. For proper ECR assessment, cone-beam computed tomography (CBCT) is\nthe recommended imaging modality, enabling a 3-D characterization of these\nlesions. While it is possible to manually identify and measure ECR resorption\nin CBCT scans, this process can be time intensive and highly subject to human\nerror. Therefore, there is an urgent need to develop an automated method to\nidentify and quantify the severity of ECR resorption using CBCT. Here, we\npresent a method for ECR lesion segmentation that is based on automatic, binary\nclassification of locally extracted voxel-wise texture features. We evaluate\nour method on 6 longitudinal CBCT datasets and show that certain\ntexture-features can be used to accurately detect subtle CBCT signal changes\ndue to ECR. We also present preliminary analyses clustering texture features\nwithin a lesion to stratify the defects and identify patterns indicative of\ncalcification. These methods are important steps in developing prognostic\nbiomarkers to predict whether ECR will continue to progress or cease,\nultimately informing treatment decisions.\n", "link": "http://arxiv.org/abs/2501.05236v1", "date": "2025-01-09", "relevancy": 2.2157, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4453}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4453}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20external%20cervical%20resorption%20segmentation%20in%20cone-beam%20CT%0A%20%20using%20local%20texture%20features&body=Title%3A%20Automated%20external%20cervical%20resorption%20segmentation%20in%20cone-beam%20CT%0A%20%20using%20local%20texture%20features%0AAuthor%3A%20Sadhana%20Ravikumar%20and%20Asma%20A.%20Khan%20and%20Matthew%20C.%20Davis%20and%20Beatriz%20Paniagua%0AAbstract%3A%20%20%20External%20cervical%20resorption%20%28ECR%29%20is%20a%20resorptive%20process%20affecting%20teeth.%0AWhile%20in%20some%20patients%2C%20active%20resorption%20ceases%20and%20gets%20replaced%20by%20osseous%0Atissue%2C%20in%20other%20cases%2C%20the%20resorption%20progresses%20and%20ultimately%20results%20in%0Atooth%20loss.%20For%20proper%20ECR%20assessment%2C%20cone-beam%20computed%20tomography%20%28CBCT%29%20is%0Athe%20recommended%20imaging%20modality%2C%20enabling%20a%203-D%20characterization%20of%20these%0Alesions.%20While%20it%20is%20possible%20to%20manually%20identify%20and%20measure%20ECR%20resorption%0Ain%20CBCT%20scans%2C%20this%20process%20can%20be%20time%20intensive%20and%20highly%20subject%20to%20human%0Aerror.%20Therefore%2C%20there%20is%20an%20urgent%20need%20to%20develop%20an%20automated%20method%20to%0Aidentify%20and%20quantify%20the%20severity%20of%20ECR%20resorption%20using%20CBCT.%20Here%2C%20we%0Apresent%20a%20method%20for%20ECR%20lesion%20segmentation%20that%20is%20based%20on%20automatic%2C%20binary%0Aclassification%20of%20locally%20extracted%20voxel-wise%20texture%20features.%20We%20evaluate%0Aour%20method%20on%206%20longitudinal%20CBCT%20datasets%20and%20show%20that%20certain%0Atexture-features%20can%20be%20used%20to%20accurately%20detect%20subtle%20CBCT%20signal%20changes%0Adue%20to%20ECR.%20We%20also%20present%20preliminary%20analyses%20clustering%20texture%20features%0Awithin%20a%20lesion%20to%20stratify%20the%20defects%20and%20identify%20patterns%20indicative%20of%0Acalcification.%20These%20methods%20are%20important%20steps%20in%20developing%20prognostic%0Abiomarkers%20to%20predict%20whether%20ECR%20will%20continue%20to%20progress%20or%20cease%2C%0Aultimately%20informing%20treatment%20decisions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05236v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520external%2520cervical%2520resorption%2520segmentation%2520in%2520cone-beam%2520CT%250A%2520%2520using%2520local%2520texture%2520features%26entry.906535625%3DSadhana%2520Ravikumar%2520and%2520Asma%2520A.%2520Khan%2520and%2520Matthew%2520C.%2520Davis%2520and%2520Beatriz%2520Paniagua%26entry.1292438233%3D%2520%2520External%2520cervical%2520resorption%2520%2528ECR%2529%2520is%2520a%2520resorptive%2520process%2520affecting%2520teeth.%250AWhile%2520in%2520some%2520patients%252C%2520active%2520resorption%2520ceases%2520and%2520gets%2520replaced%2520by%2520osseous%250Atissue%252C%2520in%2520other%2520cases%252C%2520the%2520resorption%2520progresses%2520and%2520ultimately%2520results%2520in%250Atooth%2520loss.%2520For%2520proper%2520ECR%2520assessment%252C%2520cone-beam%2520computed%2520tomography%2520%2528CBCT%2529%2520is%250Athe%2520recommended%2520imaging%2520modality%252C%2520enabling%2520a%25203-D%2520characterization%2520of%2520these%250Alesions.%2520While%2520it%2520is%2520possible%2520to%2520manually%2520identify%2520and%2520measure%2520ECR%2520resorption%250Ain%2520CBCT%2520scans%252C%2520this%2520process%2520can%2520be%2520time%2520intensive%2520and%2520highly%2520subject%2520to%2520human%250Aerror.%2520Therefore%252C%2520there%2520is%2520an%2520urgent%2520need%2520to%2520develop%2520an%2520automated%2520method%2520to%250Aidentify%2520and%2520quantify%2520the%2520severity%2520of%2520ECR%2520resorption%2520using%2520CBCT.%2520Here%252C%2520we%250Apresent%2520a%2520method%2520for%2520ECR%2520lesion%2520segmentation%2520that%2520is%2520based%2520on%2520automatic%252C%2520binary%250Aclassification%2520of%2520locally%2520extracted%2520voxel-wise%2520texture%2520features.%2520We%2520evaluate%250Aour%2520method%2520on%25206%2520longitudinal%2520CBCT%2520datasets%2520and%2520show%2520that%2520certain%250Atexture-features%2520can%2520be%2520used%2520to%2520accurately%2520detect%2520subtle%2520CBCT%2520signal%2520changes%250Adue%2520to%2520ECR.%2520We%2520also%2520present%2520preliminary%2520analyses%2520clustering%2520texture%2520features%250Awithin%2520a%2520lesion%2520to%2520stratify%2520the%2520defects%2520and%2520identify%2520patterns%2520indicative%2520of%250Acalcification.%2520These%2520methods%2520are%2520important%2520steps%2520in%2520developing%2520prognostic%250Abiomarkers%2520to%2520predict%2520whether%2520ECR%2520will%2520continue%2520to%2520progress%2520or%2520cease%252C%250Aultimately%2520informing%2520treatment%2520decisions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05236v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20external%20cervical%20resorption%20segmentation%20in%20cone-beam%20CT%0A%20%20using%20local%20texture%20features&entry.906535625=Sadhana%20Ravikumar%20and%20Asma%20A.%20Khan%20and%20Matthew%20C.%20Davis%20and%20Beatriz%20Paniagua&entry.1292438233=%20%20External%20cervical%20resorption%20%28ECR%29%20is%20a%20resorptive%20process%20affecting%20teeth.%0AWhile%20in%20some%20patients%2C%20active%20resorption%20ceases%20and%20gets%20replaced%20by%20osseous%0Atissue%2C%20in%20other%20cases%2C%20the%20resorption%20progresses%20and%20ultimately%20results%20in%0Atooth%20loss.%20For%20proper%20ECR%20assessment%2C%20cone-beam%20computed%20tomography%20%28CBCT%29%20is%0Athe%20recommended%20imaging%20modality%2C%20enabling%20a%203-D%20characterization%20of%20these%0Alesions.%20While%20it%20is%20possible%20to%20manually%20identify%20and%20measure%20ECR%20resorption%0Ain%20CBCT%20scans%2C%20this%20process%20can%20be%20time%20intensive%20and%20highly%20subject%20to%20human%0Aerror.%20Therefore%2C%20there%20is%20an%20urgent%20need%20to%20develop%20an%20automated%20method%20to%0Aidentify%20and%20quantify%20the%20severity%20of%20ECR%20resorption%20using%20CBCT.%20Here%2C%20we%0Apresent%20a%20method%20for%20ECR%20lesion%20segmentation%20that%20is%20based%20on%20automatic%2C%20binary%0Aclassification%20of%20locally%20extracted%20voxel-wise%20texture%20features.%20We%20evaluate%0Aour%20method%20on%206%20longitudinal%20CBCT%20datasets%20and%20show%20that%20certain%0Atexture-features%20can%20be%20used%20to%20accurately%20detect%20subtle%20CBCT%20signal%20changes%0Adue%20to%20ECR.%20We%20also%20present%20preliminary%20analyses%20clustering%20texture%20features%0Awithin%20a%20lesion%20to%20stratify%20the%20defects%20and%20identify%20patterns%20indicative%20of%0Acalcification.%20These%20methods%20are%20important%20steps%20in%20developing%20prognostic%0Abiomarkers%20to%20predict%20whether%20ECR%20will%20continue%20to%20progress%20or%20cease%2C%0Aultimately%20informing%20treatment%20decisions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05236v1&entry.124074799=Read"},
{"title": "FOCUS: Towards Universal Foreground Segmentation", "author": "Zuyao You and Lingyu Kong and Lingchen Meng and Zuxuan Wu", "abstract": "  Foreground segmentation is a fundamental task in computer vision,\nencompassing various subdivision tasks. Previous research has typically\ndesigned task-specific architectures for each task, leading to a lack of\nunification. Moreover, they primarily focus on recognizing foreground objects\nwithout effectively distinguishing them from the background. In this paper, we\nemphasize the importance of the background and its relationship with the\nforeground. We introduce FOCUS, the Foreground ObjeCts Universal Segmentation\nframework that can handle multiple foreground tasks. We develop a multi-scale\nsemantic network using the edge information of objects to enhance image\nfeatures. To achieve boundary-aware segmentation, we propose a novel\ndistillation method, integrating the contrastive learning strategy to refine\nthe prediction mask in multi-modal feature space. We conduct extensive\nexperiments on a total of 13 datasets across 5 tasks, and the results\ndemonstrate that FOCUS consistently outperforms the state-of-the-art\ntask-specific models on most metrics.\n", "link": "http://arxiv.org/abs/2501.05238v1", "date": "2025-01-09", "relevancy": 2.2059, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5552}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5552}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FOCUS%3A%20Towards%20Universal%20Foreground%20Segmentation&body=Title%3A%20FOCUS%3A%20Towards%20Universal%20Foreground%20Segmentation%0AAuthor%3A%20Zuyao%20You%20and%20Lingyu%20Kong%20and%20Lingchen%20Meng%20and%20Zuxuan%20Wu%0AAbstract%3A%20%20%20Foreground%20segmentation%20is%20a%20fundamental%20task%20in%20computer%20vision%2C%0Aencompassing%20various%20subdivision%20tasks.%20Previous%20research%20has%20typically%0Adesigned%20task-specific%20architectures%20for%20each%20task%2C%20leading%20to%20a%20lack%20of%0Aunification.%20Moreover%2C%20they%20primarily%20focus%20on%20recognizing%20foreground%20objects%0Awithout%20effectively%20distinguishing%20them%20from%20the%20background.%20In%20this%20paper%2C%20we%0Aemphasize%20the%20importance%20of%20the%20background%20and%20its%20relationship%20with%20the%0Aforeground.%20We%20introduce%20FOCUS%2C%20the%20Foreground%20ObjeCts%20Universal%20Segmentation%0Aframework%20that%20can%20handle%20multiple%20foreground%20tasks.%20We%20develop%20a%20multi-scale%0Asemantic%20network%20using%20the%20edge%20information%20of%20objects%20to%20enhance%20image%0Afeatures.%20To%20achieve%20boundary-aware%20segmentation%2C%20we%20propose%20a%20novel%0Adistillation%20method%2C%20integrating%20the%20contrastive%20learning%20strategy%20to%20refine%0Athe%20prediction%20mask%20in%20multi-modal%20feature%20space.%20We%20conduct%20extensive%0Aexperiments%20on%20a%20total%20of%2013%20datasets%20across%205%20tasks%2C%20and%20the%20results%0Ademonstrate%20that%20FOCUS%20consistently%20outperforms%20the%20state-of-the-art%0Atask-specific%20models%20on%20most%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05238v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFOCUS%253A%2520Towards%2520Universal%2520Foreground%2520Segmentation%26entry.906535625%3DZuyao%2520You%2520and%2520Lingyu%2520Kong%2520and%2520Lingchen%2520Meng%2520and%2520Zuxuan%2520Wu%26entry.1292438233%3D%2520%2520Foreground%2520segmentation%2520is%2520a%2520fundamental%2520task%2520in%2520computer%2520vision%252C%250Aencompassing%2520various%2520subdivision%2520tasks.%2520Previous%2520research%2520has%2520typically%250Adesigned%2520task-specific%2520architectures%2520for%2520each%2520task%252C%2520leading%2520to%2520a%2520lack%2520of%250Aunification.%2520Moreover%252C%2520they%2520primarily%2520focus%2520on%2520recognizing%2520foreground%2520objects%250Awithout%2520effectively%2520distinguishing%2520them%2520from%2520the%2520background.%2520In%2520this%2520paper%252C%2520we%250Aemphasize%2520the%2520importance%2520of%2520the%2520background%2520and%2520its%2520relationship%2520with%2520the%250Aforeground.%2520We%2520introduce%2520FOCUS%252C%2520the%2520Foreground%2520ObjeCts%2520Universal%2520Segmentation%250Aframework%2520that%2520can%2520handle%2520multiple%2520foreground%2520tasks.%2520We%2520develop%2520a%2520multi-scale%250Asemantic%2520network%2520using%2520the%2520edge%2520information%2520of%2520objects%2520to%2520enhance%2520image%250Afeatures.%2520To%2520achieve%2520boundary-aware%2520segmentation%252C%2520we%2520propose%2520a%2520novel%250Adistillation%2520method%252C%2520integrating%2520the%2520contrastive%2520learning%2520strategy%2520to%2520refine%250Athe%2520prediction%2520mask%2520in%2520multi-modal%2520feature%2520space.%2520We%2520conduct%2520extensive%250Aexperiments%2520on%2520a%2520total%2520of%252013%2520datasets%2520across%25205%2520tasks%252C%2520and%2520the%2520results%250Ademonstrate%2520that%2520FOCUS%2520consistently%2520outperforms%2520the%2520state-of-the-art%250Atask-specific%2520models%2520on%2520most%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05238v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FOCUS%3A%20Towards%20Universal%20Foreground%20Segmentation&entry.906535625=Zuyao%20You%20and%20Lingyu%20Kong%20and%20Lingchen%20Meng%20and%20Zuxuan%20Wu&entry.1292438233=%20%20Foreground%20segmentation%20is%20a%20fundamental%20task%20in%20computer%20vision%2C%0Aencompassing%20various%20subdivision%20tasks.%20Previous%20research%20has%20typically%0Adesigned%20task-specific%20architectures%20for%20each%20task%2C%20leading%20to%20a%20lack%20of%0Aunification.%20Moreover%2C%20they%20primarily%20focus%20on%20recognizing%20foreground%20objects%0Awithout%20effectively%20distinguishing%20them%20from%20the%20background.%20In%20this%20paper%2C%20we%0Aemphasize%20the%20importance%20of%20the%20background%20and%20its%20relationship%20with%20the%0Aforeground.%20We%20introduce%20FOCUS%2C%20the%20Foreground%20ObjeCts%20Universal%20Segmentation%0Aframework%20that%20can%20handle%20multiple%20foreground%20tasks.%20We%20develop%20a%20multi-scale%0Asemantic%20network%20using%20the%20edge%20information%20of%20objects%20to%20enhance%20image%0Afeatures.%20To%20achieve%20boundary-aware%20segmentation%2C%20we%20propose%20a%20novel%0Adistillation%20method%2C%20integrating%20the%20contrastive%20learning%20strategy%20to%20refine%0Athe%20prediction%20mask%20in%20multi-modal%20feature%20space.%20We%20conduct%20extensive%0Aexperiments%20on%20a%20total%20of%2013%20datasets%20across%205%20tasks%2C%20and%20the%20results%0Ademonstrate%20that%20FOCUS%20consistently%20outperforms%20the%20state-of-the-art%0Atask-specific%20models%20on%20most%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05238v1&entry.124074799=Read"},
{"title": "Domain Adaptation-Enhanced Searchlight: Enabling classification of brain\n  states from visual perception to mental imagery", "author": "Alexander Olza and David Soto and Roberto Santana", "abstract": "  In cognitive neuroscience and brain-computer interface research, accurately\npredicting imagined stimuli is crucial. This study investigates the\neffectiveness of Domain Adaptation (DA) in enhancing imagery prediction using\nprimarily visual data from fMRI scans of 18 subjects. Initially, we train a\nbaseline model on visual stimuli to predict imagined stimuli, utilizing data\nfrom 14 brain regions. We then develop several models to improve imagery\nprediction, comparing different DA methods. Our results demonstrate that DA\nsignificantly enhances imagery prediction in binary classification on our\ndataset, as well as in multiclass classification on a publicly available\ndataset. We then conduct a DA-enhanced searchlight analysis, followed by\npermutation-based statistical tests to identify brain regions where imagery\ndecoding is consistently above chance across subjects. Our DA-enhanced\nsearchlight predicts imagery contents in a highly distributed set of brain\nregions, including the visual cortex and the frontoparietal cortex, thereby\noutperforming standard cross-domain classification methods. The complete code\nand data for this paper have been made openly available for the use of the\nscientific community.\n", "link": "http://arxiv.org/abs/2408.01163v2", "date": "2025-01-09", "relevancy": 2.1794, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5463}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5463}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain%20Adaptation-Enhanced%20Searchlight%3A%20Enabling%20classification%20of%20brain%0A%20%20states%20from%20visual%20perception%20to%20mental%20imagery&body=Title%3A%20Domain%20Adaptation-Enhanced%20Searchlight%3A%20Enabling%20classification%20of%20brain%0A%20%20states%20from%20visual%20perception%20to%20mental%20imagery%0AAuthor%3A%20Alexander%20Olza%20and%20David%20Soto%20and%20Roberto%20Santana%0AAbstract%3A%20%20%20In%20cognitive%20neuroscience%20and%20brain-computer%20interface%20research%2C%20accurately%0Apredicting%20imagined%20stimuli%20is%20crucial.%20This%20study%20investigates%20the%0Aeffectiveness%20of%20Domain%20Adaptation%20%28DA%29%20in%20enhancing%20imagery%20prediction%20using%0Aprimarily%20visual%20data%20from%20fMRI%20scans%20of%2018%20subjects.%20Initially%2C%20we%20train%20a%0Abaseline%20model%20on%20visual%20stimuli%20to%20predict%20imagined%20stimuli%2C%20utilizing%20data%0Afrom%2014%20brain%20regions.%20We%20then%20develop%20several%20models%20to%20improve%20imagery%0Aprediction%2C%20comparing%20different%20DA%20methods.%20Our%20results%20demonstrate%20that%20DA%0Asignificantly%20enhances%20imagery%20prediction%20in%20binary%20classification%20on%20our%0Adataset%2C%20as%20well%20as%20in%20multiclass%20classification%20on%20a%20publicly%20available%0Adataset.%20We%20then%20conduct%20a%20DA-enhanced%20searchlight%20analysis%2C%20followed%20by%0Apermutation-based%20statistical%20tests%20to%20identify%20brain%20regions%20where%20imagery%0Adecoding%20is%20consistently%20above%20chance%20across%20subjects.%20Our%20DA-enhanced%0Asearchlight%20predicts%20imagery%20contents%20in%20a%20highly%20distributed%20set%20of%20brain%0Aregions%2C%20including%20the%20visual%20cortex%20and%20the%20frontoparietal%20cortex%2C%20thereby%0Aoutperforming%20standard%20cross-domain%20classification%20methods.%20The%20complete%20code%0Aand%20data%20for%20this%20paper%20have%20been%20made%20openly%20available%20for%20the%20use%20of%20the%0Ascientific%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01163v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain%2520Adaptation-Enhanced%2520Searchlight%253A%2520Enabling%2520classification%2520of%2520brain%250A%2520%2520states%2520from%2520visual%2520perception%2520to%2520mental%2520imagery%26entry.906535625%3DAlexander%2520Olza%2520and%2520David%2520Soto%2520and%2520Roberto%2520Santana%26entry.1292438233%3D%2520%2520In%2520cognitive%2520neuroscience%2520and%2520brain-computer%2520interface%2520research%252C%2520accurately%250Apredicting%2520imagined%2520stimuli%2520is%2520crucial.%2520This%2520study%2520investigates%2520the%250Aeffectiveness%2520of%2520Domain%2520Adaptation%2520%2528DA%2529%2520in%2520enhancing%2520imagery%2520prediction%2520using%250Aprimarily%2520visual%2520data%2520from%2520fMRI%2520scans%2520of%252018%2520subjects.%2520Initially%252C%2520we%2520train%2520a%250Abaseline%2520model%2520on%2520visual%2520stimuli%2520to%2520predict%2520imagined%2520stimuli%252C%2520utilizing%2520data%250Afrom%252014%2520brain%2520regions.%2520We%2520then%2520develop%2520several%2520models%2520to%2520improve%2520imagery%250Aprediction%252C%2520comparing%2520different%2520DA%2520methods.%2520Our%2520results%2520demonstrate%2520that%2520DA%250Asignificantly%2520enhances%2520imagery%2520prediction%2520in%2520binary%2520classification%2520on%2520our%250Adataset%252C%2520as%2520well%2520as%2520in%2520multiclass%2520classification%2520on%2520a%2520publicly%2520available%250Adataset.%2520We%2520then%2520conduct%2520a%2520DA-enhanced%2520searchlight%2520analysis%252C%2520followed%2520by%250Apermutation-based%2520statistical%2520tests%2520to%2520identify%2520brain%2520regions%2520where%2520imagery%250Adecoding%2520is%2520consistently%2520above%2520chance%2520across%2520subjects.%2520Our%2520DA-enhanced%250Asearchlight%2520predicts%2520imagery%2520contents%2520in%2520a%2520highly%2520distributed%2520set%2520of%2520brain%250Aregions%252C%2520including%2520the%2520visual%2520cortex%2520and%2520the%2520frontoparietal%2520cortex%252C%2520thereby%250Aoutperforming%2520standard%2520cross-domain%2520classification%2520methods.%2520The%2520complete%2520code%250Aand%2520data%2520for%2520this%2520paper%2520have%2520been%2520made%2520openly%2520available%2520for%2520the%2520use%2520of%2520the%250Ascientific%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01163v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain%20Adaptation-Enhanced%20Searchlight%3A%20Enabling%20classification%20of%20brain%0A%20%20states%20from%20visual%20perception%20to%20mental%20imagery&entry.906535625=Alexander%20Olza%20and%20David%20Soto%20and%20Roberto%20Santana&entry.1292438233=%20%20In%20cognitive%20neuroscience%20and%20brain-computer%20interface%20research%2C%20accurately%0Apredicting%20imagined%20stimuli%20is%20crucial.%20This%20study%20investigates%20the%0Aeffectiveness%20of%20Domain%20Adaptation%20%28DA%29%20in%20enhancing%20imagery%20prediction%20using%0Aprimarily%20visual%20data%20from%20fMRI%20scans%20of%2018%20subjects.%20Initially%2C%20we%20train%20a%0Abaseline%20model%20on%20visual%20stimuli%20to%20predict%20imagined%20stimuli%2C%20utilizing%20data%0Afrom%2014%20brain%20regions.%20We%20then%20develop%20several%20models%20to%20improve%20imagery%0Aprediction%2C%20comparing%20different%20DA%20methods.%20Our%20results%20demonstrate%20that%20DA%0Asignificantly%20enhances%20imagery%20prediction%20in%20binary%20classification%20on%20our%0Adataset%2C%20as%20well%20as%20in%20multiclass%20classification%20on%20a%20publicly%20available%0Adataset.%20We%20then%20conduct%20a%20DA-enhanced%20searchlight%20analysis%2C%20followed%20by%0Apermutation-based%20statistical%20tests%20to%20identify%20brain%20regions%20where%20imagery%0Adecoding%20is%20consistently%20above%20chance%20across%20subjects.%20Our%20DA-enhanced%0Asearchlight%20predicts%20imagery%20contents%20in%20a%20highly%20distributed%20set%20of%20brain%0Aregions%2C%20including%20the%20visual%20cortex%20and%20the%20frontoparietal%20cortex%2C%20thereby%0Aoutperforming%20standard%20cross-domain%20classification%20methods.%20The%20complete%20code%0Aand%20data%20for%20this%20paper%20have%20been%20made%20openly%20available%20for%20the%20use%20of%20the%0Ascientific%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01163v2&entry.124074799=Read"},
{"title": "Harnessing Large Language and Vision-Language Models for Robust\n  Out-of-Distribution Detection", "author": "Pei-Kang Lee and Jun-Cheng Chen and Ja-Ling Wu", "abstract": "  Out-of-distribution (OOD) detection has seen significant advancements with\nzero-shot approaches by leveraging the powerful Vision-Language Models (VLMs)\nsuch as CLIP. However, prior research works have predominantly focused on\nenhancing Far-OOD performance, while potentially compromising Near-OOD\nefficacy, as observed from our pilot study. To address this issue, we propose a\nnovel strategy to enhance zero-shot OOD detection performances for both Far-OOD\nand Near-OOD scenarios by innovatively harnessing Large Language Models (LLMs)\nand VLMs. Our approach first exploit an LLM to generate superclasses of the ID\nlabels and their corresponding background descriptions followed by feature\nextraction using CLIP. We then isolate the core semantic features for ID data\nby subtracting background features from the superclass features. The refined\nrepresentation facilitates the selection of more appropriate negative labels\nfor OOD data from a comprehensive candidate label set of WordNet, thereby\nenhancing the performance of zero-shot OOD detection in both scenarios.\nFurthermore, we introduce novel few-shot prompt tuning and visual prompt tuning\nto adapt the proposed framework to better align with the target distribution.\nExperimental results demonstrate that the proposed approach consistently\noutperforms current state-of-the-art methods across multiple benchmarks, with\nan improvement of up to 2.9% in AUROC and a reduction of up to 12.6% in FPR95.\nAdditionally, our method exhibits superior robustness against covariate shift\nacross different domains, further highlighting its effectiveness in real-world\nscenarios.\n", "link": "http://arxiv.org/abs/2501.05228v1", "date": "2025-01-09", "relevancy": 2.1792, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5621}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5443}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harnessing%20Large%20Language%20and%20Vision-Language%20Models%20for%20Robust%0A%20%20Out-of-Distribution%20Detection&body=Title%3A%20Harnessing%20Large%20Language%20and%20Vision-Language%20Models%20for%20Robust%0A%20%20Out-of-Distribution%20Detection%0AAuthor%3A%20Pei-Kang%20Lee%20and%20Jun-Cheng%20Chen%20and%20Ja-Ling%20Wu%0AAbstract%3A%20%20%20Out-of-distribution%20%28OOD%29%20detection%20has%20seen%20significant%20advancements%20with%0Azero-shot%20approaches%20by%20leveraging%20the%20powerful%20Vision-Language%20Models%20%28VLMs%29%0Asuch%20as%20CLIP.%20However%2C%20prior%20research%20works%20have%20predominantly%20focused%20on%0Aenhancing%20Far-OOD%20performance%2C%20while%20potentially%20compromising%20Near-OOD%0Aefficacy%2C%20as%20observed%20from%20our%20pilot%20study.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Anovel%20strategy%20to%20enhance%20zero-shot%20OOD%20detection%20performances%20for%20both%20Far-OOD%0Aand%20Near-OOD%20scenarios%20by%20innovatively%20harnessing%20Large%20Language%20Models%20%28LLMs%29%0Aand%20VLMs.%20Our%20approach%20first%20exploit%20an%20LLM%20to%20generate%20superclasses%20of%20the%20ID%0Alabels%20and%20their%20corresponding%20background%20descriptions%20followed%20by%20feature%0Aextraction%20using%20CLIP.%20We%20then%20isolate%20the%20core%20semantic%20features%20for%20ID%20data%0Aby%20subtracting%20background%20features%20from%20the%20superclass%20features.%20The%20refined%0Arepresentation%20facilitates%20the%20selection%20of%20more%20appropriate%20negative%20labels%0Afor%20OOD%20data%20from%20a%20comprehensive%20candidate%20label%20set%20of%20WordNet%2C%20thereby%0Aenhancing%20the%20performance%20of%20zero-shot%20OOD%20detection%20in%20both%20scenarios.%0AFurthermore%2C%20we%20introduce%20novel%20few-shot%20prompt%20tuning%20and%20visual%20prompt%20tuning%0Ato%20adapt%20the%20proposed%20framework%20to%20better%20align%20with%20the%20target%20distribution.%0AExperimental%20results%20demonstrate%20that%20the%20proposed%20approach%20consistently%0Aoutperforms%20current%20state-of-the-art%20methods%20across%20multiple%20benchmarks%2C%20with%0Aan%20improvement%20of%20up%20to%202.9%25%20in%20AUROC%20and%20a%20reduction%20of%20up%20to%2012.6%25%20in%20FPR95.%0AAdditionally%2C%20our%20method%20exhibits%20superior%20robustness%20against%20covariate%20shift%0Aacross%20different%20domains%2C%20further%20highlighting%20its%20effectiveness%20in%20real-world%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05228v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarnessing%2520Large%2520Language%2520and%2520Vision-Language%2520Models%2520for%2520Robust%250A%2520%2520Out-of-Distribution%2520Detection%26entry.906535625%3DPei-Kang%2520Lee%2520and%2520Jun-Cheng%2520Chen%2520and%2520Ja-Ling%2520Wu%26entry.1292438233%3D%2520%2520Out-of-distribution%2520%2528OOD%2529%2520detection%2520has%2520seen%2520significant%2520advancements%2520with%250Azero-shot%2520approaches%2520by%2520leveraging%2520the%2520powerful%2520Vision-Language%2520Models%2520%2528VLMs%2529%250Asuch%2520as%2520CLIP.%2520However%252C%2520prior%2520research%2520works%2520have%2520predominantly%2520focused%2520on%250Aenhancing%2520Far-OOD%2520performance%252C%2520while%2520potentially%2520compromising%2520Near-OOD%250Aefficacy%252C%2520as%2520observed%2520from%2520our%2520pilot%2520study.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%250Anovel%2520strategy%2520to%2520enhance%2520zero-shot%2520OOD%2520detection%2520performances%2520for%2520both%2520Far-OOD%250Aand%2520Near-OOD%2520scenarios%2520by%2520innovatively%2520harnessing%2520Large%2520Language%2520Models%2520%2528LLMs%2529%250Aand%2520VLMs.%2520Our%2520approach%2520first%2520exploit%2520an%2520LLM%2520to%2520generate%2520superclasses%2520of%2520the%2520ID%250Alabels%2520and%2520their%2520corresponding%2520background%2520descriptions%2520followed%2520by%2520feature%250Aextraction%2520using%2520CLIP.%2520We%2520then%2520isolate%2520the%2520core%2520semantic%2520features%2520for%2520ID%2520data%250Aby%2520subtracting%2520background%2520features%2520from%2520the%2520superclass%2520features.%2520The%2520refined%250Arepresentation%2520facilitates%2520the%2520selection%2520of%2520more%2520appropriate%2520negative%2520labels%250Afor%2520OOD%2520data%2520from%2520a%2520comprehensive%2520candidate%2520label%2520set%2520of%2520WordNet%252C%2520thereby%250Aenhancing%2520the%2520performance%2520of%2520zero-shot%2520OOD%2520detection%2520in%2520both%2520scenarios.%250AFurthermore%252C%2520we%2520introduce%2520novel%2520few-shot%2520prompt%2520tuning%2520and%2520visual%2520prompt%2520tuning%250Ato%2520adapt%2520the%2520proposed%2520framework%2520to%2520better%2520align%2520with%2520the%2520target%2520distribution.%250AExperimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520approach%2520consistently%250Aoutperforms%2520current%2520state-of-the-art%2520methods%2520across%2520multiple%2520benchmarks%252C%2520with%250Aan%2520improvement%2520of%2520up%2520to%25202.9%2525%2520in%2520AUROC%2520and%2520a%2520reduction%2520of%2520up%2520to%252012.6%2525%2520in%2520FPR95.%250AAdditionally%252C%2520our%2520method%2520exhibits%2520superior%2520robustness%2520against%2520covariate%2520shift%250Aacross%2520different%2520domains%252C%2520further%2520highlighting%2520its%2520effectiveness%2520in%2520real-world%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05228v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20Large%20Language%20and%20Vision-Language%20Models%20for%20Robust%0A%20%20Out-of-Distribution%20Detection&entry.906535625=Pei-Kang%20Lee%20and%20Jun-Cheng%20Chen%20and%20Ja-Ling%20Wu&entry.1292438233=%20%20Out-of-distribution%20%28OOD%29%20detection%20has%20seen%20significant%20advancements%20with%0Azero-shot%20approaches%20by%20leveraging%20the%20powerful%20Vision-Language%20Models%20%28VLMs%29%0Asuch%20as%20CLIP.%20However%2C%20prior%20research%20works%20have%20predominantly%20focused%20on%0Aenhancing%20Far-OOD%20performance%2C%20while%20potentially%20compromising%20Near-OOD%0Aefficacy%2C%20as%20observed%20from%20our%20pilot%20study.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Anovel%20strategy%20to%20enhance%20zero-shot%20OOD%20detection%20performances%20for%20both%20Far-OOD%0Aand%20Near-OOD%20scenarios%20by%20innovatively%20harnessing%20Large%20Language%20Models%20%28LLMs%29%0Aand%20VLMs.%20Our%20approach%20first%20exploit%20an%20LLM%20to%20generate%20superclasses%20of%20the%20ID%0Alabels%20and%20their%20corresponding%20background%20descriptions%20followed%20by%20feature%0Aextraction%20using%20CLIP.%20We%20then%20isolate%20the%20core%20semantic%20features%20for%20ID%20data%0Aby%20subtracting%20background%20features%20from%20the%20superclass%20features.%20The%20refined%0Arepresentation%20facilitates%20the%20selection%20of%20more%20appropriate%20negative%20labels%0Afor%20OOD%20data%20from%20a%20comprehensive%20candidate%20label%20set%20of%20WordNet%2C%20thereby%0Aenhancing%20the%20performance%20of%20zero-shot%20OOD%20detection%20in%20both%20scenarios.%0AFurthermore%2C%20we%20introduce%20novel%20few-shot%20prompt%20tuning%20and%20visual%20prompt%20tuning%0Ato%20adapt%20the%20proposed%20framework%20to%20better%20align%20with%20the%20target%20distribution.%0AExperimental%20results%20demonstrate%20that%20the%20proposed%20approach%20consistently%0Aoutperforms%20current%20state-of-the-art%20methods%20across%20multiple%20benchmarks%2C%20with%0Aan%20improvement%20of%20up%20to%202.9%25%20in%20AUROC%20and%20a%20reduction%20of%20up%20to%2012.6%25%20in%20FPR95.%0AAdditionally%2C%20our%20method%20exhibits%20superior%20robustness%20against%20covariate%20shift%0Aacross%20different%20domains%2C%20further%20highlighting%20its%20effectiveness%20in%20real-world%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05228v1&entry.124074799=Read"},
{"title": "Airborne Sense and Detect of Drones using Deep Learning and LiDAR Point\n  Clouds", "author": "Manduhu Manduhu and Alexander Dow and Petar Trslic and Gerard Dooly and Benjamin Blanck and James Riordan", "abstract": "  The safe operation of drone swarms beyond visual line of sight requires\nmultiple safeguards to mitigate the risk of collision between drones flying in\nclose-proximity scenarios. Cooperative navigation and flight coordination\nstrategies that rely on pre-planned trajectories, constant %{satellite and\nnetwork connectivity and reliable Global Navigation Satellite System (GNSS)\npositioning are brittle to failure. Drone embedded sense and detect offers a\ncomprehensive mode of separation between drones for deconfliction and collision\navoidance. This paper presents the first airborne LiDAR based solution for\ndrone-swarm detection and localization using 3D deep learning model. It adapts\nan existing deep learning neural network to the air-to-air drone scenario by\nexpanding the scan space vertically. A new sparse convolution is proposed and\napplied to accelerate the backbone layer, which is the most time-consuming part\nof the neural network. To collect training data of safety critical,\nclose-proximity multi-drone operations, a scenario Digital Twin is used to\naugment real datasets with high fidelity synthetic data. The trained model\nachieves over 80% recall and 96% precision when tested on real-world datasets.\nBy incorporating a tracking-by-detection algorithm the system can reliably\nmonitor the separation distance of multiple drones in challenging environments.\n", "link": "http://arxiv.org/abs/2310.09589v2", "date": "2025-01-09", "relevancy": 2.1762, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5679}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5354}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Airborne%20Sense%20and%20Detect%20of%20Drones%20using%20Deep%20Learning%20and%20LiDAR%20Point%0A%20%20Clouds&body=Title%3A%20Airborne%20Sense%20and%20Detect%20of%20Drones%20using%20Deep%20Learning%20and%20LiDAR%20Point%0A%20%20Clouds%0AAuthor%3A%20Manduhu%20Manduhu%20and%20Alexander%20Dow%20and%20Petar%20Trslic%20and%20Gerard%20Dooly%20and%20Benjamin%20Blanck%20and%20James%20Riordan%0AAbstract%3A%20%20%20The%20safe%20operation%20of%20drone%20swarms%20beyond%20visual%20line%20of%20sight%20requires%0Amultiple%20safeguards%20to%20mitigate%20the%20risk%20of%20collision%20between%20drones%20flying%20in%0Aclose-proximity%20scenarios.%20Cooperative%20navigation%20and%20flight%20coordination%0Astrategies%20that%20rely%20on%20pre-planned%20trajectories%2C%20constant%20%25%7Bsatellite%20and%0Anetwork%20connectivity%20and%20reliable%20Global%20Navigation%20Satellite%20System%20%28GNSS%29%0Apositioning%20are%20brittle%20to%20failure.%20Drone%20embedded%20sense%20and%20detect%20offers%20a%0Acomprehensive%20mode%20of%20separation%20between%20drones%20for%20deconfliction%20and%20collision%0Aavoidance.%20This%20paper%20presents%20the%20first%20airborne%20LiDAR%20based%20solution%20for%0Adrone-swarm%20detection%20and%20localization%20using%203D%20deep%20learning%20model.%20It%20adapts%0Aan%20existing%20deep%20learning%20neural%20network%20to%20the%20air-to-air%20drone%20scenario%20by%0Aexpanding%20the%20scan%20space%20vertically.%20A%20new%20sparse%20convolution%20is%20proposed%20and%0Aapplied%20to%20accelerate%20the%20backbone%20layer%2C%20which%20is%20the%20most%20time-consuming%20part%0Aof%20the%20neural%20network.%20To%20collect%20training%20data%20of%20safety%20critical%2C%0Aclose-proximity%20multi-drone%20operations%2C%20a%20scenario%20Digital%20Twin%20is%20used%20to%0Aaugment%20real%20datasets%20with%20high%20fidelity%20synthetic%20data.%20The%20trained%20model%0Aachieves%20over%2080%25%20recall%20and%2096%25%20precision%20when%20tested%20on%20real-world%20datasets.%0ABy%20incorporating%20a%20tracking-by-detection%20algorithm%20the%20system%20can%20reliably%0Amonitor%20the%20separation%20distance%20of%20multiple%20drones%20in%20challenging%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.09589v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAirborne%2520Sense%2520and%2520Detect%2520of%2520Drones%2520using%2520Deep%2520Learning%2520and%2520LiDAR%2520Point%250A%2520%2520Clouds%26entry.906535625%3DManduhu%2520Manduhu%2520and%2520Alexander%2520Dow%2520and%2520Petar%2520Trslic%2520and%2520Gerard%2520Dooly%2520and%2520Benjamin%2520Blanck%2520and%2520James%2520Riordan%26entry.1292438233%3D%2520%2520The%2520safe%2520operation%2520of%2520drone%2520swarms%2520beyond%2520visual%2520line%2520of%2520sight%2520requires%250Amultiple%2520safeguards%2520to%2520mitigate%2520the%2520risk%2520of%2520collision%2520between%2520drones%2520flying%2520in%250Aclose-proximity%2520scenarios.%2520Cooperative%2520navigation%2520and%2520flight%2520coordination%250Astrategies%2520that%2520rely%2520on%2520pre-planned%2520trajectories%252C%2520constant%2520%2525%257Bsatellite%2520and%250Anetwork%2520connectivity%2520and%2520reliable%2520Global%2520Navigation%2520Satellite%2520System%2520%2528GNSS%2529%250Apositioning%2520are%2520brittle%2520to%2520failure.%2520Drone%2520embedded%2520sense%2520and%2520detect%2520offers%2520a%250Acomprehensive%2520mode%2520of%2520separation%2520between%2520drones%2520for%2520deconfliction%2520and%2520collision%250Aavoidance.%2520This%2520paper%2520presents%2520the%2520first%2520airborne%2520LiDAR%2520based%2520solution%2520for%250Adrone-swarm%2520detection%2520and%2520localization%2520using%25203D%2520deep%2520learning%2520model.%2520It%2520adapts%250Aan%2520existing%2520deep%2520learning%2520neural%2520network%2520to%2520the%2520air-to-air%2520drone%2520scenario%2520by%250Aexpanding%2520the%2520scan%2520space%2520vertically.%2520A%2520new%2520sparse%2520convolution%2520is%2520proposed%2520and%250Aapplied%2520to%2520accelerate%2520the%2520backbone%2520layer%252C%2520which%2520is%2520the%2520most%2520time-consuming%2520part%250Aof%2520the%2520neural%2520network.%2520To%2520collect%2520training%2520data%2520of%2520safety%2520critical%252C%250Aclose-proximity%2520multi-drone%2520operations%252C%2520a%2520scenario%2520Digital%2520Twin%2520is%2520used%2520to%250Aaugment%2520real%2520datasets%2520with%2520high%2520fidelity%2520synthetic%2520data.%2520The%2520trained%2520model%250Aachieves%2520over%252080%2525%2520recall%2520and%252096%2525%2520precision%2520when%2520tested%2520on%2520real-world%2520datasets.%250ABy%2520incorporating%2520a%2520tracking-by-detection%2520algorithm%2520the%2520system%2520can%2520reliably%250Amonitor%2520the%2520separation%2520distance%2520of%2520multiple%2520drones%2520in%2520challenging%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.09589v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Airborne%20Sense%20and%20Detect%20of%20Drones%20using%20Deep%20Learning%20and%20LiDAR%20Point%0A%20%20Clouds&entry.906535625=Manduhu%20Manduhu%20and%20Alexander%20Dow%20and%20Petar%20Trslic%20and%20Gerard%20Dooly%20and%20Benjamin%20Blanck%20and%20James%20Riordan&entry.1292438233=%20%20The%20safe%20operation%20of%20drone%20swarms%20beyond%20visual%20line%20of%20sight%20requires%0Amultiple%20safeguards%20to%20mitigate%20the%20risk%20of%20collision%20between%20drones%20flying%20in%0Aclose-proximity%20scenarios.%20Cooperative%20navigation%20and%20flight%20coordination%0Astrategies%20that%20rely%20on%20pre-planned%20trajectories%2C%20constant%20%25%7Bsatellite%20and%0Anetwork%20connectivity%20and%20reliable%20Global%20Navigation%20Satellite%20System%20%28GNSS%29%0Apositioning%20are%20brittle%20to%20failure.%20Drone%20embedded%20sense%20and%20detect%20offers%20a%0Acomprehensive%20mode%20of%20separation%20between%20drones%20for%20deconfliction%20and%20collision%0Aavoidance.%20This%20paper%20presents%20the%20first%20airborne%20LiDAR%20based%20solution%20for%0Adrone-swarm%20detection%20and%20localization%20using%203D%20deep%20learning%20model.%20It%20adapts%0Aan%20existing%20deep%20learning%20neural%20network%20to%20the%20air-to-air%20drone%20scenario%20by%0Aexpanding%20the%20scan%20space%20vertically.%20A%20new%20sparse%20convolution%20is%20proposed%20and%0Aapplied%20to%20accelerate%20the%20backbone%20layer%2C%20which%20is%20the%20most%20time-consuming%20part%0Aof%20the%20neural%20network.%20To%20collect%20training%20data%20of%20safety%20critical%2C%0Aclose-proximity%20multi-drone%20operations%2C%20a%20scenario%20Digital%20Twin%20is%20used%20to%0Aaugment%20real%20datasets%20with%20high%20fidelity%20synthetic%20data.%20The%20trained%20model%0Aachieves%20over%2080%25%20recall%20and%2096%25%20precision%20when%20tested%20on%20real-world%20datasets.%0ABy%20incorporating%20a%20tracking-by-detection%20algorithm%20the%20system%20can%20reliably%0Amonitor%20the%20separation%20distance%20of%20multiple%20drones%20in%20challenging%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.09589v2&entry.124074799=Read"},
{"title": "Knowledge Transfer in Model-Based Reinforcement Learning Agents for\n  Efficient Multi-Task Learning", "author": "Dmytro Kuzmenko and Nadiya Shvai", "abstract": "  We propose an efficient knowledge transfer approach for model-based\nreinforcement learning, addressing the challenge of deploying large world\nmodels in resource-constrained environments. Our method distills a\nhigh-capacity multi-task agent (317M parameters) into a compact 1M parameter\nmodel, achieving state-of-the-art performance on the MT30 benchmark with a\nnormalized score of 28.45, a substantial improvement over the original 1M\nparameter model's score of 18.93. This demonstrates the ability of our\ndistillation technique to consolidate complex multi-task knowledge effectively.\nAdditionally, we apply FP16 post-training quantization, reducing the model size\nby 50% while maintaining performance. Our work bridges the gap between the\npower of large models and practical deployment constraints, offering a scalable\nsolution for efficient and accessible multi-task reinforcement learning in\nrobotics and other resource-limited domains.\n", "link": "http://arxiv.org/abs/2501.05329v1", "date": "2025-01-09", "relevancy": 2.176, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5683}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5406}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Transfer%20in%20Model-Based%20Reinforcement%20Learning%20Agents%20for%0A%20%20Efficient%20Multi-Task%20Learning&body=Title%3A%20Knowledge%20Transfer%20in%20Model-Based%20Reinforcement%20Learning%20Agents%20for%0A%20%20Efficient%20Multi-Task%20Learning%0AAuthor%3A%20Dmytro%20Kuzmenko%20and%20Nadiya%20Shvai%0AAbstract%3A%20%20%20We%20propose%20an%20efficient%20knowledge%20transfer%20approach%20for%20model-based%0Areinforcement%20learning%2C%20addressing%20the%20challenge%20of%20deploying%20large%20world%0Amodels%20in%20resource-constrained%20environments.%20Our%20method%20distills%20a%0Ahigh-capacity%20multi-task%20agent%20%28317M%20parameters%29%20into%20a%20compact%201M%20parameter%0Amodel%2C%20achieving%20state-of-the-art%20performance%20on%20the%20MT30%20benchmark%20with%20a%0Anormalized%20score%20of%2028.45%2C%20a%20substantial%20improvement%20over%20the%20original%201M%0Aparameter%20model%27s%20score%20of%2018.93.%20This%20demonstrates%20the%20ability%20of%20our%0Adistillation%20technique%20to%20consolidate%20complex%20multi-task%20knowledge%20effectively.%0AAdditionally%2C%20we%20apply%20FP16%20post-training%20quantization%2C%20reducing%20the%20model%20size%0Aby%2050%25%20while%20maintaining%20performance.%20Our%20work%20bridges%20the%20gap%20between%20the%0Apower%20of%20large%20models%20and%20practical%20deployment%20constraints%2C%20offering%20a%20scalable%0Asolution%20for%20efficient%20and%20accessible%20multi-task%20reinforcement%20learning%20in%0Arobotics%20and%20other%20resource-limited%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05329v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Transfer%2520in%2520Model-Based%2520Reinforcement%2520Learning%2520Agents%2520for%250A%2520%2520Efficient%2520Multi-Task%2520Learning%26entry.906535625%3DDmytro%2520Kuzmenko%2520and%2520Nadiya%2520Shvai%26entry.1292438233%3D%2520%2520We%2520propose%2520an%2520efficient%2520knowledge%2520transfer%2520approach%2520for%2520model-based%250Areinforcement%2520learning%252C%2520addressing%2520the%2520challenge%2520of%2520deploying%2520large%2520world%250Amodels%2520in%2520resource-constrained%2520environments.%2520Our%2520method%2520distills%2520a%250Ahigh-capacity%2520multi-task%2520agent%2520%2528317M%2520parameters%2529%2520into%2520a%2520compact%25201M%2520parameter%250Amodel%252C%2520achieving%2520state-of-the-art%2520performance%2520on%2520the%2520MT30%2520benchmark%2520with%2520a%250Anormalized%2520score%2520of%252028.45%252C%2520a%2520substantial%2520improvement%2520over%2520the%2520original%25201M%250Aparameter%2520model%2527s%2520score%2520of%252018.93.%2520This%2520demonstrates%2520the%2520ability%2520of%2520our%250Adistillation%2520technique%2520to%2520consolidate%2520complex%2520multi-task%2520knowledge%2520effectively.%250AAdditionally%252C%2520we%2520apply%2520FP16%2520post-training%2520quantization%252C%2520reducing%2520the%2520model%2520size%250Aby%252050%2525%2520while%2520maintaining%2520performance.%2520Our%2520work%2520bridges%2520the%2520gap%2520between%2520the%250Apower%2520of%2520large%2520models%2520and%2520practical%2520deployment%2520constraints%252C%2520offering%2520a%2520scalable%250Asolution%2520for%2520efficient%2520and%2520accessible%2520multi-task%2520reinforcement%2520learning%2520in%250Arobotics%2520and%2520other%2520resource-limited%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05329v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Transfer%20in%20Model-Based%20Reinforcement%20Learning%20Agents%20for%0A%20%20Efficient%20Multi-Task%20Learning&entry.906535625=Dmytro%20Kuzmenko%20and%20Nadiya%20Shvai&entry.1292438233=%20%20We%20propose%20an%20efficient%20knowledge%20transfer%20approach%20for%20model-based%0Areinforcement%20learning%2C%20addressing%20the%20challenge%20of%20deploying%20large%20world%0Amodels%20in%20resource-constrained%20environments.%20Our%20method%20distills%20a%0Ahigh-capacity%20multi-task%20agent%20%28317M%20parameters%29%20into%20a%20compact%201M%20parameter%0Amodel%2C%20achieving%20state-of-the-art%20performance%20on%20the%20MT30%20benchmark%20with%20a%0Anormalized%20score%20of%2028.45%2C%20a%20substantial%20improvement%20over%20the%20original%201M%0Aparameter%20model%27s%20score%20of%2018.93.%20This%20demonstrates%20the%20ability%20of%20our%0Adistillation%20technique%20to%20consolidate%20complex%20multi-task%20knowledge%20effectively.%0AAdditionally%2C%20we%20apply%20FP16%20post-training%20quantization%2C%20reducing%20the%20model%20size%0Aby%2050%25%20while%20maintaining%20performance.%20Our%20work%20bridges%20the%20gap%20between%20the%0Apower%20of%20large%20models%20and%20practical%20deployment%20constraints%2C%20offering%20a%20scalable%0Asolution%20for%20efficient%20and%20accessible%20multi-task%20reinforcement%20learning%20in%0Arobotics%20and%20other%20resource-limited%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05329v1&entry.124074799=Read"},
{"title": "Adaptive Path-Planning for Autonomous Robots: A UCH-Enhanced Q-Learning\n  Approach", "author": "Wei Liu and Ruiyang Wang and Haonan Wang and Guangwei Liu", "abstract": "  Q-learning methods are widely used in robot path planning but often face\nchallenges of inefficient search and slow convergence. We propose an Improved\nQ-learning (IQL) framework that enhances standard Q-learning in two significant\nways. First, we introduce the Path Adaptive Collaborative Optimization (PACO)\nalgorithm to optimize Q-table initialization, providing better initial\nestimates and accelerating learning. Second, we incorporate a\nUtility-Controlled Heuristic (UCH) mechanism with dynamically tuned parameters\nto optimize the reward function, enhancing the algorithm's accuracy and\neffectiveness in path-planning tasks. Extensive experiments in three different\nraster grid environments validate the superior performance of our IQL\nframework. The results demonstrate that our IQL algorithm outperforms existing\nmethods, including FIQL, PP-QL-based CPP, DFQL, and QMABC algorithms, in terms\nof path-planning capabilities.\n", "link": "http://arxiv.org/abs/2501.05411v1", "date": "2025-01-09", "relevancy": 2.1528, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5548}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5282}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5256}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Path-Planning%20for%20Autonomous%20Robots%3A%20A%20UCH-Enhanced%20Q-Learning%0A%20%20Approach&body=Title%3A%20Adaptive%20Path-Planning%20for%20Autonomous%20Robots%3A%20A%20UCH-Enhanced%20Q-Learning%0A%20%20Approach%0AAuthor%3A%20Wei%20Liu%20and%20Ruiyang%20Wang%20and%20Haonan%20Wang%20and%20Guangwei%20Liu%0AAbstract%3A%20%20%20Q-learning%20methods%20are%20widely%20used%20in%20robot%20path%20planning%20but%20often%20face%0Achallenges%20of%20inefficient%20search%20and%20slow%20convergence.%20We%20propose%20an%20Improved%0AQ-learning%20%28IQL%29%20framework%20that%20enhances%20standard%20Q-learning%20in%20two%20significant%0Aways.%20First%2C%20we%20introduce%20the%20Path%20Adaptive%20Collaborative%20Optimization%20%28PACO%29%0Aalgorithm%20to%20optimize%20Q-table%20initialization%2C%20providing%20better%20initial%0Aestimates%20and%20accelerating%20learning.%20Second%2C%20we%20incorporate%20a%0AUtility-Controlled%20Heuristic%20%28UCH%29%20mechanism%20with%20dynamically%20tuned%20parameters%0Ato%20optimize%20the%20reward%20function%2C%20enhancing%20the%20algorithm%27s%20accuracy%20and%0Aeffectiveness%20in%20path-planning%20tasks.%20Extensive%20experiments%20in%20three%20different%0Araster%20grid%20environments%20validate%20the%20superior%20performance%20of%20our%20IQL%0Aframework.%20The%20results%20demonstrate%20that%20our%20IQL%20algorithm%20outperforms%20existing%0Amethods%2C%20including%20FIQL%2C%20PP-QL-based%20CPP%2C%20DFQL%2C%20and%20QMABC%20algorithms%2C%20in%20terms%0Aof%20path-planning%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05411v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Path-Planning%2520for%2520Autonomous%2520Robots%253A%2520A%2520UCH-Enhanced%2520Q-Learning%250A%2520%2520Approach%26entry.906535625%3DWei%2520Liu%2520and%2520Ruiyang%2520Wang%2520and%2520Haonan%2520Wang%2520and%2520Guangwei%2520Liu%26entry.1292438233%3D%2520%2520Q-learning%2520methods%2520are%2520widely%2520used%2520in%2520robot%2520path%2520planning%2520but%2520often%2520face%250Achallenges%2520of%2520inefficient%2520search%2520and%2520slow%2520convergence.%2520We%2520propose%2520an%2520Improved%250AQ-learning%2520%2528IQL%2529%2520framework%2520that%2520enhances%2520standard%2520Q-learning%2520in%2520two%2520significant%250Aways.%2520First%252C%2520we%2520introduce%2520the%2520Path%2520Adaptive%2520Collaborative%2520Optimization%2520%2528PACO%2529%250Aalgorithm%2520to%2520optimize%2520Q-table%2520initialization%252C%2520providing%2520better%2520initial%250Aestimates%2520and%2520accelerating%2520learning.%2520Second%252C%2520we%2520incorporate%2520a%250AUtility-Controlled%2520Heuristic%2520%2528UCH%2529%2520mechanism%2520with%2520dynamically%2520tuned%2520parameters%250Ato%2520optimize%2520the%2520reward%2520function%252C%2520enhancing%2520the%2520algorithm%2527s%2520accuracy%2520and%250Aeffectiveness%2520in%2520path-planning%2520tasks.%2520Extensive%2520experiments%2520in%2520three%2520different%250Araster%2520grid%2520environments%2520validate%2520the%2520superior%2520performance%2520of%2520our%2520IQL%250Aframework.%2520The%2520results%2520demonstrate%2520that%2520our%2520IQL%2520algorithm%2520outperforms%2520existing%250Amethods%252C%2520including%2520FIQL%252C%2520PP-QL-based%2520CPP%252C%2520DFQL%252C%2520and%2520QMABC%2520algorithms%252C%2520in%2520terms%250Aof%2520path-planning%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05411v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Path-Planning%20for%20Autonomous%20Robots%3A%20A%20UCH-Enhanced%20Q-Learning%0A%20%20Approach&entry.906535625=Wei%20Liu%20and%20Ruiyang%20Wang%20and%20Haonan%20Wang%20and%20Guangwei%20Liu&entry.1292438233=%20%20Q-learning%20methods%20are%20widely%20used%20in%20robot%20path%20planning%20but%20often%20face%0Achallenges%20of%20inefficient%20search%20and%20slow%20convergence.%20We%20propose%20an%20Improved%0AQ-learning%20%28IQL%29%20framework%20that%20enhances%20standard%20Q-learning%20in%20two%20significant%0Aways.%20First%2C%20we%20introduce%20the%20Path%20Adaptive%20Collaborative%20Optimization%20%28PACO%29%0Aalgorithm%20to%20optimize%20Q-table%20initialization%2C%20providing%20better%20initial%0Aestimates%20and%20accelerating%20learning.%20Second%2C%20we%20incorporate%20a%0AUtility-Controlled%20Heuristic%20%28UCH%29%20mechanism%20with%20dynamically%20tuned%20parameters%0Ato%20optimize%20the%20reward%20function%2C%20enhancing%20the%20algorithm%27s%20accuracy%20and%0Aeffectiveness%20in%20path-planning%20tasks.%20Extensive%20experiments%20in%20three%20different%0Araster%20grid%20environments%20validate%20the%20superior%20performance%20of%20our%20IQL%0Aframework.%20The%20results%20demonstrate%20that%20our%20IQL%20algorithm%20outperforms%20existing%0Amethods%2C%20including%20FIQL%2C%20PP-QL-based%20CPP%2C%20DFQL%2C%20and%20QMABC%20algorithms%2C%20in%20terms%0Aof%20path-planning%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05411v1&entry.124074799=Read"},
{"title": "Constrained Optimization of Charged Particle Tracking with Multi-Agent\n  Reinforcement Learning", "author": "Tobias Kortus and Ralf Keidel and Nicolas R. Gauger and Jan Kieseler", "abstract": "  Reinforcement learning demonstrated immense success in modelling complex\nphysics-driven systems, providing end-to-end trainable solutions by interacting\nwith a simulated or real environment, maximizing a scalar reward signal. In\nthis work, we propose, building upon previous work, a multi-agent reinforcement\nlearning approach with assignment constraints for reconstructing particle\ntracks in pixelated particle detectors. Our approach optimizes collaboratively\na parametrized policy, functioning as a heuristic to a multidimensional\nassignment problem, by jointly minimizing the total amount of particle\nscattering over the reconstructed tracks in a readout frame. To satisfy\nconstraints, guaranteeing a unique assignment of particle hits, we propose a\nsafety layer solving a linear assignment problem for every joint action.\nFurther, to enforce cost margins, increasing the distance of the local policies\npredictions to the decision boundaries of the optimizer mappings, we recommend\nthe use of an additional component in the blackbox gradient estimation, forcing\nthe policy to solutions with lower total assignment costs. We empirically show\non simulated data, generated for a particle detector developed for proton\nimaging, the effectiveness of our approach, compared to multiple single- and\nmulti-agent baselines. We further demonstrate the effectiveness of constraints\nwith cost margins for both optimization and generalization, introduced by wider\nregions with high reconstruction performance as well as reduced predictive\ninstabilities. Our results form the basis for further developments in RL-based\ntracking, offering both enhanced performance with constrained policies and\ngreater flexibility in optimizing tracking algorithms through the option for\nindividual and team rewards.\n", "link": "http://arxiv.org/abs/2501.05113v1", "date": "2025-01-09", "relevancy": 2.1389, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5652}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5603}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Constrained%20Optimization%20of%20Charged%20Particle%20Tracking%20with%20Multi-Agent%0A%20%20Reinforcement%20Learning&body=Title%3A%20Constrained%20Optimization%20of%20Charged%20Particle%20Tracking%20with%20Multi-Agent%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Tobias%20Kortus%20and%20Ralf%20Keidel%20and%20Nicolas%20R.%20Gauger%20and%20Jan%20Kieseler%0AAbstract%3A%20%20%20Reinforcement%20learning%20demonstrated%20immense%20success%20in%20modelling%20complex%0Aphysics-driven%20systems%2C%20providing%20end-to-end%20trainable%20solutions%20by%20interacting%0Awith%20a%20simulated%20or%20real%20environment%2C%20maximizing%20a%20scalar%20reward%20signal.%20In%0Athis%20work%2C%20we%20propose%2C%20building%20upon%20previous%20work%2C%20a%20multi-agent%20reinforcement%0Alearning%20approach%20with%20assignment%20constraints%20for%20reconstructing%20particle%0Atracks%20in%20pixelated%20particle%20detectors.%20Our%20approach%20optimizes%20collaboratively%0Aa%20parametrized%20policy%2C%20functioning%20as%20a%20heuristic%20to%20a%20multidimensional%0Aassignment%20problem%2C%20by%20jointly%20minimizing%20the%20total%20amount%20of%20particle%0Ascattering%20over%20the%20reconstructed%20tracks%20in%20a%20readout%20frame.%20To%20satisfy%0Aconstraints%2C%20guaranteeing%20a%20unique%20assignment%20of%20particle%20hits%2C%20we%20propose%20a%0Asafety%20layer%20solving%20a%20linear%20assignment%20problem%20for%20every%20joint%20action.%0AFurther%2C%20to%20enforce%20cost%20margins%2C%20increasing%20the%20distance%20of%20the%20local%20policies%0Apredictions%20to%20the%20decision%20boundaries%20of%20the%20optimizer%20mappings%2C%20we%20recommend%0Athe%20use%20of%20an%20additional%20component%20in%20the%20blackbox%20gradient%20estimation%2C%20forcing%0Athe%20policy%20to%20solutions%20with%20lower%20total%20assignment%20costs.%20We%20empirically%20show%0Aon%20simulated%20data%2C%20generated%20for%20a%20particle%20detector%20developed%20for%20proton%0Aimaging%2C%20the%20effectiveness%20of%20our%20approach%2C%20compared%20to%20multiple%20single-%20and%0Amulti-agent%20baselines.%20We%20further%20demonstrate%20the%20effectiveness%20of%20constraints%0Awith%20cost%20margins%20for%20both%20optimization%20and%20generalization%2C%20introduced%20by%20wider%0Aregions%20with%20high%20reconstruction%20performance%20as%20well%20as%20reduced%20predictive%0Ainstabilities.%20Our%20results%20form%20the%20basis%20for%20further%20developments%20in%20RL-based%0Atracking%2C%20offering%20both%20enhanced%20performance%20with%20constrained%20policies%20and%0Agreater%20flexibility%20in%20optimizing%20tracking%20algorithms%20through%20the%20option%20for%0Aindividual%20and%20team%20rewards.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05113v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConstrained%2520Optimization%2520of%2520Charged%2520Particle%2520Tracking%2520with%2520Multi-Agent%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DTobias%2520Kortus%2520and%2520Ralf%2520Keidel%2520and%2520Nicolas%2520R.%2520Gauger%2520and%2520Jan%2520Kieseler%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520demonstrated%2520immense%2520success%2520in%2520modelling%2520complex%250Aphysics-driven%2520systems%252C%2520providing%2520end-to-end%2520trainable%2520solutions%2520by%2520interacting%250Awith%2520a%2520simulated%2520or%2520real%2520environment%252C%2520maximizing%2520a%2520scalar%2520reward%2520signal.%2520In%250Athis%2520work%252C%2520we%2520propose%252C%2520building%2520upon%2520previous%2520work%252C%2520a%2520multi-agent%2520reinforcement%250Alearning%2520approach%2520with%2520assignment%2520constraints%2520for%2520reconstructing%2520particle%250Atracks%2520in%2520pixelated%2520particle%2520detectors.%2520Our%2520approach%2520optimizes%2520collaboratively%250Aa%2520parametrized%2520policy%252C%2520functioning%2520as%2520a%2520heuristic%2520to%2520a%2520multidimensional%250Aassignment%2520problem%252C%2520by%2520jointly%2520minimizing%2520the%2520total%2520amount%2520of%2520particle%250Ascattering%2520over%2520the%2520reconstructed%2520tracks%2520in%2520a%2520readout%2520frame.%2520To%2520satisfy%250Aconstraints%252C%2520guaranteeing%2520a%2520unique%2520assignment%2520of%2520particle%2520hits%252C%2520we%2520propose%2520a%250Asafety%2520layer%2520solving%2520a%2520linear%2520assignment%2520problem%2520for%2520every%2520joint%2520action.%250AFurther%252C%2520to%2520enforce%2520cost%2520margins%252C%2520increasing%2520the%2520distance%2520of%2520the%2520local%2520policies%250Apredictions%2520to%2520the%2520decision%2520boundaries%2520of%2520the%2520optimizer%2520mappings%252C%2520we%2520recommend%250Athe%2520use%2520of%2520an%2520additional%2520component%2520in%2520the%2520blackbox%2520gradient%2520estimation%252C%2520forcing%250Athe%2520policy%2520to%2520solutions%2520with%2520lower%2520total%2520assignment%2520costs.%2520We%2520empirically%2520show%250Aon%2520simulated%2520data%252C%2520generated%2520for%2520a%2520particle%2520detector%2520developed%2520for%2520proton%250Aimaging%252C%2520the%2520effectiveness%2520of%2520our%2520approach%252C%2520compared%2520to%2520multiple%2520single-%2520and%250Amulti-agent%2520baselines.%2520We%2520further%2520demonstrate%2520the%2520effectiveness%2520of%2520constraints%250Awith%2520cost%2520margins%2520for%2520both%2520optimization%2520and%2520generalization%252C%2520introduced%2520by%2520wider%250Aregions%2520with%2520high%2520reconstruction%2520performance%2520as%2520well%2520as%2520reduced%2520predictive%250Ainstabilities.%2520Our%2520results%2520form%2520the%2520basis%2520for%2520further%2520developments%2520in%2520RL-based%250Atracking%252C%2520offering%2520both%2520enhanced%2520performance%2520with%2520constrained%2520policies%2520and%250Agreater%2520flexibility%2520in%2520optimizing%2520tracking%2520algorithms%2520through%2520the%2520option%2520for%250Aindividual%2520and%2520team%2520rewards.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05113v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constrained%20Optimization%20of%20Charged%20Particle%20Tracking%20with%20Multi-Agent%0A%20%20Reinforcement%20Learning&entry.906535625=Tobias%20Kortus%20and%20Ralf%20Keidel%20and%20Nicolas%20R.%20Gauger%20and%20Jan%20Kieseler&entry.1292438233=%20%20Reinforcement%20learning%20demonstrated%20immense%20success%20in%20modelling%20complex%0Aphysics-driven%20systems%2C%20providing%20end-to-end%20trainable%20solutions%20by%20interacting%0Awith%20a%20simulated%20or%20real%20environment%2C%20maximizing%20a%20scalar%20reward%20signal.%20In%0Athis%20work%2C%20we%20propose%2C%20building%20upon%20previous%20work%2C%20a%20multi-agent%20reinforcement%0Alearning%20approach%20with%20assignment%20constraints%20for%20reconstructing%20particle%0Atracks%20in%20pixelated%20particle%20detectors.%20Our%20approach%20optimizes%20collaboratively%0Aa%20parametrized%20policy%2C%20functioning%20as%20a%20heuristic%20to%20a%20multidimensional%0Aassignment%20problem%2C%20by%20jointly%20minimizing%20the%20total%20amount%20of%20particle%0Ascattering%20over%20the%20reconstructed%20tracks%20in%20a%20readout%20frame.%20To%20satisfy%0Aconstraints%2C%20guaranteeing%20a%20unique%20assignment%20of%20particle%20hits%2C%20we%20propose%20a%0Asafety%20layer%20solving%20a%20linear%20assignment%20problem%20for%20every%20joint%20action.%0AFurther%2C%20to%20enforce%20cost%20margins%2C%20increasing%20the%20distance%20of%20the%20local%20policies%0Apredictions%20to%20the%20decision%20boundaries%20of%20the%20optimizer%20mappings%2C%20we%20recommend%0Athe%20use%20of%20an%20additional%20component%20in%20the%20blackbox%20gradient%20estimation%2C%20forcing%0Athe%20policy%20to%20solutions%20with%20lower%20total%20assignment%20costs.%20We%20empirically%20show%0Aon%20simulated%20data%2C%20generated%20for%20a%20particle%20detector%20developed%20for%20proton%0Aimaging%2C%20the%20effectiveness%20of%20our%20approach%2C%20compared%20to%20multiple%20single-%20and%0Amulti-agent%20baselines.%20We%20further%20demonstrate%20the%20effectiveness%20of%20constraints%0Awith%20cost%20margins%20for%20both%20optimization%20and%20generalization%2C%20introduced%20by%20wider%0Aregions%20with%20high%20reconstruction%20performance%20as%20well%20as%20reduced%20predictive%0Ainstabilities.%20Our%20results%20form%20the%20basis%20for%20further%20developments%20in%20RL-based%0Atracking%2C%20offering%20both%20enhanced%20performance%20with%20constrained%20policies%20and%0Agreater%20flexibility%20in%20optimizing%20tracking%20algorithms%20through%20the%20option%20for%0Aindividual%20and%20team%20rewards.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05113v1&entry.124074799=Read"},
{"title": "Automating the Detection of Code Vulnerabilities by Analyzing GitHub\n  Issues", "author": "Daniele Cipollone and Changjie Wang and Mariano Scazzariello and Simone Ferlin and Maliheh Izadi and Dejan Kostic and Marco Chiesa", "abstract": "  In today's digital landscape, the importance of timely and accurate\nvulnerability detection has significantly increased. This paper presents a\nnovel approach that leverages transformer-based models and machine learning\ntechniques to automate the identification of software vulnerabilities by\nanalyzing GitHub issues. We introduce a new dataset specifically designed for\nclassifying GitHub issues relevant to vulnerability detection. We then examine\nvarious classification techniques to determine their effectiveness. The results\ndemonstrate the potential of this approach for real-world application in early\nvulnerability detection, which could substantially reduce the window of\nexploitation for software vulnerabilities. This research makes a key\ncontribution to the field by providing a scalable and computationally efficient\nframework for automated detection, enabling the prevention of compromised\nsoftware usage before official notifications. This work has the potential to\nenhance the security of open-source software ecosystems.\n", "link": "http://arxiv.org/abs/2501.05258v1", "date": "2025-01-09", "relevancy": 2.1247, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4265}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4254}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4229}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automating%20the%20Detection%20of%20Code%20Vulnerabilities%20by%20Analyzing%20GitHub%0A%20%20Issues&body=Title%3A%20Automating%20the%20Detection%20of%20Code%20Vulnerabilities%20by%20Analyzing%20GitHub%0A%20%20Issues%0AAuthor%3A%20Daniele%20Cipollone%20and%20Changjie%20Wang%20and%20Mariano%20Scazzariello%20and%20Simone%20Ferlin%20and%20Maliheh%20Izadi%20and%20Dejan%20Kostic%20and%20Marco%20Chiesa%0AAbstract%3A%20%20%20In%20today%27s%20digital%20landscape%2C%20the%20importance%20of%20timely%20and%20accurate%0Avulnerability%20detection%20has%20significantly%20increased.%20This%20paper%20presents%20a%0Anovel%20approach%20that%20leverages%20transformer-based%20models%20and%20machine%20learning%0Atechniques%20to%20automate%20the%20identification%20of%20software%20vulnerabilities%20by%0Aanalyzing%20GitHub%20issues.%20We%20introduce%20a%20new%20dataset%20specifically%20designed%20for%0Aclassifying%20GitHub%20issues%20relevant%20to%20vulnerability%20detection.%20We%20then%20examine%0Avarious%20classification%20techniques%20to%20determine%20their%20effectiveness.%20The%20results%0Ademonstrate%20the%20potential%20of%20this%20approach%20for%20real-world%20application%20in%20early%0Avulnerability%20detection%2C%20which%20could%20substantially%20reduce%20the%20window%20of%0Aexploitation%20for%20software%20vulnerabilities.%20This%20research%20makes%20a%20key%0Acontribution%20to%20the%20field%20by%20providing%20a%20scalable%20and%20computationally%20efficient%0Aframework%20for%20automated%20detection%2C%20enabling%20the%20prevention%20of%20compromised%0Asoftware%20usage%20before%20official%20notifications.%20This%20work%20has%20the%20potential%20to%0Aenhance%20the%20security%20of%20open-source%20software%20ecosystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomating%2520the%2520Detection%2520of%2520Code%2520Vulnerabilities%2520by%2520Analyzing%2520GitHub%250A%2520%2520Issues%26entry.906535625%3DDaniele%2520Cipollone%2520and%2520Changjie%2520Wang%2520and%2520Mariano%2520Scazzariello%2520and%2520Simone%2520Ferlin%2520and%2520Maliheh%2520Izadi%2520and%2520Dejan%2520Kostic%2520and%2520Marco%2520Chiesa%26entry.1292438233%3D%2520%2520In%2520today%2527s%2520digital%2520landscape%252C%2520the%2520importance%2520of%2520timely%2520and%2520accurate%250Avulnerability%2520detection%2520has%2520significantly%2520increased.%2520This%2520paper%2520presents%2520a%250Anovel%2520approach%2520that%2520leverages%2520transformer-based%2520models%2520and%2520machine%2520learning%250Atechniques%2520to%2520automate%2520the%2520identification%2520of%2520software%2520vulnerabilities%2520by%250Aanalyzing%2520GitHub%2520issues.%2520We%2520introduce%2520a%2520new%2520dataset%2520specifically%2520designed%2520for%250Aclassifying%2520GitHub%2520issues%2520relevant%2520to%2520vulnerability%2520detection.%2520We%2520then%2520examine%250Avarious%2520classification%2520techniques%2520to%2520determine%2520their%2520effectiveness.%2520The%2520results%250Ademonstrate%2520the%2520potential%2520of%2520this%2520approach%2520for%2520real-world%2520application%2520in%2520early%250Avulnerability%2520detection%252C%2520which%2520could%2520substantially%2520reduce%2520the%2520window%2520of%250Aexploitation%2520for%2520software%2520vulnerabilities.%2520This%2520research%2520makes%2520a%2520key%250Acontribution%2520to%2520the%2520field%2520by%2520providing%2520a%2520scalable%2520and%2520computationally%2520efficient%250Aframework%2520for%2520automated%2520detection%252C%2520enabling%2520the%2520prevention%2520of%2520compromised%250Asoftware%2520usage%2520before%2520official%2520notifications.%2520This%2520work%2520has%2520the%2520potential%2520to%250Aenhance%2520the%2520security%2520of%2520open-source%2520software%2520ecosystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automating%20the%20Detection%20of%20Code%20Vulnerabilities%20by%20Analyzing%20GitHub%0A%20%20Issues&entry.906535625=Daniele%20Cipollone%20and%20Changjie%20Wang%20and%20Mariano%20Scazzariello%20and%20Simone%20Ferlin%20and%20Maliheh%20Izadi%20and%20Dejan%20Kostic%20and%20Marco%20Chiesa&entry.1292438233=%20%20In%20today%27s%20digital%20landscape%2C%20the%20importance%20of%20timely%20and%20accurate%0Avulnerability%20detection%20has%20significantly%20increased.%20This%20paper%20presents%20a%0Anovel%20approach%20that%20leverages%20transformer-based%20models%20and%20machine%20learning%0Atechniques%20to%20automate%20the%20identification%20of%20software%20vulnerabilities%20by%0Aanalyzing%20GitHub%20issues.%20We%20introduce%20a%20new%20dataset%20specifically%20designed%20for%0Aclassifying%20GitHub%20issues%20relevant%20to%20vulnerability%20detection.%20We%20then%20examine%0Avarious%20classification%20techniques%20to%20determine%20their%20effectiveness.%20The%20results%0Ademonstrate%20the%20potential%20of%20this%20approach%20for%20real-world%20application%20in%20early%0Avulnerability%20detection%2C%20which%20could%20substantially%20reduce%20the%20window%20of%0Aexploitation%20for%20software%20vulnerabilities.%20This%20research%20makes%20a%20key%0Acontribution%20to%20the%20field%20by%20providing%20a%20scalable%20and%20computationally%20efficient%0Aframework%20for%20automated%20detection%2C%20enabling%20the%20prevention%20of%20compromised%0Asoftware%20usage%20before%20official%20notifications.%20This%20work%20has%20the%20potential%20to%0Aenhance%20the%20security%20of%20open-source%20software%20ecosystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05258v1&entry.124074799=Read"},
{"title": "Snapshot: Towards Application-centered Models for Pedestrian Trajectory\n  Prediction in Urban Traffic Environments", "author": "Nico Uhlemann and Yipeng Zhou and Tobias Simeon Mohr and Markus Lienkamp", "abstract": "  This paper explores pedestrian trajectory prediction in urban traffic while\nfocusing on both model accuracy and real-world applicability. While promising\napproaches exist, they often revolve around pedestrian datasets excluding\ntraffic-related information, or resemble architectures that are either not\nreal-time capable or robust. To address these limitations, we first introduce a\ndedicated benchmark based on Argoverse 2, specifically targeting pedestrians in\ntraffic environments. Following this, we present Snapshot, a modular,\nfeed-forward neural network that outperforms the current state of the art,\nreducing the Average Displacement Error (ADE) by 8.8% while utilizing\nsignificantly less information. Despite its agent-centric encoding scheme,\nSnapshot demonstrates scalability, real-time performance, and robustness to\nvarying motion histories. Moreover, by integrating Snapshot into a modular\nautonomous driving software stack, we showcase its real-world applicability.\n", "link": "http://arxiv.org/abs/2409.01971v2", "date": "2025-01-09", "relevancy": 2.1076, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5568}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5255}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5164}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Snapshot%3A%20Towards%20Application-centered%20Models%20for%20Pedestrian%20Trajectory%0A%20%20Prediction%20in%20Urban%20Traffic%20Environments&body=Title%3A%20Snapshot%3A%20Towards%20Application-centered%20Models%20for%20Pedestrian%20Trajectory%0A%20%20Prediction%20in%20Urban%20Traffic%20Environments%0AAuthor%3A%20Nico%20Uhlemann%20and%20Yipeng%20Zhou%20and%20Tobias%20Simeon%20Mohr%20and%20Markus%20Lienkamp%0AAbstract%3A%20%20%20This%20paper%20explores%20pedestrian%20trajectory%20prediction%20in%20urban%20traffic%20while%0Afocusing%20on%20both%20model%20accuracy%20and%20real-world%20applicability.%20While%20promising%0Aapproaches%20exist%2C%20they%20often%20revolve%20around%20pedestrian%20datasets%20excluding%0Atraffic-related%20information%2C%20or%20resemble%20architectures%20that%20are%20either%20not%0Areal-time%20capable%20or%20robust.%20To%20address%20these%20limitations%2C%20we%20first%20introduce%20a%0Adedicated%20benchmark%20based%20on%20Argoverse%202%2C%20specifically%20targeting%20pedestrians%20in%0Atraffic%20environments.%20Following%20this%2C%20we%20present%20Snapshot%2C%20a%20modular%2C%0Afeed-forward%20neural%20network%20that%20outperforms%20the%20current%20state%20of%20the%20art%2C%0Areducing%20the%20Average%20Displacement%20Error%20%28ADE%29%20by%208.8%25%20while%20utilizing%0Asignificantly%20less%20information.%20Despite%20its%20agent-centric%20encoding%20scheme%2C%0ASnapshot%20demonstrates%20scalability%2C%20real-time%20performance%2C%20and%20robustness%20to%0Avarying%20motion%20histories.%20Moreover%2C%20by%20integrating%20Snapshot%20into%20a%20modular%0Aautonomous%20driving%20software%20stack%2C%20we%20showcase%20its%20real-world%20applicability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.01971v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSnapshot%253A%2520Towards%2520Application-centered%2520Models%2520for%2520Pedestrian%2520Trajectory%250A%2520%2520Prediction%2520in%2520Urban%2520Traffic%2520Environments%26entry.906535625%3DNico%2520Uhlemann%2520and%2520Yipeng%2520Zhou%2520and%2520Tobias%2520Simeon%2520Mohr%2520and%2520Markus%2520Lienkamp%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520pedestrian%2520trajectory%2520prediction%2520in%2520urban%2520traffic%2520while%250Afocusing%2520on%2520both%2520model%2520accuracy%2520and%2520real-world%2520applicability.%2520While%2520promising%250Aapproaches%2520exist%252C%2520they%2520often%2520revolve%2520around%2520pedestrian%2520datasets%2520excluding%250Atraffic-related%2520information%252C%2520or%2520resemble%2520architectures%2520that%2520are%2520either%2520not%250Areal-time%2520capable%2520or%2520robust.%2520To%2520address%2520these%2520limitations%252C%2520we%2520first%2520introduce%2520a%250Adedicated%2520benchmark%2520based%2520on%2520Argoverse%25202%252C%2520specifically%2520targeting%2520pedestrians%2520in%250Atraffic%2520environments.%2520Following%2520this%252C%2520we%2520present%2520Snapshot%252C%2520a%2520modular%252C%250Afeed-forward%2520neural%2520network%2520that%2520outperforms%2520the%2520current%2520state%2520of%2520the%2520art%252C%250Areducing%2520the%2520Average%2520Displacement%2520Error%2520%2528ADE%2529%2520by%25208.8%2525%2520while%2520utilizing%250Asignificantly%2520less%2520information.%2520Despite%2520its%2520agent-centric%2520encoding%2520scheme%252C%250ASnapshot%2520demonstrates%2520scalability%252C%2520real-time%2520performance%252C%2520and%2520robustness%2520to%250Avarying%2520motion%2520histories.%2520Moreover%252C%2520by%2520integrating%2520Snapshot%2520into%2520a%2520modular%250Aautonomous%2520driving%2520software%2520stack%252C%2520we%2520showcase%2520its%2520real-world%2520applicability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.01971v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Snapshot%3A%20Towards%20Application-centered%20Models%20for%20Pedestrian%20Trajectory%0A%20%20Prediction%20in%20Urban%20Traffic%20Environments&entry.906535625=Nico%20Uhlemann%20and%20Yipeng%20Zhou%20and%20Tobias%20Simeon%20Mohr%20and%20Markus%20Lienkamp&entry.1292438233=%20%20This%20paper%20explores%20pedestrian%20trajectory%20prediction%20in%20urban%20traffic%20while%0Afocusing%20on%20both%20model%20accuracy%20and%20real-world%20applicability.%20While%20promising%0Aapproaches%20exist%2C%20they%20often%20revolve%20around%20pedestrian%20datasets%20excluding%0Atraffic-related%20information%2C%20or%20resemble%20architectures%20that%20are%20either%20not%0Areal-time%20capable%20or%20robust.%20To%20address%20these%20limitations%2C%20we%20first%20introduce%20a%0Adedicated%20benchmark%20based%20on%20Argoverse%202%2C%20specifically%20targeting%20pedestrians%20in%0Atraffic%20environments.%20Following%20this%2C%20we%20present%20Snapshot%2C%20a%20modular%2C%0Afeed-forward%20neural%20network%20that%20outperforms%20the%20current%20state%20of%20the%20art%2C%0Areducing%20the%20Average%20Displacement%20Error%20%28ADE%29%20by%208.8%25%20while%20utilizing%0Asignificantly%20less%20information.%20Despite%20its%20agent-centric%20encoding%20scheme%2C%0ASnapshot%20demonstrates%20scalability%2C%20real-time%20performance%2C%20and%20robustness%20to%0Avarying%20motion%20histories.%20Moreover%2C%20by%20integrating%20Snapshot%20into%20a%20modular%0Aautonomous%20driving%20software%20stack%2C%20we%20showcase%20its%20real-world%20applicability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.01971v2&entry.124074799=Read"},
{"title": "Learning In-Distribution Representations for Anomaly Detection", "author": "William T. Lunardi and Abdulrahman Banabila and Dania Herzalla and Martin L. Andreoni", "abstract": "  Anomaly detection involves identifying data patterns that deviate from the\nanticipated norm. Traditional methods struggle in high-dimensional spaces due\nto the curse of dimensionality. In recent years, self-supervised learning,\nparticularly through contrastive objectives, has driven advances in anomaly\ndetection. However, vanilla contrastive learning struggles to align with the\nunique demands of anomaly detection, as it lacks a pretext task tailored to the\nhomogeneous nature of In-Distribution (ID) data and the diversity of\nOut-of-Distribution (OOD) anomalies. Methods that attempt to address these\nchallenges, such as introducing hard negatives through synthetic outliers,\nOutlier Exposure (OE), and supervised objectives, often rely on pretext tasks\nthat fail to balance compact clustering of ID samples with sufficient\nseparation from OOD data. In this work, we propose Focused In-distribution\nRepresentation Modeling (FIRM), a contrastive learning objective specifically\ndesigned for anomaly detection. Unlike existing approaches, FIRM incorporates\nsynthetic outliers into its pretext task in a way that actively shapes the\nrepresentation space, promoting compact clustering of ID samples while\nenforcing strong separation from outliers. This formulation addresses the\nchallenges of class collision, enhancing both the compactness of ID\nrepresentations and the discriminative power of the learned feature space. We\nshow that FIRM surpasses other contrastive methods in standard benchmarks,\nsignificantly enhancing anomaly detection compared to both traditional and\nsupervised contrastive learning objectives. Our ablation studies confirm that\nFIRM consistently improves the quality of representations and shows robustness\nacross a range of scoring methods. The code is available at:\nhttps://github.com/willtl/firm.\n", "link": "http://arxiv.org/abs/2501.05130v1", "date": "2025-01-09", "relevancy": 2.0836, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5327}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5198}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20In-Distribution%20Representations%20for%20Anomaly%20Detection&body=Title%3A%20Learning%20In-Distribution%20Representations%20for%20Anomaly%20Detection%0AAuthor%3A%20William%20T.%20Lunardi%20and%20Abdulrahman%20Banabila%20and%20Dania%20Herzalla%20and%20Martin%20L.%20Andreoni%0AAbstract%3A%20%20%20Anomaly%20detection%20involves%20identifying%20data%20patterns%20that%20deviate%20from%20the%0Aanticipated%20norm.%20Traditional%20methods%20struggle%20in%20high-dimensional%20spaces%20due%0Ato%20the%20curse%20of%20dimensionality.%20In%20recent%20years%2C%20self-supervised%20learning%2C%0Aparticularly%20through%20contrastive%20objectives%2C%20has%20driven%20advances%20in%20anomaly%0Adetection.%20However%2C%20vanilla%20contrastive%20learning%20struggles%20to%20align%20with%20the%0Aunique%20demands%20of%20anomaly%20detection%2C%20as%20it%20lacks%20a%20pretext%20task%20tailored%20to%20the%0Ahomogeneous%20nature%20of%20In-Distribution%20%28ID%29%20data%20and%20the%20diversity%20of%0AOut-of-Distribution%20%28OOD%29%20anomalies.%20Methods%20that%20attempt%20to%20address%20these%0Achallenges%2C%20such%20as%20introducing%20hard%20negatives%20through%20synthetic%20outliers%2C%0AOutlier%20Exposure%20%28OE%29%2C%20and%20supervised%20objectives%2C%20often%20rely%20on%20pretext%20tasks%0Athat%20fail%20to%20balance%20compact%20clustering%20of%20ID%20samples%20with%20sufficient%0Aseparation%20from%20OOD%20data.%20In%20this%20work%2C%20we%20propose%20Focused%20In-distribution%0ARepresentation%20Modeling%20%28FIRM%29%2C%20a%20contrastive%20learning%20objective%20specifically%0Adesigned%20for%20anomaly%20detection.%20Unlike%20existing%20approaches%2C%20FIRM%20incorporates%0Asynthetic%20outliers%20into%20its%20pretext%20task%20in%20a%20way%20that%20actively%20shapes%20the%0Arepresentation%20space%2C%20promoting%20compact%20clustering%20of%20ID%20samples%20while%0Aenforcing%20strong%20separation%20from%20outliers.%20This%20formulation%20addresses%20the%0Achallenges%20of%20class%20collision%2C%20enhancing%20both%20the%20compactness%20of%20ID%0Arepresentations%20and%20the%20discriminative%20power%20of%20the%20learned%20feature%20space.%20We%0Ashow%20that%20FIRM%20surpasses%20other%20contrastive%20methods%20in%20standard%20benchmarks%2C%0Asignificantly%20enhancing%20anomaly%20detection%20compared%20to%20both%20traditional%20and%0Asupervised%20contrastive%20learning%20objectives.%20Our%20ablation%20studies%20confirm%20that%0AFIRM%20consistently%20improves%20the%20quality%20of%20representations%20and%20shows%20robustness%0Aacross%20a%20range%20of%20scoring%20methods.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/willtl/firm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05130v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520In-Distribution%2520Representations%2520for%2520Anomaly%2520Detection%26entry.906535625%3DWilliam%2520T.%2520Lunardi%2520and%2520Abdulrahman%2520Banabila%2520and%2520Dania%2520Herzalla%2520and%2520Martin%2520L.%2520Andreoni%26entry.1292438233%3D%2520%2520Anomaly%2520detection%2520involves%2520identifying%2520data%2520patterns%2520that%2520deviate%2520from%2520the%250Aanticipated%2520norm.%2520Traditional%2520methods%2520struggle%2520in%2520high-dimensional%2520spaces%2520due%250Ato%2520the%2520curse%2520of%2520dimensionality.%2520In%2520recent%2520years%252C%2520self-supervised%2520learning%252C%250Aparticularly%2520through%2520contrastive%2520objectives%252C%2520has%2520driven%2520advances%2520in%2520anomaly%250Adetection.%2520However%252C%2520vanilla%2520contrastive%2520learning%2520struggles%2520to%2520align%2520with%2520the%250Aunique%2520demands%2520of%2520anomaly%2520detection%252C%2520as%2520it%2520lacks%2520a%2520pretext%2520task%2520tailored%2520to%2520the%250Ahomogeneous%2520nature%2520of%2520In-Distribution%2520%2528ID%2529%2520data%2520and%2520the%2520diversity%2520of%250AOut-of-Distribution%2520%2528OOD%2529%2520anomalies.%2520Methods%2520that%2520attempt%2520to%2520address%2520these%250Achallenges%252C%2520such%2520as%2520introducing%2520hard%2520negatives%2520through%2520synthetic%2520outliers%252C%250AOutlier%2520Exposure%2520%2528OE%2529%252C%2520and%2520supervised%2520objectives%252C%2520often%2520rely%2520on%2520pretext%2520tasks%250Athat%2520fail%2520to%2520balance%2520compact%2520clustering%2520of%2520ID%2520samples%2520with%2520sufficient%250Aseparation%2520from%2520OOD%2520data.%2520In%2520this%2520work%252C%2520we%2520propose%2520Focused%2520In-distribution%250ARepresentation%2520Modeling%2520%2528FIRM%2529%252C%2520a%2520contrastive%2520learning%2520objective%2520specifically%250Adesigned%2520for%2520anomaly%2520detection.%2520Unlike%2520existing%2520approaches%252C%2520FIRM%2520incorporates%250Asynthetic%2520outliers%2520into%2520its%2520pretext%2520task%2520in%2520a%2520way%2520that%2520actively%2520shapes%2520the%250Arepresentation%2520space%252C%2520promoting%2520compact%2520clustering%2520of%2520ID%2520samples%2520while%250Aenforcing%2520strong%2520separation%2520from%2520outliers.%2520This%2520formulation%2520addresses%2520the%250Achallenges%2520of%2520class%2520collision%252C%2520enhancing%2520both%2520the%2520compactness%2520of%2520ID%250Arepresentations%2520and%2520the%2520discriminative%2520power%2520of%2520the%2520learned%2520feature%2520space.%2520We%250Ashow%2520that%2520FIRM%2520surpasses%2520other%2520contrastive%2520methods%2520in%2520standard%2520benchmarks%252C%250Asignificantly%2520enhancing%2520anomaly%2520detection%2520compared%2520to%2520both%2520traditional%2520and%250Asupervised%2520contrastive%2520learning%2520objectives.%2520Our%2520ablation%2520studies%2520confirm%2520that%250AFIRM%2520consistently%2520improves%2520the%2520quality%2520of%2520representations%2520and%2520shows%2520robustness%250Aacross%2520a%2520range%2520of%2520scoring%2520methods.%2520The%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/willtl/firm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05130v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20In-Distribution%20Representations%20for%20Anomaly%20Detection&entry.906535625=William%20T.%20Lunardi%20and%20Abdulrahman%20Banabila%20and%20Dania%20Herzalla%20and%20Martin%20L.%20Andreoni&entry.1292438233=%20%20Anomaly%20detection%20involves%20identifying%20data%20patterns%20that%20deviate%20from%20the%0Aanticipated%20norm.%20Traditional%20methods%20struggle%20in%20high-dimensional%20spaces%20due%0Ato%20the%20curse%20of%20dimensionality.%20In%20recent%20years%2C%20self-supervised%20learning%2C%0Aparticularly%20through%20contrastive%20objectives%2C%20has%20driven%20advances%20in%20anomaly%0Adetection.%20However%2C%20vanilla%20contrastive%20learning%20struggles%20to%20align%20with%20the%0Aunique%20demands%20of%20anomaly%20detection%2C%20as%20it%20lacks%20a%20pretext%20task%20tailored%20to%20the%0Ahomogeneous%20nature%20of%20In-Distribution%20%28ID%29%20data%20and%20the%20diversity%20of%0AOut-of-Distribution%20%28OOD%29%20anomalies.%20Methods%20that%20attempt%20to%20address%20these%0Achallenges%2C%20such%20as%20introducing%20hard%20negatives%20through%20synthetic%20outliers%2C%0AOutlier%20Exposure%20%28OE%29%2C%20and%20supervised%20objectives%2C%20often%20rely%20on%20pretext%20tasks%0Athat%20fail%20to%20balance%20compact%20clustering%20of%20ID%20samples%20with%20sufficient%0Aseparation%20from%20OOD%20data.%20In%20this%20work%2C%20we%20propose%20Focused%20In-distribution%0ARepresentation%20Modeling%20%28FIRM%29%2C%20a%20contrastive%20learning%20objective%20specifically%0Adesigned%20for%20anomaly%20detection.%20Unlike%20existing%20approaches%2C%20FIRM%20incorporates%0Asynthetic%20outliers%20into%20its%20pretext%20task%20in%20a%20way%20that%20actively%20shapes%20the%0Arepresentation%20space%2C%20promoting%20compact%20clustering%20of%20ID%20samples%20while%0Aenforcing%20strong%20separation%20from%20outliers.%20This%20formulation%20addresses%20the%0Achallenges%20of%20class%20collision%2C%20enhancing%20both%20the%20compactness%20of%20ID%0Arepresentations%20and%20the%20discriminative%20power%20of%20the%20learned%20feature%20space.%20We%0Ashow%20that%20FIRM%20surpasses%20other%20contrastive%20methods%20in%20standard%20benchmarks%2C%0Asignificantly%20enhancing%20anomaly%20detection%20compared%20to%20both%20traditional%20and%0Asupervised%20contrastive%20learning%20objectives.%20Our%20ablation%20studies%20confirm%20that%0AFIRM%20consistently%20improves%20the%20quality%20of%20representations%20and%20shows%20robustness%0Aacross%20a%20range%20of%20scoring%20methods.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/willtl/firm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05130v1&entry.124074799=Read"},
{"title": "Explainable AI-Enhanced Deep Learning for Pumpkin Leaf Disease\n  Detection: A Comparative Analysis of CNN Architectures", "author": "Md. Arafat Alam Khandaker and Ziyan Shirin Raha and Shifat Islam and Tashreef Muhammad", "abstract": "  Pumpkin leaf diseases are significant threats to agricultural productivity,\nrequiring a timely and precise diagnosis for effective management. Traditional\nidentification methods are laborious and susceptible to human error,\nemphasizing the necessity for automated solutions. This study employs on the\n\"Pumpkin Leaf Disease Dataset\", that comprises of 2000 high-resolution images\nseparated into five categories. Downy mildew, powdery mildew, mosaic disease,\nbacterial leaf spot, and healthy leaves. The dataset was rigorously assembled\nfrom several agricultural fields to ensure a strong representation for model\ntraining. We explored many proficient deep learning architectures, including\nDenseNet201, DenseNet121, DenseNet169, Xception, ResNet50, ResNet101 and\nInceptionResNetV2, and observed that ResNet50 performed most effectively, with\nan accuracy of 90.5% and comparable precision, recall, and F1-Score. We used\nExplainable AI (XAI) approaches like Grad-CAM, Grad-CAM++, Score-CAM, and\nLayer-CAM to provide meaningful representations of model decision-making\nprocesses, which improved understanding and trust in automated disease\ndiagnostics. These findings demonstrate ResNet50's potential to revolutionize\npumpkin leaf disease detection, allowing for earlier and more accurate\ntreatments.\n", "link": "http://arxiv.org/abs/2501.05449v1", "date": "2025-01-09", "relevancy": 2.0808, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5294}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5283}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5077}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explainable%20AI-Enhanced%20Deep%20Learning%20for%20Pumpkin%20Leaf%20Disease%0A%20%20Detection%3A%20A%20Comparative%20Analysis%20of%20CNN%20Architectures&body=Title%3A%20Explainable%20AI-Enhanced%20Deep%20Learning%20for%20Pumpkin%20Leaf%20Disease%0A%20%20Detection%3A%20A%20Comparative%20Analysis%20of%20CNN%20Architectures%0AAuthor%3A%20Md.%20Arafat%20Alam%20Khandaker%20and%20Ziyan%20Shirin%20Raha%20and%20Shifat%20Islam%20and%20Tashreef%20Muhammad%0AAbstract%3A%20%20%20Pumpkin%20leaf%20diseases%20are%20significant%20threats%20to%20agricultural%20productivity%2C%0Arequiring%20a%20timely%20and%20precise%20diagnosis%20for%20effective%20management.%20Traditional%0Aidentification%20methods%20are%20laborious%20and%20susceptible%20to%20human%20error%2C%0Aemphasizing%20the%20necessity%20for%20automated%20solutions.%20This%20study%20employs%20on%20the%0A%22Pumpkin%20Leaf%20Disease%20Dataset%22%2C%20that%20comprises%20of%202000%20high-resolution%20images%0Aseparated%20into%20five%20categories.%20Downy%20mildew%2C%20powdery%20mildew%2C%20mosaic%20disease%2C%0Abacterial%20leaf%20spot%2C%20and%20healthy%20leaves.%20The%20dataset%20was%20rigorously%20assembled%0Afrom%20several%20agricultural%20fields%20to%20ensure%20a%20strong%20representation%20for%20model%0Atraining.%20We%20explored%20many%20proficient%20deep%20learning%20architectures%2C%20including%0ADenseNet201%2C%20DenseNet121%2C%20DenseNet169%2C%20Xception%2C%20ResNet50%2C%20ResNet101%20and%0AInceptionResNetV2%2C%20and%20observed%20that%20ResNet50%20performed%20most%20effectively%2C%20with%0Aan%20accuracy%20of%2090.5%25%20and%20comparable%20precision%2C%20recall%2C%20and%20F1-Score.%20We%20used%0AExplainable%20AI%20%28XAI%29%20approaches%20like%20Grad-CAM%2C%20Grad-CAM%2B%2B%2C%20Score-CAM%2C%20and%0ALayer-CAM%20to%20provide%20meaningful%20representations%20of%20model%20decision-making%0Aprocesses%2C%20which%20improved%20understanding%20and%20trust%20in%20automated%20disease%0Adiagnostics.%20These%20findings%20demonstrate%20ResNet50%27s%20potential%20to%20revolutionize%0Apumpkin%20leaf%20disease%20detection%2C%20allowing%20for%20earlier%20and%20more%20accurate%0Atreatments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05449v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplainable%2520AI-Enhanced%2520Deep%2520Learning%2520for%2520Pumpkin%2520Leaf%2520Disease%250A%2520%2520Detection%253A%2520A%2520Comparative%2520Analysis%2520of%2520CNN%2520Architectures%26entry.906535625%3DMd.%2520Arafat%2520Alam%2520Khandaker%2520and%2520Ziyan%2520Shirin%2520Raha%2520and%2520Shifat%2520Islam%2520and%2520Tashreef%2520Muhammad%26entry.1292438233%3D%2520%2520Pumpkin%2520leaf%2520diseases%2520are%2520significant%2520threats%2520to%2520agricultural%2520productivity%252C%250Arequiring%2520a%2520timely%2520and%2520precise%2520diagnosis%2520for%2520effective%2520management.%2520Traditional%250Aidentification%2520methods%2520are%2520laborious%2520and%2520susceptible%2520to%2520human%2520error%252C%250Aemphasizing%2520the%2520necessity%2520for%2520automated%2520solutions.%2520This%2520study%2520employs%2520on%2520the%250A%2522Pumpkin%2520Leaf%2520Disease%2520Dataset%2522%252C%2520that%2520comprises%2520of%25202000%2520high-resolution%2520images%250Aseparated%2520into%2520five%2520categories.%2520Downy%2520mildew%252C%2520powdery%2520mildew%252C%2520mosaic%2520disease%252C%250Abacterial%2520leaf%2520spot%252C%2520and%2520healthy%2520leaves.%2520The%2520dataset%2520was%2520rigorously%2520assembled%250Afrom%2520several%2520agricultural%2520fields%2520to%2520ensure%2520a%2520strong%2520representation%2520for%2520model%250Atraining.%2520We%2520explored%2520many%2520proficient%2520deep%2520learning%2520architectures%252C%2520including%250ADenseNet201%252C%2520DenseNet121%252C%2520DenseNet169%252C%2520Xception%252C%2520ResNet50%252C%2520ResNet101%2520and%250AInceptionResNetV2%252C%2520and%2520observed%2520that%2520ResNet50%2520performed%2520most%2520effectively%252C%2520with%250Aan%2520accuracy%2520of%252090.5%2525%2520and%2520comparable%2520precision%252C%2520recall%252C%2520and%2520F1-Score.%2520We%2520used%250AExplainable%2520AI%2520%2528XAI%2529%2520approaches%2520like%2520Grad-CAM%252C%2520Grad-CAM%252B%252B%252C%2520Score-CAM%252C%2520and%250ALayer-CAM%2520to%2520provide%2520meaningful%2520representations%2520of%2520model%2520decision-making%250Aprocesses%252C%2520which%2520improved%2520understanding%2520and%2520trust%2520in%2520automated%2520disease%250Adiagnostics.%2520These%2520findings%2520demonstrate%2520ResNet50%2527s%2520potential%2520to%2520revolutionize%250Apumpkin%2520leaf%2520disease%2520detection%252C%2520allowing%2520for%2520earlier%2520and%2520more%2520accurate%250Atreatments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05449v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainable%20AI-Enhanced%20Deep%20Learning%20for%20Pumpkin%20Leaf%20Disease%0A%20%20Detection%3A%20A%20Comparative%20Analysis%20of%20CNN%20Architectures&entry.906535625=Md.%20Arafat%20Alam%20Khandaker%20and%20Ziyan%20Shirin%20Raha%20and%20Shifat%20Islam%20and%20Tashreef%20Muhammad&entry.1292438233=%20%20Pumpkin%20leaf%20diseases%20are%20significant%20threats%20to%20agricultural%20productivity%2C%0Arequiring%20a%20timely%20and%20precise%20diagnosis%20for%20effective%20management.%20Traditional%0Aidentification%20methods%20are%20laborious%20and%20susceptible%20to%20human%20error%2C%0Aemphasizing%20the%20necessity%20for%20automated%20solutions.%20This%20study%20employs%20on%20the%0A%22Pumpkin%20Leaf%20Disease%20Dataset%22%2C%20that%20comprises%20of%202000%20high-resolution%20images%0Aseparated%20into%20five%20categories.%20Downy%20mildew%2C%20powdery%20mildew%2C%20mosaic%20disease%2C%0Abacterial%20leaf%20spot%2C%20and%20healthy%20leaves.%20The%20dataset%20was%20rigorously%20assembled%0Afrom%20several%20agricultural%20fields%20to%20ensure%20a%20strong%20representation%20for%20model%0Atraining.%20We%20explored%20many%20proficient%20deep%20learning%20architectures%2C%20including%0ADenseNet201%2C%20DenseNet121%2C%20DenseNet169%2C%20Xception%2C%20ResNet50%2C%20ResNet101%20and%0AInceptionResNetV2%2C%20and%20observed%20that%20ResNet50%20performed%20most%20effectively%2C%20with%0Aan%20accuracy%20of%2090.5%25%20and%20comparable%20precision%2C%20recall%2C%20and%20F1-Score.%20We%20used%0AExplainable%20AI%20%28XAI%29%20approaches%20like%20Grad-CAM%2C%20Grad-CAM%2B%2B%2C%20Score-CAM%2C%20and%0ALayer-CAM%20to%20provide%20meaningful%20representations%20of%20model%20decision-making%0Aprocesses%2C%20which%20improved%20understanding%20and%20trust%20in%20automated%20disease%0Adiagnostics.%20These%20findings%20demonstrate%20ResNet50%27s%20potential%20to%20revolutionize%0Apumpkin%20leaf%20disease%20detection%2C%20allowing%20for%20earlier%20and%20more%20accurate%0Atreatments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05449v1&entry.124074799=Read"},
{"title": "A Contrastive Symmetric Forward-Forward Algorithm (SFFA) for Continual\n  Learning Tasks", "author": "Erik B. Terres-Escudero and Javier Del Ser and Pablo Garcia Bringas", "abstract": "  The so-called Forward-Forward Algorithm (FFA) has recently gained momentum as\nan alternative to the conventional back-propagation algorithm for neural\nnetwork learning, yielding competitive performance across various modeling\ntasks. By replacing the backward pass of gradient back-propagation with two\ncontrastive forward passes, the FFA avoids several shortcomings undergone by\nits predecessor (e.g., vanishing/exploding gradient) by enabling layer-wise\ntraining heuristics. In classification tasks, this contrastive method has been\nproven to effectively create a latent sparse representation of the input data,\nultimately favoring discriminability. However, FFA exhibits an inherent\nasymmetric gradient behavior due to an imbalanced loss function between\npositive and negative data, adversely impacting on the model's generalization\ncapabilities and leading to an accuracy degradation. To address this issue,\nthis work proposes the Symmetric Forward-Forward Algorithm (SFFA), a novel\nmodification of the original FFA which partitions each layer into positive and\nnegative neurons. This allows the local fitness function to be defined as the\nratio between the activation of positive neurons and the overall layer\nactivity, resulting in a symmetric loss landscape during the training phase. To\nevaluate the enhanced convergence of our method, we conduct several experiments\nusing multiple image classification benchmarks, comparing the accuracy of\nmodels trained with SFFA to those trained with its FFA counterpart. As a\nbyproduct of this reformulation, we explore the advantages of using a\nlayer-wise training algorithm for Continual Learning (CL) tasks. The\nspecialization of neurons and the sparsity of their activations induced by\nlayer-wise training algorithms enable efficient CL strategies that incorporate\nnew knowledge (classes) into the neural network, while preventing catastrophic\nforgetting of previously...\n", "link": "http://arxiv.org/abs/2409.07387v2", "date": "2025-01-09", "relevancy": 2.0742, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5317}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5138}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Contrastive%20Symmetric%20Forward-Forward%20Algorithm%20%28SFFA%29%20for%20Continual%0A%20%20Learning%20Tasks&body=Title%3A%20A%20Contrastive%20Symmetric%20Forward-Forward%20Algorithm%20%28SFFA%29%20for%20Continual%0A%20%20Learning%20Tasks%0AAuthor%3A%20Erik%20B.%20Terres-Escudero%20and%20Javier%20Del%20Ser%20and%20Pablo%20Garcia%20Bringas%0AAbstract%3A%20%20%20The%20so-called%20Forward-Forward%20Algorithm%20%28FFA%29%20has%20recently%20gained%20momentum%20as%0Aan%20alternative%20to%20the%20conventional%20back-propagation%20algorithm%20for%20neural%0Anetwork%20learning%2C%20yielding%20competitive%20performance%20across%20various%20modeling%0Atasks.%20By%20replacing%20the%20backward%20pass%20of%20gradient%20back-propagation%20with%20two%0Acontrastive%20forward%20passes%2C%20the%20FFA%20avoids%20several%20shortcomings%20undergone%20by%0Aits%20predecessor%20%28e.g.%2C%20vanishing/exploding%20gradient%29%20by%20enabling%20layer-wise%0Atraining%20heuristics.%20In%20classification%20tasks%2C%20this%20contrastive%20method%20has%20been%0Aproven%20to%20effectively%20create%20a%20latent%20sparse%20representation%20of%20the%20input%20data%2C%0Aultimately%20favoring%20discriminability.%20However%2C%20FFA%20exhibits%20an%20inherent%0Aasymmetric%20gradient%20behavior%20due%20to%20an%20imbalanced%20loss%20function%20between%0Apositive%20and%20negative%20data%2C%20adversely%20impacting%20on%20the%20model%27s%20generalization%0Acapabilities%20and%20leading%20to%20an%20accuracy%20degradation.%20To%20address%20this%20issue%2C%0Athis%20work%20proposes%20the%20Symmetric%20Forward-Forward%20Algorithm%20%28SFFA%29%2C%20a%20novel%0Amodification%20of%20the%20original%20FFA%20which%20partitions%20each%20layer%20into%20positive%20and%0Anegative%20neurons.%20This%20allows%20the%20local%20fitness%20function%20to%20be%20defined%20as%20the%0Aratio%20between%20the%20activation%20of%20positive%20neurons%20and%20the%20overall%20layer%0Aactivity%2C%20resulting%20in%20a%20symmetric%20loss%20landscape%20during%20the%20training%20phase.%20To%0Aevaluate%20the%20enhanced%20convergence%20of%20our%20method%2C%20we%20conduct%20several%20experiments%0Ausing%20multiple%20image%20classification%20benchmarks%2C%20comparing%20the%20accuracy%20of%0Amodels%20trained%20with%20SFFA%20to%20those%20trained%20with%20its%20FFA%20counterpart.%20As%20a%0Abyproduct%20of%20this%20reformulation%2C%20we%20explore%20the%20advantages%20of%20using%20a%0Alayer-wise%20training%20algorithm%20for%20Continual%20Learning%20%28CL%29%20tasks.%20The%0Aspecialization%20of%20neurons%20and%20the%20sparsity%20of%20their%20activations%20induced%20by%0Alayer-wise%20training%20algorithms%20enable%20efficient%20CL%20strategies%20that%20incorporate%0Anew%20knowledge%20%28classes%29%20into%20the%20neural%20network%2C%20while%20preventing%20catastrophic%0Aforgetting%20of%20previously...%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07387v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Contrastive%2520Symmetric%2520Forward-Forward%2520Algorithm%2520%2528SFFA%2529%2520for%2520Continual%250A%2520%2520Learning%2520Tasks%26entry.906535625%3DErik%2520B.%2520Terres-Escudero%2520and%2520Javier%2520Del%2520Ser%2520and%2520Pablo%2520Garcia%2520Bringas%26entry.1292438233%3D%2520%2520The%2520so-called%2520Forward-Forward%2520Algorithm%2520%2528FFA%2529%2520has%2520recently%2520gained%2520momentum%2520as%250Aan%2520alternative%2520to%2520the%2520conventional%2520back-propagation%2520algorithm%2520for%2520neural%250Anetwork%2520learning%252C%2520yielding%2520competitive%2520performance%2520across%2520various%2520modeling%250Atasks.%2520By%2520replacing%2520the%2520backward%2520pass%2520of%2520gradient%2520back-propagation%2520with%2520two%250Acontrastive%2520forward%2520passes%252C%2520the%2520FFA%2520avoids%2520several%2520shortcomings%2520undergone%2520by%250Aits%2520predecessor%2520%2528e.g.%252C%2520vanishing/exploding%2520gradient%2529%2520by%2520enabling%2520layer-wise%250Atraining%2520heuristics.%2520In%2520classification%2520tasks%252C%2520this%2520contrastive%2520method%2520has%2520been%250Aproven%2520to%2520effectively%2520create%2520a%2520latent%2520sparse%2520representation%2520of%2520the%2520input%2520data%252C%250Aultimately%2520favoring%2520discriminability.%2520However%252C%2520FFA%2520exhibits%2520an%2520inherent%250Aasymmetric%2520gradient%2520behavior%2520due%2520to%2520an%2520imbalanced%2520loss%2520function%2520between%250Apositive%2520and%2520negative%2520data%252C%2520adversely%2520impacting%2520on%2520the%2520model%2527s%2520generalization%250Acapabilities%2520and%2520leading%2520to%2520an%2520accuracy%2520degradation.%2520To%2520address%2520this%2520issue%252C%250Athis%2520work%2520proposes%2520the%2520Symmetric%2520Forward-Forward%2520Algorithm%2520%2528SFFA%2529%252C%2520a%2520novel%250Amodification%2520of%2520the%2520original%2520FFA%2520which%2520partitions%2520each%2520layer%2520into%2520positive%2520and%250Anegative%2520neurons.%2520This%2520allows%2520the%2520local%2520fitness%2520function%2520to%2520be%2520defined%2520as%2520the%250Aratio%2520between%2520the%2520activation%2520of%2520positive%2520neurons%2520and%2520the%2520overall%2520layer%250Aactivity%252C%2520resulting%2520in%2520a%2520symmetric%2520loss%2520landscape%2520during%2520the%2520training%2520phase.%2520To%250Aevaluate%2520the%2520enhanced%2520convergence%2520of%2520our%2520method%252C%2520we%2520conduct%2520several%2520experiments%250Ausing%2520multiple%2520image%2520classification%2520benchmarks%252C%2520comparing%2520the%2520accuracy%2520of%250Amodels%2520trained%2520with%2520SFFA%2520to%2520those%2520trained%2520with%2520its%2520FFA%2520counterpart.%2520As%2520a%250Abyproduct%2520of%2520this%2520reformulation%252C%2520we%2520explore%2520the%2520advantages%2520of%2520using%2520a%250Alayer-wise%2520training%2520algorithm%2520for%2520Continual%2520Learning%2520%2528CL%2529%2520tasks.%2520The%250Aspecialization%2520of%2520neurons%2520and%2520the%2520sparsity%2520of%2520their%2520activations%2520induced%2520by%250Alayer-wise%2520training%2520algorithms%2520enable%2520efficient%2520CL%2520strategies%2520that%2520incorporate%250Anew%2520knowledge%2520%2528classes%2529%2520into%2520the%2520neural%2520network%252C%2520while%2520preventing%2520catastrophic%250Aforgetting%2520of%2520previously...%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07387v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Contrastive%20Symmetric%20Forward-Forward%20Algorithm%20%28SFFA%29%20for%20Continual%0A%20%20Learning%20Tasks&entry.906535625=Erik%20B.%20Terres-Escudero%20and%20Javier%20Del%20Ser%20and%20Pablo%20Garcia%20Bringas&entry.1292438233=%20%20The%20so-called%20Forward-Forward%20Algorithm%20%28FFA%29%20has%20recently%20gained%20momentum%20as%0Aan%20alternative%20to%20the%20conventional%20back-propagation%20algorithm%20for%20neural%0Anetwork%20learning%2C%20yielding%20competitive%20performance%20across%20various%20modeling%0Atasks.%20By%20replacing%20the%20backward%20pass%20of%20gradient%20back-propagation%20with%20two%0Acontrastive%20forward%20passes%2C%20the%20FFA%20avoids%20several%20shortcomings%20undergone%20by%0Aits%20predecessor%20%28e.g.%2C%20vanishing/exploding%20gradient%29%20by%20enabling%20layer-wise%0Atraining%20heuristics.%20In%20classification%20tasks%2C%20this%20contrastive%20method%20has%20been%0Aproven%20to%20effectively%20create%20a%20latent%20sparse%20representation%20of%20the%20input%20data%2C%0Aultimately%20favoring%20discriminability.%20However%2C%20FFA%20exhibits%20an%20inherent%0Aasymmetric%20gradient%20behavior%20due%20to%20an%20imbalanced%20loss%20function%20between%0Apositive%20and%20negative%20data%2C%20adversely%20impacting%20on%20the%20model%27s%20generalization%0Acapabilities%20and%20leading%20to%20an%20accuracy%20degradation.%20To%20address%20this%20issue%2C%0Athis%20work%20proposes%20the%20Symmetric%20Forward-Forward%20Algorithm%20%28SFFA%29%2C%20a%20novel%0Amodification%20of%20the%20original%20FFA%20which%20partitions%20each%20layer%20into%20positive%20and%0Anegative%20neurons.%20This%20allows%20the%20local%20fitness%20function%20to%20be%20defined%20as%20the%0Aratio%20between%20the%20activation%20of%20positive%20neurons%20and%20the%20overall%20layer%0Aactivity%2C%20resulting%20in%20a%20symmetric%20loss%20landscape%20during%20the%20training%20phase.%20To%0Aevaluate%20the%20enhanced%20convergence%20of%20our%20method%2C%20we%20conduct%20several%20experiments%0Ausing%20multiple%20image%20classification%20benchmarks%2C%20comparing%20the%20accuracy%20of%0Amodels%20trained%20with%20SFFA%20to%20those%20trained%20with%20its%20FFA%20counterpart.%20As%20a%0Abyproduct%20of%20this%20reformulation%2C%20we%20explore%20the%20advantages%20of%20using%20a%0Alayer-wise%20training%20algorithm%20for%20Continual%20Learning%20%28CL%29%20tasks.%20The%0Aspecialization%20of%20neurons%20and%20the%20sparsity%20of%20their%20activations%20induced%20by%0Alayer-wise%20training%20algorithms%20enable%20efficient%20CL%20strategies%20that%20incorporate%0Anew%20knowledge%20%28classes%29%20into%20the%20neural%20network%2C%20while%20preventing%20catastrophic%0Aforgetting%20of%20previously...%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07387v2&entry.124074799=Read"},
{"title": "Search-o1: Agentic Search-Enhanced Large Reasoning Models", "author": "Xiaoxi Li and Guanting Dong and Jiajie Jin and Yuyao Zhang and Yujia Zhou and Yutao Zhu and Peitian Zhang and Zhicheng Dou", "abstract": "  Large reasoning models (LRMs) like OpenAI-o1 have demonstrated impressive\nlong stepwise reasoning capabilities through large-scale reinforcement\nlearning. However, their extended reasoning processes often suffer from\nknowledge insufficiency, leading to frequent uncertainties and potential\nerrors. To address this limitation, we introduce \\textbf{Search-o1}, a\nframework that enhances LRMs with an agentic retrieval-augmented generation\n(RAG) mechanism and a Reason-in-Documents module for refining retrieved\ndocuments. Search-o1 integrates an agentic search workflow into the reasoning\nprocess, enabling dynamic retrieval of external knowledge when LRMs encounter\nuncertain knowledge points. Additionally, due to the verbose nature of\nretrieved documents, we design a separate Reason-in-Documents module to deeply\nanalyze the retrieved information before injecting it into the reasoning chain,\nminimizing noise and preserving coherent reasoning flow. Extensive experiments\non complex reasoning tasks in science, mathematics, and coding, as well as six\nopen-domain QA benchmarks, demonstrate the strong performance of Search-o1.\nThis approach enhances the trustworthiness and applicability of LRMs in complex\nreasoning tasks, paving the way for more reliable and versatile intelligent\nsystems. The code is available at\n\\url{https://github.com/sunnynexus/Search-o1}.\n", "link": "http://arxiv.org/abs/2501.05366v1", "date": "2025-01-09", "relevancy": 2.0739, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5547}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5112}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5112}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Search-o1%3A%20Agentic%20Search-Enhanced%20Large%20Reasoning%20Models&body=Title%3A%20Search-o1%3A%20Agentic%20Search-Enhanced%20Large%20Reasoning%20Models%0AAuthor%3A%20Xiaoxi%20Li%20and%20Guanting%20Dong%20and%20Jiajie%20Jin%20and%20Yuyao%20Zhang%20and%20Yujia%20Zhou%20and%20Yutao%20Zhu%20and%20Peitian%20Zhang%20and%20Zhicheng%20Dou%0AAbstract%3A%20%20%20Large%20reasoning%20models%20%28LRMs%29%20like%20OpenAI-o1%20have%20demonstrated%20impressive%0Along%20stepwise%20reasoning%20capabilities%20through%20large-scale%20reinforcement%0Alearning.%20However%2C%20their%20extended%20reasoning%20processes%20often%20suffer%20from%0Aknowledge%20insufficiency%2C%20leading%20to%20frequent%20uncertainties%20and%20potential%0Aerrors.%20To%20address%20this%20limitation%2C%20we%20introduce%20%5Ctextbf%7BSearch-o1%7D%2C%20a%0Aframework%20that%20enhances%20LRMs%20with%20an%20agentic%20retrieval-augmented%20generation%0A%28RAG%29%20mechanism%20and%20a%20Reason-in-Documents%20module%20for%20refining%20retrieved%0Adocuments.%20Search-o1%20integrates%20an%20agentic%20search%20workflow%20into%20the%20reasoning%0Aprocess%2C%20enabling%20dynamic%20retrieval%20of%20external%20knowledge%20when%20LRMs%20encounter%0Auncertain%20knowledge%20points.%20Additionally%2C%20due%20to%20the%20verbose%20nature%20of%0Aretrieved%20documents%2C%20we%20design%20a%20separate%20Reason-in-Documents%20module%20to%20deeply%0Aanalyze%20the%20retrieved%20information%20before%20injecting%20it%20into%20the%20reasoning%20chain%2C%0Aminimizing%20noise%20and%20preserving%20coherent%20reasoning%20flow.%20Extensive%20experiments%0Aon%20complex%20reasoning%20tasks%20in%20science%2C%20mathematics%2C%20and%20coding%2C%20as%20well%20as%20six%0Aopen-domain%20QA%20benchmarks%2C%20demonstrate%20the%20strong%20performance%20of%20Search-o1.%0AThis%20approach%20enhances%20the%20trustworthiness%20and%20applicability%20of%20LRMs%20in%20complex%0Areasoning%20tasks%2C%20paving%20the%20way%20for%20more%20reliable%20and%20versatile%20intelligent%0Asystems.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/sunnynexus/Search-o1%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05366v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSearch-o1%253A%2520Agentic%2520Search-Enhanced%2520Large%2520Reasoning%2520Models%26entry.906535625%3DXiaoxi%2520Li%2520and%2520Guanting%2520Dong%2520and%2520Jiajie%2520Jin%2520and%2520Yuyao%2520Zhang%2520and%2520Yujia%2520Zhou%2520and%2520Yutao%2520Zhu%2520and%2520Peitian%2520Zhang%2520and%2520Zhicheng%2520Dou%26entry.1292438233%3D%2520%2520Large%2520reasoning%2520models%2520%2528LRMs%2529%2520like%2520OpenAI-o1%2520have%2520demonstrated%2520impressive%250Along%2520stepwise%2520reasoning%2520capabilities%2520through%2520large-scale%2520reinforcement%250Alearning.%2520However%252C%2520their%2520extended%2520reasoning%2520processes%2520often%2520suffer%2520from%250Aknowledge%2520insufficiency%252C%2520leading%2520to%2520frequent%2520uncertainties%2520and%2520potential%250Aerrors.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%2520%255Ctextbf%257BSearch-o1%257D%252C%2520a%250Aframework%2520that%2520enhances%2520LRMs%2520with%2520an%2520agentic%2520retrieval-augmented%2520generation%250A%2528RAG%2529%2520mechanism%2520and%2520a%2520Reason-in-Documents%2520module%2520for%2520refining%2520retrieved%250Adocuments.%2520Search-o1%2520integrates%2520an%2520agentic%2520search%2520workflow%2520into%2520the%2520reasoning%250Aprocess%252C%2520enabling%2520dynamic%2520retrieval%2520of%2520external%2520knowledge%2520when%2520LRMs%2520encounter%250Auncertain%2520knowledge%2520points.%2520Additionally%252C%2520due%2520to%2520the%2520verbose%2520nature%2520of%250Aretrieved%2520documents%252C%2520we%2520design%2520a%2520separate%2520Reason-in-Documents%2520module%2520to%2520deeply%250Aanalyze%2520the%2520retrieved%2520information%2520before%2520injecting%2520it%2520into%2520the%2520reasoning%2520chain%252C%250Aminimizing%2520noise%2520and%2520preserving%2520coherent%2520reasoning%2520flow.%2520Extensive%2520experiments%250Aon%2520complex%2520reasoning%2520tasks%2520in%2520science%252C%2520mathematics%252C%2520and%2520coding%252C%2520as%2520well%2520as%2520six%250Aopen-domain%2520QA%2520benchmarks%252C%2520demonstrate%2520the%2520strong%2520performance%2520of%2520Search-o1.%250AThis%2520approach%2520enhances%2520the%2520trustworthiness%2520and%2520applicability%2520of%2520LRMs%2520in%2520complex%250Areasoning%2520tasks%252C%2520paving%2520the%2520way%2520for%2520more%2520reliable%2520and%2520versatile%2520intelligent%250Asystems.%2520The%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/sunnynexus/Search-o1%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05366v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Search-o1%3A%20Agentic%20Search-Enhanced%20Large%20Reasoning%20Models&entry.906535625=Xiaoxi%20Li%20and%20Guanting%20Dong%20and%20Jiajie%20Jin%20and%20Yuyao%20Zhang%20and%20Yujia%20Zhou%20and%20Yutao%20Zhu%20and%20Peitian%20Zhang%20and%20Zhicheng%20Dou&entry.1292438233=%20%20Large%20reasoning%20models%20%28LRMs%29%20like%20OpenAI-o1%20have%20demonstrated%20impressive%0Along%20stepwise%20reasoning%20capabilities%20through%20large-scale%20reinforcement%0Alearning.%20However%2C%20their%20extended%20reasoning%20processes%20often%20suffer%20from%0Aknowledge%20insufficiency%2C%20leading%20to%20frequent%20uncertainties%20and%20potential%0Aerrors.%20To%20address%20this%20limitation%2C%20we%20introduce%20%5Ctextbf%7BSearch-o1%7D%2C%20a%0Aframework%20that%20enhances%20LRMs%20with%20an%20agentic%20retrieval-augmented%20generation%0A%28RAG%29%20mechanism%20and%20a%20Reason-in-Documents%20module%20for%20refining%20retrieved%0Adocuments.%20Search-o1%20integrates%20an%20agentic%20search%20workflow%20into%20the%20reasoning%0Aprocess%2C%20enabling%20dynamic%20retrieval%20of%20external%20knowledge%20when%20LRMs%20encounter%0Auncertain%20knowledge%20points.%20Additionally%2C%20due%20to%20the%20verbose%20nature%20of%0Aretrieved%20documents%2C%20we%20design%20a%20separate%20Reason-in-Documents%20module%20to%20deeply%0Aanalyze%20the%20retrieved%20information%20before%20injecting%20it%20into%20the%20reasoning%20chain%2C%0Aminimizing%20noise%20and%20preserving%20coherent%20reasoning%20flow.%20Extensive%20experiments%0Aon%20complex%20reasoning%20tasks%20in%20science%2C%20mathematics%2C%20and%20coding%2C%20as%20well%20as%20six%0Aopen-domain%20QA%20benchmarks%2C%20demonstrate%20the%20strong%20performance%20of%20Search-o1.%0AThis%20approach%20enhances%20the%20trustworthiness%20and%20applicability%20of%20LRMs%20in%20complex%0Areasoning%20tasks%2C%20paving%20the%20way%20for%20more%20reliable%20and%20versatile%20intelligent%0Asystems.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/sunnynexus/Search-o1%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05366v1&entry.124074799=Read"},
{"title": "Dexterous Manipulation of Deformable Objects via Pneumatic Gripping:\n  Lifting by One End", "author": "Roman Mykhailyshyn and Jonathan Lee and Mykhailo Mykhailyshyn and Kensuke Harada and Ann Majewicz Fey", "abstract": "  Manipulating deformable objects in robotic cells is often costly and not\nwidely accessible. However, the use of localized pneumatic gripping systems can\nenhance accessibility. Current methods that use pneumatic grippers to handle\ndeformable objects struggle with effective lifting. This paper introduces a\nmethod for the dexterous lifting of textile deformable objects from one edge,\nutilizing a previously developed gripper designed for flexible and porous\nmaterials. By precisely adjusting the orientation and position of the gripper\nduring the lifting process, we were able to significantly reduce necessary\ngripping force and minimize object vibration caused by airflow. This method was\ntested and validated on four materials with varying mass, friction, and\nflexibility. The proposed approach facilitates the lifting of deformable\nobjects from a conveyor or automated line, even when only one edge is\naccessible for grasping. Future work will involve integrating a vision system\nto optimize the manipulation of deformable objects with more complex shapes.\n", "link": "http://arxiv.org/abs/2501.05198v1", "date": "2025-01-09", "relevancy": 2.0652, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5524}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5056}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dexterous%20Manipulation%20of%20Deformable%20Objects%20via%20Pneumatic%20Gripping%3A%0A%20%20Lifting%20by%20One%20End&body=Title%3A%20Dexterous%20Manipulation%20of%20Deformable%20Objects%20via%20Pneumatic%20Gripping%3A%0A%20%20Lifting%20by%20One%20End%0AAuthor%3A%20Roman%20Mykhailyshyn%20and%20Jonathan%20Lee%20and%20Mykhailo%20Mykhailyshyn%20and%20Kensuke%20Harada%20and%20Ann%20Majewicz%20Fey%0AAbstract%3A%20%20%20Manipulating%20deformable%20objects%20in%20robotic%20cells%20is%20often%20costly%20and%20not%0Awidely%20accessible.%20However%2C%20the%20use%20of%20localized%20pneumatic%20gripping%20systems%20can%0Aenhance%20accessibility.%20Current%20methods%20that%20use%20pneumatic%20grippers%20to%20handle%0Adeformable%20objects%20struggle%20with%20effective%20lifting.%20This%20paper%20introduces%20a%0Amethod%20for%20the%20dexterous%20lifting%20of%20textile%20deformable%20objects%20from%20one%20edge%2C%0Autilizing%20a%20previously%20developed%20gripper%20designed%20for%20flexible%20and%20porous%0Amaterials.%20By%20precisely%20adjusting%20the%20orientation%20and%20position%20of%20the%20gripper%0Aduring%20the%20lifting%20process%2C%20we%20were%20able%20to%20significantly%20reduce%20necessary%0Agripping%20force%20and%20minimize%20object%20vibration%20caused%20by%20airflow.%20This%20method%20was%0Atested%20and%20validated%20on%20four%20materials%20with%20varying%20mass%2C%20friction%2C%20and%0Aflexibility.%20The%20proposed%20approach%20facilitates%20the%20lifting%20of%20deformable%0Aobjects%20from%20a%20conveyor%20or%20automated%20line%2C%20even%20when%20only%20one%20edge%20is%0Aaccessible%20for%20grasping.%20Future%20work%20will%20involve%20integrating%20a%20vision%20system%0Ato%20optimize%20the%20manipulation%20of%20deformable%20objects%20with%20more%20complex%20shapes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05198v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDexterous%2520Manipulation%2520of%2520Deformable%2520Objects%2520via%2520Pneumatic%2520Gripping%253A%250A%2520%2520Lifting%2520by%2520One%2520End%26entry.906535625%3DRoman%2520Mykhailyshyn%2520and%2520Jonathan%2520Lee%2520and%2520Mykhailo%2520Mykhailyshyn%2520and%2520Kensuke%2520Harada%2520and%2520Ann%2520Majewicz%2520Fey%26entry.1292438233%3D%2520%2520Manipulating%2520deformable%2520objects%2520in%2520robotic%2520cells%2520is%2520often%2520costly%2520and%2520not%250Awidely%2520accessible.%2520However%252C%2520the%2520use%2520of%2520localized%2520pneumatic%2520gripping%2520systems%2520can%250Aenhance%2520accessibility.%2520Current%2520methods%2520that%2520use%2520pneumatic%2520grippers%2520to%2520handle%250Adeformable%2520objects%2520struggle%2520with%2520effective%2520lifting.%2520This%2520paper%2520introduces%2520a%250Amethod%2520for%2520the%2520dexterous%2520lifting%2520of%2520textile%2520deformable%2520objects%2520from%2520one%2520edge%252C%250Autilizing%2520a%2520previously%2520developed%2520gripper%2520designed%2520for%2520flexible%2520and%2520porous%250Amaterials.%2520By%2520precisely%2520adjusting%2520the%2520orientation%2520and%2520position%2520of%2520the%2520gripper%250Aduring%2520the%2520lifting%2520process%252C%2520we%2520were%2520able%2520to%2520significantly%2520reduce%2520necessary%250Agripping%2520force%2520and%2520minimize%2520object%2520vibration%2520caused%2520by%2520airflow.%2520This%2520method%2520was%250Atested%2520and%2520validated%2520on%2520four%2520materials%2520with%2520varying%2520mass%252C%2520friction%252C%2520and%250Aflexibility.%2520The%2520proposed%2520approach%2520facilitates%2520the%2520lifting%2520of%2520deformable%250Aobjects%2520from%2520a%2520conveyor%2520or%2520automated%2520line%252C%2520even%2520when%2520only%2520one%2520edge%2520is%250Aaccessible%2520for%2520grasping.%2520Future%2520work%2520will%2520involve%2520integrating%2520a%2520vision%2520system%250Ato%2520optimize%2520the%2520manipulation%2520of%2520deformable%2520objects%2520with%2520more%2520complex%2520shapes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05198v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dexterous%20Manipulation%20of%20Deformable%20Objects%20via%20Pneumatic%20Gripping%3A%0A%20%20Lifting%20by%20One%20End&entry.906535625=Roman%20Mykhailyshyn%20and%20Jonathan%20Lee%20and%20Mykhailo%20Mykhailyshyn%20and%20Kensuke%20Harada%20and%20Ann%20Majewicz%20Fey&entry.1292438233=%20%20Manipulating%20deformable%20objects%20in%20robotic%20cells%20is%20often%20costly%20and%20not%0Awidely%20accessible.%20However%2C%20the%20use%20of%20localized%20pneumatic%20gripping%20systems%20can%0Aenhance%20accessibility.%20Current%20methods%20that%20use%20pneumatic%20grippers%20to%20handle%0Adeformable%20objects%20struggle%20with%20effective%20lifting.%20This%20paper%20introduces%20a%0Amethod%20for%20the%20dexterous%20lifting%20of%20textile%20deformable%20objects%20from%20one%20edge%2C%0Autilizing%20a%20previously%20developed%20gripper%20designed%20for%20flexible%20and%20porous%0Amaterials.%20By%20precisely%20adjusting%20the%20orientation%20and%20position%20of%20the%20gripper%0Aduring%20the%20lifting%20process%2C%20we%20were%20able%20to%20significantly%20reduce%20necessary%0Agripping%20force%20and%20minimize%20object%20vibration%20caused%20by%20airflow.%20This%20method%20was%0Atested%20and%20validated%20on%20four%20materials%20with%20varying%20mass%2C%20friction%2C%20and%0Aflexibility.%20The%20proposed%20approach%20facilitates%20the%20lifting%20of%20deformable%0Aobjects%20from%20a%20conveyor%20or%20automated%20line%2C%20even%20when%20only%20one%20edge%20is%0Aaccessible%20for%20grasping.%20Future%20work%20will%20involve%20integrating%20a%20vision%20system%0Ato%20optimize%20the%20manipulation%20of%20deformable%20objects%20with%20more%20complex%20shapes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05198v1&entry.124074799=Read"},
{"title": "EquiBoost: An Equivariant Boosting Approach to Molecular Conformation\n  Generation", "author": "Yixuan Yang and Xingyu Fang and Zhaowen Cheng and Pengju Yan and Xiaolin Li", "abstract": "  Molecular conformation generation plays key roles in computational drug\ndesign. Recently developed deep learning methods, particularly diffusion models\nhave reached competitive performance over traditional cheminformatical\napproaches. However, these methods are often time-consuming or require extra\nsupport from traditional methods. We propose EquiBoost, a boosting model that\nstacks several equivariant graph transformers as weak learners, to iteratively\nrefine 3D conformations of molecules. Without relying on diffusion techniques,\nEquiBoost balances accuracy and efficiency more effectively than\ndiffusion-based methods. Notably, compared to the previous state-of-the-art\ndiffusion method, EquiBoost improves generation quality and preserves\ndiversity, achieving considerably better precision of Average Minimum RMSD\n(AMR) on the GEOM datasets. This work rejuvenates boosting and sheds light on\nits potential to be a robust alternative to diffusion models in certain\nscenarios.\n", "link": "http://arxiv.org/abs/2501.05109v1", "date": "2025-01-09", "relevancy": 2.0643, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5314}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5163}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EquiBoost%3A%20An%20Equivariant%20Boosting%20Approach%20to%20Molecular%20Conformation%0A%20%20Generation&body=Title%3A%20EquiBoost%3A%20An%20Equivariant%20Boosting%20Approach%20to%20Molecular%20Conformation%0A%20%20Generation%0AAuthor%3A%20Yixuan%20Yang%20and%20Xingyu%20Fang%20and%20Zhaowen%20Cheng%20and%20Pengju%20Yan%20and%20Xiaolin%20Li%0AAbstract%3A%20%20%20Molecular%20conformation%20generation%20plays%20key%20roles%20in%20computational%20drug%0Adesign.%20Recently%20developed%20deep%20learning%20methods%2C%20particularly%20diffusion%20models%0Ahave%20reached%20competitive%20performance%20over%20traditional%20cheminformatical%0Aapproaches.%20However%2C%20these%20methods%20are%20often%20time-consuming%20or%20require%20extra%0Asupport%20from%20traditional%20methods.%20We%20propose%20EquiBoost%2C%20a%20boosting%20model%20that%0Astacks%20several%20equivariant%20graph%20transformers%20as%20weak%20learners%2C%20to%20iteratively%0Arefine%203D%20conformations%20of%20molecules.%20Without%20relying%20on%20diffusion%20techniques%2C%0AEquiBoost%20balances%20accuracy%20and%20efficiency%20more%20effectively%20than%0Adiffusion-based%20methods.%20Notably%2C%20compared%20to%20the%20previous%20state-of-the-art%0Adiffusion%20method%2C%20EquiBoost%20improves%20generation%20quality%20and%20preserves%0Adiversity%2C%20achieving%20considerably%20better%20precision%20of%20Average%20Minimum%20RMSD%0A%28AMR%29%20on%20the%20GEOM%20datasets.%20This%20work%20rejuvenates%20boosting%20and%20sheds%20light%20on%0Aits%20potential%20to%20be%20a%20robust%20alternative%20to%20diffusion%20models%20in%20certain%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05109v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquiBoost%253A%2520An%2520Equivariant%2520Boosting%2520Approach%2520to%2520Molecular%2520Conformation%250A%2520%2520Generation%26entry.906535625%3DYixuan%2520Yang%2520and%2520Xingyu%2520Fang%2520and%2520Zhaowen%2520Cheng%2520and%2520Pengju%2520Yan%2520and%2520Xiaolin%2520Li%26entry.1292438233%3D%2520%2520Molecular%2520conformation%2520generation%2520plays%2520key%2520roles%2520in%2520computational%2520drug%250Adesign.%2520Recently%2520developed%2520deep%2520learning%2520methods%252C%2520particularly%2520diffusion%2520models%250Ahave%2520reached%2520competitive%2520performance%2520over%2520traditional%2520cheminformatical%250Aapproaches.%2520However%252C%2520these%2520methods%2520are%2520often%2520time-consuming%2520or%2520require%2520extra%250Asupport%2520from%2520traditional%2520methods.%2520We%2520propose%2520EquiBoost%252C%2520a%2520boosting%2520model%2520that%250Astacks%2520several%2520equivariant%2520graph%2520transformers%2520as%2520weak%2520learners%252C%2520to%2520iteratively%250Arefine%25203D%2520conformations%2520of%2520molecules.%2520Without%2520relying%2520on%2520diffusion%2520techniques%252C%250AEquiBoost%2520balances%2520accuracy%2520and%2520efficiency%2520more%2520effectively%2520than%250Adiffusion-based%2520methods.%2520Notably%252C%2520compared%2520to%2520the%2520previous%2520state-of-the-art%250Adiffusion%2520method%252C%2520EquiBoost%2520improves%2520generation%2520quality%2520and%2520preserves%250Adiversity%252C%2520achieving%2520considerably%2520better%2520precision%2520of%2520Average%2520Minimum%2520RMSD%250A%2528AMR%2529%2520on%2520the%2520GEOM%2520datasets.%2520This%2520work%2520rejuvenates%2520boosting%2520and%2520sheds%2520light%2520on%250Aits%2520potential%2520to%2520be%2520a%2520robust%2520alternative%2520to%2520diffusion%2520models%2520in%2520certain%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05109v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EquiBoost%3A%20An%20Equivariant%20Boosting%20Approach%20to%20Molecular%20Conformation%0A%20%20Generation&entry.906535625=Yixuan%20Yang%20and%20Xingyu%20Fang%20and%20Zhaowen%20Cheng%20and%20Pengju%20Yan%20and%20Xiaolin%20Li&entry.1292438233=%20%20Molecular%20conformation%20generation%20plays%20key%20roles%20in%20computational%20drug%0Adesign.%20Recently%20developed%20deep%20learning%20methods%2C%20particularly%20diffusion%20models%0Ahave%20reached%20competitive%20performance%20over%20traditional%20cheminformatical%0Aapproaches.%20However%2C%20these%20methods%20are%20often%20time-consuming%20or%20require%20extra%0Asupport%20from%20traditional%20methods.%20We%20propose%20EquiBoost%2C%20a%20boosting%20model%20that%0Astacks%20several%20equivariant%20graph%20transformers%20as%20weak%20learners%2C%20to%20iteratively%0Arefine%203D%20conformations%20of%20molecules.%20Without%20relying%20on%20diffusion%20techniques%2C%0AEquiBoost%20balances%20accuracy%20and%20efficiency%20more%20effectively%20than%0Adiffusion-based%20methods.%20Notably%2C%20compared%20to%20the%20previous%20state-of-the-art%0Adiffusion%20method%2C%20EquiBoost%20improves%20generation%20quality%20and%20preserves%0Adiversity%2C%20achieving%20considerably%20better%20precision%20of%20Average%20Minimum%20RMSD%0A%28AMR%29%20on%20the%20GEOM%20datasets.%20This%20work%20rejuvenates%20boosting%20and%20sheds%20light%20on%0Aits%20potential%20to%20be%20a%20robust%20alternative%20to%20diffusion%20models%20in%20certain%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05109v1&entry.124074799=Read"},
{"title": "Attention Mechanisms Don't Learn Additive Models: Rethinking Feature\n  Importance for Transformers", "author": "Tobias Leemann and Alina Fastowski and Felix Pfeiffer and Gjergji Kasneci", "abstract": "  We address the critical challenge of applying feature attribution methods to\nthe transformer architecture, which dominates current applications in natural\nlanguage processing and beyond. Traditional attribution methods to explainable\nAI (XAI) explicitly or implicitly rely on linear or additive surrogate models\nto quantify the impact of input features on a model's output. In this work, we\nformally prove an alarming incompatibility: transformers are structurally\nincapable of representing linear or additive surrogate models used for feature\nattribution, undermining the grounding of these conventional explanation\nmethodologies. To address this discrepancy, we introduce the Softmax-Linked\nAdditive Log Odds Model (SLALOM), a novel surrogate model specifically designed\nto align with the transformer framework. SLALOM demonstrates the capacity to\ndeliver a range of insightful explanations with both synthetic and real-world\ndatasets. We highlight SLALOM's unique efficiency-quality curve by showing that\nSLALOM can produce explanations with substantially higher fidelity than\ncompeting surrogate models or provide explanations of comparable quality at a\nfraction of their computational costs. We release code for SLALOM as an\nopen-source project online at https://github.com/tleemann/slalom_explanations.\n", "link": "http://arxiv.org/abs/2405.13536v2", "date": "2025-01-09", "relevancy": 2.0605, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5495}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5225}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention%20Mechanisms%20Don%27t%20Learn%20Additive%20Models%3A%20Rethinking%20Feature%0A%20%20Importance%20for%20Transformers&body=Title%3A%20Attention%20Mechanisms%20Don%27t%20Learn%20Additive%20Models%3A%20Rethinking%20Feature%0A%20%20Importance%20for%20Transformers%0AAuthor%3A%20Tobias%20Leemann%20and%20Alina%20Fastowski%20and%20Felix%20Pfeiffer%20and%20Gjergji%20Kasneci%0AAbstract%3A%20%20%20We%20address%20the%20critical%20challenge%20of%20applying%20feature%20attribution%20methods%20to%0Athe%20transformer%20architecture%2C%20which%20dominates%20current%20applications%20in%20natural%0Alanguage%20processing%20and%20beyond.%20Traditional%20attribution%20methods%20to%20explainable%0AAI%20%28XAI%29%20explicitly%20or%20implicitly%20rely%20on%20linear%20or%20additive%20surrogate%20models%0Ato%20quantify%20the%20impact%20of%20input%20features%20on%20a%20model%27s%20output.%20In%20this%20work%2C%20we%0Aformally%20prove%20an%20alarming%20incompatibility%3A%20transformers%20are%20structurally%0Aincapable%20of%20representing%20linear%20or%20additive%20surrogate%20models%20used%20for%20feature%0Aattribution%2C%20undermining%20the%20grounding%20of%20these%20conventional%20explanation%0Amethodologies.%20To%20address%20this%20discrepancy%2C%20we%20introduce%20the%20Softmax-Linked%0AAdditive%20Log%20Odds%20Model%20%28SLALOM%29%2C%20a%20novel%20surrogate%20model%20specifically%20designed%0Ato%20align%20with%20the%20transformer%20framework.%20SLALOM%20demonstrates%20the%20capacity%20to%0Adeliver%20a%20range%20of%20insightful%20explanations%20with%20both%20synthetic%20and%20real-world%0Adatasets.%20We%20highlight%20SLALOM%27s%20unique%20efficiency-quality%20curve%20by%20showing%20that%0ASLALOM%20can%20produce%20explanations%20with%20substantially%20higher%20fidelity%20than%0Acompeting%20surrogate%20models%20or%20provide%20explanations%20of%20comparable%20quality%20at%20a%0Afraction%20of%20their%20computational%20costs.%20We%20release%20code%20for%20SLALOM%20as%20an%0Aopen-source%20project%20online%20at%20https%3A//github.com/tleemann/slalom_explanations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13536v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention%2520Mechanisms%2520Don%2527t%2520Learn%2520Additive%2520Models%253A%2520Rethinking%2520Feature%250A%2520%2520Importance%2520for%2520Transformers%26entry.906535625%3DTobias%2520Leemann%2520and%2520Alina%2520Fastowski%2520and%2520Felix%2520Pfeiffer%2520and%2520Gjergji%2520Kasneci%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520critical%2520challenge%2520of%2520applying%2520feature%2520attribution%2520methods%2520to%250Athe%2520transformer%2520architecture%252C%2520which%2520dominates%2520current%2520applications%2520in%2520natural%250Alanguage%2520processing%2520and%2520beyond.%2520Traditional%2520attribution%2520methods%2520to%2520explainable%250AAI%2520%2528XAI%2529%2520explicitly%2520or%2520implicitly%2520rely%2520on%2520linear%2520or%2520additive%2520surrogate%2520models%250Ato%2520quantify%2520the%2520impact%2520of%2520input%2520features%2520on%2520a%2520model%2527s%2520output.%2520In%2520this%2520work%252C%2520we%250Aformally%2520prove%2520an%2520alarming%2520incompatibility%253A%2520transformers%2520are%2520structurally%250Aincapable%2520of%2520representing%2520linear%2520or%2520additive%2520surrogate%2520models%2520used%2520for%2520feature%250Aattribution%252C%2520undermining%2520the%2520grounding%2520of%2520these%2520conventional%2520explanation%250Amethodologies.%2520To%2520address%2520this%2520discrepancy%252C%2520we%2520introduce%2520the%2520Softmax-Linked%250AAdditive%2520Log%2520Odds%2520Model%2520%2528SLALOM%2529%252C%2520a%2520novel%2520surrogate%2520model%2520specifically%2520designed%250Ato%2520align%2520with%2520the%2520transformer%2520framework.%2520SLALOM%2520demonstrates%2520the%2520capacity%2520to%250Adeliver%2520a%2520range%2520of%2520insightful%2520explanations%2520with%2520both%2520synthetic%2520and%2520real-world%250Adatasets.%2520We%2520highlight%2520SLALOM%2527s%2520unique%2520efficiency-quality%2520curve%2520by%2520showing%2520that%250ASLALOM%2520can%2520produce%2520explanations%2520with%2520substantially%2520higher%2520fidelity%2520than%250Acompeting%2520surrogate%2520models%2520or%2520provide%2520explanations%2520of%2520comparable%2520quality%2520at%2520a%250Afraction%2520of%2520their%2520computational%2520costs.%2520We%2520release%2520code%2520for%2520SLALOM%2520as%2520an%250Aopen-source%2520project%2520online%2520at%2520https%253A//github.com/tleemann/slalom_explanations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13536v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20Mechanisms%20Don%27t%20Learn%20Additive%20Models%3A%20Rethinking%20Feature%0A%20%20Importance%20for%20Transformers&entry.906535625=Tobias%20Leemann%20and%20Alina%20Fastowski%20and%20Felix%20Pfeiffer%20and%20Gjergji%20Kasneci&entry.1292438233=%20%20We%20address%20the%20critical%20challenge%20of%20applying%20feature%20attribution%20methods%20to%0Athe%20transformer%20architecture%2C%20which%20dominates%20current%20applications%20in%20natural%0Alanguage%20processing%20and%20beyond.%20Traditional%20attribution%20methods%20to%20explainable%0AAI%20%28XAI%29%20explicitly%20or%20implicitly%20rely%20on%20linear%20or%20additive%20surrogate%20models%0Ato%20quantify%20the%20impact%20of%20input%20features%20on%20a%20model%27s%20output.%20In%20this%20work%2C%20we%0Aformally%20prove%20an%20alarming%20incompatibility%3A%20transformers%20are%20structurally%0Aincapable%20of%20representing%20linear%20or%20additive%20surrogate%20models%20used%20for%20feature%0Aattribution%2C%20undermining%20the%20grounding%20of%20these%20conventional%20explanation%0Amethodologies.%20To%20address%20this%20discrepancy%2C%20we%20introduce%20the%20Softmax-Linked%0AAdditive%20Log%20Odds%20Model%20%28SLALOM%29%2C%20a%20novel%20surrogate%20model%20specifically%20designed%0Ato%20align%20with%20the%20transformer%20framework.%20SLALOM%20demonstrates%20the%20capacity%20to%0Adeliver%20a%20range%20of%20insightful%20explanations%20with%20both%20synthetic%20and%20real-world%0Adatasets.%20We%20highlight%20SLALOM%27s%20unique%20efficiency-quality%20curve%20by%20showing%20that%0ASLALOM%20can%20produce%20explanations%20with%20substantially%20higher%20fidelity%20than%0Acompeting%20surrogate%20models%20or%20provide%20explanations%20of%20comparable%20quality%20at%20a%0Afraction%20of%20their%20computational%20costs.%20We%20release%20code%20for%20SLALOM%20as%20an%0Aopen-source%20project%20online%20at%20https%3A//github.com/tleemann/slalom_explanations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13536v2&entry.124074799=Read"},
{"title": "Improving the U-Net Configuration for Automated Delineation of Head and\n  Neck Cancer on MRI", "author": "Andrei Iantsen", "abstract": "  Tumor volume segmentation on MRI is a challenging and time-consuming process\nthat is performed manually in typical clinical settings. This work presents an\napproach to automated delineation of head and neck tumors on MRI scans,\ndeveloped in the context of the MICCAI Head and Neck Tumor Segmentation for\nMR-Guided Applications (HNTS-MRG) 2024 Challenge. Rather than designing a new,\ntask-specific convolutional neural network, the focus of this research was to\npropose improvements to the configuration commonly used in medical segmentation\ntasks, relying solely on the traditional U-Net architecture. The empirical\nresults presented in this article suggest the superiority of patch-wise\nnormalization used for both training and sliding window inference. They also\nindicate that the performance of segmentation models can be enhanced by\napplying a scheduled data augmentation policy during training. Finally, it is\nshown that a small improvement in quality can be achieved by using Gaussian\nweighting to combine predictions for individual patches during sliding window\ninference. The model with the best configuration obtained an aggregated Dice\nSimilarity Coefficient (DSCagg) of 0.749 in Task 1 and 0.710 in Task 2 on five\ncross-validation folds. The ensemble of five models (one best model per\nvalidation fold) showed consistent results on a private test set of 50 patients\nwith an DSCagg of 0.752 in Task 1 and 0.718 in Task 2 (team name:\nandrei.iantsen). The source code and model weights are freely available at\nwww.github.com/iantsen/hntsmrg.\n", "link": "http://arxiv.org/abs/2501.05120v1", "date": "2025-01-09", "relevancy": 2.0572, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5217}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5182}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20the%20U-Net%20Configuration%20for%20Automated%20Delineation%20of%20Head%20and%0A%20%20Neck%20Cancer%20on%20MRI&body=Title%3A%20Improving%20the%20U-Net%20Configuration%20for%20Automated%20Delineation%20of%20Head%20and%0A%20%20Neck%20Cancer%20on%20MRI%0AAuthor%3A%20Andrei%20Iantsen%0AAbstract%3A%20%20%20Tumor%20volume%20segmentation%20on%20MRI%20is%20a%20challenging%20and%20time-consuming%20process%0Athat%20is%20performed%20manually%20in%20typical%20clinical%20settings.%20This%20work%20presents%20an%0Aapproach%20to%20automated%20delineation%20of%20head%20and%20neck%20tumors%20on%20MRI%20scans%2C%0Adeveloped%20in%20the%20context%20of%20the%20MICCAI%20Head%20and%20Neck%20Tumor%20Segmentation%20for%0AMR-Guided%20Applications%20%28HNTS-MRG%29%202024%20Challenge.%20Rather%20than%20designing%20a%20new%2C%0Atask-specific%20convolutional%20neural%20network%2C%20the%20focus%20of%20this%20research%20was%20to%0Apropose%20improvements%20to%20the%20configuration%20commonly%20used%20in%20medical%20segmentation%0Atasks%2C%20relying%20solely%20on%20the%20traditional%20U-Net%20architecture.%20The%20empirical%0Aresults%20presented%20in%20this%20article%20suggest%20the%20superiority%20of%20patch-wise%0Anormalization%20used%20for%20both%20training%20and%20sliding%20window%20inference.%20They%20also%0Aindicate%20that%20the%20performance%20of%20segmentation%20models%20can%20be%20enhanced%20by%0Aapplying%20a%20scheduled%20data%20augmentation%20policy%20during%20training.%20Finally%2C%20it%20is%0Ashown%20that%20a%20small%20improvement%20in%20quality%20can%20be%20achieved%20by%20using%20Gaussian%0Aweighting%20to%20combine%20predictions%20for%20individual%20patches%20during%20sliding%20window%0Ainference.%20The%20model%20with%20the%20best%20configuration%20obtained%20an%20aggregated%20Dice%0ASimilarity%20Coefficient%20%28DSCagg%29%20of%200.749%20in%20Task%201%20and%200.710%20in%20Task%202%20on%20five%0Across-validation%20folds.%20The%20ensemble%20of%20five%20models%20%28one%20best%20model%20per%0Avalidation%20fold%29%20showed%20consistent%20results%20on%20a%20private%20test%20set%20of%2050%20patients%0Awith%20an%20DSCagg%20of%200.752%20in%20Task%201%20and%200.718%20in%20Task%202%20%28team%20name%3A%0Aandrei.iantsen%29.%20The%20source%20code%20and%20model%20weights%20are%20freely%20available%20at%0Awww.github.com/iantsen/hntsmrg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05120v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520the%2520U-Net%2520Configuration%2520for%2520Automated%2520Delineation%2520of%2520Head%2520and%250A%2520%2520Neck%2520Cancer%2520on%2520MRI%26entry.906535625%3DAndrei%2520Iantsen%26entry.1292438233%3D%2520%2520Tumor%2520volume%2520segmentation%2520on%2520MRI%2520is%2520a%2520challenging%2520and%2520time-consuming%2520process%250Athat%2520is%2520performed%2520manually%2520in%2520typical%2520clinical%2520settings.%2520This%2520work%2520presents%2520an%250Aapproach%2520to%2520automated%2520delineation%2520of%2520head%2520and%2520neck%2520tumors%2520on%2520MRI%2520scans%252C%250Adeveloped%2520in%2520the%2520context%2520of%2520the%2520MICCAI%2520Head%2520and%2520Neck%2520Tumor%2520Segmentation%2520for%250AMR-Guided%2520Applications%2520%2528HNTS-MRG%2529%25202024%2520Challenge.%2520Rather%2520than%2520designing%2520a%2520new%252C%250Atask-specific%2520convolutional%2520neural%2520network%252C%2520the%2520focus%2520of%2520this%2520research%2520was%2520to%250Apropose%2520improvements%2520to%2520the%2520configuration%2520commonly%2520used%2520in%2520medical%2520segmentation%250Atasks%252C%2520relying%2520solely%2520on%2520the%2520traditional%2520U-Net%2520architecture.%2520The%2520empirical%250Aresults%2520presented%2520in%2520this%2520article%2520suggest%2520the%2520superiority%2520of%2520patch-wise%250Anormalization%2520used%2520for%2520both%2520training%2520and%2520sliding%2520window%2520inference.%2520They%2520also%250Aindicate%2520that%2520the%2520performance%2520of%2520segmentation%2520models%2520can%2520be%2520enhanced%2520by%250Aapplying%2520a%2520scheduled%2520data%2520augmentation%2520policy%2520during%2520training.%2520Finally%252C%2520it%2520is%250Ashown%2520that%2520a%2520small%2520improvement%2520in%2520quality%2520can%2520be%2520achieved%2520by%2520using%2520Gaussian%250Aweighting%2520to%2520combine%2520predictions%2520for%2520individual%2520patches%2520during%2520sliding%2520window%250Ainference.%2520The%2520model%2520with%2520the%2520best%2520configuration%2520obtained%2520an%2520aggregated%2520Dice%250ASimilarity%2520Coefficient%2520%2528DSCagg%2529%2520of%25200.749%2520in%2520Task%25201%2520and%25200.710%2520in%2520Task%25202%2520on%2520five%250Across-validation%2520folds.%2520The%2520ensemble%2520of%2520five%2520models%2520%2528one%2520best%2520model%2520per%250Avalidation%2520fold%2529%2520showed%2520consistent%2520results%2520on%2520a%2520private%2520test%2520set%2520of%252050%2520patients%250Awith%2520an%2520DSCagg%2520of%25200.752%2520in%2520Task%25201%2520and%25200.718%2520in%2520Task%25202%2520%2528team%2520name%253A%250Aandrei.iantsen%2529.%2520The%2520source%2520code%2520and%2520model%2520weights%2520are%2520freely%2520available%2520at%250Awww.github.com/iantsen/hntsmrg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05120v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20the%20U-Net%20Configuration%20for%20Automated%20Delineation%20of%20Head%20and%0A%20%20Neck%20Cancer%20on%20MRI&entry.906535625=Andrei%20Iantsen&entry.1292438233=%20%20Tumor%20volume%20segmentation%20on%20MRI%20is%20a%20challenging%20and%20time-consuming%20process%0Athat%20is%20performed%20manually%20in%20typical%20clinical%20settings.%20This%20work%20presents%20an%0Aapproach%20to%20automated%20delineation%20of%20head%20and%20neck%20tumors%20on%20MRI%20scans%2C%0Adeveloped%20in%20the%20context%20of%20the%20MICCAI%20Head%20and%20Neck%20Tumor%20Segmentation%20for%0AMR-Guided%20Applications%20%28HNTS-MRG%29%202024%20Challenge.%20Rather%20than%20designing%20a%20new%2C%0Atask-specific%20convolutional%20neural%20network%2C%20the%20focus%20of%20this%20research%20was%20to%0Apropose%20improvements%20to%20the%20configuration%20commonly%20used%20in%20medical%20segmentation%0Atasks%2C%20relying%20solely%20on%20the%20traditional%20U-Net%20architecture.%20The%20empirical%0Aresults%20presented%20in%20this%20article%20suggest%20the%20superiority%20of%20patch-wise%0Anormalization%20used%20for%20both%20training%20and%20sliding%20window%20inference.%20They%20also%0Aindicate%20that%20the%20performance%20of%20segmentation%20models%20can%20be%20enhanced%20by%0Aapplying%20a%20scheduled%20data%20augmentation%20policy%20during%20training.%20Finally%2C%20it%20is%0Ashown%20that%20a%20small%20improvement%20in%20quality%20can%20be%20achieved%20by%20using%20Gaussian%0Aweighting%20to%20combine%20predictions%20for%20individual%20patches%20during%20sliding%20window%0Ainference.%20The%20model%20with%20the%20best%20configuration%20obtained%20an%20aggregated%20Dice%0ASimilarity%20Coefficient%20%28DSCagg%29%20of%200.749%20in%20Task%201%20and%200.710%20in%20Task%202%20on%20five%0Across-validation%20folds.%20The%20ensemble%20of%20five%20models%20%28one%20best%20model%20per%0Avalidation%20fold%29%20showed%20consistent%20results%20on%20a%20private%20test%20set%20of%2050%20patients%0Awith%20an%20DSCagg%20of%200.752%20in%20Task%201%20and%200.718%20in%20Task%202%20%28team%20name%3A%0Aandrei.iantsen%29.%20The%20source%20code%20and%20model%20weights%20are%20freely%20available%20at%0Awww.github.com/iantsen/hntsmrg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05120v1&entry.124074799=Read"},
{"title": "Representation Learning of Lab Values via Masked AutoEncoder", "author": "David Restrepo and Chenwei Wu and Yueran Jia and Jaden K. Sun and Jack Gallifant and Catherine G. Bielick and Yugang Jia and Leo A. Celi", "abstract": "  Accurate imputation of missing laboratory values in electronic health records\n(EHRs) is critical to enable robust clinical predictions and reduce biases in\nAI systems in healthcare. Existing methods, such as variational autoencoders\n(VAEs) and decision tree-based approaches such as XGBoost, struggle to model\nthe complex temporal and contextual dependencies in EHR data, mainly in\nunderrepresented groups. In this work, we propose Lab-MAE, a novel\ntransformer-based masked autoencoder framework that leverages self-supervised\nlearning for the imputation of continuous sequential lab values. Lab-MAE\nintroduces a structured encoding scheme that jointly models laboratory test\nvalues and their corresponding timestamps, enabling explicit capturing temporal\ndependencies. Empirical evaluation on the MIMIC-IV dataset demonstrates that\nLab-MAE significantly outperforms the state-of-the-art baselines such as\nXGBoost across multiple metrics, including root mean square error (RMSE),\nR-squared (R2), and Wasserstein distance (WD). Notably, Lab-MAE achieves\nequitable performance across demographic groups of patients, advancing fairness\nin clinical predictions. We further investigate the role of follow-up\nlaboratory values as potential shortcut features, revealing Lab-MAE's\nrobustness in scenarios where such data is unavailable. The findings suggest\nthat our transformer-based architecture, adapted to the characteristics of the\nEHR data, offers a foundation model for more accurate and fair clinical\nimputation models. In addition, we measure and compare the carbon footprint of\nLab-MAE with the baseline XGBoost model, highlighting its environmental\nrequirements.\n", "link": "http://arxiv.org/abs/2501.02648v2", "date": "2025-01-09", "relevancy": 2.0554, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5466}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5429}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Representation%20Learning%20of%20Lab%20Values%20via%20Masked%20AutoEncoder&body=Title%3A%20Representation%20Learning%20of%20Lab%20Values%20via%20Masked%20AutoEncoder%0AAuthor%3A%20David%20Restrepo%20and%20Chenwei%20Wu%20and%20Yueran%20Jia%20and%20Jaden%20K.%20Sun%20and%20Jack%20Gallifant%20and%20Catherine%20G.%20Bielick%20and%20Yugang%20Jia%20and%20Leo%20A.%20Celi%0AAbstract%3A%20%20%20Accurate%20imputation%20of%20missing%20laboratory%20values%20in%20electronic%20health%20records%0A%28EHRs%29%20is%20critical%20to%20enable%20robust%20clinical%20predictions%20and%20reduce%20biases%20in%0AAI%20systems%20in%20healthcare.%20Existing%20methods%2C%20such%20as%20variational%20autoencoders%0A%28VAEs%29%20and%20decision%20tree-based%20approaches%20such%20as%20XGBoost%2C%20struggle%20to%20model%0Athe%20complex%20temporal%20and%20contextual%20dependencies%20in%20EHR%20data%2C%20mainly%20in%0Aunderrepresented%20groups.%20In%20this%20work%2C%20we%20propose%20Lab-MAE%2C%20a%20novel%0Atransformer-based%20masked%20autoencoder%20framework%20that%20leverages%20self-supervised%0Alearning%20for%20the%20imputation%20of%20continuous%20sequential%20lab%20values.%20Lab-MAE%0Aintroduces%20a%20structured%20encoding%20scheme%20that%20jointly%20models%20laboratory%20test%0Avalues%20and%20their%20corresponding%20timestamps%2C%20enabling%20explicit%20capturing%20temporal%0Adependencies.%20Empirical%20evaluation%20on%20the%20MIMIC-IV%20dataset%20demonstrates%20that%0ALab-MAE%20significantly%20outperforms%20the%20state-of-the-art%20baselines%20such%20as%0AXGBoost%20across%20multiple%20metrics%2C%20including%20root%20mean%20square%20error%20%28RMSE%29%2C%0AR-squared%20%28R2%29%2C%20and%20Wasserstein%20distance%20%28WD%29.%20Notably%2C%20Lab-MAE%20achieves%0Aequitable%20performance%20across%20demographic%20groups%20of%20patients%2C%20advancing%20fairness%0Ain%20clinical%20predictions.%20We%20further%20investigate%20the%20role%20of%20follow-up%0Alaboratory%20values%20as%20potential%20shortcut%20features%2C%20revealing%20Lab-MAE%27s%0Arobustness%20in%20scenarios%20where%20such%20data%20is%20unavailable.%20The%20findings%20suggest%0Athat%20our%20transformer-based%20architecture%2C%20adapted%20to%20the%20characteristics%20of%20the%0AEHR%20data%2C%20offers%20a%20foundation%20model%20for%20more%20accurate%20and%20fair%20clinical%0Aimputation%20models.%20In%20addition%2C%20we%20measure%20and%20compare%20the%20carbon%20footprint%20of%0ALab-MAE%20with%20the%20baseline%20XGBoost%20model%2C%20highlighting%20its%20environmental%0Arequirements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02648v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepresentation%2520Learning%2520of%2520Lab%2520Values%2520via%2520Masked%2520AutoEncoder%26entry.906535625%3DDavid%2520Restrepo%2520and%2520Chenwei%2520Wu%2520and%2520Yueran%2520Jia%2520and%2520Jaden%2520K.%2520Sun%2520and%2520Jack%2520Gallifant%2520and%2520Catherine%2520G.%2520Bielick%2520and%2520Yugang%2520Jia%2520and%2520Leo%2520A.%2520Celi%26entry.1292438233%3D%2520%2520Accurate%2520imputation%2520of%2520missing%2520laboratory%2520values%2520in%2520electronic%2520health%2520records%250A%2528EHRs%2529%2520is%2520critical%2520to%2520enable%2520robust%2520clinical%2520predictions%2520and%2520reduce%2520biases%2520in%250AAI%2520systems%2520in%2520healthcare.%2520Existing%2520methods%252C%2520such%2520as%2520variational%2520autoencoders%250A%2528VAEs%2529%2520and%2520decision%2520tree-based%2520approaches%2520such%2520as%2520XGBoost%252C%2520struggle%2520to%2520model%250Athe%2520complex%2520temporal%2520and%2520contextual%2520dependencies%2520in%2520EHR%2520data%252C%2520mainly%2520in%250Aunderrepresented%2520groups.%2520In%2520this%2520work%252C%2520we%2520propose%2520Lab-MAE%252C%2520a%2520novel%250Atransformer-based%2520masked%2520autoencoder%2520framework%2520that%2520leverages%2520self-supervised%250Alearning%2520for%2520the%2520imputation%2520of%2520continuous%2520sequential%2520lab%2520values.%2520Lab-MAE%250Aintroduces%2520a%2520structured%2520encoding%2520scheme%2520that%2520jointly%2520models%2520laboratory%2520test%250Avalues%2520and%2520their%2520corresponding%2520timestamps%252C%2520enabling%2520explicit%2520capturing%2520temporal%250Adependencies.%2520Empirical%2520evaluation%2520on%2520the%2520MIMIC-IV%2520dataset%2520demonstrates%2520that%250ALab-MAE%2520significantly%2520outperforms%2520the%2520state-of-the-art%2520baselines%2520such%2520as%250AXGBoost%2520across%2520multiple%2520metrics%252C%2520including%2520root%2520mean%2520square%2520error%2520%2528RMSE%2529%252C%250AR-squared%2520%2528R2%2529%252C%2520and%2520Wasserstein%2520distance%2520%2528WD%2529.%2520Notably%252C%2520Lab-MAE%2520achieves%250Aequitable%2520performance%2520across%2520demographic%2520groups%2520of%2520patients%252C%2520advancing%2520fairness%250Ain%2520clinical%2520predictions.%2520We%2520further%2520investigate%2520the%2520role%2520of%2520follow-up%250Alaboratory%2520values%2520as%2520potential%2520shortcut%2520features%252C%2520revealing%2520Lab-MAE%2527s%250Arobustness%2520in%2520scenarios%2520where%2520such%2520data%2520is%2520unavailable.%2520The%2520findings%2520suggest%250Athat%2520our%2520transformer-based%2520architecture%252C%2520adapted%2520to%2520the%2520characteristics%2520of%2520the%250AEHR%2520data%252C%2520offers%2520a%2520foundation%2520model%2520for%2520more%2520accurate%2520and%2520fair%2520clinical%250Aimputation%2520models.%2520In%2520addition%252C%2520we%2520measure%2520and%2520compare%2520the%2520carbon%2520footprint%2520of%250ALab-MAE%2520with%2520the%2520baseline%2520XGBoost%2520model%252C%2520highlighting%2520its%2520environmental%250Arequirements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02648v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Representation%20Learning%20of%20Lab%20Values%20via%20Masked%20AutoEncoder&entry.906535625=David%20Restrepo%20and%20Chenwei%20Wu%20and%20Yueran%20Jia%20and%20Jaden%20K.%20Sun%20and%20Jack%20Gallifant%20and%20Catherine%20G.%20Bielick%20and%20Yugang%20Jia%20and%20Leo%20A.%20Celi&entry.1292438233=%20%20Accurate%20imputation%20of%20missing%20laboratory%20values%20in%20electronic%20health%20records%0A%28EHRs%29%20is%20critical%20to%20enable%20robust%20clinical%20predictions%20and%20reduce%20biases%20in%0AAI%20systems%20in%20healthcare.%20Existing%20methods%2C%20such%20as%20variational%20autoencoders%0A%28VAEs%29%20and%20decision%20tree-based%20approaches%20such%20as%20XGBoost%2C%20struggle%20to%20model%0Athe%20complex%20temporal%20and%20contextual%20dependencies%20in%20EHR%20data%2C%20mainly%20in%0Aunderrepresented%20groups.%20In%20this%20work%2C%20we%20propose%20Lab-MAE%2C%20a%20novel%0Atransformer-based%20masked%20autoencoder%20framework%20that%20leverages%20self-supervised%0Alearning%20for%20the%20imputation%20of%20continuous%20sequential%20lab%20values.%20Lab-MAE%0Aintroduces%20a%20structured%20encoding%20scheme%20that%20jointly%20models%20laboratory%20test%0Avalues%20and%20their%20corresponding%20timestamps%2C%20enabling%20explicit%20capturing%20temporal%0Adependencies.%20Empirical%20evaluation%20on%20the%20MIMIC-IV%20dataset%20demonstrates%20that%0ALab-MAE%20significantly%20outperforms%20the%20state-of-the-art%20baselines%20such%20as%0AXGBoost%20across%20multiple%20metrics%2C%20including%20root%20mean%20square%20error%20%28RMSE%29%2C%0AR-squared%20%28R2%29%2C%20and%20Wasserstein%20distance%20%28WD%29.%20Notably%2C%20Lab-MAE%20achieves%0Aequitable%20performance%20across%20demographic%20groups%20of%20patients%2C%20advancing%20fairness%0Ain%20clinical%20predictions.%20We%20further%20investigate%20the%20role%20of%20follow-up%0Alaboratory%20values%20as%20potential%20shortcut%20features%2C%20revealing%20Lab-MAE%27s%0Arobustness%20in%20scenarios%20where%20such%20data%20is%20unavailable.%20The%20findings%20suggest%0Athat%20our%20transformer-based%20architecture%2C%20adapted%20to%20the%20characteristics%20of%20the%0AEHR%20data%2C%20offers%20a%20foundation%20model%20for%20more%20accurate%20and%20fair%20clinical%0Aimputation%20models.%20In%20addition%2C%20we%20measure%20and%20compare%20the%20carbon%20footprint%20of%0ALab-MAE%20with%20the%20baseline%20XGBoost%20model%2C%20highlighting%20its%20environmental%0Arequirements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02648v2&entry.124074799=Read"},
{"title": "Comparison Study: Glacier Calving Front Delineation in Synthetic\n  Aperture Radar Images With Deep Learning", "author": "Nora Gourmelon and Konrad Heidler and Erik Loebel and Daniel Cheng and Julian Klink and Anda Dong and Fei Wu and Noah Maul and Moritz Koch and Marcel Dreier and Dakota Pyles and Thorsten Seehaus and Matthias Braun and Andreas Maier and Vincent Christlein", "abstract": "  Calving front position variation of marine-terminating glaciers is an\nindicator of ice mass loss and a crucial parameter in numerical glacier models.\nDeep Learning (DL) systems can automatically extract this position from\nSynthetic Aperture Radar (SAR) imagery, enabling continuous, weather- and\nillumination-independent, large-scale monitoring. This study presents the first\ncomparison of DL systems on a common calving front benchmark dataset. A\nmulti-annotator study with ten annotators is performed to contrast the\nbest-performing DL system against human performance. The best DL model's\noutputs deviate 221 m on average, while the average deviation of the human\nannotators is 38 m. This significant difference shows that current DL systems\ndo not yet match human performance and that further research is needed to\nenable fully automated monitoring of glacier calving fronts. The study of\nVision Transformers, foundation models, and the inclusion and processing\nstrategy of more information are identified as avenues for future research.\n", "link": "http://arxiv.org/abs/2501.05281v1", "date": "2025-01-09", "relevancy": 2.054, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5147}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5147}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5076}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparison%20Study%3A%20Glacier%20Calving%20Front%20Delineation%20in%20Synthetic%0A%20%20Aperture%20Radar%20Images%20With%20Deep%20Learning&body=Title%3A%20Comparison%20Study%3A%20Glacier%20Calving%20Front%20Delineation%20in%20Synthetic%0A%20%20Aperture%20Radar%20Images%20With%20Deep%20Learning%0AAuthor%3A%20Nora%20Gourmelon%20and%20Konrad%20Heidler%20and%20Erik%20Loebel%20and%20Daniel%20Cheng%20and%20Julian%20Klink%20and%20Anda%20Dong%20and%20Fei%20Wu%20and%20Noah%20Maul%20and%20Moritz%20Koch%20and%20Marcel%20Dreier%20and%20Dakota%20Pyles%20and%20Thorsten%20Seehaus%20and%20Matthias%20Braun%20and%20Andreas%20Maier%20and%20Vincent%20Christlein%0AAbstract%3A%20%20%20Calving%20front%20position%20variation%20of%20marine-terminating%20glaciers%20is%20an%0Aindicator%20of%20ice%20mass%20loss%20and%20a%20crucial%20parameter%20in%20numerical%20glacier%20models.%0ADeep%20Learning%20%28DL%29%20systems%20can%20automatically%20extract%20this%20position%20from%0ASynthetic%20Aperture%20Radar%20%28SAR%29%20imagery%2C%20enabling%20continuous%2C%20weather-%20and%0Aillumination-independent%2C%20large-scale%20monitoring.%20This%20study%20presents%20the%20first%0Acomparison%20of%20DL%20systems%20on%20a%20common%20calving%20front%20benchmark%20dataset.%20A%0Amulti-annotator%20study%20with%20ten%20annotators%20is%20performed%20to%20contrast%20the%0Abest-performing%20DL%20system%20against%20human%20performance.%20The%20best%20DL%20model%27s%0Aoutputs%20deviate%20221%20m%20on%20average%2C%20while%20the%20average%20deviation%20of%20the%20human%0Aannotators%20is%2038%20m.%20This%20significant%20difference%20shows%20that%20current%20DL%20systems%0Ado%20not%20yet%20match%20human%20performance%20and%20that%20further%20research%20is%20needed%20to%0Aenable%20fully%20automated%20monitoring%20of%20glacier%20calving%20fronts.%20The%20study%20of%0AVision%20Transformers%2C%20foundation%20models%2C%20and%20the%20inclusion%20and%20processing%0Astrategy%20of%20more%20information%20are%20identified%20as%20avenues%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05281v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparison%2520Study%253A%2520Glacier%2520Calving%2520Front%2520Delineation%2520in%2520Synthetic%250A%2520%2520Aperture%2520Radar%2520Images%2520With%2520Deep%2520Learning%26entry.906535625%3DNora%2520Gourmelon%2520and%2520Konrad%2520Heidler%2520and%2520Erik%2520Loebel%2520and%2520Daniel%2520Cheng%2520and%2520Julian%2520Klink%2520and%2520Anda%2520Dong%2520and%2520Fei%2520Wu%2520and%2520Noah%2520Maul%2520and%2520Moritz%2520Koch%2520and%2520Marcel%2520Dreier%2520and%2520Dakota%2520Pyles%2520and%2520Thorsten%2520Seehaus%2520and%2520Matthias%2520Braun%2520and%2520Andreas%2520Maier%2520and%2520Vincent%2520Christlein%26entry.1292438233%3D%2520%2520Calving%2520front%2520position%2520variation%2520of%2520marine-terminating%2520glaciers%2520is%2520an%250Aindicator%2520of%2520ice%2520mass%2520loss%2520and%2520a%2520crucial%2520parameter%2520in%2520numerical%2520glacier%2520models.%250ADeep%2520Learning%2520%2528DL%2529%2520systems%2520can%2520automatically%2520extract%2520this%2520position%2520from%250ASynthetic%2520Aperture%2520Radar%2520%2528SAR%2529%2520imagery%252C%2520enabling%2520continuous%252C%2520weather-%2520and%250Aillumination-independent%252C%2520large-scale%2520monitoring.%2520This%2520study%2520presents%2520the%2520first%250Acomparison%2520of%2520DL%2520systems%2520on%2520a%2520common%2520calving%2520front%2520benchmark%2520dataset.%2520A%250Amulti-annotator%2520study%2520with%2520ten%2520annotators%2520is%2520performed%2520to%2520contrast%2520the%250Abest-performing%2520DL%2520system%2520against%2520human%2520performance.%2520The%2520best%2520DL%2520model%2527s%250Aoutputs%2520deviate%2520221%2520m%2520on%2520average%252C%2520while%2520the%2520average%2520deviation%2520of%2520the%2520human%250Aannotators%2520is%252038%2520m.%2520This%2520significant%2520difference%2520shows%2520that%2520current%2520DL%2520systems%250Ado%2520not%2520yet%2520match%2520human%2520performance%2520and%2520that%2520further%2520research%2520is%2520needed%2520to%250Aenable%2520fully%2520automated%2520monitoring%2520of%2520glacier%2520calving%2520fronts.%2520The%2520study%2520of%250AVision%2520Transformers%252C%2520foundation%2520models%252C%2520and%2520the%2520inclusion%2520and%2520processing%250Astrategy%2520of%2520more%2520information%2520are%2520identified%2520as%2520avenues%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05281v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparison%20Study%3A%20Glacier%20Calving%20Front%20Delineation%20in%20Synthetic%0A%20%20Aperture%20Radar%20Images%20With%20Deep%20Learning&entry.906535625=Nora%20Gourmelon%20and%20Konrad%20Heidler%20and%20Erik%20Loebel%20and%20Daniel%20Cheng%20and%20Julian%20Klink%20and%20Anda%20Dong%20and%20Fei%20Wu%20and%20Noah%20Maul%20and%20Moritz%20Koch%20and%20Marcel%20Dreier%20and%20Dakota%20Pyles%20and%20Thorsten%20Seehaus%20and%20Matthias%20Braun%20and%20Andreas%20Maier%20and%20Vincent%20Christlein&entry.1292438233=%20%20Calving%20front%20position%20variation%20of%20marine-terminating%20glaciers%20is%20an%0Aindicator%20of%20ice%20mass%20loss%20and%20a%20crucial%20parameter%20in%20numerical%20glacier%20models.%0ADeep%20Learning%20%28DL%29%20systems%20can%20automatically%20extract%20this%20position%20from%0ASynthetic%20Aperture%20Radar%20%28SAR%29%20imagery%2C%20enabling%20continuous%2C%20weather-%20and%0Aillumination-independent%2C%20large-scale%20monitoring.%20This%20study%20presents%20the%20first%0Acomparison%20of%20DL%20systems%20on%20a%20common%20calving%20front%20benchmark%20dataset.%20A%0Amulti-annotator%20study%20with%20ten%20annotators%20is%20performed%20to%20contrast%20the%0Abest-performing%20DL%20system%20against%20human%20performance.%20The%20best%20DL%20model%27s%0Aoutputs%20deviate%20221%20m%20on%20average%2C%20while%20the%20average%20deviation%20of%20the%20human%0Aannotators%20is%2038%20m.%20This%20significant%20difference%20shows%20that%20current%20DL%20systems%0Ado%20not%20yet%20match%20human%20performance%20and%20that%20further%20research%20is%20needed%20to%0Aenable%20fully%20automated%20monitoring%20of%20glacier%20calving%20fronts.%20The%20study%20of%0AVision%20Transformers%2C%20foundation%20models%2C%20and%20the%20inclusion%20and%20processing%0Astrategy%20of%20more%20information%20are%20identified%20as%20avenues%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05281v1&entry.124074799=Read"},
{"title": "AnCoGen: Analysis, Control and Generation of Speech with a Masked\n  Autoencoder", "author": "Samir Sadok and Simon Leglaive and Laurent Girin and Ga\u00ebl Richard and Xavier Alameda-Pineda", "abstract": "  This article introduces AnCoGen, a novel method that leverages a masked\nautoencoder to unify the analysis, control, and generation of speech signals\nwithin a single model. AnCoGen can analyze speech by estimating key attributes,\nsuch as speaker identity, pitch, content, loudness, signal-to-noise ratio, and\nclarity index. In addition, it can generate speech from these attributes and\nallow precise control of the synthesized speech by modifying them. Extensive\nexperiments demonstrated the effectiveness of AnCoGen across speech\nanalysis-resynthesis, pitch estimation, pitch modification, and speech\nenhancement.\n", "link": "http://arxiv.org/abs/2501.05332v1", "date": "2025-01-09", "relevancy": 2.0532, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.54}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5321}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnCoGen%3A%20Analysis%2C%20Control%20and%20Generation%20of%20Speech%20with%20a%20Masked%0A%20%20Autoencoder&body=Title%3A%20AnCoGen%3A%20Analysis%2C%20Control%20and%20Generation%20of%20Speech%20with%20a%20Masked%0A%20%20Autoencoder%0AAuthor%3A%20Samir%20Sadok%20and%20Simon%20Leglaive%20and%20Laurent%20Girin%20and%20Ga%C3%ABl%20Richard%20and%20Xavier%20Alameda-Pineda%0AAbstract%3A%20%20%20This%20article%20introduces%20AnCoGen%2C%20a%20novel%20method%20that%20leverages%20a%20masked%0Aautoencoder%20to%20unify%20the%20analysis%2C%20control%2C%20and%20generation%20of%20speech%20signals%0Awithin%20a%20single%20model.%20AnCoGen%20can%20analyze%20speech%20by%20estimating%20key%20attributes%2C%0Asuch%20as%20speaker%20identity%2C%20pitch%2C%20content%2C%20loudness%2C%20signal-to-noise%20ratio%2C%20and%0Aclarity%20index.%20In%20addition%2C%20it%20can%20generate%20speech%20from%20these%20attributes%20and%0Aallow%20precise%20control%20of%20the%20synthesized%20speech%20by%20modifying%20them.%20Extensive%0Aexperiments%20demonstrated%20the%20effectiveness%20of%20AnCoGen%20across%20speech%0Aanalysis-resynthesis%2C%20pitch%20estimation%2C%20pitch%20modification%2C%20and%20speech%0Aenhancement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05332v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnCoGen%253A%2520Analysis%252C%2520Control%2520and%2520Generation%2520of%2520Speech%2520with%2520a%2520Masked%250A%2520%2520Autoencoder%26entry.906535625%3DSamir%2520Sadok%2520and%2520Simon%2520Leglaive%2520and%2520Laurent%2520Girin%2520and%2520Ga%25C3%25ABl%2520Richard%2520and%2520Xavier%2520Alameda-Pineda%26entry.1292438233%3D%2520%2520This%2520article%2520introduces%2520AnCoGen%252C%2520a%2520novel%2520method%2520that%2520leverages%2520a%2520masked%250Aautoencoder%2520to%2520unify%2520the%2520analysis%252C%2520control%252C%2520and%2520generation%2520of%2520speech%2520signals%250Awithin%2520a%2520single%2520model.%2520AnCoGen%2520can%2520analyze%2520speech%2520by%2520estimating%2520key%2520attributes%252C%250Asuch%2520as%2520speaker%2520identity%252C%2520pitch%252C%2520content%252C%2520loudness%252C%2520signal-to-noise%2520ratio%252C%2520and%250Aclarity%2520index.%2520In%2520addition%252C%2520it%2520can%2520generate%2520speech%2520from%2520these%2520attributes%2520and%250Aallow%2520precise%2520control%2520of%2520the%2520synthesized%2520speech%2520by%2520modifying%2520them.%2520Extensive%250Aexperiments%2520demonstrated%2520the%2520effectiveness%2520of%2520AnCoGen%2520across%2520speech%250Aanalysis-resynthesis%252C%2520pitch%2520estimation%252C%2520pitch%2520modification%252C%2520and%2520speech%250Aenhancement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05332v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnCoGen%3A%20Analysis%2C%20Control%20and%20Generation%20of%20Speech%20with%20a%20Masked%0A%20%20Autoencoder&entry.906535625=Samir%20Sadok%20and%20Simon%20Leglaive%20and%20Laurent%20Girin%20and%20Ga%C3%ABl%20Richard%20and%20Xavier%20Alameda-Pineda&entry.1292438233=%20%20This%20article%20introduces%20AnCoGen%2C%20a%20novel%20method%20that%20leverages%20a%20masked%0Aautoencoder%20to%20unify%20the%20analysis%2C%20control%2C%20and%20generation%20of%20speech%20signals%0Awithin%20a%20single%20model.%20AnCoGen%20can%20analyze%20speech%20by%20estimating%20key%20attributes%2C%0Asuch%20as%20speaker%20identity%2C%20pitch%2C%20content%2C%20loudness%2C%20signal-to-noise%20ratio%2C%20and%0Aclarity%20index.%20In%20addition%2C%20it%20can%20generate%20speech%20from%20these%20attributes%20and%0Aallow%20precise%20control%20of%20the%20synthesized%20speech%20by%20modifying%20them.%20Extensive%0Aexperiments%20demonstrated%20the%20effectiveness%20of%20AnCoGen%20across%20speech%0Aanalysis-resynthesis%2C%20pitch%20estimation%2C%20pitch%20modification%2C%20and%20speech%0Aenhancement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05332v1&entry.124074799=Read"},
{"title": "JAQ: Joint Efficient Architecture Design and Low-Bit Quantization with\n  Hardware-Software Co-Exploration", "author": "Mingzi Wang and Yuan Meng and Chen Tang and Weixiang Zhang and Yijian Qin and Yang Yao and Yingxin Li and Tongtong Feng and Xin Wang and Xun Guan and Zhi Wang and Wenwu Zhu", "abstract": "  The co-design of neural network architectures, quantization precisions, and\nhardware accelerators offers a promising approach to achieving an optimal\nbalance between performance and efficiency, particularly for model deployment\non resource-constrained edge devices. In this work, we propose the JAQ\nFramework, which jointly optimizes the three critical dimensions. However,\neffectively automating the design process across the vast search space of those\nthree dimensions poses significant challenges, especially when pursuing\nextremely low-bit quantization. Specifical, the primary challenges include: (1)\nMemory overhead in software-side: Low-precision quantization-aware training can\nlead to significant memory usage due to storing large intermediate features and\nlatent weights for back-propagation, potentially causing memory exhaustion. (2)\nSearch time-consuming in hardware-side: The discrete nature of hardware\nparameters and the complex interplay between compiler optimizations and\nindividual operators make the accelerator search time-consuming. To address\nthese issues, JAQ mitigates the memory overhead through a channel-wise sparse\nquantization (CSQ) scheme, selectively applying quantization to the most\nsensitive components of the model during optimization. Additionally, JAQ\ndesigns BatchTile, which employs a hardware generation network to encode all\npossible tiling modes, thereby speeding up the search for the optimal compiler\nmapping strategy. Extensive experiments demonstrate the effectiveness of JAQ,\nachieving approximately 7% higher Top-1 accuracy on ImageNet compared to\nprevious methods and reducing the hardware search time per iteration to 0.15\nseconds.\n", "link": "http://arxiv.org/abs/2501.05339v1", "date": "2025-01-09", "relevancy": 2.047, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5399}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5243}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JAQ%3A%20Joint%20Efficient%20Architecture%20Design%20and%20Low-Bit%20Quantization%20with%0A%20%20Hardware-Software%20Co-Exploration&body=Title%3A%20JAQ%3A%20Joint%20Efficient%20Architecture%20Design%20and%20Low-Bit%20Quantization%20with%0A%20%20Hardware-Software%20Co-Exploration%0AAuthor%3A%20Mingzi%20Wang%20and%20Yuan%20Meng%20and%20Chen%20Tang%20and%20Weixiang%20Zhang%20and%20Yijian%20Qin%20and%20Yang%20Yao%20and%20Yingxin%20Li%20and%20Tongtong%20Feng%20and%20Xin%20Wang%20and%20Xun%20Guan%20and%20Zhi%20Wang%20and%20Wenwu%20Zhu%0AAbstract%3A%20%20%20The%20co-design%20of%20neural%20network%20architectures%2C%20quantization%20precisions%2C%20and%0Ahardware%20accelerators%20offers%20a%20promising%20approach%20to%20achieving%20an%20optimal%0Abalance%20between%20performance%20and%20efficiency%2C%20particularly%20for%20model%20deployment%0Aon%20resource-constrained%20edge%20devices.%20In%20this%20work%2C%20we%20propose%20the%20JAQ%0AFramework%2C%20which%20jointly%20optimizes%20the%20three%20critical%20dimensions.%20However%2C%0Aeffectively%20automating%20the%20design%20process%20across%20the%20vast%20search%20space%20of%20those%0Athree%20dimensions%20poses%20significant%20challenges%2C%20especially%20when%20pursuing%0Aextremely%20low-bit%20quantization.%20Specifical%2C%20the%20primary%20challenges%20include%3A%20%281%29%0AMemory%20overhead%20in%20software-side%3A%20Low-precision%20quantization-aware%20training%20can%0Alead%20to%20significant%20memory%20usage%20due%20to%20storing%20large%20intermediate%20features%20and%0Alatent%20weights%20for%20back-propagation%2C%20potentially%20causing%20memory%20exhaustion.%20%282%29%0ASearch%20time-consuming%20in%20hardware-side%3A%20The%20discrete%20nature%20of%20hardware%0Aparameters%20and%20the%20complex%20interplay%20between%20compiler%20optimizations%20and%0Aindividual%20operators%20make%20the%20accelerator%20search%20time-consuming.%20To%20address%0Athese%20issues%2C%20JAQ%20mitigates%20the%20memory%20overhead%20through%20a%20channel-wise%20sparse%0Aquantization%20%28CSQ%29%20scheme%2C%20selectively%20applying%20quantization%20to%20the%20most%0Asensitive%20components%20of%20the%20model%20during%20optimization.%20Additionally%2C%20JAQ%0Adesigns%20BatchTile%2C%20which%20employs%20a%20hardware%20generation%20network%20to%20encode%20all%0Apossible%20tiling%20modes%2C%20thereby%20speeding%20up%20the%20search%20for%20the%20optimal%20compiler%0Amapping%20strategy.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20JAQ%2C%0Aachieving%20approximately%207%25%20higher%20Top-1%20accuracy%20on%20ImageNet%20compared%20to%0Aprevious%20methods%20and%20reducing%20the%20hardware%20search%20time%20per%20iteration%20to%200.15%0Aseconds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05339v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJAQ%253A%2520Joint%2520Efficient%2520Architecture%2520Design%2520and%2520Low-Bit%2520Quantization%2520with%250A%2520%2520Hardware-Software%2520Co-Exploration%26entry.906535625%3DMingzi%2520Wang%2520and%2520Yuan%2520Meng%2520and%2520Chen%2520Tang%2520and%2520Weixiang%2520Zhang%2520and%2520Yijian%2520Qin%2520and%2520Yang%2520Yao%2520and%2520Yingxin%2520Li%2520and%2520Tongtong%2520Feng%2520and%2520Xin%2520Wang%2520and%2520Xun%2520Guan%2520and%2520Zhi%2520Wang%2520and%2520Wenwu%2520Zhu%26entry.1292438233%3D%2520%2520The%2520co-design%2520of%2520neural%2520network%2520architectures%252C%2520quantization%2520precisions%252C%2520and%250Ahardware%2520accelerators%2520offers%2520a%2520promising%2520approach%2520to%2520achieving%2520an%2520optimal%250Abalance%2520between%2520performance%2520and%2520efficiency%252C%2520particularly%2520for%2520model%2520deployment%250Aon%2520resource-constrained%2520edge%2520devices.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520JAQ%250AFramework%252C%2520which%2520jointly%2520optimizes%2520the%2520three%2520critical%2520dimensions.%2520However%252C%250Aeffectively%2520automating%2520the%2520design%2520process%2520across%2520the%2520vast%2520search%2520space%2520of%2520those%250Athree%2520dimensions%2520poses%2520significant%2520challenges%252C%2520especially%2520when%2520pursuing%250Aextremely%2520low-bit%2520quantization.%2520Specifical%252C%2520the%2520primary%2520challenges%2520include%253A%2520%25281%2529%250AMemory%2520overhead%2520in%2520software-side%253A%2520Low-precision%2520quantization-aware%2520training%2520can%250Alead%2520to%2520significant%2520memory%2520usage%2520due%2520to%2520storing%2520large%2520intermediate%2520features%2520and%250Alatent%2520weights%2520for%2520back-propagation%252C%2520potentially%2520causing%2520memory%2520exhaustion.%2520%25282%2529%250ASearch%2520time-consuming%2520in%2520hardware-side%253A%2520The%2520discrete%2520nature%2520of%2520hardware%250Aparameters%2520and%2520the%2520complex%2520interplay%2520between%2520compiler%2520optimizations%2520and%250Aindividual%2520operators%2520make%2520the%2520accelerator%2520search%2520time-consuming.%2520To%2520address%250Athese%2520issues%252C%2520JAQ%2520mitigates%2520the%2520memory%2520overhead%2520through%2520a%2520channel-wise%2520sparse%250Aquantization%2520%2528CSQ%2529%2520scheme%252C%2520selectively%2520applying%2520quantization%2520to%2520the%2520most%250Asensitive%2520components%2520of%2520the%2520model%2520during%2520optimization.%2520Additionally%252C%2520JAQ%250Adesigns%2520BatchTile%252C%2520which%2520employs%2520a%2520hardware%2520generation%2520network%2520to%2520encode%2520all%250Apossible%2520tiling%2520modes%252C%2520thereby%2520speeding%2520up%2520the%2520search%2520for%2520the%2520optimal%2520compiler%250Amapping%2520strategy.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520JAQ%252C%250Aachieving%2520approximately%25207%2525%2520higher%2520Top-1%2520accuracy%2520on%2520ImageNet%2520compared%2520to%250Aprevious%2520methods%2520and%2520reducing%2520the%2520hardware%2520search%2520time%2520per%2520iteration%2520to%25200.15%250Aseconds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05339v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JAQ%3A%20Joint%20Efficient%20Architecture%20Design%20and%20Low-Bit%20Quantization%20with%0A%20%20Hardware-Software%20Co-Exploration&entry.906535625=Mingzi%20Wang%20and%20Yuan%20Meng%20and%20Chen%20Tang%20and%20Weixiang%20Zhang%20and%20Yijian%20Qin%20and%20Yang%20Yao%20and%20Yingxin%20Li%20and%20Tongtong%20Feng%20and%20Xin%20Wang%20and%20Xun%20Guan%20and%20Zhi%20Wang%20and%20Wenwu%20Zhu&entry.1292438233=%20%20The%20co-design%20of%20neural%20network%20architectures%2C%20quantization%20precisions%2C%20and%0Ahardware%20accelerators%20offers%20a%20promising%20approach%20to%20achieving%20an%20optimal%0Abalance%20between%20performance%20and%20efficiency%2C%20particularly%20for%20model%20deployment%0Aon%20resource-constrained%20edge%20devices.%20In%20this%20work%2C%20we%20propose%20the%20JAQ%0AFramework%2C%20which%20jointly%20optimizes%20the%20three%20critical%20dimensions.%20However%2C%0Aeffectively%20automating%20the%20design%20process%20across%20the%20vast%20search%20space%20of%20those%0Athree%20dimensions%20poses%20significant%20challenges%2C%20especially%20when%20pursuing%0Aextremely%20low-bit%20quantization.%20Specifical%2C%20the%20primary%20challenges%20include%3A%20%281%29%0AMemory%20overhead%20in%20software-side%3A%20Low-precision%20quantization-aware%20training%20can%0Alead%20to%20significant%20memory%20usage%20due%20to%20storing%20large%20intermediate%20features%20and%0Alatent%20weights%20for%20back-propagation%2C%20potentially%20causing%20memory%20exhaustion.%20%282%29%0ASearch%20time-consuming%20in%20hardware-side%3A%20The%20discrete%20nature%20of%20hardware%0Aparameters%20and%20the%20complex%20interplay%20between%20compiler%20optimizations%20and%0Aindividual%20operators%20make%20the%20accelerator%20search%20time-consuming.%20To%20address%0Athese%20issues%2C%20JAQ%20mitigates%20the%20memory%20overhead%20through%20a%20channel-wise%20sparse%0Aquantization%20%28CSQ%29%20scheme%2C%20selectively%20applying%20quantization%20to%20the%20most%0Asensitive%20components%20of%20the%20model%20during%20optimization.%20Additionally%2C%20JAQ%0Adesigns%20BatchTile%2C%20which%20employs%20a%20hardware%20generation%20network%20to%20encode%20all%0Apossible%20tiling%20modes%2C%20thereby%20speeding%20up%20the%20search%20for%20the%20optimal%20compiler%0Amapping%20strategy.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20JAQ%2C%0Aachieving%20approximately%207%25%20higher%20Top-1%20accuracy%20on%20ImageNet%20compared%20to%0Aprevious%20methods%20and%20reducing%20the%20hardware%20search%20time%20per%20iteration%20to%200.15%0Aseconds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05339v1&entry.124074799=Read"},
{"title": "Large Physics Models: Towards a collaborative approach with Large\n  Language Models and Foundation Models", "author": "Kristian G. Barman and Sascha Caron and Emily Sullivan and Henk W. de Regt and Roberto Ruiz de Austri and Mieke Boon and Michael F\u00e4rber and Stefan Fr\u00f6se and Faegheh Hasibi and Andreas Ipp and Rukshak Kapoor and Gregor Kasieczka and Daniel Kosti\u0107 and Michael Kr\u00e4mer and Tobias Golling and Luis G. Lopez and Jesus Marco and Sydney Otten and Pawel Pawlowski and Pietro Vischia and Erik Weber and Christoph Weniger", "abstract": "  This paper explores ideas and provides a potential roadmap for the\ndevelopment and evaluation of physics-specific large-scale AI models, which we\ncall Large Physics Models (LPMs). These models, based on foundation models such\nas Large Language Models (LLMs) - trained on broad data - are tailored to\naddress the demands of physics research. LPMs can function independently or as\npart of an integrated framework. This framework can incorporate specialized\ntools, including symbolic reasoning modules for mathematical manipulations,\nframeworks to analyse specific experimental and simulated data, and mechanisms\nfor synthesizing theories and scientific literature. We begin by examining\nwhether the physics community should actively develop and refine dedicated\nmodels, rather than relying solely on commercial LLMs. We then outline how LPMs\ncan be realized through interdisciplinary collaboration among experts in\nphysics, computer science, and philosophy of science. To integrate these models\neffectively, we identify three key pillars: Development, Evaluation, and\nPhilosophical Reflection. Development focuses on constructing models capable of\nprocessing physics texts, mathematical formulations, and diverse physical data.\nEvaluation assesses accuracy and reliability by testing and benchmarking.\nFinally, Philosophical Reflection encompasses the analysis of broader\nimplications of LLMs in physics, including their potential to generate new\nscientific understanding and what novel collaboration dynamics might arise in\nresearch. Inspired by the organizational structure of experimental\ncollaborations in particle physics, we propose a similarly interdisciplinary\nand collaborative approach to building and refining Large Physics Models. This\nroadmap provides specific objectives, defines pathways to achieve them, and\nidentifies challenges that must be addressed to realise physics-specific large\nscale AI models.\n", "link": "http://arxiv.org/abs/2501.05382v1", "date": "2025-01-09", "relevancy": 2.0337, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5137}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5137}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Physics%20Models%3A%20Towards%20a%20collaborative%20approach%20with%20Large%0A%20%20Language%20Models%20and%20Foundation%20Models&body=Title%3A%20Large%20Physics%20Models%3A%20Towards%20a%20collaborative%20approach%20with%20Large%0A%20%20Language%20Models%20and%20Foundation%20Models%0AAuthor%3A%20Kristian%20G.%20Barman%20and%20Sascha%20Caron%20and%20Emily%20Sullivan%20and%20Henk%20W.%20de%20Regt%20and%20Roberto%20Ruiz%20de%20Austri%20and%20Mieke%20Boon%20and%20Michael%20F%C3%A4rber%20and%20Stefan%20Fr%C3%B6se%20and%20Faegheh%20Hasibi%20and%20Andreas%20Ipp%20and%20Rukshak%20Kapoor%20and%20Gregor%20Kasieczka%20and%20Daniel%20Kosti%C4%87%20and%20Michael%20Kr%C3%A4mer%20and%20Tobias%20Golling%20and%20Luis%20G.%20Lopez%20and%20Jesus%20Marco%20and%20Sydney%20Otten%20and%20Pawel%20Pawlowski%20and%20Pietro%20Vischia%20and%20Erik%20Weber%20and%20Christoph%20Weniger%0AAbstract%3A%20%20%20This%20paper%20explores%20ideas%20and%20provides%20a%20potential%20roadmap%20for%20the%0Adevelopment%20and%20evaluation%20of%20physics-specific%20large-scale%20AI%20models%2C%20which%20we%0Acall%20Large%20Physics%20Models%20%28LPMs%29.%20These%20models%2C%20based%20on%20foundation%20models%20such%0Aas%20Large%20Language%20Models%20%28LLMs%29%20-%20trained%20on%20broad%20data%20-%20are%20tailored%20to%0Aaddress%20the%20demands%20of%20physics%20research.%20LPMs%20can%20function%20independently%20or%20as%0Apart%20of%20an%20integrated%20framework.%20This%20framework%20can%20incorporate%20specialized%0Atools%2C%20including%20symbolic%20reasoning%20modules%20for%20mathematical%20manipulations%2C%0Aframeworks%20to%20analyse%20specific%20experimental%20and%20simulated%20data%2C%20and%20mechanisms%0Afor%20synthesizing%20theories%20and%20scientific%20literature.%20We%20begin%20by%20examining%0Awhether%20the%20physics%20community%20should%20actively%20develop%20and%20refine%20dedicated%0Amodels%2C%20rather%20than%20relying%20solely%20on%20commercial%20LLMs.%20We%20then%20outline%20how%20LPMs%0Acan%20be%20realized%20through%20interdisciplinary%20collaboration%20among%20experts%20in%0Aphysics%2C%20computer%20science%2C%20and%20philosophy%20of%20science.%20To%20integrate%20these%20models%0Aeffectively%2C%20we%20identify%20three%20key%20pillars%3A%20Development%2C%20Evaluation%2C%20and%0APhilosophical%20Reflection.%20Development%20focuses%20on%20constructing%20models%20capable%20of%0Aprocessing%20physics%20texts%2C%20mathematical%20formulations%2C%20and%20diverse%20physical%20data.%0AEvaluation%20assesses%20accuracy%20and%20reliability%20by%20testing%20and%20benchmarking.%0AFinally%2C%20Philosophical%20Reflection%20encompasses%20the%20analysis%20of%20broader%0Aimplications%20of%20LLMs%20in%20physics%2C%20including%20their%20potential%20to%20generate%20new%0Ascientific%20understanding%20and%20what%20novel%20collaboration%20dynamics%20might%20arise%20in%0Aresearch.%20Inspired%20by%20the%20organizational%20structure%20of%20experimental%0Acollaborations%20in%20particle%20physics%2C%20we%20propose%20a%20similarly%20interdisciplinary%0Aand%20collaborative%20approach%20to%20building%20and%20refining%20Large%20Physics%20Models.%20This%0Aroadmap%20provides%20specific%20objectives%2C%20defines%20pathways%20to%20achieve%20them%2C%20and%0Aidentifies%20challenges%20that%20must%20be%20addressed%20to%20realise%20physics-specific%20large%0Ascale%20AI%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05382v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Physics%2520Models%253A%2520Towards%2520a%2520collaborative%2520approach%2520with%2520Large%250A%2520%2520Language%2520Models%2520and%2520Foundation%2520Models%26entry.906535625%3DKristian%2520G.%2520Barman%2520and%2520Sascha%2520Caron%2520and%2520Emily%2520Sullivan%2520and%2520Henk%2520W.%2520de%2520Regt%2520and%2520Roberto%2520Ruiz%2520de%2520Austri%2520and%2520Mieke%2520Boon%2520and%2520Michael%2520F%25C3%25A4rber%2520and%2520Stefan%2520Fr%25C3%25B6se%2520and%2520Faegheh%2520Hasibi%2520and%2520Andreas%2520Ipp%2520and%2520Rukshak%2520Kapoor%2520and%2520Gregor%2520Kasieczka%2520and%2520Daniel%2520Kosti%25C4%2587%2520and%2520Michael%2520Kr%25C3%25A4mer%2520and%2520Tobias%2520Golling%2520and%2520Luis%2520G.%2520Lopez%2520and%2520Jesus%2520Marco%2520and%2520Sydney%2520Otten%2520and%2520Pawel%2520Pawlowski%2520and%2520Pietro%2520Vischia%2520and%2520Erik%2520Weber%2520and%2520Christoph%2520Weniger%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520ideas%2520and%2520provides%2520a%2520potential%2520roadmap%2520for%2520the%250Adevelopment%2520and%2520evaluation%2520of%2520physics-specific%2520large-scale%2520AI%2520models%252C%2520which%2520we%250Acall%2520Large%2520Physics%2520Models%2520%2528LPMs%2529.%2520These%2520models%252C%2520based%2520on%2520foundation%2520models%2520such%250Aas%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520-%2520trained%2520on%2520broad%2520data%2520-%2520are%2520tailored%2520to%250Aaddress%2520the%2520demands%2520of%2520physics%2520research.%2520LPMs%2520can%2520function%2520independently%2520or%2520as%250Apart%2520of%2520an%2520integrated%2520framework.%2520This%2520framework%2520can%2520incorporate%2520specialized%250Atools%252C%2520including%2520symbolic%2520reasoning%2520modules%2520for%2520mathematical%2520manipulations%252C%250Aframeworks%2520to%2520analyse%2520specific%2520experimental%2520and%2520simulated%2520data%252C%2520and%2520mechanisms%250Afor%2520synthesizing%2520theories%2520and%2520scientific%2520literature.%2520We%2520begin%2520by%2520examining%250Awhether%2520the%2520physics%2520community%2520should%2520actively%2520develop%2520and%2520refine%2520dedicated%250Amodels%252C%2520rather%2520than%2520relying%2520solely%2520on%2520commercial%2520LLMs.%2520We%2520then%2520outline%2520how%2520LPMs%250Acan%2520be%2520realized%2520through%2520interdisciplinary%2520collaboration%2520among%2520experts%2520in%250Aphysics%252C%2520computer%2520science%252C%2520and%2520philosophy%2520of%2520science.%2520To%2520integrate%2520these%2520models%250Aeffectively%252C%2520we%2520identify%2520three%2520key%2520pillars%253A%2520Development%252C%2520Evaluation%252C%2520and%250APhilosophical%2520Reflection.%2520Development%2520focuses%2520on%2520constructing%2520models%2520capable%2520of%250Aprocessing%2520physics%2520texts%252C%2520mathematical%2520formulations%252C%2520and%2520diverse%2520physical%2520data.%250AEvaluation%2520assesses%2520accuracy%2520and%2520reliability%2520by%2520testing%2520and%2520benchmarking.%250AFinally%252C%2520Philosophical%2520Reflection%2520encompasses%2520the%2520analysis%2520of%2520broader%250Aimplications%2520of%2520LLMs%2520in%2520physics%252C%2520including%2520their%2520potential%2520to%2520generate%2520new%250Ascientific%2520understanding%2520and%2520what%2520novel%2520collaboration%2520dynamics%2520might%2520arise%2520in%250Aresearch.%2520Inspired%2520by%2520the%2520organizational%2520structure%2520of%2520experimental%250Acollaborations%2520in%2520particle%2520physics%252C%2520we%2520propose%2520a%2520similarly%2520interdisciplinary%250Aand%2520collaborative%2520approach%2520to%2520building%2520and%2520refining%2520Large%2520Physics%2520Models.%2520This%250Aroadmap%2520provides%2520specific%2520objectives%252C%2520defines%2520pathways%2520to%2520achieve%2520them%252C%2520and%250Aidentifies%2520challenges%2520that%2520must%2520be%2520addressed%2520to%2520realise%2520physics-specific%2520large%250Ascale%2520AI%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05382v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Physics%20Models%3A%20Towards%20a%20collaborative%20approach%20with%20Large%0A%20%20Language%20Models%20and%20Foundation%20Models&entry.906535625=Kristian%20G.%20Barman%20and%20Sascha%20Caron%20and%20Emily%20Sullivan%20and%20Henk%20W.%20de%20Regt%20and%20Roberto%20Ruiz%20de%20Austri%20and%20Mieke%20Boon%20and%20Michael%20F%C3%A4rber%20and%20Stefan%20Fr%C3%B6se%20and%20Faegheh%20Hasibi%20and%20Andreas%20Ipp%20and%20Rukshak%20Kapoor%20and%20Gregor%20Kasieczka%20and%20Daniel%20Kosti%C4%87%20and%20Michael%20Kr%C3%A4mer%20and%20Tobias%20Golling%20and%20Luis%20G.%20Lopez%20and%20Jesus%20Marco%20and%20Sydney%20Otten%20and%20Pawel%20Pawlowski%20and%20Pietro%20Vischia%20and%20Erik%20Weber%20and%20Christoph%20Weniger&entry.1292438233=%20%20This%20paper%20explores%20ideas%20and%20provides%20a%20potential%20roadmap%20for%20the%0Adevelopment%20and%20evaluation%20of%20physics-specific%20large-scale%20AI%20models%2C%20which%20we%0Acall%20Large%20Physics%20Models%20%28LPMs%29.%20These%20models%2C%20based%20on%20foundation%20models%20such%0Aas%20Large%20Language%20Models%20%28LLMs%29%20-%20trained%20on%20broad%20data%20-%20are%20tailored%20to%0Aaddress%20the%20demands%20of%20physics%20research.%20LPMs%20can%20function%20independently%20or%20as%0Apart%20of%20an%20integrated%20framework.%20This%20framework%20can%20incorporate%20specialized%0Atools%2C%20including%20symbolic%20reasoning%20modules%20for%20mathematical%20manipulations%2C%0Aframeworks%20to%20analyse%20specific%20experimental%20and%20simulated%20data%2C%20and%20mechanisms%0Afor%20synthesizing%20theories%20and%20scientific%20literature.%20We%20begin%20by%20examining%0Awhether%20the%20physics%20community%20should%20actively%20develop%20and%20refine%20dedicated%0Amodels%2C%20rather%20than%20relying%20solely%20on%20commercial%20LLMs.%20We%20then%20outline%20how%20LPMs%0Acan%20be%20realized%20through%20interdisciplinary%20collaboration%20among%20experts%20in%0Aphysics%2C%20computer%20science%2C%20and%20philosophy%20of%20science.%20To%20integrate%20these%20models%0Aeffectively%2C%20we%20identify%20three%20key%20pillars%3A%20Development%2C%20Evaluation%2C%20and%0APhilosophical%20Reflection.%20Development%20focuses%20on%20constructing%20models%20capable%20of%0Aprocessing%20physics%20texts%2C%20mathematical%20formulations%2C%20and%20diverse%20physical%20data.%0AEvaluation%20assesses%20accuracy%20and%20reliability%20by%20testing%20and%20benchmarking.%0AFinally%2C%20Philosophical%20Reflection%20encompasses%20the%20analysis%20of%20broader%0Aimplications%20of%20LLMs%20in%20physics%2C%20including%20their%20potential%20to%20generate%20new%0Ascientific%20understanding%20and%20what%20novel%20collaboration%20dynamics%20might%20arise%20in%0Aresearch.%20Inspired%20by%20the%20organizational%20structure%20of%20experimental%0Acollaborations%20in%20particle%20physics%2C%20we%20propose%20a%20similarly%20interdisciplinary%0Aand%20collaborative%20approach%20to%20building%20and%20refining%20Large%20Physics%20Models.%20This%0Aroadmap%20provides%20specific%20objectives%2C%20defines%20pathways%20to%20achieve%20them%2C%20and%0Aidentifies%20challenges%20that%20must%20be%20addressed%20to%20realise%20physics-specific%20large%0Ascale%20AI%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05382v1&entry.124074799=Read"},
{"title": "$DPF^*$: improved Depth Potential Function for scale-invariant sulcal\n  depth estimation", "author": "Maxime Dieudonn\u00e9 and Guillaume Auzias and Julien Lef\u00e8vre", "abstract": "  The shape of human brain is complex and highly variable, with interactions\nbetween brain size, cortical folding, and age well-documented in the\nliterature. However, few studies have explored how global brain size influences\ngeometric features of the cortical surface derived from anatomical MRI. In this\nwork, we focus on sulcal depth, an imaging phenotype that has gained\nsignificant attention in both basic research and clinical applications. We make\nkey contributions to the field by: 1) providing the first quantitative analysis\nof how brain size affects sulcal depth measurements; 2) introducing a novel,\nscale-invariant method for sulcal depth estimation based on an original\nformalization of the problem; 3) presenting a validation framework and sharing\nour code and benchmark data with the community; and 4) demonstrating the\nbiological relevance of our new sulcal depth measure using a large sample of\n1,987 subjects spanning the developmental period from 26 weeks post-conception\nto adulthood.\n", "link": "http://arxiv.org/abs/2501.05436v1", "date": "2025-01-09", "relevancy": 2.0172, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5115}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5023}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24DPF%5E%2A%24%3A%20improved%20Depth%20Potential%20Function%20for%20scale-invariant%20sulcal%0A%20%20depth%20estimation&body=Title%3A%20%24DPF%5E%2A%24%3A%20improved%20Depth%20Potential%20Function%20for%20scale-invariant%20sulcal%0A%20%20depth%20estimation%0AAuthor%3A%20Maxime%20Dieudonn%C3%A9%20and%20Guillaume%20Auzias%20and%20Julien%20Lef%C3%A8vre%0AAbstract%3A%20%20%20The%20shape%20of%20human%20brain%20is%20complex%20and%20highly%20variable%2C%20with%20interactions%0Abetween%20brain%20size%2C%20cortical%20folding%2C%20and%20age%20well-documented%20in%20the%0Aliterature.%20However%2C%20few%20studies%20have%20explored%20how%20global%20brain%20size%20influences%0Ageometric%20features%20of%20the%20cortical%20surface%20derived%20from%20anatomical%20MRI.%20In%20this%0Awork%2C%20we%20focus%20on%20sulcal%20depth%2C%20an%20imaging%20phenotype%20that%20has%20gained%0Asignificant%20attention%20in%20both%20basic%20research%20and%20clinical%20applications.%20We%20make%0Akey%20contributions%20to%20the%20field%20by%3A%201%29%20providing%20the%20first%20quantitative%20analysis%0Aof%20how%20brain%20size%20affects%20sulcal%20depth%20measurements%3B%202%29%20introducing%20a%20novel%2C%0Ascale-invariant%20method%20for%20sulcal%20depth%20estimation%20based%20on%20an%20original%0Aformalization%20of%20the%20problem%3B%203%29%20presenting%20a%20validation%20framework%20and%20sharing%0Aour%20code%20and%20benchmark%20data%20with%20the%20community%3B%20and%204%29%20demonstrating%20the%0Abiological%20relevance%20of%20our%20new%20sulcal%20depth%20measure%20using%20a%20large%20sample%20of%0A1%2C987%20subjects%20spanning%20the%20developmental%20period%20from%2026%20weeks%20post-conception%0Ato%20adulthood.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05436v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524DPF%255E%252A%2524%253A%2520improved%2520Depth%2520Potential%2520Function%2520for%2520scale-invariant%2520sulcal%250A%2520%2520depth%2520estimation%26entry.906535625%3DMaxime%2520Dieudonn%25C3%25A9%2520and%2520Guillaume%2520Auzias%2520and%2520Julien%2520Lef%25C3%25A8vre%26entry.1292438233%3D%2520%2520The%2520shape%2520of%2520human%2520brain%2520is%2520complex%2520and%2520highly%2520variable%252C%2520with%2520interactions%250Abetween%2520brain%2520size%252C%2520cortical%2520folding%252C%2520and%2520age%2520well-documented%2520in%2520the%250Aliterature.%2520However%252C%2520few%2520studies%2520have%2520explored%2520how%2520global%2520brain%2520size%2520influences%250Ageometric%2520features%2520of%2520the%2520cortical%2520surface%2520derived%2520from%2520anatomical%2520MRI.%2520In%2520this%250Awork%252C%2520we%2520focus%2520on%2520sulcal%2520depth%252C%2520an%2520imaging%2520phenotype%2520that%2520has%2520gained%250Asignificant%2520attention%2520in%2520both%2520basic%2520research%2520and%2520clinical%2520applications.%2520We%2520make%250Akey%2520contributions%2520to%2520the%2520field%2520by%253A%25201%2529%2520providing%2520the%2520first%2520quantitative%2520analysis%250Aof%2520how%2520brain%2520size%2520affects%2520sulcal%2520depth%2520measurements%253B%25202%2529%2520introducing%2520a%2520novel%252C%250Ascale-invariant%2520method%2520for%2520sulcal%2520depth%2520estimation%2520based%2520on%2520an%2520original%250Aformalization%2520of%2520the%2520problem%253B%25203%2529%2520presenting%2520a%2520validation%2520framework%2520and%2520sharing%250Aour%2520code%2520and%2520benchmark%2520data%2520with%2520the%2520community%253B%2520and%25204%2529%2520demonstrating%2520the%250Abiological%2520relevance%2520of%2520our%2520new%2520sulcal%2520depth%2520measure%2520using%2520a%2520large%2520sample%2520of%250A1%252C987%2520subjects%2520spanning%2520the%2520developmental%2520period%2520from%252026%2520weeks%2520post-conception%250Ato%2520adulthood.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05436v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24DPF%5E%2A%24%3A%20improved%20Depth%20Potential%20Function%20for%20scale-invariant%20sulcal%0A%20%20depth%20estimation&entry.906535625=Maxime%20Dieudonn%C3%A9%20and%20Guillaume%20Auzias%20and%20Julien%20Lef%C3%A8vre&entry.1292438233=%20%20The%20shape%20of%20human%20brain%20is%20complex%20and%20highly%20variable%2C%20with%20interactions%0Abetween%20brain%20size%2C%20cortical%20folding%2C%20and%20age%20well-documented%20in%20the%0Aliterature.%20However%2C%20few%20studies%20have%20explored%20how%20global%20brain%20size%20influences%0Ageometric%20features%20of%20the%20cortical%20surface%20derived%20from%20anatomical%20MRI.%20In%20this%0Awork%2C%20we%20focus%20on%20sulcal%20depth%2C%20an%20imaging%20phenotype%20that%20has%20gained%0Asignificant%20attention%20in%20both%20basic%20research%20and%20clinical%20applications.%20We%20make%0Akey%20contributions%20to%20the%20field%20by%3A%201%29%20providing%20the%20first%20quantitative%20analysis%0Aof%20how%20brain%20size%20affects%20sulcal%20depth%20measurements%3B%202%29%20introducing%20a%20novel%2C%0Ascale-invariant%20method%20for%20sulcal%20depth%20estimation%20based%20on%20an%20original%0Aformalization%20of%20the%20problem%3B%203%29%20presenting%20a%20validation%20framework%20and%20sharing%0Aour%20code%20and%20benchmark%20data%20with%20the%20community%3B%20and%204%29%20demonstrating%20the%0Abiological%20relevance%20of%20our%20new%20sulcal%20depth%20measure%20using%20a%20large%20sample%20of%0A1%2C987%20subjects%20spanning%20the%20developmental%20period%20from%2026%20weeks%20post-conception%0Ato%20adulthood.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05436v1&entry.124074799=Read"},
{"title": "Is Your Autonomous Vehicle Safe? Understanding the Threat of\n  Electromagnetic Signal Injection Attacks on Traffic Scene Perception", "author": "Wenhao Liao and Sineng Yan and Youqian Zhang and Xinwei Zhai and Yuanyuan Wang and Eugene Yujun Fu", "abstract": "  Autonomous vehicles rely on camera-based perception systems to comprehend\ntheir driving environment and make crucial decisions, thereby ensuring vehicles\nto steer safely. However, a significant threat known as Electromagnetic Signal\nInjection Attacks (ESIA) can distort the images captured by these cameras,\nleading to incorrect AI decisions and potentially compromising the safety of\nautonomous vehicles. Despite the serious implications of ESIA, there is limited\nunderstanding of its impacts on the robustness of AI models across various and\ncomplex driving scenarios. To address this gap, our research analyzes the\nperformance of different models under ESIA, revealing their vulnerabilities to\nthe attacks. Moreover, due to the challenges in obtaining real-world attack\ndata, we develop a novel ESIA simulation method and generate a simulated attack\ndataset for different driving scenarios. Our research provides a comprehensive\nsimulation and evaluation framework, aiming to enhance the development of more\nrobust AI models and secure intelligent systems, ultimately contributing to the\nadvancement of safer and more reliable technology across various fields.\n", "link": "http://arxiv.org/abs/2501.05239v1", "date": "2025-01-09", "relevancy": 2.0106, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5168}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4934}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20Your%20Autonomous%20Vehicle%20Safe%3F%20Understanding%20the%20Threat%20of%0A%20%20Electromagnetic%20Signal%20Injection%20Attacks%20on%20Traffic%20Scene%20Perception&body=Title%3A%20Is%20Your%20Autonomous%20Vehicle%20Safe%3F%20Understanding%20the%20Threat%20of%0A%20%20Electromagnetic%20Signal%20Injection%20Attacks%20on%20Traffic%20Scene%20Perception%0AAuthor%3A%20Wenhao%20Liao%20and%20Sineng%20Yan%20and%20Youqian%20Zhang%20and%20Xinwei%20Zhai%20and%20Yuanyuan%20Wang%20and%20Eugene%20Yujun%20Fu%0AAbstract%3A%20%20%20Autonomous%20vehicles%20rely%20on%20camera-based%20perception%20systems%20to%20comprehend%0Atheir%20driving%20environment%20and%20make%20crucial%20decisions%2C%20thereby%20ensuring%20vehicles%0Ato%20steer%20safely.%20However%2C%20a%20significant%20threat%20known%20as%20Electromagnetic%20Signal%0AInjection%20Attacks%20%28ESIA%29%20can%20distort%20the%20images%20captured%20by%20these%20cameras%2C%0Aleading%20to%20incorrect%20AI%20decisions%20and%20potentially%20compromising%20the%20safety%20of%0Aautonomous%20vehicles.%20Despite%20the%20serious%20implications%20of%20ESIA%2C%20there%20is%20limited%0Aunderstanding%20of%20its%20impacts%20on%20the%20robustness%20of%20AI%20models%20across%20various%20and%0Acomplex%20driving%20scenarios.%20To%20address%20this%20gap%2C%20our%20research%20analyzes%20the%0Aperformance%20of%20different%20models%20under%20ESIA%2C%20revealing%20their%20vulnerabilities%20to%0Athe%20attacks.%20Moreover%2C%20due%20to%20the%20challenges%20in%20obtaining%20real-world%20attack%0Adata%2C%20we%20develop%20a%20novel%20ESIA%20simulation%20method%20and%20generate%20a%20simulated%20attack%0Adataset%20for%20different%20driving%20scenarios.%20Our%20research%20provides%20a%20comprehensive%0Asimulation%20and%20evaluation%20framework%2C%20aiming%20to%20enhance%20the%20development%20of%20more%0Arobust%20AI%20models%20and%20secure%20intelligent%20systems%2C%20ultimately%20contributing%20to%20the%0Aadvancement%20of%20safer%20and%20more%20reliable%20technology%20across%20various%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520Your%2520Autonomous%2520Vehicle%2520Safe%253F%2520Understanding%2520the%2520Threat%2520of%250A%2520%2520Electromagnetic%2520Signal%2520Injection%2520Attacks%2520on%2520Traffic%2520Scene%2520Perception%26entry.906535625%3DWenhao%2520Liao%2520and%2520Sineng%2520Yan%2520and%2520Youqian%2520Zhang%2520and%2520Xinwei%2520Zhai%2520and%2520Yuanyuan%2520Wang%2520and%2520Eugene%2520Yujun%2520Fu%26entry.1292438233%3D%2520%2520Autonomous%2520vehicles%2520rely%2520on%2520camera-based%2520perception%2520systems%2520to%2520comprehend%250Atheir%2520driving%2520environment%2520and%2520make%2520crucial%2520decisions%252C%2520thereby%2520ensuring%2520vehicles%250Ato%2520steer%2520safely.%2520However%252C%2520a%2520significant%2520threat%2520known%2520as%2520Electromagnetic%2520Signal%250AInjection%2520Attacks%2520%2528ESIA%2529%2520can%2520distort%2520the%2520images%2520captured%2520by%2520these%2520cameras%252C%250Aleading%2520to%2520incorrect%2520AI%2520decisions%2520and%2520potentially%2520compromising%2520the%2520safety%2520of%250Aautonomous%2520vehicles.%2520Despite%2520the%2520serious%2520implications%2520of%2520ESIA%252C%2520there%2520is%2520limited%250Aunderstanding%2520of%2520its%2520impacts%2520on%2520the%2520robustness%2520of%2520AI%2520models%2520across%2520various%2520and%250Acomplex%2520driving%2520scenarios.%2520To%2520address%2520this%2520gap%252C%2520our%2520research%2520analyzes%2520the%250Aperformance%2520of%2520different%2520models%2520under%2520ESIA%252C%2520revealing%2520their%2520vulnerabilities%2520to%250Athe%2520attacks.%2520Moreover%252C%2520due%2520to%2520the%2520challenges%2520in%2520obtaining%2520real-world%2520attack%250Adata%252C%2520we%2520develop%2520a%2520novel%2520ESIA%2520simulation%2520method%2520and%2520generate%2520a%2520simulated%2520attack%250Adataset%2520for%2520different%2520driving%2520scenarios.%2520Our%2520research%2520provides%2520a%2520comprehensive%250Asimulation%2520and%2520evaluation%2520framework%252C%2520aiming%2520to%2520enhance%2520the%2520development%2520of%2520more%250Arobust%2520AI%2520models%2520and%2520secure%2520intelligent%2520systems%252C%2520ultimately%2520contributing%2520to%2520the%250Aadvancement%2520of%2520safer%2520and%2520more%2520reliable%2520technology%2520across%2520various%2520fields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Your%20Autonomous%20Vehicle%20Safe%3F%20Understanding%20the%20Threat%20of%0A%20%20Electromagnetic%20Signal%20Injection%20Attacks%20on%20Traffic%20Scene%20Perception&entry.906535625=Wenhao%20Liao%20and%20Sineng%20Yan%20and%20Youqian%20Zhang%20and%20Xinwei%20Zhai%20and%20Yuanyuan%20Wang%20and%20Eugene%20Yujun%20Fu&entry.1292438233=%20%20Autonomous%20vehicles%20rely%20on%20camera-based%20perception%20systems%20to%20comprehend%0Atheir%20driving%20environment%20and%20make%20crucial%20decisions%2C%20thereby%20ensuring%20vehicles%0Ato%20steer%20safely.%20However%2C%20a%20significant%20threat%20known%20as%20Electromagnetic%20Signal%0AInjection%20Attacks%20%28ESIA%29%20can%20distort%20the%20images%20captured%20by%20these%20cameras%2C%0Aleading%20to%20incorrect%20AI%20decisions%20and%20potentially%20compromising%20the%20safety%20of%0Aautonomous%20vehicles.%20Despite%20the%20serious%20implications%20of%20ESIA%2C%20there%20is%20limited%0Aunderstanding%20of%20its%20impacts%20on%20the%20robustness%20of%20AI%20models%20across%20various%20and%0Acomplex%20driving%20scenarios.%20To%20address%20this%20gap%2C%20our%20research%20analyzes%20the%0Aperformance%20of%20different%20models%20under%20ESIA%2C%20revealing%20their%20vulnerabilities%20to%0Athe%20attacks.%20Moreover%2C%20due%20to%20the%20challenges%20in%20obtaining%20real-world%20attack%0Adata%2C%20we%20develop%20a%20novel%20ESIA%20simulation%20method%20and%20generate%20a%20simulated%20attack%0Adataset%20for%20different%20driving%20scenarios.%20Our%20research%20provides%20a%20comprehensive%0Asimulation%20and%20evaluation%20framework%2C%20aiming%20to%20enhance%20the%20development%20of%20more%0Arobust%20AI%20models%20and%20secure%20intelligent%20systems%2C%20ultimately%20contributing%20to%20the%0Aadvancement%20of%20safer%20and%20more%20reliable%20technology%20across%20various%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05239v1&entry.124074799=Read"},
{"title": "AgroGPT: Efficient Agricultural Vision-Language Model with Expert Tuning", "author": "Muhammad Awais and Ali Husain Salem Abdulla Alharthi and Amandeep Kumar and Hisham Cholakkal and Rao Muhammad Anwer", "abstract": "  Significant progress has been made in advancing large multimodal\nconversational models (LMMs), capitalizing on vast repositories of image-text\ndata available online. Despite this progress, these models often encounter\nsubstantial domain gaps, hindering their ability to engage in complex\nconversations across new domains. Recent efforts have aimed to mitigate this\nissue, albeit relying on domain-specific image-text data to curate\ninstruction-tuning data. However, many domains, such as agriculture, lack such\nvision-language data. In this work, we propose an approach to construct\ninstruction-tuning data that harnesses vision-only data for the agriculture\ndomain. We utilize diverse agricultural datasets spanning multiple domains,\ncurate class-specific information, and employ large language models (LLMs) to\nconstruct an expert-tuning set, resulting in a 70k expert-tuning dataset called\nAgroInstruct. Subsequently, we expert-tuned and created AgroGPT, an efficient\nLMM that can hold complex agriculture-related conversations and provide useful\ninsights. We also develop AgroEvals for evaluation and compare {AgroGPT's}\nperformance with large open and closed-source models. {AgroGPT} excels at\nidentifying fine-grained agricultural concepts, can act as an agriculture\nexpert, and provides helpful information for multimodal agriculture questions.\nThe code, datasets, and models are available at\nhttps://github.com/awaisrauf/agroGPT.\n", "link": "http://arxiv.org/abs/2410.08405v2", "date": "2025-01-09", "relevancy": 2.0004, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5017}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4998}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AgroGPT%3A%20Efficient%20Agricultural%20Vision-Language%20Model%20with%20Expert%20Tuning&body=Title%3A%20AgroGPT%3A%20Efficient%20Agricultural%20Vision-Language%20Model%20with%20Expert%20Tuning%0AAuthor%3A%20Muhammad%20Awais%20and%20Ali%20Husain%20Salem%20Abdulla%20Alharthi%20and%20Amandeep%20Kumar%20and%20Hisham%20Cholakkal%20and%20Rao%20Muhammad%20Anwer%0AAbstract%3A%20%20%20Significant%20progress%20has%20been%20made%20in%20advancing%20large%20multimodal%0Aconversational%20models%20%28LMMs%29%2C%20capitalizing%20on%20vast%20repositories%20of%20image-text%0Adata%20available%20online.%20Despite%20this%20progress%2C%20these%20models%20often%20encounter%0Asubstantial%20domain%20gaps%2C%20hindering%20their%20ability%20to%20engage%20in%20complex%0Aconversations%20across%20new%20domains.%20Recent%20efforts%20have%20aimed%20to%20mitigate%20this%0Aissue%2C%20albeit%20relying%20on%20domain-specific%20image-text%20data%20to%20curate%0Ainstruction-tuning%20data.%20However%2C%20many%20domains%2C%20such%20as%20agriculture%2C%20lack%20such%0Avision-language%20data.%20In%20this%20work%2C%20we%20propose%20an%20approach%20to%20construct%0Ainstruction-tuning%20data%20that%20harnesses%20vision-only%20data%20for%20the%20agriculture%0Adomain.%20We%20utilize%20diverse%20agricultural%20datasets%20spanning%20multiple%20domains%2C%0Acurate%20class-specific%20information%2C%20and%20employ%20large%20language%20models%20%28LLMs%29%20to%0Aconstruct%20an%20expert-tuning%20set%2C%20resulting%20in%20a%2070k%20expert-tuning%20dataset%20called%0AAgroInstruct.%20Subsequently%2C%20we%20expert-tuned%20and%20created%20AgroGPT%2C%20an%20efficient%0ALMM%20that%20can%20hold%20complex%20agriculture-related%20conversations%20and%20provide%20useful%0Ainsights.%20We%20also%20develop%20AgroEvals%20for%20evaluation%20and%20compare%20%7BAgroGPT%27s%7D%0Aperformance%20with%20large%20open%20and%20closed-source%20models.%20%7BAgroGPT%7D%20excels%20at%0Aidentifying%20fine-grained%20agricultural%20concepts%2C%20can%20act%20as%20an%20agriculture%0Aexpert%2C%20and%20provides%20helpful%20information%20for%20multimodal%20agriculture%20questions.%0AThe%20code%2C%20datasets%2C%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/awaisrauf/agroGPT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08405v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgroGPT%253A%2520Efficient%2520Agricultural%2520Vision-Language%2520Model%2520with%2520Expert%2520Tuning%26entry.906535625%3DMuhammad%2520Awais%2520and%2520Ali%2520Husain%2520Salem%2520Abdulla%2520Alharthi%2520and%2520Amandeep%2520Kumar%2520and%2520Hisham%2520Cholakkal%2520and%2520Rao%2520Muhammad%2520Anwer%26entry.1292438233%3D%2520%2520Significant%2520progress%2520has%2520been%2520made%2520in%2520advancing%2520large%2520multimodal%250Aconversational%2520models%2520%2528LMMs%2529%252C%2520capitalizing%2520on%2520vast%2520repositories%2520of%2520image-text%250Adata%2520available%2520online.%2520Despite%2520this%2520progress%252C%2520these%2520models%2520often%2520encounter%250Asubstantial%2520domain%2520gaps%252C%2520hindering%2520their%2520ability%2520to%2520engage%2520in%2520complex%250Aconversations%2520across%2520new%2520domains.%2520Recent%2520efforts%2520have%2520aimed%2520to%2520mitigate%2520this%250Aissue%252C%2520albeit%2520relying%2520on%2520domain-specific%2520image-text%2520data%2520to%2520curate%250Ainstruction-tuning%2520data.%2520However%252C%2520many%2520domains%252C%2520such%2520as%2520agriculture%252C%2520lack%2520such%250Avision-language%2520data.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520approach%2520to%2520construct%250Ainstruction-tuning%2520data%2520that%2520harnesses%2520vision-only%2520data%2520for%2520the%2520agriculture%250Adomain.%2520We%2520utilize%2520diverse%2520agricultural%2520datasets%2520spanning%2520multiple%2520domains%252C%250Acurate%2520class-specific%2520information%252C%2520and%2520employ%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%250Aconstruct%2520an%2520expert-tuning%2520set%252C%2520resulting%2520in%2520a%252070k%2520expert-tuning%2520dataset%2520called%250AAgroInstruct.%2520Subsequently%252C%2520we%2520expert-tuned%2520and%2520created%2520AgroGPT%252C%2520an%2520efficient%250ALMM%2520that%2520can%2520hold%2520complex%2520agriculture-related%2520conversations%2520and%2520provide%2520useful%250Ainsights.%2520We%2520also%2520develop%2520AgroEvals%2520for%2520evaluation%2520and%2520compare%2520%257BAgroGPT%2527s%257D%250Aperformance%2520with%2520large%2520open%2520and%2520closed-source%2520models.%2520%257BAgroGPT%257D%2520excels%2520at%250Aidentifying%2520fine-grained%2520agricultural%2520concepts%252C%2520can%2520act%2520as%2520an%2520agriculture%250Aexpert%252C%2520and%2520provides%2520helpful%2520information%2520for%2520multimodal%2520agriculture%2520questions.%250AThe%2520code%252C%2520datasets%252C%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/awaisrauf/agroGPT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08405v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgroGPT%3A%20Efficient%20Agricultural%20Vision-Language%20Model%20with%20Expert%20Tuning&entry.906535625=Muhammad%20Awais%20and%20Ali%20Husain%20Salem%20Abdulla%20Alharthi%20and%20Amandeep%20Kumar%20and%20Hisham%20Cholakkal%20and%20Rao%20Muhammad%20Anwer&entry.1292438233=%20%20Significant%20progress%20has%20been%20made%20in%20advancing%20large%20multimodal%0Aconversational%20models%20%28LMMs%29%2C%20capitalizing%20on%20vast%20repositories%20of%20image-text%0Adata%20available%20online.%20Despite%20this%20progress%2C%20these%20models%20often%20encounter%0Asubstantial%20domain%20gaps%2C%20hindering%20their%20ability%20to%20engage%20in%20complex%0Aconversations%20across%20new%20domains.%20Recent%20efforts%20have%20aimed%20to%20mitigate%20this%0Aissue%2C%20albeit%20relying%20on%20domain-specific%20image-text%20data%20to%20curate%0Ainstruction-tuning%20data.%20However%2C%20many%20domains%2C%20such%20as%20agriculture%2C%20lack%20such%0Avision-language%20data.%20In%20this%20work%2C%20we%20propose%20an%20approach%20to%20construct%0Ainstruction-tuning%20data%20that%20harnesses%20vision-only%20data%20for%20the%20agriculture%0Adomain.%20We%20utilize%20diverse%20agricultural%20datasets%20spanning%20multiple%20domains%2C%0Acurate%20class-specific%20information%2C%20and%20employ%20large%20language%20models%20%28LLMs%29%20to%0Aconstruct%20an%20expert-tuning%20set%2C%20resulting%20in%20a%2070k%20expert-tuning%20dataset%20called%0AAgroInstruct.%20Subsequently%2C%20we%20expert-tuned%20and%20created%20AgroGPT%2C%20an%20efficient%0ALMM%20that%20can%20hold%20complex%20agriculture-related%20conversations%20and%20provide%20useful%0Ainsights.%20We%20also%20develop%20AgroEvals%20for%20evaluation%20and%20compare%20%7BAgroGPT%27s%7D%0Aperformance%20with%20large%20open%20and%20closed-source%20models.%20%7BAgroGPT%7D%20excels%20at%0Aidentifying%20fine-grained%20agricultural%20concepts%2C%20can%20act%20as%20an%20agriculture%0Aexpert%2C%20and%20provides%20helpful%20information%20for%20multimodal%20agriculture%20questions.%0AThe%20code%2C%20datasets%2C%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/awaisrauf/agroGPT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08405v2&entry.124074799=Read"},
{"title": "PFML: Self-Supervised Learning of Time-Series Data Without\n  Representation Collapse", "author": "Einari Vaaras and Manu Airaksinen and Okko R\u00e4s\u00e4nen", "abstract": "  Self-supervised learning (SSL) is a data-driven learning approach that\nutilizes the innate structure of the data to guide the learning process. In\ncontrast to supervised learning, which depends on external labels, SSL utilizes\nthe inherent characteristics of the data to produce its own supervisory signal.\nHowever, one frequent issue with SSL methods is representation collapse, where\nthe model outputs a constant input-invariant feature representation. This issue\nhinders the potential application of SSL methods to new data modalities, as\ntrying to avoid representation collapse wastes researchers' time and effort.\nThis paper introduces a novel SSL algorithm for time-series data called\nPrediction of Functionals from Masked Latents (PFML). Instead of predicting\nmasked input signals or their latent representations directly, PFML operates by\npredicting statistical functionals of the input signal corresponding to masked\nembeddings, given a sequence of unmasked embeddings. The algorithm is designed\nto avoid representation collapse, rendering it straightforwardly applicable to\ndifferent time-series data domains, such as novel sensor modalities in clinical\ndata. We demonstrate the effectiveness of PFML through complex, real-life\nclassification tasks across three different data modalities: infant posture and\nmovement classification from multi-sensor inertial measurement unit data,\nemotion recognition from speech data, and sleep stage classification from EEG\ndata. The results show that PFML is superior to a conceptually similar SSL\nmethod and a contrastive learning-based SSL method. Additionally, PFML is on\npar with the current state-of-the-art SSL method, while also being conceptually\nsimpler and without suffering from representation collapse.\n", "link": "http://arxiv.org/abs/2411.10087v2", "date": "2025-01-09", "relevancy": 1.9886, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5029}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5019}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PFML%3A%20Self-Supervised%20Learning%20of%20Time-Series%20Data%20Without%0A%20%20Representation%20Collapse&body=Title%3A%20PFML%3A%20Self-Supervised%20Learning%20of%20Time-Series%20Data%20Without%0A%20%20Representation%20Collapse%0AAuthor%3A%20Einari%20Vaaras%20and%20Manu%20Airaksinen%20and%20Okko%20R%C3%A4s%C3%A4nen%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20is%20a%20data-driven%20learning%20approach%20that%0Autilizes%20the%20innate%20structure%20of%20the%20data%20to%20guide%20the%20learning%20process.%20In%0Acontrast%20to%20supervised%20learning%2C%20which%20depends%20on%20external%20labels%2C%20SSL%20utilizes%0Athe%20inherent%20characteristics%20of%20the%20data%20to%20produce%20its%20own%20supervisory%20signal.%0AHowever%2C%20one%20frequent%20issue%20with%20SSL%20methods%20is%20representation%20collapse%2C%20where%0Athe%20model%20outputs%20a%20constant%20input-invariant%20feature%20representation.%20This%20issue%0Ahinders%20the%20potential%20application%20of%20SSL%20methods%20to%20new%20data%20modalities%2C%20as%0Atrying%20to%20avoid%20representation%20collapse%20wastes%20researchers%27%20time%20and%20effort.%0AThis%20paper%20introduces%20a%20novel%20SSL%20algorithm%20for%20time-series%20data%20called%0APrediction%20of%20Functionals%20from%20Masked%20Latents%20%28PFML%29.%20Instead%20of%20predicting%0Amasked%20input%20signals%20or%20their%20latent%20representations%20directly%2C%20PFML%20operates%20by%0Apredicting%20statistical%20functionals%20of%20the%20input%20signal%20corresponding%20to%20masked%0Aembeddings%2C%20given%20a%20sequence%20of%20unmasked%20embeddings.%20The%20algorithm%20is%20designed%0Ato%20avoid%20representation%20collapse%2C%20rendering%20it%20straightforwardly%20applicable%20to%0Adifferent%20time-series%20data%20domains%2C%20such%20as%20novel%20sensor%20modalities%20in%20clinical%0Adata.%20We%20demonstrate%20the%20effectiveness%20of%20PFML%20through%20complex%2C%20real-life%0Aclassification%20tasks%20across%20three%20different%20data%20modalities%3A%20infant%20posture%20and%0Amovement%20classification%20from%20multi-sensor%20inertial%20measurement%20unit%20data%2C%0Aemotion%20recognition%20from%20speech%20data%2C%20and%20sleep%20stage%20classification%20from%20EEG%0Adata.%20The%20results%20show%20that%20PFML%20is%20superior%20to%20a%20conceptually%20similar%20SSL%0Amethod%20and%20a%20contrastive%20learning-based%20SSL%20method.%20Additionally%2C%20PFML%20is%20on%0Apar%20with%20the%20current%20state-of-the-art%20SSL%20method%2C%20while%20also%20being%20conceptually%0Asimpler%20and%20without%20suffering%20from%20representation%20collapse.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10087v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPFML%253A%2520Self-Supervised%2520Learning%2520of%2520Time-Series%2520Data%2520Without%250A%2520%2520Representation%2520Collapse%26entry.906535625%3DEinari%2520Vaaras%2520and%2520Manu%2520Airaksinen%2520and%2520Okko%2520R%25C3%25A4s%25C3%25A4nen%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520%2528SSL%2529%2520is%2520a%2520data-driven%2520learning%2520approach%2520that%250Autilizes%2520the%2520innate%2520structure%2520of%2520the%2520data%2520to%2520guide%2520the%2520learning%2520process.%2520In%250Acontrast%2520to%2520supervised%2520learning%252C%2520which%2520depends%2520on%2520external%2520labels%252C%2520SSL%2520utilizes%250Athe%2520inherent%2520characteristics%2520of%2520the%2520data%2520to%2520produce%2520its%2520own%2520supervisory%2520signal.%250AHowever%252C%2520one%2520frequent%2520issue%2520with%2520SSL%2520methods%2520is%2520representation%2520collapse%252C%2520where%250Athe%2520model%2520outputs%2520a%2520constant%2520input-invariant%2520feature%2520representation.%2520This%2520issue%250Ahinders%2520the%2520potential%2520application%2520of%2520SSL%2520methods%2520to%2520new%2520data%2520modalities%252C%2520as%250Atrying%2520to%2520avoid%2520representation%2520collapse%2520wastes%2520researchers%2527%2520time%2520and%2520effort.%250AThis%2520paper%2520introduces%2520a%2520novel%2520SSL%2520algorithm%2520for%2520time-series%2520data%2520called%250APrediction%2520of%2520Functionals%2520from%2520Masked%2520Latents%2520%2528PFML%2529.%2520Instead%2520of%2520predicting%250Amasked%2520input%2520signals%2520or%2520their%2520latent%2520representations%2520directly%252C%2520PFML%2520operates%2520by%250Apredicting%2520statistical%2520functionals%2520of%2520the%2520input%2520signal%2520corresponding%2520to%2520masked%250Aembeddings%252C%2520given%2520a%2520sequence%2520of%2520unmasked%2520embeddings.%2520The%2520algorithm%2520is%2520designed%250Ato%2520avoid%2520representation%2520collapse%252C%2520rendering%2520it%2520straightforwardly%2520applicable%2520to%250Adifferent%2520time-series%2520data%2520domains%252C%2520such%2520as%2520novel%2520sensor%2520modalities%2520in%2520clinical%250Adata.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520PFML%2520through%2520complex%252C%2520real-life%250Aclassification%2520tasks%2520across%2520three%2520different%2520data%2520modalities%253A%2520infant%2520posture%2520and%250Amovement%2520classification%2520from%2520multi-sensor%2520inertial%2520measurement%2520unit%2520data%252C%250Aemotion%2520recognition%2520from%2520speech%2520data%252C%2520and%2520sleep%2520stage%2520classification%2520from%2520EEG%250Adata.%2520The%2520results%2520show%2520that%2520PFML%2520is%2520superior%2520to%2520a%2520conceptually%2520similar%2520SSL%250Amethod%2520and%2520a%2520contrastive%2520learning-based%2520SSL%2520method.%2520Additionally%252C%2520PFML%2520is%2520on%250Apar%2520with%2520the%2520current%2520state-of-the-art%2520SSL%2520method%252C%2520while%2520also%2520being%2520conceptually%250Asimpler%2520and%2520without%2520suffering%2520from%2520representation%2520collapse.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10087v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PFML%3A%20Self-Supervised%20Learning%20of%20Time-Series%20Data%20Without%0A%20%20Representation%20Collapse&entry.906535625=Einari%20Vaaras%20and%20Manu%20Airaksinen%20and%20Okko%20R%C3%A4s%C3%A4nen&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20is%20a%20data-driven%20learning%20approach%20that%0Autilizes%20the%20innate%20structure%20of%20the%20data%20to%20guide%20the%20learning%20process.%20In%0Acontrast%20to%20supervised%20learning%2C%20which%20depends%20on%20external%20labels%2C%20SSL%20utilizes%0Athe%20inherent%20characteristics%20of%20the%20data%20to%20produce%20its%20own%20supervisory%20signal.%0AHowever%2C%20one%20frequent%20issue%20with%20SSL%20methods%20is%20representation%20collapse%2C%20where%0Athe%20model%20outputs%20a%20constant%20input-invariant%20feature%20representation.%20This%20issue%0Ahinders%20the%20potential%20application%20of%20SSL%20methods%20to%20new%20data%20modalities%2C%20as%0Atrying%20to%20avoid%20representation%20collapse%20wastes%20researchers%27%20time%20and%20effort.%0AThis%20paper%20introduces%20a%20novel%20SSL%20algorithm%20for%20time-series%20data%20called%0APrediction%20of%20Functionals%20from%20Masked%20Latents%20%28PFML%29.%20Instead%20of%20predicting%0Amasked%20input%20signals%20or%20their%20latent%20representations%20directly%2C%20PFML%20operates%20by%0Apredicting%20statistical%20functionals%20of%20the%20input%20signal%20corresponding%20to%20masked%0Aembeddings%2C%20given%20a%20sequence%20of%20unmasked%20embeddings.%20The%20algorithm%20is%20designed%0Ato%20avoid%20representation%20collapse%2C%20rendering%20it%20straightforwardly%20applicable%20to%0Adifferent%20time-series%20data%20domains%2C%20such%20as%20novel%20sensor%20modalities%20in%20clinical%0Adata.%20We%20demonstrate%20the%20effectiveness%20of%20PFML%20through%20complex%2C%20real-life%0Aclassification%20tasks%20across%20three%20different%20data%20modalities%3A%20infant%20posture%20and%0Amovement%20classification%20from%20multi-sensor%20inertial%20measurement%20unit%20data%2C%0Aemotion%20recognition%20from%20speech%20data%2C%20and%20sleep%20stage%20classification%20from%20EEG%0Adata.%20The%20results%20show%20that%20PFML%20is%20superior%20to%20a%20conceptually%20similar%20SSL%0Amethod%20and%20a%20contrastive%20learning-based%20SSL%20method.%20Additionally%2C%20PFML%20is%20on%0Apar%20with%20the%20current%20state-of-the-art%20SSL%20method%2C%20while%20also%20being%20conceptually%0Asimpler%20and%20without%20suffering%20from%20representation%20collapse.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10087v2&entry.124074799=Read"},
{"title": "Robust Conformal Prediction Using Privileged Information", "author": "Shai Feldman and Yaniv Romano", "abstract": "  We develop a method to generate prediction sets with a guaranteed coverage\nrate that is robust to corruptions in the training data, such as missing or\nnoisy variables. Our approach builds on conformal prediction, a powerful\nframework to construct prediction sets that are valid under the i.i.d\nassumption. Importantly, naively applying conformal prediction does not provide\nreliable predictions in this setting, due to the distribution shift induced by\nthe corruptions. To account for the distribution shift, we assume access to\nprivileged information (PI). The PI is formulated as additional features that\nexplain the distribution shift, however, they are only available during\ntraining and absent at test time. We approach this problem by introducing a\nnovel generalization of weighted conformal prediction and support our method\nwith theoretical coverage guarantees. Empirical experiments on both real and\nsynthetic datasets indicate that our approach achieves a valid coverage rate\nand constructs more informative predictions compared to existing methods, which\nare not supported by theoretical guarantees.\n", "link": "http://arxiv.org/abs/2406.05405v3", "date": "2025-01-09", "relevancy": 1.9791, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5002}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4996}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Conformal%20Prediction%20Using%20Privileged%20Information&body=Title%3A%20Robust%20Conformal%20Prediction%20Using%20Privileged%20Information%0AAuthor%3A%20Shai%20Feldman%20and%20Yaniv%20Romano%0AAbstract%3A%20%20%20We%20develop%20a%20method%20to%20generate%20prediction%20sets%20with%20a%20guaranteed%20coverage%0Arate%20that%20is%20robust%20to%20corruptions%20in%20the%20training%20data%2C%20such%20as%20missing%20or%0Anoisy%20variables.%20Our%20approach%20builds%20on%20conformal%20prediction%2C%20a%20powerful%0Aframework%20to%20construct%20prediction%20sets%20that%20are%20valid%20under%20the%20i.i.d%0Aassumption.%20Importantly%2C%20naively%20applying%20conformal%20prediction%20does%20not%20provide%0Areliable%20predictions%20in%20this%20setting%2C%20due%20to%20the%20distribution%20shift%20induced%20by%0Athe%20corruptions.%20To%20account%20for%20the%20distribution%20shift%2C%20we%20assume%20access%20to%0Aprivileged%20information%20%28PI%29.%20The%20PI%20is%20formulated%20as%20additional%20features%20that%0Aexplain%20the%20distribution%20shift%2C%20however%2C%20they%20are%20only%20available%20during%0Atraining%20and%20absent%20at%20test%20time.%20We%20approach%20this%20problem%20by%20introducing%20a%0Anovel%20generalization%20of%20weighted%20conformal%20prediction%20and%20support%20our%20method%0Awith%20theoretical%20coverage%20guarantees.%20Empirical%20experiments%20on%20both%20real%20and%0Asynthetic%20datasets%20indicate%20that%20our%20approach%20achieves%20a%20valid%20coverage%20rate%0Aand%20constructs%20more%20informative%20predictions%20compared%20to%20existing%20methods%2C%20which%0Aare%20not%20supported%20by%20theoretical%20guarantees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05405v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Conformal%2520Prediction%2520Using%2520Privileged%2520Information%26entry.906535625%3DShai%2520Feldman%2520and%2520Yaniv%2520Romano%26entry.1292438233%3D%2520%2520We%2520develop%2520a%2520method%2520to%2520generate%2520prediction%2520sets%2520with%2520a%2520guaranteed%2520coverage%250Arate%2520that%2520is%2520robust%2520to%2520corruptions%2520in%2520the%2520training%2520data%252C%2520such%2520as%2520missing%2520or%250Anoisy%2520variables.%2520Our%2520approach%2520builds%2520on%2520conformal%2520prediction%252C%2520a%2520powerful%250Aframework%2520to%2520construct%2520prediction%2520sets%2520that%2520are%2520valid%2520under%2520the%2520i.i.d%250Aassumption.%2520Importantly%252C%2520naively%2520applying%2520conformal%2520prediction%2520does%2520not%2520provide%250Areliable%2520predictions%2520in%2520this%2520setting%252C%2520due%2520to%2520the%2520distribution%2520shift%2520induced%2520by%250Athe%2520corruptions.%2520To%2520account%2520for%2520the%2520distribution%2520shift%252C%2520we%2520assume%2520access%2520to%250Aprivileged%2520information%2520%2528PI%2529.%2520The%2520PI%2520is%2520formulated%2520as%2520additional%2520features%2520that%250Aexplain%2520the%2520distribution%2520shift%252C%2520however%252C%2520they%2520are%2520only%2520available%2520during%250Atraining%2520and%2520absent%2520at%2520test%2520time.%2520We%2520approach%2520this%2520problem%2520by%2520introducing%2520a%250Anovel%2520generalization%2520of%2520weighted%2520conformal%2520prediction%2520and%2520support%2520our%2520method%250Awith%2520theoretical%2520coverage%2520guarantees.%2520Empirical%2520experiments%2520on%2520both%2520real%2520and%250Asynthetic%2520datasets%2520indicate%2520that%2520our%2520approach%2520achieves%2520a%2520valid%2520coverage%2520rate%250Aand%2520constructs%2520more%2520informative%2520predictions%2520compared%2520to%2520existing%2520methods%252C%2520which%250Aare%2520not%2520supported%2520by%2520theoretical%2520guarantees.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05405v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Conformal%20Prediction%20Using%20Privileged%20Information&entry.906535625=Shai%20Feldman%20and%20Yaniv%20Romano&entry.1292438233=%20%20We%20develop%20a%20method%20to%20generate%20prediction%20sets%20with%20a%20guaranteed%20coverage%0Arate%20that%20is%20robust%20to%20corruptions%20in%20the%20training%20data%2C%20such%20as%20missing%20or%0Anoisy%20variables.%20Our%20approach%20builds%20on%20conformal%20prediction%2C%20a%20powerful%0Aframework%20to%20construct%20prediction%20sets%20that%20are%20valid%20under%20the%20i.i.d%0Aassumption.%20Importantly%2C%20naively%20applying%20conformal%20prediction%20does%20not%20provide%0Areliable%20predictions%20in%20this%20setting%2C%20due%20to%20the%20distribution%20shift%20induced%20by%0Athe%20corruptions.%20To%20account%20for%20the%20distribution%20shift%2C%20we%20assume%20access%20to%0Aprivileged%20information%20%28PI%29.%20The%20PI%20is%20formulated%20as%20additional%20features%20that%0Aexplain%20the%20distribution%20shift%2C%20however%2C%20they%20are%20only%20available%20during%0Atraining%20and%20absent%20at%20test%20time.%20We%20approach%20this%20problem%20by%20introducing%20a%0Anovel%20generalization%20of%20weighted%20conformal%20prediction%20and%20support%20our%20method%0Awith%20theoretical%20coverage%20guarantees.%20Empirical%20experiments%20on%20both%20real%20and%0Asynthetic%20datasets%20indicate%20that%20our%20approach%20achieves%20a%20valid%20coverage%20rate%0Aand%20constructs%20more%20informative%20predictions%20compared%20to%20existing%20methods%2C%20which%0Aare%20not%20supported%20by%20theoretical%20guarantees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05405v3&entry.124074799=Read"},
{"title": "Zero-1-to-G: Taming Pretrained 2D Diffusion Model for Direct 3D\n  Generation", "author": "Xuyi Meng and Chen Wang and Jiahui Lei and Kostas Daniilidis and Jiatao Gu and Lingjie Liu", "abstract": "  Recent advances in 2D image generation have achieved remarkable\nquality,largely driven by the capacity of diffusion models and the availability\nof large-scale datasets. However, direct 3D generation is still constrained by\nthe scarcity and lower fidelity of 3D datasets. In this paper, we introduce\nZero-1-to-G, a novel approach that addresses this problem by enabling direct\nsingle-view generation on Gaussian splats using pretrained 2D diffusion models.\nOur key insight is that Gaussian splats, a 3D representation, can be decomposed\ninto multi-view images encoding different attributes. This reframes the\nchallenging task of direct 3D generation within a 2D diffusion framework,\nallowing us to leverage the rich priors of pretrained 2D diffusion models. To\nincorporate 3D awareness, we introduce cross-view and cross-attribute attention\nlayers, which capture complex correlations and enforce 3D consistency across\ngenerated splats. This makes Zero-1-to-G the first direct image-to-3D\ngenerative model to effectively utilize pretrained 2D diffusion priors,\nenabling efficient training and improved generalization to unseen objects.\nExtensive experiments on both synthetic and in-the-wild datasets demonstrate\nsuperior performance in 3D object generation, offering a new approach to\nhigh-quality 3D generation.\n", "link": "http://arxiv.org/abs/2501.05427v1", "date": "2025-01-09", "relevancy": 1.9761, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6755}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6542}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-1-to-G%3A%20Taming%20Pretrained%202D%20Diffusion%20Model%20for%20Direct%203D%0A%20%20Generation&body=Title%3A%20Zero-1-to-G%3A%20Taming%20Pretrained%202D%20Diffusion%20Model%20for%20Direct%203D%0A%20%20Generation%0AAuthor%3A%20Xuyi%20Meng%20and%20Chen%20Wang%20and%20Jiahui%20Lei%20and%20Kostas%20Daniilidis%20and%20Jiatao%20Gu%20and%20Lingjie%20Liu%0AAbstract%3A%20%20%20Recent%20advances%20in%202D%20image%20generation%20have%20achieved%20remarkable%0Aquality%2Clargely%20driven%20by%20the%20capacity%20of%20diffusion%20models%20and%20the%20availability%0Aof%20large-scale%20datasets.%20However%2C%20direct%203D%20generation%20is%20still%20constrained%20by%0Athe%20scarcity%20and%20lower%20fidelity%20of%203D%20datasets.%20In%20this%20paper%2C%20we%20introduce%0AZero-1-to-G%2C%20a%20novel%20approach%20that%20addresses%20this%20problem%20by%20enabling%20direct%0Asingle-view%20generation%20on%20Gaussian%20splats%20using%20pretrained%202D%20diffusion%20models.%0AOur%20key%20insight%20is%20that%20Gaussian%20splats%2C%20a%203D%20representation%2C%20can%20be%20decomposed%0Ainto%20multi-view%20images%20encoding%20different%20attributes.%20This%20reframes%20the%0Achallenging%20task%20of%20direct%203D%20generation%20within%20a%202D%20diffusion%20framework%2C%0Aallowing%20us%20to%20leverage%20the%20rich%20priors%20of%20pretrained%202D%20diffusion%20models.%20To%0Aincorporate%203D%20awareness%2C%20we%20introduce%20cross-view%20and%20cross-attribute%20attention%0Alayers%2C%20which%20capture%20complex%20correlations%20and%20enforce%203D%20consistency%20across%0Agenerated%20splats.%20This%20makes%20Zero-1-to-G%20the%20first%20direct%20image-to-3D%0Agenerative%20model%20to%20effectively%20utilize%20pretrained%202D%20diffusion%20priors%2C%0Aenabling%20efficient%20training%20and%20improved%20generalization%20to%20unseen%20objects.%0AExtensive%20experiments%20on%20both%20synthetic%20and%20in-the-wild%20datasets%20demonstrate%0Asuperior%20performance%20in%203D%20object%20generation%2C%20offering%20a%20new%20approach%20to%0Ahigh-quality%203D%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05427v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-1-to-G%253A%2520Taming%2520Pretrained%25202D%2520Diffusion%2520Model%2520for%2520Direct%25203D%250A%2520%2520Generation%26entry.906535625%3DXuyi%2520Meng%2520and%2520Chen%2520Wang%2520and%2520Jiahui%2520Lei%2520and%2520Kostas%2520Daniilidis%2520and%2520Jiatao%2520Gu%2520and%2520Lingjie%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%25202D%2520image%2520generation%2520have%2520achieved%2520remarkable%250Aquality%252Clargely%2520driven%2520by%2520the%2520capacity%2520of%2520diffusion%2520models%2520and%2520the%2520availability%250Aof%2520large-scale%2520datasets.%2520However%252C%2520direct%25203D%2520generation%2520is%2520still%2520constrained%2520by%250Athe%2520scarcity%2520and%2520lower%2520fidelity%2520of%25203D%2520datasets.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AZero-1-to-G%252C%2520a%2520novel%2520approach%2520that%2520addresses%2520this%2520problem%2520by%2520enabling%2520direct%250Asingle-view%2520generation%2520on%2520Gaussian%2520splats%2520using%2520pretrained%25202D%2520diffusion%2520models.%250AOur%2520key%2520insight%2520is%2520that%2520Gaussian%2520splats%252C%2520a%25203D%2520representation%252C%2520can%2520be%2520decomposed%250Ainto%2520multi-view%2520images%2520encoding%2520different%2520attributes.%2520This%2520reframes%2520the%250Achallenging%2520task%2520of%2520direct%25203D%2520generation%2520within%2520a%25202D%2520diffusion%2520framework%252C%250Aallowing%2520us%2520to%2520leverage%2520the%2520rich%2520priors%2520of%2520pretrained%25202D%2520diffusion%2520models.%2520To%250Aincorporate%25203D%2520awareness%252C%2520we%2520introduce%2520cross-view%2520and%2520cross-attribute%2520attention%250Alayers%252C%2520which%2520capture%2520complex%2520correlations%2520and%2520enforce%25203D%2520consistency%2520across%250Agenerated%2520splats.%2520This%2520makes%2520Zero-1-to-G%2520the%2520first%2520direct%2520image-to-3D%250Agenerative%2520model%2520to%2520effectively%2520utilize%2520pretrained%25202D%2520diffusion%2520priors%252C%250Aenabling%2520efficient%2520training%2520and%2520improved%2520generalization%2520to%2520unseen%2520objects.%250AExtensive%2520experiments%2520on%2520both%2520synthetic%2520and%2520in-the-wild%2520datasets%2520demonstrate%250Asuperior%2520performance%2520in%25203D%2520object%2520generation%252C%2520offering%2520a%2520new%2520approach%2520to%250Ahigh-quality%25203D%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05427v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-1-to-G%3A%20Taming%20Pretrained%202D%20Diffusion%20Model%20for%20Direct%203D%0A%20%20Generation&entry.906535625=Xuyi%20Meng%20and%20Chen%20Wang%20and%20Jiahui%20Lei%20and%20Kostas%20Daniilidis%20and%20Jiatao%20Gu%20and%20Lingjie%20Liu&entry.1292438233=%20%20Recent%20advances%20in%202D%20image%20generation%20have%20achieved%20remarkable%0Aquality%2Clargely%20driven%20by%20the%20capacity%20of%20diffusion%20models%20and%20the%20availability%0Aof%20large-scale%20datasets.%20However%2C%20direct%203D%20generation%20is%20still%20constrained%20by%0Athe%20scarcity%20and%20lower%20fidelity%20of%203D%20datasets.%20In%20this%20paper%2C%20we%20introduce%0AZero-1-to-G%2C%20a%20novel%20approach%20that%20addresses%20this%20problem%20by%20enabling%20direct%0Asingle-view%20generation%20on%20Gaussian%20splats%20using%20pretrained%202D%20diffusion%20models.%0AOur%20key%20insight%20is%20that%20Gaussian%20splats%2C%20a%203D%20representation%2C%20can%20be%20decomposed%0Ainto%20multi-view%20images%20encoding%20different%20attributes.%20This%20reframes%20the%0Achallenging%20task%20of%20direct%203D%20generation%20within%20a%202D%20diffusion%20framework%2C%0Aallowing%20us%20to%20leverage%20the%20rich%20priors%20of%20pretrained%202D%20diffusion%20models.%20To%0Aincorporate%203D%20awareness%2C%20we%20introduce%20cross-view%20and%20cross-attribute%20attention%0Alayers%2C%20which%20capture%20complex%20correlations%20and%20enforce%203D%20consistency%20across%0Agenerated%20splats.%20This%20makes%20Zero-1-to-G%20the%20first%20direct%20image-to-3D%0Agenerative%20model%20to%20effectively%20utilize%20pretrained%202D%20diffusion%20priors%2C%0Aenabling%20efficient%20training%20and%20improved%20generalization%20to%20unseen%20objects.%0AExtensive%20experiments%20on%20both%20synthetic%20and%20in-the-wild%20datasets%20demonstrate%0Asuperior%20performance%20in%203D%20object%20generation%2C%20offering%20a%20new%20approach%20to%0Ahigh-quality%203D%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05427v1&entry.124074799=Read"},
{"title": "Distributed Learning and Inference Systems: A Networking Perspective", "author": "Hesham G. Moussa and Arashmid Akhavain and S. Maryam Hosseini and Bill McCormick", "abstract": "  Machine learning models have achieved, and in some cases surpassed,\nhuman-level performance in various tasks, mainly through centralized training\nof static models and the use of large models stored in centralized clouds for\ninference. However, this centralized approach has several drawbacks, including\nprivacy concerns, high storage demands, a single point of failure, and\nsignificant computing requirements. These challenges have driven interest in\ndeveloping alternative decentralized and distributed methods for AI training\nand inference. Distribution introduces additional complexity, as it requires\nmanaging multiple moving parts. To address these complexities and fill a gap in\nthe development of distributed AI systems, this work proposes a novel\nframework, Data and Dynamics-Aware Inference and Training Networks (DA-ITN).\nThe different components of DA-ITN and their functions are explored, and the\nassociated challenges and research areas are highlighted.\n", "link": "http://arxiv.org/abs/2501.05323v1", "date": "2025-01-09", "relevancy": 1.9648, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5223}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4818}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distributed%20Learning%20and%20Inference%20Systems%3A%20A%20Networking%20Perspective&body=Title%3A%20Distributed%20Learning%20and%20Inference%20Systems%3A%20A%20Networking%20Perspective%0AAuthor%3A%20Hesham%20G.%20Moussa%20and%20Arashmid%20Akhavain%20and%20S.%20Maryam%20Hosseini%20and%20Bill%20McCormick%0AAbstract%3A%20%20%20Machine%20learning%20models%20have%20achieved%2C%20and%20in%20some%20cases%20surpassed%2C%0Ahuman-level%20performance%20in%20various%20tasks%2C%20mainly%20through%20centralized%20training%0Aof%20static%20models%20and%20the%20use%20of%20large%20models%20stored%20in%20centralized%20clouds%20for%0Ainference.%20However%2C%20this%20centralized%20approach%20has%20several%20drawbacks%2C%20including%0Aprivacy%20concerns%2C%20high%20storage%20demands%2C%20a%20single%20point%20of%20failure%2C%20and%0Asignificant%20computing%20requirements.%20These%20challenges%20have%20driven%20interest%20in%0Adeveloping%20alternative%20decentralized%20and%20distributed%20methods%20for%20AI%20training%0Aand%20inference.%20Distribution%20introduces%20additional%20complexity%2C%20as%20it%20requires%0Amanaging%20multiple%20moving%20parts.%20To%20address%20these%20complexities%20and%20fill%20a%20gap%20in%0Athe%20development%20of%20distributed%20AI%20systems%2C%20this%20work%20proposes%20a%20novel%0Aframework%2C%20Data%20and%20Dynamics-Aware%20Inference%20and%20Training%20Networks%20%28DA-ITN%29.%0AThe%20different%20components%20of%20DA-ITN%20and%20their%20functions%20are%20explored%2C%20and%20the%0Aassociated%20challenges%20and%20research%20areas%20are%20highlighted.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05323v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistributed%2520Learning%2520and%2520Inference%2520Systems%253A%2520A%2520Networking%2520Perspective%26entry.906535625%3DHesham%2520G.%2520Moussa%2520and%2520Arashmid%2520Akhavain%2520and%2520S.%2520Maryam%2520Hosseini%2520and%2520Bill%2520McCormick%26entry.1292438233%3D%2520%2520Machine%2520learning%2520models%2520have%2520achieved%252C%2520and%2520in%2520some%2520cases%2520surpassed%252C%250Ahuman-level%2520performance%2520in%2520various%2520tasks%252C%2520mainly%2520through%2520centralized%2520training%250Aof%2520static%2520models%2520and%2520the%2520use%2520of%2520large%2520models%2520stored%2520in%2520centralized%2520clouds%2520for%250Ainference.%2520However%252C%2520this%2520centralized%2520approach%2520has%2520several%2520drawbacks%252C%2520including%250Aprivacy%2520concerns%252C%2520high%2520storage%2520demands%252C%2520a%2520single%2520point%2520of%2520failure%252C%2520and%250Asignificant%2520computing%2520requirements.%2520These%2520challenges%2520have%2520driven%2520interest%2520in%250Adeveloping%2520alternative%2520decentralized%2520and%2520distributed%2520methods%2520for%2520AI%2520training%250Aand%2520inference.%2520Distribution%2520introduces%2520additional%2520complexity%252C%2520as%2520it%2520requires%250Amanaging%2520multiple%2520moving%2520parts.%2520To%2520address%2520these%2520complexities%2520and%2520fill%2520a%2520gap%2520in%250Athe%2520development%2520of%2520distributed%2520AI%2520systems%252C%2520this%2520work%2520proposes%2520a%2520novel%250Aframework%252C%2520Data%2520and%2520Dynamics-Aware%2520Inference%2520and%2520Training%2520Networks%2520%2528DA-ITN%2529.%250AThe%2520different%2520components%2520of%2520DA-ITN%2520and%2520their%2520functions%2520are%2520explored%252C%2520and%2520the%250Aassociated%2520challenges%2520and%2520research%2520areas%2520are%2520highlighted.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05323v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributed%20Learning%20and%20Inference%20Systems%3A%20A%20Networking%20Perspective&entry.906535625=Hesham%20G.%20Moussa%20and%20Arashmid%20Akhavain%20and%20S.%20Maryam%20Hosseini%20and%20Bill%20McCormick&entry.1292438233=%20%20Machine%20learning%20models%20have%20achieved%2C%20and%20in%20some%20cases%20surpassed%2C%0Ahuman-level%20performance%20in%20various%20tasks%2C%20mainly%20through%20centralized%20training%0Aof%20static%20models%20and%20the%20use%20of%20large%20models%20stored%20in%20centralized%20clouds%20for%0Ainference.%20However%2C%20this%20centralized%20approach%20has%20several%20drawbacks%2C%20including%0Aprivacy%20concerns%2C%20high%20storage%20demands%2C%20a%20single%20point%20of%20failure%2C%20and%0Asignificant%20computing%20requirements.%20These%20challenges%20have%20driven%20interest%20in%0Adeveloping%20alternative%20decentralized%20and%20distributed%20methods%20for%20AI%20training%0Aand%20inference.%20Distribution%20introduces%20additional%20complexity%2C%20as%20it%20requires%0Amanaging%20multiple%20moving%20parts.%20To%20address%20these%20complexities%20and%20fill%20a%20gap%20in%0Athe%20development%20of%20distributed%20AI%20systems%2C%20this%20work%20proposes%20a%20novel%0Aframework%2C%20Data%20and%20Dynamics-Aware%20Inference%20and%20Training%20Networks%20%28DA-ITN%29.%0AThe%20different%20components%20of%20DA-ITN%20and%20their%20functions%20are%20explored%2C%20and%20the%0Aassociated%20challenges%20and%20research%20areas%20are%20highlighted.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05323v1&entry.124074799=Read"},
{"title": "Entangled Mean Estimation in High-Dimensions", "author": "Ilias Diakonikolas and Daniel M. Kane and Sihan Liu and Thanasis Pittas", "abstract": "  We study the task of high-dimensional entangled mean estimation in the\nsubset-of-signals model. Specifically, given $N$ independent random points\n$x_1,\\ldots,x_N$ in $\\mathbb{R}^D$ and a parameter $\\alpha \\in (0, 1)$ such\nthat each $x_i$ is drawn from a Gaussian with mean $\\mu$ and unknown\ncovariance, and an unknown $\\alpha$-fraction of the points have\nidentity-bounded covariances, the goal is to estimate the common mean $\\mu$.\nThe one-dimensional version of this task has received significant attention in\ntheoretical computer science and statistics over the past decades. Recent work\n[LY20; CV24] has given near-optimal upper and lower bounds for the\none-dimensional setting. On the other hand, our understanding of even the\ninformation-theoretic aspects of the multivariate setting has remained limited.\n  In this work, we design a computationally efficient algorithm achieving an\ninformation-theoretically near-optimal error. Specifically, we show that the\noptimal error (up to polylogarithmic factors) is $f(\\alpha,N) + \\sqrt{D/(\\alpha\nN)}$, where the term $f(\\alpha,N)$ is the error of the one-dimensional problem\nand the second term is the sub-Gaussian error rate. Our algorithmic approach\nemploys an iterative refinement strategy, whereby we progressively learn more\naccurate approximations $\\hat \\mu$ to $\\mu$. This is achieved via a novel\nrejection sampling procedure that removes points significantly deviating from\n$\\hat \\mu$, as an attempt to filter out unusually noisy samples. A complication\nthat arises is that rejection sampling introduces bias in the distribution of\nthe remaining points. To address this issue, we perform a careful analysis of\nthe bias, develop an iterative dimension-reduction strategy, and employ a novel\nsubroutine inspired by list-decodable learning that leverages the\none-dimensional result.\n", "link": "http://arxiv.org/abs/2501.05425v1", "date": "2025-01-09", "relevancy": 1.9554, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4947}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4891}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Entangled%20Mean%20Estimation%20in%20High-Dimensions&body=Title%3A%20Entangled%20Mean%20Estimation%20in%20High-Dimensions%0AAuthor%3A%20Ilias%20Diakonikolas%20and%20Daniel%20M.%20Kane%20and%20Sihan%20Liu%20and%20Thanasis%20Pittas%0AAbstract%3A%20%20%20We%20study%20the%20task%20of%20high-dimensional%20entangled%20mean%20estimation%20in%20the%0Asubset-of-signals%20model.%20Specifically%2C%20given%20%24N%24%20independent%20random%20points%0A%24x_1%2C%5Cldots%2Cx_N%24%20in%20%24%5Cmathbb%7BR%7D%5ED%24%20and%20a%20parameter%20%24%5Calpha%20%5Cin%20%280%2C%201%29%24%20such%0Athat%20each%20%24x_i%24%20is%20drawn%20from%20a%20Gaussian%20with%20mean%20%24%5Cmu%24%20and%20unknown%0Acovariance%2C%20and%20an%20unknown%20%24%5Calpha%24-fraction%20of%20the%20points%20have%0Aidentity-bounded%20covariances%2C%20the%20goal%20is%20to%20estimate%20the%20common%20mean%20%24%5Cmu%24.%0AThe%20one-dimensional%20version%20of%20this%20task%20has%20received%20significant%20attention%20in%0Atheoretical%20computer%20science%20and%20statistics%20over%20the%20past%20decades.%20Recent%20work%0A%5BLY20%3B%20CV24%5D%20has%20given%20near-optimal%20upper%20and%20lower%20bounds%20for%20the%0Aone-dimensional%20setting.%20On%20the%20other%20hand%2C%20our%20understanding%20of%20even%20the%0Ainformation-theoretic%20aspects%20of%20the%20multivariate%20setting%20has%20remained%20limited.%0A%20%20In%20this%20work%2C%20we%20design%20a%20computationally%20efficient%20algorithm%20achieving%20an%0Ainformation-theoretically%20near-optimal%20error.%20Specifically%2C%20we%20show%20that%20the%0Aoptimal%20error%20%28up%20to%20polylogarithmic%20factors%29%20is%20%24f%28%5Calpha%2CN%29%20%2B%20%5Csqrt%7BD/%28%5Calpha%0AN%29%7D%24%2C%20where%20the%20term%20%24f%28%5Calpha%2CN%29%24%20is%20the%20error%20of%20the%20one-dimensional%20problem%0Aand%20the%20second%20term%20is%20the%20sub-Gaussian%20error%20rate.%20Our%20algorithmic%20approach%0Aemploys%20an%20iterative%20refinement%20strategy%2C%20whereby%20we%20progressively%20learn%20more%0Aaccurate%20approximations%20%24%5Chat%20%5Cmu%24%20to%20%24%5Cmu%24.%20This%20is%20achieved%20via%20a%20novel%0Arejection%20sampling%20procedure%20that%20removes%20points%20significantly%20deviating%20from%0A%24%5Chat%20%5Cmu%24%2C%20as%20an%20attempt%20to%20filter%20out%20unusually%20noisy%20samples.%20A%20complication%0Athat%20arises%20is%20that%20rejection%20sampling%20introduces%20bias%20in%20the%20distribution%20of%0Athe%20remaining%20points.%20To%20address%20this%20issue%2C%20we%20perform%20a%20careful%20analysis%20of%0Athe%20bias%2C%20develop%20an%20iterative%20dimension-reduction%20strategy%2C%20and%20employ%20a%20novel%0Asubroutine%20inspired%20by%20list-decodable%20learning%20that%20leverages%20the%0Aone-dimensional%20result.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEntangled%2520Mean%2520Estimation%2520in%2520High-Dimensions%26entry.906535625%3DIlias%2520Diakonikolas%2520and%2520Daniel%2520M.%2520Kane%2520and%2520Sihan%2520Liu%2520and%2520Thanasis%2520Pittas%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520task%2520of%2520high-dimensional%2520entangled%2520mean%2520estimation%2520in%2520the%250Asubset-of-signals%2520model.%2520Specifically%252C%2520given%2520%2524N%2524%2520independent%2520random%2520points%250A%2524x_1%252C%255Cldots%252Cx_N%2524%2520in%2520%2524%255Cmathbb%257BR%257D%255ED%2524%2520and%2520a%2520parameter%2520%2524%255Calpha%2520%255Cin%2520%25280%252C%25201%2529%2524%2520such%250Athat%2520each%2520%2524x_i%2524%2520is%2520drawn%2520from%2520a%2520Gaussian%2520with%2520mean%2520%2524%255Cmu%2524%2520and%2520unknown%250Acovariance%252C%2520and%2520an%2520unknown%2520%2524%255Calpha%2524-fraction%2520of%2520the%2520points%2520have%250Aidentity-bounded%2520covariances%252C%2520the%2520goal%2520is%2520to%2520estimate%2520the%2520common%2520mean%2520%2524%255Cmu%2524.%250AThe%2520one-dimensional%2520version%2520of%2520this%2520task%2520has%2520received%2520significant%2520attention%2520in%250Atheoretical%2520computer%2520science%2520and%2520statistics%2520over%2520the%2520past%2520decades.%2520Recent%2520work%250A%255BLY20%253B%2520CV24%255D%2520has%2520given%2520near-optimal%2520upper%2520and%2520lower%2520bounds%2520for%2520the%250Aone-dimensional%2520setting.%2520On%2520the%2520other%2520hand%252C%2520our%2520understanding%2520of%2520even%2520the%250Ainformation-theoretic%2520aspects%2520of%2520the%2520multivariate%2520setting%2520has%2520remained%2520limited.%250A%2520%2520In%2520this%2520work%252C%2520we%2520design%2520a%2520computationally%2520efficient%2520algorithm%2520achieving%2520an%250Ainformation-theoretically%2520near-optimal%2520error.%2520Specifically%252C%2520we%2520show%2520that%2520the%250Aoptimal%2520error%2520%2528up%2520to%2520polylogarithmic%2520factors%2529%2520is%2520%2524f%2528%255Calpha%252CN%2529%2520%252B%2520%255Csqrt%257BD/%2528%255Calpha%250AN%2529%257D%2524%252C%2520where%2520the%2520term%2520%2524f%2528%255Calpha%252CN%2529%2524%2520is%2520the%2520error%2520of%2520the%2520one-dimensional%2520problem%250Aand%2520the%2520second%2520term%2520is%2520the%2520sub-Gaussian%2520error%2520rate.%2520Our%2520algorithmic%2520approach%250Aemploys%2520an%2520iterative%2520refinement%2520strategy%252C%2520whereby%2520we%2520progressively%2520learn%2520more%250Aaccurate%2520approximations%2520%2524%255Chat%2520%255Cmu%2524%2520to%2520%2524%255Cmu%2524.%2520This%2520is%2520achieved%2520via%2520a%2520novel%250Arejection%2520sampling%2520procedure%2520that%2520removes%2520points%2520significantly%2520deviating%2520from%250A%2524%255Chat%2520%255Cmu%2524%252C%2520as%2520an%2520attempt%2520to%2520filter%2520out%2520unusually%2520noisy%2520samples.%2520A%2520complication%250Athat%2520arises%2520is%2520that%2520rejection%2520sampling%2520introduces%2520bias%2520in%2520the%2520distribution%2520of%250Athe%2520remaining%2520points.%2520To%2520address%2520this%2520issue%252C%2520we%2520perform%2520a%2520careful%2520analysis%2520of%250Athe%2520bias%252C%2520develop%2520an%2520iterative%2520dimension-reduction%2520strategy%252C%2520and%2520employ%2520a%2520novel%250Asubroutine%2520inspired%2520by%2520list-decodable%2520learning%2520that%2520leverages%2520the%250Aone-dimensional%2520result.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Entangled%20Mean%20Estimation%20in%20High-Dimensions&entry.906535625=Ilias%20Diakonikolas%20and%20Daniel%20M.%20Kane%20and%20Sihan%20Liu%20and%20Thanasis%20Pittas&entry.1292438233=%20%20We%20study%20the%20task%20of%20high-dimensional%20entangled%20mean%20estimation%20in%20the%0Asubset-of-signals%20model.%20Specifically%2C%20given%20%24N%24%20independent%20random%20points%0A%24x_1%2C%5Cldots%2Cx_N%24%20in%20%24%5Cmathbb%7BR%7D%5ED%24%20and%20a%20parameter%20%24%5Calpha%20%5Cin%20%280%2C%201%29%24%20such%0Athat%20each%20%24x_i%24%20is%20drawn%20from%20a%20Gaussian%20with%20mean%20%24%5Cmu%24%20and%20unknown%0Acovariance%2C%20and%20an%20unknown%20%24%5Calpha%24-fraction%20of%20the%20points%20have%0Aidentity-bounded%20covariances%2C%20the%20goal%20is%20to%20estimate%20the%20common%20mean%20%24%5Cmu%24.%0AThe%20one-dimensional%20version%20of%20this%20task%20has%20received%20significant%20attention%20in%0Atheoretical%20computer%20science%20and%20statistics%20over%20the%20past%20decades.%20Recent%20work%0A%5BLY20%3B%20CV24%5D%20has%20given%20near-optimal%20upper%20and%20lower%20bounds%20for%20the%0Aone-dimensional%20setting.%20On%20the%20other%20hand%2C%20our%20understanding%20of%20even%20the%0Ainformation-theoretic%20aspects%20of%20the%20multivariate%20setting%20has%20remained%20limited.%0A%20%20In%20this%20work%2C%20we%20design%20a%20computationally%20efficient%20algorithm%20achieving%20an%0Ainformation-theoretically%20near-optimal%20error.%20Specifically%2C%20we%20show%20that%20the%0Aoptimal%20error%20%28up%20to%20polylogarithmic%20factors%29%20is%20%24f%28%5Calpha%2CN%29%20%2B%20%5Csqrt%7BD/%28%5Calpha%0AN%29%7D%24%2C%20where%20the%20term%20%24f%28%5Calpha%2CN%29%24%20is%20the%20error%20of%20the%20one-dimensional%20problem%0Aand%20the%20second%20term%20is%20the%20sub-Gaussian%20error%20rate.%20Our%20algorithmic%20approach%0Aemploys%20an%20iterative%20refinement%20strategy%2C%20whereby%20we%20progressively%20learn%20more%0Aaccurate%20approximations%20%24%5Chat%20%5Cmu%24%20to%20%24%5Cmu%24.%20This%20is%20achieved%20via%20a%20novel%0Arejection%20sampling%20procedure%20that%20removes%20points%20significantly%20deviating%20from%0A%24%5Chat%20%5Cmu%24%2C%20as%20an%20attempt%20to%20filter%20out%20unusually%20noisy%20samples.%20A%20complication%0Athat%20arises%20is%20that%20rejection%20sampling%20introduces%20bias%20in%20the%20distribution%20of%0Athe%20remaining%20points.%20To%20address%20this%20issue%2C%20we%20perform%20a%20careful%20analysis%20of%0Athe%20bias%2C%20develop%20an%20iterative%20dimension-reduction%20strategy%2C%20and%20employ%20a%20novel%0Asubroutine%20inspired%20by%20list-decodable%20learning%20that%20leverages%20the%0Aone-dimensional%20result.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05425v1&entry.124074799=Read"},
{"title": "Biomedical Relation Extraction via Adaptive Document-Relation\n  Cross-Mapping and Concept Unique Identifier", "author": "Yufei Shang and Yanrong Guo and Shijie Hao and Richang Hong", "abstract": "  Document-Level Biomedical Relation Extraction (Bio-RE) aims to identify\nrelations between biomedical entities within extensive texts, serving as a\ncrucial subfield of biomedical text mining. Existing Bio-RE methods struggle\nwith cross-sentence inference, which is essential for capturing relations\nspanning multiple sentences. Moreover, previous methods often overlook the\nincompleteness of documents and lack the integration of external knowledge,\nlimiting contextual richness. Besides, the scarcity of annotated data further\nhampers model training. Recent advancements in large language models (LLMs)\nhave inspired us to explore all the above issues for document-level Bio-RE.\nSpecifically, we propose a document-level Bio-RE framework via LLM Adaptive\nDocument-Relation Cross-Mapping (ADRCM) Fine-Tuning and Concept Unique\nIdentifier (CUI) Retrieval-Augmented Generation (RAG). First, we introduce the\nIteration-of-REsummary (IoRs) prompt for solving the data scarcity issue. In\nthis way, Bio-RE task-specific synthetic data can be generated by guiding\nChatGPT to focus on entity relations and iteratively refining synthetic data.\nNext, we propose ADRCM fine-tuning, a novel fine-tuning recipe that establishes\nmappings across different documents and relations, enhancing the model's\ncontextual understanding and cross-sentence inference capabilities. Finally,\nduring the inference, a biomedical-specific RAG approach, named CUI RAG, is\ndesigned to leverage CUIs as indexes for entities, narrowing the retrieval\nscope and enriching the relevant document contexts. Experiments conducted on\nthree Bio-RE datasets (GDA, CDR, and BioRED) demonstrate the state-of-the-art\nperformance of our proposed method by comparing it with other related works.\n", "link": "http://arxiv.org/abs/2501.05155v1", "date": "2025-01-09", "relevancy": 1.9485, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4874}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4874}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Biomedical%20Relation%20Extraction%20via%20Adaptive%20Document-Relation%0A%20%20Cross-Mapping%20and%20Concept%20Unique%20Identifier&body=Title%3A%20Biomedical%20Relation%20Extraction%20via%20Adaptive%20Document-Relation%0A%20%20Cross-Mapping%20and%20Concept%20Unique%20Identifier%0AAuthor%3A%20Yufei%20Shang%20and%20Yanrong%20Guo%20and%20Shijie%20Hao%20and%20Richang%20Hong%0AAbstract%3A%20%20%20Document-Level%20Biomedical%20Relation%20Extraction%20%28Bio-RE%29%20aims%20to%20identify%0Arelations%20between%20biomedical%20entities%20within%20extensive%20texts%2C%20serving%20as%20a%0Acrucial%20subfield%20of%20biomedical%20text%20mining.%20Existing%20Bio-RE%20methods%20struggle%0Awith%20cross-sentence%20inference%2C%20which%20is%20essential%20for%20capturing%20relations%0Aspanning%20multiple%20sentences.%20Moreover%2C%20previous%20methods%20often%20overlook%20the%0Aincompleteness%20of%20documents%20and%20lack%20the%20integration%20of%20external%20knowledge%2C%0Alimiting%20contextual%20richness.%20Besides%2C%20the%20scarcity%20of%20annotated%20data%20further%0Ahampers%20model%20training.%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%0Ahave%20inspired%20us%20to%20explore%20all%20the%20above%20issues%20for%20document-level%20Bio-RE.%0ASpecifically%2C%20we%20propose%20a%20document-level%20Bio-RE%20framework%20via%20LLM%20Adaptive%0ADocument-Relation%20Cross-Mapping%20%28ADRCM%29%20Fine-Tuning%20and%20Concept%20Unique%0AIdentifier%20%28CUI%29%20Retrieval-Augmented%20Generation%20%28RAG%29.%20First%2C%20we%20introduce%20the%0AIteration-of-REsummary%20%28IoRs%29%20prompt%20for%20solving%20the%20data%20scarcity%20issue.%20In%0Athis%20way%2C%20Bio-RE%20task-specific%20synthetic%20data%20can%20be%20generated%20by%20guiding%0AChatGPT%20to%20focus%20on%20entity%20relations%20and%20iteratively%20refining%20synthetic%20data.%0ANext%2C%20we%20propose%20ADRCM%20fine-tuning%2C%20a%20novel%20fine-tuning%20recipe%20that%20establishes%0Amappings%20across%20different%20documents%20and%20relations%2C%20enhancing%20the%20model%27s%0Acontextual%20understanding%20and%20cross-sentence%20inference%20capabilities.%20Finally%2C%0Aduring%20the%20inference%2C%20a%20biomedical-specific%20RAG%20approach%2C%20named%20CUI%20RAG%2C%20is%0Adesigned%20to%20leverage%20CUIs%20as%20indexes%20for%20entities%2C%20narrowing%20the%20retrieval%0Ascope%20and%20enriching%20the%20relevant%20document%20contexts.%20Experiments%20conducted%20on%0Athree%20Bio-RE%20datasets%20%28GDA%2C%20CDR%2C%20and%20BioRED%29%20demonstrate%20the%20state-of-the-art%0Aperformance%20of%20our%20proposed%20method%20by%20comparing%20it%20with%20other%20related%20works.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05155v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiomedical%2520Relation%2520Extraction%2520via%2520Adaptive%2520Document-Relation%250A%2520%2520Cross-Mapping%2520and%2520Concept%2520Unique%2520Identifier%26entry.906535625%3DYufei%2520Shang%2520and%2520Yanrong%2520Guo%2520and%2520Shijie%2520Hao%2520and%2520Richang%2520Hong%26entry.1292438233%3D%2520%2520Document-Level%2520Biomedical%2520Relation%2520Extraction%2520%2528Bio-RE%2529%2520aims%2520to%2520identify%250Arelations%2520between%2520biomedical%2520entities%2520within%2520extensive%2520texts%252C%2520serving%2520as%2520a%250Acrucial%2520subfield%2520of%2520biomedical%2520text%2520mining.%2520Existing%2520Bio-RE%2520methods%2520struggle%250Awith%2520cross-sentence%2520inference%252C%2520which%2520is%2520essential%2520for%2520capturing%2520relations%250Aspanning%2520multiple%2520sentences.%2520Moreover%252C%2520previous%2520methods%2520often%2520overlook%2520the%250Aincompleteness%2520of%2520documents%2520and%2520lack%2520the%2520integration%2520of%2520external%2520knowledge%252C%250Alimiting%2520contextual%2520richness.%2520Besides%252C%2520the%2520scarcity%2520of%2520annotated%2520data%2520further%250Ahampers%2520model%2520training.%2520Recent%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%250Ahave%2520inspired%2520us%2520to%2520explore%2520all%2520the%2520above%2520issues%2520for%2520document-level%2520Bio-RE.%250ASpecifically%252C%2520we%2520propose%2520a%2520document-level%2520Bio-RE%2520framework%2520via%2520LLM%2520Adaptive%250ADocument-Relation%2520Cross-Mapping%2520%2528ADRCM%2529%2520Fine-Tuning%2520and%2520Concept%2520Unique%250AIdentifier%2520%2528CUI%2529%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529.%2520First%252C%2520we%2520introduce%2520the%250AIteration-of-REsummary%2520%2528IoRs%2529%2520prompt%2520for%2520solving%2520the%2520data%2520scarcity%2520issue.%2520In%250Athis%2520way%252C%2520Bio-RE%2520task-specific%2520synthetic%2520data%2520can%2520be%2520generated%2520by%2520guiding%250AChatGPT%2520to%2520focus%2520on%2520entity%2520relations%2520and%2520iteratively%2520refining%2520synthetic%2520data.%250ANext%252C%2520we%2520propose%2520ADRCM%2520fine-tuning%252C%2520a%2520novel%2520fine-tuning%2520recipe%2520that%2520establishes%250Amappings%2520across%2520different%2520documents%2520and%2520relations%252C%2520enhancing%2520the%2520model%2527s%250Acontextual%2520understanding%2520and%2520cross-sentence%2520inference%2520capabilities.%2520Finally%252C%250Aduring%2520the%2520inference%252C%2520a%2520biomedical-specific%2520RAG%2520approach%252C%2520named%2520CUI%2520RAG%252C%2520is%250Adesigned%2520to%2520leverage%2520CUIs%2520as%2520indexes%2520for%2520entities%252C%2520narrowing%2520the%2520retrieval%250Ascope%2520and%2520enriching%2520the%2520relevant%2520document%2520contexts.%2520Experiments%2520conducted%2520on%250Athree%2520Bio-RE%2520datasets%2520%2528GDA%252C%2520CDR%252C%2520and%2520BioRED%2529%2520demonstrate%2520the%2520state-of-the-art%250Aperformance%2520of%2520our%2520proposed%2520method%2520by%2520comparing%2520it%2520with%2520other%2520related%2520works.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05155v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Biomedical%20Relation%20Extraction%20via%20Adaptive%20Document-Relation%0A%20%20Cross-Mapping%20and%20Concept%20Unique%20Identifier&entry.906535625=Yufei%20Shang%20and%20Yanrong%20Guo%20and%20Shijie%20Hao%20and%20Richang%20Hong&entry.1292438233=%20%20Document-Level%20Biomedical%20Relation%20Extraction%20%28Bio-RE%29%20aims%20to%20identify%0Arelations%20between%20biomedical%20entities%20within%20extensive%20texts%2C%20serving%20as%20a%0Acrucial%20subfield%20of%20biomedical%20text%20mining.%20Existing%20Bio-RE%20methods%20struggle%0Awith%20cross-sentence%20inference%2C%20which%20is%20essential%20for%20capturing%20relations%0Aspanning%20multiple%20sentences.%20Moreover%2C%20previous%20methods%20often%20overlook%20the%0Aincompleteness%20of%20documents%20and%20lack%20the%20integration%20of%20external%20knowledge%2C%0Alimiting%20contextual%20richness.%20Besides%2C%20the%20scarcity%20of%20annotated%20data%20further%0Ahampers%20model%20training.%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%0Ahave%20inspired%20us%20to%20explore%20all%20the%20above%20issues%20for%20document-level%20Bio-RE.%0ASpecifically%2C%20we%20propose%20a%20document-level%20Bio-RE%20framework%20via%20LLM%20Adaptive%0ADocument-Relation%20Cross-Mapping%20%28ADRCM%29%20Fine-Tuning%20and%20Concept%20Unique%0AIdentifier%20%28CUI%29%20Retrieval-Augmented%20Generation%20%28RAG%29.%20First%2C%20we%20introduce%20the%0AIteration-of-REsummary%20%28IoRs%29%20prompt%20for%20solving%20the%20data%20scarcity%20issue.%20In%0Athis%20way%2C%20Bio-RE%20task-specific%20synthetic%20data%20can%20be%20generated%20by%20guiding%0AChatGPT%20to%20focus%20on%20entity%20relations%20and%20iteratively%20refining%20synthetic%20data.%0ANext%2C%20we%20propose%20ADRCM%20fine-tuning%2C%20a%20novel%20fine-tuning%20recipe%20that%20establishes%0Amappings%20across%20different%20documents%20and%20relations%2C%20enhancing%20the%20model%27s%0Acontextual%20understanding%20and%20cross-sentence%20inference%20capabilities.%20Finally%2C%0Aduring%20the%20inference%2C%20a%20biomedical-specific%20RAG%20approach%2C%20named%20CUI%20RAG%2C%20is%0Adesigned%20to%20leverage%20CUIs%20as%20indexes%20for%20entities%2C%20narrowing%20the%20retrieval%0Ascope%20and%20enriching%20the%20relevant%20document%20contexts.%20Experiments%20conducted%20on%0Athree%20Bio-RE%20datasets%20%28GDA%2C%20CDR%2C%20and%20BioRED%29%20demonstrate%20the%20state-of-the-art%0Aperformance%20of%20our%20proposed%20method%20by%20comparing%20it%20with%20other%20related%20works.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05155v1&entry.124074799=Read"},
{"title": "Automotive Speed Estimation: Sensor Types and Error Characteristics from\n  OBD-II to ADAS", "author": "Hany Ragab and Sidney Givigi and Aboelmagd Noureldin", "abstract": "  Modern on-road navigation systems heavily depend on integrating speed\nmeasurements with inertial navigation systems (INS) and global navigation\nsatellite systems (GNSS). Telemetry-based applications typically source speed\ndata from the On-Board Diagnostic II (OBD-II) system. However, the method of\nderiving speed, as well as the types of sensors used to measure wheel speed,\ndiffers across vehicles. These differences result in varying error\ncharacteristics that must be accounted for in navigation and autonomy\napplications. This paper addresses this gap by examining the diverse\nspeed-sensing technologies employed in standard automotive systems and\nalternative techniques used in advanced systems designed for higher levels of\nautonomy, such as Advanced Driver Assistance Systems (ADAS), Autonomous Driving\n(AD), or surveying applications. We propose a method to identify the type of\nspeed sensor in a vehicle and present strategies for accurately modeling its\nerror characteristics. To validate our approach, we collected and analyzed data\nfrom three long real road trajectories conducted in urban environments in\nToronto and Kingston, Ontario, Canada. The results underscore the critical role\nof integrating multiple sensor modalities to achieve more accurate speed\nestimation, thus improving automotive navigation state estimation, particularly\nin GNSS-denied environments.\n", "link": "http://arxiv.org/abs/2501.00242v2", "date": "2025-01-09", "relevancy": 1.9421, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5012}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4903}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automotive%20Speed%20Estimation%3A%20Sensor%20Types%20and%20Error%20Characteristics%20from%0A%20%20OBD-II%20to%20ADAS&body=Title%3A%20Automotive%20Speed%20Estimation%3A%20Sensor%20Types%20and%20Error%20Characteristics%20from%0A%20%20OBD-II%20to%20ADAS%0AAuthor%3A%20Hany%20Ragab%20and%20Sidney%20Givigi%20and%20Aboelmagd%20Noureldin%0AAbstract%3A%20%20%20Modern%20on-road%20navigation%20systems%20heavily%20depend%20on%20integrating%20speed%0Ameasurements%20with%20inertial%20navigation%20systems%20%28INS%29%20and%20global%20navigation%0Asatellite%20systems%20%28GNSS%29.%20Telemetry-based%20applications%20typically%20source%20speed%0Adata%20from%20the%20On-Board%20Diagnostic%20II%20%28OBD-II%29%20system.%20However%2C%20the%20method%20of%0Aderiving%20speed%2C%20as%20well%20as%20the%20types%20of%20sensors%20used%20to%20measure%20wheel%20speed%2C%0Adiffers%20across%20vehicles.%20These%20differences%20result%20in%20varying%20error%0Acharacteristics%20that%20must%20be%20accounted%20for%20in%20navigation%20and%20autonomy%0Aapplications.%20This%20paper%20addresses%20this%20gap%20by%20examining%20the%20diverse%0Aspeed-sensing%20technologies%20employed%20in%20standard%20automotive%20systems%20and%0Aalternative%20techniques%20used%20in%20advanced%20systems%20designed%20for%20higher%20levels%20of%0Aautonomy%2C%20such%20as%20Advanced%20Driver%20Assistance%20Systems%20%28ADAS%29%2C%20Autonomous%20Driving%0A%28AD%29%2C%20or%20surveying%20applications.%20We%20propose%20a%20method%20to%20identify%20the%20type%20of%0Aspeed%20sensor%20in%20a%20vehicle%20and%20present%20strategies%20for%20accurately%20modeling%20its%0Aerror%20characteristics.%20To%20validate%20our%20approach%2C%20we%20collected%20and%20analyzed%20data%0Afrom%20three%20long%20real%20road%20trajectories%20conducted%20in%20urban%20environments%20in%0AToronto%20and%20Kingston%2C%20Ontario%2C%20Canada.%20The%20results%20underscore%20the%20critical%20role%0Aof%20integrating%20multiple%20sensor%20modalities%20to%20achieve%20more%20accurate%20speed%0Aestimation%2C%20thus%20improving%20automotive%20navigation%20state%20estimation%2C%20particularly%0Ain%20GNSS-denied%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.00242v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomotive%2520Speed%2520Estimation%253A%2520Sensor%2520Types%2520and%2520Error%2520Characteristics%2520from%250A%2520%2520OBD-II%2520to%2520ADAS%26entry.906535625%3DHany%2520Ragab%2520and%2520Sidney%2520Givigi%2520and%2520Aboelmagd%2520Noureldin%26entry.1292438233%3D%2520%2520Modern%2520on-road%2520navigation%2520systems%2520heavily%2520depend%2520on%2520integrating%2520speed%250Ameasurements%2520with%2520inertial%2520navigation%2520systems%2520%2528INS%2529%2520and%2520global%2520navigation%250Asatellite%2520systems%2520%2528GNSS%2529.%2520Telemetry-based%2520applications%2520typically%2520source%2520speed%250Adata%2520from%2520the%2520On-Board%2520Diagnostic%2520II%2520%2528OBD-II%2529%2520system.%2520However%252C%2520the%2520method%2520of%250Aderiving%2520speed%252C%2520as%2520well%2520as%2520the%2520types%2520of%2520sensors%2520used%2520to%2520measure%2520wheel%2520speed%252C%250Adiffers%2520across%2520vehicles.%2520These%2520differences%2520result%2520in%2520varying%2520error%250Acharacteristics%2520that%2520must%2520be%2520accounted%2520for%2520in%2520navigation%2520and%2520autonomy%250Aapplications.%2520This%2520paper%2520addresses%2520this%2520gap%2520by%2520examining%2520the%2520diverse%250Aspeed-sensing%2520technologies%2520employed%2520in%2520standard%2520automotive%2520systems%2520and%250Aalternative%2520techniques%2520used%2520in%2520advanced%2520systems%2520designed%2520for%2520higher%2520levels%2520of%250Aautonomy%252C%2520such%2520as%2520Advanced%2520Driver%2520Assistance%2520Systems%2520%2528ADAS%2529%252C%2520Autonomous%2520Driving%250A%2528AD%2529%252C%2520or%2520surveying%2520applications.%2520We%2520propose%2520a%2520method%2520to%2520identify%2520the%2520type%2520of%250Aspeed%2520sensor%2520in%2520a%2520vehicle%2520and%2520present%2520strategies%2520for%2520accurately%2520modeling%2520its%250Aerror%2520characteristics.%2520To%2520validate%2520our%2520approach%252C%2520we%2520collected%2520and%2520analyzed%2520data%250Afrom%2520three%2520long%2520real%2520road%2520trajectories%2520conducted%2520in%2520urban%2520environments%2520in%250AToronto%2520and%2520Kingston%252C%2520Ontario%252C%2520Canada.%2520The%2520results%2520underscore%2520the%2520critical%2520role%250Aof%2520integrating%2520multiple%2520sensor%2520modalities%2520to%2520achieve%2520more%2520accurate%2520speed%250Aestimation%252C%2520thus%2520improving%2520automotive%2520navigation%2520state%2520estimation%252C%2520particularly%250Ain%2520GNSS-denied%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.00242v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automotive%20Speed%20Estimation%3A%20Sensor%20Types%20and%20Error%20Characteristics%20from%0A%20%20OBD-II%20to%20ADAS&entry.906535625=Hany%20Ragab%20and%20Sidney%20Givigi%20and%20Aboelmagd%20Noureldin&entry.1292438233=%20%20Modern%20on-road%20navigation%20systems%20heavily%20depend%20on%20integrating%20speed%0Ameasurements%20with%20inertial%20navigation%20systems%20%28INS%29%20and%20global%20navigation%0Asatellite%20systems%20%28GNSS%29.%20Telemetry-based%20applications%20typically%20source%20speed%0Adata%20from%20the%20On-Board%20Diagnostic%20II%20%28OBD-II%29%20system.%20However%2C%20the%20method%20of%0Aderiving%20speed%2C%20as%20well%20as%20the%20types%20of%20sensors%20used%20to%20measure%20wheel%20speed%2C%0Adiffers%20across%20vehicles.%20These%20differences%20result%20in%20varying%20error%0Acharacteristics%20that%20must%20be%20accounted%20for%20in%20navigation%20and%20autonomy%0Aapplications.%20This%20paper%20addresses%20this%20gap%20by%20examining%20the%20diverse%0Aspeed-sensing%20technologies%20employed%20in%20standard%20automotive%20systems%20and%0Aalternative%20techniques%20used%20in%20advanced%20systems%20designed%20for%20higher%20levels%20of%0Aautonomy%2C%20such%20as%20Advanced%20Driver%20Assistance%20Systems%20%28ADAS%29%2C%20Autonomous%20Driving%0A%28AD%29%2C%20or%20surveying%20applications.%20We%20propose%20a%20method%20to%20identify%20the%20type%20of%0Aspeed%20sensor%20in%20a%20vehicle%20and%20present%20strategies%20for%20accurately%20modeling%20its%0Aerror%20characteristics.%20To%20validate%20our%20approach%2C%20we%20collected%20and%20analyzed%20data%0Afrom%20three%20long%20real%20road%20trajectories%20conducted%20in%20urban%20environments%20in%0AToronto%20and%20Kingston%2C%20Ontario%2C%20Canada.%20The%20results%20underscore%20the%20critical%20role%0Aof%20integrating%20multiple%20sensor%20modalities%20to%20achieve%20more%20accurate%20speed%0Aestimation%2C%20thus%20improving%20automotive%20navigation%20state%20estimation%2C%20particularly%0Ain%20GNSS-denied%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.00242v2&entry.124074799=Read"},
{"title": "Preference-Based Multi-Agent Reinforcement Learning: Data Coverage and\n  Algorithmic Techniques", "author": "Natalia Zhang and Xinqi Wang and Qiwen Cui and Runlong Zhou and Sham M. Kakade and Simon S. Du", "abstract": "  We initiate the study of Preference-Based Multi-Agent Reinforcement Learning\n(PbMARL), exploring both theoretical foundations and empirical validations. We\ndefine the task as identifying the Nash equilibrium from a preference-only\noffline dataset in general-sum games, a problem marked by the challenge of\nsparse feedback signals. Our theory establishes the upper complexity bounds for\nNash Equilibrium in effective PbMARL, demonstrating that single-policy coverage\nis inadequate and highlighting the importance of unilateral dataset coverage.\nThese theoretical insights are verified through comprehensive experiments. To\nenhance the practical performance, we further introduce two algorithmic\ntechniques. (1) We propose a Mean Squared Error (MSE) regularization along the\ntime axis to achieve a more uniform reward distribution and improve reward\nlearning outcomes. (2) We propose an additional penalty based on the\ndistribution of the dataset to incorporate pessimism, improving stability and\neffectiveness during training. Our findings underscore the multifaceted\napproach required for PbMARL, paving the way for effective preference-based\nmulti-agent systems.\n", "link": "http://arxiv.org/abs/2409.00717v3", "date": "2025-01-09", "relevancy": 1.933, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5359}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4788}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Preference-Based%20Multi-Agent%20Reinforcement%20Learning%3A%20Data%20Coverage%20and%0A%20%20Algorithmic%20Techniques&body=Title%3A%20Preference-Based%20Multi-Agent%20Reinforcement%20Learning%3A%20Data%20Coverage%20and%0A%20%20Algorithmic%20Techniques%0AAuthor%3A%20Natalia%20Zhang%20and%20Xinqi%20Wang%20and%20Qiwen%20Cui%20and%20Runlong%20Zhou%20and%20Sham%20M.%20Kakade%20and%20Simon%20S.%20Du%0AAbstract%3A%20%20%20We%20initiate%20the%20study%20of%20Preference-Based%20Multi-Agent%20Reinforcement%20Learning%0A%28PbMARL%29%2C%20exploring%20both%20theoretical%20foundations%20and%20empirical%20validations.%20We%0Adefine%20the%20task%20as%20identifying%20the%20Nash%20equilibrium%20from%20a%20preference-only%0Aoffline%20dataset%20in%20general-sum%20games%2C%20a%20problem%20marked%20by%20the%20challenge%20of%0Asparse%20feedback%20signals.%20Our%20theory%20establishes%20the%20upper%20complexity%20bounds%20for%0ANash%20Equilibrium%20in%20effective%20PbMARL%2C%20demonstrating%20that%20single-policy%20coverage%0Ais%20inadequate%20and%20highlighting%20the%20importance%20of%20unilateral%20dataset%20coverage.%0AThese%20theoretical%20insights%20are%20verified%20through%20comprehensive%20experiments.%20To%0Aenhance%20the%20practical%20performance%2C%20we%20further%20introduce%20two%20algorithmic%0Atechniques.%20%281%29%20We%20propose%20a%20Mean%20Squared%20Error%20%28MSE%29%20regularization%20along%20the%0Atime%20axis%20to%20achieve%20a%20more%20uniform%20reward%20distribution%20and%20improve%20reward%0Alearning%20outcomes.%20%282%29%20We%20propose%20an%20additional%20penalty%20based%20on%20the%0Adistribution%20of%20the%20dataset%20to%20incorporate%20pessimism%2C%20improving%20stability%20and%0Aeffectiveness%20during%20training.%20Our%20findings%20underscore%20the%20multifaceted%0Aapproach%20required%20for%20PbMARL%2C%20paving%20the%20way%20for%20effective%20preference-based%0Amulti-agent%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.00717v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreference-Based%2520Multi-Agent%2520Reinforcement%2520Learning%253A%2520Data%2520Coverage%2520and%250A%2520%2520Algorithmic%2520Techniques%26entry.906535625%3DNatalia%2520Zhang%2520and%2520Xinqi%2520Wang%2520and%2520Qiwen%2520Cui%2520and%2520Runlong%2520Zhou%2520and%2520Sham%2520M.%2520Kakade%2520and%2520Simon%2520S.%2520Du%26entry.1292438233%3D%2520%2520We%2520initiate%2520the%2520study%2520of%2520Preference-Based%2520Multi-Agent%2520Reinforcement%2520Learning%250A%2528PbMARL%2529%252C%2520exploring%2520both%2520theoretical%2520foundations%2520and%2520empirical%2520validations.%2520We%250Adefine%2520the%2520task%2520as%2520identifying%2520the%2520Nash%2520equilibrium%2520from%2520a%2520preference-only%250Aoffline%2520dataset%2520in%2520general-sum%2520games%252C%2520a%2520problem%2520marked%2520by%2520the%2520challenge%2520of%250Asparse%2520feedback%2520signals.%2520Our%2520theory%2520establishes%2520the%2520upper%2520complexity%2520bounds%2520for%250ANash%2520Equilibrium%2520in%2520effective%2520PbMARL%252C%2520demonstrating%2520that%2520single-policy%2520coverage%250Ais%2520inadequate%2520and%2520highlighting%2520the%2520importance%2520of%2520unilateral%2520dataset%2520coverage.%250AThese%2520theoretical%2520insights%2520are%2520verified%2520through%2520comprehensive%2520experiments.%2520To%250Aenhance%2520the%2520practical%2520performance%252C%2520we%2520further%2520introduce%2520two%2520algorithmic%250Atechniques.%2520%25281%2529%2520We%2520propose%2520a%2520Mean%2520Squared%2520Error%2520%2528MSE%2529%2520regularization%2520along%2520the%250Atime%2520axis%2520to%2520achieve%2520a%2520more%2520uniform%2520reward%2520distribution%2520and%2520improve%2520reward%250Alearning%2520outcomes.%2520%25282%2529%2520We%2520propose%2520an%2520additional%2520penalty%2520based%2520on%2520the%250Adistribution%2520of%2520the%2520dataset%2520to%2520incorporate%2520pessimism%252C%2520improving%2520stability%2520and%250Aeffectiveness%2520during%2520training.%2520Our%2520findings%2520underscore%2520the%2520multifaceted%250Aapproach%2520required%2520for%2520PbMARL%252C%2520paving%2520the%2520way%2520for%2520effective%2520preference-based%250Amulti-agent%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.00717v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preference-Based%20Multi-Agent%20Reinforcement%20Learning%3A%20Data%20Coverage%20and%0A%20%20Algorithmic%20Techniques&entry.906535625=Natalia%20Zhang%20and%20Xinqi%20Wang%20and%20Qiwen%20Cui%20and%20Runlong%20Zhou%20and%20Sham%20M.%20Kakade%20and%20Simon%20S.%20Du&entry.1292438233=%20%20We%20initiate%20the%20study%20of%20Preference-Based%20Multi-Agent%20Reinforcement%20Learning%0A%28PbMARL%29%2C%20exploring%20both%20theoretical%20foundations%20and%20empirical%20validations.%20We%0Adefine%20the%20task%20as%20identifying%20the%20Nash%20equilibrium%20from%20a%20preference-only%0Aoffline%20dataset%20in%20general-sum%20games%2C%20a%20problem%20marked%20by%20the%20challenge%20of%0Asparse%20feedback%20signals.%20Our%20theory%20establishes%20the%20upper%20complexity%20bounds%20for%0ANash%20Equilibrium%20in%20effective%20PbMARL%2C%20demonstrating%20that%20single-policy%20coverage%0Ais%20inadequate%20and%20highlighting%20the%20importance%20of%20unilateral%20dataset%20coverage.%0AThese%20theoretical%20insights%20are%20verified%20through%20comprehensive%20experiments.%20To%0Aenhance%20the%20practical%20performance%2C%20we%20further%20introduce%20two%20algorithmic%0Atechniques.%20%281%29%20We%20propose%20a%20Mean%20Squared%20Error%20%28MSE%29%20regularization%20along%20the%0Atime%20axis%20to%20achieve%20a%20more%20uniform%20reward%20distribution%20and%20improve%20reward%0Alearning%20outcomes.%20%282%29%20We%20propose%20an%20additional%20penalty%20based%20on%20the%0Adistribution%20of%20the%20dataset%20to%20incorporate%20pessimism%2C%20improving%20stability%20and%0Aeffectiveness%20during%20training.%20Our%20findings%20underscore%20the%20multifaceted%0Aapproach%20required%20for%20PbMARL%2C%20paving%20the%20way%20for%20effective%20preference-based%0Amulti-agent%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.00717v3&entry.124074799=Read"},
{"title": "Stream Aligner: Efficient Sentence-Level Alignment via Distribution\n  Induction", "author": "Hantao Lou and Jiaming Ji and Kaile Wang and Yaodong Yang", "abstract": "  The rapid advancement of large language models (LLMs) has led to significant\nimprovements in their capabilities, but also to increased concerns about their\nalignment with human values and intentions. Current alignment strategies,\nincluding adaptive training and inference-time methods, have demonstrated\npotential in this area. However, these approaches still struggle to balance\ndeployment complexity and capability across various tasks and difficulties. In\nthis work, we introduce the Streaming Distribution Induce Aligner (Stream\nAligner), a novel alignment paradigm that combines efficiency with enhanced\nperformance in various tasks throughout the generation process. Stream Aligner\nachieves dynamic sentence-level correction by using a small model to learn the\npreferences of the suffix sentence, iteratively correcting the suffix sentence\noutput by the upstream model, and then using the corrected sentence to replace\nthe suffix sentence in subsequent generations. Compared to Aligner, our\nexperiments demonstrate that Stream Aligner reduces reliance on the\ncapabilities of additional models, enhances the reasoning abilities of LLMs,\nand decreases latency during user interaction. Specifically, Stream Aligner-2B\nmodel has achieved an improvement of 76.1% in helpfulness, 36.0% in\nharmlessness on the tested Llama2-70B-chat model, and Stream Aligner-8B has\nachieved an improvement of 3.5% on the math ability of the tested\nLlama3-70B-Instruct model.\n", "link": "http://arxiv.org/abs/2501.05336v1", "date": "2025-01-09", "relevancy": 1.9203, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4966}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4689}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stream%20Aligner%3A%20Efficient%20Sentence-Level%20Alignment%20via%20Distribution%0A%20%20Induction&body=Title%3A%20Stream%20Aligner%3A%20Efficient%20Sentence-Level%20Alignment%20via%20Distribution%0A%20%20Induction%0AAuthor%3A%20Hantao%20Lou%20and%20Jiaming%20Ji%20and%20Kaile%20Wang%20and%20Yaodong%20Yang%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20large%20language%20models%20%28LLMs%29%20has%20led%20to%20significant%0Aimprovements%20in%20their%20capabilities%2C%20but%20also%20to%20increased%20concerns%20about%20their%0Aalignment%20with%20human%20values%20and%20intentions.%20Current%20alignment%20strategies%2C%0Aincluding%20adaptive%20training%20and%20inference-time%20methods%2C%20have%20demonstrated%0Apotential%20in%20this%20area.%20However%2C%20these%20approaches%20still%20struggle%20to%20balance%0Adeployment%20complexity%20and%20capability%20across%20various%20tasks%20and%20difficulties.%20In%0Athis%20work%2C%20we%20introduce%20the%20Streaming%20Distribution%20Induce%20Aligner%20%28Stream%0AAligner%29%2C%20a%20novel%20alignment%20paradigm%20that%20combines%20efficiency%20with%20enhanced%0Aperformance%20in%20various%20tasks%20throughout%20the%20generation%20process.%20Stream%20Aligner%0Aachieves%20dynamic%20sentence-level%20correction%20by%20using%20a%20small%20model%20to%20learn%20the%0Apreferences%20of%20the%20suffix%20sentence%2C%20iteratively%20correcting%20the%20suffix%20sentence%0Aoutput%20by%20the%20upstream%20model%2C%20and%20then%20using%20the%20corrected%20sentence%20to%20replace%0Athe%20suffix%20sentence%20in%20subsequent%20generations.%20Compared%20to%20Aligner%2C%20our%0Aexperiments%20demonstrate%20that%20Stream%20Aligner%20reduces%20reliance%20on%20the%0Acapabilities%20of%20additional%20models%2C%20enhances%20the%20reasoning%20abilities%20of%20LLMs%2C%0Aand%20decreases%20latency%20during%20user%20interaction.%20Specifically%2C%20Stream%20Aligner-2B%0Amodel%20has%20achieved%20an%20improvement%20of%2076.1%25%20in%20helpfulness%2C%2036.0%25%20in%0Aharmlessness%20on%20the%20tested%20Llama2-70B-chat%20model%2C%20and%20Stream%20Aligner-8B%20has%0Aachieved%20an%20improvement%20of%203.5%25%20on%20the%20math%20ability%20of%20the%20tested%0ALlama3-70B-Instruct%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05336v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStream%2520Aligner%253A%2520Efficient%2520Sentence-Level%2520Alignment%2520via%2520Distribution%250A%2520%2520Induction%26entry.906535625%3DHantao%2520Lou%2520and%2520Jiaming%2520Ji%2520and%2520Kaile%2520Wang%2520and%2520Yaodong%2520Yang%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520led%2520to%2520significant%250Aimprovements%2520in%2520their%2520capabilities%252C%2520but%2520also%2520to%2520increased%2520concerns%2520about%2520their%250Aalignment%2520with%2520human%2520values%2520and%2520intentions.%2520Current%2520alignment%2520strategies%252C%250Aincluding%2520adaptive%2520training%2520and%2520inference-time%2520methods%252C%2520have%2520demonstrated%250Apotential%2520in%2520this%2520area.%2520However%252C%2520these%2520approaches%2520still%2520struggle%2520to%2520balance%250Adeployment%2520complexity%2520and%2520capability%2520across%2520various%2520tasks%2520and%2520difficulties.%2520In%250Athis%2520work%252C%2520we%2520introduce%2520the%2520Streaming%2520Distribution%2520Induce%2520Aligner%2520%2528Stream%250AAligner%2529%252C%2520a%2520novel%2520alignment%2520paradigm%2520that%2520combines%2520efficiency%2520with%2520enhanced%250Aperformance%2520in%2520various%2520tasks%2520throughout%2520the%2520generation%2520process.%2520Stream%2520Aligner%250Aachieves%2520dynamic%2520sentence-level%2520correction%2520by%2520using%2520a%2520small%2520model%2520to%2520learn%2520the%250Apreferences%2520of%2520the%2520suffix%2520sentence%252C%2520iteratively%2520correcting%2520the%2520suffix%2520sentence%250Aoutput%2520by%2520the%2520upstream%2520model%252C%2520and%2520then%2520using%2520the%2520corrected%2520sentence%2520to%2520replace%250Athe%2520suffix%2520sentence%2520in%2520subsequent%2520generations.%2520Compared%2520to%2520Aligner%252C%2520our%250Aexperiments%2520demonstrate%2520that%2520Stream%2520Aligner%2520reduces%2520reliance%2520on%2520the%250Acapabilities%2520of%2520additional%2520models%252C%2520enhances%2520the%2520reasoning%2520abilities%2520of%2520LLMs%252C%250Aand%2520decreases%2520latency%2520during%2520user%2520interaction.%2520Specifically%252C%2520Stream%2520Aligner-2B%250Amodel%2520has%2520achieved%2520an%2520improvement%2520of%252076.1%2525%2520in%2520helpfulness%252C%252036.0%2525%2520in%250Aharmlessness%2520on%2520the%2520tested%2520Llama2-70B-chat%2520model%252C%2520and%2520Stream%2520Aligner-8B%2520has%250Aachieved%2520an%2520improvement%2520of%25203.5%2525%2520on%2520the%2520math%2520ability%2520of%2520the%2520tested%250ALlama3-70B-Instruct%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05336v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stream%20Aligner%3A%20Efficient%20Sentence-Level%20Alignment%20via%20Distribution%0A%20%20Induction&entry.906535625=Hantao%20Lou%20and%20Jiaming%20Ji%20and%20Kaile%20Wang%20and%20Yaodong%20Yang&entry.1292438233=%20%20The%20rapid%20advancement%20of%20large%20language%20models%20%28LLMs%29%20has%20led%20to%20significant%0Aimprovements%20in%20their%20capabilities%2C%20but%20also%20to%20increased%20concerns%20about%20their%0Aalignment%20with%20human%20values%20and%20intentions.%20Current%20alignment%20strategies%2C%0Aincluding%20adaptive%20training%20and%20inference-time%20methods%2C%20have%20demonstrated%0Apotential%20in%20this%20area.%20However%2C%20these%20approaches%20still%20struggle%20to%20balance%0Adeployment%20complexity%20and%20capability%20across%20various%20tasks%20and%20difficulties.%20In%0Athis%20work%2C%20we%20introduce%20the%20Streaming%20Distribution%20Induce%20Aligner%20%28Stream%0AAligner%29%2C%20a%20novel%20alignment%20paradigm%20that%20combines%20efficiency%20with%20enhanced%0Aperformance%20in%20various%20tasks%20throughout%20the%20generation%20process.%20Stream%20Aligner%0Aachieves%20dynamic%20sentence-level%20correction%20by%20using%20a%20small%20model%20to%20learn%20the%0Apreferences%20of%20the%20suffix%20sentence%2C%20iteratively%20correcting%20the%20suffix%20sentence%0Aoutput%20by%20the%20upstream%20model%2C%20and%20then%20using%20the%20corrected%20sentence%20to%20replace%0Athe%20suffix%20sentence%20in%20subsequent%20generations.%20Compared%20to%20Aligner%2C%20our%0Aexperiments%20demonstrate%20that%20Stream%20Aligner%20reduces%20reliance%20on%20the%0Acapabilities%20of%20additional%20models%2C%20enhances%20the%20reasoning%20abilities%20of%20LLMs%2C%0Aand%20decreases%20latency%20during%20user%20interaction.%20Specifically%2C%20Stream%20Aligner-2B%0Amodel%20has%20achieved%20an%20improvement%20of%2076.1%25%20in%20helpfulness%2C%2036.0%25%20in%0Aharmlessness%20on%20the%20tested%20Llama2-70B-chat%20model%2C%20and%20Stream%20Aligner-8B%20has%0Aachieved%20an%20improvement%20of%203.5%25%20on%20the%20math%20ability%20of%20the%20tested%0ALlama3-70B-Instruct%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05336v1&entry.124074799=Read"},
{"title": "Conditional Deep Canonical Time Warping", "author": "Afek Steinberg and Ran Eisenberg and Ofir Lindenbaum", "abstract": "  Temporal alignment of sequences is a fundamental challenge in many\napplications, such as computer vision and bioinformatics, where local time\nshifting needs to be accounted for. Misalignment can lead to poor model\ngeneralization, especially in high-dimensional sequences. Existing methods\noften struggle with optimization when dealing with high-dimensional sparse\ndata, falling into poor alignments. Feature selection is frequently used to\nenhance model performance for sparse data. However, a fixed set of selected\nfeatures would not generally work for dynamically changing sequences and would\nneed to be modified based on the state of the sequence. Therefore, modifying\nthe selected feature based on contextual input would result in better\nalignment. Our suggested method, Conditional Deep Canonical Temporal Time\nWarping (CDCTW), is designed for temporal alignment in sparse temporal data to\naddress these challenges. CDCTW enhances alignment accuracy for high\ndimensional time-dependent views be performing dynamic time warping on data\nembedded in maximally correlated subspace which handles sparsity with novel\nfeature selection method. We validate the effectiveness of CDCTW through\nextensive experiments on various datasets, demonstrating superior performance\nover previous techniques.\n", "link": "http://arxiv.org/abs/2412.18234v2", "date": "2025-01-09", "relevancy": 1.9045, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4864}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4772}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conditional%20Deep%20Canonical%20Time%20Warping&body=Title%3A%20Conditional%20Deep%20Canonical%20Time%20Warping%0AAuthor%3A%20Afek%20Steinberg%20and%20Ran%20Eisenberg%20and%20Ofir%20Lindenbaum%0AAbstract%3A%20%20%20Temporal%20alignment%20of%20sequences%20is%20a%20fundamental%20challenge%20in%20many%0Aapplications%2C%20such%20as%20computer%20vision%20and%20bioinformatics%2C%20where%20local%20time%0Ashifting%20needs%20to%20be%20accounted%20for.%20Misalignment%20can%20lead%20to%20poor%20model%0Ageneralization%2C%20especially%20in%20high-dimensional%20sequences.%20Existing%20methods%0Aoften%20struggle%20with%20optimization%20when%20dealing%20with%20high-dimensional%20sparse%0Adata%2C%20falling%20into%20poor%20alignments.%20Feature%20selection%20is%20frequently%20used%20to%0Aenhance%20model%20performance%20for%20sparse%20data.%20However%2C%20a%20fixed%20set%20of%20selected%0Afeatures%20would%20not%20generally%20work%20for%20dynamically%20changing%20sequences%20and%20would%0Aneed%20to%20be%20modified%20based%20on%20the%20state%20of%20the%20sequence.%20Therefore%2C%20modifying%0Athe%20selected%20feature%20based%20on%20contextual%20input%20would%20result%20in%20better%0Aalignment.%20Our%20suggested%20method%2C%20Conditional%20Deep%20Canonical%20Temporal%20Time%0AWarping%20%28CDCTW%29%2C%20is%20designed%20for%20temporal%20alignment%20in%20sparse%20temporal%20data%20to%0Aaddress%20these%20challenges.%20CDCTW%20enhances%20alignment%20accuracy%20for%20high%0Adimensional%20time-dependent%20views%20be%20performing%20dynamic%20time%20warping%20on%20data%0Aembedded%20in%20maximally%20correlated%20subspace%20which%20handles%20sparsity%20with%20novel%0Afeature%20selection%20method.%20We%20validate%20the%20effectiveness%20of%20CDCTW%20through%0Aextensive%20experiments%20on%20various%20datasets%2C%20demonstrating%20superior%20performance%0Aover%20previous%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18234v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConditional%2520Deep%2520Canonical%2520Time%2520Warping%26entry.906535625%3DAfek%2520Steinberg%2520and%2520Ran%2520Eisenberg%2520and%2520Ofir%2520Lindenbaum%26entry.1292438233%3D%2520%2520Temporal%2520alignment%2520of%2520sequences%2520is%2520a%2520fundamental%2520challenge%2520in%2520many%250Aapplications%252C%2520such%2520as%2520computer%2520vision%2520and%2520bioinformatics%252C%2520where%2520local%2520time%250Ashifting%2520needs%2520to%2520be%2520accounted%2520for.%2520Misalignment%2520can%2520lead%2520to%2520poor%2520model%250Ageneralization%252C%2520especially%2520in%2520high-dimensional%2520sequences.%2520Existing%2520methods%250Aoften%2520struggle%2520with%2520optimization%2520when%2520dealing%2520with%2520high-dimensional%2520sparse%250Adata%252C%2520falling%2520into%2520poor%2520alignments.%2520Feature%2520selection%2520is%2520frequently%2520used%2520to%250Aenhance%2520model%2520performance%2520for%2520sparse%2520data.%2520However%252C%2520a%2520fixed%2520set%2520of%2520selected%250Afeatures%2520would%2520not%2520generally%2520work%2520for%2520dynamically%2520changing%2520sequences%2520and%2520would%250Aneed%2520to%2520be%2520modified%2520based%2520on%2520the%2520state%2520of%2520the%2520sequence.%2520Therefore%252C%2520modifying%250Athe%2520selected%2520feature%2520based%2520on%2520contextual%2520input%2520would%2520result%2520in%2520better%250Aalignment.%2520Our%2520suggested%2520method%252C%2520Conditional%2520Deep%2520Canonical%2520Temporal%2520Time%250AWarping%2520%2528CDCTW%2529%252C%2520is%2520designed%2520for%2520temporal%2520alignment%2520in%2520sparse%2520temporal%2520data%2520to%250Aaddress%2520these%2520challenges.%2520CDCTW%2520enhances%2520alignment%2520accuracy%2520for%2520high%250Adimensional%2520time-dependent%2520views%2520be%2520performing%2520dynamic%2520time%2520warping%2520on%2520data%250Aembedded%2520in%2520maximally%2520correlated%2520subspace%2520which%2520handles%2520sparsity%2520with%2520novel%250Afeature%2520selection%2520method.%2520We%2520validate%2520the%2520effectiveness%2520of%2520CDCTW%2520through%250Aextensive%2520experiments%2520on%2520various%2520datasets%252C%2520demonstrating%2520superior%2520performance%250Aover%2520previous%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18234v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conditional%20Deep%20Canonical%20Time%20Warping&entry.906535625=Afek%20Steinberg%20and%20Ran%20Eisenberg%20and%20Ofir%20Lindenbaum&entry.1292438233=%20%20Temporal%20alignment%20of%20sequences%20is%20a%20fundamental%20challenge%20in%20many%0Aapplications%2C%20such%20as%20computer%20vision%20and%20bioinformatics%2C%20where%20local%20time%0Ashifting%20needs%20to%20be%20accounted%20for.%20Misalignment%20can%20lead%20to%20poor%20model%0Ageneralization%2C%20especially%20in%20high-dimensional%20sequences.%20Existing%20methods%0Aoften%20struggle%20with%20optimization%20when%20dealing%20with%20high-dimensional%20sparse%0Adata%2C%20falling%20into%20poor%20alignments.%20Feature%20selection%20is%20frequently%20used%20to%0Aenhance%20model%20performance%20for%20sparse%20data.%20However%2C%20a%20fixed%20set%20of%20selected%0Afeatures%20would%20not%20generally%20work%20for%20dynamically%20changing%20sequences%20and%20would%0Aneed%20to%20be%20modified%20based%20on%20the%20state%20of%20the%20sequence.%20Therefore%2C%20modifying%0Athe%20selected%20feature%20based%20on%20contextual%20input%20would%20result%20in%20better%0Aalignment.%20Our%20suggested%20method%2C%20Conditional%20Deep%20Canonical%20Temporal%20Time%0AWarping%20%28CDCTW%29%2C%20is%20designed%20for%20temporal%20alignment%20in%20sparse%20temporal%20data%20to%0Aaddress%20these%20challenges.%20CDCTW%20enhances%20alignment%20accuracy%20for%20high%0Adimensional%20time-dependent%20views%20be%20performing%20dynamic%20time%20warping%20on%20data%0Aembedded%20in%20maximally%20correlated%20subspace%20which%20handles%20sparsity%20with%20novel%0Afeature%20selection%20method.%20We%20validate%20the%20effectiveness%20of%20CDCTW%20through%0Aextensive%20experiments%20on%20various%20datasets%2C%20demonstrating%20superior%20performance%0Aover%20previous%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18234v2&entry.124074799=Read"},
{"title": "The explanation dialogues: an expert focus study to understand\n  requirements towards explanations within the GDPR", "author": "Laura State and Alejandra Bringas Colmenarejo and Andrea Beretta and Salvatore Ruggieri and Franco Turini and Stephanie Law", "abstract": "  Explainable AI (XAI) provides methods to understand non-interpretable machine\nlearning models. However, we have little knowledge about what legal experts\nexpect from these explanations, including their legal compliance with, and\nvalue against European Union legislation. To close this gap, we present the\nExplanation Dialogues, an expert focus study to uncover the expectations,\nreasoning, and understanding of legal experts and practitioners towards XAI,\nwith a specific focus on the European General Data Protection Regulation. The\nstudy consists of an online questionnaire and follow-up interviews, and is\ncentered around a use-case in the credit domain. We extract both a set of\nhierarchical and interconnected codes using grounded theory, and present the\nstandpoints of the participating experts towards XAI. We find that the\npresented explanations are hard to understand and lack information, and discuss\nissues that can arise from the different interests of the data controller and\nsubject. Finally, we present a set of recommendations for developers of XAI\nmethods, and indications of legal areas of discussion. Among others,\nrecommendations address the presentation, choice, and content of an\nexplanation, technical risks as well as the end-user, while we provide legal\npointers to the contestability of explanations, transparency thresholds,\nintellectual property rights as well as the relationship between involved\nparties.\n", "link": "http://arxiv.org/abs/2501.05325v1", "date": "2025-01-09", "relevancy": 1.8818, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4754}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4754}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20explanation%20dialogues%3A%20an%20expert%20focus%20study%20to%20understand%0A%20%20requirements%20towards%20explanations%20within%20the%20GDPR&body=Title%3A%20The%20explanation%20dialogues%3A%20an%20expert%20focus%20study%20to%20understand%0A%20%20requirements%20towards%20explanations%20within%20the%20GDPR%0AAuthor%3A%20Laura%20State%20and%20Alejandra%20Bringas%20Colmenarejo%20and%20Andrea%20Beretta%20and%20Salvatore%20Ruggieri%20and%20Franco%20Turini%20and%20Stephanie%20Law%0AAbstract%3A%20%20%20Explainable%20AI%20%28XAI%29%20provides%20methods%20to%20understand%20non-interpretable%20machine%0Alearning%20models.%20However%2C%20we%20have%20little%20knowledge%20about%20what%20legal%20experts%0Aexpect%20from%20these%20explanations%2C%20including%20their%20legal%20compliance%20with%2C%20and%0Avalue%20against%20European%20Union%20legislation.%20To%20close%20this%20gap%2C%20we%20present%20the%0AExplanation%20Dialogues%2C%20an%20expert%20focus%20study%20to%20uncover%20the%20expectations%2C%0Areasoning%2C%20and%20understanding%20of%20legal%20experts%20and%20practitioners%20towards%20XAI%2C%0Awith%20a%20specific%20focus%20on%20the%20European%20General%20Data%20Protection%20Regulation.%20The%0Astudy%20consists%20of%20an%20online%20questionnaire%20and%20follow-up%20interviews%2C%20and%20is%0Acentered%20around%20a%20use-case%20in%20the%20credit%20domain.%20We%20extract%20both%20a%20set%20of%0Ahierarchical%20and%20interconnected%20codes%20using%20grounded%20theory%2C%20and%20present%20the%0Astandpoints%20of%20the%20participating%20experts%20towards%20XAI.%20We%20find%20that%20the%0Apresented%20explanations%20are%20hard%20to%20understand%20and%20lack%20information%2C%20and%20discuss%0Aissues%20that%20can%20arise%20from%20the%20different%20interests%20of%20the%20data%20controller%20and%0Asubject.%20Finally%2C%20we%20present%20a%20set%20of%20recommendations%20for%20developers%20of%20XAI%0Amethods%2C%20and%20indications%20of%20legal%20areas%20of%20discussion.%20Among%20others%2C%0Arecommendations%20address%20the%20presentation%2C%20choice%2C%20and%20content%20of%20an%0Aexplanation%2C%20technical%20risks%20as%20well%20as%20the%20end-user%2C%20while%20we%20provide%20legal%0Apointers%20to%20the%20contestability%20of%20explanations%2C%20transparency%20thresholds%2C%0Aintellectual%20property%20rights%20as%20well%20as%20the%20relationship%20between%20involved%0Aparties.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05325v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520explanation%2520dialogues%253A%2520an%2520expert%2520focus%2520study%2520to%2520understand%250A%2520%2520requirements%2520towards%2520explanations%2520within%2520the%2520GDPR%26entry.906535625%3DLaura%2520State%2520and%2520Alejandra%2520Bringas%2520Colmenarejo%2520and%2520Andrea%2520Beretta%2520and%2520Salvatore%2520Ruggieri%2520and%2520Franco%2520Turini%2520and%2520Stephanie%2520Law%26entry.1292438233%3D%2520%2520Explainable%2520AI%2520%2528XAI%2529%2520provides%2520methods%2520to%2520understand%2520non-interpretable%2520machine%250Alearning%2520models.%2520However%252C%2520we%2520have%2520little%2520knowledge%2520about%2520what%2520legal%2520experts%250Aexpect%2520from%2520these%2520explanations%252C%2520including%2520their%2520legal%2520compliance%2520with%252C%2520and%250Avalue%2520against%2520European%2520Union%2520legislation.%2520To%2520close%2520this%2520gap%252C%2520we%2520present%2520the%250AExplanation%2520Dialogues%252C%2520an%2520expert%2520focus%2520study%2520to%2520uncover%2520the%2520expectations%252C%250Areasoning%252C%2520and%2520understanding%2520of%2520legal%2520experts%2520and%2520practitioners%2520towards%2520XAI%252C%250Awith%2520a%2520specific%2520focus%2520on%2520the%2520European%2520General%2520Data%2520Protection%2520Regulation.%2520The%250Astudy%2520consists%2520of%2520an%2520online%2520questionnaire%2520and%2520follow-up%2520interviews%252C%2520and%2520is%250Acentered%2520around%2520a%2520use-case%2520in%2520the%2520credit%2520domain.%2520We%2520extract%2520both%2520a%2520set%2520of%250Ahierarchical%2520and%2520interconnected%2520codes%2520using%2520grounded%2520theory%252C%2520and%2520present%2520the%250Astandpoints%2520of%2520the%2520participating%2520experts%2520towards%2520XAI.%2520We%2520find%2520that%2520the%250Apresented%2520explanations%2520are%2520hard%2520to%2520understand%2520and%2520lack%2520information%252C%2520and%2520discuss%250Aissues%2520that%2520can%2520arise%2520from%2520the%2520different%2520interests%2520of%2520the%2520data%2520controller%2520and%250Asubject.%2520Finally%252C%2520we%2520present%2520a%2520set%2520of%2520recommendations%2520for%2520developers%2520of%2520XAI%250Amethods%252C%2520and%2520indications%2520of%2520legal%2520areas%2520of%2520discussion.%2520Among%2520others%252C%250Arecommendations%2520address%2520the%2520presentation%252C%2520choice%252C%2520and%2520content%2520of%2520an%250Aexplanation%252C%2520technical%2520risks%2520as%2520well%2520as%2520the%2520end-user%252C%2520while%2520we%2520provide%2520legal%250Apointers%2520to%2520the%2520contestability%2520of%2520explanations%252C%2520transparency%2520thresholds%252C%250Aintellectual%2520property%2520rights%2520as%2520well%2520as%2520the%2520relationship%2520between%2520involved%250Aparties.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05325v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20explanation%20dialogues%3A%20an%20expert%20focus%20study%20to%20understand%0A%20%20requirements%20towards%20explanations%20within%20the%20GDPR&entry.906535625=Laura%20State%20and%20Alejandra%20Bringas%20Colmenarejo%20and%20Andrea%20Beretta%20and%20Salvatore%20Ruggieri%20and%20Franco%20Turini%20and%20Stephanie%20Law&entry.1292438233=%20%20Explainable%20AI%20%28XAI%29%20provides%20methods%20to%20understand%20non-interpretable%20machine%0Alearning%20models.%20However%2C%20we%20have%20little%20knowledge%20about%20what%20legal%20experts%0Aexpect%20from%20these%20explanations%2C%20including%20their%20legal%20compliance%20with%2C%20and%0Avalue%20against%20European%20Union%20legislation.%20To%20close%20this%20gap%2C%20we%20present%20the%0AExplanation%20Dialogues%2C%20an%20expert%20focus%20study%20to%20uncover%20the%20expectations%2C%0Areasoning%2C%20and%20understanding%20of%20legal%20experts%20and%20practitioners%20towards%20XAI%2C%0Awith%20a%20specific%20focus%20on%20the%20European%20General%20Data%20Protection%20Regulation.%20The%0Astudy%20consists%20of%20an%20online%20questionnaire%20and%20follow-up%20interviews%2C%20and%20is%0Acentered%20around%20a%20use-case%20in%20the%20credit%20domain.%20We%20extract%20both%20a%20set%20of%0Ahierarchical%20and%20interconnected%20codes%20using%20grounded%20theory%2C%20and%20present%20the%0Astandpoints%20of%20the%20participating%20experts%20towards%20XAI.%20We%20find%20that%20the%0Apresented%20explanations%20are%20hard%20to%20understand%20and%20lack%20information%2C%20and%20discuss%0Aissues%20that%20can%20arise%20from%20the%20different%20interests%20of%20the%20data%20controller%20and%0Asubject.%20Finally%2C%20we%20present%20a%20set%20of%20recommendations%20for%20developers%20of%20XAI%0Amethods%2C%20and%20indications%20of%20legal%20areas%20of%20discussion.%20Among%20others%2C%0Arecommendations%20address%20the%20presentation%2C%20choice%2C%20and%20content%20of%20an%0Aexplanation%2C%20technical%20risks%20as%20well%20as%20the%20end-user%2C%20while%20we%20provide%20legal%0Apointers%20to%20the%20contestability%20of%20explanations%2C%20transparency%20thresholds%2C%0Aintellectual%20property%20rights%20as%20well%20as%20the%20relationship%20between%20involved%0Aparties.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05325v1&entry.124074799=Read"},
{"title": "Time Transfer: On Optimal Learning Rate and Batch Size In The Infinite\n  Data Limit", "author": "Oleg Filatov and Jan Ebert and Jiangtao Wang and Stefan Kesselheim", "abstract": "  One of the main challenges in optimal scaling of large language models (LLMs)\nis the prohibitive cost of hyperparameter tuning, particularly learning rate\n$\\eta$ and batch size $B$. While techniques like $\\mu$P (Yang et al., 2022)\nprovide scaling rules for optimal $\\eta$ transfer in the infinite model size\nlimit, the optimal scaling behavior in the infinite data size limit remains\nunknown. We fill in this gap by observing for the first time an intricate\ndependence of optimal $\\eta$ scaling on the pretraining token budget $T$, $B$\nand its relation to the critical batch size $B_\\mathrm{crit}$, which we measure\nto evolve as $B_\\mathrm{crit} \\propto T$. Furthermore, we show that the optimal\nbatch size is positively correlated with $B_\\mathrm{crit}$: keeping it fixed\nbecomes suboptimal over time even if learning rate is scaled optimally.\nSurprisingly, our results demonstrate that the observed optimal $\\eta$ and $B$\ndynamics are preserved with $\\mu$P model scaling, challenging the conventional\nview of $B_\\mathrm{crit}$ dependence solely on loss value. Complementing\noptimality, we examine the sensitivity of loss to changes in learning rate,\nwhere we find the sensitivity to decrease with increase of $T$ and to remain\nconstant with $\\mu$P model scaling. We hope our results make the first step\ntowards a unified picture of the joint optimal data and model scaling.\n", "link": "http://arxiv.org/abs/2410.05838v2", "date": "2025-01-09", "relevancy": 1.8778, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5179}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4647}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Time%20Transfer%3A%20On%20Optimal%20Learning%20Rate%20and%20Batch%20Size%20In%20The%20Infinite%0A%20%20Data%20Limit&body=Title%3A%20Time%20Transfer%3A%20On%20Optimal%20Learning%20Rate%20and%20Batch%20Size%20In%20The%20Infinite%0A%20%20Data%20Limit%0AAuthor%3A%20Oleg%20Filatov%20and%20Jan%20Ebert%20and%20Jiangtao%20Wang%20and%20Stefan%20Kesselheim%0AAbstract%3A%20%20%20One%20of%20the%20main%20challenges%20in%20optimal%20scaling%20of%20large%20language%20models%20%28LLMs%29%0Ais%20the%20prohibitive%20cost%20of%20hyperparameter%20tuning%2C%20particularly%20learning%20rate%0A%24%5Ceta%24%20and%20batch%20size%20%24B%24.%20While%20techniques%20like%20%24%5Cmu%24P%20%28Yang%20et%20al.%2C%202022%29%0Aprovide%20scaling%20rules%20for%20optimal%20%24%5Ceta%24%20transfer%20in%20the%20infinite%20model%20size%0Alimit%2C%20the%20optimal%20scaling%20behavior%20in%20the%20infinite%20data%20size%20limit%20remains%0Aunknown.%20We%20fill%20in%20this%20gap%20by%20observing%20for%20the%20first%20time%20an%20intricate%0Adependence%20of%20optimal%20%24%5Ceta%24%20scaling%20on%20the%20pretraining%20token%20budget%20%24T%24%2C%20%24B%24%0Aand%20its%20relation%20to%20the%20critical%20batch%20size%20%24B_%5Cmathrm%7Bcrit%7D%24%2C%20which%20we%20measure%0Ato%20evolve%20as%20%24B_%5Cmathrm%7Bcrit%7D%20%5Cpropto%20T%24.%20Furthermore%2C%20we%20show%20that%20the%20optimal%0Abatch%20size%20is%20positively%20correlated%20with%20%24B_%5Cmathrm%7Bcrit%7D%24%3A%20keeping%20it%20fixed%0Abecomes%20suboptimal%20over%20time%20even%20if%20learning%20rate%20is%20scaled%20optimally.%0ASurprisingly%2C%20our%20results%20demonstrate%20that%20the%20observed%20optimal%20%24%5Ceta%24%20and%20%24B%24%0Adynamics%20are%20preserved%20with%20%24%5Cmu%24P%20model%20scaling%2C%20challenging%20the%20conventional%0Aview%20of%20%24B_%5Cmathrm%7Bcrit%7D%24%20dependence%20solely%20on%20loss%20value.%20Complementing%0Aoptimality%2C%20we%20examine%20the%20sensitivity%20of%20loss%20to%20changes%20in%20learning%20rate%2C%0Awhere%20we%20find%20the%20sensitivity%20to%20decrease%20with%20increase%20of%20%24T%24%20and%20to%20remain%0Aconstant%20with%20%24%5Cmu%24P%20model%20scaling.%20We%20hope%20our%20results%20make%20the%20first%20step%0Atowards%20a%20unified%20picture%20of%20the%20joint%20optimal%20data%20and%20model%20scaling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05838v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTime%2520Transfer%253A%2520On%2520Optimal%2520Learning%2520Rate%2520and%2520Batch%2520Size%2520In%2520The%2520Infinite%250A%2520%2520Data%2520Limit%26entry.906535625%3DOleg%2520Filatov%2520and%2520Jan%2520Ebert%2520and%2520Jiangtao%2520Wang%2520and%2520Stefan%2520Kesselheim%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520main%2520challenges%2520in%2520optimal%2520scaling%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%250Ais%2520the%2520prohibitive%2520cost%2520of%2520hyperparameter%2520tuning%252C%2520particularly%2520learning%2520rate%250A%2524%255Ceta%2524%2520and%2520batch%2520size%2520%2524B%2524.%2520While%2520techniques%2520like%2520%2524%255Cmu%2524P%2520%2528Yang%2520et%2520al.%252C%25202022%2529%250Aprovide%2520scaling%2520rules%2520for%2520optimal%2520%2524%255Ceta%2524%2520transfer%2520in%2520the%2520infinite%2520model%2520size%250Alimit%252C%2520the%2520optimal%2520scaling%2520behavior%2520in%2520the%2520infinite%2520data%2520size%2520limit%2520remains%250Aunknown.%2520We%2520fill%2520in%2520this%2520gap%2520by%2520observing%2520for%2520the%2520first%2520time%2520an%2520intricate%250Adependence%2520of%2520optimal%2520%2524%255Ceta%2524%2520scaling%2520on%2520the%2520pretraining%2520token%2520budget%2520%2524T%2524%252C%2520%2524B%2524%250Aand%2520its%2520relation%2520to%2520the%2520critical%2520batch%2520size%2520%2524B_%255Cmathrm%257Bcrit%257D%2524%252C%2520which%2520we%2520measure%250Ato%2520evolve%2520as%2520%2524B_%255Cmathrm%257Bcrit%257D%2520%255Cpropto%2520T%2524.%2520Furthermore%252C%2520we%2520show%2520that%2520the%2520optimal%250Abatch%2520size%2520is%2520positively%2520correlated%2520with%2520%2524B_%255Cmathrm%257Bcrit%257D%2524%253A%2520keeping%2520it%2520fixed%250Abecomes%2520suboptimal%2520over%2520time%2520even%2520if%2520learning%2520rate%2520is%2520scaled%2520optimally.%250ASurprisingly%252C%2520our%2520results%2520demonstrate%2520that%2520the%2520observed%2520optimal%2520%2524%255Ceta%2524%2520and%2520%2524B%2524%250Adynamics%2520are%2520preserved%2520with%2520%2524%255Cmu%2524P%2520model%2520scaling%252C%2520challenging%2520the%2520conventional%250Aview%2520of%2520%2524B_%255Cmathrm%257Bcrit%257D%2524%2520dependence%2520solely%2520on%2520loss%2520value.%2520Complementing%250Aoptimality%252C%2520we%2520examine%2520the%2520sensitivity%2520of%2520loss%2520to%2520changes%2520in%2520learning%2520rate%252C%250Awhere%2520we%2520find%2520the%2520sensitivity%2520to%2520decrease%2520with%2520increase%2520of%2520%2524T%2524%2520and%2520to%2520remain%250Aconstant%2520with%2520%2524%255Cmu%2524P%2520model%2520scaling.%2520We%2520hope%2520our%2520results%2520make%2520the%2520first%2520step%250Atowards%2520a%2520unified%2520picture%2520of%2520the%2520joint%2520optimal%2520data%2520and%2520model%2520scaling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05838v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Time%20Transfer%3A%20On%20Optimal%20Learning%20Rate%20and%20Batch%20Size%20In%20The%20Infinite%0A%20%20Data%20Limit&entry.906535625=Oleg%20Filatov%20and%20Jan%20Ebert%20and%20Jiangtao%20Wang%20and%20Stefan%20Kesselheim&entry.1292438233=%20%20One%20of%20the%20main%20challenges%20in%20optimal%20scaling%20of%20large%20language%20models%20%28LLMs%29%0Ais%20the%20prohibitive%20cost%20of%20hyperparameter%20tuning%2C%20particularly%20learning%20rate%0A%24%5Ceta%24%20and%20batch%20size%20%24B%24.%20While%20techniques%20like%20%24%5Cmu%24P%20%28Yang%20et%20al.%2C%202022%29%0Aprovide%20scaling%20rules%20for%20optimal%20%24%5Ceta%24%20transfer%20in%20the%20infinite%20model%20size%0Alimit%2C%20the%20optimal%20scaling%20behavior%20in%20the%20infinite%20data%20size%20limit%20remains%0Aunknown.%20We%20fill%20in%20this%20gap%20by%20observing%20for%20the%20first%20time%20an%20intricate%0Adependence%20of%20optimal%20%24%5Ceta%24%20scaling%20on%20the%20pretraining%20token%20budget%20%24T%24%2C%20%24B%24%0Aand%20its%20relation%20to%20the%20critical%20batch%20size%20%24B_%5Cmathrm%7Bcrit%7D%24%2C%20which%20we%20measure%0Ato%20evolve%20as%20%24B_%5Cmathrm%7Bcrit%7D%20%5Cpropto%20T%24.%20Furthermore%2C%20we%20show%20that%20the%20optimal%0Abatch%20size%20is%20positively%20correlated%20with%20%24B_%5Cmathrm%7Bcrit%7D%24%3A%20keeping%20it%20fixed%0Abecomes%20suboptimal%20over%20time%20even%20if%20learning%20rate%20is%20scaled%20optimally.%0ASurprisingly%2C%20our%20results%20demonstrate%20that%20the%20observed%20optimal%20%24%5Ceta%24%20and%20%24B%24%0Adynamics%20are%20preserved%20with%20%24%5Cmu%24P%20model%20scaling%2C%20challenging%20the%20conventional%0Aview%20of%20%24B_%5Cmathrm%7Bcrit%7D%24%20dependence%20solely%20on%20loss%20value.%20Complementing%0Aoptimality%2C%20we%20examine%20the%20sensitivity%20of%20loss%20to%20changes%20in%20learning%20rate%2C%0Awhere%20we%20find%20the%20sensitivity%20to%20decrease%20with%20increase%20of%20%24T%24%20and%20to%20remain%0Aconstant%20with%20%24%5Cmu%24P%20model%20scaling.%20We%20hope%20our%20results%20make%20the%20first%20step%0Atowards%20a%20unified%20picture%20of%20the%20joint%20optimal%20data%20and%20model%20scaling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05838v2&entry.124074799=Read"},
{"title": "BRATI: Bidirectional Recurrent Attention for Time-Series Imputation", "author": "Armando Collado-Villaverde and Pablo Mu\u00f1oz and Maria D. R-Moreno", "abstract": "  Missing data in time-series analysis poses significant challenges, affecting\nthe reliability of downstream applications. Imputation, the process of\nestimating missing values, has emerged as a key solution. This paper introduces\nBRATI, a novel deep-learning model designed to address multivariate time-series\nimputation by combining Bidirectional Recurrent Networks and Attention\nmechanisms. BRATI processes temporal dependencies and feature correlations\nacross long and short time horizons, utilizing two imputation blocks that\noperate in opposite temporal directions. Each block integrates recurrent layers\nand attention mechanisms to effectively resolve long-term dependencies.\n  We evaluate BRATI on three real-world datasets under diverse missing-data\nscenarios: randomly missing values, fixed-length missing sequences, and\nvariable-length missing sequences. Our findings demonstrate that BRATI\nconsistently outperforms state-of-the-art models, delivering superior accuracy\nand robustness in imputing multivariate time-series data.\n", "link": "http://arxiv.org/abs/2501.05401v1", "date": "2025-01-09", "relevancy": 1.8762, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4753}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4689}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BRATI%3A%20Bidirectional%20Recurrent%20Attention%20for%20Time-Series%20Imputation&body=Title%3A%20BRATI%3A%20Bidirectional%20Recurrent%20Attention%20for%20Time-Series%20Imputation%0AAuthor%3A%20Armando%20Collado-Villaverde%20and%20Pablo%20Mu%C3%B1oz%20and%20Maria%20D.%20R-Moreno%0AAbstract%3A%20%20%20Missing%20data%20in%20time-series%20analysis%20poses%20significant%20challenges%2C%20affecting%0Athe%20reliability%20of%20downstream%20applications.%20Imputation%2C%20the%20process%20of%0Aestimating%20missing%20values%2C%20has%20emerged%20as%20a%20key%20solution.%20This%20paper%20introduces%0ABRATI%2C%20a%20novel%20deep-learning%20model%20designed%20to%20address%20multivariate%20time-series%0Aimputation%20by%20combining%20Bidirectional%20Recurrent%20Networks%20and%20Attention%0Amechanisms.%20BRATI%20processes%20temporal%20dependencies%20and%20feature%20correlations%0Aacross%20long%20and%20short%20time%20horizons%2C%20utilizing%20two%20imputation%20blocks%20that%0Aoperate%20in%20opposite%20temporal%20directions.%20Each%20block%20integrates%20recurrent%20layers%0Aand%20attention%20mechanisms%20to%20effectively%20resolve%20long-term%20dependencies.%0A%20%20We%20evaluate%20BRATI%20on%20three%20real-world%20datasets%20under%20diverse%20missing-data%0Ascenarios%3A%20randomly%20missing%20values%2C%20fixed-length%20missing%20sequences%2C%20and%0Avariable-length%20missing%20sequences.%20Our%20findings%20demonstrate%20that%20BRATI%0Aconsistently%20outperforms%20state-of-the-art%20models%2C%20delivering%20superior%20accuracy%0Aand%20robustness%20in%20imputing%20multivariate%20time-series%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05401v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBRATI%253A%2520Bidirectional%2520Recurrent%2520Attention%2520for%2520Time-Series%2520Imputation%26entry.906535625%3DArmando%2520Collado-Villaverde%2520and%2520Pablo%2520Mu%25C3%25B1oz%2520and%2520Maria%2520D.%2520R-Moreno%26entry.1292438233%3D%2520%2520Missing%2520data%2520in%2520time-series%2520analysis%2520poses%2520significant%2520challenges%252C%2520affecting%250Athe%2520reliability%2520of%2520downstream%2520applications.%2520Imputation%252C%2520the%2520process%2520of%250Aestimating%2520missing%2520values%252C%2520has%2520emerged%2520as%2520a%2520key%2520solution.%2520This%2520paper%2520introduces%250ABRATI%252C%2520a%2520novel%2520deep-learning%2520model%2520designed%2520to%2520address%2520multivariate%2520time-series%250Aimputation%2520by%2520combining%2520Bidirectional%2520Recurrent%2520Networks%2520and%2520Attention%250Amechanisms.%2520BRATI%2520processes%2520temporal%2520dependencies%2520and%2520feature%2520correlations%250Aacross%2520long%2520and%2520short%2520time%2520horizons%252C%2520utilizing%2520two%2520imputation%2520blocks%2520that%250Aoperate%2520in%2520opposite%2520temporal%2520directions.%2520Each%2520block%2520integrates%2520recurrent%2520layers%250Aand%2520attention%2520mechanisms%2520to%2520effectively%2520resolve%2520long-term%2520dependencies.%250A%2520%2520We%2520evaluate%2520BRATI%2520on%2520three%2520real-world%2520datasets%2520under%2520diverse%2520missing-data%250Ascenarios%253A%2520randomly%2520missing%2520values%252C%2520fixed-length%2520missing%2520sequences%252C%2520and%250Avariable-length%2520missing%2520sequences.%2520Our%2520findings%2520demonstrate%2520that%2520BRATI%250Aconsistently%2520outperforms%2520state-of-the-art%2520models%252C%2520delivering%2520superior%2520accuracy%250Aand%2520robustness%2520in%2520imputing%2520multivariate%2520time-series%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05401v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BRATI%3A%20Bidirectional%20Recurrent%20Attention%20for%20Time-Series%20Imputation&entry.906535625=Armando%20Collado-Villaverde%20and%20Pablo%20Mu%C3%B1oz%20and%20Maria%20D.%20R-Moreno&entry.1292438233=%20%20Missing%20data%20in%20time-series%20analysis%20poses%20significant%20challenges%2C%20affecting%0Athe%20reliability%20of%20downstream%20applications.%20Imputation%2C%20the%20process%20of%0Aestimating%20missing%20values%2C%20has%20emerged%20as%20a%20key%20solution.%20This%20paper%20introduces%0ABRATI%2C%20a%20novel%20deep-learning%20model%20designed%20to%20address%20multivariate%20time-series%0Aimputation%20by%20combining%20Bidirectional%20Recurrent%20Networks%20and%20Attention%0Amechanisms.%20BRATI%20processes%20temporal%20dependencies%20and%20feature%20correlations%0Aacross%20long%20and%20short%20time%20horizons%2C%20utilizing%20two%20imputation%20blocks%20that%0Aoperate%20in%20opposite%20temporal%20directions.%20Each%20block%20integrates%20recurrent%20layers%0Aand%20attention%20mechanisms%20to%20effectively%20resolve%20long-term%20dependencies.%0A%20%20We%20evaluate%20BRATI%20on%20three%20real-world%20datasets%20under%20diverse%20missing-data%0Ascenarios%3A%20randomly%20missing%20values%2C%20fixed-length%20missing%20sequences%2C%20and%0Avariable-length%20missing%20sequences.%20Our%20findings%20demonstrate%20that%20BRATI%0Aconsistently%20outperforms%20state-of-the-art%20models%2C%20delivering%20superior%20accuracy%0Aand%20robustness%20in%20imputing%20multivariate%20time-series%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05401v1&entry.124074799=Read"},
{"title": "Exosense: A Vision-Based Scene Understanding System For Exoskeletons", "author": "Jianeng Wang and Matias Mattamala and Christina Kassab and Guillaume Burger and Fabio Elnecave and Lintong Zhang and Marine Petriaux and Maurice Fallon", "abstract": "  Self-balancing exoskeletons are a key enabling technology for individuals\nwith mobility impairments. While the current challenges focus on\nhuman-compliant hardware and control, unlocking their use for daily activities\nrequires a scene perception system. In this work, we present Exosense, a\nvision-centric scene understanding system for self-balancing exoskeletons. We\nintroduce a multi-sensor visual-inertial mapping device as well as a navigation\nstack for state estimation, terrain mapping and long-term operation. We tested\nExosense attached to both a human leg and Wandercraft's Personal Exoskeleton in\nreal-world indoor scenarios. This enabled us to test the system during typical\nperiodic walking gaits, as well as future uses in multi-story environments. We\ndemonstrate that Exosense can achieve an odometry drift of about 4 cm per meter\ntraveled, and construct terrain maps under 1 cm average reconstruction error.\nIt can also work in a visual localization mode in a previously mapped\nenvironment, providing a step towards long-term operation of exoskeletons.\n", "link": "http://arxiv.org/abs/2403.14320v3", "date": "2025-01-09", "relevancy": 1.8695, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6446}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5979}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exosense%3A%20A%20Vision-Based%20Scene%20Understanding%20System%20For%20Exoskeletons&body=Title%3A%20Exosense%3A%20A%20Vision-Based%20Scene%20Understanding%20System%20For%20Exoskeletons%0AAuthor%3A%20Jianeng%20Wang%20and%20Matias%20Mattamala%20and%20Christina%20Kassab%20and%20Guillaume%20Burger%20and%20Fabio%20Elnecave%20and%20Lintong%20Zhang%20and%20Marine%20Petriaux%20and%20Maurice%20Fallon%0AAbstract%3A%20%20%20Self-balancing%20exoskeletons%20are%20a%20key%20enabling%20technology%20for%20individuals%0Awith%20mobility%20impairments.%20While%20the%20current%20challenges%20focus%20on%0Ahuman-compliant%20hardware%20and%20control%2C%20unlocking%20their%20use%20for%20daily%20activities%0Arequires%20a%20scene%20perception%20system.%20In%20this%20work%2C%20we%20present%20Exosense%2C%20a%0Avision-centric%20scene%20understanding%20system%20for%20self-balancing%20exoskeletons.%20We%0Aintroduce%20a%20multi-sensor%20visual-inertial%20mapping%20device%20as%20well%20as%20a%20navigation%0Astack%20for%20state%20estimation%2C%20terrain%20mapping%20and%20long-term%20operation.%20We%20tested%0AExosense%20attached%20to%20both%20a%20human%20leg%20and%20Wandercraft%27s%20Personal%20Exoskeleton%20in%0Areal-world%20indoor%20scenarios.%20This%20enabled%20us%20to%20test%20the%20system%20during%20typical%0Aperiodic%20walking%20gaits%2C%20as%20well%20as%20future%20uses%20in%20multi-story%20environments.%20We%0Ademonstrate%20that%20Exosense%20can%20achieve%20an%20odometry%20drift%20of%20about%204%20cm%20per%20meter%0Atraveled%2C%20and%20construct%20terrain%20maps%20under%201%20cm%20average%20reconstruction%20error.%0AIt%20can%20also%20work%20in%20a%20visual%20localization%20mode%20in%20a%20previously%20mapped%0Aenvironment%2C%20providing%20a%20step%20towards%20long-term%20operation%20of%20exoskeletons.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14320v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExosense%253A%2520A%2520Vision-Based%2520Scene%2520Understanding%2520System%2520For%2520Exoskeletons%26entry.906535625%3DJianeng%2520Wang%2520and%2520Matias%2520Mattamala%2520and%2520Christina%2520Kassab%2520and%2520Guillaume%2520Burger%2520and%2520Fabio%2520Elnecave%2520and%2520Lintong%2520Zhang%2520and%2520Marine%2520Petriaux%2520and%2520Maurice%2520Fallon%26entry.1292438233%3D%2520%2520Self-balancing%2520exoskeletons%2520are%2520a%2520key%2520enabling%2520technology%2520for%2520individuals%250Awith%2520mobility%2520impairments.%2520While%2520the%2520current%2520challenges%2520focus%2520on%250Ahuman-compliant%2520hardware%2520and%2520control%252C%2520unlocking%2520their%2520use%2520for%2520daily%2520activities%250Arequires%2520a%2520scene%2520perception%2520system.%2520In%2520this%2520work%252C%2520we%2520present%2520Exosense%252C%2520a%250Avision-centric%2520scene%2520understanding%2520system%2520for%2520self-balancing%2520exoskeletons.%2520We%250Aintroduce%2520a%2520multi-sensor%2520visual-inertial%2520mapping%2520device%2520as%2520well%2520as%2520a%2520navigation%250Astack%2520for%2520state%2520estimation%252C%2520terrain%2520mapping%2520and%2520long-term%2520operation.%2520We%2520tested%250AExosense%2520attached%2520to%2520both%2520a%2520human%2520leg%2520and%2520Wandercraft%2527s%2520Personal%2520Exoskeleton%2520in%250Areal-world%2520indoor%2520scenarios.%2520This%2520enabled%2520us%2520to%2520test%2520the%2520system%2520during%2520typical%250Aperiodic%2520walking%2520gaits%252C%2520as%2520well%2520as%2520future%2520uses%2520in%2520multi-story%2520environments.%2520We%250Ademonstrate%2520that%2520Exosense%2520can%2520achieve%2520an%2520odometry%2520drift%2520of%2520about%25204%2520cm%2520per%2520meter%250Atraveled%252C%2520and%2520construct%2520terrain%2520maps%2520under%25201%2520cm%2520average%2520reconstruction%2520error.%250AIt%2520can%2520also%2520work%2520in%2520a%2520visual%2520localization%2520mode%2520in%2520a%2520previously%2520mapped%250Aenvironment%252C%2520providing%2520a%2520step%2520towards%2520long-term%2520operation%2520of%2520exoskeletons.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.14320v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exosense%3A%20A%20Vision-Based%20Scene%20Understanding%20System%20For%20Exoskeletons&entry.906535625=Jianeng%20Wang%20and%20Matias%20Mattamala%20and%20Christina%20Kassab%20and%20Guillaume%20Burger%20and%20Fabio%20Elnecave%20and%20Lintong%20Zhang%20and%20Marine%20Petriaux%20and%20Maurice%20Fallon&entry.1292438233=%20%20Self-balancing%20exoskeletons%20are%20a%20key%20enabling%20technology%20for%20individuals%0Awith%20mobility%20impairments.%20While%20the%20current%20challenges%20focus%20on%0Ahuman-compliant%20hardware%20and%20control%2C%20unlocking%20their%20use%20for%20daily%20activities%0Arequires%20a%20scene%20perception%20system.%20In%20this%20work%2C%20we%20present%20Exosense%2C%20a%0Avision-centric%20scene%20understanding%20system%20for%20self-balancing%20exoskeletons.%20We%0Aintroduce%20a%20multi-sensor%20visual-inertial%20mapping%20device%20as%20well%20as%20a%20navigation%0Astack%20for%20state%20estimation%2C%20terrain%20mapping%20and%20long-term%20operation.%20We%20tested%0AExosense%20attached%20to%20both%20a%20human%20leg%20and%20Wandercraft%27s%20Personal%20Exoskeleton%20in%0Areal-world%20indoor%20scenarios.%20This%20enabled%20us%20to%20test%20the%20system%20during%20typical%0Aperiodic%20walking%20gaits%2C%20as%20well%20as%20future%20uses%20in%20multi-story%20environments.%20We%0Ademonstrate%20that%20Exosense%20can%20achieve%20an%20odometry%20drift%20of%20about%204%20cm%20per%20meter%0Atraveled%2C%20and%20construct%20terrain%20maps%20under%201%20cm%20average%20reconstruction%20error.%0AIt%20can%20also%20work%20in%20a%20visual%20localization%20mode%20in%20a%20previously%20mapped%0Aenvironment%2C%20providing%20a%20step%20towards%20long-term%20operation%20of%20exoskeletons.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14320v3&entry.124074799=Read"},
{"title": "Domain-Incremental Semantic Segmentation for Autonomous Driving under\n  Adverse Driving Conditions", "author": "Shishir Muralidhara and Ren\u00e9 Schuster and Didier Stricker", "abstract": "  Semantic segmentation for autonomous driving is an even more challenging task\nwhen faced with adverse driving conditions. Standard models trained on data\nrecorded under ideal conditions show a deteriorated performance in unfavorable\nweather or illumination conditions. Fine-tuning on the new task or condition\nwould lead to overwriting the previously learned information resulting in\ncatastrophic forgetting. Adapting to the new conditions through traditional\ndomain adaption methods improves the performance on the target domain at the\nexpense of the source domain. Addressing these issues, we propose an\narchitecture-based domain-incremental learning approach called Progressive\nSemantic Segmentation (PSS). PSS is a task-agnostic, dynamically growing\ncollection of domain-specific segmentation models. The task of inferring the\ndomain and subsequently selecting the appropriate module for segmentation is\ncarried out using a collection of convolutional autoencoders. We extensively\nevaluate our proposed approach using several datasets at varying levels of\ngranularity in the categorization of adverse driving conditions. Furthermore,\nwe demonstrate the generalization of the proposed approach to similar and\nunseen domains.\n", "link": "http://arxiv.org/abs/2501.05246v1", "date": "2025-01-09", "relevancy": 1.6147, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5553}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5384}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain-Incremental%20Semantic%20Segmentation%20for%20Autonomous%20Driving%20under%0A%20%20Adverse%20Driving%20Conditions&body=Title%3A%20Domain-Incremental%20Semantic%20Segmentation%20for%20Autonomous%20Driving%20under%0A%20%20Adverse%20Driving%20Conditions%0AAuthor%3A%20Shishir%20Muralidhara%20and%20Ren%C3%A9%20Schuster%20and%20Didier%20Stricker%0AAbstract%3A%20%20%20Semantic%20segmentation%20for%20autonomous%20driving%20is%20an%20even%20more%20challenging%20task%0Awhen%20faced%20with%20adverse%20driving%20conditions.%20Standard%20models%20trained%20on%20data%0Arecorded%20under%20ideal%20conditions%20show%20a%20deteriorated%20performance%20in%20unfavorable%0Aweather%20or%20illumination%20conditions.%20Fine-tuning%20on%20the%20new%20task%20or%20condition%0Awould%20lead%20to%20overwriting%20the%20previously%20learned%20information%20resulting%20in%0Acatastrophic%20forgetting.%20Adapting%20to%20the%20new%20conditions%20through%20traditional%0Adomain%20adaption%20methods%20improves%20the%20performance%20on%20the%20target%20domain%20at%20the%0Aexpense%20of%20the%20source%20domain.%20Addressing%20these%20issues%2C%20we%20propose%20an%0Aarchitecture-based%20domain-incremental%20learning%20approach%20called%20Progressive%0ASemantic%20Segmentation%20%28PSS%29.%20PSS%20is%20a%20task-agnostic%2C%20dynamically%20growing%0Acollection%20of%20domain-specific%20segmentation%20models.%20The%20task%20of%20inferring%20the%0Adomain%20and%20subsequently%20selecting%20the%20appropriate%20module%20for%20segmentation%20is%0Acarried%20out%20using%20a%20collection%20of%20convolutional%20autoencoders.%20We%20extensively%0Aevaluate%20our%20proposed%20approach%20using%20several%20datasets%20at%20varying%20levels%20of%0Agranularity%20in%20the%20categorization%20of%20adverse%20driving%20conditions.%20Furthermore%2C%0Awe%20demonstrate%20the%20generalization%20of%20the%20proposed%20approach%20to%20similar%20and%0Aunseen%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05246v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain-Incremental%2520Semantic%2520Segmentation%2520for%2520Autonomous%2520Driving%2520under%250A%2520%2520Adverse%2520Driving%2520Conditions%26entry.906535625%3DShishir%2520Muralidhara%2520and%2520Ren%25C3%25A9%2520Schuster%2520and%2520Didier%2520Stricker%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520for%2520autonomous%2520driving%2520is%2520an%2520even%2520more%2520challenging%2520task%250Awhen%2520faced%2520with%2520adverse%2520driving%2520conditions.%2520Standard%2520models%2520trained%2520on%2520data%250Arecorded%2520under%2520ideal%2520conditions%2520show%2520a%2520deteriorated%2520performance%2520in%2520unfavorable%250Aweather%2520or%2520illumination%2520conditions.%2520Fine-tuning%2520on%2520the%2520new%2520task%2520or%2520condition%250Awould%2520lead%2520to%2520overwriting%2520the%2520previously%2520learned%2520information%2520resulting%2520in%250Acatastrophic%2520forgetting.%2520Adapting%2520to%2520the%2520new%2520conditions%2520through%2520traditional%250Adomain%2520adaption%2520methods%2520improves%2520the%2520performance%2520on%2520the%2520target%2520domain%2520at%2520the%250Aexpense%2520of%2520the%2520source%2520domain.%2520Addressing%2520these%2520issues%252C%2520we%2520propose%2520an%250Aarchitecture-based%2520domain-incremental%2520learning%2520approach%2520called%2520Progressive%250ASemantic%2520Segmentation%2520%2528PSS%2529.%2520PSS%2520is%2520a%2520task-agnostic%252C%2520dynamically%2520growing%250Acollection%2520of%2520domain-specific%2520segmentation%2520models.%2520The%2520task%2520of%2520inferring%2520the%250Adomain%2520and%2520subsequently%2520selecting%2520the%2520appropriate%2520module%2520for%2520segmentation%2520is%250Acarried%2520out%2520using%2520a%2520collection%2520of%2520convolutional%2520autoencoders.%2520We%2520extensively%250Aevaluate%2520our%2520proposed%2520approach%2520using%2520several%2520datasets%2520at%2520varying%2520levels%2520of%250Agranularity%2520in%2520the%2520categorization%2520of%2520adverse%2520driving%2520conditions.%2520Furthermore%252C%250Awe%2520demonstrate%2520the%2520generalization%2520of%2520the%2520proposed%2520approach%2520to%2520similar%2520and%250Aunseen%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05246v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain-Incremental%20Semantic%20Segmentation%20for%20Autonomous%20Driving%20under%0A%20%20Adverse%20Driving%20Conditions&entry.906535625=Shishir%20Muralidhara%20and%20Ren%C3%A9%20Schuster%20and%20Didier%20Stricker&entry.1292438233=%20%20Semantic%20segmentation%20for%20autonomous%20driving%20is%20an%20even%20more%20challenging%20task%0Awhen%20faced%20with%20adverse%20driving%20conditions.%20Standard%20models%20trained%20on%20data%0Arecorded%20under%20ideal%20conditions%20show%20a%20deteriorated%20performance%20in%20unfavorable%0Aweather%20or%20illumination%20conditions.%20Fine-tuning%20on%20the%20new%20task%20or%20condition%0Awould%20lead%20to%20overwriting%20the%20previously%20learned%20information%20resulting%20in%0Acatastrophic%20forgetting.%20Adapting%20to%20the%20new%20conditions%20through%20traditional%0Adomain%20adaption%20methods%20improves%20the%20performance%20on%20the%20target%20domain%20at%20the%0Aexpense%20of%20the%20source%20domain.%20Addressing%20these%20issues%2C%20we%20propose%20an%0Aarchitecture-based%20domain-incremental%20learning%20approach%20called%20Progressive%0ASemantic%20Segmentation%20%28PSS%29.%20PSS%20is%20a%20task-agnostic%2C%20dynamically%20growing%0Acollection%20of%20domain-specific%20segmentation%20models.%20The%20task%20of%20inferring%20the%0Adomain%20and%20subsequently%20selecting%20the%20appropriate%20module%20for%20segmentation%20is%0Acarried%20out%20using%20a%20collection%20of%20convolutional%20autoencoders.%20We%20extensively%0Aevaluate%20our%20proposed%20approach%20using%20several%20datasets%20at%20varying%20levels%20of%0Agranularity%20in%20the%20categorization%20of%20adverse%20driving%20conditions.%20Furthermore%2C%0Awe%20demonstrate%20the%20generalization%20of%20the%20proposed%20approach%20to%20similar%20and%0Aunseen%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05246v1&entry.124074799=Read"},
{"title": "Regret Analysis: a control perspective", "author": "Travis E. Gibson and Sawal Acharya", "abstract": "  Online learning and model reference adaptive control have many interesting\nintersections. One area where they differ however is in how the algorithms are\nanalyzed and what objective or metric is used to discriminate \"good\" algorithms\nfrom \"bad\" algorithms. In adaptive control there are usually two objectives: 1)\nprove that all time varying parameters/states of the system are bounded, and 2)\nthat the instantaneous error between the adaptively controlled system and a\nreference system converges to zero over time (or at least a compact set). For\nonline learning the performance of algorithms is often characterized by the\nregret the algorithm incurs. Regret is defined as the cumulative loss (cost)\nover time from the online algorithm minus the cumulative loss (cost) of the\nsingle optimal fixed parameter choice in hindsight. Another significant\ndifference between the two areas of research is with regard to the assumptions\nmade in order to obtain said results. Adaptive control makes assumptions about\nthe input-output properties of the control problem and derives solutions for a\nfixed error model or optimization task. In the online learning literature\nresults are derived for classes of loss functions (i.e. convex) while a priori\nassuming that all time varying parameters are bounded, which for many\noptimization tasks is not unrealistic, but is a non starter in control\napplications. In this work we discuss these differences in detail through the\nregret based analysis of gradient descent for convex functions and the control\nbased analysis of a streaming regression problem. We close with a discussion\nabout the newly defined paradigm of online adaptive control and ask the\nfollowing question \"Are regret optimal control strategies deployable?\"\n", "link": "http://arxiv.org/abs/2501.04572v2", "date": "2025-01-09", "relevancy": 1.7988, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4881}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4444}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4135}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Regret%20Analysis%3A%20a%20control%20perspective&body=Title%3A%20Regret%20Analysis%3A%20a%20control%20perspective%0AAuthor%3A%20Travis%20E.%20Gibson%20and%20Sawal%20Acharya%0AAbstract%3A%20%20%20Online%20learning%20and%20model%20reference%20adaptive%20control%20have%20many%20interesting%0Aintersections.%20One%20area%20where%20they%20differ%20however%20is%20in%20how%20the%20algorithms%20are%0Aanalyzed%20and%20what%20objective%20or%20metric%20is%20used%20to%20discriminate%20%22good%22%20algorithms%0Afrom%20%22bad%22%20algorithms.%20In%20adaptive%20control%20there%20are%20usually%20two%20objectives%3A%201%29%0Aprove%20that%20all%20time%20varying%20parameters/states%20of%20the%20system%20are%20bounded%2C%20and%202%29%0Athat%20the%20instantaneous%20error%20between%20the%20adaptively%20controlled%20system%20and%20a%0Areference%20system%20converges%20to%20zero%20over%20time%20%28or%20at%20least%20a%20compact%20set%29.%20For%0Aonline%20learning%20the%20performance%20of%20algorithms%20is%20often%20characterized%20by%20the%0Aregret%20the%20algorithm%20incurs.%20Regret%20is%20defined%20as%20the%20cumulative%20loss%20%28cost%29%0Aover%20time%20from%20the%20online%20algorithm%20minus%20the%20cumulative%20loss%20%28cost%29%20of%20the%0Asingle%20optimal%20fixed%20parameter%20choice%20in%20hindsight.%20Another%20significant%0Adifference%20between%20the%20two%20areas%20of%20research%20is%20with%20regard%20to%20the%20assumptions%0Amade%20in%20order%20to%20obtain%20said%20results.%20Adaptive%20control%20makes%20assumptions%20about%0Athe%20input-output%20properties%20of%20the%20control%20problem%20and%20derives%20solutions%20for%20a%0Afixed%20error%20model%20or%20optimization%20task.%20In%20the%20online%20learning%20literature%0Aresults%20are%20derived%20for%20classes%20of%20loss%20functions%20%28i.e.%20convex%29%20while%20a%20priori%0Aassuming%20that%20all%20time%20varying%20parameters%20are%20bounded%2C%20which%20for%20many%0Aoptimization%20tasks%20is%20not%20unrealistic%2C%20but%20is%20a%20non%20starter%20in%20control%0Aapplications.%20In%20this%20work%20we%20discuss%20these%20differences%20in%20detail%20through%20the%0Aregret%20based%20analysis%20of%20gradient%20descent%20for%20convex%20functions%20and%20the%20control%0Abased%20analysis%20of%20a%20streaming%20regression%20problem.%20We%20close%20with%20a%20discussion%0Aabout%20the%20newly%20defined%20paradigm%20of%20online%20adaptive%20control%20and%20ask%20the%0Afollowing%20question%20%22Are%20regret%20optimal%20control%20strategies%20deployable%3F%22%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04572v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegret%2520Analysis%253A%2520a%2520control%2520perspective%26entry.906535625%3DTravis%2520E.%2520Gibson%2520and%2520Sawal%2520Acharya%26entry.1292438233%3D%2520%2520Online%2520learning%2520and%2520model%2520reference%2520adaptive%2520control%2520have%2520many%2520interesting%250Aintersections.%2520One%2520area%2520where%2520they%2520differ%2520however%2520is%2520in%2520how%2520the%2520algorithms%2520are%250Aanalyzed%2520and%2520what%2520objective%2520or%2520metric%2520is%2520used%2520to%2520discriminate%2520%2522good%2522%2520algorithms%250Afrom%2520%2522bad%2522%2520algorithms.%2520In%2520adaptive%2520control%2520there%2520are%2520usually%2520two%2520objectives%253A%25201%2529%250Aprove%2520that%2520all%2520time%2520varying%2520parameters/states%2520of%2520the%2520system%2520are%2520bounded%252C%2520and%25202%2529%250Athat%2520the%2520instantaneous%2520error%2520between%2520the%2520adaptively%2520controlled%2520system%2520and%2520a%250Areference%2520system%2520converges%2520to%2520zero%2520over%2520time%2520%2528or%2520at%2520least%2520a%2520compact%2520set%2529.%2520For%250Aonline%2520learning%2520the%2520performance%2520of%2520algorithms%2520is%2520often%2520characterized%2520by%2520the%250Aregret%2520the%2520algorithm%2520incurs.%2520Regret%2520is%2520defined%2520as%2520the%2520cumulative%2520loss%2520%2528cost%2529%250Aover%2520time%2520from%2520the%2520online%2520algorithm%2520minus%2520the%2520cumulative%2520loss%2520%2528cost%2529%2520of%2520the%250Asingle%2520optimal%2520fixed%2520parameter%2520choice%2520in%2520hindsight.%2520Another%2520significant%250Adifference%2520between%2520the%2520two%2520areas%2520of%2520research%2520is%2520with%2520regard%2520to%2520the%2520assumptions%250Amade%2520in%2520order%2520to%2520obtain%2520said%2520results.%2520Adaptive%2520control%2520makes%2520assumptions%2520about%250Athe%2520input-output%2520properties%2520of%2520the%2520control%2520problem%2520and%2520derives%2520solutions%2520for%2520a%250Afixed%2520error%2520model%2520or%2520optimization%2520task.%2520In%2520the%2520online%2520learning%2520literature%250Aresults%2520are%2520derived%2520for%2520classes%2520of%2520loss%2520functions%2520%2528i.e.%2520convex%2529%2520while%2520a%2520priori%250Aassuming%2520that%2520all%2520time%2520varying%2520parameters%2520are%2520bounded%252C%2520which%2520for%2520many%250Aoptimization%2520tasks%2520is%2520not%2520unrealistic%252C%2520but%2520is%2520a%2520non%2520starter%2520in%2520control%250Aapplications.%2520In%2520this%2520work%2520we%2520discuss%2520these%2520differences%2520in%2520detail%2520through%2520the%250Aregret%2520based%2520analysis%2520of%2520gradient%2520descent%2520for%2520convex%2520functions%2520and%2520the%2520control%250Abased%2520analysis%2520of%2520a%2520streaming%2520regression%2520problem.%2520We%2520close%2520with%2520a%2520discussion%250Aabout%2520the%2520newly%2520defined%2520paradigm%2520of%2520online%2520adaptive%2520control%2520and%2520ask%2520the%250Afollowing%2520question%2520%2522Are%2520regret%2520optimal%2520control%2520strategies%2520deployable%253F%2522%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04572v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Regret%20Analysis%3A%20a%20control%20perspective&entry.906535625=Travis%20E.%20Gibson%20and%20Sawal%20Acharya&entry.1292438233=%20%20Online%20learning%20and%20model%20reference%20adaptive%20control%20have%20many%20interesting%0Aintersections.%20One%20area%20where%20they%20differ%20however%20is%20in%20how%20the%20algorithms%20are%0Aanalyzed%20and%20what%20objective%20or%20metric%20is%20used%20to%20discriminate%20%22good%22%20algorithms%0Afrom%20%22bad%22%20algorithms.%20In%20adaptive%20control%20there%20are%20usually%20two%20objectives%3A%201%29%0Aprove%20that%20all%20time%20varying%20parameters/states%20of%20the%20system%20are%20bounded%2C%20and%202%29%0Athat%20the%20instantaneous%20error%20between%20the%20adaptively%20controlled%20system%20and%20a%0Areference%20system%20converges%20to%20zero%20over%20time%20%28or%20at%20least%20a%20compact%20set%29.%20For%0Aonline%20learning%20the%20performance%20of%20algorithms%20is%20often%20characterized%20by%20the%0Aregret%20the%20algorithm%20incurs.%20Regret%20is%20defined%20as%20the%20cumulative%20loss%20%28cost%29%0Aover%20time%20from%20the%20online%20algorithm%20minus%20the%20cumulative%20loss%20%28cost%29%20of%20the%0Asingle%20optimal%20fixed%20parameter%20choice%20in%20hindsight.%20Another%20significant%0Adifference%20between%20the%20two%20areas%20of%20research%20is%20with%20regard%20to%20the%20assumptions%0Amade%20in%20order%20to%20obtain%20said%20results.%20Adaptive%20control%20makes%20assumptions%20about%0Athe%20input-output%20properties%20of%20the%20control%20problem%20and%20derives%20solutions%20for%20a%0Afixed%20error%20model%20or%20optimization%20task.%20In%20the%20online%20learning%20literature%0Aresults%20are%20derived%20for%20classes%20of%20loss%20functions%20%28i.e.%20convex%29%20while%20a%20priori%0Aassuming%20that%20all%20time%20varying%20parameters%20are%20bounded%2C%20which%20for%20many%0Aoptimization%20tasks%20is%20not%20unrealistic%2C%20but%20is%20a%20non%20starter%20in%20control%0Aapplications.%20In%20this%20work%20we%20discuss%20these%20differences%20in%20detail%20through%20the%0Aregret%20based%20analysis%20of%20gradient%20descent%20for%20convex%20functions%20and%20the%20control%0Abased%20analysis%20of%20a%20streaming%20regression%20problem.%20We%20close%20with%20a%20discussion%0Aabout%20the%20newly%20defined%20paradigm%20of%20online%20adaptive%20control%20and%20ask%20the%0Afollowing%20question%20%22Are%20regret%20optimal%20control%20strategies%20deployable%3F%22%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04572v2&entry.124074799=Read"},
{"title": "RadioTransformer: Accurate Radio Map Construction and Coverage\n  Prediction", "author": "Yuxuan Li and Cheng Zhang and Wen Wang and Yongming Huang", "abstract": "  Radio map, or pathloss map prediction, is a crucial method for wireless\nnetwork modeling and management. By leveraging deep learning to construct\npathloss patterns from geographical maps, an accurate digital replica of the\ntransmission environment could be established with less computational overhead\nand lower prediction error compared to traditional model-driven techniques.\nWhile existing state-of-the-art (SOTA) methods predominantly rely on\nconvolutional architectures, this paper introduces a hybrid\ntransformer-convolution model, termed RadioTransformer, to enhance the accuracy\nof radio map prediction. The proposed model features a multi-scale\ntransformer-based encoder for efficient feature extraction and a\nconvolution-based decoder for precise pixel-level image reconstruction.\nSimulation results demonstrate that the proposed scheme significantly improves\nprediction accuracy, and over a 30% reduction in root mean square error (RMSE)\nis achieved compared to typical SOTA approaches.\n", "link": "http://arxiv.org/abs/2501.05190v1", "date": "2025-01-09", "relevancy": 1.4688, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5252}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4828}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RadioTransformer%3A%20Accurate%20Radio%20Map%20Construction%20and%20Coverage%0A%20%20Prediction&body=Title%3A%20RadioTransformer%3A%20Accurate%20Radio%20Map%20Construction%20and%20Coverage%0A%20%20Prediction%0AAuthor%3A%20Yuxuan%20Li%20and%20Cheng%20Zhang%20and%20Wen%20Wang%20and%20Yongming%20Huang%0AAbstract%3A%20%20%20Radio%20map%2C%20or%20pathloss%20map%20prediction%2C%20is%20a%20crucial%20method%20for%20wireless%0Anetwork%20modeling%20and%20management.%20By%20leveraging%20deep%20learning%20to%20construct%0Apathloss%20patterns%20from%20geographical%20maps%2C%20an%20accurate%20digital%20replica%20of%20the%0Atransmission%20environment%20could%20be%20established%20with%20less%20computational%20overhead%0Aand%20lower%20prediction%20error%20compared%20to%20traditional%20model-driven%20techniques.%0AWhile%20existing%20state-of-the-art%20%28SOTA%29%20methods%20predominantly%20rely%20on%0Aconvolutional%20architectures%2C%20this%20paper%20introduces%20a%20hybrid%0Atransformer-convolution%20model%2C%20termed%20RadioTransformer%2C%20to%20enhance%20the%20accuracy%0Aof%20radio%20map%20prediction.%20The%20proposed%20model%20features%20a%20multi-scale%0Atransformer-based%20encoder%20for%20efficient%20feature%20extraction%20and%20a%0Aconvolution-based%20decoder%20for%20precise%20pixel-level%20image%20reconstruction.%0ASimulation%20results%20demonstrate%20that%20the%20proposed%20scheme%20significantly%20improves%0Aprediction%20accuracy%2C%20and%20over%20a%2030%25%20reduction%20in%20root%20mean%20square%20error%20%28RMSE%29%0Ais%20achieved%20compared%20to%20typical%20SOTA%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05190v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRadioTransformer%253A%2520Accurate%2520Radio%2520Map%2520Construction%2520and%2520Coverage%250A%2520%2520Prediction%26entry.906535625%3DYuxuan%2520Li%2520and%2520Cheng%2520Zhang%2520and%2520Wen%2520Wang%2520and%2520Yongming%2520Huang%26entry.1292438233%3D%2520%2520Radio%2520map%252C%2520or%2520pathloss%2520map%2520prediction%252C%2520is%2520a%2520crucial%2520method%2520for%2520wireless%250Anetwork%2520modeling%2520and%2520management.%2520By%2520leveraging%2520deep%2520learning%2520to%2520construct%250Apathloss%2520patterns%2520from%2520geographical%2520maps%252C%2520an%2520accurate%2520digital%2520replica%2520of%2520the%250Atransmission%2520environment%2520could%2520be%2520established%2520with%2520less%2520computational%2520overhead%250Aand%2520lower%2520prediction%2520error%2520compared%2520to%2520traditional%2520model-driven%2520techniques.%250AWhile%2520existing%2520state-of-the-art%2520%2528SOTA%2529%2520methods%2520predominantly%2520rely%2520on%250Aconvolutional%2520architectures%252C%2520this%2520paper%2520introduces%2520a%2520hybrid%250Atransformer-convolution%2520model%252C%2520termed%2520RadioTransformer%252C%2520to%2520enhance%2520the%2520accuracy%250Aof%2520radio%2520map%2520prediction.%2520The%2520proposed%2520model%2520features%2520a%2520multi-scale%250Atransformer-based%2520encoder%2520for%2520efficient%2520feature%2520extraction%2520and%2520a%250Aconvolution-based%2520decoder%2520for%2520precise%2520pixel-level%2520image%2520reconstruction.%250ASimulation%2520results%2520demonstrate%2520that%2520the%2520proposed%2520scheme%2520significantly%2520improves%250Aprediction%2520accuracy%252C%2520and%2520over%2520a%252030%2525%2520reduction%2520in%2520root%2520mean%2520square%2520error%2520%2528RMSE%2529%250Ais%2520achieved%2520compared%2520to%2520typical%2520SOTA%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05190v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RadioTransformer%3A%20Accurate%20Radio%20Map%20Construction%20and%20Coverage%0A%20%20Prediction&entry.906535625=Yuxuan%20Li%20and%20Cheng%20Zhang%20and%20Wen%20Wang%20and%20Yongming%20Huang&entry.1292438233=%20%20Radio%20map%2C%20or%20pathloss%20map%20prediction%2C%20is%20a%20crucial%20method%20for%20wireless%0Anetwork%20modeling%20and%20management.%20By%20leveraging%20deep%20learning%20to%20construct%0Apathloss%20patterns%20from%20geographical%20maps%2C%20an%20accurate%20digital%20replica%20of%20the%0Atransmission%20environment%20could%20be%20established%20with%20less%20computational%20overhead%0Aand%20lower%20prediction%20error%20compared%20to%20traditional%20model-driven%20techniques.%0AWhile%20existing%20state-of-the-art%20%28SOTA%29%20methods%20predominantly%20rely%20on%0Aconvolutional%20architectures%2C%20this%20paper%20introduces%20a%20hybrid%0Atransformer-convolution%20model%2C%20termed%20RadioTransformer%2C%20to%20enhance%20the%20accuracy%0Aof%20radio%20map%20prediction.%20The%20proposed%20model%20features%20a%20multi-scale%0Atransformer-based%20encoder%20for%20efficient%20feature%20extraction%20and%20a%0Aconvolution-based%20decoder%20for%20precise%20pixel-level%20image%20reconstruction.%0ASimulation%20results%20demonstrate%20that%20the%20proposed%20scheme%20significantly%20improves%0Aprediction%20accuracy%2C%20and%20over%20a%2030%25%20reduction%20in%20root%20mean%20square%20error%20%28RMSE%29%0Ais%20achieved%20compared%20to%20typical%20SOTA%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05190v1&entry.124074799=Read"},
{"title": "Design and Control of a Bipedal Robotic Character", "author": "Ruben Grandia and Espen Knoop and Michael A. Hopkins and Georg Wiedebach and Jared Bishop and Steven Pickles and David M\u00fcller and Moritz B\u00e4cher", "abstract": "  Legged robots have achieved impressive feats in dynamic locomotion in\nchallenging unstructured terrain. However, in entertainment applications, the\ndesign and control of these robots face additional challenges in appealing to\nhuman audiences. This work aims to unify expressive, artist-directed motions\nand robust dynamic mobility for legged robots. To this end, we introduce a new\nbipedal robot, designed with a focus on character-driven mechanical features.\nWe present a reinforcement learning-based control architecture to robustly\nexecute artistic motions conditioned on command signals. During runtime, these\ncommand signals are generated by an animation engine which composes and blends\nbetween multiple animation sources. Finally, an intuitive operator interface\nenables real-time show performances with the robot. The complete system results\nin a believable robotic character, and paves the way for enhanced human-robot\nengagement in various contexts, in entertainment robotics and beyond.\n", "link": "http://arxiv.org/abs/2501.05204v1", "date": "2025-01-09", "relevancy": 1.759, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5895}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5871}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Design%20and%20Control%20of%20a%20Bipedal%20Robotic%20Character&body=Title%3A%20Design%20and%20Control%20of%20a%20Bipedal%20Robotic%20Character%0AAuthor%3A%20Ruben%20Grandia%20and%20Espen%20Knoop%20and%20Michael%20A.%20Hopkins%20and%20Georg%20Wiedebach%20and%20Jared%20Bishop%20and%20Steven%20Pickles%20and%20David%20M%C3%BCller%20and%20Moritz%20B%C3%A4cher%0AAbstract%3A%20%20%20Legged%20robots%20have%20achieved%20impressive%20feats%20in%20dynamic%20locomotion%20in%0Achallenging%20unstructured%20terrain.%20However%2C%20in%20entertainment%20applications%2C%20the%0Adesign%20and%20control%20of%20these%20robots%20face%20additional%20challenges%20in%20appealing%20to%0Ahuman%20audiences.%20This%20work%20aims%20to%20unify%20expressive%2C%20artist-directed%20motions%0Aand%20robust%20dynamic%20mobility%20for%20legged%20robots.%20To%20this%20end%2C%20we%20introduce%20a%20new%0Abipedal%20robot%2C%20designed%20with%20a%20focus%20on%20character-driven%20mechanical%20features.%0AWe%20present%20a%20reinforcement%20learning-based%20control%20architecture%20to%20robustly%0Aexecute%20artistic%20motions%20conditioned%20on%20command%20signals.%20During%20runtime%2C%20these%0Acommand%20signals%20are%20generated%20by%20an%20animation%20engine%20which%20composes%20and%20blends%0Abetween%20multiple%20animation%20sources.%20Finally%2C%20an%20intuitive%20operator%20interface%0Aenables%20real-time%20show%20performances%20with%20the%20robot.%20The%20complete%20system%20results%0Ain%20a%20believable%20robotic%20character%2C%20and%20paves%20the%20way%20for%20enhanced%20human-robot%0Aengagement%20in%20various%20contexts%2C%20in%20entertainment%20robotics%20and%20beyond.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05204v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesign%2520and%2520Control%2520of%2520a%2520Bipedal%2520Robotic%2520Character%26entry.906535625%3DRuben%2520Grandia%2520and%2520Espen%2520Knoop%2520and%2520Michael%2520A.%2520Hopkins%2520and%2520Georg%2520Wiedebach%2520and%2520Jared%2520Bishop%2520and%2520Steven%2520Pickles%2520and%2520David%2520M%25C3%25BCller%2520and%2520Moritz%2520B%25C3%25A4cher%26entry.1292438233%3D%2520%2520Legged%2520robots%2520have%2520achieved%2520impressive%2520feats%2520in%2520dynamic%2520locomotion%2520in%250Achallenging%2520unstructured%2520terrain.%2520However%252C%2520in%2520entertainment%2520applications%252C%2520the%250Adesign%2520and%2520control%2520of%2520these%2520robots%2520face%2520additional%2520challenges%2520in%2520appealing%2520to%250Ahuman%2520audiences.%2520This%2520work%2520aims%2520to%2520unify%2520expressive%252C%2520artist-directed%2520motions%250Aand%2520robust%2520dynamic%2520mobility%2520for%2520legged%2520robots.%2520To%2520this%2520end%252C%2520we%2520introduce%2520a%2520new%250Abipedal%2520robot%252C%2520designed%2520with%2520a%2520focus%2520on%2520character-driven%2520mechanical%2520features.%250AWe%2520present%2520a%2520reinforcement%2520learning-based%2520control%2520architecture%2520to%2520robustly%250Aexecute%2520artistic%2520motions%2520conditioned%2520on%2520command%2520signals.%2520During%2520runtime%252C%2520these%250Acommand%2520signals%2520are%2520generated%2520by%2520an%2520animation%2520engine%2520which%2520composes%2520and%2520blends%250Abetween%2520multiple%2520animation%2520sources.%2520Finally%252C%2520an%2520intuitive%2520operator%2520interface%250Aenables%2520real-time%2520show%2520performances%2520with%2520the%2520robot.%2520The%2520complete%2520system%2520results%250Ain%2520a%2520believable%2520robotic%2520character%252C%2520and%2520paves%2520the%2520way%2520for%2520enhanced%2520human-robot%250Aengagement%2520in%2520various%2520contexts%252C%2520in%2520entertainment%2520robotics%2520and%2520beyond.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05204v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Design%20and%20Control%20of%20a%20Bipedal%20Robotic%20Character&entry.906535625=Ruben%20Grandia%20and%20Espen%20Knoop%20and%20Michael%20A.%20Hopkins%20and%20Georg%20Wiedebach%20and%20Jared%20Bishop%20and%20Steven%20Pickles%20and%20David%20M%C3%BCller%20and%20Moritz%20B%C3%A4cher&entry.1292438233=%20%20Legged%20robots%20have%20achieved%20impressive%20feats%20in%20dynamic%20locomotion%20in%0Achallenging%20unstructured%20terrain.%20However%2C%20in%20entertainment%20applications%2C%20the%0Adesign%20and%20control%20of%20these%20robots%20face%20additional%20challenges%20in%20appealing%20to%0Ahuman%20audiences.%20This%20work%20aims%20to%20unify%20expressive%2C%20artist-directed%20motions%0Aand%20robust%20dynamic%20mobility%20for%20legged%20robots.%20To%20this%20end%2C%20we%20introduce%20a%20new%0Abipedal%20robot%2C%20designed%20with%20a%20focus%20on%20character-driven%20mechanical%20features.%0AWe%20present%20a%20reinforcement%20learning-based%20control%20architecture%20to%20robustly%0Aexecute%20artistic%20motions%20conditioned%20on%20command%20signals.%20During%20runtime%2C%20these%0Acommand%20signals%20are%20generated%20by%20an%20animation%20engine%20which%20composes%20and%20blends%0Abetween%20multiple%20animation%20sources.%20Finally%2C%20an%20intuitive%20operator%20interface%0Aenables%20real-time%20show%20performances%20with%20the%20robot.%20The%20complete%20system%20results%0Ain%20a%20believable%20robotic%20character%2C%20and%20paves%20the%20way%20for%20enhanced%20human-robot%0Aengagement%20in%20various%20contexts%2C%20in%20entertainment%20robotics%20and%20beyond.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05204v1&entry.124074799=Read"},
{"title": "Seeing Sound: Assembling Sounds from Visuals for Audio-to-Image\n  Generation", "author": "Darius Petermann and Mahdi M. Kalayeh", "abstract": "  Training audio-to-image generative models requires an abundance of diverse\naudio-visual pairs that are semantically aligned. Such data is almost always\ncurated from in-the-wild videos, given the cross-modal semantic correspondence\nthat is inherent to them. In this work, we hypothesize that insisting on the\nabsolute need for ground truth audio-visual correspondence, is not only\nunnecessary, but also leads to severe restrictions in scale, quality, and\ndiversity of the data, ultimately impairing its use in the modern generative\nmodels. That is, we propose a scalable image sonification framework where\ninstances from a variety of high-quality yet disjoint uni-modal origins can be\nartificially paired through a retrieval process that is empowered by reasoning\ncapabilities of modern vision-language models. To demonstrate the efficacy of\nthis approach, we use our sonified images to train an audio-to-image generative\nmodel that performs competitively against state-of-the-art. Finally, through a\nseries of ablation studies, we exhibit several intriguing auditory capabilities\nlike semantic mixing and interpolation, loudness calibration and acoustic space\nmodeling through reverberation that our model has implicitly developed to guide\nthe image generation process.\n", "link": "http://arxiv.org/abs/2501.05413v1", "date": "2025-01-09", "relevancy": 1.8031, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6055}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6024}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20Sound%3A%20Assembling%20Sounds%20from%20Visuals%20for%20Audio-to-Image%0A%20%20Generation&body=Title%3A%20Seeing%20Sound%3A%20Assembling%20Sounds%20from%20Visuals%20for%20Audio-to-Image%0A%20%20Generation%0AAuthor%3A%20Darius%20Petermann%20and%20Mahdi%20M.%20Kalayeh%0AAbstract%3A%20%20%20Training%20audio-to-image%20generative%20models%20requires%20an%20abundance%20of%20diverse%0Aaudio-visual%20pairs%20that%20are%20semantically%20aligned.%20Such%20data%20is%20almost%20always%0Acurated%20from%20in-the-wild%20videos%2C%20given%20the%20cross-modal%20semantic%20correspondence%0Athat%20is%20inherent%20to%20them.%20In%20this%20work%2C%20we%20hypothesize%20that%20insisting%20on%20the%0Aabsolute%20need%20for%20ground%20truth%20audio-visual%20correspondence%2C%20is%20not%20only%0Aunnecessary%2C%20but%20also%20leads%20to%20severe%20restrictions%20in%20scale%2C%20quality%2C%20and%0Adiversity%20of%20the%20data%2C%20ultimately%20impairing%20its%20use%20in%20the%20modern%20generative%0Amodels.%20That%20is%2C%20we%20propose%20a%20scalable%20image%20sonification%20framework%20where%0Ainstances%20from%20a%20variety%20of%20high-quality%20yet%20disjoint%20uni-modal%20origins%20can%20be%0Aartificially%20paired%20through%20a%20retrieval%20process%20that%20is%20empowered%20by%20reasoning%0Acapabilities%20of%20modern%20vision-language%20models.%20To%20demonstrate%20the%20efficacy%20of%0Athis%20approach%2C%20we%20use%20our%20sonified%20images%20to%20train%20an%20audio-to-image%20generative%0Amodel%20that%20performs%20competitively%20against%20state-of-the-art.%20Finally%2C%20through%20a%0Aseries%20of%20ablation%20studies%2C%20we%20exhibit%20several%20intriguing%20auditory%20capabilities%0Alike%20semantic%20mixing%20and%20interpolation%2C%20loudness%20calibration%20and%20acoustic%20space%0Amodeling%20through%20reverberation%20that%20our%20model%20has%20implicitly%20developed%20to%20guide%0Athe%20image%20generation%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05413v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520Sound%253A%2520Assembling%2520Sounds%2520from%2520Visuals%2520for%2520Audio-to-Image%250A%2520%2520Generation%26entry.906535625%3DDarius%2520Petermann%2520and%2520Mahdi%2520M.%2520Kalayeh%26entry.1292438233%3D%2520%2520Training%2520audio-to-image%2520generative%2520models%2520requires%2520an%2520abundance%2520of%2520diverse%250Aaudio-visual%2520pairs%2520that%2520are%2520semantically%2520aligned.%2520Such%2520data%2520is%2520almost%2520always%250Acurated%2520from%2520in-the-wild%2520videos%252C%2520given%2520the%2520cross-modal%2520semantic%2520correspondence%250Athat%2520is%2520inherent%2520to%2520them.%2520In%2520this%2520work%252C%2520we%2520hypothesize%2520that%2520insisting%2520on%2520the%250Aabsolute%2520need%2520for%2520ground%2520truth%2520audio-visual%2520correspondence%252C%2520is%2520not%2520only%250Aunnecessary%252C%2520but%2520also%2520leads%2520to%2520severe%2520restrictions%2520in%2520scale%252C%2520quality%252C%2520and%250Adiversity%2520of%2520the%2520data%252C%2520ultimately%2520impairing%2520its%2520use%2520in%2520the%2520modern%2520generative%250Amodels.%2520That%2520is%252C%2520we%2520propose%2520a%2520scalable%2520image%2520sonification%2520framework%2520where%250Ainstances%2520from%2520a%2520variety%2520of%2520high-quality%2520yet%2520disjoint%2520uni-modal%2520origins%2520can%2520be%250Aartificially%2520paired%2520through%2520a%2520retrieval%2520process%2520that%2520is%2520empowered%2520by%2520reasoning%250Acapabilities%2520of%2520modern%2520vision-language%2520models.%2520To%2520demonstrate%2520the%2520efficacy%2520of%250Athis%2520approach%252C%2520we%2520use%2520our%2520sonified%2520images%2520to%2520train%2520an%2520audio-to-image%2520generative%250Amodel%2520that%2520performs%2520competitively%2520against%2520state-of-the-art.%2520Finally%252C%2520through%2520a%250Aseries%2520of%2520ablation%2520studies%252C%2520we%2520exhibit%2520several%2520intriguing%2520auditory%2520capabilities%250Alike%2520semantic%2520mixing%2520and%2520interpolation%252C%2520loudness%2520calibration%2520and%2520acoustic%2520space%250Amodeling%2520through%2520reverberation%2520that%2520our%2520model%2520has%2520implicitly%2520developed%2520to%2520guide%250Athe%2520image%2520generation%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05413v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20Sound%3A%20Assembling%20Sounds%20from%20Visuals%20for%20Audio-to-Image%0A%20%20Generation&entry.906535625=Darius%20Petermann%20and%20Mahdi%20M.%20Kalayeh&entry.1292438233=%20%20Training%20audio-to-image%20generative%20models%20requires%20an%20abundance%20of%20diverse%0Aaudio-visual%20pairs%20that%20are%20semantically%20aligned.%20Such%20data%20is%20almost%20always%0Acurated%20from%20in-the-wild%20videos%2C%20given%20the%20cross-modal%20semantic%20correspondence%0Athat%20is%20inherent%20to%20them.%20In%20this%20work%2C%20we%20hypothesize%20that%20insisting%20on%20the%0Aabsolute%20need%20for%20ground%20truth%20audio-visual%20correspondence%2C%20is%20not%20only%0Aunnecessary%2C%20but%20also%20leads%20to%20severe%20restrictions%20in%20scale%2C%20quality%2C%20and%0Adiversity%20of%20the%20data%2C%20ultimately%20impairing%20its%20use%20in%20the%20modern%20generative%0Amodels.%20That%20is%2C%20we%20propose%20a%20scalable%20image%20sonification%20framework%20where%0Ainstances%20from%20a%20variety%20of%20high-quality%20yet%20disjoint%20uni-modal%20origins%20can%20be%0Aartificially%20paired%20through%20a%20retrieval%20process%20that%20is%20empowered%20by%20reasoning%0Acapabilities%20of%20modern%20vision-language%20models.%20To%20demonstrate%20the%20efficacy%20of%0Athis%20approach%2C%20we%20use%20our%20sonified%20images%20to%20train%20an%20audio-to-image%20generative%0Amodel%20that%20performs%20competitively%20against%20state-of-the-art.%20Finally%2C%20through%20a%0Aseries%20of%20ablation%20studies%2C%20we%20exhibit%20several%20intriguing%20auditory%20capabilities%0Alike%20semantic%20mixing%20and%20interpolation%2C%20loudness%20calibration%20and%20acoustic%20space%0Amodeling%20through%20reverberation%20that%20our%20model%20has%20implicitly%20developed%20to%20guide%0Athe%20image%20generation%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05413v1&entry.124074799=Read"},
{"title": "Adaptive Probabilistic Planning for the Uncertain and Dynamic\n  Orienteering Problem", "author": "Qiuchen Qian and Yanran Wang and David Boyle", "abstract": "  The Orienteering Problem (OP) is a well-studied routing problem that has been\nextended to incorporate uncertainties, reflecting stochastic or dynamic travel\ncosts, prize-collection costs, and prizes. Existing approaches may, however, be\ninefficient in real-world applications due to insufficient modeling knowledge\nand initially unknowable parameters in online scenarios. Thus, we propose the\nUncertain and Dynamic Orienteering Problem (UDOP), modeling travel costs as\ndistributions with unknown and time-variant parameters. UDOP also associates\nuncertain travel costs with dynamic prizes and prize-collection costs for its\nobjective and budget constraints. To address UDOP, we develop an ADaptive\nApproach for Probabilistic paThs - ADAPT, that iteratively performs 'execution'\nand 'online planning' based on an initial 'offline' solution. The execution\nphase updates system status and records online cost observations. The online\nplanner employs a Bayesian approach to adaptively estimate power consumption\nand optimize path sequence based on safety beliefs. We evaluate ADAPT in a\npractical Unmanned Aerial Vehicle (UAV) charging scheduling problem for\nWireless Rechargeable Sensor Networks. The UAV must optimize its path to\nrecharge sensor nodes efficiently while managing its energy under uncertain\nconditions. ADAPT maintains comparable solution quality and computation time\nwhile offering superior robustness. Extensive simulations show that ADAPT\nachieves a 100% Mission Success Rate (MSR) across all tested scenarios,\noutperforming comparable heuristic-based and frequentist approaches that fail\nup to 70% (under challenging conditions) and averaging 67% MSR, respectively.\nThis work advances the field of OP with uncertainties, offering a reliable and\nefficient approach for real-world applications in uncertain and dynamic\nenvironments.\n", "link": "http://arxiv.org/abs/2409.05545v2", "date": "2025-01-09", "relevancy": 1.6307, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5602}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5459}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Probabilistic%20Planning%20for%20the%20Uncertain%20and%20Dynamic%0A%20%20Orienteering%20Problem&body=Title%3A%20Adaptive%20Probabilistic%20Planning%20for%20the%20Uncertain%20and%20Dynamic%0A%20%20Orienteering%20Problem%0AAuthor%3A%20Qiuchen%20Qian%20and%20Yanran%20Wang%20and%20David%20Boyle%0AAbstract%3A%20%20%20The%20Orienteering%20Problem%20%28OP%29%20is%20a%20well-studied%20routing%20problem%20that%20has%20been%0Aextended%20to%20incorporate%20uncertainties%2C%20reflecting%20stochastic%20or%20dynamic%20travel%0Acosts%2C%20prize-collection%20costs%2C%20and%20prizes.%20Existing%20approaches%20may%2C%20however%2C%20be%0Ainefficient%20in%20real-world%20applications%20due%20to%20insufficient%20modeling%20knowledge%0Aand%20initially%20unknowable%20parameters%20in%20online%20scenarios.%20Thus%2C%20we%20propose%20the%0AUncertain%20and%20Dynamic%20Orienteering%20Problem%20%28UDOP%29%2C%20modeling%20travel%20costs%20as%0Adistributions%20with%20unknown%20and%20time-variant%20parameters.%20UDOP%20also%20associates%0Auncertain%20travel%20costs%20with%20dynamic%20prizes%20and%20prize-collection%20costs%20for%20its%0Aobjective%20and%20budget%20constraints.%20To%20address%20UDOP%2C%20we%20develop%20an%20ADaptive%0AApproach%20for%20Probabilistic%20paThs%20-%20ADAPT%2C%20that%20iteratively%20performs%20%27execution%27%0Aand%20%27online%20planning%27%20based%20on%20an%20initial%20%27offline%27%20solution.%20The%20execution%0Aphase%20updates%20system%20status%20and%20records%20online%20cost%20observations.%20The%20online%0Aplanner%20employs%20a%20Bayesian%20approach%20to%20adaptively%20estimate%20power%20consumption%0Aand%20optimize%20path%20sequence%20based%20on%20safety%20beliefs.%20We%20evaluate%20ADAPT%20in%20a%0Apractical%20Unmanned%20Aerial%20Vehicle%20%28UAV%29%20charging%20scheduling%20problem%20for%0AWireless%20Rechargeable%20Sensor%20Networks.%20The%20UAV%20must%20optimize%20its%20path%20to%0Arecharge%20sensor%20nodes%20efficiently%20while%20managing%20its%20energy%20under%20uncertain%0Aconditions.%20ADAPT%20maintains%20comparable%20solution%20quality%20and%20computation%20time%0Awhile%20offering%20superior%20robustness.%20Extensive%20simulations%20show%20that%20ADAPT%0Aachieves%20a%20100%25%20Mission%20Success%20Rate%20%28MSR%29%20across%20all%20tested%20scenarios%2C%0Aoutperforming%20comparable%20heuristic-based%20and%20frequentist%20approaches%20that%20fail%0Aup%20to%2070%25%20%28under%20challenging%20conditions%29%20and%20averaging%2067%25%20MSR%2C%20respectively.%0AThis%20work%20advances%20the%20field%20of%20OP%20with%20uncertainties%2C%20offering%20a%20reliable%20and%0Aefficient%20approach%20for%20real-world%20applications%20in%20uncertain%20and%20dynamic%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05545v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Probabilistic%2520Planning%2520for%2520the%2520Uncertain%2520and%2520Dynamic%250A%2520%2520Orienteering%2520Problem%26entry.906535625%3DQiuchen%2520Qian%2520and%2520Yanran%2520Wang%2520and%2520David%2520Boyle%26entry.1292438233%3D%2520%2520The%2520Orienteering%2520Problem%2520%2528OP%2529%2520is%2520a%2520well-studied%2520routing%2520problem%2520that%2520has%2520been%250Aextended%2520to%2520incorporate%2520uncertainties%252C%2520reflecting%2520stochastic%2520or%2520dynamic%2520travel%250Acosts%252C%2520prize-collection%2520costs%252C%2520and%2520prizes.%2520Existing%2520approaches%2520may%252C%2520however%252C%2520be%250Ainefficient%2520in%2520real-world%2520applications%2520due%2520to%2520insufficient%2520modeling%2520knowledge%250Aand%2520initially%2520unknowable%2520parameters%2520in%2520online%2520scenarios.%2520Thus%252C%2520we%2520propose%2520the%250AUncertain%2520and%2520Dynamic%2520Orienteering%2520Problem%2520%2528UDOP%2529%252C%2520modeling%2520travel%2520costs%2520as%250Adistributions%2520with%2520unknown%2520and%2520time-variant%2520parameters.%2520UDOP%2520also%2520associates%250Auncertain%2520travel%2520costs%2520with%2520dynamic%2520prizes%2520and%2520prize-collection%2520costs%2520for%2520its%250Aobjective%2520and%2520budget%2520constraints.%2520To%2520address%2520UDOP%252C%2520we%2520develop%2520an%2520ADaptive%250AApproach%2520for%2520Probabilistic%2520paThs%2520-%2520ADAPT%252C%2520that%2520iteratively%2520performs%2520%2527execution%2527%250Aand%2520%2527online%2520planning%2527%2520based%2520on%2520an%2520initial%2520%2527offline%2527%2520solution.%2520The%2520execution%250Aphase%2520updates%2520system%2520status%2520and%2520records%2520online%2520cost%2520observations.%2520The%2520online%250Aplanner%2520employs%2520a%2520Bayesian%2520approach%2520to%2520adaptively%2520estimate%2520power%2520consumption%250Aand%2520optimize%2520path%2520sequence%2520based%2520on%2520safety%2520beliefs.%2520We%2520evaluate%2520ADAPT%2520in%2520a%250Apractical%2520Unmanned%2520Aerial%2520Vehicle%2520%2528UAV%2529%2520charging%2520scheduling%2520problem%2520for%250AWireless%2520Rechargeable%2520Sensor%2520Networks.%2520The%2520UAV%2520must%2520optimize%2520its%2520path%2520to%250Arecharge%2520sensor%2520nodes%2520efficiently%2520while%2520managing%2520its%2520energy%2520under%2520uncertain%250Aconditions.%2520ADAPT%2520maintains%2520comparable%2520solution%2520quality%2520and%2520computation%2520time%250Awhile%2520offering%2520superior%2520robustness.%2520Extensive%2520simulations%2520show%2520that%2520ADAPT%250Aachieves%2520a%2520100%2525%2520Mission%2520Success%2520Rate%2520%2528MSR%2529%2520across%2520all%2520tested%2520scenarios%252C%250Aoutperforming%2520comparable%2520heuristic-based%2520and%2520frequentist%2520approaches%2520that%2520fail%250Aup%2520to%252070%2525%2520%2528under%2520challenging%2520conditions%2529%2520and%2520averaging%252067%2525%2520MSR%252C%2520respectively.%250AThis%2520work%2520advances%2520the%2520field%2520of%2520OP%2520with%2520uncertainties%252C%2520offering%2520a%2520reliable%2520and%250Aefficient%2520approach%2520for%2520real-world%2520applications%2520in%2520uncertain%2520and%2520dynamic%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05545v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Probabilistic%20Planning%20for%20the%20Uncertain%20and%20Dynamic%0A%20%20Orienteering%20Problem&entry.906535625=Qiuchen%20Qian%20and%20Yanran%20Wang%20and%20David%20Boyle&entry.1292438233=%20%20The%20Orienteering%20Problem%20%28OP%29%20is%20a%20well-studied%20routing%20problem%20that%20has%20been%0Aextended%20to%20incorporate%20uncertainties%2C%20reflecting%20stochastic%20or%20dynamic%20travel%0Acosts%2C%20prize-collection%20costs%2C%20and%20prizes.%20Existing%20approaches%20may%2C%20however%2C%20be%0Ainefficient%20in%20real-world%20applications%20due%20to%20insufficient%20modeling%20knowledge%0Aand%20initially%20unknowable%20parameters%20in%20online%20scenarios.%20Thus%2C%20we%20propose%20the%0AUncertain%20and%20Dynamic%20Orienteering%20Problem%20%28UDOP%29%2C%20modeling%20travel%20costs%20as%0Adistributions%20with%20unknown%20and%20time-variant%20parameters.%20UDOP%20also%20associates%0Auncertain%20travel%20costs%20with%20dynamic%20prizes%20and%20prize-collection%20costs%20for%20its%0Aobjective%20and%20budget%20constraints.%20To%20address%20UDOP%2C%20we%20develop%20an%20ADaptive%0AApproach%20for%20Probabilistic%20paThs%20-%20ADAPT%2C%20that%20iteratively%20performs%20%27execution%27%0Aand%20%27online%20planning%27%20based%20on%20an%20initial%20%27offline%27%20solution.%20The%20execution%0Aphase%20updates%20system%20status%20and%20records%20online%20cost%20observations.%20The%20online%0Aplanner%20employs%20a%20Bayesian%20approach%20to%20adaptively%20estimate%20power%20consumption%0Aand%20optimize%20path%20sequence%20based%20on%20safety%20beliefs.%20We%20evaluate%20ADAPT%20in%20a%0Apractical%20Unmanned%20Aerial%20Vehicle%20%28UAV%29%20charging%20scheduling%20problem%20for%0AWireless%20Rechargeable%20Sensor%20Networks.%20The%20UAV%20must%20optimize%20its%20path%20to%0Arecharge%20sensor%20nodes%20efficiently%20while%20managing%20its%20energy%20under%20uncertain%0Aconditions.%20ADAPT%20maintains%20comparable%20solution%20quality%20and%20computation%20time%0Awhile%20offering%20superior%20robustness.%20Extensive%20simulations%20show%20that%20ADAPT%0Aachieves%20a%20100%25%20Mission%20Success%20Rate%20%28MSR%29%20across%20all%20tested%20scenarios%2C%0Aoutperforming%20comparable%20heuristic-based%20and%20frequentist%20approaches%20that%20fail%0Aup%20to%2070%25%20%28under%20challenging%20conditions%29%20and%20averaging%2067%25%20MSR%2C%20respectively.%0AThis%20work%20advances%20the%20field%20of%20OP%20with%20uncertainties%2C%20offering%20a%20reliable%20and%0Aefficient%20approach%20for%20real-world%20applications%20in%20uncertain%20and%20dynamic%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05545v2&entry.124074799=Read"},
{"title": "A General Framework for Clustering and Distribution Matching with Bandit\n  Feedback", "author": "Recep Can Yavas and Yuqi Huang and Vincent Y. F. Tan and Jonathan Scarlett", "abstract": "  We develop a general framework for clustering and distribution matching\nproblems with bandit feedback. We consider a $K$-armed bandit model where some\nsubset of $K$ arms is partitioned into $M$ groups. Within each group, the\nrandom variable associated to each arm follows the same distribution on a\nfinite alphabet. At each time step, the decision maker pulls an arm and\nobserves its outcome from the random variable associated to that arm.\nSubsequent arm pulls depend on the history of arm pulls and their outcomes. The\ndecision maker has no knowledge of the distributions of the arms or the\nunderlying partitions. The task is to devise an online algorithm to learn the\nunderlying partition of arms with the least number of arm pulls on average and\nwith an error probability not exceeding a pre-determined value~$\\delta$.\nSeveral existing problems fall under our general framework, including finding\n$M$ pairs of arms, odd arm identification, and $N$-ary clustering of $K$ arms\nbelong to our general framework. We derive a non-asymptotic lower bound on the\naverage number of arm pulls for any online algorithm with an error probability\nnot exceeding $\\delta$. Furthermore, we develop a computationally-efficient\nonline algorithm based on the Track-and-Stop method and Frank--Wolfe algorithm,\nand show that the average number of arm pulls of our algorithm asymptotically\nmatches that of the lower bound. Our refined analysis also uncovers a novel\nbound on the speed at which the average number of arm pulls of our algorithm\nconverges to the fundamental limit as $\\delta$ vanishes.\n", "link": "http://arxiv.org/abs/2409.05072v2", "date": "2025-01-09", "relevancy": 1.7227, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.448}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4445}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.41}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20General%20Framework%20for%20Clustering%20and%20Distribution%20Matching%20with%20Bandit%0A%20%20Feedback&body=Title%3A%20A%20General%20Framework%20for%20Clustering%20and%20Distribution%20Matching%20with%20Bandit%0A%20%20Feedback%0AAuthor%3A%20Recep%20Can%20Yavas%20and%20Yuqi%20Huang%20and%20Vincent%20Y.%20F.%20Tan%20and%20Jonathan%20Scarlett%0AAbstract%3A%20%20%20We%20develop%20a%20general%20framework%20for%20clustering%20and%20distribution%20matching%0Aproblems%20with%20bandit%20feedback.%20We%20consider%20a%20%24K%24-armed%20bandit%20model%20where%20some%0Asubset%20of%20%24K%24%20arms%20is%20partitioned%20into%20%24M%24%20groups.%20Within%20each%20group%2C%20the%0Arandom%20variable%20associated%20to%20each%20arm%20follows%20the%20same%20distribution%20on%20a%0Afinite%20alphabet.%20At%20each%20time%20step%2C%20the%20decision%20maker%20pulls%20an%20arm%20and%0Aobserves%20its%20outcome%20from%20the%20random%20variable%20associated%20to%20that%20arm.%0ASubsequent%20arm%20pulls%20depend%20on%20the%20history%20of%20arm%20pulls%20and%20their%20outcomes.%20The%0Adecision%20maker%20has%20no%20knowledge%20of%20the%20distributions%20of%20the%20arms%20or%20the%0Aunderlying%20partitions.%20The%20task%20is%20to%20devise%20an%20online%20algorithm%20to%20learn%20the%0Aunderlying%20partition%20of%20arms%20with%20the%20least%20number%20of%20arm%20pulls%20on%20average%20and%0Awith%20an%20error%20probability%20not%20exceeding%20a%20pre-determined%20value~%24%5Cdelta%24.%0ASeveral%20existing%20problems%20fall%20under%20our%20general%20framework%2C%20including%20finding%0A%24M%24%20pairs%20of%20arms%2C%20odd%20arm%20identification%2C%20and%20%24N%24-ary%20clustering%20of%20%24K%24%20arms%0Abelong%20to%20our%20general%20framework.%20We%20derive%20a%20non-asymptotic%20lower%20bound%20on%20the%0Aaverage%20number%20of%20arm%20pulls%20for%20any%20online%20algorithm%20with%20an%20error%20probability%0Anot%20exceeding%20%24%5Cdelta%24.%20Furthermore%2C%20we%20develop%20a%20computationally-efficient%0Aonline%20algorithm%20based%20on%20the%20Track-and-Stop%20method%20and%20Frank--Wolfe%20algorithm%2C%0Aand%20show%20that%20the%20average%20number%20of%20arm%20pulls%20of%20our%20algorithm%20asymptotically%0Amatches%20that%20of%20the%20lower%20bound.%20Our%20refined%20analysis%20also%20uncovers%20a%20novel%0Abound%20on%20the%20speed%20at%20which%20the%20average%20number%20of%20arm%20pulls%20of%20our%20algorithm%0Aconverges%20to%20the%20fundamental%20limit%20as%20%24%5Cdelta%24%20vanishes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05072v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520General%2520Framework%2520for%2520Clustering%2520and%2520Distribution%2520Matching%2520with%2520Bandit%250A%2520%2520Feedback%26entry.906535625%3DRecep%2520Can%2520Yavas%2520and%2520Yuqi%2520Huang%2520and%2520Vincent%2520Y.%2520F.%2520Tan%2520and%2520Jonathan%2520Scarlett%26entry.1292438233%3D%2520%2520We%2520develop%2520a%2520general%2520framework%2520for%2520clustering%2520and%2520distribution%2520matching%250Aproblems%2520with%2520bandit%2520feedback.%2520We%2520consider%2520a%2520%2524K%2524-armed%2520bandit%2520model%2520where%2520some%250Asubset%2520of%2520%2524K%2524%2520arms%2520is%2520partitioned%2520into%2520%2524M%2524%2520groups.%2520Within%2520each%2520group%252C%2520the%250Arandom%2520variable%2520associated%2520to%2520each%2520arm%2520follows%2520the%2520same%2520distribution%2520on%2520a%250Afinite%2520alphabet.%2520At%2520each%2520time%2520step%252C%2520the%2520decision%2520maker%2520pulls%2520an%2520arm%2520and%250Aobserves%2520its%2520outcome%2520from%2520the%2520random%2520variable%2520associated%2520to%2520that%2520arm.%250ASubsequent%2520arm%2520pulls%2520depend%2520on%2520the%2520history%2520of%2520arm%2520pulls%2520and%2520their%2520outcomes.%2520The%250Adecision%2520maker%2520has%2520no%2520knowledge%2520of%2520the%2520distributions%2520of%2520the%2520arms%2520or%2520the%250Aunderlying%2520partitions.%2520The%2520task%2520is%2520to%2520devise%2520an%2520online%2520algorithm%2520to%2520learn%2520the%250Aunderlying%2520partition%2520of%2520arms%2520with%2520the%2520least%2520number%2520of%2520arm%2520pulls%2520on%2520average%2520and%250Awith%2520an%2520error%2520probability%2520not%2520exceeding%2520a%2520pre-determined%2520value~%2524%255Cdelta%2524.%250ASeveral%2520existing%2520problems%2520fall%2520under%2520our%2520general%2520framework%252C%2520including%2520finding%250A%2524M%2524%2520pairs%2520of%2520arms%252C%2520odd%2520arm%2520identification%252C%2520and%2520%2524N%2524-ary%2520clustering%2520of%2520%2524K%2524%2520arms%250Abelong%2520to%2520our%2520general%2520framework.%2520We%2520derive%2520a%2520non-asymptotic%2520lower%2520bound%2520on%2520the%250Aaverage%2520number%2520of%2520arm%2520pulls%2520for%2520any%2520online%2520algorithm%2520with%2520an%2520error%2520probability%250Anot%2520exceeding%2520%2524%255Cdelta%2524.%2520Furthermore%252C%2520we%2520develop%2520a%2520computationally-efficient%250Aonline%2520algorithm%2520based%2520on%2520the%2520Track-and-Stop%2520method%2520and%2520Frank--Wolfe%2520algorithm%252C%250Aand%2520show%2520that%2520the%2520average%2520number%2520of%2520arm%2520pulls%2520of%2520our%2520algorithm%2520asymptotically%250Amatches%2520that%2520of%2520the%2520lower%2520bound.%2520Our%2520refined%2520analysis%2520also%2520uncovers%2520a%2520novel%250Abound%2520on%2520the%2520speed%2520at%2520which%2520the%2520average%2520number%2520of%2520arm%2520pulls%2520of%2520our%2520algorithm%250Aconverges%2520to%2520the%2520fundamental%2520limit%2520as%2520%2524%255Cdelta%2524%2520vanishes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05072v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20General%20Framework%20for%20Clustering%20and%20Distribution%20Matching%20with%20Bandit%0A%20%20Feedback&entry.906535625=Recep%20Can%20Yavas%20and%20Yuqi%20Huang%20and%20Vincent%20Y.%20F.%20Tan%20and%20Jonathan%20Scarlett&entry.1292438233=%20%20We%20develop%20a%20general%20framework%20for%20clustering%20and%20distribution%20matching%0Aproblems%20with%20bandit%20feedback.%20We%20consider%20a%20%24K%24-armed%20bandit%20model%20where%20some%0Asubset%20of%20%24K%24%20arms%20is%20partitioned%20into%20%24M%24%20groups.%20Within%20each%20group%2C%20the%0Arandom%20variable%20associated%20to%20each%20arm%20follows%20the%20same%20distribution%20on%20a%0Afinite%20alphabet.%20At%20each%20time%20step%2C%20the%20decision%20maker%20pulls%20an%20arm%20and%0Aobserves%20its%20outcome%20from%20the%20random%20variable%20associated%20to%20that%20arm.%0ASubsequent%20arm%20pulls%20depend%20on%20the%20history%20of%20arm%20pulls%20and%20their%20outcomes.%20The%0Adecision%20maker%20has%20no%20knowledge%20of%20the%20distributions%20of%20the%20arms%20or%20the%0Aunderlying%20partitions.%20The%20task%20is%20to%20devise%20an%20online%20algorithm%20to%20learn%20the%0Aunderlying%20partition%20of%20arms%20with%20the%20least%20number%20of%20arm%20pulls%20on%20average%20and%0Awith%20an%20error%20probability%20not%20exceeding%20a%20pre-determined%20value~%24%5Cdelta%24.%0ASeveral%20existing%20problems%20fall%20under%20our%20general%20framework%2C%20including%20finding%0A%24M%24%20pairs%20of%20arms%2C%20odd%20arm%20identification%2C%20and%20%24N%24-ary%20clustering%20of%20%24K%24%20arms%0Abelong%20to%20our%20general%20framework.%20We%20derive%20a%20non-asymptotic%20lower%20bound%20on%20the%0Aaverage%20number%20of%20arm%20pulls%20for%20any%20online%20algorithm%20with%20an%20error%20probability%0Anot%20exceeding%20%24%5Cdelta%24.%20Furthermore%2C%20we%20develop%20a%20computationally-efficient%0Aonline%20algorithm%20based%20on%20the%20Track-and-Stop%20method%20and%20Frank--Wolfe%20algorithm%2C%0Aand%20show%20that%20the%20average%20number%20of%20arm%20pulls%20of%20our%20algorithm%20asymptotically%0Amatches%20that%20of%20the%20lower%20bound.%20Our%20refined%20analysis%20also%20uncovers%20a%20novel%0Abound%20on%20the%20speed%20at%20which%20the%20average%20number%20of%20arm%20pulls%20of%20our%20algorithm%0Aconverges%20to%20the%20fundamental%20limit%20as%20%24%5Cdelta%24%20vanishes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05072v2&entry.124074799=Read"},
{"title": "COCOLA: Coherence-Oriented Contrastive Learning of Musical Audio\n  Representations", "author": "Ruben Ciranni and Giorgio Mariani and Michele Mancusi and Emilian Postolache and Giorgio Fabbro and Emanuele Rodol\u00e0 and Luca Cosmo", "abstract": "  We present COCOLA (Coherence-Oriented Contrastive Learning for Audio), a\ncontrastive learning method for musical audio representations that captures the\nharmonic and rhythmic coherence between samples. Our method operates at the\nlevel of the stems composing music tracks and can input features obtained via\nHarmonic-Percussive Separation (HPS). COCOLA allows the objective evaluation of\ngenerative models for music accompaniment generation, which are difficult to\nbenchmark with established metrics. In this regard, we evaluate recent music\naccompaniment generation models, demonstrating the effectiveness of the\nproposed method. We release the model checkpoints trained on public datasets\ncontaining separate stems (MUSDB18-HQ, MoisesDB, Slakh2100, and CocoChorales).\n", "link": "http://arxiv.org/abs/2404.16969v4", "date": "2025-01-09", "relevancy": 1.7942, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4524}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4478}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COCOLA%3A%20Coherence-Oriented%20Contrastive%20Learning%20of%20Musical%20Audio%0A%20%20Representations&body=Title%3A%20COCOLA%3A%20Coherence-Oriented%20Contrastive%20Learning%20of%20Musical%20Audio%0A%20%20Representations%0AAuthor%3A%20Ruben%20Ciranni%20and%20Giorgio%20Mariani%20and%20Michele%20Mancusi%20and%20Emilian%20Postolache%20and%20Giorgio%20Fabbro%20and%20Emanuele%20Rodol%C3%A0%20and%20Luca%20Cosmo%0AAbstract%3A%20%20%20We%20present%20COCOLA%20%28Coherence-Oriented%20Contrastive%20Learning%20for%20Audio%29%2C%20a%0Acontrastive%20learning%20method%20for%20musical%20audio%20representations%20that%20captures%20the%0Aharmonic%20and%20rhythmic%20coherence%20between%20samples.%20Our%20method%20operates%20at%20the%0Alevel%20of%20the%20stems%20composing%20music%20tracks%20and%20can%20input%20features%20obtained%20via%0AHarmonic-Percussive%20Separation%20%28HPS%29.%20COCOLA%20allows%20the%20objective%20evaluation%20of%0Agenerative%20models%20for%20music%20accompaniment%20generation%2C%20which%20are%20difficult%20to%0Abenchmark%20with%20established%20metrics.%20In%20this%20regard%2C%20we%20evaluate%20recent%20music%0Aaccompaniment%20generation%20models%2C%20demonstrating%20the%20effectiveness%20of%20the%0Aproposed%20method.%20We%20release%20the%20model%20checkpoints%20trained%20on%20public%20datasets%0Acontaining%20separate%20stems%20%28MUSDB18-HQ%2C%20MoisesDB%2C%20Slakh2100%2C%20and%20CocoChorales%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16969v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOCOLA%253A%2520Coherence-Oriented%2520Contrastive%2520Learning%2520of%2520Musical%2520Audio%250A%2520%2520Representations%26entry.906535625%3DRuben%2520Ciranni%2520and%2520Giorgio%2520Mariani%2520and%2520Michele%2520Mancusi%2520and%2520Emilian%2520Postolache%2520and%2520Giorgio%2520Fabbro%2520and%2520Emanuele%2520Rodol%25C3%25A0%2520and%2520Luca%2520Cosmo%26entry.1292438233%3D%2520%2520We%2520present%2520COCOLA%2520%2528Coherence-Oriented%2520Contrastive%2520Learning%2520for%2520Audio%2529%252C%2520a%250Acontrastive%2520learning%2520method%2520for%2520musical%2520audio%2520representations%2520that%2520captures%2520the%250Aharmonic%2520and%2520rhythmic%2520coherence%2520between%2520samples.%2520Our%2520method%2520operates%2520at%2520the%250Alevel%2520of%2520the%2520stems%2520composing%2520music%2520tracks%2520and%2520can%2520input%2520features%2520obtained%2520via%250AHarmonic-Percussive%2520Separation%2520%2528HPS%2529.%2520COCOLA%2520allows%2520the%2520objective%2520evaluation%2520of%250Agenerative%2520models%2520for%2520music%2520accompaniment%2520generation%252C%2520which%2520are%2520difficult%2520to%250Abenchmark%2520with%2520established%2520metrics.%2520In%2520this%2520regard%252C%2520we%2520evaluate%2520recent%2520music%250Aaccompaniment%2520generation%2520models%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520method.%2520We%2520release%2520the%2520model%2520checkpoints%2520trained%2520on%2520public%2520datasets%250Acontaining%2520separate%2520stems%2520%2528MUSDB18-HQ%252C%2520MoisesDB%252C%2520Slakh2100%252C%2520and%2520CocoChorales%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.16969v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COCOLA%3A%20Coherence-Oriented%20Contrastive%20Learning%20of%20Musical%20Audio%0A%20%20Representations&entry.906535625=Ruben%20Ciranni%20and%20Giorgio%20Mariani%20and%20Michele%20Mancusi%20and%20Emilian%20Postolache%20and%20Giorgio%20Fabbro%20and%20Emanuele%20Rodol%C3%A0%20and%20Luca%20Cosmo&entry.1292438233=%20%20We%20present%20COCOLA%20%28Coherence-Oriented%20Contrastive%20Learning%20for%20Audio%29%2C%20a%0Acontrastive%20learning%20method%20for%20musical%20audio%20representations%20that%20captures%20the%0Aharmonic%20and%20rhythmic%20coherence%20between%20samples.%20Our%20method%20operates%20at%20the%0Alevel%20of%20the%20stems%20composing%20music%20tracks%20and%20can%20input%20features%20obtained%20via%0AHarmonic-Percussive%20Separation%20%28HPS%29.%20COCOLA%20allows%20the%20objective%20evaluation%20of%0Agenerative%20models%20for%20music%20accompaniment%20generation%2C%20which%20are%20difficult%20to%0Abenchmark%20with%20established%20metrics.%20In%20this%20regard%2C%20we%20evaluate%20recent%20music%0Aaccompaniment%20generation%20models%2C%20demonstrating%20the%20effectiveness%20of%20the%0Aproposed%20method.%20We%20release%20the%20model%20checkpoints%20trained%20on%20public%20datasets%0Acontaining%20separate%20stems%20%28MUSDB18-HQ%2C%20MoisesDB%2C%20Slakh2100%2C%20and%20CocoChorales%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16969v4&entry.124074799=Read"},
{"title": "EVA-S2PLoR: A Secure Element-wise Multiplication Meets Logistic\n  Regression on Heterogeneous Database", "author": "Tianle Tao and Shizhao Peng and Tianyu Mei and Shoumo Li and Haogang Zhu", "abstract": "  Accurate nonlinear computation is a key challenge in privacy-preserving\nmachine learning (PPML). Most existing frameworks approximate it through linear\noperations, resulting in significant precision loss. This paper proposes an\nefficient, verifiable and accurate security 2-party logistic regression\nframework (EVA-S2PLoR), which achieves accurate nonlinear function computation\nthrough a novel secure element-wise multiplication protocol and its derived\nprotocols. Our framework primarily includes secure 2-party vector element-wise\nmultiplication, addition to multiplication, reciprocal, and sigmoid function\nbased on data disguising technology, where high efficiency and accuracy are\nguaranteed by the simple computation flow based on the real number domain and\nthe few number of fixed communication rounds. We provide secure and robust\nanomaly detection through dimension transformation and Monte Carlo methods.\nEVA-S2PLoR outperforms many advanced frameworks in terms of precision\n(improving the performance of the sigmoid function by about 10 orders of\nmagnitude compared to most frameworks) and delivers the best overall\nperformance in secure logistic regression experiments.\n", "link": "http://arxiv.org/abs/2501.05223v1", "date": "2025-01-09", "relevancy": 1.4219, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4805}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4771}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4701}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EVA-S2PLoR%3A%20A%20Secure%20Element-wise%20Multiplication%20Meets%20Logistic%0A%20%20Regression%20on%20Heterogeneous%20Database&body=Title%3A%20EVA-S2PLoR%3A%20A%20Secure%20Element-wise%20Multiplication%20Meets%20Logistic%0A%20%20Regression%20on%20Heterogeneous%20Database%0AAuthor%3A%20Tianle%20Tao%20and%20Shizhao%20Peng%20and%20Tianyu%20Mei%20and%20Shoumo%20Li%20and%20Haogang%20Zhu%0AAbstract%3A%20%20%20Accurate%20nonlinear%20computation%20is%20a%20key%20challenge%20in%20privacy-preserving%0Amachine%20learning%20%28PPML%29.%20Most%20existing%20frameworks%20approximate%20it%20through%20linear%0Aoperations%2C%20resulting%20in%20significant%20precision%20loss.%20This%20paper%20proposes%20an%0Aefficient%2C%20verifiable%20and%20accurate%20security%202-party%20logistic%20regression%0Aframework%20%28EVA-S2PLoR%29%2C%20which%20achieves%20accurate%20nonlinear%20function%20computation%0Athrough%20a%20novel%20secure%20element-wise%20multiplication%20protocol%20and%20its%20derived%0Aprotocols.%20Our%20framework%20primarily%20includes%20secure%202-party%20vector%20element-wise%0Amultiplication%2C%20addition%20to%20multiplication%2C%20reciprocal%2C%20and%20sigmoid%20function%0Abased%20on%20data%20disguising%20technology%2C%20where%20high%20efficiency%20and%20accuracy%20are%0Aguaranteed%20by%20the%20simple%20computation%20flow%20based%20on%20the%20real%20number%20domain%20and%0Athe%20few%20number%20of%20fixed%20communication%20rounds.%20We%20provide%20secure%20and%20robust%0Aanomaly%20detection%20through%20dimension%20transformation%20and%20Monte%20Carlo%20methods.%0AEVA-S2PLoR%20outperforms%20many%20advanced%20frameworks%20in%20terms%20of%20precision%0A%28improving%20the%20performance%20of%20the%20sigmoid%20function%20by%20about%2010%20orders%20of%0Amagnitude%20compared%20to%20most%20frameworks%29%20and%20delivers%20the%20best%20overall%0Aperformance%20in%20secure%20logistic%20regression%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05223v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEVA-S2PLoR%253A%2520A%2520Secure%2520Element-wise%2520Multiplication%2520Meets%2520Logistic%250A%2520%2520Regression%2520on%2520Heterogeneous%2520Database%26entry.906535625%3DTianle%2520Tao%2520and%2520Shizhao%2520Peng%2520and%2520Tianyu%2520Mei%2520and%2520Shoumo%2520Li%2520and%2520Haogang%2520Zhu%26entry.1292438233%3D%2520%2520Accurate%2520nonlinear%2520computation%2520is%2520a%2520key%2520challenge%2520in%2520privacy-preserving%250Amachine%2520learning%2520%2528PPML%2529.%2520Most%2520existing%2520frameworks%2520approximate%2520it%2520through%2520linear%250Aoperations%252C%2520resulting%2520in%2520significant%2520precision%2520loss.%2520This%2520paper%2520proposes%2520an%250Aefficient%252C%2520verifiable%2520and%2520accurate%2520security%25202-party%2520logistic%2520regression%250Aframework%2520%2528EVA-S2PLoR%2529%252C%2520which%2520achieves%2520accurate%2520nonlinear%2520function%2520computation%250Athrough%2520a%2520novel%2520secure%2520element-wise%2520multiplication%2520protocol%2520and%2520its%2520derived%250Aprotocols.%2520Our%2520framework%2520primarily%2520includes%2520secure%25202-party%2520vector%2520element-wise%250Amultiplication%252C%2520addition%2520to%2520multiplication%252C%2520reciprocal%252C%2520and%2520sigmoid%2520function%250Abased%2520on%2520data%2520disguising%2520technology%252C%2520where%2520high%2520efficiency%2520and%2520accuracy%2520are%250Aguaranteed%2520by%2520the%2520simple%2520computation%2520flow%2520based%2520on%2520the%2520real%2520number%2520domain%2520and%250Athe%2520few%2520number%2520of%2520fixed%2520communication%2520rounds.%2520We%2520provide%2520secure%2520and%2520robust%250Aanomaly%2520detection%2520through%2520dimension%2520transformation%2520and%2520Monte%2520Carlo%2520methods.%250AEVA-S2PLoR%2520outperforms%2520many%2520advanced%2520frameworks%2520in%2520terms%2520of%2520precision%250A%2528improving%2520the%2520performance%2520of%2520the%2520sigmoid%2520function%2520by%2520about%252010%2520orders%2520of%250Amagnitude%2520compared%2520to%2520most%2520frameworks%2529%2520and%2520delivers%2520the%2520best%2520overall%250Aperformance%2520in%2520secure%2520logistic%2520regression%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05223v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EVA-S2PLoR%3A%20A%20Secure%20Element-wise%20Multiplication%20Meets%20Logistic%0A%20%20Regression%20on%20Heterogeneous%20Database&entry.906535625=Tianle%20Tao%20and%20Shizhao%20Peng%20and%20Tianyu%20Mei%20and%20Shoumo%20Li%20and%20Haogang%20Zhu&entry.1292438233=%20%20Accurate%20nonlinear%20computation%20is%20a%20key%20challenge%20in%20privacy-preserving%0Amachine%20learning%20%28PPML%29.%20Most%20existing%20frameworks%20approximate%20it%20through%20linear%0Aoperations%2C%20resulting%20in%20significant%20precision%20loss.%20This%20paper%20proposes%20an%0Aefficient%2C%20verifiable%20and%20accurate%20security%202-party%20logistic%20regression%0Aframework%20%28EVA-S2PLoR%29%2C%20which%20achieves%20accurate%20nonlinear%20function%20computation%0Athrough%20a%20novel%20secure%20element-wise%20multiplication%20protocol%20and%20its%20derived%0Aprotocols.%20Our%20framework%20primarily%20includes%20secure%202-party%20vector%20element-wise%0Amultiplication%2C%20addition%20to%20multiplication%2C%20reciprocal%2C%20and%20sigmoid%20function%0Abased%20on%20data%20disguising%20technology%2C%20where%20high%20efficiency%20and%20accuracy%20are%0Aguaranteed%20by%20the%20simple%20computation%20flow%20based%20on%20the%20real%20number%20domain%20and%0Athe%20few%20number%20of%20fixed%20communication%20rounds.%20We%20provide%20secure%20and%20robust%0Aanomaly%20detection%20through%20dimension%20transformation%20and%20Monte%20Carlo%20methods.%0AEVA-S2PLoR%20outperforms%20many%20advanced%20frameworks%20in%20terms%20of%20precision%0A%28improving%20the%20performance%20of%20the%20sigmoid%20function%20by%20about%2010%20orders%20of%0Amagnitude%20compared%20to%20most%20frameworks%29%20and%20delivers%20the%20best%20overall%0Aperformance%20in%20secure%20logistic%20regression%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05223v1&entry.124074799=Read"},
{"title": "An Algorithmic Approach for Causal Health Equity: A Look at Race\n  Differentials in Intensive Care Unit (ICU) Outcomes", "author": "Drago Plecko and Paul Secombe and Andrea Clarke and Amelia Fiske and Samarra Toby and Donisha Duff and David Pilcher and Leo Anthony Celi and Rinaldo Bellomo and Elias Bareinboim", "abstract": "  The new era of large-scale data collection and analysis presents an\nopportunity for diagnosing and understanding the causes of health inequities.\nIn this study, we describe a framework for systematically analyzing health\ndisparities using causal inference. The framework is illustrated by\ninvestigating racial and ethnic disparities in intensive care unit (ICU)\noutcome between majority and minority groups in Australia (Indigenous vs.\nNon-Indigenous) and the United States (African-American vs. White). We\ndemonstrate that commonly used statistical measures for quantifying inequity\nare insufficient, and focus on attributing the observed disparity to the causal\nmechanisms that generate it. We find that minority patients are younger at\nadmission, have worse chronic health, are more likely to be admitted for urgent\nand non-elective reasons, and have higher illness severity. At the same time,\nhowever, we find a protective direct effect of belonging to a minority group,\nwith minority patients showing improved survival compared to their majority\ncounterparts, with all other variables kept equal. We demonstrate that this\nprotective effect is related to the increased probability of being admitted to\nICU, with minority patients having an increased risk of ICU admission. We also\nfind that minority patients, while showing improved survival, are more likely\nto be readmitted to ICU. Thus, due to worse access to primary health care,\nminority patients are more likely to end up in ICU for preventable conditions,\ncausing a reduction in the mortality rates and creating an effect that appears\nto be protective. Since the baseline risk of ICU admission may serve as proxy\nfor lack of access to primary care, we developed the Indigenous Intensive Care\nEquity (IICE) Radar, a monitoring system for tracking the over-utilization of\nICU resources by the Indigenous population of Australia across geographical\nareas.\n", "link": "http://arxiv.org/abs/2501.05197v1", "date": "2025-01-09", "relevancy": 0.999, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3344}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3329}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.3325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Algorithmic%20Approach%20for%20Causal%20Health%20Equity%3A%20A%20Look%20at%20Race%0A%20%20Differentials%20in%20Intensive%20Care%20Unit%20%28ICU%29%20Outcomes&body=Title%3A%20An%20Algorithmic%20Approach%20for%20Causal%20Health%20Equity%3A%20A%20Look%20at%20Race%0A%20%20Differentials%20in%20Intensive%20Care%20Unit%20%28ICU%29%20Outcomes%0AAuthor%3A%20Drago%20Plecko%20and%20Paul%20Secombe%20and%20Andrea%20Clarke%20and%20Amelia%20Fiske%20and%20Samarra%20Toby%20and%20Donisha%20Duff%20and%20David%20Pilcher%20and%20Leo%20Anthony%20Celi%20and%20Rinaldo%20Bellomo%20and%20Elias%20Bareinboim%0AAbstract%3A%20%20%20The%20new%20era%20of%20large-scale%20data%20collection%20and%20analysis%20presents%20an%0Aopportunity%20for%20diagnosing%20and%20understanding%20the%20causes%20of%20health%20inequities.%0AIn%20this%20study%2C%20we%20describe%20a%20framework%20for%20systematically%20analyzing%20health%0Adisparities%20using%20causal%20inference.%20The%20framework%20is%20illustrated%20by%0Ainvestigating%20racial%20and%20ethnic%20disparities%20in%20intensive%20care%20unit%20%28ICU%29%0Aoutcome%20between%20majority%20and%20minority%20groups%20in%20Australia%20%28Indigenous%20vs.%0ANon-Indigenous%29%20and%20the%20United%20States%20%28African-American%20vs.%20White%29.%20We%0Ademonstrate%20that%20commonly%20used%20statistical%20measures%20for%20quantifying%20inequity%0Aare%20insufficient%2C%20and%20focus%20on%20attributing%20the%20observed%20disparity%20to%20the%20causal%0Amechanisms%20that%20generate%20it.%20We%20find%20that%20minority%20patients%20are%20younger%20at%0Aadmission%2C%20have%20worse%20chronic%20health%2C%20are%20more%20likely%20to%20be%20admitted%20for%20urgent%0Aand%20non-elective%20reasons%2C%20and%20have%20higher%20illness%20severity.%20At%20the%20same%20time%2C%0Ahowever%2C%20we%20find%20a%20protective%20direct%20effect%20of%20belonging%20to%20a%20minority%20group%2C%0Awith%20minority%20patients%20showing%20improved%20survival%20compared%20to%20their%20majority%0Acounterparts%2C%20with%20all%20other%20variables%20kept%20equal.%20We%20demonstrate%20that%20this%0Aprotective%20effect%20is%20related%20to%20the%20increased%20probability%20of%20being%20admitted%20to%0AICU%2C%20with%20minority%20patients%20having%20an%20increased%20risk%20of%20ICU%20admission.%20We%20also%0Afind%20that%20minority%20patients%2C%20while%20showing%20improved%20survival%2C%20are%20more%20likely%0Ato%20be%20readmitted%20to%20ICU.%20Thus%2C%20due%20to%20worse%20access%20to%20primary%20health%20care%2C%0Aminority%20patients%20are%20more%20likely%20to%20end%20up%20in%20ICU%20for%20preventable%20conditions%2C%0Acausing%20a%20reduction%20in%20the%20mortality%20rates%20and%20creating%20an%20effect%20that%20appears%0Ato%20be%20protective.%20Since%20the%20baseline%20risk%20of%20ICU%20admission%20may%20serve%20as%20proxy%0Afor%20lack%20of%20access%20to%20primary%20care%2C%20we%20developed%20the%20Indigenous%20Intensive%20Care%0AEquity%20%28IICE%29%20Radar%2C%20a%20monitoring%20system%20for%20tracking%20the%20over-utilization%20of%0AICU%20resources%20by%20the%20Indigenous%20population%20of%20Australia%20across%20geographical%0Aareas.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05197v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Algorithmic%2520Approach%2520for%2520Causal%2520Health%2520Equity%253A%2520A%2520Look%2520at%2520Race%250A%2520%2520Differentials%2520in%2520Intensive%2520Care%2520Unit%2520%2528ICU%2529%2520Outcomes%26entry.906535625%3DDrago%2520Plecko%2520and%2520Paul%2520Secombe%2520and%2520Andrea%2520Clarke%2520and%2520Amelia%2520Fiske%2520and%2520Samarra%2520Toby%2520and%2520Donisha%2520Duff%2520and%2520David%2520Pilcher%2520and%2520Leo%2520Anthony%2520Celi%2520and%2520Rinaldo%2520Bellomo%2520and%2520Elias%2520Bareinboim%26entry.1292438233%3D%2520%2520The%2520new%2520era%2520of%2520large-scale%2520data%2520collection%2520and%2520analysis%2520presents%2520an%250Aopportunity%2520for%2520diagnosing%2520and%2520understanding%2520the%2520causes%2520of%2520health%2520inequities.%250AIn%2520this%2520study%252C%2520we%2520describe%2520a%2520framework%2520for%2520systematically%2520analyzing%2520health%250Adisparities%2520using%2520causal%2520inference.%2520The%2520framework%2520is%2520illustrated%2520by%250Ainvestigating%2520racial%2520and%2520ethnic%2520disparities%2520in%2520intensive%2520care%2520unit%2520%2528ICU%2529%250Aoutcome%2520between%2520majority%2520and%2520minority%2520groups%2520in%2520Australia%2520%2528Indigenous%2520vs.%250ANon-Indigenous%2529%2520and%2520the%2520United%2520States%2520%2528African-American%2520vs.%2520White%2529.%2520We%250Ademonstrate%2520that%2520commonly%2520used%2520statistical%2520measures%2520for%2520quantifying%2520inequity%250Aare%2520insufficient%252C%2520and%2520focus%2520on%2520attributing%2520the%2520observed%2520disparity%2520to%2520the%2520causal%250Amechanisms%2520that%2520generate%2520it.%2520We%2520find%2520that%2520minority%2520patients%2520are%2520younger%2520at%250Aadmission%252C%2520have%2520worse%2520chronic%2520health%252C%2520are%2520more%2520likely%2520to%2520be%2520admitted%2520for%2520urgent%250Aand%2520non-elective%2520reasons%252C%2520and%2520have%2520higher%2520illness%2520severity.%2520At%2520the%2520same%2520time%252C%250Ahowever%252C%2520we%2520find%2520a%2520protective%2520direct%2520effect%2520of%2520belonging%2520to%2520a%2520minority%2520group%252C%250Awith%2520minority%2520patients%2520showing%2520improved%2520survival%2520compared%2520to%2520their%2520majority%250Acounterparts%252C%2520with%2520all%2520other%2520variables%2520kept%2520equal.%2520We%2520demonstrate%2520that%2520this%250Aprotective%2520effect%2520is%2520related%2520to%2520the%2520increased%2520probability%2520of%2520being%2520admitted%2520to%250AICU%252C%2520with%2520minority%2520patients%2520having%2520an%2520increased%2520risk%2520of%2520ICU%2520admission.%2520We%2520also%250Afind%2520that%2520minority%2520patients%252C%2520while%2520showing%2520improved%2520survival%252C%2520are%2520more%2520likely%250Ato%2520be%2520readmitted%2520to%2520ICU.%2520Thus%252C%2520due%2520to%2520worse%2520access%2520to%2520primary%2520health%2520care%252C%250Aminority%2520patients%2520are%2520more%2520likely%2520to%2520end%2520up%2520in%2520ICU%2520for%2520preventable%2520conditions%252C%250Acausing%2520a%2520reduction%2520in%2520the%2520mortality%2520rates%2520and%2520creating%2520an%2520effect%2520that%2520appears%250Ato%2520be%2520protective.%2520Since%2520the%2520baseline%2520risk%2520of%2520ICU%2520admission%2520may%2520serve%2520as%2520proxy%250Afor%2520lack%2520of%2520access%2520to%2520primary%2520care%252C%2520we%2520developed%2520the%2520Indigenous%2520Intensive%2520Care%250AEquity%2520%2528IICE%2529%2520Radar%252C%2520a%2520monitoring%2520system%2520for%2520tracking%2520the%2520over-utilization%2520of%250AICU%2520resources%2520by%2520the%2520Indigenous%2520population%2520of%2520Australia%2520across%2520geographical%250Aareas.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05197v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Algorithmic%20Approach%20for%20Causal%20Health%20Equity%3A%20A%20Look%20at%20Race%0A%20%20Differentials%20in%20Intensive%20Care%20Unit%20%28ICU%29%20Outcomes&entry.906535625=Drago%20Plecko%20and%20Paul%20Secombe%20and%20Andrea%20Clarke%20and%20Amelia%20Fiske%20and%20Samarra%20Toby%20and%20Donisha%20Duff%20and%20David%20Pilcher%20and%20Leo%20Anthony%20Celi%20and%20Rinaldo%20Bellomo%20and%20Elias%20Bareinboim&entry.1292438233=%20%20The%20new%20era%20of%20large-scale%20data%20collection%20and%20analysis%20presents%20an%0Aopportunity%20for%20diagnosing%20and%20understanding%20the%20causes%20of%20health%20inequities.%0AIn%20this%20study%2C%20we%20describe%20a%20framework%20for%20systematically%20analyzing%20health%0Adisparities%20using%20causal%20inference.%20The%20framework%20is%20illustrated%20by%0Ainvestigating%20racial%20and%20ethnic%20disparities%20in%20intensive%20care%20unit%20%28ICU%29%0Aoutcome%20between%20majority%20and%20minority%20groups%20in%20Australia%20%28Indigenous%20vs.%0ANon-Indigenous%29%20and%20the%20United%20States%20%28African-American%20vs.%20White%29.%20We%0Ademonstrate%20that%20commonly%20used%20statistical%20measures%20for%20quantifying%20inequity%0Aare%20insufficient%2C%20and%20focus%20on%20attributing%20the%20observed%20disparity%20to%20the%20causal%0Amechanisms%20that%20generate%20it.%20We%20find%20that%20minority%20patients%20are%20younger%20at%0Aadmission%2C%20have%20worse%20chronic%20health%2C%20are%20more%20likely%20to%20be%20admitted%20for%20urgent%0Aand%20non-elective%20reasons%2C%20and%20have%20higher%20illness%20severity.%20At%20the%20same%20time%2C%0Ahowever%2C%20we%20find%20a%20protective%20direct%20effect%20of%20belonging%20to%20a%20minority%20group%2C%0Awith%20minority%20patients%20showing%20improved%20survival%20compared%20to%20their%20majority%0Acounterparts%2C%20with%20all%20other%20variables%20kept%20equal.%20We%20demonstrate%20that%20this%0Aprotective%20effect%20is%20related%20to%20the%20increased%20probability%20of%20being%20admitted%20to%0AICU%2C%20with%20minority%20patients%20having%20an%20increased%20risk%20of%20ICU%20admission.%20We%20also%0Afind%20that%20minority%20patients%2C%20while%20showing%20improved%20survival%2C%20are%20more%20likely%0Ato%20be%20readmitted%20to%20ICU.%20Thus%2C%20due%20to%20worse%20access%20to%20primary%20health%20care%2C%0Aminority%20patients%20are%20more%20likely%20to%20end%20up%20in%20ICU%20for%20preventable%20conditions%2C%0Acausing%20a%20reduction%20in%20the%20mortality%20rates%20and%20creating%20an%20effect%20that%20appears%0Ato%20be%20protective.%20Since%20the%20baseline%20risk%20of%20ICU%20admission%20may%20serve%20as%20proxy%0Afor%20lack%20of%20access%20to%20primary%20care%2C%20we%20developed%20the%20Indigenous%20Intensive%20Care%0AEquity%20%28IICE%29%20Radar%2C%20a%20monitoring%20system%20for%20tracking%20the%20over-utilization%20of%0AICU%20resources%20by%20the%20Indigenous%20population%20of%20Australia%20across%20geographical%0Aareas.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05197v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


