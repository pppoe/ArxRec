<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251103.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Human Mesh Modeling for Anny Body", "author": "Romain Br\u00e9gier and Gu\u00e9nol\u00e9 Fiche and Laura Bravo-S\u00e1nchez and Thomas Lucas and Matthieu Armando and Philippe Weinzaepfel and Gr\u00e9gory Rogez and Fabien Baradel", "abstract": "  Parametric body models are central to many human-centric tasks, yet existing\nmodels often rely on costly 3D scans and learned shape spaces that are\nproprietary and demographically narrow. We introduce Anny, a simple, fully\ndifferentiable, and scan-free human body model grounded in anthropometric\nknowledge from the MakeHuman community. Anny defines a continuous,\ninterpretable shape space, where phenotype parameters (e.g. gender, age,\nheight, weight) control blendshapes spanning a wide range of human forms --\nacross ages (from infants to elders), body types, and proportions. Calibrated\nusing WHO population statistics, it provides realistic and demographically\ngrounded human shape variation within a single unified model. Thanks to its\nopenness and semantic control, Anny serves as a versatile foundation for 3D\nhuman modeling -- supporting millimeter-accurate scan fitting, controlled\nsynthetic data generation, and Human Mesh Recovery (HMR). We further introduce\nAnny-One, a collection of 800k photorealistic humans generated with Anny,\nshowing that despite its simplicity, HMR models trained with Anny can match the\nperformance of those trained with scan-based body models, while remaining\ninterpretable and broadly representative. The Anny body model and its code are\nreleased under the Apache 2.0 license, making Anny an accessible foundation for\nhuman-centric 3D modeling.\n", "link": "http://arxiv.org/abs/2511.03589v1", "date": "2025-11-05", "relevancy": 2.9422, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6153}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5914}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human%20Mesh%20Modeling%20for%20Anny%20Body&body=Title%3A%20Human%20Mesh%20Modeling%20for%20Anny%20Body%0AAuthor%3A%20Romain%20Br%C3%A9gier%20and%20Gu%C3%A9nol%C3%A9%20Fiche%20and%20Laura%20Bravo-S%C3%A1nchez%20and%20Thomas%20Lucas%20and%20Matthieu%20Armando%20and%20Philippe%20Weinzaepfel%20and%20Gr%C3%A9gory%20Rogez%20and%20Fabien%20Baradel%0AAbstract%3A%20%20%20Parametric%20body%20models%20are%20central%20to%20many%20human-centric%20tasks%2C%20yet%20existing%0Amodels%20often%20rely%20on%20costly%203D%20scans%20and%20learned%20shape%20spaces%20that%20are%0Aproprietary%20and%20demographically%20narrow.%20We%20introduce%20Anny%2C%20a%20simple%2C%20fully%0Adifferentiable%2C%20and%20scan-free%20human%20body%20model%20grounded%20in%20anthropometric%0Aknowledge%20from%20the%20MakeHuman%20community.%20Anny%20defines%20a%20continuous%2C%0Ainterpretable%20shape%20space%2C%20where%20phenotype%20parameters%20%28e.g.%20gender%2C%20age%2C%0Aheight%2C%20weight%29%20control%20blendshapes%20spanning%20a%20wide%20range%20of%20human%20forms%20--%0Aacross%20ages%20%28from%20infants%20to%20elders%29%2C%20body%20types%2C%20and%20proportions.%20Calibrated%0Ausing%20WHO%20population%20statistics%2C%20it%20provides%20realistic%20and%20demographically%0Agrounded%20human%20shape%20variation%20within%20a%20single%20unified%20model.%20Thanks%20to%20its%0Aopenness%20and%20semantic%20control%2C%20Anny%20serves%20as%20a%20versatile%20foundation%20for%203D%0Ahuman%20modeling%20--%20supporting%20millimeter-accurate%20scan%20fitting%2C%20controlled%0Asynthetic%20data%20generation%2C%20and%20Human%20Mesh%20Recovery%20%28HMR%29.%20We%20further%20introduce%0AAnny-One%2C%20a%20collection%20of%20800k%20photorealistic%20humans%20generated%20with%20Anny%2C%0Ashowing%20that%20despite%20its%20simplicity%2C%20HMR%20models%20trained%20with%20Anny%20can%20match%20the%0Aperformance%20of%20those%20trained%20with%20scan-based%20body%20models%2C%20while%20remaining%0Ainterpretable%20and%20broadly%20representative.%20The%20Anny%20body%20model%20and%20its%20code%20are%0Areleased%20under%20the%20Apache%202.0%20license%2C%20making%20Anny%20an%20accessible%20foundation%20for%0Ahuman-centric%203D%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03589v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman%2520Mesh%2520Modeling%2520for%2520Anny%2520Body%26entry.906535625%3DRomain%2520Br%25C3%25A9gier%2520and%2520Gu%25C3%25A9nol%25C3%25A9%2520Fiche%2520and%2520Laura%2520Bravo-S%25C3%25A1nchez%2520and%2520Thomas%2520Lucas%2520and%2520Matthieu%2520Armando%2520and%2520Philippe%2520Weinzaepfel%2520and%2520Gr%25C3%25A9gory%2520Rogez%2520and%2520Fabien%2520Baradel%26entry.1292438233%3D%2520%2520Parametric%2520body%2520models%2520are%2520central%2520to%2520many%2520human-centric%2520tasks%252C%2520yet%2520existing%250Amodels%2520often%2520rely%2520on%2520costly%25203D%2520scans%2520and%2520learned%2520shape%2520spaces%2520that%2520are%250Aproprietary%2520and%2520demographically%2520narrow.%2520We%2520introduce%2520Anny%252C%2520a%2520simple%252C%2520fully%250Adifferentiable%252C%2520and%2520scan-free%2520human%2520body%2520model%2520grounded%2520in%2520anthropometric%250Aknowledge%2520from%2520the%2520MakeHuman%2520community.%2520Anny%2520defines%2520a%2520continuous%252C%250Ainterpretable%2520shape%2520space%252C%2520where%2520phenotype%2520parameters%2520%2528e.g.%2520gender%252C%2520age%252C%250Aheight%252C%2520weight%2529%2520control%2520blendshapes%2520spanning%2520a%2520wide%2520range%2520of%2520human%2520forms%2520--%250Aacross%2520ages%2520%2528from%2520infants%2520to%2520elders%2529%252C%2520body%2520types%252C%2520and%2520proportions.%2520Calibrated%250Ausing%2520WHO%2520population%2520statistics%252C%2520it%2520provides%2520realistic%2520and%2520demographically%250Agrounded%2520human%2520shape%2520variation%2520within%2520a%2520single%2520unified%2520model.%2520Thanks%2520to%2520its%250Aopenness%2520and%2520semantic%2520control%252C%2520Anny%2520serves%2520as%2520a%2520versatile%2520foundation%2520for%25203D%250Ahuman%2520modeling%2520--%2520supporting%2520millimeter-accurate%2520scan%2520fitting%252C%2520controlled%250Asynthetic%2520data%2520generation%252C%2520and%2520Human%2520Mesh%2520Recovery%2520%2528HMR%2529.%2520We%2520further%2520introduce%250AAnny-One%252C%2520a%2520collection%2520of%2520800k%2520photorealistic%2520humans%2520generated%2520with%2520Anny%252C%250Ashowing%2520that%2520despite%2520its%2520simplicity%252C%2520HMR%2520models%2520trained%2520with%2520Anny%2520can%2520match%2520the%250Aperformance%2520of%2520those%2520trained%2520with%2520scan-based%2520body%2520models%252C%2520while%2520remaining%250Ainterpretable%2520and%2520broadly%2520representative.%2520The%2520Anny%2520body%2520model%2520and%2520its%2520code%2520are%250Areleased%2520under%2520the%2520Apache%25202.0%2520license%252C%2520making%2520Anny%2520an%2520accessible%2520foundation%2520for%250Ahuman-centric%25203D%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03589v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human%20Mesh%20Modeling%20for%20Anny%20Body&entry.906535625=Romain%20Br%C3%A9gier%20and%20Gu%C3%A9nol%C3%A9%20Fiche%20and%20Laura%20Bravo-S%C3%A1nchez%20and%20Thomas%20Lucas%20and%20Matthieu%20Armando%20and%20Philippe%20Weinzaepfel%20and%20Gr%C3%A9gory%20Rogez%20and%20Fabien%20Baradel&entry.1292438233=%20%20Parametric%20body%20models%20are%20central%20to%20many%20human-centric%20tasks%2C%20yet%20existing%0Amodels%20often%20rely%20on%20costly%203D%20scans%20and%20learned%20shape%20spaces%20that%20are%0Aproprietary%20and%20demographically%20narrow.%20We%20introduce%20Anny%2C%20a%20simple%2C%20fully%0Adifferentiable%2C%20and%20scan-free%20human%20body%20model%20grounded%20in%20anthropometric%0Aknowledge%20from%20the%20MakeHuman%20community.%20Anny%20defines%20a%20continuous%2C%0Ainterpretable%20shape%20space%2C%20where%20phenotype%20parameters%20%28e.g.%20gender%2C%20age%2C%0Aheight%2C%20weight%29%20control%20blendshapes%20spanning%20a%20wide%20range%20of%20human%20forms%20--%0Aacross%20ages%20%28from%20infants%20to%20elders%29%2C%20body%20types%2C%20and%20proportions.%20Calibrated%0Ausing%20WHO%20population%20statistics%2C%20it%20provides%20realistic%20and%20demographically%0Agrounded%20human%20shape%20variation%20within%20a%20single%20unified%20model.%20Thanks%20to%20its%0Aopenness%20and%20semantic%20control%2C%20Anny%20serves%20as%20a%20versatile%20foundation%20for%203D%0Ahuman%20modeling%20--%20supporting%20millimeter-accurate%20scan%20fitting%2C%20controlled%0Asynthetic%20data%20generation%2C%20and%20Human%20Mesh%20Recovery%20%28HMR%29.%20We%20further%20introduce%0AAnny-One%2C%20a%20collection%20of%20800k%20photorealistic%20humans%20generated%20with%20Anny%2C%0Ashowing%20that%20despite%20its%20simplicity%2C%20HMR%20models%20trained%20with%20Anny%20can%20match%20the%0Aperformance%20of%20those%20trained%20with%20scan-based%20body%20models%2C%20while%20remaining%0Ainterpretable%20and%20broadly%20representative.%20The%20Anny%20body%20model%20and%20its%20code%20are%0Areleased%20under%20the%20Apache%202.0%20license%2C%20making%20Anny%20an%20accessible%20foundation%20for%0Ahuman-centric%203D%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03589v1&entry.124074799=Read"},
{"title": "Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for\n  Improving Video Generation", "author": "Jie Du and Xinyu Gong and Qingshan Tan and Wen Li and Yangming Cheng and Weitao Wang and Chenlu Zhan and Suhui Wu and Hao Zhang and Jun Zhang", "abstract": "  Recent studies have identified Direct Preference Optimization (DPO) as an\nefficient and reward-free approach to improving video generation quality.\nHowever, existing methods largely follow image-domain paradigms and are mainly\ndeveloped on small-scale models (approximately 2B parameters), limiting their\nability to address the unique challenges of video tasks, such as costly data\nconstruction, unstable training, and heavy memory consumption. To overcome\nthese limitations, we introduce a GT-Pair that automatically builds\nhigh-quality preference pairs by using real videos as positives and\nmodel-generated videos as negatives, eliminating the need for any external\nannotation. We further present Reg-DPO, which incorporates the SFT loss as a\nregularization term into the DPO loss to enhance training stability and\ngeneration fidelity. Additionally, by combining the FSDP framework with\nmultiple memory optimization techniques, our approach achieves nearly three\ntimes higher training capacity than using FSDP alone. Extensive experiments on\nboth I2V and T2V tasks across multiple datasets demonstrate that our method\nconsistently outperforms existing approaches, delivering superior video\ngeneration quality.\n", "link": "http://arxiv.org/abs/2511.01450v2", "date": "2025-11-05", "relevancy": 2.8719, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5788}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5767}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reg-DPO%3A%20SFT-Regularized%20Direct%20Preference%20Optimization%20with%20GT-Pair%20for%0A%20%20Improving%20Video%20Generation&body=Title%3A%20Reg-DPO%3A%20SFT-Regularized%20Direct%20Preference%20Optimization%20with%20GT-Pair%20for%0A%20%20Improving%20Video%20Generation%0AAuthor%3A%20Jie%20Du%20and%20Xinyu%20Gong%20and%20Qingshan%20Tan%20and%20Wen%20Li%20and%20Yangming%20Cheng%20and%20Weitao%20Wang%20and%20Chenlu%20Zhan%20and%20Suhui%20Wu%20and%20Hao%20Zhang%20and%20Jun%20Zhang%0AAbstract%3A%20%20%20Recent%20studies%20have%20identified%20Direct%20Preference%20Optimization%20%28DPO%29%20as%20an%0Aefficient%20and%20reward-free%20approach%20to%20improving%20video%20generation%20quality.%0AHowever%2C%20existing%20methods%20largely%20follow%20image-domain%20paradigms%20and%20are%20mainly%0Adeveloped%20on%20small-scale%20models%20%28approximately%202B%20parameters%29%2C%20limiting%20their%0Aability%20to%20address%20the%20unique%20challenges%20of%20video%20tasks%2C%20such%20as%20costly%20data%0Aconstruction%2C%20unstable%20training%2C%20and%20heavy%20memory%20consumption.%20To%20overcome%0Athese%20limitations%2C%20we%20introduce%20a%20GT-Pair%20that%20automatically%20builds%0Ahigh-quality%20preference%20pairs%20by%20using%20real%20videos%20as%20positives%20and%0Amodel-generated%20videos%20as%20negatives%2C%20eliminating%20the%20need%20for%20any%20external%0Aannotation.%20We%20further%20present%20Reg-DPO%2C%20which%20incorporates%20the%20SFT%20loss%20as%20a%0Aregularization%20term%20into%20the%20DPO%20loss%20to%20enhance%20training%20stability%20and%0Ageneration%20fidelity.%20Additionally%2C%20by%20combining%20the%20FSDP%20framework%20with%0Amultiple%20memory%20optimization%20techniques%2C%20our%20approach%20achieves%20nearly%20three%0Atimes%20higher%20training%20capacity%20than%20using%20FSDP%20alone.%20Extensive%20experiments%20on%0Aboth%20I2V%20and%20T2V%20tasks%20across%20multiple%20datasets%20demonstrate%20that%20our%20method%0Aconsistently%20outperforms%20existing%20approaches%2C%20delivering%20superior%20video%0Ageneration%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.01450v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReg-DPO%253A%2520SFT-Regularized%2520Direct%2520Preference%2520Optimization%2520with%2520GT-Pair%2520for%250A%2520%2520Improving%2520Video%2520Generation%26entry.906535625%3DJie%2520Du%2520and%2520Xinyu%2520Gong%2520and%2520Qingshan%2520Tan%2520and%2520Wen%2520Li%2520and%2520Yangming%2520Cheng%2520and%2520Weitao%2520Wang%2520and%2520Chenlu%2520Zhan%2520and%2520Suhui%2520Wu%2520and%2520Hao%2520Zhang%2520and%2520Jun%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520studies%2520have%2520identified%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%2520as%2520an%250Aefficient%2520and%2520reward-free%2520approach%2520to%2520improving%2520video%2520generation%2520quality.%250AHowever%252C%2520existing%2520methods%2520largely%2520follow%2520image-domain%2520paradigms%2520and%2520are%2520mainly%250Adeveloped%2520on%2520small-scale%2520models%2520%2528approximately%25202B%2520parameters%2529%252C%2520limiting%2520their%250Aability%2520to%2520address%2520the%2520unique%2520challenges%2520of%2520video%2520tasks%252C%2520such%2520as%2520costly%2520data%250Aconstruction%252C%2520unstable%2520training%252C%2520and%2520heavy%2520memory%2520consumption.%2520To%2520overcome%250Athese%2520limitations%252C%2520we%2520introduce%2520a%2520GT-Pair%2520that%2520automatically%2520builds%250Ahigh-quality%2520preference%2520pairs%2520by%2520using%2520real%2520videos%2520as%2520positives%2520and%250Amodel-generated%2520videos%2520as%2520negatives%252C%2520eliminating%2520the%2520need%2520for%2520any%2520external%250Aannotation.%2520We%2520further%2520present%2520Reg-DPO%252C%2520which%2520incorporates%2520the%2520SFT%2520loss%2520as%2520a%250Aregularization%2520term%2520into%2520the%2520DPO%2520loss%2520to%2520enhance%2520training%2520stability%2520and%250Ageneration%2520fidelity.%2520Additionally%252C%2520by%2520combining%2520the%2520FSDP%2520framework%2520with%250Amultiple%2520memory%2520optimization%2520techniques%252C%2520our%2520approach%2520achieves%2520nearly%2520three%250Atimes%2520higher%2520training%2520capacity%2520than%2520using%2520FSDP%2520alone.%2520Extensive%2520experiments%2520on%250Aboth%2520I2V%2520and%2520T2V%2520tasks%2520across%2520multiple%2520datasets%2520demonstrate%2520that%2520our%2520method%250Aconsistently%2520outperforms%2520existing%2520approaches%252C%2520delivering%2520superior%2520video%250Ageneration%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.01450v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reg-DPO%3A%20SFT-Regularized%20Direct%20Preference%20Optimization%20with%20GT-Pair%20for%0A%20%20Improving%20Video%20Generation&entry.906535625=Jie%20Du%20and%20Xinyu%20Gong%20and%20Qingshan%20Tan%20and%20Wen%20Li%20and%20Yangming%20Cheng%20and%20Weitao%20Wang%20and%20Chenlu%20Zhan%20and%20Suhui%20Wu%20and%20Hao%20Zhang%20and%20Jun%20Zhang&entry.1292438233=%20%20Recent%20studies%20have%20identified%20Direct%20Preference%20Optimization%20%28DPO%29%20as%20an%0Aefficient%20and%20reward-free%20approach%20to%20improving%20video%20generation%20quality.%0AHowever%2C%20existing%20methods%20largely%20follow%20image-domain%20paradigms%20and%20are%20mainly%0Adeveloped%20on%20small-scale%20models%20%28approximately%202B%20parameters%29%2C%20limiting%20their%0Aability%20to%20address%20the%20unique%20challenges%20of%20video%20tasks%2C%20such%20as%20costly%20data%0Aconstruction%2C%20unstable%20training%2C%20and%20heavy%20memory%20consumption.%20To%20overcome%0Athese%20limitations%2C%20we%20introduce%20a%20GT-Pair%20that%20automatically%20builds%0Ahigh-quality%20preference%20pairs%20by%20using%20real%20videos%20as%20positives%20and%0Amodel-generated%20videos%20as%20negatives%2C%20eliminating%20the%20need%20for%20any%20external%0Aannotation.%20We%20further%20present%20Reg-DPO%2C%20which%20incorporates%20the%20SFT%20loss%20as%20a%0Aregularization%20term%20into%20the%20DPO%20loss%20to%20enhance%20training%20stability%20and%0Ageneration%20fidelity.%20Additionally%2C%20by%20combining%20the%20FSDP%20framework%20with%0Amultiple%20memory%20optimization%20techniques%2C%20our%20approach%20achieves%20nearly%20three%0Atimes%20higher%20training%20capacity%20than%20using%20FSDP%20alone.%20Extensive%20experiments%20on%0Aboth%20I2V%20and%20T2V%20tasks%20across%20multiple%20datasets%20demonstrate%20that%20our%20method%0Aconsistently%20outperforms%20existing%20approaches%2C%20delivering%20superior%20video%0Ageneration%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.01450v2&entry.124074799=Read"},
{"title": "Geometry-Aware Global Feature Aggregation for Real-Time Indirect\n  Illumination", "author": "Meng Gai and Guoping Wang and Sheng Li", "abstract": "  Real-time rendering with global illumination is crucial to afford the user\nrealistic experience in virtual environments. We present a learning-based\nestimator to predict diffuse indirect illumination in screen space, which then\nis combined with direct illumination to synthesize globally-illuminated high\ndynamic range (HDR) results. Our approach tackles the challenges of capturing\nlong-range/long-distance indirect illumination when employing neural networks\nand is generalized to handle complex lighting and scenarios.\n  From the neural network thinking of the solver to the rendering equation, we\npresent a novel network architecture to predict indirect illumination. Our\nnetwork is equipped with a modified attention mechanism that aggregates global\ninformation guided by spacial geometry features, as well as a monochromatic\ndesign that encodes each color channel individually.\n  We conducted extensive evaluations, and the experimental results demonstrate\nour superiority over previous learning-based techniques. Our approach excels at\nhandling complex lighting such as varying-colored lighting and environment\nlighting. It can successfully capture distant indirect illumination and\nsimulates the interreflections between textured surfaces well (i.e., color\nbleeding effects); it can also effectively handle new scenes that are not\npresent in the training dataset.\n", "link": "http://arxiv.org/abs/2508.08826v3", "date": "2025-11-05", "relevancy": 2.7837, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5678}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5545}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry-Aware%20Global%20Feature%20Aggregation%20for%20Real-Time%20Indirect%0A%20%20Illumination&body=Title%3A%20Geometry-Aware%20Global%20Feature%20Aggregation%20for%20Real-Time%20Indirect%0A%20%20Illumination%0AAuthor%3A%20Meng%20Gai%20and%20Guoping%20Wang%20and%20Sheng%20Li%0AAbstract%3A%20%20%20Real-time%20rendering%20with%20global%20illumination%20is%20crucial%20to%20afford%20the%20user%0Arealistic%20experience%20in%20virtual%20environments.%20We%20present%20a%20learning-based%0Aestimator%20to%20predict%20diffuse%20indirect%20illumination%20in%20screen%20space%2C%20which%20then%0Ais%20combined%20with%20direct%20illumination%20to%20synthesize%20globally-illuminated%20high%0Adynamic%20range%20%28HDR%29%20results.%20Our%20approach%20tackles%20the%20challenges%20of%20capturing%0Along-range/long-distance%20indirect%20illumination%20when%20employing%20neural%20networks%0Aand%20is%20generalized%20to%20handle%20complex%20lighting%20and%20scenarios.%0A%20%20From%20the%20neural%20network%20thinking%20of%20the%20solver%20to%20the%20rendering%20equation%2C%20we%0Apresent%20a%20novel%20network%20architecture%20to%20predict%20indirect%20illumination.%20Our%0Anetwork%20is%20equipped%20with%20a%20modified%20attention%20mechanism%20that%20aggregates%20global%0Ainformation%20guided%20by%20spacial%20geometry%20features%2C%20as%20well%20as%20a%20monochromatic%0Adesign%20that%20encodes%20each%20color%20channel%20individually.%0A%20%20We%20conducted%20extensive%20evaluations%2C%20and%20the%20experimental%20results%20demonstrate%0Aour%20superiority%20over%20previous%20learning-based%20techniques.%20Our%20approach%20excels%20at%0Ahandling%20complex%20lighting%20such%20as%20varying-colored%20lighting%20and%20environment%0Alighting.%20It%20can%20successfully%20capture%20distant%20indirect%20illumination%20and%0Asimulates%20the%20interreflections%20between%20textured%20surfaces%20well%20%28i.e.%2C%20color%0Ableeding%20effects%29%3B%20it%20can%20also%20effectively%20handle%20new%20scenes%20that%20are%20not%0Apresent%20in%20the%20training%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08826v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry-Aware%2520Global%2520Feature%2520Aggregation%2520for%2520Real-Time%2520Indirect%250A%2520%2520Illumination%26entry.906535625%3DMeng%2520Gai%2520and%2520Guoping%2520Wang%2520and%2520Sheng%2520Li%26entry.1292438233%3D%2520%2520Real-time%2520rendering%2520with%2520global%2520illumination%2520is%2520crucial%2520to%2520afford%2520the%2520user%250Arealistic%2520experience%2520in%2520virtual%2520environments.%2520We%2520present%2520a%2520learning-based%250Aestimator%2520to%2520predict%2520diffuse%2520indirect%2520illumination%2520in%2520screen%2520space%252C%2520which%2520then%250Ais%2520combined%2520with%2520direct%2520illumination%2520to%2520synthesize%2520globally-illuminated%2520high%250Adynamic%2520range%2520%2528HDR%2529%2520results.%2520Our%2520approach%2520tackles%2520the%2520challenges%2520of%2520capturing%250Along-range/long-distance%2520indirect%2520illumination%2520when%2520employing%2520neural%2520networks%250Aand%2520is%2520generalized%2520to%2520handle%2520complex%2520lighting%2520and%2520scenarios.%250A%2520%2520From%2520the%2520neural%2520network%2520thinking%2520of%2520the%2520solver%2520to%2520the%2520rendering%2520equation%252C%2520we%250Apresent%2520a%2520novel%2520network%2520architecture%2520to%2520predict%2520indirect%2520illumination.%2520Our%250Anetwork%2520is%2520equipped%2520with%2520a%2520modified%2520attention%2520mechanism%2520that%2520aggregates%2520global%250Ainformation%2520guided%2520by%2520spacial%2520geometry%2520features%252C%2520as%2520well%2520as%2520a%2520monochromatic%250Adesign%2520that%2520encodes%2520each%2520color%2520channel%2520individually.%250A%2520%2520We%2520conducted%2520extensive%2520evaluations%252C%2520and%2520the%2520experimental%2520results%2520demonstrate%250Aour%2520superiority%2520over%2520previous%2520learning-based%2520techniques.%2520Our%2520approach%2520excels%2520at%250Ahandling%2520complex%2520lighting%2520such%2520as%2520varying-colored%2520lighting%2520and%2520environment%250Alighting.%2520It%2520can%2520successfully%2520capture%2520distant%2520indirect%2520illumination%2520and%250Asimulates%2520the%2520interreflections%2520between%2520textured%2520surfaces%2520well%2520%2528i.e.%252C%2520color%250Ableeding%2520effects%2529%253B%2520it%2520can%2520also%2520effectively%2520handle%2520new%2520scenes%2520that%2520are%2520not%250Apresent%2520in%2520the%2520training%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08826v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry-Aware%20Global%20Feature%20Aggregation%20for%20Real-Time%20Indirect%0A%20%20Illumination&entry.906535625=Meng%20Gai%20and%20Guoping%20Wang%20and%20Sheng%20Li&entry.1292438233=%20%20Real-time%20rendering%20with%20global%20illumination%20is%20crucial%20to%20afford%20the%20user%0Arealistic%20experience%20in%20virtual%20environments.%20We%20present%20a%20learning-based%0Aestimator%20to%20predict%20diffuse%20indirect%20illumination%20in%20screen%20space%2C%20which%20then%0Ais%20combined%20with%20direct%20illumination%20to%20synthesize%20globally-illuminated%20high%0Adynamic%20range%20%28HDR%29%20results.%20Our%20approach%20tackles%20the%20challenges%20of%20capturing%0Along-range/long-distance%20indirect%20illumination%20when%20employing%20neural%20networks%0Aand%20is%20generalized%20to%20handle%20complex%20lighting%20and%20scenarios.%0A%20%20From%20the%20neural%20network%20thinking%20of%20the%20solver%20to%20the%20rendering%20equation%2C%20we%0Apresent%20a%20novel%20network%20architecture%20to%20predict%20indirect%20illumination.%20Our%0Anetwork%20is%20equipped%20with%20a%20modified%20attention%20mechanism%20that%20aggregates%20global%0Ainformation%20guided%20by%20spacial%20geometry%20features%2C%20as%20well%20as%20a%20monochromatic%0Adesign%20that%20encodes%20each%20color%20channel%20individually.%0A%20%20We%20conducted%20extensive%20evaluations%2C%20and%20the%20experimental%20results%20demonstrate%0Aour%20superiority%20over%20previous%20learning-based%20techniques.%20Our%20approach%20excels%20at%0Ahandling%20complex%20lighting%20such%20as%20varying-colored%20lighting%20and%20environment%0Alighting.%20It%20can%20successfully%20capture%20distant%20indirect%20illumination%20and%0Asimulates%20the%20interreflections%20between%20textured%20surfaces%20well%20%28i.e.%2C%20color%0Ableeding%20effects%29%3B%20it%20can%20also%20effectively%20handle%20new%20scenes%20that%20are%20not%0Apresent%20in%20the%20training%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08826v3&entry.124074799=Read"},
{"title": "TABLET: A Large-Scale Dataset for Robust Visual Table Understanding", "author": "I\u00f1igo Alonso and Imanol Miranda and Eneko Agirre and Mirella Lapata", "abstract": "  While table understanding increasingly relies on pixel-only settings where\ntables are processed as visual representations, current benchmarks\npredominantly use synthetic renderings that lack the complexity and visual\ndiversity of real-world tables. Additionally, existing visual table\nunderstanding (VTU) datasets offer fixed examples with single visualizations\nand pre-defined instructions, providing no access to underlying serialized data\nfor reformulation. We introduce TABLET, a large-scale VTU dataset with 4\nmillion examples across 20 tasks, grounded in 2 million unique tables where 88%\npreserve original visualizations. Each example includes paired image-HTML\nrepresentations, comprehensive metadata, and provenance information linking\nback to the source datasets. Fine-tuning vision-language models like\nQwen2.5-VL-7B on TABLET improves performance on seen and unseen VTU tasks while\nincreasing robustness on real-world table visualizations. By preserving\noriginal visualizations and maintaining example traceability in a unified\nlarge-scale collection, TABLET establishes a foundation for robust training and\nextensible evaluation of future VTU models.\n", "link": "http://arxiv.org/abs/2509.21205v2", "date": "2025-11-05", "relevancy": 2.6148, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5322}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5183}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TABLET%3A%20A%20Large-Scale%20Dataset%20for%20Robust%20Visual%20Table%20Understanding&body=Title%3A%20TABLET%3A%20A%20Large-Scale%20Dataset%20for%20Robust%20Visual%20Table%20Understanding%0AAuthor%3A%20I%C3%B1igo%20Alonso%20and%20Imanol%20Miranda%20and%20Eneko%20Agirre%20and%20Mirella%20Lapata%0AAbstract%3A%20%20%20While%20table%20understanding%20increasingly%20relies%20on%20pixel-only%20settings%20where%0Atables%20are%20processed%20as%20visual%20representations%2C%20current%20benchmarks%0Apredominantly%20use%20synthetic%20renderings%20that%20lack%20the%20complexity%20and%20visual%0Adiversity%20of%20real-world%20tables.%20Additionally%2C%20existing%20visual%20table%0Aunderstanding%20%28VTU%29%20datasets%20offer%20fixed%20examples%20with%20single%20visualizations%0Aand%20pre-defined%20instructions%2C%20providing%20no%20access%20to%20underlying%20serialized%20data%0Afor%20reformulation.%20We%20introduce%20TABLET%2C%20a%20large-scale%20VTU%20dataset%20with%204%0Amillion%20examples%20across%2020%20tasks%2C%20grounded%20in%202%20million%20unique%20tables%20where%2088%25%0Apreserve%20original%20visualizations.%20Each%20example%20includes%20paired%20image-HTML%0Arepresentations%2C%20comprehensive%20metadata%2C%20and%20provenance%20information%20linking%0Aback%20to%20the%20source%20datasets.%20Fine-tuning%20vision-language%20models%20like%0AQwen2.5-VL-7B%20on%20TABLET%20improves%20performance%20on%20seen%20and%20unseen%20VTU%20tasks%20while%0Aincreasing%20robustness%20on%20real-world%20table%20visualizations.%20By%20preserving%0Aoriginal%20visualizations%20and%20maintaining%20example%20traceability%20in%20a%20unified%0Alarge-scale%20collection%2C%20TABLET%20establishes%20a%20foundation%20for%20robust%20training%20and%0Aextensible%20evaluation%20of%20future%20VTU%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21205v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTABLET%253A%2520A%2520Large-Scale%2520Dataset%2520for%2520Robust%2520Visual%2520Table%2520Understanding%26entry.906535625%3DI%25C3%25B1igo%2520Alonso%2520and%2520Imanol%2520Miranda%2520and%2520Eneko%2520Agirre%2520and%2520Mirella%2520Lapata%26entry.1292438233%3D%2520%2520While%2520table%2520understanding%2520increasingly%2520relies%2520on%2520pixel-only%2520settings%2520where%250Atables%2520are%2520processed%2520as%2520visual%2520representations%252C%2520current%2520benchmarks%250Apredominantly%2520use%2520synthetic%2520renderings%2520that%2520lack%2520the%2520complexity%2520and%2520visual%250Adiversity%2520of%2520real-world%2520tables.%2520Additionally%252C%2520existing%2520visual%2520table%250Aunderstanding%2520%2528VTU%2529%2520datasets%2520offer%2520fixed%2520examples%2520with%2520single%2520visualizations%250Aand%2520pre-defined%2520instructions%252C%2520providing%2520no%2520access%2520to%2520underlying%2520serialized%2520data%250Afor%2520reformulation.%2520We%2520introduce%2520TABLET%252C%2520a%2520large-scale%2520VTU%2520dataset%2520with%25204%250Amillion%2520examples%2520across%252020%2520tasks%252C%2520grounded%2520in%25202%2520million%2520unique%2520tables%2520where%252088%2525%250Apreserve%2520original%2520visualizations.%2520Each%2520example%2520includes%2520paired%2520image-HTML%250Arepresentations%252C%2520comprehensive%2520metadata%252C%2520and%2520provenance%2520information%2520linking%250Aback%2520to%2520the%2520source%2520datasets.%2520Fine-tuning%2520vision-language%2520models%2520like%250AQwen2.5-VL-7B%2520on%2520TABLET%2520improves%2520performance%2520on%2520seen%2520and%2520unseen%2520VTU%2520tasks%2520while%250Aincreasing%2520robustness%2520on%2520real-world%2520table%2520visualizations.%2520By%2520preserving%250Aoriginal%2520visualizations%2520and%2520maintaining%2520example%2520traceability%2520in%2520a%2520unified%250Alarge-scale%2520collection%252C%2520TABLET%2520establishes%2520a%2520foundation%2520for%2520robust%2520training%2520and%250Aextensible%2520evaluation%2520of%2520future%2520VTU%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21205v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TABLET%3A%20A%20Large-Scale%20Dataset%20for%20Robust%20Visual%20Table%20Understanding&entry.906535625=I%C3%B1igo%20Alonso%20and%20Imanol%20Miranda%20and%20Eneko%20Agirre%20and%20Mirella%20Lapata&entry.1292438233=%20%20While%20table%20understanding%20increasingly%20relies%20on%20pixel-only%20settings%20where%0Atables%20are%20processed%20as%20visual%20representations%2C%20current%20benchmarks%0Apredominantly%20use%20synthetic%20renderings%20that%20lack%20the%20complexity%20and%20visual%0Adiversity%20of%20real-world%20tables.%20Additionally%2C%20existing%20visual%20table%0Aunderstanding%20%28VTU%29%20datasets%20offer%20fixed%20examples%20with%20single%20visualizations%0Aand%20pre-defined%20instructions%2C%20providing%20no%20access%20to%20underlying%20serialized%20data%0Afor%20reformulation.%20We%20introduce%20TABLET%2C%20a%20large-scale%20VTU%20dataset%20with%204%0Amillion%20examples%20across%2020%20tasks%2C%20grounded%20in%202%20million%20unique%20tables%20where%2088%25%0Apreserve%20original%20visualizations.%20Each%20example%20includes%20paired%20image-HTML%0Arepresentations%2C%20comprehensive%20metadata%2C%20and%20provenance%20information%20linking%0Aback%20to%20the%20source%20datasets.%20Fine-tuning%20vision-language%20models%20like%0AQwen2.5-VL-7B%20on%20TABLET%20improves%20performance%20on%20seen%20and%20unseen%20VTU%20tasks%20while%0Aincreasing%20robustness%20on%20real-world%20table%20visualizations.%20By%20preserving%0Aoriginal%20visualizations%20and%20maintaining%20example%20traceability%20in%20a%20unified%0Alarge-scale%20collection%2C%20TABLET%20establishes%20a%20foundation%20for%20robust%20training%20and%0Aextensible%20evaluation%20of%20future%20VTU%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21205v2&entry.124074799=Read"},
{"title": "Visualization Biases MLLM's Decision Making in Network Data Tasks", "author": "Timo Brand and Henry F\u00f6rster and Stephen G. Kobourov and Jacob Miller", "abstract": "  We evaluate how visualizations can influence the judgment of MLLMs about the\npresence or absence of bridges in a network. We show that the inclusion of\nvisualization improves confidence over a structured text-based input that could\ntheoretically be helpful for answering the question. On the other hand, we\nobserve that standard visualization techniques create a strong bias towards\naccepting or refuting the presence of a bridge -- independently of whether or\nnot a bridge actually exists in the network. While our results indicate that\nthe inclusion of visualization techniques can effectively influence the MLLM's\njudgment without compromising its self-reported confidence, they also imply\nthat practitioners must be careful of allowing users to include visualizations\nin generative AI applications so as to avoid undesired hallucinations.\n", "link": "http://arxiv.org/abs/2511.03617v1", "date": "2025-11-05", "relevancy": 2.5899, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5362}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5088}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visualization%20Biases%20MLLM%27s%20Decision%20Making%20in%20Network%20Data%20Tasks&body=Title%3A%20Visualization%20Biases%20MLLM%27s%20Decision%20Making%20in%20Network%20Data%20Tasks%0AAuthor%3A%20Timo%20Brand%20and%20Henry%20F%C3%B6rster%20and%20Stephen%20G.%20Kobourov%20and%20Jacob%20Miller%0AAbstract%3A%20%20%20We%20evaluate%20how%20visualizations%20can%20influence%20the%20judgment%20of%20MLLMs%20about%20the%0Apresence%20or%20absence%20of%20bridges%20in%20a%20network.%20We%20show%20that%20the%20inclusion%20of%0Avisualization%20improves%20confidence%20over%20a%20structured%20text-based%20input%20that%20could%0Atheoretically%20be%20helpful%20for%20answering%20the%20question.%20On%20the%20other%20hand%2C%20we%0Aobserve%20that%20standard%20visualization%20techniques%20create%20a%20strong%20bias%20towards%0Aaccepting%20or%20refuting%20the%20presence%20of%20a%20bridge%20--%20independently%20of%20whether%20or%0Anot%20a%20bridge%20actually%20exists%20in%20the%20network.%20While%20our%20results%20indicate%20that%0Athe%20inclusion%20of%20visualization%20techniques%20can%20effectively%20influence%20the%20MLLM%27s%0Ajudgment%20without%20compromising%20its%20self-reported%20confidence%2C%20they%20also%20imply%0Athat%20practitioners%20must%20be%20careful%20of%20allowing%20users%20to%20include%20visualizations%0Ain%20generative%20AI%20applications%20so%20as%20to%20avoid%20undesired%20hallucinations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03617v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisualization%2520Biases%2520MLLM%2527s%2520Decision%2520Making%2520in%2520Network%2520Data%2520Tasks%26entry.906535625%3DTimo%2520Brand%2520and%2520Henry%2520F%25C3%25B6rster%2520and%2520Stephen%2520G.%2520Kobourov%2520and%2520Jacob%2520Miller%26entry.1292438233%3D%2520%2520We%2520evaluate%2520how%2520visualizations%2520can%2520influence%2520the%2520judgment%2520of%2520MLLMs%2520about%2520the%250Apresence%2520or%2520absence%2520of%2520bridges%2520in%2520a%2520network.%2520We%2520show%2520that%2520the%2520inclusion%2520of%250Avisualization%2520improves%2520confidence%2520over%2520a%2520structured%2520text-based%2520input%2520that%2520could%250Atheoretically%2520be%2520helpful%2520for%2520answering%2520the%2520question.%2520On%2520the%2520other%2520hand%252C%2520we%250Aobserve%2520that%2520standard%2520visualization%2520techniques%2520create%2520a%2520strong%2520bias%2520towards%250Aaccepting%2520or%2520refuting%2520the%2520presence%2520of%2520a%2520bridge%2520--%2520independently%2520of%2520whether%2520or%250Anot%2520a%2520bridge%2520actually%2520exists%2520in%2520the%2520network.%2520While%2520our%2520results%2520indicate%2520that%250Athe%2520inclusion%2520of%2520visualization%2520techniques%2520can%2520effectively%2520influence%2520the%2520MLLM%2527s%250Ajudgment%2520without%2520compromising%2520its%2520self-reported%2520confidence%252C%2520they%2520also%2520imply%250Athat%2520practitioners%2520must%2520be%2520careful%2520of%2520allowing%2520users%2520to%2520include%2520visualizations%250Ain%2520generative%2520AI%2520applications%2520so%2520as%2520to%2520avoid%2520undesired%2520hallucinations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03617v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visualization%20Biases%20MLLM%27s%20Decision%20Making%20in%20Network%20Data%20Tasks&entry.906535625=Timo%20Brand%20and%20Henry%20F%C3%B6rster%20and%20Stephen%20G.%20Kobourov%20and%20Jacob%20Miller&entry.1292438233=%20%20We%20evaluate%20how%20visualizations%20can%20influence%20the%20judgment%20of%20MLLMs%20about%20the%0Apresence%20or%20absence%20of%20bridges%20in%20a%20network.%20We%20show%20that%20the%20inclusion%20of%0Avisualization%20improves%20confidence%20over%20a%20structured%20text-based%20input%20that%20could%0Atheoretically%20be%20helpful%20for%20answering%20the%20question.%20On%20the%20other%20hand%2C%20we%0Aobserve%20that%20standard%20visualization%20techniques%20create%20a%20strong%20bias%20towards%0Aaccepting%20or%20refuting%20the%20presence%20of%20a%20bridge%20--%20independently%20of%20whether%20or%0Anot%20a%20bridge%20actually%20exists%20in%20the%20network.%20While%20our%20results%20indicate%20that%0Athe%20inclusion%20of%20visualization%20techniques%20can%20effectively%20influence%20the%20MLLM%27s%0Ajudgment%20without%20compromising%20its%20self-reported%20confidence%2C%20they%20also%20imply%0Athat%20practitioners%20must%20be%20careful%20of%20allowing%20users%20to%20include%20visualizations%0Ain%20generative%20AI%20applications%20so%20as%20to%20avoid%20undesired%20hallucinations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03617v1&entry.124074799=Read"},
{"title": "PLUTO-4: Frontier Pathology Foundation Models", "author": "Harshith Padigela and Shima Nofallah and Atchuth Naveen Chilaparasetti and Ryun Han and Andrew Walker and Judy Shen and Chintan Shah and Blake Martin and Aashish Sood and Elliot Miller and Ben Glass and Andy Beck and Harsha Pokkalla and Syed Ashar Javed", "abstract": "  Foundation models trained on large-scale pathology image corpora have\ndemonstrated strong transfer capabilities across diverse histopathology tasks.\nBuilding on this progress, we introduce PLUTO-4, our next generation of\npathology foundation models that extend the Pathology-Universal Transformer\n(PLUTO) to frontier scale. We share two complementary Vision Transformer\narchitectures in the PLUTO-4 family: a compact and efficient PLUTO-4S model\noptimized for multi-scale deployment using a FlexiViT setup with 2D-RoPE\nembeddings, and a frontier-scale PLUTO-4G model trained with a single patch\nsize to maximize representation capacity and stability. Both models are\npretrained using a self-supervised objective derived from DINOv2 on a large\nmulti-institutional corpus containing 551,164 WSIs from 137,144 patients across\nover 50 institutions, spanning over 60 disease types and over 100 stains.\nComprehensive evaluation across public and internal benchmarks demonstrates\nthat PLUTO-4 achieves state-of-the-art performance on tasks requiring varying\nspatial and biological context, including patch-level classification,\nsegmentation, and slide-level diagnosis. The compact PLUTO-4S provides\nhigh-throughput and robust performance for practical deployment, while PLUTO-4G\nestablishes new performance frontiers across multiple pathology benchmarks,\nincluding an 11% improvement in dermatopathology diagnosis. These diverse\nimprovements underscore PLUTO-4's potential to transform real-world\napplications as a backbone for translational research and diagnostic use cases.\n", "link": "http://arxiv.org/abs/2511.02826v2", "date": "2025-11-05", "relevancy": 2.5823, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5216}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5216}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PLUTO-4%3A%20Frontier%20Pathology%20Foundation%20Models&body=Title%3A%20PLUTO-4%3A%20Frontier%20Pathology%20Foundation%20Models%0AAuthor%3A%20Harshith%20Padigela%20and%20Shima%20Nofallah%20and%20Atchuth%20Naveen%20Chilaparasetti%20and%20Ryun%20Han%20and%20Andrew%20Walker%20and%20Judy%20Shen%20and%20Chintan%20Shah%20and%20Blake%20Martin%20and%20Aashish%20Sood%20and%20Elliot%20Miller%20and%20Ben%20Glass%20and%20Andy%20Beck%20and%20Harsha%20Pokkalla%20and%20Syed%20Ashar%20Javed%0AAbstract%3A%20%20%20Foundation%20models%20trained%20on%20large-scale%20pathology%20image%20corpora%20have%0Ademonstrated%20strong%20transfer%20capabilities%20across%20diverse%20histopathology%20tasks.%0ABuilding%20on%20this%20progress%2C%20we%20introduce%20PLUTO-4%2C%20our%20next%20generation%20of%0Apathology%20foundation%20models%20that%20extend%20the%20Pathology-Universal%20Transformer%0A%28PLUTO%29%20to%20frontier%20scale.%20We%20share%20two%20complementary%20Vision%20Transformer%0Aarchitectures%20in%20the%20PLUTO-4%20family%3A%20a%20compact%20and%20efficient%20PLUTO-4S%20model%0Aoptimized%20for%20multi-scale%20deployment%20using%20a%20FlexiViT%20setup%20with%202D-RoPE%0Aembeddings%2C%20and%20a%20frontier-scale%20PLUTO-4G%20model%20trained%20with%20a%20single%20patch%0Asize%20to%20maximize%20representation%20capacity%20and%20stability.%20Both%20models%20are%0Apretrained%20using%20a%20self-supervised%20objective%20derived%20from%20DINOv2%20on%20a%20large%0Amulti-institutional%20corpus%20containing%20551%2C164%20WSIs%20from%20137%2C144%20patients%20across%0Aover%2050%20institutions%2C%20spanning%20over%2060%20disease%20types%20and%20over%20100%20stains.%0AComprehensive%20evaluation%20across%20public%20and%20internal%20benchmarks%20demonstrates%0Athat%20PLUTO-4%20achieves%20state-of-the-art%20performance%20on%20tasks%20requiring%20varying%0Aspatial%20and%20biological%20context%2C%20including%20patch-level%20classification%2C%0Asegmentation%2C%20and%20slide-level%20diagnosis.%20The%20compact%20PLUTO-4S%20provides%0Ahigh-throughput%20and%20robust%20performance%20for%20practical%20deployment%2C%20while%20PLUTO-4G%0Aestablishes%20new%20performance%20frontiers%20across%20multiple%20pathology%20benchmarks%2C%0Aincluding%20an%2011%25%20improvement%20in%20dermatopathology%20diagnosis.%20These%20diverse%0Aimprovements%20underscore%20PLUTO-4%27s%20potential%20to%20transform%20real-world%0Aapplications%20as%20a%20backbone%20for%20translational%20research%20and%20diagnostic%20use%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.02826v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPLUTO-4%253A%2520Frontier%2520Pathology%2520Foundation%2520Models%26entry.906535625%3DHarshith%2520Padigela%2520and%2520Shima%2520Nofallah%2520and%2520Atchuth%2520Naveen%2520Chilaparasetti%2520and%2520Ryun%2520Han%2520and%2520Andrew%2520Walker%2520and%2520Judy%2520Shen%2520and%2520Chintan%2520Shah%2520and%2520Blake%2520Martin%2520and%2520Aashish%2520Sood%2520and%2520Elliot%2520Miller%2520and%2520Ben%2520Glass%2520and%2520Andy%2520Beck%2520and%2520Harsha%2520Pokkalla%2520and%2520Syed%2520Ashar%2520Javed%26entry.1292438233%3D%2520%2520Foundation%2520models%2520trained%2520on%2520large-scale%2520pathology%2520image%2520corpora%2520have%250Ademonstrated%2520strong%2520transfer%2520capabilities%2520across%2520diverse%2520histopathology%2520tasks.%250ABuilding%2520on%2520this%2520progress%252C%2520we%2520introduce%2520PLUTO-4%252C%2520our%2520next%2520generation%2520of%250Apathology%2520foundation%2520models%2520that%2520extend%2520the%2520Pathology-Universal%2520Transformer%250A%2528PLUTO%2529%2520to%2520frontier%2520scale.%2520We%2520share%2520two%2520complementary%2520Vision%2520Transformer%250Aarchitectures%2520in%2520the%2520PLUTO-4%2520family%253A%2520a%2520compact%2520and%2520efficient%2520PLUTO-4S%2520model%250Aoptimized%2520for%2520multi-scale%2520deployment%2520using%2520a%2520FlexiViT%2520setup%2520with%25202D-RoPE%250Aembeddings%252C%2520and%2520a%2520frontier-scale%2520PLUTO-4G%2520model%2520trained%2520with%2520a%2520single%2520patch%250Asize%2520to%2520maximize%2520representation%2520capacity%2520and%2520stability.%2520Both%2520models%2520are%250Apretrained%2520using%2520a%2520self-supervised%2520objective%2520derived%2520from%2520DINOv2%2520on%2520a%2520large%250Amulti-institutional%2520corpus%2520containing%2520551%252C164%2520WSIs%2520from%2520137%252C144%2520patients%2520across%250Aover%252050%2520institutions%252C%2520spanning%2520over%252060%2520disease%2520types%2520and%2520over%2520100%2520stains.%250AComprehensive%2520evaluation%2520across%2520public%2520and%2520internal%2520benchmarks%2520demonstrates%250Athat%2520PLUTO-4%2520achieves%2520state-of-the-art%2520performance%2520on%2520tasks%2520requiring%2520varying%250Aspatial%2520and%2520biological%2520context%252C%2520including%2520patch-level%2520classification%252C%250Asegmentation%252C%2520and%2520slide-level%2520diagnosis.%2520The%2520compact%2520PLUTO-4S%2520provides%250Ahigh-throughput%2520and%2520robust%2520performance%2520for%2520practical%2520deployment%252C%2520while%2520PLUTO-4G%250Aestablishes%2520new%2520performance%2520frontiers%2520across%2520multiple%2520pathology%2520benchmarks%252C%250Aincluding%2520an%252011%2525%2520improvement%2520in%2520dermatopathology%2520diagnosis.%2520These%2520diverse%250Aimprovements%2520underscore%2520PLUTO-4%2527s%2520potential%2520to%2520transform%2520real-world%250Aapplications%2520as%2520a%2520backbone%2520for%2520translational%2520research%2520and%2520diagnostic%2520use%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.02826v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PLUTO-4%3A%20Frontier%20Pathology%20Foundation%20Models&entry.906535625=Harshith%20Padigela%20and%20Shima%20Nofallah%20and%20Atchuth%20Naveen%20Chilaparasetti%20and%20Ryun%20Han%20and%20Andrew%20Walker%20and%20Judy%20Shen%20and%20Chintan%20Shah%20and%20Blake%20Martin%20and%20Aashish%20Sood%20and%20Elliot%20Miller%20and%20Ben%20Glass%20and%20Andy%20Beck%20and%20Harsha%20Pokkalla%20and%20Syed%20Ashar%20Javed&entry.1292438233=%20%20Foundation%20models%20trained%20on%20large-scale%20pathology%20image%20corpora%20have%0Ademonstrated%20strong%20transfer%20capabilities%20across%20diverse%20histopathology%20tasks.%0ABuilding%20on%20this%20progress%2C%20we%20introduce%20PLUTO-4%2C%20our%20next%20generation%20of%0Apathology%20foundation%20models%20that%20extend%20the%20Pathology-Universal%20Transformer%0A%28PLUTO%29%20to%20frontier%20scale.%20We%20share%20two%20complementary%20Vision%20Transformer%0Aarchitectures%20in%20the%20PLUTO-4%20family%3A%20a%20compact%20and%20efficient%20PLUTO-4S%20model%0Aoptimized%20for%20multi-scale%20deployment%20using%20a%20FlexiViT%20setup%20with%202D-RoPE%0Aembeddings%2C%20and%20a%20frontier-scale%20PLUTO-4G%20model%20trained%20with%20a%20single%20patch%0Asize%20to%20maximize%20representation%20capacity%20and%20stability.%20Both%20models%20are%0Apretrained%20using%20a%20self-supervised%20objective%20derived%20from%20DINOv2%20on%20a%20large%0Amulti-institutional%20corpus%20containing%20551%2C164%20WSIs%20from%20137%2C144%20patients%20across%0Aover%2050%20institutions%2C%20spanning%20over%2060%20disease%20types%20and%20over%20100%20stains.%0AComprehensive%20evaluation%20across%20public%20and%20internal%20benchmarks%20demonstrates%0Athat%20PLUTO-4%20achieves%20state-of-the-art%20performance%20on%20tasks%20requiring%20varying%0Aspatial%20and%20biological%20context%2C%20including%20patch-level%20classification%2C%0Asegmentation%2C%20and%20slide-level%20diagnosis.%20The%20compact%20PLUTO-4S%20provides%0Ahigh-throughput%20and%20robust%20performance%20for%20practical%20deployment%2C%20while%20PLUTO-4G%0Aestablishes%20new%20performance%20frontiers%20across%20multiple%20pathology%20benchmarks%2C%0Aincluding%20an%2011%25%20improvement%20in%20dermatopathology%20diagnosis.%20These%20diverse%0Aimprovements%20underscore%20PLUTO-4%27s%20potential%20to%20transform%20real-world%0Aapplications%20as%20a%20backbone%20for%20translational%20research%20and%20diagnostic%20use%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.02826v2&entry.124074799=Read"},
{"title": "Voost: A Unified and Scalable Diffusion Transformer for Bidirectional\n  Virtual Try-On and Try-Off", "author": "Seungyong Lee and Jeong-gi Kwak", "abstract": "  Virtual try-on aims to synthesize a realistic image of a person wearing a\ntarget garment, but accurately modeling garment-body correspondence remains a\npersistent challenge, especially under pose and appearance variation. In this\npaper, we propose Voost - a unified and scalable framework that jointly learns\nvirtual try-on and try-off with a single diffusion transformer. By modeling\nboth tasks jointly, Voost enables each garment-person pair to supervise both\ndirections and supports flexible conditioning over generation direction and\ngarment category, enhancing garment-body relational reasoning without\ntask-specific networks, auxiliary losses, or additional labels. In addition, we\nintroduce two inference-time techniques: attention temperature scaling for\nrobustness to resolution or mask variation, and self-corrective sampling that\nleverages bidirectional consistency between tasks. Extensive experiments\ndemonstrate that Voost achieves state-of-the-art results on both try-on and\ntry-off benchmarks, consistently outperforming strong baselines in alignment\naccuracy, visual fidelity, and generalization.\n", "link": "http://arxiv.org/abs/2508.04825v2", "date": "2025-11-05", "relevancy": 2.5803, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.66}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6358}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Voost%3A%20A%20Unified%20and%20Scalable%20Diffusion%20Transformer%20for%20Bidirectional%0A%20%20Virtual%20Try-On%20and%20Try-Off&body=Title%3A%20Voost%3A%20A%20Unified%20and%20Scalable%20Diffusion%20Transformer%20for%20Bidirectional%0A%20%20Virtual%20Try-On%20and%20Try-Off%0AAuthor%3A%20Seungyong%20Lee%20and%20Jeong-gi%20Kwak%0AAbstract%3A%20%20%20Virtual%20try-on%20aims%20to%20synthesize%20a%20realistic%20image%20of%20a%20person%20wearing%20a%0Atarget%20garment%2C%20but%20accurately%20modeling%20garment-body%20correspondence%20remains%20a%0Apersistent%20challenge%2C%20especially%20under%20pose%20and%20appearance%20variation.%20In%20this%0Apaper%2C%20we%20propose%20Voost%20-%20a%20unified%20and%20scalable%20framework%20that%20jointly%20learns%0Avirtual%20try-on%20and%20try-off%20with%20a%20single%20diffusion%20transformer.%20By%20modeling%0Aboth%20tasks%20jointly%2C%20Voost%20enables%20each%20garment-person%20pair%20to%20supervise%20both%0Adirections%20and%20supports%20flexible%20conditioning%20over%20generation%20direction%20and%0Agarment%20category%2C%20enhancing%20garment-body%20relational%20reasoning%20without%0Atask-specific%20networks%2C%20auxiliary%20losses%2C%20or%20additional%20labels.%20In%20addition%2C%20we%0Aintroduce%20two%20inference-time%20techniques%3A%20attention%20temperature%20scaling%20for%0Arobustness%20to%20resolution%20or%20mask%20variation%2C%20and%20self-corrective%20sampling%20that%0Aleverages%20bidirectional%20consistency%20between%20tasks.%20Extensive%20experiments%0Ademonstrate%20that%20Voost%20achieves%20state-of-the-art%20results%20on%20both%20try-on%20and%0Atry-off%20benchmarks%2C%20consistently%20outperforming%20strong%20baselines%20in%20alignment%0Aaccuracy%2C%20visual%20fidelity%2C%20and%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04825v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVoost%253A%2520A%2520Unified%2520and%2520Scalable%2520Diffusion%2520Transformer%2520for%2520Bidirectional%250A%2520%2520Virtual%2520Try-On%2520and%2520Try-Off%26entry.906535625%3DSeungyong%2520Lee%2520and%2520Jeong-gi%2520Kwak%26entry.1292438233%3D%2520%2520Virtual%2520try-on%2520aims%2520to%2520synthesize%2520a%2520realistic%2520image%2520of%2520a%2520person%2520wearing%2520a%250Atarget%2520garment%252C%2520but%2520accurately%2520modeling%2520garment-body%2520correspondence%2520remains%2520a%250Apersistent%2520challenge%252C%2520especially%2520under%2520pose%2520and%2520appearance%2520variation.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520Voost%2520-%2520a%2520unified%2520and%2520scalable%2520framework%2520that%2520jointly%2520learns%250Avirtual%2520try-on%2520and%2520try-off%2520with%2520a%2520single%2520diffusion%2520transformer.%2520By%2520modeling%250Aboth%2520tasks%2520jointly%252C%2520Voost%2520enables%2520each%2520garment-person%2520pair%2520to%2520supervise%2520both%250Adirections%2520and%2520supports%2520flexible%2520conditioning%2520over%2520generation%2520direction%2520and%250Agarment%2520category%252C%2520enhancing%2520garment-body%2520relational%2520reasoning%2520without%250Atask-specific%2520networks%252C%2520auxiliary%2520losses%252C%2520or%2520additional%2520labels.%2520In%2520addition%252C%2520we%250Aintroduce%2520two%2520inference-time%2520techniques%253A%2520attention%2520temperature%2520scaling%2520for%250Arobustness%2520to%2520resolution%2520or%2520mask%2520variation%252C%2520and%2520self-corrective%2520sampling%2520that%250Aleverages%2520bidirectional%2520consistency%2520between%2520tasks.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520Voost%2520achieves%2520state-of-the-art%2520results%2520on%2520both%2520try-on%2520and%250Atry-off%2520benchmarks%252C%2520consistently%2520outperforming%2520strong%2520baselines%2520in%2520alignment%250Aaccuracy%252C%2520visual%2520fidelity%252C%2520and%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04825v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Voost%3A%20A%20Unified%20and%20Scalable%20Diffusion%20Transformer%20for%20Bidirectional%0A%20%20Virtual%20Try-On%20and%20Try-Off&entry.906535625=Seungyong%20Lee%20and%20Jeong-gi%20Kwak&entry.1292438233=%20%20Virtual%20try-on%20aims%20to%20synthesize%20a%20realistic%20image%20of%20a%20person%20wearing%20a%0Atarget%20garment%2C%20but%20accurately%20modeling%20garment-body%20correspondence%20remains%20a%0Apersistent%20challenge%2C%20especially%20under%20pose%20and%20appearance%20variation.%20In%20this%0Apaper%2C%20we%20propose%20Voost%20-%20a%20unified%20and%20scalable%20framework%20that%20jointly%20learns%0Avirtual%20try-on%20and%20try-off%20with%20a%20single%20diffusion%20transformer.%20By%20modeling%0Aboth%20tasks%20jointly%2C%20Voost%20enables%20each%20garment-person%20pair%20to%20supervise%20both%0Adirections%20and%20supports%20flexible%20conditioning%20over%20generation%20direction%20and%0Agarment%20category%2C%20enhancing%20garment-body%20relational%20reasoning%20without%0Atask-specific%20networks%2C%20auxiliary%20losses%2C%20or%20additional%20labels.%20In%20addition%2C%20we%0Aintroduce%20two%20inference-time%20techniques%3A%20attention%20temperature%20scaling%20for%0Arobustness%20to%20resolution%20or%20mask%20variation%2C%20and%20self-corrective%20sampling%20that%0Aleverages%20bidirectional%20consistency%20between%20tasks.%20Extensive%20experiments%0Ademonstrate%20that%20Voost%20achieves%20state-of-the-art%20results%20on%20both%20try-on%20and%0Atry-off%20benchmarks%2C%20consistently%20outperforming%20strong%20baselines%20in%20alignment%0Aaccuracy%2C%20visual%20fidelity%2C%20and%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04825v2&entry.124074799=Read"},
{"title": "Going Beyond Expert Performance via Deep Implicit Imitation\n  Reinforcement Learning", "author": "Iason Chrysomallis and Georgios Chalkiadakis", "abstract": "  Imitation learning traditionally requires complete state-action\ndemonstrations from optimal or near-optimal experts. These requirements\nseverely limit practical applicability, as many real-world scenarios provide\nonly state observations without corresponding actions and expert performance is\noften suboptimal. In this paper we introduce a deep implicit imitation\nreinforcement learning framework that addresses both limitations by combining\ndeep reinforcement learning with implicit imitation learning from\nobservation-only datasets. Our main algorithm, Deep Implicit Imitation\nQ-Network (DIIQN), employs an action inference mechanism that reconstructs\nexpert actions through online exploration and integrates a dynamic confidence\nmechanism that adaptively balances expert-guided and self-directed learning.\nThis enables the agent to leverage expert guidance for accelerated training\nwhile maintaining capacity to surpass suboptimal expert performance. We further\nextend our framework with a Heterogeneous Actions DIIQN (HA-DIIQN) algorithm to\ntackle scenarios where expert and agent possess different action sets, a\nchallenge previously unaddressed in the implicit imitation learning literature.\nHA-DIIQN introduces an infeasibility detection mechanism and a bridging\nprocedure identifying alternative pathways connecting agent capabilities to\nexpert guidance when direct action replication is impossible. Our experimental\nresults demonstrate that DIIQN achieves up to 130% higher episodic returns\ncompared to standard DQN, while consistently outperforming existing implicit\nimitation methods that cannot exceed expert performance. In heterogeneous\naction settings, HA-DIIQN learns up to 64% faster than baselines, leveraging\nexpert datasets unusable by conventional approaches. Extensive parameter\nsensitivity analysis reveals the framework's robustness across varying dataset\nsizes and hyperparameter configurations.\n", "link": "http://arxiv.org/abs/2511.03616v1", "date": "2025-11-05", "relevancy": 2.5573, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.537}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5008}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Going%20Beyond%20Expert%20Performance%20via%20Deep%20Implicit%20Imitation%0A%20%20Reinforcement%20Learning&body=Title%3A%20Going%20Beyond%20Expert%20Performance%20via%20Deep%20Implicit%20Imitation%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Iason%20Chrysomallis%20and%20Georgios%20Chalkiadakis%0AAbstract%3A%20%20%20Imitation%20learning%20traditionally%20requires%20complete%20state-action%0Ademonstrations%20from%20optimal%20or%20near-optimal%20experts.%20These%20requirements%0Aseverely%20limit%20practical%20applicability%2C%20as%20many%20real-world%20scenarios%20provide%0Aonly%20state%20observations%20without%20corresponding%20actions%20and%20expert%20performance%20is%0Aoften%20suboptimal.%20In%20this%20paper%20we%20introduce%20a%20deep%20implicit%20imitation%0Areinforcement%20learning%20framework%20that%20addresses%20both%20limitations%20by%20combining%0Adeep%20reinforcement%20learning%20with%20implicit%20imitation%20learning%20from%0Aobservation-only%20datasets.%20Our%20main%20algorithm%2C%20Deep%20Implicit%20Imitation%0AQ-Network%20%28DIIQN%29%2C%20employs%20an%20action%20inference%20mechanism%20that%20reconstructs%0Aexpert%20actions%20through%20online%20exploration%20and%20integrates%20a%20dynamic%20confidence%0Amechanism%20that%20adaptively%20balances%20expert-guided%20and%20self-directed%20learning.%0AThis%20enables%20the%20agent%20to%20leverage%20expert%20guidance%20for%20accelerated%20training%0Awhile%20maintaining%20capacity%20to%20surpass%20suboptimal%20expert%20performance.%20We%20further%0Aextend%20our%20framework%20with%20a%20Heterogeneous%20Actions%20DIIQN%20%28HA-DIIQN%29%20algorithm%20to%0Atackle%20scenarios%20where%20expert%20and%20agent%20possess%20different%20action%20sets%2C%20a%0Achallenge%20previously%20unaddressed%20in%20the%20implicit%20imitation%20learning%20literature.%0AHA-DIIQN%20introduces%20an%20infeasibility%20detection%20mechanism%20and%20a%20bridging%0Aprocedure%20identifying%20alternative%20pathways%20connecting%20agent%20capabilities%20to%0Aexpert%20guidance%20when%20direct%20action%20replication%20is%20impossible.%20Our%20experimental%0Aresults%20demonstrate%20that%20DIIQN%20achieves%20up%20to%20130%25%20higher%20episodic%20returns%0Acompared%20to%20standard%20DQN%2C%20while%20consistently%20outperforming%20existing%20implicit%0Aimitation%20methods%20that%20cannot%20exceed%20expert%20performance.%20In%20heterogeneous%0Aaction%20settings%2C%20HA-DIIQN%20learns%20up%20to%2064%25%20faster%20than%20baselines%2C%20leveraging%0Aexpert%20datasets%20unusable%20by%20conventional%20approaches.%20Extensive%20parameter%0Asensitivity%20analysis%20reveals%20the%20framework%27s%20robustness%20across%20varying%20dataset%0Asizes%20and%20hyperparameter%20configurations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03616v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGoing%2520Beyond%2520Expert%2520Performance%2520via%2520Deep%2520Implicit%2520Imitation%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DIason%2520Chrysomallis%2520and%2520Georgios%2520Chalkiadakis%26entry.1292438233%3D%2520%2520Imitation%2520learning%2520traditionally%2520requires%2520complete%2520state-action%250Ademonstrations%2520from%2520optimal%2520or%2520near-optimal%2520experts.%2520These%2520requirements%250Aseverely%2520limit%2520practical%2520applicability%252C%2520as%2520many%2520real-world%2520scenarios%2520provide%250Aonly%2520state%2520observations%2520without%2520corresponding%2520actions%2520and%2520expert%2520performance%2520is%250Aoften%2520suboptimal.%2520In%2520this%2520paper%2520we%2520introduce%2520a%2520deep%2520implicit%2520imitation%250Areinforcement%2520learning%2520framework%2520that%2520addresses%2520both%2520limitations%2520by%2520combining%250Adeep%2520reinforcement%2520learning%2520with%2520implicit%2520imitation%2520learning%2520from%250Aobservation-only%2520datasets.%2520Our%2520main%2520algorithm%252C%2520Deep%2520Implicit%2520Imitation%250AQ-Network%2520%2528DIIQN%2529%252C%2520employs%2520an%2520action%2520inference%2520mechanism%2520that%2520reconstructs%250Aexpert%2520actions%2520through%2520online%2520exploration%2520and%2520integrates%2520a%2520dynamic%2520confidence%250Amechanism%2520that%2520adaptively%2520balances%2520expert-guided%2520and%2520self-directed%2520learning.%250AThis%2520enables%2520the%2520agent%2520to%2520leverage%2520expert%2520guidance%2520for%2520accelerated%2520training%250Awhile%2520maintaining%2520capacity%2520to%2520surpass%2520suboptimal%2520expert%2520performance.%2520We%2520further%250Aextend%2520our%2520framework%2520with%2520a%2520Heterogeneous%2520Actions%2520DIIQN%2520%2528HA-DIIQN%2529%2520algorithm%2520to%250Atackle%2520scenarios%2520where%2520expert%2520and%2520agent%2520possess%2520different%2520action%2520sets%252C%2520a%250Achallenge%2520previously%2520unaddressed%2520in%2520the%2520implicit%2520imitation%2520learning%2520literature.%250AHA-DIIQN%2520introduces%2520an%2520infeasibility%2520detection%2520mechanism%2520and%2520a%2520bridging%250Aprocedure%2520identifying%2520alternative%2520pathways%2520connecting%2520agent%2520capabilities%2520to%250Aexpert%2520guidance%2520when%2520direct%2520action%2520replication%2520is%2520impossible.%2520Our%2520experimental%250Aresults%2520demonstrate%2520that%2520DIIQN%2520achieves%2520up%2520to%2520130%2525%2520higher%2520episodic%2520returns%250Acompared%2520to%2520standard%2520DQN%252C%2520while%2520consistently%2520outperforming%2520existing%2520implicit%250Aimitation%2520methods%2520that%2520cannot%2520exceed%2520expert%2520performance.%2520In%2520heterogeneous%250Aaction%2520settings%252C%2520HA-DIIQN%2520learns%2520up%2520to%252064%2525%2520faster%2520than%2520baselines%252C%2520leveraging%250Aexpert%2520datasets%2520unusable%2520by%2520conventional%2520approaches.%2520Extensive%2520parameter%250Asensitivity%2520analysis%2520reveals%2520the%2520framework%2527s%2520robustness%2520across%2520varying%2520dataset%250Asizes%2520and%2520hyperparameter%2520configurations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03616v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Going%20Beyond%20Expert%20Performance%20via%20Deep%20Implicit%20Imitation%0A%20%20Reinforcement%20Learning&entry.906535625=Iason%20Chrysomallis%20and%20Georgios%20Chalkiadakis&entry.1292438233=%20%20Imitation%20learning%20traditionally%20requires%20complete%20state-action%0Ademonstrations%20from%20optimal%20or%20near-optimal%20experts.%20These%20requirements%0Aseverely%20limit%20practical%20applicability%2C%20as%20many%20real-world%20scenarios%20provide%0Aonly%20state%20observations%20without%20corresponding%20actions%20and%20expert%20performance%20is%0Aoften%20suboptimal.%20In%20this%20paper%20we%20introduce%20a%20deep%20implicit%20imitation%0Areinforcement%20learning%20framework%20that%20addresses%20both%20limitations%20by%20combining%0Adeep%20reinforcement%20learning%20with%20implicit%20imitation%20learning%20from%0Aobservation-only%20datasets.%20Our%20main%20algorithm%2C%20Deep%20Implicit%20Imitation%0AQ-Network%20%28DIIQN%29%2C%20employs%20an%20action%20inference%20mechanism%20that%20reconstructs%0Aexpert%20actions%20through%20online%20exploration%20and%20integrates%20a%20dynamic%20confidence%0Amechanism%20that%20adaptively%20balances%20expert-guided%20and%20self-directed%20learning.%0AThis%20enables%20the%20agent%20to%20leverage%20expert%20guidance%20for%20accelerated%20training%0Awhile%20maintaining%20capacity%20to%20surpass%20suboptimal%20expert%20performance.%20We%20further%0Aextend%20our%20framework%20with%20a%20Heterogeneous%20Actions%20DIIQN%20%28HA-DIIQN%29%20algorithm%20to%0Atackle%20scenarios%20where%20expert%20and%20agent%20possess%20different%20action%20sets%2C%20a%0Achallenge%20previously%20unaddressed%20in%20the%20implicit%20imitation%20learning%20literature.%0AHA-DIIQN%20introduces%20an%20infeasibility%20detection%20mechanism%20and%20a%20bridging%0Aprocedure%20identifying%20alternative%20pathways%20connecting%20agent%20capabilities%20to%0Aexpert%20guidance%20when%20direct%20action%20replication%20is%20impossible.%20Our%20experimental%0Aresults%20demonstrate%20that%20DIIQN%20achieves%20up%20to%20130%25%20higher%20episodic%20returns%0Acompared%20to%20standard%20DQN%2C%20while%20consistently%20outperforming%20existing%20implicit%0Aimitation%20methods%20that%20cannot%20exceed%20expert%20performance.%20In%20heterogeneous%0Aaction%20settings%2C%20HA-DIIQN%20learns%20up%20to%2064%25%20faster%20than%20baselines%2C%20leveraging%0Aexpert%20datasets%20unusable%20by%20conventional%20approaches.%20Extensive%20parameter%0Asensitivity%20analysis%20reveals%20the%20framework%27s%20robustness%20across%20varying%20dataset%0Asizes%20and%20hyperparameter%20configurations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03616v1&entry.124074799=Read"},
{"title": "OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single\n  Panoramic Camera", "author": "Hao Shi and Ze Wang and Shangwei Guo and Mengfei Duan and Song Wang and Teng Chen and Kailun Yang and Lin Wang and Kaiwei Wang", "abstract": "  Robust 3D semantic occupancy is crucial for legged/humanoid robots, yet most\nsemantic scene completion (SSC) systems target wheeled platforms with\nforward-facing sensors. We present OneOcc, a vision-only panoramic SSC\nframework designed for gait-introduced body jitter and 360{\\deg} continuity.\nOneOcc combines: (i) Dual-Projection fusion (DP-ER) to exploit the annular\npanorama and its equirectangular unfolding, preserving 360{\\deg} continuity and\ngrid alignment; (ii) Bi-Grid Voxelization (BGV) to reason in Cartesian and\ncylindrical-polar spaces, reducing discretization bias and sharpening\nfree/occupied boundaries; (iii) a lightweight decoder with Hierarchical AMoE-3D\nfor dynamic multi-scale fusion and better long-range/occlusion reasoning; and\n(iv) plug-and-play Gait Displacement Compensation (GDC) learning feature-level\nmotion correction without extra sensors. We also release two panoramic\noccupancy benchmarks: QuadOcc (real quadruped, first-person 360{\\deg}) and\nHuman360Occ (H3O) (CARLA human-ego 360{\\deg} with RGB, Depth, semantic\noccupancy; standardized within-/cross-city splits). OneOcc sets new\nstate-of-the-art (SOTA): on QuadOcc it beats strong vision baselines and\npopular LiDAR ones; on H3O it gains +3.83 mIoU (within-city) and +8.08\n(cross-city). Modules are lightweight, enabling deployable full-surround\nperception for legged/humanoid robots. Datasets and code will be publicly\navailable at https://github.com/MasterHow/OneOcc.\n", "link": "http://arxiv.org/abs/2511.03571v1", "date": "2025-11-05", "relevancy": 2.5192, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.674}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6271}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6149}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OneOcc%3A%20Semantic%20Occupancy%20Prediction%20for%20Legged%20Robots%20with%20a%20Single%0A%20%20Panoramic%20Camera&body=Title%3A%20OneOcc%3A%20Semantic%20Occupancy%20Prediction%20for%20Legged%20Robots%20with%20a%20Single%0A%20%20Panoramic%20Camera%0AAuthor%3A%20Hao%20Shi%20and%20Ze%20Wang%20and%20Shangwei%20Guo%20and%20Mengfei%20Duan%20and%20Song%20Wang%20and%20Teng%20Chen%20and%20Kailun%20Yang%20and%20Lin%20Wang%20and%20Kaiwei%20Wang%0AAbstract%3A%20%20%20Robust%203D%20semantic%20occupancy%20is%20crucial%20for%20legged/humanoid%20robots%2C%20yet%20most%0Asemantic%20scene%20completion%20%28SSC%29%20systems%20target%20wheeled%20platforms%20with%0Aforward-facing%20sensors.%20We%20present%20OneOcc%2C%20a%20vision-only%20panoramic%20SSC%0Aframework%20designed%20for%20gait-introduced%20body%20jitter%20and%20360%7B%5Cdeg%7D%20continuity.%0AOneOcc%20combines%3A%20%28i%29%20Dual-Projection%20fusion%20%28DP-ER%29%20to%20exploit%20the%20annular%0Apanorama%20and%20its%20equirectangular%20unfolding%2C%20preserving%20360%7B%5Cdeg%7D%20continuity%20and%0Agrid%20alignment%3B%20%28ii%29%20Bi-Grid%20Voxelization%20%28BGV%29%20to%20reason%20in%20Cartesian%20and%0Acylindrical-polar%20spaces%2C%20reducing%20discretization%20bias%20and%20sharpening%0Afree/occupied%20boundaries%3B%20%28iii%29%20a%20lightweight%20decoder%20with%20Hierarchical%20AMoE-3D%0Afor%20dynamic%20multi-scale%20fusion%20and%20better%20long-range/occlusion%20reasoning%3B%20and%0A%28iv%29%20plug-and-play%20Gait%20Displacement%20Compensation%20%28GDC%29%20learning%20feature-level%0Amotion%20correction%20without%20extra%20sensors.%20We%20also%20release%20two%20panoramic%0Aoccupancy%20benchmarks%3A%20QuadOcc%20%28real%20quadruped%2C%20first-person%20360%7B%5Cdeg%7D%29%20and%0AHuman360Occ%20%28H3O%29%20%28CARLA%20human-ego%20360%7B%5Cdeg%7D%20with%20RGB%2C%20Depth%2C%20semantic%0Aoccupancy%3B%20standardized%20within-/cross-city%20splits%29.%20OneOcc%20sets%20new%0Astate-of-the-art%20%28SOTA%29%3A%20on%20QuadOcc%20it%20beats%20strong%20vision%20baselines%20and%0Apopular%20LiDAR%20ones%3B%20on%20H3O%20it%20gains%20%2B3.83%20mIoU%20%28within-city%29%20and%20%2B8.08%0A%28cross-city%29.%20Modules%20are%20lightweight%2C%20enabling%20deployable%20full-surround%0Aperception%20for%20legged/humanoid%20robots.%20Datasets%20and%20code%20will%20be%20publicly%0Aavailable%20at%20https%3A//github.com/MasterHow/OneOcc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03571v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOneOcc%253A%2520Semantic%2520Occupancy%2520Prediction%2520for%2520Legged%2520Robots%2520with%2520a%2520Single%250A%2520%2520Panoramic%2520Camera%26entry.906535625%3DHao%2520Shi%2520and%2520Ze%2520Wang%2520and%2520Shangwei%2520Guo%2520and%2520Mengfei%2520Duan%2520and%2520Song%2520Wang%2520and%2520Teng%2520Chen%2520and%2520Kailun%2520Yang%2520and%2520Lin%2520Wang%2520and%2520Kaiwei%2520Wang%26entry.1292438233%3D%2520%2520Robust%25203D%2520semantic%2520occupancy%2520is%2520crucial%2520for%2520legged/humanoid%2520robots%252C%2520yet%2520most%250Asemantic%2520scene%2520completion%2520%2528SSC%2529%2520systems%2520target%2520wheeled%2520platforms%2520with%250Aforward-facing%2520sensors.%2520We%2520present%2520OneOcc%252C%2520a%2520vision-only%2520panoramic%2520SSC%250Aframework%2520designed%2520for%2520gait-introduced%2520body%2520jitter%2520and%2520360%257B%255Cdeg%257D%2520continuity.%250AOneOcc%2520combines%253A%2520%2528i%2529%2520Dual-Projection%2520fusion%2520%2528DP-ER%2529%2520to%2520exploit%2520the%2520annular%250Apanorama%2520and%2520its%2520equirectangular%2520unfolding%252C%2520preserving%2520360%257B%255Cdeg%257D%2520continuity%2520and%250Agrid%2520alignment%253B%2520%2528ii%2529%2520Bi-Grid%2520Voxelization%2520%2528BGV%2529%2520to%2520reason%2520in%2520Cartesian%2520and%250Acylindrical-polar%2520spaces%252C%2520reducing%2520discretization%2520bias%2520and%2520sharpening%250Afree/occupied%2520boundaries%253B%2520%2528iii%2529%2520a%2520lightweight%2520decoder%2520with%2520Hierarchical%2520AMoE-3D%250Afor%2520dynamic%2520multi-scale%2520fusion%2520and%2520better%2520long-range/occlusion%2520reasoning%253B%2520and%250A%2528iv%2529%2520plug-and-play%2520Gait%2520Displacement%2520Compensation%2520%2528GDC%2529%2520learning%2520feature-level%250Amotion%2520correction%2520without%2520extra%2520sensors.%2520We%2520also%2520release%2520two%2520panoramic%250Aoccupancy%2520benchmarks%253A%2520QuadOcc%2520%2528real%2520quadruped%252C%2520first-person%2520360%257B%255Cdeg%257D%2529%2520and%250AHuman360Occ%2520%2528H3O%2529%2520%2528CARLA%2520human-ego%2520360%257B%255Cdeg%257D%2520with%2520RGB%252C%2520Depth%252C%2520semantic%250Aoccupancy%253B%2520standardized%2520within-/cross-city%2520splits%2529.%2520OneOcc%2520sets%2520new%250Astate-of-the-art%2520%2528SOTA%2529%253A%2520on%2520QuadOcc%2520it%2520beats%2520strong%2520vision%2520baselines%2520and%250Apopular%2520LiDAR%2520ones%253B%2520on%2520H3O%2520it%2520gains%2520%252B3.83%2520mIoU%2520%2528within-city%2529%2520and%2520%252B8.08%250A%2528cross-city%2529.%2520Modules%2520are%2520lightweight%252C%2520enabling%2520deployable%2520full-surround%250Aperception%2520for%2520legged/humanoid%2520robots.%2520Datasets%2520and%2520code%2520will%2520be%2520publicly%250Aavailable%2520at%2520https%253A//github.com/MasterHow/OneOcc.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03571v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OneOcc%3A%20Semantic%20Occupancy%20Prediction%20for%20Legged%20Robots%20with%20a%20Single%0A%20%20Panoramic%20Camera&entry.906535625=Hao%20Shi%20and%20Ze%20Wang%20and%20Shangwei%20Guo%20and%20Mengfei%20Duan%20and%20Song%20Wang%20and%20Teng%20Chen%20and%20Kailun%20Yang%20and%20Lin%20Wang%20and%20Kaiwei%20Wang&entry.1292438233=%20%20Robust%203D%20semantic%20occupancy%20is%20crucial%20for%20legged/humanoid%20robots%2C%20yet%20most%0Asemantic%20scene%20completion%20%28SSC%29%20systems%20target%20wheeled%20platforms%20with%0Aforward-facing%20sensors.%20We%20present%20OneOcc%2C%20a%20vision-only%20panoramic%20SSC%0Aframework%20designed%20for%20gait-introduced%20body%20jitter%20and%20360%7B%5Cdeg%7D%20continuity.%0AOneOcc%20combines%3A%20%28i%29%20Dual-Projection%20fusion%20%28DP-ER%29%20to%20exploit%20the%20annular%0Apanorama%20and%20its%20equirectangular%20unfolding%2C%20preserving%20360%7B%5Cdeg%7D%20continuity%20and%0Agrid%20alignment%3B%20%28ii%29%20Bi-Grid%20Voxelization%20%28BGV%29%20to%20reason%20in%20Cartesian%20and%0Acylindrical-polar%20spaces%2C%20reducing%20discretization%20bias%20and%20sharpening%0Afree/occupied%20boundaries%3B%20%28iii%29%20a%20lightweight%20decoder%20with%20Hierarchical%20AMoE-3D%0Afor%20dynamic%20multi-scale%20fusion%20and%20better%20long-range/occlusion%20reasoning%3B%20and%0A%28iv%29%20plug-and-play%20Gait%20Displacement%20Compensation%20%28GDC%29%20learning%20feature-level%0Amotion%20correction%20without%20extra%20sensors.%20We%20also%20release%20two%20panoramic%0Aoccupancy%20benchmarks%3A%20QuadOcc%20%28real%20quadruped%2C%20first-person%20360%7B%5Cdeg%7D%29%20and%0AHuman360Occ%20%28H3O%29%20%28CARLA%20human-ego%20360%7B%5Cdeg%7D%20with%20RGB%2C%20Depth%2C%20semantic%0Aoccupancy%3B%20standardized%20within-/cross-city%20splits%29.%20OneOcc%20sets%20new%0Astate-of-the-art%20%28SOTA%29%3A%20on%20QuadOcc%20it%20beats%20strong%20vision%20baselines%20and%0Apopular%20LiDAR%20ones%3B%20on%20H3O%20it%20gains%20%2B3.83%20mIoU%20%28within-city%29%20and%20%2B8.08%0A%28cross-city%29.%20Modules%20are%20lightweight%2C%20enabling%20deployable%20full-surround%0Aperception%20for%20legged/humanoid%20robots.%20Datasets%20and%20code%20will%20be%20publicly%0Aavailable%20at%20https%3A//github.com/MasterHow/OneOcc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03571v1&entry.124074799=Read"},
{"title": "Generative View Stitching", "author": "Chonghyuk Song and Michal Stary and Boyuan Chen and George Kopanas and Vincent Sitzmann", "abstract": "  Autoregressive video diffusion models are capable of long rollouts that are\nstable and consistent with history, but they are unable to guide the current\ngeneration with conditioning from the future. In camera-guided video generation\nwith a predefined camera trajectory, this limitation leads to collisions with\nthe generated scene, after which autoregression quickly collapses. To address\nthis, we propose Generative View Stitching (GVS), which samples the entire\nsequence in parallel such that the generated scene is faithful to every part of\nthe predefined camera trajectory. Our main contribution is a sampling algorithm\nthat extends prior work on diffusion stitching for robot planning to video\ngeneration. While such stitching methods usually require a specially trained\nmodel, GVS is compatible with any off-the-shelf video model trained with\nDiffusion Forcing, a prevalent sequence diffusion framework that we show\nalready provides the affordances necessary for stitching. We then introduce\nOmni Guidance, a technique that enhances the temporal consistency in stitching\nby conditioning on both the past and future, and that enables our proposed\nloop-closing mechanism for delivering long-range coherence. Overall, GVS\nachieves camera-guided video generation that is stable, collision-free,\nframe-to-frame consistent, and closes loops for a variety of predefined camera\npaths, including Oscar Reutersv\\\"ard's Impossible Staircase. Results are best\nviewed as videos at https://andrewsonga.github.io/gvs.\n", "link": "http://arxiv.org/abs/2510.24718v2", "date": "2025-11-05", "relevancy": 2.5157, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6477}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6339}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6164}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20View%20Stitching&body=Title%3A%20Generative%20View%20Stitching%0AAuthor%3A%20Chonghyuk%20Song%20and%20Michal%20Stary%20and%20Boyuan%20Chen%20and%20George%20Kopanas%20and%20Vincent%20Sitzmann%0AAbstract%3A%20%20%20Autoregressive%20video%20diffusion%20models%20are%20capable%20of%20long%20rollouts%20that%20are%0Astable%20and%20consistent%20with%20history%2C%20but%20they%20are%20unable%20to%20guide%20the%20current%0Ageneration%20with%20conditioning%20from%20the%20future.%20In%20camera-guided%20video%20generation%0Awith%20a%20predefined%20camera%20trajectory%2C%20this%20limitation%20leads%20to%20collisions%20with%0Athe%20generated%20scene%2C%20after%20which%20autoregression%20quickly%20collapses.%20To%20address%0Athis%2C%20we%20propose%20Generative%20View%20Stitching%20%28GVS%29%2C%20which%20samples%20the%20entire%0Asequence%20in%20parallel%20such%20that%20the%20generated%20scene%20is%20faithful%20to%20every%20part%20of%0Athe%20predefined%20camera%20trajectory.%20Our%20main%20contribution%20is%20a%20sampling%20algorithm%0Athat%20extends%20prior%20work%20on%20diffusion%20stitching%20for%20robot%20planning%20to%20video%0Ageneration.%20While%20such%20stitching%20methods%20usually%20require%20a%20specially%20trained%0Amodel%2C%20GVS%20is%20compatible%20with%20any%20off-the-shelf%20video%20model%20trained%20with%0ADiffusion%20Forcing%2C%20a%20prevalent%20sequence%20diffusion%20framework%20that%20we%20show%0Aalready%20provides%20the%20affordances%20necessary%20for%20stitching.%20We%20then%20introduce%0AOmni%20Guidance%2C%20a%20technique%20that%20enhances%20the%20temporal%20consistency%20in%20stitching%0Aby%20conditioning%20on%20both%20the%20past%20and%20future%2C%20and%20that%20enables%20our%20proposed%0Aloop-closing%20mechanism%20for%20delivering%20long-range%20coherence.%20Overall%2C%20GVS%0Aachieves%20camera-guided%20video%20generation%20that%20is%20stable%2C%20collision-free%2C%0Aframe-to-frame%20consistent%2C%20and%20closes%20loops%20for%20a%20variety%20of%20predefined%20camera%0Apaths%2C%20including%20Oscar%20Reutersv%5C%22ard%27s%20Impossible%20Staircase.%20Results%20are%20best%0Aviewed%20as%20videos%20at%20https%3A//andrewsonga.github.io/gvs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.24718v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520View%2520Stitching%26entry.906535625%3DChonghyuk%2520Song%2520and%2520Michal%2520Stary%2520and%2520Boyuan%2520Chen%2520and%2520George%2520Kopanas%2520and%2520Vincent%2520Sitzmann%26entry.1292438233%3D%2520%2520Autoregressive%2520video%2520diffusion%2520models%2520are%2520capable%2520of%2520long%2520rollouts%2520that%2520are%250Astable%2520and%2520consistent%2520with%2520history%252C%2520but%2520they%2520are%2520unable%2520to%2520guide%2520the%2520current%250Ageneration%2520with%2520conditioning%2520from%2520the%2520future.%2520In%2520camera-guided%2520video%2520generation%250Awith%2520a%2520predefined%2520camera%2520trajectory%252C%2520this%2520limitation%2520leads%2520to%2520collisions%2520with%250Athe%2520generated%2520scene%252C%2520after%2520which%2520autoregression%2520quickly%2520collapses.%2520To%2520address%250Athis%252C%2520we%2520propose%2520Generative%2520View%2520Stitching%2520%2528GVS%2529%252C%2520which%2520samples%2520the%2520entire%250Asequence%2520in%2520parallel%2520such%2520that%2520the%2520generated%2520scene%2520is%2520faithful%2520to%2520every%2520part%2520of%250Athe%2520predefined%2520camera%2520trajectory.%2520Our%2520main%2520contribution%2520is%2520a%2520sampling%2520algorithm%250Athat%2520extends%2520prior%2520work%2520on%2520diffusion%2520stitching%2520for%2520robot%2520planning%2520to%2520video%250Ageneration.%2520While%2520such%2520stitching%2520methods%2520usually%2520require%2520a%2520specially%2520trained%250Amodel%252C%2520GVS%2520is%2520compatible%2520with%2520any%2520off-the-shelf%2520video%2520model%2520trained%2520with%250ADiffusion%2520Forcing%252C%2520a%2520prevalent%2520sequence%2520diffusion%2520framework%2520that%2520we%2520show%250Aalready%2520provides%2520the%2520affordances%2520necessary%2520for%2520stitching.%2520We%2520then%2520introduce%250AOmni%2520Guidance%252C%2520a%2520technique%2520that%2520enhances%2520the%2520temporal%2520consistency%2520in%2520stitching%250Aby%2520conditioning%2520on%2520both%2520the%2520past%2520and%2520future%252C%2520and%2520that%2520enables%2520our%2520proposed%250Aloop-closing%2520mechanism%2520for%2520delivering%2520long-range%2520coherence.%2520Overall%252C%2520GVS%250Aachieves%2520camera-guided%2520video%2520generation%2520that%2520is%2520stable%252C%2520collision-free%252C%250Aframe-to-frame%2520consistent%252C%2520and%2520closes%2520loops%2520for%2520a%2520variety%2520of%2520predefined%2520camera%250Apaths%252C%2520including%2520Oscar%2520Reutersv%255C%2522ard%2527s%2520Impossible%2520Staircase.%2520Results%2520are%2520best%250Aviewed%2520as%2520videos%2520at%2520https%253A//andrewsonga.github.io/gvs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.24718v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20View%20Stitching&entry.906535625=Chonghyuk%20Song%20and%20Michal%20Stary%20and%20Boyuan%20Chen%20and%20George%20Kopanas%20and%20Vincent%20Sitzmann&entry.1292438233=%20%20Autoregressive%20video%20diffusion%20models%20are%20capable%20of%20long%20rollouts%20that%20are%0Astable%20and%20consistent%20with%20history%2C%20but%20they%20are%20unable%20to%20guide%20the%20current%0Ageneration%20with%20conditioning%20from%20the%20future.%20In%20camera-guided%20video%20generation%0Awith%20a%20predefined%20camera%20trajectory%2C%20this%20limitation%20leads%20to%20collisions%20with%0Athe%20generated%20scene%2C%20after%20which%20autoregression%20quickly%20collapses.%20To%20address%0Athis%2C%20we%20propose%20Generative%20View%20Stitching%20%28GVS%29%2C%20which%20samples%20the%20entire%0Asequence%20in%20parallel%20such%20that%20the%20generated%20scene%20is%20faithful%20to%20every%20part%20of%0Athe%20predefined%20camera%20trajectory.%20Our%20main%20contribution%20is%20a%20sampling%20algorithm%0Athat%20extends%20prior%20work%20on%20diffusion%20stitching%20for%20robot%20planning%20to%20video%0Ageneration.%20While%20such%20stitching%20methods%20usually%20require%20a%20specially%20trained%0Amodel%2C%20GVS%20is%20compatible%20with%20any%20off-the-shelf%20video%20model%20trained%20with%0ADiffusion%20Forcing%2C%20a%20prevalent%20sequence%20diffusion%20framework%20that%20we%20show%0Aalready%20provides%20the%20affordances%20necessary%20for%20stitching.%20We%20then%20introduce%0AOmni%20Guidance%2C%20a%20technique%20that%20enhances%20the%20temporal%20consistency%20in%20stitching%0Aby%20conditioning%20on%20both%20the%20past%20and%20future%2C%20and%20that%20enables%20our%20proposed%0Aloop-closing%20mechanism%20for%20delivering%20long-range%20coherence.%20Overall%2C%20GVS%0Aachieves%20camera-guided%20video%20generation%20that%20is%20stable%2C%20collision-free%2C%0Aframe-to-frame%20consistent%2C%20and%20closes%20loops%20for%20a%20variety%20of%20predefined%20camera%0Apaths%2C%20including%20Oscar%20Reutersv%5C%22ard%27s%20Impossible%20Staircase.%20Results%20are%20best%0Aviewed%20as%20videos%20at%20https%3A//andrewsonga.github.io/gvs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.24718v2&entry.124074799=Read"},
{"title": "Disentanglement with Factor Quantized Variational Autoencoders", "author": "Gulcin Baykal and Melih Kandemir and Gozde Unal", "abstract": "  Disentangled representation learning aims to represent the underlying\ngenerative factors of a dataset in a latent representation independently of one\nanother. In our work, we propose a discrete variational autoencoder (VAE) based\nmodel where the ground truth information about the generative factors are not\nprovided to the model. We demonstrate the advantages of learning discrete\nrepresentations over learning continuous representations in facilitating\ndisentanglement. Furthermore, we propose incorporating an inductive bias into\nthe model to further enhance disentanglement. Precisely, we propose scalar\nquantization of the latent variables in a latent representation with scalar\nvalues from a global codebook, and we add a total correlation term to the\noptimization as an inductive bias. Our method called FactorQVAE combines\noptimization based disentanglement approaches with discrete representation\nlearning, and it outperforms the former disentanglement methods in terms of two\ndisentanglement metrics (DCI and InfoMEC) while improving the reconstruction\nperformance. Our code can be found at\nhttps://github.com/ituvisionlab/FactorQVAE.\n", "link": "http://arxiv.org/abs/2409.14851v3", "date": "2025-11-05", "relevancy": 2.4959, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5263}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.491}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentanglement%20with%20Factor%20Quantized%20Variational%20Autoencoders&body=Title%3A%20Disentanglement%20with%20Factor%20Quantized%20Variational%20Autoencoders%0AAuthor%3A%20Gulcin%20Baykal%20and%20Melih%20Kandemir%20and%20Gozde%20Unal%0AAbstract%3A%20%20%20Disentangled%20representation%20learning%20aims%20to%20represent%20the%20underlying%0Agenerative%20factors%20of%20a%20dataset%20in%20a%20latent%20representation%20independently%20of%20one%0Aanother.%20In%20our%20work%2C%20we%20propose%20a%20discrete%20variational%20autoencoder%20%28VAE%29%20based%0Amodel%20where%20the%20ground%20truth%20information%20about%20the%20generative%20factors%20are%20not%0Aprovided%20to%20the%20model.%20We%20demonstrate%20the%20advantages%20of%20learning%20discrete%0Arepresentations%20over%20learning%20continuous%20representations%20in%20facilitating%0Adisentanglement.%20Furthermore%2C%20we%20propose%20incorporating%20an%20inductive%20bias%20into%0Athe%20model%20to%20further%20enhance%20disentanglement.%20Precisely%2C%20we%20propose%20scalar%0Aquantization%20of%20the%20latent%20variables%20in%20a%20latent%20representation%20with%20scalar%0Avalues%20from%20a%20global%20codebook%2C%20and%20we%20add%20a%20total%20correlation%20term%20to%20the%0Aoptimization%20as%20an%20inductive%20bias.%20Our%20method%20called%20FactorQVAE%20combines%0Aoptimization%20based%20disentanglement%20approaches%20with%20discrete%20representation%0Alearning%2C%20and%20it%20outperforms%20the%20former%20disentanglement%20methods%20in%20terms%20of%20two%0Adisentanglement%20metrics%20%28DCI%20and%20InfoMEC%29%20while%20improving%20the%20reconstruction%0Aperformance.%20Our%20code%20can%20be%20found%20at%0Ahttps%3A//github.com/ituvisionlab/FactorQVAE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.14851v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentanglement%2520with%2520Factor%2520Quantized%2520Variational%2520Autoencoders%26entry.906535625%3DGulcin%2520Baykal%2520and%2520Melih%2520Kandemir%2520and%2520Gozde%2520Unal%26entry.1292438233%3D%2520%2520Disentangled%2520representation%2520learning%2520aims%2520to%2520represent%2520the%2520underlying%250Agenerative%2520factors%2520of%2520a%2520dataset%2520in%2520a%2520latent%2520representation%2520independently%2520of%2520one%250Aanother.%2520In%2520our%2520work%252C%2520we%2520propose%2520a%2520discrete%2520variational%2520autoencoder%2520%2528VAE%2529%2520based%250Amodel%2520where%2520the%2520ground%2520truth%2520information%2520about%2520the%2520generative%2520factors%2520are%2520not%250Aprovided%2520to%2520the%2520model.%2520We%2520demonstrate%2520the%2520advantages%2520of%2520learning%2520discrete%250Arepresentations%2520over%2520learning%2520continuous%2520representations%2520in%2520facilitating%250Adisentanglement.%2520Furthermore%252C%2520we%2520propose%2520incorporating%2520an%2520inductive%2520bias%2520into%250Athe%2520model%2520to%2520further%2520enhance%2520disentanglement.%2520Precisely%252C%2520we%2520propose%2520scalar%250Aquantization%2520of%2520the%2520latent%2520variables%2520in%2520a%2520latent%2520representation%2520with%2520scalar%250Avalues%2520from%2520a%2520global%2520codebook%252C%2520and%2520we%2520add%2520a%2520total%2520correlation%2520term%2520to%2520the%250Aoptimization%2520as%2520an%2520inductive%2520bias.%2520Our%2520method%2520called%2520FactorQVAE%2520combines%250Aoptimization%2520based%2520disentanglement%2520approaches%2520with%2520discrete%2520representation%250Alearning%252C%2520and%2520it%2520outperforms%2520the%2520former%2520disentanglement%2520methods%2520in%2520terms%2520of%2520two%250Adisentanglement%2520metrics%2520%2528DCI%2520and%2520InfoMEC%2529%2520while%2520improving%2520the%2520reconstruction%250Aperformance.%2520Our%2520code%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/ituvisionlab/FactorQVAE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.14851v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentanglement%20with%20Factor%20Quantized%20Variational%20Autoencoders&entry.906535625=Gulcin%20Baykal%20and%20Melih%20Kandemir%20and%20Gozde%20Unal&entry.1292438233=%20%20Disentangled%20representation%20learning%20aims%20to%20represent%20the%20underlying%0Agenerative%20factors%20of%20a%20dataset%20in%20a%20latent%20representation%20independently%20of%20one%0Aanother.%20In%20our%20work%2C%20we%20propose%20a%20discrete%20variational%20autoencoder%20%28VAE%29%20based%0Amodel%20where%20the%20ground%20truth%20information%20about%20the%20generative%20factors%20are%20not%0Aprovided%20to%20the%20model.%20We%20demonstrate%20the%20advantages%20of%20learning%20discrete%0Arepresentations%20over%20learning%20continuous%20representations%20in%20facilitating%0Adisentanglement.%20Furthermore%2C%20we%20propose%20incorporating%20an%20inductive%20bias%20into%0Athe%20model%20to%20further%20enhance%20disentanglement.%20Precisely%2C%20we%20propose%20scalar%0Aquantization%20of%20the%20latent%20variables%20in%20a%20latent%20representation%20with%20scalar%0Avalues%20from%20a%20global%20codebook%2C%20and%20we%20add%20a%20total%20correlation%20term%20to%20the%0Aoptimization%20as%20an%20inductive%20bias.%20Our%20method%20called%20FactorQVAE%20combines%0Aoptimization%20based%20disentanglement%20approaches%20with%20discrete%20representation%0Alearning%2C%20and%20it%20outperforms%20the%20former%20disentanglement%20methods%20in%20terms%20of%20two%0Adisentanglement%20metrics%20%28DCI%20and%20InfoMEC%29%20while%20improving%20the%20reconstruction%0Aperformance.%20Our%20code%20can%20be%20found%20at%0Ahttps%3A//github.com/ituvisionlab/FactorQVAE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.14851v3&entry.124074799=Read"},
{"title": "Matryoshka Pilot: Learning to Drive Black-Box LLMs with LLMs", "author": "Changhao Li and Yuchen Zhuang and Rushi Qiang and Haotian Sun and Hanjun Dai and Chao Zhang and Bo Dai", "abstract": "  Despite the impressive generative abilities of black-box large language\nmodels (LLMs), their inherent opacity hinders further advancements in\ncapabilities such as reasoning, planning, and personalization. Existing works\naim to enhance LLM capabilities via domain-specific adaptation, which require\nadditional training on accessible model parameters, an infeasible option for\nblack-box LLMs. To address this challenge, we introduce Matryoshka Pilot\n(M-Pilot), a lightweight white-box LLM controller that guides a large-scale\nblack-box LLM generator by decomposing complex tasks into a series of\nintermediate outputs. Specifically, we consider the black-box LLM as an\nenvironment, with M-Pilot serving as a policy to provide intermediate guidance\nthrough prompts for driving the black-box LLM. M-Pilot is trained to pivot the\noutputs of the black-box LLM aligning with preferences during iterative\ninteraction, which enables controllable multi-turn generation and\nself-improvement in optimizing intermediate guidance. Empirical evaluations on\ndiverse tasks demonstrate that our method effectively enhances the capabilities\nof black-box LLMs in complex, long-horizon tasks. Our code is publicly\navailable at: https://github.com/lichangh20/Matryoshka.\n", "link": "http://arxiv.org/abs/2410.20749v3", "date": "2025-11-05", "relevancy": 2.4901, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5035}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4977}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Matryoshka%20Pilot%3A%20Learning%20to%20Drive%20Black-Box%20LLMs%20with%20LLMs&body=Title%3A%20Matryoshka%20Pilot%3A%20Learning%20to%20Drive%20Black-Box%20LLMs%20with%20LLMs%0AAuthor%3A%20Changhao%20Li%20and%20Yuchen%20Zhuang%20and%20Rushi%20Qiang%20and%20Haotian%20Sun%20and%20Hanjun%20Dai%20and%20Chao%20Zhang%20and%20Bo%20Dai%0AAbstract%3A%20%20%20Despite%20the%20impressive%20generative%20abilities%20of%20black-box%20large%20language%0Amodels%20%28LLMs%29%2C%20their%20inherent%20opacity%20hinders%20further%20advancements%20in%0Acapabilities%20such%20as%20reasoning%2C%20planning%2C%20and%20personalization.%20Existing%20works%0Aaim%20to%20enhance%20LLM%20capabilities%20via%20domain-specific%20adaptation%2C%20which%20require%0Aadditional%20training%20on%20accessible%20model%20parameters%2C%20an%20infeasible%20option%20for%0Ablack-box%20LLMs.%20To%20address%20this%20challenge%2C%20we%20introduce%20Matryoshka%20Pilot%0A%28M-Pilot%29%2C%20a%20lightweight%20white-box%20LLM%20controller%20that%20guides%20a%20large-scale%0Ablack-box%20LLM%20generator%20by%20decomposing%20complex%20tasks%20into%20a%20series%20of%0Aintermediate%20outputs.%20Specifically%2C%20we%20consider%20the%20black-box%20LLM%20as%20an%0Aenvironment%2C%20with%20M-Pilot%20serving%20as%20a%20policy%20to%20provide%20intermediate%20guidance%0Athrough%20prompts%20for%20driving%20the%20black-box%20LLM.%20M-Pilot%20is%20trained%20to%20pivot%20the%0Aoutputs%20of%20the%20black-box%20LLM%20aligning%20with%20preferences%20during%20iterative%0Ainteraction%2C%20which%20enables%20controllable%20multi-turn%20generation%20and%0Aself-improvement%20in%20optimizing%20intermediate%20guidance.%20Empirical%20evaluations%20on%0Adiverse%20tasks%20demonstrate%20that%20our%20method%20effectively%20enhances%20the%20capabilities%0Aof%20black-box%20LLMs%20in%20complex%2C%20long-horizon%20tasks.%20Our%20code%20is%20publicly%0Aavailable%20at%3A%20https%3A//github.com/lichangh20/Matryoshka.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.20749v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatryoshka%2520Pilot%253A%2520Learning%2520to%2520Drive%2520Black-Box%2520LLMs%2520with%2520LLMs%26entry.906535625%3DChanghao%2520Li%2520and%2520Yuchen%2520Zhuang%2520and%2520Rushi%2520Qiang%2520and%2520Haotian%2520Sun%2520and%2520Hanjun%2520Dai%2520and%2520Chao%2520Zhang%2520and%2520Bo%2520Dai%26entry.1292438233%3D%2520%2520Despite%2520the%2520impressive%2520generative%2520abilities%2520of%2520black-box%2520large%2520language%250Amodels%2520%2528LLMs%2529%252C%2520their%2520inherent%2520opacity%2520hinders%2520further%2520advancements%2520in%250Acapabilities%2520such%2520as%2520reasoning%252C%2520planning%252C%2520and%2520personalization.%2520Existing%2520works%250Aaim%2520to%2520enhance%2520LLM%2520capabilities%2520via%2520domain-specific%2520adaptation%252C%2520which%2520require%250Aadditional%2520training%2520on%2520accessible%2520model%2520parameters%252C%2520an%2520infeasible%2520option%2520for%250Ablack-box%2520LLMs.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520Matryoshka%2520Pilot%250A%2528M-Pilot%2529%252C%2520a%2520lightweight%2520white-box%2520LLM%2520controller%2520that%2520guides%2520a%2520large-scale%250Ablack-box%2520LLM%2520generator%2520by%2520decomposing%2520complex%2520tasks%2520into%2520a%2520series%2520of%250Aintermediate%2520outputs.%2520Specifically%252C%2520we%2520consider%2520the%2520black-box%2520LLM%2520as%2520an%250Aenvironment%252C%2520with%2520M-Pilot%2520serving%2520as%2520a%2520policy%2520to%2520provide%2520intermediate%2520guidance%250Athrough%2520prompts%2520for%2520driving%2520the%2520black-box%2520LLM.%2520M-Pilot%2520is%2520trained%2520to%2520pivot%2520the%250Aoutputs%2520of%2520the%2520black-box%2520LLM%2520aligning%2520with%2520preferences%2520during%2520iterative%250Ainteraction%252C%2520which%2520enables%2520controllable%2520multi-turn%2520generation%2520and%250Aself-improvement%2520in%2520optimizing%2520intermediate%2520guidance.%2520Empirical%2520evaluations%2520on%250Adiverse%2520tasks%2520demonstrate%2520that%2520our%2520method%2520effectively%2520enhances%2520the%2520capabilities%250Aof%2520black-box%2520LLMs%2520in%2520complex%252C%2520long-horizon%2520tasks.%2520Our%2520code%2520is%2520publicly%250Aavailable%2520at%253A%2520https%253A//github.com/lichangh20/Matryoshka.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.20749v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Matryoshka%20Pilot%3A%20Learning%20to%20Drive%20Black-Box%20LLMs%20with%20LLMs&entry.906535625=Changhao%20Li%20and%20Yuchen%20Zhuang%20and%20Rushi%20Qiang%20and%20Haotian%20Sun%20and%20Hanjun%20Dai%20and%20Chao%20Zhang%20and%20Bo%20Dai&entry.1292438233=%20%20Despite%20the%20impressive%20generative%20abilities%20of%20black-box%20large%20language%0Amodels%20%28LLMs%29%2C%20their%20inherent%20opacity%20hinders%20further%20advancements%20in%0Acapabilities%20such%20as%20reasoning%2C%20planning%2C%20and%20personalization.%20Existing%20works%0Aaim%20to%20enhance%20LLM%20capabilities%20via%20domain-specific%20adaptation%2C%20which%20require%0Aadditional%20training%20on%20accessible%20model%20parameters%2C%20an%20infeasible%20option%20for%0Ablack-box%20LLMs.%20To%20address%20this%20challenge%2C%20we%20introduce%20Matryoshka%20Pilot%0A%28M-Pilot%29%2C%20a%20lightweight%20white-box%20LLM%20controller%20that%20guides%20a%20large-scale%0Ablack-box%20LLM%20generator%20by%20decomposing%20complex%20tasks%20into%20a%20series%20of%0Aintermediate%20outputs.%20Specifically%2C%20we%20consider%20the%20black-box%20LLM%20as%20an%0Aenvironment%2C%20with%20M-Pilot%20serving%20as%20a%20policy%20to%20provide%20intermediate%20guidance%0Athrough%20prompts%20for%20driving%20the%20black-box%20LLM.%20M-Pilot%20is%20trained%20to%20pivot%20the%0Aoutputs%20of%20the%20black-box%20LLM%20aligning%20with%20preferences%20during%20iterative%0Ainteraction%2C%20which%20enables%20controllable%20multi-turn%20generation%20and%0Aself-improvement%20in%20optimizing%20intermediate%20guidance.%20Empirical%20evaluations%20on%0Adiverse%20tasks%20demonstrate%20that%20our%20method%20effectively%20enhances%20the%20capabilities%0Aof%20black-box%20LLMs%20in%20complex%2C%20long-horizon%20tasks.%20Our%20code%20is%20publicly%0Aavailable%20at%3A%20https%3A//github.com/lichangh20/Matryoshka.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.20749v3&entry.124074799=Read"},
{"title": "Dense SAE Latents Are Features, Not Bugs", "author": "Xiaoqing Sun and Alessandro Stolfo and Joshua Engels and Ben Wu and Senthooran Rajamanoharan and Mrinmaya Sachan and Max Tegmark", "abstract": "  Sparse autoencoders (SAEs) are designed to extract interpretable features\nfrom language models by enforcing a sparsity constraint. Ideally, training an\nSAE would yield latents that are both sparse and semantically meaningful.\nHowever, many SAE latents activate frequently (i.e., are \\emph{dense}), raising\nconcerns that they may be undesirable artifacts of the training procedure. In\nthis work, we systematically investigate the geometry, function, and origin of\ndense latents and show that they are not only persistent but often reflect\nmeaningful model representations. We first demonstrate that dense latents tend\nto form antipodal pairs that reconstruct specific directions in the residual\nstream, and that ablating their subspace suppresses the emergence of new dense\nfeatures in retrained SAEs -- suggesting that high density features are an\nintrinsic property of the residual space. We then introduce a taxonomy of dense\nlatents, identifying classes tied to position tracking, context binding,\nentropy regulation, letter-specific output signals, part-of-speech, and\nprincipal component reconstruction. Finally, we analyze how these features\nevolve across layers, revealing a shift from structural features in early\nlayers, to semantic features in mid layers, and finally to output-oriented\nsignals in the last layers of the model. Our findings indicate that dense\nlatents serve functional roles in language model computation and should not be\ndismissed as training noise.\n", "link": "http://arxiv.org/abs/2506.15679v2", "date": "2025-11-05", "relevancy": 2.4288, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4917}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4917}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dense%20SAE%20Latents%20Are%20Features%2C%20Not%20Bugs&body=Title%3A%20Dense%20SAE%20Latents%20Are%20Features%2C%20Not%20Bugs%0AAuthor%3A%20Xiaoqing%20Sun%20and%20Alessandro%20Stolfo%20and%20Joshua%20Engels%20and%20Ben%20Wu%20and%20Senthooran%20Rajamanoharan%20and%20Mrinmaya%20Sachan%20and%20Max%20Tegmark%0AAbstract%3A%20%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20designed%20to%20extract%20interpretable%20features%0Afrom%20language%20models%20by%20enforcing%20a%20sparsity%20constraint.%20Ideally%2C%20training%20an%0ASAE%20would%20yield%20latents%20that%20are%20both%20sparse%20and%20semantically%20meaningful.%0AHowever%2C%20many%20SAE%20latents%20activate%20frequently%20%28i.e.%2C%20are%20%5Cemph%7Bdense%7D%29%2C%20raising%0Aconcerns%20that%20they%20may%20be%20undesirable%20artifacts%20of%20the%20training%20procedure.%20In%0Athis%20work%2C%20we%20systematically%20investigate%20the%20geometry%2C%20function%2C%20and%20origin%20of%0Adense%20latents%20and%20show%20that%20they%20are%20not%20only%20persistent%20but%20often%20reflect%0Ameaningful%20model%20representations.%20We%20first%20demonstrate%20that%20dense%20latents%20tend%0Ato%20form%20antipodal%20pairs%20that%20reconstruct%20specific%20directions%20in%20the%20residual%0Astream%2C%20and%20that%20ablating%20their%20subspace%20suppresses%20the%20emergence%20of%20new%20dense%0Afeatures%20in%20retrained%20SAEs%20--%20suggesting%20that%20high%20density%20features%20are%20an%0Aintrinsic%20property%20of%20the%20residual%20space.%20We%20then%20introduce%20a%20taxonomy%20of%20dense%0Alatents%2C%20identifying%20classes%20tied%20to%20position%20tracking%2C%20context%20binding%2C%0Aentropy%20regulation%2C%20letter-specific%20output%20signals%2C%20part-of-speech%2C%20and%0Aprincipal%20component%20reconstruction.%20Finally%2C%20we%20analyze%20how%20these%20features%0Aevolve%20across%20layers%2C%20revealing%20a%20shift%20from%20structural%20features%20in%20early%0Alayers%2C%20to%20semantic%20features%20in%20mid%20layers%2C%20and%20finally%20to%20output-oriented%0Asignals%20in%20the%20last%20layers%20of%20the%20model.%20Our%20findings%20indicate%20that%20dense%0Alatents%20serve%20functional%20roles%20in%20language%20model%20computation%20and%20should%20not%20be%0Adismissed%20as%20training%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15679v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDense%2520SAE%2520Latents%2520Are%2520Features%252C%2520Not%2520Bugs%26entry.906535625%3DXiaoqing%2520Sun%2520and%2520Alessandro%2520Stolfo%2520and%2520Joshua%2520Engels%2520and%2520Ben%2520Wu%2520and%2520Senthooran%2520Rajamanoharan%2520and%2520Mrinmaya%2520Sachan%2520and%2520Max%2520Tegmark%26entry.1292438233%3D%2520%2520Sparse%2520autoencoders%2520%2528SAEs%2529%2520are%2520designed%2520to%2520extract%2520interpretable%2520features%250Afrom%2520language%2520models%2520by%2520enforcing%2520a%2520sparsity%2520constraint.%2520Ideally%252C%2520training%2520an%250ASAE%2520would%2520yield%2520latents%2520that%2520are%2520both%2520sparse%2520and%2520semantically%2520meaningful.%250AHowever%252C%2520many%2520SAE%2520latents%2520activate%2520frequently%2520%2528i.e.%252C%2520are%2520%255Cemph%257Bdense%257D%2529%252C%2520raising%250Aconcerns%2520that%2520they%2520may%2520be%2520undesirable%2520artifacts%2520of%2520the%2520training%2520procedure.%2520In%250Athis%2520work%252C%2520we%2520systematically%2520investigate%2520the%2520geometry%252C%2520function%252C%2520and%2520origin%2520of%250Adense%2520latents%2520and%2520show%2520that%2520they%2520are%2520not%2520only%2520persistent%2520but%2520often%2520reflect%250Ameaningful%2520model%2520representations.%2520We%2520first%2520demonstrate%2520that%2520dense%2520latents%2520tend%250Ato%2520form%2520antipodal%2520pairs%2520that%2520reconstruct%2520specific%2520directions%2520in%2520the%2520residual%250Astream%252C%2520and%2520that%2520ablating%2520their%2520subspace%2520suppresses%2520the%2520emergence%2520of%2520new%2520dense%250Afeatures%2520in%2520retrained%2520SAEs%2520--%2520suggesting%2520that%2520high%2520density%2520features%2520are%2520an%250Aintrinsic%2520property%2520of%2520the%2520residual%2520space.%2520We%2520then%2520introduce%2520a%2520taxonomy%2520of%2520dense%250Alatents%252C%2520identifying%2520classes%2520tied%2520to%2520position%2520tracking%252C%2520context%2520binding%252C%250Aentropy%2520regulation%252C%2520letter-specific%2520output%2520signals%252C%2520part-of-speech%252C%2520and%250Aprincipal%2520component%2520reconstruction.%2520Finally%252C%2520we%2520analyze%2520how%2520these%2520features%250Aevolve%2520across%2520layers%252C%2520revealing%2520a%2520shift%2520from%2520structural%2520features%2520in%2520early%250Alayers%252C%2520to%2520semantic%2520features%2520in%2520mid%2520layers%252C%2520and%2520finally%2520to%2520output-oriented%250Asignals%2520in%2520the%2520last%2520layers%2520of%2520the%2520model.%2520Our%2520findings%2520indicate%2520that%2520dense%250Alatents%2520serve%2520functional%2520roles%2520in%2520language%2520model%2520computation%2520and%2520should%2520not%2520be%250Adismissed%2520as%2520training%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15679v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dense%20SAE%20Latents%20Are%20Features%2C%20Not%20Bugs&entry.906535625=Xiaoqing%20Sun%20and%20Alessandro%20Stolfo%20and%20Joshua%20Engels%20and%20Ben%20Wu%20and%20Senthooran%20Rajamanoharan%20and%20Mrinmaya%20Sachan%20and%20Max%20Tegmark&entry.1292438233=%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20designed%20to%20extract%20interpretable%20features%0Afrom%20language%20models%20by%20enforcing%20a%20sparsity%20constraint.%20Ideally%2C%20training%20an%0ASAE%20would%20yield%20latents%20that%20are%20both%20sparse%20and%20semantically%20meaningful.%0AHowever%2C%20many%20SAE%20latents%20activate%20frequently%20%28i.e.%2C%20are%20%5Cemph%7Bdense%7D%29%2C%20raising%0Aconcerns%20that%20they%20may%20be%20undesirable%20artifacts%20of%20the%20training%20procedure.%20In%0Athis%20work%2C%20we%20systematically%20investigate%20the%20geometry%2C%20function%2C%20and%20origin%20of%0Adense%20latents%20and%20show%20that%20they%20are%20not%20only%20persistent%20but%20often%20reflect%0Ameaningful%20model%20representations.%20We%20first%20demonstrate%20that%20dense%20latents%20tend%0Ato%20form%20antipodal%20pairs%20that%20reconstruct%20specific%20directions%20in%20the%20residual%0Astream%2C%20and%20that%20ablating%20their%20subspace%20suppresses%20the%20emergence%20of%20new%20dense%0Afeatures%20in%20retrained%20SAEs%20--%20suggesting%20that%20high%20density%20features%20are%20an%0Aintrinsic%20property%20of%20the%20residual%20space.%20We%20then%20introduce%20a%20taxonomy%20of%20dense%0Alatents%2C%20identifying%20classes%20tied%20to%20position%20tracking%2C%20context%20binding%2C%0Aentropy%20regulation%2C%20letter-specific%20output%20signals%2C%20part-of-speech%2C%20and%0Aprincipal%20component%20reconstruction.%20Finally%2C%20we%20analyze%20how%20these%20features%0Aevolve%20across%20layers%2C%20revealing%20a%20shift%20from%20structural%20features%20in%20early%0Alayers%2C%20to%20semantic%20features%20in%20mid%20layers%2C%20and%20finally%20to%20output-oriented%0Asignals%20in%20the%20last%20layers%20of%20the%20model.%20Our%20findings%20indicate%20that%20dense%0Alatents%20serve%20functional%20roles%20in%20language%20model%20computation%20and%20should%20not%20be%0Adismissed%20as%20training%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15679v2&entry.124074799=Read"},
{"title": "Graph Sampling for Scalable and Expressive Graph Neural Networks on\n  Homophilic Graphs", "author": "Haolin Li and Haoyu Wang and Luana Ruiz", "abstract": "  Graph Neural Networks (GNNs) excel in many graph machine learning tasks but\nface challenges when scaling to large networks. GNN transferability allows\ntraining on smaller graphs and applying the model to larger ones, but existing\nmethods often rely on random subsampling, leading to disconnected subgraphs and\nreduced model expressivity. We propose a novel graph sampling algorithm that\nleverages feature homophily to preserve graph structure. By minimizing the\ntrace of the data correlation matrix, our method better preserves the graph\nLaplacian trace -- a proxy for the graph connectivity -- than random sampling,\nwhile achieving lower complexity than spectral methods. Experiments on citation\nnetworks show improved performance in preserving Laplacian trace and GNN\ntransferability compared to random sampling.\n", "link": "http://arxiv.org/abs/2410.16593v5", "date": "2025-11-05", "relevancy": 2.4258, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4861}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4851}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Sampling%20for%20Scalable%20and%20Expressive%20Graph%20Neural%20Networks%20on%0A%20%20Homophilic%20Graphs&body=Title%3A%20Graph%20Sampling%20for%20Scalable%20and%20Expressive%20Graph%20Neural%20Networks%20on%0A%20%20Homophilic%20Graphs%0AAuthor%3A%20Haolin%20Li%20and%20Haoyu%20Wang%20and%20Luana%20Ruiz%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20excel%20in%20many%20graph%20machine%20learning%20tasks%20but%0Aface%20challenges%20when%20scaling%20to%20large%20networks.%20GNN%20transferability%20allows%0Atraining%20on%20smaller%20graphs%20and%20applying%20the%20model%20to%20larger%20ones%2C%20but%20existing%0Amethods%20often%20rely%20on%20random%20subsampling%2C%20leading%20to%20disconnected%20subgraphs%20and%0Areduced%20model%20expressivity.%20We%20propose%20a%20novel%20graph%20sampling%20algorithm%20that%0Aleverages%20feature%20homophily%20to%20preserve%20graph%20structure.%20By%20minimizing%20the%0Atrace%20of%20the%20data%20correlation%20matrix%2C%20our%20method%20better%20preserves%20the%20graph%0ALaplacian%20trace%20--%20a%20proxy%20for%20the%20graph%20connectivity%20--%20than%20random%20sampling%2C%0Awhile%20achieving%20lower%20complexity%20than%20spectral%20methods.%20Experiments%20on%20citation%0Anetworks%20show%20improved%20performance%20in%20preserving%20Laplacian%20trace%20and%20GNN%0Atransferability%20compared%20to%20random%20sampling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16593v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Sampling%2520for%2520Scalable%2520and%2520Expressive%2520Graph%2520Neural%2520Networks%2520on%250A%2520%2520Homophilic%2520Graphs%26entry.906535625%3DHaolin%2520Li%2520and%2520Haoyu%2520Wang%2520and%2520Luana%2520Ruiz%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520excel%2520in%2520many%2520graph%2520machine%2520learning%2520tasks%2520but%250Aface%2520challenges%2520when%2520scaling%2520to%2520large%2520networks.%2520GNN%2520transferability%2520allows%250Atraining%2520on%2520smaller%2520graphs%2520and%2520applying%2520the%2520model%2520to%2520larger%2520ones%252C%2520but%2520existing%250Amethods%2520often%2520rely%2520on%2520random%2520subsampling%252C%2520leading%2520to%2520disconnected%2520subgraphs%2520and%250Areduced%2520model%2520expressivity.%2520We%2520propose%2520a%2520novel%2520graph%2520sampling%2520algorithm%2520that%250Aleverages%2520feature%2520homophily%2520to%2520preserve%2520graph%2520structure.%2520By%2520minimizing%2520the%250Atrace%2520of%2520the%2520data%2520correlation%2520matrix%252C%2520our%2520method%2520better%2520preserves%2520the%2520graph%250ALaplacian%2520trace%2520--%2520a%2520proxy%2520for%2520the%2520graph%2520connectivity%2520--%2520than%2520random%2520sampling%252C%250Awhile%2520achieving%2520lower%2520complexity%2520than%2520spectral%2520methods.%2520Experiments%2520on%2520citation%250Anetworks%2520show%2520improved%2520performance%2520in%2520preserving%2520Laplacian%2520trace%2520and%2520GNN%250Atransferability%2520compared%2520to%2520random%2520sampling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16593v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Sampling%20for%20Scalable%20and%20Expressive%20Graph%20Neural%20Networks%20on%0A%20%20Homophilic%20Graphs&entry.906535625=Haolin%20Li%20and%20Haoyu%20Wang%20and%20Luana%20Ruiz&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20excel%20in%20many%20graph%20machine%20learning%20tasks%20but%0Aface%20challenges%20when%20scaling%20to%20large%20networks.%20GNN%20transferability%20allows%0Atraining%20on%20smaller%20graphs%20and%20applying%20the%20model%20to%20larger%20ones%2C%20but%20existing%0Amethods%20often%20rely%20on%20random%20subsampling%2C%20leading%20to%20disconnected%20subgraphs%20and%0Areduced%20model%20expressivity.%20We%20propose%20a%20novel%20graph%20sampling%20algorithm%20that%0Aleverages%20feature%20homophily%20to%20preserve%20graph%20structure.%20By%20minimizing%20the%0Atrace%20of%20the%20data%20correlation%20matrix%2C%20our%20method%20better%20preserves%20the%20graph%0ALaplacian%20trace%20--%20a%20proxy%20for%20the%20graph%20connectivity%20--%20than%20random%20sampling%2C%0Awhile%20achieving%20lower%20complexity%20than%20spectral%20methods.%20Experiments%20on%20citation%0Anetworks%20show%20improved%20performance%20in%20preserving%20Laplacian%20trace%20and%20GNN%0Atransferability%20compared%20to%20random%20sampling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16593v5&entry.124074799=Read"},
{"title": "Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist\n  Annotation Scheme for MapTask", "author": "Nan Li and Albert Gatt and Massimo Poesio", "abstract": "  Collaborative dialogue relies on participants incrementally establishing\ncommon ground, yet in asymmetric settings they may believe they agree while\nreferring to different entities. We introduce a perspectivist annotation scheme\nfor the HCRC MapTask corpus (Anderson et al., 1991) that separately captures\nspeaker and addressee grounded interpretations for each reference expression,\nenabling us to trace how understanding emerges, diverges, and repairs over\ntime. Using a scheme-constrained LLM annotation pipeline, we obtain 13k\nannotated reference expressions with reliability estimates and analyze the\nresulting understanding states. The results show that full misunderstandings\nare rare once lexical variants are unified, but multiplicity discrepancies\nsystematically induce divergences, revealing how apparent grounding can mask\nreferential misalignment. Our framework provides both a resource and an\nanalytic lens for studying grounded misunderstanding and for evaluating\n(V)LLMs' capacity to model perspective-dependent grounding in collaborative\ndialogue.\n", "link": "http://arxiv.org/abs/2511.03718v1", "date": "2025-11-05", "relevancy": 2.4247, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.492}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.492}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grounded%20Misunderstandings%20in%20Asymmetric%20Dialogue%3A%20A%20Perspectivist%0A%20%20Annotation%20Scheme%20for%20MapTask&body=Title%3A%20Grounded%20Misunderstandings%20in%20Asymmetric%20Dialogue%3A%20A%20Perspectivist%0A%20%20Annotation%20Scheme%20for%20MapTask%0AAuthor%3A%20Nan%20Li%20and%20Albert%20Gatt%20and%20Massimo%20Poesio%0AAbstract%3A%20%20%20Collaborative%20dialogue%20relies%20on%20participants%20incrementally%20establishing%0Acommon%20ground%2C%20yet%20in%20asymmetric%20settings%20they%20may%20believe%20they%20agree%20while%0Areferring%20to%20different%20entities.%20We%20introduce%20a%20perspectivist%20annotation%20scheme%0Afor%20the%20HCRC%20MapTask%20corpus%20%28Anderson%20et%20al.%2C%201991%29%20that%20separately%20captures%0Aspeaker%20and%20addressee%20grounded%20interpretations%20for%20each%20reference%20expression%2C%0Aenabling%20us%20to%20trace%20how%20understanding%20emerges%2C%20diverges%2C%20and%20repairs%20over%0Atime.%20Using%20a%20scheme-constrained%20LLM%20annotation%20pipeline%2C%20we%20obtain%2013k%0Aannotated%20reference%20expressions%20with%20reliability%20estimates%20and%20analyze%20the%0Aresulting%20understanding%20states.%20The%20results%20show%20that%20full%20misunderstandings%0Aare%20rare%20once%20lexical%20variants%20are%20unified%2C%20but%20multiplicity%20discrepancies%0Asystematically%20induce%20divergences%2C%20revealing%20how%20apparent%20grounding%20can%20mask%0Areferential%20misalignment.%20Our%20framework%20provides%20both%20a%20resource%20and%20an%0Aanalytic%20lens%20for%20studying%20grounded%20misunderstanding%20and%20for%20evaluating%0A%28V%29LLMs%27%20capacity%20to%20model%20perspective-dependent%20grounding%20in%20collaborative%0Adialogue.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03718v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrounded%2520Misunderstandings%2520in%2520Asymmetric%2520Dialogue%253A%2520A%2520Perspectivist%250A%2520%2520Annotation%2520Scheme%2520for%2520MapTask%26entry.906535625%3DNan%2520Li%2520and%2520Albert%2520Gatt%2520and%2520Massimo%2520Poesio%26entry.1292438233%3D%2520%2520Collaborative%2520dialogue%2520relies%2520on%2520participants%2520incrementally%2520establishing%250Acommon%2520ground%252C%2520yet%2520in%2520asymmetric%2520settings%2520they%2520may%2520believe%2520they%2520agree%2520while%250Areferring%2520to%2520different%2520entities.%2520We%2520introduce%2520a%2520perspectivist%2520annotation%2520scheme%250Afor%2520the%2520HCRC%2520MapTask%2520corpus%2520%2528Anderson%2520et%2520al.%252C%25201991%2529%2520that%2520separately%2520captures%250Aspeaker%2520and%2520addressee%2520grounded%2520interpretations%2520for%2520each%2520reference%2520expression%252C%250Aenabling%2520us%2520to%2520trace%2520how%2520understanding%2520emerges%252C%2520diverges%252C%2520and%2520repairs%2520over%250Atime.%2520Using%2520a%2520scheme-constrained%2520LLM%2520annotation%2520pipeline%252C%2520we%2520obtain%252013k%250Aannotated%2520reference%2520expressions%2520with%2520reliability%2520estimates%2520and%2520analyze%2520the%250Aresulting%2520understanding%2520states.%2520The%2520results%2520show%2520that%2520full%2520misunderstandings%250Aare%2520rare%2520once%2520lexical%2520variants%2520are%2520unified%252C%2520but%2520multiplicity%2520discrepancies%250Asystematically%2520induce%2520divergences%252C%2520revealing%2520how%2520apparent%2520grounding%2520can%2520mask%250Areferential%2520misalignment.%2520Our%2520framework%2520provides%2520both%2520a%2520resource%2520and%2520an%250Aanalytic%2520lens%2520for%2520studying%2520grounded%2520misunderstanding%2520and%2520for%2520evaluating%250A%2528V%2529LLMs%2527%2520capacity%2520to%2520model%2520perspective-dependent%2520grounding%2520in%2520collaborative%250Adialogue.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03718v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grounded%20Misunderstandings%20in%20Asymmetric%20Dialogue%3A%20A%20Perspectivist%0A%20%20Annotation%20Scheme%20for%20MapTask&entry.906535625=Nan%20Li%20and%20Albert%20Gatt%20and%20Massimo%20Poesio&entry.1292438233=%20%20Collaborative%20dialogue%20relies%20on%20participants%20incrementally%20establishing%0Acommon%20ground%2C%20yet%20in%20asymmetric%20settings%20they%20may%20believe%20they%20agree%20while%0Areferring%20to%20different%20entities.%20We%20introduce%20a%20perspectivist%20annotation%20scheme%0Afor%20the%20HCRC%20MapTask%20corpus%20%28Anderson%20et%20al.%2C%201991%29%20that%20separately%20captures%0Aspeaker%20and%20addressee%20grounded%20interpretations%20for%20each%20reference%20expression%2C%0Aenabling%20us%20to%20trace%20how%20understanding%20emerges%2C%20diverges%2C%20and%20repairs%20over%0Atime.%20Using%20a%20scheme-constrained%20LLM%20annotation%20pipeline%2C%20we%20obtain%2013k%0Aannotated%20reference%20expressions%20with%20reliability%20estimates%20and%20analyze%20the%0Aresulting%20understanding%20states.%20The%20results%20show%20that%20full%20misunderstandings%0Aare%20rare%20once%20lexical%20variants%20are%20unified%2C%20but%20multiplicity%20discrepancies%0Asystematically%20induce%20divergences%2C%20revealing%20how%20apparent%20grounding%20can%20mask%0Areferential%20misalignment.%20Our%20framework%20provides%20both%20a%20resource%20and%20an%0Aanalytic%20lens%20for%20studying%20grounded%20misunderstanding%20and%20for%20evaluating%0A%28V%29LLMs%27%20capacity%20to%20model%20perspective-dependent%20grounding%20in%20collaborative%0Adialogue.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03718v1&entry.124074799=Read"},
{"title": "Depth Matters: Multimodal RGB-D Perception for Robust Autonomous Agents", "author": "Mihaela-Larisa Clement and M\u00f3nika Farsang and Felix Resch and Mihai-Teodor Stanusoiu and Radu Grosu", "abstract": "  Autonomous agents that rely purely on perception to make real-time control\ndecisions require efficient and robust architectures. In this work, we\ndemonstrate that augmenting RGB input with depth information significantly\nenhances our agents' ability to predict steering commands compared to using RGB\nalone. We benchmark lightweight recurrent controllers that leverage the fused\nRGB-D features for sequential decision-making. To train our models, we collect\nhigh-quality data using a small-scale autonomous car controlled by an expert\ndriver via a physical steering wheel, capturing varying levels of steering\ndifficulty. Our models were successfully deployed on real hardware and\ninherently avoided dynamic and static obstacles, under out-of-distribution\nconditions. Specifically, our findings reveal that the early fusion of depth\ndata results in a highly robust controller, which remains effective even with\nframe drops and increased noise levels, without compromising the network's\nfocus on the task.\n", "link": "http://arxiv.org/abs/2503.16711v2", "date": "2025-11-05", "relevancy": 2.3877, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6341}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6028}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth%20Matters%3A%20Multimodal%20RGB-D%20Perception%20for%20Robust%20Autonomous%20Agents&body=Title%3A%20Depth%20Matters%3A%20Multimodal%20RGB-D%20Perception%20for%20Robust%20Autonomous%20Agents%0AAuthor%3A%20Mihaela-Larisa%20Clement%20and%20M%C3%B3nika%20Farsang%20and%20Felix%20Resch%20and%20Mihai-Teodor%20Stanusoiu%20and%20Radu%20Grosu%0AAbstract%3A%20%20%20Autonomous%20agents%20that%20rely%20purely%20on%20perception%20to%20make%20real-time%20control%0Adecisions%20require%20efficient%20and%20robust%20architectures.%20In%20this%20work%2C%20we%0Ademonstrate%20that%20augmenting%20RGB%20input%20with%20depth%20information%20significantly%0Aenhances%20our%20agents%27%20ability%20to%20predict%20steering%20commands%20compared%20to%20using%20RGB%0Aalone.%20We%20benchmark%20lightweight%20recurrent%20controllers%20that%20leverage%20the%20fused%0ARGB-D%20features%20for%20sequential%20decision-making.%20To%20train%20our%20models%2C%20we%20collect%0Ahigh-quality%20data%20using%20a%20small-scale%20autonomous%20car%20controlled%20by%20an%20expert%0Adriver%20via%20a%20physical%20steering%20wheel%2C%20capturing%20varying%20levels%20of%20steering%0Adifficulty.%20Our%20models%20were%20successfully%20deployed%20on%20real%20hardware%20and%0Ainherently%20avoided%20dynamic%20and%20static%20obstacles%2C%20under%20out-of-distribution%0Aconditions.%20Specifically%2C%20our%20findings%20reveal%20that%20the%20early%20fusion%20of%20depth%0Adata%20results%20in%20a%20highly%20robust%20controller%2C%20which%20remains%20effective%20even%20with%0Aframe%20drops%20and%20increased%20noise%20levels%2C%20without%20compromising%20the%20network%27s%0Afocus%20on%20the%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.16711v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth%2520Matters%253A%2520Multimodal%2520RGB-D%2520Perception%2520for%2520Robust%2520Autonomous%2520Agents%26entry.906535625%3DMihaela-Larisa%2520Clement%2520and%2520M%25C3%25B3nika%2520Farsang%2520and%2520Felix%2520Resch%2520and%2520Mihai-Teodor%2520Stanusoiu%2520and%2520Radu%2520Grosu%26entry.1292438233%3D%2520%2520Autonomous%2520agents%2520that%2520rely%2520purely%2520on%2520perception%2520to%2520make%2520real-time%2520control%250Adecisions%2520require%2520efficient%2520and%2520robust%2520architectures.%2520In%2520this%2520work%252C%2520we%250Ademonstrate%2520that%2520augmenting%2520RGB%2520input%2520with%2520depth%2520information%2520significantly%250Aenhances%2520our%2520agents%2527%2520ability%2520to%2520predict%2520steering%2520commands%2520compared%2520to%2520using%2520RGB%250Aalone.%2520We%2520benchmark%2520lightweight%2520recurrent%2520controllers%2520that%2520leverage%2520the%2520fused%250ARGB-D%2520features%2520for%2520sequential%2520decision-making.%2520To%2520train%2520our%2520models%252C%2520we%2520collect%250Ahigh-quality%2520data%2520using%2520a%2520small-scale%2520autonomous%2520car%2520controlled%2520by%2520an%2520expert%250Adriver%2520via%2520a%2520physical%2520steering%2520wheel%252C%2520capturing%2520varying%2520levels%2520of%2520steering%250Adifficulty.%2520Our%2520models%2520were%2520successfully%2520deployed%2520on%2520real%2520hardware%2520and%250Ainherently%2520avoided%2520dynamic%2520and%2520static%2520obstacles%252C%2520under%2520out-of-distribution%250Aconditions.%2520Specifically%252C%2520our%2520findings%2520reveal%2520that%2520the%2520early%2520fusion%2520of%2520depth%250Adata%2520results%2520in%2520a%2520highly%2520robust%2520controller%252C%2520which%2520remains%2520effective%2520even%2520with%250Aframe%2520drops%2520and%2520increased%2520noise%2520levels%252C%2520without%2520compromising%2520the%2520network%2527s%250Afocus%2520on%2520the%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.16711v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth%20Matters%3A%20Multimodal%20RGB-D%20Perception%20for%20Robust%20Autonomous%20Agents&entry.906535625=Mihaela-Larisa%20Clement%20and%20M%C3%B3nika%20Farsang%20and%20Felix%20Resch%20and%20Mihai-Teodor%20Stanusoiu%20and%20Radu%20Grosu&entry.1292438233=%20%20Autonomous%20agents%20that%20rely%20purely%20on%20perception%20to%20make%20real-time%20control%0Adecisions%20require%20efficient%20and%20robust%20architectures.%20In%20this%20work%2C%20we%0Ademonstrate%20that%20augmenting%20RGB%20input%20with%20depth%20information%20significantly%0Aenhances%20our%20agents%27%20ability%20to%20predict%20steering%20commands%20compared%20to%20using%20RGB%0Aalone.%20We%20benchmark%20lightweight%20recurrent%20controllers%20that%20leverage%20the%20fused%0ARGB-D%20features%20for%20sequential%20decision-making.%20To%20train%20our%20models%2C%20we%20collect%0Ahigh-quality%20data%20using%20a%20small-scale%20autonomous%20car%20controlled%20by%20an%20expert%0Adriver%20via%20a%20physical%20steering%20wheel%2C%20capturing%20varying%20levels%20of%20steering%0Adifficulty.%20Our%20models%20were%20successfully%20deployed%20on%20real%20hardware%20and%0Ainherently%20avoided%20dynamic%20and%20static%20obstacles%2C%20under%20out-of-distribution%0Aconditions.%20Specifically%2C%20our%20findings%20reveal%20that%20the%20early%20fusion%20of%20depth%0Adata%20results%20in%20a%20highly%20robust%20controller%2C%20which%20remains%20effective%20even%20with%0Aframe%20drops%20and%20increased%20noise%20levels%2C%20without%20compromising%20the%20network%27s%0Afocus%20on%20the%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.16711v2&entry.124074799=Read"},
{"title": "TabTune: A Unified Library for Inference and Fine-Tuning Tabular\n  Foundation Models", "author": "Aditya Tanna and Pratinav Seth and Mohamed Bouadi and Utsav Avaiya and Vinay Kumar Sankarapu", "abstract": "  Tabular foundation models represent a growing paradigm in structured data\nlearning, extending the benefits of large-scale pretraining to tabular domains.\nHowever, their adoption remains limited due to heterogeneous preprocessing\npipelines, fragmented APIs, inconsistent fine-tuning procedures, and the\nabsence of standardized evaluation for deployment-oriented metrics such as\ncalibration and fairness. We present TabTune, a unified library that\nstandardizes the complete workflow for tabular foundation models through a\nsingle interface. TabTune provides consistent access to seven state-of-the-art\nmodels supporting multiple adaptation strategies, including zero-shot\ninference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient\nfine-tuning (PEFT). The framework automates model-aware preprocessing, manages\narchitectural heterogeneity internally, and integrates evaluation modules for\nperformance, calibration, and fairness. Designed for extensibility and\nreproducibility, TabTune enables consistent benchmarking of adaptation\nstrategies of tabular foundation models.\n", "link": "http://arxiv.org/abs/2511.02802v2", "date": "2025-11-05", "relevancy": 2.386, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5091}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4613}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TabTune%3A%20A%20Unified%20Library%20for%20Inference%20and%20Fine-Tuning%20Tabular%0A%20%20Foundation%20Models&body=Title%3A%20TabTune%3A%20A%20Unified%20Library%20for%20Inference%20and%20Fine-Tuning%20Tabular%0A%20%20Foundation%20Models%0AAuthor%3A%20Aditya%20Tanna%20and%20Pratinav%20Seth%20and%20Mohamed%20Bouadi%20and%20Utsav%20Avaiya%20and%20Vinay%20Kumar%20Sankarapu%0AAbstract%3A%20%20%20Tabular%20foundation%20models%20represent%20a%20growing%20paradigm%20in%20structured%20data%0Alearning%2C%20extending%20the%20benefits%20of%20large-scale%20pretraining%20to%20tabular%20domains.%0AHowever%2C%20their%20adoption%20remains%20limited%20due%20to%20heterogeneous%20preprocessing%0Apipelines%2C%20fragmented%20APIs%2C%20inconsistent%20fine-tuning%20procedures%2C%20and%20the%0Aabsence%20of%20standardized%20evaluation%20for%20deployment-oriented%20metrics%20such%20as%0Acalibration%20and%20fairness.%20We%20present%20TabTune%2C%20a%20unified%20library%20that%0Astandardizes%20the%20complete%20workflow%20for%20tabular%20foundation%20models%20through%20a%0Asingle%20interface.%20TabTune%20provides%20consistent%20access%20to%20seven%20state-of-the-art%0Amodels%20supporting%20multiple%20adaptation%20strategies%2C%20including%20zero-shot%0Ainference%2C%20meta-learning%2C%20supervised%20fine-tuning%20%28SFT%29%2C%20and%20parameter-efficient%0Afine-tuning%20%28PEFT%29.%20The%20framework%20automates%20model-aware%20preprocessing%2C%20manages%0Aarchitectural%20heterogeneity%20internally%2C%20and%20integrates%20evaluation%20modules%20for%0Aperformance%2C%20calibration%2C%20and%20fairness.%20Designed%20for%20extensibility%20and%0Areproducibility%2C%20TabTune%20enables%20consistent%20benchmarking%20of%20adaptation%0Astrategies%20of%20tabular%20foundation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.02802v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTabTune%253A%2520A%2520Unified%2520Library%2520for%2520Inference%2520and%2520Fine-Tuning%2520Tabular%250A%2520%2520Foundation%2520Models%26entry.906535625%3DAditya%2520Tanna%2520and%2520Pratinav%2520Seth%2520and%2520Mohamed%2520Bouadi%2520and%2520Utsav%2520Avaiya%2520and%2520Vinay%2520Kumar%2520Sankarapu%26entry.1292438233%3D%2520%2520Tabular%2520foundation%2520models%2520represent%2520a%2520growing%2520paradigm%2520in%2520structured%2520data%250Alearning%252C%2520extending%2520the%2520benefits%2520of%2520large-scale%2520pretraining%2520to%2520tabular%2520domains.%250AHowever%252C%2520their%2520adoption%2520remains%2520limited%2520due%2520to%2520heterogeneous%2520preprocessing%250Apipelines%252C%2520fragmented%2520APIs%252C%2520inconsistent%2520fine-tuning%2520procedures%252C%2520and%2520the%250Aabsence%2520of%2520standardized%2520evaluation%2520for%2520deployment-oriented%2520metrics%2520such%2520as%250Acalibration%2520and%2520fairness.%2520We%2520present%2520TabTune%252C%2520a%2520unified%2520library%2520that%250Astandardizes%2520the%2520complete%2520workflow%2520for%2520tabular%2520foundation%2520models%2520through%2520a%250Asingle%2520interface.%2520TabTune%2520provides%2520consistent%2520access%2520to%2520seven%2520state-of-the-art%250Amodels%2520supporting%2520multiple%2520adaptation%2520strategies%252C%2520including%2520zero-shot%250Ainference%252C%2520meta-learning%252C%2520supervised%2520fine-tuning%2520%2528SFT%2529%252C%2520and%2520parameter-efficient%250Afine-tuning%2520%2528PEFT%2529.%2520The%2520framework%2520automates%2520model-aware%2520preprocessing%252C%2520manages%250Aarchitectural%2520heterogeneity%2520internally%252C%2520and%2520integrates%2520evaluation%2520modules%2520for%250Aperformance%252C%2520calibration%252C%2520and%2520fairness.%2520Designed%2520for%2520extensibility%2520and%250Areproducibility%252C%2520TabTune%2520enables%2520consistent%2520benchmarking%2520of%2520adaptation%250Astrategies%2520of%2520tabular%2520foundation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.02802v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TabTune%3A%20A%20Unified%20Library%20for%20Inference%20and%20Fine-Tuning%20Tabular%0A%20%20Foundation%20Models&entry.906535625=Aditya%20Tanna%20and%20Pratinav%20Seth%20and%20Mohamed%20Bouadi%20and%20Utsav%20Avaiya%20and%20Vinay%20Kumar%20Sankarapu&entry.1292438233=%20%20Tabular%20foundation%20models%20represent%20a%20growing%20paradigm%20in%20structured%20data%0Alearning%2C%20extending%20the%20benefits%20of%20large-scale%20pretraining%20to%20tabular%20domains.%0AHowever%2C%20their%20adoption%20remains%20limited%20due%20to%20heterogeneous%20preprocessing%0Apipelines%2C%20fragmented%20APIs%2C%20inconsistent%20fine-tuning%20procedures%2C%20and%20the%0Aabsence%20of%20standardized%20evaluation%20for%20deployment-oriented%20metrics%20such%20as%0Acalibration%20and%20fairness.%20We%20present%20TabTune%2C%20a%20unified%20library%20that%0Astandardizes%20the%20complete%20workflow%20for%20tabular%20foundation%20models%20through%20a%0Asingle%20interface.%20TabTune%20provides%20consistent%20access%20to%20seven%20state-of-the-art%0Amodels%20supporting%20multiple%20adaptation%20strategies%2C%20including%20zero-shot%0Ainference%2C%20meta-learning%2C%20supervised%20fine-tuning%20%28SFT%29%2C%20and%20parameter-efficient%0Afine-tuning%20%28PEFT%29.%20The%20framework%20automates%20model-aware%20preprocessing%2C%20manages%0Aarchitectural%20heterogeneity%20internally%2C%20and%20integrates%20evaluation%20modules%20for%0Aperformance%2C%20calibration%2C%20and%20fairness.%20Designed%20for%20extensibility%20and%0Areproducibility%2C%20TabTune%20enables%20consistent%20benchmarking%20of%20adaptation%0Astrategies%20of%20tabular%20foundation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.02802v2&entry.124074799=Read"},
{"title": "Text-guided Fine-Grained Video Anomaly Detection", "author": "Jihao Gu and Kun Li and He Wang and Kaan Ak\u015fit", "abstract": "  Video Anomaly Detection (VAD) aims to identify anomalous events within video\nsegments. In scenarios such as surveillance or industrial process monitoring,\nanomaly detection is of critical importance. While existing approaches are\nsemi-automated, requiring human assessment for anomaly detection, traditional\nVADs offer limited output as either normal or anomalous. We propose Text-guided\nFine-Grained Video Anomaly Detection (T-VAD), a framework built upon Large\nVision-Language Model (LVLM). T-VAD introduces an Anomaly Heatmap Decoder (AHD)\nthat performs pixel-wise visual-textual feature alignment to generate\nfine-grained anomaly heatmaps. Furthermore, we design a Region-aware Anomaly\nEncoder (RAE) that transforms the heatmaps into learnable textual embeddings,\nguiding the LVLM to accurately identify and localize anomalous events in\nvideos. This significantly enhances both the granularity and interactivity of\nanomaly detection. The proposed method achieving SOTA performance by\ndemonstrating 94.8% Area Under the Curve (AUC, specifically micro-AUC) and\n67.8%/76.7% accuracy in anomaly heatmaps (RBDC/TBDC) on the UBnormal dataset,\nand subjectively verified more preferable textual description on the\nShanghaiTech-based dataset (BLEU-4: 62.67 for targets, 88.84 for trajectories;\nYes/No accuracy: 97.67%), and on the UBnormal dataset (BLEU-4: 50.32 for\ntargets, 78.10 for trajectories; Yes/No accuracy: 89.73%).\n", "link": "http://arxiv.org/abs/2511.00524v2", "date": "2025-11-05", "relevancy": 2.3857, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6037}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5973}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-guided%20Fine-Grained%20Video%20Anomaly%20Detection&body=Title%3A%20Text-guided%20Fine-Grained%20Video%20Anomaly%20Detection%0AAuthor%3A%20Jihao%20Gu%20and%20Kun%20Li%20and%20He%20Wang%20and%20Kaan%20Ak%C5%9Fit%0AAbstract%3A%20%20%20Video%20Anomaly%20Detection%20%28VAD%29%20aims%20to%20identify%20anomalous%20events%20within%20video%0Asegments.%20In%20scenarios%20such%20as%20surveillance%20or%20industrial%20process%20monitoring%2C%0Aanomaly%20detection%20is%20of%20critical%20importance.%20While%20existing%20approaches%20are%0Asemi-automated%2C%20requiring%20human%20assessment%20for%20anomaly%20detection%2C%20traditional%0AVADs%20offer%20limited%20output%20as%20either%20normal%20or%20anomalous.%20We%20propose%20Text-guided%0AFine-Grained%20Video%20Anomaly%20Detection%20%28T-VAD%29%2C%20a%20framework%20built%20upon%20Large%0AVision-Language%20Model%20%28LVLM%29.%20T-VAD%20introduces%20an%20Anomaly%20Heatmap%20Decoder%20%28AHD%29%0Athat%20performs%20pixel-wise%20visual-textual%20feature%20alignment%20to%20generate%0Afine-grained%20anomaly%20heatmaps.%20Furthermore%2C%20we%20design%20a%20Region-aware%20Anomaly%0AEncoder%20%28RAE%29%20that%20transforms%20the%20heatmaps%20into%20learnable%20textual%20embeddings%2C%0Aguiding%20the%20LVLM%20to%20accurately%20identify%20and%20localize%20anomalous%20events%20in%0Avideos.%20This%20significantly%20enhances%20both%20the%20granularity%20and%20interactivity%20of%0Aanomaly%20detection.%20The%20proposed%20method%20achieving%20SOTA%20performance%20by%0Ademonstrating%2094.8%25%20Area%20Under%20the%20Curve%20%28AUC%2C%20specifically%20micro-AUC%29%20and%0A67.8%25/76.7%25%20accuracy%20in%20anomaly%20heatmaps%20%28RBDC/TBDC%29%20on%20the%20UBnormal%20dataset%2C%0Aand%20subjectively%20verified%20more%20preferable%20textual%20description%20on%20the%0AShanghaiTech-based%20dataset%20%28BLEU-4%3A%2062.67%20for%20targets%2C%2088.84%20for%20trajectories%3B%0AYes/No%20accuracy%3A%2097.67%25%29%2C%20and%20on%20the%20UBnormal%20dataset%20%28BLEU-4%3A%2050.32%20for%0Atargets%2C%2078.10%20for%20trajectories%3B%20Yes/No%20accuracy%3A%2089.73%25%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.00524v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-guided%2520Fine-Grained%2520Video%2520Anomaly%2520Detection%26entry.906535625%3DJihao%2520Gu%2520and%2520Kun%2520Li%2520and%2520He%2520Wang%2520and%2520Kaan%2520Ak%25C5%259Fit%26entry.1292438233%3D%2520%2520Video%2520Anomaly%2520Detection%2520%2528VAD%2529%2520aims%2520to%2520identify%2520anomalous%2520events%2520within%2520video%250Asegments.%2520In%2520scenarios%2520such%2520as%2520surveillance%2520or%2520industrial%2520process%2520monitoring%252C%250Aanomaly%2520detection%2520is%2520of%2520critical%2520importance.%2520While%2520existing%2520approaches%2520are%250Asemi-automated%252C%2520requiring%2520human%2520assessment%2520for%2520anomaly%2520detection%252C%2520traditional%250AVADs%2520offer%2520limited%2520output%2520as%2520either%2520normal%2520or%2520anomalous.%2520We%2520propose%2520Text-guided%250AFine-Grained%2520Video%2520Anomaly%2520Detection%2520%2528T-VAD%2529%252C%2520a%2520framework%2520built%2520upon%2520Large%250AVision-Language%2520Model%2520%2528LVLM%2529.%2520T-VAD%2520introduces%2520an%2520Anomaly%2520Heatmap%2520Decoder%2520%2528AHD%2529%250Athat%2520performs%2520pixel-wise%2520visual-textual%2520feature%2520alignment%2520to%2520generate%250Afine-grained%2520anomaly%2520heatmaps.%2520Furthermore%252C%2520we%2520design%2520a%2520Region-aware%2520Anomaly%250AEncoder%2520%2528RAE%2529%2520that%2520transforms%2520the%2520heatmaps%2520into%2520learnable%2520textual%2520embeddings%252C%250Aguiding%2520the%2520LVLM%2520to%2520accurately%2520identify%2520and%2520localize%2520anomalous%2520events%2520in%250Avideos.%2520This%2520significantly%2520enhances%2520both%2520the%2520granularity%2520and%2520interactivity%2520of%250Aanomaly%2520detection.%2520The%2520proposed%2520method%2520achieving%2520SOTA%2520performance%2520by%250Ademonstrating%252094.8%2525%2520Area%2520Under%2520the%2520Curve%2520%2528AUC%252C%2520specifically%2520micro-AUC%2529%2520and%250A67.8%2525/76.7%2525%2520accuracy%2520in%2520anomaly%2520heatmaps%2520%2528RBDC/TBDC%2529%2520on%2520the%2520UBnormal%2520dataset%252C%250Aand%2520subjectively%2520verified%2520more%2520preferable%2520textual%2520description%2520on%2520the%250AShanghaiTech-based%2520dataset%2520%2528BLEU-4%253A%252062.67%2520for%2520targets%252C%252088.84%2520for%2520trajectories%253B%250AYes/No%2520accuracy%253A%252097.67%2525%2529%252C%2520and%2520on%2520the%2520UBnormal%2520dataset%2520%2528BLEU-4%253A%252050.32%2520for%250Atargets%252C%252078.10%2520for%2520trajectories%253B%2520Yes/No%2520accuracy%253A%252089.73%2525%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.00524v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-guided%20Fine-Grained%20Video%20Anomaly%20Detection&entry.906535625=Jihao%20Gu%20and%20Kun%20Li%20and%20He%20Wang%20and%20Kaan%20Ak%C5%9Fit&entry.1292438233=%20%20Video%20Anomaly%20Detection%20%28VAD%29%20aims%20to%20identify%20anomalous%20events%20within%20video%0Asegments.%20In%20scenarios%20such%20as%20surveillance%20or%20industrial%20process%20monitoring%2C%0Aanomaly%20detection%20is%20of%20critical%20importance.%20While%20existing%20approaches%20are%0Asemi-automated%2C%20requiring%20human%20assessment%20for%20anomaly%20detection%2C%20traditional%0AVADs%20offer%20limited%20output%20as%20either%20normal%20or%20anomalous.%20We%20propose%20Text-guided%0AFine-Grained%20Video%20Anomaly%20Detection%20%28T-VAD%29%2C%20a%20framework%20built%20upon%20Large%0AVision-Language%20Model%20%28LVLM%29.%20T-VAD%20introduces%20an%20Anomaly%20Heatmap%20Decoder%20%28AHD%29%0Athat%20performs%20pixel-wise%20visual-textual%20feature%20alignment%20to%20generate%0Afine-grained%20anomaly%20heatmaps.%20Furthermore%2C%20we%20design%20a%20Region-aware%20Anomaly%0AEncoder%20%28RAE%29%20that%20transforms%20the%20heatmaps%20into%20learnable%20textual%20embeddings%2C%0Aguiding%20the%20LVLM%20to%20accurately%20identify%20and%20localize%20anomalous%20events%20in%0Avideos.%20This%20significantly%20enhances%20both%20the%20granularity%20and%20interactivity%20of%0Aanomaly%20detection.%20The%20proposed%20method%20achieving%20SOTA%20performance%20by%0Ademonstrating%2094.8%25%20Area%20Under%20the%20Curve%20%28AUC%2C%20specifically%20micro-AUC%29%20and%0A67.8%25/76.7%25%20accuracy%20in%20anomaly%20heatmaps%20%28RBDC/TBDC%29%20on%20the%20UBnormal%20dataset%2C%0Aand%20subjectively%20verified%20more%20preferable%20textual%20description%20on%20the%0AShanghaiTech-based%20dataset%20%28BLEU-4%3A%2062.67%20for%20targets%2C%2088.84%20for%20trajectories%3B%0AYes/No%20accuracy%3A%2097.67%25%29%2C%20and%20on%20the%20UBnormal%20dataset%20%28BLEU-4%3A%2050.32%20for%0Atargets%2C%2078.10%20for%20trajectories%3B%20Yes/No%20accuracy%3A%2089.73%25%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.00524v2&entry.124074799=Read"},
{"title": "Interpretable Tile-Based Classification of Paclitaxel Exposure", "author": "Sean Fletcher and Gabby Scott and Douglas Currie and Xin Zhang and Yuqi Song and Bruce MacLeod", "abstract": "  Medical image analysis is central to drug discovery and preclinical\nevaluation, where scalable, objective readouts can accelerate decision-making.\nWe address classification of paclitaxel (Taxol) exposure from phase-contrast\nmicroscopy of C6 glioma cells -- a task with subtle dose differences that\nchallenges full-image models. We propose a simple tiling-and-aggregation\npipeline that operates on local patches and combines tile outputs into an image\nlabel, achieving state-of-the-art accuracy on the benchmark dataset and\nimproving over the published baseline by around 20 percentage points, with\ntrends confirmed by cross-validation. To understand why tiling is effective, we\nfurther apply Grad-CAM and Score-CAM and attention analyses, which enhance\nmodel interpretability and point toward robustness-oriented directions for\nfuture medical image research. Code is released to facilitate reproduction and\nextension.\n", "link": "http://arxiv.org/abs/2510.23363v2", "date": "2025-11-05", "relevancy": 2.3757, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5149}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4624}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Tile-Based%20Classification%20of%20Paclitaxel%20Exposure&body=Title%3A%20Interpretable%20Tile-Based%20Classification%20of%20Paclitaxel%20Exposure%0AAuthor%3A%20Sean%20Fletcher%20and%20Gabby%20Scott%20and%20Douglas%20Currie%20and%20Xin%20Zhang%20and%20Yuqi%20Song%20and%20Bruce%20MacLeod%0AAbstract%3A%20%20%20Medical%20image%20analysis%20is%20central%20to%20drug%20discovery%20and%20preclinical%0Aevaluation%2C%20where%20scalable%2C%20objective%20readouts%20can%20accelerate%20decision-making.%0AWe%20address%20classification%20of%20paclitaxel%20%28Taxol%29%20exposure%20from%20phase-contrast%0Amicroscopy%20of%20C6%20glioma%20cells%20--%20a%20task%20with%20subtle%20dose%20differences%20that%0Achallenges%20full-image%20models.%20We%20propose%20a%20simple%20tiling-and-aggregation%0Apipeline%20that%20operates%20on%20local%20patches%20and%20combines%20tile%20outputs%20into%20an%20image%0Alabel%2C%20achieving%20state-of-the-art%20accuracy%20on%20the%20benchmark%20dataset%20and%0Aimproving%20over%20the%20published%20baseline%20by%20around%2020%20percentage%20points%2C%20with%0Atrends%20confirmed%20by%20cross-validation.%20To%20understand%20why%20tiling%20is%20effective%2C%20we%0Afurther%20apply%20Grad-CAM%20and%20Score-CAM%20and%20attention%20analyses%2C%20which%20enhance%0Amodel%20interpretability%20and%20point%20toward%20robustness-oriented%20directions%20for%0Afuture%20medical%20image%20research.%20Code%20is%20released%20to%20facilitate%20reproduction%20and%0Aextension.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.23363v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Tile-Based%2520Classification%2520of%2520Paclitaxel%2520Exposure%26entry.906535625%3DSean%2520Fletcher%2520and%2520Gabby%2520Scott%2520and%2520Douglas%2520Currie%2520and%2520Xin%2520Zhang%2520and%2520Yuqi%2520Song%2520and%2520Bruce%2520MacLeod%26entry.1292438233%3D%2520%2520Medical%2520image%2520analysis%2520is%2520central%2520to%2520drug%2520discovery%2520and%2520preclinical%250Aevaluation%252C%2520where%2520scalable%252C%2520objective%2520readouts%2520can%2520accelerate%2520decision-making.%250AWe%2520address%2520classification%2520of%2520paclitaxel%2520%2528Taxol%2529%2520exposure%2520from%2520phase-contrast%250Amicroscopy%2520of%2520C6%2520glioma%2520cells%2520--%2520a%2520task%2520with%2520subtle%2520dose%2520differences%2520that%250Achallenges%2520full-image%2520models.%2520We%2520propose%2520a%2520simple%2520tiling-and-aggregation%250Apipeline%2520that%2520operates%2520on%2520local%2520patches%2520and%2520combines%2520tile%2520outputs%2520into%2520an%2520image%250Alabel%252C%2520achieving%2520state-of-the-art%2520accuracy%2520on%2520the%2520benchmark%2520dataset%2520and%250Aimproving%2520over%2520the%2520published%2520baseline%2520by%2520around%252020%2520percentage%2520points%252C%2520with%250Atrends%2520confirmed%2520by%2520cross-validation.%2520To%2520understand%2520why%2520tiling%2520is%2520effective%252C%2520we%250Afurther%2520apply%2520Grad-CAM%2520and%2520Score-CAM%2520and%2520attention%2520analyses%252C%2520which%2520enhance%250Amodel%2520interpretability%2520and%2520point%2520toward%2520robustness-oriented%2520directions%2520for%250Afuture%2520medical%2520image%2520research.%2520Code%2520is%2520released%2520to%2520facilitate%2520reproduction%2520and%250Aextension.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.23363v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Tile-Based%20Classification%20of%20Paclitaxel%20Exposure&entry.906535625=Sean%20Fletcher%20and%20Gabby%20Scott%20and%20Douglas%20Currie%20and%20Xin%20Zhang%20and%20Yuqi%20Song%20and%20Bruce%20MacLeod&entry.1292438233=%20%20Medical%20image%20analysis%20is%20central%20to%20drug%20discovery%20and%20preclinical%0Aevaluation%2C%20where%20scalable%2C%20objective%20readouts%20can%20accelerate%20decision-making.%0AWe%20address%20classification%20of%20paclitaxel%20%28Taxol%29%20exposure%20from%20phase-contrast%0Amicroscopy%20of%20C6%20glioma%20cells%20--%20a%20task%20with%20subtle%20dose%20differences%20that%0Achallenges%20full-image%20models.%20We%20propose%20a%20simple%20tiling-and-aggregation%0Apipeline%20that%20operates%20on%20local%20patches%20and%20combines%20tile%20outputs%20into%20an%20image%0Alabel%2C%20achieving%20state-of-the-art%20accuracy%20on%20the%20benchmark%20dataset%20and%0Aimproving%20over%20the%20published%20baseline%20by%20around%2020%20percentage%20points%2C%20with%0Atrends%20confirmed%20by%20cross-validation.%20To%20understand%20why%20tiling%20is%20effective%2C%20we%0Afurther%20apply%20Grad-CAM%20and%20Score-CAM%20and%20attention%20analyses%2C%20which%20enhance%0Amodel%20interpretability%20and%20point%20toward%20robustness-oriented%20directions%20for%0Afuture%20medical%20image%20research.%20Code%20is%20released%20to%20facilitate%20reproduction%20and%0Aextension.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.23363v2&entry.124074799=Read"},
{"title": "Imitation Learning in the Deep Learning Era: A Novel Taxonomy and Recent\n  Advances", "author": "Iason Chrysomallis and Georgios Chalkiadakis", "abstract": "  Imitation learning (IL) enables agents to acquire skills by observing and\nreplicating the behavior of one or multiple experts. In recent years, advances\nin deep learning have significantly expanded the capabilities and scalability\nof imitation learning across a range of domains, where expert data can range\nfrom full state-action trajectories to partial observations or unlabeled\nsequences. Alongside this growth, novel approaches have emerged, with new\nmethodologies being developed to address longstanding challenges such as\ngeneralization, covariate shift, and demonstration quality. In this survey, we\nreview the latest advances in imitation learning research, highlighting recent\ntrends, methodological innovations, and practical applications. We propose a\nnovel taxonomy that is distinct from existing categorizations to better reflect\nthe current state of the IL research stratum and its trends. Throughout the\nsurvey, we critically examine the strengths, limitations, and evaluation\npractices of representative works, and we outline key challenges and open\ndirections for future research.\n", "link": "http://arxiv.org/abs/2511.03565v1", "date": "2025-11-05", "relevancy": 2.3693, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4826}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4774}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imitation%20Learning%20in%20the%20Deep%20Learning%20Era%3A%20A%20Novel%20Taxonomy%20and%20Recent%0A%20%20Advances&body=Title%3A%20Imitation%20Learning%20in%20the%20Deep%20Learning%20Era%3A%20A%20Novel%20Taxonomy%20and%20Recent%0A%20%20Advances%0AAuthor%3A%20Iason%20Chrysomallis%20and%20Georgios%20Chalkiadakis%0AAbstract%3A%20%20%20Imitation%20learning%20%28IL%29%20enables%20agents%20to%20acquire%20skills%20by%20observing%20and%0Areplicating%20the%20behavior%20of%20one%20or%20multiple%20experts.%20In%20recent%20years%2C%20advances%0Ain%20deep%20learning%20have%20significantly%20expanded%20the%20capabilities%20and%20scalability%0Aof%20imitation%20learning%20across%20a%20range%20of%20domains%2C%20where%20expert%20data%20can%20range%0Afrom%20full%20state-action%20trajectories%20to%20partial%20observations%20or%20unlabeled%0Asequences.%20Alongside%20this%20growth%2C%20novel%20approaches%20have%20emerged%2C%20with%20new%0Amethodologies%20being%20developed%20to%20address%20longstanding%20challenges%20such%20as%0Ageneralization%2C%20covariate%20shift%2C%20and%20demonstration%20quality.%20In%20this%20survey%2C%20we%0Areview%20the%20latest%20advances%20in%20imitation%20learning%20research%2C%20highlighting%20recent%0Atrends%2C%20methodological%20innovations%2C%20and%20practical%20applications.%20We%20propose%20a%0Anovel%20taxonomy%20that%20is%20distinct%20from%20existing%20categorizations%20to%20better%20reflect%0Athe%20current%20state%20of%20the%20IL%20research%20stratum%20and%20its%20trends.%20Throughout%20the%0Asurvey%2C%20we%20critically%20examine%20the%20strengths%2C%20limitations%2C%20and%20evaluation%0Apractices%20of%20representative%20works%2C%20and%20we%20outline%20key%20challenges%20and%20open%0Adirections%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03565v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImitation%2520Learning%2520in%2520the%2520Deep%2520Learning%2520Era%253A%2520A%2520Novel%2520Taxonomy%2520and%2520Recent%250A%2520%2520Advances%26entry.906535625%3DIason%2520Chrysomallis%2520and%2520Georgios%2520Chalkiadakis%26entry.1292438233%3D%2520%2520Imitation%2520learning%2520%2528IL%2529%2520enables%2520agents%2520to%2520acquire%2520skills%2520by%2520observing%2520and%250Areplicating%2520the%2520behavior%2520of%2520one%2520or%2520multiple%2520experts.%2520In%2520recent%2520years%252C%2520advances%250Ain%2520deep%2520learning%2520have%2520significantly%2520expanded%2520the%2520capabilities%2520and%2520scalability%250Aof%2520imitation%2520learning%2520across%2520a%2520range%2520of%2520domains%252C%2520where%2520expert%2520data%2520can%2520range%250Afrom%2520full%2520state-action%2520trajectories%2520to%2520partial%2520observations%2520or%2520unlabeled%250Asequences.%2520Alongside%2520this%2520growth%252C%2520novel%2520approaches%2520have%2520emerged%252C%2520with%2520new%250Amethodologies%2520being%2520developed%2520to%2520address%2520longstanding%2520challenges%2520such%2520as%250Ageneralization%252C%2520covariate%2520shift%252C%2520and%2520demonstration%2520quality.%2520In%2520this%2520survey%252C%2520we%250Areview%2520the%2520latest%2520advances%2520in%2520imitation%2520learning%2520research%252C%2520highlighting%2520recent%250Atrends%252C%2520methodological%2520innovations%252C%2520and%2520practical%2520applications.%2520We%2520propose%2520a%250Anovel%2520taxonomy%2520that%2520is%2520distinct%2520from%2520existing%2520categorizations%2520to%2520better%2520reflect%250Athe%2520current%2520state%2520of%2520the%2520IL%2520research%2520stratum%2520and%2520its%2520trends.%2520Throughout%2520the%250Asurvey%252C%2520we%2520critically%2520examine%2520the%2520strengths%252C%2520limitations%252C%2520and%2520evaluation%250Apractices%2520of%2520representative%2520works%252C%2520and%2520we%2520outline%2520key%2520challenges%2520and%2520open%250Adirections%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03565v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imitation%20Learning%20in%20the%20Deep%20Learning%20Era%3A%20A%20Novel%20Taxonomy%20and%20Recent%0A%20%20Advances&entry.906535625=Iason%20Chrysomallis%20and%20Georgios%20Chalkiadakis&entry.1292438233=%20%20Imitation%20learning%20%28IL%29%20enables%20agents%20to%20acquire%20skills%20by%20observing%20and%0Areplicating%20the%20behavior%20of%20one%20or%20multiple%20experts.%20In%20recent%20years%2C%20advances%0Ain%20deep%20learning%20have%20significantly%20expanded%20the%20capabilities%20and%20scalability%0Aof%20imitation%20learning%20across%20a%20range%20of%20domains%2C%20where%20expert%20data%20can%20range%0Afrom%20full%20state-action%20trajectories%20to%20partial%20observations%20or%20unlabeled%0Asequences.%20Alongside%20this%20growth%2C%20novel%20approaches%20have%20emerged%2C%20with%20new%0Amethodologies%20being%20developed%20to%20address%20longstanding%20challenges%20such%20as%0Ageneralization%2C%20covariate%20shift%2C%20and%20demonstration%20quality.%20In%20this%20survey%2C%20we%0Areview%20the%20latest%20advances%20in%20imitation%20learning%20research%2C%20highlighting%20recent%0Atrends%2C%20methodological%20innovations%2C%20and%20practical%20applications.%20We%20propose%20a%0Anovel%20taxonomy%20that%20is%20distinct%20from%20existing%20categorizations%20to%20better%20reflect%0Athe%20current%20state%20of%20the%20IL%20research%20stratum%20and%20its%20trends.%20Throughout%20the%0Asurvey%2C%20we%20critically%20examine%20the%20strengths%2C%20limitations%2C%20and%20evaluation%0Apractices%20of%20representative%20works%2C%20and%20we%20outline%20key%20challenges%20and%20open%0Adirections%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03565v1&entry.124074799=Read"},
{"title": "TabGemma: Text-Based Tabular ICL via LLM using Continued Pretraining and\n  Retrieval", "author": "G\u00fcnther Schindler and Maximilian Schambach and Michael Medek and Sam Thelin", "abstract": "  We study LLMs for tabular prediction with mixed text, numeric, and\ncategorical fields. We introduce TabGemma, a schema-agnostic in-context learner\nthat treats rows as sequences and tackles two practical hurdles when adapting\npretrained LLMs for tabular predictions: unstable numeric tokenization and\nlimited context size. We propose to canonicalize numbers via signed scientific\nnotation and continue pretraining of a 12B Gemma 3 model with a target\nimputation objective using a large-scale real world dataset. For inference, we\nuse a compact n-gram-based retrieval to select informative exemplars that fit\nwithin a 128k-token window.\n  On semantically rich benchmarks, TabGemma establishes a new state of the art\non classification across low- and high-data regimes and improves monotonically\nwith more context rows. For regression, it is competitive at small sample sizes\nbut trails conventional approaches as data grows. Our results show that LLMs\ncan be effective tabular in-context learners on highly semantic tasks when\npaired with dedicated numeric handling and context retrieval, while motivating\nfurther advances in numeric modeling and long-context scaling.\n", "link": "http://arxiv.org/abs/2511.03570v1", "date": "2025-11-05", "relevancy": 2.3593, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5216}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4716}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TabGemma%3A%20Text-Based%20Tabular%20ICL%20via%20LLM%20using%20Continued%20Pretraining%20and%0A%20%20Retrieval&body=Title%3A%20TabGemma%3A%20Text-Based%20Tabular%20ICL%20via%20LLM%20using%20Continued%20Pretraining%20and%0A%20%20Retrieval%0AAuthor%3A%20G%C3%BCnther%20Schindler%20and%20Maximilian%20Schambach%20and%20Michael%20Medek%20and%20Sam%20Thelin%0AAbstract%3A%20%20%20We%20study%20LLMs%20for%20tabular%20prediction%20with%20mixed%20text%2C%20numeric%2C%20and%0Acategorical%20fields.%20We%20introduce%20TabGemma%2C%20a%20schema-agnostic%20in-context%20learner%0Athat%20treats%20rows%20as%20sequences%20and%20tackles%20two%20practical%20hurdles%20when%20adapting%0Apretrained%20LLMs%20for%20tabular%20predictions%3A%20unstable%20numeric%20tokenization%20and%0Alimited%20context%20size.%20We%20propose%20to%20canonicalize%20numbers%20via%20signed%20scientific%0Anotation%20and%20continue%20pretraining%20of%20a%2012B%20Gemma%203%20model%20with%20a%20target%0Aimputation%20objective%20using%20a%20large-scale%20real%20world%20dataset.%20For%20inference%2C%20we%0Ause%20a%20compact%20n-gram-based%20retrieval%20to%20select%20informative%20exemplars%20that%20fit%0Awithin%20a%20128k-token%20window.%0A%20%20On%20semantically%20rich%20benchmarks%2C%20TabGemma%20establishes%20a%20new%20state%20of%20the%20art%0Aon%20classification%20across%20low-%20and%20high-data%20regimes%20and%20improves%20monotonically%0Awith%20more%20context%20rows.%20For%20regression%2C%20it%20is%20competitive%20at%20small%20sample%20sizes%0Abut%20trails%20conventional%20approaches%20as%20data%20grows.%20Our%20results%20show%20that%20LLMs%0Acan%20be%20effective%20tabular%20in-context%20learners%20on%20highly%20semantic%20tasks%20when%0Apaired%20with%20dedicated%20numeric%20handling%20and%20context%20retrieval%2C%20while%20motivating%0Afurther%20advances%20in%20numeric%20modeling%20and%20long-context%20scaling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03570v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTabGemma%253A%2520Text-Based%2520Tabular%2520ICL%2520via%2520LLM%2520using%2520Continued%2520Pretraining%2520and%250A%2520%2520Retrieval%26entry.906535625%3DG%25C3%25BCnther%2520Schindler%2520and%2520Maximilian%2520Schambach%2520and%2520Michael%2520Medek%2520and%2520Sam%2520Thelin%26entry.1292438233%3D%2520%2520We%2520study%2520LLMs%2520for%2520tabular%2520prediction%2520with%2520mixed%2520text%252C%2520numeric%252C%2520and%250Acategorical%2520fields.%2520We%2520introduce%2520TabGemma%252C%2520a%2520schema-agnostic%2520in-context%2520learner%250Athat%2520treats%2520rows%2520as%2520sequences%2520and%2520tackles%2520two%2520practical%2520hurdles%2520when%2520adapting%250Apretrained%2520LLMs%2520for%2520tabular%2520predictions%253A%2520unstable%2520numeric%2520tokenization%2520and%250Alimited%2520context%2520size.%2520We%2520propose%2520to%2520canonicalize%2520numbers%2520via%2520signed%2520scientific%250Anotation%2520and%2520continue%2520pretraining%2520of%2520a%252012B%2520Gemma%25203%2520model%2520with%2520a%2520target%250Aimputation%2520objective%2520using%2520a%2520large-scale%2520real%2520world%2520dataset.%2520For%2520inference%252C%2520we%250Ause%2520a%2520compact%2520n-gram-based%2520retrieval%2520to%2520select%2520informative%2520exemplars%2520that%2520fit%250Awithin%2520a%2520128k-token%2520window.%250A%2520%2520On%2520semantically%2520rich%2520benchmarks%252C%2520TabGemma%2520establishes%2520a%2520new%2520state%2520of%2520the%2520art%250Aon%2520classification%2520across%2520low-%2520and%2520high-data%2520regimes%2520and%2520improves%2520monotonically%250Awith%2520more%2520context%2520rows.%2520For%2520regression%252C%2520it%2520is%2520competitive%2520at%2520small%2520sample%2520sizes%250Abut%2520trails%2520conventional%2520approaches%2520as%2520data%2520grows.%2520Our%2520results%2520show%2520that%2520LLMs%250Acan%2520be%2520effective%2520tabular%2520in-context%2520learners%2520on%2520highly%2520semantic%2520tasks%2520when%250Apaired%2520with%2520dedicated%2520numeric%2520handling%2520and%2520context%2520retrieval%252C%2520while%2520motivating%250Afurther%2520advances%2520in%2520numeric%2520modeling%2520and%2520long-context%2520scaling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03570v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TabGemma%3A%20Text-Based%20Tabular%20ICL%20via%20LLM%20using%20Continued%20Pretraining%20and%0A%20%20Retrieval&entry.906535625=G%C3%BCnther%20Schindler%20and%20Maximilian%20Schambach%20and%20Michael%20Medek%20and%20Sam%20Thelin&entry.1292438233=%20%20We%20study%20LLMs%20for%20tabular%20prediction%20with%20mixed%20text%2C%20numeric%2C%20and%0Acategorical%20fields.%20We%20introduce%20TabGemma%2C%20a%20schema-agnostic%20in-context%20learner%0Athat%20treats%20rows%20as%20sequences%20and%20tackles%20two%20practical%20hurdles%20when%20adapting%0Apretrained%20LLMs%20for%20tabular%20predictions%3A%20unstable%20numeric%20tokenization%20and%0Alimited%20context%20size.%20We%20propose%20to%20canonicalize%20numbers%20via%20signed%20scientific%0Anotation%20and%20continue%20pretraining%20of%20a%2012B%20Gemma%203%20model%20with%20a%20target%0Aimputation%20objective%20using%20a%20large-scale%20real%20world%20dataset.%20For%20inference%2C%20we%0Ause%20a%20compact%20n-gram-based%20retrieval%20to%20select%20informative%20exemplars%20that%20fit%0Awithin%20a%20128k-token%20window.%0A%20%20On%20semantically%20rich%20benchmarks%2C%20TabGemma%20establishes%20a%20new%20state%20of%20the%20art%0Aon%20classification%20across%20low-%20and%20high-data%20regimes%20and%20improves%20monotonically%0Awith%20more%20context%20rows.%20For%20regression%2C%20it%20is%20competitive%20at%20small%20sample%20sizes%0Abut%20trails%20conventional%20approaches%20as%20data%20grows.%20Our%20results%20show%20that%20LLMs%0Acan%20be%20effective%20tabular%20in-context%20learners%20on%20highly%20semantic%20tasks%20when%0Apaired%20with%20dedicated%20numeric%20handling%20and%20context%20retrieval%2C%20while%20motivating%0Afurther%20advances%20in%20numeric%20modeling%20and%20long-context%20scaling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03570v1&entry.124074799=Read"},
{"title": "Disentangled Concepts Speak Louder Than Words:Explainable Video Action\n  Recognition", "author": "Jongseo Lee and Wooil Lee and Gyeong-Moon Park and Seong Tae Kim and Jinwoo Choi", "abstract": "  Effective explanations of video action recognition models should disentangle\nhow movements unfold over time from the surrounding spatial context. However,\nexisting methods based on saliency produce entangled explanations, making it\nunclear whether predictions rely on motion or spatial context. Language-based\napproaches offer structure but often fail to explain motions due to their tacit\nnature -- intuitively understood but difficult to verbalize. To address these\nchallenges, we propose Disentangled Action aNd Context concept-based\nExplainable (DANCE) video action recognition, a framework that predicts actions\nthrough disentangled concept types: motion dynamics, objects, and scenes. We\ndefine motion dynamics concepts as human pose sequences. We employ a large\nlanguage model to automatically extract object and scene concepts. Built on an\nante-hoc concept bottleneck design, DANCE enforces prediction through these\nconcepts. Experiments on four datasets -- KTH, Penn Action, HAA500, and UCF-101\n-- demonstrate that DANCE significantly improves explanation clarity with\ncompetitive performance. We validate the superior interpretability of DANCE\nthrough a user study. Experimental results also show that DANCE is beneficial\nfor model debugging, editing, and failure analysis.\n", "link": "http://arxiv.org/abs/2511.03725v1", "date": "2025-11-05", "relevancy": 2.3306, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5863}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5863}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangled%20Concepts%20Speak%20Louder%20Than%20Words%3AExplainable%20Video%20Action%0A%20%20Recognition&body=Title%3A%20Disentangled%20Concepts%20Speak%20Louder%20Than%20Words%3AExplainable%20Video%20Action%0A%20%20Recognition%0AAuthor%3A%20Jongseo%20Lee%20and%20Wooil%20Lee%20and%20Gyeong-Moon%20Park%20and%20Seong%20Tae%20Kim%20and%20Jinwoo%20Choi%0AAbstract%3A%20%20%20Effective%20explanations%20of%20video%20action%20recognition%20models%20should%20disentangle%0Ahow%20movements%20unfold%20over%20time%20from%20the%20surrounding%20spatial%20context.%20However%2C%0Aexisting%20methods%20based%20on%20saliency%20produce%20entangled%20explanations%2C%20making%20it%0Aunclear%20whether%20predictions%20rely%20on%20motion%20or%20spatial%20context.%20Language-based%0Aapproaches%20offer%20structure%20but%20often%20fail%20to%20explain%20motions%20due%20to%20their%20tacit%0Anature%20--%20intuitively%20understood%20but%20difficult%20to%20verbalize.%20To%20address%20these%0Achallenges%2C%20we%20propose%20Disentangled%20Action%20aNd%20Context%20concept-based%0AExplainable%20%28DANCE%29%20video%20action%20recognition%2C%20a%20framework%20that%20predicts%20actions%0Athrough%20disentangled%20concept%20types%3A%20motion%20dynamics%2C%20objects%2C%20and%20scenes.%20We%0Adefine%20motion%20dynamics%20concepts%20as%20human%20pose%20sequences.%20We%20employ%20a%20large%0Alanguage%20model%20to%20automatically%20extract%20object%20and%20scene%20concepts.%20Built%20on%20an%0Aante-hoc%20concept%20bottleneck%20design%2C%20DANCE%20enforces%20prediction%20through%20these%0Aconcepts.%20Experiments%20on%20four%20datasets%20--%20KTH%2C%20Penn%20Action%2C%20HAA500%2C%20and%20UCF-101%0A--%20demonstrate%20that%20DANCE%20significantly%20improves%20explanation%20clarity%20with%0Acompetitive%20performance.%20We%20validate%20the%20superior%20interpretability%20of%20DANCE%0Athrough%20a%20user%20study.%20Experimental%20results%20also%20show%20that%20DANCE%20is%20beneficial%0Afor%20model%20debugging%2C%20editing%2C%20and%20failure%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03725v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangled%2520Concepts%2520Speak%2520Louder%2520Than%2520Words%253AExplainable%2520Video%2520Action%250A%2520%2520Recognition%26entry.906535625%3DJongseo%2520Lee%2520and%2520Wooil%2520Lee%2520and%2520Gyeong-Moon%2520Park%2520and%2520Seong%2520Tae%2520Kim%2520and%2520Jinwoo%2520Choi%26entry.1292438233%3D%2520%2520Effective%2520explanations%2520of%2520video%2520action%2520recognition%2520models%2520should%2520disentangle%250Ahow%2520movements%2520unfold%2520over%2520time%2520from%2520the%2520surrounding%2520spatial%2520context.%2520However%252C%250Aexisting%2520methods%2520based%2520on%2520saliency%2520produce%2520entangled%2520explanations%252C%2520making%2520it%250Aunclear%2520whether%2520predictions%2520rely%2520on%2520motion%2520or%2520spatial%2520context.%2520Language-based%250Aapproaches%2520offer%2520structure%2520but%2520often%2520fail%2520to%2520explain%2520motions%2520due%2520to%2520their%2520tacit%250Anature%2520--%2520intuitively%2520understood%2520but%2520difficult%2520to%2520verbalize.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520Disentangled%2520Action%2520aNd%2520Context%2520concept-based%250AExplainable%2520%2528DANCE%2529%2520video%2520action%2520recognition%252C%2520a%2520framework%2520that%2520predicts%2520actions%250Athrough%2520disentangled%2520concept%2520types%253A%2520motion%2520dynamics%252C%2520objects%252C%2520and%2520scenes.%2520We%250Adefine%2520motion%2520dynamics%2520concepts%2520as%2520human%2520pose%2520sequences.%2520We%2520employ%2520a%2520large%250Alanguage%2520model%2520to%2520automatically%2520extract%2520object%2520and%2520scene%2520concepts.%2520Built%2520on%2520an%250Aante-hoc%2520concept%2520bottleneck%2520design%252C%2520DANCE%2520enforces%2520prediction%2520through%2520these%250Aconcepts.%2520Experiments%2520on%2520four%2520datasets%2520--%2520KTH%252C%2520Penn%2520Action%252C%2520HAA500%252C%2520and%2520UCF-101%250A--%2520demonstrate%2520that%2520DANCE%2520significantly%2520improves%2520explanation%2520clarity%2520with%250Acompetitive%2520performance.%2520We%2520validate%2520the%2520superior%2520interpretability%2520of%2520DANCE%250Athrough%2520a%2520user%2520study.%2520Experimental%2520results%2520also%2520show%2520that%2520DANCE%2520is%2520beneficial%250Afor%2520model%2520debugging%252C%2520editing%252C%2520and%2520failure%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03725v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangled%20Concepts%20Speak%20Louder%20Than%20Words%3AExplainable%20Video%20Action%0A%20%20Recognition&entry.906535625=Jongseo%20Lee%20and%20Wooil%20Lee%20and%20Gyeong-Moon%20Park%20and%20Seong%20Tae%20Kim%20and%20Jinwoo%20Choi&entry.1292438233=%20%20Effective%20explanations%20of%20video%20action%20recognition%20models%20should%20disentangle%0Ahow%20movements%20unfold%20over%20time%20from%20the%20surrounding%20spatial%20context.%20However%2C%0Aexisting%20methods%20based%20on%20saliency%20produce%20entangled%20explanations%2C%20making%20it%0Aunclear%20whether%20predictions%20rely%20on%20motion%20or%20spatial%20context.%20Language-based%0Aapproaches%20offer%20structure%20but%20often%20fail%20to%20explain%20motions%20due%20to%20their%20tacit%0Anature%20--%20intuitively%20understood%20but%20difficult%20to%20verbalize.%20To%20address%20these%0Achallenges%2C%20we%20propose%20Disentangled%20Action%20aNd%20Context%20concept-based%0AExplainable%20%28DANCE%29%20video%20action%20recognition%2C%20a%20framework%20that%20predicts%20actions%0Athrough%20disentangled%20concept%20types%3A%20motion%20dynamics%2C%20objects%2C%20and%20scenes.%20We%0Adefine%20motion%20dynamics%20concepts%20as%20human%20pose%20sequences.%20We%20employ%20a%20large%0Alanguage%20model%20to%20automatically%20extract%20object%20and%20scene%20concepts.%20Built%20on%20an%0Aante-hoc%20concept%20bottleneck%20design%2C%20DANCE%20enforces%20prediction%20through%20these%0Aconcepts.%20Experiments%20on%20four%20datasets%20--%20KTH%2C%20Penn%20Action%2C%20HAA500%2C%20and%20UCF-101%0A--%20demonstrate%20that%20DANCE%20significantly%20improves%20explanation%20clarity%20with%0Acompetitive%20performance.%20We%20validate%20the%20superior%20interpretability%20of%20DANCE%0Athrough%20a%20user%20study.%20Experimental%20results%20also%20show%20that%20DANCE%20is%20beneficial%0Afor%20model%20debugging%2C%20editing%2C%20and%20failure%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03725v1&entry.124074799=Read"},
{"title": "Assessing the Macro and Micro Effects of Random Seeds on Fine-Tuning\n  Large Language Models", "author": "Nghia Bui and Guergana Savova and Lijing Wang", "abstract": "  The impact of random seeds in fine-tuning large language models (LLMs) has\nbeen largely overlooked despite its potential influence on model performance.In\nthis study, we systematically evaluate the effects of random seeds on LLMs\nusing the GLUE and SuperGLUE benchmarks. We analyze the macro-level impact\nthrough traditional metrics like accuracy and F1, calculating their mean and\nvariance to quantify performance fluctuations. To capture the micro-level\neffects, we introduce a novel metric, consistency, measuring the stability of\nindividual predictions across runs. Our experiments reveal significant variance\nat both macro and micro levels, underscoring the need for careful consideration\nof random seeds in fine-tuning and evaluation.\n", "link": "http://arxiv.org/abs/2503.07329v2", "date": "2025-11-05", "relevancy": 2.3174, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4645}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4645}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assessing%20the%20Macro%20and%20Micro%20Effects%20of%20Random%20Seeds%20on%20Fine-Tuning%0A%20%20Large%20Language%20Models&body=Title%3A%20Assessing%20the%20Macro%20and%20Micro%20Effects%20of%20Random%20Seeds%20on%20Fine-Tuning%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Nghia%20Bui%20and%20Guergana%20Savova%20and%20Lijing%20Wang%0AAbstract%3A%20%20%20The%20impact%20of%20random%20seeds%20in%20fine-tuning%20large%20language%20models%20%28LLMs%29%20has%0Abeen%20largely%20overlooked%20despite%20its%20potential%20influence%20on%20model%20performance.In%0Athis%20study%2C%20we%20systematically%20evaluate%20the%20effects%20of%20random%20seeds%20on%20LLMs%0Ausing%20the%20GLUE%20and%20SuperGLUE%20benchmarks.%20We%20analyze%20the%20macro-level%20impact%0Athrough%20traditional%20metrics%20like%20accuracy%20and%20F1%2C%20calculating%20their%20mean%20and%0Avariance%20to%20quantify%20performance%20fluctuations.%20To%20capture%20the%20micro-level%0Aeffects%2C%20we%20introduce%20a%20novel%20metric%2C%20consistency%2C%20measuring%20the%20stability%20of%0Aindividual%20predictions%20across%20runs.%20Our%20experiments%20reveal%20significant%20variance%0Aat%20both%20macro%20and%20micro%20levels%2C%20underscoring%20the%20need%20for%20careful%20consideration%0Aof%20random%20seeds%20in%20fine-tuning%20and%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.07329v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssessing%2520the%2520Macro%2520and%2520Micro%2520Effects%2520of%2520Random%2520Seeds%2520on%2520Fine-Tuning%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DNghia%2520Bui%2520and%2520Guergana%2520Savova%2520and%2520Lijing%2520Wang%26entry.1292438233%3D%2520%2520The%2520impact%2520of%2520random%2520seeds%2520in%2520fine-tuning%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%250Abeen%2520largely%2520overlooked%2520despite%2520its%2520potential%2520influence%2520on%2520model%2520performance.In%250Athis%2520study%252C%2520we%2520systematically%2520evaluate%2520the%2520effects%2520of%2520random%2520seeds%2520on%2520LLMs%250Ausing%2520the%2520GLUE%2520and%2520SuperGLUE%2520benchmarks.%2520We%2520analyze%2520the%2520macro-level%2520impact%250Athrough%2520traditional%2520metrics%2520like%2520accuracy%2520and%2520F1%252C%2520calculating%2520their%2520mean%2520and%250Avariance%2520to%2520quantify%2520performance%2520fluctuations.%2520To%2520capture%2520the%2520micro-level%250Aeffects%252C%2520we%2520introduce%2520a%2520novel%2520metric%252C%2520consistency%252C%2520measuring%2520the%2520stability%2520of%250Aindividual%2520predictions%2520across%2520runs.%2520Our%2520experiments%2520reveal%2520significant%2520variance%250Aat%2520both%2520macro%2520and%2520micro%2520levels%252C%2520underscoring%2520the%2520need%2520for%2520careful%2520consideration%250Aof%2520random%2520seeds%2520in%2520fine-tuning%2520and%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.07329v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20the%20Macro%20and%20Micro%20Effects%20of%20Random%20Seeds%20on%20Fine-Tuning%0A%20%20Large%20Language%20Models&entry.906535625=Nghia%20Bui%20and%20Guergana%20Savova%20and%20Lijing%20Wang&entry.1292438233=%20%20The%20impact%20of%20random%20seeds%20in%20fine-tuning%20large%20language%20models%20%28LLMs%29%20has%0Abeen%20largely%20overlooked%20despite%20its%20potential%20influence%20on%20model%20performance.In%0Athis%20study%2C%20we%20systematically%20evaluate%20the%20effects%20of%20random%20seeds%20on%20LLMs%0Ausing%20the%20GLUE%20and%20SuperGLUE%20benchmarks.%20We%20analyze%20the%20macro-level%20impact%0Athrough%20traditional%20metrics%20like%20accuracy%20and%20F1%2C%20calculating%20their%20mean%20and%0Avariance%20to%20quantify%20performance%20fluctuations.%20To%20capture%20the%20micro-level%0Aeffects%2C%20we%20introduce%20a%20novel%20metric%2C%20consistency%2C%20measuring%20the%20stability%20of%0Aindividual%20predictions%20across%20runs.%20Our%20experiments%20reveal%20significant%20variance%0Aat%20both%20macro%20and%20micro%20levels%2C%20underscoring%20the%20need%20for%20careful%20consideration%0Aof%20random%20seeds%20in%20fine-tuning%20and%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.07329v2&entry.124074799=Read"},
{"title": "Towards Transparent Stance Detection: A Zero-Shot Approach Using\n  Implicit and Explicit Interpretability", "author": "Apoorva Upadhyaya and Wolfgang Nejdl and Marco Fisichella", "abstract": "  Zero-Shot Stance Detection (ZSSD) identifies the attitude of the post toward\nunseen targets. Existing research using contrastive, meta-learning, or data\naugmentation suffers from generalizability issues or lack of coherence between\ntext and target. Recent works leveraging large language models (LLMs) for ZSSD\nfocus either on improving unseen target-specific knowledge or generating\nexplanations for stance analysis. However, most of these works are limited by\ntheir over-reliance on explicit reasoning, provide coarse explanations that\nlack nuance, and do not explicitly model the reasoning process, making it\ndifficult to interpret the model's predictions. To address these issues, in our\nstudy, we develop a novel interpretable ZSSD framework, IRIS. We provide an\ninterpretable understanding of the attitude of the input towards the target\nimplicitly based on sequences within the text (implicit rationales) and\nexplicitly based on linguistic measures (explicit rationales). IRIS considers\nstance detection as an information retrieval ranking task, understanding the\nrelevance of implicit rationales for different stances to guide the model\ntowards correct predictions without requiring the ground-truth of rationales,\nthus providing inherent interpretability. In addition, explicit rationales\nbased on communicative features help decode the emotional and cognitive\ndimensions of stance, offering an interpretable understanding of the author's\nattitude towards the given target. Extensive experiments on the benchmark\ndatasets of VAST, EZ-STANCE, P-Stance, and RFD using 50%, 30%, and even 10%\ntraining data prove the generalizability of our model, benefiting from the\nproposed architecture and interpretable design.\n", "link": "http://arxiv.org/abs/2511.03635v1", "date": "2025-11-05", "relevancy": 2.2329, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5623}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5623}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5376}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Transparent%20Stance%20Detection%3A%20A%20Zero-Shot%20Approach%20Using%0A%20%20Implicit%20and%20Explicit%20Interpretability&body=Title%3A%20Towards%20Transparent%20Stance%20Detection%3A%20A%20Zero-Shot%20Approach%20Using%0A%20%20Implicit%20and%20Explicit%20Interpretability%0AAuthor%3A%20Apoorva%20Upadhyaya%20and%20Wolfgang%20Nejdl%20and%20Marco%20Fisichella%0AAbstract%3A%20%20%20Zero-Shot%20Stance%20Detection%20%28ZSSD%29%20identifies%20the%20attitude%20of%20the%20post%20toward%0Aunseen%20targets.%20Existing%20research%20using%20contrastive%2C%20meta-learning%2C%20or%20data%0Aaugmentation%20suffers%20from%20generalizability%20issues%20or%20lack%20of%20coherence%20between%0Atext%20and%20target.%20Recent%20works%20leveraging%20large%20language%20models%20%28LLMs%29%20for%20ZSSD%0Afocus%20either%20on%20improving%20unseen%20target-specific%20knowledge%20or%20generating%0Aexplanations%20for%20stance%20analysis.%20However%2C%20most%20of%20these%20works%20are%20limited%20by%0Atheir%20over-reliance%20on%20explicit%20reasoning%2C%20provide%20coarse%20explanations%20that%0Alack%20nuance%2C%20and%20do%20not%20explicitly%20model%20the%20reasoning%20process%2C%20making%20it%0Adifficult%20to%20interpret%20the%20model%27s%20predictions.%20To%20address%20these%20issues%2C%20in%20our%0Astudy%2C%20we%20develop%20a%20novel%20interpretable%20ZSSD%20framework%2C%20IRIS.%20We%20provide%20an%0Ainterpretable%20understanding%20of%20the%20attitude%20of%20the%20input%20towards%20the%20target%0Aimplicitly%20based%20on%20sequences%20within%20the%20text%20%28implicit%20rationales%29%20and%0Aexplicitly%20based%20on%20linguistic%20measures%20%28explicit%20rationales%29.%20IRIS%20considers%0Astance%20detection%20as%20an%20information%20retrieval%20ranking%20task%2C%20understanding%20the%0Arelevance%20of%20implicit%20rationales%20for%20different%20stances%20to%20guide%20the%20model%0Atowards%20correct%20predictions%20without%20requiring%20the%20ground-truth%20of%20rationales%2C%0Athus%20providing%20inherent%20interpretability.%20In%20addition%2C%20explicit%20rationales%0Abased%20on%20communicative%20features%20help%20decode%20the%20emotional%20and%20cognitive%0Adimensions%20of%20stance%2C%20offering%20an%20interpretable%20understanding%20of%20the%20author%27s%0Aattitude%20towards%20the%20given%20target.%20Extensive%20experiments%20on%20the%20benchmark%0Adatasets%20of%20VAST%2C%20EZ-STANCE%2C%20P-Stance%2C%20and%20RFD%20using%2050%25%2C%2030%25%2C%20and%20even%2010%25%0Atraining%20data%20prove%20the%20generalizability%20of%20our%20model%2C%20benefiting%20from%20the%0Aproposed%20architecture%20and%20interpretable%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Transparent%2520Stance%2520Detection%253A%2520A%2520Zero-Shot%2520Approach%2520Using%250A%2520%2520Implicit%2520and%2520Explicit%2520Interpretability%26entry.906535625%3DApoorva%2520Upadhyaya%2520and%2520Wolfgang%2520Nejdl%2520and%2520Marco%2520Fisichella%26entry.1292438233%3D%2520%2520Zero-Shot%2520Stance%2520Detection%2520%2528ZSSD%2529%2520identifies%2520the%2520attitude%2520of%2520the%2520post%2520toward%250Aunseen%2520targets.%2520Existing%2520research%2520using%2520contrastive%252C%2520meta-learning%252C%2520or%2520data%250Aaugmentation%2520suffers%2520from%2520generalizability%2520issues%2520or%2520lack%2520of%2520coherence%2520between%250Atext%2520and%2520target.%2520Recent%2520works%2520leveraging%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%2520ZSSD%250Afocus%2520either%2520on%2520improving%2520unseen%2520target-specific%2520knowledge%2520or%2520generating%250Aexplanations%2520for%2520stance%2520analysis.%2520However%252C%2520most%2520of%2520these%2520works%2520are%2520limited%2520by%250Atheir%2520over-reliance%2520on%2520explicit%2520reasoning%252C%2520provide%2520coarse%2520explanations%2520that%250Alack%2520nuance%252C%2520and%2520do%2520not%2520explicitly%2520model%2520the%2520reasoning%2520process%252C%2520making%2520it%250Adifficult%2520to%2520interpret%2520the%2520model%2527s%2520predictions.%2520To%2520address%2520these%2520issues%252C%2520in%2520our%250Astudy%252C%2520we%2520develop%2520a%2520novel%2520interpretable%2520ZSSD%2520framework%252C%2520IRIS.%2520We%2520provide%2520an%250Ainterpretable%2520understanding%2520of%2520the%2520attitude%2520of%2520the%2520input%2520towards%2520the%2520target%250Aimplicitly%2520based%2520on%2520sequences%2520within%2520the%2520text%2520%2528implicit%2520rationales%2529%2520and%250Aexplicitly%2520based%2520on%2520linguistic%2520measures%2520%2528explicit%2520rationales%2529.%2520IRIS%2520considers%250Astance%2520detection%2520as%2520an%2520information%2520retrieval%2520ranking%2520task%252C%2520understanding%2520the%250Arelevance%2520of%2520implicit%2520rationales%2520for%2520different%2520stances%2520to%2520guide%2520the%2520model%250Atowards%2520correct%2520predictions%2520without%2520requiring%2520the%2520ground-truth%2520of%2520rationales%252C%250Athus%2520providing%2520inherent%2520interpretability.%2520In%2520addition%252C%2520explicit%2520rationales%250Abased%2520on%2520communicative%2520features%2520help%2520decode%2520the%2520emotional%2520and%2520cognitive%250Adimensions%2520of%2520stance%252C%2520offering%2520an%2520interpretable%2520understanding%2520of%2520the%2520author%2527s%250Aattitude%2520towards%2520the%2520given%2520target.%2520Extensive%2520experiments%2520on%2520the%2520benchmark%250Adatasets%2520of%2520VAST%252C%2520EZ-STANCE%252C%2520P-Stance%252C%2520and%2520RFD%2520using%252050%2525%252C%252030%2525%252C%2520and%2520even%252010%2525%250Atraining%2520data%2520prove%2520the%2520generalizability%2520of%2520our%2520model%252C%2520benefiting%2520from%2520the%250Aproposed%2520architecture%2520and%2520interpretable%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Transparent%20Stance%20Detection%3A%20A%20Zero-Shot%20Approach%20Using%0A%20%20Implicit%20and%20Explicit%20Interpretability&entry.906535625=Apoorva%20Upadhyaya%20and%20Wolfgang%20Nejdl%20and%20Marco%20Fisichella&entry.1292438233=%20%20Zero-Shot%20Stance%20Detection%20%28ZSSD%29%20identifies%20the%20attitude%20of%20the%20post%20toward%0Aunseen%20targets.%20Existing%20research%20using%20contrastive%2C%20meta-learning%2C%20or%20data%0Aaugmentation%20suffers%20from%20generalizability%20issues%20or%20lack%20of%20coherence%20between%0Atext%20and%20target.%20Recent%20works%20leveraging%20large%20language%20models%20%28LLMs%29%20for%20ZSSD%0Afocus%20either%20on%20improving%20unseen%20target-specific%20knowledge%20or%20generating%0Aexplanations%20for%20stance%20analysis.%20However%2C%20most%20of%20these%20works%20are%20limited%20by%0Atheir%20over-reliance%20on%20explicit%20reasoning%2C%20provide%20coarse%20explanations%20that%0Alack%20nuance%2C%20and%20do%20not%20explicitly%20model%20the%20reasoning%20process%2C%20making%20it%0Adifficult%20to%20interpret%20the%20model%27s%20predictions.%20To%20address%20these%20issues%2C%20in%20our%0Astudy%2C%20we%20develop%20a%20novel%20interpretable%20ZSSD%20framework%2C%20IRIS.%20We%20provide%20an%0Ainterpretable%20understanding%20of%20the%20attitude%20of%20the%20input%20towards%20the%20target%0Aimplicitly%20based%20on%20sequences%20within%20the%20text%20%28implicit%20rationales%29%20and%0Aexplicitly%20based%20on%20linguistic%20measures%20%28explicit%20rationales%29.%20IRIS%20considers%0Astance%20detection%20as%20an%20information%20retrieval%20ranking%20task%2C%20understanding%20the%0Arelevance%20of%20implicit%20rationales%20for%20different%20stances%20to%20guide%20the%20model%0Atowards%20correct%20predictions%20without%20requiring%20the%20ground-truth%20of%20rationales%2C%0Athus%20providing%20inherent%20interpretability.%20In%20addition%2C%20explicit%20rationales%0Abased%20on%20communicative%20features%20help%20decode%20the%20emotional%20and%20cognitive%0Adimensions%20of%20stance%2C%20offering%20an%20interpretable%20understanding%20of%20the%20author%27s%0Aattitude%20towards%20the%20given%20target.%20Extensive%20experiments%20on%20the%20benchmark%0Adatasets%20of%20VAST%2C%20EZ-STANCE%2C%20P-Stance%2C%20and%20RFD%20using%2050%25%2C%2030%25%2C%20and%20even%2010%25%0Atraining%20data%20prove%20the%20generalizability%20of%20our%20model%2C%20benefiting%20from%20the%0Aproposed%20architecture%20and%20interpretable%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03635v1&entry.124074799=Read"},
{"title": "Compliance Minimization via Physics-Informed Gaussian Processes", "author": "Xiangyu Sun and Amin Yousefpour and Shirin Hosseinmardi and Ramin Bostanabad", "abstract": "  Machine learning (ML) techniques have recently gained significant attention\nfor solving compliance minimization (CM) problems. However, these methods\ntypically provide poor feature boundaries, are very expensive, and lack a\nsystematic mechanism to control the design complexity. Herein, we address these\nlimitations by proposing a mesh-free and simultaneous framework based on\nphysics-informed Gaussian processes (GPs). In our approach, we parameterize the\ndesign and state variables with GP priors which have independent kernels but\nshare a multi-output neural network (NN) as their mean function. The\narchitecture of this NN is based on Parametric Grid Convolutional Attention\nNetworks (PGCANs) which not only mitigate spectral bias issues, but also\nprovide an interpretable mechanism to control design complexity. We estimate\nall the parameters of our GP-based representations by simultaneously minimizing\nthe compliance, total potential energy, and residual of volume fraction\nconstraint. Importantly, our loss function exclude all data-based residuals as\nGPs automatically satisfy them. We also develop computational schemes based on\ncurriculum training and numerical integration to increase the efficiency and\nrobustness of our approach which is shown to (1) produce super-resolution\ntopologies with fast convergence, (2) achieve comparable compliance and less\ngray area fraction compared to traditional numerical methods, (3) provide\ncontrol over fine-scale features, and (4) outperform competing ML-based\nmethods.\n", "link": "http://arxiv.org/abs/2507.09968v2", "date": "2025-11-05", "relevancy": 2.2098, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5802}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5336}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compliance%20Minimization%20via%20Physics-Informed%20Gaussian%20Processes&body=Title%3A%20Compliance%20Minimization%20via%20Physics-Informed%20Gaussian%20Processes%0AAuthor%3A%20Xiangyu%20Sun%20and%20Amin%20Yousefpour%20and%20Shirin%20Hosseinmardi%20and%20Ramin%20Bostanabad%0AAbstract%3A%20%20%20Machine%20learning%20%28ML%29%20techniques%20have%20recently%20gained%20significant%20attention%0Afor%20solving%20compliance%20minimization%20%28CM%29%20problems.%20However%2C%20these%20methods%0Atypically%20provide%20poor%20feature%20boundaries%2C%20are%20very%20expensive%2C%20and%20lack%20a%0Asystematic%20mechanism%20to%20control%20the%20design%20complexity.%20Herein%2C%20we%20address%20these%0Alimitations%20by%20proposing%20a%20mesh-free%20and%20simultaneous%20framework%20based%20on%0Aphysics-informed%20Gaussian%20processes%20%28GPs%29.%20In%20our%20approach%2C%20we%20parameterize%20the%0Adesign%20and%20state%20variables%20with%20GP%20priors%20which%20have%20independent%20kernels%20but%0Ashare%20a%20multi-output%20neural%20network%20%28NN%29%20as%20their%20mean%20function.%20The%0Aarchitecture%20of%20this%20NN%20is%20based%20on%20Parametric%20Grid%20Convolutional%20Attention%0ANetworks%20%28PGCANs%29%20which%20not%20only%20mitigate%20spectral%20bias%20issues%2C%20but%20also%0Aprovide%20an%20interpretable%20mechanism%20to%20control%20design%20complexity.%20We%20estimate%0Aall%20the%20parameters%20of%20our%20GP-based%20representations%20by%20simultaneously%20minimizing%0Athe%20compliance%2C%20total%20potential%20energy%2C%20and%20residual%20of%20volume%20fraction%0Aconstraint.%20Importantly%2C%20our%20loss%20function%20exclude%20all%20data-based%20residuals%20as%0AGPs%20automatically%20satisfy%20them.%20We%20also%20develop%20computational%20schemes%20based%20on%0Acurriculum%20training%20and%20numerical%20integration%20to%20increase%20the%20efficiency%20and%0Arobustness%20of%20our%20approach%20which%20is%20shown%20to%20%281%29%20produce%20super-resolution%0Atopologies%20with%20fast%20convergence%2C%20%282%29%20achieve%20comparable%20compliance%20and%20less%0Agray%20area%20fraction%20compared%20to%20traditional%20numerical%20methods%2C%20%283%29%20provide%0Acontrol%20over%20fine-scale%20features%2C%20and%20%284%29%20outperform%20competing%20ML-based%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.09968v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompliance%2520Minimization%2520via%2520Physics-Informed%2520Gaussian%2520Processes%26entry.906535625%3DXiangyu%2520Sun%2520and%2520Amin%2520Yousefpour%2520and%2520Shirin%2520Hosseinmardi%2520and%2520Ramin%2520Bostanabad%26entry.1292438233%3D%2520%2520Machine%2520learning%2520%2528ML%2529%2520techniques%2520have%2520recently%2520gained%2520significant%2520attention%250Afor%2520solving%2520compliance%2520minimization%2520%2528CM%2529%2520problems.%2520However%252C%2520these%2520methods%250Atypically%2520provide%2520poor%2520feature%2520boundaries%252C%2520are%2520very%2520expensive%252C%2520and%2520lack%2520a%250Asystematic%2520mechanism%2520to%2520control%2520the%2520design%2520complexity.%2520Herein%252C%2520we%2520address%2520these%250Alimitations%2520by%2520proposing%2520a%2520mesh-free%2520and%2520simultaneous%2520framework%2520based%2520on%250Aphysics-informed%2520Gaussian%2520processes%2520%2528GPs%2529.%2520In%2520our%2520approach%252C%2520we%2520parameterize%2520the%250Adesign%2520and%2520state%2520variables%2520with%2520GP%2520priors%2520which%2520have%2520independent%2520kernels%2520but%250Ashare%2520a%2520multi-output%2520neural%2520network%2520%2528NN%2529%2520as%2520their%2520mean%2520function.%2520The%250Aarchitecture%2520of%2520this%2520NN%2520is%2520based%2520on%2520Parametric%2520Grid%2520Convolutional%2520Attention%250ANetworks%2520%2528PGCANs%2529%2520which%2520not%2520only%2520mitigate%2520spectral%2520bias%2520issues%252C%2520but%2520also%250Aprovide%2520an%2520interpretable%2520mechanism%2520to%2520control%2520design%2520complexity.%2520We%2520estimate%250Aall%2520the%2520parameters%2520of%2520our%2520GP-based%2520representations%2520by%2520simultaneously%2520minimizing%250Athe%2520compliance%252C%2520total%2520potential%2520energy%252C%2520and%2520residual%2520of%2520volume%2520fraction%250Aconstraint.%2520Importantly%252C%2520our%2520loss%2520function%2520exclude%2520all%2520data-based%2520residuals%2520as%250AGPs%2520automatically%2520satisfy%2520them.%2520We%2520also%2520develop%2520computational%2520schemes%2520based%2520on%250Acurriculum%2520training%2520and%2520numerical%2520integration%2520to%2520increase%2520the%2520efficiency%2520and%250Arobustness%2520of%2520our%2520approach%2520which%2520is%2520shown%2520to%2520%25281%2529%2520produce%2520super-resolution%250Atopologies%2520with%2520fast%2520convergence%252C%2520%25282%2529%2520achieve%2520comparable%2520compliance%2520and%2520less%250Agray%2520area%2520fraction%2520compared%2520to%2520traditional%2520numerical%2520methods%252C%2520%25283%2529%2520provide%250Acontrol%2520over%2520fine-scale%2520features%252C%2520and%2520%25284%2529%2520outperform%2520competing%2520ML-based%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.09968v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compliance%20Minimization%20via%20Physics-Informed%20Gaussian%20Processes&entry.906535625=Xiangyu%20Sun%20and%20Amin%20Yousefpour%20and%20Shirin%20Hosseinmardi%20and%20Ramin%20Bostanabad&entry.1292438233=%20%20Machine%20learning%20%28ML%29%20techniques%20have%20recently%20gained%20significant%20attention%0Afor%20solving%20compliance%20minimization%20%28CM%29%20problems.%20However%2C%20these%20methods%0Atypically%20provide%20poor%20feature%20boundaries%2C%20are%20very%20expensive%2C%20and%20lack%20a%0Asystematic%20mechanism%20to%20control%20the%20design%20complexity.%20Herein%2C%20we%20address%20these%0Alimitations%20by%20proposing%20a%20mesh-free%20and%20simultaneous%20framework%20based%20on%0Aphysics-informed%20Gaussian%20processes%20%28GPs%29.%20In%20our%20approach%2C%20we%20parameterize%20the%0Adesign%20and%20state%20variables%20with%20GP%20priors%20which%20have%20independent%20kernels%20but%0Ashare%20a%20multi-output%20neural%20network%20%28NN%29%20as%20their%20mean%20function.%20The%0Aarchitecture%20of%20this%20NN%20is%20based%20on%20Parametric%20Grid%20Convolutional%20Attention%0ANetworks%20%28PGCANs%29%20which%20not%20only%20mitigate%20spectral%20bias%20issues%2C%20but%20also%0Aprovide%20an%20interpretable%20mechanism%20to%20control%20design%20complexity.%20We%20estimate%0Aall%20the%20parameters%20of%20our%20GP-based%20representations%20by%20simultaneously%20minimizing%0Athe%20compliance%2C%20total%20potential%20energy%2C%20and%20residual%20of%20volume%20fraction%0Aconstraint.%20Importantly%2C%20our%20loss%20function%20exclude%20all%20data-based%20residuals%20as%0AGPs%20automatically%20satisfy%20them.%20We%20also%20develop%20computational%20schemes%20based%20on%0Acurriculum%20training%20and%20numerical%20integration%20to%20increase%20the%20efficiency%20and%0Arobustness%20of%20our%20approach%20which%20is%20shown%20to%20%281%29%20produce%20super-resolution%0Atopologies%20with%20fast%20convergence%2C%20%282%29%20achieve%20comparable%20compliance%20and%20less%0Agray%20area%20fraction%20compared%20to%20traditional%20numerical%20methods%2C%20%283%29%20provide%0Acontrol%20over%20fine-scale%20features%2C%20and%20%284%29%20outperform%20competing%20ML-based%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.09968v2&entry.124074799=Read"},
{"title": "A Lightweight 3D-CNN for Event-Based Human Action Recognition with\n  Privacy-Preserving Potential", "author": "Mehdi Sefidgar Dilmaghani and Francis Fowley and Peter Corcoran", "abstract": "  This paper presents a lightweight three-dimensional convolutional neural\nnetwork (3DCNN) for human activity recognition (HAR) using event-based vision\ndata. Privacy preservation is a key challenge in human monitoring systems, as\nconventional frame-based cameras capture identifiable personal information. In\ncontrast, event cameras record only changes in pixel intensity, providing an\ninherently privacy-preserving sensing modality. The proposed network\neffectively models both spatial and temporal dynamics while maintaining a\ncompact design suitable for edge deployment. To address class imbalance and\nenhance generalization, focal loss with class reweighting and targeted data\naugmentation strategies are employed. The model is trained and evaluated on a\ncomposite dataset derived from the Toyota Smart Home and ETRI datasets.\nExperimental results demonstrate an F1-score of 0.9415 and an overall accuracy\nof 94.17%, outperforming benchmark 3D-CNN architectures such as C3D, ResNet3D,\nand MC3_18 by up to 3%. These results highlight the potential of event-based\ndeep learning for developing accurate, efficient, and privacy-aware human\naction recognition systems suitable for real-world edge applications.\n", "link": "http://arxiv.org/abs/2511.03665v1", "date": "2025-11-05", "relevancy": 2.1757, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5646}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5485}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Lightweight%203D-CNN%20for%20Event-Based%20Human%20Action%20Recognition%20with%0A%20%20Privacy-Preserving%20Potential&body=Title%3A%20A%20Lightweight%203D-CNN%20for%20Event-Based%20Human%20Action%20Recognition%20with%0A%20%20Privacy-Preserving%20Potential%0AAuthor%3A%20Mehdi%20Sefidgar%20Dilmaghani%20and%20Francis%20Fowley%20and%20Peter%20Corcoran%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20lightweight%20three-dimensional%20convolutional%20neural%0Anetwork%20%283DCNN%29%20for%20human%20activity%20recognition%20%28HAR%29%20using%20event-based%20vision%0Adata.%20Privacy%20preservation%20is%20a%20key%20challenge%20in%20human%20monitoring%20systems%2C%20as%0Aconventional%20frame-based%20cameras%20capture%20identifiable%20personal%20information.%20In%0Acontrast%2C%20event%20cameras%20record%20only%20changes%20in%20pixel%20intensity%2C%20providing%20an%0Ainherently%20privacy-preserving%20sensing%20modality.%20The%20proposed%20network%0Aeffectively%20models%20both%20spatial%20and%20temporal%20dynamics%20while%20maintaining%20a%0Acompact%20design%20suitable%20for%20edge%20deployment.%20To%20address%20class%20imbalance%20and%0Aenhance%20generalization%2C%20focal%20loss%20with%20class%20reweighting%20and%20targeted%20data%0Aaugmentation%20strategies%20are%20employed.%20The%20model%20is%20trained%20and%20evaluated%20on%20a%0Acomposite%20dataset%20derived%20from%20the%20Toyota%20Smart%20Home%20and%20ETRI%20datasets.%0AExperimental%20results%20demonstrate%20an%20F1-score%20of%200.9415%20and%20an%20overall%20accuracy%0Aof%2094.17%25%2C%20outperforming%20benchmark%203D-CNN%20architectures%20such%20as%20C3D%2C%20ResNet3D%2C%0Aand%20MC3_18%20by%20up%20to%203%25.%20These%20results%20highlight%20the%20potential%20of%20event-based%0Adeep%20learning%20for%20developing%20accurate%2C%20efficient%2C%20and%20privacy-aware%20human%0Aaction%20recognition%20systems%20suitable%20for%20real-world%20edge%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Lightweight%25203D-CNN%2520for%2520Event-Based%2520Human%2520Action%2520Recognition%2520with%250A%2520%2520Privacy-Preserving%2520Potential%26entry.906535625%3DMehdi%2520Sefidgar%2520Dilmaghani%2520and%2520Francis%2520Fowley%2520and%2520Peter%2520Corcoran%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520lightweight%2520three-dimensional%2520convolutional%2520neural%250Anetwork%2520%25283DCNN%2529%2520for%2520human%2520activity%2520recognition%2520%2528HAR%2529%2520using%2520event-based%2520vision%250Adata.%2520Privacy%2520preservation%2520is%2520a%2520key%2520challenge%2520in%2520human%2520monitoring%2520systems%252C%2520as%250Aconventional%2520frame-based%2520cameras%2520capture%2520identifiable%2520personal%2520information.%2520In%250Acontrast%252C%2520event%2520cameras%2520record%2520only%2520changes%2520in%2520pixel%2520intensity%252C%2520providing%2520an%250Ainherently%2520privacy-preserving%2520sensing%2520modality.%2520The%2520proposed%2520network%250Aeffectively%2520models%2520both%2520spatial%2520and%2520temporal%2520dynamics%2520while%2520maintaining%2520a%250Acompact%2520design%2520suitable%2520for%2520edge%2520deployment.%2520To%2520address%2520class%2520imbalance%2520and%250Aenhance%2520generalization%252C%2520focal%2520loss%2520with%2520class%2520reweighting%2520and%2520targeted%2520data%250Aaugmentation%2520strategies%2520are%2520employed.%2520The%2520model%2520is%2520trained%2520and%2520evaluated%2520on%2520a%250Acomposite%2520dataset%2520derived%2520from%2520the%2520Toyota%2520Smart%2520Home%2520and%2520ETRI%2520datasets.%250AExperimental%2520results%2520demonstrate%2520an%2520F1-score%2520of%25200.9415%2520and%2520an%2520overall%2520accuracy%250Aof%252094.17%2525%252C%2520outperforming%2520benchmark%25203D-CNN%2520architectures%2520such%2520as%2520C3D%252C%2520ResNet3D%252C%250Aand%2520MC3_18%2520by%2520up%2520to%25203%2525.%2520These%2520results%2520highlight%2520the%2520potential%2520of%2520event-based%250Adeep%2520learning%2520for%2520developing%2520accurate%252C%2520efficient%252C%2520and%2520privacy-aware%2520human%250Aaction%2520recognition%2520systems%2520suitable%2520for%2520real-world%2520edge%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Lightweight%203D-CNN%20for%20Event-Based%20Human%20Action%20Recognition%20with%0A%20%20Privacy-Preserving%20Potential&entry.906535625=Mehdi%20Sefidgar%20Dilmaghani%20and%20Francis%20Fowley%20and%20Peter%20Corcoran&entry.1292438233=%20%20This%20paper%20presents%20a%20lightweight%20three-dimensional%20convolutional%20neural%0Anetwork%20%283DCNN%29%20for%20human%20activity%20recognition%20%28HAR%29%20using%20event-based%20vision%0Adata.%20Privacy%20preservation%20is%20a%20key%20challenge%20in%20human%20monitoring%20systems%2C%20as%0Aconventional%20frame-based%20cameras%20capture%20identifiable%20personal%20information.%20In%0Acontrast%2C%20event%20cameras%20record%20only%20changes%20in%20pixel%20intensity%2C%20providing%20an%0Ainherently%20privacy-preserving%20sensing%20modality.%20The%20proposed%20network%0Aeffectively%20models%20both%20spatial%20and%20temporal%20dynamics%20while%20maintaining%20a%0Acompact%20design%20suitable%20for%20edge%20deployment.%20To%20address%20class%20imbalance%20and%0Aenhance%20generalization%2C%20focal%20loss%20with%20class%20reweighting%20and%20targeted%20data%0Aaugmentation%20strategies%20are%20employed.%20The%20model%20is%20trained%20and%20evaluated%20on%20a%0Acomposite%20dataset%20derived%20from%20the%20Toyota%20Smart%20Home%20and%20ETRI%20datasets.%0AExperimental%20results%20demonstrate%20an%20F1-score%20of%200.9415%20and%20an%20overall%20accuracy%0Aof%2094.17%25%2C%20outperforming%20benchmark%203D-CNN%20architectures%20such%20as%20C3D%2C%20ResNet3D%2C%0Aand%20MC3_18%20by%20up%20to%203%25.%20These%20results%20highlight%20the%20potential%20of%20event-based%0Adeep%20learning%20for%20developing%20accurate%2C%20efficient%2C%20and%20privacy-aware%20human%0Aaction%20recognition%20systems%20suitable%20for%20real-world%20edge%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03665v1&entry.124074799=Read"},
{"title": "MAROON: A Framework for the Joint Characterization of Near-Field\n  High-Resolution Radar and Optical Depth Imaging Techniques", "author": "Vanessa Wirth and Johanna Br\u00e4unig and Nikolai Hofmann and Martin Vossiek and Tim Weyrich and Marc Stamminger", "abstract": "  Utilizing the complementary strengths of wavelength-specific range or depth\nsensors is crucial for robust computer-assisted tasks such as autonomous\ndriving. Despite this, there is still little research done at the intersection\nof optical depth sensors and radars operating close range, where the target is\ndecimeters away from the sensors. Together with a growing interest in\nhigh-resolution imaging radars operating in the near field, the question arises\nhow these sensors behave in comparison to their traditional optical\ncounterparts.\n  In this work, we take on the unique challenge of jointly characterizing depth\nimagers from both, the optical and radio-frequency domain using a multimodal\nspatial calibration. We collect data from four depth imagers, with three\noptical sensors of varying operation principle and an imaging radar. We provide\na comprehensive evaluation of their depth measurements with respect to distinct\nobject materials, geometries, and object-to-sensor distances. Specifically, we\nreveal scattering effects of partially transmissive materials and investigate\nthe response of radio-frequency signals. All object measurements will be made\npublic in form of a multimodal dataset, called MAROON.\n", "link": "http://arxiv.org/abs/2411.00527v3", "date": "2025-11-05", "relevancy": 2.1427, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5435}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5382}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAROON%3A%20A%20Framework%20for%20the%20Joint%20Characterization%20of%20Near-Field%0A%20%20High-Resolution%20Radar%20and%20Optical%20Depth%20Imaging%20Techniques&body=Title%3A%20MAROON%3A%20A%20Framework%20for%20the%20Joint%20Characterization%20of%20Near-Field%0A%20%20High-Resolution%20Radar%20and%20Optical%20Depth%20Imaging%20Techniques%0AAuthor%3A%20Vanessa%20Wirth%20and%20Johanna%20Br%C3%A4unig%20and%20Nikolai%20Hofmann%20and%20Martin%20Vossiek%20and%20Tim%20Weyrich%20and%20Marc%20Stamminger%0AAbstract%3A%20%20%20Utilizing%20the%20complementary%20strengths%20of%20wavelength-specific%20range%20or%20depth%0Asensors%20is%20crucial%20for%20robust%20computer-assisted%20tasks%20such%20as%20autonomous%0Adriving.%20Despite%20this%2C%20there%20is%20still%20little%20research%20done%20at%20the%20intersection%0Aof%20optical%20depth%20sensors%20and%20radars%20operating%20close%20range%2C%20where%20the%20target%20is%0Adecimeters%20away%20from%20the%20sensors.%20Together%20with%20a%20growing%20interest%20in%0Ahigh-resolution%20imaging%20radars%20operating%20in%20the%20near%20field%2C%20the%20question%20arises%0Ahow%20these%20sensors%20behave%20in%20comparison%20to%20their%20traditional%20optical%0Acounterparts.%0A%20%20In%20this%20work%2C%20we%20take%20on%20the%20unique%20challenge%20of%20jointly%20characterizing%20depth%0Aimagers%20from%20both%2C%20the%20optical%20and%20radio-frequency%20domain%20using%20a%20multimodal%0Aspatial%20calibration.%20We%20collect%20data%20from%20four%20depth%20imagers%2C%20with%20three%0Aoptical%20sensors%20of%20varying%20operation%20principle%20and%20an%20imaging%20radar.%20We%20provide%0Aa%20comprehensive%20evaluation%20of%20their%20depth%20measurements%20with%20respect%20to%20distinct%0Aobject%20materials%2C%20geometries%2C%20and%20object-to-sensor%20distances.%20Specifically%2C%20we%0Areveal%20scattering%20effects%20of%20partially%20transmissive%20materials%20and%20investigate%0Athe%20response%20of%20radio-frequency%20signals.%20All%20object%20measurements%20will%20be%20made%0Apublic%20in%20form%20of%20a%20multimodal%20dataset%2C%20called%20MAROON.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.00527v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAROON%253A%2520A%2520Framework%2520for%2520the%2520Joint%2520Characterization%2520of%2520Near-Field%250A%2520%2520High-Resolution%2520Radar%2520and%2520Optical%2520Depth%2520Imaging%2520Techniques%26entry.906535625%3DVanessa%2520Wirth%2520and%2520Johanna%2520Br%25C3%25A4unig%2520and%2520Nikolai%2520Hofmann%2520and%2520Martin%2520Vossiek%2520and%2520Tim%2520Weyrich%2520and%2520Marc%2520Stamminger%26entry.1292438233%3D%2520%2520Utilizing%2520the%2520complementary%2520strengths%2520of%2520wavelength-specific%2520range%2520or%2520depth%250Asensors%2520is%2520crucial%2520for%2520robust%2520computer-assisted%2520tasks%2520such%2520as%2520autonomous%250Adriving.%2520Despite%2520this%252C%2520there%2520is%2520still%2520little%2520research%2520done%2520at%2520the%2520intersection%250Aof%2520optical%2520depth%2520sensors%2520and%2520radars%2520operating%2520close%2520range%252C%2520where%2520the%2520target%2520is%250Adecimeters%2520away%2520from%2520the%2520sensors.%2520Together%2520with%2520a%2520growing%2520interest%2520in%250Ahigh-resolution%2520imaging%2520radars%2520operating%2520in%2520the%2520near%2520field%252C%2520the%2520question%2520arises%250Ahow%2520these%2520sensors%2520behave%2520in%2520comparison%2520to%2520their%2520traditional%2520optical%250Acounterparts.%250A%2520%2520In%2520this%2520work%252C%2520we%2520take%2520on%2520the%2520unique%2520challenge%2520of%2520jointly%2520characterizing%2520depth%250Aimagers%2520from%2520both%252C%2520the%2520optical%2520and%2520radio-frequency%2520domain%2520using%2520a%2520multimodal%250Aspatial%2520calibration.%2520We%2520collect%2520data%2520from%2520four%2520depth%2520imagers%252C%2520with%2520three%250Aoptical%2520sensors%2520of%2520varying%2520operation%2520principle%2520and%2520an%2520imaging%2520radar.%2520We%2520provide%250Aa%2520comprehensive%2520evaluation%2520of%2520their%2520depth%2520measurements%2520with%2520respect%2520to%2520distinct%250Aobject%2520materials%252C%2520geometries%252C%2520and%2520object-to-sensor%2520distances.%2520Specifically%252C%2520we%250Areveal%2520scattering%2520effects%2520of%2520partially%2520transmissive%2520materials%2520and%2520investigate%250Athe%2520response%2520of%2520radio-frequency%2520signals.%2520All%2520object%2520measurements%2520will%2520be%2520made%250Apublic%2520in%2520form%2520of%2520a%2520multimodal%2520dataset%252C%2520called%2520MAROON.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00527v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAROON%3A%20A%20Framework%20for%20the%20Joint%20Characterization%20of%20Near-Field%0A%20%20High-Resolution%20Radar%20and%20Optical%20Depth%20Imaging%20Techniques&entry.906535625=Vanessa%20Wirth%20and%20Johanna%20Br%C3%A4unig%20and%20Nikolai%20Hofmann%20and%20Martin%20Vossiek%20and%20Tim%20Weyrich%20and%20Marc%20Stamminger&entry.1292438233=%20%20Utilizing%20the%20complementary%20strengths%20of%20wavelength-specific%20range%20or%20depth%0Asensors%20is%20crucial%20for%20robust%20computer-assisted%20tasks%20such%20as%20autonomous%0Adriving.%20Despite%20this%2C%20there%20is%20still%20little%20research%20done%20at%20the%20intersection%0Aof%20optical%20depth%20sensors%20and%20radars%20operating%20close%20range%2C%20where%20the%20target%20is%0Adecimeters%20away%20from%20the%20sensors.%20Together%20with%20a%20growing%20interest%20in%0Ahigh-resolution%20imaging%20radars%20operating%20in%20the%20near%20field%2C%20the%20question%20arises%0Ahow%20these%20sensors%20behave%20in%20comparison%20to%20their%20traditional%20optical%0Acounterparts.%0A%20%20In%20this%20work%2C%20we%20take%20on%20the%20unique%20challenge%20of%20jointly%20characterizing%20depth%0Aimagers%20from%20both%2C%20the%20optical%20and%20radio-frequency%20domain%20using%20a%20multimodal%0Aspatial%20calibration.%20We%20collect%20data%20from%20four%20depth%20imagers%2C%20with%20three%0Aoptical%20sensors%20of%20varying%20operation%20principle%20and%20an%20imaging%20radar.%20We%20provide%0Aa%20comprehensive%20evaluation%20of%20their%20depth%20measurements%20with%20respect%20to%20distinct%0Aobject%20materials%2C%20geometries%2C%20and%20object-to-sensor%20distances.%20Specifically%2C%20we%0Areveal%20scattering%20effects%20of%20partially%20transmissive%20materials%20and%20investigate%0Athe%20response%20of%20radio-frequency%20signals.%20All%20object%20measurements%20will%20be%20made%0Apublic%20in%20form%20of%20a%20multimodal%20dataset%2C%20called%20MAROON.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.00527v3&entry.124074799=Read"},
{"title": "Navigating High Dimensional Concept Space with Metalearning", "author": "Max Gupta", "abstract": "  Rapidly learning abstract concepts from limited examples is a hallmark of\nhuman intelligence. This work investigates whether gradient-based meta-learning\ncan equip neural networks with inductive biases for efficient few-shot\nacquisition of discrete concepts. I compare meta-learning methods against a\nsupervised learning baseline on Boolean concepts (logical statements) generated\nby a probabilistic context-free grammar (PCFG). By systematically varying\nconcept dimensionality (number of features) and recursive compositionality\n(depth of grammar recursion), I delineate between complexity regimes in which\nmeta-learning robustly improves few-shot concept learning and regimes in which\nit does not. Meta-learners are much better able to handle compositional\ncomplexity than featural complexity. I highlight some reasons for this with a\nrepresentational analysis of the weights of meta-learners and a loss landscape\nanalysis demonstrating how featural complexity increases the roughness of loss\ntrajectories, allowing curvature-aware optimization to be more effective than\nfirst-order methods. I find improvements in out-of-distribution generalization\non complex concepts by increasing the number of adaptation steps in meta-SGD,\nwhere adaptation acts as a way of encouraging exploration of rougher loss\nbasins. Overall, this work highlights the intricacies of learning compositional\nversus featural complexity in high dimensional concept spaces and provides a\nroad to understanding the role of 2nd order methods and extended gradient\nadaptation in few-shot concept learning.\n", "link": "http://arxiv.org/abs/2508.01948v3", "date": "2025-11-05", "relevancy": 2.1247, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5432}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5257}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Navigating%20High%20Dimensional%20Concept%20Space%20with%20Metalearning&body=Title%3A%20Navigating%20High%20Dimensional%20Concept%20Space%20with%20Metalearning%0AAuthor%3A%20Max%20Gupta%0AAbstract%3A%20%20%20Rapidly%20learning%20abstract%20concepts%20from%20limited%20examples%20is%20a%20hallmark%20of%0Ahuman%20intelligence.%20This%20work%20investigates%20whether%20gradient-based%20meta-learning%0Acan%20equip%20neural%20networks%20with%20inductive%20biases%20for%20efficient%20few-shot%0Aacquisition%20of%20discrete%20concepts.%20I%20compare%20meta-learning%20methods%20against%20a%0Asupervised%20learning%20baseline%20on%20Boolean%20concepts%20%28logical%20statements%29%20generated%0Aby%20a%20probabilistic%20context-free%20grammar%20%28PCFG%29.%20By%20systematically%20varying%0Aconcept%20dimensionality%20%28number%20of%20features%29%20and%20recursive%20compositionality%0A%28depth%20of%20grammar%20recursion%29%2C%20I%20delineate%20between%20complexity%20regimes%20in%20which%0Ameta-learning%20robustly%20improves%20few-shot%20concept%20learning%20and%20regimes%20in%20which%0Ait%20does%20not.%20Meta-learners%20are%20much%20better%20able%20to%20handle%20compositional%0Acomplexity%20than%20featural%20complexity.%20I%20highlight%20some%20reasons%20for%20this%20with%20a%0Arepresentational%20analysis%20of%20the%20weights%20of%20meta-learners%20and%20a%20loss%20landscape%0Aanalysis%20demonstrating%20how%20featural%20complexity%20increases%20the%20roughness%20of%20loss%0Atrajectories%2C%20allowing%20curvature-aware%20optimization%20to%20be%20more%20effective%20than%0Afirst-order%20methods.%20I%20find%20improvements%20in%20out-of-distribution%20generalization%0Aon%20complex%20concepts%20by%20increasing%20the%20number%20of%20adaptation%20steps%20in%20meta-SGD%2C%0Awhere%20adaptation%20acts%20as%20a%20way%20of%20encouraging%20exploration%20of%20rougher%20loss%0Abasins.%20Overall%2C%20this%20work%20highlights%20the%20intricacies%20of%20learning%20compositional%0Aversus%20featural%20complexity%20in%20high%20dimensional%20concept%20spaces%20and%20provides%20a%0Aroad%20to%20understanding%20the%20role%20of%202nd%20order%20methods%20and%20extended%20gradient%0Aadaptation%20in%20few-shot%20concept%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.01948v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNavigating%2520High%2520Dimensional%2520Concept%2520Space%2520with%2520Metalearning%26entry.906535625%3DMax%2520Gupta%26entry.1292438233%3D%2520%2520Rapidly%2520learning%2520abstract%2520concepts%2520from%2520limited%2520examples%2520is%2520a%2520hallmark%2520of%250Ahuman%2520intelligence.%2520This%2520work%2520investigates%2520whether%2520gradient-based%2520meta-learning%250Acan%2520equip%2520neural%2520networks%2520with%2520inductive%2520biases%2520for%2520efficient%2520few-shot%250Aacquisition%2520of%2520discrete%2520concepts.%2520I%2520compare%2520meta-learning%2520methods%2520against%2520a%250Asupervised%2520learning%2520baseline%2520on%2520Boolean%2520concepts%2520%2528logical%2520statements%2529%2520generated%250Aby%2520a%2520probabilistic%2520context-free%2520grammar%2520%2528PCFG%2529.%2520By%2520systematically%2520varying%250Aconcept%2520dimensionality%2520%2528number%2520of%2520features%2529%2520and%2520recursive%2520compositionality%250A%2528depth%2520of%2520grammar%2520recursion%2529%252C%2520I%2520delineate%2520between%2520complexity%2520regimes%2520in%2520which%250Ameta-learning%2520robustly%2520improves%2520few-shot%2520concept%2520learning%2520and%2520regimes%2520in%2520which%250Ait%2520does%2520not.%2520Meta-learners%2520are%2520much%2520better%2520able%2520to%2520handle%2520compositional%250Acomplexity%2520than%2520featural%2520complexity.%2520I%2520highlight%2520some%2520reasons%2520for%2520this%2520with%2520a%250Arepresentational%2520analysis%2520of%2520the%2520weights%2520of%2520meta-learners%2520and%2520a%2520loss%2520landscape%250Aanalysis%2520demonstrating%2520how%2520featural%2520complexity%2520increases%2520the%2520roughness%2520of%2520loss%250Atrajectories%252C%2520allowing%2520curvature-aware%2520optimization%2520to%2520be%2520more%2520effective%2520than%250Afirst-order%2520methods.%2520I%2520find%2520improvements%2520in%2520out-of-distribution%2520generalization%250Aon%2520complex%2520concepts%2520by%2520increasing%2520the%2520number%2520of%2520adaptation%2520steps%2520in%2520meta-SGD%252C%250Awhere%2520adaptation%2520acts%2520as%2520a%2520way%2520of%2520encouraging%2520exploration%2520of%2520rougher%2520loss%250Abasins.%2520Overall%252C%2520this%2520work%2520highlights%2520the%2520intricacies%2520of%2520learning%2520compositional%250Aversus%2520featural%2520complexity%2520in%2520high%2520dimensional%2520concept%2520spaces%2520and%2520provides%2520a%250Aroad%2520to%2520understanding%2520the%2520role%2520of%25202nd%2520order%2520methods%2520and%2520extended%2520gradient%250Aadaptation%2520in%2520few-shot%2520concept%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.01948v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Navigating%20High%20Dimensional%20Concept%20Space%20with%20Metalearning&entry.906535625=Max%20Gupta&entry.1292438233=%20%20Rapidly%20learning%20abstract%20concepts%20from%20limited%20examples%20is%20a%20hallmark%20of%0Ahuman%20intelligence.%20This%20work%20investigates%20whether%20gradient-based%20meta-learning%0Acan%20equip%20neural%20networks%20with%20inductive%20biases%20for%20efficient%20few-shot%0Aacquisition%20of%20discrete%20concepts.%20I%20compare%20meta-learning%20methods%20against%20a%0Asupervised%20learning%20baseline%20on%20Boolean%20concepts%20%28logical%20statements%29%20generated%0Aby%20a%20probabilistic%20context-free%20grammar%20%28PCFG%29.%20By%20systematically%20varying%0Aconcept%20dimensionality%20%28number%20of%20features%29%20and%20recursive%20compositionality%0A%28depth%20of%20grammar%20recursion%29%2C%20I%20delineate%20between%20complexity%20regimes%20in%20which%0Ameta-learning%20robustly%20improves%20few-shot%20concept%20learning%20and%20regimes%20in%20which%0Ait%20does%20not.%20Meta-learners%20are%20much%20better%20able%20to%20handle%20compositional%0Acomplexity%20than%20featural%20complexity.%20I%20highlight%20some%20reasons%20for%20this%20with%20a%0Arepresentational%20analysis%20of%20the%20weights%20of%20meta-learners%20and%20a%20loss%20landscape%0Aanalysis%20demonstrating%20how%20featural%20complexity%20increases%20the%20roughness%20of%20loss%0Atrajectories%2C%20allowing%20curvature-aware%20optimization%20to%20be%20more%20effective%20than%0Afirst-order%20methods.%20I%20find%20improvements%20in%20out-of-distribution%20generalization%0Aon%20complex%20concepts%20by%20increasing%20the%20number%20of%20adaptation%20steps%20in%20meta-SGD%2C%0Awhere%20adaptation%20acts%20as%20a%20way%20of%20encouraging%20exploration%20of%20rougher%20loss%0Abasins.%20Overall%2C%20this%20work%20highlights%20the%20intricacies%20of%20learning%20compositional%0Aversus%20featural%20complexity%20in%20high%20dimensional%20concept%20spaces%20and%20provides%20a%0Aroad%20to%20understanding%20the%20role%20of%202nd%20order%20methods%20and%20extended%20gradient%0Aadaptation%20in%20few-shot%20concept%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.01948v3&entry.124074799=Read"},
{"title": "Uncovering Code Insights: Leveraging GitHub Artifacts for Deeper Code\n  Understanding", "author": "Ziv Nevo and Orna Raz and Karen Yorav", "abstract": "  Understanding the purpose of source code is a critical task in software\nmaintenance, onboarding, and modernization. While large language models (LLMs)\nhave shown promise in generating code explanations, they often lack grounding\nin the broader software engineering context. We propose a novel approach that\nleverages natural language artifacts from GitHub -- such as pull request\ndescriptions, issue descriptions and discussions, and commit messages -- to\nenhance LLM-based code understanding. Our system consists of three components:\none that extracts and structures relevant GitHub context, another that uses\nthis context to generate high-level explanations of the code's purpose, and a\nthird that validates the explanation. We implemented this as a standalone tool,\nas well as a server within the Model Context Protocol (MCP), enabling\nintegration with other AI-assisted development tools. Our main use case is that\nof enhancing a standard LLM-based code explanation with code insights that our\nsystem generates. To evaluate explanations' quality, we conducted a small scale\nuser study, with developers of several open projects, as well as developers of\nproprietary projects. Our user study indicates that when insights are generated\nthey often are helpful and non trivial, and are free from hallucinations.\n", "link": "http://arxiv.org/abs/2511.03549v1", "date": "2025-11-05", "relevancy": 2.1139, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5318}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5318}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncovering%20Code%20Insights%3A%20Leveraging%20GitHub%20Artifacts%20for%20Deeper%20Code%0A%20%20Understanding&body=Title%3A%20Uncovering%20Code%20Insights%3A%20Leveraging%20GitHub%20Artifacts%20for%20Deeper%20Code%0A%20%20Understanding%0AAuthor%3A%20Ziv%20Nevo%20and%20Orna%20Raz%20and%20Karen%20Yorav%0AAbstract%3A%20%20%20Understanding%20the%20purpose%20of%20source%20code%20is%20a%20critical%20task%20in%20software%0Amaintenance%2C%20onboarding%2C%20and%20modernization.%20While%20large%20language%20models%20%28LLMs%29%0Ahave%20shown%20promise%20in%20generating%20code%20explanations%2C%20they%20often%20lack%20grounding%0Ain%20the%20broader%20software%20engineering%20context.%20We%20propose%20a%20novel%20approach%20that%0Aleverages%20natural%20language%20artifacts%20from%20GitHub%20--%20such%20as%20pull%20request%0Adescriptions%2C%20issue%20descriptions%20and%20discussions%2C%20and%20commit%20messages%20--%20to%0Aenhance%20LLM-based%20code%20understanding.%20Our%20system%20consists%20of%20three%20components%3A%0Aone%20that%20extracts%20and%20structures%20relevant%20GitHub%20context%2C%20another%20that%20uses%0Athis%20context%20to%20generate%20high-level%20explanations%20of%20the%20code%27s%20purpose%2C%20and%20a%0Athird%20that%20validates%20the%20explanation.%20We%20implemented%20this%20as%20a%20standalone%20tool%2C%0Aas%20well%20as%20a%20server%20within%20the%20Model%20Context%20Protocol%20%28MCP%29%2C%20enabling%0Aintegration%20with%20other%20AI-assisted%20development%20tools.%20Our%20main%20use%20case%20is%20that%0Aof%20enhancing%20a%20standard%20LLM-based%20code%20explanation%20with%20code%20insights%20that%20our%0Asystem%20generates.%20To%20evaluate%20explanations%27%20quality%2C%20we%20conducted%20a%20small%20scale%0Auser%20study%2C%20with%20developers%20of%20several%20open%20projects%2C%20as%20well%20as%20developers%20of%0Aproprietary%20projects.%20Our%20user%20study%20indicates%20that%20when%20insights%20are%20generated%0Athey%20often%20are%20helpful%20and%20non%20trivial%2C%20and%20are%20free%20from%20hallucinations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03549v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncovering%2520Code%2520Insights%253A%2520Leveraging%2520GitHub%2520Artifacts%2520for%2520Deeper%2520Code%250A%2520%2520Understanding%26entry.906535625%3DZiv%2520Nevo%2520and%2520Orna%2520Raz%2520and%2520Karen%2520Yorav%26entry.1292438233%3D%2520%2520Understanding%2520the%2520purpose%2520of%2520source%2520code%2520is%2520a%2520critical%2520task%2520in%2520software%250Amaintenance%252C%2520onboarding%252C%2520and%2520modernization.%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%250Ahave%2520shown%2520promise%2520in%2520generating%2520code%2520explanations%252C%2520they%2520often%2520lack%2520grounding%250Ain%2520the%2520broader%2520software%2520engineering%2520context.%2520We%2520propose%2520a%2520novel%2520approach%2520that%250Aleverages%2520natural%2520language%2520artifacts%2520from%2520GitHub%2520--%2520such%2520as%2520pull%2520request%250Adescriptions%252C%2520issue%2520descriptions%2520and%2520discussions%252C%2520and%2520commit%2520messages%2520--%2520to%250Aenhance%2520LLM-based%2520code%2520understanding.%2520Our%2520system%2520consists%2520of%2520three%2520components%253A%250Aone%2520that%2520extracts%2520and%2520structures%2520relevant%2520GitHub%2520context%252C%2520another%2520that%2520uses%250Athis%2520context%2520to%2520generate%2520high-level%2520explanations%2520of%2520the%2520code%2527s%2520purpose%252C%2520and%2520a%250Athird%2520that%2520validates%2520the%2520explanation.%2520We%2520implemented%2520this%2520as%2520a%2520standalone%2520tool%252C%250Aas%2520well%2520as%2520a%2520server%2520within%2520the%2520Model%2520Context%2520Protocol%2520%2528MCP%2529%252C%2520enabling%250Aintegration%2520with%2520other%2520AI-assisted%2520development%2520tools.%2520Our%2520main%2520use%2520case%2520is%2520that%250Aof%2520enhancing%2520a%2520standard%2520LLM-based%2520code%2520explanation%2520with%2520code%2520insights%2520that%2520our%250Asystem%2520generates.%2520To%2520evaluate%2520explanations%2527%2520quality%252C%2520we%2520conducted%2520a%2520small%2520scale%250Auser%2520study%252C%2520with%2520developers%2520of%2520several%2520open%2520projects%252C%2520as%2520well%2520as%2520developers%2520of%250Aproprietary%2520projects.%2520Our%2520user%2520study%2520indicates%2520that%2520when%2520insights%2520are%2520generated%250Athey%2520often%2520are%2520helpful%2520and%2520non%2520trivial%252C%2520and%2520are%2520free%2520from%2520hallucinations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03549v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncovering%20Code%20Insights%3A%20Leveraging%20GitHub%20Artifacts%20for%20Deeper%20Code%0A%20%20Understanding&entry.906535625=Ziv%20Nevo%20and%20Orna%20Raz%20and%20Karen%20Yorav&entry.1292438233=%20%20Understanding%20the%20purpose%20of%20source%20code%20is%20a%20critical%20task%20in%20software%0Amaintenance%2C%20onboarding%2C%20and%20modernization.%20While%20large%20language%20models%20%28LLMs%29%0Ahave%20shown%20promise%20in%20generating%20code%20explanations%2C%20they%20often%20lack%20grounding%0Ain%20the%20broader%20software%20engineering%20context.%20We%20propose%20a%20novel%20approach%20that%0Aleverages%20natural%20language%20artifacts%20from%20GitHub%20--%20such%20as%20pull%20request%0Adescriptions%2C%20issue%20descriptions%20and%20discussions%2C%20and%20commit%20messages%20--%20to%0Aenhance%20LLM-based%20code%20understanding.%20Our%20system%20consists%20of%20three%20components%3A%0Aone%20that%20extracts%20and%20structures%20relevant%20GitHub%20context%2C%20another%20that%20uses%0Athis%20context%20to%20generate%20high-level%20explanations%20of%20the%20code%27s%20purpose%2C%20and%20a%0Athird%20that%20validates%20the%20explanation.%20We%20implemented%20this%20as%20a%20standalone%20tool%2C%0Aas%20well%20as%20a%20server%20within%20the%20Model%20Context%20Protocol%20%28MCP%29%2C%20enabling%0Aintegration%20with%20other%20AI-assisted%20development%20tools.%20Our%20main%20use%20case%20is%20that%0Aof%20enhancing%20a%20standard%20LLM-based%20code%20explanation%20with%20code%20insights%20that%20our%0Asystem%20generates.%20To%20evaluate%20explanations%27%20quality%2C%20we%20conducted%20a%20small%20scale%0Auser%20study%2C%20with%20developers%20of%20several%20open%20projects%2C%20as%20well%20as%20developers%20of%0Aproprietary%20projects.%20Our%20user%20study%20indicates%20that%20when%20insights%20are%20generated%0Athey%20often%20are%20helpful%20and%20non%20trivial%2C%20and%20are%20free%20from%20hallucinations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03549v1&entry.124074799=Read"},
{"title": "Dynamical loss functions shape landscape topography and improve learning\n  in artificial neural networks", "author": "Eduardo Lavin Pallero and Miguel Ruiz-Garcia", "abstract": "  Dynamical loss functions are derived from standard loss functions used in\nsupervised classification tasks, but are modified so that the contribution from\neach class periodically increases and decreases. These oscillations globally\nalter the loss landscape without affecting the global minima. In this paper, we\ndemonstrate how to transform cross-entropy and mean squared error into\ndynamical loss functions. We begin by discussing the impact of increasing the\nsize of the neural network or the learning rate on the depth and sharpness of\nthe minima that the system explores. Building on this intuition, we propose\nseveral versions of dynamical loss functions and use a simple classification\nproblem where we can show how they significantly improve validation accuracy\nfor networks of varying sizes. Finally, we explore how the landscape of these\ndynamical loss functions evolves during training, highlighting the emergence of\ninstabilities that may be linked to edge-of-instability minimization.\n", "link": "http://arxiv.org/abs/2410.10690v3", "date": "2025-11-05", "relevancy": 2.0992, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5713}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4967}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamical%20loss%20functions%20shape%20landscape%20topography%20and%20improve%20learning%0A%20%20in%20artificial%20neural%20networks&body=Title%3A%20Dynamical%20loss%20functions%20shape%20landscape%20topography%20and%20improve%20learning%0A%20%20in%20artificial%20neural%20networks%0AAuthor%3A%20Eduardo%20Lavin%20Pallero%20and%20Miguel%20Ruiz-Garcia%0AAbstract%3A%20%20%20Dynamical%20loss%20functions%20are%20derived%20from%20standard%20loss%20functions%20used%20in%0Asupervised%20classification%20tasks%2C%20but%20are%20modified%20so%20that%20the%20contribution%20from%0Aeach%20class%20periodically%20increases%20and%20decreases.%20These%20oscillations%20globally%0Aalter%20the%20loss%20landscape%20without%20affecting%20the%20global%20minima.%20In%20this%20paper%2C%20we%0Ademonstrate%20how%20to%20transform%20cross-entropy%20and%20mean%20squared%20error%20into%0Adynamical%20loss%20functions.%20We%20begin%20by%20discussing%20the%20impact%20of%20increasing%20the%0Asize%20of%20the%20neural%20network%20or%20the%20learning%20rate%20on%20the%20depth%20and%20sharpness%20of%0Athe%20minima%20that%20the%20system%20explores.%20Building%20on%20this%20intuition%2C%20we%20propose%0Aseveral%20versions%20of%20dynamical%20loss%20functions%20and%20use%20a%20simple%20classification%0Aproblem%20where%20we%20can%20show%20how%20they%20significantly%20improve%20validation%20accuracy%0Afor%20networks%20of%20varying%20sizes.%20Finally%2C%20we%20explore%20how%20the%20landscape%20of%20these%0Adynamical%20loss%20functions%20evolves%20during%20training%2C%20highlighting%20the%20emergence%20of%0Ainstabilities%20that%20may%20be%20linked%20to%20edge-of-instability%20minimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10690v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamical%2520loss%2520functions%2520shape%2520landscape%2520topography%2520and%2520improve%2520learning%250A%2520%2520in%2520artificial%2520neural%2520networks%26entry.906535625%3DEduardo%2520Lavin%2520Pallero%2520and%2520Miguel%2520Ruiz-Garcia%26entry.1292438233%3D%2520%2520Dynamical%2520loss%2520functions%2520are%2520derived%2520from%2520standard%2520loss%2520functions%2520used%2520in%250Asupervised%2520classification%2520tasks%252C%2520but%2520are%2520modified%2520so%2520that%2520the%2520contribution%2520from%250Aeach%2520class%2520periodically%2520increases%2520and%2520decreases.%2520These%2520oscillations%2520globally%250Aalter%2520the%2520loss%2520landscape%2520without%2520affecting%2520the%2520global%2520minima.%2520In%2520this%2520paper%252C%2520we%250Ademonstrate%2520how%2520to%2520transform%2520cross-entropy%2520and%2520mean%2520squared%2520error%2520into%250Adynamical%2520loss%2520functions.%2520We%2520begin%2520by%2520discussing%2520the%2520impact%2520of%2520increasing%2520the%250Asize%2520of%2520the%2520neural%2520network%2520or%2520the%2520learning%2520rate%2520on%2520the%2520depth%2520and%2520sharpness%2520of%250Athe%2520minima%2520that%2520the%2520system%2520explores.%2520Building%2520on%2520this%2520intuition%252C%2520we%2520propose%250Aseveral%2520versions%2520of%2520dynamical%2520loss%2520functions%2520and%2520use%2520a%2520simple%2520classification%250Aproblem%2520where%2520we%2520can%2520show%2520how%2520they%2520significantly%2520improve%2520validation%2520accuracy%250Afor%2520networks%2520of%2520varying%2520sizes.%2520Finally%252C%2520we%2520explore%2520how%2520the%2520landscape%2520of%2520these%250Adynamical%2520loss%2520functions%2520evolves%2520during%2520training%252C%2520highlighting%2520the%2520emergence%2520of%250Ainstabilities%2520that%2520may%2520be%2520linked%2520to%2520edge-of-instability%2520minimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10690v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamical%20loss%20functions%20shape%20landscape%20topography%20and%20improve%20learning%0A%20%20in%20artificial%20neural%20networks&entry.906535625=Eduardo%20Lavin%20Pallero%20and%20Miguel%20Ruiz-Garcia&entry.1292438233=%20%20Dynamical%20loss%20functions%20are%20derived%20from%20standard%20loss%20functions%20used%20in%0Asupervised%20classification%20tasks%2C%20but%20are%20modified%20so%20that%20the%20contribution%20from%0Aeach%20class%20periodically%20increases%20and%20decreases.%20These%20oscillations%20globally%0Aalter%20the%20loss%20landscape%20without%20affecting%20the%20global%20minima.%20In%20this%20paper%2C%20we%0Ademonstrate%20how%20to%20transform%20cross-entropy%20and%20mean%20squared%20error%20into%0Adynamical%20loss%20functions.%20We%20begin%20by%20discussing%20the%20impact%20of%20increasing%20the%0Asize%20of%20the%20neural%20network%20or%20the%20learning%20rate%20on%20the%20depth%20and%20sharpness%20of%0Athe%20minima%20that%20the%20system%20explores.%20Building%20on%20this%20intuition%2C%20we%20propose%0Aseveral%20versions%20of%20dynamical%20loss%20functions%20and%20use%20a%20simple%20classification%0Aproblem%20where%20we%20can%20show%20how%20they%20significantly%20improve%20validation%20accuracy%0Afor%20networks%20of%20varying%20sizes.%20Finally%2C%20we%20explore%20how%20the%20landscape%20of%20these%0Adynamical%20loss%20functions%20evolves%20during%20training%2C%20highlighting%20the%20emergence%20of%0Ainstabilities%20that%20may%20be%20linked%20to%20edge-of-instability%20minimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10690v3&entry.124074799=Read"},
{"title": "Signal Intensity-weighted coordinate channels improve learning stability\n  and generalisation in 1D and 2D CNNs in localisation tasks on biomedical\n  signals", "author": "Vittal L. Rao", "abstract": "  Localisation tasks in biomedical data often require models to learn\nmeaningful spatial or temporal relationships from signals with complex\nintensity distributions. A common strategy, exemplified by CoordConv layers, is\nto append coordinate channels to convolutional inputs, enabling networks to\nlearn absolute positions. In this work, we propose a signal intensity-weighted\ncoordinate representation that replaces the pure coordinate channels with\nchannels scaled by local signal intensity. This modification embeds an\nintensity-position coupling directly in the input representation, introducing a\nsimple and modality-agnostic inductive bias. We evaluate the approach on two\ndistinct localisation problems: (i) predicting the time of morphological\ntransition in 20-second, two-lead ECG signals, and (ii) regressing the\ncoordinates of nuclear centres in cytological images from the SiPaKMeD dataset.\nIn both cases, the proposed representation yields faster convergence and higher\ngeneralisation performance relative to conventional coordinate-channel\napproaches, demonstrating its effectiveness across both one-dimensional and\ntwo-dimensional biomedical signals.\n", "link": "http://arxiv.org/abs/2511.03645v1", "date": "2025-11-05", "relevancy": 2.0909, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5482}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5075}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Signal%20Intensity-weighted%20coordinate%20channels%20improve%20learning%20stability%0A%20%20and%20generalisation%20in%201D%20and%202D%20CNNs%20in%20localisation%20tasks%20on%20biomedical%0A%20%20signals&body=Title%3A%20Signal%20Intensity-weighted%20coordinate%20channels%20improve%20learning%20stability%0A%20%20and%20generalisation%20in%201D%20and%202D%20CNNs%20in%20localisation%20tasks%20on%20biomedical%0A%20%20signals%0AAuthor%3A%20Vittal%20L.%20Rao%0AAbstract%3A%20%20%20Localisation%20tasks%20in%20biomedical%20data%20often%20require%20models%20to%20learn%0Ameaningful%20spatial%20or%20temporal%20relationships%20from%20signals%20with%20complex%0Aintensity%20distributions.%20A%20common%20strategy%2C%20exemplified%20by%20CoordConv%20layers%2C%20is%0Ato%20append%20coordinate%20channels%20to%20convolutional%20inputs%2C%20enabling%20networks%20to%0Alearn%20absolute%20positions.%20In%20this%20work%2C%20we%20propose%20a%20signal%20intensity-weighted%0Acoordinate%20representation%20that%20replaces%20the%20pure%20coordinate%20channels%20with%0Achannels%20scaled%20by%20local%20signal%20intensity.%20This%20modification%20embeds%20an%0Aintensity-position%20coupling%20directly%20in%20the%20input%20representation%2C%20introducing%20a%0Asimple%20and%20modality-agnostic%20inductive%20bias.%20We%20evaluate%20the%20approach%20on%20two%0Adistinct%20localisation%20problems%3A%20%28i%29%20predicting%20the%20time%20of%20morphological%0Atransition%20in%2020-second%2C%20two-lead%20ECG%20signals%2C%20and%20%28ii%29%20regressing%20the%0Acoordinates%20of%20nuclear%20centres%20in%20cytological%20images%20from%20the%20SiPaKMeD%20dataset.%0AIn%20both%20cases%2C%20the%20proposed%20representation%20yields%20faster%20convergence%20and%20higher%0Ageneralisation%20performance%20relative%20to%20conventional%20coordinate-channel%0Aapproaches%2C%20demonstrating%20its%20effectiveness%20across%20both%20one-dimensional%20and%0Atwo-dimensional%20biomedical%20signals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03645v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSignal%2520Intensity-weighted%2520coordinate%2520channels%2520improve%2520learning%2520stability%250A%2520%2520and%2520generalisation%2520in%25201D%2520and%25202D%2520CNNs%2520in%2520localisation%2520tasks%2520on%2520biomedical%250A%2520%2520signals%26entry.906535625%3DVittal%2520L.%2520Rao%26entry.1292438233%3D%2520%2520Localisation%2520tasks%2520in%2520biomedical%2520data%2520often%2520require%2520models%2520to%2520learn%250Ameaningful%2520spatial%2520or%2520temporal%2520relationships%2520from%2520signals%2520with%2520complex%250Aintensity%2520distributions.%2520A%2520common%2520strategy%252C%2520exemplified%2520by%2520CoordConv%2520layers%252C%2520is%250Ato%2520append%2520coordinate%2520channels%2520to%2520convolutional%2520inputs%252C%2520enabling%2520networks%2520to%250Alearn%2520absolute%2520positions.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520signal%2520intensity-weighted%250Acoordinate%2520representation%2520that%2520replaces%2520the%2520pure%2520coordinate%2520channels%2520with%250Achannels%2520scaled%2520by%2520local%2520signal%2520intensity.%2520This%2520modification%2520embeds%2520an%250Aintensity-position%2520coupling%2520directly%2520in%2520the%2520input%2520representation%252C%2520introducing%2520a%250Asimple%2520and%2520modality-agnostic%2520inductive%2520bias.%2520We%2520evaluate%2520the%2520approach%2520on%2520two%250Adistinct%2520localisation%2520problems%253A%2520%2528i%2529%2520predicting%2520the%2520time%2520of%2520morphological%250Atransition%2520in%252020-second%252C%2520two-lead%2520ECG%2520signals%252C%2520and%2520%2528ii%2529%2520regressing%2520the%250Acoordinates%2520of%2520nuclear%2520centres%2520in%2520cytological%2520images%2520from%2520the%2520SiPaKMeD%2520dataset.%250AIn%2520both%2520cases%252C%2520the%2520proposed%2520representation%2520yields%2520faster%2520convergence%2520and%2520higher%250Ageneralisation%2520performance%2520relative%2520to%2520conventional%2520coordinate-channel%250Aapproaches%252C%2520demonstrating%2520its%2520effectiveness%2520across%2520both%2520one-dimensional%2520and%250Atwo-dimensional%2520biomedical%2520signals.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03645v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Signal%20Intensity-weighted%20coordinate%20channels%20improve%20learning%20stability%0A%20%20and%20generalisation%20in%201D%20and%202D%20CNNs%20in%20localisation%20tasks%20on%20biomedical%0A%20%20signals&entry.906535625=Vittal%20L.%20Rao&entry.1292438233=%20%20Localisation%20tasks%20in%20biomedical%20data%20often%20require%20models%20to%20learn%0Ameaningful%20spatial%20or%20temporal%20relationships%20from%20signals%20with%20complex%0Aintensity%20distributions.%20A%20common%20strategy%2C%20exemplified%20by%20CoordConv%20layers%2C%20is%0Ato%20append%20coordinate%20channels%20to%20convolutional%20inputs%2C%20enabling%20networks%20to%0Alearn%20absolute%20positions.%20In%20this%20work%2C%20we%20propose%20a%20signal%20intensity-weighted%0Acoordinate%20representation%20that%20replaces%20the%20pure%20coordinate%20channels%20with%0Achannels%20scaled%20by%20local%20signal%20intensity.%20This%20modification%20embeds%20an%0Aintensity-position%20coupling%20directly%20in%20the%20input%20representation%2C%20introducing%20a%0Asimple%20and%20modality-agnostic%20inductive%20bias.%20We%20evaluate%20the%20approach%20on%20two%0Adistinct%20localisation%20problems%3A%20%28i%29%20predicting%20the%20time%20of%20morphological%0Atransition%20in%2020-second%2C%20two-lead%20ECG%20signals%2C%20and%20%28ii%29%20regressing%20the%0Acoordinates%20of%20nuclear%20centres%20in%20cytological%20images%20from%20the%20SiPaKMeD%20dataset.%0AIn%20both%20cases%2C%20the%20proposed%20representation%20yields%20faster%20convergence%20and%20higher%0Ageneralisation%20performance%20relative%20to%20conventional%20coordinate-channel%0Aapproaches%2C%20demonstrating%20its%20effectiveness%20across%20both%20one-dimensional%20and%0Atwo-dimensional%20biomedical%20signals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03645v1&entry.124074799=Read"},
{"title": "Learning Under Laws: A Constraint-Projected Neural PDE Solver that\n  Eliminates Hallucinations", "author": "Mainak Singha", "abstract": "  Neural networks can approximate solutions to partial differential equations,\nbut they often break the very laws they are meant to model-creating mass from\nnowhere, drifting shocks, or violating conservation and entropy. We address\nthis by training within the laws of physics rather than beside them. Our\nframework, called Constraint-Projected Learning (CPL), keeps every update\nphysically admissible by projecting network outputs onto the intersection of\nconstraint sets defined by conservation, Rankine-Hugoniot balance, entropy, and\npositivity. The projection is differentiable and adds only about 10%\ncomputational overhead, making it fully compatible with back-propagation. We\nfurther stabilize training with total-variation damping (TVD) to suppress small\noscillations and a rollout curriculum that enforces consistency over long\nprediction horizons. Together, these mechanisms eliminate both hard and soft\nviolations: conservation holds at machine precision, total-variation growth\nvanishes, and entropy and error remain bounded. On Burgers and Euler systems,\nCPL produces stable, physically lawful solutions without loss of accuracy.\nInstead of hoping neural solvers will respect physics, CPL makes that behavior\nan intrinsic property of the learning process.\n", "link": "http://arxiv.org/abs/2511.03578v1", "date": "2025-11-05", "relevancy": 2.0858, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.548}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5293}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Under%20Laws%3A%20A%20Constraint-Projected%20Neural%20PDE%20Solver%20that%0A%20%20Eliminates%20Hallucinations&body=Title%3A%20Learning%20Under%20Laws%3A%20A%20Constraint-Projected%20Neural%20PDE%20Solver%20that%0A%20%20Eliminates%20Hallucinations%0AAuthor%3A%20Mainak%20Singha%0AAbstract%3A%20%20%20Neural%20networks%20can%20approximate%20solutions%20to%20partial%20differential%20equations%2C%0Abut%20they%20often%20break%20the%20very%20laws%20they%20are%20meant%20to%20model-creating%20mass%20from%0Anowhere%2C%20drifting%20shocks%2C%20or%20violating%20conservation%20and%20entropy.%20We%20address%0Athis%20by%20training%20within%20the%20laws%20of%20physics%20rather%20than%20beside%20them.%20Our%0Aframework%2C%20called%20Constraint-Projected%20Learning%20%28CPL%29%2C%20keeps%20every%20update%0Aphysically%20admissible%20by%20projecting%20network%20outputs%20onto%20the%20intersection%20of%0Aconstraint%20sets%20defined%20by%20conservation%2C%20Rankine-Hugoniot%20balance%2C%20entropy%2C%20and%0Apositivity.%20The%20projection%20is%20differentiable%20and%20adds%20only%20about%2010%25%0Acomputational%20overhead%2C%20making%20it%20fully%20compatible%20with%20back-propagation.%20We%0Afurther%20stabilize%20training%20with%20total-variation%20damping%20%28TVD%29%20to%20suppress%20small%0Aoscillations%20and%20a%20rollout%20curriculum%20that%20enforces%20consistency%20over%20long%0Aprediction%20horizons.%20Together%2C%20these%20mechanisms%20eliminate%20both%20hard%20and%20soft%0Aviolations%3A%20conservation%20holds%20at%20machine%20precision%2C%20total-variation%20growth%0Avanishes%2C%20and%20entropy%20and%20error%20remain%20bounded.%20On%20Burgers%20and%20Euler%20systems%2C%0ACPL%20produces%20stable%2C%20physically%20lawful%20solutions%20without%20loss%20of%20accuracy.%0AInstead%20of%20hoping%20neural%20solvers%20will%20respect%20physics%2C%20CPL%20makes%20that%20behavior%0Aan%20intrinsic%20property%20of%20the%20learning%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03578v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Under%2520Laws%253A%2520A%2520Constraint-Projected%2520Neural%2520PDE%2520Solver%2520that%250A%2520%2520Eliminates%2520Hallucinations%26entry.906535625%3DMainak%2520Singha%26entry.1292438233%3D%2520%2520Neural%2520networks%2520can%2520approximate%2520solutions%2520to%2520partial%2520differential%2520equations%252C%250Abut%2520they%2520often%2520break%2520the%2520very%2520laws%2520they%2520are%2520meant%2520to%2520model-creating%2520mass%2520from%250Anowhere%252C%2520drifting%2520shocks%252C%2520or%2520violating%2520conservation%2520and%2520entropy.%2520We%2520address%250Athis%2520by%2520training%2520within%2520the%2520laws%2520of%2520physics%2520rather%2520than%2520beside%2520them.%2520Our%250Aframework%252C%2520called%2520Constraint-Projected%2520Learning%2520%2528CPL%2529%252C%2520keeps%2520every%2520update%250Aphysically%2520admissible%2520by%2520projecting%2520network%2520outputs%2520onto%2520the%2520intersection%2520of%250Aconstraint%2520sets%2520defined%2520by%2520conservation%252C%2520Rankine-Hugoniot%2520balance%252C%2520entropy%252C%2520and%250Apositivity.%2520The%2520projection%2520is%2520differentiable%2520and%2520adds%2520only%2520about%252010%2525%250Acomputational%2520overhead%252C%2520making%2520it%2520fully%2520compatible%2520with%2520back-propagation.%2520We%250Afurther%2520stabilize%2520training%2520with%2520total-variation%2520damping%2520%2528TVD%2529%2520to%2520suppress%2520small%250Aoscillations%2520and%2520a%2520rollout%2520curriculum%2520that%2520enforces%2520consistency%2520over%2520long%250Aprediction%2520horizons.%2520Together%252C%2520these%2520mechanisms%2520eliminate%2520both%2520hard%2520and%2520soft%250Aviolations%253A%2520conservation%2520holds%2520at%2520machine%2520precision%252C%2520total-variation%2520growth%250Avanishes%252C%2520and%2520entropy%2520and%2520error%2520remain%2520bounded.%2520On%2520Burgers%2520and%2520Euler%2520systems%252C%250ACPL%2520produces%2520stable%252C%2520physically%2520lawful%2520solutions%2520without%2520loss%2520of%2520accuracy.%250AInstead%2520of%2520hoping%2520neural%2520solvers%2520will%2520respect%2520physics%252C%2520CPL%2520makes%2520that%2520behavior%250Aan%2520intrinsic%2520property%2520of%2520the%2520learning%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03578v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Under%20Laws%3A%20A%20Constraint-Projected%20Neural%20PDE%20Solver%20that%0A%20%20Eliminates%20Hallucinations&entry.906535625=Mainak%20Singha&entry.1292438233=%20%20Neural%20networks%20can%20approximate%20solutions%20to%20partial%20differential%20equations%2C%0Abut%20they%20often%20break%20the%20very%20laws%20they%20are%20meant%20to%20model-creating%20mass%20from%0Anowhere%2C%20drifting%20shocks%2C%20or%20violating%20conservation%20and%20entropy.%20We%20address%0Athis%20by%20training%20within%20the%20laws%20of%20physics%20rather%20than%20beside%20them.%20Our%0Aframework%2C%20called%20Constraint-Projected%20Learning%20%28CPL%29%2C%20keeps%20every%20update%0Aphysically%20admissible%20by%20projecting%20network%20outputs%20onto%20the%20intersection%20of%0Aconstraint%20sets%20defined%20by%20conservation%2C%20Rankine-Hugoniot%20balance%2C%20entropy%2C%20and%0Apositivity.%20The%20projection%20is%20differentiable%20and%20adds%20only%20about%2010%25%0Acomputational%20overhead%2C%20making%20it%20fully%20compatible%20with%20back-propagation.%20We%0Afurther%20stabilize%20training%20with%20total-variation%20damping%20%28TVD%29%20to%20suppress%20small%0Aoscillations%20and%20a%20rollout%20curriculum%20that%20enforces%20consistency%20over%20long%0Aprediction%20horizons.%20Together%2C%20these%20mechanisms%20eliminate%20both%20hard%20and%20soft%0Aviolations%3A%20conservation%20holds%20at%20machine%20precision%2C%20total-variation%20growth%0Avanishes%2C%20and%20entropy%20and%20error%20remain%20bounded.%20On%20Burgers%20and%20Euler%20systems%2C%0ACPL%20produces%20stable%2C%20physically%20lawful%20solutions%20without%20loss%20of%20accuracy.%0AInstead%20of%20hoping%20neural%20solvers%20will%20respect%20physics%2C%20CPL%20makes%20that%20behavior%0Aan%20intrinsic%20property%20of%20the%20learning%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03578v1&entry.124074799=Read"},
{"title": "Trustworthy Representation Learning via Information Funnels and\n  Bottlenecks", "author": "Jo\u00e3o Machado de Freitas and Bernhard C. Geiger", "abstract": "  Ensuring trustworthiness in machine learning -- by balancing utility,\nfairness, and privacy -- remains a critical challenge, particularly in\nrepresentation learning. In this work, we investigate a family of closely\nrelated information-theoretic objectives, including information funnels and\nbottlenecks, designed to extract invariant representations from data. We\nintroduce the Conditional Privacy Funnel with Side-information (CPFSI), a novel\nformulation within this family, applicable in both fully and semi-supervised\nsettings. Given the intractability of these objectives, we derive\nneural-network-based approximations via amortized variational inference. We\nsystematically analyze the trade-offs between utility, invariance, and\nrepresentation fidelity, offering new insights into the Pareto frontiers of\nthese methods. Our results demonstrate that CPFSI effectively balances these\ncompeting objectives and frequently outperforms existing approaches.\nFurthermore, we show that by intervening on sensitive attributes in CPFSI's\npredictive posterior enhances fairness while maintaining predictive\nperformance. Finally, we focus on the real-world applicability of these\napproaches, particularly for learning robust and fair representations from\ntabular datasets in data scarce-environments -- a modality where these methods\nare often especially relevant.\n", "link": "http://arxiv.org/abs/2211.01446v2", "date": "2025-11-05", "relevancy": 2.0833, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5283}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5271}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trustworthy%20Representation%20Learning%20via%20Information%20Funnels%20and%0A%20%20Bottlenecks&body=Title%3A%20Trustworthy%20Representation%20Learning%20via%20Information%20Funnels%20and%0A%20%20Bottlenecks%0AAuthor%3A%20Jo%C3%A3o%20Machado%20de%20Freitas%20and%20Bernhard%20C.%20Geiger%0AAbstract%3A%20%20%20Ensuring%20trustworthiness%20in%20machine%20learning%20--%20by%20balancing%20utility%2C%0Afairness%2C%20and%20privacy%20--%20remains%20a%20critical%20challenge%2C%20particularly%20in%0Arepresentation%20learning.%20In%20this%20work%2C%20we%20investigate%20a%20family%20of%20closely%0Arelated%20information-theoretic%20objectives%2C%20including%20information%20funnels%20and%0Abottlenecks%2C%20designed%20to%20extract%20invariant%20representations%20from%20data.%20We%0Aintroduce%20the%20Conditional%20Privacy%20Funnel%20with%20Side-information%20%28CPFSI%29%2C%20a%20novel%0Aformulation%20within%20this%20family%2C%20applicable%20in%20both%20fully%20and%20semi-supervised%0Asettings.%20Given%20the%20intractability%20of%20these%20objectives%2C%20we%20derive%0Aneural-network-based%20approximations%20via%20amortized%20variational%20inference.%20We%0Asystematically%20analyze%20the%20trade-offs%20between%20utility%2C%20invariance%2C%20and%0Arepresentation%20fidelity%2C%20offering%20new%20insights%20into%20the%20Pareto%20frontiers%20of%0Athese%20methods.%20Our%20results%20demonstrate%20that%20CPFSI%20effectively%20balances%20these%0Acompeting%20objectives%20and%20frequently%20outperforms%20existing%20approaches.%0AFurthermore%2C%20we%20show%20that%20by%20intervening%20on%20sensitive%20attributes%20in%20CPFSI%27s%0Apredictive%20posterior%20enhances%20fairness%20while%20maintaining%20predictive%0Aperformance.%20Finally%2C%20we%20focus%20on%20the%20real-world%20applicability%20of%20these%0Aapproaches%2C%20particularly%20for%20learning%20robust%20and%20fair%20representations%20from%0Atabular%20datasets%20in%20data%20scarce-environments%20--%20a%20modality%20where%20these%20methods%0Aare%20often%20especially%20relevant.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.01446v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrustworthy%2520Representation%2520Learning%2520via%2520Information%2520Funnels%2520and%250A%2520%2520Bottlenecks%26entry.906535625%3DJo%25C3%25A3o%2520Machado%2520de%2520Freitas%2520and%2520Bernhard%2520C.%2520Geiger%26entry.1292438233%3D%2520%2520Ensuring%2520trustworthiness%2520in%2520machine%2520learning%2520--%2520by%2520balancing%2520utility%252C%250Afairness%252C%2520and%2520privacy%2520--%2520remains%2520a%2520critical%2520challenge%252C%2520particularly%2520in%250Arepresentation%2520learning.%2520In%2520this%2520work%252C%2520we%2520investigate%2520a%2520family%2520of%2520closely%250Arelated%2520information-theoretic%2520objectives%252C%2520including%2520information%2520funnels%2520and%250Abottlenecks%252C%2520designed%2520to%2520extract%2520invariant%2520representations%2520from%2520data.%2520We%250Aintroduce%2520the%2520Conditional%2520Privacy%2520Funnel%2520with%2520Side-information%2520%2528CPFSI%2529%252C%2520a%2520novel%250Aformulation%2520within%2520this%2520family%252C%2520applicable%2520in%2520both%2520fully%2520and%2520semi-supervised%250Asettings.%2520Given%2520the%2520intractability%2520of%2520these%2520objectives%252C%2520we%2520derive%250Aneural-network-based%2520approximations%2520via%2520amortized%2520variational%2520inference.%2520We%250Asystematically%2520analyze%2520the%2520trade-offs%2520between%2520utility%252C%2520invariance%252C%2520and%250Arepresentation%2520fidelity%252C%2520offering%2520new%2520insights%2520into%2520the%2520Pareto%2520frontiers%2520of%250Athese%2520methods.%2520Our%2520results%2520demonstrate%2520that%2520CPFSI%2520effectively%2520balances%2520these%250Acompeting%2520objectives%2520and%2520frequently%2520outperforms%2520existing%2520approaches.%250AFurthermore%252C%2520we%2520show%2520that%2520by%2520intervening%2520on%2520sensitive%2520attributes%2520in%2520CPFSI%2527s%250Apredictive%2520posterior%2520enhances%2520fairness%2520while%2520maintaining%2520predictive%250Aperformance.%2520Finally%252C%2520we%2520focus%2520on%2520the%2520real-world%2520applicability%2520of%2520these%250Aapproaches%252C%2520particularly%2520for%2520learning%2520robust%2520and%2520fair%2520representations%2520from%250Atabular%2520datasets%2520in%2520data%2520scarce-environments%2520--%2520a%2520modality%2520where%2520these%2520methods%250Aare%2520often%2520especially%2520relevant.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.01446v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trustworthy%20Representation%20Learning%20via%20Information%20Funnels%20and%0A%20%20Bottlenecks&entry.906535625=Jo%C3%A3o%20Machado%20de%20Freitas%20and%20Bernhard%20C.%20Geiger&entry.1292438233=%20%20Ensuring%20trustworthiness%20in%20machine%20learning%20--%20by%20balancing%20utility%2C%0Afairness%2C%20and%20privacy%20--%20remains%20a%20critical%20challenge%2C%20particularly%20in%0Arepresentation%20learning.%20In%20this%20work%2C%20we%20investigate%20a%20family%20of%20closely%0Arelated%20information-theoretic%20objectives%2C%20including%20information%20funnels%20and%0Abottlenecks%2C%20designed%20to%20extract%20invariant%20representations%20from%20data.%20We%0Aintroduce%20the%20Conditional%20Privacy%20Funnel%20with%20Side-information%20%28CPFSI%29%2C%20a%20novel%0Aformulation%20within%20this%20family%2C%20applicable%20in%20both%20fully%20and%20semi-supervised%0Asettings.%20Given%20the%20intractability%20of%20these%20objectives%2C%20we%20derive%0Aneural-network-based%20approximations%20via%20amortized%20variational%20inference.%20We%0Asystematically%20analyze%20the%20trade-offs%20between%20utility%2C%20invariance%2C%20and%0Arepresentation%20fidelity%2C%20offering%20new%20insights%20into%20the%20Pareto%20frontiers%20of%0Athese%20methods.%20Our%20results%20demonstrate%20that%20CPFSI%20effectively%20balances%20these%0Acompeting%20objectives%20and%20frequently%20outperforms%20existing%20approaches.%0AFurthermore%2C%20we%20show%20that%20by%20intervening%20on%20sensitive%20attributes%20in%20CPFSI%27s%0Apredictive%20posterior%20enhances%20fairness%20while%20maintaining%20predictive%0Aperformance.%20Finally%2C%20we%20focus%20on%20the%20real-world%20applicability%20of%20these%0Aapproaches%2C%20particularly%20for%20learning%20robust%20and%20fair%20representations%20from%0Atabular%20datasets%20in%20data%20scarce-environments%20--%20a%20modality%20where%20these%20methods%0Aare%20often%20especially%20relevant.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.01446v2&entry.124074799=Read"},
{"title": "GDS Agent for Graph Algorithmic Reasoning", "author": "Borun Shi and Ioannis Panagiotas", "abstract": "  Large language models (LLMs) have shown remarkable multimodal information\nprocessing and reasoning ability. When equipped with tools through function\ncalling and enhanced with retrieval-augmented techniques, compound LLM-based\nsystems can access closed data sources and answer questions about them.\nHowever, they still struggle to process and reason over large-scale\ngraph-structure data. We introduce the GDS (Graph Data Science) agent in this\ntechnical report. The GDS agent introduces a comprehensive set of graph\nalgorithms as tools, together with preprocessing (retrieval) and postprocessing\nof algorithm results, in a model context protocol (MCP) server. The server can\nbe used with any modern LLM out-of-the-box. GDS agent allows users to ask any\nquestion that implicitly and intrinsically requires graph algorithmic reasoning\nabout their data, and quickly obtain accurate and grounded answers. We\nintroduce new benchmarks that evaluate intermediate tool calls as well as final\nresponses. The results indicate that GDS agent is able to solve a wide spectrum\nof graph tasks. We also provide detailed case studies for more open-ended tasks\nand study scenarios where the agent struggles. Finally, we discuss the\nremaining challenges and the future roadmap.\n", "link": "http://arxiv.org/abs/2508.20637v2", "date": "2025-11-05", "relevancy": 2.0219, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5566}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5006}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GDS%20Agent%20for%20Graph%20Algorithmic%20Reasoning&body=Title%3A%20GDS%20Agent%20for%20Graph%20Algorithmic%20Reasoning%0AAuthor%3A%20Borun%20Shi%20and%20Ioannis%20Panagiotas%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20multimodal%20information%0Aprocessing%20and%20reasoning%20ability.%20When%20equipped%20with%20tools%20through%20function%0Acalling%20and%20enhanced%20with%20retrieval-augmented%20techniques%2C%20compound%20LLM-based%0Asystems%20can%20access%20closed%20data%20sources%20and%20answer%20questions%20about%20them.%0AHowever%2C%20they%20still%20struggle%20to%20process%20and%20reason%20over%20large-scale%0Agraph-structure%20data.%20We%20introduce%20the%20GDS%20%28Graph%20Data%20Science%29%20agent%20in%20this%0Atechnical%20report.%20The%20GDS%20agent%20introduces%20a%20comprehensive%20set%20of%20graph%0Aalgorithms%20as%20tools%2C%20together%20with%20preprocessing%20%28retrieval%29%20and%20postprocessing%0Aof%20algorithm%20results%2C%20in%20a%20model%20context%20protocol%20%28MCP%29%20server.%20The%20server%20can%0Abe%20used%20with%20any%20modern%20LLM%20out-of-the-box.%20GDS%20agent%20allows%20users%20to%20ask%20any%0Aquestion%20that%20implicitly%20and%20intrinsically%20requires%20graph%20algorithmic%20reasoning%0Aabout%20their%20data%2C%20and%20quickly%20obtain%20accurate%20and%20grounded%20answers.%20We%0Aintroduce%20new%20benchmarks%20that%20evaluate%20intermediate%20tool%20calls%20as%20well%20as%20final%0Aresponses.%20The%20results%20indicate%20that%20GDS%20agent%20is%20able%20to%20solve%20a%20wide%20spectrum%0Aof%20graph%20tasks.%20We%20also%20provide%20detailed%20case%20studies%20for%20more%20open-ended%20tasks%0Aand%20study%20scenarios%20where%20the%20agent%20struggles.%20Finally%2C%20we%20discuss%20the%0Aremaining%20challenges%20and%20the%20future%20roadmap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20637v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGDS%2520Agent%2520for%2520Graph%2520Algorithmic%2520Reasoning%26entry.906535625%3DBorun%2520Shi%2520and%2520Ioannis%2520Panagiotas%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520remarkable%2520multimodal%2520information%250Aprocessing%2520and%2520reasoning%2520ability.%2520When%2520equipped%2520with%2520tools%2520through%2520function%250Acalling%2520and%2520enhanced%2520with%2520retrieval-augmented%2520techniques%252C%2520compound%2520LLM-based%250Asystems%2520can%2520access%2520closed%2520data%2520sources%2520and%2520answer%2520questions%2520about%2520them.%250AHowever%252C%2520they%2520still%2520struggle%2520to%2520process%2520and%2520reason%2520over%2520large-scale%250Agraph-structure%2520data.%2520We%2520introduce%2520the%2520GDS%2520%2528Graph%2520Data%2520Science%2529%2520agent%2520in%2520this%250Atechnical%2520report.%2520The%2520GDS%2520agent%2520introduces%2520a%2520comprehensive%2520set%2520of%2520graph%250Aalgorithms%2520as%2520tools%252C%2520together%2520with%2520preprocessing%2520%2528retrieval%2529%2520and%2520postprocessing%250Aof%2520algorithm%2520results%252C%2520in%2520a%2520model%2520context%2520protocol%2520%2528MCP%2529%2520server.%2520The%2520server%2520can%250Abe%2520used%2520with%2520any%2520modern%2520LLM%2520out-of-the-box.%2520GDS%2520agent%2520allows%2520users%2520to%2520ask%2520any%250Aquestion%2520that%2520implicitly%2520and%2520intrinsically%2520requires%2520graph%2520algorithmic%2520reasoning%250Aabout%2520their%2520data%252C%2520and%2520quickly%2520obtain%2520accurate%2520and%2520grounded%2520answers.%2520We%250Aintroduce%2520new%2520benchmarks%2520that%2520evaluate%2520intermediate%2520tool%2520calls%2520as%2520well%2520as%2520final%250Aresponses.%2520The%2520results%2520indicate%2520that%2520GDS%2520agent%2520is%2520able%2520to%2520solve%2520a%2520wide%2520spectrum%250Aof%2520graph%2520tasks.%2520We%2520also%2520provide%2520detailed%2520case%2520studies%2520for%2520more%2520open-ended%2520tasks%250Aand%2520study%2520scenarios%2520where%2520the%2520agent%2520struggles.%2520Finally%252C%2520we%2520discuss%2520the%250Aremaining%2520challenges%2520and%2520the%2520future%2520roadmap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20637v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GDS%20Agent%20for%20Graph%20Algorithmic%20Reasoning&entry.906535625=Borun%20Shi%20and%20Ioannis%20Panagiotas&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20multimodal%20information%0Aprocessing%20and%20reasoning%20ability.%20When%20equipped%20with%20tools%20through%20function%0Acalling%20and%20enhanced%20with%20retrieval-augmented%20techniques%2C%20compound%20LLM-based%0Asystems%20can%20access%20closed%20data%20sources%20and%20answer%20questions%20about%20them.%0AHowever%2C%20they%20still%20struggle%20to%20process%20and%20reason%20over%20large-scale%0Agraph-structure%20data.%20We%20introduce%20the%20GDS%20%28Graph%20Data%20Science%29%20agent%20in%20this%0Atechnical%20report.%20The%20GDS%20agent%20introduces%20a%20comprehensive%20set%20of%20graph%0Aalgorithms%20as%20tools%2C%20together%20with%20preprocessing%20%28retrieval%29%20and%20postprocessing%0Aof%20algorithm%20results%2C%20in%20a%20model%20context%20protocol%20%28MCP%29%20server.%20The%20server%20can%0Abe%20used%20with%20any%20modern%20LLM%20out-of-the-box.%20GDS%20agent%20allows%20users%20to%20ask%20any%0Aquestion%20that%20implicitly%20and%20intrinsically%20requires%20graph%20algorithmic%20reasoning%0Aabout%20their%20data%2C%20and%20quickly%20obtain%20accurate%20and%20grounded%20answers.%20We%0Aintroduce%20new%20benchmarks%20that%20evaluate%20intermediate%20tool%20calls%20as%20well%20as%20final%0Aresponses.%20The%20results%20indicate%20that%20GDS%20agent%20is%20able%20to%20solve%20a%20wide%20spectrum%0Aof%20graph%20tasks.%20We%20also%20provide%20detailed%20case%20studies%20for%20more%20open-ended%20tasks%0Aand%20study%20scenarios%20where%20the%20agent%20struggles.%20Finally%2C%20we%20discuss%20the%0Aremaining%20challenges%20and%20the%20future%20roadmap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20637v2&entry.124074799=Read"},
{"title": "Part-Aware Bottom-Up Group Reasoning for Fine-Grained Social Interaction\n  Detection", "author": "Dongkeun Kim and Minsu Cho and Suha Kwak", "abstract": "  Social interactions often emerge from subtle, fine-grained cues such as\nfacial expressions, gaze, and gestures. However, existing methods for social\ninteraction detection overlook such nuanced cues and primarily rely on holistic\nrepresentations of individuals. Moreover, they directly detect social groups\nwithout explicitly modeling the underlying interactions between individuals.\nThese drawbacks limit their ability to capture localized social signals and\nintroduce ambiguity when group configurations should be inferred from social\ninteractions grounded in nuanced cues. In this work, we propose a part-aware\nbottom-up group reasoning framework for fine-grained social interaction\ndetection. The proposed method infers social groups and their interactions\nusing body part features and their interpersonal relations. Our model first\ndetects individuals and enhances their features using part-aware cues, and then\ninfers group configuration by associating individuals via similarity-based\nreasoning, which considers not only spatial relations but also subtle social\ncues that signal interactions, leading to more accurate group inference.\nExperiments on the NVI dataset demonstrate that our method outperforms prior\nmethods, achieving the new state of the art.\n", "link": "http://arxiv.org/abs/2511.03666v1", "date": "2025-11-05", "relevancy": 1.9962, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5233}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5021}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Part-Aware%20Bottom-Up%20Group%20Reasoning%20for%20Fine-Grained%20Social%20Interaction%0A%20%20Detection&body=Title%3A%20Part-Aware%20Bottom-Up%20Group%20Reasoning%20for%20Fine-Grained%20Social%20Interaction%0A%20%20Detection%0AAuthor%3A%20Dongkeun%20Kim%20and%20Minsu%20Cho%20and%20Suha%20Kwak%0AAbstract%3A%20%20%20Social%20interactions%20often%20emerge%20from%20subtle%2C%20fine-grained%20cues%20such%20as%0Afacial%20expressions%2C%20gaze%2C%20and%20gestures.%20However%2C%20existing%20methods%20for%20social%0Ainteraction%20detection%20overlook%20such%20nuanced%20cues%20and%20primarily%20rely%20on%20holistic%0Arepresentations%20of%20individuals.%20Moreover%2C%20they%20directly%20detect%20social%20groups%0Awithout%20explicitly%20modeling%20the%20underlying%20interactions%20between%20individuals.%0AThese%20drawbacks%20limit%20their%20ability%20to%20capture%20localized%20social%20signals%20and%0Aintroduce%20ambiguity%20when%20group%20configurations%20should%20be%20inferred%20from%20social%0Ainteractions%20grounded%20in%20nuanced%20cues.%20In%20this%20work%2C%20we%20propose%20a%20part-aware%0Abottom-up%20group%20reasoning%20framework%20for%20fine-grained%20social%20interaction%0Adetection.%20The%20proposed%20method%20infers%20social%20groups%20and%20their%20interactions%0Ausing%20body%20part%20features%20and%20their%20interpersonal%20relations.%20Our%20model%20first%0Adetects%20individuals%20and%20enhances%20their%20features%20using%20part-aware%20cues%2C%20and%20then%0Ainfers%20group%20configuration%20by%20associating%20individuals%20via%20similarity-based%0Areasoning%2C%20which%20considers%20not%20only%20spatial%20relations%20but%20also%20subtle%20social%0Acues%20that%20signal%20interactions%2C%20leading%20to%20more%20accurate%20group%20inference.%0AExperiments%20on%20the%20NVI%20dataset%20demonstrate%20that%20our%20method%20outperforms%20prior%0Amethods%2C%20achieving%20the%20new%20state%20of%20the%20art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPart-Aware%2520Bottom-Up%2520Group%2520Reasoning%2520for%2520Fine-Grained%2520Social%2520Interaction%250A%2520%2520Detection%26entry.906535625%3DDongkeun%2520Kim%2520and%2520Minsu%2520Cho%2520and%2520Suha%2520Kwak%26entry.1292438233%3D%2520%2520Social%2520interactions%2520often%2520emerge%2520from%2520subtle%252C%2520fine-grained%2520cues%2520such%2520as%250Afacial%2520expressions%252C%2520gaze%252C%2520and%2520gestures.%2520However%252C%2520existing%2520methods%2520for%2520social%250Ainteraction%2520detection%2520overlook%2520such%2520nuanced%2520cues%2520and%2520primarily%2520rely%2520on%2520holistic%250Arepresentations%2520of%2520individuals.%2520Moreover%252C%2520they%2520directly%2520detect%2520social%2520groups%250Awithout%2520explicitly%2520modeling%2520the%2520underlying%2520interactions%2520between%2520individuals.%250AThese%2520drawbacks%2520limit%2520their%2520ability%2520to%2520capture%2520localized%2520social%2520signals%2520and%250Aintroduce%2520ambiguity%2520when%2520group%2520configurations%2520should%2520be%2520inferred%2520from%2520social%250Ainteractions%2520grounded%2520in%2520nuanced%2520cues.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520part-aware%250Abottom-up%2520group%2520reasoning%2520framework%2520for%2520fine-grained%2520social%2520interaction%250Adetection.%2520The%2520proposed%2520method%2520infers%2520social%2520groups%2520and%2520their%2520interactions%250Ausing%2520body%2520part%2520features%2520and%2520their%2520interpersonal%2520relations.%2520Our%2520model%2520first%250Adetects%2520individuals%2520and%2520enhances%2520their%2520features%2520using%2520part-aware%2520cues%252C%2520and%2520then%250Ainfers%2520group%2520configuration%2520by%2520associating%2520individuals%2520via%2520similarity-based%250Areasoning%252C%2520which%2520considers%2520not%2520only%2520spatial%2520relations%2520but%2520also%2520subtle%2520social%250Acues%2520that%2520signal%2520interactions%252C%2520leading%2520to%2520more%2520accurate%2520group%2520inference.%250AExperiments%2520on%2520the%2520NVI%2520dataset%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520prior%250Amethods%252C%2520achieving%2520the%2520new%2520state%2520of%2520the%2520art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Part-Aware%20Bottom-Up%20Group%20Reasoning%20for%20Fine-Grained%20Social%20Interaction%0A%20%20Detection&entry.906535625=Dongkeun%20Kim%20and%20Minsu%20Cho%20and%20Suha%20Kwak&entry.1292438233=%20%20Social%20interactions%20often%20emerge%20from%20subtle%2C%20fine-grained%20cues%20such%20as%0Afacial%20expressions%2C%20gaze%2C%20and%20gestures.%20However%2C%20existing%20methods%20for%20social%0Ainteraction%20detection%20overlook%20such%20nuanced%20cues%20and%20primarily%20rely%20on%20holistic%0Arepresentations%20of%20individuals.%20Moreover%2C%20they%20directly%20detect%20social%20groups%0Awithout%20explicitly%20modeling%20the%20underlying%20interactions%20between%20individuals.%0AThese%20drawbacks%20limit%20their%20ability%20to%20capture%20localized%20social%20signals%20and%0Aintroduce%20ambiguity%20when%20group%20configurations%20should%20be%20inferred%20from%20social%0Ainteractions%20grounded%20in%20nuanced%20cues.%20In%20this%20work%2C%20we%20propose%20a%20part-aware%0Abottom-up%20group%20reasoning%20framework%20for%20fine-grained%20social%20interaction%0Adetection.%20The%20proposed%20method%20infers%20social%20groups%20and%20their%20interactions%0Ausing%20body%20part%20features%20and%20their%20interpersonal%20relations.%20Our%20model%20first%0Adetects%20individuals%20and%20enhances%20their%20features%20using%20part-aware%20cues%2C%20and%20then%0Ainfers%20group%20configuration%20by%20associating%20individuals%20via%20similarity-based%0Areasoning%2C%20which%20considers%20not%20only%20spatial%20relations%20but%20also%20subtle%20social%0Acues%20that%20signal%20interactions%2C%20leading%20to%20more%20accurate%20group%20inference.%0AExperiments%20on%20the%20NVI%20dataset%20demonstrate%20that%20our%20method%20outperforms%20prior%0Amethods%2C%20achieving%20the%20new%20state%20of%20the%20art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03666v1&entry.124074799=Read"},
{"title": "Step-Audio-EditX Technical Report", "author": "Chao Yan and Boyong Wu and Peng Yang and Pengfei Tan and Guoqiang Hu and Yuxin Zhang and  Xiangyu and  Zhang and Fei Tian and Xuerui Yang and Xiangyu Zhang and Daxin Jiang and Gang Yu", "abstract": "  We present Step-Audio-EditX, the first open-source LLM-based audio model\nexcelling at expressive and iterative audio editing encompassing emotion,\nspeaking style, and paralinguistics alongside robust zero-shot text-to-speech\n(TTS) capabilities.Our core innovation lies in leveraging only large-margin\nsynthetic data, which circumvents the need for embedding-based priors or\nauxiliary modules. This large-margin learning approach enables both iterative\ncontrol and high expressivity across voices, and represents a fundamental pivot\nfrom the conventional focus on representation-level disentanglement. Evaluation\nresults demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and\nDoubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks.\n", "link": "http://arxiv.org/abs/2511.03601v1", "date": "2025-11-05", "relevancy": 1.9931, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4994}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4994}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Step-Audio-EditX%20Technical%20Report&body=Title%3A%20Step-Audio-EditX%20Technical%20Report%0AAuthor%3A%20Chao%20Yan%20and%20Boyong%20Wu%20and%20Peng%20Yang%20and%20Pengfei%20Tan%20and%20Guoqiang%20Hu%20and%20Yuxin%20Zhang%20and%20%20Xiangyu%20and%20%20Zhang%20and%20Fei%20Tian%20and%20Xuerui%20Yang%20and%20Xiangyu%20Zhang%20and%20Daxin%20Jiang%20and%20Gang%20Yu%0AAbstract%3A%20%20%20We%20present%20Step-Audio-EditX%2C%20the%20first%20open-source%20LLM-based%20audio%20model%0Aexcelling%20at%20expressive%20and%20iterative%20audio%20editing%20encompassing%20emotion%2C%0Aspeaking%20style%2C%20and%20paralinguistics%20alongside%20robust%20zero-shot%20text-to-speech%0A%28TTS%29%20capabilities.Our%20core%20innovation%20lies%20in%20leveraging%20only%20large-margin%0Asynthetic%20data%2C%20which%20circumvents%20the%20need%20for%20embedding-based%20priors%20or%0Aauxiliary%20modules.%20This%20large-margin%20learning%20approach%20enables%20both%20iterative%0Acontrol%20and%20high%20expressivity%20across%20voices%2C%20and%20represents%20a%20fundamental%20pivot%0Afrom%20the%20conventional%20focus%20on%20representation-level%20disentanglement.%20Evaluation%0Aresults%20demonstrate%20that%20Step-Audio-EditX%20surpasses%20both%20MiniMax-2.6-hd%20and%0ADoubao-Seed-TTS-2.0%20in%20emotion%20editing%20and%20other%20fine-grained%20control%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03601v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStep-Audio-EditX%2520Technical%2520Report%26entry.906535625%3DChao%2520Yan%2520and%2520Boyong%2520Wu%2520and%2520Peng%2520Yang%2520and%2520Pengfei%2520Tan%2520and%2520Guoqiang%2520Hu%2520and%2520Yuxin%2520Zhang%2520and%2520%2520Xiangyu%2520and%2520%2520Zhang%2520and%2520Fei%2520Tian%2520and%2520Xuerui%2520Yang%2520and%2520Xiangyu%2520Zhang%2520and%2520Daxin%2520Jiang%2520and%2520Gang%2520Yu%26entry.1292438233%3D%2520%2520We%2520present%2520Step-Audio-EditX%252C%2520the%2520first%2520open-source%2520LLM-based%2520audio%2520model%250Aexcelling%2520at%2520expressive%2520and%2520iterative%2520audio%2520editing%2520encompassing%2520emotion%252C%250Aspeaking%2520style%252C%2520and%2520paralinguistics%2520alongside%2520robust%2520zero-shot%2520text-to-speech%250A%2528TTS%2529%2520capabilities.Our%2520core%2520innovation%2520lies%2520in%2520leveraging%2520only%2520large-margin%250Asynthetic%2520data%252C%2520which%2520circumvents%2520the%2520need%2520for%2520embedding-based%2520priors%2520or%250Aauxiliary%2520modules.%2520This%2520large-margin%2520learning%2520approach%2520enables%2520both%2520iterative%250Acontrol%2520and%2520high%2520expressivity%2520across%2520voices%252C%2520and%2520represents%2520a%2520fundamental%2520pivot%250Afrom%2520the%2520conventional%2520focus%2520on%2520representation-level%2520disentanglement.%2520Evaluation%250Aresults%2520demonstrate%2520that%2520Step-Audio-EditX%2520surpasses%2520both%2520MiniMax-2.6-hd%2520and%250ADoubao-Seed-TTS-2.0%2520in%2520emotion%2520editing%2520and%2520other%2520fine-grained%2520control%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03601v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Step-Audio-EditX%20Technical%20Report&entry.906535625=Chao%20Yan%20and%20Boyong%20Wu%20and%20Peng%20Yang%20and%20Pengfei%20Tan%20and%20Guoqiang%20Hu%20and%20Yuxin%20Zhang%20and%20%20Xiangyu%20and%20%20Zhang%20and%20Fei%20Tian%20and%20Xuerui%20Yang%20and%20Xiangyu%20Zhang%20and%20Daxin%20Jiang%20and%20Gang%20Yu&entry.1292438233=%20%20We%20present%20Step-Audio-EditX%2C%20the%20first%20open-source%20LLM-based%20audio%20model%0Aexcelling%20at%20expressive%20and%20iterative%20audio%20editing%20encompassing%20emotion%2C%0Aspeaking%20style%2C%20and%20paralinguistics%20alongside%20robust%20zero-shot%20text-to-speech%0A%28TTS%29%20capabilities.Our%20core%20innovation%20lies%20in%20leveraging%20only%20large-margin%0Asynthetic%20data%2C%20which%20circumvents%20the%20need%20for%20embedding-based%20priors%20or%0Aauxiliary%20modules.%20This%20large-margin%20learning%20approach%20enables%20both%20iterative%0Acontrol%20and%20high%20expressivity%20across%20voices%2C%20and%20represents%20a%20fundamental%20pivot%0Afrom%20the%20conventional%20focus%20on%20representation-level%20disentanglement.%20Evaluation%0Aresults%20demonstrate%20that%20Step-Audio-EditX%20surpasses%20both%20MiniMax-2.6-hd%20and%0ADoubao-Seed-TTS-2.0%20in%20emotion%20editing%20and%20other%20fine-grained%20control%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03601v1&entry.124074799=Read"},
{"title": "MultiZebraLogic: A Multilingual Logical Reasoning Benchmark", "author": "Sofie Helene Bruun and Dan Saattrup Smart", "abstract": "  Measuring the full abilities of large language models (LLMs) requires\nbenchmarks representing multiple tasks. We aim to create large, high-quality\ndatasets for comparison of logical reasoning skills across several languages\nand of suitable difficulty for LLMs of various reasoning ability. We explore\nmultiple ways of increasing difficulty. We generate zebra puzzles in multiple\nlanguages, themes, sizes and including 14 different clue types and 8 red\nherring types (uninformative clues). We find puzzle sizes 2x3 and 4x5 are\nsufficiently challenging for GPT-4o mini (a non-reasoning model) and o3-mini (a\nreasoning model), respectively. Including 5 red herrings decreases o3-mini\npuzzle-level accuracy on 4x5 puzzles by 15$\\pm$7 %. Scores of o3-mini on 4x5\npuzzles are not significantly affected by use of English vs. Danish or the\ncommon houses theme vs. the country-specific smoerrebroed theme. We find no\ncorrelation between difficulty and the selected clue types. Datasets of\n128+1024 puzzles are published as MultiZebraLogic in each of nine Germanic\nlanguages for sizes 2x3 and 4x5. We publish code for puzzle generation,\ndesigned for adaptablity into more languages and themes.\n", "link": "http://arxiv.org/abs/2511.03553v1", "date": "2025-11-05", "relevancy": 1.9923, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5131}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5131}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiZebraLogic%3A%20A%20Multilingual%20Logical%20Reasoning%20Benchmark&body=Title%3A%20MultiZebraLogic%3A%20A%20Multilingual%20Logical%20Reasoning%20Benchmark%0AAuthor%3A%20Sofie%20Helene%20Bruun%20and%20Dan%20Saattrup%20Smart%0AAbstract%3A%20%20%20Measuring%20the%20full%20abilities%20of%20large%20language%20models%20%28LLMs%29%20requires%0Abenchmarks%20representing%20multiple%20tasks.%20We%20aim%20to%20create%20large%2C%20high-quality%0Adatasets%20for%20comparison%20of%20logical%20reasoning%20skills%20across%20several%20languages%0Aand%20of%20suitable%20difficulty%20for%20LLMs%20of%20various%20reasoning%20ability.%20We%20explore%0Amultiple%20ways%20of%20increasing%20difficulty.%20We%20generate%20zebra%20puzzles%20in%20multiple%0Alanguages%2C%20themes%2C%20sizes%20and%20including%2014%20different%20clue%20types%20and%208%20red%0Aherring%20types%20%28uninformative%20clues%29.%20We%20find%20puzzle%20sizes%202x3%20and%204x5%20are%0Asufficiently%20challenging%20for%20GPT-4o%20mini%20%28a%20non-reasoning%20model%29%20and%20o3-mini%20%28a%0Areasoning%20model%29%2C%20respectively.%20Including%205%20red%20herrings%20decreases%20o3-mini%0Apuzzle-level%20accuracy%20on%204x5%20puzzles%20by%2015%24%5Cpm%247%20%25.%20Scores%20of%20o3-mini%20on%204x5%0Apuzzles%20are%20not%20significantly%20affected%20by%20use%20of%20English%20vs.%20Danish%20or%20the%0Acommon%20houses%20theme%20vs.%20the%20country-specific%20smoerrebroed%20theme.%20We%20find%20no%0Acorrelation%20between%20difficulty%20and%20the%20selected%20clue%20types.%20Datasets%20of%0A128%2B1024%20puzzles%20are%20published%20as%20MultiZebraLogic%20in%20each%20of%20nine%20Germanic%0Alanguages%20for%20sizes%202x3%20and%204x5.%20We%20publish%20code%20for%20puzzle%20generation%2C%0Adesigned%20for%20adaptablity%20into%20more%20languages%20and%20themes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03553v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiZebraLogic%253A%2520A%2520Multilingual%2520Logical%2520Reasoning%2520Benchmark%26entry.906535625%3DSofie%2520Helene%2520Bruun%2520and%2520Dan%2520Saattrup%2520Smart%26entry.1292438233%3D%2520%2520Measuring%2520the%2520full%2520abilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520requires%250Abenchmarks%2520representing%2520multiple%2520tasks.%2520We%2520aim%2520to%2520create%2520large%252C%2520high-quality%250Adatasets%2520for%2520comparison%2520of%2520logical%2520reasoning%2520skills%2520across%2520several%2520languages%250Aand%2520of%2520suitable%2520difficulty%2520for%2520LLMs%2520of%2520various%2520reasoning%2520ability.%2520We%2520explore%250Amultiple%2520ways%2520of%2520increasing%2520difficulty.%2520We%2520generate%2520zebra%2520puzzles%2520in%2520multiple%250Alanguages%252C%2520themes%252C%2520sizes%2520and%2520including%252014%2520different%2520clue%2520types%2520and%25208%2520red%250Aherring%2520types%2520%2528uninformative%2520clues%2529.%2520We%2520find%2520puzzle%2520sizes%25202x3%2520and%25204x5%2520are%250Asufficiently%2520challenging%2520for%2520GPT-4o%2520mini%2520%2528a%2520non-reasoning%2520model%2529%2520and%2520o3-mini%2520%2528a%250Areasoning%2520model%2529%252C%2520respectively.%2520Including%25205%2520red%2520herrings%2520decreases%2520o3-mini%250Apuzzle-level%2520accuracy%2520on%25204x5%2520puzzles%2520by%252015%2524%255Cpm%25247%2520%2525.%2520Scores%2520of%2520o3-mini%2520on%25204x5%250Apuzzles%2520are%2520not%2520significantly%2520affected%2520by%2520use%2520of%2520English%2520vs.%2520Danish%2520or%2520the%250Acommon%2520houses%2520theme%2520vs.%2520the%2520country-specific%2520smoerrebroed%2520theme.%2520We%2520find%2520no%250Acorrelation%2520between%2520difficulty%2520and%2520the%2520selected%2520clue%2520types.%2520Datasets%2520of%250A128%252B1024%2520puzzles%2520are%2520published%2520as%2520MultiZebraLogic%2520in%2520each%2520of%2520nine%2520Germanic%250Alanguages%2520for%2520sizes%25202x3%2520and%25204x5.%2520We%2520publish%2520code%2520for%2520puzzle%2520generation%252C%250Adesigned%2520for%2520adaptablity%2520into%2520more%2520languages%2520and%2520themes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03553v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiZebraLogic%3A%20A%20Multilingual%20Logical%20Reasoning%20Benchmark&entry.906535625=Sofie%20Helene%20Bruun%20and%20Dan%20Saattrup%20Smart&entry.1292438233=%20%20Measuring%20the%20full%20abilities%20of%20large%20language%20models%20%28LLMs%29%20requires%0Abenchmarks%20representing%20multiple%20tasks.%20We%20aim%20to%20create%20large%2C%20high-quality%0Adatasets%20for%20comparison%20of%20logical%20reasoning%20skills%20across%20several%20languages%0Aand%20of%20suitable%20difficulty%20for%20LLMs%20of%20various%20reasoning%20ability.%20We%20explore%0Amultiple%20ways%20of%20increasing%20difficulty.%20We%20generate%20zebra%20puzzles%20in%20multiple%0Alanguages%2C%20themes%2C%20sizes%20and%20including%2014%20different%20clue%20types%20and%208%20red%0Aherring%20types%20%28uninformative%20clues%29.%20We%20find%20puzzle%20sizes%202x3%20and%204x5%20are%0Asufficiently%20challenging%20for%20GPT-4o%20mini%20%28a%20non-reasoning%20model%29%20and%20o3-mini%20%28a%0Areasoning%20model%29%2C%20respectively.%20Including%205%20red%20herrings%20decreases%20o3-mini%0Apuzzle-level%20accuracy%20on%204x5%20puzzles%20by%2015%24%5Cpm%247%20%25.%20Scores%20of%20o3-mini%20on%204x5%0Apuzzles%20are%20not%20significantly%20affected%20by%20use%20of%20English%20vs.%20Danish%20or%20the%0Acommon%20houses%20theme%20vs.%20the%20country-specific%20smoerrebroed%20theme.%20We%20find%20no%0Acorrelation%20between%20difficulty%20and%20the%20selected%20clue%20types.%20Datasets%20of%0A128%2B1024%20puzzles%20are%20published%20as%20MultiZebraLogic%20in%20each%20of%20nine%20Germanic%0Alanguages%20for%20sizes%202x3%20and%204x5.%20We%20publish%20code%20for%20puzzle%20generation%2C%0Adesigned%20for%20adaptablity%20into%20more%20languages%20and%20themes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03553v1&entry.124074799=Read"},
{"title": "The OpenHands Software Agent SDK: A Composable and Extensible Foundation\n  for Production Agents", "author": "Xingyao Wang and Simon Rosenberg and Juan Michelini and Calvin Smith and Hoang Tran and Engel Nyst and Rohit Malhotra and Xuhui Zhou and Valerie Chen and Robert Brennan and Graham Neubig", "abstract": "  Agents are now used widely in the process of software development, but\nbuilding production-ready software engineering agents is a complex task.\nDeploying software agents effectively requires flexibility in implementation\nand experimentation, reliable and secure execution, and interfaces for users to\ninteract with agents. In this paper, we present the OpenHands Software Agent\nSDK, a toolkit for implementing software development agents that satisfy these\ndesiderata. This toolkit is a complete architectural redesign of the agent\ncomponents of the popular OpenHands framework for software development agents,\nwhich has 64k+ GitHub stars. To achieve flexibility, we design a simple\ninterface for implementing agents that requires only a few lines of code in the\ndefault case, but is easily extensible to more complex, full-featured agents\nwith features such as custom tools, memory management, and more. For security\nand reliability, it delivers seamless local-to-remote execution portability,\nintegrated REST/WebSocket services. For interaction with human users, it can\nconnect directly to a variety of interfaces, such as visual workspaces (VS\nCode, VNC, browser), command-line interfaces, and APIs. Compared with existing\nSDKs from OpenAI, Claude, and Google, OpenHands uniquely integrates native\nsandboxed execution, lifecycle control, model-agnostic multi-LLM routing, and\nbuilt-in security analysis. Empirical results on SWE-Bench Verified and GAIA\nbenchmarks demonstrate strong performance. Put together, these elements allow\nthe OpenHands Software Agent SDK to provide a practical foundation for\nprototyping, unlocking new classes of custom applications, and reliably\ndeploying agents at scale.\n", "link": "http://arxiv.org/abs/2511.03690v1", "date": "2025-11-05", "relevancy": 1.9843, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5177}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5064}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20OpenHands%20Software%20Agent%20SDK%3A%20A%20Composable%20and%20Extensible%20Foundation%0A%20%20for%20Production%20Agents&body=Title%3A%20The%20OpenHands%20Software%20Agent%20SDK%3A%20A%20Composable%20and%20Extensible%20Foundation%0A%20%20for%20Production%20Agents%0AAuthor%3A%20Xingyao%20Wang%20and%20Simon%20Rosenberg%20and%20Juan%20Michelini%20and%20Calvin%20Smith%20and%20Hoang%20Tran%20and%20Engel%20Nyst%20and%20Rohit%20Malhotra%20and%20Xuhui%20Zhou%20and%20Valerie%20Chen%20and%20Robert%20Brennan%20and%20Graham%20Neubig%0AAbstract%3A%20%20%20Agents%20are%20now%20used%20widely%20in%20the%20process%20of%20software%20development%2C%20but%0Abuilding%20production-ready%20software%20engineering%20agents%20is%20a%20complex%20task.%0ADeploying%20software%20agents%20effectively%20requires%20flexibility%20in%20implementation%0Aand%20experimentation%2C%20reliable%20and%20secure%20execution%2C%20and%20interfaces%20for%20users%20to%0Ainteract%20with%20agents.%20In%20this%20paper%2C%20we%20present%20the%20OpenHands%20Software%20Agent%0ASDK%2C%20a%20toolkit%20for%20implementing%20software%20development%20agents%20that%20satisfy%20these%0Adesiderata.%20This%20toolkit%20is%20a%20complete%20architectural%20redesign%20of%20the%20agent%0Acomponents%20of%20the%20popular%20OpenHands%20framework%20for%20software%20development%20agents%2C%0Awhich%20has%2064k%2B%20GitHub%20stars.%20To%20achieve%20flexibility%2C%20we%20design%20a%20simple%0Ainterface%20for%20implementing%20agents%20that%20requires%20only%20a%20few%20lines%20of%20code%20in%20the%0Adefault%20case%2C%20but%20is%20easily%20extensible%20to%20more%20complex%2C%20full-featured%20agents%0Awith%20features%20such%20as%20custom%20tools%2C%20memory%20management%2C%20and%20more.%20For%20security%0Aand%20reliability%2C%20it%20delivers%20seamless%20local-to-remote%20execution%20portability%2C%0Aintegrated%20REST/WebSocket%20services.%20For%20interaction%20with%20human%20users%2C%20it%20can%0Aconnect%20directly%20to%20a%20variety%20of%20interfaces%2C%20such%20as%20visual%20workspaces%20%28VS%0ACode%2C%20VNC%2C%20browser%29%2C%20command-line%20interfaces%2C%20and%20APIs.%20Compared%20with%20existing%0ASDKs%20from%20OpenAI%2C%20Claude%2C%20and%20Google%2C%20OpenHands%20uniquely%20integrates%20native%0Asandboxed%20execution%2C%20lifecycle%20control%2C%20model-agnostic%20multi-LLM%20routing%2C%20and%0Abuilt-in%20security%20analysis.%20Empirical%20results%20on%20SWE-Bench%20Verified%20and%20GAIA%0Abenchmarks%20demonstrate%20strong%20performance.%20Put%20together%2C%20these%20elements%20allow%0Athe%20OpenHands%20Software%20Agent%20SDK%20to%20provide%20a%20practical%20foundation%20for%0Aprototyping%2C%20unlocking%20new%20classes%20of%20custom%20applications%2C%20and%20reliably%0Adeploying%20agents%20at%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03690v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520OpenHands%2520Software%2520Agent%2520SDK%253A%2520A%2520Composable%2520and%2520Extensible%2520Foundation%250A%2520%2520for%2520Production%2520Agents%26entry.906535625%3DXingyao%2520Wang%2520and%2520Simon%2520Rosenberg%2520and%2520Juan%2520Michelini%2520and%2520Calvin%2520Smith%2520and%2520Hoang%2520Tran%2520and%2520Engel%2520Nyst%2520and%2520Rohit%2520Malhotra%2520and%2520Xuhui%2520Zhou%2520and%2520Valerie%2520Chen%2520and%2520Robert%2520Brennan%2520and%2520Graham%2520Neubig%26entry.1292438233%3D%2520%2520Agents%2520are%2520now%2520used%2520widely%2520in%2520the%2520process%2520of%2520software%2520development%252C%2520but%250Abuilding%2520production-ready%2520software%2520engineering%2520agents%2520is%2520a%2520complex%2520task.%250ADeploying%2520software%2520agents%2520effectively%2520requires%2520flexibility%2520in%2520implementation%250Aand%2520experimentation%252C%2520reliable%2520and%2520secure%2520execution%252C%2520and%2520interfaces%2520for%2520users%2520to%250Ainteract%2520with%2520agents.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520OpenHands%2520Software%2520Agent%250ASDK%252C%2520a%2520toolkit%2520for%2520implementing%2520software%2520development%2520agents%2520that%2520satisfy%2520these%250Adesiderata.%2520This%2520toolkit%2520is%2520a%2520complete%2520architectural%2520redesign%2520of%2520the%2520agent%250Acomponents%2520of%2520the%2520popular%2520OpenHands%2520framework%2520for%2520software%2520development%2520agents%252C%250Awhich%2520has%252064k%252B%2520GitHub%2520stars.%2520To%2520achieve%2520flexibility%252C%2520we%2520design%2520a%2520simple%250Ainterface%2520for%2520implementing%2520agents%2520that%2520requires%2520only%2520a%2520few%2520lines%2520of%2520code%2520in%2520the%250Adefault%2520case%252C%2520but%2520is%2520easily%2520extensible%2520to%2520more%2520complex%252C%2520full-featured%2520agents%250Awith%2520features%2520such%2520as%2520custom%2520tools%252C%2520memory%2520management%252C%2520and%2520more.%2520For%2520security%250Aand%2520reliability%252C%2520it%2520delivers%2520seamless%2520local-to-remote%2520execution%2520portability%252C%250Aintegrated%2520REST/WebSocket%2520services.%2520For%2520interaction%2520with%2520human%2520users%252C%2520it%2520can%250Aconnect%2520directly%2520to%2520a%2520variety%2520of%2520interfaces%252C%2520such%2520as%2520visual%2520workspaces%2520%2528VS%250ACode%252C%2520VNC%252C%2520browser%2529%252C%2520command-line%2520interfaces%252C%2520and%2520APIs.%2520Compared%2520with%2520existing%250ASDKs%2520from%2520OpenAI%252C%2520Claude%252C%2520and%2520Google%252C%2520OpenHands%2520uniquely%2520integrates%2520native%250Asandboxed%2520execution%252C%2520lifecycle%2520control%252C%2520model-agnostic%2520multi-LLM%2520routing%252C%2520and%250Abuilt-in%2520security%2520analysis.%2520Empirical%2520results%2520on%2520SWE-Bench%2520Verified%2520and%2520GAIA%250Abenchmarks%2520demonstrate%2520strong%2520performance.%2520Put%2520together%252C%2520these%2520elements%2520allow%250Athe%2520OpenHands%2520Software%2520Agent%2520SDK%2520to%2520provide%2520a%2520practical%2520foundation%2520for%250Aprototyping%252C%2520unlocking%2520new%2520classes%2520of%2520custom%2520applications%252C%2520and%2520reliably%250Adeploying%2520agents%2520at%2520scale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03690v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20OpenHands%20Software%20Agent%20SDK%3A%20A%20Composable%20and%20Extensible%20Foundation%0A%20%20for%20Production%20Agents&entry.906535625=Xingyao%20Wang%20and%20Simon%20Rosenberg%20and%20Juan%20Michelini%20and%20Calvin%20Smith%20and%20Hoang%20Tran%20and%20Engel%20Nyst%20and%20Rohit%20Malhotra%20and%20Xuhui%20Zhou%20and%20Valerie%20Chen%20and%20Robert%20Brennan%20and%20Graham%20Neubig&entry.1292438233=%20%20Agents%20are%20now%20used%20widely%20in%20the%20process%20of%20software%20development%2C%20but%0Abuilding%20production-ready%20software%20engineering%20agents%20is%20a%20complex%20task.%0ADeploying%20software%20agents%20effectively%20requires%20flexibility%20in%20implementation%0Aand%20experimentation%2C%20reliable%20and%20secure%20execution%2C%20and%20interfaces%20for%20users%20to%0Ainteract%20with%20agents.%20In%20this%20paper%2C%20we%20present%20the%20OpenHands%20Software%20Agent%0ASDK%2C%20a%20toolkit%20for%20implementing%20software%20development%20agents%20that%20satisfy%20these%0Adesiderata.%20This%20toolkit%20is%20a%20complete%20architectural%20redesign%20of%20the%20agent%0Acomponents%20of%20the%20popular%20OpenHands%20framework%20for%20software%20development%20agents%2C%0Awhich%20has%2064k%2B%20GitHub%20stars.%20To%20achieve%20flexibility%2C%20we%20design%20a%20simple%0Ainterface%20for%20implementing%20agents%20that%20requires%20only%20a%20few%20lines%20of%20code%20in%20the%0Adefault%20case%2C%20but%20is%20easily%20extensible%20to%20more%20complex%2C%20full-featured%20agents%0Awith%20features%20such%20as%20custom%20tools%2C%20memory%20management%2C%20and%20more.%20For%20security%0Aand%20reliability%2C%20it%20delivers%20seamless%20local-to-remote%20execution%20portability%2C%0Aintegrated%20REST/WebSocket%20services.%20For%20interaction%20with%20human%20users%2C%20it%20can%0Aconnect%20directly%20to%20a%20variety%20of%20interfaces%2C%20such%20as%20visual%20workspaces%20%28VS%0ACode%2C%20VNC%2C%20browser%29%2C%20command-line%20interfaces%2C%20and%20APIs.%20Compared%20with%20existing%0ASDKs%20from%20OpenAI%2C%20Claude%2C%20and%20Google%2C%20OpenHands%20uniquely%20integrates%20native%0Asandboxed%20execution%2C%20lifecycle%20control%2C%20model-agnostic%20multi-LLM%20routing%2C%20and%0Abuilt-in%20security%20analysis.%20Empirical%20results%20on%20SWE-Bench%20Verified%20and%20GAIA%0Abenchmarks%20demonstrate%20strong%20performance.%20Put%20together%2C%20these%20elements%20allow%0Athe%20OpenHands%20Software%20Agent%20SDK%20to%20provide%20a%20practical%20foundation%20for%0Aprototyping%2C%20unlocking%20new%20classes%20of%20custom%20applications%2C%20and%20reliably%0Adeploying%20agents%20at%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03690v1&entry.124074799=Read"},
{"title": "Watermarking Large Language Models in Europe: Interpreting the AI Act in\n  Light of Technology", "author": "Thomas Souverain", "abstract": "  To foster trustworthy Artificial Intelligence (AI) within the European Union,\nthe AI Act requires providers to mark and detect the outputs of their\ngeneral-purpose models. The Article 50 and Recital 133 call for marking methods\nthat are ''sufficiently reliable, interoperable, effective and robust''. Yet,\nthe rapidly evolving and heterogeneous landscape of watermarks for Large\nLanguage Models (LLMs) makes it difficult to determine how these four standards\ncan be translated into concrete and measurable evaluations. Our paper addresses\nthis challenge, anchoring the normativity of European requirements in the\nmultiplicity of watermarking techniques. Introducing clear and distinct\nconcepts on LLM watermarking, our contribution is threefold. (1) Watermarking\nCategorisation: We propose an accessible taxonomy of watermarking methods\naccording to the stage of the LLM lifecycle at which they are applied - before,\nduring, or after training, and during next-token distribution or sampling. (2)\nWatermarking Evaluation: We interpret the EU AI Act's requirements by mapping\neach criterion with state-of-the-art evaluations on robustness and\ndetectability of the watermark, and of quality of the LLM. Since\ninteroperability remains largely untheorised in LLM watermarking research, we\npropose three normative dimensions to frame its assessment. (3) Watermarking\nComparison: We compare current watermarking methods for LLMs against the\noperationalised European criteria and show that no approach yet satisfies all\nfour standards. Encouraged by emerging empirical tests, we recommend further\nresearch into watermarking directly embedded within the low-level architecture\nof LLMs.\n", "link": "http://arxiv.org/abs/2511.03641v1", "date": "2025-11-05", "relevancy": 1.9788, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5128}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4911}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Watermarking%20Large%20Language%20Models%20in%20Europe%3A%20Interpreting%20the%20AI%20Act%20in%0A%20%20Light%20of%20Technology&body=Title%3A%20Watermarking%20Large%20Language%20Models%20in%20Europe%3A%20Interpreting%20the%20AI%20Act%20in%0A%20%20Light%20of%20Technology%0AAuthor%3A%20Thomas%20Souverain%0AAbstract%3A%20%20%20To%20foster%20trustworthy%20Artificial%20Intelligence%20%28AI%29%20within%20the%20European%20Union%2C%0Athe%20AI%20Act%20requires%20providers%20to%20mark%20and%20detect%20the%20outputs%20of%20their%0Ageneral-purpose%20models.%20The%20Article%2050%20and%20Recital%20133%20call%20for%20marking%20methods%0Athat%20are%20%27%27sufficiently%20reliable%2C%20interoperable%2C%20effective%20and%20robust%27%27.%20Yet%2C%0Athe%20rapidly%20evolving%20and%20heterogeneous%20landscape%20of%20watermarks%20for%20Large%0ALanguage%20Models%20%28LLMs%29%20makes%20it%20difficult%20to%20determine%20how%20these%20four%20standards%0Acan%20be%20translated%20into%20concrete%20and%20measurable%20evaluations.%20Our%20paper%20addresses%0Athis%20challenge%2C%20anchoring%20the%20normativity%20of%20European%20requirements%20in%20the%0Amultiplicity%20of%20watermarking%20techniques.%20Introducing%20clear%20and%20distinct%0Aconcepts%20on%20LLM%20watermarking%2C%20our%20contribution%20is%20threefold.%20%281%29%20Watermarking%0ACategorisation%3A%20We%20propose%20an%20accessible%20taxonomy%20of%20watermarking%20methods%0Aaccording%20to%20the%20stage%20of%20the%20LLM%20lifecycle%20at%20which%20they%20are%20applied%20-%20before%2C%0Aduring%2C%20or%20after%20training%2C%20and%20during%20next-token%20distribution%20or%20sampling.%20%282%29%0AWatermarking%20Evaluation%3A%20We%20interpret%20the%20EU%20AI%20Act%27s%20requirements%20by%20mapping%0Aeach%20criterion%20with%20state-of-the-art%20evaluations%20on%20robustness%20and%0Adetectability%20of%20the%20watermark%2C%20and%20of%20quality%20of%20the%20LLM.%20Since%0Ainteroperability%20remains%20largely%20untheorised%20in%20LLM%20watermarking%20research%2C%20we%0Apropose%20three%20normative%20dimensions%20to%20frame%20its%20assessment.%20%283%29%20Watermarking%0AComparison%3A%20We%20compare%20current%20watermarking%20methods%20for%20LLMs%20against%20the%0Aoperationalised%20European%20criteria%20and%20show%20that%20no%20approach%20yet%20satisfies%20all%0Afour%20standards.%20Encouraged%20by%20emerging%20empirical%20tests%2C%20we%20recommend%20further%0Aresearch%20into%20watermarking%20directly%20embedded%20within%20the%20low-level%20architecture%0Aof%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03641v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWatermarking%2520Large%2520Language%2520Models%2520in%2520Europe%253A%2520Interpreting%2520the%2520AI%2520Act%2520in%250A%2520%2520Light%2520of%2520Technology%26entry.906535625%3DThomas%2520Souverain%26entry.1292438233%3D%2520%2520To%2520foster%2520trustworthy%2520Artificial%2520Intelligence%2520%2528AI%2529%2520within%2520the%2520European%2520Union%252C%250Athe%2520AI%2520Act%2520requires%2520providers%2520to%2520mark%2520and%2520detect%2520the%2520outputs%2520of%2520their%250Ageneral-purpose%2520models.%2520The%2520Article%252050%2520and%2520Recital%2520133%2520call%2520for%2520marking%2520methods%250Athat%2520are%2520%2527%2527sufficiently%2520reliable%252C%2520interoperable%252C%2520effective%2520and%2520robust%2527%2527.%2520Yet%252C%250Athe%2520rapidly%2520evolving%2520and%2520heterogeneous%2520landscape%2520of%2520watermarks%2520for%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520makes%2520it%2520difficult%2520to%2520determine%2520how%2520these%2520four%2520standards%250Acan%2520be%2520translated%2520into%2520concrete%2520and%2520measurable%2520evaluations.%2520Our%2520paper%2520addresses%250Athis%2520challenge%252C%2520anchoring%2520the%2520normativity%2520of%2520European%2520requirements%2520in%2520the%250Amultiplicity%2520of%2520watermarking%2520techniques.%2520Introducing%2520clear%2520and%2520distinct%250Aconcepts%2520on%2520LLM%2520watermarking%252C%2520our%2520contribution%2520is%2520threefold.%2520%25281%2529%2520Watermarking%250ACategorisation%253A%2520We%2520propose%2520an%2520accessible%2520taxonomy%2520of%2520watermarking%2520methods%250Aaccording%2520to%2520the%2520stage%2520of%2520the%2520LLM%2520lifecycle%2520at%2520which%2520they%2520are%2520applied%2520-%2520before%252C%250Aduring%252C%2520or%2520after%2520training%252C%2520and%2520during%2520next-token%2520distribution%2520or%2520sampling.%2520%25282%2529%250AWatermarking%2520Evaluation%253A%2520We%2520interpret%2520the%2520EU%2520AI%2520Act%2527s%2520requirements%2520by%2520mapping%250Aeach%2520criterion%2520with%2520state-of-the-art%2520evaluations%2520on%2520robustness%2520and%250Adetectability%2520of%2520the%2520watermark%252C%2520and%2520of%2520quality%2520of%2520the%2520LLM.%2520Since%250Ainteroperability%2520remains%2520largely%2520untheorised%2520in%2520LLM%2520watermarking%2520research%252C%2520we%250Apropose%2520three%2520normative%2520dimensions%2520to%2520frame%2520its%2520assessment.%2520%25283%2529%2520Watermarking%250AComparison%253A%2520We%2520compare%2520current%2520watermarking%2520methods%2520for%2520LLMs%2520against%2520the%250Aoperationalised%2520European%2520criteria%2520and%2520show%2520that%2520no%2520approach%2520yet%2520satisfies%2520all%250Afour%2520standards.%2520Encouraged%2520by%2520emerging%2520empirical%2520tests%252C%2520we%2520recommend%2520further%250Aresearch%2520into%2520watermarking%2520directly%2520embedded%2520within%2520the%2520low-level%2520architecture%250Aof%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03641v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Watermarking%20Large%20Language%20Models%20in%20Europe%3A%20Interpreting%20the%20AI%20Act%20in%0A%20%20Light%20of%20Technology&entry.906535625=Thomas%20Souverain&entry.1292438233=%20%20To%20foster%20trustworthy%20Artificial%20Intelligence%20%28AI%29%20within%20the%20European%20Union%2C%0Athe%20AI%20Act%20requires%20providers%20to%20mark%20and%20detect%20the%20outputs%20of%20their%0Ageneral-purpose%20models.%20The%20Article%2050%20and%20Recital%20133%20call%20for%20marking%20methods%0Athat%20are%20%27%27sufficiently%20reliable%2C%20interoperable%2C%20effective%20and%20robust%27%27.%20Yet%2C%0Athe%20rapidly%20evolving%20and%20heterogeneous%20landscape%20of%20watermarks%20for%20Large%0ALanguage%20Models%20%28LLMs%29%20makes%20it%20difficult%20to%20determine%20how%20these%20four%20standards%0Acan%20be%20translated%20into%20concrete%20and%20measurable%20evaluations.%20Our%20paper%20addresses%0Athis%20challenge%2C%20anchoring%20the%20normativity%20of%20European%20requirements%20in%20the%0Amultiplicity%20of%20watermarking%20techniques.%20Introducing%20clear%20and%20distinct%0Aconcepts%20on%20LLM%20watermarking%2C%20our%20contribution%20is%20threefold.%20%281%29%20Watermarking%0ACategorisation%3A%20We%20propose%20an%20accessible%20taxonomy%20of%20watermarking%20methods%0Aaccording%20to%20the%20stage%20of%20the%20LLM%20lifecycle%20at%20which%20they%20are%20applied%20-%20before%2C%0Aduring%2C%20or%20after%20training%2C%20and%20during%20next-token%20distribution%20or%20sampling.%20%282%29%0AWatermarking%20Evaluation%3A%20We%20interpret%20the%20EU%20AI%20Act%27s%20requirements%20by%20mapping%0Aeach%20criterion%20with%20state-of-the-art%20evaluations%20on%20robustness%20and%0Adetectability%20of%20the%20watermark%2C%20and%20of%20quality%20of%20the%20LLM.%20Since%0Ainteroperability%20remains%20largely%20untheorised%20in%20LLM%20watermarking%20research%2C%20we%0Apropose%20three%20normative%20dimensions%20to%20frame%20its%20assessment.%20%283%29%20Watermarking%0AComparison%3A%20We%20compare%20current%20watermarking%20methods%20for%20LLMs%20against%20the%0Aoperationalised%20European%20criteria%20and%20show%20that%20no%20approach%20yet%20satisfies%20all%0Afour%20standards.%20Encouraged%20by%20emerging%20empirical%20tests%2C%20we%20recommend%20further%0Aresearch%20into%20watermarking%20directly%20embedded%20within%20the%20low-level%20architecture%0Aof%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03641v1&entry.124074799=Read"},
{"title": "LiveTradeBench: Seeking Real-World Alpha with Large Language Models", "author": "Haofei Yu and Fenghai Li and Jiaxuan You", "abstract": "  Large language models (LLMs) achieve strong performance across\nbenchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but\nthese tests occur in static settings, lacking real dynamics and uncertainty.\nConsequently, they evaluate isolated reasoning or problem-solving rather than\ndecision-making under uncertainty. To address this, we introduce\nLiveTradeBench, a live trading environment for evaluating LLM agents in\nrealistic and evolving markets. LiveTradeBench follows three design principles:\n(i) Live data streaming of market prices and news, eliminating dependence on\noffline backtesting and preventing information leakage while capturing\nreal-time uncertainty; (ii) a portfolio-management abstraction that extends\ncontrol from single-asset actions to multi-asset allocation, integrating risk\nmanagement and cross-asset reasoning; and (iii) multi-market evaluation across\nstructurally distinct environments--U.S. stocks and Polymarket prediction\nmarkets--differing in volatility, liquidity, and information flow. At each\nstep, an agent observes prices, news, and its portfolio, then outputs\npercentage allocations that balance risk and return. Using LiveTradeBench, we\nrun 50-day live evaluations of 21 LLMs across families. Results show that (1)\nhigh LMArena scores do not imply superior trading outcomes; (2) models display\ndistinct portfolio styles reflecting risk appetite and reasoning dynamics; and\n(3) some LLMs effectively leverage live signals to adapt decisions. These\nfindings expose a gap between static evaluation and real-world competence,\nmotivating benchmarks that test sequential decision making and consistency\nunder live uncertainty.\n", "link": "http://arxiv.org/abs/2511.03628v1", "date": "2025-11-05", "relevancy": 1.9771, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4944}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4944}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiveTradeBench%3A%20Seeking%20Real-World%20Alpha%20with%20Large%20Language%20Models&body=Title%3A%20LiveTradeBench%3A%20Seeking%20Real-World%20Alpha%20with%20Large%20Language%20Models%0AAuthor%3A%20Haofei%20Yu%20and%20Fenghai%20Li%20and%20Jiaxuan%20You%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20achieve%20strong%20performance%20across%0Abenchmarks--from%20knowledge%20quizzes%20and%20math%20reasoning%20to%20web-agent%20tasks--but%0Athese%20tests%20occur%20in%20static%20settings%2C%20lacking%20real%20dynamics%20and%20uncertainty.%0AConsequently%2C%20they%20evaluate%20isolated%20reasoning%20or%20problem-solving%20rather%20than%0Adecision-making%20under%20uncertainty.%20To%20address%20this%2C%20we%20introduce%0ALiveTradeBench%2C%20a%20live%20trading%20environment%20for%20evaluating%20LLM%20agents%20in%0Arealistic%20and%20evolving%20markets.%20LiveTradeBench%20follows%20three%20design%20principles%3A%0A%28i%29%20Live%20data%20streaming%20of%20market%20prices%20and%20news%2C%20eliminating%20dependence%20on%0Aoffline%20backtesting%20and%20preventing%20information%20leakage%20while%20capturing%0Areal-time%20uncertainty%3B%20%28ii%29%20a%20portfolio-management%20abstraction%20that%20extends%0Acontrol%20from%20single-asset%20actions%20to%20multi-asset%20allocation%2C%20integrating%20risk%0Amanagement%20and%20cross-asset%20reasoning%3B%20and%20%28iii%29%20multi-market%20evaluation%20across%0Astructurally%20distinct%20environments--U.S.%20stocks%20and%20Polymarket%20prediction%0Amarkets--differing%20in%20volatility%2C%20liquidity%2C%20and%20information%20flow.%20At%20each%0Astep%2C%20an%20agent%20observes%20prices%2C%20news%2C%20and%20its%20portfolio%2C%20then%20outputs%0Apercentage%20allocations%20that%20balance%20risk%20and%20return.%20Using%20LiveTradeBench%2C%20we%0Arun%2050-day%20live%20evaluations%20of%2021%20LLMs%20across%20families.%20Results%20show%20that%20%281%29%0Ahigh%20LMArena%20scores%20do%20not%20imply%20superior%20trading%20outcomes%3B%20%282%29%20models%20display%0Adistinct%20portfolio%20styles%20reflecting%20risk%20appetite%20and%20reasoning%20dynamics%3B%20and%0A%283%29%20some%20LLMs%20effectively%20leverage%20live%20signals%20to%20adapt%20decisions.%20These%0Afindings%20expose%20a%20gap%20between%20static%20evaluation%20and%20real-world%20competence%2C%0Amotivating%20benchmarks%20that%20test%20sequential%20decision%20making%20and%20consistency%0Aunder%20live%20uncertainty.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiveTradeBench%253A%2520Seeking%2520Real-World%2520Alpha%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DHaofei%2520Yu%2520and%2520Fenghai%2520Li%2520and%2520Jiaxuan%2520You%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520achieve%2520strong%2520performance%2520across%250Abenchmarks--from%2520knowledge%2520quizzes%2520and%2520math%2520reasoning%2520to%2520web-agent%2520tasks--but%250Athese%2520tests%2520occur%2520in%2520static%2520settings%252C%2520lacking%2520real%2520dynamics%2520and%2520uncertainty.%250AConsequently%252C%2520they%2520evaluate%2520isolated%2520reasoning%2520or%2520problem-solving%2520rather%2520than%250Adecision-making%2520under%2520uncertainty.%2520To%2520address%2520this%252C%2520we%2520introduce%250ALiveTradeBench%252C%2520a%2520live%2520trading%2520environment%2520for%2520evaluating%2520LLM%2520agents%2520in%250Arealistic%2520and%2520evolving%2520markets.%2520LiveTradeBench%2520follows%2520three%2520design%2520principles%253A%250A%2528i%2529%2520Live%2520data%2520streaming%2520of%2520market%2520prices%2520and%2520news%252C%2520eliminating%2520dependence%2520on%250Aoffline%2520backtesting%2520and%2520preventing%2520information%2520leakage%2520while%2520capturing%250Areal-time%2520uncertainty%253B%2520%2528ii%2529%2520a%2520portfolio-management%2520abstraction%2520that%2520extends%250Acontrol%2520from%2520single-asset%2520actions%2520to%2520multi-asset%2520allocation%252C%2520integrating%2520risk%250Amanagement%2520and%2520cross-asset%2520reasoning%253B%2520and%2520%2528iii%2529%2520multi-market%2520evaluation%2520across%250Astructurally%2520distinct%2520environments--U.S.%2520stocks%2520and%2520Polymarket%2520prediction%250Amarkets--differing%2520in%2520volatility%252C%2520liquidity%252C%2520and%2520information%2520flow.%2520At%2520each%250Astep%252C%2520an%2520agent%2520observes%2520prices%252C%2520news%252C%2520and%2520its%2520portfolio%252C%2520then%2520outputs%250Apercentage%2520allocations%2520that%2520balance%2520risk%2520and%2520return.%2520Using%2520LiveTradeBench%252C%2520we%250Arun%252050-day%2520live%2520evaluations%2520of%252021%2520LLMs%2520across%2520families.%2520Results%2520show%2520that%2520%25281%2529%250Ahigh%2520LMArena%2520scores%2520do%2520not%2520imply%2520superior%2520trading%2520outcomes%253B%2520%25282%2529%2520models%2520display%250Adistinct%2520portfolio%2520styles%2520reflecting%2520risk%2520appetite%2520and%2520reasoning%2520dynamics%253B%2520and%250A%25283%2529%2520some%2520LLMs%2520effectively%2520leverage%2520live%2520signals%2520to%2520adapt%2520decisions.%2520These%250Afindings%2520expose%2520a%2520gap%2520between%2520static%2520evaluation%2520and%2520real-world%2520competence%252C%250Amotivating%2520benchmarks%2520that%2520test%2520sequential%2520decision%2520making%2520and%2520consistency%250Aunder%2520live%2520uncertainty.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiveTradeBench%3A%20Seeking%20Real-World%20Alpha%20with%20Large%20Language%20Models&entry.906535625=Haofei%20Yu%20and%20Fenghai%20Li%20and%20Jiaxuan%20You&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20achieve%20strong%20performance%20across%0Abenchmarks--from%20knowledge%20quizzes%20and%20math%20reasoning%20to%20web-agent%20tasks--but%0Athese%20tests%20occur%20in%20static%20settings%2C%20lacking%20real%20dynamics%20and%20uncertainty.%0AConsequently%2C%20they%20evaluate%20isolated%20reasoning%20or%20problem-solving%20rather%20than%0Adecision-making%20under%20uncertainty.%20To%20address%20this%2C%20we%20introduce%0ALiveTradeBench%2C%20a%20live%20trading%20environment%20for%20evaluating%20LLM%20agents%20in%0Arealistic%20and%20evolving%20markets.%20LiveTradeBench%20follows%20three%20design%20principles%3A%0A%28i%29%20Live%20data%20streaming%20of%20market%20prices%20and%20news%2C%20eliminating%20dependence%20on%0Aoffline%20backtesting%20and%20preventing%20information%20leakage%20while%20capturing%0Areal-time%20uncertainty%3B%20%28ii%29%20a%20portfolio-management%20abstraction%20that%20extends%0Acontrol%20from%20single-asset%20actions%20to%20multi-asset%20allocation%2C%20integrating%20risk%0Amanagement%20and%20cross-asset%20reasoning%3B%20and%20%28iii%29%20multi-market%20evaluation%20across%0Astructurally%20distinct%20environments--U.S.%20stocks%20and%20Polymarket%20prediction%0Amarkets--differing%20in%20volatility%2C%20liquidity%2C%20and%20information%20flow.%20At%20each%0Astep%2C%20an%20agent%20observes%20prices%2C%20news%2C%20and%20its%20portfolio%2C%20then%20outputs%0Apercentage%20allocations%20that%20balance%20risk%20and%20return.%20Using%20LiveTradeBench%2C%20we%0Arun%2050-day%20live%20evaluations%20of%2021%20LLMs%20across%20families.%20Results%20show%20that%20%281%29%0Ahigh%20LMArena%20scores%20do%20not%20imply%20superior%20trading%20outcomes%3B%20%282%29%20models%20display%0Adistinct%20portfolio%20styles%20reflecting%20risk%20appetite%20and%20reasoning%20dynamics%3B%20and%0A%283%29%20some%20LLMs%20effectively%20leverage%20live%20signals%20to%20adapt%20decisions.%20These%0Afindings%20expose%20a%20gap%20between%20static%20evaluation%20and%20real-world%20competence%2C%0Amotivating%20benchmarks%20that%20test%20sequential%20decision%20making%20and%20consistency%0Aunder%20live%20uncertainty.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03628v1&entry.124074799=Read"},
{"title": "CLAX: Fast and Flexible Neural Click Models in JAX", "author": "Philipp Hager and Onno Zoeter and Maarten de Rijke", "abstract": "  CLAX is a JAX-based library that implements classic click models using modern\ngradient-based optimization. While neural click models have emerged over the\npast decade, complex click models based on probabilistic graphical models\n(PGMs) have not systematically adopted gradient-based optimization, preventing\npractitioners from leveraging modern deep learning frameworks while preserving\nthe interpretability of classic models. CLAX addresses this gap by replacing\nEM-based optimization with direct gradient-based optimization in a numerically\nstable manner. The framework's modular design enables the integration of any\ncomponent, from embeddings and deep networks to custom modules, into classic\nclick models for end-to-end optimization. We demonstrate CLAX's efficiency by\nrunning experiments on the full Baidu-ULTR dataset comprising over a billion\nuser sessions in $\\approx$ 2 hours on a single GPU, orders of magnitude faster\nthan traditional EM approaches. CLAX implements ten classic click models,\nserving both industry practitioners seeking to understand user behavior and\nimprove ranking performance at scale and researchers developing new click\nmodels. CLAX is available at: https://github.com/philipphager/clax\n", "link": "http://arxiv.org/abs/2511.03620v1", "date": "2025-11-05", "relevancy": 1.9396, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4867}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4837}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLAX%3A%20Fast%20and%20Flexible%20Neural%20Click%20Models%20in%20JAX&body=Title%3A%20CLAX%3A%20Fast%20and%20Flexible%20Neural%20Click%20Models%20in%20JAX%0AAuthor%3A%20Philipp%20Hager%20and%20Onno%20Zoeter%20and%20Maarten%20de%20Rijke%0AAbstract%3A%20%20%20CLAX%20is%20a%20JAX-based%20library%20that%20implements%20classic%20click%20models%20using%20modern%0Agradient-based%20optimization.%20While%20neural%20click%20models%20have%20emerged%20over%20the%0Apast%20decade%2C%20complex%20click%20models%20based%20on%20probabilistic%20graphical%20models%0A%28PGMs%29%20have%20not%20systematically%20adopted%20gradient-based%20optimization%2C%20preventing%0Apractitioners%20from%20leveraging%20modern%20deep%20learning%20frameworks%20while%20preserving%0Athe%20interpretability%20of%20classic%20models.%20CLAX%20addresses%20this%20gap%20by%20replacing%0AEM-based%20optimization%20with%20direct%20gradient-based%20optimization%20in%20a%20numerically%0Astable%20manner.%20The%20framework%27s%20modular%20design%20enables%20the%20integration%20of%20any%0Acomponent%2C%20from%20embeddings%20and%20deep%20networks%20to%20custom%20modules%2C%20into%20classic%0Aclick%20models%20for%20end-to-end%20optimization.%20We%20demonstrate%20CLAX%27s%20efficiency%20by%0Arunning%20experiments%20on%20the%20full%20Baidu-ULTR%20dataset%20comprising%20over%20a%20billion%0Auser%20sessions%20in%20%24%5Capprox%24%202%20hours%20on%20a%20single%20GPU%2C%20orders%20of%20magnitude%20faster%0Athan%20traditional%20EM%20approaches.%20CLAX%20implements%20ten%20classic%20click%20models%2C%0Aserving%20both%20industry%20practitioners%20seeking%20to%20understand%20user%20behavior%20and%0Aimprove%20ranking%20performance%20at%20scale%20and%20researchers%20developing%20new%20click%0Amodels.%20CLAX%20is%20available%20at%3A%20https%3A//github.com/philipphager/clax%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03620v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLAX%253A%2520Fast%2520and%2520Flexible%2520Neural%2520Click%2520Models%2520in%2520JAX%26entry.906535625%3DPhilipp%2520Hager%2520and%2520Onno%2520Zoeter%2520and%2520Maarten%2520de%2520Rijke%26entry.1292438233%3D%2520%2520CLAX%2520is%2520a%2520JAX-based%2520library%2520that%2520implements%2520classic%2520click%2520models%2520using%2520modern%250Agradient-based%2520optimization.%2520While%2520neural%2520click%2520models%2520have%2520emerged%2520over%2520the%250Apast%2520decade%252C%2520complex%2520click%2520models%2520based%2520on%2520probabilistic%2520graphical%2520models%250A%2528PGMs%2529%2520have%2520not%2520systematically%2520adopted%2520gradient-based%2520optimization%252C%2520preventing%250Apractitioners%2520from%2520leveraging%2520modern%2520deep%2520learning%2520frameworks%2520while%2520preserving%250Athe%2520interpretability%2520of%2520classic%2520models.%2520CLAX%2520addresses%2520this%2520gap%2520by%2520replacing%250AEM-based%2520optimization%2520with%2520direct%2520gradient-based%2520optimization%2520in%2520a%2520numerically%250Astable%2520manner.%2520The%2520framework%2527s%2520modular%2520design%2520enables%2520the%2520integration%2520of%2520any%250Acomponent%252C%2520from%2520embeddings%2520and%2520deep%2520networks%2520to%2520custom%2520modules%252C%2520into%2520classic%250Aclick%2520models%2520for%2520end-to-end%2520optimization.%2520We%2520demonstrate%2520CLAX%2527s%2520efficiency%2520by%250Arunning%2520experiments%2520on%2520the%2520full%2520Baidu-ULTR%2520dataset%2520comprising%2520over%2520a%2520billion%250Auser%2520sessions%2520in%2520%2524%255Capprox%2524%25202%2520hours%2520on%2520a%2520single%2520GPU%252C%2520orders%2520of%2520magnitude%2520faster%250Athan%2520traditional%2520EM%2520approaches.%2520CLAX%2520implements%2520ten%2520classic%2520click%2520models%252C%250Aserving%2520both%2520industry%2520practitioners%2520seeking%2520to%2520understand%2520user%2520behavior%2520and%250Aimprove%2520ranking%2520performance%2520at%2520scale%2520and%2520researchers%2520developing%2520new%2520click%250Amodels.%2520CLAX%2520is%2520available%2520at%253A%2520https%253A//github.com/philipphager/clax%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03620v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLAX%3A%20Fast%20and%20Flexible%20Neural%20Click%20Models%20in%20JAX&entry.906535625=Philipp%20Hager%20and%20Onno%20Zoeter%20and%20Maarten%20de%20Rijke&entry.1292438233=%20%20CLAX%20is%20a%20JAX-based%20library%20that%20implements%20classic%20click%20models%20using%20modern%0Agradient-based%20optimization.%20While%20neural%20click%20models%20have%20emerged%20over%20the%0Apast%20decade%2C%20complex%20click%20models%20based%20on%20probabilistic%20graphical%20models%0A%28PGMs%29%20have%20not%20systematically%20adopted%20gradient-based%20optimization%2C%20preventing%0Apractitioners%20from%20leveraging%20modern%20deep%20learning%20frameworks%20while%20preserving%0Athe%20interpretability%20of%20classic%20models.%20CLAX%20addresses%20this%20gap%20by%20replacing%0AEM-based%20optimization%20with%20direct%20gradient-based%20optimization%20in%20a%20numerically%0Astable%20manner.%20The%20framework%27s%20modular%20design%20enables%20the%20integration%20of%20any%0Acomponent%2C%20from%20embeddings%20and%20deep%20networks%20to%20custom%20modules%2C%20into%20classic%0Aclick%20models%20for%20end-to-end%20optimization.%20We%20demonstrate%20CLAX%27s%20efficiency%20by%0Arunning%20experiments%20on%20the%20full%20Baidu-ULTR%20dataset%20comprising%20over%20a%20billion%0Auser%20sessions%20in%20%24%5Capprox%24%202%20hours%20on%20a%20single%20GPU%2C%20orders%20of%20magnitude%20faster%0Athan%20traditional%20EM%20approaches.%20CLAX%20implements%20ten%20classic%20click%20models%2C%0Aserving%20both%20industry%20practitioners%20seeking%20to%20understand%20user%20behavior%20and%0Aimprove%20ranking%20performance%20at%20scale%20and%20researchers%20developing%20new%20click%0Amodels.%20CLAX%20is%20available%20at%3A%20https%3A//github.com/philipphager/clax%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03620v1&entry.124074799=Read"},
{"title": "Tensor-Efficient High-Dimensional Q-learning", "author": "Junyi Wu and Dan Li", "abstract": "  High-dimensional reinforcement learning faces challenges with complex\ncalculations and low sample efficiency in large state-action spaces. Q-learning\nalgorithms struggle particularly with the curse of dimensionality, where the\nnumber of state-action pairs grows exponentially with problem size. While\nneural network-based approaches like Deep Q-Networks have shown success, recent\ntensor-based methods using low-rank decomposition offer more\nparameter-efficient alternatives. Building upon existing tensor-based methods,\nwe propose Tensor-Efficient Q-Learning (TEQL), which enhances low-rank tensor\ndecomposition via improved block coordinate descent on discretized state-action\nspaces, incorporating novel exploration and regularization mechanisms. The key\ninnovation is an exploration strategy that combines approximation error with\nvisit count-based upper confidence bound to prioritize actions with high\nuncertainty, avoiding wasteful random exploration. Additionally, we incorporate\na frequency-based penalty term in the objective function to encourage\nexploration of less-visited state-action pairs and reduce overfitting to\nfrequently visited regions. Empirical results on classic control tasks\ndemonstrate that TEQL outperforms conventional matrix-based methods and deep RL\napproaches in both sample efficiency and total rewards, making it suitable for\nresource-constrained applications, such as space and healthcare where sampling\ncosts are high.\n", "link": "http://arxiv.org/abs/2511.03595v1", "date": "2025-11-05", "relevancy": 1.9059, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4965}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4851}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tensor-Efficient%20High-Dimensional%20Q-learning&body=Title%3A%20Tensor-Efficient%20High-Dimensional%20Q-learning%0AAuthor%3A%20Junyi%20Wu%20and%20Dan%20Li%0AAbstract%3A%20%20%20High-dimensional%20reinforcement%20learning%20faces%20challenges%20with%20complex%0Acalculations%20and%20low%20sample%20efficiency%20in%20large%20state-action%20spaces.%20Q-learning%0Aalgorithms%20struggle%20particularly%20with%20the%20curse%20of%20dimensionality%2C%20where%20the%0Anumber%20of%20state-action%20pairs%20grows%20exponentially%20with%20problem%20size.%20While%0Aneural%20network-based%20approaches%20like%20Deep%20Q-Networks%20have%20shown%20success%2C%20recent%0Atensor-based%20methods%20using%20low-rank%20decomposition%20offer%20more%0Aparameter-efficient%20alternatives.%20Building%20upon%20existing%20tensor-based%20methods%2C%0Awe%20propose%20Tensor-Efficient%20Q-Learning%20%28TEQL%29%2C%20which%20enhances%20low-rank%20tensor%0Adecomposition%20via%20improved%20block%20coordinate%20descent%20on%20discretized%20state-action%0Aspaces%2C%20incorporating%20novel%20exploration%20and%20regularization%20mechanisms.%20The%20key%0Ainnovation%20is%20an%20exploration%20strategy%20that%20combines%20approximation%20error%20with%0Avisit%20count-based%20upper%20confidence%20bound%20to%20prioritize%20actions%20with%20high%0Auncertainty%2C%20avoiding%20wasteful%20random%20exploration.%20Additionally%2C%20we%20incorporate%0Aa%20frequency-based%20penalty%20term%20in%20the%20objective%20function%20to%20encourage%0Aexploration%20of%20less-visited%20state-action%20pairs%20and%20reduce%20overfitting%20to%0Afrequently%20visited%20regions.%20Empirical%20results%20on%20classic%20control%20tasks%0Ademonstrate%20that%20TEQL%20outperforms%20conventional%20matrix-based%20methods%20and%20deep%20RL%0Aapproaches%20in%20both%20sample%20efficiency%20and%20total%20rewards%2C%20making%20it%20suitable%20for%0Aresource-constrained%20applications%2C%20such%20as%20space%20and%20healthcare%20where%20sampling%0Acosts%20are%20high.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03595v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTensor-Efficient%2520High-Dimensional%2520Q-learning%26entry.906535625%3DJunyi%2520Wu%2520and%2520Dan%2520Li%26entry.1292438233%3D%2520%2520High-dimensional%2520reinforcement%2520learning%2520faces%2520challenges%2520with%2520complex%250Acalculations%2520and%2520low%2520sample%2520efficiency%2520in%2520large%2520state-action%2520spaces.%2520Q-learning%250Aalgorithms%2520struggle%2520particularly%2520with%2520the%2520curse%2520of%2520dimensionality%252C%2520where%2520the%250Anumber%2520of%2520state-action%2520pairs%2520grows%2520exponentially%2520with%2520problem%2520size.%2520While%250Aneural%2520network-based%2520approaches%2520like%2520Deep%2520Q-Networks%2520have%2520shown%2520success%252C%2520recent%250Atensor-based%2520methods%2520using%2520low-rank%2520decomposition%2520offer%2520more%250Aparameter-efficient%2520alternatives.%2520Building%2520upon%2520existing%2520tensor-based%2520methods%252C%250Awe%2520propose%2520Tensor-Efficient%2520Q-Learning%2520%2528TEQL%2529%252C%2520which%2520enhances%2520low-rank%2520tensor%250Adecomposition%2520via%2520improved%2520block%2520coordinate%2520descent%2520on%2520discretized%2520state-action%250Aspaces%252C%2520incorporating%2520novel%2520exploration%2520and%2520regularization%2520mechanisms.%2520The%2520key%250Ainnovation%2520is%2520an%2520exploration%2520strategy%2520that%2520combines%2520approximation%2520error%2520with%250Avisit%2520count-based%2520upper%2520confidence%2520bound%2520to%2520prioritize%2520actions%2520with%2520high%250Auncertainty%252C%2520avoiding%2520wasteful%2520random%2520exploration.%2520Additionally%252C%2520we%2520incorporate%250Aa%2520frequency-based%2520penalty%2520term%2520in%2520the%2520objective%2520function%2520to%2520encourage%250Aexploration%2520of%2520less-visited%2520state-action%2520pairs%2520and%2520reduce%2520overfitting%2520to%250Afrequently%2520visited%2520regions.%2520Empirical%2520results%2520on%2520classic%2520control%2520tasks%250Ademonstrate%2520that%2520TEQL%2520outperforms%2520conventional%2520matrix-based%2520methods%2520and%2520deep%2520RL%250Aapproaches%2520in%2520both%2520sample%2520efficiency%2520and%2520total%2520rewards%252C%2520making%2520it%2520suitable%2520for%250Aresource-constrained%2520applications%252C%2520such%2520as%2520space%2520and%2520healthcare%2520where%2520sampling%250Acosts%2520are%2520high.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03595v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tensor-Efficient%20High-Dimensional%20Q-learning&entry.906535625=Junyi%20Wu%20and%20Dan%20Li&entry.1292438233=%20%20High-dimensional%20reinforcement%20learning%20faces%20challenges%20with%20complex%0Acalculations%20and%20low%20sample%20efficiency%20in%20large%20state-action%20spaces.%20Q-learning%0Aalgorithms%20struggle%20particularly%20with%20the%20curse%20of%20dimensionality%2C%20where%20the%0Anumber%20of%20state-action%20pairs%20grows%20exponentially%20with%20problem%20size.%20While%0Aneural%20network-based%20approaches%20like%20Deep%20Q-Networks%20have%20shown%20success%2C%20recent%0Atensor-based%20methods%20using%20low-rank%20decomposition%20offer%20more%0Aparameter-efficient%20alternatives.%20Building%20upon%20existing%20tensor-based%20methods%2C%0Awe%20propose%20Tensor-Efficient%20Q-Learning%20%28TEQL%29%2C%20which%20enhances%20low-rank%20tensor%0Adecomposition%20via%20improved%20block%20coordinate%20descent%20on%20discretized%20state-action%0Aspaces%2C%20incorporating%20novel%20exploration%20and%20regularization%20mechanisms.%20The%20key%0Ainnovation%20is%20an%20exploration%20strategy%20that%20combines%20approximation%20error%20with%0Avisit%20count-based%20upper%20confidence%20bound%20to%20prioritize%20actions%20with%20high%0Auncertainty%2C%20avoiding%20wasteful%20random%20exploration.%20Additionally%2C%20we%20incorporate%0Aa%20frequency-based%20penalty%20term%20in%20the%20objective%20function%20to%20encourage%0Aexploration%20of%20less-visited%20state-action%20pairs%20and%20reduce%20overfitting%20to%0Afrequently%20visited%20regions.%20Empirical%20results%20on%20classic%20control%20tasks%0Ademonstrate%20that%20TEQL%20outperforms%20conventional%20matrix-based%20methods%20and%20deep%20RL%0Aapproaches%20in%20both%20sample%20efficiency%20and%20total%20rewards%2C%20making%20it%20suitable%20for%0Aresource-constrained%20applications%2C%20such%20as%20space%20and%20healthcare%20where%20sampling%0Acosts%20are%20high.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03595v1&entry.124074799=Read"},
{"title": "Why Isn't Relational Learning Taking Over the World?", "author": "David Poole", "abstract": "  Artificial intelligence seems to be taking over the world with systems that\nmodel pixels, words, and phonemes. The world is arguably made up, not of\npixels, words, and phonemes but of entities (objects, things, including events)\nwith properties and relations among them. Surely we should model these, not the\nperception or description of them. You might suspect that concentrating on\nmodeling words and pixels is because all of the (valuable) data in the world is\nin terms of text and images. If you look into almost any company you will find\ntheir most valuable data is in spreadsheets, databases and other relational\nformats. These are not the form that are studied in introductory machine\nlearning, but are full of product numbers, student numbers, transaction numbers\nand other identifiers that can't be interpreted naively as numbers. The field\nthat studies this sort of data has various names including relational learning,\nstatistical relational AI, and many others. This paper explains why relational\nlearning is not taking over the world -- except in a few cases with restricted\nrelations -- and what needs to be done to bring it to it's rightful prominence.\n", "link": "http://arxiv.org/abs/2507.13558v5", "date": "2025-11-05", "relevancy": 1.8968, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4766}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4766}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Why%20Isn%27t%20Relational%20Learning%20Taking%20Over%20the%20World%3F&body=Title%3A%20Why%20Isn%27t%20Relational%20Learning%20Taking%20Over%20the%20World%3F%0AAuthor%3A%20David%20Poole%0AAbstract%3A%20%20%20Artificial%20intelligence%20seems%20to%20be%20taking%20over%20the%20world%20with%20systems%20that%0Amodel%20pixels%2C%20words%2C%20and%20phonemes.%20The%20world%20is%20arguably%20made%20up%2C%20not%20of%0Apixels%2C%20words%2C%20and%20phonemes%20but%20of%20entities%20%28objects%2C%20things%2C%20including%20events%29%0Awith%20properties%20and%20relations%20among%20them.%20Surely%20we%20should%20model%20these%2C%20not%20the%0Aperception%20or%20description%20of%20them.%20You%20might%20suspect%20that%20concentrating%20on%0Amodeling%20words%20and%20pixels%20is%20because%20all%20of%20the%20%28valuable%29%20data%20in%20the%20world%20is%0Ain%20terms%20of%20text%20and%20images.%20If%20you%20look%20into%20almost%20any%20company%20you%20will%20find%0Atheir%20most%20valuable%20data%20is%20in%20spreadsheets%2C%20databases%20and%20other%20relational%0Aformats.%20These%20are%20not%20the%20form%20that%20are%20studied%20in%20introductory%20machine%0Alearning%2C%20but%20are%20full%20of%20product%20numbers%2C%20student%20numbers%2C%20transaction%20numbers%0Aand%20other%20identifiers%20that%20can%27t%20be%20interpreted%20naively%20as%20numbers.%20The%20field%0Athat%20studies%20this%20sort%20of%20data%20has%20various%20names%20including%20relational%20learning%2C%0Astatistical%20relational%20AI%2C%20and%20many%20others.%20This%20paper%20explains%20why%20relational%0Alearning%20is%20not%20taking%20over%20the%20world%20--%20except%20in%20a%20few%20cases%20with%20restricted%0Arelations%20--%20and%20what%20needs%20to%20be%20done%20to%20bring%20it%20to%20it%27s%20rightful%20prominence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13558v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhy%2520Isn%2527t%2520Relational%2520Learning%2520Taking%2520Over%2520the%2520World%253F%26entry.906535625%3DDavid%2520Poole%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520seems%2520to%2520be%2520taking%2520over%2520the%2520world%2520with%2520systems%2520that%250Amodel%2520pixels%252C%2520words%252C%2520and%2520phonemes.%2520The%2520world%2520is%2520arguably%2520made%2520up%252C%2520not%2520of%250Apixels%252C%2520words%252C%2520and%2520phonemes%2520but%2520of%2520entities%2520%2528objects%252C%2520things%252C%2520including%2520events%2529%250Awith%2520properties%2520and%2520relations%2520among%2520them.%2520Surely%2520we%2520should%2520model%2520these%252C%2520not%2520the%250Aperception%2520or%2520description%2520of%2520them.%2520You%2520might%2520suspect%2520that%2520concentrating%2520on%250Amodeling%2520words%2520and%2520pixels%2520is%2520because%2520all%2520of%2520the%2520%2528valuable%2529%2520data%2520in%2520the%2520world%2520is%250Ain%2520terms%2520of%2520text%2520and%2520images.%2520If%2520you%2520look%2520into%2520almost%2520any%2520company%2520you%2520will%2520find%250Atheir%2520most%2520valuable%2520data%2520is%2520in%2520spreadsheets%252C%2520databases%2520and%2520other%2520relational%250Aformats.%2520These%2520are%2520not%2520the%2520form%2520that%2520are%2520studied%2520in%2520introductory%2520machine%250Alearning%252C%2520but%2520are%2520full%2520of%2520product%2520numbers%252C%2520student%2520numbers%252C%2520transaction%2520numbers%250Aand%2520other%2520identifiers%2520that%2520can%2527t%2520be%2520interpreted%2520naively%2520as%2520numbers.%2520The%2520field%250Athat%2520studies%2520this%2520sort%2520of%2520data%2520has%2520various%2520names%2520including%2520relational%2520learning%252C%250Astatistical%2520relational%2520AI%252C%2520and%2520many%2520others.%2520This%2520paper%2520explains%2520why%2520relational%250Alearning%2520is%2520not%2520taking%2520over%2520the%2520world%2520--%2520except%2520in%2520a%2520few%2520cases%2520with%2520restricted%250Arelations%2520--%2520and%2520what%2520needs%2520to%2520be%2520done%2520to%2520bring%2520it%2520to%2520it%2527s%2520rightful%2520prominence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13558v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20Isn%27t%20Relational%20Learning%20Taking%20Over%20the%20World%3F&entry.906535625=David%20Poole&entry.1292438233=%20%20Artificial%20intelligence%20seems%20to%20be%20taking%20over%20the%20world%20with%20systems%20that%0Amodel%20pixels%2C%20words%2C%20and%20phonemes.%20The%20world%20is%20arguably%20made%20up%2C%20not%20of%0Apixels%2C%20words%2C%20and%20phonemes%20but%20of%20entities%20%28objects%2C%20things%2C%20including%20events%29%0Awith%20properties%20and%20relations%20among%20them.%20Surely%20we%20should%20model%20these%2C%20not%20the%0Aperception%20or%20description%20of%20them.%20You%20might%20suspect%20that%20concentrating%20on%0Amodeling%20words%20and%20pixels%20is%20because%20all%20of%20the%20%28valuable%29%20data%20in%20the%20world%20is%0Ain%20terms%20of%20text%20and%20images.%20If%20you%20look%20into%20almost%20any%20company%20you%20will%20find%0Atheir%20most%20valuable%20data%20is%20in%20spreadsheets%2C%20databases%20and%20other%20relational%0Aformats.%20These%20are%20not%20the%20form%20that%20are%20studied%20in%20introductory%20machine%0Alearning%2C%20but%20are%20full%20of%20product%20numbers%2C%20student%20numbers%2C%20transaction%20numbers%0Aand%20other%20identifiers%20that%20can%27t%20be%20interpreted%20naively%20as%20numbers.%20The%20field%0Athat%20studies%20this%20sort%20of%20data%20has%20various%20names%20including%20relational%20learning%2C%0Astatistical%20relational%20AI%2C%20and%20many%20others.%20This%20paper%20explains%20why%20relational%0Alearning%20is%20not%20taking%20over%20the%20world%20--%20except%20in%20a%20few%20cases%20with%20restricted%0Arelations%20--%20and%20what%20needs%20to%20be%20done%20to%20bring%20it%20to%20it%27s%20rightful%20prominence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13558v5&entry.124074799=Read"},
{"title": "AILA--First Experiments with Localist Language Models", "author": "Joachim Diederich", "abstract": "  This paper presents the first empirical demonstration of controllable\nlocality in transformer language models, a novel architectural framework that\nenables continuous control over the degree of representation localization\nthrough a tunable locality dial parameter. Unlike traditional language models\nthat rely exclusively on distributed representations, our approach allows\ndynamic interpolation between highly interpretable localist encodings and\nefficient distributed representations without requiring model retraining. We\nconducted experiments on the WikiText corpus using a two-layer transformer\narchitecture, systematically varying the locality parameter {\\lambda} across\nthe full spectrum from 1.0 (fully localist) to 0.0 (fully distributed). Our\nresults demonstrate that localist configurations achieve dramatically lower\nattention entropy, with {\\lambda} = 1.0 yielding 5.36 bits compared to 7.18\nbits at {\\lambda} = 0.0, while maintaining substantially higher pointer\nfidelity scores reflecting stronger alignment with rule-specified targets.\nPrediction experiments reveal that intermediate locality values optimize the\ntradeoff between interpretability and performance, with {\\lambda} = 0.6\nachieving test perplexity of 4.65 and accuracy of 84.7%. These findings\nestablish that localist language models provide a practical framework for\napplications in regulated domains requiring both transparency and capability,\noffering precise mathematical control over the interpretability-performance\nspectrum through explicit penalty thresholds and information-theoretic design\nprinciples.\n", "link": "http://arxiv.org/abs/2511.03559v1", "date": "2025-11-05", "relevancy": 1.8967, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4824}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4761}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AILA--First%20Experiments%20with%20Localist%20Language%20Models&body=Title%3A%20AILA--First%20Experiments%20with%20Localist%20Language%20Models%0AAuthor%3A%20Joachim%20Diederich%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20first%20empirical%20demonstration%20of%20controllable%0Alocality%20in%20transformer%20language%20models%2C%20a%20novel%20architectural%20framework%20that%0Aenables%20continuous%20control%20over%20the%20degree%20of%20representation%20localization%0Athrough%20a%20tunable%20locality%20dial%20parameter.%20Unlike%20traditional%20language%20models%0Athat%20rely%20exclusively%20on%20distributed%20representations%2C%20our%20approach%20allows%0Adynamic%20interpolation%20between%20highly%20interpretable%20localist%20encodings%20and%0Aefficient%20distributed%20representations%20without%20requiring%20model%20retraining.%20We%0Aconducted%20experiments%20on%20the%20WikiText%20corpus%20using%20a%20two-layer%20transformer%0Aarchitecture%2C%20systematically%20varying%20the%20locality%20parameter%20%7B%5Clambda%7D%20across%0Athe%20full%20spectrum%20from%201.0%20%28fully%20localist%29%20to%200.0%20%28fully%20distributed%29.%20Our%0Aresults%20demonstrate%20that%20localist%20configurations%20achieve%20dramatically%20lower%0Aattention%20entropy%2C%20with%20%7B%5Clambda%7D%20%3D%201.0%20yielding%205.36%20bits%20compared%20to%207.18%0Abits%20at%20%7B%5Clambda%7D%20%3D%200.0%2C%20while%20maintaining%20substantially%20higher%20pointer%0Afidelity%20scores%20reflecting%20stronger%20alignment%20with%20rule-specified%20targets.%0APrediction%20experiments%20reveal%20that%20intermediate%20locality%20values%20optimize%20the%0Atradeoff%20between%20interpretability%20and%20performance%2C%20with%20%7B%5Clambda%7D%20%3D%200.6%0Aachieving%20test%20perplexity%20of%204.65%20and%20accuracy%20of%2084.7%25.%20These%20findings%0Aestablish%20that%20localist%20language%20models%20provide%20a%20practical%20framework%20for%0Aapplications%20in%20regulated%20domains%20requiring%20both%20transparency%20and%20capability%2C%0Aoffering%20precise%20mathematical%20control%20over%20the%20interpretability-performance%0Aspectrum%20through%20explicit%20penalty%20thresholds%20and%20information-theoretic%20design%0Aprinciples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03559v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAILA--First%2520Experiments%2520with%2520Localist%2520Language%2520Models%26entry.906535625%3DJoachim%2520Diederich%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520first%2520empirical%2520demonstration%2520of%2520controllable%250Alocality%2520in%2520transformer%2520language%2520models%252C%2520a%2520novel%2520architectural%2520framework%2520that%250Aenables%2520continuous%2520control%2520over%2520the%2520degree%2520of%2520representation%2520localization%250Athrough%2520a%2520tunable%2520locality%2520dial%2520parameter.%2520Unlike%2520traditional%2520language%2520models%250Athat%2520rely%2520exclusively%2520on%2520distributed%2520representations%252C%2520our%2520approach%2520allows%250Adynamic%2520interpolation%2520between%2520highly%2520interpretable%2520localist%2520encodings%2520and%250Aefficient%2520distributed%2520representations%2520without%2520requiring%2520model%2520retraining.%2520We%250Aconducted%2520experiments%2520on%2520the%2520WikiText%2520corpus%2520using%2520a%2520two-layer%2520transformer%250Aarchitecture%252C%2520systematically%2520varying%2520the%2520locality%2520parameter%2520%257B%255Clambda%257D%2520across%250Athe%2520full%2520spectrum%2520from%25201.0%2520%2528fully%2520localist%2529%2520to%25200.0%2520%2528fully%2520distributed%2529.%2520Our%250Aresults%2520demonstrate%2520that%2520localist%2520configurations%2520achieve%2520dramatically%2520lower%250Aattention%2520entropy%252C%2520with%2520%257B%255Clambda%257D%2520%253D%25201.0%2520yielding%25205.36%2520bits%2520compared%2520to%25207.18%250Abits%2520at%2520%257B%255Clambda%257D%2520%253D%25200.0%252C%2520while%2520maintaining%2520substantially%2520higher%2520pointer%250Afidelity%2520scores%2520reflecting%2520stronger%2520alignment%2520with%2520rule-specified%2520targets.%250APrediction%2520experiments%2520reveal%2520that%2520intermediate%2520locality%2520values%2520optimize%2520the%250Atradeoff%2520between%2520interpretability%2520and%2520performance%252C%2520with%2520%257B%255Clambda%257D%2520%253D%25200.6%250Aachieving%2520test%2520perplexity%2520of%25204.65%2520and%2520accuracy%2520of%252084.7%2525.%2520These%2520findings%250Aestablish%2520that%2520localist%2520language%2520models%2520provide%2520a%2520practical%2520framework%2520for%250Aapplications%2520in%2520regulated%2520domains%2520requiring%2520both%2520transparency%2520and%2520capability%252C%250Aoffering%2520precise%2520mathematical%2520control%2520over%2520the%2520interpretability-performance%250Aspectrum%2520through%2520explicit%2520penalty%2520thresholds%2520and%2520information-theoretic%2520design%250Aprinciples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03559v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AILA--First%20Experiments%20with%20Localist%20Language%20Models&entry.906535625=Joachim%20Diederich&entry.1292438233=%20%20This%20paper%20presents%20the%20first%20empirical%20demonstration%20of%20controllable%0Alocality%20in%20transformer%20language%20models%2C%20a%20novel%20architectural%20framework%20that%0Aenables%20continuous%20control%20over%20the%20degree%20of%20representation%20localization%0Athrough%20a%20tunable%20locality%20dial%20parameter.%20Unlike%20traditional%20language%20models%0Athat%20rely%20exclusively%20on%20distributed%20representations%2C%20our%20approach%20allows%0Adynamic%20interpolation%20between%20highly%20interpretable%20localist%20encodings%20and%0Aefficient%20distributed%20representations%20without%20requiring%20model%20retraining.%20We%0Aconducted%20experiments%20on%20the%20WikiText%20corpus%20using%20a%20two-layer%20transformer%0Aarchitecture%2C%20systematically%20varying%20the%20locality%20parameter%20%7B%5Clambda%7D%20across%0Athe%20full%20spectrum%20from%201.0%20%28fully%20localist%29%20to%200.0%20%28fully%20distributed%29.%20Our%0Aresults%20demonstrate%20that%20localist%20configurations%20achieve%20dramatically%20lower%0Aattention%20entropy%2C%20with%20%7B%5Clambda%7D%20%3D%201.0%20yielding%205.36%20bits%20compared%20to%207.18%0Abits%20at%20%7B%5Clambda%7D%20%3D%200.0%2C%20while%20maintaining%20substantially%20higher%20pointer%0Afidelity%20scores%20reflecting%20stronger%20alignment%20with%20rule-specified%20targets.%0APrediction%20experiments%20reveal%20that%20intermediate%20locality%20values%20optimize%20the%0Atradeoff%20between%20interpretability%20and%20performance%2C%20with%20%7B%5Clambda%7D%20%3D%200.6%0Aachieving%20test%20perplexity%20of%204.65%20and%20accuracy%20of%2084.7%25.%20These%20findings%0Aestablish%20that%20localist%20language%20models%20provide%20a%20practical%20framework%20for%0Aapplications%20in%20regulated%20domains%20requiring%20both%20transparency%20and%20capability%2C%0Aoffering%20precise%20mathematical%20control%20over%20the%20interpretability-performance%0Aspectrum%20through%20explicit%20penalty%20thresholds%20and%20information-theoretic%20design%0Aprinciples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03559v1&entry.124074799=Read"},
{"title": "DQN Performance with Epsilon Greedy Policies and Prioritized Experience\n  Replay", "author": "Daniel Perkins and Oscar J. Escobar and Luke Green", "abstract": "  We present a detailed study of Deep Q-Networks in finite environments,\nemphasizing the impact of epsilon-greedy exploration schedules and prioritized\nexperience replay. Through systematic experimentation, we evaluate how\nvariations in epsilon decay schedules affect learning efficiency, convergence\nbehavior, and reward optimization. We investigate how prioritized experience\nreplay leads to faster convergence and higher returns and show empirical\nresults comparing uniform, no replay, and prioritized strategies across\nmultiple simulations. Our findings illuminate the trade-offs and interactions\nbetween exploration strategies and memory management in DQN training, offering\npractical recommendations for robust reinforcement learning in\nresource-constrained settings.\n", "link": "http://arxiv.org/abs/2511.03670v1", "date": "2025-11-05", "relevancy": 1.8827, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4823}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.476}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DQN%20Performance%20with%20Epsilon%20Greedy%20Policies%20and%20Prioritized%20Experience%0A%20%20Replay&body=Title%3A%20DQN%20Performance%20with%20Epsilon%20Greedy%20Policies%20and%20Prioritized%20Experience%0A%20%20Replay%0AAuthor%3A%20Daniel%20Perkins%20and%20Oscar%20J.%20Escobar%20and%20Luke%20Green%0AAbstract%3A%20%20%20We%20present%20a%20detailed%20study%20of%20Deep%20Q-Networks%20in%20finite%20environments%2C%0Aemphasizing%20the%20impact%20of%20epsilon-greedy%20exploration%20schedules%20and%20prioritized%0Aexperience%20replay.%20Through%20systematic%20experimentation%2C%20we%20evaluate%20how%0Avariations%20in%20epsilon%20decay%20schedules%20affect%20learning%20efficiency%2C%20convergence%0Abehavior%2C%20and%20reward%20optimization.%20We%20investigate%20how%20prioritized%20experience%0Areplay%20leads%20to%20faster%20convergence%20and%20higher%20returns%20and%20show%20empirical%0Aresults%20comparing%20uniform%2C%20no%20replay%2C%20and%20prioritized%20strategies%20across%0Amultiple%20simulations.%20Our%20findings%20illuminate%20the%20trade-offs%20and%20interactions%0Abetween%20exploration%20strategies%20and%20memory%20management%20in%20DQN%20training%2C%20offering%0Apractical%20recommendations%20for%20robust%20reinforcement%20learning%20in%0Aresource-constrained%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03670v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDQN%2520Performance%2520with%2520Epsilon%2520Greedy%2520Policies%2520and%2520Prioritized%2520Experience%250A%2520%2520Replay%26entry.906535625%3DDaniel%2520Perkins%2520and%2520Oscar%2520J.%2520Escobar%2520and%2520Luke%2520Green%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520detailed%2520study%2520of%2520Deep%2520Q-Networks%2520in%2520finite%2520environments%252C%250Aemphasizing%2520the%2520impact%2520of%2520epsilon-greedy%2520exploration%2520schedules%2520and%2520prioritized%250Aexperience%2520replay.%2520Through%2520systematic%2520experimentation%252C%2520we%2520evaluate%2520how%250Avariations%2520in%2520epsilon%2520decay%2520schedules%2520affect%2520learning%2520efficiency%252C%2520convergence%250Abehavior%252C%2520and%2520reward%2520optimization.%2520We%2520investigate%2520how%2520prioritized%2520experience%250Areplay%2520leads%2520to%2520faster%2520convergence%2520and%2520higher%2520returns%2520and%2520show%2520empirical%250Aresults%2520comparing%2520uniform%252C%2520no%2520replay%252C%2520and%2520prioritized%2520strategies%2520across%250Amultiple%2520simulations.%2520Our%2520findings%2520illuminate%2520the%2520trade-offs%2520and%2520interactions%250Abetween%2520exploration%2520strategies%2520and%2520memory%2520management%2520in%2520DQN%2520training%252C%2520offering%250Apractical%2520recommendations%2520for%2520robust%2520reinforcement%2520learning%2520in%250Aresource-constrained%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03670v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DQN%20Performance%20with%20Epsilon%20Greedy%20Policies%20and%20Prioritized%20Experience%0A%20%20Replay&entry.906535625=Daniel%20Perkins%20and%20Oscar%20J.%20Escobar%20and%20Luke%20Green&entry.1292438233=%20%20We%20present%20a%20detailed%20study%20of%20Deep%20Q-Networks%20in%20finite%20environments%2C%0Aemphasizing%20the%20impact%20of%20epsilon-greedy%20exploration%20schedules%20and%20prioritized%0Aexperience%20replay.%20Through%20systematic%20experimentation%2C%20we%20evaluate%20how%0Avariations%20in%20epsilon%20decay%20schedules%20affect%20learning%20efficiency%2C%20convergence%0Abehavior%2C%20and%20reward%20optimization.%20We%20investigate%20how%20prioritized%20experience%0Areplay%20leads%20to%20faster%20convergence%20and%20higher%20returns%20and%20show%20empirical%0Aresults%20comparing%20uniform%2C%20no%20replay%2C%20and%20prioritized%20strategies%20across%0Amultiple%20simulations.%20Our%20findings%20illuminate%20the%20trade-offs%20and%20interactions%0Abetween%20exploration%20strategies%20and%20memory%20management%20in%20DQN%20training%2C%20offering%0Apractical%20recommendations%20for%20robust%20reinforcement%20learning%20in%0Aresource-constrained%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03670v1&entry.124074799=Read"},
{"title": "Flying Robotics Art: ROS-based Drone Draws the Record-Breaking Mural", "author": "Andrei A. Korigodskii and Oleg D. Kalachev and Artem E. Vasiunik and Matvei V. Urvantsev and Georgii E. Bondar", "abstract": "  This paper presents the innovative design and successful deployment of a\npioneering autonomous unmanned aerial system developed for executing the\nworld's largest mural painted by a drone. Addressing the dual challenges of\nmaintaining artistic precision and operational reliability under adverse\noutdoor conditions such as wind and direct sunlight, our work introduces a\nrobust system capable of navigating and painting outdoors with unprecedented\naccuracy. Key to our approach is a novel navigation system that combines an\ninfrared (IR) motion capture camera and LiDAR technology, enabling precise\nlocation tracking tailored specifically for largescale artistic applications.\nWe employ a unique control architecture that uses different regulation in\ntangential and normal directions relative to the planned path, enabling precise\ntrajectory tracking and stable line rendering. We also present algorithms for\ntrajectory planning and path optimization, allowing for complex curve drawing\nand area filling. The system includes a custom-designed paint spraying\nmechanism, specifically engineered to function effectively amidst the turbulent\nairflow generated by the drone's propellers, which also protects the drone's\ncritical components from paint-related damage, ensuring longevity and\nconsistent performance. Experimental results demonstrate the system's\nrobustness and precision in varied conditions, showcasing its potential for\nautonomous large-scale art creation and expanding the functional applications\nof robotics in creative fields.\n", "link": "http://arxiv.org/abs/2511.03651v1", "date": "2025-11-05", "relevancy": 1.8678, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4853}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4739}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flying%20Robotics%20Art%3A%20ROS-based%20Drone%20Draws%20the%20Record-Breaking%20Mural&body=Title%3A%20Flying%20Robotics%20Art%3A%20ROS-based%20Drone%20Draws%20the%20Record-Breaking%20Mural%0AAuthor%3A%20Andrei%20A.%20Korigodskii%20and%20Oleg%20D.%20Kalachev%20and%20Artem%20E.%20Vasiunik%20and%20Matvei%20V.%20Urvantsev%20and%20Georgii%20E.%20Bondar%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20innovative%20design%20and%20successful%20deployment%20of%20a%0Apioneering%20autonomous%20unmanned%20aerial%20system%20developed%20for%20executing%20the%0Aworld%27s%20largest%20mural%20painted%20by%20a%20drone.%20Addressing%20the%20dual%20challenges%20of%0Amaintaining%20artistic%20precision%20and%20operational%20reliability%20under%20adverse%0Aoutdoor%20conditions%20such%20as%20wind%20and%20direct%20sunlight%2C%20our%20work%20introduces%20a%0Arobust%20system%20capable%20of%20navigating%20and%20painting%20outdoors%20with%20unprecedented%0Aaccuracy.%20Key%20to%20our%20approach%20is%20a%20novel%20navigation%20system%20that%20combines%20an%0Ainfrared%20%28IR%29%20motion%20capture%20camera%20and%20LiDAR%20technology%2C%20enabling%20precise%0Alocation%20tracking%20tailored%20specifically%20for%20largescale%20artistic%20applications.%0AWe%20employ%20a%20unique%20control%20architecture%20that%20uses%20different%20regulation%20in%0Atangential%20and%20normal%20directions%20relative%20to%20the%20planned%20path%2C%20enabling%20precise%0Atrajectory%20tracking%20and%20stable%20line%20rendering.%20We%20also%20present%20algorithms%20for%0Atrajectory%20planning%20and%20path%20optimization%2C%20allowing%20for%20complex%20curve%20drawing%0Aand%20area%20filling.%20The%20system%20includes%20a%20custom-designed%20paint%20spraying%0Amechanism%2C%20specifically%20engineered%20to%20function%20effectively%20amidst%20the%20turbulent%0Aairflow%20generated%20by%20the%20drone%27s%20propellers%2C%20which%20also%20protects%20the%20drone%27s%0Acritical%20components%20from%20paint-related%20damage%2C%20ensuring%20longevity%20and%0Aconsistent%20performance.%20Experimental%20results%20demonstrate%20the%20system%27s%0Arobustness%20and%20precision%20in%20varied%20conditions%2C%20showcasing%20its%20potential%20for%0Aautonomous%20large-scale%20art%20creation%20and%20expanding%20the%20functional%20applications%0Aof%20robotics%20in%20creative%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03651v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlying%2520Robotics%2520Art%253A%2520ROS-based%2520Drone%2520Draws%2520the%2520Record-Breaking%2520Mural%26entry.906535625%3DAndrei%2520A.%2520Korigodskii%2520and%2520Oleg%2520D.%2520Kalachev%2520and%2520Artem%2520E.%2520Vasiunik%2520and%2520Matvei%2520V.%2520Urvantsev%2520and%2520Georgii%2520E.%2520Bondar%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520innovative%2520design%2520and%2520successful%2520deployment%2520of%2520a%250Apioneering%2520autonomous%2520unmanned%2520aerial%2520system%2520developed%2520for%2520executing%2520the%250Aworld%2527s%2520largest%2520mural%2520painted%2520by%2520a%2520drone.%2520Addressing%2520the%2520dual%2520challenges%2520of%250Amaintaining%2520artistic%2520precision%2520and%2520operational%2520reliability%2520under%2520adverse%250Aoutdoor%2520conditions%2520such%2520as%2520wind%2520and%2520direct%2520sunlight%252C%2520our%2520work%2520introduces%2520a%250Arobust%2520system%2520capable%2520of%2520navigating%2520and%2520painting%2520outdoors%2520with%2520unprecedented%250Aaccuracy.%2520Key%2520to%2520our%2520approach%2520is%2520a%2520novel%2520navigation%2520system%2520that%2520combines%2520an%250Ainfrared%2520%2528IR%2529%2520motion%2520capture%2520camera%2520and%2520LiDAR%2520technology%252C%2520enabling%2520precise%250Alocation%2520tracking%2520tailored%2520specifically%2520for%2520largescale%2520artistic%2520applications.%250AWe%2520employ%2520a%2520unique%2520control%2520architecture%2520that%2520uses%2520different%2520regulation%2520in%250Atangential%2520and%2520normal%2520directions%2520relative%2520to%2520the%2520planned%2520path%252C%2520enabling%2520precise%250Atrajectory%2520tracking%2520and%2520stable%2520line%2520rendering.%2520We%2520also%2520present%2520algorithms%2520for%250Atrajectory%2520planning%2520and%2520path%2520optimization%252C%2520allowing%2520for%2520complex%2520curve%2520drawing%250Aand%2520area%2520filling.%2520The%2520system%2520includes%2520a%2520custom-designed%2520paint%2520spraying%250Amechanism%252C%2520specifically%2520engineered%2520to%2520function%2520effectively%2520amidst%2520the%2520turbulent%250Aairflow%2520generated%2520by%2520the%2520drone%2527s%2520propellers%252C%2520which%2520also%2520protects%2520the%2520drone%2527s%250Acritical%2520components%2520from%2520paint-related%2520damage%252C%2520ensuring%2520longevity%2520and%250Aconsistent%2520performance.%2520Experimental%2520results%2520demonstrate%2520the%2520system%2527s%250Arobustness%2520and%2520precision%2520in%2520varied%2520conditions%252C%2520showcasing%2520its%2520potential%2520for%250Aautonomous%2520large-scale%2520art%2520creation%2520and%2520expanding%2520the%2520functional%2520applications%250Aof%2520robotics%2520in%2520creative%2520fields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03651v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flying%20Robotics%20Art%3A%20ROS-based%20Drone%20Draws%20the%20Record-Breaking%20Mural&entry.906535625=Andrei%20A.%20Korigodskii%20and%20Oleg%20D.%20Kalachev%20and%20Artem%20E.%20Vasiunik%20and%20Matvei%20V.%20Urvantsev%20and%20Georgii%20E.%20Bondar&entry.1292438233=%20%20This%20paper%20presents%20the%20innovative%20design%20and%20successful%20deployment%20of%20a%0Apioneering%20autonomous%20unmanned%20aerial%20system%20developed%20for%20executing%20the%0Aworld%27s%20largest%20mural%20painted%20by%20a%20drone.%20Addressing%20the%20dual%20challenges%20of%0Amaintaining%20artistic%20precision%20and%20operational%20reliability%20under%20adverse%0Aoutdoor%20conditions%20such%20as%20wind%20and%20direct%20sunlight%2C%20our%20work%20introduces%20a%0Arobust%20system%20capable%20of%20navigating%20and%20painting%20outdoors%20with%20unprecedented%0Aaccuracy.%20Key%20to%20our%20approach%20is%20a%20novel%20navigation%20system%20that%20combines%20an%0Ainfrared%20%28IR%29%20motion%20capture%20camera%20and%20LiDAR%20technology%2C%20enabling%20precise%0Alocation%20tracking%20tailored%20specifically%20for%20largescale%20artistic%20applications.%0AWe%20employ%20a%20unique%20control%20architecture%20that%20uses%20different%20regulation%20in%0Atangential%20and%20normal%20directions%20relative%20to%20the%20planned%20path%2C%20enabling%20precise%0Atrajectory%20tracking%20and%20stable%20line%20rendering.%20We%20also%20present%20algorithms%20for%0Atrajectory%20planning%20and%20path%20optimization%2C%20allowing%20for%20complex%20curve%20drawing%0Aand%20area%20filling.%20The%20system%20includes%20a%20custom-designed%20paint%20spraying%0Amechanism%2C%20specifically%20engineered%20to%20function%20effectively%20amidst%20the%20turbulent%0Aairflow%20generated%20by%20the%20drone%27s%20propellers%2C%20which%20also%20protects%20the%20drone%27s%0Acritical%20components%20from%20paint-related%20damage%2C%20ensuring%20longevity%20and%0Aconsistent%20performance.%20Experimental%20results%20demonstrate%20the%20system%27s%0Arobustness%20and%20precision%20in%20varied%20conditions%2C%20showcasing%20its%20potential%20for%0Aautonomous%20large-scale%20art%20creation%20and%20expanding%20the%20functional%20applications%0Aof%20robotics%20in%20creative%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03651v1&entry.124074799=Read"},
{"title": "\"Accessibility people, you go work on that thing of yours over there\":\n  Addressing Disability Inclusion in AI Product Organizations", "author": "Sanika Moharana and Cynthia L. Bennett and Erin Buehler and Michael Madaio and Vinita Tibdewal and Shaun K. Kane", "abstract": "  The rapid emergence of generative AI has changed the way that technology is\ndesigned, constructed, maintained, and evaluated. Decisions made when creating\nAI-powered systems may impact some users disproportionately, such as people\nwith disabilities. In this paper, we report on an interview study with 25 AI\npractitioners across multiple roles (engineering, research, UX, and responsible\nAI) about how their work processes and artifacts may impact end users with\ndisabilities. We found that practitioners experienced friction when triaging\nproblems at the intersection of responsible AI and accessibility practices,\nnavigated contradictions between accessibility and responsible AI guidelines,\nidentified gaps in data about users with disabilities, and gathered support for\naddressing the needs of disabled stakeholders by leveraging informal volunteer\nand community groups within their company. Based on these findings, we offer\nsuggestions for new resources and process changes to better support people with\ndisabilities as end users of AI.\n", "link": "http://arxiv.org/abs/2508.16607v2", "date": "2025-11-05", "relevancy": 1.8656, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4947}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4464}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%22Accessibility%20people%2C%20you%20go%20work%20on%20that%20thing%20of%20yours%20over%20there%22%3A%0A%20%20Addressing%20Disability%20Inclusion%20in%20AI%20Product%20Organizations&body=Title%3A%20%22Accessibility%20people%2C%20you%20go%20work%20on%20that%20thing%20of%20yours%20over%20there%22%3A%0A%20%20Addressing%20Disability%20Inclusion%20in%20AI%20Product%20Organizations%0AAuthor%3A%20Sanika%20Moharana%20and%20Cynthia%20L.%20Bennett%20and%20Erin%20Buehler%20and%20Michael%20Madaio%20and%20Vinita%20Tibdewal%20and%20Shaun%20K.%20Kane%0AAbstract%3A%20%20%20The%20rapid%20emergence%20of%20generative%20AI%20has%20changed%20the%20way%20that%20technology%20is%0Adesigned%2C%20constructed%2C%20maintained%2C%20and%20evaluated.%20Decisions%20made%20when%20creating%0AAI-powered%20systems%20may%20impact%20some%20users%20disproportionately%2C%20such%20as%20people%0Awith%20disabilities.%20In%20this%20paper%2C%20we%20report%20on%20an%20interview%20study%20with%2025%20AI%0Apractitioners%20across%20multiple%20roles%20%28engineering%2C%20research%2C%20UX%2C%20and%20responsible%0AAI%29%20about%20how%20their%20work%20processes%20and%20artifacts%20may%20impact%20end%20users%20with%0Adisabilities.%20We%20found%20that%20practitioners%20experienced%20friction%20when%20triaging%0Aproblems%20at%20the%20intersection%20of%20responsible%20AI%20and%20accessibility%20practices%2C%0Anavigated%20contradictions%20between%20accessibility%20and%20responsible%20AI%20guidelines%2C%0Aidentified%20gaps%20in%20data%20about%20users%20with%20disabilities%2C%20and%20gathered%20support%20for%0Aaddressing%20the%20needs%20of%20disabled%20stakeholders%20by%20leveraging%20informal%20volunteer%0Aand%20community%20groups%20within%20their%20company.%20Based%20on%20these%20findings%2C%20we%20offer%0Asuggestions%20for%20new%20resources%20and%20process%20changes%20to%20better%20support%20people%20with%0Adisabilities%20as%20end%20users%20of%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16607v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2522Accessibility%2520people%252C%2520you%2520go%2520work%2520on%2520that%2520thing%2520of%2520yours%2520over%2520there%2522%253A%250A%2520%2520Addressing%2520Disability%2520Inclusion%2520in%2520AI%2520Product%2520Organizations%26entry.906535625%3DSanika%2520Moharana%2520and%2520Cynthia%2520L.%2520Bennett%2520and%2520Erin%2520Buehler%2520and%2520Michael%2520Madaio%2520and%2520Vinita%2520Tibdewal%2520and%2520Shaun%2520K.%2520Kane%26entry.1292438233%3D%2520%2520The%2520rapid%2520emergence%2520of%2520generative%2520AI%2520has%2520changed%2520the%2520way%2520that%2520technology%2520is%250Adesigned%252C%2520constructed%252C%2520maintained%252C%2520and%2520evaluated.%2520Decisions%2520made%2520when%2520creating%250AAI-powered%2520systems%2520may%2520impact%2520some%2520users%2520disproportionately%252C%2520such%2520as%2520people%250Awith%2520disabilities.%2520In%2520this%2520paper%252C%2520we%2520report%2520on%2520an%2520interview%2520study%2520with%252025%2520AI%250Apractitioners%2520across%2520multiple%2520roles%2520%2528engineering%252C%2520research%252C%2520UX%252C%2520and%2520responsible%250AAI%2529%2520about%2520how%2520their%2520work%2520processes%2520and%2520artifacts%2520may%2520impact%2520end%2520users%2520with%250Adisabilities.%2520We%2520found%2520that%2520practitioners%2520experienced%2520friction%2520when%2520triaging%250Aproblems%2520at%2520the%2520intersection%2520of%2520responsible%2520AI%2520and%2520accessibility%2520practices%252C%250Anavigated%2520contradictions%2520between%2520accessibility%2520and%2520responsible%2520AI%2520guidelines%252C%250Aidentified%2520gaps%2520in%2520data%2520about%2520users%2520with%2520disabilities%252C%2520and%2520gathered%2520support%2520for%250Aaddressing%2520the%2520needs%2520of%2520disabled%2520stakeholders%2520by%2520leveraging%2520informal%2520volunteer%250Aand%2520community%2520groups%2520within%2520their%2520company.%2520Based%2520on%2520these%2520findings%252C%2520we%2520offer%250Asuggestions%2520for%2520new%2520resources%2520and%2520process%2520changes%2520to%2520better%2520support%2520people%2520with%250Adisabilities%2520as%2520end%2520users%2520of%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16607v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%22Accessibility%20people%2C%20you%20go%20work%20on%20that%20thing%20of%20yours%20over%20there%22%3A%0A%20%20Addressing%20Disability%20Inclusion%20in%20AI%20Product%20Organizations&entry.906535625=Sanika%20Moharana%20and%20Cynthia%20L.%20Bennett%20and%20Erin%20Buehler%20and%20Michael%20Madaio%20and%20Vinita%20Tibdewal%20and%20Shaun%20K.%20Kane&entry.1292438233=%20%20The%20rapid%20emergence%20of%20generative%20AI%20has%20changed%20the%20way%20that%20technology%20is%0Adesigned%2C%20constructed%2C%20maintained%2C%20and%20evaluated.%20Decisions%20made%20when%20creating%0AAI-powered%20systems%20may%20impact%20some%20users%20disproportionately%2C%20such%20as%20people%0Awith%20disabilities.%20In%20this%20paper%2C%20we%20report%20on%20an%20interview%20study%20with%2025%20AI%0Apractitioners%20across%20multiple%20roles%20%28engineering%2C%20research%2C%20UX%2C%20and%20responsible%0AAI%29%20about%20how%20their%20work%20processes%20and%20artifacts%20may%20impact%20end%20users%20with%0Adisabilities.%20We%20found%20that%20practitioners%20experienced%20friction%20when%20triaging%0Aproblems%20at%20the%20intersection%20of%20responsible%20AI%20and%20accessibility%20practices%2C%0Anavigated%20contradictions%20between%20accessibility%20and%20responsible%20AI%20guidelines%2C%0Aidentified%20gaps%20in%20data%20about%20users%20with%20disabilities%2C%20and%20gathered%20support%20for%0Aaddressing%20the%20needs%20of%20disabled%20stakeholders%20by%20leveraging%20informal%20volunteer%0Aand%20community%20groups%20within%20their%20company.%20Based%20on%20these%20findings%2C%20we%20offer%0Asuggestions%20for%20new%20resources%20and%20process%20changes%20to%20better%20support%20people%20with%0Adisabilities%20as%20end%20users%20of%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16607v2&entry.124074799=Read"},
{"title": "Flat Minima and Generalization: Insights from Stochastic Convex\n  Optimization", "author": "Matan Schliserman and Shira Vansover-Hager and Tomer Koren", "abstract": "  Understanding the generalization behavior of learning algorithms is a central\ngoal of learning theory. A recently emerging explanation is that learning\nalgorithms are successful in practice because they converge to flat minima,\nwhich have been consistently associated with improved generalization\nperformance. In this work, we study the link between flat minima and\ngeneralization in the canonical setting of stochastic convex optimization with\na non-negative, $\\beta$-smooth objective. Our first finding is that, even in\nthis fundamental and well-studied setting, flat empirical minima may incur\ntrivial $\\Omega(1)$ population risk while sharp minima generalizes optimally.\nThen, we show that this poor generalization behavior extends to two natural\n''sharpness-aware'' algorithms originally proposed by Foret et al. (2021),\ndesigned to bias optimization toward flat solutions: Sharpness-Aware Gradient\nDescent (SA-GD) and Sharpness-Aware Minimization (SAM). For SA-GD, which\nperforms gradient steps on the maximal loss in a predefined neighborhood, we\nprove that while it successfully converges to a flat minimum at a fast rate,\nthe population risk of the solution can still be as large as $\\Omega(1)$,\nindicating that even flat minima found algorithmically using a sharpness-aware\ngradient method might generalize poorly. For SAM, a computationally efficient\napproximation of SA-GD based on normalized ascent steps, we show that although\nit minimizes the empirical loss, it may converge to a sharp minimum and also\nincur population risk $\\Omega(1)$. Finally, we establish population risk upper\nbounds for both SA-GD and SAM using algorithmic stability techniques.\n", "link": "http://arxiv.org/abs/2511.03548v1", "date": "2025-11-05", "relevancy": 1.8544, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4686}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4638}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flat%20Minima%20and%20Generalization%3A%20Insights%20from%20Stochastic%20Convex%0A%20%20Optimization&body=Title%3A%20Flat%20Minima%20and%20Generalization%3A%20Insights%20from%20Stochastic%20Convex%0A%20%20Optimization%0AAuthor%3A%20Matan%20Schliserman%20and%20Shira%20Vansover-Hager%20and%20Tomer%20Koren%0AAbstract%3A%20%20%20Understanding%20the%20generalization%20behavior%20of%20learning%20algorithms%20is%20a%20central%0Agoal%20of%20learning%20theory.%20A%20recently%20emerging%20explanation%20is%20that%20learning%0Aalgorithms%20are%20successful%20in%20practice%20because%20they%20converge%20to%20flat%20minima%2C%0Awhich%20have%20been%20consistently%20associated%20with%20improved%20generalization%0Aperformance.%20In%20this%20work%2C%20we%20study%20the%20link%20between%20flat%20minima%20and%0Ageneralization%20in%20the%20canonical%20setting%20of%20stochastic%20convex%20optimization%20with%0Aa%20non-negative%2C%20%24%5Cbeta%24-smooth%20objective.%20Our%20first%20finding%20is%20that%2C%20even%20in%0Athis%20fundamental%20and%20well-studied%20setting%2C%20flat%20empirical%20minima%20may%20incur%0Atrivial%20%24%5COmega%281%29%24%20population%20risk%20while%20sharp%20minima%20generalizes%20optimally.%0AThen%2C%20we%20show%20that%20this%20poor%20generalization%20behavior%20extends%20to%20two%20natural%0A%27%27sharpness-aware%27%27%20algorithms%20originally%20proposed%20by%20Foret%20et%20al.%20%282021%29%2C%0Adesigned%20to%20bias%20optimization%20toward%20flat%20solutions%3A%20Sharpness-Aware%20Gradient%0ADescent%20%28SA-GD%29%20and%20Sharpness-Aware%20Minimization%20%28SAM%29.%20For%20SA-GD%2C%20which%0Aperforms%20gradient%20steps%20on%20the%20maximal%20loss%20in%20a%20predefined%20neighborhood%2C%20we%0Aprove%20that%20while%20it%20successfully%20converges%20to%20a%20flat%20minimum%20at%20a%20fast%20rate%2C%0Athe%20population%20risk%20of%20the%20solution%20can%20still%20be%20as%20large%20as%20%24%5COmega%281%29%24%2C%0Aindicating%20that%20even%20flat%20minima%20found%20algorithmically%20using%20a%20sharpness-aware%0Agradient%20method%20might%20generalize%20poorly.%20For%20SAM%2C%20a%20computationally%20efficient%0Aapproximation%20of%20SA-GD%20based%20on%20normalized%20ascent%20steps%2C%20we%20show%20that%20although%0Ait%20minimizes%20the%20empirical%20loss%2C%20it%20may%20converge%20to%20a%20sharp%20minimum%20and%20also%0Aincur%20population%20risk%20%24%5COmega%281%29%24.%20Finally%2C%20we%20establish%20population%20risk%20upper%0Abounds%20for%20both%20SA-GD%20and%20SAM%20using%20algorithmic%20stability%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03548v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlat%2520Minima%2520and%2520Generalization%253A%2520Insights%2520from%2520Stochastic%2520Convex%250A%2520%2520Optimization%26entry.906535625%3DMatan%2520Schliserman%2520and%2520Shira%2520Vansover-Hager%2520and%2520Tomer%2520Koren%26entry.1292438233%3D%2520%2520Understanding%2520the%2520generalization%2520behavior%2520of%2520learning%2520algorithms%2520is%2520a%2520central%250Agoal%2520of%2520learning%2520theory.%2520A%2520recently%2520emerging%2520explanation%2520is%2520that%2520learning%250Aalgorithms%2520are%2520successful%2520in%2520practice%2520because%2520they%2520converge%2520to%2520flat%2520minima%252C%250Awhich%2520have%2520been%2520consistently%2520associated%2520with%2520improved%2520generalization%250Aperformance.%2520In%2520this%2520work%252C%2520we%2520study%2520the%2520link%2520between%2520flat%2520minima%2520and%250Ageneralization%2520in%2520the%2520canonical%2520setting%2520of%2520stochastic%2520convex%2520optimization%2520with%250Aa%2520non-negative%252C%2520%2524%255Cbeta%2524-smooth%2520objective.%2520Our%2520first%2520finding%2520is%2520that%252C%2520even%2520in%250Athis%2520fundamental%2520and%2520well-studied%2520setting%252C%2520flat%2520empirical%2520minima%2520may%2520incur%250Atrivial%2520%2524%255COmega%25281%2529%2524%2520population%2520risk%2520while%2520sharp%2520minima%2520generalizes%2520optimally.%250AThen%252C%2520we%2520show%2520that%2520this%2520poor%2520generalization%2520behavior%2520extends%2520to%2520two%2520natural%250A%2527%2527sharpness-aware%2527%2527%2520algorithms%2520originally%2520proposed%2520by%2520Foret%2520et%2520al.%2520%25282021%2529%252C%250Adesigned%2520to%2520bias%2520optimization%2520toward%2520flat%2520solutions%253A%2520Sharpness-Aware%2520Gradient%250ADescent%2520%2528SA-GD%2529%2520and%2520Sharpness-Aware%2520Minimization%2520%2528SAM%2529.%2520For%2520SA-GD%252C%2520which%250Aperforms%2520gradient%2520steps%2520on%2520the%2520maximal%2520loss%2520in%2520a%2520predefined%2520neighborhood%252C%2520we%250Aprove%2520that%2520while%2520it%2520successfully%2520converges%2520to%2520a%2520flat%2520minimum%2520at%2520a%2520fast%2520rate%252C%250Athe%2520population%2520risk%2520of%2520the%2520solution%2520can%2520still%2520be%2520as%2520large%2520as%2520%2524%255COmega%25281%2529%2524%252C%250Aindicating%2520that%2520even%2520flat%2520minima%2520found%2520algorithmically%2520using%2520a%2520sharpness-aware%250Agradient%2520method%2520might%2520generalize%2520poorly.%2520For%2520SAM%252C%2520a%2520computationally%2520efficient%250Aapproximation%2520of%2520SA-GD%2520based%2520on%2520normalized%2520ascent%2520steps%252C%2520we%2520show%2520that%2520although%250Ait%2520minimizes%2520the%2520empirical%2520loss%252C%2520it%2520may%2520converge%2520to%2520a%2520sharp%2520minimum%2520and%2520also%250Aincur%2520population%2520risk%2520%2524%255COmega%25281%2529%2524.%2520Finally%252C%2520we%2520establish%2520population%2520risk%2520upper%250Abounds%2520for%2520both%2520SA-GD%2520and%2520SAM%2520using%2520algorithmic%2520stability%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03548v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flat%20Minima%20and%20Generalization%3A%20Insights%20from%20Stochastic%20Convex%0A%20%20Optimization&entry.906535625=Matan%20Schliserman%20and%20Shira%20Vansover-Hager%20and%20Tomer%20Koren&entry.1292438233=%20%20Understanding%20the%20generalization%20behavior%20of%20learning%20algorithms%20is%20a%20central%0Agoal%20of%20learning%20theory.%20A%20recently%20emerging%20explanation%20is%20that%20learning%0Aalgorithms%20are%20successful%20in%20practice%20because%20they%20converge%20to%20flat%20minima%2C%0Awhich%20have%20been%20consistently%20associated%20with%20improved%20generalization%0Aperformance.%20In%20this%20work%2C%20we%20study%20the%20link%20between%20flat%20minima%20and%0Ageneralization%20in%20the%20canonical%20setting%20of%20stochastic%20convex%20optimization%20with%0Aa%20non-negative%2C%20%24%5Cbeta%24-smooth%20objective.%20Our%20first%20finding%20is%20that%2C%20even%20in%0Athis%20fundamental%20and%20well-studied%20setting%2C%20flat%20empirical%20minima%20may%20incur%0Atrivial%20%24%5COmega%281%29%24%20population%20risk%20while%20sharp%20minima%20generalizes%20optimally.%0AThen%2C%20we%20show%20that%20this%20poor%20generalization%20behavior%20extends%20to%20two%20natural%0A%27%27sharpness-aware%27%27%20algorithms%20originally%20proposed%20by%20Foret%20et%20al.%20%282021%29%2C%0Adesigned%20to%20bias%20optimization%20toward%20flat%20solutions%3A%20Sharpness-Aware%20Gradient%0ADescent%20%28SA-GD%29%20and%20Sharpness-Aware%20Minimization%20%28SAM%29.%20For%20SA-GD%2C%20which%0Aperforms%20gradient%20steps%20on%20the%20maximal%20loss%20in%20a%20predefined%20neighborhood%2C%20we%0Aprove%20that%20while%20it%20successfully%20converges%20to%20a%20flat%20minimum%20at%20a%20fast%20rate%2C%0Athe%20population%20risk%20of%20the%20solution%20can%20still%20be%20as%20large%20as%20%24%5COmega%281%29%24%2C%0Aindicating%20that%20even%20flat%20minima%20found%20algorithmically%20using%20a%20sharpness-aware%0Agradient%20method%20might%20generalize%20poorly.%20For%20SAM%2C%20a%20computationally%20efficient%0Aapproximation%20of%20SA-GD%20based%20on%20normalized%20ascent%20steps%2C%20we%20show%20that%20although%0Ait%20minimizes%20the%20empirical%20loss%2C%20it%20may%20converge%20to%20a%20sharp%20minimum%20and%20also%0Aincur%20population%20risk%20%24%5COmega%281%29%24.%20Finally%2C%20we%20establish%20population%20risk%20upper%0Abounds%20for%20both%20SA-GD%20and%20SAM%20using%20algorithmic%20stability%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03548v1&entry.124074799=Read"},
{"title": "Intelligent Computing Social Modeling and Methodological Innovations in\n  Political Science in the Era of Large Language Models", "author": "Zhenyu Wang and Dequan Wang and Yi Xu and Lingfeng Zhou and Yiqi Zhou", "abstract": "  The recent wave of artificial intelligence, epitomized by large language\nmodels (LLMs),has presented opportunities and challenges for methodological\ninnovation in political science,sparking discussions on a potential paradigm\nshift in the social sciences. However, how can weunderstand the impact of LLMs\non knowledge production and paradigm transformation in thesocial sciences from\na comprehensive perspective that integrates technology and methodology? What\nare LLMs' specific applications and representative innovative methods in\npolitical scienceresearch? These questions, particularly from a practical\nmethodological standpoint, remainunderexplored. This paper proposes the\n\"Intelligent Computing Social Modeling\" (ICSM) methodto address these issues by\nclarifying the critical mechanisms of LLMs. ICSM leverages thestrengths of LLMs\nin idea synthesis and action simulation, advancing intellectual exploration\ninpolitical science through \"simulated social construction\" and \"simulation\nvalidation.\" Bysimulating the U.S. presidential election, this study\nempirically demonstrates the operationalpathways and methodological advantages\nof ICSM. By integrating traditional social scienceparadigms, ICSM not only\nenhances the quantitative paradigm's capability to apply big data toassess the\nimpact of factors but also provides qualitative paradigms with evidence for\nsocialmechanism discovery at the individual level, offering a powerful tool\nthat balances interpretabilityand predictability in social science research.\nThe findings suggest that LLMs will drivemethodological innovation in political\nscience through integration and improvement rather thandirect substitution.\n", "link": "http://arxiv.org/abs/2410.16301v2", "date": "2025-11-05", "relevancy": 1.8377, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4613}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4613}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intelligent%20Computing%20Social%20Modeling%20and%20Methodological%20Innovations%20in%0A%20%20Political%20Science%20in%20the%20Era%20of%20Large%20Language%20Models&body=Title%3A%20Intelligent%20Computing%20Social%20Modeling%20and%20Methodological%20Innovations%20in%0A%20%20Political%20Science%20in%20the%20Era%20of%20Large%20Language%20Models%0AAuthor%3A%20Zhenyu%20Wang%20and%20Dequan%20Wang%20and%20Yi%20Xu%20and%20Lingfeng%20Zhou%20and%20Yiqi%20Zhou%0AAbstract%3A%20%20%20The%20recent%20wave%20of%20artificial%20intelligence%2C%20epitomized%20by%20large%20language%0Amodels%20%28LLMs%29%2Chas%20presented%20opportunities%20and%20challenges%20for%20methodological%0Ainnovation%20in%20political%20science%2Csparking%20discussions%20on%20a%20potential%20paradigm%0Ashift%20in%20the%20social%20sciences.%20However%2C%20how%20can%20weunderstand%20the%20impact%20of%20LLMs%0Aon%20knowledge%20production%20and%20paradigm%20transformation%20in%20thesocial%20sciences%20from%0Aa%20comprehensive%20perspective%20that%20integrates%20technology%20and%20methodology%3F%20What%0Aare%20LLMs%27%20specific%20applications%20and%20representative%20innovative%20methods%20in%0Apolitical%20scienceresearch%3F%20These%20questions%2C%20particularly%20from%20a%20practical%0Amethodological%20standpoint%2C%20remainunderexplored.%20This%20paper%20proposes%20the%0A%22Intelligent%20Computing%20Social%20Modeling%22%20%28ICSM%29%20methodto%20address%20these%20issues%20by%0Aclarifying%20the%20critical%20mechanisms%20of%20LLMs.%20ICSM%20leverages%20thestrengths%20of%20LLMs%0Ain%20idea%20synthesis%20and%20action%20simulation%2C%20advancing%20intellectual%20exploration%0Ainpolitical%20science%20through%20%22simulated%20social%20construction%22%20and%20%22simulation%0Avalidation.%22%20Bysimulating%20the%20U.S.%20presidential%20election%2C%20this%20study%0Aempirically%20demonstrates%20the%20operationalpathways%20and%20methodological%20advantages%0Aof%20ICSM.%20By%20integrating%20traditional%20social%20scienceparadigms%2C%20ICSM%20not%20only%0Aenhances%20the%20quantitative%20paradigm%27s%20capability%20to%20apply%20big%20data%20toassess%20the%0Aimpact%20of%20factors%20but%20also%20provides%20qualitative%20paradigms%20with%20evidence%20for%0Asocialmechanism%20discovery%20at%20the%20individual%20level%2C%20offering%20a%20powerful%20tool%0Athat%20balances%20interpretabilityand%20predictability%20in%20social%20science%20research.%0AThe%20findings%20suggest%20that%20LLMs%20will%20drivemethodological%20innovation%20in%20political%0Ascience%20through%20integration%20and%20improvement%20rather%20thandirect%20substitution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16301v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntelligent%2520Computing%2520Social%2520Modeling%2520and%2520Methodological%2520Innovations%2520in%250A%2520%2520Political%2520Science%2520in%2520the%2520Era%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DZhenyu%2520Wang%2520and%2520Dequan%2520Wang%2520and%2520Yi%2520Xu%2520and%2520Lingfeng%2520Zhou%2520and%2520Yiqi%2520Zhou%26entry.1292438233%3D%2520%2520The%2520recent%2520wave%2520of%2520artificial%2520intelligence%252C%2520epitomized%2520by%2520large%2520language%250Amodels%2520%2528LLMs%2529%252Chas%2520presented%2520opportunities%2520and%2520challenges%2520for%2520methodological%250Ainnovation%2520in%2520political%2520science%252Csparking%2520discussions%2520on%2520a%2520potential%2520paradigm%250Ashift%2520in%2520the%2520social%2520sciences.%2520However%252C%2520how%2520can%2520weunderstand%2520the%2520impact%2520of%2520LLMs%250Aon%2520knowledge%2520production%2520and%2520paradigm%2520transformation%2520in%2520thesocial%2520sciences%2520from%250Aa%2520comprehensive%2520perspective%2520that%2520integrates%2520technology%2520and%2520methodology%253F%2520What%250Aare%2520LLMs%2527%2520specific%2520applications%2520and%2520representative%2520innovative%2520methods%2520in%250Apolitical%2520scienceresearch%253F%2520These%2520questions%252C%2520particularly%2520from%2520a%2520practical%250Amethodological%2520standpoint%252C%2520remainunderexplored.%2520This%2520paper%2520proposes%2520the%250A%2522Intelligent%2520Computing%2520Social%2520Modeling%2522%2520%2528ICSM%2529%2520methodto%2520address%2520these%2520issues%2520by%250Aclarifying%2520the%2520critical%2520mechanisms%2520of%2520LLMs.%2520ICSM%2520leverages%2520thestrengths%2520of%2520LLMs%250Ain%2520idea%2520synthesis%2520and%2520action%2520simulation%252C%2520advancing%2520intellectual%2520exploration%250Ainpolitical%2520science%2520through%2520%2522simulated%2520social%2520construction%2522%2520and%2520%2522simulation%250Avalidation.%2522%2520Bysimulating%2520the%2520U.S.%2520presidential%2520election%252C%2520this%2520study%250Aempirically%2520demonstrates%2520the%2520operationalpathways%2520and%2520methodological%2520advantages%250Aof%2520ICSM.%2520By%2520integrating%2520traditional%2520social%2520scienceparadigms%252C%2520ICSM%2520not%2520only%250Aenhances%2520the%2520quantitative%2520paradigm%2527s%2520capability%2520to%2520apply%2520big%2520data%2520toassess%2520the%250Aimpact%2520of%2520factors%2520but%2520also%2520provides%2520qualitative%2520paradigms%2520with%2520evidence%2520for%250Asocialmechanism%2520discovery%2520at%2520the%2520individual%2520level%252C%2520offering%2520a%2520powerful%2520tool%250Athat%2520balances%2520interpretabilityand%2520predictability%2520in%2520social%2520science%2520research.%250AThe%2520findings%2520suggest%2520that%2520LLMs%2520will%2520drivemethodological%2520innovation%2520in%2520political%250Ascience%2520through%2520integration%2520and%2520improvement%2520rather%2520thandirect%2520substitution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16301v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intelligent%20Computing%20Social%20Modeling%20and%20Methodological%20Innovations%20in%0A%20%20Political%20Science%20in%20the%20Era%20of%20Large%20Language%20Models&entry.906535625=Zhenyu%20Wang%20and%20Dequan%20Wang%20and%20Yi%20Xu%20and%20Lingfeng%20Zhou%20and%20Yiqi%20Zhou&entry.1292438233=%20%20The%20recent%20wave%20of%20artificial%20intelligence%2C%20epitomized%20by%20large%20language%0Amodels%20%28LLMs%29%2Chas%20presented%20opportunities%20and%20challenges%20for%20methodological%0Ainnovation%20in%20political%20science%2Csparking%20discussions%20on%20a%20potential%20paradigm%0Ashift%20in%20the%20social%20sciences.%20However%2C%20how%20can%20weunderstand%20the%20impact%20of%20LLMs%0Aon%20knowledge%20production%20and%20paradigm%20transformation%20in%20thesocial%20sciences%20from%0Aa%20comprehensive%20perspective%20that%20integrates%20technology%20and%20methodology%3F%20What%0Aare%20LLMs%27%20specific%20applications%20and%20representative%20innovative%20methods%20in%0Apolitical%20scienceresearch%3F%20These%20questions%2C%20particularly%20from%20a%20practical%0Amethodological%20standpoint%2C%20remainunderexplored.%20This%20paper%20proposes%20the%0A%22Intelligent%20Computing%20Social%20Modeling%22%20%28ICSM%29%20methodto%20address%20these%20issues%20by%0Aclarifying%20the%20critical%20mechanisms%20of%20LLMs.%20ICSM%20leverages%20thestrengths%20of%20LLMs%0Ain%20idea%20synthesis%20and%20action%20simulation%2C%20advancing%20intellectual%20exploration%0Ainpolitical%20science%20through%20%22simulated%20social%20construction%22%20and%20%22simulation%0Avalidation.%22%20Bysimulating%20the%20U.S.%20presidential%20election%2C%20this%20study%0Aempirically%20demonstrates%20the%20operationalpathways%20and%20methodological%20advantages%0Aof%20ICSM.%20By%20integrating%20traditional%20social%20scienceparadigms%2C%20ICSM%20not%20only%0Aenhances%20the%20quantitative%20paradigm%27s%20capability%20to%20apply%20big%20data%20toassess%20the%0Aimpact%20of%20factors%20but%20also%20provides%20qualitative%20paradigms%20with%20evidence%20for%0Asocialmechanism%20discovery%20at%20the%20individual%20level%2C%20offering%20a%20powerful%20tool%0Athat%20balances%20interpretabilityand%20predictability%20in%20social%20science%20research.%0AThe%20findings%20suggest%20that%20LLMs%20will%20drivemethodological%20innovation%20in%20political%0Ascience%20through%20integration%20and%20improvement%20rather%20thandirect%20substitution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16301v2&entry.124074799=Read"},
{"title": "Proximal Regret and Proximal Correlated Equilibria: A New Tractable\n  Solution Concept for Online Learning and Games", "author": "Yang Cai and Constantinos Daskalakis and Haipeng Luo and Chen-Yu Wei and Weiqiang Zheng", "abstract": "  Learning and computation of equilibria are central problems in game theory,\ntheory of computation, and artificial intelligence. In this work, we introduce\nproximal regret, a new notion of regret based on proximal operators that lies\nstrictly between external and swap regret. When every player employs a\nno-proximal-regret algorithm in a general convex game, the empirical\ndistribution of play converges to proximal correlated equilibria (PCE), a\nrefinement of coarse correlated equilibria. Our framework unifies several\nemerging notions in online learning and game theory-such as gradient\nequilibrium and semicoarse correlated equilibrium-and introduces new ones. Our\nmain result shows that the classic Online Gradient Descent (GD) algorithm\nachieves an optimal $O(\\sqrt{T})$ bound on proximal regret, revealing that GD,\nwithout modification, minimizes a stronger regret notion than external regret.\nThis provides a new explanation for the empirically superior performance of\ngradient descent in online learning and games. We further extend our analysis\nto Mirror Descent in the Bregman setting and to Optimistic Gradient Descent,\nwhich yields faster convergence in smooth convex games.\n", "link": "http://arxiv.org/abs/2511.01852v2", "date": "2025-11-05", "relevancy": 1.8288, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4758}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4543}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Proximal%20Regret%20and%20Proximal%20Correlated%20Equilibria%3A%20A%20New%20Tractable%0A%20%20Solution%20Concept%20for%20Online%20Learning%20and%20Games&body=Title%3A%20Proximal%20Regret%20and%20Proximal%20Correlated%20Equilibria%3A%20A%20New%20Tractable%0A%20%20Solution%20Concept%20for%20Online%20Learning%20and%20Games%0AAuthor%3A%20Yang%20Cai%20and%20Constantinos%20Daskalakis%20and%20Haipeng%20Luo%20and%20Chen-Yu%20Wei%20and%20Weiqiang%20Zheng%0AAbstract%3A%20%20%20Learning%20and%20computation%20of%20equilibria%20are%20central%20problems%20in%20game%20theory%2C%0Atheory%20of%20computation%2C%20and%20artificial%20intelligence.%20In%20this%20work%2C%20we%20introduce%0Aproximal%20regret%2C%20a%20new%20notion%20of%20regret%20based%20on%20proximal%20operators%20that%20lies%0Astrictly%20between%20external%20and%20swap%20regret.%20When%20every%20player%20employs%20a%0Ano-proximal-regret%20algorithm%20in%20a%20general%20convex%20game%2C%20the%20empirical%0Adistribution%20of%20play%20converges%20to%20proximal%20correlated%20equilibria%20%28PCE%29%2C%20a%0Arefinement%20of%20coarse%20correlated%20equilibria.%20Our%20framework%20unifies%20several%0Aemerging%20notions%20in%20online%20learning%20and%20game%20theory-such%20as%20gradient%0Aequilibrium%20and%20semicoarse%20correlated%20equilibrium-and%20introduces%20new%20ones.%20Our%0Amain%20result%20shows%20that%20the%20classic%20Online%20Gradient%20Descent%20%28GD%29%20algorithm%0Aachieves%20an%20optimal%20%24O%28%5Csqrt%7BT%7D%29%24%20bound%20on%20proximal%20regret%2C%20revealing%20that%20GD%2C%0Awithout%20modification%2C%20minimizes%20a%20stronger%20regret%20notion%20than%20external%20regret.%0AThis%20provides%20a%20new%20explanation%20for%20the%20empirically%20superior%20performance%20of%0Agradient%20descent%20in%20online%20learning%20and%20games.%20We%20further%20extend%20our%20analysis%0Ato%20Mirror%20Descent%20in%20the%20Bregman%20setting%20and%20to%20Optimistic%20Gradient%20Descent%2C%0Awhich%20yields%20faster%20convergence%20in%20smooth%20convex%20games.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.01852v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProximal%2520Regret%2520and%2520Proximal%2520Correlated%2520Equilibria%253A%2520A%2520New%2520Tractable%250A%2520%2520Solution%2520Concept%2520for%2520Online%2520Learning%2520and%2520Games%26entry.906535625%3DYang%2520Cai%2520and%2520Constantinos%2520Daskalakis%2520and%2520Haipeng%2520Luo%2520and%2520Chen-Yu%2520Wei%2520and%2520Weiqiang%2520Zheng%26entry.1292438233%3D%2520%2520Learning%2520and%2520computation%2520of%2520equilibria%2520are%2520central%2520problems%2520in%2520game%2520theory%252C%250Atheory%2520of%2520computation%252C%2520and%2520artificial%2520intelligence.%2520In%2520this%2520work%252C%2520we%2520introduce%250Aproximal%2520regret%252C%2520a%2520new%2520notion%2520of%2520regret%2520based%2520on%2520proximal%2520operators%2520that%2520lies%250Astrictly%2520between%2520external%2520and%2520swap%2520regret.%2520When%2520every%2520player%2520employs%2520a%250Ano-proximal-regret%2520algorithm%2520in%2520a%2520general%2520convex%2520game%252C%2520the%2520empirical%250Adistribution%2520of%2520play%2520converges%2520to%2520proximal%2520correlated%2520equilibria%2520%2528PCE%2529%252C%2520a%250Arefinement%2520of%2520coarse%2520correlated%2520equilibria.%2520Our%2520framework%2520unifies%2520several%250Aemerging%2520notions%2520in%2520online%2520learning%2520and%2520game%2520theory-such%2520as%2520gradient%250Aequilibrium%2520and%2520semicoarse%2520correlated%2520equilibrium-and%2520introduces%2520new%2520ones.%2520Our%250Amain%2520result%2520shows%2520that%2520the%2520classic%2520Online%2520Gradient%2520Descent%2520%2528GD%2529%2520algorithm%250Aachieves%2520an%2520optimal%2520%2524O%2528%255Csqrt%257BT%257D%2529%2524%2520bound%2520on%2520proximal%2520regret%252C%2520revealing%2520that%2520GD%252C%250Awithout%2520modification%252C%2520minimizes%2520a%2520stronger%2520regret%2520notion%2520than%2520external%2520regret.%250AThis%2520provides%2520a%2520new%2520explanation%2520for%2520the%2520empirically%2520superior%2520performance%2520of%250Agradient%2520descent%2520in%2520online%2520learning%2520and%2520games.%2520We%2520further%2520extend%2520our%2520analysis%250Ato%2520Mirror%2520Descent%2520in%2520the%2520Bregman%2520setting%2520and%2520to%2520Optimistic%2520Gradient%2520Descent%252C%250Awhich%2520yields%2520faster%2520convergence%2520in%2520smooth%2520convex%2520games.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.01852v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Proximal%20Regret%20and%20Proximal%20Correlated%20Equilibria%3A%20A%20New%20Tractable%0A%20%20Solution%20Concept%20for%20Online%20Learning%20and%20Games&entry.906535625=Yang%20Cai%20and%20Constantinos%20Daskalakis%20and%20Haipeng%20Luo%20and%20Chen-Yu%20Wei%20and%20Weiqiang%20Zheng&entry.1292438233=%20%20Learning%20and%20computation%20of%20equilibria%20are%20central%20problems%20in%20game%20theory%2C%0Atheory%20of%20computation%2C%20and%20artificial%20intelligence.%20In%20this%20work%2C%20we%20introduce%0Aproximal%20regret%2C%20a%20new%20notion%20of%20regret%20based%20on%20proximal%20operators%20that%20lies%0Astrictly%20between%20external%20and%20swap%20regret.%20When%20every%20player%20employs%20a%0Ano-proximal-regret%20algorithm%20in%20a%20general%20convex%20game%2C%20the%20empirical%0Adistribution%20of%20play%20converges%20to%20proximal%20correlated%20equilibria%20%28PCE%29%2C%20a%0Arefinement%20of%20coarse%20correlated%20equilibria.%20Our%20framework%20unifies%20several%0Aemerging%20notions%20in%20online%20learning%20and%20game%20theory-such%20as%20gradient%0Aequilibrium%20and%20semicoarse%20correlated%20equilibrium-and%20introduces%20new%20ones.%20Our%0Amain%20result%20shows%20that%20the%20classic%20Online%20Gradient%20Descent%20%28GD%29%20algorithm%0Aachieves%20an%20optimal%20%24O%28%5Csqrt%7BT%7D%29%24%20bound%20on%20proximal%20regret%2C%20revealing%20that%20GD%2C%0Awithout%20modification%2C%20minimizes%20a%20stronger%20regret%20notion%20than%20external%20regret.%0AThis%20provides%20a%20new%20explanation%20for%20the%20empirically%20superior%20performance%20of%0Agradient%20descent%20in%20online%20learning%20and%20games.%20We%20further%20extend%20our%20analysis%0Ato%20Mirror%20Descent%20in%20the%20Bregman%20setting%20and%20to%20Optimistic%20Gradient%20Descent%2C%0Awhich%20yields%20faster%20convergence%20in%20smooth%20convex%20games.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.01852v2&entry.124074799=Read"},
{"title": "Towards Formalizing Reinforcement Learning Theory", "author": "Shangtong Zhang", "abstract": "  In this paper, we formalize the almost sure convergence of $Q$-learning and\nlinear temporal difference (TD) learning with Markovian samples using the Lean\n4 theorem prover based on the Mathlib library. $Q$-learning and linear TD are\namong the earliest and most influential reinforcement learning (RL) algorithms.\nThe investigation of their convergence properties is not only a major research\ntopic during the early development of the RL field but also receives increasing\nattention nowadays. This paper formally verifies their almost sure convergence\nin a unified framework based on the Robbins-Siegmund theorem. The framework\ndeveloped in this work can be easily extended to convergence rates and other\nmodes of convergence. This work thus makes an important step towards fully\nformalizing convergent RL results. The code is available at\nhttps://github.com/ShangtongZhang/rl-theory-in-lean.\n", "link": "http://arxiv.org/abs/2511.03618v1", "date": "2025-11-05", "relevancy": 1.8279, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4707}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4573}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Formalizing%20Reinforcement%20Learning%20Theory&body=Title%3A%20Towards%20Formalizing%20Reinforcement%20Learning%20Theory%0AAuthor%3A%20Shangtong%20Zhang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20formalize%20the%20almost%20sure%20convergence%20of%20%24Q%24-learning%20and%0Alinear%20temporal%20difference%20%28TD%29%20learning%20with%20Markovian%20samples%20using%20the%20Lean%0A4%20theorem%20prover%20based%20on%20the%20Mathlib%20library.%20%24Q%24-learning%20and%20linear%20TD%20are%0Aamong%20the%20earliest%20and%20most%20influential%20reinforcement%20learning%20%28RL%29%20algorithms.%0AThe%20investigation%20of%20their%20convergence%20properties%20is%20not%20only%20a%20major%20research%0Atopic%20during%20the%20early%20development%20of%20the%20RL%20field%20but%20also%20receives%20increasing%0Aattention%20nowadays.%20This%20paper%20formally%20verifies%20their%20almost%20sure%20convergence%0Ain%20a%20unified%20framework%20based%20on%20the%20Robbins-Siegmund%20theorem.%20The%20framework%0Adeveloped%20in%20this%20work%20can%20be%20easily%20extended%20to%20convergence%20rates%20and%20other%0Amodes%20of%20convergence.%20This%20work%20thus%20makes%20an%20important%20step%20towards%20fully%0Aformalizing%20convergent%20RL%20results.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/ShangtongZhang/rl-theory-in-lean.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03618v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Formalizing%2520Reinforcement%2520Learning%2520Theory%26entry.906535625%3DShangtong%2520Zhang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520formalize%2520the%2520almost%2520sure%2520convergence%2520of%2520%2524Q%2524-learning%2520and%250Alinear%2520temporal%2520difference%2520%2528TD%2529%2520learning%2520with%2520Markovian%2520samples%2520using%2520the%2520Lean%250A4%2520theorem%2520prover%2520based%2520on%2520the%2520Mathlib%2520library.%2520%2524Q%2524-learning%2520and%2520linear%2520TD%2520are%250Aamong%2520the%2520earliest%2520and%2520most%2520influential%2520reinforcement%2520learning%2520%2528RL%2529%2520algorithms.%250AThe%2520investigation%2520of%2520their%2520convergence%2520properties%2520is%2520not%2520only%2520a%2520major%2520research%250Atopic%2520during%2520the%2520early%2520development%2520of%2520the%2520RL%2520field%2520but%2520also%2520receives%2520increasing%250Aattention%2520nowadays.%2520This%2520paper%2520formally%2520verifies%2520their%2520almost%2520sure%2520convergence%250Ain%2520a%2520unified%2520framework%2520based%2520on%2520the%2520Robbins-Siegmund%2520theorem.%2520The%2520framework%250Adeveloped%2520in%2520this%2520work%2520can%2520be%2520easily%2520extended%2520to%2520convergence%2520rates%2520and%2520other%250Amodes%2520of%2520convergence.%2520This%2520work%2520thus%2520makes%2520an%2520important%2520step%2520towards%2520fully%250Aformalizing%2520convergent%2520RL%2520results.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/ShangtongZhang/rl-theory-in-lean.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03618v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Formalizing%20Reinforcement%20Learning%20Theory&entry.906535625=Shangtong%20Zhang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20formalize%20the%20almost%20sure%20convergence%20of%20%24Q%24-learning%20and%0Alinear%20temporal%20difference%20%28TD%29%20learning%20with%20Markovian%20samples%20using%20the%20Lean%0A4%20theorem%20prover%20based%20on%20the%20Mathlib%20library.%20%24Q%24-learning%20and%20linear%20TD%20are%0Aamong%20the%20earliest%20and%20most%20influential%20reinforcement%20learning%20%28RL%29%20algorithms.%0AThe%20investigation%20of%20their%20convergence%20properties%20is%20not%20only%20a%20major%20research%0Atopic%20during%20the%20early%20development%20of%20the%20RL%20field%20but%20also%20receives%20increasing%0Aattention%20nowadays.%20This%20paper%20formally%20verifies%20their%20almost%20sure%20convergence%0Ain%20a%20unified%20framework%20based%20on%20the%20Robbins-Siegmund%20theorem.%20The%20framework%0Adeveloped%20in%20this%20work%20can%20be%20easily%20extended%20to%20convergence%20rates%20and%20other%0Amodes%20of%20convergence.%20This%20work%20thus%20makes%20an%20important%20step%20towards%20fully%0Aformalizing%20convergent%20RL%20results.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/ShangtongZhang/rl-theory-in-lean.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03618v1&entry.124074799=Read"},
{"title": "Shift Before You Learn: Enabling Low-Rank Representations in\n  Reinforcement Learning", "author": "Bastien Dubail and Stefan Stojanovic and Alexandre Prouti\u00e8re", "abstract": "  Low-rank structure is a common implicit assumption in many modern\nreinforcement learning (RL) algorithms. For instance, reward-free and\ngoal-conditioned RL methods often presume that the successor measure admits a\nlow-rank representation. In this work, we challenge this assumption by first\nremarking that the successor measure itself is not approximately low-rank.\nInstead, we demonstrate that a low-rank structure naturally emerges in the\nshifted successor measure, which captures the system dynamics after bypassing a\nfew initial transitions. We provide finite-sample performance guarantees for\nthe entry-wise estimation of a low-rank approximation of the shifted successor\nmeasure from sampled entries. Our analysis reveals that both the approximation\nand estimation errors are primarily governed by a newly introduced quantitity:\nthe spectral recoverability of the corresponding matrix. To bound this\nparameter, we derive a new class of functional inequalities for Markov chains\nthat we call Type II Poincar\\'e inequalities and from which we can quantify the\namount of shift needed for effective low-rank approximation and estimation.\nThis analysis shows in particular that the required shift depends on decay of\nthe high-order singular values of the shifted successor measure and is hence\ntypically small in practice. Additionally, we establish a connection between\nthe necessary shift and the local mixing properties of the underlying dynamical\nsystem, which provides a natural way of selecting the shift. Finally, we\nvalidate our theoretical findings with experiments, and demonstrate that\nshifting the successor measure indeed leads to improved performance in\ngoal-conditioned RL.\n", "link": "http://arxiv.org/abs/2509.05193v2", "date": "2025-11-05", "relevancy": 1.8182, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4993}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.448}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shift%20Before%20You%20Learn%3A%20Enabling%20Low-Rank%20Representations%20in%0A%20%20Reinforcement%20Learning&body=Title%3A%20Shift%20Before%20You%20Learn%3A%20Enabling%20Low-Rank%20Representations%20in%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Bastien%20Dubail%20and%20Stefan%20Stojanovic%20and%20Alexandre%20Prouti%C3%A8re%0AAbstract%3A%20%20%20Low-rank%20structure%20is%20a%20common%20implicit%20assumption%20in%20many%20modern%0Areinforcement%20learning%20%28RL%29%20algorithms.%20For%20instance%2C%20reward-free%20and%0Agoal-conditioned%20RL%20methods%20often%20presume%20that%20the%20successor%20measure%20admits%20a%0Alow-rank%20representation.%20In%20this%20work%2C%20we%20challenge%20this%20assumption%20by%20first%0Aremarking%20that%20the%20successor%20measure%20itself%20is%20not%20approximately%20low-rank.%0AInstead%2C%20we%20demonstrate%20that%20a%20low-rank%20structure%20naturally%20emerges%20in%20the%0Ashifted%20successor%20measure%2C%20which%20captures%20the%20system%20dynamics%20after%20bypassing%20a%0Afew%20initial%20transitions.%20We%20provide%20finite-sample%20performance%20guarantees%20for%0Athe%20entry-wise%20estimation%20of%20a%20low-rank%20approximation%20of%20the%20shifted%20successor%0Ameasure%20from%20sampled%20entries.%20Our%20analysis%20reveals%20that%20both%20the%20approximation%0Aand%20estimation%20errors%20are%20primarily%20governed%20by%20a%20newly%20introduced%20quantitity%3A%0Athe%20spectral%20recoverability%20of%20the%20corresponding%20matrix.%20To%20bound%20this%0Aparameter%2C%20we%20derive%20a%20new%20class%20of%20functional%20inequalities%20for%20Markov%20chains%0Athat%20we%20call%20Type%20II%20Poincar%5C%27e%20inequalities%20and%20from%20which%20we%20can%20quantify%20the%0Aamount%20of%20shift%20needed%20for%20effective%20low-rank%20approximation%20and%20estimation.%0AThis%20analysis%20shows%20in%20particular%20that%20the%20required%20shift%20depends%20on%20decay%20of%0Athe%20high-order%20singular%20values%20of%20the%20shifted%20successor%20measure%20and%20is%20hence%0Atypically%20small%20in%20practice.%20Additionally%2C%20we%20establish%20a%20connection%20between%0Athe%20necessary%20shift%20and%20the%20local%20mixing%20properties%20of%20the%20underlying%20dynamical%0Asystem%2C%20which%20provides%20a%20natural%20way%20of%20selecting%20the%20shift.%20Finally%2C%20we%0Avalidate%20our%20theoretical%20findings%20with%20experiments%2C%20and%20demonstrate%20that%0Ashifting%20the%20successor%20measure%20indeed%20leads%20to%20improved%20performance%20in%0Agoal-conditioned%20RL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05193v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShift%2520Before%2520You%2520Learn%253A%2520Enabling%2520Low-Rank%2520Representations%2520in%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DBastien%2520Dubail%2520and%2520Stefan%2520Stojanovic%2520and%2520Alexandre%2520Prouti%25C3%25A8re%26entry.1292438233%3D%2520%2520Low-rank%2520structure%2520is%2520a%2520common%2520implicit%2520assumption%2520in%2520many%2520modern%250Areinforcement%2520learning%2520%2528RL%2529%2520algorithms.%2520For%2520instance%252C%2520reward-free%2520and%250Agoal-conditioned%2520RL%2520methods%2520often%2520presume%2520that%2520the%2520successor%2520measure%2520admits%2520a%250Alow-rank%2520representation.%2520In%2520this%2520work%252C%2520we%2520challenge%2520this%2520assumption%2520by%2520first%250Aremarking%2520that%2520the%2520successor%2520measure%2520itself%2520is%2520not%2520approximately%2520low-rank.%250AInstead%252C%2520we%2520demonstrate%2520that%2520a%2520low-rank%2520structure%2520naturally%2520emerges%2520in%2520the%250Ashifted%2520successor%2520measure%252C%2520which%2520captures%2520the%2520system%2520dynamics%2520after%2520bypassing%2520a%250Afew%2520initial%2520transitions.%2520We%2520provide%2520finite-sample%2520performance%2520guarantees%2520for%250Athe%2520entry-wise%2520estimation%2520of%2520a%2520low-rank%2520approximation%2520of%2520the%2520shifted%2520successor%250Ameasure%2520from%2520sampled%2520entries.%2520Our%2520analysis%2520reveals%2520that%2520both%2520the%2520approximation%250Aand%2520estimation%2520errors%2520are%2520primarily%2520governed%2520by%2520a%2520newly%2520introduced%2520quantitity%253A%250Athe%2520spectral%2520recoverability%2520of%2520the%2520corresponding%2520matrix.%2520To%2520bound%2520this%250Aparameter%252C%2520we%2520derive%2520a%2520new%2520class%2520of%2520functional%2520inequalities%2520for%2520Markov%2520chains%250Athat%2520we%2520call%2520Type%2520II%2520Poincar%255C%2527e%2520inequalities%2520and%2520from%2520which%2520we%2520can%2520quantify%2520the%250Aamount%2520of%2520shift%2520needed%2520for%2520effective%2520low-rank%2520approximation%2520and%2520estimation.%250AThis%2520analysis%2520shows%2520in%2520particular%2520that%2520the%2520required%2520shift%2520depends%2520on%2520decay%2520of%250Athe%2520high-order%2520singular%2520values%2520of%2520the%2520shifted%2520successor%2520measure%2520and%2520is%2520hence%250Atypically%2520small%2520in%2520practice.%2520Additionally%252C%2520we%2520establish%2520a%2520connection%2520between%250Athe%2520necessary%2520shift%2520and%2520the%2520local%2520mixing%2520properties%2520of%2520the%2520underlying%2520dynamical%250Asystem%252C%2520which%2520provides%2520a%2520natural%2520way%2520of%2520selecting%2520the%2520shift.%2520Finally%252C%2520we%250Avalidate%2520our%2520theoretical%2520findings%2520with%2520experiments%252C%2520and%2520demonstrate%2520that%250Ashifting%2520the%2520successor%2520measure%2520indeed%2520leads%2520to%2520improved%2520performance%2520in%250Agoal-conditioned%2520RL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05193v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shift%20Before%20You%20Learn%3A%20Enabling%20Low-Rank%20Representations%20in%0A%20%20Reinforcement%20Learning&entry.906535625=Bastien%20Dubail%20and%20Stefan%20Stojanovic%20and%20Alexandre%20Prouti%C3%A8re&entry.1292438233=%20%20Low-rank%20structure%20is%20a%20common%20implicit%20assumption%20in%20many%20modern%0Areinforcement%20learning%20%28RL%29%20algorithms.%20For%20instance%2C%20reward-free%20and%0Agoal-conditioned%20RL%20methods%20often%20presume%20that%20the%20successor%20measure%20admits%20a%0Alow-rank%20representation.%20In%20this%20work%2C%20we%20challenge%20this%20assumption%20by%20first%0Aremarking%20that%20the%20successor%20measure%20itself%20is%20not%20approximately%20low-rank.%0AInstead%2C%20we%20demonstrate%20that%20a%20low-rank%20structure%20naturally%20emerges%20in%20the%0Ashifted%20successor%20measure%2C%20which%20captures%20the%20system%20dynamics%20after%20bypassing%20a%0Afew%20initial%20transitions.%20We%20provide%20finite-sample%20performance%20guarantees%20for%0Athe%20entry-wise%20estimation%20of%20a%20low-rank%20approximation%20of%20the%20shifted%20successor%0Ameasure%20from%20sampled%20entries.%20Our%20analysis%20reveals%20that%20both%20the%20approximation%0Aand%20estimation%20errors%20are%20primarily%20governed%20by%20a%20newly%20introduced%20quantitity%3A%0Athe%20spectral%20recoverability%20of%20the%20corresponding%20matrix.%20To%20bound%20this%0Aparameter%2C%20we%20derive%20a%20new%20class%20of%20functional%20inequalities%20for%20Markov%20chains%0Athat%20we%20call%20Type%20II%20Poincar%5C%27e%20inequalities%20and%20from%20which%20we%20can%20quantify%20the%0Aamount%20of%20shift%20needed%20for%20effective%20low-rank%20approximation%20and%20estimation.%0AThis%20analysis%20shows%20in%20particular%20that%20the%20required%20shift%20depends%20on%20decay%20of%0Athe%20high-order%20singular%20values%20of%20the%20shifted%20successor%20measure%20and%20is%20hence%0Atypically%20small%20in%20practice.%20Additionally%2C%20we%20establish%20a%20connection%20between%0Athe%20necessary%20shift%20and%20the%20local%20mixing%20properties%20of%20the%20underlying%20dynamical%0Asystem%2C%20which%20provides%20a%20natural%20way%20of%20selecting%20the%20shift.%20Finally%2C%20we%0Avalidate%20our%20theoretical%20findings%20with%20experiments%2C%20and%20demonstrate%20that%0Ashifting%20the%20successor%20measure%20indeed%20leads%20to%20improved%20performance%20in%0Agoal-conditioned%20RL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05193v2&entry.124074799=Read"},
{"title": "ChiMDQA: Towards Comprehensive Chinese Document QA with Fine-grained\n  Evaluation", "author": "Jing Gao and Shutiao Luo and Yumeng Liu and Yuanming Li and Hongji Zeng", "abstract": "  With the rapid advancement of natural language processing (NLP) technologies,\nthe demand for high-quality Chinese document question-answering datasets is\nsteadily growing. To address this issue, we present the Chinese Multi-Document\nQuestion Answering Dataset(ChiMDQA), specifically designed for downstream\nbusiness scenarios across prevalent domains including academic, education,\nfinance, law, medical treatment, and news. ChiMDQA encompasses long-form\ndocuments from six distinct fields, consisting of 6,068 rigorously curated,\nhigh-quality question-answer (QA) pairs further classified into ten\nfine-grained categories. Through meticulous document screening and a systematic\nquestion-design methodology, the dataset guarantees both diversity and high\nquality, rendering it applicable to various NLP tasks such as document\ncomprehension, knowledge extraction, and intelligent QA systems. Additionally,\nthis paper offers a comprehensive overview of the dataset's design objectives,\nconstruction methodologies, and fine-grained evaluation system, supplying a\nsubstantial foundation for future research and practical applications in\nChinese QA. The code and data are available at:\nhttps://anonymous.4open.science/r/Foxit-CHiMDQA/.\n", "link": "http://arxiv.org/abs/2511.03656v1", "date": "2025-11-05", "relevancy": 1.8153, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4939}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4462}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChiMDQA%3A%20Towards%20Comprehensive%20Chinese%20Document%20QA%20with%20Fine-grained%0A%20%20Evaluation&body=Title%3A%20ChiMDQA%3A%20Towards%20Comprehensive%20Chinese%20Document%20QA%20with%20Fine-grained%0A%20%20Evaluation%0AAuthor%3A%20Jing%20Gao%20and%20Shutiao%20Luo%20and%20Yumeng%20Liu%20and%20Yuanming%20Li%20and%20Hongji%20Zeng%0AAbstract%3A%20%20%20With%20the%20rapid%20advancement%20of%20natural%20language%20processing%20%28NLP%29%20technologies%2C%0Athe%20demand%20for%20high-quality%20Chinese%20document%20question-answering%20datasets%20is%0Asteadily%20growing.%20To%20address%20this%20issue%2C%20we%20present%20the%20Chinese%20Multi-Document%0AQuestion%20Answering%20Dataset%28ChiMDQA%29%2C%20specifically%20designed%20for%20downstream%0Abusiness%20scenarios%20across%20prevalent%20domains%20including%20academic%2C%20education%2C%0Afinance%2C%20law%2C%20medical%20treatment%2C%20and%20news.%20ChiMDQA%20encompasses%20long-form%0Adocuments%20from%20six%20distinct%20fields%2C%20consisting%20of%206%2C068%20rigorously%20curated%2C%0Ahigh-quality%20question-answer%20%28QA%29%20pairs%20further%20classified%20into%20ten%0Afine-grained%20categories.%20Through%20meticulous%20document%20screening%20and%20a%20systematic%0Aquestion-design%20methodology%2C%20the%20dataset%20guarantees%20both%20diversity%20and%20high%0Aquality%2C%20rendering%20it%20applicable%20to%20various%20NLP%20tasks%20such%20as%20document%0Acomprehension%2C%20knowledge%20extraction%2C%20and%20intelligent%20QA%20systems.%20Additionally%2C%0Athis%20paper%20offers%20a%20comprehensive%20overview%20of%20the%20dataset%27s%20design%20objectives%2C%0Aconstruction%20methodologies%2C%20and%20fine-grained%20evaluation%20system%2C%20supplying%20a%0Asubstantial%20foundation%20for%20future%20research%20and%20practical%20applications%20in%0AChinese%20QA.%20The%20code%20and%20data%20are%20available%20at%3A%0Ahttps%3A//anonymous.4open.science/r/Foxit-CHiMDQA/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03656v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChiMDQA%253A%2520Towards%2520Comprehensive%2520Chinese%2520Document%2520QA%2520with%2520Fine-grained%250A%2520%2520Evaluation%26entry.906535625%3DJing%2520Gao%2520and%2520Shutiao%2520Luo%2520and%2520Yumeng%2520Liu%2520and%2520Yuanming%2520Li%2520and%2520Hongji%2520Zeng%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520advancement%2520of%2520natural%2520language%2520processing%2520%2528NLP%2529%2520technologies%252C%250Athe%2520demand%2520for%2520high-quality%2520Chinese%2520document%2520question-answering%2520datasets%2520is%250Asteadily%2520growing.%2520To%2520address%2520this%2520issue%252C%2520we%2520present%2520the%2520Chinese%2520Multi-Document%250AQuestion%2520Answering%2520Dataset%2528ChiMDQA%2529%252C%2520specifically%2520designed%2520for%2520downstream%250Abusiness%2520scenarios%2520across%2520prevalent%2520domains%2520including%2520academic%252C%2520education%252C%250Afinance%252C%2520law%252C%2520medical%2520treatment%252C%2520and%2520news.%2520ChiMDQA%2520encompasses%2520long-form%250Adocuments%2520from%2520six%2520distinct%2520fields%252C%2520consisting%2520of%25206%252C068%2520rigorously%2520curated%252C%250Ahigh-quality%2520question-answer%2520%2528QA%2529%2520pairs%2520further%2520classified%2520into%2520ten%250Afine-grained%2520categories.%2520Through%2520meticulous%2520document%2520screening%2520and%2520a%2520systematic%250Aquestion-design%2520methodology%252C%2520the%2520dataset%2520guarantees%2520both%2520diversity%2520and%2520high%250Aquality%252C%2520rendering%2520it%2520applicable%2520to%2520various%2520NLP%2520tasks%2520such%2520as%2520document%250Acomprehension%252C%2520knowledge%2520extraction%252C%2520and%2520intelligent%2520QA%2520systems.%2520Additionally%252C%250Athis%2520paper%2520offers%2520a%2520comprehensive%2520overview%2520of%2520the%2520dataset%2527s%2520design%2520objectives%252C%250Aconstruction%2520methodologies%252C%2520and%2520fine-grained%2520evaluation%2520system%252C%2520supplying%2520a%250Asubstantial%2520foundation%2520for%2520future%2520research%2520and%2520practical%2520applications%2520in%250AChinese%2520QA.%2520The%2520code%2520and%2520data%2520are%2520available%2520at%253A%250Ahttps%253A//anonymous.4open.science/r/Foxit-CHiMDQA/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03656v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChiMDQA%3A%20Towards%20Comprehensive%20Chinese%20Document%20QA%20with%20Fine-grained%0A%20%20Evaluation&entry.906535625=Jing%20Gao%20and%20Shutiao%20Luo%20and%20Yumeng%20Liu%20and%20Yuanming%20Li%20and%20Hongji%20Zeng&entry.1292438233=%20%20With%20the%20rapid%20advancement%20of%20natural%20language%20processing%20%28NLP%29%20technologies%2C%0Athe%20demand%20for%20high-quality%20Chinese%20document%20question-answering%20datasets%20is%0Asteadily%20growing.%20To%20address%20this%20issue%2C%20we%20present%20the%20Chinese%20Multi-Document%0AQuestion%20Answering%20Dataset%28ChiMDQA%29%2C%20specifically%20designed%20for%20downstream%0Abusiness%20scenarios%20across%20prevalent%20domains%20including%20academic%2C%20education%2C%0Afinance%2C%20law%2C%20medical%20treatment%2C%20and%20news.%20ChiMDQA%20encompasses%20long-form%0Adocuments%20from%20six%20distinct%20fields%2C%20consisting%20of%206%2C068%20rigorously%20curated%2C%0Ahigh-quality%20question-answer%20%28QA%29%20pairs%20further%20classified%20into%20ten%0Afine-grained%20categories.%20Through%20meticulous%20document%20screening%20and%20a%20systematic%0Aquestion-design%20methodology%2C%20the%20dataset%20guarantees%20both%20diversity%20and%20high%0Aquality%2C%20rendering%20it%20applicable%20to%20various%20NLP%20tasks%20such%20as%20document%0Acomprehension%2C%20knowledge%20extraction%2C%20and%20intelligent%20QA%20systems.%20Additionally%2C%0Athis%20paper%20offers%20a%20comprehensive%20overview%20of%20the%20dataset%27s%20design%20objectives%2C%0Aconstruction%20methodologies%2C%20and%20fine-grained%20evaluation%20system%2C%20supplying%20a%0Asubstantial%20foundation%20for%20future%20research%20and%20practical%20applications%20in%0AChinese%20QA.%20The%20code%20and%20data%20are%20available%20at%3A%0Ahttps%3A//anonymous.4open.science/r/Foxit-CHiMDQA/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03656v1&entry.124074799=Read"},
{"title": "The ODE Method for Stochastic Approximation and Reinforcement Learning\n  with Markovian Noise", "author": "Shuze Daniel Liu and Shuhang Chen and Shangtong Zhang", "abstract": "  Stochastic approximation is a class of algorithms that update a vector\niteratively, incrementally, and stochastically, including, e.g., stochastic\ngradient descent and temporal difference learning. One fundamental challenge in\nanalyzing a stochastic approximation algorithm is to establish its stability,\ni.e., to show that the stochastic vector iterates are bounded almost surely. In\nthis paper, we extend the celebrated Borkar-Meyn theorem for stability from the\nMartingale difference noise setting to the Markovian noise setting, which\ngreatly improves its applicability in reinforcement learning, especially in\nthose off-policy reinforcement learning algorithms with linear function\napproximation and eligibility traces. Central to our analysis is the\ndiminishing asymptotic rate of change of a few functions, which is implied by\nboth a form of the strong law of large numbers and a form of the law of the\niterated logarithm.\n", "link": "http://arxiv.org/abs/2401.07844v7", "date": "2025-11-05", "relevancy": 1.7915, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5123}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4418}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20ODE%20Method%20for%20Stochastic%20Approximation%20and%20Reinforcement%20Learning%0A%20%20with%20Markovian%20Noise&body=Title%3A%20The%20ODE%20Method%20for%20Stochastic%20Approximation%20and%20Reinforcement%20Learning%0A%20%20with%20Markovian%20Noise%0AAuthor%3A%20Shuze%20Daniel%20Liu%20and%20Shuhang%20Chen%20and%20Shangtong%20Zhang%0AAbstract%3A%20%20%20Stochastic%20approximation%20is%20a%20class%20of%20algorithms%20that%20update%20a%20vector%0Aiteratively%2C%20incrementally%2C%20and%20stochastically%2C%20including%2C%20e.g.%2C%20stochastic%0Agradient%20descent%20and%20temporal%20difference%20learning.%20One%20fundamental%20challenge%20in%0Aanalyzing%20a%20stochastic%20approximation%20algorithm%20is%20to%20establish%20its%20stability%2C%0Ai.e.%2C%20to%20show%20that%20the%20stochastic%20vector%20iterates%20are%20bounded%20almost%20surely.%20In%0Athis%20paper%2C%20we%20extend%20the%20celebrated%20Borkar-Meyn%20theorem%20for%20stability%20from%20the%0AMartingale%20difference%20noise%20setting%20to%20the%20Markovian%20noise%20setting%2C%20which%0Agreatly%20improves%20its%20applicability%20in%20reinforcement%20learning%2C%20especially%20in%0Athose%20off-policy%20reinforcement%20learning%20algorithms%20with%20linear%20function%0Aapproximation%20and%20eligibility%20traces.%20Central%20to%20our%20analysis%20is%20the%0Adiminishing%20asymptotic%20rate%20of%20change%20of%20a%20few%20functions%2C%20which%20is%20implied%20by%0Aboth%20a%20form%20of%20the%20strong%20law%20of%20large%20numbers%20and%20a%20form%20of%20the%20law%20of%20the%0Aiterated%20logarithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.07844v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520ODE%2520Method%2520for%2520Stochastic%2520Approximation%2520and%2520Reinforcement%2520Learning%250A%2520%2520with%2520Markovian%2520Noise%26entry.906535625%3DShuze%2520Daniel%2520Liu%2520and%2520Shuhang%2520Chen%2520and%2520Shangtong%2520Zhang%26entry.1292438233%3D%2520%2520Stochastic%2520approximation%2520is%2520a%2520class%2520of%2520algorithms%2520that%2520update%2520a%2520vector%250Aiteratively%252C%2520incrementally%252C%2520and%2520stochastically%252C%2520including%252C%2520e.g.%252C%2520stochastic%250Agradient%2520descent%2520and%2520temporal%2520difference%2520learning.%2520One%2520fundamental%2520challenge%2520in%250Aanalyzing%2520a%2520stochastic%2520approximation%2520algorithm%2520is%2520to%2520establish%2520its%2520stability%252C%250Ai.e.%252C%2520to%2520show%2520that%2520the%2520stochastic%2520vector%2520iterates%2520are%2520bounded%2520almost%2520surely.%2520In%250Athis%2520paper%252C%2520we%2520extend%2520the%2520celebrated%2520Borkar-Meyn%2520theorem%2520for%2520stability%2520from%2520the%250AMartingale%2520difference%2520noise%2520setting%2520to%2520the%2520Markovian%2520noise%2520setting%252C%2520which%250Agreatly%2520improves%2520its%2520applicability%2520in%2520reinforcement%2520learning%252C%2520especially%2520in%250Athose%2520off-policy%2520reinforcement%2520learning%2520algorithms%2520with%2520linear%2520function%250Aapproximation%2520and%2520eligibility%2520traces.%2520Central%2520to%2520our%2520analysis%2520is%2520the%250Adiminishing%2520asymptotic%2520rate%2520of%2520change%2520of%2520a%2520few%2520functions%252C%2520which%2520is%2520implied%2520by%250Aboth%2520a%2520form%2520of%2520the%2520strong%2520law%2520of%2520large%2520numbers%2520and%2520a%2520form%2520of%2520the%2520law%2520of%2520the%250Aiterated%2520logarithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.07844v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20ODE%20Method%20for%20Stochastic%20Approximation%20and%20Reinforcement%20Learning%0A%20%20with%20Markovian%20Noise&entry.906535625=Shuze%20Daniel%20Liu%20and%20Shuhang%20Chen%20and%20Shangtong%20Zhang&entry.1292438233=%20%20Stochastic%20approximation%20is%20a%20class%20of%20algorithms%20that%20update%20a%20vector%0Aiteratively%2C%20incrementally%2C%20and%20stochastically%2C%20including%2C%20e.g.%2C%20stochastic%0Agradient%20descent%20and%20temporal%20difference%20learning.%20One%20fundamental%20challenge%20in%0Aanalyzing%20a%20stochastic%20approximation%20algorithm%20is%20to%20establish%20its%20stability%2C%0Ai.e.%2C%20to%20show%20that%20the%20stochastic%20vector%20iterates%20are%20bounded%20almost%20surely.%20In%0Athis%20paper%2C%20we%20extend%20the%20celebrated%20Borkar-Meyn%20theorem%20for%20stability%20from%20the%0AMartingale%20difference%20noise%20setting%20to%20the%20Markovian%20noise%20setting%2C%20which%0Agreatly%20improves%20its%20applicability%20in%20reinforcement%20learning%2C%20especially%20in%0Athose%20off-policy%20reinforcement%20learning%20algorithms%20with%20linear%20function%0Aapproximation%20and%20eligibility%20traces.%20Central%20to%20our%20analysis%20is%20the%0Adiminishing%20asymptotic%20rate%20of%20change%20of%20a%20few%20functions%2C%20which%20is%20implied%20by%0Aboth%20a%20form%20of%20the%20strong%20law%20of%20large%20numbers%20and%20a%20form%20of%20the%20law%20of%20the%0Aiterated%20logarithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.07844v7&entry.124074799=Read"},
{"title": "The Adaptivity Barrier in Batched Nonparametric Bandits: Sharp\n  Characterization of the Price of Unknown Margin", "author": "Rong Jiang and Cong Ma", "abstract": "  We study batched nonparametric contextual bandits under a margin condition\nwhen the margin parameter $\\alpha$ is unknown. To capture the statistical price\nof this ignorance, we introduce the regret inflation criterion, defined as the\nratio between the regret of an adaptive algorithm and that of an oracle knowing\n$\\alpha$. We show that the optimal regret inflation grows polynomial with the\nhorizon $T$, with exponent precisely given by the value of a convex\noptimization problem involving the dimension, smoothness, and batch budget.\nMoreover, the minimizers of this optimization problem directly prescribe the\nbatch allocation and exploration strategy of a rate-optimal algorithm. Building\non this principle, we develop RoBIN (RObust batched algorithm with adaptive\nBINning), which achieves the optimal regret inflation up to logarithmic\nfactors. These results reveal a new adaptivity barrier: under batching,\nadaptation to an unknown margin parameter inevitably incurs a polynomial\npenalty, sharply characterized by a variational problem. Remarkably, this\nbarrier vanishes when the number of batches exceeds $\\log \\log T$; with only a\ndoubly logarithmic number of updates, one can recover the oracle regret rate up\nto polylogarithmic factors.\n", "link": "http://arxiv.org/abs/2511.03708v1", "date": "2025-11-05", "relevancy": 1.7899, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4699}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4644}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Adaptivity%20Barrier%20in%20Batched%20Nonparametric%20Bandits%3A%20Sharp%0A%20%20Characterization%20of%20the%20Price%20of%20Unknown%20Margin&body=Title%3A%20The%20Adaptivity%20Barrier%20in%20Batched%20Nonparametric%20Bandits%3A%20Sharp%0A%20%20Characterization%20of%20the%20Price%20of%20Unknown%20Margin%0AAuthor%3A%20Rong%20Jiang%20and%20Cong%20Ma%0AAbstract%3A%20%20%20We%20study%20batched%20nonparametric%20contextual%20bandits%20under%20a%20margin%20condition%0Awhen%20the%20margin%20parameter%20%24%5Calpha%24%20is%20unknown.%20To%20capture%20the%20statistical%20price%0Aof%20this%20ignorance%2C%20we%20introduce%20the%20regret%20inflation%20criterion%2C%20defined%20as%20the%0Aratio%20between%20the%20regret%20of%20an%20adaptive%20algorithm%20and%20that%20of%20an%20oracle%20knowing%0A%24%5Calpha%24.%20We%20show%20that%20the%20optimal%20regret%20inflation%20grows%20polynomial%20with%20the%0Ahorizon%20%24T%24%2C%20with%20exponent%20precisely%20given%20by%20the%20value%20of%20a%20convex%0Aoptimization%20problem%20involving%20the%20dimension%2C%20smoothness%2C%20and%20batch%20budget.%0AMoreover%2C%20the%20minimizers%20of%20this%20optimization%20problem%20directly%20prescribe%20the%0Abatch%20allocation%20and%20exploration%20strategy%20of%20a%20rate-optimal%20algorithm.%20Building%0Aon%20this%20principle%2C%20we%20develop%20RoBIN%20%28RObust%20batched%20algorithm%20with%20adaptive%0ABINning%29%2C%20which%20achieves%20the%20optimal%20regret%20inflation%20up%20to%20logarithmic%0Afactors.%20These%20results%20reveal%20a%20new%20adaptivity%20barrier%3A%20under%20batching%2C%0Aadaptation%20to%20an%20unknown%20margin%20parameter%20inevitably%20incurs%20a%20polynomial%0Apenalty%2C%20sharply%20characterized%20by%20a%20variational%20problem.%20Remarkably%2C%20this%0Abarrier%20vanishes%20when%20the%20number%20of%20batches%20exceeds%20%24%5Clog%20%5Clog%20T%24%3B%20with%20only%20a%0Adoubly%20logarithmic%20number%20of%20updates%2C%20one%20can%20recover%20the%20oracle%20regret%20rate%20up%0Ato%20polylogarithmic%20factors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03708v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Adaptivity%2520Barrier%2520in%2520Batched%2520Nonparametric%2520Bandits%253A%2520Sharp%250A%2520%2520Characterization%2520of%2520the%2520Price%2520of%2520Unknown%2520Margin%26entry.906535625%3DRong%2520Jiang%2520and%2520Cong%2520Ma%26entry.1292438233%3D%2520%2520We%2520study%2520batched%2520nonparametric%2520contextual%2520bandits%2520under%2520a%2520margin%2520condition%250Awhen%2520the%2520margin%2520parameter%2520%2524%255Calpha%2524%2520is%2520unknown.%2520To%2520capture%2520the%2520statistical%2520price%250Aof%2520this%2520ignorance%252C%2520we%2520introduce%2520the%2520regret%2520inflation%2520criterion%252C%2520defined%2520as%2520the%250Aratio%2520between%2520the%2520regret%2520of%2520an%2520adaptive%2520algorithm%2520and%2520that%2520of%2520an%2520oracle%2520knowing%250A%2524%255Calpha%2524.%2520We%2520show%2520that%2520the%2520optimal%2520regret%2520inflation%2520grows%2520polynomial%2520with%2520the%250Ahorizon%2520%2524T%2524%252C%2520with%2520exponent%2520precisely%2520given%2520by%2520the%2520value%2520of%2520a%2520convex%250Aoptimization%2520problem%2520involving%2520the%2520dimension%252C%2520smoothness%252C%2520and%2520batch%2520budget.%250AMoreover%252C%2520the%2520minimizers%2520of%2520this%2520optimization%2520problem%2520directly%2520prescribe%2520the%250Abatch%2520allocation%2520and%2520exploration%2520strategy%2520of%2520a%2520rate-optimal%2520algorithm.%2520Building%250Aon%2520this%2520principle%252C%2520we%2520develop%2520RoBIN%2520%2528RObust%2520batched%2520algorithm%2520with%2520adaptive%250ABINning%2529%252C%2520which%2520achieves%2520the%2520optimal%2520regret%2520inflation%2520up%2520to%2520logarithmic%250Afactors.%2520These%2520results%2520reveal%2520a%2520new%2520adaptivity%2520barrier%253A%2520under%2520batching%252C%250Aadaptation%2520to%2520an%2520unknown%2520margin%2520parameter%2520inevitably%2520incurs%2520a%2520polynomial%250Apenalty%252C%2520sharply%2520characterized%2520by%2520a%2520variational%2520problem.%2520Remarkably%252C%2520this%250Abarrier%2520vanishes%2520when%2520the%2520number%2520of%2520batches%2520exceeds%2520%2524%255Clog%2520%255Clog%2520T%2524%253B%2520with%2520only%2520a%250Adoubly%2520logarithmic%2520number%2520of%2520updates%252C%2520one%2520can%2520recover%2520the%2520oracle%2520regret%2520rate%2520up%250Ato%2520polylogarithmic%2520factors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03708v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Adaptivity%20Barrier%20in%20Batched%20Nonparametric%20Bandits%3A%20Sharp%0A%20%20Characterization%20of%20the%20Price%20of%20Unknown%20Margin&entry.906535625=Rong%20Jiang%20and%20Cong%20Ma&entry.1292438233=%20%20We%20study%20batched%20nonparametric%20contextual%20bandits%20under%20a%20margin%20condition%0Awhen%20the%20margin%20parameter%20%24%5Calpha%24%20is%20unknown.%20To%20capture%20the%20statistical%20price%0Aof%20this%20ignorance%2C%20we%20introduce%20the%20regret%20inflation%20criterion%2C%20defined%20as%20the%0Aratio%20between%20the%20regret%20of%20an%20adaptive%20algorithm%20and%20that%20of%20an%20oracle%20knowing%0A%24%5Calpha%24.%20We%20show%20that%20the%20optimal%20regret%20inflation%20grows%20polynomial%20with%20the%0Ahorizon%20%24T%24%2C%20with%20exponent%20precisely%20given%20by%20the%20value%20of%20a%20convex%0Aoptimization%20problem%20involving%20the%20dimension%2C%20smoothness%2C%20and%20batch%20budget.%0AMoreover%2C%20the%20minimizers%20of%20this%20optimization%20problem%20directly%20prescribe%20the%0Abatch%20allocation%20and%20exploration%20strategy%20of%20a%20rate-optimal%20algorithm.%20Building%0Aon%20this%20principle%2C%20we%20develop%20RoBIN%20%28RObust%20batched%20algorithm%20with%20adaptive%0ABINning%29%2C%20which%20achieves%20the%20optimal%20regret%20inflation%20up%20to%20logarithmic%0Afactors.%20These%20results%20reveal%20a%20new%20adaptivity%20barrier%3A%20under%20batching%2C%0Aadaptation%20to%20an%20unknown%20margin%20parameter%20inevitably%20incurs%20a%20polynomial%0Apenalty%2C%20sharply%20characterized%20by%20a%20variational%20problem.%20Remarkably%2C%20this%0Abarrier%20vanishes%20when%20the%20number%20of%20batches%20exceeds%20%24%5Clog%20%5Clog%20T%24%3B%20with%20only%20a%0Adoubly%20logarithmic%20number%20of%20updates%2C%20one%20can%20recover%20the%20oracle%20regret%20rate%20up%0Ato%20polylogarithmic%20factors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03708v1&entry.124074799=Read"},
{"title": "Shrinking the Variance: Shrinkage Baselines for Reinforcement Learning\n  with Verifiable Rewards", "author": "Guanning Zeng and Zhaoyi Zhou and Daman Arora and Andrea Zanette", "abstract": "  Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful paradigm for post-training large reasoning models (LRMs) using\npolicy-gradient methods such as GRPO. To stabilize training, these methods\ntypically center trajectory rewards by subtracting the empirical mean for each\nprompt. Statistically, this centering acts as a control variate (or baseline),\nreducing the variance of the policy-gradient estimator.\n  Typically, the mean reward is estimated using per-prompt empirical averages\nfor each prompt in a batch. Drawing inspiration from Stein's paradox, we\npropose using shrinkage estimators that combine per-prompt and across-prompt\nmeans to improve the overall per-prompt mean estimation accuracy --\nparticularly in the low-generation regime typical of RLVR. Theoretically, we\nconstruct a shrinkage-based baseline that provably yields lower-variance\npolicy-gradient estimators across algorithms. Our proposed baseline serves as a\ndrop-in replacement for existing per-prompt mean baselines, requiring no\nadditional hyper-parameters or computation. Empirically, shrinkage baselines\nconsistently outperform standard empirical-mean baselines, leading to\nlower-variance gradient updates and improved training stability.\n", "link": "http://arxiv.org/abs/2511.03710v1", "date": "2025-11-05", "relevancy": 1.763, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4566}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.441}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shrinking%20the%20Variance%3A%20Shrinkage%20Baselines%20for%20Reinforcement%20Learning%0A%20%20with%20Verifiable%20Rewards&body=Title%3A%20Shrinking%20the%20Variance%3A%20Shrinkage%20Baselines%20for%20Reinforcement%20Learning%0A%20%20with%20Verifiable%20Rewards%0AAuthor%3A%20Guanning%20Zeng%20and%20Zhaoyi%20Zhou%20and%20Daman%20Arora%20and%20Andrea%20Zanette%0AAbstract%3A%20%20%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20emerged%20as%20a%0Apowerful%20paradigm%20for%20post-training%20large%20reasoning%20models%20%28LRMs%29%20using%0Apolicy-gradient%20methods%20such%20as%20GRPO.%20To%20stabilize%20training%2C%20these%20methods%0Atypically%20center%20trajectory%20rewards%20by%20subtracting%20the%20empirical%20mean%20for%20each%0Aprompt.%20Statistically%2C%20this%20centering%20acts%20as%20a%20control%20variate%20%28or%20baseline%29%2C%0Areducing%20the%20variance%20of%20the%20policy-gradient%20estimator.%0A%20%20Typically%2C%20the%20mean%20reward%20is%20estimated%20using%20per-prompt%20empirical%20averages%0Afor%20each%20prompt%20in%20a%20batch.%20Drawing%20inspiration%20from%20Stein%27s%20paradox%2C%20we%0Apropose%20using%20shrinkage%20estimators%20that%20combine%20per-prompt%20and%20across-prompt%0Ameans%20to%20improve%20the%20overall%20per-prompt%20mean%20estimation%20accuracy%20--%0Aparticularly%20in%20the%20low-generation%20regime%20typical%20of%20RLVR.%20Theoretically%2C%20we%0Aconstruct%20a%20shrinkage-based%20baseline%20that%20provably%20yields%20lower-variance%0Apolicy-gradient%20estimators%20across%20algorithms.%20Our%20proposed%20baseline%20serves%20as%20a%0Adrop-in%20replacement%20for%20existing%20per-prompt%20mean%20baselines%2C%20requiring%20no%0Aadditional%20hyper-parameters%20or%20computation.%20Empirically%2C%20shrinkage%20baselines%0Aconsistently%20outperform%20standard%20empirical-mean%20baselines%2C%20leading%20to%0Alower-variance%20gradient%20updates%20and%20improved%20training%20stability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03710v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShrinking%2520the%2520Variance%253A%2520Shrinkage%2520Baselines%2520for%2520Reinforcement%2520Learning%250A%2520%2520with%2520Verifiable%2520Rewards%26entry.906535625%3DGuanning%2520Zeng%2520and%2520Zhaoyi%2520Zhou%2520and%2520Daman%2520Arora%2520and%2520Andrea%2520Zanette%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520has%2520emerged%2520as%2520a%250Apowerful%2520paradigm%2520for%2520post-training%2520large%2520reasoning%2520models%2520%2528LRMs%2529%2520using%250Apolicy-gradient%2520methods%2520such%2520as%2520GRPO.%2520To%2520stabilize%2520training%252C%2520these%2520methods%250Atypically%2520center%2520trajectory%2520rewards%2520by%2520subtracting%2520the%2520empirical%2520mean%2520for%2520each%250Aprompt.%2520Statistically%252C%2520this%2520centering%2520acts%2520as%2520a%2520control%2520variate%2520%2528or%2520baseline%2529%252C%250Areducing%2520the%2520variance%2520of%2520the%2520policy-gradient%2520estimator.%250A%2520%2520Typically%252C%2520the%2520mean%2520reward%2520is%2520estimated%2520using%2520per-prompt%2520empirical%2520averages%250Afor%2520each%2520prompt%2520in%2520a%2520batch.%2520Drawing%2520inspiration%2520from%2520Stein%2527s%2520paradox%252C%2520we%250Apropose%2520using%2520shrinkage%2520estimators%2520that%2520combine%2520per-prompt%2520and%2520across-prompt%250Ameans%2520to%2520improve%2520the%2520overall%2520per-prompt%2520mean%2520estimation%2520accuracy%2520--%250Aparticularly%2520in%2520the%2520low-generation%2520regime%2520typical%2520of%2520RLVR.%2520Theoretically%252C%2520we%250Aconstruct%2520a%2520shrinkage-based%2520baseline%2520that%2520provably%2520yields%2520lower-variance%250Apolicy-gradient%2520estimators%2520across%2520algorithms.%2520Our%2520proposed%2520baseline%2520serves%2520as%2520a%250Adrop-in%2520replacement%2520for%2520existing%2520per-prompt%2520mean%2520baselines%252C%2520requiring%2520no%250Aadditional%2520hyper-parameters%2520or%2520computation.%2520Empirically%252C%2520shrinkage%2520baselines%250Aconsistently%2520outperform%2520standard%2520empirical-mean%2520baselines%252C%2520leading%2520to%250Alower-variance%2520gradient%2520updates%2520and%2520improved%2520training%2520stability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03710v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shrinking%20the%20Variance%3A%20Shrinkage%20Baselines%20for%20Reinforcement%20Learning%0A%20%20with%20Verifiable%20Rewards&entry.906535625=Guanning%20Zeng%20and%20Zhaoyi%20Zhou%20and%20Daman%20Arora%20and%20Andrea%20Zanette&entry.1292438233=%20%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20emerged%20as%20a%0Apowerful%20paradigm%20for%20post-training%20large%20reasoning%20models%20%28LRMs%29%20using%0Apolicy-gradient%20methods%20such%20as%20GRPO.%20To%20stabilize%20training%2C%20these%20methods%0Atypically%20center%20trajectory%20rewards%20by%20subtracting%20the%20empirical%20mean%20for%20each%0Aprompt.%20Statistically%2C%20this%20centering%20acts%20as%20a%20control%20variate%20%28or%20baseline%29%2C%0Areducing%20the%20variance%20of%20the%20policy-gradient%20estimator.%0A%20%20Typically%2C%20the%20mean%20reward%20is%20estimated%20using%20per-prompt%20empirical%20averages%0Afor%20each%20prompt%20in%20a%20batch.%20Drawing%20inspiration%20from%20Stein%27s%20paradox%2C%20we%0Apropose%20using%20shrinkage%20estimators%20that%20combine%20per-prompt%20and%20across-prompt%0Ameans%20to%20improve%20the%20overall%20per-prompt%20mean%20estimation%20accuracy%20--%0Aparticularly%20in%20the%20low-generation%20regime%20typical%20of%20RLVR.%20Theoretically%2C%20we%0Aconstruct%20a%20shrinkage-based%20baseline%20that%20provably%20yields%20lower-variance%0Apolicy-gradient%20estimators%20across%20algorithms.%20Our%20proposed%20baseline%20serves%20as%20a%0Adrop-in%20replacement%20for%20existing%20per-prompt%20mean%20baselines%2C%20requiring%20no%0Aadditional%20hyper-parameters%20or%20computation.%20Empirically%2C%20shrinkage%20baselines%0Aconsistently%20outperform%20standard%20empirical-mean%20baselines%2C%20leading%20to%0Alower-variance%20gradient%20updates%20and%20improved%20training%20stability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03710v1&entry.124074799=Read"},
{"title": "Decision-aware training of spatiotemporal forecasting models to select a\n  top K subset of sites for intervention", "author": "Kyle Heuton and F. Samuel Muench and Shikhar Shrestha and Thomas J. Stopka and Michael C. Hughes", "abstract": "  Optimal allocation of scarce resources is a common problem for decision\nmakers faced with choosing a limited number of locations for intervention.\nSpatiotemporal prediction models could make such decisions data-driven. A\nrecent performance metric called fraction of best possible reach (BPR) measures\nthe impact of using a model's recommended size K subset of sites compared to\nthe best possible top-K in hindsight. We tackle two open problems related to\nBPR. First, we explore how to rank all sites numerically given a probabilistic\nmodel that predicts event counts jointly across sites. Ranking via the per-site\nmean is suboptimal for BPR. Instead, we offer a better ranking for BPR backed\nby decision theory. Second, we explore how to train a probabilistic model's\nparameters to maximize BPR. Discrete selection of K sites implies all-zero\nparameter gradients which prevent standard gradient training. We overcome this\nbarrier via advances in perturbed optimizers. We further suggest a training\nobjective that combines likelihood with a decision-aware BPR constraint to\ndeliver high-quality top-K rankings as well as good forecasts for all sites. We\ndemonstrate our approach on two where-to-intervene applications: mitigating\nopioid-related fatal overdoses for public health and monitoring endangered\nwildlife.\n", "link": "http://arxiv.org/abs/2503.05622v3", "date": "2025-11-05", "relevancy": 1.745, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4695}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4307}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decision-aware%20training%20of%20spatiotemporal%20forecasting%20models%20to%20select%20a%0A%20%20top%20K%20subset%20of%20sites%20for%20intervention&body=Title%3A%20Decision-aware%20training%20of%20spatiotemporal%20forecasting%20models%20to%20select%20a%0A%20%20top%20K%20subset%20of%20sites%20for%20intervention%0AAuthor%3A%20Kyle%20Heuton%20and%20F.%20Samuel%20Muench%20and%20Shikhar%20Shrestha%20and%20Thomas%20J.%20Stopka%20and%20Michael%20C.%20Hughes%0AAbstract%3A%20%20%20Optimal%20allocation%20of%20scarce%20resources%20is%20a%20common%20problem%20for%20decision%0Amakers%20faced%20with%20choosing%20a%20limited%20number%20of%20locations%20for%20intervention.%0ASpatiotemporal%20prediction%20models%20could%20make%20such%20decisions%20data-driven.%20A%0Arecent%20performance%20metric%20called%20fraction%20of%20best%20possible%20reach%20%28BPR%29%20measures%0Athe%20impact%20of%20using%20a%20model%27s%20recommended%20size%20K%20subset%20of%20sites%20compared%20to%0Athe%20best%20possible%20top-K%20in%20hindsight.%20We%20tackle%20two%20open%20problems%20related%20to%0ABPR.%20First%2C%20we%20explore%20how%20to%20rank%20all%20sites%20numerically%20given%20a%20probabilistic%0Amodel%20that%20predicts%20event%20counts%20jointly%20across%20sites.%20Ranking%20via%20the%20per-site%0Amean%20is%20suboptimal%20for%20BPR.%20Instead%2C%20we%20offer%20a%20better%20ranking%20for%20BPR%20backed%0Aby%20decision%20theory.%20Second%2C%20we%20explore%20how%20to%20train%20a%20probabilistic%20model%27s%0Aparameters%20to%20maximize%20BPR.%20Discrete%20selection%20of%20K%20sites%20implies%20all-zero%0Aparameter%20gradients%20which%20prevent%20standard%20gradient%20training.%20We%20overcome%20this%0Abarrier%20via%20advances%20in%20perturbed%20optimizers.%20We%20further%20suggest%20a%20training%0Aobjective%20that%20combines%20likelihood%20with%20a%20decision-aware%20BPR%20constraint%20to%0Adeliver%20high-quality%20top-K%20rankings%20as%20well%20as%20good%20forecasts%20for%20all%20sites.%20We%0Ademonstrate%20our%20approach%20on%20two%20where-to-intervene%20applications%3A%20mitigating%0Aopioid-related%20fatal%20overdoses%20for%20public%20health%20and%20monitoring%20endangered%0Awildlife.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.05622v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecision-aware%2520training%2520of%2520spatiotemporal%2520forecasting%2520models%2520to%2520select%2520a%250A%2520%2520top%2520K%2520subset%2520of%2520sites%2520for%2520intervention%26entry.906535625%3DKyle%2520Heuton%2520and%2520F.%2520Samuel%2520Muench%2520and%2520Shikhar%2520Shrestha%2520and%2520Thomas%2520J.%2520Stopka%2520and%2520Michael%2520C.%2520Hughes%26entry.1292438233%3D%2520%2520Optimal%2520allocation%2520of%2520scarce%2520resources%2520is%2520a%2520common%2520problem%2520for%2520decision%250Amakers%2520faced%2520with%2520choosing%2520a%2520limited%2520number%2520of%2520locations%2520for%2520intervention.%250ASpatiotemporal%2520prediction%2520models%2520could%2520make%2520such%2520decisions%2520data-driven.%2520A%250Arecent%2520performance%2520metric%2520called%2520fraction%2520of%2520best%2520possible%2520reach%2520%2528BPR%2529%2520measures%250Athe%2520impact%2520of%2520using%2520a%2520model%2527s%2520recommended%2520size%2520K%2520subset%2520of%2520sites%2520compared%2520to%250Athe%2520best%2520possible%2520top-K%2520in%2520hindsight.%2520We%2520tackle%2520two%2520open%2520problems%2520related%2520to%250ABPR.%2520First%252C%2520we%2520explore%2520how%2520to%2520rank%2520all%2520sites%2520numerically%2520given%2520a%2520probabilistic%250Amodel%2520that%2520predicts%2520event%2520counts%2520jointly%2520across%2520sites.%2520Ranking%2520via%2520the%2520per-site%250Amean%2520is%2520suboptimal%2520for%2520BPR.%2520Instead%252C%2520we%2520offer%2520a%2520better%2520ranking%2520for%2520BPR%2520backed%250Aby%2520decision%2520theory.%2520Second%252C%2520we%2520explore%2520how%2520to%2520train%2520a%2520probabilistic%2520model%2527s%250Aparameters%2520to%2520maximize%2520BPR.%2520Discrete%2520selection%2520of%2520K%2520sites%2520implies%2520all-zero%250Aparameter%2520gradients%2520which%2520prevent%2520standard%2520gradient%2520training.%2520We%2520overcome%2520this%250Abarrier%2520via%2520advances%2520in%2520perturbed%2520optimizers.%2520We%2520further%2520suggest%2520a%2520training%250Aobjective%2520that%2520combines%2520likelihood%2520with%2520a%2520decision-aware%2520BPR%2520constraint%2520to%250Adeliver%2520high-quality%2520top-K%2520rankings%2520as%2520well%2520as%2520good%2520forecasts%2520for%2520all%2520sites.%2520We%250Ademonstrate%2520our%2520approach%2520on%2520two%2520where-to-intervene%2520applications%253A%2520mitigating%250Aopioid-related%2520fatal%2520overdoses%2520for%2520public%2520health%2520and%2520monitoring%2520endangered%250Awildlife.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.05622v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decision-aware%20training%20of%20spatiotemporal%20forecasting%20models%20to%20select%20a%0A%20%20top%20K%20subset%20of%20sites%20for%20intervention&entry.906535625=Kyle%20Heuton%20and%20F.%20Samuel%20Muench%20and%20Shikhar%20Shrestha%20and%20Thomas%20J.%20Stopka%20and%20Michael%20C.%20Hughes&entry.1292438233=%20%20Optimal%20allocation%20of%20scarce%20resources%20is%20a%20common%20problem%20for%20decision%0Amakers%20faced%20with%20choosing%20a%20limited%20number%20of%20locations%20for%20intervention.%0ASpatiotemporal%20prediction%20models%20could%20make%20such%20decisions%20data-driven.%20A%0Arecent%20performance%20metric%20called%20fraction%20of%20best%20possible%20reach%20%28BPR%29%20measures%0Athe%20impact%20of%20using%20a%20model%27s%20recommended%20size%20K%20subset%20of%20sites%20compared%20to%0Athe%20best%20possible%20top-K%20in%20hindsight.%20We%20tackle%20two%20open%20problems%20related%20to%0ABPR.%20First%2C%20we%20explore%20how%20to%20rank%20all%20sites%20numerically%20given%20a%20probabilistic%0Amodel%20that%20predicts%20event%20counts%20jointly%20across%20sites.%20Ranking%20via%20the%20per-site%0Amean%20is%20suboptimal%20for%20BPR.%20Instead%2C%20we%20offer%20a%20better%20ranking%20for%20BPR%20backed%0Aby%20decision%20theory.%20Second%2C%20we%20explore%20how%20to%20train%20a%20probabilistic%20model%27s%0Aparameters%20to%20maximize%20BPR.%20Discrete%20selection%20of%20K%20sites%20implies%20all-zero%0Aparameter%20gradients%20which%20prevent%20standard%20gradient%20training.%20We%20overcome%20this%0Abarrier%20via%20advances%20in%20perturbed%20optimizers.%20We%20further%20suggest%20a%20training%0Aobjective%20that%20combines%20likelihood%20with%20a%20decision-aware%20BPR%20constraint%20to%0Adeliver%20high-quality%20top-K%20rankings%20as%20well%20as%20good%20forecasts%20for%20all%20sites.%20We%0Ademonstrate%20our%20approach%20on%20two%20where-to-intervene%20applications%3A%20mitigating%0Aopioid-related%20fatal%20overdoses%20for%20public%20health%20and%20monitoring%20endangered%0Awildlife.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.05622v3&entry.124074799=Read"},
{"title": "SHIELD: Securing Healthcare IoT with Efficient Machine Learning\n  Techniques for Anomaly Detection", "author": "Mahek Desai and Apoorva Rumale and Marjan Asadinia", "abstract": "  The integration of IoT devices in healthcare introduces significant security\nand reliability challenges, increasing susceptibility to cyber threats and\noperational anomalies. This study proposes a machine learning-driven framework\nfor (1) detecting malicious cyberattacks and (2) identifying faulty device\nanomalies, leveraging a dataset of 200,000 records. Eight machine learning\nmodels are evaluated across three learning approaches: supervised learning\n(XGBoost, K-Nearest Neighbors (K- NN)), semi-supervised learning (Generative\nAdversarial Networks (GAN), Variational Autoencoders (VAE)), and unsupervised\nlearning (One-Class Support Vector Machine (SVM), Isolation Forest, Graph\nNeural Networks (GNN), and Long Short-Term Memory (LSTM) Autoencoders). The\ncomprehensive evaluation was conducted across multiple metrics like F1-score,\nprecision, recall, accuracy, ROC-AUC, computational efficiency. XGBoost\nachieved 99\\% accuracy with minimal computational overhead (0.04s) for anomaly\ndetection, while Isolation Forest balanced precision and recall effectively.\nLSTM Autoencoders underperformed with lower accuracy and higher latency. For\nattack detection, KNN achieved near-perfect precision, recall, and F1-score\nwith the lowest computational cost (0.05s), followed by VAE at 97% accuracy.\nGAN showed the highest computational cost with lowest accuracy and ROC-AUC.\nThese findings enhance IoT-enabled healthcare security through effective\nanomaly detection strategies. By improving early detection of cyber threats and\ndevice failures, this framework has the potential to prevent data breaches,\nminimize system downtime, and ensure the continuous and safe operation of\nmedical devices, ultimately safeguarding patient health and trust in IoT-driven\nhealthcare solutions.\n", "link": "http://arxiv.org/abs/2511.03661v1", "date": "2025-11-05", "relevancy": 1.7138, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4603}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4282}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SHIELD%3A%20Securing%20Healthcare%20IoT%20with%20Efficient%20Machine%20Learning%0A%20%20Techniques%20for%20Anomaly%20Detection&body=Title%3A%20SHIELD%3A%20Securing%20Healthcare%20IoT%20with%20Efficient%20Machine%20Learning%0A%20%20Techniques%20for%20Anomaly%20Detection%0AAuthor%3A%20Mahek%20Desai%20and%20Apoorva%20Rumale%20and%20Marjan%20Asadinia%0AAbstract%3A%20%20%20The%20integration%20of%20IoT%20devices%20in%20healthcare%20introduces%20significant%20security%0Aand%20reliability%20challenges%2C%20increasing%20susceptibility%20to%20cyber%20threats%20and%0Aoperational%20anomalies.%20This%20study%20proposes%20a%20machine%20learning-driven%20framework%0Afor%20%281%29%20detecting%20malicious%20cyberattacks%20and%20%282%29%20identifying%20faulty%20device%0Aanomalies%2C%20leveraging%20a%20dataset%20of%20200%2C000%20records.%20Eight%20machine%20learning%0Amodels%20are%20evaluated%20across%20three%20learning%20approaches%3A%20supervised%20learning%0A%28XGBoost%2C%20K-Nearest%20Neighbors%20%28K-%20NN%29%29%2C%20semi-supervised%20learning%20%28Generative%0AAdversarial%20Networks%20%28GAN%29%2C%20Variational%20Autoencoders%20%28VAE%29%29%2C%20and%20unsupervised%0Alearning%20%28One-Class%20Support%20Vector%20Machine%20%28SVM%29%2C%20Isolation%20Forest%2C%20Graph%0ANeural%20Networks%20%28GNN%29%2C%20and%20Long%20Short-Term%20Memory%20%28LSTM%29%20Autoencoders%29.%20The%0Acomprehensive%20evaluation%20was%20conducted%20across%20multiple%20metrics%20like%20F1-score%2C%0Aprecision%2C%20recall%2C%20accuracy%2C%20ROC-AUC%2C%20computational%20efficiency.%20XGBoost%0Aachieved%2099%5C%25%20accuracy%20with%20minimal%20computational%20overhead%20%280.04s%29%20for%20anomaly%0Adetection%2C%20while%20Isolation%20Forest%20balanced%20precision%20and%20recall%20effectively.%0ALSTM%20Autoencoders%20underperformed%20with%20lower%20accuracy%20and%20higher%20latency.%20For%0Aattack%20detection%2C%20KNN%20achieved%20near-perfect%20precision%2C%20recall%2C%20and%20F1-score%0Awith%20the%20lowest%20computational%20cost%20%280.05s%29%2C%20followed%20by%20VAE%20at%2097%25%20accuracy.%0AGAN%20showed%20the%20highest%20computational%20cost%20with%20lowest%20accuracy%20and%20ROC-AUC.%0AThese%20findings%20enhance%20IoT-enabled%20healthcare%20security%20through%20effective%0Aanomaly%20detection%20strategies.%20By%20improving%20early%20detection%20of%20cyber%20threats%20and%0Adevice%20failures%2C%20this%20framework%20has%20the%20potential%20to%20prevent%20data%20breaches%2C%0Aminimize%20system%20downtime%2C%20and%20ensure%20the%20continuous%20and%20safe%20operation%20of%0Amedical%20devices%2C%20ultimately%20safeguarding%20patient%20health%20and%20trust%20in%20IoT-driven%0Ahealthcare%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSHIELD%253A%2520Securing%2520Healthcare%2520IoT%2520with%2520Efficient%2520Machine%2520Learning%250A%2520%2520Techniques%2520for%2520Anomaly%2520Detection%26entry.906535625%3DMahek%2520Desai%2520and%2520Apoorva%2520Rumale%2520and%2520Marjan%2520Asadinia%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520IoT%2520devices%2520in%2520healthcare%2520introduces%2520significant%2520security%250Aand%2520reliability%2520challenges%252C%2520increasing%2520susceptibility%2520to%2520cyber%2520threats%2520and%250Aoperational%2520anomalies.%2520This%2520study%2520proposes%2520a%2520machine%2520learning-driven%2520framework%250Afor%2520%25281%2529%2520detecting%2520malicious%2520cyberattacks%2520and%2520%25282%2529%2520identifying%2520faulty%2520device%250Aanomalies%252C%2520leveraging%2520a%2520dataset%2520of%2520200%252C000%2520records.%2520Eight%2520machine%2520learning%250Amodels%2520are%2520evaluated%2520across%2520three%2520learning%2520approaches%253A%2520supervised%2520learning%250A%2528XGBoost%252C%2520K-Nearest%2520Neighbors%2520%2528K-%2520NN%2529%2529%252C%2520semi-supervised%2520learning%2520%2528Generative%250AAdversarial%2520Networks%2520%2528GAN%2529%252C%2520Variational%2520Autoencoders%2520%2528VAE%2529%2529%252C%2520and%2520unsupervised%250Alearning%2520%2528One-Class%2520Support%2520Vector%2520Machine%2520%2528SVM%2529%252C%2520Isolation%2520Forest%252C%2520Graph%250ANeural%2520Networks%2520%2528GNN%2529%252C%2520and%2520Long%2520Short-Term%2520Memory%2520%2528LSTM%2529%2520Autoencoders%2529.%2520The%250Acomprehensive%2520evaluation%2520was%2520conducted%2520across%2520multiple%2520metrics%2520like%2520F1-score%252C%250Aprecision%252C%2520recall%252C%2520accuracy%252C%2520ROC-AUC%252C%2520computational%2520efficiency.%2520XGBoost%250Aachieved%252099%255C%2525%2520accuracy%2520with%2520minimal%2520computational%2520overhead%2520%25280.04s%2529%2520for%2520anomaly%250Adetection%252C%2520while%2520Isolation%2520Forest%2520balanced%2520precision%2520and%2520recall%2520effectively.%250ALSTM%2520Autoencoders%2520underperformed%2520with%2520lower%2520accuracy%2520and%2520higher%2520latency.%2520For%250Aattack%2520detection%252C%2520KNN%2520achieved%2520near-perfect%2520precision%252C%2520recall%252C%2520and%2520F1-score%250Awith%2520the%2520lowest%2520computational%2520cost%2520%25280.05s%2529%252C%2520followed%2520by%2520VAE%2520at%252097%2525%2520accuracy.%250AGAN%2520showed%2520the%2520highest%2520computational%2520cost%2520with%2520lowest%2520accuracy%2520and%2520ROC-AUC.%250AThese%2520findings%2520enhance%2520IoT-enabled%2520healthcare%2520security%2520through%2520effective%250Aanomaly%2520detection%2520strategies.%2520By%2520improving%2520early%2520detection%2520of%2520cyber%2520threats%2520and%250Adevice%2520failures%252C%2520this%2520framework%2520has%2520the%2520potential%2520to%2520prevent%2520data%2520breaches%252C%250Aminimize%2520system%2520downtime%252C%2520and%2520ensure%2520the%2520continuous%2520and%2520safe%2520operation%2520of%250Amedical%2520devices%252C%2520ultimately%2520safeguarding%2520patient%2520health%2520and%2520trust%2520in%2520IoT-driven%250Ahealthcare%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SHIELD%3A%20Securing%20Healthcare%20IoT%20with%20Efficient%20Machine%20Learning%0A%20%20Techniques%20for%20Anomaly%20Detection&entry.906535625=Mahek%20Desai%20and%20Apoorva%20Rumale%20and%20Marjan%20Asadinia&entry.1292438233=%20%20The%20integration%20of%20IoT%20devices%20in%20healthcare%20introduces%20significant%20security%0Aand%20reliability%20challenges%2C%20increasing%20susceptibility%20to%20cyber%20threats%20and%0Aoperational%20anomalies.%20This%20study%20proposes%20a%20machine%20learning-driven%20framework%0Afor%20%281%29%20detecting%20malicious%20cyberattacks%20and%20%282%29%20identifying%20faulty%20device%0Aanomalies%2C%20leveraging%20a%20dataset%20of%20200%2C000%20records.%20Eight%20machine%20learning%0Amodels%20are%20evaluated%20across%20three%20learning%20approaches%3A%20supervised%20learning%0A%28XGBoost%2C%20K-Nearest%20Neighbors%20%28K-%20NN%29%29%2C%20semi-supervised%20learning%20%28Generative%0AAdversarial%20Networks%20%28GAN%29%2C%20Variational%20Autoencoders%20%28VAE%29%29%2C%20and%20unsupervised%0Alearning%20%28One-Class%20Support%20Vector%20Machine%20%28SVM%29%2C%20Isolation%20Forest%2C%20Graph%0ANeural%20Networks%20%28GNN%29%2C%20and%20Long%20Short-Term%20Memory%20%28LSTM%29%20Autoencoders%29.%20The%0Acomprehensive%20evaluation%20was%20conducted%20across%20multiple%20metrics%20like%20F1-score%2C%0Aprecision%2C%20recall%2C%20accuracy%2C%20ROC-AUC%2C%20computational%20efficiency.%20XGBoost%0Aachieved%2099%5C%25%20accuracy%20with%20minimal%20computational%20overhead%20%280.04s%29%20for%20anomaly%0Adetection%2C%20while%20Isolation%20Forest%20balanced%20precision%20and%20recall%20effectively.%0ALSTM%20Autoencoders%20underperformed%20with%20lower%20accuracy%20and%20higher%20latency.%20For%0Aattack%20detection%2C%20KNN%20achieved%20near-perfect%20precision%2C%20recall%2C%20and%20F1-score%0Awith%20the%20lowest%20computational%20cost%20%280.05s%29%2C%20followed%20by%20VAE%20at%2097%25%20accuracy.%0AGAN%20showed%20the%20highest%20computational%20cost%20with%20lowest%20accuracy%20and%20ROC-AUC.%0AThese%20findings%20enhance%20IoT-enabled%20healthcare%20security%20through%20effective%0Aanomaly%20detection%20strategies.%20By%20improving%20early%20detection%20of%20cyber%20threats%20and%0Adevice%20failures%2C%20this%20framework%20has%20the%20potential%20to%20prevent%20data%20breaches%2C%0Aminimize%20system%20downtime%2C%20and%20ensure%20the%20continuous%20and%20safe%20operation%20of%0Amedical%20devices%2C%20ultimately%20safeguarding%20patient%20health%20and%20trust%20in%20IoT-driven%0Ahealthcare%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03661v1&entry.124074799=Read"},
{"title": "RoboRAN: A Unified Robotics Framework for Reinforcement Learning-Based\n  Autonomous Navigation", "author": "Matteo El-Hariry and Antoine Richard and Ricard M. Castan and Luis F. W. Batista and Matthieu Geist and Cedric Pradalier and Miguel Olivares-Mendez", "abstract": "  Autonomous robots must navigate and operate in diverse environments, from\nterrestrial and aquatic settings to aerial and space domains. While\nReinforcement Learning (RL) has shown promise in training policies for specific\nautonomous robots, existing frameworks and benchmarks are often constrained to\nunique platforms, limiting generalization and fair comparisons across different\nmobility systems. In this paper, we present a multi-domain framework for\ntraining, evaluating and deploying RL-based navigation policies across diverse\nrobotic platforms and operational environments. Our work presents four key\ncontributions: (1) a scalable and modular framework, facilitating seamless\nrobot-task interchangeability and reproducible training pipelines; (2)\nsim-to-real transfer demonstrated through real-world experiments with multiple\nrobots, including a satellite robotic simulator, an unmanned surface vessel,\nand a wheeled ground vehicle; (3) the release of the first open-source API for\ndeploying Isaac Lab-trained policies to real robots, enabling lightweight\ninference and rapid field validation; and (4) uniform tasks and metrics for\ncross-medium evaluation, through a unified evaluation testbed to assess\nperformance of navigation tasks in diverse operational conditions (aquatic,\nterrestrial and space). By ensuring consistency between simulation and\nreal-world deployment, RoboRAN lowers the barrier to developing adaptable\nRL-based navigation strategies. Its modular design enables straightforward\nintegration of new robots and tasks through predefined templates, fostering\nreproducibility and extension to diverse domains. To support the community, we\nrelease RoboRAN as open-source.\n", "link": "http://arxiv.org/abs/2505.14526v2", "date": "2025-11-05", "relevancy": 1.7104, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5878}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5771}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoboRAN%3A%20A%20Unified%20Robotics%20Framework%20for%20Reinforcement%20Learning-Based%0A%20%20Autonomous%20Navigation&body=Title%3A%20RoboRAN%3A%20A%20Unified%20Robotics%20Framework%20for%20Reinforcement%20Learning-Based%0A%20%20Autonomous%20Navigation%0AAuthor%3A%20Matteo%20El-Hariry%20and%20Antoine%20Richard%20and%20Ricard%20M.%20Castan%20and%20Luis%20F.%20W.%20Batista%20and%20Matthieu%20Geist%20and%20Cedric%20Pradalier%20and%20Miguel%20Olivares-Mendez%0AAbstract%3A%20%20%20Autonomous%20robots%20must%20navigate%20and%20operate%20in%20diverse%20environments%2C%20from%0Aterrestrial%20and%20aquatic%20settings%20to%20aerial%20and%20space%20domains.%20While%0AReinforcement%20Learning%20%28RL%29%20has%20shown%20promise%20in%20training%20policies%20for%20specific%0Aautonomous%20robots%2C%20existing%20frameworks%20and%20benchmarks%20are%20often%20constrained%20to%0Aunique%20platforms%2C%20limiting%20generalization%20and%20fair%20comparisons%20across%20different%0Amobility%20systems.%20In%20this%20paper%2C%20we%20present%20a%20multi-domain%20framework%20for%0Atraining%2C%20evaluating%20and%20deploying%20RL-based%20navigation%20policies%20across%20diverse%0Arobotic%20platforms%20and%20operational%20environments.%20Our%20work%20presents%20four%20key%0Acontributions%3A%20%281%29%20a%20scalable%20and%20modular%20framework%2C%20facilitating%20seamless%0Arobot-task%20interchangeability%20and%20reproducible%20training%20pipelines%3B%20%282%29%0Asim-to-real%20transfer%20demonstrated%20through%20real-world%20experiments%20with%20multiple%0Arobots%2C%20including%20a%20satellite%20robotic%20simulator%2C%20an%20unmanned%20surface%20vessel%2C%0Aand%20a%20wheeled%20ground%20vehicle%3B%20%283%29%20the%20release%20of%20the%20first%20open-source%20API%20for%0Adeploying%20Isaac%20Lab-trained%20policies%20to%20real%20robots%2C%20enabling%20lightweight%0Ainference%20and%20rapid%20field%20validation%3B%20and%20%284%29%20uniform%20tasks%20and%20metrics%20for%0Across-medium%20evaluation%2C%20through%20a%20unified%20evaluation%20testbed%20to%20assess%0Aperformance%20of%20navigation%20tasks%20in%20diverse%20operational%20conditions%20%28aquatic%2C%0Aterrestrial%20and%20space%29.%20By%20ensuring%20consistency%20between%20simulation%20and%0Areal-world%20deployment%2C%20RoboRAN%20lowers%20the%20barrier%20to%20developing%20adaptable%0ARL-based%20navigation%20strategies.%20Its%20modular%20design%20enables%20straightforward%0Aintegration%20of%20new%20robots%20and%20tasks%20through%20predefined%20templates%2C%20fostering%0Areproducibility%20and%20extension%20to%20diverse%20domains.%20To%20support%20the%20community%2C%20we%0Arelease%20RoboRAN%20as%20open-source.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14526v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoboRAN%253A%2520A%2520Unified%2520Robotics%2520Framework%2520for%2520Reinforcement%2520Learning-Based%250A%2520%2520Autonomous%2520Navigation%26entry.906535625%3DMatteo%2520El-Hariry%2520and%2520Antoine%2520Richard%2520and%2520Ricard%2520M.%2520Castan%2520and%2520Luis%2520F.%2520W.%2520Batista%2520and%2520Matthieu%2520Geist%2520and%2520Cedric%2520Pradalier%2520and%2520Miguel%2520Olivares-Mendez%26entry.1292438233%3D%2520%2520Autonomous%2520robots%2520must%2520navigate%2520and%2520operate%2520in%2520diverse%2520environments%252C%2520from%250Aterrestrial%2520and%2520aquatic%2520settings%2520to%2520aerial%2520and%2520space%2520domains.%2520While%250AReinforcement%2520Learning%2520%2528RL%2529%2520has%2520shown%2520promise%2520in%2520training%2520policies%2520for%2520specific%250Aautonomous%2520robots%252C%2520existing%2520frameworks%2520and%2520benchmarks%2520are%2520often%2520constrained%2520to%250Aunique%2520platforms%252C%2520limiting%2520generalization%2520and%2520fair%2520comparisons%2520across%2520different%250Amobility%2520systems.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520multi-domain%2520framework%2520for%250Atraining%252C%2520evaluating%2520and%2520deploying%2520RL-based%2520navigation%2520policies%2520across%2520diverse%250Arobotic%2520platforms%2520and%2520operational%2520environments.%2520Our%2520work%2520presents%2520four%2520key%250Acontributions%253A%2520%25281%2529%2520a%2520scalable%2520and%2520modular%2520framework%252C%2520facilitating%2520seamless%250Arobot-task%2520interchangeability%2520and%2520reproducible%2520training%2520pipelines%253B%2520%25282%2529%250Asim-to-real%2520transfer%2520demonstrated%2520through%2520real-world%2520experiments%2520with%2520multiple%250Arobots%252C%2520including%2520a%2520satellite%2520robotic%2520simulator%252C%2520an%2520unmanned%2520surface%2520vessel%252C%250Aand%2520a%2520wheeled%2520ground%2520vehicle%253B%2520%25283%2529%2520the%2520release%2520of%2520the%2520first%2520open-source%2520API%2520for%250Adeploying%2520Isaac%2520Lab-trained%2520policies%2520to%2520real%2520robots%252C%2520enabling%2520lightweight%250Ainference%2520and%2520rapid%2520field%2520validation%253B%2520and%2520%25284%2529%2520uniform%2520tasks%2520and%2520metrics%2520for%250Across-medium%2520evaluation%252C%2520through%2520a%2520unified%2520evaluation%2520testbed%2520to%2520assess%250Aperformance%2520of%2520navigation%2520tasks%2520in%2520diverse%2520operational%2520conditions%2520%2528aquatic%252C%250Aterrestrial%2520and%2520space%2529.%2520By%2520ensuring%2520consistency%2520between%2520simulation%2520and%250Areal-world%2520deployment%252C%2520RoboRAN%2520lowers%2520the%2520barrier%2520to%2520developing%2520adaptable%250ARL-based%2520navigation%2520strategies.%2520Its%2520modular%2520design%2520enables%2520straightforward%250Aintegration%2520of%2520new%2520robots%2520and%2520tasks%2520through%2520predefined%2520templates%252C%2520fostering%250Areproducibility%2520and%2520extension%2520to%2520diverse%2520domains.%2520To%2520support%2520the%2520community%252C%2520we%250Arelease%2520RoboRAN%2520as%2520open-source.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14526v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoboRAN%3A%20A%20Unified%20Robotics%20Framework%20for%20Reinforcement%20Learning-Based%0A%20%20Autonomous%20Navigation&entry.906535625=Matteo%20El-Hariry%20and%20Antoine%20Richard%20and%20Ricard%20M.%20Castan%20and%20Luis%20F.%20W.%20Batista%20and%20Matthieu%20Geist%20and%20Cedric%20Pradalier%20and%20Miguel%20Olivares-Mendez&entry.1292438233=%20%20Autonomous%20robots%20must%20navigate%20and%20operate%20in%20diverse%20environments%2C%20from%0Aterrestrial%20and%20aquatic%20settings%20to%20aerial%20and%20space%20domains.%20While%0AReinforcement%20Learning%20%28RL%29%20has%20shown%20promise%20in%20training%20policies%20for%20specific%0Aautonomous%20robots%2C%20existing%20frameworks%20and%20benchmarks%20are%20often%20constrained%20to%0Aunique%20platforms%2C%20limiting%20generalization%20and%20fair%20comparisons%20across%20different%0Amobility%20systems.%20In%20this%20paper%2C%20we%20present%20a%20multi-domain%20framework%20for%0Atraining%2C%20evaluating%20and%20deploying%20RL-based%20navigation%20policies%20across%20diverse%0Arobotic%20platforms%20and%20operational%20environments.%20Our%20work%20presents%20four%20key%0Acontributions%3A%20%281%29%20a%20scalable%20and%20modular%20framework%2C%20facilitating%20seamless%0Arobot-task%20interchangeability%20and%20reproducible%20training%20pipelines%3B%20%282%29%0Asim-to-real%20transfer%20demonstrated%20through%20real-world%20experiments%20with%20multiple%0Arobots%2C%20including%20a%20satellite%20robotic%20simulator%2C%20an%20unmanned%20surface%20vessel%2C%0Aand%20a%20wheeled%20ground%20vehicle%3B%20%283%29%20the%20release%20of%20the%20first%20open-source%20API%20for%0Adeploying%20Isaac%20Lab-trained%20policies%20to%20real%20robots%2C%20enabling%20lightweight%0Ainference%20and%20rapid%20field%20validation%3B%20and%20%284%29%20uniform%20tasks%20and%20metrics%20for%0Across-medium%20evaluation%2C%20through%20a%20unified%20evaluation%20testbed%20to%20assess%0Aperformance%20of%20navigation%20tasks%20in%20diverse%20operational%20conditions%20%28aquatic%2C%0Aterrestrial%20and%20space%29.%20By%20ensuring%20consistency%20between%20simulation%20and%0Areal-world%20deployment%2C%20RoboRAN%20lowers%20the%20barrier%20to%20developing%20adaptable%0ARL-based%20navigation%20strategies.%20Its%20modular%20design%20enables%20straightforward%0Aintegration%20of%20new%20robots%20and%20tasks%20through%20predefined%20templates%2C%20fostering%0Areproducibility%20and%20extension%20to%20diverse%20domains.%20To%20support%20the%20community%2C%20we%0Arelease%20RoboRAN%20as%20open-source.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14526v2&entry.124074799=Read"},
{"title": "Beyond Covariance Matrix: The Statistical Complexity of Private Linear\n  Regression", "author": "Fan Chen and Jiachun Li and Alexander Rakhlin and David Simchi-Levi", "abstract": "  We study the statistical complexity of private linear regression under an\nunknown, potentially ill-conditioned covariate distribution. Somewhat\nsurprisingly, under privacy constraints the intrinsic complexity is \\emph{not}\ncaptured by the usual covariance matrix but rather its $L_1$ analogues.\nBuilding on this insight, we establish minimax convergence rates for both the\ncentral and local privacy models and introduce an Information-Weighted\nRegression method that attains the optimal rates.\n  As application, in private linear contextual bandits, we propose an efficient\nalgorithm that achieves rate-optimal regret bounds of order\n$\\sqrt{T}+\\frac{1}{\\alpha}$ and $\\sqrt{T}/\\alpha$ under joint and local\n$\\alpha$-privacy models, respectively. Notably, our results demonstrate that\njoint privacy comes at almost no additional cost, addressing the open problems\nposed by Azize and Basu (2024).\n", "link": "http://arxiv.org/abs/2502.13115v2", "date": "2025-11-05", "relevancy": 1.7077, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4644}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4249}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Covariance%20Matrix%3A%20The%20Statistical%20Complexity%20of%20Private%20Linear%0A%20%20Regression&body=Title%3A%20Beyond%20Covariance%20Matrix%3A%20The%20Statistical%20Complexity%20of%20Private%20Linear%0A%20%20Regression%0AAuthor%3A%20Fan%20Chen%20and%20Jiachun%20Li%20and%20Alexander%20Rakhlin%20and%20David%20Simchi-Levi%0AAbstract%3A%20%20%20We%20study%20the%20statistical%20complexity%20of%20private%20linear%20regression%20under%20an%0Aunknown%2C%20potentially%20ill-conditioned%20covariate%20distribution.%20Somewhat%0Asurprisingly%2C%20under%20privacy%20constraints%20the%20intrinsic%20complexity%20is%20%5Cemph%7Bnot%7D%0Acaptured%20by%20the%20usual%20covariance%20matrix%20but%20rather%20its%20%24L_1%24%20analogues.%0ABuilding%20on%20this%20insight%2C%20we%20establish%20minimax%20convergence%20rates%20for%20both%20the%0Acentral%20and%20local%20privacy%20models%20and%20introduce%20an%20Information-Weighted%0ARegression%20method%20that%20attains%20the%20optimal%20rates.%0A%20%20As%20application%2C%20in%20private%20linear%20contextual%20bandits%2C%20we%20propose%20an%20efficient%0Aalgorithm%20that%20achieves%20rate-optimal%20regret%20bounds%20of%20order%0A%24%5Csqrt%7BT%7D%2B%5Cfrac%7B1%7D%7B%5Calpha%7D%24%20and%20%24%5Csqrt%7BT%7D/%5Calpha%24%20under%20joint%20and%20local%0A%24%5Calpha%24-privacy%20models%2C%20respectively.%20Notably%2C%20our%20results%20demonstrate%20that%0Ajoint%20privacy%20comes%20at%20almost%20no%20additional%20cost%2C%20addressing%20the%20open%20problems%0Aposed%20by%20Azize%20and%20Basu%20%282024%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13115v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Covariance%2520Matrix%253A%2520The%2520Statistical%2520Complexity%2520of%2520Private%2520Linear%250A%2520%2520Regression%26entry.906535625%3DFan%2520Chen%2520and%2520Jiachun%2520Li%2520and%2520Alexander%2520Rakhlin%2520and%2520David%2520Simchi-Levi%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520statistical%2520complexity%2520of%2520private%2520linear%2520regression%2520under%2520an%250Aunknown%252C%2520potentially%2520ill-conditioned%2520covariate%2520distribution.%2520Somewhat%250Asurprisingly%252C%2520under%2520privacy%2520constraints%2520the%2520intrinsic%2520complexity%2520is%2520%255Cemph%257Bnot%257D%250Acaptured%2520by%2520the%2520usual%2520covariance%2520matrix%2520but%2520rather%2520its%2520%2524L_1%2524%2520analogues.%250ABuilding%2520on%2520this%2520insight%252C%2520we%2520establish%2520minimax%2520convergence%2520rates%2520for%2520both%2520the%250Acentral%2520and%2520local%2520privacy%2520models%2520and%2520introduce%2520an%2520Information-Weighted%250ARegression%2520method%2520that%2520attains%2520the%2520optimal%2520rates.%250A%2520%2520As%2520application%252C%2520in%2520private%2520linear%2520contextual%2520bandits%252C%2520we%2520propose%2520an%2520efficient%250Aalgorithm%2520that%2520achieves%2520rate-optimal%2520regret%2520bounds%2520of%2520order%250A%2524%255Csqrt%257BT%257D%252B%255Cfrac%257B1%257D%257B%255Calpha%257D%2524%2520and%2520%2524%255Csqrt%257BT%257D/%255Calpha%2524%2520under%2520joint%2520and%2520local%250A%2524%255Calpha%2524-privacy%2520models%252C%2520respectively.%2520Notably%252C%2520our%2520results%2520demonstrate%2520that%250Ajoint%2520privacy%2520comes%2520at%2520almost%2520no%2520additional%2520cost%252C%2520addressing%2520the%2520open%2520problems%250Aposed%2520by%2520Azize%2520and%2520Basu%2520%25282024%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13115v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Covariance%20Matrix%3A%20The%20Statistical%20Complexity%20of%20Private%20Linear%0A%20%20Regression&entry.906535625=Fan%20Chen%20and%20Jiachun%20Li%20and%20Alexander%20Rakhlin%20and%20David%20Simchi-Levi&entry.1292438233=%20%20We%20study%20the%20statistical%20complexity%20of%20private%20linear%20regression%20under%20an%0Aunknown%2C%20potentially%20ill-conditioned%20covariate%20distribution.%20Somewhat%0Asurprisingly%2C%20under%20privacy%20constraints%20the%20intrinsic%20complexity%20is%20%5Cemph%7Bnot%7D%0Acaptured%20by%20the%20usual%20covariance%20matrix%20but%20rather%20its%20%24L_1%24%20analogues.%0ABuilding%20on%20this%20insight%2C%20we%20establish%20minimax%20convergence%20rates%20for%20both%20the%0Acentral%20and%20local%20privacy%20models%20and%20introduce%20an%20Information-Weighted%0ARegression%20method%20that%20attains%20the%20optimal%20rates.%0A%20%20As%20application%2C%20in%20private%20linear%20contextual%20bandits%2C%20we%20propose%20an%20efficient%0Aalgorithm%20that%20achieves%20rate-optimal%20regret%20bounds%20of%20order%0A%24%5Csqrt%7BT%7D%2B%5Cfrac%7B1%7D%7B%5Calpha%7D%24%20and%20%24%5Csqrt%7BT%7D/%5Calpha%24%20under%20joint%20and%20local%0A%24%5Calpha%24-privacy%20models%2C%20respectively.%20Notably%2C%20our%20results%20demonstrate%20that%0Ajoint%20privacy%20comes%20at%20almost%20no%20additional%20cost%2C%20addressing%20the%20open%20problems%0Aposed%20by%20Azize%20and%20Basu%20%282024%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13115v2&entry.124074799=Read"},
{"title": "Unconscious and Intentional Human Motion Cues for Expressive Robot-Arm\n  Motion Design", "author": "Taito Tashiro and Tomoko Yonezawa and Hirotake Yamazoe", "abstract": "  This study investigates how human motion cues can be used to design\nexpressive robot-arm movements. Using the imperfect-information game Geister,\nwe analyzed two types of human piece-moving motions: natural gameplay\n(unconscious tendencies) and instructed expressions (intentional cues). Based\non these findings, we created phase-specific robot motions by varying movement\nspeed and stop duration, and evaluated observer impressions under two\npresentation modalities: a physical robot and a recorded video. Results\nindicate that late-phase motion timing, particularly during withdrawal, plays\nan important role in impression formation and that physical embodiment enhances\nthe interpretability of motion cues. These findings provide insights for\ndesigning expressive robot motions based on human timing behavior.\n", "link": "http://arxiv.org/abs/2511.03676v1", "date": "2025-11-05", "relevancy": 1.6898, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5731}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5668}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unconscious%20and%20Intentional%20Human%20Motion%20Cues%20for%20Expressive%20Robot-Arm%0A%20%20Motion%20Design&body=Title%3A%20Unconscious%20and%20Intentional%20Human%20Motion%20Cues%20for%20Expressive%20Robot-Arm%0A%20%20Motion%20Design%0AAuthor%3A%20Taito%20Tashiro%20and%20Tomoko%20Yonezawa%20and%20Hirotake%20Yamazoe%0AAbstract%3A%20%20%20This%20study%20investigates%20how%20human%20motion%20cues%20can%20be%20used%20to%20design%0Aexpressive%20robot-arm%20movements.%20Using%20the%20imperfect-information%20game%20Geister%2C%0Awe%20analyzed%20two%20types%20of%20human%20piece-moving%20motions%3A%20natural%20gameplay%0A%28unconscious%20tendencies%29%20and%20instructed%20expressions%20%28intentional%20cues%29.%20Based%0Aon%20these%20findings%2C%20we%20created%20phase-specific%20robot%20motions%20by%20varying%20movement%0Aspeed%20and%20stop%20duration%2C%20and%20evaluated%20observer%20impressions%20under%20two%0Apresentation%20modalities%3A%20a%20physical%20robot%20and%20a%20recorded%20video.%20Results%0Aindicate%20that%20late-phase%20motion%20timing%2C%20particularly%20during%20withdrawal%2C%20plays%0Aan%20important%20role%20in%20impression%20formation%20and%20that%20physical%20embodiment%20enhances%0Athe%20interpretability%20of%20motion%20cues.%20These%20findings%20provide%20insights%20for%0Adesigning%20expressive%20robot%20motions%20based%20on%20human%20timing%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03676v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnconscious%2520and%2520Intentional%2520Human%2520Motion%2520Cues%2520for%2520Expressive%2520Robot-Arm%250A%2520%2520Motion%2520Design%26entry.906535625%3DTaito%2520Tashiro%2520and%2520Tomoko%2520Yonezawa%2520and%2520Hirotake%2520Yamazoe%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520how%2520human%2520motion%2520cues%2520can%2520be%2520used%2520to%2520design%250Aexpressive%2520robot-arm%2520movements.%2520Using%2520the%2520imperfect-information%2520game%2520Geister%252C%250Awe%2520analyzed%2520two%2520types%2520of%2520human%2520piece-moving%2520motions%253A%2520natural%2520gameplay%250A%2528unconscious%2520tendencies%2529%2520and%2520instructed%2520expressions%2520%2528intentional%2520cues%2529.%2520Based%250Aon%2520these%2520findings%252C%2520we%2520created%2520phase-specific%2520robot%2520motions%2520by%2520varying%2520movement%250Aspeed%2520and%2520stop%2520duration%252C%2520and%2520evaluated%2520observer%2520impressions%2520under%2520two%250Apresentation%2520modalities%253A%2520a%2520physical%2520robot%2520and%2520a%2520recorded%2520video.%2520Results%250Aindicate%2520that%2520late-phase%2520motion%2520timing%252C%2520particularly%2520during%2520withdrawal%252C%2520plays%250Aan%2520important%2520role%2520in%2520impression%2520formation%2520and%2520that%2520physical%2520embodiment%2520enhances%250Athe%2520interpretability%2520of%2520motion%2520cues.%2520These%2520findings%2520provide%2520insights%2520for%250Adesigning%2520expressive%2520robot%2520motions%2520based%2520on%2520human%2520timing%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03676v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unconscious%20and%20Intentional%20Human%20Motion%20Cues%20for%20Expressive%20Robot-Arm%0A%20%20Motion%20Design&entry.906535625=Taito%20Tashiro%20and%20Tomoko%20Yonezawa%20and%20Hirotake%20Yamazoe&entry.1292438233=%20%20This%20study%20investigates%20how%20human%20motion%20cues%20can%20be%20used%20to%20design%0Aexpressive%20robot-arm%20movements.%20Using%20the%20imperfect-information%20game%20Geister%2C%0Awe%20analyzed%20two%20types%20of%20human%20piece-moving%20motions%3A%20natural%20gameplay%0A%28unconscious%20tendencies%29%20and%20instructed%20expressions%20%28intentional%20cues%29.%20Based%0Aon%20these%20findings%2C%20we%20created%20phase-specific%20robot%20motions%20by%20varying%20movement%0Aspeed%20and%20stop%20duration%2C%20and%20evaluated%20observer%20impressions%20under%20two%0Apresentation%20modalities%3A%20a%20physical%20robot%20and%20a%20recorded%20video.%20Results%0Aindicate%20that%20late-phase%20motion%20timing%2C%20particularly%20during%20withdrawal%2C%20plays%0Aan%20important%20role%20in%20impression%20formation%20and%20that%20physical%20embodiment%20enhances%0Athe%20interpretability%20of%20motion%20cues.%20These%20findings%20provide%20insights%20for%0Adesigning%20expressive%20robot%20motions%20based%20on%20human%20timing%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03676v1&entry.124074799=Read"},
{"title": "Whisper Leak: a side-channel attack on Large Language Models", "author": "Geoff McDonald and Jonathan Bar Or", "abstract": "  Large Language Models (LLMs) are increasingly deployed in sensitive domains\nincluding healthcare, legal services, and confidential communications, where\nprivacy is paramount. This paper introduces Whisper Leak, a side-channel attack\nthat infers user prompt topics from encrypted LLM traffic by analyzing packet\nsize and timing patterns in streaming responses. Despite TLS encryption\nprotecting content, these metadata patterns leak sufficient information to\nenable topic classification. We demonstrate the attack across 28 popular LLMs\nfrom major providers, achieving near-perfect classification (often >98% AUPRC)\nand high precision even at extreme class imbalance (10,000:1 noise-to-target\nratio). For many models, we achieve 100% precision in identifying sensitive\ntopics like \"money laundering\" while recovering 5-20% of target conversations.\nThis industry-wide vulnerability poses significant risks for users under\nnetwork surveillance by ISPs, governments, or local adversaries. We evaluate\nthree mitigation strategies - random padding, token batching, and packet\ninjection - finding that while each reduces attack effectiveness, none provides\ncomplete protection. Through responsible disclosure, we have collaborated with\nproviders to implement initial countermeasures. Our findings underscore the\nneed for LLM providers to address metadata leakage as AI systems handle\nincreasingly sensitive information.\n", "link": "http://arxiv.org/abs/2511.03675v1", "date": "2025-11-05", "relevancy": 1.6897, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4386}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4292}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Whisper%20Leak%3A%20a%20side-channel%20attack%20on%20Large%20Language%20Models&body=Title%3A%20Whisper%20Leak%3A%20a%20side-channel%20attack%20on%20Large%20Language%20Models%0AAuthor%3A%20Geoff%20McDonald%20and%20Jonathan%20Bar%20Or%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20deployed%20in%20sensitive%20domains%0Aincluding%20healthcare%2C%20legal%20services%2C%20and%20confidential%20communications%2C%20where%0Aprivacy%20is%20paramount.%20This%20paper%20introduces%20Whisper%20Leak%2C%20a%20side-channel%20attack%0Athat%20infers%20user%20prompt%20topics%20from%20encrypted%20LLM%20traffic%20by%20analyzing%20packet%0Asize%20and%20timing%20patterns%20in%20streaming%20responses.%20Despite%20TLS%20encryption%0Aprotecting%20content%2C%20these%20metadata%20patterns%20leak%20sufficient%20information%20to%0Aenable%20topic%20classification.%20We%20demonstrate%20the%20attack%20across%2028%20popular%20LLMs%0Afrom%20major%20providers%2C%20achieving%20near-perfect%20classification%20%28often%20%3E98%25%20AUPRC%29%0Aand%20high%20precision%20even%20at%20extreme%20class%20imbalance%20%2810%2C000%3A1%20noise-to-target%0Aratio%29.%20For%20many%20models%2C%20we%20achieve%20100%25%20precision%20in%20identifying%20sensitive%0Atopics%20like%20%22money%20laundering%22%20while%20recovering%205-20%25%20of%20target%20conversations.%0AThis%20industry-wide%20vulnerability%20poses%20significant%20risks%20for%20users%20under%0Anetwork%20surveillance%20by%20ISPs%2C%20governments%2C%20or%20local%20adversaries.%20We%20evaluate%0Athree%20mitigation%20strategies%20-%20random%20padding%2C%20token%20batching%2C%20and%20packet%0Ainjection%20-%20finding%20that%20while%20each%20reduces%20attack%20effectiveness%2C%20none%20provides%0Acomplete%20protection.%20Through%20responsible%20disclosure%2C%20we%20have%20collaborated%20with%0Aproviders%20to%20implement%20initial%20countermeasures.%20Our%20findings%20underscore%20the%0Aneed%20for%20LLM%20providers%20to%20address%20metadata%20leakage%20as%20AI%20systems%20handle%0Aincreasingly%20sensitive%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03675v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhisper%2520Leak%253A%2520a%2520side-channel%2520attack%2520on%2520Large%2520Language%2520Models%26entry.906535625%3DGeoff%2520McDonald%2520and%2520Jonathan%2520Bar%2520Or%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520deployed%2520in%2520sensitive%2520domains%250Aincluding%2520healthcare%252C%2520legal%2520services%252C%2520and%2520confidential%2520communications%252C%2520where%250Aprivacy%2520is%2520paramount.%2520This%2520paper%2520introduces%2520Whisper%2520Leak%252C%2520a%2520side-channel%2520attack%250Athat%2520infers%2520user%2520prompt%2520topics%2520from%2520encrypted%2520LLM%2520traffic%2520by%2520analyzing%2520packet%250Asize%2520and%2520timing%2520patterns%2520in%2520streaming%2520responses.%2520Despite%2520TLS%2520encryption%250Aprotecting%2520content%252C%2520these%2520metadata%2520patterns%2520leak%2520sufficient%2520information%2520to%250Aenable%2520topic%2520classification.%2520We%2520demonstrate%2520the%2520attack%2520across%252028%2520popular%2520LLMs%250Afrom%2520major%2520providers%252C%2520achieving%2520near-perfect%2520classification%2520%2528often%2520%253E98%2525%2520AUPRC%2529%250Aand%2520high%2520precision%2520even%2520at%2520extreme%2520class%2520imbalance%2520%252810%252C000%253A1%2520noise-to-target%250Aratio%2529.%2520For%2520many%2520models%252C%2520we%2520achieve%2520100%2525%2520precision%2520in%2520identifying%2520sensitive%250Atopics%2520like%2520%2522money%2520laundering%2522%2520while%2520recovering%25205-20%2525%2520of%2520target%2520conversations.%250AThis%2520industry-wide%2520vulnerability%2520poses%2520significant%2520risks%2520for%2520users%2520under%250Anetwork%2520surveillance%2520by%2520ISPs%252C%2520governments%252C%2520or%2520local%2520adversaries.%2520We%2520evaluate%250Athree%2520mitigation%2520strategies%2520-%2520random%2520padding%252C%2520token%2520batching%252C%2520and%2520packet%250Ainjection%2520-%2520finding%2520that%2520while%2520each%2520reduces%2520attack%2520effectiveness%252C%2520none%2520provides%250Acomplete%2520protection.%2520Through%2520responsible%2520disclosure%252C%2520we%2520have%2520collaborated%2520with%250Aproviders%2520to%2520implement%2520initial%2520countermeasures.%2520Our%2520findings%2520underscore%2520the%250Aneed%2520for%2520LLM%2520providers%2520to%2520address%2520metadata%2520leakage%2520as%2520AI%2520systems%2520handle%250Aincreasingly%2520sensitive%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03675v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Whisper%20Leak%3A%20a%20side-channel%20attack%20on%20Large%20Language%20Models&entry.906535625=Geoff%20McDonald%20and%20Jonathan%20Bar%20Or&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20deployed%20in%20sensitive%20domains%0Aincluding%20healthcare%2C%20legal%20services%2C%20and%20confidential%20communications%2C%20where%0Aprivacy%20is%20paramount.%20This%20paper%20introduces%20Whisper%20Leak%2C%20a%20side-channel%20attack%0Athat%20infers%20user%20prompt%20topics%20from%20encrypted%20LLM%20traffic%20by%20analyzing%20packet%0Asize%20and%20timing%20patterns%20in%20streaming%20responses.%20Despite%20TLS%20encryption%0Aprotecting%20content%2C%20these%20metadata%20patterns%20leak%20sufficient%20information%20to%0Aenable%20topic%20classification.%20We%20demonstrate%20the%20attack%20across%2028%20popular%20LLMs%0Afrom%20major%20providers%2C%20achieving%20near-perfect%20classification%20%28often%20%3E98%25%20AUPRC%29%0Aand%20high%20precision%20even%20at%20extreme%20class%20imbalance%20%2810%2C000%3A1%20noise-to-target%0Aratio%29.%20For%20many%20models%2C%20we%20achieve%20100%25%20precision%20in%20identifying%20sensitive%0Atopics%20like%20%22money%20laundering%22%20while%20recovering%205-20%25%20of%20target%20conversations.%0AThis%20industry-wide%20vulnerability%20poses%20significant%20risks%20for%20users%20under%0Anetwork%20surveillance%20by%20ISPs%2C%20governments%2C%20or%20local%20adversaries.%20We%20evaluate%0Athree%20mitigation%20strategies%20-%20random%20padding%2C%20token%20batching%2C%20and%20packet%0Ainjection%20-%20finding%20that%20while%20each%20reduces%20attack%20effectiveness%2C%20none%20provides%0Acomplete%20protection.%20Through%20responsible%20disclosure%2C%20we%20have%20collaborated%20with%0Aproviders%20to%20implement%20initial%20countermeasures.%20Our%20findings%20underscore%20the%0Aneed%20for%20LLM%20providers%20to%20address%20metadata%20leakage%20as%20AI%20systems%20handle%0Aincreasingly%20sensitive%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03675v1&entry.124074799=Read"},
{"title": "Indicating Robot Vision Capabilities with Augmented Reality", "author": "Hong Wang and Ridhima Phatak and James Ocampo and Zhao Han", "abstract": "  Research indicates that humans can mistakenly assume that robots and humans\nhave the same field of view (FoV), possessing an inaccurate mental model of\nrobots. This misperception may lead to failures during human-robot\ncollaboration tasks where robots might be asked to complete impossible tasks\nabout out-of-view objects. The issue is more severe when robots do not have a\nchance to scan the scene to update their world model while focusing on assigned\ntasks. To help align humans' mental models of robots' vision capabilities, we\npropose four FoV indicators in augmented reality (AR) and conducted a user\nhuman-subjects experiment (N=41) to evaluate them in terms of accuracy,\nconfidence, task efficiency, and workload. These indicators span a spectrum\nfrom egocentric (robot's eye and head space) to allocentric (task space).\nResults showed that the allocentric blocks at the task space had the highest\naccuracy with a delay in interpreting the robot's FoV. The egocentric indicator\nof deeper eye sockets, possible for physical alteration, also increased\naccuracy. In all indicators, participants' confidence was high while cognitive\nload remained low. Finally, we contribute six guidelines for practitioners to\napply our AR indicators or physical alterations to align humans' mental models\nwith robots' vision capabilities.\n", "link": "http://arxiv.org/abs/2511.03550v1", "date": "2025-11-05", "relevancy": 1.6879, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5668}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5577}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Indicating%20Robot%20Vision%20Capabilities%20with%20Augmented%20Reality&body=Title%3A%20Indicating%20Robot%20Vision%20Capabilities%20with%20Augmented%20Reality%0AAuthor%3A%20Hong%20Wang%20and%20Ridhima%20Phatak%20and%20James%20Ocampo%20and%20Zhao%20Han%0AAbstract%3A%20%20%20Research%20indicates%20that%20humans%20can%20mistakenly%20assume%20that%20robots%20and%20humans%0Ahave%20the%20same%20field%20of%20view%20%28FoV%29%2C%20possessing%20an%20inaccurate%20mental%20model%20of%0Arobots.%20This%20misperception%20may%20lead%20to%20failures%20during%20human-robot%0Acollaboration%20tasks%20where%20robots%20might%20be%20asked%20to%20complete%20impossible%20tasks%0Aabout%20out-of-view%20objects.%20The%20issue%20is%20more%20severe%20when%20robots%20do%20not%20have%20a%0Achance%20to%20scan%20the%20scene%20to%20update%20their%20world%20model%20while%20focusing%20on%20assigned%0Atasks.%20To%20help%20align%20humans%27%20mental%20models%20of%20robots%27%20vision%20capabilities%2C%20we%0Apropose%20four%20FoV%20indicators%20in%20augmented%20reality%20%28AR%29%20and%20conducted%20a%20user%0Ahuman-subjects%20experiment%20%28N%3D41%29%20to%20evaluate%20them%20in%20terms%20of%20accuracy%2C%0Aconfidence%2C%20task%20efficiency%2C%20and%20workload.%20These%20indicators%20span%20a%20spectrum%0Afrom%20egocentric%20%28robot%27s%20eye%20and%20head%20space%29%20to%20allocentric%20%28task%20space%29.%0AResults%20showed%20that%20the%20allocentric%20blocks%20at%20the%20task%20space%20had%20the%20highest%0Aaccuracy%20with%20a%20delay%20in%20interpreting%20the%20robot%27s%20FoV.%20The%20egocentric%20indicator%0Aof%20deeper%20eye%20sockets%2C%20possible%20for%20physical%20alteration%2C%20also%20increased%0Aaccuracy.%20In%20all%20indicators%2C%20participants%27%20confidence%20was%20high%20while%20cognitive%0Aload%20remained%20low.%20Finally%2C%20we%20contribute%20six%20guidelines%20for%20practitioners%20to%0Aapply%20our%20AR%20indicators%20or%20physical%20alterations%20to%20align%20humans%27%20mental%20models%0Awith%20robots%27%20vision%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03550v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIndicating%2520Robot%2520Vision%2520Capabilities%2520with%2520Augmented%2520Reality%26entry.906535625%3DHong%2520Wang%2520and%2520Ridhima%2520Phatak%2520and%2520James%2520Ocampo%2520and%2520Zhao%2520Han%26entry.1292438233%3D%2520%2520Research%2520indicates%2520that%2520humans%2520can%2520mistakenly%2520assume%2520that%2520robots%2520and%2520humans%250Ahave%2520the%2520same%2520field%2520of%2520view%2520%2528FoV%2529%252C%2520possessing%2520an%2520inaccurate%2520mental%2520model%2520of%250Arobots.%2520This%2520misperception%2520may%2520lead%2520to%2520failures%2520during%2520human-robot%250Acollaboration%2520tasks%2520where%2520robots%2520might%2520be%2520asked%2520to%2520complete%2520impossible%2520tasks%250Aabout%2520out-of-view%2520objects.%2520The%2520issue%2520is%2520more%2520severe%2520when%2520robots%2520do%2520not%2520have%2520a%250Achance%2520to%2520scan%2520the%2520scene%2520to%2520update%2520their%2520world%2520model%2520while%2520focusing%2520on%2520assigned%250Atasks.%2520To%2520help%2520align%2520humans%2527%2520mental%2520models%2520of%2520robots%2527%2520vision%2520capabilities%252C%2520we%250Apropose%2520four%2520FoV%2520indicators%2520in%2520augmented%2520reality%2520%2528AR%2529%2520and%2520conducted%2520a%2520user%250Ahuman-subjects%2520experiment%2520%2528N%253D41%2529%2520to%2520evaluate%2520them%2520in%2520terms%2520of%2520accuracy%252C%250Aconfidence%252C%2520task%2520efficiency%252C%2520and%2520workload.%2520These%2520indicators%2520span%2520a%2520spectrum%250Afrom%2520egocentric%2520%2528robot%2527s%2520eye%2520and%2520head%2520space%2529%2520to%2520allocentric%2520%2528task%2520space%2529.%250AResults%2520showed%2520that%2520the%2520allocentric%2520blocks%2520at%2520the%2520task%2520space%2520had%2520the%2520highest%250Aaccuracy%2520with%2520a%2520delay%2520in%2520interpreting%2520the%2520robot%2527s%2520FoV.%2520The%2520egocentric%2520indicator%250Aof%2520deeper%2520eye%2520sockets%252C%2520possible%2520for%2520physical%2520alteration%252C%2520also%2520increased%250Aaccuracy.%2520In%2520all%2520indicators%252C%2520participants%2527%2520confidence%2520was%2520high%2520while%2520cognitive%250Aload%2520remained%2520low.%2520Finally%252C%2520we%2520contribute%2520six%2520guidelines%2520for%2520practitioners%2520to%250Aapply%2520our%2520AR%2520indicators%2520or%2520physical%2520alterations%2520to%2520align%2520humans%2527%2520mental%2520models%250Awith%2520robots%2527%2520vision%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03550v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Indicating%20Robot%20Vision%20Capabilities%20with%20Augmented%20Reality&entry.906535625=Hong%20Wang%20and%20Ridhima%20Phatak%20and%20James%20Ocampo%20and%20Zhao%20Han&entry.1292438233=%20%20Research%20indicates%20that%20humans%20can%20mistakenly%20assume%20that%20robots%20and%20humans%0Ahave%20the%20same%20field%20of%20view%20%28FoV%29%2C%20possessing%20an%20inaccurate%20mental%20model%20of%0Arobots.%20This%20misperception%20may%20lead%20to%20failures%20during%20human-robot%0Acollaboration%20tasks%20where%20robots%20might%20be%20asked%20to%20complete%20impossible%20tasks%0Aabout%20out-of-view%20objects.%20The%20issue%20is%20more%20severe%20when%20robots%20do%20not%20have%20a%0Achance%20to%20scan%20the%20scene%20to%20update%20their%20world%20model%20while%20focusing%20on%20assigned%0Atasks.%20To%20help%20align%20humans%27%20mental%20models%20of%20robots%27%20vision%20capabilities%2C%20we%0Apropose%20four%20FoV%20indicators%20in%20augmented%20reality%20%28AR%29%20and%20conducted%20a%20user%0Ahuman-subjects%20experiment%20%28N%3D41%29%20to%20evaluate%20them%20in%20terms%20of%20accuracy%2C%0Aconfidence%2C%20task%20efficiency%2C%20and%20workload.%20These%20indicators%20span%20a%20spectrum%0Afrom%20egocentric%20%28robot%27s%20eye%20and%20head%20space%29%20to%20allocentric%20%28task%20space%29.%0AResults%20showed%20that%20the%20allocentric%20blocks%20at%20the%20task%20space%20had%20the%20highest%0Aaccuracy%20with%20a%20delay%20in%20interpreting%20the%20robot%27s%20FoV.%20The%20egocentric%20indicator%0Aof%20deeper%20eye%20sockets%2C%20possible%20for%20physical%20alteration%2C%20also%20increased%0Aaccuracy.%20In%20all%20indicators%2C%20participants%27%20confidence%20was%20high%20while%20cognitive%0Aload%20remained%20low.%20Finally%2C%20we%20contribute%20six%20guidelines%20for%20practitioners%20to%0Aapply%20our%20AR%20indicators%20or%20physical%20alterations%20to%20align%20humans%27%20mental%20models%0Awith%20robots%27%20vision%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03550v1&entry.124074799=Read"},
{"title": "The Structure of Cross-Validation Error: Stability, Covariance, and\n  Minimax Limits", "author": "Ido Nachum and R\u00fcdiger Urbanke and Thomas Weinberger", "abstract": "  Despite ongoing theoretical research on cross-validation (CV), many\ntheoretical questions about CV remain widely open. This motivates our\ninvestigation into how properties of algorithm-distribution pairs can affect\nthe choice for the number of folds in $k$-fold cross-validation.\n  Our results consist of a novel decomposition of the mean-squared error of\ncross-validation for risk estimation, which explicitly captures the\ncorrelations of error estimates across overlapping folds and includes a novel\nalgorithmic stability notion, squared loss stability, that is considerably\nweaker than the typically required hypothesis stability in other comparable\nworks.\n  Furthermore, we prove:\n  1. For every learning algorithm that minimizes empirical error, a minimax\nlower bound on the mean-squared error of $k$-fold CV estimating the population\nrisk $L_\\mathcal{D}$: \\[ \\min_{k \\mid n}\\; \\max_{\\mathcal{D}}\\;\n\\mathbb{E}\\!\\left[\\big(\\widehat{L}_{\\mathrm{CV}}^{(k)} -\nL_{\\mathcal{D}}\\big)^{2}\\right] \\;=\\; \\Omega\\!\\big(\\sqrt{k}/n\\big), \\] where\n$n$ is the sample size and $k$ the number of folds. This shows that even under\nidealized conditions, for large values of $k$, CV cannot attain the optimum of\norder $1/n$ achievable by a validation set of size $n$, reflecting an inherent\npenalty caused by dependence between folds.\n  2. Complementing this, we exhibit learning rules for which \\[\n  \\max_{\\mathcal{D}}\\; \\mathbb{E}\\!\\left[\\big(\\widehat{L}_{\\mathrm{CV}}^{(k)} -\nL_{\\mathcal{D}}\\big)^{2}\\right] \\;=\\; \\Omega(k/n), \\] matching (up to\nconstants) the accuracy of a hold-out estimator of a single fold of size $n/k$.\n  Together these results delineate the fundamental trade-off in\nresampling-based risk estimation: CV cannot fully exploit all $n$ samples for\nunbiased risk evaluation, and its minimax performance is pinned between the\n$k/n$ and $\\sqrt{k}/n$ regimes.\n", "link": "http://arxiv.org/abs/2511.03554v1", "date": "2025-11-05", "relevancy": 1.6333, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4284}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3981}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Structure%20of%20Cross-Validation%20Error%3A%20Stability%2C%20Covariance%2C%20and%0A%20%20Minimax%20Limits&body=Title%3A%20The%20Structure%20of%20Cross-Validation%20Error%3A%20Stability%2C%20Covariance%2C%20and%0A%20%20Minimax%20Limits%0AAuthor%3A%20Ido%20Nachum%20and%20R%C3%BCdiger%20Urbanke%20and%20Thomas%20Weinberger%0AAbstract%3A%20%20%20Despite%20ongoing%20theoretical%20research%20on%20cross-validation%20%28CV%29%2C%20many%0Atheoretical%20questions%20about%20CV%20remain%20widely%20open.%20This%20motivates%20our%0Ainvestigation%20into%20how%20properties%20of%20algorithm-distribution%20pairs%20can%20affect%0Athe%20choice%20for%20the%20number%20of%20folds%20in%20%24k%24-fold%20cross-validation.%0A%20%20Our%20results%20consist%20of%20a%20novel%20decomposition%20of%20the%20mean-squared%20error%20of%0Across-validation%20for%20risk%20estimation%2C%20which%20explicitly%20captures%20the%0Acorrelations%20of%20error%20estimates%20across%20overlapping%20folds%20and%20includes%20a%20novel%0Aalgorithmic%20stability%20notion%2C%20squared%20loss%20stability%2C%20that%20is%20considerably%0Aweaker%20than%20the%20typically%20required%20hypothesis%20stability%20in%20other%20comparable%0Aworks.%0A%20%20Furthermore%2C%20we%20prove%3A%0A%20%201.%20For%20every%20learning%20algorithm%20that%20minimizes%20empirical%20error%2C%20a%20minimax%0Alower%20bound%20on%20the%20mean-squared%20error%20of%20%24k%24-fold%20CV%20estimating%20the%20population%0Arisk%20%24L_%5Cmathcal%7BD%7D%24%3A%20%5C%5B%20%5Cmin_%7Bk%20%5Cmid%20n%7D%5C%3B%20%5Cmax_%7B%5Cmathcal%7BD%7D%7D%5C%3B%0A%5Cmathbb%7BE%7D%5C%21%5Cleft%5B%5Cbig%28%5Cwidehat%7BL%7D_%7B%5Cmathrm%7BCV%7D%7D%5E%7B%28k%29%7D%20-%0AL_%7B%5Cmathcal%7BD%7D%7D%5Cbig%29%5E%7B2%7D%5Cright%5D%20%5C%3B%3D%5C%3B%20%5COmega%5C%21%5Cbig%28%5Csqrt%7Bk%7D/n%5Cbig%29%2C%20%5C%5D%20where%0A%24n%24%20is%20the%20sample%20size%20and%20%24k%24%20the%20number%20of%20folds.%20This%20shows%20that%20even%20under%0Aidealized%20conditions%2C%20for%20large%20values%20of%20%24k%24%2C%20CV%20cannot%20attain%20the%20optimum%20of%0Aorder%20%241/n%24%20achievable%20by%20a%20validation%20set%20of%20size%20%24n%24%2C%20reflecting%20an%20inherent%0Apenalty%20caused%20by%20dependence%20between%20folds.%0A%20%202.%20Complementing%20this%2C%20we%20exhibit%20learning%20rules%20for%20which%20%5C%5B%0A%20%20%5Cmax_%7B%5Cmathcal%7BD%7D%7D%5C%3B%20%5Cmathbb%7BE%7D%5C%21%5Cleft%5B%5Cbig%28%5Cwidehat%7BL%7D_%7B%5Cmathrm%7BCV%7D%7D%5E%7B%28k%29%7D%20-%0AL_%7B%5Cmathcal%7BD%7D%7D%5Cbig%29%5E%7B2%7D%5Cright%5D%20%5C%3B%3D%5C%3B%20%5COmega%28k/n%29%2C%20%5C%5D%20matching%20%28up%20to%0Aconstants%29%20the%20accuracy%20of%20a%20hold-out%20estimator%20of%20a%20single%20fold%20of%20size%20%24n/k%24.%0A%20%20Together%20these%20results%20delineate%20the%20fundamental%20trade-off%20in%0Aresampling-based%20risk%20estimation%3A%20CV%20cannot%20fully%20exploit%20all%20%24n%24%20samples%20for%0Aunbiased%20risk%20evaluation%2C%20and%20its%20minimax%20performance%20is%20pinned%20between%20the%0A%24k/n%24%20and%20%24%5Csqrt%7Bk%7D/n%24%20regimes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03554v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Structure%2520of%2520Cross-Validation%2520Error%253A%2520Stability%252C%2520Covariance%252C%2520and%250A%2520%2520Minimax%2520Limits%26entry.906535625%3DIdo%2520Nachum%2520and%2520R%25C3%25BCdiger%2520Urbanke%2520and%2520Thomas%2520Weinberger%26entry.1292438233%3D%2520%2520Despite%2520ongoing%2520theoretical%2520research%2520on%2520cross-validation%2520%2528CV%2529%252C%2520many%250Atheoretical%2520questions%2520about%2520CV%2520remain%2520widely%2520open.%2520This%2520motivates%2520our%250Ainvestigation%2520into%2520how%2520properties%2520of%2520algorithm-distribution%2520pairs%2520can%2520affect%250Athe%2520choice%2520for%2520the%2520number%2520of%2520folds%2520in%2520%2524k%2524-fold%2520cross-validation.%250A%2520%2520Our%2520results%2520consist%2520of%2520a%2520novel%2520decomposition%2520of%2520the%2520mean-squared%2520error%2520of%250Across-validation%2520for%2520risk%2520estimation%252C%2520which%2520explicitly%2520captures%2520the%250Acorrelations%2520of%2520error%2520estimates%2520across%2520overlapping%2520folds%2520and%2520includes%2520a%2520novel%250Aalgorithmic%2520stability%2520notion%252C%2520squared%2520loss%2520stability%252C%2520that%2520is%2520considerably%250Aweaker%2520than%2520the%2520typically%2520required%2520hypothesis%2520stability%2520in%2520other%2520comparable%250Aworks.%250A%2520%2520Furthermore%252C%2520we%2520prove%253A%250A%2520%25201.%2520For%2520every%2520learning%2520algorithm%2520that%2520minimizes%2520empirical%2520error%252C%2520a%2520minimax%250Alower%2520bound%2520on%2520the%2520mean-squared%2520error%2520of%2520%2524k%2524-fold%2520CV%2520estimating%2520the%2520population%250Arisk%2520%2524L_%255Cmathcal%257BD%257D%2524%253A%2520%255C%255B%2520%255Cmin_%257Bk%2520%255Cmid%2520n%257D%255C%253B%2520%255Cmax_%257B%255Cmathcal%257BD%257D%257D%255C%253B%250A%255Cmathbb%257BE%257D%255C%2521%255Cleft%255B%255Cbig%2528%255Cwidehat%257BL%257D_%257B%255Cmathrm%257BCV%257D%257D%255E%257B%2528k%2529%257D%2520-%250AL_%257B%255Cmathcal%257BD%257D%257D%255Cbig%2529%255E%257B2%257D%255Cright%255D%2520%255C%253B%253D%255C%253B%2520%255COmega%255C%2521%255Cbig%2528%255Csqrt%257Bk%257D/n%255Cbig%2529%252C%2520%255C%255D%2520where%250A%2524n%2524%2520is%2520the%2520sample%2520size%2520and%2520%2524k%2524%2520the%2520number%2520of%2520folds.%2520This%2520shows%2520that%2520even%2520under%250Aidealized%2520conditions%252C%2520for%2520large%2520values%2520of%2520%2524k%2524%252C%2520CV%2520cannot%2520attain%2520the%2520optimum%2520of%250Aorder%2520%25241/n%2524%2520achievable%2520by%2520a%2520validation%2520set%2520of%2520size%2520%2524n%2524%252C%2520reflecting%2520an%2520inherent%250Apenalty%2520caused%2520by%2520dependence%2520between%2520folds.%250A%2520%25202.%2520Complementing%2520this%252C%2520we%2520exhibit%2520learning%2520rules%2520for%2520which%2520%255C%255B%250A%2520%2520%255Cmax_%257B%255Cmathcal%257BD%257D%257D%255C%253B%2520%255Cmathbb%257BE%257D%255C%2521%255Cleft%255B%255Cbig%2528%255Cwidehat%257BL%257D_%257B%255Cmathrm%257BCV%257D%257D%255E%257B%2528k%2529%257D%2520-%250AL_%257B%255Cmathcal%257BD%257D%257D%255Cbig%2529%255E%257B2%257D%255Cright%255D%2520%255C%253B%253D%255C%253B%2520%255COmega%2528k/n%2529%252C%2520%255C%255D%2520matching%2520%2528up%2520to%250Aconstants%2529%2520the%2520accuracy%2520of%2520a%2520hold-out%2520estimator%2520of%2520a%2520single%2520fold%2520of%2520size%2520%2524n/k%2524.%250A%2520%2520Together%2520these%2520results%2520delineate%2520the%2520fundamental%2520trade-off%2520in%250Aresampling-based%2520risk%2520estimation%253A%2520CV%2520cannot%2520fully%2520exploit%2520all%2520%2524n%2524%2520samples%2520for%250Aunbiased%2520risk%2520evaluation%252C%2520and%2520its%2520minimax%2520performance%2520is%2520pinned%2520between%2520the%250A%2524k/n%2524%2520and%2520%2524%255Csqrt%257Bk%257D/n%2524%2520regimes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03554v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Structure%20of%20Cross-Validation%20Error%3A%20Stability%2C%20Covariance%2C%20and%0A%20%20Minimax%20Limits&entry.906535625=Ido%20Nachum%20and%20R%C3%BCdiger%20Urbanke%20and%20Thomas%20Weinberger&entry.1292438233=%20%20Despite%20ongoing%20theoretical%20research%20on%20cross-validation%20%28CV%29%2C%20many%0Atheoretical%20questions%20about%20CV%20remain%20widely%20open.%20This%20motivates%20our%0Ainvestigation%20into%20how%20properties%20of%20algorithm-distribution%20pairs%20can%20affect%0Athe%20choice%20for%20the%20number%20of%20folds%20in%20%24k%24-fold%20cross-validation.%0A%20%20Our%20results%20consist%20of%20a%20novel%20decomposition%20of%20the%20mean-squared%20error%20of%0Across-validation%20for%20risk%20estimation%2C%20which%20explicitly%20captures%20the%0Acorrelations%20of%20error%20estimates%20across%20overlapping%20folds%20and%20includes%20a%20novel%0Aalgorithmic%20stability%20notion%2C%20squared%20loss%20stability%2C%20that%20is%20considerably%0Aweaker%20than%20the%20typically%20required%20hypothesis%20stability%20in%20other%20comparable%0Aworks.%0A%20%20Furthermore%2C%20we%20prove%3A%0A%20%201.%20For%20every%20learning%20algorithm%20that%20minimizes%20empirical%20error%2C%20a%20minimax%0Alower%20bound%20on%20the%20mean-squared%20error%20of%20%24k%24-fold%20CV%20estimating%20the%20population%0Arisk%20%24L_%5Cmathcal%7BD%7D%24%3A%20%5C%5B%20%5Cmin_%7Bk%20%5Cmid%20n%7D%5C%3B%20%5Cmax_%7B%5Cmathcal%7BD%7D%7D%5C%3B%0A%5Cmathbb%7BE%7D%5C%21%5Cleft%5B%5Cbig%28%5Cwidehat%7BL%7D_%7B%5Cmathrm%7BCV%7D%7D%5E%7B%28k%29%7D%20-%0AL_%7B%5Cmathcal%7BD%7D%7D%5Cbig%29%5E%7B2%7D%5Cright%5D%20%5C%3B%3D%5C%3B%20%5COmega%5C%21%5Cbig%28%5Csqrt%7Bk%7D/n%5Cbig%29%2C%20%5C%5D%20where%0A%24n%24%20is%20the%20sample%20size%20and%20%24k%24%20the%20number%20of%20folds.%20This%20shows%20that%20even%20under%0Aidealized%20conditions%2C%20for%20large%20values%20of%20%24k%24%2C%20CV%20cannot%20attain%20the%20optimum%20of%0Aorder%20%241/n%24%20achievable%20by%20a%20validation%20set%20of%20size%20%24n%24%2C%20reflecting%20an%20inherent%0Apenalty%20caused%20by%20dependence%20between%20folds.%0A%20%202.%20Complementing%20this%2C%20we%20exhibit%20learning%20rules%20for%20which%20%5C%5B%0A%20%20%5Cmax_%7B%5Cmathcal%7BD%7D%7D%5C%3B%20%5Cmathbb%7BE%7D%5C%21%5Cleft%5B%5Cbig%28%5Cwidehat%7BL%7D_%7B%5Cmathrm%7BCV%7D%7D%5E%7B%28k%29%7D%20-%0AL_%7B%5Cmathcal%7BD%7D%7D%5Cbig%29%5E%7B2%7D%5Cright%5D%20%5C%3B%3D%5C%3B%20%5COmega%28k/n%29%2C%20%5C%5D%20matching%20%28up%20to%0Aconstants%29%20the%20accuracy%20of%20a%20hold-out%20estimator%20of%20a%20single%20fold%20of%20size%20%24n/k%24.%0A%20%20Together%20these%20results%20delineate%20the%20fundamental%20trade-off%20in%0Aresampling-based%20risk%20estimation%3A%20CV%20cannot%20fully%20exploit%20all%20%24n%24%20samples%20for%0Aunbiased%20risk%20evaluation%2C%20and%20its%20minimax%20performance%20is%20pinned%20between%20the%0A%24k/n%24%20and%20%24%5Csqrt%7Bk%7D/n%24%20regimes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03554v1&entry.124074799=Read"},
{"title": "Motion Planning Under Temporal Logic Specifications In Semantically\n  Unknown Environments", "author": "Azizollah Taheri and Derya Aksaray", "abstract": "  This paper addresses a motion planning problem to achieve\nspatio-temporal-logical tasks, expressed by syntactically co-safe linear\ntemporal logic specifications (scLTL\\next), in uncertain environments. Here,\nthe uncertainty is modeled as some probabilistic knowledge on the semantic\nlabels of the environment. For example, the task is \"first go to region 1, then\ngo to region 2\"; however, the exact locations of regions 1 and 2 are not known\na priori, instead a probabilistic belief is available. We propose a novel\nautomata-theoretic approach, where a special product automaton is constructed\nto capture the uncertainty related to semantic labels, and a reward function is\ndesigned for each edge of this product automaton. The proposed algorithm\nutilizes value iteration for online replanning. We show some theoretical\nresults and present some simulations/experiments to demonstrate the efficacy of\nthe proposed approach.\n", "link": "http://arxiv.org/abs/2511.03652v1", "date": "2025-11-05", "relevancy": 1.6102, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5657}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5341}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5262}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Motion%20Planning%20Under%20Temporal%20Logic%20Specifications%20In%20Semantically%0A%20%20Unknown%20Environments&body=Title%3A%20Motion%20Planning%20Under%20Temporal%20Logic%20Specifications%20In%20Semantically%0A%20%20Unknown%20Environments%0AAuthor%3A%20Azizollah%20Taheri%20and%20Derya%20Aksaray%0AAbstract%3A%20%20%20This%20paper%20addresses%20a%20motion%20planning%20problem%20to%20achieve%0Aspatio-temporal-logical%20tasks%2C%20expressed%20by%20syntactically%20co-safe%20linear%0Atemporal%20logic%20specifications%20%28scLTL%5Cnext%29%2C%20in%20uncertain%20environments.%20Here%2C%0Athe%20uncertainty%20is%20modeled%20as%20some%20probabilistic%20knowledge%20on%20the%20semantic%0Alabels%20of%20the%20environment.%20For%20example%2C%20the%20task%20is%20%22first%20go%20to%20region%201%2C%20then%0Ago%20to%20region%202%22%3B%20however%2C%20the%20exact%20locations%20of%20regions%201%20and%202%20are%20not%20known%0Aa%20priori%2C%20instead%20a%20probabilistic%20belief%20is%20available.%20We%20propose%20a%20novel%0Aautomata-theoretic%20approach%2C%20where%20a%20special%20product%20automaton%20is%20constructed%0Ato%20capture%20the%20uncertainty%20related%20to%20semantic%20labels%2C%20and%20a%20reward%20function%20is%0Adesigned%20for%20each%20edge%20of%20this%20product%20automaton.%20The%20proposed%20algorithm%0Autilizes%20value%20iteration%20for%20online%20replanning.%20We%20show%20some%20theoretical%0Aresults%20and%20present%20some%20simulations/experiments%20to%20demonstrate%20the%20efficacy%20of%0Athe%20proposed%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03652v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotion%2520Planning%2520Under%2520Temporal%2520Logic%2520Specifications%2520In%2520Semantically%250A%2520%2520Unknown%2520Environments%26entry.906535625%3DAzizollah%2520Taheri%2520and%2520Derya%2520Aksaray%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520a%2520motion%2520planning%2520problem%2520to%2520achieve%250Aspatio-temporal-logical%2520tasks%252C%2520expressed%2520by%2520syntactically%2520co-safe%2520linear%250Atemporal%2520logic%2520specifications%2520%2528scLTL%255Cnext%2529%252C%2520in%2520uncertain%2520environments.%2520Here%252C%250Athe%2520uncertainty%2520is%2520modeled%2520as%2520some%2520probabilistic%2520knowledge%2520on%2520the%2520semantic%250Alabels%2520of%2520the%2520environment.%2520For%2520example%252C%2520the%2520task%2520is%2520%2522first%2520go%2520to%2520region%25201%252C%2520then%250Ago%2520to%2520region%25202%2522%253B%2520however%252C%2520the%2520exact%2520locations%2520of%2520regions%25201%2520and%25202%2520are%2520not%2520known%250Aa%2520priori%252C%2520instead%2520a%2520probabilistic%2520belief%2520is%2520available.%2520We%2520propose%2520a%2520novel%250Aautomata-theoretic%2520approach%252C%2520where%2520a%2520special%2520product%2520automaton%2520is%2520constructed%250Ato%2520capture%2520the%2520uncertainty%2520related%2520to%2520semantic%2520labels%252C%2520and%2520a%2520reward%2520function%2520is%250Adesigned%2520for%2520each%2520edge%2520of%2520this%2520product%2520automaton.%2520The%2520proposed%2520algorithm%250Autilizes%2520value%2520iteration%2520for%2520online%2520replanning.%2520We%2520show%2520some%2520theoretical%250Aresults%2520and%2520present%2520some%2520simulations/experiments%2520to%2520demonstrate%2520the%2520efficacy%2520of%250Athe%2520proposed%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03652v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Motion%20Planning%20Under%20Temporal%20Logic%20Specifications%20In%20Semantically%0A%20%20Unknown%20Environments&entry.906535625=Azizollah%20Taheri%20and%20Derya%20Aksaray&entry.1292438233=%20%20This%20paper%20addresses%20a%20motion%20planning%20problem%20to%20achieve%0Aspatio-temporal-logical%20tasks%2C%20expressed%20by%20syntactically%20co-safe%20linear%0Atemporal%20logic%20specifications%20%28scLTL%5Cnext%29%2C%20in%20uncertain%20environments.%20Here%2C%0Athe%20uncertainty%20is%20modeled%20as%20some%20probabilistic%20knowledge%20on%20the%20semantic%0Alabels%20of%20the%20environment.%20For%20example%2C%20the%20task%20is%20%22first%20go%20to%20region%201%2C%20then%0Ago%20to%20region%202%22%3B%20however%2C%20the%20exact%20locations%20of%20regions%201%20and%202%20are%20not%20known%0Aa%20priori%2C%20instead%20a%20probabilistic%20belief%20is%20available.%20We%20propose%20a%20novel%0Aautomata-theoretic%20approach%2C%20where%20a%20special%20product%20automaton%20is%20constructed%0Ato%20capture%20the%20uncertainty%20related%20to%20semantic%20labels%2C%20and%20a%20reward%20function%20is%0Adesigned%20for%20each%20edge%20of%20this%20product%20automaton.%20The%20proposed%20algorithm%0Autilizes%20value%20iteration%20for%20online%20replanning.%20We%20show%20some%20theoretical%0Aresults%20and%20present%20some%20simulations/experiments%20to%20demonstrate%20the%20efficacy%20of%0Athe%20proposed%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03652v1&entry.124074799=Read"},
{"title": "A Polynomial-Time Algorithm for Variational Inequalities under the Minty\n  Condition", "author": "Ioannis Anagnostides and Gabriele Farina and Tuomas Sandholm and Brian Hu Zhang", "abstract": "  Solving variational inequalities (SVIs) is a foundational problem at the\nheart of optimization. However, this expressivity comes at the cost of\ncomputational hardness. As a result, most research has focused on carving out\nspecific subclasses that elude those intractability barriers. A classical\nproperty that goes back to the 1960s is the Minty condition, which postulates\nthat the Minty VI (MVI) problem admits a solution.\n  In this paper, we establish the first polynomial-time algorithm -- that is,\nwith complexity growing polynomially in the dimension $d$ and\n$\\log(1/\\epsilon)$ -- for solving $\\epsilon$-SVIs for Lipschitz continuous\nmappings under the Minty condition. Prior approaches either incurred an\nexponentially worse dependence on $1/\\epsilon$ or made restrictive assumptions.\nTo do so, we introduce a new variant of the ellipsoid algorithm whereby\nseparating hyperplanes are obtained after taking a gradient descent step from\nthe center of the ellipsoid. It succeeds even though the set of SVIs can be\nnonconvex and not fully dimensional. Moreover, when our algorithm is applied to\nan instance with no MVI solution and fails to identify an SVI solution, it\nproduces a succinct certificate of MVI infeasibility. We also show that\ndeciding whether the Minty condition holds is $\\mathsf{coNP}$-complete, thereby\nestablishing that the disjunction of those two problems is polynomial-time\nsolvable even though each problem is individually intractable.\n  We provide several extensions and new applications of our main results. Most\nnotably, we obtain the first polynomial-time algorithms for i) globally\nminimizing a (potentially nonsmooth) quasar-convex function, and ii) computing\nNash equilibria in multi-player harmonic games. Finally, in two-player\ngeneral-sum concave games, we give the first polynomial-time algorithm that\noutputs either a Nash equilibrium or a strict coarse correlated equilibrium.\n", "link": "http://arxiv.org/abs/2504.03432v2", "date": "2025-11-05", "relevancy": 1.5956, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.406}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3965}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.3928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Polynomial-Time%20Algorithm%20for%20Variational%20Inequalities%20under%20the%20Minty%0A%20%20Condition&body=Title%3A%20A%20Polynomial-Time%20Algorithm%20for%20Variational%20Inequalities%20under%20the%20Minty%0A%20%20Condition%0AAuthor%3A%20Ioannis%20Anagnostides%20and%20Gabriele%20Farina%20and%20Tuomas%20Sandholm%20and%20Brian%20Hu%20Zhang%0AAbstract%3A%20%20%20Solving%20variational%20inequalities%20%28SVIs%29%20is%20a%20foundational%20problem%20at%20the%0Aheart%20of%20optimization.%20However%2C%20this%20expressivity%20comes%20at%20the%20cost%20of%0Acomputational%20hardness.%20As%20a%20result%2C%20most%20research%20has%20focused%20on%20carving%20out%0Aspecific%20subclasses%20that%20elude%20those%20intractability%20barriers.%20A%20classical%0Aproperty%20that%20goes%20back%20to%20the%201960s%20is%20the%20Minty%20condition%2C%20which%20postulates%0Athat%20the%20Minty%20VI%20%28MVI%29%20problem%20admits%20a%20solution.%0A%20%20In%20this%20paper%2C%20we%20establish%20the%20first%20polynomial-time%20algorithm%20--%20that%20is%2C%0Awith%20complexity%20growing%20polynomially%20in%20the%20dimension%20%24d%24%20and%0A%24%5Clog%281/%5Cepsilon%29%24%20--%20for%20solving%20%24%5Cepsilon%24-SVIs%20for%20Lipschitz%20continuous%0Amappings%20under%20the%20Minty%20condition.%20Prior%20approaches%20either%20incurred%20an%0Aexponentially%20worse%20dependence%20on%20%241/%5Cepsilon%24%20or%20made%20restrictive%20assumptions.%0ATo%20do%20so%2C%20we%20introduce%20a%20new%20variant%20of%20the%20ellipsoid%20algorithm%20whereby%0Aseparating%20hyperplanes%20are%20obtained%20after%20taking%20a%20gradient%20descent%20step%20from%0Athe%20center%20of%20the%20ellipsoid.%20It%20succeeds%20even%20though%20the%20set%20of%20SVIs%20can%20be%0Anonconvex%20and%20not%20fully%20dimensional.%20Moreover%2C%20when%20our%20algorithm%20is%20applied%20to%0Aan%20instance%20with%20no%20MVI%20solution%20and%20fails%20to%20identify%20an%20SVI%20solution%2C%20it%0Aproduces%20a%20succinct%20certificate%20of%20MVI%20infeasibility.%20We%20also%20show%20that%0Adeciding%20whether%20the%20Minty%20condition%20holds%20is%20%24%5Cmathsf%7BcoNP%7D%24-complete%2C%20thereby%0Aestablishing%20that%20the%20disjunction%20of%20those%20two%20problems%20is%20polynomial-time%0Asolvable%20even%20though%20each%20problem%20is%20individually%20intractable.%0A%20%20We%20provide%20several%20extensions%20and%20new%20applications%20of%20our%20main%20results.%20Most%0Anotably%2C%20we%20obtain%20the%20first%20polynomial-time%20algorithms%20for%20i%29%20globally%0Aminimizing%20a%20%28potentially%20nonsmooth%29%20quasar-convex%20function%2C%20and%20ii%29%20computing%0ANash%20equilibria%20in%20multi-player%20harmonic%20games.%20Finally%2C%20in%20two-player%0Ageneral-sum%20concave%20games%2C%20we%20give%20the%20first%20polynomial-time%20algorithm%20that%0Aoutputs%20either%20a%20Nash%20equilibrium%20or%20a%20strict%20coarse%20correlated%20equilibrium.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.03432v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Polynomial-Time%2520Algorithm%2520for%2520Variational%2520Inequalities%2520under%2520the%2520Minty%250A%2520%2520Condition%26entry.906535625%3DIoannis%2520Anagnostides%2520and%2520Gabriele%2520Farina%2520and%2520Tuomas%2520Sandholm%2520and%2520Brian%2520Hu%2520Zhang%26entry.1292438233%3D%2520%2520Solving%2520variational%2520inequalities%2520%2528SVIs%2529%2520is%2520a%2520foundational%2520problem%2520at%2520the%250Aheart%2520of%2520optimization.%2520However%252C%2520this%2520expressivity%2520comes%2520at%2520the%2520cost%2520of%250Acomputational%2520hardness.%2520As%2520a%2520result%252C%2520most%2520research%2520has%2520focused%2520on%2520carving%2520out%250Aspecific%2520subclasses%2520that%2520elude%2520those%2520intractability%2520barriers.%2520A%2520classical%250Aproperty%2520that%2520goes%2520back%2520to%2520the%25201960s%2520is%2520the%2520Minty%2520condition%252C%2520which%2520postulates%250Athat%2520the%2520Minty%2520VI%2520%2528MVI%2529%2520problem%2520admits%2520a%2520solution.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520establish%2520the%2520first%2520polynomial-time%2520algorithm%2520--%2520that%2520is%252C%250Awith%2520complexity%2520growing%2520polynomially%2520in%2520the%2520dimension%2520%2524d%2524%2520and%250A%2524%255Clog%25281/%255Cepsilon%2529%2524%2520--%2520for%2520solving%2520%2524%255Cepsilon%2524-SVIs%2520for%2520Lipschitz%2520continuous%250Amappings%2520under%2520the%2520Minty%2520condition.%2520Prior%2520approaches%2520either%2520incurred%2520an%250Aexponentially%2520worse%2520dependence%2520on%2520%25241/%255Cepsilon%2524%2520or%2520made%2520restrictive%2520assumptions.%250ATo%2520do%2520so%252C%2520we%2520introduce%2520a%2520new%2520variant%2520of%2520the%2520ellipsoid%2520algorithm%2520whereby%250Aseparating%2520hyperplanes%2520are%2520obtained%2520after%2520taking%2520a%2520gradient%2520descent%2520step%2520from%250Athe%2520center%2520of%2520the%2520ellipsoid.%2520It%2520succeeds%2520even%2520though%2520the%2520set%2520of%2520SVIs%2520can%2520be%250Anonconvex%2520and%2520not%2520fully%2520dimensional.%2520Moreover%252C%2520when%2520our%2520algorithm%2520is%2520applied%2520to%250Aan%2520instance%2520with%2520no%2520MVI%2520solution%2520and%2520fails%2520to%2520identify%2520an%2520SVI%2520solution%252C%2520it%250Aproduces%2520a%2520succinct%2520certificate%2520of%2520MVI%2520infeasibility.%2520We%2520also%2520show%2520that%250Adeciding%2520whether%2520the%2520Minty%2520condition%2520holds%2520is%2520%2524%255Cmathsf%257BcoNP%257D%2524-complete%252C%2520thereby%250Aestablishing%2520that%2520the%2520disjunction%2520of%2520those%2520two%2520problems%2520is%2520polynomial-time%250Asolvable%2520even%2520though%2520each%2520problem%2520is%2520individually%2520intractable.%250A%2520%2520We%2520provide%2520several%2520extensions%2520and%2520new%2520applications%2520of%2520our%2520main%2520results.%2520Most%250Anotably%252C%2520we%2520obtain%2520the%2520first%2520polynomial-time%2520algorithms%2520for%2520i%2529%2520globally%250Aminimizing%2520a%2520%2528potentially%2520nonsmooth%2529%2520quasar-convex%2520function%252C%2520and%2520ii%2529%2520computing%250ANash%2520equilibria%2520in%2520multi-player%2520harmonic%2520games.%2520Finally%252C%2520in%2520two-player%250Ageneral-sum%2520concave%2520games%252C%2520we%2520give%2520the%2520first%2520polynomial-time%2520algorithm%2520that%250Aoutputs%2520either%2520a%2520Nash%2520equilibrium%2520or%2520a%2520strict%2520coarse%2520correlated%2520equilibrium.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.03432v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Polynomial-Time%20Algorithm%20for%20Variational%20Inequalities%20under%20the%20Minty%0A%20%20Condition&entry.906535625=Ioannis%20Anagnostides%20and%20Gabriele%20Farina%20and%20Tuomas%20Sandholm%20and%20Brian%20Hu%20Zhang&entry.1292438233=%20%20Solving%20variational%20inequalities%20%28SVIs%29%20is%20a%20foundational%20problem%20at%20the%0Aheart%20of%20optimization.%20However%2C%20this%20expressivity%20comes%20at%20the%20cost%20of%0Acomputational%20hardness.%20As%20a%20result%2C%20most%20research%20has%20focused%20on%20carving%20out%0Aspecific%20subclasses%20that%20elude%20those%20intractability%20barriers.%20A%20classical%0Aproperty%20that%20goes%20back%20to%20the%201960s%20is%20the%20Minty%20condition%2C%20which%20postulates%0Athat%20the%20Minty%20VI%20%28MVI%29%20problem%20admits%20a%20solution.%0A%20%20In%20this%20paper%2C%20we%20establish%20the%20first%20polynomial-time%20algorithm%20--%20that%20is%2C%0Awith%20complexity%20growing%20polynomially%20in%20the%20dimension%20%24d%24%20and%0A%24%5Clog%281/%5Cepsilon%29%24%20--%20for%20solving%20%24%5Cepsilon%24-SVIs%20for%20Lipschitz%20continuous%0Amappings%20under%20the%20Minty%20condition.%20Prior%20approaches%20either%20incurred%20an%0Aexponentially%20worse%20dependence%20on%20%241/%5Cepsilon%24%20or%20made%20restrictive%20assumptions.%0ATo%20do%20so%2C%20we%20introduce%20a%20new%20variant%20of%20the%20ellipsoid%20algorithm%20whereby%0Aseparating%20hyperplanes%20are%20obtained%20after%20taking%20a%20gradient%20descent%20step%20from%0Athe%20center%20of%20the%20ellipsoid.%20It%20succeeds%20even%20though%20the%20set%20of%20SVIs%20can%20be%0Anonconvex%20and%20not%20fully%20dimensional.%20Moreover%2C%20when%20our%20algorithm%20is%20applied%20to%0Aan%20instance%20with%20no%20MVI%20solution%20and%20fails%20to%20identify%20an%20SVI%20solution%2C%20it%0Aproduces%20a%20succinct%20certificate%20of%20MVI%20infeasibility.%20We%20also%20show%20that%0Adeciding%20whether%20the%20Minty%20condition%20holds%20is%20%24%5Cmathsf%7BcoNP%7D%24-complete%2C%20thereby%0Aestablishing%20that%20the%20disjunction%20of%20those%20two%20problems%20is%20polynomial-time%0Asolvable%20even%20though%20each%20problem%20is%20individually%20intractable.%0A%20%20We%20provide%20several%20extensions%20and%20new%20applications%20of%20our%20main%20results.%20Most%0Anotably%2C%20we%20obtain%20the%20first%20polynomial-time%20algorithms%20for%20i%29%20globally%0Aminimizing%20a%20%28potentially%20nonsmooth%29%20quasar-convex%20function%2C%20and%20ii%29%20computing%0ANash%20equilibria%20in%20multi-player%20harmonic%20games.%20Finally%2C%20in%20two-player%0Ageneral-sum%20concave%20games%2C%20we%20give%20the%20first%20polynomial-time%20algorithm%20that%0Aoutputs%20either%20a%20Nash%20equilibrium%20or%20a%20strict%20coarse%20correlated%20equilibrium.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.03432v2&entry.124074799=Read"},
{"title": "Manifold-constrained Hamilton-Jacobi Reachability Learning for\n  Decentralized Multi-Agent Motion Planning", "author": "Qingyi Chen and Ruiqi Ni and Jun Kim and Ahmed H. Qureshi", "abstract": "  Safe multi-agent motion planning (MAMP) under task-induced constraints is a\ncritical challenge in robotics. Many real-world scenarios require robots to\nnavigate dynamic environments while adhering to manifold constraints imposed by\ntasks. For example, service robots must carry cups upright while avoiding\ncollisions with humans or other robots. Despite recent advances in\ndecentralized MAMP for high-dimensional systems, incorporating manifold\nconstraints remains difficult. To address this, we propose a\nmanifold-constrained Hamilton-Jacobi reachability (HJR) learning framework for\ndecentralized MAMP. Our method solves HJR problems under manifold constraints\nto capture task-aware safety conditions, which are then integrated into a\ndecentralized trajectory optimization planner. This enables robots to generate\nmotion plans that are both safe and task-feasible without requiring assumptions\nabout other agents' policies. Our approach generalizes across diverse\nmanifold-constrained tasks and scales effectively to high-dimensional\nmulti-agent manipulation problems. Experiments show that our method outperforms\nexisting constrained motion planners and operates at speeds suitable for\nreal-world applications. Video demonstrations are available at\nhttps://youtu.be/RYcEHMnPTH8 .\n", "link": "http://arxiv.org/abs/2511.03591v1", "date": "2025-11-05", "relevancy": 1.5874, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5584}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5228}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.52}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Manifold-constrained%20Hamilton-Jacobi%20Reachability%20Learning%20for%0A%20%20Decentralized%20Multi-Agent%20Motion%20Planning&body=Title%3A%20Manifold-constrained%20Hamilton-Jacobi%20Reachability%20Learning%20for%0A%20%20Decentralized%20Multi-Agent%20Motion%20Planning%0AAuthor%3A%20Qingyi%20Chen%20and%20Ruiqi%20Ni%20and%20Jun%20Kim%20and%20Ahmed%20H.%20Qureshi%0AAbstract%3A%20%20%20Safe%20multi-agent%20motion%20planning%20%28MAMP%29%20under%20task-induced%20constraints%20is%20a%0Acritical%20challenge%20in%20robotics.%20Many%20real-world%20scenarios%20require%20robots%20to%0Anavigate%20dynamic%20environments%20while%20adhering%20to%20manifold%20constraints%20imposed%20by%0Atasks.%20For%20example%2C%20service%20robots%20must%20carry%20cups%20upright%20while%20avoiding%0Acollisions%20with%20humans%20or%20other%20robots.%20Despite%20recent%20advances%20in%0Adecentralized%20MAMP%20for%20high-dimensional%20systems%2C%20incorporating%20manifold%0Aconstraints%20remains%20difficult.%20To%20address%20this%2C%20we%20propose%20a%0Amanifold-constrained%20Hamilton-Jacobi%20reachability%20%28HJR%29%20learning%20framework%20for%0Adecentralized%20MAMP.%20Our%20method%20solves%20HJR%20problems%20under%20manifold%20constraints%0Ato%20capture%20task-aware%20safety%20conditions%2C%20which%20are%20then%20integrated%20into%20a%0Adecentralized%20trajectory%20optimization%20planner.%20This%20enables%20robots%20to%20generate%0Amotion%20plans%20that%20are%20both%20safe%20and%20task-feasible%20without%20requiring%20assumptions%0Aabout%20other%20agents%27%20policies.%20Our%20approach%20generalizes%20across%20diverse%0Amanifold-constrained%20tasks%20and%20scales%20effectively%20to%20high-dimensional%0Amulti-agent%20manipulation%20problems.%20Experiments%20show%20that%20our%20method%20outperforms%0Aexisting%20constrained%20motion%20planners%20and%20operates%20at%20speeds%20suitable%20for%0Areal-world%20applications.%20Video%20demonstrations%20are%20available%20at%0Ahttps%3A//youtu.be/RYcEHMnPTH8%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03591v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DManifold-constrained%2520Hamilton-Jacobi%2520Reachability%2520Learning%2520for%250A%2520%2520Decentralized%2520Multi-Agent%2520Motion%2520Planning%26entry.906535625%3DQingyi%2520Chen%2520and%2520Ruiqi%2520Ni%2520and%2520Jun%2520Kim%2520and%2520Ahmed%2520H.%2520Qureshi%26entry.1292438233%3D%2520%2520Safe%2520multi-agent%2520motion%2520planning%2520%2528MAMP%2529%2520under%2520task-induced%2520constraints%2520is%2520a%250Acritical%2520challenge%2520in%2520robotics.%2520Many%2520real-world%2520scenarios%2520require%2520robots%2520to%250Anavigate%2520dynamic%2520environments%2520while%2520adhering%2520to%2520manifold%2520constraints%2520imposed%2520by%250Atasks.%2520For%2520example%252C%2520service%2520robots%2520must%2520carry%2520cups%2520upright%2520while%2520avoiding%250Acollisions%2520with%2520humans%2520or%2520other%2520robots.%2520Despite%2520recent%2520advances%2520in%250Adecentralized%2520MAMP%2520for%2520high-dimensional%2520systems%252C%2520incorporating%2520manifold%250Aconstraints%2520remains%2520difficult.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%250Amanifold-constrained%2520Hamilton-Jacobi%2520reachability%2520%2528HJR%2529%2520learning%2520framework%2520for%250Adecentralized%2520MAMP.%2520Our%2520method%2520solves%2520HJR%2520problems%2520under%2520manifold%2520constraints%250Ato%2520capture%2520task-aware%2520safety%2520conditions%252C%2520which%2520are%2520then%2520integrated%2520into%2520a%250Adecentralized%2520trajectory%2520optimization%2520planner.%2520This%2520enables%2520robots%2520to%2520generate%250Amotion%2520plans%2520that%2520are%2520both%2520safe%2520and%2520task-feasible%2520without%2520requiring%2520assumptions%250Aabout%2520other%2520agents%2527%2520policies.%2520Our%2520approach%2520generalizes%2520across%2520diverse%250Amanifold-constrained%2520tasks%2520and%2520scales%2520effectively%2520to%2520high-dimensional%250Amulti-agent%2520manipulation%2520problems.%2520Experiments%2520show%2520that%2520our%2520method%2520outperforms%250Aexisting%2520constrained%2520motion%2520planners%2520and%2520operates%2520at%2520speeds%2520suitable%2520for%250Areal-world%2520applications.%2520Video%2520demonstrations%2520are%2520available%2520at%250Ahttps%253A//youtu.be/RYcEHMnPTH8%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03591v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Manifold-constrained%20Hamilton-Jacobi%20Reachability%20Learning%20for%0A%20%20Decentralized%20Multi-Agent%20Motion%20Planning&entry.906535625=Qingyi%20Chen%20and%20Ruiqi%20Ni%20and%20Jun%20Kim%20and%20Ahmed%20H.%20Qureshi&entry.1292438233=%20%20Safe%20multi-agent%20motion%20planning%20%28MAMP%29%20under%20task-induced%20constraints%20is%20a%0Acritical%20challenge%20in%20robotics.%20Many%20real-world%20scenarios%20require%20robots%20to%0Anavigate%20dynamic%20environments%20while%20adhering%20to%20manifold%20constraints%20imposed%20by%0Atasks.%20For%20example%2C%20service%20robots%20must%20carry%20cups%20upright%20while%20avoiding%0Acollisions%20with%20humans%20or%20other%20robots.%20Despite%20recent%20advances%20in%0Adecentralized%20MAMP%20for%20high-dimensional%20systems%2C%20incorporating%20manifold%0Aconstraints%20remains%20difficult.%20To%20address%20this%2C%20we%20propose%20a%0Amanifold-constrained%20Hamilton-Jacobi%20reachability%20%28HJR%29%20learning%20framework%20for%0Adecentralized%20MAMP.%20Our%20method%20solves%20HJR%20problems%20under%20manifold%20constraints%0Ato%20capture%20task-aware%20safety%20conditions%2C%20which%20are%20then%20integrated%20into%20a%0Adecentralized%20trajectory%20optimization%20planner.%20This%20enables%20robots%20to%20generate%0Amotion%20plans%20that%20are%20both%20safe%20and%20task-feasible%20without%20requiring%20assumptions%0Aabout%20other%20agents%27%20policies.%20Our%20approach%20generalizes%20across%20diverse%0Amanifold-constrained%20tasks%20and%20scales%20effectively%20to%20high-dimensional%0Amulti-agent%20manipulation%20problems.%20Experiments%20show%20that%20our%20method%20outperforms%0Aexisting%20constrained%20motion%20planners%20and%20operates%20at%20speeds%20suitable%20for%0Areal-world%20applications.%20Video%20demonstrations%20are%20available%20at%0Ahttps%3A//youtu.be/RYcEHMnPTH8%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03591v1&entry.124074799=Read"},
{"title": "Multi-User Personalisation in Human-Robot Interaction: Using\n  Quantitative Bipolar Argumentation Frameworks for Preferences Conflict\n  Resolution", "author": "Aniol Civit and Antonio Andriella and Carles Sierra and Guillem Aleny\u00e0", "abstract": "  While personalisation in Human-Robot Interaction (HRI) has advanced\nsignificantly, most existing approaches focus on single-user adaptation,\noverlooking scenarios involving multiple stakeholders with potentially\nconflicting preferences. To address this, we propose the Multi-User Preferences\nQuantitative Bipolar Argumentation Framework (MUP-QBAF), a novel multi-user\npersonalisation framework based on Quantitative Bipolar Argumentation\nFrameworks (QBAFs) that explicitly models and resolves multi-user preference\nconflicts. Unlike prior work in Argumentation Frameworks, which typically\nassumes static inputs, our approach is tailored to robotics: it incorporates\nboth users' arguments and the robot's dynamic observations of the environment,\nallowing the system to adapt over time and respond to changing contexts.\nPreferences, both positive and negative, are represented as arguments whose\nstrength is recalculated iteratively based on new information. The framework's\nproperties and capabilities are presented and validated through a realistic\ncase study, where an assistive robot mediates between the conflicting\npreferences of a caregiver and a care recipient during a frailty assessment\ntask. This evaluation further includes a sensitivity analysis of argument base\nscores, demonstrating how preference outcomes can be shaped by user input and\ncontextual observations. By offering a transparent, structured, and\ncontext-sensitive approach to resolving competing user preferences, this work\nadvances the field of multi-user HRI. It provides a principled alternative to\ndata-driven methods, enabling robots to navigate conflicts in real-world\nenvironments.\n", "link": "http://arxiv.org/abs/2511.03576v1", "date": "2025-11-05", "relevancy": 1.5677, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5529}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5224}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-User%20Personalisation%20in%20Human-Robot%20Interaction%3A%20Using%0A%20%20Quantitative%20Bipolar%20Argumentation%20Frameworks%20for%20Preferences%20Conflict%0A%20%20Resolution&body=Title%3A%20Multi-User%20Personalisation%20in%20Human-Robot%20Interaction%3A%20Using%0A%20%20Quantitative%20Bipolar%20Argumentation%20Frameworks%20for%20Preferences%20Conflict%0A%20%20Resolution%0AAuthor%3A%20Aniol%20Civit%20and%20Antonio%20Andriella%20and%20Carles%20Sierra%20and%20Guillem%20Aleny%C3%A0%0AAbstract%3A%20%20%20While%20personalisation%20in%20Human-Robot%20Interaction%20%28HRI%29%20has%20advanced%0Asignificantly%2C%20most%20existing%20approaches%20focus%20on%20single-user%20adaptation%2C%0Aoverlooking%20scenarios%20involving%20multiple%20stakeholders%20with%20potentially%0Aconflicting%20preferences.%20To%20address%20this%2C%20we%20propose%20the%20Multi-User%20Preferences%0AQuantitative%20Bipolar%20Argumentation%20Framework%20%28MUP-QBAF%29%2C%20a%20novel%20multi-user%0Apersonalisation%20framework%20based%20on%20Quantitative%20Bipolar%20Argumentation%0AFrameworks%20%28QBAFs%29%20that%20explicitly%20models%20and%20resolves%20multi-user%20preference%0Aconflicts.%20Unlike%20prior%20work%20in%20Argumentation%20Frameworks%2C%20which%20typically%0Aassumes%20static%20inputs%2C%20our%20approach%20is%20tailored%20to%20robotics%3A%20it%20incorporates%0Aboth%20users%27%20arguments%20and%20the%20robot%27s%20dynamic%20observations%20of%20the%20environment%2C%0Aallowing%20the%20system%20to%20adapt%20over%20time%20and%20respond%20to%20changing%20contexts.%0APreferences%2C%20both%20positive%20and%20negative%2C%20are%20represented%20as%20arguments%20whose%0Astrength%20is%20recalculated%20iteratively%20based%20on%20new%20information.%20The%20framework%27s%0Aproperties%20and%20capabilities%20are%20presented%20and%20validated%20through%20a%20realistic%0Acase%20study%2C%20where%20an%20assistive%20robot%20mediates%20between%20the%20conflicting%0Apreferences%20of%20a%20caregiver%20and%20a%20care%20recipient%20during%20a%20frailty%20assessment%0Atask.%20This%20evaluation%20further%20includes%20a%20sensitivity%20analysis%20of%20argument%20base%0Ascores%2C%20demonstrating%20how%20preference%20outcomes%20can%20be%20shaped%20by%20user%20input%20and%0Acontextual%20observations.%20By%20offering%20a%20transparent%2C%20structured%2C%20and%0Acontext-sensitive%20approach%20to%20resolving%20competing%20user%20preferences%2C%20this%20work%0Aadvances%20the%20field%20of%20multi-user%20HRI.%20It%20provides%20a%20principled%20alternative%20to%0Adata-driven%20methods%2C%20enabling%20robots%20to%20navigate%20conflicts%20in%20real-world%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03576v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-User%2520Personalisation%2520in%2520Human-Robot%2520Interaction%253A%2520Using%250A%2520%2520Quantitative%2520Bipolar%2520Argumentation%2520Frameworks%2520for%2520Preferences%2520Conflict%250A%2520%2520Resolution%26entry.906535625%3DAniol%2520Civit%2520and%2520Antonio%2520Andriella%2520and%2520Carles%2520Sierra%2520and%2520Guillem%2520Aleny%25C3%25A0%26entry.1292438233%3D%2520%2520While%2520personalisation%2520in%2520Human-Robot%2520Interaction%2520%2528HRI%2529%2520has%2520advanced%250Asignificantly%252C%2520most%2520existing%2520approaches%2520focus%2520on%2520single-user%2520adaptation%252C%250Aoverlooking%2520scenarios%2520involving%2520multiple%2520stakeholders%2520with%2520potentially%250Aconflicting%2520preferences.%2520To%2520address%2520this%252C%2520we%2520propose%2520the%2520Multi-User%2520Preferences%250AQuantitative%2520Bipolar%2520Argumentation%2520Framework%2520%2528MUP-QBAF%2529%252C%2520a%2520novel%2520multi-user%250Apersonalisation%2520framework%2520based%2520on%2520Quantitative%2520Bipolar%2520Argumentation%250AFrameworks%2520%2528QBAFs%2529%2520that%2520explicitly%2520models%2520and%2520resolves%2520multi-user%2520preference%250Aconflicts.%2520Unlike%2520prior%2520work%2520in%2520Argumentation%2520Frameworks%252C%2520which%2520typically%250Aassumes%2520static%2520inputs%252C%2520our%2520approach%2520is%2520tailored%2520to%2520robotics%253A%2520it%2520incorporates%250Aboth%2520users%2527%2520arguments%2520and%2520the%2520robot%2527s%2520dynamic%2520observations%2520of%2520the%2520environment%252C%250Aallowing%2520the%2520system%2520to%2520adapt%2520over%2520time%2520and%2520respond%2520to%2520changing%2520contexts.%250APreferences%252C%2520both%2520positive%2520and%2520negative%252C%2520are%2520represented%2520as%2520arguments%2520whose%250Astrength%2520is%2520recalculated%2520iteratively%2520based%2520on%2520new%2520information.%2520The%2520framework%2527s%250Aproperties%2520and%2520capabilities%2520are%2520presented%2520and%2520validated%2520through%2520a%2520realistic%250Acase%2520study%252C%2520where%2520an%2520assistive%2520robot%2520mediates%2520between%2520the%2520conflicting%250Apreferences%2520of%2520a%2520caregiver%2520and%2520a%2520care%2520recipient%2520during%2520a%2520frailty%2520assessment%250Atask.%2520This%2520evaluation%2520further%2520includes%2520a%2520sensitivity%2520analysis%2520of%2520argument%2520base%250Ascores%252C%2520demonstrating%2520how%2520preference%2520outcomes%2520can%2520be%2520shaped%2520by%2520user%2520input%2520and%250Acontextual%2520observations.%2520By%2520offering%2520a%2520transparent%252C%2520structured%252C%2520and%250Acontext-sensitive%2520approach%2520to%2520resolving%2520competing%2520user%2520preferences%252C%2520this%2520work%250Aadvances%2520the%2520field%2520of%2520multi-user%2520HRI.%2520It%2520provides%2520a%2520principled%2520alternative%2520to%250Adata-driven%2520methods%252C%2520enabling%2520robots%2520to%2520navigate%2520conflicts%2520in%2520real-world%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03576v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-User%20Personalisation%20in%20Human-Robot%20Interaction%3A%20Using%0A%20%20Quantitative%20Bipolar%20Argumentation%20Frameworks%20for%20Preferences%20Conflict%0A%20%20Resolution&entry.906535625=Aniol%20Civit%20and%20Antonio%20Andriella%20and%20Carles%20Sierra%20and%20Guillem%20Aleny%C3%A0&entry.1292438233=%20%20While%20personalisation%20in%20Human-Robot%20Interaction%20%28HRI%29%20has%20advanced%0Asignificantly%2C%20most%20existing%20approaches%20focus%20on%20single-user%20adaptation%2C%0Aoverlooking%20scenarios%20involving%20multiple%20stakeholders%20with%20potentially%0Aconflicting%20preferences.%20To%20address%20this%2C%20we%20propose%20the%20Multi-User%20Preferences%0AQuantitative%20Bipolar%20Argumentation%20Framework%20%28MUP-QBAF%29%2C%20a%20novel%20multi-user%0Apersonalisation%20framework%20based%20on%20Quantitative%20Bipolar%20Argumentation%0AFrameworks%20%28QBAFs%29%20that%20explicitly%20models%20and%20resolves%20multi-user%20preference%0Aconflicts.%20Unlike%20prior%20work%20in%20Argumentation%20Frameworks%2C%20which%20typically%0Aassumes%20static%20inputs%2C%20our%20approach%20is%20tailored%20to%20robotics%3A%20it%20incorporates%0Aboth%20users%27%20arguments%20and%20the%20robot%27s%20dynamic%20observations%20of%20the%20environment%2C%0Aallowing%20the%20system%20to%20adapt%20over%20time%20and%20respond%20to%20changing%20contexts.%0APreferences%2C%20both%20positive%20and%20negative%2C%20are%20represented%20as%20arguments%20whose%0Astrength%20is%20recalculated%20iteratively%20based%20on%20new%20information.%20The%20framework%27s%0Aproperties%20and%20capabilities%20are%20presented%20and%20validated%20through%20a%20realistic%0Acase%20study%2C%20where%20an%20assistive%20robot%20mediates%20between%20the%20conflicting%0Apreferences%20of%20a%20caregiver%20and%20a%20care%20recipient%20during%20a%20frailty%20assessment%0Atask.%20This%20evaluation%20further%20includes%20a%20sensitivity%20analysis%20of%20argument%20base%0Ascores%2C%20demonstrating%20how%20preference%20outcomes%20can%20be%20shaped%20by%20user%20input%20and%0Acontextual%20observations.%20By%20offering%20a%20transparent%2C%20structured%2C%20and%0Acontext-sensitive%20approach%20to%20resolving%20competing%20user%20preferences%2C%20this%20work%0Aadvances%20the%20field%20of%20multi-user%20HRI.%20It%20provides%20a%20principled%20alternative%20to%0Adata-driven%20methods%2C%20enabling%20robots%20to%20navigate%20conflicts%20in%20real-world%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03576v1&entry.124074799=Read"},
{"title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large\n  Model Token Routing", "author": "Tianyu Fu and Yi Ge and Yichen You and Enshu Liu and Zhihang Yuan and Guohao Dai and Shengen Yan and Huazhong Yang and Yu Wang", "abstract": "  Large Language Models (LLMs) achieve impressive reasoning capabilities at the\ncost of substantial inference overhead, posing substantial deployment\nchallenges. Although distilled Small Language Models (SLMs) significantly\nenhance efficiency, their performance suffers as they fail to follow LLMs'\nreasoning paths. Luckily, we reveal that only a small fraction of tokens\ngenuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens\nare either identical or exhibit neutral differences, such as minor variations\nin abbreviations or expressions. Leveraging this insight, we introduce **Roads\nto Rome (R2R)**, a neural token routing method that selectively utilizes LLMs\nonly for these critical, path-divergent tokens, while leaving the majority of\ntoken generation to the SLM. We also develop an automatic data generation\npipeline that identifies divergent tokens and generates token-level routing\nlabels to train the lightweight router. We apply R2R to combine R1-1.5B and\nR1-32B models from the DeepSeek family, and evaluate on challenging math,\ncoding, and QA benchmarks. With an average activated parameter size of 5.6B,\nR2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the\nR1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with\ncomparable performance, advancing the Pareto frontier of test-time scaling\nefficiency. Our code is available at https://github.com/thu-nics/R2R.\n", "link": "http://arxiv.org/abs/2505.21600v2", "date": "2025-11-05", "relevancy": 1.5635, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.535}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5122}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20R2R%3A%20Efficiently%20Navigating%20Divergent%20Reasoning%20Paths%20with%20Small-Large%0A%20%20Model%20Token%20Routing&body=Title%3A%20R2R%3A%20Efficiently%20Navigating%20Divergent%20Reasoning%20Paths%20with%20Small-Large%0A%20%20Model%20Token%20Routing%0AAuthor%3A%20Tianyu%20Fu%20and%20Yi%20Ge%20and%20Yichen%20You%20and%20Enshu%20Liu%20and%20Zhihang%20Yuan%20and%20Guohao%20Dai%20and%20Shengen%20Yan%20and%20Huazhong%20Yang%20and%20Yu%20Wang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20achieve%20impressive%20reasoning%20capabilities%20at%20the%0Acost%20of%20substantial%20inference%20overhead%2C%20posing%20substantial%20deployment%0Achallenges.%20Although%20distilled%20Small%20Language%20Models%20%28SLMs%29%20significantly%0Aenhance%20efficiency%2C%20their%20performance%20suffers%20as%20they%20fail%20to%20follow%20LLMs%27%0Areasoning%20paths.%20Luckily%2C%20we%20reveal%20that%20only%20a%20small%20fraction%20of%20tokens%0Agenuinely%20diverge%20reasoning%20paths%20between%20LLMs%20and%20SLMs.%20Most%20generated%20tokens%0Aare%20either%20identical%20or%20exhibit%20neutral%20differences%2C%20such%20as%20minor%20variations%0Ain%20abbreviations%20or%20expressions.%20Leveraging%20this%20insight%2C%20we%20introduce%20%2A%2ARoads%0Ato%20Rome%20%28R2R%29%2A%2A%2C%20a%20neural%20token%20routing%20method%20that%20selectively%20utilizes%20LLMs%0Aonly%20for%20these%20critical%2C%20path-divergent%20tokens%2C%20while%20leaving%20the%20majority%20of%0Atoken%20generation%20to%20the%20SLM.%20We%20also%20develop%20an%20automatic%20data%20generation%0Apipeline%20that%20identifies%20divergent%20tokens%20and%20generates%20token-level%20routing%0Alabels%20to%20train%20the%20lightweight%20router.%20We%20apply%20R2R%20to%20combine%20R1-1.5B%20and%0AR1-32B%20models%20from%20the%20DeepSeek%20family%2C%20and%20evaluate%20on%20challenging%20math%2C%0Acoding%2C%20and%20QA%20benchmarks.%20With%20an%20average%20activated%20parameter%20size%20of%205.6B%2C%0AR2R%20surpasses%20the%20average%20accuracy%20of%20R1-7B%20by%201.6x%2C%20outperforming%20even%20the%0AR1-14B%20model.%20Compared%20to%20R1-32B%2C%20it%20delivers%20a%202.8x%20wall-clock%20speedup%20with%0Acomparable%20performance%2C%20advancing%20the%20Pareto%20frontier%20of%20test-time%20scaling%0Aefficiency.%20Our%20code%20is%20available%20at%20https%3A//github.com/thu-nics/R2R.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21600v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DR2R%253A%2520Efficiently%2520Navigating%2520Divergent%2520Reasoning%2520Paths%2520with%2520Small-Large%250A%2520%2520Model%2520Token%2520Routing%26entry.906535625%3DTianyu%2520Fu%2520and%2520Yi%2520Ge%2520and%2520Yichen%2520You%2520and%2520Enshu%2520Liu%2520and%2520Zhihang%2520Yuan%2520and%2520Guohao%2520Dai%2520and%2520Shengen%2520Yan%2520and%2520Huazhong%2520Yang%2520and%2520Yu%2520Wang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520achieve%2520impressive%2520reasoning%2520capabilities%2520at%2520the%250Acost%2520of%2520substantial%2520inference%2520overhead%252C%2520posing%2520substantial%2520deployment%250Achallenges.%2520Although%2520distilled%2520Small%2520Language%2520Models%2520%2528SLMs%2529%2520significantly%250Aenhance%2520efficiency%252C%2520their%2520performance%2520suffers%2520as%2520they%2520fail%2520to%2520follow%2520LLMs%2527%250Areasoning%2520paths.%2520Luckily%252C%2520we%2520reveal%2520that%2520only%2520a%2520small%2520fraction%2520of%2520tokens%250Agenuinely%2520diverge%2520reasoning%2520paths%2520between%2520LLMs%2520and%2520SLMs.%2520Most%2520generated%2520tokens%250Aare%2520either%2520identical%2520or%2520exhibit%2520neutral%2520differences%252C%2520such%2520as%2520minor%2520variations%250Ain%2520abbreviations%2520or%2520expressions.%2520Leveraging%2520this%2520insight%252C%2520we%2520introduce%2520%252A%252ARoads%250Ato%2520Rome%2520%2528R2R%2529%252A%252A%252C%2520a%2520neural%2520token%2520routing%2520method%2520that%2520selectively%2520utilizes%2520LLMs%250Aonly%2520for%2520these%2520critical%252C%2520path-divergent%2520tokens%252C%2520while%2520leaving%2520the%2520majority%2520of%250Atoken%2520generation%2520to%2520the%2520SLM.%2520We%2520also%2520develop%2520an%2520automatic%2520data%2520generation%250Apipeline%2520that%2520identifies%2520divergent%2520tokens%2520and%2520generates%2520token-level%2520routing%250Alabels%2520to%2520train%2520the%2520lightweight%2520router.%2520We%2520apply%2520R2R%2520to%2520combine%2520R1-1.5B%2520and%250AR1-32B%2520models%2520from%2520the%2520DeepSeek%2520family%252C%2520and%2520evaluate%2520on%2520challenging%2520math%252C%250Acoding%252C%2520and%2520QA%2520benchmarks.%2520With%2520an%2520average%2520activated%2520parameter%2520size%2520of%25205.6B%252C%250AR2R%2520surpasses%2520the%2520average%2520accuracy%2520of%2520R1-7B%2520by%25201.6x%252C%2520outperforming%2520even%2520the%250AR1-14B%2520model.%2520Compared%2520to%2520R1-32B%252C%2520it%2520delivers%2520a%25202.8x%2520wall-clock%2520speedup%2520with%250Acomparable%2520performance%252C%2520advancing%2520the%2520Pareto%2520frontier%2520of%2520test-time%2520scaling%250Aefficiency.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/thu-nics/R2R.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21600v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=R2R%3A%20Efficiently%20Navigating%20Divergent%20Reasoning%20Paths%20with%20Small-Large%0A%20%20Model%20Token%20Routing&entry.906535625=Tianyu%20Fu%20and%20Yi%20Ge%20and%20Yichen%20You%20and%20Enshu%20Liu%20and%20Zhihang%20Yuan%20and%20Guohao%20Dai%20and%20Shengen%20Yan%20and%20Huazhong%20Yang%20and%20Yu%20Wang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20achieve%20impressive%20reasoning%20capabilities%20at%20the%0Acost%20of%20substantial%20inference%20overhead%2C%20posing%20substantial%20deployment%0Achallenges.%20Although%20distilled%20Small%20Language%20Models%20%28SLMs%29%20significantly%0Aenhance%20efficiency%2C%20their%20performance%20suffers%20as%20they%20fail%20to%20follow%20LLMs%27%0Areasoning%20paths.%20Luckily%2C%20we%20reveal%20that%20only%20a%20small%20fraction%20of%20tokens%0Agenuinely%20diverge%20reasoning%20paths%20between%20LLMs%20and%20SLMs.%20Most%20generated%20tokens%0Aare%20either%20identical%20or%20exhibit%20neutral%20differences%2C%20such%20as%20minor%20variations%0Ain%20abbreviations%20or%20expressions.%20Leveraging%20this%20insight%2C%20we%20introduce%20%2A%2ARoads%0Ato%20Rome%20%28R2R%29%2A%2A%2C%20a%20neural%20token%20routing%20method%20that%20selectively%20utilizes%20LLMs%0Aonly%20for%20these%20critical%2C%20path-divergent%20tokens%2C%20while%20leaving%20the%20majority%20of%0Atoken%20generation%20to%20the%20SLM.%20We%20also%20develop%20an%20automatic%20data%20generation%0Apipeline%20that%20identifies%20divergent%20tokens%20and%20generates%20token-level%20routing%0Alabels%20to%20train%20the%20lightweight%20router.%20We%20apply%20R2R%20to%20combine%20R1-1.5B%20and%0AR1-32B%20models%20from%20the%20DeepSeek%20family%2C%20and%20evaluate%20on%20challenging%20math%2C%0Acoding%2C%20and%20QA%20benchmarks.%20With%20an%20average%20activated%20parameter%20size%20of%205.6B%2C%0AR2R%20surpasses%20the%20average%20accuracy%20of%20R1-7B%20by%201.6x%2C%20outperforming%20even%20the%0AR1-14B%20model.%20Compared%20to%20R1-32B%2C%20it%20delivers%20a%202.8x%20wall-clock%20speedup%20with%0Acomparable%20performance%2C%20advancing%20the%20Pareto%20frontier%20of%20test-time%20scaling%0Aefficiency.%20Our%20code%20is%20available%20at%20https%3A//github.com/thu-nics/R2R.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21600v2&entry.124074799=Read"},
{"title": "Autonomous Robotic Drilling System for Mice Cranial Window Creation", "author": "Enduo Zhao and Murilo M. Marinho and Kanako Harada", "abstract": "  Robotic assistance for experimental manipulation in the life sciences is\nexpected to enable favorable outcomes, regardless of the skill of the\nscientist. Experimental specimens in the life sciences are subject to\nindividual variability and hence require intricate algorithms for successful\nautonomous robotic control. As a use case, we are studying the cranial window\ncreation in mice. This operation requires the removal of an 8-mm circular patch\nof the skull, which is approximately 300 um thick, but the shape and thickness\nof the mouse skull significantly varies depending on the strain of the mouse,\nsex, and age. In this work, we develop an autonomous robotic drilling system\nwith no offline planning, consisting of a trajectory planner with\nexecution-time feedback with drilling completion level recognition based on\nimage and force information. In the experiments, we first evaluate the\nimage-and-force-based drilling completion level recognition by comparing it\nwith other state-of-the-art deep learning image processing methods and conduct\nan ablation study in eggshell drilling to evaluate the impact of each module on\nsystem performance. Finally, the system performance is further evaluated in\npostmortem mice, achieving a success rate of 70% (14/20 trials) with an average\ndrilling time of 9.3 min.\n", "link": "http://arxiv.org/abs/2406.14135v2", "date": "2025-11-05", "relevancy": 1.5439, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5587}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5037}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autonomous%20Robotic%20Drilling%20System%20for%20Mice%20Cranial%20Window%20Creation&body=Title%3A%20Autonomous%20Robotic%20Drilling%20System%20for%20Mice%20Cranial%20Window%20Creation%0AAuthor%3A%20Enduo%20Zhao%20and%20Murilo%20M.%20Marinho%20and%20Kanako%20Harada%0AAbstract%3A%20%20%20Robotic%20assistance%20for%20experimental%20manipulation%20in%20the%20life%20sciences%20is%0Aexpected%20to%20enable%20favorable%20outcomes%2C%20regardless%20of%20the%20skill%20of%20the%0Ascientist.%20Experimental%20specimens%20in%20the%20life%20sciences%20are%20subject%20to%0Aindividual%20variability%20and%20hence%20require%20intricate%20algorithms%20for%20successful%0Aautonomous%20robotic%20control.%20As%20a%20use%20case%2C%20we%20are%20studying%20the%20cranial%20window%0Acreation%20in%20mice.%20This%20operation%20requires%20the%20removal%20of%20an%208-mm%20circular%20patch%0Aof%20the%20skull%2C%20which%20is%20approximately%20300%20um%20thick%2C%20but%20the%20shape%20and%20thickness%0Aof%20the%20mouse%20skull%20significantly%20varies%20depending%20on%20the%20strain%20of%20the%20mouse%2C%0Asex%2C%20and%20age.%20In%20this%20work%2C%20we%20develop%20an%20autonomous%20robotic%20drilling%20system%0Awith%20no%20offline%20planning%2C%20consisting%20of%20a%20trajectory%20planner%20with%0Aexecution-time%20feedback%20with%20drilling%20completion%20level%20recognition%20based%20on%0Aimage%20and%20force%20information.%20In%20the%20experiments%2C%20we%20first%20evaluate%20the%0Aimage-and-force-based%20drilling%20completion%20level%20recognition%20by%20comparing%20it%0Awith%20other%20state-of-the-art%20deep%20learning%20image%20processing%20methods%20and%20conduct%0Aan%20ablation%20study%20in%20eggshell%20drilling%20to%20evaluate%20the%20impact%20of%20each%20module%20on%0Asystem%20performance.%20Finally%2C%20the%20system%20performance%20is%20further%20evaluated%20in%0Apostmortem%20mice%2C%20achieving%20a%20success%20rate%20of%2070%25%20%2814/20%20trials%29%20with%20an%20average%0Adrilling%20time%20of%209.3%20min.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14135v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutonomous%2520Robotic%2520Drilling%2520System%2520for%2520Mice%2520Cranial%2520Window%2520Creation%26entry.906535625%3DEnduo%2520Zhao%2520and%2520Murilo%2520M.%2520Marinho%2520and%2520Kanako%2520Harada%26entry.1292438233%3D%2520%2520Robotic%2520assistance%2520for%2520experimental%2520manipulation%2520in%2520the%2520life%2520sciences%2520is%250Aexpected%2520to%2520enable%2520favorable%2520outcomes%252C%2520regardless%2520of%2520the%2520skill%2520of%2520the%250Ascientist.%2520Experimental%2520specimens%2520in%2520the%2520life%2520sciences%2520are%2520subject%2520to%250Aindividual%2520variability%2520and%2520hence%2520require%2520intricate%2520algorithms%2520for%2520successful%250Aautonomous%2520robotic%2520control.%2520As%2520a%2520use%2520case%252C%2520we%2520are%2520studying%2520the%2520cranial%2520window%250Acreation%2520in%2520mice.%2520This%2520operation%2520requires%2520the%2520removal%2520of%2520an%25208-mm%2520circular%2520patch%250Aof%2520the%2520skull%252C%2520which%2520is%2520approximately%2520300%2520um%2520thick%252C%2520but%2520the%2520shape%2520and%2520thickness%250Aof%2520the%2520mouse%2520skull%2520significantly%2520varies%2520depending%2520on%2520the%2520strain%2520of%2520the%2520mouse%252C%250Asex%252C%2520and%2520age.%2520In%2520this%2520work%252C%2520we%2520develop%2520an%2520autonomous%2520robotic%2520drilling%2520system%250Awith%2520no%2520offline%2520planning%252C%2520consisting%2520of%2520a%2520trajectory%2520planner%2520with%250Aexecution-time%2520feedback%2520with%2520drilling%2520completion%2520level%2520recognition%2520based%2520on%250Aimage%2520and%2520force%2520information.%2520In%2520the%2520experiments%252C%2520we%2520first%2520evaluate%2520the%250Aimage-and-force-based%2520drilling%2520completion%2520level%2520recognition%2520by%2520comparing%2520it%250Awith%2520other%2520state-of-the-art%2520deep%2520learning%2520image%2520processing%2520methods%2520and%2520conduct%250Aan%2520ablation%2520study%2520in%2520eggshell%2520drilling%2520to%2520evaluate%2520the%2520impact%2520of%2520each%2520module%2520on%250Asystem%2520performance.%2520Finally%252C%2520the%2520system%2520performance%2520is%2520further%2520evaluated%2520in%250Apostmortem%2520mice%252C%2520achieving%2520a%2520success%2520rate%2520of%252070%2525%2520%252814/20%2520trials%2529%2520with%2520an%2520average%250Adrilling%2520time%2520of%25209.3%2520min.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14135v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20Robotic%20Drilling%20System%20for%20Mice%20Cranial%20Window%20Creation&entry.906535625=Enduo%20Zhao%20and%20Murilo%20M.%20Marinho%20and%20Kanako%20Harada&entry.1292438233=%20%20Robotic%20assistance%20for%20experimental%20manipulation%20in%20the%20life%20sciences%20is%0Aexpected%20to%20enable%20favorable%20outcomes%2C%20regardless%20of%20the%20skill%20of%20the%0Ascientist.%20Experimental%20specimens%20in%20the%20life%20sciences%20are%20subject%20to%0Aindividual%20variability%20and%20hence%20require%20intricate%20algorithms%20for%20successful%0Aautonomous%20robotic%20control.%20As%20a%20use%20case%2C%20we%20are%20studying%20the%20cranial%20window%0Acreation%20in%20mice.%20This%20operation%20requires%20the%20removal%20of%20an%208-mm%20circular%20patch%0Aof%20the%20skull%2C%20which%20is%20approximately%20300%20um%20thick%2C%20but%20the%20shape%20and%20thickness%0Aof%20the%20mouse%20skull%20significantly%20varies%20depending%20on%20the%20strain%20of%20the%20mouse%2C%0Asex%2C%20and%20age.%20In%20this%20work%2C%20we%20develop%20an%20autonomous%20robotic%20drilling%20system%0Awith%20no%20offline%20planning%2C%20consisting%20of%20a%20trajectory%20planner%20with%0Aexecution-time%20feedback%20with%20drilling%20completion%20level%20recognition%20based%20on%0Aimage%20and%20force%20information.%20In%20the%20experiments%2C%20we%20first%20evaluate%20the%0Aimage-and-force-based%20drilling%20completion%20level%20recognition%20by%20comparing%20it%0Awith%20other%20state-of-the-art%20deep%20learning%20image%20processing%20methods%20and%20conduct%0Aan%20ablation%20study%20in%20eggshell%20drilling%20to%20evaluate%20the%20impact%20of%20each%20module%20on%0Asystem%20performance.%20Finally%2C%20the%20system%20performance%20is%20further%20evaluated%20in%0Apostmortem%20mice%2C%20achieving%20a%20success%20rate%20of%2070%25%20%2814/20%20trials%29%20with%20an%20average%0Adrilling%20time%20of%209.3%20min.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14135v2&entry.124074799=Read"},
{"title": "Neural Beamforming with Doppler-Aware Sparse Attention for High Mobility\n  Environments", "author": "Cemil Vahapoglu and Timothy J. O'Shea and Wan Liu and Sennur Ulukus", "abstract": "  Beamforming has significance for enhancing spectral efficiency and mitigating\ninterference in multi-antenna wireless systems, facilitating spatial\nmultiplexing and diversity in dense and high mobility scenarios. Traditional\nbeamforming techniques such as zero-forcing beamforming (ZFBF) and minimum mean\nsquare error (MMSE) beamforming experience performance deterioration under\nadverse channel conditions. Deep learning-based beamforming offers an\nalternative with nonlinear mappings from channel state information (CSI) to\nbeamforming weights by improving robustness against dynamic channel\nenvironments. Transformer-based models are particularly effective due to their\nability to model long-range dependencies across time and frequency. However,\ntheir quadratic attention complexity limits scalability in large OFDM grids.\nRecent studies address this issue through sparse attention mechanisms that\nreduce complexity while maintaining expressiveness, yet often employ patterns\nthat disregard channel dynamics, as they are not specifically designed for\nwireless communication scenarios. In this work, we propose a Doppler-aware\nSparse Neural Network Beamforming (Doppler-aware Sparse NNBF) model that\nincorporates a channel-adaptive sparse attention mechanism in a multi-user\nsingle-input multiple-output (MU-SIMO) setting. The proposed sparsity structure\nis configurable along 2D time-frequency axes based on channel dynamics and is\ntheoretically proven to ensure full connectivity within p hops, where p is the\nnumber of attention heads. Simulation results under urban macro (UMa) channel\nconditions show that Doppler-aware Sparse NNBF significantly outperforms both a\nfixed-pattern baseline, referred to as Standard Sparse NNBF, and conventional\nbeamforming techniques ZFBF and MMSE beamforming in high mobility scenarios,\nwhile maintaining structured sparsity with a controlled number of attended keys\nper query.\n", "link": "http://arxiv.org/abs/2511.03632v1", "date": "2025-11-05", "relevancy": 1.5176, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5134}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4988}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Beamforming%20with%20Doppler-Aware%20Sparse%20Attention%20for%20High%20Mobility%0A%20%20Environments&body=Title%3A%20Neural%20Beamforming%20with%20Doppler-Aware%20Sparse%20Attention%20for%20High%20Mobility%0A%20%20Environments%0AAuthor%3A%20Cemil%20Vahapoglu%20and%20Timothy%20J.%20O%27Shea%20and%20Wan%20Liu%20and%20Sennur%20Ulukus%0AAbstract%3A%20%20%20Beamforming%20has%20significance%20for%20enhancing%20spectral%20efficiency%20and%20mitigating%0Ainterference%20in%20multi-antenna%20wireless%20systems%2C%20facilitating%20spatial%0Amultiplexing%20and%20diversity%20in%20dense%20and%20high%20mobility%20scenarios.%20Traditional%0Abeamforming%20techniques%20such%20as%20zero-forcing%20beamforming%20%28ZFBF%29%20and%20minimum%20mean%0Asquare%20error%20%28MMSE%29%20beamforming%20experience%20performance%20deterioration%20under%0Aadverse%20channel%20conditions.%20Deep%20learning-based%20beamforming%20offers%20an%0Aalternative%20with%20nonlinear%20mappings%20from%20channel%20state%20information%20%28CSI%29%20to%0Abeamforming%20weights%20by%20improving%20robustness%20against%20dynamic%20channel%0Aenvironments.%20Transformer-based%20models%20are%20particularly%20effective%20due%20to%20their%0Aability%20to%20model%20long-range%20dependencies%20across%20time%20and%20frequency.%20However%2C%0Atheir%20quadratic%20attention%20complexity%20limits%20scalability%20in%20large%20OFDM%20grids.%0ARecent%20studies%20address%20this%20issue%20through%20sparse%20attention%20mechanisms%20that%0Areduce%20complexity%20while%20maintaining%20expressiveness%2C%20yet%20often%20employ%20patterns%0Athat%20disregard%20channel%20dynamics%2C%20as%20they%20are%20not%20specifically%20designed%20for%0Awireless%20communication%20scenarios.%20In%20this%20work%2C%20we%20propose%20a%20Doppler-aware%0ASparse%20Neural%20Network%20Beamforming%20%28Doppler-aware%20Sparse%20NNBF%29%20model%20that%0Aincorporates%20a%20channel-adaptive%20sparse%20attention%20mechanism%20in%20a%20multi-user%0Asingle-input%20multiple-output%20%28MU-SIMO%29%20setting.%20The%20proposed%20sparsity%20structure%0Ais%20configurable%20along%202D%20time-frequency%20axes%20based%20on%20channel%20dynamics%20and%20is%0Atheoretically%20proven%20to%20ensure%20full%20connectivity%20within%20p%20hops%2C%20where%20p%20is%20the%0Anumber%20of%20attention%20heads.%20Simulation%20results%20under%20urban%20macro%20%28UMa%29%20channel%0Aconditions%20show%20that%20Doppler-aware%20Sparse%20NNBF%20significantly%20outperforms%20both%20a%0Afixed-pattern%20baseline%2C%20referred%20to%20as%20Standard%20Sparse%20NNBF%2C%20and%20conventional%0Abeamforming%20techniques%20ZFBF%20and%20MMSE%20beamforming%20in%20high%20mobility%20scenarios%2C%0Awhile%20maintaining%20structured%20sparsity%20with%20a%20controlled%20number%20of%20attended%20keys%0Aper%20query.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03632v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Beamforming%2520with%2520Doppler-Aware%2520Sparse%2520Attention%2520for%2520High%2520Mobility%250A%2520%2520Environments%26entry.906535625%3DCemil%2520Vahapoglu%2520and%2520Timothy%2520J.%2520O%2527Shea%2520and%2520Wan%2520Liu%2520and%2520Sennur%2520Ulukus%26entry.1292438233%3D%2520%2520Beamforming%2520has%2520significance%2520for%2520enhancing%2520spectral%2520efficiency%2520and%2520mitigating%250Ainterference%2520in%2520multi-antenna%2520wireless%2520systems%252C%2520facilitating%2520spatial%250Amultiplexing%2520and%2520diversity%2520in%2520dense%2520and%2520high%2520mobility%2520scenarios.%2520Traditional%250Abeamforming%2520techniques%2520such%2520as%2520zero-forcing%2520beamforming%2520%2528ZFBF%2529%2520and%2520minimum%2520mean%250Asquare%2520error%2520%2528MMSE%2529%2520beamforming%2520experience%2520performance%2520deterioration%2520under%250Aadverse%2520channel%2520conditions.%2520Deep%2520learning-based%2520beamforming%2520offers%2520an%250Aalternative%2520with%2520nonlinear%2520mappings%2520from%2520channel%2520state%2520information%2520%2528CSI%2529%2520to%250Abeamforming%2520weights%2520by%2520improving%2520robustness%2520against%2520dynamic%2520channel%250Aenvironments.%2520Transformer-based%2520models%2520are%2520particularly%2520effective%2520due%2520to%2520their%250Aability%2520to%2520model%2520long-range%2520dependencies%2520across%2520time%2520and%2520frequency.%2520However%252C%250Atheir%2520quadratic%2520attention%2520complexity%2520limits%2520scalability%2520in%2520large%2520OFDM%2520grids.%250ARecent%2520studies%2520address%2520this%2520issue%2520through%2520sparse%2520attention%2520mechanisms%2520that%250Areduce%2520complexity%2520while%2520maintaining%2520expressiveness%252C%2520yet%2520often%2520employ%2520patterns%250Athat%2520disregard%2520channel%2520dynamics%252C%2520as%2520they%2520are%2520not%2520specifically%2520designed%2520for%250Awireless%2520communication%2520scenarios.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520Doppler-aware%250ASparse%2520Neural%2520Network%2520Beamforming%2520%2528Doppler-aware%2520Sparse%2520NNBF%2529%2520model%2520that%250Aincorporates%2520a%2520channel-adaptive%2520sparse%2520attention%2520mechanism%2520in%2520a%2520multi-user%250Asingle-input%2520multiple-output%2520%2528MU-SIMO%2529%2520setting.%2520The%2520proposed%2520sparsity%2520structure%250Ais%2520configurable%2520along%25202D%2520time-frequency%2520axes%2520based%2520on%2520channel%2520dynamics%2520and%2520is%250Atheoretically%2520proven%2520to%2520ensure%2520full%2520connectivity%2520within%2520p%2520hops%252C%2520where%2520p%2520is%2520the%250Anumber%2520of%2520attention%2520heads.%2520Simulation%2520results%2520under%2520urban%2520macro%2520%2528UMa%2529%2520channel%250Aconditions%2520show%2520that%2520Doppler-aware%2520Sparse%2520NNBF%2520significantly%2520outperforms%2520both%2520a%250Afixed-pattern%2520baseline%252C%2520referred%2520to%2520as%2520Standard%2520Sparse%2520NNBF%252C%2520and%2520conventional%250Abeamforming%2520techniques%2520ZFBF%2520and%2520MMSE%2520beamforming%2520in%2520high%2520mobility%2520scenarios%252C%250Awhile%2520maintaining%2520structured%2520sparsity%2520with%2520a%2520controlled%2520number%2520of%2520attended%2520keys%250Aper%2520query.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03632v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Beamforming%20with%20Doppler-Aware%20Sparse%20Attention%20for%20High%20Mobility%0A%20%20Environments&entry.906535625=Cemil%20Vahapoglu%20and%20Timothy%20J.%20O%27Shea%20and%20Wan%20Liu%20and%20Sennur%20Ulukus&entry.1292438233=%20%20Beamforming%20has%20significance%20for%20enhancing%20spectral%20efficiency%20and%20mitigating%0Ainterference%20in%20multi-antenna%20wireless%20systems%2C%20facilitating%20spatial%0Amultiplexing%20and%20diversity%20in%20dense%20and%20high%20mobility%20scenarios.%20Traditional%0Abeamforming%20techniques%20such%20as%20zero-forcing%20beamforming%20%28ZFBF%29%20and%20minimum%20mean%0Asquare%20error%20%28MMSE%29%20beamforming%20experience%20performance%20deterioration%20under%0Aadverse%20channel%20conditions.%20Deep%20learning-based%20beamforming%20offers%20an%0Aalternative%20with%20nonlinear%20mappings%20from%20channel%20state%20information%20%28CSI%29%20to%0Abeamforming%20weights%20by%20improving%20robustness%20against%20dynamic%20channel%0Aenvironments.%20Transformer-based%20models%20are%20particularly%20effective%20due%20to%20their%0Aability%20to%20model%20long-range%20dependencies%20across%20time%20and%20frequency.%20However%2C%0Atheir%20quadratic%20attention%20complexity%20limits%20scalability%20in%20large%20OFDM%20grids.%0ARecent%20studies%20address%20this%20issue%20through%20sparse%20attention%20mechanisms%20that%0Areduce%20complexity%20while%20maintaining%20expressiveness%2C%20yet%20often%20employ%20patterns%0Athat%20disregard%20channel%20dynamics%2C%20as%20they%20are%20not%20specifically%20designed%20for%0Awireless%20communication%20scenarios.%20In%20this%20work%2C%20we%20propose%20a%20Doppler-aware%0ASparse%20Neural%20Network%20Beamforming%20%28Doppler-aware%20Sparse%20NNBF%29%20model%20that%0Aincorporates%20a%20channel-adaptive%20sparse%20attention%20mechanism%20in%20a%20multi-user%0Asingle-input%20multiple-output%20%28MU-SIMO%29%20setting.%20The%20proposed%20sparsity%20structure%0Ais%20configurable%20along%202D%20time-frequency%20axes%20based%20on%20channel%20dynamics%20and%20is%0Atheoretically%20proven%20to%20ensure%20full%20connectivity%20within%20p%20hops%2C%20where%20p%20is%20the%0Anumber%20of%20attention%20heads.%20Simulation%20results%20under%20urban%20macro%20%28UMa%29%20channel%0Aconditions%20show%20that%20Doppler-aware%20Sparse%20NNBF%20significantly%20outperforms%20both%20a%0Afixed-pattern%20baseline%2C%20referred%20to%20as%20Standard%20Sparse%20NNBF%2C%20and%20conventional%0Abeamforming%20techniques%20ZFBF%20and%20MMSE%20beamforming%20in%20high%20mobility%20scenarios%2C%0Awhile%20maintaining%20structured%20sparsity%20with%20a%20controlled%20number%20of%20attended%20keys%0Aper%20query.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03632v1&entry.124074799=Read"},
{"title": "Efficient Testing Implies Structured Symmetry", "author": "Cynthia Dwork and Pranay Tankala", "abstract": "  Given a small random sample of $n$-bit strings labeled by an unknown Boolean\nfunction, which properties of this function can be tested computationally\nefficiently? We show an equivalence between properties that are efficiently\ntestable from few samples and properties with structured symmetry, which depend\nonly on the function's average values on parts of a low-complexity partition of\nthe domain. Without the efficiency constraint, a similar characterization in\nterms of unstructured symmetry was obtained by Blais and Yoshida (2019). Our\nmain technical tool is supersimulation, which builds on methods from the\nalgorithmic fairness literature to approximate arbitrarily complex functions by\nsmall-circuit simulators that fool significantly larger distinguishers.\n  We extend the characterization along other axes as well. We show that\nallowing parts to overlap exponentially reduces their required number,\nbroadening the scope of the construction from properties testable with $O(\\log\nn)$ samples to properties testable with $O(n)$ samples. For larger sample\nsizes, we show that any efficient tester is essentially checking for\nindistinguishability from a bounded collection of small circuits, in the spirit\nof a characterization of testable graph properties. Finally, we show that our\nresults for Boolean function testing generalize to high-entropy distribution\ntesting on arbitrary domains.\n", "link": "http://arxiv.org/abs/2511.03653v1", "date": "2025-11-05", "relevancy": 1.5063, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3899}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3675}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Testing%20Implies%20Structured%20Symmetry&body=Title%3A%20Efficient%20Testing%20Implies%20Structured%20Symmetry%0AAuthor%3A%20Cynthia%20Dwork%20and%20Pranay%20Tankala%0AAbstract%3A%20%20%20Given%20a%20small%20random%20sample%20of%20%24n%24-bit%20strings%20labeled%20by%20an%20unknown%20Boolean%0Afunction%2C%20which%20properties%20of%20this%20function%20can%20be%20tested%20computationally%0Aefficiently%3F%20We%20show%20an%20equivalence%20between%20properties%20that%20are%20efficiently%0Atestable%20from%20few%20samples%20and%20properties%20with%20structured%20symmetry%2C%20which%20depend%0Aonly%20on%20the%20function%27s%20average%20values%20on%20parts%20of%20a%20low-complexity%20partition%20of%0Athe%20domain.%20Without%20the%20efficiency%20constraint%2C%20a%20similar%20characterization%20in%0Aterms%20of%20unstructured%20symmetry%20was%20obtained%20by%20Blais%20and%20Yoshida%20%282019%29.%20Our%0Amain%20technical%20tool%20is%20supersimulation%2C%20which%20builds%20on%20methods%20from%20the%0Aalgorithmic%20fairness%20literature%20to%20approximate%20arbitrarily%20complex%20functions%20by%0Asmall-circuit%20simulators%20that%20fool%20significantly%20larger%20distinguishers.%0A%20%20We%20extend%20the%20characterization%20along%20other%20axes%20as%20well.%20We%20show%20that%0Aallowing%20parts%20to%20overlap%20exponentially%20reduces%20their%20required%20number%2C%0Abroadening%20the%20scope%20of%20the%20construction%20from%20properties%20testable%20with%20%24O%28%5Clog%0An%29%24%20samples%20to%20properties%20testable%20with%20%24O%28n%29%24%20samples.%20For%20larger%20sample%0Asizes%2C%20we%20show%20that%20any%20efficient%20tester%20is%20essentially%20checking%20for%0Aindistinguishability%20from%20a%20bounded%20collection%20of%20small%20circuits%2C%20in%20the%20spirit%0Aof%20a%20characterization%20of%20testable%20graph%20properties.%20Finally%2C%20we%20show%20that%20our%0Aresults%20for%20Boolean%20function%20testing%20generalize%20to%20high-entropy%20distribution%0Atesting%20on%20arbitrary%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03653v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Testing%2520Implies%2520Structured%2520Symmetry%26entry.906535625%3DCynthia%2520Dwork%2520and%2520Pranay%2520Tankala%26entry.1292438233%3D%2520%2520Given%2520a%2520small%2520random%2520sample%2520of%2520%2524n%2524-bit%2520strings%2520labeled%2520by%2520an%2520unknown%2520Boolean%250Afunction%252C%2520which%2520properties%2520of%2520this%2520function%2520can%2520be%2520tested%2520computationally%250Aefficiently%253F%2520We%2520show%2520an%2520equivalence%2520between%2520properties%2520that%2520are%2520efficiently%250Atestable%2520from%2520few%2520samples%2520and%2520properties%2520with%2520structured%2520symmetry%252C%2520which%2520depend%250Aonly%2520on%2520the%2520function%2527s%2520average%2520values%2520on%2520parts%2520of%2520a%2520low-complexity%2520partition%2520of%250Athe%2520domain.%2520Without%2520the%2520efficiency%2520constraint%252C%2520a%2520similar%2520characterization%2520in%250Aterms%2520of%2520unstructured%2520symmetry%2520was%2520obtained%2520by%2520Blais%2520and%2520Yoshida%2520%25282019%2529.%2520Our%250Amain%2520technical%2520tool%2520is%2520supersimulation%252C%2520which%2520builds%2520on%2520methods%2520from%2520the%250Aalgorithmic%2520fairness%2520literature%2520to%2520approximate%2520arbitrarily%2520complex%2520functions%2520by%250Asmall-circuit%2520simulators%2520that%2520fool%2520significantly%2520larger%2520distinguishers.%250A%2520%2520We%2520extend%2520the%2520characterization%2520along%2520other%2520axes%2520as%2520well.%2520We%2520show%2520that%250Aallowing%2520parts%2520to%2520overlap%2520exponentially%2520reduces%2520their%2520required%2520number%252C%250Abroadening%2520the%2520scope%2520of%2520the%2520construction%2520from%2520properties%2520testable%2520with%2520%2524O%2528%255Clog%250An%2529%2524%2520samples%2520to%2520properties%2520testable%2520with%2520%2524O%2528n%2529%2524%2520samples.%2520For%2520larger%2520sample%250Asizes%252C%2520we%2520show%2520that%2520any%2520efficient%2520tester%2520is%2520essentially%2520checking%2520for%250Aindistinguishability%2520from%2520a%2520bounded%2520collection%2520of%2520small%2520circuits%252C%2520in%2520the%2520spirit%250Aof%2520a%2520characterization%2520of%2520testable%2520graph%2520properties.%2520Finally%252C%2520we%2520show%2520that%2520our%250Aresults%2520for%2520Boolean%2520function%2520testing%2520generalize%2520to%2520high-entropy%2520distribution%250Atesting%2520on%2520arbitrary%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03653v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Testing%20Implies%20Structured%20Symmetry&entry.906535625=Cynthia%20Dwork%20and%20Pranay%20Tankala&entry.1292438233=%20%20Given%20a%20small%20random%20sample%20of%20%24n%24-bit%20strings%20labeled%20by%20an%20unknown%20Boolean%0Afunction%2C%20which%20properties%20of%20this%20function%20can%20be%20tested%20computationally%0Aefficiently%3F%20We%20show%20an%20equivalence%20between%20properties%20that%20are%20efficiently%0Atestable%20from%20few%20samples%20and%20properties%20with%20structured%20symmetry%2C%20which%20depend%0Aonly%20on%20the%20function%27s%20average%20values%20on%20parts%20of%20a%20low-complexity%20partition%20of%0Athe%20domain.%20Without%20the%20efficiency%20constraint%2C%20a%20similar%20characterization%20in%0Aterms%20of%20unstructured%20symmetry%20was%20obtained%20by%20Blais%20and%20Yoshida%20%282019%29.%20Our%0Amain%20technical%20tool%20is%20supersimulation%2C%20which%20builds%20on%20methods%20from%20the%0Aalgorithmic%20fairness%20literature%20to%20approximate%20arbitrarily%20complex%20functions%20by%0Asmall-circuit%20simulators%20that%20fool%20significantly%20larger%20distinguishers.%0A%20%20We%20extend%20the%20characterization%20along%20other%20axes%20as%20well.%20We%20show%20that%0Aallowing%20parts%20to%20overlap%20exponentially%20reduces%20their%20required%20number%2C%0Abroadening%20the%20scope%20of%20the%20construction%20from%20properties%20testable%20with%20%24O%28%5Clog%0An%29%24%20samples%20to%20properties%20testable%20with%20%24O%28n%29%24%20samples.%20For%20larger%20sample%0Asizes%2C%20we%20show%20that%20any%20efficient%20tester%20is%20essentially%20checking%20for%0Aindistinguishability%20from%20a%20bounded%20collection%20of%20small%20circuits%2C%20in%20the%20spirit%0Aof%20a%20characterization%20of%20testable%20graph%20properties.%20Finally%2C%20we%20show%20that%20our%0Aresults%20for%20Boolean%20function%20testing%20generalize%20to%20high-entropy%20distribution%0Atesting%20on%20arbitrary%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03653v1&entry.124074799=Read"},
{"title": "Data-Driven Probabilistic Air-Sea Flux Parameterization", "author": "Jiarong Wu and Pavel Perezhogin and David John Gagne and Brandon Reichl and Aneesh C. Subramanian and Elizabeth Thompson and Laure Zanna", "abstract": "  Accurately quantifying air-sea fluxes is important for understanding air-sea\ninteractions and improving coupled weather and climate systems. This study\nintroduces a probabilistic framework to represent the highly variable nature of\nair-sea fluxes, which is missing in deterministic bulk algorithms. Assuming\nGaussian distributions conditioned on the input variables, we use artificial\nneural networks and eddy-covariance measurement data to estimate the mean and\nvariance by minimizing negative log-likelihood loss. The trained neural\nnetworks provide alternative mean flux estimates to existing bulk algorithms,\nand quantify the uncertainty around the mean estimates. Stochastic\nparameterization of air-sea turbulent fluxes can be constructed by sampling\nfrom the predicted distributions. Tests in a single-column forced upper-ocean\nmodel suggest that changes in flux algorithms influence sea surface temperature\nand mixed layer depth seasonally. The ensemble spread in stochastic runs is\nmost pronounced during spring restratification.\n", "link": "http://arxiv.org/abs/2503.03990v2", "date": "2025-11-05", "relevancy": 1.4848, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5329}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4859}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4795}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Driven%20Probabilistic%20Air-Sea%20Flux%20Parameterization&body=Title%3A%20Data-Driven%20Probabilistic%20Air-Sea%20Flux%20Parameterization%0AAuthor%3A%20Jiarong%20Wu%20and%20Pavel%20Perezhogin%20and%20David%20John%20Gagne%20and%20Brandon%20Reichl%20and%20Aneesh%20C.%20Subramanian%20and%20Elizabeth%20Thompson%20and%20Laure%20Zanna%0AAbstract%3A%20%20%20Accurately%20quantifying%20air-sea%20fluxes%20is%20important%20for%20understanding%20air-sea%0Ainteractions%20and%20improving%20coupled%20weather%20and%20climate%20systems.%20This%20study%0Aintroduces%20a%20probabilistic%20framework%20to%20represent%20the%20highly%20variable%20nature%20of%0Aair-sea%20fluxes%2C%20which%20is%20missing%20in%20deterministic%20bulk%20algorithms.%20Assuming%0AGaussian%20distributions%20conditioned%20on%20the%20input%20variables%2C%20we%20use%20artificial%0Aneural%20networks%20and%20eddy-covariance%20measurement%20data%20to%20estimate%20the%20mean%20and%0Avariance%20by%20minimizing%20negative%20log-likelihood%20loss.%20The%20trained%20neural%0Anetworks%20provide%20alternative%20mean%20flux%20estimates%20to%20existing%20bulk%20algorithms%2C%0Aand%20quantify%20the%20uncertainty%20around%20the%20mean%20estimates.%20Stochastic%0Aparameterization%20of%20air-sea%20turbulent%20fluxes%20can%20be%20constructed%20by%20sampling%0Afrom%20the%20predicted%20distributions.%20Tests%20in%20a%20single-column%20forced%20upper-ocean%0Amodel%20suggest%20that%20changes%20in%20flux%20algorithms%20influence%20sea%20surface%20temperature%0Aand%20mixed%20layer%20depth%20seasonally.%20The%20ensemble%20spread%20in%20stochastic%20runs%20is%0Amost%20pronounced%20during%20spring%20restratification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.03990v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Driven%2520Probabilistic%2520Air-Sea%2520Flux%2520Parameterization%26entry.906535625%3DJiarong%2520Wu%2520and%2520Pavel%2520Perezhogin%2520and%2520David%2520John%2520Gagne%2520and%2520Brandon%2520Reichl%2520and%2520Aneesh%2520C.%2520Subramanian%2520and%2520Elizabeth%2520Thompson%2520and%2520Laure%2520Zanna%26entry.1292438233%3D%2520%2520Accurately%2520quantifying%2520air-sea%2520fluxes%2520is%2520important%2520for%2520understanding%2520air-sea%250Ainteractions%2520and%2520improving%2520coupled%2520weather%2520and%2520climate%2520systems.%2520This%2520study%250Aintroduces%2520a%2520probabilistic%2520framework%2520to%2520represent%2520the%2520highly%2520variable%2520nature%2520of%250Aair-sea%2520fluxes%252C%2520which%2520is%2520missing%2520in%2520deterministic%2520bulk%2520algorithms.%2520Assuming%250AGaussian%2520distributions%2520conditioned%2520on%2520the%2520input%2520variables%252C%2520we%2520use%2520artificial%250Aneural%2520networks%2520and%2520eddy-covariance%2520measurement%2520data%2520to%2520estimate%2520the%2520mean%2520and%250Avariance%2520by%2520minimizing%2520negative%2520log-likelihood%2520loss.%2520The%2520trained%2520neural%250Anetworks%2520provide%2520alternative%2520mean%2520flux%2520estimates%2520to%2520existing%2520bulk%2520algorithms%252C%250Aand%2520quantify%2520the%2520uncertainty%2520around%2520the%2520mean%2520estimates.%2520Stochastic%250Aparameterization%2520of%2520air-sea%2520turbulent%2520fluxes%2520can%2520be%2520constructed%2520by%2520sampling%250Afrom%2520the%2520predicted%2520distributions.%2520Tests%2520in%2520a%2520single-column%2520forced%2520upper-ocean%250Amodel%2520suggest%2520that%2520changes%2520in%2520flux%2520algorithms%2520influence%2520sea%2520surface%2520temperature%250Aand%2520mixed%2520layer%2520depth%2520seasonally.%2520The%2520ensemble%2520spread%2520in%2520stochastic%2520runs%2520is%250Amost%2520pronounced%2520during%2520spring%2520restratification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.03990v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Driven%20Probabilistic%20Air-Sea%20Flux%20Parameterization&entry.906535625=Jiarong%20Wu%20and%20Pavel%20Perezhogin%20and%20David%20John%20Gagne%20and%20Brandon%20Reichl%20and%20Aneesh%20C.%20Subramanian%20and%20Elizabeth%20Thompson%20and%20Laure%20Zanna&entry.1292438233=%20%20Accurately%20quantifying%20air-sea%20fluxes%20is%20important%20for%20understanding%20air-sea%0Ainteractions%20and%20improving%20coupled%20weather%20and%20climate%20systems.%20This%20study%0Aintroduces%20a%20probabilistic%20framework%20to%20represent%20the%20highly%20variable%20nature%20of%0Aair-sea%20fluxes%2C%20which%20is%20missing%20in%20deterministic%20bulk%20algorithms.%20Assuming%0AGaussian%20distributions%20conditioned%20on%20the%20input%20variables%2C%20we%20use%20artificial%0Aneural%20networks%20and%20eddy-covariance%20measurement%20data%20to%20estimate%20the%20mean%20and%0Avariance%20by%20minimizing%20negative%20log-likelihood%20loss.%20The%20trained%20neural%0Anetworks%20provide%20alternative%20mean%20flux%20estimates%20to%20existing%20bulk%20algorithms%2C%0Aand%20quantify%20the%20uncertainty%20around%20the%20mean%20estimates.%20Stochastic%0Aparameterization%20of%20air-sea%20turbulent%20fluxes%20can%20be%20constructed%20by%20sampling%0Afrom%20the%20predicted%20distributions.%20Tests%20in%20a%20single-column%20forced%20upper-ocean%0Amodel%20suggest%20that%20changes%20in%20flux%20algorithms%20influence%20sea%20surface%20temperature%0Aand%20mixed%20layer%20depth%20seasonally.%20The%20ensemble%20spread%20in%20stochastic%20runs%20is%0Amost%20pronounced%20during%20spring%20restratification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.03990v2&entry.124074799=Read"},
{"title": "Multi-robot searching with limited sensing range for static and mobile\n  intruders", "author": "Swadhin Agrawal and Sujoy Bhore and Joseph S. B. Mitchell and P. B. Sujit and Aayush Gohil", "abstract": "  We consider the problem of searching for an intruder in a geometric domain by\nutilizing multiple search robots. The domain is a simply connected orthogonal\npolygon with edges parallel to the cartesian coordinate axes. Each robot has a\nlimited sensing capability. We study the problem for both static and mobile\nintruders. It turns out that the problem of finding an intruder is NP-hard,\neven for a stationary intruder. Given this intractability, we turn our\nattention towards developing efficient and robust algorithms, namely methods\nbased on space-filling curves, random search, and cooperative random search.\nMoreover, for each proposed algorithm, we evaluate the trade-off between the\nnumber of search robots and the time required for the robots to complete the\nsearch process while considering the geometric properties of the connected\northogonal search area.\n", "link": "http://arxiv.org/abs/2511.03622v1", "date": "2025-11-05", "relevancy": 1.4842, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5167}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5104}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-robot%20searching%20with%20limited%20sensing%20range%20for%20static%20and%20mobile%0A%20%20intruders&body=Title%3A%20Multi-robot%20searching%20with%20limited%20sensing%20range%20for%20static%20and%20mobile%0A%20%20intruders%0AAuthor%3A%20Swadhin%20Agrawal%20and%20Sujoy%20Bhore%20and%20Joseph%20S.%20B.%20Mitchell%20and%20P.%20B.%20Sujit%20and%20Aayush%20Gohil%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20searching%20for%20an%20intruder%20in%20a%20geometric%20domain%20by%0Autilizing%20multiple%20search%20robots.%20The%20domain%20is%20a%20simply%20connected%20orthogonal%0Apolygon%20with%20edges%20parallel%20to%20the%20cartesian%20coordinate%20axes.%20Each%20robot%20has%20a%0Alimited%20sensing%20capability.%20We%20study%20the%20problem%20for%20both%20static%20and%20mobile%0Aintruders.%20It%20turns%20out%20that%20the%20problem%20of%20finding%20an%20intruder%20is%20NP-hard%2C%0Aeven%20for%20a%20stationary%20intruder.%20Given%20this%20intractability%2C%20we%20turn%20our%0Aattention%20towards%20developing%20efficient%20and%20robust%20algorithms%2C%20namely%20methods%0Abased%20on%20space-filling%20curves%2C%20random%20search%2C%20and%20cooperative%20random%20search.%0AMoreover%2C%20for%20each%20proposed%20algorithm%2C%20we%20evaluate%20the%20trade-off%20between%20the%0Anumber%20of%20search%20robots%20and%20the%20time%20required%20for%20the%20robots%20to%20complete%20the%0Asearch%20process%20while%20considering%20the%20geometric%20properties%20of%20the%20connected%0Aorthogonal%20search%20area.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03622v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-robot%2520searching%2520with%2520limited%2520sensing%2520range%2520for%2520static%2520and%2520mobile%250A%2520%2520intruders%26entry.906535625%3DSwadhin%2520Agrawal%2520and%2520Sujoy%2520Bhore%2520and%2520Joseph%2520S.%2520B.%2520Mitchell%2520and%2520P.%2520B.%2520Sujit%2520and%2520Aayush%2520Gohil%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520searching%2520for%2520an%2520intruder%2520in%2520a%2520geometric%2520domain%2520by%250Autilizing%2520multiple%2520search%2520robots.%2520The%2520domain%2520is%2520a%2520simply%2520connected%2520orthogonal%250Apolygon%2520with%2520edges%2520parallel%2520to%2520the%2520cartesian%2520coordinate%2520axes.%2520Each%2520robot%2520has%2520a%250Alimited%2520sensing%2520capability.%2520We%2520study%2520the%2520problem%2520for%2520both%2520static%2520and%2520mobile%250Aintruders.%2520It%2520turns%2520out%2520that%2520the%2520problem%2520of%2520finding%2520an%2520intruder%2520is%2520NP-hard%252C%250Aeven%2520for%2520a%2520stationary%2520intruder.%2520Given%2520this%2520intractability%252C%2520we%2520turn%2520our%250Aattention%2520towards%2520developing%2520efficient%2520and%2520robust%2520algorithms%252C%2520namely%2520methods%250Abased%2520on%2520space-filling%2520curves%252C%2520random%2520search%252C%2520and%2520cooperative%2520random%2520search.%250AMoreover%252C%2520for%2520each%2520proposed%2520algorithm%252C%2520we%2520evaluate%2520the%2520trade-off%2520between%2520the%250Anumber%2520of%2520search%2520robots%2520and%2520the%2520time%2520required%2520for%2520the%2520robots%2520to%2520complete%2520the%250Asearch%2520process%2520while%2520considering%2520the%2520geometric%2520properties%2520of%2520the%2520connected%250Aorthogonal%2520search%2520area.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03622v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-robot%20searching%20with%20limited%20sensing%20range%20for%20static%20and%20mobile%0A%20%20intruders&entry.906535625=Swadhin%20Agrawal%20and%20Sujoy%20Bhore%20and%20Joseph%20S.%20B.%20Mitchell%20and%20P.%20B.%20Sujit%20and%20Aayush%20Gohil&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20searching%20for%20an%20intruder%20in%20a%20geometric%20domain%20by%0Autilizing%20multiple%20search%20robots.%20The%20domain%20is%20a%20simply%20connected%20orthogonal%0Apolygon%20with%20edges%20parallel%20to%20the%20cartesian%20coordinate%20axes.%20Each%20robot%20has%20a%0Alimited%20sensing%20capability.%20We%20study%20the%20problem%20for%20both%20static%20and%20mobile%0Aintruders.%20It%20turns%20out%20that%20the%20problem%20of%20finding%20an%20intruder%20is%20NP-hard%2C%0Aeven%20for%20a%20stationary%20intruder.%20Given%20this%20intractability%2C%20we%20turn%20our%0Aattention%20towards%20developing%20efficient%20and%20robust%20algorithms%2C%20namely%20methods%0Abased%20on%20space-filling%20curves%2C%20random%20search%2C%20and%20cooperative%20random%20search.%0AMoreover%2C%20for%20each%20proposed%20algorithm%2C%20we%20evaluate%20the%20trade-off%20between%20the%0Anumber%20of%20search%20robots%20and%20the%20time%20required%20for%20the%20robots%20to%20complete%20the%0Asearch%20process%20while%20considering%20the%20geometric%20properties%20of%20the%20connected%0Aorthogonal%20search%20area.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03622v1&entry.124074799=Read"},
{"title": "Using latent representations to link disjoint longitudinal data for\n  mixed-effects regression", "author": "Clemens Sch\u00e4chter and Maren Hackenberg and Michelle Pfaffenlehner and F\u00e9lix B. Tambe-Ndonfack and Thorsten Schmidt and Astrid Pechmann and Janbernd Kirschner and Jan Hasenauer and Harald Binder", "abstract": "  Many rare diseases offer limited established treatment options, leading\npatients to switch therapies when new medications emerge. To analyze the impact\nof such treatment switches within the low sample size limitations of rare\ndisease trials, it is important to use all available data sources. This,\nhowever, is complicated when usage of measurement instruments change during the\nobservation period, for example when instruments are adapted to specific age\nranges. The resulting disjoint longitudinal data trajectories, complicate the\napplication of traditional modeling approaches like mixed-effects regression.\nWe tackle this by mapping observations of each instrument to a aligned\nlow-dimensional temporal trajectory, enabling longitudinal modeling across\ninstruments. Specifically, we employ a set of variational autoencoder\narchitectures to embed item values into a shared latent space for each time\npoint. Temporal disease dynamics and treatment switch effects are then captured\nthrough a mixed-effects regression model applied to latent representations. To\nenable statistical inference, we present a novel statistical testing approach\nthat accounts for the joint parameter estimation of mixed-effects regression\nand variational autoencoders. The methodology is applied to quantify the impact\nof treatment switches for patients with spinal muscular atrophy. Here, our\napproach aligns motor performance items from different measurement instruments\nfor mixed-effects regression and maps estimated effects back to the observed\nitem level to quantify the treatment switch effect. Our approach allows for\nmodel selection as well as for assessing effects of treatment switching. The\nresults highlight the potential of modeling in joint latent representations for\naddressing small data challenges.\n", "link": "http://arxiv.org/abs/2510.25531v2", "date": "2025-11-05", "relevancy": 1.4694, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5397}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4896}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20latent%20representations%20to%20link%20disjoint%20longitudinal%20data%20for%0A%20%20mixed-effects%20regression&body=Title%3A%20Using%20latent%20representations%20to%20link%20disjoint%20longitudinal%20data%20for%0A%20%20mixed-effects%20regression%0AAuthor%3A%20Clemens%20Sch%C3%A4chter%20and%20Maren%20Hackenberg%20and%20Michelle%20Pfaffenlehner%20and%20F%C3%A9lix%20B.%20Tambe-Ndonfack%20and%20Thorsten%20Schmidt%20and%20Astrid%20Pechmann%20and%20Janbernd%20Kirschner%20and%20Jan%20Hasenauer%20and%20Harald%20Binder%0AAbstract%3A%20%20%20Many%20rare%20diseases%20offer%20limited%20established%20treatment%20options%2C%20leading%0Apatients%20to%20switch%20therapies%20when%20new%20medications%20emerge.%20To%20analyze%20the%20impact%0Aof%20such%20treatment%20switches%20within%20the%20low%20sample%20size%20limitations%20of%20rare%0Adisease%20trials%2C%20it%20is%20important%20to%20use%20all%20available%20data%20sources.%20This%2C%0Ahowever%2C%20is%20complicated%20when%20usage%20of%20measurement%20instruments%20change%20during%20the%0Aobservation%20period%2C%20for%20example%20when%20instruments%20are%20adapted%20to%20specific%20age%0Aranges.%20The%20resulting%20disjoint%20longitudinal%20data%20trajectories%2C%20complicate%20the%0Aapplication%20of%20traditional%20modeling%20approaches%20like%20mixed-effects%20regression.%0AWe%20tackle%20this%20by%20mapping%20observations%20of%20each%20instrument%20to%20a%20aligned%0Alow-dimensional%20temporal%20trajectory%2C%20enabling%20longitudinal%20modeling%20across%0Ainstruments.%20Specifically%2C%20we%20employ%20a%20set%20of%20variational%20autoencoder%0Aarchitectures%20to%20embed%20item%20values%20into%20a%20shared%20latent%20space%20for%20each%20time%0Apoint.%20Temporal%20disease%20dynamics%20and%20treatment%20switch%20effects%20are%20then%20captured%0Athrough%20a%20mixed-effects%20regression%20model%20applied%20to%20latent%20representations.%20To%0Aenable%20statistical%20inference%2C%20we%20present%20a%20novel%20statistical%20testing%20approach%0Athat%20accounts%20for%20the%20joint%20parameter%20estimation%20of%20mixed-effects%20regression%0Aand%20variational%20autoencoders.%20The%20methodology%20is%20applied%20to%20quantify%20the%20impact%0Aof%20treatment%20switches%20for%20patients%20with%20spinal%20muscular%20atrophy.%20Here%2C%20our%0Aapproach%20aligns%20motor%20performance%20items%20from%20different%20measurement%20instruments%0Afor%20mixed-effects%20regression%20and%20maps%20estimated%20effects%20back%20to%20the%20observed%0Aitem%20level%20to%20quantify%20the%20treatment%20switch%20effect.%20Our%20approach%20allows%20for%0Amodel%20selection%20as%20well%20as%20for%20assessing%20effects%20of%20treatment%20switching.%20The%0Aresults%20highlight%20the%20potential%20of%20modeling%20in%20joint%20latent%20representations%20for%0Aaddressing%20small%20data%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25531v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520latent%2520representations%2520to%2520link%2520disjoint%2520longitudinal%2520data%2520for%250A%2520%2520mixed-effects%2520regression%26entry.906535625%3DClemens%2520Sch%25C3%25A4chter%2520and%2520Maren%2520Hackenberg%2520and%2520Michelle%2520Pfaffenlehner%2520and%2520F%25C3%25A9lix%2520B.%2520Tambe-Ndonfack%2520and%2520Thorsten%2520Schmidt%2520and%2520Astrid%2520Pechmann%2520and%2520Janbernd%2520Kirschner%2520and%2520Jan%2520Hasenauer%2520and%2520Harald%2520Binder%26entry.1292438233%3D%2520%2520Many%2520rare%2520diseases%2520offer%2520limited%2520established%2520treatment%2520options%252C%2520leading%250Apatients%2520to%2520switch%2520therapies%2520when%2520new%2520medications%2520emerge.%2520To%2520analyze%2520the%2520impact%250Aof%2520such%2520treatment%2520switches%2520within%2520the%2520low%2520sample%2520size%2520limitations%2520of%2520rare%250Adisease%2520trials%252C%2520it%2520is%2520important%2520to%2520use%2520all%2520available%2520data%2520sources.%2520This%252C%250Ahowever%252C%2520is%2520complicated%2520when%2520usage%2520of%2520measurement%2520instruments%2520change%2520during%2520the%250Aobservation%2520period%252C%2520for%2520example%2520when%2520instruments%2520are%2520adapted%2520to%2520specific%2520age%250Aranges.%2520The%2520resulting%2520disjoint%2520longitudinal%2520data%2520trajectories%252C%2520complicate%2520the%250Aapplication%2520of%2520traditional%2520modeling%2520approaches%2520like%2520mixed-effects%2520regression.%250AWe%2520tackle%2520this%2520by%2520mapping%2520observations%2520of%2520each%2520instrument%2520to%2520a%2520aligned%250Alow-dimensional%2520temporal%2520trajectory%252C%2520enabling%2520longitudinal%2520modeling%2520across%250Ainstruments.%2520Specifically%252C%2520we%2520employ%2520a%2520set%2520of%2520variational%2520autoencoder%250Aarchitectures%2520to%2520embed%2520item%2520values%2520into%2520a%2520shared%2520latent%2520space%2520for%2520each%2520time%250Apoint.%2520Temporal%2520disease%2520dynamics%2520and%2520treatment%2520switch%2520effects%2520are%2520then%2520captured%250Athrough%2520a%2520mixed-effects%2520regression%2520model%2520applied%2520to%2520latent%2520representations.%2520To%250Aenable%2520statistical%2520inference%252C%2520we%2520present%2520a%2520novel%2520statistical%2520testing%2520approach%250Athat%2520accounts%2520for%2520the%2520joint%2520parameter%2520estimation%2520of%2520mixed-effects%2520regression%250Aand%2520variational%2520autoencoders.%2520The%2520methodology%2520is%2520applied%2520to%2520quantify%2520the%2520impact%250Aof%2520treatment%2520switches%2520for%2520patients%2520with%2520spinal%2520muscular%2520atrophy.%2520Here%252C%2520our%250Aapproach%2520aligns%2520motor%2520performance%2520items%2520from%2520different%2520measurement%2520instruments%250Afor%2520mixed-effects%2520regression%2520and%2520maps%2520estimated%2520effects%2520back%2520to%2520the%2520observed%250Aitem%2520level%2520to%2520quantify%2520the%2520treatment%2520switch%2520effect.%2520Our%2520approach%2520allows%2520for%250Amodel%2520selection%2520as%2520well%2520as%2520for%2520assessing%2520effects%2520of%2520treatment%2520switching.%2520The%250Aresults%2520highlight%2520the%2520potential%2520of%2520modeling%2520in%2520joint%2520latent%2520representations%2520for%250Aaddressing%2520small%2520data%2520challenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25531v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20latent%20representations%20to%20link%20disjoint%20longitudinal%20data%20for%0A%20%20mixed-effects%20regression&entry.906535625=Clemens%20Sch%C3%A4chter%20and%20Maren%20Hackenberg%20and%20Michelle%20Pfaffenlehner%20and%20F%C3%A9lix%20B.%20Tambe-Ndonfack%20and%20Thorsten%20Schmidt%20and%20Astrid%20Pechmann%20and%20Janbernd%20Kirschner%20and%20Jan%20Hasenauer%20and%20Harald%20Binder&entry.1292438233=%20%20Many%20rare%20diseases%20offer%20limited%20established%20treatment%20options%2C%20leading%0Apatients%20to%20switch%20therapies%20when%20new%20medications%20emerge.%20To%20analyze%20the%20impact%0Aof%20such%20treatment%20switches%20within%20the%20low%20sample%20size%20limitations%20of%20rare%0Adisease%20trials%2C%20it%20is%20important%20to%20use%20all%20available%20data%20sources.%20This%2C%0Ahowever%2C%20is%20complicated%20when%20usage%20of%20measurement%20instruments%20change%20during%20the%0Aobservation%20period%2C%20for%20example%20when%20instruments%20are%20adapted%20to%20specific%20age%0Aranges.%20The%20resulting%20disjoint%20longitudinal%20data%20trajectories%2C%20complicate%20the%0Aapplication%20of%20traditional%20modeling%20approaches%20like%20mixed-effects%20regression.%0AWe%20tackle%20this%20by%20mapping%20observations%20of%20each%20instrument%20to%20a%20aligned%0Alow-dimensional%20temporal%20trajectory%2C%20enabling%20longitudinal%20modeling%20across%0Ainstruments.%20Specifically%2C%20we%20employ%20a%20set%20of%20variational%20autoencoder%0Aarchitectures%20to%20embed%20item%20values%20into%20a%20shared%20latent%20space%20for%20each%20time%0Apoint.%20Temporal%20disease%20dynamics%20and%20treatment%20switch%20effects%20are%20then%20captured%0Athrough%20a%20mixed-effects%20regression%20model%20applied%20to%20latent%20representations.%20To%0Aenable%20statistical%20inference%2C%20we%20present%20a%20novel%20statistical%20testing%20approach%0Athat%20accounts%20for%20the%20joint%20parameter%20estimation%20of%20mixed-effects%20regression%0Aand%20variational%20autoencoders.%20The%20methodology%20is%20applied%20to%20quantify%20the%20impact%0Aof%20treatment%20switches%20for%20patients%20with%20spinal%20muscular%20atrophy.%20Here%2C%20our%0Aapproach%20aligns%20motor%20performance%20items%20from%20different%20measurement%20instruments%0Afor%20mixed-effects%20regression%20and%20maps%20estimated%20effects%20back%20to%20the%20observed%0Aitem%20level%20to%20quantify%20the%20treatment%20switch%20effect.%20Our%20approach%20allows%20for%0Amodel%20selection%20as%20well%20as%20for%20assessing%20effects%20of%20treatment%20switching.%20The%0Aresults%20highlight%20the%20potential%20of%20modeling%20in%20joint%20latent%20representations%20for%0Aaddressing%20small%20data%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25531v2&entry.124074799=Read"},
{"title": "Colorectal Cancer Histopathological Grading using Multi-Scale Federated\n  Learning", "author": "Md Ahasanul Arafath and Abhijit Kumar Ghosh and Md Rony Ahmed and Sabrin Afroz and Minhazul Hosen and Md Hasan Moon and Md Tanzim Reza and Md Ashad Alam", "abstract": "  Colorectal cancer (CRC) grading is a critical prognostic factor but remains\nhampered by inter-observer variability and the privacy constraints of\nmulti-institutional data sharing. While deep learning offers a path to\nautomation, centralized training models conflict with data governance\nregulations and neglect the diagnostic importance of multi-scale analysis. In\nthis work, we propose a scalable, privacy-preserving federated learning (FL)\nframework for CRC histopathological grading that integrates multi-scale feature\nlearning within a distributed training paradigm. Our approach employs a\ndual-stream ResNetRS50 backbone to concurrently capture fine-grained nuclear\ndetail and broader tissue-level context. This architecture is integrated into a\nrobust FL system stabilized using FedProx to mitigate client drift across\nheterogeneous data distributions from multiple hospitals. Extensive evaluation\non the CRC-HGD dataset demonstrates that our framework achieves an overall\naccuracy of 83.5%, outperforming a comparable centralized model (81.6%).\nCrucially, the system excels in identifying the most aggressive Grade III\ntumors with a high recall of 87.5%, a key clinical priority to prevent\ndangerous false negatives. Performance further improves with higher\nmagnification, reaching 88.0% accuracy at 40x. These results validate that our\nfederated multi-scale approach not only preserves patient privacy but also\nenhances model performance and generalization. The proposed modular pipeline,\nwith built-in preprocessing, checkpointing, and error handling, establishes a\nfoundational step toward deployable, privacy-aware clinical AI for digital\npathology.\n", "link": "http://arxiv.org/abs/2511.03693v1", "date": "2025-11-05", "relevancy": 1.4649, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4981}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4771}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Colorectal%20Cancer%20Histopathological%20Grading%20using%20Multi-Scale%20Federated%0A%20%20Learning&body=Title%3A%20Colorectal%20Cancer%20Histopathological%20Grading%20using%20Multi-Scale%20Federated%0A%20%20Learning%0AAuthor%3A%20Md%20Ahasanul%20Arafath%20and%20Abhijit%20Kumar%20Ghosh%20and%20Md%20Rony%20Ahmed%20and%20Sabrin%20Afroz%20and%20Minhazul%20Hosen%20and%20Md%20Hasan%20Moon%20and%20Md%20Tanzim%20Reza%20and%20Md%20Ashad%20Alam%0AAbstract%3A%20%20%20Colorectal%20cancer%20%28CRC%29%20grading%20is%20a%20critical%20prognostic%20factor%20but%20remains%0Ahampered%20by%20inter-observer%20variability%20and%20the%20privacy%20constraints%20of%0Amulti-institutional%20data%20sharing.%20While%20deep%20learning%20offers%20a%20path%20to%0Aautomation%2C%20centralized%20training%20models%20conflict%20with%20data%20governance%0Aregulations%20and%20neglect%20the%20diagnostic%20importance%20of%20multi-scale%20analysis.%20In%0Athis%20work%2C%20we%20propose%20a%20scalable%2C%20privacy-preserving%20federated%20learning%20%28FL%29%0Aframework%20for%20CRC%20histopathological%20grading%20that%20integrates%20multi-scale%20feature%0Alearning%20within%20a%20distributed%20training%20paradigm.%20Our%20approach%20employs%20a%0Adual-stream%20ResNetRS50%20backbone%20to%20concurrently%20capture%20fine-grained%20nuclear%0Adetail%20and%20broader%20tissue-level%20context.%20This%20architecture%20is%20integrated%20into%20a%0Arobust%20FL%20system%20stabilized%20using%20FedProx%20to%20mitigate%20client%20drift%20across%0Aheterogeneous%20data%20distributions%20from%20multiple%20hospitals.%20Extensive%20evaluation%0Aon%20the%20CRC-HGD%20dataset%20demonstrates%20that%20our%20framework%20achieves%20an%20overall%0Aaccuracy%20of%2083.5%25%2C%20outperforming%20a%20comparable%20centralized%20model%20%2881.6%25%29.%0ACrucially%2C%20the%20system%20excels%20in%20identifying%20the%20most%20aggressive%20Grade%20III%0Atumors%20with%20a%20high%20recall%20of%2087.5%25%2C%20a%20key%20clinical%20priority%20to%20prevent%0Adangerous%20false%20negatives.%20Performance%20further%20improves%20with%20higher%0Amagnification%2C%20reaching%2088.0%25%20accuracy%20at%2040x.%20These%20results%20validate%20that%20our%0Afederated%20multi-scale%20approach%20not%20only%20preserves%20patient%20privacy%20but%20also%0Aenhances%20model%20performance%20and%20generalization.%20The%20proposed%20modular%20pipeline%2C%0Awith%20built-in%20preprocessing%2C%20checkpointing%2C%20and%20error%20handling%2C%20establishes%20a%0Afoundational%20step%20toward%20deployable%2C%20privacy-aware%20clinical%20AI%20for%20digital%0Apathology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03693v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DColorectal%2520Cancer%2520Histopathological%2520Grading%2520using%2520Multi-Scale%2520Federated%250A%2520%2520Learning%26entry.906535625%3DMd%2520Ahasanul%2520Arafath%2520and%2520Abhijit%2520Kumar%2520Ghosh%2520and%2520Md%2520Rony%2520Ahmed%2520and%2520Sabrin%2520Afroz%2520and%2520Minhazul%2520Hosen%2520and%2520Md%2520Hasan%2520Moon%2520and%2520Md%2520Tanzim%2520Reza%2520and%2520Md%2520Ashad%2520Alam%26entry.1292438233%3D%2520%2520Colorectal%2520cancer%2520%2528CRC%2529%2520grading%2520is%2520a%2520critical%2520prognostic%2520factor%2520but%2520remains%250Ahampered%2520by%2520inter-observer%2520variability%2520and%2520the%2520privacy%2520constraints%2520of%250Amulti-institutional%2520data%2520sharing.%2520While%2520deep%2520learning%2520offers%2520a%2520path%2520to%250Aautomation%252C%2520centralized%2520training%2520models%2520conflict%2520with%2520data%2520governance%250Aregulations%2520and%2520neglect%2520the%2520diagnostic%2520importance%2520of%2520multi-scale%2520analysis.%2520In%250Athis%2520work%252C%2520we%2520propose%2520a%2520scalable%252C%2520privacy-preserving%2520federated%2520learning%2520%2528FL%2529%250Aframework%2520for%2520CRC%2520histopathological%2520grading%2520that%2520integrates%2520multi-scale%2520feature%250Alearning%2520within%2520a%2520distributed%2520training%2520paradigm.%2520Our%2520approach%2520employs%2520a%250Adual-stream%2520ResNetRS50%2520backbone%2520to%2520concurrently%2520capture%2520fine-grained%2520nuclear%250Adetail%2520and%2520broader%2520tissue-level%2520context.%2520This%2520architecture%2520is%2520integrated%2520into%2520a%250Arobust%2520FL%2520system%2520stabilized%2520using%2520FedProx%2520to%2520mitigate%2520client%2520drift%2520across%250Aheterogeneous%2520data%2520distributions%2520from%2520multiple%2520hospitals.%2520Extensive%2520evaluation%250Aon%2520the%2520CRC-HGD%2520dataset%2520demonstrates%2520that%2520our%2520framework%2520achieves%2520an%2520overall%250Aaccuracy%2520of%252083.5%2525%252C%2520outperforming%2520a%2520comparable%2520centralized%2520model%2520%252881.6%2525%2529.%250ACrucially%252C%2520the%2520system%2520excels%2520in%2520identifying%2520the%2520most%2520aggressive%2520Grade%2520III%250Atumors%2520with%2520a%2520high%2520recall%2520of%252087.5%2525%252C%2520a%2520key%2520clinical%2520priority%2520to%2520prevent%250Adangerous%2520false%2520negatives.%2520Performance%2520further%2520improves%2520with%2520higher%250Amagnification%252C%2520reaching%252088.0%2525%2520accuracy%2520at%252040x.%2520These%2520results%2520validate%2520that%2520our%250Afederated%2520multi-scale%2520approach%2520not%2520only%2520preserves%2520patient%2520privacy%2520but%2520also%250Aenhances%2520model%2520performance%2520and%2520generalization.%2520The%2520proposed%2520modular%2520pipeline%252C%250Awith%2520built-in%2520preprocessing%252C%2520checkpointing%252C%2520and%2520error%2520handling%252C%2520establishes%2520a%250Afoundational%2520step%2520toward%2520deployable%252C%2520privacy-aware%2520clinical%2520AI%2520for%2520digital%250Apathology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03693v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Colorectal%20Cancer%20Histopathological%20Grading%20using%20Multi-Scale%20Federated%0A%20%20Learning&entry.906535625=Md%20Ahasanul%20Arafath%20and%20Abhijit%20Kumar%20Ghosh%20and%20Md%20Rony%20Ahmed%20and%20Sabrin%20Afroz%20and%20Minhazul%20Hosen%20and%20Md%20Hasan%20Moon%20and%20Md%20Tanzim%20Reza%20and%20Md%20Ashad%20Alam&entry.1292438233=%20%20Colorectal%20cancer%20%28CRC%29%20grading%20is%20a%20critical%20prognostic%20factor%20but%20remains%0Ahampered%20by%20inter-observer%20variability%20and%20the%20privacy%20constraints%20of%0Amulti-institutional%20data%20sharing.%20While%20deep%20learning%20offers%20a%20path%20to%0Aautomation%2C%20centralized%20training%20models%20conflict%20with%20data%20governance%0Aregulations%20and%20neglect%20the%20diagnostic%20importance%20of%20multi-scale%20analysis.%20In%0Athis%20work%2C%20we%20propose%20a%20scalable%2C%20privacy-preserving%20federated%20learning%20%28FL%29%0Aframework%20for%20CRC%20histopathological%20grading%20that%20integrates%20multi-scale%20feature%0Alearning%20within%20a%20distributed%20training%20paradigm.%20Our%20approach%20employs%20a%0Adual-stream%20ResNetRS50%20backbone%20to%20concurrently%20capture%20fine-grained%20nuclear%0Adetail%20and%20broader%20tissue-level%20context.%20This%20architecture%20is%20integrated%20into%20a%0Arobust%20FL%20system%20stabilized%20using%20FedProx%20to%20mitigate%20client%20drift%20across%0Aheterogeneous%20data%20distributions%20from%20multiple%20hospitals.%20Extensive%20evaluation%0Aon%20the%20CRC-HGD%20dataset%20demonstrates%20that%20our%20framework%20achieves%20an%20overall%0Aaccuracy%20of%2083.5%25%2C%20outperforming%20a%20comparable%20centralized%20model%20%2881.6%25%29.%0ACrucially%2C%20the%20system%20excels%20in%20identifying%20the%20most%20aggressive%20Grade%20III%0Atumors%20with%20a%20high%20recall%20of%2087.5%25%2C%20a%20key%20clinical%20priority%20to%20prevent%0Adangerous%20false%20negatives.%20Performance%20further%20improves%20with%20higher%0Amagnification%2C%20reaching%2088.0%25%20accuracy%20at%2040x.%20These%20results%20validate%20that%20our%0Afederated%20multi-scale%20approach%20not%20only%20preserves%20patient%20privacy%20but%20also%0Aenhances%20model%20performance%20and%20generalization.%20The%20proposed%20modular%20pipeline%2C%0Awith%20built-in%20preprocessing%2C%20checkpointing%2C%20and%20error%20handling%2C%20establishes%20a%0Afoundational%20step%20toward%20deployable%2C%20privacy-aware%20clinical%20AI%20for%20digital%0Apathology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03693v1&entry.124074799=Read"},
{"title": "OrdShap: Feature Position Importance for Sequential Black-Box Models", "author": "Davin Hill and Brian L. Hill and Aria Masoomi and Vijay S. Nori and Robert E. Tillman and Jennifer Dy", "abstract": "  Sequential deep learning models excel in domains with temporal or sequential\ndependencies, but their complexity necessitates post-hoc feature attribution\nmethods for understanding their predictions. While existing techniques quantify\nfeature importance, they inherently assume fixed feature ordering - conflating\nthe effects of (1) feature values and (2) their positions within input\nsequences. To address this gap, we introduce OrdShap, a novel attribution\nmethod that disentangles these effects by quantifying how a model's predictions\nchange in response to permuting feature position. We establish a game-theoretic\nconnection between OrdShap and Sanchez-Berganti\\~nos values, providing a\ntheoretically grounded approach to position-sensitive attribution. Empirical\nresults from health, natural language, and synthetic datasets highlight\nOrdShap's effectiveness in capturing feature value and feature position\nattributions, and provide deeper insight into model behavior.\n", "link": "http://arxiv.org/abs/2507.11855v2", "date": "2025-11-05", "relevancy": 1.4512, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5019}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4856}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OrdShap%3A%20Feature%20Position%20Importance%20for%20Sequential%20Black-Box%20Models&body=Title%3A%20OrdShap%3A%20Feature%20Position%20Importance%20for%20Sequential%20Black-Box%20Models%0AAuthor%3A%20Davin%20Hill%20and%20Brian%20L.%20Hill%20and%20Aria%20Masoomi%20and%20Vijay%20S.%20Nori%20and%20Robert%20E.%20Tillman%20and%20Jennifer%20Dy%0AAbstract%3A%20%20%20Sequential%20deep%20learning%20models%20excel%20in%20domains%20with%20temporal%20or%20sequential%0Adependencies%2C%20but%20their%20complexity%20necessitates%20post-hoc%20feature%20attribution%0Amethods%20for%20understanding%20their%20predictions.%20While%20existing%20techniques%20quantify%0Afeature%20importance%2C%20they%20inherently%20assume%20fixed%20feature%20ordering%20-%20conflating%0Athe%20effects%20of%20%281%29%20feature%20values%20and%20%282%29%20their%20positions%20within%20input%0Asequences.%20To%20address%20this%20gap%2C%20we%20introduce%20OrdShap%2C%20a%20novel%20attribution%0Amethod%20that%20disentangles%20these%20effects%20by%20quantifying%20how%20a%20model%27s%20predictions%0Achange%20in%20response%20to%20permuting%20feature%20position.%20We%20establish%20a%20game-theoretic%0Aconnection%20between%20OrdShap%20and%20Sanchez-Berganti%5C~nos%20values%2C%20providing%20a%0Atheoretically%20grounded%20approach%20to%20position-sensitive%20attribution.%20Empirical%0Aresults%20from%20health%2C%20natural%20language%2C%20and%20synthetic%20datasets%20highlight%0AOrdShap%27s%20effectiveness%20in%20capturing%20feature%20value%20and%20feature%20position%0Aattributions%2C%20and%20provide%20deeper%20insight%20into%20model%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11855v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOrdShap%253A%2520Feature%2520Position%2520Importance%2520for%2520Sequential%2520Black-Box%2520Models%26entry.906535625%3DDavin%2520Hill%2520and%2520Brian%2520L.%2520Hill%2520and%2520Aria%2520Masoomi%2520and%2520Vijay%2520S.%2520Nori%2520and%2520Robert%2520E.%2520Tillman%2520and%2520Jennifer%2520Dy%26entry.1292438233%3D%2520%2520Sequential%2520deep%2520learning%2520models%2520excel%2520in%2520domains%2520with%2520temporal%2520or%2520sequential%250Adependencies%252C%2520but%2520their%2520complexity%2520necessitates%2520post-hoc%2520feature%2520attribution%250Amethods%2520for%2520understanding%2520their%2520predictions.%2520While%2520existing%2520techniques%2520quantify%250Afeature%2520importance%252C%2520they%2520inherently%2520assume%2520fixed%2520feature%2520ordering%2520-%2520conflating%250Athe%2520effects%2520of%2520%25281%2529%2520feature%2520values%2520and%2520%25282%2529%2520their%2520positions%2520within%2520input%250Asequences.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520OrdShap%252C%2520a%2520novel%2520attribution%250Amethod%2520that%2520disentangles%2520these%2520effects%2520by%2520quantifying%2520how%2520a%2520model%2527s%2520predictions%250Achange%2520in%2520response%2520to%2520permuting%2520feature%2520position.%2520We%2520establish%2520a%2520game-theoretic%250Aconnection%2520between%2520OrdShap%2520and%2520Sanchez-Berganti%255C~nos%2520values%252C%2520providing%2520a%250Atheoretically%2520grounded%2520approach%2520to%2520position-sensitive%2520attribution.%2520Empirical%250Aresults%2520from%2520health%252C%2520natural%2520language%252C%2520and%2520synthetic%2520datasets%2520highlight%250AOrdShap%2527s%2520effectiveness%2520in%2520capturing%2520feature%2520value%2520and%2520feature%2520position%250Aattributions%252C%2520and%2520provide%2520deeper%2520insight%2520into%2520model%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11855v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OrdShap%3A%20Feature%20Position%20Importance%20for%20Sequential%20Black-Box%20Models&entry.906535625=Davin%20Hill%20and%20Brian%20L.%20Hill%20and%20Aria%20Masoomi%20and%20Vijay%20S.%20Nori%20and%20Robert%20E.%20Tillman%20and%20Jennifer%20Dy&entry.1292438233=%20%20Sequential%20deep%20learning%20models%20excel%20in%20domains%20with%20temporal%20or%20sequential%0Adependencies%2C%20but%20their%20complexity%20necessitates%20post-hoc%20feature%20attribution%0Amethods%20for%20understanding%20their%20predictions.%20While%20existing%20techniques%20quantify%0Afeature%20importance%2C%20they%20inherently%20assume%20fixed%20feature%20ordering%20-%20conflating%0Athe%20effects%20of%20%281%29%20feature%20values%20and%20%282%29%20their%20positions%20within%20input%0Asequences.%20To%20address%20this%20gap%2C%20we%20introduce%20OrdShap%2C%20a%20novel%20attribution%0Amethod%20that%20disentangles%20these%20effects%20by%20quantifying%20how%20a%20model%27s%20predictions%0Achange%20in%20response%20to%20permuting%20feature%20position.%20We%20establish%20a%20game-theoretic%0Aconnection%20between%20OrdShap%20and%20Sanchez-Berganti%5C~nos%20values%2C%20providing%20a%0Atheoretically%20grounded%20approach%20to%20position-sensitive%20attribution.%20Empirical%0Aresults%20from%20health%2C%20natural%20language%2C%20and%20synthetic%20datasets%20highlight%0AOrdShap%27s%20effectiveness%20in%20capturing%20feature%20value%20and%20feature%20position%0Aattributions%2C%20and%20provide%20deeper%20insight%20into%20model%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11855v2&entry.124074799=Read"},
{"title": "Behavior-Adaptive Q-Learning: A Unifying Framework for Offline-to-Online\n  RL", "author": "Lipeng Zu and Hansong Zhou and Xiaonan Zhang", "abstract": "  Offline reinforcement learning (RL) enables training from fixed data without\nonline interaction, but policies learned offline often struggle when deployed\nin dynamic environments due to distributional shift and unreliable value\nestimates on unseen state-action pairs. We introduce Behavior-Adaptive\nQ-Learning (BAQ), a framework designed to enable a smooth and reliable\ntransition from offline to online RL. The key idea is to leverage an implicit\nbehavioral model derived from offline data to provide a behavior-consistency\nsignal during online fine-tuning. BAQ incorporates a dual-objective loss that\n(i) aligns the online policy toward the offline behavior when uncertainty is\nhigh, and (ii) gradually relaxes this constraint as more confident online\nexperience is accumulated. This adaptive mechanism reduces error propagation\nfrom out-of-distribution estimates, stabilizes early online updates, and\naccelerates adaptation to new scenarios. Across standard benchmarks, BAQ\nconsistently outperforms prior offline-to-online RL approaches, achieving\nfaster recovery, improved robustness, and higher overall performance. Our\nresults demonstrate that implicit behavior adaptation is a principled and\npractical solution for reliable real-world policy deployment.\n", "link": "http://arxiv.org/abs/2511.03695v1", "date": "2025-11-05", "relevancy": 1.4487, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5093}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4839}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Behavior-Adaptive%20Q-Learning%3A%20A%20Unifying%20Framework%20for%20Offline-to-Online%0A%20%20RL&body=Title%3A%20Behavior-Adaptive%20Q-Learning%3A%20A%20Unifying%20Framework%20for%20Offline-to-Online%0A%20%20RL%0AAuthor%3A%20Lipeng%20Zu%20and%20Hansong%20Zhou%20and%20Xiaonan%20Zhang%0AAbstract%3A%20%20%20Offline%20reinforcement%20learning%20%28RL%29%20enables%20training%20from%20fixed%20data%20without%0Aonline%20interaction%2C%20but%20policies%20learned%20offline%20often%20struggle%20when%20deployed%0Ain%20dynamic%20environments%20due%20to%20distributional%20shift%20and%20unreliable%20value%0Aestimates%20on%20unseen%20state-action%20pairs.%20We%20introduce%20Behavior-Adaptive%0AQ-Learning%20%28BAQ%29%2C%20a%20framework%20designed%20to%20enable%20a%20smooth%20and%20reliable%0Atransition%20from%20offline%20to%20online%20RL.%20The%20key%20idea%20is%20to%20leverage%20an%20implicit%0Abehavioral%20model%20derived%20from%20offline%20data%20to%20provide%20a%20behavior-consistency%0Asignal%20during%20online%20fine-tuning.%20BAQ%20incorporates%20a%20dual-objective%20loss%20that%0A%28i%29%20aligns%20the%20online%20policy%20toward%20the%20offline%20behavior%20when%20uncertainty%20is%0Ahigh%2C%20and%20%28ii%29%20gradually%20relaxes%20this%20constraint%20as%20more%20confident%20online%0Aexperience%20is%20accumulated.%20This%20adaptive%20mechanism%20reduces%20error%20propagation%0Afrom%20out-of-distribution%20estimates%2C%20stabilizes%20early%20online%20updates%2C%20and%0Aaccelerates%20adaptation%20to%20new%20scenarios.%20Across%20standard%20benchmarks%2C%20BAQ%0Aconsistently%20outperforms%20prior%20offline-to-online%20RL%20approaches%2C%20achieving%0Afaster%20recovery%2C%20improved%20robustness%2C%20and%20higher%20overall%20performance.%20Our%0Aresults%20demonstrate%20that%20implicit%20behavior%20adaptation%20is%20a%20principled%20and%0Apractical%20solution%20for%20reliable%20real-world%20policy%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03695v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBehavior-Adaptive%2520Q-Learning%253A%2520A%2520Unifying%2520Framework%2520for%2520Offline-to-Online%250A%2520%2520RL%26entry.906535625%3DLipeng%2520Zu%2520and%2520Hansong%2520Zhou%2520and%2520Xiaonan%2520Zhang%26entry.1292438233%3D%2520%2520Offline%2520reinforcement%2520learning%2520%2528RL%2529%2520enables%2520training%2520from%2520fixed%2520data%2520without%250Aonline%2520interaction%252C%2520but%2520policies%2520learned%2520offline%2520often%2520struggle%2520when%2520deployed%250Ain%2520dynamic%2520environments%2520due%2520to%2520distributional%2520shift%2520and%2520unreliable%2520value%250Aestimates%2520on%2520unseen%2520state-action%2520pairs.%2520We%2520introduce%2520Behavior-Adaptive%250AQ-Learning%2520%2528BAQ%2529%252C%2520a%2520framework%2520designed%2520to%2520enable%2520a%2520smooth%2520and%2520reliable%250Atransition%2520from%2520offline%2520to%2520online%2520RL.%2520The%2520key%2520idea%2520is%2520to%2520leverage%2520an%2520implicit%250Abehavioral%2520model%2520derived%2520from%2520offline%2520data%2520to%2520provide%2520a%2520behavior-consistency%250Asignal%2520during%2520online%2520fine-tuning.%2520BAQ%2520incorporates%2520a%2520dual-objective%2520loss%2520that%250A%2528i%2529%2520aligns%2520the%2520online%2520policy%2520toward%2520the%2520offline%2520behavior%2520when%2520uncertainty%2520is%250Ahigh%252C%2520and%2520%2528ii%2529%2520gradually%2520relaxes%2520this%2520constraint%2520as%2520more%2520confident%2520online%250Aexperience%2520is%2520accumulated.%2520This%2520adaptive%2520mechanism%2520reduces%2520error%2520propagation%250Afrom%2520out-of-distribution%2520estimates%252C%2520stabilizes%2520early%2520online%2520updates%252C%2520and%250Aaccelerates%2520adaptation%2520to%2520new%2520scenarios.%2520Across%2520standard%2520benchmarks%252C%2520BAQ%250Aconsistently%2520outperforms%2520prior%2520offline-to-online%2520RL%2520approaches%252C%2520achieving%250Afaster%2520recovery%252C%2520improved%2520robustness%252C%2520and%2520higher%2520overall%2520performance.%2520Our%250Aresults%2520demonstrate%2520that%2520implicit%2520behavior%2520adaptation%2520is%2520a%2520principled%2520and%250Apractical%2520solution%2520for%2520reliable%2520real-world%2520policy%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03695v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Behavior-Adaptive%20Q-Learning%3A%20A%20Unifying%20Framework%20for%20Offline-to-Online%0A%20%20RL&entry.906535625=Lipeng%20Zu%20and%20Hansong%20Zhou%20and%20Xiaonan%20Zhang&entry.1292438233=%20%20Offline%20reinforcement%20learning%20%28RL%29%20enables%20training%20from%20fixed%20data%20without%0Aonline%20interaction%2C%20but%20policies%20learned%20offline%20often%20struggle%20when%20deployed%0Ain%20dynamic%20environments%20due%20to%20distributional%20shift%20and%20unreliable%20value%0Aestimates%20on%20unseen%20state-action%20pairs.%20We%20introduce%20Behavior-Adaptive%0AQ-Learning%20%28BAQ%29%2C%20a%20framework%20designed%20to%20enable%20a%20smooth%20and%20reliable%0Atransition%20from%20offline%20to%20online%20RL.%20The%20key%20idea%20is%20to%20leverage%20an%20implicit%0Abehavioral%20model%20derived%20from%20offline%20data%20to%20provide%20a%20behavior-consistency%0Asignal%20during%20online%20fine-tuning.%20BAQ%20incorporates%20a%20dual-objective%20loss%20that%0A%28i%29%20aligns%20the%20online%20policy%20toward%20the%20offline%20behavior%20when%20uncertainty%20is%0Ahigh%2C%20and%20%28ii%29%20gradually%20relaxes%20this%20constraint%20as%20more%20confident%20online%0Aexperience%20is%20accumulated.%20This%20adaptive%20mechanism%20reduces%20error%20propagation%0Afrom%20out-of-distribution%20estimates%2C%20stabilizes%20early%20online%20updates%2C%20and%0Aaccelerates%20adaptation%20to%20new%20scenarios.%20Across%20standard%20benchmarks%2C%20BAQ%0Aconsistently%20outperforms%20prior%20offline-to-online%20RL%20approaches%2C%20achieving%0Afaster%20recovery%2C%20improved%20robustness%2C%20and%20higher%20overall%20performance.%20Our%0Aresults%20demonstrate%20that%20implicit%20behavior%20adaptation%20is%20a%20principled%20and%0Apractical%20solution%20for%20reliable%20real-world%20policy%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03695v1&entry.124074799=Read"},
{"title": "AlignIQL: Policy Alignment in Implicit Q-Learning through Constrained\n  Optimization", "author": "Longxiang He and Li Shen and Xueqian Wang", "abstract": "  Implicit Q-learning (IQL) serves as a strong baseline for offline RL, which\nlearns the value function using only dataset actions through quantile\nregression. However, it is unclear how to recover the implicit policy from the\nlearned implicit Q-function and why IQL can utilize weighted regression for\npolicy extraction. IDQL reinterprets IQL as an actor-critic method and gets\nweights of implicit policy, however, this weight only holds for the optimal\nvalue function. In this work, we introduce a different way to solve the\nimplicit policy-finding problem (IPF) by formulating this problem as an\noptimization problem. Based on this optimization problem, we further propose\ntwo practical algorithms AlignIQL and AlignIQL-hard, which inherit the\nadvantages of decoupling actor from critic in IQL and provide insights into why\nIQL can use weighted regression for policy extraction. Compared with IQL and\nIDQL, we find our method keeps the simplicity of IQL and solves the implicit\npolicy-finding problem. Experimental results on D4RL datasets show that our\nmethod achieves competitive or superior results compared with other SOTA\noffline RL methods. Especially in complex sparse reward tasks like Antmaze and\nAdroit, our method outperforms IQL and IDQL by a significant margin.\n", "link": "http://arxiv.org/abs/2405.18187v2", "date": "2025-11-05", "relevancy": 1.4364, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.504}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4552}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlignIQL%3A%20Policy%20Alignment%20in%20Implicit%20Q-Learning%20through%20Constrained%0A%20%20Optimization&body=Title%3A%20AlignIQL%3A%20Policy%20Alignment%20in%20Implicit%20Q-Learning%20through%20Constrained%0A%20%20Optimization%0AAuthor%3A%20Longxiang%20He%20and%20Li%20Shen%20and%20Xueqian%20Wang%0AAbstract%3A%20%20%20Implicit%20Q-learning%20%28IQL%29%20serves%20as%20a%20strong%20baseline%20for%20offline%20RL%2C%20which%0Alearns%20the%20value%20function%20using%20only%20dataset%20actions%20through%20quantile%0Aregression.%20However%2C%20it%20is%20unclear%20how%20to%20recover%20the%20implicit%20policy%20from%20the%0Alearned%20implicit%20Q-function%20and%20why%20IQL%20can%20utilize%20weighted%20regression%20for%0Apolicy%20extraction.%20IDQL%20reinterprets%20IQL%20as%20an%20actor-critic%20method%20and%20gets%0Aweights%20of%20implicit%20policy%2C%20however%2C%20this%20weight%20only%20holds%20for%20the%20optimal%0Avalue%20function.%20In%20this%20work%2C%20we%20introduce%20a%20different%20way%20to%20solve%20the%0Aimplicit%20policy-finding%20problem%20%28IPF%29%20by%20formulating%20this%20problem%20as%20an%0Aoptimization%20problem.%20Based%20on%20this%20optimization%20problem%2C%20we%20further%20propose%0Atwo%20practical%20algorithms%20AlignIQL%20and%20AlignIQL-hard%2C%20which%20inherit%20the%0Aadvantages%20of%20decoupling%20actor%20from%20critic%20in%20IQL%20and%20provide%20insights%20into%20why%0AIQL%20can%20use%20weighted%20regression%20for%20policy%20extraction.%20Compared%20with%20IQL%20and%0AIDQL%2C%20we%20find%20our%20method%20keeps%20the%20simplicity%20of%20IQL%20and%20solves%20the%20implicit%0Apolicy-finding%20problem.%20Experimental%20results%20on%20D4RL%20datasets%20show%20that%20our%0Amethod%20achieves%20competitive%20or%20superior%20results%20compared%20with%20other%20SOTA%0Aoffline%20RL%20methods.%20Especially%20in%20complex%20sparse%20reward%20tasks%20like%20Antmaze%20and%0AAdroit%2C%20our%20method%20outperforms%20IQL%20and%20IDQL%20by%20a%20significant%20margin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18187v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlignIQL%253A%2520Policy%2520Alignment%2520in%2520Implicit%2520Q-Learning%2520through%2520Constrained%250A%2520%2520Optimization%26entry.906535625%3DLongxiang%2520He%2520and%2520Li%2520Shen%2520and%2520Xueqian%2520Wang%26entry.1292438233%3D%2520%2520Implicit%2520Q-learning%2520%2528IQL%2529%2520serves%2520as%2520a%2520strong%2520baseline%2520for%2520offline%2520RL%252C%2520which%250Alearns%2520the%2520value%2520function%2520using%2520only%2520dataset%2520actions%2520through%2520quantile%250Aregression.%2520However%252C%2520it%2520is%2520unclear%2520how%2520to%2520recover%2520the%2520implicit%2520policy%2520from%2520the%250Alearned%2520implicit%2520Q-function%2520and%2520why%2520IQL%2520can%2520utilize%2520weighted%2520regression%2520for%250Apolicy%2520extraction.%2520IDQL%2520reinterprets%2520IQL%2520as%2520an%2520actor-critic%2520method%2520and%2520gets%250Aweights%2520of%2520implicit%2520policy%252C%2520however%252C%2520this%2520weight%2520only%2520holds%2520for%2520the%2520optimal%250Avalue%2520function.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520different%2520way%2520to%2520solve%2520the%250Aimplicit%2520policy-finding%2520problem%2520%2528IPF%2529%2520by%2520formulating%2520this%2520problem%2520as%2520an%250Aoptimization%2520problem.%2520Based%2520on%2520this%2520optimization%2520problem%252C%2520we%2520further%2520propose%250Atwo%2520practical%2520algorithms%2520AlignIQL%2520and%2520AlignIQL-hard%252C%2520which%2520inherit%2520the%250Aadvantages%2520of%2520decoupling%2520actor%2520from%2520critic%2520in%2520IQL%2520and%2520provide%2520insights%2520into%2520why%250AIQL%2520can%2520use%2520weighted%2520regression%2520for%2520policy%2520extraction.%2520Compared%2520with%2520IQL%2520and%250AIDQL%252C%2520we%2520find%2520our%2520method%2520keeps%2520the%2520simplicity%2520of%2520IQL%2520and%2520solves%2520the%2520implicit%250Apolicy-finding%2520problem.%2520Experimental%2520results%2520on%2520D4RL%2520datasets%2520show%2520that%2520our%250Amethod%2520achieves%2520competitive%2520or%2520superior%2520results%2520compared%2520with%2520other%2520SOTA%250Aoffline%2520RL%2520methods.%2520Especially%2520in%2520complex%2520sparse%2520reward%2520tasks%2520like%2520Antmaze%2520and%250AAdroit%252C%2520our%2520method%2520outperforms%2520IQL%2520and%2520IDQL%2520by%2520a%2520significant%2520margin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18187v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlignIQL%3A%20Policy%20Alignment%20in%20Implicit%20Q-Learning%20through%20Constrained%0A%20%20Optimization&entry.906535625=Longxiang%20He%20and%20Li%20Shen%20and%20Xueqian%20Wang&entry.1292438233=%20%20Implicit%20Q-learning%20%28IQL%29%20serves%20as%20a%20strong%20baseline%20for%20offline%20RL%2C%20which%0Alearns%20the%20value%20function%20using%20only%20dataset%20actions%20through%20quantile%0Aregression.%20However%2C%20it%20is%20unclear%20how%20to%20recover%20the%20implicit%20policy%20from%20the%0Alearned%20implicit%20Q-function%20and%20why%20IQL%20can%20utilize%20weighted%20regression%20for%0Apolicy%20extraction.%20IDQL%20reinterprets%20IQL%20as%20an%20actor-critic%20method%20and%20gets%0Aweights%20of%20implicit%20policy%2C%20however%2C%20this%20weight%20only%20holds%20for%20the%20optimal%0Avalue%20function.%20In%20this%20work%2C%20we%20introduce%20a%20different%20way%20to%20solve%20the%0Aimplicit%20policy-finding%20problem%20%28IPF%29%20by%20formulating%20this%20problem%20as%20an%0Aoptimization%20problem.%20Based%20on%20this%20optimization%20problem%2C%20we%20further%20propose%0Atwo%20practical%20algorithms%20AlignIQL%20and%20AlignIQL-hard%2C%20which%20inherit%20the%0Aadvantages%20of%20decoupling%20actor%20from%20critic%20in%20IQL%20and%20provide%20insights%20into%20why%0AIQL%20can%20use%20weighted%20regression%20for%20policy%20extraction.%20Compared%20with%20IQL%20and%0AIDQL%2C%20we%20find%20our%20method%20keeps%20the%20simplicity%20of%20IQL%20and%20solves%20the%20implicit%0Apolicy-finding%20problem.%20Experimental%20results%20on%20D4RL%20datasets%20show%20that%20our%0Amethod%20achieves%20competitive%20or%20superior%20results%20compared%20with%20other%20SOTA%0Aoffline%20RL%20methods.%20Especially%20in%20complex%20sparse%20reward%20tasks%20like%20Antmaze%20and%0AAdroit%2C%20our%20method%20outperforms%20IQL%20and%20IDQL%20by%20a%20significant%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18187v2&entry.124074799=Read"},
{"title": "Source-Free Bistable Fluidic Gripper for Size-Selective and\n  Stiffness-Adaptive Grasping", "author": "Zhihang Qin and Yueheng Zhang and Wan Su and Linxin Hou and Shenghao Zhou and Zhijun Chen and Yu Jun Tan and Cecilia Laschi", "abstract": "  Conventional fluid-driven soft grippers typically depend on external sources,\nwhich limit portability and long-term autonomy. This work introduces a\nself-contained soft gripper with fixed size that operates solely through\ninternal liquid redistribution among three interconnected bistable snap-through\nchambers. When the top sensing chamber deforms upon contact, the displaced\nliquid triggers snap-through expansion of the grasping chambers, enabling\nstable and size-selective grasping without continuous energy input. The\ninternal hydraulic feedback further allows passive adaptation of gripping\npressure to object stiffness. This source-free and compact design opens new\npossibilities for lightweight, stiffness-adaptive fluid-driven manipulation in\nsoft robotics, providing a feasible approach for targeted size-specific\nsampling and operation in underwater and field environments.\n", "link": "http://arxiv.org/abs/2511.03691v1", "date": "2025-11-05", "relevancy": 1.4319, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5023}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4467}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Source-Free%20Bistable%20Fluidic%20Gripper%20for%20Size-Selective%20and%0A%20%20Stiffness-Adaptive%20Grasping&body=Title%3A%20Source-Free%20Bistable%20Fluidic%20Gripper%20for%20Size-Selective%20and%0A%20%20Stiffness-Adaptive%20Grasping%0AAuthor%3A%20Zhihang%20Qin%20and%20Yueheng%20Zhang%20and%20Wan%20Su%20and%20Linxin%20Hou%20and%20Shenghao%20Zhou%20and%20Zhijun%20Chen%20and%20Yu%20Jun%20Tan%20and%20Cecilia%20Laschi%0AAbstract%3A%20%20%20Conventional%20fluid-driven%20soft%20grippers%20typically%20depend%20on%20external%20sources%2C%0Awhich%20limit%20portability%20and%20long-term%20autonomy.%20This%20work%20introduces%20a%0Aself-contained%20soft%20gripper%20with%20fixed%20size%20that%20operates%20solely%20through%0Ainternal%20liquid%20redistribution%20among%20three%20interconnected%20bistable%20snap-through%0Achambers.%20When%20the%20top%20sensing%20chamber%20deforms%20upon%20contact%2C%20the%20displaced%0Aliquid%20triggers%20snap-through%20expansion%20of%20the%20grasping%20chambers%2C%20enabling%0Astable%20and%20size-selective%20grasping%20without%20continuous%20energy%20input.%20The%0Ainternal%20hydraulic%20feedback%20further%20allows%20passive%20adaptation%20of%20gripping%0Apressure%20to%20object%20stiffness.%20This%20source-free%20and%20compact%20design%20opens%20new%0Apossibilities%20for%20lightweight%2C%20stiffness-adaptive%20fluid-driven%20manipulation%20in%0Asoft%20robotics%2C%20providing%20a%20feasible%20approach%20for%20targeted%20size-specific%0Asampling%20and%20operation%20in%20underwater%20and%20field%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSource-Free%2520Bistable%2520Fluidic%2520Gripper%2520for%2520Size-Selective%2520and%250A%2520%2520Stiffness-Adaptive%2520Grasping%26entry.906535625%3DZhihang%2520Qin%2520and%2520Yueheng%2520Zhang%2520and%2520Wan%2520Su%2520and%2520Linxin%2520Hou%2520and%2520Shenghao%2520Zhou%2520and%2520Zhijun%2520Chen%2520and%2520Yu%2520Jun%2520Tan%2520and%2520Cecilia%2520Laschi%26entry.1292438233%3D%2520%2520Conventional%2520fluid-driven%2520soft%2520grippers%2520typically%2520depend%2520on%2520external%2520sources%252C%250Awhich%2520limit%2520portability%2520and%2520long-term%2520autonomy.%2520This%2520work%2520introduces%2520a%250Aself-contained%2520soft%2520gripper%2520with%2520fixed%2520size%2520that%2520operates%2520solely%2520through%250Ainternal%2520liquid%2520redistribution%2520among%2520three%2520interconnected%2520bistable%2520snap-through%250Achambers.%2520When%2520the%2520top%2520sensing%2520chamber%2520deforms%2520upon%2520contact%252C%2520the%2520displaced%250Aliquid%2520triggers%2520snap-through%2520expansion%2520of%2520the%2520grasping%2520chambers%252C%2520enabling%250Astable%2520and%2520size-selective%2520grasping%2520without%2520continuous%2520energy%2520input.%2520The%250Ainternal%2520hydraulic%2520feedback%2520further%2520allows%2520passive%2520adaptation%2520of%2520gripping%250Apressure%2520to%2520object%2520stiffness.%2520This%2520source-free%2520and%2520compact%2520design%2520opens%2520new%250Apossibilities%2520for%2520lightweight%252C%2520stiffness-adaptive%2520fluid-driven%2520manipulation%2520in%250Asoft%2520robotics%252C%2520providing%2520a%2520feasible%2520approach%2520for%2520targeted%2520size-specific%250Asampling%2520and%2520operation%2520in%2520underwater%2520and%2520field%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Source-Free%20Bistable%20Fluidic%20Gripper%20for%20Size-Selective%20and%0A%20%20Stiffness-Adaptive%20Grasping&entry.906535625=Zhihang%20Qin%20and%20Yueheng%20Zhang%20and%20Wan%20Su%20and%20Linxin%20Hou%20and%20Shenghao%20Zhou%20and%20Zhijun%20Chen%20and%20Yu%20Jun%20Tan%20and%20Cecilia%20Laschi&entry.1292438233=%20%20Conventional%20fluid-driven%20soft%20grippers%20typically%20depend%20on%20external%20sources%2C%0Awhich%20limit%20portability%20and%20long-term%20autonomy.%20This%20work%20introduces%20a%0Aself-contained%20soft%20gripper%20with%20fixed%20size%20that%20operates%20solely%20through%0Ainternal%20liquid%20redistribution%20among%20three%20interconnected%20bistable%20snap-through%0Achambers.%20When%20the%20top%20sensing%20chamber%20deforms%20upon%20contact%2C%20the%20displaced%0Aliquid%20triggers%20snap-through%20expansion%20of%20the%20grasping%20chambers%2C%20enabling%0Astable%20and%20size-selective%20grasping%20without%20continuous%20energy%20input.%20The%0Ainternal%20hydraulic%20feedback%20further%20allows%20passive%20adaptation%20of%20gripping%0Apressure%20to%20object%20stiffness.%20This%20source-free%20and%20compact%20design%20opens%20new%0Apossibilities%20for%20lightweight%2C%20stiffness-adaptive%20fluid-driven%20manipulation%20in%0Asoft%20robotics%2C%20providing%20a%20feasible%20approach%20for%20targeted%20size-specific%0Asampling%20and%20operation%20in%20underwater%20and%20field%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03691v1&entry.124074799=Read"},
{"title": "PerfDojo: Automated ML Library Generation for Heterogeneous\n  Architectures", "author": "Andrei Ivanov and Siyuan Shen and Gioele Gottardo and Marcin Chrapek and Afif Boudaoud and Timo Schneider and Luca Benini and Torsten Hoefler", "abstract": "  The increasing complexity of machine learning models and the proliferation of\ndiverse hardware architectures (CPUs, GPUs, accelerators) make achieving\noptimal performance a significant challenge. Heterogeneity in instruction sets,\nspecialized kernel requirements for different data types and model features\n(e.g., sparsity, quantization), and architecture-specific optimizations\ncomplicate performance tuning. Manual optimization is resource-intensive, while\nexisting automatic approaches often rely on complex hardware-specific\nheuristics and uninterpretable intermediate representations, hindering\nperformance portability. We introduce PerfLLM, a novel automatic optimization\nmethodology leveraging Large Language Models (LLMs) and Reinforcement Learning\n(RL). Central to this is PerfDojo, an environment framing optimization as an RL\ngame using a human-readable, mathematically-inspired code representation that\nguarantees semantic validity through transformations. This allows effective\noptimization without prior hardware knowledge, facilitating both human analysis\nand RL agent training. We demonstrate PerfLLM's ability to achieve significant\nperformance gains across diverse CPU (x86, Arm, RISC-V) and GPU architectures.\n", "link": "http://arxiv.org/abs/2511.03586v1", "date": "2025-11-05", "relevancy": 1.4261, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.499}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4697}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PerfDojo%3A%20Automated%20ML%20Library%20Generation%20for%20Heterogeneous%0A%20%20Architectures&body=Title%3A%20PerfDojo%3A%20Automated%20ML%20Library%20Generation%20for%20Heterogeneous%0A%20%20Architectures%0AAuthor%3A%20Andrei%20Ivanov%20and%20Siyuan%20Shen%20and%20Gioele%20Gottardo%20and%20Marcin%20Chrapek%20and%20Afif%20Boudaoud%20and%20Timo%20Schneider%20and%20Luca%20Benini%20and%20Torsten%20Hoefler%0AAbstract%3A%20%20%20The%20increasing%20complexity%20of%20machine%20learning%20models%20and%20the%20proliferation%20of%0Adiverse%20hardware%20architectures%20%28CPUs%2C%20GPUs%2C%20accelerators%29%20make%20achieving%0Aoptimal%20performance%20a%20significant%20challenge.%20Heterogeneity%20in%20instruction%20sets%2C%0Aspecialized%20kernel%20requirements%20for%20different%20data%20types%20and%20model%20features%0A%28e.g.%2C%20sparsity%2C%20quantization%29%2C%20and%20architecture-specific%20optimizations%0Acomplicate%20performance%20tuning.%20Manual%20optimization%20is%20resource-intensive%2C%20while%0Aexisting%20automatic%20approaches%20often%20rely%20on%20complex%20hardware-specific%0Aheuristics%20and%20uninterpretable%20intermediate%20representations%2C%20hindering%0Aperformance%20portability.%20We%20introduce%20PerfLLM%2C%20a%20novel%20automatic%20optimization%0Amethodology%20leveraging%20Large%20Language%20Models%20%28LLMs%29%20and%20Reinforcement%20Learning%0A%28RL%29.%20Central%20to%20this%20is%20PerfDojo%2C%20an%20environment%20framing%20optimization%20as%20an%20RL%0Agame%20using%20a%20human-readable%2C%20mathematically-inspired%20code%20representation%20that%0Aguarantees%20semantic%20validity%20through%20transformations.%20This%20allows%20effective%0Aoptimization%20without%20prior%20hardware%20knowledge%2C%20facilitating%20both%20human%20analysis%0Aand%20RL%20agent%20training.%20We%20demonstrate%20PerfLLM%27s%20ability%20to%20achieve%20significant%0Aperformance%20gains%20across%20diverse%20CPU%20%28x86%2C%20Arm%2C%20RISC-V%29%20and%20GPU%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerfDojo%253A%2520Automated%2520ML%2520Library%2520Generation%2520for%2520Heterogeneous%250A%2520%2520Architectures%26entry.906535625%3DAndrei%2520Ivanov%2520and%2520Siyuan%2520Shen%2520and%2520Gioele%2520Gottardo%2520and%2520Marcin%2520Chrapek%2520and%2520Afif%2520Boudaoud%2520and%2520Timo%2520Schneider%2520and%2520Luca%2520Benini%2520and%2520Torsten%2520Hoefler%26entry.1292438233%3D%2520%2520The%2520increasing%2520complexity%2520of%2520machine%2520learning%2520models%2520and%2520the%2520proliferation%2520of%250Adiverse%2520hardware%2520architectures%2520%2528CPUs%252C%2520GPUs%252C%2520accelerators%2529%2520make%2520achieving%250Aoptimal%2520performance%2520a%2520significant%2520challenge.%2520Heterogeneity%2520in%2520instruction%2520sets%252C%250Aspecialized%2520kernel%2520requirements%2520for%2520different%2520data%2520types%2520and%2520model%2520features%250A%2528e.g.%252C%2520sparsity%252C%2520quantization%2529%252C%2520and%2520architecture-specific%2520optimizations%250Acomplicate%2520performance%2520tuning.%2520Manual%2520optimization%2520is%2520resource-intensive%252C%2520while%250Aexisting%2520automatic%2520approaches%2520often%2520rely%2520on%2520complex%2520hardware-specific%250Aheuristics%2520and%2520uninterpretable%2520intermediate%2520representations%252C%2520hindering%250Aperformance%2520portability.%2520We%2520introduce%2520PerfLLM%252C%2520a%2520novel%2520automatic%2520optimization%250Amethodology%2520leveraging%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520Reinforcement%2520Learning%250A%2528RL%2529.%2520Central%2520to%2520this%2520is%2520PerfDojo%252C%2520an%2520environment%2520framing%2520optimization%2520as%2520an%2520RL%250Agame%2520using%2520a%2520human-readable%252C%2520mathematically-inspired%2520code%2520representation%2520that%250Aguarantees%2520semantic%2520validity%2520through%2520transformations.%2520This%2520allows%2520effective%250Aoptimization%2520without%2520prior%2520hardware%2520knowledge%252C%2520facilitating%2520both%2520human%2520analysis%250Aand%2520RL%2520agent%2520training.%2520We%2520demonstrate%2520PerfLLM%2527s%2520ability%2520to%2520achieve%2520significant%250Aperformance%2520gains%2520across%2520diverse%2520CPU%2520%2528x86%252C%2520Arm%252C%2520RISC-V%2529%2520and%2520GPU%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PerfDojo%3A%20Automated%20ML%20Library%20Generation%20for%20Heterogeneous%0A%20%20Architectures&entry.906535625=Andrei%20Ivanov%20and%20Siyuan%20Shen%20and%20Gioele%20Gottardo%20and%20Marcin%20Chrapek%20and%20Afif%20Boudaoud%20and%20Timo%20Schneider%20and%20Luca%20Benini%20and%20Torsten%20Hoefler&entry.1292438233=%20%20The%20increasing%20complexity%20of%20machine%20learning%20models%20and%20the%20proliferation%20of%0Adiverse%20hardware%20architectures%20%28CPUs%2C%20GPUs%2C%20accelerators%29%20make%20achieving%0Aoptimal%20performance%20a%20significant%20challenge.%20Heterogeneity%20in%20instruction%20sets%2C%0Aspecialized%20kernel%20requirements%20for%20different%20data%20types%20and%20model%20features%0A%28e.g.%2C%20sparsity%2C%20quantization%29%2C%20and%20architecture-specific%20optimizations%0Acomplicate%20performance%20tuning.%20Manual%20optimization%20is%20resource-intensive%2C%20while%0Aexisting%20automatic%20approaches%20often%20rely%20on%20complex%20hardware-specific%0Aheuristics%20and%20uninterpretable%20intermediate%20representations%2C%20hindering%0Aperformance%20portability.%20We%20introduce%20PerfLLM%2C%20a%20novel%20automatic%20optimization%0Amethodology%20leveraging%20Large%20Language%20Models%20%28LLMs%29%20and%20Reinforcement%20Learning%0A%28RL%29.%20Central%20to%20this%20is%20PerfDojo%2C%20an%20environment%20framing%20optimization%20as%20an%20RL%0Agame%20using%20a%20human-readable%2C%20mathematically-inspired%20code%20representation%20that%0Aguarantees%20semantic%20validity%20through%20transformations.%20This%20allows%20effective%0Aoptimization%20without%20prior%20hardware%20knowledge%2C%20facilitating%20both%20human%20analysis%0Aand%20RL%20agent%20training.%20We%20demonstrate%20PerfLLM%27s%20ability%20to%20achieve%20significant%0Aperformance%20gains%20across%20diverse%20CPU%20%28x86%2C%20Arm%2C%20RISC-V%29%20and%20GPU%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03586v1&entry.124074799=Read"},
{"title": "Explaining Human Choice Probabilities with Simple Vector Representations", "author": "Peter DiBerardino and Britt Anderson", "abstract": "  When people pursue rewards in stochastic environments, they often match their\nchoice frequencies to the observed target frequencies, even when this policy is\ndemonstrably sub-optimal. We used a ``hide and seek'' task to evaluate this\nbehavior under conditions where pursuit (seeking) could be toggled to avoidance\n(hiding), while leaving the probability distribution fixed, or varying\ncomplexity by changing the number of possible choices. We developed a model for\nparticipant choice built from choice frequency histograms treated as vectors.\nWe posited the existence of a probability antimatching strategy for avoidance\n(hiding) rounds, and formalized this as a vector reflection of probability\nmatching. We found that only two basis policies: matching/antimatching and\nmaximizing/minimizing were sufficient to account for participant choices across\na range of room numbers and opponent probability distributions. This schema\nrequires only that people have the ability to remember the relative frequency\nof the different outcomes. With this knowledge simple operations can construct\nthe maximizing and minimizing policies as well as matching and antimatching\nstrategies. A mixture of these two policies captures human choice patterns in a\nstochastic environment.\n", "link": "http://arxiv.org/abs/2511.03643v1", "date": "2025-11-05", "relevancy": 1.4199, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5064}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4775}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explaining%20Human%20Choice%20Probabilities%20with%20Simple%20Vector%20Representations&body=Title%3A%20Explaining%20Human%20Choice%20Probabilities%20with%20Simple%20Vector%20Representations%0AAuthor%3A%20Peter%20DiBerardino%20and%20Britt%20Anderson%0AAbstract%3A%20%20%20When%20people%20pursue%20rewards%20in%20stochastic%20environments%2C%20they%20often%20match%20their%0Achoice%20frequencies%20to%20the%20observed%20target%20frequencies%2C%20even%20when%20this%20policy%20is%0Ademonstrably%20sub-optimal.%20We%20used%20a%20%60%60hide%20and%20seek%27%27%20task%20to%20evaluate%20this%0Abehavior%20under%20conditions%20where%20pursuit%20%28seeking%29%20could%20be%20toggled%20to%20avoidance%0A%28hiding%29%2C%20while%20leaving%20the%20probability%20distribution%20fixed%2C%20or%20varying%0Acomplexity%20by%20changing%20the%20number%20of%20possible%20choices.%20We%20developed%20a%20model%20for%0Aparticipant%20choice%20built%20from%20choice%20frequency%20histograms%20treated%20as%20vectors.%0AWe%20posited%20the%20existence%20of%20a%20probability%20antimatching%20strategy%20for%20avoidance%0A%28hiding%29%20rounds%2C%20and%20formalized%20this%20as%20a%20vector%20reflection%20of%20probability%0Amatching.%20We%20found%20that%20only%20two%20basis%20policies%3A%20matching/antimatching%20and%0Amaximizing/minimizing%20were%20sufficient%20to%20account%20for%20participant%20choices%20across%0Aa%20range%20of%20room%20numbers%20and%20opponent%20probability%20distributions.%20This%20schema%0Arequires%20only%20that%20people%20have%20the%20ability%20to%20remember%20the%20relative%20frequency%0Aof%20the%20different%20outcomes.%20With%20this%20knowledge%20simple%20operations%20can%20construct%0Athe%20maximizing%20and%20minimizing%20policies%20as%20well%20as%20matching%20and%20antimatching%0Astrategies.%20A%20mixture%20of%20these%20two%20policies%20captures%20human%20choice%20patterns%20in%20a%0Astochastic%20environment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplaining%2520Human%2520Choice%2520Probabilities%2520with%2520Simple%2520Vector%2520Representations%26entry.906535625%3DPeter%2520DiBerardino%2520and%2520Britt%2520Anderson%26entry.1292438233%3D%2520%2520When%2520people%2520pursue%2520rewards%2520in%2520stochastic%2520environments%252C%2520they%2520often%2520match%2520their%250Achoice%2520frequencies%2520to%2520the%2520observed%2520target%2520frequencies%252C%2520even%2520when%2520this%2520policy%2520is%250Ademonstrably%2520sub-optimal.%2520We%2520used%2520a%2520%2560%2560hide%2520and%2520seek%2527%2527%2520task%2520to%2520evaluate%2520this%250Abehavior%2520under%2520conditions%2520where%2520pursuit%2520%2528seeking%2529%2520could%2520be%2520toggled%2520to%2520avoidance%250A%2528hiding%2529%252C%2520while%2520leaving%2520the%2520probability%2520distribution%2520fixed%252C%2520or%2520varying%250Acomplexity%2520by%2520changing%2520the%2520number%2520of%2520possible%2520choices.%2520We%2520developed%2520a%2520model%2520for%250Aparticipant%2520choice%2520built%2520from%2520choice%2520frequency%2520histograms%2520treated%2520as%2520vectors.%250AWe%2520posited%2520the%2520existence%2520of%2520a%2520probability%2520antimatching%2520strategy%2520for%2520avoidance%250A%2528hiding%2529%2520rounds%252C%2520and%2520formalized%2520this%2520as%2520a%2520vector%2520reflection%2520of%2520probability%250Amatching.%2520We%2520found%2520that%2520only%2520two%2520basis%2520policies%253A%2520matching/antimatching%2520and%250Amaximizing/minimizing%2520were%2520sufficient%2520to%2520account%2520for%2520participant%2520choices%2520across%250Aa%2520range%2520of%2520room%2520numbers%2520and%2520opponent%2520probability%2520distributions.%2520This%2520schema%250Arequires%2520only%2520that%2520people%2520have%2520the%2520ability%2520to%2520remember%2520the%2520relative%2520frequency%250Aof%2520the%2520different%2520outcomes.%2520With%2520this%2520knowledge%2520simple%2520operations%2520can%2520construct%250Athe%2520maximizing%2520and%2520minimizing%2520policies%2520as%2520well%2520as%2520matching%2520and%2520antimatching%250Astrategies.%2520A%2520mixture%2520of%2520these%2520two%2520policies%2520captures%2520human%2520choice%2520patterns%2520in%2520a%250Astochastic%2520environment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explaining%20Human%20Choice%20Probabilities%20with%20Simple%20Vector%20Representations&entry.906535625=Peter%20DiBerardino%20and%20Britt%20Anderson&entry.1292438233=%20%20When%20people%20pursue%20rewards%20in%20stochastic%20environments%2C%20they%20often%20match%20their%0Achoice%20frequencies%20to%20the%20observed%20target%20frequencies%2C%20even%20when%20this%20policy%20is%0Ademonstrably%20sub-optimal.%20We%20used%20a%20%60%60hide%20and%20seek%27%27%20task%20to%20evaluate%20this%0Abehavior%20under%20conditions%20where%20pursuit%20%28seeking%29%20could%20be%20toggled%20to%20avoidance%0A%28hiding%29%2C%20while%20leaving%20the%20probability%20distribution%20fixed%2C%20or%20varying%0Acomplexity%20by%20changing%20the%20number%20of%20possible%20choices.%20We%20developed%20a%20model%20for%0Aparticipant%20choice%20built%20from%20choice%20frequency%20histograms%20treated%20as%20vectors.%0AWe%20posited%20the%20existence%20of%20a%20probability%20antimatching%20strategy%20for%20avoidance%0A%28hiding%29%20rounds%2C%20and%20formalized%20this%20as%20a%20vector%20reflection%20of%20probability%0Amatching.%20We%20found%20that%20only%20two%20basis%20policies%3A%20matching/antimatching%20and%0Amaximizing/minimizing%20were%20sufficient%20to%20account%20for%20participant%20choices%20across%0Aa%20range%20of%20room%20numbers%20and%20opponent%20probability%20distributions.%20This%20schema%0Arequires%20only%20that%20people%20have%20the%20ability%20to%20remember%20the%20relative%20frequency%0Aof%20the%20different%20outcomes.%20With%20this%20knowledge%20simple%20operations%20can%20construct%0Athe%20maximizing%20and%20minimizing%20policies%20as%20well%20as%20matching%20and%20antimatching%0Astrategies.%20A%20mixture%20of%20these%20two%20policies%20captures%20human%20choice%20patterns%20in%20a%0Astochastic%20environment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03643v1&entry.124074799=Read"},
{"title": "Quantifying Weighted Morphological Content of Large-Scale Structures via\n  Simulation-Based Inference", "author": "M. H. Jalali Kanafi and S. M. S. Movahed", "abstract": "  In this work, we perform a simulation-based forecasting analysis to compare\nthe constraining power of two higher-order summary statistics of the\nlarge-scale structure (LSS), the Minkowski Functionals (MFs) and the\nConditional Moments of Derivative (CMD), with a particular focus on their\nsensitivity to nonlinear and anisotropic features in redshift-space. Our\nanalysis relies on halo catalogs from the Big Sobol Sequence(BSQ) simulations\nat redshift $z=0.5$, employing a likelihood-free inference framework\nimplemented via neural posterior estimation. At the fiducial cosmology of the\nQuijote simulations $(\\Omega_{m}=0.3175,\\,\\sigma_{8}=0.834)$, and for the\nsmoothing scale $R=15\\,h^{-1}$Mpc, we find that the CMD yields tighter\nforecasts for $(\\Omega_{m}},\\,\\sigma_{8})$ than the zeroth- to third-order MFs\ncomponents, improving the constraint precision by ${\\sim}(44\\%,\\,52\\%)$,\n${\\sim}(30\\%,\\,45\\%)$, ${\\sim}(27\\%,\\,17\\%)$, and ${\\sim}(26\\%,\\,17\\%)$,\nrespectively. A joint configuration combining the MFs and CMD further enhances\nthe precision by approximately ${\\sim}27\\%$ compared to the standard MFs alone,\nhighlighting the complementary anisotropy-sensitive information captured by the\nCMD in contrast to the scalar morphological content encapsulated by the MFs. We\nfurther extend the forecasting analysis to a continuous range of cosmological\nparameter values and multiple smoothing scales. Our results show that, although\nthe absolute forecast uncertainty for each component of summary statistics\ndepends on the underlying parameter values and the adopted smoothing scale, the\nrelative constraining power among the summary statistics remains nearly\nconstant throughout.\n", "link": "http://arxiv.org/abs/2511.03636v1", "date": "2025-11-05", "relevancy": 1.4153, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4925}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4683}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantifying%20Weighted%20Morphological%20Content%20of%20Large-Scale%20Structures%20via%0A%20%20Simulation-Based%20Inference&body=Title%3A%20Quantifying%20Weighted%20Morphological%20Content%20of%20Large-Scale%20Structures%20via%0A%20%20Simulation-Based%20Inference%0AAuthor%3A%20M.%20H.%20Jalali%20Kanafi%20and%20S.%20M.%20S.%20Movahed%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20perform%20a%20simulation-based%20forecasting%20analysis%20to%20compare%0Athe%20constraining%20power%20of%20two%20higher-order%20summary%20statistics%20of%20the%0Alarge-scale%20structure%20%28LSS%29%2C%20the%20Minkowski%20Functionals%20%28MFs%29%20and%20the%0AConditional%20Moments%20of%20Derivative%20%28CMD%29%2C%20with%20a%20particular%20focus%20on%20their%0Asensitivity%20to%20nonlinear%20and%20anisotropic%20features%20in%20redshift-space.%20Our%0Aanalysis%20relies%20on%20halo%20catalogs%20from%20the%20Big%20Sobol%20Sequence%28BSQ%29%20simulations%0Aat%20redshift%20%24z%3D0.5%24%2C%20employing%20a%20likelihood-free%20inference%20framework%0Aimplemented%20via%20neural%20posterior%20estimation.%20At%20the%20fiducial%20cosmology%20of%20the%0AQuijote%20simulations%20%24%28%5COmega_%7Bm%7D%3D0.3175%2C%5C%2C%5Csigma_%7B8%7D%3D0.834%29%24%2C%20and%20for%20the%0Asmoothing%20scale%20%24R%3D15%5C%2Ch%5E%7B-1%7D%24Mpc%2C%20we%20find%20that%20the%20CMD%20yields%20tighter%0Aforecasts%20for%20%24%28%5COmega_%7Bm%7D%7D%2C%5C%2C%5Csigma_%7B8%7D%29%24%20than%20the%20zeroth-%20to%20third-order%20MFs%0Acomponents%2C%20improving%20the%20constraint%20precision%20by%20%24%7B%5Csim%7D%2844%5C%25%2C%5C%2C52%5C%25%29%24%2C%0A%24%7B%5Csim%7D%2830%5C%25%2C%5C%2C45%5C%25%29%24%2C%20%24%7B%5Csim%7D%2827%5C%25%2C%5C%2C17%5C%25%29%24%2C%20and%20%24%7B%5Csim%7D%2826%5C%25%2C%5C%2C17%5C%25%29%24%2C%0Arespectively.%20A%20joint%20configuration%20combining%20the%20MFs%20and%20CMD%20further%20enhances%0Athe%20precision%20by%20approximately%20%24%7B%5Csim%7D27%5C%25%24%20compared%20to%20the%20standard%20MFs%20alone%2C%0Ahighlighting%20the%20complementary%20anisotropy-sensitive%20information%20captured%20by%20the%0ACMD%20in%20contrast%20to%20the%20scalar%20morphological%20content%20encapsulated%20by%20the%20MFs.%20We%0Afurther%20extend%20the%20forecasting%20analysis%20to%20a%20continuous%20range%20of%20cosmological%0Aparameter%20values%20and%20multiple%20smoothing%20scales.%20Our%20results%20show%20that%2C%20although%0Athe%20absolute%20forecast%20uncertainty%20for%20each%20component%20of%20summary%20statistics%0Adepends%20on%20the%20underlying%20parameter%20values%20and%20the%20adopted%20smoothing%20scale%2C%20the%0Arelative%20constraining%20power%20among%20the%20summary%20statistics%20remains%20nearly%0Aconstant%20throughout.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03636v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantifying%2520Weighted%2520Morphological%2520Content%2520of%2520Large-Scale%2520Structures%2520via%250A%2520%2520Simulation-Based%2520Inference%26entry.906535625%3DM.%2520H.%2520Jalali%2520Kanafi%2520and%2520S.%2520M.%2520S.%2520Movahed%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520perform%2520a%2520simulation-based%2520forecasting%2520analysis%2520to%2520compare%250Athe%2520constraining%2520power%2520of%2520two%2520higher-order%2520summary%2520statistics%2520of%2520the%250Alarge-scale%2520structure%2520%2528LSS%2529%252C%2520the%2520Minkowski%2520Functionals%2520%2528MFs%2529%2520and%2520the%250AConditional%2520Moments%2520of%2520Derivative%2520%2528CMD%2529%252C%2520with%2520a%2520particular%2520focus%2520on%2520their%250Asensitivity%2520to%2520nonlinear%2520and%2520anisotropic%2520features%2520in%2520redshift-space.%2520Our%250Aanalysis%2520relies%2520on%2520halo%2520catalogs%2520from%2520the%2520Big%2520Sobol%2520Sequence%2528BSQ%2529%2520simulations%250Aat%2520redshift%2520%2524z%253D0.5%2524%252C%2520employing%2520a%2520likelihood-free%2520inference%2520framework%250Aimplemented%2520via%2520neural%2520posterior%2520estimation.%2520At%2520the%2520fiducial%2520cosmology%2520of%2520the%250AQuijote%2520simulations%2520%2524%2528%255COmega_%257Bm%257D%253D0.3175%252C%255C%252C%255Csigma_%257B8%257D%253D0.834%2529%2524%252C%2520and%2520for%2520the%250Asmoothing%2520scale%2520%2524R%253D15%255C%252Ch%255E%257B-1%257D%2524Mpc%252C%2520we%2520find%2520that%2520the%2520CMD%2520yields%2520tighter%250Aforecasts%2520for%2520%2524%2528%255COmega_%257Bm%257D%257D%252C%255C%252C%255Csigma_%257B8%257D%2529%2524%2520than%2520the%2520zeroth-%2520to%2520third-order%2520MFs%250Acomponents%252C%2520improving%2520the%2520constraint%2520precision%2520by%2520%2524%257B%255Csim%257D%252844%255C%2525%252C%255C%252C52%255C%2525%2529%2524%252C%250A%2524%257B%255Csim%257D%252830%255C%2525%252C%255C%252C45%255C%2525%2529%2524%252C%2520%2524%257B%255Csim%257D%252827%255C%2525%252C%255C%252C17%255C%2525%2529%2524%252C%2520and%2520%2524%257B%255Csim%257D%252826%255C%2525%252C%255C%252C17%255C%2525%2529%2524%252C%250Arespectively.%2520A%2520joint%2520configuration%2520combining%2520the%2520MFs%2520and%2520CMD%2520further%2520enhances%250Athe%2520precision%2520by%2520approximately%2520%2524%257B%255Csim%257D27%255C%2525%2524%2520compared%2520to%2520the%2520standard%2520MFs%2520alone%252C%250Ahighlighting%2520the%2520complementary%2520anisotropy-sensitive%2520information%2520captured%2520by%2520the%250ACMD%2520in%2520contrast%2520to%2520the%2520scalar%2520morphological%2520content%2520encapsulated%2520by%2520the%2520MFs.%2520We%250Afurther%2520extend%2520the%2520forecasting%2520analysis%2520to%2520a%2520continuous%2520range%2520of%2520cosmological%250Aparameter%2520values%2520and%2520multiple%2520smoothing%2520scales.%2520Our%2520results%2520show%2520that%252C%2520although%250Athe%2520absolute%2520forecast%2520uncertainty%2520for%2520each%2520component%2520of%2520summary%2520statistics%250Adepends%2520on%2520the%2520underlying%2520parameter%2520values%2520and%2520the%2520adopted%2520smoothing%2520scale%252C%2520the%250Arelative%2520constraining%2520power%2520among%2520the%2520summary%2520statistics%2520remains%2520nearly%250Aconstant%2520throughout.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03636v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20Weighted%20Morphological%20Content%20of%20Large-Scale%20Structures%20via%0A%20%20Simulation-Based%20Inference&entry.906535625=M.%20H.%20Jalali%20Kanafi%20and%20S.%20M.%20S.%20Movahed&entry.1292438233=%20%20In%20this%20work%2C%20we%20perform%20a%20simulation-based%20forecasting%20analysis%20to%20compare%0Athe%20constraining%20power%20of%20two%20higher-order%20summary%20statistics%20of%20the%0Alarge-scale%20structure%20%28LSS%29%2C%20the%20Minkowski%20Functionals%20%28MFs%29%20and%20the%0AConditional%20Moments%20of%20Derivative%20%28CMD%29%2C%20with%20a%20particular%20focus%20on%20their%0Asensitivity%20to%20nonlinear%20and%20anisotropic%20features%20in%20redshift-space.%20Our%0Aanalysis%20relies%20on%20halo%20catalogs%20from%20the%20Big%20Sobol%20Sequence%28BSQ%29%20simulations%0Aat%20redshift%20%24z%3D0.5%24%2C%20employing%20a%20likelihood-free%20inference%20framework%0Aimplemented%20via%20neural%20posterior%20estimation.%20At%20the%20fiducial%20cosmology%20of%20the%0AQuijote%20simulations%20%24%28%5COmega_%7Bm%7D%3D0.3175%2C%5C%2C%5Csigma_%7B8%7D%3D0.834%29%24%2C%20and%20for%20the%0Asmoothing%20scale%20%24R%3D15%5C%2Ch%5E%7B-1%7D%24Mpc%2C%20we%20find%20that%20the%20CMD%20yields%20tighter%0Aforecasts%20for%20%24%28%5COmega_%7Bm%7D%7D%2C%5C%2C%5Csigma_%7B8%7D%29%24%20than%20the%20zeroth-%20to%20third-order%20MFs%0Acomponents%2C%20improving%20the%20constraint%20precision%20by%20%24%7B%5Csim%7D%2844%5C%25%2C%5C%2C52%5C%25%29%24%2C%0A%24%7B%5Csim%7D%2830%5C%25%2C%5C%2C45%5C%25%29%24%2C%20%24%7B%5Csim%7D%2827%5C%25%2C%5C%2C17%5C%25%29%24%2C%20and%20%24%7B%5Csim%7D%2826%5C%25%2C%5C%2C17%5C%25%29%24%2C%0Arespectively.%20A%20joint%20configuration%20combining%20the%20MFs%20and%20CMD%20further%20enhances%0Athe%20precision%20by%20approximately%20%24%7B%5Csim%7D27%5C%25%24%20compared%20to%20the%20standard%20MFs%20alone%2C%0Ahighlighting%20the%20complementary%20anisotropy-sensitive%20information%20captured%20by%20the%0ACMD%20in%20contrast%20to%20the%20scalar%20morphological%20content%20encapsulated%20by%20the%20MFs.%20We%0Afurther%20extend%20the%20forecasting%20analysis%20to%20a%20continuous%20range%20of%20cosmological%0Aparameter%20values%20and%20multiple%20smoothing%20scales.%20Our%20results%20show%20that%2C%20although%0Athe%20absolute%20forecast%20uncertainty%20for%20each%20component%20of%20summary%20statistics%0Adepends%20on%20the%20underlying%20parameter%20values%20and%20the%20adopted%20smoothing%20scale%2C%20the%0Arelative%20constraining%20power%20among%20the%20summary%20statistics%20remains%20nearly%0Aconstant%20throughout.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03636v1&entry.124074799=Read"},
{"title": "Structured Matrix Scaling for Multi-Class Calibration", "author": "Eug\u00e8ne Berta and David Holzm\u00fcller and Michael I. Jordan and Francis Bach", "abstract": "  Post-hoc recalibration methods are widely used to ensure that classifiers\nprovide faithful probability estimates. We argue that parametric recalibration\nfunctions based on logistic regression can be motivated from a simple\ntheoretical setting for both binary and multiclass classification. This insight\nmotivates the use of more expressive calibration methods beyond standard\ntemperature scaling. For multi-class calibration however, a key challenge lies\nin the increasing number of parameters introduced by more complex models, often\ncoupled with limited calibration data, which can lead to overfitting. Through\nextensive experiments, we demonstrate that the resulting bias-variance tradeoff\ncan be effectively managed by structured regularization, robust preprocessing\nand efficient optimization. The resulting methods lead to substantial gains\nover existing logistic-based calibration techniques. We provide efficient and\neasy-to-use open-source implementations of our methods, making them an\nattractive alternative to common temperature, vector, and matrix scaling\nimplementations.\n", "link": "http://arxiv.org/abs/2511.03685v1", "date": "2025-11-05", "relevancy": 1.3825, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4895}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4554}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structured%20Matrix%20Scaling%20for%20Multi-Class%20Calibration&body=Title%3A%20Structured%20Matrix%20Scaling%20for%20Multi-Class%20Calibration%0AAuthor%3A%20Eug%C3%A8ne%20Berta%20and%20David%20Holzm%C3%BCller%20and%20Michael%20I.%20Jordan%20and%20Francis%20Bach%0AAbstract%3A%20%20%20Post-hoc%20recalibration%20methods%20are%20widely%20used%20to%20ensure%20that%20classifiers%0Aprovide%20faithful%20probability%20estimates.%20We%20argue%20that%20parametric%20recalibration%0Afunctions%20based%20on%20logistic%20regression%20can%20be%20motivated%20from%20a%20simple%0Atheoretical%20setting%20for%20both%20binary%20and%20multiclass%20classification.%20This%20insight%0Amotivates%20the%20use%20of%20more%20expressive%20calibration%20methods%20beyond%20standard%0Atemperature%20scaling.%20For%20multi-class%20calibration%20however%2C%20a%20key%20challenge%20lies%0Ain%20the%20increasing%20number%20of%20parameters%20introduced%20by%20more%20complex%20models%2C%20often%0Acoupled%20with%20limited%20calibration%20data%2C%20which%20can%20lead%20to%20overfitting.%20Through%0Aextensive%20experiments%2C%20we%20demonstrate%20that%20the%20resulting%20bias-variance%20tradeoff%0Acan%20be%20effectively%20managed%20by%20structured%20regularization%2C%20robust%20preprocessing%0Aand%20efficient%20optimization.%20The%20resulting%20methods%20lead%20to%20substantial%20gains%0Aover%20existing%20logistic-based%20calibration%20techniques.%20We%20provide%20efficient%20and%0Aeasy-to-use%20open-source%20implementations%20of%20our%20methods%2C%20making%20them%20an%0Aattractive%20alternative%20to%20common%20temperature%2C%20vector%2C%20and%20matrix%20scaling%0Aimplementations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03685v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructured%2520Matrix%2520Scaling%2520for%2520Multi-Class%2520Calibration%26entry.906535625%3DEug%25C3%25A8ne%2520Berta%2520and%2520David%2520Holzm%25C3%25BCller%2520and%2520Michael%2520I.%2520Jordan%2520and%2520Francis%2520Bach%26entry.1292438233%3D%2520%2520Post-hoc%2520recalibration%2520methods%2520are%2520widely%2520used%2520to%2520ensure%2520that%2520classifiers%250Aprovide%2520faithful%2520probability%2520estimates.%2520We%2520argue%2520that%2520parametric%2520recalibration%250Afunctions%2520based%2520on%2520logistic%2520regression%2520can%2520be%2520motivated%2520from%2520a%2520simple%250Atheoretical%2520setting%2520for%2520both%2520binary%2520and%2520multiclass%2520classification.%2520This%2520insight%250Amotivates%2520the%2520use%2520of%2520more%2520expressive%2520calibration%2520methods%2520beyond%2520standard%250Atemperature%2520scaling.%2520For%2520multi-class%2520calibration%2520however%252C%2520a%2520key%2520challenge%2520lies%250Ain%2520the%2520increasing%2520number%2520of%2520parameters%2520introduced%2520by%2520more%2520complex%2520models%252C%2520often%250Acoupled%2520with%2520limited%2520calibration%2520data%252C%2520which%2520can%2520lead%2520to%2520overfitting.%2520Through%250Aextensive%2520experiments%252C%2520we%2520demonstrate%2520that%2520the%2520resulting%2520bias-variance%2520tradeoff%250Acan%2520be%2520effectively%2520managed%2520by%2520structured%2520regularization%252C%2520robust%2520preprocessing%250Aand%2520efficient%2520optimization.%2520The%2520resulting%2520methods%2520lead%2520to%2520substantial%2520gains%250Aover%2520existing%2520logistic-based%2520calibration%2520techniques.%2520We%2520provide%2520efficient%2520and%250Aeasy-to-use%2520open-source%2520implementations%2520of%2520our%2520methods%252C%2520making%2520them%2520an%250Aattractive%2520alternative%2520to%2520common%2520temperature%252C%2520vector%252C%2520and%2520matrix%2520scaling%250Aimplementations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03685v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structured%20Matrix%20Scaling%20for%20Multi-Class%20Calibration&entry.906535625=Eug%C3%A8ne%20Berta%20and%20David%20Holzm%C3%BCller%20and%20Michael%20I.%20Jordan%20and%20Francis%20Bach&entry.1292438233=%20%20Post-hoc%20recalibration%20methods%20are%20widely%20used%20to%20ensure%20that%20classifiers%0Aprovide%20faithful%20probability%20estimates.%20We%20argue%20that%20parametric%20recalibration%0Afunctions%20based%20on%20logistic%20regression%20can%20be%20motivated%20from%20a%20simple%0Atheoretical%20setting%20for%20both%20binary%20and%20multiclass%20classification.%20This%20insight%0Amotivates%20the%20use%20of%20more%20expressive%20calibration%20methods%20beyond%20standard%0Atemperature%20scaling.%20For%20multi-class%20calibration%20however%2C%20a%20key%20challenge%20lies%0Ain%20the%20increasing%20number%20of%20parameters%20introduced%20by%20more%20complex%20models%2C%20often%0Acoupled%20with%20limited%20calibration%20data%2C%20which%20can%20lead%20to%20overfitting.%20Through%0Aextensive%20experiments%2C%20we%20demonstrate%20that%20the%20resulting%20bias-variance%20tradeoff%0Acan%20be%20effectively%20managed%20by%20structured%20regularization%2C%20robust%20preprocessing%0Aand%20efficient%20optimization.%20The%20resulting%20methods%20lead%20to%20substantial%20gains%0Aover%20existing%20logistic-based%20calibration%20techniques.%20We%20provide%20efficient%20and%0Aeasy-to-use%20open-source%20implementations%20of%20our%20methods%2C%20making%20them%20an%0Aattractive%20alternative%20to%20common%20temperature%2C%20vector%2C%20and%20matrix%20scaling%0Aimplementations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03685v1&entry.124074799=Read"},
{"title": "Kosmos: An AI Scientist for Autonomous Discovery", "author": "Ludovico Mitchener and Angela Yiu and Benjamin Chang and Mathieu Bourdenx and Tyler Nadolski and Arvis Sulovari and Eric C. Landsness and Daniel L. Barabasi and Siddharth Narayanan and Nicky Evans and Shriya Reddy and Martha Foiani and Aizad Kamal and Leah P. Shriver and Fang Cao and Asmamaw T. Wassie and Jon M. Laurent and Edwin Melville-Green and Mayk Caldas and Albert Bou and Kaleigh F. Roberts and Sladjana Zagorac and Timothy C. Orr and Miranda E. Orr and Kevin J. Zwezdaryk and Ali E. Ghareeb and Laurie McCoy and Bruna Gomes and Euan A. Ashley and Karen E. Duff and Tonio Buonassisi and Tom Rainforth and Randall J. Bateman and Michael Skarlinski and Samuel G. Rodriques and Michaela M. Hinks and Andrew D. White", "abstract": "  Data-driven scientific discovery requires iterative cycles of literature\nsearch, hypothesis generation, and data analysis. Substantial progress has been\nmade towards AI agents that can automate scientific research, but all such\nagents remain limited in the number of actions they can take before losing\ncoherence, thus limiting the depth of their findings. Here we present Kosmos,\nan AI scientist that automates data-driven discovery. Given an open-ended\nobjective and a dataset, Kosmos runs for up to 12 hours performing cycles of\nparallel data analysis, literature search, and hypothesis generation before\nsynthesizing discoveries into scientific reports. Unlike prior systems, Kosmos\nuses a structured world model to share information between a data analysis\nagent and a literature search agent. The world model enables Kosmos to\ncoherently pursue the specified objective over 200 agent rollouts, collectively\nexecuting an average of 42,000 lines of code and reading 1,500 papers per run.\nKosmos cites all statements in its reports with code or primary literature,\nensuring its reasoning is traceable. Independent scientists found 79.4% of\nstatements in Kosmos reports to be accurate, and collaborators reported that a\nsingle 20-cycle Kosmos run performed the equivalent of 6 months of their own\nresearch time on average. Furthermore, collaborators reported that the number\nof valuable scientific findings generated scales linearly with Kosmos cycles\n(tested up to 20 cycles). We highlight seven discoveries made by Kosmos that\nspan metabolomics, materials science, neuroscience, and statistical genetics.\nThree discoveries independently reproduce findings from preprinted or\nunpublished manuscripts that were not accessed by Kosmos at runtime, while four\nmake novel contributions to the scientific literature.\n", "link": "http://arxiv.org/abs/2511.02824v2", "date": "2025-11-05", "relevancy": 1.3821, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5117}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4503}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kosmos%3A%20An%20AI%20Scientist%20for%20Autonomous%20Discovery&body=Title%3A%20Kosmos%3A%20An%20AI%20Scientist%20for%20Autonomous%20Discovery%0AAuthor%3A%20Ludovico%20Mitchener%20and%20Angela%20Yiu%20and%20Benjamin%20Chang%20and%20Mathieu%20Bourdenx%20and%20Tyler%20Nadolski%20and%20Arvis%20Sulovari%20and%20Eric%20C.%20Landsness%20and%20Daniel%20L.%20Barabasi%20and%20Siddharth%20Narayanan%20and%20Nicky%20Evans%20and%20Shriya%20Reddy%20and%20Martha%20Foiani%20and%20Aizad%20Kamal%20and%20Leah%20P.%20Shriver%20and%20Fang%20Cao%20and%20Asmamaw%20T.%20Wassie%20and%20Jon%20M.%20Laurent%20and%20Edwin%20Melville-Green%20and%20Mayk%20Caldas%20and%20Albert%20Bou%20and%20Kaleigh%20F.%20Roberts%20and%20Sladjana%20Zagorac%20and%20Timothy%20C.%20Orr%20and%20Miranda%20E.%20Orr%20and%20Kevin%20J.%20Zwezdaryk%20and%20Ali%20E.%20Ghareeb%20and%20Laurie%20McCoy%20and%20Bruna%20Gomes%20and%20Euan%20A.%20Ashley%20and%20Karen%20E.%20Duff%20and%20Tonio%20Buonassisi%20and%20Tom%20Rainforth%20and%20Randall%20J.%20Bateman%20and%20Michael%20Skarlinski%20and%20Samuel%20G.%20Rodriques%20and%20Michaela%20M.%20Hinks%20and%20Andrew%20D.%20White%0AAbstract%3A%20%20%20Data-driven%20scientific%20discovery%20requires%20iterative%20cycles%20of%20literature%0Asearch%2C%20hypothesis%20generation%2C%20and%20data%20analysis.%20Substantial%20progress%20has%20been%0Amade%20towards%20AI%20agents%20that%20can%20automate%20scientific%20research%2C%20but%20all%20such%0Aagents%20remain%20limited%20in%20the%20number%20of%20actions%20they%20can%20take%20before%20losing%0Acoherence%2C%20thus%20limiting%20the%20depth%20of%20their%20findings.%20Here%20we%20present%20Kosmos%2C%0Aan%20AI%20scientist%20that%20automates%20data-driven%20discovery.%20Given%20an%20open-ended%0Aobjective%20and%20a%20dataset%2C%20Kosmos%20runs%20for%20up%20to%2012%20hours%20performing%20cycles%20of%0Aparallel%20data%20analysis%2C%20literature%20search%2C%20and%20hypothesis%20generation%20before%0Asynthesizing%20discoveries%20into%20scientific%20reports.%20Unlike%20prior%20systems%2C%20Kosmos%0Auses%20a%20structured%20world%20model%20to%20share%20information%20between%20a%20data%20analysis%0Aagent%20and%20a%20literature%20search%20agent.%20The%20world%20model%20enables%20Kosmos%20to%0Acoherently%20pursue%20the%20specified%20objective%20over%20200%20agent%20rollouts%2C%20collectively%0Aexecuting%20an%20average%20of%2042%2C000%20lines%20of%20code%20and%20reading%201%2C500%20papers%20per%20run.%0AKosmos%20cites%20all%20statements%20in%20its%20reports%20with%20code%20or%20primary%20literature%2C%0Aensuring%20its%20reasoning%20is%20traceable.%20Independent%20scientists%20found%2079.4%25%20of%0Astatements%20in%20Kosmos%20reports%20to%20be%20accurate%2C%20and%20collaborators%20reported%20that%20a%0Asingle%2020-cycle%20Kosmos%20run%20performed%20the%20equivalent%20of%206%20months%20of%20their%20own%0Aresearch%20time%20on%20average.%20Furthermore%2C%20collaborators%20reported%20that%20the%20number%0Aof%20valuable%20scientific%20findings%20generated%20scales%20linearly%20with%20Kosmos%20cycles%0A%28tested%20up%20to%2020%20cycles%29.%20We%20highlight%20seven%20discoveries%20made%20by%20Kosmos%20that%0Aspan%20metabolomics%2C%20materials%20science%2C%20neuroscience%2C%20and%20statistical%20genetics.%0AThree%20discoveries%20independently%20reproduce%20findings%20from%20preprinted%20or%0Aunpublished%20manuscripts%20that%20were%20not%20accessed%20by%20Kosmos%20at%20runtime%2C%20while%20four%0Amake%20novel%20contributions%20to%20the%20scientific%20literature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.02824v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKosmos%253A%2520An%2520AI%2520Scientist%2520for%2520Autonomous%2520Discovery%26entry.906535625%3DLudovico%2520Mitchener%2520and%2520Angela%2520Yiu%2520and%2520Benjamin%2520Chang%2520and%2520Mathieu%2520Bourdenx%2520and%2520Tyler%2520Nadolski%2520and%2520Arvis%2520Sulovari%2520and%2520Eric%2520C.%2520Landsness%2520and%2520Daniel%2520L.%2520Barabasi%2520and%2520Siddharth%2520Narayanan%2520and%2520Nicky%2520Evans%2520and%2520Shriya%2520Reddy%2520and%2520Martha%2520Foiani%2520and%2520Aizad%2520Kamal%2520and%2520Leah%2520P.%2520Shriver%2520and%2520Fang%2520Cao%2520and%2520Asmamaw%2520T.%2520Wassie%2520and%2520Jon%2520M.%2520Laurent%2520and%2520Edwin%2520Melville-Green%2520and%2520Mayk%2520Caldas%2520and%2520Albert%2520Bou%2520and%2520Kaleigh%2520F.%2520Roberts%2520and%2520Sladjana%2520Zagorac%2520and%2520Timothy%2520C.%2520Orr%2520and%2520Miranda%2520E.%2520Orr%2520and%2520Kevin%2520J.%2520Zwezdaryk%2520and%2520Ali%2520E.%2520Ghareeb%2520and%2520Laurie%2520McCoy%2520and%2520Bruna%2520Gomes%2520and%2520Euan%2520A.%2520Ashley%2520and%2520Karen%2520E.%2520Duff%2520and%2520Tonio%2520Buonassisi%2520and%2520Tom%2520Rainforth%2520and%2520Randall%2520J.%2520Bateman%2520and%2520Michael%2520Skarlinski%2520and%2520Samuel%2520G.%2520Rodriques%2520and%2520Michaela%2520M.%2520Hinks%2520and%2520Andrew%2520D.%2520White%26entry.1292438233%3D%2520%2520Data-driven%2520scientific%2520discovery%2520requires%2520iterative%2520cycles%2520of%2520literature%250Asearch%252C%2520hypothesis%2520generation%252C%2520and%2520data%2520analysis.%2520Substantial%2520progress%2520has%2520been%250Amade%2520towards%2520AI%2520agents%2520that%2520can%2520automate%2520scientific%2520research%252C%2520but%2520all%2520such%250Aagents%2520remain%2520limited%2520in%2520the%2520number%2520of%2520actions%2520they%2520can%2520take%2520before%2520losing%250Acoherence%252C%2520thus%2520limiting%2520the%2520depth%2520of%2520their%2520findings.%2520Here%2520we%2520present%2520Kosmos%252C%250Aan%2520AI%2520scientist%2520that%2520automates%2520data-driven%2520discovery.%2520Given%2520an%2520open-ended%250Aobjective%2520and%2520a%2520dataset%252C%2520Kosmos%2520runs%2520for%2520up%2520to%252012%2520hours%2520performing%2520cycles%2520of%250Aparallel%2520data%2520analysis%252C%2520literature%2520search%252C%2520and%2520hypothesis%2520generation%2520before%250Asynthesizing%2520discoveries%2520into%2520scientific%2520reports.%2520Unlike%2520prior%2520systems%252C%2520Kosmos%250Auses%2520a%2520structured%2520world%2520model%2520to%2520share%2520information%2520between%2520a%2520data%2520analysis%250Aagent%2520and%2520a%2520literature%2520search%2520agent.%2520The%2520world%2520model%2520enables%2520Kosmos%2520to%250Acoherently%2520pursue%2520the%2520specified%2520objective%2520over%2520200%2520agent%2520rollouts%252C%2520collectively%250Aexecuting%2520an%2520average%2520of%252042%252C000%2520lines%2520of%2520code%2520and%2520reading%25201%252C500%2520papers%2520per%2520run.%250AKosmos%2520cites%2520all%2520statements%2520in%2520its%2520reports%2520with%2520code%2520or%2520primary%2520literature%252C%250Aensuring%2520its%2520reasoning%2520is%2520traceable.%2520Independent%2520scientists%2520found%252079.4%2525%2520of%250Astatements%2520in%2520Kosmos%2520reports%2520to%2520be%2520accurate%252C%2520and%2520collaborators%2520reported%2520that%2520a%250Asingle%252020-cycle%2520Kosmos%2520run%2520performed%2520the%2520equivalent%2520of%25206%2520months%2520of%2520their%2520own%250Aresearch%2520time%2520on%2520average.%2520Furthermore%252C%2520collaborators%2520reported%2520that%2520the%2520number%250Aof%2520valuable%2520scientific%2520findings%2520generated%2520scales%2520linearly%2520with%2520Kosmos%2520cycles%250A%2528tested%2520up%2520to%252020%2520cycles%2529.%2520We%2520highlight%2520seven%2520discoveries%2520made%2520by%2520Kosmos%2520that%250Aspan%2520metabolomics%252C%2520materials%2520science%252C%2520neuroscience%252C%2520and%2520statistical%2520genetics.%250AThree%2520discoveries%2520independently%2520reproduce%2520findings%2520from%2520preprinted%2520or%250Aunpublished%2520manuscripts%2520that%2520were%2520not%2520accessed%2520by%2520Kosmos%2520at%2520runtime%252C%2520while%2520four%250Amake%2520novel%2520contributions%2520to%2520the%2520scientific%2520literature.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.02824v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kosmos%3A%20An%20AI%20Scientist%20for%20Autonomous%20Discovery&entry.906535625=Ludovico%20Mitchener%20and%20Angela%20Yiu%20and%20Benjamin%20Chang%20and%20Mathieu%20Bourdenx%20and%20Tyler%20Nadolski%20and%20Arvis%20Sulovari%20and%20Eric%20C.%20Landsness%20and%20Daniel%20L.%20Barabasi%20and%20Siddharth%20Narayanan%20and%20Nicky%20Evans%20and%20Shriya%20Reddy%20and%20Martha%20Foiani%20and%20Aizad%20Kamal%20and%20Leah%20P.%20Shriver%20and%20Fang%20Cao%20and%20Asmamaw%20T.%20Wassie%20and%20Jon%20M.%20Laurent%20and%20Edwin%20Melville-Green%20and%20Mayk%20Caldas%20and%20Albert%20Bou%20and%20Kaleigh%20F.%20Roberts%20and%20Sladjana%20Zagorac%20and%20Timothy%20C.%20Orr%20and%20Miranda%20E.%20Orr%20and%20Kevin%20J.%20Zwezdaryk%20and%20Ali%20E.%20Ghareeb%20and%20Laurie%20McCoy%20and%20Bruna%20Gomes%20and%20Euan%20A.%20Ashley%20and%20Karen%20E.%20Duff%20and%20Tonio%20Buonassisi%20and%20Tom%20Rainforth%20and%20Randall%20J.%20Bateman%20and%20Michael%20Skarlinski%20and%20Samuel%20G.%20Rodriques%20and%20Michaela%20M.%20Hinks%20and%20Andrew%20D.%20White&entry.1292438233=%20%20Data-driven%20scientific%20discovery%20requires%20iterative%20cycles%20of%20literature%0Asearch%2C%20hypothesis%20generation%2C%20and%20data%20analysis.%20Substantial%20progress%20has%20been%0Amade%20towards%20AI%20agents%20that%20can%20automate%20scientific%20research%2C%20but%20all%20such%0Aagents%20remain%20limited%20in%20the%20number%20of%20actions%20they%20can%20take%20before%20losing%0Acoherence%2C%20thus%20limiting%20the%20depth%20of%20their%20findings.%20Here%20we%20present%20Kosmos%2C%0Aan%20AI%20scientist%20that%20automates%20data-driven%20discovery.%20Given%20an%20open-ended%0Aobjective%20and%20a%20dataset%2C%20Kosmos%20runs%20for%20up%20to%2012%20hours%20performing%20cycles%20of%0Aparallel%20data%20analysis%2C%20literature%20search%2C%20and%20hypothesis%20generation%20before%0Asynthesizing%20discoveries%20into%20scientific%20reports.%20Unlike%20prior%20systems%2C%20Kosmos%0Auses%20a%20structured%20world%20model%20to%20share%20information%20between%20a%20data%20analysis%0Aagent%20and%20a%20literature%20search%20agent.%20The%20world%20model%20enables%20Kosmos%20to%0Acoherently%20pursue%20the%20specified%20objective%20over%20200%20agent%20rollouts%2C%20collectively%0Aexecuting%20an%20average%20of%2042%2C000%20lines%20of%20code%20and%20reading%201%2C500%20papers%20per%20run.%0AKosmos%20cites%20all%20statements%20in%20its%20reports%20with%20code%20or%20primary%20literature%2C%0Aensuring%20its%20reasoning%20is%20traceable.%20Independent%20scientists%20found%2079.4%25%20of%0Astatements%20in%20Kosmos%20reports%20to%20be%20accurate%2C%20and%20collaborators%20reported%20that%20a%0Asingle%2020-cycle%20Kosmos%20run%20performed%20the%20equivalent%20of%206%20months%20of%20their%20own%0Aresearch%20time%20on%20average.%20Furthermore%2C%20collaborators%20reported%20that%20the%20number%0Aof%20valuable%20scientific%20findings%20generated%20scales%20linearly%20with%20Kosmos%20cycles%0A%28tested%20up%20to%2020%20cycles%29.%20We%20highlight%20seven%20discoveries%20made%20by%20Kosmos%20that%0Aspan%20metabolomics%2C%20materials%20science%2C%20neuroscience%2C%20and%20statistical%20genetics.%0AThree%20discoveries%20independently%20reproduce%20findings%20from%20preprinted%20or%0Aunpublished%20manuscripts%20that%20were%20not%20accessed%20by%20Kosmos%20at%20runtime%2C%20while%20four%0Amake%20novel%20contributions%20to%20the%20scientific%20literature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.02824v2&entry.124074799=Read"},
{"title": "Diagrams-to-Dynamics (D2D): Exploring Causal Loop Diagram Leverage\n  Points under Uncertainty", "author": "Jeroen F. Uleman and Loes Crielaard and Leonie K. Elsenburg and Guido A. Veldhuis and Naja Hulvej Rod and Rick Quax and V\u00edtor V. Vasconcelos", "abstract": "  Causal loop diagrams (CLDs) are widely used in health and environmental\nresearch to represent hypothesized causal structures underlying complex\nproblems. However, as qualitative and static representations, CLDs are limited\nin their ability to support dynamic analysis and inform intervention\nstrategies. We propose Diagrams-to-Dynamics (D2D), a method for converting CLDs\ninto exploratory system dynamics models (SDMs) in the absence of empirical\ndata. With minimal user input - following a protocol to label variables as\nstocks, flows or auxiliaries, and constants - D2D leverages the structural\ninformation already encoded in CLDs, namely, link existence and polarity, to\nsimulate hypothetical interventions and explore potential leverage points under\nuncertainty. Results suggest that D2D helps distinguish between high- and\nlow-ranked leverage points. We compare D2D to a data-driven SDM constructed\nfrom the same CLD and variable labels. D2D showed greater consistency with the\ndata-driven model compared to static network centrality analysis, while\nproviding uncertainty estimates and guidance for future data collection. The\nD2D method is implemented in an open-source Python package and a web-based\napplication to support further testing and lower the barrier to dynamic\nmodeling for researchers working with CLDs. We expect that additional\nvalidation studies will further establish the approach's utility across a broad\nrange of cases and domains.\n", "link": "http://arxiv.org/abs/2508.05659v3", "date": "2025-11-05", "relevancy": 1.3298, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.454}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4443}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diagrams-to-Dynamics%20%28D2D%29%3A%20Exploring%20Causal%20Loop%20Diagram%20Leverage%0A%20%20Points%20under%20Uncertainty&body=Title%3A%20Diagrams-to-Dynamics%20%28D2D%29%3A%20Exploring%20Causal%20Loop%20Diagram%20Leverage%0A%20%20Points%20under%20Uncertainty%0AAuthor%3A%20Jeroen%20F.%20Uleman%20and%20Loes%20Crielaard%20and%20Leonie%20K.%20Elsenburg%20and%20Guido%20A.%20Veldhuis%20and%20Naja%20Hulvej%20Rod%20and%20Rick%20Quax%20and%20V%C3%ADtor%20V.%20Vasconcelos%0AAbstract%3A%20%20%20Causal%20loop%20diagrams%20%28CLDs%29%20are%20widely%20used%20in%20health%20and%20environmental%0Aresearch%20to%20represent%20hypothesized%20causal%20structures%20underlying%20complex%0Aproblems.%20However%2C%20as%20qualitative%20and%20static%20representations%2C%20CLDs%20are%20limited%0Ain%20their%20ability%20to%20support%20dynamic%20analysis%20and%20inform%20intervention%0Astrategies.%20We%20propose%20Diagrams-to-Dynamics%20%28D2D%29%2C%20a%20method%20for%20converting%20CLDs%0Ainto%20exploratory%20system%20dynamics%20models%20%28SDMs%29%20in%20the%20absence%20of%20empirical%0Adata.%20With%20minimal%20user%20input%20-%20following%20a%20protocol%20to%20label%20variables%20as%0Astocks%2C%20flows%20or%20auxiliaries%2C%20and%20constants%20-%20D2D%20leverages%20the%20structural%0Ainformation%20already%20encoded%20in%20CLDs%2C%20namely%2C%20link%20existence%20and%20polarity%2C%20to%0Asimulate%20hypothetical%20interventions%20and%20explore%20potential%20leverage%20points%20under%0Auncertainty.%20Results%20suggest%20that%20D2D%20helps%20distinguish%20between%20high-%20and%0Alow-ranked%20leverage%20points.%20We%20compare%20D2D%20to%20a%20data-driven%20SDM%20constructed%0Afrom%20the%20same%20CLD%20and%20variable%20labels.%20D2D%20showed%20greater%20consistency%20with%20the%0Adata-driven%20model%20compared%20to%20static%20network%20centrality%20analysis%2C%20while%0Aproviding%20uncertainty%20estimates%20and%20guidance%20for%20future%20data%20collection.%20The%0AD2D%20method%20is%20implemented%20in%20an%20open-source%20Python%20package%20and%20a%20web-based%0Aapplication%20to%20support%20further%20testing%20and%20lower%20the%20barrier%20to%20dynamic%0Amodeling%20for%20researchers%20working%20with%20CLDs.%20We%20expect%20that%20additional%0Avalidation%20studies%20will%20further%20establish%20the%20approach%27s%20utility%20across%20a%20broad%0Arange%20of%20cases%20and%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05659v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiagrams-to-Dynamics%2520%2528D2D%2529%253A%2520Exploring%2520Causal%2520Loop%2520Diagram%2520Leverage%250A%2520%2520Points%2520under%2520Uncertainty%26entry.906535625%3DJeroen%2520F.%2520Uleman%2520and%2520Loes%2520Crielaard%2520and%2520Leonie%2520K.%2520Elsenburg%2520and%2520Guido%2520A.%2520Veldhuis%2520and%2520Naja%2520Hulvej%2520Rod%2520and%2520Rick%2520Quax%2520and%2520V%25C3%25ADtor%2520V.%2520Vasconcelos%26entry.1292438233%3D%2520%2520Causal%2520loop%2520diagrams%2520%2528CLDs%2529%2520are%2520widely%2520used%2520in%2520health%2520and%2520environmental%250Aresearch%2520to%2520represent%2520hypothesized%2520causal%2520structures%2520underlying%2520complex%250Aproblems.%2520However%252C%2520as%2520qualitative%2520and%2520static%2520representations%252C%2520CLDs%2520are%2520limited%250Ain%2520their%2520ability%2520to%2520support%2520dynamic%2520analysis%2520and%2520inform%2520intervention%250Astrategies.%2520We%2520propose%2520Diagrams-to-Dynamics%2520%2528D2D%2529%252C%2520a%2520method%2520for%2520converting%2520CLDs%250Ainto%2520exploratory%2520system%2520dynamics%2520models%2520%2528SDMs%2529%2520in%2520the%2520absence%2520of%2520empirical%250Adata.%2520With%2520minimal%2520user%2520input%2520-%2520following%2520a%2520protocol%2520to%2520label%2520variables%2520as%250Astocks%252C%2520flows%2520or%2520auxiliaries%252C%2520and%2520constants%2520-%2520D2D%2520leverages%2520the%2520structural%250Ainformation%2520already%2520encoded%2520in%2520CLDs%252C%2520namely%252C%2520link%2520existence%2520and%2520polarity%252C%2520to%250Asimulate%2520hypothetical%2520interventions%2520and%2520explore%2520potential%2520leverage%2520points%2520under%250Auncertainty.%2520Results%2520suggest%2520that%2520D2D%2520helps%2520distinguish%2520between%2520high-%2520and%250Alow-ranked%2520leverage%2520points.%2520We%2520compare%2520D2D%2520to%2520a%2520data-driven%2520SDM%2520constructed%250Afrom%2520the%2520same%2520CLD%2520and%2520variable%2520labels.%2520D2D%2520showed%2520greater%2520consistency%2520with%2520the%250Adata-driven%2520model%2520compared%2520to%2520static%2520network%2520centrality%2520analysis%252C%2520while%250Aproviding%2520uncertainty%2520estimates%2520and%2520guidance%2520for%2520future%2520data%2520collection.%2520The%250AD2D%2520method%2520is%2520implemented%2520in%2520an%2520open-source%2520Python%2520package%2520and%2520a%2520web-based%250Aapplication%2520to%2520support%2520further%2520testing%2520and%2520lower%2520the%2520barrier%2520to%2520dynamic%250Amodeling%2520for%2520researchers%2520working%2520with%2520CLDs.%2520We%2520expect%2520that%2520additional%250Avalidation%2520studies%2520will%2520further%2520establish%2520the%2520approach%2527s%2520utility%2520across%2520a%2520broad%250Arange%2520of%2520cases%2520and%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05659v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diagrams-to-Dynamics%20%28D2D%29%3A%20Exploring%20Causal%20Loop%20Diagram%20Leverage%0A%20%20Points%20under%20Uncertainty&entry.906535625=Jeroen%20F.%20Uleman%20and%20Loes%20Crielaard%20and%20Leonie%20K.%20Elsenburg%20and%20Guido%20A.%20Veldhuis%20and%20Naja%20Hulvej%20Rod%20and%20Rick%20Quax%20and%20V%C3%ADtor%20V.%20Vasconcelos&entry.1292438233=%20%20Causal%20loop%20diagrams%20%28CLDs%29%20are%20widely%20used%20in%20health%20and%20environmental%0Aresearch%20to%20represent%20hypothesized%20causal%20structures%20underlying%20complex%0Aproblems.%20However%2C%20as%20qualitative%20and%20static%20representations%2C%20CLDs%20are%20limited%0Ain%20their%20ability%20to%20support%20dynamic%20analysis%20and%20inform%20intervention%0Astrategies.%20We%20propose%20Diagrams-to-Dynamics%20%28D2D%29%2C%20a%20method%20for%20converting%20CLDs%0Ainto%20exploratory%20system%20dynamics%20models%20%28SDMs%29%20in%20the%20absence%20of%20empirical%0Adata.%20With%20minimal%20user%20input%20-%20following%20a%20protocol%20to%20label%20variables%20as%0Astocks%2C%20flows%20or%20auxiliaries%2C%20and%20constants%20-%20D2D%20leverages%20the%20structural%0Ainformation%20already%20encoded%20in%20CLDs%2C%20namely%2C%20link%20existence%20and%20polarity%2C%20to%0Asimulate%20hypothetical%20interventions%20and%20explore%20potential%20leverage%20points%20under%0Auncertainty.%20Results%20suggest%20that%20D2D%20helps%20distinguish%20between%20high-%20and%0Alow-ranked%20leverage%20points.%20We%20compare%20D2D%20to%20a%20data-driven%20SDM%20constructed%0Afrom%20the%20same%20CLD%20and%20variable%20labels.%20D2D%20showed%20greater%20consistency%20with%20the%0Adata-driven%20model%20compared%20to%20static%20network%20centrality%20analysis%2C%20while%0Aproviding%20uncertainty%20estimates%20and%20guidance%20for%20future%20data%20collection.%20The%0AD2D%20method%20is%20implemented%20in%20an%20open-source%20Python%20package%20and%20a%20web-based%0Aapplication%20to%20support%20further%20testing%20and%20lower%20the%20barrier%20to%20dynamic%0Amodeling%20for%20researchers%20working%20with%20CLDs.%20We%20expect%20that%20additional%0Avalidation%20studies%20will%20further%20establish%20the%20approach%27s%20utility%20across%20a%20broad%0Arange%20of%20cases%20and%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05659v3&entry.124074799=Read"},
{"title": "Do Automatic Factuality Metrics Measure Factuality? A Critical\n  Evaluation", "author": "Sanjana Ramprasad and Byron C. Wallace", "abstract": "  Modern LLMs can now produce highly readable abstractive summaries, to the\npoint that traditional automated metrics for evaluating summary quality, such\nas ROUGE, have saturated. However, LLMs still sometimes introduce inaccuracies\ninto summaries, i.e., information inconsistent with or unsupported by the\ncorresponding source. Measuring the occurrence of these often subtle factual\ninconsistencies automatically has proved challenging. This in turn has\nmotivated development of metrics intended to measure the factual consistency of\ngenerated summaries against sources. But are these approaches measuring what\nthey purport to? Or are they mostly exploiting artifacts? In this work, we\nstress test a range of automatic factuality metrics, including specialized\nmodels and LLM-based prompting methods, to probe what they actually capture.\nUsing a shallow classifier to separate ``easy'' examples for factual evaluation\nwhere surface features suffice from ``hard'' cases requiring deeper reasoning,\nwe find that all metrics show substantial performance drops on the latter.\nFurthermore, some metrics are more sensitive to benign, fact-preserving edits\nthan to factual corrections. Building on this observation, we demonstrate that\nmost automatic factuality metrics can be gamed, i.e., their scores can be\nartificially inflated by appending innocuous, content-free sentences to\nsummaries. Among the metrics tested, the prompt based ChatGPT-DA approach is\nthe most robust and reliable. However, this comes with a notable caveat:\nPrompting LLMs to assess factuality may overly rely on their parametric\nknowledge rather than the provided reference when making judgments. Taken\ntogether, our findings call into question the reliability of current factuality\nmetrics and prompt a broader reflection on what these metrics are truly\nmeasuring.\n", "link": "http://arxiv.org/abs/2411.16638v4", "date": "2025-11-05", "relevancy": 1.3276, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4687}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4355}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Automatic%20Factuality%20Metrics%20Measure%20Factuality%3F%20A%20Critical%0A%20%20Evaluation&body=Title%3A%20Do%20Automatic%20Factuality%20Metrics%20Measure%20Factuality%3F%20A%20Critical%0A%20%20Evaluation%0AAuthor%3A%20Sanjana%20Ramprasad%20and%20Byron%20C.%20Wallace%0AAbstract%3A%20%20%20Modern%20LLMs%20can%20now%20produce%20highly%20readable%20abstractive%20summaries%2C%20to%20the%0Apoint%20that%20traditional%20automated%20metrics%20for%20evaluating%20summary%20quality%2C%20such%0Aas%20ROUGE%2C%20have%20saturated.%20However%2C%20LLMs%20still%20sometimes%20introduce%20inaccuracies%0Ainto%20summaries%2C%20i.e.%2C%20information%20inconsistent%20with%20or%20unsupported%20by%20the%0Acorresponding%20source.%20Measuring%20the%20occurrence%20of%20these%20often%20subtle%20factual%0Ainconsistencies%20automatically%20has%20proved%20challenging.%20This%20in%20turn%20has%0Amotivated%20development%20of%20metrics%20intended%20to%20measure%20the%20factual%20consistency%20of%0Agenerated%20summaries%20against%20sources.%20But%20are%20these%20approaches%20measuring%20what%0Athey%20purport%20to%3F%20Or%20are%20they%20mostly%20exploiting%20artifacts%3F%20In%20this%20work%2C%20we%0Astress%20test%20a%20range%20of%20automatic%20factuality%20metrics%2C%20including%20specialized%0Amodels%20and%20LLM-based%20prompting%20methods%2C%20to%20probe%20what%20they%20actually%20capture.%0AUsing%20a%20shallow%20classifier%20to%20separate%20%60%60easy%27%27%20examples%20for%20factual%20evaluation%0Awhere%20surface%20features%20suffice%20from%20%60%60hard%27%27%20cases%20requiring%20deeper%20reasoning%2C%0Awe%20find%20that%20all%20metrics%20show%20substantial%20performance%20drops%20on%20the%20latter.%0AFurthermore%2C%20some%20metrics%20are%20more%20sensitive%20to%20benign%2C%20fact-preserving%20edits%0Athan%20to%20factual%20corrections.%20Building%20on%20this%20observation%2C%20we%20demonstrate%20that%0Amost%20automatic%20factuality%20metrics%20can%20be%20gamed%2C%20i.e.%2C%20their%20scores%20can%20be%0Aartificially%20inflated%20by%20appending%20innocuous%2C%20content-free%20sentences%20to%0Asummaries.%20Among%20the%20metrics%20tested%2C%20the%20prompt%20based%20ChatGPT-DA%20approach%20is%0Athe%20most%20robust%20and%20reliable.%20However%2C%20this%20comes%20with%20a%20notable%20caveat%3A%0APrompting%20LLMs%20to%20assess%20factuality%20may%20overly%20rely%20on%20their%20parametric%0Aknowledge%20rather%20than%20the%20provided%20reference%20when%20making%20judgments.%20Taken%0Atogether%2C%20our%20findings%20call%20into%20question%20the%20reliability%20of%20current%20factuality%0Ametrics%20and%20prompt%20a%20broader%20reflection%20on%20what%20these%20metrics%20are%20truly%0Ameasuring.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16638v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Automatic%2520Factuality%2520Metrics%2520Measure%2520Factuality%253F%2520A%2520Critical%250A%2520%2520Evaluation%26entry.906535625%3DSanjana%2520Ramprasad%2520and%2520Byron%2520C.%2520Wallace%26entry.1292438233%3D%2520%2520Modern%2520LLMs%2520can%2520now%2520produce%2520highly%2520readable%2520abstractive%2520summaries%252C%2520to%2520the%250Apoint%2520that%2520traditional%2520automated%2520metrics%2520for%2520evaluating%2520summary%2520quality%252C%2520such%250Aas%2520ROUGE%252C%2520have%2520saturated.%2520However%252C%2520LLMs%2520still%2520sometimes%2520introduce%2520inaccuracies%250Ainto%2520summaries%252C%2520i.e.%252C%2520information%2520inconsistent%2520with%2520or%2520unsupported%2520by%2520the%250Acorresponding%2520source.%2520Measuring%2520the%2520occurrence%2520of%2520these%2520often%2520subtle%2520factual%250Ainconsistencies%2520automatically%2520has%2520proved%2520challenging.%2520This%2520in%2520turn%2520has%250Amotivated%2520development%2520of%2520metrics%2520intended%2520to%2520measure%2520the%2520factual%2520consistency%2520of%250Agenerated%2520summaries%2520against%2520sources.%2520But%2520are%2520these%2520approaches%2520measuring%2520what%250Athey%2520purport%2520to%253F%2520Or%2520are%2520they%2520mostly%2520exploiting%2520artifacts%253F%2520In%2520this%2520work%252C%2520we%250Astress%2520test%2520a%2520range%2520of%2520automatic%2520factuality%2520metrics%252C%2520including%2520specialized%250Amodels%2520and%2520LLM-based%2520prompting%2520methods%252C%2520to%2520probe%2520what%2520they%2520actually%2520capture.%250AUsing%2520a%2520shallow%2520classifier%2520to%2520separate%2520%2560%2560easy%2527%2527%2520examples%2520for%2520factual%2520evaluation%250Awhere%2520surface%2520features%2520suffice%2520from%2520%2560%2560hard%2527%2527%2520cases%2520requiring%2520deeper%2520reasoning%252C%250Awe%2520find%2520that%2520all%2520metrics%2520show%2520substantial%2520performance%2520drops%2520on%2520the%2520latter.%250AFurthermore%252C%2520some%2520metrics%2520are%2520more%2520sensitive%2520to%2520benign%252C%2520fact-preserving%2520edits%250Athan%2520to%2520factual%2520corrections.%2520Building%2520on%2520this%2520observation%252C%2520we%2520demonstrate%2520that%250Amost%2520automatic%2520factuality%2520metrics%2520can%2520be%2520gamed%252C%2520i.e.%252C%2520their%2520scores%2520can%2520be%250Aartificially%2520inflated%2520by%2520appending%2520innocuous%252C%2520content-free%2520sentences%2520to%250Asummaries.%2520Among%2520the%2520metrics%2520tested%252C%2520the%2520prompt%2520based%2520ChatGPT-DA%2520approach%2520is%250Athe%2520most%2520robust%2520and%2520reliable.%2520However%252C%2520this%2520comes%2520with%2520a%2520notable%2520caveat%253A%250APrompting%2520LLMs%2520to%2520assess%2520factuality%2520may%2520overly%2520rely%2520on%2520their%2520parametric%250Aknowledge%2520rather%2520than%2520the%2520provided%2520reference%2520when%2520making%2520judgments.%2520Taken%250Atogether%252C%2520our%2520findings%2520call%2520into%2520question%2520the%2520reliability%2520of%2520current%2520factuality%250Ametrics%2520and%2520prompt%2520a%2520broader%2520reflection%2520on%2520what%2520these%2520metrics%2520are%2520truly%250Ameasuring.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16638v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Automatic%20Factuality%20Metrics%20Measure%20Factuality%3F%20A%20Critical%0A%20%20Evaluation&entry.906535625=Sanjana%20Ramprasad%20and%20Byron%20C.%20Wallace&entry.1292438233=%20%20Modern%20LLMs%20can%20now%20produce%20highly%20readable%20abstractive%20summaries%2C%20to%20the%0Apoint%20that%20traditional%20automated%20metrics%20for%20evaluating%20summary%20quality%2C%20such%0Aas%20ROUGE%2C%20have%20saturated.%20However%2C%20LLMs%20still%20sometimes%20introduce%20inaccuracies%0Ainto%20summaries%2C%20i.e.%2C%20information%20inconsistent%20with%20or%20unsupported%20by%20the%0Acorresponding%20source.%20Measuring%20the%20occurrence%20of%20these%20often%20subtle%20factual%0Ainconsistencies%20automatically%20has%20proved%20challenging.%20This%20in%20turn%20has%0Amotivated%20development%20of%20metrics%20intended%20to%20measure%20the%20factual%20consistency%20of%0Agenerated%20summaries%20against%20sources.%20But%20are%20these%20approaches%20measuring%20what%0Athey%20purport%20to%3F%20Or%20are%20they%20mostly%20exploiting%20artifacts%3F%20In%20this%20work%2C%20we%0Astress%20test%20a%20range%20of%20automatic%20factuality%20metrics%2C%20including%20specialized%0Amodels%20and%20LLM-based%20prompting%20methods%2C%20to%20probe%20what%20they%20actually%20capture.%0AUsing%20a%20shallow%20classifier%20to%20separate%20%60%60easy%27%27%20examples%20for%20factual%20evaluation%0Awhere%20surface%20features%20suffice%20from%20%60%60hard%27%27%20cases%20requiring%20deeper%20reasoning%2C%0Awe%20find%20that%20all%20metrics%20show%20substantial%20performance%20drops%20on%20the%20latter.%0AFurthermore%2C%20some%20metrics%20are%20more%20sensitive%20to%20benign%2C%20fact-preserving%20edits%0Athan%20to%20factual%20corrections.%20Building%20on%20this%20observation%2C%20we%20demonstrate%20that%0Amost%20automatic%20factuality%20metrics%20can%20be%20gamed%2C%20i.e.%2C%20their%20scores%20can%20be%0Aartificially%20inflated%20by%20appending%20innocuous%2C%20content-free%20sentences%20to%0Asummaries.%20Among%20the%20metrics%20tested%2C%20the%20prompt%20based%20ChatGPT-DA%20approach%20is%0Athe%20most%20robust%20and%20reliable.%20However%2C%20this%20comes%20with%20a%20notable%20caveat%3A%0APrompting%20LLMs%20to%20assess%20factuality%20may%20overly%20rely%20on%20their%20parametric%0Aknowledge%20rather%20than%20the%20provided%20reference%20when%20making%20judgments.%20Taken%0Atogether%2C%20our%20findings%20call%20into%20question%20the%20reliability%20of%20current%20factuality%0Ametrics%20and%20prompt%20a%20broader%20reflection%20on%20what%20these%20metrics%20are%20truly%0Ameasuring.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16638v4&entry.124074799=Read"},
{"title": "SME-TEAM: Leveraging Trust and Ethics for Secure and Responsible Use of\n  AI and LLMs in SMEs", "author": "Iqbal H. Sarker and Helge Janicke and Ahmad Mohsin and Leandros Maglaras", "abstract": "  Artificial Intelligence (AI) and Large Language Models (LLMs) are\nrevolutionizing today's business practices; however, their adoption within\nsmall and medium-sized enterprises (SMEs) raises serious trust, ethical, and\ntechnical issues. In this perspective paper, we introduce a structured,\nmulti-phased framework, \"SME-TEAM\" for the secure and responsible use of these\ntechnologies in SMEs. Based on a conceptual structure of four key pillars,\ni.e., Data, Algorithms, Human Oversight, and Model Architecture, SME-TEAM\nbridges theoretical ethical principles with operational practice, enhancing AI\ncapabilities across a wide range of applications in SMEs. Ultimately, this\npaper provides a structured roadmap for the adoption of these emerging\ntechnologies, positioning trust and ethics as a driving force for resilience,\ncompetitiveness, and sustainable innovation within the area of business\nanalytics and SMEs.\n", "link": "http://arxiv.org/abs/2509.10594v2", "date": "2025-11-05", "relevancy": 1.3119, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4697}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4303}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SME-TEAM%3A%20Leveraging%20Trust%20and%20Ethics%20for%20Secure%20and%20Responsible%20Use%20of%0A%20%20AI%20and%20LLMs%20in%20SMEs&body=Title%3A%20SME-TEAM%3A%20Leveraging%20Trust%20and%20Ethics%20for%20Secure%20and%20Responsible%20Use%20of%0A%20%20AI%20and%20LLMs%20in%20SMEs%0AAuthor%3A%20Iqbal%20H.%20Sarker%20and%20Helge%20Janicke%20and%20Ahmad%20Mohsin%20and%20Leandros%20Maglaras%0AAbstract%3A%20%20%20Artificial%20Intelligence%20%28AI%29%20and%20Large%20Language%20Models%20%28LLMs%29%20are%0Arevolutionizing%20today%27s%20business%20practices%3B%20however%2C%20their%20adoption%20within%0Asmall%20and%20medium-sized%20enterprises%20%28SMEs%29%20raises%20serious%20trust%2C%20ethical%2C%20and%0Atechnical%20issues.%20In%20this%20perspective%20paper%2C%20we%20introduce%20a%20structured%2C%0Amulti-phased%20framework%2C%20%22SME-TEAM%22%20for%20the%20secure%20and%20responsible%20use%20of%20these%0Atechnologies%20in%20SMEs.%20Based%20on%20a%20conceptual%20structure%20of%20four%20key%20pillars%2C%0Ai.e.%2C%20Data%2C%20Algorithms%2C%20Human%20Oversight%2C%20and%20Model%20Architecture%2C%20SME-TEAM%0Abridges%20theoretical%20ethical%20principles%20with%20operational%20practice%2C%20enhancing%20AI%0Acapabilities%20across%20a%20wide%20range%20of%20applications%20in%20SMEs.%20Ultimately%2C%20this%0Apaper%20provides%20a%20structured%20roadmap%20for%20the%20adoption%20of%20these%20emerging%0Atechnologies%2C%20positioning%20trust%20and%20ethics%20as%20a%20driving%20force%20for%20resilience%2C%0Acompetitiveness%2C%20and%20sustainable%20innovation%20within%20the%20area%20of%20business%0Aanalytics%20and%20SMEs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.10594v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSME-TEAM%253A%2520Leveraging%2520Trust%2520and%2520Ethics%2520for%2520Secure%2520and%2520Responsible%2520Use%2520of%250A%2520%2520AI%2520and%2520LLMs%2520in%2520SMEs%26entry.906535625%3DIqbal%2520H.%2520Sarker%2520and%2520Helge%2520Janicke%2520and%2520Ahmad%2520Mohsin%2520and%2520Leandros%2520Maglaras%26entry.1292438233%3D%2520%2520Artificial%2520Intelligence%2520%2528AI%2529%2520and%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%250Arevolutionizing%2520today%2527s%2520business%2520practices%253B%2520however%252C%2520their%2520adoption%2520within%250Asmall%2520and%2520medium-sized%2520enterprises%2520%2528SMEs%2529%2520raises%2520serious%2520trust%252C%2520ethical%252C%2520and%250Atechnical%2520issues.%2520In%2520this%2520perspective%2520paper%252C%2520we%2520introduce%2520a%2520structured%252C%250Amulti-phased%2520framework%252C%2520%2522SME-TEAM%2522%2520for%2520the%2520secure%2520and%2520responsible%2520use%2520of%2520these%250Atechnologies%2520in%2520SMEs.%2520Based%2520on%2520a%2520conceptual%2520structure%2520of%2520four%2520key%2520pillars%252C%250Ai.e.%252C%2520Data%252C%2520Algorithms%252C%2520Human%2520Oversight%252C%2520and%2520Model%2520Architecture%252C%2520SME-TEAM%250Abridges%2520theoretical%2520ethical%2520principles%2520with%2520operational%2520practice%252C%2520enhancing%2520AI%250Acapabilities%2520across%2520a%2520wide%2520range%2520of%2520applications%2520in%2520SMEs.%2520Ultimately%252C%2520this%250Apaper%2520provides%2520a%2520structured%2520roadmap%2520for%2520the%2520adoption%2520of%2520these%2520emerging%250Atechnologies%252C%2520positioning%2520trust%2520and%2520ethics%2520as%2520a%2520driving%2520force%2520for%2520resilience%252C%250Acompetitiveness%252C%2520and%2520sustainable%2520innovation%2520within%2520the%2520area%2520of%2520business%250Aanalytics%2520and%2520SMEs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.10594v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SME-TEAM%3A%20Leveraging%20Trust%20and%20Ethics%20for%20Secure%20and%20Responsible%20Use%20of%0A%20%20AI%20and%20LLMs%20in%20SMEs&entry.906535625=Iqbal%20H.%20Sarker%20and%20Helge%20Janicke%20and%20Ahmad%20Mohsin%20and%20Leandros%20Maglaras&entry.1292438233=%20%20Artificial%20Intelligence%20%28AI%29%20and%20Large%20Language%20Models%20%28LLMs%29%20are%0Arevolutionizing%20today%27s%20business%20practices%3B%20however%2C%20their%20adoption%20within%0Asmall%20and%20medium-sized%20enterprises%20%28SMEs%29%20raises%20serious%20trust%2C%20ethical%2C%20and%0Atechnical%20issues.%20In%20this%20perspective%20paper%2C%20we%20introduce%20a%20structured%2C%0Amulti-phased%20framework%2C%20%22SME-TEAM%22%20for%20the%20secure%20and%20responsible%20use%20of%20these%0Atechnologies%20in%20SMEs.%20Based%20on%20a%20conceptual%20structure%20of%20four%20key%20pillars%2C%0Ai.e.%2C%20Data%2C%20Algorithms%2C%20Human%20Oversight%2C%20and%20Model%20Architecture%2C%20SME-TEAM%0Abridges%20theoretical%20ethical%20principles%20with%20operational%20practice%2C%20enhancing%20AI%0Acapabilities%20across%20a%20wide%20range%20of%20applications%20in%20SMEs.%20Ultimately%2C%20this%0Apaper%20provides%20a%20structured%20roadmap%20for%20the%20adoption%20of%20these%20emerging%0Atechnologies%2C%20positioning%20trust%20and%20ethics%20as%20a%20driving%20force%20for%20resilience%2C%0Acompetitiveness%2C%20and%20sustainable%20innovation%20within%20the%20area%20of%20business%0Aanalytics%20and%20SMEs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.10594v2&entry.124074799=Read"},
{"title": "nanoTabPFN: A Lightweight and Educational Reimplementation of TabPFN", "author": "Alexander Pfefferle and Johannes Hog and Lennart Purucker and Frank Hutter", "abstract": "  Tabular foundation models such as TabPFN have revolutionized predictive\nmachine learning for tabular data. At the same time, the driving factors of\nthis revolution are hard to understand. Existing open-source tabular foundation\nmodels are implemented in complicated pipelines boasting over 10,000 lines of\ncode, lack architecture documentation or code quality. In short, the\nimplementations are hard to understand, not beginner-friendly, and complicated\nto adapt for new experiments. We introduce nanoTabPFN, a simplified and\nlightweight implementation of the TabPFN v2 architecture and a corresponding\ntraining loop that uses pre-generated training data. nanoTabPFN makes tabular\nfoundation models more accessible to students and researchers alike. For\nexample, restricted to a small data setting it achieves a performance\ncomparable to traditional machine learning baselines within one minute of\npre-training on a single GPU (160,000x faster than TabPFN v2 pretraining). This\neliminated requirement of large computational resources makes pre-training\ntabular foundation models accessible for educational purposes. Our code is\navailable at https://github.com/automl/nanoTabPFN.\n", "link": "http://arxiv.org/abs/2511.03634v1", "date": "2025-11-05", "relevancy": 1.3039, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4401}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4284}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20nanoTabPFN%3A%20A%20Lightweight%20and%20Educational%20Reimplementation%20of%20TabPFN&body=Title%3A%20nanoTabPFN%3A%20A%20Lightweight%20and%20Educational%20Reimplementation%20of%20TabPFN%0AAuthor%3A%20Alexander%20Pfefferle%20and%20Johannes%20Hog%20and%20Lennart%20Purucker%20and%20Frank%20Hutter%0AAbstract%3A%20%20%20Tabular%20foundation%20models%20such%20as%20TabPFN%20have%20revolutionized%20predictive%0Amachine%20learning%20for%20tabular%20data.%20At%20the%20same%20time%2C%20the%20driving%20factors%20of%0Athis%20revolution%20are%20hard%20to%20understand.%20Existing%20open-source%20tabular%20foundation%0Amodels%20are%20implemented%20in%20complicated%20pipelines%20boasting%20over%2010%2C000%20lines%20of%0Acode%2C%20lack%20architecture%20documentation%20or%20code%20quality.%20In%20short%2C%20the%0Aimplementations%20are%20hard%20to%20understand%2C%20not%20beginner-friendly%2C%20and%20complicated%0Ato%20adapt%20for%20new%20experiments.%20We%20introduce%20nanoTabPFN%2C%20a%20simplified%20and%0Alightweight%20implementation%20of%20the%20TabPFN%20v2%20architecture%20and%20a%20corresponding%0Atraining%20loop%20that%20uses%20pre-generated%20training%20data.%20nanoTabPFN%20makes%20tabular%0Afoundation%20models%20more%20accessible%20to%20students%20and%20researchers%20alike.%20For%0Aexample%2C%20restricted%20to%20a%20small%20data%20setting%20it%20achieves%20a%20performance%0Acomparable%20to%20traditional%20machine%20learning%20baselines%20within%20one%20minute%20of%0Apre-training%20on%20a%20single%20GPU%20%28160%2C000x%20faster%20than%20TabPFN%20v2%20pretraining%29.%20This%0Aeliminated%20requirement%20of%20large%20computational%20resources%20makes%20pre-training%0Atabular%20foundation%20models%20accessible%20for%20educational%20purposes.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/automl/nanoTabPFN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03634v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DnanoTabPFN%253A%2520A%2520Lightweight%2520and%2520Educational%2520Reimplementation%2520of%2520TabPFN%26entry.906535625%3DAlexander%2520Pfefferle%2520and%2520Johannes%2520Hog%2520and%2520Lennart%2520Purucker%2520and%2520Frank%2520Hutter%26entry.1292438233%3D%2520%2520Tabular%2520foundation%2520models%2520such%2520as%2520TabPFN%2520have%2520revolutionized%2520predictive%250Amachine%2520learning%2520for%2520tabular%2520data.%2520At%2520the%2520same%2520time%252C%2520the%2520driving%2520factors%2520of%250Athis%2520revolution%2520are%2520hard%2520to%2520understand.%2520Existing%2520open-source%2520tabular%2520foundation%250Amodels%2520are%2520implemented%2520in%2520complicated%2520pipelines%2520boasting%2520over%252010%252C000%2520lines%2520of%250Acode%252C%2520lack%2520architecture%2520documentation%2520or%2520code%2520quality.%2520In%2520short%252C%2520the%250Aimplementations%2520are%2520hard%2520to%2520understand%252C%2520not%2520beginner-friendly%252C%2520and%2520complicated%250Ato%2520adapt%2520for%2520new%2520experiments.%2520We%2520introduce%2520nanoTabPFN%252C%2520a%2520simplified%2520and%250Alightweight%2520implementation%2520of%2520the%2520TabPFN%2520v2%2520architecture%2520and%2520a%2520corresponding%250Atraining%2520loop%2520that%2520uses%2520pre-generated%2520training%2520data.%2520nanoTabPFN%2520makes%2520tabular%250Afoundation%2520models%2520more%2520accessible%2520to%2520students%2520and%2520researchers%2520alike.%2520For%250Aexample%252C%2520restricted%2520to%2520a%2520small%2520data%2520setting%2520it%2520achieves%2520a%2520performance%250Acomparable%2520to%2520traditional%2520machine%2520learning%2520baselines%2520within%2520one%2520minute%2520of%250Apre-training%2520on%2520a%2520single%2520GPU%2520%2528160%252C000x%2520faster%2520than%2520TabPFN%2520v2%2520pretraining%2529.%2520This%250Aeliminated%2520requirement%2520of%2520large%2520computational%2520resources%2520makes%2520pre-training%250Atabular%2520foundation%2520models%2520accessible%2520for%2520educational%2520purposes.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/automl/nanoTabPFN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03634v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=nanoTabPFN%3A%20A%20Lightweight%20and%20Educational%20Reimplementation%20of%20TabPFN&entry.906535625=Alexander%20Pfefferle%20and%20Johannes%20Hog%20and%20Lennart%20Purucker%20and%20Frank%20Hutter&entry.1292438233=%20%20Tabular%20foundation%20models%20such%20as%20TabPFN%20have%20revolutionized%20predictive%0Amachine%20learning%20for%20tabular%20data.%20At%20the%20same%20time%2C%20the%20driving%20factors%20of%0Athis%20revolution%20are%20hard%20to%20understand.%20Existing%20open-source%20tabular%20foundation%0Amodels%20are%20implemented%20in%20complicated%20pipelines%20boasting%20over%2010%2C000%20lines%20of%0Acode%2C%20lack%20architecture%20documentation%20or%20code%20quality.%20In%20short%2C%20the%0Aimplementations%20are%20hard%20to%20understand%2C%20not%20beginner-friendly%2C%20and%20complicated%0Ato%20adapt%20for%20new%20experiments.%20We%20introduce%20nanoTabPFN%2C%20a%20simplified%20and%0Alightweight%20implementation%20of%20the%20TabPFN%20v2%20architecture%20and%20a%20corresponding%0Atraining%20loop%20that%20uses%20pre-generated%20training%20data.%20nanoTabPFN%20makes%20tabular%0Afoundation%20models%20more%20accessible%20to%20students%20and%20researchers%20alike.%20For%0Aexample%2C%20restricted%20to%20a%20small%20data%20setting%20it%20achieves%20a%20performance%0Acomparable%20to%20traditional%20machine%20learning%20baselines%20within%20one%20minute%20of%0Apre-training%20on%20a%20single%20GPU%20%28160%2C000x%20faster%20than%20TabPFN%20v2%20pretraining%29.%20This%0Aeliminated%20requirement%20of%20large%20computational%20resources%20makes%20pre-training%0Atabular%20foundation%20models%20accessible%20for%20educational%20purposes.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/automl/nanoTabPFN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03634v1&entry.124074799=Read"},
{"title": "Vector-valued self-normalized concentration inequalities beyond\n  sub-Gaussianity", "author": "Diego Martinez-Taboada and Tomas Gonzalez and Aaditya Ramdas", "abstract": "  The study of self-normalized processes plays a crucial role in a wide range\nof applications, from sequential decision-making to econometrics. While the\nbehavior of self-normalized concentration has been widely investigated for\nscalar-valued processes, vector-valued processes remain comparatively\nunderexplored, especially outside of the sub-Gaussian framework. In this\ncontribution, we provide concentration bounds for self-normalized processes\nwith light tails beyond sub-Gaussianity (such as Bennett or Bernstein bounds).\nWe illustrate the relevance of our results in the context of online linear\nregression, with applications in (kernelized) linear bandits.\n", "link": "http://arxiv.org/abs/2511.03606v1", "date": "2025-11-05", "relevancy": 1.2917, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.435}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4308}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vector-valued%20self-normalized%20concentration%20inequalities%20beyond%0A%20%20sub-Gaussianity&body=Title%3A%20Vector-valued%20self-normalized%20concentration%20inequalities%20beyond%0A%20%20sub-Gaussianity%0AAuthor%3A%20Diego%20Martinez-Taboada%20and%20Tomas%20Gonzalez%20and%20Aaditya%20Ramdas%0AAbstract%3A%20%20%20The%20study%20of%20self-normalized%20processes%20plays%20a%20crucial%20role%20in%20a%20wide%20range%0Aof%20applications%2C%20from%20sequential%20decision-making%20to%20econometrics.%20While%20the%0Abehavior%20of%20self-normalized%20concentration%20has%20been%20widely%20investigated%20for%0Ascalar-valued%20processes%2C%20vector-valued%20processes%20remain%20comparatively%0Aunderexplored%2C%20especially%20outside%20of%20the%20sub-Gaussian%20framework.%20In%20this%0Acontribution%2C%20we%20provide%20concentration%20bounds%20for%20self-normalized%20processes%0Awith%20light%20tails%20beyond%20sub-Gaussianity%20%28such%20as%20Bennett%20or%20Bernstein%20bounds%29.%0AWe%20illustrate%20the%20relevance%20of%20our%20results%20in%20the%20context%20of%20online%20linear%0Aregression%2C%20with%20applications%20in%20%28kernelized%29%20linear%20bandits.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03606v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVector-valued%2520self-normalized%2520concentration%2520inequalities%2520beyond%250A%2520%2520sub-Gaussianity%26entry.906535625%3DDiego%2520Martinez-Taboada%2520and%2520Tomas%2520Gonzalez%2520and%2520Aaditya%2520Ramdas%26entry.1292438233%3D%2520%2520The%2520study%2520of%2520self-normalized%2520processes%2520plays%2520a%2520crucial%2520role%2520in%2520a%2520wide%2520range%250Aof%2520applications%252C%2520from%2520sequential%2520decision-making%2520to%2520econometrics.%2520While%2520the%250Abehavior%2520of%2520self-normalized%2520concentration%2520has%2520been%2520widely%2520investigated%2520for%250Ascalar-valued%2520processes%252C%2520vector-valued%2520processes%2520remain%2520comparatively%250Aunderexplored%252C%2520especially%2520outside%2520of%2520the%2520sub-Gaussian%2520framework.%2520In%2520this%250Acontribution%252C%2520we%2520provide%2520concentration%2520bounds%2520for%2520self-normalized%2520processes%250Awith%2520light%2520tails%2520beyond%2520sub-Gaussianity%2520%2528such%2520as%2520Bennett%2520or%2520Bernstein%2520bounds%2529.%250AWe%2520illustrate%2520the%2520relevance%2520of%2520our%2520results%2520in%2520the%2520context%2520of%2520online%2520linear%250Aregression%252C%2520with%2520applications%2520in%2520%2528kernelized%2529%2520linear%2520bandits.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03606v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vector-valued%20self-normalized%20concentration%20inequalities%20beyond%0A%20%20sub-Gaussianity&entry.906535625=Diego%20Martinez-Taboada%20and%20Tomas%20Gonzalez%20and%20Aaditya%20Ramdas&entry.1292438233=%20%20The%20study%20of%20self-normalized%20processes%20plays%20a%20crucial%20role%20in%20a%20wide%20range%0Aof%20applications%2C%20from%20sequential%20decision-making%20to%20econometrics.%20While%20the%0Abehavior%20of%20self-normalized%20concentration%20has%20been%20widely%20investigated%20for%0Ascalar-valued%20processes%2C%20vector-valued%20processes%20remain%20comparatively%0Aunderexplored%2C%20especially%20outside%20of%20the%20sub-Gaussian%20framework.%20In%20this%0Acontribution%2C%20we%20provide%20concentration%20bounds%20for%20self-normalized%20processes%0Awith%20light%20tails%20beyond%20sub-Gaussianity%20%28such%20as%20Bennett%20or%20Bernstein%20bounds%29.%0AWe%20illustrate%20the%20relevance%20of%20our%20results%20in%20the%20context%20of%20online%20linear%0Aregression%2C%20with%20applications%20in%20%28kernelized%29%20linear%20bandits.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03606v1&entry.124074799=Read"},
{"title": "HAFixAgent: History-Aware Automated Program Repair Agent", "author": "Yu Shi and Hao Li and Bram Adams and Ahmed E. Hassan", "abstract": "  Automated program repair (APR) has recently shifted toward large language\nmodels and agent-based systems, yet most systems rely on local snapshot\ncontext, overlooking repository history. Prior work shows that repository\nhistory helps repair single-line bugs, since the last commit touching the buggy\nline is often the bug-introducing one. In this paper, we investigate whether\nrepository history can also improve agentic APR systems at scale, especially\nfor complex multi-hunk bugs. We present HAFixAgent, a History-Aware Bug-Fixing\nAgent that injects blame-derived repository heuristics into its repair loop. A\npreliminary study of all 854 real-world bugs from Defects4J motivates our\ndesign, showing that bug-relevant history is both widely available and highly\nconcentrated. Empirical comparison of HAFixAgent with two state-of-the-art\nbaselines shows: (1) Effectiveness: HAFixAgent significantly improves over the\nagent-based baseline (by 212.3%) and the multi-hunk baseline (by 29.9%). (2)\nEfficiency: history does not significantly increase agent steps and keeps token\ncosts comparable, with notably lower median costs for complex\nmulti-file-multi-hunk bugs. (3) Practicality: combining different historical\nheuristics repairs more bugs, offering a clear cost-benefit trade-off.\nHAFixAgent offers a practical recipe for history-aware agentic APR: ground the\nagent in version control history, prioritize diff-based historical context, and\nintegrate complementary heuristics when needed.\n", "link": "http://arxiv.org/abs/2511.01047v2", "date": "2025-11-05", "relevancy": 1.2089, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4095}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4077}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HAFixAgent%3A%20History-Aware%20Automated%20Program%20Repair%20Agent&body=Title%3A%20HAFixAgent%3A%20History-Aware%20Automated%20Program%20Repair%20Agent%0AAuthor%3A%20Yu%20Shi%20and%20Hao%20Li%20and%20Bram%20Adams%20and%20Ahmed%20E.%20Hassan%0AAbstract%3A%20%20%20Automated%20program%20repair%20%28APR%29%20has%20recently%20shifted%20toward%20large%20language%0Amodels%20and%20agent-based%20systems%2C%20yet%20most%20systems%20rely%20on%20local%20snapshot%0Acontext%2C%20overlooking%20repository%20history.%20Prior%20work%20shows%20that%20repository%0Ahistory%20helps%20repair%20single-line%20bugs%2C%20since%20the%20last%20commit%20touching%20the%20buggy%0Aline%20is%20often%20the%20bug-introducing%20one.%20In%20this%20paper%2C%20we%20investigate%20whether%0Arepository%20history%20can%20also%20improve%20agentic%20APR%20systems%20at%20scale%2C%20especially%0Afor%20complex%20multi-hunk%20bugs.%20We%20present%20HAFixAgent%2C%20a%20History-Aware%20Bug-Fixing%0AAgent%20that%20injects%20blame-derived%20repository%20heuristics%20into%20its%20repair%20loop.%20A%0Apreliminary%20study%20of%20all%20854%20real-world%20bugs%20from%20Defects4J%20motivates%20our%0Adesign%2C%20showing%20that%20bug-relevant%20history%20is%20both%20widely%20available%20and%20highly%0Aconcentrated.%20Empirical%20comparison%20of%20HAFixAgent%20with%20two%20state-of-the-art%0Abaselines%20shows%3A%20%281%29%20Effectiveness%3A%20HAFixAgent%20significantly%20improves%20over%20the%0Aagent-based%20baseline%20%28by%20212.3%25%29%20and%20the%20multi-hunk%20baseline%20%28by%2029.9%25%29.%20%282%29%0AEfficiency%3A%20history%20does%20not%20significantly%20increase%20agent%20steps%20and%20keeps%20token%0Acosts%20comparable%2C%20with%20notably%20lower%20median%20costs%20for%20complex%0Amulti-file-multi-hunk%20bugs.%20%283%29%20Practicality%3A%20combining%20different%20historical%0Aheuristics%20repairs%20more%20bugs%2C%20offering%20a%20clear%20cost-benefit%20trade-off.%0AHAFixAgent%20offers%20a%20practical%20recipe%20for%20history-aware%20agentic%20APR%3A%20ground%20the%0Aagent%20in%20version%20control%20history%2C%20prioritize%20diff-based%20historical%20context%2C%20and%0Aintegrate%20complementary%20heuristics%20when%20needed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.01047v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHAFixAgent%253A%2520History-Aware%2520Automated%2520Program%2520Repair%2520Agent%26entry.906535625%3DYu%2520Shi%2520and%2520Hao%2520Li%2520and%2520Bram%2520Adams%2520and%2520Ahmed%2520E.%2520Hassan%26entry.1292438233%3D%2520%2520Automated%2520program%2520repair%2520%2528APR%2529%2520has%2520recently%2520shifted%2520toward%2520large%2520language%250Amodels%2520and%2520agent-based%2520systems%252C%2520yet%2520most%2520systems%2520rely%2520on%2520local%2520snapshot%250Acontext%252C%2520overlooking%2520repository%2520history.%2520Prior%2520work%2520shows%2520that%2520repository%250Ahistory%2520helps%2520repair%2520single-line%2520bugs%252C%2520since%2520the%2520last%2520commit%2520touching%2520the%2520buggy%250Aline%2520is%2520often%2520the%2520bug-introducing%2520one.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520whether%250Arepository%2520history%2520can%2520also%2520improve%2520agentic%2520APR%2520systems%2520at%2520scale%252C%2520especially%250Afor%2520complex%2520multi-hunk%2520bugs.%2520We%2520present%2520HAFixAgent%252C%2520a%2520History-Aware%2520Bug-Fixing%250AAgent%2520that%2520injects%2520blame-derived%2520repository%2520heuristics%2520into%2520its%2520repair%2520loop.%2520A%250Apreliminary%2520study%2520of%2520all%2520854%2520real-world%2520bugs%2520from%2520Defects4J%2520motivates%2520our%250Adesign%252C%2520showing%2520that%2520bug-relevant%2520history%2520is%2520both%2520widely%2520available%2520and%2520highly%250Aconcentrated.%2520Empirical%2520comparison%2520of%2520HAFixAgent%2520with%2520two%2520state-of-the-art%250Abaselines%2520shows%253A%2520%25281%2529%2520Effectiveness%253A%2520HAFixAgent%2520significantly%2520improves%2520over%2520the%250Aagent-based%2520baseline%2520%2528by%2520212.3%2525%2529%2520and%2520the%2520multi-hunk%2520baseline%2520%2528by%252029.9%2525%2529.%2520%25282%2529%250AEfficiency%253A%2520history%2520does%2520not%2520significantly%2520increase%2520agent%2520steps%2520and%2520keeps%2520token%250Acosts%2520comparable%252C%2520with%2520notably%2520lower%2520median%2520costs%2520for%2520complex%250Amulti-file-multi-hunk%2520bugs.%2520%25283%2529%2520Practicality%253A%2520combining%2520different%2520historical%250Aheuristics%2520repairs%2520more%2520bugs%252C%2520offering%2520a%2520clear%2520cost-benefit%2520trade-off.%250AHAFixAgent%2520offers%2520a%2520practical%2520recipe%2520for%2520history-aware%2520agentic%2520APR%253A%2520ground%2520the%250Aagent%2520in%2520version%2520control%2520history%252C%2520prioritize%2520diff-based%2520historical%2520context%252C%2520and%250Aintegrate%2520complementary%2520heuristics%2520when%2520needed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.01047v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HAFixAgent%3A%20History-Aware%20Automated%20Program%20Repair%20Agent&entry.906535625=Yu%20Shi%20and%20Hao%20Li%20and%20Bram%20Adams%20and%20Ahmed%20E.%20Hassan&entry.1292438233=%20%20Automated%20program%20repair%20%28APR%29%20has%20recently%20shifted%20toward%20large%20language%0Amodels%20and%20agent-based%20systems%2C%20yet%20most%20systems%20rely%20on%20local%20snapshot%0Acontext%2C%20overlooking%20repository%20history.%20Prior%20work%20shows%20that%20repository%0Ahistory%20helps%20repair%20single-line%20bugs%2C%20since%20the%20last%20commit%20touching%20the%20buggy%0Aline%20is%20often%20the%20bug-introducing%20one.%20In%20this%20paper%2C%20we%20investigate%20whether%0Arepository%20history%20can%20also%20improve%20agentic%20APR%20systems%20at%20scale%2C%20especially%0Afor%20complex%20multi-hunk%20bugs.%20We%20present%20HAFixAgent%2C%20a%20History-Aware%20Bug-Fixing%0AAgent%20that%20injects%20blame-derived%20repository%20heuristics%20into%20its%20repair%20loop.%20A%0Apreliminary%20study%20of%20all%20854%20real-world%20bugs%20from%20Defects4J%20motivates%20our%0Adesign%2C%20showing%20that%20bug-relevant%20history%20is%20both%20widely%20available%20and%20highly%0Aconcentrated.%20Empirical%20comparison%20of%20HAFixAgent%20with%20two%20state-of-the-art%0Abaselines%20shows%3A%20%281%29%20Effectiveness%3A%20HAFixAgent%20significantly%20improves%20over%20the%0Aagent-based%20baseline%20%28by%20212.3%25%29%20and%20the%20multi-hunk%20baseline%20%28by%2029.9%25%29.%20%282%29%0AEfficiency%3A%20history%20does%20not%20significantly%20increase%20agent%20steps%20and%20keeps%20token%0Acosts%20comparable%2C%20with%20notably%20lower%20median%20costs%20for%20complex%0Amulti-file-multi-hunk%20bugs.%20%283%29%20Practicality%3A%20combining%20different%20historical%0Aheuristics%20repairs%20more%20bugs%2C%20offering%20a%20clear%20cost-benefit%20trade-off.%0AHAFixAgent%20offers%20a%20practical%20recipe%20for%20history-aware%20agentic%20APR%3A%20ground%20the%0Aagent%20in%20version%20control%20history%2C%20prioritize%20diff-based%20historical%20context%2C%20and%0Aintegrate%20complementary%20heuristics%20when%20needed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.01047v2&entry.124074799=Read"},
{"title": "Financial Management System for SMEs: Real-World Deployment of Accounts\n  Receivable and Cash Flow Prediction", "author": "Bart\u0142omiej Ma\u0142kus and Szymon Bobek and Grzegorz J. Nalepa", "abstract": "  Small and Medium Enterprises (SMEs), particularly freelancers and early-stage\nbusinesses, face unique financial management challenges due to limited\nresources, small customer bases, and constrained data availability. This paper\npresents the development and deployment of an integrated financial prediction\nsystem that combines accounts receivable prediction and cash flow forecasting\nspecifically designed for SME operational constraints. Our system addresses the\ngap between enterprise-focused financial tools and the practical needs of\nfreelancers and small businesses. The solution integrates two key components: a\nbinary classification model for predicting invoice payment delays, and a\nmulti-module cash flow forecasting model that handles incomplete and limited\nhistorical data. A prototype system has been implemented and deployed as a web\napplication with integration into Cluee's platform, a startup providing\nfinancial management tools for freelancers, demonstrating practical feasibility\nfor real-world SME financial management.\n", "link": "http://arxiv.org/abs/2511.03631v1", "date": "2025-11-05", "relevancy": 0.7668, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4108}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3702}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.3692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Financial%20Management%20System%20for%20SMEs%3A%20Real-World%20Deployment%20of%20Accounts%0A%20%20Receivable%20and%20Cash%20Flow%20Prediction&body=Title%3A%20Financial%20Management%20System%20for%20SMEs%3A%20Real-World%20Deployment%20of%20Accounts%0A%20%20Receivable%20and%20Cash%20Flow%20Prediction%0AAuthor%3A%20Bart%C5%82omiej%20Ma%C5%82kus%20and%20Szymon%20Bobek%20and%20Grzegorz%20J.%20Nalepa%0AAbstract%3A%20%20%20Small%20and%20Medium%20Enterprises%20%28SMEs%29%2C%20particularly%20freelancers%20and%20early-stage%0Abusinesses%2C%20face%20unique%20financial%20management%20challenges%20due%20to%20limited%0Aresources%2C%20small%20customer%20bases%2C%20and%20constrained%20data%20availability.%20This%20paper%0Apresents%20the%20development%20and%20deployment%20of%20an%20integrated%20financial%20prediction%0Asystem%20that%20combines%20accounts%20receivable%20prediction%20and%20cash%20flow%20forecasting%0Aspecifically%20designed%20for%20SME%20operational%20constraints.%20Our%20system%20addresses%20the%0Agap%20between%20enterprise-focused%20financial%20tools%20and%20the%20practical%20needs%20of%0Afreelancers%20and%20small%20businesses.%20The%20solution%20integrates%20two%20key%20components%3A%20a%0Abinary%20classification%20model%20for%20predicting%20invoice%20payment%20delays%2C%20and%20a%0Amulti-module%20cash%20flow%20forecasting%20model%20that%20handles%20incomplete%20and%20limited%0Ahistorical%20data.%20A%20prototype%20system%20has%20been%20implemented%20and%20deployed%20as%20a%20web%0Aapplication%20with%20integration%20into%20Cluee%27s%20platform%2C%20a%20startup%20providing%0Afinancial%20management%20tools%20for%20freelancers%2C%20demonstrating%20practical%20feasibility%0Afor%20real-world%20SME%20financial%20management.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03631v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFinancial%2520Management%2520System%2520for%2520SMEs%253A%2520Real-World%2520Deployment%2520of%2520Accounts%250A%2520%2520Receivable%2520and%2520Cash%2520Flow%2520Prediction%26entry.906535625%3DBart%25C5%2582omiej%2520Ma%25C5%2582kus%2520and%2520Szymon%2520Bobek%2520and%2520Grzegorz%2520J.%2520Nalepa%26entry.1292438233%3D%2520%2520Small%2520and%2520Medium%2520Enterprises%2520%2528SMEs%2529%252C%2520particularly%2520freelancers%2520and%2520early-stage%250Abusinesses%252C%2520face%2520unique%2520financial%2520management%2520challenges%2520due%2520to%2520limited%250Aresources%252C%2520small%2520customer%2520bases%252C%2520and%2520constrained%2520data%2520availability.%2520This%2520paper%250Apresents%2520the%2520development%2520and%2520deployment%2520of%2520an%2520integrated%2520financial%2520prediction%250Asystem%2520that%2520combines%2520accounts%2520receivable%2520prediction%2520and%2520cash%2520flow%2520forecasting%250Aspecifically%2520designed%2520for%2520SME%2520operational%2520constraints.%2520Our%2520system%2520addresses%2520the%250Agap%2520between%2520enterprise-focused%2520financial%2520tools%2520and%2520the%2520practical%2520needs%2520of%250Afreelancers%2520and%2520small%2520businesses.%2520The%2520solution%2520integrates%2520two%2520key%2520components%253A%2520a%250Abinary%2520classification%2520model%2520for%2520predicting%2520invoice%2520payment%2520delays%252C%2520and%2520a%250Amulti-module%2520cash%2520flow%2520forecasting%2520model%2520that%2520handles%2520incomplete%2520and%2520limited%250Ahistorical%2520data.%2520A%2520prototype%2520system%2520has%2520been%2520implemented%2520and%2520deployed%2520as%2520a%2520web%250Aapplication%2520with%2520integration%2520into%2520Cluee%2527s%2520platform%252C%2520a%2520startup%2520providing%250Afinancial%2520management%2520tools%2520for%2520freelancers%252C%2520demonstrating%2520practical%2520feasibility%250Afor%2520real-world%2520SME%2520financial%2520management.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03631v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Financial%20Management%20System%20for%20SMEs%3A%20Real-World%20Deployment%20of%20Accounts%0A%20%20Receivable%20and%20Cash%20Flow%20Prediction&entry.906535625=Bart%C5%82omiej%20Ma%C5%82kus%20and%20Szymon%20Bobek%20and%20Grzegorz%20J.%20Nalepa&entry.1292438233=%20%20Small%20and%20Medium%20Enterprises%20%28SMEs%29%2C%20particularly%20freelancers%20and%20early-stage%0Abusinesses%2C%20face%20unique%20financial%20management%20challenges%20due%20to%20limited%0Aresources%2C%20small%20customer%20bases%2C%20and%20constrained%20data%20availability.%20This%20paper%0Apresents%20the%20development%20and%20deployment%20of%20an%20integrated%20financial%20prediction%0Asystem%20that%20combines%20accounts%20receivable%20prediction%20and%20cash%20flow%20forecasting%0Aspecifically%20designed%20for%20SME%20operational%20constraints.%20Our%20system%20addresses%20the%0Agap%20between%20enterprise-focused%20financial%20tools%20and%20the%20practical%20needs%20of%0Afreelancers%20and%20small%20businesses.%20The%20solution%20integrates%20two%20key%20components%3A%20a%0Abinary%20classification%20model%20for%20predicting%20invoice%20payment%20delays%2C%20and%20a%0Amulti-module%20cash%20flow%20forecasting%20model%20that%20handles%20incomplete%20and%20limited%0Ahistorical%20data.%20A%20prototype%20system%20has%20been%20implemented%20and%20deployed%20as%20a%20web%0Aapplication%20with%20integration%20into%20Cluee%27s%20platform%2C%20a%20startup%20providing%0Afinancial%20management%20tools%20for%20freelancers%2C%20demonstrating%20practical%20feasibility%0Afor%20real-world%20SME%20financial%20management.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03631v1&entry.124074799=Read"},
{"title": "SecRepoBench: Benchmarking Code Agents for Secure Code Completion in\n  Real-World Repositories", "author": "Chihao Shen and Connor Dilgren and Purva Chiniya and Luke Griffith and Yu Ding and Yizheng Chen", "abstract": "  This paper introduces SecRepoBench, a benchmark to evaluate code agents on\nsecure code completion in real-world repositories. SecRepoBench has 318 code\ncompletion tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 28\nstandalone LLMs and 13 code agents across 3 state-of-the-art agent frameworks\nusing our benchmark. We find that state-of-the-art LLMs struggle with\ngenerating correct and secure code completions. However, code agents\nsignificantly outperform standalone LLMs. We show that SecRepoBench is more\ndifficult than the prior state-of-the-art benchmark. Finally, our comprehensive\nanalysis provides insights into potential directions for enhancing the ability\nof code agents to write correct and secure code in real-world repositories.\n", "link": "http://arxiv.org/abs/2504.21205v2", "date": "2025-11-05", "relevancy": 1.2433, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4368}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.41}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SecRepoBench%3A%20Benchmarking%20Code%20Agents%20for%20Secure%20Code%20Completion%20in%0A%20%20Real-World%20Repositories&body=Title%3A%20SecRepoBench%3A%20Benchmarking%20Code%20Agents%20for%20Secure%20Code%20Completion%20in%0A%20%20Real-World%20Repositories%0AAuthor%3A%20Chihao%20Shen%20and%20Connor%20Dilgren%20and%20Purva%20Chiniya%20and%20Luke%20Griffith%20and%20Yu%20Ding%20and%20Yizheng%20Chen%0AAbstract%3A%20%20%20This%20paper%20introduces%20SecRepoBench%2C%20a%20benchmark%20to%20evaluate%20code%20agents%20on%0Asecure%20code%20completion%20in%20real-world%20repositories.%20SecRepoBench%20has%20318%20code%0Acompletion%20tasks%20in%2027%20C/C%2B%2B%20repositories%2C%20covering%2015%20CWEs.%20We%20evaluate%2028%0Astandalone%20LLMs%20and%2013%20code%20agents%20across%203%20state-of-the-art%20agent%20frameworks%0Ausing%20our%20benchmark.%20We%20find%20that%20state-of-the-art%20LLMs%20struggle%20with%0Agenerating%20correct%20and%20secure%20code%20completions.%20However%2C%20code%20agents%0Asignificantly%20outperform%20standalone%20LLMs.%20We%20show%20that%20SecRepoBench%20is%20more%0Adifficult%20than%20the%20prior%20state-of-the-art%20benchmark.%20Finally%2C%20our%20comprehensive%0Aanalysis%20provides%20insights%20into%20potential%20directions%20for%20enhancing%20the%20ability%0Aof%20code%20agents%20to%20write%20correct%20and%20secure%20code%20in%20real-world%20repositories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21205v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSecRepoBench%253A%2520Benchmarking%2520Code%2520Agents%2520for%2520Secure%2520Code%2520Completion%2520in%250A%2520%2520Real-World%2520Repositories%26entry.906535625%3DChihao%2520Shen%2520and%2520Connor%2520Dilgren%2520and%2520Purva%2520Chiniya%2520and%2520Luke%2520Griffith%2520and%2520Yu%2520Ding%2520and%2520Yizheng%2520Chen%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520SecRepoBench%252C%2520a%2520benchmark%2520to%2520evaluate%2520code%2520agents%2520on%250Asecure%2520code%2520completion%2520in%2520real-world%2520repositories.%2520SecRepoBench%2520has%2520318%2520code%250Acompletion%2520tasks%2520in%252027%2520C/C%252B%252B%2520repositories%252C%2520covering%252015%2520CWEs.%2520We%2520evaluate%252028%250Astandalone%2520LLMs%2520and%252013%2520code%2520agents%2520across%25203%2520state-of-the-art%2520agent%2520frameworks%250Ausing%2520our%2520benchmark.%2520We%2520find%2520that%2520state-of-the-art%2520LLMs%2520struggle%2520with%250Agenerating%2520correct%2520and%2520secure%2520code%2520completions.%2520However%252C%2520code%2520agents%250Asignificantly%2520outperform%2520standalone%2520LLMs.%2520We%2520show%2520that%2520SecRepoBench%2520is%2520more%250Adifficult%2520than%2520the%2520prior%2520state-of-the-art%2520benchmark.%2520Finally%252C%2520our%2520comprehensive%250Aanalysis%2520provides%2520insights%2520into%2520potential%2520directions%2520for%2520enhancing%2520the%2520ability%250Aof%2520code%2520agents%2520to%2520write%2520correct%2520and%2520secure%2520code%2520in%2520real-world%2520repositories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21205v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SecRepoBench%3A%20Benchmarking%20Code%20Agents%20for%20Secure%20Code%20Completion%20in%0A%20%20Real-World%20Repositories&entry.906535625=Chihao%20Shen%20and%20Connor%20Dilgren%20and%20Purva%20Chiniya%20and%20Luke%20Griffith%20and%20Yu%20Ding%20and%20Yizheng%20Chen&entry.1292438233=%20%20This%20paper%20introduces%20SecRepoBench%2C%20a%20benchmark%20to%20evaluate%20code%20agents%20on%0Asecure%20code%20completion%20in%20real-world%20repositories.%20SecRepoBench%20has%20318%20code%0Acompletion%20tasks%20in%2027%20C/C%2B%2B%20repositories%2C%20covering%2015%20CWEs.%20We%20evaluate%2028%0Astandalone%20LLMs%20and%2013%20code%20agents%20across%203%20state-of-the-art%20agent%20frameworks%0Ausing%20our%20benchmark.%20We%20find%20that%20state-of-the-art%20LLMs%20struggle%20with%0Agenerating%20correct%20and%20secure%20code%20completions.%20However%2C%20code%20agents%0Asignificantly%20outperform%20standalone%20LLMs.%20We%20show%20that%20SecRepoBench%20is%20more%0Adifficult%20than%20the%20prior%20state-of-the-art%20benchmark.%20Finally%2C%20our%20comprehensive%0Aanalysis%20provides%20insights%20into%20potential%20directions%20for%20enhancing%20the%20ability%0Aof%20code%20agents%20to%20write%20correct%20and%20secure%20code%20in%20real-world%20repositories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21205v2&entry.124074799=Read"},
{"title": "Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via\n  Self-Play and Reinforcement Learning", "author": "Richard Dewey and Janos Botyanszki and Ciamac C. Moallemi and Andrew T. Zheng", "abstract": "  AI researchers have long focused on poker-like games as a testbed for\nenvironments characterized by multi-player dynamics, imperfect information, and\nreasoning under uncertainty. While recent breakthroughs have matched elite\nhuman play at no-limit Texas hold'em, the multi-player dynamics are subdued:\nmost hands converge quickly with only two players engaged through multiple\nrounds of bidding. In this paper, we present Solly, the first AI agent to\nachieve elite human play in reduced-format Liar's Poker, a game characterized\nby extensive multi-player engagement. We trained Solly using self-play with a\nmodel-free, actor-critic, deep reinforcement learning algorithm. Solly played\nat an elite human level as measured by win rate (won over 50% of hands) and\nequity (money won) in heads-up and multi-player Liar's Poker. Solly also\noutperformed large language models (LLMs), including those with reasoning\nabilities, on the same metrics. Solly developed novel bidding strategies,\nrandomized play effectively, and was not easily exploitable by world-class\nhuman players.\n", "link": "http://arxiv.org/abs/2511.03724v1", "date": "2025-11-05", "relevancy": 1.2847, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4384}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4316}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Outbidding%20and%20Outbluffing%20Elite%20Humans%3A%20Mastering%20Liar%27s%20Poker%20via%0A%20%20Self-Play%20and%20Reinforcement%20Learning&body=Title%3A%20Outbidding%20and%20Outbluffing%20Elite%20Humans%3A%20Mastering%20Liar%27s%20Poker%20via%0A%20%20Self-Play%20and%20Reinforcement%20Learning%0AAuthor%3A%20Richard%20Dewey%20and%20Janos%20Botyanszki%20and%20Ciamac%20C.%20Moallemi%20and%20Andrew%20T.%20Zheng%0AAbstract%3A%20%20%20AI%20researchers%20have%20long%20focused%20on%20poker-like%20games%20as%20a%20testbed%20for%0Aenvironments%20characterized%20by%20multi-player%20dynamics%2C%20imperfect%20information%2C%20and%0Areasoning%20under%20uncertainty.%20While%20recent%20breakthroughs%20have%20matched%20elite%0Ahuman%20play%20at%20no-limit%20Texas%20hold%27em%2C%20the%20multi-player%20dynamics%20are%20subdued%3A%0Amost%20hands%20converge%20quickly%20with%20only%20two%20players%20engaged%20through%20multiple%0Arounds%20of%20bidding.%20In%20this%20paper%2C%20we%20present%20Solly%2C%20the%20first%20AI%20agent%20to%0Aachieve%20elite%20human%20play%20in%20reduced-format%20Liar%27s%20Poker%2C%20a%20game%20characterized%0Aby%20extensive%20multi-player%20engagement.%20We%20trained%20Solly%20using%20self-play%20with%20a%0Amodel-free%2C%20actor-critic%2C%20deep%20reinforcement%20learning%20algorithm.%20Solly%20played%0Aat%20an%20elite%20human%20level%20as%20measured%20by%20win%20rate%20%28won%20over%2050%25%20of%20hands%29%20and%0Aequity%20%28money%20won%29%20in%20heads-up%20and%20multi-player%20Liar%27s%20Poker.%20Solly%20also%0Aoutperformed%20large%20language%20models%20%28LLMs%29%2C%20including%20those%20with%20reasoning%0Aabilities%2C%20on%20the%20same%20metrics.%20Solly%20developed%20novel%20bidding%20strategies%2C%0Arandomized%20play%20effectively%2C%20and%20was%20not%20easily%20exploitable%20by%20world-class%0Ahuman%20players.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03724v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOutbidding%2520and%2520Outbluffing%2520Elite%2520Humans%253A%2520Mastering%2520Liar%2527s%2520Poker%2520via%250A%2520%2520Self-Play%2520and%2520Reinforcement%2520Learning%26entry.906535625%3DRichard%2520Dewey%2520and%2520Janos%2520Botyanszki%2520and%2520Ciamac%2520C.%2520Moallemi%2520and%2520Andrew%2520T.%2520Zheng%26entry.1292438233%3D%2520%2520AI%2520researchers%2520have%2520long%2520focused%2520on%2520poker-like%2520games%2520as%2520a%2520testbed%2520for%250Aenvironments%2520characterized%2520by%2520multi-player%2520dynamics%252C%2520imperfect%2520information%252C%2520and%250Areasoning%2520under%2520uncertainty.%2520While%2520recent%2520breakthroughs%2520have%2520matched%2520elite%250Ahuman%2520play%2520at%2520no-limit%2520Texas%2520hold%2527em%252C%2520the%2520multi-player%2520dynamics%2520are%2520subdued%253A%250Amost%2520hands%2520converge%2520quickly%2520with%2520only%2520two%2520players%2520engaged%2520through%2520multiple%250Arounds%2520of%2520bidding.%2520In%2520this%2520paper%252C%2520we%2520present%2520Solly%252C%2520the%2520first%2520AI%2520agent%2520to%250Aachieve%2520elite%2520human%2520play%2520in%2520reduced-format%2520Liar%2527s%2520Poker%252C%2520a%2520game%2520characterized%250Aby%2520extensive%2520multi-player%2520engagement.%2520We%2520trained%2520Solly%2520using%2520self-play%2520with%2520a%250Amodel-free%252C%2520actor-critic%252C%2520deep%2520reinforcement%2520learning%2520algorithm.%2520Solly%2520played%250Aat%2520an%2520elite%2520human%2520level%2520as%2520measured%2520by%2520win%2520rate%2520%2528won%2520over%252050%2525%2520of%2520hands%2529%2520and%250Aequity%2520%2528money%2520won%2529%2520in%2520heads-up%2520and%2520multi-player%2520Liar%2527s%2520Poker.%2520Solly%2520also%250Aoutperformed%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520including%2520those%2520with%2520reasoning%250Aabilities%252C%2520on%2520the%2520same%2520metrics.%2520Solly%2520developed%2520novel%2520bidding%2520strategies%252C%250Arandomized%2520play%2520effectively%252C%2520and%2520was%2520not%2520easily%2520exploitable%2520by%2520world-class%250Ahuman%2520players.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03724v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Outbidding%20and%20Outbluffing%20Elite%20Humans%3A%20Mastering%20Liar%27s%20Poker%20via%0A%20%20Self-Play%20and%20Reinforcement%20Learning&entry.906535625=Richard%20Dewey%20and%20Janos%20Botyanszki%20and%20Ciamac%20C.%20Moallemi%20and%20Andrew%20T.%20Zheng&entry.1292438233=%20%20AI%20researchers%20have%20long%20focused%20on%20poker-like%20games%20as%20a%20testbed%20for%0Aenvironments%20characterized%20by%20multi-player%20dynamics%2C%20imperfect%20information%2C%20and%0Areasoning%20under%20uncertainty.%20While%20recent%20breakthroughs%20have%20matched%20elite%0Ahuman%20play%20at%20no-limit%20Texas%20hold%27em%2C%20the%20multi-player%20dynamics%20are%20subdued%3A%0Amost%20hands%20converge%20quickly%20with%20only%20two%20players%20engaged%20through%20multiple%0Arounds%20of%20bidding.%20In%20this%20paper%2C%20we%20present%20Solly%2C%20the%20first%20AI%20agent%20to%0Aachieve%20elite%20human%20play%20in%20reduced-format%20Liar%27s%20Poker%2C%20a%20game%20characterized%0Aby%20extensive%20multi-player%20engagement.%20We%20trained%20Solly%20using%20self-play%20with%20a%0Amodel-free%2C%20actor-critic%2C%20deep%20reinforcement%20learning%20algorithm.%20Solly%20played%0Aat%20an%20elite%20human%20level%20as%20measured%20by%20win%20rate%20%28won%20over%2050%25%20of%20hands%29%20and%0Aequity%20%28money%20won%29%20in%20heads-up%20and%20multi-player%20Liar%27s%20Poker.%20Solly%20also%0Aoutperformed%20large%20language%20models%20%28LLMs%29%2C%20including%20those%20with%20reasoning%0Aabilities%2C%20on%20the%20same%20metrics.%20Solly%20developed%20novel%20bidding%20strategies%2C%0Arandomized%20play%20effectively%2C%20and%20was%20not%20easily%20exploitable%20by%20world-class%0Ahuman%20players.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03724v1&entry.124074799=Read"},
{"title": "An explicit construction of Kaleidocycles by elliptic theta functions", "author": "Shizuo Kaji and Kenji Kajiwara and Shota Shigetomi", "abstract": "  We consider the configuration space of ordered points on the two-dimensional\nsphere that satisfy a specific system of quadratic equations. We construct\nperiodic orbits in this configuration space using elliptic theta functions and\nshow that they simultaneously satisfy semi-discrete analogues of mKdV and\nsine-Gordon equations. The configuration space we investigate corresponds to\nthe state space of a linkage mechanism known as the Kaleidocycle, and the\nconstructed orbits describe the characteristic motion of the Kaleidocycle. A\nkey consequence of our construction is the proof that Kaleidocycles exist for\nany number of tetrahedra greater than five. Our approach is founded on the\nrelationship between the deformation of spatial curves and integrable systems,\noffering an intriguing example where an integrable system is explicitly solved\nto generate an orbit in the space of real solutions to polynomial equations\ndefined by geometric constraints.\n", "link": "http://arxiv.org/abs/2308.04977v3", "date": "2025-11-05", "relevancy": 1.076, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.3655}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.3583}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.3527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20explicit%20construction%20of%20Kaleidocycles%20by%20elliptic%20theta%20functions&body=Title%3A%20An%20explicit%20construction%20of%20Kaleidocycles%20by%20elliptic%20theta%20functions%0AAuthor%3A%20Shizuo%20Kaji%20and%20Kenji%20Kajiwara%20and%20Shota%20Shigetomi%0AAbstract%3A%20%20%20We%20consider%20the%20configuration%20space%20of%20ordered%20points%20on%20the%20two-dimensional%0Asphere%20that%20satisfy%20a%20specific%20system%20of%20quadratic%20equations.%20We%20construct%0Aperiodic%20orbits%20in%20this%20configuration%20space%20using%20elliptic%20theta%20functions%20and%0Ashow%20that%20they%20simultaneously%20satisfy%20semi-discrete%20analogues%20of%20mKdV%20and%0Asine-Gordon%20equations.%20The%20configuration%20space%20we%20investigate%20corresponds%20to%0Athe%20state%20space%20of%20a%20linkage%20mechanism%20known%20as%20the%20Kaleidocycle%2C%20and%20the%0Aconstructed%20orbits%20describe%20the%20characteristic%20motion%20of%20the%20Kaleidocycle.%20A%0Akey%20consequence%20of%20our%20construction%20is%20the%20proof%20that%20Kaleidocycles%20exist%20for%0Aany%20number%20of%20tetrahedra%20greater%20than%20five.%20Our%20approach%20is%20founded%20on%20the%0Arelationship%20between%20the%20deformation%20of%20spatial%20curves%20and%20integrable%20systems%2C%0Aoffering%20an%20intriguing%20example%20where%20an%20integrable%20system%20is%20explicitly%20solved%0Ato%20generate%20an%20orbit%20in%20the%20space%20of%20real%20solutions%20to%20polynomial%20equations%0Adefined%20by%20geometric%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.04977v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520explicit%2520construction%2520of%2520Kaleidocycles%2520by%2520elliptic%2520theta%2520functions%26entry.906535625%3DShizuo%2520Kaji%2520and%2520Kenji%2520Kajiwara%2520and%2520Shota%2520Shigetomi%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520configuration%2520space%2520of%2520ordered%2520points%2520on%2520the%2520two-dimensional%250Asphere%2520that%2520satisfy%2520a%2520specific%2520system%2520of%2520quadratic%2520equations.%2520We%2520construct%250Aperiodic%2520orbits%2520in%2520this%2520configuration%2520space%2520using%2520elliptic%2520theta%2520functions%2520and%250Ashow%2520that%2520they%2520simultaneously%2520satisfy%2520semi-discrete%2520analogues%2520of%2520mKdV%2520and%250Asine-Gordon%2520equations.%2520The%2520configuration%2520space%2520we%2520investigate%2520corresponds%2520to%250Athe%2520state%2520space%2520of%2520a%2520linkage%2520mechanism%2520known%2520as%2520the%2520Kaleidocycle%252C%2520and%2520the%250Aconstructed%2520orbits%2520describe%2520the%2520characteristic%2520motion%2520of%2520the%2520Kaleidocycle.%2520A%250Akey%2520consequence%2520of%2520our%2520construction%2520is%2520the%2520proof%2520that%2520Kaleidocycles%2520exist%2520for%250Aany%2520number%2520of%2520tetrahedra%2520greater%2520than%2520five.%2520Our%2520approach%2520is%2520founded%2520on%2520the%250Arelationship%2520between%2520the%2520deformation%2520of%2520spatial%2520curves%2520and%2520integrable%2520systems%252C%250Aoffering%2520an%2520intriguing%2520example%2520where%2520an%2520integrable%2520system%2520is%2520explicitly%2520solved%250Ato%2520generate%2520an%2520orbit%2520in%2520the%2520space%2520of%2520real%2520solutions%2520to%2520polynomial%2520equations%250Adefined%2520by%2520geometric%2520constraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.04977v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20explicit%20construction%20of%20Kaleidocycles%20by%20elliptic%20theta%20functions&entry.906535625=Shizuo%20Kaji%20and%20Kenji%20Kajiwara%20and%20Shota%20Shigetomi&entry.1292438233=%20%20We%20consider%20the%20configuration%20space%20of%20ordered%20points%20on%20the%20two-dimensional%0Asphere%20that%20satisfy%20a%20specific%20system%20of%20quadratic%20equations.%20We%20construct%0Aperiodic%20orbits%20in%20this%20configuration%20space%20using%20elliptic%20theta%20functions%20and%0Ashow%20that%20they%20simultaneously%20satisfy%20semi-discrete%20analogues%20of%20mKdV%20and%0Asine-Gordon%20equations.%20The%20configuration%20space%20we%20investigate%20corresponds%20to%0Athe%20state%20space%20of%20a%20linkage%20mechanism%20known%20as%20the%20Kaleidocycle%2C%20and%20the%0Aconstructed%20orbits%20describe%20the%20characteristic%20motion%20of%20the%20Kaleidocycle.%20A%0Akey%20consequence%20of%20our%20construction%20is%20the%20proof%20that%20Kaleidocycles%20exist%20for%0Aany%20number%20of%20tetrahedra%20greater%20than%20five.%20Our%20approach%20is%20founded%20on%20the%0Arelationship%20between%20the%20deformation%20of%20spatial%20curves%20and%20integrable%20systems%2C%0Aoffering%20an%20intriguing%20example%20where%20an%20integrable%20system%20is%20explicitly%20solved%0Ato%20generate%20an%20orbit%20in%20the%20space%20of%20real%20solutions%20to%20polynomial%20equations%0Adefined%20by%20geometric%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.04977v3&entry.124074799=Read"},
{"title": "AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and\n  Sample-Efficient Analog Circuit Sizing", "author": "Mohsen Ahmadzadeh and Kaichang Chen and Georges Gielen", "abstract": "  Analog/mixed-signal circuits are key for interfacing electronics with the\nphysical world. Their design, however, remains a largely handcrafted process,\nresulting in long and error-prone design cycles. While the recent rise of\nAI-based reinforcement learning and generative AI has created new techniques to\nautomate this task, the need for many time-consuming simulations is a critical\nbottleneck hindering the overall efficiency. Furthermore, the lack of\nexplainability of the resulting design solutions hampers widespread adoption of\nthe tools. To address these issues, a novel agentic AI framework for\nsample-efficient and explainable analog circuit sizing is presented. It employs\na multi-agent workflow where specialized Large Language Model (LLM)-based\nagents collaborate to interpret the circuit topology, to understand the design\ngoals, and to iteratively refine the circuit's design parameters towards the\ntarget goals with human-interpretable reasoning. The adaptive simulation\nstrategy creates an intelligent control that yields a high sample efficiency.\nThe AnaFlow framework is demonstrated for two circuits of varying complexity\nand is able to complete the sizing task fully automatically, differently from\npure Bayesian optimization and reinforcement learning approaches. The system\nlearns from its optimization history to avoid past mistakes and to accelerate\nconvergence. The inherent explainability makes this a powerful tool for analog\ndesign space exploration and a new paradigm in analog EDA, where AI agents\nserve as transparent design assistants.\n", "link": "http://arxiv.org/abs/2511.03697v1", "date": "2025-11-05", "relevancy": 0.9785, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5007}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4918}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnaFlow%3A%20Agentic%20LLM-based%20Workflow%20for%20Reasoning-Driven%20Explainable%20and%0A%20%20Sample-Efficient%20Analog%20Circuit%20Sizing&body=Title%3A%20AnaFlow%3A%20Agentic%20LLM-based%20Workflow%20for%20Reasoning-Driven%20Explainable%20and%0A%20%20Sample-Efficient%20Analog%20Circuit%20Sizing%0AAuthor%3A%20Mohsen%20Ahmadzadeh%20and%20Kaichang%20Chen%20and%20Georges%20Gielen%0AAbstract%3A%20%20%20Analog/mixed-signal%20circuits%20are%20key%20for%20interfacing%20electronics%20with%20the%0Aphysical%20world.%20Their%20design%2C%20however%2C%20remains%20a%20largely%20handcrafted%20process%2C%0Aresulting%20in%20long%20and%20error-prone%20design%20cycles.%20While%20the%20recent%20rise%20of%0AAI-based%20reinforcement%20learning%20and%20generative%20AI%20has%20created%20new%20techniques%20to%0Aautomate%20this%20task%2C%20the%20need%20for%20many%20time-consuming%20simulations%20is%20a%20critical%0Abottleneck%20hindering%20the%20overall%20efficiency.%20Furthermore%2C%20the%20lack%20of%0Aexplainability%20of%20the%20resulting%20design%20solutions%20hampers%20widespread%20adoption%20of%0Athe%20tools.%20To%20address%20these%20issues%2C%20a%20novel%20agentic%20AI%20framework%20for%0Asample-efficient%20and%20explainable%20analog%20circuit%20sizing%20is%20presented.%20It%20employs%0Aa%20multi-agent%20workflow%20where%20specialized%20Large%20Language%20Model%20%28LLM%29-based%0Aagents%20collaborate%20to%20interpret%20the%20circuit%20topology%2C%20to%20understand%20the%20design%0Agoals%2C%20and%20to%20iteratively%20refine%20the%20circuit%27s%20design%20parameters%20towards%20the%0Atarget%20goals%20with%20human-interpretable%20reasoning.%20The%20adaptive%20simulation%0Astrategy%20creates%20an%20intelligent%20control%20that%20yields%20a%20high%20sample%20efficiency.%0AThe%20AnaFlow%20framework%20is%20demonstrated%20for%20two%20circuits%20of%20varying%20complexity%0Aand%20is%20able%20to%20complete%20the%20sizing%20task%20fully%20automatically%2C%20differently%20from%0Apure%20Bayesian%20optimization%20and%20reinforcement%20learning%20approaches.%20The%20system%0Alearns%20from%20its%20optimization%20history%20to%20avoid%20past%20mistakes%20and%20to%20accelerate%0Aconvergence.%20The%20inherent%20explainability%20makes%20this%20a%20powerful%20tool%20for%20analog%0Adesign%20space%20exploration%20and%20a%20new%20paradigm%20in%20analog%20EDA%2C%20where%20AI%20agents%0Aserve%20as%20transparent%20design%20assistants.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.03697v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnaFlow%253A%2520Agentic%2520LLM-based%2520Workflow%2520for%2520Reasoning-Driven%2520Explainable%2520and%250A%2520%2520Sample-Efficient%2520Analog%2520Circuit%2520Sizing%26entry.906535625%3DMohsen%2520Ahmadzadeh%2520and%2520Kaichang%2520Chen%2520and%2520Georges%2520Gielen%26entry.1292438233%3D%2520%2520Analog/mixed-signal%2520circuits%2520are%2520key%2520for%2520interfacing%2520electronics%2520with%2520the%250Aphysical%2520world.%2520Their%2520design%252C%2520however%252C%2520remains%2520a%2520largely%2520handcrafted%2520process%252C%250Aresulting%2520in%2520long%2520and%2520error-prone%2520design%2520cycles.%2520While%2520the%2520recent%2520rise%2520of%250AAI-based%2520reinforcement%2520learning%2520and%2520generative%2520AI%2520has%2520created%2520new%2520techniques%2520to%250Aautomate%2520this%2520task%252C%2520the%2520need%2520for%2520many%2520time-consuming%2520simulations%2520is%2520a%2520critical%250Abottleneck%2520hindering%2520the%2520overall%2520efficiency.%2520Furthermore%252C%2520the%2520lack%2520of%250Aexplainability%2520of%2520the%2520resulting%2520design%2520solutions%2520hampers%2520widespread%2520adoption%2520of%250Athe%2520tools.%2520To%2520address%2520these%2520issues%252C%2520a%2520novel%2520agentic%2520AI%2520framework%2520for%250Asample-efficient%2520and%2520explainable%2520analog%2520circuit%2520sizing%2520is%2520presented.%2520It%2520employs%250Aa%2520multi-agent%2520workflow%2520where%2520specialized%2520Large%2520Language%2520Model%2520%2528LLM%2529-based%250Aagents%2520collaborate%2520to%2520interpret%2520the%2520circuit%2520topology%252C%2520to%2520understand%2520the%2520design%250Agoals%252C%2520and%2520to%2520iteratively%2520refine%2520the%2520circuit%2527s%2520design%2520parameters%2520towards%2520the%250Atarget%2520goals%2520with%2520human-interpretable%2520reasoning.%2520The%2520adaptive%2520simulation%250Astrategy%2520creates%2520an%2520intelligent%2520control%2520that%2520yields%2520a%2520high%2520sample%2520efficiency.%250AThe%2520AnaFlow%2520framework%2520is%2520demonstrated%2520for%2520two%2520circuits%2520of%2520varying%2520complexity%250Aand%2520is%2520able%2520to%2520complete%2520the%2520sizing%2520task%2520fully%2520automatically%252C%2520differently%2520from%250Apure%2520Bayesian%2520optimization%2520and%2520reinforcement%2520learning%2520approaches.%2520The%2520system%250Alearns%2520from%2520its%2520optimization%2520history%2520to%2520avoid%2520past%2520mistakes%2520and%2520to%2520accelerate%250Aconvergence.%2520The%2520inherent%2520explainability%2520makes%2520this%2520a%2520powerful%2520tool%2520for%2520analog%250Adesign%2520space%2520exploration%2520and%2520a%2520new%2520paradigm%2520in%2520analog%2520EDA%252C%2520where%2520AI%2520agents%250Aserve%2520as%2520transparent%2520design%2520assistants.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03697v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnaFlow%3A%20Agentic%20LLM-based%20Workflow%20for%20Reasoning-Driven%20Explainable%20and%0A%20%20Sample-Efficient%20Analog%20Circuit%20Sizing&entry.906535625=Mohsen%20Ahmadzadeh%20and%20Kaichang%20Chen%20and%20Georges%20Gielen&entry.1292438233=%20%20Analog/mixed-signal%20circuits%20are%20key%20for%20interfacing%20electronics%20with%20the%0Aphysical%20world.%20Their%20design%2C%20however%2C%20remains%20a%20largely%20handcrafted%20process%2C%0Aresulting%20in%20long%20and%20error-prone%20design%20cycles.%20While%20the%20recent%20rise%20of%0AAI-based%20reinforcement%20learning%20and%20generative%20AI%20has%20created%20new%20techniques%20to%0Aautomate%20this%20task%2C%20the%20need%20for%20many%20time-consuming%20simulations%20is%20a%20critical%0Abottleneck%20hindering%20the%20overall%20efficiency.%20Furthermore%2C%20the%20lack%20of%0Aexplainability%20of%20the%20resulting%20design%20solutions%20hampers%20widespread%20adoption%20of%0Athe%20tools.%20To%20address%20these%20issues%2C%20a%20novel%20agentic%20AI%20framework%20for%0Asample-efficient%20and%20explainable%20analog%20circuit%20sizing%20is%20presented.%20It%20employs%0Aa%20multi-agent%20workflow%20where%20specialized%20Large%20Language%20Model%20%28LLM%29-based%0Aagents%20collaborate%20to%20interpret%20the%20circuit%20topology%2C%20to%20understand%20the%20design%0Agoals%2C%20and%20to%20iteratively%20refine%20the%20circuit%27s%20design%20parameters%20towards%20the%0Atarget%20goals%20with%20human-interpretable%20reasoning.%20The%20adaptive%20simulation%0Astrategy%20creates%20an%20intelligent%20control%20that%20yields%20a%20high%20sample%20efficiency.%0AThe%20AnaFlow%20framework%20is%20demonstrated%20for%20two%20circuits%20of%20varying%20complexity%0Aand%20is%20able%20to%20complete%20the%20sizing%20task%20fully%20automatically%2C%20differently%20from%0Apure%20Bayesian%20optimization%20and%20reinforcement%20learning%20approaches.%20The%20system%0Alearns%20from%20its%20optimization%20history%20to%20avoid%20past%20mistakes%20and%20to%20accelerate%0Aconvergence.%20The%20inherent%20explainability%20makes%20this%20a%20powerful%20tool%20for%20analog%0Adesign%20space%20exploration%20and%20a%20new%20paradigm%20in%20analog%20EDA%2C%20where%20AI%20agents%0Aserve%20as%20transparent%20design%20assistants.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.03697v1&entry.124074799=Read"},
{"title": "Fast weight programming and linear transformers: from machine learning\n  to neurobiology", "author": "Kazuki Irie and Samuel J. Gershman", "abstract": "  Recent advances in artificial neural networks for machine learning, and\nlanguage modeling in particular, have established a family of recurrent neural\nnetwork (RNN) architectures that, unlike conventional RNNs with vector-form\nhidden states, use two-dimensional (2D) matrix-form hidden states. Such\n2D-state RNNs, known as Fast Weight Programmers (FWPs), can be interpreted as a\nneural network whose synaptic weights (called fast weights) dynamically change\nover time as a function of input observations, and serve as short-term memory\nstorage; corresponding synaptic weight modifications are controlled or\nprogrammed by another network (the programmer) whose parameters are trained\n(e.g., by gradient descent). In this Primer, we review the technical\nfoundations of FWPs, their computational characteristics, and their connections\nto transformers and state space models. We also discuss connections between\nFWPs and models of synaptic plasticity in the brain, suggesting a convergence\nof natural and artificial intelligence.\n", "link": "http://arxiv.org/abs/2508.08435v2", "date": "2025-11-05", "relevancy": 0.9335, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5178}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4425}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.44}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20weight%20programming%20and%20linear%20transformers%3A%20from%20machine%20learning%0A%20%20to%20neurobiology&body=Title%3A%20Fast%20weight%20programming%20and%20linear%20transformers%3A%20from%20machine%20learning%0A%20%20to%20neurobiology%0AAuthor%3A%20Kazuki%20Irie%20and%20Samuel%20J.%20Gershman%0AAbstract%3A%20%20%20Recent%20advances%20in%20artificial%20neural%20networks%20for%20machine%20learning%2C%20and%0Alanguage%20modeling%20in%20particular%2C%20have%20established%20a%20family%20of%20recurrent%20neural%0Anetwork%20%28RNN%29%20architectures%20that%2C%20unlike%20conventional%20RNNs%20with%20vector-form%0Ahidden%20states%2C%20use%20two-dimensional%20%282D%29%20matrix-form%20hidden%20states.%20Such%0A2D-state%20RNNs%2C%20known%20as%20Fast%20Weight%20Programmers%20%28FWPs%29%2C%20can%20be%20interpreted%20as%20a%0Aneural%20network%20whose%20synaptic%20weights%20%28called%20fast%20weights%29%20dynamically%20change%0Aover%20time%20as%20a%20function%20of%20input%20observations%2C%20and%20serve%20as%20short-term%20memory%0Astorage%3B%20corresponding%20synaptic%20weight%20modifications%20are%20controlled%20or%0Aprogrammed%20by%20another%20network%20%28the%20programmer%29%20whose%20parameters%20are%20trained%0A%28e.g.%2C%20by%20gradient%20descent%29.%20In%20this%20Primer%2C%20we%20review%20the%20technical%0Afoundations%20of%20FWPs%2C%20their%20computational%20characteristics%2C%20and%20their%20connections%0Ato%20transformers%20and%20state%20space%20models.%20We%20also%20discuss%20connections%20between%0AFWPs%20and%20models%20of%20synaptic%20plasticity%20in%20the%20brain%2C%20suggesting%20a%20convergence%0Aof%20natural%20and%20artificial%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08435v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520weight%2520programming%2520and%2520linear%2520transformers%253A%2520from%2520machine%2520learning%250A%2520%2520to%2520neurobiology%26entry.906535625%3DKazuki%2520Irie%2520and%2520Samuel%2520J.%2520Gershman%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520artificial%2520neural%2520networks%2520for%2520machine%2520learning%252C%2520and%250Alanguage%2520modeling%2520in%2520particular%252C%2520have%2520established%2520a%2520family%2520of%2520recurrent%2520neural%250Anetwork%2520%2528RNN%2529%2520architectures%2520that%252C%2520unlike%2520conventional%2520RNNs%2520with%2520vector-form%250Ahidden%2520states%252C%2520use%2520two-dimensional%2520%25282D%2529%2520matrix-form%2520hidden%2520states.%2520Such%250A2D-state%2520RNNs%252C%2520known%2520as%2520Fast%2520Weight%2520Programmers%2520%2528FWPs%2529%252C%2520can%2520be%2520interpreted%2520as%2520a%250Aneural%2520network%2520whose%2520synaptic%2520weights%2520%2528called%2520fast%2520weights%2529%2520dynamically%2520change%250Aover%2520time%2520as%2520a%2520function%2520of%2520input%2520observations%252C%2520and%2520serve%2520as%2520short-term%2520memory%250Astorage%253B%2520corresponding%2520synaptic%2520weight%2520modifications%2520are%2520controlled%2520or%250Aprogrammed%2520by%2520another%2520network%2520%2528the%2520programmer%2529%2520whose%2520parameters%2520are%2520trained%250A%2528e.g.%252C%2520by%2520gradient%2520descent%2529.%2520In%2520this%2520Primer%252C%2520we%2520review%2520the%2520technical%250Afoundations%2520of%2520FWPs%252C%2520their%2520computational%2520characteristics%252C%2520and%2520their%2520connections%250Ato%2520transformers%2520and%2520state%2520space%2520models.%2520We%2520also%2520discuss%2520connections%2520between%250AFWPs%2520and%2520models%2520of%2520synaptic%2520plasticity%2520in%2520the%2520brain%252C%2520suggesting%2520a%2520convergence%250Aof%2520natural%2520and%2520artificial%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08435v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20weight%20programming%20and%20linear%20transformers%3A%20from%20machine%20learning%0A%20%20to%20neurobiology&entry.906535625=Kazuki%20Irie%20and%20Samuel%20J.%20Gershman&entry.1292438233=%20%20Recent%20advances%20in%20artificial%20neural%20networks%20for%20machine%20learning%2C%20and%0Alanguage%20modeling%20in%20particular%2C%20have%20established%20a%20family%20of%20recurrent%20neural%0Anetwork%20%28RNN%29%20architectures%20that%2C%20unlike%20conventional%20RNNs%20with%20vector-form%0Ahidden%20states%2C%20use%20two-dimensional%20%282D%29%20matrix-form%20hidden%20states.%20Such%0A2D-state%20RNNs%2C%20known%20as%20Fast%20Weight%20Programmers%20%28FWPs%29%2C%20can%20be%20interpreted%20as%20a%0Aneural%20network%20whose%20synaptic%20weights%20%28called%20fast%20weights%29%20dynamically%20change%0Aover%20time%20as%20a%20function%20of%20input%20observations%2C%20and%20serve%20as%20short-term%20memory%0Astorage%3B%20corresponding%20synaptic%20weight%20modifications%20are%20controlled%20or%0Aprogrammed%20by%20another%20network%20%28the%20programmer%29%20whose%20parameters%20are%20trained%0A%28e.g.%2C%20by%20gradient%20descent%29.%20In%20this%20Primer%2C%20we%20review%20the%20technical%0Afoundations%20of%20FWPs%2C%20their%20computational%20characteristics%2C%20and%20their%20connections%0Ato%20transformers%20and%20state%20space%20models.%20We%20also%20discuss%20connections%20between%0AFWPs%20and%20models%20of%20synaptic%20plasticity%20in%20the%20brain%2C%20suggesting%20a%20convergence%0Aof%20natural%20and%20artificial%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08435v2&entry.124074799=Read"},
{"title": "Harmonious Color Pairings: Insights from Human Preference and Natural\n  Hue Statistics", "author": "Ortensia Forni and Alexandre Darmon and Michael Benzaquen", "abstract": "  While color harmony has long been studied in art and design, a clear\nconsensus remains elusive, as most models are grounded in qualitative insights\nor limited datasets. In this work, we present a quantitative, data-driven study\nof color pairing preferences using controlled hue-based palettes in the HSL\ncolor space. Participants evaluated combinations of thirteen distinct hues,\nenabling us to construct a preference matrix and define a combinability index\nfor each color. Our results reveal that preferences are highly hue dependent,\nchallenging the assumption of universal harmony rules proposed in the\nliterature. Yet, when averaged over hues, statistically meaningful patterns of\naesthetic preference emerge, with certain hue separations perceived as more\nharmonious. Strikingly, these patterns align with hue distributions found in\nnatural landscapes, pointing to a statistical correspondence between human\ncolor preferences and the structure of color in nature. Finally, we analyze our\ncolor-pairing score matrix through principal component analysis, which uncovers\ntwo complementary hue groups whose interplay underlies the global structure of\ncolor-pairing preferences. Together, these findings offer a quantitative\nframework for studying color harmony and its potential perceptual and\necological underpinnings.\n", "link": "http://arxiv.org/abs/2508.15777v2", "date": "2025-11-05", "relevancy": 1.2743, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4478}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4204}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harmonious%20Color%20Pairings%3A%20Insights%20from%20Human%20Preference%20and%20Natural%0A%20%20Hue%20Statistics&body=Title%3A%20Harmonious%20Color%20Pairings%3A%20Insights%20from%20Human%20Preference%20and%20Natural%0A%20%20Hue%20Statistics%0AAuthor%3A%20Ortensia%20Forni%20and%20Alexandre%20Darmon%20and%20Michael%20Benzaquen%0AAbstract%3A%20%20%20While%20color%20harmony%20has%20long%20been%20studied%20in%20art%20and%20design%2C%20a%20clear%0Aconsensus%20remains%20elusive%2C%20as%20most%20models%20are%20grounded%20in%20qualitative%20insights%0Aor%20limited%20datasets.%20In%20this%20work%2C%20we%20present%20a%20quantitative%2C%20data-driven%20study%0Aof%20color%20pairing%20preferences%20using%20controlled%20hue-based%20palettes%20in%20the%20HSL%0Acolor%20space.%20Participants%20evaluated%20combinations%20of%20thirteen%20distinct%20hues%2C%0Aenabling%20us%20to%20construct%20a%20preference%20matrix%20and%20define%20a%20combinability%20index%0Afor%20each%20color.%20Our%20results%20reveal%20that%20preferences%20are%20highly%20hue%20dependent%2C%0Achallenging%20the%20assumption%20of%20universal%20harmony%20rules%20proposed%20in%20the%0Aliterature.%20Yet%2C%20when%20averaged%20over%20hues%2C%20statistically%20meaningful%20patterns%20of%0Aaesthetic%20preference%20emerge%2C%20with%20certain%20hue%20separations%20perceived%20as%20more%0Aharmonious.%20Strikingly%2C%20these%20patterns%20align%20with%20hue%20distributions%20found%20in%0Anatural%20landscapes%2C%20pointing%20to%20a%20statistical%20correspondence%20between%20human%0Acolor%20preferences%20and%20the%20structure%20of%20color%20in%20nature.%20Finally%2C%20we%20analyze%20our%0Acolor-pairing%20score%20matrix%20through%20principal%20component%20analysis%2C%20which%20uncovers%0Atwo%20complementary%20hue%20groups%20whose%20interplay%20underlies%20the%20global%20structure%20of%0Acolor-pairing%20preferences.%20Together%2C%20these%20findings%20offer%20a%20quantitative%0Aframework%20for%20studying%20color%20harmony%20and%20its%20potential%20perceptual%20and%0Aecological%20underpinnings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15777v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarmonious%2520Color%2520Pairings%253A%2520Insights%2520from%2520Human%2520Preference%2520and%2520Natural%250A%2520%2520Hue%2520Statistics%26entry.906535625%3DOrtensia%2520Forni%2520and%2520Alexandre%2520Darmon%2520and%2520Michael%2520Benzaquen%26entry.1292438233%3D%2520%2520While%2520color%2520harmony%2520has%2520long%2520been%2520studied%2520in%2520art%2520and%2520design%252C%2520a%2520clear%250Aconsensus%2520remains%2520elusive%252C%2520as%2520most%2520models%2520are%2520grounded%2520in%2520qualitative%2520insights%250Aor%2520limited%2520datasets.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520quantitative%252C%2520data-driven%2520study%250Aof%2520color%2520pairing%2520preferences%2520using%2520controlled%2520hue-based%2520palettes%2520in%2520the%2520HSL%250Acolor%2520space.%2520Participants%2520evaluated%2520combinations%2520of%2520thirteen%2520distinct%2520hues%252C%250Aenabling%2520us%2520to%2520construct%2520a%2520preference%2520matrix%2520and%2520define%2520a%2520combinability%2520index%250Afor%2520each%2520color.%2520Our%2520results%2520reveal%2520that%2520preferences%2520are%2520highly%2520hue%2520dependent%252C%250Achallenging%2520the%2520assumption%2520of%2520universal%2520harmony%2520rules%2520proposed%2520in%2520the%250Aliterature.%2520Yet%252C%2520when%2520averaged%2520over%2520hues%252C%2520statistically%2520meaningful%2520patterns%2520of%250Aaesthetic%2520preference%2520emerge%252C%2520with%2520certain%2520hue%2520separations%2520perceived%2520as%2520more%250Aharmonious.%2520Strikingly%252C%2520these%2520patterns%2520align%2520with%2520hue%2520distributions%2520found%2520in%250Anatural%2520landscapes%252C%2520pointing%2520to%2520a%2520statistical%2520correspondence%2520between%2520human%250Acolor%2520preferences%2520and%2520the%2520structure%2520of%2520color%2520in%2520nature.%2520Finally%252C%2520we%2520analyze%2520our%250Acolor-pairing%2520score%2520matrix%2520through%2520principal%2520component%2520analysis%252C%2520which%2520uncovers%250Atwo%2520complementary%2520hue%2520groups%2520whose%2520interplay%2520underlies%2520the%2520global%2520structure%2520of%250Acolor-pairing%2520preferences.%2520Together%252C%2520these%2520findings%2520offer%2520a%2520quantitative%250Aframework%2520for%2520studying%2520color%2520harmony%2520and%2520its%2520potential%2520perceptual%2520and%250Aecological%2520underpinnings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15777v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harmonious%20Color%20Pairings%3A%20Insights%20from%20Human%20Preference%20and%20Natural%0A%20%20Hue%20Statistics&entry.906535625=Ortensia%20Forni%20and%20Alexandre%20Darmon%20and%20Michael%20Benzaquen&entry.1292438233=%20%20While%20color%20harmony%20has%20long%20been%20studied%20in%20art%20and%20design%2C%20a%20clear%0Aconsensus%20remains%20elusive%2C%20as%20most%20models%20are%20grounded%20in%20qualitative%20insights%0Aor%20limited%20datasets.%20In%20this%20work%2C%20we%20present%20a%20quantitative%2C%20data-driven%20study%0Aof%20color%20pairing%20preferences%20using%20controlled%20hue-based%20palettes%20in%20the%20HSL%0Acolor%20space.%20Participants%20evaluated%20combinations%20of%20thirteen%20distinct%20hues%2C%0Aenabling%20us%20to%20construct%20a%20preference%20matrix%20and%20define%20a%20combinability%20index%0Afor%20each%20color.%20Our%20results%20reveal%20that%20preferences%20are%20highly%20hue%20dependent%2C%0Achallenging%20the%20assumption%20of%20universal%20harmony%20rules%20proposed%20in%20the%0Aliterature.%20Yet%2C%20when%20averaged%20over%20hues%2C%20statistically%20meaningful%20patterns%20of%0Aaesthetic%20preference%20emerge%2C%20with%20certain%20hue%20separations%20perceived%20as%20more%0Aharmonious.%20Strikingly%2C%20these%20patterns%20align%20with%20hue%20distributions%20found%20in%0Anatural%20landscapes%2C%20pointing%20to%20a%20statistical%20correspondence%20between%20human%0Acolor%20preferences%20and%20the%20structure%20of%20color%20in%20nature.%20Finally%2C%20we%20analyze%20our%0Acolor-pairing%20score%20matrix%20through%20principal%20component%20analysis%2C%20which%20uncovers%0Atwo%20complementary%20hue%20groups%20whose%20interplay%20underlies%20the%20global%20structure%20of%0Acolor-pairing%20preferences.%20Together%2C%20these%20findings%20offer%20a%20quantitative%0Aframework%20for%20studying%20color%20harmony%20and%20its%20potential%20perceptual%20and%0Aecological%20underpinnings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15777v2&entry.124074799=Read"},
{"title": "Bridging the Gap between Empirical Welfare Maximization and Conditional\n  Average Treatment Effect Estimation in Policy Learning", "author": "Masahiro Kato", "abstract": "  The goal of policy learning is to train a policy function that recommends a\ntreatment given covariates to maximize population welfare. There are two major\napproaches in policy learning: the empirical welfare maximization (EWM)\napproach and the plug-in approach. The EWM approach is analogous to a\nclassification problem, where one first builds an estimator of the population\nwelfare, which is a functional of policy functions, and then trains a policy by\nmaximizing the estimated welfare. In contrast, the plug-in approach is based on\nregression, where one first estimates the conditional average treatment effect\n(CATE) and then recommends the treatment with the highest estimated outcome.\nThis study bridges the gap between the two approaches by showing that both are\nbased on essentially the same optimization problem. In particular, we prove an\nexact equivalence between EWM and least squares over a reparameterization of\nthe policy class. As a consequence, the two approaches are interchangeable in\nseveral respects and share the same theoretical guarantees under common\nconditions. Leveraging this equivalence, we propose a regularization method for\npolicy learning. The reduction to least squares yields a smooth surrogate that\nis typically easier to optimize in practice. At the same time, for many natural\npolicy classes the inherent combinatorial hardness of exact EWM generally\nremains, so the reduction should be viewed as an optimization aid rather than a\nuniversal bypass of NP-hardness.\n", "link": "http://arxiv.org/abs/2510.26723v2", "date": "2025-11-05", "relevancy": 1.2837, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4517}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4225}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20the%20Gap%20between%20Empirical%20Welfare%20Maximization%20and%20Conditional%0A%20%20Average%20Treatment%20Effect%20Estimation%20in%20Policy%20Learning&body=Title%3A%20Bridging%20the%20Gap%20between%20Empirical%20Welfare%20Maximization%20and%20Conditional%0A%20%20Average%20Treatment%20Effect%20Estimation%20in%20Policy%20Learning%0AAuthor%3A%20Masahiro%20Kato%0AAbstract%3A%20%20%20The%20goal%20of%20policy%20learning%20is%20to%20train%20a%20policy%20function%20that%20recommends%20a%0Atreatment%20given%20covariates%20to%20maximize%20population%20welfare.%20There%20are%20two%20major%0Aapproaches%20in%20policy%20learning%3A%20the%20empirical%20welfare%20maximization%20%28EWM%29%0Aapproach%20and%20the%20plug-in%20approach.%20The%20EWM%20approach%20is%20analogous%20to%20a%0Aclassification%20problem%2C%20where%20one%20first%20builds%20an%20estimator%20of%20the%20population%0Awelfare%2C%20which%20is%20a%20functional%20of%20policy%20functions%2C%20and%20then%20trains%20a%20policy%20by%0Amaximizing%20the%20estimated%20welfare.%20In%20contrast%2C%20the%20plug-in%20approach%20is%20based%20on%0Aregression%2C%20where%20one%20first%20estimates%20the%20conditional%20average%20treatment%20effect%0A%28CATE%29%20and%20then%20recommends%20the%20treatment%20with%20the%20highest%20estimated%20outcome.%0AThis%20study%20bridges%20the%20gap%20between%20the%20two%20approaches%20by%20showing%20that%20both%20are%0Abased%20on%20essentially%20the%20same%20optimization%20problem.%20In%20particular%2C%20we%20prove%20an%0Aexact%20equivalence%20between%20EWM%20and%20least%20squares%20over%20a%20reparameterization%20of%0Athe%20policy%20class.%20As%20a%20consequence%2C%20the%20two%20approaches%20are%20interchangeable%20in%0Aseveral%20respects%20and%20share%20the%20same%20theoretical%20guarantees%20under%20common%0Aconditions.%20Leveraging%20this%20equivalence%2C%20we%20propose%20a%20regularization%20method%20for%0Apolicy%20learning.%20The%20reduction%20to%20least%20squares%20yields%20a%20smooth%20surrogate%20that%0Ais%20typically%20easier%20to%20optimize%20in%20practice.%20At%20the%20same%20time%2C%20for%20many%20natural%0Apolicy%20classes%20the%20inherent%20combinatorial%20hardness%20of%20exact%20EWM%20generally%0Aremains%2C%20so%20the%20reduction%20should%20be%20viewed%20as%20an%20optimization%20aid%20rather%20than%20a%0Auniversal%20bypass%20of%20NP-hardness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26723v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520the%2520Gap%2520between%2520Empirical%2520Welfare%2520Maximization%2520and%2520Conditional%250A%2520%2520Average%2520Treatment%2520Effect%2520Estimation%2520in%2520Policy%2520Learning%26entry.906535625%3DMasahiro%2520Kato%26entry.1292438233%3D%2520%2520The%2520goal%2520of%2520policy%2520learning%2520is%2520to%2520train%2520a%2520policy%2520function%2520that%2520recommends%2520a%250Atreatment%2520given%2520covariates%2520to%2520maximize%2520population%2520welfare.%2520There%2520are%2520two%2520major%250Aapproaches%2520in%2520policy%2520learning%253A%2520the%2520empirical%2520welfare%2520maximization%2520%2528EWM%2529%250Aapproach%2520and%2520the%2520plug-in%2520approach.%2520The%2520EWM%2520approach%2520is%2520analogous%2520to%2520a%250Aclassification%2520problem%252C%2520where%2520one%2520first%2520builds%2520an%2520estimator%2520of%2520the%2520population%250Awelfare%252C%2520which%2520is%2520a%2520functional%2520of%2520policy%2520functions%252C%2520and%2520then%2520trains%2520a%2520policy%2520by%250Amaximizing%2520the%2520estimated%2520welfare.%2520In%2520contrast%252C%2520the%2520plug-in%2520approach%2520is%2520based%2520on%250Aregression%252C%2520where%2520one%2520first%2520estimates%2520the%2520conditional%2520average%2520treatment%2520effect%250A%2528CATE%2529%2520and%2520then%2520recommends%2520the%2520treatment%2520with%2520the%2520highest%2520estimated%2520outcome.%250AThis%2520study%2520bridges%2520the%2520gap%2520between%2520the%2520two%2520approaches%2520by%2520showing%2520that%2520both%2520are%250Abased%2520on%2520essentially%2520the%2520same%2520optimization%2520problem.%2520In%2520particular%252C%2520we%2520prove%2520an%250Aexact%2520equivalence%2520between%2520EWM%2520and%2520least%2520squares%2520over%2520a%2520reparameterization%2520of%250Athe%2520policy%2520class.%2520As%2520a%2520consequence%252C%2520the%2520two%2520approaches%2520are%2520interchangeable%2520in%250Aseveral%2520respects%2520and%2520share%2520the%2520same%2520theoretical%2520guarantees%2520under%2520common%250Aconditions.%2520Leveraging%2520this%2520equivalence%252C%2520we%2520propose%2520a%2520regularization%2520method%2520for%250Apolicy%2520learning.%2520The%2520reduction%2520to%2520least%2520squares%2520yields%2520a%2520smooth%2520surrogate%2520that%250Ais%2520typically%2520easier%2520to%2520optimize%2520in%2520practice.%2520At%2520the%2520same%2520time%252C%2520for%2520many%2520natural%250Apolicy%2520classes%2520the%2520inherent%2520combinatorial%2520hardness%2520of%2520exact%2520EWM%2520generally%250Aremains%252C%2520so%2520the%2520reduction%2520should%2520be%2520viewed%2520as%2520an%2520optimization%2520aid%2520rather%2520than%2520a%250Auniversal%2520bypass%2520of%2520NP-hardness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26723v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20the%20Gap%20between%20Empirical%20Welfare%20Maximization%20and%20Conditional%0A%20%20Average%20Treatment%20Effect%20Estimation%20in%20Policy%20Learning&entry.906535625=Masahiro%20Kato&entry.1292438233=%20%20The%20goal%20of%20policy%20learning%20is%20to%20train%20a%20policy%20function%20that%20recommends%20a%0Atreatment%20given%20covariates%20to%20maximize%20population%20welfare.%20There%20are%20two%20major%0Aapproaches%20in%20policy%20learning%3A%20the%20empirical%20welfare%20maximization%20%28EWM%29%0Aapproach%20and%20the%20plug-in%20approach.%20The%20EWM%20approach%20is%20analogous%20to%20a%0Aclassification%20problem%2C%20where%20one%20first%20builds%20an%20estimator%20of%20the%20population%0Awelfare%2C%20which%20is%20a%20functional%20of%20policy%20functions%2C%20and%20then%20trains%20a%20policy%20by%0Amaximizing%20the%20estimated%20welfare.%20In%20contrast%2C%20the%20plug-in%20approach%20is%20based%20on%0Aregression%2C%20where%20one%20first%20estimates%20the%20conditional%20average%20treatment%20effect%0A%28CATE%29%20and%20then%20recommends%20the%20treatment%20with%20the%20highest%20estimated%20outcome.%0AThis%20study%20bridges%20the%20gap%20between%20the%20two%20approaches%20by%20showing%20that%20both%20are%0Abased%20on%20essentially%20the%20same%20optimization%20problem.%20In%20particular%2C%20we%20prove%20an%0Aexact%20equivalence%20between%20EWM%20and%20least%20squares%20over%20a%20reparameterization%20of%0Athe%20policy%20class.%20As%20a%20consequence%2C%20the%20two%20approaches%20are%20interchangeable%20in%0Aseveral%20respects%20and%20share%20the%20same%20theoretical%20guarantees%20under%20common%0Aconditions.%20Leveraging%20this%20equivalence%2C%20we%20propose%20a%20regularization%20method%20for%0Apolicy%20learning.%20The%20reduction%20to%20least%20squares%20yields%20a%20smooth%20surrogate%20that%0Ais%20typically%20easier%20to%20optimize%20in%20practice.%20At%20the%20same%20time%2C%20for%20many%20natural%0Apolicy%20classes%20the%20inherent%20combinatorial%20hardness%20of%20exact%20EWM%20generally%0Aremains%2C%20so%20the%20reduction%20should%20be%20viewed%20as%20an%20optimization%20aid%20rather%20than%20a%0Auniversal%20bypass%20of%20NP-hardness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26723v2&entry.124074799=Read"},
{"title": "PDE-SHARP: PDE Solver Hybrids through Analysis and Refinement Passes", "author": "Shaghayegh Fazliani and Madeleine Udell", "abstract": "  Current LLM-driven approaches using test-time computing to generate PDE\nsolvers execute a large number of solver samples to identify high-accuracy\nsolvers. These paradigms are especially costly for complex PDEs requiring\nsubstantial computational resources for numerical evaluation. We introduce\nPDE-SHARP, a framework to reduce computational costs by replacing expensive\nscientific computation by cheaper LLM inference that achieves superior solver\naccuracy with 60-75% fewer computational evaluations. PDE-SHARP employs three\nstages: (1) Analysis: mathematical chain-of-thought analysis including PDE\nclassification, solution type detection, and stability analysis; (2) Genesis:\nsolver generation based on mathematical insights from the previous stage; and\n(3) Synthesis: collaborative selection-hybridization tournaments in which LLM\njudges iteratively refine implementations through flexible performance\nfeedback. To generate high-quality solvers, PDE-SHARP requires fewer than 13\nsolver evaluations on average compared to 30+ for baseline methods, improving\naccuracy uniformly across tested PDEs by $4\\times$ on average, and demonstrates\nrobust performance across LLM architectures, from general-purpose to\nspecialized reasoning models.\n", "link": "http://arxiv.org/abs/2511.00183v2", "date": "2025-11-05", "relevancy": 1.2827, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.433}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4266}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PDE-SHARP%3A%20PDE%20Solver%20Hybrids%20through%20Analysis%20and%20Refinement%20Passes&body=Title%3A%20PDE-SHARP%3A%20PDE%20Solver%20Hybrids%20through%20Analysis%20and%20Refinement%20Passes%0AAuthor%3A%20Shaghayegh%20Fazliani%20and%20Madeleine%20Udell%0AAbstract%3A%20%20%20Current%20LLM-driven%20approaches%20using%20test-time%20computing%20to%20generate%20PDE%0Asolvers%20execute%20a%20large%20number%20of%20solver%20samples%20to%20identify%20high-accuracy%0Asolvers.%20These%20paradigms%20are%20especially%20costly%20for%20complex%20PDEs%20requiring%0Asubstantial%20computational%20resources%20for%20numerical%20evaluation.%20We%20introduce%0APDE-SHARP%2C%20a%20framework%20to%20reduce%20computational%20costs%20by%20replacing%20expensive%0Ascientific%20computation%20by%20cheaper%20LLM%20inference%20that%20achieves%20superior%20solver%0Aaccuracy%20with%2060-75%25%20fewer%20computational%20evaluations.%20PDE-SHARP%20employs%20three%0Astages%3A%20%281%29%20Analysis%3A%20mathematical%20chain-of-thought%20analysis%20including%20PDE%0Aclassification%2C%20solution%20type%20detection%2C%20and%20stability%20analysis%3B%20%282%29%20Genesis%3A%0Asolver%20generation%20based%20on%20mathematical%20insights%20from%20the%20previous%20stage%3B%20and%0A%283%29%20Synthesis%3A%20collaborative%20selection-hybridization%20tournaments%20in%20which%20LLM%0Ajudges%20iteratively%20refine%20implementations%20through%20flexible%20performance%0Afeedback.%20To%20generate%20high-quality%20solvers%2C%20PDE-SHARP%20requires%20fewer%20than%2013%0Asolver%20evaluations%20on%20average%20compared%20to%2030%2B%20for%20baseline%20methods%2C%20improving%0Aaccuracy%20uniformly%20across%20tested%20PDEs%20by%20%244%5Ctimes%24%20on%20average%2C%20and%20demonstrates%0Arobust%20performance%20across%20LLM%20architectures%2C%20from%20general-purpose%20to%0Aspecialized%20reasoning%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.00183v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPDE-SHARP%253A%2520PDE%2520Solver%2520Hybrids%2520through%2520Analysis%2520and%2520Refinement%2520Passes%26entry.906535625%3DShaghayegh%2520Fazliani%2520and%2520Madeleine%2520Udell%26entry.1292438233%3D%2520%2520Current%2520LLM-driven%2520approaches%2520using%2520test-time%2520computing%2520to%2520generate%2520PDE%250Asolvers%2520execute%2520a%2520large%2520number%2520of%2520solver%2520samples%2520to%2520identify%2520high-accuracy%250Asolvers.%2520These%2520paradigms%2520are%2520especially%2520costly%2520for%2520complex%2520PDEs%2520requiring%250Asubstantial%2520computational%2520resources%2520for%2520numerical%2520evaluation.%2520We%2520introduce%250APDE-SHARP%252C%2520a%2520framework%2520to%2520reduce%2520computational%2520costs%2520by%2520replacing%2520expensive%250Ascientific%2520computation%2520by%2520cheaper%2520LLM%2520inference%2520that%2520achieves%2520superior%2520solver%250Aaccuracy%2520with%252060-75%2525%2520fewer%2520computational%2520evaluations.%2520PDE-SHARP%2520employs%2520three%250Astages%253A%2520%25281%2529%2520Analysis%253A%2520mathematical%2520chain-of-thought%2520analysis%2520including%2520PDE%250Aclassification%252C%2520solution%2520type%2520detection%252C%2520and%2520stability%2520analysis%253B%2520%25282%2529%2520Genesis%253A%250Asolver%2520generation%2520based%2520on%2520mathematical%2520insights%2520from%2520the%2520previous%2520stage%253B%2520and%250A%25283%2529%2520Synthesis%253A%2520collaborative%2520selection-hybridization%2520tournaments%2520in%2520which%2520LLM%250Ajudges%2520iteratively%2520refine%2520implementations%2520through%2520flexible%2520performance%250Afeedback.%2520To%2520generate%2520high-quality%2520solvers%252C%2520PDE-SHARP%2520requires%2520fewer%2520than%252013%250Asolver%2520evaluations%2520on%2520average%2520compared%2520to%252030%252B%2520for%2520baseline%2520methods%252C%2520improving%250Aaccuracy%2520uniformly%2520across%2520tested%2520PDEs%2520by%2520%25244%255Ctimes%2524%2520on%2520average%252C%2520and%2520demonstrates%250Arobust%2520performance%2520across%2520LLM%2520architectures%252C%2520from%2520general-purpose%2520to%250Aspecialized%2520reasoning%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.00183v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PDE-SHARP%3A%20PDE%20Solver%20Hybrids%20through%20Analysis%20and%20Refinement%20Passes&entry.906535625=Shaghayegh%20Fazliani%20and%20Madeleine%20Udell&entry.1292438233=%20%20Current%20LLM-driven%20approaches%20using%20test-time%20computing%20to%20generate%20PDE%0Asolvers%20execute%20a%20large%20number%20of%20solver%20samples%20to%20identify%20high-accuracy%0Asolvers.%20These%20paradigms%20are%20especially%20costly%20for%20complex%20PDEs%20requiring%0Asubstantial%20computational%20resources%20for%20numerical%20evaluation.%20We%20introduce%0APDE-SHARP%2C%20a%20framework%20to%20reduce%20computational%20costs%20by%20replacing%20expensive%0Ascientific%20computation%20by%20cheaper%20LLM%20inference%20that%20achieves%20superior%20solver%0Aaccuracy%20with%2060-75%25%20fewer%20computational%20evaluations.%20PDE-SHARP%20employs%20three%0Astages%3A%20%281%29%20Analysis%3A%20mathematical%20chain-of-thought%20analysis%20including%20PDE%0Aclassification%2C%20solution%20type%20detection%2C%20and%20stability%20analysis%3B%20%282%29%20Genesis%3A%0Asolver%20generation%20based%20on%20mathematical%20insights%20from%20the%20previous%20stage%3B%20and%0A%283%29%20Synthesis%3A%20collaborative%20selection-hybridization%20tournaments%20in%20which%20LLM%0Ajudges%20iteratively%20refine%20implementations%20through%20flexible%20performance%0Afeedback.%20To%20generate%20high-quality%20solvers%2C%20PDE-SHARP%20requires%20fewer%20than%2013%0Asolver%20evaluations%20on%20average%20compared%20to%2030%2B%20for%20baseline%20methods%2C%20improving%0Aaccuracy%20uniformly%20across%20tested%20PDEs%20by%20%244%5Ctimes%24%20on%20average%2C%20and%20demonstrates%0Arobust%20performance%20across%20LLM%20architectures%2C%20from%20general-purpose%20to%0Aspecialized%20reasoning%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.00183v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


